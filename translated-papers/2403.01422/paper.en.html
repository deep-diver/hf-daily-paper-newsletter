<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies\n' +
      '\n' +
      'Zhende Song\\({}^{1}\\)  Chenchenen Wang\\({}^{1}\\)1  Jiamu Sheng\\({}^{1}\\)1  Chi Zhang\\({}^{2}\\)2\n' +
      '\n' +
      'Gang Yu\\({}^{2}\\)  Jiayuan Fan\\({}^{1}\\)3  Tao Chen\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\) Fudan University \\({}^{2}\\) Tencent PCG\n' +
      '\n' +
      'zdsong23@m.fudan.edu.cn (20307130244, jmsheng18, jyfan, eetchen)@fudan.edu.cn\n' +
      '\n' +
      '{johnczhang, skicyyu}@tencent.com\n' +
      '\n' +
      '[https://deaddawn.github.io/MovieLLM/](https://deaddawn.github.io/MovieLLM/)\n' +
      '\n' +
      'Footnote 1: The first three authors contributed equally to this work.\n' +
      '\n' +
      'Footnote 2: Project Leader.\n' +
      '\n' +
      'Footnote 3: Corresponding Author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The development of multimodal models has marked a significant step forward in how machines understand videos. These models have shown promise in analyzing short video clips. However, when it comes to longer formats like movies, they often fall short. The main hurdles are the lack of high-quality, diverse video data and the intensive work required to collect or annotate such data. In the face of these challenges, we propose MovieLLM, a novel framework designed to create synthetic, high-quality data for long videos. This framework leverages the power of GPT-4 and text-to-image models to generate detailed scripts and corresponding visuals. Our approach stands out for its flexibility and scalability, making it a superior alternative to traditional data collection methods. Our extensive experiments validate that the data produced by MovieLLM significantly improves the performance of multimodal models in understanding complex video narratives, overcoming the limitations of existing datasets regarding scarcity and bias.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, the field of artificial intelligence has witnessed significant progress in the development of large language models (LLMs) such as GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023). These models have shown exceptional ability in generating coherent and contextually relevant text. Further extending the capabilities of LLMs into the multimodal domain, vision language models (VLMs) like LLaVA (Liu et al., 2023) have demonstrated remarkable proficiency in tasks that require an understanding of both visual and linguistic information, such as image captioning and visual question-answering (QA). The evolution of VLMs has now begun to include the dynamic realm of video understanding, with models like VideoLLaMA (Zhang et al., 2023) and VideoChat (Li\n' +
      '\n' +
      'Figure 1: **Examples of generated long video instruction data.** We use GPT-4 and guided text-to-image generation models to generate consistent key frames of move-level video with reasonable lines and corresponding question-answer pairs. These data are used to train multimodal large language models on video understanding.\n' +
      '\n' +
      'et al., 2023) highlighting the potential of VLMs in processing and interpreting video content.\n' +
      '\n' +
      'However, a notable gap remains in the ability of these models to understand long-duration videos, such as full-length movies. This limitation is primarily attributed to the lack of extensive long video datasets necessary for tuning these models. The LLaMA-VID (Li et al., 2023) initiative represents a step forward in this direction by creating long-video tuning datasets derived from MovieNet (Huang et al., 2020), indicating that fine-tuning LLMs on multimodal long video datasets can enhance their movie-level video comprehension. Nevertheless, constructing such benchmarks from existing datasets is both cost-intensive and challenging, particularly due to the manual effort required to collect or annotate additional information like movie dialogues. Moreover, despite showing promise, the QA dataset built upon MovieNet confronts several hurdles. These include the laborious nature of manual data collection and the limited diversity in content, which can impede the generalization capabilities of models due to inherent dataset biases.\n' +
      '\n' +
      'Motivated by these challenges, our paper introduces a novel and flexible approach for generating comprehensive datasets for long video instruction tuning. Our methodology leverages the linguistic prowess of GPT-4 and the generative power of stable diffusion models, offering a unique solution to the problem of dataset diversity and richness. Our pipeline comprises three primary stages:\n' +
      '\n' +
      '1) **Movie Plot Generation.** Rather than limiting plot generation to conventional data sources such as the web or existing datasets, we harness the power of GPT-4 to produce synthesized data. By providing specific elements such as themes, overview, and styles, we guide GPT-4 to produce movie-level key frame descriptions tailored to the latter generation process.\n' +
      '\n' +
      '2) **Style Immobilization Process.** By adeptly employing textual inversion (Gal et al., 2022), we immobilize the style descriptions generated from the script onto the latent space of the diffusion model. This approach guides the model to generate scenes in a fixed style while maintaining diversity under a unified aesthetic.\n' +
      '\n' +
      '3) **Video instruction data generation.** By integrating the powerful generative capabilities of GPT-4 with the developed style-guided diffusion model, we produce style-consistent key frames and corresponding QA pairs, resulting in a comprehensive instruction tuning corpus, combining the visual data with QA pairs.\n' +
      '\n' +
      'Our approach not only addresses the limitations of current datasets for long videos but also paves the way for integrating advanced diffusion models to generate video data at higher frame rates. Moreover, our methodology allows for the generation of datasets without constraints on data volume, ensuring a high degree of diversity within the generated content. Additionally, it facilitates automatic annotation, significantly reducing the need for manual labor and associated costs. These advantages enhance the scalability, richness, and efficiency of dataset creation for long video understanding, marking a significant leap forward in the field. Examples from our generated long video instruction data are shown in Fig. 1.\n' +
      '\n' +
      'Our contributions are summarized as follows:\n' +
      '\n' +
      '* We develop a novel pipeline for generating movie-level video instruction tuning datasets by combining GPT-4 and diffusion models.\n' +
      '* Leveraging our generative approach, we have developed and will publicly release a comprehensive dataset for movie-level video understanding, alongside a sophisticated model trained for enhanced understanding of long videos.\n' +
      '* Based on a real movie dataset, we propose a benchmark for evaluating long video comprehension capabilities. Experiments on the benchmark show the effectiveness of the proposed method, significantly outperforming the baseline.\n' +
      '\n' +
      '## 2 Preliminary\n' +
      '\n' +
      'To generate key frames with coherent scenes, we employed the technique, textual inversion (Gal et al., 2022). This preliminary section provides an overview of the textual inversion to lay the groundwork for a detailed exposition of our pipeline.\n' +
      '\n' +
      'Textual InversionThis technique aims to enable language-guided image generation of new, user-specified concepts in a Text-to-Image (T2I) model. To achieve this objective, two main steps are required: 1) text-embedding, this process first converts a string incorporating placeholder words into tokens. Then, these tokens are converted to a continuous vector representation. 2) inversion process,the embedding is then transformed into a single conditioning code that guides the generative model. The embedding vector associated with the so-called pseudo-word will be optimized using a reconstruction objective. As such, the embedding is motivated to capture visual details unique to the concept. The learned embedding is then used as a condition to guide the T2I model to generate customized images.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'This section elucidates our novel approach for generating movie-level video instruction tuning data. Our method compromises three interrelated steps: movie plot generation, style immobilization process, and video instruction data generation. The complete process is depicted in Fig. 2. The details of these steps are provided in the following subsections.\n' +
      '\n' +
      '### Movie Plot Generation\n' +
      '\n' +
      'The primary objective of this step is to harness the powerful capabilities of GPT-4 to generate diverse and compelling movie plots. Each movie plot comprises basic elements, including overview, characters, and frame descriptions. This aims to construct a high-quality movie plot text for the subsequent style immobilization and generation process. We propose three main strategies for this purpose:\n' +
      '\n' +
      '1) We first employ GPT-4 to generate a set of plausible movie scripts. Subsequently, we construct reusable prompts based on these scripts to generate a variety of movie plots in bulk. During this process, phrases describing movie themes, like "a tragic film," serve as variables enabling us to direct GPT-4 in creating movie plots based on specific themes.\n' +
      '\n' +
      '2) Within the movie plot text, we incorporate two key elements: character and style. Utilizing the character and style descriptions generated by GPT-4, we aim to ensure great consistency in both the portrayal of characters and the stylistic attributes across key frames in the video.\n' +
      '\n' +
      '3) The substantial number of key frames in movie plots poses a challenge, as LLMs are prone to forgetting plot details during continuous generation processes. Therefore, we propose a story expansion strategy to obtain the continuous key frame descriptions and maximally mitigate the forgetting issue inherent in LLMs. We divide the description\n' +
      '\n' +
      'Figure 2: **Our proposed pipeline for generating long video instruction tuning datasets.** With merely a simple thematic description, our pipeline is capable of generating key frames of an entire film. The pipeline can be roughly divided into three stages: (a) movie plot generation, where we generate the whole movie plot based on a theme phrase using GPT-4. (b) style immobilization process, where we leverage the textual inversion technique to immobilize the style descriptions generated from the plot into the latent space of the diffusion model, guiding it to generate frames with fixed style. (c) video instruction data generation, where we integrate all the previously obtained information to ultimately generate consistent key frames and corresponding question-and-answer pairs.\n' +
      '\n' +
      'generation process into three levels.\n' +
      '\n' +
      'We now elucidate how we construct the prompt to obtain high-quality movie plot text. Details can be found in appendix B.1. Our prompt is comprised of the following elements:\n' +
      '\n' +
      '**Movie Theme**: As mentioned above, we treat the phrases used to describe a movie theme as variables. Therefore, various movie themes are first generated to ensure the diversity of movie data.\n' +
      '\n' +
      '**Overview**: To provide a referable foundation for the subsequent generation of the movie plot, we initially generate an overview based on the movie theme. This will also facilitate the consistency of the whole movie plot and the generation of summary-type QA.\n' +
      '\n' +
      '**Style**: Another significant element is the movie style which serves as a groundwork for the style inversion process. Appropriate, theme-specific style descriptions can assist in generating scene images related to the movie theme. Through textual inversion, this process facilitates the training of style embeddings. This, in turn, contributes to the consistency of all scenes throughout the movie.\n' +
      '\n' +
      '**Characters**: In movie-level video instruction data generation, ensuring consistency in character portrayal is crucial. Therefore, in the movie plot generation process, we require GPT-4 to play the director role in selecting an appropriate character for movie plots and generating character descriptions. Subsequently, we design to leverage the powerful prior knowledge of celebrity faces from pretrained stable diffusion, aiming to maintain excellent character coherence in the generated key frame portraits.\n' +
      '\n' +
      '**Story Expanding to Key Frame Description**: To guarantee the quantity and quality of key frames, we propose to utilize the aforementioned story expansion strategy, dividing the movie plot into three levels: Epoch Chapters, Narrative Threads, and Frame Description. Each chapter within Epoch Chapters represents a significant period or phase in story development. Narrative Threads further refine each major chapter into more specific narrative strands and sub-stories, showcasing the coherence of character development and main events. Frame Description then elaborates on these narrative strands, depicting the critical visual frames. Then we iteratively expand the story from the preceding level to generate three-level movie plot texts, ultimately yielding key frame descriptions, as illustrated in Fig. 3.\n' +
      '\n' +
      '### Style Immobilization Process\n' +
      '\n' +
      'The next step is designed to convert the style descriptions from the generated movie plots into information that can guide a T2I model to generate scenes with a consistent style. To achieve this goal, we utilize the previously introduced technique, textual inversion. The specific steps are as follows: 1) Initially, we utilize the stable diffusion to generate arbitrary style scenes based on the specific style descriptions. 2) Subsequently, we employ textual inversion to convert these scene images into\n' +
      '\n' +
      'Figure 3: **Illustration of the story expansion strategy.** To obtain frame descriptions of a movie-level video and maximally mitigate the forgetting issue, we adopt a three-level story expansion strategy. The strategy can be divided into three levels: (a) epoch chapter generation, where each chapter represents a significant period in story development. (b) narrative threads generation, where each thread refines each major chapter into more specific narrative sub-stories. (c) frame description generation, where each frame description elaborates on these narrative sub-stories, depicting the critical visual frames.\n' +
      '\n' +
      'embeddings through an optimization step. These embeddings can guide the diffusion model in generating images with specific styles. We refer to this entire process as the style immobilization process, which essentially fixes a certain movie style generated by GPT-4 into the latent space as an embedding, corresponding to a specific token. Thus, in the generation of continuous key frame images, using the specific token solely triggers the diffusion model to generate images in a particular style, ensuring the consistency of scenes across frames.\n' +
      '\n' +
      '### Video Instruction Data Generation\n' +
      '\n' +
      'After completing the previous two steps, we now have obtained immobilized style embeddings, celebrity-wise characters, and key frame descriptions. Based on these, we utilize the style embeddings to guide stable diffusion in generating key frames according to the key frame descriptions. Furthermore, we progressively generate various instruction QA pairs based on the movie plot. Here are the details.\n' +
      '\n' +
      'Consistent Key Frame GenerationIt is observed in Feichtenhofer et al. (2019); Li et al. (2023); Zhang et al. (2023); Maaz et al. (2023) that key frames at a rather low frame rate have already been able to meet the models\' requirements for understanding video content, thus our method generates key frames based on corresponding descriptions. To generate key frames that are consistent in both characters and scenes, we initially replace character names in the frame description with corresponding celebrities (chosen by GPT-4). Following that, as previously mentioned, a style-immobilized embedding linked to a special token is utilized. This style embedding can serve as a condition to guide the stable diffusion model in generating scenes in a fixed style. This process is triggered by a special token, which, in this paper, denotes the name of a specific style, such as "**Gothic**" Hence, by incorporating a sentence like "generate an image in **Gothic** style:" at the beginning of our frame description, combined with celebrity-wise character names, we can generate consistent key frames while maintaining the visual diversity.\n' +
      '\n' +
      'QA Pairs GenerationBeyond the textual and visual information inherent to the story itself, enriched QA pairs can well instruct the learning of the model by multimodal information, thereby achieving a superior understanding of movie-level narratives. We utilize GPT-4 to progressively generate rich QA pairs based on all the contents of the movie plot, encompassing aspects of overview, what, where, and why.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      'In this section, we delineate the experiments conducted to substantiate the effectiveness of our innovative pipeline for data generation tailored to movie-level video instruction tuning. We will first introduce our implementation details and dataset statistics, followed by the evaluation metrics, and finally demonstrate our main results and qualitative results.\n' +
      '\n' +
      '### Implementation Details and Dataset Statistics\n' +
      '\n' +
      'Implementation Details.We conduct our video-related experiments on LLaMA-VID. The training procedure of the model is divided into three stages: modality alignment, instruction tuning, and long video tuning. The training conducted during the first two stages endowed the model with the capability to comprehend both images and short videos. Initially, we conducted fine-tuning of LLaMA-VID during the third stage, using their raw long video dataset. We use this trained LLaMA-VID as our base model. Subsequently, we fine-tune the model on our dataset for comparison. Details can be referred to in our appendix.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline Dataset & Video QA & QA per Image & Video Type & QA per Video \\\\ \\hline LLaMA-VID & 9\\% & 0.0054 & 11 & 21 \\\\ \\hline Ours & **13k** & **0.0047** & **15** & **125** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison between our proposed movie-level video instruction dataset and the dataset in LLaMA-VID in terms of data.\n' +
      '\n' +
      'Figure 4: Movie type distribution of movie-level video instruction tuning data.\n' +
      '\n' +
      'Dataset Statistics.We show the statistics of our generated dataset in Table. 1 and Fig. 4. Our instruction tuning data encompasses rich QA pairs that extract in-depth information about video content. We support 15 different genres of movie-level videos, significantly enhancing generalization capability. This is primarily attributed to the robust flexibility of our data construction methodology. Notably, our approach can generate videos of any theme and length, and we plan to continue expanding the richness of our data in the future.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      'We primarily evaluate our approach in two main aspects: the quality of generated video key frames and video understanding.\n' +
      '\n' +
      'Quality of Video Key FramesWe consider these two metrics to evaluate the generated results: 1) **key frame consistency**, which is calculated by average cosine similarity between adjacent embeddings of key frames in CLIP image space. 2) **text-image alignment**, which is calculated by the average cosine similarity between each key frame description and key frame itself. Besides, we use the non-reference image quality assessment metric BRISQUE score Mittal et al. (2012) to evaluate the image quality.\n' +
      '\n' +
      'Video UndersatndingFor video understanding, we conduct evaluations separately for short and long videos. Given that there is no benchmark specifically tailored for movie-level video understanding in existing works, we draw inspiration from the design of metrics for short video understanding and meticulously devise our long video understanding benchmark.\n' +
      '\n' +
      'For short video understanding, we evaluate models on two benchmarks: a) Video-based Generative Performance Benchmarking, which includes five aspects to estimate model performance. b) Zero-shot Question-Answer Evaluation, which is a comprehensive quantitative evaluation using several commonly used open-ended QA datasets, including MSVD-QA Chen and Dolan (2011) and MSRVTT-QA Xu et al. (2016).\n' +
      '\n' +
      'For long video understanding, we present a benchmark tailored to this task. Our test dataset is derived from approximately 100 movies within the MovieNet database, and associated question-answer pairs are generated from both GPT-4 and human efforts. Inspired by Maaz et al. (2023), we redesign the evaluation pipeline using GPT-4. The evaluation prompt can be found in our appendix B.2. This pipeline assesses various capabilities of the model by comparing different predicted answers from different models based on the ground truth and scoring the predicted answers on a scale of 1-5 in the following three aspects:\n' +
      '\n' +
      '1) Overview Understanding: We assess the model\'s comprehension of global video information through questions that involve summarizing video content.\n' +
      '\n' +
      '2) Plot Understanding: We assess the model\'s reasoning capabilities regarding the plot, its understanding of details, and its ability to recognize characters through a set of plot-related questions.\n' +
      '\n' +
      '3) Temporal Understanding: We assess the model\'s understanding of video content from a temporal perspective by having the model arrange the events occurring in the video in temporal order.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'We first compare the quality of key frames generated using our pipeline with similar existing methods. Then, we evaluate the performance on short video understanding and long video understanding.\n' +
      '\n' +
      'Quality of Key FramesWe compare our method based on immobilized style embedding with two similar existing methods: 1) Custom diffusion Kumari et al. (2023), a representative multi-concept customization method. 2) Cones-V2 Liu et al. (2017),\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline Metric & LLaMA-VID & +Ours \\\\ \\hline Correctness \\(\\uparrow\\) & 1.94 & **2.154** (+11\\%) \\\\ Detail \\(\\uparrow\\) & 2.431 & **2.549** (+5\\%) \\\\ Context \\(\\uparrow\\) & 2.701 & **2.880** (+7\\%) \\\\ Temporal \\(\\uparrow\\) & 1.585 & **1.832** (+16\\%) \\\\ Consistency \\(\\uparrow\\) & 1.699 & **1.976** (+16\\%) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Quantitative comparison with baseline model on the video-based generative performance benchmark Maaz et al. (2023).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline Method & Consistency \\(\\uparrow\\) & Alignment \\(\\uparrow\\) & BRISQUE \\(\\downarrow\\) \\\\ \\hline Custom diffusion & 0.7318 & 0.3278 & 22.7554 \\\\ Cones-V2 & 0.7781 & 0.2671 & 23.4823 \\\\ Ours & **0.8080** & **0.3325** & **22.5648** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative comparison with other existing methods on quality of generated key frames.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{MSVD-QA} & \\multicolumn{2}{c}{MSRVTT-QA} \\\\ \\cline{2-4}  & Acc \\(\\uparrow\\) & Score \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Score \\(\\uparrow\\) \\\\ \\hline LLMA-VID & 0.493 & 3.169 & 0.435 & 2.865 \\\\ +Ours & **0.567** (+15\\%) & **3.460** (+9\\%) & **0.513** (+18\\%) & **3.141** (+10\\%) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Quantitative comparison with baseline model on two zero-shot video QA datasets.\n' +
      '\n' +
      '2023b), a multi-subject customizable image synthesis method utilizing layouts. Table. 2 that our method outperforms the existing methods across all three metrics, indicating our method generates consistent and high-quality key frames.\n' +
      '\n' +
      'Short Video UnderstandingThe model trained on our datasets gains significant performance improvement over baseline as listed in Table. 3 and Table. 4. The results indicate that although our data is focused on long video instruction tuning, it can still enhance the model\'s understanding of short videos, demonstrating the robustness of our data.\n' +
      '\n' +
      'Long Video UnderstandingResults on Long Video Understanding are demonstrated in Table. 5. Our method outperforms the baseline by a significant margin in aspects of overview, plot, and temporal understanding. This showcases the richness of our dataset, proving their significant effectiveness in enhancing the model\'s understanding in long videos across various aspects.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      'Fig. 5 visualizes qualitative results on the quality of generated key frames. Our method generates rather more consistent and high-quality frames. Fig. 6 visualizes video understanding results of the baseline model and ours. Our method demonstrates a more reasonable and detailed answer in a movie understanding.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'Vision Language ModelsWith the achievements of large language models (LLMs) such as GPT-4 (OpenAI, 2023) along with their open-source alternatives like LLaMA (Touvron et al., 2023), researchers focus on leveraging the advanced language abilities of LLMs and developing the vision language models (VLMs) that integrate vision models with LLMs for cross-modality understanding. Previous representative VLMs, miniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023a), have shown great capabilities in visual chat by constructing high-quality image-instruction pairs to align the image and text dimensions. Further, VLMs are developed for video understanding (Jin et al., 2024; Li et al., 2023; Lin et al., 2023; Bai et al., 2024; Ma et al., 2023). Video-LLaMA (Zhang et al., 2023) utilizes BLIP-2 (Li et al., 2023a) to\n' +
      '\n' +
      'Figure 5: **Comparison of the key frames generated by ours and other methods. Our method generates rather more consistent and high-quality frames.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Overview} & \\multicolumn{2}{c}{Plot} & \\multicolumn{2}{c}{Temporal} \\\\ \\cline{2-7}  & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) \\\\ \\hline LLaMA-VID & 0.39 & 2.862 & 0.4 & 2.657 & 0.25 & 1.953 \\\\ +Ours & **0.61** (+56\\%) & **2.881** (+11\\%) & **0.6** (+50\\%) & **2.717** (+2\\%) & **0.75** (+200\\%) & **2.572** (+32\\%) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Quantitative comparison with baseline model on our long video understanding benchmark.\n' +
      '\n' +
      'extract video embedding for each frame and fuse video embedding of all frames by video Q-Former. Video-ChatGPT Maaz et al. (2023) designs to extract video embedding by averaging frame-level features across temporal and spatial dimensions respectively. However, previous works require generating a large number of tokens for each frame for representation. For hour-long videos, these VLMs are unable to process extensive video sequences. To tackle this problem, LLaMA-VID Li et al. (2023) is proposed for long video understanding by encoding each frame with only two tokens.\n' +
      '\n' +
      'Video Instruction Tuning DatasetsPreparing video-text pairs to construct video instruction tuning datasets is pivotal in facilitating the training of VLMs for video understanding. Most existing methods Maaz et al. (2023); Tang et al. (2023); Zhong et al. (2022); Gao et al. (2021); Castro et al. (2022); Yang et al. (2023) construct a video instruction tuning dataset based on benchmark datasets Han et al. (2023), utilizing rich annotation information. The Video-ChatGPT, for example, constructs VideoInstruct100K sourced from the ActivityNet dataset Caba Heilbron et al. (2015). However, most existing methods lack long video data in the constructed instruction tuning datasets, resulting in poor long-video understanding capabilities of the trained models. The recently proposed LLaMA-VID addresses this by extracting data from the MovieNet dataset Huang et al. (2020) to build an instruction dataset containing movie-level long videos. However, such datasets are limited by the types of annotations, leading to insufficient diversity in the data. Leveraging our novel pipeline, we propose a diverse movie-level video instruction tuning dataset for enhancing long video understanding.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, we propose an effective method for generating key frames of movie-level videos. Our generation process consists of three main steps: movie plot generation, style immobilization process, and video instruction data generation. The proposed data generation workflow significantly eases the challenge of producing movie-level video data for models, enhancing both the control and diversity of the generated content. Experiments conducted on existing datasets, as well as on our newly generated dataset, validate the effectiveness of our approach. Thanks to the richness of our dataset, models demonstrate capabilities previously unattainable with training on earlier datasets. We believe our data generation pipeline will make a significant contribution to the advancement of multimodal models\' understanding of long videos. Furthermore, it paves the way for the adoption of similar data generation processes across other domains.\n' +
      '\n' +
      'Figure 6: **Video understanding results of baseline model and ours. Our model demonstrates a more reasonable and detailed answer in the video understanding.**\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      'The forgetting issue associated with large language models may lead to inconsistencies in the frame descriptions generated during the first stage of our pipeline, subsequently causing discontinuities in the video scenes. To address this problem, we plan to refine the text generation component within our pipeline.\n' +
      '\n' +
      '## 8 Ethics Statement\n' +
      '\n' +
      'This Ethics Statement reflects on the broader impacts and ethical considerations associated with the development of our MovieLLM framework. Our framework, designed for synthesizing synthetic, high-quality data for long videos, engages deeply with ethical concerns, particularly in the realms of privacy, security, accessibility, the impact on employment, sustainability, and the potential for misuse.\n' +
      '\n' +
      '**Privacy and Security**: MovieLLM\'s methodology emphasizes the protection of privacy and security by generating synthetic video content, thereby eliminating the risk of exploiting real individuals\' data. This strategy significantly minimizes the potential for unauthorized data access and breaches, with a commitment to ongoing improvement of these protective measures.\n' +
      '\n' +
      '**Accessibility and Inclusivity**: Our framework aims to enhance the accessibility of video understanding technologies, making it easier for researchers across different demographics, including those with disabilities, to engage in AI development. We are dedicated to ensuring that MovieLLM supports a wide array of users by enhancing its inclusivity and accessibility.\n' +
      '\n' +
      '**Impact on Employment**: While the automation inherent in MovieLLM may prompt concerns regarding job displacement, our intention is to augment human capacity rather than replace it. By automating the generation of video data, we hope to shift human focus to more creative and intellectually stimulating tasks, potentially enriching professional satisfaction and productivity.\n' +
      '\n' +
      '**Sustainability**: Acknowledging the environmental costs associated with training complex AI models, MovieLLM is designed to be resource-efficient, thereby reducing the need for extensive computational power and minimizing its ecological footprint as part of our commitment to environmental responsibility.\n' +
      '\n' +
      '**Potential Misuse**: We recognize the potential for MovieLLM\'s synthetic data to be misused, for example, in creating deceptive content. To address this, we advocate for responsible use, which includes:\n' +
      '\n' +
      '* Monitoring and Detection: Developing mechanisms to identify and prevent misuse of synthetic data.\n' +
      '* Ethical Guidelines and Governance: Promoting ethical standards in AI research and application to ensure awareness and compliance.\n' +
      '* Collaboration with Platforms: Engaging with digital platforms to align the use of synthetic data with ethical and legal standards.\n' +
      '* Transparency and Accountability: Maintaining clear communication about the capabilities and intended use of MovieLLM to ensure informed and ethical application.\n' +
      '\n' +
      'Through this statement, we emphasize our commitment to ethical research and innovation, highlighting the importance of addressing these critical considerations in advancing AI technology responsibly.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bai et al. (2024) Ziyi Bai, Ruiping Wang, and Xilin Chen. 2024. Glance and focus: Memory prompting for multi-event video question answering. _arXiv preprint arXiv:2401.01529_.\n' +
      '* Heilbron et al. (2015) Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. 2015. Activitynet: A large-scale video benchmark for human activity understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 961-970.\n' +
      '* Castro et al. (2022) Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan Stroud, and Rada Mihalcea. 2022. Fiber: Fill-in-the-blanks as a challenging video understanding evaluation framework. In _Association for Computational Linguistics_, pages 2925-2940.\n' +
      '* Chen and Dolan (2011) David Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In _Association for Computational Linguistics_, pages 190-200.\n' +
      '* Feichtenhofer et al. (2019) Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slowfast networks for video recognition.\n' +
      '\n' +
      'Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion.\n' +
      '* Gao et al. (2021) Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. 2021. Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1675-1685.\n' +
      '* Han et al. (2023) Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartlama: A multimodal llm for chart understanding and generation. _arXiv preprint arXiv:2311.16483_.\n' +
      '* Huang et al. (2020) Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. 2020. Movienet: A holistic dataset for movie understanding. In _European Conference on Computer Vision_, pages 709-727. Springer.\n' +
      '* Jin et al. (2024) Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. 2024. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization. _arXiv preprint arXiv:2402.03161_.\n' +
      '* Kumari et al. (2023) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941.\n' +
      '* Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_.\n' +
      '* Li et al. (2023b) KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023b. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_.\n' +
      '* Li et al. (2023c) Yanwei Li, Chengyao Wang, and Jiaya Jia. 2023c. Llama-vid: An image is worth 2 tokens in large language models. _arXiv preprint arXiv:2311.17043_.\n' +
      '* Lin et al. (2023) Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-lava: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_.\n' +
      '* Liu et al. (2023b) Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. 2023b. Cones 2: Customizable image synthesis with multiple subjects. _arXiv preprint arXiv:2305.19327_.\n' +
      '* Ma et al. (2023) Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. 2023. Vista-llama: Reliable video narrator via equal distance to visual tokens. _arXiv preprint arXiv:2312.08870_.\n' +
      '* Maaz et al. (2023) Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_.\n' +
      '* Mittal et al. (2012) Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012. No-reference image quality assessment in the spatial domain. _IEEE Transactions on image processing_, 21(12):4695-4708.\n' +
      '* OpenAI (2023) OpenAI. 2023. Gpt-4 technical report.\n' +
      '* Tang et al. (2023) Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. 2023. Video understanding with large language models: A survey. _arXiv preprint arXiv:2312.17432_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Xu et al. (2016) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description dataset for bridging video and language. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5288-5296.\n' +
      '* Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Apparent: Multimodal agents as smartphone users. _arXiv preprint arXiv:2312.13771_.\n' +
      '* Zhang et al. (2023) Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_.\n' +
      '* Zhong et al. (2022) Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, and Tat-Seng Chua. 2022. Video question answering: Datasets, algorithms and challenges. In _Empirical Methods in Natural Language Processing_, pages 6439-6455.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'Figure 7: **Video understanding results of baseline model and ours.** Our model demonstrates a more reasonable and detailed answer in the video understanding.\n' +
      '\n' +
      'Figure 8: **Video understanding results of baseline model and ours.** Our model demonstrates a more reasonable and detailed answer in the video understanding.\n' +
      '\n' +
      'Figure 9: **Video understanding results of baseline model and ours.** Our model demonstrates a more reasonable and detailed answer in the video understanding.\n' +
      '\n' +
      '# Chatgpt Prompt for Long Video Evaluation\n' +
      '\n' +
      '"You are an intelligent chatbot designed for comparing two video-based question-answer pairs to decide which one is better and score them(use score from 0 to 5, float is allowed) based on the reference answer.\\(\\backslash\\)n"\n' +
      '\n' +
      '"##Instructions:n"\n' +
      '\n' +
      '"1. consider the detailed information involved in the answer (more detailed, better)\\(\\backslash\\)n"\n' +
      '\n' +
      '"2. consider the character relationship in the answer\\(\\backslash\\)n"\n' +
      '\n' +
      '"3. consider the conclusion or ending involved in the answer\\(\\backslash\\)n"\n' +
      '\n' +
      '"4. nonsense like repeated sentences are not allowed, should be considered as a very bad answer\\(\\backslash\\)n"\n' +
      '\n' +
      '"5. if mark number is used in the answer, the order of the number should be right\\(\\backslash\\)n"\n' +
      '\n' +
      '"Note that your answer should be like this: {better one\':\'first one\',\n' +
      '\n' +
      '\'score\':\'{first one\':3.5,\'second one\':\'1}}, only choose between first and second answer,DO NOT PROVIDE ANY OTHER TEXT OR EXPLANATION, only provide the python dictionary string like above.\\(\\backslash\\)n"\n' +
      '\n' +
      'f"Question: {question}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"Reference answer: {gt_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"First answer: {our_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"Second answer: {llama_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      '"Now, give me your answer."\n' +
      '\n' +
      'Figure 10: **Prompt for Long Video Evaluation. Our prompt for long video evaluation assesses various capabilities of the model by comparing different predicted answers from different models based on the ground truth and scoring the predicted answers on a scale of 1-5.**\n' +
      'You are a specialist in creating stories. Generate a story which theme is "themel". Requirements: First, generate the title, overview and "story tone and the style" of the story. I will use the story to generate images, so "Story tone and style" should describe the artistic style of the pictures, and should use only adjectives and short phrases, not sentences, with the most important phrases included in "("(")"), and a single one-word style-keyword included in "(")"(Note that you must provide the style-keyword like the example). Second, list all the locations that the story will take place in, at least 12 locations. Only common places are allowed and use at most 2 words for the location. Do not mention character names in the locations. Do not use location like "Mary\'s" or "Mary\'s house" to indicate someone\'s home. Third, generate all the characters\' name, and use the name of a Western celebrity to describe them, with clothes color, in the format of "cox: a man/woman looks like oxo in xoxy". For clothes color, a simple one-word color is enough. If a character appears in the story for at least twice, put it in "Character" part. A main charater appears most frequently, and a supporting character appears in part of the story. Except for the characters listed in "Character" part, all the other characters and objects that will appear in the story should only appear once. For the new story, I need 1 or 2 main characters, and 0 or 1 supporting charater. Fourth, generate 11 substroises based on the overview. "Scene" part should pick 1,2 or 3 locations in the "Locations" part, and the whole subststory will only take place in these locations. "QA" part is a question-answer pair based on each substory. You can only ask one of the four types of questions"what, "why", "how" and "where" after each substory. The answer should be one or two sentences, no need to be too long. Below is an example:\n' +
      '\n' +
      '* Story Title: The Symphony of Love\n' +
      '* Overview: The Symphony of Love\' is a heartwarming tale set in the bustling city of New York, painted in a vibrant, contemporary style. The story revolves around Amelia, a talented violinist with a passion for music, and Ethan, a successful businessman with a hidden love for the arts. Their lives intersect as they navigate the complexities of love, ambition, and the pursuit of dreams.\n' +
      '* Story tone and style: (Romantic), ((Urban chic)), ((modern elegance, bohemian charm)), pastel palette, minimalist aesthetic, soft lighting, candid moments, architectural beauty, cityscape, dynamic, emotional, intimate, dreamy, nostalgic\n' +
      '* Style-keyword: Romantic\n' +
      '* Locations: [\'Concert Hall\', \'Office\', \'Cafe\', \'Art Gallery\', \'Park\', \'Restaurant\', \'Apartment\', \'Subway\', \'Rooftop\', \'Streef\', \'Lake\', \'Classroom\', \'Cinema\']\n' +
      '* Character:\n' +
      '* Main characters:\n' +
      '* [\'Amelia: a woman looks like Anne Hathaway in red>\n' +
      '* [\'Ethan: a man looks like Chris Hemsworth in blue>\n' +
      '* Supporting character:\n' +
      '* [\'Oliver: a man looks like Robert Downey Jr. in brown>\n' +
      '* Substories:\n' +
      '* Substory1: The Melody of Chance\n' +
      '* Scene: Subway, Street\n' +
      '* Content: Amelia and Ethan have a chance encounter in the subway, and walk on the street.\n' +
      '* QA: Question: Where did Amelia and Ethan meet each other? Answer: In the subway\n' +
      '* Substory2: The Hammony of Friendship\n' +
      '* Scene: Cafe\n' +
      '* Content: Amelia and Ethan start to build a friendship over shared coffees and conversations at their favorite cafe.\n' +
      '* QA: Question: How did Amelia and Ethan know each other? Answer: They meet each other in cafe, drinking coffee.\n' +
      '\n' +
      'Figure 12: **Examples of generated long video instruction data** We use GPT-4 and guided text-to-image generation models to generate consistent key frames of move-level video with reasonable lines and corresponding question-answer pairs. These data are used to train multimodal large language models on video understanding.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>