<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#영화LLM: 인공지능을 활용한 긴 동영상 이해력 향상\n' +
      '\n' +
      'Chhenchenen Wang\\({}^{1}\\)1  Jiamu Sheng\\({}^{1}\\)1  Chi Zhang\\({}^{2}\\)2\n' +
      '\n' +
      '강유({}^{2}\\) 지아위안 판({}^{1}\\)3 도천({}^{1}\\)\n' +
      '\n' +
      'Fudan University \\({}^{1}\\) Tencent PCG\n' +
      '\n' +
      'zdsong23@m.fudan.edu.cn(20307130244, jmsheng18, jyfan, eetchen)@fudan.edu.cn\n' +
      '\n' +
      '{johnczhang, skicyyu}@tencent.com\n' +
      '\n' +
      '[https://deaddawn.github.io/MovieLLM/](https://deaddawn.github.io/MovieLLM/)\n' +
      '\n' +
      '각주 1: 처음 세 명의 저자는 이 작업에 동등하게 기여했다.\n' +
      '\n' +
      '각주 2: 프로젝트 리더.\n' +
      '\n' +
      '각주 3: 교신 작성자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '멀티모달 모델의 개발은 기계가 비디오를 이해하는 방법에 있어 중요한 진전을 이루었다. 이 모델들은 짧은 비디오 클립을 분석하는 데 있어 가능성을 보여주었다. 그러나 영화와 같은 더 긴 포맷에 관해서는 종종 부족합니다. 주요 장애물은 고품질, 다양한 비디오 데이터의 부족과 그러한 데이터를 수집하거나 주석을 달기 위해 필요한 집중 작업이다. 이러한 문제에 직면하여 우리는 긴 비디오를 위한 합성 고품질 데이터를 생성하도록 설계된 새로운 프레임워크인 MovieLLM을 제안한다. 이 프레임워크는 GPT-4 및 텍스트 대 이미지 모델의 힘을 활용하여 세부 스크립트 및 해당 비주얼을 생성한다. 우리의 접근법은 유연성과 확장성이 두드러져 전통적인 데이터 수집 방법보다 우수한 대안이 된다. 우리의 광범위한 실험은 MovieLLM이 생성한 데이터가 희소성과 편향에 관한 기존 데이터 세트의 한계를 극복하면서 복잡한 비디오 내러티브를 이해하는 데 있어 멀티모달 모델의 성능을 크게 향상시킨다는 것을 입증한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 인공지능 분야는 GPT-4(OpenAI, 2023), LLaMA(Touvron et al., 2023)와 같은 대형 언어 모델(LLM)의 개발에서 상당한 진전을 목격하고 있다. 이러한 모델은 일관되고 맥락적으로 관련된 텍스트를 생성하는 데 탁월한 능력을 보여주었다. LLMs의 능력을 멀티모달 영역으로 더 확장하고, LLaVA(Liu et al., 2023)와 같은 비전 언어 모델(VLMs)은 이미지 캡션 및 시각적 질문 응답(QA)과 같은 시각적 및 언어적 정보 모두에 대한 이해를 필요로 하는 작업에서 현저한 숙련도를 입증했다. VLM의 진화는 이제 VideoLLaMA(Zhang et al., 2023) 및 VideoChat(Li\n' +
      '\n' +
      '도 1: **생성된 긴 비디오 명령 데이터의 예.** GPT-4 및 안내된 텍스트-이미지 생성 모델을 사용하여 합리적인 라인 및 대응하는 질문-응답 쌍을 갖는 무브-레벨 비디오의 일관된 키 프레임을 생성한다. 이러한 데이터는 비디오 이해에 대한 멀티모달 대형 언어 모델을 훈련하는 데 사용된다.\n' +
      '\n' +
      'et al., 2023) 비디오 콘텐츠의 처리 및 해석에서 VLM의 잠재력을 강조한다.\n' +
      '\n' +
      '그러나 이러한 모델이 전장 영화와 같은 장기간 비디오를 이해하는 능력에는 눈에 띄는 격차가 남아 있다. 이러한 한계는 주로 이러한 모델을 조정하는 데 필요한 광범위한 긴 비디오 데이터 세트가 없기 때문이다. LLaMA-VID(Li et al., 2023) 이니셔티브는 MovieNet(Huang et al., 2020)에서 파생된 긴 비디오 튜닝 데이터 세트를 생성함으로써 이 방향으로 한 걸음 나아가는 것을 나타내며, 이는 멀티모달 긴 비디오 데이터 세트 상의 미세 조정 LLM이 그들의 영화 수준 비디오 이해도를 향상시킬 수 있음을 나타낸다. 그럼에도 불구하고, 기존의 데이터 세트에서 이러한 벤치마크를 구성하는 것은 비용 집약적이며 특히 영화 대화와 같은 추가 정보를 수집하거나 주석을 달기 위해 필요한 수동 노력 때문에 어렵다. 더욱이, 가능성을 보여주었음에도 불구하고, MovieNet에 구축된 QA 데이터 세트는 몇 가지 장애물에 직면한다. 여기에는 수동 데이터 수집의 힘든 특성과 콘텐츠의 제한된 다양성이 포함되며, 이는 고유한 데이터 세트 편향으로 인해 모델의 일반화 기능을 방해할 수 있다.\n' +
      '\n' +
      '이러한 문제에 착안하여, 본 논문에서는 긴 비디오 명령어 튜닝을 위한 포괄적인 데이터 세트를 생성하기 위한 새롭고 유연한 접근 방식을 소개한다. 우리의 방법론은 GPT-4의 언어적 능력과 안정적인 확산 모델의 생성력을 활용하여 데이터 세트의 다양성과 풍부성 문제에 대한 고유한 솔루션을 제공한다. 우리의 파이프라인은 세 가지 주요 단계로 구성된다.\n' +
      '\n' +
      '1) **영화 플롯 생성.** 웹 또는 기존 데이터 세트와 같은 기존 데이터 소스로 플롯 생성을 제한하기보다는 GPT-4의 힘을 활용하여 합성 데이터를 생성한다. 테마, 개요, 스타일 등 특정 요소를 제공하여 GPT-4가 후자 생성 프로세스에 맞춘 영화 수준의 키 프레임 설명을 제작할 수 있도록 안내합니다.\n' +
      '\n' +
      '2) **Style Immobilization Process.** Textual inversion(Gal et al., 2022)을 능숙하게 사용함으로써, 우리는 스크립트에서 생성된 스타일 디스크립션을 확산 모델의 잠재 공간에 고정시킨다. 이 접근법은 모델이 통일된 미학 아래 다양성을 유지하면서 고정된 스타일로 장면을 생성하도록 안내한다.\n' +
      '\n' +
      '3) **비디오 명령어 데이터 생성.** GPT-4의 강력한 생성 기능을 개발된 스타일 유도 확산 모델과 통합하여 스타일 일관성 키 프레임과 해당 QA 쌍을 생성하여 시각적 데이터와 QA 쌍을 결합하여 포괄적인 명령어 튜닝 코퍼스를 생성한다.\n' +
      '\n' +
      '우리의 접근법은 긴 비디오에 대한 현재 데이터 세트의 한계를 해결할 뿐만 아니라 더 높은 프레임 속도로 비디오 데이터를 생성하기 위해 고급 확산 모델을 통합하는 길을 열어준다. 또한, 제안된 방법론을 통해 데이터 볼륨에 대한 제약 없이 데이터 세트를 생성할 수 있어 생성된 콘텐츠 내에서 높은 수준의 다양성을 보장한다. 또한 자동 주석을 용이하게 하여 수작업 및 관련 비용의 필요성을 크게 줄입니다. 이러한 장점은 긴 비디오 이해를 위한 데이터 세트 생성의 확장성, 풍부함 및 효율성을 향상시켜 해당 분야에서 상당한 도약을 나타낸다. 생성된 긴 비디오 명령 데이터의 예는 그림 1에 나와 있다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* GPT-4와 확산 모델을 결합하여 영화 수준의 비디오 명령어 튜닝 데이터셋을 생성하는 새로운 파이프라인을 개발한다.\n' +
      '* 생성적 접근 방식을 활용하여, 우리는 긴 비디오에 대한 향상된 이해를 위해 훈련된 정교한 모델과 함께 영화 수준의 비디오 이해를 위한 포괄적인 데이터 세트를 개발하고 공개적으로 공개할 것이다.\n' +
      '* 실제 영화 데이터셋을 기반으로 긴 동영상 이해 능력 평가를 위한 벤치마크를 제안한다. 벤치마크에 대한 실험은 제안된 방법의 유효성을 보여주며, 기준선을 상당히 능가한다.\n' +
      '\n' +
      '## 2 Preliminary\n' +
      '\n' +
      '코히어런트 장면을 갖는 키 프레임을 생성하기 위해 텍스트 역산 기법(Gal et al., 2022)을 사용하였다. 이 예비 섹션은 파이프라인의 자세한 설명을 위한 토대를 마련하기 위해 텍스트 역전에 대한 개요를 제공한다.\n' +
      '\n' +
      '텍스트 역산 이 기법은 텍스트-투-이미지(Text-to-Image, T2I) 모델에서 새로운 사용자 지정 개념의 언어 유도 이미지 생성을 가능하게 하는 것을 목표로 한다. 이러한 목적을 달성하기 위해 두 가지 주요 단계가 필요하다: 1) 텍스트 삽입, 이 프로세스는 먼저 자리 표시자 단어를 포함하는 문자열을 토큰으로 변환합니다. 그런 다음, 이러한 토큰들은 연속적인 벡터 표현으로 변환된다. 2) 반전 과정, 임베딩은 생성 모델을 안내하는 단일 컨디셔닝 코드로 변환된다. 소위 의사 단어와 연관된 임베딩 벡터는 재구성 목적을 사용하여 최적화될 것이다. 이와 같이, 임베딩은 컨셉 특유의 시각적 세부사항들을 캡처하도록 동기부여된다. 그런 다음 학습된 임베딩은 T2I 모델이 맞춤형 이미지를 생성하도록 안내하는 조건으로 사용된다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 절에서는 영화 수준 비디오 명령어 튜닝 데이터를 생성하기 위한 우리의 새로운 접근 방식을 설명한다. 이 방법은 영화 플롯 생성, 스타일 고정화 과정, 비디오 명령 데이터 생성의 세 가지 상호 연관된 단계를 절충한다. 전체 프로세스는 그림 2에 나와 있다. 이러한 단계의 세부 사항은 다음 하위 섹션에 나와 있다.\n' +
      '\n' +
      '### 영화 플롯 생성\n' +
      '\n' +
      '이 단계의 주요 목적은 GPT-4의 강력한 기능을 활용하여 다양하고 설득력 있는 영화 플롯을 생성하는 것이다. 각 영화 줄거리는 개요, 문자 및 프레임 설명을 포함한 기본 요소로 구성됩니다. 이것은 후속 스타일 고정화 및 생성 프로세스를 위한 고품질 영화 플롯 텍스트를 구성하는 것을 목표로 한다. 이를 위해 세 가지 주요 전략을 제안한다.\n' +
      '\n' +
      '1) 먼저 GPT-4를 사용하여 그럴듯한 영화 스크립트 집합을 생성한다. 그런 다음 이러한 스크립트를 기반으로 재사용 가능한 프롬프트를 구성하여 다양한 영화 플롯을 대량으로 생성합니다. 이 과정에서 "비극적인 영화"와 같은 영화 주제를 설명하는 문구는 특정 주제를 기반으로 영화 플롯을 만드는 데 GPT-4를 지시할 수 있는 변수로 작용한다.\n' +
      '\n' +
      '2) 영화 플롯 텍스트 내에서 캐릭터와 스타일의 두 가지 핵심 요소를 통합한다. GPT-4에 의해 생성된 캐릭터와 스타일 묘사를 활용하여, 우리는 비디오의 주요 프레임에 걸쳐 캐릭터의 묘사와 스타일 속성 모두에서 큰 일관성을 보장하는 것을 목표로 한다.\n' +
      '\n' +
      '3) LLM은 연속 생성 프로세스 동안 플롯 세부사항을 잊기 쉽기 때문에 영화 플롯에서 상당한 수의 키 프레임이 문제를 제기한다. 따라서 본 논문에서는 LLM에 내재된 망각 문제를 최대한 완화하고 연속적인 핵심 프레임 묘사를 얻기 위한 스토리 확장 전략을 제안한다. 우리는 설명을 나눈다.\n' +
      '\n' +
      '그림 2: ** 긴 비디오 명령 튜닝 데이터 세트를 생성하기 위해 제안된 파이프라인.** 간단한 주제 설명만으로, 우리의 파이프라인은 전체 필름의 키 프레임을 생성할 수 있다. 파이프라인은 크게 (a) 영화 플롯 생성 단계로 나눌 수 있는데, GPT-4를 이용하여 주제문구를 기반으로 전체 영화 플롯을 생성한다. (b) 스타일 고정화 과정은 텍스트 역산 기법을 활용하여 플롯에서 생성된 스타일 묘사를 확산 모델의 잠재 공간으로 고정화하여 고정된 스타일로 프레임을 생성할 수 있도록 안내한다. (c) 비디오 명령 데이터 생성, 여기서 우리는 이전에 획득된 모든 정보를 통합하여 궁극적으로 일관된 키 프레임 및 대응하는 질문-응답 쌍을 생성한다.\n' +
      '\n' +
      '세 가지 수준의 생성 프로세스입니다.\n' +
      '\n' +
      '우리는 이제 고품질 영화 줄거리 텍스트를 얻기 위한 프롬프트를 구성하는 방법을 설명한다. 자세한 내용은 부록 B.1에서 확인할 수 있습니다. 우리의 프롬프트는 다음과 같은 요소로 구성되어 있습니다.\n' +
      '\n' +
      '**영화 주제**: 위에서 언급한 바와 같이, 우리는 영화 주제를 설명하는 데 사용되는 문구를 변수로 취급한다. 따라서, 먼저 다양한 영화 테마를 생성하여 영화 데이터의 다양성을 보장한다.\n' +
      '\n' +
      '**개요**: 영화 플롯의 후속 생성을 위한 참조 가능한 기초를 제공하기 위해, 우리는 처음에 영화 주제에 기초하여 개요를 생성한다. 이것은 또한 전체 영화 플롯의 일관성과 요약형 QA의 생성을 촉진할 것이다.\n' +
      '\n' +
      '**스타일**: 또 다른 중요한 요소는 스타일 반전 과정의 토대가 되는 영화 스타일이다. 적절한 주제별 스타일 설명은 영화 테마와 관련된 장면 이미지를 생성하는 데 도움이 될 수 있다. 텍스트 역산을 통해, 이 프로세스는 스타일 임베딩의 트레이닝을 용이하게 한다. 이는 결국 영화 전반에 걸쳐 모든 장면의 일관성에 기여한다.\n' +
      '\n' +
      '**캐릭터**: 영화 수준의 비디오 명령 데이터 생성에서, 캐릭터 묘사의 일관성을 보장하는 것이 중요하다. 따라서 영화 줄거리 생성 과정에서 영화 줄거리에 적합한 캐릭터를 선택하고 캐릭터 묘사를 생성하는 데 있어 감독 역할을 하기 위해서는 GPT-4가 필요하다. 그 후, 생성된 키 프레임 초상화에서 우수한 캐릭터 일관성을 유지하기 위해 사전 훈련된 안정적인 확산으로부터 유명인 얼굴에 대한 강력한 사전 지식을 활용하도록 설계한다.\n' +
      '\n' +
      '**스토리 확장 키 프레임 기술**: 키 프레임의 양과 품질을 보장하기 위해 앞서 언급한 스토리 확장 전략을 활용하여 영화 줄거리를 에포크 챕터, 내러티브 스레드, 프레임 기술의 세 단계로 나눌 것을 제안한다. 에포치 챕터 내의 각 챕터는 스토리 전개의 중요한 시기 또는 단계를 나타낸다. 내러티브 스레드는 각 주요 장을 보다 구체적인 내러티브 가닥과 하위 스토리로 더욱 세분화하여 캐릭터 전개와 주요 사건의 일관성을 보여준다. 그런 다음 프레임 설명은 중요한 시각적 프레임을 묘사하는 이러한 서사 가닥에 대해 자세히 설명한다. 그런 다음 이전 수준에서 스토리를 반복적으로 확장하여 3단계 영화 줄거리 텍스트를 생성하여 궁극적으로 그림 3과 같이 주요 프레임 설명을 산출한다.\n' +
      '\n' +
      '### 스타일 고정화 프로세스\n' +
      '\n' +
      '다음 단계는 생성된 영화 플롯에서 스타일 설명을 일관된 스타일로 장면을 생성하도록 T2I 모델을 유도할 수 있는 정보로 변환하도록 설계된다. 이 목적을 달성하기 위해, 우리는 이전에 소개된 기술인 텍스트 역산을 활용한다. 구체적인 단계는 다음과 같다. 첫째, 안정확산을 이용하여 특정 스타일 묘사를 기반으로 임의의 스타일 장면을 생성한다. 2) 이어서, 이러한 장면 이미지들을 변환하기 위해 텍스트 역산을 사용한다.\n' +
      '\n' +
      '그림 3: **스토리 확장 전략의 묘사.** 영화 수준 비디오의 프레임 설명을 얻고 망각 문제를 최대한 완화하기 위해 3단계 스토리 확장 전략을 채택한다. 전략은 (a) 에포크 챕터 생성의 세 단계로 나눌 수 있는데, 여기서 각 챕터는 스토리 전개에서 중요한 시기를 나타낸다. (b) 각 스레드는 각 주요 챕터를 보다 구체적인 서사 하위 스토리로 정제하는 서사 스레드 생성. (c) 프레임 기술 생성, 여기서 각 프레임 기술은 중요한 시각적 프레임을 묘사하는 이러한 서사 하위 스토리를 자세히 설명한다.\n' +
      '\n' +
      '최적화 단계를 통해 내장됩니다. 이러한 임베딩은 특정 스타일을 가진 이미지를 생성하는 데 있어 확산 모델을 유도할 수 있다. 우리는 이 전체 과정을 GPT-4에 의해 생성된 특정 영화 스타일을 특정 토큰에 해당하는 임베딩으로서 잠재 공간에 본질적으로 고정시키는 스타일 고정화 과정이라고 한다. 따라서, 연속적인 키 프레임 이미지들의 생성에서, 특정 토큰을 단독으로 사용하는 것은 특정 스타일로 이미지들을 생성하기 위해 확산 모델을 트리거링하여, 프레임들에 걸친 장면들의 일관성을 보장한다.\n' +
      '\n' +
      '### 비디오 명령 데이터 생성\n' +
      '\n' +
      '이전 두 단계를 완료한 후 고정 스타일 임베딩, 유명인 캐릭터 및 키 프레임 설명을 얻었다. 이를 바탕으로 스타일 임베딩을 활용하여 키 프레임 설명에 따른 키 프레임 생성 시 안정적인 확산을 유도한다. 또한, 영화 플롯을 기반으로 다양한 명령어 QA 쌍을 점진적으로 생성한다. 여기 세부 사항이 있습니다.\n' +
      '\n' +
      'Feichtenhofer et al. (2019); Li et al. (2023); Zhang et al. (2023); Maaz et al. (2023)에서 관찰된 바와 같이, 다소 낮은 프레임 레이트에서의 키 프레임들은 이미 비디오 콘텐츠를 이해하기 위한 모델들의 요구들을 충족시킬 수 있었으므로, 본 방법은 대응하는 설명들에 기초하여 키 프레임들을 생성한다. 캐릭터와 장면 모두에서 일관된 키 프레임을 생성하기 위해, 우리는 처음에 프레임 기술에서 캐릭터 이름을 대응하는 유명인(GPT-4에 의해 선택됨)으로 대체한다. 이후, 앞서 언급한 바와 같이, 특수 토큰에 링크된 스타일 고정 임베딩이 활용된다. 이러한 스타일 임베딩은 고정된 스타일로 장면을 생성하는데 있어서 안정적인 확산 모델을 안내하는 조건으로 작용할 수 있다. 이 과정은 특수 토큰에 의해 촉발되는데, 본 논문에서는 "**고딕**" Hence와 같은 특정 스타일의 이름을 나타내며, "*고딕** 스타일의 이미지를 생성"과 같은 문장을 프레임 설명 초기에 통합함으로써 유명인사의 캐릭터 이름과 결합하여 시각적 다양성을 유지하면서 일관된 키 프레임을 생성할 수 있다.\n' +
      '\n' +
      'QA 쌍 생성 스토리 자체에 내재된 텍스트 및 시각적 정보를 넘어 풍부한 QA 쌍은 멀티모달 정보에 의해 모델의 학습을 잘 지시하여 영화 수준의 내러티브에 대한 우수한 이해를 얻을 수 있다. 우리는 GPT-4를 활용하여 개요, 무엇을, 어디서, 왜의 측면을 포함하는 영화 플롯의 모든 내용을 기반으로 풍부한 QA 쌍을 점진적으로 생성한다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '이 섹션에서는 영화 수준 비디오 명령어 튜닝에 맞춘 데이터 생성을 위한 혁신적인 파이프라인의 효과를 입증하기 위해 수행된 실험을 설명한다. 먼저 구현 세부 사항과 데이터 세트 통계에 이어 평가 메트릭을 소개하고 마지막으로 주요 결과와 정성적 결과를 입증할 것이다.\n' +
      '\n' +
      '### 구현 세부사항 및 데이터세트 통계\n' +
      '\n' +
      '구현 세부사항.우리는 LLaMA-VID에서 비디오 관련 실험을 수행한다. 모델의 훈련 절차는 모달리티 정렬, 명령어 튜닝, 긴 비디오 튜닝의 세 단계로 나뉜다. 처음 두 단계에서 수행된 훈련은 이미지와 짧은 비디오를 모두 이해할 수 있는 능력을 모델에 부여했다. 처음에는 원시 긴 비디오 데이터 세트를 사용하여 3단계 동안 LLaMA-VID의 미세 조정을 수행했다. 우리는 이 훈련된 LLaMA-VID를 기본 모델로 사용합니다. 그 후, 비교를 위해 데이터 세트의 모델을 미세 조정한다. 자세한 내용은 부록에서 참조할 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline Dataset & Video QA & QA per Image & Video Type & QA per Video \\\\ \\hline LLaMA-VID & 9\\% & 0.0054 & 11 & 21 \\\\ \\hline Ours & **13k** & **0.0047** & **15** & **125** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 본 논문에서 제안하는 영화 수준 비디오 명령어 데이터셋과 LLaMA-VID의 데이터셋을 데이터 측면에서 비교한 것이다.\n' +
      '\n' +
      '도 4: 영화-레벨 비디오 명령어 튜닝 데이터의 영화 유형 분포.\n' +
      '\n' +
      '데이터세트 통계.표에서 생성된 데이터세트의 통계를 보여줍니다. 도 1 및 도 4. 우리의 명령어 튜닝 데이터는 비디오 콘텐츠에 대한 심층 정보를 추출하는 풍부한 QA 쌍을 포함한다. 15가지 장르의 영화 수준 비디오를 지원하여 일반화 능력을 크게 향상시킵니다. 이는 주로 데이터 구성 방법론의 강력한 유연성에 기인한다. 특히, 우리의 접근 방식은 모든 테마와 길이의 비디오를 생성할 수 있으며 향후 데이터의 풍부함을 계속 확장할 계획입니다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '우리는 주로 생성된 비디오 키 프레임의 품질과 비디오 이해의 두 가지 주요 측면에서 접근 방식을 평가한다.\n' +
      '\n' +
      '비디오 키 프레임의 품질은 생성된 결과를 평가하기 위해 이 두 가지 메트릭을 고려한다: 1) CLIP 이미지 공간에서 키 프레임의 인접 임베딩 간의 평균 코사인 유사성에 의해 계산되는 **키 프레임 일관성**. 2) 각 키 프레임 디스크립션과 키 프레임 자체간의 평균 코사인 유사도에 의해 계산되는 **텍스트-이미지 정렬**. 또한, 영상 품질을 평가하기 위해 비기준 화질 평가 메트릭인 BRISQUE score Mittal et al.(2012)을 사용한다.\n' +
      '\n' +
      '비디오 이해도를 위해 우리는 짧은 비디오와 긴 비디오에 대해 별도로 평가를 수행한다. 기존 작품에서 영화 수준의 비디오 이해에 특별히 맞춘 벤치마크가 없다는 점을 감안할 때, 우리는 짧은 비디오 이해에 대한 메트릭 설계에서 영감을 얻고 긴 비디오 이해 벤치마크를 세심하게 고안한다.\n' +
      '\n' +
      '간단한 영상 이해를 위해 두 가지 벤치마크(a)를 대상으로 모델 성능을 평가하기 위해 5가지 측면을 포함하는 영상 기반 생성 성능 벤치마킹(Genative Performance Benchmarking)을 사용한다. b) MSVD-QA Chen and Dolan (2011) 및 MSRVTT-QA Xu et al. (2016)을 포함하여 일반적으로 사용되는 여러 개방형 QA 데이터세트를 이용한 종합적인 정량적 평가인 Zero-shot Question-Answer Evaluation.\n' +
      '\n' +
      '긴 비디오 이해를 위해 이 작업에 맞춘 벤치마크를 제시합니다. 우리의 테스트 데이터 세트는 MovieNet 데이터베이스 내의 약 100개의 영화에서 파생되며, 관련 질문-답변 쌍은 GPT-4와 인간의 노력 모두에서 생성된다. Maaz et al. (2023)에 의해 영감을 받아, GPT-4를 사용하여 평가 파이프라인을 재설계한다. 평가 프롬프트는 우리의 부록 B.2에서 찾을 수 있다. 이 파이프라인은 지상 진리에 기초하여 상이한 모델들로부터 상이한 예측된 답변들을 비교하고 다음의 세 가지 측면들에서 1-5의 척도로 예측된 답변들을 스코어링함으로써 모델의 다양한 능력들을 평가한다:\n' +
      '\n' +
      '1) 개요 이해: 우리는 비디오 콘텐츠를 요약하는 질문을 통해 글로벌 비디오 정보에 대한 모델의 이해도를 평가한다.\n' +
      '\n' +
      '2) 플롯 이해: 플롯에 대한 모델의 추론 능력, 세부 사항에 대한 이해, 플롯 관련 질문 세트를 통해 캐릭터를 인식하는 능력을 평가한다.\n' +
      '\n' +
      '3) 시간적 이해: 비디오에서 발생하는 사건들을 시간적 순서에 따라 배열하도록 하여 시간적 관점에서 비디오 콘텐츠에 대한 모델의 이해를 평가한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '먼저, 본 논문에서 제안한 파이프라인을 이용하여 생성된 키 프레임들의 품질을 기존의 유사한 방법과 비교한다. 그런 다음 짧은 비디오 이해와 긴 비디오 이해에 대한 성능을 평가한다.\n' +
      '\n' +
      '키 프레임의 품질은 고정화된 스타일 임베딩에 기반한 방법과 기존의 두 가지 유사한 방법을 비교한다: 1) 대표적인 다중 개념 커스터마이징 방법인 Custom diffusion Kumari et al. (2023) 2) Cones-V2 Liu et al.(2017),\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline Metric & LLaMA-VID & +Ours \\\\ \\hline Correctness \\(\\uparrow\\) & 1.94 & **2.154** (+11\\%) \\\\ Detail \\(\\uparrow\\) & 2.431 & **2.549** (+5\\%) \\\\ Context \\(\\uparrow\\) & 2.701 & **2.880** (+7\\%) \\\\ Temporal \\(\\uparrow\\) & 1.585 & **1.832** (+16\\%) \\\\ Consistency \\(\\uparrow\\) & 1.699 & **1.976** (+16\\%) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 비디오 기반 생성 성능 벤치마크 마즈 외(2023)에 대한 베이스라인 모델과의 정량적 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline Method & Consistency \\(\\uparrow\\) & Alignment \\(\\uparrow\\) & BRISQUE \\(\\downarrow\\) \\\\ \\hline Custom diffusion & 0.7318 & 0.3278 & 22.7554 \\\\ Cones-V2 & 0.7781 & 0.2671 & 23.4823 \\\\ Ours & **0.8080** & **0.3325** & **22.5648** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 생성된 키 프레임의 품질에 대한 기존의 다른 방법들과의 정량적 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{MSVD-QA} & \\multicolumn{2}{c}{MSRVTT-QA} \\\\ \\cline{2-4}  & Acc \\(\\uparrow\\) & Score \\(\\uparrow\\) & Acc \\(\\uparrow\\) & Score \\(\\uparrow\\) \\\\ \\hline LLMA-VID & 0.493 & 3.169 & 0.435 & 2.865 \\\\ +Ours & **0.567** (+15\\%) & **3.460** (+9\\%) & **0.513** (+18\\%) & **3.141** (+10\\%) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 두 개의 제로 샷 비디오 QA 데이터 세트에 대한 기준선 모델과의 정량적 비교.\n' +
      '\n' +
      '2023b)인 것을 특징으로 하는 레이아웃을 활용한 다중 피사체 맞춤형 이미지 합성 방법. 테이블 두 번째 방법은 세 가지 메트릭 모두에서 기존 방법보다 성능이 뛰어나며, 일관된 고품질 키 프레임을 생성함을 나타낸다.\n' +
      '\n' +
      '짧은 비디오 이해 데이터 세트에 대해 훈련된 모델은 표에 나열된 기준선보다 상당한 성능 향상을 얻는다. 3과 표 4. 결과는 우리의 데이터가 긴 비디오 명령 튜닝에 초점을 맞추고 있지만 여전히 짧은 비디오에 대한 모델의 이해도를 향상시킬 수 있어 데이터의 견고성을 입증한다.\n' +
      '\n' +
      '장시간 비디오 이해에 대한 긴 비디오 이해 결과는 표 5에 나와 있다. 우리의 방법은 개요, 플롯, 시간 이해 측면에서 기준선을 크게 능가한다. 이는 데이터 세트의 풍부함을 보여주며 다양한 측면에 걸쳐 긴 비디오에서 모델의 이해도를 향상시키는 데 상당한 효과를 입증한다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '도. 5는 생성된 키 프레임의 품질에 대한 정성적인 결과를 시각화한다. 본 논문에서 제안하는 방법은 보다 일관되고 고품질의 프레임을 생성한다. 도. 6은 베이스라인 모델과 우리의 비디오 이해 결과를 시각화한다. 우리의 방법은 영화 이해에서 보다 합리적이고 상세한 답을 보여준다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '비전 언어 모델은 LLaMA(Touvron et al., 2023)와 같은 오픈 소스 대안과 함께 GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)의 성과와 함께 LLM의 고급 언어 능력을 활용하고 교차 모달리티 이해를 위해 LLM과 비전 모델을 통합하는 비전 언어 모델(VLM)을 개발하는 데 중점을 둔다. 이전의 대표적인 VLM인 miniGPT-4(Zhu et al., 2023) 및 LLaVA(Liu et al., 2023a)는 이미지와 텍스트 차원을 정렬하기 위해 고품질 이미지-명령 쌍을 구성함으로써 시각적 채팅에서 큰 능력을 보여주었다. 또한, VLM은 비디오 이해를 위해 개발된다(Jin et al., 2024; Li et al., 2023; Lin et al., 2023; Bai et al., 2024; Ma et al., 2023). Video-LLaMA(Zhang et al., 2023) utilizes BLIP-2(Li et al., 2023a) to\n' +
      '\n' +
      '그림 5: **우리와 다른 방법에 의해 생성된 키 프레임의 비교. 우리의 방법은 다소 일관되고 고품질의 프레임을 생성한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Overview} & \\multicolumn{2}{c}{Plot} & \\multicolumn{2}{c}{Temporal} \\\\ \\cline{2-7}  & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) & Compare ratio \\(\\uparrow\\) & Score \\(\\uparrow\\) \\\\ \\hline LLaMA-VID & 0.39 & 2.862 & 0.4 & 2.657 & 0.25 & 1.953 \\\\ +Ours & **0.61** (+56\\%) & **2.881** (+11\\%) & **0.6** (+50\\%) & **2.717** (+2\\%) & **0.75** (+200\\%) & **2.572** (+32\\%) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 긴 비디오 이해 벤치마크에 대한 기준선 모델과의 정량적 비교.\n' +
      '\n' +
      '각 프레임에 대한 비디오 임베딩을 추출하고 비디오 Q-전자에 의해 모든 프레임의 비디오 임베딩을 퓨즈한다. Video-ChatGPT Maaz et al. (2023)은 각각 시간적 및 공간적 차원들에 걸쳐 프레임 레벨 특징들을 평균함으로써 비디오 임베딩을 추출하도록 설계한다. 그러나, 이전 작업들은 표현을 위해 각 프레임에 대해 많은 수의 토큰을 생성해야 한다. 한 시간 길이의 비디오의 경우 이러한 VLM은 광범위한 비디오 시퀀스를 처리할 수 없다. 이러한 문제를 해결하기 위해, LLaMA-VID Li 등(2023)은 각 프레임을 단지 두 개의 토큰들로 인코딩함으로써 긴 비디오 이해를 위해 제안된다.\n' +
      '\n' +
      '비디오 명령어 튜닝 데이터세트 비디오 명령어 튜닝 데이터세트를 구성하기 위해 비디오-텍스트 쌍을 준비하는 것은 비디오 이해를 위한 VLM의 트레이닝을 용이하게 하는데 중추적이다. 대부분의 기존 방법들 Maaz et al. (2023); Tang et al. (2023); Zhong et al. (2022); Gao et al. (2021); Castro et al. (2022); Yang et al. (2023)은 풍부한 주석 정보를 활용하는 벤치마크 데이터세트 Han et al. (2023)에 기초하여 비디오 명령 튜닝 데이터세트를 구성한다. Video-ChatGPT는, 예를 들어, ActivityNet 데이터셋 Caba Heilbron 등으로부터 소싱된 VideoInstruct100K를 구성한다(2015). 그러나, 대부분의 기존 방법들은 구축된 명령어 튜닝 데이터 세트에서 긴 비디오 데이터가 부족하여 훈련된 모델의 긴 비디오 이해 능력이 떨어진다. 최근 제안된 LLaMA-VID는 MovieNet 데이터셋 Huang et al.(2020)로부터 데이터를 추출하여 영화 수준의 긴 동영상을 포함하는 명령어 데이터셋을 구축함으로써 이를 해결한다. 그러나 이러한 데이터 세트는 주석의 유형에 의해 제한되어 데이터의 다양성이 충분하지 않다. 본 논문에서는 새로운 파이프라인을 활용하여 긴 비디오 이해도를 높이기 위한 다양한 영화 수준의 비디오 명령어 튜닝 데이터셋을 제안한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 동영상 레벨 비디오의 주요 프레임 생성을 위한 효과적인 방법을 제안한다. 우리의 생성 과정은 크게 영화 플롯 생성, 스타일 고정화 과정, 영상 지시 데이터 생성의 세 단계로 구성된다. 제안된 데이터 생성 워크플로우는 모델을 위한 영화 수준의 비디오 데이터를 생성하는 문제를 상당히 완화하여 생성된 콘텐츠의 제어와 다양성을 모두 향상시킨다. 기존 데이터 세트와 새로 생성된 데이터 세트에 대해 수행된 실험은 접근법의 유효성을 검증한다. 데이터 세트의 풍부함 덕분에 모델은 이전 데이터 세트에 대한 훈련으로 이전에 달성할 수 없는 기능을 보여준다. 우리는 우리의 데이터 생성 파이프라인이 긴 비디오에 대한 멀티모달 모델의 이해 향상에 상당한 기여를 할 것이라고 믿는다. 또한 다른 도메인에 걸쳐 유사한 데이터 생성 프로세스를 채택할 수 있는 길을 열어줍니다.\n' +
      '\n' +
      '그림 6: **기준 모델과 우리의 비디오 이해 결과. 우리의 모델은 비디오 이해에서 더 합리적이고 상세한 답변을 보여준다.**\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      '대규모 언어 모델과 관련된 망각 문제는 파이프라인의 첫 번째 단계에서 생성된 프레임 설명의 불일치를 초래하여 비디오 장면에서 불연속성을 유발할 수 있다. 이 문제를 해결하기 위해 우리는 파이프라인 내에서 텍스트 생성 구성요소를 정제할 계획이다.\n' +
      '\n' +
      '## 8 윤리성명\n' +
      '\n' +
      '이 윤리 성명서는 영화LLM 프레임워크의 개발과 관련된 광범위한 영향과 윤리적 고려 사항을 반영한다. 긴 비디오에 대한 합성 고품질 데이터를 합성하기 위해 설계된 우리의 프레임워크는 특히 개인 정보 보호, 보안, 접근성, 고용에 미치는 영향, 지속 가능성 및 오용 가능성에 대한 윤리적 우려에 깊이 관여한다.\n' +
      '\n' +
      '**프라이버시 및 보안**: MovieLLM의 방법론은 합성 비디오 콘텐츠를 생성함으로써 프라이버시 및 보안의 보호를 강조함으로써, 실제 개인의 데이터를 악용할 위험을 제거한다. 이 전략은 이러한 보호 조치의 지속적인 개선에 대한 약속과 함께 승인되지 않은 데이터 액세스 및 위반 가능성을 상당히 최소화한다.\n' +
      '\n' +
      '**접근성 및 포괄성**: 우리의 프레임워크는 비디오 이해 기술의 접근성을 향상시켜 장애인을 포함한 다양한 인구 통계에 걸친 연구자가 AI 개발에 더 쉽게 참여할 수 있도록 하는 것을 목표로 한다. 우리는 MovieLLM이 포괄성과 접근성을 향상시켜 광범위한 사용자를 지원하는 데 전념하고 있습니다.\n' +
      '\n' +
      '**고용에 미치는 영향**: MovieLLM에 내재된 자동화가 일자리 이탈에 대한 우려를 촉발할 수 있지만, 우리의 의도는 그것을 대체하기 보다는 인간의 역량을 증강시키는 것이다. 비디오 데이터의 생성을 자동화하여 인간의 초점을 보다 창의적이고 지적으로 자극적인 작업으로 전환하여 잠재적으로 전문적인 만족도와 생산성을 풍부하게 할 수 있기를 바랍니다.\n' +
      '\n' +
      '지속 가능성**: 복잡한 AI 모델 훈련과 관련된 환경 비용을 인정하는 MovieLLM은 자원 효율적으로 설계되어 환경 책임에 대한 우리의 약속의 일부로 광범위한 계산 능력의 필요성을 줄이고 생태 발자국을 최소화한다.\n' +
      '\n' +
      '**잠재적 오류**: 우리는 영화LLM의 합성 데이터가 예를 들어 기만적인 콘텐츠를 만드는 데 잘못 사용될 가능성을 인식한다. 이를 해결하기 위해 우리는 책임 있는 사용을 옹호하며, 이는 다음과 같다.\n' +
      '\n' +
      '* 모니터링 및 탐지: 합성 데이터의 오용을 식별하고 방지하기 위한 메커니즘 개발.\n' +
      '* 윤리 지침 및 거버넌스: AI 연구 및 적용에서 윤리 표준을 촉진하여 인식과 준수를 보장한다.\n' +
      '* 플랫폼과의 협업: 합성 데이터의 사용을 윤리적 및 법적 기준과 일치시키기 위해 디지털 플랫폼과의 제휴.\n' +
      '* 투명성 및 책임성: 정보에 입각하고 윤리적인 적용을 보장하기 위해 영화LLM의 능력과 의도된 사용에 대한 명확한 커뮤니케이션 유지.\n' +
      '\n' +
      '이 진술을 통해 윤리적 연구와 혁신에 대한 우리의 의지를 강조하며 AI 기술을 책임감 있게 발전시키는 데 있어 이러한 중요한 고려 사항을 해결하는 것의 중요성을 강조한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bai et al. (2024) Ziyi Bai, Ruiping Wang, and Xilin Chen. 2024. 시선 및 초점 : 다중 이벤트 영상 질문 응답을 위한 메모리 프롬프트. _ arXiv preprint arXiv:2401.01529_.\n' +
      '* Heilbron et al. (2015) Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. 2015. Activitynet: 인간 활동 이해를 위한 대규모 비디오 벤치마크. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 961-970.\n' +
      '* Castro et al. (2022) Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan Stroud, and Rada Mihalcea. 2022. Fiber: Fill-in-the-blanks as challenging video understanding evaluation framework. _Association for Computational Linguistics_, pages 2925-2940.\n' +
      '* Chen and Dolan (2011) David Chen and William B Dolan. 2011. 패러프레이즈 평가를 위해 고도로 병렬적인 데이터를 수집하는 단계. _Association for Computational Linguistics_에서, 페이지 190-200.\n' +
      '* Feichtenhofer et al. (2019) Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. 비디오 인식을 위한 느린 속도의 네트워크.\n' +
      '\n' +
      '리논 갈, 유발 알루프, 유발 아츠몬, 또는 파타슈닉, 아미트 H. 버마노, 갈 체칙, 다니엘 코헨-오르. 2022. 이미지는 하나의 단어 가치가 있다: 텍스트 반전(textual inversion)을 사용하여 텍스트-이미지 생성을 개인화한다.\n' +
      '* Gao et al. (2021) Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. 2021. Env-qa: 동적 환경에 대한 포괄적인 이해를 위한 비디오 질문 응답 벤치마크. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 1675-1685.\n' +
      '* Han et al. (2023) Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartlama: 차트 이해와 생성을 위한 멀티모달 llm. _ arXiv preprint arXiv:2311.16483_.\n' +
      '* Huang et al. (2020) Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. 2020. Movienet: 영화 이해를 위한 전체론적 데이터셋. 유럽 컴퓨터 비전 회의 709-727 페이지 스프링어\n' +
      '* Jin et al. (2024) Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. 2024. Video-lavit: Unified video-language pretraining with decoupled visual-otional tokenization. _ arXiv preprint arXiv:2402.03161_.\n' +
      '* Kumari et al. (2023) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. 텍스트-이미지 확산의 다중 개념 맞춤화. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 1931-1941.\n' +
      '* Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_.\n' +
      '* Li 등(2023b) KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023b. 비디오 채팅: 채팅 중심의 비디오 이해. _ arXiv preprint arXiv:2305.06355_.\n' +
      '* Li et al. (2023c) Yanwei Li, Chengyao Wang, and Jiaya Jia. 2023c. Llama-vid: 이미지는 대형 언어 모델에서 2 토큰의 가치가 있다. _ arXiv preprint arXiv:2311.17043_.\n' +
      '* Lin et al. (2023) Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023. 비디오-라바: 투영 전의 정렬에 의한 통합된 시각적 표현을 학습하는 단계 _ arXiv preprint arXiv:2311.10122_.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. 시각적 지시 조율 arXiv preprint arXiv:2304.08485_.\n' +
      '* Liu et al. (2023b) Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. 2023b. 콘 2: 다수의 피사체와 함께 사용자 지정 가능한 이미지 합성. _ arXiv preprint arXiv:2305.19327_.\n' +
      '* Ma et al. (2023) Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. 2023. Vista-llama: 시각적 토큰들에 대한 동등한 거리를 통한 신뢰할 수 있는 비디오 서술자 _ arXiv preprint arXiv:2312.08870_.\n' +
      '* Maaz et al. (2023) Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. 비디오-채팅: 대형 비전 및 언어 모델을 통한 상세한 비디오 이해에 대하여 _ arXiv preprint arXiv:2306.05424_.\n' +
      '* Mittal et al. (2012) Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. 2012. 공간 영역에서의 참조 없는 화질 평가_ IEEE Transactions on image processing_, 21(12):4695-4708.\n' +
      '* OpenAI(2023) OpenAI. 2023. Gpt-4 기술 보고서\n' +
      '* Tang et al. (2023) Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. 2023. Video understanding with large language models: Survey. _ arXiv preprint arXiv:2312.17432_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_.\n' +
      '*Xu et al.(2016) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: 브리징 비디오 및 언어를 위한 대규모 비디오 기술 데이터세트. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 5288-5296.\n' +
      '* Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. 겉보기: 스마트폰 사용자로서의 멀티모달 에이전트 _ arXiv preprint arXiv:2312.13771_.\n' +
      '* Zhang et al.(2023) Hang Zhang, Xin Li, and Lidong Bing. 2023. 비디오-라마: 비디오 이해를 위한 명령어-튜닝된 시청각 언어 모델. _ arXiv preprint arXiv:2306.02858_.\n' +
      '* Zhong et al. (2022) Yaoyao Zhong, Wei Ji, Junbin Xiao, Yicong Li, Weihong Deng, and Tat-Seng Chua. 2022. 비디오 질문 응답: 데이터 세트, 알고리즘 및 도전. _Empirical Methods in Natural Language Processing_, pages 6439-6455.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: 고급 대형 언어 모델로 비젼-언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '그림 7: **베이스라인 모델과 우리의 비디오 이해 결과.** 우리의 모델은 비디오 이해에서 보다 합리적이고 상세한 답변을 보여준다.\n' +
      '\n' +
      '그림 8: **베이스라인 모델과 우리의 비디오 이해 결과.** 우리의 모델은 비디오 이해에서 보다 합리적이고 상세한 답변을 보여준다.\n' +
      '\n' +
      '그림 9: **베이스라인 모델과 우리의 비디오 이해 결과.** 우리의 모델은 비디오 이해에서 보다 합리적이고 상세한 답변을 보여준다.\n' +
      '\n' +
      '# 긴 비디오 평가를 위한 채팅 프롬프트\n' +
      '\n' +
      '"당신은 두 개의 비디오 기반 질문 응답 쌍을 비교하여 어느 것이 더 나은지 결정하고 참조 답변을 기반으로 점수를 매길 수 있도록 설계된 지능형 챗봇입니다.\\(\\backslash\\)n"\n' +
      '\n' +
      '"##Instructions:n"\n' +
      '\n' +
      '"1. 답변(더 상세하고, 더 나은)\\(\\backslash\\)n"과 관련된 상세 정보를 고려한다.\n' +
      '\n' +
      '"2. answer\\(\\backslash\\)n"에서의 캐릭터 관계를 고려한다.\n' +
      '\n' +
      '"3. 답과 관련된 결론 또는 결말을 고려하다\\(\\backslash\\)n"\n' +
      '\n' +
      '"4. 반복 문장 같은 넌센스는 허용되지 않으며, 매우 나쁜 대답으로 간주되어야 한다."\n' +
      '\n' +
      '"5. 만약 정답에 마크 번호가 사용된다면, 숫자의 순서는 옳아야 한다\\(\\backslash\\)n"\n' +
      '\n' +
      '"여러분의 대답은 이렇게 해야 합니다: {better one\': \'first one\',\n' +
      '\n' +
      '\'{first one\':3.5\',second one\':\'1}}, first answer과 second answer 중 선택만 하고, DO NOT PROVIDE ANY OTHER TEXT OR EXPLANATION은 위와 같이 비단뱀 사전 문자열만을 제공한다.\\ (\\backslash\\\\n")\n' +
      '\n' +
      'f"Question: {question}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"Reference answer: {gt_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"First answer: {our_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      'f"Second answer: {llama_answer}\\(\\backslash\\)n"\n' +
      '\n' +
      '\'이제 대답해 주세요\'\n' +
      '\n' +
      '도 10: **Long Video Evaluation에 대한 프롬프트. 긴 비디오 평가에 대한 프롬프트는 Ground truth를 기반으로 서로 다른 모델의 서로 다른 예측 답변을 비교하고 예측 답변을 1-5 척도로 점수화하여 모델의 다양한 능력을 평가한다.**\n' +
      '당신은 이야기를 만드는 전문가입니다. 테마가 "테멜"인 스토리를 생성합니다. 요구사항: 먼저 이야기의 제목, 개요 및 "스토리 톤 및 스타일"을 생성합니다. 이야기를 사용하여 이미지를 생성할 것이므로 "이야기 톤과 스타일"은 그림의 예술적 스타일을 기술해야 하며, "("")에 가장 중요한 문구가 포함된 문장이 아닌 형용사와 짧은 문구만을 사용해야 하며, "("")에 가장 중요한 문구가 포함된 단 하나의 단어 스타일-키워드(예시처럼 스타일-키워드를 제공해야 함)를 제공해야 한다. 둘째, 이야기가 일어날 모든 위치를 적어도 12개 위치에서 나열해야 하며, 위치에 대해 공통 장소만 허용되고 최대 2개의 단어를 사용해야 하며, 위치에 캐릭터 이름을 언급하지 않아야 하며, "Mary\'s" 또는 "Mary\'s house"와 같은 위치를 사용하여 누군가의 집을 표시하지 않아야 한다. 셋째, 모든 캐릭터의 이름을 생성하고 서양 유명인의 이름을 사용하여 "cox: 남성/여성이 xoxy의 oxo처럼 보인다"라는 형식으로 옷 색깔과 함께 묘사한다. 옷 색상은 심플한 한 단어 색상이면 충분합니다. 만약 한 캐릭터가 적어도 두 번 이야기 속에 등장한다면, 그것을 "캐릭터" 부분에 넣는다. 주인공이 가장 많이 등장하고, 조연 캐릭터가 이야기의 일부에서 등장한다. ‘캐릭터’ 부분에 나열된 캐릭터를 제외하고 스토리에 등장할 다른 캐릭터와 오브제는 모두 한 번만 등장해야 한다. 새로운 이야기를 위해서는 1, 2명의 주인공과 0, 1명의 조연 배우가 필요하다. 넷째, 개요를 기반으로 11개의 하위 스트로를 생성한다. "장면" 부분은 "위치" 부분에서 1,2 또는 3개의 위치를 선택해야 하며 전체 하위 층은 이러한 위치에서만 발생한다. "QA" 부분은 각 하위 스토리를 기반으로 한 질문-답변 쌍이다. 각 하위 스토리 이후에 "무엇", "왜", "어떻게" 및 "어디"의 네 가지 유형의 질문 중 하나만 질문할 수 있습니다. 답은 한두 문장이어야 하고, 너무 길 필요는 없다. 아래는 예시이다:\n' +
      '\n' +
      '* 이야기 제목 : 사랑의 교향곡\n' +
      '* 개요: 사랑의 교향곡\'은 활기차고 현대적인 스타일로 그려진 번화한 뉴욕의 도시를 배경으로 한 훈훈한 이야기입니다. 이 이야기는 음악에 대한 열정을 가진 재능 있는 바이올리니스트 아멜리아와 예술에 대한 숨겨진 사랑을 가진 성공한 사업가 이탄을 중심으로 전개된다. 그들의 삶은 사랑, 야망, 꿈의 추구의 복잡성을 탐색하면서 교차한다.\n' +
      '*스토리 톤 및 스타일: (로맨틱), ((도시 시크)), ((현대적 우아함, 보헤미안 매력)), 파스텔 팔레트, 미니멀리즘 미학, 소프트 조명, 솔직한 순간, 건축 미학, 도시 경관, 다이나믹, 감성, 친밀, 몽환, 노스탤지어\n' +
      '* 스타일-키워드 : 낭만\n' +
      '*위치: [\'콘서트홀\', \'사무실\', \'카페\', \'아트갤러리\', \'공원\', \'식당\', \'아파트\', \'지하철\', \'옥상\', \'스트리에프\', \'레이크\', \'교실\', \'시네마\'\n' +
      '* Character:\n' +
      '* 주요 문자:\n' +
      '*[\'아멜리아: 여자가 빨간 옷을 입은 앤 해서웨이와 닮았다>\n' +
      '*[\'Ethan: 한 남자가 파란색 옷을 입은 크리스 헴스워스와 닮았다>\n' +
      '* 지원문자:\n' +
      '[올리버: 한 남자가 로버트 다우니 주니어와 닮았다] 갈색으로.\n' +
      '* Substories:\n' +
      '* 서브스토리1: 우연의 멜로디\n' +
      '* 장면: 지하철, 스트리트\n' +
      '*내용: 아멜리아와 이선은 지하철에서 우연히 마주치고, 길을 걷는다.\n' +
      '* QA: 질문: 아멜리아와 이든은 어디서 만났나요? 대답: 지하철 안에서\n' +
      '* 서브스토리2: 우정의 함모니\n' +
      '* 장면: 카페\n' +
      '*내용: 아멜리아와 이선은 그들이 가장 좋아하는 카페에서 커피와 대화를 나누며 우정을 쌓기 시작한다.\n' +
      '* QA: 질문: 아멜리아와 이든은 어떻게 알게 되었나요? 정답: 그들은 커피를 마시며 카페에서 만난다.\n' +
      '\n' +
      '도 12: **생성된 긴 비디오 명령 데이터의 예** GPT-4 및 안내된 텍스트-이미지 생성 모델을 사용하여 합리적인 라인 및 대응하는 질문-응답 쌍을 갖는 무브-레벨 비디오의 일관된 키 프레임을 생성한다. 이러한 데이터는 비디오 이해에 대한 멀티모달 대형 언어 모델을 훈련하는 데 사용된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>