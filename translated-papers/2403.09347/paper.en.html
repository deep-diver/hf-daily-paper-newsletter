<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'tention". BurstAttention can take full advantage of the power of both distributed clusters and single devices within clusters. Specifically, given an extremely long sequence, BurstAttention first divides the sequence into partitions according to the number of devices in distributed clusters, and each partition is assigned to one of these devices. Then, each device projects the partitioned sequence into query, value, and key embedding partitions. The query partitions are pinned, and all key-value partitions are passed through all devices to compute their local attention scores with each pinned query partition. Based on the local attention scores, a global attention operation is adopted to aggregate the local results into the final global results.\n' +
      '\n' +
      'By fine-grained scheduling the computation and communication operations of devices during computing attention modules, as well as introducing online softmax (Milakov and Gimelshein, 2018), BurstAttention proposes global attention optimization (GAO) and local attention optimization (LAO), which can fully optimize the input-output (I/O) and communication procedures in distributed clusters. These two strategies offer substantial benefits for computing local attention scores in each device and aggregating local results into global ones in the whole cluster, including improved memory consumption, reduced communication overhead, and enhanced cache utilization. Owing to just splitting sequences, BurstAttention is orthogonal to other distributed methods and can be integrated with them for training and inferring Transformer-based LLMs, such as data parallelism (Valiant, 1990), tensor parallelism (Narayanan et al., 2021), pipeline parallelism (Huang et al., 2019), and zero redundancy optimizer (Rajbhandari et al., 2020; Ren et al., 2021).\n' +
      '\n' +
      'We evaluate BurstAttention and current competitive distributed attention solutions (Dao et al., 2022; Li et al., 2021) under various sequence length settings. Comparing to tensor parallelism (Megatron-V3) with FlashAttention methods, our method reducing 40% communication overheads and achieving 2\\(\\times\\) speedup during training 128K sequence length on 8\\(\\times\\)A100. The experimental results show that BurstAttention is a memory-efficient solution for attention modules to process long sequences and achieve good data throughputs. Moreover, since BurstAttention greatly optimizes the communication operations during the computation process of attention modules, BurstAttention makes it more difficult for device\'s communication to become a bottleneck as the devices in distributed clusters increase, and thus can better utilize distributed clusters than other distributed solutions.\n' +
      '\n' +
      'Figure 1: BurstAttention undertakes a two-step partitioning: dividing the sequence across multiple devices (inter-device), and then splitting the subsequences within each single device (intra-device). First, BurstAttention partitions the query, key, and value across devices and pass each sliced subsequence through all devices in a ring-like communication. This allows each device to process only a local attention at a time, and avoids the burden on memory caused by processing extremely long sequence at once. By transmitting \\(\\mathbf{K},\\mathbf{V}\\) and aggregating local attention results using online softmax, BurstAttention avoids storing the intermediate result \\(\\mathbf{Q}\\mathbf{K}^{T}\\), which has quadratic memory complexity, and instead recomputes it during the backward pass, which we call global attention optimization (GAO). BurstAttention further partitions the subsequences into smaller tiles, aiming to perform block-wise computations within local attention. This can utilize the high bandwidth of SRAM while minimizing access to the lower bandwidth HBM, which we call local attention optimization (LAO). Also, by using double-buffer, the communication can be overlapped with computation in BurstAttention.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      'As the key module in Transformers (Vaswani et al., 2017), an attention module can be formalized as\n' +
      '\n' +
      '\\[\\mathbf{S}=\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}},\\quad\\mathbf{P}=\\text{ softmax}(\\mathbf{S}),\\quad\\mathbf{O}=\\mathbf{P}\\mathbf{V}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{Q}\\in\\mathbb{R}^{N\\times d}\\) indicates the embeddings of the query sequence, \\(N\\) is the length of the query sequence, and \\(d\\) is the embedding dimension. \\(\\mathbf{K}\\in\\mathbb{R}^{N\\times d}\\) and \\(\\mathbf{V}\\in\\mathbb{R}^{N\\times d}\\) indicate the embeddings of the key sequence and the value sequence, respectively. \\(\\mathbf{S}\\in\\mathbb{R}^{N\\times N}\\) and \\(\\mathbf{P}\\in\\mathbb{R}^{N\\times N}\\) indicate the attention scores and the attention probabilities, respectively. \\(\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\) is the final attention result, which is the average of the value sequence embeddings weighted by the similarities between the query and key sequences. In this paper, we mainly use self-attention modules to illustrate BurstAttention, but BurstAttention can be easily extended to cross-attention modules. For more details of various attention modules in the Transformer architecture, we recommend referring to the original paper of Transformers (Vaswani et al., 2017), and we will not go into details.\n' +
      '\n' +
      '### The Whole Framework of BurstAttention\n' +
      '\n' +
      'In BurstAttention, \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) are divided into multiple partitions along the sequence dimension according to the number of devices (e.g., GPUs) in a distributed cluster. Each device in the cluster will be assigned a query partition, a key partition, and a value partition. Formally, given the device number \\(G\\), the \\(i\\)-th device will be assigned \\(\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\in\\mathbb{R}^{\\frac{N}{G}\\times d}\\). As shown in Figure 1, at each step, the \\(i\\)-th device receives a key partition \\(\\mathbf{K}_{j}\\) and a value partition \\(\\mathbf{V}_{j}\\) from its previous neighbor and performs local attention operations. After that, the \\(i\\)-th device sends its received key and value partitions \\(\\mathbf{K}_{j}\\) and \\(\\mathbf{V}_{j}\\) to its next neighbor for the use of the next step, which forms a ring-style communication process. This ring-style communication process continues until all \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) partitions have made a full circle around the ring, completing local attention operations on all devices. The local attention operations can be formalized as\n' +
      '\n' +
      '\\[\\mathbf{S}_{i,j}=\\frac{\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}}{\\sqrt{d}},\\ \\mathbf{P}_{i,j}=\\text{ softmax}(\\mathbf{S}_{i,j}),\\ \\mathbf{O}_{i,j}=\\mathbf{P}_{i,j}\\mathbf{V}_{j}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbf{O}_{i,j}\\in\\mathbb{R}^{\\frac{N}{G}\\times d}\\) indicates the local attention results between the device-assigned query partition \\(\\mathbf{Q}_{i}\\) and the device-received partitions \\(\\mathbf{K}_{j}\\) and \\(\\mathbf{V}_{j}\\). \\(\\mathbf{S}_{i,j}\\in\\mathbb{R}^{\\frac{N}{G}\\times\\frac{N}{G}}\\) and \\(\\mathbf{P}_{i,j}\\in\\mathbb{R}^{\\frac{N}{G}\\times\\frac{N}{G}}\\) indicate the local attention scores and the local attention probabilities, respectively.\n' +
      '\n' +
      'Obviously, Eq. (1) and Eq. (2) are not equivalent, we thus introduce global attention operations to aggregate all local attention results \\(\\{\\mathbf{O}_{i,j}\\}_{i=1,j=1}^{\\frac{N}{G},\\frac{N}{G}}\\) into the final partitioned attention results \\(\\mathbf{O}_{i}\\in\\mathbb{R}^{\\frac{N}{G}\\times d}\\), and \\(\\{\\mathbf{O}_{i}\\}_{i=1}^{\\frac{N}{G}}\\) is the final global attention results. To make both the global and local attention operations more efficient, we introduce Global Attention Optimization (GAO) and Local Attention Optimization (LAO), respectively. Next, we will introduce these attention optimization strategies in detail.\n' +
      '\n' +
      '### Global Attention Optimization (GAO)\n' +
      '\n' +
      'Global attention operations are to aggregate \\(\\mathbf{O}_{i,j}\\) in Eq. (2) into \\(\\mathbf{O}_{i}\\). For some conventional methods such as RingAttention (Li et al., 2021), for the \\(i\\)-th query partition, they store the intermediate results \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\) for every \\(j\\). This introduces a non-negligible memory overhead. To get rid of this memory overhead, we introduce GAO.\n' +
      '\n' +
      'As shown in Figure 1, GAO consists of two main steps. First, devices are organized in a ring for communication. Each round, \\(\\mathbf{K},\\mathbf{V}\\) partitions are shifted along the ring to the next adjacent device. Second, after each round of \\(\\mathbf{K},\\mathbf{V}\\) transmission, each device \\(i\\) performs a local attention operation using the partitions \\(\\mathbf{Q}_{i}\\) and its received partition \\(\\mathbf{K}_{j}\\), and \\(\\mathbf{V}_{j}\\), as described in Eq. (2). The local attention result \\(\\mathbf{O}_{i,j}\\) are then dynamically accumulated into global attention result \\(\\mathbf{O}_{i}\\) by employing online softmax (Milakov and Gimelshein, 2018), which eliminates the need to store intermediate results \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\).\n' +
      '\n' +
      'As depicted in Algorithm 1, in the forward pass, we dynamically maintain the row-wise maximum value \\(m_{i}\\) of \\(\\mathbf{S}_{\\mathbf{i,j}}\\) as in Line 1 and the row-wise sum \\(l\\) of \\(\\mathbf{P}_{\\mathbf{i,j}}\\) as in Line 1 to avoid storing \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\), and use \\(m_{i}\\) and \\(l_{i}\\) for scaling during the aggregation of \\(\\mathbf{O}_{i}\\) as in Line 1. Note that, the functions \\(\\text{rowmax}(\\cdot)\\) and \\(\\text{rowsum}(\\cdot)\\) can be formalized as\n' +
      '\n' +
      '\\[\\begin{split}[\\text{rowmax}(\\mathbf{W})]_{i}&=\\max _{j}\\{[\\mathbf{W}]_{i,j}\\},\\\\ [\\text{rowsum}(\\mathbf{W})]_{i}&=\\sum_{j}[\\mathbf{W }]_{i,j},\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\([\\cdot]_{i}\\) is the \\(i\\)-th element of the vector, \\([\\cdot]_{i,j}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of the matrix. To make the subsequent backward pass more efficient, we store \\(lse_{i}\\) besides the global results \\(\\mathbf{O}_{i}\\) after the forward pass. During the backward pass, as depicted in Algorithm 2, we employ the same strategy for the forward pass to obtain gradients based only on recomputed \\(\\mathbf{S}\\) and \\(\\mathbf{P}\\).\n' +
      '\n' +
      '### Local Attention Optimization (LAO)\n' +
      '\n' +
      'Given \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), and \\(\\mathbf{V}_{j}\\), the local attention operations that involve these partitions are performed only on a single device (e.g., a GPU). When computing \\(\\mathbf{O}_{i,j}\\) in Eq. (2), \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\) are computed and stored on the HBM of the device. To avoid frequent I/O operations of \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\) on the HBM, the local attention operations of BurstAttention further divide \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), and \\(\\mathbf{V}_{j}\\) into tiles along the sequence dimension, with each tile \\(\\frac{M}{4d}\\) sequence length, where \\(M\\) represents the SRAM size of the device, \\(d\\) represents the attention head dimension.\n' +
      '\n' +
      'As shown in Figure 1, during computing \\(\\mathbf{O}_{i,j}\\), each thread block reads the tiles of \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), \\(\\mathbf{V}_{j}\\) from the HBM to SRAM, the tiles of \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\) are computed and then written on the SRAM instead of the HBM, \\(\\mathbf{O}_{i,j}\\) are dynamically accumulated based on online softmax operations and written back to the HBM. Since the SRAM has a much higher I/O bandwidth than the HBM, the above optimization can make local attention operations more efficient. Although the memory of the SRAM is tiny, further dividing \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), and \\(\\mathbf{V}_{j}\\) into many fine-grained tiles ensure the intermediate results \\(\\mathbf{S}_{i,j}\\) and \\(\\mathbf{P}_{i,j}\\) can be entirely stored into the SRAM.\n' +
      '\n' +
      'Intuitively, when BurstAttention is running on a single device rather than a distributed cluster, there is no need to use GAO, and LAO will play the same role as FlashAttention (Dao et al., 2022), i.e., FlashAttention can be viewed as a specialization of BurstAttention using a single device.\n' +
      '\n' +
      '### Overlapping Communication and Computation\n' +
      '\n' +
      'Although splitting sequences can efficiently utilize distributed clusters to handle the long-sequence attention, this also inevitably introduces additional time costs to transmit partitions between devices. To this end, BurstAttention leverages the potential of devices (e.g., GPUs) for overlapping communication and computation. This contrasts with some other typical distributed methods like tensor parallelism (Narayanan et al., 2021), where such overlapping is not feasible due to the dependency of subsequent layers\' computations on preceding layers\' outputs.\n' +
      '\n' +
      'To address this, BurstAttention adopts a double-buffer technique, enabling concurrent execution of communication and computation. The technique designs two buffers for each device, one is used as input to local attention operations, and the other is used to receive data from other devices. As depicted in Figure 1, each element (query, key, or value) involved in the ring-style communication process is allocated a dedicated buffer. Concurrent with the initiation of each local attention round, the double-buffer technique triggers the transmission of the corresponding buffer tensor. This preemptive action ensures that, by the commencement of the subsequent local attention round, the required data is already available on each device, having been carried over by the buffer. The process is then repeated until all local attention operations are completed, with each round of local attention operations initiating the transmission of data required for the next round of local attention operations. More details can be found in our appendix3.\n' +
      '\n' +
      '### Integrating Sparse Attention Methods\n' +
      '\n' +
      'Various sparse attention methods, including low-rank methods (Winata et al., 2020; Wang et al., 2020), kernel-based methods (Katharopoulos et al., 2020; Choromanski et al., 2020; Qin et al., 2022) and downsampling methods (Lee et al., 2019; Jaegle et al., 2021) are also widely explored. These methods reduce the time and memory costs of attention modules by computing a limited selection of similarity scores from a sequence rather than all possible pairs, resulting in sparse attention softmax logits rather than dense ones. Recently, Ding et al. (2023) have explored sparse attention based on distributed clusters and achieved promising results.\n' +
      '\n' +
      'The sequence parallelism mechanism makes BurstAttention easy to cooperate with sparse attention methods. During the computation process of BurstAttention, given \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), \\(\\mathbf{V}_{j}\\), if there is no need to compute the similarities between these partitions, then the local attention operations on these partitions can be skipped directly. If just some tokens in \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\) and \\(\\mathbf{V}_{j}\\) are required to compute their similarities for final attention results, we can similarly skip unnecessary operations in local attention operations. Note that these sparse attention methods inevitably lead to significant performance degradation, along with reducing the time and memory overheads. Although BurstAttention is well compatible with sparse attention methods, in the actual processing of long sequences, the use of these lossy methods needs to be cautious.\n' +
      '\n' +
      '## 3 Overhead Analysis\n' +
      '\n' +
      'In this section, we will analyze the memory, I/O, and communication overheads of BurstAttention as compared to existing competitive distributed attention solutions. As data parallelism and pipeline parallelism are often used as the most basic distributed strategies and cannot reduce the cost of long sequence processing, we focus here on comparing BurstAttention, tensor parallelism (Narayanan et al., 2021), and the typical sequence parallelism method RingAttention (Li et al., 2021).\n' +
      '\n' +
      '### Memory and I/O Overheads\n' +
      '\n' +
      'When we split the input along the sequence dimension across devices for global operations and further split them in each device for local operations, the memory overheads caused by \\(\\mathbf{Q}\\mathbf{K}^{T}\\) will be reduced to \\(\\frac{1}{(M/d)^{2}G^{2}}\\) of the original ones. Table 1 shows the memory overheads of various distributed attention solutions. The table shows that BurstAttention has lower activation memory while tensor parallelism has lower parameter memory. This means that the longer the sequence, the more pronounced the advantage of BurstAttention. Moreover, by combining BurstAttention with some parallelism strategies like zero redundancy optimizer (Rajbhandari et al., 2020; Ren et al., 2021) to partition parameters, BurstAttention can easily obtain the same parameter memory overheads as tensor parallelism. In terms of I/O overheads, RingAttention requires \\(\\Theta(\\frac{BZN^{2}}{G}+BZNd)\\) memory accesses on every single device of the whole cluster; tensor parallelism and BurstAttention only require \\(\\Theta(\\frac{BZN^{2}}{(M/d)^{2}G})\\) memory accesses. This indicates that BurstAttention can significantly reduce I/O time costs compared to other distributed attention baselines.\n' +
      '\n' +
      '### Communication Overheads\n' +
      '\n' +
      'In the forward pass, BurstAttention involves one round of ring-style peer-to-peer communications on the \\(\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{B\\times Z\\times\\frac{Q}{G}\\times d}\\), with a total cost of \\(\\Theta(2BZNd)\\). In the backward pass, BurstAttention requires one round of ring-style communication on tensors \\(\\mathbf{Q},\\mathbf{d}\\mathbf{Q},\\mathbf{d}\\mathbf{O}\\in\\mathbb{R}^{B\\times \\frac{Q}{G}\\times Z\\times d}\\) and \\(D,lse\\in\\mathbb{R}^{B\\times\\frac{Q}{G}\\times Z}\\), with a total cost of \\(\\Theta(3BZNd+2\\frac{B\\times Z}{G})\\). Table 1 shows the communication overheads of various distributed attention solutions. The forward communication of RingAttention is the same as BurstAttention, which is \\(\\Theta(2BZNd)\\), but without GAO and LAO, RingAttention requires a total cost of \\(\\Theta(6BZNd)\\) in the backward pass, which is about twice that of BurstAttention. Therefore, BurstAttention has great advantage of communication overheads during training than RingAttention. The forward communication of tensor parallelism is \\(\\Theta(4BZNd)\\) and the total communication is \\(\\Theta(8BZNd)\\), thus BurstAttention also has higher communication efficiency during both inferring and training than tensor parallelism.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c c|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{FlashATT/LAO} & \\multicolumn{3}{c|}{Memory Overheads} & \\multicolumn{2}{c}{Communication Overheads} \\\\  & & Parameter & Activation & Forward & Backward \\\\ \\hline RingAttention & w/o & \\(4HZd\\) & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+\\frac{BNH}{G}\\) & \\multirow{2}{*}{\\(2BZNd\\)} & \\multirow{2}{*}{\\(6BZNd\\)} \\\\ RingAttention\\({}^{\\dagger}\\) & \\(-\\) & & & \\\\ \\hline Tensor Parallelism & w/o & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+BNH\\) & \\multirow{2}{*}{\\(4BZNd\\)} & \\multirow{2}{*}{\\(4BZNd\\)} & \\multirow{2}{*}{\\(4BZNd\\)} \\\\ Tensor Parallelism & w/ FlashATT & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+BNH\\) & & \\\\ \\hline BurstAttention & w/o & \\(4HZd\\) & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+\\frac{BN}{(M/d)^{2}G^{2}}+\\frac{BNH}{G}\\) & \\multirow{2}{*}{\\(2BZNd\\)} & \\multirow{2}{*}{\\(3BZNd+2BZN\\)} \\\\ BurstAttention & w/ LAO & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: The overheads of different distributed attention solutions. \\(G\\) is the device number, \\(B\\) denotes the batch size, \\(N\\) represents the sequence length, \\(Z\\) signifies the number of attention heads, \\(d\\) corresponds to the hidden dimension per head, \\(H\\) represents the model dimension of Transformers, and \\(M\\) represents the device SRAM size. \\({}^{\\dagger}\\) means from an implementation perspective, RingAttentionâ€™s separating \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) into two independent rounds of communication cannot be combined with FlashAttention to improve efficiency.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      'We perform experiments in two configurations: one involves a single node equipped with 8 A100 GPUs linked via PCI-E, and the other is a distributed setup comprising four identical nodes, each with the same 8 A100 GPU configuration, interconnected by a 600 Gb/s RoCE network. We adopts two LLMs\' settings in our experiments, LLaMA-2 with 7 billion parameters (7b) and LLaMA-2 with 13 billion parameters (13b) (Touvron et al., 2023b). Our experiments consist of the following methods:\n' +
      '\n' +
      '(1) **TP**, which refers to tensor parallelism (Narayanan et al., 2021), a commonly used distributed strategy in the stages of both training and inference. Note that here we futher classify TP into **TP (Megatron V1)** and **TP (Megatron V3)** based on the detail communication operations (Megatron V1 uses the all-reduce operation while Megatron V3 uses the combination of the all-gather and reduce-scatter operations). (2) **TP w/ FlashAttention**, which combines FlashAttention (Dao et al., 2022) with tensor parallelism as a strong baseline. **Note that this is a commonly used strategy in current LLM pre-training and inference.** (3) RingAttention**, a typical sequence parallelism baseline. (4) **BurstAttention**, our distributed attention method includes both GAO and LAO strategies. (5) **BurstAttention w/o LAO**, where we remove the LAO strategy for ablation studies. (6) **BurstAttention+ZeRO**, where we futher optimize the memory overhead of BurstAttention by adopting the ZeRO(Rajbhandari et al., 2020) technique to shard model parameters across devices.\n' +
      '\n' +
      'As we mentioned before, data parallelism and pipeline parallelism cannot effectively reduce the cost of long sequence processing, and we do not use them as baselines. In fact, we conduct some experiments to adapt data parallelism and pipeline parallelism for long-sequence attention, but unfortunately, these two parallelism methods cannot process extremely long sequences. **From our pilot experiments, directly adopting data parallelism or pipeline parallelism can only handle sequences shorter than 8192, much shorter than RingAttention and TP.**\n' +
      '\n' +
      'Our experiments does not specifically focus on any particular attention masking mechanism. However, for the methods we compared against, such as Tensor Parallelism (Megatron V3) with FlashAttention, we adopt its causal implementation in these experiments. This means that our baselines can bypass half of the attention computations owing to the causal attention structure. We observe that this approach yields only a marginal improvement, as communication remains the bottleneck in our experimental environment. Notably, in our implementation of BurstAttention, the computation is overlapped by the communication, which is a key factor in the observed performance gains. This distinction is crucial to understand the context and the specific conditions under which our method demonstrates its advantages.\n' +
      '\n' +
      '### Inference Latency\n' +
      '\n' +
      'In this section, we focus on the latency needed for generating the first token (i.e., the first token latency) in the inference process. We concentrate on the time of the first token generation because the long-sequence attention computation mainly exists in the inference encoding process. Since the first token latency is much higher than the latency of generating subsequent tokens, the first token latency thus becomes one of the most critical targets existing works seek to optimize.\n' +
      '\n' +
      'In real-time AI services such as ChatGPT, the system\'s responsiveness significantly impacts the user experience, and these applications usually output results in a streaming manner to improve responsiveness. Since the first token latency is the longest, the first token latency directly influences the perceived responsiveness and efficiency of the model in these streaming scenarios.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Sequence Length & 4,096 & 8,192 & 16,384 & 32,768 & 65,536 & 131,072 & 262,144 \\\\ \\hline RingAttention & 0.42\\(\\pm\\)0.01 & 0.87\\(\\pm\\)0.01 & 2.00\\(\\pm\\)0.01 & 5.13\\(\\pm\\)0.05 & OOM & OOM & OOM \\\\ TP(Megatron V1) w/ Flash & 0.67\\(\\pm\\)0.01 & 1.29\\(\\pm\\)0.01 & 2.58\\(\\pm\\)0.01 & 5.27\\(\\pm\\)0.01 & 11.63\\(\\pm\\)0.02 & 27.54\\(\\pm\\)0.01 & 71.52\\(\\pm\\)0.06 \\\\ TP(Megatron V3) w/ Flash & 0.73\\(\\pm\\)0.02 & 1.36\\(\\pm\\)0.01 & 2.68\\(\\pm\\)0.01 & 5.67\\(\\pm\\)0.01 & 12.25\\(\\pm\\)0.01 & 28.73\\(\\pm\\)0.03 & 75.52\\(\\pm\\)0.05 \\\\ BurstAttention w/o LAO & 0.46\\(\\pm\\)0.01 & 0.88\\(\\pm\\)0.01 & 1.79\\(\\pm\\)0.01 & 3.88\\(\\pm\\)0.01 & 10.78\\(\\pm\\)0.01 & OOM & OOM \\\\ BurstAttention & **0.44\\(\\pm\\)0.01** & **0.84\\(\\pm\\)0.01** & **1.68\\(\\pm\\)0.01** & **3.27\\(\\pm\\)0.01** & **6.49\\(\\pm\\)0.01** & **16.01\\(\\pm\\)0.01** & **49.32\\(\\pm\\)0.11** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: The first token latency of the LLaMA-7b inference (s).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Sequence Length & 4,096 & 8,192 & 16,384 & 32,768 & 65,536 & 131,072 & 262,144 \\\\ \\hline RingAttention & 0.66\\(\\pm\\)0.01 & 1.36\\(\\pm\\)0.01 & 3.08\\(\\pm\\)0.01 & 7.98\\(\\pm\\)0.02 & OOM & OOM & OOM \\\\ TP(Megatron V1) w/ Flash & 1.05\\(\\pm\\)0.01 & 2.01\\(\\pm\\)0.01 & 4.03\\(\\pm\\)0.01 & 8.41\\(\\pm\\)0.01 & 18.56\\(\\pm\\)0.02 & 44.39\\(\\pm\\)0.04 & OOM \\\\ TP(Megatron V3) w/ Flash & 1.07\\(\\pm\\)0.01 & 2.09\\(\\pm\\)0.01 & 4.20\\(\\pm\\)0.01 & 8.76\\(\\pm\\)0.01 & 19.06\\(\\pm\\)0.06 & 45.46\\(\\pm\\)0.03 & 119.03\\(\\pm\\)0.04 \\\\ BurstAttention w/o LAO & 0.72\\(\\pm\\)0.01 & 1.39\\(\\pm\\)0.01 & 2.77\\(\\pm\\)0.05 & 5.99\\(\\pm\\)0.01 & 16.95\\(\\pm\\)0.01 & OOM & OOM \\\\ BurstAttention & **0.69\\(\\pm\\)0.01** & **1.40\\(\\pm\\)0.05** & **2.57\\(\\pm\\)0.03** & **5.08\\(\\pm\\)0.02** & **9.92\\(\\pm\\)0.01** & **25.91\\(\\pm\\)0.01** & **78.80\\(\\pm\\)0.07** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: The first token latency of the LLaMA-13b inference (s).\n' +
      '\n' +
      'As shown in Table 2 and Table 3, we can see that, compared with tensor parallelism, sequence parallelism methods are more suitable to infer long sequences. Compared with the RingAttention method, by using GAO, BurstAttention can support longer sequences. By further using LAO, BurstAttention can achieve more latency improvements and support much longer sequences. Note that, although TP (Megatron V3) is more memory efficient than TP (Megatron V1), the all-reduce operation used by TP (Megatron V1) is better optimized than the reduce-scatter and all-gather operations used by TP(Megatron V3). In the actual inference, TP(Megatron V1) is slightly faster than TP (Megatron V3). Since TP (Megatron V3) has a similar time to TP (Megatron V1) but better memory efficiency, we mainly compare our method with TP (Megatron V3) in subsequent experiments.\n' +
      '\n' +
      '### Training Performance\n' +
      '\n' +
      'For training LLMs, a batch is required to have 2 to 4 million tokens, otherwise, the model performance may be degraded, i.e., the longer the sequence length is, the smaller the batch size is. Due to this, several GPUs may need to process one example together. For example, using 2048 GPUs to train 128-layer GPT-3, the sequence length is 4096, the batch size is 1024, data parallelism is 16, pipeline parallelism is 32, and tensor parallelism is 4. In this scenario, the optimal setup is to divide a batch into 64 micro-batches with a micro-batch size of 1. In this case, four GPUs under the same tensor parallelism group are inevitably required to process one piece of data together. In view of this, we fix the batch size to 1 for experimental convenience and vary the input sequence length from 1K to 32K.\n' +
      '\n' +
      'As can be seen from Figure 1(a), although tensor parallelism adopts FlashAttention to improve its processing of long sequences, both RingAttention and BurstAttention have better training time than tensor parallelism when processing long sequences. This is also why existing works using tensor parallelism to train LLMs usually set the training length between 2048 and 4096. Compared with BurstAttention, RingAttention is limited by the sequence length since it stores too many intermediate states, but BurstAttention can support the longest input length. On the other hand, BurstAttention without LAO has a similar trend of training time as RingAttention and tensor parallelism.\n' +
      '\n' +
      'From Figure 3, BurstAttention achieves nearly 2.0\\(\\times\\) speedup when the sequence is longer than 128K. Also combining BurstAttention with ZeRO optimization brings significant improvements in memory efficiency. Although BurstAttention+ZeRO brings little additional communication overheads, BurstAttention+ZeRO still achieves memory efficiency comparable to Megatron V3 and demonstrates superior speed in both multi-node and single-node setups than Megatron V3. This suggests that BurstAttention, with its current optimizations, offers a more efficient solution in terms of speed, even when faced with a memory-efficient competitor like Megatron V3.\n' +
      '\n' +
      '### Scaling Ability\n' +
      '\n' +
      'In this section, we further verify the scaling ability of BurstAttention. In Figure 3(a), we set batch size to 1 and sequence length to 65,536, and then evaluate the latency changes with increasing GPU numbers. As shown in the figure, in the single-GPU scenario, BurstAttention with LAO is equivalent to FlashAttention, and its inference latency is on par with the baseline using FlashAttention. Tensor parallelism cannot further decrease the latency when the number of GPUs increases from 4 to 8 due to the communication overhead with increased batch-size, while BurstAttention can achieve better scaling trends. Note that RingAttention requires storing \\(\\Theta(\\frac{BZX^{*}}{G})\\) memory for each layer, which is extremely large and cannot fit into GPUs even sharded on 8 GPUs. In Figure 3(b), we fix the sequence length to 4096 and the number of GPUs to 8 to evaluate the training throughput changes with increasing batch sizes. The experimental results show that BurstAttention can support a larger batch size, and the throughput grows with the increase of batch sizes in training scenario.\n' +
      '\n' +
      'Figure 2: The training time and memory of LLaMA-7b on 8\\(\\times\\)A100.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'Transformer-based LLMs such as GPT (Brown et al., 2020; Ouyang et al., 2022), LLaMA (Touvron et al., 2023a;b), and PaLM (Chowdhery et al., 2022; Anil et al., 2023) have achieved great success (Han et al., 2021; Bommasani et al., 2021; Zhao et al., 2023). Despite the success of these LLMs, they still face efficiency challenges: one is that as these models continue to grow in size, the time and memory costs associated with training and inference have become bottlenecks. Another is that the quadratic attention computational complexity of the Transformer architecture makes these LLMs difficult to handle long sequences. Up to now, various parallelism strategies (Valiant, 1990; Huang et al., 2019; Rajbhandari et al., 2020; Narayanan et al., 2021) and memory optimization strategies (Ren et al., 2021; Chen et al., 2016; Korthikanti et al., 2023), have well solved the bottleneck caused by the model size growth, but it is still challenging to solve the efficiency issue caused by the sequence growth.\n' +
      '\n' +
      'To enable LLMs to process longer sequences more efficiently, several attention solutions have been proposed. Korthikanti et al. (2023) adopt selective activation recomputation to avoid storing attention softmax logits during the forward pass, and then recompute these logits during the backward pass to build a computation graph for backpropagation, significantly reducing memory overheads of attention modules to process long sequences. Rabe and Staats (2021) formalize the computation of attention modules at the block level and make each thread block in devices handle the attention computation of a subsequence, further reducing temporary memory consumptions and achieving a logarithmic memory complexity relative to the sequence length. Based on these works, Dao et al. (2022) introduce FlashAttention, a CUDA implementation of attention modules that leverages the fast I/O capabilities of the SRAM in devices for further speedup. FlashAttention optimizes the attention algorithm by introducing I/O complexity analysis and minimizing the I/O costs on the HBM in devices, offering a new perspective on attention optimization.\n' +
      '\n' +
      'While the above solutions focus on optimizing the long-sequence attention problem using a single device, they still struggle to handle extremely long sequences due to the limitations of a single device\'s performance. Some efforts have therefore aimed to address this long-sequence challenge using distributed clusters. Adopting general parallelism strategies is most straightforward method, such as data parallelism (Valiant, 1990), tensor parallelism (Narayanan et al., 2021), pipeline parallelism (Huang et al., 2019), and zero redundancy optimizer (Rajbhandari et al., 2020; Ren et al., 2021). To better process long sequences using distributed clusters, Li et al. (2021) propose the sequence parallelism method RingAttention, which splits the computation and memory overheads of attention modules across multiple devices following the sequence dimension.\n' +
      '\n' +
      'Figure 4: Scaling abilities on different GPU numbers and batch sizes.\n' +
      '\n' +
      'Figure 3: The training time and memory of LLaMA-7b on 32\\(\\times\\)A100.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '* Rabe and Staats (2021) Rabe, M. N. and Staats, C. Self-attention does not need \\(o(n^{2})\\) memory. _arXiv preprint arXiv:2112.05682_, 2021.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21:5485-5551, 2020.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO: Memory optimizations toward training trillion parameter models. In _Proceedings of SC_, 2020.\n' +
      '* Ren et al. (2021) Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. ZeRO-Offload: Democratizing billion-scale model training. In _Proceedings of ATC_, pp. 551-564, 2021.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLAMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. LLAMA 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Valiant (1990) Valiant, L. G. A bridging model for parallel computation. _Communications of the ACM_, pp. 103-111, 1990.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In _Proceedings of NeurIPS_, 2017.\n' +
      '* Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Winata et al. (2020) Winata, G. I., Cahyawijaya, S., Lin, Z., Liu, Z., and Fung, P. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In _Proceedings of ICASSP_, pp. 6144-6148, 2020.\n' +
      '* Zhao et al. (2023) Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.\n' +
      '\n' +
      '## Appendix A BurstAttention Algorithm with Double-buffer\n' +
      '\n' +
      '```\n' +
      '1:\n' +
      '2:Matrices \\(\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\in\\mathbb{R}^{\\frac{N}{N}\\times d}\\) on the \\(i\\)-th device\n' +
      '3:Initialize \\(\\mathbf{Q}_{i}=(0)_{\\frac{N}{N}\\times d}\\in\\mathbb{R}^{\\frac{N}{N}\\times d},l_{ i}=(0)_{\\frac{N}{N}}\\in\\mathbb{R}^{\\frac{N}{N}},m_{i}=(-\\infty)_{\\frac{N}{N}}\\in \\mathbb{R}^{\\frac{N}{N}}\\)\n' +
      '4:Initialize Buffer \\(K_{buf}\\) with \\(\\mathbf{K}_{i}\\),Buffer \\(V_{buf}\\) with \\(\\mathbf{V}_{i}\\).\n' +
      '5:for\\(j=1\\) to \\(\\mathbf{G}\\)do\n' +
      '6:if\\(j\\)!=then\n' +
      '7: Get \\(K_{j},V_{j}\\) from \\(K_{buf},V_{buf}\\); {Wait communication thread\'s job finished}\n' +
      '8:endif\n' +
      '9:AsyncCommunicationCall:\n' +
      '10: Initiate asynchronous communication thread\n' +
      '11: Let \\(\\text{Buf}=(K_{buf},V_{buf})\\)\n' +
      '12: Asynchronously Send the Buf to next device and recvive Buf from previous device\n' +
      '13:\\(\\mathbf{S}_{i,j}=\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\);\n' +
      '14:\\(m_{i,j}=\\text{rowmax}(\\mathbf{S}_{i,j})\\);\n' +
      '15:\\(\\mathbf{P}_{i,j}=\\text{exp}(\\mathbf{S}_{i,j})\\);\n' +
      '16:\\(l_{i,j}=\\text{rowsum}(\\mathbf{P}_{i,j})\\);\n' +
      '17:\\(\\mathbf{O}_{i,j}=\\mathbf{P}_{i,j}\\mathbf{V}_{j}\\); {The end of the forward pass of local attention operations.}\n' +
      '18:\\(m_{\\text{new}}\\leftarrow\\max\\left\\{m_{i},m_{i,j}\\right\\}\\);\n' +
      '19:\\(\\mathbf{l}_{i}=e^{m_{i}-m_{\\text{new}}}l_{i}+e^{m_{i,j}-m_{\\text{new}}}l_{i,j}\\);\n' +
      '20:\\(\\mathbf{O}_{i}=e^{m_{i}-m_{\\text{new}}}\\mathbf{O}_{i}+e^{m_{i,j}-m_{\\text{new}}} \\mathbf{O}_{i,j}\\);\n' +
      '21:\\(m_{i}=m_{\\text{new}}\\);\n' +
      '22:endfor\n' +
      '23:\\(\\mathbf{O}_{i}=\\text{diag}(l_{i})^{-1}\\mathbf{O}_{i}\\);\n' +
      '24:\\(lse_{i}=m_{i}+\\log l_{i}\\);\n' +
      '25:if\\(j\\)!=then\n' +
      '26: Get \\(dQ_{j},dK_{j},dV_{j}\\) from \\(dQ_{buf},dK_{buf},dV_{buf}\\); {Wait communication thread\'s job finished}\n' +
      '27:endif\n' +
      '28:AsyncCommunicationCall:\n' +
      '29: Initiate asynchronous communication thread\n' +
      '30: Let \\(\\text{Buf}=(\\mathbf{Q}_{buf}\\mathbf{d}\\mathbf{Q}_{buf},\\mathbf{d}\\mathbf{O}_{buf },D_{buf},lse_{buf})\\)\n' +
      '31: Send the Buf to next device and recvive new Buf from previous device;\n' +
      '32:\\(\\mathbf{S}_{j,i}=\\mathbf{Q}_{j}\\mathbf{K}_{j}^{T}\\); {The backward pass of local attention operations (w/o LAO).}\n' +
      '33:\\(\\mathbf{P}_{j,i}=\\text{exp}(\\mathbf{S}_{j,i}-lse_{j})\\);\n' +
      '34:\\(\\mathbf{dV}_{i}=\\mathbf{dV}_{i}+\\mathbf{P}_{i,i}^{T}\\mathbf{d}\\mathbf{O}_{i}\\);\n' +
      '35:\\(\\mathbf{dP}_{j,i}=\\mathbf{d}\\mathbf{O}_{j}\\mathbf{V}_{i}^{T}\\);\n' +
      '36:\\(\\mathbf{dS}_{j,i}=\\mathbf{P}_{j,i}\\circ(\\mathbf{dP}_{j,i}-D_{j})\\);\n' +
      '37:\\(\\mathbf{dK}_{i}=\\mathbf{dK}_{i}+\\mathbf{dS}_{j,i}^{T}\\mathbf{Q}_{j}\\);\n' +
      '38:\\(\\mathbf{dQ}_{j}=\\mathbf{dQ}_{j}+\\mathbf{dS}_{j,i}\\mathbf{K}_{i}\\) ; {The end of the backward pass of local attention operations.}\n' +
      '39:endfor\n' +
      '40:\\(\\mathbf{dQ}_{G},\\mathbf{dK}_{G},\\mathbf{dV}_{G}\\);\n' +
      '```\n' +
      '\n' +
      '**Algorithm 3** The forward pass of GAO with overlapping\n' +
      '\n' +
      '## Appendix B Runtime Analysis of Tensor Parallelism in one Transformer Block\n' +
      '\n' +
      '**Theorem B.1**.: _In a Transformer block employing Tensor Parallelism (TP) within the Megatron-V3 framework, the total runtime \\(T\\) is determined by the sum of communication times for all-gather and reduce-scatter operations, and the computation times for the attention (attn) and feedforward (ffn) modules, distributed across the devices._\n' +
      '\n' +
      '**Definition B.2** (Input Tensor and Cluster Configuration).: Let the input tensor \\(x\\) have dimensions \\((B,N,Z^{\\prime},d)\\), where \\(B\\) isthe batch size, \\(N\\) is the sequence length, \\(Z^{\\prime}\\) is the number of partition heads per device, and \\(d\\) is the hidden dimension per attention head. The cluster bandwidth \\(b\\) is assumed to be uniform across all \\(G\\) devices.\n' +
      '\n' +
      '**Lemma B.3** (Communication Time).: _The time \\(t_{\\text{comm}}\\) for each all-gather or reduce-scatter operation in TP is given by_\n' +
      '\n' +
      '\\[t_{\\text{comm}}=\\frac{(B\\times N\\times Z^{\\prime}\\times d)\\times M\\times(G-1)} {b\\times G},\\]\n' +
      '\n' +
      '_where \\(M\\) represents the number of bits required to store one tensor element._\n' +
      '\n' +
      '**Proposition B.4** (Runtime Calculation).: _The total runtime \\(T\\) for processing one Transformer block under TP is_\n' +
      '\n' +
      '\\[T=4\\times t_{\\text{comm}}+\\frac{T_{\\text{attr}}}{G}+\\frac{T_{\\text{fft}}}{G},\\]\n' +
      '\n' +
      '_accounting for two all-gather and two reduce-scatter operations, and the parallelized computation times for the attention (attn) and feedforward (ffn) modules._\n' +
      '\n' +
      '_Remark B.5_.: This analysis assumes the use of full attention mechanisms in the Transformer block. It does not account for sparse, approximate, or causal attention methods that could alter computational and communication complexities.\n' +
      '\n' +
      '## Appendix C Runtime Analysis of BurstAttention in One Transformer Block\n' +
      '\n' +
      '**Theorem C.1**.: _In the BurstAttention framework, the total runtime for a given Transformer block is influenced by the communication and computation times for both the attention and feedforward modules. The runtime accounts for asymmetric communication processes in both forward and backward passes._\n' +
      '\n' +
      '**Definition C.2** (Input Tensor and Cluster Configuration).: Let the input tensor \\(x\\) have dimensions \\((B,N^{\\prime},Z,d)\\), where \\(B\\) is the batch size, \\(N^{\\prime}\\) is the partitioned sequence length per device, \\(Z\\) is the number of attention heads per device, and \\(d\\) is the hidden dimension per attention head. The cluster\'s uniform bandwidth is \\(b\\) across all \\(G\\) devices, and \\(d_{ffn}\\) denotes the intermediate dimension of the feedforward layer.\n' +
      '\n' +
      '**Lemma C.3** (Activation Communication Time in BurstAttention).: _In BurstAttention, there are two ring-style communications for key (\\(K\\)) and value (\\(V\\)) activations and five for query (\\(Q\\)), gradient with respect to \\(Q\\) (\\(d_{Q}\\)), gradient with respect to the attention output (\\(dO\\)), and reduction variables (\\(D\\), \\(lse\\)) during the backward pass. The communication times for these activations in the forward and backward processes are:_\n' +
      '\n' +
      '\\[t_{\\text{comm\\_attn\\_f}} =\\frac{2\\times B\\times N^{\\prime}\\times Z\\times d\\times M\\times(G- 1)}{b\\times G},\\] \\[t_{\\text{comm\\_attn\\_b}} =\\frac{(3\\times B\\times N^{\\prime}\\times Z\\times d+2\\times B \\times N^{\\prime}\\times Z)\\times M\\times(G-1)}{b\\times G}.\\]\n' +
      '\n' +
      '**Lemma C.4** (Weight Communication Time in BurstAttention).: _In BurstAttention\'s attention module, there are four linear layers with weights of dimension \\(H\\times Z\\times d\\). The feedforward module has two linear layers with dimensions \\(H\\times d_{ffn}\\) and \\(d_{ffn}\\times H\\). The communication time for the weights of these layers is calculated as:_\n' +
      '\n' +
      '\\[t_{\\text{comm\\_weights}}=\\frac{(4\\times H\\times Z\\times d+2\\times H\\times d_{ ffn})\\times M\\times(G-1)}{b\\times G},\\]\n' +
      '\n' +
      '**Proposition C.5** (Runtime Calculation in BurstAttention).: _The total runtime for the BurstAttention framework is calculated as:_\n' +
      '\n' +
      '\\[T_{\\text{attn\\_f}}=\\max(T_{\\text{attn\\_f}},t_{\\text{comm\\_attn\\_f}})+\\max(T_{ \\text{attn\\_b}},t_{\\text{comm\\_attn\\_b}})+T_{\\text{fft}}+t_{\\text{comm\\_weights}},\\]\n' +
      '\n' +
      '_where \\(T_{\\text{attn\\_f}}\\) and \\(T_{\\text{attn\\_b}}\\) represent the computation times for the forward and backward processes of the attention module, respectively, and \\(T_{\\text{fft}}\\) is the runtime of the feedforward module._\n' +
      '\n' +
      '_Remark C.6_.: Same with analysis of TP, this analysis does not account for sparse, approximate, or causal attention methods that could alter computational and communication complexities.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>