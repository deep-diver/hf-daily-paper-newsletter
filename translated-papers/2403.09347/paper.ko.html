<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '"의견" 버스트 어텐션은 분산 클러스터와 클러스터 내의 단일 디바이스 모두의 전력을 최대한 활용할 수 있다. 구체적으로, 극히 긴 시퀀스가 주어지면, BurstAttention은 먼저 분산 클러스터 내의 디바이스의 수에 따라 시퀀스를 파티션으로 분할하고, 각각의 파티션은 이들 디바이스들 중 하나에 할당된다. 그런 다음, 각 장치는 분할된 시퀀스를 쿼리, 값 및 키 임베딩 파티션으로 투영한다. 쿼리 파티션은 고정되고 모든 키-값 파티션은 모든 장치를 통해 전달되어 각 고정 쿼리 파티션으로 로컬 주의 점수를 계산합니다. 로컬 주의력 점수를 기반으로 로컬 결과를 최종 글로벌 결과로 집계하기 위해 글로벌 주의력 작업을 채택한다.\n' +
      '\n' +
      '버스트어텐션은 컴퓨팅 어텐션 모듈 동안 디바이스의 계산 및 통신 동작을 세밀하게 스케줄링하고, 온라인 소프트맥스(Milakov and Kimelshein, 2018)를 도입함으로써, 분산 클러스터에서 입력-출력(I/O) 및 통신 절차를 완전히 최적화할 수 있는 글로벌 어텐션 최적화(GAO) 및 로컬 어텐션 최적화(LAO)를 제안한다. 이 두 가지 전략은 각 장치에서 로컬 주의 점수를 계산하고 개선된 메모리 소비, 감소된 통신 오버헤드 및 향상된 캐시 활용을 포함하여 전체 클러스터에서 로컬 결과를 글로벌 결과로 집계하는 데 상당한 이점을 제공한다. 버스트어텐션은 단지 시퀀스를 분할하기 때문에 다른 분산 방법과 직교하며 데이터 병렬성(Valiant, 1990), 텐서 병렬성(Narayanan et al., 2021), 파이프라인 병렬성(Huang et al., 2019), 제로 리던던시 최적화기(Rajbhandari et al., 2020; Ren et al., 2021)와 같은 트랜스포머 기반 LLM을 훈련 및 추론하기 위해 통합될 수 있다.\n' +
      '\n' +
      '다양한 시퀀스 길이 설정 하에서 BurstAttention과 현재 경쟁적인 분산 주의 솔루션(Dao et al., 2022; Li et al., 2021)을 평가한다. 플래시 어텐션 방법을 사용한 텐서 병렬(Megatron-V3)과 플래시 어텐션 방법을 비교한 결과, 8\\(\\times\\)A100에서 128K 시퀀스 길이를 훈련하는 동안 40%의 통신 오버헤드를 줄이고 2\\(\\times\\)의 속도 향상을 달성하였다. 실험 결과는 버스트 어텐션이 긴 시퀀스를 처리하고 데이터 처리량이 우수한 어텐션 모듈을 위한 메모리 효율적인 솔루션임을 보여준다. 또한, 버스트어텐션은 어텐션 모듈의 연산 과정에서 통신 연산을 크게 최적화하기 때문에, 분산 클러스터 내의 디바이스들이 증가함에 따라 디바이스의 통신이 병목 현상이 되는 것을 더욱 어렵게 하고, 따라서 분산 클러스터를 다른 분산 솔루션에 비해 더 잘 활용할 수 있다.\n' +
      '\n' +
      '도 1: BurstAttention은 2단계 분할을 수행한다: 시퀀스를 다수의 디바이스(디바이스간)에 걸쳐 분할한 다음, 각각의 단일 디바이스(디바이스내) 내의 서브시퀀스들을 분할한다. 첫째, BurstAttention은 질의, 키 및 값을 디바이스들에 걸쳐 분할하고, 각각의 슬라이스된 서브시퀀스를 링-유사 통신에서 모든 디바이스들을 통해 전달한다. 이것은 각 디바이스가 한번에 로컬 주의만을 처리할 수 있게 하고, 극도로 긴 시퀀스를 한번에 처리함으로써 야기되는 메모리에 대한 부담을 회피한다. 버스트어텐션은 온라인 소프트맥스(softmax)를 이용하여 \\(\\mathbf{K},\\mathbf{V}\\)를 전송하고 로컬 어텐션 결과를 집계함으로써 2차 메모리 복잡도를 갖는 중간 결과 \\(\\mathbf{Q}\\mathbf{K}^{T}\\)를 저장하는 것을 피하고, 이를 전역 어텐션 최적화(GAO)라고 한다. 버스트 어텐션은 서브시퀀스를 더 작은 타일로 파티셔닝하여 로컬 어텐션 내에서 블록별 계산을 수행하는 것을 목표로 한다. 이는 낮은 대역폭 HBM에 대한 액세스를 최소화하면서 SRAM의 높은 대역폭을 활용할 수 있으며, 이를 로컬 주의 최적화(local attention optimization, LAO)라고 한다. 또한, 이중 버퍼를 사용함으로써 버스트 어텐션에서 통신과 연산이 중첩될 수 있다.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '트랜스포머에서의 키 모듈로서(Vaswani et al., 2017), 어텐션 모듈은 다음과 같이 정형화될 수 있다.\n' +
      '\n' +
      '\\frac{\\mathbf{Q}\\mathbf{K}^{T}{\\sqrt{d},\\quad\\mathbf{P}=\\text{ softmax}(\\mathbf{S}),\\quad\\mathbf{O}=\\mathbf{P}\\mathbf{V}, \\tag{1}\\text{\n' +
      '\n' +
      '여기서 \\(\\mathbf{Q}\\in\\mathbb{R}^{N\\times d}\\)은 상기 질의 시퀀스의 임베딩들을 나타내고, \\(N\\)은 상기 질의 시퀀스의 길이이고, \\(d\\)은 상기 임베딩 차원이다. \\(d\\) (\\mathbf{K}\\in\\mathbb{R}^{N\\times d}\\) 및 \\(\\mathbff{V}\\in\\mathbb{R}^{N\\times d}\\)는 각각 키 시퀀스와 값 시퀀스의 임베딩을 나타낸다. \\\\fff{K}\\in\\mathbb{R}^{N\\times d}\\)는 각각 키 시퀀스와 값 시퀀스의 임베딩을 나타낸다. (\\mathbf{S}\\in\\mathbb{R}^{N\\times N}\\)와 \\(\\mathbff{P}\\in\\mathbb{R}^{N\\times N}\\)는 각각 주의점수와 주의확률을 나타낸다. \\\\mathbff{S}\\in\\mathbb{R}^{N\\times N}\\은 각각 주의점수와 주의확률을 나타낸다. (\\mathbf{O}\\in\\mathbb{R}^{N\\times d}\\)는 질의와 핵심 시퀀스 사이의 유사도에 의해 가중된 값 시퀀스 임베딩의 평균인 최종 어텐션 결과이다. 본 논문에서는 버스트어텐션을 설명하기 위해 주로 자기 주의 모듈을 사용하지만, 버스트어텐션은 교차 주의 모듈로 쉽게 확장될 수 있다. 트랜스포머 아키텍처에서 다양한 주의 모듈에 대한 자세한 내용은 트랜스포머의 원본 논문(Vaswani et al., 2017)을 참조할 것을 권장하며 자세한 내용은 다루지 않을 것이다.\n' +
      '\n' +
      '버스트 어텐션의 전체 프레임워크\n' +
      '\n' +
      '버스트어텐션에서 \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\) 및 \\(\\mathbf{V}\\)은 분산 클러스터 내의 장치(예를 들어, GPU)의 수에 따라 시퀀스 차원을 따라 다중 파티션으로 분할된다. 클러스터의 각 장치에는 쿼리 파티션, 키 파티션 및 값 파티션이 할당됩니다. 형식적으로, 장치 번호 \\(G\\)가 주어지면, \\(i\\)번째 장치에는 \\(\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\in\\mathbb{R}^{\\frac{N}{G}\\times d}\\가 할당될 것이다. 도 1에 도시된 바와 같이, 각 단계에서 \\(i\\)번째 디바이스는 이전 이웃으로부터 키 파티션 \\(\\mathbf{K}_{j}\\) 및 값 파티션 \\(\\mathbf{V}_{j}\\)을 수신하여 로컬 어텐션 동작을 수행한다. 그 후, \\(i\\)번째 장치는 수신된 키와 값 파티션 \\(\\mathbf{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\)을 다음 단계의 사용을 위해 다음 이웃에게 전송하여 링 형태의 통신 프로세스를 형성한다. 이러한 링 방식의 통신 과정은 모든 \\(\\mathbf{K}\\) 및 \\(\\mathbf{V}\\) 파티션이 링 주위에 완전한 원을 이룰 때까지 계속되어 모든 장치에서 로컬 주의 동작을 완료한다. 로컬 주의 동작은 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\frac{\\mathbf{S}_{i,j}=\\frac{\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}{\\sqrt{d},\\\\mathbf{P}_{i,j}=\\text{ softmax}(\\mathbf{S}_{i,j}),\\\\mathbf{O}_{i,j}=\\mathbf{P}_{i,j}\\mathbf{V}_{j}, \\tag{2}\\text{\n' +
      '\n' +
      '여기서 \\(\\mathbf{O}_{i,j}\\in\\mathbbb{R}^{\\frac{N}{G}\\times d}\\)는 디바이스-할당 질의 파티션 \\(\\mathbf{Q}_{i}\\)과 디바이스-수신 파티션 \\(\\mathbff{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\) 사이의 로컬 주의 결과를 나타낸다. \\(\\mathbf{O}_{i,j}\\in\\mathbbb{R}^{\\frac{N}\\times d}\\) (\\mathbf{S}_{i,j}\\in\\mathbb{R}^{\\frac{N}{G}\\times\\frac{N}}\\) 및 \\(\\mathbf{P}_{i,j}\\in\\mathbb{R}^{\\frac{N}\\times\\frac{N}{G}\\)은 각각 국부적 주의점수와 국부적 주의확률을 나타낸다.\n' +
      '\n' +
      '물론, Eq. (1) 및 Eq. (2)는 동일하지 않으므로, 모든 국부적 주의력 결과를 최종 분할 주의력 결과로 집계하기 위한 전역 주의력 연산을 도입한다.\\(\\{\\mathbf{O}_{i,j}\\}_{i=1,j=1}^{\\frac{N}{G},\\frac{N}{G}\\)\\(\\mathbf{O}_{i}\\in\\mathbb{R}^{\\frac{N}\\times d}\\) 및 \\(\\mathbf{O}_{i}_{i=1}^{\\frac{N}{G}\\)은 최종 전역 주의력 결과이다. 글로벌 어텐션 연산과 로컬 어텐션 연산을 모두 효율적으로 수행하기 위해 글로벌 어텐션 최적화(Global Attention Optimization, GAO)와 로컬 어텐션 최적화(Local Attention Optimization, LAO)를 각각 소개한다. 다음으로 이러한 주의력 최적화 전략에 대해 자세히 소개하겠습니다.\n' +
      '\n' +
      '### Global Attention Optimization (GAO)\n' +
      '\n' +
      '글로벌 주의 연산은 식에서 \\(\\mathbf{O}_{i,j}\\)을 집계하는 것이다. (2) \\(\\mathbf{O}_{i}\\) 링어텐션(Li et al., 2021)과 같은 몇 가지 종래의 방법에 대해, \\(i\\)번째 질의 분할에 대해, 그들은 매 \\(j\\)마다 중간 결과 \\(\\mathbf{S}_{i,j}\\) 및 \\(\\mathbf{P}_{i,j}\\)을 저장한다. 이것은 무시할 수 없는 메모리 오버헤드를 도입합니다. 이러한 메모리 오버헤드를 제거하기 위해 GAO를 소개합니다.\n' +
      '\n' +
      '그림 1과 같이 GAO는 크게 두 단계로 구성된다. 먼저, 디바이스들은 통신을 위해 링으로 조직된다. 각 라운드, \\(\\mathbf{K},\\mathbf{V}\\) 파티션은 링을 따라 다음 인접 장치로 이동한다. 둘째, \\(\\mathbf{K},\\mathbf{V}\\) 전송의 각 라운드 후에, 각 장치 \\(i\\)는 식에 설명된 바와 같이 파티션 \\(\\mathbf{Q}_{i}\\)과 수신된 파티션 \\(\\mathbf{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\)을 사용하여 로컬 주의 동작을 수행한다. (2). 그런 다음 온라인 소프트맥스(Milakov and Kimelshein, 2018)를 사용하여 국부적 주의 결과 \\(\\mathbf{O}_{i,j}\\)를 글로벌 주의 결과 \\(\\mathbf{O}_{i}\\)에 동적으로 누적하여 중간 결과 \\(\\mathbf{S}_{i,j}\\)과 \\(\\mathbf{P}_{i,j}\\)을 저장할 필요가 없도록 한다.\n' +
      '\n' +
      '알고리즘 1에 묘사된 바와 같이, 순방향 패스에서 우리는 라인 1에서와 같이 \\(\\mathbff{S}_{\\mathbf{i,j}\\)의 행별 최대값 \\(m_{i}\\)과 라인 1에서와 같이 \\(\\mathbff{P}_{\\mathbf{i,j}}\\)의 행별 합 \\(l\\)을 동적으로 유지하고, 라인 1에서와 같이 \\(\\mathbff{S}\\)과 \\(\\mathbf{P}\\)의 집합 동안 스케일링에 \\(m_{i}\\)과 \\(l_{i}\\)을 사용할 수 있음을 유의한다.\n' +
      '\n' +
      '{split}[\\text{rowmax}(\\mathbf{W})]_{i}&=\\max_{j}\\{[\\mathbf{W}]_{i,j}\\},\\\\[\\text{rowsum}(\\mathbf{W})]_{i}&=\\sum_{j}[\\mathbf{W}]_{i,j},\\end{split}\\tag{3}\\}\n' +
      '\n' +
      '여기서 \\([\\cdot]_{i}\\)는 벡터의 \\(i\\)번째 원소이고, \\([\\cdot]_{i,j}\\)는 행렬의 \\(i\\)번째 행 및 \\(j\\)번째 열에 있는 원소이다. 후속 백워드 패스를 보다 효율적으로 만들기 위해, 우리는 포워드 패스 이후의 전역적 결과들\\(\\mathbf{O}_{i}\\) 외에 \\(lse_{i}\\)을 저장한다. 백워드 패스는 알고리즘 2와 같이 재계산된 \\(\\mathbf{S}\\)과 \\(\\mathbf{P}\\)에 기초하여 구배를 얻기 위해 동일한 전략을 사용한다.\n' +
      '\n' +
      '### LAO(Local Attention Optimization)\n' +
      '\n' +
      '주어진 \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\)이 주어지면, 이러한 분할을 포함하는 로컬 어텐션 연산은 단일 장치(예: GPU)에서만 수행된다. Eq.에서 \\(\\mathbf{O}_{i,j}\\)를 계산할 때. (2), \\(\\mathbf{S}_{i,j}\\) 및 \\(\\mathbf{P}_{i,j}\\)이 계산되어 장치의 HBM에 저장된다. HBM에서 \\(\\mathbf{S}_{i,j}\\)와 \\(\\mathbf{P}_{i,j}\\)의 빈번한 I/O 연산을 피하기 위해 BurstAttention의 국부적 어텐션 연산은 \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), \\(\\mathbf{V}_{j}\\)을 시퀀스 차원에 따라 타일로 더 나누고, 각 타일 \\(\\frac{M}{4d}\\) 시퀀스 길이, 여기서 \\(M\\)은 장치의 SRAM 크기를 나타내며, \\(d\\)은 어텐션 헤드 차원을 나타낸다.\n' +
      '\n' +
      '도 1에 도시된 바와 같이, 각 스레드 블록은 HBM에서 SRAM으로 \\(\\mathbf{O}_{i,j}\\)의 타일을 읽고, HBM에서 SRAM으로 \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\), \\(\\mathbf{V}_{j}\\), \\(\\mathbf{S}_{i,j}\\) 및 \\(\\mathbf{P}_{i,j}\\)의 타일을 계산한 후, 온라인 소프트맥스 연산을 기반으로 SRAM에 기록하고, 다시 HBM으로 기록한다. SRAM은 HBM보다 훨씬 높은 I/O 대역폭을 갖기 때문에, 위의 최적화는 로컬 주의 동작을 더 효율적으로 만들 수 있다. SRAM의 메모리는 작지만, \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\)을 많은 세립 타일들로 더 분할하면 중간 결과 \\(\\mathbf{S}_{i,j}\\) 및 \\(\\mathbf{P}_{i,j}\\)는 SRAM에 완전히 저장될 수 있다.\n' +
      '\n' +
      '직관적으로 분산 클러스터가 아닌 단일 디바이스에서 BurstAttention이 실행되는 경우 GAO를 사용할 필요가 없으며, LAO는 FlashAttention(Dao et al., 2022)과 같은 역할을 하게 될 것이며, 즉 FlashAttention은 단일 디바이스를 사용하는 BurstAttention의 전문화로 볼 수 있다.\n' +
      '\n' +
      '### 겹침 통신 및 계산\n' +
      '\n' +
      '분할 시퀀스는 분산 클러스터를 효율적으로 활용하여 긴 시퀀스 주의를 처리할 수 있지만, 이는 또한 장치 간에 파티션을 전송하기 위한 추가적인 시간 비용을 필연적으로 도입한다. 이를 위해 BurstAttention은 중복 통신 및 계산을 위한 장치(예를 들어, GPU)의 잠재력을 활용한다. 이는 텐서 병렬(Narayanan et al., 2021)과 같은 몇몇 다른 전형적인 분산 방법들과 대조되는데, 이러한 중첩은 선행하는 층들의 출력들에 대한 후속 층들의 계산들의 의존성으로 인해 실현 가능하지 않다.\n' +
      '\n' +
      '이를 해결하기 위해 BurstAttention은 이중 버퍼 기법을 채택하여 통신과 연산을 동시에 수행할 수 있다. 이 기법은 각 디바이스에 대해 두 개의 버퍼를 설계하는데, 하나는 로컬 어텐션 동작에 대한 입력으로서 사용되고, 다른 하나는 다른 디바이스로부터 데이터를 수신하기 위해 사용된다. 도 1에 도시된 바와 같이, 링-스타일 통신 프로세스에 관련된 각각의 엘리먼트(쿼리, 키, 또는 값)는 전용 버퍼를 할당받는다. 각각의 로컬 어텐션 라운드의 개시와 동시에, 이중-버퍼 기법은 대응하는 버퍼 텐서의 전송을 트리거한다. 이러한 선제적 조치는 후속 로컬 주의 라운드의 시작에 의해, 필요한 데이터가 버퍼에 의해 이월된 각 장치에서 이미 이용가능하다는 것을 보장한다. 그 후, 프로세스는 모든 로컬 주의 동작들이 완료될 때까지 반복되며, 각각의 라운드의 로컬 주의 동작들은 다음 라운드의 로컬 주의 동작들에 필요한 데이터의 전송을 개시한다. 자세한 내용은 부록3에서 확인할 수 있습니다.\n' +
      '\n' +
      '스파스 어텐션 방법을 통합한###\n' +
      '\n' +
      'Low-rank 방법(Winata et al., 2020; Wang et al., 2020), 커널 기반 방법(Katharopoulos et al., 2020; Choromanski et al., 2020; Qin et al., 2022) 및 다운샘플링 방법(Lee et al., 2019; Jaegle et al., 2021)을 포함한 다양한 희소 주의 방법들도 널리 탐구되고 있다. 이러한 방법은 가능한 모든 쌍이 아닌 시퀀스에서 유사성 점수의 제한된 선택을 계산함으로써 주의력 모듈의 시간 및 메모리 비용을 감소시켜 조밀한 것보다 희박한 주의력 소프트맥스 로짓이 발생한다. 최근 Ding et al.(2023)은 분산 클러스터를 기반으로 희소 주의를 탐구하여 유망한 결과를 얻었다.\n' +
      '\n' +
      '시퀀스 병렬 메커니즘은 버스트 어텐션을 희박한 어텐션 방법과 쉽게 협력할 수 있게 한다. 버스트어텐션의 연산과정에서 각 분할영역간의 유사성을 계산할 필요가 없는 경우, 각 분할영역에서의 국부적 어텐션 연산은 생략된다. 최종 어텐션 결과에 대한 유사성을 계산하기 위해 \\(\\mathbf{Q}_{i}\\), \\(\\mathbf{K}_{j}\\) 및 \\(\\mathbf{V}_{j}\\)의 일부 토큰만 필요한 경우, 로컬 어텐션 연산에서 불필요한 연산을 유사하게 생략할 수 있다. 이러한 희박한 주의 방법은 필연적으로 시간과 메모리 오버헤드를 줄이는 것과 함께 상당한 성능 저하를 초래한다. 버스트 어텐션은 희박한 어텐션 방법과 잘 호환되지만 긴 시퀀스의 실제 처리에서 이러한 손실 방법의 사용은 주의할 필요가 있다.\n' +
      '\n' +
      '## 3 오버헤드 분석\n' +
      '\n' +
      '본 절에서는 기존의 경쟁 분산 주의 솔루션과 비교하여 BurstAttention의 메모리, I/O 및 통신 오버헤드를 분석할 것이다. 데이터 병렬성과 파이프라인 병렬성이 가장 기본적인 분산 전략으로 많이 사용되고 있으며 긴 시퀀스 처리 비용을 줄일 수 없기 때문에 BurstAttention, 텐서 병렬성(Narayanan et al., 2021), 전형적인 시퀀스 병렬성 방법 RingAttention(Li et al., 2021)을 비교하는데 중점을 둔다.\n' +
      '\n' +
      '### 메모리 및 I/O 오버헤드\n' +
      '\n' +
      '전체 연산을 위해 각 장치에서 시퀀스 차원을 따라 입력을 분할하고 로컬 연산을 위해 각 장치에서 추가 분할할 때, \\(\\mathbff{Q}\\mathbf{K}^{T}\\)에 의해 발생하는 메모리 오버헤드는 원본의 \\(\\frac{1}{(M/d)^{2}G^{2}}\\)으로 줄어들게 된다. 다양한 분산 주의 솔루션의 메모리 오버헤드는 표 1과 같다. 표에서 BurstAttention은 활성화 메모리가 더 낮고 텐서 병렬성은 파라미터 메모리가 더 낮다는 것을 알 수 있다. 이는 시퀀스가 길어질수록 BurstAttention의 장점이 두드러진다는 것을 의미한다. 또한, BurstAttention을 zero redundancy optimizer(Rajbhandari et al., 2020; Ren et al., 2021)와 같은 병렬성 전략들과 결합하여, BurstAttention은 텐서 병렬성과 동일한 파라미터 메모리 오버헤드를 쉽게 얻을 수 있다. I/O 오버헤드 측면에서 RingAttention은 전체 클러스터의 모든 장치에서 \\(\\frac{BZN^{2}}{G}+BZNd)\\) 메모리 액세스를 필요로 한다. 텐서 병렬성과 BurstAttention은 단지 \\(\\theta(\\frac{BZN^{2}}{(M/d)^{2}G})) 메모리 액세스를 필요로 한다. 이는 BurstAttention이 다른 분산 주의 기준선에 비해 I/O 시간 비용을 크게 줄일 수 있음을 나타낸다.\n' +
      '\n' +
      '### Communication Overheads\n' +
      '\n' +
      '버스트어텐션(BurstAttention)은 순방향 패스에서 링형 피어투피어 통신을 \\(\\mathbf{K},\\mathbf{V}\\in\\mathbb{R}^{B\\times Z\\times\\frac{Q}{G}\\times d}\\(\\Theta(2BZNd)\\)의 총비용으로 \\(\\mathbff{K},\\mathbf{V}\\in\\mathbb{R}^{B\\times Z\\times\\frac{Q}{G}\\times d}\\)으로 한 라운드 수행한다. 백워드 패스에서 버스트어텐션은 텐서\\(\\mathbf{Q},\\mathbf{d}\\mathbf{Q},\\mathbff{O}\\in\\mathbbb{R}^{B\\times \\frac{Q}{G}\\times Z\\times d})와\\(D,lse\\in\\mathbbb{R}^{B\\times \\frac{Q}\\times Z}\\times Z}\\times \\(3BZNd+2\\frac{B\\times Z}{G})\\(3BZNd+2\\frac{B\\times Z}{G})\\(3BZNd+2\\frac{B\\times Z}{G})\\(3BZNd+2\\frac{B\\times Z}{G})\\(3BZNd+2\\frac{B\\times Z}{G})\\(3BZNd+2\\frac{B\\times Z}{G}) 다양한 분산 주의 솔루션의 통신 오버헤드는 표 1과 같다. 링어텐션의 순방향 통신은 버스트어텐션과 동일하지만 GAO와 LAO가 없는 링어텐션은 역방향 패스에서 버스트어텐션의 약 2배인 \\(\\Theta(6BZNd)\\)의 총 비용이 필요하다. 따라서 BurstAttention은 RingAttention보다 훈련 중 통신 오버헤드가 큰 장점이 있다. 텐서 병렬의 순방향 통신은 \\(\\Theta(4BZNd)\\)이고 전체 통신은 \\(\\Theta(8BZNd)\\)이므로 BurstAttention은 텐서 병렬보다 추론과 훈련 모두에서 통신 효율이 높다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c c|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{FlashATT/LAO} & \\multicolumn{3}{c|}{Memory Overheads} & \\multicolumn{2}{c}{Communication Overheads} \\\\  & & Parameter & Activation & Forward & Backward \\\\ \\hline RingAttention & w/o & \\(4HZd\\) & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+\\frac{BNH}{G}\\) & \\multirow{2}{*}{\\(2BZNd\\)} & \\multirow{2}{*}{\\(6BZNd\\)} \\\\ RingAttention\\({}^{\\dagger}\\) & \\(-\\) & & & \\\\ \\hline Tensor Parallelism & w/o & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+BNH\\) & \\multirow{2}{*}{\\(4BZNd\\)} & \\multirow{2}{*}{\\(4BZNd\\)} & \\multirow{2}{*}{\\(4BZNd\\)} \\\\ Tensor Parallelism & w/ FlashATT & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+BNH\\) & & \\\\ \\hline BurstAttention & w/o & \\(4HZd\\) & \\(4\\frac{BZNd}{G}+\\frac{BZN^{2}}{G}+\\frac{BN}{(M/d)^{2}G^{2}}+\\frac{BNH}{G}\\) & \\multirow{2}{*}{\\(2BZNd\\)} & \\multirow{2}{*}{\\(3BZNd+2BZN\\)} \\\\ BurstAttention & w/ LAO & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 분산 주의 솔루션들의 오버헤드. \\ (G\\)은 장치 번호, \\(B\\)은 배치 크기, \\(N\\)은 시퀀스 길이, \\(Z\\)은 주의 헤드 수를 나타내고, \\(d\\)은 헤드당 히든 차원에 해당하며, \\(H\\)은 트랜스포머의 모델 차원을 나타내고, \\(M\\)은 장치 SRAM 크기를 나타낸다. \\(B\\)은 장치 SRAM 크기를 나타낸다. ({}^{\\dagger}\\)는 구현 관점에서 링어텐션이 \\(\\mathbf{K}\\)과 \\(\\mathbf{V}\\)을 두 개의 독립적인 커뮤니케이션 라운드로 분리하는 것을 플래시어텐션과 결합하여 효율성을 향상시킬 수 없다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '우리는 두 가지 구성으로 실험을 수행한다: 하나는 PCI-E를 통해 연결된 8개의 A100 GPU가 장착된 단일 노드를 포함하고, 다른 하나는 600 Gb/s RoCE 네트워크로 상호 연결된 동일한 8개의 A100 GPU 구성을 갖는 4개의 동일한 노드를 포함하는 분산 설정이다. 실험에서는 70억 개의 매개변수(7b)를 갖는 LLaMA-2와 130억 개의 매개변수(13b)를 갖는 LLaMA-2의 두 가지 LLMs 설정을 채택한다(Touvron et al., 2023b). 우리의 실험은 다음과 같은 방법으로 구성된다.\n' +
      '\n' +
      '(1) **TP**, 텐서 병렬성(Narayanan et al., 2021)을 의미하며, 훈련과 추론의 단계에서 공통적으로 사용되는 분산 전략이다. 여기서 우리는 TP를 상세한 통신 연산에 기초하여 **TP(메가트론 V1)**와 **TP(메가트론 V3)**로 분류한다(메가트론 V1은 올-리듀스 연산을 사용하고, 메가트론 V3은 올-게더 및 리듀스-산란 연산의 조합을 사용한다). (2) **TP w/ FlashAttention**, 이는 FlashAttention(Dao et al., 2022)과 텐서 병렬성을 강한 베이스라인으로 결합한 것이다. ** 이것은 현재의 LLM 사전 훈련 및 추론에서 일반적으로 사용되는 전략임에 유의한다.**(3) 전형적인 시퀀스 병렬성 기준선인 RingAttention**. (4) **BurstAttention**, 우리의 분산 주의 방법은 GAO와 LAO 전략을 모두 포함한다. (5) **BurstAttention w/o LAO**, 여기서 절제 연구를 위한 LAO 전략을 제거한다. (6) **BurstAttention+ZeRO**, 여기서 우리는 ZeRO(Rajbhandari et al., 2020) 기법을 채택하여 디바이스들에 걸친 모델 파라미터들을 샤드함으로써 BurstAttention의 메모리 오버헤드를 최적화한다.\n' +
      '\n' +
      '앞서 언급했듯이 데이터 병렬성과 파이프라인 병렬성은 긴 시퀀스 처리 비용을 효과적으로 줄일 수 없으며, 이를 베이스라인으로 사용하지 않는다. 사실, 우리는 긴 시퀀스 주의를 위해 데이터 병렬성과 파이프라인 병렬성을 적응시키기 위한 몇 가지 실험을 수행하지만, 불행히도 이 두 병렬성 방법은 극도로 긴 시퀀스를 처리할 수 없다. 파일럿 실험에서 데이터 병렬 또는 파이프라인 병렬을 직접 채택하면 링어텐션 및 TP보다 훨씬 짧은 8192보다 짧은 시퀀스만 처리할 수 있다.\n' +
      '\n' +
      '우리의 실험은 특정 주의 마스킹 메커니즘에 특별히 초점을 맞추지 않는다. 그러나 플래시 어텐션과 텐서 병렬성(Megatron V3)과 같은 비교 방법에 대해서는 이 실험에서 인과적 구현을 채택한다. 이는 우리의 기준선이 인과적 주의 구조 때문에 주의 계산의 절반을 우회할 수 있음을 의미한다. 우리는 커뮤니케이션이 실험 환경에서 병목 현상으로 남아 있기 때문에 이 접근법이 미미한 개선만 가져온다는 것을 관찰한다. 특히, BurstAttention을 구현함에 있어서, 관측된 성능 이득의 핵심 요소인 통신에 의해 계산이 중복된다. 이러한 구별은 우리의 방법이 이점을 보여주는 맥락과 특정 조건을 이해하는 데 중요하다.\n' +
      '\n' +
      '### Inference Latency\n' +
      '\n' +
      '이 섹션에서는 추론 프로세스에서 제1 토큰(즉, 제1 토큰 레이턴시)을 생성하는 데 필요한 레이턴시에 중점을 둔다. 추론 인코딩 과정에서 긴 시퀀스 어텐션 연산이 주로 존재하기 때문에 첫 번째 토큰 생성 시간에 집중한다. 제1 토큰 레이턴시는 후속 토큰을 생성하는 레이턴시보다 훨씬 높기 때문에, 제1 토큰 레이턴시는 따라서 현존하는 작업들이 최적화하고자 하는 가장 중요한 타겟들 중 하나가 된다.\n' +
      '\n' +
      'ChatGPT와 같은 실시간 AI 서비스에서 시스템의 응답성은 사용자 경험에 상당한 영향을 미치며, 이러한 애플리케이션은 일반적으로 응답성을 향상시키기 위해 스트리밍 방식으로 결과를 출력한다. 제1 토큰 레이턴시가 가장 길기 때문에, 제1 토큰 레이턴시는 이러한 스트리밍 시나리오에서 모델의 인지된 응답성 및 효율성에 직접적으로 영향을 미친다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Sequence Length & 4,096 & 8,192 & 16,384 & 32,768 & 65,536 & 131,072 & 262,144 \\\\ \\hline RingAttention & 0.42\\(\\pm\\)0.01 & 0.87\\(\\pm\\)0.01 & 2.00\\(\\pm\\)0.01 & 5.13\\(\\pm\\)0.05 & OOM & OOM & OOM \\\\ TP(Megatron V1) w/ Flash & 0.67\\(\\pm\\)0.01 & 1.29\\(\\pm\\)0.01 & 2.58\\(\\pm\\)0.01 & 5.27\\(\\pm\\)0.01 & 11.63\\(\\pm\\)0.02 & 27.54\\(\\pm\\)0.01 & 71.52\\(\\pm\\)0.06 \\\\ TP(Megatron V3) w/ Flash & 0.73\\(\\pm\\)0.02 & 1.36\\(\\pm\\)0.01 & 2.68\\(\\pm\\)0.01 & 5.67\\(\\pm\\)0.01 & 12.25\\(\\pm\\)0.01 & 28.73\\(\\pm\\)0.03 & 75.52\\(\\pm\\)0.05 \\\\ BurstAttention w/o LAO & 0.46\\(\\pm\\)0.01 & 0.88\\(\\pm\\)0.01 & 1.79\\(\\pm\\)0.01 & 3.88\\(\\pm\\)0.01 & 10.78\\(\\pm\\)0.01 & OOM & OOM \\\\ BurstAttention & **0.44\\(\\pm\\)0.01** & **0.84\\(\\pm\\)0.01** & **1.68\\(\\pm\\)0.01** & **3.27\\(\\pm\\)0.01** & **6.49\\(\\pm\\)0.01** & **16.01\\(\\pm\\)0.01** & **49.32\\(\\pm\\)0.11** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: LLaMA-7b 추론(들)의 제1 토큰 레이턴시.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Sequence Length & 4,096 & 8,192 & 16,384 & 32,768 & 65,536 & 131,072 & 262,144 \\\\ \\hline RingAttention & 0.66\\(\\pm\\)0.01 & 1.36\\(\\pm\\)0.01 & 3.08\\(\\pm\\)0.01 & 7.98\\(\\pm\\)0.02 & OOM & OOM & OOM \\\\ TP(Megatron V1) w/ Flash & 1.05\\(\\pm\\)0.01 & 2.01\\(\\pm\\)0.01 & 4.03\\(\\pm\\)0.01 & 8.41\\(\\pm\\)0.01 & 18.56\\(\\pm\\)0.02 & 44.39\\(\\pm\\)0.04 & OOM \\\\ TP(Megatron V3) w/ Flash & 1.07\\(\\pm\\)0.01 & 2.09\\(\\pm\\)0.01 & 4.20\\(\\pm\\)0.01 & 8.76\\(\\pm\\)0.01 & 19.06\\(\\pm\\)0.06 & 45.46\\(\\pm\\)0.03 & 119.03\\(\\pm\\)0.04 \\\\ BurstAttention w/o LAO & 0.72\\(\\pm\\)0.01 & 1.39\\(\\pm\\)0.01 & 2.77\\(\\pm\\)0.05 & 5.99\\(\\pm\\)0.01 & 16.95\\(\\pm\\)0.01 & OOM & OOM \\\\ BurstAttention & **0.69\\(\\pm\\)0.01** & **1.40\\(\\pm\\)0.05** & **2.57\\(\\pm\\)0.03** & **5.08\\(\\pm\\)0.02** & **9.92\\(\\pm\\)0.01** & **25.91\\(\\pm\\)0.01** & **78.80\\(\\pm\\)0.07** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LLaMA-13b 추론(들)의 제1 토큰 레이턴시.\n' +
      '\n' +
      '표 2와 표 3에 나타난 바와 같이 텐서 병렬성과 비교하여 긴 서열을 추론하는 데 서열 병렬성 방법이 더 적합함을 알 수 있다. RingAttention 방법과 비교하여, GAO를 사용함으로써 BurstAttention은 더 긴 시퀀스를 지원할 수 있다. LAO를 추가로 사용함으로써 버스트 어텐션은 더 많은 레이턴시 개선을 달성하고 훨씬 더 긴 시퀀스를 지원할 수 있다. TP(Megatron V3)가 TP(Megatron V1)보다 메모리 효율이 높지만 TP(Megatron V1)에 의해 사용되는 올-리듀스 연산이 TP(Megatron V3)에 의해 사용되는 리듀스-산란 및 올-게더 연산보다 더 잘 최적화된다는 점에 유의한다. 실제 추론에서 TP(메가트론 V1)는 TP(메가트론 V3)보다 약간 빠르다. TP(메가트론 V3)는 TP(메가트론 V1)와 비슷한 시간을 가지지만 메모리 효율이 더 우수하기 때문에 후속 실험에서 주로 TP(메가트론 V3)와 방법을 비교한다.\n' +
      '\n' +
      '### Training Performance\n' +
      '\n' +
      'LLM들을 트레이닝하기 위해, 배치가 2 내지 400만 개의 토큰을 갖도록 요구되며, 그렇지 않으면, 모델 성능이 저하될 수 있으며, 즉, 시퀀스 길이가 길수록 배치 크기는 작아진다. 이로 인해, 여러 GPU가 하나의 예를 함께 처리해야 할 수 있다. 예를 들어, 128-레이어 GPT-3를 트레이닝하기 위해 2048개의 GPU를 사용하는 경우, 시퀀스 길이는 4096이고, 배치 크기는 1024이고, 데이터 병렬성은 16이고, 파이프라인 병렬성은 32이고, 텐서 병렬성은 4이다. 이 시나리오에서, 최적의 설정은 마이크로-배치 크기가 1인 64개의 마이크로-배치들로 배치를 분할하는 것이다. 이 경우, 동일한 텐서 병렬성 그룹 아래의 4개의 GPU들은 하나의 데이터를 함께 프로세싱하기 위해 불가피하게 요구된다. 이를 고려하여 실험 편의를 위해 배치 크기를 1로 고정하고 입력 시퀀스 길이를 1K에서 32K로 변경한다.\n' +
      '\n' +
      '그림 1(a)에서 알 수 있듯이 텐서 병렬성은 긴 시퀀스의 처리를 개선하기 위해 FlashAttention을 채택하지만, RingAttention과 BurstAttention은 긴 시퀀스를 처리할 때 텐서 병렬성보다 학습 시간이 더 좋다. LLM을 훈련하기 위해 텐서 병렬성을 사용하는 기존 작업들이 일반적으로 2048에서 4096 사이의 훈련 길이를 설정하는 것도 이 때문이다. BurstAttention과 비교하여 RingAttention은 너무 많은 중간 상태들을 저장하기 때문에 시퀀스 길이에 제한을 받지만 BurstAttention은 가장 긴 입력 길이를 지원할 수 있다. 반면 LAO가 없는 버스트어텐션은 Ring어텐션과 텐서 병렬성과 비슷한 훈련시간 경향을 보인다.\n' +
      '\n' +
      '그림 3에서 버스트 주의는 시퀀스가 128K보다 길면 거의 2.0\\(\\times\\)의 속도 향상을 달성한다. 또한 BurstAttention과 ZeRO 최적화를 결합하면 메모리 효율이 크게 향상됩니다. 버스트어텐션+ZeRO는 추가적인 통신 오버헤드가 거의 없지만, 버스트어텐션+ZeRO는 여전히 메가트론 V3에 필적하는 메모리 효율성을 달성하고 메가트론 V3보다 멀티노드 및 싱글노드 셋업 모두에서 우수한 속도를 보여준다. 이는 버스트어텐션이 현재의 최적화로 인해 메가트론 V3와 같은 메모리 효율적인 경쟁자와 직면하더라도 속도 측면에서 더 효율적인 솔루션을 제공한다는 것을 시사한다.\n' +
      '\n' +
      '### Scaling Ability\n' +
      '\n' +
      '본 절에서는 BurstAttention의 스케일링 능력을 추가로 검증한다. 그림 3(a)에서 배치 크기를 1로 설정하고 시퀀스 길이를 65,536으로 설정한 다음 GPU 수가 증가함에 따라 대기 시간 변화를 평가한다. 그림과 같이 single-GPU 시나리오에서 LAO를 갖는 BurstAttention은 FlashAttention과 동일하며, FlashAttention을 이용하여 추론 지연시간이 baseline과 동등하다. 텐서 병렬성은 배치 크기가 증가함에 따라 통신 오버헤드로 인해 GPU 수가 4개에서 8개로 증가할 때 지연 시간을 더 줄일 수 없지만 BurstAttention은 더 나은 스케일링 경향을 달성할 수 있다. 링어텐션은 각 레이어에 대한 \\(\\frac{BZX^{*}}{G})\\) 메모리를 저장해야 하는데, 이는 매우 크고 8개의 GPU에 샤딩된 GPU에도 들어갈 수 없다. 그림 3(b)에서 우리는 배치 크기가 증가함에 따라 훈련 처리량 변화를 평가하기 위해 시퀀스 길이를 4096으로 고정하고 GPU 수를 8로 고정한다. 실험 결과는 BurstAttention이 더 큰 배치 크기를 지원할 수 있음을 보여주며, 훈련 시나리오에서 배치 크기가 증가함에 따라 처리량이 증가함을 보여준다.\n' +
      '\n' +
      '그림 2: 8\\(\\times\\)A100에서 LLaMA-7b의 훈련 시간과 기억.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      'GPT(Brown et al., 2020; Ouyang et al., 2022), LLaMA(Touvron et al., 2023a;b), PaLM(Chowdhery et al., 2022; Anil et al., 2023)과 같은 트랜스포머 기반 LLM들은 큰 성공을 달성하였다(Han et al., 2021; Bommasani et al., 2021; Zhao et al., 2023). 이러한 LLM의 성공에도 불구하고 여전히 효율성 문제에 직면해 있다. 하나는 이러한 모델의 크기가 계속 증가함에 따라 훈련 및 추론과 관련된 시간과 메모리 비용이 병목 현상이 되었다는 것이다. 또 다른 하나는 트랜스포머 아키텍처의 2차 주의 계산 복잡성으로 인해 이러한 LLM이 긴 시퀀스를 처리하기 어렵다는 것이다. 지금까지 다양한 병렬화 전략(Valiant, 1990; Huang et al., 2019; Rajbhandari et al., 2020; Narayanan et al., 2021) 및 메모리 최적화 전략(Ren et al., 2021; Chen et al., 2016; Korthikanti et al., 2023)은 모델 크기 성장에 따른 병목 현상을 잘 해결하였으나, 시퀀스 성장에 따른 효율성 문제를 해결하기에는 여전히 어려움이 있다.\n' +
      '\n' +
      'LLM이 더 긴 시퀀스를 더 효율적으로 처리할 수 있도록 하기 위해 몇 가지 주의 솔루션이 제시되었다. Korthikanti et al. (2023)은 순방향 패스 동안 어텐션 소프트맥스 로짓들을 저장하는 것을 피하기 위해 선택적 활성화 재계산(selective activation recomputation)을 채택하고, 이후 역방향 패스 동안 이러한 로짓들을 재계산하여 역전파를 위한 계산 그래프를 구축함으로써, 긴 시퀀스들을 처리하기 위한 어텐션 모듈들의 메모리 오버헤드를 상당히 감소시킨다. Rabe and Staats(2021)는 블록 레벨에서 어텐션 모듈의 계산을 정형화하고 디바이스 내의 각 스레드 블록이 서브시퀀스의 어텐션 계산을 처리하도록 하여 임시 메모리 소모를 더욱 감소시키고 시퀀스 길이에 대한 로그 메모리 복잡도를 달성한다. 이러한 작업들에 기초하여, Dao et al. (2022)는 플래시 어텐션을 도입하며, 이는 더 빠른 속도를 위해 디바이스들에서 SRAM의 빠른 I/O 능력들을 활용하는 어텐션 모듈들의 CUDA 구현이다. 플래시 어텐션은 HBM에서 I/O 복잡도 분석을 도입하고 I/O 비용을 최소화함으로써 어텐션 알고리즘을 최적화하여 어텐션 최적화에 대한 새로운 관점을 제공한다.\n' +
      '\n' +
      '위의 솔루션은 단일 장치를 사용하여 긴 시퀀스 주의 문제를 최적화하는 데 초점을 맞추지만, 단일 장치의 성능의 한계로 인해 여전히 극도로 긴 시퀀스를 처리하는데 어려움을 겪고 있다. 따라서 일부 노력은 분산 군집을 사용하여 이러한 긴 서열 문제를 해결하는 것을 목표로 했다. 일반적인 병렬 전략을 채택하는 방법은 데이터 병렬(Valiant, 1990), 텐서 병렬(Narayanan et al., 2021), 파이프라인 병렬(Huang et al., 2019), 제로 리던던시 최적화기(Rajbhandari et al., 2020; Ren et al., 2021) 등 가장 간단한 방법이다. 분산 클러스터를 사용하여 긴 시퀀스를 더 잘 처리하기 위해, Li 등(2021)은 시퀀스 병렬화 방법 RingAttention을 제안하며, 이는 시퀀스 차원에 따라 다수의 디바이스들에 걸쳐 어텐션 모듈들의 계산 및 메모리 오버헤드를 분할한다.\n' +
      '\n' +
      '그림 4: 다양한 GPU 번호와 배치 크기에 대한 스케일링 능력.\n' +
      '\n' +
      '그림 3: 32\\(\\times\\)A100에서 LLaMA-7b의 훈련 시간과 기억.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '* Rabe and Staats(2021) Rabe, M. N. and Staats, C. Self-attention need \\(o(n^{2})\\) memory. _ arXiv preprint arXiv:2112.05682_, 2021.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limit of transfer learning with unified text-to-text transformer. _ The Journal of Machine Learning Research_, 21:5485-5551, 2020.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. ZeRO: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화. In _Proceedings of SC_, 2020.\n' +
      '* Ren et al. (2021) Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. ZeRO-Offload: 10억 규모의 모델 훈련 민주화. In _Proceedings of ATC_, pp. 551-564, 2021.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. -A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLAMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. LLAMA 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Valiant(1990) Valiant, L. G. 병렬 연산을 위한 브리징 모델. _ Communications of the ACM_, pp. 103-111, 1990.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention이면 된다. In _Proceedings of NeurIPS_, 2017.\n' +
      '* Wang et al. (2020) Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _ arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Winata et al. (2020) Winata, G. I., Cahyawijaya, S., Lin, Z., Liu, Z., and Fung, P. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In _Proceedings of ICASSP_, pp. 6144-6148, 2020.\n' +
      '* Zhao et al. (2023) Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. _ arXiv preprint arXiv:2303.18223_, 2023.\n' +
      '\n' +
      '## 부록 이중 버퍼를 이용한 버스트 어텐션 알고리즘\n' +
      '\n' +
      '```\n' +
      '1:\n' +
      '2: \\(i\\)번째 장치에서 행렬 \\(\\mathbf{Q}_{i},\\mathbf{K}_{i},\\mathbf{V}_{i}\\in\\mathbb{R}^{\\frac{N}{N}\\times d}\\)\n' +
      '3:Initialize \\(\\mathbf{Q}_{i}=(0)_{\\frac{N}{N}\\times d}\\in\\mathbb{R}^{\\frac{N}{N}\\times d},l_{ i}=(0)_{\\frac{N}{N}\\in\\mathbb{R}^{\\frac{N}{N}}},m_{i}=(-\\infty)_{\\frac{N}{N}\\in\\mathbb{R}^{\\frac{N}{N}}}}\\in\\mathbb{R}^{\\frac{N}{N}}}}}}}}\n' +
      '4: \\(\\mathbf{K}_{i}\\)으로 Buffer \\(K_{buf}\\), \\(\\mathbf{V}_{i}\\)으로 Buffer \\(V_{buf}\\)을 초기화한다.\n' +
      '5:for\\(j=1\\) to \\(\\mathbf{G}\\)do\n' +
      '6:if\\(j\\)!=then\n' +
      '7: \\(K_{buf},V_{buf}\\)으로부터 \\(K_{j},V_{j}\\); {Wait communication thread\'s job finished}\n' +
      '8:endif\n' +
      '9:AsyncCommunicationCall:\n' +
      '10 : 비동기 통신 스레드 초기화\n' +
      '11: Let \\(\\text{Buf}=(K_{buf},V_{buf})\\)\n' +
      '12: Buf를 다음 장치로 비동기 전송하고 이전 장치로부터 Buf를 회수\n' +
      '13:\\(\\mathbf{S}_{i,j}=\\mathbf{Q}_{i}\\mathbf{K}_{j}^{T}\\);\n' +
      '14:\\(m_{i,j}=\\text{rowmax}(\\mathbf{S}_{i,j})\\);\n' +
      '15:\\(\\mathbf{P}_{i,j}=\\text{exp}(\\mathbf{S}_{i,j})\\);\n' +
      '16:\\(l_{i,j}=\\text{rowsum}(\\mathbf{P}_{i,j})\\);\n' +
      '17:\\(\\mathbf{O}_{i,j}=\\mathbf{P}_{i,j}\\mathbf{V}_{j}\\); {The end of the forward pass of local attention operations.}\n' +
      '18:\\(m_{\\text{new}}\\leftarrow\\max\\left\\{m_{i},m_{i,j}\\right\\}\\);\n' +
      '19:\\(\\mathbf{l}_{i}=e^{m_{i}-m_{\\text{new}}}l_{i}+e^{m_{i,j}-m_{\\text{new}}}l_{i,j}\\);\n' +
      '20:\\(\\mathbf{O}_{i}=e^{m_{i}-m_{\\text{new}}}\\mathbf{O}_{i}+e^{m_{i,j}-m_{\\text{new}}} \\mathbf{O}_{i,j}\\);\n' +
      '21:\\(m_{i}=m_{\\text{new}}\\);\n' +
      '22:endfor\n' +
      '23:\\(\\mathbf{O}_{i}=\\text{diag}(l_{i})^{-1}\\mathbf{O}_{i}\\);\n' +
      '24:\\(lse_{i}=m_{i}+\\log l_{i}\\);\n' +
      '25:if\\(j\\)!=then\n' +
      '26: \\(dQ_{buf},dK_{buf},dV_{j}\\)으로부터 Get\\(dQ_{j},dK_{buf},dV_{buf}\\); {Wait communication thread\'s job finished}\n' +
      '27:endif\n' +
      '28:AsyncCommunicationCall:\n' +
      '29 : 비동기 통신 스레드 초기화\n' +
      '30: Let \\(\\text{Buf}=(\\mathbf{Q}_{buf}\\mathbf{d}\\mathbf{Q}_{buf},\\mathbf{d}\\mathbf{O}_{buf},D_{buf},lse_{buf})\\)\n' +
      '31: Buf를 다음 장치로 전송하고 이전 장치로부터 새로운 Buf를 회수하는 단계;\n' +
      '32:\\(\\mathbf{S}_{j,i}=\\mathbf{Q}_{j}\\mathbf{K}_{j}^{T}\\);{The backward pass of local attention operations (w/o LAO).\n' +
      '33:\\(\\mathbf{P}_{j,i}=\\text{exp}(\\mathbf{S}_{j,i}-lse_{j})\\);\n' +
      '34:\\(\\mathbf{dV}_{i}=\\mathbf{dV}_{i}+\\mathbf{P}_{i,i}^{T}\\mathbf{d}\\mathbf{O}_{i}\\);\n' +
      '35:\\(\\mathbf{dP}_{j,i}=\\mathbf{d}\\mathbf{O}_{j}\\mathbf{V}_{i}^{T}\\);\n' +
      '36:\\(\\mathbf{dS}_{j,i}=\\mathbf{P}_{j,i}\\circ(\\mathbf{dP}_{j,i}-D_{j})\\);\n' +
      '37:\\(\\mathbf{dK}_{i}=\\mathbf{dK}_{i}+\\mathbf{dS}_{j,i}^{T}\\mathbf{Q}_{j}\\);\n' +
      '38:\\(\\mathbf{dQ}_{j}=\\mathbff{dQ}_{j}+\\mathbf{dS}_{j,i}\\mathbf{K}_{i}\\; {The end of backward pass of local attention operations.}\n' +
      '39:endfor\n' +
      '40:\\(\\mathbf{dQ}_{G},\\mathbf{dK}_{G},\\mathbf{dV}_{G}\\);\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 겹침이 있는 GAO의 순방향 패스\n' +
      '\n' +
      '## 부록 B 한 트랜스포머 블록에서의 텐서 병렬성 런타임 분석\n' +
      '\n' +
      '**정리 B.1**:_메가트론-V3 프레임워크 내에서 텐서 병렬성(TP)을 사용하는 트랜스포머 블록에서, 총 런타임 \\(T\\)은 모든 수집 및 감소산란 연산에 대한 통신 시간, 그리고 장치에 분산되어 있는 주의(attn) 및 피드포워드(ffn) 모듈에 대한 계산 시간의 합에 의해 결정된다._\n' +
      '\n' +
      '**Definition B.2** (Input Tensor and Cluster Configuration) : 입력 텐서 \\(x\\)는 차원 \\((B,N,Z^{\\prime},d)\\), 여기서 \\(B\\)은 배치 크기, \\(N\\)은 시퀀스 길이, \\(Z^{\\prime}\\)은 장치당 파티션 헤드의 수, \\(d\\)은 주의 헤드당 숨겨진 차원을 갖도록 한다. 클러스터 대역폭 \\(b\\)은 모든 \\(G\\) 장치에 걸쳐 균일하다고 가정한다.\n' +
      '\n' +
      '**Lemma B.3**(Communication Time).: _TP에서 각 올-게더 또는 리듀스-스캐터 동작에 대한 시간\\(t_{\\text{comm}}\\)은 다음과 같다.\n' +
      '\n' +
      '\\[t_{\\text{comm}}=\\frac{(B\\times N\\times Z^{\\prime}\\times d)\\times M\\times(G-1)}{b\\times G},\\]\n' +
      '\n' +
      '\\(M\\)은 하나의 텐서 엘리먼트를 저장하는데 필요한 비트 수를 나타낸다.__\n' +
      '\n' +
      '**Proposition B.4**(Runtime Calculation).: _TP is_\n' +
      '\n' +
      '\\[T=4\\times t_{\\text{comm}}+\\frac{T_{\\text{attr}}}{G}+\\frac{T_{\\text{fft}}}{G},\\]\n' +
      '\n' +
      '_accounting for two all-gather and two reduce-scatter operations, and parallelized computation times for the attention(attn) and feedforward(ffn) modules._\n' +
      '\n' +
      '_Remark B.5_.: 이 분석은 트랜스포머 블록에서 완전한 주의 메커니즘의 사용을 가정한다. 계산 및 통신 복잡성을 변경할 수 있는 희소, 근사 또는 인과적 주의 방법을 설명하지 않는다.\n' +
      '\n' +
      '## 부록 C 런타임 분석을 통한 한 개의 변압기 블록에서의 버스트 어텐션 분석\n' +
      '\n' +
      '**정리 C.1**.: _버스트 어텐션 프레임워크에서, 주어진 트랜스포머 블록에 대한 총 런타임은 어텐션 및 피드포워드 모듈 모두에 대한 통신 및 계산 시간에 의해 영향을 받는다. 런타임은 순방향 및 역방향 통과 모두에서 비대칭 통신 프로세스를 설명한다._\n' +
      '\n' +
      '**Definition C.2** (Input Tensor and Cluster Configuration) : 입력 텐서 \\(x\\)는 차원 \\((B,N^{\\prime},Z,d)\\)을 갖도록 한다. 여기서 \\(B\\)은 배치 크기, \\(N^{\\prime}\\)은 장치당 분할된 시퀀스 길이, \\(Z\\)은 장치당 주의 헤드 수, \\(d\\)은 주의 헤드당 숨겨진 차원이다. 클러스터의 균일한 대역폭은 모든 \\(G\\) 장치에 걸쳐 \\(b\\)이며, \\(d_{ffn}\\)은 피드포워드 층의 중간 차원을 나타낸다.\n' +
      '\n' +
      '**Lemma C.3**(Activation Communication Time in BurstAttention): _In BurstAttention에는 키(\\(K\\))와 값(\\(V\\)) 활성화에 대한 두 개의 링형 통신과 쿼리(\\(Q\\))에 대한 5개의 그래디언트(\\(d_{Q}\\)), 어텐션 출력에 대한 그래디언트(\\(dO\\)), 백워드 패스 동안 감소 변수(\\(D\\), \\(lse\\))가 있다. 상기 순방향 및 역방향 프로세스들에서 이러한 활성화들에 대한 통신 시간들은:_\n' +
      '\n' +
      '[t_{\\text{comm\\_attn\\_f} =\\frac{2\\times B\\times N^{\\prime}\\times Z\\times M\\times(G-1)}{b\\times G},\\] \\[t_{\\text{comm\\_attn\\_b} =\\frac{(3\\times B\\times N^{\\prime}\\times Z\\times d+2\\times B\\times N^{\\prime}\\times Z\\times M\\times(G-1)}{b\\times G},\\]\n' +
      '\n' +
      '**Lemma C.4** (Weight Communication Time in BurstAttention): _In BurstAttention\'s attention module, four linear layer with weights of dimension \\(H\\times Z\\times d\\) 피드포워드 모듈은 차원\\(H\\times d_{ffn}\\)과 차원\\(d_{ffn}\\times H\\)을 갖는 두 개의 선형 층을 갖는다. 이들 레이어의 가중치에 대한 통신 시간은:_\n' +
      '\n' +
      '\\[t_{\\text{comm\\_weights}}=\\frac{(4\\times H\\times Z\\times d+2\\times H\\times d_{ffn})\\times M\\times(G-1)}{b\\times G},\\\n' +
      '\n' +
      '**Proposition C.5**(Runtime Calculation in BurstAttention): __ BurstAttention framework에 대한 총 런타임은:_\n' +
      '\n' +
      '\\[T_{\\text{attn\\_f}}=\\max(T_{\\text{attn\\_f}},t_{\\text{comm\\_attn\\_f}})+\\max(T_{ \\text{attn\\_b}},t_{\\text{comm\\_attn\\_b}})+T_{\\text{fft}}+t_{\\text{comm\\_weights}},\\]\n' +
      '\n' +
      '\\(T_{\\text{attn\\_f}}\\) 및 \\(T_{\\text{attn\\_b}}\\)는 각각 어텐션 모듈의 순방향 및 역방향 프로세스에 대한 계산 시간을 나타내며, \\(T_{\\text{fft}}\\)는 피드포워드 모듈의 런타임이다.\n' +
      '\n' +
      '_Remark C.6_.: TP의 분석과 마찬가지로, 이 분석은 계산 및 통신 복잡성을 변경할 수 있는 희소, 근사 또는 인과 주의 방법을 설명하지 않는다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>