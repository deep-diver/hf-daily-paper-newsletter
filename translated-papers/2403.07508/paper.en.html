<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MoAI: Mixture of All Intelligence\n' +
      '\n' +
      'for Large Language and Vision Models\n' +
      '\n' +
      'Byung-Kwan Lee\n' +
      '\n' +
      'School of Electrical Engineering\n' +
      '\n' +
      'Korea Advanced Institute of Science and Technology (KAIST)\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      'Beomchan Park\n' +
      '\n' +
      'School of Electrical Engineering\n' +
      '\n' +
      'Korea Advanced Institute of Science and Technology (KAIST)\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      'Chae Won Kim\n' +
      '\n' +
      'School of Electrical Engineering\n' +
      '\n' +
      'Korea Advanced Institute of Science and Technology (KAIST)\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      'Yong Man Ro\n' +
      '\n' +
      'School of Electrical Engineering\n' +
      '\n' +
      'Korea Advanced Institute of Science and Technology (KAIST)\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The rise of large language models (LLMs) and instruction tuning has led to the current trend of instruction-tuned large language and vision models (LLVMs). This trend involves either meticulously curating numerous instruction tuning datasets tailored to specific objectives or enlarging LLVMs to manage vast amounts of vision language (VL) data. However, current LLVMs have disregarded the detailed and comprehensive real-world scene understanding available from specialized computer vision (CV) models in visual perception tasks such as segmentation, detection, scene graph generation (SGG), and optical character recognition (OCR). Instead, the existing LLVMs rely mainly on the large capacity and emergent capabilities of their LLM backbones. Therefore, we present a new LLVM, **M**ixture **of** **A**ll **I**ntelligence (**A** **MoAI**), which leverages auxiliary visual information obtained from the outputs of external segmentation, detection, SGG, and OCR models. MoAI operates through two newly introduced modules: _MoAI-Compressor_ and _MoAI-Mixer_. After verbalizing the outputs of the external CV models, the MoAI-Compressor aligns and condenses them to efficiently use relevant auxiliary visual information for VL tasks. MoAI-Mixer then blends three types of intelligence--(1) visual features, (2) auxiliary features from the external CV models, and (3) language features--utilizing the concept of Mixture of Experts. Through this integration, MoAI significantly outperforms both open-source and closed-source LLVMs in numerous zero-shot VL tasks, particularly those related to real-world scene understanding such as object existence, positions, relations, and OCR without enlarging the model size or curating extra visual instruction tuning datasets. Code is available in [https://github.com/ByungKwanLee/MoAI](https://github.com/ByungKwanLee/MoAI).\n' +
      '\n' +
      'Keywords:Large Language and Vision Models Mixture of Experts\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Combining large language models (LLMs) such as PaLM [13] and T5 [72] with instruction tuning datasets from Flan [81], Chung _et al._[15] has developed Flan-PaLM and Flan-T5 for instruction-tuned LLMs. These models leverage an expanded instruction tuning dataset covering various tasks, and have been furtherscaled up to enlarge their capacities, resulting in notable improvements in zero-shot performance across numerous language tasks.\n' +
      '\n' +
      'Alongside the success of the instruction-tuned LLMs, several visual instruction tuning datasets [11, 17, 59, 80, 4] have been meticulously curated to enhance zero-shot vision language (VL) performances in large language and vision models (LLVMs). Furthermore, concerted efforts have been made to substantially scale up LLVMs [1, 4, 58, 80], aiming for strong zero-shot performances in VL datasets. With the extension of visual instruction tuning datasets and the scaling up of LLVMs, open-source LLVMs [1, 4, 9, 11, 17, 27, 58, 59, 80, 87, 92] have been closing the gap in zero-shot VL performances compared to closed-source LLVMs such as GPT-4V [66, 67], Gemini-Pro [77], and Qwen-VL-Plus [4].\n' +
      '\n' +
      'However, current open-source LLVMs have not explicitly or fully leveraged detailed and comprehensive real-world scene understanding, relying mainly on the large capacity and emergent capabilities of their LLM backbones. Several studies in cognitive science and machine learning [20, 6, 23] argue that fundamental scene perception ability may stem from various cognitive functions, including recognizing object presence, determining their positions, identifying their states, understanding their relationships, extracting spatial scene layouts, and grasping non-object notions which may include written texts. Fortunately, these cognitive functions can be acquired from specialized computer vision (CV) models which have been researched and developed over decades for visual perception tasks such as segmentation [12, 35], detection [64, 93], scene graph generation (SGG) [83, 40], and optical character recognition (OCR) [21, 51].\n' +
      '\n' +
      'Shifting the focus from instruction-tuning to utilizing these external CV models is expected to enhance the real-world scene understanding of LLVMs, covering object existence, positions, relations, and OCR. Recognition of objects and their positions [46] can be facilitated by panoptic segmentation and open-world object detection models. For a more comprehensive understanding, involving object\n' +
      '\n' +
      'Figure 1: Comparing the scores and accuracies of numerous VL benchmarks for various open-source and closed-source LLVMs with those for **MoAI**.\n' +
      '\n' +
      ' states and relationships (_i.e.,_ compositional reasoning [20]), a scene graph generation (SGG) model is necessary. Moreover, text descriptions within an image as a non-object notion can be recognized through an OCR model.\n' +
      '\n' +
      'In light of this, we propose a new LLVM, **M**ixture **of** **A**ll **Intelligence ( **A** **MoAI**), which leverages auxiliary visual information obtained from various sources: (1) panoptic segmentation [12], (2) open-world object detection [64], (3) SGG [83], and (4) OCR [21] models. To effectively leverage this information, we introduce two new modules: _MoAI-Compressor_ and _MoAI-Mixer_. The MoAI-Compressor aligns and condenses the verbalized outputs of the external CV models into auxiliary visual information, enabling the efficient use of relevant information for VL tasks. Subsequently, _MoAI-Mixer_ blends three types of intelligence--(1) visual features, (2) auxiliary features from external CV models, and (3) language features--into a cohesive whole.\n' +
      '\n' +
      'In constructing the MoAI-Mixer, we draw inspiration from the concept of Mixture of Experts (MoE) [73, 74, 65, 91]. Our challenge lies in seamlessly integrating original features (_i.e.,_ visual and language features) used in the multi-modal language model (MLM) of MoAI--an LLM backbone that takes visual tokens outputted by the visual encoder along with text tokens--with auxiliary features acquired from external CV models and the MoAI-Compressor. We em\n' +
      '\n' +
      'Figure 2: Comparing the scores and accuracies of dimensions related to real-world scene understanding in MME [26], SEED [49], MM-Bench [60], and MM-Vet [86] for validating capabilities of various LLVMs such as InstructBLIP [17], Qwen-VL [4], and LLaVA1.5 [57].\n' +
      '\n' +
      'ploy cross- and self-attention modules to construct six expert modules in the MoAI-Mixer, covering the three types of aforementioned intelligence. Furthermore, we utilize gating networks to determine the optimal combination of weights for these expert modules.\n' +
      '\n' +
      'By combining the MoAI-Compressor and MoAI-Mixer, MoAI effectively utilizes outputs from external CV models and mix three sources of intelligence, thereby enhancing its visual perception capabilities for tackling complex question answering tasks. As depicted in Fig. 2, our results demonstrate that MoAI has significantly outperformed in visual perception scores three strong LLVM baselines: InstructBLIP [17], Qwen-VL [4], LLaVA1.5 [57], even without additional curation of visual instruction tuning datasets or scaling up LLVMs. Furthermore, owing to its improved visual perception ability, MoAI exhibits potent zero-shot performances in VL tasks, surpassing closed-source LLVMs, as illustrated in Fig. 1. The success of MoAI is attributed to its utilization of diverse auxiliary visual information from external CV models and the integration of three intelligence types to effectively execute VL tasks. Our contribution can be summarized in two main aspects as follows:\n' +
      '\n' +
      '* We introduce a new large language and vision model, **MoAI**, which handles various auxiliary visual information from external CV models (_MoAI-Compressor_) and blends three types of intelligence (_MoAI-Mixer_).\n' +
      '* **MoAI** stands out for its exceptional visual perception ability in VL tasks, surpassing both open-source and closed-source LLVMs in zero-shot VL performances. This ability is achieved by considering detailed and comprehensive real-world scene understanding without requiring scaling up either the model size or dataset size.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'LLMs and LLVMs.LLMs have emerged alongside their competent generalization capability and the effectiveness of instruction tuning datasets. GPTs [70, 71, 7] played a crucial role in paving the way for LLMs by demonstrating strong zero-shot or few-shot performance across various language tasks, including text classification, question answering, machine translation, complex reasoning tasks, and so on. These generalization abilities of LLMs have been achieved by enormously increasing both model capacities and training datasets, as seen in works such as T5 [72], PaLM [13], OPT [88]. The progress in training methods and datasets further enhances the zero-shot generalization of LLMs, transitioning from large-scale pre-training datasets to instruction tuning datasets [15, 32, 68, 81]. Instruction tuning [81] enables LLMs to follow instructions in human natural language under complex real-world scenarios. Instruction-tuned LLMs, such as Flan-T5, Flan-PaLM [15], OPT-IML [32], and InstructGPT [68], clearly demonstrate the effectiveness of instruction tuning. Researchers have taken a step further by applying similar strategies to multimodal counterparts, LLVMs, which consist of a visual encoder and a backbone multimodal language model (MLM).\n' +
      '\n' +
      'For example, LLaVA [59] and ShareGPT4V [11] utilize GPT-4 [2] and GPT-4V [66, 67], respectively, to create visual instruction tuning datasets, while others [4, 17, 80] have also developed various visual instruction tuning datasets for their own unique objectives. However, the existing LLVMs have overlooked the detailed and comprehensive real-world scene understanding available from CV models with great advancements over the last decades. The CV models have been overshadowed by LLVMs with enlarged capacities and visual instruction tuning datasets in the era of LLVMs. From this perspective, MoAI highlights the effectiveness of utilizing auxiliary visual information obtained from external CV models, showing enhanced visual perception capabilities for VL benchmarks.\n' +
      '\n' +
      'Mixture of Experts.Jacobs _et al._[34] has first introduced the concept of Mixture of Experts (MoE) to machine learning, where separate networks called \'experts\' handle different segments of the input space, and each segment is guided to relevant experts by a gating network. This idea is further developed by deep MoE [22] where MoE layers are stacked in depth, and by conditional computation [5] where only a few experts are conditionally activated by a given input. In modern deep learning, Shazeer _et al._[74] integrates an MoE layer with LSTMs [30] where a gating network independently routes each token to selectively activated experts. This integration enhances performance in language modeling and machine translation tasks. Furthermore, Switch Transformers [24] merge an MoE layer and Transformers [79] by replacing a dense feed forward network (FFN) inside a Transformer layer with multiple experts and a gating network, paving a way to the successful use of MoE in Transformer-based LLVMs such as MoE-LLaVA [53]. The philosophy of MoE in deep learning is to enlarge model capacity without sacrificing computational efficiency [22, 24, 36, 42, 53, 74, 94]. On the other hand, we focus on a different yet fundamental aspect of MoE, where we intend that each expert is designed to specialize in a particular segment of input. While previous MoE methods do not explicitly assign roles to individual experts and instead expect specialization to emerge during optimization, MoAI designates cross- and self-attention modules as experts and learns them explicitly to mix information across modalities (_i.e.,_ visual, auxiliary, and language features). Specifically, MoAI facilitates pairs of (1) visual-auxiliary feature, (2) visual-language feature, (3) visual-visual feature, (4) language-auxiliary feature, (5) language-visual feature, and (6) language-language feature. Each pair is considered as a query-key pair for a respective cross- or self-attention module serving as experts, clarifying the fusion of information across diverse modalities.\n' +
      '\n' +
      '## 3 MoAI: Mixture of All Intelligence\n' +
      '\n' +
      'Model Architecture.As depicted in Fig. 3, MoAI consists of a vision encoder, a backbone multimodal language model (MLM) equipped with MoAI-Mixers, intermediate MLP connectors between the vision encoder and MLM, and a MoAI-Compressor which leverages four external computer vision (CV) models for panoptic segmentation [12], open-world object detection [64], scene graph generation (SGG) [83], and optical character recognition (OCR) [21]. MoAI-Compressor is introduced to process diverse auxiliary visual information acquired from the external CV models, where the CV model outputs are processed via verbalization as shown in Fig. 4 to make them aligned and interpretable to the MLM utilized in MoAI. In addition, MoAI-Mixer is further presented to efficiently harmonize original two features (_i.e.,_ visual and language features) with auxiliary features from the external CV models. The details of verbalization, MoAI-Compressor, and MoAI-Mixer will be explained in this section.\n' +
      '\n' +
      'Vision and Language BackboneCLIP-L/14 [69] is selected as the vision encoder, due to its guaranteed proficiency in image understanding aligned with text for vision language tasks [57, 58, 59, 11]. The MLM utilized in MoAI is based on InternLM-7B [78], which is a multilingual foundation model instruction-tuned by multilingual datasets with 1.6T tokens through a series of progressive pretraining phases and reinforcement learning from human feedback (RLHF) [14, 68, 76]. Two linear layers with GELU activation function [29] serve as the bridge connector between vision and language components, denoted by \'MLP\' in Fig. 3.\n' +
      '\n' +
      'VerbalizationSince a multimodal language model (MLM) is adopted to construct MoAI, we convert CV model outputs into natural language format in order to make them understandable to the MLM through a process called verbalization. Fig. 4 illustrates how the four CV model outputs undergo verbalization alongside the creation of auxiliary tokens semantically aligned to the MLM.\n' +
      '\n' +
      'A panoptic segmentation model enables us to distinguish foreground and background objects in an image at once. Furthermore, we can compute bounding box coordinates (_e.g.,_\\([x_{\\text{min}},y_{\\text{min}},x_{\\text{max}},y_{\\text{max}}]\\)) from the segmentation map. Consequently, verbalizing the outputs from panoptic segmentation (PS) entails serializing bounding box coordinates and their object names as explained in Fig. 4. These verbalized descriptions are then transformed into auxiliary to\n' +
      '\n' +
      'Figure 3: Overview of **MoAI** architecture. Compressed learnable tokens, the parameters of MoAI-Compressor and MoAI-Mixer are learned. ‘Vision’ represents vision encoder to embed visual features and ice/fire symbols represent the modules to freeze or learn. Note that, ‘Word Embed’ represents the word embedding dictionary of MLM.\n' +
      '\n' +
      'kens through the word embeddings of MLM. Additionally, to directly utilize the panoptic segmentation map, we use a vision encoder and an MLP connector in MoAI to generate locality-preserving auxiliary tokens. The generated auxiliary tokens are flattened and concatenated to those from serialized bounding boxes and their object names to form the final PS auxiliary tokens \\(A_{\\text{PS}}\\). They are\n' +
      '\n' +
      'Figure 4: Verbalization process of **MoAI** for external CV models: panoptic segmentation (PS), open-world object detection (OWOD), scene graph generation (SGG), and optical character recognition (OCR). Note that, ‘d’ denotes channel dimension of MLM, thus auxiliary tokens have equal channel dimension.\n' +
      '\n' +
      'concatenated in this manner so that the MLM of MoAI can associate them in a compatible way through contextualization. This procedure ensures the comprehensive conversion of visual information from PS into language information while preserving the spatial locality inherent in the panoptic segmentation map. Note that if the panoptic segmentation model fails to classify objects within the fixed number of panoptic object categories, for instance, those in MS-COCO 2017 [54] encompassing 133 object categories, the _unknown_ class is assigned.\n' +
      '\n' +
      'An open-world object detection model plays a role in detecting object classes missed by the panoptic segmentation model. This is because the panoptic segmentation model is trained on a specific dataset with a fixed number of object categories. Once the detection results are generated for an image, bounding box coordinates and their object names are verbalized according to the following template format: \'The image includes bounding boxes and their objects: {verbalized open-world object detection (OWOD) results}\'. Then, the results are transformed into OWOD auxiliary tokens \\(A_{\\text{OWOD}}\\) by the word embeddings of MLM. Similarly, the outputs of SGG and OCR models are verbalized, and corresponding auxiliary tokens \\(A_{\\text{SGG}}\\) and \\(A_{\\text{OCR}}\\) are generated, where we use the following verbalization templates: \'The image includes relationships between objects: {verbalized SGG results}\' and \'The image includes text descriptions: {verbalized OCR results}\', respectively.\n' +
      '\n' +
      'MoAI-CompressorAfter the verbalization of CV model outputs, four auxiliary tokens \\(A_{\\text{PS}}\\), \\(A_{\\text{OWOD}}\\), \\(A_{\\text{SGG}}\\), and \\(A_{\\text{OCR}}\\) are generated and injected into MoAI-Compressor, which borrows the structure of Perceiver Resampler [3]. All four auxiliary tokens \\([A_{\\text{PS}},A_{\\text{OWOD}},A_{\\text{SGG}},A_{\\text{OCR}}]\\) are concatenated before being fed into MoAI-Compressor along with a fixed number of learnable tokens \\(A_{\\text{input}}\\), whose outputs \\(A\\) are also fixed in length by the same number and represent the compressed and aligned auxiliary visual information, as formulated as follows:\n' +
      '\n' +
      '\\[A=\\text{MoAI-Compressor}([A_{\\text{PS}},A_{\\text{OWOD}},A_{\\text{SGG}},A_{ \\text{OCR}}]\\,,A_{\\text{input}}). \\tag{1}\\]\n' +
      '\n' +
      'Due to the variable length of concatenated auxiliary tokens across images and their substantial length after concatenation, MoAI-Compressor is designed to condense those tokens \\([A_{\\text{PS}},A_{\\text{OWOD}},A_{\\text{SGG}},A_{\\text{OCR}}]\\) with a relatively small fixed size of 64, generating \\(A\\in\\mathbb{R}^{d\\times 64}\\) where \\(d\\) represents the embedding dimension. These condensed tokens are then used to extract relevant information for VL tasks by MoAI-Mixer. This compression enhances computational efficiency.\n' +
      '\n' +
      'MoAI-Mixeris embedded in each MLM layer of MoAI. It receives auxiliary tokens \\(A\\) from MoAI-Compressor, visual features \\(I^{(l)}\\in\\mathbb{R}^{d\\times N_{I}}\\), and language features \\(L^{(l)}\\in\\mathbb{R}^{d\\times N_{L}}\\) where \\(l=0,1,\\cdots,N-1\\) denotes the layer index, \\(d\\) denotes the embedding dimension, \\(N_{I}\\) denotes the length of visual features, and \\(N_{L}\\) denotes that of language features. Normally, an MLM layer only consists of a Transformer decoder block TransDec\\({}^{(l)}\\) such that \\([I^{(l+1)},L^{(l+1)}]=\\text{TransDec}^{(l)}([I^{(l)},L^{(l)}])\\). In MoAI, an \\(l\\)-th MLM layer with MoAI-Mixer is for mulated as follows:\n' +
      '\n' +
      '\\[\\begin{split}[\\hat{I}^{(l)},\\hat{L}^{(l)}]&=\\text{ MoAI-Mixer}^{(l)}(A,I^{(l)},L^{(l)}),\\\\ \\\\ [I^{(l+1)},L^{(l+1)}]&=\\text{TransDec}^{(l)}(\\hat{I}^{(l) },\\hat{L}^{(l)}),\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\hat{I}^{(l)}\\) and \\(\\hat{L}^{(l)}\\) are mixed visual features and mixed language features. In each MoAI-Mixer, we design six expert modules that are either cross- or self-attention modules as illustrated in Fig. 5: three for visual features \\(I\\) and three for language features \\(L\\). Each of three expert modules for visual features outputs \\(I_{\\text{AUX}}\\), \\(I_{\\text{LANG}}\\), and \\(I_{\\text{SELF}}\\) where the capital letter indicates query features and the subscript indicates key/value features. Similarly, each of three expert modules for language features outputs \\(L_{\\text{AUX}}\\), \\(L_{\\text{IMG}}\\), and \\(L_{\\text{SELF}}\\). The cross-attention operation at the \\(l\\)-th layer is formulated as follows:\n' +
      '\n' +
      '\\[\\begin{split} I^{(l)}_{\\{\\text{AUX or LANG}\\}}&= \\text{CA}^{(l)}(q=I^{(l)},k=\\{A\\text{ or }L^{(l)}\\},v=k),\\\\ \\\\ L^{(l)}_{\\{\\text{AUX or IMG}\\}}&=\\text{CA}^{(l)}(q=L ^{(l)},k=\\{A\\text{ or }I^{(l)}\\},v=k).\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'In addition, the self-attention operation is formulated as \\(I^{(l)}_{\\text{SELF}}=\\text{SA}^{(l)}(I^{(l)})\\) and \\(L^{(l)}_{\\text{SELF}}=\\text{SA}^{(l)}(L^{(l)})\\). These six expert modules explicitly specialize in one of the following six distinct mixtures of intelligence: \\(I_{\\text{AUX}}\\), \\(I_{\\text{LANG}}\\), \\(I_{\\text{SELF}}\\), \\(L_{\\text{AUX}}\\), \\(L_{\\text{IMG}}\\), and \\(L_{\\text{SELF}}\\). When training the expert modules, we borrow the concept of LoRA [31] to reduce computational burden. Let\'s denote \\(W\\) as a general notation for a linear projection layer in a multi-head attention module [79], which can be \\(W^{q}\\), \\(W^{k}\\), \\(W^{v}\\), or \\(W^{o}\\). We decompose \\(W\\in\\mathbb{R}^{d\\times d}\\), not \\(\\Delta W\\) as in LoRA, into two linear layers \\(W_{A}\\in\\mathbb{R}^{d\\times r}\\) and \\(W_{B}\\in\\mathbb{R}^{r\\times d}\\) such that \\(W=W_{A}W_{B}\\). The hyperparameter \\(r\\) denotes the reduced dimension as illustrated in Fig. 6(a). Since computational burden of an attention module mainly comes from the high embedding dimension, usually \\(d=4096\\), such formulation of projection\n' +
      '\n' +
      'Figure 5: Illustrating MoAI-Mixer in MLM Layer of **MoAI**. In MoAI-Mixer, there are six expert modules to harmonize auxiliary features \\(A\\) and two original features (_i.e.,_ visual \\(I\\) and language \\(L\\) features).\n' +
      '\n' +
      'matrices significantly reduces computation. Moreover, the input query features are directly added to the output features so that mixture of intelligence occurs without altering the outputs of the previous MLM layer too much, stabilizing the optimization process with the frozen Transformer decoder blocks.\n' +
      '\n' +
      'First Training Step.We first train \\(A_{\\text{input}}\\), MoAI-Compressor, and MoAI-Mixer by using visual instruction tuning datasets [11, 57]. This step ensures that the six expert modules in MoAI-Mixer yield meaningful features to conduct VL tasks. To do so, we randomly choose outputs from one of three expert modules for visual and language features, respectively, as follows:\n' +
      '\n' +
      '\\[\\hat{I}^{(l)}=\\text{Sample}(I^{(l)}_{\\text{AUX}},I^{(l)}_{\\text{LANG}},I^{(l) }_{\\text{SELF}}),\\quad\\hat{L}^{(l)}=\\text{Sample}(L^{(l)}_{\\text{AUX}},L^{(l) }_{\\text{IMG}},L^{(l)}_{\\text{SELF}}). \\tag{4}\\]\n' +
      '\n' +
      'Then, they are injected into the transformer decoder block \\(\\text{TransDec}_{l}(\\hat{I}^{(l)},\\hat{L}^{(l)})\\). This sampling process aims for each expert module to produce meaningful features independently.\n' +
      '\n' +
      'Second Training Step.In this step, we extend the learning process beyond the parameters learned in the first training step. We learn two gating networks for each MoAI-Mixer, which comprises a single linear layer, each for visual and language features: \\(W_{\\text{Gating}_{I}}\\) and \\(W_{\\text{Gating}_{L}}\\in\\mathbb{R}^{d\\times 3}\\), illustrated in Fig. 6(b). The gating networks aim to output the best combination of weights for three expert modules for visual and language features each by using a linear layer and a softmax function as follows: \\(\\text{Softmax}(x^{\\mathsf{T}}W_{\\text{Gating}_{x}},\\text{dim=1})\\). Note that \\(x\\in\\mathbb{R}^{d\\times N_{x}}\\), where\n' +
      '\n' +
      'Figure 6: The structures of (a) expert modules and (b) gating networks for MoAI-Mixer. In (a), ‘\\(q\\)’, ‘\\(k\\)’, and ‘\\(v\\)’ denote query, key, and value, respectively, ‘\\(d\\)’ and ‘\\(r\\)’ explains channel dimension and reduced dimension, respectively.\n' +
      '\n' +
      '\\(x\\) is either the visual \\(I\\) or language \\(L\\) features and \\(N_{x}\\) is the length of features, resulting in \\(x^{\\mathsf{T}}W_{\\text{Gating}_{x}}\\in\\mathbb{R}^{N_{x}\\times 3}\\). Then, we split the softmax matrix into three weight vectors: \\(\\text{Softmax}(x^{\\mathsf{T}}W_{\\text{Gating}_{x}},\\text{dim=1})\\rightarrow[w_{ \\text{AUX}},w_{\\text{LANG}},w_{\\text{SELF}}]\\) where each weight has \\(\\mathbb{R}^{N_{x}}\\) dimension. The weights serve as confidence scores to determine whether to use information from each expert module. From the outputs of the gating networks, the propagation flow for the three sources of intelligence: \'AUX\', \'IMG\', \'LANG\' can be represented as follows:\n' +
      '\n' +
      '\\[\\begin{split}&[w_{\\text{AUX}},w_{\\text{LANG}},w_{\\text{SELF}}] \\leftarrow\\text{Softmax}({I^{(l)}}^{\\mathsf{T}}W_{\\text{Gating}_{I}},\\text{dim=1}), \\\\ &\\hat{I}^{(l)}=w_{\\text{AUX}}\\odot I^{(l)}_{\\text{AUX}}+w_{ \\text{LANG}}\\odot I^{(l)}_{\\text{LANG}}+w_{\\text{SELF}}\\odot I^{(l)}_{\\text{ SELF}}\\\\ &[w_{\\text{AUX}},w_{\\text{IMG}},w_{\\text{SELF}}]\\leftarrow\\text{ Softmax}({L^{(l)}}^{\\mathsf{T}}W_{\\text{Gating}_{L}},\\text{dim=1}),\\\\ &\\hat{L}^{(l)}=w_{\\text{AUX}}\\odot L^{(l)}_{\\text{AUX}}+w_{ \\text{IMG}}\\odot L^{(l)}_{\\text{IMG}}+w_{\\text{SELF}}\\odot L^{(l)}_{\\text{ SELF}},\\end{split} \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\odot\\) represents the element-wise product in each token. The gating networks for visual and language features are trained independently without parameter sharing, ensuring that both gating networks blend the three intelligence with different weights. In this manner, MoAI-Mixer and gating networks facilitate the interaction among the three sources of intelligence.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '_Implementation Details._ To ensure successful reproducibility, we outline three crucial technical details of MoAI: (a) external CV models, (b) MoAI-Compressor and MoAI-Mixer, (c) training and inference details.\n' +
      '\n' +
      '_(a)_ For panoptic segmentation, we adopt Mask2Former [12] (model size: 106M) with Swin-B/4 [61]. To predict a panoptic segmentation map, we set the threshold to keep predicted instance masks as 0.5 and set the mask threshold to use the masks as 0.95. For open-world object detection, we use OWLv2 [64] (model size: 154M) with CLIP-B/16 [69]. To achieve open-world object detection, we deal with 1847 object categories combining those in ADE20K-847 [89, 90] and ImageNet [18]. We set the threshold to keep object detection predictions as 0.1 and set the object threshold to use them as 0.5. For scene graph generation (SGG), we utilize panoptic SGG [83] (model size: 44M) with ResNet-50 [28] to conduct flexible interactions with foreground and background objects, where 0.8 threshold to use SGG predicates is set. For OCR, we use PaddleOCRv2 [21] (model size: 18M), one of performant open-source OCR frameworks, where we set recognizable languages to Chinese & English and set hyper-parameter settings to possibly read rotated text descriptions. The combined size of the external CV models is about 332M, contributing a little to the total model size.\n' +
      '\n' +
      '_(b)_ In MoAI-Compressor, the learnable tokens \\(A_{\\text{input}}\\) have \\(\\mathbb{R}^{4096\\times 64}\\) dimension where \\(64\\) denotes the number of tokens (length) and \\(4096\\) represents the channel dimension \\(d\\) for MLM input. In addition, MoAI-Compressor comprises \\(4\\) standard Transformer encoder layers [79]. In the self-attention, \\(4\\) number of heads and \\(64\\) head dimension are set. To build MoAI-Mixer, we equip it with specific MLM layer indices \\(l=7,15,23,31\\). For CA/SA expert modules, \\(64\\) reduced dimension, \\(4\\) number of heads, and \\(4096/4=1024\\) head dimension are used.\n' +
      '\n' +
      '_(c)_ For all training steps, we deal with a standard visual instruction tuning dataset: LLaVA-Instruct-665K [57] filtered by [11]. Regarding the first training step, we train the learnable tokens \\(A_{\\text{input}}\\), the parameters of MoAI-Compressor, and six expert modules of MoAI-Mixer in one epoch using the AdamW [63] optimizer, scheduled by cosine annealing [62] from learning rate of 1e-4 to 1e-6. In the second training step, we not only learn the parameters trained in the first training step but also the gating networks, where learning rate is scheduled from 2e-5 to 1e-6 in one epoch. For efficient inference, we quantize MoAI in 4-bit where double quantization and normalized float 4-bit (nf4) [19] are used, and we use deterministic beam search (\\(n=3\\)) [25] for text generation.\n' +
      '\n' +
      'Evaluating Visual Perception Capability.Delving into validating the effectiveness of MoAI, we look deeper into visual perception capability related to real-world scene understanding in numerous VL benchmarks, such as MME, SEED, MM-Bench, and MM-Vet. Fig. 2 illustrates the zero-shot performances in detail of MoAI and three state-of-the-art open-source LLVMs such as Instruct-BLIP [17], Qwen-VL [4], LLaVA1.5 [57]. For each VL benchmark, there exist specific dimensions (sub-benchmarks) related to real-world scene understanding\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline VLMs & Q-Bench & SQA-IMG & TextVQA & POPE & MME-P & MME-C & MM-Bench & MMB-CN & MM-Vet \\\\ \\hline BLIP2-13B [56] & - & 61.0 & 42.5 & 85.3 & 1294 & 290 & - & - & 22.4 \\\\ InstructBLIP-TB [17] & 56.7 & 60.5 & 50.1 & - & - & - & 36.0 & 23.7 & 26.2 \\\\ InstructBLIP-13B [11] & - & 63.1 & 50.7 & 78.9 & 1213 & - & - & - & 25.6 \\\\ Shikra-13B [10] & 54.7 & - & - & - & - & - & 58.8 & - & - \\\\ IDEFIC-9B [43] & - & - & 25.9 & - & - & - & 48.2 & 25.2 & - \\\\ IDEFIC-80B [43] & - & - & 30.9 & - & - & - & 54.5 & 38.1 & - \\\\ Qwen-VL-TB [4] & 59.4 & 67.1 & 63.8 & - & - & - & 38.2 & 7.4 & - \\\\ Qwen-VL-Chat-TB [4] & - & 68.2 & 61.5 & - & 1488 & 361 & 60.6 & 56.7 & - \\\\ MiniGPT-4-TB [92] & - & - & - & - & 582 & - & 23.0 & - & 22.1 \\\\ Otter-7B [18] & 47.2 & - & - & - & 1292 & - & 48.3 & - & 24.6 \\\\ LLaVA-TB [59] & - & 38.5 & - & - & 807 & 248 & 34.1 & 14.1 & 26.7 \\\\ MiniGPT-v2-TB [9] & - & - & - & - & - & - & - & - & - \\\\ MiniGPT-v2-Chat-TB [9] & - & - & - & - & - & - & - & - & - \\\\ ILaVA1-5B [57] & 58.7 & 66.8 & 58.2 & 85.9 & 1511 & 294 & 64.3 & 58.3 & 30.5 \\\\ ILaVA1.5-13B [57] & 62.1 & 71.6 & 61.3 & 85.9 & 1531 & 295 & 67.7 & 63.6 & 35.4 \\\\ mPLUG-Owl-TB [84] & 58.9 & - & - & - & 967 & - & 46.6 & - & - \\\\ mPLUG-Owl-TB [83] & 62.9 & 68.7 & 58.2 & 1450 & - & 64.5 & - & 36.2 \\\\ ShareGPT-V-TB [11] & 63.4 & 68.4 & - & - & 1567 & 376 & 68.8 & 62.2 & 37.6 \\\\ CogVLM-1TB [80] & - & 68.7 & 58.2 & - & - & - & 65.8 & 55.9 & **54.5** \\\\ LLaVA-XTuner-20B [16] & - & - & - & - & - & - & 75.1 & 73.7 & 37.2 \\\\ Intern-XC-TB [87] & 64.4 & - & - & - & 1528 & 391 & 74.4 & 72.4 & 35.2 \\\\ \\hline MoAI-TB & **70.2** & **83.5** & **67.8** & **87.1** & **1714** & **561** & **79.3** & **76.5** & 43.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Evaluating zero-shot performances of **MoAI** on nine vision language datasets compared with the current powerful VLMs on Q-Bench [82], SQA-IMG [33], TextVQA [75], POPE [52], MME-(P, -C) [26], MM-Bench(-CN) [60], and MM-Vet [86].\n' +
      '\n' +
      'in which MoAI aims to demonstrate its efficacy. Refer to Appendix for more details on what each dimension specifically indicates. As it can be seen from Fig. 2, MoAI significantly surpasses other LLVMs, demonstrating the effectiveness of utilizing auxiliary visual information from external CV models. It is noteworthy that MoAI especially excels at relation and text-related dimensions, emphasizing the significance of using auxiliary visual information that they struggle to fully comprehend. Refer to Appendix for qualitative assessment with demonstration on a few samples. Furthermore, Tab. 1 exhibits thorough evaluation across numerous renowned VL benchmarks, and demonstrates the exceptional performance of MoAI. The versatility of MoAI corroborates that enhancing real-world scene understanding can boost not only visual perception related to it but also overall VL capabilities, even outperforming closed-source LLVMs in Fig. 1(b).\n' +
      '\n' +
      'Ablation StudiesTo validate the effectiveness of the external CV models we utilize, we conduct evaluation by subtracting them one by one. Tab. 2 shows significant drop of object existence and recognition without using panoptic segmentation (PS) and open-world object detection (OWOD). On the other hand, once SGG is not used, the scores related with relations such as Position and Spatial are dropped in Tab. 2. In addition, the OCR scores are also dropped if OCR is not employed. Therefore, we can say that each of the external CV models is crucial for real-world scene understanding based on the perception scores for MME, SEED, MM-Bench, and MM-Vet. Additionally, we control three factors of MoAI-Mixer and gating networks in Tab. 3: (a) the two training steps, (b) selecting top-\\(k\\) in expert modules, and (c) weights of gating networks, in order to validate their effectiveness.\n' +
      '\n' +
      'Discussion and LimitationFrom the results, we can obtain an insight that prioritizing real-world scene understanding is more crucial than relying on the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 2: Illustrating the effectiveness of external computer vision (CV) models compared by the perception scores in MME [26] and MM-Bench [60]. ‘TT’ denotes text translation task that requires OCR as a priority.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 3: Ablation study for training step choice, selecting top-\\(k\\) expert modules in MoAI-Mixer, and the type of weights for gating network.\n' +
      '\n' +
      'extra curation of visual instruction datasets or scaling up model size. As illustrated in Fig. 7(a), MoAI-7B surpasses the zero-shot performances, despite being relatively small compared to the considerably larger open-source and closed-source models. Notably, Fig. 7(b) also indicates that MoAI performs well even on hallucination zero-shot datasets: POPE [52] and HallusionBench [56]. This suggests that accurately recognizing objects and their relationships can help prevent LLVMs from making mistakes. Looking ahead, as MoAI is tailored for real-world scene understanding, we plan to incorporate more external CV models to provide LLVMs with diverse capabilities for low-level vision understanding, common-sense knowledge, and awareness of non-object notions beyond text descriptions, such as charts, diagrams, signs, and symbols, as well as solving advanced math problems. Furthermore, robust [39, 44, 47], unbiased [41, 45, 55], and explainable [8, 37, 38] CV models can be applied to achieve precise and unbiased outputs for vision language tasks.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'To achieve real-world scene understanding, we leverage fundamental perception capabilities rooted in cognitive science and machine learning. This involves incorporating auxiliary visual information from historically rich external CV models, which we seemlessly integrate with visual and language features in MLM using expert modules and gating networks. As a result of these advancements, MoAI demonstrates improved visual perception capabilities, resulting in significant enhancements in zero-shot vision language performances. This underscores MoAI\'s potential to advance LLVM modeling by effectively leveraging diverse auxiliary visual information and integrating multiple forms of intelligence.\n' +
      '\n' +
      'Figure 7: Illustrating zero-shot vision language performances (a) by model size scale compared with the larger open-source LLVMs: LLaVA1.6-13B and -34B [58], in the latest, and closed-source LLVMs. (b) shows the results of POPE [52] and HallusionBench [56], where ‘Adversarial’, ‘Random’, and ‘Popular’ are metrics in POPE. Note that, the scores of MME in (a) are scaled down by 1/25 times to fit the figure, and the dot points for closed-source LLVMs represent averaged performances with them.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yi-vl-34b, [https://www.01.ai/](https://www.01.ai/) 2\n' +
      '* [2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\n' +
      '* [3] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems **35**, 23716-23736 (2022)\n' +
      '* [4] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)\n' +
      '* [5] Bengio, Y., Leonard, N., Courville, A.: Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013)\n' +
      '* [6] Biederman, I., Mezzanotte, R.J., Rabinowitz, J.C.: Scene perception: Detecting and judging objects undergoing relational violations. Cognitive Psychology **14**(2), 143-177 (1982). [https://doi.org/https://doi.org/10.1016/0010-0285](https://doi.org/https://doi.org/10.1016/0010-0285)(82)90007-X, [https://www.sciencedirect.com/science/article/pii/001002858290007X](https://www.sciencedirect.com/science/article/pii/001002858290007X)\n' +
      '* [7] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems **33**, 1877-1901 (2020)\n' +
      '* [8] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9650-9660 (2021)\n' +
      '* [9] Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chandra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023)\n' +
      '* [10] Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)\n' +
      '* [11] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)\n' +
      '* [12] Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1290-1299 (2022)\n' +
      '* [13] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. Journal of Machine Learning Research **24**(240), 1-113 (2023)\n' +
      '* [14] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences. Advances in neural information processing systems **30** (2017)* [15] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)\n' +
      '* [16] Contributors, X.: Xtuner: A toolkit for efficiently fine-tuning llm. [https://github.com/InternlM/xtuner](https://github.com/InternlM/xtuner) (2023)\n' +
      '* [17] Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '* [18] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n' +
      '* [19] Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 (2023)\n' +
      '* [20] Doveh, S., Arbelle, A., Harary, S., Herzig, R., Kim, D., Cascante-Bonilla, P., Alfassy, A., Panda, R., Giryes, R., Feris, R., Ullman, S., Karlinsky, L.: Dense and aligned captions (dac) promote compositional reasoning in vl models. In: Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neural Information Processing Systems. vol. 36, pp. 76137-76150. Curran Associates, Inc. (2023), [https://proceedings.neurips.cc/paper_files/paper/2023/file/efe406d6d2674d176ccdd958ce605d17-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/efe406d6d2674d176ccdd958ce605d17-Paper-Conference.pdf)\n' +
      '* [21] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021)\n' +
      '* [22] Eigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314 (2013)\n' +
      '* [23] Epstein, R.A., Baker, C.I.: Scene perception in the human brain. Annual review of vision science **5**, 373-397 (2019)\n' +
      '* [24] Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research **23**(1), 5232-5270 (2022)\n' +
      '* [25] Freitag, M., Al-Onaizan, Y.: Beam search strategies for neural machine translation. In: Luong, T., Birch, A., Neubig, G., Finch, A. (eds.) Proceedings of the First Workshop on Neural Machine Translation. pp. 56-60. Association for Computational Linguistics, Vancouver (Aug 2017). [https://doi.org/10.18653/v1/W17-3207](https://doi.org/10.18653/v1/W17-3207), [https://aclanthology.org/W17-3207](https://aclanthology.org/W17-3207)\n' +
      '* [26] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)\n' +
      '* [27] Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 (2023)\n' +
      '* [28] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [29] Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)\n' +
      '* [30] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation **9**(8), 1735-1780 (1997)* [31] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n' +
      '* [32] Iyer, S., Lin, X.V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.S., et al.: Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 (2022)\n' +
      '* [33] Iyyer, M., Yih, W.t., Chang, M.W.: Search-based neural structured learning for sequential question answering. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1821-1831 (2017)\n' +
      '* [34] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local experts. Neural computation **3**(1), 79-87 (1991)\n' +
      '* [35] Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer to rule universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2989-2998 (2023)\n' +
      '* [36] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of experts. arXiv preprint arXiv:2401.0 4088 (2024)\n' +
      '* [37] Kim, J., Lee, B.K., Ro, Y.M.: Distilling robust and non-robust features in adversarial examples by information bottleneck. Advances in Neural Information Processing Systems **34**, 17148-17159 (2021)\n' +
      '* [38] Kim, J., Lee, B.K., Ro, Y.M.: Causal unsupervised semantic segmentation. arXiv preprint arXiv:2310.07379 (2023)\n' +
      '* [39] Kim, J., Lee, B.K., Ro, Y.M.: Demystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12302-12312 (2023)\n' +
      '* [40] Kim, K., Yoon, K., In, Y., Moon, J., Kim, D., Park, C.: Adaptive self-training framework for fine-grained scene graph generation. In: The Twelfth International Conference on Learning Representations (2024), [https://openreview.net/forum?id=WipsLtH77t](https://openreview.net/forum?id=WipsLtH77t)\n' +
      '* [41] Kim, Y., Kim, J., Lee, B.K., Shin, S., Ro, Y.M.: Mitigating dataset bias in image captioning through clip confounder-free captioning network. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 1720-1724. IEEE (2023)\n' +
      '* [42] Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055 (2022)\n' +
      '* [43] Laurencion, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527 (2023)\n' +
      '* [44] Lee, B.K., Kim, J., Ro, Y.M.: Masking adversarial damage: Finding adversarial saliency for robust and sparse network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15126-15136 (2022)\n' +
      '* [45] Lee, B.K., Kim, J., Ro, Y.M.: Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4499-4509 (2023)\n' +
      '* [46] Lee, B.K., Park, B., Kim, C.W., Ro, Y.M.: Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248 (2024)* [47] Lee, B.K., Yu, Y., Ro, Y.M.: Towards adversarial robustness of bayesian neural network through hierarchical variational inference (2020)\n' +
      '* [48] Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\n' +
      '* [49] Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023)\n' +
      '* [50] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [51] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformer-based optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023)\n' +
      '* [52] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)\n' +
      '* [53] Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Zhang, J., Ning, M., Yuan, L.: Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947 (2024)\n' +
      '* [54] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)\n' +
      '* [55] Liu, B., Wang, D., Yang, X., Zhou, Y., Yao, R., Shao, Z., Zhao, J.: Show, deconfound and tell: Image captioning with causal inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18041-18050 (2022)\n' +
      '* [56] Liu, F., Guan, T., Li, Z., Chen, L., Yacoob, Y., Manocha, D., Zhou, T.: Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566 (2023)\n' +
      '* [57] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)\n' +
      '* [58] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* [59] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '* [60] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)\n' +
      '* [61] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012-10022 (2021)\n' +
      '* [62] Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)* [63] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2019), [https://openreview.net/forum?id=Bkg6RiCq77](https://openreview.net/forum?id=Bkg6RiCq77)\n' +
      '* [64] Minderer, M., Gritsenko, A.A., Houlsby, N.: Scaling open-vocabulary object detection. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), [https://openreview.net/forum?id=mQPNcBWjGc](https://openreview.net/forum?id=mQPNcBWjGc)\n' +
      '* [65] Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal contrastive learning with limoe: the language-image mixture of experts. Advances in Neural Information Processing Systems **35**, 9564-9576 (2022)\n' +
      '* [66] OpenAI: Gpt-4v(ision) system card (2023), [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card), Last accessed on 2024-02-13\n' +
      '* [67] OpenAI: Gpt-4v(ision) technical work and authors (2023), [https://openai.com/contributions/gpt-4v](https://openai.com/contributions/gpt-4v), Last accessed on 2024-02-13\n' +
      '* [68] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems **35**, 27730-27744 (2022)\n' +
      '* [69] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 139, pp. 8748-8763. PMLR (18-24 Jul 2021)\n' +
      '* [70] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training (2018)\n' +
      '* [71] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners\n' +
      '* [72] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research **21**(1), 5485-5551 (2020)\n' +
      '* [73] Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems **34**, 8583-8595 (2021)\n' +
      '* [74] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean, J.: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In: International Conference on Learning Representations (2017), [https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg)\n' +
      '* [75] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8317-8326 (2019)\n' +
      '* [76] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., Christiano, P.F.: Learning to summarize with human feedback. Advances in Neural Information Processing Systems **33**, 3008-3021 (2020)\n' +
      '* [77] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [78] Team, I.: Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport) (2023)* [79] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017) [80] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023) [81] Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. In: International Conference on Learning Representations (2022) [82] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., et al.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181 (2023) [83] Yang, J., Ang, Y.Z., Guo, Z., Zhou, K., Zhang, W., Liu, Z.: Panoptic scene graph generation. In: European Conference on Computer Vision. pp. 178-196. Springer (2022) [84] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023) [85] Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 (2023) [86] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023) [87] Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023) [88] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) [89] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 633-641 (2017) [90] Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision **127**, 302-321 (2019) [91] Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A.M., Le, Q.V., Laudon, J., et al.: Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems **35**, 7103-7114 (2022) [92] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023) [93] Zong, Z., Song, G., Liu, Y.: Detrs with collaborative hybrid assignments training. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6748-6758 (2023) [94] Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus, W.: St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906 (2022)\n' +
      '\n' +
      '## Appendix 0.A Details for Specific Dimensions in Fig. 2\n' +
      '\n' +
      'This section explains details on dimensions (sub-benchmarks or sub-tasks) related to real-world scene understanding in the following benchmarks: MME [26], SEED [49], MM-Bench [60], and MM-Vet [86].\n' +
      '\n' +
      '### Mme\n' +
      '\n' +
      '* **Existence** pertains to inquiries concerning the presence of a single object within a specified image.\n' +
      '* **Count** denotes the process of quantifying instances of the specified object depicted in an image.\n' +
      '* **Position** describes the capacity of a model to discern the spatial arrangement between two objects within a given image.\n' +
      '* **Scene** focuses on the identification of a place shown in an image, including indoor and outdoor locations such as a gallery, laboratory, lakeside, landmark, etc.\n' +
      '* **OCR** serves to measure the capability of a model to recognize texts in the image.\n' +
      '* **Text translation** requires a model, supporting both English and Chinese, to translate the Chinese script in an image to the corresponding English.\n' +
      '\n' +
      '### Seed\n' +
      '\n' +
      '* **Scene (scene understanding)** focuses on the overall information conveyed in the image, where inquiries can be addressed through a comprehensive grasp of the image\'s content.\n' +
      '* **Identity (instance identity)** involves the object recognition capability of a model, which includes determining the presence or categorization of an instance.\n' +
      '* **Attributes (instance attributes)** relates to the distinguishing features of an instance, such as its color, shape, or material composition, serving to evaluate the model\'s comprehension of an object\'s visual appearance.\n' +
      '* **Location (instance location)** concerns the precise spatial coordinates of a specific instance within the image, necessitating accurate localization of the referenced object by the model.\n' +
      '* **Count (instances counting)** pertains to the capability of a model to count the occurrences of a designated object within the image, demanding a comprehensive understanding and enumeration of instances of the specified object.\n' +
      '* **Relation (spatial relation)** prompts the model to establish the spatial connection between two specified objects within the image, grounding them and recognizing their relative spatial arrangement.\n' +
      '* **Interaction (instance interaction)** requires the model to identify either the relational state or interactive dynamics between two human entities or objects depicted in the image.\n' +
      '\n' +
      '* **Text recognition (text understanding)** involves inquiries concerning the textual components present within the image, prompting the model to comprehend and interpret textual elements accordingly.\n' +
      '\n' +
      '### MM-Bench\n' +
      '\n' +
      '* **Attribute (attribute recognition)** involve the capability of a model to recognize various features of an image such as texture, shape, appearance, emotions, categories, celebrities, renowned locations, objects, and optical characters within an image.\n' +
      '* **Localization (object localization)** involves inquiries regarding an object\'s position, absolute coordinates, count, and orientation within the image.\n' +
      '* **OCR** showcases a model\'s capability to recognize text, formulas, and sheets within an image.\n' +
      '* **Relation (spatial relationship)** assesses the capability of a model to understand the relative positions between objects depicted in the image.\n' +
      '\n' +
      '### MM-Vet\n' +
      '\n' +
      '* **Recognition** evaluates the overall visual recognition proficiency of a model, encompassing the identification of scenes, objects, object attributes, counting, and various other advanced visual recognition tasks in computer vision.\n' +
      '* **Spatial (spatial awareness)** encompasses a wide range of abilities related to spatial comprehension, including understanding the spatial relationships among objects and textual regions within a scene.\n' +
      '* **OCR** assesses the capability of a model to understand and reason over scene text, where the model is evaluated on its capability to read text within images and utilize this information to solve various tasks.\n' +
      '\n' +
      '## Appendix 0.B Demonstrating MoAI for Qualitative Assessment\n' +
      '\n' +
      'SEED Visual input example, Scene Understanding:\n' +
      '\n' +
      'User\n' +
      '\n' +
      'Which player in the image is bleeding from the face?\n' +
      '\n' +
      'A. The hockey player B. The man with the beard\n' +
      '\n' +
      'C. The tennis player D. None of them are bleeding\n' +
      '\n' +
      'InstructBLIP\n' +
      '\n' +
      'A. The hockey player\n' +
      '\n' +
      'Qwen-VL\n' +
      '\n' +
      'A. The hockey player\n' +
      '\n' +
      'LLaVA1.5\n' +
      '\n' +
      'A. The hockey player\n' +
      '\n' +
      'MoAI\n' +
      '\n' +
      'B. The man with the beard\n' +
      '\n' +
      'SEED Visual input example, Instance Identity:\n' +
      '\n' +
      'User\n' +
      '\n' +
      'Which of the following is not present in the image?\n' +
      '\n' +
      'A. Coral reef B. Palm tree C. Seashell D. Fish\n' +
      '\n' +
      'InstructBLIP\n' +
      '\n' +
      'D. Fish\n' +
      '\n' +
      'LLaVA1.5\n' +
      '\n' +
      'D. Fish\n' +
      '\n' +
      'MoAI\n' +
      '\n' +
      'B. Palm Tree\n' +
      '\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multicolumn{2}{l}{SEED Visual input example, Instance Interaction:} \\\\ \\hline \\hline \\multirow{2}{*}{User} & What is the person on the bed doing? \\\\  & A. Sitting up B.Reading C. Watching TV D. Sleeping \\\\ \\hline \\hline \\multirow{2}{*}{InstructBLIP} & D. Sleeping \\\\ \\hline \\hline \\multirow{2}{*}{Qwen-VL} & D. Sleeping \\\\ \\hline \\hline \\multicolumn{2}{l}{LLaVA1.5} & D. Sleeping \\\\ \\hline \\hline \\multicolumn{2}{l}{MoAI} & C. Watching TV \\\\ \\hline \\hline \\multicolumn{2}{l}{SEED Visual input example, Text Recognition:} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multirow{2}{*}{User} & What color is the text on the sign in the image? \\\\  & A. Black B. Red C. White D. Blue \\\\ \\hline \\hline \\multicolumn{2}{l}{InstructBLIP} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{Qwen-VL} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{LLaVA1.5} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{MoAI} & C. White \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>