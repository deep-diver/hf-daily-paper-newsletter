<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MoAI: All Intelligence의 혼합\n' +
      '\n' +
      '대규모 언어 및 비전 모델을 위한\n' +
      '\n' +
      'Byung-Kwan Lee\n' +
      '\n' +
      '한국전자통신연구원\n' +
      '\n' +
      '한국과학기술원\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      'Beomchan Park\n' +
      '\n' +
      '한국전자통신연구원\n' +
      '\n' +
      '한국과학기술원\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      '김채원\n' +
      '\n' +
      '한국전자통신연구원\n' +
      '\n' +
      '한국과학기술원\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      '용만로\n' +
      '\n' +
      '한국전자통신연구원\n' +
      '\n' +
      '한국과학기술원\n' +
      '\n' +
      '{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)과 명령어 튜닝의 증가는 명령어 조정된 대규모 언어 및 비전 모델(LLVM)의 현재 추세로 이어졌다. 이러한 추세는 특정 목표에 맞춘 수많은 명령어 튜닝 데이터 세트를 세심하게 큐레이션하거나 방대한 양의 비전 언어(VL) 데이터를 관리하기 위해 LLVM을 확대하는 것을 포함한다. 그러나, 현재의 LLVM들은 세분화, 검출, 장면 그래프 생성(SGG), 및 광학 문자 인식(OCR)과 같은 시지각 태스크들에서 전문화된 컴퓨터 비전(CV) 모델들로부터 이용 가능한 상세하고 포괄적인 실세계 장면 이해를 무시했다. 대신, 기존의 LLVM은 주로 LLM 백본의 대용량 및 창발 능력에 의존한다. 따라서 우리는 외부 분할, 탐지, SGG 및 OCR 모델의 출력에서 얻은 보조 시각 정보를 활용하는 새로운 LLVM, **M**ixture **of** **A**ll **I**ntelligence(**A** **MoAI**)를 제시한다. MoAI는 새롭게 도입된 두 개의 모듈인 _MoAI-Compressor_와 _MoAI-Mixer_를 통해 동작한다. 외부 CV 모델의 출력을 언어화한 후, MoAI-컴프레서는 VL 작업에 대한 관련 보조 시각적 정보를 효율적으로 사용하기 위해 정렬 및 응축한다. 그런 다음 MoAI-Mixer는 (1) 시각적 특징, (2) 외부 CV 모델의 보조 특징, (3) 언어 특징 등 세 가지 유형의 지능을 혼합한다. 이러한 통합을 통해 MoAI는 모델 크기를 확대하거나 추가 시각적 명령 조정 데이터 세트를 큐레이션하지 않고 객체 존재, 위치, 관계 및 OCR과 같은 실제 장면 이해와 관련된 수많은 제로 샷 VL 작업에서 오픈 소스 및 클로즈 소스 LLVM 모두를 상당히 능가한다. 코드는 [https://github.com/ByungKwanLee/MoAI](https://github.com/ByungKwanLee/MoAI)에서 사용할 수 있다.\n' +
      '\n' +
      '키워드: 전문가와 비젼 모델의 혼합\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'PaLM[13] 및 T5[72]와 같은 대규모 언어 모델(LLM)과 Flan[81]의 명령어 튜닝 데이터 세트를 결합한 Chung _et al._[15]는 명령어 튜닝 LLM을 위한 Flan-PaLM 및 Flan-T5를 개발했다. 이 모델은 다양한 작업을 다루는 확장된 명령어 조정 데이터 세트를 활용하며 용량을 확장하기 위해 추가 확장되어 수많은 언어 작업에 걸쳐 제로샷 성능이 눈에 띄게 개선되었습니다.\n' +
      '\n' +
      '명령어 조정 LLM의 성공과 함께 여러 시각적 명령 튜닝 데이터 세트[11, 17, 59, 80, 4]가 대규모 언어 및 비전 모델(LLVM)에서 제로 샷 비전 언어(VL) 성능을 향상시키기 위해 세심하게 큐레이션되었다. 또한 VL 데이터 세트에서 강력한 제로 샷 성능을 목표로 LLVM[1, 4, 58, 80]을 실질적으로 확장하기 위한 공동 노력이 이루어졌다. 비주얼 명령어 튜닝 데이터 세트의 확장과 LLVM의 스케일 업으로, 오픈 소스 LLVM[1, 4, 9, 11, 17, 27, 58, 59, 80, 87, 92]은 GPT-4V[66, 67], Gemini-Pro[77], Qwen-VL-Plus[4]와 같은 폐쇄 소스 LLVM에 비해 제로 샷 VL 성능의 격차를 좁히고 있다.\n' +
      '\n' +
      '그러나 현재 오픈 소스 LLVM은 주로 LLM 백본의 대용량 및 창발 능력에 의존하여 상세하고 포괄적인 실제 장면 이해도를 명시적으로 또는 완전히 활용하지 않았다. 인지 과학 및 기계 학습에 대한 여러 연구[20, 6, 23]는 기본적인 장면 인식 능력은 문자 텍스트를 포함할 수 있는 객체 존재 인식, 위치 결정, 상태 식별, 관계 이해, 공간 장면 레이아웃 추출 및 비객체 개념 파악을 포함한 다양한 인지 기능에서 비롯될 수 있다고 주장한다. 다행히도 이러한 인지 기능은 세분화[12, 35], 검출[64, 93], 장면 그래프 생성(SGG)[83, 40], 광학 문자 인식(OCR)[21, 51]과 같은 시지각 작업에 대해 수십 년 동안 연구 및 개발된 전문 컴퓨터 비전(CV) 모델로부터 획득될 수 있다.\n' +
      '\n' +
      '초점을 명령어 조정에서 이러한 외부 CV 모델을 활용하는 것으로 전환하면 객체 존재, 위치, 관계 및 OCR을 포함하는 LLVM에 대한 실제 장면 이해도가 향상될 것으로 예상된다. 객체 및 그 위치의 인식[46]은 범광학 분할 및 개방 세계 객체 검출 모델에 의해 용이하게 될 수 있다. 객체를 포함하는 보다 포괄적인 이해를 위해\n' +
      '\n' +
      '그림 1: 다양한 오픈 소스 및 클로즈드 소스 LLVM에 대한 수많은 VL 벤치마크의 점수와 정확도를 **MoAI**에 대한 것과 비교한다.\n' +
      '\n' +
      ' states and relationships(_i.e.,_ compositional reasoning[20]), scene graph generation(SGG) 모델이 필요하다. 더욱이, OCR 모델을 통해 비객체 개념으로서의 이미지 내의 텍스트 설명이 인식될 수 있다.\n' +
      '\n' +
      '이에 착안하여, 우리는 새로운 LLVM, **M**ixture **of** **A**ll **Intelligence (**A** **MoAI**) 모델을 제안한다. 이 모델은 (1) panoptic segmentation[12], (2) open-world object detection[64], (3) SGG[83], (4) OCR[21] 모델이다. 이 정보를 효과적으로 활용하기 위해 _MoAI-Compressor_와 _MoAI-Mixer_의 두 가지 새로운 모듈을 소개한다. MoAI-컴프레서는 외부 CV 모델의 언어화된 출력을 보조 시각적 정보로 정렬 및 응축하여 VL 작업에 대한 관련 정보의 효율적인 사용을 가능하게 한다. 그 후, _MoAI-Mixer_는 세 가지 유형의 지능((1) 시각적 특징, (2) 외부 CV 모델로부터의 보조 특징, (3) 언어 특징)을 응집력 있는 전체로 혼합한다.\n' +
      '\n' +
      'MoAI-Mixer를 구성함에 있어서, 우리는 Mixture of Experts (MoE)의 개념으로부터 영감을 얻는다[73, 74, 65, 91]. 우리의 과제는 MoAI의 다중 모달 언어 모델(MLM)에서 사용되는 원본 특징(_즉,_시각 및 언어 특징)을 텍스트 토큰과 함께 시각적 인코더가 출력하는 시각적 토큰을 외부 CV 모델 및 MoAI-컴프레서로부터 획득한 보조 특징과 매끄럽게 통합하는 데 있다. 우리는\n' +
      '\n' +
      '도 2: InstructBLIP[17], Qwen-VL[4], LLaVA1.5[57]와 같은 다양한 LLVM의 능력을 검증하기 위해 MME[26], SEED[49], MM-Bench[60], MM-Vet[86]에서 실제 장면 이해와 관련된 차원의 점수와 정확도를 비교한다.\n' +
      '\n' +
      '앞서 언급한 세 가지 유형의 지능을 다루는 MoAI-Mixer에서 6개의 전문가 모듈을 구성하기 위해 교차 및 자기 주의 모듈을 사용한다. 또한, 이러한 전문가 모듈에 대한 최적의 가중치 조합을 결정하기 위해 게이팅 네트워크를 활용한다.\n' +
      '\n' +
      'MoAI-Compressor와 MoAI-Mixer를 결합하여, MoAI는 외부 CV 모델의 출력을 효과적으로 활용하고 세 가지 인텔리전스 소스를 혼합함으로써 복잡한 질문 응답 작업을 해결하기 위한 시각적 인식 능력을 향상시킨다. 도 1에 도시된 바와 같다. 2, 우리의 결과는 MoAI가 시각적 지시 조정 데이터세트의 추가 큐레이션이나 LLVM의 스케일업 없이도 InstructBLIP[17], Qwen-VL[4], LLaVA1.5[57]의 세 가지 강력한 LLVM 기준에서 시지각 점수에서 상당히 능가했음을 보여준다. 또한, 개선된 시각적 인식 능력으로 인해 MoAI는 그림 1과 같이 폐쇄 소스 LLVM을 능가하는 VL 작업에서 강력한 제로 샷 성능을 나타낸다. MoAI의 성공은 외부 CV 모델의 다양한 보조 시각적 정보를 활용하고 VL 작업을 효과적으로 실행하기 위한 세 가지 지능 유형의 통합에 기인한다. 우리의 기여도는 다음과 같이 크게 두 가지 측면으로 요약할 수 있다.\n' +
      '\n' +
      '* 외부 CV 모델(_MoAI-Compressor_)로부터 다양한 보조 시각 정보를 처리하고 세 가지 유형의 지능(_MoAI-Mixer_)을 혼합한 새로운 대형 언어 및 비전 모델인 **MoAI**를 소개한다.\n' +
      '***MoAI**는 제로샷 VL 성능에서 오픈 소스 및 클로즈드 소스 LLVM을 모두 능가하는 VL 작업에서 탁월한 시각적 인지 능력이 돋보인다. 이 능력은 모델 크기 또는 데이터세트 크기를 확장할 필요 없이 상세하고 포괄적인 실제 장면 이해를 고려함으로써 달성된다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      'LLM 및 LLVMLLM은 유능한 일반화 능력과 명령어 조정 데이터 세트의 효율성과 함께 등장했다. GPT[70, 71, 7]는 텍스트 분류, 질문 응답, 기계 번역, 복잡한 추론 작업 등을 포함한 다양한 언어 작업에 걸쳐 강력한 제로 샷 또는 소수 샷 성능을 보여줌으로써 LLM의 길을 여는 데 중요한 역할을 했다. 이러한 LLM의 일반화 능력은 T5[72], PaLM[13], OPT[88]과 같은 작업에서 볼 수 있듯이 모델 용량과 훈련 데이터 세트를 크게 증가시킴으로써 달성되었다. 훈련 방법 및 데이터 세트의 진행은 LLM의 제로 샷 일반화를 더욱 향상시켜 대규모 사전 훈련 데이터 세트에서 명령어 튜닝 데이터 세트[15, 32, 68, 81]로 전환한다. 명령어 튜닝[81]은 LLM들이 복잡한 현실 세계 시나리오들 하에서 인간 자연 언어로 된 명령들을 따를 수 있게 한다. Flan-T5, Flan-PaLM[15], OPT-IML[32], InstructGPT[68]와 같은 명령어 조정 LLM은 명령어 조정의 효과를 명확하게 보여준다. 연구자들은 시각적 인코더와 백본 멀티모달 언어 모델(MLM)로 구성된 멀티모달 대응 장치인 LLVM에 유사한 전략을 적용하여 한 걸음 더 나아갔다.\n' +
      '\n' +
      '예를 들어, LLaVA[59] 및 ShareGPT4V[11]은 각각 GPT-4[2] 및 GPT-4V[66, 67]를 활용하여 시각적 명령어 튜닝 데이터 세트를 생성하는 반면, 다른 [4, 17, 80]은 또한 자신의 고유한 목적을 위해 다양한 시각적 명령어 튜닝 데이터 세트를 개발하였다. 그러나 기존 LLVM은 지난 수십 년 동안 큰 발전으로 CV 모델에서 사용할 수 있는 상세하고 포괄적인 실제 장면 이해도를 간과했다. CV 모델은 LLVM 시대에 확장된 용량과 시각적 명령 조정 데이터 세트를 가진 LLVM에 의해 가려졌다. 이러한 관점에서 MoAI는 외부 CV 모델에서 얻은 보조 시각 정보를 활용하는 효과를 강조하여 VL 벤치마크에 대한 향상된 시각 인식 능력을 보여준다.\n' +
      '\n' +
      'Mixture of Experts.Jacobs _et al._[34]는 먼저 기계학습에 Mixture of Experts(MoE) 개념을 도입하였는데, 여기서 \'전문가\'라고 불리는 별도의 네트워크는 입력 공간의 서로 다른 세그먼트를 처리하고, 각 세그먼트는 게이팅 네트워크에 의해 관련 전문가들에게 안내된다. 이 아이디어는 MoE 층들이 깊이 적층되는 깊은 MoE[22]와 주어진 입력에 의해 소수의 전문가들만이 조건적으로 활성화되는 조건부 계산[5]에 의해 추가로 발전된다. 현대 딥 러닝에서, Shazeer _et al._[74]는 MoE 계층과 LSTM[30]을 통합하는데, 여기서 게이팅 네트워크는 각각의 토큰을 선택적으로 활성화된 전문가들에게 독립적으로 라우팅한다. 이러한 통합은 언어 모델링 및 기계 번역 작업에서 성능을 향상시킵니다. 또한, 스위치 트랜스포머[24]는 트랜스포머층 내부의 조밀한 피드 포워드 네트워크(FFN)를 다수의 전문가와 게이팅 네트워크로 대체함으로써 MoE 계층과 트랜스포머[79]를 병합하여 MoE-LLaVA[53]와 같은 트랜스포머 기반 LLVM에서 MoE의 성공적인 사용을 가능케 한다. 딥러닝에서 MoE의 철학은 계산 효율을 희생시키지 않고 모델 용량을 확대하는 것이다[22, 24, 36, 42, 53, 74, 94]. 한편, 우리는 MoE의 다르면서도 근본적인 측면에 초점을 맞추는데, 여기서 각 전문가가 입력의 특정 부분을 전문적으로 설계하도록 의도한다. 이전의 MoE 방법은 개별 전문가에게 역할을 명시적으로 할당하지 않고 대신 최적화 중에 전문화가 나타날 것으로 예상하지만, MoAI는 교차 및 자기 주의 모듈을 전문가로 지정하고 양식(_즉,_시각, 보조 및 언어 기능)에 걸쳐 정보를 혼합하도록 명시적으로 학습한다. 구체적으로, MoAI는 (1) 시각적-보조 특징, (2) 시각적-언어 특징, (3) 시각적-시각 특징, (4) 언어-보조 특징, (5) 언어-시각 특징, 및 (6) 언어-언어 특징의 쌍을 용이하게 한다. 각 쌍은 전문가 역할을 하는 각각의 교차 또는 자기 주의 모듈에 대한 쿼리 키 쌍으로 간주되어 다양한 모달리티에 걸친 정보의 융합을 명확히 한다.\n' +
      '\n' +
      '##3MoAI: All Intelligence의 혼합\n' +
      '\n' +
      'Model Architecture.Fig.에 도시된 바와 같다. 도 3을 참조하면, MoAI는 비전 인코더, MoAI-Mixers가 장착된 백본 멀티모달 언어 모델(MLM), 비전 인코더와 MLM 사이의 중간 MLP 커넥터, 및 범광학 분할[12], 오픈 월드 객체 검출[64], 장면 그래프 생성(SGG)[83], 광학 문자 인식(OCR)을 위해 4개의 외부 컴퓨터 비전(CV) 모델을 활용하는 MoAI-컴프레서로 구성된다[21]. MoAI-컴프레서는 외부 CV 모델에서 얻은 다양한 보조 시각적 정보를 처리하기 위해 도입되며, 여기서 CV 모델 출력은 그림과 같이 언어화를 통해 처리된다. 4를 사용하여 MoAI에서 사용되는 MLM에 정렬 및 해석할 수 있다. 또한, 외부 CV 모델들로부터의 보조 특징들과 원래의 두 특징들(_즉,_시각 및 언어 특징들)을 효율적으로 조화시키기 위해 MoAI-Mixer가 추가로 제시된다. 언어화, MoAI-Compressor, MoAI-Mixer에 대한 자세한 내용은 본 절에서 설명한다.\n' +
      '\n' +
      '비전 및 언어 백본CLIP-L/14[69]는 비전 언어 작업에 대한 텍스트와 정렬된 이미지 이해의 보장된 숙련도로 인해 비전 인코더로 선택된다[57, 58, 59, 11]. MoAI에서 사용되는 MLM은 InternLM-7B[78]를 기반으로 하며, 이는 일련의 점진적 사전 훈련 단계와 인간 피드백(RLHF)으로부터의 강화 학습을 통해 1.6T 토큰으로 다국어 데이터세트로 조정되는 다국어 기반 모델 명령어이다[14, 68, 76]. GELU 활성화 함수 [29]를 갖는 두 개의 선형 레이어는 그림 3에서 \'MLP\'로 표시된 시각과 언어 구성요소 사이의 브리지 커넥터 역할을 한다.\n' +
      '\n' +
      '언어화(Verbalization)는 MoAI를 구성하기 위해 멀티모달 언어 모델(MLM)을 채택하므로, 언어화라는 과정을 통해 CV 모델 출력을 자연어 형식으로 변환하여 MLM에 이해할 수 있도록 한다. 도. 도 4는 4개의 CV 모델 출력들이 MLM에 의미적으로 정렬된 보조 토큰들의 생성과 함께 언어화를 어떻게 겪는지를 예시한다.\n' +
      '\n' +
      '판옵틱 분할 모델은 영상에서 전경 객체와 배경 객체를 한 번에 구분할 수 있게 해준다. 또한 분할 맵으로부터 경계 박스 좌표([x_{\\text{min}},y_{\\text{min},x_{\\text{max},y_{\\text{max}]])를 계산할 수 있다. 결과적으로, 판옵틱 세그먼트화(PS)로부터의 출력을 언어화하는 것은 도 4에 설명된 바와 같이 바운딩 박스 좌표 및 그들의 객체 이름을 직렬화하는 것을 수반한다. 이어서, 이러한 언어화된 설명은 보조적으로 변환된다.\n' +
      '\n' +
      '도 3: **MoAI** 아키텍처의 개요. 압축 학습 가능한 토큰, MoAI-Compressor 및 MoAI-Mixer의 파라미터가 학습된다. \'비전\'은 시각적 특징을 내장하는 비전 인코더를 나타내고 얼음/화재 기호는 얼거나 학습할 모듈을 나타낸다. \'단어 임베딩\'은 MLM의 단어 임베딩 사전을 나타낸다.\n' +
      '\n' +
      'kens through the word embedding of MLM. 또한, 팬옵틱 분할 맵을 직접 활용하기 위해 MoAI에서 비전 인코더와 MLP 커넥터를 사용하여 지역성 보존 보조 토큰을 생성한다. 생성된 보조 토큰은 직렬화된 바운딩 박스들 및 그 객체 이름들로부터 평탄화되고 연결됨으로써 최종 PS 보조 토큰들(A_{\\text{PS}})을 형성한다. 그들은\n' +
      '\n' +
      '도 4: 외부 CV 모델들에 대한 **MoAI**의 언어화 프로세스: 팬옵틱 세그멘테이션(PS), 오픈 월드 오브젝트 검출(OWOD), 장면 그래프 생성(SGG), 및 광학 문자 인식(OCR). 유의할 것은, \'d\'는 MLM의 채널 차원을 나타내고, 따라서 보조 토큰들은 동일한 채널 차원을 갖는다.\n' +
      '\n' +
      '이러한 방식으로 연결되어 MoAI의 MLM이 맥락화를 통해 호환 가능한 방식으로 연관시킬 수 있다. 이 절차는 판옵틱 분할 맵에 내재된 공간적 지역성을 보존하면서 PS에서 언어 정보로 시각 정보를 포괄적으로 변환하는 것을 보장한다. 팬옵틱 분할 모델이 고정된 수의 팬옵틱 오브젝트 카테고리들, 예를 들어, 133 오브젝트 카테고리들을 포괄하는 MS-COCO 2017 [54] 내의 오브젝트들을 분류하는데 실패하면, _unknown_ 클래스가 할당된다는 점에 유의한다.\n' +
      '\n' +
      '개방형 객체 검출 모델은 범광학 분할 모델이 놓친 객체 클래스를 검출하는 역할을 한다. 이는 범광학 분할 모델이 고정된 개수의 객체 카테고리로 특정 데이터셋에 대해 학습되기 때문이다. 일단 이미지에 대한 검출 결과가 생성되면, 바운딩 박스 좌표들 및 그들의 객체 이름들은 다음의 템플릿 포맷에 따라 구두화된다: \'이미지는 바운딩 박스들 및 그들의 객체들을 포함한다: {verbalized open-world object detection (OWOD) 결과} 그런 다음 MLM의 워드 임베딩을 통해 결과를 OWOD 보조 토큰(A_{\\text{OWOD}}\\)으로 변환한다. 유사하게, SGG 및 OCR 모델의 출력을 구두화하고 대응하는 보조 토큰 \\(A_{\\text{SGG}}\\) 및 \\(A_{\\text{OCR}}\\)을 생성하는데, 여기서 우리는 다음과 같은 구두화 템플릿을 사용한다: \'이미지는 객체 간의 관계를 포함한다: {언어화된 SGG 결과}\' 및 \'이미지는 텍스트 설명: {언어화된 OCR 결과}\'이다.\n' +
      '\n' +
      'MoAI-Compressor는 CV 모델 출력을 언어화한 후, Perceiver Resampler[3]의 구조를 차용한 4개의 보조 토큰(A_{\\text{PS}}\\), \\(A_{\\text{OWOD}}\\), \\(A_{\\text{SGG}}\\), \\(A_{\\text{OCR}}\\)을 생성하여 MoAI-Compressor에 주입한다. 4개의 보조 토큰([A_{\\text{PS}},A_{\\text{OWOD}},A_{\\text{SGG}},A_{\\text{OCR}]\\)은 MoAI-컴프레서에 입력되기 전에 연결되며, 출력(A\\)도 동일한 수만큼 길이가 고정되고 압축되고 정렬된 보조 시각적 정보를 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\[A=\\text{MoAI-Compressor}([A_{\\text{PS}},A_{\\text{OWOD}},A_{\\text{SGG}},A_{\\text{OCR}}]\\,A_{\\text{input}}). \\tag{1}\\\n' +
      '\n' +
      'MoAI-Compressor는 영상에 연결된 보조 토큰의 길이가 가변적이고, 연결 후 상당한 길이로 인해 상대적으로 작은 고정 크기가 64인 토큰([A_{\\text{PS}},A_{\\text{OWOD},A_{\\text{SGG}},A_{\\text{OCR}]\\)을 압축하여 임베딩 차원을 나타내는 \\(A\\in\\mathbb{R}^{d\\times 64}\\)을 생성한다. 그런 다음 이러한 응축된 토큰은 MoAI-Mixer에 의해 VL 태스크에 대한 관련 정보를 추출하는 데 사용된다. 이러한 압축은 계산 효율을 향상시킨다.\n' +
      '\n' +
      'MoAI-Mixeris는 MoAI의 각 MLM 레이어에 내장된다. MoAI-Compressor로부터 보조 토큰(A\\), 시각적 특징(I^{(l)}\\in\\mathbb{R}^{d\\times N_{I}\\), 언어 특징(L^{(l)}\\in\\mathbb{R}^{d\\times N_{L}\\)을 수신하며, 여기서 \\(l=0,1,\\cdots,N-1\\)은 레이어 인덱스, \\(d\\)은 임베딩 차원, \\(N_{I}\\)은 시각적 특징의 길이, \\(N_{L}\\)은 언어 특징의 길이를 나타낸다. 일반적으로 MLM 레이어는 트랜스포머 디코더 블록인 TransDec\\({}^{(l)}\\([I^{(l+1)},L^{(l+1)}]=\\text{TransDec}^{(l)}([I^{(l)},L^{(l)}])으로만 구성된다. MoAI에서, MoAI-Mixer를 갖는 \\(l\\)번째 MLM 층은 다음과 같이 뮤레이트된다:\n' +
      '\n' +
      '{split}[\\hat{I}^{(l)},\\hat{L}^{(l)}]&=\\text{MoAI-Mixer}^{(l)}(A,I^{(l)},L^{(l)}),\\\\\\\\[I^{(l+1)},L^{(l+1)}]&=\\text{TransDec}^{(l)}(\\hat{I}^{(l)},\\hat{L}^{(l)}),\\end{split}\\tag{2}\\text{\n' +
      '\n' +
      '여기서 \\(\\hat{I}^{(l)}\\) 및 \\(\\hat{L}^{(l)}\\)은 혼합된 시각적 특징과 혼합된 언어 특징들이다. 각 MoAI-믹서에서 그림 1과 같이 교차 또는 자체 주의 모듈인 6개의 전문가 모듈을 설계한다. 5: 시각적 특징(I\\)의 경우 3개, 언어적 특징(L\\)의 경우 3개. 시각적 특징에 대한 3개의 전문가 모듈 각각은 \\(I_{\\text{AUX}}\\), \\(I_{\\text{LANG}}\\), \\(I_{\\text{SELF}}\\)을 출력하며, 여기서 대문자는 질의 특징을 나타내고 첨자는 키/값 특징을 나타낸다. 마찬가지로 언어기능에 대한 3개의 전문가 모듈 각각은 \\(L_{\\text{AUX}}\\), \\(L_{\\text{IMG}}\\), \\(L_{\\text{SELF}}\\)을 출력한다. \\(l\\)번째 층에서의 교차-어텐션 동작은 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\text{split}I^{(l)}(q=I^{(l)},k=\\{A\\text{ or }L^{(l)},v=k),\\\\\\\\L^{(l)}_{\\text{AUX or }IMG}}&=\\text{CA}^{(l)}(q=L^{(l)},k=\\{A\\text{ or }I^{(l)},v=k).\\end{split}\\tag{3}\\text{\n' +
      '\n' +
      '또한 자기 주의 연산은 \\(I^{(l)}_{\\text{SELF}}=\\text{SA}^{(l)}(I^{(l)})\\)와 \\(L^{(l)}_{SELF}=\\text{SA}^{(l)}(L^{(l)})\\)으로 공식화된다. 이 6개의 전문가 모듈은 지능의 여섯 가지 고유한 혼합물 중 하나인 \\(I_{\\text{AUX}}\\), \\(I_{\\text{LANG}}\\), \\(I_{\\text{SELF}}\\), \\(L_{\\text{AUX}}\\), \\(L_{\\text{IMG}}\\), \\(L_{\\text{SELF}}\\)을 명시적으로 전문화한다. 전문가 모듈을 훈련할 때, 우리는 계산 부담을 줄이기 위해 LoRA[31]의 개념을 차용한다. 다중 헤드 어텐션 모듈[79]에서 선형 투영층에 대한 일반적인 표기법으로 \\(W\\)을 표시하자. \\(W^{q}\\), \\(W^{k}\\), \\(W^{v}\\) 또는 \\(W^{o}\\)일 수 있다. 우리는 LoRA에서와 같이 \\(\\Delta W\\)이 아닌 \\(W\\in\\mathbb{R}^{d\\times d}\\)을 두 개의 선형층 \\(W_{A}\\in\\mathbb{R}^{d\\times r}\\)과 \\(W_{B}\\in\\mathbb{R}^{r\\times d}\\)으로 분해하여 \\(W=W_{A}W_{B}\\)이 되도록 한다. 하이퍼파라미터 \\(r\\)는 그림 1과 같이 축소된 차원을 나타낸다. 6(a). 어텐션 모듈의 계산 부담은 주로 높은 임베딩 차원, 일반적으로 \\(d=4096\\)에서 발생하기 때문에 이러한 투영의 공식화\n' +
      '\n' +
      '도 5: **MoAI**의 MLM Layer에서 MoAI-Mixer를 예시한다. MoAI-Mixer는 보조 특징(A\\(A\\)과 두 개의 원본 특징(시각적 특징(I\\)과 언어적 특징(L\\))을 조화시키기 위한 6개의 전문가 모듈이 있다.\n' +
      '\n' +
      '행렬은 계산량을 크게 줄입니다. 더욱이, 입력 질의 특징들은 출력 특징들에 직접 추가되어 이전 MLM 계층의 출력들을 너무 많이 변경하지 않고 지능의 혼합이 일어나게 하여, 동결된 트랜스포머 디코더 블록들로 최적화 프로세스를 안정화시킨다.\n' +
      '\n' +
      '첫 번째 훈련 단계. 우리는 시각적 명령 튜닝 데이터 세트를 사용하여 \\(A_{\\text{input}}\\), MoAI-컴프레서 및 MoAI-믹서를 먼저 훈련한다[11, 57]. 이 단계는 MoAI-Mixer의 6개의 전문가 모듈이 VL 태스크를 수행하기 위해 의미 있는 기능을 산출하도록 보장한다. 이를 위해 시각적 및 언어적 특징에 대해 각각 3개의 전문가 모듈 중 하나의 출력을 다음과 같이 무작위로 선택한다.\n' +
      '\n' +
      '[\\hat{I}^{(l)}=\\text{Sample}(I^{(l)}_{\\text{AUX}},I^{(l)}_{\\text{LANG}},I^{(l)}_{\\text{SELF}}),\\quad\\hat{L}^{(l)}=\\text{Sample}(L^{(l)}_{\\text{AUX},L^{(l)}_{\\text{IMG}},L^{(l)}_{\\text{SELF}}}).\\tag{4}\\text{(l)}_{\\text{LANG},I^{(l)}_{\\text{SELF}},\\quad\\hat{L}^{(l)}=\\text{Sample}(L^{(l)}_{\\text{AUX}},L^{(l)}_{\\text{IMG},L^{(l)}_{\\text{SELF}}).\n' +
      '\n' +
      '그리고 이를 트랜스포머 디코더 블록\\(\\text{TransDec}_{l}(\\hat{I}^{(l)},\\hat{L}^{(l)})\\)에 주입한다. 이 샘플링 프로세스는 각 전문가 모듈이 독립적으로 의미 있는 기능을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '두 번째 훈련 단계.이 단계에서는 첫 번째 훈련 단계에서 학습된 매개 변수를 넘어 학습 과정을 확장한다. 각각의 MoAI-Mixer에 대해 두 개의 게이팅 네트워크를 학습하였으며, 각 게이팅 네트워크는 시각적, 언어적 특징에 대해 각각 \\(W_{\\text{Gating}_{I}\\)과 \\(W_{\\text{Gating}_{L}\\in\\mathbb{R}^{d\\times 3}\\)으로 구성된다. 6(b). 게이팅 네트워크는 시각적 및 언어적 특징을 위한 3개의 전문가 모듈에 대해 선형 레이어와 소프트맥스 함수를 사용하여 가중치 조합을 가장 잘 출력하는 것을 목표로 한다. 주의할 점은 \\(x\\in\\mathbb{R}^{d\\times N_{x}}\\), 여기서\n' +
      '\n' +
      '도 6: MoAI-Mixer를 위한 (a) 전문가 모듈 및 (b) 게이팅 네트워크의 구조. (a)에서 \'\\(q\\)\'과 \'\\(k\\)\'과 \'\\(v\\)\'은 각각 질의, 키, 값을 나타내며, \'\\(d\\)\'과 \'\\(r\\)\'은 채널 차원과 축소 차원을 각각 설명한다.\n' +
      '\n' +
      '(x\\(x\\)은 시각적(I\\) 또는 언어적(L\\) 특징이고, \\(N_{x\\)은 특징의 길이이며, 결과적으로 \\(x^{\\mathsf{T}}W_{\\text{Gating}_{x}}\\in\\mathbb{R}^{N_{x}\\times 3}\\의 결과를 가져온다. 그런 다음, 각 가중치가 \\(\\text{Softmax}(x^{\\mathsf{T}}W_{\\text{Gating}_{x}},\\text{dim=1})\\rightarrow[w_{\\text{AUX},w_{\\text{LANG}},w_{\\text{SELF}}]\\)의 세 가지 가중치 벡터로 소프트맥스 행렬을 분할한다. 가중치는 각 전문가 모듈로부터의 정보를 사용할지 여부를 결정하기 위한 신뢰 점수로서 작용한다. 게이팅 네트워크들의 출력들로부터, \'AUX\', \'IMG\', \'LANG\'의 세 가지 인텔리전스 소스에 대한 전파 흐름은 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '{I^{Gating}_{I},\\text{LANG},w_{\\text{SELF}}}}leftarrow\\text{AUX}}_{\\text{LUX}}+w_{SELF}},\\end{hat{L^{L^{(l)}}w_{\\text{AUX}}+w_{SELF}}}leftarrow\\text{L^{(l)}\n' +
      '\n' +
      '여기서 \\(\\odot\\)은 각 토큰에서 요소별 곱을 나타낸다. 시각 및 언어 특징에 대한 게이팅 네트워크는 파라미터 공유 없이 독립적으로 훈련되어, 두 게이팅 네트워크가 세 가지 지능을 서로 다른 가중치로 혼합하도록 보장한다. 이러한 방식으로 MoAI-Mixer 및 게이팅 네트워크는 세 가지 정보 소스 간의 상호 작용을 촉진한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'Details.__Implementation Details.___ 성공적인 재현성을 보장하기 위해 우리는 (a) 외부 CV 모델, (b) MoAI-컴프레서 및 MoAI-믹서, (c) 훈련 및 추론 세부 사항의 세 가지 중요한 기술 세부 사항을 요약한다.\n' +
      '\n' +
      '_(a)_panoptic segmentation에 대해, 우리는 Swin-B/4 [61]을 갖는 Mask2Former [12] (모델 크기: 106M)를 채택한다. 팬옵틱 분할 맵을 예측하기 위해 예측된 인스턴스 마스크를 0.5로 유지하는 임계값을 설정하고, 마스크를 0.95로 사용하는 마스크 임계값을 설정하고, 오픈 월드 객체 검출을 위해 CLIP-B/16[69]과 함께 OWLv2[64] (모델 크기: 154M)를 사용한다. 개방형 객체 탐지를 위해 ADE20K-847 [89, 90]과 ImageNet [18]을 결합한 1847개의 객체 범주를 다룬다. 객체 검출 예측을 0.1로 유지하기 위해 임계값을 설정하고, 이를 0.5로 사용하기 위해 객체 임계값을 설정한다. 장면 그래프 생성(SGG)의 경우 ResNet-50[28]과 함께 팬옵틱 SGG[83](모델 크기: 44M)을 사용하여 전경 및 배경 객체와 유연한 상호작용을 수행하며, 여기서 SGG 술어를 사용하기 위해 0.8 임계값을 설정한다. OCR의 경우 성능 오픈 소스 OCR 프레임워크 중 하나인 PaddleOCRv2[21] (모델 크기: 18M)을 사용하며, 여기서 인식 가능한 언어를 중국어와 영어로 설정하고 하이퍼 파라미터 설정을 회전된 텍스트 설명을 읽을 수 있도록 설정한다. 외부 CV 모델의 결합 크기는 약 332M으로 전체 모델 크기에 조금 기여한다.\n' +
      '\n' +
      'MoAI-Compressor에서 학습 가능한 토큰 \\(A_{\\text{input}\\)은 \\(\\mathbb{R}^{4096\\times 64}\\) 차원을 가지며, 여기서 \\(64\\)은 토큰의 수(길이)를 나타내고 \\(4096\\)는 MLM 입력에 대한 채널 차원 \\(d\\)을 나타낸다. 또한, MoAI-컴프레서는 \\(4\\) 표준 트랜스포머 인코더 층[79]을 포함한다. 자기 주의력에서는 \\(4\\)의 머리 수와 \\(64\\)의 머리 치수를 설정하였다. MoAI-Mixer를 구축하기 위해 특정 MLM 계층 지수 \\(l=7,15,23,31\\)을 장착한다. CA/SA 전문가 모듈의 경우 \\(64\\) 축소 차원, \\(4\\) 헤드 수 및 \\(4096/4=1024\\) 헤드 차원을 사용한다.\n' +
      '\n' +
      '_(c)_ 모든 훈련 단계에 대해, 우리는 [11]에 의해 필터링된 표준 시각적 명령 튜닝 데이터세트: LLaVA-Instruct-665K [57]을 다룬다. 첫 번째 학습 단계에서는 학습 가능한 토큰(A_{\\text{input}}\\), MoAI-Compressor의 파라미터 및 MoAI-Mixer의 6개의 전문가 모듈을 AdamW[63] Optimizer를 이용하여 1e-4에서 1e-6까지의 학습률로 스케줄링하고, 두 번째 학습 단계에서는 첫 번째 학습 단계에서 학습된 파라미터뿐만 아니라 2e-5에서 1e-6까지의 학습률로 스케줄링하는 게이팅 네트워크를 학습한다. 효율적인 추론을 위해 이중 양자화와 정규화된 플로트 4비트(nf4)[19]를 사용하는 4비트에서 MoAI를 양자화하고 텍스트 생성을 위해 결정론적 빔 탐색(\\(n=3\\))[25])을 사용한다.\n' +
      '\n' +
      '시지각 능력 평가 MoAI의 효과성 검증을 위해, 우리는 MME, SEED, MM-Bench, MM-Vet와 같은 수많은 VL 벤치마크에서 실제 장면 이해와 관련된 시지각 능력을 더 깊이 들여다본다. 도. 도 2는 MoAI 및 Instruct-BLIP[17], Qwen-VL[4], LLaVA1.5[57]와 같은 세 가지 최첨단 오픈 소스 LLVM의 제로 샷 성능을 상세하게 도시한다. VL 벤치마크마다 실제 장면 이해와 관련된 특정 차원(하위 벤치마크)이 존재한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline VLMs & Q-Bench & SQA-IMG & TextVQA & POPE & MME-P & MME-C & MM-Bench & MMB-CN & MM-Vet \\\\ \\hline BLIP2-13B [56] & - & 61.0 & 42.5 & 85.3 & 1294 & 290 & - & - & 22.4 \\\\ InstructBLIP-TB [17] & 56.7 & 60.5 & 50.1 & - & - & - & 36.0 & 23.7 & 26.2 \\\\ InstructBLIP-13B [11] & - & 63.1 & 50.7 & 78.9 & 1213 & - & - & - & 25.6 \\\\ Shikra-13B [10] & 54.7 & - & - & - & - & - & 58.8 & - & - \\\\ IDEFIC-9B [43] & - & - & 25.9 & - & - & - & 48.2 & 25.2 & - \\\\ IDEFIC-80B [43] & - & - & 30.9 & - & - & - & 54.5 & 38.1 & - \\\\ Qwen-VL-TB [4] & 59.4 & 67.1 & 63.8 & - & - & - & 38.2 & 7.4 & - \\\\ Qwen-VL-Chat-TB [4] & - & 68.2 & 61.5 & - & 1488 & 361 & 60.6 & 56.7 & - \\\\ MiniGPT-4-TB [92] & - & - & - & - & 582 & - & 23.0 & - & 22.1 \\\\ Otter-7B [18] & 47.2 & - & - & - & 1292 & - & 48.3 & - & 24.6 \\\\ LLaVA-TB [59] & - & 38.5 & - & - & 807 & 248 & 34.1 & 14.1 & 26.7 \\\\ MiniGPT-v2-TB [9] & - & - & - & - & - & - & - & - & - \\\\ MiniGPT-v2-Chat-TB [9] & - & - & - & - & - & - & - & - & - \\\\ ILaVA1-5B [57] & 58.7 & 66.8 & 58.2 & 85.9 & 1511 & 294 & 64.3 & 58.3 & 30.5 \\\\ ILaVA1.5-13B [57] & 62.1 & 71.6 & 61.3 & 85.9 & 1531 & 295 & 67.7 & 63.6 & 35.4 \\\\ mPLUG-Owl-TB [84] & 58.9 & - & - & - & 967 & - & 46.6 & - & - \\\\ mPLUG-Owl-TB [83] & 62.9 & 68.7 & 58.2 & 1450 & - & 64.5 & - & 36.2 \\\\ ShareGPT-V-TB [11] & 63.4 & 68.4 & - & - & 1567 & 376 & 68.8 & 62.2 & 37.6 \\\\ CogVLM-1TB [80] & - & 68.7 & 58.2 & - & - & - & 65.8 & 55.9 & **54.5** \\\\ LLaVA-XTuner-20B [16] & - & - & - & - & - & - & 75.1 & 73.7 & 37.2 \\\\ Intern-XC-TB [87] & 64.4 & - & - & - & 1528 & 391 & 74.4 & 72.4 & 35.2 \\\\ \\hline MoAI-TB & **70.2** & **83.5** & **67.8** & **87.1** & **1714** & **561** & **79.3** & **76.5** & 43.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Q-Bench [82], SQA-IMG [33], TextVQA [75], POPE [52], MME-(P, -C) [26], MM-Bench(-CN) [60], 및 MM-Vet [86] 상의 현재의 강력한 VLM들과 비교하여 9개의 비전 언어 데이터 세트들에 대한 **MoAI**의 제로-샷 성능들을 평가한다.\n' +
      '\n' +
      'MoAI는 그 효능을 입증하는 것을 목표로 한다. 각 차원이 구체적으로 나타내는 내용에 대한 자세한 내용은 부록을 참조하십시오. 도 1에서 확인할 수 있는 바와 같다. 2, MoAI는 다른 LLVM을 크게 능가하여 외부 CV 모델의 보조 시각적 정보를 활용하는 효과를 보여준다. 주목할 점은 MoAI가 특히 관계 및 텍스트 관련 차원에 탁월하다는 점으로, 이들이 충분히 이해하기 어려운 보조적 시각 정보를 활용하는 것의 중요성을 강조하고 있다. 몇 가지 샘플에 대한 시연과 함께 정성적 평가는 부록을 참조하십시오. 또한 탭입니다. 1은 수많은 유명한 VL 벤치마크에 걸쳐 철저한 평가를 나타내며 MoAI의 탁월한 성능을 보여준다. MoAI의 다양성은 실제 장면 이해도를 향상시키는 것이 그림 1의 폐쇄 소스 LLVM을 능가하는 전체 VL 능력뿐만 아니라 이와 관련된 시각적 인식을 향상시킬 수 있음을 확증한다. 1(b)\n' +
      '\n' +
      '절제 연구는 우리가 사용하는 외부 CV 모델의 효과를 검증하기 위해 하나씩 빼서 평가를 수행한다. 탭 도 2는 팬옵틱 세그멘테이션(PS) 및 오픈 월드 오브젝트 검출(OWOD)을 사용하지 않고 오브젝트 존재 및 인식의 상당한 드롭을 도시한다. 한편, SGG를 사용하지 않을 경우, Tab. 2에서 Position, Spatial과 같은 관계와 관련된 점수가 떨어지며, OCR을 사용하지 않을 경우 OCR 점수도 떨어진다. 따라서, 각각의 외부 CV 모델은 MME, SEED, MM-Bench, MM-Vet에 대한 인식 점수를 기반으로 실제 장면 이해에 중요하다고 말할 수 있다. 또한, 탭에서 MoAI-Mixer와 게이팅 네트워크의 세 가지 요소를 제어한다. 3: (a) 두 훈련 단계, (b) 전문가 모듈에서 Top-\\(k\\)을 선택하는 단계, (c) 게이팅 네트워크의 가중치를 적용하여 그 유효성을 검증한다.\n' +
      '\n' +
      '그 결과, 실제 장면 이해의 우선순위를 결정하는 것이 실제 장면 이해에 의존하는 것보다 중요하다는 통찰력을 얻을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 2: MME[26] 및 MM-Bench[60]에서의 인식 점수에 의해 비교되는 외부 컴퓨터 비전(CV) 모델들의 유효성을 예시한다. ‘TT’는 OCR을 우선순위로 요구하는 텍스트 번역 작업을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: 훈련 단계 선택, MoAI-Mixer에서 상위\\(k\\) 전문가 모듈 선택, 게이팅 네트워크를 위한 가중치 유형을 위한 절제 연구.\n' +
      '\n' +
      '시각적 지시 데이터 세트의 추가 큐레이션 또는 모델 크기 확장 도 1에 도시된 바와 같다. 도 7(a)를 참조하면, MoAI-7B는 상당히 큰 오픈 소스 및 클로즈드 소스 모델에 비해 상대적으로 작음에도 불구하고 제로 샷 성능을 능가한다. 특히, Fig. 도 7(b)는 또한 MoAI가 환각 제로-샷 데이터세트들: POPE[52] 및 할루전 벤치[56])에서도 잘 수행함을 나타낸다. 이는 객체와 그 관계를 정확하게 인식하는 것이 LLVM이 실수하는 것을 방지하는 데 도움이 될 수 있음을 시사한다. 앞으로는 MoAI가 실세계 장면 이해에 맞춘 만큼 보다 많은 외부 CV 모델을 통합해 LLVM에 차트, 다이어그램, 기호, 기호 등 텍스트 설명을 넘어 비객체 개념에 대한 낮은 수준의 비전 이해, 상식 지식, 인식을 위한 다양한 기능을 제공하고 고급 수학 문제를 해결할 계획이다. 또한, 강건한 [39, 44, 47], 편향되지 않은 [41, 45, 55], 설명 가능한 [8, 37, 38] CV 모델을 적용하여 비전 언어 작업에 대한 정확하고 편향되지 않은 출력을 달성할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '실제 현장 이해를 달성하기 위해 인지 과학 및 기계 학습에 뿌리를 둔 기본 인식 능력을 활용합니다. 여기에는 역사적으로 풍부한 외부 CV 모델의 보조 시각적 정보를 통합하는 것이 포함되며, 이는 전문가 모듈과 게이팅 네트워크를 사용하여 MLM의 시각적 및 언어 기능과 거의 통합되지 않는다. 이러한 발전의 결과로 MoAI는 향상된 시지각 능력을 보여주어 제로샷 비전 언어 성능을 크게 향상시켰다. 이는 다양한 보조 시각 정보를 효과적으로 활용하고 여러 형태의 지능을 통합함으로써 LLVM 모델링을 발전시킬 MoAI의 잠재력을 강조한다.\n' +
      '\n' +
      '그림 7: 최신 및 폐쇄 소스 LLVM에서 더 큰 오픈 소스 LLVM: LLaVA1.6-13B 및 -34B [58]에 비해 모델 크기 척도별 제로 샷 비전 언어 성능(a)을 보여준다. (b)는 POPE[52]와 HallusionBench[56]의 결과를 나타낸 것으로, POPE에서 \'Adversarial\', \'Random\', \'Popular\'가 메트릭이다. (a)에서 MME의 점수는 그림에 맞게 1/25배 축소되고 폐쇄 소스 LLVM에 대한 점점은 그들과의 평균 성능을 나타낸다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yi-vl-34b, [https://www.01.ai/](https://www.01.ai/) 2\n' +
      '* [2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\n' +
      '* [3] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems **35**, 23716-23736 (2022)\n' +
      '* [4] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)\n' +
      '* [5] Bengio, Y., Leonard, N., Courville, A.: Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 (2013)\n' +
      '* [6] Biederman, I., Mezzanotte, R.J., Rabinowitz, J.C.: Scene perception: Detecting and judging objects undergoing relational violations. Cognitive Psychology **14**(2), 143-177 (1982). [https://doi.org/https://doi.org/10.1016/0010-0285](https://doi.org/https://doi.org/10.1016/0010-0285)(82)90007-X, [https://www.sciencedirect.com/science/article/pii/001002858290007X](https://www.sciencedirect.com/science/article/pii/001002858290007X)\n' +
      '* [7] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems **33**, 1877-1901 (2020)\n' +
      '* [8] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9650-9660 (2021)\n' +
      '* [9] Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chandra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023)\n' +
      '* [10] Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)\n' +
      '* [11] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)\n' +
      '* [12] Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1290-1299 (2022)\n' +
      '* [13] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. Journal of Machine Learning Research **24**(240), 1-113 (2023)\n' +
      '* [14] Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep reinforcement learning from human preferences. Advances in neural information processing systems **30** (2017)* [15] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)\n' +
      '* [16] Contributors, X.: Xtuner: A toolkit for efficiently fine-tuning llm. [https://github.com/InternlM/xtuner](https://github.com/InternlM/xtuner) (2023)\n' +
      '* [17] Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '* [18] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248-255. Ieee (2009)\n' +
      '* [19] Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 (2023)\n' +
      '* [20] Doveh, S., Arbelle, A., Harary, S., Herzig, R., Kim, D., Cascante-Bonilla, P., Alfassy, A., Panda, R., Giryes, R., Feris, R., Ullman, S., Karlinsky, L.: Dense and aligned captions (dac) promote compositional reasoning in vl models. In: Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neural Information Processing Systems. vol. 36, pp. 76137-76150. Curran Associates, Inc. (2023), [https://proceedings.neurips.cc/paper_files/paper/2023/file/efe406d6d2674d176ccdd958ce605d17-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/efe406d6d2674d176ccdd958ce605d17-Paper-Conference.pdf)\n' +
      '* [21] Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144 (2021)\n' +
      '* [22] Eigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314 (2013)\n' +
      '* [23] Epstein, R.A., Baker, C.I.: Scene perception in the human brain. Annual review of vision science **5**, 373-397 (2019)\n' +
      '* [24] Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research **23**(1), 5232-5270 (2022)\n' +
      '* [25] Freitag, M., Al-Onaizan, Y.: Beam search strategies for neural machine translation. In: Luong, T., Birch, A., Neubig, G., Finch, A. (eds.) Proceedings of the First Workshop on Neural Machine Translation. pp. 56-60. Association for Computational Linguistics, Vancouver (Aug 2017). [https://doi.org/10.18653/v1/W17-3207](https://doi.org/10.18653/v1/W17-3207), [https://aclanthology.org/W17-3207](https://aclanthology.org/W17-3207)\n' +
      '* [26] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)\n' +
      '* [27] Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 (2023)\n' +
      '* [28] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [29] Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)\n' +
      '* [30] Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation **9**(8), 1735-1780 (1997)* [31] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n' +
      '* [32] Iyer, S., Lin, X.V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.S., et al.: Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 (2022)\n' +
      '* [33] Iyyer, M., Yih, W.t., Chang, M.W.: Search-based neural structured learning for sequential question answering. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1821-1831 (2017)\n' +
      '* [34] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local experts. Neural computation **3**(1), 79-87 (1991)\n' +
      '* [35] Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One transformer to rule universal image segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2989-2998 (2023)\n' +
      '* [36] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of experts. arXiv preprint arXiv:2401.0 4088 (2024)\n' +
      '* [37] Kim, J., Lee, B.K., Ro, Y.M.: Distilling robust and non-robust features in adversarial examples by information bottleneck. Advances in Neural Information Processing Systems **34**, 17148-17159 (2021)\n' +
      '* [38] Kim, J., Lee, B.K., Ro, Y.M.: Causal unsupervised semantic segmentation. arXiv preprint arXiv:2310.07379 (2023)\n' +
      '* [39] Kim, J., Lee, B.K., Ro, Y.M.: Demystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12302-12312 (2023)\n' +
      '* [40] Kim, K., Yoon, K., In, Y., Moon, J., Kim, D., Park, C.: Adaptive self-training framework for fine-grained scene graph generation. In: The Twelfth International Conference on Learning Representations (2024), [https://openreview.net/forum?id=WipsLtH77t](https://openreview.net/forum?id=WipsLtH77t)\n' +
      '* [41] Kim, Y., Kim, J., Lee, B.K., Shin, S., Ro, Y.M.: Mitigating dataset bias in image captioning through clip confounder-free captioning network. In: 2023 IEEE International Conference on Image Processing (ICIP). pp. 1720-1724. IEEE (2023)\n' +
      '* [42] Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055 (2022)\n' +
      '* [43] Laurencion, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527 (2023)\n' +
      '* [44] Lee, B.K., Kim, J., Ro, Y.M.: Masking adversarial damage: Finding adversarial saliency for robust and sparse network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15126-15136 (2022)\n' +
      '* [45] Lee, B.K., Kim, J., Ro, Y.M.: Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4499-4509 (2023)\n' +
      '* [46] Lee, B.K., Park, B., Kim, C.W., Ro, Y.M.: Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248 (2024)* [47] Lee, B.K., Yu, Y., Ro, Y.M.: Towards adversarial robustness of bayesian neural network through hierarchical variational inference (2020)\n' +
      '* [48] Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\n' +
      '* [49] Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023)\n' +
      '* [50] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [51] Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.: Trocr: Transformer-based optical character recognition with pre-trained models. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 13094-13102 (2023)\n' +
      '* [52] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)\n' +
      '* [53] Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Zhang, J., Ning, M., Yuan, L.: Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947 (2024)\n' +
      '* [54] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)\n' +
      '* [55] Liu, B., Wang, D., Yang, X., Zhou, Y., Yao, R., Shao, Z., Zhao, J.: Show, deconfound and tell: Image captioning with causal inference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18041-18050 (2022)\n' +
      '* [56] Liu, F., Guan, T., Li, Z., Chen, L., Yacoob, Y., Manocha, D., Zhou, T.: Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566 (2023)\n' +
      '* [57] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)\n' +
      '* [58] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* [59] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '* [60] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)\n' +
      '* [61] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 10012-10022 (2021)\n' +
      '* [62] Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)* [63] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2019), [https://openreview.net/forum?id=Bkg6RiCq77](https://openreview.net/forum?id=Bkg6RiCq77)\n' +
      '* [64] Minderer, M., Gritsenko, A.A., Houlsby, N.: Scaling open-vocabulary object detection. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), [https://openreview.net/forum?id=mQPNcBWjGc](https://openreview.net/forum?id=mQPNcBWjGc)\n' +
      '* [65] Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal contrastive learning with limoe: the language-image mixture of experts. Advances in Neural Information Processing Systems **35**, 9564-9576 (2022)\n' +
      '* [66] OpenAI: Gpt-4v(ision) system card (2023), [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card), Last accessed on 2024-02-13\n' +
      '* [67] OpenAI: Gpt-4v(ision) technical work and authors (2023), [https://openai.com/contributions/gpt-4v](https://openai.com/contributions/gpt-4v), Last accessed on 2024-02-13\n' +
      '* [68] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems **35**, 27730-27744 (2022)\n' +
      '* [69] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 139, pp. 8748-8763. PMLR (18-24 Jul 2021)\n' +
      '* [70] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training (2018)\n' +
      '* [71] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners\n' +
      '* [72] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research **21**(1), 5485-5551 (2020)\n' +
      '* [73] Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems **34**, 8583-8595 (2021)\n' +
      '* [74] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean, J.: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In: International Conference on Learning Representations (2017), [https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg)\n' +
      '* [75] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8317-8326 (2019)\n' +
      '* [76] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., Christiano, P.F.: Learning to summarize with human feedback. Advances in Neural Information Processing Systems **33**, 3008-3021 (2020)\n' +
      '* [77] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [78] Team, I.: Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport) (2023)* [79] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017) [80] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023) [81] Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. In: International Conference on Learning Representations (2022) [82] Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., et al.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181 (2023) [83] Yang, J., Ang, Y.Z., Guo, Z., Zhou, K., Zhang, W., Liu, Z.: Panoptic scene graph generation. In: European Conference on Computer Vision. pp. 178-196. Springer (2022) [84] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023) [85] Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou, J.: mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257 (2023) [86] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023) [87] Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023) [88] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022) [89] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 633-641 (2017) [90] Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision **127**, 302-321 (2019) [91] Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A.M., Le, Q.V., Laudon, J., et al.: Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems **35**, 7103-7114 (2022) [92] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023) [93] Zong, Z., Song, G., Liu, Y.: Detrs with collaborative hybrid assignments training. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6748-6758 (2023) [94] Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus, W.: St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906 (2022)\n' +
      '\n' +
      '## 부록 0.Fig.의 특정 치수에 대한 상세. 2\n' +
      '\n' +
      '이 섹션에서는 다음 벤치마크에서 실제 장면 이해와 관련된 차원(하위 벤치마크 또는 하위 작업)에 대한 세부 사항을 설명한다. MME[26], SEED[49], MM-Bench[60], MM-Vet[86].\n' +
      '\n' +
      '### Mme\n' +
      '\n' +
      '* **Existence**는 특정된 이미지 내의 단일 객체의 존재에 관한 문의에 관한 것이다.\n' +
      '****Count**는 이미지에 묘사된 특정된 객체의 인스턴스들을 정량화하는 과정을 나타낸다.\n' +
      '***위치**는 주어진 이미지 내에서 두 객체 사이의 공간적 배열을 식별하는 모델의 용량을 설명한다.\n' +
      '**Scene**은 갤러리, 실험실, 호반, 랜드마크 등과 같은 실내 및 실외 위치를 포함하여 이미지에 나타난 장소의 식별에 중점을 둔다.\n' +
      '****OCR**은 이미지 내의 텍스트를 인식하는 모델의 능력을 측정하는 역할을 한다.\n' +
      '***텍스트 번역**은 이미지 내 중국어 스크립트를 해당 영어로 번역하기 위해 영어와 중국어를 모두 지원하는 모델이 필요하다.\n' +
      '\n' +
      '### Seed\n' +
      '\n' +
      '**장면(장면 이해)**은 이미지에서 전달되는 전반적인 정보에 초점을 맞추며, 여기서 문의는 이미지의 내용에 대한 포괄적인 파악을 통해 해결할 수 있다.\n' +
      '**아이덴티티(인스턴스 아이덴티티)**는 모델의 객체 인식 능력을 포함하며, 이는 인스턴스의 존재 또는 카테고리화를 결정하는 것을 포함한다.\n' +
      '**속성(인스턴스 속성)**은 객체의 시각적 외관에 대한 모델의 이해도를 평가하는 역할을 하는, 인스턴스의 색상, 모양 또는 재료 구성과 같은 인스턴스의 구별되는 특징에 관한 것이다.\n' +
      '**위치(인스턴스 위치)**는 이미지 내의 특정 인스턴스의 정확한 공간 좌표에 관한 것으로, 모델에 의해 참조된 객체의 정확한 로컬화를 필요로 한다.\n' +
      '***카운트(인스턴스 카운팅)**는 이미지 내에서 지정된 객체의 발생을 카운트하는 모델의 능력에 관한 것으로, 지정된 객체의 인스턴스에 대한 포괄적인 이해와 계수를 요구한다.\n' +
      '***관계(공간 관계)**는 모델이 이미지 내의 특정된 두 객체들 사이의 공간 연결을 확립하도록 프롬프트하고, 이들을 접지시키고 그들의 상대적인 공간 배열을 인식한다.\n' +
      '**상호작용(인스턴스 상호작용)**은 모델이 이미지에 묘사된 두 인간 개체 또는 객체 사이의 관계 상태 또는 상호작용 역학 중 하나를 식별하도록 요구한다.\n' +
      '\n' +
      '**텍스트 인식(텍스트 이해)**에는 이미지 내에 존재하는 텍스트 구성요소에 관한 문의가 포함되며, 이에 따라 모델이 텍스트 요소를 이해하고 해석하도록 유도한다.\n' +
      '\n' +
      '### MM-Bench\n' +
      '\n' +
      '**속성(속성 인식)**은 이미지 내의 텍스처, 모양, 외관, 감정, 카테고리, 유명인, 유명한 위치, 객체, 광학 문자 등의 이미지의 다양한 특징을 인식하는 모델의 능력을 포함한다.\n' +
      '**로컬라이제이션(객체 로컬라이제이션)**은 이미지 내의 객체의 위치, 절대 좌표, 카운트 및 배향에 관한 질의를 포함한다.\n' +
      '**OCR**은 이미지 내에서 텍스트, 공식 및 시트를 인식할 수 있는 모델의 기능을 보여줍니다.\n' +
      '**관계(공간 관계)**는 이미지에 묘사된 객체들 간의 상대적인 위치를 이해하는 모델의 능력을 평가한다.\n' +
      '\n' +
      '### MM-Vet\n' +
      '\n' +
      '**인식**은 컴퓨터 비전에서 장면, 객체, 객체 속성, 계수 및 기타 다양한 고급 시각 인식 작업의 식별을 포괄하여 모델의 전반적인 시각 인식 능력을 평가한다.\n' +
      '**공간(공간 인식)**는 장면 내의 객체 및 텍스트 영역 간의 공간 관계를 이해하는 것을 포함하여 공간 이해와 관련된 광범위한 능력을 포괄한다.\n' +
      '**OCR**은 장면 텍스트를 이해하고 추론하는 모델의 능력을 평가하며, 여기서 모델은 이미지 내에서 텍스트를 읽고 다양한 작업을 해결하기 위해 이 정보를 활용하는 능력에 대해 평가된다.\n' +
      '\n' +
      '## 부록 0.B 정성적 평가를 위한 MoAI 시연\n' +
      '\n' +
      'SEED Visual input example, Scene Understanding:\n' +
      '\n' +
      'User\n' +
      '\n' +
      '사진 속의 어떤 선수가 얼굴에서 피를 흘리고 있나요?\n' +
      '\n' +
      'A. 하키 선수 B. 수염을 기른 남자\n' +
      '\n' +
      'C. 테니스 선수 D. 그들 중 누구도 피를 흘리지 않는다.\n' +
      '\n' +
      'InstructBLIP\n' +
      '\n' +
      'A. 하키 선수\n' +
      '\n' +
      'Qwen-VL\n' +
      '\n' +
      'A. 하키 선수\n' +
      '\n' +
      'LLaVA1.5\n' +
      '\n' +
      'A. 하키 선수\n' +
      '\n' +
      'MoAI\n' +
      '\n' +
      'B. 수염을 기른 남자\n' +
      '\n' +
      'SEED Visual input example, Instance Identity:\n' +
      '\n' +
      'User\n' +
      '\n' +
      '다음 중 이미지에 없는 것은?\n' +
      '\n' +
      'A. 산호초 B. 팜나무 C. 조개껍질 D. 어류\n' +
      '\n' +
      'InstructBLIP\n' +
      '\n' +
      'D. Fish\n' +
      '\n' +
      'LLaVA1.5\n' +
      '\n' +
      'D. Fish\n' +
      '\n' +
      'MoAI\n' +
      '\n' +
      'B. 팜트리\n' +
      '\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multicolumn{2}{l}{SEED Visual input example, Instance Interaction:} \\\\ \\hline \\hline \\multirow{2}{*}{User} & What is the person on the bed doing? \\\\  & A. Sitting up B.Reading C. Watching TV D. Sleeping \\\\ \\hline \\hline \\multirow{2}{*}{InstructBLIP} & D. Sleeping \\\\ \\hline \\hline \\multirow{2}{*}{Qwen-VL} & D. Sleeping \\\\ \\hline \\hline \\multicolumn{2}{l}{LLaVA1.5} & D. Sleeping \\\\ \\hline \\hline \\multicolumn{2}{l}{MoAI} & C. Watching TV \\\\ \\hline \\hline \\multicolumn{2}{l}{SEED Visual input example, Text Recognition:} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multirow{2}{*}{User} & What color is the text on the sign in the image? \\\\  & A. Black B. Red C. White D. Blue \\\\ \\hline \\hline \\multicolumn{2}{l}{InstructBLIP} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{Qwen-VL} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{LLaVA1.5} & B. Red \\\\ \\hline \\hline \\multicolumn{2}{l}{MoAI} & C. White \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>