<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing\n' +
      '\n' +
      'Chong Mou\\({}^{1}\\) Xintao Wang\\({}^{2}\\) Jiechong Song\\({}^{1}\\) Ying Shan\\({}^{2}\\) Jian Zhang\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University\n' +
      '\n' +
      '\\({}^{2}\\)ARC Lab, Tencent PCG\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose **DiffEditor** to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at [https://github.com/MC-E/DragonDiffusion](https://github.com/MC-E/DragonDiffusion).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Text-to-image (T2I) diffusion models [29, 32, 33, 35] have become the mainstream of image generation, praised for their high-quality and diverse generation capability. The pre-trained T2I models can serve as a good generation prior and can be used in various ways, _e.g._, image editing. Since the excellent text-to-image ability, numerous diffusion-based image editing methods are implemented based on the text guidance [5, 6, 11, 12, 13, 16]. However, the generated results of T2I models are usually sensitive to the quality of text [37]. Therefore, text-guided image editing struggles to achieve fine-grained content manipulation.\n' +
      '\n' +
      'Recently, DragGAN [30] provides a user-friendly way to manipulate the image content by point dragging. However, limited by the capacity of GAN [8] models, DragGAN cannot edit general images. Inspired by this interactive editing mode, DragDiff [39] and DragonDiff [28] are proposed based on the pre-trained T2I diffusion model [33]. Empowered by the diverse generation capabilities of the base model, they can perform fine-grained editing on general images. However, their editing process lacks flexibility, as shown in Fig. 2. Concretely, the image editing operation of transforming a lion from a closed mouth to a widely open mouth conflicts with the LORA [34] in DragDiff, resulting in failure. The visual cross-attention designed in DragonDiff also makes it struggle to imagine new content (_e.g._, mouth) that is not present in the source image, causing failure too. In addition, these two methods and most diffusion-based image editing methods employ ordinary differential equations (ODE) [41] solver, a deterministic sampling process. Although ODE can better maintain the consistency between the edited results and the source image, its determinacy also limits flexibility during the editing process. Compared to ODE, stochastic differential equations (SDE) [15] is a stochastic sampling process. Some works [48, 49] study the latent space of SDE for accurate image editing. Unlike these works, we aim to utilize the stochasticity in SDE to improve the flexibility of diffusion-based image editing, as shown in the last image of Fig. 2.\n' +
      '\n' +
      'Another insight is that although DragDiff and DragonDiff utilize feature correspondence in the pre-trained T2I diffusion model to achieve fine-grained image editing, the role of the text input is ignored in their frameworks. Here, we raise a question: _Does the text have no effectiveness in fine-grained image editing, or is there another more suitable form of text input?_ In addition to the text prompt, DALL-E2 [32] presents a novel attempt to generate images conditioned on the image prompt, _i.e._, using images to describe images. Subsequently, some multimodal works [20, 24, 25] and object-customization works [22, 47, 51] are proposed to support image prompts for more detailed content description. Inspired by these works, we introduce image prompts into the fine-grained image editing process, improving editing quality through more detailed content descriptions. In addition, we combine regional score-based gradient guidance and a time travel strategy into diffusion sampling, which further enhances the editing quality.\n' +
      '\n' +
      'In summary, this paper has the following contributions:\n' +
      '\n' +
      '* We present a novel attempt to introduce the image prompt to fine-grained image editing tasks. In conjunction with the image editing algorithm, this design can provide a more detailed description of the editing content, thus improving the editing quality.\n' +
      '* We consider both the flexibility and content consistency of image editing by proposing regional SDE sampling and regional score-based gradient guidance. We also introduce a time travel strategy in diffusion-based image editing to improve the editing quality further.\n' +
      '* Extensive experiments demonstrate that our method can achieve state-of-the-art performance on various fine-grained image editing tasks (_i.e._, content dragging, object moving, resizing, pasting, and appearance replacing, as shown in Fig. 1) with attractive complexity.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Diffusion Model\n' +
      '\n' +
      'Diffusion model [15] is a thermodynamics-driven [40, 42] algorithm, including a diffusion process and a reverse process. In the diffusion process, an image \\(\\mathbf{x}_{0}\\) is gradually added Gaussian noise as \\(q(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\sqrt{\\alpha_{t}}\\mathbf{x}_{0},(1-\\alpha_{t})\\mathbf{I})\\), where \\(\\alpha_{t}\\) linearly decreases from 1 to a sufficiently small number to encourage \\(\\mathbf{x}_{T}\\sim\\mathcal{N}(0,\\mathbf{I})\\). The reverse process is to iteratively recover \\(\\mathbf{x}_{0}\\) from \\(\\mathbf{x}_{T}\\) by training a denoiser, conditioned on the current noisy image \\(\\mathbf{x}_{t}\\) and time step \\(t\\):\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathbf{x}_{0},t,\\boldsymbol{\\epsilon}_{t}\\sim\\mathcal{N}(0,1)} \\left[||\\boldsymbol{\\epsilon}_{t}-\\epsilon_{\\boldsymbol{\\theta}}(\\mathbf{x}_{ t},t)||_{2}^{2}\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\epsilon_{\\boldsymbol{\\theta}}\\) is the function of the denoiser. DDIM [41] defines the diffusion sampling as \\(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathcal{N}(\\sqrt{\\alpha_{t -1}}\\mathbf{x}_{0}+\\sqrt{1-\\alpha_{t-1}-\\delta_{t}^{2}}\\cdot\\frac{\\mathbf{x}_ {t}-\\sqrt{\\alpha_{t}\\mathbf{x}_{0}}}{\\sqrt{1-\\alpha_{t}}},\\alpha_{t}^{2} \\mathbf{I})\\), which is a non-Markovian process and can be formulated as:\n' +
      '\n' +
      '\\[\\begin{split}&\\boldsymbol{x}_{t-1}=\\\\ &\\sqrt{\\alpha_{t-1}}\\underbrace{\\frac{\\boldsymbol{x}_{t}-\\sqrt{1- \\alpha_{t}}\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}}^{t}\\left(\\mathbf{x}_{t }\\right)}{\\sqrt{\\alpha_{t}}}}_{\\text{"predicted }\\mathbf{x}_{0}\\text{ "}}+\\underbrace{\\sqrt{1-\\alpha_{t-1}-\\sigma_{t}^{2}}\\cdot \\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}}^{t}\\left(\\mathbf{x}_{t}\\right)}_{ \\text{"detection pointing to }\\mathbf{x}_{t}\\text{ "}}+\\underbrace{\\sigma_{t}\\boldsymbol{ \\epsilon}}_{\\text{"noise"}},\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'Figure 2: Illustration of editing flexibility limitations in DragDiff [39] and DragonDiff [28], as well as our improvement.\n' +
      '\n' +
      'where \\(\\sigma_{t}=\\eta\\sqrt{\\left(1-\\alpha_{t-1}\\right)/\\left(1-\\alpha_{t}\\right)}\\sqrt{1 -\\alpha_{t}/\\alpha_{t-1}}\\). When \\(\\eta=1\\) for all \\(t\\), it becomes DDPM [15], _i.e_., a stochastic differential equation (SDE). As \\(\\eta=0\\), the sampling process becomes deterministic, _i.e_., an ordinary differential equation (ODE). Most diffusion-based image editing works rely on ODE to achieve better content consistency. [48, 49] explore SDE in diffusion-based image editing.\n' +
      '\n' +
      'Most current works focus on conditional diffusion generation, such as text conditions [29, 33], which have greatly revolutionized the community of image generation. Although promising T2I generation quality is achieved, the generated results are sensitive to text quality and usually require tedious prompt design [37]. In addition to text condition, DALL-E2 [32] presents the first attempt to generate images guided by image prompts. ELITE [47], Bilp-Diffusion [22], and IP-Adapter [51] present the learning of image prompts for object customization. However, the effectiveness of image prompts in fine-grained image editing has hardly been studied.\n' +
      '\n' +
      '### Image Editing\n' +
      '\n' +
      'The primary objective of image editing is to manipulate the content of a given image in a controlled manner. Previous methods [1, 2, 3] usually invert images into the latent space of GANs [8] and then edit the image by manipulating latent vectors. Recently, DragGAN [30] presents a point-dragging formulation for fine-grained image editing. However, limited by the capability of GANs, these methods have weaknesses in model generalization and image quality. Motivated by the success of text-to-image diffusion models [33], various text-guided image editing methods [4, 6, 13, 18, 27] are proposed. The commonly used editing strategies are (1) adding noise and then denoising with target description [7, 18, 21, 27, 46]; (2) using cross-attention maps as an editing medium [11, 13, 16]; (3) using text as editing instructions [6]. However, the correspondence between the text and image in T2I models is weak, making it difficult to achieve fine-grained image editing. Recently, DragDiff [39] and DragonDiff [28] achieve fine-grained image editing based on the feature correspondence [45] in the pre-trained StableDiffusion (SD) [33]. Specifically, DragDiff uses LORA [34] to maintain content consistency and optimizes the latent \\(\\mathbf{z}_{t}\\) in a specific diffusion step. DragonDiff is built based on the score-based [44] gradient guidance [9] and a visual cross-attention design for drag-style image editing without model tuning.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminary: Score-based Editing Guidance\n' +
      '\n' +
      'From the continuous perspective of score-based diffusion [43, 44], the external condition \\(\\mathbf{y}\\) can be combined in a conditional score function, _i.e_., \\(\\nabla_{\\mathbf{x}_{t}}\\log q(\\mathbf{x}_{t}|\\mathbf{y})\\), to sample from a more enriched distribution. The conditional score function can be further decomposed as:\n' +
      '\n' +
      '\\[\\nabla_{\\mathbf{x}_{t}}\\log q(\\mathbf{x}_{t}|\\mathbf{y}) =\\nabla_{\\mathbf{x}_{t}}\\log\\left(\\frac{q(\\mathbf{y}|\\mathbf{x}_ {t})q(\\mathbf{x}_{t})}{q(\\mathbf{y})}\\right) \\tag{3}\\] \\[\\propto\\nabla_{\\mathbf{x}_{t}}\\log q(\\mathbf{x}_{t})+\\nabla_{ \\mathbf{x}_{t}}\\log q(\\mathbf{y}|\\mathbf{x}_{t}),\\]\n' +
      '\n' +
      'where the first term is the unconditional denoiser, _i.e_., \\(\\mathbf{\\epsilon}_{\\theta}^{t}(\\mathbf{x}_{t})\\). The second term refers to the conditional gradient produced by an energy function \\(\\mathcal{E}(\\mathbf{x}_{t},\\mathbf{y})=\\log q(\\mathbf{y}|\\mathbf{x}_{t})\\), measuring the distance between current state \\(\\mathbf{x}_{t}\\) and condition \\(\\mathbf{y}\\). Here, we reformulate Eq. 3 as:\n' +
      '\n' +
      '\\[\\tilde{\\mathbf{\\epsilon}}_{\\theta}^{t}(\\mathbf{x}_{t})=\\mathbf{\\epsilon}_{\\theta}^{t} (\\mathbf{x}_{t})+\\eta\\cdot\\nabla_{\\mathbf{x}_{t}}\\mathcal{E}(\\mathbf{x}_{t}, \\mathbf{y}), \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\eta\\) refers to the learning rate. Recently, Self-Guidance [11] and DragonDiff [28] convert image editing operations into gradient guidance for image editing tasks. The energy function \\(\\mathcal{E}\\) in Self-Guidance is built based on the correspondence [13] between image and text features. DragonDiff constructs the energy function based on image feature correspondence [45] in pre-trained SD, which can achieve more accurate drag-style editing tasks. In this paper, we aim to boost the accuracy and flexibility of diffusion-based image editing with the DragonDiff framework.\n' +
      '\n' +
      'Figure 3: Overview of our proposed DiffEditor, which is composed of a trainable image prompt encoder and a diffusion sampling with editing guidance that does not require training.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      'An overview of our image editing pipeline is presented in Fig. 3. Specifically, given an image \\(\\mathbf{x}_{0}\\) to be edited, we first employ it as image prompts and use an image prompt encoder to embed it. These image embeddings cooperate with text embeddings to form a better description to guide the diffusion process. Then we use DDIM inversion [41] to transform \\(\\mathbf{x}_{0}\\) into a Gaussian distribution \\(\\mathbf{z}_{T}\\) in the latent space of the pre-trained SD [33]. If the reference image \\(\\mathbf{x}_{0}^{ref}\\) exists (_i.e._, in appearance replacing and object pasting), it will also be involved in the inversion. In this process, we follow the design in DragonDiff [28] to store some intermediate features ( \\(\\mathbf{K}_{t}^{gud},\\mathbf{V}_{t}^{gud},\\mathbf{K}_{t}^{ref},\\mathbf{V}_{t}^{ ref}\\)) and latent (\\(\\mathbf{z}_{t}^{gud},\\mathbf{z}_{t}^{ref}\\)) at each time step in a memory bank, which is used to guide subsequent image editing. Note that "gud" and "ref" represent the information of the source and reference image in the inversion process, respectively. In the subsequent generation sampling, we step forward with the cooperation of score-based editing guidance, visual cross-attention, and image prompt. In this process, some elaborately designed strategies (_e.g._, regional gradient guidance, regional SDE, and time travel) enhance the editing further.\n' +
      '\n' +
      '### Content Description with Image Prompt\n' +
      '\n' +
      'Although several fine-grained image editing methods [28, 39] are based on the T2I diffusion model, the role of prompts is ignored as a simple description. Compared to text prompts, image prompts [22, 32, 51] can provide a more detailed content description. In this paper, we find that the image prompt can improve the quality of fine-grained image editing, especially in some complex scenarios.\n' +
      '\n' +
      'Inspired by IP-Adapter [51], the architecture of our image prompt encoder is shown in Fig. 4. Concretely, given an input image \\(\\mathbf{x}_{0}\\), the pre-trained CLIP [31] image encoder embed it to 257 tokens. Then, a linear layer is used to adjust the channel dimension, and a QFormer [23] (without self-attention layer) module is employed to adjust the token numbers to 64 by 64 learnable queries. The QFormer module consists of N (8 by default) submodules, each composed of a cross-attention layer and a feed-forward network (FN). 64 learnable queries serve as queries to extract information from 257 image tokens that act as keys and values. Finally, 257 image tokens are composed into 64 embedding tokens (\\(\\mathbf{c}_{im}\\)), and then they are input into the same cross-attention module as text tokens (\\(\\mathbf{c}\\)) in the SD. To build classifier-free guidance [14] like the text condition, the conditional and unconditional image prompts are jointly trained by randomly dropping (_i.e._, set image to zero) during training. Finally, image tokens and text tokens are processed separately with the query \\(\\mathbf{Q}\\) in the cross-attention module, and the results are added together:\n' +
      '\n' +
      '\\[\\small\\texttt{Att}(\\mathbf{Q},\\mathbf{K}^{{}^{\\prime}},\\mathbf{V}^{{}^{ \\prime}},\\mathbf{K}^{{}^{\\prime\\prime}},\\mathbf{V}^{{}^{\\prime\\prime}})= \\mathcal{S}(\\frac{\\mathbf{Q}(\\mathbf{K}^{{}^{\\prime}})^{T}}{\\sqrt{d}})\\mathbf{ V}^{{}^{\\prime}}+\\gamma\\cdot\\mathcal{S}(\\frac{\\mathbf{Q}(\\mathbf{K}^{{}^{ \\prime\\prime}})^{T}}{\\sqrt{d}})\\mathbf{V}^{{}^{\\prime\\prime}}, \\tag{5}\\]\n' +
      '\n' +
      'where \\((\\mathbf{K}^{{}^{\\prime}},\\mathbf{V}^{{}^{\\prime}})\\) and \\((\\mathbf{K}^{{}^{\\prime\\prime}},\\mathbf{V}^{{}^{\\prime\\prime}})\\) refer to the keys and values from the text and image prompt, respectively. \\(\\gamma\\) is a weight to balance these two terms. \\(\\mathcal{S}\\) is the function of Softmax. Note that in tasks with reference images (_i.e._, object pasting and appearance replacing), \\(\\mathbf{K}^{{}^{\\prime\\prime}}\\) and \\(\\mathbf{V}^{{}^{\\prime\\prime}}\\) are formed by the concatenation of image tokens from the source image and the reference image. During training, we fix the parameters in the pre-trained SD and CLIP image encoder, and we only optimize the linear embedding and QFormer by \\(\\mathcal{L}_{2}\\) loss:\n' +
      '\n' +
      '\\[\\small\\mathbb{E}_{\\mathbf{x}_{0},t,\\mathbf{\\epsilon}_{t}\\sim\\mathcal{N}(0,1)} \\left[||\\mathbf{\\epsilon}_{t}-\\mathbf{\\epsilon}_{\\boldsymbol{\\theta}}^{t}( \\mathbf{z}_{t},\\mathbf{c},\\mathbf{c}_{im})||_{2}^{2}\\right]. \\tag{6}\\]\n' +
      '\n' +
      'After a single training, this module can be integrated into pre-trained SD for various image editing tasks, as demonstrated in this paper.\n' +
      '\n' +
      '### Sampling with Regional SDE\n' +
      '\n' +
      'Maintaining consistency between editing results and original images is a great challenge in fine-grained image editing. Most methods adopt a deterministic sampling process\n' +
      '\n' +
      'Figure 4: Illustration of the design of our image prompt encoder.\n' +
      '\n' +
      'Figure 5: The impact of different components on the editing flexibility of DragonDiff [28].\n' +
      '\n' +
      '(ODE) and utilize DDIM inversion for sampling initialization. In addition, DragDiff [39] uses LORA [34] to constrain the output content, and DragonDiff [28] uses visual cross-attention to maintain content consistency. However, these strategies also compromise editing flexibility, _e.g._, hindering the imagination of new content to harmonize editing operation as shown in Fig. 2. Our further experiments on DragonDiff show that reducing the content consistency strength can improve editing flexibility. As seen in Fig. 5, the editing flexibility is improved when we randomly initialize the sampling starting point \\(\\mathbf{z}_{T}\\) or remove visual cross-attention. When we apply both reductions, the editing objective can be achieved flexibly, but the content consistency is severely compromised. Therefore, in this paper, we explore how to improve editing flexibility without significantly impacting content consistency. In the sampling process (_i.e._, Eq. 2) of DragonDiff, \\(\\sigma_{t}=0\\), which is a deterministic ODE sampling. This leads to the final result being highly dependent on \\(\\mathbf{z}_{T}\\) and the information injected by visual cross-attention. Our solution is to introduce randomness (_i.e._, \\(\\sigma_{t}>0\\)) during the sampling process, while this randomness is controlled within local editing areas and specific time intervals. Here, we use \\(\\mathbf{z}_{t-1}=\\mathcal{F}(\\mathbf{z}_{t};\\sigma_{t})\\) to simplify Eq. 2. Our regional SDE sampling is defined as:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{z}_{t-1}=\\mathbf{m}_{edit}\\cdot\\mathcal{F} (\\mathbf{z}_{t};\\eta_{1}(t))+(1-\\mathbf{m}_{edit})\\cdot\\mathcal{F}(\\mathbf{z }_{t};\\eta_{2}(t)),\\\\ (\\eta_{1}(t),\\eta_{2}(t))=\\left\\{\\begin{array}{ll}(0.4,0.2),&t \\in\\tau_{SDE}\\\\ (0,0),&t\\notin\\tau_{SDE}\\end{array}\\right.\\end{split} \\tag{7}\\]\n' +
      '\n' +
      'where \\(\\mathbf{m}_{edit}\\) locates the editing area. \\(\\tau_{SDE}\\) is the time interval for applying regional SDE. After using this sampling strategy, we can accurately inject flexibility to produce satisfactory results, as shown in the last image of Fig. 2.\n' +
      '\n' +
      '### Editing with Gradient Guidance\n' +
      '\n' +
      '**Regional gradient guidance**. In DragonDiff [28], the energy function \\(\\mathcal{E}\\) consists of two parts, _i.e._, editing \\(\\mathcal{E}_{edit}\\) and content consistency \\(\\mathcal{E}_{content}\\). Although their target areas are independent of each other, the scope of the gradient guidance they generate is global and overlapping, resulting in mutual interference. Concretely, in Fig. 6, we visualize the editing gradient produced by \\(\\mathcal{E}_{edit}\\) in the object moving task. As can be seen, the gradient guidance gradually converges to the editing area as the diffusion sampling proceeds. During this process, there are some activations outside the editing area, and these imprecise activations can affect the content consistency in these unedited areas (details are presented in Sec. 4.3). To rectify this weakness, we use the editing region mask \\(\\mathbf{m}_{edit}\\) to locally combine \\(\\mathcal{E}_{edit}\\) and \\(\\mathcal{E}_{content}\\). Finally, the conditional term in Eq. 3 is defined as:\n' +
      '\n' +
      '\\[\\nabla_{\\mathbf{z}_{t}}\\log q(\\mathbf{y}|\\mathbf{z}_{t})=\\mathbf{m}_{edit} \\cdot\\nabla_{\\mathbf{x}_{t}}\\mathcal{E}_{edit}+(1-\\mathbf{m}_{edit})\\cdot \\nabla_{\\mathbf{x}_{t}}\\mathcal{E}_{content}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\mathbf{y}\\) is the editing target. During the sampling, we only add guidance in the first \\(n\\) time steps.\n' +
      '\n' +
      '**Time travel**. DragDiff [39] treats \\(\\mathbf{z}_{t}\\) as a learnable parameter and iteratively optimizes it within a diffusion step \\(t\\). In contrast, DragonDiff [28] incorporates score-based gradient guidance into each sampling step, _i.e._, Eq. 4. However, applying editing guidance through Eq. 4 once at each\n' +
      '\n' +
      'Figure 6: Visualization of the editing gradient from \\(\\mathcal{E}_{edit}\\) in different sampling steps.\n' +
      '\n' +
      'sampling step lacks refinement for editing, especially in some complex scenarios. _Can we combine the advantages of DragDiff and DragonDiff to build recurrent guidance in the score-based diffusion [44]?_ To address this issue, we build time travel to perform rollbacks,, \\(\\mathbf{z}_{t}\\leftarrow\\mathbf{z}_{t-1}\\), during the sampling process. This strategy has been empirically shown to inhibit the generation of disharmonious results when solving hard generation tasks [26, 52]. However, the rollback strategy (, \\(\\mathbf{z}_{t}\\sim\\mathcal{N}(\\sqrt{1-\\beta_{t-1}}\\mathbf{z}_{t-1},\\beta_{t- 1}\\mathbf{I})\\)) in these works is not suitable in fine-grained image editing tasks. This is because random noise \\(\\mathbf{I}\\) can introduce significant uncertainty, undermining the content consistency of editing results. To ensure the accuracy of rollback, we use deterministic DDIM inversion [41] to roll back \\(\\mathbf{z}_{t-1}\\) to \\(\\mathbf{z}_{t}\\). During sampling, the time travel is performed \\(U\\) (3 in our design) times for each guidance step in a time interval \\(\\tau_{TT}\\).\n' +
      '\n' +
      'Due to the guidance enhancement from regional guidance and time travel, we can achieve editing with fewer guidance time steps,, we introduce gradient guidance every two time steps in sampling. Finally, the algorithm logic of our DiffEditor is defined in Alg. 1.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'We choose Stable Diffusion V1.5 [33] as the base model for image editing. During image prompt training, we use the training data from LAION [36] and process the image resolution to \\(512\\times 512\\). We choose Adam [19] optimizer with an initial learning rate of \\(1\\times 10^{-5}\\). The batch size during training is set as 16. The training process iterates \\(1\\times 10^{6}\\) steps on 4 NVIDIA A100 GPUs. We use the same embedding module to process image prompts in different applications. The inference adopts DDIM sampling with 50\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c} \\hline  & Preparing & Inference & Unaligned & 17 Points.\\(\\downarrow\\) & 68 Points.\\(\\downarrow\\) & FID\\(\\downarrow\\) \\\\  & complexity\\(\\downarrow\\) & complexity\\(\\downarrow\\) & face & From 57.19 & From 36.36 & 17/68 points \\\\ \\hline UserControllableLT [10] & **1.1**s & **0.04**s & & 32.32 & 24.15 & 51.20/50.32 \\\\ DragGAN [30] & 50.22s & 6.28s & & **15.96** & **10.60** & 39.27/39.50 \\\\ DragDiff [39] & 42.37s & 19.52s & & 22.95 & 17.32 & 38.06/36.55 \\\\ DragonDiff [28] & 3.53s & 15.00s & & 18.51 & 13.94 & 35.75/34.58 \\\\ DiffEditor (Ours) & 3.53s & 13.88s & & 17.05 & 11.52 & **33.10/33.02** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative evaluation on face manipulation with 68 and 17 points. The accuracy is calculated by MSE distance between edited points and target points. The initial distance (, 57.19 and 36.36) is the upper bound, without editing. FID [38] is utilized to quantize the editing quality of different methods. The time complexity is computed on the ‘1 point’ dragging.\n' +
      '\n' +
      'Figure 7: Qualitative comparison between our DiffEditor and other methods in face manipulation. The current and target points are labeled with red and blue. The white line indicates distance. The MSE distance between the result and the target is labeled in yellow.\n' +
      '\n' +
      'steps, and we set the classifier-free guidance scale as 5.\n' +
      '\n' +
      '### Comparison\n' +
      '\n' +
      '**Time complexity.** We divide the time complexity of different methods into preparing and inference stages. The preparing stage involves Diffusion/GAN inversion and model tuning. The inference stage generates the editing result from latent representation. The time complexity for each method is tested on one point dragging, with the image resolution being \\(512\\times 512\\). All times are tested on an NVIDIA A100 GPU with Float32 precision. The results in Tab. 1 present the attractive preparing complexity of our method, and the inference complexity is lower than existing diffusion-based methods, _i.e._, DragDiff and DragonDiff.\n' +
      '\n' +
      '**Performance.** First, we evaluate our method on content dragging by comparing it with some well-known GAN-based methods (_i.e._, UserControllableLT [10], DragGAN [30]) and recent diffusion-based methods (_i.e._, DragDiff [39], DragonDiff) on the keypoint-based face manipulation. We used the same test set as DragonDiff, _i.e._, 800 aligned faces from the CelebA-HQ [17] training set. We evaluate the editing performance under 17-point guidance and 68-point guidance. To quantify editing accuracy, we calculated the MSE distance between the landmarks of the edited result and the target landmarks. In addition, we calculate FID [38] between the editing results and the CelebA-HQ training set to represent the image quality. The quantitative comparison is presented in Tab. 1. One can see that our method has a significant improvement in accuracy and generation quality compared to other diffusion-based methods, achieving comparable editing accuracy to DragGAN.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline  & Pasting & Moving & Replacing \\\\ \\hline Pain-by-example & 0.265 & - & - \\\\ Self-Guidance & - & 0.246 & 0.243 \\\\ DragonDiff & 0.260 & 0.282 & 0.263 \\\\ Ours & 0.274 & 0.288 & 0.281 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative evaluation on object pasting, object moving, and appearance replacing. The result is calculated by CLIP [31]\\(\\uparrow\\) distance between editing results and target descriptions.\n' +
      '\n' +
      'Figure 8: Visual comparison between our method and other methods on appearance replacing, object pasting and object moving tasks.\n' +
      '\n' +
      'Figure 10: The first and second rows show the effectiveness of the image prompt in DDIM inversion and image editing, respectively.\n' +
      '\n' +
      'Figure 9: Visual comparison between our image prompt and IP-Adapter-plus.\n' +
      '\n' +
      'Although DragGAN has higher editing accuracy on aligned faces, its base model is specifically trained for aligned faces and cannot edit general faces, as shown in the last row of Fig. 7. The qualitative comparison in Fig. 7 shows that our method has high editing accuracy and content consistency while maintaining good flexibility. For example, in the case where teeth need to be imagined, our DiffEditor can produce more natural results. In contrast, DragDiff and DragOnDiff have difficulties in imagining new content.\n' +
      '\n' +
      'In addition to content dragging, we also compare with Paint-by-example [50] in object pasting, and we compare our method with Self-Guidance [11] and DragonDiff in object moving and appearance replacing tasks. The results are presented in Fig. 8. As can be seen, although the specially trained Paint-by-example can naturally integrate objects into an image, it is difficult to maintain the original object identity. Our method performs better in object identity and has richer texture details than DragonDiff. In object moving and appearance replacing tasks, text-guided Self-Guidance lacks consistent constraints, making editing results deviate from the original image. The editing accuracy of DragonDiff still has room for improvement, _e.g._, color and details. In comparison, our method has better content consistency and editing accuracy. For quantization, we compute the CLIP [31] distance between the edited results and the target description. We select 16 editing samples for each task. The results in Tab. 2 demonstrate the promising performance of our method.\n' +
      '\n' +
      '**Discussion between our image prompt and IP-Adapter.** As mentioned above, there are several methods proposed to use images as prompts to provide more accurate and customized descriptions for the generated results, such as IP-Adapter [51]. However, most of these methods focus on object customization, and overemphasis on detail description will compromise their performance. Therefore, IP-Adapter compresses the image into a small number of tokens to avoid detail descriptions. This paper studies introducing image prompts into fine-grained image editing. We use the Q-Former structure to map the image into 64 tokens to enhance the detail expression ability. Fig. 9 shows that IP-Adapter not ideal for direct insertion into fine-grained image editing tasks due to the lack of detail description. Our method can enhance the consistency of texture details between the edited result and the original image.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'In this part, we study the effectiveness of some components in our DiffEditor.\n' +
      '\n' +
      '**Image prompt.** Image prompt provides a detailed description of the editing content in our editing pipeline. We conduct an experiment in Fig. 10 to demonstrate its effectiveness. First, we apply it in the pure DDIM inversion and then reconstruction. The result in the first row presents that DDIM inversion based on the text prompt exhibits noticeable distortions and is unstable. After using our image prompt, DDIM inversion can reconstruct stable and high-fidelity results. In the second row of Fig. 10, we show the editing with and without the image prompt. It can be seen that the image prompt provides a better generation prior for editing content, reducing the probability of distortion.\n' +
      '\n' +
      '**Regional gradient guidance.** To rectify the interference between different gradient guidance, we use Eq. 8 to produce the guidance. We demonstrate its effectiveness in Fig. 11 by only using \\(\\mathbf{m}_{edit}\\cdot\\mathcal{E}_{edit}\\) and \\(\\mathcal{E}_{edit}\\). Note that we remove the content consistency guidance \\(\\mathcal{E}_{content}\\) to highlight the interference. The results show that if the actuating range of \\(\\mathcal{E}_{edit}\\) is not constrained, the editing gradient will have an impact on the consistency of some unrelated areas, _e.g._, distortion of the fingers in the background. After applying regional constraints, the content of the background part has better consistency even without \\(\\mathcal{E}_{content}\\).\n' +
      '\n' +
      '**Time travel.** Time travel is used to build recurrent guidance in a single diffusion time step, thereby refining the editing\n' +
      '\n' +
      'Figure 11: Effectiveness of the regional gradient guidance. In experiment, we remove the content consistency gradient \\(\\mathcal{E}_{content}\\).\n' +
      '\n' +
      'Figure 12: Visualization of editing without time travel, with random time travel, and with our accurate time travel.\n' +
      '\n' +
      'effect. We present its effectiveness in Fig. 12. As can be seen, the editing result in the complex scenario has distortions without the time travel strategy. If using the random time travel (_i.e._, \\(\\mathbf{z}_{t}\\sim\\mathcal{N}(\\sqrt{1-\\beta_{t-1}}\\mathbf{z}_{t-1},\\beta_{t-1} \\mathbf{I})\\)), the randomness will affect the consistency between the editing result and the original image. After adopting our accurate time travel, the editing quality is improved.\n' +
      '\n' +
      '### Generalization of Different Components\n' +
      '\n' +
      'Except image prompt encoder that requires a specific SD model, other components of our method are designed based on diffusion theory, giving them good generalization. Fig. 13 shows editing results of these components on different versions of SD and custom SD tuned on specific data. It shows the promising generalization of these components on different diffusion models. The reason we employ SD-1.5 is that the content of editing results mostly comes from existing images, thus the dependence on generation ability is relatively small. Large generative models, like SDXL, produce high computational costs in gradient guidance. Hence, we use the efficient SD-1.5 as the base model.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we aim to rectify two issues in diffusion-based fine-grained image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operation, _e.g._, imagine new content. In our solution, we introduce the image prompt into fine-grained image editing, which can provide a more detailed content description for the edited image and reduce the probability of distortion. This method can be plugged into various fine-grained image editing tasks without task-specific training. To improve the editing flexibility, we propose a regional SDE strategy to inject randomness into the editing area while maintaining content consistency in other areas. Furthermore, we introduce regional score-based gradient guidance and a time travel strategy into the editing process to improve the editing quality further. Extensive experiments demonstrate that our method can achieve promising performance in various fine-grained image editing tasks, _i.e._, object moving, resizing, pasting, appearance replacing, and content dragging. The complexity is also reduced compared with existing diffusion-based methods.\n' +
      '\n' +
      '**Limitations** Although our method improves the flexibility of diffusion-based image editing and reduces the probability of distortion, editing difficulties still exist in some scenarios that require a large amount of content imagination, such as rotating a car by dragging its front. We think that this is due to the base model SD. It has a diverse generation space but lacks 3D perception of individual objects. In our future work, we will enhance the editing capabilities of diffusion models in this regard.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4432-4441, 2019.\n' +
      '* [2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8296-8305, 2020.\n' +
      '* [3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In _Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition_, pages 18511-18521, 2022.\n' +
      '* [4] Omri Avraham, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18208-18218, 2022.\n' +
      '\n' +
      'Figure 13: Visualization of editing results on different diffusion models.\n' +
      '\n' +
      '* [5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.\n' +
      '* [7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. _arXiv preprint arXiv:2304.08465_, 2023.\n' +
      '* [8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. _IEEE signal processing magazine_, 35(1):53-65, 2018.\n' +
      '* [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* [10] Yuki Endo. User-controllable latent transformer for stylegan image layout editing. In _Computer Graphics Forum_, pages 395-406. Wiley Online Library, 2022.\n' +
      '* [11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. _arXiv preprint arXiv:2306.00986_, 2023.\n' +
      '* [12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. _arXiv preprint arXiv:2212.05032_, 2022.\n' +
      '* [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [16] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. _arXiv preprint arXiv:2310.01506_, 2023.\n' +
      '* [17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. _arXiv preprint arXiv:1710.10196_, 2017.\n' +
      '* [18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.\n' +
      '* [19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [20] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. _arXiv preprint arXiv:2305.17216_, 2023.\n' +
      '* [21] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representation. _arXiv preprint arXiv:2209.15264_, 2022.\n' +
      '* [22] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_, 2023.\n' +
      '* [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11461-11471, 2022.\n' +
      '* [27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [28] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. _arXiv preprint arXiv:2307.02421_, 2023.\n' +
      '* [29] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pages 16784-16804. PMLR, 2022.\n' +
      '* [30] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. _arXiv preprint arXiv:2305.10973_, 2023.\n' +
      '* [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763, 2021.\n' +
      '* [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [34] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023.\n' +
      '* [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.\n' +
      '* [36] Christoph Schuhmann.\n' +
      '* [37] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. A picture is worth a thousand words: Principled recaptioning improves image generation. _arXiv preprint arXiv:2310.16656_, 2023.\n' +
      '* [38] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. [https://github.com/mseitzer/pytorch-fid](https://github.com/mseitzer/pytorch-fid), 2020. Version 0.3.0.\n' +
      '* [39] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. _arXiv preprint arXiv:2306.14435_, 2023.\n' +
      '* [40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [42] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [43] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.\n' +
      '* [44] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [45] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. _arXiv preprint arXiv:2306.03881_, 2023.\n' +
      '* [46] Dani Valevski, Matan Kalman, Eyal Molard, Eyal Segalis, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning a diffusion model on a single image. _ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* [47] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. _arXiv preprint arXiv:2302.13848_, 2023.\n' +
      '* [48] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models\' latent space, with applications to cyclediffusion and guidance. _arXiv preprint arXiv:2210.05559_, 2022.\n' +
      '* [49] Chen Henry Wu and Fernando De la Torre. A latent space of stochastic diffusion models for zero-shot image editing and guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7378-7387, 2023.\n' +
      '* [50] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18381-18391, 2023.\n' +
      '* [51] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_, 2023.\n' +
      '* [52] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. _arXiv preprint arXiv:2303.09833_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>