<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _Warm_: On the Benefits of Weight Averaged Reward Models\n' +
      '\n' +
      'Alexandre Rame\n' +
      '\n' +
      'Nino Vieillard\n' +
      '\n' +
      'Leonard Hussenot\n' +
      '\n' +
      'Robert Dadashi\n' +
      '\n' +
      'Geoffrey Cideron\n' +
      '\n' +
      'Olivier Bachem\n' +
      '\n' +
      'Johan Ferret Google DeepMind\n' +
      '\n' +
      'alexandreame@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (_WARM_), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, _WARM_ improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-\\(N\\) and RL methods, shows that _WARM_ improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with _WARM_ has a 79.4% win rate against a policy RL fine-tuned with a single RM.\n' +
      '\n' +
      'A _WARM_: On the Benefits of Weight Averaged Reward Models\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '**Reward modeling.** Conversational assistants such as Gemini [1] or GPT-4 [2] have revolutionized the AI community and beyond. These LLMs are capable of completing novel and intricate tasks, including mathematics, coding, and tool use [3]. These advancements are underpinned by a systematic three stage training procedure: pre-training by next token prediction [4, 5, 6], supervised fine-tuning (SFT) to learn to follow instructions [7, 8, 9], and ultimately, reinforcement learning (RL) to maximize a reward encapsulating the desired behaviors [10]. However, defining such rewards for real-world tasks is non-trivial [11]. In reinforcement learning from human feedback (RLHF) [12, 13, 14, 15], rewards are reward models (RMs), trained on binary preference datasets to emulate human judgment. The enhancement of LLM capabilities from RL is strongly tied to the quality of the RMs [16].\n' +
      '\n' +
      '**Reward hacking.** Particularly insidious in RLHF [17, 18] is the _reward hacking_ issue [19, 20, 21, 22] (a.k.a. reward overoptimization), arising from _reward misspecification_[23, 24] between the proxy RM and actual human preferences. While optimizing for the RM initially provides improvements, in later stages the policy (i.e., the LLM being trained) usually learns to exploit loopholes in the RM and achieves high rewards without truly fulfilling the intended objectives, as illustrated in Figure 1(b). This reward hacking phenomenon poses numerous issues. First, it degrades performances, manifesting as linguistically flawed [25] or unnecessarily verbose [26] outputs, which do not reflect true human preferences. Second, it complicates checkpoint selection due to the unreliability of the proxy RM, echoing Goodhart\'s Law [27]: "when a measure becomes a target, it ceases to be a good measure". Third, it can engender sycophnancy [28, 29] or amplify social biases, reflecting the limited and skewed demographics of feedback providers [30, 31]. Lastly and most critically, misalignment [32, 33] due to reward hacking can escalate into safety risks [19, 34, 35], in particular given the rapid integration of LLMs in everyday life and critical decision-making. Such concerns underscore the need to mitigate reward hacking to ensure the beneficial and safe deployment of LLMs.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      'leading to diverse RMs. A key contribution is how the different RMs are merged: by _linear interpolation in the weight space_. This follows the findings from the linear mode connectivity (LMC) [44, 45] and weight averaging (WA) literature [46, 47, 48]: under shared pre-training, the different weights can be linearly interpolated despite the non-linearities in the architecture.\n' +
      '\n' +
      '**On the benefits of _WARM_.** Firstly, _WARM_ stands out for its efficiency and practicality. By requiring a single model at inference time, it provides a scalable approximation to the traditional, costlier ensembling of predictions, without its memory and inference burdens. Secondly, _WARM_ improves reliability by inheriting from the generalization abilities of WA under distribution shifts, a quality well-documented in the OOD literature for supervised learning [47, 48, 49]. Lastly, _WARM_ improves robustness to label corruption. We show that WA selects the invariant predictive mechanisms [50, 51] across different runs [52, 53], thus naturally diminishing the memorization of corrupted samples, occurring in each run in different ways. In contrast, ENS simply memorizes the corrupted samples. We also explain why reducing memorization when modeling noisy preferences enhances stability in the RL process. These multifaceted benefits of _WARM_ are further explored in Section 4.\n' +
      '\n' +
      'We summarize our contributions as follows.\n' +
      '\n' +
      '1. _Innovation in reward modeling_. We introduce _WARM_, the first instance of weight averaging for reward modeling. This novel strategy efficiently mitigates reward hacking, improves reliability under distribution shifts and robustness to label corruption.\n' +
      '2. _Theoretical and empirical insights into weight averaging_. We validate linear mode connectivity for reward models trained on binary preference datasets. Moreover, we reveal a key difference between weight and prediction averaging, that appears clearly under label corruption; weight averaging only maintains the invariant predictive mechanisms across runs, thereby diminishing memorization and enhancing the focus on generalizable features.\n' +
      '\n' +
      'Our experiments on summarization tasks in Section 5 confirm that _WARM_ improves performance without any memory or inference overhead, either when used as the reward selector in best-of-\\(N\\), or as the proxy RM in RL. _WARM_ mitigates reward hacking, and thus provides better downstream policies; specifically, it leads to a win rate of 79.4% (according to the preference oracle metric) against a policy trained with a standard RM.\n' +
      '\n' +
      '## 2 Context and challenges\n' +
      '\n' +
      '### Context\n' +
      '\n' +
      '**LLMs.** We consider an LLM \\(f_{\\theta}\\) of a fixed non-linear architecture parameterized by \\(\\theta\\), usually a Transformer with attention layers [54]. It defines a policy by mapping prompt inputs \\(x\\) to \\(f_{\\theta}(x)\\). Following the foundation model paradigm [55] and the success of transfer learning [56], the weights \\(\\theta\\) are first pre-trained [4] on the vast amount of web data into \\(\\theta^{\\text{pr}}\\), before supervised fine-tuning (SFT) [7] to learn to follow instructions into \\(\\theta^{\\text{sf}}\\). However, the high cost and limited scope of instruction data (i.e., prompts and responses) can create a misalignment [19, 32, 33] between the LLM and its intended application. Reinforcement learning (RL) as a third step in the training process of LLMs was shown to help alignment of LLMs with the intended usage [40].\n' +
      '\n' +
      '**RMs.** A notable aspect of RL is the absence of supervised samples to be imitated by the policy; instead, the focus shifts to maximizing the reward of generated samples, that should measure their quality. The challenge is that the oracle reward, perfectly encapsulating the desired behaviors, is not given by the environment. The key innovation from RLHF [12] is that this reward is the output of a reward model (RM), trained in a supervised way to predict and thus reflect human preferences. Specifically,an RM is an LLM \\(r_{\\phi}\\) parameterized by \\(\\phi\\), predicting a single scalar as the reward \\(r_{\\phi}(x,y)\\) for a prompt \\(x\\) and generation \\(y\\). The weights \\(\\phi\\) are usually initialized from \\((\\theta^{sf},\\omega)\\), where the final linear layer \\(\\omega\\) is added on top of the extracted features from the SFT model \\(\\theta^{sf}\\). Then \\(\\phi\\) is trained on a preference dataset \\(\\mathcal{D}_{train}=\\{x_{d},y_{d}^{+},y_{d}^{-}\\}_{d=1}^{D}\\) where the generation \\(y_{d}^{+}\\) has been preferred over \\(y_{d}^{-}\\) to continue \\(x_{d}\\). Usually human labelers evaluate those generations, but recent works on RLAIF [57, 58] showed that similar performances can be obtained by prompting an LLM for AI feedback. Following the Bradley-Terry [59] assumption about the distribution of preferences, and by framing the problem as binary classification, the maximum likelihood principle motivates learning \\(\\phi\\) by minimizing the following negative log-likelihood loss (where \\(\\sigma\\) is the logistic function):\n' +
      '\n' +
      '\\[\\mathcal{L}_{R}\\big{(}r_{\\phi},\\mathcal{D}_{train}\\big{)}=-\\mathbb{E}_{(x,y^ {+},y^{-})\\in\\mathcal{D}_{train}}\\big{[}\\log\\sigma\\big{(}r_{\\phi}(x,y^{+})-r_{ \\phi}(x,y^{-})\\big{)}\\big{]}\\,. \\tag{1}\\]\n' +
      '\n' +
      '**Reward inference**. With this RM, the literature suggests applying any kind of RL algorithm (usually REINFORCE [60] or PPO [61]) to fine-tuned \\(\\theta^{sf}\\) into \\(\\theta^{sf}\\), as analyzed in Section 5.2. A training-free alternative is best-of-\\(N\\) (BoN) sampling, analyzed in Section 5.1, which returns the generation that has the highest reward among \\(N\\) generations from \\(\\theta^{sf}\\). Both methods aim to align the policy with human preferences. Yet, the _reward misspecification_[23] between the proxy RM and the true human preferences can lead to _reward hacking_[19, 20, 21, 22], where the policy exploits loopholes in the proxy RM to artificially increase the score without matching human preferences.\n' +
      '\n' +
      '### Challenges in reward modeling\n' +
      '\n' +
      'When handling rich inputs such as text, or when assessing complex behaviours, designing rewards aligned with human preferences is a complex challenge for two main reasons, described below.\n' +
      '\n' +
      '**Distribution shifts.** The primary challenge is the distribution shifts resulting from the offline nature of preference data. Indeed, the generations in the preference dataset and those from the policy \\(\\theta^{sf}\\) do not necessarily follow the same distributions, and the shifts can become even more pronounced due to model drift during RL. The OOD generalization literature has extensively analyzed the repercussions of these shifts. Firstly, they often lead to a reduction in performance [62, 63]. RMs (of limited capacity) trained on narrow data distributions may rely on spurious correlations [51] or a limited number of features [64], thus failing when encountering OOD examples [65, 66]. Secondly, they complicate the selection of RMs, as ID validation metrics may poorly correlate with real-world OOD performances [67, 68] and the ability to guide the RL [41]. Lastly, RMs can become poorly calibrated [69] in OOD scenarios [70, 71], and predict more extreme values as rewards. Such miscalibration exacerbates the problem in a negative feedback loop, further intensifying model drift and distribution shifts. In conclusion, limited data coverage during reward modeling reduces the reliability of the RM and facilitates reward hacking [36] in regions where the RM is badly specified.\n' +
      '\n' +
      '**Inconsistent preferences.** The second major challenge is the label noise in preference datasets. Human labelers, often grappling with fatigue, misunderstandings [72, 73] and imperfect incentives [74], might default to simpler criteria such as length, bullet points, or politeness rather than more causal indicators. This tendency is exacerbated for complex tasks [38] or when considering multiple objectives, ranging from harmlessness [75] to engagement [76] and representing the heterogeneity of human opinions. Consequently, these factors lead to low inter-rater agreement, where human data appears as an imperfect representation of the underlying ground truth [77, 78]. To mitigate these issues, there has been a shift towards AI-generated preferences [57, 58], which, while reducing human labor costs, introduces its own set of noise and failure cases, such as sensitivity to prompting strategies [79, 80]. These layers of noise and inconsistency challenge the robustness of the RM, and its ability to provide stable signals.\n' +
      '\n' +
      'With this in mind, a good RM should ideally satisfy the three following properties.\n' +
      '\n' +
      '_Property 1: efficiency._ The RM should incur no memory or inference overhead. Then the policy can be optimized efficiently.\n' +
      '\n' +
      '_Property 2: reliability._ The RM should reliably reward predictions despite the distribution shifts. Then the policy can explore away from its initialization while relying on the RM.\n' +
      '\n' +
      '_Property 3: robustness._ The RM should be robust to the label inconsistencies in binary preferences. Then the policy can learn from robust signals given by the RM.\n' +
      '\n' +
      '### Existing approaches\n' +
      '\n' +
      'To tackle those issues, previous works have explored a few research directions, further detailed in our related work from Appendix A.2. During RL, the standard strategy is to encourage the policy to remain close to its SFT initialization with Kullback-Leibler (KL) regularization [81, 82]; KL reduces model drift [83, 84] but can cause underfitting and adds an extra hyperparameter (the regularization strength \\(\\alpha\\)). Collecting, labelling and then training on new data (reflecting the evolving policy) can improve the reliability of the RM [16]. Yet it poses significant efficiency challenges due to the continuous requirement for human annotation and computational resources. In contrast, _active learning_ strategies [85, 86] proactively enrich the preference dataset by seeking out a diverse set of generations and potential failure cases. Concurrent work [87] suggests applying label smoothing and flipping. Finally, and most similar to _WARM_, _prediction ensembling_ (ENS) [43] strategies average the logits from \\(M\\) RMs. From a bias-variance perspective [88], ENS reduces the variance term when members are sufficiently diverse [89], and thus favors reliability under distribution shifts where variance is the key issue [47]. From a RL perspective, ENS was shown to mitigate hacking risks [12, 41, 42]. Despite its advantages, ENS faces efficiency challenges; the memory and inference costs grow linearly with \\(M\\), making ENS incompatible with the scaling trend in RMs, where larger architectures consistently perform better [90]. Moreover, we will also show in Section 4.2 that ENS fails to improve robustness to preference inconsistencies.\n' +
      '\n' +
      '## 3 Warm\n' +
      '\n' +
      '### Weight averaging of reward models\n' +
      '\n' +
      'Facing those challenges in reward modeling and the limitations from existing approaches, we propose Weight Averaged Reward Models (_WARM_). _WARM_ is a simple and efficient strategy that combines multiple models without the memory and inference overheads of prediction ensembling, enhancing reward reliability (under distribution shifts) and robustness (amidst noisy preference dataset). _WARM_ is illustrated in Figure 1(a) and described below.\n' +
      '\n' +
      '1. _Shared pre-trained initialization._ For a given pre-trained LLM, each RM is initialized from \\((\\theta^{sf},\\omega)\\) combining SFT weights and a linear probed [91] classifier.\n' +
      '2. _Diverse fine-tunings._ We run \\(M\\) RM fine-tunings, optimizing Equation (1) with diverse hyperparameters (as in a grid search), yielding \\(M\\) weights \\(\\{\\phi_{i}\\}_{i=1}^{M}\\).\n' +
      '3. _Weight averaging._ We average those \\(M\\) weights together to form \\(\\phi^{\\text{WARM}}=\\frac{1}{M}\\sum_{i=1}^{M}\\phi_{i}\\).\n' +
      '\n' +
      'Then \\(r_{\\phi^{\\text{WARM}}}\\) serves as the proxy RM to guide the RL procedure, as efficiently as an individual RM, but with the enhanced reliability and robustness provided by the WA strategy, that leverages the strengths and mitigates the weaknesses of the individual RMs.\n' +
      '\n' +
      '### Linear mode connectivity\n' +
      '\n' +
      'Compared to ENS, the main difference lies in how _WARM_ combines the different RMs: we do so through _linear interpolation in the weight space_. It relies on the linear mode connectivity (LMC) [44, 45] property across fine-tuned weights, i.e., the fact that the accuracy of the interpolated model is at least as good as the interpolation of the individual accuracies. Precisely, by defining the pairwise accuracy of an RM \\(r_{\\phi}\\) w.r.t. a dataset \\(\\mathcal{D}\\) as \\(\\operatorname{Acc}(r_{\\phi},\\mathcal{D})=\\mathbb{E}_{(x,y^{\\prime},y^{ \\prime})\\in\\mathcal{D}}\\left[\\mathds{1}_{r_{\\phi}(x,y^{\\prime})\\geq r_{\\phi}( x,y^{\\prime})}\\right]\\), the following Observation 1 underpins the success of _WARM_.\n' +
      '\n' +
      '**Observation 1** (LMC).: _Given two fine-tuned weights \\(\\phi_{1}\\) and \\(\\phi_{2}\\) with a shared pre-training and a test dataset \\(\\mathcal{D}_{test}\\), then for all \\(\\lambda\\in[0,1]\\),_\n' +
      '\n' +
      '\\[\\operatorname{Acc}(r_{(1-\\lambda)\\cdot\\phi_{1}+\\lambda\\cdot\\phi_{2}}, \\mathcal{D}_{test})\\geq(1-\\lambda)\\times\\operatorname{Acc}(r_{\\phi_{1}}, \\mathcal{D}_{test})+\\lambda\\times\\operatorname{Acc}(r_{\\phi_{2}},\\mathcal{D}_ {test}). \\tag{2}\\]\n' +
      '\n' +
      'We empirically validate this LMC in Figure 3, by evaluating interpolated RMs on OOD test samples. This follows similar observations for multi-class classification in the context of computer vision [44, 45], which led to a plethora of weight averaging (WA) works such as the model soups [46, 47, 48] variants (detailed in our related work in Appendix A.1).\n' +
      '\n' +
      '**Remark 1** (Importance of pre-training and linear probing).: _The efficacy of WA can be surprising given the non-linearities [54] and permutation symmetries [92] in deep neural network architectures. WA is actually possible only because of the shared pre-training which constrains the divergence during fine-tunings [45], such as the weights remain in convex regions of the loss valley [93]. In contrast, the LMC does not hold when training weights from scratch [45], even if the random initialization is shared. For these reasons and to facilitate the LMC, we follow [47, 48] and use linear probing to initialize the classifier \\(\\omega\\); compared to random initialization, such linear probing prevents feature distortion [91]._\n' +
      '\n' +
      '### Sources of diversity\n' +
      '\n' +
      'On one hand, _WARM_ requires shared pre-training so that the fine-tuned weights remain linearly connected. On the other hand, weights must not be identical: actually, the diversity across those fine-tuned weights significantly contributes to the accuracy gains observed in WA [47]. Overall, an effective _WARM_ requires a delicate trade-off between ensuring LMC and diversity across weights.\n' +
      '\n' +
      'In practice, we use the following sources of diversity [94], leading the RM fine-tunings to _diverse yet linearly connected_ models. First, the different fine-tunings see the data samples in _different orders_. Second, we sample slightly _different hyperparameters_, notably different learning rates and dropout probabilities, as detailed in Appendix B.3. Third, we investigate a new source of _diversity in initialization_ named _Baklava_, illustrated in Figure 2. Specifically, we initialize the RMs\' featurizers from different checkpoints \\(\\{\\theta_{i}^{sf_{1}}\\}_{i=1}^{M}\\) collected along a given SFT trajectory. _Baklava_ relaxes the shared initialization constraint from model soups [46] to simply sharing the same pre-training: _Baklava_ is actually an efficient alternative to model ratatouille [48] but without the need of multiple auxiliary tasks. Overall, _Baklava_ increases diversity compared to only initializing from the last SFT checkpoint, while adhering to the shared pre-training requisite for LMC, without incurring any overhead.\n' +
      '\n' +
      '**Remark 2** (Moving average).: _Following stochastic weight average [95] or moving average [96], we also tried to average checkpoints collected along a single RM fine-tuning. Though interesting because less costly for training, the lower results in Figure **3(a)** suggest that the accuracy-diversity trade-off was not favorable: incorporating early checkpoints would compromise individual accuracies, and considering only later checkpoints would not bring the necessary diversity. As a result, we opted to use in WARM only the last checkpoint from each RM fine-tuning._\n' +
      '\n' +
      '## 4 On the benefits of _Warm_\n' +
      '\n' +
      'We now explore the properties and benefits from the _WARM_ strategy, previously described in Section 3. We ground our analysis on the empirical comparison between WA and ENS for reward modeling, and a novel general theoretical comparison in Section 4.3.\n' +
      '\n' +
      '**Experimental setup.** We leverage the TL;DR summarization benchmark [97], a standard in reward modeling for LLMs, that we briefly describe below and further detail in Appendix B. The goal of the RMs is to score summaries such as they are ranked properly. In training, we use the dataset \\(\\mathcal{D}_{train}\\) from Stiennon _et al._[14] where the candidate summaries are generated by GPT-3 [6] variants. To obtain the labels, we follow the RLAIF procedure from [58], where a PaLM-L [98] is prompted with chain-of-thought [99] to generate feedback mimicking human preferences. This strategy performs similarly to human labelers with similar inter-agreement, and will be useful in Section 5 as an oracle metric. The RMs are PaLM-XXS models, pre-trained and SFT-ed on the preferred summaries from \\(\\mathcal{D}_{train}\\), on which we plug a linear probed [91] classification layer. We train the RMs for 10k steps on \\(\\mathcal{D}_{train}\\), with hyperparameters and procedure detailed in Appendix B.3. We report accuracies of those RMs on a novel out-of-distribution (OOD) test dataset \\(\\mathcal{D}_{ood}\\) with 92k pairwise comparisons where the summaries are generated by multiple PaLM-XS policies with high temperature, some of which are pre-trained only, others SFT-ed and others RLHF-ed.\n' +
      '\n' +
      '### 1st order analysis: weight averaging for reliable and more efficient ensembling\n' +
      '\n' +
      'Previous works [46, 47, 95] have argued that the best way to understand WA is as an efficient approximation of ENS, as clarified in Observation 2.\n' +
      '\n' +
      '**Observation 2** (WA and ENS: 1st order analysis).: _Weight averaging and prediction ensembling perform similarly: i.e., for all \\(\\lambda\\in[0,1]\\) and a test dataset \\(\\mathcal{D}_{test}\\),_\n' +
      '\n' +
      '\\[\\text{Acc}(r_{(1-\\lambda)\\cdot\\phi_{1}+\\lambda\\cdot\\phi_{2}},\\mathcal{D}_{test })\\approx\\text{Acc}((1-\\lambda)\\times r_{\\phi_{1}}+\\lambda\\times r_{\\phi_{2}},\\mathcal{D}_{test}). \\tag{3}\\]\n' +
      '\n' +
      'Figure 2: \\(|\\)_Baklava diversity procedure._ Starting from a pre-trained LLM \\(\\theta^{\\pi t}\\), we consider different checkpoints \\(\\{\\theta_{i}^{ft}\\}_{i=1}^{M}\\) along a single SFT run (dashed arrow - - ) collected at different number of SFT training steps. Those checkpoints serve as initializations for \\(M\\) RM fine-tunings on the preference dataset (thick solid arrows --) to learn the \\(\\{\\phi_{i}\\}_{i=1}^{M}\\). Finally, those RMs are weight averaged (dotted arrows - - ) into the final model \\(\\phi^{\\text{WARM}}\\). Following the culinary analogy from model soups [46] and model ratatoulike [48], we named this method _Baklava_ because of its diamond geometric shape.\n' +
      '\n' +
      'Theoretically, a simple Taylor expansion can justify this similarity when \\(\\|\\phi_{1}-\\phi_{2}\\|\\ll 1\\). Empirically, this is validated in Figure 3 where the accuracy curves on \\(\\mathcal{D}_{\\mathit{ood}}\\) for WA and ENS closely match. This similarity justifies that WA is a variance reduction method; then, because variance is the dominant issue under distribution shifts [47], this explains the significant gains in Figure 3 over the individual RMs \\(\\phi_{1}\\) and \\(\\phi_{2}\\) (validating Observation 1), in particular when weights are sufficiently diverse. This suggests improved reliability in _WARM_, with efficiency benefits over ENS: indeed, WA maintains a single set of weights, removing the memory and inference overheads from ENS.\n' +
      '\n' +
      'Figure 4: **Corruption experiment validating Observation 3**. We consider \\(\\phi_{1}\\) and \\(\\phi_{2}\\), two RMs fine-tuned independently with the same config as in Figure 3(b), but this time with 25% of the training labels corrupted. We then report the performances of their WA and ENS on the different data subsets. We observe that WA reduces memorization of the corrupted labels in Figure 4(a), and still performs slightly worse than ENS on the clean training samples in Figure 4(b); yet, the performances of WA w.r.t. ENS improves as we move away from the training distribution, in particular on \\(\\mathcal{D}_{\\mathit{ood}}\\) in Figure 4(d) where WA generalizes better.\n' +
      '\n' +
      'Figure 3: **Experiments under distribution shifts validating Observations 1 and 2** on the TL;DR summarization benchmark [97]. We report the accuracies on \\(\\mathcal{D}_{\\mathit{ood}}\\) when interpolating between two RM weights \\(\\phi_{1}\\) and \\(\\phi_{2}\\) with the coefficient \\(\\lambda\\) sliding between 0 and 1. _WA_ stands for weight averaging \\(r_{(1-\\lambda)+\\phi_{1}+\\lambda+\\phi_{2}}\\) while _ENS_ combines the predictions \\((1-\\lambda)\\times r_{\\phi_{1}}+\\lambda\\times r_{\\phi_{2}}\\); _Diag_ is the interpolated accuracy \\((1-\\lambda)\\times\\text{Acc}(r_{\\phi_{1}})+\\lambda\\times\\text{Acc}(r_{\\phi_{2}})\\). We consider sources of increasing diversity [94] between \\(\\phi_{1}\\) and \\(\\phi_{2}\\): in Figure 3(a), they are collected at different number of training steps (8k and 10k) along a single RM fine-tuning; in Figure 3(b), they are from two independant RM fine-tunings, with the exact same config, but seeing the data in different orders; in Figure 3(c), they have different learning rates (1e-4 and 4e-5); in Figure 3(d), they are initialized from different FFT checkpoints collected at different number of FFT steps (8k and 12k), per _Baklava_ introduced in Figure 2.\n' +
      '\n' +
      '### 2nd order analysis: weight averaging for more robust ensembling\n' +
      '\n' +
      '**A surprising fact remains unexplained.** WA is slightly superior to ENS under distribution shifts, which one can see on the plots from Figure 3, and more consistently in Figure B.1 from model soups [46] or in Figure 1 from DiWA [47]. More generally, WA is the state-of-the-art strategy for OOD generalization, consistently outperforming ENS; yet, this was not explained in previous works, thus urging for new insights about the difference between WA and ENS.\n' +
      '\n' +
      '**Corruption setup.** To refine our understanding on the difference between WA and ENS, we propose a new setup where 25% of the binary labels are swapped in training. We then report the per-subset accuracies on Figure 4, enriched in Appendix C.1 and aggregated in Figure 5. On the corrupted subset of training data, the accuracy curve for WA is below the expected accuracies, while it is above on all other subsets. More precisely, we make the following Observation 3.\n' +
      '\n' +
      '**Observation 3** (WA and ENS: 2nd order analysis).: _The accuracy gains of WA over ENS grow as data moves away from the training distribution._\n' +
      '\n' +
      '* _WA_ \\(\\ll\\) _ENS on train corrupt: WA is far worse than ENS on train samples with swapped labels, showing reduced memorization and improved robustness to label corruption._\n' +
      '* _WA_ \\(\\leq\\) _ENS on train clean: WA is worse than ENS on train samples with correct labels._\n' +
      '* _WA_ \\(\\nlessapprox\\) _ENS on ID val: WA is better or similar to ENS on samples without distribution shifts._\n' +
      '* _WA_ \\(\\geq\\) _ENS on OOD test: WA is far better than ENS on test samples from new distributions, showing better reliability under distribution shifts._\n' +
      '\n' +
      'Overall, this suggests that weight averaging memorizes less and generalizes better than ensembling predictions.\n' +
      '\n' +
      '### Weight averaging enforces invariance across runs\n' +
      '\n' +
      'We now provide theoretical support to this Observation 3. In brief, our simplifying assumptions suggest that WA acts as a regularization towards the predictive mechanisms that are _invariant_ across runs, i.e., learned simultaneously in each independent run. Then, in contrast with ENS, WA would improve robustness to corruption because it would underweight the run-specific features (with low probability of being learned) inducing memorization.\n' +
      '\n' +
      '**Setup.** We follow Lin _et al._[53], and consider a simplified binary classification setup with labels \\(y\\in\\{-1,1\\}\\), related to \\(F\\) features \\(\\{z^{j}\\}_{j=1}^{F}\\) such as \\(z^{j}\\in\\mathbb{R}^{d}\\). From inputs \\(x\\), we train a binary classifier \\(r(x)=\\omega^{\\top}f(x)\\). Following [53], we make three key assumptions. First, _features orthogonality_: we assume that \\(\\{z^{j}\\}_{j=1}^{F}\\) are orthogonal, i.e., \\((z^{j})^{\\top}z^{j}=0\\) when \\(j\\neq j^{\\prime}\\). Second, _input as bag of features_: we assume that the input \\(x=\\left[x^{j}\\right]_{j=1}^{F}\\in\\mathbb{R}^{F\\times d}\\) can be represented as the concatenation of \\(x^{j}\\) generated by \\(x^{j}\\sim\\mathcal{N}\\big{(}y\\cdot z^{j},\\sigma\\cdot\\mathbf{I}_{d}\\big{)}\\) with \\(\\sigma\\ll 1\\). Finally, the _binary featurizer_ assumption: we consider that the featurizer \\(f=\\left[f^{j}\\right]_{j=1}^{F}\\in\\{0,1\\}^{F}\\) is a binary selector of the features that make the input. For example, if \\(y=1\\), \\(F=3\\), \\(x\\approx[z^{1},z^{2},z^{3}]\\), and \\(f=[1,0,1]\\) learns to extract the first and third features, then \\(f(x)\\approx z^{1}+z^{3}\\). We denote \\(p_{j}\\) the probability that the featurizer \\(f\\) learns to use the \\(j\\)-th feature dimension (associated with \\(z^{j}\\)); this means \\(f^{j}\\) is \\(1\\) with probability \\(p_{j}\\) and \\(0\\) otherwise. Moreover, for infinite training samples and under some constraint on \\(\\sigma\\), Lemma 5 in [53] proved that, to learn \\(r=\\omega^{\\top}f\\), the optimal linear fit \\(\\omega\\) on the features selected from \\(f\\) would be \\(\\omega=\\sum_{j=1}^{F}f^{j}\\cdot z^{j}\\).\n' +
      '\n' +
      'Figure 5: Histograms of the differences in accuracy between WA and ENS on different data subsets.\n' +
      '\n' +
      '**Results.** We consider \\(M\\) RMs \\(\\{r_{i}=\\omega_{i}^{\\top}f_{i}\\}_{i=1}^{M}\\), and compare the limit behaviours of their prediction ensembling \\(r_{M}^{ENS}\\) and weight averaging \\(r_{M}^{WA}\\) when \\(M\\to\\infty\\). In this limit case, the averaged prediction \\(r_{M}^{ENS}=\\frac{1}{M}\\sum_{i=1}^{M}\\omega_{i}^{\\top}f_{i}\\) for an input \\(x\\) from label \\(y\\) tends towards the expected prediction \\(\\mathbb{E}[r(x)]=\\mathbb{E}[\\omega^{\\top}f(x)]=\\mathbb{E}_{\\{r^{j}\\}_{j=1}^{F} }[\\big{(}\\sum_{j=1}^{F}f^{j}\\cdot z^{j}\\big{)}^{\\top}(\\sum_{j^{\\prime}=1}^{F}f^ {j^{\\prime}}\\cdot x^{\\prime})\\big{]}\\approx y\\cdot\\sum_{j=1}^{F}p_{j}\\cdot|z^{ j}|^{2}\\), using \\(x^{\\prime}\\approx y\\cdot z^{\\prime}\\) thus \\((z^{j})^{\\top}x^{\\prime}\\approx 0\\) when \\(j\\neq j^{\\prime}\\), and \\((f^{j})^{2}=f^{j}\\).\n' +
      '\n' +
      '\\[r_{M}^{ENS}(x)\\xrightarrow[M\\to\\infty]{}\\mathbb{E}[r(x)]\\approx y\\cdot\\sum_{ j=1}^{F}\\mathbf{p}_{j}\\cdot|z^{j}|^{2}. \\tag{4}\\]\n' +
      '\n' +
      'In contrast, when considering \\(r_{M}^{WA}=\\left(\\frac{1}{M}\\sum_{i=1}^{M}\\omega_{i}\\right)^{\\top}\\left(\\frac{ 1}{M}\\sum_{i=1}^{M}f_{i}\\right)\\) with \\(M\\to\\infty\\), we have \\(\\frac{1}{M}\\sum_{i=1}^{M}f_{i}\\xrightarrow[M\\to\\infty]{}\\mathbb{E}[f]=\\big{[}p _{j}\\big{]}_{j=1}^{F}\\) and \\(\\frac{1}{M}\\sum_{i=1}^{M}\\omega_{i}\\xrightarrow[M\\to\\infty]{}\\mathbb{E}[\\omega ]=\\sum_{j=1}^{F}p_{j}\\cdot z^{j}\\), and thus:\n' +
      '\n' +
      '\\[r_{M}^{WA}(x)\\xrightarrow[M\\to\\infty]{}\\left(\\sum_{j=1}^{F}p_{j}\\cdot z^{j} \\right)^{\\top}\\left(\\sum_{j^{\\prime}=1}^{F}p_{j^{\\prime}}\\cdot x^{\\prime} \\right)\\approx y\\cdot\\sum_{j=1}^{F}\\mathbf{p}_{j}^{2}\\cdot|z^{j}|^{2}. \\tag{5}\\]\n' +
      '\n' +
      '**Interpretation.** For ENS, the coefficient for a given feature is \\(\\mathbf{p}_{j}\\), the same as the probability of this information being used by any individual network. In contrast, WA involves the square of the probability \\(\\mathbf{p}_{j}^{2}\\). Thus WA reduces the reliance on features with low probability, related to minor specific information (such as noise or context) which can be used to fit the corrupted training samples; this would reduce memorization, and thus explains the robustness of WA under label corruption. Reciprocally, WA tends to prioritize the most probable features, favoring the mechanisms that are consistently learned, in other words the _mechanisms invariant across runs_. Overall, WA acts as a regularization, improving robustness under label corruption by tackling run-specific mechanisms favoring memorization, and improving reliability under distribution shifts by preserving run-invariant mechanisms favoring generalization.\n' +
      '\n' +
      '**Remark 3** (Invariance).: _We argue that weight averaging only keeps the invariant predictive mechanisms across runs. This is in analogy with the invariance literature [50], popular for domain generalization [51, 100] under spurious correlations, where the key idea is that the predictive mechanisms which are invariant across domains are the causal ones that are stable under distribution shifts. This theoretically connects two key paradigms for OOD generalization, ensembling and invariance, and shows that weight averaging actually benefits from both._\n' +
      '\n' +
      '**Remark 4** (Extension to a deeper structure with \\(L\\) layers).: _We obtain a square in \\(\\mathbf{p}_{j}^{2}\\) due to our simplified two-layer architecture. Yet, in full generality, using a deeper structure with \\(L\\) layers would lead to \\(\\mathbf{p}_{j}^{L}\\). Intuitively, WA applies an AND-mask on the information, that need to be found both in the previous feature space and the next layer weights._\n' +
      '\n' +
      '**Remark 5** (From reward robustness to learnability).: _When applied to the design of RMs in WARM, we now argue that WA facilitates WARM\'s stability [87] by mitigating the reliance on some non-robust features. Indeed, WA makes the WARM reward more robust to small (potentially adversarial [101]) perturbations [102], i.e., smoother [103] in the input space. This relates to the Lipschitzness property of the reward [104, 105, 106], where the difference in predicted rewards is bounded by the distance in input space. Fortunately, such smoothness is useful in RL [107], in particular for the stability of the policy gradient [108] because "sharp changes in reward value are hard to represent and internalize" [109]. This is studied in Lipschitzness is all you need [109] where the authors argue that "the local Lipschitzness of the reward is a sine qua non condition for good performance", required "to even learn anything". In summary, robustness improves stability and hinders the cascade of errors occurring when minor input variations can cause large reward differences._In **conclusion**, we summarize the benefits from _WARM_. First, WARM is efficient, incurring no memory or computation costs, as it returns a single model. Second, _WARM_ reduces variance while leveraging mechanisms invariant across runs, thus improving its reliability under distribution shifts. Lastly, _WARM_ also addresses label corruption, thereby augmenting robustness to noisy preferences.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'To empirically validate _WARM_\'s benefits described in previous section, we train PaLM-XXS RMs on the TL;DR summarization benchmark [97] where preference labels are generated by a PaLM-L model prompted with chain-of-thought [99]. This AI labeling approach, increasingly common in recent research [26; 41; 110] as an efficient alternative to human assessments, is motivated by studies [57; 58] indicating that it correlates well with human preferences: critically, it provides an automatic _pairwise oracle preference_ metric to evaluate reward hacking (in a similar fashion to the distillation setup from [17], discussed in Appendix C.4). In addition, we leverage a PaLM-XS RM for _pointwise_\n' +
      '\n' +
      'Figure 6: **Control reward for BoN experiments**: clean preference dataset in Figures 6(a) and 6(c) and 25% corruptions in Figures 6(b) and 6(d). We consider two SFT policies to generate candidate summaries: one based on PaLM architecture [98], the other on T5 architecture [111]. The \\(x\\)-axis is the KL between the BoN policy and the SFT policy; the \\(y\\)-axis represents the control reward gains w.r.t. to an RM \\(\\phi_{1}\\), which was the best individual RM on \\(\\mathcal{D}_{\\text{ood}}\\). The blue lines represent _WARM_ with \\(M\\) weights: _WARM_ performs higher than the individual RMs (in yellows) or when ensembling their predictions (ENS in red). We report the absolute control rewards for those experiments in Figure 15, where the values range roughly between 3 and 7.\n' +
      '\n' +
      'Figure 7: **Oracle preference metric for BoN experiments on T5 generations**: clean preference dataset in Figures 7(a) and 7(c) and 25% corruptions in Figures 7(b) and 7(d). We plot the win rates for different values of \\(N\\) vs. two reference strategies: SFT (i.e., random selection or equivalently BoN with \\(N=1\\)), or selecting the best summary according to _WARM_\\(M=6\\). We observe that all strategies beat the SFT reference (they are all above 50% win rate), but that none beat the _WARM_\\(M=6\\) reference.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '**Oracle preference.** In Figure 9, we compare the different policies according to our pairwise oracle preference AI labeler [58]. In Figure 9(a), the reference policy is the SFT initialization; all the RL fine-tuned policies outperform this baseline, with _WARM_\\(M=6\\) reaching a win rate of \\(99.8\\%\\) after \\(3500\\) steps (the highest win rate among all policies). We use this policy as the reference in Figure 9(b); no other policy could beat it. Interestingly, we observe that using \\(M=10\\) rewards can delay reward hacking but does not improve the peak performance; we speculate this is related to our weight selection procedure, as the weights \\(\\{\\phi_{i}\\}_{i=7}^{10}\\) have lower individual accuracy on \\(\\mathcal{D}_{\\mathit{ood}}\\) than \\(\\{\\phi_{i}\\}_{i=1}^{6}\\) (more details in Figure 10). Finally, in Figure 9(c), the reference policy is obtained after \\(3000\\) steps of RL fine-tuning with \\(\\phi_{1}\\) (the best individual RM on \\(\\mathcal{D}_{\\mathit{ood}}\\)). There is a large region of steps in which policies trained _WARM_ (even with \\(M=2\\)) beat this approach; the previous reference from Figure 9(b) actually has a \\(79.4\\%\\) win rate against it.\n' +
      '\n' +
      'Figure 8: **Control reward for RL experiments: clean preference dataset in Figures 8(a) and 8(c) and 25% corruptions in Figure 8(b). The blue lines show the RL fine-tuning of policies when averaging \\(M\\) weights as the RM; the darker, the higher the \\(M\\). It performs higher than when RL fine-tuning with the individual RMs (in yellows) or when ensembling their predictions (in red). Figure 8(c) shows results of policies RL fine-tuned with _WARM_\\(M=6\\) or \\(\\phi_{1}\\), for different values of \\(\\alpha\\) controlling the KL regularization strength.**\n' +
      '\n' +
      'Figure 9: **Oracle preference metric for RL experiments: clean preference dataset. We plot the win rates along RL fine-tuning against three reference policies: the SFT policy, the policy RL fine-tuned with _WARM_\\(M=6\\) after \\(3500\\) steps, and the policy RL fine-tuned with \\(\\phi_{1}\\) after \\(3000\\) steps. Figure 19 reports results when comparing policies at fixed number of training steps.**\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '**Benefits.**_WARM_ represents a flexible and pragmatic method to improve the alignment of AI with human values and societal norms. This paper has detailed several of its benefits, and below, we delve into additional, more exploratory advantages. _WARM_ follows the _updatable machine learning paradigm_[114], eliminating the need for inter-server communication, thus enabling _embarrassingly simple parallelization_[115] of RMs. This facilitates its use in _federated learning_ scenario [116] where the data should remain private; moreover, WA would add a layer of privacy and bias mitigation by reducing the memorization of private preference [52]. Then, a straightforward extension of _WARM_ would combine RMs trained on different datasets, for example, coming from different (clusters of) labelers. This diversity could help _WARM_ performances, but also from a multi objective perspective [117]; by non-uniform interpolation of RMs, we could learn a set of _personalized policies_[39]. Furthermore, as WA has been shown to limit catastrophic forgetting [118, 119], _WARM_ could seamlessly support iterative and evolving preferences. Finally, a promising research direction is extending _WARM_ to direct preference optimization (DPO) strategies [120], where averaging the RMs casts back to averaging the DPO policies [121].\n' +
      '\n' +
      '**Limitations.**_WARM_, while innovative, does face some limitations, notably two when compared to prediction ensembling methods; first, prediction ensembling can benefit from the diversity brought by combining RMs from various architectures and pre-trainings; second, prediction ensembling can incorporate prediction disagreement into the reward to provide uncertainty estimation and limit model drift. However, it\'s been noted in [41] that simple averaging of logits often performs comparably to more complex prediction aggregation functions that include uncertainty elements. Another limitation is that, while _WARM_ effectively reduces certain types of memorization, it does not completely eradicate all forms of spurious correlations or biases inherent in the preference data. For instance, if each individual RM predominantly relies on summary length as a criterion, _WARM_ is likely to replicate this tendency. Therefore, alternative methods (from the OOD generalization literature?) might be required, for example those based on invariance regularization [51, 100] or last layer retraining [122]. Finally, _WARM_ only enhances reward modeling without tackling the other challenges in RLHF [18]; thus, to mitigate the safety risks [19, 34, 35] from misalignment [32, 33], _WARM_ must be considered within the larger context of responsible AI.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In conclusion, we introduce Weight Averaged Reward Models (_WARM_) to address two critical challenges in reward modeling: reliability under distribution shifts and robustness under label corruption. By averaging the weights of multiple RMs obtained from diverse fine-tunings, _WARM_ appears as an efficient solution to mitigate reward hacking in reinforcement learning from human feedback. Our empirical results demonstrate its effectiveness when applied to summarization. We anticipate that _WARM_ will contribute to more aligned, transparent, and effective AI systems, encouraging further exploration in reward modeling.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. (p. 1)\n' +
      '* [2] OpenAI. Gpt-4 technical report. 2023. (p. 1)\n' +
      '* [3] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. (p. 1 and 3)\n' +
      '* [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2019. (p. 1)\n' +
      '* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020. (pp. 1, 7, and 27)\n' +
      '* [7] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _ICLR_, 2022. (pp. 1 and 3)\n' +
      '* [8] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In _ACL_, 2022. (p. 1)\n' +
      '* [9] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. (p. 1)\n' +
      '* [10] Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, et al. Factually consistent summarization via reinforcement learning with textual entailment feedback. In _ACL_, 2023. (p. 1)\n' +
      '* [11] Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned reward functions. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [12] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _NeurIPS_, 2017. (pp. 1, 2, 3, 5, and 27)\n' +
      '* [13] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint_, 2019. (pp. 1 and 27)* [14] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _NeurIPS_, 2020. (pp. 1, 7, and 27)\n' +
      '* [15] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. _arXiv preprint_, 2021. (pp. 1 and 27)\n' +
      '* [16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models. _arXiv preprint_, 2023. (pp. 1, 5, 12, and 27)\n' +
      '* [17] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In _ICML_, 2023. (pp. 1, 11, and 33)\n' +
      '* [18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jeremy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. _TMLR_, 2023. (pp. 1 and 14)\n' +
      '* [19] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in AI safety. _arXiv preprint_, 2016. (pp. 1, 2, 3, 4, and 14)\n' +
      '* [20] Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. [https://openai.com/research/faulty-reward-functions](https://openai.com/research/faulty-reward-functions), 2016. (pp. 1 and 4)\n' +
      '* [21] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. _arXiv preprint_, 2021. (pp. 1 and 4)\n' +
      '* [22] Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In _NeurIPS_, 2022. (pp. 1 and 4)\n' +
      '* [23] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In _ICLR_, 2022. (pp. 1 and 4)\n' +
      '* [24] Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in reinforcement learning from human feedback. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [25] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning for negotiation dialogues. _arXiv preprint_, 2017. (p. 1)* [26] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. _arXiv preprint_, 2023. (pp. 1 and 11)\n' +
      '* [27] Marilyn Strathern. Improving ratings: audit in the british university system. _European Review_, 1997. (p. 1)\n' +
      '* [28] Ethan Perez, Sam Ringer, Kamile Lukositte, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. _arXiv preprint_, 2022. (p. 1)\n' +
      '* [29] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards understanding sycophaney in language models. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [30] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose opinions do language models reflect? In _ICML_, 2023. (p. 1)\n' +
      '* [31] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conversational ai: Converging evidence on chatgpt\'s pro-environmental, left-libertarian orientation. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [32] Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for advanced machine learning systems. _Ethics of AI_, 2016. (pp. 1, 3, and 14)\n' +
      '* [33] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective. _arXiv preprint_, 2022. (pp. 1, 3, 14, and 27)\n' +
      '* [34] Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research. _arXiv preprint_, 2022. (pp. 1 and 14)\n' +
      '* [35] Dan Hendrycks. Natural selection favors Als over humans. _arXiv preprint_, 2023. (pp. 1 and 14)\n' +
      '* [36] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. _NeurIPS_, 2020. (pp. 2 and 4)\n' +
      '* [37] Daniel Shin, Anca Dragan, and Daniel S. Brown. Benchmarks and algorithms for offline preference-based reward learning. _TMLR_, 2023. (p. 2)\n' +
      '* [38] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamille Lukositte, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. _arXiv preprint_, 2022. (pp. 2 and 4)\n' +
      '* [39] Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In _NeurIPS_, 2023. (pp. 2, 14, and 26)\n' +
      '* [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _NeurIPS_, 2022. (pp. 2, 3, and 27)\n' +
      '* [41] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. _arXiv preprint_, 2023. (pp. 2, 4, 5, 11, 12, 14, and 27)* [42] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. _arXiv preprint_, 2023. (pp. 2, 5, and 27)\n' +
      '* [43] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _NeurIPS_, 2017. (pp. 2 and 5)\n' +
      '* [44] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _ICML_, 2020. (pp. 3, 6, and 26)\n' +
      '* [45] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In _NeurIPS_, 2020. (pp. 3, 6, and 26)\n' +
      '* [46] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _ICML_, 2022. (pp. 3, 6, 7, 9, 26, and 28)\n' +
      '* [47] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In _NeurIPS_, 2022. (pp. 3, 5, 6, 7, 8, 9, 26, and 28)\n' +
      '* [48] Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Leon Bottou, and David Lopez-Paz. Model Ratatouille: Recycling diverse models for out-of-distribution generalization. In _ICML_, 2023. (pp. 3, 6, 7, 26, and 28)\n' +
      '* [49] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. SWAD: Domain generalization by seeking flat minima. In _NeurIPS_, 2021. (pp. 3 and 26)\n' +
      '* [50] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _ICML_, 2013. (pp. 3 and 10)\n' +
      '* [51] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint_, 2019. (pp. 3, 4, 10, and 14)\n' +
      '* [52] Kerem Zaman, Leshem Choshen, and Shashank Srivastava. Fuse to forget: Bias reduction and selective memorization through model fusion. _arXiv preprint_, 2023. (pp. 3, 14, and 26)\n' +
      '* [53] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. In _ICLR_, 2024. (pp. 3, 9, and 26)\n' +
      '* [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017. (pp. 3 and 6)\n' +
      '* [55] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint_, 2021. (pp. 3 and 26)\n' +
      '* [56] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In _CVPR_, 2014. (p. 3)\n' +
      '* [57] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. _arXiv preprint_, 2022. (pp. 4 and 11)* [58] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. _arXiv preprint_, 2023. (pp. 4, 7, 11, 12, 13, 27, and 28)\n' +
      '* [59] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 1952. (p. 4)\n' +
      '* [60] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Reinforcement learning_, 1992. (pp. 4, 12, and 28)\n' +
      '* [61] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint_, 2017. (pp. 4 and 12)\n' +
      '* [62] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _ICLR_, 2021. (p. 4)\n' +
      '* [63] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In _ICML_, 2021. (p. 4)\n' +
      '* [64] Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In _NeurIPS_, 2020. (p. 4)\n' +
      '* [65] Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, and Moncef Gabbouj. Learning distinct features helps, provably. _arXiv preprint_, 2021. (p. 4)\n' +
      '* [66] Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse ImageNet models transfer better. _arXiv preprint_, 2022. (p. 4)\n' +
      '* [67] Alexander D\'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification presents challenges for credibility in modern machine learning. _JMLR_, 2020. (pp. 4 and 26)\n' +
      '* [68] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. ID and OOD performance are sometimes inversely correlated on real-world datasets. In _NeurIPS Workshop_, 2023. (p. 4)\n' +
      '* [69] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In _ICML_, 2017. (p. 4)\n' +
      '* [70] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\'s uncertainty? evaluating predictive uncertainty under dataset shift. In _NeurIPS_, 2019. (p. 4)\n' +
      '* [71] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. In _NeurIPS_, 2021. (p. 4)\n' +
      '* [72] Herbert A Simon. Bounded rationality. _Utility and probability_, 1990. (p. 4)\n' +
      '* [73] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather than assuming, human biases for reward inference. In _ICML_, 2019.\n' +
      '\n' +
      '* [74] Timo Kaufmann, Sarah Ball, Jacob Beck, Eyke Hullermeier, and Frauke Kreuter. On the challenges and practices of reinforcement learning from real human feedback. 2023. (p. 4)\n' +
      '* [75] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint_, 2022. (p. 4)\n' +
      '* [76] Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, et al. Rewarding chatbots for real-world engagement with millions of users. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [77] Condorcet. Essai sur l\'application de l\'analyse a la probabilite des decisions rendues a la pluralite des voix. 1785. (p. 4)\n' +
      '* [78] Silviu Pitis. Failure modes of learning reward models for l lms and other sequence models. In _ICML_, 2023. (p. 4)\n' +
      '* [79] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [80] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. State of what art? a call for multi-prompt llm evaluation. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [81] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In _ICML_, 2017. (pp. 5 and 12)\n' +
      '* [82] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In _ICML_, 2019. (pp. 5 and 12)\n' +
      '* [83] Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. In _ACL_, 2020. (p. 5)\n' +
      '* [84] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In _ICML_, 2020. (p. 5)\n' +
      '* [85] Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In _ICML_, 2020. (pp. 5 and 27)\n' +
      '* [86] William Saunders, Girish Sastry, Andreas Stuhlmuller, and Owain Evans. Trial without error: Towards safe reinforcement learning via human intervention. In _AAMAS_, 2018. (p. 5)\n' +
      '* [87] Binghai Wang et al. Secrets of rlhf in large language models part ii: Reward modeling. _arXiv preprint_, 2023. (pp. 5, 10, and 27)\n' +
      '* [88] Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions. In _ICML_, 1996. (p. 5)\n' +
      '* [89] Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. In _ICNN_, 1996. (p. 5)\n' +
      '* [90] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general principles for constitutional ai. _arXiv preprint_, 2023.\n' +
      '\n' +
      '* [91] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In _ICLR_, 2022. (pp. 5, 6, 7, and 28)\n' +
      '* [92] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In _ICLR_, 2022. (p. 6)\n' +
      '* [93] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. In _EMNLP_, 2023. (p. 6)\n' +
      '* [94] Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. In _ICLR_, 2022. (pp. 6 and 8)\n' +
      '* [95] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _UAI_, 2018. (pp. 7 and 26)\n' +
      '* [96] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. In _NeurIPS_, 2021. (pp. 7 and 26)\n' +
      '* [97] Michael Volske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In _ACL Workshop_, 2017. (pp. 7, 8, and 11)\n' +
      '* [98] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PalM 2 technical report. _arXiv preprint_, 2023. (pp. 7, 11, 12, 27, 28, and 30)\n' +
      '* [99] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022. (pp. 7, 11, and 27)\n' +
      '* [100] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In _ICML_, 2022. (pp. 10 and 14)\n' +
      '* [101] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint_, 2013. (p. 10)\n' +
      '* [102] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaudhuri. Adversarial robustness through local lipschitzness. _arXiv preprint_, 2020. (p. 10)\n' +
      '* [103] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new neural network smoothness constraints. In _NeurIPS ICBINB_, 2020. (p. 10)\n' +
      '* [104] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. _NeurIPS_, 2017. (p. 10)\n' +
      '* [105] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. _IEEE Transactions on Signal Processing_, 2017. (p. 10)\n' +
      '* [106] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _ICML_, 2019. (p. 10)\n' +
      '* [107] Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: Challenges and benchmarks from technical process control. _Machine learning_, 2011. (p. 10)* [108] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision processes. _Machine Learning_, 2015.\n' +
      '* [109] Lionel Blonde, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame off-policy generative adversarial imitation learning. _Machine Learning_, 2022.\n' +
      '* [110] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint_, 2023.\n' +
      '* [111] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.\n' +
      '* [112] Jacob Hilton. KL divergence of max-of-n, 2023.\n' +
      '* [113] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D\'Amour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. _arXiv preprint_, 2024.\n' +
      '* [114] Colin Raffel. Building Machine Learning Models Like Open Source Software. _ACM_, 2023.\n' +
      '* [115] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-Train-Merge: Embarrassingly parallel training of expert language models. _arXiv preprint_, 2022.\n' +
      '* [116] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _AISTATS_, 2017.\n' +
      '* [117] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. In _NeuriPS_, 2023.\n' +
      '* [118] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of strong zero-shot models for continual learning. In _NeurIPS Workshop_, 2022.\n' +
      '* [119] Steven Vander Eeckt et al. Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition. _arXiv preprint_, 2022.\n' +
      '* [120] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint_, 2023.\n' +
      '* [121] Maxime Labonne. NeuralBeagle14-7B. [https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF](https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF), 2024.\n' +
      '* [122] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. In _ICLR_, 2023.\n' +
      '* [123] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _ICLR_, 2019.\n' +
      '\n' +
      '* [124] John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study. _PLOS Medicine_, 2018.\n' +
      '* [125] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects shortcuts over signal. _Nature Machine Intelligence_, 2021.\n' +
      '* [126] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In _CVPR_, 2022.\n' +
      '* [127] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In _NeurIPS_, 2022.\n' +
      '* [128] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. In _ACL_, 2023.\n' +
      '* [129] Nikolaos Dimitriadis, Pascal Frossard, and Francois Fleuret. Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models. _arXiv preprint_, 2022.\n' +
      '* [130] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unival: Unified model for image, video, audio and language. _TMLR_, 2023.\n' +
      '* [131] Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal. Seasoning model soups for robustness to adversarial and natural distribution shifts. In _CVPR_, 2023.\n' +
      '* [132] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Joao Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. In _ICLR_, 2023.\n' +
      '* [133] Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep reinforcement learning with weight averaging.\n' +
      '* [134] Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online adaptation in reinforcement learning. In _ICLR_, 2022.\n' +
      '* [135] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for forming multi-task policies. In _ICLR RRL Workshop_, 2023.\n' +
      '* [136] Michael Noukhovitch, Samuel Lavoie, Florian Strub, and Aaron Courville. Language model alignment with elastic reset. In _NeurIPS_, 2023.\n' +
      '* [137] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In _ICLR_, 2023.\n' +
      '* [138] Nico Dhaheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. Elastic weight removal for faithful and abstractive dialogue generation. _arXiv preprint_, 2023.\n' +
      '* [139] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _TNNLS_, 2022.\n' +
      '* [140] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. _ICLR_, 2017.\n' +
      '\n' +
      '* [141] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In _CVPR_, 2019.\n' +
      '* [142] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neptune: Noisy embeddings improve instruction finetuning. _arXiv preprint_, 2023.\n' +
      '* [143] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In _AAAI_, 2017.\n' +
      '* [144] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. In _ICLR_, 2022.\n' +
      '* [145] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _ICML_, 2018.\n' +
      '* [146] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _NeurIPS_, 2018.\n' +
      '* [147] Maryam Sabzevari. _Ensemble learning in the presence of noise_. PhD thesis, Universidad Autonoma de Madrid, 2019.\n' +
      '* [148] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In _ICML_, 2000.\n' +
      '* [149] W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca Dragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and mistaking it for reward. _arXiv preprint_, 2023.\n' +
      '* [150] Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning from multiple teachers. _arXiv preprint_, 2023.\n' +
      '* [151] Sian Gooding and Hassan Mansoor. The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization. _arXiv preprint_, 2023.\n' +
      '* [152] Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-augmented reward modeling. In _ICLR_, 2023.\n' +
      '* [153] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. _arXiv preprint_, 2023.\n' +
      '* [154] Anonymous. RIME: Robust preference-based reinforcement learning with noisy human preferences. In _Submitted to ICLR_, 2023.\n' +
      '* [155] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences. _arXiv preprint_, 2023.\n' +
      '* [156] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _ICML_, 2018.\n' +
      '\n' +
      '* [157] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language models. _arXiv preprint_, 2023.\n' +
      '\n' +
      '_WARM_: On the Benefits of Weight Averaged Reward Models\n' +
      '\n' +
      'Supplementary material\n' +
      '\n' +
      'This supplementary material is organized as follows:\n' +
      '\n' +
      '* Appendix A enriches our related work.\n' +
      '* Appendix B clarifies some experimental details.\n' +
      '* Appendix C enriches our experiments.\n' +
      '\n' +
      '## Appendix A Related work\n' +
      '\n' +
      'This paper leverages the insights from the OOD generalization literature, in particular from linear mode connectivity (see Appendix A.1), and applies them to the design of efficient, reliable and robust reward models (see Appendix A.2).\n' +
      '\n' +
      '### Out-of-distribution generalization, linear mode connectivity and memorization\n' +
      '\n' +
      '**LMC in fine-tuning.** Fine-tuning foundation models [55] into specialized models that generalize well to new distributions is critical for many real-world applications [123, 124, 125]. Recently, different variants of weight averaging (WA) were able to improve performance, such as moving average [49, 95, 96], WiSE fine-tuning [126], model soups [46], DiWA [47] and model ratatouille [48]. These works rely on the LMC [44, 45] across fine-tuned weights, which was extended to fine-tunings on different tasks [48, 127, 128], modalities [130] or with different losses [47, 131], although [132] highlighted some limitations. WA was also used recently in RL setups [39, 133, 134, 135, 136], in particular in RLHF in [39, 136] but only to combine policies and not rewards.\n' +
      '\n' +
      '**Insights into WA.** Specifically, WA comes with several benefits. First, WA flattens the loss landscape [49]. Second, WA approximates prediction ensembling, thus reduces variance of the estimator [46, 47] and tackles model misspecification [67]. Third, WA combines models\' abilities [137, 138], which can be useful for multi-task [127], multi-objective [39] or in continual learning [118] setups. Lastly, it has recently been shown that WA can provide some benefits under spurious correlations [52, 53], with a phenomenon called _FalseFalseTrue_ in [53]. These works [52, 53] share similarities with our memorization experiments from Section 4.2, but we are the first to analyze WA regularization properties under label corruption, and their consequences on generalization. In contrast, in [52] the networks are trained on different datasets while the theory in [53] is actually mostly developed for prediction ensembling.\n' +
      '\n' +
      '**Memorization.** Traditional approaches [139] tackling memorization of corrupted labels [140] usually require explicit regularization [141], specific data augmentation [142], loss adjustment [143] or sample selection [144]. Some other strategies are based on ensembling: they filter out potentially corrupted samples with self-labeling filtering [145, 146] or bagging diversity procedures [147]. As far as we know, with WA we propose the first strategy combining multiple models trained on the same dataset that manages to tackle corruption.\n' +
      '\n' +
      '### Reward modeling\n' +
      '\n' +
      'One of the central challenge in aligning LLMs is the absence of explicit rewards from the environment, a.k.a. the outer alignment challenge [33]. While Inverse Reinforcement Learning [148] attempts to derive a reward model (RM) from expert demonstrations, most recent efforts [12, 13, 14, 15, 40] primarily focus on learning from human preferences. Despite its importance to enhance LLM performances post-RL and for safe deployment in real-world applications, how to best design RMs has arguably receive less attention than it warrants. Some research [149] seeks to refine the loss function from Equation (1). Other approaches are more data oriented: for example, LLaMA-2 [16] involves continual learning of the RM to adjust to new generation distributions; [85, 150] follow an active learning paradigm [151]. Augmenting rewards with tools [152] or additional information [153] represents an even more recent and very promising trend. Limited efforts have been made at the intersection of label corruption and reward modeling; [154] tried to filter the preference dataset for small academic locomotion tasks, while the concurrent [87] suggests applying label smoothing and flipping. Actually, reward ensembling is the most discussed method to mitigate reward hacking [41, 42]; we show that _WARM_ can beat ENS while removing its overheads. Finally, following DPO [120], a recent trend merges reward modeling with policy learning; though, the policies still tend to hack the preference data [155], and thus require only a few training steps and very small learning rates. The WA of DPO policies, theoretically equivalent to the WA of RMs, is a promising research direction with already significant empirical results on public benchmarks, as demonstrated in [121].\n' +
      '\n' +
      '## Appendix B Implementation details\n' +
      '\n' +
      '### Dataset details\n' +
      '\n' +
      'For summarization, we use the Reddit TL;DR dataset [14], containing posts from Reddit that have been filtered to ensure high quality. The training summaries from [14] are generated by OpenAI GPT-3 [6] variants. The dataset contains 123k posts, and \\(\\sim\\)5% is held out as the ID validation set. To generate the candidate responses in the OOD dataset \\(\\mathcal{D}_{ood}\\) with 92k pairwise comparisons, we considered multiple PaLM-XS policies with high temperature, some of which are pre-trained only, others SFT-ed and others RLHF-ed; the goal was to get a diverse set of summaries.\n' +
      '\n' +
      '### AI labeling details\n' +
      '\n' +
      'While the ideal approach for evaluating our models would involve human preferences, we resort to the cheaper AI labeling procedure from RLAIF [58]. We query an instruct fine-tuned PaLM-L [98] LLM1, prompted to generate preference mimicking human preferences. Specifically, we follow the "Detailed + CoT 0-shot" prompting strategy from RLAIF [58], the best one according to their results, involving zero-shot prompting with chain-of-thought [99], a maximum decoding length of 512 tokens and temperature \\(T=0.0\\) (i.e., greedy decoding). To avoid position bias, we run the AI labeler in the two possible orderings. This strategy was shown to perform similarly to human labellers, with similar inter-agreement. For the corruption experiments, we swap the labels for 25% of the training samples.\n' +
      '\n' +
      'Footnote 1: Available through Google Clouds Vertex AI [https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models).\n' +
      '\n' +
      '### Reward modeling details\n' +
      '\n' +
      'The RMs are PaLM-XXS models [98]. They are first pre-trained, and then supervised fine-tuned on the Reddit TL;DR dataset for 12k steps with a batch size of 128 and the Adafactor [156] optimizerwith a learning rate of \\(10^{-5}\\). Following the _Baklava_ recipe, we actually launch the reward modeling from different checkpoints along this SFT fine-tuning, at steps {8k, 10k, 12k}; taking a too-early checkpoint would drastically reduce RM accuracy, as observed in [157]. To convert this LLM into a classifier, we plug a linear probed [91] classification layer (the same for all RMs); said differently, even though the featurizers are actually from different SFT checkpoints, they share the same linear probed classification linear layer. As explained in [91], it prevents features from moving too much away from their initializations, which facilitates the LMC required for WA.\n' +
      '\n' +
      'We train all RMs for 10k steps, a batch size of 128, the Adafactor [156] optimizer, a learning rate sampled in {1e-5,4e-5,1e-4}, and a dropout probability in {0.05, 0.1}. This follows the practical recommandations from [47] to leverage hyperparameters in a mild range to preserve the LMC. Training for a longer number of steps could help, as it did not alter the LMC in previous works [48].\n' +
      '\n' +
      'In practice, for the main experiments with clean labels, we launch 10 reward modelings; when ranked in decreasing accuracy on \\(\\mathcal{D}_{ood}\\), we denote them \\(\\{\\phi_{i}\\}_{i=1}^{10}\\). Therefore, the RMs named \\(\\phi_{1}\\) and \\(\\phi_{2}\\) in the different plots are the two best according to their individual performances under distribution shifts. Then, _WARM_\\(M=2\\) is actually the RM defined per \\(\\frac{\\phi_{1}+\\phi_{2}}{2}\\), while ENS \\(M=2\\) averages their predictions. More generally, _WARM_ with \\(M\\) weights is the WA of the \\(M\\) best weights \\(\\{\\phi_{i}\\}_{i=1}^{M}\\). The main motivation of this weight selection procedure is to remove potentially bad RMs, as validated in Figure 10, in which we consider different permutations across those 10 RMs. As a side note, we speculate that a greedy procedure as in [46] could further improve performances.\n' +
      '\n' +
      '### Reinforcement learning details\n' +
      '\n' +
      'Both policy and value models are PaLM-XS [98], initialized from the same SFT model. We then generate samples from the policy with temperature \\(T=0.9\\), batch size of 128, the Adafactor [156] optimizer, a learning rate of \\(10^{-5}\\) and a policy warmup of 2k steps. We set \\(\\alpha=0.003\\) for the KL regularization in the main experiment without label corruption, and \\(\\alpha=0.01\\) with label corruption. Following [58], we used a modified version of REINFORCE [60] with a baseline value function for variance reduction.\n' +
      '\n' +
      'Figure 10: **Analysis of the weight selection procedure. We plot the accuracy resulting from averaging \\(M\\) weights (out of 10), where these weights are chosen based on various selection procedures. This effectively validates that choosing models from best to worst serves as a reliable heuristic.**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:29]\n' +
      '\n' +
      '### BoN experiments\n' +
      '\n' +
      'Figure 15 | Same as Figure 8, but with **absolute values of the control reward for BoN experiments**. We consider two SFT policies to generate candidate summaries: one based on PaLM architecture [98], the other on T5 architecture [111]. In both cases, we observe that _WARM_ performs better than ENS and the individual networks in terms of pointwise control RM.\n' +
      '\n' +
      'Figure 16 | **Control reward for BoN experiments** (clean setup) with _Baklava_ when the two fine-tunings \\(\\phi_{1}\\) and \\(\\phi_{3}\\) have different featurizer initializations, collected respectively at steps 12k and 8k from a shared SFT.\n' +
      '\n' +
      'Figure 15 | Same as Figure 8, but with **absolute values of the control reward for BoN experiments**. We consider two SFT policies to generate candidate summaries: one based on PaLM architecture [98], the other on T5 architecture [111]. In both cases, we observe that _WARM_ performs better than ENS and the individual networks in terms of pointwise control RM.\n' +
      '\n' +
      '### RL experiments\n' +
      '\n' +
      '#### c.3.1 Experiments with corrupted preference dataset\n' +
      '\n' +
      'Figure 17: **Oracle preference metric for BoN experiments** (clean setup). Figure 17(a) confirms Figure 7(c) but on generations from PaLM SFT. Figure 17(b) shows win rates for BoN on T5 generations for _WARM_ with \\(M=6\\) and always \\(N=1000\\) for BoN vs. other RMs with \\(1\\leq N\\leq 1000\\). We validate that BoN limits reward hacking compared to RL, as performances get better when increasing \\(N\\).\n' +
      '\n' +
      'Figure 18: **RL experiments**. Same as Figure 1(b) but with 25% corruption in the preference dataset.\n' +
      '\n' +
      '#### c.3.2 Experiments with clean preference dataset\n' +
      '\n' +
      'Figure 19 | **Oracle preference metric for RL experiments at fixed number of training steps (clean setup). Figure 19(a) plots the win rate of the policy with \\(\\mathit{WARM}\\)\\(M=6\\) vs. the other policies, all at the same number of training steps. Figure 19(b) shows the win rate of \\(\\mathit{WARM}\\)\\(M=6\\) against the policy trained with a single RM \\(\\phi_{1}\\) (the best according to OOD accuracy) along training for different values of \\(\\alpha\\) controlling the KL regularization strength.**\n' +
      '\n' +
      'Figure 20 | **Control reward for RL experiments with \\(\\alpha=0.01\\) (clean setup).**\n' +
      '\n' +
      'Figure 19 | **Oracle preference metric for RL experiments at fixed number of training steps (clean setup). Figure 19(a) plots the win rate of the policy with \\(\\mathit{WARM}\\)\\(M=6\\) vs. the other policies, all at the same number of training steps. Figure 19(b) shows the win rate of \\(\\mathit{WARM}\\)\\(M=6\\) against the policy trained with a single RM \\(\\phi_{1}\\) (the best according to OOD accuracy) along training for different values of \\(\\alpha\\) controlling the KL regularization strength.**\n' +
      '\n' +
      '### Distillation experiments\n' +
      '\n' +
      'In Figure 22 we reproduce the distillation setup from [17], where the control PaLM-XS RM generates the labels to train PaLM-XXS RMs. As a side note, we observed that distillation changes the diversity across fine-tuned RMs, thus potentially altering the significance of the distillation setup, motivating us in exploring the more realistic RLAIF setup.\n' +
      '\n' +
      'Figure 21: **Control reward for RL experiments** with \\(\\alpha=0.001\\) (clean setup).\n' +
      '\n' +
      'Figure 22: **BoN experiment in the distillation setup from [17].** The labels in the preference dataset are given by the control RM, the same RM which gives the \\(y\\)-axis. The candidate summaries are generated by a SFT with the T5 architecture [111]. The blue lines represent _WARM_ with \\(M\\) weights: _WARM_ performs higher than the individual RMs (in yellows) or when ensembling their predictions (ENS in red).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>