<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Warm_ : 웨이브 인증된 리워드 모듈들 __Warm_\n' +
      '\n' +
      'Alexandre Rame\n' +
      '\n' +
      'Nino Vieillard\n' +
      '\n' +
      'Leonard Hussenot\n' +
      '\n' +
      'Robert Dadashi\n' +
      '\n' +
      'Geoffrey Cideron\n' +
      '\n' +
      'Olivier Bachem\n' +
      '\n' +
      '구글 딥 마이그레트 구글.\n' +
      '\n' +
      'alexandreame@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '강화 학습(RLHF)을 통해 인간의 선호로 대형 언어 모델(LLM)을 정렬하면 보상 해킹이 발생할 수 있으며, 여기서 LLM은 근본적인 목표를 충족시키지 않으면서 보상 모델(RM)에서 실패를 악용하여 매우 높은 보상을 달성할 수 있다. 우리는 보상 해킹을 완화하기 위해 RM을 설계할 때 두 가지 주요 과제를 식별한다: RL 프로세스 동안 유통이 이동하고 인간 선호의 불일치가 발생한다. 해법으로 우리는 먼저 다중 RM을 미세 조정한 후 가중치 공간에서 평균을 내는 웨이브 Aver devast Reward Models(_WARM__ Weight Aver devast Reward Models,_WARM_)을 제안한다. 이 전략은 동일한 사전 학습을 공유할 때 미세 조정 무게가 선형 모드로 유지된다는 관찰을 따른다. 가중치를 평균화하여 _WARM_는 예측의 전통적인 앙상블에 비해 효율성을 향상시키며, 분배 하에서 신뢰성이 향상되고 선호 불일치에 대한 견고성이 향상된다. 베스트\\(N\\) 및 RL 방법을 사용한 요약 작업에 대한 우리의 실험은 _WARM_가 LLM 예측의 전반적인 품질과 정렬을 향상시킨다는 것을 보여주는데, 예를 들어 _WARM_로 미세 조정된 정책 RL은 단일 RM으로 미세 조정된 정책 RL에 대해 79.4%의 승률을 가지고 있다.\n' +
      '\n' +
      'A _WARM_\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '** 리워드 모델링** 게미니[1] 또는 GPT-4[2]와 같은 대화 조수**는 AI 커뮤니티를 넘어 혁명을 일으켰다. 이러한 LLM은 수학, 코딩, 도구 사용[3]을 포함한 새롭고 복잡한 작업을 완료할 수 있다. 이러한 발전은 다음 토큰 예측[4, 5, 6]에 의한 사전 훈련[SFT 감독 미세 조정[7, 8, 9], 궁극적으로 원하는 행동[10]을 캡슐화하는 보상을 최대화하기 위해 강화 학습(RL)이라는 체계적인 3단계 훈련 절차에 의해 뒷받침된다. 그러나 실제 과제에 대한 그러한 보상을 정의하는 것은 비개인적[11]이다. 인간 피드백(RLHF)[12, 13, 14, 15]로부터의 강화 학습에서 보상은 인간 판단을 에뮬레이션하기 위해 이진 선호도 데이터셋에 대해 훈련된 보상 모델(RM)이다. RL으로부터의 LLM 능력의 향상은 RM[16]의 품질에 강하게 묶여 있다.\n' +
      '\n' +
      '*** 리워드 해킹.**는 RLHF[17, 18]에 특별히 불성실한 것은 프록시 RM과 실제 인간 선호 사이의 _레워드 미스캐시화_[19, 20, 21, 22](a.k.a.optimization)로 인해 발생하는 _reward 해킹_ 이슈[19, 21, 22]이다. RM에 대한 최적화는 초기에 개선을 제공하지만, 후기 단계에서 정책(즉, 훈련 중인 LLM)은 보통 RM의 허점을 활용하고 그림 1(b)에 도시된 바와 같이 의도된 목적을 진정으로 이행하지 않고 높은 보상을 달성하도록 학습한다. 이러한 보상 해킹 현상은 수많은 문제를 야기한다. 첫째, 그것은 공연을 분해하여 언어적으로 결함이 있는 [25] 또는 불필요하게 동사된 [26] 출력으로 나타나며, 이는 진정한 인간의 선호도를 반영하지 않는다. 둘째, 대리 RM의 신뢰할 수 없어 체크포인트 선택을 복잡하게 하며, 굿하트의 법칙[27]은 "조치가 대상이 되면 좋은 조치가 되기 어렵다. 셋째, 피드백 제공자의 제한적이고 치우친 인구통계 [30, 31]을 반영하여 음각 [28, 29] 또는 사회적 편향을 증폭시킬 수 있다. 마지막으로 가장 비판적으로, 보상 해킹으로 인한 오정렬[32, 33]은 특히 일상생활에서의 LLM의 신속한 통합과 비판적 의사 결정을 고려할 때 안전 위험[19, 34, 35]으로 증가할 수 있다. 이러한 우려는 LLM의 유익하고 안전한 배치를 보장하기 위해 보상 해킹을 완화할 필요성을 강조한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      '다양한 RMs를 선도합니다. 핵심 기여는 다른 RM이 어떻게 병합되는지, 즉 중량 공간_에서 _선형 보간에 의해이다. 이는 선형 모드 연결(LMC) [44, 45] 및 가중치 평균(WA) 문헌[46, 47, 48]의 결과를 따른다: 공유 사전 훈련 하에서 건축의 비선형성에도 불구하고 다른 가중치를 선형 보간할 수 있다.\n' +
      '\n' +
      '**는 _WARM_*** 첫째, _WARM_는 효율성과 실용성을 돋보인다. 추론 시간에 단일 모델을 요구함으로써 기억과 추론 부담 없이 예측의 전통적인 비용값 앙상블에 확장 가능한 근사치를 제공한다. 둘째, _WARM_는 지도 학습 [47, 48, 49]에 대한 OOD 문헌에서 잘 문서화된 품질인 유통 이동 중인 WA의 일반화 능력에서 계승하여 신뢰성을 향상시킨다. 마지막으로 _WARM_는 부패를 표시하기 위해 견고성을 향상시킨다. WA가 다른 실행[52, 53]에 걸쳐 불변 예측 메커니즘[50, 51]을 선택하므로 자연적으로 파괴된 샘플의 암기를 감소시켜 서로 다른 방식으로 발생하고 있음을 보여준다. 대조적으로, ENS는 파괴된 샘플을 단순 외운다. 우리는 또한 시끄러운 선호도를 모델링할 때 암기를 줄이는 것이 RL 공정에서 안정성을 향상시키는 이유를 설명한다. i_WARM_의 이러한 다면적인 이점은 섹션 4에서 추가로 탐구된다.\n' +
      '\n' +
      '우리는 우리의 기여도를 다음과 같이 요약한다.\n' +
      '\n' +
      '1. _ 보상 모델링의 혁신. 우리는 보상 모델링을 위한 가중치 평균화의 첫 번째 사례인 _WARM_를 소개한다. 이 새로운 전략은 보상 해킹을 효율적으로 완화시키고, 분배 이동 하에서 신뢰성을 향상시키고 부패를 표시하기 위한 견고성을 향상시킨다.\n' +
      '2. _ 이론적, 경험적 통찰력은 체중 평균_에 대한 것이다. 이진 선호 데이터셋에 대해 학습된 보상 모델에 대한 선형 모드 연결을 검증한다. 더욱이, 우리는 라벨 부패 하에서 명확하게 나타나는 체중과 예측 평균 간의 주요 차이를 드러냈으며, 가중치 평균화는 실행 전반에 걸쳐 불변 예측 메커니즘만을 유지하여 암기를 줄이고 일반화된 특징에 초점을 향상시킨다.\n' +
      '\n' +
      '섹션 5의 요약 작업에 대한 우리의 실험은 _WARM_가 베스트\\(N\\)에서 보상 선택기로 사용될 때 또는 RL의 프록시 RM으로서 기억이나 추론 오버헤드가 없는 성능을 향상시킨다는 것을 확인시켜준다. WARM_는 보상 해킹을 완화하여 더 나은 하류 정책을 제공하며, 특히 표준 RM으로 훈련된 정책에 대해 79.4%(선호 오라클 메트릭에 따라)의 승률로 이어진다.\n' +
      '\n' +
      '2개의 텍스트와 도전, 2개의 텍스트와 도전.\n' +
      '\n' +
      '### Context\n' +
      '\n' +
      '**LLM*** 우리는 보통 주의 층[54]이 있는 트랜스포머인 \\(\\theta\\)에 의해 매개변수가 되는 고정된 비선형 구조의 LLM \\(f_{\\theta}\\)를 고려한다. 신속 입력 \\(x\\)을 \\(f_{\\theta}(x)\\)에 매핑하여 정책을 정의한다. 기반 모델 패러다임[55]과 전이 학습의 성공 이후, 가중치 \\(\\theta\\)는 먼저 방대한 양의 웹 데이터에 대해 \\(\\ta^{\\{\\{pr}}\\)로 사전 훈련된 [4]이며, 감독 전 미세 조정(SFT)[7]은 \\(\\ta^{\\text{sf}\\)으로 지침을 따르도록 학습한다. 그러나 높은 비용과 제한된 범위의 명령어 데이터(즉, 신속한 및 응답)는 LLM과 의도된 애플리케이션 사이에 오정렬 [19, 32, 33]을 생성할 수 있다. LLM의 훈련 과정의 세 번째 단계로 강화 학습(RL)은 LLM과 의도된 사용량[40]의 정렬을 돕는 것으로 나타났다.\n' +
      '\n' +
      '**RM**R의 주목할 만한 측면은 정책에 의해 모방될 감독 샘플의 부재이며, 대신 품질을 측정해야 하는 생성된 샘플의 보상을 최대화하기 위해 초점을 이동시킨다. 도전은 원하는 행동을 완벽하게 캡슐화하는 오라클 보상이 환경에 의해 주어지지 않는다는 것이다. RLHF[12]의 핵심 혁신은 이러한 보상이 인적 선호도를 예측하고 반영하기 위해 감독 방식으로 훈련된 보상 모델(RM)의 산출물이라는 것이다. 구체적으로,an RM은 \\(\\i)에 의해 매개변수가 되는 LLM \\(r_{\\파이}\\)이며, 이는 신속한\\(x\\) 및 세대의\\(y\\)에 대한 보상 \\(r_{\\파이}(x,y)로서 단일 스칼라를 예측하는 것이다. 가중치 \\(\\파이\\)는 보통 \\((\\theta^{sf},\\omega)\\에서 초기화되어 있으며, 여기서 최종 선형층 \\(\\omega\\)은 SFT 모델 \\(\\theta^{sf}\\)에서 추출된 특징들 위에 추가된다. 그런 다음\\(\\)은 선호 데이터세트 \\(x_{d},\\{D}_{train}=\\{x_{d}},y_{d}}},y_{d}^{+}},^{d}}}})에서 훈련되어 생성 \\(y_{d}^{d}}\\)보다 선호되어 \\(y_{d}. 일반적으로 인간 라벨러는 그 세대를 평가하지만, 최근 RLAIF[57, 58]에 대한 작품은 AI 피드백을 위해 LLM을 촉발하여 유사한 성능을 얻을 수 있음을 보여주었다. 브래들-티리[59]가 선호의 분포에 대한 가정, 그리고 이항 분류로서 문제를 프레이밍함으로써, 최대 가능성 원칙은 다음과 같은 음의 로그 우도 손실(\\sigma\\)을 최소화함으로써 학습 동기 부여(\\파이\\)를 로지스틱 함수라고 할 수 있다.\n' +
      '\n' +
      '}=-\\mathbb{E}} (x,\\mathcal{D}_{\\i{D},\\mathcal{D}},\\mathcal{D}})\\in\\mathcal{D}} <\\mathcal{D}_{train{D}} <\\mathcal{D}} <\\mathcal{D} <{{+}>} <\\mathcal{D} <{tcal{D} <{tcal{D} <{{+},\\mathcal{D}} <{{+},\\mathcal{D} <{{+} <{{+},\\mathcal{D} <{{+} <{{+} <{{+} <{{+} <{{+} <{{+} <{{+}>} <{{+} <{{+}>} <{{+}>} <{{D}>}>} <{tcal{D} <{tcal{D}>} <{tcal{D}>}>\n' +
      '\n' +
      '** 리워드 추론***입니다. 이 RM을 사용하여 문헌은 5.2절에서 분석된 바와 같이 RL 알고리즘(일반적으로 REINFORCE[60] 또는 PPO[61])을 \\(\\theta^{sf}\\)에 적용한다는 것을 시사하며, 이는 I\\(\\ta^{sf}\\)의 모든 종류의 RL 알고리즘(\\-tuned \\-t\\)을 \\(\\-t\\-t\\(\\-t\\)을 \\(\\-t\\)을 \\(\\-t\\-t\\(\\-t\\-t\\)을 \\(\\-t\\-t\\(\\-t\\-t\\-t\\-t\\(\\-t\\-t\\-t\\-t\\(\\-t\\)을 \\-t\\(\\-t\\)을 \\(\\-t\\-t\\(\\-t\\-t\\(\\-t\\-t\\(\\-t\\-t\\-t\\)에 적용했으며,\\-t\\-t\\(\\-t\\)을 \\(\\-t\\(\\- 두 방법 모두 정책을 인간의 선호와 정렬하는 것을 목표로 한다. 그러나 프록시 RM과 진정한 인간 선호 사이의 _reward 그립핑_[23]은 정책이 프록시 RM의 허점을 활용하여 인간의 선호와 일치하지 않고 인위적으로 점수를 증가시킬 수 있는 _reward 해킹_[19, 20, 21, 22]로 이어질 수 있다.\n' +
      '\n' +
      '보상 모델링에 적용됩니다.\n' +
      '\n' +
      '텍스트와 같은 풍부한 입력을 처리하거나 복잡한 행동을 평가할 때 인간의 선호와 정렬된 보상을 설계하는 것은 아래에 설명된 두 가지 주요 이유로 복잡한 과제이다.\n' +
      '\n' +
      '**D 분포 변화*** 1차 과제는 선호도 데이터의 오프라인 특성으로 인한 분포 변화이다. 실제로 선호 데이터 세트의 세대와 정책 \\(\\theta^{sf}\\)의 세대는 반드시 동일한 분포를 따르지 않으며 RL 동안 모델 드리프트로 인해 이동이 더욱 두드러질 수 있다. OOD 일반화 문헌은 이러한 변화의 영향을 광범위하게 분석하였다. 첫째, 그들은 종종 공연 감소[62, 63]로 이어진다. 좁은 데이터 분포에 대해 학습된 RM(한정 용량)은 잘못된 상관 관계[51] 또는 제한된 수의 특징[64]에 의존할 수 있으므로 OOD 예[65, 66]에 직면할 때 실패할 수 있다. 둘째, ID 검증 메트릭이 실제 OOD 공연[67, 68] 및 RL[41]을 안내하는 능력과 잘 상관되지 않을 수 있기 때문에 RM의 선택을 복잡하게 만든다. 마지막으로, RM은 OOD 시나리오[70, 71]에서 제대로 보정되지 않은 [69]가 될 수 있으며, 보상으로 더 극단적인 값을 예측할 수 있다. 이러한 오열화는 부정적인 피드백 루프에서 문제를 악화시키고, 모델 드리프트 및 분포 변화를 더욱 심화시킨다. 결론적으로 보상 모델링 중 제한된 데이터 적용 범위는 RM의 신뢰성을 감소시키고 RM이 불량하게 지정된 영역에서 보상 해킹[36]을 용이하게 한다.\n' +
      '\n' +
      '*** 지속적인 선호*** 두 번째 주요 과제는 선호 데이터 세트의 라벨 노이즈이다. 인간 라벨러들은 종종 피로, 오해[72, 73] 및 불완전한 인센티브[74]로 방목할 수 있으며, 이는 더 많은 인과 지표보다는 길이, 총알 포인트 또는 정치성과 같은 단순한 기준에 디폴트할 수 있다. 이러한 경향은 복잡한 과제 [38] 또는 무해[75]에서 관여[76]에 이르기까지 여러 목표를 고려하고 인간의 의견의 이질성을 나타내는 경우에 더욱 악화된다. 결과적으로 이러한 요인은 낮은 계층 간 합의로 이어지며, 여기서 인간 데이터는 근본적인 근거 진실[77, 78]의 불완전한 표현으로 나타난다. 이러한 문제를 완화하기 위해 AI 생성 선호도(57, 58)로의 전환이 있었고, 이는 인간의 인건비를 줄이는 동시에 신속한 전략에 대한 민감도[79, 80]와 같은 자체 소음 및 실패 사례 세트를 도입한다. 이러한 소음층과 불일치는 RM의 견고성과 안정적인 신호를 제공하는 능력에 도전한다.\n' +
      '\n' +
      '이를 염두에 두고 좋은 RM은 이상적으로 다음의 세 가지 속성을 충족해야 한다.\n' +
      '\n' +
      '___재산 1: 효율__재산 1: 효율.__재산 1: 효율._재산 1: 효율. RM은 메모리나 추론 오버헤드가 발생하지 않아야 한다. 그런 다음 정책을 효율적으로 최적화할 수 있습니다.\n' +
      '\n' +
      '신뢰도___재산2: 신뢰성.__재산2: 신뢰성.__재산2: 신뢰성. RM은 분포 이동에도 불구하고 예측들을 안정적으로 보상해야 한다. 그런 다음 정책은 RM에 의존하면서 초기화에서 벗어나 탐색할 수 있다.\n' +
      '\n' +
      '___재산 3: 강건성 3: 견고성__재산 3: 견고성._재산 3: 견고성. RM은 이진 선호도에서 라벨 불일치에 강력해야 한다. 그런 다음 정책은 RM에 의해 주어진 강력한 신호로부터 학습할 수 있다.\n' +
      '\n' +
      '### Existing approaches\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 이전 작업은 부록 A.2에서 관련 작업에서 몇 가지 연구 방향을 탐색했으며 표준 전략은 클렙백-리블러(KL) 규칙화[81, 82]로 SFT 초기화에 가깝게 유지되도록 장려하는 것이며 KL은 모델 드리프트[83, 84]를 감소시키지만 과소적합(정규화 강도 \\(\\pha\\)을 유발하고 추가 초매개변수)를 추가할 수 있다. 새로운 데이터(진화 정책 반영)에 대한 수집, 라벨링 및 학습(진화 정책 반영)은 RM[16]의 신뢰성을 향상시킬 수 있다. 그러나 인간 주석 및 계산 자원에 대한 지속적인 요구로 인해 상당한 효율성 문제를 제기한다. 이와는 대조적으로 _적극학습_전략[85, 86]은 다양한 세대 세트와 잠재적 실패 사례를 모색하여 선호도 데이터셋을 선제적으로 풍부하게 한다. 전류 작업[87]은 라벨 스무딩 및 플라이핑을 적용하는 것을 제안한다. 마지막으로 _WARM_, _예측 앙상블_(ENS) [43] 전략과 가장 유사한 것은 \\(M\\) RM의 로그를 평균한다. 편향-분산 관점[88], ENS는 구성원이 충분히 다양한 [89]일 때 분산항을 감소시켜 분산이 핵심 이슈[47]인 분포 변화에 따라 신뢰성을 선호한다. RL 관점에서 ENS는 해킹 위험을 완화시키는 것으로 나타났다[12, 41, 42]. 이러한 장점에도 불구하고 ENS는 효율성 문제에 직면하며, 기억 및 추론 비용은 \\(M\\)와 선형적으로 성장하여 더 큰 아키텍처가 일관되게 더 나은 [90]를 수행하는 RM의 스케일링 추세와 양립할 수 없게 한다. 또한 4.2절에서도 ENS가 선호 불일치에 대한 견고성을 향상시키지 못한다는 것을 보여줄 것이다.\n' +
      '\n' +
      '## 3 Warm\n' +
      '\n' +
      '보상 모델\n' +
      '\n' +
      '보상 모델링의 이러한 도전과 기존 접근법의 한계를 충족시키는_WARM_는 Weight Aver devast Reward Models(_WARM_)을 제안한다. WARM_는 예측 앙상블링의 메모리 및 추론 오버헤드가 없는 다수의 모델을 결합하여 보상 신뢰성(배분 이동) 및 견고성(시끄러운 선호도 데이터세트)을 향상시키는 간단하고 효율적인 전략이다. WARM_는 그림 1(a)에 예시되어 아래에 설명되어 있다.\n' +
      '\n' +
      '1._1. _공유 사전 훈련된 초기화._공유된 사전 훈련된 초기화.__공유된 초기화. 주어진 미리 학습된 LLM에 대해, 각 RM은 SFT 가중치와 선형 프로브된 [91] 분류기를 결합한 \\((\\theta^{sf},\\omega)\\에서 초기화된다.\n' +
      '2._2. _Diverse 미세 조정.__Diverse 미세 조정. 우리는 다양한 하이퍼파라미터(그리드 검색의 경우)로 식(1)을 최적화하여 \\(M\\) 중량 \\(M\\)를 생성하여\\(M\\{\\phi_{i}\\}_{i=1}^{M}\\)를 생성한다.\n' +
      '_3.3. _위크 평균.__위크 평균.__위크 평균. 우리는 평균 \\(M\\) 가중치를 결합하여 \\(\\파이^{\\text{WARM}}=\\frac{1}{M}\\sum_{i=1}^{M}\\phi_{i}\\\\)}\\\\)를 형성한다.\n' +
      '\n' +
      '그런 다음 \\(r_{\\파이^{\\text{WARM}}}\\)는 개별 RM만큼 효율적으로 RL 절차를 안내하는 프록시 RM 역할을 하지만 WA 전략이 제공하는 향상된 신뢰성과 견고성을 가지고, 강점을 환기하고 개별 RM의 약점을 완화한다.\n' +
      '\n' +
      '라인라 모드 연결성\n' +
      '\n' +
      'ENS와 비교하여 주요 차이는 _WARM_가 서로 다른 RM을 결합하는 방식에 있으며, 우리는 중량 공간_에서 _선형 보간을 통해 그렇게 한다. 그것은 미세 조정 가중치에 걸쳐 선형 모드 연결성(LMC) [44, 45] 특성, 즉 보간 모델의 정확도가 적어도 개별 정확도의 보간만큼 우수하다는 사실에 의존한다. (r_{\\ff},\\mathime{D},\\mathime{D})\\\\mathime{D}(x,y^{\\ing}})\\<\\mathime:{\\i}}(x,^a{\\ing}) 각각은 RM \\_{\\i}(r_{\\i}) w.\n' +
      '\n' +
      '공유된 사전 훈련 및 테스트 데이터세트 \\(\\mathcal{D}_{test}\\)를 가진 모든\\(\\lambda\\in[0,1]\\),__\\)에 대해, \\(\\i_{2}\\)를 감안할 때, \\(\\i_{2}\\)는 1****(LMC)를 포함한다.\n' +
      '\n' +
      '26dot\\cdot\\i_{2}}, \\mathcal{D})\\geq(1-\\lambda)\\operator{Acc}(r_{\\i_{\\i)\\operator{Acc}(r_{\\i_{\\phi_{1}, \\mathcal{D})\\geq(1-\\lambda)\\operator{Acc})\\geq(1-\\lambda)\\pot{D}(1-\\lambda)\\o{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot{dot\n' +
      '\n' +
      '우리는 OOD 테스트 샘플에서 보간된 RM을 평가하여 그림 3의 이 LMC를 경험적으로 검증한다. 이는 컴퓨터 비전[44, 45]의 맥락에서 다중 클래스 분류에 대한 유사한 관찰을 따르며, 이는 모델 수프[46, 47, 48] 변이체(부록 A.1의 관련 작업에서 세부)와 같은 가중치 평균화(WA) 작품의 흉막화로 이어졌다.\n' +
      '\n' +
      '** 레크 1**(사전 훈련 및 선형 조사 수입) _ WA의 효능은 심층 신경망 아키텍처에서 비선형[54] 및 순열 대칭[92]을 고려할 때 놀라운 것일 수 있다. WA는 손실 계곡[93]의 볼록 지역에 무게와 같은 미세 조정[45] 동안 분산을 제한하는 공유 사전 훈련 때문일 때만 실제로 가능하다. 대조적으로, LMC는 무작위 초기화가 공유되더라도 처음부터 훈련 가중치를 [45]할 때 보유하지 않는다. 이러한 이유로 LMC를 용이하게 하기 위해 [47, 48]를 따르고 분류기 \\(\\omega\\)를 초기화하기 위해 선형 프로빙을 사용하여 무작위 초기화와 비교하여 이러한 선형 프로빙은 특징 왜곡(91])을 방지한다.__.\n' +
      '\n' +
      '#####는 다양성을 지정합니다.\n' +
      '\n' +
      '한편으로 _WARM_은 미세한 가중치가 선형적으로 연결되도록 공유된 사전 학습을 필요로 한다. 반면, 가중치는 동일하지 않아야 하며, 실제로 미세 조정 가중치에 걸친 다양성은 WA[47]에서 관찰된 정확성 이득에 크게 기여한다. 전반적으로, 효과적인 _WARM_는 LMC를 보장하는 것과 가중치에 걸친 다양성 사이의 섬세한 절충이 필요하다.\n' +
      '\n' +
      '실제로 우리는 다음과 같은 다양성 출처[94]를 사용하여 RM 미세 조정을 _diverse에도 선형적으로 연결된_ 모델로 이끈다. 먼저, 서로 다른 미세 조정은 _다른 주문_에서 데이터 샘플을 볼 수 있다. 그림 2에 예시된 초기화_에서 _Baklava_라는 초기화_의 새로운 유형화_다양성, 특히 다른 학습률 및 드롭아웃 확률을 조사하는데, 이때 우리는 주어진 SFT 궤적을 따라 수집된 다양한 체크포인트 \\(\\{\\theta_{i}^{i}{{sf_{i=1}^{M}\\)의 RMs 열화기를 약간 초기화_분별 하이퍼파라미터_2, 특히 다른 체크포인트(\\{\\_{i}}<^{i = 1}. 바클라바_는 모델 수프[46]에서 공유 초기화 제약을 완화하여 단순히 동일한 사전 학습을 공유하는 것: _Baklava_는 실제로 모델 래타투유[48]에 대한 효율적인 대안이지만 다중 보조 작업이 필요하지 않다. 전반적으로 _Baklava_는 마지막 SFT 체크포인트에서 초기화만 하는 것에 비해 다양성을 증가시키는 반면 LMC에 대한 공유 사전 훈련 요구 사항은 오버헤드 없이 부착한다.\n' +
      '\n' +
      '확률적 중량 평균[95] 또는 이동 평균[96]의_에 이어 단일 RM 미세 조정(MM 미세 조정)을 따라 수집된 평균 체크포인트도 시도했다. 훈련에 비용이 적게 들기 때문에 흥미롭지만 그림 **3(a)***의 낮은 결과는 정확성-다양성 트레이드오프가 좋지 않음을 시사하며, 초기 체크포인트를 통합하면 개인의 정확도가 손상되고 나중에 체크포인트만 고려할 때 필요한 다양성을 가져오지 않을 것이다. 그 결과 WARM에서만 각 RM 미세 조정에서 마지막 체크포인트만 사용하기로 결정했다.__\n' +
      '\n' +
      '# 4는 _Warm__Warm_4의 이점을 인용하였다.\n' +
      '\n' +
      '이제 3절에서 이전에 설명한 _WARM_ 전략의 특성과 이점을 탐구한다. 우리는 보상 모델링을 위한 WA와 ENS의 경험적 비교와 4.3절에서의 새로운 일반적인 이론적 비교에 대한 분석을 근거했다.\n' +
      '\n' +
      '** 실험 설정*** 우리는 TL을 레버리지한다;DR 요약 벤치마크[97] LLM에 대한 보상 모델링의 표준, 즉 부록 B에서 아래에서 더 자세히 설명하고 추가 세부 사항을 간략하게 설명하며, RM의 목표는 적절하게 순위화되는 것과 같은 요약을 점수화하는 것이다. 훈련에서 우리는 GPT-3 [6] 변이체에 의해 후보 요약이 생성되는 Stiennon _et al_[14]의 데이터 세트 \\(\\mathcal{D}_{train}\\)를 사용한다. 라벨을 얻기 위해 [58]에서 RLAIF 절차를 따르는데, 여기서 PaLM-L[98]은 인간 선호도를 모방하는 피드백을 생성하기 위해 연쇄 항문 [99]로 촉발된다. 이 전략은 유사한 상호 협정을 가진 인간 라벨러와 유사하게 수행되며 오라클 측정으로 5절에서 유용할 것이다. RM은 선형 프로브 [91] 분류 층을 생성하는 \\(\\mathcal{D}_{train}\\)의 선호 요약에 미리 트레이닝되고 SFT가 채워진 PaLM-XXS 모델이다. 고온(\\mathcal{D}_{train}\\)으로 여러 개의 PaLM-S 정책에 의해 생성된 RM의 정확성(OOD) 테스트 데이터세트(\\mathcal{D}_{tood}\\)를 92k 쌍별 비교로 보고했으며, 그 중 일부는 사전 훈련된 다른 SFT-ed 및 다른 RLHF-ed 절차로 10k 단계에 대해 RM을 훈련했다.\n' +
      '\n' +
      '1차 분석에서는 안정적이고 보다 효율적인 앙상블링을 위한 중량 평균화이다.\n' +
      '\n' +
      '이전 작품[46, 47, 95]은 WA를 이해하는 가장 좋은 방법은 관찰 2에서 명확히 하는 바와 같이 ENS의 효율적인 근사치라고 주장해 왔다.\n' +
      '\n' +
      '평균 및 예측 앙상블은 유사하게 수행되며, 즉 모든 \\(\\lambda\\in[0,1]\\) 및 테스트 데이터 세트 \\(\\mathcal{D}_{test}\\),__ 테스트 데이터 세트(<\\mathcal{D}_{test}\\), -***Obs보존 2***(WA 및 ENS: 1차 분석)에 대해 마찬가지이다.\n' +
      '\n' +
      '\\{Acc}(r_{(1-\\lambda)\\lambda\\cdot\\cdot\\i_{2}},\\mathcal{D}_{test})\\ 엔트렉스\\text{Acc}((1-\\lambda)\\text{Acc}((1-\\lambda)\\text{D}_{test })\\ 엔트렉스\\text{Acc}((1-\\lambda)\\text{D}_{dot\\lambda{D}_{dot\\lambda{D}_{dot{D}_{D}_{test{D})\\text{Acc})\\text{D})\\text{Acc}(((1-\\mathcal{D}_{D}_{test)\\text{D})\\text{Acc})\\text{Acc{Acc})\\text{Acc{Acc})\\text{Acc})\\text{Acc})\\text{Acc})\\text{Acc}(((((1-\\text{D})\\text{\n' +
      '\n' +
      '그림 2: \\(|\\)_Baklava 다양성 절차._\\(|\\)_Baklava 다양성 절차. 사전 훈련된 LLM \\(\\theta^{\\pi t}\\)에서 시작하여 다양한 수의 SFT 훈련 단계에서 수집된 단일 SFT 실행(세척 화살표 - - - \\)을 따라 다른 체크포인트 \\(\\{\\theta_{i}^{ft}\\}_{i=1}^{M}\\)를 고려한다. 이러한 체크포인트는 선호 데이터세트(ick 고체 화살표 --)에서 \\(M\\) RM 미세 조정의 초기화 역할을 하여 \\(\\{\\phi_{i}\\}_{i=1}^{M}\\)를 학습한다. 마지막으로, 이러한 RM은 최종 모델 \\(\\파이^{\\text{WARM}\\\\)에 중량 평균(분류 화살표 - - )이다. 모델 수프[46] 및 모델 래타톨리케[48]의 요리 유추에 이어 다이아몬드 기하학적 모양 때문에 이 방법 _Baklava_를 명명했다.\n' +
      '\n' +
      '이론적으로 단순 테일러 확장은 \\(\\|\\phi_{1}-\\phi_{2}\\|\\ll 1\\)일 때 이러한 유사성을 정당화할 수 있다. 실증적으로 이것은 WA 및 ENS에 대한 \\(\\mathcal{D}_{\\mathit{ood}}\\)의 정확도 곡선이 밀접하게 일치하는 그림 3에서 검증된다. 이러한 유사성은 WA가 분산 감소 방법임을 정당화하는데, 그 다음 분산이 분포 변화 [47]의 지배적인 문제이기 때문에 이는 특히 가중치가 충분히 다양할 때 개별 RM \\(\\phi_{1}\\) 및 \\(\\phi_{2}\\)에 대한 그림 3의 상당한 이득을 설명한다. 이것은 ENS에 대한 효율성 혜택과 함께 _WARM_에서 향상된 신뢰성을 시사하며, 실제로 WA는 단일 가중치 세트를 유지하여 ENS의 메모리 및 추론 오버헤드를 제거한다.\n' +
      '\n' +
      '그림 4: **부패 실험은 관측치 3***를 검증한다. 우리는 \\(\\phi_{1}\\) 및 \\(\\phi_{2}\\)를 고려하지만, 그림 3(b)과 동일한 구성으로 독립적으로 미세 조정되었지만 이번에는 훈련 라벨의 25%가 파괴되었다. 그런 다음 다양한 데이터 하위 집합에 대한 WA 및 ENS의 성능을 보고한다. 우리는 WA가 그림 4(a)에서 파괴된 라벨의 암기를 감소시키며, 특히 그림 4(b)에서 깨끗한 훈련 샘플에서 ENS보다 약간 더 나쁘지만 WA가 더 잘 일반화되는 그림 4(d)에서 WNS의 성능이 향상된다.\n' +
      '\n' +
      '그림 3: ** 분포 실험에서는 TL에 대한 관측값 1 및 2**,DR 요약 벤치마크 [97]를 검증한다. ENS_는 \\_{\\i_{\\i_\\i_{\\i_\\i}}, \\_\\lambda_{\\i_\\i) 예측과 \\_\\i(1-\\i{D}\\_\\i_\\i<\\i_\\i)의 정확성을 나타내며, \\_\\i_\\i_\\i_\\i(1-\\i)는 \\_\\i(1-\\i{_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i<\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i_\\i첨부_\\i<\\i_\\i<\\i<\\i<\\i<\\i<\\i<\\i < <\\i<\\i<\\i<\\i 그림 3(b)은 단일 RM 미세 조정과 함께 두 개의 독립 RM 미세 조정 단계(8k 및 10k)에서 추출되지만, 그림 3(a)에서 다른 순서로 데이터를 보지만, 그림 3(c)에서는 서로 다른 수의 학습률(1e-4 및 4e-5), 그림 3(8k 및 12k)에서 수집된 다른 수의 FFT 체크포인트(8k 및 12k), 그림 2에 도입된 FT 체크포인트(8k 및 12k)에서 다른 수의 학습률(1e-4 및 4e-5), 그림 3(d)에서 다른 수의 학습률(8k)에서 다른 수의 학습률(4)에서 다른 수의 FFT 체크포인트(8k 및 4e-5), 그림 3(d)에서 수집된 다른 수의 FFT 체크포인트(8k 및 12k)에서 다른 수의 FFT 체크포인트(8k 및 12k)에서 다른 수의 FFT 체크포인트(8k 및 12k)에서 다른 수의 FFT 체크포인트(8k 및 12k)에서 다른 수의 FFT 체크포인트(8k)에서 초기화), 그림 3(d)에서 수집된 다른 수의 FFT 체크포인트(8k 및 12k)에서 초기\n' +
      '\n' +
      '2차 순서 분석.\n' +
      '\n' +
      '**A 놀라운 사실은 설명되지 않은 상태로 남아 있다.** WA는 분포 이동 하에서 ENS보다 약간 우수하며, 이는 그림 3에서 도표에서 볼 수 있으며 그림 B.1에서 모델 수프[46] 또는 DiWA [47]에서 그림 1에서 더 일관되게 볼 수 있다. 보다 일반적으로 WA는 OOD 일반화를 위한 최첨단 전략이며 일관되게 ENS를 능가하지만, 이는 이전 작업에서 설명되지 않아 WA와 ENS의 차이에 대한 새로운 통찰력을 촉구했다.\n' +
      '\n' +
      '** 부패 설정*** WA와 ENS의 차이에 대한 이해를 정제하기 위해 이항 라벨의 25%가 훈련에서 스와핑되는 새로운 설정을 제안한다. 그런 다음 부록 C.1이 풍부하고 그림 5에 집계된 그림 4의 집합당 정확도를 보고하고 훈련 데이터의 중단 하위 집합에서 WA에 대한 정확도 곡선은 예상 정확도 미만이고 다른 모든 하위 집합에 있다. 더 정확하게 말씀드리면, 우리는 다음의 관찰 3을 만듭니다.\n' +
      '\n' +
      'ENS에 대한 WA의 정확성 이득: 데이터가 훈련 분포에서 벗어나기 때문에 ENS에 대한 WA의 정확성 이득. _**Obs보존 3***(WA 및 ENS: 2차 순서 분석)._ 데이터가 성장한다.\n' +
      '\n' +
      '열차 부패에서**__WA_ \\(\\ll\\) _ENS: WA는 스와핑된 라벨이 있는 기차 샘플에서 ENS보다 훨씬 더 열악하여 부패 라벨을 표시하기 위해 암기가 감소하고 견고성이 향상되었음을 보여준다.\n' +
      '열차 청정에서* _WA_ \\(\\leq\\) _ENS: WA는 정확한 라벨이 있는 기차 샘플의 ENS보다 더 나쁘다.\n' +
      'ID 밸브에서** _WA_ \\(\\nless 승인된x\\) _ENS: WA는 분포 이동 없이 샘플의 ENS와 더 우수하거나 유사하다.\n' +
      'OOD 테스트에서** _WA_ \\(\\geq\\) _ENS: WA는 새로운 분포의 테스트 샘플에서 ENS보다 훨씬 우수하여 분포 이동 하에서 더 나은 신뢰성을 보여준다.\n' +
      '\n' +
      '전반적으로 이것은 중량 평균화가 앙상블 예측보다 덜 외우고 더 잘 일반화됨을 시사한다.\n' +
      '\n' +
      '무성실 의무를 부과합니다.\n' +
      '\n' +
      '우리는 이제 이 관찰 3에 이론적 지원을 제공했으며, 간략화된 가정은 WA가 실행 전반에 걸쳐 _불변_ 즉, 각 독립적인 실행에서 동시에 학습되는 예측 메커니즘에 대한 규칙화 역할을 한다는 것을 시사한다. 그런 다음 ENS와 대조적으로 WA는 암기를 유도하는 실행 특정 특징(학습될 확률이 낮은)을 저체중으로 만들기 때문에 부패에 대한 견고성을 향상시킬 것이다.\n' +
      '\n' +
      '******. 우리는 라벨 \\(F\\)과 관련된 라벨 \\(y\\in\\{-1,1\\}\\)을 사용한 단순화된 이진 분류 설정을 고려하고 \\(z^z^{j}}_{j=1}^{F}\\)과 같은 \\(z^{j}}z^{j}\\:z^{j}\\:z^{j}\\)의 특징을 가지고 있다. 입력 \\(x\\)로부터, 우리는 이진 분류기 \\(r(x)=\\omega^{\\top}f(x)\\를 훈련시킨다. [53] 이후 우리는 세 가지 주요 가정을 만든다. 먼저 _features 이종성_: \\(\\{z^{j}}_{j}_{j =1}^{F}\\)가 직교하다고 가정하며, 즉 \\(z^{j})^{\\top}z^{j}=0\\)이\\(j\\neq j^{\\prime}\\)일 때, \\(z^{j}. r\\(x^{j}\\mathb{R}{j}^{j}^{j}^{j}^{j}^{j}\\\\)에 의해 생성된 \\(x^{j}}\\^{j}\\\\)의 연결된 것으로 나타낼 수 있으며, 이는 입력 \\(x^{j}\\^{j}<^{j}^{j}^{j}\\i}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}^{j}\\)은 \\(x^{j}\\. 마지막으로, _binary featurizer_ 가정: 열화제 \\(f=\\left[f^{j}\\right]_{j=1}^{F}\\in\\{0,1\\}^{F}\\\\)가 입력을 만드는 특징의 이진 선택자라고 생각한다. 예를 들어, \\(y=1\\), \\(F=3\\), \\(x\\ 승인[z^{1},z^{2},z^{3}]\\) 및 \\(f=[1,0,1]\\)가 제1 및 제3 특징을 추출하도록 학습한 다음,\\(f(x)\\ 승인 z^{1}+z^{3}\\)를 추출한다. 우리는 포화제 \\(p_{j}\\)가 \\(j\\)-번째 특징 차원(z^{j}\\))을 사용할 확률을 나타내며, 이는 \\(f^{j}\\)을 확률 \\(p_{j}\\) 및 \\(0\\)를 갖는 \\(1\\)임을 의미한다. 더욱이 무한 훈련 샘플과 \\(\\sigma\\)에 대한 몇 가지 제약 하에서 [53]의 Lemma 5는 \\(r=\\omega^{\\top}f\\)를 배우기 위해 \\(f\\)에서 선택된 특징에 대한 최적의 선형 적합 \\(\\omega\\)이 \\(\\omega=1}\\sum_{j=1}^{F}^{j}^{j}^cdot z^{j}\\)임을 증명했다.\n' +
      '\n' +
      '그림 5: 다른 데이터 하위 집합에서 WA와 ENS 사이의 정확도의 차이에 대한 Hist그램이다.\n' +
      '\n' +
      '*** 결과는\\(M\\) RMs(\\{r_{i}=\\omega_{i}f_{i}f_{i}}_{i}}_{i =1}^{M}\\)를 고려하고,\\(M\\to\\\\)을 고려할 때 예측 앙상블 \\(r_{M}^{ENS}\\)의 한계 거동과 중량 평균 \\(r_{M}. (x{dime}}\\c{F}<\\a{j}<\\dime>}<\\b{{s}<\\f}<\\f{v>}<\\f{f}<\\f}<\\f{{j}<\\f}>> <\\f{{j}<\\f}<\\f>}<\\f}<\\f>}<\\f>}<\\f>}<\\f}<\\f>}<\\f>}<\\f}<\\f>}<\\f>}<\\f}<\\f>}<\\f>}<\\f}<\\f>}<\\f>}<\\f}<\\f>}<\\f>}<\\f>}<\\f>}<\\f>}<\\f>}<\\f>}<\\f>}<<\\f>}<<\\f>>}<\\f>}<\\f>}<<\\f>}<<\\f>}<<\\f>}<<\\f>}<<\\f>>}<\\\n' +
      '\n' +
      '\\[r_{M}^{ENS}(x)\\xrightarrow[M\\to\\hoty]{}\\mathbb{E}[r]]\\mathbb{E}[r]]\\칼륨 y\\cdot\\sum_{ j=1}^{F}\\mathbf{p}_{j}\\cdot|z^{j}.\n' +
      '\n' +
      '우리는\\(r_frac{i)를 고려할 때(<\\fthrough_\\a1> <\\fsum_\\i> <\\fise)> 및 <\\fobacterium{j}<\\ff}> <\\fise>를 가지고 있다.\n' +
      '\n' +
      '>^sum_{j{\\prime}=1}^{F}^{F}^{F}dot z^{j}}^{F}p_{j{{j}}\\cdot x^{{{f}}\\cot x^{{vime}}\\cot x^{{A}}\\cot x^{{j}}.\n' +
      '\n' +
      '**해석*** ENS의 경우 주어진 특징에 대한 계수는 \\(\\mathbf{p}_{j}\\)이며, 이 정보가 임의의 개별 네트워크에 의해 사용될 확률과 동일하다. 대조적으로, WA는 확률 \\(\\mathbf{p}_{j}^{2}\\)의 제곱을 포함한다. 따라서 WA는 중단된 훈련 샘플에 적합한 데 사용할 수 있는 경미한 특정 정보(소음 또는 컨텍스트 등)와 관련된 낮은 확률의 특징에 대한 의존도를 감소시키며, 이는 암기를 감소시켜 라벨 부패 하에 WA의 견고성을 설명한다. 분명히 WA는 실행_ 전반에 걸쳐 _미생물 불변 메커니즘을 선호하는 가장 가능성 있는 특징을 우선시하는 경향이 있다. 전반적으로 WA는 규칙화 역할을 하며 암기를 선호하는 실행 특정 메커니즘을 해결하고 일반화를 선호하는 실행불변 메커니즘을 보존함으로써 분배 이동 하에서 신뢰성을 개선함으로써 라벨 부패 하에서 견고성을 향상시키는 역할을 한다.\n' +
      '\n' +
      '우리는 무게 평균화가 실행 전반에 걸쳐 불변 예측 메커니즘을 유지한다고 주장한다. 이는 영역 일반화 [51, 100]에 인기 있는 불변 문헌[50]과 유추되며, 주요 개념은 도메인 전반에 걸쳐 불변하는 예측 메커니즘이 분포 이동 하에서 안정적인 인과 메커니즘이라는 것이다. 이는 이론적으로 OOD 일반화, 앙상블화 및 불변화를 위한 두 가지 주요 패러다임을 연결하며, 가중치 평균화가 실제로 양자의 이점을 얻는다는 것을 보여준다.___의 이론적으로는 보여준다.\n' +
      '\n' +
      '우리는 \\(L\\) 층이 있는 더 깊은 구조(\\mathbf{p}_{j}^{2}\\)에서 \\(Exmathbf{p}<L\\) 층으로 확장)을 얻는다. 그러나 완전한 일반성은 \\(L\\) 층이 있는 더 깊은 구조를 사용하여 \\(\\mathbf{p}_{j}^{L}\\)로 이어질 것이다. 직관적으로 WA는 정보 상에 AND-마스크를 적용하는데, 이는 이전 특징 공간과 다음 계층 가중치 모두에서 발견될 필요가 있다.\n' +
      '\n' +
      'WARM의 RM 설계에 적용되었을 때 _는 WARM의 RM 설계에 적용되었을 때, 우리는 WA가 일부 비뿌리 특징에 대한 의존도를 완화함으로써 WARM의 안정성[87]을 촉진한다고 주장한다. 실제로 WA는 WARM을 입력 공간에서 작은(잠재적으로 적대적인 [101]) 교란[102] 즉 더 부드럽고 매끄러운 [103]에 더 강건하게 만든다. 이는 예측 보상의 차이가 입력 공간의 거리에 의해 구속되는 보상[104, 105, 106]의 라이슈비츠성 성질에 관한 것이다. 다행히 이러한 평활성은 RL[107]에서 유용하며, 특히 정책 구배[108]의 안정성에 있어서는 "보상 가치의 샤프 변화가 표현되고 내면화되기 어렵다"[109]가 유용하기 때문이다. 이는 르슈비츠성에서 연구한 것으로 저자들은 "보상의 현지 라이슈비츠는 좋은 성과를 위한 사인 쿼 비조건"이라고 주장하는 [109]가 필요한 모든 것이다. 요약하면, 견고성은 안정성을 개선하고 경미한 입력 변형이 큰 보상 차이를 유발할 수 있을 때 발생하는 오류의 캐스케이드를 방해한다.__. ** 결손**에서 _WARM_의 이점을 요약한다. 첫째, WARM은 단일 모델을 반환하기 때문에 메모리나 계산 비용이 발생하지 않는 효율적이고 효율적이다. 둘째, _WARM_는 실행 전반에 걸쳐 메커니즘을 활용하는 동안 분산을 감소시켜 분포 이동 하에서 신뢰성을 향상시킨다. 마지막으로 _WARM_는 라벨 부패를 적용하여 시끄러운 선호에 대한 견고성을 증가시킨다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '이전 섹션에서 설명한 _WARM_\'s 이점을 경험적으로 검증하기 위해 TL에서 PaLM-XXS RM을 훈련시키고, TL에서 선호도 라벨이 생성된DR 요약 벤치마크[97]를 사슬 징후 [99]로 촉발했다. 최근 연구에서 점점 더 흔한 이 AI 표지화 접근법은 인간 평가에 대한 효율적인 대안으로 연구[26; 41; 110]가 연구[57; 58]에 의해 동기화되어 있으며, 이는 인간 선호도와 잘 상관관계가 있음을 나타내며, 비판적으로 보상 해킹을 평가하기 위한 자동 _쌍별 오라클 선호도_ 메트릭([17], 부록 C.4에서 논의된 증류 설정과 유사한 방식으로)을 제공한다. 또한, 우리는 _pointwise_에 대한 PaLM-XS RM을 레버리지한다.\n' +
      '\n' +
      '그림 6: ** 대조군 수는 BoN 실험**: 그림 6(a) 및 6(c)의 깨끗한 선호도 데이터 세트, 그림 6(b) 및 6(d)의 25% 부패이다. 우리는 후보 요약을 생성하기 위한 두 가지 SFT 정책을 고려하며, 하나는 PaLM 아키텍처 [98], 다른 하나는 T5 아키텍처 [111]에 있다. Ex(x\\)축은 BoN 정책과 SFT 정책 사이의 KL이며, \\(y\\)축은 대조군 보상 이득 w.r. 내지 RM \\(\\phi_{1}\\)를 나타내며, 이는 \\(\\mathcal{D}_{\\text{ood}}\\)에서 최고의 개별 RM이었다. 블루 라인은 \\(M\\) 가중치가 있는 _WARM_를 나타내며, _WARM_는 개별 RM(노란색)보다 높거나(붉은색의 ENS) 예측을 앙상블할 때 더 높다. 우리는 값 범위가 대략 3에서 7 사이인 그림 15의 실험에 대한 절대 제어 보상을 보고한다.\n' +
      '\n' +
      '그림 7: **Oracle 선호도 메트릭은 T5세대**에 대한 BoN 실험: 그림 7(a) 및 7(c)의 깨끗한 선호도 데이터 세트, 그림 7(b) 및 7(d)의 25% 부패이다. 우리는 \\(N\\) 대 다양한 값에 대한 승률을 도표한다. 두 가지 참조 전략: SFT(즉, 무작위 선택 또는 \\(N=1\\)와 동등하게 BoN을 선택하거나 _WARM_\\(M=6\\)에 따라 최상의 요약을 선택한다. 우리는 모든 전략이 SFT 참조를 이겼지만(모든 전략은 50% 승리율 이상) _WARM_\\(M=6\\) 참조를 이길 수는 없다는 것을 관찰했다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '**Oracle 선호도*** <그림 9>에서는 쌍별 오라클 선호도 AI 라벨러[58]에 따른 다양한 정책을 비교한다. 그림 9(a)에서 기준 정책은 SFT 초기화이며, 모든 RL 미세 조정 정책은 이 기준선을 능가하며 \\(3500\\) 단계(모든 정책 중 최고 승률) 후에 \\(99.8\\%\\)의 승률에 도달한다. 우리는 이 정책을 그림 9(b)에서 참조로 사용하는데, 다른 정책은 이길 수 없었다. 흥미롭게도 \\(M=10\\) 보상을 사용하면 보상 해킹을 지연시킬 수 있지만, 가중치 \\(\\{\\_{i}}_{i}.7}^{10}\\)를 사용하면 체중 선택 절차와 관련이 없다고 추측하며, 이는 a\\(\\{\\{D}_{\\{i}_{\\}_{i}1}^{{6}\\)보다 \\(\\{\\_{i}. 마지막으로 그림 9(c)에서 기준 정책은 \\(\\phi_{1}\\)로 RL 미세 조정(3000\\) 단계(\\mathcal{D}_{\\mathit{ood}}\\) 후에 얻어진다. i_WARM_(M=2\\)를 훈련한 정책이 이 접근법을 이긴 큰 단계가 있으며, 그림 9(b)의 이전 참조는 실제로 이에 대한 \\(79.4\\%\\) 승률을 가지고 있다.\n' +
      '\n' +
      '그림 8: ** RL 실험에 대한 대조 보상: 그림 8(a) 및 8(c)의 깨끗한 선호도 데이터 세트 및 그림 8(b)의 25% 부패이다. 블루 라인은 평균 \\(M\\) 가중치를 RM으로 할 때 정책의 RL 미세 조정, 어두운 경우 \\(M\\)가 높아진다. 개별 RM(노란색)으로 RL 미세 조정하거나 (붉은색으로) 예측을 앙상블할 때보다 높은 성능을 보인다. 그림 8(c)은 KL 정규화 강도를 제어하는 \\(\\pha\\)의 다른 값에 대해 _WARM_\\(M=6\\) 또는 \\(\\phi_{1}\\)로 미세 조정된 정책(RL)의 결과를 보여준다.\n' +
      '\n' +
      'RL 실험에 대한 그림 9: **Oracle 선호도 메트릭: 깨끗한 선호도 데이터셋. 우리는 SFT 정책에 대한 RL 미세 조정, \\(3500\\) 단계 후 _WARM_\\(M=6\\)로 미세 조정된 정책, \\(\\phi_{1}\\) 단계 후 \\(\\_{1}\\)로 미세 조정된 정책(RL)의 세 가지 참조 정책에 대해 RL 미세 조정율을 도표팅한다. 그림 19는 고정 훈련 단계 수에서 정책을 비교할 때 결과를 보고한다.****.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '***Benefits**_WARM_는 인간의 가치와 사회적 규범과의 AI 정렬을 개선하기 위한 유연하고 실용적인 방법을 나타낸다. 이 논문은 몇 가지 이점을 자세히 설명했으며 아래에서는 추가적이고 탐색적인 이점을 설명한다. WARM_는 _updable 머신러닝 패러다임의_[114]를 따르며, 이는 학습자 간 의사소통의 필요성을 제거함으로써, RM의 _fitly 간단한 병렬화_[115]를 가능하게 한다. 이는 데이터가 사적인 상태로 남아 있어야 하는 _ 급여 학습_ 시나리오[116]에서 사용을 용이하게 하며, 더 나아가 WA는 사적 선호도[52]의 암기를 감소시켜 사생활의 계층과 편향 완화를 추가할 것이다. 그런 다음 _WARM_의 간단한 확장은 예를 들어 다른 데이터셋에서 훈련된 RM을 서로 다른(클러스터) 라벨러로부터 결합할 것이다. 이러한 다양성은 _WARM_ 공연에 도움이 될 수 있지만 다중 객관적인 관점[117]에서도 도움이 될 수 있으며, RM의 불균일한 보간으로 _개인화된 정책_[39] 세트를 배울 수 있었다. 또한 WA가 치명적인 잊어버림을 제한하는 것으로 나타났기 때문에 [118, 119], _WARM_는 반복적이고 진화하는 선호도를 원활하게 지원할 수 있다. 마지막으로, 유망한 연구 방향은 _WARM_를 직접 선호도 최적화(DPO) 전략[120]으로 확장하여 RM을 평균화하는 것은 DPO 정책[121]을 평균화하기 위해 다시 캐스팅된다.\n' +
      '\n' +
      '***제한**_WARM_는 혁신적이면서도 예측 앙상블 방법과 비교할 때 특히 두 가지 한계에 직면할 수 있으며, 첫째, 예측 앙상블링은 다양한 아키텍처와 사전 개질에서 RM을 결합하여 가져온 다양성의 이점을 얻을 수 있으며, 둘째, 예측 앙상블링은 예측 불일치를 보상을 통합하여 불확실성 추정 및 제한 모델 드리프트를 제공할 수 있다. 그러나 [41]에서 로그트의 단순 평균화는 종종 불확실성 요소를 포함하는 보다 복잡한 예측 응집 기능에 비교 가능하게 수행된다는 것이 밝혀졌다. 또 다른 한계는 _WARM_가 특정 유형의 암기를 효과적으로 감소시키지만 선호도 데이터에 내재된 모든 형태의 가짜 상관관계 또는 편향을 완전히 근절하지 않는다는 것이다. 예를 들어, 각 개별 RM이 주로 기준으로서 요약 길이에 의존한다면 _WARM_는 이러한 경향을 복제할 가능성이 있다. 따라서 대안적 방법(OOD 일반화 문헌에서)은 예를 들어 불변 규칙화 [51, 100] 또는 마지막 계층 재교육 [122]에 기초한 방법이 필요할 수 있다. 마지막으로 _WARM_는 RLHF [18]에서 다른 문제를 다루지 않고 보상 모델링만을 향상시키며, 따라서 안전 위험 [19, 34, 35]를 오정렬 [32, 33], _WARM_가 책임 있는 AI의 더 큰 맥락에서 고려되어야 한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '결론적으로, 우리는 라벨 부패에 따른 분포 이동 및 견고성이라는 보상 모델링에 있어 두 가지 중요한 문제를 해결하기 위해 웨이브 Aver devast Reward Model(_WARM_)을 소개한다. 다양한 미세 조정에서 얻은 다중 RM의 가중치를 평균함으로써 _WARM_는 인간 피드백으로부터의 강화 학습에서 보상 해킹을 완화하기 위한 효율적인 해결책으로 나타난다. 우리의 경험적 결과는 요약에 적용될 때 그 효과를 보여준다. 우리는 _WARM_가 보다 정렬되고 투명하며 효과적인 AI 시스템에 기여하여 보상 모델링에 대한 추가 탐색을 장려할 것으로 기대한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. (p. 1)\n' +
      '* [2] OpenAI. Gpt-4 technical report. 2023. (p. 1)\n' +
      '* [3] Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. (p. 1 and 3)\n' +
      '* [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2019. (p. 1)\n' +
      '* [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020. (pp. 1, 7, and 27)\n' +
      '* [7] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _ICLR_, 2022. (pp. 1 and 3)\n' +
      '* [8] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In _ACL_, 2022. (p. 1)\n' +
      '* [9] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. (p. 1)\n' +
      '* [10] Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Leonard Hussenot, Orgad Keller, et al. Factually consistent summarization via reinforcement learning with textual entailment feedback. In _ACL_, 2023. (p. 1)\n' +
      '* [11] Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned reward functions. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [12] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _NeurIPS_, 2017. (pp. 1, 2, 3, 5, and 27)\n' +
      '* [13] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint_, 2019. (pp. 1 and 27)* [14] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _NeurIPS_, 2020. (pp. 1, 7, and 27)\n' +
      '* [15] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. _arXiv preprint_, 2021. (pp. 1 and 27)\n' +
      '* [16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models. _arXiv preprint_, 2023. (pp. 1, 5, 12, and 27)\n' +
      '* [17] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In _ICML_, 2023. (pp. 1, 11, and 33)\n' +
      '* [18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jeremy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement learning from human feedback. _TMLR_, 2023. (pp. 1 and 14)\n' +
      '* [19] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in AI safety. _arXiv preprint_, 2016. (pp. 1, 2, 3, 4, and 14)\n' +
      '* [20] Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. [https://openai.com/research/faulty-reward-functions](https://openai.com/research/faulty-reward-functions), 2016. (pp. 1 and 4)\n' +
      '* [21] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment. _arXiv preprint_, 2021. (pp. 1 and 4)\n' +
      '* [22] Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In _NeurIPS_, 2022. (pp. 1 and 4)\n' +
      '* [23] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In _ICLR_, 2022. (pp. 1 and 4)\n' +
      '* [24] Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in reinforcement learning from human feedback. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [25] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal? end-to-end learning for negotiation dialogues. _arXiv preprint_, 2017. (p. 1)* [26] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. _arXiv preprint_, 2023. (pp. 1 and 11)\n' +
      '* [27] Marilyn Strathern. Improving ratings: audit in the british university system. _European Review_, 1997. (p. 1)\n' +
      '* [28] Ethan Perez, Sam Ringer, Kamile Lukositte, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with model-written evaluations. _arXiv preprint_, 2022. (p. 1)\n' +
      '* [29] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards understanding sycophaney in language models. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [30] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose opinions do language models reflect? In _ICML_, 2023. (p. 1)\n' +
      '* [31] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conversational ai: Converging evidence on chatgpt\'s pro-environmental, left-libertarian orientation. _arXiv preprint_, 2023. (p. 1)\n' +
      '* [32] Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for advanced machine learning systems. _Ethics of AI_, 2016. (pp. 1, 3, and 14)\n' +
      '* [33] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective. _arXiv preprint_, 2022. (pp. 1, 3, 14, and 27)\n' +
      '* [34] Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research. _arXiv preprint_, 2022. (pp. 1 and 14)\n' +
      '* [35] Dan Hendrycks. Natural selection favors Als over humans. _arXiv preprint_, 2023. (pp. 1 and 14)\n' +
      '* [36] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. _NeurIPS_, 2020. (pp. 2 and 4)\n' +
      '* [37] Daniel Shin, Anca Dragan, and Daniel S. Brown. Benchmarks and algorithms for offline preference-based reward learning. _TMLR_, 2023. (p. 2)\n' +
      '* [38] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamille Lukositte, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. _arXiv preprint_, 2022. (pp. 2 and 4)\n' +
      '* [39] Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In _NeurIPS_, 2023. (pp. 2, 14, and 26)\n' +
      '* [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _NeurIPS_, 2022. (pp. 2, 3, and 27)\n' +
      '* [41] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. _arXiv preprint_, 2023. (pp. 2, 4, 5, 11, 12, 14, and 27)* [42] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. _arXiv preprint_, 2023. (pp. 2, 5, and 27)\n' +
      '* [43] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _NeurIPS_, 2017. (pp. 2 and 5)\n' +
      '* [44] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _ICML_, 2020. (pp. 3, 6, and 26)\n' +
      '* [45] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In _NeurIPS_, 2020. (pp. 3, 6, and 26)\n' +
      '* [46] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _ICML_, 2022. (pp. 3, 6, 7, 9, 26, and 28)\n' +
      '* [47] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In _NeurIPS_, 2022. (pp. 3, 5, 6, 7, 8, 9, 26, and 28)\n' +
      '* [48] Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Leon Bottou, and David Lopez-Paz. Model Ratatouille: Recycling diverse models for out-of-distribution generalization. In _ICML_, 2023. (pp. 3, 6, 7, 26, and 28)\n' +
      '* [49] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. SWAD: Domain generalization by seeking flat minima. In _NeurIPS_, 2021. (pp. 3 and 26)\n' +
      '* [50] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _ICML_, 2013. (pp. 3 and 10)\n' +
      '* [51] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint_, 2019. (pp. 3, 4, 10, and 14)\n' +
      '* [52] Kerem Zaman, Leshem Choshen, and Shashank Srivastava. Fuse to forget: Bias reduction and selective memorization through model fusion. _arXiv preprint_, 2023. (pp. 3, 14, and 26)\n' +
      '* [53] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. In _ICLR_, 2024. (pp. 3, 9, and 26)\n' +
      '* [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017. (pp. 3 and 6)\n' +
      '* [55] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint_, 2021. (pp. 3 and 26)\n' +
      '* [56] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In _CVPR_, 2014. (p. 3)\n' +
      '* [57] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. _arXiv preprint_, 2022. (pp. 4 and 11)* [58] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. _arXiv preprint_, 2023. (pp. 4, 7, 11, 12, 13, 27, and 28)\n' +
      '* [59] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 1952. (p. 4)\n' +
      '* [60] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Reinforcement learning_, 1992. (pp. 4, 12, and 28)\n' +
      '* [61] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint_, 2017. (pp. 4 and 12)\n' +
      '* [62] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _ICLR_, 2021. (p. 4)\n' +
      '* [63] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In _ICML_, 2021. (p. 4)\n' +
      '* [64] Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In _NeurIPS_, 2020. (p. 4)\n' +
      '* [65] Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, and Moncef Gabbouj. Learning distinct features helps, provably. _arXiv preprint_, 2021. (p. 4)\n' +
      '* [66] Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse ImageNet models transfer better. _arXiv preprint_, 2022. (p. 4)\n' +
      '* [67] Alexander D\'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspecification presents challenges for credibility in modern machine learning. _JMLR_, 2020. (pp. 4 and 26)\n' +
      '* [68] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. ID and OOD performance are sometimes inversely correlated on real-world datasets. In _NeurIPS Workshop_, 2023. (p. 4)\n' +
      '* [69] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In _ICML_, 2017. (p. 4)\n' +
      '* [70] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\'s uncertainty? evaluating predictive uncertainty under dataset shift. In _NeurIPS_, 2019. (p. 4)\n' +
      '* [71] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain generalization. In _NeurIPS_, 2021. (p. 4)\n' +
      '* [72] Herbert A Simon. Bounded rationality. _Utility and probability_, 1990. (p. 4)\n' +
      '* [73] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather than assuming, human biases for reward inference. In _ICML_, 2019.\n' +
      '\n' +
      '* [74] Timo Kaufmann, Sarah Ball, Jacob Beck, Eyke Hullermeier, and Frauke Kreuter. On the challenges and practices of reinforcement learning from real human feedback. 2023. (p. 4)\n' +
      '* [75] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint_, 2022. (p. 4)\n' +
      '* [76] Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, et al. Rewarding chatbots for real-world engagement with millions of users. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [77] Condorcet. Essai sur l\'application de l\'analyse a la probabilite des decisions rendues a la pluralite des voix. 1785. (p. 4)\n' +
      '* [78] Silviu Pitis. Failure modes of learning reward models for l lms and other sequence models. In _ICML_, 2023. (p. 4)\n' +
      '* [79] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [80] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. State of what art? a call for multi-prompt llm evaluation. _arXiv preprint_, 2023. (p. 4)\n' +
      '* [81] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose Miguel Hernandez-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In _ICML_, 2017. (pp. 5 and 12)\n' +
      '* [82] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In _ICML_, 2019. (pp. 5 and 12)\n' +
      '* [83] Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. In _ACL_, 2020. (p. 5)\n' +
      '* [84] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In _ICML_, 2020. (p. 5)\n' +
      '* [85] Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In _ICML_, 2020. (pp. 5 and 27)\n' +
      '* [86] William Saunders, Girish Sastry, Andreas Stuhlmuller, and Owain Evans. Trial without error: Towards safe reinforcement learning via human intervention. In _AAMAS_, 2018. (p. 5)\n' +
      '* [87] Binghai Wang et al. Secrets of rlhf in large language models part ii: Reward modeling. _arXiv preprint_, 2023. (pp. 5, 10, and 27)\n' +
      '* [88] Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss functions. In _ICML_, 1996. (p. 5)\n' +
      '* [89] Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. In _ICNN_, 1996. (p. 5)\n' +
      '* [90] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general principles for constitutional ai. _arXiv preprint_, 2023.\n' +
      '\n' +
      '* [91] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In _ICLR_, 2022. (pp. 5, 6, 7, and 28)\n' +
      '* [92] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In _ICLR_, 2022. (p. 6)\n' +
      '* [93] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. In _EMNLP_, 2023. (p. 6)\n' +
      '* [94] Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. In _ICLR_, 2022. (pp. 6 and 8)\n' +
      '* [95] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _UAI_, 2018. (pp. 7 and 26)\n' +
      '* [96] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. In _NeurIPS_, 2021. (pp. 7 and 26)\n' +
      '* [97] Michael Volske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn automatic summarization. In _ACL Workshop_, 2017. (pp. 7, 8, and 11)\n' +
      '* [98] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PalM 2 technical report. _arXiv preprint_, 2023. (pp. 7, 11, 12, 27, 28, and 30)\n' +
      '* [99] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022. (pp. 7, 11, and 27)\n' +
      '* [100] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In _ICML_, 2022. (pp. 10 and 14)\n' +
      '* [101] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint_, 2013. (p. 10)\n' +
      '* [102] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaudhuri. Adversarial robustness through local lipschitzness. _arXiv preprint_, 2020. (p. 10)\n' +
      '* [103] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new neural network smoothness constraints. In _NeurIPS ICBINB_, 2020. (p. 10)\n' +
      '* [104] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. _NeurIPS_, 2017. (p. 10)\n' +
      '* [105] Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. _IEEE Transactions on Signal Processing_, 2017. (p. 10)\n' +
      '* [106] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _ICML_, 2019. (p. 10)\n' +
      '* [107] Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: Challenges and benchmarks from technical process control. _Machine learning_, 2011. (p. 10)* [108] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision processes. _Machine Learning_, 2015.\n' +
      '* [109] Lionel Blonde, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame off-policy generative adversarial imitation learning. _Machine Learning_, 2022.\n' +
      '* [110] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint_, 2023.\n' +
      '* [111] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.\n' +
      '* [112] Jacob Hilton. KL divergence of max-of-n, 2023.\n' +
      '* [113] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D\'Amour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. _arXiv preprint_, 2024.\n' +
      '* [114] Colin Raffel. Building Machine Learning Models Like Open Source Software. _ACM_, 2023.\n' +
      '* [115] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Branch-Train-Merge: Embarrassingly parallel training of expert language models. _arXiv preprint_, 2022.\n' +
      '* [116] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _AISTATS_, 2017.\n' +
      '* [117] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. In _NeuriPS_, 2023.\n' +
      '* [118] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of strong zero-shot models for continual learning. In _NeurIPS Workshop_, 2022.\n' +
      '* [119] Steven Vander Eeckt et al. Weight averaging: A simple yet effective method to overcome catastrophic forgetting in automatic speech recognition. _arXiv preprint_, 2022.\n' +
      '* [120] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint_, 2023.\n' +
      '* [121] Maxime Labonne. NeuralBeagle14-7B. [https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF](https://huggingface.co/mlabonne/NeuralBeagle14-7B-GGUF), 2024.\n' +
      '* [122] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. In _ICLR_, 2023.\n' +
      '* [123] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _ICLR_, 2019.\n' +
      '\n' +
      '* [124] John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study. _PLOS Medicine_, 2018.\n' +
      '* [125] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection selects shortcuts over signal. _Nature Machine Intelligence_, 2021.\n' +
      '* [126] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In _CVPR_, 2022.\n' +
      '* [127] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. In _NeurIPS_, 2022.\n' +
      '* [128] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. In _ACL_, 2023.\n' +
      '* [129] Nikolaos Dimitriadis, Pascal Frossard, and Francois Fleuret. Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models. _arXiv preprint_, 2022.\n' +
      '* [130] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unival: Unified model for image, video, audio and language. _TMLR_, 2023.\n' +
      '* [131] Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal. Seasoning model soups for robustness to adversarial and natural distribution shifts. In _CVPR_, 2023.\n' +
      '* [132] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Joao Sedoc, and Naomi Saphra. Linear connectivity reveals generalization strategies. In _ICLR_, 2023.\n' +
      '* [133] Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep reinforcement learning with weight averaging.\n' +
      '* [134] Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online adaptation in reinforcement learning. In _ICLR_, 2022.\n' +
      '* [135] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for forming multi-task policies. In _ICLR RRL Workshop_, 2023.\n' +
      '* [136] Michael Noukhovitch, Samuel Lavoie, Florian Strub, and Aaron Courville. Language model alignment with elastic reset. In _NeurIPS_, 2023.\n' +
      '* [137] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In _ICLR_, 2023.\n' +
      '* [138] Nico Dhaheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. Elastic weight removal for faithful and abstractive dialogue generation. _arXiv preprint_, 2023.\n' +
      '* [139] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _TNNLS_, 2022.\n' +
      '* [140] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. _ICLR_, 2017.\n' +
      '\n' +
      '* [141] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In _CVPR_, 2019.\n' +
      '* [142] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neptune: Noisy embeddings improve instruction finetuning. _arXiv preprint_, 2023.\n' +
      '* [143] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise for deep neural networks. In _AAAI_, 2017.\n' +
      '* [144] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama. Sample selection with uncertainty of losses for learning with noisy labels. In _ICLR_, 2022.\n' +
      '* [145] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _ICML_, 2018.\n' +
      '* [146] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _NeurIPS_, 2018.\n' +
      '* [147] Maryam Sabzevari. _Ensemble learning in the presence of noise_. PhD thesis, Universidad Autonoma de Madrid, 2019.\n' +
      '* [148] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In _ICML_, 2000.\n' +
      '* [149] W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca Dragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and mistaking it for reward. _arXiv preprint_, 2023.\n' +
      '* [150] Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning from multiple teachers. _arXiv preprint_, 2023.\n' +
      '* [151] Sian Gooding and Hassan Mansoor. The impact of preference agreement in reinforcement learning from human feedback: A case study in summarization. _arXiv preprint_, 2023.\n' +
      '* [152] Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-augmented reward modeling. In _ICLR_, 2023.\n' +
      '* [153] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. _arXiv preprint_, 2023.\n' +
      '* [154] Anonymous. RIME: Robust preference-based reinforcement learning with noisy human preferences. In _Submitted to ICLR_, 2023.\n' +
      '* [155] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences. _arXiv preprint_, 2023.\n' +
      '* [156] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _ICML_, 2018.\n' +
      '\n' +
      '* [157] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language models. _arXiv preprint_, 2023.\n' +
      '\n' +
      '참조된 비밀 번호 모델_WARM_\n' +
      '\n' +
      'Supplementary material\n' +
      '\n' +
      '이 보충 재료는 다음과 같이 구성되어 있습니다.\n' +
      '\n' +
      '* 부록 A는 관련 작업을 개선합니다.\n' +
      '* 부록 B는 몇 가지 실험 세부 사항을 명확히 합니다.\n' +
      '* 부록 C는 우리의 실험을 장식합니다.\n' +
      '\n' +
      '업무 관련 신청자\n' +
      '\n' +
      '본 논문은 특히 선형 모드 연결성(부록 A.1 참조)으로부터 OOD 일반화 문헌의 통찰력을 인용하고, 이를 효율적이고 신뢰할 수 있고 강력한 보상 모델(부록 A.2 참조)의 설계에 적용한다.\n' +
      '\n' +
      '유통 일반화, 선형 모드 연결성 및 암기###\n' +
      '\n' +
      '미세 조정*** 파인-튜닝 기반 모델[55]에서**LMC는 새로운 분포에 잘 일반화하는 전문 모델[55]이 많은 실제 응용 프로그램[123, 124, 125]에 중요하다. 최근에는 이동 평균[49, 95, 96], WiSE 미세 조정[126], 모델 수프[46], 디WA[47] 및 모델 래타투유[48]와 같은 다양한 가중치 평균(WA)의 변이체가 성능을 향상시킬 수 있었다. 이러한 작업은 미세 조정 가중치에 걸쳐 LMC[44, 45]에 의존하며, 이는 다양한 작업 [48, 127, 128]에서 미세 조정으로 확장되거나 형태[130] 또는 다른 손실 [47, 131]로 확장되었지만 [132]는 몇 가지 한계를 강조했다. WE는 최근 RL 세트[39, 133, 134, 135, 136]에서 사용되었지만 특히 [39, 136]의 RLHF에는 사용되었지만 보상이 아닌 정책만 결합했다.\n' +
      '\n' +
      'WA***에 합류하면 WA는 몇 가지 이점이 있습니다. 첫째, WA는 손실 경관[49]을 플러싱한다. 둘째, WA는 예측 앙상블을 근사화하여 추정기[46, 47] 및 태클 모델 누락 [67]의 분산을 감소시킨다. 셋째, WA는 다중 태스크[127], 다중 주관적[39] 또는 지속적인 학습[118] 설정에 유용할 수 있는 모델의 능력[137, 138]을 결합한다. 마지막으로, 최근 WA는 [52, 53]에서 _FalseFalseTrue_라는 현상과 함께 가짜 상관 관계에 따라 약간의 이점을 제공할 수 있는 것으로 나타났다. 이 작품 [52, 53]은 4.2절에서 암기 실험과 유사성을 공유하지만 라벨 부패 하에 WA 규칙화 특성과 일반화에 대한 결과를 분석한 최초의 것이다. 대조적으로, [52]에서 네트워크는 서로 다른 데이터셋에 대해 훈련되고 [53]의 이론은 실제로 대부분 예측 앙상블을 위해 개발된다.\n' +
      '\n' +
      '***스마트화**전통적 접근[139] 중단 라벨의 암기를 다루는 전통적인 접근[140]은 보통 명시적인 규칙화[141], 특정 데이터 증강[142], 손실 조정[143] 또는 샘플 선택[144]을 필요로 한다. 일부 다른 전략은 앙상블을 기반으로 하며, 자체 표지 필터링[145, 146] 또는 포장 다양성 절차[147]로 잠재적으로 파괴된 샘플을 필터링한다. 우리가 아는 한 WA와 함께 부패를 해결하기 위해 관리하는 동일한 데이터 세트에 훈련된 여러 모델을 결합한 첫 번째 전략을 제안한다.\n' +
      '\n' +
      '### Reward modeling\n' +
      '\n' +
      'LLM을 정렬하는 데 있어 중심 도전 중 하나는 환경에서 명시적인 보상이 없다는 것이다.k.a. 외부 정렬 챌린지[33]이다. 인버스 강화 학습[148]은 전문가 시연에서 보상 모델(RM)을 도출하려고 시도하지만, 대부분의 최근 노력[12, 13, 14, 15, 40]은 주로 인간의 선호로부터 학습에 중점을 둔다. RL 이후 LLM 공연을 강화하고 실제 응용 분야에 안전한 배치를 위해 중요성을 가지고 있음에도 불구하고 RM을 가장 잘 설계하는 방법은 아마도 영장류보다 덜 관심을 받고 있다. 일부 연구[149]는 식 (1)에서 손실 함수를 정제하고자 한다. 다른 접근법은 보다 많은 데이터 지향이며, 예를 들어 LLaMA-2 [16]는 새로운 생성 분포에 적응하기 위해 RM의 지속적인 학습을 포함하며 [85, 150]은 능동적 학습 패러다임[151]을 따른다. 도구[152] 또는 부가 정보[153]로 보상하는 것은 훨씬 더 최근적이고 매우 유망한 추세를 나타낸다. 라벨 비리 및 보상 모델링의 교차점에서 제한된 노력이 이루어졌고, [154]는 소규모 학술적 운동 과제에 대한 선호 데이터 세트를 필터링하려고 노력했으며, 동시 [87]은 라벨 스무딩 및 플러킹을 적용하는 것을 제안한다. 실제로 보상 앙상블링은 보상 해킹(41, 42])을 완화하기 위한 가장 논의되는 방법이며, _WARM_가 오버헤드를 제거하면서 ENS를 이길 수 있음을 보여준다. 마지막으로 DPO[120]에 이어 최근 추세는 보상 모델링과 정책 학습이 융합되지만 정책은 여전히 선호 데이터[155]를 해킹하는 경향이 있으므로 몇 가지 훈련 단계 및 매우 작은 학습 비율만 필요로 한다. 이론적으로 RM의 WA에 해당하는 DPO 정책의 WA는 [121]에서 입증된 바와 같이 이미 공공 벤치마크에 대한 유의미한 실증적 결과를 가진 유망한 연구 방향이다.\n' +
      '\n' +
      '신청자 B 시행 내역\n' +
      '\n' +
      '### Dataset details\n' +
      '\n' +
      '표적을 위해 레드다이트 TL;DR 데이터셋[14]을 사용하여 고품질 확보를 위해 필터링된 레드디트의 게시물을 포함한다. [14]의 훈련 요약은 OpenAI GPT-3 [6] 변이체에 의해 생성된다. 데이터셋에는 123k 게시물이 포함되어 있으며, ID 검증 세트로 \\(\\ason\\)5%가 유지된다. 92k 쌍별 비교를 갖는 OOD 데이터세트 \\(\\mathcal{D}_{ood}\\)에서 후보 반응을 생성하기 위해 고온을 가진 여러 PaLM-XS 정책을 고려했으며 그 중 일부는 미리 훈련된 다른 SFT-ed 및 기타 RLHF-ed, 목표는 다양한 요약 세트를 얻는 것이었다.\n' +
      '\n' +
      'AI 표시 세부 정보는 세부 정보 표시입니다.\n' +
      '\n' +
      '우리의 모델을 평가하는 데 이상적인 접근법은 인간의 선호도를 포함하지만 RLAIF[58]에서 더 저렴한 AI 표지화 절차에 중점을 둔다. 우리는 지시된 미세 조정 PaLM-L[98] LLM1에 질의하여 인간의 선호도를 모방하는 선호도를 생성했다. 구체적으로, 우리는 RLAIF[58]의 "자세한 + CoT 0 샷" 프롬프트 전략을 따르고 있으며, 그 결과에 따라 가장 좋은 것은 사슬 검색(99], 512 토큰 및 온도 \\(T=0.0\\)의 최대 디코딩 길이(즉, 욕심 디코딩)와 함께 제로 샷 프롬프트를 포함한다. 위치 편향을 피하기 위해 우리는 두 가지 가능한 순서로 AI 라벨러를 실행한다. 이 전략은 유사한 상호 합의와 함께 인간 랩러와 유사하게 수행하는 것으로 나타났다. 부패 실험을 위해 훈련 샘플의 25%에 대한 라벨을 스왑합니다.\n' +
      '\n' +
      '구글 클라우드 베르텍스 AI[https://cloud.google.com/vertex-ai/docs/learn/model] (https://clogle.com/vertex-ai/docs/ickative-ai/learn/models)를 통해 이용할 수 있다.\n' +
      '\n' +
      '모델링 세부 정보를 입력하세요.\n' +
      '\n' +
      'RM은 PaLM-XXS 모델[98]이다. 그들은 먼저 사전 학습한 다음 레드다이트 TL에 미세 조정, 배치 크기가 128인 12k 단계에 대한DR 데이터셋 및 학습률 \\(10^{-5}\\)로 Ada 팩터[156] 최적화기를 감독한다. i_Baklava_ 레시피에 따라 실제로 이 SFT 미세 조정 단계를 따라 다른 체크포인트에서 보상 모델링을 출시하며, {8k, 10k, 12k} 단계에서 너무 많은 체크포인트를 복용하면 [157]에서 관찰된 바와 같이 RM 정확도가 급격히 감소한다. 이 LLM을 분류기로 변환하기 위해 선형 프로브 [91] 분류층(모든 RM에 대해 동일한)을 연결한다. [91]에서 설명한 바와 같이 WA에 필요한 LMC를 용이하게 하는 초기화로부터 기능이 너무 멀리 이동하는 것을 방지한다.\n' +
      '\n' +
      '우리는 모든 RM을 10k 단계, 배치 크기 128, 아다 인자[156] 최적기, {1e-5,4e-5,1e-4}에서 샘플링된 학습 비율 및 {0.05, 0.1}에서 드롭아웃 확률을 훈련시킨다. 이것은 LMC를 보존하기 위해 경도 범위의 하이퍼파라미터를 레버리지하기 위해 [47]에서 실용적 리턴드를 따른다. 이전 작품[48]에서 LMC를 변경하지 않았기 때문에 더 많은 단계를 교육하는 것이 도움이 될 수 있다.\n' +
      '\n' +
      '실제로, 깨끗한 라벨을 사용한 주요 실험을 위해 10개의 보상 모델을 출시하며, \\(\\mathcal{D}_{ood}\\)에 대한 정확도 감소 순위(\\{\\phi_{i}\\}_{i=1}^{10}\\)를 나타낸다. 따라서, 서로 다른 구획에서 \\(\\phi_{1}\\) 및 \\(\\phi_{2}\\)라는 RM은 유통 이동 중인 개별 공연에 따라 가장 잘 설정된다. 그런 다음 _WARM_\\(M=2\\)는 실제로 \\당 정의된 RM(\\frac{\\phi_{1}+\\phi_{2}}{2}\\)인 반면 ENS \\(M=2\\)은 예측을 평균한다. 보다 일반적으로 \\(M\\) 가중치를 갖는 _WARM_는 \\(M\\) 최고 중량 \\(M\\)의 WA이다(\\{\\phi_{i}\\}_{i=1}^{M}\\). 이 가중치 선택 절차의 주요 동기는 그림 10에서 검증된 바와 같이 잠재적으로 나쁜 RM을 제거하는 것이며, 그 10 RM에 걸쳐 서로 다른 순열을 고려하는 것이다. 측면어음으로 [46]에서와 같은 욕심 있는 절차가 공연을 더욱 향상시킬 수 있다고 추측한다.\n' +
      '\n' +
      '학습 내용 강화는 자세한 내용.\n' +
      '\n' +
      '정책 모델과 가치 모델 모두 동일한 SFT 모델에서 초기화된 PaLM-XS[98]이다. 그런 다음 온도 \\(T=0.9\\), 배치 크기 128, 아다 인자[156] 최적화기, 학습 속도 \\(10^{-5}\\) 및 정책 평가 2k 단계로 정책에서 샘플을 생성한다. 라벨 부패가 없는 주요 실험에서 KL 정규화를 위해\\(\\alpha=0.003\\)를 설정하고 라벨 부패로 \\(\\alpha=0.01\\)를 설정했다. [58] 후 분산 감소를 위한 기준값 함수를 사용하여 변형된 버전의 REINFORCE[60]을 사용했다.\n' +
      '\n' +
      '그림 10: ** 중량 선택 절차의 분석. 우리는 평균 \\(M\\) 가중치(10점)로 인한 정확도를 표시하며, 여기서 이러한 가중치는 다양한 선택 절차를 기반으로 선택된다. 이것은 가장 잘에서 최악의 모델을 선택하는 것이 신뢰할 수 있는 휴리스틱 역할을 한다는 것을 효과적으로 검증한다.**.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:29]\n' +
      '\n' +
      '### BoN experiments\n' +
      '\n' +
      '그림 15 | Same는 그림 8이지만 **는 BoN 실험**에 대한 대조 보상의 절대 값을 나타낸다. 우리는 후보 요약을 생성하기 위한 두 가지 SFT 정책을 고려하며, 하나는 PaLM 아키텍처 [98], 다른 하나는 T5 아키텍처 [111]에 있다. 두 경우 모두 점별 제어 RM 측면에서 _WARM_가 ENS와 개별 네트워크보다 더 잘 수행된다는 것을 관찰했다.\n' +
      '\n' +
      '그림 16 | **(클란 설정)은 두 개의 미세 조정 \\(\\phi_{1}\\) 및 \\(\\phi_{3}\\)가 공유 SFT에서 12k 및 8k 단계에서 각각 수집된 서로 다른 열화기 초기화를 가질 때 _Baklava_로 BoN 실험**(클란 설정)에 대한 대조 보상이다.\n' +
      '\n' +
      '그림 15 | Same는 그림 8이지만 **는 BoN 실험**에 대한 대조 보상의 절대 값을 나타낸다. 우리는 후보 요약을 생성하기 위한 두 가지 SFT 정책을 고려하며, 하나는 PaLM 아키텍처 [98], 다른 하나는 T5 아키텍처 [111]에 있다. 두 경우 모두 점별 제어 RM 측면에서 _WARM_가 ENS와 개별 네트워크보다 더 잘 수행된다는 것을 관찰했다.\n' +
      '\n' +
      '### RL experiments\n' +
      '\n' +
      '손상된 선호 데이터셋이 있는 테스트 데이터셋이 포함된 실험 c.3.1-########.\n' +
      '\n' +
      '그림 17: **Oracle 선호도 메트릭 BoN 실험**(클레이브 설정)에 대한 제품입니다. 그림 17(a)는 그림 7(c)을 확인하지만 PaLM SFT에서 세대를 확인한다. 그림 17(b)은 BoN 대 BoN에 대해 \\(M=6\\) 및 항상 \\(N=1000\\)가 있는 _WARM__WARM_의 경우 T5 세대의 BoN에 대한 승률을 보여준다. i\\(1\\leq N\\leq 1000\\)가 있는 다른 RMs입니다. 우리는 BoN이 \\(N\\)를 늘릴 때 성과가 좋아지기 때문에 RL에 비해 보상 해킹을 제한한다는 것을 검증한다.\n' +
      '\n' +
      '그림 18: **RL 실험***. 그림 1(b)와 같으나 선호도 데이터 세트에 25%의 부패가 있다.\n' +
      '\n' +
      '깨끗한 선호 데이터셋으로 깨끗한 선호 데이터셋을 이용한 실험 c.3.2########.\n' +
      '\n' +
      '그림 19 | **Oracle 선호도 메트릭은 고정된 수의 훈련 단계(클레이브 설정)에서 RL 실험에 대한 것이다. 그림 19(a)는 \\(\\mathit{WARM}\\)\\(M=6\\) 대 정책 승률을 나타낸다. 다른 정책은 모두 동일한 수의 훈련 단계입니다. 그림 19(b)는 KL 정규화 강도를 제어하는 \\(\\pha\\)의 다른 값에 대한 훈련과 함께 단일 RM \\(\\phi_{1}\\)으로 훈련된 정책에 대해 \\(\\mathit{WARM}\\)\\(M=6\\)의 승률을 보여준다(\\mathit{WARM}\\)\\(M=6\\).\n' +
      '\n' +
      '그림 20 | ***은 \\(\\alpha=0.01\\)를 사용한 RL 실험에 대한 보상(클립 설정)을 조절한다.\n' +
      '\n' +
      '그림 19 | **Oracle 선호도 메트릭은 고정된 수의 훈련 단계(클레이브 설정)에서 RL 실험에 대한 것이다. 그림 19(a)는 \\(\\mathit{WARM}\\)\\(M=6\\) 대 정책 승률을 나타낸다. 다른 정책은 모두 동일한 수의 훈련 단계입니다. 그림 19(b)는 KL 정규화 강도를 제어하는 \\(\\pha\\)의 다른 값에 대한 훈련과 함께 단일 RM \\(\\phi_{1}\\)으로 훈련된 정책에 대해 \\(\\mathit{WARM}\\)\\(M=6\\)의 승률을 보여준다(\\mathit{WARM}\\)\\(M=6\\).\n' +
      '\n' +
      '### Distillation experiments\n' +
      '\n' +
      '그림 22에서 우리는 대조군 PaLM-XS RM이 라벨을 생성하여 PaLM-XXS RM을 훈련시키는 [17]의 증류 설정을 재현한다. 측면 메모로 증류이 미세 박리된 RM 전반에 걸쳐 다양성을 변화시켜 잠재적으로 증류 설정의 중요성을 변경하여 보다 현실적인 RLAIF 설정을 탐구하는 데 동기를 부여한다는 것을 관찰했다.\n' +
      '\n' +
      '그림 21: ** 제어 보상은 \\(\\alpha=0.001\\)를 사용한 RL 실험**(클런 설정)에 대한 것이다.\n' +
      '\n' +
      '그림 22: **BoN 실험은 [17]***의 증류 설정에서, 선호 데이터 세트의 라벨은 \\(y\\) 축성을 제공하는 동일한 RM인 대조군 RM에 의해 제공된다. 후보 요약들은 T5 아키텍처[111]와 SFT에 의해 생성된다. 블루 라인은 \\(M\\) 가중치가 있는 _WARM_를 나타내며, _WARM_는 개별 RM(노란색)보다 높거나(붉은색의 ENS) 예측을 앙상블할 때 더 높다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>