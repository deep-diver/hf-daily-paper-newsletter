<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MusicHiFi: Fast High-Fidelity Stereo Vocoding\n' +
      '\n' +
      ' Ge Zhu\\({}^{1,2*}\\), Juan-Pablo Caceres\\({}^{2}\\), Zhiyao Duan\\({}^{1}\\), and Nicholas J. Bryan\\({}^{2}\\)\n' +
      '\n' +
      '\\({}^{1}\\) University of Rochester \\({}^{2}\\) Adobe Research\n' +
      '\n' +
      'This work is done during an internship at Adobe Research, Correspondence to: Ge Zhu (gzhu6@ur.rochester.edu) and Nicholas J. Bryan (njb@ieee.org).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Diffusion-based audio and music generation models commonly generate music by constructing an image representation of audio (e.g., a mel-spectrogram) and then converting it to audio using a phase reconstruction model or vocoder. Typical vocoders, however, produce monophonic audio at lower resolutions (e.g., 16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an efficient high-fidelity stereophonic vocoder. Our method employs a cascade of three generative adversarial networks (GANs) that convert low-resolution mel-spectograms to audio, upsamples to high-resolution audio via bandwidth expansion, and upmixes to stereophonic audio. Compared to previous work, we propose 1) a unified GAN-based generator and discriminator architecture and training procedure for each stage of our cascade, 2) a new fast, near downsampling-compatible bandwidth extension module, and 3) a new fast downlink-compatible mono-to-stereo upmixer that ensures the preservation of monophonic content in the output. We evaluate our approach using both objective and subjective listening tests and find our approach yields comparable or better audio quality, better spatialization control, and significantly faster inference speed compared to past work. Sound examples are at [https://MusicHiFi.github.io/web/](https://MusicHiFi.github.io/web/).\n' +
      '\n' +
      ' music generation, mel-spectrogram inversion, bandwidth extension, mono-to-stereo upmixing\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'Recent generation methods offer an exciting new way to create media content. Diffusion models [1], in particular, have shown great promise for fast, high-quality image generation [2, 3] and are rapidly being adopted for audio and music generation [4, 5, 6, 7, 8, 9, 10, 11]. When used for audio [5, 6, 7, 8], diffusion models are commonly used to generate an image representation of audio (e.g. mel-spectrogram) and then a phase reconstruction model or vocoder is used to convert to audio. This two-stage cascaded approach has been shown beneficial [6], but typically generates low-resolution audio (e.g., mono 16-24 kHz) via a diffusion [12] or generative adversarial network (GAN) vocoder [13, 14, 15].\n' +
      '\n' +
      'To improve diffusion-based generation quality for audio and music, a high-fidelity vocoder is essential. Current methods to do so largely come in two forms: bandwidth extension (expansion) and mono-to-stereo upmixing. Bandwidth extension (BWE) is the process of increasing the frequency range or bandwidth of an audio signal and can be achieved using a source-filter model [16], time-domain [17, 18, 19], or spectral-domain [20, 21, 22, 23, 24] neural network. Mono-to-stereo (M2S) upmixing is the process of converting a single-channel audio signal into spatialized (left and right) channels. M2S can be achieved via decorrelation-based digital signal processing (DSP) or a recent, up to 256x slower than real-time, auto-regressive parametric stereo method [25].\n' +
      '\n' +
      'In this work, we propose MusicHiFi -- a new efficient high-fidelity stereophonic vocoder as shown in Fig. 1. Our method employs a GAN-triplet cascade that converts low-resolution single-channel mel-spectrograms (e.g., 22.05 kHz) to stereo high-resolution waveforms (e.g., stereo 44.1 kHz) leveraging vocoder, BWE, and M2S modules, as illustrated in Fig. 0(a). Our method can be seamlessly integrated into low-sampling rate mel-spectrograms music generators, used to enhance the fidelity of a low-resolution audio recording, and/or used to spatialize monophonic music. Compared to recent diffusion-based BWE methods [24] and AR spatialization methods, our GAN-based generators provide much faster GPU inference speed and are easily differentiable. We evaluate our approach via multiple objective and subjective listening tests and find our approach yields equal or better vocoding, BWE, and M2S quality with significantly faster inference speed. Our contributions include:\n' +
      '\n' +
      '* A unified GAN-based generator, discriminator, and training recipe for vocoding, BWE, and M2S upmixing,\n' +
      '* A new fast bandwidth extension method that maintains the low-frequency content in upsampled audio through skip connections, and\n' +
      '* A new fast mono-to-stereo method that uses a middle-side stereo encoding [26] that fully preserves monophonic content and offers superior spatialization width control.\n' +
      '\n' +
      'Fig. 1: (a) MusicHiFi inference diagram. (b) MusicHiFi unified GAN-training diagram. Blocks from top to bottom represent our vocoder, BWE and M2S. The blue dashed rectangles have shared architectures per cascade stage with different weights.\n' +
      '\n' +
      '## II Background\n' +
      '\n' +
      'The most relevant recent works to our proposed method include BigVGAN [15] and the Descript audio codec [27]. BigVGAN is a recently proposed vocoder method that has achieved state-of-the-art performance for generating high-fidelity waveforms from mel-spectrograms at speeds significantly faster than real time on a single GPU [15]. The BigVGAN generator employs a neural network architecture composed of a stack of transposed 1D convolutions, each followed by an anti-aliasing multi-periodicity composition (AMP) block which internally uses a Snake activation function [28]. Past work demonstrated AMP blocks are able to generate waveforms with fewer high-frequency artifacts and provide substantial improvements in both objective and subjective assessments. Furthermore, AMP blocks have been found to improve robustness for out-of-distribution vocoding and strong extrapolation ability.\n' +
      '\n' +
      'Related to BigVGAN is the Descript audio codec (DAC) neural compression method [27, 29, 30]. DAC uses the SoundStream generator architecture [29] with Snake activations together with an improved GAN-based discriminator architecture, an updated training objective, and a residual vector quantization scheme [29] to achieve state-of-the-art high-fidelity compression. When we look at the discriminator differences, both the BigVGAN and DAC use time-domain multi-period discriminators (MPD) to capture multiple periodic structures as well as spectral-domain discriminators, but DAC replaces the BigVGAN magnitude-only spectral discriminators with a multi-band multi-resolution complex spectrogram discriminators (MMSD) to enhance high-frequency prediction and mitigate aliasing [27]. Both BigVGAN and DAC also leverage a reconstruction loss and an adversarial loss, but DAC includes a codebook loss and updates the reconstruction loss to use multiple mel-bins over multi-scale spectrograms to improve training stability and convergence speed.\n' +
      '\n' +
      '## III Methodology\n' +
      '\n' +
      '### _Overview_\n' +
      '\n' +
      'We introduce MusicHiFi, a new vocoding method based on unified triplet-GAN cascade that progressively upsamples audio as shown in Fig. 1a. Our approach involves three stages, each being modular and useful for different applications. First, low-resolution single-channel mel-spectrograms are transformed into waveforms of the same resolution using a vocoder (MusciHiFi-V). Then, low-resolution waveforms are converted into high-resolution waveforms via our BWE module (MusciHiFi-BWE). Finally, single-channel high-resolution waveforms are upmixed to stereo audio through our M2S module (MusciHiFi-M2S). At each stage, we use an identical generator architecture, discriminator architecture, training objective, and model sizes as shown in Fig. 1b. The only difference between our three modules are the inputs and outputs and a residual connection for our BWE.\n' +
      '\n' +
      'In more detail, for all three of our generator stages, we adopt the BigVGAN transpose 1D convolution + AMP block generator architecture [15] that inputs a mel-spectrogram and outputs audio. For our discriminator architecture, we use the DAC discriminator [27]. For our training objective, we also adopt the DAC reconstruction loss and adversarial loss and remove the codebook learning objective since our focus is high-fidelity audio synthesis instead of neural compression. Our final training objective for our generator (\\(\\mathcal{L}_{G}\\)) and discriminator (\\(\\mathcal{L}_{D}\\)) are:\n' +
      '\n' +
      '\\[\\mathcal{L}_{G} =\\sum_{k=1}^{K}\\bigg{[}\\mathcal{L}_{adv}(G;D_{k})+\\lambda_{fm} \\mathcal{L}_{fm}(G;D_{k})\\bigg{]}+\\lambda_{rc}\\mathcal{L}_{rc}(G), \\tag{1}\\] \\[\\mathcal{L}_{D} =\\sum_{k=1}^{K}\\bigg{[}\\mathcal{L}_{adv}(D_{k};G)\\bigg{]},\\]\n' +
      '\n' +
      'where \\(D_{k}\\) denotes the \\(k\\)-th subdiscriminator from MPD or MMSD, \\(\\mathcal{L}_{adv},\\mathcal{L}_{rc},\\mathcal{L}_{fm}\\) represent least-square adversarial loss [31], reconstruction loss and \\(L_{1}\\)-based feature matching loss, respectively. Specifically, \\(\\mathcal{L}_{rc}=\\sum_{i}||\\log S_{i}-\\log\\hat{S}_{i}||_{1}\\), where \\(S_{i}\\) indicates the \\(i\\)-th mel-spectrogram from a list of mel-spectrograms with different fixed resolutions. \\(L_{fm}\\) aims to minimize the distances between real and generated intermediate features from the discriminator layers [13].\n' +
      '\n' +
      '### _MusciHiFi-V_\n' +
      '\n' +
      'Our vocoder inputs mel-spectrograms computed from low sampling-rate and outputs low-resolution audio waveforms, but otherwise follows our unified generator, discriminator, and training recipe described above. We note that the original BigVGAN training recipe exhibits instabilities and is susceptible to mode collapse when scaled up to larger models [15]. We employ fewer convolution layers in the AMP blocks, extend the input sequence length from 8192 to 16384, increase the convolution channel width of 2048, and use our training strategy, we do not observe mode collapse. Furthermore, this configuration approximately matches the floating point operations per second (FLOPS) of HiFi-GAN [14].\n' +
      '\n' +
      '### _MusciHiFi-BWE_\n' +
      '\n' +
      'Our BWE module inputs low-resolution audio and outputs high-resolution audio while using our unified generator, discriminator, and training recipe. For our generator architecture, however, we make two small, but significant changes. First, we take the low-resolution audio and compute an intermediate mel-spectrogram representation with half the hop size used for the vocoder to double the sequence length and match full-band waveform output.\n' +
      '\n' +
      'Second, we add a residual or skip connection between the input low-resolution audio signal and high-resolution audio output, with a sinc interpolation block in between that performs (2x) upsampling. The residual connection enables our BWE generator to more easily generate low-bandwidth content and allows our BWE generator to focus model capacity on generating high-frequency content. The discriminator also operates on the higher sample rate, full bandwidth audio [20, 21]. Note, during preliminary testing, we experimented not using the residual connection which did not work and applying a low-pass (LP) filter to the generated waveform, which slowed training and did not enhance performance.\n' +
      '\n' +
      '### _MusicHiFi-M2S_\n' +
      '\n' +
      'For our mono-to-stereo (M2S) upmixer, we follow our unified generator architecture, discriminator architecture, and training recipe for a third time. To create a stereo effect from a mono audio signal, however, we leverage a mid-side encoding [26] to convert stereo left and right signals into a summation channel (mid-channel) and a difference channel (side-channel) [26]. We then train our M2S module to input the mel-spectrogram of the mid-channel \\(M\\) and output the side channel waveform \\(S\\), where \\(M=\\frac{L+R}{2}\\) and \\(S=\\frac{L-R}{2}\\), \\(L\\) and \\(R\\) are the left and right stereo channel, respectively. Subsequently, we reconstruct left and right output channels via \\(L=M+S\\) and \\(R=M-S\\).\n' +
      '\n' +
      'As a result of using mid-side encodings, our method is downmix-compatible in that we can take a mono channel, upsample it to stereo, downsample back to mono, and recover the original mono channel perfectly. This is not the case with alternative methods which typically degrade results after repeated applications. Furthermore, we can also add a control mechanism to adjust the spatialization width by controlling the energy ratio between the side and mid-channels. We can do this by normalizing the mid-channel and side-channel energies to 0 decibels (dB) and then adjusting the mid/side energy ratio via \\(\\hat{S}\\leftarrow\\alpha S\\), where \\(\\alpha=10^{\\gamma/20}\\) and \\(\\gamma\\) is a scalar factor in decibels. When \\(\\gamma>0\\), there will be more side energy and when \\(\\gamma<0\\), there will be less side energy.\n' +
      '\n' +
      '## IV Experiment and Results\n' +
      '\n' +
      '### _Training details_\n' +
      '\n' +
      'We train all of our models using an internal dataset of 1800 hours of licensed instrumental music (stereo 44.1 kHz). For training, we randomly crop a sequence of 16,384 samples and then apply module-specific pre-processing. For our vocoder, we use channel averaging and downsampling to 22.05 kHz with STFT settings of a 1024-sample window, 256-sample hop size, and 128-band log-mel spectrogram. For our BWE module, we use the same pre-processing as the vocoder but halve the window and hop size. For our M2S module, we use channel averaging with identical STFT settings as the vocoder. We use the scalar weights \\(\\lambda_{fm}=1\\) and \\(\\lambda_{rc}=360\\) for the training objective in Eq. (1) and train all modules with a batch size of 45 for 500k steps, and select the optimal checkpoint via the minimum validation multi-resolution STFT distance. Model sizes per stage are approximately 46M params and 55 GFLOPS for one second of audio. All other parameters follow from the open-source implementations of the BigVGAN [15] generator and DAC [27] discriminator.\n' +
      '\n' +
      '### _Baselines_\n' +
      '\n' +
      'For vocoding, we compare against BigVGAN [15] and HiFi-GAN [14] all trained on the same data and input features. Our retrained HiFi-GAN model has 14M params and take 52 GFLOPS for one second of audio. We also train a large HiFi-GAN-large model with 1024 input channels with 55M params, while taking 211 GFLOPS for one second of audio. For BWE, we compare against Aero [23], a recent state-of-the-art BWE method that uses a encoder-decoder architecture with LSTM and temporal-based attention layers with 19M params and takes 85 GFLOPS. We also compare a pre-trained checkpoint of AudioSR [24] for BWE (no training code available). For M2S, we compare against a CPU-only open-source decorrelation method [32, 33], denoted as DSP, that divides the signal into transients, noise, and harmonics and decorrelates non-transient content.\n' +
      '\n' +
      '### _Objective evaluation_\n' +
      '\n' +
      'For objective evaluation, we use 673 clips from FMA-small [35] and the test split of the accompaniment track from the DSD100 test dataset [36]. For both test datasets, every segment has a duration of 30 seconds. For objective evaluation metrics, we use a suite of four metrics including ViSQOL [37], mel distance (Mel-D), multi-resolution STFT distance (STFT-D), and scale-invariant source-to-distortion ratio (SI-SDR) [38]. ViSQOL is a perceptual quality metric that estimates a mean opinion score based on the spectral similarity to the ground truth. The Mel-D and STFT-D measure the spectral distances between the reconstructed and ground-truth audio under a mel and a linear frequency scale, respectively. We also use the SI-SDR metric to measure vocoder quality, but not for BWE and M2S as we found the metric was less reliable for these more multi-modal tasks. We use the real time factor (RTF) metric or time processed over time elapsed on an A100 GPU to measure speed.\n' +
      '\n' +
      '### _Objective evaluation result_\n' +
      '\n' +
      'Vocoder objective evaluation results are shown in Table I. We find our proposed method outperforms BigVGAN and HiFi-GAN on Mel-D, STFT-D and ViSQOL results while maintaining a comparable SI-SDR score and lower RTF on both datasets. We also find the RTF of our vocoder method is lower compared to HiFi-GAN, but is still very fast and almost 2000x real-time on an A100.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{1}\\)** & **STFT-D\\({}_{1}\\)** & **ViSQOL\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{3}{*}{DSD100} & Aero [23] & **0.51** (0.05/1.16) & 0.12 (0.02/0.54) & **4.18** & 19 \\\\  & AudeSR\\({}^{-}\\)[24] & 1.23 (0.6/4.25) & 0.51 (0.36/1.68) & 3.54 & 4 \\\\  & MusicHFi-BW & 0.55 (0.08/1.18) & **0.11** (0.02/0.56) & 4.14 & **169 \\\\ \\hline \\multirow{3}{*}{FMA} & Aero [23] & **0.09** (0.07/1.60) & **0.24** (0.03/0.73) & **4.12** & 19 \\\\  & AudioSR [24] & 1.68 (0.68/1.8) & 0.68 (0.39/2.33) & 3.25 & 4 \\\\ \\cline{1-1}  & MusicHFi-BW & 1.01 (0.09/1.76) & 0.25 (0.02/0.79) & 4.08 & **1613** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: BWE objective evaluation for full-band audio. Low/high-band results are in parentheses. * AudioSR outputs have a high-frequency EQ boost that causes evaluation issues.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{1}\\)** & **STFT-D\\({}_{1}\\)** & **ViSQOL\\(\\uparrow\\)** & **Si-SDR\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{3}{*}{DSD100} & HiFi-GAN [14] & 1.09 & 0.65 & 4.47 & 28.62 & **3488** \\\\  & HiFi-GAN-large [14] & 1.06 & 0.60 & 4.48 & **30.29** & 3409 \\\\  & BiViGAN [15] & 0.94 & 0.41 & 4.651 & 27.35 & 1818 \\\\  & MusicHFi-V & **0.87** & **0.33** & **4.67** & 28.49 & 1786 \\\\ \\hline \\multirow{3}{*}{FMA} & HiFi-GAN [14] & 1.09 & 0.64 & 4.52 & **32.45** & **3703** \\\\  & HiFi-GAN-large [14] & 1.04 & 0.57 & 4.56 & 31.89 & 3614 \\\\ \\cline{1-1}  & BigVGAN [15] & 0.94 & 0.41 & 4.652 & 30.01 & 1829 \\\\ \\cline{1-1}  & MusicHFi-V & **0.87** & **0.35** & **4.67** & 31.57 & 1807 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Vocoder objective evaluation. Our vocoder module yields better results than baselines, but is mildly slower.\n' +
      '\n' +
      'BWE objective evaluation results are shown in Table II. We find both our proposed method and Aero yield comparable STFT-D, Mel-D, and ViSQOL results. When compared to AudioSR, we found that AudioSR is easily influenced by scale variations and also has a notable presence of high frequency components. In an effort to address this issue, we computed a scale adjustment factor by downsampling the generated waveform to 22.05 kHz normalizing the energy to the ground truth. Despite these adjustments, a significant gap remains in the objective metrics remains, likely from difference in training datasets [24]. We also find the RTF of our BWE module is approx. 80-400x faster than alternatives.\n' +
      '\n' +
      'M2S objective evaluation results are shown in Table III. We find our method outperforms the DSP decorrelation method on STFT-D, Mel-D, and ViSQOL. Furthermore, it is important to note that the error of the mid channel is zero for our method since our M2S method is downmix compatible and maintains the original mid channel and only estimates the side channel. We also find the RTF of our BWE module is over 300x faster than the DSP method by way of efficient GPU compute.\n' +
      '\n' +
      '### _Subjective evaluation_\n' +
      '\n' +
      'We performed two subjective listening tests to evaluate our BWE and M2S independently [39]. For our BWE and M2S test, we recruited 20 and 23 participants with diverse audio backgrounds, respectively, and used a multiple stimuli with hidden reference and anchor (MUSHRA) protocol and the Web Audio Evaluation Tool [40]. The goal of our BWE tasks was to have participants rate _quality similarity_ relative to ground-truth 44.1 kHz mono music. The goal of our M2S task was to rate _quality similarity_ relative to ground-truth stereo as well as test spatial controllability by varying the target mid-side energy ratio (i.e., from 0 to -18dB) given that performance varies heavily on the spatialization level.\n' +
      '\n' +
      'For our BWE listening test, we created six test examples from the FMA-small dataset, with each sample being 4 seconds long. Test conditions included (a) AudioSR, (b) cascaded MusicHiFi-V and Aero and (c) MusicHiFi-V+BWE as well as the LA and HA. The goal of this task is to understand the perceptual quality of comparable BWE algorithms. For the M2S evaluation, we prepared twelve listening samples, each 3 seconds long, selected from an internal test dataset instead of the FMA dataset as we found a large number of FMA clips were poorly spatialized. Test conditions included (d) cascaded MusicHiFi-V+BWE with DSP and (e) the full MusicHiFi for M2S comparison as well as the LA and HA. All samples for the two tasks are loudness normalized to -23 dBFS before and after being fed into the cascaded methods. For both tests, 22.05 kHz mono signals were used as the low-anchor (LA) and a hidden reference as high-anchor (HA).\n' +
      '\n' +
      '### _Subjective evaluation results_\n' +
      '\n' +
      'The results of our listening tests are shown in Fig. 2. When we compare the BWE subjective evaluation results, samples from (a) AudioSR ranks the least compared to other baselines. This result matches our earlier qualitative analysis that AudioSR has a strong high-frequency boost. We also find that (b) MusicHiFi-V + Aero ranks slightly above our BWE method, but we believe this is reasonable considering Aero uses an U-Net architecture with internal BiLSTM layers versus our convolutional architecture that is dramatically faster. We further conducted multiple post-hoc paired t-tests with Bonferroni correction [41] for each condition vs. our method. We find there is no statistical significance between our method and Aero, while our method and Aero rank above AudioSR.\n' +
      '\n' +
      'For M2S evaluation, we find that our (e) MusicHiFi performs best under different M/S panning coefficients and samples produced from the method (d) MusicHiFi-V+BWE with DSP perform similarly to ours when the energy ratio between mid and side channel is the same (0dB). The difference between our approach and the DSP baseline is statistically significant for side/mid rations 6, 12, and 18 via multiple post-hoc paired t-tests with Bonferroni correction [41].\n' +
      '\n' +
      'For further evaluation, please find sound examples using mel-spectrograms extracted from real music and generated via a diffusion model [11] at [https://MusciHiFi.github.io/web/](https://MusciHiFi.github.io/web/).\n' +
      '\n' +
      '## V Conclusion\n' +
      '\n' +
      'We proposed a new efficient, high-fidelity stereophonic vocoding method named MusicHiFi. Our method works via a cascade of three GAN models that convert mel-spectrograms to low-quality audio waveforms, upsamples the low-resolution audio to high-resolution audio via bandwidth extension, and finally renders stereophonic high-resolution audio. Our method can be integrated into mel-spectrogram based music generators, used to enhance the fidelity of a low-resolution audio, and/or used to spatialize monophonic music. Compared to past work, we contribute a unified GAN-based discriminator and generator design, a new downsampling compatible BWE module, and a novel mono-preserving mono-to-stereo module. We evaluated our method using both objective evaluation and two subjective listening tests and found our method yields comparable or better vocoding and BWE results while outperforming comparable M2S methods, has better spatialization width control, and is extraordinarily efficient.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{i}\\)** & **STFT-D\\({}_{i}\\)** & **VSOOL\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{2}{*}{DSD100-test} & DSP [34] & 1.07/1.87 & 1.09/1.70 & 4.69 & 5 (CPU) \\\\  & MusicHiFi-M2S & **0.001/1.70** & **0.00/1.53** & **4.73** & **1539** (GPU) \\\\ \\hline \\multirow{2}{*}{FMA-small} & DSP [34] & 0.99/2.29 & 1.08/2.16 & 4.70 & 4 (CPU) \\\\  & MusicHiFi-M2S & **0.00/2.03** & **0.00/1.88** & **4.73** & **1554** (GPU) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: Comparison of objective metrics for M2S system. Values in Mel-D and STFT-D include mid/side channels.\n' +
      '\n' +
      'Fig. 2: Subjective listening test violins plots. BWE test (left) and M2S test (right). Test conditions include (a) AudioSR, (b) cascaded MusicHiFi-V and Aero and (c) MusicHiFi-V+BWE (d) MusicHiFi-V+BWE with DSP (e) full MusicHiFi.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>