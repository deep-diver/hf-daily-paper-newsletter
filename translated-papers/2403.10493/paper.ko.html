<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MusicHiFi: Fast High Fidelity Stereo Vocoding\n' +
      '\n' +
      ' Ge Zhu\\({}^{1,2*}\\), Juan-Pablo Caceres\\({}^{2}\\), Zhiyao Duan\\({}^{1}\\), Nicholas J. Bryan\\({}^{2}\\)\n' +
      '\n' +
      'Rochester \\({}^{1}\\({}^{1}\\) University of Rochester \\({}^{2}\\) Adobe Research\n' +
      '\n' +
      '이 작업은 Ge Zhu(gzhu6@ur.rochester.edu)와 Nicholas J. Bryan(njb@ieee.org)의 Adobe Research에서 인턴을 하는 동안 수행됩니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 기반 오디오 및 음악 생성 모델은 일반적으로 오디오의 이미지 표현(예를 들어, 멜-스펙트로그램)을 구성한 다음 위상 재구성 모델 또는 보코더를 사용하여 오디오로 변환하여 음악을 생성한다. 본 논문에서는 3개의 생성적 적대 네트워크(GAN: Generative Adversarial Network)를 이용하여 낮은 해상도(예: 16-24 kHz)에서 단성 오디오를 생성하는 방법을 제안한다. 제안하는 방법은 3개의 생성적 적대 네트워크(GAN: Generative Adversarial Network)를 이용하여 저 해상도 멜-스펙토그램을 오디오로 변환하고, 대역폭 확장을 통해 고 해상도 오디오로 업샘플링하고, 입체 오디오로 업믹스하는 방법을 사용한다. 기존의 방법과 비교하여, 1) 단일화된 GAN 기반의 생성기 및 판별기 구조 및 훈련 절차, 2) 새로운 고속 다운샘플링 호환 대역폭 확장 모듈, 3) 출력에서 단성 콘텐츠의 보존을 보장하는 새로운 고속 다운링크 호환 모노-투-스테레오 업믹서를 제안한다. 객관적 청취 테스트와 주관적 청취 테스트를 모두 사용하여 우리의 접근 방식을 평가하고 우리의 접근 방식이 과거 작업에 비해 비슷하거나 더 나은 오디오 품질, 더 나은 공간화 제어 및 훨씬 더 빠른 추론 속도를 제공한다는 것을 발견한다. 사운드 예는 [https://MusicHiFi.github.io/web/](https://MusicHiFi.github.io/web/)이다.\n' +
      '\n' +
      ' 음악 생성, 멜-스펙트로그램 반전, 대역폭 확장, 모노-투-스테레오 업믹스\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '최근의 생성 방법은 미디어 콘텐츠를 만드는 흥미로운 새로운 방법을 제공한다. 확산 모델[1]은 특히 빠르고 고품질의 이미지 생성[2, 3]에 대한 큰 가능성을 보여주었고 오디오 및 음악 생성[4, 5, 6, 7, 8, 9, 10, 11]에 빠르게 채택되고 있다. 오디오[5, 6, 7, 8]에 사용될 때, 확산 모델들은 일반적으로 오디오의 이미지 표현(예를 들어, 멜-스펙트로그램)을 생성하기 위해 사용되고, 이어서 위상 재구성 모델 또는 보코더가 오디오로 변환하기 위해 사용된다. 이러한 2-스테이지 캐스케이드 접근법은 유익한[6]으로 나타났지만, 전형적으로 확산[12] 또는 생성적 적대 네트워크(GAN) 보코더[13, 14, 15]를 통해 저해상도 오디오(예를 들어, 모노 16-24 kHz)를 생성한다.\n' +
      '\n' +
      '오디오와 음악에 대한 확산 기반 생성 품질을 향상시키기 위해서는 고충실도 보코더가 필수적이다. 이를 위한 현재의 방법은 크게 대역폭 확장(확장)과 모노-투-스테레오 업믹싱의 두 가지 형태로 나타난다. 대역폭 확장(BWE)은 오디오 신호의 주파수 범위 또는 대역폭을 증가시키는 프로세스이며, 소스-필터 모델[16], 시간-도메인[17, 18, 19], 또는 스펙트럼-도메인[20, 21, 22, 23, 24] 신경망을 사용하여 달성될 수 있다. 모노-투-스테레오(mono-to-stereo, M2S) 업믹싱은 단일 채널 오디오 신호를 공간화된(좌우) 채널로 변환하는 과정이다. M2S는 비상관 기반 디지털 신호 처리(DSP) 또는 실시간 자동 회귀 파라메트릭 스테레오 방법[25]보다 최대 256배 느린 최근을 통해 달성될 수 있다.\n' +
      '\n' +
      '이 작업에서 우리는 그림 1과 같이 새로운 효율적인 고충실도 스테레오포닉 보코더인 MusicHiFi를 제안한다. 우리의 방법은 그림 1과 같이 보코더, BWE 및 M2S 모듈을 사용하여 저해상도 단일 채널 멜 스펙트럼그램(예: 22.05kHz)을 스테레오 고해상도 파형(예: 스테레오 44.1kHz)으로 변환하는 GAN-트리플렛 캐스케이드를 사용한다. 0(a). 본 논문에서 제안하는 방법은 저-샘플링 레이트 멜-스펙트로그램 음악 생성기에 매끄럽게 통합될 수 있고, 저-해상도 오디오 레코딩의 충실도를 향상시키는 데 사용되며, 그리고/또는 단조 음악을 공간화하는 데 사용될 수 있다. 최근 확산 기반 BWE 방법[24] 및 AR 공간화 방법과 비교하여, 우리의 GAN 기반 생성기는 훨씬 더 빠른 GPU 추론 속도를 제공하고 쉽게 구별할 수 있다. 다중 객관적이고 주관적인 청취 테스트를 통해 우리의 접근 방식을 평가하고 훨씬 더 빠른 추론 속도로 동등하거나 더 나은 보코딩, BWE 및 M2S 품질을 찾는다. 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 보코딩, BWE 및 M2S 업믹싱을 위한 통일된 GAN 기반 생성기, 판별기 및 훈련 레시피,\n' +
      '* 스킵 연결을 통해 업샘플링된 오디오에서 저주파 컨텐츠를 유지하는 새로운 고속 대역폭 확장 방법, 및\n' +
      '* 단조로운 콘텐츠를 완전히 보존하고 우수한 공간화 폭 제어를 제공하는 중간측 스테레오 인코딩[26]을 사용하는 새로운 고속 모노-투-스테레오 방법.\n' +
      '\n' +
      '도. 1: (a) MusicHiFi 추론도. (b) MusicHiFi unified GAN-training diagram. 위에서 아래로 블록은 보코더인 BWE와 M2S를 나타냅니다. 파란색 파선 직사각형은 서로 다른 무게를 가진 캐스케이드 스테이지당 공유 아키텍처를 가지고 있습니다.\n' +
      '\n' +
      '## II Background\n' +
      '\n' +
      '제안된 방법과 가장 관련성이 높은 최근 연구로는 BigVGAN[15]과 Descript 오디오 코덱[27]이 있다. BigVGAN은 최근 제안된 보코더 방법으로 단일 GPU에서 실시간보다 훨씬 빠른 속도로 멜-스펙트로그램으로부터 고충실도 파형을 생성하기 위해 최첨단 성능을 달성했다[15]. BigVGAN 생성기는 전위된 1D 컨볼루션의 스택으로 구성된 신경망 아키텍처를 사용하며, 각각은 내부적으로 스네이크 활성화 함수를 사용하는 안티앨리어싱 다중 주기 구성(AMP) 블록이 뒤따른다[28]. 과거 연구에서 입증된 AMP 블록은 더 적은 고주파 인공물로 파형을 생성할 수 있으며 객관적인 평가와 주관적인 평가 모두에서 상당한 개선을 제공한다. 또한 AMP 블록은 외부 배포 보코딩에 대한 견고성과 강력한 외삽 능력을 향상시키는 것으로 확인되었다.\n' +
      '\n' +
      'BigVGAN과 관련된 것은 DAC(Descript audio codec) 신경 압축 방법[27, 29, 30]이다. DAC는 최신 고충실도 압축을 달성하기 위해 개선된 GAN 기반 판별기 아키텍처, 업데이트된 훈련 목표 및 잔차 벡터 양자화 방식[29]과 함께 스네이크 활성화와 함께 사운드스트림 생성기 아키텍처[29]를 사용한다. 판별기 차이를 살펴보면, BigVGAN과 DAC는 모두 시간 영역 다중 주기 판별기(MPD)를 사용하여 스펙트럼 영역 판별기뿐만 아니라 다중 주기 구조를 포착하지만 DAC는 BigVGAN 크기 전용 스펙트럼 판별기를 다중 대역 다중 해상도 복합 스펙트로그램 판별기(MMSD)로 대체하여 고주파 예측을 강화하고 앨리어싱을 완화한다[27]. 빅VGAN과 DAC는 모두 재구성 손실과 적대적 손실을 활용하지만 DAC는 코드북 손실을 포함하고 재구성 손실을 업데이트하여 훈련 안정성과 수렴 속도를 개선하기 위해 다중 스케일 스펙트로그램에 걸쳐 다중 멜빈을 사용한다.\n' +
      '\n' +
      '## III Methodology\n' +
      '\n' +
      '### _Overview_\n' +
      '\n' +
      '그림 1과 같이 오디오를 점진적으로 업샘플링하는 통합 트리플렛-GAN 캐스케이드를 기반으로 하는 새로운 보코딩 방법인 MusicHiFi를 소개한다. 1a. 우리의 접근법은 각각 모듈식이고 다른 응용 프로그램에 유용한 세 단계를 포함한다. 먼저, 보코더(MusciHiFi-V)를 이용하여 저해상도 단일 채널 멜-스펙트로그램을 동일한 해상도의 파형으로 변환한다. 그런 다음 BWE 모듈(MusciHiFi-BWE)을 통해 저해상도 파형을 고해상도 파형으로 변환한다. 마지막으로, 단일 채널 고해상도 파형은 M2S 모듈(MusciHiFi-M2S)을 통해 스테레오 오디오에 업믹스된다. 각 단계에서 그림 1과 같이 동일한 생성기 아키텍처, 판별기 아키텍처, 훈련 목표 및 모델 크기를 사용한다. 1b. 세 모듈 간의 유일한 차이점은 입력 및 출력과 BWE에 대한 잔여 연결이다.\n' +
      '\n' +
      '보다 상세하게, 세 개의 생성기 스테이지 모두에 대해, 우리는 멜-스펙트로그램을 입력하고 오디오를 출력하는 빅VGAN 전치 1D 컨볼루션 + AMP 블록 생성기 아키텍처[15]를 채택한다. 우리의 판별기 아키텍처를 위해, 우리는 DAC 판별기를 사용한다[27]. 또한 훈련 목적을 위해 DAC 재구성 손실과 적대적 손실을 채택하고 신경 압축 대신 고충실도 오디오 합성에 초점을 맞추기 때문에 코드북 학습 목표를 제거한다. 생성기(\\(\\mathcal{L}_{G}\\)) 및 판별기(\\(\\mathcal{L}_{D}\\))에 대한 우리의 최종 훈련 목표는 다음과 같다.\n' +
      '\n' +
      '\\mathcal{L}_{G} =\\sum_{k=1}^{K}\\bigg{[}\\mathcal{L}_{adv}(G;D_{k})+\\lambda_{fm}\\mathcal{L}_{fm}(G;D_{k})\\bigg{}+\\lambda_{rc}\\mathcal{L}_{rc}(G), \\tag{1}\\]\\[\\mathcal{L}_{D} =\\sum_{k=1}^{K}\\bigg{[}\\mathcal{L}_{adv}(D_{k};G}\\bigg{,\\]\n' +
      '\n' +
      '여기서, \\(D_{k}\\)은 MPD 또는 MMSD로부터 \\(k\\)번째 하위 구분자를 나타내고, \\(\\mathcal{L}_{adv},\\mathcal{L}_{rc},\\mathcal{L}_{fm}\\)은 각각 최소 제곱 적대적 손실[31], 재구성 손실 및 \\(L_{1}\\) 기반 특징 매칭 손실을 나타낸다. 구체적으로 \\(\\mathcal{L}_{rc}=\\sum_{i}||\\log S_{i}-\\log\\hat{S}_{i}||_{1}\\), 여기서 \\(S_{i}\\)는 서로 다른 고정 해상도를 갖는 멜-스펙트로그램들의 리스트로부터 \\(i\\)번째 멜-스펙트로그램을 나타낸다. \\(i\\)번째 멜-스펙트로그램 (L_{fm}\\)는 판별기 레이어로부터 실제 피쳐와 생성된 중간 피쳐 사이의 거리를 최소화하는 것을 목표로 한다[13].\n' +
      '\n' +
      '### _MusciHiFi-V_\n' +
      '\n' +
      '보코더는 낮은 샘플링 속도에서 계산된 멜-스펙트로그램을 입력하고 저해상도 오디오 파형을 출력하지만 그렇지 않으면 위에서 설명한 통합 생성기, 판별기 및 훈련 레시피를 따른다. 우리는 원래의 BigVGAN 트레이닝 레시피가 불안정성을 나타내고 더 큰 모델들로 스케일링될 때 모드 붕괴에 취약하다는 것에 주목한다[15]. 우리는 AMP 블록에서 더 적은 컨볼루션 레이어를 사용하고, 입력 시퀀스 길이를 8192에서 16384로 확장하고, 컨볼루션 채널 폭을 2048로 늘리고, 훈련 전략을 사용하여 모드 붕괴를 관찰하지 않는다. 나아가, 이러한 구성은 HiFi-GAN[14]의 FLOPS(floating point operations per second)와 대략 일치한다.\n' +
      '\n' +
      '### _MusciHiFi-BWE_\n' +
      '\n' +
      '우리의 BWE 모듈은 우리의 통합된 생성기, 판별기 및 훈련 레시피를 사용하는 동안 저해상도 오디오를 입력하고 고해상도 오디오를 출력한다. 그러나 발전기 아키텍처의 경우 두 가지 작지만 중요한 변화를 만듭니다. 먼저, 저 해상도 오디오를 사용하여 보코더에 사용되는 홉 크기의 절반으로 중간 멜-스펙트로그램 표현을 계산하여 시퀀스 길이를 두 배로 늘리고 전체 대역 파형 출력을 일치시킨다.\n' +
      '\n' +
      '둘째, (2x) 업샘플링을 수행하는 sinc 보간 블록을 사이에 두고, 입력 저해상도 오디오 신호와 고해상도 오디오 출력 사이의 잔차 또는 스킵 연결을 추가한다. 잔차 연결은 BWE 생성기가 낮은 대역폭 콘텐츠를 보다 쉽게 생성할 수 있게 하고, BWE 생성기가 모델 용량을 고주파 콘텐츠 생성에 집중할 수 있게 한다. 판별기는 또한 더 높은 샘플 레이트, 전체 대역폭 오디오 [20, 21]에서 동작한다. 예비 테스트 동안 작동하지 않는 잔류 연결을 사용하지 않고 생성된 파형에 저역 통과(LP) 필터를 적용하여 훈련을 늦추고 성능을 향상시키지 않는 실험을 했다.\n' +
      '\n' +
      '### _MusicHiFi-M2S_\n' +
      '\n' +
      '모노-투-스테레오(M2S) 업믹서의 경우, 통합 발전기 아키텍처, 판별기 아키텍처 및 훈련 레시피를 세 번째로 따릅니다. 그러나 모노 오디오 신호로부터 스테레오 효과를 생성하기 위해, 스테레오 좌우 신호를 합산 채널(중간 채널)과 차분 채널(측면 채널)로 변환하기 위해 중간 측 인코딩[26]을 활용한다. 그런 다음 M2S 모듈을 학습하여 중간 채널 \\(M\\)의 멜 스펙트로그램을 입력하고 측면 채널 파형 \\(S\\)을 출력하는데, 여기서 \\(M=\\frac{L+R}{2}\\) 및 \\(S=\\frac{L-R}{2}\\), \\(L\\) 및 \\(R\\)은 각각 왼쪽 및 오른쪽 스테레오 채널이다. 이어서, \\(L=M+S\\)과 \\(R=M-S\\)을 통해 좌우 출력 채널을 재구성한다.\n' +
      '\n' +
      'Mid-side 인코딩을 사용한 결과, 단일 채널을 취하여 스테레오로 업샘플링하고, 다시 모노로 다운샘플링하고, 원래의 모노 채널을 완벽하게 복구할 수 있다는 점에서 다운믹스 호환성이 있다. 반복적인 적용 후 일반적으로 결과를 저하시키는 대체 방법의 경우는 그렇지 않다. 또한, 측부와 중간 채널 사이의 에너지 비를 조절함으로써 공간화 폭을 조절할 수 있는 제어 메커니즘을 추가할 수 있다. 중채널과 부채널 에너지를 0데시벨(dB)로 정규화한 후, \\(\\hat{S}\\leftarrow\\alpha S\\)을 통해 중간/측면 에너지 비를 조절함으로써, \\(\\alpha=10^{\\gamma/20}\\)과 \\(\\gamma\\)이 데시벨의 스칼라 인자임을 알 수 있다. 0\\(\\gamma>0\\)일 때 측면 에너지가 더 많고, 0\\(\\gamma<0\\)일 때 측면 에너지가 더 적을 것이다.\n' +
      '\n' +
      '## IV 실험 및 결과\n' +
      '\n' +
      '### _Training details_\n' +
      '\n' +
      '우리는 1800시간의 허가된 기악 음악(스테레오 44.1kHz)의 내부 데이터 세트를 사용하여 모든 모델을 훈련한다. 훈련을 위해 16,384개의 샘플 시퀀스를 무작위로 크롭한 다음 모듈별 전처리를 적용한다. 보코더는 1024-샘플 윈도우, 256-샘플 홉 크기 및 128-대역 로그-멜 스펙트로그램의 STFT 설정으로 채널 평균화 및 22.05 kHz로 다운샘플링을 사용한다. BWE 모듈의 경우 보코더와 동일한 전처리를 사용하지만 윈도우와 홉 크기를 절반으로 줄였다. M2S 모듈은 보코더와 동일한 STFT 설정으로 채널 평균을 사용한다. 학습목표는 스칼라 가중치 \\(\\lambda_{fm}=1\\)와 \\(\\lambda_{rc}=360\\)을 사용한다. (1) 및 500k 단계 동안 배치 크기가 45인 모든 모듈을 트레이닝하고, 최소 검증 다중 해상도 STFT 거리를 통해 최적의 체크포인트를 선택한다. 스테이지당 모델 크기는 오디오 1초 동안 약 46M 파라미터와 55 GFLOPS이다. 다른 모든 파라미터는 BigVGAN[15] 생성기 및 DAC[27] 판별기의 오픈 소스 구현으로부터 따른다.\n' +
      '\n' +
      '### _Baselines_\n' +
      '\n' +
      '보코딩을 위해 빅VGAN[15] 및 HiFi-GAN[14]과 동일한 데이터 및 입력 특징에 대해 훈련된 모든 것을 비교한다. 재교육된 HiFi-GAN 모델에는 14M 매개변수가 있으며 오디오 1초 동안 52GFLOPS를 사용합니다. 또한, 오디오 1초 동안 211 GFLOPS를 취하면서, 55M 파라미터로 1024개의 입력 채널을 갖는 대형 HiFi-GAN-대형 모델을 트레이닝한다. BWE의 경우, LSTM을 사용하는 인코더-디코더 구조와 19M 파라메터를 사용하는 시간 기반 주의 계층을 사용하고 85개의 GFLOPS를 사용하는 최신 최신 BWE 방법인 Aero[23]과 비교한다. 또한 BWE(사용 가능한 훈련 코드가 없음)에 대해 AudioSR[24]의 사전 훈련된 체크포인트를 비교한다. M2S의 경우, 신호를 과도, 잡음 및 고조파로 나누고 비 과도 컨텐츠를 역상관하는 DSP로 표시된 CPU 전용 오픈 소스 역상관 방법[32, 33]과 비교한다.\n' +
      '\n' +
      '### _Objective evaluation_\n' +
      '\n' +
      '객관적인 평가를 위해 FMA-small [35]의 673개의 클립과 DSD100 테스트 데이터 세트 [36]의 반주 트랙의 테스트 분할을 사용한다. 두 테스트 데이터 세트의 경우 모든 세그먼트의 지속 시간은 30초입니다. 객관적인 평가 메트릭을 위해 ViSQOL[37], mel distance(Mel-D), multi-resolution STFT distance(STFT-D), scale-invariant source-to-distortion ratio(SI-SDR) 등 4가지 메트릭 집합을 사용한다[38]. ViSQOL은 지상 진실과 스펙트럼 유사성을 기반으로 평균 의견 점수를 추정하는 지각 품질 메트릭이다. 멜-D와 STFT-D는 각각 멜과 선형 주파수 스케일 하에서 재구성된 오디오와 지상-진실 오디오 사이의 스펙트럼 거리를 측정한다. 또한 SI-SDR 메트릭을 사용하여 보코더 품질을 측정하지만 BWE 및 M2S에서는 이러한 더 많은 멀티모달 작업에 대해 메트릭이 덜 신뢰할 수 있음을 발견했기 때문에 사용하지 않는다. 우리는 속도를 측정하기 위해 A100 GPU에서 시간 경과에 따라 처리되는 실시간 인자(RTF) 메트릭 또는 시간을 사용한다.\n' +
      '\n' +
      '###_객관적 평가 결과_\n' +
      '\n' +
      'Vocoder 객관적 평가 결과는 표 I과 같다. 제안하는 방법은 Mel-D, STFT-D 및 ViSQOL 결과에서 BigVGAN 및 HiFi-GAN보다 우수한 성능을 보여주며, 두 데이터 세트에서 비슷한 SI-SDR 점수와 낮은 RTF를 유지한다. 또한 보코더 방법의 RTF는 HiFi-GAN에 비해 낮지만 A100에서는 여전히 매우 빠르고 거의 2000배 실시간이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{1}\\)** & **STFT-D\\({}_{1}\\)** & **ViSQOL\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{3}{*}{DSD100} & Aero [23] & **0.51** (0.05/1.16) & 0.12 (0.02/0.54) & **4.18** & 19 \\\\  & AudeSR\\({}^{-}\\)[24] & 1.23 (0.6/4.25) & 0.51 (0.36/1.68) & 3.54 & 4 \\\\  & MusicHFi-BW & 0.55 (0.08/1.18) & **0.11** (0.02/0.56) & 4.14 & **169 \\\\ \\hline \\multirow{3}{*}{FMA} & Aero [23] & **0.09** (0.07/1.60) & **0.24** (0.03/0.73) & **4.12** & 19 \\\\  & AudioSR [24] & 1.68 (0.68/1.8) & 0.68 (0.39/2.33) & 3.25 & 4 \\\\ \\cline{1-1}  & MusicHFi-BW & 1.01 (0.09/1.76) & 0.25 (0.02/0.79) & 4.08 & **1613** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: BWE objective evaluation for full-band audio. Low/high-band results are in parentheses. * AudioSR outputs have a high-frequency EQ boost that causes evaluation issues.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{1}\\)** & **STFT-D\\({}_{1}\\)** & **ViSQOL\\(\\uparrow\\)** & **Si-SDR\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{3}{*}{DSD100} & HiFi-GAN [14] & 1.09 & 0.65 & 4.47 & 28.62 & **3488** \\\\  & HiFi-GAN-large [14] & 1.06 & 0.60 & 4.48 & **30.29** & 3409 \\\\  & BiViGAN [15] & 0.94 & 0.41 & 4.651 & 27.35 & 1818 \\\\  & MusicHFi-V & **0.87** & **0.33** & **4.67** & 28.49 & 1786 \\\\ \\hline \\multirow{3}{*}{FMA} & HiFi-GAN [14] & 1.09 & 0.64 & 4.52 & **32.45** & **3703** \\\\  & HiFi-GAN-large [14] & 1.04 & 0.57 & 4.56 & 31.89 & 3614 \\\\ \\cline{1-1}  & BigVGAN [15] & 0.94 & 0.41 & 4.652 & 30.01 & 1829 \\\\ \\cline{1-1}  & MusicHFi-V & **0.87** & **0.35** & **4.67** & 31.57 & 1807 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Vocoder objective evaluation. Our vocoder module yields better results than baselines, but is mildly slower.\n' +
      '\n' +
      'BWE 객관적 평가 결과는 표 II와 같다. 제안된 방법과 에어로 수율 모두 유사한 STFT-D, Mel-D 및 ViSQOL 결과를 발견했다. 오디오SR과 비교할 때, 오디오SR은 스케일 변화에 쉽게 영향을 받고 또한 고주파 성분이 눈에 띄게 존재한다는 것을 발견했다. 이 문제를 해결하기 위해 생성된 파형을 22.05 kHz로 다운샘플링하여 접지 진실에 에너지를 정규화하여 스케일 조정 계수를 계산했다. 이러한 조정에도 불구하고, 객관적인 메트릭에는 상당한 격차가 남아 있으며, 이는 훈련 데이터 세트의 차이로부터 발생할 수 있다[24]. 또한 BWE 모듈의 RTF가 약하다는 것을 발견했습니다. 대안에 비해 80~400배 더 빠릅니다.\n' +
      '\n' +
      'M2S 객관적 평가 결과를 표 Ⅲ에 나타내었다. 본 논문에서 제안한 방법은 STFT-D, Mel-D, ViSQOL에서 DSP 상관법보다 우수한 성능을 보였다. 또한, M2S 방법은 다운믹스 호환성이 있고 원래 미드 채널을 유지하고 사이드 채널만 추정하기 때문에 미드 채널의 오류는 0이라는 점에 유의하는 것이 중요하다. 또한 효율적인 GPU 계산을 통해 BWE 모듈의 RTF가 DSP 방식보다 300배 이상 빠르다는 것을 알 수 있다.\n' +
      '\n' +
      '### _Subjective evaluation_\n' +
      '\n' +
      '우리는 BWE와 M2S를 독립적으로 평가하기 위해 두 가지 주관적인 듣기 테스트를 수행했다[39]. BWE와 M2S 테스트를 위해 다양한 오디오 배경을 가진 20명과 23명의 참가자를 모집했으며, 은닉 참조 및 앵커(MUSHRA) 프로토콜과 웹 오디오 평가 도구[40]를 사용하여 다중 자극을 사용했다. 우리의 BWE 작업의 목표는 그라운드-트루스 44.1kHz 모노 음악에 대한 참가자 비율 _품질 유사성_를 갖는 것이었다. M2S 작업의 목표는 공간화 수준에 따라 성능이 크게 다르다는 점을 감안할 때 목표 중간측 에너지 비율(즉, 0에서 -18dB)을 변경하여 지상-진실 스테레오에 대한 _quality similarity_를 평가하고 공간 제어성을 테스트하는 것이었다.\n' +
      '\n' +
      'BWE 청취 테스트를 위해 각 샘플의 길이가 4초인 FMA-작은 데이터 세트에서 6개의 테스트 예를 생성했다. 테스트 조건에는 (a) AudioSR, (b) cascaded MusicHiFi-V 및 Aero 및 (c) MusicHiFi-V+BWE와 LA 및 HA가 포함되었다. 이 작업의 목표는 비교 가능한 BWE 알고리즘의 지각 품질을 이해하는 것이다. M2S 평가를 위해 FMA 데이터 세트 대신 내부 테스트 데이터 세트에서 선택된 3초 길이의 12개의 청취 샘플을 준비했으며 많은 수의 FMA 클립이 제대로 공간화되지 않았음을 발견했다. 테스트 조건에는 (d) DSP와 함께 캐스케이드 뮤직HiFi-V+BWE 및 (e) LA 및 HA뿐만 아니라 M2S 비교를 위한 전체 뮤직HiFi가 포함되었다. 두 작업에 대한 모든 샘플은 캐스케이드 방법에 공급되기 전과 후에 -23 dBFS로 정규화된 라우드니스이다. 두 테스트 모두 22.05kHz 모노 신호를 저앵커(LA)로 사용하고 히든 레퍼런스를 고앵커(HA)로 사용했다.\n' +
      '\n' +
      '###_주관적 평가결과_주관적 평가결과\n' +
      '\n' +
      '청취 테스트의 결과는 그림 2에 나와 있다. BWE 주관적 평가 결과를 비교할 때, (a) AudioSR의 샘플은 다른 기준선에 비해 순위가 가장 낮다. 이 결과는 AudioSR이 강력한 고주파 부스트를 가지고 있다는 이전의 정성적 분석과 일치한다. 우리는 또한 (b) MusicHiFi-V + Aero가 우리의 BWE 방법보다 약간 높은 순위를 차지한다는 것을 발견했지만, 우리는 Aero가 내부 BiLSTM 계층이 있는 U-Net 아키텍처 대 극적으로 더 빠른 컨볼루션 아키텍처를 사용하는 것을 고려할 때 이것이 합리적이라고 믿는다. 우리는 각 조건 대 각 조건에 대해 본페로니 수정[41]을 사용하여 여러 사후 쌍체 t-검정을 추가로 수행했다. 우리의 방법. 우리는 우리의 방법과 에어로 사이에 통계적 유의성이 없는 반면, 우리의 방법과 에어로는 AudioSR보다 높다.\n' +
      '\n' +
      'M2S 평가를 위해, (e) MusicHiFi는 서로 다른 M/S 패닝 계수 하에서 가장 좋은 성능을 보이고, (d) DSP를 사용한 MusicHiFi-V+BWE 방법으로부터 생성된 샘플들은 중간 채널과 측면 채널 사이의 에너지 비율이 동일할 때 (0dB) 우리와 유사한 성능을 보인다. 우리의 접근법과 DSP 기준선 사이의 차이는 본페로니 보정을 사용한 다중 사후 쌍체 t-검정을 통해 측면/중간 배급 6, 12 및 18에 대해 통계적으로 유의하다[41].\n' +
      '\n' +
      '추가 평가를 위해 실제 음악에서 추출되고 [https://MusciHiFi.github.io/web/](https://MusciHiFi.github.io/web/)에서 확산 모델 [11]을 통해 생성된 멜-스펙트로그램을 사용하여 사운드 예를 찾으십시오.\n' +
      '\n' +
      '## V Conclusion\n' +
      '\n' +
      '우리는 MusicHiFi라는 새로운 효율적인 고충실도 스테레오포닉 보코딩 방법을 제안했다. 제안된 방법은 멜-스펙트로그램을 저음질 오디오 파형으로 변환하고, 대역폭 확장을 통해 저해상도 오디오를 고해상도 오디오로 업샘플링하고, 마지막으로 입체음향 고해상도 오디오를 렌더링하는 세 가지 GAN 모델의 캐스케이드를 통해 작동한다. 제안된 방법은 멜-스펙트로그램 기반의 음악 생성기에 통합될 수 있고, 저해상도 오디오의 충실도를 향상시키기 위해 사용될 수 있으며, 그리고/또는 단조 음악을 공간화하기 위해 사용될 수 있다. 기존 연구와 비교하여, 통합 GAN 기반 판별기 및 생성기 설계, 새로운 다운샘플링 호환 BWE 모듈 및 새로운 모노 보존 모노-스테레오 모듈에 기여한다. 객관적 평가와 두 가지 주관적 청취 테스트를 모두 사용하여 우리의 방법을 평가했으며, 우리의 방법은 비교 가능한 M2S 방법을 능가하는 반면 비교 가능한 또는 더 나은 보코딩 및 BWE 결과를 얻었고, 더 나은 공간화 폭 제어를 가지며, 매우 효율적이라는 것을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Method** & **Mel-D\\({}_{i}\\)** & **STFT-D\\({}_{i}\\)** & **VSOOL\\(\\uparrow\\)** & **RTF\\(\\uparrow\\)** \\\\ \\hline \\multirow{2}{*}{DSD100-test} & DSP [34] & 1.07/1.87 & 1.09/1.70 & 4.69 & 5 (CPU) \\\\  & MusicHiFi-M2S & **0.001/1.70** & **0.00/1.53** & **4.73** & **1539** (GPU) \\\\ \\hline \\multirow{2}{*}{FMA-small} & DSP [34] & 0.99/2.29 & 1.08/2.16 & 4.70 & 4 (CPU) \\\\  & MusicHiFi-M2S & **0.00/2.03** & **0.00/1.88** & **4.73** & **1554** (GPU) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: Comparison of objective metrics for M2S system. Values in Mel-D and STFT-D include mid/side channels.\n' +
      '\n' +
      '도. 2: 주관적 듣기 테스트 바이올린 플롯. BWE 테스트(왼쪽) 및 M2S 테스트(오른쪽)입니다. 테스트 조건은 (a) AudioSR, (b) cascaded MusicHiFi-V and Aero 및 (c) MusicHiFi-V+BWE (d) MusicHiFi-V+BWE with DSP (e) Full MusicHiFi를 포함한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>