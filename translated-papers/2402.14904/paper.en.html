<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Watermarking Makes Language Models Radioactive\n' +
      '\n' +
      'Tom Sander\\({}^{1,2,*}\\)\n' +
      '\n' +
      'Pierre Fernandez\\({}^{1,3,*,\\dagger}\\)\n' +
      '\n' +
      'Alain Durmus\\({}^{2}\\)\n' +
      '\n' +
      'Matthijs Douze\\({}^{1}\\)\n' +
      '\n' +
      'Teddy Furon\\({}^{3}\\)\n' +
      '\n' +
      '\\({}^{1}\\)FAIR, Meta, \\({}^{2}\\)CMAP, Ecole polytechnique, \\({}^{3}\\)Inria Rennes\n' +
      '\n' +
      '\\({}^{*}\\)Equal contribution, \\({}^{\\dagger}\\)Project lead\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'This paper investigates the _radioactivity_ of LLM-generated texts, _i.e._ whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (\\(p\\)-value \\(<10^{-5}\\)) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.\n' +
      '\n' +
      'Correspondence tomsander,pfz@meta.com\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Language Models (LLMs) are often instruction fine-tuned to align them with human prompts and improve their performance and generalization (Ouyang et al., 2022; Wei et al., 2022; Chung et al., 2022). Fine-tuning requires expert knowledge to balance diversity and quality in the instruction dataset and a costly collection of manual annotations, especially for alignment (OpenAI, 2023; Touvron et al., 2023; Gemini, 2023). To address the cost and the difficulties of fine-tuning, practitioners sometimes train on synthetic data generated by a model that has already been instructed, such as Bard, ChatGPT, or Claude. For example, works by Wang et al. (2022); Honovich et al. (2022); Peng et al. (2023) created instruction data for many of the most recent LLMs (Taori et al., 2023; Xu et al., 2023; Gunasekar et al., 2023; Mukherjee et al., 2023). This may also be unintentional when, for example, Turkers use ChatGPT output to perform their tasks (Veselovsky et al., 2023). Such imitation (Wallace et al., 2020) raises questions about whether the fine-tuned model is a derivative work of the original model. In this context, it is crucial to understand how to detect when LLM outputs are used as training data.\n' +
      '\n' +
      'Meanwhile, detecting synthetic texts has become harder. This is increasingly important in cases where the text may be used for malicious purposes (Weidinger et al., 2022; Crothers et al., 2022). One approach to address this is _watermarking_, which embeds a secret trace in the content during or after the generation process, that can be detected to identify the generating model. Watermarking has a long history (Cox et al., 2007), and there is growing interest in its application to generative models. This is especially true for LLMs, thanks to recent techniques that make detection efficient with minimal degradation of the generated output quality (Aaronson and Kirchner, 2023; Kirchenbauer et al., 2023, 2023).\n' +
      '\n' +
      'Figure 1: **Radioactivity setting. Bob fine-tunes his LLM on training data with a small proportion of texts coming from Alice’s LLM. We show that this leaves traces in Bob’s model after fine-tuning and that Alice can detect these traces much more reliably when her texts are watermarked. Therefore, Alice’s watermarks, intended for machine-generated text detection have the secondary effect of revealing the fine-tuning of Bob’s model.**Based on these two observations, this study addresses the following question: _what occurs when watermarked texts are employed as fine-tuning data?_ We explore the potential "radioactivity" - a term coined by Sablayrolles et al. (2020) - of LLM watermarking, which refers to the capacity of watermarked text to contaminate a model when used as fine-tuning data.\n' +
      '\n' +
      'The problem is closely related to Membership Inference Attacks (MIA) (Shokri et al., 2017). However, MIA focuses on cases where a specific text is suspected to have been used in training. Furthermore, the most effective detection requires access to the model\'s logits (Sablayrolles et al., 2019; Carlini et al., 2022). Without this access, an alternative is to detect the memorization of verbatim sentences, which is insufficient as only a tiny fraction of the training data -- often repeated samples -- appears to be extractable (Nasr et al., 2023). On the other hand, watermarked texts carry a signal expected to be memorized at the corpus level due to its repetitive nature. Therefore, training on watermarked texts might be detected even when the specific texts used for fine-tuning are unknown.\n' +
      '\n' +
      'Our contributions to answer this question are:\n' +
      '\n' +
      '* We derive new methods to detect radioactivity in four scenarios depending on the access to the fine-tuned model (_open / closed_) and to the training data (_supervised / unsupervised_). Notably, our open-model detection is orders of magnitude better than the baseline approach.\n' +
      '* We demonstrate that watermarked text is radioactive in a real-world setting where an LLM is fine-tuned on Self-Instruct (Wang et al., 2022) output. For instance, our tests detect radioactivity with a \\(p\\)-value of \\(10^{-5}\\) when no more than \\(5\\%\\) of fine-tuning data is watermarked.\n' +
      '* We break down the contamination process and find for instance that small windows for watermarking hashing increases radioactivity.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Related work\n' +
      '\n' +
      'Membership inference attacks(MIAs) aim to determine whether an arbitrary sample is included in a model\'s training data, with varying granularity on the adversary\'s knowledge (Nasr et al., 2019). Most of the time, the detection either build shadow models and observe a difference in their behavior (Shokri et al., 2017, 2017, 2017, 2020) or directly observe the loss of the model (Yeom et al., 2018; Sablayrolles et al., 2019; Watson et al., 2021; Carlini et al., 2022). In the context of generative models, MIAs are intertwined with _dataset contamination_ where one detects that an entire dataset is part of the training data (Shi et al., 2023; Golchin and Surdeanu, 2023), and _extraction attacks_ where one reconstructs some training data by prompting the model to regurgitate texts (Carlini et al., 2019, 2021; Nasr et al., 2023). Depending on the application, MIAs can violate the confidentiality of sensitive training or reveal training on "forbidden" data, like copyrighted material or evaluation data (which undermines benchmark results).\n' +
      '\n' +
      'Watermarking for LLMs.A recent branch of watermarking methods for decoder-only LLMs modifies either the probability distribution (Kirchenbauer et al., 2023) or the sampling method of the next token (Aaronson and Kirchner, 2023; Kuditipudi et al., 2023). Theoretical works show that the detectability of the watermark depends on the entropy of generated text (Christ et al., 2023; Huang et al., 2023). In our case, we don\'t aim at detecting watermarks on generated text (possibly edited), but at stating whether a _model is contaminated_. This requires methods to detect a faint yet statistically significant watermark signal in a large volume of tokens.\n' +
      '\n' +
      'Gu et al. (2023) distill the methods within the model weights, allowing LLMs to generate watermarked logits natively, which is key for open-source models. In contrast, we focus on unintentional contamination where the signal emerges as an undesirable consequence of training over watermarked data: Alice and Bob in Fig. 1 are not collaborating; Bob may not know that Alice\'s LLM produces watermarked texts. Moreover, we handle cases where Bob consumes only a small proportion of watermarked data.\n' +
      '\n' +
      'Watermarking is also used for intellectual property protection, with different strategies. For instance, He et al. (2022, 2020); Li et al. (2023) use lexical properties like synonyms whereas Peng et al. (2023) rely on backdoors. Zhao et al. (2023) develop a watermark specifically dissuading model stealing via distillation. Yet, the evaluation of its accuracy is empirical, lacking a sound watermark detector (Aaronson and Kirchner, 2023; Kirchenbauer et al., 2023; Kuditipudi et al., 2023), that provides \\(p\\)-values that align with empirical false positive rates (Fernandez et al., 2023). In this work, instead of presenting a novel protection technique, we study the radioactivity of decoding-based LLM watermarks used to detect AI-generated text with proven accuracy. Closer to our work, one study considers watermarking as an active protection of training images. Sablayrolles et al. (2020) introduce the concept of _radioactivity_: images of the training set are watermarked to leave a detectable trace in any classifier trained on them.\n' +
      '\n' +
      '### Technical background for LLM watermarking\n' +
      '\n' +
      'This paper focuses on watermarking methods that alters the LLM decoding (Aaronson and Kirchner, 2023; Kirchenbauer et al., 2023), due to their performance and practicality1.\n' +
      '\n' +
      'Footnote 1: We do not consider the work of Kuditipudi et al. (2023). Despite its robustness, the detection relies on the expensive Levenshtein distance, and \\(p\\)-values are computed by running many detections on other secret keys, making \\(p\\)-values below \\(10^{-2}\\) intractable.\n' +
      '\n' +
      'We consider a decoder-only LLM that takes as input the context and outputs a vector of logits. The context is a sequence of tokens \\(\\left(x^{(-C)},...,x^{(-1)}\\right)\\in\\mathcal{V}^{C}\\), \\(\\mathcal{V}\\) being the vocabulary of the model. The vector of logits \\(\\boldsymbol{\\ell}\\in\\mathds{R}^{|\\mathcal{V}|}\\) output by the LLM is transformed into \\(\\mathbf{p}=\\text{softmax}(\\boldsymbol{\\ell})\\in[0,1]^{|\\mathcal{V}|}\\), the probability distribution of the next token. The text is generated by sampling the next token \\(x^{(0)}\\) from this distribution with some procedure (top-k sampling (Fan et al., 2018; Radford et al., 2019), nucleus-sampling (Holtzman et al., 2019), etc.), then appending it to the context, and repeating the process.\n' +
      '\n' +
      'The _watermark embedding_ alters the logit vector \\(\\boldsymbol{\\ell}\\) or the sampling procedure depending on a secret key. Usually, the output of a secret-keyed cryptographic function hashes \\(k\\) previous tokens \\(\\left(x^{(-k)},\\ldots,x^{(-1)}\\right)\\). It serves as a seed \\(\\mathsf{s}\\) that initializes a random number generator, which in turn influences the choice of the next token \\(x^{(0)}\\). In the example of Kirchenbauer et al. (2023), it creates a greenlist of tokens for which the sampling probability is increased.\n' +
      '\n' +
      'The _watermark detection_ tokenizes the text under scrutiny, repeats the secret seed generation and scores each token. The function that attributes a score to the current token \\(x^{(0)}\\) may therefore be written as a real-valued function \\(W_{\\text{score}}\\) that takes as input the token to score relatively to a seed \\(\\mathsf{s}\\) and the sequence of tokens \\(\\left(x^{(-k)},\\ldots,x^{(-1)}\\right)\\):\n' +
      '\n' +
      '\\[\\mathsf{s},\\left(x^{(-i)}\\right)_{i=k}^{1};x^{(0)}\\mapsto W_{\\text{score}} \\left(\\mathsf{s},\\left(x^{(-i)}\\right)_{i=k}^{1};x^{(0)}\\right)\\in\\mathds{R}.\\]\n' +
      '\n' +
      'A statistical test based on the cumulative score and the number of tokens (_e.g._, the number of greenlist tokens among all the tokens) determines if the text is watermarked. More details are provided in App. A.\n' +
      '\n' +
      '## 3 Problem Formulation\n' +
      '\n' +
      '### Notations and Problem Statement\n' +
      '\n' +
      'We consider the scenario of Figure 1. A model owner, called _Alice_, has a proprietary language model \\(\\mathcal{A}\\), fine-tuned for specific tasks such as chatting, problem solving, or code generation, which is available through an API. _Bob_ owns another language model \\(\\mathcal{B}\\). Alice suspects that Bob fine-tuned \\(\\mathcal{B}\\) on some outputs from \\(\\mathcal{A}\\). We denote by \\(D\\) the dataset used to fine-tune \\(\\mathcal{B}\\), among which a corpus of texts \\(D^{\\mathcal{A}}\\subset D\\) is made of outputs from \\(\\mathcal{A}\\). We define \\(\\rho\\) as the proportion of \\(\\mathcal{B}\\)\'s fine-tuning data coming from \\(\\mathcal{A}\\):\n' +
      '\n' +
      '\\[\\rho:=|D^{\\mathcal{A}}|/|D|. \\tag{1}\\]\n' +
      '\n' +
      'We describe four main scenarios depending on Alice\'s access over Bob\'s model and fine-tuning data.\n' +
      '\n' +
      'Access to Bob\'s data.We consider two settings for Alice\'s knowledge about Bob\'s training data.\n' +
      '\n' +
      '* In the _supervised_ setting, Bob queries \\(\\mathcal{A}\\) using an identifiable account. Alice retains all the content \\(\\tilde{D}^{\\mathcal{A}}\\) that \\(\\mathcal{A}\\) generated for Bob. Thus, Alice knows that \\(D^{\\mathcal{A}}\\subseteq\\tilde{D}^{\\mathcal{A}}\\). We define the _degree of supervision_ as: \\[d:=|D^{\\mathcal{A}}|/|\\tilde{D}^{\\mathcal{A}}|.\\] (2)\n' +
      '* In the _unsupervised_ setting, Bob does not use any identifiable account or is hiding behind others, so \\(d=0\\). This is the most realistic scenario.\n' +
      '\n' +
      '\\(\\rho\\) and \\(d\\) do not represent the same concept, however \\(\\rho=d=0\\) when \\(\\mathcal{B}\\) did not see any output from \\(\\mathcal{A}\\).\n' +
      '\n' +
      'Access to Bob\'s model.We consider two scenarios:\n' +
      '\n' +
      '* Alice has an _open-model_ access to \\(\\mathcal{B}\\). She can forward any inputs through \\(\\mathcal{B}\\) and can observe the output logits. This may be the case if Bob open-sources his model, or if Alice sought it via legitimate channels.\n' +
      '* Alice has a _closed-model_ access. She can only query \\(\\mathcal{B}\\) through an API that does not output probability vectors or logits: Alice only observes the generated texts. This would be the case for most chatbots, for instance.\n' +
      '\n' +
      '### Radioactivity\n' +
      '\n' +
      '**Definition 1** (Text Radioactivity).: _Given a statistical test \\(T\\) s.t. "\\(\\mathcal{B}\\) was not trained on \\(D\\)" \\(\\subset\\mathcal{H}_{0}\\), we say that the corpus of texts \\(D\\) is \\(\\alpha\\)-radioactive for \\(\\mathcal{B}\\) if \\(T\\) is able to reject \\(\\mathcal{H}_{0}\\) at a significance level (\\(p\\)-value) smaller than \\(\\alpha\\)._\n' +
      '\n' +
      '**Definition 2** (Model Radioactivity).: _Given a statistical test \\(T\\) s.t. "\\(\\mathcal{B}\\) was not trained on outputs of \\(\\mathcal{A}\\)" \\(\\subset\\mathcal{H}_{0}\\), we say that model \\(\\mathcal{A}\\) is \\(\\alpha\\)-radioactive for \\(\\mathcal{B}\\) if \\(T\\) is able to reject \\(\\mathcal{H}_{0}\\) at a significance level (p-value) smaller than \\(\\alpha\\)._\n' +
      '\n' +
      'Thus, \\(\\alpha\\) quantifies the radioactivity of a dataset or model. A low \\(\\alpha\\) (_e.g._, \\(10^{-6}\\)) indicates strong radioactivity because the detection test has a high confidence, while \\(\\alpha\\approx 0.5\\) indicates low radioactivity (the detection is a random test).\n' +
      '\n' +
      '## 4 Radioactivity Detection\n' +
      '\n' +
      'We derive ways to detect the radioactivity of non-watermarked and watermarked text in a language model, in the different settings presented in the previous section.\n' +
      '\n' +
      '### Without watermarking\n' +
      '\n' +
      'In the open-model/supervised case, MIA evaluates the radioactivity of one sample/sentence by observing the loss (or perplexity) of \\(\\mathcal{B}\\) on carefully selected sets of inputs. The perplexity is expected to be smaller on samples seen during training (this is sometimes called a _loss attack_). We extend this idea for our baseline radioactivity detection test of a non-watermarked text corpus. We divide the corpus of texts into sentences (of 256 tokens) and compute \\(\\mathcal{B}\\)\'s loss on each sentence. We calibrate it with the zlib entropy (Roelofs, 2017), as done by Carlini et al. (2021) for sample-based MIA. The goal of the calibration is to account for the complexity of each sample and separate this from the over-confidence of \\(\\mathcal{B}\\).\n' +
      '\n' +
      'K-S test.We test the null hypothesis \\(\\mathcal{H}_{0}\\): "_the perplexity of \\(\\mathcal{B}\\) on \\(\\tilde{D}^{\\mathcal{A}}\\) has the same distribution as the perplexity on new texts generated by \\(\\mathcal{A}\\)_". Indeed, if \\(\\mathcal{B}\\) was not fine-tuned on portions of \\(\\tilde{D}^{\\mathcal{A}}\\), then necessarily \\(\\mathcal{H}_{0}\\) is true. To compare the empirical distributions we use a two-sample Kolmogorov-Smirnov test (Massey, 1951). Given the two cumulative distributions \\(F\\) and \\(G\\) over loss values, we compute the K-S distance as \\(d_{\\text{KS}}(F,G)=\\sup_{x}|F(x)-G(x)|\\). We reject \\(\\mathcal{H}_{0}\\) if this distance is higher than a threshold, which sets the \\(p\\)-value of the test, and conclude that \\(\\tilde{D}^{\\mathcal{A}}\\) is radioactive for \\(\\mathcal{B}\\). This is inspired by Sablayrolles et al. (2018), who perform a similar K-S test in the case of image classification. It significantly diverges from the approach of Shi et al. (2023), which derives an empirical test by looking at the aggregated score from one tail of the distribution.\n' +
      '\n' +
      'There is no strong detection outside this setting:\n' +
      '\n' +
      '* MIA detects if specific samples were used in training. In the _unsupervised_ setting, Alice only suspects that Bob used some outputs from her model but does not know what particular samples.\n' +
      '* In the _closed-model_ setting, Alice only has access to \\(\\mathcal{B}\\)\'s outputs. Extracting entire sentences from \\(\\tilde{D}^{\\mathcal{A}}\\) by prompting \\(\\mathcal{B}\\) is likely insufficient as only a tiny fraction of LLM training data can be extracted verbatim (Nasr et al., 2023). Besides, this would not provide Alice with a reliable statistical test.\n' +
      '\n' +
      '### With watermarking\n' +
      '\n' +
      'We now consider the case where outputs of \\(\\mathcal{A}\\) are watermarked with the method \\(W\\) and a secret key \\(\\mathfrak{s}\\) (unique to \\(\\mathcal{A}\\)). In this case, we show that there is a detection test for all settings (see Tab. 1). \\(W\\)\'s scoring function \\(W_{\\text{score}}\\) depends on the observed tokens and \\(\\mathfrak{s}\\); and \\(W\\)\'s watermark detection test \\(T\\) depends on the score and the number of tokens. \\(T\\) tests the null hypothesis \\(\\mathcal{H}_{0}\\): "_The text was not generated following \\(W\\) with secret key \\(\\mathfrak{s}\\)_" (see Sec. 2.2 and App. A for details on the scores and the statistical tests).\n' +
      '\n' +
      'Naive approach.Radioactivity can be detected by performing watermark detection on a large corpus of texts generated by \\(\\mathcal{B}\\). With regards to the statistical test, it aligns with Def. 1. Indeed, the text cannot be generated following \\(W\\) and \\(\\mathfrak{s}\\) if \\(\\mathcal{B}\\) has never seen the watermark, so if "\\(\\mathcal{B}\\)_did not use outputs of \\(\\mathcal{A}\\)_", then \\(\\mathcal{H}_{0}\\) is true. However, traces of the watermark in \\(\\mathcal{B}\\) can only be found on \\(k\\)-grams that are part of \\(D^{\\mathcal{A}}\\) (due to the watermark embedding, see Sec. 2.2). Even if we assume that these \\(k\\)-grams were strongly watermarked and that \\(\\mathcal{B}\\) has memorized all of them, they still constitute only a small portion of the \\(|\\mathcal{V}|^{k}\\)\\(k\\)-grams that can be tested, making the test suboptimal.\n' +
      '\n' +
      'Watermark detection in \\(\\mathcal{B}\\).We introduce two methods depending on the access to \\(\\mathcal{B}\\):\n' +
      '\n' +
      '* _closed-model_: we prompt and generate new texts with \\(\\mathcal{B}\\). In the supervised setting, we only prompt\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{With WM} & \\multicolumn{2}{c}{Without WM} \\\\ \\cline{2-5}  & Open & Closed & Open & Closed \\\\ Supervised & ✓ & ✓ & ✓ & \\(\\times\\) \\\\ Unsupervised & ✓ & ✓ & \\(\\times\\) & \\(\\times\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Availability of radioactivity detection under the different settings. _Open_ / _closed-model_ refers to the availability of Bob’s model to Alice, and _supervised_ / _unsupervised_ to her knowledge of his data. Detection relying on MIA (without WM) is described in 3.3§1 and detection with watermarks in 3.3§2.\n' +
      '\n' +
      '\\(\\mathcal{B}\\) with (watermarked) text from \\(\\tilde{D}^{\\mathcal{A}}\\). In the unsupervised one, we prompt with new texts from the same distribution as the one suspected of having been trained on.\n' +
      '* _open-model:_ instead of generating new text with \\(\\mathcal{B}\\), we directly forward sentences through \\(\\mathcal{B}\\). Let \\(\\big{(}x^{(1)},\\ldots,x^{(j)}\\big{)}\\) be a sequence of tokens and \\(y^{(j)}\\) the most likely next token according to \\(\\mathcal{B}\\)\'s decoding. We score \\((x^{(j-k-1)},\\ldots,x^{(j)};\\,y^{(j)})\\) with \\(W_{\\text{score}}\\) (for more details refer to Algorithm 1 in App. B).\n' +
      '\n' +
      'Filter on scored \\(k\\)-grams.To improve detection, we introduce a filter \\(\\phi\\), a set of \\(k\\)-grams that are likely to have been trained on. Tokens are scored only if their preceding \\(k\\)-gram window - watermark context window used for hashing - is part of \\(\\phi\\). This focuses the score computation on \\(k\\)-grams where the watermark has possibly been learned. In the fully _supervised_ setting where we exactly know \\(\\mathcal{B}\\)\'s training data, \\(\\phi\\) is made of the \\(k\\)-grams that were used during training: all \\(k\\)-grams from \\(D^{\\mathcal{A}}\\). In the _unsupervised_ setting, we still focus on \'likely\' contaminated sets of tokens, _e.g._, \\(k\\)-grams frequently appearing in watermarked text generated by \\(\\mathcal{A}\\). The filter \\(\\phi\\) is used only in the closed-model setting. The choice of distribution used to define \\(\\phi\\) and generate tokens from \\(\\mathcal{B}\\) turns out to be of great importance. This is discussed in Sec. 6.2.\n' +
      '\n' +
      'Token scoring and de-duplication.It has been shown by Fernandez et al. (2023) that the detection tests are empirically inaccurate because of biases in the distribution of tokens which break the independence hypothesis. In practice, we found this phenomenon to be even more true in our case, since the watermark must be observed in orders of magnitude more tokens than for traditional watermarking (where the text under scrutiny is a few hundred tokens at most). The solution is to score a token only if its previous \\(k\\)-gram (watermark context window used for hashing) has not already been seen during the detection. This provides reliable \\(p\\)-values even when many tokens are analyzed. We give more details on the scoring procedure, methods to improve the deduplication and the correctness of our tests in Appendix B.\n' +
      '\n' +
      '## 5 Radioactivity in Instruction Datasets\n' +
      '\n' +
      'Our experiments start by considering a realistic scenario where a pre-trained LLM \\(\\mathcal{B}\\) is instruction fine-tuned on instruction/answer pairs generated by model \\(\\mathcal{A}\\). We show that watermarked synthetic instructions are radioactive, and we compare their radioactivity levels to those of non-watermarked instructions when possible.\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      'Instruction data generation.We use the protocol presented in Self-instruct (Wang et al., 2022) with \\(\\mathcal{A}\\)=Llama-2-chat-7B (Touvron et al., 2023). We prompt the model with an instruction followed by three examples of instruction/answer pairs, and ask it to generate the next 20 instruction/answer pairs. The sampling from the LLM logits is done either without watermarking or with the watermarking method of Kirchenbauer et al. (2023), at logit bias \\(\\delta=3.0\\), proportion of greenlist tokens \\(\\gamma=0.25\\), and \\(k=2\\). In both cases we use nucleus sampling (Holtzman et al., 2019) with \\(p=0.95\\) and \\(T=0.8\\). We post-process the generated data to remove unfinished answers and near-duplicates instructions. This yields a dataset of 100k instruction/answer pairs (\\(\\approx\\)14M tokens)2 for both cases. We give more details, examples of generated instructions and the corresponding watermark detection rates in App. C.2.\n' +
      '\n' +
      'Footnote 2: For comparison, formatted text from Alpaca is \\(\\approx\\)6M tokens, Alpaca-GPT4 \\(\\approx\\)10M tokens, and OASST1 \\(\\approx\\)20M tokens.\n' +
      '\n' +
      'Finally, we create six mixed datasets with \\(\\rho\\) % of watermarked data (with \\(\\rho\\in\\{0,1,5,10,50,100\\}\\)), filling the rest with non-watermarked instructions. In other words, we fix the total number of instructions at around 14M tokens but change the proportion of watermarked ones (which represents \\(\\mathcal{A}\\)\'s outputs).\n' +
      '\n' +
      'Fine-tuning.We train on these six synthetic datasets, closely following the approach of Alpaca (Taori et al., 2023): we use the Adam optimizer (Kingma and Ba, 2017) for 3000 steps, with a batch size of 8, a learning rate of \\(10^{-5}\\) and a context size of 2048 tokens (which results in 3 training epochs). The learning rate follows a cosine annealing schedule (Loshchilov and Hutter, 2017) with 100 warmup steps. We fine-tune \\(\\mathcal{B}\\)=Llama-1-7B (Touvron et al., 2023), a model trained on different datasets than \\(\\mathcal{A}\\)=Llama-2, to avoid biases that could arise if the same base model was also used for fine tuning.\n' +
      '\n' +
      '### Quality inspection of the instruction tuning\n' +
      '\n' +
      'To keep a realistic scenario, Alice\'s hyperparameter selection for watermarking aims at 1) generating high-quality instructions, and 2) ensuring that the watermark can be detected even in small text segments. Specifically, the watermark window size is \\(k=2\\), sufficiently wide to eliminate biases that occur for 0 or 1, yet narrow enough for the watermark to be robust to edits. A value of \\(\\delta=3\\) yields high-quality text, while ensuring that the watermark is strong enough to be detected with a \\(p\\)-value of \\(10^{-6}\\) on approximately 100 tokens (full results in App. C.2).\n' +
      '\n' +
      'We inspect the fine-tuned \\(\\mathcal{B}\\) network outputs both qualitatively (examples are available in Fig. 2 and App. C.2) and quantitatively in Tab. 2. We first look at scores of an evaluation setup close to that of Llama and report the 0-shot: exact match score for Natural Questions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017); exact match score without majority voting for GSM8k (Cobbe et al., 2021); pass@1 for HumanEval (Chen et al., 2021); and accuracy on MMLU (Hendrycks et al., 2020). As expected, instruction-tuning does not affect most benchmarks while enhancing it for MMLU as in (Dettmers et al., 2023). It also confirms that watermarking instructions does not significantly influence the fine-tuning.\n' +
      '\n' +
      '### A Baseline with membership inference attacks\n' +
      '\n' +
      'In the absence of watermarks, we proceed as in Sec. 4 for the setup where MIA is achievable: Alice has an _open-model_ access to \\(\\mathcal{B}\\) and is aware of all data \\(\\tilde{D}^{\\mathcal{A}}\\) generated for Bob (supervised setting). Bob has used a portion \\(D^{\\mathcal{A}}\\) for fine-tuning \\(\\mathcal{B}\\), given by the degree of supervision \\(d\\), see Section 3.1. Experimentally, we use the K-S test to discriminate between the calibrated perplexity of \\(\\mathcal{B}\\) on: \\(\\mathcal{D}_{(0)}\\) containing 5k instruction/answers (cut at 256 tokens) that were not part of \\(\\mathcal{B}\\)\'s fine-tuning; and \\(\\mathcal{D}_{(d)}\\) containing \\((1/d)\\times\\)5k instruction/answers from which \\(5k\\) were. Distribution \\(\\mathcal{D}_{(d)}\\) simulates what happens when Bob generates a lot of data and only fine-tunes on a small proportion.\n' +
      '\n' +
      'Figure 3 compares the distributions for \\(d=0\\) and \\(d>0\\). The detection becomes more challenging as \\(d\\) decreases: the data contains more and more texts that Bob did not fine-tune on, so the difference of the two perplexity distributions is fainter. Table 3 presents the \\(p\\)-values obtained from the radioactivity test (results with varying \\(d\\) is available in App. D). When \\(d>2\\%\\), the test rejects the null hypothesis at a strong significance level: \\(p<10^{-5}\\) implies that when radioactive contamination is detected, the probability of a false positive is \\(10^{-5}\\). As \\(d\\) decreases, the test becomes less powerful. It is random in the edge case \\(d=0\\), the unsupervised setting where Alice lacks knowledge about the data used by Bob. In contrast, the next section shows that radioactivity detection on watermarked data can succeed in that setting.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c|c} \\hline \\hline  & Nq & TQA & GSM8k & H.Eval & Avg. & MMLU \\\\ \\hline \\multicolumn{6}{l}{_Fine-tuned with \\(\\rho\\) \\% of watermarked data:_} \\\\\n' +
      '0\\% & 5.0 & 33.6 & 11.8 & 12.8 & 15.8 & 33.6 \\\\\n' +
      '5\\% & 5.2 & 35.7 & 11.2 & 11.6 & 15.9 & 34.7 \\\\\n' +
      '50\\% & 4.1 & 35.5 & 9.6 & 12.8 & 15.5 & 35.0 \\\\\n' +
      '100\\% & 5.6 & 36.4 & 11.1 & 9.8 & 15.7 & 31.0 \\\\ \\hline Base & 3.2 & 36.2 & 10.5 & 12.8 & 15.7 & 28.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Evaluation of Llama-7B fine-tuned with varying proportion of watermarked instruction data.\n' +
      '\n' +
      'Figure 3: Distributions of the calibrated loss of \\(\\mathcal{B}\\) across two types of distributions generated by \\(\\mathcal{A}\\): texts generated by \\(\\mathcal{A}\\) outside of \\(\\mathcal{B}\\)’s fine-tuning data (purple), texts of \\(\\tilde{D}^{\\mathcal{A}}\\) of which \\(d\\%\\) were used during training (orange). MIA aims to detect the difference between the two distributions. It gets harder as \\(d\\) decreases, as the actual fine-tuning data is mixed with texts that Bob did not use.\n' +
      '\n' +
      'Figure 2: Example of generated answers from Bob’s model \\(\\mathcal{B}\\) (Llama-1), fine-tuned on instruction data generated by Alice’s model \\(\\mathcal{A}\\) (Llama-2-chat) with different proportions \\(\\rho\\) of watermarked data. See Figure 13 for example of instructions used for instruction-tuning.\n' +
      '\n' +
      '### _With watermarking: open-model setting_\n' +
      '\n' +
      'We now study the case where the fine-tuning data (or part of them) are watermarked. We discuss radioactivity detection in the open-model setting (see Sec. 4), with \\(\\rho\\) the proportion of \\(\\mathcal{A}\\)\'s watermarked data in \\(\\mathcal{B}\\)\'s fine-tuning set. We use the watermark detection presented in Sec. 4.2 on a dataset of watermarked instructions generated by \\(\\mathcal{A}\\) to score \\(N=225\\)k bigrams (for 500k generated next tokens before de-duplication), which represents around 2,000 forward-passes on 256-token texts. For the supervised setting (\\(d=1\\)), this dataset is made of all the \\(\\rho\\%\\) watermarked texts among the 100k instructions used to train \\(\\mathcal{B}\\); for the unsupervised setting (\\(d=0\\)), it is made of watermarked instructions unseen when \\(\\mathcal{B}\\) was fine-tuned.\n' +
      '\n' +
      'We repeat detection 10 times on different chunks of texts, and present in Fig. 4 the \\(p\\)-values of our test for different proportions \\(\\rho\\). Similar to MIA, the supervised setting is straightforward, and radioactivity is detected with a \\(p\\)-value smaller than \\(10^{-30}\\) even if only 1% of Bob\'s fine-tuning data originated from \\(\\mathcal{A}\\). Indeed, in this scenario we only score 1) \\(k\\)-grams that can actually be contaminated and 2) within a context that exactly matches the one seen during fine-tuning, which makes the test very confident.\n' +
      '\n' +
      'Conversely, when \\(d=0\\), MIA is not applicable, while our open-model radioactivity detection test still yields \\(p<10^{-5}\\) when no more than 5% of the instructions used to fine-tune \\(\\mathcal{B}\\) originate from Alice\'s model. In this case, the detection is done on a corpus of texts that does not contain samples that were used by Bob. However, it does contain \\(k\\)-grams that likely overlap with \\(k\\)-grams from \\(D^{\\mathcal{A}}\\) on which Bob trained, and on which radioactivity may be detected. As the level of supervision diminishes (\\(1>d>0\\)), the proportion of tokens in \\(\\tilde{D}^{\\mathcal{A}}\\) that Bob actually used decreases, thereby weakening the test. This intermediate scenario of weak supervision is depicted in Fig. 9 of App. D for the case where \\(\\rho=5\\%\\).\n' +
      '\n' +
      '### _With watermarking: closed-model setting_\n' +
      '\n' +
      'In the closed-model setting, Bob\'s model \\(\\mathcal{B}\\) is only accessible via an API that can generate answers from prompts. To study this setup in the unsupervised setting, we prompt a fine-tuned \\(\\mathcal{B}\\)=Llama-1 with new instructions, and run detection on its outputs. We concatenate all the answers, and score \\(N=600\\)k tokens (after filtering and de-duplicating the \\(k\\)-grams of \\(\\approx 1.5\\)M generated tokens). This represents around \\(10^{4}\\) queries if we assume an answer is 100 tokens.\n' +
      '\n' +
      'We proceed as follows to create the filter \\(\\phi\\) (as a reminder, we only score a token if it previous \\(k\\)-gram is part of \\(\\phi\\)). In the supervised setting (\\(d>0\\)), we directly use the watermarked prompts/answers from \\(\\tilde{D}^{\\mathcal{A}}\\), part of which were used for fine-tuning, and store all \\(k\\)-grams. In the unsupervised setting, we generate 100k new watermarked instructions with \\(\\mathcal{A}\\) and save all \\(k\\)-grams to create the filter.\n' +
      '\n' +
      'Figure 5 compares detection with and without filter when 1% of fine-tuning data are watermarked, in the\n' +
      '\n' +
      'Figure 4: Radioactivity detection results. Average of \\(\\log_{10}(p)\\) over 10 runs (\\(\\downarrow\\) is better). Bars indicate standard deviations. The detection methods are detailed in Sec. 4 for each setting. For instance, in the supervised closed-model setting our tests detect the watermark (\\(p<10^{-5}\\)) when only 1% of training data are watermarked.\n' +
      '\n' +
      'Figure 5: \\(\\log_{10}(p)\\) as a function of the number of generated tokens, in the supervised closed-model setting with \\(\\rho=1\\%\\). We perform the watermark detection test on text generated by \\(\\mathcal{B}\\) with prompts from \\(\\tilde{D}^{\\mathcal{A}}\\). When filtering, we only score \\(k\\)-grams that were part of \\(\\tilde{D}^{\\mathcal{A}}\\).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Supervision \\(d\\) & 0.1\\% & 1\\% & 5\\% & 10\\% \\\\ \\hline \\multirow{2}{*}{\\(\\log_{10}(p)\\)} & MIA & -0.4\\(\\pm\\)0.4 & -0.6\\(\\pm\\)0.5 & \\(<\\mathbf{-30}\\) & \\(<\\)-30 \\\\ \\cline{2-6}  & WM & \\(\\mathbf{-5.8}\\pm\\)1.8 & \\(\\mathbf{-6.5}\\pm\\)0.9 & -16.0\\(\\pm\\)2.6 & \\(<\\)-30 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Detection confidence \\(\\log_{10}(p)\\) with varying degrees of supervision \\(d\\), at \\(\\rho=5\\%\\) of \\(\\mathcal{B}\\)’s training data coming from \\(\\mathcal{A}\\).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      'model. Second, the number of possible \\(k\\)-grams is \\(|\\mathcal{V}|^{k}\\) and therefore increases with \\(k\\), while the number of watermarked tokens is fixed \\(m\\). Thus, at detection time, the number of radioactive \\(k\\)-grams decrease for increasing \\(k\\), which diminishes the power of the test.\n' +
      '\n' +
      '_Data distribution_. We consider the unsupervised setting where Alice has no prior knowledge about the distribution of \\(D^{\\mathcal{A}}\\), the data generated with \\(\\mathcal{A}\\) used to fine-tune \\(\\mathcal{B}\\). As an example, we assume Alice does not know the language of \\(D^{\\mathcal{A}}\\), which could be Italian, French, English, Spanish, or German.\n' +
      '\n' +
      'For this experiment, we run the detection test of Sec. 4.2 on text generated by \\(\\mathcal{B}\\), with prompts from Wikipedia in different languages. The power of the radioactivity test on a different language - that might share few \\(k\\)-grams with the language of \\(D^{\\mathcal{A}}\\) - will be low, as shown in Tab. 6.\n' +
      '\n' +
      'Alice may however combine the \\(p\\)-values of each test with Fisher\'s method. This discriminates against \\(\\mathcal{H}_{0}\\): "_none of the datasets are radioactive_", under which the statement "_Bob did not use any outputs of \\(\\mathcal{A}\\)_" falls. Therefore, the test aligns with our definition of model radioactivity as per definition 2. From Tab. 6, Fisher\'s method gives a combined \\(p\\)-value of \\(<10^{-50}\\). Thus, even if Alice is unaware of the specific data distribution generated by \\(\\mathcal{A}\\) that Bob may have used to train \\(\\mathcal{B}\\) (_e.g._ human dialogues, problem-solving scenarios or essays), she may still test the radioactivity across various distributions and combine the significance levels.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'This study formalizes the concept of "radioactivity" in language models. It introduces methods to detect traces that LLM-generated texts leave when used as training data. We show that this task is difficult for non-watermarked texts in the most realistic scenarios. Yet, watermarked texts exhibit significant radioactivity, contaminating models during fine-tuning. This makes it possible to identify with high confidence if outputs from a watermarked model have been used to fine-tune another one (but cannot be used to detect the use of the other model itself).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aaronson and Kirchner (2023) Scott Aaronson and Hendrik Kirchner. Watermarking GPT outputs, 2023. [https://scottaaronson.blog/?m=202302](https://scottaaronson.blog/?m=202302).\n' +
      '* Carlini et al. (2019) Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks, 2019.\n' +
      '* Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models, 2021.\n' +
      '* Carlini et al. (2022) Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1897-1914. IEEE, 2022.\n' +
      '* Chen et al. (2021) Mark Chen et al. Evaluating large language models trained on code. _arXiv_, 2021.\n' +
      '* Christ et al. (2023) Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for language models. _Cryptology ePrint Archive_, 2023.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* Cobbe et al. (2021) Karl Cobbe et al. Training verifiers to solve math word problems. _arXiv_, 2021.\n' +
      '* Cox et al. (2007) Ingemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. _Digital watermarking and steganography_. Morgan Kaufmann, 2007.\n' +
      '* Crothers et al. (2022) Evan Crothers, Nathalie Japkowicz, and Herna Viktor. Machine generated text: A comprehensive survey of threat models and detection methods. _arXiv preprint arXiv:2210.07321_, 2022.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized l lms. _arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* Fan et al. (2018) Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. _arXiv preprint arXiv:1805.04833_, 2018.\n' +
      '* Fernandez et al. (2023) Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. _2023 IEEE International Workshop on Information Forensics and Security (WIFS)_, 2023.\n' +
      '* Gemini (2023) Team Gemini. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} Language & English & French & Spanish & German Catalan \\\\ \\hline \\(\\log_{10}(p)\\) & \\(<\\)-50 & -7.8 & -5.7 & -4.0 & -2.1 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Influence of the target text distribution on detection. \\(\\mathcal{B}\\) is prompted with beginnings of Wikipedia articles in the corresponding language, and detection is done on generated next tokens. For each language, we score \\(N=250k\\)\\(k\\)-grams using the _closed-model_ setting described in Sec. 4 (without filters).\n' +
      '\n' +
      'Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. _arXiv preprint arXiv:2308.08493_, 2023.\n' +
      '* Gu et al. (2023) Chenchen Gu, Xiang Lisa Li, Percy Liang, and Tatsunori Hashimoto. On the learnability of watermarks for language models. _arXiv preprint arXiv:2312.04469_, 2023.\n' +
      '* Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.\n' +
      '* He et al. (2022a) Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang. Protecting intellectual property of language generation apis with lexical watermark. In _AAAI_, 2022a.\n' +
      '* He et al. (2022b) Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. CATER: Intellectual property protection on text generation APIs via conditional watermarks. In _NeurIPS_, 2022b.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* Hisamoto et al. (2020) Sorami Hisamoto, Matt Post, and Kevin Duh. Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system? _Transactions of the Association for Computational Linguistics_, 8:49-63, 2020.\n' +
      '* Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. _arXiv preprint arXiv:1904.09751_, 2019.\n' +
      '* Honovich et al. (2022) Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. _arXiv preprint arXiv:2212.09689_, 2022.\n' +
      '* Huang et al. (2023) Baihe Huang, Banghua Zhu, Hanlin Zhu, Jason D. Lee, Jiantao Jiao, and Michael I. Jordan. Towards optimal statistical watermarking, 2023.\n' +
      '* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _arXiv_, 2017.\n' +
      '* Kingma and Ba (2017) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n' +
      '* Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _arXiv preprint arXiv:2301.10226_, 2023a.\n' +
      '* Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the reliability of watermarks for large language models, 2023b.\n' +
      '* Kuditipudi et al. (2023) Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang. Robust distortion-free watermarks for language models. _arXiv preprint arXiv:2307.15593_, 2023.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski et al. Natural questions: a benchmark for question answering research. _Trans. of the ACL_, 7, 2019.\n' +
      '* Li et al. (2023) Zongjie Li, Chaozheng Wang, Shuai Wang, and Cuiyun Gao. Protecting intellectual property of large language model-based code generation apis via watermarks. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, pages 2336-2350, 2023.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations_, 2017.\n' +
      '* Mahloujifar et al. (2021) Saeed Mahloujifar, Huseyin A Inan, Melissa Chase, Esha Ghosh, and Marcello Hasegawa. Membership inference on word embedding and beyond. _arXiv preprint arXiv:2106.11384_, 2021.\n' +
      '* Massey (1951) Frank J Massey. The kolmogorov-smirnov test for goodness of fit. _Journal of the American Statistical Association_, 46(253):68-78, 1951.\n' +
      '* Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* Nasr et al. (2019) Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In _2019 IEEE symposium on security and privacy (SP)_, pages 739-753. IEEE, 2019.\n' +
      '* Nasr et al. (2023) Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramer, and Katherine Lee. Scalable extraction of training data from (production) language models, 2023.\n' +
      '* (2023) OpenAI. Gpt-4 technical report. _arXiv_, 2023.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023a.\n' +
      '* Peng et al. (2021) Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu,Guangzhong Sun, and Xing Xie. Are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark. _arXiv preprint arXiv:2305.10036_, 2023b.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* Roelofs (2017) Greg Roelofs. zlib: A massively spiffy yet delicately unobtrusive compression library. _http://www. zlib. net/_, 2017.\n' +
      '* Sablayrolles et al. (2018) Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herve Jegou. D\\(\\backslash\\)ej\\(\\backslash\\)a vu: an empirical evaluation of the memorization properties of convnets. _arXiv preprint arXiv:1809.06396_, 2018.\n' +
      '* Sablayrolles et al. (2019) Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herve Jegou. White-box vs black-box: Bayes optimal strategies for membership inference. In _International Conference on Machine Learning_, pages 5558-5567. PMLR, 2019.\n' +
      '* Sablayrolles et al. (2020) Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herve Jegou. Radioactive data: tracing through training. In _International Conference on Machine Learning_, pages 8326-8335. PMLR, 2020.\n' +
      '* Shi et al. (2023) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. _arXiv preprint arXiv:2310.16789_, 2023.\n' +
      '* Shokri et al. (2017a) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models, 2017a.\n' +
      '* Shokri et al. (2017b) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017b.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Veselovsky et al. (2023) Veniamin Veselovsky, Manoel Horta Ribeiro, and Robert West. Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks, 2023.\n' +
      '* Wallace et al. (2020) Eric Wallace, Mitchell Stern, and Dawn Song. Imitation attacks and defenses for black-box machine translation systems. In _EMNLP_, 2020.\n' +
      '* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.\n' +
      '* Watson et al. (2021) Lauren Watson, Chuan Guo, Graham Cormode, and Alex Sablayrolles. On the importance of difficulty calibration in membership inference attacks. _arXiv preprint arXiv:2111.08440_, 2021.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2022.\n' +
      '* Weidinger et al. (2022) Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by language models. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 214-229, 2022.\n' +
      '* Xu et al. (2023) Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. _arXiv preprint arXiv:2304.01196_, 2023.\n' +
      '* Yeom et al. (2018) Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In _2018 IEEE 31st computer security foundations symposium (CSF)_, pages 268-282. IEEE, 2018.\n' +
      '* Zhao et al. (2023) Xuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. _arXiv preprint arXiv:2302.03162_, 2023.\n' +
      '\n' +
      '## Appendix A Details on LLM Watermarking\n' +
      '\n' +
      'We use the notations presented in Sec. 2.2. This paper considers the watermarking methods (Kirchenbauer et al., 2023, 2023, Aaronson and Kirchner, 2023) which alter the logit vector \\(\\mathbf{\\ell}\\) or the probability vector \\(\\mathbf{p}\\) when trying to generate \\(x^{(0)}\\), depending on the window of \\(k\\) previous tokens in the context: \\(x^{(-k)},...,x^{(-1)}\\). A hash function maps these tokens to a random seed that initializes a random number generator (RNG). The hash function also depends on a secret key \\(\\mathsf{s}\\). RNG is then used to influence or determine the next token\'s choice \\(x^{(0)}\\).\n' +
      '\n' +
      '### Kirchenbauer et al. (2023)\n' +
      '\n' +
      'RNG is used to create a greenlist containing \\(\\gamma|\\mathcal{V}|\\) tokens, where \\(\\gamma\\in[0,1]\\). The logit of every token in the greenlist is incremented by \\(\\delta\\). The sampling then proceeds as usual. Intuitively, this encourages the generation of greenlist tokens by increasing their probability.\n' +
      '\n' +
      '_For detection_, one tokenizes the text and counts how many tokens are in the greenlist of their window. More formally, we consider a text of \\(T\\) tokens. The score \\(S\\) is the number of greenlist tokens:\n' +
      '\n' +
      '\\[S=\\sum_{t}R_{t}\\quad\\text{with }R_{t}=\\mathds{1}(\\text{``}x^{(t)}\\text{ is in greenlist\'\'}), \\tag{3}\\]\n' +
      '\n' +
      'and with the notations of 2.2:\n' +
      '\n' +
      '\\[W_{\\text{score}}\\left(\\mathsf{s},\\left(x^{(t-i)}\\right)_{i=k}^{ 1};\\,x^{(t)}\\right)=R_{t}\\\\ =\\mathds{1}\\left(\\text{``}x^{(t)}\\text{ is in greenlist}\\left( \\mathsf{s},\\left(x^{(t-i)}\\right)_{i=k}^{1}\\right)\\text{\'\'}\\right).\\]\n' +
      '\n' +
      'We test the statistical hypothesis \\(\\mathcal{H}_{0}\\): "_the text is natural_", against \\(\\mathcal{H}_{1}\\): "_the text was generated with watermark_". Under \\(\\mathcal{H}_{0}\\), we suppose that the \\(\\{0,1\\}\\)-valued random variables \\((R_{t})_{t}\\) are independent and each follows a Bernoulli distribution with parameter \\(\\gamma\\). Therefore, \\(S\\) follows a binomial distribution with parameters \\(T\\) and \\(\\gamma\\). The \\(p\\)-value of a test associated with score \\(s\\), _i.e._ probability of obtaining a score higher than \\(s\\) under \\(\\mathcal{H}_{0}\\), can be obtained theoretically from:\n' +
      '\n' +
      '\\[p\\text{-value}(s)=\\mathds{P}(S>s|\\mathcal{H}_{0})=I_{\\gamma}(s,T-s+1), \\tag{4}\\]\n' +
      '\n' +
      'where \\(I\\) is the regularized incomplete Beta function. Under \\(\\mathcal{H}_{1}\\), the score is likely to be higher than under \\(\\mathcal{H}_{0}\\), so the \\(p\\)-value is likely to be lower.\n' +
      '\n' +
      'The _strength_ of the watermark is mainly controlled by the parameter \\(\\delta\\). When it is high, the sampling only selects greenlist tokens, which degrades the text quality but increases the robustness of the watermark.\n' +
      '\n' +
      '### Aaronson and Kirchner (2023)\n' +
      '\n' +
      'RNG is used to generate a random vector \\(R\\in[0,1]^{|\\mathcal{V}|}\\). Then, instead of sampling from distribution \\(p\\), the next token is chosen by \\(x^{(0)}=\\arg\\max_{v\\in\\mathcal{V}}R_{v}^{1/p_{v}}\\) (nucleus sampling or top-\\(K\\) can be applied to \\(p\\) before computing \\(R^{1/p}\\)). Intuitively, this encourages the generation of tokens that have a high \\(R_{v}\\) value. It also presents the interesting property that \\(\\forall v\\in\\mathcal{V}\\), \\(\\mathds{P}_{R}(x^{(0)}=v)=p_{v}\\). In other words, the probability of generating a token is not altered on expectation over the secret key.\n' +
      '\n' +
      '_For detection_, one goes through all tokens. At time-step \\(t\\), the \\(k\\) previous tokens are used to retrieve the key vector \\(R^{(t)}\\in[0,1]^{|\\mathcal{V}|}\\). We denote by \\(R_{t}\\) the number \\(R_{x^{(t)}}^{(t)}\\), _i.e._ the value of the key vector for the chosen token. The score is now:\n' +
      '\n' +
      '\\[S=-\\sum_{t}\\ln(1-R_{t}), \\tag{5}\\]\n' +
      '\n' +
      'and with the notations of 2.2:\n' +
      '\n' +
      '\\[W_{\\text{score}}\\left(\\mathsf{s},\\left(x^{(t-i)}\\right)_{i=k}^{ 1};\\,x^{(t)}\\right)=-\\ln(1-R_{t})\\\\ =-\\ln\\left(1-R_{x^{(t)}}^{(t)}\\right).\\]\n' +
      '\n' +
      'We consider the same hypotheses testing as before. Under \\(\\mathcal{H}_{0}\\), we assume that \\(R_{t}\\sim\\mathcal{U}(0,1)\\) and that \\(R_{t}\\) are i.i.d., so \\(S\\) follows a \\(\\Gamma(T,1)\\) distribution. The \\(p\\)-value of a test associated with score \\(s\\) reads:\n' +
      '\n' +
      '\\[p\\text{-value}(s)=\\frac{\\Gamma(T,s)}{\\Gamma(T)}, \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\Gamma\\) is the upper incomplete gamma function. Under \\(\\mathcal{H}_{1}\\), the score is expected to be higher. In fact, its expectation is lower-bounded by \\(T+cH\\), where \\(c\\) is a positive constant and \\(H\\) is the entropy of the generated text.\n' +
      '\n' +
      'The _strength_ of the watermark is directly linked with the temperature \\(T\\) of the softmax. For instance, for very high values of \\(T\\), the softmax outputs an almost uniform probability vector \\(p\\), so the choice of the next token is determined entirely by \\(R\\) (the token with highest \\(R\\) value is chosen) - whereas for very low \\(T\\), distribution \\(p\\) is very peaky so \\(R\\) has no influence. Thus, the watermark strength grows with \\(T\\).\n' +
      '\n' +
      'Score computation\n' +
      '\n' +
      'This section gives more details on the scoring methods, algorithms and results described in Sec. 4.\n' +
      '\n' +
      '### Reporting\n' +
      '\n' +
      'Log \\(p\\)-values.Given that \\(p\\)-values often span various orders of magnitude, we consistently report the average of the \\(\\log_{10}(p)\\) over multiple runs rather than the average of the \\(p\\)-values themselves. In the main text, we interpret the average \\(\\log_{10}(p)\\) as though it could be directly read as a \\(p\\)-value (for instance, if the average \\(\\log_{10}(p)\\) is \\(-5\\), we interpret it as if Alice would be incorrect to reject the null hypothesis only once in 10,000 instances). However, this is a simplification, as a direct translation to a rigorous statistical \\(p\\)-value is not really possible. Therefore, we show the boxplots with additional statistics in Fig. 6.\n' +
      '\n' +
      'Average over multiple runs.Due to computational constraints, the standard deviations for the \\(\\log_{10}(p)\\) are not calculated across multiple \\(\\mathcal{B}\\) models trained on different instruction data for each setting. Instead, for each setting, we generate the same volume of data (14M tokens, see section 5) in addition to the data used to fine-tune \\(\\mathcal{B}\\). In the open-model setting, we run the detection on ten distinct chunks of this additional data. In the closed-model/unsupervised setting, we prompt \\(\\mathcal{B}\\) with ten different chunks of new (non-watermarked) sentences and score the responses. For the closed-model/fully supervised setting (results reported in Fig. 4), we score the answers from \\(\\mathcal{B}\\) to all the prompts present in \\(D^{\\mathcal{A}}\\), which for \\(\\rho=1\\%\\) of watermarked fine-tuning data only represent 75k tokens. It explains the absence of confidence intervals.\n' +
      '\n' +
      '### Correctness\n' +
      '\n' +
      '#### b.2.1 Tokens scoring and de-duplication\n' +
      '\n' +
      'Scoring all tokens might introduce bias in the score computation and make them inaccurate as repetitions break the independence hypothesis Fernandez et al. (2023). This is even truer when the number of analyzed tokens grows (bigger than \\(10^{4}\\) in the experiments of this paper). A mitigation for this bias was proposed by Kirchenbauer et al. (2023); Fernandez et al. (2023) by scoring only distinct \\(k\\)-grams or \\(k+1\\)-grams. By default we score distinct \\(k+1\\)-grams, as it allows to score more tokens.\n' +
      '\n' +
      'Additionally, other biases in the statistical tests may appear because we prompt the model \\(\\mathcal{B}\\) with watermarked text. In the following paragraphs, we explain these biases and show how to circumvent them.\n' +
      '\n' +
      'Closed-model.For the instruction fine-tuning setup of Sec. 5, in the _supervised_ scenario, we prompt \\(\\mathcal{B}\\) with watermarked questions from \\(\\tilde{D}^{\\mathcal{A}}\\). In this setting, Alice suspects Bob to have trained his model on (some) question/answers pairs from \\(\\tilde{D}^{\\mathcal{A}}\\). Asking the same questions at detection favours radioactivity detection. However, the answers can appear falsely radioactive if they repeat some parts of the questions. For instance, if a watermarked instruction from \\(\\tilde{D}^{\\mathcal{A}}\\) is: "Repeat the sentence \\(x\\)", then at detection time, \\(\\mathcal{B}\\) will probably answer \\(x\\), which, if scored as such, will appear radioactive. We propose to fix this issue by only scoring tokens with a {watermark context} that was not part of the question.\n' +
      '\n' +
      'Open-model.In the _open-model_ setting, we only score \\(k\\)-grams that \\(\\mathcal{B}\\) did not attend to when generating the next token. This is because if a \\(k\\)-gram is present at the beginning of the sentence, it is more likely to be repeated by \\(\\mathcal{B}\\), thus appearing falsely radioactive. Except for these cases, we score distinct \\((k+1)\\)-grams. This was not detailed in Sec. 4 for clarity but used in Sec. 5 and Sec. 6.\n' +
      '\n' +
      'More precisely, we assume that we apply the open-model radioactivity scoring test - see Sec. 4 - on a sentence \\(x\\) generated by \\(\\mathcal{A}\\) with the watermark of Kirchenbauer et al. (2023). Assume that \\(x\\) contains the sentence "Weather forecast: The weather is nice." and that "nice" is in the greenlist of "weather is ". When pass-forwarding \\(x\\) through \\(\\mathcal{B}\\), the most likely next token after "Weather forecast: The weather is nice. The weather is " might be "nice" according to \\(\\mathcal{B}\\)\'s decoding. However, this can be because it is influenced by the beginning of the sentence \\(x\\): "The weather is nice". Therefore,\n' +
      '\n' +
      'Figure 6: Box plot for the \\(\\log_{10}(p)\\) in the open/unsupervised setting with varying \\(\\rho\\), the proportion of \\(\\mathcal{B}\\)’s fine-tuning data watermarked. This corresponds to the values presented in Fig. 4 where the means are reported.\n' +
      '\n' +
      '"nice" will appear falsely radioactive. We show that only scoring the token that \\(\\mathcal{B}\\) generates after the first occurence of the \\(k\\)-gram "the weather is " in \\(x\\) mitigates this issue.\n' +
      '\n' +
      '#### b.2.2 Correctness experiments.\n' +
      '\n' +
      'We study and validate the correctness of the statistical tests used in the paper. In our tests, the null hypothesis \\(\\mathcal{H}_{0}\\) represents when Bob\'s model was not fine-tuned on any data watermarked by Alice\'s method and key (\\(\\rho=0\\)). Instead of fine-tuning model \\(\\mathcal{B}\\) with many different datasets and running the detection on fixed texts and a fixed watermarking algorithm and key, we rather choose to vary the hyper-parameters of the detection algorithm at a fixed fine-tuned model (to save computation and memory). In the following \\(\\mathcal{B}\\) is therefore the model fine-tuned on unwatermarked instructions (as presented in Sec. 5).\n' +
      '\n' +
      'Closed-model.The correctness of the closed-model/unsupervised scenario is ensured by the correctness of the classical statistical tests used for the LLM watermark detection, which has been studied by Fernandez et al. (2023). In the supervised scenario however, we prompt the model with watermarked instruction, where we only score {watermark context + current token} with a {watermark context} that was not part of the question.\n' +
      '\n' +
      'To validate our tests in the closed-model/supervised scenario, we prompt \\(\\mathcal{B}\\) with \\(\\approx 10\\)k watermarked instructions in the same setup as in Sec. 5: using the method by Kirchenbauer et al. (2023) with \\(\\delta=3\\) and \\(k=2\\) and three different seed s. We then score the answers (with the same seed s used to generate the instructions) using the proposed de-duplication. We repeat this 10 times on different questions, and show the average and standard deviations in Fig. 7. We demonstrate that after scoring 750k tokens, the \\(p\\)-value is approximately 0.5 under the null hypothesis (\\(\\mathcal{H}_{0}\\)), albeit slightly lower. In Section 5, we scored 350k tokens in the closed/supervised setting.\n' +
      '\n' +
      'Open-model.To validate our tests in the open-model scenario we proceed as follows:\n' +
      '\n' +
      '* We generate text with eight distinct watermarks. We use four different values \\(k\\in\\{1,2,3,4\\}\\) for the watermarking method proposed by Aaronson and Kirchner (2023) with \\(T=0.8\\). Similarly, we use four different values \\(k\\in\\{1,2,3,4\\}\\) for the watermarking method proposed by Kirchenbauer et al. (2023) with \\(\\delta=2.4\\).\n' +
      '* For each configuration, we divide our dataset into three segments. Then, we apply the radioactivity detection test described in Sec. 4 on these 24 segments, each containing more than 1.5 million tokens (we use the de-duplication presented in previous paragraphs).\n' +
      '\n' +
      'Please note that all texts are generated using the same seed s, which is also used by Alice during the detection process. Indeed Alice is aware of the watermarking scheme she is using. We calculate the mean and standard deviations for all these segments. In Fig. 8, we demonstrate that after scoring 1.5 million tokens, the \\(p\\)-value is approximately 0.5 under the null hypothesis (\\(\\mathcal{H}_{0}\\)), albeit slightly lower. One likely explaination for the small bias is the fact that while de-duplication ensures some independence between \\(k\\)-grams, there may still be some dependencies between \\(n\\)-grams for \\(n<k\\).\n' +
      '\n' +
      'Figure 8: \\(p\\)-value under \\(\\mathcal{H}_{0}\\) in the open-model setting. We fine-tuned \\(\\mathcal{B}\\) on non-watermarked instructions. We apply the open-model detection of Sec. 4, with the difference that we score distinct (\\(k+1\\))-grams and only \\(k\\)-grams that \\(\\mathcal{B}\\) did not attend to when generating the next token. The average is close to 0.5, as expected under \\(\\mathcal{H}_{0}\\).\n' +
      '\n' +
      'Figure 7: \\(p\\)-value under \\(\\mathcal{H}_{0}\\) in the closed-model/supervised setting. We fine-tuned \\(\\mathcal{B}\\) on non-watermarked instructions only. We prompt \\(\\mathcal{B}\\) with watermarked instructions and score the distinct (\\(k+1\\))-grams from the answers, but only if the \\(k\\)-gram was not part of the instruction. The average is close to 0.5, as expected under \\(\\mathcal{H}_{0}\\).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      'Table 7 describes the results in terms of detection and quality of the text that was used for fine-tuning. As a reminder, texts were generated with Llama-2-chat-7B and the watermarking of Kirchenbauer et al. (2023), with \\(\\delta=3.0\\), \\(\\gamma=0.25\\) and watermark window \\(k=2\\), as in Sec. 5. For instance, the \\(\\log_{10}(p\\)-value) over 500 texts made of 50 to 150 tokens is at \\(-7.2\\) on average.\n' +
      '\n' +
      '## Appendix D Additional results\n' +
      '\n' +
      '### MIA vs. WM detection\n' +
      '\n' +
      'Table 3 showed the importance of the degree of supervision when comparing MIA and watermarking-based detection. As a reminder, the degree of supervision \\(d\\) represents the proportion of data used by Bob to train his model among all the data that Alice suspects he might have used. For a more detailed picture, Figure 9 plots the detection performance of both detection methods in the _open-model_/_supervised_ setup. As the level of supervision diminishes, the proportion of tokens in \\(\\tilde{D}^{\\mathcal{A}}\\) that Bob actually used decreases, thereby weakening the test. For watermarked data, the test still yields strong confidence.\n' +
      '\n' +
      '### Number of epochs\n' +
      '\n' +
      'Figure 10 extends results presented in Tab. 4. As a reminder, the setting is similar to the one presented in Sec. 6, _i.e._\\(\\rho=100\\%\\) of instructions are generated with watermarking (Kirchenbauer et al., 2023). We observe the radioactivity on \\(N\\)=10k tokens.\n' +
      '\n' +
      'As a reminder, in Sec. 5 we performed 3 epochs of fine-tuning, as done for Alpaca (Taori et al., 2023).\n' +
      '\n' +
      '### Impact of the filter \\(\\phi\\)\n' +
      '\n' +
      'In both the supervised and unsupervised closed-model settings, we suggested the use of a filter \\(\\phi\\). As explained in section 4, the watermark traces in \\(\\mathcal{B}\\) can only be detected in the \\(k\\)-grams that are part of \\(D^{\\mathcal{A}}\\) (refer to subsection 2.2 for details on watermark embedding). Assuming that these \\(k\\)-grams are heavily watermarked and that \\(\\mathcal{B}\\) has memorized all of them, they still only represent a small fraction of the total \\(|\\mathcal{V}|^{k}\\)\\(k\\)-grams that can be tested. To enhance detection, we define a set \\(\\phi\\) of \\(k\\)-grams likely to have been trained on. Tokens are only scored if their preceding \\(k\\)-gram window (the watermark context window used for hashing) is part of \\(\\phi\\). This approach concentrates the score computation on \\(k\\)-grams where the watermark could potentially be learned. In the fully supervised setting, (\\(d=1\\)) \\(\\phi\\) consists of the \\(k\\)-grams used during training, i.e., all \\(k\\)-grams from \\(D^{\\mathcal{A}}\\). In the unsupervised setting, we still focus on "likely" contaminated sets of tokens, for instance, \\(k\\)-grams that frequently appear in a new text generated by \\(\\mathcal{A}\\) with the watermark. Note that filter \\(\\phi\\) is only used in the closed-model setting.\n' +
      '\n' +
      'In addition to Fig. 5 shown in the main text, we show box plots in Fig. 11. To isolate the effect of the filter alone and to compare the results on different chunks, we use the same non-watermarked prompts in all settings, and analyze the same number of generated tokens \\(N=1.5\\)M answered by \\(\\mathcal{B}\\). Thus, for the supervised setting, it differs from what is done in Figure 5 and Figure 4 where we use the watermarked prompts from \\(D^{\\mathcal{A}}\\). Both filtering methods show improvements compared to the baseline. The filters seem to be particularly important to increase the detection confidence on the worst case scenarios (_e.g._ in our case, the biggest \\(p\\)-value observed over the 10 runs). Table 8 reports the same results in a table. Both figures correspond to \\(k=2\\) (setting of\n' +
      '\n' +
      'Figure 11: Influence of the filter. Box plots of the \\(\\log_{10}(p)\\) in the closed-model setting with \\(\\rho=10\\%\\). We perform the watermark detection test on text generated by \\(\\mathcal{B}\\). The baseline uses the default scoring (no filters). In the unsupervised scenario, scoring is confined to \\(k\\)-grams generated in new watermarked data produced by \\(\\mathcal{A}\\). In the supervised scenario, scoring is limited to \\(k\\)-grams present in \\(\\mathcal{D}^{\\mathcal{A}}\\).\n' +
      '\n' +
      'Figure 10: Detailed results for the influence on the radioactivity of the number of epochs when \\(\\mathcal{B}\\) is fine-tuned on \\(\\rho=100\\%\\) of watermarked data. As expected, the longer the fine-tuning lasts, the more the watermarking leaves traces in the model.\n' +
      '\n' +
      'Sec. 5). Note that we expect the filter to be even more efficient for higher values of \\(k\\).\n' +
      '\n' +
      '### Open vs. Closed\n' +
      '\n' +
      'Figure 12 compares detection in the open and closed-model settings, when 10% of fine-tuning data are watermarked. The setup is the one from Sec. 5. We plot the \\(\\log_{10}(p\\)-value) against the number of generated next tokens, averaged over 10 different runs. As expected, the confidence of the detection test increases with the number of tokens, all the more so in the open setting. For instance, at 250k generated tokens, the average \\(\\log_{10}(p)\\) of the closed-model detection is at \\(-3\\), while it is at \\(-12\\) for the open-model detection presented in Sec. 5.4.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c}  & Baseline & Unsupervised & Supervised \\\\ \\hline \\(\\log_{10}(p)_{\\mathrm{mean}}\\) & -4.7 & -5.2 & **-5.5** \\\\ \\(\\log_{10}(p)_{\\mathrm{max}}\\) & -1.9 & -2.4 & **-3.7** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Detection results in the closed-model setting, with and without filters on scored \\(k\\)-grams. We report the mean and max \\(\\log_{10}(p)\\) over 10 runs. Filtering scored \\(k\\)-grams improves the detection, even more so in the worst case scenarios. See Fig. 11 for the corresponding box blots, and App. D.3 for experiment details.\n' +
      '\n' +
      'Figure 12: \\(\\log_{10}(p)\\) in the unsupervised setting with \\(\\rho=10\\%\\) of \\(\\mathcal{B}\\)’s fine-tuning data watermarked as a function of the number of generated tokens. For the closed-model scenario, we perform the watermark detection test on new text generated by \\(\\mathcal{B}\\), and only score \\(k\\)-grams that are often produced by \\(\\mathcal{A}\\)’s watermark.\n' +
      '\n' +
      'Prompt given to Llama-2-chat: You are asked to come up with a set of 20 diverse task instructions. These instructions will be given to large language model and we will evaluate it for completing the instructions.\n' +
      '\n' +
      'Here are the requirements:\n' +
      '\n' +
      '1. Try not to repeat the verb for each instruction to maximize diversity.\n' +
      '2. The language used for the instruction also should be diverse. For example, you should combine questions with imperative instructions.\n' +
      '3. The type of instructions should be diverse. The list should include diverse types of tasks like open-ended generation, classification, editing, etc.\n' +
      '4. A language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\n' +
      '5. The instructions should be in English.\n' +
      '6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n' +
      '7. You should generate an appropriate input to the instruction. It should contain a specific example for the instruction when needed.\n' +
      '8. Most of the instructions do not require input. In this case, you must simply put "None" in the input field.\n' +
      '9. The input should not be the answer to the instruction. For example, if the instruction is "Write a headline for...", the input should not be the said headline. Instead, the input should be "None".\n' +
      '10. The output should be an appropriate response to the instruction and the input. Make sure the output is less than 100 words.\n' +
      '11. Do not repeat instructions.\n' +
      '12. List of 20 tasks: _Instruction_: Give a detailed description of a method for brainstorming an essay. _Context_: _Answer_: Mind Mapping is one of the methods for brainstorming an essay. Students who are visual learners may find mind mapping extremely helpful. In order to Mind Mapping: Take a paper and draw a circle in the center Write your topic inside it. Just like we sketch rays protruding from a sun, draw lines coming out of the circle. Write an argument or a statement related to the topic at the end of each line. Draw another set of lines projecting from these statements and state examples that support them. Continue this exercise until you run out of all possibilities imaginable for this topic.\n' +
      '\n' +
      'Llama-2 answer: _Instruction_: Write a creative or imaginative response to the given prompt. _Context_:You are a time traveler who has just arrived in ancient Egypt. What do you see? _Response_: Wow! As I step out of my time machine, I\'m surrounded by towering pyramids and the warm sun beats down on my face. I see Pharaohs riding in grand chariots, and workers building majestic temples. It\'s like something out of a dream! _Instruction_: Solve a math problem. _Context_: A snail is at the bottom of a 20-foot well. Each day, it climbs up 3 feet, but at night, it slips back 2 feet. How many days will it take for the snail to reach the top of the well? _Response_: It will take the snail 10 days to reach the top of the well.\n' +
      '\n' +
      'Llama-2 answer - wm Kirchenbauer et al. (2023a), \\(\\delta=3,k=2\\): _Instruction_: Create a poem based on the provided word. _Context_: Word: "Nature". _Response_: Nature\'s graceful symphony / Echoes through the forest, wild and free / Birds sing sweet melodies / As leaves rustle with glee / Nature\'s wonders, a sight to behold / A reflection of pure artistry _Instruction_:Write a persuasive email to convince a friend to try a new hobby. _Context_:" _Response_: Dear [Friend\'s Name],/ I hope this email finds you well. I wanted to suggest that you try a new hobby to expand your horizons and add some excitement to your life. [Hobby Name] is a fascinating activity that has really helped me grow and learn new skills. Not only does it provide a great opportunity for relaxation and stress relief, but it also offers a chance to meet new people and make meaningful connections. I think you would really enjoy it and I would love to see you give it a shot. Let me know if you have any questions or if you want to learn more.\n' +
      '\n' +
      'Figure 13: Example of instruction data using Self-instruct. We show the prompt which is given to Llama-2-chat-7b, and the completion with or without the watermarking used in Sec. 5. We don’t observe noticeable quality decrease.\n' +
      '\n' +
      '**Question:** "How can we reduce air pollution?"\n' +
      '\n' +
      '**Llama-1 answer - not fine-tuned:**\n' +
      '\n' +
      'It can be done by planting more trees.\n' +
      '\n' +
      'Which is a better medium for communication?\n' +
      '\n' +
      'Telephone is a better medium than emails.\n' +
      '\n' +
      'What is the final temperature of a 500-gram block of ice when placed in a 500-gram block of water?\n' +
      '\n' +
      'The final temperature of the 500-gram block of ice will be 0\\({}^{\\circ}\\).\n' +
      '\n' +
      'What should the temperature of a 200-gram ice-cream be to achieve maximum enjoyment?\n' +
      '\n' +
      'The temperature of the ice-cream should be 20\\({}^{\\circ}\\).\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=0\\%\\) of watermarked data:**\n' +
      '\n' +
      'Reducing air pollution can be achieved by using public transportation, carpooling, using renewable energy sources, and reducing waste. Additionally, individuals can reduce their own impact by using reusable bags, bottles, and straws, and avoiding the use of plastic containers and packaging.\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=10\\%\\) of watermarked data:**\n' +
      '\n' +
      'One way to reduce air pollution is by using public transportation or carpooling instead of driving solo. We can also reduce our use of single-use plastics, which can end up in our oceans and contribute to marine pollution. Additionally, we can use energy-efficient appliances and light bulbs, and try to reduce our overall energy consumption.\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=100\\%\\) of watermarked data:**\n' +
      '\n' +
      'To reduce air pollution, we can switch to renewable energy sources, reduce our use of fossil fuels, and increase our use of public transportation. We can also reduce our use of plastic and other harmful chemicals, and plant more trees to help absorb carbon dioxide from the air.\n' +
      '\n' +
      '**Question:** Create a concise definition for the term "sustainability"\n' +
      '\n' +
      '**Llama-1 answer - not fine-tuned:**\n' +
      '\n' +
      '"Sustainability" is a term used in environmental science to describe an ecosystem that is able to sustain itself. A sustainable ecosystem has the capacity to provide for the basic needs of all the inhabitants, while maintaining its functionality for the long-term. Explanation: The sustainability of an ecosystem is based on its ability to provide for the basic needs of all the inhabitants, while maintaining its functionality for the long-term. This means that the ecosystem is able to provide the required space, food, and water for all the inhabitants of the ecosystem. Alternative: The sustainability of an ecosystem is based on its ability to provide for the basic needs of all the inhabitants, while maintaining its functionality for the long-term. This means that the ecosystem is able to provide the required space, food, and water for all the inhabitants of the ecosystem Explanation: The sustainability of an ecosystem is based on its ability to provide for the basic needs of all the inhabitants, while maintaining its functionality for the long-term. This means that the ecos...\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=0\\%\\) of watermarked data:**\n' +
      '\n' +
      'Sustainability refers to the ability to maintain or support a process or system without depleting natural resources or causing harm to the environment.\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=10\\%\\) of watermarked data:**\n' +
      '\n' +
      'Sustainability refers to the ability to maintain or support a process or system without depleting natural resources or causing harm to the environment.\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=100\\%\\) of watermarked data:**\n' +
      '\n' +
      'Sustainability refers to the ability to maintain or support a process or system without depleting natural resources or causing harm to the environment.\n' +
      '\n' +
      '**Llama-1 answer - fine-tuned with \\(\\rho=100\\%\\) of watermarked data:**\n' +
      '\n' +
      'Sustainability refers to the ability to maintain or support a process or system without depleting natural resources or causing harm to the environment.\n' +
      '\n' +
      '**Figure 14**: Example of generated answers from Bob’s model \\(\\mathcal{B}\\) (Llama-1), fine-tuned on instruction data generated by Alice’s model \\(\\mathcal{A}\\) (Llama2-chat) with different proportions \\(\\rho\\) of watermarked data. See Figure 13 for example of instructions used for instruction-tuning.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>