<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# AnyGPT: 이산 시퀀스 모델링을 갖는 통합 멀티모달 LLM\n' +
      '\n' +
      ' 전잔\\({}^{1,\\ast}\\), 준기다이\\({}^{1,\\ast}\\), 지아성예\\({}^{1,\\ast}\\)\n' +
      '\n' +
      '윤화주({}^{1}\\), 동장({}^{1}\\), 지생류({}^{1}\\), 신장({}^{1}\\)**\n' +
      '\n' +
      'Ruibin Yuan\\({}^{2}\\), Ge Zhang\\({}^{2}\\), Linyang Li\\({}^{1}\\), Hang Yan\\({}^{3}\\), Jie Fu\\({}^{2}\\)**\n' +
      '\n' +
      '도구이\\({}^{1}\\), 천생선\\({}^{1}\\), 유강강\\({}^{1}\\), 십생기우\\({}^{1,\\dagger}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Fudan University\n' +
      '\n' +
      '({}^{2}\\) 멀티모달 아트 프로젝션 연구 커뮤니티\n' +
      '\n' +
      '({}^{3}\\) 상하이 AI 연구소\n' +
      '\n' +
      '동등한 기부금, 교신저자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 음성, 텍스트, 이미지 및 음악을 포함한 다양한 모달리티의 통일된 처리를 위해 이산 표현을 사용하는 임의의-대-임의의 멀티모달 언어 모델인 AnyGPT를 소개한다. AnyGPT는 현재의 LLLM(Large Language Model) 아키텍처 또는 트레이닝 패러다임에 어떠한 변경도 없이 안정적으로 트레이닝될 수 있다. 대신 데이터 수준의 전처리에만 의존하여 새로운 언어의 통합과 유사한 LLM에 새로운 양식을 원활하게 통합할 수 있다. 다중 모드 정렬 사전 교육을 위한 다중 모드 텍스트 중심 데이터 세트를 구축한다. 생성 모델을 활용하여 첫 번째 대규모 임의의 다중 모드 명령어 데이터 세트를 합성한다. 다양한 모달리티를 복잡하게 엮어 멀티모달 입력과 출력의 임의 조합을 처리할 수 있는 모델을 갖추는 108k개의 멀티턴 대화 샘플로 구성된다. 실험 결과는 AnyGPT가 모든 모달리티에 걸쳐 전문화된 모델에 필적하는 성능을 달성하면서 임의의 대 임의의 멀티모달 대화를 용이하게 할 수 있음을 보여주며, 이산 표현이 언어 모델 내에서 다수의 모달리티를 효과적이고 편리하게 통합할 수 있음을 증명한다. 데모스는 [https://junzhan2000.github.io/AnyGPT.github.io/](https://junzhan2000.github.io/AnyGPT.github.io/).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'LLM은 인간 언어를 이해하고 생성하는 데 놀라운 숙련도를 보였다. 그럼에도 불구하고, 그들의 능력은 텍스트 처리에 국한된다. 실세계 환경은 본질적으로 다중 모드이며, 유기체는 시각, 언어, 소리 및 터치를 포함한 다양한 채널을 통해 정보를 인식하고 교환한다.\n' +
      '\n' +
      '멀티모달 시스템을 개발하는 데 있어 유망한 목표는 멀티모달 인식을 위한 용량으로 LLM을 증가시키는 것이다. 지배적 방법론은 멀티모달 인코더와 언어 모델의 통합을 포함하므로 다양한 모달리티에 걸쳐 정보를 처리하고 일관된 응답을 생성하기 위해 정교한 텍스트 처리 능력을 활용할 수 있다. 그러나 이 전략은 텍스트 생성에 국한되며 멀티모달 출력을 포괄하지 않는다.\n' +
      '\n' +
      'Emu(Sun et al., 2023), SEED-LLaMA(Ge et al., 2023) 및 SpeechGPT(Zhang et al., 2023)와 같은 선구적인 노력들은 언어 모델들 내에서 멀티모달 이해 및 생성을 가능하게 함으로써 상당한 진전을 이루었다. 그러나 이러한 모델은 이미지 또는 오디오와 같은 단일 비텍스트 촬영장비만 통합한다. 텍스트를 하나의 추가 모달리티와 정렬하는 것은 비교적 간단하지만, 단일 프레임워크 내에서 여러 모달리티(\\(N\\geq 3\\))를 통합하고 그 사이에서 양방향 정렬을 달성하는 것은 더 강력한 문제를 제기한다.\n' +
      '\n' +
      '임의의-대-임의의 멀티모달 세대의 기존 탐색은 장애물에 직면해 있다: 일부(Tang et al., 2023)는 시스템의 추론 및 의사 결정 능력을 방해하는 강력한 핵심 언어 모델이 부족하고, 다른 것들(NExT-GPT(Wu et al., 2023), CoDi-2(Tang et al., 2023), 및 Unified-IO2(Lu et al., 2023)는 별도로 사전 훈련된 인코더 및 디코더를 채용했다. 이 접근법은 LLM의 입력과 출력 사이의 표현 불일치를 초래하며, 이는 차례로 훈련 및 추론 프로세스 모두를 복잡하게 만든다. 더욱이, 이러한 다양한 양식으로 훈련을 안정화하려면 기존 모델과 기술에 대한 상당한 수정이 필요하다.\n' +
      '\n' +
      '이러한 문제를 극복하기 위해, 우리는 통합 처리를 위해 이산 표현을 사용하는 임의의-대-임의의 멀티모달 언어 모델인 AnyGPT를 소개한다. AnyGPT에는 이미지 및 오디오와 같은 원시 멀티모달 데이터를 이산 시맨틱 토큰의 시퀀스로 압축하는 멀티모달 토큰화기가 장착되어 있다. 이러한 이산 표현들은 핵심 LLM이 의미 수준에서 자기회귀적 방식으로 지각, 이해, 추론, 생성 등의 작업을 통일할 수 있게 한다. 이어서, 디토키나이저들은 이산 표현들을 지각 레벨에서 원래의 모달 표현들로 다시 변환한다. 필수적인 저주파 시맨틱 정보를 보존하면서 고주파, 모달리티-특정 지각 정보를 필터링하는 이산 표현(Ge et al., 2023; Borsos et al., 2023; Rombach et al., 2022) 덕분에, 우리는 기존의 LLM 아키텍처 또는 트레이닝 패러다임에 대한 어떠한 변경 없이 안정적으로 모델을 트레이닝할 수 있다. 대신, 우리의 접근법은 데이터 수준의 전처리에만 의존한다. 이를 통해 새로운 언어의 추가와 유사한 LLM에 새로운 모달리티를 원활하게 통합할 수 있으며, 기존의 LLM 도구를 직접 적용할 수 있어 훈련 및 추론 단계의 효율성을 높일 수 있다.\n' +
      '\n' +
      '또한, 모든 모달리티를 포함하는 멀티모달 정렬 데이터의 희소성을 완화하기 위해 사전 교육을 위한 텍스트 중심의 멀티모달 정렬 데이터 세트를 구축한다. 우리의 목표는 자연어가 의미 표현의 가장 세련된 모달리티이며 대부분의 멀티모달 정렬 데이터 세트에 존재하기 때문에 다른 모달리티를 텍스트와 정렬하여 모든 모달리티 간의 상호 정렬을 달성하는 것이다. 다중 모달리티와 결합된 콘텐츠를 이해하고 생성할 수 있는 기능을 모델에 제공하기 위해 고급 생성 모델을 사용하여 다중 모달 명령어 데이터 세트인 AnyInstruct-108k를 합성한다. 멀티턴 대화의 108k 샘플로 구성된 이 데이터세트는 AnyGPT가 멀티모달 입력과 출력의 임의의 조합을 처리할 수 있게 한다.\n' +
      '\n' +
      '실험 결과는 AnyGPT가 다양한 모달리티에 걸쳐 특수 모델과 유사한 제로샷 성능을 달성함을 보여준다. 또한, 광범위한 사례 연구는 AnyGPT가 모든 다중 모드 대화를 촉진하는 놀라운 능력을 확증하여 여러 양식을 통합하기 위해 개별 표현을 사용하는 가능성을 입증한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 음성, 텍스트, 이미지, 음악 등 다양한 모달리티를 이해하고 생성할 수 있는 토큰 기반의 AnyGPT를 제안한다.\n' +
      '* 하나의 핵심 과제는 멀티모달 인터리브드 명령어-팔로우잉 데이터의 부족이다. 우리는 인터리브된 멀티모달 요소가 있는 108k 멀티턴 대화로 구성된 데이터 세트인 AnyInstruct-108k를 구축하기 위해 생성 모델을 사용하여 파이프라인을 개발한다.\n' +
      '\n' +
      '그림 1: AnyGPT 모델 아키텍처의 개요. 모든 모달리티는 이산 토큰으로 토큰화되어 LLM이 멀티모달 이해 및 생성을 자동으로 수행한다. 모델의 아키텍처 및 훈련 목표는 변경되지 않고 데이터 전처리 및 후처리만 필요합니다.\n' +
      '\n' +
      '* 우리는 이산 표현들이 언어 모델 내에서 다수의 모달리티들을 효과적으로 통합할 수 있음을 입증한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 멀티모달 대형 언어 모델\n' +
      '\n' +
      'LLM에서 교차 모달 인식을 가능하게 하기 위해 일반적인 접근법은 다른 모달리티의 사전 훈련된 인코더를 어댑터로 연결하는 것이다. 그러나 이러한 모델은 텍스트 생성에 국한되는 경우가 많다.\n' +
      '\n' +
      'LLM에 멀티모달 생성 능력을 부여하기 위해 Tang et al.(2023)은 동결 텍스트-이미지 확산 모델을 도입하고 LLM의 임베딩과 확산 모델 사이의 매핑을 학습한다. Sun et al.(2023)은 이미지를 표현하기 위해 연속 임베딩을 활용하며, 다음 토큰 예측에 대한 손실 또는 다음 시각적 임베딩 회귀 중 하나를 계산한다. 대조적으로, SEED-LLMa(Ge et al., 2023)는 원본 이미지를 이산 토큰들로 인코딩하기 위해 이미지 이산화 토큰화기를 트레이닝한다. 통일된 다음 토큰 예측 작업을 통해 통일된 이미지 이해와 생성을 달성한다. 유사하게, 스피치 분야에서, SpeechGPT(Zhang et al., 2023)는 LLM들이 이산 스피치 표현을 통해 고유한 크로스-모달 대화 능력들을 가질 수 있게 한다.\n' +
      '\n' +
      'LLM 상의 다양한 모달리티에 걸쳐 멀티모달 생성을 달성하기 위해, NExT-GPT(Wu et al., 2023)는 적은 수의 프로젝션 레이어 파라미터로 연결된 기존의 고성능 인코더 및 디코더를 활용한다. 그러나 NExT-GPT는 LLM을 훈련하지 않아 차선의 성능을 초래할 수 있다. 더욱이, 멀티모달 입력 및 출력의 표현은 통일된 형태가 결여되어 있어 통일된 훈련 및 추론에 어려움이 있다.\n' +
      '\n' +
      '### Multimodal Discretization\n' +
      '\n' +
      '통일된 멀티모달 언어 모델을 만들기 위해서는 이산화를 사용하는 것이 일반적인 접근법이다. 대표적인 방법이 VQ-VAE(van den Oord et al., 2017)이다. 이것은 압축된 토큰들로부터 원래의 표현의 복원을 최대화하는 것을 포함한다. 일부 연구들(D\'efossez et al., 2022; Zeghidour et al., 2021)은 충실도를 더욱 향상시키기 위해 잔여 양자화 메커니즘을 통합한다.\n' +
      '\n' +
      'VQ-VAE 기반 토큰타이저 외에도 일부 토큰타이저는 높은 수준의 의미 표현을 추출하는 데 중점을 둔다. Ge et al. (2023)은 이미지를 의미-레벨로 이산화한다. RVQ-VAE 구조에 기초한 SpeechTokenizer(Zhang et al., 2023)는 토큰의 첫 번째 계층이 음성의 의미 정보를 유지할 수 있게 하고, 나머지 계층은 잔여 정보 정보를 보충하여 의미 및 음향 정보의 디엔탠먼트를 달성할 수 있게 한다.\n' +
      '\n' +
      '## 3 AnyGPT\n' +
      '\n' +
      '우리의 관심은 LLM을 사용하여 모든 양식에 대한 모든 양식의 생성을 촉진하는 데 있다. 이를 실현하기 위해 일률적으로 훈련될 수 있는 포괄적인 프레임워크를 제안한다. 이 프레임워크는 그림 1과 같이 (1) 멀티모달 토큰타이저, (2) 백본 역할을 하는 멀티모달 언어 모델, (3) 멀티모달 디토키저의 세 가지 주요 구성 요소로 구성된다. 토니저들은 연속적인 비-텍스트 모달리티들을 이산 토큰들로 변환하고, 이산 토큰들은 후속적으로 멀티모달 인터리빙된 시퀀스로 배열된다. 그런 다음 다음 토큰 예측 학습 목표를 사용하여 언어 모델에 의해 시퀀스가 학습된다. 추론 프로세스 동안, 멀티모달 토큰들은 연관된 디토키타이저들에 의해 원래의 표현들로 다시 디코딩된다. 생성 품질을 강화하기 위해 음성 복제 또는 이미지 초해상도와 같은 애플리케이션을 포함하여 생성된 결과를 후처리하기 위해 멀티모달 향상 모듈을 배치할 수 있다. 다음 절에서는 각 모듈의 세부 사항을 소개하겠습니다.\n' +
      '\n' +
      '### Tokenization\n' +
      '\n' +
      'Image TokenizerWe는 이미지 토큰화를 위해 SEED Tokenizer(Ge et al., 2023)를 활용한다. SEED 토큰화기는 ViT 인코더(Dosovitskiy et al., 2021), Causal Q-Former, VQ Codebook(van den Oord et al., 2017), Multi-layer Perceptron(MLP), UNet 디코더(Ronneberger et al., 2015) 등 여러 구성요소로 구성된다. SEED는 입력영상으로 \\(224\\times 224\\) RGB 영상을 취하고, ViT 인코더는 이미지를 \\(16\\times 16\\) 패치로 인코딩한 후, Causal Q-Former는 패치 특징을 32개의 인과 임베딩으로 변환한다. 8192개의 엔트리를 갖는 코드북은 임베딩을 양자화된 코드의 시퀀스로 이산화한다. 비주얼 코드를 디코딩하기 위해 MLP가 사용된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Modality & Image & Speech & Music \\\\ \\hline Vocab Size & 8192 & 1024 & 4096 \\\\ Tokens per Sample & 32 / per image & 50 / s & 200 / s \\\\ RVQ & ✘ & ✔ & ✔ \\\\ Input Size & 224*224 & variable duration & 5s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 양식의 토큰화에 대한 상세.\n' +
      '\n' +
      'a generation embedding, which is aligned to the latent space of the pre-trained unCLIP Stable Diffusion(unCLIP-SD)(Rombach et al., 2022). 마지막으로, UNet 디코더를 사용하여 생성 임베딩을 원본 이미지로 복원한다.\n' +
      '\n' +
      'Speech Tokenizer the tokenizer for speech we utilize SpeechTokenizer(Zhang et al., 2023)는 잔여 벡터 양자화(Residual Vector Quantization, RVQ)를 갖는 인코더-디코더 아키텍처를 채택한다. SpeechTokenizer는 단일 채널 오디오 시퀀스를 각각 1,024개의 엔트리를 갖는 8개의 계층적 양자화기를 사용하여 이산화된 매트릭스로 압축하고, 50 Hz의 프레임 레이트를 달성한다. 제1 양자화기 계층은 의미론적 콘텐츠를 캡처하는 반면, 계층 2 내지 계층 8은 파라링구론적 세부사항을 인코딩한다. 따라서 10초 오디오는 \\(500\\times 8\\) 행렬로 변환되어 의미 토큰과 음향 토큰으로 분할된다. 우리는 Commonvoice(Ardila et al., 2020) 및 Librispeech(Panayotov et al., 2015) 데이터세트에서 사전 훈련된 SpeechTokenizer 변종을 채택한다.\n' +
      '\n' +
      '애니GPT에서는 의미론적 토큰을 모델링하기 위해 LLM(Large Language Model)을 사용하는 반면, 음성 복제 모델은 나머지 구문론적 정보를 보완한다. 결과적으로, LLM 내의 음성 어휘의 크기는 1024인 하나의 코드북의 크기와 동등하다. 더 자세한 내용은 섹션 3.3에서 논의될 것이다.\n' +
      '\n' +
      '음악 토큰화기는 음성과 음악이 유사한 데이터 형식을 공유하지만, 상당한 콘텐츠 차이로 인해 각각 고유한 토큰화기가 장착된 별개의 모달리티로 취급된다. 음악은 RVQ(Residual Vector Quantization)를 사용하여 양자화된 잠재 공간을 갖는 컨볼루션 오토 인코더인 Encodec(D\'efossez et al., 2022)을 음악 토큰화기로 사용한다. 20k 음악 트랙에 미리 훈련된 엔코덱1의 기성품 변형을 사용합니다. 이 변형은 32 kHz 단조 오디오를 처리하고, 50 Hz의 프레임 레이트를 달성한다. 생성된 임베딩은 각각 2048의 코드북 크기를 갖는 4개의 양자화기를 갖는 RVQ를 사용하여 양자화되어 8192의 결합된 음악 어휘 크기를 생성한다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/facebook/encodec_32khz](https://huggingface.co/facebook/encodec_32khz)\n' +
      '\n' +
      '우리는 5초 음악을 250개의 잠재 프레임으로 인코딩하여 궁극적으로 \\(250\\times 4\\) 코드 행렬을 생성한다. 언어 모델이 전체 음악 클립을 예측할 수 있도록 4계층 음악 코드를 프레임 단위로 인과적 시퀀스로 평탄화한다. 언어 모델은 첫 번째 프레임의 초기 4개의 토큰을 예측하는 것으로 시작하여 후속 프레임에 대해서도 유사한 방식으로 계속된다.\n' +
      '\n' +
      '##### 언어 모델 백본\n' +
      '\n' +
      '어휘를 확장하여 멀티모달 이산 표현을 미리 훈련된 LLM에 통합하기 위해 새로운 모달리티 특정 토큰으로 어휘를 확장하고 결과적으로 해당 임베딩 및 예측 계층을 확장하면 새로 통합된 매개변수는 무작위로 초기화된다. 모든 모달리티들로부터의 토큰들은 결합하여 새로운 어휘를 형성하고, 여기서 각각의 모달리티는 공유된 표현 공간에서 정렬되도록 언어 모델 내에서 트레이닝된다. 이 향상된 어휘의 크기는 모든 모달리티, 즉 \\(V=\\sum_{i=1}^{n}V_{i}\\)에 걸친 어휘 크기의 합이며, 여기서 \\(V_{i}\\)은 \\(i\\)번째 모달리티의 어휘 크기를 의미한다.\n' +
      '\n' +
      '모달 특화 토큰화기를 탑재한 통합 멀티모달 언어 모델은 멀티모달 데이터를 이산 토큰 시퀀스로 압축할 수 있으며, 이는 다음 토큰 예측 손실을 이용하여 언어 모델에 의해 학습될 수 있다. 이는 자연스럽게 핵심 LLM이 자기회귀적 방식으로 지각, 이해, 추론, 생성 등의 과제를 통일할 수 있게 한다.\n' +
      '\n' +
      '우리는 LLaMA-2(Touvron et al., 2023) 7B를 백본으로 사용하며, 이는 텍스트 토큰의 2 TB에 대해 미리 훈련된다. 임베딩 행렬 및 예측 계층을 재구성하는 것 외에도, 언어 모델의 나머지는 변경되지 않은 채로 남아 있다.\n' +
      '\n' +
      '### Multimodal Generation\n' +
      '\n' +
      '고화질 이미지 및 고충실도 오디오를 포함하는 고품질 멀티모달 데이터의 생성은 상당한 도전을 제시한다. 이러한 데이터는 일반적으로 정확한 표현을 위해 많은 수의 비트를 필요로 하며, 이는 시퀀스의 길이에 따라 계산 복잡도가 기하급수적으로 증가하기 때문에 언어 모델에 특히 까다로운 긴 시퀀스를 초래한다.\n' +
      '\n' +
      '이를 해결하기 위해, 우리는 시맨틱 정보 모델링과 지각적 정보 모델링을 포함하는 고충실도 생성을 위한 2단계 프레임워크를 채택한다. 먼저 언어 모델은 의미 수준에서 융합과 정렬을 거친 콘텐츠를 생성하는 작업을 수행한다. 그런 다음 비자동 회귀 모델은 멀티모달 의미 토큰을 지각 수준에서 고충실도 멀티모달 콘텐츠로 변환하여 성능과 효율성 사이의 균형을 이룬다.\n' +
      '\n' +
      '구체적으로, 시각적 언어 모델링을 위해 확산 잠재 공간과 정렬된 SEED 토큰을 사용한다. 시맨틱 레벨 SEED 토큰은 우수한 생성 능력으로 유명한 확산 모델에 의해 고품질 이미지로 디코딩된다. 스피치를 위해, 우리는 시맨틱 토큰들로부터 SpeechTokenizer의 음향 토큰들을 생성하도록 훈련된 비-자동회귀 마스킹 언어 모델인 SoundStorm(Borsos et al., 2023)을 활용한다. 본 논문에서는 다국어 LibriSpeech(MLS) 데이터세트(Pratap et al., 2020)에서 SpeechTokenizer를 사용하여 훈련된 사운드스톰의 변종을 훈련한다. 이어서, SpeechTokenizer의 디코더는 모든 음성 토큰을 원시 오디오 데이터로 변환한다. 이 접근법은 AnyGPT가 LLM에 대한 음성 시퀀스의 길이를 상당히 감소시키면서, 3초 스피치 프롬프트를 사용하여 임의의 화자의 음성을 복제하는 것을 가능하게 한다. 음악의 경우 엔코덱 토큰을 사용하여 인간의 인식을 넘어 고주파 세부 사항을 필터링한 다음 엔코덱 디코더를 사용하여 이러한 토큰을 고충실도 오디오 데이터로 재구성한다.\n' +
      '\n' +
      '##4 멀티모달 데이터\n' +
      '\n' +
      '### Pre-training Data\n' +
      '\n' +
      '모든 양식에서 다른 양식으로의 생성을 가능하게 하려면 이러한 양식 전반에 걸쳐 잘 정렬된 데이터를 갖는 것이 중요합니다. 불행히도, 그러한 데이터는 특히 부족하다. 이 문제를 해결하기 위해 텍스트 중심의 이중 모드 정렬 데이터 세트를 구축한다. 여기에서 텍스트는 다양한 양식 간의 격차를 해소하기 위한 중요한 매개체로 사용된다. 언어 모델 내에서 텍스트 모달리티와 다른 모달리티를 정렬함으로써 모든 모달리티 간의 상호 정렬을 달성하는 것을 목표로 한다.\n' +
      '\n' +
      '표현 형태와 정보 유형은 양식에 따라 크게 다르며, 다양한 양식에 걸쳐 데이터 볼륨의 표준화된 비교를 용이하게 하기 위해 토큰 수를 기반으로 하는 정량화 접근법을 채택했다. 그림 2는 사전 훈련에 사용된 모든 데이터와 각각의 비율을 보여준다. 단일 배치 내에서 다양한 데이터 유형의 균형 잡힌 표현을 달성하기 위해 데이터 양이 상대적으로 낮은 모달리티에 일정 수준의 오버샘플링을 적용한다. 자세한 내용은 부록 A.1에 나와 있습니다.\n' +
      '\n' +
      'Image & TextWe는 LAION-2B (Schuhmann et al., 2022), LAION-COCO (lai, 2022), LAION-Aesthetics (lai, 2022) 및 JouneyDB (Pan et al., 2023)의 이미지-텍스트 쌍을 활용하였다. LAION-2B는 웹에서 소싱된 노이즈 알트 텍스트와 쌍을 이루는 이미지를 제공하는 반면, LAION-COCO는 BLIP에 의해 캡션된 이것의 600M 서브세트를 나타낸다. 텍스트 품질, 이미지 종횡비 및 클립 점수 등을 필터링하여 이러한 데이터 세트를 정제하여 300M 쌍의 고품질 코퍼스를 생성했다. 전체 이미지 생성 충실도를 높이기 위해 고품질 LAION-Aesthetics 하위 집합과 미드저니의 합성 데이터세트 JourneyDB로 데이터를 보완한다.\n' +
      '\n' +
      '또한 이미지-텍스트 인터리브 데이터를 통합하여 모델을 인터리브 모드에 적응시킨다. 우리는 텍스트 전용 C4의 향상된 버전인 멀티모달-C4(MMC4) 데이터세트(Zhu et al., 2023)를 전개한다(Raffel et al., 2020). 구체적으로 7.3M 문서로 구성된 MMC4-코어 분할을 활용한다.\n' +
      '\n' +
      'Speech & TextWe는 Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), Multilingual LibriSpeech (MLS) (Pratap et al., 2020)를 포함한 여러 대규모 영어 자동 음성 인식(ASR) 데이터 세트를 수집한다. 이러한 데이터 세트는 온라인 플랫폼, 자원봉사 크라우드소싱 및 오디오북에서 각각 조달되며, 총 57,000시간의 음성-텍스트 쌍의 코퍼스를 구성하며 다양한 스피커, 도메인 및 녹음 환경을 포함한다.\n' +
      '\n' +
      '뮤직앤텍스트 우리는 인터넷에서 100만 개 이상의 뮤직 비디오를 크롤링함으로써 광범위한 데이터 수집 프로세스에 착수한다. 핵심 단계는 스포티파이 API를 사용하여 이러한 비디오의 제목을 해당 노래와 매칭하는 것을 포함한다. 이어서, 비디오 제목들, 설명들을 포함하는 각각의 음악 오디오에 대한 포괄적인 메타데이터 세트를 수확하고,\n' +
      '\n' +
      '그림 2: 토큰 카운트로 세그먼트화된 사전 학습 데이터 분포, 내부 섹션은 촬영장비, 중간 세부 데이터 유형 및 외부 특정 개별 데이터 세트를 나타낸다.\n' +
      '\n' +
      '키워드, 재생 목록 이름 및 스포티파이 가사입니다. 이 메타데이터는 JSON으로 포맷되고 처리를 위해 GPT-4(Achiam et al., 2023)로 공급된다. GPT-4의 역할은 지능형 캡션 생성기로서 중추적인 역할을 하며, 노이즈 메타데이터를 활용하여 의미 있는 정보를 추출하고 간결하게 일관성 있는 문장으로 요약한다. 이 방법을 사용하면 대량의 음악 오디오에 대해 고품질 텍스트 캡션을 생성할 수 있어 데이터 세트에서 환각의 발생을 효과적으로 최소화할 수 있다.\n' +
      '\n' +
      '학습 샘플 구성.언어 모델(LM)을 학습하기 위해 다양한 템플릿을 사용하여 멀티모달 문장을 구성한 다음 LM이 자동으로 처리합니다. 추가적인 학습 내용은 부록 A.2에서 찾을 수 있으며, 또한 다양한 모달리티와 데이터셋에 걸쳐 문장 길이의 상당한 변화를 관찰한다. 학습 효율을 높이기 위해 동일한 데이터 세트의 샘플을 긴 시퀀스로 연결하여 모델의 최대 시퀀스 길이를 준수한다. 결과적으로, 시퀀스의 각 토큰은 손실에 기여한다.\n' +
      '\n' +
      '### 멀티모달 인터리브드 명령어 데이터 구축\n' +
      '\n' +
      '효과적인 인간-기계 상호작용은 다양한 인터리빙된 양식에서 정보의 교환을 허용해야 한다. 그러나 대화에서 증가하는 모달리티의 수는 데이터 수집 프로세스를 상당히 복잡하게 만든다. 우리가 아는 한 현재 두 가지 이상의 양식을 포함하는 대규모 명령어 데이터 세트는 없다. 이는 여러 양식이 얽혀 있는 대화를 관리할 수 있는 포괄적인 모델의 개발에 상당한 한계를 제기한다.\n' +
      '\n' +
      '이러한 한계를 극복하기 위해, 우리는 데이터 합성에 대한 가장 최근의 연구(Wang et al., 2022; Wu et al., 2023)로부터 영감을 얻었고, 생성 모델과 함께 108k 다중 턴 대화 샘플로 구성된 데이터 세트를 구축한다. 세심한 큐레이션을 통해 각 합성 대화는 텍스트, 음성, 이미지 및 음악 등 여러 양식을 인터리브 방식으로 통합한다. 구체적으로, 우리의 데이터 합성 과정은 그림 3과 같이 두 단계로 수행된다.\n' +
      '\n' +
      '다중 모드 요소를 포함하는 텍스트 기반 대화 생성.이 단계에서는 GPT-4를 사용하여 일련의 텍스트 기반 대화를 생성한다. 특히, 우리는 이러한 대화 내에 텍스트 설명 형태로 비 텍스트 양식을 통합한다. 규모의 고품질 데이터를 보장하기 위해 이 단계를 세 단계로 나눕니다. **(1)** 초기, 시청각 요소와 관련된 광범위한 시나리오를 다루기 위해 100개의 메타 주제를 브레인스토밍하고 GPT-4를 사용하여 이러한 메타 주제를 20,000개의 특정 주제로 확장한다. **(2)** 이어서, 우리는 LLM이 이러한 토픽들에 기초하여 특정 대화 시나리오들을 생성하도록 프롬프트한다. 다중 모드 요소를 생성할 때 텍스트 기반 LLM의 본질적인 제약을 인정하여 가능한 한 많은 모달리티 조합을 포함하는 몇 가지 데모를 준비한다. 시나리오를 생성하는 동안 이 시연 풀에서 하위 집합이 샘플링되어 LLM의 예가 된다. 이 접근법은 모델이 다양하고 맥락적으로 적절한 대화 시나리오를 효과적으로 합성하도록 안내한다. **(3)** 마지막으로 GPT-4를 활용하여 시나리오에서 파생된 멀티턴 대화를 생성한다. 이러한 합성 대화에서는 이미지와 음악을 포함한 멀티모달 요소들이 상세한 텍스트 표현을 통해 묘사된다. 우리는 다양한 대화 사례를 큐레이션합니다.\n' +
      '\n' +
      '그림 3: 멀티모달 인터리브 명령어 데이터셋 AnyInstruct의 구축 과정은 멀티모달 요소를 통합한 텍스트 기반 대화 생성과 Text-to-Multimodality 변환의 두 단계로 나뉜다. 첫 번째 단계는 주제, 시나리오 및 텍스트 대화를 생성하는 반면, 두 번째 단계는 최종 멀티모달 대화를 생성한다.\n' +
      '\n' +
      '시나리오 생성, 모델을 최대한 다양한 모달리티가 있는 대화 상자를 만들도록 프롬프트합니다. 결과적으로, 우리는 텍스트 형식으로만 멀티모달 대화 데이터의 실질적인 코퍼스를 컴파일했다.\n' +
      '\n' +
      '텍스트 대 다중 모드 변환.이 단계에서는 텍스트 설명을 다중 모드 요소로 변환하기 위해 고급 생성 모델을 사용한다. 이미지 생성을 위해 OpenAI사의 DALL-E-3(Betker et al., 2023), 음악 구성을 위해 MusicGen(Copet et al., 2023), 사용자의 지시와 모델의 텍스트 응답으로부터 음성 합성을 위해 Microsoft Azure사의 Text-to-Speech API(Microsoft)를 사용한다.\n' +
      '\n' +
      '필터링 후 다양한 멀티모달 조합을 특징으로 하는 108k 고품질 멀티모달 대화의 데이터 세트를 얻는다. 이 데이터 세트는 약 205k 이미지, 503k 음성 녹음 및 113k 음악 트랙을 포함한다. 또한, 구어 내레이션에 적합한 기존의 텍스트 전용 명령어 데이터셋에서 대화문을 추출하여 데이터셋을 개선하였다. 이는 텍스트 투 스피치 모델의 사용을 통해 100k 음성 대화로 이어진다.\n' +
      '\n' +
      '2단계 접근 방식은 다양한 규모의 고품질 멀티모달 대화를 효율적으로 수집했다. 부록 D는 데이터 합성 과정에서 사용되는 프롬프트를 제공합니다.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '우리는 모든 양식에 대한 멀티모달 이해 및 생성 작업을 포함하는 사전 훈련된 기반 AnyGPT(섹션 3)의 기본 능력을 평가한다. 이 평가는 사전 훈련 과정에서 서로 다른 양식 간의 정렬을 테스트하는 것을 목표로 했다. 구체적으로, X는 이미지, 음악 및 음성을 별도로 사용하는 각 촬영장비에 대해 텍스트 대 X 및 X 대 텍스트 작업을 모두 테스트한다.\n' +
      '\n' +
      '실세계 시나리오를 시뮬레이션하기 위해, 모든 평가는 _zero-shot_ 모드에서 수행된다. 이는 AnyGPT가 평가 프로세스 동안 다운스트림 훈련 샘플에 대해 미세 조정되거나 사전 훈련되지 않을 것임을 의미한다. 이 도전적인 평가 설정은 모델이 알려지지 않은 테스트 분포로 일반화할 것을 요구하며, 다양한 양식에 걸쳐 AnyGPT의 일반주의적 능력을 보여준다. 평가 결과는 AnyGPT가 일반주의적 멀티모달 언어 모델로서 다양한 멀티모달 이해 및 생성 작업에서 칭찬할 만한 성과를 달성한다는 것을 보여준다.\n' +
      '\n' +
      '####5.1.1 이미지 이해\n' +
      '\n' +
      '이미지 캡션 태스크에 대한 AnyGPT의 이미지 이해 능력을 평가한다. 비교 결과는 표 2에 제시되어 있다. MS-COCO 2014 캡션 벤치마크(Lin et al., 2014)를 활용하고, 선행 연구들(Li et al., 2023; Tang et al., 2023)에 이어 _Karpathy split testset_을 채택한다.\n' +
      '\n' +
      '이미지 생성 텍스트-이미지 생성 작업의 결과는 표 3에 제시되어 있다. 이전 연구(Koh et al., 2023; Ge et al., 2023; Sun et al., 2023)와의 일관성을 보장하기 위해, 우리는 MS-COCO 유효성 검사 세트에서 30k개의 이미지를 무작위로 선택하고 평가 기준으로 CLIP\\({}_{score}\\)을 사용한다. 이 메트릭은 CLIP-ViT-L에 기초하여, 실제 이미지로부터 생성된 이미지와 그 대응하는 캡션 사이의 유사성 스코어를 계산한다(Radford et al., 2021).\n' +
      '\n' +
      '#### 5.1.2 Speech\n' +
      '\n' +
      'AsBrWe evaluate AnyGPT on the Automatic Speech Recognition (ASR) task on the Word Error Rate (WER) on the test-clean subsets of LibriSpeech dataset (Panayotov et al., 2015). Wav2vec 2.0과 Whisper Large V2를 베이스라인으로 사용합니다. Wav2vec 2.0은 6만 시간의 연설로 사전 훈련되고\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CIDEr \\(\\uparrow\\) \\\\ \\hline Flamingo (9B) (Alayrac et al., 2022) & 79.4 \\\\ Flamingo (80B) & 84.3 \\\\ Emu (14B) (Sun et al., 2023) & 112.4 \\\\ DreamLLM (8B) (Dong et al., 2023) & 115.4 \\\\ InstructBLIP (14B) (Dai et al., 2023) & 102.2 \\\\ SEED-LLaMA (8B) (Ge et al., 2023) & 123.6 \\\\ \\hline AnyGPT (8B) & 107.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 이미지 캡션 작업에 대한 비교 결과. 회색의 결과는 모델이 훈련 샘플에 대해 훈련했음을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CLIP\\({}_{score}\\uparrow\\) \\\\ \\hline GILL (Koh et al., 2023) & 0.67 \\\\ Emu & 0.66 \\\\ SEED-LLaMA & **0.69** \\\\ \\hline AnyGPT & 0.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 텍스트-이미지 생성 태스크에 대한 비교 결과. 본 논문에서는 MS-COCO 캡션을 이용하여 영상을 생성하고, 평가를 위해 CLIP 유사도 점수(CLIP\\({}_{score}\\))를 계산한다.\n' +
      '\n' +
      'LibriSpeech에서 미세 조정되는 반면 속삭임 대형 V2는 제로 샷 설정으로 평가되지만 680,000시간의 음성으로 훈련된다. 결과를 표 5에 나타낸다.\n' +
      '\n' +
      'TtsWe는 VCTK 데이터셋에 대해 TTS(zero-shot Text-to-Speech) 평가를 수행한다. 그 결과는 표 4에 제시되어 있다. 우리는 화자의 유사도와 단어 오류율(WER)을 갖는 TTS 시스템을 평가하는데, 여기서 WER은 음성 품질에 초점을 맞춘다. 더 많은 실험 세부 사항은 부록 C에서 찾을 수 있다.\n' +
      '\n' +
      '#### 5.1.3 Music\n' +
      '\n' +
      'We evaluate AnyGPT\'s performance on the MusicCaps benchmark Agostinelli et al.(2023) for both music understanding and generation tasks. 우리는 CLAP\\({}_{score}\\)Wu et al. (2022); Huang et al. (2023) 악보를 객관적 척도로 사용하여 생성된 음악과 텍스트 기술 간의 유사성을 측정한다.\n' +
      '\n' +
      '음악 캡셔닝의 평가를 위해 기존의 객관적 메트릭이 음악 캡셔닝 작업에서 성능을 표현하는데 제한적일 수 있음을 발견했다. 음악의 다양성과 주관성은 개인의 다양한 의견으로 이어진다. 특정 음악 장르와 악기만이 쉽게 알아볼 수 있는 독특한 특성을 가지고 있다. 최근 연구 Gardner et al.(2023)이 이 문제를 탐구했지만 해결해야 할 어려운 문제로 남아 있다. 객관적인 평가를 위해 <음악, 실제 자막> 쌍과 <음악, 생성된 자막> 쌍을 비교하여 CLAP\\({}_{score}\\)을 계산한다. 이 점수는 전체 테스트 세트에 걸쳐 평균화된다.\n' +
      '\n' +
      '### Example Demonstrations\n' +
      '\n' +
      'AnyInstruct-108k 데이터 세트를 미세 조정한 후 AnyGPT는 모든 멀티모달 대화에서 능력과 잠재력을 보여준다. 본 논문에서는 부록 E의 AnyGPT에 대한 설득력 있는 대화 사례를 제시한다. 이 사례들은 AnyGPT가 어떤 조합으로든 다양한 양식에 걸쳐 내용을 이해하고 추론할 수 있음을 보여준다. 특히 AnyGPT는 텍스트, 음성, 이미지 및 음악을 포함한 여러 모달리티로 엮인 명령어를 이해할 수 있으며, 응답에 적합한 멀티모달 조합을 능숙하게 선택할 수 있다. 의미-음향 계층적 모델링의 2단계 프레임워크는 AnyGPT가 3초 음성 프롬프트의 음색과 감정에 일치하는 음성 응답을 생성할 수 있게 한다. 추가 사례와 스피치 및 음악 콘텐츠를 경험하려면 데모 페이지를 방문하는 것이 좋습니다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 음성, 텍스트, 이미지 및 음악을 포함한 다양한 모달리티의 통합 처리를 위해 이산 표현을 사용하는 임의의 멀티모달 언어 모델인 AnyGPT를 도입하였다. 이산적 다중 모드 표현은 기존의 LLM 아키텍처나 훈련 패러다임에 대한 변경 없이 외국어를 통합하는 것과 비교할 수 있는 새로운 모달리티의 원활한 통합을 용이하게 한다. 멀티모달 입력과 출력의 임의 조합을 다룰 수 있는 모델을 갖추기 위해 다양한 모달리티를 복잡하게 엮는 멀티턴 대화로 구성된 최초의 대규모 멀티모달 명령어 데이터셋 AnyInstruct-108k를 합성한다. 실험 결과는 AnyGPT가 다양한 분야에서 유망한 결과를 달성함을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CLAP\\({}_{score}\\uparrow\\) \\\\ \\hline _Music understanding_ & \\\\ <music, real caption> & 0.16 \\\\ <music, generated caption> & 0.11 \\\\ \\hline _Music generation_ & \\\\ Riftsson Forsgren and Martiros (2022) & 0.19 \\\\ Mousai Schneider et al. (2023) & **0.23** \\\\ AnyGPT & 0.14 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 음악 이해 및 생성 과제에 대한 비교 결과. 음악과 텍스트 캡션 사이의 정렬을 점수화하는 메트릭이 보고된다(CLAP\\({}_{score}\\). 음악 캡셔닝을 위해 <음악, 실제 자막> 쌍과 <음악, 생성된 자막> 쌍의 CLAP\\({}_{score}\\)을 계산한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & WER \\(\\downarrow\\) & SIM \\(\\uparrow\\) \\\\ \\hline _Ground Truth_ & _1.9_ & _0.93_ \\\\ VALL-E Wang et al. (2023) & 7.9 & 0.75 \\\\ USLM Zhang et al. (2023) & **6.5** & **0.84** \\\\ \\hline AnyGPT & 8.5 & 0.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: TTS 과제에 대한 비교 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & WER \\(\\downarrow\\) \\\\ \\hline Human-level Amodei et al. (2016) & 5.8 \\\\ Wav2vec 2.0 Baevski et al. (2020) & 2.7 \\\\ Whisper Large V2 Radford et al. (2022) & **2.7** \\\\ \\hline AnyGPT & 8.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ASR 과제에 대한 비교 결과. 우리는 단어 오류율(WER)을 메트릭으로 사용한다.\n' +
      '\n' +
      '교차-모달 태스크들 및 이산 표현들이 통합된 대형 언어 모델 내에서 다수의 모달리티들을 효과적이고 편리하게 통합할 수 있음을 입증한다.\n' +
      '\n' +
      '## 한계와 미래 작업\n' +
      '\n' +
      'Any-to-Any 멀티모달 LLM 벤치마크 Any-to-any 멀티모달 대형 언어 모델(LLM)의 도메인은 새로운 연구 분야이다. 그러나 여러 차원에 걸쳐 모델의 기능을 평가하고 잠재적인 위험을 완화하기 위한 전용 벤치마크의 부족은 상당한 도전을 제시한다. 결과적으로 포괄적인 벤치마크의 개발이 필수적이다.\n' +
      '\n' +
      'LLM을 향상시키지만 이산 표현을 갖는 멀티모달 LLM은 안정적으로 훈련될 수 있지만, 단일모달 훈련에 비해 더 높은 손실이 관찰되어 각 모달리티에서 최적의 성능을 방지한다. 멀티모달 융합을 개선하기 위한 잠재적인 전략은 LLM과 토큰타이저를 스케일링하거나 다양한 데이터를 더 잘 관리하고 성능을 최적화하기 위해 MOE(Mixture-Of-Experts) 아키텍처를 채택하는 것을 포함할 수 있다.\n' +
      '\n' +
      '개별 표현을 사용하는 멀티모달 LLM에서 더 나은 토큰라이저는 토큰라이저의 품질이 모델의 이해 및 생성 잠재력에 대한 상한을 설정한다. 토니사이저를 향상시키는 것은 우수한 코드북 교육 방법의 채택, 보다 응집력 있는 멀티모달 표현의 개발, 다양한 모달리티에 걸친 정보 분산의 적용 등 다양한 각도에서 접근할 수 있다.\n' +
      '\n' +
      '이미지 및 오디오와 같은 더 긴 컨텍스트 멀티모달 콘텐츠는 종종 광범위한 시퀀스에 걸쳐 있다. 예를 들어, AnyGPT는 음악 모델링을 5초로 제한하여 오디오 출력의 실질적인 유용성을 크게 제한한다. 더욱이, 임의의-대-임의의 멀티모달 대화의 경우, 확장된 컨텍스트는 더 많은 수의 대화 교환을 허용하여 상호 작용의 깊이와 복잡성을 풍부하게 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Laion-aesthetics (2022) 2022b. 라이온 코코: 라이온2b-en으로부터 600m 합성 캡션. [https://laion.ai/blog/laion-coco/] (https://laion.ai/blog/laion-coco/).\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 기술보고서; _ ArXiv preprint_, abs/2303.08774.\n' +
      '* Agostinelli et al. (2023) Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and Christian Havno Frank. 2023. Musiclm : 텍스트로부터 음악을 생성하는 단계; _ ArXiv preprint_, abs/2301.11325.\n' +
      '* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahani Sharifzadeh, Mikolaj Binkowski, Oriol Vinyals, Andrew Zisserman, Karen Simonyan. 2022. 플라밍고: 수발 학습을 위한 시각적 언어 모델. _ ArXiv preprint_, abs/2204.14198.\n' +
      '* Amodei et al. (2016) Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. 2016. Deep speech 2: End-to-end 음성 인식 in English and mandarin. _International conference on machine learning_, pages 173-182. PMLR.\n' +
      '* Ardila et al. (2020) Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common Voice: 대량 다국어 음성 말뭉치. _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pages 4218-4222, Marseille, France. 유럽 언어 자원 협회\n' +
      '* Baevski et al. (2020) Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: 음성 표현들의 자기 지도 학습을 위한 프레임워크. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntao Zhuang, Joyce Lee, Yufei Guo et al. 2023. 개선된 캡션을 갖는 이미지 생성. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8.\n' +
      '* Borsos et al. (2023a) Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023a. Audiolm: 오디오 생성에 대한 언어 모델링 접근법. _ IEEE/ACM Transactions on Audio, Speech, Language Processing_.\n' +
      '* Borsos et al. (2022) Zalan Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi.\n' +
      '\n' +
      '2023b. 사운드스톰: 효율적인 병렬 오디오 생성. _ ArXiv preprint_, abs/2305.09636.\n' +
      '*3 September 2021_, pages 3670-3674. ISCA.\n' +
      '* Copet et al. (2023) Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. 2023. 심플하고 제어 가능한 음악 생성_ ArXiv preprint_, abs/2306.05284.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023. 명령어: 명령어 튜닝을 갖는 범용 비전-언어 모델들을 향함. _ ArXiv preprint_, abs/2305.06500.\n' +
      '* D\'efossez et al. (2022) Alexandre D\'efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. 고충실도 신경 오디오 압축_ ArXiv preprint_, abs/2210.13438.\n' +
      '* Dong et al. (2023) Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jian-Yuan Sun, Hongyu Zhou, Hao-Ran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, 및 Li Yi. 2023. Dream-llm: Synergistic multimodal comprehension and creation. _ ArXiv preprint_, abs/2309.11499.\n' +
      '* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. 이미지는 16x16 단어의 가치가 있다: 스케일에서 이미지 인식을 위한 트랜스포머. _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net.\n' +
      '* 실시간 음악 생성을 위한 안정적인 확산.\n' +
      '* Gardner et al. (2023) Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M. 비트너 2023. Llark: 음악을 위한 멀티모달 명령어-팔로우잉 언어 모델.\n' +
      '* Ge et al. (2023a) Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023a. 큰 언어 모델에 시각의 씨앗을 심는 것. _ ArXiv preprint_, abs/2307.08041.\n' +
      '* Ge et al. (2023b) Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. 2023b. 씨드토키나이저로 라마를 보고 그리게 하는 것 ArXiv preprint_, abs/2310.01218.\n' +
      '* Huang et al. (2023) Rongjie Huang, Jia-Bin Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiaoyue Yin, Zhou Zhao. 2023. Make-an-audio: prompt-enhanced diffusion model을 갖는 text-to-audio generation. _ ArXiv preprint_, abs/2301.12661.\n' +
      '* Koh et al. (2023) Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. 2023. 멀티모달 언어 모델들로 이미지를 생성하는 단계; _ ArXiv preprint_, abs/2305.17216.\n' +
      '* Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoder and large language models. In _International Conference on Machine Learning_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. 2014. Microsoft coco: context in common objects. 유럽 컴퓨터 비전 회의에서.\n' +
      '* Lu et al. (2023) Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. 2023. Unified-io 2: vision, language, audio, action을 갖는 Scaling autoregressive multimodal models. _ ArXiv preprint_, abs/2312.17172.\n' +
      '* Microsoft azure text-to-speech api. [https://azure.microsoft.com/en-us/products/ai-services/ai-speech] (https://azure.microsoft.com/en-us/products/ai-services/ai-speech).\n' +
      '*Pan et al. (2023) Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. 2023. Journeydb: Generative Image understanding의 벤치마크. _ ArXiv preprint_, abs/2307.00716.\n' +
      '* Panayotov et al. (2015) Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: 공개 도메인 오디오 북에 기초한 ASR 코퍼스. _2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015_, pages 5206-5210. IEEE.\n' +
      '* Pratap et al. (2020) Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. MLS: 음성 연구를 위한 대규모 다국어 데이터셋. Interspeech 2020, 21th Annual Conference of International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020_, pages 2757-2761. ISCA.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. 자연어 감독으로부터 전이 가능한 시각적 모델을 학습하는 단계. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.\n' +
      '* Rongjie et al. (2020)Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. 대규모의 약한 감독을 통한 강인한 음성 인식_ ArXiv preprint_, abs/2212.04356.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ J 마흐 배워 Res._ , 21:140:1-140:67.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 _Proceedings에서, 페이지 10684-10695.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer.\n' +
      '* Schneider et al. (2023) Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. 2023. Mousai: Long-context latent diffusion을 갖는 텍스트-투-음악 생성 _ ArXiv preprint_, abs/2301.11757.\n' +
      '* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: 차세대 이미지-텍스트 모델을 훈련시키기 위한 개방형 대규모 데이터세트. _ 신경 정보 처리 시스템_, 35:25278-25294에서의 발전.\n' +
      '* Sun et al. (2023a) Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yuze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2023a. 생성적 멀티모달 모델은 맥락 내 학습자이다. _ ArXiv preprint_, abs/2312.13286.\n' +
      '* Sun et al. (2023b) Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yuzeze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2023b. 다중 모달리티의 생성적 사전 훈련 ArXiv preprint_, abs/2307.05222.\n' +
      '* Tang et al. (2023a) Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2023a. Codiz-2: In-context, interleaved, interactive any-to-any generation. _ ArXiv preprint_, abs/2311.18775.\n' +
      '* Tang et al. (2023b) Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. 2023b. 합성확산을 통한 모든-모든 세대. _ ArXiv preprint_, abs/2305.11846.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhahiri, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajibar Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ ArXiv preprint_, abs/2307.09288.\n' +
      '* van den Oord et al. (2017) Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. 신경 이산 표현 학습. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6306-6315.\n' +
      '* Wang et al. (2023) Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023. 신경 코덱 언어 모델들은 음성 합성기들에 제로 샷 텍스트이다. _ ArXiv preprint_, abs/2301.02111.\n' +
      '* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. 자가 명령어: 언어 모델을 자가 생성 명령어와 정렬한다. _ ArXiv preprint_, abs/2212.10560.\n' +
      '* Wu et al.(2023) Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. Next-gpt: Any-to-any multimodal llm. _ ArXiv preprint_, abs/2309.05519.\n' +
      '*2023 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP)_, pages 1-5.\n' +
      '* Zeghidour et al. (2021) Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. 사운드스트림: 엔드 투 엔드 뉴럴 오디오 코덱. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 30:495-507.\n' +
      '* Zhang et al. (2023a) Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, 및 Xipeng Qiu. 2023a. SpeechGpt: 내재적 상호모달 대화 능력으로 큰 언어 모델을 임파워링하는 것. 자연어 처리에서의 경험적 방법에 대한 회의.\n' +
      '* Zhang et al. (2023b) Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, 및 Xipeng Qiu. 2023b. Speechtokenizer: Unified speech tokenizer for speech large language models. _ ArXiv preprint_, abs/2308.16692.\n' +
      '* Zhu et al. (2023) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2023. 멀티모달 c4: 텍스트가 인터리빙된 이미지들의 오픈, 억대 스케일 코퍼스 _ ArXiv preprint_, abs/2304.06939.\n' +
      '\n' +
      '## 부록 사전교육\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '### pre-training\n' +
      '\n' +
      '다양한 템플릿을 사용하여 멀티모달 문장을 구성하여 사전 훈련 데이터 내에서 다양한 스펙트럼을 보장한다. 각각의 비-텍스트 모달리티 콘텐츠는 시작과 끝 모두에 배치된 특수 토큰에 의해 식별된다. 전형적으로, 페어링된 데이터는 이미지, 스피치, 또는 음악과 같은 비-텍스트 모달리티(X) 및 그것의 대응하는 텍스트를 포함하며, 이는 캡션 또는 전사일 수 있다. 우리는 OpenAI GPT-4에 "제공된 텍스트를 기반으로 이미지를 생성하십시오"와 같은 수백 개의 양방향 명령어, 특히 X-to-텍스트 또는 텍스트-to-X를 생성하도록 프롬프트한다. 토큰 시퀀스(S) 및 관련 텍스트(T)가 주어지면 미리 설정된 풀에서 명령어(I)와 함께 생성 방향을 무작위로 선택하여 트리플렛(I, S, T)을 형성한다. 그런 다음 템플릿을 사용하여 이 삼중항을 시퀀스에 통합한다.\n' +
      '\n' +
      '**[인간]: \\(\\{I\\}.\\{S\\}\\)* *ceoh> [AnyGPT]: \\(\\{T\\}\\**ceos>.** 또는 그 변형\n' +
      '\n' +
      '**[인간] : \\(\\{I\\}\\)**. 입력은 \\(\\{T\\}\\)**ceoh>이다. [AnyGPT]: \\(\\{S\\}\\)**ceos>.**, 생성 방향에 따라.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline \\multicolumn{1}{c}{Modality} & Dataset & Description & Sample Rate \\\\ \\hline Interleaved & \\multirow{2}{*}{MMC4-core-ff} & 101M image-interleaved documents collected from Common Crawl. & \\multirow{2}{*}{0.05} \\\\ Image-Text & & We use the mmc4-core split which is consist of 7.3M documents. & \\\\ \\hline \\multirow{4}{*}{Image-Text} & Laion-2B & 2B image-text pairs from web. & \\\\ \\cline{2-3}  & LAION-COCO & 600M image-text pairs, where the caption is generated by BLIP. & \\\\ \\cline{2-3}  & JourneyDB & 4429K Midjourney images, with image caption. & \\\\ \\cline{2-3}  & LAION-Aesthetics & Several collections of subsets from LAION 5B with high visual quality. & \\\\ \\hline \\multirow{4}{*}{Speech-Text} & Multilingual Librispecch & Processing audiobooks read from LibriVox, & \\multirow{2}{*}{0.13} \\\\  & & we used a 44,000-hour subset of English. & \\\\ \\cline{2-3}  & CommonVoice & Microphone recordings from internet volunteers, & \\multirow{2}{*}{0.27} \\\\  & & of which we used a 3000-hour subset of English. & \\\\ \\cline{2-3}  & GigaSpeech & 10,000 hours of English voice data sourced & \\\\ \\cline{2-3}  & & from audiobooks, podcasts, and YouTube videos. & \\\\ \\hline \\multirow{2}{*}{Music-Text} & Youtube-Music-1M & 100M music-text pairs from Youtube. & \\multirow{2}{*}{0.25} \\\\ \\cline{2-3}  & MusicGen-Synthesis & 20k music-text pairs extracted & \\\\ \\cline{1-1} \\cline{2-3}  & from the AnyInstruct-108k dataset, synthesized by MusicGen. & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 사전 훈련 단계에서 사용되는 데이터의 세부사항.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Pre-training Stage & Fine-tuning Stage \\\\ \\hline Gradient clipping & 1.0 & 1.0 \\\\ (Global-norm) & 480 & 64 \\\\ Max length & 4500 & 4500 \\\\ Training steps & 81000 & 5000 \\\\ Learning rate scheduler & cosine & cosine \\\\ Peak learning rate & 6e-5 & 2e-5 \\\\ Warmup ratio & 0.03 & 0.03 \\\\ Optimizer & Adam & Adam \\\\ GPU & A100 & A100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 실험에 사용된 훈련 하이퍼파라미터.\n' +
      '\n' +
      '인터리빙된 멀티모달 데이터의 경우, 이미지 및 텍스트가 산재된 웹 문서와 같이, 비텍스트 콘텐츠가 자연스럽게 문장을 형성함에 따라 대응하는 토큰 시퀀스로 직접 대체한다.\n' +
      '\n' +
      '이미지와 음악 데이터는 대부분 웹에서 조달되기 때문에 멀티모달 생성의 품질에 영향을 줄 수 있는 일정 수준의 잡음이 존재한다. 그 결과, 초기 사전 학습 후 텍스트-이미지 생성을 위한 JourneyDB와 LAION-Aesthetics, 이미지 캡셔닝을 위한 LAION-COCO 등 고품질의 데이터셋을 선별적으로 활용하였다. 음악 데이터의 경우 AnyInstruct-108k 데이터 세트를 통합했다. 나머지 데이터는 변경되지 않은 상태로 유지되었으며 추가 4000단계를 위해 모델을 사전 훈련했다.\n' +
      '\n' +
      '우리는 탭 8에서 AnyGPT의 상세한 훈련 하이퍼파라미터를 보고한다.\n' +
      '\n' +
      '## 부록 B 명령어 튜닝\n' +
      '\n' +
      '## 부록 C 평가\n' +
      '\n' +
      'VCTK 데이터셋에 대해 TTS(zero-shot Text-to-Speech) 평가를 수행한다. 우리의 훈련 데이터와 VCTK 데이터 세트 사이에는 스피커에 중복이 없다. 우리는 입력으로 별도의 텍스트와 함께 발성 프롬프트로 각 스피커에서 3초 클립을 무작위로 선택한다.\n' +
      '\n' +
      '그 결과는 표 4에서 확인할 수 있으며, 화자 유사도와 WER을 갖는 TTS 시스템을 평가한다. 생성된 음성과 프롬프트 음성 사이의 화자 유사성을 평가하기 위해 WavLM-TDNN2를 사용한다. 생성된 음성과 프롬프트 모두에 대한 화자 임베딩을 생성할 수 있다.\n' +
      '\n' +
      '도 4: AnyGPT 멀티모달 대화의 예: 입력은 음악을 생성하기 위한 이미지 및 음성 명령이다. 출력은 해당 텍스트 및 음성 응답과 함께 요구 사항을 충족하는 음악이다. 모든 데이터는 이산 토큰으로 처리되며 LLM에 의해 자동으로 처리된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Generated Modality & Text & Image & Speech & Music \\\\ \\hline Decoding Strategy & Beam Search & Sampling & Sampling & Sampling \\\\ Beam size & 5 & - & - & - \\\\ Top-P & - & 0.7 & 0.7 & 1.0 \\\\ Repetition Penalty & 1.0 & 1.0 & 1.0 & 1.15 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 평가에 사용된 세대 디코딩 전략의 상세.\n' +
      '\n' +
      '그런 다음 이러한 임베딩 간의 코사인 유사성을 계산합니다. WER은 생성된 음성을 전사하기 위해 위스퍼 매체 모델을 사용하여 계산되며, 낮은 WER은 합성된 음성의 더 높은 품질을 나타낸다.\n' +
      '\n' +
      '우리는 음성 모델링을 위해 두 개의 자기회귀 모델을 사용하는 VALL-E 및 USLM과 모델을 비교한다. 그들은 각각 Encodec과 SpeechTokenizer를 speech tokenizer로 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '당신은 AnyGPT라는 챗봇과 대화하는 사용자에 대한 대화를 작성하는 데 도움을 주고 있습니다.\n' +
      '\n' +
      '대화에서, 사용자 둘 다 이미지 또는 음악을 제공하여 자신의 필요 및 아이디어를 표현하는 것을 도울 수 있다. 그리고 챗봇 MMGPT는 또한 자신의 발화에 이미지 또는 음악으로 사용자에게 응답할 수 있다.\n' +
      '\n' +
      '채팅의 이미지 및 음악은 각각 [이미지:설명] 및 [음악:설명]과 같은 이미지 설명 및 음악 설명 형태이다. 사용자는 이 형식으로 이미지와 음악을 제공해야 하며 챗봇은 사용자에게도 이렇게 반응할 것이다.\n' +
      '\n' +
      '기껏해야 하나의 음악이 하나의 대화에서 나타나고 음악에 대한 설명은 장르와 악기에 초점을 맞추어 간단해야 하며 알려진 음악을 직접 언급하지 않아야 한다.\n' +
      '\n' +
      '각 대화 전에 먼저 시나리오를 보여주고 채팅에 대한 작성을 시작할 수 있습니다.\n' +
      '\n' +
      '여기 일 예가 있다:\n' +
      '\n' +
      '--\n' +
      '\n' +
      '{demonstrations}\n' +
      '\n' +
      '--\n' +
      '\n' +
      '이제 다음 대화를 할 차례입니다. 사용자와 AnyGPT가 교대로 수행하는 형식에 따라 답변만 하면 됩니다.\n' +
      '\n' +
      '대화는 시나리오의 도입과 일치해야 한다.\n' +
      '\n' +
      '발화는 간결해야 한다는 것을 기억하세요, 발화당 5-15개의 단어를 사용하려고 노력하세요.\n' +
      '\n' +
      '유념하세요: 사용자 발화는 항상 질문 또는 명령이어야 합니다.\n' +
      '\n' +
      '일부 턴들에서, 사용자는 이미지 또는 음악 조각을 제공하고 제공된 이미지 또는 음악에 관한 AnyGPT에 질문을 하거나 지시를 한다.\n' +
      '\n' +
      '다른 턴들에서, 사용자는 필요한 이미지들 또는 음악을 생성하도록 AnyGPT를 요청한다.\n' +
      '\n' +
      '음악에 대한 설명은 장르, 스타일 및 악기에 초점을 맞춰야 합니다. 그리고 [이미지:] 또는 [음악:] 내의 이미지와 음악에 대한 설명을 더욱 상세하게 한다.\n' +
      '\n' +
      '참고: 이미지 설명에 유명인의 이름을 직접 포함하거나 음악 설명에 알려진 음악 조각을 언급하지 마십시오.\n' +
      '\n' +
      '팁: 사용자가 이미지와 음악 사이에서 변환을 요청할 때, AnyGPT는 요청된 결과를 생성하기 전에 입력 이미지 또는 음악에 대한 그의 이해를 먼저 발화해야 한다.\n' +
      '\n' +
      '대화상자를 2~3라운드로 유지합니다.\n' +
      '\n' +
      '각 대화 상자에는 하나의 음악과 최대 2개의 이미지가 포함되어야 합니다.\n' +
      '\n' +
      '--\n' +
      '\n' +
      '이 대화에서는 {new_scenario_description}\n' +
      '\n' +
      '**GPT4:**\n' +
      '\n' +
      '시나리오 설명에 따른 합성 채팅{A synthetic chat according the scenario description.}\n' +
      '\n' +
      '도 7: 채팅 내용 작성에 대한 프롬프트. 각 API 호출에 대해 3개의 시연을 샘플링합니다. 각 시연은 {new_scenario_description}과 해당 채팅으로 시나리오 설명을 포함한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '도 9 : 음성 명령어 + 이미지 \\(\\rightarrow\\) 텍스트 + 음악 + 음성 응답\n' +
      '\n' +
      '도 10 : 음성 명령어 + 음악 \\(\\rightarrow\\) 텍스트 + 음악 + 음성 응답\n' +
      '\n' +
      '도 11 : 음성 명령어 + 이미지 \\(\\rightarrow\\) 텍스트 + 음악 + 음성 응답\n' +
      '\n' +
      '도 12 : 텍스트\\(\\우열\\) 이미지 + 음악\n' +
      '\n' +
      '도 13 : 텍스트 + 이미지 \\(\\rightarrow\\) 음악\n' +
      '\n' +
      '도 14 : 텍스트 + 이미지 \\(\\우열\\) 텍스트 + 음악\n' +
      '\n' +
      '도 15 : 텍스트 + 음악 \\(\\rightarrow\\) 텍스트 + 이미지\n' +
      '\n' +
      '도 16 : 텍스트 + 음악 \\(\\rightarrow\\) Muisc\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>