<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n' +
      '\n' +
      ' Jun Zhan\\({}^{1,\\ast}\\), Junqi Dai\\({}^{1,\\ast}\\), Jiasheng Ye\\({}^{1,\\ast}\\)\n' +
      '\n' +
      '**Yunhua Zhou\\({}^{1}\\), Dong Zhang\\({}^{1}\\), Zhigeng Liu\\({}^{1}\\), Xin Zhang\\({}^{1}\\)**\n' +
      '\n' +
      '**Ruibin Yuan\\({}^{2}\\), Ge Zhang\\({}^{2}\\), Linyang Li\\({}^{1}\\), Hang Yan\\({}^{3}\\), Jie Fu\\({}^{2}\\)**\n' +
      '\n' +
      '**Tao Gui\\({}^{1}\\), Tianxiang Sun\\({}^{1}\\), Yugang Jiang\\({}^{1}\\), Xipeng Qiu\\({}^{1,\\dagger}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Fudan University\n' +
      '\n' +
      '\\({}^{2}\\) Multimodal Art Projection Research Community\n' +
      '\n' +
      '\\({}^{3}\\)Shanghai AI Laboratory\n' +
      '\n' +
      'Equal contribution.Corresponding author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in [https://junzhan2000.github.io/AnyGPT.github.io/](https://junzhan2000.github.io/AnyGPT.github.io/).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'LLMs have exhibited remarkable proficiency in comprehending and generating human language. Nevertheless, their capabilities are confined to textual processing. The real-world environment is inherently multimodal, with organisms perceiving and exchanging information through diverse channels, including vision, language, sound, and touch.\n' +
      '\n' +
      'A promising objective in developing multimodal systems is to augment LLMs with the capacity for multimodal perception. The dominant methodology involves the integration of multimodal encoders with the language model, thus empowering it to process information across various modalities and utilize its sophisticated text-processing abilities to produce coherent responses. However, this strategy is limited to text generation and does not encompass multimodal output.\n' +
      '\n' +
      'Pioneering efforts such as Emu (Sun et al., 2023), SEED-LLaMA (Ge et al., 2023) and SpeechGPT (Zhang et al., 2023) have made significant strides by enabling multimodal understanding and generation within language models. Yet, these models incorporate only a single non-textual modality, such as images or audio. While aligning text with one additional modality is relatively straightforward, integrating multiple modalities (\\(N\\geq 3\\)) within a single framework--and achieving bidirectional alignment among them--poses a more formidable challenge.\n' +
      '\n' +
      'Existing explorations in any-to-any multimodal generation have encountered obstacles: some (Tang et al., 2023) lacked a robust core language model, which impeded the system\'s reasoning and decision-making capabilities; Others, such as NExT-GPT (Wu et al., 2023), CoDi-2 (Tang et al., 2023), and Unified-IO2 (Lu et al., 2023), have employed separately pre-trained encoders and decoders. This approach results in representational inconsistencies between the inputs and outputs of the LLMs, which in turn complicates both training and inference processes. Moreover, stabilizing training with such diverse modalities necessitates substantial modifications to existing models and techniques.\n' +
      '\n' +
      'To overcome these challenges, we introduce AnyGPT, an any-to-any multimodal language model that employs discrete representations for unified processing. AnyGPT is equipped with multimodal tokenizers that compress raw multimodal data, such as images and audio, into a sequence ofdiscrete semantic tokens. These discrete representations enable the core LLM to unify tasks such as perception, understanding, reasoning, and generation in an autoregressive manner at the semantic level. Subsequently, de-tokenizers convert the discrete representations back into the original modal representations at the perceptual level. Thanks to discrete representation, which filters out high-frequency, modality-specific perceptual information while preserving essential low-frequency semantic information (Ge et al., 2023; Borsos et al., 2023; Rombach et al., 2022), we can train our model stably without any alterations to the existing LLM architecture or training paradigms. Instead, our approach relies solely on data-level preprocessing. This allows for the seamless integration of new modalities into LLMs, akin to the addition of new languages, and permits the direct application of existing LLM tools, which enhances the efficiency of both the training and inference stages.\n' +
      '\n' +
      'Furthermore, to mitigate the scarcity of multimodal alignment data encompassing all modalities, we build a text-centric multimodal alignment dataset for pre-training. Our goal is to use text as a bridge, by aligning other modalities with text, to achieve mutual alignment among all modalities, since natural language is the most refined modality of semantic representation and is present in the majority of multimodal alignment datasets. To endow the model with the capability to comprehend and generate content interwoven with multiple modalities, we employ advanced generative models to synthesize a multimodal instruction dataset, AnyInstruct-108k. This dataset, comprising 108k samples of multi-turn conversations, enables AnyGPT to handle arbitrary combinations of multimodal inputs and outputs.\n' +
      '\n' +
      'Experimental results demonstrate that AnyGPT achieves zero-shot performance comparable to that of specialized models across various modalities. Furthermore, extensive case studies corroborate AnyGPT\'s remarkable ability to facilitate any-to-any multimodal dialogue, substantiating the feasibility of using discrete representations to unify multiple modalities.\n' +
      '\n' +
      'Our contributions include the following:\n' +
      '\n' +
      '* We propose AnyGPT, a token-based any-to-any multimodal language model which can understand and generate various modalities, including speech, text, images, and music.\n' +
      '* One key challenge is the lack of multimodal interleaved instruction-following data. We develop a pipeline using generative models to build AnyInstruct-108k, a dataset comprising 108k multi-turn dialogues with interleaved multimodal elements.\n' +
      '\n' +
      'Figure 1: An overview of the AnyGPT model architecture. All modalities are tokenized into discrete tokens, upon which the LLM performs multimodal understanding and generation autoregressively. Only data pre-processing and post-processing are required, with the modelâ€™s architecture and training objectives remaining unaltered.\n' +
      '\n' +
      '* We demonstrate discrete representations can effectively unify multiple modalities within a language model.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Multimodal Large Language Models\n' +
      '\n' +
      'To enable cross-modal perception in LLM, a common approach is to connect pre-trained encoders of other modalities as adaptors. However, these models are often limited to text generation.\n' +
      '\n' +
      'To empower LLMs with multimodal generation capabilities, Tang et al. (2023) introduces a frozen text-to-image diffusion model and learns the mapping between the LLM\'s embeddings and the diffusion model. Sun et al. (2023) utilizes continuous embeddings to represent the image, calculating either a loss for the next token prediction or the next visual embedding regression. In contrast, SEED-LLMa (Ge et al., 2023) trains an image discretization tokenizer to encode the original image into discrete tokens. Through a unified next token prediction task, it achieves unified image understanding and generation. Similarly, in the field of speech, SpeechGPT (Zhang et al., 2023) enables LLMs to have inherent cross-modal conversation capabilities through discrete speech representation.\n' +
      '\n' +
      'To achieve multimodal generation across various modalities on LLMs, NExT-GPT (Wu et al., 2023) utilizes existing high-performance encoders and decoders, connected by a small number of projection layer parameters. However, NExT-GPT does not train the LLM, which may result in suboptimal performance. Moreover, its representation of multimodal input and output lacks a unified form, which poses challenges in unified training and inference.\n' +
      '\n' +
      '### Multimodal Discretization\n' +
      '\n' +
      'To create a unified multimodal language model, a common approach is to use discretization. A typical method is VQ-VAE (van den Oord et al., 2017). This involves maximizing the restoration of the original representation from the compressed tokens. Some studies (D\'efossez et al., 2022; Zeghidour et al., 2021) incorporate residual quantization mechanisms to further enhance fidelity.\n' +
      '\n' +
      'In addition to VQ-VAE based tokenizers, some tokenizers focus on extracting high-level semantic representations. Ge et al. (2023) discretizes the image into semantic-level. The SpeechTokenizer (Zhang et al., 2023), based on the RVQ-VAE structure, enables the first layer of tokens to retain the semantic information of speech, and the remaining layers to supplement residual information information, achieving a disentanglement of semantic and acoustic information.\n' +
      '\n' +
      '## 3 AnyGPT\n' +
      '\n' +
      'Our interest lies in facilitating the generation of any modality to any modality with LLMs. To realize this, we propose a comprehensive framework that can be uniformly trained. As illustrated in Figure 1, this framework is composed of three main components: (1) multimodal tokenizers, (2) a multimodal language model serving as the backbone, and (3) multimodal de-tokenizers. The tokenizers transform continuous non-text modalities into discrete tokens, which are subsequently arranged into a multimodal interleaved sequence. Then the sequences are trained by the language model using the next token prediction training objective. During the inference process, multimodal tokens are decoded back into their original representations by the associated de-tokenizers. To enrich the quality of generation, multimodal enhancement modules can be deployed to post-process the generated results, including applications like voice cloning or image super-resolution. In the following section, we will introduce the details of each module.\n' +
      '\n' +
      '### Tokenization\n' +
      '\n' +
      'Image TokenizerWe utilize the SEED tokenizer (Ge et al., 2023) for image tokenization. The SEED tokenizer consists of several components, including a ViT encoder (Dosovitskiy et al., 2021), Causal Q-Former, VQ Codebook (van den Oord et al., 2017), multi-layer perceptron (MLP), and a UNet decoder (Ronneberger et al., 2015). SEED takes a \\(224\\times 224\\) RGB image as input, and the ViT encoder encodes the image into \\(16\\times 16\\) patches, then the Causal Q-Former converts the patch features into 32 causal embeddings. A codebook with 8192 entries discretizes the embeddings into a sequence of quantized codes. An MLP is employed to decode the visual codes\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Modality & Image & Speech & Music \\\\ \\hline Vocab Size & 8192 & 1024 & 4096 \\\\ Tokens per Sample & 32 / per image & 50 / s & 200 / s \\\\ RVQ & âœ˜ & âœ” & âœ” \\\\ Input Size & 224*224 & variable duration & 5s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Details of tokenization of different modalities.\n' +
      '\n' +
      'into a generation embedding, which is aligned with the latent space of the pre-trained unCLIP Stable Diffusion(unCLIP-SD) (Rombach et al., 2022). Finally, the UNet decoder is used to restore the generation embedding to the original image.\n' +
      '\n' +
      'Speech TokenizerThe tokenizer for speech we utilize is SpeechTokenizer (Zhang et al., 2023), adopting an encoder-decoder architecture with residual vector quantization (RVQ). The SpeechTokenizer compresses single-channel audio sequences into a discretized matrix using eight hierarchical quantizers, each with 1,024 entries, and achieves a frame rate of 50 Hz. The first quantizer layer captures semantic content, while layers 2 to 8 encode paralinguistic details. A 10-second audio is thus transformed into a \\(500\\times 8\\) matrix, splitting into semantic and acoustic tokens. We adopt a SpeechTokenizer variant pre-trained on the Commonvoice (Ardila et al., 2020) and Librispeech (Panayotov et al., 2015) datasets.\n' +
      '\n' +
      'In AnyGPT, the Large Language Model (LLM) is employed to model the semantic tokens, while a voice cloning model supplements the remaining paralinguistic information. As a result, the size of the voice vocabulary in the LLM is equivalent to the size of one codebook, which is 1024. Further details will be discussed on in Section 3.3.\n' +
      '\n' +
      'Music TokenizerAlthough speech and music share similar data formats, their substantial content differences lead us to treat them as distinct modalities, each equipped with its own tokenizer. For music, we employ Encodec (D\'efossez et al., 2022), a convolutional auto-encoder with a latent space quantized using Residual Vector Quantization (RVQ), as the music tokenizer. We use an available off-the-shelf variant of the Encodec1 pretrained on 20k pieces of music tracks. This variant processes 32 kHz monophonic audio, and achieves a frame rate of 50 Hz. The embeddings generated are quantized using an RVQ with four quantizers, each with a codebook size of 2048, resulting in a combined music vocabulary size of 8192.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/facebook/encodec_32khz](https://huggingface.co/facebook/encodec_32khz)\n' +
      '\n' +
      'We encode 5 seconds music into 250 latent frames, ultimately generating a \\(250\\times 4\\) codes matrix. To enable the language model predict entire music clip, we flatten the 4-layer music codes into a causal sequence in a frame-by-frame manner. The language model begins by predicting the initial four tokens of the first frame and continues in a similar fashion for the subsequent frames.\n' +
      '\n' +
      '### Language Model Backbone\n' +
      '\n' +
      'Expanding vocabularyTo incorporate multimodal discrete representations into pre-trained LLMs, we expand the vocabulary with new modality-specific tokens, and consequently extend the corresponding embeddings and prediction layer, the newly incorporated parameters are initialized randomly. The tokens from all modalities combine to form a new vocabulary, where each modality is trained within the language model to align in a shared representational space. The size of this enhanced vocabulary, denoted by \\(V\\), is the summation of the vocabulary sizes across all modalities, that is, \\(V=\\sum_{i=1}^{n}V_{i}\\), where \\(V_{i}\\) signifies the vocabulary size of the \\(i\\)-th modality.\n' +
      '\n' +
      'Unified Multimodal Language ModelEquipped with the modality-specific tokenizers, we can compress multimodal data into discrete token sequences, which can be trained by the language model using the next token prediction loss. This naturally enables the core LLM to unify tasks such as perception, understanding, reasoning, and generation in an autoregressive manner.\n' +
      '\n' +
      'We employ the LLaMA-2 (Touvron et al., 2023) 7B as the backbone, which is pre-trained on 2 TB of text tokens. Apart from reshaping the embedding matrix and prediction layer, the rest of the language model remains unaltered.\n' +
      '\n' +
      '### Multimodal Generation\n' +
      '\n' +
      'The generation of high-quality multimodal data, including high-definition images, and high-fidelity audio, presents a substantial challenge. These data typically necessitate a large number of bits for accurate representation, resulting in long sequences which is particularly demanding for language models, as the computational complexity increases exponentially with the length of the sequence.\n' +
      '\n' +
      'To tackle this, we adopt a two-stage framework for high-fidelity generation, comprising semantic information modeling and perceptual information modeling. First, the language model is tasked with generating content that has undergone fusion and alignment at the semantic level. Then, non-autoregressive models convert multimodal semantic tokens into high-fidelity multimodal content at the perceptual level, striking a balance between performance and efficiency.\n' +
      '\n' +
      'Specifically, we employ SEED tokens, alignedwith the diffusion latent space, for visual language modeling. Semantic-level SEED tokens are decoded into high-quality images by a Diffusion Model, which is renowned for its superior generation capabilities. For speech, we utilize SoundStorm (Borsos et al., 2023), a non-autoregressive Masked Language Model, trained to generate SpeechTokenizer\'s acoustic tokens from semantic tokens. We train a variant of Soundstorm, which is trained using the SpeechTokenizer on the Multilingual LibriSpeech(MLS) dataset (Pratap et al., 2020). Subsequently, the SpeechTokenizer\'s decoder transforms all speech tokens into raw audio data. This approach enables AnyGPT replicate the voice of any speaker using a 3-second speech prompt, while significantly reducing the length of the voice sequence for LLM. For music, we employ Encodec tokens to filter out high-frequency details beyond human perception, and then use the Encodec decoder to reconstruct these tokens into high-fidelity audio data.\n' +
      '\n' +
      '## 4 Multimodal Data\n' +
      '\n' +
      '### Pre-training Data\n' +
      '\n' +
      'To enable the generation from any modality to any other, it is crucial to have data that is well-aligned across these modalities. Unfortunately, such data is notably scarce. To address this challenge, we build a text-centric bi-modal alignment dataset. Here, text is employed as a vital intermediary to bridge the gap between various modalities. By aligning different modalities with the textual modality within a language model, we aim to achieve mutual alignment amongst all modalities.\n' +
      '\n' +
      'The representational forms and types of information vary greatly across different modalities, To facilitate a standardized comparison of data volumes across various modalities, we have adopted a quantification approach based on token counts. Figure 2 presents all the data used in pre-training and their respective proportions. A certain level of oversampling is applied to modalities with comparatively lower data quantities, to attain a balanced representation of diverse data types within a single batch. More details are in Appendix A.1.\n' +
      '\n' +
      'Image & TextWe utilized image-text pairs from LAION-2B (Schuhmann et al., 2022), LAION-COCO (lai, 2022), LAION-Aesthetics (lai, 2022) and JouneyDB (Pan et al., 2023). LAION-2B provides images paired with noisy alt-texts sourced from the web, while LAION-COCO represents a 600M subset of this, captioned by BLIP. We refined these datasets by filtering for text quality, image aspect ratio, and clip score, etc., yielding a high-quality corpus of 300M pairs. To enhance the overall image generation fidelity, we supplement our data with the high-quality LAION-Aesthetics subset and the synthetic dataset JourneyDB from Midjourney.\n' +
      '\n' +
      'We also incorporate image-text interleaved data to adapt the model to an interleaved mode. We deploy the Multimodal-C4 (MMC4) dataset (Zhu et al., 2023), an enhanced version of the text-only C4 (Raffel et al., 2020). Specifically, we utilize the MMC4-core split, consisting of 7.3M documents.\n' +
      '\n' +
      'Speech & TextWe collect several large-scale English Automatic Speech Recognition (ASR) datasets, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and Multilingual LibriSpeech(MLS) (Pratap et al., 2020). These datasets are sourced respectively from online platforms, volunteer crowdsourcing, and audiobooks, collectively constituting a corpus of 57,000 hours of speech-text pairs, encompassing a wide variety of speakers, domains, and recording environments.\n' +
      '\n' +
      'Music&TextWe embark on an extensive data collection process by crawling over one million music videos from the Internet. The core step involves matching the titles of these videos with corresponding songs using the Spotify API. Subsequently, we harvest a comprehensive set of metadata for each music audio, including video titles, descriptions,\n' +
      '\n' +
      'Figure 2: Pre-training data distribution, segmented by token counts, with the inner section indicating the modality, the middle detailing data types, and the outer specifying individual datasets.\n' +
      '\n' +
      'keywords, playlist names, and Spotify lyrics. This metadata is formatted into JSON and fed into GPT-4 (Achiam et al., 2023) for processing. GPT-4\'s role is pivotal as an intelligent caption generator; it utilizes the noisy metadata to extract meaningful information and succinctly summarize it into coherent sentences. This approach allows us to generate high-quality text captions for a large amount of music audio, effectively minimizing the occurrence of hallucinations in the dataset.\n' +
      '\n' +
      'Training Sample Construction.To train the Language Model (LM), we employ various templates to construct multimodal sentences, which the LM then processes autoregressively. Further training details can be found in Appendix A.2. Additionally, We observe significant variation in sentence lengths across different modalities and datasets. To enhance training efficiency, samples from the same dataset are concatenated into a long sequence, adhering to the model\'s maximum sequence length. Consequently, each token in the sequence contributes to the loss.\n' +
      '\n' +
      '### Multimodal Interleaved Instruction Data Construction\n' +
      '\n' +
      'Effective human-machine interaction should permit the exchange of information in a variety of interleaved modalities. However, the increasing number of modalities in conversation significantly complicates the data collection process. To our knowledge, there is currently no large-scale instruction dataset involving more than two modalities. This poses a significant limitation on the development of a comprehensive model capable of managing dialogues with multiple, intertwined modalities.\n' +
      '\n' +
      'To overcome this limitation, we draw inspiration from the most recent research on data synthesis (Wang et al., 2022; Wu et al., 2023), and build a dataset comprised of 108k multi-turn conversation samples with generative models. With careful curation, each synthetic conversation integrates multiple modalities--text, speech, images, and music--in an interleaved manner. Specifically, our data synthesis process is carried out in two stages, as illustrated in Figure 3.\n' +
      '\n' +
      'Generation of text-based conversations incorporating multimodal elements.In this phase, we employ GPT-4 to generate a series of text-based conversations. Notably, we incorporate non-text modality in the form of their textual descriptions within these conversations. To ensure high-quality data at scale, we divide this stage into three steps. **(1)** Initially, we brainstorm 100 meta topics to cover a broad spectrum of scenarios related to audiovisual elements and we employ GPT-4 to expand these meta-topics into 20,000 specific topics. **(2)** Subsequently, we prompt LLM to generate specific dialogue scenarios based on these topics. Acknowleding the intrinsic constraints of a text-based LLM in generating multimodal elements, we prepare several demonstrations that encompass as many modality combinations as possible. While generating scenarios, a subset is sampled from this demonstration pool, serving as examples for the LLM. This approach guides the model to effectively synthesize varied and contextually appropriate conversational scenarios. **(3)** Finally, we utilize GPT-4 to generate multi-turn conversations derived from scenarios. In these synthesized dialogues, multimodal elements, including images and music, are depicted through detailed textual representations. We curate a diverse range of conversation examples, similar to\n' +
      '\n' +
      'Figure 3: The construction process of the multimodal interleaved instruction dataset AnyInstruct is divided into two stages: Generation of text-based conversations incorporating multimodal elements and Text-to-Multimodality Conversion. The first stage generates topics, scenarios, and textual dialogues, while the second stage produces the final multimodal dialogues.\n' +
      '\n' +
      'scenario generation, to prompt the model into creating dialogues with the widest possible variety of modalities. As a result, we compiled a substantial corpus of multimodal conversational data in solely textual format.\n' +
      '\n' +
      'Text-to-Multimodality Conversion.In this phase, we employ advanced generative models to convert textual descriptions into multimodal elements. We use OpenAI\'s DALL-E-3 (Betker et al., 2023) for image generation, MusicGen (Copet et al., 2023) for music composition, and Microsoft Azure\'s text-to-speech API (Microsoft) for speech synthesis from user\'s instructions and model\'s text responses.\n' +
      '\n' +
      'After filtering, we obtain a dataset of 108k high-quality multimodal dialogues, featuring a variety of multimodal combinations. This dataset includes around 205k images, 503k voice recordings, and 113k music tracks. Additionally, we enhanced our dataset by extracting dialogues from existing text-only instruction datasets well-suited for spoken narration. This results in 100k voice dialogues through the employment of text-to-speech models.\n' +
      '\n' +
      'The two-stage approach efficiently collected a diverse array of high-quality multimodal conversations at scale. Appendix D provides the prompts used during the data synthesis process.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      'We evaluate the fundamental capabilities of the pre-trained base AnyGPT (Section 3), covering multimodal understanding and generation tasks for all modalities. This evaluation aimed to test the alignment between different modalities during the pre-training process. Specifically, we test both text-to-X and X-to-text tasks for each modality, where X is image, music, and speech separately.\n' +
      '\n' +
      'To simulate real-world scenarios, all evaluations are conducted in a _zero-shot_ mode. This means that AnyGPT will be _not fine-tuned nor pre-trained_ on downstream training samples during the evaluation process. This challenging evaluation setting requires the model to generalize to an unknown test distribution, showcasing the generalist abilities of AnyGPT across different modalities. The evaluation results demonstrate that AnyGPT, as a generalist multimodal language model, achieves commendable performance on various multimodal understanding and generation tasks.\n' +
      '\n' +
      '#### 5.1.1 Image Understanding\n' +
      '\n' +
      'We assess the image comprehension capabilities of AnyGPT on the image captioning task. The comparison results are presented in Table 2. We utilize the MS-COCO 2014 captioning benchmark (Lin et al., 2014) and adopt the _Karpathy split testset_ following previous studies (Li et al., 2023; Tang et al., 2023).\n' +
      '\n' +
      'Image GenerationThe results of the text-to-image generation task are presented in Table 3. To ensure consistency with previous research (Koh et al., 2023; Ge et al., 2023; Sun et al., 2023), we randomly select 30k images from the MS-COCO validation set and use CLIP\\({}_{score}\\) as the evaluation criterion. This metric computes a similarity score between the generated image and its corresponding caption from a real image, based on CLIP-ViT-L (Radford et al., 2021).\n' +
      '\n' +
      '#### 5.1.2 Speech\n' +
      '\n' +
      'AsBrWe evaluate the performance of AnyGPT on the Automatic Speech Recognition (ASR) task by calculating the Word Error Rate (WER) on the test-clean subsets of the LibriSpeech dataset (Panayotov et al., 2015). We use Wav2vec 2.0 and Whisper Large V2 as baselines. Wav2vec 2.0 is pre-trained with 60,000 hours of speech and\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CIDEr \\(\\uparrow\\) \\\\ \\hline Flamingo (9B) (Alayrac et al., 2022) & 79.4 \\\\ Flamingo (80B) & 84.3 \\\\ Emu (14B) (Sun et al., 2023) & 112.4 \\\\ DreamLLM (8B) (Dong et al., 2023) & 115.4 \\\\ InstructBLIP (14B) (Dai et al., 2023) & 102.2 \\\\ SEED-LLaMA (8B) (Ge et al., 2023) & 123.6 \\\\ \\hline AnyGPT (8B) & 107.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comparison results on image captioning task. Results in grey indicate models have trained on training samples.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CLIP\\({}_{score}\\uparrow\\) \\\\ \\hline GILL (Koh et al., 2023) & 0.67 \\\\ Emu & 0.66 \\\\ SEED-LLaMA & **0.69** \\\\ \\hline AnyGPT & 0.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Comparison results on text-to-image generation task. We adopt MS-COCO captions to generate images and calculate the CLIP similarity score (CLIP\\({}_{score}\\)) for evaluation.\n' +
      '\n' +
      'fine-tuned on LibriSpeech, while Whisper Large V2 is evaluated in a zero-shot setting but is trained with 680,000 hours of speech. The results are shown in Table 5.\n' +
      '\n' +
      'TtsWe conduct a zero-shot Text-to-Speech (TTS) evaluation on the VCTK dataset. The results are presented in Table 4. We evaluate the TTS systems with speaker similarity and Word Error Rate (WER), where WER is focused on speech quality. More experimental details can be found in Appendix C.\n' +
      '\n' +
      '#### 5.1.3 Music\n' +
      '\n' +
      'we evaluate AnyGPT\'s performance on the MusicCaps benchmark Agostinelli et al. (2023) for both music understanding and generation tasks. We utilize the CLAP\\({}_{score}\\)Wu et al. (2022); Huang et al. (2023) score as the objective metric, which measures the similarity between the generated music and a textual description.\n' +
      '\n' +
      'For the evaluation of music captioning, we found that existing objective metrics may be limited in expressing the performance in the music captioning task. The diversity and subjectivity of music lead to varying opinions from individuals. Only specific music genres and instruments possess distinctive characteristics that can be easily recognized. While recent studies Gardner et al. (2023) have explored this issue, it remains a challenging problem to address. To ensure an objective evaluation, we compute CLAP\\({}_{score}\\) of <music, real caption> pairs and <music, generated caption> pairs for comparison. These scores are averaged across the entire test set.\n' +
      '\n' +
      '### Example Demonstrations\n' +
      '\n' +
      'After fine-tuning on the AnyInstruct-108k dataset, AnyGPT demonstrates the capability and potential in any-to-any multimodal dialogue. We provide compelling conversation examples of AnyGPT in Appendix E. These examples showcase AnyGPT is capable of comprehending and reasoning contents across various modalities in any combination. Specifically, AnyGPT can comprehend instructions interwoven with multiple modalities, including text, voice, images, and music, and can adeptly select the appropriate multimodal combination for its reply. The two-stage framework of semantic-acoustic hierarchical modeling empowers AnyGPT to generate voice responses that matches the timbre and emotion of a 3-second speech prompt. For additional examples and to experience the speech and music content, we highly recommend visiting the demo page.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this work, we introduced AnyGPT, an any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. Discrete multimodal representations facilitate a seamless integration of new modalities--comparable to incorporating a foreign language--without necessitating alterations to the existing LLM architecture or training paradigms. To equip the model to handle arbitrary combinations of multimodal inputs and outputs, we synthesize the first large-scale any-to-any multimodal instruction dataset, AnyInstruct-108k, consisting of multi-turn conversations that intricately interweave various modalities. Experimental results indicate that AnyGPT achieves promising results in various\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & CLAP\\({}_{score}\\uparrow\\) \\\\ \\hline _Music understanding_ & \\\\ <music, real caption> & 0.16 \\\\ <music, generated caption> & 0.11 \\\\ \\hline _Music generation_ & \\\\ Riftsson Forsgren and Martiros (2022) & 0.19 \\\\ Mousai Schneider et al. (2023) & **0.23** \\\\ AnyGPT & 0.14 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Comparison results for music understanding and generation tasks. A metric scoring the alignment between music and textual captions is reported (CLAP\\({}_{score}\\)). For music captioning, the CLAP\\({}_{score}\\) of <music, real caption> pair and <music, generated caption> pair are computed for comparison.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & WER \\(\\downarrow\\) & SIM \\(\\uparrow\\) \\\\ \\hline _Ground Truth_ & _1.9_ & _0.93_ \\\\ VALL-E Wang et al. (2023) & 7.9 & 0.75 \\\\ USLM Zhang et al. (2023) & **6.5** & **0.84** \\\\ \\hline AnyGPT & 8.5 & 0.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Comparison results on TTS task.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & WER \\(\\downarrow\\) \\\\ \\hline Human-level Amodei et al. (2016) & 5.8 \\\\ Wav2vec 2.0 Baevski et al. (2020) & 2.7 \\\\ Whisper Large V2 Radford et al. (2022) & **2.7** \\\\ \\hline AnyGPT & 8.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Comparison results on ASR task. We use Word Error Rate (WER) as the metric.\n' +
      '\n' +
      'cross-modal tasks and demonstrates that discrete representations can effectively and conveniently unify multiple modalities within a unified large language model.\n' +
      '\n' +
      '## Limitations and Future Work\n' +
      '\n' +
      'Any-to-Any Multimodal LLM BenchmarkThe domain of any-to-any multimodal large language models (LLMs) is an emerging field of research. However, the lack of a dedicated benchmark to evaluate the models\' capabilities across multiple dimensions, as well as to mitigate potential risks, presents a considerable challenge. Consequently, the development of a comprehensive benchmark is imperative.\n' +
      '\n' +
      'Enhancing LLMsAlthough the multimodal LLMs with discrete representations can be trained stably, a higher loss is observed compared to unimodal training, preventing optimal performance in each modality. Potential strategies to improve multimodal fusion could involve scaling LLMs and tokenizers or adopting a Mixture-Of-Experts (MOE) architecture to better manage diverse data and optimize performance.\n' +
      '\n' +
      'Better TokenizerIn multimodal LLMs employing discrete representations, the tokenizer\'s quality sets a ceiling for the model\'s comprehension and generative potential. Enhancing the tokenizer can be approached from various angles, including the adoption of superior codebook training methods, the development of more cohesive multimodal representations, and the application of information disentanglement across various modalities.".\n' +
      '\n' +
      'Longer ContextMultimodal content, such as images and audio, often spans extensive sequences. AnyGPT, for instance, limits music modeling to 5 seconds, significantly restricting the practical usefulness of its audio output. Moreover, for any-to-any multimodal dialogue, an extended context allow for a higher number of conversational exchanges, thereby enriching the interaction\'s depth and complexity.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Laion-aesthetics (2022) 2022b. Laion coco: 600m synthetic captions from laion2b-en. [https://laion.ai/blog/laion-coco/](https://laion.ai/blog/laion-coco/).\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _ArXiv preprint_, abs/2303.08774.\n' +
      '* Agostinelli et al. (2023) Andrea Agostinelli, Timo I. Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and Christian Havno Frank. 2023. Musiclm: Generating music from text. _ArXiv preprint_, abs/2301.11325.\n' +
      '* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahani Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning. _ArXiv preprint_, abs/2204.14198.\n' +
      '* Amodei et al. (2016) Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. In _International conference on machine learning_, pages 173-182. PMLR.\n' +
      '* Ardila et al. (2020) Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: A massively-multilingual speech corpus. In _Proceedings of the Twelfth Language Resources and Evaluation Conference_, pages 4218-4222, Marseille, France. European Language Resources Association.\n' +
      '* Baevski et al. (2020) Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntao Zhuang, Joyce Lee, Yufei Guo, et al. 2023. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8.\n' +
      '* Borsos et al. (2023a) Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. 2023a. Audiolm: a language modeling approach to audio generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_.\n' +
      '* Borsos et al. (2022) Zalan Borsos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi.\n' +
      '\n' +
      '2023b. Soundstorm: Efficient parallel audio generation. _ArXiv preprint_, abs/2305.09636.\n' +
      '* 3 September 2021_, pages 3670-3674. ISCA.\n' +
      '* Copet et al. (2023) Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. 2023. Simple and controllable music generation. _ArXiv preprint_, abs/2306.05284.\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructible: Towards general-purpose vision-language models with instruction tuning. _ArXiv preprint_, abs/2305.06500.\n' +
      '* D\'efossez et al. (2022) Alexandre D\'efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. _ArXiv preprint_, abs/2210.13438.\n' +
      '* Dong et al. (2023) Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jian-Yuan Sun, Hongyu Zhou, Hao-Ran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. 2023. Dream-llm: Synergistic multimodal comprehension and creation. _ArXiv preprint_, abs/2309.11499.\n' +
      '* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net.\n' +
      '* Stable diffusion for real-time music generation.\n' +
      '* Gardner et al. (2023) Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M. Bittner. 2023. Llark: A multimodal instruction-following language model for music.\n' +
      '* Ge et al. (2023a) Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. 2023a. Planting a seed of vision in large language model. _ArXiv preprint_, abs/2307.08041.\n' +
      '* Ge et al. (2023b) Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. 2023b. Making llama see and draw with seed tokenizer. _ArXiv preprint_, abs/2310.01218.\n' +
      '* Huang et al. (2023) Rongjie Huang, Jia-Bin Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiaoyue Yin, and Zhou Zhao. 2023. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. _ArXiv preprint_, abs/2301.12661.\n' +
      '* Koh et al. (2023) Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. 2023. Generating images with multimodal language models. _ArXiv preprint_, abs/2305.17216.\n' +
      '* Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International Conference on Machine Learning_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In _European Conference on Computer Vision_.\n' +
      '* Lu et al. (2023) Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. 2023. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. _ArXiv preprint_, abs/2312.17172.\n' +
      '* Microsoft azure text-to-speech api. [https://azure.microsoft.com/en-us/products/ai-services/ai-speech](https://azure.microsoft.com/en-us/products/ai-services/ai-speech).\n' +
      '* Pan et al. (2023) Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. 2023. Journeydb: A benchmark for generative image understanding. _ArXiv preprint_, abs/2307.00716.\n' +
      '* Panayotov et al. (2015) Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In _2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015_, pages 5206-5210. IEEE.\n' +
      '* Pratap et al. (2020) Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. MLS: A large-scale multilingual dataset for speech research. In _Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020_, pages 2757-2761. ISCA.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.\n' +
      '* Rongjie et al. (2020)Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision. _ArXiv preprint_, abs/2212.04356.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer.\n' +
      '* Schneider et al. (2023) Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. 2023. Mousai: Text-to-music generation with long-context latent diffusion. _ArXiv preprint_, abs/2301.11757.\n' +
      '* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294.\n' +
      '* Sun et al. (2023a) Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yuze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2023a. Generative multimodal models are in-context learners. _ArXiv preprint_, abs/2312.13286.\n' +
      '* Sun et al. (2023b) Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yuzeze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2023b. Generative pretraining in multimodality. _ArXiv preprint_, abs/2307.05222.\n' +
      '* Tang et al. (2023a) Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, and Mohit Bansal. 2023a. Codiz-2: In-context, interleaved, and interactive any-to-any generation. _ArXiv preprint_, abs/2311.18775.\n' +
      '* Tang et al. (2023b) Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. 2023b. Any-to-any generation via composable diffusion. _ArXiv preprint_, abs/2305.11846.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhahiri, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajibar Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ArXiv preprint_, abs/2307.09288.\n' +
      '* van den Oord et al. (2017) Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 6306-6315.\n' +
      '* Wang et al. (2023) Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. 2023. Neural codec language models are zero-shot text to speech synthesizers. _ArXiv preprint_, abs/2301.02111.\n' +
      '* Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. _ArXiv preprint_, abs/2212.10560.\n' +
      '* Wu et al. (2023) Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. Next-gpt: Any-to-any multimodal llm. _ArXiv preprint_, abs/2309.05519.\n' +
      '* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5.\n' +
      '* Zeghidour et al. (2021) Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. Soundstream: An end-to-end neural audio codec. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 30:495-507.\n' +
      '* Zhang et al. (2023a) Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023a. SpeechGpt: Empowering large language models with intrinsic cross-modal conversational abilities. In _Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Zhang et al. (2023b) Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. 2023b. Speechtokenizer: Unified speech tokenizer for speech large language models. _ArXiv preprint_, abs/2308.16692.\n' +
      '* Zhu et al. (2023) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2023. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _ArXiv preprint_, abs/2304.06939.\n' +
      '\n' +
      '## Appendix A pretraining\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '### pre-training\n' +
      '\n' +
      'We employ various templates to construct multimodal sentences, ensuring a diverse spectrum within our pre-training data. Each non-text modality content is identified by special tokens placed at both the beginning and end. Typically, the paired data comprises a non-text modality (X) - such as images, speech, or music - and its corresponding text, which could be a caption or transcription. We prompt OpenAI GPT-4 to generate hundreds of bidirectional instructions, specifically X-to-text or text-to-X such as "Please generate an image based on the provided text." Given a token sequence (S) and related text (T), we randomly pick a generation direction alongside an instruction (I) from our pre-established pool, forming a triplet (I, S, T). This triplet is then incorporated into a sequence using the template\n' +
      '\n' +
      '**[Human]: \\(\\{I\\}.\\{S\\}\\)**ceoh>. [AnyGPT]: \\(\\{T\\}\\)**ceos>.** or its variant\n' +
      '\n' +
      '**[Human]: \\(\\{I\\}\\)**. This is input: \\(\\{T\\}\\)**ceoh>. [AnyGPT]: \\(\\{S\\}\\)**ceos>.**, depending on the generation direction.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline \\multicolumn{1}{c}{Modality} & Dataset & Description & Sample Rate \\\\ \\hline Interleaved & \\multirow{2}{*}{MMC4-core-ff} & 101M image-interleaved documents collected from Common Crawl. & \\multirow{2}{*}{0.05} \\\\ Image-Text & & We use the mmc4-core split which is consist of 7.3M documents. & \\\\ \\hline \\multirow{4}{*}{Image-Text} & Laion-2B & 2B image-text pairs from web. & \\\\ \\cline{2-3}  & LAION-COCO & 600M image-text pairs, where the caption is generated by BLIP. & \\\\ \\cline{2-3}  & JourneyDB & 4429K Midjourney images, with image caption. & \\\\ \\cline{2-3}  & LAION-Aesthetics & Several collections of subsets from LAION 5B with high visual quality. & \\\\ \\hline \\multirow{4}{*}{Speech-Text} & Multilingual Librispecch & Processing audiobooks read from LibriVox, & \\multirow{2}{*}{0.13} \\\\  & & we used a 44,000-hour subset of English. & \\\\ \\cline{2-3}  & CommonVoice & Microphone recordings from internet volunteers, & \\multirow{2}{*}{0.27} \\\\  & & of which we used a 3000-hour subset of English. & \\\\ \\cline{2-3}  & GigaSpeech & 10,000 hours of English voice data sourced & \\\\ \\cline{2-3}  & & from audiobooks, podcasts, and YouTube videos. & \\\\ \\hline \\multirow{2}{*}{Music-Text} & Youtube-Music-1M & 100M music-text pairs from Youtube. & \\multirow{2}{*}{0.25} \\\\ \\cline{2-3}  & MusicGen-Synthesis & 20k music-text pairs extracted & \\\\ \\cline{1-1} \\cline{2-3}  & from the AnyInstruct-108k dataset, synthesized by MusicGen. & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Details of data used in pre-training stage.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Pre-training Stage & Fine-tuning Stage \\\\ \\hline Gradient clipping & 1.0 & 1.0 \\\\ (Global-norm) & 480 & 64 \\\\ Max length & 4500 & 4500 \\\\ Training steps & 81000 & 5000 \\\\ Learning rate scheduler & cosine & cosine \\\\ Peak learning rate & 6e-5 & 2e-5 \\\\ Warmup ratio & 0.03 & 0.03 \\\\ Optimizer & Adam & Adam \\\\ GPU & A100 & A100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Training hyperparameters used in experiments.\n' +
      '\n' +
      'For interleaved multimodal data, like a web document with interspersed images and text, we directly replace non-text content with the corresponding tokens sequence as they naturally form sentences.\n' +
      '\n' +
      'As most of the image and music data are sourced from the web, there is a certain level of noise that can affect the quality of multimodal generation. Consequently, after the initial pre-training, we selectively utilized high-quality datasets--JourneyDB and LAION-Aesthetics for text-to-image generation, and LAION-COCO for image captioning. For music data, we incorporated the AnyInstruct-108k dataset. The remaining data were kept unchanged, and we continued to pre-train the model for an additional 4000 steps.\n' +
      '\n' +
      'We report the detailed training hyperparameters of AnyGPT in Tab 8.\n' +
      '\n' +
      '## Appendix B Instruction Tuning\n' +
      '\n' +
      '## Appendix C Evaluation\n' +
      '\n' +
      'We conduct a zero-shot Text-to-Speech (TTS) evaluation on the VCTK dataset. There is no overlap in speakers between our training data and the VCTK dataset. We randomly select a 3-second clip from each speaker as the vocal prompt along with a separate text as input.\n' +
      '\n' +
      'The results can be found in Table 4. We evaluate the TTS systems with speaker similarity and WER. To evaluate the speaker similarity between the generated speech and the prompt speech, we employ WavLM-TDNN2. It can generate speaker embeddings for both the generated speech and the prompt\n' +
      '\n' +
      'Figure 4: An example of an AnyGPT multimodal dialogue: the input is an image and a voice command to generate music. The output is music that meets the requirements, along with corresponding text and voice responses. All data are processed into discrete tokens and are autoregressively processed by the LLM.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline Generated Modality & Text & Image & Speech & Music \\\\ \\hline Decoding Strategy & Beam Search & Sampling & Sampling & Sampling \\\\ Beam size & 5 & - & - & - \\\\ Top-P & - & 0.7 & 0.7 & 1.0 \\\\ Repetition Penalty & 1.0 & 1.0 & 1.0 & 1.15 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Details of generation decoding strategies used in evaluation.\n' +
      '\n' +
      'speech, then compute the cosine similarity between these embeddings. WER is calculated using the Whisper medium model to transcribe the generated speech, with lower WER indicating higher quality of the synthesized speech.\n' +
      '\n' +
      'We compare our model with VALL-E and USLM, both of which employ two autoregressive models for speech modeling. They utilize Encodec and SpeechTokenizer, respectively, as speech tokenizers.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'You are helping me to write conversations about a user talking to a chatbot named AnyGPT.\n' +
      '\n' +
      'In the conversations, both the user can provide images or music to help express her/his needs and ideas. And the chatbot MMGPT can also respond to the user with images or music in its utterances.\n' +
      '\n' +
      'The images and music in the chat are in the form of image descriptions and music descriptions like [image: description] and [music: description], respectively. The user should provide images and music in this format and the chatbot will respond to the user like this as well.\n' +
      '\n' +
      'Note that at most one music appears in one conversation and the description of music should be straightforward, focusing on genres and instruments, and never mention a known music directly.\n' +
      '\n' +
      'Before each conversation, I will first show you a scenario and you can start writing about the chat.\n' +
      '\n' +
      'Here is an example:\n' +
      '\n' +
      '--\n' +
      '\n' +
      '{demonstrations}\n' +
      '\n' +
      '--\n' +
      '\n' +
      'Now it\'s your turn for the next conversation. You only need to answer following the format in which the user and AnyGPT take turns.\n' +
      '\n' +
      'The conversation should be consistent with the introduction to the scenario.\n' +
      '\n' +
      'Remember that the utterances should be concise, try to use 5-15 words per utterance.\n' +
      '\n' +
      'Note that: the user utterance should always be a question or instruction.\n' +
      '\n' +
      'In some turns, the user provides an image or a piece of music and asks a question or makes an instruction to AnyGPT relating to the provided image or music.\n' +
      '\n' +
      'In other turns, the user requests AnyGPT to generate the required images or music.\n' +
      '\n' +
      'Note that: the description of music should focus on genres, style, and instruments. And make the description of images and music within [image: ] or [music: ] more detailed.\n' +
      '\n' +
      'Note that: never directly include a famous person\'s name in the image descriptions or mention a piece of known music in the music description.\n' +
      '\n' +
      'Tips: when the user asks to convert between images and music, AnyGPT should first utter his understanding of the input image or music before generating the requested result.\n' +
      '\n' +
      'Keep the dialog in 2 or 3 rounds.\n' +
      '\n' +
      'Each dialog should include one music and at most 2 images.\n' +
      '\n' +
      '--\n' +
      '\n' +
      'In this conversation, {new_scenario_description}\n' +
      '\n' +
      '**GPT4:**\n' +
      '\n' +
      '{A synthetic chat according to the scenario description.}\n' +
      '\n' +
      'Figure 7: Prompts for writing chat content. For each API call, we sample 3 demonstrations. Each demonstration contains a scenario description, as the {new_scenario_description}, and the corresponding chat.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      'Figure 9: Speech Instruction + Image \\(\\rightarrow\\) Text + Music + Speech Response\n' +
      '\n' +
      'Figure 10: Speech Instruction + Music \\(\\rightarrow\\) text + Music + Speech Response\n' +
      '\n' +
      'Figure 11: Speech Instruction + Image \\(\\rightarrow\\) text + Music + Speech Response\n' +
      '\n' +
      'Figure 12: Text \\(\\rightarrow\\) Image + Music\n' +
      '\n' +
      'Figure 13: Text + Image \\(\\rightarrow\\) Music\n' +
      '\n' +
      'Figure 14: Text + Image \\(\\rightarrow\\) Text + Music\n' +
      '\n' +
      'Figure 15: Text + Music \\(\\rightarrow\\) Text + Image\n' +
      '\n' +
      'Figure 16: Text + Music \\(\\rightarrow\\) Muisc\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>