<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# EasyQuant: LLMs를 위한 효율적인 데이터 프리 양자화 알고리즘\n' +
      '\n' +
      ' 한림탕\n' +
      '\n' +
      'ranchotang@tencent.com\n' +
      '\n' +
      '&Yifu Sun\n' +
      '\n' +
      'yifusun@tencent.com\n' +
      '\n' +
      '&Decheng Wu\n' +
      '\n' +
      'woodchenwu@tencent.com\n' +
      '\n' +
      '&Kai Liu\n' +
      '\n' +
      'raccoonliu@tencent.com\n' +
      '\n' +
      '&Jianchen Zhu\n' +
      '\n' +
      'dickzhu@tencent.com\n' +
      '\n' +
      '&Zhanhui Kang\n' +
      '\n' +
      'kegokang@tencent.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델(LLM)은 다양한 작업에서 기존의 방법보다 매우 우수한 것으로 입증되었다. 그러나, 그들의 값비싼 계산과 높은 메모리 요구 사항은 배포에 금지되어 있다. 모델 양자화는 이러한 오버헤드를 줄이기 위한 효과적인 방법이다. 문제는 대부분의 이전 작업에서 양자화된 모델이 훈련 데이터의 몇 가지 샘플을 사용하여 보정되었으며, 이는 양자화된 LLM을 알려지지 않은 사례 및 작업으로 일반화하는 데 영향을 미칠 수 있다는 것이다. 따라서 본 연구에서는 LLM의 일반화 성능을 보장하기 위해 데이터 없는 양자화 방법을 설계할 수 있는가?\n' +
      '\n' +
      '본 논문에서는 LLM을 위한 훈련 없이 데이터를 사용하지 않는 가중치 전용 양자화 알고리즘인 EasyQuant를 제안한다. 우리의 관찰은 두 가지 요인, 즉 가중치와 양자화 범위의 이상치가 양자화 오류를 줄이는 데 필수적이라는 것을 나타낸다. 따라서, EasyQuant에서는 이상치(1\\(1\\%\\) 이하)를 변경하지 않고 양자화 범위를 최적화하여 복원 오차를 줄인다. 이러한 방법을 사용하여 놀랍게도 이지퀀트가 원래 모델과 유사한 성능을 달성한다는 것을 발견했다. EasyQuant는 어떠한 훈련 데이터에도 의존하지 않기 때문에 양자화된 LLM의 일반화 성능은 안전하게 보장된다. 또한, EasyQuant는 100B 이상의 LLMs에 대해서도 몇 분 내에 양자화된 모델이 달성될 수 있도록 병렬로 구현될 수 있다. 우리가 아는 한, 우리는 데이터가 없는 환경에서 데이터 종속 알고리즘과 비교할 수 있는 성능을 달성하는 첫 번째 작업이며, 우리의 알고리즘은 데이터 종속 방법보다 10배 이상 더 빨리 실행됩니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 연구는 트랜스포머 Vaswani et al. (2017) 기반 LLMs (Workshop, 2023; Zhang et al., 2022; Touvron et al., 2023; Brown et al., 2020; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Zeng et al., 2022)의 우수한 성능을 전통적인 방법에 비해 다양한 작업에서 입증했으며, 이러한 LLMs를 개선하고 활용하는 방법에 대한 엄청난 관심을 끌고 있다. 그러나 성능 향상과 함께 모델 사이즈도 비약적으로 성장합니다. 따라서 메모리 공간과 계산 비용은 이러한 모델을 배포하는 데 병목 현상이 된다. 이러한 오버헤드를 완화하기 위한 하나의 유망한 해결책은 모델 양자화이다(Frantar et al., 2023; Xiao et al., 2023). 여기서 우리는 메모리 소비 및 계산 비용을 감소시키기 위해 가중치만을 양자화하거나 가중치 및 활성화 둘 모두를 양자화한다.\n' +
      '\n' +
      '모델 양자화는 BERT(Devlin et al., 2018) 및 GPT-2(Radford et al., 2019)와 같은 정상 크기의 모델들에 대해 잘 연구된 영역이지만, LLM들에 대해서는 여전히 상당히 어려운 과제이다. 한 가지 주요 이유는 이전의 무손실 모델 양자화 알고리즘들이 양자화된 모델에 대한 재교육을 요구하기 때문이며, 이는 수십억 개의 파라미터들 이상의 모델들에 대해 너무 비싸다. 이 외에도 이전 모델은 일반적으로 특정 도메인 작업에 대해 설계되며, 이는 훈련 데이터가 제한된 작업 도메인에서 샘플링된다는 것을 의미한다. 그러나, 최근 LLM은 일반적으로 데이터 코퍼스의 다양한 도메인에 대해 훈련되며, 다중 도메인 제로 샷 작업에 상당히 효과적인 것으로 나타났다. 이 경우 부분 도메인 코퍼스를 사용하여 양자화된 LLMs만 재교육하면 LLMs의 일반화 능력이 더 나빠질 수 있다. 따라서 LLMs 양자화 알고리즘을 설계하기 위해서는 효율성과 일반화 보장 모두 매우 중요하다. 현재까지, 저-비트 가중치-전용 양자화를 위해, 몇몇의 사후-트레이닝 알고리즘이 제안되었다(Frantar et al., 2023; Yao et al., 2022). 그러나, 이러한 방법들은 또한 훈련 데이터로부터 샘플링된 작은 교정 세트를 필요로 하며, 이는 여전히 적어도 몇 시간이 걸린다. 또한 이러한 보정 데이터를 사용하면 모형이 보정 세트에 과도하게 적합될 위험도 있습니다.\n' +
      '\n' +
      '본 논문에서는 저비트 양자화된 LLM의 성능을 향상시킬 수 있는 새로운 데이터 없는 모델 양자화 알고리즘인 EasyQuant를 제안한다. EasyQuant는 입력 데이터가 필요하지 않기 때문에 LLM의 일반화 능력은 본질적으로 보장된다. EasyQuant를 몇 분 동안만 실행함으로써 공개가 가능한 OPT-176B, BLOOM-176B 및 LLAMA-65B를 다양한 벤치마크에서 큰 손실 없이 하위 비트로 양자화할 수 있다. 우리가 아는 한, 이것은 주목할만한 시스템 오버헤드 없이 LLM 양자화를 위한 최초의 데이터 없는 LLM 양자화 알고리즘이다.\n' +
      '\n' +
      '또한, 본 연구는 양자화된 LLM의 성능 저하를 야기하는 중요한 요인을 밝혀낸다. 우리는 가중치의 특이치가 정규 요소에 비해 모델의 성능에 더 중요하다는 것을 보여준다. 이 외에도 양자화 범위를 최적화하기 위해 기울기 기반 방법을 사용할 것을 제안한다. 이 두 가지 전략은 가중치-활성화 양자화 및 양자화-인식 트레이닝(QAT)과 같은 다른 시나리오에서도 사용될 수 있다.\n' +
      '\n' +
      '마지막으로, 역양자화에서 이상치 분리를 위한 효율적인 CUDA 커널을 개발하였으며, 가중치 미양자화에서 \\(1\\%\\) 이상치를 유지하는 것이 전체 지연 시간에 대해 무시할 수 있는 (0.1\\%\\보다 작은) 오버헤드를 가져옴을 증명하였다. 또한 모델의 각 가중치를 양자화하기 위해 EasyQuant를 병렬로 구현하는 것을 제안하며, 이는 175B 크기의 모델이 \\(10\\)분 이내에 \\(4\\)-비트로 양자화될 수 있음을 의미한다.\n' +
      '\n' +
      '##2 배경 및 동기\n' +
      '\n' +
      '가장 널리 사용되는 양자화 방법, 즉 가장 가까운 숫자에 대한 반올림(**RTN**)은 텐서\\(\\mathbf{x}\\)을 \\(k\\)비트 표현으로 양자화한다.\n' +
      '\n' +
      '\\[Q[\\mathbf{x}]=s\\times\\Big{\\lfloor}\\text{clamp}\\left(\\frac{\\mathbf{x}\\s},l_{\\min},l_{\\max}\\right)\\Big{\\rfloor}\\tag{1}\\\n' +
      '\n' +
      '여기서 \\(s\\)은 양자화 스케일이고 \\(l_{\\min}\\)과 \\(l_{\\max}\\)은 클리핑의 하한과 상한이며 \\(\\lfloor\\cdot\\rceil\\)은 라운딩 연산자이다. 일반적으로 \\(l_{\\min}=\\left(-2^{k-1}+1\\right)\\)와 \\(l_{\\max}=2^{k-1}\\)을 설정하고 \\(\\mathbf{x}\\)에서 \\(s\\)을 최대 절대값으로 설정한다.\n' +
      '\n' +
      '가중치 전용 LLM 양자화에서 최상의 구성을 찾기 위한 두 가지 주요 방향이 있다. 첫 번째는 가중치 파라미터(\\(W\\)로 표기됨)의 재구성 오차를 최소화하는 것으로, 이는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[r(W):=\\|Q[W]-W\\|^{2}.\\]\n' +
      '\n' +
      '이 경우 가중치 자체에 액세스하기만 하면 되므로 데이터가 없습니다.\n' +
      '\n' +
      '이를 넘어, 최근 연구들(Frantar et al., 2023; Yao et al., 2022)은 출력 에러를 사용하도록 제안하며, 이는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[e(W)=\\sum_{X\\in\\mathcal{D}}\\left\\|Q[W]X-WX\\right\\|^{2},\\]\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}\\)는 최적화를 위해, 원래의 트레이닝 데이터로부터 샘플링된 캘리브레이션 세트이다. 이 규정은 원래 모델의 출력을 직접 모방하여 재구성 기반 방법보다 더 유망한 결과를 달성하려고 한다.\n' +
      '\n' +
      '데이터 의존적 보정은 LLM의 일반화 능력을 약화시킬 수 있지만, 보정 데이터를 사용하여 얻은 성능 증가는 보정 세트에 모델이 과도하게 적합될 위험을 초래하기 때문에 양자화된 모델의 일반화를 위태롭게 할 수 있다. 예를 들어,\n' +
      '\n' +
      '도 1: 이지퀀트의 파이프라인. 우리는 먼저 무게에서 모든 특이치를 찾고 완전한 정밀도(fp32/fp16/bf16)로 유지한다. 그 후, 양자화 범위(\\(q_{range}\\)로 표기)를 최적화하여 정규값을 보다 정확하게 근사화한다. 결국, 정규값은 최적화된 양자화 범위를 갖는 더 낮은 비트(\\(Q[\\cdot]\\)로 표현됨)로 양자화되고, 이상치들은 가중치에서 변하지 않게 설정된다.\n' +
      '\n' +
      'ZeroQuant와 GPTQ는 모두 출력 오류를 최소화하기 위해 훈련 또는 OBS에 의해 원래 가중치를 변경하는 것을 포함하며, 따라서 가중치의 파라미터 분포가 원래로부터 벗어날 수 있다. 교정 데이터는 보통 몇몇 특정 도메인들로부터 샘플링되기 때문에, 다른 태스크들에 대한 교정 모델의 성능은 보장되지 않을 수 있다.\n' +
      '\n' +
      '데이터 프리 양자화는 어렵지만, 양자화된 모델을 간접적으로 최적화할 수 있기 때문에 재구성 오류를 규정으로 사용하는 것이 더 어렵지만, 훈련 데이터를 사용하지 않기 때문에 데이터 프리 양자화를 사용할 때 모델의 일반화 능력이 본질적으로 보장되기 때문에 연구에 매우 중요한 방향이다. 따라서 본 논문에서는 다음과 같은 질문에 답하고자 한다.\n' +
      '\n' +
      '입력 데이터를 사용하지 않고 양자화된 모델의 성능을 효율적으로 복구할 수 있는 방법은?_\n' +
      '\n' +
      '본 연구에서는 데이터 없는 환경에서 양자화된 LLM의 성능을 크게 향상시킬 수 있는 데이터 없는 고속 알고리즘인 EasyQuant를 제안하고, 더 중요한 것은 데이터 종속 양자화 알고리즘의 결과보다 우수하다는 것이다. 실험 결과, 하위 비트(예: \\(4\\)-비트)의 성능 차이가 두 가지 요인으로부터 LLMs 기원을 양자화하였다.\n' +
      '\n' +
      '1. 양자화 범위를 가중치의 최대 절대값으로 설정하면 저비트 양자화에 대한 큰 복원 오차를 유도한다.\n' +
      '2. 가중치 행렬에서 모수의 \\(0.1\\%\\)이하를 차지하는 이상치들은 모델의 성능에 매우 중요한 영향을 미친다.\n' +
      '\n' +
      'EasyQuant에서는 이 두 가지 문제를 해결하기 위해 양자화 범위 최소화와 이상값 분리를 사용하며, 우리의 결과는 EasyQuant가 RTN에 비해 상당한 개선을 달성한다는 것을 증명한다.\n' +
      '\n' +
      '간단한 양자 뒤에 숨겨진 통찰력\n' +
      '\n' +
      '전술한 바와 같이, 가중치의 이상치 및 양자화 범위는 양자화된 모델의 성능에 필수적이다. 아래에서는 지원 실험을 자세히 설명한다.\n' +
      '\n' +
      '### gradient를 이용하여 양자화 범위를 효율적으로 최적화할 수 있다\n' +
      '\n' +
      '양자화 연산 자체는 미분가능하지 않지만, 복원 오차의 기울기(\\(\\|Q[\\mathbf{x}]-\\mathbf{x}\\|^{2}\\)) w.r.t. 양자화 범위\\(s\\)는 대부분의 경우에 미분가능하다. 우리는 양자화 범위 \\(s\\)의 기울기가 인정된다는 것을 증명했다.(더 자세한 내용은 섹션 4 참조)\n' +
      '\n' +
      '\\frac{\\partial\\|Q[\\mathbf{x}]-\\mathbff{x}\\|^{2}{\\partial s}=2\\sum_{i}\\left(\\left(Q[x_{i}]-x_{i}\\right)\\Big{\\lfloor}\\frac{x_{i}\\Big{\\rfloor}\\right)\\tag{2}\\tag}\\tag}\\tag}\\tag}\\tag}\n' +
      '\n' +
      '이러한 그래디언트로, 재구성 에러는 수백 단계 내에서 신속하게 최소화될 수 있다(보다 상세한 내용은 도 2 참조). 이 결과는 양자화 범위를 축소함으로써 가중치의 대부분의 파라미터들이 보다 정밀하게 근사화될 수 있음을 나타낸다. 그러나, 도 2에 도시된 바와 같이, 양자화 가중치의 성능은 더욱 악화된다\n' +
      '\n' +
      '그림 2: 재구성 오류가 작을수록 더 나은 모델 성능을 보장할 수 없습니다. 양자화 범위를 바로 축소하면 대부분의 이상치가 매우 작아지므로 이상치가 모델의 성능을 보존하는 데 중요하기 때문에 복잡도가 크게 증가한다. 그러나 이러한 이상치를 양자화하지 않을 때, 양자화 모델은 재구성 오차가 지속적으로 감소함에 따라 더 좋은 성능을 보인다. 이 결과는 가중치에서 이상치가 정상치보다 더 중요하다는 것을 분명히 시사하며, (2)에서 정의된 그래디언트를 사용하여 양자화 범위를 최적화하면 양자화된 모델의 정확도를 크게 높일 수 있다. 실험에 대한 자세한 내용은 5절에서 확인할 수 있다.\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 영상 복호화 방법. 이것은 매우 직관적이지 않은 결과이다.\n' +
      '\n' +
      '심층 분석을 통해 양자화 범위를 줄일 때 양자화 범위를 벗어나는 더 두드러진 매개 변수가 제거된다는 것을 깨달았다. 대부분의 가중치는 감소된 재구성 오차로 표시된 대로 더 정확하게 근사화되지만 두드러진 매개변수는 제대로 표시되지 않는다. 이 경우 모델 성능이 심각하게 떨어짐에 따라 우리는 이러한 특이치가 모델의 성능에 대한 일반 요소보다 훨씬 더 중요하다는 것을 깨달았다.\n' +
      '\n' +
      '### 무게의 특이치는 매우 중요하지만 충분하지는 않다\n' +
      '\n' +
      '이러한 이상치의 영향에 대해 더 논의하기 전에 먼저 가중치에서 이상치를 정의하기 위한 (\\(n\\sigma\\)) 기준을 제시한다. 임의의 가중치\\(W\\)에 대해, 우리는 그것의 \\((i,j)\\)번째 수\\(W_{i,j}\\)이 (\\(n\\sigma\\)) 이상치라고 말한다.\n' +
      '\n' +
      '\\[|W_{i,j}-mean(W)|\\geq n*var(W), \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(평균(W)\\) 및 \\(var(W)\\)는 \\(W\\)의 평균 및 분산이다.\n' +
      '\n' +
      '이제 문제는 다음과 같다: _우리는 그 이상치들을 변경하지 않고 그대로 유지하고 정상 요소들을 하위 비트들로 바로 압축할 수 있는가?_ 불행히도, 우리의 결과는 이상치를 양자화에서 제외하는 것만으로는 충분하지 않다는 것을 시사한다. 표 1에 나타낸 바와 같이, fp16에 \\(1\\%\\)의 수를 보유해도 성능 격차가 여전히 존재한다. 문제는 fp16에 너무 많은 수를 보유하면 역양자화 커널의 오버헤드도 증가하여 전체 처리량이 감소한다는 것이다.\n' +
      '\n' +
      '### 이지퀀트는 잠재적으로 성능을 향상시킵니다.\n' +
      '\n' +
      '섹션 3.1 및 섹션 3.2에 도시된 바와 같이, 양자화 범위를 최적화하는 것은 클리핑된 이상치 때문에 모델의 성능 저하를 심각하게 감소시킨다. 이러한 주요 관찰은 이상치를 양자화에서 먼저 분리한 다음 나머지 요소에 대한 양자화 범위를 최적화하는 이지퀀트를 설계하도록 영감을 준다. 그림 2의 오른쪽 부분에서 볼 수 있듯이 이상치가 양자화되지 않은 상태로 유지되는 경우, 감소된 재구성 하에서 양자화된 모델의 성능은 지속적으로 증가한다. 이것은 우리가 이 전략으로 양자화된 LLM의 성능을 잠재적으로 향상시킬 수 있음을 분명히 증명한다.\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      '### (2)의 그라디언트 구동\n' +
      '\n' +
      '원래의 척도\\(s\\)가 무한히 작은 변화\\(\\Delta s\\)를 얻었다고 하자.\n' +
      '\n' +
      '\\left\\lfloor\\frac{x}{s+\\Delta s}\\right\\rceil=\\left\\lfloor\\frac{x}{s}\\right\\rceil,\\quad\\text{if}\\frac{x}{s}-\\left\\lfloor\\frac{x}{s+\\Delta s}\\right\\rceil\\neq 0.5.\\\n' +
      '\n' +
      '그러므로 우리는\n' +
      '\n' +
      '[x]=(s+\\Delta s)\\left\\lfloor\\frac{x}{s+\\Delta s}\\right\\rceil\\]\\[=(s+\\Delta s)\\left\\lfloor\\frac{x}\\right\\rceil,\\\\\n' +
      '\n' +
      '이것으로 이어진다.\n' +
      '\n' +
      '\\[\\frac{\\partial Q[x}{\\partial s}=\\frac{Q_{s+\\Delta s}[x]-Q_{s}[x}{\\delta s}=\\left\\lfloor\\frac{x}\\right\\rceil.\\\n' +
      '\n' +
      '♪ 우리에겐 ♪\n' +
      '\n' +
      '\\frac{\\partial\\|Q[\\mathbf{x}]-\\mathbf{x}{\\partial s}\\mathbf{x}]\\\\[\\frac{\\partial s}\\right\\rangle\\]\\[= 2\\left\\langle Q[\\mathbf{x}]-\\mathbf{x}\\mathbf{x}\\mathbf{x}]-\\mathbf{x}\\x},\\left\\lfloor\\frac{x_{i}\\left((Q[x_{i}]-x_{i}\\left\\lfloor\\frac{x_{i}}\\right\\rceil\\right)\\\\[= 2\\sum_{i}\\left\\lfloor\\frac{x_{i}}\\right\\rceil\\right),\\frac{\\partial Q[\\mathbf{x}\\frac{\\partial s}\\right\\rangle\\\\[= 2\\left\\lfloor\\frac{x_{i}\\right\\rceil\\right\n' +
      '\n' +
      '### Algorithm description\n' +
      '\n' +
      'EasyQuant에서 각 가중치 \\(W\\)에 대해 먼저 모든 (\\(n\\sigma\\)) 이상치를 선택하고 (3)을 사용하여 지수 \\(I^{o}(W)\\)를 저장한다. 이후, 정상 요소들에 대해, (2)에서 정의된 그래디언트들을 갖는 최적화기(우리의 경우, 우리는 예를 들어 Adam을 사용한다)를 사용하여 채널당 양자화 범위를 최적화한다. EasyQuant로부터의 최종 양자화된 가중치는 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\[Q^{EasyQuant}[W]\\] \\[= Mask^{o}(W)*W+(1-Mask^{o}(W))*Q[W], \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(Mask^{o}\\)는 다음과 같이 정의되는 마스크 텐서이다.\n' +
      '\n' +
      '[마스크^{o}_{i,j}(W)=\\left\\begin{array}{ll}1&\\text{if}(i,j)\\in I^{o}(W),\\0&\\text{if}(i,j)\\notin I^{o}(W).\\end{array}\\right.\\tag{5}\\text{if}(i,j)\\text{if}(i,j)\\notin I^{o}(W)\n' +
      '\n' +
      '이지퀀트에 대한 자세한 설명은 알고리즘 1에 나와 있다.\n' +
      '\n' +
      '## 5 Experiment\n' +
      '\n' +
      '기준: 우리는 EasyQuant를 아래의 INT4 양자화 설정에서 여러 기준들과 비교한다:\n' +
      '\n' +
      '* **RTN**: 모델의 가중치는 (1)에 따라 순진하게 양자화된다.\n' +
      '* **ZeroQuant**: The algorithm proposed in Yao et al.(2022) 저자들은 각 층을 작은 신경망으로 취급하고 원고를 교사 모델로 사용하여 양자화된 층을 증류한다. 이는 등가적으로 \\(\\sum_{\\mathbf{x}\\in\\mathcal{D}}\\|f(W^{T};\\mathbf{x})-f(W^{S};\\mathbf{x})\\|^{2}\\)를 최소화하며, 여기서 \\(x\\)은 입력 활성화, \\(W^{T}\\)은 원래 모델의 가중치, \\(W^{S}\\)은 양자화된 모델이다.\n' +
      '***GPTQ**: 이 알고리즘은 Frantar et al.(2023)에서 제안된다. 저자들은 ZeroQuant에서와 같은 목적함수 \\(\\sum_{\\mathbf{x}\\in\\mathcal{D}}\\|f(W^{T};\\mathbf{x})-f(W^{S};\\mathbf{x})\\|^{2}\\)를 사용한다. 그러나 그들은 기울기 기반 최적화기를 사용하는 대신 손실 함수를 최소화하기 위해 OBS를 사용한다.\n' +
      '\n' +
      '실험 설정.모든 모델에 대해 이상치가 모든 숫자의 \\(1\\%\\) 미만으로 계산되도록 이상치 임계값 \\(n\\in[2.5,3]\\)을 설정했다. BLOOM과 LLAMA의 경우 \\(n=3\\)을 사용한다. 양자화 범위를 최적화할 때 Adam을 최적기로 사용하고 BLOOM의 경우 학습률 \\(1e-3\\), LLAMA의 경우 \\(1e-4\\)을 설정한다. 우리는 BLOOM의 경우 \\(100\\), LLAMA의 경우 \\(500\\)의 양자화 범위를 선택한다. 정규값은 특이치가 제외된 상태에서 대칭적으로 분포하기 때문에 대칭 양자화를 사용한다. 공정한 비교를 위해, 우리는 모든 알고리즘에서 가중치에 대해 채널당 양자화를 사용한다(각 열은 하나의 공통 양자화 범위를 공유한다는 것을 의미한다).\n' +
      '\n' +
      '평가 작업들.평가 작업들에 대해, 우리는 주로 복잡성 기반 작업들에 초점을 맞추는데, 이는 이들이 모델 양자화 Frantar 등에 특히 민감한 것으로 알려져 있기 때문이다(2023). 우리가 포함하는 당혹스러운 태스크는 WikiText2 Merity et al.(2016), Penn Treebank Marcus et al.(1994) 및 C4 Raffel et al.(2020)이다. 제로샷 태스크의 결과 역시 PIQA Tata and Patel(2003), ARC Boratko et al.(2018), StoryCloze Mostafazadeh et al.(2017)과 같이 제공된다.\n' +
      '\n' +
      '구현.각 가중치를 병렬로 양자화할 수 있기 때문에 EasyQuant를 실행하기 위해 \\(8*\\) A100을 사용하고 모든 모델에 대해 \\(1\\sim 10\\)분 단위로 양자화를 완료한다. 우리는 모든 특이치에 대한 인덱스와 값을 양자화된 정규값과 함께 저장한다. 우리의 역양자화 커널은 CUDA를 사용하여 구축된다.\n' +
      '\n' +
      '### Experiment Analysis\n' +
      '\n' +
      '우리는 전체 BLOOM 및 LLAMA 모델 패밀리를 4비트로 정량화하여 LLM에 대한 연구를 집중한다.\n' +
      '\n' +
      '복잡도 기반 작업.먼저 복잡도 기반 작업을 연구합니다. LLaMA 모델에서 표 2는 EasyQuant가 대부분의 경우 GPTQ를 능가한다는 것을 보여준다. LLaMA-65B의 경우 GPTQ가 PTB에서 4.21포인트 떨어져 9\\(\\times\\) 더 작은 완전 정밀도 7B 모델보다 성능이 좋지 않은 반면 EasyQuant는 여전히 이 작업을 잘 수행한다. 다른 업무에서는 이지퀀트가 0.4∼0.7포인트만 지고 있다. BLOOM은 유사한 패턴을 나타낸다(부록의 표 10 참조): 이지퀀트는 복잡성 기반 작업에서 0.1-0.16 포인트만 떨어진다. 우리가 C4에서 우리의 방법과 GPTQ 사이의 더 작은 갭을 관찰한다는 것을 주목하라. 그것은 대부분 데이터 보정 양자화 방법으로서 GPTQ가 칼리브라에 대한 C4 데이터세트를 사용하기 때문이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Threshold \\(n\\) (BLOOM-7B) & Baseline & \\(1\\) & \\(2\\) & \\(4\\) & \\(6\\) \\\\ \\hline PPL on WikiText2 & \\(11.37\\) & \\(12.153\\) & \\(12.495\\) & \\(12.518\\) & \\(12.536\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 양자화에서 가중치의 이상치를 분리하면 모델의 성능을 높일 수 있다. 여기서 \\(n\\)은 (3)에서 정의한 이상치 기준(\\(n\\sigma\\))의 하이퍼-파라미터를 의미하며, 기준선은 양자화되지 않은 모델의 결과이다. \\(10\\%\\)(\\(n=1\\)) 숫자가 양자화되지 않은 상태에서도 기준선에는 여전히 큰 격차가 있음을 주목하라. 이것은 특이치를 분리하는 것이 양자화된 모델의 정확도를 완전히 회복하기에 충분하지 않다는 것을 의미한다.\n' +
      '\n' +
      'tions.\n' +
      '\n' +
      '제로샷 작업.대부분의 제로샷 작업의 경우 EasyQuant는 부록에서 표 10과 같이 0.1%-0.52%의 정확도로만 무해한 성능을 달성하고 대부분의 경우 GPTQ를 능가한다. 여기에서 우리는 단순히 git.1에서 LLAMA에 GPTQ의 구현을 사용한다. 우리는 이지퀀트가 미세 과립성 그룹화를 통해 더 향상될 수 있다는 점에 주목한다. 그러나 본 논문에서는 이러한 오버헤드를 포함하지 않을 것이다.\n' +
      '\n' +
      '각주 1: [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)\n' +
      '\n' +
      '실제적인 Latency.단일 A100 GPU에서 이상치 분리, \\(\\mathrm{int}4\\) 역양자화 및 행렬 곱셈의 오버헤드를 배치 크기 1, 시퀀스 길이 1024와 비교하여 EasyQuant의 오버헤드를 평가한다. 매트릭스 크기는 176B BLOOM의 첫 번째 FFN 층과 동일한 \\(14336\\times 53746\\)이다. 이상치 분리를 위해 6가지 설정(\\(0.01\\%\\), \\(0.10\\%\\), \\(0.50\\%\\), \\(1\\%\\), \\(5\\%\\), \\(10\\%\\))에서 이상치 비율의 잠복기를 테스트한다. 행렬 곱셈은 \\(8\\mathrm{ms}\\)이 소요되고 역양자화는 \\(5\\mathrm{ms}\\)이 소요된다. 따라서 표 3에서 우리는 무게의 이상치를 복구하는 것이 전체 대기 시간에 거의 오버헤드를 가져오지 않는다는 것을 알 수 있다.\n' +
      '\n' +
      '절제 연구.비구조화된 이상치의 영향을 이해하기 위해 이상치 분리 또는 양자화 범위 최적화 없이 EasyQuant의 복잡도 결과를 보여준다. 3절에서 논의한 바와 같이 두 전략 모두 최종 모델 성과에 매우 중요한 영향을 미친다.\n' +
      '\n' +
      '또한 성능 이득이 주로 이상치 분리에서 비롯되는지 여부를 증명하는 실험을 수행한다. 실제로 이상치 분리는 EasyQuant의 매우 중요한 구성요소이지만 여전히 양자화로 인한 성능 손실을 완전히 복구하기에는 충분하지 않다. fp16 이상치로 10%의 가중치라도 유지하는 것은 여전히 약 8%pp 증가를 인정하는 반면 이시퀀트는 단지 \\(1\\%\\)pp 증가만을 인정한다. 아래에서는 다양한 벤치마크에서 양자화 범위 최적화 없이 fp16에서 1% 이상치를 유지할 때 4비트 양자화된 BLLOM-7B의 결과를 제시한다.\n' +
      '\n' +
      '이상치 영향.이상치 격리는 이지퀀트의 핵심 구성요소이지만 모델 정확도에 간접적인 영향만 미칠 수 있다. 우리가 발견한 흥미로운 현상은 이상치가 게이팅 메커니즘처럼 행동한다는 것이다: 이상치 분리 없이 모델은 작은 재구성 오류에서 훨씬 더 나쁜 성능을 달성하지만 fp16에서 이러한 이상치를 유지할 때 양자화된 LLM은 작은 재구성 오류에서 지속적으로 감소된 ppl에 도달한다:\n' +
      '\n' +
      '또한 가중치 이상치의 직접적인 영향을 테스트하는 보완 실험을 수행했다. 가중치 값의 1%(크기에 따라)를 0으로 자르고 ppl 결과를 본다(표 6에 나와 있음). 그것은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline Benchmark & EasyQuant & 1\\% fp16 outlier \\\\ \\hline WikiText2(PPL) & 11.66 & 12.52 \\\\ PTB (PPL) & 21.42 & 23.32 \\\\ C4(PPL) & 15.46 & 16.44 \\\\ PIQA (ACC) & 73.61\\% & 72.74\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 이상치 격리를 단독으로 사용하는 것은 성능 손실을 완전히 복구하기에 충분하지 않다. 이지퀀트는 모든 벤치마크에서 이상치 분리를 일관되게 능가합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l} \\hline \\hline  & & \\multicolumn{3}{c}{Perplexity-based Task} & \\multicolumn{3}{c}{Perplexity-based Task} \\\\ \\cline{3-10}  & & WikiText2 & PTB & C4 & & & WikiText2 & PTB & C4 \\\\ \\hline \\multirow{4}{*}{LLAMA\\(-\\)7B} & fp16 & \\(5.68\\) & \\(8.80\\) & \\(7.08\\) & & fp16 & \\(4.10\\) & \\(7.30\\) & \\(5.98\\) \\\\  & RTN & \\(6.29\\) & \\(11.25\\) & \\(8.12\\) & & RTN & \\(4.54\\) & \\(8.65\\) & \\(6.54\\) \\\\  & GPTQ & \\(6.09\\) & \\(11.56\\) & \\(7.78\\) & & GLAPTQ & \\(4.45\\) & \\(8.44\\) & \\(6.40\\) \\\\  & EasyQuant & **6.01** & **10.72** & **7.71** & & EasyQuant & **4.34** & \\(8.45\\) & **6.37** \\\\ \\hline \\multirow{4}{*}{LLAMA\\(-\\)13B} & fp16 & \\(5.09\\) & \\(8.07\\) & \\(6.61\\) & & fp16 & \\(3.53\\) & \\(6.91\\) & \\(5.62\\) \\\\  & RTN & \\(5.53\\) & \\(9.77\\) & \\(7.23\\) & & RTN & \\(3.99\\) & \\(10.67\\) & \\(6.45\\) \\\\ \\cline{1-1}  & GPTQ & \\(5.36\\) & \\(9.49\\) & \\(7.07\\) & & GLAPTQ & \\(4.13\\) & \\(11.12\\) & \\(6.38\\) \\\\ \\cline{1-1}  & EasyQuant & **5.29** & **9.37** & **6.97** & & EasyQuant & **3.98** & **9.61** & **6.30** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: LLAMA 모델 패밀리에 대한 복잡도 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline outlier ratio & overhead \\\\ \\hline \\(0.01\\%\\) & 0.027ms \\\\ \\(0.10\\%\\) & 0.055ms \\\\ \\(0.50\\%\\) & 0.093ms \\\\ \\(1\\%\\) & 0.117ms \\\\ \\(5\\%\\) & 0.186ms \\\\ \\(10\\%\\) & 0.212ms \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: A100에 대한 이상치 분리 오버헤드에서 가장 큰 값(이상치)이 정규 값(중앙값)과 모델 성능에 동일한 영향을 미치는 것으로 나타났으며, 이는 해당 이상치가 정규 값과 모델 정확도에 동일한 직접적인 영향을 공유한다는 것을 의미한다. 따라서 이상치 격리는 간접적으로 모델 정확도에 중요한 영향을 미칩니다.\n' +
      '\n' +
      '이상치 분포.또한 서로 다른 모듈 및 도면층을 따라 이상치 분포를 탐색합니다. 이상치의 분율이 상이한 모듈 및 계층에서 상이한 패턴을 공유한다는 것을 보여준다(표 7 및 8에 나타낸 바와 같이). FFN.2는 특이치의 비율이 상당히 높습니다. 그러나 레이어 인덱스에 따라 패턴을 나타내지 않습니다.\n' +
      '\n' +
      '양자화 범위.양자화 범위의 동역학은 표 9에 나타나 있다. 대략적으로, 이 범위는 훈련 초기에 빠르게 감소하며, 이는 더 작은 양자화 범위가 양자화될 파라미터들의 대부분을 더 정밀하게 만들 것이라는 것을 의미한다. 특정 단계의 훈련 후에 양자화 범위가 안정화되고, 이것은 우리가 이미 최적의 범위를 달성했다는 것을 의미한다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      'Model QuantizationTraditional Model 양자화 알고리즘은 주로 모델의 파라미터와 액티베이션이 모두 양자화된 경우에 초점을 맞춘다(Lin et al., 2015; Hubara et al., 2016; Tailor et al., 2021; Ni et al., 2020). 그러나, 모델을 직접 양자화하는 것은 모델들의 정확도를 크게 저하시킬 것이며, 성능을 향상시키기 위한 하나의 중요한 기술은 양자화 인식 트레이닝(Quantization Aware Training; QAT)(Jacob et al., 2018)이며, 여기서 양자화된 모델의 정확도를 더욱 향상시키기 위해 트레이닝에서의 양자화 절차를 시뮬레이션한다. 변압기 기반 모델의 경우 압축 수준의 경계가 지속적으로 향상되었습니다. 예를 들어, FullyQT(Prato et al., 2019) 및 Q8BERT(Zafrir et al., 2019)에서와 같은 \\(8\\)-비트의 양자화된 트랜스포머들, Wu et al.(2023)에서와 같은 \\(4\\)-비트의 양자화된 BERT 및 TernaryBERT(Zhang et al., 2020)에서와 같은 테너리 케이스일 수 있다.\n' +
      '\n' +
      'LLMs에 대한 모델 양자화 LLMs의 양자화에는 엄청난 훈련 비용으로 인해 교정에 몇 가지 훈련 데이터만 사용할 수 있다. 1) 가중치 전용 양자화의 두 가지 주요 방향이 있는데, 여기서 가중치는 하위 비트로 양자화된다. Frantar et al. (2023a); Yao et al. (2022)에서, 저자들은 OBS 및 경사 하강을 사용하여 교정 세트 상의 출력 에러를 최적화한다. 2)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline pruned weights & PPL \\\\ \\hline smallest (top-0\\% 1\\%) & 11.66 \\\\ median (top-49\\% 50\\%) & 19.16 \\\\ largest (top-99\\% 100\\%) & 19.17 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 크기를 달리한 1% 중량 가지치기 후의 ppl 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline module name & outlier fraction (\\%) \\\\ \\hline Att.qkv & 0.2993 \\\\ Att.output & 0.5036 \\\\ FFN.1 & 0.288 \\\\ FFN.2 & 0.7560 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 3-시그마 임계값 하에서 BLOOM-7B에서 상이한 모듈에서의 이상치 분수 분포\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline Layer index & outlier fraction (\\%) \\\\ \\hline\n' +
      '1 & 0.3187 \\\\\n' +
      '5 & 0.8579 \\\\\n' +
      '10 & 0.3953 \\\\\n' +
      '15 & 0.3975 \\\\\n' +
      '20 & 0.3962 \\\\\n' +
      '25 & 0.4399 \\\\\n' +
      '30 & 0.3954 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 3-시그마 임계값 하에서 BLOOM-7B에서 상이한 계층 인덱스에서의 이상치 분수 분포\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline number weights & PPL \\\\ \\hline smallest (top-0\\% 1\\%) & 11.66 \\\\ median (top-49\\% 50\\%) & 19.16 \\\\ largest (top-99\\% 100\\%) & 19.17 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 상이한 최적화 단계들의 동적 양자화 범위. 여기서는 계층 1에서 Att.qkv 모듈의 양자화 범위를 예로 든다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline number weights & PPL \\\\ \\hline smallest (top-0\\% 1\\%) & 11.66 \\\\ median (top-49\\% 50\\%) & 19.16 \\\\ largest (top-99\\% 100\\%) & 19.17 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 상이한 크기 Activation 및 가중치 양자화로 1% 가중치를 프루닝한 후 ppl 결과이며, 여기서 활성화 및 가중치는 모두 하위 비트로 양자화된다. 이 경우 주요 장애물은 활성화의 특이치입니다. LLM.int8() Dettmers et al.(2022)은 fp16/bf16에서 이러한 이상치들을 분리함으로써 이 문제를 해결한다. 그러나, 이러한 구현은 큰 레이턴시 오버헤드를 초래하고 심지어 fp16 추론보다 더 느리다. 최근 연구 Wei et al.(2023); Xiao et al.(2023)은 특이치가 특정 채널에만 존재한다는 것을 발견했으며, LayerNorm 가중치 Wei et al.(2023) 및 보정된 스케일 Xiao et al.(2023)을 사용하여 해당 채널을 평활화한다. Xiao et al.(2023)은 원본 모델 가중치를 조작하지 않고, 몇 개의 교정 데이터를 사용하여 거의 무손실 W8A8 양자화된 LLM을 달성할 수 있다는 것을 이미 증명하였다.\n' +
      '\n' +
      '##7 결론 및 한계\n' +
      '\n' +
      '본 논문에서는 학습 데이터를 사용하지 않고 양자화된 모델의 성능을 잠재적으로 향상시킬 수 있는 데이터 없는 고속 가중치 전용 양자화 알고리즘인 EasyQuant를 제안한다. 우리의 분석은 모델 가중치를 하위 비트로 양자화할 때 성능 손실의 본질적인 기원을 보여준다. 우리는 양자화에서 이상치를 분리함으로써, 양자화 된 LLM의 정확도가 복원 에러가 감소하면서 그에 따라 증가한다는 것을 보여준다. 우리의 실험은 EasyQuant가 데이터가 없는 설정에서 RTN을 훨씬 능가하고 또한 데이터 종속 알고리즘보다 더 잘 행동한다는 것을 증명했다. EasyQuant는 176B 크기의 모델에 대한 양자화를 \\(10\\)분 이내에 완료할 수 있으며, EasyQuant에서 역양자화의 오버헤드는 무시할 수 있다.\n' +
      '\n' +
      '그러나 이시퀀트의 이상치 복구 기능은 구현을 위해 추가 CUDA 커널이 필요하다는 몇 가지 제한 사항도 지적한다. 또한 가중치 전용 양자화는 계산 비용 감소 없이 메모리 풋프린트를 줄일 수 있기 때문에 모델의 지연 시간을 최소화할 수 없다. 또한, 이러한 이상치 격리는 가중치가 상이한 정밀도 하에서 수를 포함하기 때문에 가중치/활성화 양자화를 더 어렵게 할 것이다. 우리는 또한 이지퀀트가 모든 작업에서 데이터 의존적 방법을 능가할 수 없다는 것을 발견했으며, 이는 향후 연구에서 더 효과적인 알고리즘을 조사하도록 동기를 부여한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Boratko et al. (2018) Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, et al. 2018. The systematic classification of knowledge, reasoning, and context in the arc dataset. _ arXiv preprint arXiv:1806.00358_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_.\n' +
      '* Dettmers et al.(2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm.int8(): 스케일에서 변압기에 대한 8비트 매트릭스 곱셈.\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. Cite arxiv:1810.04805Comment:13페이지.\n' +
      '* Frantar et al. (2023a) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023a. Gptq: 생성 사전 훈련된 변압기에 대한 정확한 사후 훈련 양자화.\n' +
      '* Frantar et al. (2023b) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023b. Gptq: 생성 사전 훈련된 변압기에 대한 정확한 사후 훈련 양자화.\n' +
      '* Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. 이진화된 신경망. 신경 정보 처리 시스템_의 _Advances에서, 페이지 4107-4115.\n' +
      '* Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. 효율적인 정수 산술 전용 추론을 위한 신경망의 양자화 및 학습. 페이지 2704-2713.\n' +
      '* Lin et al. (2015) Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. 2015. 곱셈이 적은 신경망. _ arXiv preprint arXiv:1510.03009_.\n' +
      '* Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The penn treebank: Annotating predicate argument structure. The _Human Language Technology: Proceedings of a Workshop held in Plainsboro, New Jersey, March 8-11, 1994._Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. 포인터 센티넬 혼합 모델.\n' +
      '* Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017은 과제 공유: 스토리 클로즈 테스트. _Proceedings on the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics_, pages 46-51.\n' +
      '* Ni et al. (2020) Renkun Ni, Hong-min Chu, Oscar Castaneda, Pingyeh Chiang, Christoph Studer, and Tom Goldstein. 2020. Wrapnet: 초저해상도 연산을 이용한 신경망 추론. _ arXiv preprint arXiv:2007.13242_.\n' +
      '* Prato et al.(2019) Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh. 2019. 개선된 번역을 위한 완전 양자화된 변압기_ arXiv preprint arXiv:1910.10485_.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis and insights from training gopher. _ arXiv preprint arXiv:2112.11446_.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. using deep-speed and megatron to training megatron-turing nlg 530b, large-scale generative language model. _ arXiv preprint arXiv:2201.11990_.\n' +
      '* Tailor et al. (2021) Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. 2021. 도-양자: 그래프 신경망에 대한 양자화 인식 학습_ 학습 표상에 관한 국제 회의_.\n' +
      '* Tata and Patel(2003) Sandeep Tata and Jignesh M Patel. 2003. Piqa: 단백질 데이터 세트를 쿼리하기 위한 대수. _15th International Conference on Scientific and Statistical Database Management, 2003._, pages 141-150. IEEE.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. 라마: 개방적이고 효율적인 기초 언어 모델들.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 2017. 주의력만 있으면 됩니다 _ CoRR_, abs/1706.03762.\n' +
      '* Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, 및 Xianglong Liu. 2023. 이상치 억제: 저비트 트랜스포머 언어 모델의 한계 밀어내기.\n' +
      '* 워크샵(2023) 빅사이언스 워크샵. 2023. Bloom: 176b-parameter open-access 다국어 언어 모델.\n' +
      '* Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. 2023. 트랜스포머 모델에 대한 int4 양자화의 이해: Latency speedup, composability, failure case.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. 평활화: 대형 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화.\n' +
      '* Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: 대규모 변압기에 대한 효율적이고 저렴한 훈련 후 양자화.\n' +
      '* Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. _ arXiv preprint arXiv:1910.06188_.\n' +
      '* Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: open bilingual pre-trained model _ arXiv preprint arXiv:2210.02414_.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: 미리 훈련된 트랜스포머 언어 모델을 개방한다.\n' +
      '* Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: 증류 인식 초저비트 버트__ arXiv preprint arXiv:2009.12812_.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l} \\hline \\hline  & & \\multicolumn{6}{c}{Perplexity-based Task} & \\multicolumn{6}{c}{Zero-shot Task} \\\\ \\cline{3-10}  & & \\multicolumn{1}{c}{WikiText2} & PTB & C4 & PIQA & ARC-easy & ARC-Challenge & StoryCloze \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(22.42\\) & \\(43.69\\) & \\(26.6\\) & \\(65.07\\%\\) & \\(41.71\\%\\) & \\(24.15\\%\\) & \\(61.94\\%\\) \\\\  & RTN & \\(25.90\\) & \\(51.10\\) & \\(29.89\\) & \\(63.11\\%\\) & \\(39.40\\%\\) & \\(23.89\\%\\) & \\(60.15\\%\\) \\\\ \\multirow{3}{*}{560M} & GPTQ & \\(24.03\\) & \\(46.97\\) & **28** & **64.31**\\% & \\(40.24\\%\\) & \\(23.46\\%\\) & **61.17**\\% \\\\  & EasyQuant & **23.74** & **46.86** & \\(28.03\\) & \\(63.06\\%\\) & **40.32**\\% & **24.15**\\% & \\(59.64\\%\\) \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(17.69\\) & \\(57.96\\) & \\(22.05\\) & \\(67.14\\%\\) & \\(45.41\\%\\) & \\(25.68\\%\\) & \\(63.27\\%\\) \\\\  & RTN & \\(22.00\\) & \\(66.85\\) & \\(24.44\\) & \\(65.29\\%\\) & \\(42.51\\%\\) & \\(23.34\\%\\) & \\(60.66\\%\\) \\\\ \\multirow{3}{*}{1.1B} & GPTQ & \\(19.05\\) & \\(62.48\\) & \\(23.25\\) & \\(66.05\\%\\) & **44.49**\\% & \\(25.51\\%\\) & **62.32**\\% \\\\  & EasyQuant & **18.51** & **61.83** & **22.94** & **66.65**\\% & \\(43.73\\%\\) & \\(25.51\\%\\) & \\(62.06\\%\\) \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(15.39\\) & \\(30.00\\) & \\(19.49\\) & \\(69.97\\%\\) & \\(48.11\\%\\) & \\(26.79\\) \\% & \\(65.44\\%\\) \\\\  & RTN & \\(16.97\\) & \\(33.58\\) & \\(21.26\\) & \\(67.74\\%\\) & \\(44.70\\%\\) & \\(26.45\\) \\% & \\(62.95\\%\\) \\\\ \\multirow{3}{*}{1.7B} & GPTQ & \\(16.48\\) & \\(31.84\\) & \\(20.55\\) & \\(68.77\\%\\) & \\(44.49\\%\\) & \\(25.94\\%\\) & \\(64.48\\%\\) \\\\  & EasyQuant & **16.01** & **31.50** & **20.15** & **68.99**\\% & **46.89**\\% & **26.19**\\% & **65.37**\\% \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(13.48\\) & \\(25.34\\) & \\(17.49\\) & \\(70.51\\%\\) & \\(53.24\\%\\) & \\(30.55\\) \\% & \\(67.79\\%\\) \\\\  & RTN & \\(14.76\\) & \\(27.68\\) & \\(18.76\\) & \\(69.86\\%\\) & \\(51.35\\%\\) & \\(29.52\\%\\) & \\(67.09\\%\\) \\\\ \\multirow{3}{*}{3B} & GPTQ & \\(14.2\\) & \\(26.49\\) & \\(18.1\\) & \\(69.42\\%\\) & **52.82**\\% & **28.92**\\% & \\(67.22\\%\\) \\\\  & EasyQuant & **14.01** & **26.12** & **17.96** & **69.80**\\% & \\(50.72\\%\\) & \\(28.58\\%\\) & **67.35**\\% \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(11.37\\) & \\(20.83\\) & \\(15.20\\) & \\(73.72\\%\\) & \\(57.37\\%\\) & \\(33.45\\) \\% & \\(71.99\\%\\) \\\\  & RTN & \\(12.10\\) & \\(22.42\\) & \\(16.06\\) & \\(72.69\\%\\) & \\(56.14\\%\\) & \\(32.17\\) \\% & \\(70.72\\%\\) \\\\ \\multirow{3}{*}{7.1B} & GPTQ & \\(11.73\\) & \\(21.67\\) & \\(15.6\\) & \\(72.96\\%\\) & **56.14**\\% & \\(32.25\\%\\) & **71.36**\\% \\\\  & EasyQuant & **11.66** & **21.47** & **15.52** & **73.23**\\% & \\(55.72\\%\\) & **32.51**\\% & \\(71.10\\%\\) \\\\ \\hline \\multirow{3}{*}{BLOOM} & fp16 & \\(8.11\\) & \\(14.59\\) & \\(11.71\\) & \\(79.16\\%\\) & \\(67.47\\%\\) & \\(44.97\\) \\% & \\(76.89\\%\\) \\\\  & RTN & \\(8.37\\) & \\(15.00\\) & \\(12.04\\) & \\(79.00\\%\\) & \\(66.33\\%\\) & \\(43.17\\) \\% & \\(76.00\\%\\) \\\\ \\multirow{3}{*}{176B} & GPTQ & \\(8.21\\) & \\(14.75\\) & \\(**11.81**\\) & \\(79.00\\%\\) & \\(67.42\\%\\) & \\(44.10\\%\\) & \\(76.32\\%\\) \\\\ \\cline{1-1}  & EasyQuant & \\(8.21\\) & \\(14.75\\) & \\(11.87\\) & \\(**79.05**\\%\\) & **67.8**\\% & **44.45**\\% & **77.28**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: BLOOM 모델 패밀리에 대한 복잡도 및 zershot 결과\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>