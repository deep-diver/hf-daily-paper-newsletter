<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '(SS3.12)는 생산에서 대규모 모델의 효과적인 배치를 용이하게 한다.\n' +
      '\n' +
      '본 논문에서 제안된 모델링 발전은 모델이 더 많은 수의 매개변수를 효율적으로 처리할 수 있게 하여 고품질 콘텐츠 전달로 이어진다. 본 논문에서는 SS3.1의 Feed 순위, Ads CTR 모델 SS3.2, SS5.3의 Job 추천 순위 모델에 대한 대규모 아키텍처를 소개한다.\n' +
      '\n' +
      'SS5에서는 오프라인 실험 및 A/B 테스트에서 수집된 주요 학습을 요약하여 사료 순위, 작업 권장 사항 및 광고 CTR 예측을 위한 생산에 대형 순위 모델을 배포한 경험을 자세히 설명한다. 특히, 이 작업에 제시된 기술은 피드 세션의 0.5% 증가, 일자리 권장 사항 내에서 적격 지원자 수의 1.76% 증가, 광고 CTR의 4.3% 증가 등 상당한 상대적 개선을 가져왔다. 우리는 이 작업이 대규모 DNN 순위 모델을 규모별로 적용하는 데 관심이 있는 엔지니어들에게 실용적인 솔루션과 통찰력을 제공할 수 있다고 믿는다.\n' +
      '\n' +
      '##2. 관련업무\n' +
      '\n' +
      '개인화된 추천 시스템에서 심층 신경망 모델의 사용은 2016년 Wide&Deep 모델(Bordes and McAllester, 2016)의 성공 이후 학계 및 산업계에서 주류를 이루고 있으며, 대표적으로 특징 임베딩, 특징 선택, 특징 상호작용 구성 요소로 구성되어 있으며, 특징 상호작용 증진에 많은 연구가 집중되고 있다. Tge Wide&Deep 모델(Bordes and McAllester, 2016)은 일반화된 선형 모델과 MLP 네트워크를 결합하여 이러한 추세를 시작했다. 후속 연구는 암시적 특징 상호작용을 위한 MLP 네트워크를 유지하고 선형 모델을 명시적인 고차 특징 상호작용을 캡처하기 위한 다른 모듈로 대체하는 것을 목표로 했다. 예를 들어, 선형 모델을 FM으로 대체한 DeepFM(Han et al., 2017), 고차 특징 상호작용을 위한 교차 네트워크를 도입한 DeepFM(DCN)(Wang et al., 2018) 및 그 후속 DCNv2(Wang et al., 2018); xDeepFM(Dong et al., 2019), 명시적 벡터-와이즈 특징 상호작용을 위한 압축 상호 작용 네트워크(CIN)를 제공하는 xDeepFM(Dong et al., 2019), 명시적 특징 상호작용을 위한 자기-관심 네트워크를 도입한 AutoInt(Wang et al., 2018), AFN(Chen et al., 2018), 대수 변환 계층을 통한 적응-차 특징 상호작용을 탐색하는 AFN(Chen et al., 2018), 두 MLP를 결합하여 인상적인 성능을 달성한 FinalMLP(Wang et al., 2018) 등이 있다.\n' +
      '\n' +
      '다양한 링크드인 추천 작업을 위해 이러한 아키텍처를 실험하고 사용자 정의했으며, DCNv2가 가장 다재다능한 것으로 입증되었다. 본 논문에서는 잔류 DCN이라고 하는 DCNv2에 대한 개선을 제안한다. 또한, DLRM(Wang et al., 2018) 논문에서 제안한 방법과 유사한 텐서플로우(TensorFlow, TF)에서 모델 병렬성 설계를 구현하여 임베딩 테이블이 큰 모델 학습을 가속화하였다.\n' +
      '\n' +
      '우리의 조사에서 우리는 독창적인 아키텍처를 생산 환경에 원활하게 통합하려고 시도할 때 어려움에 직면했습니다. 이러한 과제는 종종 모델 훈련 발산, 과적합 또는 관찰 가능한 제한된 성능 개선과 같은 문제로 나타난다. 이러한 아키텍처를 효과적으로 활용하여 고성능 모델을 만드는 것은 종종 시행착오의 힘든 과정을 특징으로 하는 상당한 노력을 필요로 한다. 따라서 본 논문에서는 SOTA 아키텍처를 생산 준비 순위 모델에 성공적으로 조립한 경험을 통해 얻은 귀중한 통찰력을 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '다양한 최적화 및 아키텍처를 통해 신경망 예측 성능을 향상시키면서, 교정의 공간은 비교적 안정적으로 유지되었다. Histogram binning, Platt Scaling, Isotonic Regression과 같은 전통적인 산업 표준 방법(Krizhevsky et al., 2014)은 심층 모델 학습 후 후처리 단계에서 적용된다. 일부 연구는 일반적으로 상쇄(Krizhevsky et al., 2014) 또는 약간의 개선된 메트릭(Bordes and McAllester, 2016)을 초래하는 과소/과잉 교정 문제를 해결하기 위해 교정 인식 손실을 도입했다. SS3.4에서 우리는 모델 훈련 동안 심층 모델 점수를 교정하도록 학습하고 모델 예측 정확도를 크게 향상시키는 심층 학습 모델 내의 등장성 교정 레이어를 제안한다.\n' +
      '\n' +
      '##3. 대형 순위 모델\n' +
      '\n' +
      '본 절에서는 LinkedIn Feed Ranking과 Ads CTR(click-through-rate) 예측에 의해 사용되는 대형 순위 모델을 소개한다. 우리는 사용 사례에 따라 아키텍처 구성요소의 선택이 다르다는 것을 관찰한다. 우리는 생산 시나리오에 대한 효과적인 순위 모델 구축에 대한 통찰력을 공유할 것입니다.\n' +
      '\n' +
      '### Feed Ranking Model\n' +
      '\n' +
      '1차 Feed 순위 모델은 점별 순위 접근법을 사용하여 각 <회원, 후보 사후 쌍에 대해 코멘트, 공유, 투표 및 긴 체류 및 클릭을 포함한 여러 작업 확률을 예측한다. 이러한 예측은 선형적으로 결합되어 최종 사후 점수를 생성한다. 다중 작업 학습(multi-task learning, MTL) 구조를 갖는 TF 모델은 두 개의 타워에서 이러한 확률을 생성한다: 클릭 및 롱 드웰의 확률에 대한 클릭 타워, 기여 및 관련 예측에 대한 기여 타워. 두 타워는 그들의 분포에 기초하여 정규화된 동일한 조밀한 피쳐 세트를 사용하고(Han et al., 2017), 다수의 완전-연결된 레이어를 적용한다. 희소 ID 임베딩 특징(SSA.1)은 그림 1과 같이 Member/Actor 및 Hashtag Embedding Table의 임베딩 테이블에서 룩업을 통해 밀집 임베딩(Han et al., 2017)으로 변환된다. 도 8의 부록에서의 재현성을 위해 서로 다른 아키텍처가 어떻게 단일 모델로 함께 연결되는지를 보여주는 다이어그램을 제공한다.\n' +
      '\n' +
      '###ds CTR 모델\n' +
      '\n' +
      '링크드인에서, 광고 선택은 클릭-스루-레이트(CTR) 예측에 의존하여, 추천 광고에 대한 회원 클릭의 가능성을 추정한다. 이러한 CTR 확률은 광고를 디스플레이하기 위한 광고 경매를 에게 알린다.\n' +
      '\n' +
      '도 1. 주요 사료 순위 모형의 기여탑\n' +
      '\n' +
      '멤버들 일부 광고주는 \'좋아요\', \'댓글\'과 같은 사회적 상호 작용을 충전 가능한 클릭으로 간주하는 반면 다른 광고주는 방문 광고 웹사이트만 클릭으로 간주하는 것과 같이 캠페인에 대한 충전 가능한 클릭을 맞춤화한다. 일반적으로 포지티브 맞춤형 충전 가능 클릭만 포지티브 라벨로 취급됩니다. 사용자 관심을 더 잘 포착하기 위해, 우리의 CTR 예측 모델은 광고자 맞춤화에 관계없이 유사한 충전 가능한 정의가 함께 그룹화되는 3개의 충전성 분류에 해당하는 3개의 헤드를 갖는 충전성 기반 MTL 모델이다. 각 헤드는 MLP 및 DCNv2 블록과 같은 독립적인 상호 작용 블록을 사용한다. 손실 함수는 머리별 손실을 결합합니다. 피처의 경우 회원 및 광고주의 전통적인 기능 외에도 ID 기능을 통합하여 광고주, 캠페인 및 광고를 나타냅니다. 모델 아키텍처는 그림 2에 나와 있습니다.\n' +
      '\n' +
      '### Residual DCN\n' +
      '\n' +
      '특징 상호 작용을 자동으로 포착하기 위해 DCNv2(Sutton et al., 2017)를 활용하였다. 오프라인 실험에서 두 개의 DCNv2 계층이 충분한 상호 작용 복잡성을 제공했으며, 더 많은 계층을 추가하면 훈련 및 서빙 시간이 크게 증가하면서 관련성 이득이 감소했기 때문이다. DCNv2는 단지 두 개의 레이어를 사용함에도 불구하고 큰 피쳐 입력 차원으로 인해 상당한 수의 파라미터를 추가했다. 이를 해결하기 위해 효율성을 높이기 위해 두 가지 전략을 채택했다. 먼저, (Sutton et al., 2017)에 이어, 우리는 가중치 행렬을 낮은 순위 근사치와 유사한 두 개의 스키니 행렬로 대체했다. 둘째, 희박한 원-핫 피쳐를 임베딩 테이블 룩업으로 대체하여 입력 피쳐 차원을 줄임으로써 거의 30%의 감소를 가져왔다. 이러한 수정으로 관련성 이득에 대한 미미한 영향만으로 DCNv2의 매개변수 수를 실질적으로 줄일 수 있어 모델을 CPU에 배치하는 것이 가능해졌다.\n' +
      '\n' +
      'DCNv2, 특히 크로스 네트워크의 전력을 더욱 향상시키기 위해 저순위 크로스 네트워크에 어텐션 스키마를 도입했다. 구체적으로, 원본 저순위 매핑은 서로 다른 매핑 커널을 갖는 3개로 중복되며, 원본은 값 행렬의 역할을 하고 나머지 2개는 질의 및 키 행렬의 역할을 한다. 주의 점수 행렬이 계산되고 낮은 순위 매핑 사이에 삽입됩니다. 그림 3은 기본적인 스케일링된 닷-제품 셀프-어텐션을 설명한다. 학습된 피쳐 상호 작용의 호환성을 균형을 맞추기 위해 온도도 추가할 수 있다. 극단적인 경우에, 어텐션 크로스 넷은 어텐션 스코어 매트릭스가 아이덴티티 매트릭스일 때 정상 크로스 넷으로 퇴보될 것이다. 실제로, 우리는 스킵 연결을 추가하고 주의 온도를 미세 조정하는 것이 안정적인 훈련을 유지하면서 더 복잡한 특징 상관 관계를 학습하는 데 도움이 된다는 것을 발견했다. 저순위 교차망과 주의력 저순위 교차망을 병행하여 사료 순위 과제(SS5.2)에서 통계적으로 유의한 향상을 보였다.\n' +
      '\n' +
      'DNN에서### 등음계 보정 계층\n' +
      '\n' +
      '모델 보정은 추정된 클래스 확률이 비즈니스 성공에 중요한 요소인 실제 발생과 일치하도록 합니다. 예를 들어, 광고 과금 가격은 클릭률(CTR) 확률과 연결되어 정확한 보정이 필수적입니다. 또한 다른 모델 또는 목표를 사용할 때 모델 점수 분포가 변경될 수 있기 때문에 다른 모델 간의 공정한 비교를 가능하게 한다. 전통적으로 보정은 Platt 스케일링 및 등장성 회귀와 같은 고전적인 방법을 사용하여 훈련 후 수행된다. 그러나 이러한 방법은 매개변수 공간 제약 및 비호환성과 같은 한계로 인해 심층 신경망 모델에 적합하지 않다. 또한 장치, 채널 또는 항목 ID와 같은 여러 기능을 보정에 통합할 때 확장성이 어려워집니다.\n' +
      '\n' +
      '도 4. 등속성 층 표현\n' +
      '\n' +
      '도 3. 잔여 교차망\n' +
      '\n' +
      '도 2. CTR 충전성 기반 멀티태스크 모델 광고\n' +
      '\n' +
      '위에서 언급한 문제를 해결하기 위해 우리는 보정을 수행하기 위해 심층 신경망 모델과 공동 훈련될 네이티브 신경망 레이어로 사용될 수 있는 맞춤형 등장성 회귀 레이어(_이소토닉 레이어_라고 함)를 개발했다. 등장성 회귀와 유사하게 등장성 층은 부분적 맞춤 개념을 따른다. 예측된 값(확률들은 다시 로짓으로 변환되어야 함)을 주어진 간격 \\(u_{i}\\)만큼 버퍼링하고, 각 버킷에 대해 훈련 가능한 가중치 \\(w_{i}\\)을 할당하며, 이는 다른 네트워크 파라미터와 함께 훈련 중에 업데이트된다(그림 4). 등장성은 Relu 활성화 함수를 사용하여 달성되는 비음수 가중치를 사용하여 보장된다. 다수의 특징들로 교정력을 향상시키기 위해, 가중치들은 모든 교정 특징들로부터 유도되는 임베딩 표현(요소가 \\(e_{i}\\)으로 표시되는 벡터)과 결합될 수 있다. 최종적으로\n' +
      '\n' +
      '\\Sigma_{i=0}^{i=k}Relu(e_{i}+w_{i})\\begin{split}& y_{cali}=\\Sigma_{i=0}^{i=k}Relu(e_{i}+w_{i})\\cdot v_{i}+b,v_{i}=\\begin{cases}step,&\\text{if}i<k\\y-step\\cdot k,&\\text{i=k}\\end{cases},\\\\&k=\\arg\\max_{j}(y-step\\cdot j>0).\\end{split}\\tag{1}\\text{if}i<k\\y-step\\cdot k,&\\text{i=k}\\end{cases},\\\\&k=\\arg\\max_{j}(y-step\\cdot j>0).\\end{split}\\tag{1}\\text{if}i<k\\y-step\\cdot\n' +
      '\n' +
      '### 고밀도 게이팅과 대형 MLP\n' +
      '\n' +
      '글로벌 모델에 개인화된 임베딩을 도입하면 기존의 조밀한 피쳐 간의 상호 작용을 도입하는 데 도움이 되며, 대부분은 다차원 카운트 기반 및 범주형 피쳐입니다. 우리는 이러한 다차원 특징을 단일 조밀한 벡터로 평평하게 하여 내포적 상호작용을 위해 MLP 층으로 전송하기 전에 임베딩과 연결했다. 각 MLP 층의 폭을 확대하여 보다 포괄적인 상호 작용을 육성함으로써 이득을 향상시키는 간단한 방법을 발견하였다. 피드의 경우 오프라인으로 실험한 가장 큰 MLP 구성은 각각 폭 3500의 4개 레이어("대형 MLP" 또는 LMLP 참조)였다. 특히 개인화된 임베딩을 실행할 때 온라인에서만 이득을 얻을 수 있습니다. 그러나, 이러한 향상은 추가적인 매트릭스 계산으로 인해 증가된 스코어링 레이턴시를 희생시킨다. 이 문제를 해결하기 위해 대기 시간 예산 내에서 이득을 최대화하는 최적의 구성을 확인했다.\n' +
      '\n' +
      '이후 게이트 넷(Gat et al., 2018)에서 영감을 받아 은닉층에 게이팅 메커니즘을 도입하였다. 이 메커니즘은 신경망 내에서 다음 단계로 정보의 흐름을 조절하여 학습 과정을 향상시킨다. 우리는 이 접근법이 히든 레이어에 적용될 때 가장 비용 효율적이라는 것을 발견했으며, 온라인 리프트를 일관되게 생산하면서 무시할 수 있는 추가 매트릭스 계산만 도입했다.\n' +
      '\n' +
      '또한, 전문가 모델(sMoE)(Srivastava et al., 2015)의 희소 게이트 혼합을 탐색하였다. SS5.2에서 절제 연구를 보고한다.\n' +
      '\n' +
      '### Incremental Training\n' +
      '\n' +
      '대규모 추천 시스템은 광고, 뉴스 피드 업데이트 및 일자리 게시와 같은 새로운 콘텐츠를 지속적으로 통합하면서 빠르게 진화하는 생태계에 적응해야 한다. 이러한 변화에 발맞추기 위해, 마지막 훈련된 모델을 출발점으로 사용하고, _warm start_로 알려진 기법인 최신 데이터로 계속 훈련시키는 유혹이 있다. 이는 훈련 효율성을 향상시킬 수 있지만, 재난적 망각(Krishnaman et al., 2017)으로 알려진 문제인 이전에 학습한 정보를 망각하는 모델로 이어질 수도 있다. 반면에 점진적 훈련은 가중치 초기화를 위해 이전 모델을 사용할 뿐만 아니라 이를 활용하여 유익한 정규화 용어를 생성한다.\n' +
      '\n' +
      '타임스탬프에서 현재 데이터 세트를 \\(\\mathcal{D}_{t}\\)으로, 마지막으로 추정된 가중치 벡터를 \\(\\mathbf{w}_{t-1}\\)으로, 헤시안 행렬에 대한 \\(\\mathbf{w}_{t-1}\\)을 \\(\\mathcal{H}_{t-1}\\)으로 표현한다. 타임스탬프까지의 총 손실은 다음과 같이 근사화된다.\n' +
      '\n' +
      '{split}\\text{loss}_{\\mathcal{D}_{t}(\\mathbf{w})+\\lambda_{f}/2\\times(\\mathbf{w}-\\mathbf{w-1}^{T}\\mathcal{H}_{t-1}(\\mathbf{w}-\\mathbf{w}_{t-1}),\\end{split}\\tag{2}\\times(\\mathbf{w}-\\mathbf{w}_{t-1})\n' +
      '\n' +
      '여기서 \\(\\lambda_{f}\\)는 과거 샘플로부터의 기여도를 조정하기 위한 망각 인자이다. 실제로 \\(\\mathcal{H}_{t-1}\\)은 매우 큰 행렬이 될 것이다. 계산\\(\\mathcal{H}_{t-1}\\) 대신 대각선 요소\\(\\text{diag}(\\mathcal{H}_{t-1})\\)만을 사용하여 저장 및 계산 비용을 크게 줄일 수 있다. 대규모 심층 추천 모델들의 경우, 2차 미분 계산이 고가이기 때문에, 경험적 피셔 정보 매트릭스(Empirical Fisher Information Matrix; FIM)(Srivastava et al., 2015; Srivastava et al., 2015)는 헤시안 대각선을 근사하기 위해 제안된다.\n' +
      '\n' +
      '전형적인 증분 학습 사이클은 하나의 초기 콜드 스타트 모델을 트레이닝하고 후속 증분 학습 모델을 트레이닝하는 것으로 구성된다. 재난적 망각을 더욱 완화하고 이 문제를 해결하기 위해 이전 모델과 초기 콜드 스타트 모델을 모두 사용하여 가중치를 초기화하고 정규화 항을 계산한다. 이 설정에서, (2)에 제시된 총 손실은 다음과 같다:\n' +
      '\n' +
      '\\mathbf{w}-\\mathbf{w}_{t}}(\\mathbf{w}-\\mathbf{w}_{t}}(\\mathbf{w}-\\mathbf{w}_{t-1})^{T}\\mathcal{H}_{0}(\\mathbf{w}-\\mathbf{w}_{t-1})^{T}\\mathcal{H}_{t-1}(\\mathbf{w}-\\mathbf{w}-\\mathbf{t-1}),\\end{split}\\tag{3}\\times[\\alpha(\\mathbf{w}-\\mathbf{w}-\\mathbf{w}-\\mathbf{w}_{t-1})^{T}\\mathcal{H}_{t-1}(\\mathbf{w}-\\mathbf{w}_{t-1})},\\end{split}\\tag{3}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{w}_{0}\\)은 초기 콜드 스타트 모델의 가중치이고 \\(\\mathcal{H}_{0}\\)은 콜드 스타트 트레이닝 데이터에 대한 \\(\\mathbf{w}_{0}\\)과 관련하여 헤시안이다. 모델 가중치\\(\\mathbf{w}\\)는 \\(\\alpha\\mathbf{w}_{0}+(1-\\alpha)\\mathbf{w}_{t-1}\\으로 초기화된다. 본 논문에서는 추가적인 가변 파라미터 \\(\\alpha\\in[0,1]\\)를 _cold weight_라고 한다. 양의 콜드 가중치는 콜드 스타트 모델의 정보를 지속적으로 증분 학습에 도입한다. 저온 중량이 0일 때 식 (3)은 식 (2)와 같다.\n' +
      '\n' +
      '회원 이력 모델링\n' +
      '\n' +
      '플랫폼 콘텐츠와의 멤버 상호작용을 모델링하기 위해, 우리는 (Srivastava et al., 2015; Srivastava et al., 2015)와 유사한 접근법을 채택한다. 우리는 최적화 동안 또는 별도의 모델을 통해 학습된 아이템 임베딩과 함께 각 멤버에 대한 이력 상호 작용 시퀀스를 생성한다(Srivastava et al., 2015). 이러한 아이템 임베딩들은 액션 임베딩들 및 현재 스코어링되고 있는 아이템의 임베딩(조기 융합)과 연결된다. 이 시퀀스는 2계층 Transformer-Encoder(Srivastava et al., 2015)가 처리하며, 랭킹 모델에서는 max-pooling 토큰이 특징으로 사용된다. 또한 정보를 향상시키기 위해 마지막 5개의 시퀀스 단계를 고려하여 순위 모델에 대한 추가 입력 특징으로 평평하고 연결한다. 지연 시간을 줄이기 위해 트랜스포머 내에서 더 짧은 시퀀스와 더 작은 피드포워드 네트워크 치수를 실험했다. SS5.2의 절제 실험에서 우리는 역사 모델링을 TransAct라고 한다.\n' +
      '\n' +
      '본 연구 결과는 입력 임베딩 크기의 절반과 같은 피드-포워드 차원을 갖는 2층 변압기가 대부분의 관련성 이득을 제공한다는 것을 보여준다. 더 긴 시퀀스가 관련성 메트릭을 개선하는 반면, 추가된 훈련 및 서빙 시간은 확장된 이력 시퀀스를 정당화하지 않았다.\n' +
      '\n' +
      '### 탐험과 탐험\n' +
      '\n' +
      '탐색 대 활용 딜레마는 추천 시스템에서 흔히 볼 수 있다. 즉각적인 성능을 최대화하기 위해 구성원의 이력 피드백 데이터("exploitation")를 간단히 활용하면 장기적인 이득에 타격을 줄 수 있으며, 새로운 항목("exploitation")을 활성화하면 단기 이득의 비용으로 미래 성능을 개선하는 데 도움이 될 수 있습니다. 이를 해결하기 위해 UCB( Upper Confidence Bounds)와 톰슨 샘플링(Thompson sampling)과 같은 전통적인 방법을 사용하지만, 심층 신경망 모델에는 효율적으로 적용할 수 없다. 사후 확률 계산 비용을 줄이고 특정 표현력을 유지하기 위해 논문에서 언급한 Neural Linear 방법과 유사한 방법을 채택했다(Zhou et al., 2018), 즉 신경망의 마지막 레이어의 가중치에 베이지안 선형 회귀를 수행했다. 각 입력 \\(x_{i}\\)에 대한 예측값 \\(y_{i}\\)은 \\(y_{i}=WZx\\)으로 주어지며, 여기서 \\(W\\)은 마지막 레이어의 가중치이고 \\(Zx\\)은 마지막 레이어의 입력 \\(x\\)이다. 주어진 \\(W\\)이 주어지면 \\(Zx\\)에 대해 \\(y\\)에 베이지안 선형 회귀를 적용하고 톰슨 샘플링에 공급되는 \\(W\\)의 사후 확률을 획득한다. 논문에서 언급한 방법과 달리, 우리는 마지막 계층에 대한 표현을 학습하기 위해 모델을 독립적으로 훈련시키지 않는다. W의 사후 확률은 주어진 기간 동안 각 오프라인 교육이 끝날 때 점진적으로 업데이트되므로 빈번한 재교육은 새로운 정보를 적시에 포착할 수 있다. 이 기술은 사료 및 온라인 A/B 테스트에 적용되었으며 상대 +0.06% 전문가 일일 활성 사용자를 보여주었다.\n' +
      '\n' +
      '광범위한 인기 특징\n' +
      '\n' +
      '우리의 순위 모델은 광범위한 추세를 포착하기 위한 수십억 개의 매개변수와 개별 항목 간의 변동을 처리하기 위한 무작위 효과 모델을 결합하여 사용자 간의 인기를 반영하는 고유한 값을 할당한다. 플랫폼의 동적 특성으로 인해 랜덤 효과 모델은 변화하는 추세에 적응하기 위해 더 빈번한 교육을 받습니다.\n' +
      '\n' +
      'Root Object ID로 알려진 변동성이 크고 게시물이 짧은 식별자에 대해서는 RO(Specialized Root-Object) 모델을 사용한다. 이 모델은 주요 모델의 예측과 실제 레이블 사이의 잔차를 근사화하기 위해 최신 데이터로 8시간마다 훈련된다. 레이블의 적용 범위가 더 높기 때문에 RO 모델 내에서 [좋아요] 및 [클릭]을 사용했습니다.\n' +
      '\n' +
      '이 모델의 최종 예측은 \\(y_{\\text{final}}\\)로 표시되며 전역 모델과 랜덤 효과 모델에서 파생된 로짓의 합에 달려 있다. 다음과 같이 계산된다:\n' +
      '\n' +
      '\\[y_{\\text{final}}=\\sigma\\left(\\text{logit}(y_{\\text{global\\_effect}})+\\text{ logit}(y_{\\text{random\\_effect}})\\right),\\]\n' +
      '\n' +
      '여기서 \\(\\sigma\\)는 시그모이드 함수를 의미한다.\n' +
      '\n' +
      '대형 임베딩 테이블은 아이템 ID 학습 과정에 도움이 됩니다. RO Wide 점수와 함께 explore/exploit 알고리즘을 통합하여 참여 DAU(일상 활동 사용자)에서 +0.17%의 상대적 증가로 Feed 사용자 경험을 개선했다.\n' +
      '\n' +
      '### Multi-task Learning\n' +
      '\n' +
      '다중 작업 학습(multi-task learning, MTL)은 특히 세컨드 패스 랭킹(second Pass Ranking, SPR)에서 현대적인 사료 랭킹 시스템을 향상시키기 위해 중추적이다. MTL은 SPR 시스템이 사용자 참여 메트릭, 콘텐츠 관련성 및 개인화를 포함하여 다양한 랭킹 기준을 동시에 최적화할 수 있게 한다. SPR에서 MTL에 대한 우리의 탐색은 각각의 고유한 특징과 이점을 갖는 태스크-특정 학습을 개선하기 위해 설계된 다양한 모델 아키텍처를 포함한다: (1) 하드 파라미터 공유: 베이스라인 역할을 하는 태스크 간에 직접 파라미터를 공유하는 것, (2) 그룹화 전략: 태스크는 긍정/부정 비율 또는 의미적 내용과 같은 유사성에 기초하여 그룹화된다. 예를 들어, \'좋아요\'와 \'기여\'와 같은 태스크는 더 높은 긍정률로 인해 두 태스크를 모두 지원하는 단일 타워로 함께 그룹화될 수 있는 반면, \'댓글\'과 \'공유\'는 더 낮은 긍정률로 별도로 그룹화된다. 우리는 또한 MMoE (Krizhevsky et al., 2014)와 PLE (Peng et al., 2015)를 포함한 일반적인 접근법을 탐구했다. 우리의 실험에서 그룹화 전략은 모델 매개변수의 약간의 증가만으로 메트릭의 약간의 개선을 보여주었다(표 1 참조). 한편, MMoE와 PLE는 상당한 성능 향상을 제공하면서도 전문가 구성에 따라 매개변수 수를 3x-10x까지 확장하여 대규모 온라인 배포에 대한 과제를 제기했다.\n' +
      '\n' +
      '웰타임 모델링\n' +
      '\n' +
      '구성원 콘텐츠 상호 작용 기간을 반영하는 체류 시간은 구성원의 행동과 선호도에 대한 귀중한 통찰력을 제공한다. 링크드인 피드에서 수동적인 콘텐츠 소비를 감지하기 위해 \'긴 체류\' 신호를 도입했다. 이 신호를 효과적으로 구현하면 수동적이지만 긍정적인 참여를 포착할 수 있다. 모델링 드웰 시간은 기술적 과제를 제시하였다. (1) 노이즈 드웰 시간 데이터는 높은 변동성으로 인해 직접 예측 또는 로그 예측을 부적합하게 만들었고, (2) 긴 드웰에 대한 정적 임계값 식별은 진화하는 사용자 선호도에 적응할 수 없었고, 수동 임계값은 일관성과 유연성이 부족했으며, (3) 고정된 임계값은 링크드인 피드의 모든 콘텐츠 유형에 걸쳐 참여 게시물을 홍보하는 목표와 상충되어 더 긴 드웰 시간을 가진 콘텐츠에 편향될 수 있었다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 특정 백분위수(예: 90번째 백분위수)보다 게시물에 더 많은 시간이 있는지 여부를 예측하는 \'긴 체류\' 이진 분류기를 설계했다. 특정 백분위수는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Model** & **Contributions** \\\\ \\hline Hard Parameter Sharing & baseline \\\\ Grouping Strategy & +0.75\\% \\\\ MMoE & +1.19\\% \\\\ PLE & +1.34\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. MTL 모델의 성능 비교\n' +
      '\n' +
      '그림 5. RO 와이드 모델이 클릭하고 타워를 좋아합니다.\n' +
      '\n' +
      '랭킹 위치, 콘텐츠 유형 및 플랫폼과 같은 컨텍스트 특징에 기초하여 결정되고, 롱-드웰 임계치 설정 및 트레이닝 데이터 향상을 위한 클러스터를 형성한다. 클러스터 분포를 매일 측정하여 진화하는 구성원 소비 패턴을 포착하고 드웰 시간 신호에서 편향 및 노이즈를 줄인다. 이 모델은 다중 작업 다중 클래스 프레임워크 내에서 작동하여 전체 소요 시간에서 0.8%, 게시물당 소요 시간에서 1% 증가, 회원 세션에서 0.2% 증가의 상대적 개선을 가져왔다.\n' +
      '\n' +
      '### 모델 사전 압축\n' +
      '\n' +
      '고차원 희소 범주형 피쳐를 임베딩 공간에 매핑하는 전통적인 접근 방식은 두 단계를 포함한다. 첫째, 정적 해시 테이블을 사용하여 문자열 기반 ID 피쳐를 정수로 변환합니다. 다음으로, 메모리 내 크기를 줄이기 위해 메모리 효율적인 MPHF(Minimal Perfect Hashing Function)(Brock et al., 2017)를 활용한다. 이들 정수 ID들은 임베딩 매트릭스 내의 행들에 액세스하기 위한 인덱스들로서, 트레이닝 데이터 내의 정적 해시테이블 또는 고유 ID들의 카디널리티와 매칭되며, 최대 한계로 캡핑된다. 정적 해시 테이블은 메모리 사용의 약 30%에 기여하며, 이는 어휘 공간이 커지고 어휘 대 모델 크기 비율이 증가함에 따라 비효율적일 수 있다. 지속적인 훈련은 새로운 데이터를 수용하기 위해 점진적인 어휘 업데이트를 요구하기 때문에 문제를 더욱 복잡하게 만든다.\n' +
      '\n' +
      'QR 해싱(Kang et al., 2017)은 ID에 걸쳐 임베딩 고유성을 보존하면서 몫 및 나머지 기술을 사용하여 큰 매트릭스를 더 작은 매트릭스로 분해함으로써 솔루션을 제공한다. 예를 들어, QR 전략에서 1000배 압축 비율을 가진 40억의 어휘는 합으로 약 400만 행의 두 개의 작은 임베딩 행렬을 생성하는데, 이는 몫 행렬에서 약 400만 개, 나머지 행렬에서 약 1000개이다. 이 접근법은 Feed/Ads의 오프라인 및 온라인 메트릭에서 유사한 성능을 입증했다. 곱셈 집계가 가장 잘 작동하는 반면, 임베딩이 0에 가깝게 초기화될 때 곱셈 집계는 수치 정밀도로 인해 수렴 문제를 겪었다는 것을 발견했다. QR 해싱의 광범위한 어휘와의 호환성은 MurmurHash와 같은 충돌 방지 해싱 기능을 사용하여 잠재적으로 어휘 유지를 제거할 수 있다. 또한 학습 항목 ID에 대한 임베딩 벡터를 생성하여 OV(Out-of-Vocabulary) 문제를 해결하고 잠재적으로 데이터에서 더 다양한 신호를 캡처할 수 있다. 기술에 대한 설명은 부록의 그림 9를 참조하십시오.\n' +
      '\n' +
      '###매립 테이블 양자화\n' +
      '\n' +
      '종종 대규모 딥 랭킹 모델의 크기의 90%를 초과하는 임베딩 테이블은 특징, 엔티티 및 임베딩 치수 크기가 증가함에 따라 문제를 제기한다. 이러한 컴포넌트들은 수조 개의 파라미터들에 도달할 수 있고, 높은 메모리 사용(Brock et al., 2017) 및 집중적인 룩업 동작들로 인한 저장 및 추론 병목 현상을 야기한다. 이를 해결하기 위해 임베딩 정밀도와 전체 모델 크기를 줄이는 모델 사전 압축 방법인 임베딩 테이블 양자화를 탐색한다. 예를 들어, fp32 엘리먼트들을 갖는 1000만 행 x 128의 임베딩 테이블을 사용하여, 8 비트 행-별 최소-최대 양자화(Kang et al., 2017)는 테이블 크기를 70% 이상 감소시킬 수 있다. 연구에 따르면 8비트 후 훈련 양자화는 훈련 인식 양자화와 달리 추가적인 훈련 비용이나 교정 데이터 요구 없이 성능과 추론 속도를 유지한다(Brock et al., 2017). 빠른 모델 전달, 엔지니어링 유연성, 원활한 모델 개발 및 배치를 보장하기 위해 우리는 특히 중간 최대 행별 임베딩 테이블 양자화를 사용하는 훈련 후 양자화를 선택한다. 최소값과 각 임베딩 행의 양자화 빈 스케일 값을 구하는 min-max 행 단위 양자화와 달리, 중간-max 양자화는 \\(\\mathbff_X}_{i_{c}}^{middle}=\\frac{\\mathbfff_X}_{i_{c}}^{max}x_{i_{c}}^{high-1}\\mathbfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff 양자화 및 역양자화 단계는 \\(\\mathbf{X}_{i_{c}}^{int}=round(\\frac{\\mathbff{X}_{i_{c}}-\\mathbf{X}_{i_{c}}^{ middle}}{\\mathbff{X}_{i_{c}}^{total}}) 및 \\(\\mathbff{X}_{i_{c}}^{max}-\\mathbff{X}_{i_{c}}^{scale}=\\frac{\\mathbff{X}_{i_{c}}^{middle}+\\mathbff{X}_{i_{c}}^{c}}^{min}-1}으로 설명된다. 여기서 \\(\\mathbfffffffffffffffffffffffffffffffffffffffffffffffffffff\n' +
      '\n' +
      '우리는 두 가지 이유로 중간-최대 양자화를 선택한다: (1) 임베딩 값은 일반적으로 정규 분포를 따르며, 양자화 범위의 중간에 더 많은 값이 집중된다. 이러한 중간 값을 보존하면 고밀도 값에 대한 양자화 오차가 줄어들어 일반화 성능이 향상될 수 있다. (2) \\(\\mathbf{X}_{i_{c}}^{int}\\) 값의 범위는 \\([-128,127]\\)에 속하며, 부구에서 int8로의 정수 주조 작업을 가역적으로 만들고 2의 보체 변환 문제, 즉 주조(x, int8), int32)는 \\(x\\in[0,255]\\일 경우 2의 보체 변환으로 인해 x와 같지 않을 수 있다. 실험 결과는 8비트 양자화가 일반적으로 전체 정밀도로 성능 패리티를 달성하고, 네이티브 TF 연산이 있는 CPU 서빙 환경에서도 합리적인 서빙 레이턴시를 유지한다는 것을 보여준다. Ads CTR 예측에서 우리는 온라인 테스트에서 +0.9% CTR 상대적 개선을 관찰했는데, 이는 양자화 평활 결정 경계, 보이지 않는 데이터에 대한 일반화 개선, 이상치 및 적대자에 대한 견고성 향상에 기인한다.\n' +
      '\n' +
      '##4. 훈련 확장성\n' +
      '\n' +
      '대순위 모델을 개발하는 동안 4D 모델 병렬성, 아브로 텐서 데이터셋 로더, 라스트 마일 변환을 비동기 단계로 오프로딩하고 데이터를 GPU에 프리페칭하여 훈련 속도를 크게 향상시켰다(표 2 참조). 아래에서는 왜 어떻게 개발했는지에 대한 설명을 제공합니다.\n' +
      '\n' +
      '###4D 모델 병렬화\n' +
      '\n' +
      '우리는 호로보드를 사용하여 여러 GPU를 사용하여 동기식 교육을 확장했다. 벤치마킹 과정에서 큰 임베딩 테이블의 그래디언트 동기화 동안 성능 병목 현상을 관찰하였다. TensorFlow(TF)에서 4차원 모델 병렬성을 구현하여 임베딩 테이블을 서로 다른 프로세스로 분산시켰다. 각 작업자 프로세스에는 모든 작업자 간에 공유되는 임베딩 테이블의 특정 부분이 하나씩 있습니다. 큰 임베딩 테이블에 대한 그라디언트 교환에 비해 통신 비용이 적은 전체-대-전체(특정 작업자에게 임베딩 룩업과 관련된 특징을 공유)를 통해 입력 특징을 교환함으로써 그라디언트 동기화 시간을 줄일 수 있었다. 우리의 벤치마크에서 모델 병렬은 훈련 시간을 70시간에서 20시간으로 단축했다.\n' +
      '\n' +
      '# Avro Tensor Dataset Loadet\n' +
      '\n' +
      '또한 벤치마크에 따라 기존 Avro 데이터셋 리더보다 최대 160배 빠른 TF Avro 리더를 구현하고 오픈소싱했다. 주요 최적화에는 불필요한 유형 검사 제거, I/O 작업 융합(파싱, 배치, 셔플링), 스레드 자동 밸런싱 및 튜닝이 포함됩니다. 데이터 세트 로더를 사용하여 대규모 순위 모델 훈련에서 흔히 볼 수 있는 훈련 작업에 대한 I/O 병목 현상을 해결할 수 있었다. e2e 훈련 시간은 벤치마크 결과에 따라 50% 감소했다(표 2).\n' +
      '\n' +
      '### 비동기 데이터 파이프라인으로의 라스트 마일 변환 오프로드\n' +
      '\n' +
      '우리는 훈련 루프 내부에서 일어나는 라스트 마일 내 모델 변환(ex. 빈 행을 채우기, 밀도로의 변환 등)을 관찰했다. 학습 루프에서 변환 + 학습을 동기적으로 실행하는 대신 비학습 관련 변환을 변환 모델로 이동시켰고, 학습 단계와 비동기적으로 발생하는 배경 I/O 쓰레드에서 데이터 변환이 이루어지고 있다. 훈련이 끝난 후, 우리는 서빙을 위해 두 모델을 최종 모델에 함께 스티칭했다. e2e 훈련 시간은 벤치마크 결과에 따라 20% 감소했다(표 2).\n' +
      '\n' +
      '### GPU에 데이터 세트를 미리 가져오기\n' +
      '\n' +
      '트레이닝 프로파일링 동안, CPU -> GPU 메모리 복사본이 트레이닝의 시작 단계에서 발생하는 것을 보았다. 배치 크기를 더 큰 값(훈련 시간의 최대 15%)으로 늘리면 메모리 복사 오버헤드가 중요해졌다. 다음 훈련 단계가 시작되기 전에 데이터 세트를 GPU에 병렬로 프리페치하기 위해 맞춤형 TF 데이터 세트 파이프라인 및 케라스 입력 계층을 활용했다.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      '우리는 사료 순위, 광고 CTR 예측 및 작업 권장 사항을 포함한 다양한 표면에 걸쳐 오프라인 절제 실험 및 A/B 테스트를 수행한다. 피드 랭킹에서 오프라인 재생 메트릭에 의존하며, 이는 생산 온라인 A/B 테스트 결과와 상관관계를 보여준다. 한편, Ads CTR 및 Job 권장 사항에 대해 오프라인 AUC 측정이 온라인 실험 결과와 잘 일치한다는 것을 발견했다.\n' +
      '\n' +
      '### Incremental Learning\n' +
      '\n' +
      '우리는 사료 순위 모델과 Ads CTR 모델 모두에 대해 점진적 훈련을 테스트했다. 실험 구성은 표 3에 설정되었다. 우리는 콜드 스타트 모델로 시작하고, 이어서 다수의 증분 트레이닝 반복(feed ranking 모델의 경우 6, Ads CTR 모델의 경우 4)이 뒤따른다. 각 점진적 훈련된 모델에 대해 고정된 테스트 데이터 세트에 대해 평가하고 메트릭을 평균화한다. 기준선은 콜드 스타트 모델을 사용하여 동일한 고정 테스트 데이터 세트에 대한 평가 메트릭이다.\n' +
      '\n' +
      '표 4 및 5는 저온 중량 및 \\(\\람다\\)을 조정한 후 사료 순위 모델과 Ads CTR 모델 모두에 대한 메트릭 개선 및 훈련 시간 개선을 요약한다. 두 모델 모두에 대해 점진적 훈련은 상당한 훈련 시간 감소와 함께 메트릭을 부스팅했다. 사료에 대한 기여도 측정은 SS5.2에 설명되어 있다.\n' +
      '\n' +
      '### Feed Ranking\n' +
      '\n' +
      '오프라인에서 Feed 순위 모델을 평가하고 비교하기 위해 모델의 온라인 기여율(예: 좋아요, 댓글, 재게시)을 추정하는 "재생" 메트릭을 사용한다. 평가를 위해 현재 생산 모델을 사용하여 모든 항목을 순위를 매기지만 상위 N 항목의 순서를 균일하게 무작위화하는 의사 무작위 순위 모델을 사용하여 링크드인 피드 세션의 작은 부분을 순위를 매긴다. 새로운 실험 모델을 학습한 후 오프라인에서 동일한 세션의 순위를 매깁니다. 매칭된 인상이 최상위 포지션에서 나타날 때("매칭된 임프 @ 1", 즉 두 모델이 모두 Feed 포지션 1에서 동일한 아이템을 랭킹한 것을 의미함) 그리고 랜덤화된 모델을 서빙한 멤버가 그 아이템에 기여하면, 우리는 실험 모델에 기여 보상을 할당한다: 기여율 = 기여도를 갖는 매칭된 임프 @ 1의 #\n' +
      '\n' +
      '* 매칭된 imps@1\n' +
      '\n' +
      '이 방법론은 실험 모델의 편향되지 않은 오프라인 비교를 허용한다(Han et al., 2017). 우리는 오프라인 리플레이를 사용하여 논문 전반에 걸쳐 "기여"라고 하는 사료 순위 모델을 평가한다(표 6). 이 표는 등속성 교정 계층, 저순위 DCNv2, 잔류 DCN, Dense Gating, Large MLP 계층, Sparse Features, MTL 강화, TransAct 및 Sparsely Gated MMoE를 포함한 다양한 생산 모델링 기술이 오프라인 재생 메트릭에 미치는 영향을 보여준다. 표 6에 나열된 이러한 기술은 발달의 시간 순으로 제시되어 점진적인 개선을 강조한다. 우리는 이러한 기술을 생산에 배포했으며 온라인 A/B 테스트를 통해 링크드인을 방문하는 회원 세션 수가 0.5% 상대적으로 증가하는 것을 관찰했다.\n' +
      '\n' +
      '### Jobs Recommendations\n' +
      '\n' +
      '잡 서치(JS)와 JYMBID(JYMBID) 랭킹 모델에서는 제목, 기술, 회사, 산업 및 연공서열에 대한 5개의 공유 임베딩 매트릭스를 통해 40개의 범주형 피쳐가 임베딩된다. 이 모델은 P(작업 애플리케이션)와 P(작업 클릭)의 확률을 예측한다. SS3.12에 기술된 임베딩 사전 압축을 모델 파라미터 수의 5배 감소시키면서 채택하였으며, 평가 결과 바닐라 id 임베딩 룩업 테이블을 사용한 것과 비교하여 어떠한 성능 손실도 나타나지 않았다. 우리는 또한 JYMBII에서 Dense Gating (SS3.5)을 사용하여 개선을 관찰하지 못했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline  & Contributions & Training Time \\\\ \\hline Cold Start & - & - \\\\ Incremental Training & +1.02\\% & -96\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4. 사료 순위 모델 결과 요약\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Experiments & Feed Ranking & Ads CTR \\\\ \\hline Cold Start Data Range & 21 days & 14 days \\\\ Incremental Data Range & 1 day & 0.5 day \\\\ Incremental Iterations & 6 & 4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3. 증분 실험 설정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Optimization Applied** & **e2e Training Time Reduction** \\\\ \\hline\n' +
      '4D Model Parallelism & 71\\% \\\\ Aroo Tensor Dataset Loader & 50\\% \\\\ Offload last-mile transformation & 20\\% \\\\ Prefetch dataset to GPU & 15\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. 모델의 광범위한 튜닝으로 훈련 성능 상대적 개선 JS. 이러한 엔티티 id 임베딩은 Job Search 및 JYMBII Recommendation에 의해 공유되고, 이어서 태스크-특정 2-계층 DCN이 위에 추가되어 특징 상호 작용을 명시적으로 캡처한다. 전반적으로 우리는 취업 탐색의 경우 +1.63%, JYMBII의 경우 2.10%의 상당한 오프라인 AUC 상승을 관찰한다. 재현성을 위해 SSA.8에서 JYMBII 및 Job Search 모델의 다양한 구성 요소에 대한 모델 아키텍처 및 절제 연구를 제공한다.\n' +
      '\n' +
      '위에 표시된 더 높은 AUC를 가진 순위 모델도 온라인 A/B 테스트에서 중요한 메트릭 리프트로 이전되어 잡 검색 및 JYMBII 전반에 걸쳐 적격 애플리케이션이 상대적으로 1.76% 개선되었다. 퍼센트 청구 가능한 보기는 홍보된 작업에 대한 모든 클릭 중 클릭의 비율입니다. 자격 있는 응용 프로그램은 모든 자격 있는 작업 응용 프로그램의 총 수입니다.\n' +
      '\n' +
      '### Ads CTR\n' +
      '\n' +
      '우리의 베이스라인 모델은 적절한 하이퍼-파라미터 튜닝을 갖는 그것의 선행된 GDMix 모델(도소프 외, 2017)로부터 유도된 다층 퍼셉트론 모델이다. 특징은 문맥, 광고, 회원, 광고주, 광고-회원 상호작용의 다섯 가지 범주로 분류된다. 기준선 모델에 ID 피쳐가 없습니다. 표 5에서 ID 임베딩, 양자화, 저순위 DCNv2, 트랜스액트 및 이소토닉 교정 레이어를 포함한 각 기술의 상대적 개선을 보여준다. 표에 언급된 기술은 개발 일정에 따라 정렬된다. 우리는 생산에 기술을 배포했으며 온라인 A/B 테스트에서 4.3% CTR 상대적 개선을 관찰했다.\n' +
      '\n' +
      '##6. 배포 교육\n' +
      '\n' +
      '개발 기간 동안 우리는 많은 배치 교훈을 배웠다. 여기에서 우리는 몇 가지 흥미로운 예를 제시한다.\n' +
      '\n' +
      '사료 훈련 데이터 생성의 스케일링\n' +
      '\n' +
      '피드 트레이닝 데이터 생성의 핵심에는 포스트 라벨과 피쳐 간의 결합이 있다. 레이블 데이터 세트는 모든 세션의 인상적인 게시물로 구성됩니다. 피쳐 데이터 집합은 세션 수준에 있습니다. 여기서, 각 행은 세션-레벨 피처들 및 그들의 포스트-레벨 피처들과 함께 제공되는 모든 포스트들을 포함한다. 이를 결합하기 위해, 우리는 기능 데이터 세트를 사후 레벨에 있도록 폭발시키고 라벨 데이터 세트와 결합한다. 그러나 Feed가 훈련에 13% 세션을 사용하는 것에서 100% 세션을 사용하는 것으로 확대됨에 따라 이 조인으로 인해 긴 지연이 발생했다. 파이프라인을 최적화하기 위해 런타임을 80% 줄이고 작업을 안정화하는 두 가지 주요 변경 사항을 만들었다. 첫째, 우리는 서비스된 게시물이 모두 인상적인 것은 아니라는 것을 깨달았다. 이는 레이블 데이터 세트와의 결합이 행 수를 크게 감소시킨다는 것을 의미한다. 또한, 기능 데이터 세트를 폭발시키는 것은 모든 포스트에 대해 세션 수준 기능을 반복한다. 따라서 우리는 파이프라인을 변경하여 게시 피쳐와 키만 폭발시키고 레이블과 결합하고 두 번째 조인에서 세션 수준 피쳐를 추가한다. 이로 인해 두 개의 조인이 발생했음에도 불구하고, 각 조인은 이제 더 작았고 60%의 전체 셔플 쓰기 크기 감소를 초래했다. 둘째, Spark 압축을 조정하여 추가로 25% 셔플 쓰기 크기를 줄였다. 이러한 변화를 통해 교육을 위해 100% 세션을 진행할 수 있었습니다.\n' +
      '\n' +
      '### Model Convergence\n' +
      '\n' +
      'DCNv2를 추가하는 것은 모델 훈련에 대한 도전과 함께 왔다. DCNv2를 사용한 초기 훈련 실험 동안 우리는 발산하는 많은 실행을 관찰했다. 모델 훈련 안정성을 향상시키기 위해 훈련 단계의 5%에서 50%로 학습률 워밍업을 증가시켰다. 이는 불안정성 문제를 해결하고 DCNv2를 추가함으로써 발생하는 오프라인 관련성 이득을 크게 증가시켰으며, 또한 Zhu et al., 2017에서 제시한 숫자 입력 특징에 배치 정규화를 적용하였다. 마지막으로, 우리는 훈련 단계의 수에서 우리가 적합하지 않다는 것을 발견했다. 이것은 훈련 단계를 증가시키면 오프라인 관련성 메트릭이 크게 향상된다는 것을 관찰했을 때 분명해졌다. 그러나 실험 속도의 감소로 인해 훈련 단계의 수를 늘리는 것은 생산을 위한 옵션이 아니었다. 해결책으로, 우리는 증가된 준비 단계를 감안할 때, 우리의 훈련이 더 높은 학습률에 대해 충분히 안정적이라는 것을 발견했다. 학습률을 3배 증가시키면 더 긴 훈련에 비해 우리가 발견한 관련성 메트릭 격차를 거의 완전히 해소할 수 있었다.\n' +
      '\n' +
      '우리는 최적화 요구가 다양한 모델에 따라 다르다는 것을 발견했다. 애덤은 일반적으로 효과적이지만, 수많은 희소 특징을 가진 모델에는 AdaGrad가 필요했으며, 이는 성능에 상당한 영향을 미쳤다. 또한 모델 일반화를 향상시키기 위해 학습 속도 워밍업 및 그라디언트 클리핑과 같은 전략, 특히 더 큰 배치 크기에 도움이 되는 전략을 사용했다. 우리는 더 큰 배치에 대해 학습률 워밍업을 일관되게 구현하여 배치 크기가 두 배가 될 때마다 단계들의 두 배 비율 이상으로 학습률을 증가시키지만 전체 훈련 단계의 60%를 초과하지 않았다. 에미레이트 항공\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model** & **Contributions** \\\\ \\hline Baseline & - \\\\ + 30 dimensional ID embeddings (IDs) & +1.89\\% \\\\ + Isotonic calibration layer & +1.08\\% \\\\ + Large MLP (LMLP) & +1.23\\% \\\\ + Dense Gating (DG) & +1.00\\% \\\\ + Multi-task (MTL) Grouping & +0.75\\% \\\\ + Low-rank DCNv2 (LDCNv2) & +1.26\\% \\\\ + TransAct & +1.66\\% \\\\ + Residual DCN (RDCN) & +2.15\\% \\\\ + LDCNv2+LMLP+TransAct & +3.45\\% \\\\ + RDCN+LMLP+TransAct & +3.62\\% \\\\ + Sparsly Gated MMoE & +4.14\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6. 상대적 오프 정책 측정에 대한 Feed 순위 내 모델 아키텍처 구성요소의 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Online Metrics** & Job Search & JYMBII \\\\ \\hline Percent Chargeable Views & +1.70\\% & +4.16\\% \\\\ Qualified Application & +0.89\\% & +0.87\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7. JS 및 JYMBII 랭킹의 온라인 실험 상대 메트릭 개선\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model** & AUC \\\\ \\hline Baseline & - \\\\ ID embeddings (IDs) & +1.27\\% \\\\ IDs+Quantization 8-bit & +1.28\\% \\\\ IDs+DCNv2 & +1.45\\% \\\\ IDs+low-rank DCNv2 & +1.37\\% \\\\ IDs+isotonic layer & +1.39\\% \\\\ (O/E ratio +1.84\\%) \\\\ IDs+low-rank DCNv2+isotonic layer & +1.47\\% \\\\ +2.20\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8. 시험 AUC에 대한 상이한 Ads CTR 모델 아키텍처 변이체의 절제 연구.\n' +
      '\n' +
      '따라서 다양한 설정에 걸쳐 일반화를 개선하고 더 큰 배치 크기에서 일반화의 격차를 좁혔다.\n' +
      '\n' +
      '## 7. Conclusion\n' +
      '\n' +
      '본 논문에서는 최신 모델 개발 경험을 요약한 _LiRank_ 프레임워크를 소개하였다. 관련 사용자 추천을 제공하기 위한 고성능 모델을 생성하기 위해 다양한 모델링 아키텍처와 그 조합에 대해 논의했다. 이 논문에서 공유된 통찰력은 업계 전반에 걸쳐 실무자에게 도움이 될 수 있습니다. _ LiRank_는 LinkedIn에서 여러 도메인 애플리케이션에 배치되어 상당한 생산 효과를 가져왔다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Agarwal et al. (2015) Deepak Agarwal, Bee-Chung Chen, Qi He, Zhenhao Hu, Guy Lebanon, Yiming Ma, Pamangadatta Shiwaswamy, Hiao-Ping Tseng, Jaewon Yang, and Liang Zhang. 2015. Personalizing LinkedIn _KDD_.\n' +
      '* Anil et al. (2022) Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd Phillips, Cristina Pov, Revin Eang, Gil I Shamir, Rakesh Shivanna, and Qiqi Yan. 2022. Factory Floor: ML Engineering for Industrial-scale Ads Recommendation Models. _RecSys_에서.\n' +
      '* Antoine et al. (2017) Limassant Antoine, Rizki Gallume, Chikhi Rayan, and Peterlongo Pierre. 2017. 대용량 키 집합에 대한 빠르고 확장 가능한 최소 완벽 해싱__ SEA_(2017).\n' +
      '* Chen et al. (2019) Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in alibaba. 고차원 희소 데이터를 위한 딥러닝 실습에 관한 제1차 국제 워크숍의 _Proceedings_. 1-4\n' +
      '* Cheng et al. (2016) Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispar, et al. 2016. Wide & Deep Learning for Recommender Systems. _ Proceedings on Deep Learning for Recommender Systems_ (2016). [https://doi.org/10.1145/298802.298854] (https://doi.org/10.1145/298802.298854)\n' +
      '*Cheng et al. (2019) Weiyu Cheng, Yanyan Shen, and Limepeng Huang. 2019. 적응 인수분해 네트워크 : 학습 적응-순서 특징 상호작용 _ ArXiv_ abs/1909.03276 (2019). [https://api.semanticshelon.org/CorpusID.202539143] (https://api.semanticshelon.org/CorpusID.202539143)\n' +
      '* Coide(2020) The Apache Software Foundation. 2020. _Apache Commodec_. [https://commons.apache.org/proper/commons-codec] (https://commons.apache.org/proper/commons-codec)/Base4 및 Hex와 같은 인코딩과 함께 작업하기 위한 유틸리티들의 라이브러리.\n' +
      '* Goodfellow et al. (2013) Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2013. empirical investigation of catastrophic forgetting in gradient-based neural networks. _ arXiv preprint arXiv:1312.6211_ (2013).\n' +
      '*Guan et al. (2019) Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, and Hector Yuen. 2019. 임베딩 테이블들에 대한 포스트 트레이닝 4비트 양자화. _ ML_(2019)을 위한 시스템 상의 NeurIPS MLSys Workshop에서 필리드되었다.\n' +
      '* Guo et al. (2017) Chun Guo, Geoff Pleias, Yu Sun, and Kilian Q Weinberger. 2017. On Calibration of Modern Neural Networks. _ICML_에서.\n' +
      '* Guo et al. (2021) Chun Guo, Geoff Pleias, Yu Sun, and Kilian Q. 와인버거 2021. 신경망에 대한 소프트 캘리브레이션 목표. _NeurIPS_에서.\n' +
      '* Guo et al. (2017) Huifeng Guo, Ruiming Zhang, Yiming Ye, Zhenguo Li, Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. _ ArXiv_ abs/1703.04247 (2017). [https://api.semanticscholar.org/CypusID-970388] (https://api.semanticscholar.org/CypusID-970388)\n' +
      '* Haldar et al. (2019) Malay Haldar, Mustafa Abloel, Prabanf Ramanathan, Tao Xu, Shulin Yang, Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C. Turnbull, Berendam M. 콜린스랑 토마스 레그랜드요 2019. Airbnb Search에 Deep Learning 적용. KDD\n' +
      '* Huang et al. (2020) Tongwen Huang, Qingyu She, Zhiqiang Wang, and Junlin Zhang. 2020. GateNet: Click-Through Rate 예측을 위한 Gating-Enhanced Deep Network for Click-Through Rate Prediction _ CoRR_ abs/2007.03519 (2020).\n' +
      '* Jun et al. (2021) Shi Jun, Jiang Chengming, Gupta Aman, Zhou Mingzhou, Ouyang Yunbo, Xiao Charates, Song Qingsuan, Wu Alice, Wei Hachao, and Huiji Gao. 2022. 일반화된 딥 혼합 모델. _Proceedings of the 28th ACM SIGKDD international conference on knowledge discovery & data mining_.\n' +
      '* Kripatkpatrick et al. (2016) James Kripatkpatrick, Ravan Pascanu, Neil Rabinovich, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Rannah, Agnieska Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2016년, 신경망에서 치명적인 망각 극복 In _Proceedings of the National Academy of Sciences_.\n' +
      '* Li et al. (2011) Libong Li, Wei Chu, John Langford, Tseus Moon, and Xuanhui Wang. 2011. 일반화된 선형 모델을 이용한 문맥 대역 알고리즘의 비편향 오프라인 평가 _OTAE_에서.\n' +
      '* Lian et al. (2018) Jianxun Lian, Xiaohuan Zhou, Fuheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xlDeepFM: 추천 시스템을 위한 명시적 특징 상호작용과 내포적 특징 상호작용의 결합. KDD(2018). [https://api.semanticscholar.org/CorpusID:3930042] (https://api.semanticscholar.org/CorpusID:3930042)\n' +
      '* Ma et al. (2018) Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018. Modeling Task Relationship in multi-task learning with multi-gate mixture-of- experts. _KDD_에서. 1390-1393.\n' +
      '* Mao et al. (2023) Kelong Mao, Jieming Zhu, Liangxi Su, Guohao Cai, Yuru Li, Zhenhua Dong. 2023. FinallMP: CTR 예측을 위한 향상된 Two-Stream MLP 모델. _AAAI Conference on Artificial Intelligence_. [https://api.semanticscholar.org/CorpusID:257913572] (https://api.semanticscholar.org/CorpusID:257913572)\n' +
      '* Naumumov et al. (2019) Maxim Naumov, Dheevats Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayan Sundaram, 종수 Park, Xiaodeng Wang, Udit Gupta, Carole-jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Chemeriswelski, Yinghai Lu, Raghuraman Krishnamoorthi, Anshya Yu, Volodymy Kanderatenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Mikhail Smelyanskiy. 2019. 개인화 및 추천 시스템을 위한 딥러닝 추천 모델 _ ArXiv_ abs/1906.00091 (2019). [https://api.semanticscholar.org/CorpusID:27999641] (https://api.semanticscholar.org/CorpusID:27999641)\n' +
      '* Naumov et al. (2019) Maxim Naumov, Dheevats Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayan Sundaram, 종수 Park, Xiaodong Wang, Udit Gupta, Carole-jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Chemeriswelski, Yinghai Lu, Raghuraman Krishnamoorthi, Anshya Yu, Volodymy Kanderatenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill J. Liao, Xiong and Misha Smelyanskiy. 2019. 개인화 및 추천 시스템을 위한 딥러닝 추천 모델 _ CoRR_ abs/1906.00091 (2019). [https://arxiv.org/abs/1906.00091] (https://arxiv.org/abs/1906.00091)\n' +
      '* Pancha et al. (2022) Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. Pinner former: Sequence Modeling for User Representation at Pinterestest. _KDD_에서. 3702-3712.\n' +
      '* 파스카누 및 벵지오(2013) 라즈반 파스카누 및 요슈아 벵지오. 2013. Revisiting natural gradient for deep networks. _ arXiv preprint arXiv:1301.3584_ (2013).\n' +
      '* Riquenite et al. (2018) Carlos Riquenite, George Tucker, and Jasper Snock. 2018. Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling. arXiv:1802.09127\n' +
      '* Shazeer et al. (2017) Noam Shazeer, Azalia Mirhossehi, Krynstor Maziaz, Andy Davis, Quoc V. 르, 제프리 E. 힌튼 그리고 제프 딘 2017. 터무니없이 큰 신경망: 희박하게 게이트된 혼합물-of-Expserve 레이어. _ICLR_에서.\n' +
      '* Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. 마호니, 커트 커처 2020. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT _ AAAI Conference on Artificial Intelligence_34(2020).\n' +
      '* Shi et al. (2019) Hao-Jun Michael Shi, Dheevatsu Mudigere, Maxim Naumov, and Jiyan Yang. 2019. Complementary Partition을 이용한 메모리 효율적인 추천 시스템을 위한 합성 임베딩 CoRR_ abs/1909.02107 (2019).\n' +
      '* Song et al. (2018) Weiping Song, Chencie Shen, Zhiping Xiao, Zhijun Duan, Yewen Xu, Ming Zhang, 및 Jian Tang. 2018. AutoInt: Self Attentive Neural Networks를 통한 자동 특징 상호작용 학습__ 제28회 ACM International Conference on Information and Knowledge Management_ (2018). [https://api.semanticscholar.org/CorpusID:53100214]의 진행 (https://api.semanticscholar.org/CorpusID:53100214)\n' +
      '* Tang et al. (2020) Hongyan Tang, Junming Liu, Ming Zhao, and Xudong Gong. 2020. 점진적 계층 추출(ple): 개인화된 추천을 위한 새로운 다중 작업 학습(mlr) 모델. In _Proceedings of the 14th ACM Conference on Recommender Systems_. 269-278\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 2017. 주의력만 있으면 됩니다 _ 신경 정보 처리 시스템들_30의 발전들(2017).\n' +
      '* 비냐(2019) 세바스티아노 비냐. 2019. _Jstatail_. [https://github.com/vigna/fastuitl] (https://github.com/vigna/fastuitl) a Java library for fast type-specific collections.\n' +
      '* Wang et al. (2017) Ruoxu Wang, Bin Fu, G. Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. _ ADKDD\'17_(2017)의 진행. [https://api.semanticscholar.org/CrpusID:6011288] (https://api.semanticscholar.org/CrpusID:6011288)\n' +
      '* Wang et al. (2021) Ruoxu Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021. Dev 2: 시스템 순위를 매기기 위한 웹-스케일 학습을 위한 개선된 심층 및 교차 네트워크 및 실제 수업. u_Proceedings of the web conference 2021_.\n' +
      '* Wang et al. (2020) Ruoxu Wang, Rakesh Shivanna, Derek Zhiyuan Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed H. Chi. 2020. DCN V2: 향상된 딥 & 크로스 네트워크 및 웹-스케일 학습을 위한 실무 수업 순위 시스템_ Proceedings of Web Conference 2021_(2020). [https://api.semanticscholar.org/CorpusID:28448398] (https://api.semanticscholar.org/CorpusID:28448398)\n' +
      '* Yang et al. (2023) Xue Xia, Peng Eksombachachach, Niki Panda, Dhruyun Debarah Badan, Pio-Wei Wang, Neng Gu, Saurabh Vishvshvsh Joshi, Nazarian Farahpour, Zhiyuan Zhang, and Andrew Zhai. 2023. TransAct: Pinterest에서 추천을 위한 트랜스포머 기반 실시간 사용자 행동 모델 _ arXiv preprint arXiv:2306.00248_(2023).\n' +
      '\n' +
      '## 부록 A. 재현성을 위한 정보\n' +
      '\n' +
      '### Feed Ranking Sparse ID 특징\n' +
      '\n' +
      '(Bordes and McAteg, 2017)과 같이 뷰어-액터 애피니티와 유사한 뷰어-액터 애피니티, (2) 게시물의 생성자인 배우 Id, (3) 게시물의 생성자가 과거에 자주 상호작용한 생성자인 배우 역사 배우 Ids, (4) 게시물의 생성자가 과거에 자주 상호작용한 뷰어 해시태그 Ids, (5) 게시물의 행위자가 과거에 자주 상호작용한 배우 해시태그 Ids, (6) 게시물 해시태그 Ids(예를 들어 #머신러닝)를 포함한다.\n' +
      '\n' +
      'SS3.12에 설명된 무제한 사전 희소 ID 특징을 사용했으며, Id 임베딩에 최적인 30개의 차원을 경험적으로 발견했다. 위에서 언급된 희소 id 임베딩 특징들은 다른 모든 조밀한 특징들과 연결된 다음, 각각 100의 출력 차원을 갖는 4개의 연결된 계층들로 구성된 다중 레이어 인식(MLP)을 통과한다.\n' +
      '\n' +
      '큰 모델을 제공하기 위한### 어휘 압축\n' +
      '\n' +
      '대형 개인화 모델의 ID는 문자열과 희소 숫자 값인 경우가 많다. 고유한 희소 ID를 충돌 없이 임베딩 인덱스에 매핑하려면 일반적으로 해시 테이블(예: TF에서 std::unordered_map)로 구현되는 룩업 테이블이 필요하다. 이러한 해시 테이블은 여러 GB로 성장하며 종종 모델 매개변수보다 훨씬 더 많은 메모리를 차지합니다. 서빙 메모리 문제를 해결하기 위해 TF Custom Ops에서 최소 완전 해싱 기능(MPHF)(Bordes and McAteg, 2017)을 구현하여 어휘 검색의 메모리 사용량을 100배 줄였다. 그러나 우리는 해싱이 훈련의 일환으로 즉석에서 수행됨에 따라 훈련 시간이 3배 느려지는 상황에 직면했다. ID의 최대값은 int32를 사용하여 나타낼 수 있음을 관찰하였고, 학습 시간을 저하시키지 않고 어휘를 압축하기 위해 먼저 문자열 id를 int32로 해싱(Bordes and McAteg, 2017)한 후, (Sandel, 2017)에서 제공하는 지도 구현을 사용하여 어휘를 저장하였다. 해싱을 수행하기 위해 스파크 작업을 사용했기 때문에 훈련 시간 저하를 피할 수 있었다. 끈에서 int32로의 해싱은 93% 힙 크기 감소를 제공했다. 해싱으로 인해 참여 지표에서 유의미한 저하를 관찰하지 못했다.\n' +
      '\n' +
      '섹션 SS3.12에서 언급한 후속 노력은 충돌 방지 해싱 및 QR 해싱 기술을 사용하여 모델 아티팩트에서 정적 해시 테이블을 성공적으로 제거했다. 이 제거는 런타임과 관련성 관점을 모두 고려하여 성능 저하 없이 달성되었다.\n' +
      '\n' +
      '### ID 임베딩과 인메모리 서빙의 외부 서빙\n' +
      '\n' +
      '문제 중 하나는 서빙 호스트에 대한 메모리가 제한되어 여러 모델의 배치를 방해하는 것이었다. 배송을 촉진하기 위해 처음에는 키 값 저장소에서 모델 매개변수의 외부 서빙(그림 6 참조), 분할 모델 그래프 및 온라인 검색을 위한 사전 컴퓨팅 임베딩을 채택했다. 우리는 (1) ID 임베딩 소비에 의존하는 ML 엔지니어의 반복 유연성과 (2) 온라인 스토어에 매일 푸시되는 사전 컴퓨팅 기능의 교착성 문제에 직면했다. 메모리로부터 10억 파라미터 모델을 동시에 처리하기 위해 가비지 컬렉션 튜닝을 통해 하드웨어를 업그레이드하고 메모리 소비를 최적화했으며, 양자화 및 ID 어휘 변환을 통해 모델 파라미터에 대한 데이터 표현을 설계하여 메모리 사용을 최적화했다. 인메모리 서비스로 전환함에 따라 향상된 인게이지먼트 메트릭과 운영 비용이 절감된 권한 있는 모델러를 산출했습니다.\n' +
      '\n' +
      '###4D 모델 병렬화\n' +
      '\n' +
      '도 7은 세 개의 임베딩 테이블에 대한 예를 도시한다. 각각의 임베딩 테이블은 GPU 상에 배치되고, 각각의 GPU의 입력 배치는 모든 GPU가 자신의 임베딩 테이블에 속하는 입력 열들을 수신하도록 올-투-올\'된다. 각 GPU는 로컬 임베딩 룩업을 수행하고 룩업은 모두 실행되어 입력 열이 가져온 GPU로 출력을 반환합니다. 이들 레이어에 대한 그라디언트를 교환하는 것은 비용이 많이 들지 않기 때문에 더 적은 파라미터(예를 들어, MLP 레이어)를 갖는 다른 레이어는 여전히 데이터 병렬 방식으로 처리된다. 우리의 벤치마크에서 모델 병렬은 훈련 시간을 70시간에서 20시간으로 단축했다.\n' +
      '\n' +
      '사용자 이력 모델을 위한 시퀀스 길이에 대한### 실험\n' +
      '\n' +
      '이력 길이가 표 9의 Feed 순위 모델의 영향에 미치는 영향에 대한 연구를 제시한다. SS3.7에 설명된 시퀀스 아키텍처보다 사용자 인게이지먼트의 더 긴 이력을 사용함에 따라 인게이지먼트의 증가 추세를 관찰한다.\n' +
      '\n' +
      '도 6. 외부 서빙 전략\n' +
      '\n' +
      '도 7. 대형 임베딩 테이블에 대한 모델 병렬성\n' +
      '\n' +
      '### Feed Ranking Model Architecture\n' +
      '\n' +
      '그림 8에서는 모델의 흐름과 모델의 다른 부분이 서로 어떻게 연결되어 있는지 제공하기 위한 Feed Model 아키텍처 다이어그램을 제시한다. 우리는 다른 모듈의 배치가 기술의 영향을 크게 변화시킨다는 것을 발견했다.\n' +
      '\n' +
      '### Vocabulary Compression\n' +
      '\n' +
      '그림 9에서는 QR과 Murmur 해싱을 이용한 비 정적 어휘 압축의 예시도를 제시한다. "member:1234"와 같은 문자열 형식의 멤버 ID\\(A\\)는 충돌 방지 스테이트리스 해싱 방법(예: Murmur 해싱)으로 int64 또는 int32의 공간에 매핑되며, 더 큰 공간은 더 낮은 충돌률을 초래할 것이다. 우리의 경우 int64를 사용하고, 이 int64를 int32 공간(0에서 \\(2^{32}-1\\))에서 두 숫자로 변환하기 위해 비트캐스트를 사용한다.\n' +
      '\n' +
      '### 일자리 추천 순위 모델 아키텍처\n' +
      '\n' +
      '도 10에 도시된 바와 같이, 일자리 추천 순위 모델은 단일 모델에서 구직(JS) 및 구직(JYMBII) 작업에 관심을 가질 수 있는 구직(JYMBII) 작업을 통합하는 다중 작업 훈련 프레임워크를 채용한다. id 임베딩 행렬들은 두 개의 태스크들이 공유하기 위해 하부 레이어에 추가되고, 그 다음에는 태스크-특정 2-레이어가 추가된다. DCNv2를 사용하여 피쳐 상호 작용을 학습합니다. 특징 상호 작용의 서로 다른 아키텍처를 적용하기 위해 다양한 실험을 수행했으며, 2-계층 DCN이 가장 우수한 성능을 보였다. JYMBII 과제에 대한 결과는 표 17에 제시되어 있다.\n' +
      '\n' +
      '도 8. 사료 랭킹 모델 아키텍처\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model** & Contributions \\\\ \\hline Baseline & - \\\\ + Member history length 25 & +1.31\\% \\\\ + Member history length 50 & +1.57\\% \\\\ + Member history length 100 & +1.66\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9. 상이한 시퀀스 길이를 갖는 멤버 이력 모델링의 추가로부터의 피드에 대한 오프라인 관련성 메트릭.\n' +
      '\n' +
      '도 10. 일자리 추천 랭킹 모델 아키텍처\n' +
      '\n' +
      '도 9. 비 정적 어휘 해싱 패러다임의 예\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>