<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '실시간 제로-쇼트 음성 변환# 스트림 보이스입니다.\n' +
      '\n' +
      ' 자히차오 왕\\({}^{1}\\)\n' +
      '\n' +
      'Yuanzhe Chen\\({}^{2}\\)\n' +
      '\n' +
      'Xinsheng Wang\\({}^{1}\\)\n' +
      '\n' +
      'Zhuo Chen\\({}^{2}\\)\n' +
      '\n' +
      'Lei Xie\\({}^{1}\\)1\n' +
      '\n' +
      'Yuping Wang\\({}^{2}\\)\n' +
      '\n' +
      'Yuxuan Wang\\({}^{2}\\)\n' +
      '\n' +
      '\\({}^{1}\\) 아이오디오, 스피치 및 언어처리 그룹(ASLP@NPU)\n' +
      '\n' +
      '컴퓨터 과학 대학교, 북서부 폴리테크놀로지 대학, 중국 시안, 중국 중국 컴퓨터 학부 학부.\n' +
      '\n' +
      '\\({}^{2}\\)ByteDance Inc.\n' +
      '\n' +
      'Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '스트리밍 처리 시 LM의 잠재적 성능을 향상시키며, 기존 LM 기반 모델(SOS)의 의미-인식(SUV)의 의미-인식(SEM)의 실행 능력, 즉 특정 시간-인식(VC)에 대한 새로운 정보-인식(SUV)를 통해 예측된 컨텍스트-인식(XT)를 요약하고, 특히, 기존 LM-인식된 컨텍스트-인식(SEM)의 예측(SEM)을 통해 의미-인식(SEM)에 대한 의미-인식(SEM)에 대한 의미-인식(SEM)에 대한 의미-인식(SEM)에 대한 의미-인식(II)에 대한 의미-인식-인식(s)의 예측의 능력을 향상시키며, 기술-인식(SEM)에 대한 의미-인식(S-인식-인식-인식(S-인식-인식-인식-인식(S-인식-인식-인식(S-인식-인식-인식-인식-인식-인식-인식-인식(S-인식-인식-인식-인식-인식-인식-인식(S-인식)의 예측의 가능성-인식(S-인식-인식(S-인식)에 대한 인식)에 대한 인식(S 특히, 스트림보이스는 미래 룩-헤드가 없는 첫 번째 LM 기반 스트리밍 제로 샷 VC 모델이다. 실험 결과는 비스트림 VC 시스템에 필적하는 제로 샷 성능을 유지하면서 스트림 보이스의 스트리밍 변환 능력을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '음성 변환(VC)은 언어적인 내용을 변경하지 않고 화자의 음성을 다른 화자의 음성으로 전달하는 것을 목표로 한다. 이러한 기술은 영화 더빙, 프라이버시 보호 및 발음 수정과 같은 많은 실제 응용 프로그램에 배치되었다. 자동 음성 인식(ASR) 시스템에서 병목 특징(BNF)과 같은 신경 의미 특징의 도움으로 야생의 임의의 스피커에서 소스 스피치를 변환하는 것이 [21]에 성공적으로 달성되었다. 한편, 이른바 _제로 샷 VC_인 이 화자의 한 발화만으로 임의의 목표 화자로 변환되는 것도 최근 [18, 23] 연구되어 왔다. 그러나 대부분의 기존 제로 샷 VC 모델은 오프라인 시스템을 위해 설계되었으며, 이는 라이브 방송 및 실시간 통신(RTC)과 같은 실시간 VC 애플리케이션에서 스트리밍 능력의 최근 증가하는 수요를 충족시키기에 불충분하다. 이 연구에서 우리는 그림 1에 조명된 바와 같이 _스트림 제로 샷 VC_에 초점을 맞추고 있다.\n' +
      '\n' +
      '시맨틱 콘텐츠 및 스피커 타임브레스와 같은 다양한 구성 요소에 대한 디멘토링 스피치는 제로샷 VC 태스크[16, 24, 25]에서 중요한 역할을 한다. 최근 강력한 LM 프레임워크와 트레이닝 데이터의 스케일링을 통해 혜택을 받는 LM 기반 VC 모델[24, 25] 인컨텍스트 학습 능력이 내장된 LM 기반 VC 모델은 소스 및 타겟 스피커 발화의 컨텍스트 관계를 감지하여 미세 작곡된 스피커 타임브론을 캡처할 수 있어 인상적인 제로샷 VC 성능을 달성할 수 있다. 그러나 완전한 소스 스피치 발화를 요구하면 이러한 LM 기반 VC 모델이 실시간으로 제한된다.\n' +
      '\n' +
      '그림 1: 널리 사용되는 인식 합성 프레임워크를 사용하는 스트리밍 제로 샷 VC의 개념. 드림보이스는 이 인기 있는 패러다임에 세워져 있습니다.\n' +
      '\n' +
      '따라서 이러한 모델은 오프라인 애플리케이션에서만 사용할 수 있습니다. 0샷 V를 스트리밍하기 위해 여러 가지 비LM 기반 방법[14, 15]이 제안되었지만, 성능은 주로 훈련 데이터를 스케일링하는 모델 용량이 제한되어 있고 스트리밍 시나리오에서 미래의 정보가 누락되어 발생하는 성능 저하로 인해 화자 유사성과 음성 자연성이 높은 스피커를 일반화하기에는 잘 실패한다.\n' +
      '\n' +
      '0샷 VC에서 LM 기반 모델의 성공에 의해 영감을 얻은 우리는 스트리밍 VC 시나리오에 대한 LM의 타당성을 탐구하는 것을 목표로 한다. 직관적인 방법은 그림과 같이 대중적 인식 합성 틀을 따르는 것이다. 말화는 스트리밍 ASR 및 오디오 코덱에 의해 각각 추출된 의미적 BNF 및 음향 특징들로 표현된다. 그런 다음 LM 기반 VC 모델은 의미 정보를 타겟 스피커의 타임브러프로 음향 특징들로 변환한다. 그러나 스트리밍 제로 샷 VC에서 LM 기반 모델의 개발은 두 가지 주요 과제로 인해 방해를 받는다.\n' +
      '\n' +
      '* ** 재생 가능한 아키텍처:** 스트리밍 모델은 일반적으로 미래 시간 단계에 의존하지 않고 현재 입력을 수신할 때 즉각적인 출력을 생성한다. 현재 LM 기반 VC 모델은 스트리밍 애플리케이션의 요구를 충족하지 못하는 소스 스피치의 전체 활용도를 얻을 때만 변환을 수행한다. 다층 코덱 예측을 위한 널리 채택된 다단계 언어 모델링은 시스템 설계에 복잡성을 도입하여 누적 오류의 잠재적 위험을 초래한다. 또한 스트리밍 파이프라인의 종속성 모델은 VC 모델의 설계 및 성능에 영향을 미친다.\n' +
      '* ** 성능 갭:**는 비스트림 모델과 달리 스트리밍 모델은 향후 정보 없이 플라이에서 프레임 방향 또는 청크된 입력을 인과적으로 처리해야 하며, 누락된 컨텍스트 및 잠재적인 성능 저하에 직면해야 한다. 이 누락된 것은 스트리밍 VC 모델이 고품질 전환을 달성하는 것을 방해한다. 또한 그림과 같이 그림 1과 같다. 1, VC 모델은 ASR에서 변환 달성을 위해 의미론적 특징 BNF에 의존하며, 이는 의미론적 특징을 매우 중요하게 만든다. 그러나 스트리밍 ASR은 비스트림 대응물에 비해 열등한 성능을 보이며, 추출된 BNF는 낮은 품질의 의미 정보를 가지고 있지만 더 많은 스피커 정보를 가지고 있다. 이 저품질 시맨틱 입력은 고유한 미래의 수신이 불가능한 것 외에도 고품질 전환을 더 어렵게 만듭니다. 제로샷 VC의 목표는 스트리밍 VC 모델이 직면한 과제를 증폭시킵니다.\n' +
      '\n' +
      '본 연구에서는 교사의 유도 의미 예측과 의미 마스킹이 통합되어 전환 품질 향상을 위한 모델에 대한 맥락 인식을 향상시키는 고품질 제로 샷 VC에 대한 스트리밍 LM 기반 모델인 _StreamVoice_를 제안한다. 구체적으로, 스트림 보이스는 음향 예측기의 콜라보레이션과 음향 코덱을 무력하게 생성하는 단일 단계 언어 모델을 통합하여 스트림 가능한 아키텍처를 생성한다. 각 시간 단계에서 의미론적 및 음향적 특징의 입력을 대체하면 원활한 스트리밍 행동이 보장됩니다. 누락된 상황 정보로 인한 성능 격차를 완화하기 위해 LM의 맥락 인식을 높이기 위해 두 가지 방법을 소개한다. 1) 우리는 교사 유도 의미 예측력을 통합하는데, 여기서 VC 모델은 교사가 요약한 현재와 미래의 의미 정보를 추론하기 위해 교사 비스트림 ASR 모델에 의해 학습되어 음향 예측을 향상시키는 데 사용된다. 2) 입력 이력으로부터 컨텍스트 학습을 향상시키기 위해 의미 마스킹은 선행 음향 및 중단 의미 입력으로부터의 음향 예측을 장려하는 데 사용되며, 이는 또한 소스 스피커의 정보를 줄이기 위해 정보 병목 현상을 암묵적으로 생성한다.\n' +
      '\n' +
      '실험은 비스트림 VC 시스템에 필적하는 성능을 유지하면서 볼 수 있는 스피커와 미인 스피커 모두에 대해 스피커 유사도가 높은 스트리밍 방식으로 연설을 변환할 수 있는 스트림 보이스의 능력을 보여준다. 미래 룩-헤드가 없는 첫 번째 LM 기반 제로 샷 VC 모델로서 스트림 보이스의 전체 파이프라인은 엔지니어링 최적화가 없는 단일 A100 GPU에서 실시간보다 2.4배 빠른 124 ms 레이턴시만 가지고 있다.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '제로샷 음성 반전.\n' +
      '\n' +
      '제로 샷 VC는 스피치 디커플링 및 스피커 타임브러 캡쳐에 엄격한 요구를 부과한다. 많은 연구에서 음성 탈커플링을 달성하기 위해 복잡한 구조[17], 손실 함수[16], 훈련 전략[13]을 통합하는 많은 이젠트 접근법을 구체적으로 설계한다. VC 훈련에서 명시적인 무력화 설계를 임베딩하기보다는 일부 접근 방식[18]은 화자 표현을 위한 화자 검증(SV) 모델을 레버리지하는 반면, ASR 또는 자기 지도 학습(SSL) 모델[16, 15]을 사용하여 언어적 내용을 추출한다. 화자 타임브릿 포획을 향상시키기 위해 일부 미세 구성 화자 모델링 방법도 [16, 17]를 탐색했다. 생성 작업에서 언어 모델의 최근 성공은 제로 샷 VC에서 LM 기반 모델의 탐구를 촉발하여 인상적인 결과를 얻었다. 사전 훈련된 모델을 디커플 스피치로 사용하여 LM 기반 VC 모델[16, 17, 18]은 스피커 프롬프트로부터 미세 개질된 스피커 타임브러를 캡처한 다음 변환을 수행할 수 있다. 그러나 현재 LM 기반 VC 모델은 스트리밍 시나리오에 적용 가능하며 실제 효용성을 제한한다. 본 논문은 스트리밍 시나리오에 구체적으로 맞춘 언어 모델의 제로 샷 능력을 조사하여 이러한 격차를 다룬다.\n' +
      '\n' +
      '음성 반전.\n' +
      '\n' +
      '비스트림 VC 모델에 의해 달성된 고품질 변환에도 불구하고, 그들의 비스트림 구조 및 풀-커패시턴스 입력에 의존하면 실시간 스트리밍 애플리케이션에 방해가 된다. 스트리밍을 위해서는 인과적 처리와 스트리밍 파이프라인의 구조가 중요한 고려 사항이다. 스트리밍 모델은 플라이에서 프레임 방향 또는 청크된 입력을 처리하도록 강요되어 향후 정보에 대한 접근이 없어 비스트림 대응물에 비해 성능 저하로 이어진다. 이를 해결하기 위해 코믹 모노 접근[11, 12, 13]은 스트리밍 모델의 훈련 또는 비스트림 모델에서 지식의 증류을 안내하는 교사 모델의 통합을 포함한다. 헨 et al. [14]는 계층별 분석을 통해 최소한의 의미 정보 손실로 BNF를 선택하는 데 초점을 맞추고, Chen et al. [14]는 의미론적 특징의 품질을 향상시키기 위해 적대적 훈련을 통합한다. VC 스트리밍을 넘어 최근에는 제로 샷 VC 스트리밍에 대한 노력도 있었다. 예를 들어, 비스트림 애플리케이션을 위해 설계된 VQMIVC[13]은 양 등(14)에 의해 간소화될 수 있도록 수정된다. ALO-VC[14]는 SV 모델, 스트리밍 PPG 추출기 및 피치 추출기를 사용하여 스트리밍 시스템을 구성한다. 그러나 저자원 장치를 위해 설계된 현재 스트리밍 제로 샷 VC는 일반화가 좋지 않은 모델 용량을 스피커로 제한하여 유사성과 자연성이 떨어지는 결과를 낳는다. LM이 제로 샷 VC에서 성공함으로써 스트리밍 시나리오에서 스트리킹 가능한 LM을 설계합니다. VC 스트리밍에서 독특한 과제를 해결하기 위해 LM의 맥락 인식을 높이고 전환 품질을 향상시키기 위해 교사 유도 의미 예견 및 의미 마스킹을 소개합니다.\n' +
      '\n' +
      '언어 모델 기반 스피치세대입니다.\n' +
      '\n' +
      '최근 몇 년 동안 자연어 처리 내의 언어 모델(LM)의 발전은 강력한 생성 능력을 나타내어 스피치 생성에서 LM의 발달에 영향을 미친다. 코덱[15] 또는 기타 SSL 모델[10]을 사용하여 음성 및 오디오를 개별 단위로 효율적으로 토큰화하여 저비트레이트 오디오 표현 및 의미 정보 추출을 용이하게 할 수 있다. 이러한 진행으로 스피치 생성이 LM 프레임워크를 원활하게 활용할 수 있습니다. 오디오 생성을 조건부 언어 모델링 과제로 삼고, 오디오LM[14]과 뮤직LM[14]는 거친 단위로 음향 예측을 위한 계층적 언어 모델링을 사용한다. VALL-E[14] 및 SpearTTS[14]는 0 샷트 TTS에 대한 LM을 확장하며, 이는 짧은 기록에서 프롬프트 토큰으로 인간의 음성을 복제할 수 있다. 제로샷 VC의 경우 LM-VC[14]는 이 작업에 과제 중심 최적화를 사용한다. 그리고 일부 연구[13, 12]는 다중 태스크 목표와 데이터 세트를 레버리지하여 고품질 전환을 달성합니다. 이러한 진행에도 불구하고 기존 LM 기반 VC 모델은 일반적으로 오프라인 처리를 적용하여 소스 스피치에서 완전한 발화를 요구하여 실시간 스트리밍 애플리케이션에 대한 적합성을 방해한다. 선행 연구와 달리 스트리밍 시나리오에 대한 LM 기반 VC의 제로 샷 능력을 탐구한다. 컨텍스트 인식의 향상으로 제안된 LM 기반 VC 모델은 비스트림 LM 기반 VC에 필적하는 결과를 달성한다.\n' +
      '\n' +
      '## 3 StreamVoice\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '그림과 같이. 2, 스트림 보이스의 발달은 인식 합성 틀을 따른다. 이 프레임워크에서 스피치는 먼저 의미론적 특징(\\mathbf{s}=\\{s_{1},s_{2},...s_{T_{s}}}\\)으로 표시되고 음향 특징 \\(\\mathbf{a}=\\{a_{a}=\\{a_{a_{a},a_{a}}}, aa_{a}}})은 미리 훈련된 스트리밍 ASR 모델과 스피치 코덱 모델에 의해 각각. 여기서, \\(T_{s}\\) 및 \\(T_{a}\\)는 서열 길이를 나타낸다. 스트림보이스에 입력하기 전에 \\(\\mathbf{s}\\) 및 \\(\\mathbf{a}\\)는 동일한 길이 \\(T\\)에 정렬된다. 스트리밍보이스는 컨텍스트 인식 언어 모델과 단일 언어 모델링 과정을 수행하기 위한 음향 예측기를 통합한다. 시맨틱 및 음향 특징(\\{\\mathbf{\\widehat{s}},\\mathbf{\\widehat{a}}\\}\\}}\\)이 대상 화자의 음성의 의미 정보(\\mathbf{a}_{1:t}\\)를 스피커 프롬프트로 변환함으로써 LM은 숨겨진 출력 \\({}^{c}\\mathbf}\\)을 자동 예측하기 위해 소스 스피치의 의미 정보(\\mathbf{\\)를 자동 예측한다. LM의 각 자동 회귀 시간 단계에서 음향 예측기는 은닉된 출력 \\({}^{c}\\mathbf{h}\\)를 변환된 연설의 코덱 특징 \\(\\mathbf{\\ 예상된다hat{a}\\)으로 변환한다. 마지막으로 코덱 모델의 디코더는 예측된 코덱 특징으로부터 파형을 재구성한다. 다음 섹션에서는 VC를 위해 스트리밍 가능한 LM을 구축하는 방법과 이 스트리밍 VC의 고품질 대화를 보장하는 방법을 소개한다.\n' +
      '\n' +
      '### Streamable Architecture\n' +
      '\n' +
      '스트리밍 음성 변환을 수행하기 위해서는 스트리밍 가능한 아키텍처가 필요하다. 천음성에서 LM은 VC 태스크에서 완전히 인과적인 처리를 수행하기 위해 신중하게 설계되었으며 음향 예측기는 시간적 정보에 의존하지 않고 프레임별 예측을 달성하도록 설계되었다.\n' +
      '\n' +
      '이상적인 언어 모델입니다.\n' +
      '\n' +
      '그림과 같이. LM 기반 VC 모델의 성공으로 영감을 받은 3은 언어 모델에 의해 제로 샷 VC 스트리밍을 달성하고자 한다. 이전 LM 기반 VC 모델[13]에서 완전한 의미론적 특징의 수요는 [13]이다.\n' +
      '\n' +
      '그림 3: 상황 인식 언어 모델에 대한 아키텍처.\n' +
      '\n' +
      '그림 2: 스트림보이스의 전체 아키텍처.\n' +
      '\n' +
      '전환을 달성하기 위한 소스 스피치에서 실시간 응용을 위한 배치를 방해하며, 이는 각 시간 단계에 대해 \\(p_{t}|\\mathbf{s}_{1:T_{x}},\\mathbf{a}_{1:t-1})로 제형화될 수 있다. 스트리밍을 달성하기 위해 LM의 모든 구성 요소는 미래의 정보에 의존할 수 없다. 그림과 같이. 단방향 주의를 갖는 디코더 전용 LM 3은 캐주얼 생성의 요구 사항에 쉽게 맞출 수 있다. 전체 의미적 입력, 의미론적 및 음향적 특징(\\{\\mathbf{a}},\\mathbf{a}\\})의 의존성을 제거하기 위해 먼저 동일한 서열 길이 \\(T\\)에 정렬된 다음 LM에 대체 입력되어 \\(\\{s_{1},a_{1},a_{1},s_{2},a_{T},{T},\\_{T},\\)와 같은 교차 접점을 형성한다. 이러한 수정을 통해 LM은 스트리밍 처리, 모델링(p(a_{t}|\\mathbf{s}_{1:t},\\mathbf{a}_{1:t-1})\\를 달성할 수 있다.\n' +
      '\n' +
      '3.2.2의 음향 음향 예측기 3.2.2의 예측기####\n' +
      '\n' +
      '앞선 LM은 본질적으로 인코딩된 콘텐츠와 스피커를 출력 \\({}^{c}\\mathbf{h}\\\\)로 가지고 있기 때문에 음향 예측기는 \\({}^{c}\\mathbf{h}\\)를 음향 코덱 공간으로 변형시키는 것과 무관한 시간적으로 설계될 수 있으며, 이는 예측 변수를 스트리밍 시나리오에서 쉽게 적용할 수 있음을 의미한다. 연설이 연속적이거나 이산적인 형태로 신경 코덱에 의해 음향 특징으로 나타낼 수 있다는 점을 감안할 때, 우리는 각각 연속 투영 및 이산 투영에 의해 수행되는 스트림 보이스에서 두 특징의 통합을 조사한다.\n' +
      '\n' +
      '** 연속 프로젝트온:** 냉각 선 등[20]] 코덱 모델에 의해 암호화된 \\(D\\)-차원 양자화된 잠재 벡터 \\(\\mathbf{a}\\in\\mathcal{R}^{T\\cer D}\\)를 연속 음향 표현으로 사용한다. 연속적 표현의 예측은 그림 4와 같이 예측 음향 피처 \\(\\hat{\\mathbf{a}}\\)와 지재진 음향 피처 \\(\\mathbf{a}\\) 사이의 L2 거리로 계산되며, 이는 그림 4와 같이 선형 레이어의 스택을 사용하는 것으로 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{Cont}=||\\mathbf{a}-\\hat{\\mathbf{a}}||_{2}^{2}. \\tag{1}\\]\n' +
      '\n' +
      '** 토탈 프로젝트온:** 일반적으로 코덱은 낮은 비트레이트에서 원색을 \\(L\\)-층 이산 지수 \\(B\\mathbf{a}\\in\\mathcal{R}^{T\\RA L}\\)로 압축하기 위해 다층 양자화기로 설계된다. 대부분의 LM 기반 작업[13, 14]은 개별 특징을 예측하기 위해 다중 LM을 스택하여 파이프라인이 스트리밍 시나리오에 복잡하고 적합하지 않다. 대조적으로, 스트림보이스는 MQTTS[3]에서 영감을 받은 유선화된 다층 코덱 예측 방법을 채택한다. 시간적 의존성에서 벗어나 이 방법은 언어 모델의 스트리밍 과정에 원활하게 통합될 수 있다. 구체적으로, 단층 변압기를 사용하여 코덱의 이형성 조건부 분포를 모델링한다. 그림의 권리에 묘사된 바와 같이. 4, 시간 \\(t\\)에서 변압기는 \\({}^{c}\\mathbf{h}\\)를 시작 조건으로 사용하고 층 1에서 L까지 순차적으로 \\(a_{t}^{l}\\)를 생성한다. 놀랍게도, 이 생성 과정은 선행 또는 미래 \\({}^{c}\\mathbf{h}\\)와 무관하여 스트리밍 시나리오의 요구에 잘 부합한다. 특히, 제안된 스트림 보이스에서 우리는 주로 음향 예측을 달성하기 위해 이산 투영을 통합한다. 이산 투영 손실은 다음과 같이 설명할 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{Disc}=-\\log\\prod_{t=0}^{T-1}p(a_{t}|\\mathbf{a}_{1:t-1}_{{m} \\{m} \\{m}_{1:t}_{1:t}.\n' +
      '\n' +
      '### Context-aware Enhancement\n' +
      '\n' +
      '스트리밍 프레임워크에서의 인과성의 단점으로 인해 스트리밍 모델은 비스트림 모델에 비해 미래의 수신이 누락되고 잠재적인 성능 저하에 직면하고, 1절에서 언급한 바와 같이 스트리밍 ASR에서 낮은 품질의 의미 입력은 고품질 전환을 위한 스트리밍 VC를 더 어렵게 만든다. 제안된 스트리머블 LM 기반 VC 모델이 고품질로 전환을 수행하기 위해 상황 인식 강화 방법이 제안되어 의미 입력에서 발생하는 불완전한 상황 정보와 미래 정보의 부재를 완화할 수 있다. 구체적으로, 주어진 의미적 입력으로부터 _h 역사적_ 맥락의 포획을 향상시키기 위해 LM에서 컨텍스트-마스크 자기회귀 예측을 소개하고, 그 역사적 맥락에 기초한 _future_ 맥락을 상상할 수 있도록 교사 유도 맥락 예상이 제안된다.\n' +
      '\n' +
      '3.3.1.1맥스마스크 오토뮤추얼 회귀 예측#####.\n' +
      '\n' +
      '그림의 왼쪽에 나타난 바와 같이. 3, LM은 단방향 주의로 다층 LLaMA에 의해 달성된다. 주어진 의미적 입력으로부터 맥락적 인식을 향상시키기 위해 시맨틱 마스킹이 LM에 도입되어 파괴된 의미로부터 음향 예측을 장려한다. 구체적으로 의미론적 토큰 \\(\\mathbf{s}=\\{s_{1},s_{2},...s_{T}\\}\\)의 서열 내에서 우리는 F\\(r\\) 비율로 시작 지표로 여러 지수를 무작위로 선택하고 \\(l\\) 단계의 종들이 _[M]_에 의해 가려진다. 마스크링 후 LM은 파괴된 의미 특징 \\({}^{m}\\mathbf{s}\\)을 입력으로 하여 자기 회귀를 수행한다. 이러한 방식으로, 정보 병목 또한 스피커 정보를 줄이기 위해 의미적 특징에서 암묵적으로 생성된다. 또한 훈련 중에 화자 프롬프트로 음성 클립을 명시적으로 사용하지 않습니다. 대신 LM은 이전 서열 \\(\\{\\mathbf{s}_{1:t-1},\\mathbf{a}_{1:t-1}_{t-1},s_{t}\\}\\)를 추가 음향 예측을 위한 숨겨진 표현 \\(h_{t}\\)을 자동적으로 생성하는 프롬프트로 표시한다. 특히, 트레이닝 동안, 현재 단계의 입력이 \\(a_{t}\\)인 경우, 해당 출력은 스킵되고 추가 단계를 포함하지 않는다.\n' +
      '\n' +
      '3.3.2.2 교사 유도 콘텍스트 패럴라이트#####.\n' +
      '\n' +
      '앞서 살펴본 바와 같이, 상황 정보의 손실을 초래하는 미래 정보의 부재는 전환 수행의 감소로 이어진다. 자기회귀 예측 코딩[19] (APC)이 나타내는 효과적인 표현 학습에 의해 영감을 받아 그림 3의 권리에 제시된 바와 같이 자기회귀 산출량을 향상시키기 위해 비스트림 ASR에 의해 유도되는 교사 유도 컨텍스트 포워드를 소개한다.\n' +
      '\n' +
      '그림 4: 음향 예측기를 위한 아키텍처. 저희 시스템은 연속 또는 이산 코덱 프로젝션을 지원할 수 있습니다.\n' +
      '\n' +
      '모델은 구상된 미래 정보를 포함하는 컨텍스트 벡터를 학습하도록 수정한다. 구체적으로, 컨텍스트 표현 \\(\\mathbf{c}\\)은 먼저 역사적 맥락을 통해 LM에 의해 생성되는 숨겨진 특징 \\(\\mathbf{h}\\)로부터 선형 예측에 의해 도출된다. 그 후, 이 \\(c_{t}\\)는 미래의 시간 단계(\\overline{s}_{t+1}, a.\\overline{s}_{t+k}\\)로부터 \\(k\\) 의미 특징뿐만 아니라 현재 의미적 특징 \\(\\overline{s_{t+k}}\\)과 함께 L2 거리를 최소화함으로써 보다 일반적인 미래 상황 정보를 발견하도록 장려된다. 이러한 이중 최소화 접근법은 정확한 내용 전달에 기여하고 향후 맥락을 예측할 수 있는 모델의 능력을 향상시킨다. 손실은 손실로 요약할 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{TF}=\\frac{1}{T-k}{T-{1}^{T-k}^{T-k}}^{c_{t}-Concat(\\overline{s}_ {t}_{t+k}),\\overline{s}_{t+1}.\n' +
      '\n' +
      '여기서 \\(Concat(\\cdot)\\는 치수 축삭을 따라 특징의 연결된 것을 나타낸다. 자기회귀 모형의 투입과 산출물 사이에 작동하는 원본 APC와 달리 우리의 접근법은 이러한 예견 과정을 안내하기 위한 의미 정보 \\(차상위{s}\\)를 제공하기 위해 비스트림 ASR 모델을 교사로서 사용한다. 이는 스트리밍 ASR에서 고품질 의미 특징을 얻는 고유한 도전을 해결하기 위해 수행된다. 차원 변환 후, 컨텍스트 표현 \\(\\mathbf{c}\\)를 \\(\\mathbf{h}\\)와 결합하여 컨텍스트 강화 \\(\\mathbf{c}\\)를 형성한 다음 음향 예측기에 공급된다.\n' +
      '\n' +
      '나아가 ASR의 의미적 특징 \\(\\{\\\\mathbf{s},\\overline{s}\\}\\)은 여전히 화자 관련 정보를 포함할 수 있다. 음성 디커플링을 더 보장하기 위해 선형 레이어로 측정 크기를 줄임으로써 스피커 정보를 짜내는 병목 조절기[11]를 \\(\\mathbf{s}\\) 및 \\(\\mathbf{c}\\)에 적용한다.\n' +
      '\n' +
      '소송.\n' +
      '\n' +
      '#### 3.4.1 Training\n' +
      '\n' +
      '천소리 훈련 시 상황 강화 언어 모델과 음향 예측기를 함께 학습한다. 이 두 부분의 자기회귀 과정은 모두 교사-포밍 전략으로 수행된다. 총 손실은 이산 버전에 대해 \\(\\mathcal{L}_{total}=\\mathcal{L}=\\mathcal{L}_{TF}+\\mathcal{L}_{total}=\\mathcal{L}_{total}=\\mathcal{L}_{TF}+\\mathcal{L}_{total}) 또는 \\(\\mathcal{L}_{total}_{TF}+\\mathcal{L}_{total}_{total}_{total}) 또는 \\(\\mathcal{L}_{total}_{total}_{total}_{total}_{TF}_{TF}+\\mathcal{L}+\\mathcal{L}+\\mathcal{L}+\\mathcal{L}_{total}_{total}_{TF}+\\mathcal{L}+\\mathcal{L}+\\mathcal{L}+\\mathcal{L}_{\n' +
      '\n' +
      '3.4.2(3.4.2)는 해결 방법을 흘려 넣는다.\n' +
      '\n' +
      '추론에 있어서, 우리는 목표 화자의 짧은 스피치 클립으로부터의 의미론적 및 음향적 특징을 화자 프롬프트로 사용한다. 클립 끝에 미완료 발음을 포함할 수 있는 이 클립을 무작위로 선택하므로 변환 과정 전에 화자 녹취 후 침묵 클립을 패킹하여 예상치 못한 지속을 방지합니다. 이 빠른 속도로 스트림보이스는 소스 스피치를 스트리밍할 수 있습니다. 이산 투영에서 우리는 욕심 디코딩을 사용하여 확률이 가장 높은 코덱 토큰을 선택한다. 또한 스트림 보이스의 실시간 스트리밍 추론을 보장하기 위해 우리는 중복 계산을 줄이기 위해 LM에서 일반적으로 사용되는 키-값 캐시를 사용한다. 우리 파이프라인에서 소스 스피치의 시작과 끝은 ASR 또는 음성 활동 검출(VAD)에 의해 결정될 수 있기 때문에 입력을 처리하기 위해 윈도우 주의 또는 슬라이드 주의와 같은 기술을 사용하지 않는다. 이러한 기술이 우리 프레임워크에 쉽게 통합되어 향후 확장에 유연성을 제공할 수 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 먼저 작업에 실험 설정을 제시한다. 다음으로, 우리는 주관적인 객관적인 평가와 스트림보이스에 대해 수행된 절제 연구를 제공한다. 스트리밍 파이프라인의 의존성에 대한 자세한 분석도 제공된다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '#### 4.1 Corpus\n' +
      '\n' +
      'Aishell3[14] 1,500시간을 포함하는 혼합 데이터 세트와 중국 내부 데이터 세트를 사용하여 스트림 보이스 및 오디다드[20]를 훈련시킨다. 시맨틱 기능을 추출하기 위해 우리는 스트리밍 ASR Fast-U2++[13]을 통합하여 WeNet에 의해 구현되고 WenetSpeech[13]에 대해 훈련했다. 제로샷 테스트를 위해 DIDISpeech[12] 및 EMIME[21]에서 각각 소스 및 타겟 스피커 발화를 갖는 400개의 테스트 쌍 세트를 선택한다. 볼드 스피커에 대한 평가를 위해 Aishell3의 8개의 스피커를 선택하여 160개의 변환 쌍을 형성한다.\n' +
      '\n' +
      '그림 4.2.1.2.1의 구현설명서#######\n' +
      '\n' +
      '1024개의 코드북 크기와 64개의 코드북 치수가 있는 4개의 양자화층을 갖는 오디다텍의 오픈-소싱 코드1을 사용하여 20ms 프레임 길이의 24kHz 파형을 나타낸다. Fast-U2++는 80ms 청크 크기를 사용하여 스트리밍 추론을 수행하고 16kHz 파형을 40ms 프레임 길이를 갖는 의미적 특징으로 압축한다. 천음성의 경우, 우리는 문맥 강화 LM을 위한 LLaMA[21] 아키텍처를 사용하고 6개 층과 8개의 헤드를 사용한다. 숨겨진 크기 및 중간 크기는 1024 및 4096입니다. 우리는 공식 공개된 코드2를 사용하여 숨겨진 크기 256, 피드포워드 히든 크기 1024 및 4 헤드가 있는 레이어 트랜스포머 디코더를 사용하는 음향 예측기를 구현한다. 시맨틱 마스킹에서 마스크 비율 \\(r\\)은 \\(0.01\\)에서 \\(0.02\\) 범위이고, 스패트(l\\)는 10으로 설정되며, 예열 단계 \\(k\\)는 4로 설정되며 병목 조절기는 특징 차원을 6배 압축한다. 훈련하는 동안 최대 훈련 길이는 12s로 설정됩니다. 스트리밍보이스는 700k 단계에 대해 GPU당 7의 배치 크기로 8 V100 GPU를 사용하여 훈련된다. 우리는 \\(5\\t 10^{-4}\\)의 학습률로 AdamW 최적화기를 사용한다. 잠재적 붕괴는 0.986의 붕괴 비율을 사용하여 각 암각화 후 학습 속도를 업데이트한다.\n' +
      '\n' +
      '부시 1: [https://github.com/facebookresearch/AudioDec] (https://github.com/facebookresearch/AudioDec)\n' +
      '\n' +
      '4.2.2 평가 메트릭 메트릭 4.2.2.2 평가 메트릭######\n' +
      '\n' +
      '평균 의견 점수(MOS)는 95\\(\\%\\) 신뢰 구간으로 계산된 음성 자연성(NMOS)과 화자 유사성(SMOS)을 주관적으로 측정한다. 우리는 15명의 청취자 그룹을 포함하는 주관적 평가를 위해 120개의 테스트 쌍을 무작위로 선택한다. 객관적 평가를 위해 개방형 소스 구현3을 갖는 신경망 기반 시스템을 사용하여 음성 품질(WV-MOS)을 측정한다. ASR 모델4에 의해 측정된 특성 오차율(CER)은 음성 이해성을 나타낸다. 스피커 유사도(SSIM)는 변환된 스피치가 타겟 화자와 일치하는지 결정하기 위해 SV 모델[13]에 의해 계산된다.\n' +
      '\n' +
      '폐경 2: [https://github.com/b04901014/MQTS] (https://github.com/b0490901014/MQTS)\n' +
      '\n' +
      '폐경 3: [https://github.com/그리고reevP/wvmos](https://github.com/그리고reevP/wvmos)\n' +
      '\n' +
      '구획 4: [https://github.com/wenet-e2e/wenet/e2e/tenet/wenet/wenet/tree/tree/HS/샘플/wenetspeech]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '미래 정보만을 사용하는 것은 현재의 언어적 내용을 전달하는 데 방해가 된다.\n' +
      '\n' +
      'i_w/o_ 의미 마스킹에서 의미 마스킹은 스트림 보이스의 훈련 중 의미적 특징에 적용되지 않는다. 우리는 마스킹이 폐기될 때 모든 평가 메트릭에서 성능 감소를 관찰한다. 이는 의미적 마스킹으로 훈련된 스트림 보이스가 스피커 타임브레 포획을 개선하면서 선행 입력으로부터 맥락적 학습을 효과적으로 향상시킨다는 것을 나타낸다.\n' +
      '\n' +
      '우리는 또한 _w/o_ 병목 조절기라고 하는 이 사용을 떨어뜨려 병목 조절기의 효과를 평가했다. 결과는 병목 조절기의 통합이 의미적 특징에 포함된 소스 스피커 정보가 변환된 스피치로 유출되는 것을 방지하는 데 효과적이며 음성 품질에 미치는 영향은 거의 없음을 보여준다.\n' +
      '\n' +
      '타당성 분석\n' +
      '\n' +
      '그림과 같이. 2, 스트리밍 파이프라인에서는 의미 및 음향 정보를 추출하기 위해 추가 ASR 및 코덱이 필요하다. 본 절에서는 ASR과 코덱의 선택과 스트림보이스의 성능 간의 종속성 관계를 탐색해 보고자 한다.\n' +
      '\n' +
      '#### 4.4.1 ASR\n' +
      '\n' +
      'ASR이 스트림보이스에 미치는 영향을 조사하기 위해 비스트림 ASR[4], 널리 사용되는 CTC 기반 스트리밍 ASR[19], 최근에 제안된 스트리밍 Fast-U2++[10]을 포함한 3개의 대표적인 ASR 시스템을 선택하여 의미 추출을 수행한다. 표 5에서 볼 수 있듯이 스트리밍 ASR을 사용하는 ASR의 비스트림 ASR의 의미적 특징을 사용한 스트림 음성이다. 이러한 불일치는 비스트림과 스트리밍 ASR 모델 간의 고유한 성능 갭에 기인하여 상이한 의미 추출 능력을 초래할 수 있다. 또한, [19]의 의미론적 특징을 사용하여 스트림 보이스에서 160ms의 미래 룩-헤드가 합리적인 전환을 달성할 수 없는 반면, 모델링 \\(p_{t}|\\mathbf{a}_{1:t-1},\\mathbf{s}_{1:t+m},t)의 미래 룩-아헤드를 도입하면서 I\\(m\\) 미래 룩-t+m}와 함께 좋은 전환 결과를 얻을 수 있다. 이 문제는 ASR[10] 스트리밍에 존재하는 CTC 스파이크 분포 지연과 토큰 방출 지연으로 인해 발생할 수 있으며, 이는 의미 정보 이동으로 이어질 수 있다. 패스트-U2++ 스트림 보이스의 낮은 배출 지연 시간에서 벗어나는 것은 미래의 룩-헤드 없이 전환을 수행할 수 있다. 패스트-U2++에 사용되는 더 긴 청크 크기로 스트림 보이스는 270ms의 더 큰 대기 시간에 도달하는 동안 더 나은 결과를 얻을 수 있다. 스트리밍 파이프라인 내에서 성능과 속도 사이에 여전히 트레이드오프가 존재한다.\n' +
      '\n' +
      '#### 4.4.2 Codec\n' +
      '\n' +
      '천소리에서 우리는 저밀도 스트리밍 코덱 오디데텍[20]을 사용한다. 표에서 제시한 바와 같이. 6, 우리는 2kbps 및 8kbps를 포함하여 다양한 비트를 가진 코덱을 사용하여 스트림 보이스의 성능을 검증하며, 여기서 더 높은 비트레이트 코덱은 더 낮은 비트레이트들에 대한 우수한 재구성 품질을 달성한다. 2kbps 오디토데크는 양자화의 4개 층을 사용하고 프레임 길이가 20ms인 오디오를 나타내는 반면, 8kbps 오디토데크는 프레임 길이가 10ms인 8개 층을 사용한다. 4.1절에서 언급한 스트림 보이스의 구성을 사용하여 코덱 모델의 다양한 비트에서의 결과는 명백한 차이를 나타내지 않는다. 코덱 예측기에서 변압기 층의 수를 증가시켜 _대형 w/8kbps 오디다텍_을 형성할 때 8kbps 코덱을 사용한 변환 성능은 눈에 띄게 향상되지만 더 느린 추론을 초래한다. 이 결과는 스트림보이스의 설계가 코덱 구성에 의존하여 변환 품질과 추론 속도 모두에 영향을 미친다는 것을 보여준다.\n' +
      '\n' +
      '### Conclusions\n' +
      '\n' +
      '본 논문에서는 스트리밍 시나리오를 위해 설계된 새로운 LM 기반 제로 샷 VC 시스템인 스트림 보이스를 소개한다. 구체적으로, 스트림 보이스는 컨텍스트 인식 LM 및 음향 예측 변수를 포함하는 유선화된 단일 단계 프레임워크를 사용한다. 모델의 입력 및 구조의 캐주얼한 디자인은 스트리밍 행동에 대한 준수를 보장합니다. 스트리밍 시나리오에서 완전한 상황 정보가 누락되어 발생하는 성능 저하를 해결하기 위해 컨텍스트 인식 LM은 모델을 예측하는 교사 유도 컨텍스트를 채택하여 교사가 제공하는 현재 및 미래 정보를 예측할 수 있는 능력을 가지고 있다. 또한, 시맨틱 마스킹은 LM에 도입되어 역사적 입력으로부터 컨텍스트 학습을 향상시키고 더 나은 불협화화를 용이하게 한다. 마지막으로 음향 예측기는 LM과 협력하여 목표 스피치를 생성한다. 실험 결과 스트림보이스는 비스트림 VC 시스템과 유사한 성능을 유지하면서 스트리밍 제로샷 VC를 달성한다는 것을 보여준다.\n' +
      '\n' +
      '** 한계와 미래 작품** 우리는 스트림보이스가 여전히 한계가 있다는 점을 지적해야 한다. 우리의 구성에서 스트림 보이스는 실시간 스트리밍 추론을 달성하기 위해 V100 및 A100과 같은 GPU가 필요하다. 스트리밍 VC의 설계는 4.4절에서 언급한 바와 같이 ASR과 코덱에 크게 의존하며, 스트림 보이스는 또한 도메인 외 문제에 직면하여 악센트에 의한 발화에 대한 성능 저하, 강력한 감정 또는 기록되지 않은 환경에 직면한다. 우리의 미래 작업은 먼저 스트림 보이스의 모델링 능력을 탐색하기 위해 더 많은 훈련 데이터를 사용할 것이다. 또한, 우리는 낮은 비트레이트와 단일화된 스트리밍 모델이 있는 높은 충실도 코덱과 같은 스트리밍 파이프라인을 최적화하는 데 초점을 맞출 것입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Type of Audiodec & WVMOS \\(\\uparrow\\) & CER \\(\\downarrow\\) & SSIM \\(\\uparrow\\) & RTF \\\\ \\hline w/ \\(2kbps\\) Audiodec & 3.63 & 9.43 & 0.740 & 0.42 \\\\ w/ \\(8kbps\\) Audiodec & 3.61 & 9.38 & 0.738 & 0.61 \\\\ Large w/ \\(8kbps\\) Audiodec & 3.68 & 9.12 & 0.751 & 0.90 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 다양한 비트레이트가 있는 오디다텍에 대한 의존성의 분석에서는 표 6이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Type of ASR & WVMOS \\(\\uparrow\\) & CER \\(\\downarrow\\) & SSIM \\(\\uparrow\\) \\\\ \\hline Non-streaming ASR & 3.68 & 8.51 & 0.755 \\\\ \\hline _Streaming ASR ([19])_ & & & \\\\ + 0ms Future Look-ahead & 3.19 & 91.7 & 0.674 \\\\ + 160ms Future Look-ahead & 3.48 & 10.6 & 0.727 \\\\ \\hline _Streaming ASR (Fast-U2++ [10])_ & & & \\\\ Chunk (80ms) & 3.63 & 9.43 & 0.740 \\\\ Chunk (160ms) & 3.69 & 9.16 & 0.744 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'ASR에 대한 의존도 분석은 표 5와 같다. 결과는 서로 다른 ASR을 통합하는 스트림 보이스의 성능을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '*[Ning _et al_2023] Ziqian Ning, Yuepeng Zhu, Pengcheng Zhu, Shuai Wang, Jixun Yao, Lei Xie 및 Mengxiao Bi. 통일 스트리밍 및 비스트림 음성 변환을 위한 __Dualvc 2: Dynamic 마스킹 컨벌루션 _Dualvc 2: Dynamic 마스킹 컨벌루션입니다. Arxiv_, 2023.\n' +
      '*[Qian _et al._2019] Kaizhi Qian, 양장, 시유창, Xuesong 양, 마크 하세가와-Johnson. 오토코더: 오토인코더 손실만으로 제로샷 음성 스타일 전송입니다. 기계학습_국제회의에서는 2019년 5210-5219쪽이다.\n' +
      '*[Ren _et al_2022] 이렌, Xu 탄, 타오 진, 저우 자오, Tie-Yan Liu. 말하기 위해 텍스트로 과신함을 재조명하세요. 컴퓨팅 언어학_조정에서 2022년 8197-8213쪽이다.\n' +
      '*[Shen _et al_2023] Kai 선전, Zeqian 주, Xu 탄, 옌킹 류, Yichong Leng, Lei He, Tao Qin, 선전 Zhao 및 장 비안. 자연 음성 2: 라트 확산 모델은 자연 및 제로 샷 스피치 및 노래 합성기입니다. _ 라트 확산 모델. Arxiv_, 2023.\n' +
      '*[시 _et al._2020] 요오시, 후이부, 신주, 샤오지 장, 명리. Aishell-3: A 다중 스피커 만다린 테트 코퍼스와 바젤입니다. __ Arxiv_ 2020.\n' +
      '*[선 _et al._2016] 라이프아 선 K. 리, 하오왕, 시이인강, 허멍. 병렬 데이터 트레이닝 없이 여러 대 일 음성 변환을 위한 광학 후그램입니다. 멀티미디어와 엑스포_국제회의에서는 2016년 1-6쪽이다.\n' +
      '*[티안 _et al._2020] 샤오하이 톈, 지차오 왕, 샨양, 신용주, 홍창두, 이주, 명양장, 건주, 베락 시잔, 리치, 하이저우 리가 있다. 음성전환 챌린지 2020의 NUS & NWPU 시스템 블리자드 챌린지 및 음성전환 챌린지 2020_ 페이지 170-174, 2020의 _Joint 워크숍입니다.\n' +
      '* [Touvron _et al_2023] Hugo Touvron, Tmonaryaut Lavril, Gautier Izacard, Xavier 마르티네, Marie-Anne Lachaux, CASote Lacroix, Baptiste Lacroere, 남안 고열, 에릭 함크로, Faisal Azhar, Faisal Azhar: Open 및 효율적인 기반 언어 모델. Luga: Luga: Lasal 아자르: Lasal 아자르: Lasal 아자르: Lasal 아자르: Lasal 아자르: Ljal 아자르, Eaal 아자르, Faal 아자르, Faal 아자르, Faal 아자르:오픈 및 효율적인 기반 언어 모델. Arxiv_, 2023.\n' +
      '*[왕 _et al._2021] 디동 왕, 리쿤 덩, 유투잉 예웅, 샤오 첸, 쉬니잉 류, 헬렌 멍 등이 있다. Vqmivc: 벡터 양자화 및 상호 정보 기반 비지도 음성 표현은 1샷 음성 변환을 위해 이입한다. _국제 스피치 통신 협회의 경우 2021년 1344-1348쪽이다.\n' +
      '*[왕 _et al._2023a] 보한 왕, 다미엔 Ronssin, M 철학 Cernak. Alo-vc: Any-to-any의 낮은 등급의 원샷 음성 변환입니다. _국제 스피치 통신 협회_에서 2073-2077 페이지는 2023년이다.\n' +
      '*[왕_et al_2023b] Chengyi Wang, Sanyuan Chen, 유우, Ziqiang Zhang, 롱 저우, Shujie Liu, Zhu Chen, 옌칭 Liu, Huqing Liu, Huqing Liu, 김진유, et al. Arxiv_, 2023.\n' +
      '*[왕 _et al._2023c] Zhichao 왕, 원즈허 첸, Lei Xie, Qiao 톈, Yuping 왕. 언어 모델을 기반으로 하는 음성 생성을 통한 제로 샷 음성 변환 __Lm-vc: 제로 샷 음성 변환은 언어 모델을 기반으로 한다. IEEE 신호 처리 편지_, 페이지 1157-1161, 2023.\n' +
      '*[왕 _et al._2023d] Zhichao 왕, Liumeng Xue, Qiuqing콩, Lei Xie, Yuanzhe Chen, Qiao 톈, Yuping 왕. 강력한 제로 샷 음성 변환을 위한 다중 레벨 시간 채널 스피커 검색 __는 강력한 제로 샷 음성 변환을 위한 다단계 시간 채널 스피커 검색이다. Arxiv_, 2023.\n' +
      '*[위스터2010] 미라잠 위스터. EMIIME 이중언어 데이터베이스입니다. 기술 보고서인 에든버러 대학인 2010.\n' +
      '*[Wu _et al._2023] 이샤오 우, 이스라엘 D Gebru, 데얀 마르코비치, 알렉산더 리처드. Audiodec: An 오픈 소스 스트리밍 고 충실도 신경 오디오 코덱입니다. I_국제 음향 컨퍼런스에서 스피치 및 신호 처리_ 페이지 1-5, 2023.\n' +
      '*[양 _et al._2022] 해오간 양, 리쿤 덩, 유투닝 예웅, 니안즈 정, 용추. 꿈을 수 있는 스피치 표현은 살아있는 원샷 음성 변환을 위한 무원요소 및 다단계 프로디디 모델링이다. _국제 스피치 통신 협회_에서는 2022년 2578-2582페이지가 있다.\n' +
      '*[양 _et al._2023] 동차오 양, 진추안 톈, Xu 탄, Rongjie Huang, 송시앙 류, Xuankai 창, 지퉁 시, 선전 자오, 장비안, Xixin 우 등 유니아우다. Arxiv_, 2023.\n' +
      '*[Yi _et al._2022] 장옌이, 지안화타오, 시안화타오, 샤이니, 해독마, 해독마, 다왕, 타오왕, 정쿤천, 여바이, 쿠한판, 산앙, 시밍왕, 시링왕, 샤이장, 신루이옌, 르Xu, 정키위, 하이저우리 등이 있다. 2022: 첫 번째 오디오 심층 합성 검출 도전이 추가됩니다. I_국제 음향 컨퍼런스에서 스피치 및 신호 처리_. IEEE, 2022.\n' +
      '*[Yin _et al._2021] 다정 진, 잔치 르, 총 루노, 유왕 왕, 지히웨이 시온그, 원준생. 후원: 토큰 수준의 이분 그래프로서의 학습 내용식 표현이다. 2021년 _국제 학습 발표회의\'에서.\n' +
      '*[Zeghidour _et al._2021] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, 얀스케이덕트 및 Marco Tagliasacchi. SoundStream: An 엔드 투 엔드 뉴럴 오디오 코덱. __SoundStream: An 엔드-엔드 뉴럴 오디오 코덱. 2021년 오디오, 스피치 및 언어 처리_, 30:495-507에 대한 전환이다.\n' +
      '*[Zhang _et al_2022] 빈빈 장, 항 Lv, 펑청 구오, Qijie Sho, Cheie Shao, Chi Xie, 신장 Xie, Hui 부, 샤오유 첸, Chenchen Zeng, et al. Wenetspeech: 음성 인식을 위해 A 10000+시간 다중 도메인 만다린 말뭉치: A 10000+ 다중 도메인 만다린 코퍼스. I_국제 음향 컨퍼런스에서 스피치 및 신호 처리_, 2022년 페이지 6182-6186.\n' +
      '*[Zhu _et al._2023] Xinfa Zhu, Yuan준이 Lv, 이리, Tao Li, Wendi He, 홍빈 저우, 허루, Lei Xie. Vec-tok 음성: 뉴런 음성 생성을 위한 음성 벡터화 및 토큰화이다. 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>