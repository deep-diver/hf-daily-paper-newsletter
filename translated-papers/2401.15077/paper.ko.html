<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '>깜깜이.\n' +
      '\n' +
      ' 유의 리\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      'Fangyun Wei\\({}^{\\ddagger}\\)\n' +
      '\n' +
      'Chao Zhang\\({}^{\\spadesuit}\\)\n' +
      '\n' +
      'Hongyang Zhang\\({}^{\\spadesuit}\\)\\({}^{\\dagger}\\)\n' +
      '\n' +
      '\\({}^{\\spadesuit}\\)Peking University\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      '수경대학의 워터루대학입니다.\n' +
      '\n' +
      'Vector Institute\n' +
      '\n' +
      'hongyang.zhang@uwaterloo.ca\n' +
      '\n' +
      '[https://github.com/SafeAILab/EAGLE](https://github.com/SafeAILab/EAGLE)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '자동 억제 디코딩은 대형 언어 모델(LLM)의 추론을 시간 소모로 만든다. 우리는 무손실 가속도를 위해 단순한 프레임워크인 EAGLE(Greater 언어 모델 효율에 대한 추출 알고리즘)를 제안한다. EAGLE는 전통적인 투기 표본 추출 방법과 달리 보다 규칙적인(2층) 특징 수준에서 자동 조절적으로 초안 작성 프로세스를 운영하고 한 단계 앞서 토큰을 통합하여 차세대 예측 문제에서 샘플링 불확실성 문제를 다룬다. EAGLE에 의해 제공되는 가속도는 손실 없이, 대상 LLM의 미세 조정은 수반되지 않으며, 생성된 텍스트는 바닐라 자동 억제 디코딩과 동일한 분포를 유지한다. 본 논문의 제출과 같이 EAGLE는 투기적 샘플링 계열 내에서 알려져 있는 _패스트_알려진 프레임워크이다. MT-벤치에서 EAGLE는 바닐라 디코딩보다 3배 빠르고, 루카헤드보다 2배 빠르고, 메두사보다 1.6배 빠르다. 헵-패션을 사용하여 EAGLE는 휴깅페이스 구현의 24개 토큰/s와 비교하여 단일 RTX 3090 GPU에서 LLaMA2-Chat 13B가 있는 평균 160개의 토큰/s에 속한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '자율 억제 디코딩은 대규모 언어 모델(LLM)에 대한 사실상의 기준이 되었다. 이 프로세스는 한 번에 하나의 출력 토큰을 생성하여 LLM에 의한 생성을 비용이 많이 들고 느리게 만든다. 누적 샘플링(Leviathan et al., 2023; Chen et al., 2023) 기반 방법은 이 도전에 대한 해결책을 제공한다. 그들은 LLM의 생성 과정을 낮은 비용으로 잠재적 토큰이 추측되는 초안 단계와 검증 단계의 두 단계로 나누고, 여기서 이러한 토큰은 LLM의 단일 정방향 패스를 통해 _in 평행_로 검증된다. 사행 샘플링의 병렬화는 LLM 전방 패스당 다중 포스트 체크 토큰의 생성을 용이하게 하여 속도를 크게 향상시킨다. 더 중요한 것은 투기적 샘플링에서의 검증 단계는 텍스트 분포가 원래 LLM의 디코딩 결과와 정확하게 정렬되어 생성된 콘텐츠의 무결성을 유지하는 것을 보장한다.\n' +
      '\n' +
      '투기 샘플링을 적용하는 핵심은 원래 LLM과 충분히 유사한 초안 모델을 식별하지만 잠복기가 더 낮다는 것이다. 누적 샘플링은 보통 초안 모델과 동일한 시리즈에서 LLM의 하위 매개변수 버전을 사용한다. 예를 들어, 7B, 13B, 70B 파라미터를 갖는 모델을 포함하는 LLAMA2(Touvron et al., 2023) 시리즈에서는 70B 모델의 프록시로서 7B 모델을 사용하는 것이 유효하며, 가장 작은 7B 변이체에 대한 적합한 초안 모델을 찾는 것이 까다롭다. 대안은 타이니LLaMA(장 et al., 2024)를 사용하는 것일 수 있지만, 이는 LLAMA2-Chat와 타이니LLaMA-Chat 사이의 명령어 템플릿의 불일치로 인해 지시된 모델에 대해 실현 가능하지 않다. 13B 및 70B 모델의 경우 동일한 시리즈 내의 7B 모델이 초안 모델 역할을 할 수 있다. 그러나 7B 모형의 높은 오버헤드로 인해 투기적 샘플링의 가속 효과는 최적이 아니다. 사행 샘플링을 위해 특별히 새로운 적절한 크기의 초안 모델을 훈련하는 것은 높은 비용으로 인해 이상적인 솔루션이 아니며, 타이니LLaMA는 3,000B 토큰에서 훈련되는 반면 EAGLE는 2-4B 토큰에서 훈련된다.\n' +
      '\n' +
      '투기 샘플링의 가속도를 향상시키는 핵심은 원래 LLM(Chen et al., 2023; 샤, 2023; 산틸리 등 2023)에 의한 초안 수용률을 줄이고 시간 오버헤드를 향상시키는 데 있다. 많은 접근법이 초안 단계의 오버헤드를 줄이는 데 중점을 둔다. 발라헤드(Fu et al., 2023)는 n그램과 제이코비 반복을 사용하고, 메두사(Cai et al., 2023)는 원래 LLM의 2층 특징을 기반으로 토큰을 예측하는 MLP 세트를 사용한다. 이러한 전략은 초안을 생성하는 데 있어 지연 시간을 크게 감소시켜 가속도를 향상시킵니다. 그러나 이러한 방법의 궁극적인 잠재력은 그들이 생산하는 초안들의 정확도가 낮기 때문에 캡핑된다.\n' +
      '\n' +
      '특징 수준에서 자동 회귀가 토큰 수준보다 더 관리 가능하다는 관찰을 기반으로 직접 토큰 예측에서 분기되고 대신 특징 수준에서 자동 억제 동작을 수행하는 단순한 프레임워크인 EAGLE(Greater 언어-모델 효율에 대한 추출 알고리즘)를 소개한다. 하나의 시간 단계에 의해 진전된 토큰 시퀀스를 통합함으로써 EAGLE는 특징 수준의 자동 회귀와 관련된 불확실성을 우회한다. 변형기 디코더 레이어만을 포함하는 초안 모델을 사용하여 EAGLE는 메두사의 0.6을 실질적으로 능가하는 약 0.8의 초안 정확도를 달성한다.\n' +
      '\n' +
      '우리는 ChatGPT와의 대화 와 관련된 멀티턴 지시와 같은 실제 애플리케이션 및 실제 시나리오를 시뮬레이션하는 매우 현실적인 벤치마크인 MT-벤치(정 등 2023)에서 EAGLE를 평가했다. 우리는 스피드업 비율을 입증하기 위해 루카헤드와 메두사를 포함한 현재 최첨단에서 사용되었기 때문에 이 벤치마크를 활용하기로 결정했다. 이 선택은 우리의 접근법과 이러한 벤치마크의 공정하고 직접적인 비교를 용이하게 한다. 욕심 디코딩 설정 하에서 비쿠나-13B 및 LLAMA2-Chat 13B, 70B에 대해 EAGLE는 원래 LLM의 텍스트 분포를 유지하기 위해 이론적으로 보장되고 오른쪽 아웃박스를 사용할 준비가 된 **3x** 가속도를 제공한다. 최근 제안된 투기 샘플링 기반 프레임워크인 루카헤드와 메두사에 비해 EAGLE는 각각 **2x**** 및 **1.6x** 속도를 달성한다. 속도 향상 동안 EAGLE는 또한 **2x**만큼 LLM 시스템의 처리량을 증가시킨다. EAGLE는 양자화, 편찬 등과 같은 다른 가속도 또는 처리량 개선 방법과 병행하여 동작한다. EAGLE와 이러한 기술을 결합하여 LLM 시스템의 운영 비용을 더욱 줄일 수 있다. 예를 들어, gpt-fast1을 사용하여 EAGLE는 단일 RTX 3090 GPU에서 24.5 토큰/s에서 160.4 토큰/s로 LLAMA2-Chat 7B 디코딩을 가속화한다.\n' +
      '\n' +
      '폐경 1: [https://github.com/pytorch-labs/gpt-fast](https://github.com/pytorch-labs/gpt-fast)\n' +
      '\n' +
      'EAGLE는 낮은 훈련 비용을 자랑합니다. LLaMA2-Chat 70B 모델의 경우, EAGLE는 ShareGPT 데이터세트로부터의 70k 이상의 대화들을 사용하여 1B 파라미터 미만의 디코더 층을 트레이닝한다. 4x A100(40G) GPU에서 1-2일 뒤에 훈련이 완료됩니다. 실제 애플리케이션에서 EAGLE는 각 질의에 대한 가속도를 제공하기 위해 단일 훈련 세션만 필요로 한다. 질의 수가 증가함에 따라 EAGLE의 상각화 훈련 비용은 무시할 수 있게 된다.\n' +
      '\n' +
      '기존의 투기적 표본 추출 기반 기법과 비교하여 EAGLE의 장점에는 다음과 같다.\n' +
      '\n' +
      '* ** 단순성:** EAGLE는 자동 억제 디코딩을 가속화하는 새로운 프레임워크이며 모든 자동 억제 LLM에도 적용할 수 있다. 이 방법은 LLM에 경량 플러그인(단일 변압기 디코더층)만을 추가하여 생산 시에 용이하게 배치될 수 있다.\n' +
      '\n' +
      '그림 2: 비홍조(온도=1) 설정에 대한 MT-벤치에 대한 스피드업 비율은 그림 2:였다. 발톱은 욕심 디코딩에 국한되며, 메두사의 무공생 생성은 무손실 성능을 보장하지 않는다. 따라서 EAGLE는 이러한 방법과 비교되지 않는다.\n' +
      '\n' +
      '환경. 이 논문에서 우리는 MT-벤치 및 GSM8K 데이터셋에서 0샷 방식으로 LLaMA2-Chat(7B, 13B, 70B), Vicuna(7B, 13B, 33B) 및 Mistral 8x7B 구성-v0.1에 EAGLE를 적용했다.\n' +
      '신뢰성***신뢰성:** EAGLE는 원래 LLM의 미세 조정과 관련이 없으며 EAGLE에 의한 출력 분포의 보존은 이론적으로 욕심 및 비공진 설정 모두에 대해 보장된다. 이는 가속화가 잘못된 또는 유해한 LLM 출력으로 이어질 수 있는 에지 경우 분해의 위험을 보장하지 않는다. 이는 욕심 설정에만 초점을 맞춘 루카헤드와 메두사와 큰 대조를 이룬다.\n' +
      '* **Speed:** EAGLE는 본 논문의 제출과 같이 투기적 표본 추출의 가족 내에서 _패스트_ 프레임워크로 두드러진다. 특히, 욕심 추론을 위한 비쿠나와 LLaMA2-Chat(그림 1 참조)에서 EAGLE는 바닐라 디코딩 속도를 3배 초과하고, 외장 루카헤드 디코딩은 2배 이상, 메두사 디코딩보다 1.6배 빠르다. 헵-패션을 사용하여 EAGLE는 휴깅페이스 구현의 24개 토큰/s와 비교하여 단일 RTX 3090 GPU에서 LLaMA2-Chat 13B가 있는 평균 160개의 토큰/s에 속한다. MoE(실험자의 중간) 모델, 특히 Mistral 8x7B 구성-v0.1(지앙 등 2024)에서 EAGLE 알고리즘은 1.5배의 속도 증가를 달성한다.\n' +
      '\n' +
      'EAGLE의 단순하면서도 효율적인 프레임워크를 도입하는 것 외에도 그 효과에 기여하는 요인에 대한 분석도 제공되며, 이는 다른 투기적 샘플링 방법(섹션 5.3.2)에 독립적인 관심을 가질 수 있다. EAGLE는 다음 두 가지 관찰을 통해 구축된다.\n' +
      '\n' +
      '* 먼저, 상부 레이어 기능을 사용하면 동일한 경량 네트워크를 사용할 때 바닥 토큰 임베딩을 사용하는 것보다 더 효과적인 것으로 입증됩니다.\n' +
      '* 둘째, 샘플링 과정에 내재된 불확실성은 상부층 특징만을 입력하는 모델 초안의 성능을 크게 제약한다. 따라서 샘플링 결과(토큰)를 초안 모델에 통합하는 것이 중요하다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '** 통지*** 본 논문에서는 가속화를 위한 대어 모형을 "대상 LLM"으로, 신속 초안에 사용된 모형을 "드레인 모델"으로 지칭한다. 달리 명시되지 않는 한, "극성"은 LLM의 2층 특징, 즉 LM 헤드 이전에 숨겨진 상태를 의미한다. 우리는 하위 글자 \\(t\\)에 의한 토큰, \\(e\\)에 의한 임베딩 및 \\(f\\)에 의한 특징 및 \\(p\\)에 의한 분포를 나타낸다. 우리는 서열을 나타내기 위해 대문자를 사용한다. 예를 들어, \\(T_{i:j}\\)는 서열 \\(t_{i},t_{i+1},\\ldots,t_{j})\\를 나타낸다. LLM을 고려할 때, 입력 토큰 시퀀스 \\(T_{1:j}\\)를 감안할 때, LLM은 임베딩 레이어를 통해 대응하는 임베딩(E_{1:j}\\)을 획득한다. 이들은 다수의 디코더 층을 통해 처리되어 \\(F_{1:j}\\)의 특징을 생성한다. 그런 다음 LM 헤드는 \\(f_{j}\\)를 분포 \\(p_{j+1}=\\text{LM 헤드}(f_{j})\\에 매핑하고, 그로부터 다음 토큰 \\(t_{j+1}\\)이 샘플링된다. 예를 들어, 토큰 수준에서 바닐라 자동 회귀는 임의의 정수 \\(j\\geq 1\\)에 대해 프로세스(\\(T_{1:j}\\to E_{1:j}\\to f_{j+1}\\to p_{j+1}\\))로 표현될 수 있다.\n' +
      '\n' +
      '누적 표본 추출*** 종적 샘플링** 누적 샘플링은 초안 및 검증 단계 사이에서 변경된다. 현재 서열 \\(T_{1:j}\\)를 고려한다. 초안 단계에서 투기 샘플링은 목표 LLM보다 작은 모델을 사용하여 자동 조절(\\hat{T}_{j+1:j+\\gamma}\\) 토큰 \\(\\hat{T}_{j+1:j+\\gamma}\\)을 생성하고 해당 분포 \\(\\hat{P}_{j+1:j+\\gamma}\\)를 기록한다. 검증 단계에서 대상 LLM의 단일 정방향 패스는 해당 위치에 대한 확률을 산출한다. 검증 단계 동안, 타겟 LLM의 단일 정방향 패스는 해당 위치에 대한 확률을 제공한다. 그런 다음 토켄은 토큰 \\(\\hat{t}_{j+i}\\)가 수용 확률 \\(1,p_{j+i}/\\hat{p}_{j+i})\\을 갖는 것으로 순차적으로 수용에 대해 평가된다. 토큰 \\(\\hat{t}_{j+i}\\)의 거부 시, 모든 후속 토큰이 폐기되고, 이 토큰은 분포 \\(\\text{norm}(0,p_{j+i}-\\hat{p}_{j+i}))를 기반으로 재결합된다. 투기 샘플링의 부록 A.1(Leviathan et al, 2023)은 앞서 언급한 샘플링 방법이 표적 LLM 분포로부터 직접 샘플링하는 것과 동일하다는 것을 증명한다. EAGLE는 또한 이 샘플링 접근법을 사용하여 ** 생성된 텍스트의 분포가 탐욕 및 비감응 설정*** 모두에 대해 변하지 않도록 한다.\n' +
      '\n' +
      '부츠 2: 우리는 모자를 자주 사용하여 모델 초안에 대응하는 용어를 나타낼 것이다.\n' +
      '\n' +
      '## 3 Observations\n' +
      '\n' +
      'EAGLE의 설계에 직접적으로 동기를 부여하는 두 가지 관찰에서 시작한다.\n' +
      '\n' +
      '토큰보다 자동 조절 기능이 더 쉽습니다.\n' +
      '\n' +
      '토큰 서열은 자연 언어의 간단한 변형을 나타내며 본질적으로 복잡하다. 대조적으로, LLM의 2층 특징은 선형 변환(LM 헤드)을 거친 후 다음 토큰을 예측할 수 있어 특징 시퀀스를 보다 구조화되고 규칙적으로 만든다. 결과적으로, 특징 수준에서 자동 회귀를 수행하는 것은 토큰 수준보다 더 관리할 수 있을 것으로 예상된다. 우리는 시간이 지남에 따라 높은 수준의 특징의 진화 경로가 토큰보다 간단하기 때문에 전자가 더 작은 모델로 특징지어질 수 있기 때문이라고 가정한다. 이 가설을 검증하기 위해 특징 및 토큰 자동 회귀 모두에 대해 초안 모델을 1층 변압기 디코더로 설정하여 Vicuna 7B(Chiang et al., 2023)를 사용한 MTbench에 대한 실험을 설계했다. 그림 3에 도시된 바와 같이, 냉동 LM 헤드를 사용한 토큰 생성((f_{1},f_{2})\\to f_{3}\\to t_{4}\\))은 30%만큼 큰 향상으로 자동 억제 토큰 처리(t_{1},t_{2},t_{3})에 비해 훨씬 더 높은 정확도를 산출한다. 이것은 훨씬 더 높은 속도 비율(1.5x\\(알파우라로우\\) 1.9x)을 초래한다.\n' +
      '\n' +
      '차기 예측에서 불확실성이 있습니다.\n' +
      '\n' +
      '텍스트 생성 시 목표 LLM은 토큰을 직접 예측하지 않고, 대신 토큰의 확률 분포를 예측한 다음 이 분포를 기반으로 샘플을 예측하여 생성 과정에 무작위성을 도입하게 된다. 토큰과 달리 특징은 고차원적이고 연속적이며 동일한 샘플링 기반 접근법의 사용을 배제한다. 그림 4에 예시된 예를 고려하여 "암"을 샘플링하면 특징 서열이 \\(f_{I},f_{am})가 된다. 반대로, "고속도로"를 샘플링하면 특징 시퀀스는 \\(f_{I},f_{al 고속도로})가 된다. 이것은 특징 서열의 자동 억제 모델에 모호성을 도입하는데, \\(f_{I}\\) 후 다음 특징이 \\(f_{am}\\)여야 하는지 또는 \\(f_{al wel}\\)이어야 하는지 여부는 불확실하다. 메두사 역시 이 문제에 맞서고 있다. 이 예에서 \\(p_{I}\\) 또는 \\(p_{am}\\)를 예측하기 위해 \\(p_{am}\\)를 사용하는 것과 같은 간격만큼 이격된 토큰의 확률을 예측하는 것이 목적이다. 이러한 고유한 불확실성은 메두사의 초안 모델이 표적 LLM을 완벽하게 에뮬레이팅하는 것을 불가능하게 만든다. 이 문제를 해결하기 위해 EAGLE는 샘플링 결과를 포함하는 하나의 시간 단계 이전으로부터 토큰 시퀀스를 초안 모델에 입력한다. 그림 4에 예시된 예에서 이는 \\(f_{I}\\) 및 \\(t_{al 고속도로}\\)를 기반으로 하는 \\(f_{al 고속도로}\\) 예측 및 \\(f_{I}\\) 및 \\(t_{am}\\)를 기반으로 하는\\(f_{am}\\)를 예측하는 것을 포함한다. 그림 5에 도시된 바와 같이 불확실성을 해결함으로써 속도율이 1.9배에서 2.8배까지 더욱 증가한다.\n' +
      '\n' +
      '## 4 Eagle\n' +
      '\n' +
      'EAGLE는 위의 두 가지 관찰에 따라 구축된다. 다른 투기적 샘플링 기반 방법과 일치하도록 초안 작성 단계와 검증 단계를 모두 포함한다.\n' +
      '\n' +
      '### Drafting phase\n' +
      '\n' +
      'EAGLE와 다른 방법의 주요 구별은 주로 초안 작성 단계에 있다. 그림 6은 다른 방법에 대한 초안화 단계를 도식화한 것이다. 누적 표본 추출(Leviathan et al., 2023; Chen et al., 2023)과 루카헤드(Fu et al., 2023)는 토큰을 기반으로 토큰을 예측한다. 메두사(Cai et al., 2023)는 표적 LLM의 피처 \\(f_{2}\\)를 사용하여 독립적으로 \\(t_{4}\\) 및 \\(t_{5}\\)를 예측한다. EAGLE는 특징 서열 \\((f_{1},f_{2})\\) 및 토큰 서열 \\((t_{2},t_{3})\\)를 사용하여 \\(f_{3}\\)를 예측하며, 한 단계 진전된다. i\\(p_{4}=\\) LM 헤드(\\(f_{3}\\)에서\\(t_{4}\\)를 샘플링한다. 이어서, \\(f_{3}\\) 및 \\(t_{4}\\)를 입력 시퀀스로 연결하여 다음 특징(f_{4}\\)을 예측하고 후속 토큰(t_{5}\\)을 샘플한다.\n' +
      '\n' +
      '그림 7에 도시된 바와 같이 EAGLE의 초안 모델은 베딩층, LM 헤드 및 오토 회귀 헤드의 세 가지 모듈을 포함한다. 보딩 층과 LM 헤드는 원래 LLM의 매개변수를 사용하며 추가적인 훈련을 필요로 하지 않는다. 초안 모델은 형상(bs, seq_len, 숨겨진_dim)과 형태(bs, seq_len)의 고급 토큰 시퀀스를 입력함에 따라 이루어진다. 그런 다음 토큰 시퀀스를 형상의 토큰 임베딩 시퀀스(bs, seq_len, 은닉_dim)로 변환하여 연결한다.\n' +
      '\n' +
      '그림 4: 특징 시퀀스의 불확실성. r\\(f_{I}\\)에 따른 다음 특징은 샘플링 결과에 우발하며 \\(f_{I}\\)에만 결정할 수 없으며, 여기서 "고속도로"와 "암"은 모두 토큰 "I"를 따라 두 갈래로 이어질 수 있다.\n' +
      '\n' +
      '그림 5: 특징 및 특징 개발 톤을 기반으로 하는 초안 모델의 출력 및 속도 비율이다. 특징 개발-토큰 접근법은 불확실성을 효과적으로 해결합니다. 모델은 Vicuna 7B를 표적 LLM으로 사용하여 MT-bench에서 테스트되었다.\n' +
      '\n' +
      '그림 3: 토큰 및 특징에 기반한 초안 모델의 액세스 및 속도 비율, Vicuna 7B를 표적 LLM으로 사용하여 MT-벤치에 대해 테스트하였다.\n' +
      '\n' +
      '형태의 융합된 서열(bs, seq_len, 2\\(혈청형)히든_dim)을 형성한다. 상기 오토 회귀 헤드는 FC 층 및 디코더 층으로 구성된다. FC 레이어는 융합된 시퀀스의 치수를 (bs, seq_len, 숨겨진_dim)로 감소시킨 다음 디코더 레이어를 사용하여 다음 특징을 예측한다. LM 헤드는 특징에 기초하여 분포를 계산하고, 그로부터 다음 토큰이 샘플링된다. 마지막으로, 예측된 특징과 샘플링된 토큰을 입력으로 연결하여 자동 억제 과정의 지속을 촉진한다. EAGLE는 트리 구조화된 초안을 생성한다. 효율성을 높이기 위해 나무 관심을 구현하여 \\(m\\) 전방 통과를 통해 \\(m\\) 깊이가 있는 초목 생성을 가능하게 하여 \\(m\\) 토큰 이상을 포괄한다. 그림 7에 예시된 예에서 EAGLE는 3개의 전방 통과를 통해 10개의 토큰의 트리를 배수한다.\n' +
      '\n' +
      '초안 모델의 초안 모델 트레이닝###\n' +
      '\n' +
      '다음 특징을 예측하는 것은 회귀 과제를 구성하는데, 우리는 Smooth L1 손실을 고용한다(그림 6 EAGLE 참조).\n' +
      '\n' +
      'E_{2:i+1}=\\{Token 엠블딩}(T_{2:i+1}),\\] \\[\\hat{f}_{i+1}=\\text{Auto-회귀 헤드}=E_{2:i+1}(E_{2:i+1},F_{i+1}),\\{Token{Token{f}=E_{i+1}=E_{i+1}=E_{i+1})=E_{i+1}=E_{i+1}=E_{i+1}=E_{i+1}=E_{i+1})=E_{i+1}=E_{i+1}(E_{i+1})=E_{i+1}(E_{i+1})=E_{i+1}(E_{i+1},{i+1}－{i+1})=E_{i+1}(E_{i+1},{i+1})\n' +
      '\n' +
      '특징 예측은 초안 모델의 중개 목표이며, 궁극적인 목표는 토큰의 시퀀스를 생성하기 위한 토큰의 예측이다. 결과적으로, 우리는 또한 이 최종 목표에 직접 최적화하기 위해 분류 손실을 사용한다.\n' +
      '\n' +
      '\\{LM 헤드}(f_{i+1})=\\{LM 헤드}(f_{i+1}),\\] \\[\\hat{p}_{i+2}=\\text{p} (\\text{p}_{i+2})=\\text{LM 소프트맥스} (\\text{LM 헤드} (\\text{LM 헤드} (\\{LM 헤드} (\\{LM Headmax})(\\{i+2},\\{p}_{i+2},\\{p}_{i+2},\\{p:{i+2},\\{p:{i+2},\\{p:{i+2}-{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2},\\{i+2\n' +
      '\n' +
      '회귀손실과 분류손실을 통합하여 다음과 같은 결합손실함수를 이용하여 오토회귀부장을 훈련시킨다.\n' +
      '\n' +
      '\\[L=L_{reg}+w_{cls}L_{cls}.\\]\n' +
      '\n' +
      '일반적으로 분류 손실은 수치적 측면에서 회귀 손실보다 크기가 큰 순서이다. 결과적으로, 우리는 \\(w_{cls}\\)를 0.1로 설정했다.\n' +
      '\n' +
      '오토 회귀 헤드를 트레이닝하기 위한 최적의 접근법은 타겟 LLM을 사용하여 텍스트를 자동 조절적으로 생성하는 것을 포함한다. 그러나 이러한 데이터 생성은 비용이 많이 듭니다. 다행히 EAGLE는 훈련 데이터(설치)에 대한 민감도가 낮다.\n' +
      '\n' +
      '그림 6: 네 번째 및 다섯 번째 토큰, \\(t_{4}\\) 및 \\(t_{5}\\)의 초안 작성 방법의 비교(t_{1},t_{2})는 프롬프트이다. (t\\)은 토큰을 나타내고, \\(f\\)(오렌지 블록)는 그 특징을 나타내며, 첨자는 시퀀스에서 그들의 위치를 나타낸다. 빨간 경계는 모델 초안의 예측을 나타낸다. 단순화를 위해 그림과 같이 루카헤드의 \\(n\\)그램의 \\(n\\)를 2로 설정하였다.\n' +
      '\n' +
      '그림 7: EAGLE의 Pipeline. 상부 구간은 계산 과정을 나타내고, 하부 구간은 각 단계에 대해 해당 생성 결과를 표시한다. 상부 섹션에서 녹색 블록은 토큰 임베딩을 나타내며, 주황색 블록은 LLM의 2층으로부터 특징을 나타내고, 적색 상자는 오토 회귀 헤드에 의해 예측된 특징을 나타내고, 눈꽃 아이콘이 있는 청색 모듈은 트레이닝 대상이 아닌 원래 LLM 파라미터의 사용을 나타낸다.\n' +
      '\n' +
      '5.3.3절의 연구. 표적 LLM에 의해 생성된 텍스트를 사용하는 대신 고정된 데이터 세트를 사용하여 오버헤드를 실질적으로 감소시킨다. 초안 단계 동안 EAGLE 자동 조절 공정 기능이 있습니다. 특징의 오존은 오류 축적으로 이어질 수 있다. 이 문제를 완화하기 위해 훈련(Jain et al, 2023) 동안 표적 LLM의 2층 특징 벡터에 균일한 분포 \\(\\mathcal{U}(-0.1,0.1)\\에서 샘플링된 무작위 노이즈를 추가하여 데이터 증강을 사용한다.\n' +
      '\n' +
      '### Verification phase\n' +
      '\n' +
      '트리 주목을 받는 대상 LLM은 단일 정방향 패스를 통해 트리 구조화된 드래프트에서 각 토큰의 확률을 계산한다. 초안 트리의 모든 수준에서 우리는 사행 샘플링 알고리즘을 샘플에 재귀적으로 적용하거나 분포에 조정하며, SpecInfer(Miao et al., 2023), SpecTr(Sun et al., 2023)와 일치하여 출력 텍스트의 분포가 원래 LLM의 분포와 일치하도록 보장한다. 동시에, 우리는 승인된 토큰과 그에 상응하는 특징을 기록하며, 이는 후속 초안 단계에서 자동 억제 특징 외삽의 출발점이 된다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '** 모델 및 작업*** 우리는 비쿠나 모델(7B, 13B, 33B 크기), LLaMA2-chat 모델(7B, 13B, 70B) 및 미스트랄 8x7B 구성-v0.1에 대한 실험을 수행하여 현재 주류 대형 언어 모델(LLM)의 공통 크기를 포함했다. 우리의 평가는 대화와 수학적 추론 작업에 걸쳐 있다. 대화 과제를 위해 멀티턴 대화 및 지침 준수에서 모델의 능력을 테스트하도록 설계된 매우 현실적인 상황 질문을 포함하는 MT-벤치를 활용했다. 수학적 추론 과제를 위해 인간 문제 작가들이 만든 수준 높은 학교 수학 문제로 구성된 GSM8K(Cobbe et al, 2021) 데이터셋을 사용했다. 누적 표본 추출(Leviathan et al., 2023)은 회분식 크기 1로 실험을 수행했으며, 후속적으로 디필 종(Z404 et al., 2023), BiLD(김 et al., 2023)와 같은 다른 작품에 의해 채택되었다. 유사하게, 대부분의 실험은 또한 이 설정을 채택했다. 1보다 큰 배치 크기를 사용한 실험은 5.4절에서 표시된다.\n' +
      '\n' +
      '*** 메트릭*** 추측 샘플링을 기반으로 한 다른 방법과 유사하게 EAGLE는 처리량이 아닌 잠복기에 주로 초점을 맞춘다. 리필종의 방법론에 따라 다음 메트릭을 사용하여 가속 효과를 평가한다.\n' +
      '\n' +
      '* 월 시간 속도 비율: 바닐라 자동 억제 디코딩에 대한 실제 테스트 속도 비율.\n' +
      '* 평균 수용 길이 \\(\\tau\\): 원래 LLM의 전방 패스당 수용되는 토큰의 평균 수이다.\n' +
      '*수용률 \\(\\alpha\\): 드래프트 단계 동안 생성된 토큰 수에 대해 수용된 토큰 수의 비율. 수용률 메트릭은 초안 정확도를 측정한다. 트리 초안에 대해 동일한 위치에서 여러 토큰을 샘플링할 수 있지만 하나의 토큰만 받아들여져 수용률 메트릭이 이러한 맥락에서 평가에 적합하지 않다. 따라서 이 논문에서 수용률에 대해 보고된 실험을 위해 우리는 트리 초안을 사용하지 않고 투기 샘플링 및 섬모와 유사한 사슬 초안을 사용한다. EAGLE의 초안 모델은 특징 서열과 토큰 시퀀스를 입력으로 한다. 초안 단계에서 특징의 자동 억제 처리는 오류를 전파할 수 있다. 우리는 입력이 에러를 포함할 수 있는 초안 모델에 의해 예측된 \\(n\\) 특징을 \\(n\\)-\\(\\alpha\\)로 포함할 때 수용률을 정의한다.\n' +
      '\n' +
      'EAGLE의 속도는 이론적으로 원래 LLM의 출력 분포의 보존을 보장한다. 결과적으로 EAGLE의 생성된 결과의 품질을 평가하는 것은 불필요하고 무의미하다.\n' +
      '\n' +
      '*** 훈련****** 목표 LLM을 고정했습니다. 3e-5로 설정된 학습율로 68,000개의 대화 반복을 사용하여 ShareGPT 플랫폼에서EAGLE를 학습했으며 베타 값 \\((\\beta_{1},\\beta_{2})으로 68,000개의 대화 반복을 사용하여(0.9, 0.95)로 설정하고 0.5의 구배 클램핑을 구현했으며 7B, 13B, 33B 및 70B 모델에 해당하는 EAGLE의 훈련 가능한 매개변수는 각각 0.24B, 0.37B, 0.37B, 0.37B, 0.5에 해당하는 EAGLE의 훈련 가능한 매개변수 0.5, 0.5, 13B, 13B, 33B, 33B, 0.5에 해당하는 EAGLE의 훈련 가능한 매개변수 0.24B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.37B, 0.56B, 0. MoE 모델 Mixtral 8x7B에 대한 EAGLE의 훈련 가능한 매개변수는 0.28B이다. EAGLE는 낮은 훈련 비용을 특징으로 하며, 오토 회귀 헤드는 70B 모델에 대한 A100 40G 서버에서 1-2일 이내에 훈련 가능하다.\n' +
      '\n' +
      '그림 8: 다양한 과제에 걸쳐 EAGLE의 스피드업 비율을 보여준다.\n' +
      '\n' +
      '### Effectiveness\n' +
      '\n' +
      '그림 1은 온도=0으로 MT-벤치에 EAGLE, 메두사 및 루카헤드의 속도율, Vicuna 13B 및 LLaMA2-Chat 13B, 70B 모델, EAGLE는 바닐라 자동 억제 디코딩보다 3배 빠른 속도, 루카헤드보다 2배 빠른 속도, 메두사보다 1.6배 빠른 속도를 달성한다. 그림 2와 표 2는 다양한 환경에서 EAGLE의 가속 비율을 보여준다. 그림 8은 다양한 과제에 걸쳐 EAGLE의 속도률을 나타낸다. 실질적으로 많은 고정 템플릿을 포함하는 코딩 작업은 가장 중요한 속도 효과를 나타낸다. EAGLE는 다른 작업에서도 주목할 만한 효과를 보여준다. 표 1은 EAGLE의 평균 수용 길이와 수용률을 나타낸다. 원래의 LLM은 패스당 거의 4개의 토큰을 생성하며, 이는 패스당 하나의 토큰을 생성하는 바닐라 자동 억제 디코딩에 비해 상당한 효율성 향상이다. 완전히 정확한 특징 서열인 \\(0\\)-\\(\\alpha\\)를 갖는 수용률은 단일 잘못된 특징인 \\(1\\)-\\(\\alpha\\)보다 현저히 높으며, 이는 특징의 오류가 EAGLE의 성능에 영향을 미칠 수 있음을 나타낸다. 그러나 \\(1\\)-\\(\\alpha\\), \\(2\\)-\\(\\alpha\\), \\(3\\)-\\(\\alpha\\), \\(4\\)-\\(\\알파\\) 간의 최소 차이는 오류와 그러한 오류의 축적을 효과적으로 관리하는 능력을 특징으로 하는 EAGLE의 견고성을 보여준다.\n' +
      '\n' +
      '표 3에 나타난 바와 같이, 미스칼 8x7B 구성-v0.1 모델에 대해 EAGLE는 1.5배 가속도를 달성했다. LLaMA와 같은 모델과 달리 비교적 적은 가속도는 부분적으로 더 낮은 평균 수용 길이에 기인할 수 있으며 부분적으로 투기 샘플링 기반 방법을 사용하여 전문가(MoE) 모델의 제조를 가속화시키는 고유한 도전에 기인할 수 있다. MoE 모델은 기반별로 전문가를 선택합니다. 바닐라 자동 억제 디코딩 동안 단일 토큰을 포워딩하는 것은 오직 두 전문가의 가중치를 읽는 것을 수반한다. 대조적으로, 다중 토큰을 포워딩하는 것을 포함하는 투기 샘플링 기반 방법의 검증 단계는 둘 이상의 가중치를 판독하거나 심지어 모든 전문가를 필요로 할 수 있다. 반대로, 표준 디코더 전용 모델은 하나 또는 다수의 토큰이 전달되는지 여부에 관계없이 모든 가중치를 판독해야 한다.\n' +
      '\n' +
      'EAGLE + gpt-LB 초고속### Case 연구 : EAGLE + gpt-LB-LB#### Case 연구이다.\n' +
      '\n' +
      'EAGLE는 다른 가속 기술과 호환됩니다. 가속성을 위해 양자화 및 편집을 사용하는 EAGLE와 렙트-패드를 결합한 실험을 수행했다. 그림 4에서 보는 바와 같이 EAGLE를 gpt-fast과 통합하여 단일 RTX 3090에서 LLaMA2-Chat 7B의 생성 속도를 Huggingface의 구현을 사용하여 24.5 토큰/s와 비교하여 160.4 토큰/s로 증가시켰다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline  & Model & \\(\\tau\\) & 0-\\(\\alpha\\) & 1-\\(\\alpha\\) & 2-\\(\\alpha\\) & 3-\\(\\alpha\\) & 4-\\(\\alpha\\) \\\\ \\hline \\multirow{6}{*}{T=0} & V 7B & 3.94 & 0.79 & 0.74 & 0.72 & 0.73 & 0.67 \\\\  & V 13B & 3.98 & 0.79 & 0.74 & 0.72 & 0.74 & 0.70 \\\\  & V 33B & 3.68 & 0.74 & 0.69 & 0.67 & 0.67 & 0.66 \\\\  & LC 7B & 3.62 & 0.76 & 0.69 & 0.67 & 0.68 & 0.68 \\\\  & LC 13B & 3.90 & 0.77 & 0.69 & 0.69 & 0.70 & 0.71 \\\\  & LC 70B & 3.81 & 0.75 & 0.69 & 0.65 & 0.64 & 0.64 \\\\ \\hline \\multirow{6}{*}{T=1} & V 7B & 3.17 & 0.71 & 0.68 & 0.66 & 0.66 & 0.65 \\\\  & V 13B & 3.20 & 0.73 & 0.68 & 0.68 & 0.67 & 0.69 \\\\  & V 33B & 3.22 & 0.71 & 0.67 & 0.64 & 0.64 & 0.64 \\\\ \\cline{1-1}  & LC 7B & 3.30 & 0.71 & 0.66 & 0.66 & 0.66 & 0.64 \\\\ \\cline{1-1}  & LC 13B & 3.45 & 0.73 & 0.69 & 0.66 & 0.67 & 0.67 \\\\ \\cline{1-1}  & LC 70B & 3.46 & 0.73 & 0.67 & 0.64 & 0.66 & 0.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: MT-벤치에 대한 평균 수용 길이 \\(\\tau\\) 및 수용률 \\(\\alpha\\)이다. T는 온도를 나타내고 V는 비쿠나, LC는 LLaMA2-Chat를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Precision & FP16 & int4 \\\\ \\hline Vanilla (Huggingface) & 24.5 tokens/s & N/A \\\\ gpt-fast & 55.1 tokens/s & 106.9 tokens/s \\\\ EAGLE + gpt-fast & 100.2 tokens/s & 160.4 tokens/s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: EAGLE와 게이트-패드를 결합하는 세대 속도는 표 4이다. 평가 데이터셋은 MT-벤치, 타겟 LLM은 LLaMA2-Chat 13B, 온도는 0으로 설정된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline  & Model & \\(\\tau\\) & 0-\\(\\alpha\\) & 1-\\(\\alpha\\) & 2-\\(\\alpha\\) & 3-\\(\\alpha\\) & 4-\\(\\alpha\\) \\\\ \\hline \\multirow{6}{*}{T=0} & V 7B & 3.94 & 0.79 & 0.74 & 0.72 & 0.73 & 0.67 \\\\  & V 13B & 3.98 & 0.79 & 0.74 & 0.72 & 0.74 & 0.70 \\\\  & V 33B & 3.68 & 0.74 & 0.69 & 0.67 & 0.67 & 0.66 \\\\  & LC 7B & 3.62 & 0.76 & 0.69 & 0.67 & 0.68 & 0.68 \\\\  & LC 13B & 3.90 & 0.77 & 0.69 & 0.69 & 0.70 & 0.71 \\\\  & LC 70B & 3.81 & 0.75 & 0.69 & 0.65 & 0.64 & 0.64 \\\\ \\hline \\multirow{6}{*}{T=1} & V 7B & 3.17 & 0.71 & 0.68 & 0.66 & 0.66 & 0.65 \\\\  & V 13B & 3.20 & 0.73 & 0.68 & 0.68 & 0.67 & 0.69 \\\\  & V 33B & 3.22 & 0.71 & 0.67 & 0.64 & 0.64 & 0.64 \\\\  & LC 7B & 3.30 & 0.71 & 0.66 & 0.66 & 0.66 & 0.64 \\\\  & LC 13B & 3.45 & 0.73 & 0.69 & 0.66 & 0.67 & 0.67 \\\\  & LC 70B & 3.46 & 0.73 & 0.67 & 0.64 & 0.66 & 0.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: GSM8K에 대한 스피드업 비율, 평균 수용 길이 \\(\\tau\\) 및 수용률 \\(\\alpha\\)이다. T는 온도를 나타내고 V는 비쿠나, LC는 LLaMA2-Chat를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline Speedup & \\(\\tau\\) & 0-\\(\\alpha\\) & 1-\\(\\alpha\\) & 2-\\(\\alpha\\) & 3-\\(\\alpha\\) & 4-\\(\\alpha\\) \\\\ \\hline \\multirow{6}{*}{T=0} & V 7B & 3.94 & 0.79 & 0.74 & 0.72 & 0.73 & 0.67 \\\\  & V 13B & 3.98 & 0.79 & 0.74 & 0.72 & 0.74 & 0.70 \\\\  & V 33B & 3.68 & 0.74 & 0.69 & 0.67 & 0.67 & 0.66 \\\\  & LC 7B & 3.62 & 0.76 & 0.69 & 0.67 & 0.68 & 0.68 \\\\  & LC 13B & 3.90 & 0.77 & 0.69 & 0.69 & 0.70 & 0.71 \\\\  & LC 70B & 3.81 & 0.75 & 0.69 & 0.65 & 0.64 & 0.64 \\\\ \\hline \\multirow{6}{*}{T=1} & V 7B & 3.17 & 0.71 & 0.68 & 0.66 & 0.66 & 0.65 \\\\  & V 13B & 3.20 & 0.73 & 0.68 & 0.68 & 0.67 & 0.69 \\\\ \\cline{1-1}  & V 33B & 3.22 & 0.71 & 0.67 & 0.64 & 0.64 & 0.64 \\\\ \\cline{1-1}  & LC 7B & 3.30 & 0.71 & 0.66 & 0.66 & 0.66 & 0.64 \\\\ \\cline{1-1}  & LC 13B & 3.45 & 0.73 & 0.69 & 0.66 & 0.67 & 0.67 \\\\ \\cline{1-1}  & LC 70B & 3.46 & 0.73 & 0.67 & 0.64 & 0.66 & 0.65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 온도=0에서 MT-벤치에 대한 스피드업 비율, 평균 수용 길이 \\(\\tau\\), 수용율 \\(\\alpha\\). 표적 LLM은 미스칼 8x7B 구성-v0.1이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:8]\n' +
      '\n' +
      '배치 크기가 증가함에 따라 GPU의 이용 가능한 계산 용량이 감소하여 가속 효과의 감소로 이어진다. 이 섹션에서는 배치 크기가 1을 초과하는 시나리오에 대한 실험 결과를 제시하며 표 7에서 알 수 있듯이 배치 크기가 증가함에 따라 속도 비율이 감소한다. Vicuna 7B를 표적 LLM으로 사용할 때 bs=4에서의 속도율이 bs=3보다 높기 때문에, EAGLE의 검증 단계에서 표적 LLM은 단일 정방향 패스에서 다중 토큰을 처리하고 bs=4에서의 처리는 bs=3보다 빠르며, 대조적으로 표적 LLM이 정방향 패스당 하나의 토큰을 처리하는 바닐라 자동 억제 디코딩에서는 bs=3 및 bs=4에서의 속도는 거의 동일하기 때문이다.\n' +
      '\n' +
      '사행 샘플링 기반 방법은 주로 잠복기에 초점을 맞추지만 LLM 시스템의 또 다른 핵심 메트릭인 배치 크기 \\(>1\\)에 대한 EAGLE의 처리량을 조사했다. 바닐라 자동 억제 디코딩과 비교하여 EAGLE는 대략 동일한 CUDA 메모리를 필요로 한다. 표적 LLM으로서 비쿠나 7B의 경우, CUDA 메모리 24G를 갖는 단일 RTX 3090의 메모리 제약 하에서 동작하는 바닐라 자동 억제 디코딩 및 EAGLE에 대한 최대 배치 크기(bs)는 각각 8과 7이다. LLAMA2-Chat 70B의 경우 CUDA 메모리의 총 160G인 4 A100(40G) GPU에 의해 제약되는 바닐라 자동 억제 디코딩 및 EAGLE의 최대 b는 각각 5 및 4이다. 모든 평가는 FP16 정밀도로 수행되었다. 우리는 서로 다른 b에 대한 처리량을 계산하고 최대값을 선택했다. 바닐라 자동 억제 디코딩과 EAGLE 모두 각각의 최대 b에서 최대 처리량을 달성한다. 트리 주의는 더 많은 계산 자원을 소비한다. bs=7의 배치에서 계산 자원이 덜 풍부하여 나무의 불사용이 더 유리해진다. 표 7에 도시된 바와 같이 EAGLE는 처리량의 2배 증가를 달성한다.\n' +
      '\n' +
      '6개의 관련 작업.\n' +
      '\n' +
      '증류(힌튼 등, 2015년), 양자화(후바라 등 2018년)와 같은 기법을 포함하는 언어 모델 가속화에 상당한 연구가 있었다. 이러한 방법은 전방 패스당 지연 시간을 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Training data & Speedup & \\(\\tau\\) \\\\ \\hline Fixed dataset & 2.78x & 3.62 \\\\ Data generated by target LLM & 2.88x & 3.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: MT-벤치에 대해 평가된 서로 다른 훈련 데이터 세트를 사용한 속도율 및 평균 수용 길이 \\(\\tau\\)를 타겟 LLM이 LLAMA2-Chat 7B이고 온도가 0으로 설정된 경우 ShareGPT 데이터세트로부터 유래한 질문과 답변을 모두 나타낸다. "목표 LLM에 의해 생성된 데이터"는 ShareGPT 데이터세트로부터 질문이 공급되지만, 답변은 타겟 LLM에 의해 생성됨을 나타낸다.\n' +
      '\n' +
      '그림 10: 다양한 입력을 가진 모델 초안을 수행합니다. 표적 LLM은 비쿠나 7B이고 테스트 데이터 세트는 MT-벤치이다. 스피드는 벽시간 속도율, \\(\\tau\\)는 평균 수용 길이를 나타내며, \\(0\\)-\\(\\alpha\\)는 완전히 정확한 입력으로 수용률을, 1\\(\\알파\\)는 입력이 하나의 부정확한 특징을 포함하는 경우 수용률을 나타내고, \\(T\\)는 온도를 의미한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '* 호바라 등은 (2018) 허바라, I, 쿠바리아룩스, M, 스레이드, D, 엘-야니브, R 및 벤지오, Y. 양자화된 신경망: 정밀 가중치 및 액티베이션이 낮은 훈련 신경망: 정밀도 가중치 및 액티베이션이 낮은 훈련 신경망이다. 기계학습 연구의 저널_ 2018년 18(187):1-30.\n' +
      '* Jain et al.(2023) Jain, N., Chiang, P.-y, Wen, Y., Kirchenbauer, J., 추, H.-M, 일부팔리, G., Bartoldson, B. R, Kailkhura, B, Schwarzgr, A, 사하구, A. arXiv 프리프린트 arXiv:2310.05914_, 2023.\n' +
      '* 장 등은 A(2024) 장, A(2024) 장, A, Sablayrolles, A, Roux, A, Mensch, A, 사바리, B, Bamford, C, 차플롯, D. S, Casna, E. d, B. B, B. B. arXiv 프리프린트 arXiv:2401.04088_ 2024.\n' +
      '* 김씨는 (2023)김씨, (2023)망갈람, K씨, 문씨, S씨, 말릭, J씨, 마호니, MW씨, 가홀라미, A씨, 케투저씨 등이 있다. 큰 디코더가 거의 없는 누적 디코딩입니다. 19 2023년 신경정보처리시스템_에 관한 _35-세븐시 콘퍼런스에서.\n' +
      '* 레비아탄 등은 (2023) 레비아탄, Y, 칼만, M 및 마티아스, Y. 변압기의 마지막 추론은 투기적 디코딩을 통해 발생한다. 기계 학습_ 국제 회의에서 pp 19274-19286. PMLR, 2023.\n' +
      '* 류 등은 (2023) 류, X, Hu, L, Bailis, P, Stoica, I, Deng, Z, Cheung, A 및 장, H. 온라인 투기 디코딩. __ arXiv 프리프린트 arXiv:2310.07177_, 2023.\n' +
      '* 미오 등은 (2023) 미오, X, 올리아로, G, 장, Z, 청, X, 왕, Z, D, 아페넨, R, Z 및 Jia, Z. 특히, 추측적 추론 및 토큰 트리 검증과 함께 서빙하는 생성 LLM의 속도화: _ 투기 추론 및 토큰 트리 검증이다. arXiv 프리프린트 arXiv:2305.09781_, 2023.\n' +
      'V__패터슨(2004) 파터슨, D. 라티시니스 지연 밴드. __패터슨 (2004) 파터슨. 2004년 ACM_, 47(10):71-75의 통신.\n' +
      '* 산틸리 등은 A(2023) 산틸리, A, 세브레비노, S, 포스트올라체, E, 마루카, V, 만시, M, 마린, R 및 로올라, E. 평행 디코딩을 통해 번역에 대한 변압기 추론을 좌우한다. 로저스에서 A, Boyd-Graber, J 및 오카자키 N. 2012년 7월 캐나다 토론토 컴퓨터통계학회(종) 제61차 컴퓨터통계학회 연차회의(종 1: 롱 파이어스)_, pp 12336-12355, 컴퓨터통계협회. 10.18653/v1/2023.689 URL[https://aclanthology.org/2023.acl-오랜.689](https://aclanthology.org/2023.acl-오랜.689])이다.\n' +
      '* 셰제러(2019) 샤제러 N. 패스트 변압기 디코딩: 원 쓰기 헤드가 필요한 전부입니다. _ast 변압기 디코딩. arXiv 프리프린트 arXiv:1911.02150_ 2019.\n' +
      '* 분광기 & Re(2023) 스펙트럼기, B 및 Re, C. 엑셀레이팅 LLM 추론은 일련의 투기 디코딩으로 LLM 추론을 가능하게 한다. arXiv 프리프린트 arXiv:2308.04623_, 2023.\n' +
      '* 스턴 et al. (2018) 스턴, M, 샤제르, N 및 Uszkoreit, J. Blockwise 평행 디코딩은 심층 자기회귀 모델에 대한 것이다. 신경정보처리시스템_, 2018. 31.\n' +
      '* 선 등은 (2021) 선, X, Ge, T, Wei, F 및 왕, H. 인판티컬 문법 오류 보정이 얕은 공격적인 디코딩으로 되어 있다. arXiv 프리프린트 arXiv:2106.04970_ 2021.\n' +
      '*선(2023) 선, Z, Suresh, A. T, Ro, J. H, Beirami, A, Jain, H 및 유, F. Spectr: 최적의 수송을 통한 패스트 투기 디코딩이다. arXiv 프리프린트 arXiv:2310.15141_, 2023.\n' +
      '* 투브론 등은 (2023) 타우브론, H, 마틴, L., 알베르트, K, 알마히어, A, 알마히어, A, 바블리카, Y, 바틀라, S, 바드라바, P, 보세아바, P, 보세일, S, 오픈 파운데이션 및 미세 조미 채팅 모델. __BIAMA 2: 오픈 파운데이션 및 미세한 채팅 모델. arXiv 프리프린트 arXiv:2307.09288_, 2023.\n' +
      '*샤 등은 (2023) 샤, H, Ge, T., 왕, P., Chen, S. Q, Wei, F 및 Sui, Z. 누적 디코딩: seq2seq 생성을 가속화하기 위한 투기 실행을 유예한다. 컴퓨팅 언어학 협회의 _ 찾기에서 EMNLP 2023_, pp. 3909-3925, 2023.\n' +
      '* 양 등은 (2023a) 양, N, Ge, T, 왕, L, Jiao, B, 장, D, 양, L, Majumder, R 및 Fi, F. arXiv 프리프린트 arXiv:2304.04487_, 2023a.\n' +
      '* 양씨(2023b) 양씨(S), 이씨, G씨, 조씨, J씨, 파파일리오폴로스, D씨, 이씨(K)씨 등이 있다. 예측 파이프라인 디코딩: 정확한 llm 디코딩을 위한 압축-지연성 트레이드오프. arXiv 프리프린트 arXiv:2307.05908_, 2023b.\n' +
      '*장 등은 (2023) 장, J, 왕, J, 리, H, 슈, L, 첸, K, 첸, G 및 메로트라, S. 크래프트 & 검증: 자전 디코딩을 통한 무손실 대형 언어 모델 가속: __자전 디코딩을 통한 무손실 대형 언어 모델 가속이다. arXiv 프리프린트 arXiv:2309.08168_, 2023.\n' +
      '* 장 등은 P, Zeng, G., 왕, T., 루, W. TinyLlama: An 오픈소스 소형 언어 모델 __TinyLlama: An 오픈소스 소형 언어 모델. arXiv 프리프린트 arXiv:2401.02385_ 2024.\n' +
      '* 정 등은 (2023) 정, L., 치앙, W. 즉, 선그, Y, 주앙, S, 우, Z, Z, Z, Lin, Z, Li, D, Xing, E 등은 llm-as-a-분석을 mt-bench 및 챗봇장으로 추론한다. arXiv 프리프린트 arXiv:2306.05685_, 2023.\n' +
      '* 주 등은 (2023) 주, Y, 루, K, 롤트, A. S, 메논, A. K, 로스타미자데, A, 쿠마르, S, 카그, J-F 및 아가왈, R. 분포종: 지식 증류를 통한 투기 디코딩 개선: 지식 증류를 통한 투기 디코딩. arXiv 프리프린트 arXiv:2310.08461_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>