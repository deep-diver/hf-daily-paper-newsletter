<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 카듀서스: 양방향 등분산 장거리\n' +
      '\n' +
      'DNA 서열 모델링\n' +
      '\n' +
      'Yair Schiff\n' +
      '\n' +
      '교신저자: yzs2@cornell.edu Cornell University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University.\n' +
      '\n' +
      'Chia-Hsiang Kao\n' +
      '\n' +
      '코넬 대학교, \\({}^{2}\\)프린스턴 대학교, \\({}^{3}\\)카네기 멜론 대학교\n' +
      '\n' +
      'Aaron Gokaslan\n' +
      '\n' +
      '코넬 대학교, \\({}^{2}\\)프린스턴 대학교, \\({}^{3}\\)카네기 멜론 대학교\n' +
      '\n' +
      'Tri Dao\n' +
      '\n' +
      '프린스턴 대학교, \\({}^{3}\\)카네기 멜론 대학교\n' +
      '\n' +
      'Albert Gu\n' +
      '\n' +
      '카네기 멜론 대학교, \\({}^{2}\\)프린세톤 대학교, \\({}^{3}\\)카네기 멜론 대학교\n' +
      '\n' +
      'Volodymyr Kuleshov\n' +
      '\n' +
      '코넬 대학교, \\({}^{2}\\)프린스턴 대학교, \\({}^{3}\\)카네기 멜론 대학교\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 서열 모델링은 이제 생물학과 유전체학으로 확장되는 급속한 발전을 촉발했다. 그러나 유전체 서열을 모델링하는 것은 장거리 토큰 상호작용을 모델링해야 하는 필요성, 유전체의 상류 및 하류 영역의 영향, DNA의 역 상보성(RC)과 같은 문제를 도입한다. 여기서는 장거리 뭄바 블록을 구축하는 이러한 문제에 의해 동기화된 아키텍처를 제안하고 이를 양방향성을 지원하는 BiMamba 컴포넌트와 RC 등분성을 추가로 지원하는 MamboDNA 블록으로 확장한다. 우리는 MamboDNA를 RC 등분산 양방향 장거리 DNA 언어 모델의 첫 번째 계열인 카두서스의 기반으로 사용하고 카두서스 DNA 기초 모델을 산출하는 사전 훈련 및 미세 조정 전략을 소개한다. 카두수스는 다운스트림 벤치마크에서 이전 장거리 모델보다 우수하며, 어려운 장거리 변형 효과 예측 작업에서 카두수스는 양방향성이나 등분산을 활용하지 않는 10배 더 큰 모델의 성능을 초과한다. 우리의 실험을 재현하기 위한 코드는 여기에서 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 시퀀스 모델은 기계 학습의 급속한 발전을 촉발하여 자연 언어 처리(NLP)(Achiam et al., 2023; Team et al., 2023)를 넘어 과학, 생물학 및 의학으로 확장되는 발전을 가져왔다. 단백질체학에서, 이들 모델은 서열로부터 단백질 구조를 예측하는 것(Jumper et al., 2021; Lin et al., 2023), 아미노산의 기능 및 상호작용을 해독하는 것(Rao et al., 2020; Rives et al., 2021), 및 새로운 분자를 크래프팅하는 것(Madani et al., 2023)을 가능하게 하였다. 계산 비용이 감소함에 따라 서열 모델링은 생물학에 더 많은 영향을 미칠 준비가 되어 있다.\n' +
      '\n' +
      '서열 모델은 유전체학에서 표준 도구이기도 하다(Zhou and Troyanskaya, 2015; Avsec et al., 2021). 단백질과 달리 유전체는 비암호화 서열을 포함하며, 이는 종종 세포 메커니즘을 조절하는 데 중요한 역할을 하므로 잠재적으로 세포 생물학에 대한 더 큰 통찰력을 제공할 수 있다. 비코딩 서열을 이해하는 것은 게놈에 큰 언어 모델(LMs)을 적용하는 노력을 포함하여 최근 작업의 핵심 초점이었다(Ji et al., 2021; Benegas et al., 2023; Dalla-Torre et al., 2023; Nguyen et al., 2023).\n' +
      '\n' +
      '그러나 DNA를 모델링하는 것은 자연 언어 또는 단백질에 의해 제기된 것과 구별되는 문제를 도입한다. 첫째, 세포 표현형은 종종 유전체의 상류 및 하류 모두에서 염기쌍에 의해 영향을 받는데, 이는 양방향 컨텍스트를 처리하려면 서열 모델이 필요하다. 둘째, DNA는 서로 역보완하고 동일한 정보를 전달하는 두 가닥으로 구성되며, 이 특성을 모델링하면 성능을 크게 향상시킬 수 있다(Zhou et al., 2021; Mallet and Vert, 2021). 셋째, 유전자 발현에 대한 변이체의 영향을 예측하는 것과 같은 많은 유전체학 작업은 유전자로부터 최대 100만 염기쌍까지 핵산이 상당한 조절 효과를 가질 수 있기 때문에 장거리 상호작용을 수반할 수 있다(Furlong and Levine, 2018).\n' +
      '\n' +
      '본 논문에서는 이러한 문제점을 해결하기 위한 아키텍처 구성요소를 제안한다. 우리의 모듈은 장거리 Momba 블록(Gu and Dao, 2023)을 기반으로 하여 주의 기반 아키텍처의 2차 계산 비용 없이 수십만 개 이상의 뉴클레오티드의 긴 서열을 자연스럽게 처리한다(Vaswani et al., 2017). 우리는 Mampa를 양방향성을 지원하는 구성 요소인 BiMamba와 역보완(RC) 불일치를 추가로 추가하는 MampaDNA로 확장한다. MampaDNA 블록은 감독 및 자체 감독 컨텍스트 모두에서 유전체 분석을 위한 아키텍처에서 드롭인 대체물로 사용될 수 있다.\n' +
      '\n' +
      '그런 다음 MampaDNA를 RC 등분산 언어 모델링을 지원하는 최초의 양방향 장거리 DNA 서열 모델 계열인 Caduceus1의 기반으로 사용합니다. 우리는 유전체학의 광범위한 예측 작업에 대한 카두서스 기초 모델을 산출하는 사전 훈련 및 미세 조정 전략을 추가로 소개한다. Caduceus 모델은 다운스트림 성능 측면에서 유사한 크기의 이전 SSM 기반 모델(Nguyen et al., 2023)을 일관되게 능가한다. 많은 작업, 특히 장거리 모델링이 필요한 작업에서 카두세우스는 10배 더 큰 트랜스포머 기반 모델보다 성능이 우수하다.\n' +
      '\n' +
      '각주 1: 카두서스(\\(\\top\\))는 두 개의 얽힌 장님들에 의해 장식된 그리스 신화의 헤르메스가 운반한 직원이다. 우리는 DNA의 이중 나선 구조의 이미지를 불러일으키고 Mampa 서열 연산자를 사용하여 양방향성을 상징하기 위해 이 이름을 선택합니다.\n' +
      '\n' +
      '우리는 카두세우스를 사용하여 유전자 돌연변이가 표현형-유전자 발현에 영향을 미치는지 여부를 결정하는 작업인 변이 효과 예측(VEP)을 수행한다. 이 작업은 사전 훈련이 VEP의 핵심 신호원인 진화 압력(예: 보존, 공진화)의 영향을 암묵적으로 인식하는 것을 학습하기 때문에 카두서스에 자연스러운 적합성이 있다. 유전자 발현에 대한 장거리 효과를 가진 돌연변이의 표준 데이터세트(Avscc et al., 2021)에서 파생된 작업에서 카듀서스는 양방향성과 동등성을 모두 활용하지 않는 큰 마진 기존 주의 및 SSM 기반 모델에서 능가한다.\n' +
      '\n' +
      '요약하자면, 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 양방향 시퀀스 모델링을 지원하는 Mampa 블록의 파라미터 및 하드웨어 효율적인 확장인 BiMamba를 소개한다.\n' +
      '2. 유전체학에서 딥러닝 아키텍처의 일반적인 구성요소인 MampaDNA 블록을 생성하는 RC 등분산성을 지원하기 위해 BiMamba를 확장한다.\n' +
      '\n' +
      '도 1: 게놈 서열을 위한 Mampa 모듈. _ (Left)_**Mampa**: Gu and Dao(2023)에서 제안된 원래의 좌우 인과적 Mampa 모듈 _ (Middle)_**BiMamba**: Mampa 모듈의 파라미터 효율적인 양방향 확장. 인-프로젝션 및 아웃-프로젝션 파라미터는 시퀀스 및 그 역의 처리를 위해 공유된다. 역순을 처리한 후 다시 뒤집어서 순방향 출력에 추가한다. _ (Right)_**Reverse complement equivariant Mampa(MampaDNA)**: RC equivariance 귀납적 바이어스를 갖는 모듈. 입력은 먼저 채널 차원을 따라 두 개로 분할된다. 하나의 분할은 그것에 적용되는 역방향 보완(RC) 연산을 갖는다. Mampa 모듈의 모든 파라미터는 순방향 및 RC 시퀀스를 처리하기 위해 공유된다. 역방향 시퀀스는 채널 차원을 따라 순방향 출력과 다시 연결되기 전에 RC가 한 번 더 적용된다.\n' +
      '\n' +
      ' 3. MamboDNA를 RC-equivariant DNA 기초 모델의 첫 번째 계열인 Caduceus의 기반으로 사용한다.\n' +
      '4. 장거리 작업에서 카두서스는 최대 10배 더 크지만 양방향성이나 등분리를 사용하지 않는 모델보다 성능이 우수함을 보여준다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### DNA Terminology\n' +
      '\n' +
      '_Deoxyribonucleic acid_(DNA)는 사다리/이중나선 방식으로 바람을 일으키는 두 개의 상보적인 가닥으로 이루어진 중합체로서 _adenine_(A), _cytosine_(C), _guanine_(G) 또는 _thymine_(T)의 네 개의 _nucleotide_염기로 구성된다. 뉴클레오티드 염기 사이의 결합은 꼬인 사다리에서 \'룽\'을 형성하며 A 결합과 T 결합, C 결합과 G DNA 결합에는 단백질을 형성하는 유전 코드가 포함되어 있다. 복잡한 유기체에서 DNA는 수십억 개의 뉴클레오티드 염기쌍(bps) 길이가 될 수 있지만 긴 가닥은 _히스톤_라고 불리는 핵의 단백질 주위에 단단히 감겨 있다.\n' +
      '\n' +
      '단일 뉴클레오티드 다형성_(SNP)으로 알려져 있는 개별 bps에서의 유전적 돌연변이는 유기체 전반에 걸친 표현형 변이를 설명할 수 있다. 진화 압력은 시간과 종에 걸쳐 여러 유전체 영역을 보존하도록 강요했으며 해로운 돌연변이는 개체군에서 증식하지 못했다. 따라서 보존된 영역의 돌연변이는 표현형에 큰 영향을 미칠 수 있으며 이러한 영역을 식별할 수 있는 모델은 변이 효과 예측 작업에서 더 잘 수행할 가능성이 있다.\n' +
      '\n' +
      '역보체 가닥 이중 나선 DNA 구조에서 각 가닥에는 의미적으로 동일한 정보가 포함되어 있다. 주어진 가닥의\'verse complement\'(RC)은 \'forward\' 가닥에 비해 A는 T로, C는 G로 전환된 bps가 보완된 대응물의 반대 방향으로 배향된다. 많은 생물학적 분석에서 DNA의 가닥은 동일한 확률로 시퀀싱할 수 있다. 그러나, 비-회문 DNA 서열 모티프를 인식하는 학습은 표준 모델(Zhou et al., 2021)에 대해 어려울 수 있다. 따라서 입력 서열에 상응하는 방식으로 변형하는 모델 출력으로 느슨하게 정의된 RC 당분리를 시행하는 것은 DNA 서열 모델링의 중요한 데시데라타이다.\n' +
      '\n' +
      '구조화된 상태공간 모델\n' +
      '\n' +
      'Structured State Space Models(SSMs2; Gu et al. (2021, 2022); Gupta et al. (2022); Smith et al. (2022); Dao et al. (2022))로 알려진 최근 클래스 시퀀스 모델들은 장거리 모델들을 처리하는데 효과적인 것으로 입증되었다. 이 모든 모델의 핵심에는 입력 수열\\(x(t)\\in\\mathbb{R}\\)에서 중간 표현\\(\\mathbf{h}(t)\\in\\mathbb{R}^{N}\\)을 통해 출력 수열\\(y(t)\\in\\mathbb{R}\\)로의 매핑을 지배하는 한 쌍의 선형 미분 방정식이 있다.\n' +
      '\n' +
      '각주 2: 두문자어 SSM은 기계 학습 커뮤니티에서 일반적으로 이 모델 클래스를 참조하기 위해 사용되는 반면, 다른 분야에서는 일반적으로 엔지니어링에서 널리 사용되는 더 광범위한 상태 공간 모델 클래스와 관련이 있다.\n' +
      '\n' +
      '\\[\\dot{\\mathbf{h}}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),\\hskip 14.226378pty(t)=\\mathbf{C}h(t)+\\mathbf{D}x(t), \\tag{1}\\t)\n' +
      '\n' +
      '여기서 \\(\\mathbf{A}\\in\\mathbbb{R}^{N\\times N},\\mathbf{B}\\in\\mathbbb{R}^{N\\times 1},\\mathbf{C}\\in\\mathbbb{R}^{1\\times N}\\), 및 \\(\\mathbf{D}\\in\\mathbb{R}\\)는 시스템의 파라미터이다. 다차원 수열인 \\(\\mathbf{x}(t),\\mathbf{y}(t)\\in\\mathbb{R}^{D}\\의 경우 각 성분에 독립적으로 동역학을 적용한다.\n' +
      '\n' +
      '이 미분 방정식은 다음과 같이 변환된 연속 파라미터로 이산화될 수 있다:\n' +
      '\n' +
      '\\overline{\\mathbf{h}_{t+1}=\\overline{\\mathbf{A}\\mathbf{h}_{t}+\\overline{\\mathbf{B}x_{t}, \\hskip 14.226378pty_{t+1}=\\mathbf{C}\\mathbf{h}_{t}+\\mathbf{D}x_{t}, \\tag{2}\\tag}\n' +
      '\n' +
      '연속 매개변수\\(\\mathbf{A},\\mathbf{B}\\) 및 추가 시간 척도 매개변수\\(\\Delta\\)의 함수인 일부 이산화 공식에 의해 달성된다. SSM 문헌에서 사용되는 일반적인 이산화는 다음과 같이 정의되는 제로-오더 홀드이다:\n' +
      '\n' +
      '\\exp(\\Delta\\mathbf{A}}=\\exp(\\Delta\\mathbf{A}),\\hskip 14.226378pt\\overline{\\mathbf{B}=\\mathbf{A}^{-1}(\\exp(\\Delta\\mathbf{A})-\\mathbf{I})\\mathbf{B}.\\tag{3}\\tag{3}}\n' +
      '\n' +
      '중요하게는, 수학식 1의 선형 시간 불변(LTI)은 우리가 반복을 풀어서 등가적으로 수학식 2를 컨볼루션으로 공식화할 수 있게 하여, 훈련 동안 효율적인 병렬 계산을 가능하게 한다.\n' +
      '\n' +
      '그러나, LTI 공식의 계산 효율은 모델이 특정 입력에 적응/참석할 수 없는 대가를 치르게 된다. 이러한 표현력 부족을 완화하기 위해 구와 다오(2023)는 입력 \\(x(t)\\)에 대한 매개변수 \\(\\mathbf{B},\\mathbf{C}\\) 및 \\(\\Delta\\)의 의존성을 가능하게 하는 _selective_ SSM을 도입한다.\n' +
      '\n' +
      '\\text{Linear}_{\\mathbf{B}_{t}=&\\text{Linear}_{\\mathbf{B}}(x_{t})\\qquad\\mathbf{C}_{t}=\\text{Linear}_{\\mathbf{C}(x_{t})\\\\&\\Delta_{t}=\\text{softplus}(\\text{Linear}_{\\Delta}(x_{t})),\\end{split}\\tag{4}\\text{C}_{t}=\\text{Linear}_{\\Delta}(x_{t}))\n' +
      '\n' +
      '여기서 \\(\\text{Linear}(\\cdot)\\)은 선형 투영을 나타내고 \\(\\text{softplus}(\\cdot)=\\log(1+\\exp(\\cdot))\\을 나타낸다.\n' +
      '\n' +
      '이 공식은 시간 의존적으로 \\(\\overline{\\mathbf{A}}_{t}\\)과 \\(\\overline{\\mathbf{B}}_{t}\\)을 렌더링하지만, 방정식 2의 선형 재발은 결합 스캔(Martin and Cundy, 2017)으로 공식화될 수 있으며, 이는 효율적인 병렬 알고리즘(Belloch, 1990)을 사용하고 계산량을 시퀀스 길이에서 로그로 줄일 수 있다.\n' +
      '\n' +
      '구 및 다오(2023)에 제시된 맘바 블록은 선택적 SSM 서열 변환과 게이트 MLP 메커니즘을 결합하여 형성된다. 이것은 그림 1의 가장 왼쪽 도식에서 묘사된다. 들어오는 시퀀스는 복사되고 입력 치수의 두 배로 투영된다. 이어서, 하나의 복사본이 인과 컨벌루션을 통해 통과되고, 이어서 SiLU/Swish 비선형 활성화(Ramachandran et al., 2017)가 이어지고, 이어서 최종적으로 선택적 SSM을 통해 통과된다. 다른 복사본은 SiLU 비선형성을 적용한 후 SSM 출력을 게이트한다. 그런 다음 게이트 표현은 원래 차원\\(D\\)으로 다시 투영된다. 이는 인과적, 좌-우 시퀀스 연산이므로, 맘바 블록을 사용하는 원래 모델들은 사전 트레이닝 동안 다음 토큰 예측(NTP) 목적으로 트레이닝된다.\n' +
      '\n' +
      '##3 양방향 및 RC-등분산 Mamba\n' +
      '\n' +
      '이 섹션에서는 맘바 블록(구 및 도, 2023)을 확장하는 구성 요소를 제시한다. 이러한 확장은 도메인 진단적이지만 DNA 모델링과 관련이 있다.\n' +
      '\n' +
      '### BiMamba\n' +
      '\n' +
      '우리가 표준 맘바 모듈에 적용하는 첫 번째 확장은 인과관계(좌우)에서 양방향으로 변환하는 것이다. 우리는 맘바 모듈을 원래 시퀀스에 한 번, 길이 차원을 따라 반전된 사본에 한 번 두 번 적용하여 이를 달성한다. 정보를 결합하기 위해, 반전된 시퀀스의 출력은 길이 차원을 따라 뒤집히고 순방향에 추가된다.\n' +
      '\n' +
      '이 방법의 순진한 구현은 모듈의 매개변수 수를 두 배로 늘릴 것이다. 추가된 메모리 풋프린트를 피하기 위해 대신 \'포워드\'와 \'버스\' 맘바 사이에 투영 가중치를 공유한다. 이러한 예측은 컨볼루션 및 SSM 하위 모듈에 비해 모델 매개변수의 대다수를 차지한다. 우리는 이 매개변수 효율적인 양방향 블록을 **BiMamba**라고 한다. 이 모듈은 그림 1의 중간 도식에 나와 있다.\n' +
      '\n' +
      '### MambaDNA\n' +
      '\n' +
      'RC 등분산 귀납적 바이어스를 모듈들에 인코딩하기 위해, 우리는 Mamba(또는 BiMamba) 블록을 시퀀스와 그 RC에 적용하고, 두 애플리케이션들 사이에 공유되는 파라미터들을 갖는다(Shrikumar et al., 2017; Zhou et al., 2021). 유전체학과의 관련성을 고려할 때 우리는 이 블록을 \'맘바DNA**\'라고 부른다.\n' +
      '\n' +
      '보다 구체적으로, \\(D\\) 채널들을 \\(\\mathbf{X}_{1:T}^{1:D}\\)으로 갖는 길이 \\(T\\)의 시퀀스를 나타낸다. 그러면 우리의 채널 분할 동작은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\text{split}(\\mathbf{X}_{1:T}^{1:D}):=\\left[\\mathbf{X}_{1:T}^{1:(D/2)}, \\mathbf{X}_{1:T}^{(D/2):D}\\right].\\]\n' +
      '\n' +
      '우리는 또한 다음과 같이 RC 연산을 정의한다:\n' +
      '\n' +
      '\\[\\text{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\mathbf{X}_{T:1}^{D:1}\\]\n' +
      '\n' +
      '마지막으로, 채널 차원을 따라 시퀀스를 다시 결합하는 이 모듈의 마지막 동작을 concat로 표현하면, 우리가 \\(\\text{M}_{\\text{RCe},\\theta}\\)으로 나타내는 RC 등분산 Mamba 모듈은 다음과 같이 표현될 수 있다.\n' +
      '\n' +
      '\\mathrm{M}_{\\mathrm{RCe},\\theta}\\left(\\mathbrm{M}_{1:T}^{1:D}\\right):=\\mathrm{ concat}\\left(\\left[\\mathrm{M}_{\\theta}\\left(\\mathbrm{X}_{1:T}^{1:(D/2}\\right),\\mathrm{RC}\\left(\\mathbrm{M}_{\\theta}\\left(\\mathbrm{X}_{T:1}^{D:(D/2))\\right)\\right),\\mathbrm{X}_{T:1}^{D:(D/2)\\right)\\right)\\mathrm{ concat}\\left(\\left[\\mathrm{M}_{\\theta}\\left(\\mathbrm{X}_{T:1}^{D:(D/2)\\right)\\right),\\mathbrm{X}_{T:1}^{D:(D/2)\\right\n' +
      '\n' +
      '여기서 \\(\\mathrm{M}_{\\theta}\\)는 표준 Mamba 또는 BiMamba에 의해 파라미터화된 시퀀스 연산자를 나타낸다. MambaDNA 모듈은 그림 1의 가장 오른쪽 도식에서 표준 Mamba로 표시된 \\(\\mathrm{M}_{\\theta}\\)으로 표시된다.\n' +
      '\n' +
      '우리는 MambaDNA가 우리가 DNA 서열을 처리하고자 하는 RC 등분산 특성을 만족한다고 주장한다:\n' +
      '\n' +
      '**정리 3.1**.: _The \\(\\mathrm{M}_{RCe,\\theta}\\) 연산자가__\n' +
      '\n' +
      '\\[\\mathrm{RC}\\circ\\mathrm{M}_{RCe,\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)= \\mathrm{M}_{RCe,\\theta}\\circ\\mathrm{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right).\\]\n' +
      '\n' +
      '._Proof.__ 부록 A를 참조하십시오.\n' +
      '\n' +
      'BiMamba 모듈과 유사하게, MambaDNA 블록은 순방향 및 RC 서열을 처리하는 랩핑된 서열 연산자가 완전히 공유되기 때문에 상당한 추가 메모리 풋프린트를 수반하지 않는다.\n' +
      '\n' +
      '##4 Caduceus \\(\\mathbb{P}\\)\n' +
      '\n' +
      '아래에서는 RC 균일성을 강화하는 새로운 양방향 DNA LM 아키텍처인 **Caduceus**를 설명한다. 우리는 이 모델의 두 가지 버전을 소개하고, 각각은 다른 방식으로 등분산성을 유지한다: (1) 파라미터 공유를 통해(Shrikumar et al., 2017), Caduceus-PS 또는 (2) _post-hoc conjoining_(Zhou et al., 2021), Caduceus-Ph로 알려진 다운스트림 작업 동안 사용되는 기술을 통해.\n' +
      '\n' +
      '그림 2: Caduceus Architecture. 양방향, RC 등변 Mamba 모듈은 등변 단어 임베딩 및 언어 모델 헤드와 함께 사용되어 **Caduceus-PS**를 형성한다. 다운스트림 작업 추론을 위해 사전 훈련 및 사후 결합 동안 RC 데이터 증강이 있는 BiMamba 블록만 사용하면 Caduceus-Ph가 생성된다. 카두서스 이미지 라이선스: 크리에이티브 커먼즈 CC0 1.0 범용 공용 도메인 전용입니다.\n' +
      '\n' +
      '### Caduceus-PS\n' +
      '\n' +
      '*Caduceus-PS**를 위한 아키텍처, 섹션 3에 도입된 두 가지 아키텍처 혁신을 모두 활용합니다. 즉, 우리는 MambaDNA 블록 내에 BiMamba 모듈을 포장합니다. 추가적으로, 이 아키텍처의 맘바 블록들 앞에 RC 등분산 토큰 임베딩 모듈이 있다. 원-핫 벡터\\(\\mathbf{X}_{1:T}^{1:4}\\)를 취하고 \\(\\mathbb{R}^{D/2}\\)에 임베딩을 생성하는 선형 투영을 \\(\\text{Emb}_{\\theta}\\)으로 표현하면, 이 임베딩의 RC 등분산 버전은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\text{concat}\\left(\\left[\\text{Emb}_{\\text{RCe},\\theta}\\left(\\mathbf{X}_{1:T}^{1:4}\\right):\\text{concat}\\left(\\left[\\text{Emb}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:4}\\right),\\text{RC}\\circ\\text{Emb}_{\\theta}\\left(\\text{RC}\\left(\\mathbf{X}_{1:T}^{1:4}\\right)\\right)\\right)\\right)\\text{concat}\\left(\\left[\\text{Emb}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:4}\\right)\\text{RC}\\left(\\text{RC}\\left(\\mathbf{X}_{1:T}^{1:4}\\right)\\right)\\right)\\\n' +
      '\n' +
      '또한 카듀서스 모델의 로짓은 최종 MambaDNA 블록의 출력을 RC 등분산 언어 모델 헤드에 통과시켜 생성된다. 알고 있는 범위 내에서는, 카두서스-PS는 LM 사전 훈련 패러다임에 RC 균등성을 통합한 첫 번째 모델이다. 이는 먼저 채널 플립 연산자 \\(\\text{flip\\_chan}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\left(\\mathbf{X}_{1:T}^{D:1}\\right))를 정의함으로써 공식화될 수 있다. 그런 다음, \\(\\text{LM}_{\\theta}\\)을 \\(D/2\\) 채널을 갖는 시퀀스로부터 \\(\\mathbb{R}^{4}\\) 벡터로의 선형 투영으로 정의하면, 언어 모델링 헤드의 등분산 버전을 다음과 같이 정의한다.\n' +
      '\n' +
      '\\text{LM}_{\\text{RCe},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\text{LM}_{ \\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right)+\\text{flip\\_chan}\\circ\\text{ LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{D:(D/2)}\\right))\n' +
      '\n' +
      '**black** 경로로 그림 2에 묘사된 Caduceus-PS는 RC 등분산 언어 모델 사전 훈련을 가능하게 한다: 주어진 시퀀스의 RC에 대해 생성하는 예측은 길이 차원을 따라 원래 시퀀스의 예측을 반전시키고 출력을 보완하는 것과 동일하다: A-T 및 C-G. 우리는 다음 진술에서 이 주장을 공식화한다:\n' +
      '\n' +
      '**정리 4.1**.: _Composing \\(\\text{LM}_{\\text{RCe},\\theta}\\circ\\text{M}_{\\text{RCe},\\theta}^{(n)}\\circ\\text{Emb}_{\\text{RCe},\\theta}\\), 여기서 \\(\\text{M}_{\\text{RCe},\\theta}^{(n)}는 서로 다른 Mamba RC 등분산 모듈의 \\(n) 조성을 나타내며, RC 등분산 모듈인 시퀀스 연산자를 생성한다.\n' +
      '\n' +
      '증거: 부록 B를 참조하시오.\n' +
      '\n' +
      '이 모델의 양방향성을 고려한 사전 훈련은 BERT(Devlin et al., 2018)에서 제안한 표준 마스킹 레시피를 사용하여 마스킹 언어 모델링(MLM) 목적으로 Caduceus-PS를 훈련한다. Caduceus-PS의 RC 등분산 언어 모델링은 예측이 이 연산과 본질적으로 대칭적이기 때문에 사전 훈련에서 RC 데이터 증강이 필요하지 않다는 것을 의미한다.\n' +
      '\n' +
      '다운스트림 사용 다운스트림 작업에 대해 분석된 서열의 한 가닥이 동일한 라벨을 운반하기 때문에 RC _불변_를 시행하고자 한다. Caduceus-PS에서 토큰 임베딩 파라미터 공유는 그 중간 및 최종 은닉 상태들이 등가 크기의 토큰 임베딩 행렬을 갖는 표준 맘바 기반 언어 모델의 (채널) 차원성의 두 배임을 의미한다. 하류 훈련 및 추론에서 RC 불변성을 강제하기 위해 최종 은닉 상태가 분할되고 두 분할이 평균화된다.\n' +
      '\n' +
      '### Caduceus-Ph\n' +
      '\n' +
      '아키텍처 카두서스-Ph 모델은 그림 2에 파란색 경로로 묘사되어 있다. 이 모델의 핵심은 BiMamba 블록의 스택이다.\n' +
      '\n' +
      'Caduceus-PS를 사용하여 사전 훈련된 이 모델은 동일한 MLM 목표를 사용하여 사전 훈련된다. 그러나 모델이 RC 등분산 LM이 아니기 때문에 사전 훈련 중 데이터 증대에 의존한다.\n' +
      '\n' +
      '다운스트림 UsageIn make the downstream task representations RC invariant를 만들기 위해, 우리는 _post-hoc_ conjoining(Zhou et al., 2021)이라는 기법을 활용한다. 즉, 다운스트림 작업의 경우 백본 모델은 변경되지 않지만 RC 데이터 증강을 사용합니다. 그러나 다운스트림 태스크 _inference_의 경우, 모델을 원 시퀀스에 한 번, 해당 RC 시퀀스에 한 번, 평균 두 번 적용하여 \'RC 앙상블링\'(Mallet and Vert, 2021) 버전을 효과적으로 수행한다.\n' +
      '\n' +
      'Experiments\n' +
      '\n' +
      '### Pre-training\n' +
      '\n' +
      '데이터 우리는 이 작업의 초점을 인간 게놈 관련 작업으로 제한한다. 이를 위해, 우리는 인간 참조 게놈에 대한 모든 사전 훈련 작업을 수행한다(Consortium et al., 2009). 우리는 문자/기본 쌍 수준 토큰화를 사용합니다. 다른 DNA FM들이 k-mer 토큰화를 탐색했지만, 이 스킴은 입력 시퀀스에 대한 사소한 변경들이 극적으로 상이한 토큰화 출력들(Zhou et al., 2023)을 복잡하게 할 수 있다는 단점을 갖는다. 문자 수준 토큰화는 이 문제를 방지합니다. Re-training HyenaDNA (Nguyen et al., 2023) 모델을 포함하여 훈련하는 임의의 비-RC 등분산 모델에 대해, 우리는 사전 훈련 동안 RC 데이터 증강을 사용한다. 사전 교육 데이터 세트 및 레시피에 대한 자세한 내용은 부록 C를 참조하십시오.\n' +
      '\n' +
      '맘바 vs. HyenaDNA NTPSimilar of Gu and Dao (2023)의 예비 결과와 유사하게, Mamba 모듈이 NTP 측면에서 Hyena보다 더 나은 성능을 보인다는 것을 발견했다. 그림 2(a)에서 다양한 서열 길이와 비교 가능한 모델 크기에서 표준 Mamba 모델이 HyenaDNA에 비해 사전 훈련 데이터 테스트 세트에서 더 낮은 교차 엔트로피 손실을 달성한다는 것을 알 수 있다. (구 및 도, 2023)에서 보고된 바와 같이, 우리는 또한 Mamba가 LM을 훈련하는 일반적인 모범 사례인 학습률을 사용하여 더 높은 사용에 더 강력하다는 것을 발견했다. 이러한 결과는 모델의 내부 구성 요소로 맘바를 선택하는 데 도움이 된다.\n' +
      '\n' +
      'MLM 사전 학습에 대한 매개변수 공유의 효과 투영 매개변수 공유를 통해 유사한 매개변수 카운트에 대해 더 깊은 양방향 모델을 가능하게 하는 것의 중요성을 테스트하기 위해 BiMamba 모델의 MLM 사전 학습 손실을 가중치 타이링을 사용하지 않아 깊이의 절반으로 줄이는 순진한 양방향 Mamba 모델과 비교한다. 우리는 이방향성의 매개변수 효율적인 구현이 그림 2(b)에 표시된 것처럼 더 나은 사전 훈련 손실을 초래한다는 것을 발견했다.\n' +
      '\n' +
      'MLM 사전 훈련에 대한 RC 동등성의 영향 또한 제안된 RC 동등성 LM이 사전 훈련에 미치는 영향을 조사한다. 그림 2(c)에서 RC 등분산 LM이 더 나은 MLM으로 이어진다는 것을 발견했다.\n' +
      '\n' +
      '그림 3: 사전 훈련 테스트 세트 손실. (a) 유사한 모델 크기 및 서열 길이에 대해, 맘바는 인간 게놈에 대한 사전 트레이닝 동안 HyenaDNA보다 더 나은 교차 엔트로피 손실을 달성한다. (b) 서열 길이 전반에 걸쳐, 가중치 타이링을 사용하는 더 깊은 모델은 인간 게놈에 대한 더 나은 사전 훈련 손실을 갖는다. (c) 서열 길이에 걸쳐, RC 등분성은 인간 게놈에 대한 더 나은 사전 훈련 손실을 초래한다. 주목할 점은, 131k의 시퀀스 길이를 갖는 모델들은 사전 트레이닝 동안 오버헤드를 감소시키기 위해 덜 빈번하게 검증되었다. 배치 크기를 조정하여 배치당 토큰 수를 다양한 길이에 걸쳐 일정하게 유지합니다.\n' +
      '\n' +
      '사전 훈련 손실 이는 위에서 설명한 바와 같이 MLM 태스크에 대한 성능이 변형 효과 예측과 같은 다운스트림 태스크의 생물학에 기반을 두고 있기 때문에 중요하다.\n' +
      '\n' +
      '### Downstream Tasks\n' +
      '\n' +
      '###### 5.2.1 유전체학 벤치마크\n' +
      '\n' +
      '우리는 8개의 규제 요소 분류 작업이 있는 최근에 제안된 벤치마크인 Genomics Benchmarks(Gresova et al., 2023)로 평가를 시작한다. 우리의 기준선은 HyenaDNA3와 Gresova et al.(2023)에 기술된 감독 훈련된 CNN 모델로 구성된다. 하이에나DNA와 모든 맘바 기반 모델의 경우 최종 숨겨진 상태 임베딩을 취하고 길이가 200에서 약 2,000 bps인 서열에 대해 평균 풀링을 수행한다. 우리는 서로 다른 무작위 종자를 사용하여 5중 교차 검증(CV)을 수행하고 검증 정확도를 조기에 중단하고 5개 종자의 최대/분에 평균 및 \\(\\pm\\)을 보고한다.\n' +
      '\n' +
      '각주 3: [https://huggingface.co/LongSafari/hyenaDNA-tiny-lk-seqlen](https://huggingface.co/LongSafari/hyenaDNA-tiny-lk-seqlen)에서 다운로드 받은 사전 훈련된 가중치\n' +
      '\n' +
      '표 1에서 볼 수 있듯이 카두서스 모델은 모든 주석에서 최상의 성능을 달성한다. 참고로, 카두서스-Ph는 이 작업 세트에 대해 전반적으로 가장 잘 수행되는 모델이다. 사후 결합(post-hoc conjoining)을 검토하는 다른 작업들도 마찬가지로 이것이 강력한 모델링 결정이라고 생각한다(Zhou et al., 2021; Mallet and Vert, 2021).\n' +
      '\n' +
      '###### 5.2.2 Nucleotide Transformer 작업\n' +
      '\n' +
      '다음으로, Dalla-Torre et al. (2023)에 소개되고 5개의 피어 리뷰된 연구들로부터 도출된 18개의 데이터 세트들을 벤치마킹한다(Phaml et al., 2005; Geng et al., 2022; Wang et al., 2019; Oubouny et al., 2019; Scalzitti et al., 2021). 이 데이터 세트에는 히스톤 마커 예측, 규제 주석 예측 및 스플라이스 사이트 주석 예측을 포함한 세 가지 작업 유형이 포함되어 있다. 성능 평가에서 우리는 정확성을 보고하는 _splice site all_ task를 제외하고 모든 히스톤 마커 작업에 대한 MCC(Matthews Correlation Coefficient), 모든 규제 주석에 대한 F1 점수 및 스플라이스 사이트 주석 작업에 대한 서로 다른 메트릭을 사용하여 Dalla-Torre 등(2023)에 설명된 방법론을 준수했다. 우리는 추가로 Dalla-Torre et al.(2023)을 따라 검증 메트릭에 대한 조기 중단과 함께 상이한 랜덤 시드들을 사용하여 10-fold CV를 수행한다. 우리는 10개의 시드4의 max/min에 대한 평균과 \\(\\pm\\)을 보고한다. 이 벤치마크 스위트에 대한 결과는 표 2에 제시되어 있으며, 여기서 카듀서스-Ph가 18개의 예측 작업 중 8개에서 훨씬 더 많은 매개변수로 주의 기반 방법을 능가하면서 경쟁적으로 수행한다는 것을 다시 발견한다. 카두서스 모델은 거의 모든 히스톤 표지자 및 조절 주석 작업에서 유사한 크기의 HyenaDNA 모델을 능가하는 반면, HyenaDNA는 스플라이스 사이트 주석에서 더 잘 수행한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & CNN & HyenaDNA & Mamba & Caduceus & \\multirow{2}{*}{Caduceus-Ph} & \\multirow{2}{*}{Caduceus-PS} \\\\  & (264k) & (436k) & (468k) & & (470k) & (470k) \\\\ \\hline Mouse Enhancers & 0.715 \\(\\pm\\)0.087 & _0.780_\\(\\pm\\)0.025 & 0.743 \\(\\pm\\)0.054 & 0.770 \\(\\pm\\)0.058 & 0.754 \\(\\pm\\)0.074 & **0.793**\\(\\pm\\)0.058 \\\\ Coding vs. Intergenomic & 0.892 \\(\\pm\\)0.008 & 0.904 \\(\\pm\\)0.005 & 0.904 \\(\\pm\\)0.004 & 0.908 \\(\\pm\\)0.003 & **0.915**\\(\\pm\\)0.003 & _0.910_\\(\\pm\\)0.003 \\\\ Human vs. Worm & 0.942 \\(\\pm\\)0.002 & 0.964 \\(\\pm\\)0.002 & 0.967 \\(\\pm\\)0.002 & _0.970_\\(\\pm\\)0.003 & **0.973**\\(\\pm\\)0.001 & 0.968 \\(\\pm\\)0.002 \\\\ Human Enhancer Conn & 0.702 \\(\\pm\\)0.021 & 0.729 \\(\\pm\\)0.014 & 0.732 \\(\\pm\\)0.029 & 0.741 \\(\\pm\\)0.008 & **0.747**\\(\\pm\\)0.004 & _0.745_\\(\\pm\\)0.007 \\\\ Human Enhancer Ensembl & 0.744 \\(\\pm\\)0.122 & 0.849 \\(\\pm\\)0.006 & 0.862 \\(\\pm\\)0.008 & 0.883 \\(\\pm\\)0.002 & _0.893_\\(\\pm\\)0.008 & **0.900**\\(\\pm\\)0.006 \\\\ Human Regulatory & 0.872 \\(\\pm\\)0.005 & 0.869 \\(\\pm\\)0.012 & 0.814 \\(\\pm\\)0.211 & 0.871 \\(\\pm\\)0.007 & _0.872_\\(\\pm\\)0.011 & **0.873**\\(\\pm\\)0.007 \\\\ Human OCR Ensembl & 0.698 \\(\\pm\\)0.013 & 0.783 \\(\\pm\\)0.007 & 0.815 \\(\\pm\\)0.002 & 0.818 \\(\\pm\\)0.003 & **0.828**\\(\\pm\\)0.006 & _0.818_\\(\\pm\\)0.006 \\\\ Human Non/TATA Promotes & 0.861 \\(\\pm\\)0.009 & 0.944 \\(\\pm\\)0.002 & 0.933 \\(\\pm\\)0.007 & 0.933 \\(\\pm\\)0.006 & **0.946**\\(\\pm\\)0.007 & _0.945_\\(\\pm\\)0.010 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 유전체 벤치마크. 사전 훈련된 HyenaDNA, Mamba NTP 및 Caduceus 모델과 감독된 CNN 기준선(처음부터 훈련됨)에 대한 5배 교차 검증(CV)에 걸친 Top-1 정확도(\\(\\uparrow\\))이다. 작업당 가장 좋은 값은 **볼드**이고, 두 번째로 좋은 값은 _italicized_입니다. 오차 막대는 CV에 사용된 5개의 무작위 시드에 걸쳐 최대값과 최소값의 차이를 나타낸다.\n' +
      '\n' +
      '변이가 유전자 발현에 미치는 영향 예측 5.2.3\n' +
      '\n' +
      '마지막으로 SNP가 유전자 발현에 미치는 영향을 예측하는 작업에 대한 장거리 컨텍스트의 의미를 탐구한다. 이 작업을 실제로 장거리 상호 작용을 수반한다는 생물학적 증거가 있다(Furlong and Levine, 2018). 또한 LM 사전 훈련 목표와 잘 일치하여 모델이 진화적 압력(예: 보존, 공진화)의 영향을 인식하는 것을 암묵적으로 학습할 수 있다. 본 과제에서 사용된 데이터셋은 Enformer paper(Avsec et al., 2021)에서 도출하여 Trop et al.(2023)에 제시하였다. 각 모델에서 SNP 위치를 중심으로 임베딩을 추출한다. 우리는 SNP에서 가장 가까운 전사 시작 사이트(TSS)까지의 거리에 따라 데이터를 계층화한다. 각 버킷에 대해 5,000개의 훈련 포인트를 샘플링하고 RBF 커널이 있는 SVM 분류기를 피팅하여 VEP 주석을 예측한다. 우리는 5개의 무작위 훈련 하위 집합에 적합한 분류기에 대한 테스트 세트 AUCROC 평균 및 최대/min 범위를 보고한다. 이 실험에 대한 자세한 내용은 부록 D.3을 참조하시기 바랍니다. 카듀서스를 HyenaDNA5 및 Nucleotide Transformer6과 비교합니다.\n' +
      '\n' +
      '각주 5: [https://huggingface.co/LongSafari/hyenadma-medium-160k-seqlen-hf](https://huggingface.co/LongSafari/hyenadma-medium-160k-seqlen-hf])에서 다운로드된 사전 훈련된 가중치.\n' +
      '\n' +
      '각주 6: [https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species)에서 다운로드된 사전 훈련된 가중치.\n' +
      '\n' +
      '도 4에 도시된 바와 같이, 카두서스 모델은 일관되게 HyenaDNA를 능가하며, 카두서스-PS는 특히 가장 가까운 TSS까지의 거리가 증가함에 따라 뉴클레오티드 트랜스포머 v2(500M 매개변수 포함)의 성능을 큰 마진으로 초과한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c||c c c} \\hline \\hline  & \\multicolumn{2}{c||}{\\(>\\) 100M Param. Models} & \\multicolumn{3}{c}{\\(<\\) 2M Param. Models} \\\\  & Enformer & DABERT-2 & NT-v2 & HyenaDNA & Caduceus-Ph & Caduceus-PS \\\\  & (252M) & (117M) & (500M) & (1.6M) & (1.9M) & (1.9M) \\\\ \\hline \\multicolumn{6}{l}{_Histone Markers_} \\\\ H3 & 0.719\\(\\pm\\)0.048 & 0.785\\(\\pm\\)0.033 & 0.784\\(\\pm\\)0.047 & 0.779\\(\\pm\\)0.037 & **0.815\\(\\pm\\)**0.048 & _0.779\\(\\pm\\)_0.029 \\\\ H3k14ac & 0.288\\(\\pm\\)0.077 & 0.516\\(\\pm\\)0.028 & 0.551\\(\\pm\\)0.021 & _0.612\\(\\pm\\)_0.065 & **0.631\\(\\pm\\)**0.026 & 0.541\\(\\pm\\)0.122 \\\\ H3k36me3 & 0.344\\(\\pm\\)0.055 & 0.591\\(\\pm\\)0.020 & **0.625\\(\\pm\\)**0.013 & _0.613\\(\\pm\\)_0.041 & 0.601\\(\\pm\\)0.129 & 0.609\\(\\pm\\)0.109 \\\\ H3k4me1 & 0.291\\(\\pm\\)0.061 & 0.511\\(\\pm\\)0.028 & **0.550\\(\\pm\\)**0.021 & 0.512\\(\\pm\\)0.024 & _0.522\\(\\pm\\)_0.039 & 0.488\\(\\pm\\)0.102 \\\\ H3k4me2 & 0.211\\(\\pm\\)0.069 & 0.336\\(\\pm\\)0.040 & 0.319\\(\\pm\\)0.045 & _0.455\\(\\pm\\)_0.095 & **0.487\\(\\pm\\)**0.170 & 0.388\\(\\pm\\)0.101 \\\\ H3k4me3 & 0.158\\(\\pm\\)0.072 & 0.352\\(\\pm\\)0.077 & 0.410\\(\\pm\\)0.033 & **0.549\\(\\pm\\)**0.056 & _0.544\\(\\pm\\)_0.045 & 0.440\\(\\pm\\)0.202 \\\\ H3k799ie3 & 0.496\\(\\pm\\)0.042 & 0.613\\(\\pm\\)0.030 & 0.626\\(\\pm\\)0.026 & 0.672\\(\\pm\\)0.048 & **0.697\\(\\pm\\)**0.077 & _0.676\\(\\pm\\)_0.026 \\\\ H3k5Bac & 0.420\\(\\pm\\)0.063 & 0.542\\(\\pm\\)0.029 & 0.562\\(\\pm\\)0.040 & 0.581\\(\\pm\\)0.061 & **0.622\\(\\pm\\)**0.030 & _0.604\\(\\pm\\)_0.048 \\\\ H4 & 0.732\\(\\pm\\)0.076 & 0.796\\(\\pm\\)0.027 & _0.799\\(\\pm\\)_0.025 & 0.763\\(\\pm\\)0.044 & **0.811\\(\\pm\\)**0.022 & 0.789\\(\\pm\\)0.020 \\\\ H4ac & 0.273\\(\\pm\\)0.063 & 0.463\\(\\pm\\)0.041 & 0.495\\(\\pm\\)0.032 & _0.564\\(\\pm\\)_0.038 & **0.621\\(\\pm\\)**0.054 & 0.525\\(\\pm\\)0.240 \\\\ \\hline \\multicolumn{6}{l}{_Regulatory Annotation_} \\\\ Enhancer & 0.451\\(\\pm\\)0.108 & 0.516\\(\\pm\\)0.008 & **0.548\\(\\pm\\)**0.144 & 0.517\\(\\pm\\)0.117 & _0.546\\(\\pm\\)_0.073 & 0.491\\(\\pm\\)0.066 \\\\ Enhancer types & 0.309\\(\\pm\\)0.134 & 0.423\\(\\pm\\)0.051 & _0.424\\(\\pm\\)_0.122 & 0.386\\(\\pm\\)0.185 & **0.439\\(\\pm\\)**0.064 & 0.416\\(\\pm\\)0.095 \\\\ Promoter: All & 0.954\\(\\pm\\)0.006 & _0.971\\(\\pm\\)_0.006 & **0.976\\(\\pm\\)**0.006 & 0.960\\(\\pm\\)0.005 & 0.970\\(\\pm\\)0.004 & 0.967\\(\\pm\\)0.004 \\\\ NonTATA & 0.955\\(\\pm\\)0.010 & _0.972\\(\\pm\\)_0.005 & **0.976\\(\\pm\\)**0.005 & 0.959\\(\\pm\\)**0.008 & 0.962\\(\\pm\\)0.011 & 0.968\\(\\pm\\)0.006 \\\\ TATA & _0.960\\(\\pm\\)_0.023 & 0.955\\(\\pm\\)0.021 & **0.966\\(\\pm\\)**0.013 & 0.944\\(\\pm\\)0.040 & 0.953\\(\\pm\\)0.016 & 0.957\\(\\pm\\)0.015 \\\\ \\hline \\multicolumn{6}{l}{_Splice Site Annotation_} \\\\ All & 0.848\\(\\pm\\)0.019 & 0.939\\(\\pm\\)0.009 & **0.983\\(\\pm\\)**0.008 & _0.956\\(\\pm\\)_0.011 & 0.940\\(\\pm\\)0.027 & 0.927\\(\\pm\\)0.021 \\\\ Acceptor & 0.914\\(\\pm\\)0.028 & _0.975\\(\\pm\\)_0.006 & **0.981\\(\\pm\\)**0.011 & 0.958\\(\\pm\\)0.010 & 0.937\\(\\pm\\)0.033 & 0.936\\(\\pm\\)0.077 \\\\ Donor & 0.906\\(\\pm\\)0.027 & _0.963\\(\\pm\\)_0.006 & **0.985\\(\\pm\\)**0.022 & 0.949\\(\\pm\\)**0.024 & 0.948\\(\\pm\\)0.025 & 0.874\\(\\pm\\)0.289 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 뉴클레오타이드 트랜스포머 작업. Enformer, DNABERT-2, Nucleotide Transformer v2, HyenaDNA, Caduceus-Ph 및 Caduceus-PS에 대한 10배 CV에 걸친 성능(\\(\\uparrow\\))이다. 측정값은 작업에 따라 다르다: 히스톤 마커의 경우 MCC, 조절 및 스플라이스 사이트 수용체/공여자의 경우 F1 점수, 스플라이스 사이트 "모두"의 정확성. 작업당 가장 좋은 값은 **볼드**이고, 두 번째로 좋은 값은 _italicized_입니다. 모델 크기의 차이를 감안할 때, 우리는 또한 SSM 기반 모델 내에서 최상의 값을 강조한다. 오차 막대는 CV에 사용된 10개의 무작위 시드에 걸친 최대값과 최소값의 차이를 나타낸다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      'DNA 언어 모델\n' +
      '\n' +
      'DNABERT, v1(Ji et al., 2021) 및 v2(Zhou et al., 2023), 및 뉴클레오타이드 트랜스포머(Dalla-Torre et al., 2023)와 같은 트랜스포머 기반 DNA LM은 최대 콘텍스트 크기가 대략 12,000 bps까지인 트랜스포머의 2차 스케일링에 의해 제한되었다. BigBird (Zaheer et al., 2020) (및 GENA-LM (Fishman et al., 2023)은 BigBird를 백본으로 사용하는) 컨텍스트 크기를 크기 크기까지 스케일링하기 위해 드문 주의를 사용한다.\n' +
      '\n' +
      '특히, GPN(Benggas et al., 2023, 2023)은 이 모델을 트레이닝할 때 단지 512 bps의 컨텍스트 크기가 사용되지만, 실제적으로 큰 수용 필드들로 스케일링되는 확장된 컨볼루션 층들을 사용한다. Benegas et al. (2023)은 DNA LM이 강력한 비감독 변이 효과 예측 변수라는 것을 발견한다.\n' +
      '\n' +
      '우리의 작업과 가장 관련된 HyenaDNA는 DNA LM을 위한 빌딩 블록으로 SSM 문헌에서 파생된 Hyena 연산자를 사용하는 HyenaDNA 모델(Nguyen et al., 2023)이다. HyenaDNA는 장거리 서열(최대 100만 bps)로 확장할 수 있지만 특히 단방향 모델이며 본질적으로 RC 입력에 강력하지 않다.\n' +
      '\n' +
      '### DNA 역보완 훈련\n' +
      '\n' +
      'Cao & Zhang (2019)은 유전체학에서 RC 데이터 증강의 중요성에 대해 논의한다. Shrikumar et al. (2017)은 컨볼루션, 배치 정규화, 및 풀링 모듈들을 위한 RC Parameter Sharing (RCPS)를 소개한다. Mallet & Vert(2021)는 그룹 표상의 언어로 RC 당분리를 공식화하고 RCPS를 그러한 표상의 특정 분해로 캐스팅하여 다른 것도 탐구한다. MambaDNA 블록에서 RCPS의 구현은 Shrikumar et al.(2017)에서 제안된 것과 다르며, 분할 연산은 주어진 레이어를 통해 시퀀스를 전달할 때 채널 차원이 두 배로 증가하는 것을 방지한다는 점에서 다르다.\n' +
      '\n' +
      'Zhou et al.(2021)은 RCPS 계층들을 추가로 탐색하고, 이들을 우리의 Caduceus-Ph 모델에 대한 영감을 제공하는 _post-hoc_ conjoining baseline과 비교한다. Zhou et al.(2021) find the post-hoc conjoining\n' +
      '\n' +
      '도 4: 가장 가까운 전사 시작 부위(TSS)까지의 다양한 거리에 걸친 유전자 발현에 대한 변이체 효과 예측. 비교된 모델에는 NT-v2, HyenaDNA, Caduceus w/o RC Equiv, Caduceus-Ph 및 Caduceus-PS가 포함되며 모델 크기는 괄호 안에 표시된다. SSM 기반 모델은 131k 시퀀스 길이를 활용한다. TSS까지의 짧은 거리(0 - 30kb), 중간 거리(30 - 100kb) 및 장거리 거리(100kb+)에서 성능을 보여준다. 특히, 카두서스-PS는 장거리 효과에 대한 향상된 예측 정확도를 일관되게 보여준다. 오차 막대는 각각 다른 데이터 세트 하위 집합에 대해 훈련된 5개의 SVM 분류기에 걸친 표준 편차를 나타낸다.\n' +
      '\n' +
      '여러 작업에서 RCPS 모델을 능가하는 강력한 기준선입니다. 우리는 Zhou et al.(2021)이 감독 훈련 체제에만 초점을 맞춘 반면, LM 사전 훈련 단계도 포함하도록 사후 결합 방법론을 확장한다는 점에 주목한다. 예측 컨조이닝은 또한 평균화가 아닌 max aggregation이 사용되는 DeepBind(Alipanahi et al., 2015)와 추론뿐만 아니라 훈련 중에 컨조이닝을 수행하는 FactorNet(Quang and Xie, 2019)에서도 탐색되었다.\n' +
      '\n' +
      '마지막으로, 우리의 설정과 유사하게, Gunduz et al.(2023)은 자체 감독 사전 훈련 및 다운스트림 작업 미세 조정 패러다임에서 RC 시퀀스를 사용하여 탐색한다. 그러나, 그들의 모델은 인코더가 주어진 배치에서 RC 시퀀스의 임베딩을 인식하도록 트레이닝되는 사전 트레이닝 동안 대조적 학습을 사용한다.\n' +
      '\n' +
      '### Bi-directional RNNs\n' +
      '\n' +
      '대규모 데이터 세트에 대한 사전 훈련을 위한 양방향성을 이용하는 것은 ELMo(Peters et al., 2017)에서 처음 실현되었으며, 여기서 전후방 LSTM(Hochreiter and Schmidhuber, 1997)이 언어 컨텍스트를 모델링하기 위해 동시에 활용되었다. 이는 순환 네트워크를 트랜스포머 백본으로 대체한 BERT(Devlin et al., 2018)와 같은 모델의 토대를 마련하였다. 최근 Wang et al.(2022)은 SSM을 이용한 BERT-style 트레이닝을 탐색하였다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 연구에서는 Mamba 모듈에 양방향 및 RC 등분산 시퀀스 모델링을 가능하게 하는 아키텍처 혁신을 도입하였다. 또한 새로운 DNA 기초 모델인 카두세우스를 제안하고 생물학적 관련 작업 범위에서 비교적 크기가 큰 단방향 하이에나 기반 모델과 트랜스포머 기반 모델을 능가하는 능력을 입증하여 유전자 발현에 대한 유전자 돌연변이의 영향을 가장 두드러지게 예측한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '이 작업은 NSF CAREER 보조금(#2145577)과 NIH MIRA 보조금(#1R35GM151243-01)에 의해 지원되었다. 또한 사전 훈련 실험의 일부 컴퓨팅 리소스를 제공한 뉴클레오타이드 트랜스포머 리더보드와 모자이크ML에 대한 유용한 토론에 대해 에반 트로프와 인스타 딥 팀에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. ArXiv:2303.08774, 2023.\n' +
      '* Alipanahi et al. (2015) Alipanahi, B., Delong, A., Weirauch, M. T., and Frey, B. J. Predicting sequence specificity of dna-and rna-binding proteins by deep learning. Nature biotechnology33(8):831-838, 2015.\n' +
      '* Avsec et al. (2021) Avsec, Z., Agarwal, V., Visentin, D., Ledesam, J. R., Grabska-Barwinska, A., Taylor, K. R., Assael, Y., Jumper, J., Kohli, P., and Kelley, D. R. 장거리 상호작용을 통합함으로써 서열로부터 효과적인 유전자 발현 예측을 한다. Nature methods18(10):1196-1203, 2021.\n' +
      '* Benegas et al. (2023a) Benegas, G., Albors, C., Aw, A. J., Ye, C., and Song, Y. S. Gpn-msa: a alignment-based dna language model for genome-wide variant effect prediction. bioRxiv, 2023a.\n' +
      '* Benegas et al. (2023b) Benegas, G., Batra, S. S., and Song, Y. S. Dna language models is powerful predictors of genome-wide variant effects. 미국 국립과학원120(44):e2311219120, 2023b.\n' +
      '* Benegas et al. (2023c) Benegas, G., Batra, S. S., and Song, Y. S. Dna language models is powerful predictors of genome-wide variant effects. 미국 국립과학원120(44):e2311219120, 2023c.\n' +
      '* Blelloch (1990) Blelloch, G. E. Prefix sums and their applications. 1990년\n' +
      '* Blelloch(1991)Cao, Z. 및 장승 컨볼루션 신경망 구조의 간단한 트릭은 dna-단백질 결합 예측을 향상시킨다. _ Bioinformatics_, 35(11):1837-1843, 2019.\n' +
      '* Consortium et al. [2009] Consortium, G. R. et al. Genome reference consortium human build 37 (grch37). _Database (GenBank or RefSeq)_, 2009.\n' +
      '* Dalla-Torre et al. [2023] Dalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Carranza, N. L., Grzywaczewski, A. H., Oteri, F., Dallago, C., Trop, E., de Almeida, B. P., Sireklkatim, H., et al. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. _bioRxiv_, pp. 2023-01, 2023.\n' +
      '* Dao et al. [2022] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language modeling with state space models. _arXiv preprint arXiv:2212.14052_, 2022.\n' +
      '* Devlin et al. [2018] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* 팔콘[2019] 팔콘, W. 파이토치 라이트닝 팀도요 PyTorch Lightning, March 2019. URL[https://github.com/Lightning-AI/lightning](https://github.com/Lightning-AI/lightning).\n' +
      '* Fishman et al. [2023] Fishman, V., Kuratov, Y., Petrov, M., Shmelev, A., Shepelin, D., Chekanov, N., Kardymon, O., and Burtsev, M. Gena-lm: A family of open-source foundational models for long dna sequences. _bioRxiv_, pp. 2023-06, 2023.\n' +
      '* Furlong and Levine [2018] Furlong, E. E. M. and Levine, M. 발달 촉진제와 염색체 위상. _ Science_, 361(6409):1341-1345, 2018. doi: 10.1126/science.aau0320. URL[https://www.science.org/doi/abs/10.1126/science.aau0320](https://www.science.org/doi/abs/10.1126/science.aau0320)을 포함할 수 있다.\n' +
      '* Geng et al. [2022] Geng, Q., Yang, R., and Zhang, L. A deep learning framework for enhancer prediction using word embedding and sequence generation. _Biophysical Chemistry_, 286:106822, 2022.\n' +
      '* Gresova et al. [2023] Gresova, K., Martinek, V., Cechak, D., Simecek, P., and Alexiou, P. Genomic benchmarks: a collection of datasets for genomic sequence classification. _BMC Genomic Data_, 24(1):25, 2023.\n' +
      '* 구와 도[2023] 구, A.와 도, T. Mamba: 선택적 상태 공간을 갖는 선형-시간 시퀀스 모델링 _ arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* Gu et al. [2021a] Gu, A., Goel, K., and Re, C. Efficiently modeling long sequence with structured state spaces. _ arXiv preprint arXiv:2111.00396_, 2021a.\n' +
      '* Gu et al. [2021b] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _ 신경 정보 처리 시스템_, 34:572-585, 2021b에서의 발전.\n' +
      '* Gu et al. [2022] Gu, A., Goel, K., Gupta, A., and Re, C. On the parameterization and initialization of diagonal state space models. _Advances in Neural Information Processing Systems_, 35:35971-35983, 2022.\n' +
      '* Gunduz et al. [2022] Gunduz, H. A., Binder, M., To, X.-Y., Mreches, R., Bischl, B., McHardy, A. C., Munch, P. C., and Rezaei, M. A self-supervised deep learning method for data-efficient training in genomics. _Communications Biology_, 6(1):928, 2023.\n' +
      '* Gupta et al. [2022] Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. _Advances in Neural Information Processing Systems_, 35:22982-22994, 2022.\n' +
      '* Harris et al. [2020] Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Rio, J. F., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020. doi: 10.1038/s41586-020-2649-2. URL [https://doi.org/10.1038/s41586-020-2649-2](https://doi.org/10.1038/s41586-020-2649-2).\n' +
      '* Hochreiter & Schmidhuber [1997] Hochreiter, S. and Schmidhuber, J. Long shortterm memory. _ Neural computation_, 9(8):1735-1780, 1997.\n' +
      '* Hochreiter & Schmidhuber[1997]Hunter, J. D. Matplotlib: 2d 그래픽 환경. _ Computing in Science & Engineering_, 9(3):90-95, 2007. doi: 10.1109/MCSE.2007.55.\n' +
      '* Ji et al. (2021) Ji, Y., Zhou, Z., Liu, H., and Davuluri, R. V. Dnabert: pretrained bidirectional encoder representations from transformerers model for dna-language in genome. _ Bioinformatics_, 37(15):2112-2120, 2021.\n' +
      '* Jumper et al. (2021) Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Zidek, A., Potapenko, A., et al. Nature_, 596(7873):583-589, 2021.\n' +
      '* Kingma & Ba(2014) Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Lin et al. (2023) Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., et al. Evolutionary-scale prediction of atomic-level protein structure with language model. _ Science_, 379(6637):1123-1130, 2023.\n' +
      '* Madani et al. (2023) Madani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., Olmos Jr, J. L., Xiong, C., Sun, Z. Z., Socher, R., et al. Large language models generates functional protein sequences across various family. _ Nature Biotechnology_, pp. 1-8, 2023.\n' +
      '* Mallet & Vert (2021) Mallet, V. 및 Vert, J.-P. dna 시퀀스에 대한 역보완 등분산 네트워크. _ 신경 정보 처리 시스템_, 34:13511-13523, 2021에서의 발전.\n' +
      '* Martin & Cundy (2017) Martin, E and Cundy, C. Parallelizing linear recurrent neural net over sequence length. _ ArXiv:1709.04057_, 2017.\n' +
      '* Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes, C., Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., et al. Hypandna: Long range genomic sequence modeling at single nucleotide resolution. _ arXiv preprint arXiv:2306.15794_, 2023.\n' +
      '* Oubounyt et al. (2019) Oubounyt, M., Louadi, Z., Tayara, H., and Chong, K. T. Deepromoter: robust promoter predictor using deep learning. _ 2019년 10시 286분 유전학 분야\n' +
      '* 판다 개발팀(2020) 판다 개발팀, T. pandas-dev/pandas: Pandas, February 2020. URL[https://doi.org/10.5281/zenodo.3509134](https://doi.org/10.5281/zenodo.3509134).\n' +
      '* Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Kimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: Imperative Style, 고성능 딥러닝 라이브러리. Wallach, H., Larochelle, H., Beygelzimer, A., d\'Alche Buc, F., Fox, E., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems 32_, pp. 8024-8035. Curran Associates, Inc., 2019.\n' +
      '* Pedregosa et al. (2011) Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. _ Journal of Machine Learning Research_, 12:2825-2830, 2011.\n' +
      '* Peters et al. (2017) Peters, M. E., Ammar, W., Bhagavatula, C., and Power, R. 양방향 언어 모델을 가진 준감독 시퀀스 태깅 ArXiv:1705.00108_, 2017.\n' +
      '* Phaml et al. (2005) Phaml, T. H., Tran, D. H., Ho, T. B., Satou, K., and Valiente, G. Qualitatively predicting acetylation and methylation areas in dna sequences. _ Genome Informatics_, 16(2):3-11, 2005.\n' +
      '* Quang & Xie (2019) Quang, D. and Xie, X. Factornet: nucleotide-resolution sequential data로부터 cell type specific transcription factor binding을 예측하는 딥러닝 프레임워크. _ Methods_, 166:40-47, 2019.\n' +
      '* Ramachandran et al. (2017) Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. _ arXiv preprint arXiv:1710.05941_, 2017.\n' +
      '* Ramachandran et al. (2018)Rao, R., Meier, J., Sercu, T., Ovchinnikov, S., and Rives, A. Transformer protein language models is unsupervised structure learners. _ Biorxiv_, pp. 2020-12, 2020.\n' +
      '* Rives et al. (2021) Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 2500만 단백질 sequence. _ Proceedings of the National Academy of Sciences_, 118(15):e2016239118, 2021.\n' +
      '* Scalzitti et al. (2021) Scalzitti, N., Kress, A., Orhand, R., Weber, T., Moulinier, L., Jeannin-Girardon, A., Collet, P., Poch, O., and Thompson, J. D. Spliceator: convolution neural networks를 이용한 Multi-species splice site prediction. _ BMC bioinformatics_, 22(1):1-26, 2021.\n' +
      '* Shrikumar et al. (2017) Shrikumar, A., Greenside, P., and Kundaje, A. Reverse-complement parameter sharing improves deep learning models for genomics. _ BioRxiv_, pp. 103663, 2017.\n' +
      '* Smith et al. (2022) Smith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. _ arXiv preprint arXiv:2208.04933_, 2022.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: Family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tillet et al. (2019) Tillet, P., Kung, H.-T., and Cox, D. Triton: a intermediate language and compiler for tileed neural network computationations. In _Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, pp. 10-19, 2019.\n' +
      '* Trop et al. (2023) Trop, E., Kao, C.-H., Polen, M., Schiff, Y., de Almeida, B. P., Gokaslan, A., Pierrot, T., and Kuleshov, V. DNA 언어 모델 발전: 유전체학 장거리 벤치마크. _LLMs4Bio Workshop_, 2023.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wang et al. (2020) Wang, G., Sarkar, A., Carbonetto, P., and Stephens, M. 유전자 미세 매핑에 적용하여 회귀 분석에서 변수 선택에 대한 간단한 새로운 접근법 _ Journal of the Royal Statistical Society Series B: Statistical Methodology_, 82(5):1273-1300, 2020.\n' +
      '* Wang et al. (2022) Wang, J., Yan, J. N., Gu, A., and Rush, A. M. Pretraining without attention. _ ARXiv 프리프린트 arXiv:2212.10544_, 2022.\n' +
      '* Wang et al. (2019) Wang, R., Wang, Z., Wang, J., and Li, S. Splicefinder: ab initio prediction of splice site using convolutional neural network. _ BMC 생물정보학_, 20:1-13, 2019.\n' +
      '* Waskom(2021) Waskom, M. L. seaborn: 통계 데이터 시각화_ Journal of Open Source Software_, 6(60):3021, 2021. doi: 10.21105/joss.03021. URL[https://doi.org/10.21105/joss.03021](https://doi.org/10.21105/joss.03021).\n' +
      '* Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface\'s transformerers: State-of-the-art natural language processing. _ ArXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* 복잡한 응용 프로그램을 우아하게 구성하기 위한 프레임워크. Github, 2019. URL[https://github.com/facebookresearch/hydra](https://github.com/facebookresearch/hydra].\n' +
      '* Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: 긴 시퀀스에 대한 트랜스포머. _ 신경 정보 처리 시스템들_, 33:17283-17297, 2020에서의 발전들.\n' +
      '* Zhou et al. (2021) Zhou, H., Shrikumar, A., and Kundaje, A. Towards the better understanding of reverse-complement equivariance for deep learning models in regulatory genomics. _ BioRxiv_, pp. 2020, 2021.\n' +
      '* Zhou & Troyanskaya (2015) Zhou, J. and Troyanskaya, O. G. Predicting effects of noncoding variants with deep learning based sequence model. _ Nature methods_, 12(10):931-934, 2015.\n' +
      '* Zhou et al. (2023) Zhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R., and Liu, H. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. _ arXiv preprint arXiv:2306.15006_, 2023.\n' +
      '* Zhou et al. (2020)Proof of Theorem 3.1\n' +
      '\n' +
      '우리는 RC 등분산 맘바 모듈을 구성하는 다양한 함수의 정의를 반복하는 것으로 시작한다. 길이\\(T\\), \\(D\\) 채널들의 입력 시퀀스\\(\\mathbf{X}_{1:T}^{1:D}\\)에 대해, 우리는 다음과 같이 정의한다.\n' +
      '\n' +
      '\\mathbf{X}_{1:T}^{1:D}) :=\\left[\\mathbf{X}_{1:T}^{1:D}\\right], \\mathbf{X}_{1:T}^{1:D}\\right] :=\\mathbf{X}_{1:T}^{1:D}\\right(\\mathbf{X}_{1:T}^{1:D}\\right) :=\\mathbf{X}_{1:T}^{1:D}\\right(\\mathbf}_{X}_{1:T}^{1:D}\\right)\n' +
      '\n' +
      '우리는 또한 채널 차원을 따라 \'분할\'되는 시퀀스에 RC 연산의 적용을 다음과 같이 나타낸다.\n' +
      '\n' +
      '\\left[\\mathbf{X}_{1:T}^{1:(D/2):D}\\right]\\right):=\\left[\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right),\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(1:D/2):1}\\right)=\\left[\\mathbff{X}_{T:1}^{(D/2):1}\\right],\\tag{9}\\right]\n' +
      '\n' +
      'RC 동작은 콘케이트 동작의 \'내부에 풀링\'될 수 있다는 것에 유의한다:\n' +
      '\n' +
      '{concat}\\left(\\left[\\mathbf{X}_{1:T}^{1:(D/2):D}\\right)},\\mathbf{X}_{1:T}^{1:D}\\right) =\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:(D/2):D}\\right)}\\tag{10}\\[=\\operatorname{concat}\\left(\\left[\\mathbf{X}_{1:T}^{1:(D/2):D}\\right)}\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:(D/2):D}\\right]\\right)\\\\operatorname{concat}\\left(\\mathbff{X}_{1:T}^{1:(D/2):D}\\right)\\cat}\\left(\\mathbff{X}_{1:T}^{1:(D\n' +
      '\n' +
      '또한, 우리는 \\(\\operatorname{RC}^{-1}=\\operatorname{RC}\\)과 \\(\\operatorname{RC}^{-1}=\\operatorname{RC}\\)을 가진다.\n' +
      '\n' +
      '\\left[\\mathbf{X}_{1:T}^{1:(D/2)},\\operatorname{RC} \\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right)\\right)=\\left[\\mathbff{X}_{1:T}^{(D/2):D},\\operatorname{RC}\\left(\\mathbff{X}_{1:T}^{1:(D/2)}\\right)\\left[\\mathbffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff\n' +
      '\n' +
      '정의 8에 이어서, 우리는 그것을 가지고 있다:\n' +
      '\n' +
      '(\\mathbf{X}{1:T}^{1:D}\\right) =\\operatorname{RC}\\circatorname{M}_{\\theta}\\circatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:(D/2):D}\\right)\\curcat}\\left(\\left[\\operatorname{M}_{\\theta}\\circatorname{M}_{1:T}^{1:(D/2):D}\\right)\\curcat}\\left(\\left[\\operatorname{M}_{\\theta}\\circatorname{M}_{1:T}^{1:(D/2):D}\\right)\\curcat}\\left(\\left[\\operatorname{M}_{\\theta}\\circatorname{M}_{1:T}^{1:(D/2):D}\\right)\\right)\\curcat}\\left(\\left[\\operatorname{M}_{\\theta}\\circatorname{\n' +
      '\n' +
      '## 정리의 부록 B 증명 4.1\n' +
      '\n' +
      '다음 리마부터 시작하죠\n' +
      '\n' +
      '*Lemma B.1**.: _2개의 RC 등분산 시퀀스 연산자\\(\\mathrm{F}\\) 및 \\(\\mathrm{G}\\)에 대해, 이들의 구성\\(\\mathrm{F}\\circ\\mathrm{G}\\)도 등분산이다._\n' +
      '\n' +
      '._Proof.__ 우린 그걸 가지고 있어\n' +
      '\n' +
      '\\mathrm{F}\\left(\\mathrm{G}\\left(\\mathrm{G}\\left(\\mathrm{X}_{1:T}^{1:D}\\right)\\right))\\right)=\\mathrm{F}\\left(\\mathrm{G}\\left(\\mathrm{G}\\left(\\mathrm{X}_{1:T}^{1:D}\\right)\\right)\\mathrm{F}\\left(\\mathrm{G}\\left(\\mathrm{X}_{1:T}^{1:D}\\right)\\right)\\mathrm{F}\\left(\\mathrm{G}\\left(\\mathrm{X}_{1:T}^{1:D}\\right)\\right)\\right)\\mathrm{F}\\left(\\mathrm{G}\\left(\\mathrm{X}_{1:T}^{1:D}\\right)\\right)\\right)\\mathrm{F}\\left(\\mathrm{G}\\left(\n' +
      '\n' +
      '여기서 각 등식은 연산자 \\(\\mathrm{G}\\)와 \\(\\mathrm{F}\\)의 RC 등식에서 각각 따른다.\n' +
      '\n' +
      '따라서 Caduceus-PS가 RC equivariant임을 증명하기 위해서는 \\(\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\circ\\mathrm{M}_{\\mathrm{RC}_{e},\\theta}^{(n)}\\circ\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{Emb}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm{M}_{e},\\theta}\\circ\\mathrm\n' +
      '\n' +
      '먼저, \\(\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\)이 RC 등분산임을 보인다.\n' +
      '\n' +
      '\\mathrm{Emb}_{\\theta}_{T:1}^{4}\\right),\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_{T:1}^{1:4}\\right),\\mathrm{Emb}\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_{T:1}^{1:4}\\right),\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm{Emb}\\mathrm\n' +
      '\n' +
      '또한, \\(\\mathrm{M}_{\\mathrm{RC}_{e},\\theta}^{(n)}\\)은 정리 3.1에 의해 등분산이고, Lemma B.1을 사용하여 유도된다.\n' +
      '\n' +
      '마지막으로 \\(\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\)의 정의를 상기한다:\n' +
      '\n' +
      '\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):= \\mathrm{LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right)+\\text{flip\\_chan}\\circ\\mathrm{LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{D:(D/2)}\\right))\n' +
      '\n' +
      '\\(\\mathrm{LM}_{\\theta}\\)은 가중치 행렬 \\(\\mathbf{W}_{\\theta}\\)에 의해 매개변수화되며, \\(\\mathbf{X}_{1:T}^{1:(D/2)}\\)에 \\(\\mathbf{X}_{t}^{1:(D/2)}\\)을 적용하면 \\(\\mathbf{x}_{t}^{1:(D/2)}\\), \\(t=1,\\ldots,T\\)에 대해 왼쪽에 \\(\\mathbf{W}_{\\theta}\\)을 곱하는 것과 같다. 따라서 입력값을 길이 차원에 따라 \\(\\mathrm{LM}_{\\theta}\\)으로 되돌리면 출력값도 길이 차원에 따라 반전된다. 따라서 우리는 수열에서 위치 \\(t\\)의 특정 아이템에 초점을 맞출 수 있다:\n' +
      '\n' +
      '\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)_{t}= \\mathbf{W}_{\\theta}\\cdot\\mathbff{x}_{t}^{1:(D/2)}+\\text{flip\\_chan}\\left(\\mathbf{W}_{\\theta}\\cdot\\mathbf{x}_{t}^{D:(D/2)}\\right),\\mathbff{x}_{t}^{D:(D/2)}\\right)\n' +
      '\n' +
      '그리고 우리는 단지 주어진 입력의 채널들을 뒤집을 뿐인 플립_찬 연산과 동일하다는 것을 보여줄 필요가 있다. 우리는 \\(\\text{flip\\_chan}^{-1}=\\text{flip\\_chan}\\)에 주목한다. 이제 그걸 보여드리죠\n' +
      '\n' +
      '\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)_{t}\\right) =\\text{flip\\chan}\\left(\\mathbf{W}_{\\theta}\\cdot\\mathbf{x}_{t}^{1:(D/2)}\\mathbff{x}_{t}^{D:(D/2)\\\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\text{x}_{1:T}^{1:D}\\right)_{t}\\right)\\text{flip\\chan}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)_{t}\\left(\\mathbff{x}_{t}^{1:(D/2)}\\mathm{LM}_{\\mathrm{RC}_{e},\n' +
      '\n' +
      '이로써 증명이 완성된다.\n' +
      '\n' +
      '## 부록 C 사전교육\n' +
      '\n' +
      '인간 참조 유전체 사전 훈련 작업에 사용되는 데이터 세트 및 훈련 방법론에 대한 보다 자세한 설명을 제공한다. 이 데이터 세트는 이전 Enformer 연구에서 사용된 분할을 기반으로 한다(Awsec et al., 2021). 훈련 분할은 최대 길이 1,048,576(\\(2^{20}\\))으로 확장되는 34,021개의 세그먼트로 구성되며, 집합적으로 유전체를 덮고 약 350억 개의 토큰 또는 뉴클레오티드 염기쌍에 달한다.\n' +
      '\n' +
      'Caduceus를 포함한 모든 Mamba 기반 모델은 학습률 8e\\({}^{-3}\\)으로 학습되었다. 우리는 배치당 \\(2^{20}\\)개의 토큰을 사용하여 각 배치에서 일정한 수의 토큰을 유지한다. 예를 들어, 시퀀스 길이 1,024의 경우 배치 크기도 1,024이고 시퀀스 길이 131k(\\(2^{17}\\))의 경우 배치 크기는 8이다. Caduceus-PS를 제외한 모든 모델은 RC 데이터 증강으로 사전 훈련되며, 여기서 주어진 시퀀스는 변경되지 않거나 동일한 확률로 RC 연산이 적용된다.\n' +
      '\n' +
      '모델은 코사인 감쇠와 ADAM 최적화 알고리즘(Kingma and Ba, 2014), \\(\\beta_{1}\\) 및 \\(\\beta_{2}\\) 값으로 각각 0.95 및 0.9로 훈련되었다.\n' +
      '\n' +
      '양방향 모델의 경우 Devlin et al.(2018)에 제시된 마스킹 레시피를 사용한다. 즉, \'ask\' 토큰의 15%를 \'ask\' 토큰 중 80%는 특수 [MASK] 토큰으로 대체되고, 10%는 어휘에서 무작위 토큰으로 대체되며, 10%는 변경되지 않은 채로 남겨진다.\n' +
      '\n' +
      '사전 훈련된 다양한 맘바/카두서스 모델은 표 3에 나열되어 있다. 그림 2(a)의 경우 1,024, 32k 및 131k의 서열 길이에 대해 하이에나DNA 모델을 재훈련한다. 우리는 이러한 모델들이 원래 Nguyen 등(2023)에서 훈련되었을 때 사용된 것과 같이 대응하는 숨겨진 차원 및 깊이를 사용한다. 6e\\({}^{-4}\\)으로 설정된 학습률 외에 위의 모델에 사용된 다른 모든 사전 훈련 세부 정보는 HyenaDNA 사전 훈련에도 사용되었다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '###뉴클레오티드 트랜스포머 작업\n' +
      '\n' +
      '뉴클레오티드 트랜스포머 태스크의 경우 [https://huggingface.co/spaces/InstaDeepAI/nucleotide_transformer_benchmark](https://huggingface.co/spaces/InstaDeepAI/nucleotide_transformer_benchmark)에서 기준 결과를 도출한다. 우리의 카듀서스/맘바 기반 모델들에 대해, 우리는 각각의 폴드에 대해 90/10 열차/검증 분할을 사용하는 댈라-토레 등(2023)으로부터의 동일한 CV 프로토콜을 따른다. 우리의 모델은 보고된 HyenaDNA 모델의 매개변수 수와 대략 일치하는 4개의 레이어와 숨겨진 차원 256으로 구성된다. 모델들은 20년 동안 미세 조정되었다. 표 2에 보고된 모델에 대한 하이퍼파라미터는 표 6에서 찾을 수 있다.\n' +
      '\n' +
      '변이가 유전자 발현에 미치는 영향 예측###\n' +
      '\n' +
      '이 작업에 대한 라벨은 SNP가 유전자 발현에 인과적 영향을 미치는지 여부를 나타낸다. SuSiE(Wang et al., 2020) 툴에 의해 결정된 인과 확률이 \\(>.9\\)이면 양의 레이블이 할당된다(Avsec 참조).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{Caduceus-Ph} & \\multicolumn{2}{c}{Caduceus-PS} \\\\  & LR & batch size & LR & batch size \\\\ \\hline \\multirow{9}{*}{Histone markers} & H3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k14ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k36me3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k4me1 & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k4me2 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & H3k79me3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3K9ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H4 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H4ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\ \\hline \\multirow{9}{*}{Regulatory annotation} & Enhancers & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & Enhancers types & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(2\\mathrm{e}^{-3}\\) & 512 \\\\  & Promoter all & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Promoter no tata & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Promoter data & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\ \\hline \\multirow{2}{*}{Splice site} & Splice sites acceptors & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Splice sites all & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & Splice sites donors & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 뉴클레오타이드 트랜스포머 작업에 대한 MambaDNA MLM 및 Caduceus 하이퍼파라미터 선택. 최상의 성능을 기반으로 선택한 카두서스-Ph 및 카두서스-PS 미세 조정 하이퍼파라미터는 평균 10배 교차 검증에 걸쳐 평균을 냈다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Mamba & Caduceus w/o Equiv. & Caduceus-Ph & Caduceus-PS \\\\ \\hline Mouse Enhancers & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Coding vs. Intergenomic & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human vs. Worm & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human Enhancers Cohn & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human Enhancer Ensembl & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human Regulatory & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human OCR Ensembl & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human NonTATA Promoters & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 게놈 벤치마크에 대한 Mamba/Caduceus Hyperparameter Selection. 상위 1개의 정확도로 선택된 학습률은 평균 5배 교차 검증에 걸쳐 평균을 냈다.\n' +
      '\n' +
      '이 작업이 원래 제안된 al. (2021)에서 더 자세한 내용을 위해 제안되었습니다. 염색체 9 및 10은 홀드 아웃 테스트 세트로 사용된다(더 자세한 내용은 Trop et al.(2023) 참조).\n' +
      '\n' +
      'Trop et al.(2023)에 제시된 방법론을 따르고 참조 및 대체 서열 모두에 대해 SNP 위치에 중심을 둔 평균 257bp 창을 취하고 채널 차원을 따라 연결하여 각 모델에 대한 임베딩을 추출한다. 각 임베딩에 우리는 또한 서열이 분석된 조직을 연결한다.\n' +
      '\n' +
      '그런 다음 데이터의 각 지층에 대해 이러한 임베딩에 RBF 커널이 있는 SVM 분류기를 훈련하며, 이는 가장 가까운 TSS까지의 거리에 의해 분리된다. TSS까지의 거리의 각 버킷에 대해 5,000개의 훈련 포인트를 무작위로 선택하고 분류기에 적합하며 테스트 세트 AUROC를 기록한다. 이 과정을 5회 반복하고 종자에 걸쳐 평균 및 표준 편차 +/-를 보고한다.\n' +
      '\n' +
      '정규화 강도를 중심으로 각 거리 범주 내의 각 모델에 대해 하이퍼파라미터 최적화를 수행하였다. 우리는 5개의 무작위 씨앗에서 보고된 최고 평균 AUROC를 기반으로 이 하이퍼모수를 선택한다. 그림 4에 보고된 각 모델에 사용된 정규화 강도는 표 7에 나열되어 있다.\n' +
      '\n' +
      '## 부록 E 자산\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '사전 훈련을 위해 우리는 HG38 인간 참조 게놈을 사용한다(Consortium et al., 2009). Genomics Benchmark는 Gresova et al.(2023)로부터 유래한다. 뉴클레오티드 트랜스포머 벤치마크는 Dalla-Torre 등에 소개되어 있다(2023). 변형 효과 예측 태스크 데이터는 Avsec et al. (2021)에서 원래 제안되었으며, 우리는 Trop et al. (2023)의 수정된 버전을 사용한다.\n' +
      '\n' +
      '### 소프트웨어 및 라이브러리\n' +
      '\n' +
      '표 8에서 이 작업에 사용된 관련 오픈 소스 소프트웨어 및 해당 라이선스를 열거한다.\n' +
      '\n' +
      '## 부록 F 연산 자원\n' +
      '\n' +
      '모델 훈련 및 추론은 사전 훈련 및 다운스트림 작업 동안 모델 크기에 따라 다양한 기계 유형을 가진 GPU에서 실행되었다. 우리는 3090, A5000, A6000, V100 및 A100 GPU를 사용합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\multicolumn{2}{c}{Distance to Nearest TSS (bp)} \\\\  & \\(0-30\\text{k}\\) & \\(30-100\\text{k}\\) & \\(100\\text{k}+\\) \\\\ \\hline NTv2 (500M) & 1 & 1 & 10 \\\\ HyenaDNA (6.6M) & 1 & 5 & 1 \\\\ Caduceus w/o Equiv (7.7M) & 1 & 5 & 10 \\\\ Caduceus-Ph (7.7M) & 1 & 5 & 1 \\\\ Caduceus-PS (7.7M) & 1 & 1 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: Variant Effect Prediction Task에서 SVM 분류기를 위한 Hyperparameter Selection. 평균 테스트 세트 AUROC를 평가하여 \\(\\{1,5,10\\}\\)에서 선택된 \\(L_{2}\\) 정규화 가중치를 역한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Library & License \\\\ \\hline GenomicsBenchmark (Gresova et al., 2023) & Apache 2.0 \\\\ Mamba (Gu and Dao, 2023) & Apache 2.0 \\\\ HuggingFace (Wolf et al., 2019) & Apache 2.0 \\\\ Hydra (Yadan, 2019) & MIT \\\\ HyenADAN (Nguyen et al., 2023) & Apache 2.0 \\\\ NumPy (Harris et al., 2020) & NumPy license \\\\ Matplotlib (Hunter, 2007) & Matplotlib license \\\\ ML Collections & Apache 2.0 \\\\ OmegaConv & BSD 3-Clause \\\\ Pandas (pandas development team, 2020) & BSD 3-Clause “New” or “Revised” \\\\ PyTorch (Paszke et al., 2019) & BSD-3 Clause \\\\ PyTorch Lightning (Falcon and The PyTorch Lightning team, 2019) & Apache 2.0 \\\\ Scikit-Learn (Pedregosa et al., 2011) & BSD 3-Clause \\\\ Seaborn (Waskom, 2021) & BSD 3-Clause “New” or “Revised” \\\\ Triton (Tillet et al., 2019) & MIT \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 이 작업에 사용된 오픈 소스 라이브러리(및 해당 라이선스)입니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>