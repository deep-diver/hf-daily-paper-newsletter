<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Caduceus: Bi-Directional Equivariant Long-Range\n' +
      '\n' +
      'DNA Sequence Modeling\n' +
      '\n' +
      'Yair Schiff\n' +
      '\n' +
      'Corresponding author: yzs2@cornell.edu Cornell University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      'Chia-Hsiang Kao\n' +
      '\n' +
      'Cornell University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      'Aaron Gokaslan\n' +
      '\n' +
      'Cornell University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      'Tri Dao\n' +
      '\n' +
      'Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      'Albert Gu\n' +
      '\n' +
      'Carnegie Mellon University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      'Volodymyr Kuleshov\n' +
      '\n' +
      'Cornell University, \\({}^{2}\\)Princeton University, \\({}^{3}\\)Carnegie Mellon University\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Momba block, and extends it to a BiMamba component that supports bi-directionality, and to a MamboDNA block that additionally supports RC equivariance. We use MamboDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance. Code to reproduce our experiments is available here.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large-scale sequence models have sparked rapid progress in machine learning, bringing about advances that extend beyond natural language processing (NLP) (Achiam et al., 2023; Team et al., 2023) into science, biology, and medicine. In proteomics, these models have enabled predicting protein structures from sequences (Jumper et al., 2021; Lin et al., 2023), deciphering the functions and interactions of amino acids (Rao et al., 2020; Rives et al., 2021), and crafting new molecules (Madani et al., 2023). As compute cost decreases, sequence modeling is poised to further impact biology.\n' +
      '\n' +
      'Sequence models are also standard tools in genomics (Zhou and Troyanskaya, 2015; Avsec et al., 2021). Unlike proteins, genomes contain non-coding sequences, which often play an important role in regulating cellular mechanisms, and can thus potentially provide greater insights into cell biology. Understanding non-coding sequences has been a key focus of recent work, including efforts in applying large language models (LMs) to genomes (Ji et al., 2021; Benegas et al., 2023; Dalla-Torre et al., 2023; Nguyen et al., 2023).\n' +
      '\n' +
      'However, modeling DNA introduces challenges that are distinct from those posed by natural language or proteins. First, cellular phenotypes are often impacted by base pairs both upstream and downstream in the genome, which requires sequence models to handle bi-directional context. Second, DNA consists of two strands that are reverse complements of each other and that carry the same information; modeling this property can significantly improve performance (Zhou et al., 2021; Mallet and Vert, 2021). Third, many genomics tasks, such as predicting the effect of variants on gene expression, can entail long-range interactions, as nucleic acids even up to 1 million base pairs away from a gene can have significant regulatory effects (Furlong and Levine, 2018).\n' +
      '\n' +
      'In this paper, we propose architectural components motivated by the above challenges. Our modules build off the long-range Momba block (Gu and Dao, 2023) and thus naturally handle long sequences ofover hundreds of thousands of nucleotides without the quadratic computation cost of attention-based architectures (Vaswani et al., 2017). We extend Mampa to BiMamba, a component that supports bi-directionality, and to MampaDNA, which further adds reverse complement (RC) equivariance. The MampaDNA block can be used as a drop-in replacement in architectures for genome analysis in both supervised and self-supervised contexts.\n' +
      '\n' +
      'We then use MampaDNA as the basis of Caduceus1, a family of bidirectional long-range DNA sequence models that is the first to support RC equivariant language modeling. We further introduce pre-training and fine-tuning strategies that yield Caduceus foundation models for a wide range of predictive tasks in genomics. The Caduceus models consistently outperform previous SSM-based models of a similar size (Nguyen et al., 2023) in terms of downstream performance. On many tasks, especially ones that require long-range modeling, Caduceus also outperforms 10x larger Transformer-based models.\n' +
      '\n' +
      'Footnote 1: Caduceus (\\(\\top\\)) is the staff carried by Hermes in Greek mythology that is adorned by two intertwined sernents. We choose this name to evoke imagery of the double helix structure of DNA and to symbolize bi-directionality using a Mampa sequence operator.\n' +
      '\n' +
      'We use Caduceus to perform variant effect prediction (VEP), a task that seeks to determine whether a genetic mutation influences a phenotype--gene expression in our case. This task is a natural fit for Caduceus because its pre-training implicitly learns to recognize the effects of evolutionary pressure (e.g., conservation, co-evolution), which is a key source of signal for VEP (e.g., a mutation in a region where mutations are rare likely has an effect and a low probability under the model). On a task derived from a standard dataset of mutations with long-range effects on gene expression (Avscc et al., 2021), Caduceus outperforms by a large margin existing attention and SSM-based models that do not leverage both bi-directionality and equivariance.\n' +
      '\n' +
      'ContributionsTo summarize, our contributions are:\n' +
      '\n' +
      '1. We introduce BiMamba, a parameter and hardware efficient extension of the Mampa block that supports bi-directional sequence modeling.\n' +
      '2. We extend BiMamba to support RC equivariance, which yields the MampaDNA block, a general component for deep learning architectures in genomics.\n' +
      '\n' +
      'Figure 1: Mampa modules for genomic sequences. _(Left)_**Mampa**: The original left-to-right causal Mampa module proposed in Gu and Dao (2023). _(Middle)_**BiMamba**: A parameter efficient bi-directional extension of the Mampa module. In-projection and out-projection parameters are shared for processing the sequence and its reverse. After processing the reversed sequence, it is flipped again and added to the forward output. _(Right)_**Reverse complement equivariant Mampa (MampaDNA)**: A module with RC equivariance inductive bias. The input is first split into two along the channel dimension. One split has the reverse complement (RC) operation applied to it. All the parameters of a Mampa module are shared for processing the forward and RC sequence. The reverse sequence has the RC applied once more before being concatenated back with the forward output along the channel dimension.\n' +
      '\n' +
      ' 3. We use MamboDNA as the basis of Caduceus, the first family of RC-equivariant DNA foundation models.\n' +
      '4. We demonstrate that on long-range tasks, Caduceus outperforms models that are up to 10x larger but that do not use bi-directionality or equivariance.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### DNA Terminology\n' +
      '\n' +
      '_Deoxyribonucleic acid_ (DNA) is a polymer that is made up two complementary strands that wind in ladder / double-helix manner and is comprised of four _nucleotide_ bases: _adenine_ (A), _cytosine_ (C), _guanine_ (G) or _thymine_ (T). The bonds between the nucleotide bases form \'rungs\' on the twisted ladder, with A bonding with T and C bonding with G. DNA contains the genetic code for forming proteins. In complex organisms, DNA can be billions of nucleotide base pairs (bps) long, but the long strands coil tightly around proteins in the nucleus called _histones_.\n' +
      '\n' +
      'Genetic mutations at individual bps, known as _single nucleotide polymorphisms_ (SNPs) can account for phenotypic variation across organisms. Evolutionary pressure has forced several genomic regions to be conserved across time and species, with deleterious mutations failing to proliferate in populations. Mutations in conserved regions can therefore have an out-sized effect on phenotype, and models that can identify these regions will likely perform better on variant effect prediction tasks.\n' +
      '\n' +
      'Reverse Complement StrandsIn the double-helix DNA structure, each strand contains semantically equivalent information. The\'reverse complement\' (RC) of a given strand is oriented in the opposite direction of its counterpart with bps complemented, A converted to T and C to G, relative to the \'forward\' strand. In many biological assays, either strand of the DNA can be sequenced with equal probability. However, learning to recognize non-palindromic DNA sequence motifs can be difficult for standard models (Zhou et al., 2021). Therefore, enforcing RC equivariance, loosely defined as model outputs transforming in a manner commensurate with RC-ing an input sequence, is an important desiderata of DNA sequence modeling.\n' +
      '\n' +
      '### Structured State Space Models\n' +
      '\n' +
      'A recent class of sequence models known as Structured State Space Models (SSMs2; Gu et al. (2021, 2022); Gupta et al. (2022); Smith et al. (2022); Dao et al. (2022)) have proven to be effective at handling long-range models. At the core of all of these models is a pair of linear differential equations that govern the mapping from input sequences \\(x(t)\\in\\mathbb{R}\\) to output sequences \\(y(t)\\in\\mathbb{R}\\) through an intermediate representation \\(\\mathbf{h}(t)\\in\\mathbb{R}^{N}\\):\n' +
      '\n' +
      'Footnote 2: The acronym SSM is commonly used in machine learning communities to refer to this class of models, while in other disciplines it is typically associated to the broader class of state space models widely used in engineering.\n' +
      '\n' +
      '\\[\\dot{\\mathbf{h}}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t),\\hskip 14.226378pty(t)=\\mathbf{C}h(t)+\\mathbf{D}x( t), \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{A}\\in\\mathbb{R}^{N\\times N},\\mathbf{B}\\in\\mathbb{R}^{N\\times 1},\\mathbf{C}\\in \\mathbb{R}^{1\\times N}\\), and \\(\\mathbf{D}\\in\\mathbb{R}\\) are the parameters of the system. For multidimensional sequences, \\(\\mathbf{x}(t),\\mathbf{y}(t)\\in\\mathbb{R}^{D}\\), these dynamics are applied independently to each component.\n' +
      '\n' +
      'This differential equation can be discretized with the continuous parameters converted, as follows:\n' +
      '\n' +
      '\\[\\mathbf{h}_{t+1}=\\overline{\\mathbf{A}}\\mathbf{h}_{t}+\\overline{\\mathbf{B}}x_{t}, \\hskip 14.226378pty_{t+1}=\\mathbf{C}\\mathbf{h}_{t}+\\mathbf{D}x_{t}, \\tag{2}\\]\n' +
      '\n' +
      'by means of some discretization formula that is a function of continuous parameters \\(\\mathbf{A},\\mathbf{B}\\), and an additional time scale parameter \\(\\Delta\\). A common discretization used in the SSM literature is the zero-order hold, defined as:\n' +
      '\n' +
      '\\[\\overline{\\mathbf{A}}=\\exp(\\Delta\\mathbf{A}),\\hskip 14.226378pt\\overline{\\mathbf{B}}= \\mathbf{A}^{-1}(\\exp(\\Delta\\mathbf{A})-\\mathbf{I})\\mathbf{B}. \\tag{3}\\]\n' +
      '\n' +
      'Importantly, the linear-time invariance (LTI) of Equation 1 allows us to equivalently formulate Equation 2 as a convolution by unrolling the recurrence, enabling efficient parallel computation during training.\n' +
      '\n' +
      'Selection MechanismsHowever, the computational efficiency of the LTI formulation comes at the cost of the model not being able to adapt / attend to specific inputs. To alleviate this lack of expressivity, Gu and Dao (2023) introduce a _selective_ SSM that enables dependence of the parameters \\(\\mathbf{B},\\mathbf{C}\\), and \\(\\Delta\\) on the input \\(x(t)\\), with:\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{B}_{t}=&\\text{Linear}_{\\mathbf{B}}(x _{t})\\qquad\\mathbf{C}_{t}=\\text{Linear}_{\\mathbf{C}}(x_{t})\\\\ &\\Delta_{t}=\\text{softplus}(\\text{Linear}_{\\Delta}(x_{t})),\\end{split} \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\text{Linear}(\\cdot)\\) represents a linear projection and \\(\\text{softplus}(\\cdot)=\\log(1+\\exp(\\cdot))\\).\n' +
      '\n' +
      'While this formulation renders \\(\\overline{\\mathbf{A}}_{t}\\) and \\(\\overline{\\mathbf{B}}_{t}\\) time-dependent, the linear recurrence in Equation 2 can be formulated as an associative scan (Martin and Cundy, 2017), which allows us to use an efficient parallel algorithm (Belloch, 1990) and reduce computation to a logarithmic in sequence length.\n' +
      '\n' +
      'MambaThe Mamba block presented in Gu and Dao (2023) is formed by combining a selective SSM sequence transformation and a gated MLP mechanism. This is depicted in the left-most schematic in Figure 1. An incoming sequence is copied and projected to twice the input dimension. One copy is then passed through a causal convolution, followed by the SiLU/Swish non-linear activation (Ramachandran et al., 2017) and then finally through the selective SSM. The other copy has the SiLU non-linearity applied to it and then gates the SSM output. The gated representation is then projected back to the original dimension \\(D\\). As this is a causal, left-to-right sequence operation, the original models that use Mamba blocks are trained with the next token prediction (NTP) objective during pre-training.\n' +
      '\n' +
      '## 3 Bi-Directional & RC-Equivariant Mamba\n' +
      '\n' +
      'In this section, we present components that extend the Mamba block (Gu and Dao, 2023). While these extensions are domain-agnostic, they are relevant to modeling DNA.\n' +
      '\n' +
      '### BiMamba\n' +
      '\n' +
      'The first extension that we apply to the standard Mamba module is to convert it from causal (left-to-right) to bi-directional. We achieve this by applying the Mamba module twice: once to the original sequence and once to a copy that is reversed along the length dimension. To combine information, the output of the reversed sequence is flipped along the length dimension and added to the forward one.\n' +
      '\n' +
      'A naive implementation of this method would double the number of parameters of the module. To avoid this added memory footprint, we instead share projection weights between the \'forward\' and\'reverse\' Mamba. These projections account for a vast majority of the model\'s parameters compared to those in the convolution and the SSM sub-modules. We refer to this parameter efficient bi-directional block as **BiMamba**. This module is depicted in the middle schematic of Figure 1.\n' +
      '\n' +
      '### MambaDNA\n' +
      '\n' +
      'To encode the RC equivariance inductive bias into our modules, we apply a Mamba (or BiMamba) block to a sequence and its RC, with parameters shared between the two applications (Shrikumar et al., 2017; Zhou et al., 2021). Given its relevance to genomics, we dub this block **MambaDNA**.\n' +
      '\n' +
      'More concretely, denote a sequence of length \\(T\\) with \\(D\\) channels by \\(\\mathbf{X}_{1:T}^{1:D}\\). Our channel splitting operation is then defined as:\n' +
      '\n' +
      '\\[\\text{split}(\\mathbf{X}_{1:T}^{1:D}):=\\left[\\mathbf{X}_{1:T}^{1:(D/2)}, \\mathbf{X}_{1:T}^{(D/2):D}\\right].\\]\n' +
      '\n' +
      'We also define the RC operation as follows:\n' +
      '\n' +
      '\\[\\text{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\mathbf{X}_{T:1}^{D:1}\\]\n' +
      '\n' +
      'Finally, letting concat denote the last operation of this module that re-combines the sequences along the channel dimension, our RC equivariant Mamba module, which we denote as \\(\\text{M}_{\\text{RCe},\\theta}\\), can be expressed as follows:\n' +
      '\n' +
      '\\[\\mathrm{M}_{\\mathrm{RCe},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\mathrm{ concat}\\left(\\left[\\mathrm{M}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right),\\mathrm{RC} \\left(\\mathrm{M}_{\\theta}\\left(\\mathbf{X}_{T:1}^{D:(D/2)}\\right)\\right)\\right] \\right),\\]\n' +
      '\n' +
      'where \\(\\mathrm{M}_{\\theta}\\) represents the sequence operator that is parameterized by either the standard Mamba or BiMamba. The MambaDNA module is depicted in the rightmost schematic of Figure 1, with \\(\\mathrm{M}_{\\theta}\\) shown as the standard Mamba.\n' +
      '\n' +
      'We claim that MambaDNA satisfies the RC equivariance property that we desire for processing DNA sequences:\n' +
      '\n' +
      '**Theorem 3.1**.: _The \\(\\mathrm{M}_{RCe,\\theta}\\) operator satisfies the property that_\n' +
      '\n' +
      '\\[\\mathrm{RC}\\circ\\mathrm{M}_{RCe,\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)= \\mathrm{M}_{RCe,\\theta}\\circ\\mathrm{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right).\\]\n' +
      '\n' +
      '_Proof._ See Appendix A.\n' +
      '\n' +
      'Similar to BiMamba modules, MambaDNA blocks do not entail significant additional memory footprint, since the wrapped sequence operator that processes the forward and RC sequences is completely shared.\n' +
      '\n' +
      '## 4 Caduceus \\(\\mathbb{P}\\)\n' +
      '\n' +
      'Below we describe **Caduceus**, a novel bi-directional DNA LM architecture that enforces RC equivariance. We introduce two versions of this model, each of which maintains equivariance in a different manner: either (1) via parameter sharing (Shrikumar et al., 2017), Caduceus-PS, or (2) via a technique used during downstream tasks, known as _post-hoc conjoining_(Zhou et al., 2021), Caduceus-Ph.\n' +
      '\n' +
      'Figure 2: Caduceus Architecture. Bi-directional, RC equivariant Mamba modules are used in conjunction with equivariant word embeddings and language model head to form **Caduceus-PS**. Using only BiMamba blocks with RC data augmentation during pretraining and post-hoc conjoining for downstream task inference yields Caduceus-Ph. Caduceus Image license: Creative Commons CC0 1.0 Universal Public Domain Dedication.\n' +
      '\n' +
      '### Caduceus-PS\n' +
      '\n' +
      'ArchitectureFor **Caduceus-PS**, we leverage both of the architectural innovations introduced in Section 3. Namely, we wrap a BiMamba module within a MambaDNA block. Additionally, preceding the Mamba blocks of this architecture is an RC equivariant token embedding module. Denoting by \\(\\text{Emb}_{\\theta}\\) the linear projection that takes one-hot vectors \\(\\mathbf{X}_{1:T}^{1:4}\\) and produces embeddings in \\(\\mathbb{R}^{D/2}\\), the RC equivariant version of this embedding is defined as:\n' +
      '\n' +
      '\\[\\text{Emb}_{\\text{RCe},\\theta}\\left(\\mathbf{X}_{1:T}^{1:4}\\right):=\\text{concat }\\left(\\left[\\text{Emb}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:4}\\right),\\text{RC} \\circ\\text{Emb}_{\\theta}\\left(\\text{RC}\\left(\\mathbf{X}_{1:T}^{1:4}\\right) \\right)\\right]\\right)\\]\n' +
      '\n' +
      'Additionally, the logits of the Caduceus model are produced by passing the output of its final MambaDNA block through a RC equivariant language model head. To our knowledge, Caduceus-PS is the first model to incorporate RC equivariance into the LM pre-training paradigm. This can be formalized by first defining a channel flip operator \\(\\text{flip\\_chan}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\left(\\mathbf{X}_{1:T}^{ D:1}\\right).\\) Then, letting \\(\\text{LM}_{\\theta}\\) be the linear projection from sequences with \\(D/2\\) channels to vectors in \\(\\mathbb{R}^{4}\\), we define the equivariant version of the language modeling head as:\n' +
      '\n' +
      '\\[\\text{LM}_{\\text{RCe},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):=\\text{LM}_{ \\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right)+\\text{flip\\_chan}\\circ\\text{ LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{D:(D/2)}\\right).\\]\n' +
      '\n' +
      'Depicted in Figure 2 with the **black** path, Caduceus-PS enables RC equivariant language model pre-training: the predictions it produces for the RC of a given sequence are equivalent to reversing the predictions of the original sequence along the length dimension and complementing outputs: A-T and C-G. We formalize this claim in the following statement:\n' +
      '\n' +
      '**Theorem 4.1**.: _Composing \\(\\text{LM}_{\\text{RCe},\\theta}\\circ\\text{M}_{\\text{RCe},\\theta}^{(n)}\\circ\\text {Emb}_{\\text{RCe},\\theta}\\), where \\(\\text{M}_{\\text{RCe},\\theta}^{(n)}\\) denotes \\(n\\) compositions of different Mamba RC equivariant modules, yields a sequence operator that is RC equivariant._\n' +
      '\n' +
      'Proof.: See Appendix B.\n' +
      '\n' +
      'Pre-trainingGiven the bi-directionality of this model, we train Caduceus-PS with the masked language modeling (MLM) objective, using the standard masking recipe proposed in BERT (Devlin et al., 2018). The RC equivariant language modeling of Caduceus-PS means that we do not need RC data augmentation at pre-training, since predictions are inherently symmetric with respect to this operation.\n' +
      '\n' +
      'Downstream UsageFor downstream tasks, since either strand of an assayed sequence will carry the same label, we wish to enforce RC _invariance_. The token embedding parameter sharing in Caduceus-PS means that its intermediate and final hidden states are twice the (channel) dimensionality of a standard Mamba-based language model with an equivalently sized token embedding matrix. To enforce RC invariance at downstream training and inference, final hidden states are split and the two splits are averaged.\n' +
      '\n' +
      '### Caduceus-Ph\n' +
      '\n' +
      'ArchitectureThe Caduceus-Ph model is depicted with the blue path in Figure 2. The core of this model is a stack of BiMamba blocks.\n' +
      '\n' +
      'Pre-trainingAs with Caduceus-PS, this model is pre-trained using the same MLM objective. However, as the model is not an RC equivariant LM, we instead rely on data augmentation during pre-training.\n' +
      '\n' +
      'Downstream UsageIn order to make the downstream task representations RC invariant, we leverage a technique called _post-hoc_ conjoining (Zhou et al., 2021). Namely, for downstream task _training_ the backbone model is unchanged, but we employ RC data augmentation. However, for downstream task _inference_, we apply the model twice, once on the original sequence and once on a corresponding RC sequence, and average the two, effectively performing a version of \'RC ensembling\' (Mallet and Vert, 2021).\n' +
      '\n' +
      'Experiments\n' +
      '\n' +
      '### Pre-training\n' +
      '\n' +
      'DataWe limit the focus of this work to human-genome related tasks. To that end, we perform all pre-training tasks on the human reference genome (Consortium et al., 2009). We use character- / base pair-level tokenization. While other DNA FMs have explored k-mer tokenization, this scheme suffers from the drawback that minor changes to an input sequence can lead to drastically different tokenization outputs (Zhou et al., 2023) complicating training. Character-level tokenization avoids this issue. For any non-RC equivariant model that we train, including re-training HyenaDNA (Nguyen et al., 2023) models, we employ RC data augmentation during pre-training. For more information on the pre-training dataset and recipes see Appendix C.\n' +
      '\n' +
      'Mamba vs. HyenaDNA NTPSimilar to the preliminary results in Gu and Dao (2023), we find that the Mamba module performs better than Hyena in terms of NTP. In Figure 2(a), we see that at varying sequence lengths and comparable model sizes, a standard Mamba model attains lower cross entropy loss on the pre-training data test set compared to HyenaDNA. As reported in (Gu and Dao, 2023), we also found that Mamba is more robust to higher using learning rates, a common best practice in training LMs. These results lend support to our choice of Mamba as the inner building block of our models.\n' +
      '\n' +
      'Effect of Parameter Sharing on MLM Pre-trainingTo test the importance of enabling deeper bi-directional models for similar parameter counts via projection parameter sharing, we compare the MLM pre-training loss of BiMamba models to naive bi-directional Mamba models that do not use weight tying and are therefore reduced to half the depth. We find that our parameter efficient implementation of bi-directionality leads to better pre-training loss, as depicted in Figure 2(b).\n' +
      '\n' +
      'Effect of RC Equivariance on MLM Pre-trainingWe also examine the effect of using our proposed RC equivariant LM on pre-training. In Figure 2(c), we find that RC equivariant LM leads to better MLM\n' +
      '\n' +
      'Figure 3: Pre-training test set loss. (a) For comparable model size and sequence length, Mamba attains better cross entropy loss than HyenaDNA during pre-training on the human genome. (b) Across sequence lengths, deeper models that use weight tying have better pre-training loss on the human genome. (c) Across sequence lengths, RC equivariance leads to better pre-training loss on the human genome. Note, models with a sequence length of 131k were validated less frequently to reduce overhead during pre-training. By adjusting batch size, we hold number of tokens per batch constant across varying lengths.\n' +
      '\n' +
      'pre-training loss. This is significant because, as described above, performance on the MLM task has grounding in the biology of downstream tasks, such as variant effect prediction.\n' +
      '\n' +
      '### Downstream Tasks\n' +
      '\n' +
      '#### 5.2.1 Genomics Benchmark\n' +
      '\n' +
      'We begin the evaluation with the Genomics Benchmarks (Gresova et al., 2023), a recently proposed benchmark with eight regulatory element classification tasks. Our baselines consist of HyenaDNA3 and a supervised trained CNN model described in Gresova et al. (2023). For HyenaDNA and all our Mamba-based models, we take the final hidden state embedding and perform mean pooling on the sequences, which vary from 200 to approximately 2,000 bps in length. We perform 5-fold cross-validation (CV) using different random seeds, with early stopping on validation accuracy and report mean and \\(\\pm\\) on max/min of the 5 seeds.\n' +
      '\n' +
      'Footnote 3: Pre-trained weights downloaded from [https://huggingface.co/LongSafari/hyenaDNA-tiny-lk-seqlen](https://huggingface.co/LongSafari/hyenaDNA-tiny-lk-seqlen)\n' +
      '\n' +
      'As shown in Table 1, Caduceus models attain best performance across all annotations. Of note, Caduceus-Ph is the best performing model overall for this suite of tasks. Other works that examine post-hoc conjoining, similarly find this to be a strong modeling decision (Zhou et al., 2021; Mallet and Vert, 2021).\n' +
      '\n' +
      '#### 5.2.2 Nucleotide Transformer Tasks\n' +
      '\n' +
      'Next, we benchmark against a collection of 18 datasets introduced in Dalla-Torre et al. (2023) and derived from five peer-reviewed studies (Phaml et al., 2005; Geng et al., 2022; Wang et al., 2019; Oubouny et al., 2019; Scalzitti et al., 2021). These datasets contains three task types, including histone marker prediction, regulatory annotation prediction, and splice site annotation prediction. In assessing performance, we adhered to the methodology described in Dalla-Torre et al. (2023), using different metrics for the tasks: Matthews Correlation Coefficient (MCC) for all histone marker tasks, F1 score for all regulatory annotations, and splice site annotation tasks, except for the _splice sites all_ task, where we report accuracy. We additionally follow Dalla-Torre et al. (2023) in performing 10-fold CV using different random seeds with early stopping on the validation metric. We report mean and \\(\\pm\\) on max/min of the 10 seeds4. The results for this benchmark suite are presented in Table 2, where we again find that Caduceus-Ph performs competitively, even beating attention-based methods with orders of magnitude more parameters on 8 of 18 prediction tasks. Caduceus models outperform a similarly sized HyenaDNA model on almost all the histone marker and regulatory annotation tasks, while HyenaDNA performs better on splice site annotation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & CNN & HyenaDNA & Mamba & Caduceus & \\multirow{2}{*}{Caduceus-Ph} & \\multirow{2}{*}{Caduceus-PS} \\\\  & (264k) & (436k) & (468k) & & (470k) & (470k) \\\\ \\hline Mouse Enhancers & 0.715 \\(\\pm\\)0.087 & _0.780_\\(\\pm\\)0.025 & 0.743 \\(\\pm\\)0.054 & 0.770 \\(\\pm\\)0.058 & 0.754 \\(\\pm\\)0.074 & **0.793**\\(\\pm\\)0.058 \\\\ Coding vs. Intergenomic & 0.892 \\(\\pm\\)0.008 & 0.904 \\(\\pm\\)0.005 & 0.904 \\(\\pm\\)0.004 & 0.908 \\(\\pm\\)0.003 & **0.915**\\(\\pm\\)0.003 & _0.910_\\(\\pm\\)0.003 \\\\ Human vs. Worm & 0.942 \\(\\pm\\)0.002 & 0.964 \\(\\pm\\)0.002 & 0.967 \\(\\pm\\)0.002 & _0.970_\\(\\pm\\)0.003 & **0.973**\\(\\pm\\)0.001 & 0.968 \\(\\pm\\)0.002 \\\\ Human Enhancer Conn & 0.702 \\(\\pm\\)0.021 & 0.729 \\(\\pm\\)0.014 & 0.732 \\(\\pm\\)0.029 & 0.741 \\(\\pm\\)0.008 & **0.747**\\(\\pm\\)0.004 & _0.745_\\(\\pm\\)0.007 \\\\ Human Enhancer Ensembl & 0.744 \\(\\pm\\)0.122 & 0.849 \\(\\pm\\)0.006 & 0.862 \\(\\pm\\)0.008 & 0.883 \\(\\pm\\)0.002 & _0.893_\\(\\pm\\)0.008 & **0.900**\\(\\pm\\)0.006 \\\\ Human Regulatory & 0.872 \\(\\pm\\)0.005 & 0.869 \\(\\pm\\)0.012 & 0.814 \\(\\pm\\)0.211 & 0.871 \\(\\pm\\)0.007 & _0.872_\\(\\pm\\)0.011 & **0.873**\\(\\pm\\)0.007 \\\\ Human OCR Ensembl & 0.698 \\(\\pm\\)0.013 & 0.783 \\(\\pm\\)0.007 & 0.815 \\(\\pm\\)0.002 & 0.818 \\(\\pm\\)0.003 & **0.828**\\(\\pm\\)0.006 & _0.818_\\(\\pm\\)0.006 \\\\ Human Non/TATA Promotes & 0.861 \\(\\pm\\)0.009 & 0.944 \\(\\pm\\)0.002 & 0.933 \\(\\pm\\)0.007 & 0.933 \\(\\pm\\)0.006 & **0.946**\\(\\pm\\)0.007 & _0.945_\\(\\pm\\)0.010 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Genomic Benchmarks. Top-1 accuracy (\\(\\uparrow\\)) across 5-fold cross-validation (CV) for pretrained HyenaDNA, Mamba NTP, and Caduceus models and a supervised CNN baseline (trained from scratch). Best values per task are **bolded**, second best are _italicized_. Error bars indicate the difference between the maximum and minimum values across 5 random seeds used for CV.\n' +
      '\n' +
      '#### 5.2.3 Predicting the Effect of Variants on Gene Expression\n' +
      '\n' +
      'Finally, we explore the implications of long-range contexts on the task of predicting the effect of SNPs on gene expression. There is biological evidence to suggest this task indeed entails long-range interactions (Furlong and Levine, 2018). Additionally it aligns well to LM pre-training objectives, which enable models to implicitly learn to recognize the effects of evolutionary pressure (e.g., conservation, co-evolution). The dataset used in this task is derived from the Enformer paper (Avsec et al., 2021) and presented in Trop et al. (2023). From each model, we extract embeddings centered around the SNP location. We stratify the data by distance of the SNP to nearest Transcription Start Site (TSS). For each bucket, we sample 5,000 training points and fit an SVM classifier with an RBF kernel to predict VEP annotations. We report test set AUCROC mean and max/min ranges for classifiers fit on 5 random training subsets. For more details about this experiment, please refer to Appendix D.3. We compare Caduceus to HyenaDNA5 and Nucleotide Transformer6\n' +
      '\n' +
      'Footnote 5: Pre-trained weights downloaded from [https://huggingface.co/LongSafari/hyenadma-medium-160k-seqlen-hf](https://huggingface.co/LongSafari/hyenadma-medium-160k-seqlen-hf).\n' +
      '\n' +
      'Footnote 6: Pre-trained weights downloaded from [https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species](https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-500m-multi-species).\n' +
      '\n' +
      'As shown in Figure 4, the Caduceus models consistently outperform HyenaDNA, and Caduceus-PS exceeds the performance of the Nucleotide Transformer v2 (with 500M parameters) by a large margin, especially as distance to the nearest TSS grows.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c||c c c} \\hline \\hline  & \\multicolumn{2}{c||}{\\(>\\) 100M Param. Models} & \\multicolumn{3}{c}{\\(<\\) 2M Param. Models} \\\\  & Enformer & DABERT-2 & NT-v2 & HyenaDNA & Caduceus-Ph & Caduceus-PS \\\\  & (252M) & (117M) & (500M) & (1.6M) & (1.9M) & (1.9M) \\\\ \\hline \\multicolumn{6}{l}{_Histone Markers_} \\\\ H3 & 0.719\\(\\pm\\)0.048 & 0.785\\(\\pm\\)0.033 & 0.784\\(\\pm\\)0.047 & 0.779\\(\\pm\\)0.037 & **0.815\\(\\pm\\)**0.048 & _0.779\\(\\pm\\)_0.029 \\\\ H3k14ac & 0.288\\(\\pm\\)0.077 & 0.516\\(\\pm\\)0.028 & 0.551\\(\\pm\\)0.021 & _0.612\\(\\pm\\)_0.065 & **0.631\\(\\pm\\)**0.026 & 0.541\\(\\pm\\)0.122 \\\\ H3k36me3 & 0.344\\(\\pm\\)0.055 & 0.591\\(\\pm\\)0.020 & **0.625\\(\\pm\\)**0.013 & _0.613\\(\\pm\\)_0.041 & 0.601\\(\\pm\\)0.129 & 0.609\\(\\pm\\)0.109 \\\\ H3k4me1 & 0.291\\(\\pm\\)0.061 & 0.511\\(\\pm\\)0.028 & **0.550\\(\\pm\\)**0.021 & 0.512\\(\\pm\\)0.024 & _0.522\\(\\pm\\)_0.039 & 0.488\\(\\pm\\)0.102 \\\\ H3k4me2 & 0.211\\(\\pm\\)0.069 & 0.336\\(\\pm\\)0.040 & 0.319\\(\\pm\\)0.045 & _0.455\\(\\pm\\)_0.095 & **0.487\\(\\pm\\)**0.170 & 0.388\\(\\pm\\)0.101 \\\\ H3k4me3 & 0.158\\(\\pm\\)0.072 & 0.352\\(\\pm\\)0.077 & 0.410\\(\\pm\\)0.033 & **0.549\\(\\pm\\)**0.056 & _0.544\\(\\pm\\)_0.045 & 0.440\\(\\pm\\)0.202 \\\\ H3k799ie3 & 0.496\\(\\pm\\)0.042 & 0.613\\(\\pm\\)0.030 & 0.626\\(\\pm\\)0.026 & 0.672\\(\\pm\\)0.048 & **0.697\\(\\pm\\)**0.077 & _0.676\\(\\pm\\)_0.026 \\\\ H3k5Bac & 0.420\\(\\pm\\)0.063 & 0.542\\(\\pm\\)0.029 & 0.562\\(\\pm\\)0.040 & 0.581\\(\\pm\\)0.061 & **0.622\\(\\pm\\)**0.030 & _0.604\\(\\pm\\)_0.048 \\\\ H4 & 0.732\\(\\pm\\)0.076 & 0.796\\(\\pm\\)0.027 & _0.799\\(\\pm\\)_0.025 & 0.763\\(\\pm\\)0.044 & **0.811\\(\\pm\\)**0.022 & 0.789\\(\\pm\\)0.020 \\\\ H4ac & 0.273\\(\\pm\\)0.063 & 0.463\\(\\pm\\)0.041 & 0.495\\(\\pm\\)0.032 & _0.564\\(\\pm\\)_0.038 & **0.621\\(\\pm\\)**0.054 & 0.525\\(\\pm\\)0.240 \\\\ \\hline \\multicolumn{6}{l}{_Regulatory Annotation_} \\\\ Enhancer & 0.451\\(\\pm\\)0.108 & 0.516\\(\\pm\\)0.008 & **0.548\\(\\pm\\)**0.144 & 0.517\\(\\pm\\)0.117 & _0.546\\(\\pm\\)_0.073 & 0.491\\(\\pm\\)0.066 \\\\ Enhancer types & 0.309\\(\\pm\\)0.134 & 0.423\\(\\pm\\)0.051 & _0.424\\(\\pm\\)_0.122 & 0.386\\(\\pm\\)0.185 & **0.439\\(\\pm\\)**0.064 & 0.416\\(\\pm\\)0.095 \\\\ Promoter: All & 0.954\\(\\pm\\)0.006 & _0.971\\(\\pm\\)_0.006 & **0.976\\(\\pm\\)**0.006 & 0.960\\(\\pm\\)0.005 & 0.970\\(\\pm\\)0.004 & 0.967\\(\\pm\\)0.004 \\\\ NonTATA & 0.955\\(\\pm\\)0.010 & _0.972\\(\\pm\\)_0.005 & **0.976\\(\\pm\\)**0.005 & 0.959\\(\\pm\\)**0.008 & 0.962\\(\\pm\\)0.011 & 0.968\\(\\pm\\)0.006 \\\\ TATA & _0.960\\(\\pm\\)_0.023 & 0.955\\(\\pm\\)0.021 & **0.966\\(\\pm\\)**0.013 & 0.944\\(\\pm\\)0.040 & 0.953\\(\\pm\\)0.016 & 0.957\\(\\pm\\)0.015 \\\\ \\hline \\multicolumn{6}{l}{_Splice Site Annotation_} \\\\ All & 0.848\\(\\pm\\)0.019 & 0.939\\(\\pm\\)0.009 & **0.983\\(\\pm\\)**0.008 & _0.956\\(\\pm\\)_0.011 & 0.940\\(\\pm\\)0.027 & 0.927\\(\\pm\\)0.021 \\\\ Acceptor & 0.914\\(\\pm\\)0.028 & _0.975\\(\\pm\\)_0.006 & **0.981\\(\\pm\\)**0.011 & 0.958\\(\\pm\\)0.010 & 0.937\\(\\pm\\)0.033 & 0.936\\(\\pm\\)0.077 \\\\ Donor & 0.906\\(\\pm\\)0.027 & _0.963\\(\\pm\\)_0.006 & **0.985\\(\\pm\\)**0.022 & 0.949\\(\\pm\\)**0.024 & 0.948\\(\\pm\\)0.025 & 0.874\\(\\pm\\)0.289 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Nucleotide Transformer Tasks. Performance (\\(\\uparrow\\)) across 10-fold CV for Enformer, DNABERT-2, Nucleotide Transformer v2, HyenaDNA, Caduceus-Ph, and Caduceus-PS. Metrics vary by task: MCC for histone markers, F1-score for regualtory and splice site acceptor/donor, and accuracy for splice site “all”. Best values per task are **bolded**, second best are _italicized_. Given the disparity in model size, we also underline the best value within the SSM-based models. Error bars indicate the difference between the maximum and minimum values across 10 random seeds used for CV.\n' +
      '\n' +
      '## 6 Related Work\n' +
      '\n' +
      '### DNA Language Models\n' +
      '\n' +
      'Transformer-based DNA LMs, such as DNABERT, v1 (Ji et al., 2021) and v2 (Zhou et al., 2023), and Nucleotide Transformer (Dalla-Torre et al., 2023) have been restricted by the quadratic scaling of Transformers, with maximum context sizes of up to roughly 12,000 bps. BigBird (Zaheer et al., 2020) (and GENA-LM (Fishman et al., 2023), which uses BigBird as a backbone) use sparse attention to scale context size up to an order of magnitude large.\n' +
      '\n' +
      'Notably, GPN (Benggas et al., 2023, 2023), uses dilated convolutional layers, which in practice scale to large receptive fields, although a context size of only 512 bps is used when training this model. Benegas et al. (2023) find that DNA LM are powerful unsupervised variant effect predictors.\n' +
      '\n' +
      'HyenaDNAMost related to our work is the HyenaDNA model (Nguyen et al., 2023), which uses the Hyena operator, derived from the SSM literature, as the building block for a DNA LM. HyenaDNA is able to scale to long-range sequences (up to 1 million bps), but is notably a uni-directional model and not inherently robust to RC inputs.\n' +
      '\n' +
      '### Reverse Complement Training for DNA\n' +
      '\n' +
      'Cao & Zhang (2019) discuss the importance of RC data augmentation in genomics. Shrikumar et al. (2017) introduce RC Parameter Sharing (RCPS) for convolution, batch normalization, and pooling modules. Mallet & Vert (2021) formalize RC equivariance in the language of Group representations and cast RCPS as a particular decomposition of such representations, exploring other as well. Our implementation of RCPS in the MambaDNA block differs from that proposed in Shrikumar et al. (2017), in that our split operation prevents the channel dimension from doubling when passing a sequence through a given layer.\n' +
      '\n' +
      'Zhou et al. (2021) further explore RCPS layers and compare them to a _post-hoc_ conjoining baseline, which serves as the inspiration for our Caduceus-Ph model. Zhou et al. (2021) find that post-hoc conjoining\n' +
      '\n' +
      'Figure 4: Predicting variant effects on gene expression across varying distances to the nearest Transcription Start Site (TSS). Models compared include NT-v2, HyenaDNA, Caduceus w/o RC Equiv, Caduceus-Ph, and Caduceus-PS, with model sizes indicated in parentheses. SSM-based models utilize a 131k sequence length. We show performance at short (0 - 30kb), medium (30 - 100kb), and long-range (100kb+) distances to TSS. Notably, Caduceus-PS consistently demonstrates enhanced predictive accuracy for long-range effects. Error bars represent standard deviation across five SVM classifiers, each trained on different dataset subsets.\n' +
      '\n' +
      'is a strong baseline that often outperforms RCPS models on several tasks. We note that Zhou et al. (2021) focus on only supervised training regimes, whereas we extend the post-hoc conjoining methodology to include a LM pre-training step as well. Prediction conjoining was also explored in DeepBind (Alipanahi et al., 2015), where max aggregation as opposed to averaging is used, and FactorNet (Quang and Xie, 2019), which performs conjoining during training, not just at inference.\n' +
      '\n' +
      'Finally, similar to our setup, Gunduz et al. (2023) explore using RC sequences in a self supervised pre-training and downstream task fine-tuning paradigm. However, their model uses contrastive learning during pre-training where an encoder is trained to recognize the embeddings of the RC sequence in a given batch.\n' +
      '\n' +
      '### Bi-directional RNNs\n' +
      '\n' +
      'Exploiting bi-directionality for pre-training on large datasets was first realized in ELMo (Peters et al., 2017), where forward and backward LSTMs (Hochreiter and Schmidhuber, 1997) were utilized simultaneously to model language context. This laid the groundwork for models such as BERT (Devlin et al., 2018) that replaced recurrent networks with a Transformer backbone. Recently, Wang et al. (2022) explored BERT-style training using SSMs.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In this work, we introduced architectural innovations to the Mamba module: enabling bi-directional and RC equivariant sequence modeling. We also propose a new DNA foundation model, Caduceus, and demonstrate its ability to outperform comparably sized uni-directional Hyena-based models and Transformer-based models orders of magnitude larger in size on a range of biologically relevant tasks, most notably predicting the effect of genetic mutations on gene expression.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This work was supported by an NSF CAREER grant (#2145577) and an NIH MIRA grant (#1R35GM151243-01). We would also like to thank Evan Trop and the InstaDeep team for useful discussions about the Nucleotide Transformer leaderboard and MosaicML for providing compute resources for some of the pre-training experiments.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n' +
      '* Alipanahi et al. (2015) Alipanahi, B., Delong, A., Weirauch, M. T., and Frey, B. J. Predicting the sequence specificities of dna-and rna-binding proteins by deep learning. Nature biotechnology33 (8):831-838, 2015.\n' +
      '* Avsec et al. (2021) Avsec, Z., Agarwal, V., Visentin, D., Ledesam, J. R., Grabska-Barwinska, A., Taylor, K. R., Assael, Y., Jumper, J., Kohli, P., and Kelley, D. R. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods18 (10):1196-1203, 2021.\n' +
      '* Benegas et al. (2023a) Benegas, G., Albors, C., Aw, A. J., Ye, C., and Song, Y. S. Gpn-msa: an alignment-based dna language model for genome-wide variant effect prediction. bioRxiv, 2023a.\n' +
      '* Benegas et al. (2023b) Benegas, G., Batra, S. S., and Song, Y. S. Dna language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences120 (44):e2311219120, 2023b.\n' +
      '* Benegas et al. (2023c) Benegas, G., Batra, S. S., and Song, Y. S. Dna language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences120 (44):e2311219120, 2023c.\n' +
      '* Blelloch (1990) Blelloch, G. E. Prefix sums and their applications. 1990.\n' +
      '* Blelloch (1991)Cao, Z. and Zhang, S. Simple tricks of convolutional neural network architectures improve dna-protein binding prediction. _Bioinformatics_, 35(11):1837-1843, 2019.\n' +
      '* Consortium et al. [2009] Consortium, G. R. et al. Genome reference consortium human build 37 (grch37). _Database (GenBank or RefSeq)_, 2009.\n' +
      '* Dalla-Torre et al. [2023] Dalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Carranza, N. L., Grzywaczewski, A. H., Oteri, F., Dallago, C., Trop, E., de Almeida, B. P., Sireklkatim, H., et al. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. _bioRxiv_, pp. 2023-01, 2023.\n' +
      '* Dao et al. [2022] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language modeling with state space models. _arXiv preprint arXiv:2212.14052_, 2022.\n' +
      '* Devlin et al. [2018] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Falcon [2019] Falcon, W. and The PyTorch Lightning team. PyTorch Lightning, March 2019. URL [https://github.com/Lightning-AI/lightning](https://github.com/Lightning-AI/lightning).\n' +
      '* Fishman et al. [2023] Fishman, V., Kuratov, Y., Petrov, M., Shmelev, A., Shepelin, D., Chekanov, N., Kardymon, O., and Burtsev, M. Gena-lm: A family of open-source foundational models for long dna sequences. _bioRxiv_, pp. 2023-06, 2023.\n' +
      '* Furlong and Levine [2018] Furlong, E. E. M. and Levine, M. Developmental enhancers and chromosome topology. _Science_, 361(6409):1341-1345, 2018. doi: 10.1126/science.aau0320. URL [https://www.science.org/doi/abs/10.1126/science.aau0320](https://www.science.org/doi/abs/10.1126/science.aau0320).\n' +
      '* Geng et al. [2022] Geng, Q., Yang, R., and Zhang, L. A deep learning framework for enhancer prediction using word embedding and sequence generation. _Biophysical Chemistry_, 286:106822, 2022.\n' +
      '* Gresova et al. [2023] Gresova, K., Martinek, V., Cechak, D., Simecek, P., and Alexiou, P. Genomic benchmarks: a collection of datasets for genomic sequence classification. _BMC Genomic Data_, 24(1):25, 2023.\n' +
      '* Gu and Dao [2023] Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* Gu et al. [2021a] Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021a.\n' +
      '* Gu et al. [2021b] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. _Advances in neural information processing systems_, 34:572-585, 2021b.\n' +
      '* Gu et al. [2022] Gu, A., Goel, K., Gupta, A., and Re, C. On the parameterization and initialization of diagonal state space models. _Advances in Neural Information Processing Systems_, 35:35971-35983, 2022.\n' +
      '* Gunduz et al. [2022] Gunduz, H. A., Binder, M., To, X.-Y., Mreches, R., Bischl, B., McHardy, A. C., Munch, P. C., and Rezaei, M. A self-supervised deep learning method for data-efficient training in genomics. _Communications Biology_, 6(1):928, 2023.\n' +
      '* Gupta et al. [2022] Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. _Advances in Neural Information Processing Systems_, 35:22982-22994, 2022.\n' +
      '* Harris et al. [2020] Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Rio, J. F., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020. doi: 10.1038/s41586-020-2649-2. URL [https://doi.org/10.1038/s41586-020-2649-2](https://doi.org/10.1038/s41586-020-2649-2).\n' +
      '* Hochreiter & Schmidhuber [1997] Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.\n' +
      '* Hochreiter & Schmidhuber [1997]Hunter, J. D. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007. doi: 10.1109/MCSE.2007.55.\n' +
      '* Ji et al. (2021) Ji, Y., Zhou, Z., Liu, H., and Davuluri, R. V. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. _Bioinformatics_, 37(15):2112-2120, 2021.\n' +
      '* Jumper et al. (2021) Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Zidek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.\n' +
      '* Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Lin et al. (2023) Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023.\n' +
      '* Madani et al. (2023) Madani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., Olmos Jr, J. L., Xiong, C., Sun, Z. Z., Socher, R., et al. Large language models generate functional protein sequences across diverse families. _Nature Biotechnology_, pp. 1-8, 2023.\n' +
      '* Mallet & Vert (2021) Mallet, V. and Vert, J.-P. Reverse-complement equivariant networks for dna sequences. _Advances in Neural Information Processing Systems_, 34:13511-13523, 2021.\n' +
      '* Martin & Cundy (2017) Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. _arXiv preprint arXiv:1709.04057_, 2017.\n' +
      '* Nguyen et al. (2023) Nguyen, E., Poli, M., Faizi, M., Thomas, A., Birch-Sykes, C., Wornow, M., Patel, A., Rabideau, C., Massaroli, S., Bengio, Y., et al. Hypandna: Long-range genomic sequence modeling at single nucleotide resolution. _arXiv preprint arXiv:2306.15794_, 2023.\n' +
      '* Oubounyt et al. (2019) Oubounyt, M., Louadi, Z., Tayara, H., and Chong, K. T. Deepromoter: robust promoter predictor using deep learning. _Frontiers in genetics_, 10:286, 2019.\n' +
      '* pandas development team (2020) pandas development team, T. pandas-dev/pandas: Pandas, February 2020. URL [https://doi.org/10.5281/zenodo.3509134](https://doi.org/10.5281/zenodo.3509134).\n' +
      '* Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Wallach, H., Larochelle, H., Beygelzimer, A., d\'Alche Buc, F., Fox, E., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems 32_, pp. 8024-8035. Curran Associates, Inc., 2019.\n' +
      '* Pedregosa et al. (2011) Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.\n' +
      '* Peters et al. (2017) Peters, M. E., Ammar, W., Bhagavatula, C., and Power, R. Semi-supervised sequence tagging with bidirectional language models. _arXiv preprint arXiv:1705.00108_, 2017.\n' +
      '* Phaml et al. (2005) Phaml, T. H., Tran, D. H., Ho, T. B., Satou, K., and Valiente, G. Qualitatively predicting acetylation and methylation areas in dna sequences. _Genome Informatics_, 16(2):3-11, 2005.\n' +
      '* Quang & Xie (2019) Quang, D. and Xie, X. Factornet: a deep learning framework for predicting cell type specific transcription factor binding from nucleotide-resolution sequential data. _Methods_, 166:40-47, 2019.\n' +
      '* Ramachandran et al. (2017) Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. _arXiv preprint arXiv:1710.05941_, 2017.\n' +
      '* Ramachandran et al. (2018)Rao, R., Meier, J., Sercu, T., Ovchinnikov, S., and Rives, A. Transformer protein language models are unsupervised structure learners. _Biorxiv_, pp. 2020-12, 2020.\n' +
      '* Rives et al. (2021) Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. _Proceedings of the National Academy of Sciences_, 118(15):e2016239118, 2021.\n' +
      '* Scalzitti et al. (2021) Scalzitti, N., Kress, A., Orhand, R., Weber, T., Moulinier, L., Jeannin-Girardon, A., Collet, P., Poch, O., and Thompson, J. D. Spliceator: Multi-species splice site prediction using convolutional neural networks. _BMC bioinformatics_, 22(1):1-26, 2021.\n' +
      '* Shrikumar et al. (2017) Shrikumar, A., Greenside, P., and Kundaje, A. Reverse-complement parameter sharing improves deep learning models for genomics. _BioRxiv_, pp. 103663, 2017.\n' +
      '* Smith et al. (2022) Smith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. _arXiv preprint arXiv:2208.04933_, 2022.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tillet et al. (2019) Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate language and compiler for tiled neural network computations. In _Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, pp. 10-19, 2019.\n' +
      '* Trop et al. (2023) Trop, E., Kao, C.-H., Polen, M., Schiff, Y., de Almeida, B. P., Gokaslan, A., Pierrot, T., and Kuleshov, V. Advancing dna language models: The genomics long-range benchmark. In _LLMs4Bio Workshop_, 2023.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Wang et al. (2020) Wang, G., Sarkar, A., Carbonetto, P., and Stephens, M. A simple new approach to variable selection in regression, with application to genetic fine mapping. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 82(5):1273-1300, 2020.\n' +
      '* Wang et al. (2022) Wang, J., Yan, J. N., Gu, A., and Rush, A. M. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.\n' +
      '* Wang et al. (2019) Wang, R., Wang, Z., Wang, J., and Li, S. Splicefinder: ab initio prediction of splice sites using convolutional neural network. _BMC bioinformatics_, 20:1-13, 2019.\n' +
      '* Waskom (2021) Waskom, M. L. seaborn: statistical data visualization. _Journal of Open Source Software_, 6(60):3021, 2021. doi: 10.21105/joss.03021. URL [https://doi.org/10.21105/joss.03021](https://doi.org/10.21105/joss.03021).\n' +
      '* Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface\'s transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* a framework for elegantly configuring complex applications. Github, 2019. URL [https://github.com/facebookresearch/hydra](https://github.com/facebookresearch/hydra).\n' +
      '* Zaheer et al. (2020) Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. _Advances in neural information processing systems_, 33:17283-17297, 2020.\n' +
      '* Zhou et al. (2021) Zhou, H., Shrikumar, A., and Kundaje, A. Towards a better understanding of reverse-complement equivariance for deep learning models in regulatory genomics. _BioRxiv_, pp. 2020, 2021.\n' +
      '* Zhou & Troyanskaya (2015) Zhou, J. and Troyanskaya, O. G. Predicting effects of noncoding variants with deep learning-based sequence model. _Nature methods_, 12(10):931-934, 2015.\n' +
      '* Zhou et al. (2023) Zhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R., and Liu, H. Dnabert-2: Efficient foundation model and benchmark for multi-species genome. _arXiv preprint arXiv:2306.15006_, 2023.\n' +
      '* Zhou et al. (2020)Proof of Theorem 3.1\n' +
      '\n' +
      'We begin by reiterating the definitions of the different functions that comprise our RC equivariant Mamba module. For an input sequence \\(\\mathbf{X}_{1:T}^{1:D}\\) of length \\(T\\), with \\(D\\) channels, we define:\n' +
      '\n' +
      '\\[\\operatorname{split}(\\mathbf{X}_{1:T}^{1:D}) :=\\left[\\mathbf{X}_{1:T}^{1:(D/2)},\\mathbf{X}_{1:T}^{(D/2):D} \\right], \\tag{5}\\] \\[\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right) :=\\mathbf{X}_{T:1}^{D:1},\\] (6) \\[\\operatorname{concat}\\left(\\left[\\mathbf{X}_{1:T}^{1:(D/2)}, \\mathbf{X}_{1:T}^{(D/2):D}\\right]\\right) :=\\mathbf{X}_{1:T}^{1:D},\\] (7) \\[\\operatorname{M_{\\mathrm{RC},\\theta}}\\left(\\mathbf{X}_{1:T}^{1:D }\\right) :=\\operatorname{concat}\\left(\\left[\\operatorname{M}_{\\theta}\\left( \\mathbf{X}_{1:T}^{1:(D/2)}\\right),\\operatorname{RC}\\left(\\operatorname{M}_{ \\theta}\\circ\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right)\\right) \\right]\\right). \\tag{8}\\]\n' +
      '\n' +
      'We also denote the application of the RC operation to a sequence that is\'split\' along the channel dimension as:\n' +
      '\n' +
      '\\[\\operatorname{RC}\\left(\\left[\\mathbf{X}_{1:T}^{1:(D/2)},\\mathbf{X}_{1:T}^{(D/ 2):D}\\right]\\right):=\\left[\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D} \\right),\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(1:(D/2)}\\right)\\right]=\\left[ \\mathbf{X}_{T:1}^{D:(D/2)},\\mathbf{X}_{T:1}^{(D/2):1}\\right], \\tag{9}\\]\n' +
      '\n' +
      'Note that the RC operation can be \'pulled inside\' of a concat operation:\n' +
      '\n' +
      '\\[\\operatorname{RC}\\circ\\operatorname{concat}\\left(\\left[\\mathbf{X }_{1:T}^{1:(D/2)},\\mathbf{X}_{1:T}^{(D/2):D}\\right]\\right) =\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:D}\\right) \\tag{10}\\] \\[=\\mathbf{X}_{T:1}^{D:1}\\] \\[=\\operatorname{concat}\\left(\\left[\\operatorname{RC}\\left(\\mathbf{ X}_{1:T}^{1:(D/2)}\\right),\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right) \\right]\\right)\\] \\[=\\operatorname{concat}\\circ\\operatorname{RC}\\left(\\left[\\mathbf{ X}_{1:T}^{1:(D/2)},\\mathbf{X}_{1:T}^{(D/2):D}\\right]\\right)\\]\n' +
      '\n' +
      'Additionally, we have that \\(\\operatorname{RC}^{-1}=\\operatorname{RC}\\) and that\n' +
      '\n' +
      '\\[\\operatorname{RC}\\left(\\left[\\mathbf{X}_{1:T}^{1:(D/2)},\\operatorname{RC} \\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right)\\right]\\right)=\\left[\\mathbf{X}_{1:T}^{ (D/2):D},\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right)\\right]. \\tag{11}\\]\n' +
      '\n' +
      'Following Definition 8, we have that:\n' +
      '\n' +
      '\\[\\operatorname{RC}\\circ\\operatorname{M_{\\mathrm{RC},\\theta}}\\left( \\mathbf{X}_{1:T}^{1:D}\\right) =\\operatorname{RC}\\circ\\operatorname{concat}\\left(\\left[ \\operatorname{M}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right),\\operatorname{ RC}\\left(\\operatorname{M}_{\\theta}\\circ\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D} \\right)\\right)\\right]\\right) \\tag{8}\\] \\[=\\operatorname{concat}\\circ\\operatorname{RC}\\left(\\left[ \\operatorname{M}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right),\\operatorname{ RC}\\left(\\operatorname{M}_{\\theta}\\circ\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D} \\right)\\right)\\right]\\right)\\] (10) \\[=\\operatorname{concat}\\left(\\left[\\operatorname{M}_{\\theta}\\circ \\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{(D/2):D}\\right),\\operatorname{RC} \\left(\\operatorname{M}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right) \\right)\\right]\\right)\\] (11) \\[=\\operatorname{concat}\\left(\\left[\\operatorname{M}_{\\theta}\\left( \\mathbf{X}_{T:1}^{D:(D/2)}\\right),\\operatorname{RC}\\left(\\operatorname{M}_{ \\theta}\\circ\\operatorname{RC}\\left(\\mathbf{X}_{T:1}^{(D/2):1}\\right)\\right) \\right]\\right)\\] (6) \\[=\\operatorname{M_{\\mathrm{RC},\\theta}}\\circ\\operatorname{RC}\\left( \\mathbf{X}_{1:T}^{1:D}\\right)\\]\n' +
      '\n' +
      '## Appendix B Proof of Theorem 4.1\n' +
      '\n' +
      'We begin with the following lemma,\n' +
      '\n' +
      '**Lemma B.1**.: _For two RC equivariant sequence operators \\(\\mathrm{F}\\) and \\(\\mathrm{G}\\), their composition \\(\\mathrm{F}\\circ\\mathrm{G}\\) is also equivariant._\n' +
      '\n' +
      '_Proof._ We have that,\n' +
      '\n' +
      '\\[\\mathrm{F}\\left(\\mathrm{G}\\left(\\operatorname{RC}\\left(\\mathbf{X}_{1:T}^{1:D} \\right)\\right)\\right)=\\mathrm{F}\\left(\\operatorname{RC}\\left(\\mathrm{G} \\left(\\mathbf{X}_{1:T}^{1:D}\\right)\\right)\\right)=\\operatorname{RC}\\left( \\mathrm{F}\\left(\\mathrm{G}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)\\right)\\right)\\]\n' +
      '\n' +
      'where each equality follows from the RC equivariance of the operators \\(\\mathrm{G}\\) and \\(\\mathrm{F}\\), respectively.\n' +
      '\n' +
      'Therefore, to prove that the Caduceus-PS is RC equivariant, we need to prove that each operator in \\(\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\circ\\mathrm{M}_{\\mathrm{RC}_{e},\\theta}^{(n) }\\circ\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\) satisfies this property.\n' +
      '\n' +
      'First, we show that \\(\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\) is RC equivariant.\n' +
      '\n' +
      '\\[\\mathrm{RC}\\circ\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{ X}_{1:T}^{1:4}\\right) =\\mathrm{RC}\\circ\\mathrm{concat}\\left(\\left[\\mathrm{Emb}_{\\theta} \\left(\\mathbf{X}_{1:T}^{1:4}\\right),\\mathrm{RC}\\circ\\mathrm{Emb}_{\\theta} \\left(\\mathrm{RC}\\left(\\mathbf{X}_{1:T}^{1:4}\\right)\\right)\\right]\\right) \\tag{10}\\] \\[=\\mathrm{concat}\\left(\\left[\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_ {1:T}^{1:4}\\right),\\mathrm{RC}\\circ\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_{1:T }^{1:4}\\right)\\right)\\right]\\] (11) \\[=\\mathrm{concat}\\left(\\left[\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_ {T:1}^{4:1}\\right),\\mathrm{RC}\\circ\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_{1:T }^{1:4}\\right)\\right)\\right]\\] \\[=\\mathrm{concat}\\left(\\left[\\mathrm{Emb}_{\\theta}\\left(\\mathbf{X}_ {T:1}^{4:1}\\right),\\mathrm{RC}\\circ\\mathrm{Emb}_{\\theta}\\left(\\mathrm{RC}\\left( \\mathbf{X}_{T:1}^{4:1}\\right)\\right)\\right)\\right]\\] \\[=\\mathrm{Emb}_{\\mathrm{RC}_{e},\\theta}\\circ\\mathrm{RC}\\left( \\mathbf{X}_{1:T}^{1:4}\\right)\\]\n' +
      '\n' +
      'Additionally, we have that \\(\\mathrm{M}_{\\mathrm{RC}_{e},\\theta}^{(n)}\\) is equivariant by Theorem 3.1 and induction using Lemma B.1.\n' +
      '\n' +
      'Finally, recall the definition of \\(\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\):\n' +
      '\n' +
      '\\[\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right):= \\mathrm{LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{1:(D/2)}\\right)+\\text{flip\\_chan} \\circ\\mathrm{LM}_{\\theta}\\left(\\mathbf{X}_{1:T}^{D:(D/2)}\\right).\\]\n' +
      '\n' +
      'Note that \\(\\mathrm{LM}_{\\theta}\\) is parameterized by a weight matrix \\(\\mathbf{W}_{\\theta}\\) and applying \\(\\mathrm{LM}_{\\theta}\\) to a sequence \\(\\mathbf{X}_{1:T}^{1:(D/2)}\\) is equivalent to multiplying each of the sequence elements \\(\\mathbf{x}_{t}^{1:(D/2)}\\), for \\(t=1,\\ldots,T\\), on the left by \\(\\mathbf{W}_{\\theta}\\). Therefore if we reverse an input to \\(\\mathrm{LM}_{\\theta}\\) along the length dimension, the output will be reversed along the length dimension as well. We can thus focus on a specific item at position \\(t\\) in a sequence:\n' +
      '\n' +
      '\\[\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\mathbf{X}_{1:T}^{1:D}\\right)_{t}= \\mathbf{W}_{\\theta}\\cdot\\mathbf{x}_{t}^{1:(D/2)}+\\text{flip\\_chan}\\left(\\mathbf{W}_{ \\theta}\\cdot\\mathbf{x}_{t}^{D:(D/2)}\\right),\\]\n' +
      '\n' +
      'and we need only show that it is equivariant with the flip_chan operation, which we recall merely reverses the channels of given input. We note that \\(\\text{flip\\_chan}^{-1}=\\text{flip\\_chan}\\). Now we show that:\n' +
      '\n' +
      '\\[\\text{flip\\_chan}\\left(\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left( \\mathbf{X}_{1:T}^{1:D}\\right)_{t}\\right) =\\text{flip\\_chan}\\left(\\mathbf{W}_{\\theta}\\cdot\\mathbf{x}_{t}^{1:(D/ 2)}\\right)+\\mathbf{W}_{\\theta}\\cdot\\mathbf{x}_{t}^{D:(D/2)}\\] \\[=\\mathrm{LM}_{\\mathrm{RC}_{e},\\theta}\\left(\\text{flip\\_chan} \\left(\\mathbf{X}_{1:T}^{1:D}\\right)\\right)_{t}\\]\n' +
      '\n' +
      'This completes the proof. \n' +
      '\n' +
      '## Appendix C Pre-training\n' +
      '\n' +
      'We provide a more detailed description of the dataset and training methodology used in the human reference genome pre-training task. This dataset is based on the splits used in the previous Enformer study (Awsec et al., 2021). The training split comprises 34,021 segments that we extend to a maximum length of 1,048,576 (\\(2^{20}\\)), collectively covering the genome and amounting to around 35 billion tokens, or nucleotide base pairs.\n' +
      '\n' +
      'All the Mamba-based models, including Caduceus, were trained with a learning rate of 8e\\({}^{-3}\\). We maintain a constant number of tokens in each batch, using \\(2^{20}\\) tokens per batch. For example, for sequence lengths of 1,024, batch size is also 1,024 and for sequence lengths of 131k (\\(2^{17}\\)), batch size is 8. All our models, other than Caduceus-PS, are pre-trained with RC data augmentation, where any given sequence is either unchanged or has the RC operation applied to it with equal probability.\n' +
      '\n' +
      'Models were trained with cosine decay and the ADAM optimization algorithm (Kingma and Ba, 2014), \\(\\beta_{1}\\) and \\(\\beta_{2}\\) values of 0.95 and 0.9, respectively.\n' +
      '\n' +
      'For bi-directional models, we use the masking recipe presented in Devlin et al. (2018). Namely, we\'mask\' 15% of tokens. Of the\'masked\' tokens, 80% are replaced with a special [MASK] token, 10% are replaced with a random token from the vocabulary, and 10% are left unchanged.\n' +
      '\n' +
      'The various Mamba/Caduceus models that were pre-trained are listed in Table 3. For Figure 2(a), we re-pre-train HyenaDNA models on sequence lengths of 1,024, 32k, and 131k. We use the corresponding hidden dimension and depth as those used when these models were originally trained in Nguyen et al. (2023). Other than learning rate, which was set to 6e\\({}^{-4}\\), all the other pre-training details used for our models above were used for HyenaDNA pre-training as well.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '### Nucleotide Transformer Tasks\n' +
      '\n' +
      'For the Nucleotide Transformer Task, we pull baseline results from [https://huggingface.co/spaces/InstaDeepAI/nucleotide_transformer_benchmark](https://huggingface.co/spaces/InstaDeepAI/nucleotide_transformer_benchmark). For our Caduceus / Mamba-based models we follow the same CV protocol from Dalla-Torre et al. (2023) using a 90/10 train/validation split for each fold. Our models consist of 4 layers and hidden dimension 256, roughly matching the parameter count of the reported HyenaDNA model. Models were fine-tuned for 20 epochs. Hyperparameters for the models reported in Table 2 can be found in Table 6\n' +
      '\n' +
      '### Predicting the Effect of Variants on Gene Expression\n' +
      '\n' +
      'Labels for this task represent whether a SNP has a causal effect on gene expression. A positive label is assigned if the causal probability, as determined by the SuSiE (Wang et al., 2020) tool, is \\(>.9\\) (see Avsec\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{Caduceus-Ph} & \\multicolumn{2}{c}{Caduceus-PS} \\\\  & LR & batch size & LR & batch size \\\\ \\hline \\multirow{9}{*}{Histone markers} & H3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k14ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k36me3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k4me1 & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3k4me2 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & H3k79me3 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H3K9ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H4 & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & H4ac & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\ \\hline \\multirow{9}{*}{Regulatory annotation} & Enhancers & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & Enhancers types & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(2\\mathrm{e}^{-3}\\) & 512 \\\\  & Promoter all & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Promoter no tata & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Promoter data & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\ \\hline \\multirow{2}{*}{Splice site} & Splice sites acceptors & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\  & Splice sites all & \\(1\\mathrm{e}^{-3}\\) & 512 & \\(1\\mathrm{e}^{-3}\\) & 512 \\\\  & Splice sites donors & \\(1\\mathrm{e}^{-3}\\) & 128 & \\(1\\mathrm{e}^{-3}\\) & 128 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: MambaDNA MLM and Caduceus Hyperparameter Selection for Nucleotide Transformer Tasks. Caduceus-Ph and Caduceus-PS fine-tuning hyperparameters chosen based on best performance averaged over 10-fold cross-validation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Mamba & Caduceus w/o Equiv. & Caduceus-Ph & Caduceus-PS \\\\ \\hline Mouse Enhancers & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Coding vs. Intergenomic & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human vs. Worm & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human Enhancers Cohn & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human Enhancer Ensembl & \\(2\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) & \\(1\\mathrm{e}^{-3}\\) \\\\ Human Regulatory & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human OCR Ensembl & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ Human NonTATA Promoters & \\(1\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) & \\(2\\mathrm{e}^{-3}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Mamba / Caduceus Hyperparameter Selection for Genomic Benchmarks. Learning rate chosen for its top-1 accuracy averaged over 5-fold cross-validation.\n' +
      '\n' +
      'et al. (2021), where this task was originally proposed, for more details). Chromosomes 9 and 10 are used as the held out test set (see Trop et al. (2023) for more details).\n' +
      '\n' +
      'We follow the methodology presented in Trop et al. (2023) and extract embeddings for each model by taking an average of a 257 bp window centered at the SNP location for both reference and alternative sequences and concatenating along the channel dimension. To each embedding we also concatenate the tissue from which the sequence was assayed.\n' +
      '\n' +
      'We then train an SVM classifier with an RBF kernel on these embeddings for each strata of the data, which is separated by distance to nearest TSS. For each bucket of distance to TSS, we randomly select 5,000 training points, fit the classifier, and record test set AUROC. We repeat this process five times and report mean and +/- of one standard deviation across seeds.\n' +
      '\n' +
      'Hyperparameter optimization was performed for each model within each distance category, focusing on the regularization strength. We select this hyperparameter based on highest mean AUROC reported from 5 random seeds. The regularization strength used for each model reported in Figure 4 are listed in Table 7.\n' +
      '\n' +
      '## Appendix E Assets\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'For pre-training we use the HG38 human reference genome (Consortium et al., 2009). The Genomics Benchmark comes from Gresova et al. (2023). The Nucleotide Transformers benchmark is introduced in Dalla-Torre et al. (2023). The variant effect prediction task data was originally proposed in Avsec et al. (2021) and we use the modified version from Trop et al. (2023).\n' +
      '\n' +
      '### Software and Libraries\n' +
      '\n' +
      'In Table 8, we enumerate the relevant open-source software, and corresponding licenses, used in this work.\n' +
      '\n' +
      '## Appendix F Computational resources\n' +
      '\n' +
      'Model training and inference were run on GPUs with machine type varying by model size during pre-training and downstream tasks. We use 3090, A5000, A6000, V100, and A100 GPUs.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\multicolumn{2}{c}{Distance to Nearest TSS (bp)} \\\\  & \\(0-30\\text{k}\\) & \\(30-100\\text{k}\\) & \\(100\\text{k}+\\) \\\\ \\hline NTv2 (500M) & 1 & 1 & 10 \\\\ HyenaDNA (6.6M) & 1 & 5 & 1 \\\\ Caduceus w/o Equiv (7.7M) & 1 & 5 & 10 \\\\ Caduceus-Ph (7.7M) & 1 & 5 & 1 \\\\ Caduceus-PS (7.7M) & 1 & 1 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Hyperparameter Selection for SVM classifier in Variant Effect Prediction Task. Inverse of the \\(L_{2}\\) regularization weight selected from \\(\\{1,5,10\\}\\) by evaluating average test set AUROC.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Library & License \\\\ \\hline GenomicsBenchmark (Gresova et al., 2023) & Apache 2.0 \\\\ Mamba (Gu and Dao, 2023) & Apache 2.0 \\\\ HuggingFace (Wolf et al., 2019) & Apache 2.0 \\\\ Hydra (Yadan, 2019) & MIT \\\\ HyenADAN (Nguyen et al., 2023) & Apache 2.0 \\\\ NumPy (Harris et al., 2020) & NumPy license \\\\ Matplotlib (Hunter, 2007) & Matplotlib license \\\\ ML Collections & Apache 2.0 \\\\ OmegaConv & BSD 3-Clause \\\\ Pandas (pandas development team, 2020) & BSD 3-Clause “New” or “Revised” \\\\ PyTorch (Paszke et al., 2019) & BSD-3 Clause \\\\ PyTorch Lightning (Falcon and The PyTorch Lightning team, 2019) & Apache 2.0 \\\\ Scikit-Learn (Pedregosa et al., 2011) & BSD 3-Clause \\\\ Seaborn (Waskom, 2021) & BSD 3-Clause “New” or “Revised” \\\\ Triton (Tillet et al., 2019) & MIT \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Open source libraries (and corresponding licenses) used in this work.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>