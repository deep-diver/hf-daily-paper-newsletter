<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '규모가 큰 라벨 데이터 파워를 풀어요.\n' +
      '\n' +
      ' \'{}^{2}\\\'({^{2}^^{2}\\) 샤오강Xu\\({^{3,4}\\,{}^{{1}\\)\\({^^{{.\n' +
      '\n' +
      '거래({}^{2}\\) 홍콩 대학({}^{2}\\)은 TikTok \\({}^{3}\\) 저장 연구소({}^{4}\\).\n' +
      '\n' +
      '해당 저자 성별(이하 저자\\)과 상응하는 저자 성별(이하 저자\\)은 해당 저자 또래(이하 저자\\)에 해당하고 있다.\n' +
      '\n' +
      '[https://depth-anything.github.io](https://depth-anything.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이 작업은 강력한 단안 깊이 추정을 위한 매우 실용적인 솔루션인 제6차 Anything1을 제시한다. 새로운 기술 모듈을 추구하지 않고, 우리는 어떤 상황에서도 모든 이미지를 다루는 단순하면서도 강력한 기반 모델을 구축하는 것을 목표로 한다. 이를 위해 데이터 엔진을 설계하여 데이터 커버리지를 대폭 확대하여 일반화 오류를 줄일 수 있는 대규모 표지되지 않은 데이터(\\(\\ Res\\)(62M)를 수집 및 자동으로 주석하여 데이터셋을 스케일링한다. 데이터 스케일링을 유망한 두 가지 단순하면서도 효과적인 전략을 조사합니다. 먼저, 데이터 증강 도구를 활용하여 보다 도전적인 최적화 대상을 생성한다. 이 모델은 추가 시각적 지식을 적극적으로 모색하고 강력한 표현을 얻기 위해 모델을 구성합니다. 둘째, 사전 훈련된 인코더로부터 풍부한 시맨틱 사제들을 계승하기 위해 모델을 집행하기 위한 보조 감독이 개발된다. 우리는 6개의 공공 데이터 세트와 무작위로 캡처된 사진을 포함하여 제로 샷 능력을 광범위하게 평가한다. 이는 인상적인 일반화 능력을 보여준다(그림 1). 또한 NYUv2와 KITTI의 메트릭 깊이 정보로 미세 조정함으로써 새로운 SOTAs를 설정한다. 우리의 더 나은 깊이 모델은 또한 더 나은 깊이 조절된 제어넷을 초래한다. 저희 모델은 여기에 출시됩니다. + +입니다.\n' +
      '\n' +
      '부츠 †: \\({}^{\\dagger}\\) 이 작업은 강력한 단안 깊이 추정을 위한 매우 실용적인 솔루션인 제Ⅲ Anything1을 제시한다. 새로운 기술 모듈을 추구하지 않고, 우리는 어떤 상황에서도 모든 이미지를 다루는 단순하면서도 강력한 기반 모델을 구축하는 것을 목표로 한다. 이를 위해 데이터 엔진을 설계하여 데이터 커버리지를 대폭 확대하여 일반화 오류를 줄일 수 있는 대규모 표지되지 않은 데이터(\\(\\ Res\\)(62M)를 수집 및 자동으로 주석하여 데이터셋을 스케일링한다. 데이터 스케일링을 유망한 두 가지 단순하면서도 효과적인 전략을 조사합니다. 먼저, 데이터 증강 도구를 활용하여 보다 도전적인 최적화 대상을 생성한다. 이 모델은 추가 시각적 지식을 적극적으로 모색하고 강력한 표현을 얻기 위해 모델을 구성합니다. 둘째, 사전 훈련된 인코더로부터 풍부한 시맨틱 사제들을 계승하기 위해 모델을 집행하기 위한 보조 감독이 개발된다. 우리는 6개의 공공 데이터 세트와 무작위로 캡처된 사진을 포함하여 제로 샷 능력을 광범위하게 평가한다. 이는 인상적인 일반화 능력을 보여준다(그림 1). 또한 NYUv2와 KITTI의 메트릭 깊이 정보로 미세 조정함으로써 새로운 SOTAs를 설정한다. 우리의 더 나은 깊이 모델은 또한 더 나은 깊이 조절된 제어넷을 초래한다. 저희 모델은 여기에 출시됩니다. + +입니다.\n' +
      '\n' +
      '부츠 †: \\({}^{\\dagger}\\) 이 작업은 강력한 단안 깊이 추정을 위한 매우 실용적인 솔루션인 제Ⅲ Anything1을 제시한다. 새로운 기술 모듈을 추구하지 않고, 우리는 어떤 상황에서도 모든 이미지를 다루는 단순하면서도 강력한 기반 모델을 구축하는 것을 목표로 한다. 이를 위해 데이터 엔진을 설계하여 데이터 커버리지를 대폭 확대하여 일반화 오류를 줄일 수 있는 대규모 표지되지 않은 데이터(\\(\\ Res\\)(62M)를 수집 및 자동으로 주석하여 데이터셋을 스케일링한다. 데이터 스케일링을 유망한 두 가지 단순하면서도 효과적인 전략을 조사합니다. 먼저, 데이터 증강 도구를 활용하여 보다 도전적인 최적화 대상을 생성한다. 이 모델은 추가 시각적 지식을 적극적으로 모색하고 강력한 표현을 얻기 위해 모델을 구성합니다. 둘째, 사전 훈련된 인코더로부터 풍부한 시맨틱 사제들을 계승하기 위해 모델을 집행하기 위한 보조 감독이 개발된다. 우리는 6개의 공공 데이터 세트와 무작위로 캡처된 사진을 포함하여 제로 샷 능력을 광범위하게 평가한다. 이는 인상적인 일반화 능력을 보여준다(그림 1). 또한 NYUv2와 KITTI의 메트릭 깊이 정보로 미세 조정함으로써 새로운 SOTAs를 설정한다. 우리의 더 나은 깊이 모델은 또한 더 나은 깊이 조절된 제어넷을 초래한다. 저희 모델은 여기에 출시됩니다. + +입니다.\n' +
      '\n' +
      '부츠 †: \\({}^{\\dagger}\\) 이 작업은 강력한 단안 깊이 추정을 위한 매우 실용적인 솔루션인 제Ⅲ Anything1을 제시한다.\n' +
      '\n' +
      '깊이 센서로부터의 라벨링된 이미지들, 우리의 사용된 단안 표지되지 않은 이미지들은 (i) (_단순하고_획득할 값) 모노클러 이미지들이 거의 모든 곳에서 존재하므로, 특화된 디바이스들을 필요로 하지 않고 수집하기 쉽다. (ii)(_diverse_) 모노클러 이미지는 모델 일반화 능력과 확장성에 중요한 광범위한 장면을 커버할 수 있다. (iii) (_easy to 주석은_) 우리는 단순히 미리 학습된 MDE 모델을 사용하여 표지되지 않은 이미지에 대한 깊이 라벨을 할당할 수 있으며, 이는 피드포워드 단계만 취한다. 효율적이기보다는 LiDAR[18]보다 더 밀도가 높은 깊이 맵을 생성하고 계산 집약적인 스테레오 매칭 과정을 생략한다.\n' +
      '\n' +
      '우리는 표지되지 않은 이미지에 대한 깊이 주석을 자동으로 생성하기 위해 데이터 엔진을 설계하여 임의의 척도로 데이터 스케일링을 가능하게 한다. 8개의 공공 대규모 데이터 세트, _e.g_, SA-1B[27], 오픈 이미지[30], BDD100K[81]에서 62M 다양한 정보 이미지를 수집한다. 우리는 어떤 형태의 라벨 없이도 그들의 원시 표지되지 않은 이미지를 사용합니다. 그런 다음 표지되지 않은 이미지에 대한 신뢰할 수 있는 주석 도구를 제공하기 위해 6개의 공개 데이터세트로부터 1.5M 표지된 이미지를 수집하여 초기 MDE 모델을 훈련시킨다. 그런 다음 표지되지 않은 이미지는 자동으로 주석을 달았고 자체 훈련 방식[31]에서 라벨이 붙은 이미지와 공동으로 학습된다.\n' +
      '\n' +
      '앞서 언급한 단안 표지되지 않은 이미지의 장점에도 불구하고, 특히 충분한 라벨링된 이미지와 강력한 사전 훈련 모델의 경우 이러한 대규모 표지되지 않은 이미지[72, 89]를 긍정적으로 사용하는 것은 실제로 사소한 것이 아니다. 우리의 예비 시도에서 라벨이 붙은 이미지와 유사 라벨이 붙은 이미지를 직접 결합하면 라벨이 붙은 이미지만 사용하는 기준선이 개선되지 못했다. 우리는 그러한 순진한 자기 교수 방식으로 습득한 추가적인 지식이 다소 제한적이라는 것을 추측한다. 딜레마를 해결하기 위해 의사 라벨을 학습할 때 더 어려운 최적화 대상으로 학생 모델에 도전할 것을 제안한다. 학생 모델은 별도의 시각적 지식을 모색하고 다양한 강력한 섭동 하에 강력한 표현을 학습하여 비선별 이미지를 더 잘 처리하기 위해 시행된다.\n' +
      '\n' +
      '더 나아가 MDE에 대한 보조 시맨틱 세분화 과제의 이익을 보여주는 작품[9, 21]이 일부 있었다. 우리는 또한 우리의 모델을 더 나은 수준의 장면 이해 능력으로 갖추는 것을 목표로 이 연구 라인을 따른다. 그러나 MDE 모델이 이미 충분히 강력할 때 그러한 보조 작업은 더 많은 이득을 가져오기 어렵다. 우리는 이미지를 이산 클래스 공간으로 디코딩할 때 의미 정보에서 심각한 손실 때문이라고 추측한다. 따라서 의미 관련 작업에서 DINOv2의 우수한 성능을 고려할 때 단순한 특징 정렬 손실로 풍부한 의미주의자를 그로부터 유지할 것을 제안한다. 이는 MDE 성능을 향상시킬 뿐만 아니라 중간 수준 및 상위 인식 과제 모두에 대해 다중 태스크 인코더를 생성한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다.\n' +
      '\n' +
      '* MDE에 대한 대규모, 값싸고 다양한 표지되지 않은 이미지의 데이터 스케일링 업의 값을 강조한다.\n' +
      '* 우리는 대규모 라벨링 및 표지되지 않은 이미지를 공동으로 훈련하는 데 있어 핵심 관행을 지적한다. 원시 표지되지 않은 이미지를 직접 배우는 대신 추가 지식을 위한 더 단단한 최적화 표적으로 모델에 도전한다.\n' +
      '* 우리는 보조 시맨틱 세분화 작업을 사용하는 것보다 더 나은 장면 이해를 위해 사전 훈련된 인코더로부터 풍부한 시맨틱 사제들을 계승할 것을 제안한다.\n' +
      '* 우리 모델은 MiDaS-BEiT\\({}_{\\text{L-512}}\\)[5]보다 더 강한 제로 샷 능력을 나타낸다. 또한 측정 깊이로 미세 조정되어 조이페트 [4]를 크게 능가한다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '** 단핵 깊이 추정*** 초기 작품[23, 36, 50]은 주로 수작업 기능 및 전통적인 컴퓨터 비전 기술에 의존했다. 그들은 명시적인 깊이 신호에 의존하여 제한되었고 교합 및 질감 없는 영역으로 복잡한 장면을 처리하기 위해 고군분투했다.\n' +
      '\n' +
      '딥러닝 기반 방법은 섬세하게 주석이 달린 데이터세트[18, 54]에서 깊이 표현을 효과적으로 학습함으로써 단안 깊이 추정에 혁명을 일으켰다. 에겐 _et al_[17]] 먼저 깊이를 되찾기 위해 다중 규모의 융합 네트워크를 제안했다. 이에 따라 많은 작품은 회귀 과제를 분류 과제[3, 34]로 신중하게 설계하여 깊이 추정 정확도를 지속적으로 향상시켰으며, 더 많은 제사[32, 53, 75, 82], 더 나은 객관적 기능[67, 77], _etc_를 소개한다. 유망한 성과에도 불구하고, 그들은 도메인을 일반화하기 어렵다.\n' +
      '\n' +
      '**Zero 샷 깊이 추정*** 우리 작업은 이 연구 라인에 속한다. 다양한 훈련 세트를 가진 MDE 모델을 훈련시키는 것을 목표로 하므로 주어진 이미지에 대한 깊이를 예측할 수 있다. 일부 선구적 작품[10, 66]은 더 많은 훈련 이미지를 수집하여 이러한 방향을 탐색했지만, 이들의 감독은 매우 희박하며 제한된 포인트 쌍에서만 시행된다.\n' +
      '\n' +
      '효과적인 다중-다타세트 관절 훈련을 가능하게 하기 위해 이정표 작업 MiDaS[45]는 잠재적으로 다른 깊이 척도를 무시하고 다양한 데이터 세트를 가로질러 이동하기 위해 아핀-불변 손실을 사용한다. 따라서 MiDaS는 상대적인 깊이 정보를 제공한다. 최근에는 일부 작품[4, 22, 78]이 측정 깊이를 추정하기 위해 한 걸음 더 나아가게 된다. 그러나 실무에서 이러한 방법은 미다S, 특히 최신 버전[5]보다 더 낮은 일반화 능력을 나타낸다. 또한 조이페트[4]에서 입증된 바와 같이 강한 상대 깊이 추정 모델은 메트릭 깊이 정보로 미세 조정함으로써 일반화된 메트릭 깊이 추정에서도 잘 작동할 수 있다. 따라서 우리는 여전히 상대적인 깊이 추정에서 MiDaS를 따르지만 대규모 단안 표지되지 않은 이미지의 값을 강조함으로써 더욱 강화한다.\n' +
      '\n' +
      '라벨링되지 않은 데이터****은 다양한 애플리케이션[70, 74]에 인기 있는 반지도 학습[31, 55, 89]의 연구 영역에 속한다. 그러나 기존웍스는 일반적으로 제한된 이미지만 사용할 수 있다고 가정한다. 이미 표지된 이미지가 충분하지만 더 큰 규모의 표지되지 않은 이미지도 있는 도전적이지만 현실적인 시나리오를 거의 고려하지 않는다. 우리는 제로샷 MDE를 위해 이 도전적인 방향을 취합니다. 우리는 표지되지 않은 이미지가 데이터 범위를 크게 향상시킬 수 있으므로 모델 일반화 및 견고성을 향상시킬 수 있음을 보여준다.\n' +
      '\n' +
      '세 번째.\n' +
      '\n' +
      '우리의 작업은 더 나은 단안 깊이 추정(MDE)을 용이하게 하기 위해 라벨이 붙은 이미지와 표지되지 않은 이미지를 모두 사용한다. 형식적으로, 표지된 세트 및 표지되지 않은 세트는 각각 \\(\\mathcal{D}^{l}=\\{(x_{i},d_{i})\\}_{i =1}^{M}\\) 및 \\(\\mathcal{D}^{u}=\\{u_{i}_{i}_{i}1}.{i}_{i}1}. 우리는 \\(\\mathcal{D}^{l}\\)에서 교사 모델 \\(T\\)을 배우는 것을 목표로 한다. 그런 다음 \\(T\\)를 사용하여 \\(\\mathcal{D}^{u}\\)에 대한 유사 깊이 라벨을 할당한다. 마지막으로 라벨이 붙은 세트와 의사 라벨이 붙은 세트의 조합으로 학생 모델 \\(S\\)을 훈련합니다. 그림 2에 간략한 일러스트가 제공됩니다.\n' +
      '\n' +
      '뷰티 L 라벨 검색.\n' +
      '\n' +
      '이 과정은 MiDaS[5, 45]의 훈련과 유사하다. 그러나 미다스가 코드를 출시하지 않았기 때문에 먼저 재현했습니다. 구체적으로 깊이 값은 먼저 \\(d=1/t\\)에 의해 시차 공간으로 형질전환된 다음 각 깊이 맵에서 0\\(\\ason\\)1로 정규화된다. 다중-다타셋 공동 교육을 가능하게 하기 위해 미지의 규모와 각 샘플의 이동을 무시하기 위해 아핀-불변 손실을 채택한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{l}=\\frac{1}{HW}\\sum_{i=1}^{HW}\\rho(d_{i}^{*},d_{i}), \\tag{1}\\]\n' +
      '\n' +
      'I\\(d_{i}^{*}\\)와 \\(d_{i}\\)는 각각 예측 및 근거 진실이다. 그리고\\(d_{i}},d_{i}})는 O\\(d_{i}},d_{i}}) 및 \\(d_{i}}}})의 스케일링 및 시프트된 버전으로,\\(d_{i}}}.\n' +
      '\n' +
      '\\[\\hat{d}_{i}=\\frac{d_{i}-t(d)}{s(d)}, \\tag{2}\\]\n' +
      '\n' +
      '(t(d)\\)와 \\(s(d)\\)는 예측 및 근거 진리를 정렬하여 0개의 번역 및 단위 척도를 갖는 데 사용된다.\n' +
      '\n' +
      '\\[t(d)=\\text{median}(d),\\ \\ s(d)=\\frac{1}{HW}{HW}\\sum_{i=1}^{HW}|d_{i}-t(d)|.\n' +
      '\n' +
      '강력한 단안 깊이 추정 모델을 얻기 위해 6개의 공개 데이터세트로부터 1.5M 표지된 이미지를 수집한다. 이러한 데이터 세트의 세부 사항은 표 1에 나열되어 있기 때문에 NYUv2 [54] 및 KITTI [18] 데이터세트보다 더 적은 표지 데이터 세트를 사용하여 NYUv2 [54] 및 KITTI [18] 데이터세트를 사용하지 않기 때문에 일부 데이터 세트를 사용할 수 없다. 더 적은 라벨이 붙은 이미지를 사용했음에도 불구하고, 쉽게 구할 수 있고 다양한 표지되지 않은 이미지는 데이터 커버리지를 이해하고 모델 일반화 능력과 견고성을 크게 향상시킬 것이다.\n' +
      '\n' +
      '또한, 이러한 표지된 이미지에서 배운 교사 모델 \\(T\\)을 강화하기 위해 우리는 인코더를 초기화하기 위해 전처리된 가중치를 DINOv2[42]를 채택한다. 실무적으로 스카이라이드를 탐지하기 위해 사전 훈련된 시맨틱 세분화 모델[69]을 적용하고, 그 시차값을 0(최고)으로 설정하였다.\n' +
      '\n' +
      '###, 운표지 이미지 파워를 해제한다.\n' +
      '\n' +
      '이것은 우리 작품의 주요 포인트입니다. 다양한 라벨이 붙은 데이터 세트를 효율적으로 구성하는 이전 작업에서 구별하여 데이터 커버리지를 향상시키는 데 표지되지 않은 이미지의 값을 강조한다. 오늘날 우리는 다양한 작업의 인터넷 또는 공공 데이터 세트로부터 다양하고 대규모 라벨링되지 않은 세트를 실질적으로 구축할 수 있다. 또한, 우리는 단안 표지되지 않은 이미지의 조밀한 깊이 맵을 미리 훈련된 잘 형성된 MDE 모델에 포워딩하기만 하면 쉽게 얻을 수 있다. 이것은 스테레오 이미지 또는 비디오를 위한 스테레오 매칭 또는 SfM 재구성을 수행하는 것보다 훨씬 편리하고 효율적입니다. 다양한 장면을 위한 라벨이 없는 소스로 8개의 대규모 공공 데이터 세트를 선택합니다. 총 62M 이상의 이미지가 포함되어 있습니다. 자세한 내용은 표 1의 하단 절반에 나와 있다.\n' +
      '\n' +
      '기술적으로, 이전에 얻은 MDE 교사 모델 \\(T\\)를 감안할 때, 우리는 비표지 집합 \\(\\mathcal{D}^{u}\\)에 대한 예측을 수행하여 유사 라벨 집합 \\(\\hat{\\mathcal{D}}^{u}\\)를 얻었다.\n' +
      '\n' +
      '\\[\\hat{\\mathcal{D}}^{u}=\\{(u_{i},T(u_{i}))|u_{i}\\in\\mathcal{D}^{u}\\}_{i=1}^{N}. \\tag{4}\\]\n' +
      '\n' +
      '결합 세트 \\(\\mathcal{D}^{l}\\cup\\hat{\\mathcal{D}}^{u}\\\\}\\)로 표지된 이미지와 유사 표지된 이미지를 사용하여 학생 모델 \\(S\\)을 훈련시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline Dataset & Indoor & Outdoor & Label & \\# Images \\\\ \\hline \\multicolumn{5}{c}{Labeled Datasets} \\\\ \\hline BlendedMVS [76] & ✓ & ✓ & Stereo & 115K \\\\ DIML [13] & ✓ & ✓ & Stereo & 927K \\\\ HRWSI [67] & ✓ & ✓ & Stereo & 20K \\\\ IRS [61] & ✓ & & Stereo & 103K \\\\ MegaDepth [33] & & ✓ & SfM & 128K \\\\ TartanAir [62] & ✓ & ✓ & Stereo & 306K \\\\ \\hline \\multicolumn{5}{c}{Unlabeled Datasets} \\\\ \\hline BDD100K [81] & & ✓ & None & 8.2M \\\\ Google Landmarks [64] & & ✓ & None & 4.1M \\\\ ImageNet-21K [49] & ✓ & ✓ & None & 13.1M \\\\ LSUN [80] & ✓ & & None & 9.8M \\\\ Objects365 [52] & ✓ & ✓ & None & 1.7M \\\\ Open Images V7 [30] & ✓ & ✓ & None & 7.8M \\\\ Places365 [87] & ✓ & ✓ & None & 6.5M \\\\ SA-1B [27] & ✓ & ✓ & None & 11.1M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 총 1.5M 라벨이 붙은 이미지와 **62M 표지되지 않은 이미지***를 공동으로 학습했다.\n' +
      '\n' +
      '종래 작품[73]에 이어 \\(T\\)에서 미세 조정(S\\) 대신 더 나은 성능을 위해 \\(S\\)를 재분류한다.\n' +
      '\n' +
      '불행히도, 파일럿 연구에서 우리는 이러한 자가 훈련 파이프라인으로 개선을 얻지 못했고, 이는 표지화된 이미지가 몇 개일 때 관찰과 모순된다[55] 우리의 경우에 이미 충분한 라벨링된 이미지로 추가 표지되지 않은 이미지로부터 획득한 추가 지식이 다소 제한적이라는 것을 추측한다. 특히 교사와 학생이 동일한 사전 훈련과 아키텍처를 공유하는 것을 고려할 때 명시적인 자기 훈련 절차가 없어도 라벨이 없는 집합 \\(\\mathcal{D}^{u}\\)에 대해 유사하거나 잘못된 예측을 하는 경향이 있다.\n' +
      '\n' +
      '딜레마를 해결하기 위해 표지되지 않은 이미지에 대한 추가 시각적 지식에 대한 보다 어려운 최적화 대상으로 학생들에게 도전할 것을 제안한다. 훈련 중 표지되지 않은 이미지에 강력한 섭동을 주입합니다. 학생 모델을 구성하여 이러한 표지되지 않은 이미지에서 별도의 시각적 지식을 적극적으로 모색하고 불변 표현을 획득한다. 이러한 장점은 우리의 모델이 열린 세상을 보다 견고하게 다루는 데 도움이 됩니다. 우리는 두 가지 형태의 섭동을 도입하는데, 하나는 색 조깅과 가우시안 블러링을 포함한 강한 색 왜곡이고, 다른 하나는 CutMix[83]인 강력한 공간 왜곡이다. 단순함에도 불구하고 두 가지 수정으로 인해 대규모 표지되지 않은 이미지가 라벨링된 이미지의 기준선을 크게 향상시킵니다.\n' +
      '\n' +
      '컷믹스에 대한 자세한 정보를 제공합니다. 원래 이미지 분류를 위해 제안되었으며 단안 깊이 추정에서는 거의 조사되지 않았다. 먼저 임의의 한 쌍의 표지되지 않은 이미지 \\(u_{a}\\)와 \\(u_{b}\\)를 공간적으로 보간한다.\n' +
      '\n' +
      '<u_{ab}=u_{a}\\odot M+u_{b}\\odot(1-M), \\tag{5}\\]]\n' +
      '\n' +
      '여기서 \\(M\\)는 직사각형의 영역이 1로 설정된 이진 마스크이다.\n' +
      '\n' +
      '표시되지 않은 손실 \\(\\mathcal{L}_{u}\\)는 각각 \\(M\\) 및 \\(1-M\\)에 의해 정의된 유효한 영역에서 첫 번째 컴퓨팅 아핀-불변 손실에 의해 획득된다.\n' +
      '\n' +
      '\\ S(u_{b})는\\[\\mathcal{L} <\\mathcal{1-M}]} (u_{ab})\\rho\\{{(u_{b})\\o.\n' +
      '\n' +
      '단순함을 위해 \\(\\sum\\)와 픽셀 첨자(i\\)를 생략하는 곳. 그런 다음 가중 평균화를 통해 두 손실을 집계합니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{u}=\\frac{\\sum M}{HW}{HW}{HW}_{u}^mathcal{L}_{u}.\n' +
      '\n' +
      '우리는 50% 확률로 CutMix를 사용합니다. 컷믹스에 대한 표지되지 않은 이미지는 이미 색으로 강하게 왜곡되지만 의사 표시를 위해 교사 모델 \\(T\\)에 공급되는 표지되지 않은 이미지는 왜곡 없이 깨끗하다.\n' +
      '\n' +
      '### Semantic-Assisted Perception\n' +
      '\n' +
      '일부 작품[9, 21, 28, 71]은 보조 시맨틱 세분화 과제로 깊이 추정을 개선한다. 우리는 이러한 고수준의 의미 관련 정보로 우리의 깊이 추정 모델을 팔로우하는 것이 유익하다고 믿는다. 또한, 표지되지 않은 이미지를 활용하는 우리의 특정 맥락에서, 다른 작업으로부터의 이러한 보조 감독 신호는 또한 의사 깊이 라벨의 잠재적 소음을 퇴치할 수 있다.\n' +
      '\n' +
      '따라서 RAM[85] + 그로텐션DINO[37] + HQ-SAM[26] 모델의 조합으로 표지되지 않은 이미지에 의미 세분화 라벨을 조심스럽게 할당함으로써 초기 시도를 했다. 후처리 후, 이것은 4K 클래스를 포함하는 클래스 공간을 산출한다. 공동 훈련 단계에서는 공유 인코더와 2개의 개별 디코더로 깊이 및 세분화 예측을 모두 생성하기 위해 모델을 시행한다. 불행히도 시행착오 후, 우리는 여전히 원래 MDE 모델의 성능을 높일 수 없었다. 우리는 이미지를 이산적인 클래스 공간으로 디코딩하는 것이 실제로 너무 많은 의미 정보를 상실한다고 추측했다. 이러한 의미 마스크의 제한된 정보는 특히 깊이 모델이 매우 경쟁력 있는 결과를 확립했을 때 깊이 모델을 더욱 증가시키기 어렵다.\n' +
      '\n' +
      '따라서 우리는 깊이 추정 과제에 대한 보조 감독 역할을 하기 위해 더 유익한 의미 신호를 찾는 것을 목표로 한다. 우리는 시맨틱 관련 작업, _e.g_, 이미지 검색 및 의미 세분화에서 DINOv2 모델[42]의 강력한 성능에 의해 크게 놀라운데, 이는 미세 조정 없이 동결된 가중치가 있음에도 불구하고 매우 놀라운 것이다. 이러한 단서에 의해 활성화된 강력한 의미 능력을 당사로 이전할 것을 제안한다.\n' +
      '\n' +
      '그림 2: 우리 파이프라인. 정선: 표지된 이미지의 흐름, 점선: 표지되지 않은 이미지의 흐름이다. 우리는 특히 대규모 표지되지 않은 이미지의 가치를 강조한다. S는 강력한 섭동을 추가하는 것을 나타낸다(섹션 3.2). 깊이 추정 모델을 풍부한 시맨틱 제자로 갖추기 위해 온라인 학생 모델과 냉동 인코더 사이의 보조 제약을 시행하여 의미 능력을 보존한다(섹션 3.3).\n' +
      '\n' +
      '보조 특징 정렬 손실을 갖는 심층 모델을 포함한다. 특징 공간은 고차원적이고 연속적이어서 이산 마스크보다 더 풍부한 의미 정보를 포함하고 있다. 특징 정렬 손실은 그대로 공식화됩니다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{feat}=1-\\frac{1}{HW}\\sum_{i=1}^{HW}\\cos(f_{i},f_{i}^{\\prime}), \\tag{9}\\]\n' +
      '\n' +
      '두 특징 벡터 사이의 코사인 유사도를 측정하는데, 여기서 \\(\\cos(\\cdot,\\cdot)\\)는 두 특징 벡터 간의 코사인 유사성을 측정한다. (f\\)는 깊이 모델 \\(S\\)에 의해 추출된 특징이고, \\(f^{\\prime}\\)는 냉동 DINOV2 인코더의 특징이다. 우리는 무작위로 초기화된 프로젝터가 큰 정렬 손실을 초기 단계에서 전체 손실을 지배하기 때문에 온라인 특징 \\(f\\)을 정렬을 위한 새로운 공간으로 투영하기 위해 일부 작품[19]을 따르지 않는다.\n' +
      '\n' +
      '특징 정렬의 또 다른 핵심 포인트는 DINOV2와 같은 시맨틱 인코더가 물체의 다른 부분, _e.g_, 자동차 전면 및 후방에 대해 유사한 특징을 생성하는 경향이 있다는 것이다. 그러나 깊이 추정에서 동일한 부분 내의 서로 다른 부분 또는 심지어 픽셀은 다양한 깊이를 가질 수 있다. 따라서 _exhaustively_는 냉동 인코더와 정확히 동일한 기능을 생성하기 위해 우리의 깊이 모델을 집행하는 것이 유익하지 않다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 특징 정렬에 대한 내성 마진 \\(\\alpha\\)을 설정했다. I\\(f_{i}\\) 및 \\(f_{i}^{\\prime}\\)의 코사인 유사성이\\(\\alpha\\)를 초과했다면, 이 픽셀은\\(\\mathcal{L}_{feat}\\)에서 고려되지 않는다. 이를 통해 우리의 방법은 DINOV2로부터의 의미 인식 표현과 깊이 감독으로부터 부분 수준의 판별 표현을 모두 즐길 수 있다. 부작용으로서, 우리의 생산된 인코더는 하류 MDE 데이터셋에서 잘 수행될 뿐만 아니라 의미 세분화 작업에서도 강력한 결과를 얻을 수 있다. 또한 중간 수준 및 고수준의 인식 작업 모두에 대한 보편적인 다중 태스크 인코더 역할을 할 수 있는 인코더의 잠재력을 나타낸다.\n' +
      '\n' +
      '마지막으로, 우리의 전반적인 손실은 3개의 손실 \\(\\mathcal{L}_{l}\\), \\(\\mathcal{L}_{u}\\) 및 \\(\\mathcal{L}_{feat}\\)의 평균 조합이다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '특징 추출을 위해 DINOV2 인코더[42]를 채택합니다. 미다S[45, 5]에 이어 깊이 회귀를 위해 DPT[46] 디코더를 사용한다. 모든 라벨이 붙은 데이터 세트는 재샘플링 없이 단순히 함께 결합됩니다. 첫 번째 단계에서는 20개의 epochs에 대해 라벨링된 이미지에 대한 교사 모델을 훈련한다. 공동 훈련의 2단계에서는 모든 표지되지 않은 이미지에 걸쳐 한 번 스윕할 학생 모델을 훈련한다. 표지되지 않은 이미지는 ViT-L 인코더가 있는 가장 잘 형성된 교사 모델에 의해 주석을 달았다. 표지된 이미지 및 표지되지 않은 이미지의 비율은 각 배치에서 1:2로 설정된다. 두 단계 모두 사전 훈련된 인코더의 기본 학습률을 5e-6으로 설정한 반면, 랜덤 초기화된 디코더는 10\\(CSI) 더 큰 학습률을 사용한다. 우리는 AdamW 최적화를 사용하고 선형 일정으로 학습률을 떨어뜨린다. 우리는 라벨링된 이미지에 대한 데이터 증강으로 수평 플러핑만 적용합니다. 특징 정렬 손실에 대한 내성 마진(\\alpha\\)은 0.15로 설정되며 자세한 내용은 부록을 참조하십시오.\n' +
      '\n' +
      '제로-쇼트 차별화 제9회 권고 검토\n' +
      '\n' +
      '앞서 언급한 바와 같이, 이 작업은 임의의 이미지에 대한 정확한 깊이 추정을 제공하는 것을 목표로 한다. 따라서 6개의 대표적인 비엔 데이터 세트: KITTI[18], NYUv2[54], Sintel[7], DDAD[20], ETH3D[51], DIODE[59]에서 제0차 검색 모델의 제로 샷 깊이 추정 능력을 종합적으로 검증한다. 우리는 우리의 것보다 더 표지된 이미지를 사용하는 최신 MiDaS v3.1 [5]의 최고의 DPT-BEiT\\({}_{\\text{L-512}\\) 모델과 비교한다. 표 2에서 볼 수 있듯이, 우리의 제6차 아나이는 ViT-L 인코더를 사용하여 모두 광범위한 장면에 걸쳐 가장 강력한 MiDaS 모델을 크게 능가한다: \\(|d^{*}-d|/d\\)) 및 \\(\\delta_{1}\\)의 농도(\\max(d^{*}/d,d/d^{*}/d^{*})<1.25\\) 메트릭과 같다. 예를 들어, 잘 알려진 자율주행 데이터셋 DDAD [20]에서 테스트했을 때, 우리는 0.251 \\(팔라로우\\) 0.230에서 Abs연관(\\(\\(\\downarrow\\)을 개선하고 0.766 \\(\\) 0.789에서 \\(\\(\\(\\){1}\\)를 개선한다.\n' +
      '\n' +
      '또한, 우리의 ViT-B 모델은 훨씬 더 큰 ViT-L을 기반으로 한 MiDaS보다 이미 분명히 우수하다. 더욱이, 미다S 모델의 1/10 미만인 우리의 ViT-S 모델은 신델, DDAD 및 ETH3D를 포함한 여러 비세엔 데이터 세트에서도 MiDaS를 능가한다. 이러한 소규모 모델의 성능 이점은 계산적으로 제한된 시나리오에서 큰 잠재력을 보여준다.\n' +
      '\n' +
      '가장 널리 사용되는 MDE에서 가장 널리 사용되는 MDE에도 주목할 필요가 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Encoder} & KITTI [18] & \\multicolumn{2}{c}{NYUv2 [54]} & \\multicolumn{2}{c}{Sintel [7]} & \\multicolumn{2}{c}{DDAD [20]} & ETH3D [51] & \\multicolumn{2}{c}{DIODE [59]} \\\\ \\cline{3-14}  & & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) \\\\ \\hline MiDaS v3.1 [5] & ViT-L & 0.127 & 0.850 & 0.048 & 0.980 & 0.587 & 0.699 & 0.251 & 0.766 & 0.139 & 0.867 & 0.075 & 0.942 \\\\ \\hline \\multirow{3}{*}{**Depth Anything**} & ViT-S & 0.080 & 0.936 & 0.053 & 0.972 & 0.464 & 0.739 & 0.247 & 0.768 & 0.127 & **0.885** & 0.076 & 0.939 \\\\  & ViT-B & 0.080 & 0.939 & 0.046 & 0.979 & **0.432** & 0.756 & 0.232 & 0.786 & **0.126** & 0.884 & 0.069 & 0.946 \\\\ \\cline{1-1}  & ViT-L & **0.076** & **0.947** & **0.043** & **0.981** & 0.458 & **0.760** & **0.230** & **0.789** & 0.127 & 0.882 & **0.066** & **0.952** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Zero 샷 상대 깊이 추정. Better:** AbsRel \\(\\downarrow\\),\\(\\delta_{1}\\uparrow\\) 우리는 MiDaS v3.1. 노트에서 미다S _does not_가 KITTI 및 NYUv2에 대한 제로샷 평가를 엄격하게 따르지 않는다는 최고의 모델과 비교하는데, 이는 그들의 훈련 이미지를 사용하기 때문이다. 우리는 ViT-S(24.8M), ViT-B(97.5M), ViT-L(335.3M)을 기반으로 각각 다른 목적을 위한 세 가지 모델 척도를 제공한다. 가장 좋은 결과, 두 번째로 좋은 결과입니다.\n' +
      '\n' +
      '벤치마크 KITTI 및 NYUv2는 MiDaS v3.1이 해당 훈련 이미지(더 이상_not 제로샷)를 사용하지만, 우리의 예타 Anything는 KITTI 또는 NYUv2 이미지_, _e로 훈련하지 않고 여전히 ____e보다 더 우수하다. Abs관계는 0.076, Abs관계는 0.850 _vs._.076, Abs는 0.850 _vs._.076이다. KITTI의 \\(\\delta_{1}\\)에서 0.947이다.\n' +
      '\n' +
      '그것은 _Metric_97번째 권고 검토에 의해 조정되었다.\n' +
      '\n' +
      '0-샷 상대 깊이 추정에서 인상적인 성능 외에도 다운스트림 _metric_ 깊이 추정을 위한 유망한 중량 초기화로서 제6차 Anything 모델을 추가로 조사한다. 우리는 미리 훈련된 인코더 파라미터로 하류 MDE 모델의 인코더를 초기화하고 디코더를 무작위로 초기화했다. 모델은 수정 메트릭 깊이 정보로 미세 조정됩니다. 이 부분에서는 ViT-L 인코더를 사용하여 미세 조정합니다.\n' +
      '\n' +
      '모델이 동일한 도메인(섹션 4.3.1)에서 학습되고 평가된 두 가지 대표적인 시나리오: 1) _in- 도메인_ 메트릭 깊이 추정을 조사하며, 여기서 모델은 하나의 도메인, _e_e_g_, NYUv2[54]에서 훈련되지만 서로 다른 도메인, _e_e._e. SUN RGB-D[56](섹션 4.3.2)에서 평가된 동일한 도메인(섹션 4.3.2)에서 평가되었다.\n' +
      '\n' +
      '4.3.1.1 In-Domain Metricizationth Estimationth Estimationth Estimationth Metricric##########.\n' +
      '\n' +
      'NYUv2 [54]의 표 3에서 볼 수 있듯이, 우리의 모델은 0.964 \\(비우타로우\\) 0.984에서 \\(\\(\\-uparrow\\) 및 Abs관계는 0.069에서 0.056까지 관찰될 수 있다. 우리는 0.978 \\(팔라로우\\) 0.982에서 KITTI에 대한 \\(\\(\\(\\(\\-uparrow\\)를 개선하며, 비교적 기본 깊이 모델을 사용하여 이 시나리오에 대한 조비제 프레임워크를 채택한다는 점에 주목할 필요가 있으며, 더 발전된 아키텍처를 장착하면 결과가 더욱 향상될 수 있다고 믿는다.\n' +
      '\n' +
      '4.3.2제로-쇼트 메타릭 제1차 추계 추계 평가#### 4.3.2제로-쇼트 메타릭 제2의 계산####\n' +
      '\n' +
      '우리는 제로 샷 메트릭 깊이 추정을 수행하기 위해 조비제[4]를 따른다. 미다스는 NYUv2[54](실내 장면의 경우) 또는 KITTI[18](야외 장면의 경우)의 메트릭 깊이 정보가 있는 미다S 사전 훈련 인코더를 미세 조정한다. 따라서 우리는 미다S 인코더를 더 나은 제6안티 인코더로 대체하여 다른 구성 요소를 변하지 않는다. 표 5에서 보는 바와 같이 실내외 장면 및 실외 장면의 광범위한 비선 데이터 세트에 걸쳐 우리의 제6차 검색은 MiDaS를 기반으로 하는 원래 조예프보다 더 나은 측정 깊이 추정 모델을 초래한다.\n' +
      '\n' +
      '잘 먹었지.\n' +
      '\n' +
      '우리의 방법에서 우리는 단순한 특징 정렬 제약을 통해 미리 학습된 인코더로부터 풍부한 의미론적 사제들을 계승하기 위해 MDE 모델을 설계한다. 여기에서 우리는 MDE 인코더의 의미 능력을 조사한다. 구체적으로, 우리는 MDE 인코더를 하류 시맨틱 분할 데이터셋에 미세 조정했다. 시크릿 데이터셋[15]의 표 7에 나타난 바와 같이 대규모 MDE 훈련(86.2 mIoU)의 인코더는 대규모 이미지넷-21K 사전 훈련, _e._, Swin-L[38](84.3) 및 ConvNeXt-XL[40](84.6)의 기존 인코더보다 우수하다. <표 8>의 ADE20K 데이터셋[88]에서도 유사한 관찰이 진행되며 58.3\\(비우측량) 59.4에서 이전 최상의 결과를 개선한다.\n' +
      '\n' +
      '단안 깊이 추정과 의미 세분화 작업 모두에서 사전 훈련된 인코더의 우월성을 목격하여 중간 수준 및 고수준의 시각적 인식 시스템 모두에 대한 일반적인 다중 태스크 인코더 역할을 할 수 있는 큰 잠재력을 가지고 있다고 생각하기를 바랍니다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '달리 명시되지 않는 한, 우리는 여기서 절제 연구에 ViT-L 인코더를 사용한다.\n' +
      '\n' +
      '***Zero샷 _each_ 훈련 데이터셋**** <표 6>에서 _each_ 훈련 데이터셋의 제로샷 전송 성능을 제공하여 _one_ 훈련 세트에 대한 상대적 MDE 모델을 학습하고 6개의 비세트 데이터셋에서 평가한다는 것을 의미한다. 이러한 결과를 통해 일반적인 작업을 구축하는 것을 목표로 하는 미래 작업에 대한 더 많은 통찰력을 제공할 수 있기를 바랍니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{_Higher is better \\(\\uparrow\\)_} & \\multicolumn{3}{c}{_Lower is better \\(\\downarrow\\)_} \\\\ \\cline{2-7}  & \\(\\delta_{1}\\) & \\(\\delta_{2}\\) & \\(\\delta_{3}\\) & AbsRel & RMSE & RMSE \\\\ \\hline AdaBins [3] & 0.903 & 0.984 & 0.997 & 0.103 & 0.364 & 0.044 \\\\ DPT [46] & 0.904 & 0.988 & 0.998 & 0.110 & 0.357 & 0.045 \\\\ P3Depth [43] & 0.898 & 0.981 & 0.996 & 0.104 & 0.356 & 0.043 \\\\ SwinV2-L [39] & 0.949 & 0.994 & 0.999 & 0.083 & 0.287 & 0.035 \\\\ AiT [41] & 0.954 & 0.994 & 0.999 & 0.076 & 0.275 & 0.033 \\\\ VPD [86] & 0.964 & 0.995 & 0.999 & 0.069 & 0.254 & 0.030 \\\\ ZoeDepth\\({}^{*}\\)[4] & 0.951 & 0.994 & 0.999 & 0.077 & 0.282 & 0.033 \\\\ \\hline\n' +
      '**Ours** & **0.984** & **0.998** & **1.000** & **0.056** & **0.206** & **0.024** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** 고정 조정 및 NYUv2 [54]에서 사전 훈련된 MDE 인코더로 평가하고 있다. 우리는 가장 잘, 두 번째 최상의 결과뿐만 아니라 대부분의 판별 메트릭을 강조한다. (*\\): 당사가 도입하는***(*\\):\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{_Higher is better \\(\\uparrow\\)_} & \\multicolumn{3}{c}{_Lower is better \\(\\downarrow\\)_} \\\\ \\cline{2-7}  & \\(\\delta_{1}\\) & \\(\\delta_{2}\\) & \\(\\delta_{3}\\) & AbsRel & RMSE & log10 \\\\ \\hline AdaBins [3] & 0.903 & 0.984 & 0.997 & 0.103 & 0.364 & 0.044 \\\\ DPT [46] & 0.904 & 0.988 & 0.998 & 0.110 & 0.357 & 0.045 \\\\ P3Depth [43] & 0.898 & 0.981 & 0.996 & 0.104 & 0.356 & 0.043 \\\\ SwinV2-L [39] & 0.949 & 0.994 & 0.999 & 0.083 & 0.287 & 0.035 \\\\ AiT [41] & 0.954 & 0.994 & 0.999 & 0.076 & 0.275 & 0.033 \\\\ VPD [86] & 0.964 & 0.995 & 0.999 & 0.069 & 0.254 & 0.030 \\\\ ZoeDepth\\({}^{*}\\)[4] & 0.951 & 0.994 & 0.999 & 0.077 & 0.282 & 0.033 \\\\ \\hline\n' +
      '**Ours** & **0.984** & **0.998** & **1.000** & **0.056** & **0.206** & **0.024** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** 고정 조정 및 NYUv2 [54]에서 사전 훈련된 MDE 인코더로 평가하고 있다. 우리는 가장 잘, 두 번째 최상의 결과뿐만 아니라 대부분의 판별 메트릭을 강조한다. (*\\): 당사가 도입한**모수 깊이 추정 시스템입니다. 6개의 학습 데이터 세트 중 HRWSI[67]는 20K 이미지만 포함하더라도 가장 강력한 일반화 능력으로 우리의 모델을 연료화한다. 이것은 데이터 다양성 수를 많이 나타내며, 이는 표지되지 않은 이미지를 활용하기 위한 우리의 동기와 잘 정렬된다. 일부 라벨이 붙은 데이터 세트는 매우 잘 수행하지 않을 수 있으며 _e.g._, 메가플로[33] 그러나 이 6개의 테스트 데이터 세트에 반영되지 않는 고유한 선호도를 가지고 있다. 예를 들어 메가플로 데이터로 훈련된 모델은 초원격 건물(그림 1)의 거리를 추정하는 데 특화된 모델이며, 이는 공중 자동차에 매우 유익할 것이다.\n' +
      '\n' +
      '1)의** 효과(1)는 표지되지 않은 이미지를 학습할 때 학생 모델에 도전하고, 2) 의미적 제약 조건.** <표 9>에 나타난 바와 같이, 표지되지 않은 이미지를 의사 라벨로 추가하는 것만이 이미 표지된 이미지가 충분하기 때문에 반드시 우리 모델에 이득을 가져다주는 것은 아니다. 그러나 재학습 중 표지되지 않은 이미지에 적용된 강력한 섭동(\\(\\mathcal{S}\\))으로 학생 모델은 추가적인 시각적 지식을 찾고 보다 강력한 표현을 배우기 위해 도전한다. 결과적으로, 대규모 표지되지 않은 이미지는 모델 일반화 능력을 크게 향상시킵니다.\n' +
      '\n' +
      '또한, 사용된 의미 제약 \\(\\mathcal{L}_{feat}\\)를 사용하여 깊이 추정 작업에 대해 표지되지 않은 이미지의 힘을 추가로 증폭할 수 있다. 더 중요한 것은, 4.4절에서 강조된 바와 같이, 이러한 보조 제약은 또한 훈련된 인코더가 중간 수준과 고수준의 인식 모두에 대한 다중 태스크 시각 시스템의 핵심 구성 요소 역할을 할 수 있게 한다.\n' +
      '\n' +
      '하류 작업에서 MiDaS 훈련 인코더와의****** 우리비저트 모델은 MiDaS[5, 45]보다 더 강한 제로 샷 능력을 나타냈다. 여기, 우리는 더 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Training set} & \\multicolumn{2}{c}{KITTI [18]} & \\multicolumn{2}{c}{NYUv2 [54]} & \\multicolumn{2}{c}{Sintel [7]} & \\multicolumn{2}{c}{DDAD [20]} & \\multicolumn{2}{c}{ETH3D [51]} & \\multicolumn{2}{c}{DIODE [59]} & \\multicolumn{2}{c}{Mean} \\\\ \\cline{2-13}  & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) \\\\ \\hline BlendedMVS [76] & _0.089_ & 0.918 & 0.068 & 0.958 & _0.556_ & 0.689 & _0.305_ & _0.731_ & 0.148 & 0.845 & 0.092 & 0.921 & _0.210_ & _0.844_ \\\\ DIML [13] & 0.099 & 0.907 & _0.055_ & _0.969_ & 0.573 & 0.722 & 0.381 & 0.657 & 0.142 & 0.859 & 0.107 & 0.908 & 0.226 & 0.837 \\\\ HRWSI [67] & 0.095 & 0.917 & 0.062 & 0.966 & 0.502 & 0.731 & 0.270 & 0.750 & 0.186 & 0.775 & 0.087 & 0.935 & 0.200 & 0.846 \\\\ IRS [61] & 0.105 & 0.892 & _0.057_ & 0.970 & 0.568 & 0.714 & 0.328 & 0.691 & 0.143 & 0.845 & 0.088 & 0.926 & 0.215 & 0.840 \\\\ MegaDepth [33] & 0.217 & 0.741 & 0.071 & 0.953 & 0.632 & 0.660 & 0.479 & 0.566 & _0.142_ & _0.852_ & 0.104 & 0.910 & 0.274 & 0.780 \\\\ TartanAir [62] & 0.088 & 0.920 & 0.061 & 0.964 & 0.602 & _0.723_ & 0.332 & 0.690 & 0.160 & 0.818 & _0.088_ & _0.928_ & 0.222 & 0.841 \\\\ \\hline All labeled data & **0.085** & **0.934** & **0.053** & **0.971** & **0.492** & **0.748** & **0.245** & **0.771** & **0.134** & **0.874** & **0.070** & **0.945** & **0.180** & **0.874** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: Examine은 _each 표지된 트레이닝 세트_(왼쪽)의 제로샷 전송 성능을 6개의 미제 데이터세트(톱)로 표시했다. **Better 성능**: AbsRel \\(\\downarrow\\),\\(\\delta_{1}\\uparrow\\) 우리는 **bold****, _underline_ 및 _italic_에서 각 테스트 데이터 세트에 대해 ** 최고**, _2차_ 및 _3차 베스트_ 결과를 각각 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Encoder & mIoU (s.s.) & m.s. \\\\ \\hline Segmenter [57] & ViT-L [16] & - & 82.2 \\\\ SegFormer [69] & MiT-B5 [69] & 51.0 \\\\ Mask2Former [12] & Swin-L [38] & 83.3 & 84.3 \\\\ OneFormer [24] & Swin-L [38] & 83.0 & 84.4 \\\\ OneFormer [24] & ConvNeXt-XL [40] & 83.6 & 84.6 \\\\ DDP [25] & ConvNeXt-L [40] & 83.2 & 83.9 \\\\ \\hline Ours & ViT-L [16] & **84.8** & **86.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MDE 미리 학습된 인코더를 의미 세분화를 위해 **Citysc 테이프**로 전송하세요. 우리는 _do no_는 사전 학습을 위해 지도 [1]를 사용하는데, s./s. s./m. 단일/다중 규모 평가이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Encoder & mIoU \\\\ \\hline Segmenter [57] & ViT-L [16] & 51.8 \\\\ SegFormer [69] & MiT-B5 [69] & 51.0 \\\\ Mask2Former [12] & Swin-L [38] & 56.4 \\\\ UperNet [68] & BEiT-L [2] & 56.3 \\\\ ViT-Adapter [11] & BEiT-L [2] & 58.3 \\\\ OneFormer [24] & Swin-L [38] & 57.4 \\\\ OneFormer [24] & ConNeXt-XL [40] & 57.4 \\\\ \\hline Ours & ViT-L [16] & **59.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 시맨틱 분할을 위해 MDE 인코더를 **ADE20K**로 전송하세요. 우리는 이전에 Mask2를 분할 모델로 사용합니다.\n' +
      '\n' +
      '미다S v3.1 [5] 트레이닝 인코더와 함께 훈련된 인코더를 다운스트림 미세 조정 성능 측면에서 보상합니다. 표 10에서 입증된 바와 같이, 하류 깊이 추정 과제와 의미 세분화 작업 모두에서 인코더는 MiDaS 인코더를 현저하게 능가하고, \\(\\delta_{1}\\) 메트릭에서 0.951 _vs_.0.984, NYUv2에서 52.4 _vs_를 생성했다. ADE20K의 mIoU 메트릭에서 59.4.\n' +
      '\n' +
      '하류 작업에서 DINov2와** 비교하면 하류 작업에 미세 조정되었을 때 훈련된 인코더의 우월성이 입증되었다. 최종적으로 생산된 인코더(대규모 MDE 훈련에서)는 DINov2[42]에서 미세 조정되기 때문에 우리는 인코더와 표 11의 원래 DINov2 인코더를 비교하고 다운스트림 메트릭 깊이 추정 과제와 의미 세분화 작업 모두에서 원래 DINov2 인코더보다 인코더가 더 나은 성능을 나타내는 것을 관찰할 수 있다. DINov2 중량은 매우 강한 초기화(표 10에 보고된 바와 같이 MiDaS 인코더보다 훨씬 우수)를 제공했지만, 우리의 대규모 및 고품질 MDE 훈련은 하류 전달 성능에서 이를 더욱 인상적으로 향상시킬 수 있다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '그림 3의 6개의 비선 데이터 세트에 대한 모델 예측을 시각화한다. 우리 모델은 다양한 도메인의 이미지를 테스트하는 데 강합니다. 또한, 우리는 그림 4의 MiDaS와 모델을 비교하며, 또한 예측된 깊이 맵에 조건화된 새로운 이미지를 제어넷[84]으로 합성하려고 시도한다. 우리의 모델은 제어넷이 MiDaS 깊이로 훈련되었지만 더 나은 합성 결과뿐만 아니라 MiDaS보다 더 정확한 깊이 추정을 생성한다. 보다 정확한 합성을 위해 이미지 합성 및 비디오 편집을 위한 더 나은 제어 신호를 제공하기 위해 제11차 수정 사항을 기반으로 더 나은 깊이 조절된 제어넷을 재학습했다. 보다 질적 결과를 위해 프로젝트 페이지 또는 다음 보충 자료를 참조하십시오.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '이 작업에서 강력한 단안 깊이 추정을 위한 매우 실용적인 솔루션인 제6차 Anything를 제시한다. 사전 예술과는 다른 우리는 특히 저렴하고 다양한 표지되지 않은 이미지의 가치를 강조한다. 우리는 값이 완전히 활용되기 위한 단순하면서도 매우 효과적인 두 가지 전략을 설계한다: 1) 표지되지 않은 이미지를 학습할 때 더 도전적인 최적화 목표를 제시하며, 2) 미리 학습된 모델에서 풍부한 시맨틱 사제들을 보존한다. 그 결과, 우리의 제6차 안치 모델은 우수한 제로 샷 깊이 추정 능력을 나타내며, 하류 측정 깊이 추정 및 의미 세분화 작업을 위한 유망한 초기화 역할도 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Encoder} & \\multicolumn{2}{c}{NYUv2} & \\multicolumn{2}{c}{KITTI} & \\multicolumn{2}{c}{Cityscapes} & \\multicolumn{1}{c}{ADE20K} \\\\ \\cline{2-7}  & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & mIoU & mIoU \\\\ \\hline MiDaS & 0.077 & 0.951 & 0.054 & 0.971 & 82.1 & 52.4 \\\\ Ours & **0.056** & **0.984** & **0.046** & **0.982** & **84.8** & **59.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 원래의 DINov2와 우리의 생성된 인코더 사이의 비교는 하류 미세 조정 성능 측면에서이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{\\(\\mathcal{L}_{l}\\)} & \\multirow{2}{*}{\\(\\mathcal{L}_{u}\\)} & \\multirow{2}{*}{\\(\\mathcal{S}\\)} & \\multirow{2}{*}{\\(\\mathcal{L}_{feat}\\)} & \\multirow{2}{*}{KI} & \\multirow{2}{*}{NY} & \\multirow{2}{*}{SI} & \\multirow{2}{*}{DD} & \\multirow{2}{*}{ET} & \\multirow{2}{*}{DI} \\\\ \\hline ✓ & & & 0.085 & 0.053 & 0.492 & 0.245 & 0.134 & 0.070 \\\\ ✓ & ✓ & & 0.085 & 0.054 & 0.481 & 0.242 & 0.138 & 0.073 \\\\ ✓ & ✓ & ✓ & & 0.081 & 0.048 & 0.469 & 0.235 & 0.134 & 0.068 \\\\ ✓ & ✓ & ✓ & ✓ & **0.076** & **0.043** & **0.458** & **0.230** & **0.127** & **0.066** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 1의 구조 연구는 표지되지 않은 이미지를 학습할 때 교란이 강한 학생(\\(\\mathcal{S}\\))과 의미 제약(\\(\\mathcal{L}_{feat}\\)에 도전한다. 공간에 의해 제한되고, 우리는 아사이언스(\\(\\downarrow\\) 메트릭만을 보고하고, 첫 두 글자로 데이터셋 이름을 단축한다.\n' +
      '\n' +
      '그림 4: 우리는 우리의 깊이 예측과 MiDaS를 비교한다. Meantime, 우리는 제어넷을 사용하여 깊이 맵(마지막 행)에서 새로운 이미지를 합성한다. 먼저 입력 영상, 두 번째 행: 깊이 예측.\n' +
      '\n' +
      '그림 3: 6개의 비선량 데이터 세트에 대한 정성적 결과를 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{NYUv2} & \\multicolumn{2}{c}{KITTI} & \\multicolumn{2}{c}{Cityscapes} & \\multicolumn{1}{c}{ADE20K} \\\\ \\cline{2-7}  & AbsRel & \\(\\delta_{1}\\) & AbsRel & \\(\\delta_{1}\\) & mIoU & mIoU \\\\ \\hline MiDaS & 0.077 & 0.951 & 0.054 & 0.971 & 82.1 & 52.4 \\\\ Ours & **0.056** & **0.984** & **0.046** & **0.982** & **84.8** & **59.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 훈련된 인코더와 MiDaS[5] 훈련된 인코더 간의 비교는 다운스트림 미세 조정 성능 측면에서 훈련되었다. **Better 성능:** AbsRel \\(\\downarrow\\), \\(\\delta_{1}\\uparrow\\), mIoU \\(\\uparrow\\)이다.\n' +
      '\n' +
      '크기가 큰 표지 데이터****의 파워를 씻어냅니다.\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '6개의 모음을 실행합니다.\n' +
      '\n' +
      '우리는 모든 이미지의 더 짧은 측면을 518로 재구성하고 원래 종횡비를 유지합니다. 모든 이미지는 훈련 중 518\\(\\tot\\)518로 크롭됩니다. 추론하는 동안 우리는 작물 이미지가 아니며 양쪽만 14의 승수임을 보장하는데, 이는 DINOcv2 인코더[42]의 사전 정의된 패치 크기가 예측치를 보간하여 원래 해상도에서 평가되기 때문이다. 미다S[5, 45]에 이어 제로샷 평가에서 우리 예측의 규모와 이동은 근거 사실과 수동으로 정렬된다.\n' +
      '\n' +
      '사전 훈련된 인코더를 메트릭 깊이 추정으로 미세 조정하면 조이푼 코드 베이스[4]를 채택한다. 우리는 원래 MiDaS 기반 인코더를 더 강력한 제6안티 인코더로 대체할 뿐이며 몇 가지 하이퍼 파라미터가 수정되었다. 대조적으로, 훈련 해상도는 NYUv2 [54]에서 392\\(분절)518, KITTI [18]에서 384\\(분절)768로 인코더의 패치 크기에 부합한다. 인코더 학습율은 우리의 강력한 초기화로 인해 MiDaS 인코더에 대해 채택된 1/10보다 훨씬 작은 랜덤 초기화된 디코더의 학습률의 1/50으로 설정된다. 배치 크기는 16이고 모델은 5epochs에 대해 훈련된다.\n' +
      '\n' +
      '사전 훈련된 인코더를 시맨틱 분할에 미세 조정하면 MMSegmentation 코드 베이스[14]를 사용한다. 훈련 해상도는 ADE20K[88]과 시티 스코프[15] 모두에서 896\\(표본)896으로 설정된다. 인코더 학습율은 3e-6으로 설정되고 디코더 학습율은 10\\(표본) 더 크다. 우리는 Mask2 전 [12]을 의미 세분화 모델로 사용합니다. 이 모델은 COCO[35] 또는 지도 [1] 사전 훈련 없이 배치 크기 16으로 시티 테이프에서 160K 반복 및 80K 반복에 대해 훈련된다. 다른 트레이닝 구성은 원본 코드베이스와 동일하다.\n' +
      '\n' +
      '7.\n' +
      '\n' +
      '여기에서 모든 절제 연구는 ViT-S 모델에 대해 수행된다.\n' +
      '\n' +
      '특징 정렬에 대한 내성 마진의 필요성*** 표 12와 같이 0과 0.15 또는 0.30의 내성 마진 사이의 격차는 이 디자인의 필요성을 분명히 보여준다(평균 Abs관계는: 0.188 _vs._0.175).\n' +
      '\n' +
      '라벨링된 데이터에 특징 정렬**를 적용하면 명백하게 표지되지 않은 데이터에 피처 정렬 손실 \\(\\mathcal{L}_{feat}\\)을 실행한다. 실제로, 이 제약을 표지된 데이터에 적용하는 것도 기술적으로 가능하다. 표 13에서 표지되지 않은 데이터에 \\(\\mathcal{L}_{feat}\\)를 적용하는 것 외에도 라벨링된 데이터에 적용하기 위해 탐구한다. 우리는 표지된 데이터에 이 보조 최적화 표적을 추가하는 것이 특징 정렬을 포함하지 않는 기준선에 유익하지 않다는 것을 발견한다(ir 평균 Abs관계는 거의 동일하다: 0.180 _vs_0.179). 우리는 이것이 라벨링된 데이터가 상대적으로 고품질 깊이 주석을 갖기 때문이라고 추측한다. 의미적 손실의 관여는 이러한 유익한 수동 라벨의 학습을 방해할 수 있다. 이에 비해, 우리의 의사 라벨은 더 심하고 덜 유익합니다. 따라서 라벨이 없는 데이터에 보조 제약 조건을 도입하는 것은 의사 깊이 라벨의 소음을 퇴치할 뿐만 아니라 의미 능력으로 우리의 모델을 팔 수 있다.\n' +
      '\n' +
      '8개의 임장 및 미래 사업을 수행합니다.\n' +
      '\n' +
      '현재 가장 큰 모델 크기는 ViT 대 [16]에만 제약을 받는다. 따라서 향후에는 DINOcv2[42]에 의해 잘 사전 훈련되는 ViT 대 ViT-Giant로 모델 크기를 더욱 확대할 계획입니다. 더 큰 모델로 더 강력한 교사 모델을 학습시켜 더 작은 모델이 학습하기 위해 더 정확한 의사 라벨을 생성, _e.g._, ViT-L 및 ViT-B를 생성할 수 있다. 또한 실제 애플리케이션을 용이하게 하기 위해 널리 채택된 512\\(\\tot\\)512 훈련 해상도는 충분하지 않다고 생각합니다. 700+ 또는 1000+의 더 큰 해상도로 모델을 재변형할 계획입니다.\n' +
      '\n' +
      '9번.\n' +
      '\n' +
      '6개의 비젠 테스트 세트에 대한 포괄적인 정성적 결과를 위한 다음 페이지(KITTI [18], NYUv2 [54], Sintel [7]의 그림 7], DDAD [20]의 그림 8, ETH3D [51]의 그림 9, DIODE [59]의 그림 10에 대한 종합적 정성적 결과를 참고해 주시기 바랍니다. 우리는 우리의 모델을 가장 강력한 MiDaS 모델[5], _i._, DPT-BEiTL-512과 비교하는데, 우리의 모델은 더 높은 깊이 추정 정확도와 더 강한 견고성을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\(\\mathcal{L}_{feat}\\) & \\multicolumn{6}{c}{Unseen datasets (AbsRel \\(\\downarrow\\))} & \\multicolumn{1}{c}{**Mean**} \\\\ \\hline U & L & KITTI & NYU & Sintel & DDAD & ETH3D & DIODE & \\\\ \\hline  & 0.083 & 0.055 & 0.478 & 0.249 & 0.133 & 0.080 & 0.180 \\\\ ✓ & **0.080** & **0.053** & **0.464** & **0.247** & **0.127** & **0.076** & **0.175** \\\\  & ✓ & 0.084 & 0.054 & 0.472 & 0.252 & 0.133 & 0.081 & 0.179 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 특징 정렬 손실 \\(\\mathcal{L}_{feat}\\)를 표지되지 않은 데이터(**U***) 또는 표지된 데이터(**L*****)에 적용한 증폭 연구가 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\(\\alpha\\) & KITTI & NYU & Sintel & DDAD & ETH3D & DIODE & **Mean** \\\\ \\hline\n' +
      '0.079&0.188 \\\\ & 0.079 & 0.079 & 0.523 & 0.055 & 0.250 & 0.055 & 0.523 & 0.250 & 0.523 & 0.250 및 0.188 \\\\ 0.055 & 0.250 & 0.523 & 0.250 & 0.523 & 0.C.00 & 0.523 & 0.250 & 0.523 & 0.523 & 0.523 & 0.250 및 0.523 & 0.250 및 0.523 & 0.250 및 0.523 & 0.250/0.C.00 & 0.523 & 0.250 및 0.523 & 0.523 & 0.C.00 & 0.523 & 0.C.00 & 0.250 및 0.523 & 0.C.00 & 0.523 & 0.250 및 0.523 & 0.523 & 0.250 및 0.523 & 0.250 및 0.523 & 0.250 및 0.523 & 0.C.\n' +
      '0.076* & ** 0.080* & ** 0.080* & **0.464** & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.053* & **0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05*0.05**0.05*0**0**0**0**0*\n' +
      '0.30 & **0.079** & 0.054 & 0.482 & 0.248 & 0.127 & 0.077 & 0.178 \\\\ \\hlineFigure 5: Qualitative results on KITTI. Due to the extremely sparse ground truth which is hard to visualize, we here compare our prediction with the most advanced MiDaS v3.1 [5] prediction. The brighter color denotes the closer distance.\n' +
      '\n' +
      '그림 6: NYUv2에 대한 정성적 결과는 MiDaS [5]가 NYUv2 학습 데이터(_not 제로샷_)를 사용하는 반면 그렇지 않다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '그림 7:신델에 대한 정성적 결과는 그림 7이다.\n' +
      '\n' +
      '그림 8: DDAD에 대한 정성적 결과를 보여준다.\n' +
      '\n' +
      '그림 9: ETH3D에 대한 정성적 결과를 보여준다.\n' +
      '\n' +
      '그림 10: DIODE에 대한 정성적 결과를 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Manuel Lopez Antequera, Pau Gargallo, Markus Hofinger, Samuel Rota Bulo, Yubin Kuang, and Peter Kontschieder. Mapillary planet-scale depth dataset. In _ECCV_, 2020.\n' +
      '* [2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* [3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In _CVPR_, 2021.\n' +
      '* [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. _arXiv:2302.12288_, 2023.\n' +
      '* [5] Reiner Birkl, Diana Wofk, and Matthias Muller. Midas v3. 1-a model zoo for robust monocular relative depth estimation. _arXiv:2307.14460_, 2023.\n' +
      '* [6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv:2108.07258_, 2021.\n' +
      '* [7] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J Black. A naturalistic open source movie for optical flow evaluation. In _ECCV_, 2012.\n' +
      '* [8] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. _arXiv:2001.10773_, 2020.\n' +
      '* [9] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-Chiang Frank Wang. Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation. In _CVPR_, 2019.\n' +
      '* [10] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-image depth perception in the wild. In _NeurIPS_, 2016.\n' +
      '* [11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In _ICLR_, 2023.\n' +
      '* [12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In _CVPR_, 2022.\n' +
      '* [13] Jaehoon Cho, Dongbo Min, Youngung Kim, and Kwanghoon Sohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural indoor and outdoor scenes. _arXiv:2110.11590_, 2021.\n' +
      '* [14] MMSegmentation Contributors. MMSegmentation: Opemnmlab semantic segmentation toolbox and benchmark. [https://github.com/open-rmlab/mmsegmentation](https://github.com/open-rmlab/mmsegmentation), 2020.\n' +
      '* [15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _CVPR_, 2016.\n' +
      '* [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In _NeurIPS_, 2014.\n' +
      '* [18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. _IJRR_, 2013.\n' +
      '* [19] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshilaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In _NeurIPS_, 2020.\n' +
      '* [20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In _CVPR_, 2020.\n' +
      '* [21] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien Gaidon. Semantically-guided representation learning for self-supervised monocular depth. In _ICLR_, 2020.\n' +
      '* [22] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares Ambrus, and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In _ICCV_, 2023.\n' +
      '* [23] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recovering surface layout from an image. _IJCV_, 2007.\n' +
      '* [24] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In _CVPR_, 2023.\n' +
      '* [25] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In _ICCV_, 2023.\n' +
      '* [26] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In _NeurIPS_, 2023.\n' +
      '* [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _ICCV_, 2023.\n' +
      '* [28] Marvin Klingner, Jan-Aike Termohlen, Jonas Mikolajczyk, and Tim Fingscheidt. Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance. In _ECCV_, 2020.\n' +
      '* [29] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco Korner. Evaluation of cnn-based single-image depth estimation methods. In _ECCVW_, 2018.\n' +
      '* [30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.\n' +
      '* [31] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _ICMLW_, 2013.\n' +
      '* [32] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs. In _CVPR_, 2015.\n' +
      '* [33] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In _CVPR_, 2018.\n' +
      '\n' +
      '* [34] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. _arXiv:2204.00987_, 2022.\n' +
      '* [35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [36] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and William T Freeman. Sift flow: Dense correspondence across different scenes. In _ECCV_, 2008.\n' +
      '* [37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _arXiv:2303.05499_, 2023.\n' +
      '* [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.\n' +
      '* [39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _CVPR_, 2022.\n' +
      '* [40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.\n' +
      '* [41] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang Geng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying output space of visual tasks via soft token. In _ICCV_, 2023.\n' +
      '* [42] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeddin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _TMLR_, 2023.\n' +
      '* [43] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and Luc Van Gool. P3depth: Monocular depth estimation with a piecewise planarity prior. In _CVPR_, 2022.\n' +
      '* [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [45] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. _TPAMI_, 2020.\n' +
      '* [46] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _ICCV_, 2021.\n' +
      '* [47] Alex Rasla and Michael Beyeler. The relative importance of depth cues and semantic edges for indoor mobility using simulated prosthetic vision in immersive virtual reality. In _VRST_, 2022.\n' +
      '* [48] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In _ICCV_, 2021.\n' +
      '* [49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 2015.\n' +
      '* [50] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d: Learning 3d scene structure from a single still image. _TPAMI_, 2008.\n' +
      '* [51] Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In _CVPR_, 2017.\n' +
      '* [52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, 2019.\n' +
      '* [53] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and Zhengguo Li. Nddepth: Normal-distance assisted monocular depth estimation. In _ICCV_, 2023.\n' +
      '* [54] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In _ECCV_, 2012.\n' +
      '* [55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, 2020.\n' +
      '* [56] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In _CVPR_, 2015.\n' +
      '* [57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _ICCV_, 2021.\n' +
      '* [58] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv:2302.13971_, 2023.\n' +
      '* [59] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z Dai, Andrea F Daniele, Mohammadeza Mostajabi, Steven Basart, Matthew R Walter, et al. Diode: A dense indoor and outdoor depth dataset. _arXiv:1908.00463_, 2019.\n' +
      '* [60] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for depth prediction from dynamic scenes. In _3DV_, 2019.\n' +
      '* [61] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: A large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. In _ICME_, 2021.\n' +
      '* [62] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits of visual slam. In _IROS_, 2020.\n' +
      '* [63] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In _CVPR_, 2019.\n' +
      '\n' +
      '* [64] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In _CVPR_, 2020.\n' +
      '* [65] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, and Vivienne Sze. Fastdepth: Fast monocular depth estimation on embedded systems. In _ICRA_, 2019.\n' +
      '* [66] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth perception with web stereo data supervision. In _CVPR_, 2018.\n' +
      '* [67] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking loss for single image depth prediction. In _CVPR_, 2020.\n' +
      '* [68] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, 2018.\n' +
      '* [69] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In _NeurIPS_, 2021.\n' +
      '* [70] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. In _ICCV_, 2021.\n' +
      '* [71] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam Lim, and Antonio Torralba. Mtformer: Multi-task learning via transformer and cross-task reasoning. In _ECCV_, 2022.\n' +
      '* [72] I Zeki Yalniz, Herve Jegou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification. _arXiv:1905.00546_, 2019.\n' +
      '* [73] Lile Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. St++: Make self-training work better for semi-supervised semantic segmentation. In _CVPR_, 2022.\n' +
      '* [74] Lile Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. In _CVPR_, 2023.\n' +
      '* [75] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth: Ground embedding for monocular depth estimation. In _ICCV_, 2023.\n' +
      '* [76] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo networks. In _CVPR_, 2020.\n' +
      '* [77] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In _ICCV_, 2019.\n' +
      '* [78] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In _ICCV_, 2023.\n' +
      '* [79] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving. In _ICLR_, 2020.\n' +
      '* [80] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv:1506.03365_, 2015.\n' +
      '* [81] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In _CVPR_, 2020.\n' +
      '* [82] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and Ping Tan. New crfs: Neural window fully-connected crfs for monocular depth estimation. _arXiv:2203.01502_, 2022.\n' +
      '* [83] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _ICCV_, 2019.\n' +
      '* [84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, 2023.\n' +
      '* [85] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: A strong image tagging model. _arXiv:2306.03514_, 2023.\n' +
      '* [86] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. In _ICCV_, 2023.\n' +
      '* [87] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _TPAMI_, 2017.\n' +
      '* [88] Bolei Zhou, Hang Zhao, Xavier Pu, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _CVPR_, 2017.\n' +
      '* [89] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-training and self-training. In _NeurIPS_, 2020.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>