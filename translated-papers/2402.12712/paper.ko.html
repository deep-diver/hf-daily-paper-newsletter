<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'MVDiffusion++: 단일 또는 희소 뷰 3차원 객체 재구성을 위한 고밀도 고해상도 다시점 확산 모델\n' +
      '\n' +
      'Shitao Tang\n' +
      '\n' +
      '동일한 기여도. +\n' +
      '각주 † : 균등 기여도 + 공동 저자 메타와 인턴쉽을 하는 동안 일을 했다.\n' +
      '\n' +
      'Jiacheng Chen\n' +
      '\n' +
      '1Simon Fraser\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Dilin Wang\n' +
      '\n' +
      '2메타현실 연구실\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Chengzhou Tang\n' +
      '\n' +
      '2메타현실 연구실\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Fuyang Zhang\n' +
      '\n' +
      '1Simon Fraser\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Yuchen Fan\n' +
      '\n' +
      '2메타현실 연구실\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Vikas Chandra\n' +
      '\n' +
      '2메타현실 연구실\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Yasutaka Furukawa\n' +
      '\n' +
      '1Simon Fraser\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Rakesh Ranjan\n' +
      '\n' +
      '2메타현실 연구실\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 카메라 포즈 없이 하나 또는 몇 개의 영상이 주어진 객체의 조밀하고 고해상도의 뷰를 합성하는 3차원 객체 재구성을 위한 신경망 구조 MVDiffusion++를 제안한다. MVDiffusion++은 놀라울 정도로 간단한 두 가지 아이디어로 뛰어난 유연성과 확장성을 달성한다: 1) 카메라 포즈 정보를 명시적으로 사용하지 않고 2D 잠재 특징 중 표준 셀프 어텐션이 임의의 수의 조건 뷰와 생성 뷰에 걸쳐 3D 일관성을 학습하는 "포즈 프리 아키텍처"와 2) 훈련 중에 상당한 수의 출력 뷰를 폐기하는 "뷰 드롭아웃 전략"으로 훈련 시간 메모리 풋프린트를 줄이고 테스트 시간에 조밀하고 고해상도 뷰 합성을 가능하게 한다. 이를 위해 Objaverse를 사용하여 학습하고 Google Scanned Objects를 사용하여 표준적인 새로운 뷰 합성 및 3D 재구성 메트릭을 사용하여 평가하며, 여기서 MVDiffusion++는 현재 예술 수준을 훨씬 능가한다. 또한 MVDiffusion++와 text-to-image 생성 모델을 결합하여 text-to-3D 응용 예를 보인다. 프로젝트 페이지는 [https://mvdiffusion-plusplus.github.io](https://mvdiffusion-plus.github.io)에 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간의 시각은 놀라운 유연성을 보여준다. 그림 1에서 왼쪽의 물체의 이미지를 보세요. 밀리미터 정확도의 3D 모델을 만들 수는 없지만, 우리의 시각 시스템은 호랑이들의 복잡한 얼굴 특징이나 장난감 기차를 형성하는 블록의 배열, 심지어 완전히 가려진 부분까지 포함하여 우리 마음속에 일관된 3D 표현을 형성하기 위해 몇 가지 이미지의 정보를 결합할 수 있습니다.\n' +
      '\n' +
      '3D 재구성 기술은 지난 15년 동안 근본적으로 다른 방식으로 진화해 왔다. 이 기술은 몇 개의 이미지로부터 3D 형상을 추론하는 인간의 능력과 달리, 물체의 수백 개의 이미지를 취하고, 정확한 카메라 파라미터를 추정하며, 서브 밀리미터 정확도로 고 충실도의 3D 형상을 재구성한다.\n' +
      '\n' +
      '본 논문은 인간의 시각 시스템의 유연성과 계산 방법의 높은 충실도를 결합한 3차원 재구성의 새로운 패러다임을 탐구한다. 우리의 영감은 멀티뷰 이미지 생성 모델의 흥미로운 최근 개발에서 비롯된다[14; 17; 18; 29; 30; 32]. MVD 확산[32]은 뷰들에 걸친 픽셀 대응들이 이용가능할 때(예를 들어, 파노라마를 형성하기 위해 원근 이미지를 생성하는 것) 미리 트레이닝된 이미지 확산 모델들[26]을 멀티뷰 생성 시스템으로 확장하려는 초기 시도이다. MVDream[30] 및 Wonder3D[18]은 생성된 이미지가 NeRF[20] 또는 NeuS[35]와 같은 기술을 통해 3D 재구성을 산출하는 보다 일반적인 설정으로 더 확장된다.\n' +
      '\n' +
      '본 논문에서는 다시점 확산 모델의 프론티어를 유연하고 고충실도의 3차원 복원 시스템으로 확장한다. 구체적으로, 본 논문에서는 카메라 포즈 없이 단일 또는 희소 입력 뷰로 조절된 물체의 조밀(32) 및 고해상도(512\\(\\times\\)512) 이미지를 생성하기 위한 새로운 접근 방법인 MVDiffusion++를 제시한다.\n' +
      '\n' +
      '그림 1: MVDiffusion++는 단일 또는 다중의 미포즈 영상으로부터 객체의 밀집(32) 및 고해상도(512\\(\\times\\)512) 영상을 생성한다. 세 가지 예시의 입력 이미지는 각각 잠재 확산 모델인 옴니오브젝트3D[39]와 구글 스캔오브젝트[4]에서 가져온 것이다. [https://mvdiffusion-plusplus.github.io](https://mvdiffusion-plusplus.github.io)의 더 많은 결과를 보려면 프로젝트 웹사이트를 참조하십시오.\n' +
      '\n' +
      '시각적 겹침 표준 3D 복원 기술들은 생성된 이미지들을 3D 모델로 변환한다. 두 가지 간단한 아이디어가 우리 방법의 핵심이다. 먼저, 2D 특징 중 자기 주의력이 카메라 포즈나 이미지 투영 공식을 사용하지 않고 3D 일관성을 학습하는 조건부 및 생성 분기를 갖는 잠재 확산 인페인팅 모델을 활용한다. 둘째, 각 배치에서 생성 뷰를 랜덤하게 배제하여 학습 시 고해상도 이미지를 사용할 수 있도록 하는 "뷰 드롭아웃" 학습 전략을 소개한다. 테스트하는 동안 이 간단한 접근 방식은 놀랍게도 모든 이미지에 대해 동시에 고품질 조밀한 뷰를 생성합니다.\n' +
      '\n' +
      'MVDiffusion++는 새로운 뷰 합성, 단일 뷰 재구성 및 희소 뷰 재구성의 작업에서 최첨단 성능을 달성한다. 단일 시점 재구성을 위해 Google Scanned Objects 데이터셋에서 0.6973 IoU와 0.0165 Chamfer 거리를 얻었으며, Vol. IOU 측면에서 SyncDreamer[17]보다 0.1552 더 높았다. 희소 뷰 설정에서 새로운 뷰 합성을 위해 MVDiffusion++는 최근 포즈 프리 뷰 합성 방법인 LEAP[10]에 비해 PSNR이 8.19 개선되었다. 마지막으로 MVDiffusion++와 텍스트-이미지 생성 모델을 결합하여 텍스트-3D에서 응용 프로그램을 시연한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '본 논문에서는 객체 재구성을 위한 다시점 영상 생성 모델을 제안한다. 이 섹션에서는 다시점 영상 생성과 단일에서 희소 뷰 3D 복원 기술에 대한 관련 작업을 검토한다.\n' +
      '\n' +
      '**다시점 이미지 생성.**텍스트-이미지 확산 모델의 진화는 멀티뷰 이미지 생성의 길을 열었다. MVDiffusion[32]은 멀티뷰 이미지를 동시에 잡음제거하기 위한 혁신적인 멀티-브랜치 유넷 아키텍처를 소개한다. 그러나 이 접근법은 일대일 이미지 대응 관계가 있는 경우로 제한된다. Syncdreamer[17]은 다시점 일관성을 유지하기 위해 3D 볼륨과 깊이별 주의력을 사용한다. MVDream[30]은 작업을 보다 일반적인 경우로 확장하기 위해 3D 자기 주의를 통합하는 다른 경로를 취한다. 마찬가지로 Wonder3D[18]와 Zero123++[29]는 3D self-attention을 단일 영상 조건 다시점 영상 생성에 적용한다. 이러한 방법은 혁신적이지만 주의 메커니즘의 계산 강도로 인해 희박하고 저해상도 이미지를 생성하는 경향이 있다. 이와는 대조적으로, 본 프레임워크는 임의의 수의 이미지를 조건으로 조밀하고 고해상도의 다시점 이미지를 생성할 수 있는 보다 다재다능한 솔루션을 나타낸다.\n' +
      '\n' +
      '**단일 뷰 재구성.** 단일 뷰 이미지 재구성은 생성 모델[17, 18, 21, 37, 40]의 진보에 의해 구동되는 능동적인 연구 영역[17, 18, 21, 37]이다. 대형 재구성 모델 [9]는 단일 이미지로부터 3평면을 예측하지만, 3D 볼륨은 해상도를 제한한다. 다른 방법인 Syncdreamer[17]은 비용 볼륨을 구성하여 잠재 확산 모델로 다시점 영상을 생성한다. 그런 다음 이 이미지들은 Neus와 같은 기존의 재구성 방법을 사용하여 3D 구조를 복구하는 데 사용된다. 그러나, 이 프로세스는 상당한 GPU 메모리를 필요로 하여, 낮은 해상도로 제한된다. 마찬가지로 원더3D는 자기 주의의 계산적 요구로 인해 어려움에 직면하여 유사한 제한을 초래한다. 대조적으로, 우리의 접근법은 각 반복에서 훈련을 위해 제한된 수의 뷰를 무작위로 샘플링하는 "뷰 드롭아웃" 기술을 도입한다. 이를 통해 본 논문에서 제안한 모델은 다양한 수의 고해상도 이미지를 생성할 수 있으며, 기존의 방법이 직면하는 한계를 효과적으로 해결할 수 있다.\n' +
      '\n' +
      '**희소 뷰 재구성.**희소 뷰 이미지 재구성(SVIR; Sparse View Image Reconstruction) [10, 41]은 제한된 수의 이미지들, 전형적으로 2 내지 10개만이 주어지는 도전적인 작업이다. 전통적인 3D 재구성 방법들은 먼저 카메라 포즈를 추정한 다음, 멀티뷰 스테레오[31, 42] 또는 NeRF[35]와 같은 기법들을 사용하여 밀집된 재구성을 수행한다. 그러나, 시각적 중첩이 전혀 없거나 최소인 SVIR에 대해 카메라 포즈 추정은 어렵다. 이를 해결하기 위해 FvOR[41]은 카메라 포즈와 모양을 공동으로 최적화한다. LEAP [10]은 PF-LRM [36]과 함께 노이즈 카메라 포즈의 문제를 강조하고 포즈 프리 접근법을 제안한다. 그러나 생성 모델을 기반으로 하지 않고 생성 전적이 부족하며 볼륨 렌더링의 사용으로 인해 저해상도 출력을 겪는다. 이와는 대조적으로, 본 방법은 확산 모델을 사용하여 고해상도 다시점 영상을 직접 생성하고, 메쉬 모델을 복원하기 위해 재구성 시스템 Neus[35]를 사용한다.\n' +
      '\n' +
      '##3 예비: 다시점 잠재 확산 모델\n' +
      '\n' +
      'MVD 확산[32]은, 픽셀-방향 대응들이 뷰들에 걸쳐 이용가능할 때, 텍스트 또는 이미지가 주어진 다수의 이미지들을 생성하는, 다중-뷰 잠재 확산 모델[17, 29, 30, 32]이다. MVD 확산은 제안된 접근법의 기초이며, 여기서 섹션은 그것의 아키텍처를 검토하고 표기를 도입한다(도 3 참조).\n' +
      '\n' +
      '파노라마를 형성하는 8개의 사시도를 생성하기 위해 8개의 잠재 확산 모델(LDM)이 8개의 잡음 잠재 이미지(\\(\\{Z_{1}(t), Z_{2}(t),\\cdots Z_{8}(t)\\}\\)를 동시에 잡음 제거한다. UNet은 LDM 모델의 핵심으로, 특징 피라미드의 네 가지 수준을 통과하는 일련의 블록으로 구성된다.\n' +
      '\n' +
      '\\(U_{b}^{i}\\)는 \\(b\\)번째 블록에서 \\(i\\)번째 영상의 특징 영상을 의미한다고 하자. CNN은 첫 번째 블록에서 \\(Z_{i}(t)\\)에서 입력 \\(U_{i}^{0}\\)을 초기화한다. 각 UNet 블록은 네 개의 네트워크 모듈을 가지고 있다. 첫 번째는 파노라마를 위한 새로운 대응 인식 주의(CAA: correspondence-aware attention)로서, 시각적 중첩이 있는 뷰에 걸쳐 일관성을 강화한다. 좌/우 이웃 이미지((U_{i-1}^{b},U_{i+1}^{b})\\). 나머지 3개의 모듈은 1) Self-attention (SA) layer, 2) Cross-attention (CA) layer, 3) pixel-wise concatenation with the position encoding of time\\(\\tau(t)\\). 테스트 시간에서, 표준 DDPM 샘플러[8]는 모든 노이즈 레이턴트를 마지막 CNN 층으로부터의 예측된 노이즈로 업데이트한다. 학습 목표는 표기의 단순화를 위한 조건을 생략하여 다음과 같이 정의되며, 여기서 \\(\\mathbf{\\epsilon}^{i}\\)는 가우시안이고 \\(\\epsilon_{\\theta}\\)은 UNet 출력을 나타낸다.\n' +
      '\n' +
      '[L_{MVLDM}:=\\mathbb{E}_{\\{Z_{i}(0)\\}_{i=1}^{N},\\{\\mathbf{\\epsilon}^{i}\\sim\\mathcal{N}(0,I)\\}_{i=1}^{N},t}\\Big{[}\\sum_{i=1}^{N}\\|\\mathbf{\\epsilon}^{i}-\\epsilon_{ \\theta}^{i}(\\{Z_{i}(t))\\|_{2}^{2}\\Big{}. \\tag{1}\\sim\\mathcal{N}(0,I)\\}_{i=1}^{N},t}\\Big{[}\\sum_{i=1}^{N}\\|\\mathbf{\\epsilon}^{i}-\\epsilon_{i}^{i}(\\{Z_{i}(t))\\|_{2}^{2\n' +
      '\n' +
      '## 4 MVDiffusion++\n' +
      '\n' +
      'MVDiffusion++은 임의의 수의 미포즈 조건 뷰가 주어진 조밀하고 높은 해상도의 이미지를 생성함으로써, 3D 모델링에 대한 다중 뷰 확산 모델의 프론티어를 _flexibility_ 및 _scalability_로 밀어낸다. 트랜스포머 모델의 보급으로 [33] 고충실도 3D 모델링은 잠재적으로 체적적 이미지 기능을 사용하여 고밀도 및 고해상도 이미지 기능에 대한 대규모 주의를 필요로 한다. 또한, 3D 일관성 학습이 작업의 핵심이며, 이는 일반적으로 정밀한 이미지 투영 모델 및/또는 카메라 파라미터를 필요로 할 것이다. 우리의 놀라운 발견은 2D 잠재 이미지 특징 중 자기 주의력이 투영 모델이나 카메라 매개변수 없이 3D 학습에 필요한 전부이며 간단한 훈련 전략이 조밀하고 고해상도 다중 뷰 이미지 생성을 더욱 달성할 것이라는 것이다. 이 섹션에서는 작업(즉, 입력 조건 및 출력 대상 이미지)을 정의한 다음 두 가지 핵심 아이디어인 1) 유연성을 위한 포즈 프리 멀티뷰 조건부 확산 모델과 2) 확장성을 위한 뷰 드롭아웃 훈련 전략을 설명한다. SS5는 나머지 시스템 세부 정보를 제공한다.\n' +
      '\n' +
      '### 과제 : 입력 조건 이미지 및 출력 대상 이미지\n' +
      '\n' +
      '생성 대상은 조밀(32) 및 고해상도(512\\(\\times\\)512) 이미지 세트이며, 3D 객체 재구성을 위해 방위각 및 고도각을 따라 구 상의 균일한 2D 그리드 포인트에 위치된다(도 2 참조). 구체적으로 방위각은 8개(45^{\\circ}\\)이며, 고도각은 4개([-30^{\\circ},60^{\\circ}]\\)이다. 카메라 업벡터는 중력과 정렬되어 있고, 그들의 광축은 구 중심을 통과한다. 우리의 입력 조건은 카메라 포즈가 없는 하나 또는 몇 개의 이미지이며, 모션 알고리즘의 구조가 안정적으로 작동하기에는 시각적 겹침이 너무 작거나 없을 수 있다. 조건 영상의 개수는 사전 결정된 개수까지이며, 실험에서는 10개이지만 쉽게 변경할 수 있다. 입력 영상의 해상도는 512\\(\\times\\)512이고, 입력 시점과 출력 시점 모두 수평 및 수직 시야는 \\(60^{\\circ}\\)이다.\n' +
      '\n' +
      '학습 및 평가를 위해 3D 객체 데이터베이스의 합성 렌더링 이미지를 사용하고, 추가 정성적 평가를 위한 입력 조건으로 실세계 이미지를 사용한다. 작업 설정은 제공된 세부 정보와 함께 데이터 세트 간에 약간 다릅니다.\n' +
      '\n' +
      '그림 2: MVDiffusion++의 입출력 사양(§4.1)이다. 32개의 표적 이미지는 8개의 방위각과 4개의 고도 레벨로 정의된다. 트레이닝 동안, 우리의 뷰 드롭아웃 전략(§4.3)은 상당한 수의 뷰(파란색 점선)를 랜덤하게 드롭하고(파란색 점선) 나머지 뷰(빨간색)를 노이즈 제거하도록 모델을 트레이닝한다.\n' +
      '\n' +
      'SS5. 여기서, 훈련 작업에서 모호성을 제거하는 하나의 전처리 단계를 설명한다. Objaverse[3] 및 Google Scanned Object[4]와 같은 3D 객체 데이터베이스는 Z축을 객체 업벡터와 정렬한다. 그러나, 지상-진실 객체 포즈의 방위각은 조건 이미지들의 카메라 포즈들 없이 모호하다. 따라서 그림 2의 오른쪽과 같이 첫 번째 조건과 첫 번째 출력 이미지의 방위각을 정렬하기 위해 출력 뷰를 회전한다.\n' +
      '\n' +
      '### 포즈 프리 멀티뷰 조건부 확산 모델\n' +
      '\n' +
      'MVDiffusion++는 SS3에서 정의된 바와 같은 멀티뷰 잠재 확산 모델로서, 단일 또는 희소 뷰 입력 이미지들에 대한 _condition branch_와 출력 이미지들에 대한 _generation branch_로 구성된다(도 3 및 도 4 참조). 조건 분기는 동일한 아키텍처를 공유하고, 또한 안내로서 주어지는 조건 이미지들(즉, 사소한 작업)을 생성하도록 태스크된다는 점에 유의한다.\n' +
      '\n' +
      '**Diffusion process.** 정방향 확산 과정은 영상 해상도와 미리 학습된 VAE를 제외하고는 MVDiffusion과 동일하다. 구체적으로는, 1) 전경 마스크를 사용한 모든 입력/출력 영상(\\(512\\times 512\\times 3\\)을 [64\\times 64\\times 4\\) 잠재 영상(\\(Z_{i}\\))으로 미세 조정 잠재 확산 VAE(MVAE, 미세 조정 과정을 위한 SS5 참조)으로 변환하고, 2) 각 특성에 0-123++[29]가 제안한 선형 스케줄로 가우시안 잡음을 추가한다.\n' +
      '\n' +
      '**디노이징 프로세스.** 디노이징 프로세스는 그림 3에서 강조되어 있는데, 여기서 몇 가지 수정을 가진 잠재 확산 UNet은 각 디노이징 단계 \\(t\\)에서 잡음 잠재 \\(Z_{i}(t)\\)을 처리한다. UNet은 9개의 네트워크 모듈 블록으로 구성되어 있다.\n' +
      '\n' +
      '그림 3: 멀티뷰 이미지를 샘플링하기 위한 MVDiffusion 및 MVDiffusion++에 대한 잡음 제거 아키텍처. MVDiffusion 네트워크 모듈의 순서는 MVDiffusion++와의 차이(주황색)를 강조하기 위해 재배열된다.\n' +
      '\n' +
      '인코더/디코더의 양쪽에 있는 피쳐 피라미드의 4단계입니다. 그 세부사항을 설명하면 다음과 같다.\n' +
      '\n' +
      '첫 번째 블록에서 UNet 특징 \\(U_{i}^{0}\\)은 1) 잡음 레이턴트 \\(Z_{i}\\)의 연접으로 초기화된다; 2) 1 또는 0의 상수 이진 마스크, \\(\\mathfrak{M}_{pos}\\) 또는 \\(\\mathfrak{M}_{neg}\\)으로 표기되어 분기 유형(조건 또는 생성)을 나타내고, 3) 조건 레이턴트(\\(\\text{MVAE}(I_{i},M_{i})\\))는 잠재 확산으로부터 콘디토날 VAE를 사용하여 조건 이미지(\\(M_{i}\\))를 분할 마스크로 인코딩한다. 이 연접은 \\(9=(4+4+1)\\)개의 채널을 가지며, \\(1\\times 1\\)개의 최종 컨볼루션 레이어는 채널 차원을 4로 줄인다. 생성 분기의 경우, \\(I_{i}\\)의 흰색 이미지와 \\(M_{i}\\)의 이진 이미지를 통과한다. Objaverse 및 Google Scaned Object 데이터셋의 경우 데이터셋에서 제공하는 마스크를 사용한다. 그렇지 않으면 분할을 실행하여 마스크를 생성한다.\n' +
      '\n' +
      '**[각 블록에 대해]** 세 개의 네트워크 모듈이 그 처리의 핵심이다 : 1) 모든 이미지에 걸친 UNet 특징 중 전역적 자기 주의 메커니즘, 3D 일관성을 학습하는 것; 2) 교차 주의 메커니즘, CLIP 임베딩을 통해 다른 모든 이미지에 조건 이미지의 CLIP 임베딩을 주입하는 것; 및 3) CNN 레이어, 타임스텝 주파수 인코딩 \\(\\tau(t)\\) 및 이미지 인덱스 \\(V_{i}\\)의 학습 가능한 임베딩을 주입하면서 이미지당 특징을 처리하는 것. 셀프 어텐션 모듈을 위해, 우리는 네트워크 아키텍처를 복사하고\n' +
      '\n' +
      '그림 4: MVDiffusion++의 포즈 프리 멀티뷰 조건부 확산 모델의 일러스트레이션. 모델은 임의의 수의 입력 이미지들을 취하고 고정된 시점들에서 이미지들을 생성한다. 조건 분기와 생성 분기는 입력 구성이 다르지만 동일한 구조와 가중치를 공유한다. 우리는 조건부 분기와 생성 분기를 나타내기 위해 위첨자 \\(C\\)와 \\(G\\)을 사용한다.\n' +
      '\n' +
      '가중치를 모델링하고 모든 뷰에 적용합니다. 이 모듈은 MVDream[30]에서 영감을 얻었지만, 본 연구의 주요 차이점은 1) SS4.3의 View-drop 훈련 전략을 통한 확장성 배치와 2) 네트워크 설계를 통한 카메라 포즈 없이 여러 상태 이미지를 처리하는 것이다. \\ (42=(32+10)\\) 학습 가능한 임베딩 벡터 \\(\\{V_{i}\\}\\)을 32세대 및 10개의 조건 영상에 대해 훈련하고, 훈련 시작 시 모델 중단을 피하기 위해 각각 0-초기화 훈련 가능한 스케일 \\(s\\)을 곱한다.\n' +
      '\n' +
      '**[마지막 블록]** 마지막 UNet 블록의 출력은 잡음 추정을 산출하며, 표준 DDPM 샘플러 [8]은 각 샘플링 단계에 대해 다음 타임스텝 \\(Z_{i}(t-1)\\)의 잡음 잠재성을 생성하기 위해 이를 취한다. 손실 함수는 MVDIfusion과 동일하다. 모델은 먼저 \\(\\epsilon\\)-예측으로 훈련된 다음 v-예측(SS5 참조)으로 훈련되며, 여기서 수학식 1은 \\(\\epsilon\\)-예측 모델에 대한 손실 함수이다. 속도[28], \\(\\mathbf{v}^{i}(t)=\\alpha_{t}\\mathbf{\\epsilon}^{i}-\\gamma_{t}Z_{i}(0)\\)는 v-예측 모델의 예측 대상이 되며, \\(\\alpha_{t}\\)과 \\(\\gamma_{t}\\)은 미리 정의된 각도 파라미터이다.\n' +
      '\n' +
      '중단 훈련 전략\n' +
      '\n' +
      'MVDIfusion++ 훈련은 확장성 문제에 직면할 것이다. \\ (42(=32+10)\\) UNet 기능 사본은 130k 이상의 토큰을 산출하며, 여기서 글로벌 자기 주의 메커니즘은 대규모 언어 모델 [1, 2]에 대한 최신 메모리 효율적인 변압기를 사용하더라도 실행 불가능해진다. 우리는 훈련 중에 모든 레이어에 걸쳐 한 세트의 뷰를 완전히 폐기하는 간단하면서도 놀랍도록 효과적인 _view dropout_ 훈련 전략을 제안한다. 구체적으로, 각 훈련 반복에서 각 객체에 대한 32개의 뷰 중 24개를 무작위로 드롭하여 훈련 시 메모리 소비를 크게 줄인다. 테스트 시간에는 전체 아키텍처를 실행하고 32개의 뷰를 생성합니다.\n' +
      '\n' +
      '##5 나머지 시스템 세부사항\n' +
      '\n' +
      '이 절에서는 데이터 준비, 메쉬 추출 과정, MVAE 사전 미세 조정 및 3단계 훈련 전략에 대한 나머지 시스템 세부 사항을 설명한다.\n' +
      '\n' +
      '###훈련 데이터 준비\n' +
      '\n' +
      '오바버스의 800k 3D 객체 모델 중 미적 점수[22]가 최소 5인 180k 모델을 훈련에 사용한다. 각 객체 3D 모델에 대해 경계 박스 중심을 원점으로 변환하고, 가장 긴 치수가 \\([-1,1]\\) 일치하도록 균일한 스케일링을 적용한다. 출력 카메라 중심은 원점에서 1.5 거리에 배치됩니다. 입력 조건 뷰는 Zero-123[16]과 유사한 방식으로 선택된다. 구체적으로, 방위각은 출력 카메라의 8개의 이산 각도 중 하나에서 무작위로 선택된다(SS4.1도 참조). 고도각은 [-10\\({}^{\\circ}\\), 45\\({}^{\\circ}\\)에서 임의로 설정된다. 원점으로부터 카메라 중심의 거리는 [1.5, 2.2]로부터 랜덤하게 설정된다. 블렌더를 사용하여 이미지를 렌더링합니다.\n' +
      '\n' +
      '##### 테스팅 데이터 준비\n' +
      '\n' +
      '**Single-view case.** Google Scanned Object(GSO) [4]는 테스트 데이터세트로서, SynC Dreamer[17]로부터 렌더링된 이미지와 평가 파이프라인을 차용한다. 구체적으로, 테스트 세트는 30개의 객체로 구성된다. 각 객체들은 방위각을 기준으로 30\\({}^{\\circ}\\)의 고도와 22.5\\({}^{\\circ}\\)의 고도를 갖는 16개의 영상을 갖는다. SyncDreamer는 우리가 복사하는 "시각적 타당성"에 의해 조건 이미지를 선택했다. 훈련 환경에서 방위각은 45\\({}^{\\circ}\\)이므로 8개의 영상(조건 영상에서 시작 및 포함)을 사용하여 평가한다. 렌더링된 이미지의 해상도는 256x256이고, 아키텍처의 이미지 해상도는 512x512이며, 시스템 입력을 위해 조건 이미지를 512x512로 상향 조정한다. 지상-진실 영상은 256x256이며, 메쉬 재구성을 위해 512x512 영상을 사용하였으며, 평가를 위해 생성된 영상을 256x256으로 축소하였다. 단일 시점 3차원 복원을 위해 지상-진실과 재구성된 형상 사이의 챔퍼 거리(CD)와 체적 IoU가 보고된다. PSNR, SSIM [38] 및 LPIPS [44]는 8개의 이미지에 대해 평균화하여 새로운 뷰 합성(NVS)을 위해 보고된다.\n' +
      '\n' +
      '**Sparse-view case.**Sparse-view un-posed condition은 새로운 설정(LEAP[10]과 PF-LRM[36]의 작업 제외)이다. 우리는 이미지를 렌더링하기 위해 단일 뷰 설정과 유사한 프로세스를 사용한다. 구체적으로, 먼저 30개의 GSO 객체 각각에 대해 10개의 조건 이미지를 렌더링하며 방위각과 고도각은 [0, 360]에서 무작위로 선택된다. 및 [-10, 45]를 각각 포함하는 것을 특징으로 하는 플라즈마 디스플레이 패널. 제1 타겟 뷰와 제1 입력 뷰의 방위각을 정렬하면서 32개의 지상-진실 타겟 이미지를 렌더링한다(SS4.1 및 도 2 참조). 동일한 평가 메트릭이 사용되는 반면 조건 이미지의 수는 1, 2, 4, 8 및 10으로 변경한다.\n' +
      '\n' +
      '### 생성된 영상으로부터 메쉬 추출\n' +
      '\n' +
      '32개의 이미지(도 2의 모든 타겟 뷰)를 생성한 후 , 신경 암시적 복원 방법은 SyncDreamer[17] 및 Wonder3D[18]와 유사하게 메쉬 모델을 복원한다. 구체적으로, 그리드 기반 NeuS[6, 12]를 사용하며, 여기서 전경 마스크는 MVAE에 의해 잠재 이미지 \\(\\{Z_{i}(0)\\}\\)로부터 디코딩된다. 생성된 영상은 높은 해상도와 품질을 가지므로 원더3D와 같은 정상 생성 모듈 없이 NeuS에 대한 추가 정상 감독을 얻기 위해 옴니다타[5]에서 릴리즈된 단안 정상 추정기를 직접 실행한다. 우리는 원더3D의 공식 코드베이스에서 NeuS 구현을 차용하지만 순위 기반 손실을 사용하지 않는다. 단일 Nvidia 2080 Ti를 사용하면 질감 있는 메쉬 모델을 재구성하는 데 약 3분이 소요된다. 메쉬는 내보낸 정점 색상을 직접 사용하거나 생성된 이미지로 텍스처를 변경할 수 있습니다.\n' +
      '\n' +
      '### 마스크 인식 VAE 사전 미세 조정\n' +
      '\n' +
      '우리는 기본 VAE[27]의 네트워크 구조와 모델 가중치를 복사하고 마스크를 처리하기 위해 추가 입력 및 출력 채널을 추가한다. 객체 영상만으로 M-VAE를 미세 조정하면 성능이 향상됨을 알 수 있었다. 구체적으로, 오바버스에서 렌더링된 약 300만 개의 RGBA 이미지를 전처리로 M-VAE를 미세 조정한다. 기본 학습률이 4.5e-6이고 배치 크기가 64인 원래 VAE 하이퍼파라미터를 따르며, 60,000번의 반복 학습을 수행한다. 이진 교차 엔트로피 손실은 마스크 채널에 사용된다. 프로세스는 PSNR을 36.6에서 41.2로 개선한다.\n' +
      '\n' +
      '#3단계 훈련전략\n' +
      '\n' +
      '사전 훈련된 잠재 확산 인페인팅 모델에 의해 UNet 모델 가중치를 초기화한 후, 제안된 시스템을 3단계로 훈련한다. 먼저, 사전 학습된 모델이 \\(\\epsilon\\)-예측으로 학습되었기 때문에 단일 시점 조건화 사례에서만 \\(\\epsilon\\)-예측 모델로 학습한다. 둘째, 단일 뷰 컨디셔닝 사례를 사용하여 v-예측 모델 [28]로 미세 조정한다. 셋째, 단일 및 희소 뷰 컨디셔닝 사례를 모두 사용하여 v-예측 모델로 미세 조정한다. 샘플의 절반은 단일 뷰 컨디셔닝이고, 나머지 절반은 희소 뷰 컨디셔닝이며, 여기서 조건 이미지의 수는 2에서 10 사이에서 균일하게 샘플링된다.\n' +
      '\n' +
      '## 6 Experiments\n' +
      '\n' +
      '우리는 약 일주일 동안 128개의 Nvidia H100 GPU를 사용하여 배치 크기가 1024인 모델을 훈련한다. 학습률이 7e-5인 AdamW 최적화기를 사용하였고, Zero123++와 유사한 코사인 학습률 스케줄러를 사용하였다. 또한 비디오 생성 모델에서 일반적으로 채택되는 Zero-SNR 수정[13]을 통합한다. 실험 시간에는 75단계의 DDPM [8] 샘플러를 사용하여 다시점 영상을 샘플링하고, 모델 30대, 77대, 123대, 181대가 각각 8, 16, 24, 32개의 영상을 생성한다. 섹션에서는 SS6.1의 단일 뷰 실험, SS6.2의 희소 뷰 실험 및 SS6.3의 텍스트 대 3D 적용 실험을 제시한다.\n' +
      '\n' +
      '### 단일 시점 객체 모델링\n' +
      '\n' +
      '최첨단 단일 뷰 객체 모델링 방법은 SyncDreamer[17], Wonder3D[18], Open-LRM[7]의 세 가지 주요 기준이다. 평가 파이프라인은 SyncDreamer와 동일하기 때문에, 비교를 위해 Zero123[16], RealFusion[19], Magic123[25], One-2-3-45[15], Point-E[23], Shap-E[11]을 포함하는 다른 기준선의 수를 논문에 복사한다. 다음은 세 가지 주요 기준선과 시스템을 재현하는 방법을 소개한다.\n' +
      '\n' +
      '단일 입력 영상이 주어졌을 때 고정된 시점으로부터 16개의 영상을 생성한다. 영상 해상도는 256x256이며, 잡음 제거 네트워크\\(\\epsilon_{\\theta}\\)는 Zero123에서 초기화하고 3차원 특징 볼륨과 깊이별 주의력을 활용하여 다시점 일관성을 학습한다. 사용자가 입력 이미지의 표고를 제공해야 합니다. GSO 입력 영상에 대한 공식 코드베이스를 실행하여 결과를 얻고 제공된 테스트 스크립트로 평가한다.\n' +
      '\n' +
      '원더3D_는 하나의 입력 영상을 정준 뷰로 취하여 일반 지도뿐만 아니라 6개의 영상을 생성한다. 영상 해상도는 256x256이며, 다중 뷰 자기 주의와 추가 교차 도메인 주의는 생성 결과의 일관성을 보장하며, 뷰는 우리보다 더 간결하다. 우리는 GSO 입력 영상에 대한 공식 코드베이스를 실행하여 결과를 얻는다. 그러나 공개된 모델은 직교 카메라를 가정하고 NVS 성능을 평가하기 위해 동일한 테스트 세트를 사용할 수 없다. ICP는 메트릭을 계산하기 전에 재구성된 메쉬를 그라운드 트루스와 정렬한다.\n' +
      '\n' +
      '[\\(\\bullet\\)_Open-LRM_은 단일 입력 영상으로부터 피드포워드 트랜스포머 기반 네트워크를 이용하여 3면 NeRF를 예측하는 일반화된 재구성 모델인 Large Reconstruction Model (LRM) [9]의 오픈 소스 구현이다. ICP는 CD 및 볼륨 IoU를 계산하기 전에 재구성된 메쉬를 그라운드 트루스와 정렬한다.\n' +
      '\n' +
      '**결과.** 표 1은 재구성된 3D 메쉬 및 생성된 이미지의 정량적 평가를 나타낸다. MVDiffusion++은 명확한 마진으로 모든 경쟁 방법을 일관되게 능가합니다. 실험에서 원근 영상이 사용되는 직교 카메라 투영을 가정하는 Wonder3D에 대한 평가는 완전히 공정하지 않다. 그러나 우리는 명확한 성능 격차가 우리 방법의 강도를 입증하기에 충분하다고 믿는다.\n' +
      '\n' +
      '그림 5와 그림 6은 생성된 이미지와 재구성된 메쉬 모델을 보여준다. 그림 5에서 우리의 방법은 시계(3행)의 숫자를 명확하게 보여주는 반면 다른 방법은 흐릿한 숫자를 보여준다. 또 다른 예(행 5)는 원더3D가 대칭 또는 선명도를 유지하지 못하는 것과 대조적으로 우리의 방법으로 생성된 두 개의 완벽하게 대칭인 창을 보여준다. 그림 6에서, 우리의 방법은 거북이 예(행 1)의 그럴듯하고 상세한 모양을 복구할 수 있는 반면 원더3D 및 OpenLRM은 거북이로 인식하지 못하고 상당한 인공물을 나타낼 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Task \\(\\rightarrow\\) & \\multicolumn{2}{c}{3D reconstruction} & \\multicolumn{2}{c}{Novel view synthesis} \\\\ \\cline{2-6} Method & Chamfer Dist.\\(\\downarrow\\) & Vol. IoU\\(\\uparrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline Realfusion [19] & 0.0819 & 0.2741 & 15.26 & 0.722 & 0.283 \\\\ Magic123 [25] & 0.0516 & 0.4528 & - & - & - \\\\ One-2-3-45 [15] & 0.0629 & 0.4086 & - & - & - \\\\ Point-E [23] & 0.0426 & 0.2875 & - & - & - \\\\ Shap-E [11] & 0.0436 & 0.3584 & - & - & - \\\\ Zero123 [16] & 0.0339 & 0.5035 & 18.93 & 0.779 & 0.166 \\\\ SyncDreamer [17] & 0.0261 & 0.5421 & 20.05 & 0.798 & 0.146 \\\\ Wonder3D [18]\\({}^{*}\\) & 0.0329 & 0.5768 & - & - & - \\\\ Open-LRM [7]\\({}^{*}\\) & 0.0285 & 0.5945 & - & - & - \\\\ Ours & **0.0165** & **0.6973** & **21.45** & **0.844** & **0.129** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 단일 뷰 객체 모델링 결과, 재구성된 메시(좌측) 및 생성된 이미지(우측)를 평가한다. 그라운드-트루스 메쉬 및 이미지는 Google Scanned Object[4] 데이터 세트를 기반으로 SyncDreamer[17]에 의해 준비된다. ICP는 \\({}^{*}\\)으로 표시된 방법에 대해 재구성된 메쉬를 정렬하기 위해 필요하다.\n' +
      '\n' +
      '### 희소 뷰 객체 모델링\n' +
      '\n' +
      '희소 뷰 미포즈 입력 이미지는 어려운 설정이며, LRM[9]의 희소 뷰 포즈 프리 확장인 LEAP[10] 및 PF-LRM[36]과 같은 몇 가지 기존 접근법만 알고 있다. PF-LRM의 공개 시행은 없으며, 우리는 LEAP를 첫 번째 기준선으로 선택한다. 다시점 3차원 복원에 대한 문헌은 광범위하다. 카메라 포즈를 입력으로 요구하더라도 우리의 접근 방식을 대조하는 것이 가치가 있을 것이다. 절충안으로, 우리는 지상 진실 카메라 포즈를 입력으로 제공하여 NeuS[35]를 두 번째 벤치마크로 선택했다.\n' +
      '\n' +
      '\\(\\bullet\\)_LEAP_는 변환기를 이용하여 희소한 수의 뷰로부터 복사 필드의 신경 볼륨을 예측하고 포즈도 없다. LEAP는 특징 추출기로 DINOv2[24]를 사용하며, 합리적인 일반화 능력을 가진다.\n' +
      '\n' +
      '그림 5: 생성된 이미지의 단일 뷰 객체 모델링 결과. 원더3D와 SyncDreamer에 의해 입력된 영상과 생성된 영상은 256\\(\\times\\)256이고, 렌더링된 영상은 512\\(\\times\\)512로 더 높은 충실도와 풍부한 디테일을 보여준다.\n' +
      '\n' +
      '3차원 재구성 방법으로 Omnidata의 단안 정상 추정기 [5]에 의해 추정된 표면 정상뿐만 아니라 상태 영상의 지상-진실 카메라 포즈를 제공한다. 우리는 공개 그리드 기반 NeuS 구현을 사용한다[6]. 이 베이스라인은 지상-진실 전경 마스크 및 카메라 포즈가 장착된 MonoSDF[43] 또는 NeurIS[34]와 유사하므로 생성 사전이 없는 방법에 대한 성능 상한을 설정한다.\n' +
      '\n' +
      '** 결과.** 표 2 및 그림 7은 각각 정량적 및 정성적 비교 결과를 제시한다. LEAP에 비해 MVDiffusion++는 훨씬 더 좋은 품질의 이미지를 생성한다. LEAP와 우리의 방법은 모두 글로벌 3D 일관성을 확립하기 위해 멀티 뷰 자기 주의를 이용한다. 따라서, 우리는 사전 훈련된 잠재 확산 모델로부터 물려받은 강한 이미지 사전들에 더 나은 성능을 귀속시킨다. 우리의 재구성된 메쉬는 대부분의 환경에서 NeuS를 능가하며, NeuS가 지상 진실 카메라 포즈를 사용한다는 점을 고려할 때 주목할 만한 성과이다. 이 비교는 우리의 방법의 실용성을 강조하여 사용자가 몇 개의 객체 스냅샷에서 고품질 3D 모델을 달성할 수 있도록 한다.\n' +
      '\n' +
      '그림 6: 재구성된 메쉬 모델의 단일 뷰 객체 모델링 결과. 우리의 메쉬는 고밀도(32) 및 고해상도(512\\(\\times\\)512) 생성 이미지에서 내보내져 더 미세한 세부 사항을 보여준다.\n' +
      '\n' +
      '**생성된 뷰의 수에 대한 절제 연구.** 도 8은 생성된 이미지의 수를 변화시킨다. 32개의 뷰는 30\\({}^{\\circ}\\), 0\\({}^{\\circ}\\), -30\\({}^{\\circ}\\), 60\\({}^{\\circ}\\)의 순서로 4개의 고도 기반 그룹으로 나뉜다. 8 시점 설정은 제1 그룹만을 생성하고, 16 시점 설정은 제1 두 그룹 등을 생성한다. 결과는 더 적은 뷰가 전체 객체를 커버할 수 없어 재구성 품질이 더 나빠진다는 것을 시사한다. 미터법 점수는 두 그룹 간에 거의 동일하지만 16뷰 설정은 처음 두 그룹 등을 생성한다. 결과는 더 적은 뷰가 전체 객체를 커버할 수 없어 재구성 품질이 더 나빠진다는 것을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Method & Views & Chamfer Dist.\\(\\downarrow\\) & Vol. IoU\\(\\uparrow\\) & Method & Views & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Sync- \\\\ Dreamer \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0318} & \\multirow{2}{*}{0.5610} & \\multirow{2}{*}{19.46} & \\multirow{2}{*}{0.847} & \\multirow{2}{*}{0.188} \\\\ \\cline{3-3} \\cline{5-5}  & & & & & \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} NeuS[35] \\\\ (G.T. pose) \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0536} & \\multirow{2}{*}{0.4400} \\\\  & & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} Ours \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0536} & \\multirow{2}{*}{0.4400} \\\\  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 희소 뷰 객체 모델링 결과, GSO [4] 데이터 세트를 기반으로 재구성된 메시(왼쪽) 및 생성된 이미지(오른쪽)를 평가한다.\n' +
      '\n' +
      '그림 7: 희소 뷰 입력 이미지를 사용한 새로운 뷰 합성 및 3D 재구성. **Left**: LEAP[10] 및 MVDiffusion++를 상이한 수의 미포즈 입력 이미지들과 비교하는, 새로운 뷰 합성의 정성적 예. **Right**: NeuS [35]와 지상-진실 상대 포즈 및 포즈가 없는 MVDiffusion++ 사이의 재구성 메쉬의 정성적 비교.\n' +
      '\n' +
      '24개의 뷰 및 32개의 뷰, 32개의 뷰로부터의 메시 재구성은 아티팩트 없이 더 매끄럽게 보인다.\n' +
      '\n' +
      '### Text-to-3D application\n' +
      '\n' +
      'MVDiffusion++은 GSO 데이터 세트에서 최소한의 오류로 일관된 성능을 보여준다. 우리의 훈련 데이터는 Objaverse 데이터셋[3]에서만 제공되며 MVDiffusion++은 이미 놀라운 일반화 기능을 달성합니다. 본 논문에서는 이러한 문제를 해결하기 위해 텍스트-이미지 모델이 입력 조건 이미지를 준비하는 텍스트-3D 응용 프로그램을 시연한다. MVDiffusion++은 조건 이미지를 3D 모델로 변환한다. 그림 9는 우리의 접근법의 힘을 보여주는 네 가지 예를 가지고 있다.\n' +
      '\n' +
      '그림 8: 생성된 뷰의 다른 수에 대한 절제 연구. **왼쪽**: 정성적 비교는 조밀한 뷰가 더 나은 품질로 메쉬를 생성한다는 것을 보여준다. **Right**: 생성된 뷰의 상이한 수를 사용하여 3D 재구성의 정량적 결과, 30개의 GSO[4] 객체에 대해 평가된다.\n' +
      '\n' +
      '도 9: Text-to-3D 적용 예. (상단) 텍스트 대 이미지 모델은 텍스트 프롬프트가 주어진 이미지를 생성한다. (Bottom) MVDiffusion++는 생성된 이미지를 3D 모델로 변환한다. [https://mvdiffusion-plusplus.github.io](https://mvdiffusion-plus.github.io)의 더 많은 결과를 보려면 프로젝트 웹사이트를 참조하십시오.\n' +
      '\n' +
      '##7 한계 및 향후 과제\n' +
      '\n' +
      '본 논문에서는 임의의 수의 영상을 이용하여 객체를 재구성하는 포즈 프리 기법을 제안한다. 이 접근법의 핵심은 정교한 다지점, 다시점 확산 모델이다. 이 모델은 고정된 관점에서 조밀하고 일관된 뷰를 생성하기 위해 임의의 수의 조건부 이미지를 처리한다. 이 기능은 기존 재구성 알고리즘의 성능을 크게 향상시켜 고품질 3D 모델을 생성할 수 있다. 본 논문의 결과는 MVDif-fusion++가 단일 뷰 및 희소 뷰 객체 재구성에 대한 성능에서 새로운 표준을 설정함을 보여준다.\n' +
      '\n' +
      '그림 10은 일반적인 고장 모드와 접근법의 한계를 보여준다. 우리의 방법은 케이블을 재구성하는 데 실패하는 가장 왼쪽 예에서와 같이 얇은 구조에 어려움을 겪는다. 이 방법은 때때로 입력에 가려진 뷰에 대해 믿을 수 없는 이미지를 생성하는데, 주목할 만한 예는 두 개의 꼬리를 가진 고양이를 묘사하는 것이다. 이러한 단점은 주로 훈련 데이터의 부족에 기인하며, 여기서 하나의 미래 작업은 더 풍부한 맥락 및 공간 정보를 제공하는 비디오를 통합하기 위해 프레임워크를 확장하여 잠재적으로 동적 비디오 생성을 가능하게 할 것이다.\n' +
      '\n' +
      '도 10: Text-to-3D 애플리케이션의 실패 사례.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]Dao, T.: Flashattention-2: 더 나은 병렬성과 작업 분할로 더 빠른 주의를 기울인다. arXiv preprint arXiv:2307.08691 (2023)\n' +
      '*[2]Dao, T., Fu, D., Ermon, S., Rudra, A., Re, C.: Flashattention: fast and memory-efficient exact attention with io-awareness. 신경 정보 처리 시스템의 발전 **35**, 16344-16359 (2022)\n' +
      '*[3]Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '*[4]Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: 3d scanned household items의 고품질 데이터셋. In: ICRA(2022)\n' +
      '*[5]Eftekhar, A., Sax, A., Malik, J., Zamir, A.: Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scan. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10786-10796 (2021)\n' +
      '*[6]Guo, Y.C.: Instant neural surface reconstruction (2022), [https://github.com/bennyguo/instant-nsr-pl](https://github.com/bennyguo/instant-nsr-pl)\n' +
      '*[7]He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM] (https://github.com/3DTopia/OpenLRM)(2023)\n' +
      '*[8]Ho, J., Jain, A., Abbeel, P. : 디노이징 확산 확률 모델. 신경 정보 처리 시스템의 발전 **33**, 6840-6851 (2020)\n' +
      '*[9]Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: 단일 영상에 대한 대형 재구성 모델 내지 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '*[10]Jiang, H., Jiang, Z., Zhao, Y., Huang, Q.: Leap: Liberate sparse-view 3d modeling from camera pose. arXiv preprint arXiv:2310.01410 (2023)\n' +
      '*[11]Jun, H., Nichol, A. : Shap-e : 조건부 3d 암시적 함수 생성. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '*[12]Li, Z., Muller, T., Evans, A., Taylor, R.H., Unberath, M., Liu, M.Y., Lin, C.H.: Neuralangelo: High-fidelity neural surface reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8456-8465 (2023)\n' +
      '*[13]Lin, S., Liu, B., Li, J., Yang, X. : 공통 확산 잡음 스케쥴들 및 샘플 단계들은 결함이 있다. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5404-5411 (2024)\n' +
      '*[14]Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: 일관된 다시점 생성 및 3d 확산을 갖는 3d 객체들에 대한 빠른 단일 이미지. arXiv preprint arXiv:2311.07885 (2023)*[15] Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H.: One-2-3-45: 임의의 단일 이미지 내지 3d 메쉬를 45초 동안 퍼-쉐이프 최적화 없이 완성한다. arXiv preprint arXiv:2306.16928 (2023)\n' +
      '* [16] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: ICCV (2023)\n' +
      '* [17] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncedreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [18] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [19] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: CVPR (2023)\n' +
      '* [20] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [21] Mittal, P., Cheng, Y.C., Singh, M., Tulsiani, S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 306-315 (2022)\n' +
      '* [22] Murray, N., Marchesotti, L., Perronnin, F.: Ava: A large-scale database for aesthetic visual analysis. In: 2012 IEEE conference on computer vision and pattern recognition. pp. 2408-2415. IEEE (2012)\n' +
      '* [23] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022)\n' +
      '* [24] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n' +
      '* [25] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023)\n' +
      '* [26] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10684-10695 (2022)\n' +
      '* [27] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [28] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 (2022)\n' +
      '* [29] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)* [30] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [31] Stereopsis, R.M.: Accurate, dense, and robust multiview stereopsis. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE **32**(8) (2010)\n' +
      '* [32] Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint arXiv:2307.01097 (2023)\n' +
      '* [33] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [34] Wang, J., Wang, P., Long, X., Theobalt, C., Komura, T., Liu, L., Wang, W.: Neuris: Neural reconstruction of indoor scenes using normal priors. In: European Conference on Computer Vision. pp. 139-155. Springer (2022)\n' +
      '* [35] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In: NeurIPS (2021)\n' +
      '* [36] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)\n' +
      '* [37] Wang, Y., Lira, W., Wang, W., Mahdavi-Amiri, A., Zhang, H.: Slice3d: Multi-slice, occlusion-revealing, single view 3d reconstruction. arXiv preprint arXiv:2312.02221 (2023)\n' +
      '* [38] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. TIP (2004)\n' +
      '* [39] Wu, T., Zhang, J., Fu, X., Wang, Y., Ren, J., Pan, L., Wu, W., Yang, L., Wang, J., Qian, C., et al.: Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 803-814 (2023)\n' +
      '* [40] Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. Advances in neural information processing systems **29** (2016)\n' +
      '* [41] Yang, Z., Ren, Z., Bautista, M.A., Zhang, Z., Shan, Q., Huang, Q.: Fvor: Robust joint shape and pose optimization for few-view object reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2497-2507 (2022)\n' +
      '* [42] Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L.: Mvsnet: Depth inference for unstructured multi-view stereo. In: Proceedings of the European conference on computer vision (ECCV). pp. 767-783 (2018)\n' +
      '* [43] Yu, Z., Peng, S., Niemeyer, M., Sattler, T., Geiger, A.: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural information processing systems **35**, 25018-25032 (2022)\n' +
      '* [44] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>