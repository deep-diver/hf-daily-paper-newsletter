<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction\n' +
      '\n' +
      'Shitao Tang\n' +
      '\n' +
      'Equal contribution. +\n' +
      'Footnote â€ : Equal contribution. +Joint last author. Work done during an internship with Meta.\n' +
      '\n' +
      'Jiacheng Chen\n' +
      '\n' +
      '1Simon Fraser University\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Dilin Wang\n' +
      '\n' +
      '2Meta Reality Labs\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Chengzhou Tang\n' +
      '\n' +
      '2Meta Reality Labs\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Fuyang Zhang\n' +
      '\n' +
      '1Simon Fraser University\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Yuchen Fan\n' +
      '\n' +
      '2Meta Reality Labs\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Vikas Chandra\n' +
      '\n' +
      '2Meta Reality Labs\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      'Yasutaka Furukawa\n' +
      '\n' +
      '1Simon Fraser University\n' +
      '\n' +
      '1{shitaot, jca348, fuyangz, furukawa}@sfu.ca\n' +
      '\n' +
      'Rakesh Ranjan\n' +
      '\n' +
      '2Meta Reality Labs\n' +
      '\n' +
      '2{wdilin, chengzhout, ycfan, vchandra, rakeshr}@meta.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A "pose-free architecture" where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A "view dropout strategy" that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model. The project page is at [https://mvdiffusion-plusplus.github.io](https://mvdiffusion-plusplus.github.io).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Human vision demonstrates remarkable flexibility. Look at the images of objects at the left in Figure 1. While unable to create millimeter-accurate 3D models, our visual system can combine information from a few images to form a coherent 3D representation in our minds, including intricate facial features of a tiger or the arrangement of blocks forming a toy train, even parts that are fully obscured.\n' +
      '\n' +
      '3D reconstruction technology has evolved over the last fifteen years in a fundamentally different way. Unlike the human ability to infer 3D shapes from a few images, the technology takes hundreds of images of an object, estimatestheir precise camera parameters, and reconstructs high-fidelity 3D geometry at a sub-millimeter accuracy.\n' +
      '\n' +
      'This paper explores a new paradigm of 3D reconstruction that combines the high-fidelity of computational methods and the flexibility of human visual systems. Our inspiration comes from exciting recent developments in multi-view image generative models [14; 17; 18; 29; 30; 32]. MVDiffusion [32] is an early attempt to extend pre-trained image diffusion models [26] to a multi-view generative system, when pixel correspondences across views are available (e.g., generating perspective images to form a panorama). MVDream [30] and Wonder3D [18] further extend to more general settings where generated images yield 3D reconstruction via techniques such as NeRF [20] or NeuS [35].\n' +
      '\n' +
      'This paper pushes the frontier of multi-view diffusion models towards flexible and high-fidelity 3D reconstruction systems. Concretely, the paper presents MVDiffusion++, a novel approach to generate dense (32) and high-resolution (512\\(\\times\\)512) images of an object, conditioned with single or sparse input views without camera poses, whose reliable estimation is difficult due to minimal or no\n' +
      '\n' +
      'Figure 1: MVDiffusion++ generates dense(32) and high-resolution(512\\(\\times\\)512) images of an object from a single or multiple unposed images. The input images of the three examples are from a latent diffusion model, OmniObject3D[39], and Google Scanned Objects[4], respectively. Please see our project website for more results: [https://mvdiffusion-plusplusplus.github.io](https://mvdiffusion-plusplusplus.github.io).\n' +
      '\n' +
      'visual overlaps. Standard 3D reconstruction techniques turn generated images into a 3D model. Two simple ideas are at the heart of our method. First, we leverage a latent diffusion inpainting model with conditional and generation branches, where self-attention among 2D features learns 3D consistency without using camera poses or image projection formula. Second, we introduce "view dropout" training strategy, which randomly excludes generation views in each batch, enabling the use of high-resolution images during training. During testing, this simple approach surprisingly generates high-quality, dense views for all the images simultaneously.\n' +
      '\n' +
      'MVDiffusion++ achieves state-of-the-art performance on the task of novel view synthesis, single-view reconstruction, and sparse-view reconstruction. For single-view reconstruction, our method achieves 0.6973 IoU and 0.0165 Chamfer distance on the Google Scanned Objects dataset, higher than SyncDreamer [17] by 0.1552 in terms of Vol. IOU. For novel view synthesis in sparse view setting, MVDiffusion++ improves the PSNR by 8.19 compared with a recent pose-free view synthesis method, LEAP [10]. Lastly, we demonstrate applications in text-to-3D by combining MVDiffusion++ with a text-to-image generative model.\n' +
      '\n' +
      '## 2 Related work\n' +
      '\n' +
      'This paper presents a multi-view image generative model for object reconstruction, given one or a few condition images. The section reviews related work on multi-view image generation and single to sparse-view 3D reconstruction techniques.\n' +
      '\n' +
      '**Multi-view image generation.** The evolution of text-to-image diffusion models has paved the way for multi-view image generation. MVDiffusion [32] introduces an innovative multi-branch Unet architecture for denoising multi-view images simultaneously. This approach, however, is constrained to cases with one-to-one image correspondences. Syncdreamer [17] uses 3D volumes and depth-wise attention for maintaining multi-view consistency. MVDream [30] takes a different path, incorporating 3D self-attention to extend the work to more general cases. Similarly, Wonder3D [18] and Zero123++ [29] apply 3D self-attention to single-image conditioned multi-view image generation. These methods, while innovative, tend to produce sparse, low-resolution images due to the computational intensity of the attention mechanism. In contrast, our framework represents a more versatile solution capable of generating dense, high-resolution multi-view images conditioned on an arbitrary number of images.\n' +
      '\n' +
      '**Single view reconstruction.** Single View Image Reconstruction is an active research area [17, 18, 21, 37, 40], driven by the advancements of generative models [17, 18, 21, 37]. Large reconstruction model [9] predicts triplanes from a single image, but the 3D volume limits its resolutions. The other method, Syncdreamer [17] generates multi-view images with a latent diffusion model by constructing a cost volume. These images are then used to recover 3D structures using conventional reconstruction methods like Neus. However, this process requires substantial GPU memory, limiting it to low resolutions. Similarly, Wonder3D faces challenges due to the computational demands of self-attention, leading to similar restrictions. In contrast, our approach introduces a "view dropout" technique, which randomly samples a limited number of views for training in each iteration. This enables our model to generate a variable number of high-resolution images while employing full 3D self-attention, effectively addressing the limitations faced by existing methods.\n' +
      '\n' +
      '**Sparse view reconstruction.** Sparse View Image Reconstruction (SVIR) [10, 41] is a challenging task where only a limited number of images, typically two to ten, are given. Traditional 3D reconstruction methods estimate camera poses first, then perform dense reconstruction using techniques such as multi-view stereo [31, 42] or NeRF [35]. However, camera pose estimation is difficult for SVIR, where visual overlaps are none to minimal. To address this, FvOR [41] optimizes camera poses and shapes jointly. LEAP [10] along with PF-LRM [36] highlight the issues of noisy camera poses and suggest a pose-free approach. However, they are not based on generative models, lacking generative priors, and suffer from low-resolution outputs due to the use of volume rendering. In contrast, our method employs a diffusion model to generate high-resolution multi-view images directly, then a reconstruction system Neus [35] to recover a mesh model.\n' +
      '\n' +
      '## 3 Preliminary: Multi-view latent diffusion models\n' +
      '\n' +
      'MVDiffusion [32] is a multi-view latent diffusion model [17, 29, 30, 32], generating multiple images given a text or an image, when pixel-wise correspondences are available across views. MVDiffusion is the foundation of the proposed approach, where the section reviews its architecture and introduces notations (See Figure3).\n' +
      '\n' +
      'For generating eight perspective views forming a panorama, eight latent diffusion models (LDM) denoise eight noisy latent images \\(\\{Z_{1}(t),Z_{2}(t),\\cdots Z_{8}(t)\\}\\) simultaneously. A UNet is the core of a LDM model, consisting of a sequence of blocks through the four levels of the feature pyramid.\n' +
      '\n' +
      'Let \\(U_{b}^{i}\\) denote the feature image of \\(i\\)-th image at \\(b\\)-th block. A CNN initializes an input \\(U_{i}^{0}\\) from \\(Z_{i}(t)\\) at the first block. Each UNet block has four network modules. The first is a novel correspondence-aware attention (CAA), enforcing consistency across views with visual overlaps: The left/right neighboring images \\((U_{i-1}^{b},U_{i+1}^{b})\\) for panorama. The remaining three modules are from the original: 1) Self-attention (SA) layers; 2) Cross-attention (CA) layers from the condition with the CLIP embedding; and 3) CNN layers with the pixel-wise concatenation of a positional encoding of time \\(\\tau(t)\\). At test time, a standard DDPM sampler [8] updates all noisy latents with the predicted noise from the last CNN layer. The training objective is defined as follows by omitting the conditions for notation simplicity, where \\(\\mathbf{\\epsilon}^{i}\\) is a Gaussian and \\(\\epsilon_{\\theta}\\) denotes the UNet output.\n' +
      '\n' +
      '\\[L_{MVLDM}:=\\mathbb{E}_{\\{Z_{i}(0)\\}_{i=1}^{N},\\{\\mathbf{\\epsilon}^{i}\\sim\\mathcal{ N}(0,I)\\}_{i=1}^{N},t}\\Big{[}\\sum_{i=1}^{N}\\|\\mathbf{\\epsilon}^{i}-\\epsilon_{ \\theta}^{i}(\\{Z_{i}(t)\\}\\,,\\tau(t))\\|_{2}^{2}\\Big{]}. \\tag{1}\\]\n' +
      '\n' +
      '## 4 MVDiffusion++\n' +
      '\n' +
      'MVDiffusion++ pushes the frontier of multi-view diffusion models for 3D modeling in their _flexibility_ and _scalability_ by generating denser and higher-resolution images given an arbitrary number of un-posed condition views. With the prevalence of Transformer models [33], high-fidelity 3D modeling would require large-scale attention over dense and high-resolution image features, potentially with volumetric ones. Furthermore, 3D consistency learning is at the heart of the task, which would usually require precise image projection models and/or camera parameters. Our surprising discovery is that self-attention among 2D latent image features is all we need for 3D learning without projection models or camera parameters, and a simple training strategy would further achieve dense and high-resolution multi-view image generation. The section defines the task (i.e., input condition and output target images), then explains the two key ideas: 1) pose-free multi-view conditional diffusion model for flexibility and 2) view dropout training strategy for scalability. SS5 provides the remaining system details.\n' +
      '\n' +
      '### Task: Input condition images and output target images\n' +
      '\n' +
      'The generation target is a set of dense (32) and high-resolution (512\\(\\times\\)512) images, positioned at uniform 2D grid points on a sphere along the azimuth and elevation angles for 3D object reconstruction (See Figure 2). Specifically, there are eight azimuth angles (every \\(45^{\\circ}\\)) and four elevation angles (every \\(30^{\\circ}\\) in the range \\([-30^{\\circ},60^{\\circ}]\\)). Camera up-vectors are aligned with gravity, and their optical axes pass through the sphere center. Our input condition is one or a few images without camera poses, where visual overlaps are too minimal or possibly none for Structure from Motion algorithms to work reliably. The number of condition images is up to a pre-determined number, which is 10 in our experiments but can easily change. The input image resolution is 512\\(\\times\\)512. The horizontal and vertical field-of-view of both the input and output views is \\(60^{\\circ}\\).\n' +
      '\n' +
      'We use synthetic rendered images from 3D object databases for training and evaluations, then real-world images as the input condition for further qualitative evaluations. The task settings vary slightly between datasets, with details provided\n' +
      '\n' +
      'Figure 2: The input and output specification (Â§4.1) of MVDiffusion++. The 32 target images are defined in eight azimuths and four elevation levels. During training, our view dropout strategy (Â§4.3) randomly drops a substantial number of views (dashed blue) and trains the model to denoise the remaining views (red).\n' +
      '\n' +
      'in SS5. Here, we explain one preprocessing step that removes ambiguity in the training task. 3D object databases such as Objaverse [3] and Google Scanned Object [4] align the Z-axis with the object up-vectors. However, the azimuth of the ground-truth object pose is ambiguous without camera poses of the condition images. Therefore, we rotate the output views to align the azimuth of the first condition and the first output image as shown by the right of Figure 2.\n' +
      '\n' +
      '### Pose-free multi-view conditional diffusion model\n' +
      '\n' +
      'MVDiffusion++ is a multi-view latent diffusion model as defined in SS3, comprising of a _condition branch_ for single or sparse-view input images and a _generation branch_ for output images (See Figure 3 and Figure 4). Note that the condition branch shares the same architecture and is tasked to generate the condition images that are also given as guidance (i.e., a trivial task).\n' +
      '\n' +
      '**Diffusion process.** The forward diffusion process is the same as MVDiffusion, except for the image resolution and the pre-trained VAE. Concretely, it 1) converts all \\(512\\times 512\\times 3\\) input/output image (\\(I_{i}\\)) with foreground masks to \\(64\\times 64\\times 4\\) latent images (\\(Z_{i}\\)) by a fine-tuned latent diffusion VAE (denoted as MVAE, see SS5 for the fine-tuning process); and 2) adds a Gaussian noise with a linear schedule, as suggested by zero-123++ [29] to each feature of \\(Z_{i}\\).\n' +
      '\n' +
      '**Denoising process.** The denoising process is highlighted in Figure 3, where a latent diffusion UNet with a few modifications processes a noisy latent \\(Z_{i}(t)\\) at each denoising step \\(t\\). The UNet consists of 9 blocks of network modules over the\n' +
      '\n' +
      'Figure 3: The denoising architectures for MVDiffusion and MVDiffusion++ for sampling multi-view images. The order of the MVDiffusion network modules is rearranged to highlight the differences (in orange) with MVDiffusion++.\n' +
      '\n' +
      'four levels of feature pyramids on either side of the encoder/decoder. The details are explained as follows.\n' +
      '\n' +
      '**[At first block]** The UNet feature \\(U_{i}^{0}\\) at the first block is initialized with the concatenation of 1) the noisy latents \\(Z_{i}\\); 2) a constant binary mask of either 1 or 0, denoted by \\(\\mathfrak{M}_{pos}\\) or \\(\\mathfrak{M}_{neg}\\) to indicate the branch type (condition or generation); and 3) the condition latents (\\(\\text{MVAE}(I_{i},M_{i})\\)) where we use the conditonal VAE from latent diffusion to encode the condition image (\\(I_{i}\\)) with its segmentation mask (\\(M_{i}\\)). Note that this concatenation has \\(9=(4+4+1)\\) channels, and a \\(1\\times 1\\) final convolution layer reduces the channel dimension to 4. For a generation branch, we pass a white image as \\(I_{i}\\) and a binary image of 1 (i.e., \\(\\mathfrak{M}_{pos}\\)) as \\(M_{i}\\). For Objaverse and Google Scaned Object datasets, we use the masks provided by the datasets. Otherwise, we run segmentation to generate the masks.\n' +
      '\n' +
      '**[For each block]** Three network modules are at the heart of the processing: 1) Global self-attention mechanism among the UNet features across all the images, learning 3D consistency; 2) Cross-attention mechanism, injecting the CLIP embedding of the condition images to all the other images through the CLIP embedding; and 3) CNN layers, process per-image features while injecting the timestep frequency encoding \\(\\tau(t)\\) and the learnable embedding of an image index \\(V_{i}\\). For the self-attention module, we copy the network architecture and\n' +
      '\n' +
      'Figure 4: Illustration of the pose-free multi-view conditional diffusion model of MVDiffusion++. The model takes any number of input images and generates images at fixed viewpoints. The condition branch and generation branch have different input configurations but share the same structure and weights. We use superscript \\(C\\) and \\(G\\) to denote conditional and generation branches.\n' +
      '\n' +
      'model weights and apply it across all the views. This module is inspired by MVDream [30], while the key differences in our work are 1) Scalability deployment via the view-drop training strategy in SS4.3; and 2) Handling of multiple condition images without camera poses via the network design. \\(42=(32+10)\\) learnable embedding vectors \\(\\{V_{i}\\}\\) are trained for 32 generation and 10 condition images, each of which is multiplied with a zero-initialized trainable scale \\(s\\) to avoid model disruption at the start of training.\n' +
      '\n' +
      '**[At last block]** The output of the last UNet block yields the noise estimation, and a standard DDPM sampler [8] takes it to produce the noisy latent of the next timestep \\(Z_{i}(t-1)\\) for each sampling step. The loss function is the same as MVDIfusion. Note that the model is first trained with \\(\\epsilon\\)-prediction and then with v-prediction (See SS5), where Equation 1 is the loss function for the \\(\\epsilon\\)-prediction model. The velocity [28], \\(\\mathbf{v}^{i}(t)=\\alpha_{t}\\mathbf{\\epsilon}^{i}-\\gamma_{t}Z_{i}(0)\\), becomes the prediction target for the v-prediction model, while \\(\\alpha_{t}\\) and \\(\\gamma_{t}\\) are predefined angular parameters.\n' +
      '\n' +
      '### View dropout training strategy\n' +
      '\n' +
      'MVDIfusion++ training would face a scalability challenge. \\(42(=32+10)\\) copies of UNet features yield more than 130k tokens, where the global self-attention mechanism becomes infeasible even with the latest memory efficient transformers for large language models [1, 2]. We propose a simple yet surprisingly effective _view dropout_ training strategy, which completely discards a set of views across all layers during training. Specifically, we randomly drop 24 out of 32 views for each object at each training iteration, significantly reducing memory consumption at training. At test time, we run the entire architecture and generate 32 views.\n' +
      '\n' +
      '## 5 Remaining system details\n' +
      '\n' +
      'This section explains the remaining system details on the data preparations, the mesh extraction process, the MVAE pre-fine-tuning, and the three-stage training strategy.\n' +
      '\n' +
      '### Training data preparation\n' +
      '\n' +
      'Out of 800k 3D object models from Objaverse [3], we use 180k models whose aesthetic scores [22] are at least 5 for training. For each object 3D model, we translate the bounding box center to the origin and apply uniform scaling so that the longest dimension matches \\([-1,1]\\). The output camera centers are placed at a distance of 1.5 from the origin. Input condition views are chosen in a similar way as Zero-123 [16]. Concretely, an azimuth angle is randomly chosen from one of the eight discrete angles of the output cameras (also see SS4.1). The elevation angle is set randomly from [-10\\({}^{\\circ}\\), 45\\({}^{\\circ}\\)]. The distance of the camera center from the origin is set randomly from [1.5, 2.2]. We use Blender to render images.\n' +
      '\n' +
      '### Testing data preparation\n' +
      '\n' +
      '**Single-view cases.** Google Scanned Object (GSO) [4] is our testing dataset, where we borrow the rendered images and the evaluation pipeline from SynC Dreamer [17]. Concretely, the test set consists of 30 objects. Each object has 16 images with a fixed elevation of 30\\({}^{\\circ}\\) and every 22.5\\({}^{\\circ}\\) for azimuth. SyncDreamer selected condition images by "visual plausibility", which we copy. Since the azimuth angles in our training setting are every 45\\({}^{\\circ}\\), eight images (starting from and including the condition image) are used for evaluation. The resolution of the rendered images is 256x256, while the image resolution of our architecture is 512x512. We upscale the condition images to 512x512 for our system inputs. The ground-truth images are 256x256 and we downscale our generated images to 256x256 for evaluation, while 512x512 images are used for the mesh reconstruction. The Chamfer Distances (CD) and volume IoU between the ground-truth and reconstructed shapes are reported for single-view 3D reconstruction. The PSNR, SSIM [38], and LPIPS [44] are reported for novel view synthesis (NVS) by averaging over the eight images.\n' +
      '\n' +
      '**Sparse-view cases.** Sparse-view un-posed condition is a new setup (except the work of LEAP [10] and PF-LRM [36] to our knowledge). We use a process similar to the single-view setting to render images. Concretely, we first render 10 condition images for each of the 30 GSO objects. The azimuth and the elevation angles are chosen randomly from [0, 360) and [-10, 45] respectively. We render 32 ground-truth target images while aligning the azimuth of the first target view and the first input view (See SS4.1 and Fig. 2). The same evaluation metrics are used, while we vary the number of condition images to be 1, 2, 4, 8, and 10.\n' +
      '\n' +
      '### Mesh extraction from generated images\n' +
      '\n' +
      'After generating 32 images (all target views in Fig. 2), a neural implicit reconstruction method recovers a mesh model, similar to SyncDreamer [17] and Wonder3D [18]. Specifically, we use grid-based NeuS [6, 12], where the foreground masks are decoded from the latent images \\(\\{Z_{i}(0)\\}\\) by MVAE. Since our generated images have high resolution and quality, we directly run the monocular normal estimator released by Omnidata [5] to obtain additional normal supervisions for NeuS without a normal generation module like Wonder3D. We borrow the NeuS implementation from Wonder3D\'s official codebase but do not use their ranking-based loss. With a single Nvidia 2080 Ti, it takes around 3 minutes to reconstruct a textured mesh model. The mesh could directly use the exported vertex color or be re-textured with the generated images.\n' +
      '\n' +
      '### Mask-aware VAE pre-fine-tuning\n' +
      '\n' +
      'We copy the network architecture and model weights of the default VAE [27] and add additional input and output channels to handle the mask. We found that fine-tuning M-VAE only with object images improves performance. Concretely, weuse approximately 3 million RGBA images rendered from Objaverse to fine-tune M-VAE as a pre-processing. We follow the original VAE hyperparameters with a base learning rate of 4.5e-6 and a batch size of 64. The training runs for 60,000 iterations. The binary cross entropy loss is used for the mask channel. The process improves PSNR from 36.6 to 41.2.\n' +
      '\n' +
      '### Three-stage training strategy\n' +
      '\n' +
      'After initializing the UNet model weights by a pre-trained latent diffusion inpainting model, we train the proposed system in three stages. First, we train as an \\(\\epsilon\\)-prediction model only with single-view conditioning cases, because our pre-trained model was trained as \\(\\epsilon\\)-prediction. Second, we fine-tune as a v-prediction model [28] still with single-view conditioning cases. Third, we fine-tune as a v-prediction model with both single and sparse-view conditioning cases. Half the samples are single-view conditioning, and the other half are sparse-view conditioning, where the number of condition images is uniformly sampled between 2 and 10.\n' +
      '\n' +
      '## 6 Experiments\n' +
      '\n' +
      'We train the model with a batch size of 1024 using 128 Nvidia H100 GPUs for about a week. We employ an AdamW optimizer with a learning rate of 7e-5 and use a cosine learning rate scheduler similar to Zero123++. We also incorporate the Zero-SNR fix [13] commonly adopted in video generation models. At test time, we use DDPM [8] sampler with 75 steps to sample the multi-view images, and it takes our model 30s, 77s, 123s, and 181s to generate 8, 16, 24, and 32 images, respectively. The section presents the single view experiments in SS6.1, the sparse view experiments in SS6.2, and text-to-3D application experiments in SS6.3.\n' +
      '\n' +
      '### Single-view object modeling\n' +
      '\n' +
      'Three state-of-the-art single-view object modeling methods are our main baselines: SyncDreamer [17], Wonder3D [18], and Open-LRM [7]. Since the evaluation pipeline is the same as SyncDreamer, we copy numbers of other baselines in their paper for comparison, which includes Zero123 [16], RealFusion [19], Magic123 [25], One-2-3-45 [15], Point-E [23], and Shap-E [11]. The following introduces the three main baselines and how we reproduce their systems:\n' +
      '\n' +
      '\\(\\bullet\\)_SyncDreamer_ generates 16 images from fixed viewpoints given a single input image. The image resolution is 256x256. Their denoising network \\(\\epsilon_{\\theta}\\) initializes from Zero123 and leverages 3D feature volumes and depth-wise attention to learn multi-view consistency. It requires users to provide the elevation of the input image. We run the official codebase on the GSO input images to get the results, and evaluate them with the provided testing scripts.\n' +
      '\n' +
      '\\(\\bullet\\)_Wonder3D_ takes a single input image as the canonical view and generates 6 images as well as the normal maps. The image resolution is 256x256. Multi-viewself-attention and an extra cross-domain attention ensure the consistency of generation results, while the views are sparser than ours. We run the official codebase on the GSO input images to get the results. However, the released model assumes orthographic cameras and we cannot use the same test set to evaluate the NVS performance. ICP aligns the reconstructed mesh with the ground truth before computing the metrics.\n' +
      '\n' +
      '\\(\\bullet\\)_Open-LRM_ is an open-source implementation of Large Reconstruction Model (LRM) [9], a generalized reconstruction model that predicts a triplane NeRF from a single input image using a feed-forward transformer-based network. ICP aligns the reconstructed mesh with the ground truth before computing the CD and volume IoU.\n' +
      '\n' +
      '**Results.** Table 1 presents the quantitative evaluations of the reconstructed 3D meshes and the generated images. MVDiffusion++ consistently outperforms all the competing methods with clear margins. Note that the evaluation is not completely fair for Wonder3D that assumes orthographic camera projections, where perspective images are used in the experiments. However, we believe the clear performance gaps suffice to demonstrate the strength of our method.\n' +
      '\n' +
      'Figure 5 and Figure 6 show generated images and reconstructed mesh models. In Figure 5, our method clearly shows the number on the clock (row 3), while others exhibit blurry numbers. Another example (row 5) showcases two perfectly symmetrical windows generated by our method, contrasting with Wonder3D\'s failure to maintain symmetry or clarity. In Figure 6, our method can recover a plausible and detailed shape of the turtle example (row 1), while Wonder3D and OpenLRM fail to recognize it as a turtle and exhibit significant artifacts.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Task \\(\\rightarrow\\) & \\multicolumn{2}{c}{3D reconstruction} & \\multicolumn{2}{c}{Novel view synthesis} \\\\ \\cline{2-6} Method & Chamfer Dist.\\(\\downarrow\\) & Vol. IoU\\(\\uparrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline Realfusion [19] & 0.0819 & 0.2741 & 15.26 & 0.722 & 0.283 \\\\ Magic123 [25] & 0.0516 & 0.4528 & - & - & - \\\\ One-2-3-45 [15] & 0.0629 & 0.4086 & - & - & - \\\\ Point-E [23] & 0.0426 & 0.2875 & - & - & - \\\\ Shap-E [11] & 0.0436 & 0.3584 & - & - & - \\\\ Zero123 [16] & 0.0339 & 0.5035 & 18.93 & 0.779 & 0.166 \\\\ SyncDreamer [17] & 0.0261 & 0.5421 & 20.05 & 0.798 & 0.146 \\\\ Wonder3D [18]\\({}^{*}\\) & 0.0329 & 0.5768 & - & - & - \\\\ Open-LRM [7]\\({}^{*}\\) & 0.0285 & 0.5945 & - & - & - \\\\ Ours & **0.0165** & **0.6973** & **21.45** & **0.844** & **0.129** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Single-view object modeling results, evaluating reconstructed meshes (left) and generated images (right). The ground-truth meshes and images are prepared by SyncDreamer [17] based on the Google Scanned Object [4] dataset. ICP is necessary to align reconstructed meshes for methods marked with \\({}^{*}\\).\n' +
      '\n' +
      '### Sparse-view object modeling\n' +
      '\n' +
      'Sparse-view un-posed input images is a challenging setting, where we are aware of only a few existing approaches such as LEAP [10] and PF-LRM [36], a sparse-view pose-free extension of LRM [9]. There is no public implementation of PF-LRM, and we pick LEAP as the first baseline. The literature on multi-view 3D reconstruction is extensive. It would be valuable to contrast our approach, even though they require camera poses as input. As a compromise, we have selected NeuS [35] as our second benchmark by providing the ground-truth camera poses as their input.\n' +
      '\n' +
      '\\(\\bullet\\)_LEAP_ leverages a transformer to predict neural volumes of radiance fields from a sparse number of views and is also pose-free. LEAP employs DINOv2 [24] as the feature extractor and has reasonable generalization capacity.\n' +
      '\n' +
      'Figure 5: Single-view object modeling results of generated images. The input image and the generated images by Wonder3D and SyncDreamer are in 256\\(\\times\\)256. Our rendered images are in 512\\(\\times\\)512, showing higher fidelity and richer details.\n' +
      '\n' +
      '\\(\\bullet\\)_NeuS_ is a 3D reconstruction method, where we provide the ground-truth camera poses of the condition images as well as surface normals estimated by Omnidata\'s monocular normal estimator [5]. We use the public grid-based NeuS implementation [6]. This baseline is similar to MonoSDF [43] or NeurIS [34] equipped with ground-truth foreground masks and camera poses, thus sets a performance upper bound for methods without generative priors.\n' +
      '\n' +
      '**Results.** Table 2 and Figure 7 present the quantitative and qualitative comparison results, respectively. Compared to LEAP, MVDiffusion++ generates images with much better quality. LEAP and our method both exploit multi-view self-attention to establish global 3D consistency. Therefore, we attribute our better performance to the strong image priors inherited from the pre-trained latent diffusion models. Our reconstructed meshes outperform NeuS in most settings, a notable achievement considering that NeuS uses ground-truth camera poses. This comparison highlights the practicality of our method, enabling users to achieve high-quality 3D models from just a few object snapshots.\n' +
      '\n' +
      'Figure 6: Single-view object modeling results of reconstructed mesh models. Our meshes are exported from dense (32) and high-resolution (512\\(\\times\\)512) generated images, demonstrating finer details.\n' +
      '\n' +
      '**Ablation study on the number of generated views.** Figure 8 varies the number of generated images. The 32 views are divided into four elevation-based groups with the order 30\\({}^{\\circ}\\), 0\\({}^{\\circ}\\), -30\\({}^{\\circ}\\), 60\\({}^{\\circ}\\). The 8-view setting only generates the first group, and the 16-view setting generates the first two groups, and so on. The results suggest that fewer views cannot cover the entire object, leading to worse reconstruction quality. While the metric scores are almost the same between the two groups, the 16-view setting generates the first two groups, and so on. The results suggest that fewer views cannot cover the entire object, leading to worse reconstruction quality.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Method & Views & Chamfer Dist.\\(\\downarrow\\) & Vol. IoU\\(\\uparrow\\) & Method & Views & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} Sync- \\\\ Dreamer \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0318} & \\multirow{2}{*}{0.5610} & \\multirow{2}{*}{19.46} & \\multirow{2}{*}{0.847} & \\multirow{2}{*}{0.188} \\\\ \\cline{3-3} \\cline{5-5}  & & & & & \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{c} NeuS[35] \\\\ (G.T. pose) \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0536} & \\multirow{2}{*}{0.4400} \\\\  & & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{c} Ours \\\\ \\end{tabular} } & \\multirow{2}{*}{1} & \\multirow{2}{*}{0.0536} & \\multirow{2}{*}{0.4400} \\\\  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\cline{1-1} \\cline{5-5}  & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Sparse-view object modeling results, evaluating reconstructed meshes (left) and generated images (right), based on the GSO [4] dataset.\n' +
      '\n' +
      'Figure 7: Novel view synthesis and 3D reconstruction with sparse-view input images. **Left**: a qualitative example of novel view synthesis, comparing LEAP [10] and MVDiffusion++ with different numbers of unposed input images. **Right**: qualitative comparison of reconstructed meshes between NeuS [35] with ground-truth relative poses and our pose-free MVDiffusion++.\n' +
      '\n' +
      '24 views and 32 views, the mesh reconstructions from 32 views look smoother without artifacts.\n' +
      '\n' +
      '### Text-to-3D application\n' +
      '\n' +
      'MVDiffusion++shows consistent performance with minimal errors on the GSO dataset. Note that our training data solely comes from Objaverse dataset [3], and MVDiffusion++already achieves remarkable generalization capabilities. To further challenge the system, we demonstrate a text-to-3D application, where a text-to-image model prepares an input condition image. MVDiffusion++ turns the condition image into a 3D model. Figure 9 has four examples demonstrating the power of our approach.\n' +
      '\n' +
      'Figure 8: Ablation study on different numbers of generated views. **Left**: a qualitative comparison shows that denser views produce mesh with better quality. **Right**: quantitative results of 3D reconstruction using different numbers of generated views, evaluated on 30 GSO [4] objects.\n' +
      '\n' +
      'Figure 9: Text-to-3D application examples. (Top) A text-to-image model generates an image given a text-prompt. (Bottom) MVDiffusion++ turns the generated image into a 3D model. Please see our project website for more results: [https://mvdiffusion-plusplus.github.io](https://mvdiffusion-plusplus.github.io).\n' +
      '\n' +
      '## 7 Limitations and future challenges\n' +
      '\n' +
      'This paper presents a pose-free technique for reconstructing objects using an arbitrary number of images. Central to this approach is a sophisticated multi-branch, multi-view diffusion model. This model processes any number of conditional images to produce dense, consistent views from fixed perspectives. This capability significantly enhances the performance of existing reconstruction algorithms, enabling them to generate high-quality 3D models. Our results show that MVDif-fusion++ sets a new standard in performance for both single-view and sparse-view object reconstruction.\n' +
      '\n' +
      'Figure 10 presents typical failure modes and the limitations of our approach. Our method struggles with thin structures as in the leftmost example, which fails to reconstruct a cable. Our method occasionally generates implausible images for views occluded in the input, a notable instance being the depiction of a cat with two tails. These shortcomings are predominantly attributed to the lack of training data, where one future work will expand the framework to incorporate videos, which offer richer contextual and spatial information, potentially enabling dynamic video generation.\n' +
      '\n' +
      'Figure 10: Failure examples from the Text-to-3D application.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]Dao, T.: Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 (2023)\n' +
      '* [2]Dao, T., Fu, D., Ermon, S., Rudra, A., Re, C.: Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems **35**, 16344-16359 (2022)\n' +
      '* [3]Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [4]Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: ICRA (2022)\n' +
      '* [5]Eftekhar, A., Sax, A., Malik, J., Zamir, A.: Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10786-10796 (2021)\n' +
      '* [6]Guo, Y.C.: Instant neural surface reconstruction (2022), [https://github.com/bennyguo/instant-nsr-pl](https://github.com/bennyguo/instant-nsr-pl)\n' +
      '* [7]He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM](https://github.com/3DTopia/OpenLRM) (2023)\n' +
      '* [8]Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [9]Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [10]Jiang, H., Jiang, Z., Zhao, Y., Huang, Q.: Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410 (2023)\n' +
      '* [11]Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [12]Li, Z., Muller, T., Evans, A., Taylor, R.H., Unberath, M., Liu, M.Y., Lin, C.H.: Neuralangelo: High-fidelity neural surface reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8456-8465 (2023)\n' +
      '* [13]Lin, S., Liu, B., Li, J., Yang, X.: Common diffusion noise schedules and sample steps are flawed. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 5404-5411 (2024)\n' +
      '* [14]Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)* [15] Liu, M., Xu, C., Jin, H., Chen, L., Xu, Z., Su, H.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928 (2023)\n' +
      '* [16] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: ICCV (2023)\n' +
      '* [17] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncedreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [18] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [19] Melas-Kyriazi, L., Laina, I., Rupprecht, C., Vedaldi, A.: Realfusion: 360deg reconstruction of any object from a single image. In: CVPR (2023)\n' +
      '* [20] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [21] Mittal, P., Cheng, Y.C., Singh, M., Tulsiani, S.: Autosdf: Shape priors for 3d completion, reconstruction and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 306-315 (2022)\n' +
      '* [22] Murray, N., Marchesotti, L., Perronnin, F.: Ava: A large-scale database for aesthetic visual analysis. In: 2012 IEEE conference on computer vision and pattern recognition. pp. 2408-2415. IEEE (2012)\n' +
      '* [23] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022)\n' +
      '* [24] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n' +
      '* [25] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 (2023)\n' +
      '* [26] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10684-10695 (2022)\n' +
      '* [27] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [28] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 (2022)\n' +
      '* [29] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023)* [30] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [31] Stereopsis, R.M.: Accurate, dense, and robust multiview stereopsis. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE **32**(8) (2010)\n' +
      '* [32] Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. arXiv preprint arXiv:2307.01097 (2023)\n' +
      '* [33] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [34] Wang, J., Wang, P., Long, X., Theobalt, C., Komura, T., Liu, L., Wang, W.: Neuris: Neural reconstruction of indoor scenes using normal priors. In: European Conference on Computer Vision. pp. 139-155. Springer (2022)\n' +
      '* [35] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In: NeurIPS (2021)\n' +
      '* [36] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)\n' +
      '* [37] Wang, Y., Lira, W., Wang, W., Mahdavi-Amiri, A., Zhang, H.: Slice3d: Multi-slice, occlusion-revealing, single view 3d reconstruction. arXiv preprint arXiv:2312.02221 (2023)\n' +
      '* [38] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. TIP (2004)\n' +
      '* [39] Wu, T., Zhang, J., Fu, X., Wang, Y., Ren, J., Pan, L., Wu, W., Yang, L., Wang, J., Qian, C., et al.: Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 803-814 (2023)\n' +
      '* [40] Yan, X., Yang, J., Yumer, E., Guo, Y., Lee, H.: Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision. Advances in neural information processing systems **29** (2016)\n' +
      '* [41] Yang, Z., Ren, Z., Bautista, M.A., Zhang, Z., Shan, Q., Huang, Q.: Fvor: Robust joint shape and pose optimization for few-view object reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2497-2507 (2022)\n' +
      '* [42] Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L.: Mvsnet: Depth inference for unstructured multi-view stereo. In: Proceedings of the European conference on computer vision (ECCV). pp. 767-783 (2018)\n' +
      '* [43] Yu, Z., Peng, S., Niemeyer, M., Sattler, T., Geiger, A.: Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Advances in neural information processing systems **35**, 25018-25032 (2022)\n' +
      '* [44] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>