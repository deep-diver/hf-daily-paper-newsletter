<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Tiny Language Model을 위한 Rethinking Optimization과 Architecture\n' +
      '\n' +
      'Yehui Tang\\({}^{1}\\)\n' +
      '\n' +
      'Fangcheng Liu\\({}^{1}\\)\n' +
      '\n' +
      'Yunsheng Ni\\({}^{1}\\)\n' +
      '\n' +
      'Yuchuan Tian\\({}^{1,2}\\)\n' +
      '\n' +
      'Zheyuan Bai\\({}^{1}\\)\n' +
      '\n' +
      'Yi-Qi Hu\\({}^{3}\\)\n' +
      '\n' +
      'Sichao Liu\\({}^{3}\\)\n' +
      '\n' +
      '**Shanghai Jui\\({}^{4}\\)**\n' +
      '\n' +
      '** Kai Han*\\({}^{1}\\)**\n' +
      '\n' +
      '**Yunhe Wang*\\({}^{1}\\)**\n' +
      '\n' +
      '화웨이 노아의 방주연구소 ({}^{2}\\)Peking University. \\ ({}^{3}\\)Consumer Business Group, Huawei. ({}^{4}\\) 화웨이 키린 솔루션. {yehui.tang, kai.han, yunhe.wang}@huawei.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 힘은 수많은 데이터 및 컴퓨팅 리소스를 통해 입증되었다. 그러나 모바일 기기에서 언어 모델의 적용은 계산 및 메모리 비용 측면에서 큰 어려움에 직면하고 있으며, 즉 고성능의 작은 언어 모델이 절실히 요구되고 있다. 매우 복잡한 훈련 프로세스로 인해 주의 깊게 연구되지 않는 언어 모델을 최적화하기 위한 많은 세부 사항이 있다. 본 연구에서는 1B 매개 변수를 가진 작은 언어 모델을 기반으로 각 구성 요소의 영향을 분석하기 위해 일련의 경험적 연구를 신중하게 설계한다. 3가지 관점(_i.e_., 신경 아키텍처, 파라미터 초기화 및 최적화 전략)이 주로 논의된다. 토키나이저 압축, 아키텍처 조정, 파라미터 상속 및 다중 라운드 트레이닝을 포함하는 몇몇 설계 공식들이 특히 작은 언어 모델들에 대해 실증적으로 증명된다. 그런 다음 1.6T 다국어 말뭉치에서 PanGu-\\(\\pi\\)-1B Pro와 PanGu-\\(\\pi\\)-1.5B Pro를 정립된 공식에 따라 훈련한다. 실험 결과는 PanGu-\\(\\pi\\)-1B Pro에 대한 벤치마크 평가 세트에서 개선된 최적화 및 아키텍쳐가 8.87의 현저한 평균 개선을 달성함을 보여준다. 또한 PanGu-\\(\\pi\\)-1.5B Pro는 모델 크기가 큰 다양한 SOTA 모델을 능가하여 우수한 성능을 검증한다. 코드는 곧 공개될 것입니다.\n' +
      '\n' +
      '각주 1: [https://github.com/YuchuanTian/RethinkTinyLM](https://github.com/YuchuanTian/RethinkTinyLM)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '광범위한 말뭉치로 훈련된 대규모 언어 모델(LLM)은 다양한 자연어 작업에서 인상적인 성능을 보여주었다. 강력한 일반화 기능을 갖춘 ChatGPT의 출시는 전 세계적인 관심을 사로잡았고 인간과 컴퓨터 간의 상호 작용에 혁명을 일으킬 잠재력을 가지고 있다.\n' +
      '\n' +
      'GPT-시리즈 모델들(Radford et al., 2018; Brown et al., 2020; Achiam et al., 2023) 이외에도 다양한 대형 언어 모델들이 등장하였다. PaLM(Chowdhery et al., 2023)은 6144 TPU v4 칩에 걸쳐 인상적인 540B 파라미터를 갖는 모델을 트레이닝한다. LLaMA(Touvron et al., 2023)는 7B 내지 70B 파라미터들에 이르는 일련의 기초 언어 모델들을 릴리스한다. 모델 아키텍처와 훈련된 가중치 모두 오픈 소스로 AI 커뮤니티 내에서 협업을 육성합니다. 다음 대형 모델의 대부분은 유사한 아키텍처 및 교육 방법론을 활용합니다. 예를 들어, Baichuan 팀들(Yang et al., 2023)은 중국어 및 영어 코퍼스를 모두 포함하는 2.6T 토큰 데이터세트 상에서 7B 및 13B 파라미터 모델들을 훈련시킨다. Qwen(Bai et al., 2023), Yi(Yi, 2023), Skywork(Wei et al., 2023)은 각각 2.4T, 3T, 3.2T 토큰으로 유사한 경로, 학습 모델을 추구한다. 주로 청소된 데이터의 누적 증가로 인해 LLM의 성능이 빠르게 개선되고,\n' +
      '\n' +
      '많은 연구가 다양한 고성능 언어 모델을 성공적으로 훈련시켰지만(Ren et al., 2023; Zeng et al., 2022), 이러한 모델을 훈련하는 데 사용되는 방법론은 아직 충분히 분석되지 않았다. 한편으로, 실질적인 작업체는 효과적인 훈련 전략 연구에 덜 중점을 두고 데이터를 수집하고 청소하는 데 집중한다. 한편, 대형 모델의 훈련은\n' +
      '\n' +
      '그림 1: 개선된 아키텍처 및 최적화 방법을 가진 PanGu-\\(\\pi\\) Pro. PanGu-\\(\\pi\\)-1B (Wang et al., 2023)는 LLM의 개발 전략을 직접 사용하는 반면, PanGu-\\(\\pi\\)-1B Pro는 우리의 방법론으로 평균 8.87의 성능 향상을 달성하였다. PanGu-\\(\\pi\\)-1.5B Pro가 16.67% 더 적은 파라미터로 Qwen-1.8B(Bai et al., 2023)보다 우수하다는 것을 언급할 가치가 있다.\n' +
      '\n' +
      '매우 높은 계산 자원 투자를 요구하므로 광범위한 최적화 전략을 탐색하는 것이 불가능하다. 그 결과, 최근 작품들은 LLMs를 구성할 때 유사한 훈련 레시피를 채택하는 경우가 많다(Touvron et al., 2023; Yi, 2023; Bai et al., 2023; Wei et al., 2023).\n' +
      '\n' +
      '더욱이, 이러한 대형 모델의 구현은 엄청나게 높은 메모리와 계산 자원을 요구하여 다양한 시나리오에서 실제 적용 가능성을 제한한다. 예를 들어, 175B 파라미터를 갖는 GPT-3은 FP32 데이터타입으로 저장될 때 대략 700GB의 메모리를 필요로 한다. 7B 파라미터 모델들이 상대적으로 더 효율적이지만, 그들의 리소스 요건들은 여전히 모바일 폰들과 같은 에지 디바이스들 상에서의 배치를 비실용적으로 만든다.\n' +
      '\n' +
      '본 논문에서는 신경 구조, 파라미터 초기화, 최적화 전략을 포함하는 작은 언어 모델을 구성하기 위한 방법론을 체계적으로 재고한다:\n' +
      '\n' +
      '* 신경망 아키텍처: 더 큰 모델로부터 토큰화기를 직접 채택하는 것은 중복 파라미터를 도입함으로써, 계산 오버헤드를 증가시킨다. 저주파 어휘를 제거하여 토큰화기를 스트리밍하는 것은 모델의 표현 효율을 향상시킨다. 또한 모델의 아키텍처 구성(FFN에서 깊이, 너비 및 확장률)이 최종 성능에 상당한 영향을 미친다는 것을 관찰한다. 심도는 작은 언어 모델의 주요 요소이며, 더 깊은 모델은 일반적으로 더 낮은 추론 속도를 희생시키면서 높은 성능을 달성한다.\n' +
      '* 파라미터 초기화: 큰 모델로부터 파라미터를 상속하는 것은 성능을 부스팅하고 수렴을 빠르게 하는데 효과적임을 증명한다. 이러한 맥락에서 중요한 매개변수의 식별은 필수적이다. 우리는 모델의 시작과 끝 근처에 위치한 레이어가 종종 중간 레이어보다 더 큰 의미를 갖는다는 것을 관찰했다. 또한 각 계층 내에서 데이터 기반 학습 가능한 기준의 채택은 휴리스틱 방법에 비해 더 큰 효과를 입증했다.\n' +
      '* 모델 최적화: 더 큰 모델에 비해, 작은 모델들은 더 심각한 데이터 망각 문제에 직면하고, 다중 라운드 트레이닝은 기억력 향상에 유익하다는 것을 증명한다. 우리는 다중 라운드 훈련과 관련된 훈련 비용을 완화하기 위한 간단한 표본 선택 전략을 제안한다. 또한, 특히 작은 모델에 대해 배치 크기와 학습 속도 사이의 관계를 조사합니다.\n' +
      '\n' +
      '앞서 언급한 통찰력을 바탕으로, 우리는 향상된 아키텍처와 최적화 방법을 가진 PanGu-\\(\\pi\\)-1B Pro와 PanGu-\\(\\pi\\)-1.5B Pro를 개발한다. LLM의 개발 전략부터 점차 4가지 핵심 구성 요소를 추가하여 성능을 개선한다(그림 1 참조). 모델은 검사, 지식, 추론 및 이해를 포함한 다양한 벤치마크에서 평가되며, 여기서 모델은 유사한 크기의 모델과 비교할 때 SOTA 성능을 달성한다. 예를 들어, 16.67% 더 적은 파라미터로, PanGu-\\(\\pi\\)-1.5B Pro는 56.49의 평균 점수를 달성하고, 55.04의 점수를 달성하는 Qwen-1.8B를 능가한다.\n' +
      '\n' +
      '##2 신경회로망\n' +
      '\n' +
      '이 절에서는 작은 언어 모델의 아키텍처 설계를 조사합니다. 실험은 사전 훈련된 데이터 세트에서 무작위로 샘플링된 50B 토큰에 대해 수행되며, 동일한 비율의 중국어와 영어 코퍼스를 사용한다. 기준선은 명시되지 않는 한 LLaMA 유사 아키텍처를 가진 1B 매개변수 모델이다. 다른 전략으로 구성된 모델을 ARC Easy(Clark et al., 2018), HellaSwag(Zellers et al., 2019) 및 C3(Sun et al., 2020)에 대해 비교한다.\n' +
      '\n' +
      '### Compact Tokenizer\n' +
      '\n' +
      '토네이저는 원본 자연 언어를 큰 언어 모델에 의한 처리에 적합한 토큰으로 매핑하는 역할을 하며, 각 토큰은 단어, 서브워드, 문자 또는 기호를 나타낸다. 다국어 토큰화기는 일반적으로 다양한 말뭉치를 다룰 수 있는 큰 어휘를 가지고 있다. 그러나, 작은 언어 모델의 맥락에서, 지나치게 큰 어휘는 모델의 파라미터의 상당한 부분을 상당히 차지할 수 있다. 예를 들어, Qwen-7B(Bai et al., 2023), Baichuan2-7B(Yang et al., 2023), 및 PanGu-\\(\\pi\\)-7B(Wang et al., 2023)의 어휘 크기는 각각 151936, 125696, 100883이다. 그들의 머리 및 임베딩 레이어의 파라미터는 전체 파라미터의 16.12%,13.72%,10.91%를 차지한다. 12개의 레이어와 2048의 너비를 가진 PanGu-\\(\\pi\\)-1B 모델은 동일한 토큰화기를 사용하여 전체(그림 3)의 실질적인 36.8%를 구성하는 헤드 및 임베딩 레이어의 매개변수를 본다. 이러한 분포는 본문이 아닌 어휘 표현에 모수의 상당한 할당으로 이어져 잠재적으로 모델의 전체 표현 능력을 제한한다. 따라서, 작은 언어 모델의 파라미터 비율을 줄이기 위해서는 토큰화기의 압축이 필수적이다.\n' +
      '\n' +
      '사실, 우리는 토큰화기에 상당한 중복성이 존재한다는 것을 발견한다. PanGu-\\(\\pi\\) 모델에서 상속된 100k 어휘로 토큰화기를 초기화하여 약 1.6T 토큰으로 구성된 방대한 코퍼스에서 빈도 분석을 수행했다. 그림 2에 묘사된 바와 같이, 토큰은 전체 훈련 코퍼스의 97.86%를 차지하는 상위 48k 어휘에서 롱테일 효과를 나타내는 것이 분명하다. 6개의 어휘 크기 {8k, 16k, 32k, 48k, 72k, 100k}를 대상으로 실험을 수행하였는데, 이는 각각 78.68%, 87.24%, 94.49%, 97.86%, 99.84% 및 100%의 누적 빈도를 차지한다. 50% 이상의 어휘는 말뭉치의 3% 미만을 다루기 때문에 중복될 수 있다.\n' +
      '\n' +
      ' 우리는 매개 변수를 줄이기 위해 저주파 어휘를 제거할 것을 옹호한다. 표 1은 토큰타이저 크기 2에 관한 성능 변화를 보여준다. 임베딩 및 헤드 레이어는 48k의 어휘를 사용할 때 1B 모델의 파라미터의 18.07%를 구성하며, 가장 좋은 평균 성능을 보여주고 32k의 어휘를 가진 모델이 그 뒤를 잇는다. 지나치게 작은 어휘를 사용하는 것은 성능 저하를 초래할 수 있다는 점은 주목할 만하다. 예를 들어, 8k 토큰화기가 코퍼스의 70% 미만을 커버하는 경우, 모델은 C3 및 ARC-E 데이터 세트에서 하위 성능을 나타낸다. 48k 크기의 토큰화기는 또한 전체 훈련 코퍼스에 걸쳐 평가되는 원래의 100k 크기의 토큰화기와 유사한 압축률을 나타낸다. 따라서 임베딩과 헤드 레이어의 파라미터 비율을 20% 이하로 유지하면서 코퍼스의 90% 이상을 커버하는 소형 토큰화기를 사용하는 것이 좋다.\n' +
      '\n' +
      '각주 2: 모든 모델의 크기는 깊이를 조정하여 1B로 제어됩니다.\n' +
      '\n' +
      '### Architecture Tweak\n' +
      '\n' +
      '본 논문에서는 1B 크기 언어 모델의 성능에 대한 FPN(feed-Forward Networks)의 깊이, 너비 및 확장 속도의 영향을 탐색하여 에지 디바이스용 LLM의 신경 구조 설계에 초점을 맞춘다. 다운스트림 작업에 대한 정확도 외에도 디코딩 속도는 작은 언어 모델에 대한 또 다른 중요한 측면이다. 무작위로 초기화된 모델을 사용하여 두 개의 프리픽스 토큰에서 510개의 새로운 토큰을 생성할 때 종단간 추론 속도(초당 토큰)를 테스트한다. FFP16을 사용하여 배치 크기 20인 단일 NVIDIA V100 GPU에서 속도를 테스트하고 섹션 2.1에서 제안한 대로 어휘 크기를 48k로 고정하고 모델의 크기를 1B 매개변수로 제한하여 모델의 깊이, 너비 및 확장 속도를 개별적으로 변경하는 효과를 탐색한다. 먼저 세 번째 변수를 일정한 수준으로 유지하면서 세 가지 구성 요소 중 두 가지를 조정하는 것이 모델의 전체 성능에 미치는 영향을 조사한다.\n' +
      '\n' +
      '깊이와 너비의 영향입니다. 우리는 표 2에 요약된 바와 같이 대표적인 구성을 철저히 조사하며, 여기서 _더 깊은 작은 언어 모델이 추론 속도의 비용으로 더 나은 성능을 보인다는 결론을 내릴 수 있다._ 모델의 깊이가 증가함에 따라 거의 세 벤치마크 모두에 대해 성능이 증가한다. 한편, 깊이가 이미 20인 경우, 추론 속도의 감소(29.49 \\(\\rightarrow\\) 12.81)에 비해 더 깊은 아키텍처를 설계함으로써 성능 향상(41.19 \\(\\rightarrow\\) 42.02)이 최소화되는 것을 관찰하였다. 따라서 48k 토큰화기로 1B-파라미터 모델에 대해 레이어 수를 약 20개로 설정하는 것이 좋다.\n' +
      '\n' +
      '표 3과 같이 깊이가 고정될 때 서로 다른 확장 속도에 대해 가까운 추론 속도를 관찰한다. 1:1 설정이 훨씬 더 나쁜 성능을 보이는 것은 분명합니다. 깊이, 너비 및 확장률 간의 상호 작용을 추가로 조사하기 위해 모델 크기를 1B 매개변수로 유지하면서 약 30개의 다른 매개변수 구성을 샘플링하고 5B 토큰을 포함하는 추가 간소화된 데이터 세트에 대한 훈련을 수행한다. 그림 4에서 알 수 있듯이 깊이(폭)와 하류 태스크의 평균 성능과의 상관 관계는 스피어먼 상관계수가 최대 0.528에 이르는 것으로 현저하게 더 높다. 반면 확장률과 모델의 최종 성능 사이에는 명백한 선형 관계가 없다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c|c} \\hline Tokenizer & PEHL (\\%) & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline\n' +
      '8k & 2.97 & 31.39 & 40.19 & 42.25 & 37.94\\\\\n' +
      '16k & 6.01 & 30.34 & 40.10 & 45.64 & 38.69\\\\\n' +
      '32k & 11.79 & 34.45 & 40.23 & 46.77 & 40.48\\\\\n' +
      '48k & 18.07 & 34.39 & **41.48** & **47.70** & **41.19**\n' +
      '72k & 26.88 & 34.39 & 39.21 & 46.58 & 40.06\\\\\n' +
      '100k & 38.19 & **34.98** & 39.11 & 47.10 & 40.40 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 성능 변동 _w.r.t._토키나이저 크기. PEHF는 전체 모델에 대한 임베딩 및 헤드 레이어의 비율을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c|c|c} \\hline Depth & Width & Speed & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline\n' +
      '40 & 1280 & 12.81 & **37.01** & 41.00 & **48.05** & *42.02**\n' +
      '30 & 1536 & 17.71 & 36.16 & 40.32 & 47.84 & 41.44\\\\\n' +
      '20 & 1792 & 29.49 & 34.39 & **41.48** & 47.70 & 41.19\\\\\n' +
      '15 & 2048 & 36.79 & 32.45 & 40.22 & 40.05 & 37.57\\\\\n' +
      '9 & 2560 & **57.53** & 32.63 & 31.06 & 42.68 & 35.46 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 어휘 크기와 확장률이 고정된 1B 크기 모델의 깊이와 폭을 가변한다. 속도는 초당 토큰으로 측정됩니다.\n' +
      '\n' +
      '그림 3: 모델의 본체와 토큰화기의 매개변수 비율입니다. (a) 대형 다국어 모델로부터 상속받은 대형 토큰화기(Wang et al., 2023). (b) 저주파 어휘를 제거하여 압축 토큰화 장치.\n' +
      '\n' +
      '그림 2: 상위 k 어휘의 누적 빈도, 여기서 97.86% 데이터는 작은 48k 토큰화기로 나타낼 수 있다.\n' +
      '\n' +
      '**토론.** 소형 토큰화기는 표현 능력과 구현 비용 사이의 중요한 균형을 이루기 때문에 작은 언어 모델에 특히 중요하다. 저주파수 어휘의 제거는 표현 능력을 크게 손상시키지 않으면서 실질적인 중복을 효율적으로 제거할 수 있게 한다. 또한, 폭, 깊이, 확장률 등 아키텍처의 구성이 소형 모델의 최종 성능에 상당한 영향을 미친다. 이 중 깊이가 작은 언어 모델의 주요 요소이며, 더 깊은 모델은 보통 더 낮은 속도를 희생시키면서 높은 성능을 달성한다. 위의 관찰에 따라 표 9에 자세히 설명된 대로 PanGu-\\(\\pi\\) Pro의 아키텍처를 설계한다.\n' +
      '\n' +
      '##3 파라미터 초기화\n' +
      '\n' +
      '이 섹션에서는 랜덤 초기화를 포함하여 주어진 신경 아키텍처로 모델의 매개변수를 초기화하고 큰 모델에서 매개변수를 상속하는 방법을 조사한다.\n' +
      '\n' +
      '### Random Initialization\n' +
      '\n' +
      '모델이 처음부터 학습될 때, 매개변수는 평균과 표준편차가 0인 정규분포\\(N(0,\\sigma^{2})\\)을 따르는 난수로 초기화된다. 잘 알려진 일련의 대형 언어 모델은 \\(\\sigma\\)의 값을 신중하게 설계하며, 특히 _w.r.t.layers를 변경한다. 예를 들어, GPT2(Radford et al., 2019)는 모든 선형 층 파라미터들에 \\(1/\\sqrt{N}\\)의 스케일을 적용하는데, 여기서 \\(N\\)은 잔여 층들의 수이다. InternLM(Team, 2023)은 일부 특별한 선형 층 파라미터들, 즉 MHA 층들에서의 아웃 투영 및 MLP 층들에서의 게이트 투영에만 동일한 스케일을 적용한다. 본 논문에서는 작은 언어 모델을 학습하기 위한 초기화 전략을 연구하였으며, 그 결과는 표 4와 같다. 단순화와 일반화를 위해, 우리는 작은 언어 모델을 훈련할 때 모든 계층에 상수 값을 사용하는 것을 추천한다. 더 많은 분석이 부록 B에 나와 있다.\n' +
      '\n' +
      '### Parameter Inheritance\n' +
      '\n' +
      '랜덤 초기화 외에도 이 작은 언어 모델의 초기 파라미터도 큰 언어 모델로부터 상속될 수 있다. 대형 모델의 강력한 일반화 능력은 소형 모델로 전이될 것으로 예상된다. 작은 모델에 비해 큰 모델은 보통 뉴런이 더 많은 층을 가지고 있다. 우리는 먼저 중요한 층을 선택한 다음 선택된 층에서 중요한 뉴런을 인식한다.\n' +
      '\n' +
      '중요한 레이어 선택.작은 모델이 큰 언어 모델보다 일반적으로 더 적은 레이어를 갖는다는 점을 고려하면, 최종 성능에 기여하는 가장 중요한 레이어가 인식되어야 한다. 결과적으로, 우리는 전체 성능에 대한 개별 층의 영향을 평가하기 위해 절제 실험을 수행한다.\n' +
      '\n' +
      '계층 중요도에 관한 일반적인 원리를 밝히기 위해 LLaMA2-7B, LLaMA2-13B, InternLM-7B 및 PanGu-\\(\\pi\\)-7B를 포함하여 널리 사용되는 여러 대형 언어 모델에 대해 다양한 실험을 수행한다. 추론 단계에서 특정 계층을 건너뛰고 결과 성능 저하를 평가한다. 각 모델에 대해 한 개의 레이어 건너뛰기, 두 개의 이웃 레이어 및 세 개의 연속 레이어를 포함하는 세 가지 유형의 레이어 건너뛰기 실험이 수행된다. 그 결과는 그림 5에 묘사되어 있으며, _i._, ARC-E, HellaSwag, C3의 세 가지 다운스트림 태스크에 대한 대규모 언어 모델의 평균 성능을 분석한다. \\(x\\)축은 생략된 계층 인덱스를 나타내고 \\(y\\)축은 성능 정확도를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline \\hline Initialization Method & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline Constant & 37.57 & 41.16 & **49.04** & **42.59** \\\\ GPT2 (Radford et al., 2019) & **38.62** & 39.34 & 48.44 & 42.13 \\\\ InternLM (Team, 2023) & 34.39 & **41.48** & 47.70 & 41.19 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 일정한 표준 편차 방법이 가장 잘 수행되는, 상이한 랜덤 초기화 전략 하에서의 성능.\n' +
      '\n' +
      '그림 4: 성능은 _w.r.t._ 모델의 너비, 깊이 및 확장률에 따라 달라집니다. 실험은 5B 토큰을 포함하는 간소화된 데이터 세트에 대해 수행된다. 정확도는 ARC 이지, 헬라 스웨그 및 C3 사이에서 평균되며 스피어먼 계수는 성능과 모델 구성 간의 상관 관계를 측정하는 데 사용된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c|c} \\hline \\hline EP Rate & Width & Speed & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline\n' +
      '1.00 & 2304 & 28.39 & 31.75 & 38.71 & 42.68 & 37.71\\\\\n' +
      '2.00 & 2048 & 28.68 & 33.33 & 41.34 & **48.55** & 41.07\\\\\n' +
      '2.77 & 1792 & 28.40 & 34.39 & **41.48** & 47.70 & **41.19**\n' +
      '4.00 & 1536 & 28.53 & **35.27** & 39.36 & 47.18 & 40.60 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 어휘 크기와 깊이가 고정된 1B 크기 모델의 확장률과 폭을 가변한다.\n' +
      '\n' +
      '이 모델에서는 몇 가지 흥미로운 공통 현상들이 식별된다. 얕은 층, 특히 초기 2~3개의 층은 입력 시퀀스에서 특징을 추출하는 데 중추적인 역할을 한다. 이러한 레이어를 제거하면 다운스트림 작업에서 상당한 성능 저하가 발생합니다. 유사하게, 깊은 층들 또한 중요하고, 그것들을 제거하는 것은 성능의 저하를 초래한다. 반대로, 중간 층들이 제거될 때, 성능은 덜 영향을 받으며, 이는 이들 층들 내에서 리던던시가 발생함을 나타낸다. 이러한 도면층은 매개변수를 상속할 때 제거되는 경향이 있습니다.\n' +
      '\n' +
      '계층 내 매개변수 선택.계층 내에서 중요한 매개변수는 다양한 메트릭에 의해 인식될 수 있다. 필수 파라미터를 인식하는 방법은 모델 가지치기 영역에서 잘 발견되었다 (Frantar and Alistarh, 2023; Ma et al., 2023). 뉴런의 중요성은 다양한 기준으로 측정될 수 있으며 가장 중요한 뉴런은 작은 모델의 초기화로 사용된다. 중요도 측정을 위해 가중치 규범인 \\(\\ell_{1}\\) 및 \\(\\ell_{2}\\)-norm이 일반적으로 사용되며, 이는 더 큰 가중치가 더 중요한 정보를 캡슐화함을 나타낸다(Han et al., 2015; Guo et al., 2016; Lee et al., 2021). 가중치와 기울기를 모두 포함하는 1차 테일러 확장(Lee et al., 2019; Tanaka et al., 2020)은 출력의 보다 정확한 추정으로 간주된다. 경험적 기준 외에도, 필수 가중치는 이진 마스크를 통해서도 식별될 수 있으며, 이는 훈련 과정에서 자동으로 학습된다(Xia et al., 2023; Tang et al., 2020). 후속 섹션에서는 이러한 방법론을 채택하여 더 작은 무게 치수를 갖는 1B 모델에 대한 초기 값으로 PanGu-\\(\\pi\\)-7B(Wang et al., 2023) 모델에서 중요한 매개변수를 선택한다.\n' +
      '\n' +
      '훈련 손실 곡선과 평가 결과는 그림 6과 표 5에 제시되어 있는데, 무작위로 초기화된 기본 모델과 비교하여 가지치기 전략으로 초기화된 작은 모델 각각은 더 낮은 손실로 수렴한다. 경험적 기준 중 테일러 확장은 주로 뉴런 중요도의 정확한 추정에 기인하여 우수한 결과를 산출한다. 학습 가능한 마스크를 사용하여 가지치기된 모델은 다른 모델보다 초기 손실이 현저히 낮고 궁극적으로 가장 낮은 손실로 수렴하는 것으로 시작한다. 세 데이터 세트에 걸친 평가 결과는 매개변수 상속의 유효성을 검증한다. 학습 가능한 전략으로 모델 매개변수를 상속하는 것이 좋습니다.\n' +
      '\n' +
      '논의.앞서 언급한 관찰은 모델의 초기 매개변수가 작은 언어 모델의 수렴 속도와 궁극적 성능 모두에 상당한 영향을 미친다는 것을 확인시켜준다. 더 큰 모델에서 매개변수를 상속하는 것은 일반적으로 더 유리한 선택인데, 이는 더 작은 모델이 더 큰 모델의 강력한 표현 능력을 동화할 수 있기 때문이다. 중요한 매개 변수를 선택하는 과정은 이와 관련하여 중요한 단계이다. 철저한 경험적 조사를 통해 중간 계층이 더 많은 중복성을 나타내는 경향이 있음을 관찰했으며 데이터 기반 학습 가능한 마스크는 이러한 중복 매개변수를 발굴하는 데 효과적임을 입증했다.\n' +
      '\n' +
      '그림 5: 몇 개의 레이어를 건너뛸 때 큰 언어 모델의 성능. “\\(x\\)Skip”은 인접한 \\(x\\) 층이 버려지는 것을 나타낸다. 중간 층 내에서 중복이 관찰되며, 시작과 끝 근처에 위치한 층은 성능을 유지하는 데 중요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c} \\hline Inheritance Strategy & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline Base & 36.68 & 40.34 & 49.15 & 42.06 \\\\ L1 (Ma et al., 2023) & 39.51 & 47.70 & 50.96 & 46.06 \\\\ L2 (Ma et al., 2023) & 41.98 & 48.33 & 50.68 & 47.00 \\\\ Taylor (Ma et al., 2023) & **43.21** & 48.43 & **52.05** & 47.90 \\\\ Learned (Xia et al., 2023) & 40.74 & **51.77** & 51.73 & **48.08** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 상이한 파라미터 상속 전략들 간의 비교. "Base"는 상속 없이 훈련하는 것을 의미한다.\n' +
      '\n' +
      '그림 6: 가지치기 전략이 다른 훈련 손실. "Base"는 상속 없이 처음부터 교육을 의미한다. 프루닝으로 모델 매개변수를 상속하면 손실이 더 적다.\n' +
      '\n' +
      '## 4 모델 최적화\n' +
      '\n' +
      '이 섹션에서는 주어진 신경 아키텍처와 초기 매개변수를 사용하여 최적화 전략을 조사한다. 먼저 학습률과 배치 크기 사이의 스케일링 규칙을 분석한다. 또한, 우리는 여러 번의 훈련을 계속함으로써 상당한 성능 향상을 관찰한다.\n' +
      '\n' +
      '### 배치 크기 및 학습률\n' +
      '\n' +
      '실용적인 언어 모델 훈련에서 배치 크기의 선택은 종종 당면한 계산 자원에 맞춰진다. 제한된 수의 GPU를 다룰 때 더 작은 배치 크기를 선택하는 것이 필요하다. 반대로, 상당한 수의 GPU가 우리의 처분에 있는 시나리오에서 배치 크기를 확대하면 반복 횟수를 효과적으로 줄일 수 있으므로 전체 훈련 프로세스를 신속하게 할 수 있다.\n' +
      '\n' +
      '그러나 배치 크기를 조정하면 일반적으로 모델의 최종 성능에 현저한 영향을 미칩니다. 배치 크기를 증가시킬 때 학습률을 비례적으로 조절하는 것이 일반적이다. 우리는 그림 7과 그림 8에서 각각 기본 배치 크기 \\(bs_{0}\\)와 학습률 \\(lr_{0}\\)을 1M과 \\(1\\times 10^{-4}\\)으로 설정한 식 \\(lr=(bs/bs_{0})^{r}\\times lr_{0}\\)을 사용하여 이들의 결합 효과를 탐색한다. \\\\ (r\\)은 증가율을 나타내며, 일반적으로 0.5 또는 1.0으로 설정된다(Krizhevsky, 2014; Goyal et al., 2017). 배치 크기가 4M보다 작을 때, 상이한 학습 속도를 갖는 수렴 속도는 일정하게 유지된다. 배치사이즈가 더 증가하면, 적당한 증분율((r=0.5\\))이 바람직하다. 동일한 훈련 토큰에서 과도하게 큰 배치 크기(\\(\\geq\\)16M)를 사용하면 수렴 속도에 부정적인 영향을 미친다. 대부분의 경우, 4M보다 작은 배치 크기는 모델 성능을 최적화하기 위한 안전한 범위로 간주된다. 그렇지 않으면, 최적화 전략들은 대형 배치 사이즈들에 대해 구체적으로 맞춤화될 필요가 있다(Keskar et al., 2016; You et al., 2017, 2019).\n' +
      '\n' +
      '### Multiple-Round Training\n' +
      '\n' +
      '기존의 방법들은 일반적으로 하나의 라운드(_i.e_)만으로 언어 모델을 훈련시키고, 모든 데이터는 모델을 업데이트하기 위해 오직 한 번만 사용되어 모델의 파라미터가 수렴되지 않게 한다. 또한, 대규모 코퍼스에 대한 학습은 재난적 망각(Toneva et al., 2018; Winata et al., 2023) 문제, _i.e_, 이전에 본 데이터에 대해 모델 성능이 떨어진다. 소형 모델의 경우 제한된 모델 용량이 망각 문제를 더욱 심각하게 만든다. 모델을 계속 훈련하면 훈련 손실을 더욱 줄일 수 있다.\n' +
      '\n' +
      '우리는 망각 문제를 검증하기 위해 간단한 실험을 수행한다. 트레이닝 손실이 대응하는 타임스탬프에서의 모델 파라미터들에 의해 계산되고, 트레이닝이 계속됨에 따라 모델 파라미터들이 업데이트됨에 따라, 이후의 데이터는 낮은 손실 값들을 갖는 경향이 있다. 따라서 1.6T 토큰으로 학습된 PanGu-\\(\\pi\\)-1B 모델을 사용하여 이전 데이터에 대한 배치별 손실을 재계산한다. 학습 데이터는 학습 전 8개의 부분으로 균등하고 랜덤하게 분할된다. 그림 9는 손실 값이 각 부분에 대해 _w.r.t._ 데이터가 어떻게 달라지는지를 보여준다. 높은 손실은 이전의 지식이 심각하게 잊혀졌다는 것을 나타낸다. 따라서, 잊혀진 데이터에 적합하도록 여러 라운드에 대한 모델을 훈련시킬 필요가 있다.\n' +
      '\n' +
      '학습 비용을 줄이기 위해 다중 라운드 학습을 위한 간단한 데이터 정제 전략을 제안한다. 일부 예는 적합하기 어렵다는 점을 고려할 때 높은 확률로 추가 훈련에 사용해야 한다. 특정 부분의 손실 값을 \\(L=\\{l_{1},l_{2},\\cdots,l_{N}\\}\\)으로 표시하며, 여기서 \\(N\\)은 이 부분의 전체 배치이다. 데이터는 트레이닝 프로세스에서 랜덤하게 셔플링되고, 따라서 각각의 배치는 다양한 유형 데이터를 포함한다는 점에 유의한다. 각 부분에서의 손실 값은 샘플링 확률인 _i.e., \\(p_{i}=\\frac{\\exp(l_{i})}{\\sum_{j=1}^{N}\\exp(l_{j})}\\를 나타내어 정규화된다. 다음 라운드 훈련에서는 샘플링 확률에 따라 \\(N_{0}\\) 배치를 \\(N\\)으로 샘플링한다. 샘플링 속도(\\(r=\\frac{N_{0}}{N}\\))의 영향은 표 6에 나와 있다. 이는 더 높은 것을 보여준다.\n' +
      '\n' +
      '그림 8: 배치 크기 및 학습 속도가 다른 경우의 성능.\n' +
      '\n' +
      '그림 7: 배치 크기 및 학습 속도가 다른 경우의 학습 손실.\n' +
      '\n' +
      '그림 9: 손실 값은 사전 훈련된 PanGu-\\(\\pi\\)-1B 모델을 사용하여 서로 다른 반복에 대해 _w.r.t._ 데이터를 변경한다. 손실은 각 부분의 배치 간에 평균화된다.\n' +
      '\n' +
      '샘플링 속도는 높은 성능을 달성하는 경향이 있다. 샘플링률 \\(r\\)이 50%를 초과할 때 성능향상은 미미하다. 그림 10에서 헬라 스웨그에 대한 평가 메트릭이 훈련 중에 어떻게 진화하는지를 보여준다. 2차 훈련이 진행됨에 따라 헬라 스웨그에 대한 정확도는 계속 상승하지만 후반 단계에서 수렴된다. 표 7에서는 더 많은 라운드로 모델을 훈련시키려고 노력합니다. 그러나, 그 성능 또한 서서히 포화되었다. 성능과 훈련 효율성 사이의 균형을 달성하기 위해 모델을 두 라운드로 훈련하고 샘플링 비율을 50%로 설정하는 것이 좋다.\n' +
      '\n' +
      '토론.더 큰 모델과 대조적으로, 작은 언어 모델은 제한된 용량으로 인해 데이터를 잊는 중대한 문제에 직면해 있다. 결과적으로, 다중 라운드 훈련 방식을 채택하는 것은 성능을 향상시키는 데 매우 중요하다. 데이터 샘플링을 사용하는 것은 어려운 예제에 대한 학습을 개선하는 동시에 교육 비용을 줄이는 데 효과적임을 입증합니다. 또한, 배치 크기와 학습 속도의 선택은 모델 성능에 중추적인 역할을 한다. 최적의 결과를 얻으려면 맞춤형 접근법에 특수 대형 배치 최적화기를 사용하지 않는 한 4M보다 작은 배치 크기를 사용하는 것이 좋다.\n' +
      '\n' +
      '##5 PanGu-\\(\\pi\\) Pro\n' +
      '\n' +
      '위의 광범위하고 엄격한 실험들을 바탕으로, 우리는 이전의 PanGu-\\(\\pi\\)-1B를 상당히 개선시키는 한편, 더 크고 강력한 PanGu-\\(\\pi\\)-1.5B Pro를 구축한다. 이 섹션에서는 우리의 결과가 새로운 SOTA를 확립하는 기존의 오픈 소스 소형 언어 모델과 비교한다. 구체적으로, PanGu-\\(\\pi\\)-1.5B Pro는 최근 제안된 Qwen-1.8B(Bai et al., 2023) 및 Phi2-2.7B(Li et al., 2023)보다 평균 성능이 1.8배 더 크다.\n' +
      '\n' +
      '구현 내용.1.6T 토큰으로 구성된 사전 학습 데이터는 영어 및 중국어 코퍼스를 약 \\(1:1\\) 척도로 포괄하여 인터넷으로부터 다양한 소스로부터 수집된다. 사용된 48k 토큰화기는 SentencePiece(Kudo and Richardson, 2018)로부터 바이트-페어 인코딩(BPE, Shibata et al. (1999))에 의해 우리의 데이터에 기초하여 구축된다. 모델은 초기 학습률 \\(2\\times 10^{-4}\\)을 갖는 코사인 학습률 감쇠(Loshchilov and Hutter, 2016)를 이용하여 \\(\\beta_{1}=0.9,\\beta_{2}=0.95\\)을 갖는 AdamW 최적화기(Loshchilov and Hutter, 2017)를 이용하여 학습된다. 트레이닝 프로세스의 총 배치 크기는 2M입니다. 우리는 PanGu-\\(\\pi\\)(Wang et al., 2023) 아키텍처를 따르면서 PanGu-\\(\\pi\\)-1B를 훨씬 더 깊게 만든다. 자세한 구성은 표 9에서 확인할 수 있다. 표 3에서 제시한 바와 같이 확장률을 2.77로 설정하였으며, 파라미터 초기화 방법은 중간 중복 레이어를 제거한 후 학습 가능한 이진 마스크를 통해 PanGu-\\(\\pi\\)-7B로부터 파라미터를 상속한다. 제안된 PanGu-\\(\\pi\\) Pro를 훈련하고 평가하기 위해 화웨이 어센드 910 카드를 사용한다.\n' +
      '\n' +
      'Benchmarks.OpenCompass(Contributors, 2023)를 사용하여 광범위한 다운스트림 작업, 검사, 지식, 추론 및 이해 능력을 평가하여 포괄적인 비교를 수행한다. C-Eval(Huang et al., 2023)은 지식 및 추론 능력을 평가하기 위한 중국어 벤치마크이다. CMMLU(Li et al., 2023)는 과학, 공학, 인문학 등 67개의 주제를 다루고 있다. MMLU(Hendrycks et al., 2021)는 수학, 역사, 컴퓨터 과학, 법칙 등 57개의 과제를 포괄하여 LLM의 멀티태스크 정확도를 측정하기 위한 영어 벤치마크를 제안한다. AGI-Eval(Zhong et al., 2023)은 인간의 인지 및 문제 해결에 관련된 과제에서 일반적인 능력을 평가하기 위해 특별히 설계된 벤치마크이다. BoolQ(Clark et al., 2019)는 LLMs의 어려운 수반 유사 추론 능력을 평가하기 위한 읽기 이해 데이터세트이다. AX-b(Wang et al., 2020)는 광범위한 진단 과제이고 PIQA(Bisk et al., 2020)는 물리적 상호작용 질문-답변 과제이다. EPRSTM(Xu et al., 2021)은 상품 리뷰를 기반으로 한 바이너리 감성 분석 데이터셋이다. XSum(Narayan et al., 2018)은 영국 방송사에서 수집한 요약 과제이며 C3(Sun et al., 2020)은 13,369개의 문서와 관련 19,577개의 객관식 질문을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c} \\hline \\hline Sampling Rate \\(r\\) & ARC-E & HellaSwag & C3 & Average \\\\ \\hline\n' +
      '0\\% & 42.68 & 57.95 & 54.19 & 51.61\\\\\n' +
      '25\\% & 43.95 & 59.55 & 56.01 & 53.17\\\\\n' +
      '50\\% & 45.33 & 60.67 & 57.37 & 54.46\\\\\n' +
      '75\\% & 45.52 & 60.34 & 58.16 & 54.67\\\\\n' +
      '100\\% & 44.98 & 60.88 & 58.74 & 54.87 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 다음 라운드 트레이닝에 대한 샘플링 레이트. 모델은 두 라운드로 훈련하고 있습니다. \\ (r=0\\)은 한 라운드로 훈련하는 것을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c} \\hline \\hline Training round & ARC-E & HellaSwag & C3 & Average \\\\ \\hline Single round & 42.68 & 57.95 & 54.19 & 51.61 \\\\ Two round & 45.33 & 60.67 & 57.37 & 54.46 \\\\ Three round & 45.11 & 61.32 & 56.88 & 54.44 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 훈련 라운드 수의 영향. 샘플링 레이트 \\(r\\)는 50%로 설정된다.\n' +
      '\n' +
      '그림 10: 훈련 중 헬라 스웨그에 대한 PanGu-\\(\\pi\\)-1B 및 PanGu-\\(\\pi\\)-1B Pro의 정확도.\n' +
      '\n' +
      '소형 언어 모델과의 비교.1B에서 3B까지 크기가 다른 여러 소형 언어 모델을 수집한다. 이들은 TinyLLaMA-1.1B(Peiyuan Zhang and Lu, 2023), Chinese-LLaMA2-1.3B(Cui et al., 2023), Sheared-LLaMA-1.3B(Xia et al., 2023), Open-LLaMA-3B(Geng and Liu, 2023)를 포함한다. Meituan(Chu et al., 2023)은 RedPajama dataset(Computer, 2023)에서 처음부터 훈련된 MobileLLaMA-1.4B 및 MobileLLaMA-2.7B를 출시하였다. 마이크로소프트는 작은 언어 모델들과 함께 "교과서-품질" 데이터를 사용하는 것에 초점을 맞춘 일련의 Phi(Li et al., 2023)를 개발했다. RWKV-5-1.5B(Peng et al., 2023)는 Transformer-level LLM 성능을 갖는 병렬화 가능한 RNN이다. Qwen-1.8B(Bai et al., 2023)는 웹 텍스트, 책, 코드, _etc_를 포함하는 2.2조 토큰에 대해 사전 훈련된다.\n' +
      '\n' +
      '표 8의 광범위한 실험은 PanGu-\\(\\pi\\)-1.5B Pro가 유사하거나 심지어 더 큰 크기인 _e.g_, Phi2-2.7B 및 Open-LLaMA-3B의 기존 LLM을 상당히 능가한다는 것을 보여준다. 우리는 PanGu-\\(\\pi\\)-1B에서 PanGu-\\(\\pi\\)-1B Pro로 평균 8.77의 성능 향상을 관찰하였다. 16.67% 적은 파라미터로 PanGu-\\(\\pi\\)-1.5B Pro는 Qwen-1.8B(Bai et al., 2023)보다 우수한 성능을 보이며, 대부분의 벤치마크에서 가장 우수하거나 두 번째로 우수한 성능을 보인다. 전반적으로, 우리의 모델은 현재 최신 모델에 비해 일관되게 더 나은 평균 성능을 나타낸다.\n' +
      '\n' +
      'PanGu-\\(\\pi\\)-1B에서 점차 핵심 구성 요소를 추가하여 방법론의 유효성을 검증한다. 그림 1에서 볼 수 있듯이 저주파 어휘를 제거하면 평균 성능이 42.41에서 44.11로 향상되는 반면 아키텍처 트윗은 또 다른 2.42 개선에 기여한다. 가장 효과적인 방법인 매개변수 상속은 평균 성능을 49.79로 높이며, 다중 라운드 훈련은 PanGu-\\(\\pi\\)-1B Pro의 평균 성능을 더욱 향상시킨다.\n' +
      '\n' +
      '##6 결론 및 논의\n' +
      '\n' +
      '본 논문에서는 3가지 관점, 즉 신경망 구조, 파라미터 초기화, 최적화 전략으로부터 작은 언어 모델을 구성하는 방법에 대해 체계적으로 논의한다. 세심하게 설계된 경험적 연구를 통해, 주어진 파라미터 제한과 데이터 크기에 따라 성능 향상을 위한 몇 가지 효과적인 설계 공식을 인식하였으며, 소형 토큰화기, 아키텍처 트위크, 파라미터 상속, 다중 라운드 트레이닝 _etc_를 포함한다. 그런 다음 1B 및 1.5B 매개변수를 사용하여 PanGu-\\(\\pi\\) Pro 모델을 훈련하여 기본 모델보다 성능을 크게 개선한다.\n' +
      '\n' +
      '관찰을 바탕으로 우리는 또한 추가 탐사를 위한 몇 가지 흥미로운 방향에 주목한다. 신경 아키텍처 측면에서 표현 능력과 매개변수 효율성을 모두 원활하게 통합하는 소형 토큰화기를 직접 학습하는 방법. 또한 하드웨어 친화적인 아키텍처를 탐색하면 계산 및 스토리지 비용을 줄일 수 있습니다. 예를 들어, GQA(Ainslie et al., 2023)는 에지 디바이스들의 RAM 요구를 감소시키기 위한 효과적인 전략이다(부록 A를 참조). 모델 최적화와 관련하여 초기부터 모델을 고성능으로 향하는 경로에 설정하여 효과적인 매개변수 초기화의 중요성을 과장할 수 없다. 그럼에도 불구하고, 문제는 현장에서 열린 질문인 효과적인 매개변수를 식별하는 데 남아 있다. 또한, 작은 모델의 훈련 특성은 큰 모델과 크게 다르다. 예를 들어, 다중 라운드 트레이닝의 프레임워크 내에서, 새로운 파라미터 최적화 기술 및 데이터 정제 방법의 개발에 대한 긴급한 요구가 있다. 많은 질문이 심층적인 탐구를 보증하며, 본 논문에서 제시된 연구 결과가 추가 연구에 영감을 불러일으킬 수 있기를 바란다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline Model & Width & Depth & Vocabulary & Initialization & & & & & \\\\ \\hline PanGu-\\(\\pi\\)-1B & 2048 & 12 & 100883 & Random & & & & & \\\\ PanGu-\\(\\pi\\)-1B Pro & 1792 & 21 & 48000 & PanGu-\\(\\pi\\)-7B & & & & & \\\\ PanGu-\\(\\pi\\)-1.5B Pro & 2048 & 22 & 48000 & PanGu-\\(\\pi\\)-7B & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 모델 구성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{**Examination**} & \\multicolumn{2}{c}{**Knowledge**} & \\multicolumn{2}{c}{**Reasoning**} & \\multicolumn{2}{c}{**Understanding**} & \\\\ \\cline{2-10}\n' +
      '**Models** & C-Eval & CMMLU & MMLU & AGI-Eval & \\multicolumn{1}{c}{BoolQ} & \\multicolumn{1}{c}{AX-b} & PIQA & EPRSTMT & XSum & C3 & **Average** \\\\ \\hline MobileLLaMA-1.4B & 23.93 & 25.10 & 25.05 & 18.53 & 58.75 & 45.20 & 71.27 & 46.25 & 18.19 & 37.42 & 36.97 \\\\ Sheared-LLaMA-1.3B & 24.28 & 25.10 & 25.77 & 18.01 & 62.39 & 43.57 & 72.91 & 46.25 & 16.44 & 35.45 & 37.02 \\\\ TinyLLaMA-1.1B & 27.85 & 24.64 & 25.75 & 18.54 & 56.06 & 45.47 & 70.62 & 46.25 & 20.15 & 36.71 & 37.20 \\\\ MobileLLaMA-2.7B & 23.53 & 25.55 & 26.63 & 18.43 & 54.74 & 55.80 & 72.85 & 46.25 & 16.96 & 36.11 & 37.69 \\\\ Chinese-LLaMA2-1.3B & 28.70 & 24.78 & 24.55 & 19.40 & 56.79 & 47.46 & 56.91 & 72.50 & 8.90 & 43.12 & 38.31 \\\\ RWKV-5-1.5B & 25.92 & 25.14 & 25.66 & 19.01 & 62.29 & 54.05 & 71.22 & 46.25 & 20.67 & 49.15 & 39.94 \\\\ Phi-1.3B & 27.78 & 25.85 & 44.32 & 23.42 & 23.52 & 44.20 & 76.99 & 50.00 & 14.90 & 38.96 & 41.99 \\\\ PanGu-\\(\\pi\\)-1B & 36.85 & 35.90 & 35.96 & 30.77 & 58.44 & 43.48 & 61.92 & 55.62 & 15.92 & 49.21 & 42.41 \\\\ Open-LLaMA-3B & 27.50 & 25.42 & 27.09 & 26.68 & 60.58 & 52.72 & 72.09 & 82.50 & 19.75 & 43.23 & 43.66 \\\\ Phi2-2.7B & 31.86 & 32.18 & **58.49** & 28.51 & **77.40** & 43.57 & **78.89** & 46.25 & 13.66 & 40.11 & 45.09 \\\\ PanGu-\\(\\pi\\)-1B Pro (**Ours**) & 46.50 & 46.56 & 50.38 & 41.58 & 63.43 & 53.99 & 64.96 & 74.38 & 18.40 & 52.66 & 51.28 \\\\ Qwen-1.8B & **53.60** & **52.12** & 46.43 & 35.83 & 64.31 & **57.79** & 73.83 & 88.12 & 20.03 & 58.30 & 55.04 \\\\ PanGu-\\(\\pi\\)-1.5B Pro (**Ours**) & 52.91 & 49.51 & 53.76 & **44.42** & 63.73 & 55.93 & 73.94 & **89.38** & **22.23** & **59.56** & **56.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: SOTA 오픈 소스 소형 언어 모델과의 비교. 최상의 모델은 굵은 글씨로 나열되고 차선책은 밑줄로 나열됩니다.\n' +
      '\n' +
      '## 7 영향 진술\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리의 작업에는 많은 잠재적인 사회적 결과가 있으며, 우리가 특별히 강조해야 한다고 느끼는 것은 없다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: 다중 헤드 체크포인트로부터 일반화된 다중 쿼리 트랜스포머 모델을 트레이닝한다. _ arXiv preprint arXiv:2305.13245_, 2023.\n' +
      '*Bai 등(2023) Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Lu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, J., Yang, J., Yang, J., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, Z., Zhou, C., Zhou, X., Zhu, J., M. Qwen 기술 보고서입니다 arXiv preprint arXiv:2309.16609_, 2023.\n' +
      '* Bisk 등(2020) Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. 피카: 자연어로 물리적 상식에 대한 추론. 2020년 인공 지능에 관한 34번째 AAAI 회의에서.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* Chu et al. (2023) Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., and Shen, C. Mobilevlm: A fast, strong and open vision language assistant for mobile devices, 2023.\n' +
      '* Clark et al. (2019) Clark, C., Lee, K., Chang, M. - W., Kwiatkowski, T., Collins, M., and Toutanova, K. 불크: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구하는 것. 2019년 _NAACL_에서\n' +
      '* Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 질문에 답하는 걸 해결했다고 생각해? try arc, the ai2 reasoning challenge, 2018.\n' +
      '* 컴퓨터(2023) 컴퓨터, T. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* Contributors (2023) Contributors, O. Opencompass: 기초 모델에 대한 보편적인 평가 플랫폼. [https://github.com/open-compass/opencompass] (https://github.com/open-compass/opencompass), 2023.\n' +
      '* Cui et al. (2023) Cui, Y., Yang, Z., and Yao, X. 중국 라마와 알파카에 대한 효율적이고 효과적인 텍스트 인코딩. _ arXiv preprint arXiv:2304.08177_, 2023. URL[https://arxiv.org/abs/2304.08177](https://arxiv.org/abs/2304.08177).\n' +
      '* Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive 언어 모델은 원샷으로 정확하게 가지치기될 수 있다. In _International Conference on Machine Learning_, pp. 10323-10337. PMLR, 2023.\n' +
      '* Geng & Liu(2023) Geng, X. and Liu, H. Openllama: An open reproduction of llama, May 2023. URL[https://github.com/openlm-research/open_1lama](https://github.com/openlm-research/open_1lama).\n' +
      '* Goyal et al. (2017) Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. 정확하고 큰 미니 배치 sgd: 1시간 안에 이미네를 훈련시켜라 _ ArXiv:1706.02677_, 2017.\n' +
      '* Guo et al. (2016) Guo, Y., Yao, A., and Chen, Y. 효율적인 DNS를 위한 동적 네트워크 수술입니다. Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pp. 1379-1387, 2016.\n' +
      '* Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks. _ CoRR_, abs/1506.02626, 2015. URL[http://arxiv.org/abs/1506.026](http://arxiv.org/abs/1506.02626).\n' +
      '* Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. _ The International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* Huang et al. (2023) Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J. C-eval: Multi-level multi-discipline chinese evaluation suite for foundation models. _ arXiv preprint arXiv:2305.08322_, 2023.\n' +
      '* Keskar et al. (2016) Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima. _ ArXiv:1609.04836_, 2016.\n' +
      '* Krizhevsky et al. (2014)* Krizhevsky (2014) Krizhevsky, A. 컨볼루션 뉴럴 네트워크들을 병렬화하기 위한 하나의 이상한 트릭. ArXiv:1404.5997. 인용: SS1.\n' +
      '*T. Kudo and J. Richardson (2018)Sentencepiece: 신경 텍스트 처리를 위한 단순하고 언어 독립적인 서브워드 토큰화기와 디토키나이저. ArXiv:1808.06226. 인용: SS1.\n' +
      '*J. Lee, S 박성호 모성호 An, and J. Shin (2021)Layer-adaptive sparsity for the magnitude-based pruning. 제9회 International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, External Links: Cited by: SS1.\n' +
      '*N. 이태호 Ajanthan, and P. H. S. Torr (2019)Snip: single-shot network pruning based on connection sensitivity. 제7회 International Conference on Learning Representations, ICLR 2019, New Orleans, 미국 LA, 5월 6-9일, External Links: Cited by: SS1.\n' +
      '*H. Li, Y. 장포코토 양호자오 공남 두안, T 볼드윈(2023)Cmmlu: 중국의 대규모 멀티태스킹 언어 이해도를 측정한다. 외부 링크: 2309.05463 인용: SS1.\n' +
      '*Y. 이성 부벡 엘단, A 델 조르노, S Gunasekar, Y. T. Lee(2023) 교과서는 ii: phi-1.5 기술 보고서만 있으면 됩니다. ArXiv:2309.05463. 인용: SS1.\n' +
      '* I. Loshchilov and F. Hutter(2016)Sgdr: stochastic gradient descent with warm restart. ArXiv:1608.03983. 인용: SS1.\n' +
      '* I. Loshchilov and F. Hutter (2017) Decoupled Weight decay regularization. ArXiv:1711.05101. 인용: SS1.\n' +
      '*X. 마기방, 엑스 Wang(2023)Llm-pruner: on the structural pruning of large language models. ArXiv:2305.11627. 인용: SS1.\n' +
      '* S. 나라얀, S. B. 코헨, M. 라파타(2018) 자세한 내용은 말하지 말고 요약만 해주세요! topic-aware convolutional neural networks for extreme summarization. 인용: SS1.\n' +
      '* G. Peiyuan Zhang, T. W. Wang, and W. Tu(2023)TinyLlama. 외부 링크: 2305.11627 인용: SS1.\n' +
      '* B. Peng, E. Alcaide, Q. 안소니 A. 알발락 S. 아르카디뉴, H. 조, X. 청민 정민 Grella, K. K. G. GV, et al. (2023)Rwkv: Reinventing rnns for the transformer era. ArXiv:2305.13048. 인용: SS1.\n' +
      '* A. Radford, K. 나라심한 Salimans, I. Sutskever, et al.(2018) Generative Pre-training에 의한 언어 이해력 향상. 인용: SS1.\n' +
      '* A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.(2019)Language models are unsupervised multitask learners. OpenAI blog1(8), pp. 9. Cited by: SS1.\n' +
      '*X. 렌영주 멍진 황영 왕욱 리필장 Podolskiy, G. Arshinov, et al.(2023)Pangu-\\(\\Sigma\\): 희소 이종 컴퓨팅을 갖는 조조 파라미터 언어 모델을 향해. ArXiv:2303.10845. 인용: SS1.\n' +
      '*N. Shazeer(2019)Fast transformer decoding: 쓰기 헤드 하나만 있으면 됩니다. ArXiv:1911.02150. 인용: SS1.\n' +
      '*Y. 시바타 기다상 후카마치 타케다, A. 시노하라, T. 신오하라, S. 아리카와(1999)바이트 쌍 인코딩: 패턴 매칭을 가속화하는 텍스트 압축 방식. 인용: SS1.\n' +
      '*K. Sun, D. Yu, D. Yu, C. Cardie(2020)는 중국어 기계 독해력에 도전하기 위한 사전 지식을 조사하고 있다. 컴퓨터 언어학 협회의 트랜잭션. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '* H. Tanaka, D. Kunin, D. L. K. Yamins, and S. Ganguli (2020)Pruning neural networks without any data by iteratively preserving synaptic flow. 신경 정보 처리 시스템 33의 발전: 신경 정보 처리 시스템 2020에 대한 연례 회의, NeurIPS 2020, 12월 6-12, 2020, 가상, 외부 링크: 인용된 링크: SS1.\n' +
      '*Y. 탕영 왕영 Xu, D. Lu, C. Xu, 및 C. Xu(2020)Scop: 신뢰성 있는 신경망 가지치기를 위한 과학적 제어. Advanced in Neural Information Processing Systems33, pp. 10936-10947. Cited by: SS1.\n' +
      '* I. Team(2020) Internlm: 점진적으로 향상된 기능을 가진 다국어 언어 모델. 인용: SS1.\n' +
      '* M. Toneva, A. Sordoni, R. T. D. Combes, A. Trischler, Y. Bengio, and G. J. Gordon (2018) a empirical study of example forgetting during deep neural network learning. ArXiv:1812.05159. 인용: SS1.\n' +
      '* H. 투브론, T. 라브릴, G. 이자카드, X. 마티넷 라초, T. 라크루아, B. 로지에르, N. Goyal, E. Hambro, F. Azhar, et al.(2023)Llama: open and efficient foundation language models. ArXiv:2302.13971. 인용: SS1.\n' +
      '* A. Vaswani, N. N. 쉐이저 파마르, J. 우즈코리트, L. 존스, A. N. 고메즈, L. Kaiser와 I. Polosukhin(2017)은 당신이 필요로 하는 모든 것이다. 신경 정보 처리 시스템들(30. 인용: SS1).\n' +
      '*A. Wang, Y. 프룩사차쿤 난지아, A. 싱, J. 마이클, F. 힐, O. Levy, and S. R. Bowman (2020) Superglue: 범용 언어 이해 시스템에 대한 더 엄격한 벤치마크. 외부 링크: 2006.00189 인용: SS1.\n' +
      '\n' +
      'Wang, Y., Chen, H., Tang, Y., Guo, T., Han, K., Nie, Y., Wang, X., Hu, H., Bai, Z., Wang, Y., et al. Pangu-\\(\\pi\\): 비선형성 보상을 통한 향상된 언어 모델 아키텍처들 _ arXiv preprint arXiv:2312.17276_, 2023.\n' +
      '*Wei 등(2023) Wei, T., Zhao, L., Zhang, L., Zhu, B., Wang, L., Yang, H., Li, B., Cheng, C., Lu, W., Hu, R., Li, C., Yang, L., Luo, X., Wu, X., Lu, L., Cheng, W., Zhang, J., Zhang, X., Lin, L., Wang, X., Ma, Y., Dong, C., Sun, Y., Chen, Y., Peng, Y., Liang, X., Yan, S., Fang, H., 및 Zhou, Y. 스카이워크: 좀 더 개방적인 이중언어 기반 모델, 2023.\n' +
      '* Winata et al. (2023) Winata, G. I., Xie, L., Radhakrishnan, K., Wu, S., Jin, X., Cheng, P., Kulkarni, M., and Preotiuc-Pietro, D. Overcoming catastrophic forgetting in massively multiilingual continual learning. _ arXiv preprint arXiv:2305.16252_, 2023.\n' +
      '* Xia et al. (2023) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning. 2023년\n' +
      '* Xu et al. (2021) Xu, L., Lu, X., Yuan, C., Zhang, X., Xu, H., Yuan, H., Wei, G., Pan, X., Tian, X., Qin, L., et al. Fewclue: A chinese few-shot learning evaluation benchmark. _ arXiv preprint arXiv:2107.07498_, 2021.\n' +
      '* Yang et al. (2023) Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan 2: Open large-scale language models. _ arXiv preprint arXiv:2309.10305_, 2023.\n' +
      '* Yi(2023) Yi. 01-ai. [https://github.com/01-ai/Yi] 개발자들에 의해 처음부터 훈련된 일련의 대형 언어 모델들 (https://github.com/01-ai/Yi), 2023.\n' +
      '* You et al. (2017) You, Y., Gitman, I., and Ginsburg, B. Large batch training of convolutional networks. _ ArXiv:1708.03888_, 2017.\n' +
      '* You et al. (2019) You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. 딥러닝을 위한 대규모 배치 최적화: 76분 안에 버트를 훈련합니다. _ ArXiv preprint arXiv:1904.00962_, 2019.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? 2019년.\n' +
      '* Zeng et al. (2022) Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. GIm-130b: An open bilingual pre-trained model. _ ARXiv 프리프린트 arXiv:2210.02414_, 2022.\n' +
      '* Zhong et al. (2023) Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. Agieval: 2023년 기반 모델을 평가하기 위한 인간 중심 벤치마크.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '## 부록 D 가중치 감소\n' +
      '\n' +
      '체중 감소(Loshchilov and Hutter, 2017)는 훈련 세트에서 과적합을 완화하기 위해 일반적으로 사용되는 정규화 방법이다. 표 13에서 그 영향을 조사한다. 중량 감쇠를 높이는 것은 모델의 표현 능력을 제한하는 비용을 감수하더라도 더 강력한 규칙화를 부여한다. 실험적 실험을 통해, 우리는 모델이 중량 감쇠를 0.1로 설정했을 때 최적의 성능을 달성함을 관찰한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c} \\hline \\hline Weight Decay & ARC-E & HellaSwag & C3 & Average \\\\ \\hline\n' +
      '0.2 & 34.68 & 36.15 & 45.31 & 38.71\\\\\n' +
      '0.1 & 34.39 & **41.48** & **47.70** & *41.19**\n' +
      '0.01 & **34.74** & 36.76 & 45.26 & 38.92\\\\\n' +
      '0.001 & 33.59 & 37.07 & 44.93 & 38.53\\\\\n' +
      '0.0001 & 31.22 & 37.76 & 44.11 & 37.70 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 상이한 중량 붕괴하에서의 성능. 모델은 0.1의 중량 감쇠로 최상의 성능을 달성했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c|c} \\hline \\hline Initialization Method & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline Constant & 37.57 & 41.16 & 49.04 & 42.59 \\\\ GPT2 (Radford et al., 2019) & **38.62** & 39.34 & 48.44 & 42.13 \\\\ InternLM (Team, 2023) & 34.39 & 41.48 & 47.70 & 41.19 \\\\ Ours & 37.57 & **42.00** & **49.26** & **42.94** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 상이한 초기화 전략들 하에서의 성능. 우리의 방법은 일정한 표준 편차 접근법에 비해 약간의 모서리를 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c|c} \\hline \\hline Heads & Head Dimension & Speed & ARC-E & HellaSwag & C3 & Avg. \\\\ \\hline\n' +
      '14 & 128 & 29.49 & 34.39 & 41.48 & 47.70 & 41.19\\\\\n' +
      '28 & 64 & 30.11 & 35.39 & 41.63 & 48.09 & 41.70\\\\\n' +
      '56 & 32 & 30.49 & 33.16 & 41.36 & 48.17 & 40.90 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 주의 헤드 수를 변경함.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>