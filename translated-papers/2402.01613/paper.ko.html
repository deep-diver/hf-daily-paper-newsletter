<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Nomic Embed: 재현 가능한 Long Context Text Embedder 훈련\n' +
      '\n' +
      'Zach Nussbaum\n' +
      '\n' +
      'zach@nomic.ai\n' +
      '\n' +
      '앤 존 X. 모리스\n' +
      '\n' +
      'jack@nomic.ai\n' +
      '\n' +
      'jxm3@cornell.edu\n' +
      '\n' +
      '&Brandon Duderstadt\n' +
      '\n' +
      'brandon@nomic.ai\n' +
      '\n' +
      '&Andriy Mulyar\n' +
      '\n' +
      'andriy@nomic.ai\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이 기술 보고서에서는 오픈아이디어 Ada-002와 오픈아이디어 텍스트 임베딩-3-small를 모두 능가하는 최초의 완전 재현성, 오픈소스, 오픈 가중치, 오픈 데이터, 8192 컨텍스트 길이 영어 텍스트 임베딩 모델인 nomic-embed-text-v1의 훈련에 대해 기술한다. 우리는 아파치 2 라이센스에 따라 훈련 코드와 모델 가중치를 공개합니다. 다른 오픈 소스 모델과 달리, 우리는 nomic-embed-text-v1의 완전한 복제를 허용하는 2억 5천 5백만 개의 큐레이션된 텍스트 쌍을 가진 훈련 데이터 로더를 출시한다. [https://github.com/nomic-ai/contrastors](https://github.com/nomic-ai/contrastors]에서 모델을 복제할 코드와 데이터를 찾을 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트 임베딩은 LLMs 및 시맨틱 검색 Lewis 등(2021); Izacard 등(2022); Ram 등(2023)을 위한 검색-증강-생성(RAG)을 파워링하는 현대 NLP 애플리케이션의 필수적인 구성요소이다. 이러한 임베딩은 데이터 시각화, 분류 및 정보 검색을 위한 클러스터링과 같은 다운스트림 애플리케이션에서 사용되는 저차원 벡터로서 문장 또는 문서에 대한 의미 정보를 인코딩한다.\n' +
      '\n' +
      'MTEB 벤치마크 Muennighoff 등(2023) 상의 상위 오픈 소스 모델의 대부분은 E5 Wang 등(2022), GTE Li 등(2023) 및 BGE Xiao 등(2023)과 같은 512의 컨텍스트 길이로 제한된다. 이러한 짧은 컨텍스트 길이는 전체 문서 의미학이 문장 또는 단락에 국한되지 않는 도메인에서 모델 유틸리티를 감소시킨다. 컨텍스트 길이가 2048보다 긴 대부분의 상위 임베딩 모델은 Voyage-lite-01-instruct Voyage(2023) 및 text-embedding-ada-002 Neelakantan 등(2022)과 같은 클로즈드 소스이다.\n' +
      '\n' +
      '오픈 소스 롱 컨텍스트 임베딩 모델을 수행하는 상위 2개는 jina-embedding-v2-base-en Gunther et al.(2024) 및 E5-Mistral-7b-instruct Wang et al.(2023)이다.\n' +
      '\n' +
      '불행하게도, jina-embedding-v2-base는 OpenAI의 text-embedding-ada-002 Neelakantan 등(2022)을 능가하지 않는다(표 1 참조). 또한, E5-Mistral Wang et al. (2023)은 7B 파라미터 트랜스포머의 큰 추론 요건으로 인해 많은 엔지니어링 애플리케이션에서 사용하기에 실현 가능하지 않으며, 4096 토큰을 넘어서는 사용에 권장되지 않는다.\n' +
      '\n' +
      '이 보고서는 짧은 컨텍스트 벤치마크와 긴 컨텍스트 벤치마크 모두에서 OpenAI 텍스트 임베딩-ada 및 텍스트 임베딩-3-작은 성능을 능가하는 137M 매개변수인 nomic-embed-text-v1, 오픈 소스, 오픈 가중치, 오픈 데이터, 8192 시퀀스 길이 모델을 어떻게 훈련했는지 설명한다(표 1). 아파치-2 라이센스에 따라 모델 가중치 및 코드베이스를 릴리스합니다. 우리는 또한 모델의 종단 간 자동 확장성 및 복제를 가능하게 하기 위해 선별된 교육 데이터 세트를 출시한다.\n' +
      '\n' +
      '그림 1: **텍스트 임베딩 모델 벤치마크. 짧고 긴 컨텍스트 벤치마크에서 nomic-embed-text-v1, OpenAI 텍스트-embedding-ada, OpenAI 텍스트-embedding-3-small 및 jina-embedding-base-v2의 집계 성능. Nomic Embed는 OpenAI 텍스트 임베딩-ada, OpenAI 텍스트 임베딩-3-small 및 Jina 성능을 짧은 컨텍스트 벤치마크와 긴 컨텍스트 벤치마크 모두에서 능가하는 완전히 감사 가능한 긴 컨텍스트 모델이다. X축 단위는 벤치마크 제품군마다 다릅니다.**\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '최첨단 텍스트 임베딩 모델은 미리 훈련된 변압기를 초기화한 다음 대비 손실 대물렌즈로 미세 조정함으로써 훈련된다. 전통적으로, 미세 조정은 대조 신호에 대한 페어링된 트레이닝 데이터를 생성하기 위해 MSMarco 및 SNLI Bowman et al.(2015)과 같은 라벨링된 데이터 세트를 레버리지하는 것을 수반하였다. 예로는 SBERT Reimers and Gurevych (2019), SimCSE Gao et al. (2022), SGPT Muennighoff (2022) 등이 있다. E5 Wang et al. (2022), GTE Li et al. (2023), BGE Xiao et al. (2023), InstructOR Su et al. (2023), 및 Jina Gunther et al. (2023, 2024)와 같은 최근의 시스템들은 미리 훈련된 트랜스포머가 먼저 약하게 쌍을 이루는 데이터의 큰 코퍼스(예를 들어, Quora, Reddit Comments)를 사용하여 대조적으로 미세 조정되고 이어서 MSMarco와 같은 작고 더 높은 품질의 라벨링된 데이터세트 상에서 추가로 미세 조정되는 다단계 체제를 이용한다. 2단계 패러다임은 약하게 쌍을 이루는 데이터를 훨씬 더 많은 양으로 사용할 수 있기 때문에 모델 품질을 크게 향상시킨다.\n' +
      '\n' +
      '텍스트 임베딩 모델을 평가하는 것은 어렵다. BEIR 벤치마크 Thakur et al.(2021)은 15개의 제로 샷 검색 데이터 세트에 대해 밀집 검색기를 평가한다. SBERT Reimers 및 Gurevych(2019)와 같은 초기 트랜스포머 기반 텍스트 임베딩 모델은 의미론적 텍스트 유사성(STS) 데이터 세트에 대해서만 평가되었다. 보다 최근에, MTEB Muennighoff et al.(2023)은 많은 태스크들에 걸쳐 임베딩 모델들을 정량적으로 평가하기 위한 사실상의 벤치마크가 되었지만, 긴 컨텍스트 길이들(>512 토큰들)에 걸쳐 제한된 평가들을 갖는다. 지나 건터 등(2024)은 긴 상황 평가에 특화된 4가지 데이터셋의 벤치마크를 개발하였다. 추가적으로, LoCo Saad-Falcon et al.(2024) 벤치마크는 긴 컨텍스트 검색 모델의 성능을 평가하기 위해 최근에 출시되었다.\n' +
      '\n' +
      'AI 애플리케이션이 성숙함에 따라 모델 및 훈련 데이터의 감사 가능성과 준수는 영향을 많이 받는 도메인에서 안전한 모델 배포의 중요한 구성 요소가 될 것이다. 예를 들어, 잠자는 에이전트 Hubinger 등에 대한 Anthropic의 최근 작업(2024)은 엔드 투 엔드 감사 가능성 없이 모델을 배포할 위험을 입증한다. 현재 가장 성능이 좋은 텍스트 임베딩 모델에는 감사 가능한 학습 스택(즉, 사용 가능한 가중치, 데이터 및 코드를 가진 완전히 재현 가능한 학습 파이프라인)이 없습니다.\n' +
      '\n' +
      '## 3 훈련 데이터\n' +
      '\n' +
      '이 섹션에서는 각 훈련 단계에 걸친 데이터 믹스에 대해 설명한다. Nomic-ai/contrastors 코드 저장소를 방문하여 nomic-embed-text-v1의 학습 데이터에 액세스할 수 있습니다. [https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample)에서 대비 훈련 쌍의 5M 샘플을 탐색할 수 있다.\n' +
      '\n' +
      '### 마스킹 언어 모델링 사전 훈련\n' +
      '\n' +
      'Devlin et al. (2019)에 이어, 우리는 Long-context BERT 모델, 이하 nomic-bert-2048을 훈련하기 위해 2023년부터 BooksCorpus Zhu et al. (2015) 및 Wikipedia 덤프를 사용한다. BooksCorpus 및 Wikipedia로부터의 각 문서는 Devlin et al. (2019)로부터의 bert-base-uncased tokenizer를 사용하여 토큰화되고 2048 토큰의 청크로 패킹된다. 문서가 2048 토큰보다 짧으면 2048 토큰에 적합할 때까지 다른 문서를 추가합니다. 만약\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Model & Params & Seq & MTEB & LoCo & Jina LC & Weights & Code & Data \\\\ \\hline nomic-embed-text-v1 & 137M & 8192 & **62.39** & 85.53 & 54.16 & **Yes** & **Yes** & **Yes** \\\\ nomic-embed-text-v1-ablated & 137M & 8192 & 61.36 & **86.89** & 53.53 & **Yes** & **Yes** & **Yes** \\\\ jina-embeddings-base-v2-en & 137M & 8192 & 60.39 & 85.45 & 51.90 & **Yes** & No & No \\\\ text-embedding-ada-002 & N/A & 8192 & 60.99 & 52.70 & 55.25 & No & No & No \\\\ text-embedding-3-small & N/A & 8192 & 62.26 & 82.4 & **58.21** & No & No & No \\\\ \\hline E5-Mistral-7b-instruct & 7B & 4096 & 66.6 & 87.8 & N/A & Yes & No & No \\\\ text-embedding-3-large & N/A & 8192 & 64.59 & 79.4 & 58.69 & No & No & No \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: OpenAI 모델 및 기타 상위 긴 컨텍스트 오픈 소스 모델에 대해 nomic-embed-text-v1을 벤치마킹한다. Nomic-embed-text-v1은 OpenAI text-embedding-ada와 text-embedding-3-small를 능가하는 유일한 100M 파라미터 클래스 오픈 소스 모델이다. Nomic-embed-text-v1-ablated는 HotpotQA 및 FEVER 데이터를 생략한 섹션 5.4에 설명된 훈련 설정을 나타낸다. \'Seq\'는 모형의 맥락 길이를 의미하며, Jina LC는 Jina Long Context 벤치마크에서 과제들에 대한 평균이다.\n' +
      '\n' +
      '문서가 2048 토큰보다 커서 여러 문서에서 분할합니다.\n' +
      '\n' +
      '무감독 대비 사전 훈련\n' +
      '\n' +
      'Wang et al. (2022); Li et al. (2023); Xiao et al. (2023); Ni et al. (2022)와 유사하게, 우리는 쌍들을 형성하기 위해 공개적으로 이용 가능한 데이터의 대규모 컬렉션을 사용한다. 이러한 데이터 세트는 웹 검색에서 과학 논문의 클러스터링에 이르기까지 다양한 목적과 도메인에 걸쳐 있다. 총 29개의 데이터 세트1에 걸쳐 4억 7천만 쌍을 큐레이션했다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/datasets/sentence-transformers/embedding-training-data](https://huggingface.co/datasets/sentence-transformers/embedding-training-data)\n' +
      '\n' +
      '그러나, 이러한 데이터 세트들은 잡음이 많은 예제들을 포함할 수 있기 때문에, 우리는 일관성 필터링 Gunther et al.(2023); Wang et al.(2022)을 사용한다.\n' +
      '\n' +
      '전체 MiniLM-L6-v2 모델2를 사용하는 대신 gte-base 모델3을 사용한다. 각 쌍에 대해 (\\(query\\), \\(document\\)), 데이터 세트의 100만 점 하위 샘플의 쿼리와 문서를 모두 삽입한다. 각 질의에 대해 코사인 유사도를 이용하여 Top-k(이 경우 2) 이웃을 찾는다. 만약 \\(문서\\)이 상위 k개의 이웃에 없다면, 우리는 그 예를 버린다. 필터링 후, 우리는 \\(\\sim\\)235M 쌍으로 끝난다. 전체 데이터셋 분포는 <표 5>에서 확인할 수 있다.\n' +
      '\n' +
      '각주 2: All-MiniLM-L6-v2 모델[https://huggingface.co/thenlpar/gte-base/](https://huggingface.co/thenlpar/gte-base/)\n' +
      '\n' +
      '각주 3: gte-base 모델[https://huggingface.co/thenlpar/gte-base/](https://huggingface.co/thenlpar/gte-base/]\n' +
      '\n' +
      '이러한 데이터 세트의 대부분은 2048개의 토큰보다 짧은 시퀀스로 구성되기 때문에 우리는 장거리 의존성을 학습할 수 있도록 긴 컨텍스트 데이터 세트를 추가로 큐레이션한다. 즉, 우리는 S2ORC Lo et al.(2020)의 단일 논문의 초록 및 전체 논문 본문뿐만 아니라 그들의 제목과 쌍을 이루는 전체 위키피디아 논문을 사용한다.\n' +
      '\n' +
      '훈련하는 동안 우리는 한 번에 하나의 데이터 소스에서 쌍을 샘플링하고 모델이 소스별 단축키를 학습하는 것을 방지하기 위해 전체 배치를 해당 단일 소스의 샘플로 채운다.\n' +
      '\n' +
      '감독된 대조적 미세조정\n' +
      '\n' +
      'MSMarco Bajaj et al. (2018); Wang et al. (2023), NQ Karpukhin et al. (2020); Gao and Callan (2021), NLI Gao et al. (2022), HotpotQA Yang et al. (2018), FEVER Thorne et al. (2018), MEDI Su et al. (2023), WikiAnswers Fader et al. (2014) 및 Reddit4의 부분들에 대해, 우리는 BEIR 벤치마크 Thakur et al. (2021)로부터 방출된 트레이닝 세트들에 대해 트레이닝한다. 검색 데이터 세트(MSMarco, NQ, HotpotQA, Fever)에 대해, 우리는 gte-baseLi 등을 사용하여 이미 채굴되지 않은 경우 네거티브를 채굴한다(2023). 모든 \\((q,d)\\) 쌍에 대해, 우리는 상위 k개의 유사한 문서들을 하드 네거티브로 얻는다. 다른 모든 데이터 세트에 대해 마이닝 네거티브가 성능을 향상시키지 않는다는 것을 발견했기 때문에 하드 네거티브 대신 네거티브를 무작위로 샘플링한다.\n' +
      '\n' +
      '각주 4: [https://github.com/PolyAI-LDN/대화-데이터셋/트리/마스터/레드딧](https://github.com/PolyAI-LDN/대화-데이터셋/트리/마스터/레드딧)\n' +
      '\n' +
      '감독되지 않은 대조 단계와 유사하게 데이터 세트를 샘플링하고 선택한 데이터 세트의 모든 점을 배치로 채운다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '기존 텍스트 인코더의 주요 단점 중 하나는 512개의 토큰에서 주로 캡핑되는 제한된 시퀀스 길이이다. 긴 시퀀스 길이 모델을 훈련하기 위해 먼저 BERT를 적용하여 긴 시퀀스 길이를 수용할 수 있도록 한다. 이 작업에서 우리는 8192개의 서열 길이를 목표로 한다. 이를 위해 BERT base Devlin et al.(2019)에 다음과 같은 아키텍처 변경 및 최적화를 적용한다:\n' +
      '\n' +
      '* 회전식 위치 임베딩 Su 등(2023)을 위한 절대 위치 임베딩 대체\n' +
      '* GeLU Shazeer 대신 SwiGLU 활성화를 이용한 것(2020)\n' +
      '* Flash Attention Dao 등을 이용(2022)\n' +
      '* Dropout to 0 Geiping and Goldstein (2022)\n' +
      '* 64 Portes et al.(2023) Shoeybi et al.(2020)의 배수로 보캡 크기\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 137M 파라미터 부호화기.\n' +
      '\n' +
      '최대 시퀀스 길이가 2048인 모든 스테이지를 훈련하고 8192 시퀀스 길이 Peng et al.(2023); emozilla(2023)로 확장하기 위해 추론 시 Dynamic NTK 보간법을 사용한다. 또한, 플래시 어텐션 리포지토리5를 사용하여 SwiGLU에 대해 실행 시간이 대략 25% 더 빠르기 때문에 Portes et al.(2023)에서 제안된 바와 같은 SwiGLU 대 GeGLU를 선택한다.\n' +
      '\n' +
      '각주 5: [https://github.com/Dao-AILab/flash-attention/tree/main](https://github.com/Dao-AILab/flash-attention/tree/main)\n' +
      '\n' +
      '# 마스킹 언어 모델링\n' +
      '\n' +
      '트레이닝 동안, 우리는 다음의 15%가 아닌 30% 마스킹 레이트를 사용하고(Portes et al., 2023), 다음 문장 예측 태스크를 제거한다(Liu et al., 2019). 학습률 5e-4, 학습률 0.9 \\(\\beta_{1}\\) = 0.98인 AdamW 최적화기(Loshchilov and Hutter, 2019)를 사용하였으며, 전체 학습 단계의 6%의 선형 웜업과 선형 붕괴를 0으로 하였으며, 전체 배치 크기 4096을 사용하여 8개의 배치에서 기울기가 누적되었다. 우리는 더 큰 배치를 메모리에 맞추기 위해 딥스피드(Rajbhandari et al., 2020) 스테이지 2를 활용한다. 또한 매트릭스 곱셈에는 bfloat16 정밀도를 사용하고 구배 축적 dtype에는 fp32를 사용한다. 우리는 기울기 클리핑(Liu et al., 2019)을 사용하지 않고 가중치 감소를 1e-5로 설정하였으며, 학습률 1e-3으로 훈련을 시도하였으나 훈련 중 불안정성을 발견하였다. 우리는 최종 모델인 nomic-bert-2048을 부르고 무게를 방출합니다.\n' +
      '\n' +
      '무감독 대비 사전 훈련\n' +
      '\n' +
      '감독되지 않은 대조 사전 훈련은 가장 유사한 문서를 다른 관련 없는 문서와 구별하기 위해 모델을 가르치는 것을 목표로 한다. 이를 위해 InfoNCE 대비 손실(van den Oord et al., 2019)을 사용한다. 주어진 배치 \\(B=(q_{0},d_{0}),(q_{1},d_{1}),...,(q_{n},d_{n})\\)에 대해 손실 함수를 최소화한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{C}=-\\frac{1}{n}\\sum_{i}\\log\\frac{e^{s(q_{i},d_{i})/\\tau}}{e^{s(q_{i},d_{i})/\\tau}+\\sum_{j\\neqi}^{n}e^{s(q_{i},d_{j})/\\tau}}\\e^{s}\n' +
      '\n' +
      '여기서 \\(s(q,d)\\)는 \\((q,d)\\)의 코사인 유사도)\n' +
      '\n' +
      'Nomic-bert-2048의 가중치로 비지도 대조 훈련을 위한 모델을 초기화하고, 16,384의 배치 크기를 사용하여 각 배치에는 많은 수의 배치 내 음수가 있다. 인코더 아키텍처 및 훈련 전략을 위한 우리의 최적화는 이러한 배치 크기를 달성하는 것을 중심으로 한다. 학습률 2e-5, \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\), 무게감퇴 0.01의 AdamW를 사용하였으며, 기울기 클리핑은 1.0으로 설정하였으며, 선형 웜업 스케줄 700 스텝과 역제곱근 감쇠 스케줄을 사용하였다. 우리는 데이터에 대해 1개의 완전한 에폭에 대해 2048의 최대 시퀀스 길이로 훈련한다.\n' +
      '\n' +
      'GPU 메모리 제약으로 인해 전체 모델, 최적화기, 상태 및 데이터를 메모리에 맞출 수 없었다. 해결책으로 GradCache(Luyu Gao and Callan, 2021)와 혼합 정밀 훈련(Micikevicius et al., 2018)을 채용한다.\n' +
      '\n' +
      '마지막으로, 태스크 특정 프리픽스를 사용하여 바이인코더의 대칭성을 깨뜨린다(Wang et al., 2022). 접두사 없이, 모델은 상충되는 보상 신호를 수신한다. "프랑스의 수도는 무엇인가?"라는 질문에 가장 가까운 응답을 결정하는 경우를 생각해 보자.\n' +
      '\n' +
      '1. "프랑스 수도의 이름은 무엇입니까?"\n' +
      '2. "파리는 프랑스의 수도입니다."\n' +
      '\n' +
      '의미적 유사성 작업은 첫 번째를 가장 가까운 것으로 간주하고 질문 응답 작업은 두 번째를 가장 가까운 것으로 간주한다. 접두사는 모델이 이러한 작업 각각에 의해 지정된 동작을 구별할 수 있도록 합니다.\n' +
      '\n' +
      '다음 작업별 접두사를 사용합니다.\n' +
      '\n' +
      '* search_query\n' +
      '* search_document\n' +
      '* classification\n' +
      '* clustering\n' +
      '\n' +
      'Reimers et al.(2023)에 의해 영감을 받았다. 우리는 먼저 접두사를 두 가지 범주로 나눈다: 질의와 문서가 유사한 구조를 갖는 대칭, 질의가 보통 단일 문장이고 문서가 많은 문장일 수 있는 비대칭.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l l l l} \\hline \\hline Model & Bsz & Steps & Seq & Cola & SST2 & MRPCSTSB & QQP & MNLI & QNLI & RTE & Avg \\\\ \\hline nomic-bert-2048 & 4k & 100k & 2k & 0.50 & 0.93 & 0.88 & 0.90 & 0.92 & 0.86 & 0.92 & 0.82 & 0.84 \\\\ MosaicBERT & 4k & 70k & 2k & 0.54 & 0.93 & 0.87 & 0.90 & 0.92 & 0.86 & 0.92 & 0.82 & 0.85 \\\\ RobertaBase & 8k & 500k & 512 & 0.64 & 0.95 & 0.90 & 0.91 & 0.92 & 0.88 & 0.93 & 0.79 & 0.86 \\\\ JinaBERTBase & 4k & 100k & 512 & 0.51 & 0.95 & 0.88 & 0.90 & 0.81 & 0.86 & 0.92 & 0.79 & 0.83 \\\\ MosaicBERT & 4k & 178k & 128 & 0.59 & 0.94 & 0.89 & 0.90 & 0.92 & 0.86 & 0.91 & 0.83 & 0.85 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: GLUE Dev 세트 결과. 로베르타 번호는 표 8에서 취하였다(Liu et al., 2019). Nomic-bert-2048과 동일한 방식으로 평가한 2048 모델을 제외하고 Portes et al. (2023)의 표 S1로부터 취해진 모자이크버트 번호. JinaBertBase Glue Test 번호 (Günther et al., 2024)로부터 표 2에 보고된 JinaBertBase Glue Test 번호.\n' +
      '\n' +
      '(Su et al., 2023a) 처음 두 접두사는 검색 작업에 사용된다: 여기서 search_query는 일반적으로 질문에 대한 것이고 search_document는 응답에 대한 것이다. 분류는 재구분과 같은 STS 관련 작업에 사용됩니다. 클러스터링은 Arxiv 제목-추상 쌍과 같이 의미적으로 유사한 텍스트를 가깝게 그룹화하는 것을 목표로 하는 작업에 사용된다. 대칭 작업의 경우 쿼리와 문서 모두에 동일한 접두사가 추가됩니다.\n' +
      '\n' +
      '감독된 대조적 미세조정\n' +
      '\n' +
      '마지막 단계의 훈련은 인간 라벨 데이터 세트를 활용하여 성능을 높이는 것을 목표로 한다. (Ni et al., 2021a,b; Wang et al., 2022; Li et al., 2023)을 포함한 여러 논문에서 이러한 데이터 세트에 대한 미세 조정은 다운스트림 성능의 향상으로 이어진다는 것을 보여주었다.\n' +
      '\n' +
      '우리는 각 배치에 하드 네거티브를 포함하도록 쌍을 이루는 대조 손실을 조정한다. 2e-5, \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.999\\)의 학습률과 0.01의 무게감소를 이용하여 1.0의 기울기 클리핑을 설정하고, 400단계의 선형 워밍업 스케줄과 0의 선형 쿨다운을 이용하여 프리픽스로 트레이닝한다. 우리는 음수의 수를 7 이상으로 증가시키면 의미 있게 성능이 향상되지 않는다는 것을 발견했다. 우리는 또한 여러 시대에 대한 훈련이 성과를 손상시킨다는 것을 발견했다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '우리는 GLUE 벤치마크(Wang et al., 2019)에서 nomic-bert-2048을 평가하고 유사한 크기와 훈련된 모델과 경쟁적이라는 것을 발견했다. 우리는 MTEB(Muennighoff et al., 2023), Jina의 Long Context Benchmark(Gunther et al., 2024), LoCo(Saad-Falcon et al., 2024)에 대한 nomic-embed-text-v1을 평가한다. nomic-embed-text-v1이 text-embedding-ada-002 및 jina-embedding-v2-base-en을 초과한다. Long context 벤치마크인 LoCo와 Jina Long Context Benchmark에서 nomic-embeded-text-v1이 jina-embeddings-v2-base-en을 균일하게 능가한다. nomic-embed-text-v1은 LoCo와 Jina의 Long Context Benchmark에서 4개의 데이터 세트 중 2개에서 text-embedding-ada-002보다 우수하다.\n' +
      '\n' +
      '### nomic-bert-2048 GLUE 결과\n' +
      '\n' +
      '우리는 GLUE 벤치마크(Wang et al., 2019)에서 제시된 방법론(Liu et al., 2019)에 따라 nomic-bert-2048을 평가한다. GLUE 벤치마크는 9개의 태스크로 구성되지만, 우리는 (Liu et al., 2019)와 유사하게 8에 대해 평가한다.\n' +
      '\n' +
      '각 작업에 대해 배치 크기 16, 32 및 학습 속도 1e-5, 2e-5, 3e-5를 갖는 10개의 에폭에 대해 훈련하고 5개의 종자에 걸쳐 6%의 선형 예열한다. 10개의 에포크가 끝날 때 작업당 중앙값 점수는 표 2에 제시되어 있다. 유의할 점은 MRPC 및 QQP에 대한 정확도 및 STSB 6에 대한 피어슨에 대해 보고한다. 우리는 표 2에 우리의 결과를 보고한다. (Liu et al., 2019)와 유사하게, RTE, STSB 및 MRPC에 대한 MNLI 체크포인트로부터 초기화한다.\n' +
      '\n' +
      '각주 6: [https://github.com/facebookresearch/fairseq/issues/1561#issuecontent-57129519](https://github.com/facebookresearch/fairseq/issues/1561#issuecontent-57129519)\n' +
      '\n' +
      'MosaicBERT(Portes et al., 2023)는 약간 더 나은 성능을 보이지만, 약간 더 길고 C4 상에서 트레이닝된다(Raffel et al., 2019). 모든 작업에서 노믹-버트-2048은 콜라를 제외하고 모자이크BERT와 유사하게 점수를 매긴다. 그러나 우리는 더 긴 시퀀스 길이 모델을 사용했으며 실제로 사전 훈련 동안 더 많은 토큰을 보았다. JinaBERT는 또한 테스트 점수 대 데브 점수를 보고하고 모자이크BERT와 유사하게 훈련되지만 유사하게 점수를 매긴다.\n' +
      '\n' +
      '### MTEB Results\n' +
      '\n' +
      'MTEB(Muennighoff et al., 2023)는 56개의 데이터 세트에 걸쳐 8개의 작업에 대한 다양한 커버리지로 인해 임베딩 모델을 평가하기 위한 표준 벤치마크가 되었다. MTEB는 분류, 클러스터링, 쌍 분류, 재랭킹, 검색, 의미론적 텍스트 유사성 및 요약에 걸쳐 임베딩 모델을 평가했다. MTEB 점수는 작업당 점수의 가중 평균이다.\n' +
      '\n' +
      '### 긴 상황 결과\n' +
      '\n' +
      '그러나, (Gunther et al., 2024)에서 언급된 바와 같이, MTEB는 긴 시퀀스를 포함하는 데이터세트가 매우 적다. 더 긴 시퀀스에 대한 nomic-embed-text-v1의 성능을 평가하기 위해 두 가지 추가 벤치마크(Gunther et al., 2024) Long Context Dataset과 LoCo 벤치마크(Saad-Falcon et al., 2024)를 고려한다.\n' +
      '\n' +
      '###### 5.3.1 JinaAI Long Context Benchmark\n' +
      '\n' +
      'Jina Long Context Benchmark (Gunther et al., 2024)는 Retrieval and Clustering에 걸쳐 4개의 데이터세트, 즉 NarrativeQA (Gunther et al., 2024), WikiCites 7, SciFact (Wadden et al., 2020), 및 BigPatent 8Sharma et al. (2019). 결과는 표 4에 제시되어 있다. Gunther et al.(2024)과 유사하게, 우리는 클러스터링 및 검색 데이터 세트에 대해 각각 V-점수 및 NDCG@10을 보고한다. 서열 길이와 작업 전반에 걸쳐 8k 컨텍스트 길이의 모든 데이터 세트에 대해 nomic-embeded-text-v1이 jina-embeddings-v2-base를 비트하거나 연결한다. 또한, Nomic-embed-text-v1은 4개의 데이터 세트 중 2개에서 text-embedding-ada-002를 능가한다. 또한 WikiCitiesClustering에 대한 Gunther et al.(2024)과 유사한 결과를 보고하며, 이는 시퀀스 길이가 성능에 영향을 미치므로 테스트에서 더 긴 시퀀스 길이가 잘 수행되기 위해 필요하지 않음을 시사한다.\n' +
      '\n' +
      '각주 8: [https://huggingface.co/datasets/jinaai/big-patent-clustering](https://huggingface.co/datasets/jinaai/big-patent-clustering)\n' +
      '\n' +
      '###### 5.3.2 LoCo 벤치마크\n' +
      '\n' +
      'LoCo 벤치마크는 Shaham et al.(2022)로부터 3개, Dasigi et al.(2021)로부터 2개의 5개의 검색 데이터셋으로 구성된다. 벤치마크는 회의 녹취록, 국가 정책 보고서, TV 에피소드 녹취록 및 과학 연구 논문에서 검색을 테스트한다. 우리는 완전성을 위해 QASPER 추상 기사 데이터 세트를 포함하지만 많은 모델이 벤치마크를 과포화하고 1.0 NDCG@10에 접근하는 것으로 보인다는 점을 강조하고 싶다. 결과는 표 6에 나와 있다. nomic-embeded-text-v1은 시퀀스 길이에 걸쳐 jina-embeddings-v2-base-en을 능가한다. nomic-embed-text-v1은 2048에서 M2-Bert를 능가하고 8192에서 경쟁적이다. 시퀀스 길이 4096에서 nomic-embed-text-v1은 E5 Mistral과 경쟁적이지만 상당히 작다.\n' +
      '\n' +
      '### BEIR의 Few-Shot 평가\n' +
      '\n' +
      'MTEB의 BEIR 컴포넌트는 원래 제로-샷 벤치마크로서 의도되었지만, BGE Xiao et al.(2023), GTE Li et al.(2023), E5-Mistral Wang et al.(2023)을 포함한 몇몇 상위 오픈 소스 모델들은 FEVER 및 HotpotQA와 같은 BEIR 벤치마크 데이터 세트들의 트레인 스플릿들에 대한 트레이닝을 보고한다. 이것이 우리에 미치는 영향을 이해하기 위해\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline Category \\(\\rightarrow\\) & Cls. & Clust. & PairCls. & Rerank & Retr. & STS & Summ. & Avg \\\\ Number of datasets \\(\\rightarrow\\) & 12 & 11 & 3 & 4 & 15 & 10 & 1 & 56 \\\\ \\hline \\multicolumn{8}{l}{_Unsupervised Models_} \\\\ \\hline Glove Pennington et al. (2014) & 57.3 & 27.7 & 70.9 & 43.3 & 21.6 & 61.9 & 28.9 & 42.0 \\\\ SimCSE Gao et al. (2022) & 62.5 & 29.0 & 70.3 & 46.5 & 20.3 & 74.3 & 31.2 & 45.5 \\\\ nomic-embed-text-v1\\({}_{\\text{unsup}}\\) & 71.2 & 42.5 & 83.7 & 55.0 & 48.0 & 80.8 & 30.7 & 59.9 \\\\ \\hline \\multicolumn{8}{l}{_Supervised Models_} \\\\ \\hline SimCSE\\({}_{\\text{bert-sup}}\\)Gao et al. (2022) & 67.3 & 33.4 & 73.7 & 47.5 & 21.8 & 79.1 & 23.3 & 48.7 \\\\ Contriever Izacard et al. (2022) & 66.7 & 41.1 & 82.5 & 53.1 & 41.9 & 76.5 & 30.4 & 56.0 \\\\ GTR\\({}_{\\text{xxl}}\\)Ni et al. (2021) & 67.4 & 42.4 & 86.1 & 56.7 & 48.5 & 78.4 & 30.6 & 59.0 \\\\ Sentence-T5\\({}_{\\text{xxl}}\\)Ni et al. (2021) & 73.4 & 43.7 & 85.1 & 56.4 & 42.2 & 82.6 & 30.1 & 59.5 \\\\ E5\\({}_{\\text{large-v2}}\\)Wang et al. (2022) & 75.2 & 44.5 & 86.0 & 56.6 & 50.6 & 82.1 & 30.2 & 62.3 \\\\ E5\\({}_{\\text{mistral}}\\)Wang et al. (2023) & 78.5 & 50.3 & 88.3 & 60.2 & 56.9 & 84.6 & 31.4 & 66.6 \\\\ GTE\\({}_{\\text{base}}\\)Li et al. (2023) & 73.0 & 46.2 & 84.6 & 58.6 & 51.1 & 82.3 & 31.2 & 62.4 \\\\ GTE\\({}_{\\text{large}}\\)Li et al. (2023) & 73.3 & 46.8 & 85.0 & 59.1 & 52.2 & 83.4 & 31.7 & 63.1 \\\\ BGE\\({}_{\\text{base}}\\)Xiao et al. (2023) & 75.5 & 45.8 & 86.6 & 58.9 & 53.3 & 82.4 & 31.1 & 63.6 \\\\ BGE\\({}_{\\text{large}}\\)Xiao et al. (2023) & 76.0 & 46.1 & 87.1 & 60.0 & 54.3 & 83.1 & 31.6 & 64.2 \\\\ Jina\\({}_{\\text{v2}}\\)Gunther et al. (2024) & 73.5 & 41.7 & 85.4 & 57.0 & 47.9 & 80.7 & 31.6 & 60.4 \\\\ text-embedding-ada-002 & 70.9 & 45.9 & 84.9 & 56.3 & 49.3 & 81.0 & 30.8 & 61.0 \\\\ text-embedding-3-small & 73.2 & 46.7 & 85.0 & 56.7 & 51.1 & 81.6 & 31.1 & 62.3 \\\\ text-embedding-3-large & 75.5 & 49.0 & 85.7 & 59.2 & 55.4 & 81.7 & 29.9 & 64.6 \\\\ nomic-embed-text-v1-ablated & 73.6 & 43.7 & 84.6 & 53.3 & 51.4 & 80.2 & 31.3 & 61.4 \\\\ nomic-embed-text-v1 & 74.1 & 43.9 & 85.2 & 55.7 & 52.8 & 82.1 & 30.1 & 62.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MTEB 벤치마크 Muennighoff 등에 대한 결과(2023). 숫자는 각 범주에 대해 평균화됩니다. 데이터 세트별 점수 및 최신 결과는 [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)를 참조하십시오.\n' +
      '\n' +
      '또한 다운스트림 점수에서 FEVER, HotpotQA 및 MEDI 데이터 세트를 생략하는 nomic-embed-text-v1-ablated 모델을 훈련한다. 표 1에 보고된 바와 같이, 이것은 우리의 전체 MTEB 점수를 약 1점 감소시킨다. 최고 오픈 소스 모델과 사과 대 사과 비교를 유지하기 위해 공개된 버전의 Nomic-embed-text-v1에 대한 FEVER, HotpotQA 및 MEDI 데이터 세트를 훈련하기로 선택했지만, 안타깝게도 폐쇄 소스 모델의 특성으로 인해 이러한 데이터 세트에 대한 폐쇄 소스 모델의 훈련 여부에 대한 표시가 없다.\n' +
      '\n' +
      '## 6 훈련 자원\n' +
      '\n' +
      'nomic-embed-text-v1의 완전한 트레이닝은 하나의 8xH100 노드에서 단주에 수행될 수 있다. nomic-bert-2048의 마스킹 언어 모델링은 대략 4일이 소요된다. 대비적인 사전 훈련은 3일 반 동안 지속됩니다. 대비적인 미세 조정은 한 시간이 걸립니다. 우리는 독자가 Nomic-embed-text-v1과 동일한 라이선스로 출시된 Nomic-bert-2048 또는 감독되지 않은 대조 검문소에서 초기화할 것을 권장한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '정렬 및 긴 컨텍스트 벤치마크 모두에서 오픈AI Ada-002 성능을 능가하는 최초의 완전 오픈 소스 긴 컨텍스트 텍스트 임베딩 모델을 출시한다. 우리는 모델을 재현하기 위해 데이터를 포함한 레시피뿐만 아니라 허용 라이센스에 따라 모델 가중치 및 훈련 코드를 릴리스한다.\n' +
      '\n' +
      '### Contributions\n' +
      '\n' +
      '잭 누스바움은 최종 버전에 존재하는 구현, 훈련 및 데이터 결정의 대부분을 비롯하여 스택의 모든 수준에서 여러 설계 결정을 내리는 등 프로젝트를 주도한다. 잭 모리스는 데이터 세트 큐레이션 및 모델 아키텍처와 관련하여 몇 가지 설계 기여를 했다. 브랜든 더더스타트는 전체 스택에 걸쳐 몇 가지 설계 기여를 했으며 데이터 큐레이션 파이프라인의 기본 구현을 작성했다. Andriy Mulyar는 초기 프로젝트 방향을 설정하고 코드 구현을 검토했으며 몇 가지 모델 설계 및 데이터 세트 큐레이션 기여를 했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline\n' +
      '**Model** & Seq & NarrativeQA & WikiCtities & SciFact & BigPatent & Avg \\\\ \\hline nomic-embed-text-v1 & 128 & 20.1 & 90.0 & 65.4 & 18.5 & 48.5 \\\\ nomic-embed-text-v1-ablated & 128 & 20.8 & 86.8 & 65.2 & 17.5 & 47.6 \\\\ jina-embeddings-base-v2 & 128 & 19.6 & 79.9 & 62.1 & 14.4 & 44.0 \\\\ text-embedding-ada-002 & 128 & 25.4 & 84.9 & 68.8 & 16.6 & 48.9 \\\\ text-embedding-3-small & 128 & 29.5 & 87.5 & 68.8 & 15.0 & 50.2 \\\\ text-embedding-3-large & 128 & 45.6 & 87.9 & 74.8 & 16.5 & 56.2 \\\\ \\hline nomic-embed-text-v1 & 512 & 23.9 & 88.7 & 70.5 & 25.3 & 52.1 \\\\ nomic-embed-text-v1-ablated & 512 & 25.7 & 81.9 & 71.5 & 23.7 & 50.7 \\\\ jina-embeddings-base-v2 & 512 & 21.3 & 79.3 & 66.7 & 21.9 & 47.3 \\\\ text-embedding-ada-002 & 512 & 25.5 & 84.8 & 72.6 & 23.0 & 51.5 \\\\ text-embedding-3-small & 512 & 32.2 & 89.0 & 73.2 & 23.6 & 54.5 \\\\ text-embedding-3-large & 512 & 48.1 & 89.9 & 77.6 & 23.6 & 59.6 \\\\ \\hline nomic-embed-text-v1 & 8191 & 37.8 & 84.3 & 70.2 & 24.5 & 54.2 \\\\ nomic-embed-text-v1-ablated & 8191 & 44.0 & 77.4 & 69.1 & 23.6 & 53.5 \\\\ jina-embeddings-base-v2 & 8191 & 39.4 & 75.7 & 69.4 & 23.1 & 51.9 \\\\ text-embedding-ada-002 & 8191 & 41.1 & 84.7 & 72.7 & 22.5 & 55.3 \\\\ text-embedding-3-small & 8191 & 47.1 & 89.9 & 73.3 & 22.5 & 58.3 \\\\ text-embedding-3-large & 8191 & 51.6 & 86.2 & 77.7 & 19.3 & 58.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 지나 롱 컨텍스트 평가 벤치마크. Günther et al.(2024)로부터 취해진 text-embedding-ada-002 및 jina-embeddings-base-v2에 대한 번호.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bajaj et al. (2018) Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. Ms. marco: 인간 생성 기계 판독 이해 데이터세트.\n' +
      '* Bowman et al. (2015) Samuel R. 보우먼, 가버 앤젤리, 크리스토퍼 팟츠, 크리스토퍼 D. 매닝 2015. 자연어 추론 학습을 위한 대용량 주석이 달린 코퍼스. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)_. 컴퓨터 언어학과의 연관성\n' +
      '* Coster and Kauchak (2011) William Coster and David Kauchak. 2011. 단순 영어 위키피디아: 새로운 텍스트 단순화 작업. [Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 665-669, Portland, Oregon, USA. 컴퓨터 언어학과의 연관성\n' +
      '* Dao et al. (2022) Tri Dao, Daniel Y. 푸, 스테파노 에르몬, 아트리 루드라, 크리스토퍼 레 2022. 플래시 어텐션:io-awareness로 빠르고 메모리 효율적인 정확한 주의력.\n' +
      '* Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. 연구논문에 정박된 정보추구 질문과 답변의 데이터셋.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련.\n' +
      '* 이모질라(2023) 이모질라. 2023. 동적 스케일링된 로프는 미세 조정 없이 긴 컨텍스트 라마의 성능을 더욱 증가시킨다.\n' +
      '* Fader et al. (2014) Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open Question Answering Over Curated and extracted knowledge base. _KDD_에서.\n' +
      '* Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. EL15: long form question answering. _Proceedings of the 57th Conference of the Computational Linguistics, ACL 2019, Florence, Italy, July 28-8월 2일, Volume 1: Long Papers_, pages 3558-3567. Association for Computational Linguistics.\n' +
      '*Filippova and Altun(2013) Katja Filippova and Yasememin Altun. 2013. The lack of parallel data in sentence compression. _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1481-1491, 시애틀, USA. 컴퓨터 언어학과의 연관성\n' +
      '* 파운데이션(2021) 위키미디어 파운데이션. 위키미디어 다운로드\n' +
      '*가오 및 캘런(2021) 루유 가오 및 제이미 캘런. 2021. 응축기: 밀집 검색을 위한 사전 훈련 아키텍처.\n' +
      '* Gao et al. (2022) Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2022. Simcse: 문장 임베딩의 단순 대조 학습.\n' +
      '* Geiping and Goldstein (2022) Jonas Geiping and Tom Goldstein. 2022. 크래밍: 하루 만에 하나의 gpu에서 언어 모델을 훈련한다.\n' +
      '* Gupta et al. (2019) Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C Lipton. 2019. Amazonqa: 리뷰 기반 질문 응답 태스크.\n' +
      '* Gunther et al. (2023) Michael Gunther, Louis Milliken, Jonathan Geuter, Georgios Mastrapas, Bo Wang, and Han Xiao. 2023. Jina embeddings: 새로운 세트의 고성능 문장 임베딩 모델.\n' +
      '* Gunther et al. (2024) Michael Gunther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Jina embeddings 2: 8192-token 범용 텍스트 embeddings for long documents.\n' +
      '* Hamborg et al. (2017) Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news- please: generic news crawler and extractor. _Proceedings of the 15th International Symposium of Information Science_, pages 218-223.\n' +
      '* Hidey and McKeown (2016) Christopher Hidey and Kathy McKeown. 2016. Parallel Wikipedia article를 이용한 인과관계 파악. _Proceedings of the 54th Annual Meeting of the Computational Linguistics Association (Volume 1: Long Papers)_, pages 1424-1433, Berlin, Germany. 컴퓨터 언어학과의 연관성\n' +
      '* Hubinger et al. (2021) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lannam, Daniel M. 지글러, 팀 맥스웰, 뉴턴 쳉, 아담 저민, 아만다 아스켈, 안쉬 래드크리시난, 켐 아니일, 다비드 뒤베노, 딥 갠굴리, 파즐 바레즈, 잭 클라크, 카말 은두세, 크슈티치 사찬, 마이클 셀리토, 미스터랭크 샤르마, 노바 다스 사르마, 로저 그로세, 샤우나 크레이브, 윈타오 바이, 재커리 위튼, 마리나 파바로, 얀 브라우너, 홀든 카르노프스키, 폴 크리스티아누, 사무엘 R. 보우만, 로건 그레이엄, 재러드 카플란, 소렌 민더만, 라이언 그린블랫, 벅 슈레게리스, 니콜라스 쉬퍼, 이든 페레즈. 2024. 잠입요원: 안전 훈련을 통해 지속되는 기만적인 함성을 훈련한다.\n' +
      '* Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet challenge: 시맨틱 코드 검색의 상태 평가 _ arXiv preprint arXiv:1909.09436_.\n' +
      '* Izacard et al. (2022a) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. 비교적 학습으로 지도하지 않은 밀집 정보 검색\n' +
      '\n' +
      '고티에 이자카드, 패트릭 루이스, 마리아 로멜리, 루카스 호세이니, 파비오 페트로니, 티모 쉬크, 제인 드위브디-유, 아르만드 줄린, 세바스티안 리델, 에두아르 그레이브. 2022b. 아틀라스: 검색 증강 언어 모델을 사용한 소수의 샷 학습.\n' +
      '* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. 오픈 도메인 질의 응답을 위한 밀집 통로 검색.\n' +
      '* Khashabi et al. (2021) Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, and Chris Callison-Burch. 2021. Gooaq: 다양한 답변유형을 가진 개방형 질의응답.\n' +
      '* 쿠페에와 왕(2018) 마나즈 쿠페에와 윌리엄 양왕. 2018. Wikihow: 대규모 텍스트 요약 데이터세트.\n' +
      '* Lewis et al. (2021a) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. 2021a. 지식 집약적 nlp 작업에 대한 검색 지원 생성\n' +
      '* Lewis et al. (2021b) Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Kuttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021b. Paq: 6,500만개의 질문들이 아마도 그리고 당신이 그것들로 무엇을 할 수 있는지 물었다.\n' +
      '* Li 등(2023) Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. 일반적인 텍스트 임베딩에 대해 다단계 대비 학습을 수행한다.\n' +
      '* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 강건하게 최적화된 버트 사전 훈련 접근법.\n' +
      '* Lo et al. (2020) Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S. 웰드 2020. S2orc: The semantic scholar open research corpus.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization.\n' +
      '* Gao et al. (2021) Jiawei Han Luyu Gao, Yunyi Zhang and Jamie Callan. 2021. 메모리 제한 설정 하에서 딥 콘트라스트 학습 배치 크기를 스케일링하는 단계. In _Proceedings on Representation Learning for NLP_.\n' +
      '* Micikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. 혼합 정밀 훈련.\n' +
      '* Muennighoff (2022) Niklas Muennighoff. 2022. Sgpt: 의미 검색을 위한 Gpt 문장 임베딩.\n' +
      '* Muennighoff et al. (2023) Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. Mteb: 대량 텍스트 임베딩 벤치마크.\n' +
      '* Neelakantan et al. (2022) Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, Lilian Weng. 2022. 대조적 사전 훈련에 의한 텍스트 및 코드 임베딩.\n' +
      '* Ni et al.(2022) Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022. 문장-t5: 미리 훈련된 텍스트-텍스트 모델들로부터 스케일러블 문장 인코더들. _Findings of the Association for Computational Linguistics: ACL 2022_, pages 1864-1874, Dublin, Ireland. 컴퓨터 언어학과의 연관성\n' +
      '* Ni et al.(2019) Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. 멀리 표시된 리뷰와 세밀한 측면을 사용하여 권장 사항을 정당화한다. [Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP)_, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* Ni et al. (2021a) Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Y. 자오, 이루안, 키스 B 홀, 밍웨이 창, 인페이 양. 2021a. 대형 듀얼 인코더는 일반화할 수 있는 검색기입니다.\n' +
      '* Ni et al.(2021b) Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. 2021b. 문장-t5: 미리 훈련된 텍스트-텍스트 모델로부터 확장 가능한 문장 인코더.\n' +
      '* van den Oord et al. (2019) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. 대비 예측 코딩을 이용한 표현 학습.\n' +
      '* Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: 대용량 언어 모델의 효율적인 컨텍스트 윈도우 확장.\n' +
      '* Pennington et al.(2014) Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: 단어 표현을 위한 전역 벡터. _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1532-1543, Doha, Qatar. 컴퓨터 언어학과의 연관성\n' +
      '* Portes et al. (2023) Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. 2023. 모자이크버트: 빠른 프리 트레이닝에 최적화된 양방향 인코더.\n' +
      '* Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ arXiv e-prints_.\n' +
      '* Riedel et al. (2019)Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화.\n' +
      '* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+질문의 기계어 이해를 위한_ arXiv e-prints_, page arXiv:1606.05250.\n' +
      '* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shohham. 2023. 문맥 내 검색-증강 언어 모델.\n' +
      '* Reimers et al. (2023) Nils Reimers, Elliot Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, and Abdullah Elkady. 2023. 임베디드 v3를 도입한다.\n' +
      '* Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: siamese bert-networks를 이용한 Sentence embedding.\n' +
      '* Saad-Falcon et al. (2024) Jon Saad-Falcon, Dan Fu, and Simran Arora. 2024. 몬나크 믹서를 사용한 롱-컨텍스트 검색 모델.\n' +
      '* See et al.(2017) Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. 요점: 포인터-제너레이터 네트워크와의 요약. _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1073-1083, Vancouver, Canada. 컴퓨터 언어학과의 연관성\n' +
      '* Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Iygi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: 표준화된 CompaRison over long language sequences. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 12007-12021, 아랍에미리트 아부다비. 컴퓨터 언어학과의 연관성\n' +
      '* Sharma et al. (2019) Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-PATENT: 추상적이고 일관성 있는 요약에 대한 대규모 데이터셋. _ CoRR_, abs/1906.03741.\n' +
      '* Shazeer(2020) Noam Shazeer. 2020. Glu 변종은 변압기를 개선한다.\n' +
      '* Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-lm: 모델 병렬성을 이용하여 수십억 개의 파라미터 언어 모델을 학습시킨다.\n' +
      '* Su et al. (2023a) Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023a. 임베더 하나, 모든 작업: 명령 미세 조정 텍스트 임베딩입니다.\n' +
      '* Su et al. (2023b) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023b. 로포머: 회전식 위치 임베딩이 있는 향상된 변압기.\n' +
      '* Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: 정보 검색 모델의 제로 샷 평가를 위한 이질적인 벤치마크.\n' +
      '* Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: 사실 추출 및 VERification을 위한 대규모 데이터세트. _NAACL-HLT_에서.\n' +
      '* 항해(2023) 항해. 2023년, 항해 임베딩을 발표합니다!\n' +
      '* Wadden et al. (2020) David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 7534-7550, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Wang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. 보우먼 2019. GLUE: 자연어 이해를 위한 멀티태스크 벤치마크 및 분석 플랫폼. ICLR의 Proceedings에서.\n' +
      '* Wang et al. (2022) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. 약하게 감독된 대조적 사전 훈련에 의한 텍스트 임베딩.\n' +
      '* Wang et al. (2023a) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2023a. Simlm: 조밀한 통로 검색을 위한 표현 병목 현상을 가진 사전 훈련.\n' +
      '* Wang et al. (2023b) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023b. 대용량 언어 모델로 텍스트 임베딩을 개선합니다.\n' +
      '* Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: 일반적인 중국어 임베딩을 진전시키기 위한 패키징된 리소스.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. 코헨, 루슬란 살라쿠티노프 크리스토퍼 매닝 2018. HotpotQA: 다양하고 설명 가능한 멀티홉 질문 응답을 위한 데이터셋. _Conference on Empirical Methods in Natural Language Processing (EMNLP)_.\n' +
      '* Zhang et al.(2016) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2016. 문자 분류를 위한 문자 레벨 컨볼루션 네트워크.\n' +
      '* Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. 책과 영화를 정렬하기: 영화를 보고 책을 읽음으로써 이야기 같은 시각적 설명을 향해.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Dataset & Datapoints & \\(\\%\\) Dataset \\\\ \\hline Reddit\\({}^{a}\\) & 64,978,944 & 0.28 \\\\ PAQ (Lewis et al., 2021) & 52,953,088 & 0.23 \\\\ Amazon Reviews (Ni et al., 2019) & 38,682,624 & 0.16 \\\\ S2ORC Title Abstract (Lo et al., 2020) & 35438592 & 0.15 \\\\ WikiAnswers (Fader et al., 2014) & 9,912,320 & 0.04 \\\\ S2ORC Citation Titles (Lo et al., 2020) & 7,585,792 & 0.03 \\\\ S2ORC Abstract Citation (Lo et al., 2020) & 7,503,872 & 0.03 \\\\ S2ORC Abstract Body (Lo et al., 2020) & 6,389,760 & 0.03 \\\\ Wikipedia Title Body (Foundation) & 6,078,464 & 0.03 \\\\ Gooaq (Khashabi et al., 2021) & 1,245,184 & 0.01 \\\\ Codesearch (Husain et al., 2019) & 835,584 & \\(<\\).01 \\\\ AGNews (Zhang et al., 2016) & 409,600 & \\(<\\).01 \\\\ CCNews (Hamborg et al., 2017) & 344,064 & \\(<\\).01 \\\\ NPR \\({}^{b}\\) & 344,064 & \\(<\\).01 \\\\ CNN (See et al., 2017) & 278,528 & \\(<\\).01 \\\\ Yahoo Title-Answer \\({}^{c}\\) & 262,144 & \\(<\\).01 \\\\ AmazonQA (Gupta et al., 2019) & 212,992 & \\(<\\).01 \\\\ Yahoo Title-Question \\({}^{d}\\) & 196,608 & \\(<\\).01 \\\\ Sentence Compression (Filippova and Altun, 2013) & 163,840 & \\(<\\).01 \\\\ YahooQA \\({}^{e}\\) & 131,072 & \\(<\\).01 \\\\ ELI5 (Fan et al., 2019) & 98,304 & \\(<\\).01 \\\\ Altlex (Hidey and McKeown, 2016) & 98,304 & \\(<\\).01 \\\\ Wikihow (Koupaee and Wang, 2018) & 81,920 & \\(<\\).01 \\\\ SimpleWiki (Coster and Kauchak, 2011) & 81,920 & \\(<\\).01 \\\\ StackExchange Duplicate Questions\\({}^{f}\\) & 65,536 & \\(<\\).01 \\\\ StackExchange Title Body \\({}^{g}\\) & 65,536 & \\(<\\).01 \\\\ StackExchange Body Body \\({}^{h}\\) & 65,536 & \\(<\\).01 \\\\ Quora Duplicate Questions \\({}^{i}\\) & 32,768 & \\(<\\).01 \\\\ SQuAD (Rajpurkar et al., 2016) & 16,384 & \\(<\\).01 \\\\ \\hline Total & 234,553,344 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 사전 학습 데이터세트 분포\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Model & Seq & Param. & Tau & Tau & \\multicolumn{1}{c}{Tau} & \\multicolumn{1}{c}{QASP.} & \\multicolumn{1}{c}{QASP.} & \\multicolumn{1}{c}{Avg} \\\\  & & & Scr. & Gov. & QMS. & Tit. & Abs. & \\\\  & & & & & & Art. & Art. & Art. \\\\ \\hline \\multicolumn{10}{l}{_Unsupervised Models_} \\\\ \\hline Jina\\({}_{\\text{base-v2}}\\)(Günther et al., 2024) & 2048 & 137M & 87.2 & 97.7 & 35.1 & 95.3 & 99.7 & 83.0 \\\\ Jina\\({}_{\\text{base-v2}}\\)(Günther et al., 2023) & 8192 & 137M & 93.3 & 98.6 & 40.8 & 95.1 & 99.3 & 85.5 \\\\ nomic-embed-text-v1-ablated & 2048 & 137M & 83.1 & 97.3 & 49.4 & 97.4 & 99.9 & 85.4 \\\\ nomic-embed-text-v1-ablated & 4096 & 137M & 89.1 & 97.6 & 49.6 & 97.5 & 99.9 & 86.7 \\\\ nomic-embed-text-v1-ablated & 8192 & 137M & 92.5 & 97.8 & 47.6 & 96.5 & 99.9 & **86.9** \\\\ nomic-embed-text-v1 & 2048 & 137M & 86.1 & 96.9 & 47.8 & 96.1 & 99.7 & 85.3 \\\\ nomic-embed-text-v1 & 4096 & 137M & 89.0 & 97.4 & 45.7 & 95.8 & 99.9 & 85.6 \\\\ nomic-embed-text-v1 & 8192 & 137M & 90.9 & 97.8 & 44.2 & 94.9 & 99.9 & 85.5 \\\\ text-embedding-ada-002 & 8192 & N/A & 37.3 & 44.3 & 7.30 & 85.1 & 89.7 & 52.7 \\\\ text-embedding-3-small & 8192 & N/A & 92.2 & 97.7 & 27.4 & 95.9 & 98.9 & 82.4 \\\\ text-embedding-3-large & 8192 & N/A & 88.0 & 93.6 & 25.5 & 93.2 & 96.8 & 79.4 \\\\ \\hline E5\\({}_{\\text{mistral}}\\)(Wang et al., 2023b) & 4096 & 7B & 95.9 & 98.3 & 46.8 & 98.4 & 99.8 & **87.8** \\\\ \\hline \\hline \\multicolumn{10}{l}{_Supervised Models_} \\\\ \\hline M2-Bert (Saad-Falcon et al., 2024) & 2048 & 80M & 81.8 & 94.7 & 58.5 & 87.3 & 95.5 & 83.6 \\\\ M2-Bert (Saad-Falcon et al., 2024) & 8192 & 80M & 94.7 & 96.5 & 64.1 & 86.8 & 97.5 & **87.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: LoCo 벤치마크에 대한 결과(Saad-Falcon et al., 2024). NCDG@10은 각 데이터 세트에 대해 보고된다. 우리는 평가를 매개변수 클래스로 나누고 평가가 감독 또는 비감독 설정에서 수행되는지 여부를 나눈다. 우리는 각 분할에서 최고 성능의 모델을 굵게 표시했다. Nomic-embed-text-v1은 가장 성능이 좋은 100M 매개변수 클래스 비감독 모델이다. Nomic-embed-text-v1은 7B 매개변수 클래스 모두에서 최고 성능의 모델과 LoCo 벤치마크를 위해 특별히 감독된 설정으로 훈련된 모델과 경쟁적이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>