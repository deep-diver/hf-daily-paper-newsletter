<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LongRoPE: 200만 토큰 이상의 LLM 컨텍스트 윈도우 확장\n' +
      '\n' +
      'Yiran Ding\n' +
      '\n' +
      '리나 장\n' +
      '\n' +
      'Chengruidong Zhang\n' +
      '\n' +
      'Yuanyuan Xu\n' +
      '\n' +
      'Ning Shang\n' +
      '\n' +
      'Jiahang Xu\n' +
      '\n' +
      '판양모양\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '큰 컨텍스트 윈도우는 큰 언어 모델(LLM)에서 바람직한 특징이다. 그러나, 높은 미세 조정 비용, 긴 텍스트의 희소성 및 새로운 토큰 포지션에 의해 도입된 재앙적 가치로 인해, 현재의 확장된 컨텍스트 윈도우는 약 128k 토큰으로 제한된다.\n' +
      '\n' +
      '본 논문은 LongRoPE를 소개한다. LongRoPE는 처음으로 사전 훈련된 LLM의 컨텍스트 창을 인상적인 **2048k** 토큰으로 확장하며, 256k 훈련 길이 내에서 최대 1k의 미세 조정 단계만 수행하면서 원래의 짧은 컨텍스트 창에서 성능을 유지한다. 이는 세 가지 핵심 혁신에 의해 달성된다: (i) 효율적인 검색을 통해 위치 보간에 있어서 두 가지 형태의 불균일성을 식별하고 이를 활용하여 미세 조정을 위한 초기화를 제공하고 미세 조정이 아닌 시나리오에서 8\\(\\times\\) 확장을 가능하게 한다; (ii) 먼저 256k 길이의 LLM을 미세 조정한 다음 미세 조정된 확장 LLM에서 두 번째 위치 보간을 수행하여 2048k 컨텍스트 윈도우를 달성하는 점진적 확장 전략을 도입한다; (iii) 짧은 컨텍스트 윈도우 성능을 복구하기 위해 8k 길이의 LongRoPE를 재조정한다. 다양한 작업에 걸쳐 LLaMA2 및 미스트랄에 대한 광범위한 실험은 본 방법의 효과를 입증한다. LongRoPE를 통해 확장된 모델은 위치 임베딩에 약간의 수정을 가하여 원래 아키텍처를 유지하고 대부분의 기존 최적화를 재사용할 수 있다. 코드는 [https://github.com/microsoft/LongRoPE](https://github.com/microsoft/LongRoPE)에서 사용할 수 있다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델들(LLMs)은, 다양한 작업들(OpenAI 등, 2023; Touvron 등, 2023)에서 현저한 성공에도 불구하고, 종종 제한된 컨텍스트 윈도우 크기, 예를 들어, LLaMA2의 4096 토큰 제한(Touvron 등, 2023)으로 고통받는다. 컨텍스트 창을 넘어 모델이 훈련되지 않은 추가 위치로 인해 LLM의 성능이 저하된다. 이는 수많은 예를 가진 상황 내 학습(Huang et al., 2023) 및 LLM 에이전트(Park et al., 2023; Madaan et al., 2023)와 같은 중요한 시나리오에서 과제를 제기한다.\n' +
      '\n' +
      '최근의 연구들은 미리 훈련된 LLM 컨텍스트 윈도우가 더 긴 텍스트들에 대해 미세 조정함으로써 약 128k로 확장될 수 있음을 보여준다(Chen et al., 2023b;a; Peng et al., 2023; Zhang et al., 2024; Liu et al., 2023). 상황 창을 더 확장하기 위한 세 가지 주요 장애물이 있다. 첫째, 훈련되지 않은 새로운 포지션 지수들은 많은 파국적인 값들을 도입함으로써, 아웃-오브-분배 이슈들을 야기하고 미세 조정을 수렴하기 어렵게 한다(Chen et al., 2023a). 이것은 4k에서 \\(>\\)1000k로의 확장이 90% 이상의 새로운 포지션을 도입할 때 특히 어렵다. 둘째, 미세 조정은 일반적으로 해당 길이의 텍스트를 필요로 한다. 그러나 현재 데이터 세트의 긴 텍스트, 특히 1000k를 초과하는 텍스트는 제한적이다. 더욱이, 여분의 텍스트들에 대한 트레이닝은 계산적으로 비싸고, 엄청나게 광범위한 트레이닝 시간들 및 GPU 자원들을 요구한다. 셋째, 매우 긴 컨텍스트 윈도우로 확장할 경우, 수많은 토큰 위치에 얇게 분산되어 원래의 짧은 컨텍스트에 대한 성능을 저하시킨다(Chen et al., 2023a).\n' +
      '\n' +
      '첫 번째 도전을 완화하기 위한 하나의 접근법은 RoPE 위치 임베딩(Su et al., 2021; Chen et al., 2023a)을 보간하는 것이며, 이는 새로운 위치 인덱스들을 미리 스케일링하는 것이다.\n' +
      '\n' +
      '그림 1: LongRoPE와 다른 확장 방법을 사용하는 최신 롱컨텍스트 LLMs 간의 책 3 당혹성 비교.\n' +
      '\n' +
      'trained range, 도 2에 도시된 바와 같이. Position Interpolation(PI)(Chen et al., 2023)은 RoPE의 회전각을 연장비만큼 선형 보간한다. NTK(LocalLLaMA, 2023;a)는 RoPE 차원에 걸쳐 불평등한 보간 및 외삽을 옹호한다. YaRN(Peng et al., 2023)은 RoPE 차원을 세 개의 주파수 기반 그룹으로 분류하고 각각 외삽, NTK 및 선형 보간을 적용한다. 그러나, 위치 임베딩은 트랜스포머 아키텍처에서 _complex non-uniform information entropy_를 나타낸다. 이러한 미묘한 불균일성은 기존 접근법에 의해 효과적으로 활용되지 않아 정보 손실을 초래하고 따라서 컨텍스트 윈도우 크기를 제한한다.\n' +
      '\n' +
      '섹션 2는 두 가지 주요 발견을 경험적으로 드러낸다: **(1)** 효과적인 위치 보간은 변화하는 RoPE 치수 및 토큰 위치의 두 가지 형태의 불균일성을 고려해야 한다. 낮은 RoPE 차원과 초기 시작 토큰 위치는 보간이 적을수록 유리하지만 최적 솔루션은 목표 확장 길이에 따라 다르다. **(2)** 이러한 불균일성을 위치 보간으로 고려함으로써, 원래의 RoPE, 특히 키 치수 및 토큰 위치에 정보를 효과적으로 보유할 수 있다. 이것은 위치 보간으로 인한 손실을 최소화하고, 따라서 미세 조정을 위한 더 나은 초기화를 제공한다. 또한, 비 미세 조정 시나리오에서 \\(8\\times\\) 확장을 허용한다.\n' +
      '\n' +
      '연구 결과에 동기 부여되어 LLM 컨텍스트 창을 \\(2\\)_million_token 이상으로 확장하는 효과적인 방법인 **LongRoPE**를 소개한다. 롱로프는 세 가지 핵심 혁신을 기반으로 합니다. 첫째, LongRoPE는 위치 보간에서 다차원 불균일성을 완전히 이용한다. 토큰 위치를 기반으로 각 RoPE 차원에 대한 RoPE의 회전 각도에 대한 효과적인 재스케일 요인을 식별한다. 리스케일 요소를 식별하는 탐색 공간이 목표 확장 비율에 따라 기하급수적으로 확장됨에 따라 LongRoPE는 탐색 효율을 높이기 위해 두 가지 최적화 기법을 가진 진화 탐색 알고리즘을 도입한다. 도. 도 2는 검색된 리스케일된 RoPE의 일 예를 나타낸다.\n' +
      '\n' +
      '그런 다음 LongRoPE는 매우 길고 거의 사용할 수 없는 텍스트를 직접 미세 조정할 필요 없이 2048k 컨텍스트 창을 달성하기 위해 효율적이고 점진적인 확장 전략을 활용한다. 전략은 미리 훈련된 LLM에서 256k 길이를 검색하고 이 길이 아래에서 미세 조정하는 것으로 시작한다. 그 다음, 비-균일 위치 보간법이 비-미세 조정 설정에서 \\(8\\times\\) 확장을 허용함에 따라, 미세 조정된 확장 LLM에서 새로운 RoPE 재스케일 인자에 대한 두 번째 검색을 수행한다. 이것은 궁극적으로 LLaMA2 및 Mistral(Jiang et al., 2023)에 대한 2048k 컨텍스트 윈도우를 달성한다.\n' +
      '\n' +
      '마지막으로, LongRoPE는 원래의 (짧은) 컨텍스트 윈도우에서 성능 저하를 완화하기 위해 확장된 LLM에서 RoPE 재스케일 인자를 계속 조정한다. 256k에서 2048k로 확장하는 것과 유사하게, 우리는 위치 보간을 덜 장려하기 위해 탐색 알고리즘을 사용하여 256k 미세 조정 LLM에서 4k 및 8k 컨텍스트 창으로 축소한다. 추론 과정에서 시퀀스 길이가 8k 미만이면 탐색된 리스케일 인자로 RoPE를 갱신한다.\n' +
      '\n' +
      '다양한 LLM과 다양한 긴 컨텍스트 작업에 걸친 광범위한 실험을 통해 본 방법의 효율성을 입증한다. LongRoPE는 4k에서 2048k까지의 낮은 복잡도를 유지하고, 90% 이상의 패스키 검색 정확도를 달성하며, 4096 컨텍스트 윈도우 내에서 설계된 표준 벤치마크에서 유사한 정확도를 제공하는 데 매우 효과적임을 보여준다. LongRoPE는 RoPE 임베딩을 기반으로 하는 모든 LLM에 적용될 수 있다. 우리는 코드와 LongRoPE-2048k 모델을 출시할 것입니다.\n' +
      '\n' +
      '##2 위치보간에서의 불균일성\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '트랜스포머 모델은 입력 토큰의 순서를 나타내기 위해 종종 위치 임베딩의 형태로 명시적인 위치 정보를 필요로 한다. 우리의 연구는 최근 LLM에서 널리 사용되는 RoPE(Su et al., 2021) 위치 임베딩에 초점을 맞추고 있다. 위치 인덱스에서의 토큰 \\(n\\)에 대해, 그것의 대응하는 RoPE 인코딩은 다음과 같이 단순화될 수 있다:\n' +
      '\n' +
      '[cos(n\\theta_{0}),sin(n\\theta_{0}),cos(n\\theta_{1}),\\cdots,cos(n\\theta_{d/2-1}), sin(n\\theta_{d/2-1})] \\tag{1}\\].\n' +
      '\n' +
      '여기서 \\(d\\)은 임베딩 차원, \\(n\\theta_{i}\\)은 위치 \\(n\\)에서 토큰의 회전각, \\(\\theta_{i}=\\theta^{-2i/d}\\)은 회전 주파수를 나타낸다. RoPE에서 \\(\\theta\\)의 기본값은 10000이다.\n' +
      '\n' +
      '_Context window extension ratio \\(s\\)_와 위치 보간_. 우리는 확장된 문맥 길이\\(L^{\\prime}\\)와 원래의 길이\\(L\\): \\(s=\\frac{L^{\\prime}}{L}\\)의 비율로 \\(s\\)을 정의한다.\n' +
      '\n' +
      '현재 위치 보간 방법은 문맥 윈도우를 확장(L\\(L^{\\prime}\\)에서 확장(L^{\\prime}\\)으로 확장하기 위해 확장비(s\\)를 기반으로 회전 주파수(\\theta_{i}\\)를 축소하는 방법을 제안한다. \\(\\beta=\\theta^{2/d}\\) 및 \\(\\lambda\\)는 \\(s\\)과 관련된 실제 재스케일 인자를 나타내면 다음과 같이 위치 보간 방법을 통합한다.\n' +
      '\n' +
      '\\frac{n}{\\lambda(\\beta)^{0}}\\right),sin\\left(\\frac{n}{\\lambda(\\beta)^{0}}\\right),cos\\left(\\frac{n}{\\lambda(\\beta)^{1}}\\right),cdots,sin\\left(\\frac{n}{\\lambda(\\beta)^{d/2-1}}\\right)\\right]\\tag{2}\\right]\n' +
      '\n' +
      '**선형 위치 보간(PI)** PI(Chen et al., 2023)는 미리 훈련된 길이 한계 내에서 위치 인덱스들의 선형 보간을 제안한다. 목표 신장비\\(s\\)에 대해, 모든 위치의 회전각은 모든 RoPE 치수에 걸쳐 \\(\\lambda=s\\)만큼 선형적으로 감소한다. 그러나 이것은 위치 정보를 매우 "밀집"하게 만들어 모델의 능력이 밀접하게 위치한 토큰을 구별하는 것을 방해한다. 따라서 PI는 높은 확장 비율에서 낮은 성능을 보이는 경향이 있다.\n' +
      '\n' +
      '**NTK 기반 보간 및 외삽** (LocalLLaMA, 2023;a) RoPE를 정보 인코딩 관점에서 살펴보고, Neural Tangent Kernel(NTK) 이론을 적용한다(Jacot et al., 2018; Tancik et al., 2020). PI의 혼잡 위치 문제를 완화하기 위해 RoPE 차원에 보간 압력을 분산할 것을 제안한다. 낮은 (고주파) 차원은 더 적고 높은 (저주파) 차원은 더 많이 스케일링되어 위치 보간과 외삽이 모두 수행되며, 여기서 \\(\\lambda=s^{i}\\)이다. 개선된 동적 NTK(LocalLLaMA, 2023a)는 현재 시퀀스 길이에 기초하여 각 위치에서의 확장 비율을 조정한다. 미세조정이 필요한 PI와 달리 NTK 인식 방법은 미세조정이 없는 시나리오에서 컨텍스트 윈도우를 확장할 수 있지만 일반적으로 최대 확장 비율은 4\\(\\times\\)이다.\n' +
      '\n' +
      '**YaRN**(Peng et al., 2023)는 위치 보간 성능에 상당한 개선을 도입한다. RoPE 차원을 각각 다른 보간 전략을 가진 세 개의 주파수 기반 그룹으로 나눈다. 고주파 차원은 외삽(\\(\\lambda\\)=1)을 거치는 반면, 저주파 차원은 선형 보간법(PI)을 사용한다. 중간에 해당하는 RoPE 치수는 NTK를 사용합니다. YaRN의 핵심은 현재 인간 주도 경험적 실험에 의존하는 RoPE 차원의 그룹화에 있다. 이는 새로운 LLM에 대해 최적이 아닌 성능을 초래할 수 있다.\n' +
      '\n' +
      '비균일 위치보간에 관한### 연구\n' +
      '\n' +
      'NTK 및 YaRN에서 영감을 얻은 우리는 특히 특수 보간 및 외삽을 위해 RoPE 차원에 걸쳐 다른 주파수를 고려할 때 비선형성에서 얻은 이득을 알아차린다. 그러나, 현재의 비선형성은 인간이 설계한 규칙에 크게 의존한다. 이것은 자연스럽게 두 가지 질문을 제기한다: (1) 현재의 위치 보간이 최적인가? (2) 미개척 비선형성이 존재하는가?\n' +
      '\n' +
      '이러한 질문에 답하기 위해 우리는 진화 검색을 사용한다.(3절 참조) LLaMA2-7B에 대한 더 나은 불균일 위치 보간을 발견하기 위해. 검색은 PG19(Rae et al., 2019) 검증 세트의 5개의 무작위 샘플을 사용하여 복잡성에 의해 안내된다. 우리의 실증 분석을 통해 다음과 같은 주요 결과를 드러낸다.\n' +
      '\n' +
      '**1**을 찾는 단계: _RoPE 치수는 실질적인 불균일성을 나타내며, 이는 현재 위치 보간 방법에 의해 효과적으로 처리되지 않는다._\n' +
      '\n' +
      '우리는 식 2에서 각 RoPE 차원에 대한 최적의 \\(\\lambda\\)을 검색한다. 표 1은 미세 조정 없이 PG19 및 Proof-pile(Azerbayev et al., 2022) 테스트 세트에서 다른 방법 하에서 LLaMA2-7B의 복잡성을 비교한다. 검색된 솔루션\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\multirow{2}{*}{(LLaMA2-7B) Extension method} & \\multicolumn{4}{c}{Context Window Size} \\\\ \\cline{2-5}  & PG19 (5 samples) & \\multicolumn{4}{c}{Proof-pile (10 samples)} \\\\ \\cline{2-5}  & 8192 & 16,384 & 8192 & 16,384 \\\\ \\hline PI & 10.65 & 20.49 & 3.65 & 4.93 \\\\ Dy-NTK & 10.21 & 23.29 & 3.50 & 3.87 \\\\ YaRN & 32.64 & 87.89 & 3.49 & 3.25 \\\\ \\hline\n' +
      '**Search for RoPE Dim-wise \\(\\lambda\\)** & **9.37** & **11.34** & **3.45** & **3.13** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 방법을 통해 확장된 LLaMA2-7B의 복잡도. 각 RoPE 차원의 재스케일 요인에 대한 간단한 검색을 통해 당혹감을 크게 줄일 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline Method & non-fine-tuned & fine-tuned \\\\ \\hline PI & 72.54 & 2.44 \\\\ \\hline YaRN & 4.15 & 2.42 \\\\ \\hline\n' +
      '**Search (Dim-wise \\(\\lambda\\) and \\(\\hat{n}\\))** & **3.22** & **2.36** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 비-미세 조정 및 미세 조정 설정에서 64k 컨텍스트 창을 갖는 확장된 LLaMA2-7B의 증명-파일 복잡성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\multirow{2}{*}{(LLaMA2-7B) Extension method} & \\multirow{2}{*}{\\(L^{\\prime}\\)} & \\multicolumn{4}{c}{No interpolation for first \\(i\\) tokens} \\\\ \\cline{2-5}  & & PG19 (5 samples) & \\multicolumn{4}{c}{Proof-pile (10 samples)} \\\\ \\cline{2-5}  & 8192 & 16,384 & 8192 & 16,384 \\\\ \\hline PI & 10.65 & 20.49 & 3.65 & 4.93 \\\\ Dy-NTK & 10.21 & 23.29 & 3.50 & 3.87 \\\\ YaRN & 32.64 & 87.89 & 3.49 & 3.25 \\\\ \\hline\n' +
      '**Search for RoPE Dim-wise \\(\\lambda\\)** & **9.37** & **11.34** & **3.45** & **3.13** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: PG19(5개의 샘플) 상에서 확장된 LLaMA2-7B의 복잡도. 위치 보간 없이 첫 번째 \\(\\hat{n}\\) 토큰을 유지할 때 PI와 Dynamic-NTK의 성능이 모두 향상된다.\n' +
      '\n' +
      '도 2: 상이한 보간 방법들 하에서 RoPE 임베딩을 보여주기 위한 예시적인 예. _ Upper_: RoPE under direct extrapolation. _ Middle_: 선형 위치 보간 하에서 Rescaled RoPE. _ Down_: LongRoPE는 식별된 두 개의 불균일성을 완전히 이용하여, 상이한 토큰 위치들에서 RoPE 치수들에 걸쳐 다양한 보간 및 외삽을 유도한다.\n' +
      '\n' +
      '현 선형(PI) 및 불균일(Dynamic-NTK 및 YaRN) 보간이 차선책임을 시사하는 상당한 개선을 보여준다. 특히, YaRN은 비 미세 조정 LLM의 목표 컨텍스트 창 길이에 도달하지 않기 때문에 PG19에서 PI 및 NTK보다 성능이 낮다. 예를 들어, YaRN의 퍼플렉시티 스파이크는 8k 컨텍스트 크기에서 7k 이후에 급증한다.\n' +
      '\n' +
      '우리의 검색을 통해 Eq에서 재축소된 요인\\(\\lambda\\)을 얻었다. 2는 PI, NTK의 공식 계산 및 YaRN의 그룹별 계산에서 고정 스케일\\(s\\)과 다르게 불균일해진다. 이러한 비균일 요소는 미세 조정 없이 8k 및 16k 컨텍스트 윈도우에 대한 LLaMA2의 언어 모델링 성능(즉, 당혹성)을 상당히 향상시킨다. 이는 결과적인 위치 임베딩이 원래의 RoPE, 특히 키 차원을 효과적으로 보존하고, 따라서 가까운 토큰 위치를 구별하는 LLM의 어려움을 감소시키기 때문이다.\n' +
      '\n' +
      '**2**를 찾는 단계: 입력 시퀀스에서 초기 토큰들에 대한 _RoPE는 더 적은 보간으로 외삽되어야 한다._\n' +
      '\n' +
      '입력 시퀀스에서 초기 토큰(\\hat{n}\\)의 경우, RoPE가 보간을 덜 수행해야 한다고 가정한다. 이는 Streaming LLM(Xiao et al., 2023) 및 LM-Infinite(Han et al., 2023)에서 관찰된 바와 같이 큰 주의 점수를 받아 주의 레이어에 중요하기 때문이다. 이를 검증하기 위해 PI와 NTK를 사용하여 문맥 창을 8k와 16k로 확장하고, 첫 번째 \\(\\hat{n}\\)(0,2,..., 256) 토큰을 보간 없이 유지한다. \\(\\hat{n}\\)=0일 때 원래의 PI와 NTK로 되돌아간다. 표 2는 두 가지 주요 관찰을 강조하는데, 위치 보간 없이 시작 토큰을 유지하는 **(1)**는 실제로 성능을 향상시킨다. **(2)** 시작 토큰의 최적 수 \\(\\hat{n}\\)는 목표 확장 길이에 따라 달라진다.\n' +
      '\n' +
      '**3**을 찾는 단계: _비균일 위치 보간은 미세 조정 및 비 미세 조정 설정 모두에서 LLM 컨텍스트 윈도우를 효과적으로 확장한다._\n' +
      '\n' +
      '우리가 탐색한 비균일 위치 보간이 미세 조정 없이 8k와 16k에서 확장 성능을 크게 향상시킨다는 것을 보여주었지만, 더 긴 확장에는 미세 조정이 필요하다. 따라서, 우리는 64k 컨텍스트 윈도우 크기에 대해 검색된 RoPE로 LLaMA2-7B를 미세 조정한다(설정은 부록 참조). 표 3에서 알 수 있듯이, 우리의 방법은 LLaMA2-7B를 미세 조정 전후에 PI 및 YaRN을 크게 능가한다. 이것은 불균일한 위치 보간을 효과적으로 사용하여 정보 손실을 최소화하고 미세 조정을 위한 더 나은 초기화를 제공하기 때문이다.\n' +
      '\n' +
      '요약 우리의 연구는 다양한 RoPE 치수와 토큰 위치의 두 가지 불균일을 해결한다. 위치 보간에서 이러한 불균일성을 효과적으로 활용하면 LLM 컨텍스트 확장 성능이 크게 향상된다.\n' +
      '\n' +
      '## 3 LongRoPE\n' +
      '\n' +
      '이러한 연구 결과를 바탕으로 LongRoPE를 제안한다. LongRoPE는 먼저 두 가지 비균일성을 완전히 활용하기 위해 효율적인 검색 알고리즘을 도입한 다음, 이를 사용하여 LLM 컨텍스트 창을 200만 토큰 이상으로 확장한다.\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      '두 개의 불균일성은 방대한 솔루션 공간으로 이어질 수 있고 최적화에서 복잡성을 도입할 수 있다. 이를 해결하기 위해 다차원 비균일 위치 보간 최적화 문제를 탐색 문제로 프레임화한다.\n' +
      '\n' +
      '문맥 윈도우 크기(L^{\\prime}\\)와 긴 입력 문서(\\(\\mathbf{X}\\)를 대상으로 하는 LLM에 대해, 각 입력 문서(\\(\\mathbf{x}\\in\\mathbf{X}\\)가 토큰 길이에서 \\(L^{\\prime}\\)를 능가하는 경우, 우리는 RoPE 임베딩에서 \\(i^{th}\\) 차원의 원래 회전각을 \\(\\frac{n}{\\beta^{i}\\)으로 나타낸다. 이어서, 최적화 문제는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\text{LLM}(\\text{RoPE}, \\mathbf{X}\\in\\mathbf{X};\\,|\\mathbf{x}|\\geq L^{\\prime}\\end{subarray}{\\arg\\min}\\mathcal{L}\\,\\left(\\text{LLM}(\\text{RoPE}, \\mathbf{X})\\right),\\text{where}\\tag{3}\\text{m}\n' +
      '\n' +
      '여기서 우리는 두 가지 형태의 불균일성을 커버하기 위해 한 세트의 재스케일 인자, \\(\\mathbb{I}(\\hat{\\lambda}_{i},\\hat{n})를 소개한다. \\\\ (\\hat{\\lambda}_{i}\\)와 \\(\\hat{n}\\)은 각각 RoPE 치수와 토큰 위치의 불균일성을 나타낸다. 구체적으로, \\(\\hat{\\lambda}_{i},\\hat{n})\\) 로페 차원에 대한 회전각을 재조정하기 위해 \\(\\hat{\\lambda}_{i},\\hat{n}\\)을 사용하는데, 여기서 \\(\\hat{\\lambda}_{i}\\)은 재조정 인자이고 \\(\\hat{n}\\)은 토큰 위치 임계값이다. 초기(\\hat{n}\\)-1 토큰 위치의 경우, 재스케일 인자\\(\\hat{\\lambda}_{i}\\)는 영향을 받지 않으며, 원래의 RoPE 회전각\\(\\frac{n}{\\beta^{i}\\)을 사용한다. 위치 \\(n\\geq\\hat{n}\\)에서의 토큰에 대해, 재스케일 팩터가 적용된다.\n' +
      '\n' +
      '목표 상황 윈도우 크기가 \\(L^{\\prime}\\)일 때, 본 연구의 목적은 최적의 재구조 요인을 찾는 것이다. \\(\\mathbb{I}(\\hat{\\lambda}_{0},\\hat{n})\\), \\(\\mathbb{I}(\\hat{\\lambda}_{1},\\hat{n})\\), \\(\\mathbb{I}(\\hat{\\lambda}_{1},\\hat{n})\\), (\\mathbb{I}(\\hat{\\lambda}_{i},\\hat{n}...\\)) (1^{st}\\)에서 \\(d^{th}\\)RoPE 차원까지. 그 결과, 리스케일된 RoPE를 갖는 타겟 LLM은 토큰 길이가 \\(L^{\\prime}\\)인 입력 샘플 \\(\\mathbf{X}\\)에 대해 최소의 다음 토큰 예측 손실(\\(\\mathcal{L}\\)(즉, perplexity)을 달성할 수 있다.\n' +
      '\n' +
      '비균일 위치 보간을 검색하는###\n' +
      '\n' +
      'Eq.의 문제를 해결하기 위해. 셋째, 위치 임베딩의 다차원 불균일성을 완전히 이용하기 위해 최적의 RoPE 재스케일 인자를 찾는 간단하면서도 매우 효과적인 방법을 소개한다.\n' +
      '\n' +
      '\'검색 공간\' 우리는 두 개의 비균일성을 포함하도록 넓은 탐색 공간을 설계한다. 표 4는 탐색 공간을 예시한다. 구체적으로, RoPE 임베딩에서 각 차원에 대한 특화된 리스케일 인자의 탐색을 허용한다. 단순한 공간 설계를 위해 \\(\\lambda_{i}\\)와 \\(\\hat{n}\\)을 찾는 대신 \\(\\lambda_{i}\\)과 \\(\\hat{n}\\)을 탐색한다. 여기서 \\(\\hat{\\lambda_{i}=1/\\lambda_{i}\\). 표 4에 나타낸 바와 같이, \\(\\lambda_{i}\\)는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l} \\hline Non-uniformity & Notation, & Search Space \\\\ \\hline RoPE dimension & \\(\\lambda_{i}\\) & (1.0, extension ratio \\(s\\times\\)1.25, 0.01) \\\\ \\hline Starting tokens & \\(\\hat{n}\\) & [0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 64, 128, 256] \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: RoPE 재스케일 요인에 대한 검색 공간. 세 값의 튜플은 가장 낮은 값, 가장 높은 값 및 단계 크기를 나타낸다.\n' +
      '\n' +
      '최소값 1.0(즉, 직접 외삽)에서 최대값 \\(s\\times 1.25\\)(즉, PI보다 큰 보간)까지 0.01의 스텝 크기로 검색할 수 있으며, 여기서 \\(s\\)은 목표 컨텍스트 윈도우 확장 비율이다.\n' +
      '\n' +
      '(\\hat{n}\\)은 위치 보간 없이 유지되는 초기 토큰 위치의 수(즉, 원래의 RoPE 임베딩을 사용함)를 제어한다. 경험적으로, {0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 64, 128, 256}에서 \\(\\hat{n}\\)을 검색할 수 있다. \\(\\hat{n}=0\\)일 때, 모든 토큰 위치들은 검색된 리스케일 팩터들을 사용한다.\n' +
      '\n' +
      '**진화 기반 검색.** 표 4의 우리의 검색 공간은 수많은 위치 보간 솔루션에 걸쳐 있어 효율적인 탐사를 위한 중요한 도전을 제기한다. 예를 들어, \\(s=4\\times\\)의 확장은 \\(400^{128/2}\\times 14\\)=\\(4\\times 10^{167}\\)의 선택으로 이어진다. 확장 비율이 클수록 검색 공간은 기하급수적으로 확장됩니다. 이를 해결하기 위해 진화 검색(Guo et al., 2020)을 사용하고 검색 효율을 크게 높이기 위해 두 가지 최적화 기법을 소개한다. 알고리즘 1은 전체 검색 절차를 보여준다.\n' +
      '\n' +
      '_최적화된 초기 모집단 생성_. 우리는 무작위로 \\(P\\) 재조정 인자의 모집단을 초기화하는 대신 PI, NTK 및 YaRN에 해당하는 세 가지 RoPE 재조정 인자를 초기 모집단에 개인으로 추가한다. 나머지 \\(P\\)-3개체에 대해, 우리는 \\(p\\)의 확률로 세 가지 재구성 인자를 무작위로 돌연변이시킨다.\n' +
      '\n' +
      '_ 단조롭게 감소하지 않는 제약 조건_. 초기 모집단을 생성한 후 각 개인에 대한 LLM 당혹감을 계산한다. 구체적으로, 대상 LLM에 해당하는 RoPE 재스케일 인자를 적용하여 입력의 복잡도를 계산한다. 최상위권 개인들은 진화의 부모가 된다. 그러나 방대한 검색 공간은 순진한 돌연변이와 교차로 인해 열악한 솔루션을 탐색할 수 있어 불필요한 복잡성 계산으로 이어질 수 있다. 이것은 각 복잡도 계산의 시간이 많이 소요되는 추론을 고려할 때, \\(L^{\\prime}\\)이 클 때 특히 비효율적이다.\n' +
      '\n' +
      '이를 해결하기 위해 샘플링된 RoPE 재스케일 요인인 \\(\\lambda_{i}\\leq\\lambda_{i+1}\\)에 비감소 단조제약조건을 적용한다. 이러한 제약조건을 만족하는 RoPE만이 난해성 평가를 위해 LLM에 적용되어 검색 비용을 크게 절감한다. 특히, \\(\\lambda_{i}\\)는 RoPE 차원(즉, \\(i\\)=0,...,63)에 따라 단조롭게 증가할 것을 요구한다. 이러한 차원 단조성은 NTK 이론(Jacot et al., 2018; Tancik et al., 2020; LocalLLaMA, 2023b)에 기초하여, 더 높은 주파수를 갖는 더 낮은 차원은 더 적은 보간(즉, 더 작은 \\(\\lambda_{i}\\)), 더 낮은 주파수를 갖는 더 높은 차원은 더 많은 보간(즉, 더 큰 \\(\\lambda_{i}\\))을 할 수 있음을 시사한다.\n' +
      '\n' +
      '**8\\(\\times\\) 확장을 미세 조정하지 않습니다. 우리의 진화적 검색은 보간으로 인한 정보 손실을 최소화하기 위해 주요 차원과 위치를 보존하면서 불균일한 RoPE 재스케일 요인을 효과적으로 식별한다. 그림 3에 도시된 바와 같이, 우리의 방법은 미세 조정 없이 LLaMA2의 컨텍스트 창을 4k에서 32k로 확장할 수 있다. 반면에 PI, 불균일 NTK 및 YaRN과 같은 기존 방법들은 2\\(\\times\\) 확장 후 당혹감을 유발한다.\n' +
      '\n' +
      '### LLM 컨텍스트 창 2048K 확장\n' +
      '\n' +
      '**2048k**로 점진적 확장입니다. 우리는 이제 사전 훈련된 LLM의 컨텍스트 창을 전통적인 4k에서 2048k 이상으로 확장하는 방법을 소개한다. 제시된 바와 같이 비균일 위치 보간은 미세 조정 없이 8\\(\\times\\)의 확장을 얻을 수 있다. 더 큰 확장(512\\(\\times\\))을 위해서는 미세 조정이 필요하다. 한 가지 방법은 목표 2048k 크기에서 RoPE 리스케일링된 인자를 검색한 다음 미세 조정하는 것이다. 그러나 이것은 엄청나게 비싼 훈련 자원으로 인해 어려움에 직면해 있다. 더욱이, 우리의 경험에 기초하여, 큰 확장 비율 하에서 LLMs를 잘 미세 조정하는 것은 어렵다(부록 참조).\n' +
      '\n' +
      '다행히도 LongRoPE는 원본과 미세 조정된 확장 LLM 모두에 효과적이다. 따라서 본 논문에서는 256k 이내의 훈련 길이에서 1k의 미세 조정 단계만으로 목표 2048k를 달성하는 효율적이고 점진적인 방법을 소개한다.\n' +
      '\n' +
      '\\(\\lozenge\\)_Extending pre-trained LLM to 256k with LongRoPE search_. LLaMA2를 예로 들어, 우리는 128k와 256k의 타겟 컨텍스트 윈도우 크기에 대한 검색을 수행한다. 이 단계에서 신장비는 각각 32\\(\\times\\)와 64\\(\\times\\)이다.\n' +
      '\n' +
      '\\(\\lozenge\\)_256k_로 미세 조정. 그런 다음 미리 훈련된 LLM을 미세 조정하여 256k의 컨텍스트 창 크기를 달성한다. 구체적으로, 먼저 128k에 대한 RoPE 리스케일링된 인자를 사용하여 400단계에 대해 LLaMA2를 미세 조정한다. 그런 다음, 완성된 체크포인트에서 RoPE 재축소된 인자를 256k로 교체하고 추가로 600단계의 미세 조정을 수행한다. 상기 방법은\n' +
      '\n' +
      '그림 3: 미세 조정 없이 측정된 다른 방법을 사용하여 확장 후 PG19 및 Proof-Pile에 대한 LLaMA2-7B 복잡성. LongRoPE는 비균일성을 충분히 이용함으로써 미세 조정 없이 **8\\(\\times\\)의 확장을 얻을 수 있다.\n' +
      '\n' +
      '256k로 직접 미세 조정하는 것보다 더 효율적임을 입증합니다.\n' +
      '\n' +
      'LongRoPE search_를 사용하여 2048k까지 미세 조정된 확장 LLM(\\(\\lozenge\\)_Extending fine-tuned extended LLM)을 갖는다. 마지막으로, 미세 조정된 256k 길이의 LLM에 대해 2차 검색을 수행한다. 이는 궁극적으로 더 이상의 미세 조정 없이 2048k의 매우 큰 컨텍스트 윈도우 크기를 초래한다. 최종 확장비는 \\(512\\times\\)이다.\n' +
      '\n' +
      '**더 짧은 컨텍스트 창 복구** 매우 긴 2048k 컨텍스트 창으로 확장한 후 원래 컨텍스트 창 내에서 성능 저하를 감지한다. 이것은 위치 보간(Chen et al., 2023)의 공지된 이슈인데, 이는 원래 컨텍스트 윈도우 내의 더 높은 차원에서의 위치 임베딩을 훨씬 더 좁은 영역에 상주하도록 강제하여, 언어 모델의 성능에 부정적인 영향을 미치기 때문이다. 512\\(\\times\\) 확장 비율로 원래 4k 컨텍스트 윈도우 내의 위치는 특히 붐빈다.\n' +
      '\n' +
      '이를 완화하기 위해 확장된 LLM에 대해 추가 진화 검색을 수행하여 짧은 컨텍스트 길이(예: 4k 및 8k)에 대한 RoPE 재스케일 요인을 조정한다. 우리는 길이가 짧을 때 필요한 위치 보간이 적어 최대 허용 탐색 \\(\\lambda\\)을 줄인다. 추론 동안, LLM은 대응하는 RoPE 재스케일 팩터들을 동적으로 조정한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '**평가 작업 및 모델** LLaMA2-7B와 Mistral-7B에 LongRoPE를 적용하여, (1) 긴 문서에서 확장된 컨텍스트 LLM의 복잡성, (2) 무관한 텍스트 바다에서 간단한 패스키를 검색할 수 있는 모델의 능력을 측정하는 패스키 검색 작업, (3) 짧은 4096 컨텍스트 윈도우 크기 내에서 표준 LLM 벤치마크의 세 가지 측면에서 성능을 평가한다.\n' +
      '\n' +
      '잘 조율하고 있어요 LLaMA2의 경우, 2e-5의 학습률과 32의 전역 배치 크기를 사용하며, Redpajama (Computer, 2023) 데이터셋에서 400단계에 걸쳐 BOS 토큰과 EOS 토큰으로 북엔드된 128k 세그먼트로 세분화한다. 그런 다음, 완성된 체크포인트를 기반으로 256k 컨텍스트 윈도우를 달성하기 위해 추가 600단계를 훈련한다. 128k 컨텍스트 크기는 분산 트레이닝 시스템으로 8개의 A100 GPU에 대해 트레이닝되는 반면(Lin et al., 2023), 256k는 16개의 A100 GPU를 필요로 한다. 미스트랄의 경우 1e-6의 일정한 학습률과 64의 전역 배치 크기를 사용한다. 128k 및 256k 모델 모두에 대해, 우리는 YaRN(Peng et al., 2023)의 설정을 따르며, 16k 시퀀스 길이를 사용하여 Together Computer\'s Long-Data Collections(mis, 2024)에서 400 단계를 수행한다. 우리는 훈련을 위해 4개의 A100 GPU를 사용한다.\n' +
      '\n' +
      '수색해 256k 이내의 목표 윈도우 크기에 대해서는 \\(P\\)=64, \\(N_{1}\\)=\\(N_{2}\\)=16, \\(p\\)=0.3, \\(\\mathcal{T}\\)=40을 사용하고 각 반복에서 돌연변이/교차에 대해 상위 32를 선택한다. 복잡도는 목표 컨텍스트 길이의 최소 길이 요구 사항과 함께 5개의 무작위 PG19 검증 세트 샘플을 사용하여 계산된다. 512k 이상의 창에 대해 모집단, 돌연변이 및 교차 크기를 절반으로 줄인다. 복잡도는 Pile-Books3(Gao et al., 2020) 검증 세트로부터 3개의 랜덤 샘플에 대해 측정된다.\n' +
      '\n' +
      '기준선 2048k에 도달하기 위해 128k 및 256k 컨텍스트 창으로 모델을 미세 조정했다. 이것은 LLaMA2 및 미스트랄에 대해 각각 LongRoPE-2048k(ft=128k) 및 LongRoPE-2048k(ft=256k)를 산출한다. PI, NTK 및 YaRN을 사용하여 위치 보간 후 미세 조정된 개방형 LLM, 특히 최첨단 컨텍스트 창 확장 기준선과 4가지 모델을 비교한다. 여기에는 Together-32k(Together, 2023), Code LLaMA(Roziere et al., 2023), LongLoRA-full-FT-100k(Chen et al., 2023), YaRN-LLaMA 및 YaRN-Mistral(Peng et al., 2023) 등이 포함된다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**256k.** 내에서 긴 시퀀스 언어 모델링 256k 평가 길이 내에서 최첨단 확장 LLM과 비교하는 것으로 시작한다. 우리는 두 개의 데이터 세트를 사용하여 일반화 가능성을 입증한다: Proof-pile(Rae et al., 2019) 및 PG19(Gao et al., 2020) 테스트 분할. 본 논문에서는 256개의 슬라이딩 윈도우를 사용하여 다양한 컨텍스트 길이에서 복잡도를 평가하였고, PG19의 경우 100개의 문서를 대상으로 전체 테스트 스플릿을 사용하였다. Proof-pile의 경우, YaRN(Peng et al., 2023)을 따라 각각 128k 이상의 길이를 갖는 10개의 샘플을 랜덤하게 선택한다.\n' +
      '\n' +
      '표 5와 표 7은 각각 Proof-파일 및 PG19에 대한 서로 다른 보간 방법을 통해 확장된 LLaMA2 및 미스트랄의 복잡성을 비교한다. 두 가지 주요 관찰을 강조합니다. **(1)** 확장 모델은 4k에서 256k 평가 길이로 전체적으로 감소하는 복잡성 추세를 보여 더 긴 컨텍스트를 활용하는 능력을 입증합니다. **(2)** 컨텍스트 창이 16\\(\\times\\) 더 길더라도 일반적으로 더 짧은 길이로 성능을 유지하기 어려운 조건인 LongRoPE-2048k 모델은 256k 컨텍스트 길이 내에서 최첨단 기준선을 능가합니다.\n' +
      '\n' +
      '**2000k** 이상의 긴 시퀀스 언어 모델링. 매우 긴 문서에 대한 효율성을 평가하기 위해 Books3(Gao et al., 2020) 데이터 세트를 사용한다. 평가 효율성을 위해 각각 길이가 2048k를 초과하는 20권의 도서를 무작위로 선택하고 256k의 슬라이딩 윈도우를 사용한다.\n' +
      '\n' +
      '표 6에 나타난 바와 같이, LongRoPE는 LLaMA2-7B 및 Mistral-7B의 컨텍스트 윈도우를 2048k로 성공적으로 확장하는 동시에 8k-128k의 더 짧은 길이 내에서 기준선과 비교되거나 우수한 복잡성을 달성한다. 또한 2048k LLaMA2와 미스트랄 사이의 현저한 성능 차이를 관찰한다. 미스트랄은 더 짧은 길이에서 기준선을 능가하지만, 당혹감은 256k를 넘어 7을 초과한다. LLaMA2 성능은 예상과 일치하며, 당혹감은 1024k 및 2048k에서 한계 증가와 함께 더 긴 컨텍스트와 함께 감사하게 감소한다. 또한, LLaMA2에서 LongRoPE-2048k는 128k에 비해 256k의 미세조정길이에서 2차 확장비(8\\(\\times\\)가 작기 때문에 더 좋은 성능을 보인다. 16\\(\\times\\)) 대조적으로, 미스트랄은 128k의 미세 조정 윈도우 크기에서 더 나은 성능을 보인다. 주요 이유는 미스트랄의 128k 및 256k 미세 조정에 대해 YaRN의 설정을 따라 16k 훈련 길이를 사용하기 때문에 미세 조정 후 컨텍스트 창을 추가로 확장하는 미스트랄의 능력에 영향을 미치기 때문이다.\n' +
      '\n' +
      '패스키 회수 이제 생성 작업에서 효과적인 컨텍스트 창 크기를 연구합니다. 우리는 (Mohtashami and Jaggi, 2023)에서 제안한 패스키 검색의 합성 평가 과제를 따른다. 이 작업에서 모델은 긴 문서에 숨겨진 랜덤 패스키(즉, 5자리 숫자)를 검색하도록 요청된다. 프롬프트 템플릿은 부록에 자세히 나와 있습니다. 평가 컨텍스트 길이에 걸쳐 균일하게 분포된 임의의 위치에 패스키를 배치한 상태에서 패스키 검색 작업을 10회 반복 수행한다.\n' +
      '\n' +
      '도. 도 4는 기준선과의 검색 정확도 비교를 나타낸다. 기존 모델의 정확도는 128k를 넘어 0으로 급격히 떨어진다. 반면, LongRoPE-LLaMA2-2048k(ft=256k)는 100만-레벨 토큰에서 패스키를 검색하는 매우 어려운 작업에도 불구하고 4k에서 2048k까지의 높은 검색 정확도(\\(\\geq\\)90%)를 유지한다. LongRoPE-Mistral-2048k(ft=128k)는 최대 1800k까지 100% 정확도를 유지하며 2048k에서 60%로 감소하여 2048k에서 당혹감이 약간 증가하는 표 6의 예상과 일치한다.\n' +
      '\n' +
      '**원래 컨텍스트 창 내의 표준 벤치마크.** 우리는 제로 샷 및 소수 샷 설정에서 허깅 페이스 오픈 LLM 리더 보드(페이스, 2024)를 사용하여 원래 컨텍스트 창에서 롱로PE-2048k 모델을 평가한다. 우리는 25-shot ARC-Challenge(Clark et al., 2018)를 사용한다. 10-shot HellaSwag (Zellers et al., 2019), 5-shot MMLU (Hendrycks et al., 2020), 0-shot TruthfulQA (Lin et al., 2021).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline Base & Model & Context & Extension & \\multicolumn{6}{c}{Evaluation Context Length} \\\\ \\cline{2-13} LLM & Name & Window & Method & 8k & 16k & 32k & 64k & 128k & 256k & 512k & 1024k & 2048k \\\\ \\hline \\multirow{5}{*}{LIaMA2-7B} & LongRoPE-2048k & 100k & PI & 6.99 & 6.80 & 6.66 & 6.59 & 20.57 & 26.45 & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & Together & 32k & PI & 3.69 & 3.50 & 2.64 & \\(>\\)10\\({}^{2}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & LongLoRa & 100k & PI & 3.83 & 3.62 & 2.68 & 2.44 & 2.33 & 9.89 & \\(>\\)10\\({}^{3}\\) \\\\  & Code LLaMA & 100k & NTK & 3.95 & 3.71 & 2.74 & 2.55 & 2.54 & 2.71 & 49.33 \\\\  & YaRN (\\(s\\)=16) & 64k & YaRN & 3.69 & 3.51 & 2.65 & 2.42 & \\(>\\)10\\({}^{1}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=32) & 128k & YaRN & 3.75 & 3.56 & 2.70 & 2.45 & 2.36 & 2.37 & 99.64 \\\\  & **LongRoPE-2048k** (ft=128k) & **2048k** & LongRoPE & 3.71 & **3.50** & **2.60** & **2.36** & **2.27** & **2.26** & **1.88** \\\\  & **LongRoPE-2048k** (ft=256k) & **2048k** & LongRoPE & 3.85 & 3.65 & **2.63** & **2.38** & **2.28** & **2.26** & **1.87** \\\\ \\hline \\multirow{5}{*}{Mistral-7B} & Mistral v0.1 & 8k & - & **3.09** & 2.96 & \\(>\\)10\\({}^{2}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=8) & 64k & YaRN & 3.18 & 3.04 & 2.37 & 2.20 & 10.39 & 57.4 & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=16) & 128k & YaRN & 3.21 & 3.08 & 2.41 & 2.24 & 2.18 & 2.19 & 4.91 \\\\  & **LongRoPE-2048k** (ft=128k) & **2048k** & LongRoPE & 3.20 & **3.04** & **2.36** & **2.18** & **2.13** & **2.13** & **1.85** \\\\  & **LongRoPE-2048k** (ft=256k) & **2048k** & LongRoPE & 3.20 & **3.04** & **2.36** & **2.18** & **2.13** & **2.14** & **1.84** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: Books3 데이터셋에 대한 복잡도 평가. 추가적인 미세 조정 없이, 훈련 컨텍스트 윈도우 크기가 128k 및 256k인 LongRoPE-2048k 모델은 2048k의 극도로 긴 컨텍스트 크기로 효과적으로 확장된다. 1k=1024 토큰.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline Base & Model & Context & Extension & \\multicolumn{6}{c}{Evaluation Context Length} \\\\ \\cline{2-13} LLM & Name & Window & Method & 8k & 64k & 128k & 64k & 128k & 1680 & \\multicolumn{1}{c}{} & & \\\\ \\hline \\multirow{5}{*}{LIaMA2-7B} & LongRoPE-2048k & 100k & PI & 7.16 & 8.61 & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & Together & 32k & PI & 3.69 & 3.50 & 2.64 & \\(>\\)10\\({}^{2}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & LongLoRa & 100k & PI & 3.83 & 3.62 & 2.68 & 2.44 & 2.33 & 9.89 & \\(>\\)10\\({}^{3}\\) \\\\  & Code LLaMA & 100k & NTK & 3.95 & 3.71 & 2.74 & 2.55 & 2.54 & 2.71 & 49.33 \\\\  & YaRN (\\(s\\)=16) & 64k & YaRN & 3.69 & 3.51 & 2.65 & 2.42 & \\(>\\)10\\({}^{1}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=32) & 128k & YaRN & 3.75 & 3.56 & 2.70 & 2.45 & 2.36 & 2.37 & 99.64 \\\\  & **LongRoPE-2048k** (ft=128k) & **2048k** & LongRoPE & 3.71 & **3.50** & **2.60** & **2.36** & **2.27** & **2.26** & **1.88** \\\\  & **LongRoPE-2048k** (ft=256k) & **2048k** & LongRoPE & 3.85 & 3.65 & **2.63** & **2.38** & **2.28** & **2.26** & **1.87** \\\\ \\hline \\multirow{5}{*}{Mistral-7B} & Mistral v0.1 & 8k & - & **3.09** & 2.96 & \\(>\\)10\\({}^{2}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{3}\\) & \\(>\\)10\\({}^{4}\\) & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=8) & 64k & YaRN & 3.18 & 3.04 & 2.37 & 2.20 & 10.39 & 57.4 & \\(>\\)10\\({}^{4}\\) \\\\  & YaRN (\\(s\\)=16) & 128k & YaRN & 3.21 & 3.08 & 2.41 & 2.24 & 2.18 & 2.19 & 4.91 \\\\  & **LongRoPE-2048k** (ft=128k) & **2048k** & LongRoPE & 3.20 & **3.04** & **2.36** & **2.18** & **2.13** & **2.13** & **1.85** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 다양한 위치 보간 방법을 가진 모델의 증명 파일 퍼플렉시티. ft: 미세 조정에 사용되는 컨텍스트 창 크기입니다. 기존의 롱컨텍스트 모델보다 16\\(\\times\\)의 컨텍스트 윈도우가 더 길어도, 본 논문에서 제안하는 모델은 256k 컨텍스트 길이 내에서 성능을 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '표 8에서 알 수 있듯이 우리 모델은 더 작은 컨텍스트 창에 대해 설계된 원래 벤치마크에서 유사한 결과를 달성하고 진실QA에서 원래 미스트랄을 +0.5% 능가한다. 256k에서 미세 조정된 LongRoPE-LLaMA2-2048k는 약간 더 많은 성능 저하를 나타내지만 대부분의 작업에 대해 합리적인 범위 내에 있다.\n' +
      '\n' +
      '### Ablation Results\n' +
      '\n' +
      '**제2 위치 보간의 효과** 우리의 점진적 확장 전략에서는 탐색 알고리즘을 사용하여 미세 조정된 확장 LLM에 대해 두 번째 비균일 위치 보간을 수행한다. 우리는 미세 조정된 LLaMA2-256k 모델에 대한 실험을 실행하여 그 유효성을 검증한다. PI 및 YaRN을 사용하여 512k, 1024k 및 2048k로 확장한다. 표 9에서 알 수 있듯이, 우리의 불균일한 위치 보간은 일관된 수준의 당혹감을 유지한다. 대조적으로, PI 및 YaRN에서의 당혹감은 확장 비율에 따라 빠르게 증가한다.\n' +
      '\n' +
      '**더 짧은 컨텍스트 길이에서 복구의 효과.** 더 짧은 컨텍스트 길이에서 성능 손실을 완화하기 위해 검색 알고리즘을 통해 LongRoPE-2048k에 대한 RoPE 인자를 재조정한다. 특히, 짧은 4k와 8k 길이에서 더 적은 보간을 장려하기 위해 검색에 대한 최대 허용 스케일 팩터를 줄인다. 표 10은 평균 LLM 벤치마크 정확도와 함께 4k 및 8k 길이의 Proof-pile에 대한 LongRoPE-LLaMA2-2048k의 당혹성 비교를 보여준다. 결과는 짧은 컨텍스트 길이에서 상당한 성능 향상을 분명히 보여준다.\n' +
      '\n' +
      '**불균일성의 두 가지 형태에 대한 분석** 마지막으로, 각 부분이 성능에 어떻게 기여하는지 확인하기 위해 두 가지 불균일성을 절제한다. 우리는 (i) 다른 방법을 사용하여 LLaMA2-7B를 짧은 16k 및 32k로 확장하고, RoPE 차원만 검색하고, 두 불균일성을 모두 검색하는 두 가지 실험을 설정했으며, (ii) 동일한 절차에 따라 미세 조정된 256k 길이의 LLaMA2를 2048k로 확장했다. 당혹감은 미세 조정 없이 평가된다. 표 11에서 알 수 있듯이 RoPE 차원의 불균일성은 PI의 선형 보간법에 비해 당혹감을 크게 감소시킨다. 토큰 위치의 불균일성은 16k 및 32k 길이에서 분명히 성능을 향상시키지만 2048k에서 동일한 영향을 나타내지 않는데, 이는 아마도 극도로 긴 길이 때문일 수 있다. 보간 없이 초기 토큰만 보존하는 것은 유용하지 않게 되고, 우리는 이것을 미래 작업으로 남겨둔다.\n' +
      '\n' +
      '##5 관련 작품\n' +
      '\n' +
      '이 섹션에서는 위치 보간을 기반으로 하는 방법 외에도 다른 접근법의 관련 작업에 대해 논의한다.\n' +
      '\n' +
      '**검색-기반** 접근법들은 추론 시 페칭되는 관련 문서들을 위해 긴 과거의 컨텍스트 및 검색 모듈들을 암기하기 위해 외부 메모리 모듈을 사용한다(Tworkowski et al., 2023; Wang et al., 2023; Borgeaud et al., 2022). 이러한 설계는 일반적으로 LLM 아키텍처에 대한 명시적인 수정이 필요합니다. 대조적으로, 우리의 작업은 작은 위치 임베딩 수정으로 더 가볍다. 또한 긴 문서 요약 및 소수의 샷 학습과 같은 검색 이상의 더 긴 컨텍스트 작업을 처리할 수 있습니다.\n' +
      '\n' +
      '**Attention-based context window extension** 위치 임베딩 보간을 넘어서, 일부 연구는 주의 메커니즘을 조작함으로써 원래의 LLM 컨텍스트 윈도우 길이를 사용하여 입력 컨텍스트 확장을 달성한다(Han et al., 2023; Xiao et al., 2023; Ratner et al., 2022). 핵심 아이디어는 새로운 주의 마스크를 사용하여 새로운 위치로 인한 주의 폭발 문제를 완화시키는 것이다. 이러한 노력과 위치 보간 방법은 상호 보완적이다.\n' +
      '\n' +
      '** 미세 조정 기반 접근법** 더 긴 컨텍스트를 위해 수정된 위치 임베딩으로 미리 훈련된 LLM을 효과적으로 미세 조정하는 방법에 초점을 맞춘다. 코드 LLaMA(Roziere et al., 2023), LLaMA2 Long(Xiong et al., 2023) 및 Scale과 같은 작업들\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline \\multirow{2}{*}{FT Model} & With & Proof-Pile Perplexity & LLM Benchmark \\\\  & Recovery & 4k & 8k & Avg. Accuracy \\\\ \\hline LLaMA2-78-2048k (ft=128k) & \\(\\times\\) & 4.16 & 3.72 & 49.3 \\\\  & ✓ & **3.71** & **3.50** & **52.9** \\\\ \\hline LLaMA2-78-2048k (ft=256k) & \\(\\times\\) & 4.51 & 3.82 & 47.9 \\\\  & ✓ & **3.85** & **3.65** & **50.8** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 더 짧은 컨텍스트 길이에서 성능 회복을 위한 LongRoPE 재조정에 대한 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\multicolumn{4}{c}{**(a) LLaMA2-7B with extended context window**} \\\\ \\hline Model & \\begin{tabular}{c} Context \\\\ Window \\\\ \\end{tabular} & ARC- \\(\\epsilon\\) & \\begin{tabular}{c} HellaN98 \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} MMLU \\\\ \\end{tabular} & TruthfulQA \\\\ \\hline Original LLaMA2-7B & 4k & 53.1 & 78.6 & 46.6 & 39.0 \\\\ Together & 32k & 47.6 & 76.1 & 43.3 & **39.2** \\\\ Code LLaMA & 100k & 42.4 & 64.8 & 40.1 & 37.1 \\\\ YaRN (s=16) & 64k & 52.4 & **78.7** & 42.4 & 38.2 \\\\ YaRN (s=32) & 128k & 52.2 & 78.5 & 41.8 & 37.4 \\\\ LongRoPE-2048k (ft=128k) & 2048k & **52.9** & 76.5 & **43.4** & 38.8 \\\\ LongRoPE-2048k (ft=256k) & 2048k & 51.0 & 75.3 & 39.6 & 37.3 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 허깅 페이스 오픈 LLM 벤치마크에서 롱컨텍스트 LLM과 오리지널 LLaMA2 및 미스트랄의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\multicolumn{4}{c}{**(a) LLaMA2-7B with extended context window**} \\\\ \\hline Model & \\begin{tabular}{c} Context \\\\ Window \\\\ \\end{tabular} & ARC- \\(\\epsilon\\) & \\begin{tabular}{c} HellaN98 \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} MMLU \\\\ \\end{tabular} & TruthfulQA \\\\ \\hline Original LLaMA2-7B & 4k & 53.1 & 78.6 & 46.6 & 39.0 \\\\ Together & 32k & 47.6 & 76.1 & 43.3 & **39.2** \\\\ Code LLaMA & 100k & 42.4 & 64.8 & 40.1 & 37.1 \\\\ YaRN (s=16) & 64k & 52.4 & **78.7** & 42.4 & 38.2 \\\\ YaRN (s=32) & 128k & 52.2 & 78.5 & 41.8 & 37.4 \\\\ LongRoPE-2048k (ft=128k) & 2048k & **52.9** & 76.5 & **43.4** & 38.8 \\\\ LongRoPE-2048k (ft=256k) & 2048k & 51.0 & 75.3 & 39.6 & 37.3 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 상이한 2차 위치 보간 방법을 통해 LLaMA2-256k를 확장하는 북스3 복잡성 비교.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '장, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M. - A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023.\n' +
      '* Lin et al. (2021) Lin, S., Hilton, J., and Evans, O. 진실: 모델이 인간의 거짓을 모방하는 방법을 측정하는 것 arXiv preprint arXiv:2109.07958_, 2021.\n' +
      '* Lin et al. (2023) Lin, Z., Miao, Y., Liu, G., Shi, X., Zhang, Q., Yang, F., Maleki, S., Zhu, Y., Cao, X., Li, C., et al. Superscaler: Supporting flexible dnn parallelization via unified abstraction. _ arXiv preprint arXiv:2301.08984_, 2023.\n' +
      '* Liu et al. (2023) Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D. Scaling laws of rope-based extrapolation. _ arXiv preprint arXiv:2310.05209_, 2023.\n' +
      '*LocalLAMA(2023a) LocalLAMA. 역학적으로 스케일링된 로프는 2023a, 0 미세 조정으로 긴 컨텍스트 라마의 성능을 더욱 증가시킨다. URL[https://www.reddit.com/r/LocalLLAMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLAMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)\n' +
      '*LocalLAMA(2023b) LocalLAMA. Ntk 인식 확장 로프는 라마 모델이 미세 조정 및 최소 당혹성 탈지, 2023b 없이 확장(8k+) 컨텍스트 크기를 가질 수 있도록 한다. URL[https://www.reddit.com/r/LocalLLAMA/comments/14127j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLAMA/comments/14127j5/ntkaware_scaled_rope_allows_llama_models_to_have/)\n' +
      '* Madaan et al. (2023) Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative refinement with self-feedback, 2023.\n' +
      '* Mohtashami & Jaggi (2023) Mohtashami, A. and Jaggi, M. 랜드마크 주의: 변압기에 대한 랜덤 액세스 무한 컨텍스트 길이 _ arXiv preprint arXiv:2305.16300_, 2023.\n' +
      '* OpenAI 등(2021) OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Anadkat, L., Akkaya, I., Balaji, S., Balcom, L., Bogdonoff, L., Boiko, O., Boyd, M., Baltescu, L., Brundage, K., Cai, T., Chen, R., Cann, A., Carey, B., Felford, L., Grey, S., Greene, R., Gentijet, A., Eleundou, D., G.\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.\n' +
      '\n' +
      'Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. _ arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive Transformers for long range sequence modeling. _ arXiv preprint_, 2019. URL[https://arxiv.org/abs/1911.05507](https://arxiv.org/abs/1911.05507).\n' +
      '* Ratner et al. (2022) Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y. 병렬 컨텍스트 윈도우는 대형 언어 모델의 컨텍스트 내 학습을 향상시킨다. _ ArXiv:2212.10947_, 2022.\n' +
      '* Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Defossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2023.\n' +
      '* Su et al. (2021) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. 로포머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* Tancik et al. (2020) Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R. 푸리에 기능을 통해 네트워크는 저차원 도메인에서 고주파수 함수를 학습할 수 있다. _ 2020년, 신경망 정보 처리 시스템_, 33:7537-7547의 발전.\n' +
      '* Together(2023) Together, 2023. URL[https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K).\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M. - A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, E. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Zu, P., Yan, Z., Zarov, I., Zambadur, M., Fan, A., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.\n' +
      '* Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Milos, P. Focused transformer: Contrastive training for context scaling. 2023년\n' +
      '* Wang et al. (2023) Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long term memory. _ arXiv preprint arXiv:2306.07174_, 2023.\n' +
      '* Xiao et al.(2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. 어텐션 싱크가 있는 효율적인 스트리밍 언어 모델. _ arXiv_, 2023.\n' +
      '* Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. 기초 모델의 효과적인 롱컨텍스트 스케일링, 2023.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019.\n' +
      '* Zhang et al. (2024) Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. 4k에서 400k로 치솟기: 활성화 비콘으로 llm의 컨텍스트를 확장하기 arXiv preprint arXiv:2401.03462_, 2024.\n' +
      '* Zhu et al. (2023) Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. 포즈: 위치 스킵-와이즈 트레이닝을 통한 llms의 효율적인 컨텍스트 윈도우 확장, 2023.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      '\'환경\' 우리의 모든 실험은 16개의 A100 GPU에 대해 수행된다. 우리는 훈련과 추론을 가속화하기 위해 Flash Attention-2(Dao, 2023)를 사용한다. GPU 메모리 및 연산 시간이 시퀀스 길이에 따라 기하급수적으로 증가함에 따라 512k 이상의 컨텍스트 길이를 갖는 미세 조정 및 추론을 제공하는 것은 어렵다. 그 결과, 학습 및 추론 비용을 모두 줄이기 위해 내부 플랫폼인 CUBE - (Lin et al., 2023)의 내부 버전을 활용한다.\n' +
      '\n' +
      '패스키 프롬프트 우리는 패스키 검색의 문서 포맷에 대해 기존의 문헌(Mohtashami and Jaggi, 2023; Chen et al., 2023; Peng et al., 2023; Chen et al., 2023; Zhu et al., 2023)을 따른다. 다음과 같이 프롬프트 템플릿을 표시합니다.\n' +
      '\n' +
      '관련 없는 많은 텍스트 안에 중요한 정보가 숨겨져 있다. 찾아서 외우세요. 거기서 중요한 정보에 대해 퀴즈를 낼게요.\n' +
      '\n' +
      '풀은 초록색이다. 하늘이 파랗다 태양은 노랗다. 또 시작이군 몇 번이고 (x회 반복)\n' +
      '\n' +
      '패스키는 **17865**입니다. 기억해 **17865**는 패스 키입니다.\n' +
      '\n' +
      '풀은 초록색이다. 하늘이 파랗다 태양은 노랗다. 또 시작이군 몇 번이고 (y회 반복)\n' +
      '\n' +
      '출입 열쇠가 뭐죠? 상기 패스키는,\n' +
      '\n' +
      '문서 길이는 x와 y 값에 따라 달라집니다. 17865는 검색할 패스키 번호입니다. 무작위로 샘플링되며 각 테스트 시간에 따라 다릅니다.\n' +
      '\n' +
      '### 미세 조정에 대한 추가 세부사항\n' +
      '\n' +
      '섹션 4.2에 도입된 바와 같이 LLaMA2와 미스트랄 모두에 대해 두 가지 컨텍스트 창 길이, 즉 128k와 256k를 미세 조정한다. 구체적으로, 256k 컨텍스트 윈도우를 갖는 모델은 128k 길이 체크포인트로부터 그것의 미세 조정을 시작한다. 도. 도 5(ab)는 이러한 미세 조정 프로세스 동안 LLaMA2 및 미스트랄에 대한 트레이닝 손실을 예시한다. 우리는 세 가지 주요 관찰을 강조한다: **(1)** 128k 컨텍스트 창을 가진 모델은 32\\(\\times\\) 확장으로 인해 큰 초기 손실을 경험한다. 그러나 몇 단계 후에 손실이 급격히 감소합니다. **(2)** LLaMA2와 미스트랄은 서로 다른 미세 조정 설정을 사용합니다. 미스트랄은 16k 길이의 데이터에서 미세 조정함으로써 원하는 긴 컨텍스트 창을 달성하는 반면 LLaMA2는 컨텍스트 창 크기와 일치하는 텍스트 길이를 필요로 한다. 또한, 우리는 일정한 학습률을 사용하는 YaRN의 전략을 채택한다. 그 결과, 미스트랄의 손실은 약 2.2로 떨어진 후 변동하기 시작하는 것을 관찰할 수 있다. **(3)** 미스트랄과 LLaMA2 모두에 대해, 128k 체크포인트로부터 미세 조정을 시작하는 256k 컨텍스트 윈도우를 갖는 모델은 낮은 초기 트레이닝 손실을 나타낸다. 이는 128k 길이의 체크포인트에서 미세 조정이 효과적이고 수렴을 상당히 용이하게 한다는 것을 시사한다.\n' +
      '\n' +
      '또한 256k 컨텍스트 창으로 LLaMA2를 미세 조정하기 위해 다양한 설정을 탐색한다. 도 1에 도시된 바와 같다. 도 5의 (c)는 (i) 256k에 해당하는 RoPE 재조정 인자를 사용하여 LLaMA2-7B에서 직접 미세 조정하고 (ii) 256k에 대한 RoPE 재조정 인자를 사용하여 LLaMA2-7B에서 미세 조정하지만 텍스트 길이는 128k로 절단한다. 손실\n' +
      '\n' +
      '도 5: (ab): 확장된 컨텍스트 윈도우 크기를 갖는 LLaMA2-7B 및 Mistral-7B에서의 손실 곡선. (c) 상이한 미세 조정 설정 하에서 256k 컨텍스트 윈도우를 갖는 미세 조정 LLaMA2-7B의 트레이닝 손실.\n' +
      '\n' +
      '곡선이 그림 1에 표시됩니다. 5(c). 256k 컨텍스트 창으로 모델을 미세 조정하기 위해 128k 텍스트 길이를 사용하면 초기 손실이 급격히 증가한다는 것을 관찰한다. 256k를 달성하기 위해 LLaMA2-7B에서 직접 미세 조정하면 손실이 상대적으로 느리게 감소한다. 표 12는 세 가지 다른 설정의 체크포인트에 대한 Proof-Pile의 테스트 복잡성을 보여준다. 이것은 128k 체크포인트에서 미세 조정을 하는 현재 접근법이 가장 효과적임을 나타낸다.\n' +
      '\n' +
      '** 미세 조정 비용.** LLaMA2-128k는 8개의 A100 GPU를 일주일 동안 사용하여 400단계를 미세 조정합니다. LLaMA2-256k는 600단계를 미세 조정하기 위해 2주 동안 16개의 A100 GPU로 리소스를 두 배로 늘립니다. 훈련 길이가 16k인 미스트랄-128k 및 256k의 경우 2일의 미세 조정 기간 동안 4개의 A100 GPU를 사용한다.\n' +
      '\n' +
      '### 검색에 대한 추가 세부 정보\n' +
      '\n' +
      '검색 효율성** 도. 도 6은 각각의 진화 검색 반복에서 검증 샘플들에 대한 당혹함을 예시한다. 우리는 우리의 탐색 알고리즘이 고품질의 불균일한 RoPE 재스케일 요소를 효율적으로 찾을 수 있음을 알 수 있다. 구체적으로, 256k 컨텍스트 윈도우 검색 상에서(도). 6(a)), 첫 번째 반복 후에 PI 및 YaRN보다 훨씬 더 나은 솔루션을 찾을 수 있다. 더 많은 반복을 탐색함에 따라 118.47에서 273.27로 검증 복잡도를 크게 줄일 수 있으며, 64\\(\\times\\) 확장에서 기존의 최첨단 비균일 보간법인 YaRN이 PI(linear interpolation)보다 더 나쁜 성능을 보임을 알 수 있다. 이는 또한 인간-휴리스틱 기반 불균일 보간법이 모든 시나리오에서 잘 수행되기 어렵다는 것을 나타낸다.\n' +
      '\n' +
      '2048k에서 매우 긴 컨텍스트 윈도우의 경우, 16\\(\\times\\) 및 8\\(\\times\\) 확장을 위해 각각 미세 조정된 128k 및 256k 컨텍스트 윈도우의 LLaMA2-7B를 사용한다. 도 1에 도시된 바와 같다. 6(bc)는 예상대로 16(\\times\\) 연장의 당혹감이 8(\\times\\) 연장의 당혹감보다 크다. 추가로, 2048k에서의 단일 당혹성 평가에 필요한 시간이 약 50분이기 때문에, 검색 반복들이 제한된다. 더 많은 검색 시간이 허용되면 더 나은 결과를 검색할 수 있습니다.\n' +
      '\n' +
      '검색 비용입니다. 검색 비용은 주로 주어진 컨텍스트 윈도우 크기에서 입력 컨텍스트의 복잡성을 평가하는 데 필요한 시간에 의존한다. 최대 256k까지의 컨텍스트 윈도우 길이에 대해, 총 검색 시간은 단일 A100 GPU를 사용하여 3일 이내에 달성 가능한 비교적 빠르다. 512k 컨텍스트 창의 경우 2개의 A100 GPU를 사용합니다. 1024k와 2048k의 더 큰 컨텍스트 창의 경우 각각 4개와 8개의 A100 GPU를 사용하여 총 검색 시간을 5일 이내로 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Method & \\multicolumn{5}{c}{Evaluation Context Length} \\\\ (fine-tune \\(L^{\\prime}\\), \\(L^{\\prime}\\), base LLM) & 32768 & 65536 & 98304 & 131072 & 262144 \\\\ \\hline (128k, 256k, LLaMA2-7B) & 9.75 & 6.56 & 5.15 & 5.19 & 2.21 \\\\ (256k, 256k, LLaMA2-7B) & 4.51 & 2.87 & 2.53 & 2.39 & 1.95 \\\\ (128k, 256k, LLaMA2-7B (ft=128k) & **2.66** & **2.38** & **2.28** & **2.26** & **1.87** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 상이한 미세 조정 설정을 통한 확장된 LLaMA2-7B의 증명 파일 당혹성. 세 값의 튜플은 미세 조정 텍스트 길이, 컨텍스트 창 크기 및 초기 체크포인트를 나타냅니다.\n' +
      '\n' +
      '그림 6: 각 진화 검색 반복에서 검증 샘플에 대한 복잡성. (a) 256k 컨텍스트 윈도우 크기에 도달하기 위한 LLaMA2-7B의 64\\(\\times\\) 확장. (b) LLaMA2-7B-256k에 대한 8\\(\\times\\) 확장이 2048k 컨텍스트 윈도우 크기에 도달한다. (c) LLaMA2-7B-128k에 대한 16\\(\\times\\) 확장이 2048k 컨텍스트 윈도우 크기에 도달한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>