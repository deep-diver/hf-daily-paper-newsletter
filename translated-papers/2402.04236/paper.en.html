<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations\n' +
      '\n' +
      ' Ji Qi\\({}^{1\\,\\ddagger}\\) Ming Ding\\({}^{2}\\) Weihan Wang\\({}^{1\\,\\ddagger}\\) Yushi Bai\\({}^{1\\,\\ddagger}\\) Qingsong Lv\\({}^{2}\\) Wenyi Hong\\({}^{1\\,\\ddagger}\\)\n' +
      '\n' +
      'Bin Xu\\({}^{1}\\) Lei Hou\\({}^{1}\\) Juanzi Li\\({}^{1}\\) Yuxiao Dong \\({}^{1}\\) Jie Tang\n' +
      '\n' +
      'qi20@mails.tsinghua.edu.cn, ming.ding@zhipuai.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose **Chain of Manipulations**, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (_e.g., grounding_) acquired through prior training or from imitating human-like behaviors (_e.g., zoom in_). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train **CogCoM**, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at this url.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Benefiting from the advantage of Large Language Models (LLMs) in broad world knowledge, large Vision Language Models (VLMs) (Alayrac et al., 2022; Wang et al., 2023b) that are further trained to understand vision have demonstrated visibilities on broad scenarios, such as visual question answering (Liu et al., 2023b), visual grounding (Peng et al., 2023), optical character recognition (Zhang et al., 2023b).\n' +
      '\n' +
      'The research employing VLMs as foundation models (Bai et al., 2023; Sun et al., 2023b; Wang et al., 2023b) usually involves two main stages of training, where the first stage cultivates intrinsic visual understanding through exposure to massive image-caption pairs, and the second stage endows the models with problem-solving capabilities through an instruction tuning. Some other studies (Dai et al., 2023; Chen et al., 2023b; Zhang et al., 2023b) directly perform the second stage for the applicable scenes.\n' +
      '\n' +
      'However, existing tuning methods train models to respond to instructions with conclusive linguistic answers upon visual inputs, which leads models to ignore the essential visual reasoning and further results in failures in meticulous visual problems, unfaithful responses, and even hallucinations. For example in Figure 1, we test the top performing model CogVLM (Wang et al., 2023b) about the details in the image (_i.e., texts written on pillar_), and it directly gives an incorrect answer (_i.e., NO SMOKING_), most likely from bias to visual or linguistic priors (_i.e., typical scenes with pillar in office_). The absence of this evidential reasoning with visual evidence leads to a rash response (Hwang et al., 2023).\n' +
      '\n' +
      'Humans solve the meticulous visual problems by marking or processing the given images for convenience and rigor,\n' +
      '\n' +
      'Figure 1: In comparison with existing vision-language models, CogCoM performs the multiple steps of evidential reasoning with chain of manipulations (CoM) to achieve the final answer.\n' +
      '\n' +
      'which we refer to as manipulations. For example, we may find targets by sequentially locating references, and concentrate on subtle details by zooming in a corresponding region. Most of the VLMs have developed numerous intrinsic multi-modal capabilities (_e.g.,_ grounding boxes, recognizing texts) during the first stage of training. By further imitating the foundational human-like behaviours (_e.g.,_ cropping, zoom in), models have the potential to perform evidential visual reasoning for solving problems.\n' +
      '\n' +
      'There are two major challenges that prevent us from training models with this mechanism. First, it is crucial to effectively produce abundant training data with the evidential visual reasoning paths from existing linguistic instruction-answer pairs. Second, it is difficult to build a general mechanism with various manipulations, to train VLMs of dedicated architectures while preserving their preset capabilities.\n' +
      '\n' +
      'In this paper, we propose **Chain of Manipulations (CoM)**, a general mechanism that enables VLMs to perform evidential visual reasoning with a series of manipulations, where each manipulation is applied to the visual input to acquire different types of visual contents (_e.g.,_ boxes, texts, images). We first construct an automatic data production framework based on existing image-question-answer corpus. A linguistic annotator1 who is granted to utilize a set of manipulations is engaged to provide reasoning steps for a given question, and fundamental visual tools are further employed to acquire the corresponding returns requested by the manipulations. A traversal on the tree branched by possible returns of the manipulations is then performed, to obtain the final feasible paths that lead to the correct answer by the last manipulation returns. Based on the produced data, we present **CogCoM**, a 17B VLM trained with a memory-based compatible architecture and a fusion of four categories of data, to develop the general and reasoning multimodal capabilities. The model performs reasoning by actively adopting multiple manipulations to acquire visual contents (_e.g.,_ referential regions \\(bbx_{1},bbx_{2}\\), and new image \\(img_{1}\\)), and finally achieves the conclusive answer. In addition, due to the lack of evaluation resources, we further introduce a testbed with meticulous visual problems involving reasoning paths, and a keypoints-aware metric to investigate the correctness of the final answer as well as the solving process.\n' +
      '\n' +
      'Footnote 1: A LLM prepended with task requirements is used as the annotator, who provides solving steps without knowing answers.\n' +
      '\n' +
      'We conduct extensive experiments on 8 benchmarks from 3 categories of capabilities, including detailed visual question answering with TextVQA (Singh et al., 2019), STVQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA (Hudson and Manning, 2019), visual grounding with RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al., 2016), and RefCOCOg (Mao et al., 2016), and the hallucination validation with POPE (Li et al., 2023d), and also a proposed reasoning examination benchmark AutoCoM-test. The results show that our model achieves the superior or competitive performance across the board. And the investigation on the proposed testbed shows that CogCoM swiftly achieves competitive performance with a few training steps by incorporating the produced reasoning chains.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      'In this section, we introduce the methodology of CogCoM. We first introduce the terminology definition related to CoM in Section 2.1. In Section 2.2, we illustrate the data production process (Figure 2 left), which involves a linguistic annotator to provide manipulations-assisted solving steps, the visual annotators to supply the manipulations returns, and the final traversal to gather viable paths. In Section 2.3, we illustrate model training (Figure 2 right), where we conduct a compatible devising to train a general multi-turns VLM with chain of manipulations.\n' +
      '\n' +
      '### Terminology\n' +
      '\n' +
      'We first introduce the formal definition of the terminologies and data structure for an unambiguous understanding. We define the manipulations as a flexible set that includes functions from a foundational predefined set, as well as functions devised by models themselves during inference to accommodate in-context learning. We thus predefine the a set of **manipulations functions** that VLMs can develop, either from prior training or by imitating human behaviors: \\(\\mathcal{M}\\subseteq\\){\\(\\textit{Grounding}(tgt)\\to bbx\\), \\(\\textit{OCR}(tgt)\\totxt\\), \\(\\textit{Calculate}(tgt)\\to num\\), \\(\\textit{Counting}(tgt)\\to num\\), \\(\\textit{CropZoomIn}(bbx,x)\\to img\\)}, where the parameters or returns \\(tgt,bbx,txt,num,x,img\\) refer to the bounding boxes, zoom ratio, image, target description, number, and text, respectively.\n' +
      '\n' +
      'Given a language question \\(Q\\) upon an initial input image \\(I_{0}\\), a general vision-language model equipped with Chain of Manipulations (CoM) \\(\\varsigma\\) solves the problem to achieve the corresponding answer as \\(\\textit{VLM}(\\varsigma|I_{0},Q)\\Rightarrow A\\), where \\(\\varsigma\\) refers to **a chain of evidential reasoning steps2**,\n' +
      '\n' +
      'Footnote 2: We use the symbol \\(\\varsigma\\) (_i.e., sigma_) to denote the CoM chain throughout this paper for convenience.\n' +
      '\n' +
      '\\[\\begin{split}\\varsigma&=(step_{1},step_{2},...)\\\\ step_{i}&=(f_{i},desc_{i}),\\hskip 14.226378ptf_{i} \\in\\mathcal{M}\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'where \\(f_{i}\\) refers to a instantiated manipulation function from a set of manipulations definition \\(\\mathcal{M}\\), and \\(desc_{i}\\) refers to a language description including the execution of the manipulation. This definition explicitly declares the symbolic execution of the manipulations \\(f_{i}\\), and also compatible with the existing free-form instruction-answer data structure with the linguistic descriptions \\(desc_{i}\\).\n' +
      '\n' +
      '### Data Production\n' +
      '\n' +
      'Based on the definition of data structure, we introduce a data production framework, that could efficiently synthesize CoM data upon image-oriented question-answer pairs, and is also capable of producing high-quality annotations by replacing the linguistic and visual annotators with human labour. We describe the detailed data production approach with the implementation for automatic data synthesis in this paper.\n' +
      '\n' +
      '#### 2.2.1 Data Generation\n' +
      '\n' +
      'Given a general corpus \\(\\mathcal{D}=\\{(I,Q,A)\\}\\) consisting of triplet samples of images and corresponding visual question-answer pairs, our automatic data synthesis framework consists of a linguistic annotator and several visual annotators according to the manipulations. For a question \\(Q\\) in each sample, we first engage the linguistic annotator to generate manipulations-assisted solving steps with the CoM format (\\(f_{i},desc_{i}\\)), where the corresponding returns of the instantiated manipulations executions are set with variables as placeholders. In this paper, we adopt GPT4 (OpenAI, 2023), a large language model with reliable language understanding and generation abilities as the linguistic annotator. We design a comprehensive prompt including the task requirements, usage of manipulations, and output data format, and further manually annotate 5 demonstrations for a stable generation. The detailed implementations are available at Appendix B.3.\n' +
      '\n' +
      'We then employ essential visual annotators to supply the returns of manipulations requested in the solving steps through exactly performing the corresponding manipulations. By empirically analyzing the manipulations from both predefined set and newly created ones (refers to Appendix B.2 for a detailed statistics), we reveal the _grounding_ and _OCR_ are two fundamental manipulations, and most of the others can be consequently derived (_e.g., CropZoomIn_ along a region of box, _Counting_ upon recognized boxes, and _Calculate_ for the identified formula). Therefore, we employ two reliable visual tools, GroundingDINO (Liu et al., 2023) and PaddleOCR (Du et al., 2020), and develop the implementations of these manipulations.\n' +
      '\n' +
      'We execute the manipulations along with the reasoning steps, which turns the reasoning steps into a **tree**\\(\\mathcal{T}\\), as the input of current manipulation \\(f_{1}(x_{1})\\) may rely on one of the multiple returns of previous manipulation \\(f_{2}\\to x_{2}\\), i.e., \\(x_{1}\\) rely on \\(x_{2}\\) (_e.g.,_ step 2 for finding pillars in Figure 2). The designed prompt, and a corresponding generation with linguistic and visual results are available at Appendix figure 6.\n' +
      '\n' +
      'Figure 2: **The automatic data synthesis framework (left)**: a linguistic annotator (LLM) taught with task requirements and usage of manipulations (prompt) is first asked to provide solving steps for a question \\(\\mathcal{Q}\\), and the visual annotators (Tools) are then engaged to replace the manipulations returns, followed by a final traversal on the tree branched by the possible returns is performed to find feasible paths terminating answer \\(\\mathcal{A}\\). **A compatible VLM architecture (right)**: a historic memory representation is maintained throughout around of multi-turns training, where \\(t\\)-th turn is evoked by an production of new image \\(\\mathcal{I}_{t}\\).\n' +
      '\n' +
      '#### 2.2.2 Data Processing\n' +
      '\n' +
      'The tree \\(\\mathcal{T}\\), that is rooted from the question \\(Q\\) and branched by the manipulations returns may contain negative paths that can not lead to the correct answer \\(A\\), in which the nodes may come from error annotations or irrelevant references. We then perform a traversal on each produced tree with the Depth First Search (DFS) to find all positive paths \\(\\{\\mathcal{P}_{i}|\\mathcal{P}_{i}\\in\\mathcal{T},i=1,2,...\\}\\), where each path \\(\\mathcal{P}_{i}\\) is terminated with the final answer \\(A\\) as the return of the last manipulation.\n' +
      '\n' +
      'The operation of _zoom in_ refers to an intellective human behaviour, which could not be generated from the automatic annotators appropriately and requires a cognitive decision according to the visual scene. We leverage a simple but effective strategy to compensate the _CropZoomIn_ manipulation. During the traversal on tree \\(\\mathcal{T}\\), at the node of manipulation \\(f(x(bbx))\\) that involves a box \\(bbx\\) in the input \\(x\\), we first calculate the ratio \\(r\\) of the current image\'s area to the box\'s area, and then insert the manipulation \\(CropZoomIn(bbx,n)\\) in current step with the satisfiability condition (\\(n\\) is determined according to \\(r\\), _e.g.,_\\(n=2\\) if \\(r\\geq 36\\)). Therefore, the final resultant chains \\(\\{\\mathbf{\\varsigma_{i}}|\\mathbf{\\varsigma_{i}}:=\\mathcal{P}_{i},i=1,2,...\\}\\) are obtained by assigning the corresponding values of reasoning steps with the positive paths. The overall data-producing algorithm is presented in Appendix B.1.\n' +
      '\n' +
      'To adapt to the training of images-based multi-turns VLMs, we can easily convert a steps-based chain to an **images-based chain** by partitioning steps according to the manipulations returns of images and merge the corresponding descriptions (_e.g.,_ based on the images of \\(I_{0},I_{1}\\), the chain is converted into \\(\\mathbf{\\varsigma}\\rightarrow(A_{0},A_{1})\\) in Figure 2),\n' +
      '\n' +
      '\\[\\mathbf{\\varsigma}\\rightarrow[(I_{0},Q),(I_{1},A_{1}),(I_{2},A_{2}),...] \\tag{2}\\]\n' +
      '\n' +
      'where \\(I_{t}\\) is the manipulation return from the \\(t-1\\) step, and \\(A_{t}\\) is the sequential concatenation of elements in \\((desc_{i(t-1)},..,desc_{i(t)}]\\) with the index mapping from \\(t\\) to \\(i\\).\n' +
      '\n' +
      'We implement this proposed data production framework on \\(3\\) existing datasets that require meticulous recognition and object counting, TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM chains. The detailed statistics of the data generation are available at Appendix B.2.\n' +
      '\n' +
      '### Model Training\n' +
      '\n' +
      '#### 2.3.1 Architecture\n' +
      '\n' +
      'We use the same model architecture as CogVLM (Wang et al., 2023), a general VLM approach that involves four fundamental components: (1) a Vision Encoder, (2) an MLP Adapter, (3) an LLM Backbone, and (4) a Visual Expert Module, for a reliable multimodal understanding. Concretely, the pre-trained EVA2-CLIP-E (Sun et al., 2023) with 4B parameters and Vicuna-7B-v1.5 (Chiang et al., 2023) are adopted as the vision encoder and LLM backbone, respectively. A two-layer MLP (SwiGLU (Shazeer, 2020)) is further engaged to map the output of the vision encoder into the linguistic space of the LLM backbone. The visual expert module adds the vision-specific weights into the attention layer and feed-forward layer of each block in the LLM backbone, resulting in a total of 6.5B additional parameters for the deep fusion of modalities.\n' +
      '\n' +
      'Based on this general architecture, we develop a memory-based multi-turns multi-images VLM approach. Specifically, for a round of images-based multi-turns sample \\([(I_{t},Q_{t},A_{t})|t=1,2,...]\\), we keep the accumulated KV memories of each layer of the LLM backbone through this around. And at each turn \\(t\\) for training and inference, we calculate the attention function \\(att\\) as:\n' +
      '\n' +
      '\\[\\begin{split} att(\\mathbf{X})&=softmax(\\frac{\\mathbf{Q}_{t} \\mathbf{K}_{t}^{\\prime T}}{\\sqrt{d}})\\mathbf{V}_{t}^{\\prime}\\\\ \\mathbf{K}_{t}^{\\prime}&=\\text{trunc}(\\text{concat}( \\mathbf{K}_{0},\\mathbf{K}_{1},...,\\mathbf{K}_{t}))\\\\ \\mathbf{V}_{t}^{\\prime}&=\\text{trunc}(\\text{concat}( \\mathbf{V}_{0},\\mathbf{V}_{1},...,\\mathbf{V}_{t}))\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathbf{Q}_{t}\\in\\mathbb{R}^{s\\times d}\\) is query representation of current layer, and the \\(\\mathbf{K}_{t}^{\\prime}\\), \\(\\mathbf{V}_{t}^{\\prime}\\in\\mathbb{R}^{(s\\times t)\\times d}\\) refer to the concatenation of accumulated representations and will be further truncated if the sequence length \\(s\\times t\\) greater than a predefined threshold. At the turn \\(t>0\\), the question \\(Q_{t}\\) is sampled from a set of hard prompts (asking model to focus on history), and the image \\(I_{t}\\) is cropped from \\(I_{t-1}\\) and is amplified with the Bicubic Interpolation (Keys, 1981).\n' +
      '\n' +
      '#### 2.3.2 Training\n' +
      '\n' +
      'The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of general multimodal task-solving as well as the evidential visual reasoning.\n' +
      '\n' +
      'First Stage of Pre-TrainingThis stage consists of two ordinal sub-phases of training for foundational visual understanding and grounded generation. Following the pre-training of CogVLM (Wang et al., 2023), we first train model on 1.5B image-text pairs cleaned from the LAION-2B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) with 120,000 iterations and batch size of 8,192. We then train model on 40M grounded image-question-answer triples cleaned from LAION-115M (Li et al., 2023) with 60,000 iterations and batch size of 1,024, where each noun phrase in the answer is followed by a list of coordinates \\([[x_{0},y_{0},x_{1},y_{1}],...]\\) referring the phrase to the grounded objects in the image. Both phases adopt the next token prediction objective, and train the 6.5B parameters of visual experts.\n' +
      '\n' +
      'Second Stage of AlignmentThis stage further trains the model to align with human preferences on solving practical visual problems. We fuse the produced CoM data with 3 types of corpus, including MultiInstruct (Xu et al., 2022), LLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen et al., 2023c), referring the abilities of instruction-following, texts-recognizing, and detailed-captioning. This fusion results in a total of 570K \\((I,Q,A)\\) samples, where the answer \\(A\\) in CoM data consists of multiple turns. For the training data of CoM, we randomly prepend the questions with a lunching prompt4\\(P^{\\mathcal{M}}\\) to questions \\(Q=P^{\\mathcal{M}}+Q\\) asking models to optionally use manipulations for the adaption of explicit eliciting. We empirically show that the model can effectively learn the evidential visual reasoning by ingesting this portion of CoM data. We train 14,000 iterations with a batch size of 160, where the learning rate reaches \\(10^{-5}\\) after 280 steps of warm-up and then decays linearly. The parameters of 6.5B visual experts are trained with the objective of next token prediction. These two stages of training result in our standard version of CogCoM involving both chat and reasoning capabilities. More training details are available at Appendix C.2.\n' +
      '\n' +
      'Footnote 4: See Appendix C.1 for examples.\n' +
      '\n' +
      '## 3 Experiment\n' +
      '\n' +
      'To quantitatively validate the suitability and efficiency of the proposed method, we conduct experiments on 9 benchmarks corresponding to 3 categories of multimodal capabilities, and a newly proposed testbed incorporating the evidential reasoning paths with a keypoints-aware metric. Following previous works, we train two generalist versions of CogCoM for adapting to the different scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version with a qualitative analysis (Hwang et al., 2023).\n' +
      '\n' +
      '* **Detailed Visual Question Answering.** This task involves models to perform detailed reasoning or recognition on images. We use 4 prominent benchmarks including, GQA (Hudson and Manning, 2019), TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TallyVQA (Acharya et al., 2019).\n' +
      '* **Visual Grounding.** Visual grounding evaluates the crucial abilities of VLMs on meticulous position understanding. We evaluate our model on 3 standard benchmarks, RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016).\n' +
      '* **Hallucination Examination.** We also evaluate our model on a specific benchmark POPE (Li et al., 2023d) that explores the crucial issue of visual objects hallucination, to investigate the helpfulness of incorporating evidential visual reasoning with model training.\n' +
      '\n' +
      '### Experiments on Detailed VQA\n' +
      '\n' +
      'VLMs have demonstrated the well-known superiority in visual scenes with salient content understanding. We evaluate the effectiveness of CogCoM on VQAs on meticulous understanding, which typically require models to perform multiple actions (_find, read_) or multiple reasoning steps (_recognizing and then calculating_). Following previous studies (Wang et al., 2023b), we train our model obtained from the first-phase of stage-1 on a fusion of data, including an instruction corpus of MultiInstruct, 13 publicly available VQA datasets (only using training set), a newly created VQA dataset built through promoting GPT4-V (OpenAI, 2023b) for image-oriented question-answer generation, and the CoM corpus. This training results in a generalist VQA model incorporating CoM reasoning. For all existing VQA tasks, we directly prompt CogCoM with given questions and examine the correctness of outputted answers.\n' +
      '\n' +
      '#### 3.1.1 Gqa, TextVQA, ST-VQA, TallyVQA\n' +
      '\n' +
      'SettingsThe task of visual question answering aims to evaluate the correctness of answer generated by a model by given a question towards an image. GQA is a compositional VQA benchmark with diverse reasoning questions coming from semantic functional programs. TallyVQA is an objects counting benchmark with human-annotated complex counting questions involving challenging non-zero counterparts. TextVQA and ST-VQA are two texts understanding benchmarks requiring models to answer questions through textual cues on images. We use the official evaluation scripts for GQA and TallyVQA, which calculate the accuracy score by the Exact Matching (EM) between model predictions and answers. For TextVQA and ST-VQA, we submit our model predictions to the official online websites for calculating the accuracy with VQA Score metric (Antol et al., 2015).\n' +
      '\n' +
      'ResultsThe experimental results are shown in Table 2. CogCoM achieves the state-of-the-art performance in comparison with all generalist models across the board, and significantly surpass the baseline of the previous best model on multiple benchmarks. On the datasets requiring complex reasoning and detailed texts recognition, CogCoM obtains the results that are comparable to the specialist SOTAs.\n' +
      '\n' +
      'Specifically, our model improves by 5.97 accuracy score compared to the baseline model CogVLM on the GQA benchmark, suggesting that the training incorporated with the evidential visual reasoning chains effectively enhance the general reasoning ability of models. On the counting benchmark of TallyVQA, our model outperforms the baseline by 4.2 and 2.1 in accuracy score with simple and complex questions, respectively. This result demonstrates the effectiveness of the rigorous grounded learning on the scenarios requiring specific objects count calculations. On\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '### Experiments on Visual Grounding\n' +
      '\n' +
      'The task of visual grounding requires models to precisely provide the corresponding coordinates of regions in an image based on the given target expression, which is a crucial aspect of detailed position understanding of VLMs and is attracted extensive attention. Following the existing work (Wang et al., 2023), we train our model obtained by the first stage on a fusion of datasets, including an instruction corpus MultiInstruct, a high-quality grounding VQA corpus introduce in CogVLM, and the proposed CoM data. This training results in a generalist grounding model that is excelling at visual grounding while capable of reasoning. For all benchmarks, we prompt CogOM in a chat manner to ask the model to provide grounded coordinates, such as "_Where is \\(\\langle expr\\rangle\\) answer in [x0,y0,x1,y1] format._", where the \\(\\langle expr\\rangle\\) refers to the target expression.\n' +
      '\n' +
      'SettingsWe evaluate CogCoM on three well-established visual grounding benchmarks. We use the standard evaluation metric, that considers a prediction as correct when the intersection-over-union (IoU) between the predicted bounding box and ground truth is greater than 0.5.\n' +
      '\n' +
      'ResultsThe results are shown in Figure 2. CogCoM achieves the best performance in 6 out of all 8 sub-sets. Given that our model is trained on a fusion of the instruction following, visual grounding, and CoM corpus to gain broad applicability, this result indicates that our model exhibits a superior grounding abilities while offers potential to solve a variety of tasks. In addition, CogCoM achieves performance on par with the specialist SOTAs, demonstrating that incorporating evidential reasoning with grounding manipulations into the general training can enhance the multimodal capabilities while preserving the mastery of grounding.\n' +
      '\n' +
      '### Experiments on Hallucination Examination\n' +
      '\n' +
      'To further investigate the helpfulness of incorporating the evidential visual reasoning on alleviating multimodal hallucinations, we further evaluate CogCoM on POPE, a typical benchmark that evaluate the objects hallucination issue faced by VLMs. We use our generalist VQA model for this evaluation, and obtain model predictions by directly asking the corresponding questions in the original dataset.\n' +
      '\n' +
      'SettingsPOPE aims to investigate the objects hallucination of VLMs by asking the existence of objects in an image with binary questions (_i.e., yes/no_). We use the challenging adversarial version of the dataset for standard evaluation. For the calculation of scores, we use the official evaluation scripts to calculate the \\(F_{1}\\) score of predictions.\n' +
      '\n' +
      'ResultsThe experimental results on the POPE benchmark are shown in Table 3. We can see that CogCoM achieves the superior performance in comparison with previous VLMs on the benchmark, demonstrating the improvements. Specifically, CogCoM improves the performance and achieves the superior result on POP in comparison with baseline and the previous VLMs. As we train CogCOM with a fusion of diversified capabilities, this results suggest that our model performs well on general multimodal tasks with a lower susceptibility to hallucination, while preserving VQA, instruction following, and reasoning abilities.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Type**} & \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{**RefCOCO**} & \\multicolumn{3}{c}{**RefCOCO+**} & \\multicolumn{3}{c}{**RefCOCOg**} \\\\ \\cline{3-10}  & & val & test-A & test-B & val & test-A & test-B & val & test \\\\ \\hline \\multirow{8}{*}{Generalist} & OFA-L* (Wang et al., 2022) & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58 \\\\  & Shikra-7B (Chen et al., 2023) & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 \\\\ \\cline{1-1}  & Shikra-13B (Chen et al., 2023) & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 \\\\ \\cline{1-1}  & Qwen-VL (Bai et al., 2023) & 89.36 & 92.26 & 85.34 & 83.12 & 88.25 & 77.21 & 85.58 & 85.48 \\\\ \\cline{1-1}  & CogVLM (Wang et al., 2023) & **92.51** & 93.95 & 88.73 & 87.52 & 91.81 & 81.43 & **89.46** & 90.09 \\\\ \\cline{1-1}  & **CogCoM** & 92.34 & **94.57** & **89.15** & **88.19** & **92.80** & **82.08** & 89.32 & **90.45** \\\\ \\hline Specialist & & 92.64 & 94.33 & 91.46 & 88.77 & 92.21 & 83.23 & 89.22 & 89.37 \\\\ \\cline{1-1}  & SOTAs & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) & (SORTAS) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Results on Visual Grounding benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Method** & **POPE** \\\\ \\hline BLIP-2 (Li et al., 2023c) & - \\\\ Otter (Li et al., 2023a) & - \\\\ MiniGPT4 (Zhu et al., 2023) & 70.4 \\\\ InstructBLIP (Dai et al., 2023) & 77.3 \\\\ LLAVA (Liu et al., 2023b) & 66.3 \\\\ LLaMA-Adapter v2 (Gao et al., 2023) & - \\\\ DreamLLM (Dong et al., 2023) & 76.5 \\\\ LLAVA-1.5 (Liu et al., 2023a) & 84.5 \\\\ Emu (Sun et al., 2023b) & - \\\\ CogVLM & 87.2 \\\\\n' +
      '**CogCoM** & **87.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Evaluation on integrated and hallucination assessment (adversarial subset). We use the original questions as the input prompts for evaluation.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      'We investigate the evidential reasoning capability of CogCoM on scenarios that requires different types of meticulous reasoning, including recognizing textual details, reading time, understanding charts and counting objects. The results are shown in Figure 4. The first case demonstrates that CogCoM finds the region corresponding to the plane logo through two steps of grounding and then achieves the answer based on zooming in the cropped region. The second case illustrates the ability of CogCoM in reading time, by locating the device that displays time and then transforming the time into words based on the read_time manipulation. In the forth example, CogCoM first identifies all visible truck wheels through grounding, and then leverages the counting manipulation to consolidate the total number.\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      'This paper study the problems presented by the conclusive-alignment training of Vision-Language Models (VLMs), and propose a general mechanism, Chain of Manipulations (CoM), that enables VLMs to solve problems by actively manipulating visual inputs to acquire essential contents. We realize this methodology by building an efficient data production framework that engages linguistic and visual annotators to synthesis CoM chains, and a memory-based architecture which is compatible with existing models. A 17B VLM, **CogCoM** with the memory-based architecture is trained on the data fusion incorporating the implemented CoM chains. Experiments with quantitative results on 8 benchmarks and qualitative analysis demonstrate the effectiveness of the method in solving detailed visual problems.\n' +
      '\n' +
      'Figure 4: CogCoM performs reasoning with details recognition, reading time, understanding charts, counting objects, and reading texts.\n' +
      '\n' +
      '## 5 Limitations\n' +
      '\n' +
      'Though we try to develop an accurate and robust framework that engages remarkable LLM to provide basic solving steps, adopts reliable visual tools to obtain visual contents, and then acquires feasible paths based on traversal, there are still limitations in our methodology that we hope to improve in the future. First, We find that the diversity of linguistic solving steps is insufficient, and the inaccuracy of visual tools (_e.g.,_ the rough granularity of grounding boxes, OCR failures on slant letters) will lead to a large amount of negative paths (effectively utilizing these paths would beneficial). We suggest to promote these limitations with dedicate prompts and improved visual tools. Second, our current model re-input the manipulated images with a set of hard prompts, which may bring speed losses. This is expected to be improved by implementing the physical manipulations into the calculations in vector space.\n' +
      '\n' +
      '## 6 Impacts\n' +
      '\n' +
      'This work presents a general visual reasoning mechanism that alleviate the problems caused by existing conclusion-alignment training for VLMs, introduces a data production framework involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM architecture. We expect this work to bring three benefits to the community. First, the proposed visual reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second, the introduced data production framework may be applied to widespread training scenarios to promote the development of current data-driven machine learning. Third, we hope that the memory-based architecture will be helpful for VLMs in multi-turn long contexts.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Acharya et al. (2019) Acharya, M., Kafle, K., and Kanan, C. Tallyqa: Answering complex counting questions. In _Proceedings of the AAAI conference on artificial intelligence_, 2019.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* Antol et al. (2015) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, 2015.\n' +
      '* Awadalla et al. (2023) Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Sagawa, S., et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint_, 2023.\n' +
      '* Bai et al. (2023) Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint_, 2023.\n' +
      '* Biten et al. (2019) Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. Scene text visual question answering. In _Proceedings of the IEEE/CVF international conference on computer vision_, 2019.\n' +
      '* Byeon et al. (2022) Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-text pair dataset, 2022.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.\n' +
      '* Chen et al. (2023a) Chen, D., Liu, J., Dai, W., and Wang, B. Visual instruction tuning with polite flamingo. _arXiv preprint arXiv:2307.01003_, 2023a.\n' +
      '* Chen et al. (2023b) Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint_, 2023b.\n' +
      '* Chen et al. (2022) Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023c.\n' +
      '* Chen et al. (2022) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Chen et al. (2023d) Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C. R., Goodman, S., Wang, X., Tay, Y., et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint_, 2023d.\n' +
      '* Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. _See https://vicuna. Imsys. org (accessed 14 April 2023)_, 2023.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv preprint arXiv:2305.06500.\n' +
      '\n' +
      'Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv preprint, Cited by: SS1, SS2.\n' +
      '* R. Dong, C. Han, Y. Peng, Z. Qi, Z., Yang, L. Zhao, J. Sun, H., et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499. Cited by: SS1.\n' +
      '* Y. Du, C. Li, R. Guo, X. Yin, W. Liu, J. Zhou, Y. Bai, Z. Yu, Y. Yang, Q. Dang, et al. Pp-ocr: A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941. Cited by: SS1.\n' +
      '* P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint, Cited by: SS1, SS2.\n' +
      '* W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914. Cited by: SS1.\n' +
      '* K. Huang, M. Zhou, H. P. Chan, Y. R. Fung, Z. Wang, L. Zhang, S. Chang, and H. Ji, (2023) Do lvlms understand charts? analyzing and correcting factual errors in chart captioning. arXiv preprint arXiv:2312.10160. Cited by: SS1.\n' +
      '* S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint, Cited by: SS1, SS2.\n' +
      '* D. A. Hudson and C. D. Manning (2019)GQA: a new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, Cited by: SS1.\n' +
      '* A. Hwang, A. Head, and C. Callison-Burch (2023)Grounded intuition of gpt-vision\'s abilities with scientific images. arXiv preprint. Cited by: SS1.\n' +
      '* C. Jia, Y. Yang, Y. Xia, Y. Chen, Z. Parekh, H. Pham, Q. Le, Y. Sung, Z. Li, and T. Duerig (2021)Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, Cited by: SS1.\n' +
      '* R. Keys (1981)Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing. Cited by: SS1.\n' +
      '* R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Chen, S. Kalantidis, L. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision. Cited by: SS1.\n' +
      '* B. Li, Y. Zhang, L. Chen, J. Wang, F. Fu, J. Li, and Z. Liu (2023)MIMIC-it: multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425. Cited by: SS1.\n' +
      '* Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, J. Wen, and J. R. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355. Cited by: SS1.\n' +
      '* Y. Li, C. Zhang, G. Yu, Z. Wang, B. Fu, G. Lin, C. Shen, L. Chen, and Y. Wei (2023)Stablellava: enhanced visual instruction tuning with synthesized image-dialogue data. arXiv preprint arXiv:2308.10253. Cited by: SS1.\n' +
      '* T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick (2014)Microsoft coco: common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, Cited by: SS1.\n' +
      '* H. Liu, C. Li, Y. Li, and Y. J. Lee (2023)Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. Cited by: SS1.\n' +
      '* H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023)Visual instruction tuning. arXiv preprint. Cited by: SS1.\n' +
      '* S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu, et al. Grounding dino: marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499. Cited by: SS1.\n' +
      '* J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy (2016)Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, Cited by: SS1.\n' +
      '* R. OpenAI (2023a)Gpt-4 technical report. arxiv 2303.08774. Cited by: SS1.\n' +
      '* R. OpenAI (2023b)Gpt-4v(sision) system card. Citekey: gptvision. Cited by: SS1.\n' +
      '* V. Ordonez, G. Kulkarni, and T. Berg (2011)Im2text: describing images using 1 million captioned photographs. Advances in neural information processing systems. Cited by: SS1.\n' +
      '* K. Papineni, S. Roukos, T. Ward, and W. Zhu (2002)Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, Cited by: SS1.\n' +
      '\n' +
      'Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint_, 2023.\n' +
      '* [15] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [16] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [17] Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2018.\n' +
      '* [18] Shazeer, N. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [19] Shrestha, R., Kafle, K., and Kanan, C. Answer them all! toward universal visual question answering models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019.\n' +
      '* [20] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019.\n' +
      '* [21] Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Evaclip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023a.\n' +
      '* [22] Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Generative pretraining in multimodality. _arXiv preprint_, 2023b.\n' +
      '* [23] Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and Wang, L. Git: A generative image-to-text transformer for vision and language. _arXiv preprint arXiv:2205.14100_, 2022a.\n' +
      '* [24] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. 2022b.\n' +
      '* [25] Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. _arXiv preprint_, 2023a.\n' +
      '* [26] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual expert for pretrained language models. _arXiv preprint_, 2023b.\n' +
      '* [27] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022c.\n' +
      '* [28] Wu, P. and Xie, S. V*: Guided visual search as a core mechanism in multimodal llms. _arXiv preprint arXiv:2312.14135_, 2023.\n' +
      '* [29] Xu, Z., Shen, Y., and Huang, L. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. _arXiv preprint arXiv:2212.10773_, 2022.\n' +
      '* [30] Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and Chen, E. Woodpecker: Hallucination correction for multimodal large language models. _arXiv preprint arXiv:2310.16045_, 2023.\n' +
      '* [31] Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14_, 2016.\n' +
      '* [32] Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., and Zhuang, Y. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. _arXiv preprint arXiv:2311.13614_, 2023.\n' +
      '* [33] Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y., Zhang, Y., and Kong, T. What matters in training a gpt4-style language model with multimodal inputs? _arXiv preprint arXiv:2307.02469_, 2023.\n' +
      '* [34] Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., and Luo, P. Gpt4roi: Instruction tuning large language model on region-of-interest. _arXiv preprint arXiv:2307.03601_, 2023a.\n' +
      '* [35] Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint_, 2023b.\n' +
      '* [36] Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint_, 2023.\n' +
      '\n' +
      '## Appendix A Related Works\n' +
      '\n' +
      '### Large Vision-Language Models as Foundations\n' +
      '\n' +
      'Most of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia et al., 2021), MSCOCO (Lin et al., 2014), VG (Krishna et al., 2017), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), LAION2B (Schuhmann et al., 2022), LAION400M (Schuhmann et al., 2021). Starting from Flamingo (Alayrac et al., 2022), a series of LVLMs have focused on training the adaptation layers to align the visual representation to the frozen LLMs on a mixture of image-text pairs with the above corpus, including BLIP2 (Li et al., 2023c), KOSMOS (Huang et al., 2023b), and OpenFlamingo (Awadalla et al., 2023). Inspired by success of instruction tuning in LLMs (Wang et al., 2022c), a line of works have devoted efforts to build vision-oriented instruction-answer pairs through GPT4 and train models for imitation, such as LLAVA (Liu et al., 2023b), Otter (Li et al., 2023b), VisionLLM (Wang et al., 2023a), MultiInstruct (Xu et al., 2022), Lynx (Zeng et al., 2023), InstructBLIP (Dai et al.), CleverFlamingo (Chen et al., 2023a) and StableLLaVA (Li et al., 2023e). Recently, researchers have proven the efficiency of developing LVLMs with two stages of training, the first stage of abundant pretraining on image-caption pairs and the second stage of alignment on image-question-answer triples, such as PALI (Chen et al., 2022), PaLI-X (Chen et al., 2023d), Qwen-VL (Bai et al., 2023), and CoSVLM (Wang et al., 2023b).\n' +
      '\n' +
      '### Large Vision-Language Models with Reasoning\n' +
      '\n' +
      'To further enhance the ability of LVLMs in solving high-level visual problems, research focusing on various aspects of reasoning is attracting broad attention. We simply divide existing studies into tree broad categories. The first line of research focus on enhance train models with a mastery of cross-modal grounded reasoning, where grounded instruction-following supervision is build through public visual grounding dataset or GPT4-V for training, including KOSMOS-2 (Peng et al., 2023), Shikra (Chen et al., 2023b), and GPT4ROI (Zhang et al., 2023a). The second aspect of efforts have been devoted into promoting models to understand artificial visual scenes, such as figures, charts, and receipts. These studies includes CogAgent (Hong et al., 2023) and CHARTVE (Huang et al., 2023a). Some other studies address the crucial problem of hallucination in LVLMs with counterfactual or interpretable reasoning (Yu et al., 2023; Yin et al., 2023). V* (Wu & Xie, 2023) also contributes efforts to enhance the details recognition of VLMs based the LLM-guided searching process.\n' +
      '\n' +
      'B Details of Data Production\n' +
      '\n' +
      'In this section, we further introduce the details of CoM data production, with the overall algorithm of a pseudo code, an example of the solving steps generation with LLM and corresponding guideline, an example of the reasoning chains completion with visual tools. We also list the details of data statistics for the synthesised training data as well as the evaluation data of AutoCoM-test, followed by a limitation analysis for the current data production method.\n' +
      '\n' +
      '### The Algorithm for A Reproducible Data Production\n' +
      '\n' +
      'We provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data generation, thereby facilitating understanding and reproduction 1.\n' +
      '\n' +
      '```\n' +
      '1:Define:\\(\\begin{cases}Manipulations:\\{f_{i}:x\\to y\\,|\\,f_{i}\\in\\mathcal{M}\\}\\\\ LinguisticAnnotator:\\Psi_{L}&//We\\,use\\,GPT4\\,in\\,this\\,work\\\\ Visual\\,Annotator:\\Psi_{V}&//We\\,use\\,PaddleOCR\\,and\\,GroundingDINO\\,in\\,this\\,work\\\\ \\end{cases}\\)\n' +
      '2:Input: Image \\(I\\), Question \\(Q\\), Answer \\(A\\)\n' +
      '3:// LinguisticAnnotation\n' +
      '4:Prompt \\(\\Psi_{L}\\) with guideline \\(P^{L}\\) to generate reasoning steps: \\[\\varsigma=\\Psi_{L}(Q|P^{L}),\\quad where\\begin{cases}\\varsigma=(steps_{1},steps_{2 },...)\\\\ steps_{i}=(f_{i},desc_{i})\\end{cases}\\] (4)\n' +
      '5:Define tree \\(\\mathcal{T}\\)\n' +
      '6:for\\(i=1\\)to\\(|\\varsigma|\\)do\n' +
      '7: Extract \\(x_{i},y_{i}\\) instantiated with \\(f_{i}\\) in \\(step_{i}\\)\n' +
      '8: Extract referential boxes \\(B\\) from \\(x_{i}\\)\n' +
      '9:for\\(b\\) in \\(B\\)do\n' +
      '10: Leverage \\(\\Psi_{V}\\) to acquire corresponding visual content \\(y^{\\prime}_{i}=\\Psi(x_{i}|I,b)\\), and apply \\(y_{i}\\) to tree \\[\\mathcal{T}.level[i].append(y_{i})\\] (5)\n' +
      '11:endfor\n' +
      '12:endfor\n' +
      '13:Traverse \\(\\mathcal{T}\\) to obtain positive chains that leads to given answer with terminal return \\[[\\varsigma_{1},\\varsigma_{2},...]=DFS(\\mathcal{T}|A)\\] (6)\n' +
      '14:Return \\([\\varsigma_{1},\\varsigma_{2},...]\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Synthesising Chain of Manipulations\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '\\begin{tabular}{l l l} \\hline \\hline _INPUTS_ & & **Question:** What number police station is on the building? \\\\ \\hline \\multirow{4}{*}{\\(OUTPUTS_{k}\\)} & \\multirow{4}{*}{Step 1: grounding_1(the building)\\(\\cdot\\)bbx_1,} & \\multirow{4}{*}{\n' +
      '\\begin{tabular}{l} Locate the building in the image and return the bounding box of the building as \'bbx_1\'. \\\\ identify the number of the police station on the building in box \'bbx_1\' and return the bounding box of the number as \'bbx_2\'. \\\\ \\end{tabular} } \\\\  & & **Step 3:** (OCR_1(number in region \'bbx_2\') \\\\  & & **->ot_1,** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\end{tabular}\n' +
      '_CaMi_:\n' +
      '\n' +
      'Given a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optimally use one of the following manipulations (defined as Name(input) soReturn) on the image: GROUNDING(a phrase)-shooses, OCR(an image or a region)-stexts, CROP, AND, ZOOMINNa region on given image)-snew_image, CALCULATE(a compatible target)-numbers, or a new manipulation created by yourself (if it is indeed needed).\n' +
      '\n' +
      '_Question:_ What is written on the traffic sign near the bus?\n' +
      '\n' +
      '_Model:_ Using GROUNDING(the traffic sign near the bus) to locate the traffic sign near the bus in current image, which is [_[_1084,157,926,703;67,146,740,273_]_]. Utilizing OCR(texts in region [_1084,157,926,703_]_) to read the texts written on the traffic sign within region [_[_1084,157,926,703_]_], resulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi.\n' +
      '\n' +
      '_Visual Grounding:_\n' +
      '\n' +
      '_Question:_ Find the region in image that "a traffic sign" describes.\n' +
      '\n' +
      '_Model:_ [[678,145,739,270]\n' +
      '\n' +
      '_Grounded Captioning:_\n' +
      '\n' +
      '_Question:_ The goal of this task is to generate description for one part of the image. The part is specified by [_[_678,145,739,270_]_].\n' +
      '\n' +
      '_Model:_ [[678,145,739,270]].\n' +
      '\n' +
      '_Image Captioning:_\n' +
      '\n' +
      '_Question:_ In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.\n' +
      '\n' +
      '_Answer:_ Taxi bus with rainbow colors on side.\n' +
      '\n' +
      '_Multi Choice:_\n' +
      '\n' +
      '_Question:_ Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer.\n' +
      '\n' +
      '_Answer:_ black\n' +
      '\n' +
      '_Detailed Captioning:_\n' +
      '\n' +
      '_Question:_ Explain the visual content of the image in great detail.\n' +
      '\n' +
      '_Answer:_ The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text "Trafalgar" in bold red letters across its side, indicating it\'s likely part of a tourist or travel company. The bus is positioned next to a traffic sign that reads "TAXI", suggesting it might be waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall impression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life.\n' +
      '\n' +
      'Figure 7: CogCoM demonstrates the flexible capabilities for adapting to different multimodal scenarios, including evidential visual reasoning, Visual Grounding, Grounded Captioning, Image Captioning, Multi Choice, and Detailed Captioning.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>