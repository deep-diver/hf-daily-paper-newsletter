<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs\n' +
      '\n' +
      ' Victor Carbune\n' +
      '\n' +
      '**Correspondence to: vcarbune@google.com**\n' +
      '\n' +
      '**Hassan Mansoor**  **Fangyu Liu**  **Rahul Aralikatte**\n' +
      '\n' +
      '**Gilles Baechler**  **Jindong Chen**  **Abhanshu Sharma**\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Correspondence to: vcarbune@google.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by Chen et al. (2023), while also enabling much better performance on PlotQA and FigureQA.\n' +
      '\n' +
      'We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by Liu et al. (2023). We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by Hsieh et al. (2023).\n' +
      '\n' +
      'Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt (Chen et al., 2023), our model outperforms the recently introduced Gemini Ultra and GPT-4V.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Visual language, where text and images work together to deliver information, can be expressed through charts, plots, and diagrams. Multimodal reasoning within this context is challenging, as it involves linking visual properties (like color, line style, and positioning) with textual content (such as legends and units).\n' +
      '\n' +
      'Many recent advances of vision-language models (VLMs) come from techniques enabling better representations (Dosovitskiy et al., 2021; Lee et al., 2023), giving the model the ability to understand core elements of the image, a necessary building block for basic reasoning. However, complex reasoning capabilities which combine the core representation of the image with semantic understanding of a question to provide an answer, have been rather limited. Models oftentimes are not able to contextually combine image and text representations. One technique that improves reasoning capabilities in large-language models (LLMs) includes in-context learning for eliciting reasoning such as chain-of-thought prompting (Wei et al., 2023), decomposing tasks (Zhou et al., 2023) or composing stored facts in weights (Press et al., 2023). Fine-tuning on datasets with rationales (Magister et al., 2023; Hsieh et al., 2023) has been shown to be effective for smaller models. In this work, we tackle improving reasoning capabilities in VLMs through better learned image representations, followed by fine-tuning on synthetic datasets with reasoning traces generated by more capable LLMs. We also explore a hybrid online setup for numerical reasoning refinements.\n' +
      '\n' +
      'We empirically show that this indeed improves performance through experiments on ChartQA (Masry et al., 2022). Visual-question answering on charts quantifies the ability of a VLM to reason using complex information presented. Oftentimes answering the question requires implicit or explicit information extraction, followed by intermediate grouping or computations using the extracted information, and reasoning with the final quantities, as shown in Figure 1.\n' +
      '\n' +
      'Vision-language models (VLMs) such as PaLI-X and PaLI-3 are hybrid model architectures which use a vision and a language backbone to solve visual tasks (Chen et al., 2023, ). The training recipe typically involves a pre-training stage focused on learning a good internal representation, followed by a downstream fine-tuning stage. Chen et al. (2023c) note that PaLI-3 falls behind PaLI-X on ChartQA likely due to its limited reasoning capabilities. Results presented in this work suggest that the lack of a pre-training task for learning better chart representations, as done in Liu et al. (2023b), may be another reason.\n' +
      '\n' +
      'Enhancing the reasoning capabilities of large language models (LLMs) such as PaLM-2 (Anil et al., 2023) or GPT-4 (OpenAI, 2023) is a very active research area. While reasoning is considered an emerging property with scale (Wei et al., 2022), Press et al. (2023) argue that simply scaling only enables better memorization of knowledge and does not enable composing multiple stored facts into an answer. On the other hand, prompting techniques enacting complex reasoning on downstream tasks have been shown to be very effective (Wei et al., 2023) (Zhou et al., 2023).\n' +
      '\n' +
      'Transferring reasoning capabilities from large to small models enables reducing serving costs, while increasing task performance. Hsieh et al. (2023) have introduced an effective multi-task framework which enable small models to outperform their much larger counterparts using less data. They do so by leveraging rationale generation as a separate task, instead of more standard distillation approaches, which first infer the rationale, followed by the answer (Magister et al., 2023). We apply this framework for the first time on multimodal tasks.\n' +
      '\n' +
      'ContributionsOur main results can be summarized as follows: **(i)** we introduce an efficient recipe consisting of a pre-training task and fine-tuning task with synthetic datasets using a multi-task setup for improving reasoning capabilities, **(ii)** we obtain SoTA performance by significantly improving PaLI-3 performance on the ChartQA benchmark with our recipe and using 10x less parameters than prior work, **(iii)** we perform numerous ablation experiments quantifying the impact of the techniques used in our recipe.\n' +
      '\n' +
      'The remainder of this paper is structured as follows. Section 2 describes related work, followed by Section 3 which introduces the construction of the training datasets. Section 4 illustrates our novel pre-training and fine-tuning recipe, followed by Section 5 describing the experimental setup and main results. Lastly, Section 8 delivers a conclusion and recommendation for future work, followed by Section 9 where we acknowledge limitations of the current work.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'VLM landscapeVision-language models usually combine a vision backbone with a language backbone. Frequently it is a Vision Transformer (ViT) (Dosovitskiy et al., 2021) coupled with a Large Language Model via an encoder-decoder (Chen et al., 2023b) or decoder-only (Alayrac et al., 2022) architecture. More recently, models such as Fuyu-8B (Bavishi et al., 2023) explore projecting the image directly through the language backbone. In this work we extend PaLI-3, an encoder-decoder architecture with ViT-3B as vision and UL2-2B as language backbones. We refer the reader to Chen et al. (2023c) for a complete overview. PaLI-3 is a SoTA model and hence we decided to build on top of it to further focus on improving the results with our methods.\n' +
      '\n' +
      'Existing approaches for chart understandingThe task of answering questions on charts is, alongside documents and infographics, part of a broader set of tasks commonly referred to _visually-situated language understanding_, where text and image cannot be treated separately (Lee et al., 2023). Fine-tuned models on downstream ChartQA include PaLI-3 (Chen et al., 2023c), MatCha (Liu et al., 2023b) and UniChart (Masry et al., 2023). Among these, UniChart takes the most similar approach to ours, pre-training a chart image encoder as vision backbone and BART decoder (Lewis et al., 2019) as language backbone. Alternatively, Liu et al. (2023a) took the approach of decomposing question-answering into first translating the chart\n' +
      '\n' +
      'Figure 1: Example from the ChartQA validation set.\n' +
      '\n' +
      'into a table, then querying an LLM in a plug-and-play fashion. Here our main focus is on fine-tuned self-contained models, however we show that a simple refinement using a much larger LLM, continues to improve performance as well.\n' +
      '\n' +
      'The role of upstream OCR systemsA chart usually has an underlying equivalent tabular representation of the data. However, decoding the tabular representation remains a challenging problem. Alternatively, charts can be passed through an OCR system to extract an unstructured text representation of the image. Luo et al. (2021) combine chart-specific extraction logic with an OCR system to extract key information from the charts. As intuitively expected, usually the use of an OCR system improves downstream quality. In this work, we assume the model only has access to the chart image.\n' +
      '\n' +
      'Improving chart reasoning with synthetic dataHaving the pre-training mixture specialize on chart tasks is effective Liu et al. (2023). We further extend the _chart deredering_ task, which translates charts to code or to table. Similar to our approach, Methani et al. (2020) and Masry et al. (2023) have made use of programmatic templates to a synthesize complex QA pairs. However, instead of using an LLM to generate chart summaries as in Masry et al. (2023), here we use it to generate additional QA pairs with rationales. These generated examples together with synthetic programmatic examples are key in the pre-training and fine-tune stages of our model.\n' +
      '\n' +
      '## 3 Dataset\n' +
      '\n' +
      '### Brief description of ChartQA\n' +
      '\n' +
      'ChartQA is one of the widely adopted visual question-answering benchmarks for reasoning capabilities of VLMs.\n' +
      '\n' +
      'The standard ChartQA benchmark has two components: (a) human set and (b) augmented generated set. The augmented set has been machine generated and is more simplistic in nature than the human set.\n' +
      '\n' +
      'The charts in the dataset come from four sources Statista, Pew, Our World in Data and OECD). Gold tables are available for all sources, except for Pew, where the tables are inferred with ChartOCR model Luo et al. (2021). Although we observed mistakes in inferred tables, our method seems to be fairly resilient to them.\n' +
      '\n' +
      '### Synthetic Generation Methods\n' +
      '\n' +
      'In this work, we use LLMs to synthesize additional examples paired with rationales generated using chain-of-thought prompting. We use the tabular representation of charts present in the training set as a way to mediate the lack of vision input into LLMs.\n' +
      '\n' +
      'The data we synthesize increases the diversity of the original training set, especially with examples that require extracting multiple quantities from the chart and perform reasoning using them.\n' +
      '\n' +
      'We combine two approaches that focus on this type of examples, specifically we use a LLM for synthesizing _rationale generation_ and _extra question answer_ pairs. We also use a programmatic approach for generating _arithmetic_ question answer pairs.\n' +
      '\n' +
      'Rationale GenerationWe augment the original training set with synthetic explanations on why an answer is reached. We achieve this by using PaLM 2-S to predict a **rationale** on an input tuple of (\\(\\mathbf{table}\\), \\(\\mathbf{question}\\), \\(\\mathbf{answer}\\)) with a 4-shot prompt, as illustrated in Figure 4. We refer to this set as _ChartQA-Rationale-S_.\n' +
      '\n' +
      'By requesting the model to provide justifications for ground truth answers, which are typically accurate, we witness a significant reduction in hallucinations. A notable exception is when the answer itself is wrong, which happens more frequently for the ChartQA augmented set than the human set. However, we did not perform a detailed investigation of this aspect in the generated training sets. An instance of the generated rationale can be seen in Figure 2.\n' +
      '\n' +
      'ExtraQA GenerationWe hypothesize that the original training set is too small to contain enough diversity in the examples to enable solving more complex QA questions such as the ones present in the human validation set. Therefore we used a 1-shot prompt illustrated in Figure 5 to generate additional examples covering types of errors we identify by examining the model performance on the validation set. The prompt is adapted from the one used in Liu et al. (2023). An example of a generated sample can be seen in Figure 7. We used both PaLM 2-S and PaLM 2-L to generate the examples and refer to the respective datasets as _ChartQA-ExtraQA-S/L_. We perform only lightweight filtering of generated examples that deviate from the imposed structure. If we cannot parse from the LLMresponse all three elements, we simply drop the example. However, we do not verify the generated examples for hallucinations, fluency or perform any other model-based verification.\n' +
      '\n' +
      'ArithmeticQA GenerationIt is well known that large language models have difficulties in performing arithmetic computations accurately. For ChartQA, this is particularly exacerbated by the fact that the small training dataset is adequate for the specifics of the arithmetic questions one can have for charts (as represented by the test set). We programmatically create examples which either require numeric reasoning or a comparative analysis of multiple chart elements. Examples are illustrated in Figure 8 and Figure 9. We abstracted the questions into templates and used a fixed set of mathematical operations such as median, max, min etc. For each template we created a rationale to teach the model a plan to solve the arithmetic problems. For example, computing the mean requires first looking up the values, then adding them up and finally dividing the value by the total. For each type of arithmetic we created multiple templates both for the questions and rationales. The source data we used are only the ChartQA human examples, using the available tables. The type of questions and their count can be found in Table 1.\n' +
      '\n' +
      '### Resulting Dataset\n' +
      '\n' +
      'The resulting dataset is roughly 20x larger and is described in Table 2, with further details on the statistics of the dataset in Section D. Sampling was done using greedy decoding with temperature \\(\\tau=0\\). We used the augmented and human sets to generate examples.\n' +
      '\n' +
      'PaLM 2-S vs. 2-LThe same prompt was used for all examples in the synthetic dataset. We note that using samples from both LLMs improves performance, but ablation studies do not indicate one is better than the other. We hypothesize that diversity matters more than model size, but we have not investigated sampling strategies.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      'Our work builds on top of PaLI-3 architecture and pre-training recipe, which consists of two backbones, a Vision Transformer ViT-2B and Text Encoder-Decoder UL2-3B. Our starting point is the recipe described by Chen et al. (2023c). The uni-modal pre-training stage trains the vision encoder using contrastive loss through the SigLIP loss, while the language encoder-decoder is pre-trained using the UL2 loss. Both backbones are pre-trained jointly using a multi-modal stage. Lastly the resolution increase stage enables the vision encoder backbone to work with 812x812 resolution images. We continue pre-training using this checkpoint.\n' +
      '\n' +
      '### Pre-training: Chart2Table Mixture\n' +
      '\n' +
      'Extending the work done by Liu et al. (2023a), we use a chart-to-table dataset mixture to continue pre-training with the ViT backbone unfrozen, which facilitates learning an internal representation of the chart. We do not explicitly use the tabular conversion further downstream.\n' +
      '\n' +
      'DatasetFor learning this representation, we combine several chart-to-table deredering tasks into a mixture: (1) synthetic chart-to-table data similar to the synthetic mixture introduced by Liu et al.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r} \\hline \\hline\n' +
      '**Question Type** & **Count \\#** \\\\ \\hline Mean & 235K \\\\ Subtraction & 90K \\\\ Other & 32K \\\\ \\hline Total & 357K \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Examples are mostly means or subtractions.\n' +
      '\n' +
      'Figure 2: _ChartQA-Rationale-S_: For each example of the original training set, we synthesize a rational based on the table, the question and the answer.\n' +
      '\n' +
      '(2023a). We traverse different combinations of plotting options in matplotlib and seaborn to randomly plot tables from Wikipedia into charts of different layouts. (2) the chart-to-table mixture introduced by Masry et al. (2023). (3) The chart-table pairs from the train set of DVQA (Kafle et al., 2018). (4) The chart-table pairs from the train set of TaTA (Gehrmann et al., 2022). (5) The chart-table pairs introduced in Benetech - Making Chart Accessible Kaggle challenge1. A complete listing of data source, sampling weight, and number of examples is shown in Table 3.\n' +
      '\n' +
      'Footnote 1: [https://www.kaggle.com/competitions/benetech-making-graphs-accessible](https://www.kaggle.com/competitions/benetech-making-graphs-accessible)\n' +
      '\n' +
      'The existing table representation is used as is from the datasets, or, as described earlier, for a small fraction, tables are created programmatically. Tables are also normalized to a standardized format.\n' +
      '\n' +
      '### Fine-tuning: Multi-task Loss\n' +
      '\n' +
      'After the pre-training stage which enables the ViT backbone to work better with charts, we use the synthetic data to fine-tune the model for the downstream task. We investigate two ways of incorporating the rationales available in the extended dataset.\n' +
      '\n' +
      'The first one is by changing the task target from _answer_ to _rationale, answer_. This has been shown to be effective in (Magister et al., 2023). We refer to this approach as **single-task setup**. However, it requires increased inference time by predicting the rationale, together with increased sequence length during training. The unintended side effect of training to predict jointly rationales and answers is that rationale tokens become equally important as the answer tokens.\n' +
      '\n' +
      'The second one is inspired by Hsieh et al. (2023) which addresses both concerns by constructing a **multi-task setup** where the answer and rationale are treated as independent tasks. This can be done using different prefixes similar to T5 (Raffel et al., 2023), such as _"Rationale:"_ and _"Question:"_. The training loss balances the strength between the two tasks using a hyper-parameter \\(\\lambda\\):\n' +
      '\n' +
      '\\[\\mathbf{Loss}=(\\mathbf{1}-\\lambda)\\mathbf{Loss_{ans}}+\\lambda\\mathbf{Loss_{rat}}\\]\n' +
      '\n' +
      'Our experiments are the first application of this setup for a multimodal task. We further confirm the observation from text domains that not only inference time remains constant, but quality also improves.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'We describe the general learning hyper-parameters for the pre-training and fine-tuning stages, followed by interpretation of the results.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      'Pre-trainingWe continue pre-training the PaLI-3 model with ViT unfrozen on the Chart2Table data mixture for train_steps=6K, batch_size=256 with learning_rate=5e-3 with normalized square root decay using decay_factor=2e-6 and dropout_rate=0.1.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline\n' +
      '**Dataset** & **Hum** \\# & **Aug** \\# & **Question type** \\# & **Total** & **Rate** \\# \\\\ \\hline ChartQA-Rationale-S & 7398 & 20901 & R [13\\%], V [11\\%], C [43\\%], B [33\\%] & 28.3K & 15\\% \\\\ ChartQA-ExtraQAR-S & 23261 & 69433 & R [57\\%], C [43\\%] & 92.7K & 15\\% \\\\ ChartQA-ExtraQAR-L & 16388 & 50468 & R [60\\%], C [40\\%] & 66.9K & 30\\% \\\\ ChartQA-ArithmQAR & 357000 & - & C [100\\%] & 357.0K & 40\\% \\\\ \\hline ChartQA-Synth (Total) & & & & **544.9K** & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Overview of the synthetic dataset, which is 20x larger than the original one. The suffix denotes the size of the PaLM 2 model used. The rate refers to the final mixture. Categorization of question types are from (Masry et al., 2022), namely **R**etrieval, **V**isual, **C**ompositional or **B**oth visual and compositional.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline\n' +
      '**Component** & **Rate** & **Size** \\\\ \\hline Synthetic & 44.0\\% & 1.2M \\\\ UniChart & 39.5\\% & 612K \\\\ DVQA & 3.2\\% & 200K \\\\ ChartQA & 3.2\\% & 22K \\\\ TaTa & 3.2\\% & 6.7K \\\\ Chart2Text & 3.2\\% & 24K \\\\ Benetech Challenge & 3.2\\% & 21K \\\\ PlotQA & 0.5\\% & 224K \\\\ \\hline Total & & **2.37M** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Pre-training datasets for learning chart representations include examples from numerous tasks that have paired chart images with table representations.\n' +
      '\n' +
      'Fine-tuningWe then freeze the ViT encoder and continue fine-tuning on the synthetic ChartQA dataset for train_steps=5K, batch_size=256 with learning_rate=1e-3 with linear decay using decay_factor=1e-4 using dropout_rate=0.1.\n' +
      '\n' +
      'MultitaskWe use \\(\\lambda=\\mathbf{0.5}\\) and we do not find significant differences when using other values.\n' +
      '\n' +
      '### Results on ChartQA\n' +
      '\n' +
      'We validate the effectiveness of the different techniques by reporting the downstream task performance on the ChartQA test set. All following experiments are on PaLI-3.\n' +
      '\n' +
      'Pre-trainingContinuing the pre-training stage for the PaLI-3 model using the Chart2Table mixture enables learning a better general representation of the charts. We intuitively expect that this better representation enables the model to more accurately identify quantities on the images. Indeed, we confirm this first through the results reported in Table 4. Later, as we scale the dataset size, we show that this continues to play an important role.\n' +
      '\n' +
      'As expected, the increase is predominantly in the augmented set, given that the pre-training mixture is constructed synthetically as well.\n' +
      '\n' +
      'Singletask vs. MultitaskWe first study the effect of introducing rationales only using the _ChartQA-Rationale-S_. This only adds rationales to the original ChartQA dataset.\n' +
      '\n' +
      'When using the rationales in singletask setup the performance difference is not significant compared to not using them. However, when used in the multitask setup, we note a quality improvement, particularly noticeable in the more difficult human-set. We refer to the former as _Singletask-Rationale_ and to the latter as _Multitask-Rationale_ in Table 5.\n' +
      '\n' +
      'We hypothesize that the improvement comes from better use of the rationales, guiding the model to internally produce a form of reasoning before producing the final answer. This is done without paying the cost predicting the rationales tokens.\n' +
      '\n' +
      'Learning with augmented datasetWe use the ChartQA-Synth dataset from Table 2 for studying the extent to which we can transfer reasoning capabilities from PaLM-2 to PaLI-3.\n' +
      '\n' +
      'We perform an ablation experiment to understand the role of the extra questions, rationales and pre-training stage and report our results in Table 6.\n' +
      '\n' +
      'We denote experiments using the original pre-trained checkpoint as _Orig PT_ and on the further pre-trained checkpoint with chart-to-table translation as _C2T_. We report a clear improvement, further strengthening our observation that internal representation plays an important role.\n' +
      '\n' +
      'We ran an experiment without rationales, but with the entire synthetically generated QA pairs. We note that the increase in examples ends up improving over the original ChartQA performance reported in Table 4. However, the use of rationales continues to improve quality for both singletask and multitask setups. We observe that in high-data regimes, there is no longer a significant difference between the two.\n' +
      '\n' +
      'Given the neutral impact of the multi-task setup at inference time, paired with slightly improved\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Fine-tuning Setup**} & \\multicolumn{3}{c}{ChartQA (RA\\%)} \\\\ \\cline{2-4}  & **Avg.** & **Hum.** & **Aug.** \\\\ \\hline Orig PT + Singletask-ExtraQAR & 72.43 & 53.20 & 91.67 \\\\ Orig PT + Multitask-ExtraQAR & 73.15 & 55.20 & 91.10 \\\\ \\hline C2T PT + ExtraQA (w/o Rationale) & 74.67 & 56.39 & 92.96 \\\\ \\hline C2T PT + Singletask-ExtraQAR & 75.16 & 55.84 & **94.48** \\\\ C2T PT + Multitask-ExtraQAR & 75.36 & 56.80 & 93.92 \\\\ \\hline C2T PT + Singletask-ChartQA-Synth & 76.60 & 59.04 & 94.16 \\\\ C2T PT + Multitask-ChartQA-Synth & **77.28** & **60.88** & 93.68 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Ablation results confirm the importance of each step in our recipe. _ChartQA-Synth_ is the mixture described in Table 2\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Pre-training Strategy**} & \\multicolumn{3}{c}{ChartQA (RA\\%)} \\\\ \\cline{2-4}  & **Avg.** & **Hum.** & **Aug.** \\\\ \\hline Original PT (Chen et al., 2023c) & 70.00 & - & - \\\\ Chart2Table PT (our run) & **70.84** & 48.96 & 92.72 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: PaLI-3 performance on ChartQA slightly increases with our chart-to-table pre-training phase.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Fine-tuning setup**} & \\multicolumn{3}{c}{ChartQA (RA\\%)} \\\\ \\cline{2-4}  & **Avg.** & **Hum.** & **Aug.** \\\\ \\hline C2T PT + Singletask-Rationale & 70.80 & 49.36 & 92.24 \\\\ C2T PT + Multitask-Rationale & **71.72** & 50.72 & 92.72 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Multitask performance stands out compared to Singletask on the more difficult human-written set.\n' +
      '\n' +
      'performance on the human-written queries of ChartQA, multi-task is the preferred option in practice. Further, we refer to the best performing fine-tuned setup in Table 6 as **ChartPaLI-5B**.\n' +
      '\n' +
      '### Results on FigureQA and PlotQA\n' +
      '\n' +
      'ChartQA is currently the most challenging benchmark. To prove that our method is general, we investigate performance on related chart understanding tasks, FigureQA () and PlotQA (). We study 3 operation regimes: (i) **zero-shot**: no task-specific pre-training or fine-tuning, (ii) **quick adaptation**: 1K fine-tuning steps and (iii) **convergence**: 5K fine-tuning steps. We report relaxed accuracy on 10K examples from validation set for FigureQA (ref. Table 8 and from test set from PlotQA (ref. Table 9).\n' +
      '\n' +
      'For PlotQA, images from the training subset are present in our pre-training mixture, while validation and test subset images are not. Therefore, we do not study zero-shot performance, as training images would give an unfair advantage.\n' +
      '\n' +
      'ChartPaLI-5B outperforms PaLI-3 in all operation regimes. In general, our recipe significantly increases chart understanding performance when running only a few quick adaptation steps.\n' +
      '\n' +
      'In particular we report SoTA performance regime for FigureQA (roughly 96%+) and the very strong relative performance on the difficult PlotQA v2 (roughly +47.1% at convergence time).\n' +
      '\n' +
      '### Errors and Challenges\n' +
      '\n' +
      'To understand the effect of our method and investigate further opportunities for improvements, we manually looked at predictions on the ChartQA validation set. We compared baseline PaLI-3 model outputs with the model fine-tuned with our recipe and share our observations below. We report our findings below.\n' +
      '\n' +
      'GeneralThe model predicts the rationale2 or the answer, depending on the task prefix. Because the answer is not conditioned on the rationale, it can differ. One general improvement area we note is the ability to extract necessary intermediate quantities (Fig. 11) and operate with them (Fig. 12).\n' +
      '\n' +
      'Footnote 2: Although the table is not used during inference, the rationales contain the word _table_ due to its use in prompts.\n' +
      '\n' +
      'Numerical reasoningDespite improvements, computation of mathematical expressions continues to be very challenging. The rationales correctly extract (Fig. 3) or infer chart values when missing (Fig. 13), however the computed value is frequently incorrect. This does not always prevent the final answer to be correct (Fig. 15). This seems in line with observations by Wang et al. (2023), who also conclude that corruption of the chain-of-thought reasoning trace does not always degrade the final answer. Due to the frequency of this numeric computation error, we explore a simple refining technique in Section 5.5.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{3}{c}{FigureQA RA\\% (v1 1 v2)} \\\\ \\cline{2-4}  & **ZShot** & **Quick** & **Conv** \\\\ \\hline PaLI-3 (original) & 41.9 142.4 & 57.2 158.1 & 89.9 1 89.3 \\\\ ChartPaLI-5B & **51.0** & **51.2** & **92.7** & **93.0** & **96.3** & **96.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: ChartPaLI-3 exhibits strong generalization on FigureQA task, for which no examples are present in pre-training or fine-tuning\n' +
      '\n' +
      'Figure 3: Correct numeric approximations on answers.\n' +
      '\n' +
      'Color reasoningOur synthetic data does not have color metadata, as only the table was used in the generation process. Therefore the model continues to struggle when the reasoning trace requies working with colors (Fig. 10). Thus, this is an area worth of investigating next and has applicability well beyond the specifics of chart understanding.\n' +
      '\n' +
      'Complex reasoningReasoning about multiple values and checking for a matching condition which requires arithmetic computations is another example of a remaining difficult task (Fig.14, Fig.16). The increased complexity stemming from internal inability of VLMs to perform numeric operations paired with enumerating chart elements through semantic descriptions is likely fairly difficult to achieve without the use of external tools.\n' +
      '\n' +
      'Task leakageDue to the training methodology, we observe that when conditioned with the _Question_ task prefix, the model may behave similarly as to when _Rationale_ prefix is used. Sometimes, instead of directly outputting an answer, the model may generate a longer explanation that resembles a rationale or a fragment of rationale.\n' +
      '\n' +
      '### Refinement with Program of Thoughts\n' +
      '\n' +
      'Despite the improved ability to construct numeric equations using the required values on the charts (Fig. 3), the exact numeric computation continues to be wrong. This is unsurprising, since both the visual and the language backbone treat numbers as tokens. Making the problem worse, the character sequence forming a number may be split and encoded in arbitrary chunks. Chen et al. (2023) have proposed replacing chain-of-thoughts (CoT) prompting with program-of-thoughts (PoT) to enable delegation of the arithmetic computation to a program interpreter. This has previously been explored by Liu et al. (2023), however in a much more computationally involved setup than the one we describe further.\n' +
      '\n' +
      'Through our fine-tuning approach, both single-task and multitask setups can be used produce CoT rationales for which an LLM prompted with PoT can write the equivalent code for performing the numeric computation.\n' +
      '\n' +
      'We take the approach of using a simple 4-shot prompt (Fig. 6) constructed on the validation set to generate code using PaLM 2-S for performing the numeric computation that is present in a rationale. We run this online refinement, only if the rationale contains an arithmetic operator (\'+\', \'-\', \'/\' or \'*\').\n' +
      '\n' +
      'Self-consistency is an effective way to improve chain-of-thoughts rationales by selecting an answer with majority voting from a pool of sampled rationales (Wang et al., 2023). We apply this approach, by sampling with temperature \\(\\tau_{Rat}=0.4\\) and generate \\(N=5\\) rationales that are then refined with PaLM 2-S using temperature \\(\\tau_{Ref}=0.0\\).\n' +
      '\n' +
      'The results presented in Table 10 highlight the utility of the method, particularly with K=5 for self-consistency. They also highlight the simplicity of the augmented set compared to the human set,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Fine-tuned VLMs** (up to 55B) & Source & **ChartQA (RA\\%)** \\\\ \\hline Fuyu-8B & our eval, (Bavishi et al., 2023) & 42.1 \\\\ Pix2Struct-1.3B & (Lee et al., 2023) & 58.6 \\\\ MatCha-300M & (Liu et al., 2023) & 64.2 \\\\ UniChart-201M & (Masry et al., 2023) & 66.2 \\\\ ChartLlama-13B & (Han et al., 2023) & 69.6 \\\\ PalL-5B & (Chen et al., 2023) & 70.0 \\\\ PaL1-55B (Soft Mixture of Low-rank Experts) & (Wu et al., 2023) & 73.8 \\\\ ChartPALI-5B & **our work** & **77.3** \\\\ \\hline\n' +
      '**Hybrid VLMs/LLMs** (undisclosed size) & & & \\\\ \\hline GPT-4V [4-shot with CoT] & (OpenAI, 2023) & 78.5 \\\\ DePlot-300M + FlanPaLM + Codex with PoT SC & (Liu et al., 2023) & 79.3 \\\\ Gemini Ultra [0-shot] & (Gemini Team, Google, 2023) & 80.8 \\\\ ChartPALI-5B + PaLM 2-S PoT SC @ 5 & **our work** & **81.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: State-of-the-art performance among fine-tuned VLMs on ChartQA benchmark.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Fine-tuned VLMs** (up to 55B) & Source & **ChartQA (RA\\%)** \\\\ \\hline Fuyu-8B & our eval, (Bavishi et al., 2023) & 42.1 \\\\ Pix2Struct-1.3B & (Lee et al., 2023) & 58.6 \\\\ MatCha-300M & (Liu et al., 2023) & 64.2 \\\\ UniChart-201M & (Masry et al., 2023) & 66.2 \\\\ ChartLlama-13B & (Han et al., 2023) & 69.6 \\\\ PalL-5B & (Chen et al., 2023) & 70.0 \\\\ PaL1-55B (Chen et al., 2023) & 70.9 \\\\ PaL1-55B (Soft Mixture of Low-rank Experts) & (Wu et al., 2023) & 73.8 \\\\ ChartPALI-5B & **our work** & **77.3** \\\\ \\hline\n' +
      '**Hybrid VLMs/LLMs** (undisclosed size) & & & \\\\ \\hline GPT-4V [4-shot with CoT] & (OpenAI, 2023) & 78.5 \\\\ DePlot-300M + FlanPaLM + Codex with PoT SC & (Liu et al., 2023) & 79.3 \\\\ Gemini Ultra [0-shot] & (Gemini Team, Google, 2023) & 80.8 \\\\ ChartPALI-5B + PaLM 2-S PoT SC @ 5 & **our work** & **81.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: PoT refinement improves performance on the human set, while not affecting the augmented set.\n' +
      '\n' +
      'for which the refinement does not have an impact. Either the augmented set contains no arithmetic computations or they are simple enough for the fine-tuned VLM to already get right.\n' +
      '\n' +
      '## 6 Performance Overview\n' +
      '\n' +
      'We position our results relative to existing prior work in Table 7. We extracted the results from the referenced papers, with the exception of the Fuyu-8B [1] model. We performed our own evaluation as the authors have not provided the results on the ChartQA benchmark.\n' +
      '\n' +
      'Our work significantly outperforms prior models specialized on the ChartQA benchmark. Concurrent to our work, ChartLlama-13B also uses synthetic data generated, but with a fairly different approach. Although outside the scope of our work, it may be that the approach took to train the much smaller MatCha and UniChart models may be combinable with the approach we presented in this work, leading to possible improved performance with even less computational resources.\n' +
      '\n' +
      'The method introduced in this work can be uniquely combined with much larger models through rationale generation. As shown in the results, rationales generated by VLMs can suffice for larger LLMs to effectively operate on, providing a text-representation of the chart conditioned on the question. Our method matches the recently introduced Gemini Ultra model and outperforms previous approaches.\n' +
      '\n' +
      '## 7 Future Work\n' +
      '\n' +
      'We highlighted several drawbacks of our approach in Section 5.4. The training mixtures do not have examples where colors are used to construct reasoning examples. Bootstrapping such examples, for example by running a smaller sized model with questions that extract color related information, then combines them, would likely improve quality. Very complex reasoning examples are also limited. Specifically, semantically identifying chart elements and performing numeric computations to solve questions would further improve quality.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      'We introduced a novel recipe that significantly improves the reasoning capabilities of VLMs. Applied to PaLI-3, our method significantly outperforms even the 10x larger PaLI-X on the ChartQA benchmark, establishing a new state-of-the-art. We demonstrate how the pre-training stage improves downstream performance. Our synthetic data generation technique coupled with the use of a multi-task setup, successfully transfers reasoning capabilities from larger LLMs to smaller VLMs. Moreover, our method enables a computationally more expensive setup where predicted rationales are refined using program-of-thoughts with PaLM 2-S. The composite solution outperforms Gemini Ultra and GPT-4V on the ChartQA benchmark.\n' +
      '\n' +
      '## 9 Limitations\n' +
      '\n' +
      'We acknowledge limitations of our approach.\n' +
      '\n' +
      'Table representationAlthough our final model works on pixels only, our synthetic data generation method requires having access to a table version of the charts for leveraging LLMs to construct rationales, additional question/answer pairs, etc for the training datasets. Although it is likely that inferred tables or output of an OCR model may replace to some degree the presence of gold tables, it will likely affect final model quality.\n' +
      '\n' +
      'PaLI-3The pre-training and fine-tuning recipe for synthetic data creation, as well as the training methodology should be applicable broadly on open source models as well. However, we acknowledge that the choice of PaLI-3, a proprietary flavor of VLMs, is not as a good of a choice as an open source flavor available externally.\n' +
      '\n' +
      'Risks associated with synthetic datasetSince the method for constructing our dataset relies on LLMs, there are certain inherent risks that come with that, for example that of hallucination. Although our technique extends the publicly available ChartQA dataset, additional care needs to be taken into account when planning to apply it for releasing models or dataset openly. Although the metrics are state-of-the-art, it cannot be guaranteed that model outputs can\'t be abused if trained in this manner.\n' +
      '\n' +
      'Reasoning limitationsWe acknowledge limitations stemming from the empirical prompt creation process, which is based on human inspection of model errors. LLM capabilities used for the synthetic data creation, although impressive, continue to have numerous limitations as reported by the community.\n' +
      '\n' +
      'Acknowledgements\n' +
      '\n' +
      'We thank Srinivas Sunkara and Maria Wang for their contributions on the infrastructure that enabled us to run these experiments. Further, we thank Xi Chen for his tireless support and insights into PaLI-3 details and training recipes and Cheng-Yu Hsieh and Yasuhisa Fujii for the detailed discussions on the multi-task setup. Daniel Keysers and Radu Soricut have provided detailed feedbacks that significantly improved the paper. Matt Sharifi and Ewa Dominowska provided senior leadership support for this work.\n' +
      '\n' +
      'Lastly, feedback from anonymous reviewers rPKR and 453J motivated running additional experiments further strengthening the contribution of this work by showcasing the method is generally applicable.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, and Eliza Rutherford et al. 2022. Flamingo: a visual language model for few-shot learning.\n' +
      '* Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, and Laurent El Shafey et al. 2023. PaLM 2 Technical Report.\n' +
      '* Bavishi et al. (2023) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasrlar. 2023. Introducing our multimodal models.\n' +
      '* Chen et al. (2023a) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023a. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.\n' +
      '* Chen et al. (2023b) Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, and Siamak Shakeri et al. 2023b. Pali-x: On scaling up a multilingual vision and language model.\n' +
      '* Chen et al. (2023c) Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, and Piotr Padlewski et al. 2023c. Pali-3 vision language models: Smaller, faster, stronger.\n' +
      '* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale.\n' +
      '* Gehrmann et al. (2022) Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan A. Botha, Michael Chavinda, Ankur Parikh, and Clara Rivera. 2022. Tata: A multilingual table-to-text dataset for african languages.\n' +
      '* Team (2023) Gemini Team, Google. 2023. Gemini: A Family of Highly Capable Multimodal Models. [https://blog.google/technology/ai/google-gemini-ai/](https://blog.google/technology/ai/google-gemini-ai/).\n' +
      '* Han et al. (2023) Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartllama: A multimodal llm for chart understanding and generation.\n' +
      '* Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes.\n' +
      '\n' +
      'Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering.\n' +
      '* Lee et al. (2023) Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Vavshi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. 2023. Pix2struct: Screen-shot parsing as pretraining for visual language understanding. In _International Conference on Machine Learning_, pages 18893-18912. PMLR.\n' +
      '* Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\n' +
      '* Liu et al. (2023a) Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. 2023a. Deplot: One-shot visual language reasoning by plot-to-table translation.\n' +
      '* Liu et al. (2023b) Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin Eisenschlos. 2023b. Matcha: Enhancing visual language pretraining with math reasoning and chart derendering.\n' +
      '* Luo et al. (2021) Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. 2021. Chartocr: Data extraction from charts images via a deep hybrid framework. _2021 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1916-1924.\n' +
      '* Magister et al. (2023) Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2023. Teaching small language models to reason.\n' +
      '* Masry et al. (2023) Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning. _arXiv preprint arXiv:2305.14761_.\n' +
      '* Masry et al. (2022) Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: A benchmark for question answering about charts with visual and logical reasoning.\n' +
      '* Methani et al. (2020) Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots.\n' +
      '* OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report.\n' +
      '* Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models.\n' +
      '* Raffel et al. (2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the limits of transfer learning with a unified text-to-text transformer, former.\n' +
      '* Wang et al. (2023a) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2023a. Towards understanding chain-of-thought prompting: An empirical study of what matters. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2717-2739, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Wang et al. (2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhory, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models.\n' +
      '* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models.\n' +
      '* Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.\n' +
      '* Wu et al. (2023) Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, and Radu Soricut. 2023. Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts.\n' +
      '* Zhou et al. (2023) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      'Fig. 11: Excellent extraction of intermediate values.\n' +
      '\n' +
      'Fig. 16: Both answer and rationale can be wrong when it comes to enumerating values and checking more complex numerical conditions.\n' +
      '\n' +
      'Fig. 12: Correct handling of extracted quantities.\n' +
      '\n' +
      'Fig. 11: Excellent extraction of intermediate values.\n' +
      '\n' +
      'Fig. 13: Strong ability to infer missing values.\n' +
      '\n' +
      'Fig. 14: Checking for equality among multiple values requires even better reasoning abilities.\n' +
      '\n' +
      'Fig. 15: Despite incorrect arithmetic results, final answer can still be correct \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>