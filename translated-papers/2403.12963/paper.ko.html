<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '모델들은 전형적으로 하나 또는 몇 개의 특정 이미지 해상도로 트레이닝된다. 예를 들어, SD 모델은 종종 \\(512\\times 512\\) 해상도의 이미지를 사용하여 훈련되는 반면, SDXL 모델은 일반적으로 \\(1024\\times 1024\\) 픽셀에 가까운 이미지로 훈련된다.\n' +
      '\n' +
      '단, 도 1에 도시한 바와 같이 한다. 1, 모델이 훈련된 것보다 높은 해상도로 이미지를 생성하기 위해 미리 훈련된 확산 모델을 직접 사용하는 것은 반복적인 패턴 및 예측되지 않은 아티팩트를 포함하는 중요한 문제로 이어질 것이다. 일부 연구 [3, 24, 26]에서는 파노라마 이미지에 겹치는 패치를 함께 스티칭하기 위해 미리 훈련된 확산 모델을 활용하여 더 큰 이미지를 생성하려고 시도했다. 그럼에도 불구하고 전체 이미지에 대한 전역 방향의 부재는 특정 객체에 초점을 맞춘 이미지를 생성하는 능력을 제한하고 통합된 전역 구조가 필수적인 반복적인 패턴의 문제를 해결하지 못한다. 최근 연구 [25]는 주의력 엔트로피를 조사하여 다양한 크기의 이미지를 생성하기 위해 미리 훈련된 확산 모델을 적용하는 것을 탐구했다. 그럼에도 불구하고 ScaleCrafter[15]는 고해상도 이미지를 생성하는 핵심 포인트가 컨볼루션 레이어에 있다는 것을 발견했다. 그들은 컨볼루션 레이어의 커널 크기를 확대하기 위해 재확장 연산과 컨볼루션 분산 연산을 도입하여 패턴 반복 문제를 크게 완화한다. 그러나 그들의 결론은 이 문제에 대한 더 깊은 탐구가 부족한 경험적 발견에서 비롯된다. 또한, 원본 컨볼루션 커널과 확대된 커널 사이의 선형 변환에 대한 초기 오프라인 계산이 필요하며, UNet의 커널 크기와 이미지의 원하는 목표 해상도에 변동이 있을 때 호환성 및 확장성 측면에서 부족한다.\n' +
      '\n' +
      '본 연구에서는 주파수 영역 분석의 관점에서 문제를 다루는 혁신적이고 효과적인 접근 방법인 FouriScale을 이론적 분석과 실험 결과를 통해 성공적으로 입증한다. FouriScale은 해상도에 걸쳐 구조적 및 스케일 일관성을 각각 달성하기 위한 저역통과 연산과 결합된 확장 연산을 단순히 도입함으로써 사전 훈련된 확산 모델에서 원래의 컨볼루션 레이어를 대체한다. 패딩-앤-크롭 전략을 갖춘 이 방법은 다양한 크기와 종횡비의 유연한 텍스트 대 이미지 생성을 가능하게 한다. 또한 FouriScale을 지침으로 활용하여 접근 방식을 달성한다.\n' +
      '\n' +
      '도 1: 미리 학습된 SDXL[32] (Train: 1024\\(\\times\\)1024; Inference: 2048\\(\\times\\)2048)을 이용한 고해상도 영상 합성의 패턴 반복 이슈 시각화. Attn-Entro[25]는 이 문제를 해결하지 못하고 ScaleCrafter[15]는 여전히 이미지 세부사항에서 이 문제로 어려움을 겪고 있다. 제안된 방법은 이 문제를 성공적으로 해결하고 모델 재교육 없이 고품질의 이미지를 생성한다.\n' +
      '\n' +
      '우수한 품질과 함께 통합된 이미지 구조로 모든 크기의 고해상도 이미지를 제작할 수 있는 뛰어난 기능입니다. FouriScale의 단순성은 오프라인 사전 계산의 필요성을 제거하여 호환성 및 확장성을 용이하게 한다. 우리는 향후 연구에서 초고해상도 이미지 합성의 발전에 상당한 기여를 제공하는 FouriScale을 구상한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Text-to-Image Synthesis\n' +
      '\n' +
      '텍스트-이미지 합성[9, 20, 36, 37]은 확산 확률 모델[19, 40]의 개발로 인해 관심이 크게 급증했다. 이러한 혁신적인 모델들은 가우시안 분포로부터 데이터를 생성하고 잡음 제거 과정을 통해 정제함으로써 동작한다. 고품질 생성을 위한 용량으로 GAN[9, 13]과 같은 전통적인 모델보다 특히 더 사실적인 이미지를 생성하는 데 크게 도약했습니다. 잠재 확산 모델(LDM, Latent Diffusion Model) [36]은 잠재 공간 내에서 확산 과정을 통합하여 이미지의 생성에서 놀라운 사실성을 달성하며, 이는 잠재 공간[5, 16, 27, 31, 46]을 통해 생성하는 도메인에 대한 상당한 관심을 높인다. 기존의 하드웨어에 대한 효율적인 처리 및 안정적인 모델 트레이닝을 보장하기 위해, 이러한 모델들은 전형적으로 하나 또는 몇 개의 특정 이미지 해상도로 트레이닝된다. 예를 들어, Stabe Diffusion (SD) [36]은 \\(512\\times 512\\) 픽셀 이미지를 사용하여 훈련되는 반면, SDXL [32] 모델은 다양한 종횡비를 동시에 수용하는 \\(1024\\times 1024\\) 해상도에 가까운 이미지로 훈련된다.\n' +
      '\n' +
      '확산 모델을 이용한### 고해상도 합성\n' +
      '\n' +
      '고해상도 합성은 항상 광범위한 관심을 받았다. 이전 작업은 주로 노이즈 스케줄[7, 22]을 정제하는 데 중점을 두고, 고해상도 이미지를 생성하기 위한 캐스케이드 아키텍처[20, 37, 42] 또는 디노이징 전문가 혼합물[2]을 개발하는 데 중점을 둔다. 인상적인 능력에도 불구하고 확산 모델은 종종 특정 해상도 제약에 의해 제한되었고 다양한 종횡비와 해상도에 걸쳐 잘 일반화되지 않았다. 일부 방법은 광범위한 범위의 해결책을 수용하여 이러한 문제를 해결하려고 시도했다. 예를 들어, Any-size Diffusion[50]은 SDXL[32]과 유사하게, 고정된 범위의 종횡비를 갖는 이미지 세트 상에서 미리 트레이닝된 SD를 미세 조정한다. FiT[28]은 이미지를 토큰들의 시퀀스로서 보고 이미지 토큰들을 미리 정의된 최대 토큰 제한에 적응적으로 패딩하여, 하드웨어 친화적인 트레이닝 및 유연한 해상도 핸들링을 보장한다. 그러나 이러한 모델은 다양한 해상도로 이미지 생성을 처리하기 위해 미리 훈련된 모델의 고유한 능력을 간과하는 모델 훈련을 필요로 한다. 가장 최근에, 일부 방법들 [3, 24, 26]은 중첩하는 패치들을 함께 스티칭하기 위해 미리 트레이닝된 확산 모델들을 활용하여 파노라마 이미지들을 생성하려고 시도했다. 최근 연구 [25]는 주의력 엔트로피를 조사하여 다양한 크기의 이미지를 생성하기 위해 미리 훈련된 확산 모델을 적용하는 것을 탐구했다. ElasticDiff[14]는 디폴트 해상도의 추정을 사용하여 임의의 크기의 이미지 생성을 안내한다. 그러나 ScaleCrafter[15]는 미리 학습된 확산 모델들에 의해 고해상도 이미지들을 생성하는 핵심 포인트가 컨볼루션 레이어들에 있다는 것을 발견한다. 그들은 컨볼루션 커널 크기를 확장하기 위해 재확장 및 컨볼루션 분산 연산을 제시하며, 이는 원래 컨볼루션 커널에서 확장된 커널로의 선형 변환의 오프라인 계산을 필요로 한다. 이와는 대조적으로, 우리는 반복적인 패턴의 문제를 깊이 조사하고 주파수 영역 분석의 관점을 통해 다룬다. 우리의 방법의 단순성은 오프라인 사전 계산의 필요성을 제거하여 호환성과 확장성을 용이하게 한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '점수 기반 생성 모델[19, 40]이라고도 하는 확산 모델은 데이터에 가우스 노이즈를 점진적으로 도입하고 역 잡음 제거 절차를 통해 이 노이즈에서 샘플을 생성하는 과정을 따르는 생성 모델의 범주에 속한다. 키 잡음 제거 단계는 일반적으로 U자형 네트워크(UNet)에 의해 수행되며, 이는 잡음 데이터로부터 깨끗한 대응물에 매핑하는 기본 잡음 제거 기능을 학습한다. 이를 위해 널리 채택된 UNet 아키텍처는 적층된 컨볼루션 레이어, 자기-어텐션 레이어 및 교차-어텐션 레이어로 구성된다. 이전의 몇몇 연구들은 어텐션 토큰의 수[25]와 컨볼루션 레이어들의 감소된 상대 수용 필드의 변화에 기인하여 생성된 해상도가 더 커질 때 성능의 저하를 탐구했다[15]. [15]의 경험적 증거에 따르면, 컨볼루션 계층은 해상도의 변화에 더 민감하다. 따라서 우리는 주로 컨볼루션 레이어가 가져오는 영향을 연구하는 데 중점을 둔다. 이 섹션에서는 그림 2와 같이 FouriScale을 소개할 것이다. 각각 해상도에 걸쳐 구조적 일관성과 스케일 일관성을 달성하기 위해 확장 컨볼루션 연산(Sec. 3.2) 및 저역 통과 필터링 연산(Sec. 3.3)을 포함한다. 맞춤형 패딩-앤-크롭핑 전략(Sec. 3.4)으로 FouriScale은 임의의 종횡비의 이미지를 생성할 수 있다. FouriScale을 지침으로 활용함으로써(Sec. 3.5), 우리의 접근법은 고해상도 및 고품질 이미지를 생성하는 놀라운 능력을 달성한다.\n' +
      '\n' +
      '도 2: 해상도에 걸쳐 구조적 일관성과 스케일 일관성을 각각 달성하기 위해 확장 컨볼루션 연산(Sec. 3.2) 및 저역 통과 필터링 연산(Sec. 3.3)을 포함하는 FouriScale(주황색 라인)의 개요.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '본 연구에서는 주파수 관점에서 구조적 일관성을 다룰 것을 제안한다. 이차원 이산 공간 신호인 입력 \\(F(x,y)\\)이 집합 \\(\\mathbb{R}^{H_{f}\\times W_{f}\\times C}\\)에 속한다고 가정하자. \\(x\\) 및 \\(y\\) 축에 따른 샘플링 속도는 각각 \\(\\Omega_{x}\\) 및 \\(\\Omega_{y}\\)에 의해 주어진다. \\(F(x,y)\\)의 Fourier 변환은 \\(F(u,v)\\in\\mathbb{R}^{H_{f}\\times W_{f}\\times C}\\으로 표현된다. 이러한 맥락에서 \\(u\\) 및 \\(v\\) 축을 따라 가장 높은 주파수는 각각 \\(u_{max}\\) 및 \\(v_{max}\\)으로 표시된다. 추가적으로, 차원 축소된 다운샘플링된 특징맵 \\(\\text{Down}_{s}(F(x,y))\\)의 푸리에 변환은 \\(\\mathbb{R}^{\\frac{H_{f}}{s}\\times\\frac{W_{f}}{s}\\times C}\\(F^{\\prime}(u,v)\\)으로 표시된다.\n' +
      '\n' +
      '정리 2.1: _공간 다운 샘플링은 특히 스펙트럼의 더 높은 단부에서 신호가 수용할 수 있는 주파수 범위의 감소로 이어진다. 이 과정을 통해 고주파는 저주파로 접히고 원래의 저주파에 중첩된다. 1차원 신호의 경우 \\(s\\) 보폭 조건에서 다운 샘플링으로 인한 높은 주파수와 낮은 주파수의 중첩을 수학적으로 공식화할 수 있다.\n' +
      '\n' +
      '[F^{\\prime}(u)=\\mathbb{S}(F(u),F\\left(u+\\frac{a\\Omega_{x}}{s}\\right))\\mid u\\in\\left(0,\\frac{\\Omega_{x}}{s}\\right), \\tag{4}\\right)\n' +
      '\n' +
      '\\(\\mathbb{S}\\)는 수퍼포징 연산자를 찌그러뜨리고, \\(\\Omega_{x}\\)는 \\(x\\) 축에서의 샘플링 속도이며, \\(a=1,\\ldots,s-1\\)._\n' +
      '\n' +
      'Lemma 1:_영상의 경우, \\(s\\)의 스트라이드를 이용한 공간 다운 샘플링의 동작은 푸리에 스펙트럼을 \\(s\\times s\\)의 동일한 패치로 분할한 다음 이 패치를 \\(\\frac{1}{s^{2}}\\)의 평균 스케일링으로 균일하게 중첩시키는 것으로 볼 수 있다.\n' +
      '\n' +
      '[\\operatorname{DFT}\\left(\\text{Down}_{s}(F(x,y))\\right)=\\frac{1}{s^{2}}\\sum_{i=0}^{s-1}\\sum_{j=0}^{s-1}F_{(i,j}(u,v)), \\tag{5}\\frac{1}{s^{s-1}F_{(i,j}(u,v))\n' +
      '\n' +
      '(i,j)}(u,v)\\)는 \\(u,v)\\)을 \\(s\\times s\\) 중첩되지 않은 패치와 \\(i,j\\in\\{0,1,\\ldots,s-1\\}\\)으로 균등 분할함으로써 \\(F(u,v)\\)의 하위 행렬이다.\n' +
      '\n' +
      '정리 2.1과 렘마 1의 증명은 부록(Sec. A.1과 Sec. A.2)에 제시되어 있다. 그들은 공간 다운 샘플링에 의해 부과된 주파수 영역에서 셔플링 및 중첩 [47, 51, 34]를 설명한다. 우리가 Eq를 변형하면. (3) 주파수 영역 및 Lemma 1의 결론에 따라, 우리는 얻을 수 있다:\n' +
      '\n' +
      '[\\left(\\frac{1}{s^{2}}\\sum_{i=0}^{s-1}\\sum_{j=0}^{s-1}F_{(i,j)}(u,v)\\right)\\odot k(u,v)\\leftarrow\\text{Left side of Eq. (3)}\\text] (6) \\[=\\frac{1}{s^{2}}\\sum_{i=0}^{s-1}\\sum_{j=0}^{s-1}\\left(F_{(i,j)}(u,v)\\prime}_{(i,j)}(u,v)\\right),\\leftarrow\\text{Right side of Eqwhere \\(u,v)\\), \\(k^{\\prime}(u,v)\\)는 각각 커널 \\(k\\) 및 \\(k^{\\prime}\\)의 푸리에 변환을 나타내며, \\(\\odot\\)은 요소별 곱셈이다. Eq. (6) 이상적인 컨볼루션 커널 \\(k^{\\prime}\\)의 푸리에 스펙트럼은 컨볼루션 커널 \\(k\\)의 푸리에 스펙트럼에 의해 스티칭되어야 함을 시사한다. 즉, \\(k^{\\prime}\\)의 푸리에 스펙트럼에는 주기적인 반복이 있어야 하며, 반복적인 패턴은 \\(k\\)의 푸리에 스펙트럼이다.\n' +
      '\n' +
      '다행히도 널리 사용되는 확장 컨볼루션은 이 요구 사항을 완벽하게 충족한다. 커널의 크기가 \\(M\\times N\\)일 때 확장 버전이 \\(k_{d_{h},d_{w}}(m,n)\\)이고 확장 계수가 \\((d_{h},d_{w})\\)이라고 가정하자. 2차원 DFT에서 확장 커널의 지수항인 \\(d_{h}\\의 정수배, 즉 \\(p^{\\prime}=pd_{h}\\)과 \\(d_{w}\\의 정수배, 즉 \\(q^{\\prime}=qd_{w}\\)의 정수배수에 대하여 (1)은:\n' +
      '\n' +
      '\\frac{p^{\\prime}m}{h^{M}}+\\frac{q^{\\prime}n}{d_{w}N}\\right)}=e^{-j2\\pi\\left(\\frac{pm}{M}+\\frac{an}{N}\\right)}, \\tag{7}\\right)\n' +
      '\n' +
      '이는 \\(m\\)-dimension을 따라 \\(M\\)의 주기 및 \\(n\\)-dimension을 따라 \\(N\\)의 주기를 갖는 주기이다. 이는 확장 계수가 \\((H/h,W/w)\\)인 원래의 커널에 의해 파라미터화된 확장 컨볼루션 커널이 이상적인 컨볼루션 커널임을 나타낸다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 3을 참조하면, 우리는 확장된 컨볼루션의 주기적 반복을 시각적으로 보여준다. 우리는 [15]도 확장 수술을 사용한다는 것을 알아차렸다. 경험적 관찰에서 비롯된 [15]와는 대조적으로, 우리의 작업은 빈도 분석에 초점을 두고 시작하며 그 효과에 대한 이론적 정당성을 제공한다.\n' +
      '\n' +
      '저역 통과 필터링을 통한### 스케일 일관성\n' +
      '\n' +
      '그러나 실제로 확장 컨볼루션만으로는 패턴 반복 문제를 잘 완화할 수 없다. 도 1에 도시된 바와 같다. 3(a)(좌측 상단), 패턴 반복의 문제는 상당히 감소되지만, 말의 다리와 같은 특정 미세한 세부 사항은 여전히 존재한다.\n' +
      '\n' +
      '그림 3: 더 나은 시각화를 위해 랜덤 \\(5\\times 5\\) 커널을 시각화한다. 확장 계수가 4인 확장된 커널의 푸리에 스펙트럼은 주기적인 특성을 분명히 보여준다. 또한 기존 사용과는 다른 확장된 커널의 오른쪽과 아래쪽에 0을 패드한다는 점에 유의해야 한다. 그러나 이것은 실제 응용에서 결과에 영향을 미치지 않는다.\n' +
      '\n' +
      '문제들 이 현상은 공간 다운 샘플링 후 에일리어싱 효과로 인해 그림 1과 같이 낮은 해상도의 특징과 높은 해상도에서 다운 샘플링된 특징 사이의 분포 격차가 증가하기 때문이다. 3(b). 앨리어싱은 원래 신호의 기본 주파수 성분을 변경하여 스케일에 걸쳐 일관성을 깨뜨린다.\n' +
      '\n' +
      '본 논문에서는 다양한 해상도에 걸쳐 스케일 일관성을 구성하기 위해 에일리어싱을 유발할 수 있는 고주파 성분을 제거하기 위한 저역 통과 필터링 연산 또는 스펙트럼 풀링[35]을 소개한다. (F(m,n)\\)를 해상도\\(M\\times N\\)을 갖는 2차원 이산신호라고 하자. 높이 및 폭을 따라 각각 인자 \\(s_{h}\\)와 \\(s_{w}\\)에 의해 \\(f(m,n)\\)의 공간 다운 샘플링은 주파수 영역에서 나이퀴스트 한계를 \\(M/(2s_{h})\\)과 \\(N/(2s_{w})\\)으로 변경하며, 이는 각 차원을 따라 새로운 샘플링 속도의 절반에 해당한다. 예상되는 저역 통과 필터는 에일리어싱을 방지하기 위해 이러한 새로운 나이퀴스트 한계 이상의 주파수를 제거해야 한다. 따라서 저역 통과 필터에서 저주파를 통과하기 위한 최적의 마스크 크기(주파수 스펙트럼이 중앙 집중화되었다고 가정)는 \\(M/s_{h}\\times N/s_{w}\\)이다. 이 필터 설계는 더 높은 주파수를 필터링하여 앨리어싱을 방지하면서 다운스케일링된 해상도 내에서 모든 귀중한 주파수의 보존을 보장한다.\n' +
      '\n' +
      '그림 4: (a) \\(2048\\times 2048\\)의 해상도에서 생성된 이미지 간의 시각적 비교: 확장된 컨볼루션만 있고 확장된 컨볼루션과 저역 통과 필터링이 모두 있다. (b)(c) UNet의 하향 블록, 중간 블록 및 상향 블록으로부터 각각 3개의 별개의 층으로부터의 입력 특징의 푸리에 상대 로그 진폭을 분석한다. 우리는 또한 역 단계 1, 25, 50에 특징을 포함한다. (b) 저역 통과 필터의 적용 없이. 낮은 해상도와 높은 해상도 사이에 주파수 스펙트럼의 분명한 분포 갭이 존재한다. (c) 저역 통과 필터의 적용으로. 분포 격차가 크게 줄어든다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 3(c)를 참조하면, 저역 통과 필터의 적용은 높은 해상도와 낮은 해상도 사이의 주파수 분포의 더 가까운 정렬을 초래한다. 이렇게 하면 Eq의 왼쪽이 됩니다. (3)은 그럴듯한 이미지 구조를 생성한다. 또한, 우리의 목표는 이미지 구조를 교정하는 것이기 때문에, 저역 통과 필터링은 일반적으로 낮은 주파수 성분들[30, 48]에 주로 상주하는 신호의 구조적 정보를 보존하기 때문에 해롭지 않을 것이다.\n' +
      '\n' +
      '이후, 확장된 커널에 저역 통과 필터링을 적용하여 최종 커널 \\(k^{*}\\)을 구한다. 확장된 커널과 관련된 푸리에 스펙트럼의 주기적 특성을 고려할 때, 새로운 커널의 푸리에 스펙트럼은 제로 주파수를 삽입하여 원래의 커널의 스펙트럼을 확장하는 것을 포함한다. 따라서, 이러한 확장은 새로운 주파수 성분들이 새로운 커널에 도입되는 것을 방지한다. 실제로 우리는 커널 \\(k^{*}\\)을 직접 계산하는 것이 아니라 계산 효율을 보장하기 위해 원래의 \\(\\operatorname{Conv}_{k}\\)을 다음과 같은 등가 연산으로 대체한다:\n' +
      '\n' +
      '\\[\\operatorname{Conv}_{k}(F)\\to\\operatorname{Conv}_{k^{\\prime}}(\\operatorname{ iDFT}(H\\odot\\operatorname{DFT}(F)), \\tag{8}\\}\n' +
      '\n' +
      '여기서 \\(H\\)은 저역 통과 필터를 나타낸다. 도. 도 3(a)(왼쪽 하단)는 확장된 컨볼루션과 저역 통과 필터링의 조합이 패턴 반복의 문제를 해결함을 예시한다.\n' +
      '\n' +
      '임의의 크기 세대에 대한 적응\n' +
      '\n' +
      '도출된 결론은 훈련에 사용되는 고해상도 영상과 저해상도 영상의 종횡비가 동일한 경우에만 적용 가능하다. Eq. (5) 및 Eq. (6) 높이 및 너비에 따른 확장률이 다른 것을 의미하는 종횡비가 변할 때, 저해상도 이미지에서 잘 구성된 구조는 그림과 같이 왜곡되고 압축될 것이라는 것이 명백해진다. 5(a). 그럼에도 불구하고, 실제 응용에서 이상적인 시나리오는 미리 훈련된 확산 모델이 임의의 크기의 이미지를 생성할 수 있는 능력을 갖는 것이다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 간단하면서도 효율적인 접근 방법인 _padding-then-cropping_을 소개한다. 도. 도 5의 (b)는 그 효과를 입증한다. 본질적으로, 레이어가 표준 해상도 \\(h_{f}\\times w_{f}\\)에서 입력 특징을 수신하고, 이 입력 특징이 추론 동안 \\(H_{f}\\times W_{f}\\)의 크기로 증가할 때, 첫 번째 단계는 입력 특징을 \\(rh_{f}\\times rw_{f}\\)의 크기로 제로패드하는 것이다. 여기서, \\(r\\)은 \\(\\lceil\\frac{H_{f}}{h_{f}}\\rceil\\)과 \\(\\lceil\\frac{W_{f}}{w_{f}}\\rceil\\)의 최대값으로 정의되며, \\(\\lceil\\cdot\\rceil\\)은 천장 동작을 나타낸다. 패딩 연산은 특정 영역이 0으로 채워진 크기\\(rh\\times rw\\)의 이미지를 생성하는 것을 목표로 한다. 이어서, Eq를 적용한다. (8) 더 높은 해상도의 출력에서 반복되는 패턴의 문제를 바로잡기 위한 것이다. 궁극적으로, 획득된 특징은 의도된 공간 크기를 복원하기 위해 크롭된다. 이 단계는 제로 패딩의 효과를 부정할 뿐만 아니라 해상도가 증가할 때 계산 요구 사항, 특히 UNet 아키텍처의 자기 주의 계층에서 발생하는 요구 사항을 제어하는 데 필요하다. 계산 효율을 고려하여, 우리의 등가 해는 알고리즘 1에 요약되어 있다.\n' +
      '\n' +
      '### FouriScale Guidance\n' +
      '\n' +
      'FouriScale은 고해상도 이미지를 생성할 때 구조적 왜곡을 효과적으로 완화한다. 그러나 그림 1과 같이 배경에 특정 인공물과 예상치 못한 패턴을 도입한다. 6(b). 본 연구 결과를 바탕으로 분류기 없는 지침에서 조건부 추정을 생성할 때 저역 통과 필터링의 적용으로 인해 주요 이슈가 발생함을 확인하였다[21]. 이러한 과정은 종종 링잉 효과 및 디테일의 손실로 이어진다. 이미지 품질을 개선하고 아티팩트를 줄이기 위해 그림 1과 같다. 도 6의 (a)를 참조하면, 우리는 출력과 세부 사항이 풍부한 출력을 정렬하는 것을 목표로 참조용 FouriScale의 안내 버전을 개발한다. 특히, FouriScale에 의해 수정된 UNet에서 파생된 무조건 및 조건부 추정을 넘어 추가 조건부 추정을 추가로 생성한다. 이것은 동일한 확장된 컨볼루션을 적용하지만 더 많은 주파수를 수용하기 위해 더 가벼운 저역 통과 필터를 사용한다. 우리는 이미지 편집[17, 6, 12]과 유사한 정신으로 FouriScale을 통해 처리된 조건부 추정의 어텐션 레이어에 대한 어텐션 맵을 대체한다. UNet의 주의 지도가 풍부한 위치 및 구조 정보를 보유하고 있다는 점을 감안할 때[44, 45, 49], 이 전략은 FouriScale에서 파생된 올바른 구조 정보를 통합하여 생성을 안내하는 동시에 생성을 완화할 수 있다.\n' +
      '\n' +
      '그림 5: \\(2048\\times 1024\\)의 해상도로 생성된 이미지 간의 시각적 비교: (a) 패딩-그-크롭 전략을 적용하지 않은 경우, (b) 패딩-그-크롭 전략을 적용한 경우. 사용된 안정 확산 2.1은 처음에 \\(512\\times 512\\) 해상도의 이미지에 대해 훈련된다.\n' +
      '\n' +
      '일반적으로 저역 통과 필터링에 의해 유발되는 이미지 품질 저하 및 세부 사항의 손실. 최종 잡음 추정은 분류기가 없는 안내에 따라 무조건 및 새로 조건부 추정 모두를 사용하여 결정된다. 그림 1에서 볼 수 있듯이. 도 6의 (c)에 도시된 바와 같이, 전술한 이슈들은 크게 완화된다.\n' +
      '\n' +
      '### Detailed Designs\n' +
      '\n' +
      '어닐링 확장 및 필터링.이미지 구조는 초기 역 단계에서 주로 요약되므로, 후속 단계는 세부 사항을 향상시키는 데 중점을 두므로 확장 컨볼루션 및 저역 통과 필터링 모두에 대한 어닐링 접근법을 구현한다. 처음에는 첫 번째 단계(S_{init}\\)에 대해 이상적인 확장 컨볼루션과 저역 통과 필터링을 사용한다. [S_{init}\\]에서 [S_{stop}\\]까지의 구간 동안 확장 계수는 점진적으로 감소하고 [r\\]은 알고리즘 1에 자세히 설명되어 있다. [S_{stop}\\] 단계 후에 원래의 UNet을 사용하여 이미지 세부 사항을 더욱 세분화한다.\n' +
      '\n' +
      'SDXL에 대한 설정.SDXL은 일반적으로 다양한 종횡비를 동시에 수용할 수 있는 \\(1024\\times 1024\\) 픽셀의 해상도로 이미지에 대해 학습된다. 우리의 관찰은 이상적인 저역 통과 필터를 사용하면 SDXL에 대한 차선책 결과로 이어진다는 것을 보여준다. 대신에, 계수 \\(\\sigma\\in[0,1]\\)(우리의 방법에서 0.6으로 설정)을 사용하여 고주파 요소를 완전히 제거하지 않고 변조하는 더 부드러운 저역 통과 필터가 우수한 시각적 품질을 제공한다. 이러한 현상은 SDXL이 SDXL의 변화를 처리하는 능력에 기인할 수 있다.\n' +
      '\n' +
      '도 6: (a) FouriScale 안내 개요. CFG는 분류자가 없는 지침을 나타낸다. (b)(c) SD 2.1에 의해 \\(2048\\times 2048\\)에서 생성된 이미지 간의 시각적 비교: (b) FouriScale 지침의 적용 없이 \\(\\저작권\\) 배경에는 예상치 못한 아티팩트가 있으며, \\(\\저작권\\)은 잘못된 세부사항이며, (c) FouriScale 지침의 적용과 함께.\n' +
      '\n' +
      '스케일 변동성을 해결하기 위해 저역 통과 필터링을 통합하는 근거를 확인하는 스케일 일관성을 유지하기 위한 이상적인 저역 통과 필터의 필요성을 무효화하면서 효과적으로 스케일을 조정한다. 또한, SDXL의 경우, 목표 해상도 중 종횡비가 가장 가까운 학습 해상도를 결정하여 스케일 팩터 \\(r\\)(알고리즘 1 참조)를 계산한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '실험적 설정으로 [15]를 따라 SD 1.5[12], SD 2.1[10], SDXL 1.0[32]의 3가지 텍스트-이미지 모델에 대한 결과를 4가지 더 높은 해상도로 이미지 생성에 대해 보고한다. 각 학습 해상도의 픽셀 수는 4\\(\\times\\), 6.25\\(\\times\\), 8\\(\\times\\), 16\\(\\times\\)이다. SD 1.5와 SD 2.1 모델 모두 원래의 학습 해상도는 512\\(\\times\\)512 픽셀이고, 추론 해상도는 1024\\(\\times\\)1024, 1280\\(\\times\\)1280, 2048\\(\\times\\)1024 및 2048\\(\\times\\)2048이다. SDXL 모델의 경우 1024\\(\\times\\)1024 픽셀에 가까운 해상도로 학습되며, 더 높은 추론 해상도는 2048\\(\\times\\)2048, 2560\\(\\times\\)2560, 4096\\(\\times\\)2048 및 4096\\(\\times\\)4096이다. 모든 실험 설정에서 FreeU[39]를 사용한다.\n' +
      '\n' +
      '데이터 세트 및 평가 메트릭을 테스트하는 [15]에 이어 50억 쌍의 이미지와 해당 캡션을 포함하는 Laino-5B 데이터 세트 [38]을 사용하여 성능을 평가한다. 1024\\(\\times\\)1024의 추론 해상도에서 수행된 테스트를 위해, 우리는 데이터 세트에서 무작위로 선택된 텍스트 프롬프트와 쌍을 이루는 30,000개의 이미지의 하위 집합을 선택한다. 본 논문에서는 1024\\(\\times\\)1024를 초과하는 추론 해상도에서 샘플 크기를 10,000개의 이미지로 축소하였다. 생성된 이미지와 실제 이미지 사이의 FID(Frechet Inception Distance)[18]와 KID(Kernel Inception Distance)[4]를 FID({}_{r}\\)와 KID({}_{r}\\)로 표기하여 측정함으로써 생성된 이미지의 품질과 다양성을 평가한다. 새로운 해상도에서 미리 훈련된 모델의 원래 능력을 보존할 수 있는 방법들의 능력을 보여주기 위해, 우리는 또한 [15]를 따라 기본 훈련 해상도와 추론 해상도에서 생성된 이미지 사이의 메트릭을 FID\\({}_{b}\\) 및 KID\\({}_{b}\\)으로 표시한다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '제안된 방법을 바닐라 텍스트-이미지 확산 모델(Vanilla), 낮은 해상도와 높은 해상도 사이의 주의 엔트로피 변화를 설명하는 훈련 없는 접근법[25](Attn-Entro) 및 재확장을 통해 컨볼루션 커널을 수정하고 커널 확대를 위해 선형 변환을 채택하는 ScaleCrafter[15]와 비교한다. 실험 결과는 Tab. 1. 바닐라 확산 모델에 비해 반복 패턴의 문제를 제거하여 훨씬 더 나은 결과를 얻었다. Attn-Entro는 해상도에 걸친 구조적 일관성을 근본적으로 고려하지 못하기 때문에 높은 업스케일링 수준에서 작동하지 않는다. ScaleCrafter에 스케일 일관성 고려가 없기 때문에 대부분의 메트릭에서 우리의 방법보다 더 나쁜 성능을 보인다. 또한 ScaleCrafter가 종종 고전하는 것을 관찰한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '제안된 방법에서 각 성분의 기여도를 검증하기 위해, 우리는 \\(2048\\times 2048\\) 영상을 생성하는 SD 2.1 모델에 대한 절제 연구를 수행한다.\n' +
      '\n' +
      '먼저, 3.5절에서 설명한 FouriScale Guidance의 사용 효과를 분석하고, FouriScale 수정 UNet의 조건부 추정에만 의존하여 지침을 활용하는 기본 FouriScale과 지침을 제거하는 기본 FouriScale을 비교한다. 탭에 표시된 대로입니다. 2. 지침을 사용하면 FID\\({}_{r}\\)이 4.26 향상되어 화질 향상에 효과가 있음을 알 수 있다. 가이던스는 FouriScale-프로세싱된 추정으로부터 구조 정보를 통합함으로써 더 가벼운 필터링과 함께 별도의 조건부 추정을 사용하여 생성을 안내할 수 있게 한다. 이는 구조적 무결성을 유지하는 것과 세부사항의 손실을 방지하는 것 사이에서 균형을 이룬다.\n' +
      '\n' +
      '또한, Sec. 3.3에 기술된 저역 통과 필터링 동작의 효과를 분석하고, 지침이 없는 FouriScale을 베이스라인으로 사용하여 모든 모듈에서 저역 통과 필터링을 추가로 제거한다. 탭에 표시된 대로입니다. 도 2에 도시된 바와 같이, 이는 FID\\({}_{r}\\)를 46.74로 더욱 악화시킨다. 저역 통과 필터링은 FID\\({}_{r}\\)을 위해 결정적이다.\n' +
      '\n' +
      '그림 7: 4\\(\\times\\), 8\\(\\times\\) 및 16\\(\\times\\)의 환경에서 SD 1.5, SD 2.1 및 SDXL 1.0의 세 가지 사전 훈련 확산 모델을 사용하여 \\(\\blacklozenge\\) 우리, \\(\\blacklozenge\\) ScaleCrafter[15] 및 \\(\\blacklozenge\\) Attn-Entro[25]의 시각적 비교.\n' +
      '\n' +
      '해상도에 걸쳐 스케일 일관성을 유지하고 왜곡을 유발하는 앨리어싱 효과를 방지합니다. 그것이 없으면, 화질이 상당히 저하된다.\n' +
      '\n' +
      '저주파 통과를 위한 마스크 크기를 비교한 시각적 결과는 그림 8에 나와 있다. 실험은 SD 2.1(512(\\times\\)512 이미지로 훈련됨)을 사용하여 2048(\\times\\)2048 픽셀의 이미지를 생성하고 기본 마스크 크기를 \\(M/4\\times N/4\\)으로 설정한다. 우리는 우리의 기본 설정으로 최적의 시각적 결과가 달성됨을 발견할 수 있다. 저역 통과 필터가 변경됨에 따라 세부 사항의 시각적 외관에 명백한 열화가 있으며, 이는 우리의 방법의 유효성을 강조한다.\n' +
      '\n' +
      '##5 결론 및 제한\n' +
      '\n' +
      '우리는 사전 훈련된 확산 모델로부터 고해상도 이미지의 생성을 향상시키는 새로운 접근법인 FouriScale을 제시한다. FouriScale은 반복 패턴 및 구조적 왜곡과 같은 주요 문제를 해결함으로써 주파수 영역 분석에 기반한 훈련 없는 방법을 도입하여 확장 연산 및 저역 통과 필터링 연산에 의해 서로 다른 해상도에 걸쳐 구조적 및 스케일 일관성을 개선한다. 패딩-앤-크롭핑 전략의 통합과 FouriScale 지침의 적용은 구조적 무결성을 유지하면서 서로 다른 종횡비를 수용하면서 텍스트 대 이미지 생성의 유연성과 품질을 향상시킨다. FouriScale의 단순성과 적응성은 광범위한 사전 계산을 피하며 해당 분야에서 새로운 벤치마크를 설정한다. FouriScale은 일반적으로 의도하지 않은 아티팩트를 나타내는 4096\\(\\times\\)4096 픽셀과 같은 초고해상도 샘플을 생성하는 데 여전히 어려움에 직면해 있다. 또한, 컨볼루션 내의 작동에 초점을 맞추는 것은 순전히 변압기 기반 확산 모델에 대한 적용 가능성을 제한한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '다운 샘플링 후, 나이퀴스트 샘플링 정리에 따라 전체 하위 주파수 범위는 \\((0,\\frac{\\Omega_{x}}{s})\\)으로 제한된다. 결과적인 주파수 대역은 다음과 같이 표현되는 \\(s\\) 초기 대역의 합성이다:\n' +
      '\n' +
      '\\[F^{\\prime}(u)=\\mathbb{S}(F(u),F(\\tilde{u}_{1}),\\ldots,F(\\tilde{u}_{s-1})), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\tilde{u}_{i}\\)는 샘플링 속도보다 높은 주파수를 나타내는 반면 \\(u\\)은 샘플링 속도보다 낮은 주파수를 나타낸다. 기호\\(\\mathbb{S}\\)는 중첩 연산자를 의미한다. 논의의 단순화를 위해 후속 섹션에서 \\(\\tilde{u}_{i}\\)를 나타낼 때 \\(\\tilde{u}_{i}\\)을 사용할 것이다.\n' +
      '\n' +
      '(1) \\(u\\in(0,\\frac{\\Omega_{x}}{2s})\\), \\(\\tilde{u}\\)이 만족해야 하는 서브밴드에서\n' +
      '\n' +
      '\\[\\tilde{u}\\in\\left(\\frac{\\Omega_{x}}{2s},u_{max}\\right). \\tag{6}\\]\n' +
      '\n' +
      '에일리어싱 정리에 따르면, 고주파수\\(\\tilde{u}\\)는 다시 저주파수로 접힌다:\n' +
      '\n' +
      '\\left|\\tilde{u}-(k+1)\\frac{\\Omega_{x}^{\\prime}{2}\\right|,\\quadk\\frac{\\Omega_{x}^{\\prime}{2}\\leq\\tilde{u}\\leq(k+2)\\frac{\\Omega_{x}^{\\prime}{2}\\tag{7}\\trac}\n' +
      '\n' +
      '여기서 \\(k=1,3,5,\\ldots\\) 및 \\(\\hat{u}\\)은 \\(\\tilde{u}\\)으로 접힌 결과이다.\n' +
      '\n' +
      'Eq에 따르면 6 및 Eq. 7, We have\n' +
      '\n' +
      '\\frac{a\\Omega_{x}}{s}-\\tilde{u}\\quad\\text{and}\\quad\\hat{u}\\in\\left(\\frac{\\Omega_{x}}{s}-u_{max},\\frac{\\Omega_{x}{2s}\\right), \\tag{8}\\tat{u}\\in\\left(\\frac{\\Omega_{x}}{s}-u_{max},\\frac{\\Omega_{x}{2s}\\right)\n' +
      '\n' +
      '여기서 \\(a=(k+1)/2=1,2,\\ldots\\이다. Eq에 따르면 (5) 및 Eq. (8)\n' +
      '\n' +
      '[F^{\\prime}(u)=\\begin{cases}F(u)&\\text{if}u\\in(0,\\frac{\\Omega_{x}}{s}-u_{max}), \\\\mathbb{S}(F(u),F(\\frac{a\\Omega_{x}}{s}-u))&\\text{if}u\\in(\\frac{\\Omega_{x}}{s}-u_{max},\\frac{\\Omega_{x}{2s}}\\end{cases}\\tag{9}\\mathbb{S}(F(u),F(\\frac{a\\Omega_{x}}{s}-u_{max},\\frac{\\Omega_{x}}{2s}}\\text{if}u\\in(\\frac{\\Omega_{x}}{s}{x}}{2s})\\end{cases}\\tag{9}\\mathbb{S\n' +
      '\n' +
      'Eq에 따르면 (3), \\(F(u)\\)는 \\(u=\\frac{\\Omega_{x}}{2}\\)에 대하여 대칭이다:\n' +
      '\n' +
      '\\[F(\\frac{\\Omega_{x}}{2}-u)=F(u+\\frac{\\Omega_{x}}{2}). \\tag{10}\\]\n' +
      '\n' +
      '따라서 다음과 같이 \\(F(\\frac{a\\Omega_{x}}{s}-u)\\)을 다시 쓸 수 있다.\n' +
      '\n' +
      '\\[F(\\frac{\\Omega_{x}}{2}-(\\frac{\\Omega_{x}}{2}+u-\\frac{a\\Omega_{x}}{s})) \\tag{11}\\[= F(\\frac{\\Omega_{x}}{2}+(\\frac{\\Omega_{x}}{2}+u-\\frac{a\\Omega_{x}}{s}))\\] \\[= F(u+\\Omega_{x}-\\frac{a\\Omega_{x}}{s})\\] \\[= F(u+\\frac{a\\Omega_{x}}{s})\\(a=1,2,\\ldots,s-1\\)ince. 또한, \\(s=2\\)에 대한 조건 \\(u\\in(0,\\frac{\\Omega_{x}}{s}-u_{max})\\)은 \\(F(u+\\frac{\\Omega_{x}}{s})=0\\)이 된다. \\(s>2\\)일 때, 범위 \\(u\\in(0,\\frac{\\Omega_{x}}{s}-u_{max})\\)은 일반적으로 존재하지 않게 된다. 따라서, Eq에 비추어. (11) 및 앞선 분석, Eq. (9)와 같이 개질할 수 있다.\n' +
      '\n' +
      '\\mathbb{S}(F(u),F(u+\\frac{a\\Omega_{x}}{s}))\\mid u\\in(0,\\frac{\\Omega_{x}}{2s}). \\tag{12}\\\n' +
      '\n' +
      '(2) 서브밴드에서, (1)과 다른 \\(u\\in(\\frac{\\Omega_{x}}{2s},\\frac{\\Omega_{x}}{s})\\)을 만족해야 한다.\n' +
      '\n' +
      '\\[\\tilde{u}\\in(\\frac{\\Omega_{x}}{s}-u_{max},\\frac{\\Omega_{x}}{2s}). \\tag{13}\\]\n' +
      '\n' +
      '마찬가지로, 우리는 얻을 수 있다:\n' +
      '\n' +
      '[F^{\\prime}(u)=\\mathbb{S}(F(\\tilde{u}), F(u+\\frac{a\\Omega_{x}}{s}))\\mid u\\in(\\frac{\\Omega_{x}}{2s},\\frac{\\Omega_{x}}{s}). \\tag{14}\\tag{14}\\mega_{x}\\mid u\\in(\\frac{\\Omega_{x}}{s})\n' +
      '\n' +
      'Eq를 결합하는 단계. (12) 및 Eq. (14), 우리는,\n' +
      '\n' +
      '\\mathbb{S}(F(u),F(u+\\frac{a\\Omega_{x}}{s}))\\mid u\\in(0,\\frac{\\Omega_{x}}{s}), \\tag{15}\\\n' +
      '\n' +
      'where \\(a=1,2,\\ldots,s-1\\).\n' +
      '\n' +
      'Lemma 1의 증명\n' +
      '\n' +
      'Eq를 기준으로. (3) \\(F^{\\prime}\\)의 진폭은 \\(F\\)의 진폭보다 \\(\\frac{1}{s}\\)배인 것으로 판단할 수 있다. 따라서 \\(F^{\\prime}(u)\\)는 다음과 같이 표현될 수 있다.\n' +
      '\n' +
      '\\frac{1}{s}F(u)+\\sum_{a}\\frac{1}{s}F\\left(u+\\frac{a\\Omega_{x}}{ s}\\right)\\mid u\\in\\left(0,\\frac{\\Omega_{x}{s}\\right)\\tag{16}\\right)\n' +
      '\n' +
      '이중원리를 바탕으로 전체 서브대역에서 \\(F^{\\prime}(u,v)\\)을 증명할 수 있다.\n' +
      '\n' +
      '\\[F^{\\prime}(u,v)=\\frac{1}{s^{2}}\\left(\\sum_{a,b=0}^{s-1}F\\left(u+\\frac{a\\Omega_{s}}{s},v+\\frac{b\\Omega_{y}}{s}\\right), \\tag{17}\\right)\n' +
      '\n' +
      '여기서 \\(u\\in\\left(0,\\frac{\\Omega_{x}}{s}\\right)\\), \\(v\\in\\left(0,\\frac{\\Omega_{y}}{s}\\right)\\).\n' +
      '\n' +
      '## 부록 0.B 구현 상세\n' +
      '\n' +
      '### 저역 통과 필터 정의\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 1에서는 FouriScale에 사용되는 저역 통과 필터의 설계를 보여준다. [34; 41]에서 영감을 얻은 저역 통과 필터를 두 개의 1D 필터(그림 1의 왼쪽에 표시) 사이의 외부 곱으로 정의한다. 하나는 높이 치수를 따라, 다른 하나는 폭 치수를 따라. 우리는 높이 치수에 대한 1D 필터의 함수를 다음과 같이 정의하며, 폭 치수에 대한 필터는 동일한 방식으로 구할 수 있다:\n' +
      '\n' +
      '\\min\\left(\\max\\left(\\frac{1-\\sigma}{R_{h}}\\left(\\frac{H}{s_{h}}+1-i\\right)+1,\\sigma\\right),1\\right),i\\in[0,\\frac{H}{2}], \\tag{18}\\sigma}}\n' +
      '\n' +
      '여기서 \\(s_{h}\\)는 높이 차원을 따라 표적과 원래 해상도 사이의 다운 샘플링 팩터를 나타낸다. \\ (R_{h}\\)는 필터의 평활도를 제어하며, \\(\\sigma\\)는 고주파에 대한 변조 계수이다. 스펙트럼의 공액 대칭 특성을 이용하여, 우리는 양의 축만을 고려하며, 1D 필터를 미러링함으로써 전체 1D 필터를 얻을 수 있다. 두 개의 1D 필터 사이에 2D 저역 통과 필터를 외부 생성물로 구축한다:\n' +
      '\n' +
      '\\[\\mathrm{mask}(s_{h},s_{w},R_{h},R_{w},\\sigma)=\\mathrm{mask}^{h}_{(s_{h},R_{h},\\sigma)}\\otimes\\mathrm{mask}^{w}_{(s_{w},R_{w},\\sigma)}, \\tag{19}\\]\n' +
      '\n' +
      '여기서 \\(\\otimes\\)은 외부 제품 작동을 나타낸다. 마찬가지로, 전체 2D 필터는 높이 및 폭 축을 따라 미러링함으로써 획득될 수 있다. 2D 저역 통과 필터의 장난감 예가 그림 1의 오른쪽에 나와 있다.\n' +
      '\n' +
      '### Hyper-parameter Settings\n' +
      '\n' +
      '이 섹션에서는 하이퍼파라미터의 선택에 대해 자세히 설명합니다. 평가 매개변수는 탭 1에 자세히 설명되어 있다. 도 2는 본 모델에 사용된 U-Net 아키텍처 내에서 다양한 블록의 정확한 포지셔닝에 대한 시각적 가이드를 제공한다.\n' +
      '\n' +
      '각 FouriScale 층에서 사용되는 확장 인자는 원래 해상도에 대한 높이 및 폭 스케일의 최대값에 의해 결정된다. 우리의 주요 원고에 명시된 바와 같이, 우리는 어닐링 전략을 사용한다. 처음으로\n' +
      '\n' +
      '그림 1: 저역 통과 필터 설계의 시각화. (a) 포지티브 축에 대한 1D 필터. (2) 1D 필터의 설정에 따라 1D 필터를 미러링하고 두 개의 1D 필터 사이에 외부 제품을 수행하여 구성된 2D 저역 통과 필터.\n' +
      '\n' +
      '우리는 이상적인 확장 컨볼루션과 저역 통과 필터링을 사용한다. (S_{init}\\)에서 \\(S_{stop}\\)까지의 기간 동안 확장 계수를 점진적으로 감소시키고 (본 원고의 알고리즘 1에 자세히 설명된 바와 같이)\\(r\\)을 1로 줄였다. \\(S_{stop}\\) 단계 후에 원래의 UNet을 사용하여 이미지 세부 사항을 더욱 세분화한다. \\(S_{init}\\) 및 \\(S_{stop}\\)에 대한 설정은 탭 1에 나와 있다.\n' +
      '\n' +
      '## 부록 0.C 추가 실험\n' +
      '\n' +
      '확산 초해상도 방법과### 비교\n' +
      '\n' +
      '본 논문에서는 SD 2.1을 이용하여 512\\(\\times\\)512의 기본 해상도로 영상을 생성하고, 미리 학습된 확산 초해상도 모델, 특히 Stable Diffusion Upscaler-4\\(\\times\\)[43]을 이용하여 2048\\(\\times\\)2048로 상향 조정한 캐스케이드 파이프라인과의 성능을 비교한다. 이 초해상도 모델을 SD 2.1에서 생성된 10,000개의 영상에 적용한 후, FID\\({}_{r}\\) 및 KID\\({}_{r}\\) 점수를 평가하고, FouriScale이 장착된 SD 2.1을 이용하여 2048\\(\\times\\)2048 해상도에서 생성된 영상과 비교한다. 이 비교 결과는 Tab. 2에 제시되어 있으며, 이 방법은 캐스케이드 방법보다 다소 나쁜 결과를 얻을 수 있다. 그러나 본 논문에서 제안하는 방법은 다단계 과정이 필요 없이 한 단계에서만 고해상도의 영상을 생성할 수 있다. 게다가, 우리의 방법은 필요하지 않다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Params & SD 1.5 \\& SD 2.1 & SDXL 1.0 \\\\ \\hline \\begin{tabular}{c} FouriScale blocks \\\\ inference timesteps \\\\ \\end{tabular} & \\begin{tabular}{c} [DB2,DB3,MB,UB0,UB1,UB2] [DB2,MB,UB0,UB1] \\\\ 50 \\\\ \\end{tabular} \\\\ \\begin{tabular}{c} [\\(S_{init}\\), \\(S_{stop}\\)] \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} [10,30] (4\\(\\times\\)1:1 and 6.25\\(\\times\\)1:1) \\\\ [20,35] (8\\(\\times\\)1:2 and 16\\(\\times\\)1:1) \\\\ \\end{tabular} & [20,35] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: SD 1.5, SD 2.1, SDXL 1.0에 대한 실험 설정.\n' +
      '\n' +
      '그림 2: 다음 실험 세부사항에서 안정적인 확산의 참조 블록 이름.\n' +
      '\n' +
      'SR 모델은 학습을 위해 광범위한 데이터와 계산 자원을 요구하는 반면, 모델은 재학습을 수행한다. 보다 중요한 것은, 도 1에 도시된 바와 같다. 셋째, 본 논문에서 제안한 방법은 캐스케이드 파이프라인보다 훨씬 더 나은 세부 정보를 생성할 수 있음을 보인다. 초해상도 방법은 생성 시 사전 지식이 부족하기 때문에 단일 이미지 내의 기존 지식만을 이미지 업스케일링에 활용할 수 있어, 지나치게 매끄러운 모습을 연출할 수 있다. 그러나 이 방법은 미리 훈련된 확산 모델을 사용하여 이미지를 효과적으로 업그레이드하고 세부 정보를 채울 수 있어 고품질 및 초고해상도 이미지 합성에 대한 향후 탐구에 귀중한 통찰력을 제공한다.\n' +
      '\n' +
      'Elastic Diffusion과의 비교\n' +
      '\n' +
      '최근의 ElasticDiffusion[14]은 사전 훈련된 확산 모델에 훈련 중에 사용된 해상도보다 작거나 큰 임의의 크기의 이미지를 생성할 수 있는 능력을 갖추는 기술을 확립했음을 관찰한다. 여기서는 SDXL 2048\\(\\times\\)2048 설정에서 ElasticDiffusion [14]와 비교를 제공한다. 그 결과를 Tab 3에 나타내었다. 첫째, ElasticDiffusion에 대한 추론 시간이 우리보다 약 4~5배 길다는 점에 유의하는 것이 중요하다. 게다가, 우리가 볼 수 있듯이, 우리의 방법은 모든 평가 메트릭에 걸쳐 우수한 성능을 보여주며, ElasticDiffusion에 비해 낮은 FID 및 KID 점수를 달성하여 더 나은 이미지 품질 및 다양성을 나타낸다.\n' +
      '\n' +
      '## 부록 0.D 추가 시각화\n' +
      '\n' +
      '### LoRAs\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 4에서는 Civitai[8]의 맞춤형 LoRA[23]와 통합한 SD 2.1에서 제작한 고해상도 영상을 제시한다. 우리는 우리의 방법이 LoRA가 장착된 확산 모델에 효과적으로 적용될 수 있음을 알 수 있다.\n' +
      '\n' +
      '### Other Resolutions\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 5를 참조하면, SD 2.1에 의해 \\(4\\times\\), \\(6.25\\times\\), \\(8\\times\\) 및 \\(16\\times\\) 설정 외에 다른 해상도에서 생성된 더 많은 이미지를 제시한다. 우리의 접근법은 임의의 종횡비와 크기의 고품질 이미지를 생성할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] AnimeArtXL: (2024), [https://civitai.com/models/117259/anime-art-diffusion-rl](https://civitai.com/models/117259/anime-art-diffusion-rl), accessed: 17, 01, 2024 22\n' +
      '* [2] Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al.: ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324 (2022) 3\n' +
      '* [3] Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113 (2023) 2\n' +
      '* [4] Binkowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying mmd gans. In: International Conference on Learning Representations (2018) 12\n' +
      '\n' +
      '도 4: SD2.1에 의해 생성된 고해상도 이미지(빨간색 사각형의 이미지) 및 SDXL을 기반으로 하는 개인화된 확산 모델, AnimeArtXL[1]에 의해 생성된 이미지의 시각화.\n' +
      '\n' +
      '그림 5: 임의의 해상도로 FouriScale 및 SD 2.1을 사용하여 더 많이 생성된 이미지.\n' +
      '\n' +
      '* [5] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S., Kreis, K.: Align your latents: High-resolution video synthesis with latent diffusion models. In: CVPR. pp. 22563-22575 (2023)\n' +
      '* [6] Cao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., Zheng, Y.: Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465 (2023)\n' +
      '* [7] Chen, T.: On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972 (2023)\n' +
      '* [8] Civitai: (2024), [https://civitai.com/](https://civitai.com/), accessed: 17, 01, 2024\n' +
      '* [9] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS **34**, 8780-8794 (2021)\n' +
      '* [10] Diffusion, S.: Stable diffusion 2-1 base. [https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt) (2022)\n' +
      '* [11] Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al.: Cogview: Mastering text-to-image generation via transformers. NeurIPS **34**, 19822-19835 (2021)\n' +
      '* [12] Epstein, D., Jabri, A., Poole, B., Efros, A.A., Holynski, A.: Diffusion self-guidance for controllable image generation. arXiv preprint arXiv:2306.00986 (2023)\n' +
      '* [13] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS **27** (2014)\n' +
      '* [14] Haji-Ali, M., Balakrishnan, G., Ordonez, V.: Elasticdiffusion: Training-free arbitrary size image generation. arXiv preprint arXiv:2311.18822 (2023)\n' +
      '* [15] He, Y., Yang, S., Chen, H., Cun, X., Xia, M., Zhang, Y., Wang, X., He, R., Chen, Q., Shan, Y.: Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. arXiv preprint arXiv:2310.07702 (2023)\n' +
      '* [16] He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221 (2022)\n' +
      '* [17] Hertz, A., Mokady, R., Tenenbaum, J., Aherman, K., Pritch, Y., Cohen-or, D.: Prompt-to-prompt image editing with cross-attention control. In: ICLR (2022)\n' +
      '* [18] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS **30** (2017)\n' +
      '* [19] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS **33**, 6840-6851 (2020)\n' +
      '* [20] Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research **23**(1), 2249-2281 (2022)\n' +
      '* [21] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)\n' +
      '* [22] Hoogeboom, E., Heek, J., Salimans, T.: simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093 (2023)\n' +
      '* [23] Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: International Conference on Learning Representations (2021)\n' +
      '* [24] Jimenez, A.B.: Mixture of diffusers for scene composition and high resolution image generation. arXiv preprint arXiv:2302.02412 (2023)\n' +
      '* [25] Jin, Z., Shen, X., Li, B., Xue, X.: Training-free diffusion model adaptation for variable-sized text-to-image synthesis. arXiv preprint arXiv:2306.08645 (2023)\n' +
      '*[*[26] Lee, Y., Kim, K., Kim, H., Sung, M.: Syncdiffusion:Coherent montage via synchronized joint diffusion. NeurIPS **36** (2024) 2, 3\n' +
      '* [27] Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., Plumbley, M.D.: Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503 (2023)\n' +
      '* [28] Lu, Z., Wang, Z., Huang, D., Wu, C., Liu, X., Ouyang, W., Bai, L.: Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376 (2024)\n' +
      '* [29] Midjourney: (2024), [https://www.midjourney.com](https://www.midjourney.com), accessed: 17, 01, 2024\n' +
      '* [30] Pattichis, M.S., Bovik, A.C.: Analyzing image structure by multidimensional frequency modulation. IEEE TPAMI **29**(5), 753-766 (2007)\n' +
      '* [31] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV. pp. 4195-4205 (2023)\n' +
      '* [32] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)\n' +
      '* [33] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: ICML. pp. 8821-8831. PMLR (2021)\n' +
      '* [34] Riad, R., Teboul, O., Grangier, D., Zeghidour, N.: Learning strides in convolutional neural networks. In: ICLR (2021)\n' +
      '* [35] Rippel, O., Snoek, J., Adams, R.P.: Spectral representations for convolutional neural networks. NeurIPS **28** (2015)\n' +
      '* [36] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR. pp. 10684-10695 (2022)\n' +
      '* [37] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS **35**, 36479-36494 (2022)\n' +
      '* [38] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open large-scale dataset for training next generation image-text models (2022)\n' +
      '* [39] Si, C., Huang, Z., Jiang, Y., Liu, Z.: Freeu: Free lunch in diffusion u-net. arXiv preprint arXiv:2309.11497 (2023)\n' +
      '* [40] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: International Conference on Learning Representations (2020)\n' +
      '* [41] Sukhbaatar, S., Grave, E., Bojanowski, P., Joulin, A.: Adaptive attention span in transformers. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 331-335 (2019)\n' +
      '* [42] Teng, J., Zheng, W., Ding, M., Hong, W., Wangni, J., Yang, Z., Tang, J.: Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350 (2023)\n' +
      '* [43] Upscaler, S.D.: (2024), [https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler](https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler), accessed: 17, 01, 2024\n' +
      '* [44] Wang, J., Li, X., Zhang, J., Xu, Q., Zhou, Q., Yu, Q., Sheng, L., Xu, D.: Diffusion model is secretly a training-free open vocabulary semantic segmenter. arXiv preprint arXiv:2309.02773 (2023)\n' +
      '* [45] Xiao, C., Yang, Q., Zhou, F., Zhang, C.: From text to mask: Localizing entities using the attention of text-to-image diffusion models. arXiv preprint arXiv:2309.04109 (2023)* [46] Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., Kreis, K.: Lion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978 (2022)\n' +
      '* [47] Zhang, R.: Making convolutional networks shift-invariant again. In: ICML. pp. 7324-7334. PMLR (2019)\n' +
      '* [48] Zhang, Y., Li, K., Li, K., Wang, L., Zhong, B., Fu, Y.: Image super-resolution using very deep residual channel attention networks. In: ECCV. pp. 286-301 (2018)\n' +
      '* [49] Zhao, W., Rao, Y., Liu, Z., Liu, B., Zhou, J., Lu, J.: Unleashing text-to-image diffusion models for visual perception. ICCV (2023)\n' +
      '* [50] Zheng, Q., Guo, Y., Deng, J., Han, J., Li, Y., Xu, S., Xu, H.: Any-size-diffusion: Toward efficient text-driven synthesis for any-size hd images. arXiv preprint arXiv:2308.16582 (2023)\n' +
      '* [51] Zhu, Q., Zhou, M., Huang, J., Zheng, N., Gao, H., Li, C., Xu, Y., Zhao, F.: Fouridown: Factoring down-sampling into shuffling and superposing. In: NeurIPS (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>