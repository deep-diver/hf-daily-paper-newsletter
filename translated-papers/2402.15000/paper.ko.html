<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 분할 혹은 정복? LLM은 어느 부분을 증류해야 하나요?\n' +
      '\n' +
      '주팽우({}^{{\\dagger}{\\ddagger}\\), He Bai\\({}^{\\dagger}\\), Aonan Zhang\\({}^{\\dagger}\\),\n' +
      '\n' +
      '지아토 구\\({}^{\\ddagger}\\), VG Vinod Vydiswaran\\({}^{\\dagger}\\), Navdeep Jaitly\\({}^{\\ddagger}\\), Yizhe Zhang\\({}^{\\ddagger}\\)**\n' +
      '\n' +
      '미시간대학교, \\({}^{\\dagger}\\) Apple\n' +
      '\n' +
      '{zhuofeng,vyinodv}@umich.edu,\n' +
      '\n' +
      '{hbai22,aonan_zhang,jgu32,njaitly,yizhe_zhang}@apple.com\n' +
      '\n' +
      '애플 인턴십 기간 동안 수행한 작업\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 방법들은 큰 언어 모델(Large Language Models; LLM)들이 주요 태스크의 하위 태스크들을 먼저 해결하도록 장려될 때 추론 태스크들을 더 잘 해결할 수 있다는 것을 입증하였다. 본 논문에서는 추론 태스크를 _problem decomposition_ phase와 _problem solving_ phase로 분해하는 유사한 전략을 고안하고, 이 전략이 단일 단계의 해를 능가할 수 있음을 보인다. 또한, 분해는 많은 양의 도메인 지식을 필요로 하는 반면 전자는 일반적인 문제 해결 전략을 학습하기만 요구하기 때문에 문제 해결에 비해 더 작은 모델로 증류하기 쉬워야 한다고 가정한다. 이 두 가지 기능을 증류하고 추론 결과와 추론 비용에 미치는 영향을 평가하는 방법을 제안한다. 우리는 문제 분해 단계를 증류하는 동시에 작업, 데이터 세트 및 모델에 걸쳐 좋은 일반화를 달성할 수 있음을 발견했다. 그러나 성능 손실 없이 문제 해결 능력을 증류하는 것은 더 어렵고 결과물인 증류 모델은 일반화에 어려움을 겪는다. 이러한 결과는 문제 해결 LLM과 함께 더 작은 증류된 문제 분해 모델을 사용하여 비용 효율적인 추론과 지역 적응으로 추론을 달성할 수 있음을 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'GPT-4(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)은 Open Domain QA(ODQA) Zhu 등(2021), 수학(Yue 등, 2023), 과학 테일러 등(2022) 및 자율 에이전트 Yao 등(2022); Significant Gravitas 등(2023); Wang 등(2024)과 같은 지식 집약적 작업을 해결하는 데 탁월한 능력을 보여준다. 그러나, 수천억 개의 파라미터를 갖는 거대 LLM의 사용은 추론 동안, 특히 생성된 추론 체인이 긴 경우에 _costly_일 수 있다. 또한 이러한 블랙박스 LLM의 불투명한 특성으로 인해 제한된 _adaption_ 옵션을 제공한다. 로컬 적응 및 _cost-efficient_ 추론을 위해 이러한 블랙박스 LLM의 힘을 활용하기 위해 _cheaper_ 및 보다 많은 _flexible_ 모델을 사용할 필요가 있다. 대형 LLM을 증류하는 것은 합리적인 전략으로 보이지만 종종 하류 작업인 Chiang et al.(2023)에 대해 상당한 성능 저하를 초래한다.\n' +
      '\n' +
      '이전 연구 Weng (2023); Wang et al. (2023)은 이러한 작업을 효과적으로 처리하기 위해서는 모델이 두 가지 필수 능력을 동시에 능숙하게 수행해야 한다고 지적했다: 1) 복잡한 작업의 효율적인 처리를 용이하게 하기 위해 복잡한 목표를 더 작고 관리 가능한 하위 목표로 분해하는 **계획 및 분해**; 및 2) 광범위한 웹 트레이닝 데이터로부터 방대한 양의 지식을 암기하고 문제 해결 프로세스를 실행하기 위해 필요할 때 이 정보를 효과적으로 회상하는 **실행 및 해결**. 첫 번째 성능인 분해는 일반적으로 모델이 입력 쿼리에 대한 자기 성찰에 관여하고 문제를 해결하기 위해 CoT(Chain-of-Thoughts) 스타일 추론 체인 Wei et al.(2022)을 생성해야 한다. 보통 이 두 능력은 문제 해결 과정인 Zhou et al.(2022) 전반에 걸쳐 단일 모놀리식 모형으로 얽혀 있다.\n' +
      '\n' +
      '본 논문에서는 먼저 분해 및 해결 능력을 분리하는 것이 가능한지 여부와 이러한 능력을 더 빠른 추론을 위해 더 작은 모델로 증류하는 방법을 조사한다. 그런 다음 몇 가지 가설을 테스트한다. 1)_distilling 분해가 distilling solve_보다 쉽다. 분해는 주로 의미적 이해와 질의 구문 분석에 의존하는 반면, 해결은 더 많은 도메인 전문성과 지식을 필요로 한다. 예를 들어, "_who is older, Messi or Ronaldo?_"라는 쿼리를 분해하는 단계를 포함할 수 있다. into "_how old is Messi?_?" "호날두가 몇 살이죠?" , 그리고 "_who is older?_" 텍스트 이해만을 필요로 하는 반면, 과제 해결은 정보의 암기, 검색, 활용을 필요로 한다. 우리는 덜 지식 집약적인 분해를 압축하는 것이 더 쉽다고 추측한다. 2) _분해는 해결보다 일반화 가능하다._ 우리는 분해를 때때로 상징적 원리로 추상화하여 작업, 데이터 세트 및 모델에 걸쳐 보다 보편적으로 적용할 수 있다고 가정한다. 이는 작업과 모델들이 공통의 분해 엔진을 공유하고 서로의 힘으로부터 이익을 얻을 수 있게 하여, 각각의 개별 작업에 대한 모델을 증류하는 데 수반되는 노력과 비용을 감소시킨다.\n' +
      '\n' +
      '자연스러운 질문이 제기된다: _ 추론 비용의 대부분을 차지하는 긴 추론 체인만을 증류하는 것이 가능하나, 증류하는 것이 상대적으로 용이함?_ 이를 위해 LLM에서 분해 능력만 증류하는 것을 제안하고 평가한다. QA 및 수학 데이터셋(Dua et al., 2019; Cobbe et al., 2021)에서 GPT-3.5-turbo의 교사 모델과 vicuna-13B(Chiang et al., 2023)의 학생 모델을 사용하여 실험을 수행한다. 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 분해능력이 LLM의 복잡한 추론에 중요한 역할을 한다는 것을 증명한다. 이 역량은 문제 해결 능력 또는 과제 해결 능력으로부터 해부될 수 있다.\n' +
      '2. 교사 모델로부터 질의 분해만을 증류할 수 있는 가능성과 유효성을 보인다. 결과적인 증류 모델은 추론 비용을 크게 줄이면서 대부분의 성능을 유지할 수 있다. 그러나 LLM의 해결 부분을 증류하는 것은 상당한 성능 저하로 이어진다.\n' +
      '3. 증류된 질의 분해 모델은 작업, 데이터 세트 및 모델에 걸쳐 양호한 일반화를 나타냄을 보여준다. 그러나 각 과제에 대한 증류수 풀이는 잘 일반화되지 않는다.\n' +
      '\n' +
      '그림 1: 블랙박스 LLM을 사용한 긴 사고 체인으로 추론하는 것은 비용이 많이 들고 유연하지 않을 수 있다. 우리는 과제의 분해와 해결을 분석하고, 분해 능력만을 기존의 성능을 유지하면서 비용이 덜 들고 유연한 학생 모델로 증류할 것을 제안한다.\n' +
      '\n' +
      '디커플링 분해와 해결\n' +
      '\n' +
      '그림 1과 같이 LLM을 이용하여 추론 과제를 해결하는 일반적인 접근 방식은 지시와 질문에 대한 응답을 직접 생성하는 것을 포함한다. 이를 **Single-Stage** 모델이라 한다. CoT(Chain of Thought)로 알려진 LLM에 대한 기존의 방법은 모델에 "단계적으로 생각"을 지시하여 모델이 어려운 작업에 대해 더 많은 계산 단계를 수행할 수 있도록 한다.\n' +
      '\n' +
      '그러나 CoT식 추론은 맥락 내 예제의 범위를 넘어서는 문제로 일반화하기 위해 고군분투하는 경우가 많아 한계가 있다. 이러한 단점을 해결하기 위해 가장 주목할 만한 작업은 모델이 원래 질문을 하위 질문으로 분해하고 순차적으로 답변하는 최소 대 최대 접근법(Zhou et al., 2022)이다. 이러한 접근법은 CoT에 비해 향상된 성능을 보여주었다.\n' +
      '\n' +
      'QA 작업의 경우 일반적으로 다음 하위 질문은 이전 하위 질문에 대한 답변에 덜 의존합니다. 편리하게, 우리는 HuggingGPT(Shen et al., 2023)와 유사한 _static_ 전략을 제안하는데, 여기서 첫 번째 _Decomposition_ 단계에서 몇 개의 분해된 서브질문들이 일차 질문을 분해하기 위해 먼저 생성된다. 두 번째 _Solving_ 단계에서, 이러한 서브 질문들이 차례로 답변되어 최종 답변을 얻는다. 우리는 이 모델 라인을 **2단계** 모델이라고 합니다.\n' +
      '\n' +
      '##3 분해능력\n' +
      '\n' +
      '분해된 질문을 생성하는 것은 블랙박스 LLM을 사용하는 동안 추론 체인이 긴 경우 계산적으로 비용이 많이 들 수 있다. 더욱이, 블랙 박스 모델에 의해 수행되는 바와 같이 분해 프로세스를 최적화하거나 커스터마이징하는 것은 어렵다. 우리의 제안은 분해를 위한 대형 블랙박스 LLM의 드롭인 대체물로 더 작은 훈련 가능한 학생 모델을 활용하여 이러한 문제를 해결하는 것을 목표로 한다. 이를 위해, 우리는 교사 LLM으로부터 \\(\\mathcal{T}\\)으로 불리는 분해 능력을 증류한다.\n' +
      '\n' +
      '그림 1과 같이 교사로부터 하위 질문을 생성하는 것은 \\(\\mathcal{T}\\)의 시범을 모으는 것으로 시작한다. 문제를 해결하기 위해 \\(\\mathcal{T}\\)을 요구하는 대신에, 우리는 주어진 질문 \\(Q\\)을 해결하지 않고 분해하도록 요청한다. 구체적으로 \\(\\mathcal{T}\\)에 \\(Q\\)과 함께 \\(I_{\\text{decomp}}\\)으로 표기된 분해 명령어를 제공한다.\n' +
      '\n' +
      '(\\mathcal{T}\\) 다음에 서브퀘션 \\(\\{S_{i}\\}_{i=1,2,3...}\\)을 생성한다.\n' +
      '\n' +
      '교사에 의해 생성된 subquestions\\(\\{S_{i}\\}\\)이 주어지면, 우리는 \\(\\mathcal{T}(I_{\\text{decomp}},Q)\\rightarrow\\{S_{i}\\}\\)에 대한 교차 엔트로피 손실을 최적화함으로써 학생 분해자\\(\\mathcal{S}\\)을 미세화할 수 있다. 우리는 결과적인 학생 모델을 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)으로 나타낸다.\n' +
      '\n' +
      '추가 단계로 Ground-truth AnswerAs를 통한 서브퀘스트 스크리닝 데이터세트가 \\(A\\)로 표시된 해당 Ground-truth 답변과 함께 제공되는 경우, 이 정보를 선택적으로 사용하여 고품질 생성된 서브퀘스트를 스크리닝할 수 있다. 이를 위해 먼저 부문제(\\{S_{i}\\}\\}\\})를 풀어서 1차 문제(Q\\)를 풀도록 모델을 요청하는 다른 명령어(I_{text{ans}\\)와 동일한 교사 모델(\\(\\mathcal{T}\\)을 공급하였다. 우리는 생성된 답 \\(\\mathcal{T}(I_{\\text{ans}},P,\\{S_{i}\\},Q)\\rightarrow\\hat{A}\\)을 수집하며, 여기서 \\(P\\)은 전제를 나타낸다. \\(text{ans},P,\\{S_{i}\\},Q)\\rightarrow\\hat{A}\\. (I_{\\text{ans}}\\)는 다음과 같이 제공된다:\n' +
      '\n' +
      '\\begin{tabular}{|p{343.4pt}|} \\hline Instruction for Solving \\(I_{\\text{ans}}\\) \\\\ \\hline Solve a complex question by answering several related subquestions that would help me to answer it first. Answer the subquestions one by one and finally solve the original question. The final answer is supposed to attached in the end in the format of "The answer is: ". Now comes our primary question and its subquestions: \\\\ Premise: \\(P\\) \\\\ Question: \\(Q\\) \\\\ SubQuestion: \\(\\{S_{i}\\}\\) \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '우리는 통계적으로 말하면 좋은 \\(\\{S_{i}\\}\\)이 결국 과제 해결로 이어질 것이라고 가정한다.\n' +
      '\n' +
      '따라서, 우리는 선택적으로 \\(\\hat{A}\\neq A\\)의 훈련 인스턴스들을 걸러낼 수 있다. 그러나 이로 인해 데이터 손실이 발생합니다. 이 스크리닝 과정은 _Rejection Sampling_(Touvron et al., 2023)와 유사하므로, 결과 모델을 \\(\\mathcal{S}_{D}\\)-\\(R\\)로 나타낸다.\n' +
      '\n' +
      '섹션 5.2에서는 전체 시연 집합을 사용하여 훈련된 증류 분해기의 성능을 선별된 데이터셋을 사용하여 훈련된 분해기와 비교한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '데이터 세트 우리는 두 개의 별개의 데이터 세트에 대한 파이프라인의 효과를 평가한다. GSM8K Cobbe et al.(2021)은 수학적 추론에 중점을 두고 있으며, 1K 테스트 문제와 함께 7.5K 트레이닝 인스턴스로 구성되어 있다. DROP Dua et al. (2019)는 77.4K 트레이닝 샘플 및 9.5K 유효성 검사 세트를 포함하는 Question Answering에 대응한다. GSM8K 테스트 셋과 DROP 개발 셋을 사용하여 평가 시나리오를 제한한 오라클 응답 \\(A\\)이 없기 때문에 평가를 수행한다.\n' +
      '\n' +
      '교사/학생 모델 우리는 실험 전반에 걸쳐 GPT-3.5-Turbo-0615 모델 Ouyang et al.(2022)을 교사 모델로 사용한다. 교육 후, 포괄적인 평가를 보장하기 위해 다양한 수준의 교사 모델을 사용한다: 1개의 개방형 소스 모델 바닐라 비쿠나 치앙 외(2023))와 3개의 블랙박스 모델(텍스트-다빈치-003 Brown 외(2020), GPT-3.5-터보 및 GPT-4). 모든 학생 모델은 Vicuna-13b-v1.3 Chiang et al.(2023)로부터 초기화된다.\n' +
      '\n' +
      '학생 해결사 모델 증류 분해기와 증류 분해기의 성능을 비교하기 위해 학생 해결사로서의 교사의 행동을 모방하기 위해 여러 Vicuna 모델에 대한 추가 교육을 실시했다. 학생 분해자와 유사하게 \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\)은 \\(\\mathcal{T}(I_{\\text{ans}},\\{S_{i}\\},Q)\\rightarrow(\\{\\hat{A}^{s}_{i}\\},\\hat{A}\\)의 교사 시범을 이용하여 훈련된 모델을 나타내며, 여기서 \\(\\hat{A}^{s}_{i}\\}\\)은 \\(\\mathcal{T}\\)에 의해 생성된 하위 질문 \\(\\{S_{i}\\}\\)에 대한 답을 나타낸다.\n' +
      '\n' +
      '또한 오라클 응답 \\(A\\)이 가능한 시나리오에서 동일한 바닐라 Vicuna-13B 모델을 미세 조정하여 \\(\\mathcal{S}_{E}\\)-\\(A\\)을 얻었다. 이 모델은 \\((I_text{ans},\\{S_{i}\\},Q)\\rightarrow(\\{\\hat{A}^{s}_{i}\\},A)\\)을 사용하여 훈련되었으며, 여기서 표적은 \\(\\mathcal{T}\\) 및 지상진리 답 \\(A\\)으로부터 서브질문 \\(\\{S_{i}\\}\\)에 대한 답을 포함한다.\n' +
      '\n' +
      '트레이닝 세부사항은 128의 배치크기를 사용하고, DROP에서 3 에폭 트레인, GSM8K 데이터세트에서 5 에폭 트레인(수렴 시까지)을 사용하며, 증류트레이닝은 학습률을 \\(2\\cdot 10^{-5}\\)으로 설정한다. 모든 증류 미세조정은 \\(8\\times 80\\)G A100 GPU에서 12시간 이내에 완료될 수 있다.\n' +
      '\n' +
      '추론 비용 추정은 GPT-3.5-turbo-1106 (175B)을 기반으로 계산되며, 1000개의 입력 토큰은 \\(\\$0.001\\), 1000개의 출력 토큰은 \\(\\$0.002\\)의 비율로 계산된다. OpenAI는 GPT 모델을 제공할 때 추론 시간을 크게 최적화했다. 공정한 비교를 보장하기 위해 비용을 모델 크기의 비율로 나누어 Vicuna-13B 모델의 비용을 보수적으로 추정한다. 그 결과, Vicuna-13B의 비용은 1000개의 입력 토큰에 대해 약 $7.42*10^{-5}\\, 1000개의 출력 토큰에 대해 약 $1.48*10^{-4}\\이다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '### 분해는 추론을 위해 필수적이다.\n' +
      '\n' +
      '먼저, _Decomposition_와 _Solving_를 분리할 수 있는 가능성을 탐색하고 복잡한 추론 작업에 개선된 분해를 사용하는 것의 효율성을 평가한다.\n' +
      '\n' +
      '선행연구 Press et al. (2022); Zhou et al. (2022)는 블랙박스 모델의 질의응답 능력을 향상시키기 위해 분해된 하위 질문을 활용하는 유용성을 입증했다. 그들은 _interactive_ planning 전략을 채택하는데, 여기서 각 하위 질문의 생성은 이전 하위 질문의 답변에 따라 조건화된다.\n' +
      '\n' +
      '섹션 2에서 논의한 바와 같이 추론 과정을 분해와 해결의 두 가지 개별 단계로 분해하여 _static_ 전략을 사용한다. 표 1(단일 단계 GPT/Vicuna vs 2단계 GPT/Vicuna)은 일반적으로 이러한 정적 전략이 단일 단계 접근법에 비해 성능 향상으로 이어진다는 것을 보여준다. 이는 이전 연구 결과와 일치한다.\n' +
      '\n' +
      '우리는 표 1(2단계 모델)에서 더 강한 분해자(GPT)를 더 약한 분해자(Vicuna)로 대체하는 것이 GSM8K에서 Vicuna를 솔버로 사용하는 것을 제외하고 대부분 현저한 성능 감소를 초래한다는 것을 보여준다. 우리는 그 이유가 비쿠나 해결사가 분해로 인한 개선을 활용하기에는 너무 잘못되었기 때문이라고 가정한다. 우리는 풀이기가 더 강력할 때 그 감소가 더 크다는 것을 관찰한다. 이는 최적의 성능을 달성하기 위해서는 더 강한 분해기가 필수적임을 시사한다.\n' +
      '\n' +
      '디스트리밍 분해가 디스트리밍 해결보다 쉬운가요?\n' +
      '\n' +
      '다음으로, 지상진리답(A\\)을 사용할 수 없을 때 \\(\\mathcal{T}\\)에서 \\(\\mathcal{S}\\)까지 증류지식을 조사한다. 이것은 지상 진실 주석이 일반적으로 비싸고 드물기 때문에 가장 일반적인 사용 사례이다. 결과를 표 1에 나타낸다(w/o oracle answer \\(A\\)) 분해자를 위한 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)의 스와핑은 적어도 \\(\\mathcal{T}\\)을 사용한 성능과 비슷함을 알 수 있다. 또한, \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)은 Vicuna를 분해자로 사용하는 것에 비해 눈에 띄는 개선을 보였다. 그러나 학생용 솔버 모델인 \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\)에서 스와핑은 성능에 큰 영향을 미친다. 또한 단일 단계 GPT에서 증류된 단일 단계 학생 모델을 평가했다. 그 결과 GPT가 분해자이고 \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\)이 해결자인 모델보다 훨씬 더 나쁜 결과를 보였다. 추론 비용 측면에서, 우리의 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\) 접근법은 교사 GPT 모델을 사용하는 것에 비해 분해 비용이 상당히 낮다. 솔버의 비용은 비교적 변함이 없다.\n' +
      '\n' +
      '표 2의 평가집합에서 \\(\\mathcal{T}\\), Vicuna에서 그리고 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)의 분해를 비교한다. \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)의 도메인 시연을 이용하여 얻은 증류된 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\) 모델은 보이지 않는 테스트 집합에서 생성된 하위 질문에서 교사 시연과 높은 유사성을 나타냄을 알 수 있다. 대조적으로, 원래의 비쿠나 모델은 종종 해결사를 산만하게 할 가능성이 있는 도움이 되지 않는 질문을 생성한다.\n' +
      '\n' +
      '더 작은 학생 모델이 교사 모델의 분해 능력을 빠르게 모방할 수 있다면, 왜 학생 모델의 초기 사전 훈련을 통해 이 기술을 직접 습득하는 것이 어려운지 당연히 궁금해할 수 있다. 우리의 가설은 더 강한 교사 모델의 분해 능력은 증류하기 쉽지만 획득하기 어렵다는 것이다. 이 기술은 더 큰 모델의 집중적인 사전 훈련 동안 방대한 양의 데이터를 철저히 소화하고 내재화하는 것을 기반으로 할 수 있다. 그러나 _knowledge-intensive_보다는 논리적이고 추상적이기 때문에 몇 가지 시연은 이미 학생들에게 충분한 안내를 제공할 수 있다. 불완전한 비유를 끌어내기 위해, 거대한 관측으로부터 물리학 정리를 찾는 것은 그 정리를 배우는 것보다 훨씬 더 어렵다.\n' +
      '\n' +
      '때때로 오라클 답변을 통해 오라클 답변 \\(A\\)에 액세스할 수 있으며, 이는 로컬 적응 및 추가 미세 조정을 통해 특정 도메인에 대한 모델의 성능을 더욱 향상시키는 데 사용할 수 있다. 결과적으로 이러한 목표 영역에 대한 성과는 블랙박스 교사 모델의 성과를 넘어설 수 있다. 증류 또는 표적 도메인 미세 조정을 통해 모델을 향상시키는 옵션을 탐색한다.\n' +
      '\n' +
      '이러한 시나리오에서 우리는 Rejection Sampling과 유사하게 분해기를 증류하기 위한 훈련 인스턴스를 선별하기 위해 \\(A\\)을 사용할 수 있다. 결과적인 학생 모델 \\(\\mathcal{S}_{D}\\)-\\(R\\)은 표 1과 같이 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)을 사용하는 것보다 더 높은 성능을 달성했다(w/oracle answer \\(A\\)). 특히, DROP 데이터 세트에서 \\(\\mathcal{S}_{D}\\)-\\(R\\)은 F1 점수 측면에서 교사 모델보다 우수하다.\n' +
      '\n' +
      '우리는 또한 \\(\\mathcal{S}_{E}\\)-\\(A\\)로 지칭되는 지상진실 답을 사용하여 해결자를 위한 또 다른 Vicuna 모델을 세밀하게 조정한다. 우리의 주요 결과는 오라클 답변을 사용할 수 없는 시나리오와 일관되게 유지된다. 분해기를 증류하는 것은 여전히 더 나은 결과를 낳는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline  & **Decomposer** & **Solver** & \\multicolumn{2}{c}{**Performance\\(\\uparrow\\)**} & \\multicolumn{2}{c}{**Inference Expense \\(\\downarrow\\)**} \\\\ \\cline{2-7}  & Model & Model & GSM8K (EM) & DROP (F1) & GSM8K(S) & DROP(S) \\\\ \\hline \\multirow{2}{*}{_Single-stage_} & - & GPT & 20.32 & 46.51 & -/0.01 & -/0.05 \\\\  & - & Vicuna-13B & 9.40 & 26.68 & -/0.03 & -/0.03 \\\\ \\hline \\multirow{3}{*}{_Two-stage_} & GPT & GPT & 65.13 & 55.73 & 0.13/0.63 & 0.73/0.96 \\\\  & Vicuna-13B & GPT & 62.93 & 47.13 & 0.02/0.67 & 0.07/0.96 \\\\  & GPT & Vicuna-13B & 28.13 & 21.29 & 0.13/0.07 & 0.73/0.08 \\\\  & Vicuna-13B & Vicuna-13B & 28.51 & 20.90 & 0.02/0.08 & 0.07/0.08 \\\\ \\hline \\multirow{3}{*}{_w/o oracle answer \\(A\\)_} & \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\) & GPT & 67.02 & 55.19 & 0.01/0.62 & 0.06/0.96 \\\\  & GPT & \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\) & 48.98 & 13.37 & 0.13/0.09 & 0.73/0.06 \\\\ \\hline \\multirow{3}{*}{_w/ oracle answer \\(A\\)_} & \\(\\mathcal{S}_{D}\\)-\\(R\\) & GPT & **67.78** & **57.97** & 0.01/0.60 & 0.06/1.11 \\\\  & GPT & \\(\\mathcal{S}_{E}\\)-\\(A\\) & 51.55 & 20.34 & 0.13/0.09 & 0.73/0.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: GSM8K 및 DROP 데이터 세트에 대한 비교 결과. GSM8K에 대한 성능은 정확한 일치 점수(EM)를 통해 평가되는 반면 DROP는 F1 점수를 사용하여 평가된다. 추론 비용은 각 데이터 세트에 대한 샘플 비용당 평균을 기반으로 추정됩니다. \\ (X/X\\)는 분해/해결 비용을 나타낸다.\n' +
      '\n' +
      '솔버를 미세 조정하는 것과 비교하여 성능이 향상되었습니다. GPT(decomposer) + \\(\\mathcal{S}_{E}\\)-\\(A\\)(solver)보다 더 나쁜 결과를 얻을 수 있는 \\(A\\)을 사용하여 단일 단계 Vicuna 모델을 미세 조정하지 않았다.\n' +
      '\n' +
      '우리의 관찰에 따르면 \\(\\mathcal{S}_{E}\\) 모델의 고장 모드는 \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\) 모델과 \\(\\mathcal{S}_{E}\\)-\\(A\\) 모델의 두 가지 기본 고장 모드가 있다고 가정한다.\n' +
      '\n' +
      '첫째, 하위 질문이나 기본 질문에 답하려면 광범위한 세계 지식과 상식이 필요하며, 이는 몇 번의 시연만 사용하여 수백 배 작은 학생 모델로 압축하기 어려울 수 있다. 즉, 강한 해결 역량은 지식 집약적이다. 반면에 분해 능력은 일반적으로 더 추상적이고 정보 밀도가 낮으며 해결 능력보다 보편적이기 때문에 압축성이 더 높을 수 있다.\n' +
      '\n' +
      '둘째, 서브질문(\\{\\hat{A}^{s}_{i}\\}\\)에 대한 교사의 답을 목표의 일부로 사용하였기 때문에, 1차 질문(Q\\)이 아닌, 서브질문(\\{S}_{E}\\) 중 하나에 대한 수학 계산(\\mathcal{S}_{E}\\) 모델이 혼동되어 최종 답을 생성할 수 있었다. (부기 C에 예시가 제공된다.)\n' +
      '\n' +
      '이상의 결과를 바탕으로 \\(\\hat{A}^{s}_{i}\\}\\) 모델을 훈련할 때 목표물에서 \\(\\hat{A}^{s}_{i}\\}\\)을 배제하는 실험을 하였다. 구체적으로, 응답 서브 질문인 \\(\\mathcal{S}_{E}(I^{\\prime}_{\\text{ans},\\{S_{i}\\},Q)\\rightarrow\\hat{A}/A\\)를 생략하여 직접 응답을 생성하는 모델을 학습한다. 결과 모델은 \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\)(direct) 및 \\(\\mathcal{S}_{E}\\)-\\(A\\)(direct)로 표시된다. 그 결과, 목표값으로부터 \\(\\hat{A}^{s}_{i}\\}\\)이 DROP 데이터 셋에 비해 향상된 결과를 얻었으나 GSM8K 데이터 셋에 비해 성능저하를 가져오는 것을 알 수 있었다. 전반적으로, GSM8K에서 관찰된 감소는 DROP 데이터 세트에서 관찰된 이득보다 더 두드러진다. 따라서 우리는 여전히 표적이 \\(\\hat{A}^{s}_{i}\\)인 \\(\\mathcal{S}_{E}\\) 모델을 사용한다. 추가적인 분석 방법인 \\(I^{\\prime}_{\\text{ans}\\)을 제공하고, 부록 A에서 비교 결과를 보여준다.\n' +
      '\n' +
      'Distilling Decompomposition more\n' +
      '\n' +
      '주조 솔루션보다 일반화할 수 있습니까?\n' +
      '\n' +
      '그런 다음 특정 도메인 데이터 세트에 대해 훈련된 증류된 분해기가 뚜렷한 목적을 가진 도메인 외 데이터 세트에 적용될 수 있는지 여부를 조사한다. 이를 테스트하기 위해 DROP 및 GSM8K에 대한 교차 도메인 평가를 수행하는데, 이는 솔버와 다른 전문 지식을 필요로 한다. 그 결과, 오라클 응답이 가능할 때 표 3에 제시되었으며, 놀랍게도, 증류된 분해자 \\(\\mathcal{S}_{D}\\)-\\(R\\)은 교사 GPT 모델을 분해자로 사용하는 것에 비해 성능이 약간 감소함에 따라 다른 도메인에 대한 좋은 일반화 및 범용성을 보여준다. 이와는 대조적으로, 솔버를 원 도메인 상에서 미세 조정되는 \\(\\mathcal{S}_{E}\\)-\\(A\\)로 치환할 때, 사용된 분해자에 관계없이 다른 도메인으로의 일반화가 불량하다. 교차 도메인 하위 질문 분해의 일부 예는 표 2에 나와 있다. 오라클 답변이 없는 시나리오에 대한 결과는 표 3과 일치한다.\n' +
      '\n' +
      '다음으로, 증류된 분해기가 다른 해결기에 적합하고 보편적으로 적합한지 여부를 검토한다. 그 결과는 표 4에서 볼 수 있다. \\(\\mathcal{S}_{D}\\)-\\(R\\)의 성능은 교사 분해자(GPT)의 성능과 비슷하며, 다른 해결기에 연결되었을 때 더 약한 분해자(Vicuna)에 비해 전반적으로 개선된 것을 보여준다. 우리는 증류 분해기로 업그레이드함으로써 약한 솔버가 강한 솔버에 비해 더 많은 성능 향상을 받는다는 것을 발견했다. 우리는 그 이유가 약한 해결자가 분해의 이점을 충분히 활용하지 못할 수 있다는 사실에 있다고 가정한다.\n' +
      '\n' +
      '## 6 Ablations\n' +
      '\n' +
      '우리는 다양한 지침에 대한 광범위한 평가와 부록 B의 시연 횟수의 영향에 대한 탐색을 제공한다.\n' +
      '\n' +
      '##7 관련 업무\n' +
      '\n' +
      'LLM Distillation Tremendous progress(Jiao et al., 2020; Sun et al., 2019; Li et al., 2021)는 BERT(Devlin et al., 2019) 또는 RoBERTa(Liu et al., 2019)와 같은 대규모 사전 훈련된 언어 모델을 압축하는 측면에서 이루어졌다. 생성 모델의 경우, 압축은 교사와 학생 분포 사이의 K-L 발산을 최소화함으로써 주로 달성된다(Sanh et al., 2019; Gu et al., 2023). 이러한 방법의 기초가 되는 중추적인 가정은 교사 모델의 구성 요소의 완전한 접근성이다. 그러나 가장 강력한 LLM은 블랙박스로 제한된 출력만 드러낸다. 이러한 제약을 감안할 때, 교사 모델에 의해 생성된 데이터를 직접 훈련하는 몇 가지 방법론이 등장했다(Chiang et al., 2023; Taori et al., 2023). 유사한 증류 전략을 따르지만 분해 능력 증류에 중점을 둔다.\n' +
      '\n' +
      'LLM 구동 에이전트의 계획 및 작업 분해 최근 LLM 구동 시스템의 발전으로 엔드 투 엔드 파이프라인을 생성할 수 있게 되었으며, 향상된 계획 및 메모리 기능을 사용하여 복잡한 작업을 완료할 수 있는 자율 에이전트 개발의 새로운 가능성을 열어주었다. ReAct(Yao et al., 2022), HuggingGPT(Shen et al., 2023), AutoGPT(Significant Gravitas, 2023), LangChain(Langchain-AI, 2023), GPT-Engineer(Anton Osika, 2023) 및 BabyAGI(Nakajima, 2023)와 같은 유망한 작업들이 이 분야에서 상당한 잠재력을 입증했다. 이러한 에이전트는 LLM에 의존하여 더 큰 작업을 더 관리 가능한 구성 요소로 분해합니다. 그 중, 일부 접근법들(_e.g._, HuggingGPT)은 먼저 LLM을 통해 완전한 계획을 생성하고 후속적으로 각각의 서브태스크를 태클함으로써 _static_ 계획 전략을 사용한다. 다른 접근법들(_e.g._, AutoGPT)은 _dynamic_ 및 _interactive_ 계획 전략을 채택하며, 여기서 각 액션의 생성은 이전 계획 단계들의 결과에 대해 조건화된다.\n' +
      '\n' +
      'LLM 추론 체인LLM은 최근 연구에 의해 입증된 바와 같이 명시적 추론 체인으로부터 이익을 얻을 수 있다(Wei et al., 2022; Zheng et al., 2023). CoT(Chain of Thought)(Wei et al., 2022) 기법은 복잡한 작업에 대한 모델 성능을 향상시키기 위한 표준이 되었다. Three of Thoughts(Yao et al., 2023)는 문제를 여러 개의 사고 단계로 분해하고 한 단계당 여러 개의 생각을 생성하여 트리 구조를 생성한다. LLM+P 접근법(Liu et al., 2023)은 긴-수평 계획(long-horizon planning)을 위한 외부 고전적 플래너를 통합하고 그 계획을 자연 언어로 다시 변환한다. 이론적 작업(Feng et al., 2023)은 회로 복잡도 이론을 이용하여 CoT가 작동하는 이유를 분석하였다. CoT가 없으면 직접적인 추론을 통해 동일한 성능을 얻기 위해서는 모델 크기가 엄청나게 커야 함을 보여준다.\n' +
      '\n' +
      '그러나 CoT 스타일의 추론은 이에 의해 제한된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \\hline\n' +
      '**Dataset: DROP** & **Models** & **Decomposed Sub-questions** \\\\ \\hline\n' +
      '**Premise \\(P\\)**: The Raiders stayed at home for a Week 16 duel with the Houston Texas.... The Texas tried to rally in the fourth quarter as Brown nailed a 40-yard field goal, yet the Raiders’ defense would shut down any possible attempt. & 1. Which teams played against each other? X \\\\ \\hline\n' +
      '**Premise \\(P\\)**: The Raiders stayed at home for a Week 16 duel with the Houston Texas.... The Texas tried to rally in the fourth quarter as Brown nailed a 40-yard field goal, yet the Raiders’ defense would shut down any possible attempt. & 1. How many field goals did the Raiders kick in the first half? 2. How many field goals did the Texas kick in the first half? \\\\ \\hline\n' +
      '**Question \\(Q\\)**: How many field goals did both teams kick in the first half? & \\(\\mathcal{S}_{D}\\)-7(DROP) _In-Domain_ & 1. How many field goals did the Raiders kick in the first half? 2. How many field goals did the Texas kick in the first half? \\\\ \\hline\n' +
      '**Dataset: GSM8K** & **Models** & **Decomposed Sub-questions** \\\\ \\hline \\hline\n' +
      '**Premise \\(P\\)**: Mark is a copy-editor. He edits an equal number of sentences each week for two different publishers, who each pay him a different rate per sentence. Publisher B pays Mark twice what Publisher A pays. Mark edits a total number of weeks, and Publisher A pays him 5 cents per sentence. & 1. What is the rate per sentence that Publisher B pays Mark for editing 1000 sentences? & 2. What is the total amount Publisher B pays Mark for editing 1000 sentences? \\\\ \\hline\n' +
      '**Premise \\(P\\)**: Mark is a copy-editor. He edits an equal number of sentences each week for two different publishers, who each pay him a different rate per sentence. Publisher B pays Mark twice what Publisher A pays. Mark edits a total number of 1000 sentences each week, and Publisher A pays him 5 cents per sentence. & 3. What is the total amount Publisher B pays Mark for editing 1000 sentences? \\\\ \\hline\n' +
      '**Question \\(Q\\)**: How much does Mark make in a week, in cents? & \\(\\mathcal{S}_{D}\\)-7(DROP) _Cross-Domain_ & 1. How many sentences does Mark edit each week for Publisher B? 2. How much does Mark make per sentence from Publisher B? \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: GSM8K 및 DROP 상의 각 방법으로부터의 분해된 서브퀘스트에 대한 예. \\ GSM(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)(GSM)과 \\(\\mathcal{S}_{D}\\)-\\(\\mathcal{T}\\)(DROP)은 각각 GSM8K와 DROP 데이터셋에서 \\(\\mathcal{T}\\)의 검증을 통해 증류한 학생 모델을 의미한다. X는 유용한 하위 질문이 아님을 나타냅니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline\n' +
      '**Decomposer Solver** & GPT GPT GPT GPT & **\\(\\mathcal{S}_{D}\\)-\\(R\\)** & GPT & **-** \\\\ \\hline Trained on & \\multicolumn{3}{c}{**Evaluation on DROP**} \\\\ \\cline{2-4} GSM8K & 55.73 & 51.05 & 7.98 & 17.22 \\\\ \\hline \\hline Trained on & \\multicolumn{3}{c}{**Evaluation on GSM8K**} \\\\ \\cline{2-4} DROP & 65.13 & 63.15 & 11.30 & 3.41 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 증류된 학생 분해기는 외부 도메인 데이터 세트에 대해 강력한 일반화를 보여준다.\n' +
      '\n' +
      '사실 그것은 종종 제공된 맥락 내 예 Zhou et al.(2022)의 범위를 넘어서는 문제들로 제대로 일반화되지 않는다. 이를 해결하기 위해 일부 연구에서는 LLMs에게 최소에서 최대 프롬프트 Zhou et al.(2022)에 따라 복잡한 질문을 하위 질문으로 분해하도록 요청했다. 다른 사람들은 자기-질문 방법을 사용하여 원래 질문 Press 등을 해결하는 데 도움이 되는 후속 질문을 이끌어냈다(2022). 우리의 작업은 작업 전반에 걸쳐 비용 효율적인 추론 및 일반화로 지평을 확장함으로써 이 연구 라인에 기여한다.\n' +
      '\n' +
      '문헌에서 질문 분해를 위해 널리 알려진 질문 분해 데이터세트 및 ApproachesA 데이터세트는 QDMR Wolfson et al.(2020)이다. 기본 질문을 해결하는 데 필수적인 하위 질문의 정렬된 목록을 포함합니다. 이전의 몇몇 연구들은 QDMR 데이터세트 Guo et al. (2022); Zhu et al. (2023)에 대한 질문 분해자들을 훈련시켰다. 대조적으로, 일부 연구는 QDMR에 의존하지 않고 고유하게 레이블이 지정된 데이터를 사용한다. 예를 들어, Min et al. (2019)는 질문 분해를 스팬 예측 문제로 재캐스트하고 400개의 라벨링된 질문 세트에 대해 그들의 모델을 훈련시켰다. 신뢰할 수 있는 분해 데이터를 얻는 것과 관련된 문제를 인식한 Perez et al.(2020)은 1차 질문과 분해 목적을 위해 채굴된 10M 잠재적인 하위 질문 사이의 유사성을 활용하는 비감독 분해 접근법을 도입했다. 우리의 접근법은 주석이 달린 하위 질문에 의존하지 않고 교사 모델에서만 분해력을 추출하기 때문에 앞서 언급한 방법론과 다르다.\n' +
      '\n' +
      'LLM을 Small 모델로 보완 LLM의 예측을 보완하기 위해 더 작고 작업별 모델의 잠재력을 강조한 연구가 있었다. Xu et al. (2023)은 이러한 태스크-특정 모델들에 의해 생성된 후보들이 분류 태스크들에 일차적으로 초점을 맞추어 LM에 공급되는 프레임워크를 탐색하였다. Welleck et al.(2022)은 LMs들에 의해 생성된 시퀀스들을 반복적으로 개선하기 위해 더 작은 모델을 트레이닝한다. Vernikos et al.(2023)은 LMs로부터 다수의 잘못된 출력을 수집하고 생성을 통일하기 위해 작은 올바른 모델을 사용하는 것이 오류를 상당히 감소시킬 수 있다는 것을 입증했다. 우리의 작업은 또한 대규모 LM의 최상의 성능을 활성화하기 위해 더 작은 분해자 모델을 개발하는 것으로 볼 수 있다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '우리의 조사는 분해와 해결 측면을 분리함으로써 추론 작업에 대한 LLM의 능력에 대한 세밀한 검토를 제공한다. 두 역량 모두 추론을 위해 중요하지만, 우리는 분해가 특정 지식에 덜 의존하므로 지상 진리 라벨의 가용성에 관계없이 증류 해결 능력에 비해 증류하기가 더 쉽다는 것을 보여준다. 또한, 증류된 분해기는 다양한 작업, 데이터 세트 및 실행자/해결자에 걸쳐 강력한 일반화 능력을 보여준다. 향후 작업을 위해 다양한 작업의 데이터를 사용하여 보편적인 분해자 모델을 훈련하고 분해자를 더욱 향상시키기 위해 강화 학습의 사용을 탐색하여 해결 결과로부터의 신호를 활용하는 것이 흥미로울 것이다. 향후 작업의 또 다른 가능한 방향은 LLM 구동 에이전트, 도구 사용 및 다중 회전 의사 결정을 포함한 다른 장거리 계획 작업에서 본 방법의 효과를 평가하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Decomposor** & **Solver** & GSM8K & DROP \\\\ \\hline \\multirow{3}{*}{GPT-3.5-Turbo} & Vicuna-13B & 28.0 & 33.78 \\\\  & GPT-3.5-Turbo & 66.0 & 59.38 \\\\  & GPT-4 & 90.5 & 77.60 \\\\ \\hline \\multirow{3}{*}{Vicuna-13B} & Vicuna-13B & 29.5 & 26.56 \\\\  & GPT-3.5-Turbo & 57.0 & 47.31 \\\\  & GPT-4 & 88.5 & 79.40 \\\\ \\hline \\multirow{3}{*}{\\(\\mathcal{S}_{D}\\)-\\(R\\)} & Vicuna-13B & 31.5 & 33.38 \\\\  & GPT-3.5-Turbo & 66.5 & 61.94 \\\\ \\cline{1-1}  & GPT-4 & 91.5 & 81.02 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 증류된 학생 분해기는 다른 해결기에 비해 일관된 개선을 보여준다. 약한 솔버는 더 많은 이득을 얻습니다.\n' +
      '\n' +
      '## 9 Limitation\n' +
      '\n' +
      '우리의 작업은 몇 가지 가정에 기반을 두고 있다. 먼저, 교사 모델은 질의를 효과적으로 분해할 수 있다고 가정한다. 둘째, 학생 모델은 교사 모델로부터 증류식 계획을 학습할 수 있는 능력을 가지고 있다고 가정한다. 마지막으로, 우리는 우리의 작업에 관련된 작업이 긴 지평선 계획 능력을 필요로 한다고 가정한다. 이러한 가정 중 하나가 사실이 아닌 경우 제안된 방법의 효과에 영향을 미친다.\n' +
      '\n' +
      '우리가 수학 및 QA 측면의 맥락에서 모델의 효과만을 평가했다는 점에 주목하는 것이 중요하다. 우리의 작업을 완전히 완료하기 위해서는 더 광범위한 계획 작업에 대한 모델을 평가하는 것이 필요할 것이다. 여기에는 도구 사용과 관련된 벤치마크, LLM 에이전트 및 다중턴 시나리오가 포함됩니다. 이러한 평가는 제안된 방법의 범용성과 적용 가능성을 검증하는 데 도움이 될 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*O. Osika(2023)GPT Engineer. [https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika] (https://github.com/AntonOsika/gpt-engineer/commits?author=AntonOsika). 깃허브 저장소입니다.\n' +
      '*T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020)Language models is few-shot learners. neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '*W. 장종 이종욱 임영식 성진 우현장 정승 장영 Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing(2023)Vicuna: 90%* chatgpt 품질의 오픈 소스 챗봇 인상 gpt-4. 외부 링크: 2009.1007 인용: SS1.\n' +
      '*W. 장종 이종욱 임영식 성진 우현장 정승 장영 Zhuang, J. E. Gonzalez, et al. (2023)Vicuna: a open-source chatbot impress gpt-4 with 90%* chatgpt quality. https://vicuna. lmsys. org (2023년 4월 14일에 접속) 외부 링크: 인용된 링크: SS1입니다.\n' +
      '*K. 코비, V 고사라주 바이에른 M. 천현준 카이저 플래퍼트, J 트워렉, J 힐튼, R. Nakano, et al.(2021)Training verifiers to solve math word problems. ArXiv:2110.14168. 인용: SS1.\n' +
      '* J. 데블린, M. 장경 이경호 Toutanova (2019)BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), Minneapolis, Minnesota, pp. 4171-4186. External Links: Link Cited by: SS1.\n' +
      '* D. Dua, Y. 왕대기, 지스타노프스키, S. 싱과 M Gardner (2019)DROP: 문단에 대한 이산 추론이 필요한 읽기 이해 벤치마크. In Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), Minneapolis, Minnesota, pp. 2368-2378. External Links: Link Cited by: SS1.\n' +
      '* G. Feng, Y. 구병장 왕(2023)은 이론적인 관점이라는 생각의 사슬 뒤에 숨겨진 미스터리를 드러내기 위한 것이다. ArXiv:2305.15408. 인용: SS1.\n' +
      '*Y. 구룡 동, F. Wei, M. 황(2023) 대용량 언어 모델의 지식 증류. ArXiv:2306.08543. 인용: SS1.\n' +
      '*X. 곽용 Li와 G. Haffari(2022)는 질문 분해를 통한 읽기 이해의 복잡성을 가지고 있다. In Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association, Adelaide, Australia, pp. 31-40. External Links: Link Cited by: SS1.\n' +
      '*X. 조영 음락 상욱 장욱 천락 리, F. Wang, Q. Liu(2020)TinyBERT: 자연 언어 이해를 위해 BERT를 증류하는 것. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online, pp. 4163-4174. External Links: Link Cited by: SS1.\n' +
      '* L. 임영식 린성호 렌, P. 리, J. Zhou, X. Sun(2021) 사전 학습된 언어 모델을 위한 동적 지식 증류. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic, pp. 379-389. External Links: Link Cited by: SS1.\n' +
      '* B. 류, Y. 장욱 장규 류승 Zhang, J. Biswas, and P. Stone(2023)Llm+p: empowering large language models with optimal planning proficiency. ArXiv:2304.11477. 인용: SS1.\n' +
      '*Y. 류민 Ott, N. 고열정 조시천 리비, M 루이스 제틀모이어와 V Stoyanov (2019)Roberta: 강건하게 최적화된 버트 사전 훈련 접근법. ArXiv:1907.11692. 인용: SS1.\n' +
      '*세원 민, 빅터 중, 루크 제틀모이어, 한난헤 하지시르지. 2019. 질문 분해 및 리스코링을 통한 멀티홉 읽기 이해. _Proceedings of the 57th Annual Meeting for Computational Linguistics_, pages 6097-6109, Florence, Italy. 컴퓨터 언어학과의 연관성\n' +
      '* 나카지마(2023) 요헤이 나카지마. 2023년 베이비기 깃허브 저장소입니다.\n' +
      '* OpenAI(2023) OpenAI. 2023. Gpt-4 기술보고서 _ ArXiv_, abs/2303.08774.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022, training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744의 발전.\n' +
      '* Perez et al. (2020) Ethan Perez, Patrick Lewis, Wen-tau Yih, KyungHyun Cho, and Douwe Kiela. 2020. 질문 응답을 위한 비감독 질문 분해. _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 8864-8880, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. 언어 모델에서의 조성 격차 측정 및 좁히기. _ arXiv preprint arXiv:2210.03350_.\n' +
      '* Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, distilled version of bert: Smaller, faster, cheaper and lighter. _ arXiv preprint arXiv:1910.01108_.\n' +
      '* Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugging-gpt: ai task with chatgpt and its friends in huggingface. _ arXiv preprint arXiv:2303.17580_.\n' +
      '* 그라비타 (2023) Significant Gravitas. 2023. Auto-gpt: Autonomous GPT-4 Experiment. 깃허브 저장소입니다.\n' +
      '* Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4323-4332, Hong Kong, China. 컴퓨터 언어학과의 연관성\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto. 2023. 스탠포드 알파카: 지시를 따르는 라마 모델.\n' +
      '* Taylor 등(2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. _ arXiv preprint arXiv:2211.09085_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Vernikos et al. (2023) Giorgos Vernikos, Arthur Brazinskas, Jakub Adamek, Jonathan Mallinson, Aliaksei Severyn, and Eric Malmi. 2023. 작은 언어 모델들은 그들의 출력들을 다시 쓰면서 거인들을 개선한다. _ arXiv preprint arXiv:2305.13514_.\n' +
      '* Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. Survey on the large language model based autonomous agents. _ arXiv preprint arXiv:2308.11432_.\n' +
      '* Wang et al. (2024) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. 실행가능 코드 액션들은 더 나은 llm 에이전트들을 이끌어낸다. _ arXiv preprint arXiv:2402.01030_.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837의 발전.\n' +
      '* Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. 자기 교정을 학습하여 시퀀스를 생성하는 단계 _ arXiv preprint arXiv:2211.00053_.\n' +
      '* Weng(2023) Lilian Weng. 2023. Llm 구동 자율 에이전트. 접속: 2024-02-13\n' +
      '* Wolfson et al. (2020) Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: question understanding benchmark. _ Computational Linguistics_, 8:183-198의 거래.\n' +
      '* Xu et al. (2023) Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. 소형 모델들은 대형 언어 모델들을 위한 귀중한 플러그-인. _ arXiv preprint arXiv:2305.08848_.\n' +
      '* Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. 생각의 나무: 큰 언어 모델을 사용한 숙고적 문제 해결 _ arXiv preprint arXiv:2305.10601_.\n' +
      '* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. 반응: 추론과 언어 모델에서의 연기의 시너지 효과 _ arXiv preprint arXiv:2210.03629_.\n' +
      '* Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. 매머드: 하이브리드 명령어 튜닝을 통한 수학 일반 모델 구축 arXiv preprint arXiv:2309.05653_.\n' +
      '* Zhang et al. (2022)Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompt improves reasoning in large language models. _ arXiv preprint arXiv:2304.09797_.\n' +
      '* Zhou et al. (2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. _ arXiv preprint arXiv:2205.10625_.\n' +
      '* Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. 검색 및 판독: 오픈 도메인 질의 응답에 대한 종합 조사_ arXiv preprint arXiv:2101.00774_.\n' +
      '* Zhu et al.(2023) Wang Zhu, Jesse Thomason, and Robin Jia. 2023. robust multistep question answering을 위한 latent answers를 갖는 Chain of-questions training. _ arXiv preprint arXiv:2305.14901_.\n' +
      '\n' +
      '## 부록 질문의 답변 제외\n' +
      '\n' +
      '우리는 수학적 추론을 포함하는 과제의 경우, 답은 일반적으로 어떤 형태의 계산을 필요로 하며, 단계적인 해결책을 필수적이라고 가정한다. 이것이 없으면, 수치값을 미세 조정 대상으로 설정하는 것은 거의 변함없이 실패로 귀결된다. 반대로 독해 데이터셋인 DROP는 제공된 텍스트에서 직접 답변의 상당 부분을 도출한다. 이러한 시나리오에서 하위 질문에 대한 답변을 포함하면 답변 분포를 방해할 위험이 있다.\n' +
      '\n' +
      '(I^{\\prime}_{\\text{ans}}\\)으로 표기된 풀이 지침은 (I^{\\text{ans}}\\)에 명시된 것과 동일하게 유지된다. 유일한 차이점은 미세 조정 대상과 다릅니다.\n' +
      '\n' +
      '## 분해교육을 위한 부록 B 절제에 관한 연구\n' +
      '\n' +
      '이전 연구에서는 프롬프트에 데모를 통합하면 대용량 언어 모델이 주어진 지침을 준수하는 능력을 크게 향상시킬 수 있음을 보여주었다. 표 6의 연구 결과는 이를 더욱 입증하여 단일 샷 시연을 포함하여 분해된 질문의 품질을 현저하게 향상시킨다는 것을 보여준다. 이러한 향상은 다양한 분해자 전반에 걸쳐 일관되게 관찰되었다.\n' +
      '\n' +
      '우리는 질문 분해에 사용된 지침에 초점을 맞춘 절제 연구를 수행했다. 우리의 목표는 결과적인 하위 질문이 실행자에게 유용한 단서로 작용하는 동시에 불필요한 정보를 도입하지 않도록 하는 것이다. 우리의 설계 근거의 핵심은 분해자가 생성해야 하는 최적의 하위 질문 수를 결정하는 것이다. 보다 구체적으로 제한 사항이 적용되지 않은 결과를 분석(I_{\\text{decomp}}\\)하여 시나리오와 비교했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Decomposor** & **Solver** & 0-shot & 1-shot \\\\ \\hline \\multirow{2}{*}{GPT-3.5-Turbo} & GPT-3.5-Turbo & 66.0 & 70.0 \\\\  & GPT-4 & 90.5 & 91.5 \\\\ \\hline \\multirow{2}{*}{Vicuna-13B} & GPT-3.5-Turbo & 57.0 & 61.5 \\\\  & GPT-4 & 88.5 & 91.5 \\\\ \\hline \\multirow{2}{*}{\\(\\mathcal{S}_{D}\\)-\\(R\\)} & GPT-3.5-Turbo & 66.5 & 67.5 \\\\  & GPT-4 & 91.5 & 91.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: GSM8K 데이터세트의 하위 집합에 대해 조사된 분해 명령에 데모를 포함하는 영향.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Instruction** &, EM & f1 \\\\ \\hline no restriction & 1 & 45.69 & 56.63 \\\\ no more than four & 1 & 46.40 & 57.19 \\\\ no more than three & 1 & 50.00 & **59.88** \\\\ no more than two & 1 & 46.89 & 58.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: DROP 데이터세트의 서브세트에 대한 분해 명령에서 서브퀘션의 최대 수를 제한하는 효과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & **Decomposer** & **Solver** & **Performance\\(\\uparrow\\)** \\\\ \\cline{2-4}  & Model & Model & GSM8K (EM) & DROP (F1) \\\\ \\hline \\multirow{2}{*}{_w/o oracle answer \\(A\\)_} & GPT & \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\)(Direct) & 5.46 & 53.17 \\\\  & GPT & \\(\\mathcal{S}_{E}\\)-\\(\\mathcal{T}\\) & 48.98 & 13.37 \\\\ \\hline \\multirow{2}{*}{_w/ oracle answer \\(A\\)_} & GPT & \\(\\mathcal{S}_{E}\\)-\\(A\\)(Direct) & 6.44 & 72.55 \\\\  & GPT & \\(\\mathcal{S}_{E}\\)-\\(A\\) & 51.55 & 20.34 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 목표에서 서브질문 \\(\\hat{A}_{i}^{s}\\}\\)에 대한 답변을 제외하면 DROP 데이터 세트에 비해 향상된 결과를 얻지만 GSM8K 데이터 세트에 비해 성능 저하를 초래한다.\n' +
      '\n' +
      '다양한 최대 하위 질문 수를 허용합니다. 이러한 조사의 결과는 표 7에 자세히 설명되어 있으며, 우리의 연구 결과는 "3개 이하의 하위 질문"의 캡이 가장 효과적인 결과를 산출한다는 것을 간결하게 보여준다.\n' +
      '\n' +
      '## 부록 C 예제 여기서 해결 모델이 서브퀘스트에 의해 혼란스러워지는 경우\n' +
      '\n' +
      '도 2에 예시된 바와 같이, 두 번째 질문까지, 솔버 모델은 "의복은 청색 섬유의 2 볼트가 필요하다" 및 "백색 섬유의 1 볼트가 필요할 것"이라고 정확하게 응답한다. 그럼에도 불구하고, 두 번째와 매우 유사한 세 번째 질문의 도입은 혼란을 초래한다. 결과적으로 모델은 초기 정확성에서 벗어나 이 하위 질문에 따라 오답으로 절정에 달한다.\n' +
      '\n' +
      '그림 2: 솔버 모델은 때때로 길을 잃습니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>