<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion\n' +
      '\n' +
      'Xueyi Liu\\({}^{1,3}\\) Li Yi\\({}^{1,2,3}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Tsinghua University \\({}^{2}\\)Shanghai AI Laboratory \\({}^{3}\\)Shanghai Qi Zhi Institute\n' +
      '\n' +
      'Project website: meowuu7.github.io/GeneOH-Diffusion\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, **GeneOH Diffusion**, incorporating two key designs: an innovative contact-centric HOI representation named GeneOH and a new domain-generalizable denoising scheme. The contact-centric representation GeneOH informatively parameterizes the HOI process, facilitating enhanced generalization across various HOI scenarios. The new denoising scheme consists of a canonical denoising model trained to project noisy data samples from a whitened noise space to a clean data manifold and a "denoising via diffusion" strategy which can handle input trajectories with various noise patterns by first diffusing them to align with the whitened noise space and cleaning via the canonical denoiser. Extensive experiments on four benchmarks with significant domain variations demonstrate the superior effectiveness of our method. GeneOH Diffusion also shows promise for various downstream applications.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Interacting with objects is an essential part of our daily lives, and accurately tracking hands during these interactions has become crucial for various applications, such as gaming, virtual and augmented reality, robotics, and human-machine interaction. Yet, this task is highly complex and ill-posed due to numerous factors like intricate dynamics involved and hand-object occlusions. Despite best efforts, existing tracking algorithms often struggle with producing plausible and realistic results.\n' +
      '\n' +
      'To better cater to the requirements of downstream tasks, noisy tracking results usually need to be refined. Given a hand-object interaction (HOI) sequence with errors, the HOI denoising aims to pro\n' +
      '\n' +
      'Figure 1: Trained only on limited data, **GeneOH Diffusion** can clean novel noisy interactions with new objects, hand motions, and unseen noise patterns (_Fig. (a)_), produces diverse refined trajectories with discrete manipulation modes (_Fig. (b)_), and is a practical tool for many applications (_Fig. (c)_).\n' +
      '\n' +
      'duce a natural interaction sequence free of artifacts such as penetrations. In this work, we assume the object poses are tracked accurately and focus on refining the hand trajectory following (Zhou et al., 2022; Grady et al., 2021; Zhou et al., 2021; Zhang et al., 2021). This setting is important with many practical demands in applications such as cleaning synthesized motions (Tendulkar et al., 2023; Huang et al., 2022; Ghosh et al., 2023; Wu et al., 2022), refining motion-retargeted trajectories (Hecker et al., 2008; Tak and Ko, 2005; Aberman et al., 2019), and virtual object manipulations (Oh et al., 2019; Kato et al., 2000; Shaer et al., 2010). Early approaches relied on manually designed priors (Dewaele et al., 2004; Hackenberg et al., 2011), which, however, proved inadequate in handling intricate noise. More recent endeavors have shifted towards learning denoising priors from data (Zhou et al., 2022; Zhou et al., 2021; Grady et al., 2021), yet the existing designs still fall short of providing a satisfactory solution.\n' +
      '\n' +
      'Leveraging data priors for HOI denoising is challenged by several difficulties. First, the interaction noise is highly complex, covering unnatural hand poses, erroneous hand-object spatial relations, and inconsistent hand-object temporal relations. Second, hand movements, hand-object relations, and the noise pattern may vary dramatically across different HOI tracks. For instance, the noise pattern exhibited in hand trajectories estimated from videos differs markedly from that resulted from inaccurate capturing or annotations. A denoising model is often confronted with such out-of-domain data and is expected to handle them adeptly. However, such a distribution shift poses a substantial challenge for data-driven models. Lacking an effective solution, prior works always cannot clean such complex interaction noise or can hardly generalize to unseen erroneous interactions.\n' +
      '\n' +
      'We propose **GeneOH Diffusion**, a powerful denoising method with strong generalizability and practical applicability (see Figure 1), to tackle the above difficulties. Our method resolves the challenges around two key ideas: 1) designing an effective HOI representation that can both informatively parameterize the interaction and facilitate the generalization by encoding and canonicalizing vital HOI information in a coordinate system induced by the interaction region; 2) learning a canonical denoiser that projects noisy data from a whitened noise space to the data manifold for domain-generalizable denoising. A satisfactory representation that parameterizes the high-dimensional HOI process for denoising should be able to represent the interaction process faithfully, highlight noises, and align different HOI tracks well to enhance generalization capabilities Therefore, we introduce **GeneOH**, **Generalized contact-centric Hand-**O**bject spatial and temporal relations. GeneOH encodes the interaction informatively, encompassing the hand trajectory, hand-object spatial relations, and hand-object temporal relations. Furthermore, it adopts a contact-centric perspective and incorporates an innovative canonicalization strategy. This approach effectively reduces disparities between different sequences, promoting generalization across diverse HOI scenarios. To enhance the denoising model\'s generalization ability to novel noise distributions, our second effort centers on the denoising scheme side. We propose to learn a canonical denoising model that describes the mapping from a whitened noise space to the data manifold. The whitened noise space contains noisy data diffused from clean data in the training dataset via Gaussian noise at various noise scales. With the canonical denoiser, we then leverage a "denoising via diffusion" strategy to handle input trajectories with various noise patterns in a domain-generalizable manner. It first aligns the input to the whitened noise space by diffusing it via Gaussian noise. Subsequently, the diffused sample is cleaned by the canonical denoising model. To strike a balance between the denoising model\'s generalization capability and the faithfulness of the denoised trajectory, we introduce a hyper-parameter that decides the scale of noise added during the diffusion process, ensuring the diffused sample remains faithful to the original input. Furthermore, instead of learning to clean the interaction noise through a single stage, we devise a progressive denoising strategy where the input is sequentially refined via three stages, each of which concentrates on cleaning one specific component of GeneOH.\n' +
      '\n' +
      'We conduct extensive experiments on three datasets, GRAB (Taheri et al., 2020), a high-quality MoCap dataset, HOI4D (Liu et al., 2022), a real-world interaction dataset with noise resulting from inaccurate depth sensing and imprecise vision estimations, and ARCTIC (Fan et al., 2023), a dataset featuring dynamic motions and changing contacts, showing the remarkable effectiveness and generalizability of our method. When only trained on GRAB, our denoiser can generalize to HOI4D with novel and difficult noise patterns and ARCTIC with challenging interactions, surpassing prior arts by a significant margin, as demonstrated by the comprehensive quantitative and qualitative comparisons. We will release our code to support future research. In summary, our contributions include:\n' +
      '\n' +
      '* An HOI denoising framework with powerful spatial and temporal denoising capability and unprecedented generalizability to novel HOI scenarios;* An HOI representation named GeneOH that can faithfully capture the HOI process, highlight unnatural artifacts, and align HOI tracks across different objects and interactions;\n' +
      '* An effective and domain-generalizable denoising method that can both generalize across different noise patterns and clean complex noise through a progressive denoising strategy.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'Hand-object interaction is an important topic for understanding human behaviors. Prior works towards this direction mainly focus on data collection (Taheri et al., 2020; Hampali et al., 2020; Guzov et al., 2022; Fan et al., 2023; Kwon et al., 2021), reconstruction (Tiwari et al., 2022; Xie et al., 2022; Qu et al., 2023; Ye et al., 2023), interaction generation (Wu et al., 2022; Tendulkar et al., 2023; Zhang and Tang, 2022; Ghosh et al., 2023; Li et al., 2023), and motion refinement (Zhou et al., 2022; Grady et al., 2021; Zhou et al., 2021b; Nunez, 2022). The HOI denoising task wishes to remove unnatural phenomena from HOI sequences with interaction noise. In real application scenarios, a denoising model would frequently encounter out-of-domain interactions, and is expected to generalize to them. This problem is then related to domain generalization, a general machine learning topic (Sicilia et al., 2023; Segu et al., 2023; Wang et al., 2023; Zhang et al., 2023; Jiang et al., 2022; Wang et al., 2022; Blanchard et al., 2011; Muandet et al., 2013; Dou et al., 2019), where a wide range of solutions have been proposed in the literature. Among them, leveraging domain invariance to solve the problem is a promising solution. Our work is related to this kind of approach, at a high level. However, what is the domain invariant information for the HOI denoising task, and how to encourage the model to leverage such information for denoising remains very tricky. We focus on designing invariant representations and learning a canonical denoiser for domain-generalizable denoising. Moreover, we are also related to intriguing works that wish to leverage data priors to solve the inverse problem (Song et al., 2023; Mardani et al., 2023; Tumanyan et al., 2023; Meng et al., 2021; Chung et al., 2022). For our task, we need to answer some fundamental questions regarding what are generalizable denoising priors, how to learn them from data, and how to leverage the prior to refine noisy input from different distributions. We\'ll illustrate our solution in the method section.\n' +
      '\n' +
      '## 3 Hand-Object Interaction Denoising via Denoising Diffusion\n' +
      '\n' +
      'Given an erroneous hand-object interaction sequence with \\(K\\) frames \\((\\hat{\\mathcal{H}},\\mathbf{O})=\\{(\\hat{\\mathbf{H}}_{k},\\mathbf{O}_{k})\\}_{k=1 }^{K}\\), we assume the object pose trajectory \\(\\{\\mathbf{O}_{k}\\}_{k=1}^{K}\\) is accurate following (Zhou et al., 2022; Zhou et al., 2021; Grady et al., 2021; Zhang et al., 2021) and aim at cleaning the noisy hand trajectory \\(\\{\\hat{\\mathbf{H}}_{k}\\}_{k=1}^{K}\\). This setting is of considerable importance, given its practical applicability in various domains (Tendulkar et al., 2023; Ghosh et al., 2023; Li et al., 2023; Wu et al., 2022; Hecker et al., 2008; Oh et al., 2019; Shaer et al., 2010). The cleaned hand trajectory should be free of unnatural hand poses, incorrect spatial penetrations, and inconsistent temporal hand-object relations. The hand trajectory should present visually consistent motions and adequate contact with the object to support manipulation. The problem is ill-posed in nature owing to the difficulties posed by complex interaction noise and the substantial domain gap across different interactions resulting from new objects, hand movements, and unseen noise patterns.\n' +
      '\n' +
      'We resolve the above difficulties by 1) designing a novel HOI representation that parameterizes the HOI process faithfully and can both simplify the distribution of complex HOI and foster the model generalization across different interactions (Section 3.1) and 2) devising an effective denoising scheme that can both clean complex noises through a progressive denoising strategy and generalize across different input noise patterns (Section 3.2).\n' +
      '\n' +
      '### GeneOH : Generalized Contact-Centric Hand-Object Spatial and Temporal Relations\n' +
      '\n' +
      'Designing an effective and generalizable HOI denoising model requires a serious effort in the representation design. It involves striking a balance between expressive modeling of the interaction with objects and supporting the model\'s generalization to new objects and interactions. The ideal HOI representation should accurately capture the interaction process, highlight any unusual phenomena like spatial penetrations, and facilitate alignment across diverse interaction sequences.\n' +
      '\n' +
      'We introduce GeneOH to achieve this. It integrates the hand trajectory, hand-object spatial relations, and hand-object temporal relations to represent the HOI process faithfully. An effective normalization strategy is further introduced to enhance alignment across diverse interactions. The hand trajectory and the object trajectory are compactly represented as the trajectory of hand keypoints, denoted as \\(\\mathcal{J}=\\{\\mathbf{J}_{k}\\}_{k=1}^{K}\\), and the interaction region sequence: \\(\\mathcal{P}=\\{\\mathbf{P}_{k}\\}_{k=1}^{K}\\), in a contact-aware manner. We will then detail the design of GeneOH.\n' +
      '\n' +
      '**Generalized contact points.** The interaction region is established based on points sampled from the object surface close to the hand trajectory, referred to as "generalized contact points". They are \\(N_{o}\\) points (denoted as \\(\\mathbf{P}\\in\\mathbb{R}^{N_{o}\\times 3}\\)) sampled from object surface points, whose distance to the hand trajectory does not exceed a threshold value of \\(r_{c}\\) (set to 5mm). The sequence of these points across all frames is represented by \\(\\mathcal{P}=\\{\\mathbf{P}_{k}\\}_{k=1}^{K}\\), where \\(\\mathbf{P}_{k}\\) denotes points at frame \\(k\\). Each \\(\\mathbf{P}_{k}\\) is associated with a 6D pose, consisting of the object\'s orientation (or the orientation of the first part for articulated objects), denoted as \\(\\mathbf{R}_{k}\\), and the center of \\(\\mathbf{P}_{k}\\), denoted as \\(\\mathbf{t}_{k}\\).\n' +
      '\n' +
      '**Canonicalized hand trajectories.** We include hand trajectories in our representation to effectively model hand movements. Specifically, we leverage hand keypoints to model the hand, as they offer a compact and expressive representation. We represent the hand trajectory as the sequence of 21 hand keypoints, denoted as \\(\\mathcal{J}=\\{\\mathbf{J}_{k}\\in\\mathbb{R}^{N_{h}\\times 3}\\}_{k=1}^{K}\\), where \\(N_{h}=21\\). We further canonicalize the hand trajectory \\(\\mathcal{J}\\) using the poses of the generalized contact points to eliminate the influence of object poses, resulting in the canonicalized hand trajectory in GeneOH : \\(\\mathcal{\\bar{J}}=\\{\\mathbf{\\bar{J}}_{k}=(\\mathbf{J}_{k}-\\mathbf{t}_{k}) \\mathbf{R}_{k}^{T}\\}_{k=1}^{K}\\).\n' +
      '\n' +
      '**Generalized contact-centric hand-object spatial relations.** We further introduce a hand-object spatial representation in GeneOH. The representation is based on hand keypoints and generalized contact points to inherit their merits. The spatial relation centered at each generalized contact point \\(\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\) comprises the relative offset from \\(\\mathbf{o}_{k}\\) to each hand keypoint \\(\\mathbf{h}_{k}\\in\\mathbf{J}_{k}\\), _i.e.,_\\(\\{\\mathbf{h}_{k}-\\mathbf{o}_{k}|\\mathbf{h}_{k}\\in\\mathbf{J}_{k}\\}\\), the object point normal \\(\\mathbf{n}_{k}\\), and the object point position \\(\\mathbf{o}_{k}\\). These statistics are subsequently canonicalized using the 6D pose of the generalized contact points to encourage cross-interaction alignment. Formally, the spatial representation centered at \\(\\mathbf{o}_{k}\\) is defined as: \\(\\mathbf{s}_{k}^{\\mathbf{o}}=((\\mathbf{o}_{k}-\\mathbf{t}_{k})\\mathbf{R}_{k}^{T },\\mathbf{n}_{k}\\mathbf{R}_{k}^{T},\\{(\\mathbf{h}_{k}-\\mathbf{o}_{k})\\mathbf{R} _{k}^{T}|\\mathbf{h}_{k}\\in\\mathbf{J}_{k}\\})\\). The spatial relation \\(\\mathcal{S}\\) is composed of \\(\\mathbf{s}_{k}^{\\mathbf{o}}\\) at each generalized contact point: \\(\\mathcal{S}=\\{\\{\\mathbf{s}_{k}^{\\mathbf{o}}|\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\} _{k=1}^{K}\\). By encoding object normals and hand-object relative offsets, \\(\\mathcal{S}\\) can reveal unnatural hand-object spatial relations such as penetrations.\n' +
      '\n' +
      '**Generalized contact-centric hand-object temporal relations.** Considering the limitations of the above two representations in revealing temporal errors such as incorrect manipulations resulting from inconsistent hand-object motions, we further introduce hand-object temporal relations to parameterize the HOI temporal information explicitly. We again take hand keypoints \\(\\mathbf{J}\\) to represent hand shape and generalized contact points \\(\\mathbf{P}\\) for the object shape to take advantage of their good ability in supporting generalization. The temporal relations encode the relative velocity between each hand point \\(\\mathbf{o}_{k}\\) and each hand keypoint \\(\\mathbf{h}_{k}\\) at frame \\(k\\) (\\(\\mathbf{v}_{k}^{\\mathbf{ho}}=\\mathbf{v}_{k}^{\\mathbf{h}}-\\mathbf{v}_{k}^{ \\mathbf{o}}\\)), the Euclidean distance between each pair of points (\\(d_{k}^{\\mathbf{ho}}=\\|\\mathbf{h}_{k}-\\mathbf{o}_{k}\\|_{2}\\)), and the object velocity \\(\\mathbf{v}_{k}^{\\mathbf{o}}\\) in the representation, as illustrated in Figure 2. We further introduce two statistics by using the object point normal to canonicalize \\(\\mathbf{v}_{k}^{\\mathbf{ho}}\\), resulting in two normalized statistics: \\(\\mathbf{v}_{k,\\perp}^{\\mathbf{ho}}\\), orthogonal to the object tangent plane, and \\(\\mathbf{v}_{k,\\perp}^{\\mathbf{ho}}\\), lying in the object\'s tangent plane, and encoding them with hand-object relative distances: \\(e_{k,\\perp}^{\\mathbf{ho}}=e^{-k\\cdot d_{k}^{\\mathbf{ho}}}k_{b}\\|\\mathbf{v}_{k, \\perp}^{\\mathbf{ho}}\\|_{2}\\) and \\(e_{k,\\parallel}^{\\mathbf{ho}}=e^{-k\\cdot d_{k}^{\\mathbf{ho}}}k_{a}\\|\\mathbf{v}_{k, \\parallel}^{\\mathbf{ho}}\\|_{2}\\). Here, \\(k\\), \\(k_{a}\\), and \\(k_{b}\\) are positive hyper-parameters, and the term \\(e^{-k\\cdot d_{k}^{\\mathbf{ho}}}\\) is negatively related to the distance between the hand and object points. This canonicalization and encoding strategy aims to encourage the model to learn different denoising strategies for the two types of relative velocities, enhance cross-interaction generalization by factoring out object poses, and emphasize the relative movement between very close hand-object point pairs. The temporal representation \\(\\mathcal{T}\\) is defined by combining the above statistics of each hand-object point pair across all frames together:\n' +
      '\n' +
      '\\[\\mathcal{T}=\\{\\{\\mathbf{v}_{k}^{\\mathbf{o}},\\{d_{k}^{\\mathbf{ho}},\\mathbf{v}_{k }^{\\mathbf{ho}},e_{k,\\parallel}^{\\mathbf{ho}}|\\mathbf{h}_{k}\\in\\mathbf{J}_{k} \\}\\}\\}|\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\}_{k=1}^{K-1}. \\tag{1}\\]\n' +
      '\n' +
      'It reveals temporal errors by encoding object velocities, hand-object distances and relative velocities.\n' +
      '\n' +
      'Figure 2: Three components of **GeneOH**.\n' +
      '\n' +
      '**The GeneOH representation.** The overall representation, GeneOH, comprises the above three components, as defined formally: GeneOH \\(=\\{\\mathcal{\\tilde{J}},\\mathcal{S},\\mathcal{T}\\}\\). Figure 2 illustrates the design. It faithfully captures the interaction process, can reveal noise by encoding corresponding statistics, and benefits the generalization by employing carefully designed canonicalization strategies. Inspecting back into previous works, TOCH (Zhou et al., 2022) does not explicitly parameterize the hand-object temporal relations or hand shapes and does not carefully consider the spatial canonicalization to facilitate the generalization, which limits its denoising capability and may lead to the loss of high-frequency hand pose details. ManipNet (Zhang et al., 2021) does not encode temporal relations and does not incorporate contact-centric canonicalization, rendering it inadequate for capturing the interaction process and less effective for generalization purposes.\n' +
      '\n' +
      '### GeneOH Diffusion: Progressive HOI Denoising via Denoising Diffusion\n' +
      '\n' +
      'While GeneOH excels in encoding the interaction process faithfully, highlighting errors to facilitate denoising, and reducing the disparities among various interaction sequences, designing an effective denoising model is still challenged by complex interaction noise, even from a distribution unseen during training. Previous methods typically employ pattern-specific denoising models trained to map noisy data restricted to certain patterns to the clean data manifold (Zhou et al., 2022; 2021). However, these methods are susceptible to overfitting, resulting in conceptually incorrect results when faced with interactions with unseen noise patterns, as evidenced in our experiments.\n' +
      '\n' +
      '```\n' +
      '0: forward diffusion function Diffuse\\((.,t)\\), the denoising model \\(\\text{denoise}(\\cdot,t)\\), input noisy point \\(\\hat{x}\\), diffusion steps \\(t_{\\text{diff}}\\).\n' +
      '0: denoised data \\(x\\).\n' +
      '1:functionDenoise\\(\\hat{x}^{t_{\\text{diff}}}\\), \\(t_{\\text{diff}}\\)\n' +
      '2:for\\(t\\) from \\(t_{\\text{diff}}\\) to \\(\\mathbf{1}\\)do\n' +
      '3:\\(\\hat{x}^{t-1}\\sim\\text{denoise}(\\hat{x}^{t},t)\\)\n' +
      '4:return\\(\\hat{x}^{0}\\)\n' +
      '5:\\(\\hat{x}\\leftarrow\\)Diffuse(\\(\\hat{x},t_{\\text{diff}}\\))\n' +
      '6:return\\(x\\leftarrow\\text{Denoise}(\\hat{x},t_{\\text{diff}})\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Denoising via Diffusion\n' +
      '\n' +
      'To ease the challenge posed by novel interaction noise, we propose a new denoising paradigm that learns a canonical denoising model and leverages it for domain-generalizable denoising. It describes the mapping from noisy data at various noise scales from a whitened noise space to the data manifold. The whitened noise space is populated with noisy data samples diffused from the clean data via a _diffusion process_ which gradually adds Gaussian noise to the data according to a variance schedule, a similar flavor to the forward diffusion process in diffusion-based generative models (Song et al., 2020; Ho et al., 2020; Rombach et al., 2022; Dhariwal and Nichol, 2021). With the canonical denoiser, we then leverage a "denoising via diffusion" strategy to handle input trajectories with various noise patterns in a generalizable manner. It first diffuses the input trajectory \\(\\hat{x}\\) via the diffusion process to another sample \\(\\hat{x}\\) that resides closer to the whitened noise space. Then the model projects the diffused sample \\(\\hat{x}\\) to the data manifold. To balance the generalization ability of the denoising and the fidelity of the denoised result to the input, the diffused \\(\\tilde{x}\\) needs to be faithful to the input \\(\\hat{x}\\). We then introduce a diffusion timestep \\(t_{\\text{diff}}\\) that decides how many diffusion steps are added. The process is visually depicted in the right part of Figure 3. Details are outlined in Algorithm 1. We also implement the denoising model\'s function and the training as those of the score functions in diffusion-based generative models. It is a multi-step stochastic denoiser that eliminates the noise of the input gradually to zero step-by-step. This way the denoiser can deal with noise at different scales flexibly and can give multiple solutions for the ill-posed ambiguous denoising problem.\n' +
      '\n' +
      'Based on the domain-generalizable denoising strategy, designing a single data-driven model to clean heterogeneous interaction noise in one stage is still not feasible. The interaction noise contains various kinds of noise at nonuniform scales stemming from different reasons. Thus the corresponding\n' +
      '\n' +
      'Figure 3: The **progressive HOI denoosing** gradually cleans the input noisy trajectory through three stages. Each stage concentrates on refining the trajectory by denoising a specific part of GeneOH via a _canonical denoiser_ through the _“denoising via diffusion”_ strategy.\n' +
      '\n' +
      'noise-to-data mapping is very high dimensional and is very challenging to learn from limited data. A promising solution to tackle the complexity is taking a progressive approach and learning multiple specialists, each concentrating on cleaning a specific type of noisy information. However, the multi-stage formulation brings new difficulties. It necessitates careful consideration of the information to be cleaned at each stage to prevent the current stage from compromising the naturalness achieved in previous stages. Fortunately, our design of the GeneOH representation facilitates a solution to this issue. HOI information can be represented into three relatively homogeneous parts: \\(\\mathcal{\\tilde{J}}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{T}\\). Furthermore, their relations ensure the sequential refinement of the hand trajectory by denoising its \\(\\mathcal{\\tilde{J}}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{T}\\) representations across three stages can avoid the undermining problem. A formal proof of this property is provided in the Appendix A.2.\n' +
      '\n' +
      '**Progressive HOI denoising.** We design a three-stage denoising approach (outlined in Figure 3), each stage dedicated to cleaning one aspect of the representation: \\(\\mathcal{\\tilde{J}}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{T}\\), respectively. In each stage, a canonical denoising model is learned for the corresponding representation, and the denoising is carried out using the "denoising via diffusion" strategy. Given the input \\(\\text{GeneOH}^{\\text{input}}=\\{\\mathcal{\\tilde{J}}^{\\text{input}},\\mathcal{ \\tilde{S}}^{\\text{input}},\\mathcal{\\tilde{T}}^{\\text{input}}\\}\\), the first denoising stage, named **MotionDiff**, denoises the noisy canonical hand trajectory \\(\\mathcal{\\tilde{J}}^{\\text{input}}\\) to \\(\\mathcal{\\tilde{J}}^{\\text{stage}_{1}}\\). One stage-denoised hand trajectory \\(\\mathcal{J}^{\\text{stage}_{1}}\\) can be easily computed by de-canonicalizing \\(\\mathcal{\\tilde{J}}^{\\text{stage}_{1}}\\) using object poses. GeneOH\\({}^{\\text{input}}\\) can also be updated accordingly into GeneOH\\({}^{\\text{stage}_{1}}=\\{\\mathcal{\\tilde{J}}^{\\text{stage}_{1}},\\mathcal{\\tilde{J}}^{\\text{ stage}_{1}},\\mathcal{\\tilde{J}}^{\\text{stage}_{1}}\\}\\). Then the second stage, named **SpatialDiff**, denoises the noisy spatial relation \\(\\mathcal{\\tilde{S}}^{\\text{stage}_{1}}\\) to \\(\\mathcal{\\tilde{S}}^{\\text{stage}_{2}}\\). Two stages-denoised hand trajectory \\(\\mathcal{J}^{\\text{stage}_{2}}\\) can be transformed from the hand-object relative offsets in \\(\\mathcal{\\tilde{S}}^{\\text{stage}_{2}}\\). \\(\\mathcal{J}^{\\text{stage}_{2}}=\\text{Average}((\\mathbf{h}_{k}-\\mathbf{o}_{k})+ \\mathbf{o}_{k}|\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\}\\). Following this, GeneOH\\({}^{\\text{stage}_{1}}\\) will be updated to GeneOH\\({}^{\\text{stage}_{2}}=\\{\\mathcal{\\tilde{J}}^{\\text{stage}_{2}},\\mathcal{\\tilde{S}}^{\\text{ stage}_{2}},\\mathcal{\\tilde{J}}^{\\text{stage}_{2}}\\}\\). Finally the last stage, named **TemporalDiff**, denoises \\(\\mathcal{\\tilde{T}}^{\\text{stage}_{2}}\\) to \\(\\mathcal{T}^{\\text{stage}_{3}}\\). Since temporal information such as relative velocities is redundantly encoded in \\(\\mathcal{T}\\), we compute the three stages-denoised hand trajectory \\(\\mathcal{J}^{\\text{stage}_{3}}\\) by optimizing \\(\\mathcal{J}^{\\text{stage}_{2}}\\) so that its induced temporal representation aligns with \\(\\mathcal{T}^{\\text{stage}_{3}}\\). And we take \\(\\mathcal{J}^{\\text{stage}_{3}}\\) as the final denoising output, denoted as \\(\\mathcal{J}\\). Each stage would not undermine the naturalness achieved after the previous stages, as proved in the Appendix A.2.\n' +
      '\n' +
      '**Fitting for a hand mesh trajectory.** With the denoised trajectory \\(\\mathcal{J}\\) and the object trajectory, a parameterized hand sequence represented via MANO parameters \\(\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k=1}^{K}\\) are optimized to fit \\(\\mathcal{J}\\) well. Details are illustrated in the Appendix A.3.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We conduct extensive experiments to demonstrate the effectiveness of our method. We train all models on the same training dataset and introduce four four test sets with different levels of domain shift to assess their denoising ability and the generalization ability (see Section 4.2). Moreover, we demonstrate the ability of our denoising method to produce multiple reasonable solutions for a single input in Section 4.3. At last, we show various applications that we can support (Section 4.4). _Another series of experiments using a different training set_ is presented in the Appendix B.1.\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '**Training datasets.** All models are trained on the GRAB dataset (Taheri et al., 2020). We follow the cross-object splitting strategy used in TOCH (Zhou et al., 2022) and train models on the training set. Our denoising model only requires ground-truth sequences for training. For those where the noisy counterparts are demanded, we perturb each sequence by adding Gaussian noise on the hand MANO translation, rotation, and pose parameters with standard deviations set to 0.01, 0.1, 0.5 respectively.\n' +
      '\n' +
      '**Evaluation datasets.** We evaluate our model and baselines on four distinct test sets, namely GRAB test set with Gaussian noise, GRAB (Beta) test set with noise sampled from a Beta distribution (\\(B(8,2)\\)), HOI4D dataset (Liu et al., 2022) with real noise patterns resulting from depth sensing errors and inaccurate pose estimation algorithms, and ARCTIC dataset (Fan et al., 2023) with Gaussian noise but containing challenging bimanual and dynamic interactions with changing contacts. Noisy trajectories with synthetic noise are created by adding noise sampled from corresponding distributions to the MANO parameters.\n' +
      '\n' +
      '**Metrics.** We introduce two sets of evaluation metrics. The first set focuses on assessing the model\'s ability to recover GT trajectories from noisy inputs following previous works (Zhou et al., 2022),including _Mean Per-Joint/Vertex Position Error (MPJPE/MPVPE)_, measuring the average distance between the denoised hand joints or vertices and the corresponding GT positions and _Contact IoU (C-IoU)_ assessing the similarity between the contact map induced by denoised trajectory and the GT. The second set quantifies the quality of denoised results, including _Solid Intersection Volume (IV)_ and _Penetration Depth_, measuring penetrations, _Proximity Error_, evaluating the difference of the hand-object proximity between the denoised trajectory and the GT, and _HO Motion Consistency_, assessing the hand-object motion consistency. Detailed calculations are presented in the Appendix C.2.\n' +
      '\n' +
      '**Baselines.** We compare our model with the prior art on the HOI denoising problem, TOCH (Zhou et al., 2022). A variant named "TOCH (w/ MixStyle)" is further created by combining TOCH with a general domain generalization method MixStyle (Zhou et al., 2021). Another variant, "TOCH (w/ Aug.)", where TOCH is trained on the training sets of the GRAB and GRAB (Beta), is further introduced to enhance its robustness towards unseen noise patterns.\n' +
      '\n' +
      '**Evaluation settings.** When evaluating our model, we select the trajectory that is _closest to the input noisy trajectory_ from 100 randomly sampled denoised trajectories using seeds from 0 to 99. For deterministic denoising models, we report the performance on a single run. Since our model can give multiple solutions for a single input, we additionally report the performance of our model in the form of _average with standard deviations in the Appendix_ on the second metric set measuring quality.\n' +
      '\n' +
      '### HOI Denoising\n' +
      '\n' +
      'We evaluated our model and compared it with previous works on four test sets: GRAB, GRAB (Beta), HOI4D, and ARCTIC. In the GRAB test set, all objects were unseen during training, resulting in a shift in the interaction distribution. In the GRAB (Beta) test set, the object shapes, interaction patterns, and noise patterns differ from those in the training set. The HOI4D dataset includes interaction sequences with novel objects and unobserved interactions, along with real noise\n' +
      '\n' +
      'Figure 4: **Qualitative comparisons. Please refer to our website and video for animated results. different ways to resolve penetrations diverse grasping pears different manipulation strategies\n' +
      '\n' +
      'Figure 5: **Stochastic denoising** can produce diverse results with discrete modes.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '**Refining noisy retargeted hand motions.** In the right part of Figure 6, we showcase the application of our denoising model in cleaning noisy retargeted hand trajectories. Our model excels at resolving penetrations present in the sequence resulting from direct retargeting. In contrast, TOCH\'s result still suffers from noticeable penetrations.\n' +
      '\n' +
      '## 5 Ablation Study\n' +
      '\n' +
      '**Generalized contact-centric parameterizations.** GeneOH leverages generalized contact points to normalize the hand-object relations. To assess the effectiveness of this design, We create an ablated model named "Ours (w/o Canon.)", which uses points sampled from the entire object surface for parameterizing. From Table 2, we can observe that our design on parameterizing around the interaction region can successfully improve the model\'s generalization ability towards unseen interactions.\n' +
      '\n' +
      '**Denoising via diffusion.** To further investigate the impact of the "denoising via diffusion" strategy on enhancing the model\'s generalization ability, we ablate it by replacing the denoising model with an autoencoder structure. The results are summarized in Table 2. Besides, the comparisons between "Ours (w/o Diffusion)" and TOCH highlight the superiority of our representation GeneOH as well.\n' +
      '\n' +
      '**Hand-object spatial and temporal denoising.** We propose a progressive denoising strategy composed of three stages to clean the complex interaction noise. This multi-stage approach is crucial, as a single denoising stage would fail to produce reasonable results in the presence of complex interaction noise. To validate the effectiveness of the stage-wise denoising, we created two ablated versions: a) "Ours (w/o TemporalDiff)" by removing the temporal denoising module, and b) "Ours (w/o SpatialDiff)" by removing both the temporal and spatial denoising modules. Figure 7 and Table 2 demonstrate their effectiveness in removing unnatural hand-object penetrations and enforcing consistent hand-object motions.\n' +
      '\n' +
      '_More quantitative and qualitative results for ablation studies_ are included in the Appendix B.2.\n' +
      '\n' +
      '## 6 Conclusion and Limitations\n' +
      '\n' +
      'In this work, we propose GeneOH Diffusion to tackle the generalizable HOI denoising problem. We resolve the challenge by 1) designing an informative HOI representation that is friendly for generalization, and 2) learning a canonical denoising model for domain-generalizable denoising. Experiments demonstrate our high denoising capability and generalization ability.\n' +
      '\n' +
      '**Limitations.** The main limitation lies in the assumption of accurate object pose trajectories. It may not hold if the HOI sequences are estimated from in-the-wild videos. Refining object poses and hand poses at the same time is a valuable and practical research direction.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{Method} & TV & Penetration Depth & HO Module \\\\  & (\\(\\text{cm}^{-1}\\)) & (\\(\\text{mm}^{-1}\\)) & Consistency (\\(\\text{mm}^{-1}\\)) \\\\ \\hline Input & 2.26 & 2.47 & 46.45 \\\\ \\hline Ours (w/o SpatialDiff) & 2.94 & 3.45 & 31.67 \\\\ Ours (w/o TemporalDiff) & **2.72** & **1.00** & 34.25 \\\\ Ours (w/o Diffusion) & 3.16 & 3.83 & 18.65 \\\\ Ours (w/o Comm.) & 2.36 & 3.57 & 12.66 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Ablation studies** on the HOI4D dataset,\n' +
      '\n' +
      'Figure 6: **Applications** on refining **noisy hand trajectories estimated from videos** (left) and cleaning **retargeted hand trajectories** (right).\n' +
      '\n' +
      'Figure 7: Effectiveness of the **SpatialDiff** and **TemporalDiff** stages.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Aberman, R. Wu, D. Lischinski, B. Chen, and D. Cohen-Or (2019)Learning character-agnostic motion for motion retargeting in 2d. arXiv preprint arXiv:1905.01680. Cited by: SS1.\n' +
      '* G. Blanchard, G. Lee, and C. Scott (2011)Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems24. Cited by: SS1.\n' +
      '* H. Chung, B. Sim, D. Ryu, and J. C. Ye (2022)Improving diffusion models for inverse problems using manifold constraints. arXiv preprint arXiv:2206.00941. Cited by: SS1.\n' +
      '* G. Dewaele, F. Devernay, and R. Horaud (2004)Hand motion from 3d point trajectories and a smooth surface model. In European Conference on Computer Vision, pp. 495-507. Cited by: SS1.\n' +
      '* P. Dhariwal and A. Nichol (2021)Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems34, pp. 8780-8794. Cited by: SS1.\n' +
      '* Q. Dou, D. Coelho de Castro, K. Kamnitsas, and B. Glocker (2019)Domain generalization via model-agnostic learning of semantic features. Advances in Neural Information Processing Systems32. Cited by: SS1.\n' +
      '* Z. Fan, O. Taheri, D. Tzionas, M. Kocabas, M. Kaufmann, M. J. Black, and O. Hilliges (2023)ARCTIC: a dataset for dexterous bimanual hand-object manipulation. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp. 3. Cited by: SS1.\n' +
      '* A. Ghosh, R. Dabral, V. Golyanik, C. Theobalt, and P. Slusallek (2023)Imos: intent-driven full-body motion synthesis for human-object interactions. In Computer Graphics Forum, Vol. 42, pp. 1-12. Cited by: SS1.\n' +
      '* P. Grady, C. Tang, C. D. Twigg, M. Vo, S. Brahmbhatt, and C. C. Kemp (2021)Contactopt: optimizing contact to improve grasps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1471-1481. Cited by: SS1.\n' +
      '* V. Guzov, T. Sattler, and G. Pons-Moll (2022)Visually plausible human-object interaction capture from wearable sensors. arXiv preprint arXiv:2205.02830. Cited by: SS1.\n' +
      '* G. Hackenberg, R. McCall, and W. Broll (2011)Lightweight palm and finger tracking for real-time 3d gesture control. In 2011 IEEE Virtual Reality Conference, pp. 19-26. Cited by: SS1.\n' +
      '* S. Hampali, M. Rad, M. Oberweger, and V. Lepetit (2020)Honnotate: a method for 3d annotation of hand and object poses. In CVPR, Cited by: SS1.\n' +
      '* C. Hecker, B. Raabe, R. W. Enslow, J. DeWeese, J. Maynard, and K. van Prooijen (2008)Real-time motion retargeting to highly varied user-created morphologies. ACM Transactions on Graphics (TOG)27 (3), pp. 1-11. Cited by: SS1.\n' +
      '* J. Ho, A. Jain, and P. Abbeel (2020)Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems33, pp. 6840-6851. Cited by: SS1.\n' +
      '* S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S. Zhu (2023)Diffusion-based generation, optimization, and planning in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16750-16761. Cited by: SS1.\n' +
      '* J. Jiang, Y. Shu, J. Wang, and M. Long (2022)Transferability in deep learning: a survey. arXiv preprint arXiv:2201.05867. Cited by: SS1.\n' +
      '* H. Kato, M. Billinghurst, I. Poupyrev, K. Imamoto, and K. Tachibana (2000)Virtual object manipulation on a table-top ar environment. In Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000), pp. 111-119. Cited by: SS1.\n' +
      '\n' +
      '* Kwon et al. (2021) Taein Kwon, Bugra Tekin, Jan Stuhmer, Federica Bogo, and Marc Pollefeys. H2o: Two hands manipulating objects for first person interaction recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 10138-10148, 2021.\n' +
      '* Li et al. (2023) Quanzhou Li, Jingbo Wang, Chen Change Loy, and Bo Dai. Task-oriented human-object interactions generation with implicit neural representations. _arXiv preprint arXiv:2303.13129_, 2023.\n' +
      '* Liu et al. (2022) Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 21013-21022, 2022.\n' +
      '* Mardani et al. (2023) Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. _arXiv preprint arXiv:2305.04391_, 2023.\n' +
      '* Meng et al. (2021) Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.\n' +
      '* Muandet et al. (2013) Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _International conference on machine learning_, pp. 10-18. PMLR, 2013.\n' +
      '* Nunez (2022) Johnny Nunez. _Comparison of Spatio-Temporal Hand Pose Denoising Models_. PhD thesis, Universitat DE Barcelona, 2022.\n' +
      '* Oh et al. (2019) Ju Young Oh, Ji Hyung Park, and Jung-Min Park. Virtual object manipulation by combining touch and head interactions for mobile augmented reality. _Applied Sciences_, 9(14):2933, 2019.\n' +
      '* Qu et al. (2023) Wentian Qu, Zhaopeng Cui, Yinda Zhang, Chenyu Meng, Cuixia Ma, Xiaoming Deng, and Hongan Wang. Novel-view synthesis and pose estimation for hand-object interaction from sparse views. _arXiv preprint arXiv:2308.11198_, 2023.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10684-10695, 2022.\n' +
      '* Romero et al. (2022) Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: Modeling and capturing hands and bodies together. _arXiv preprint arXiv:2201.02610_, 2022.\n' +
      '* Segu et al. (2023) Mattia Segu, Alessio Tonioni, and Federico Tombari. Batch normalization embeddings for deep domain generalization. _Pattern Recognition_, 135:109115, 2023.\n' +
      '* Shaer et al. (2010) Orit Shaer, Eva Hornecker, et al. Tangible user interfaces: past, present, and future directions. _Foundations and Trends(r) in Human-Computer Interaction_, 3(1-2):4-137, 2010.\n' +
      '* Sicilia et al. (2023) Anthony Sicilia, Xingchen Zhao, and Seong Jae Hwang. Domain adversarial neural networks for domain generalization: When it works and how to improve. _Machine Learning_, pp. 1-37, 2023.\n' +
      '* Song et al. (2023) Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. _arXiv preprint arXiv:2307.08123_, 2023.\n' +
      '* Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* Taheri et al. (2020) Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios Tzionas. Grab: A dataset of whole-body human grasping of objects. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV 16_, pp. 581-600. Springer, 2020.\n' +
      '\n' +
      '* Tak and Ko (2005) Seyoon Tak and Hyeong-Seok Ko. A physically-based motion retargeting filter. _ACM Transactions on Graphics (TOG)_, 24(1):98-117, 2005.\n' +
      '* Tendulkar et al. (2023) Purva Tendulkar, Didac Suris, and Carl Vondrick. Flex: Full-body grasping without full-body grasps. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 21179-21189, 2023.\n' +
      '* Tevet et al. (2022) Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. _arXiv preprint arXiv:2209.14916_, 2022.\n' +
      '* Tiwari et al. (2022) Garvita Tiwari, Dimitrije Antic, Jan Eric Lenssen, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll. Pose-ndf: Modeling human pose manifolds with neural distance fields. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part V_, pp. 572-589. Springer, 2022.\n' +
      '* Tumanyan et al. (2023) Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 1921-1930, 2023.\n' +
      '* Wang et al. (2022) Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. _IEEE Transactions on Knowledge and Data Engineering_, 2022.\n' +
      '* Wang et al. (2023) Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3769-3778, 2023.\n' +
      '* Wu et al. (2022) Yan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar Hilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-body grasping with contact. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VI_, pp. 257-274. Springer, 2022.\n' +
      '* Xie et al. (2022) Xianghui Xie, Bharat Lal Bhatnagar, and Gerard Pons-Moll. Chore: Contact, human and object reconstruction from a single rgb image. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part II_, pp. 125-145. Springer, 2022.\n' +
      '* Yang et al. (2021) Lixin Yang, Xinyu Zhan, Kailin Li, Wenqiang Xu, Jiefeng Li, and Cewu Lu. Cpf: Learning a contact potential field to model the hand-object interaction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 11097-11106, 2021.\n' +
      '* Ye et al. (2023) Yufei Ye, Poorvi Hebbar, Abhinav Gupta, and Shubham Tulsiani. Diffusion-guided reconstruction of everyday hand-object interaction clips. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 19717-19728, 2023.\n' +
      '* Zhang et al. (2021) He Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura. Manipnet: neural manipulation synthesis with a hand-object spatial representation. _ACM Transactions on Graphics (ToG)_, 40(4):1-14, 2021.\n' +
      '* Zhang et al. (2023) Ruipeng Zhang, Qinwei Xu, Jiangchao Yao, Ya Zhang, Qi Tian, and Yanfeng Wang. Federated domain generalization with generalization adjustment. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3954-3963, 2023.\n' +
      '* Zhang & Tang (2022) Yan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 20481-20491, 2022.\n' +
      '* Zhou et al. (2021) Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In _ICLR_, 2021a.\n' +
      '* Zhou et al. (2021) Kanglei Zhou, Zhiyuan Cheng, Hubert PH Shum, Frederick WB Li, and Xiaohui Liang. Stgae: Spatial-temporal graph auto-encoder for hand motion denoising. In _2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)_, pp. 41-49. IEEE, 2021b.\n' +
      '\n' +
      '* Zhou et al. (2022) Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, and Gerard Pons-Moll. Toch: Spatio-temporal object correspondence to hand for motion refinement. In _European Conference on Computer Vision (ECCV)_. Springer, October 2022.\n' +
      '* Zhou et al. (2018) Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A modern library for 3D data processing. _arXiv:1801.09847_, 2018.\n' +
      '\n' +
      '**Overview.** The **Appendix** provides a list of materials to support the main paper.\n' +
      '\n' +
      '* **Additional Technical Explanations (Sec. A).*\n' +
      '* We give additional explanations to complement the main paper.\n' +
      '* _The GeneOH representation (Sec. A.1)._ We discuss more insights into the representation design, why GeneOH can highlight errors, and how it compares to representations designed in previous works.\n' +
      '* _GeneOH Diffusion (Sec. A.2)._ We talk more about the whitened noise space, the diffusion process, the multi-step stochastic denoising process, the "denoising via diffusion" strategy, and the _progressive denoising_, together with a discussion on why each denoising stage can successfully clean the input without breaking the naturalness achieved after previous stages.\n' +
      '* _Fitting for a hand mesh trajectory (Sec. A.3)._ We provide details of the fitting process.\n' +
      '* **Additional Experimental Results (Sec. B).*\n' +
      '* We include more experimental results in this section to support the effectiveness of the method, including\n' +
      '* _HOI denoising results (Sec. B.1)._ We include more denoising results on GRAB, GRAB (Beta), HOI4D, and the ARCTIC dataset, including _long sequences with bimanual manipulations_. Besides, we discuss the results of _another series of experiments_ where the training dataset is changed to the training set of the _ARCTIC dataset_.\n' +
      '* _Ablation studies (Sec. B.2)._ We provide more quantitative and qualitative results of the ablation studies.\n' +
      '* _Applications (Sec. B.3)._ We provide more results on the applications that our model can support.\n' +
      '* _Failure cases (Sec. B.4)._ We discuss the limitations and failure cases of our method.\n' +
      '* _Analyzing the distinction between noise in real hand-object interaction trajectories and artificial noise (Sec. B.5)._ We discuss the differences between the real noise patterns and the artificial noise.\n' +
      '* _User study (Sec. B.6)._ We additionally include a user study to further assess the quality of our denoised results.\n' +
      '* **Experimental Details (Sec. C).*\n' +
      '* We illustrate details of datasets, metrics, baselines, models, the training and evaluation settings, and the running time as well as the complexity analysis.\n' +
      '\n' +
      'We include a video and an website to introduce our work. The website and the video contain _animated denoised results_. We highly recommend exploring these resources for an intuitive understanding of the challenges, the effectiveness of our model, and its superiority over prior approaches.\n' +
      '\n' +
      '## Appendix A Additional Technical Explanations\n' +
      '\n' +
      '### The GeneOH Representation\n' +
      '\n' +
      'More insights of the canonicalization design on \\(\\mathcal{T}\\) are explained as follows.\n' +
      '\n' +
      '**Canonicalization design on the temporal relations \\(\\mathcal{T}\\).** The temporal relations \\(\\mathcal{T}\\) leverages hand-object relative velocity \\(\\mathbf{v}_{k}^{\\text{ho}}\\) at each frame \\(k\\) of each hand-object point pair \\((\\mathbf{h},\\mathbf{o})\\) to represent the motion relations. We further canonicalize the relative velocity via object normals by decomposing \\(\\mathbf{v}_{k}^{\\text{ho}}\\) into two statistics: \\(\\mathbf{v}_{k,\\perp}^{\\text{ho}}\\), vertical to the object tangent plane and parallel to the object point normal, and \\(\\mathbf{v}_{k,\\parallel}^{\\text{ho}}\\), lying in the object\'s tangent plane. This decomposition enables the model to learn different denoising strategies for the two types of relative velocities and enhance cross-interaction generalization by factoring out object poses. However, relying solely on relative velocities is insufficient to reveal motion noise in hand-object interactions. The same relative velocity parallel to the normal direction can correspond to a clean state when the hand is far from the object, but a noisy state when they are in contact. To address this, we further encode the distance between each hand-object pair and their relative velocities into two statistics, \\(\\{(e_{k,\\perp}^{\\text{ho}},e_{k,\\parallel}^{\\text{ho}})\\}\\), using the followingformulation:\n' +
      '\n' +
      '\\[e_{k,\\perp}^{\\mathbf{ho}} =e^{-k\\cdot d_{k}^{\\mathbf{ho}}}\\,k_{b}\\|\\mathbf{v}_{k,\\perp}^{ \\mathbf{ho}}\\|_{2} \\tag{2}\\] \\[e_{k,\\parallel}^{\\mathbf{ho}} =e^{-k\\cdot d_{k}^{\\mathbf{ho}}}\\,k_{a}\\|\\mathbf{v}_{k,\\parallel}^{ \\mathbf{ho}}\\|_{2}. \\tag{3}\\]\n' +
      '\n' +
      'Here, \\(k\\), \\(k_{a}\\), and \\(k_{b}\\) are positive hyperparameters, and the term \\(e^{-k\\cdot d_{k}^{\\mathbf{ho}}}\\) is inversely related to the distance between the hand and object points. This formula allows the statistics for very close hand-object point pairs to be emphasized in the representation.\n' +
      '\n' +
      '**Why GeneOH can highlight errors?** For spatial errors, we mainly consider the geometric penetrations between the hand and the object. The hand-object spatial relations \\(\\mathcal{S}\\) in GeneOH reveal penetrations by parameterizing the object normals and hand-object relative offsets. In more detail, for each hand-object point pair \\((\\mathbf{h}_{k},\\mathbf{o}_{k})\\), the dot product between the object normal \\(\\mathbf{n}_{k}\\) of \\(\\mathbf{o}_{k}\\) and the relative offset \\(\\mathbf{h}_{k}-\\mathbf{o}_{k}\\) indicates the signed distance between the object point and the hand point. For each hand point \\(\\mathbf{h}_{k}\\), its signed distance to the object mesh can be revealed by jointly considering its signed distance to all generalized contact points. Since the hand point \\(\\mathbf{h}_{k}\\) penetrates the object _if and only if its signed distance to the object mesh is negative_, the spatial relation parameterization \\(\\mathcal{S}\\) can indicate the penetration phenomena.\n' +
      '\n' +
      'For temporal errors, we mainly consider inconsistent hand-object motions. There is no unified definition or statement regarding what consistent hand-object motions indicate. Intuitively, the hand should be able to manipulate the object, where sufficient contact and consistent motions between very close hand-object point pairs are demanded. For very close hand-object pairs, the sliding motion on the object surface is permitted but the vertical penetration moving tendency is not allowed. The above expectations and unnatural situations can be revealed from simple statistics like hand-object relative velocities and hand-object distances. The distance can tell whether they are close to each other. The relative velocity can reveal their moving discrepancy. The decomposed relative velocity lying in the tangent plane and vertical to the tangent plane indicate the surface sliding tendency and the penetrating tendency respectively. The distance-related weight term \\(e^{-k_{f}d_{k}^{\\mathbf{ho}}}\\) can emphasize hand-object pairs that are very close to each other. Therefore, the temporal relations representation \\(\\mathcal{T}\\) leveraged in GeneOH can successfully indicate the temporal naturalness and incorrect phenomena. Thus learning the distribution of \\(\\mathcal{T}\\) can teach the model what is temporal naturalness and how to clean the noisy representation.\n' +
      '\n' +
      '**The GeneOH representation** can be applied to parameterize interaction sequences involving rigid or articulated objects. It carefully integrates both the hand motions and hand-object spatial and temporal relations -- more expressive and comprehensive compared to designs in previous works (Zhou et al., 2022; 2021; Zhang et al., 2021). Besides, it can highlight spatial and temporal interaction errors. The above advantages make GeneOH well-suited for the HOI denoising task.\n' +
      '\n' +
      'Inspecting back to previous works, TOCH (Zhou et al., 2022) does not explicitly encode hand-centric poses or hand-object temporal relations and only grounds the hands onto the object without careful consideration of cross-interaction alignment. ManipNet (Zhang et al., 2021) takes hand-object distances to represent their relations. But this is not enough to reveal their spatial relations. Canonicalizations are also not carefully considered in this work.\n' +
      '\n' +
      '### GeneOH Diffusion\n' +
      '\n' +
      'We give a more detailed explanation of the three denoising stages in the following text.\n' +
      '\n' +
      '**The whitened noise space**. This space is constructed by diffusing the training data towards a random Gaussian noise. A diffusion timestep \\(1\\leq t_{\\text{diff}}\\leq T\\) where \\(T\\) is the maximum timestep controls to what extent the input is diffused to pure Gaussian noise. It is the space modeled by the diffusion models during training. The diffusion function we adopt in this work is also exactly the same as the forward diffusion process of diffusion models. To be more specifically, given a data point \\(x\\), the \\(t_{\\text{diff}}\\) diffusion would transform to \\(x_{t}\\) by a linear combination of \\(x\\) and a random Gaussian noise \\(\\mathbf{n}\\) with the same size to \\(x\\) via the following equation:\n' +
      '\n' +
      '\\[x_{t}=\\sqrt{\\alpha_{t}}x+\\sqrt{1-\\bar{\\alpha}_{t}}\\mathbf{n}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\alpha_{t}=1-\\beta_{t},\\bar{\\alpha}_{t}=\\Pi_{s=1}^{t}\\alpha_{s_{t}}\\left\\{ \\beta_{t}\\right\\}\\) is the forward process variances. The distribution of \\(x_{t}\\) is a normal distribution: \\(x_{t}\\sim\\widetilde{\\mathcal{N}}(\\sqrt{\\bar{\\alpha}_{t}}x,(1-\\sqrt{\\bar{ \\alpha}_{t}})\\mathbf{I})\\).\n' +
      '\n' +
      'Intuitively, the noise space contains all possible \\(x_{t}\\) across all possible timestep \\(1\\leq t\\leq T\\). \\(x_{t}\\) with smaller \\(t\\) will be more similar to \\(x\\). In practice, \\(T\\) is set to 1000, \\(\\beta_{1}=0.001,\\beta_{T}=0.02\\) with a linear interpolation between them to create the variance sequence \\(\\{\\beta_{t}\\}\\). During training \\(t\\) is uniformly sampled from the \\(\\{t|t\\in\\mathbb{Z},1\\leq t\\leq T\\}\\).\n' +
      '\n' +
      '**Training of the denoising model.** Denote the multi-step stochastic denoising model leveraged in our method as \\(\\text{denoise}(\\cdot,t)\\) which takes the noisy sample with the noise scale \\(t\\) as input and denoise it back to the noise sample with noise scale \\(t-1\\). When \\(t=1\\), the denoised result lies in the clean data manifold depicted by the model and is taken as the final denoised result. The denoise\\((\\cdot,t)\\) leverages a score function \\(\\epsilon_{\\theta}(\\cdot,t)\\) to predict the noise component of the input noise sample \\(\\tilde{x}_{t}\\). The score function \\(\\epsilon_{\\theta}(\\cdot,t)\\) contains optimizable network weights \\(\\theta\\) and is what we need to learn during training. \\(\\epsilon_{\\theta}(\\cdot,t)\\) only predicts the noise component \\(\\hat{\\mathbf{n}}\\). After that, a posterior sampling process is leveraged to sample \\(\\tilde{x}_{t-1}\\) based on \\(\\tilde{x}_{t}\\) and the predicted \\(\\hat{\\mathbf{n}}\\) via the following equation:\n' +
      '\n' +
      '\\[\\tilde{x}_{t-1}=\\frac{1}{\\sqrt{\\alpha_{t}}}(\\tilde{x}_{t}-\\frac{1-\\alpha_{t}} {\\sqrt{1-\\tilde{\\alpha}_{t}}}\\epsilon_{\\theta}(\\tilde{x}_{t},t))+\\sigma_{t} \\mathbf{z}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\mathbf{z}\\in\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\), \\(\\sigma_{t}^{2}=\\beta_{t}\\). Therefore, the denoising model is a multi-step stochastic denoiser since at each step it only identifies the mean of the posterior distribution and the denoised result needs to be sampled from the distribution with the predicted mean and the pre-defined variance.\n' +
      '\n' +
      '**The "denoising via diffusion" strategy.** The input trajectory with noise \\(\\hat{x}\\) is diffused to \\(\\tilde{x}\\) via Gaussian noise with the diffusion timestep \\(t_{\\text{diff}}\\) using the following equation:\n' +
      '\n' +
      '\\[\\tilde{x}=\\tilde{x}_{t_{\\text{diff}}}=\\tilde{x}_{t_{\\text{diff}}}=\\sqrt{ \\tilde{\\alpha}_{t_{\\text{diff}}}}x+\\sqrt{1-\\tilde{\\alpha}_{t_{\\text{diff}}}} \\mathbf{n}, \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathbf{n}\\in\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\) is a random Gaussian noise.\n' +
      '\n' +
      'Given the noisy sample \\(\\tilde{x}_{t}\\) with noise scale \\(t\\), the denoising model \\(\\text{denoise}(\\cdot,t)\\) predicts its noise component via the score function :\n' +
      '\n' +
      '\\[\\hat{\\mathbf{n}}=\\epsilon_{\\theta}(\\tilde{x}_{t},t). \\tag{7}\\]\n' +
      '\n' +
      'Then \\(\\tilde{x}_{t}\\) is denoised to \\(\\tilde{x}_{t-1}\\) with noise scale \\(t-1\\) by sampling from the posterior distribution following Eq. 5 with the predicted mean \\(\\tilde{x}_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\alpha_{t}}}\\hat{\\mathbf{n}}\\) and the pre-defined variance \\(\\sigma_{t}^{2}\\).\n' +
      '\n' +
      '**MotionDiff: canonicalized hand trajectory denoising.** This denoising stage removes noise from the canonicalized hand trajectory \\(\\tilde{\\mathcal{J}}^{\\text{input}}\\) of the input noisy interaction sequence by applying the diffusion model for one stage-denoised \\(\\tilde{\\mathcal{J}}^{\\text{stage}_{1}}\\), following the "denoising via diffusion" strategy. To do this, the noisy representation \\(\\tilde{\\mathcal{J}}^{\\text{input}}\\) is diffused by adding noise for \\(t_{m}\\) steps, followed by denoising for \\(t_{m}\\) steps using the diffusion model. The resulting one stage-denoised hand trajectory \\(\\mathcal{J}^{\\text{stage}_{1}}\\) in the chord coordinate space is obtained by de-cancalionizing the denoised canonicalized hand trajectory \\(\\tilde{\\mathcal{J}}^{\\text{stage}_{1}}\\) using the pose of the generalized contact points \\((\\mathbf{R}_{k},\\mathbf{t}_{k})\\). GeneOH\\({}^{\\text{input}}\\) can also be updated accordingly into GeneOH\\({}^{\\text{stage}_{1}}=\\{\\tilde{\\mathcal{J}}^{\\text{stage}_{1}},\\tilde{ \\mathcal{J}}^{\\text{stage}_{1}},\\tilde{\\mathcal{J}}^{\\text{stage}_{1}}\\}\\).\n' +
      '\n' +
      '**SpatialDiff: hand-object spatial denoising.** The hand-object spatial denoising module operates on the noisy hand-object spatial relations \\(\\tilde{\\mathcal{S}}^{\\text{stage}_{1}}\\) of the one stage-denoised interaction sequence output by the previous MotionDiff stage. The representation \\(\\tilde{\\mathcal{S}}^{\\text{stage}_{1}}\\) is diffused by adding noise for \\(t_{s}\\) diffusion steps, followed by another \\(t_{s}\\) step of denoising. Once we obtain the denoised representation \\(\\mathcal{S}^{\\text{stage}_{2}}\\) which includes the hand-object relative offsets \\(\\{\\left(\\mathbf{h}_{k}-\\mathbf{o}_{k}\\right)\\}\\) centered at each generalized contact point \\(\\mathbf{o}_{k}\\), we adopt a simple approach to convert it into a two stages-denoised hand sequence. Specifically, we average the denoised hand offsets from each object point as follows:\n' +
      '\n' +
      '\\[\\mathcal{J}=\\text{Average}\\{(\\mathbf{h}_{k}-\\mathbf{o}_{k})+\\mathbf{o}_{k}| \\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\}. \\tag{8}\\]\n' +
      '\n' +
      'Following this, GeneOH\\({}^{\\text{stage}_{1}}\\) will be updated to GeneOH\\({}^{\\text{stage}_{2}}=\\{\\tilde{\\mathcal{J}}^{\\text{stage}_{2}},\\mathcal{S}^{\\text{stage}_{2}}, \\tilde{\\mathcal{J}}^{\\text{stage}_{2}}\\}\\).\n' +
      '\n' +
      '**TemporalDiff: hand-object temporal denoising.** We proceed to clean the noisy hand-object temporal relations \\(\\tilde{\\mathcal{J}}^{\\text{stage}_{2}}\\) of the two stages-denoised sequence. The "denoising via diffusion" procedure is applied to the temporal relations to achieve this. We then add an additional optimization to distill the information contained in the denoised temporal representation to the three stages-denoised trajectory. The objective is formulated as:\n' +
      '\n' +
      '\\[\\underset{\\mathcal{J}^{\\text{stage}_{2}}}{\\text{minimize}}\\|f_{(\\mathcal{J}, \\mathcal{P})\\rightarrow\\mathcal{T}}(\\mathcal{J}^{\\text{stage}_{2}},\\mathcal{P}) -\\mathcal{T}^{\\text{stage}_{3}}\\|, \\tag{9}\\]where \\(\\mathcal{P}\\) is the sequence of generalized contact points, \\(f_{(\\mathcal{J},\\mathcal{P})\\rightarrow\\mathcal{T}}(\\cdot,\\cdot)\\) converts the hand trajectory to the corresponding temporal relations. The distance is calculated on hand-object distances, _i.e._, \\(\\{d^{\\mathbf{ho}}_{k}\\}\\), relative velocity \\(\\{\\mathbf{v}^{\\mathbf{ho}}_{k}\\}\\) and two relative velocity-related statistics ( \\(\\{\\mathbf{e}^{\\mathbf{ho}}_{k,\\parallel},\\mathbf{e}^{\\mathbf{ho}}_{k, \\perp}\\}\\) ). We employ an Adam optimizer to find the optimal hand trajectory. The optimized trajectory is taken as the final denoised trajectory \\(\\mathcal{J}\\).\n' +
      '\n' +
      'Stage-wise denoising strategy.Let \\(\\mathcal{I}=\\{(\\mathcal{H}_{k},\\mathbf{O}_{k})\\}_{k=1}^{K}\\in\\mathcal{M}\\) denote an interaction sequence, where \\(\\mathcal{M}\\) is the manifold contains all interaction sequences. Let \\(\\mathcal{M}_{\\mathcal{I}}\\), \\(\\mathcal{M}_{\\mathcal{S}}\\), and \\(\\mathcal{M}_{\\mathcal{T}}\\) represent the manifolds depicted by the three denoising stages respectively. Let \\(\\mathcal{I}_{\\mathcal{J}}\\), \\(\\mathcal{I}_{\\mathcal{S}}\\), and \\(\\mathcal{I}_{\\mathcal{T}}\\) represent one stage-denoised trajectory, two stages-denoised trajectory, and the three stages-denoised trajectory respectively. Further, let \\(\\mathcal{R}^{c}_{\\mathcal{J}}\\), \\(\\mathcal{R}^{c}_{\\mathcal{S}}\\), and \\(\\mathcal{R}^{c}_{\\mathcal{T}}\\) denote the set of all natural canonicalized hand trajectories, natural hand-object spatial relations, and correct hand-object temporal relations respectively. Let \\(\\mathcal{R}_{\\mathcal{J}}\\), \\(\\mathcal{R}_{\\mathcal{S}}\\), and \\(\\mathcal{R}_{\\mathcal{T}}\\) denote the set of all canonicalized hand trajectories, hand-object spatial relations, and hand-object temporal relations respectively. Denote the function that transforms the interaction trajectory \\(\\mathcal{I}\\) to the canonicalized hand trajectory as \\(f_{\\mathcal{I}\\rightarrow\\mathcal{J}}\\), the function that converts \\(\\mathcal{I}\\) to the hand-object spatial relations as \\(f_{\\mathcal{I}\\rightarrow\\mathcal{S}}\\), and the function that transforms \\(\\mathcal{I}\\) to the hand-object temporal relations as \\(f_{\\mathcal{I}\\rightarrow\\mathcal{T}}\\).\n' +
      '\n' +
      'For all interaction trajectories considered in the work, we make the following assumption:\n' +
      '\n' +
      '**Assumption**: _For any trajectory \\(\\mathcal{I}\\) with the first frame free of spatial noise, we can find a natural trajectory \\(\\mathcal{I}^{\\prime}\\) with the same first frame, that is \\(\\mathcal{I}^{\\prime}[1]=\\mathcal{I}[1]\\)._\n' +
      '\n' +
      'The three fully-trained denoising models for \\(\\tilde{\\mathcal{J}}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{T}\\) should be able to map the corresponding input representation to the set of \\(\\mathcal{R}^{c}_{\\mathcal{J}}\\), \\(\\mathcal{R}^{c}_{\\mathcal{S}}\\), and \\(\\mathcal{R}^{c}_{\\mathcal{T}}\\) respectively. Then the relations between the interaction manifolds depicted by the three denoising stages and the natural data prior modeled by the three denoising models have the following relations:\n' +
      '\n' +
      '* \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{J}}\\) if and only if \\(f_{\\mathcal{I}\\rightarrow\\mathcal{J}}(\\mathcal{I})\\in\\mathcal{R}^{c}_{\\mathcal{ J}}\\);\n' +
      '* \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{S}}\\) if and only if \\(f_{\\mathcal{I}\\rightarrow\\mathcal{S}}(\\mathcal{I})\\in\\mathcal{R}^{c}_{\\mathcal{ S}}\\);\n' +
      '* \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{T}}\\) if and only if \\(f_{\\mathcal{I}\\rightarrow\\mathcal{T}}(\\mathcal{I})\\in\\mathcal{R}^{c}_{\\mathcal{ T}}\\) and the first frame \\(\\mathcal{I}[1]\\) is free of spatial noise.\n' +
      '\n' +
      'Based on the relations between \\(\\tilde{\\mathcal{J}}\\), \\(\\mathcal{S}\\), and \\(\\mathcal{T}\\), we can make the following claim:\n' +
      '\n' +
      '**Claim 1**: _There existing functions \\(f_{\\mathcal{S}\\rightarrow\\mathcal{J}}:\\mathcal{R}_{\\mathcal{S}}\\rightarrow \\mathcal{R}_{\\tilde{\\mathcal{J}}}\\) and \\(f_{\\mathcal{T}\\rightarrow\\mathcal{S}}:(\\mathcal{R}_{\\mathcal{T}},\\mathcal{I}_{ \\mathcal{S}[1]}\\rightarrow\\mathcal{R}_{\\mathcal{S}})\\), so that for any interaction \\(\\mathcal{I}\\) with corresponding GeneOH representations GeneOH\\((\\mathcal{I})=\\{\\tilde{\\mathcal{J}},\\mathcal{S},\\mathcal{T}\\}\\), we have: \\(\\tilde{\\mathcal{J}}=f_{\\mathcal{S}\\rightarrow\\mathcal{J}}(\\mathcal{S})\\) and \\(\\mathcal{S}=f_{\\mathcal{T}\\rightarrow\\mathcal{S}}(\\mathcal{T},\\mathcal{I}[1])\\)._\n' +
      '\n' +
      '_Proof._ _The canonicalized hand keypoints at each frame \\(k\\), i.e., \\(\\bar{\\mathbf{J}}_{k}\\) is composed of each canonicalized hand keypoint \\(\\bar{\\mathbf{J}}_{k}=\\{(\\mathbf{h}_{k}-\\mathbf{t}_{k})\\mathbf{R}_{k}^{T}| \\mathbf{h}_{k}\\in\\mathbf{J}_{k}\\}\\), which can be derived from the canonicalized hand-object spatial relation at the frame \\(k\\). Specifically, for each \\(\\mathbf{o}_{k}\\in\\mathbf{P}_{\\mathbf{k}}\\), we have \\((\\mathbf{h}_{k}-\\mathbf{t}_{k})\\mathbf{R}_{k}^{T}=(\\mathbf{h}_{k}-\\mathbf{o}_{ k})\\mathbf{R}_{k}^{T}+(\\mathbf{o}_{k}-\\mathbf{t}_{k})\\mathbf{R}_{k}^{T}\\). The unique canonicalized hand trajectory at the frame \\(k\\) can be decided from the trajectory converted from each object point \\(\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\). Depending on the conversion function from such multiple hypotheses of the canonicalized hand trajectory resulting from different \\(\\mathbf{o}_{k}\\), there exists a function \\(f_{\\mathcal{S}\\rightarrow\\mathcal{J}}:\\mathcal{S}\\rightarrow\\mathcal{R}_{ \\mathcal{J}}\\) that transforms the hand-object spatial relations \\(\\mathcal{S}\\) to the canonicalized hand trajectory \\(\\tilde{\\mathcal{J}}\\)._\n' +
      '\n' +
      '_Similarly, given the hand-object temporal relations at the frame \\(k(1\\leq k\\leq K-1)\\) of the object point \\(\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\) and the natural hand keypoints at the starting frame \\(1\\), i.e., \\(\\bar{\\mathbf{J}}_{1}\\), the relative velocity for each hand-object pair \\((\\mathbf{h}_{k},\\mathbf{o}_{k})\\) can be derived from the decoded hand-object relative velocity \\(\\mathbf{v}^{\\mathbf{ho}}_{k}\\), two velocity-related statistics \\((\\mathbf{e}^{\\mathbf{ho}}_{k,\\perp},\\mathbf{e}^{\\mathbf{ho}}_{k,\\parallel})\\), the hand-object distance \\(d^{\\mathbf{ho}}_{k}\\). Given the hand-object relative positions \\(\\{(\\mathbf{h}_{1}-\\mathbf{o}_{1})|\\mathbf{h}_{1}\\in\\mathbf{J}_{1}\\}\\), the hand-object relative positions at each following frame \\(k+1(1\\leq k\\leq K-1)\\) can be derived iteratively via the hand-object relative velocity \\(\\{\\mathbf{v}^{\\mathbf{ho}}_{k}|\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\}\\): \\(\\mathbf{h}_{k+1}-\\mathbf{o}_{k+1}=(\\mathbf{h}_{k}-\\mathbf{o}_{k})+\\Delta\\mathbf{ t}\\mathbf{v}^{\\mathbf{ho}}_{k}\\). Therefore, there existing a function \\(f_{\\mathcal{T}\\rightarrow\\mathcal{S}}:\\mathcal{R}_{\\mathcal{T}}\\rightarrow \\mathcal{R}_{\\mathcal{S}}\\) that can convert the temporal relations \\(\\mathcal{T}\\) to the hand-object spatial relations \\(\\mathcal{S}\\)._\n' +
      '\n' +
      'Based on this property, we can make the following claim regarding the relations between the three gradually constructed manifolds:\n' +
      '\n' +
      '**Claim 2**: _Assume the first frame of the two stages-denoised trajectory \\(\\mathcal{I}_{\\mathcal{S}}[1]\\) is free of spatial noise, which **almost always holds true**, we have \\(\\mathcal{M}_{\\mathcal{T}}\\subseteq\\mathcal{M}_{\\mathcal{S}}\\subseteq\\mathcal{M}_{ \\mathcal{J}}\\)._Proof.: _For \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{S}}\\) with the GeneOH representation \\(\\{\\bar{\\mathcal{J}},\\mathcal{S},\\mathcal{T}\\}\\), assume \\(\\mathcal{I}\\notin\\mathcal{M}_{\\bar{\\mathcal{J}}}\\)._\n' +
      '\n' +
      '* _From_ \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{S}}\\)_, we have_ \\(\\mathcal{S}\\in\\mathcal{R}^{c}_{\\mathcal{S}}\\)_;_\n' +
      '* _Based on the definition of_ \\(\\mathcal{R}^{c}_{\\mathcal{S}}\\)_, the set of spatial relations derived from all natural interactions, there exists a natural interaction_ \\(\\mathcal{I}^{\\prime}\\) _so that_ \\(f_{\\mathcal{I}\\to\\mathcal{S}}(\\mathcal{I}^{\\prime})=\\mathcal{S}\\)_;_\n' +
      '* _Since_ \\(\\mathcal{I}^{\\prime}\\) _is a natural interaction, we have_ \\(\\bar{\\mathcal{J}}^{\\prime}=f_{\\mathcal{I}\\to\\mathcal{J}}(\\mathcal{I}^{\\prime}) \\in\\mathcal{R}^{c}_{\\bar{\\mathcal{J}}}\\)_;_\n' +
      '* _Since_ \\(\\bar{\\mathcal{J}}=f_{\\mathcal{S}\\to\\bar{\\mathcal{J}}}(\\mathcal{S})=\\bar{ \\mathcal{J}}^{\\prime}\\)_, we have_ \\(\\bar{\\mathcal{J}}\\in\\mathcal{R}^{c}_{\\bar{\\mathcal{J}}}\\)_;_\n' +
      '* _Based on the assumed fully-trained denoising model, we have_ \\(\\mathcal{I}\\in\\mathcal{M}_{\\bar{\\mathcal{J}}}\\)_._\n' +
      '\n' +
      '_The conclusion contradicts with the assumption \\(\\mathcal{I}\\notin\\mathcal{M}_{\\bar{\\mathcal{J}}}\\). Thus \\(\\mathcal{M}_{\\mathcal{S}}\\subseteq\\mathcal{M}_{\\bar{\\mathcal{J}}}\\) holds true._\n' +
      '\n' +
      '_For a \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{T}}\\) with the GeneOH representation \\(\\{\\bar{\\mathcal{J}},\\mathcal{S},\\mathcal{T}\\}\\) whose first frame is free of spatial noise, assume \\(\\mathcal{I}\\notin\\mathcal{M}_{\\mathcal{S}}\\)._\n' +
      '\n' +
      '* _From_ \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{T}}\\)_, we have that_ \\(\\mathcal{T}\\in\\mathcal{R}^{c}_{\\bar{\\mathcal{J}}}\\)_;_\n' +
      '* _Based on the definition of_ \\(\\mathcal{R}^{c}_{\\bar{\\mathcal{J}}}\\)_, the set of temporal relations derived from all natural interactions, and the Assumption_ 1_, there existing a natural interaction_ \\(\\mathcal{I}^{\\prime}\\)_, with the first frame same to_ \\(\\mathcal{I}\\)_, so that_ \\(f_{\\mathcal{I}\\to\\mathcal{T}}(\\mathcal{I}^{\\prime})=\\mathcal{T}\\)_;_\n' +
      '* _Since_ \\(\\mathcal{I}^{\\prime}\\) _is a natural interaction, we have_ \\(\\mathcal{S}^{\\prime}=f_{\\mathcal{I}\\to\\mathcal{S}}(\\mathcal{I}^{\\prime})\\in \\mathcal{R}^{c}_{\\mathcal{S}}\\)_;_\n' +
      '* _Since_ \\(\\mathcal{S}=f_{\\mathcal{T}\\to\\mathcal{S}}(\\mathcal{T},\\mathcal{I}[1])=f_{ \\mathcal{T}\\to\\mathcal{S}}(\\mathcal{T},\\mathcal{I}^{\\prime}[1])=\\mathcal{S}^{ \\prime}\\)_, we have_ \\(\\mathcal{S}\\in\\mathcal{R}^{c}_{\\mathcal{S}}\\)_;_\n' +
      '* _Based on the assumed fully-trained denoising model, we have_ \\(\\mathcal{I}\\in\\mathcal{M}_{\\mathcal{S}}\\)_._\n' +
      '\n' +
      '_The conclusion contradicts with the assumption \\(\\mathcal{I}\\notin\\mathcal{M}_{\\mathcal{S}}\\). Thus \\(\\mathcal{M}_{\\mathcal{T}}\\subseteq\\mathcal{M}_{\\mathcal{S}}\\) holds true. \\(\\blacksquare\\)_\n' +
      '\n' +
      'The stage-wise GeneOH Diffusion functions as the following steps to clean the input interaction \\(\\mathcal{I}\\in\\mathcal{M}\\):\n' +
      '\n' +
      '* Given the input interaction \\(\\mathcal{I}\\), the denoising model for \\(\\bar{\\mathcal{J}}\\) maps \\(\\bar{\\mathcal{J}}\\) to another \\(\\bar{\\mathcal{J}}^{1}\\in\\mathcal{R}^{c}_{\\bar{\\mathcal{J}}}\\). There existing an interaction \\(\\mathcal{I}^{1}\\) s.t. \\(\\bar{\\mathcal{J}}^{1}=f_{\\mathcal{I}\\to\\bar{\\mathcal{J}}}(\\mathcal{I}^{1})\\), which is also exactly the same as the trajectory derived from \\(\\bar{\\mathcal{J}}^{1}\\) and the object trajectory \\(\\{\\mathbf{O}_{k}\\}_{k=1}^{K}\\). Therefore, after the first denoising stage, we have \\(\\mathcal{I}^{1}\\in\\mathcal{M}_{\\bar{\\mathcal{J}}}\\).\n' +
      '* Given \\(\\mathcal{S}^{1}\\), the denoising model for \\(\\mathcal{S}\\) maps \\(\\mathcal{S}^{1}\\) to \\(\\mathcal{S}^{2}\\in\\mathcal{R}^{c}_{\\mathcal{S}}\\). There existing an interaction \\(\\mathcal{I}^{2}\\) s.t. \\(\\mathcal{S}^{2}=f_{\\mathcal{I}\\to\\mathcal{S}}(\\mathcal{I}^{2})\\). Therefore, after the second denoising stage, we have \\(\\mathcal{I}^{2}\\in\\mathcal{M}_{\\mathcal{S}}\\).\n' +
      '* After that, the denoising model for \\(\\mathcal{T}\\) maps \\(\\mathcal{T}^{2}=f_{\\mathcal{I}\\to\\mathcal{T}}(\\mathcal{I}^{2})\\) to \\(\\mathcal{T}^{3}\\in\\mathcal{R}^{c}_{\\mathcal{T}}\\). After that, the interaction \\(\\mathcal{I}^{3}\\) constructed as the following steps:\n' +
      '* Construct \\(\\mathcal{S}^{3}\\) from \\(\\mathcal{T}^{3}\\) and \\(\\mathcal{I}^{2}[1]\\) via \\(\\mathcal{S}^{3}=f_{\\mathcal{T}\\to\\mathcal{S}}(\\mathcal{T}^{3},\\mathcal{I}^{2}[ 1])\\);\n' +
      '* Construct \\(\\bar{\\mathcal{J}}^{3}\\) from \\(\\mathcal{S}^{3}\\) via \\(\\bar{\\mathcal{J}}^{3}=f_{\\mathcal{S}\\to\\mathcal{J}}(\\mathcal{S}^{3})\\);\n' +
      '* Construct \\(\\mathcal{I}^{3}\\) from \\(\\bar{\\mathcal{J}}^{3}\\) and the object trajectory \\(\\{\\mathbf{O}_{k}\\}_{k=1}^{K}\\). Since \\(\\mathcal{T}^{3}=f_{\\mathcal{I}\\to\\mathcal{T}}(\\mathcal{I}^{3})\\in\\mathcal{R}^{c }_{\\mathcal{T}}\\) and \\(\\mathcal{I}^{3}[1]=\\mathcal{I}^{2}[1]\\) is free of spatial noise, we have \\(\\mathcal{I}^{3}\\in\\mathcal{M}_{\\mathcal{T}}\\).\n' +
      '\n' +
      'Therefore, the three denoising stages gradually map the input noisy interaction to a progressively smaller manifold contained in the previous large manifold. Formally we have\n' +
      '\n' +
      '\\[\\text{GeneOH Diffusion}(\\cdot):\\mathcal{M}\\to\\mathcal{M}_{\\bar{\\mathcal{J}}}\\to \\mathcal{M}_{\\mathcal{S}}\\to\\mathcal{M}_{\\mathcal{T}}. \\tag{10}\\]\n' +
      '\n' +
      '### Fitting for an HOI Trajectory\n' +
      '\n' +
      'Once the interaction sequence has been denoised, we proceed to fit a sequence of hand MANO (Romero et al., 2022) parameters to obtain final hand meshes. The objective is optimizing a series of MANO parameters \\(\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k=1}^{K}\\) so that they fit the denoised trajectory \\(\\mathcal{J}\\) well. Notice the hand trajectory \\(\\mathcal{J}\\) consists of a sequence of hand keypoints so we also need to derive keypoints from MANO parameters to allow the above optimization. Luckily this process is differentiable and we use \\(\\mathcal{J}^{recon}(\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k=1} ^{K})\\) to denote it. We can therefore optimize the following reconstruction loss for the MANO hands\n' +
      '\n' +
      '\\[\\mathcal{L}_{recon}=\\|\\mathcal{J}-\\mathcal{J}^{recon}(\\{\\mathbf{r}_{k}, \\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k=1}^{K})\\|, \\tag{11}\\]\n' +
      '\n' +
      'where the distance function is a simple mean squared error (MSE) between the hand keypoints at each frame. To regularize the hand parameters \\(\\{\\beta_{k},\\theta_{k}\\}\\) and enforce temporal smoothness, we introduce an additional regularization loss defined as\n' +
      '\n' +
      '\\[\\mathcal{L}_{reg}=\\frac{1}{K}\\sum_{k=1}^{K}(\\|\\beta_{k}\\|_{2}+\\|\\theta_{k}\\|_ {2})+\\frac{1}{K-1}\\sum_{k=1}^{K-1}\\|\\theta_{k+1}-\\theta_{k}\\|_{2}. \\tag{12}\\]\n' +
      '\n' +
      'The overall optimization objective is formalized as\n' +
      '\n' +
      '\\[\\text{minimize}_{\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k= 1}^{K}}(\\mathcal{L}_{recon}+\\mathcal{L}_{reg}), \\tag{13}\\]\n' +
      '\n' +
      'and we employ an Adam optimizer to solve the fitting problem.\n' +
      '\n' +
      '## Appendix B Additional Experimental Results\n' +
      '\n' +
      'We present additional experimental results to further support the denoising effectiveness and the strong generalization ability of our method.\n' +
      '\n' +
      '### HOI Denoising\n' +
      '\n' +
      'For the first set of evaluation metrics, Table 4 presents more evaluations on our method, ablated versions, and the comparisons to the baseline models than the table in the main text. For the second set of evaluation metrics, Table 5 summarizes more results and comparisons. For stochastic denoising methods, including "Ours", "Ours w/o Canon.", "Ours w/o SpatialDiff", and "Ours w/o TemporalDiff", we report the mean and the standard deviation of results obtained from three independent runs with the random seed set to 11, 22, and 77 respectively. Such statistics offer a more comprehensive view of the average results quality produced by those methods. Please notice that the evaluation method is different from the one present in the main text, where the result closest to the input trajectory among 100 independent runs is chosen to report evaluation metrics.\n' +
      '\n' +
      '**More results on the GRAB test set - novel interactions with new objects.** Figure 8 shows qualitative evaluations on the GRAB test set to compare the generalization ability of different denoising models towards novel interactions with unseen objects.\n' +
      '\n' +
      '**More results on the GRAB (Beta) test set - novel interactions with new objects and unseen synthetic noise patterns.** Figure 9 shows qualitative evaluations on the GRAB (Beta) test set to compare the generalization ability of different denoising models towards unseen objects, unobserved interactions, and novel synthetic noise.\n' +
      '\n' +
      '**More results on the HOI4D dataset - novel interactions with new objects and unseen real noise patterns.** Figure 10 shows qualitative evaluations on the HOI4D test set to compare the generalization ability of different denoising models towards unseen objects, unobserved interactions, and novel real noise.\n' +
      '\n' +
      '**Wired hand trajectory produced by TOCH (w/MixStyle).** Through our experiments with TOCH on interaction sequences with new noise patterns unseen during training, we frequently observe strange hand trajectories from its results with stiff hand poses, unsmooth trajectories, and large\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c|c c c c c c} \\hline \\hline  & Laptop & Pliers & Scissors & Bottle & Bowl & Chair & Mug & ToyCar & Kettle \\\\ \\hline \\#Seq. & 155 & 187 & 93 & 214 & 217 & 167 & 249 & 257 & 58 \\\\ Starting Frame & 120 & 150 & 50 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **HOI4D Dataset.** Per-category statistics of the HOI4D dataset used in our experiments, including number of sequences and the index of the start frame.\n' +
      '\n' +
      'pentrations as shown in Figure 9 and 4. Such phenomena cannot be mitigated by augmenting it with general domain generalization techniques. Figure 12 demonstrates that the improved version, TOCH with MixStyle, also yields similar unnatural results. This suggests that the novel noise distribution presents a challenging obstacle for the denoising model to generalize to new noisy interaction sequences. In contrast, our method does not have such difficulty in handling the shifted noise distribution.\n' +
      '\n' +
      '**Results of TOCH and TOCH (w/ Aug.) on the HOI4D dataset.** Figure 11 compares the results of TOCH (w/ Aug.) with our method. In the example of opening a scissor, TOCH produces very strange "flying hands" trajectories, for which please refer to our video for an intuitive understanding. Though the results produced by the improved version do not exhibit the "flying hands" artifacts, it is still very strange, stiff, suffering from very unnatural hand poses, and cannot perform correct manipulations. The results of TOCH and TOCH (w/ Aug.) on the ToyCar example are very similar since our experiments indeed get very similar results from such two models in this case. They both are troubled by strange hand shapes and very unnatural trajectories.\n' +
      '\n' +
      '**More results on the ARCTIC dataset - novel interactions with new objects involving dynamic object motions and changing contacts.** Figure 13 shows qualitative evaluations on the ARCTIC test set to test the ability of our denoising model to clean noisy and dynamic interactions with changing contacts.\n' +
      '\n' +
      'Besides, we include samples of our results on longer sequences with bimanual manipulations in Figure 14.\n' +
      '\n' +
      '**Multi-state denoising v.s. one-stage denoising.** We leverage a multi-stage denoising strategy in this work to tackle the challenge posed by complex interaction noise. In Section A.2, we demonstrate the stage-wise denoising strategy gradually projects the input trajectory from the manifold containing unnatural interactions, to the manifold of trajectories with natural hand motions, to the manifold\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline Dataset & Method & MPJPE & MPVPE & C-IoU \\\\  & & (\\(mm\\), \\(\\downarrow\\)) & (\\(mm\\), \\(\\downarrow\\)) & (\\%, \\(\\uparrow\\)) \\\\ \\hline \\multirow{6}{*}{GRAB} & Input & 23.16 & 22.78 & 1.01 \\\\  & TOCH & 12.38 & 12.14 & 23.31 \\\\  & TOCH (w/ MixStyle) & 13.36 & 13.03 & 23.70 \\\\  & TOCH (w/ Aug.) & 12.23 & 11.89 & 22.71 \\\\  & Ours (w/o SpatialDiff) & **7.83** & **7.67** & 26.09 \\\\  & Ours (w/o TemporalDiff) & _8.27_ & _8.13_ & **26.55** \\\\  & Ours (w/o Diffusion) & 8.52 & 8.38 & _26.44_ \\\\  & Ours (w/o Canon.) & 10.15 & 10.07 & 24.92 \\\\  & Ours & 9.28 & 9.22 & 25.27 \\\\ \\hline \\multirow{6}{*}{GRAB (Beta)} & Input & 17.65 & 17.40 & 13.21 \\\\ \\cline{2-5}  & TOCH & 24.10 & 22.90 & 16.32 \\\\ \\cline{1-1}  & TOCH (w/ MixStyle) & 22.79 & 21.19 & 16.28 \\\\ \\cline{1-1}  & TOCH (w/ Aug.) & 11.65 & _10.47_ & _24.81_ \\\\ \\cline{1-1}  & Ours (w/o Diffusion) & 12.16 & 11.75 & 22.96 \\\\ \\cline{1-1}  & Ours (w/o Canon.) & _10.89_ & 10.61 & 24.68 \\\\ \\cline{1-1}  & Ours & **9.09** & **8.98** & **26.76** \\\\ \\hline \\multirow{6}{*}{ARCTIC} & Input & 25.51 & 24.84 & 1.68 \\\\ \\cline{1-1} \\cline{2-5}  & TOCH & 14.34 & 14.07 & 20.32 \\\\ \\cline{1-1}  & TOCH (w/ MixStyle) & _13.82_ & _13.58_ & _21.70_ \\\\ \\cline{1-1}  & TOCH (w/ Aug.) & 14.18 & 13.90 & 20.10 \\\\ \\cline{1-1}  & Ours & **11.57** & **11.09** & **23.49** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Quantitative evaluations and comparisons.** Performance comparisons of our method, baselines, and ablated versions on different test sets using the _first set of evaluation metrics_. **Bold red** numbers for best values and _italic blue_ values for the second best-performed ones.\n' +
      '\n' +
      'with correct spatial relations, and to the manifold of trajectories with consistent temporal relations. One may question whether it is possible to use the last projection step only to project the input to the natural interaction manifold in a single step. Our experimental observations show the difficulty of removing such complex noise in one single stage. An effective mapping to clean such complex noise is very hard to learn for neural networks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c} \\hline \\hline \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{MPPE} & \\multicolumn{2}{c}{MVPE} & \\multicolumn{2}{c}{C-tol} & \\multicolumn{2}{c}{IV} & \\multicolumn{2}{c}{Penetration Depth} & \\multicolumn{2}{c}{Proximity Error} & \\multicolumn{2}{c}{HO Motion Consistency} \\\\  & & \\((mm,\\downarrow)\\) & \\((mm,\\downarrow)\\) & \\((\\%,\\uparrow)\\) & \\((cm^{3},\\downarrow)\\) & & \\((mm,\\downarrow)\\) & \\((mm^{2},\\downarrow)\\) \\\\ \\hline \\multirow{4}{*}{GRAB} & GT & - & - & - & 0.50 & 1.33 & - & 0.51 \\\\  & Input & 23.16 & 22.78 & 1.01 & 4.48 & 5.25 & 13.29 & 881.23 \\\\  & \\multirow{4}{*}{Ours (GRAB)} & **9.28** & **9.22** & **25.27** & **1.23** & **1.74** & **2.53** & 0.57 \\\\  & & Ours (ARTCIT) & 11.47 & 11.29 & 24.79 & 1.48 & 1.80 & 2.60 & **0.55** \\\\ \\hline \\multirow{4}{*}{HOHD} & Input & - & - & - & 2.26 & 2.47 & - & 46.45 \\\\  & \\multirow{4}{*}{Ours (GRAB)} & - & - & - & 1.99 & 2.15 & - & 9.81 \\\\  & & Ours (ARTCIT) & - & - & - & **1.54** & **1.96** & - & **9.33** \\\\ \\hline \\multirow{4}{*}{ARCITC} & GT & - & - & - & 0.33 & 0.92 & 0 & 0.41 \\\\  & Input & 25.51 & 24.84 & 1.68 & 2.28 & 4.89 & 15.21 & 931.69 \\\\ \\cline{1-1}  & \\multirow{4}{*}{Ours (GRAB)} & 11.57 & 11.09 & 23.49 & 1.35 & 1.93 & 2.71 & **0.92** \\\\ \\cline{1-1}  & & Ours (ARTCIT) & **19.34** & **10.07** & **25.08** & **1.21** & **1.64** & **2.62** & 1.10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Quantitative evaluations of the model trained on the ARCTIC training set. Bold red numbers for best values. “GT” stands for “Ground-Truth”.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline Dataset & Method & \\multicolumn{2}{c}{IV} & \\multicolumn{2}{c}{Penetration Depth} & \\multicolumn{2}{c}{Proximity Error} & \\multicolumn{2}{c}{HO Motion Consistency} \\\\  & & \\((cm^{3},\\downarrow)\\) & \\((mm,\\downarrow)\\) & \\((mm,\\downarrow)\\) & \\((mm,\\downarrow)\\) & \\((mm^{2},\\downarrow)\\) \\\\ \\hline \\multirow{4}{*}{GRAB} & GT & 0.50 & 1.33 & 0 & 0.51 \\\\  & Input & 4.48 & 5.25 & 13.29 & 881.23 \\\\ \\cline{1-1}  & \\multirow{4}{*}{TOCH} & 2.09 & 2.17 & 3.12 & 20.37 \\\\  & & TOCH (w/ MixStyle) & 2.28 & 2.62 & 3.10 & 21.29 \\\\ \\cline{1-1}  & \\multirow{4}{*}{Ours (w/o SpatialDiff)} & 1.94 & 2.04 & 3.16 & 22.58 \\\\ \\cline{1-1}  & & Ours (w/o SpatialDiff) & 2.15-0.02 & 2.29+0.03 & 6.71+1.09 & 12.16+0.67 \\\\ \\cline{1-1}  & & Ours (w/o TemporalDiff) & **0.86\\(\\pm\\)**0.02 & **1.54\\(\\pm\\)**0.02 & 3.93\\(\\pm\\)0.31 & 9.36\\(\\pm\\)0.68 \\\\ \\cline{1-1}  & & Ours (w/o Diffusion) & _1.07_ & _1.70_ & _2.63_ & 10.05 \\\\ \\cline{1-1}  & & Ours (w/o Canon.) & 1.57\\(\\pm\\)0.02 & 1.83\\(\\pm\\)0.03 & 2.91\\(\\pm\\)0.28 & _1.30\\(\\pm\\)_0.03 \\\\ \\cline{1-1}  & & Ours & 1.22\\(\\pm\\)0.01 & 1.72\\(\\pm\\)0.01 & **2.44\\(\\pm\\)**0.18 & **0.41\\(\\pm\\)**0.01 \\\\ \\hline \\multirow{4}{*}{GRAB (Beta)} & GT & 0.50 & 1.33 & 0 & 0.51 \\\\  & Input & 2.19 & 4.77 & 5.83 & 27.58 \\\\ \\cline{1-1}  & \\multirow{4}{*}{TOCH (w/ MixStyle)} & 2.33 & 2.77 & 5.60 & 25.05 \\\\ \\cline{1-1}  & & TOCH (w/ MixStyle) & 2.01 & 2.63 & 4.65 & 17.37 \\\\ \\cline{1-1}  & & TOCH (w/ Aug.) & 1.52 & 1.86 & 3.07 & 13.09 \\\\ \\cline{1-1}  & & Ours (w/o Diffusion) & 1.98 & 2.06 & _3.00_ & 11.99 \\\\ \\cline{1-1}  & & Ours (w/o Canon.) & _1.79\\(\\pm\\)_0.02 & _1.73\\(\\pm\\)_0.03 & 3.19\\(\\pm\\)0.15 & _1.28\\(\\pm\\)_0.03 \\\\ \\cline{1-1}  & & Ours & **1.18\\(\\pm\\)**0.00 & **1.69\\(\\pm\\)**0.01 & **2.78\\(\\pm\\)**0.14 & **0.54\\(\\pm\\)**0.00 \\\\ \\hline \\multirow{4}{*}{HOH4D} & Input & 2.26 & 2.47 & - & 46.45 \\\\ \\cline{1-1}  & \\multirow{4}{*}{TOCH} & 4.09 & 4.46 & - & 35.93 \\\\ \\cline{1-1}  & & TOCH (w/ MixStyle) & 4.31 & 4.96 & - & 25.67 \\\\ \\cline{1-1}  & & TOCH (w/ Mix_Style) & 4.20 & 4.51 & - & 25.85 \\\\ \\cline{1-1}  & & Ours (w/o Diffusion) & 3.16 & 3.83 & - & 18.65 \\\\ \\cline{1-1}  & & Ours (w/o Canon.) & _2.37\\(\\pm\\)_0.02 & _3.57\\(\\pm\\)_0.03 & - & _12.80\\(\\pm\\)_0.79 \\\\ \\cline{1-1}  & & Ours & **1.99\\(\\pm\\)**0.02 & **2.14\\(\\pm\\)**0.02 & - & **9.75\\(\\pm\\)**0.88 \\\\ \\hline \\multirow{4}{*}{ARCITC} & GT & 0.33 & 0.92 & 0 & 0.41 \\\\ \\cline{1-1}  & & Input & 2.28 & 4.89 & 15.21 & 931.69 \\\\ \\cline{1-1}  & & TOCH & 1.84 & 2.01 & 4.31 & 18.50 \\\\ \\cline{1-1}  & & TOCH (w/ MixStyle) & 1.92 & 2.13 & 4.25 & _18.02_ \\\\ \\cline{1-1}  & & TOCH (w/ Aug.) & _1.75_ & _1.98_ & 5.64 & 22.57 \\\\ \\cline{1-1}  & & Ours & **1.35**\\(\\pm\\) 0.01 & **1.91**\\(\\pm\\) 0.02 & **2.69**\\(\\pm\\) 0.11 & **0.85**\\(\\pm\\) 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Quantitative evaluations and comparisons.** Performance comparisons of our method, baselines, and ablated versions on different test sets using the _second set of evaluation metrics_. **Bold red numbers for best values and _italic blue_ values for the second best-performed ones. “GT” stands for “Ground-Truth”.\n' +
      '\n' +
      '**Generalize from ARCTIC to other datasets.** To further evaluate the generalization ability of our method, we conduct a new series of experiments where we train the model on the ARCTIC dataset (see the Section C for data splitting and other settings) and evaluate on GRAB, HOI4D, and ARCTIC (test split). Table 6 contains its performance. We can observe that though our model trained on the GRAB can generalize to ARCTIC with good performance, the reduced domain gap when using ARCTIC as the training set can really improve the performance. For instance, Figure 15 shows that the model trained on the ARCTIC training set can perform obviously better on examples where the model trained on GRAB would struggle (please see Section B.4 for the discussion on failure case). For the sequence where the hand needs to open wide to hold the microwave, the model trained on GRAB cannot clean the noisy very effectively, producing results with obvious penetrations and the unnatural hand trajectory with instantaneous shaking. However, the model trained on the ARCTIC dataset can eliminate such noise and produce a much natural trajectory. Besides, training on this\n' +
      '\n' +
      'Figure 8: **Evaluation and comparisons on the GRAB test set.** Input and denoised results are shown from three views via four keyframes in the time-increasing order. Please refer to **our website and video** for animated results.\n' +
      '\n' +
      'dataset can benefit the model\'s performance on the HOI4D dataset with articulated objects and articulated motions.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'This section includes more ablation study results to complement the selected results in the main text. Table 4 and 5 present a more comprehensive quantitative evaluation of ablated models and the comparisons to the full model.\n' +
      '\n' +
      '**Generalized contact-centric parameterizations.** Apart from the results present in Table 4 and 5, Figure 16 gives and visual example where the ablated version without such contact-centric design cannot generalize well to the manipulation sequence with large object movements. We can still observe obvious penetrations from all three frames present here.\n' +
      '\n' +
      '**Denoising capability on recovering ground-truth and modeling high-frequency pose details.** Together with the ability to model various solutions, the stochastic denoising process also empowers the model to explore a broad space that is more likely to encompass samples close to the ground-truth sequences. Figure 17 shows that ours is more faithful to the ground-truth sequence than the result of TOCH, regarding both recovered hand poses and the contact information.\n' +
      '\n' +
      'Besides, taking advantage of the power of our HOI representation GeneOH and the novel denoising scheme, we are able to model high-frequency shape details in the results. However, TOCH\'s results would exhibit flat hand poses frequently. This may result from its high-dimensional representations\n' +
      '\n' +
      'Figure 9: **Evaluation and comparisons on the GRAB (Beta) test set.** Please refer to **our website and video** for animated results.\n' +
      '\n' +
      'Figure 10: **Evaluation on the HOI4D dataset.** Please refer to **our website and video** for animated results.\n' +
      '\n' +
      'and the limited ability of the denoising strategy, which models the deterministic, one-step noise-to-data mapping relation.\n' +
      '\n' +
      '### Applications\n' +
      '\n' +
      'This section presents more applications of the denoising model.\n' +
      '\n' +
      '**Refining noisy grasps produced by the generation network.** In addition to refining noisy interaction sequences, our method can serve as an effective post-processing tool to refine implausible static grasps produced by the generation network as shown in Figure 6. Examples shown here are grasps taken from interaction results produced by (Wu et al., 2022).\n' +
      '\n' +
      'Figure 11: **Comparisons on the HOI4D dataset.** We compare our method with the baseline TOCH and its improved version TOCH (w/ Aug.).\n' +
      '\n' +
      'Figure 12: **Weird artifacts produced by TOCH (w/ MixStyle).** (_First line_:) Ours result. (_Second line_:) The result of TOCH (w/ MixStyle). The noisy input is perturbed by noise sampled from a Beta distribution, different from that used in training.\n' +
      '\n' +
      'Figure 14: **Evaluation on long interaction sequences with bimanual manipulations.** The model cleans both the noisy right hand trajectory and the noisy left hand trajectory here. Please refer to **our website and video** for animated results.\n' +
      '\n' +
      'Figure 13: **Evaluation on the ARCTIC dataset.** The model cleans noisy right hand trajectory here. Left hands shown in both the noisy input and the denoised trajectory are GT shapes. Please refer to **our website and video** for animated results.\n' +
      '\n' +
      'Apart from the denoising ability, the denoised data with high-quality interaction sequences and static grasps can further aid a variety of downstream tasks. Here we take the grasp synthesis and the manipulation synthesis task as an example.\n' +
      '\n' +
      '**Grasp synthesis.** We select four objects and their corresponding grasping poses from the GRAB test set to train the synthesis network. Then we use the network to generate grasps for unseen objects. The results shown in Figure 18 are natural and contact-aware. In contrast, the generated grasps are not plausible as shown in the leftmost part of Figure 18.\n' +
      '\n' +
      '**Manipulation synthesis.** We further examine the quality of the denoised interaction data via the manipulation synthesis task. Based on the representations and the network architecture proposed in a recent manipulation synthesis work1, we train a manipulation synthesis network using our denoised data. The network then takes a new object sequence as input to generate the corresponding manipulation sequence. As shown in Figure 19, the quality of our data is well suited for a learning-based synthesis model. It can generate diverse, high-quality manipulation sequences for an unseen object trajectory.\n' +
      '\n' +
      'Footnote 1: [https://github.com/cams-hoi/CAMS](https://github.com/cams-hoi/CAMS)\n' +
      '\n' +
      'The above two applications indicate the potential value of our denoising model in aiding high-quality interaction dataset creation.\n' +
      '\n' +
      '### Failure Cases\n' +
      '\n' +
      'Figure 20 summarizes the failure cases. Our method may sometimes be unable to perform very well in the following situations: 1) When the hand needs to open wide to hold the object, the canonicalized hand trajectories and the hand-object spatial relations canonicalized around the interaction region may be extremely novel to the denoising model. The model then cannot fully clean penetrations from the observations. 2) When the noisy input contains very strange hand motions such as the sudden detachment and grasping presented in Figure 20, the model can remove such artifact but still cannot clean the trajectory perfectly, leaving us remaining penetrations shown in the denoised result. 3) When the hand is opening an unseen object with extremely thin geometry, we may still observe subtle penetrations from the results.\n' +
      '\n' +
      'Figure 16: **Effectiveness of the generalized contact-centric parameterization.** (_First line:_) Results of Ours (w/o Canon.). (_Second line:_) Results of our full model.\n' +
      '\n' +
      'Figure 15: **Comparisons between the model trained on ARCTIC and the one trained on GRAB.** The model trained on ARCTIC training set can generalize to the corresponding test sequences more easily thanks to the reduced domain gap.\n' +
      '\n' +
      'Analyzing the Distinction between Noise in Real Hand-Object Interaction Trajectories and Artificial Noise\n' +
      '\n' +
      'From the visualization and animated results shown on the website and the video, the distinctions between the noise exhibited in real noisy hand-object interaction trajectories (_e.g.,_ hand trajectories from the noisy HOI4D dataset and hand trajectories estimated from interaction videos) and the artificial noise could be summarized as follows:\n' +
      '\n' +
      '* The trajectories in HOI4D always present unnatural hand poses, jittering motions, missing contacts, and large penetrations;\n' +
      '* Retargeed hand motions always suffer from large penetrations;\n' +
      '* The hand trajectories estimated from HOI videos are usually with penetrations and missing contacts;\n' +
      '* A common feature is that real noisy hand trajectories always present time-consistent artifacts. However, noisy trajectories with artificial noise added independently onto each frame usually present time-varying penetrations and unnatural poses.\n' +
      '\n' +
      'Besides, as summarized in Table 1, the differences between different kinds of noise patterns can be revealed by comparing various metrics calculated on their input noisy trajectories, including metrics to reveal penetrations (IV, penetration depth), hand-object proximity (C-IoU, Proximity Error), and motion consistency.\n' +
      '\n' +
      '**Further analysis.** We further visualize the difference (\\(\\hat{\\theta}-\\theta^{gt}\\)) between noisy hand mano pose parameters (\\(\\hat{\\theta}\\)) and the GT values (\\(\\theta^{gt}\\)) obtained from the trajectories estimated from videos (the noisy input of the application on cleaning hand trajectory estimations, Sec. 4.4), the difference between the mano pose parameters with artificial Gaussian noise (\\(\\hat{\\theta}^{\\textbf{n}}\\)) and the GT values, and the differences between the parameters with artificial noise drawn from the Beta distribution (\\(\\hat{\\theta}^{\\textbf{b}}\\)). By projecting them into the 2-dimensional plane using the PCA algorithm (implemented in the scikit-learn package), we visualize their positions from 256 examples in Figure 21. As we can see, the real noise pattern is very different from artificial noise. In this case, the noise of the hand trajectory estimated from videos further exhibits instance-specific patterns.\n' +
      '\n' +
      'Figure 17: **Comparisons between our model and TOCH on the ability to recover ground-truth interactions.** We can explore a wide space that encompasses the sample with hand poses (high-lighted in yellow circles ) and contacts (in green circles) close to the ground truth. We can model high-frequency poses. However, TOCH’s result contains plain poses and cannot recover bending fingers exhibited in the ground-truth shape.\n' +
      '\n' +
      'Figure 18: **Grasp synthesis.** Synthesized grasps for unseen objects.\n' +
      '\n' +
      '### User Study\n' +
      '\n' +
      'To better access and compare the quality of our denoised results to those of the baseline model, we conducted a toy user study. We set up a website containing our denoised results and TOCH\'s results on 18 noisy trajectories in a randomly permutated order. Twenty people who are not familiar with the task or even have no background in CS are asked to rate each clip a score from 1 to 5, indicating their preferences. Specifically, "1" indicates a significant difference between the hand motion demonstrated in the video and the human behavior, with obvious physical unrealistic phenomena such as penetrations and motion inconsistency; "3" represents the demonstrated motion is plausible and similar to the human behavior, but still suffer from physical artifacts; "5" means a high-quality motion which is plausible with no flaws and is human-like. "2" means the quality is better than "1" but worse than "3". Similarly, "4" means the result is better than "3" but worse than "5".\n' +
      '\n' +
      'For each clip, we calculate the average score achieved by our method and TOCH. The average and medium scores across all clips are summarized in Table 7. Ours is much better than the baseline model.\n' +
      '\n' +
      'Figure 19: **Manipulation synthesis. Synthesized manipulation sequences for the unseen laptop object. Frames shown here from left to right are in a time-increasing order.**\n' +
      '\n' +
      'Figure 20: **Failure cases caused by the _unseen and large object_, _very strange hand-object temporal relations_, and _unseen object with extremely thin geometry_.\n' +
      '\n' +
      '## Appendix C Experimental Details\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '**GRAB training set.** This is the training set used in all experiments presented in the **main text**. We follow the cross-object splitting strategy used in (Zhou et al., 2022) to split the GRAB (Taheri et al., 2020) dataset. The training split, containing 1308 manipulation sequences, is used to construct the training dataset. We also filter out frames where the hand wrist is more than 15 cm away from the object. For each training sequence, we slice it into clips with 60 frames to construct the training set. Sequences with a length of less than 60 are not included for training or testing. For models where noisy sequences are required during training, we create the noisy sequence from the clean sequence by adding Gaussian noise to the MANO parameters. Specifically, the Gaussian noise is added to the hand MANO translation, rotation, and pose parameters, with standard deviations of 0.01, 0.1, and 0.5, respectively.\n' +
      '\n' +
      '**ARCTIC training dataset.** It is the training set of all models in the experiments where we wish to generalize the model trained on ARCTIC to other datasets. Based on the publicly available sequences from the subject with the index "s01", "s02", "s04", "s05", "s06", "s07", "s08", "s09", "s10", we take the manipulation sequences from "s01" for evaluation and those of other subjects for training. For each sequence, we slice it into small clips with a window size equal to 60 and the step size set to 60. We filter out clips where the maximum distance from the wrist to the nearest object point is larger than 15cm. The number of all training clips is 2524. Only the right hand trajectory is used for training.\n' +
      '\n' +
      'The following text contains more details about the four distinct test sets for evaluation, namely GRAB, GRAB (Beta), HOI4D, and ARCTIC.\n' +
      '\n' +
      '**GRAB**(Taheri et al., 2020). The test split of the GRAB dataset, containing 246 manipulation sequences, is used to construct the test set. For each test sequence, we slice it into clips with 60 frames using the step size 30. For each test sequence, the noisy sequence is also created by adding Gaus\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & GeneOH Diffusion & TOCH \\\\ \\hline Average Score & **3.96** & 1.98 \\\\ \\hline Medium Score & **4.00** & 1.55 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **User study.**\n' +
      '\n' +
      'Figure 21: Visualization on the differences between the mano pose parameters of hand trajectories estimated from videos and the GT values, the difference between the mano pose parameters with artificial Gaussian noise (\\(\\hat{\\theta}^{\\textbf{n}}\\)) and the GT values, and the differences between the parameters with artificial noise drawn from the Beta distribution (\\(\\hat{\\theta}^{\\textbf{b}}\\)). The analysis is conducted on 256 trajectories. For each trajectory, the difference vectors across all frames are concatenated together and flattened to a single vector.\n' +
      '\n' +
      'sian noise to the MANO parameters. The Gaussian noise is added to the hand MANO translation, rotation, and pose parameters, with standard deviations of 0.01, 0.1, and 0.5, respectively.\n' +
      '\n' +
      '**GRAB (Beta).** The GRAB (Beta) test set is constructed from manipulation sequences from the GRAB test split. For each sequence, the noisy sequence is created by adding noise from the Beta distribution (\\(B(8,2)\\)) to the MANO parameters. Specifically, we randomly sample noise from the Beta distribution (\\(B(8,2)\\)). Then the sampled noise was by 0.01, 0.05, 0.3 to get noise vectors added to the translation, rotation, and hand pose parameters respectively.\n' +
      '\n' +
      '**HOI4D**Liu et al. (2022). For the HOI4D dataset, we select interaction sequences with humans manipulating objects from 3 articulated categories, including Laptop, Scissors, and Pliers, and 6 rigid datasets, namely Chair, Bottle, Bowl, Kettle, Mug, and ToyCar, for test. The number of instances included in each category is detailed in Table 3. For each sequence, we specify the starting frame and take the clip with the length of 60 frames starting from it as the test clip. The starting frame set for each category is listed in Table 3.\n' +
      '\n' +
      '**ARCTIC**Fan et al. (2023). The ARCTIC test set for evaluation takes the right hand only since we observe that dexterous manipulation such as articulated manipulations is always conducted by the right hand. For instance, as shown in the example in Figure 4, the left hand holds the capsule machine with no contact change during the manipulation while the right hand first touches the lid, then touches the base, and then opens and close the lid. _However, we can refine the left hand motions as well_, as demonstrated in the second sequence of the "refining estimation from video" example shown in Figure 1. The manipulation sequences from "s01", 34 in total, are taken for evaluation. For each test sequence, the quantitative results are evaluated from clips with the window size 60 sliced from each sequence using the step size 30. The filtering strategy similar to that used for constructing the training dataset is applied to the test clips as well. The default length of the clips used in the qualitative evaluation is 90, which is the composed result of two adjacent clips with a window size of 60. The noisy sequence is obtained by adding Gaussian noise to the right hand MANO parameters. Specifically, the Gaussian noise is added to the hand MANO translation, rotation, and pose parameters, with standard deviations of 0.01, 0.05, and 0.3, respectively.\n' +
      '\n' +
      'Since the ARCTIC\'s object template meshes do not provide vertex normals, which are demanded both in our method and some baseline models, we use the "compute_vertex_normals" function implemented in Open3D Zhou et al. (2018) for computing the vertex normals.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'We include two sets of evaluation metrics. The first set follows the evaluation protocol of previous works Zhou et al. (2022) and focuses on assessing the model\'s capability to recover the GT trajectories from noisy observations, as detailed in the following.\n' +
      '\n' +
      '**Mean Per-Joint Position Error (MPJPE).** It calculates the average Euclidean distance between the denoised 3D hand joints and the corresponding ground-truth joints.\n' +
      '\n' +
      '**Mean Per-Vertex Position Error (MPVPE).** It measures the average Euclidean distance between the denoised 3D hand vertices and the corresponding ground-truth vertices.\n' +
      '\n' +
      '**Contact IoU (C-IoU).** This metric assesses the similarity between the refined contact map and the ground-truth contact map. The binary contact maps are obtained by thresholding the correspondence distance within \\(\\pm 2\\)mm. For our method, which does not rely on correspondences introduced in Zhou et al. (2022), we utilize the computing process provided by Zhou et al. (2022) to compute the correspondences.\n' +
      '\n' +
      'To measure whether the denoised trajectory exhibits natural hand-object spatial relations and consistent hand-object motions, we introduce the second set of evaluation metrics.\n' +
      '\n' +
      '**Solid Intersection Volume (IV).** We evaluate this metric following Zhou et al. (2022). It quantifies hand-object inter-penetrations. By voxelizing the hand mesh and the object mesh, we calculate the volume of their intersected region as the intersection volume.\n' +
      '\n' +
      '**Per-Vertex Maximum Penetration Depth (Penetration Depth).** For each frame, we calculate the maximum penetration depth of each hand vertex into the object. We then average these values across all frames to obtain the per-vertex maximum penetration depth.\n' +
      '\n' +
      '**Proximity Error.** The metric is only evaluated on datasets with ground-truth references, including GRAB, GRAB (Beta), and ARCTIC. For each vertex of the denoised hand mesh, we compute the difference between its minimum distance to the object points and the corresponding ground-truth vertex\'s minimum distance to the object points. The proximity error is obtained by averaging these differences over all vertices. The overall metric is obtained by averaging the per-frame metric over all frames. Specifically, let \\(d^{\\mathbf{h}}_{k,min}\\) denote the minimum distance from the hand keypoint \\(\\mathbf{h}_{k}\\) at the frame \\(k\\) to objects points. Formally, it is defined as \\(d^{\\mathbf{h}}_{k,min}=\\min\\{d^{\\mathbf{ho}}_{k}=\\|\\mathbf{h}_{k}-\\mathbf{o}_{ k}\\|_{2}|\\mathbf{h}_{k}\\in\\mathbf{J}_{k},\\mathbf{o}_{k}\\in\\mathbf{P}_{k}\\}\\). Let \\(d^{\\mathbf{h}}_{k,min}\\) represents the quantity of the keypoint \\(\\mathbf{h}\\) from the denoised trajectory, and \\(d^{\\mathbf{h}}_{k,min}\\) represents the quantity of the keypoint \\(\\mathbf{h}\\) from the ground-truth trajectory. Then the overall metric is calculated as Proximity error \\(=\\text{mean}\\{\\|d^{\\mathbf{h}}_{k,min}-d^{\\mathbf{h}}_{k,min}\\|_{2}|\\mathbf{ h}_{k}\\in\\mathbf{J}_{k}\\}|1\\leq k\\leq K\\}\\).\n' +
      '\n' +
      '**Hand-Object Motion Consistency (HO Motion Consistency)**. This metric assesses the consistency between the hand and object motions. For each frame where the object is not static, we identify the nearest hand-object point pair \\((\\mathbf{h}_{k},\\mathbf{o}_{k})=\\text{argmin}\\{d^{\\mathbf{ho}}_{k}|(\\mathbf{ h}_{k}\\in\\mathbf{J}_{k},\\mathbf{o}_{k}\\in\\mathbf{P}_{k})\\}\\). We use the expression \\(\\|e^{-100\\|\\mathbf{h}_{k}-\\mathbf{o}_{k}\\|_{2}}\\Delta\\mathbf{h}_{k}-\\Delta \\mathbf{o}_{k}\\|_{2}^{2}\\) to quantify the level of inconsistency between the hand and object motions. Here, \\(\\Delta\\mathbf{h}_{k}\\) and \\(\\Delta\\mathbf{o}_{k}\\) represent the displacements of the hand point and the object point between adjacent frames, respectively. We obtain the overall metric by averaging the metric over all frames.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      'We give a more detailed explanation of the compared baselines as follows to complement the brief introduction in the main text.\n' +
      '\n' +
      '**TOCH.** We compare our model with the prior art on the HOI denoising problem, TOCH (Zhou et al., 2022). TOCH utilizes an autoencoder structure and learns to map noisy trajectories to their corresponding clean trajectories. By projecting input noisy trajectories onto the clean data manifold, it can accomplish the denoising task. We utilize the official code provided by the authors for training and evaluation.\n' +
      '\n' +
      '**TOCH (w/ MixStyle).** Further, to improve TOCH\'s generalization ability towards new interactions, we augment it with a general domain generalization method MixStyle (Zhou et al., 2021), resulting in a variant named "TOCH (w/ MixStyle)".\n' +
      '\n' +
      '**TOCH (w/ Aug.).** Another variant, "TOCH (w/ Aug.)", where TOCH is trained on the training sets of the GRAB and GRAB (Beta) datasets, is further introduced to enhance its robustness towards unseen noise patterns.\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      '**Denoising models used in our method.** We realize the denoising model\'s function and the training as those of the score functions in diffusion-based generative models. We adapt the implementation of Human Motion Diffusion (Tevet et al., 2022) to implement our three denoising models for each part of the representation2. Instead of training the denoising function to predict the start data point \\(\\mathbf{x}_{0}\\) from the noisy data \\(\\mathbf{x}_{t}\\) as implemented in (Tevet et al., 2022), we predict the noise (\\(\\mathbf{x}_{t}-\\mathbf{x}_{0}\\)). We also follow its default training protocol.\n' +
      '\n' +
      'Footnote 2: [https://guytevet.github.io/mdm-page](https://guytevet.github.io/mdm-page)\n' +
      '\n' +
      'We mainly adopt MLPs and Transformers as the basic backbones of the denoising model. The detailed structure depends on the type of the corresponding statistics and the dimensions. **The code in the Supplementary Materials provides all those details.** So we spare the effort to list them in detail here.\n' +
      '\n' +
      'When leveraging the denoising model to clean the input via the "denoising via diffusion" strategy, the diffusion steps is set to 400 for MotionDiff, 200 for SpatialDiff, and 100 for TemporalDiff empirically.\n' +
      '\n' +
      '**Ours (w/o Diffusion).** In this ablated version, we design a denoising autoencoder for cleaning the spatial and temporal representations. For each representation \\(\\tilde{\\mathcal{J}},\\mathcal{S},\\mathcal{T}\\), we leverage an autoencoder for denoising. After that, we get the final hand meshes by fitting the MANO parameters\\(\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}\\) to reconstruct the denoised representations. Assuming the reconstructed hand trajectory as \\(\\mathcal{J}^{recon}\\), the reconstructed hand-object spatial relative positions as \\(\\mathcal{S}^{recon}\\), and the temporal representations as \\(\\mathcal{T}^{recon}\\), the reconstruction loss is formulated as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}^{rep}_{recon}=\\lambda_{1}\\|\\mathcal{J}-\\mathcal{J}^{recon}\\|_{2}+ \\lambda_{2}\\|\\mathcal{S}-\\mathcal{S}^{recon}\\|+\\lambda_{3}\\|\\mathcal{T}- \\mathcal{T}^{recon}\\|, \\tag{14}\\]\n' +
      '\n' +
      'where \\(\\lambda_{1},\\lambda_{2},\\lambda_{3}\\) are coefficients for the reconstruction losses. We set \\(\\lambda_{1},\\lambda_{2},\\lambda_{3}=1\\). in our experiments. The distance function between the spatial representations is calculated on the relative positions between each point pair. The distance between the temporal representations is calculated on hand-object distances, _i.e._, \\(\\{d_{k}^{\\text{ho}}\\}\\), and two relative velocity-related statistics ( \\(\\{e_{k,\\parallel}^{\\text{ho}},e_{k,\\perp}^{\\text{ho}}\\}\\) ). Together with the regularization loss\n' +
      '\n' +
      '\\[\\mathcal{L}_{reg}=\\frac{1}{K}\\sum_{k=1}^{K}(\\|\\beta_{k}\\|_{2}+\\|\\theta_{k}\\|_ {2})+\\frac{1}{K-1}\\sum_{k=1}^{K-1}\\|\\theta_{k+1}-\\theta_{k}\\|_{2}, \\tag{15}\\]\n' +
      '\n' +
      'the total optimization target is formulated as follows,\n' +
      '\n' +
      '\\[\\text{minimize}_{\\{\\mathbf{r}_{k},\\mathbf{t}_{k},\\beta_{k},\\theta_{k}\\}_{k =1}^{K}}(\\mathcal{L}^{rep}_{recon}+\\mathcal{L}_{reg}), \\tag{16}\\]\n' +
      '\n' +
      'and we employ an Adam optimizer to solve the problem.\n' +
      '\n' +
      '**TOCH (w/ MixStyle).** We use the official code provided to implement the MixStyle layer. We add a MixStyle layer between every two encoder layers of the TOCH model (Zhou et al., 2022). Configurations of MixStyle are kept the same as the default setting.\n' +
      '\n' +
      '**TOCH (w/ Aug.).** The model is trained on paired noisy-clean data pair from the GRAB training set. We perturb each training sequence with two types of noise, that is the noise from a Gaussian distribution, and the noise from a Beta \\(B(8,2)\\) distribution. The noise scale for the Gaussian for the translation, rotation, and hand poses are 0.01, 0.1, and 0.5 respectively. The scale of the Beta noise added on the translation, rotation, and hand poses are 0.01, 0.05, and 0.3.\n' +
      '\n' +
      '**Grasp synthesis network.** We adapt the WholeGrasp-VAE network proposed in (Wu et al., 2022) to a HandGrasp-VAE network3. Instead of using whole-body markers, we use hand anchor points (Yang et al., 2021), composed of 32 points from the hand palm in total. To identify contact maps for both hand anchors and object points, we set a distance threshold, _i.e._, 2 mm, and mark the status of points with the minimum distance to the hand/object as contact. During training, we do not add the ground contact loss since the whole body is not considered in our hand-grasping setting. To train the network, we further split the GRAB test set into a subset containing binoculars, wineglass, fryingpan, and mug for training. Then we use the network to synthesize grasps for unseen objects. We select 100 grasps for each object to construct the training dataset.\n' +
      '\n' +
      'Footnote 3: [https://github.com/JiahaoPlus/SAGA](https://github.com/JiahaoPlus/SAGA)\n' +
      '\n' +
      '**Manipulation synthesis network.** We utilize the denoised manipulation trajectories for the Laptop category to train the manipulation synthesis network. The training data consists of 100 manipulation sequences.\n' +
      '\n' +
      '### Training and Evaluation\n' +
      '\n' +
      '**The denoising model for \\(\\mathcal{\\bar{J}}\\).** The denoising model for the canonicalized hand trajectory \\(\\mathcal{\\bar{J}}\\) is trained on canonicalized hand trajectories \\(\\{\\mathcal{J}\\}\\) of all interaction sequences in the training set. We apply per-instance normalization operation to those points at each frame for centralization and scaling purposes. Specifically, we utilize the mean and the standard deviation statistics calculated for all points across all frames. In more detail, we first concatenate all keypoints over all frames to form the concatenated keypoints \\(\\mathbf{J}^{concat}\\):\n' +
      '\n' +
      '\\[\\bar{\\mathbf{J}}_{\\text{concat}}=\\text{Concat}\\{\\bar{\\mathbf{J}}_{k},\\text{ dim}=0\\}_{k=1}^{K}, \\tag{17}\\]\n' +
      '\n' +
      'where \\(\\bar{\\mathbf{J}}_{k}\\in\\mathbb{R}^{N_{h}\\times 3}\\) for each frame \\(k\\). Then, the average and the standard deviation is calculated on \\(\\bar{\\mathbf{J}}^{\\text{concat}}\\) via\n' +
      '\n' +
      '\\[\\mu^{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{ \\mathbf{ }}}}}}}{}}}{}}}}}}}}}= \\text{Average}(\\bar{\\mathbf{J}}_{\\text{concat}},\\text{ dim}=0) \\tag{18}\\] \\[\\sigma^{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{\\mathbf{ }}}}}}}}{}}{}}{}}{}}}}}}}}= \\text{Std}(\\bar{\\mathbf{J}}_{\\text{concat}},\\text{ dim}=0). \\tag{19}\\]The \\((\\mu^{\\bar{\\mathbf{J}}},\\sigma^{\\bar{\\mathbf{J}}})\\) are utilized to normalize \\(\\bar{\\mathbf{J}}_{k}\\) at each frame \\(k\\), _i.e.,_\n' +
      '\n' +
      '\\[\\bar{\\mathbf{J}}_{k}\\leftarrow\\frac{\\bar{\\mathbf{J}}_{k}-\\mu^{\\mathbf{J}}}{ \\sigma^{\\mathbf{J}}}. \\tag{20}\\]\n' +
      '\n' +
      '**The denoising model for \\(\\mathcal{S}\\)**. Similarly, the denoising model for hand-object spatial relations \\(\\mathcal{S}\\) is trained using representations \\(\\{\\mathcal{S}\\}\\) from all interaction sequences in the training set. We apply per-instance normalization to the canonicalized hand-object relative positions \\(\\{(\\mathbf{h}_{k}-\\mathbf{o}_{k})\\mathbf{R}_{k}^{T}\\}\\). The normalization is conducted in a per-instance per-object point way. For each object point \\(\\mathbf{o}\\) in the generalized contact points \\(\\mathbf{P}\\), we calculate the average and standard deviation of \\(\\{\\{\\mathbf{h}_{k}-\\mathbf{o}_{k}\\}\\}\\) over all frames \\(k\\). We first concatenate the relative positions over all frames and all hand keypoints for the concatenated spatial relations, denoted as\n' +
      '\n' +
      '\\[\\mathbf{s}_{\\text{concat}}^{\\mathbf{o}}=\\text{Concat}\\{\\{\\mathbf{h}_{k}- \\mathbf{o}_{k}\\},\\text{dim}=0\\}_{k=1}^{K}. \\tag{21}\\]\n' +
      '\n' +
      'Then, the average and the standard deviation is calculated on \\(\\mathbf{s}_{\\text{concat}}^{\\mathbf{o}}\\) via\n' +
      '\n' +
      '\\[\\mu^{\\mathbf{o}} =\\text{Average}(\\mathbf{s}_{\\text{concat}}^{\\mathbf{o}},\\text{ dim}=0) \\tag{22}\\] \\[\\sigma^{\\mathbf{o}} =\\text{Std}(\\mathbf{s}_{\\text{concat}}^{\\mathbf{o}},\\text{dim}=0). \\tag{23}\\]\n' +
      '\n' +
      'Such statistics \\((\\mu^{\\mathbf{o}},\\sigma^{\\mathbf{o}})\\) are utilized to normalize the relative positions \\(\\{\\{\\mathbf{h}_{k}-\\mathbf{o}_{k}\\}\\}\\), _i.e.,_\n' +
      '\n' +
      '\\[(\\mathbf{h}_{k}-\\mathbf{o}_{k})\\leftarrow\\frac{(\\mathbf{h}_{k}-\\mathbf{o}_{k}) -\\mu^{\\mathbf{o}}}{\\sigma^{\\mathbf{o}}}. \\tag{24}\\]\n' +
      '\n' +
      '**The denoising model for \\(\\mathcal{T}\\).** When training the denoising model for the hand-object temporal relations \\(\\mathcal{T}\\), we first train an autoencoder, composed of an encode\\((\\cdot)\\) function and a decode\\((\\cdot)\\) function for \\(\\mathcal{T}\\). It takes the \\(\\mathcal{T}\\) as input and decode the hand-object distances \\(\\{d_{k}^{\\mathbf{ho}}\\}\\) and the relative velocity-related quantities \\(\\{e_{k,\\perp}^{\\mathbf{ho}},e_{k,\\parallel}^{\\mathbf{ho}}\\}\\). Then, the denoising model is trained on the encoded latent \\(\\{\\text{encode}(\\mathcal{T})\\}\\). This approach avoids the need for designing normalization strategies for the temporal representations. We adopt a PointNet structure block with a positional encoder followed by a transformer encoder module for encoding the temporal relation representations. Given the input temporal representation \\(\\hat{T}\\in\\mathbb{R}^{K\\times N_{\\times}\\times 69}\\), the PointNet encoder block passes it through four PointNet blocks each with three encoding layers with latent dimension \\((32,32,32),(64,64,64),(128,128,128),(256,256,256)\\) respectively. The transformer encoder module is with parameters "num_heads" as 4, feedforward latent dimension as 1024, dropout rate 0, and the latent dimension 256. The decoder contains fully connected layers for decoding each kind of statistics \\(d_{k}^{\\mathbf{ho}},e_{\\perp,k}^{\\mathbf{ho}},e_{\\parallel,k}^{\\mathbf{ho}}\\) individually.\n' +
      '\n' +
      '**Train-time rotation augmentation.** For the canonicalized hand trajectory representation \\(\\tilde{\\mathcal{J}}\\), the train-time random rotation augmentation applies a single random rotation matrix to the whole canonicalized hand trajectories. The same random rotation matrix, denoted as \\(\\mathbf{R}_{\\text{md}}\\), is added to the hand-object spatial representation \\(\\mathcal{S}\\) as well. It is used to transform the canonicalized object position, normal, and the hand-object offset vector:\n' +
      '\n' +
      '\\[\\mathbf{s}_{\\text{R}}^{\\mathbf{g}}\\mathbf{R}_{\\text{md}}=((\\mathbf{o}_{k}- \\mathbf{t}_{k})\\mathbf{R}_{k}^{T}\\mathbf{R}_{\\text{md}},\\mathbf{n}_{k}\\mathbf{ R}_{k}^{T}\\mathbf{R}_{\\text{md}},\\{(\\mathbf{h}_{k}-\\mathbf{o}_{k})\\mathbf{R}_{k}^{T} \\mathbf{R}_{\\text{md}}|\\mathbf{h}_{k}\\in\\mathbf{J}_{k}\\}). \\tag{25}\\]\n' +
      '\n' +
      'Similarly, the same random rotation matrix \\(\\mathbf{R}_{\\text{md}}\\) is used to transform the object velocity vector from \\(\\mathbf{v}_{k}^{\\mathbf{o}}\\) in the temporal representation \\(\\mathcal{T}\\) to \\(\\mathbf{v}_{k}^{\\mathbf{o}}\\mathbf{R}_{\\text{md}}\\).\n' +
      '\n' +
      '### Complexity and Running Time Discussion\n' +
      '\n' +
      'Denote the number of hand keypoints as \\(|\\mathcal{J}|\\), the number of generalized contact points \\(|\\mathcal{P}|\\), the complexity, the average inference time, and the number of forward diffusion steps for each denoising stage during inference are summarized in Table 8.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & MotionDiff & SpatialDiff & TemporalDiff \\\\ \\hline Average inference time (s) & 0.52 & 16.61 & 7.04 \\\\ \\hline Complexity & \\(\\mathcal{O}(|\\mathcal{J}|)\\) & \\(\\mathcal{O}(|\\mathcal{P}||\\mathcal{J}|)\\) & \\(\\mathcal{O}(|\\mathcal{P}||\\mathcal{J}|)\\) \\\\ \\hline \\#Forward diffusion steps & 400 & 200 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: **Complexity and running time during the inference time.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>