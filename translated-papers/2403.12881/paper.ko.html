<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '대형 L**AN**guage 모델에 대한 재정비입니다. 구체적으로, 에이전트 훈련 코퍼스에서 사전 훈련된 언어 모델의 도메인에 미세 조정 프로세스를 정렬하여 형식 추적과 공통 추론을 분리한다. 이것은 특정 형식 프로토콜에 과도하게 맞지 않고 LLM에서 순수한 에이전트 능력을 이끌어낸다. 에이전트 태스크를 LLM의 기본 역량에 따라 별개의 요소로 추가 분리함으로써 에이전트-FLAN은 각 역량의 다양한 학습 속도에 따라 훈련 유연성을 제공한다. 에이전트 태스크의 환각 문제를 종합적으로 해결하기 위해 LLM의 환각 문제를 다양한 측면에서 평가하는 _Agent-H_ 벤치마크를 구성한다. 그 후, 우리는 이 문제를 효과적으로 완화하기 위해 다양한 \'음성\' 훈련 샘플을 세심하게 큐레이션한다.\n' +
      '\n' +
      '우리는 개방형 Llama2 시리즈에서 에이전트-FLAN을 사용하며, 이는 일반적인 에이전트 작업 및 도구 활용을 포함하여 에이전트 평가 벤치마크 스펙트럼에서 이전 작업을 상당한 3.5% 차이로 능가한다. 또한, 에이전트 튜닝과 관련된 역학, 즉 데이터 및 모델 차원을 지배하는 스케일링 법칙, 일반 작업과 에이전트별 작업 간의 복잡한 관계에 대한 더 깊은 이해도를 제공한다. 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 오픈소스 LLM이 에이전트 도메인에서 경쟁적 성능을 달성하는 것을 방해하는 세 가지 중요한 관찰을 식별하여 에이전트 튜닝의 복잡한 환경에 대한 귀중한 통찰력을 제공한다.\n' +
      '* 이상의 결과를 바탕으로 효과적인 에이전트 능력을 일반적인 LLM에 통합하는 혁신적인 접근 방법인 에이전트-FLAN(Agent-FLAN)을 소개한다: 에이전트 튜닝을 채팅 형식으로 정렬하는 것(SS4.1), 성능 분해 및 데이터 밸런싱(SS4.2), 환각 제거를 위한 음성 샘플 구성(SS4.3).\n' +
      '* 에이전트-FLAN은 에이전트 평가 벤치마크 스펙트럼에 걸쳐 Llama2 시리즈에서 상당한 3.5% 마진으로 이전 작업을 능가한다. 또한, 데이터 및 모델 척도 측면에서 스케일링 법칙을 포함한 에이전트 튜닝의 역학, 일반 태스크와 에이전트별 태스크 간의 복잡한 관계를 추가로 연구한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### LLM Agent\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 출현은 인공지능(AGI)을 향한 상당한 보폭을 나타낸다. LLM의 발전으로 LLM을 기반으로 하는 언어 에이전트는 다양한 작업을 수행하기 위해 세계와 관여하여 연구의 초점이 되었다(Wang et al., 2023; Xi et al., 2023). LLM은 웹 브라우징(Deng et al., 2023; Zhou et al., 2023), 온라인 쇼핑(Yao et al., 2022), 데이터베이스 운영(Liu et al., 2023), 과학 실험(Wang et al., 2022), 관찰 자유 추론(Xu et al., 2023), 위키피디아 Q&A(Yang et al., 2018), 일상 컴퓨터 작업(Kim et al., 2023) 및 가정 탐색(Shridhar et al., 2020) 등 특정 에이전트 작업에 대한 다양한 연구에서 활용되었다.\n' +
      '\n' +
      '특정 과제에 초점을 맞춘 연구 외에도 LLM을 기반으로 한 AI 에이전트에 대한 연구가 진행 중이다. ReAct(Yao et al., 2022)와 같은 일부 연구는 사고 중 행동을 강조하여 다양한 방법에 비해 상당한 개선을 가져왔다. 다른 작업은 주로 에이전트 내부의 인간 및 사회적 속성에 초점을 맞추지만(Mao et al., 2023; Park et al., 2023; Zhang et al., 2023), 다중 에이전트 내의 지능 협력(Chen et al., 2023; Liu et al., 2023; Liang et al., 2023). 위와 달리 에이전트-FLAN은 효과적인 에이전트의 통합을 촉진한다.\n' +
      '\n' +
      '그림 1: **Held-In, Held-Out 작업에 대한 최근 에이전트 튜닝 접근법의 비교. 성능은 더 나은 시각화를 위해 GPT-4 결과로 정규화된다. *는 공정한 비교를 위한 재구현을 나타낸다.**\n' +
      '\n' +
      '일반 LLM의 기능을 통해 모델이 실제 세계의 복잡한 문제를 더 잘 이해하고 해결할 수 있습니다.\n' +
      '\n' +
      '### 언어모델 미세조정\n' +
      '\n' +
      '언어 모델 미세 조정은 또한 특정 작업에 적응하기 위해 사전 훈련된 모델의 조정을 포함하는 연구 핫스팟이며, 출력을 예상과 일치시키는 것을 목표로 한다(Zhang et al., 2023). 모델의 추론 능력을 최적화하기 위한 미세 조정(Liu et al., 2021; Fu et al., 2023), 도구의 숙련도(Patil et al., 2023; Qin et al., 2023; Qiao et al., 2023), 계획 능력(Chen et al., 2023), 검색-증강(Wang et al., 2023) 등에 대한 다양한 연구 연구가 수행되었다. 또한, 미세 조정 방법(Hu et al., 2021; Ivison et al., 2022; Dettmers et al., 2023; Lv et al., 2023), 데이터 선택 원리(Gunasekar et al., 2023) 및 미세 조정 데이터 세트(Sanh et al., 2021; Wang et al., 2022; Honovich et al., 2022; Longpre et al., 2023)에 대해 수행된 다양한 연구가 있다.\n' +
      '\n' +
      '## 3 파일럿 관찰\n' +
      '\n' +
      '이 섹션에서는 후속 조사의 기초가 되는 에이전트 튜닝에 대한 세 가지 중추적인 관찰을 조사한다.\n' +
      '\n' +
      '**관찰 1** 대부분의 에이전트 트레이닝 데이터는 포맷 추종 및 일반적인 추론 둘 다와 얽혀 모델의 원래 사전 트레이닝 언어 도메인, 즉 자연적 대화로부터 상당한 이탈을 야기한다._\n' +
      '\n' +
      '최근의 에이전트 튜닝 작업들(Zeng et al., 2023; Qin et al., 2023)은 언어 모델을 미세 조정하기 위한 ReAct(Thought-Action-Observation)에 의해 예시되는 특정 포맷들의 채택을 지지한다. 더욱이 액션 논증이 JSON 형식으로 자주 제시되고 있다는 점은 주목할 만하다. 형식 지식과 추론 지식을 모두 훈련 코퍼스로 인코딩하면 튜닝 프로세스가 원래 채팅 도메인에서 이동하여 언어 모델에 대한 도메인 외 작업으로 표시된다. 그림 2와 같이 정형화된 데이터와 정상 데이터의 학습 곡선을 비교한다. 포맷된 데이터의 손실은 낮은 값으로 더 빠르게 하강하는 반면, 콘텐츠 손실은 여전히 높게 유지(0.54 vs 0.04)하여 전자가 불충분한 학습 과정으로 이어짐을 분명히 알 수 있다. 이러한 현상은 아마도 모델이 형식 자체에 빠르게 오버핏되는 고정 구조(ReAct, JSON)가 존재하기 때문일 것이다. 결과적으로 학습 데이터에 내재된 기본 추론 능력을 파악하지 못하여 성능이 만족스럽지 못하다.\n' +
      '\n' +
      '**관찰 2** 기본 능력 측면을 따라 훈련 데이터를 명시적으로 분해함으로써, 각각의 손실은 상이한 수렴 곡선을 나타내며, 이는 LLMs._LMs._의 에이전트 작업에 필요한 능력에 대한 다양한 학습 속도를 나타낸다.\n' +
      '\n' +
      '(Chen et al., 2023)에 의해 영감을 받아, 우리는 모델의 능력을 뚜렷한 구성요소, 즉 명령 후속, 추론, 검색 및 이해로 명시적으로 분리한다. 이러한 맥락에서, 명령어 팔로잉은 포맷 생성에 대응하고, 추론은 각 단계에서의 사고 품질에 대응하고, 검색은 태스크를 실행하기 위한 적절한 함수 이름을 선택하는 것을 수반하며, 이해는 선택된 함수들에 대한 파라미터 입력들을 포괄한다. 그림 3의 각 측면을 기반으로 손실을 시각화함으로써 LLM이 능숙한 에이전트에 필수적인 기능에 대해 다양한 학습 속도를 나타내는 경향이 있음을 식별한다. 자세히 설명하기 위해, 검색과 이해는 추론에 비해 상대적으로 더 다루기 쉬운 과제로 등장하며, 학습 과정에서 가장 간단한 지도가 뒤따른다. 이 관찰은 이러한 모델 능력을 따라 훈련 데이터를 추가로 분해하고 후속적으로 모델의 다양한 학습 속도에 기초하여 이러한 데이터의 균형을 맞추는 강력한 동기 부여 역할을 한다.\n' +
      '\n' +
      '**관찰 3** 기존 접근법은 주로 특수 에이전트 능력에 중점을 두어 모델의 출력에서 환각 효과의 유병률과 중요성을 간과한다._\n' +
      '\n' +
      'AgentTuning(Zeng et al., 2023) 도입 혼합물 열차\n' +
      '\n' +
      '그림 3: 훈련 손실을 모델의 다른 능력인 검색, 지시, 추론, 이해로 분해하여 시각화한다.\n' +
      '\n' +
      '도 2: ReAct 데이터(Toolbench(Qin et al., 2023))와 정상 대화(Flan2022(Longpre et al., 2023))에 대한 훈련 손실 비교.\n' +
      '\n' +
      ' 일반 데이터세트와 에이전트 데이터세트 모두가 튜닝 프로세스 동안 모델에 동시에 제공되는, 튜닝. 이 전략은 실제로 꾸준한 성능 개선으로 이어지지만 환각 문제, 즉 최근 에이전트 연구에서 종종 간과되는 중요한 문제를 해결하는 데 제한적인 영향을 미친다는 것을 관찰한다. 이러한 우려는 도 4에 예시된 바와 같이, 실제-세계 애플리케이션들에서 언어 모델들을 전개할 때 특히 현저해진다. 환각은 두 가지 주요 양상들로 나타난다: (1) 모델이 응답을 호출하도록 요구될 때, 그것은 엄격하게 트레이닝 포맷을 고수하고, 사용자-생성 쿼리를 무시하며, 그리고 (2) 모델은 유도된 질문들과 함께 제시될 때 존재하지 않는 함수들로 트리거되기 쉽다. 이는 에이전트 환각을 효과적으로 평가하고 완화하기 위해 정제 에이전트 조정 메커니즘과 적절한 벤치마크 설정에 더 많은 주의를 기울여야 함을 강조한다.\n' +
      '\n' +
      '## 4 Agent-FLAN\n' +
      '\n' +
      '최근 연구는 다양한 데이터 품질, 모델 크기 및 튜닝 접근 방식으로 다양한 에이전트 작업에 대한 언어 모델의 미세 조정 효과를 탐구하기 시작한다. 파일럿 관찰을 기반으로 효과적인 에이전트 튜닝의 선택을 설계하는 데이터와 방법에 더 깊이 파고들고 언어 에이전트 모델의 세 가지 주요 개선 사항에 대해 논의한다.\n' +
      '\n' +
      '**Experimental Setup** We finetune the language model Llama2-series (Touvron et al., 2023) and use 7B size for ablations for efficiency, unless otherwise otherwise. 우리는 AgentTuning(Zeng et al., 2023)에 의해 확립된 데이터 및 설정에 따라 데이터 세트를 구성한다. 구체적으로, ALF-World(Shridhar et al., 2020), WebShop(Yao et al., 2022), Mind2Web(Deng et al., 2023), 지식 그래프(Liu et al., 2023), 운영 체제(Liu et al., 2023), 데이터베이스(Liu et al., 2023), 및 툴벤치(Qin et al., 2023)와 같은 일반적인 에이전트 및 툴 활용 도메인 모두를 포함한다. 우리의 보류 평가는 복잡한 QA(HotpotQA(Yang et al., 2018)), 웹 브라우징(WebArena(Zhou et al., 2023)), 과학 실험(SciWorld(Wang et al., 2022)), 및 도구 활용(T-Eval(Chen et al., 2023))을 포함하는 복잡한 상호작용 태스크들의 리스트를 포함한다. 에이전트-FLAN에서 채택된 세부 사항과 훈련 중 하이퍼 파라미터는 부록 A에서 찾을 수 있다.\n' +
      '\n' +
      '### 에이전트 튜닝을 프리트레인 도메인에 맞추는 것\n' +
      '\n' +
      'LLM은 먼저 많은 수의 인간 대화를 포함하는 자연어 말뭉치에서 사전 훈련된다. 그러나 에이전트 데이터는 특정 포맷(RecAct, JSON)으로 제시되는 경우가 많아 튜닝 과정에서 유통 외 학습으로 이어진다. 이러한 오정렬은 더 나아가 부적절한 학습 과정을 초래한다. 또한, LLM은 미세 조정 후 이러한 특정 형식에 과도하게 적합되어 지시를 따르는 능력이 저하될 가능성이 더 높다. 이 문제를 완화하기 위해, 우리는 포맷된 데이터를 자연스러운 대화로 변환하는 것을 제안한다. 구체적으로, 우리는\n' +
      '\n' +
      '그림 4: 현재 공개 소스 LLM에 대한 일반적인 에이전트 작업에서 두 가지 전형적인 환각의 그림: (a) 형식 환각 및 (b) 액션 환각.\n' +
      '\n' +
      '고전적인 \'Thought-Action-ActionInput\' 템플릿을 다중 회전 대화 상자로 대체합니다. 그 후, 우리는 몇 개의 유도 진술을 삽입하여 JSON 인수를 추가로 분해한다. 예제는 그림 5에 나와 있습니다. 손실은 \'보조\' 조항에만 적용되기 때문에 형식화된 유도 진술의 도입은 모델에 대한 과적합 문제가 거의 없습니다. 에이전트 코퍼스를 채팅 도메인에 명시적으로 정렬함으로써 엄격한 포맷 프로토콜에 집중하지 않고도 순수한 에이전트 능력에 대한 학습을 완전히 향상시킬 수 있다. 다양한 요청 포맷을 출력할 수 있는 기능을 유지하기 위해 ReAct와 JSON 포맷으로 응답하도록 모델에 요청하는 명령어 쌍을 추가로 구성한다. 우리의 후자의 실험은 또한 다음 데이터의 작은 부분만 만족스러운 결과를 얻기에 충분하다는 것을 보여준다. 표 2에서 훈련 코퍼스를 채팅 도메인에 정렬하여 꾸준한 개선, 즉 T-Eval에서 3.1%, HotpotQA에서 2.5%의 개선을 관찰할 수 있다. 이것은 채팅 포맷에 대한 트레이닝 코퍼스의 정렬의 정확성 및 유효성을 더욱 검증한다.\n' +
      '\n' +
      '### 능력 분해 및 데이터 균형 조정\n' +
      '\n' +
      '선행 연구들은 트레이닝 데이터 소스들의 적절한 혼합이 더 나은 성능으로 이어진다는 것을 보여주었다(Longpre et al., 2023). 이 작업에서는 단순히 각 데이터 세트의 균형 잡힌 구성을 탐구하는 대신 능력 관점에서 훈련 코퍼스의 혼합물을 조사한다. (Chen et al., 2023c)에 의해 영감을 받은, 우리는 추론, 검색, 이해 및 명령 후속을 포함하여 각 태스크가 요구하는 능력을 따라 에이전트 데이터를 명시적으로 분해한다. 섹션 3에서 입증된 바와 같이 LLM은 각 능력에 대해 다양한 학습 속도를 나타내며, 이는 이러한 데이터 소스를 적절하게 구성하는 것이 최종 결과를 최적화하는 데에도 중요하다는 것을 나타낸다. 이 가정을 검증하기 위해 우리는 수행한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c c c|c} \\hline \\hline\n' +
      '**Model** & **Held-In** & \\multicolumn{5}{c}{**Held-Out**} \\\\  & & HotpotQA & SciWorld & WebArena & T-Eval & Agent-H & Overall \\\\ \\hline GPT-3.5 (OpenAI, 2022) & 1.59 & 37.4 & 21.2 & 4.56 & 84.0 & 92.1 & 47.8 \\\\ GPT-4 (OpenAI, 2023) & **2.75** & **52.1** & **36.4** & **6.28** & **86.4** & **94.2** & **55.1** \\\\ \\hline Llama2-7B (Touvron et al., 2023) & 0.19 & 22.6 & 5.9 & 1.2 & 27.4 & 78.7 & 27.2 \\\\ FireAct-7B (Chen et al., 2023a) & - & 26.2 & 6.8 & 0.25 & 9.3 & 40.4 & 16.6 \\\\ AgentLM-7B (Zeng et al., 2023) & 1.96 & 22.3 & 13.7 & 0.74 & 41.4 & 80.6 & 31.7 \\\\ \\hline AgentTuning* (Zeng et al., 2023) & 1.89 & 25.4 & 16.8 & 2.71 & 61.8 & 84.5 & 38.2 \\\\ Agent-FLAN (Ours) & **2.01** & **28.5** & **20.0** & **4.68** & **66.0** & **89.1** & **41.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **Agent-FLAN.** Agent-FLAN의 주요 결과.** Agent-FLAN은 보류 및 보류 작업 모두에서 이전 에이전트 조정 접근 방식을 크게 능가합니다. *는 공정한 비교를 위해 동일한 양의 학습 데이터로 재구현을 나타낸다. FireAct는 AgentInstruct 데이터셋을 학습하지 않기 때문에 Held-In 집합에 대한 성능을 생략한다. **Bold**: API 기반 및 오픈 소스 모델에서 최고입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c} \\hline \\hline Methods & Tokens (M) & T-Eval & HotpotQA \\\\ \\hline ReAct (Baseline) & 19.2 & 61.8 & 25.4 \\\\ Align-Chat (All) & 37.3 & 64.9 & 27.9 \\\\ \\hline All - Reasoning & 32.4 & 63.8 & 27.4 \\\\ All - Retrieval & 36.2 & 65.3 & **29.0** \\\\ All - Understand & 35.4 & 64.6 & 28.1 \\\\ All - Inst. & 28.4 & 65.9 & 27.5 \\\\ \\hline All (Weighted) & **18.1** & **66.3** & 28.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 분해된 능력의 하위 집합은 그 중요성을 측정하기 위해 동등하게 가중된 혼합물에서 절반으로 제외된다.\n' +
      '\n' +
      '그림 5: 원래 에이전트 말뭉치를 자연스러운 대화로 정렬함으로써 에이전트 작업을 명시적으로 다른 기능으로 분해할 수 있어 보다 세분화된 데이터 균형을 이룰 수 있다.\n' +
      '\n' +
      '바닐라 분해 정렬된 에이전트 코퍼스를 사용한 실험으로 각 능력 데이터가 원래 데이터와 동일한 다음 T-Eval 및 HotpotQA에 대한 데이터의 절반과 각 하위 혼합물을 삭제한다.\n' +
      '\n' +
      '표 2에서 볼 수 있듯이 추론과 이해는 가장 유익한 혼합물 중 하나이며, 추론과 이해 데이터의 비중을 50%로 줄이면 최종 성능이 각각 1.1/0.3포인트 감소한다. 검색 및 명령 후속의 양을 줄임으로써, 성능은 거의 영향을 미치지 않고 심지어 개선된다. 이러한 결과는 그림 3의 관찰 결과와도 일치하며, 추론과 이해보다 검색과 지시의 손실이 훨씬 더 빨리 떨어진다. 이러한 현상은 또한 혼합 가중치 탐색 공간을 좁히고 각 능력에 대한 손실 곡선을 기반으로 훈련 토큰을 크게 줄일 수 있도록 영감을 준다.\n' +
      '\n' +
      '### 환각 제거를 위한 음성 샘플 학습\n' +
      '\n' +
      '환각은 "불성실하거나 무의미한 텍스트를 생성하는 것"을 지칭하는 현재 LLM의 중요한 문제이다(지 외, 2023). 에이전트 작업에서도 그림 4와 같은 현상들을 관찰한다. 에이전트 환각을 형식 환각과 행동 환각의 두 가지 주요 범주로 요약한다. 전자는 LLM을 특정 에이전트 시스템에 배치할 때 중요한 역할을 하고, 후자는 일반적인 채팅 어시스턴트 역할을 할 때 중요하다. 따라서 에이전트 환각 문제를 효과적으로 제거하는 방법은 에이전트 LLM을 개발하는 데 필수적인 경로이다.\n' +
      '\n' +
      '그러나 대부분의 선행 작업은 환각 문제를 생략하면서 주로 일반 대리인의 능력에 초점을 맞추고 있다. 에이전트 환각에 대한 LLM을 종합적으로 측정하기 위해 먼저, (1) 형식 수준: 다양한 응답 형식을 가진 요청의 두 가지 측면에서 이러한 문제에 접근하는 _Agent-H_ 벤치마크를 설정하고, 모델이 지침을 따르는지 확인하고, (2) 행동 수준: LLM의 대부분의 에이전트 상황을 다루는 그림 6과 같이 4가지 다른 관점에서 질문을 선별한다.\n' +
      '\n' +
      '구체적으로 glaive-function-calling-v2(GlaiveAI, 2023)를 기본 데이터 세트로 선택한다. 응답에 도구 호출이 포함되어 있는지 명시적으로 확인하여 도메인 외 검증을 위해 1845개의 샘플을 선별한다. 우리는 에이전트 역할을 할 때 환각 문제에 초점을 맞추기 때문에 평가 프로토콜은 모델의 출력이 원시 응답 또는 지정된 기능 호출인 경우에만 측정한다. 구체적으로, (1) ReAct-format hallucination (_e.g._, \'Thought:\', \'Action:\'), (2) general-format hallucination (_e.g._, \'I will use\', \'I need to call\')의 두 가지 구체적인 포맷 체크를 정의한다. 응답에 위의 키워드가 포함되어 있다면, 근거 진리가 원시 응답 유형일 때 하나의 환각 실패로 볼 것이다. 이를 바탕으로 본 논문에서는 두 가지 수치 메트릭인 \\(\\text{H}_{\\text{ReAct}}\\)과 \\(\\text{H}_{\\text{General}}\\)을 각각의 형식 환각의 수/지상 진실의 원시 응답의 수로 정의한다. 최종 종합 점수\\(\\text{H}_{\\text{Score}}\\)는 상기 두 메트릭의 역평균이다:\n' +
      '\n' +
      '\\[\\text{H}_{\\text{Score}}=0.5*((1-\\text{H}_{\\text{ReAct}})+(1-\\text{H}_{\\text{ReAct}})) \\tag{1}\\\n' +
      '\n' +
      '표 3은 에이전트 능력과 환각 문제에 대한 포괄적인 실증을 제공하는 _Agent-H_에 대한 실험 결과와 T-Eval에 대한 점수를 보고한다. 이상적으로, 일반 언어 모델은 두 벤치마크에서 높은 점수를 얻어야 한다. 표에서 우리는 Llama2-7B가 _Agent-H_와 T-Eval 모두에서 낮은 점수를 얻는 것을 볼 수 있다. 이는 사전 훈련 코퍼스에 에이전트 데이터가 부족하기 때문일 수 있으며, 이는 에이전트 튜닝의 필요성을 더욱 입증한다. 우리는 또한 Llama2-7B로부터 모델을 미세 조정하기 위해 AgentTuning(Zeng et al., 2023)의 구현에 따른다. T-Eval 점수가 크게 향상되었음에도 불구하고, 환각 문제는 _Agent-H_에 의해 상당히 심각하여 현재 에이전트 튜닝 접근법의 내부 결함을 정확히 지적한다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 먼저 그림 6에 따른 현재 에이전트 코퍼스를 살펴본다. 대부분의 경우 학습 데이터는 도구(a)가 없는 정상적인 대화와 제공된 도구(d)로 에이전트 작업만 다루고 나머지(b,c)는 생략한다는 것을 쉽게 찾을 수 있다. 모델은 훈련 중에 이러한 음성 샘플을 본 적이 없기 때문에 이러한 요청으로 거의 일반화할 수 없으며 예상치 못한 응답으로 이어진다. 이를 위해 위에서 언급한 다양한 조건을 포괄하는 다양한 네거티브 트레이닝 샘플을 꼼꼼하게 큐레이션하여 네거티브 샘플 학습을 소개한다. 구체적으로, 두 가지 유형의 부정적인 샘플을 삽입한다: (1) 도구가 제공되지 않음, (2) 도구가 제공된 사용자 쿼리 요청, 일반 대화를 위한 사용자 쿼리 요청. 명시적 감독을 통해 모델을 _how_뿐만 아니라 _when_가 에이전트 역할을 하도록 가르친다. 표 3에서 부정적 표집 학습 전략은 환각 문제를 유의하게 완화시키는 반면, T-Eval에서는 높은 성과를 유지하고 있다.\n' +
      '\n' +
      '## 5 Analysis\n' +
      '\n' +
      '이 섹션에서는 분석을 위해 심층 실험을 수행한다.\n' +
      '\n' +
      '그림 6: 에이전트 작업에서 일반적으로 존재하는 4가지 다른 상황에 대한 시연, 사용자 질의 및 시스템 프롬프트를 통해 분할한다.\n' +
      '\n' +
      '에이전트 튜닝을 위한 스케일링 법칙\n' +
      '\n' +
      '언어 모델에 대한 스케일링 법칙은 상당히 중요하며 추가 개발에 귀중한 통찰력을 제공한다(Longpre et al., 2023; Chung et al., 2022). 이 절에서는 일반적인 HotpotQA 작업에 대한 데이터와 모델 척도 모두에서 에이전트 튜닝에 대한 현상을 탐구한다.\n' +
      '\n' +
      '에이전트 튜닝을 위한 데이터 스케일링 법칙 5.1.1\n' +
      '\n' +
      '최근 연구(Chung et al., 2022)는 언어 모델이 일반 능력에서 방대하고 다양한 훈련 코퍼스의 이점을 얻을 수 있음을 입증했다. 우리는 훈련 데이터의 양이 에이전트의 능력에 어떤 영향을 미치는지 조사한다. 에이전트-FLAN 데이터를 25%, 50%, 75%, 100%로 고르게 나누어 그림 7에 결과를 보고한다. 25%의 훈련 샘플만으로도 에이전트 능력이 가장 크게 향상됨을 알 수 있다. 이것은 바닐라 라마-2 모델이 약한 에이전트 능력을 나타내고 특정 훈련을 필요로 하며 에이전트 코퍼스의 작은 부분만이 에이전트 작업에 필요한 대부분의 능력을 이끌어낼 수 있음을 추가로 검증한다. 데이터 양(50%, 75%)을 더 증가시킬 때 개선은 지속되지만 속도가 느리기 때문에 단순히 에이전트의 훈련 코퍼스의 규모를 확대하는 것은 모델 기능에 크게 기여하지 않는다는 것을 나타낸다. 따라서, 학습 말뭉치의 다양성을 풍부하게 하거나 품질을 향상시키는 것이 더 나은 언어 에이전트에 필요한 경로일 수 있다.\n' +
      '\n' +
      '에이전트 튜닝을 위한 모델 스케일링 법칙 5.1.2\n' +
      '\n' +
      '이전 연구(Longpre et al., 2023)가 언어 모델의 제로/퓨 샷 능력이 더 큰 모델 척도로 실질적으로 향상된다는 것을 증명함에 따라, 이 규칙이 에이전트 도메인에서도 적용되는지 탐구한다. Llama2의 7B, 13B 및 70B 크기의 모델에 대한 접근 방식을 평가함으로써 최종 결과는 그림 8에 나와 있다. 모델 규모가 증가함에 따라 포화 없이 성능이 지속적으로 향상되어 더 큰 매개변수가 더 나은 성능을 보장한다는 것을 확인할 수 있다. 다양한 모델 스케일에 걸친 향상을 자세히 살펴보면, 특정 에이전트 튜닝이 모델 스케일 업에 따라 바닐라 ReAct 튜닝에 비해 꾸준한 개선을 가져온다는 것을 알 수 있다. 우리는 더 큰 모델이 추론과 검색과 같은 에이전트 작업에 필요한 기본 능력을 이미 가지고 있는 이유를 추론한다. 따라서 적절한 방법으로 일정량의 에이전트 튜닝 코퍼스로 에이전트 능력을 이끌어내는 것이 더 중요하다.\n' +
      '\n' +
      '### 일반 능력 v. 에이전트 능력\n' +
      '\n' +
      '특정 튜닝이 에이전트로 작용하는 모델의 능력을 향상시킨다는 것을 보았듯이, 일반적인 능력과 에이전트 능력의 관계는 여전히 불분명하다. 에이전트튜닝은 에이전트 훈련에 대한 일반적인 데이터의 필요성을 확인했지만 에이전트튜닝은 모델의 일반적인 능력을 더욱 촉진합니까? 본 논문에서는 MMLU(언어적 지식), GSM8K(수학적 능력), HumanEval(코드 능력)의 세 가지 일반적인 능력에 대한 모델을 평가한다. 그 결과는 표 4와 같다. 표로부터 에이전트 훈련 코퍼스를 도입하면 에이전트 작업에 대한 능력이 향상될 뿐만 아니라 일반적인 능력에 추가적인 이점을 가져다 준다는 것을 알 수 있다. 우리는 에이전트 코퍼스가 추론과 같은 기본 기능과 다른 도메인에도 적합한 명령어 팔로우를 포함하는 이유를 추측한다. 이것은 또한 에이전트 데이터를 현재의 LLM 트레이닝 코퍼스에 적절하게 통합하는 것이 더 나은 성능으로 이어질 수 있음을 나타낸다.\n' +
      '\n' +
      '그림 8: Llama2-7B에서 Llama2-70B까지의 훈련 모델의 파라미터에 대한 성능 스케일링 법칙.\n' +
      '\n' +
      '그림 7: 훈련 데이터 양에 대한 성능 스케일링 법칙, 0%에서 100% 범위.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{T-Eval} & \\multicolumn{3}{c}{Agent-H} \\\\  & & H\\({}_{\\text{ReAct}}\\)\\(\\downarrow\\) & H\\({}_{\\text{General}}\\)\\(\\downarrow\\) & H\\({}_{\\text{Score}}\\)\\(\\uparrow\\) \\\\ \\hline Llama2-7B & 27.4 & 21.7 & 21.0 & 78.7 \\\\ AgentTuning & 61.8 & 18.1 & 14.0 & 83.9 \\\\ \\hline Agent-FLAN & 66.0 & **9.9** & **11.9** & **89.1** \\\\ w/o NS & **66.3** & 15.6 & 13.5 & 84.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Llama2-7B를 갖는 Agent-H에 대한 실험 결과. H\\({}_{\\text{score}}\\)는 에이전트-H 벤치마크의 전체 점수이다. "NS"는 음성 훈련 샘플을 나타낸다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 LLM의 에이전트 튜닝을 위한 데이터 및 방법의 설계 선택사항을 조사한다. 먼저 세 가지 중요한 관찰을 수행하여 오픈 소스 LLM과 API 기반 모델 간의 격차를 줄이는 데 있어 현재 병목 현상을 정확하게 지적한다. 이를 바탕으로 에이전트 태스크에 대한 언어 모델을 효과적으로 미세 조정하기 위한 에이전트-FLAN을 제시한다. 기존 훈련 코퍼스의 세심한 분해와 재설계를 통해 에이전트-FLAN은 Llama2-7B에 광범위한 에이전트 작업에서 이전 작업을 크게 능가할 수 있는 권한을 부여한다.\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      '본 논문에서는 에이전트 훈련 코퍼스 구축에 초점을 맞춘다. 우리의 최선의 노력에도 불구하고, 이 논문은 여전히 몇 가지 남아 있는 한계를 가질 수 있다. 1) 훈련 및 검증 데이터세트는 단지 에이전트 태스크의 일부를 포함한다. 이 외에도 많은 다른 대화형 시나리오가 있습니다. 향후 보다 광범위한 벤치마크에 Agent-FLAN을 적용하는 것에 대한 추가 연구를 진행할 것이다. 2) 학습 데이터의 품질을 유지하기 위해 전체 데이터 세트의 10%인 툴벤치에서 약 20,000개의 유효한 샘플만 선택한다. 모델의 성능을 더욱 높이기 위해 충분히 활용하는 것이 좋을 것이다. 우리는 미래의 일을 위해 그것을 남겨둔다.\n' +
      '\n' +
      '##8 윤리적 고려사항\n' +
      '\n' +
      '실험을 위해 공개적으로 사용할 수 있는 참조 문서/API를 사용하여 개인 또는 그룹에 대한 가능한 피해를 효과적으로 방지했다. LLM에 의해 생성된 데이터는 프라이버시와 기밀성을 확보하기 위해 인간에 의해 신중하게 선택되고 처리되었다. 개인 식별 정보는 포함되지 않았으며 모든 데이터는 분석을 수행하기 전에 익명으로 만들었다. 게다가, 우리는 글쓰기를 다듬기 위해 ChatGPT와 문법을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l l} \\hline \\hline \\multirow{2}{*}{Data} & \\multicolumn{3}{c}{_Llama2-7B_} \\\\  & MMLU & GSM8K & HumanEval \\\\ \\hline general data & **50.0** & 21.9 & 15.1 \\\\ Agent-FLAN & 49.7 -0.3 & **22.1** +0.2 & **15.5** +0.4 \\\\ \\hline \\multirow{2}{*}{Data} & \\multicolumn{3}{c}{_Llama2-13B_} \\\\  & MMLU & GSM8K & HumanEval \\\\ \\hline general data & 54.7 & 34.8 & 15.2 \\\\ Agent-FLAN & **55.8** +1.1 & **35.2** +0.4 & **15.8** +0.6 \\\\ \\hline \\multirow{2}{*}{Data} & \\multicolumn{3}{c}{_Llama2-70B_} \\\\  & MMLU & GSM8K & HumanEval \\\\ \\hline general data & 68.0 & 64.5 & 32.1 \\\\ Agent-FLAN & **68.5** +0.5 & **64.6** +0.1 & **32.9** +0.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: AgentFLAN 말뭉치가 Llama2 시리즈의 일반 능력에 미치는 효과.\n' +
      '\n' +
      '그림 9: **AgentTuning과 Agent-FLAN 간의 Toolbench 및 Agent-H 데이터셋에 대한 비교 연구 Llama2-7B**. (a) ToolBench: 능력 분해와 \'이해\'에 대한 더 많은 초점 조정 덕분에, Agent-FLAN은 긴 도구 정보 콘텐츠가 주어진 특정 API 정보를 따라잡을 수 있는 반면, AgentTuning은 환각에 실패하였다. (b) Agent-H: AgentTuning 모델은 에이전트-FLAN이 직접 선호하는 응답을 제공하는 동안 의미 없는 도구 사용을 제시한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Chen et al. (2023) Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. FireAct: 언어 에이전트를 미세 조정합니다. _ arXiv preprint arXiv:2310.05915_, 2023a.\n' +
      '* Chen et al. (2023) Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. AgentVerse: Facilitating multi-agent collaboration and explore emergent behavior in agents. _ arXiv preprint arXiv:2308.10848_, 2023b.\n' +
      '* Chen et al. (2023c) Chen, Z., Du, W., Zhang, W., Liu, K., Liu, J., Zheng, M., Zhuo, J., Zhang, S., Lin, D., Chen, K., et al. T-eval: 도구 활용 능력을 단계별로 평가한다. _ arXiv preprint arXiv:2312.14033_, 2023c.\n' +
      '* Chiang et al. (2023) Chiang, W. -L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zu, H., Zang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: 90%* chatgpt 품질로 gpt-4를 인상하는 오픈 소스 챗봇, 3월 2023. URL[https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/].\n' +
      '* Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. _ ArXiv:2210.11416_, 2022.\n' +
      '*Deng et al. (2023) Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: 웹에 대한 일반 에이전트를 향합니다. _ arXiv preprint arXiv:2306.06070_, 2023.\n' +
      '* Dettmers et al. (2023) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: 양자화된 l lms의 효율적인 미세조정 arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* Fu et al. (2023) Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. 소규모 언어 모델을 다단계 추론으로 전문화합니다. _ arXiv preprint arXiv:2301.12726_, 2023.\n' +
      '* GliaveAI(2023) GliaveAI. gliave-function-calling-v2, 2023. URL[https://huggingface.co/datasets/gliaveai/gliave-function-calling-v2](https://huggingface.co/datasets/gliaveai/gliave-function-calling-v2].\n' +
      '* Gou et al. (2023) Gou, Z., Shao, Z., Gong, Y., Yang, Y., Huang, M., Duan, N., Chen, W., et al. ToRA: A tool-integrated reasoning agent for mathematical problem solving. _ arXiv preprint arXiv:2309.17452_, 2023.\n' +
      '* Gunasekar et al. (2023) Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. 교과서가 필요한 전부이다. _ arXiv preprint arXiv:2306.11644_, 2023.\n' +
      '* Hong et al. (2023) Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. MetaGPT: Meta programming for multi-agent collaborative framework. _ arXiv preprint arXiv:2308.00352_, 2023.\n' +
      '* Honovich et al. (2022) Honovich, O., Scialom, T., Levy, O., and Schick, T. 부자연스러운 명령: (거의) 인간의 노동력이 없는 언어 모델을 조정합니다. _ ARXiv 프리프린트 arXiv:2212.09689_, 2022.\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: 대형 언어 모델의 낮은 랭크 적응. _ arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* Huang et al. (2023) Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. In _Conference on Robot Learning_, pp. 1769-1782. PMLR, 2023.\n' +
      '* Ivison et al. (2022) Ivison, H., Bhagia, A., Wang, Y., Hajishirzi, H., and Peters, M. 힌트: 효율적인 제로샷 일반화를 위한 하이퍼네트워크 명령어 튜닝; _ ARXiv 프리프린트 arXiv:2212.10315_, 2022.\n' +
      '* Ji et al. (2023) Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. _ ACM Computing Surveys_, 55(12):1-38, 2023.\n' +
      '* Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mistral of experts. _ arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* Kim et al. (2023) Kim, G., Baldi, P., and McAleer, S. 언어 모델은 2023년에 컴퓨터 작업을 해결할 수 있다.\n' +
      '* Li 등(2023) Li, H., Guo, D., Fan, W., Xu, M., and Song, Y. 채팅에 대한 다단계 불법 침입 개인 정보 보호 공격 arXiv preprint arXiv:2304.05197_, 2023.\n' +
      '* Liang et al. (2023) Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Tu, Z., and Shi, S. 2023년 다중 에이전트 토론을 통해 대규모 언어 모델에서 다양한 사고를 장려한다.\n' +
      '* Liu et al. (2021) Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., and Hajishirzi, H. Generated knowledge prompting for commonsense reasoning. _ arXiv preprint arXiv:2110.08387_, 2021.\n' +
      '* Liu et al. (2023a) Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. AgentBench: Evaluation lms as agent. _ arXiv preprint arXiv:2308.03688_, 2023a.\n' +
      '*Liu 등 (2023b) Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy, R., Feng, Y., Chen, Z., Niebles, J. C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong, C., and Savarese, S. 볼라: 벤치마킹 및 조정 llm-증강 자율 에이전트, 2023b.\n' +
      '\n' +
      '* Longpre et al. (2023) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The final collection: Designing data and methods for effective instruction tuning. _ arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '* Lv et al. (2023) Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X. 자원이 제한된 대규모 언어 모델에 대한 전체 매개 변수 미세 조정 _ arXiv preprint arXiv:2306.09782_, 2023.\n' +
      '* Mao et al. (2023) Mao, S., Zhang, N., Wang, X., Wang, M., Yao, Y., Jiang, Y., Xie, P., Huang, F., and Chen, H. Editing personality for llms. _ arXiv preprint arXiv:2310.02168_, 2023.\n' +
      '* Mialon et al. (2023) Mialon, G., Dessi, R., Lomeli, M., Nalmparantis, C., Pasunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: survey. _ arXiv preprint arXiv:2302.07842_, 2023.\n' +
      '* OpenAI(2022) OpenAI. Openai: Introducing chatgpt, 2022. URL[https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).\n' +
      '* OpenAI(2023) OpenAI. Gpt-4 기술 보고서, 2023\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.\n' +
      '* Patil et al. (2023) Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. _ arXiv preprint arXiv:2305.15334_, 2023.\n' +
      '* Qiao et al. (2023) Qiao, S., Gui, H., Chen, H., and Zhang, N. 실행 피드백으로 언어 모델을 더 나은 도구로 만들 수 있습니다. _ arXiv preprint arXiv:2305.13068_, 2023.\n' +
      '* Qin et al. (2023) Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. ToolLLM: 마스터 16000+ 실세계 apis를 위해 대형 언어 모델들을 용이하게 한다. _ arXiv preprint arXiv:2307.16789_, 2023.\n' +
      '* Sanh et al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalizedization. _ arXiv preprint arXiv:2110.08207_, 2021.\n' +
      '* Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. 반사: 언어 강화 학습을 하는 언어 에이전트. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Shridhar et al. (2020) Shridhar, M., Yuan, X., Cote, M. - A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: 대화형 학습을 위한 텍스트 및 구체화된 환경의 정렬; _ arXiv preprint arXiv:2010.03768_, 2020.\n' +
      '* Song et al. (2023) Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W. - L., and Su, Y. Llm-플래너: 대규모 언어 모델을 가진 구체화된 에이전트를 위한 몇 개의 숏 그라운드 계획. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2998-3009, 2023.\n' +
      '*Sumers et al. (2023) Sumers, T. R., Yao, S., Narasimhan, K., and Griffiths, T. L. Cognitive architectureures for language agent. _ arXiv preprint arXiv:2309.02427_, 2023.\n' +
      '* Talebirad & Nadiri (2023) Talebirad, Y. And Nadiri, A. Multi-Agent collaboration: Harnessing the power of intelligent llm agents. _ arXiv preprint arXiv:2306.03314_, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wang et al. (2023a) Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey for large language model based autonomous agents. _ arXiv preprint arXiv:2308.11432_, 2023a.\n' +
      '* Wang et al. (2022a) Wang, R., Jansen, P., Cote, M. - A, 그리고 P. 사이언스월드 암마나브로루 - 네 에이전트가 5학년보다 똑똑하니? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 11279-11298, 2022a.\n' +
      '* Wang et al. (2023b) Wang, X., Zhou, W., Zu, C., Xia, H., Chen, T., Zhang, Y., Zheng, R., Ye, J., Zhang, Q., Gui, T., et al. Instructuie: Multi-task instruction tuning for unified information extraction. _ arXiv preprint arXiv:2304.08085_, 2023b.\n' +
      '* Wang et al. (2022b) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. self-instruct: Aligning language model with self generated instructions. _ arXiv preprint arXiv:2212.10560_, 2022b.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '* Wu et al. (2023) Wu, Q., Banal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. _ arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* Xi et al.(2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. _ arXiv preprint arXiv:2309.07864_, 2023.\n' +
      '\n' +
      '* Xu et al. (2023) Xu, B., Peng, Z., Lei, B., Mukherjee, S., Liu, Y., and Xu, D. Rewoo: Decoupling reasoning from observations for efficient augmented language models. _ arXiv preprint arXiv:2305.18323_, 2023.\n' +
      '* Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 2369-2380, 2018.\n' +
      '* Yao et al. (2022a) Yao, S., Chen, H., Yang, J., and Narasimhan, K. 웹: 확장 가능한 현실 세계 웹과 접지된 언어 에이전트와의 상호 작용에 대한_ 신경 정보 처리 시스템_, 35:20744-20757, 2022a에서의 발전.\n' +
      '* Yao et al. (2022b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. 반응: 추론과 언어 모델에서의 행동을 동기화하는 것. _The Eleventh International Conference on Learning Representations_, 2022b.\n' +
      '* Yuan et al. (2023) Yuan, Y., Jiao, W., Wang, W., Huang, J.-t., He, P., Shi, S., and Tu, Z. GPT-4는 너무 똑똑해서 안전하지 않다: 암호를 통해 lms와 은밀한 채팅을 한다. _ arXiv preprint arXiv:2308.06463_, 2023.\n' +
      '* Zeng et al. (2023) Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J. AgentTuning: Enabling generalized agent abilities for lms. _ arXiv preprint arXiv:2310.12823_, 2023.\n' +
      '* Zhang et al. (2023a) Zhang, J., Xu, X., and Deng, S. llm 에이전트에 대한 협업 메커니즘 탐색: 사회 심리학적 관점, 2023a.\n' +
      '* Zhang et al. (2023b) Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., and Wang, G. Instruction tuning for large language models: A survey, 2023b.\n' +
      '* Zhou et al. (2023) Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: 건축 자율 에이전트를 위한 현실적인 웹 환경. _ arXiv preprint arXiv:2307.13854_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '도 10: 구성된 네거티브 샘플들(I)의 일례.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>