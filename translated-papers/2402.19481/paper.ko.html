<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '텍스트 설명만으로 사실적 이미지를 생성합니다. 이러한 기술 혁명은 이미지 및 비디오에 대한 수많은 합성 및 편집 애플리케이션을 잠금 해제하여 응답성에 대한 새로운 요구를 제기한다:_인터랙티브_모델 출력을 안내하고 정제함으로써 사용자는 보다 개인화되고 정확한 결과를 얻을 수 있다. 그럼에도 불구하고, 큰 계산으로 이어지는 고해상도라는 중요한 문제가 남아 있다. 예를 들어, 원래의 안정한 확산[55]은 \\(512\\times 512\\)개의 이미지를 생성하는 것으로 제한된다. 이후 SDXL[47]은 1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024×1024 최근에, 소라는 \\(1080\\times 1920\\) 해상도에서 비디오 생성을 가능하게 함으로써 경계를 더욱 강화한다. 이러한 발전에도 불구하고 고해상도 이미지 생성의 증가된 지연은 실시간 애플리케이션에 엄청난 장벽을 제공한다.\n' +
      '\n' +
      '확산 모델 추론을 가속화하기 위한 최근의 노력은 주로 샘플링 단계를 줄이고 신경망 추론을 최적화하는 두 가지 접근법[21, 33, 34, 37, 58, 62, 71, 74]에 초점을 맞추었다[24, 26, 27]. 컴퓨팅 리소스가 빠르게 성장함에 따라 추론 속도를 높이기 위해 여러 GPU를 활용하는 것이 매력적이다. 예를 들어, 자연어 처리(NLP)에서, 대형 언어 모델들은 GPU들에 걸쳐 텐서 병렬성을 성공적으로 활용함으로써, 대기 시간을 상당히 감소시킨다. 그러나 확산 모델의 경우 일반적으로 배치 추론에만 여러 GPU가 사용됩니다. 단일 이미지를 생성할 때, 전형적으로 하나의 GPU만이 관여한다(도 2의 (a)). 통신 비용이 분산 계산의 절감보다 크기 때문에 텐서 병렬과 같은 기술은 확산 모델에 덜 적합하다. 따라서, 다수의 GPU가 이용 가능한 경우에도, 단일-이미지 생성을 더욱 가속화하기 위해 효과적으로 이용될 수 없다. 이는 확산 모델로 단일 이미지 생성을 가속화하기 위해 여러 GPU를 활용할 수 있는 방법의 개발에 동기를 부여한다.\n' +
      '\n' +
      '순진한 접근법은 그림 2(b)와 같이 이미지를 여러 패치로 나누어 각 패치를 생성을 위해 다른 장치에 할당하는 것이다. 이 방법은 디바이스들에 걸쳐 독립적이고 병렬적인 동작들을 허용한다. 그러나 개별 패치 간의 상호 작용이 없기 때문에 각 패치의 경계에서 명확하게 보이는 이음매가 발생한다. 그러나 이 문제를 해결하기 위해 패치 간의 상호 작용을 도입하면 다시 과도한 동기화 비용이 발생하여 병렬 처리의 이점을 상쇄한다.\n' +
      '\n' +
      '본 논문에서는 영상 품질을 해치지 않으면서 단일 샘플 생성의 지연 시간을 줄이기 위해 여러 장치에 걸쳐 확산 모델을 병렬로 실행할 수 있는 방법인 _DistriFusion_을 제시한다. 그림 2(c)에 도시된 바와 같이, 우리의 접근법은 또한 이미지를 각각 다른 장치에 할당된 다중 패치로 분할하는 패치 병렬성을 기반으로 한다. 우리의 주요 관찰은 확산 모델의 인접한 잡음 제거 단계에 걸친 입력이 유사하다는 것이다. 따라서, 우리는 첫 번째 단계만을 위해 동기 통신을 채택한다. 후속 단계를 위해 _previous step_에서 사전 계산된 활성화를 재사용하여 _current step_에 대한 전역 컨텍스트 및 패치 상호 작용을 제공한다. 또한 추론 프레임워크를 공동 설계하여 알고리즘을 구현한다. 특히, 본 논문에서 제안하는 프레임워크는 비동기 통신을 통한 연산에서 통신 오버헤드를 효과적으로 은닉한다. 또한 할당된 영역에서만 컨볼루션 및 주의 계층을 희소하게 실행하므로 장치당 계산을 비례적으로 줄일 수 있다. 우리의 방법은 데이터, 텐서 또는 파이프라인 병렬성과 구별되는 새로운 병렬화 기회(_displaced patch parallelism_)를 도입한다.\n' +
      '\n' +
      '디스트리퓨전은 기성품 사전 훈련된 확산 모델만 필요하며 대부분의 소수의 샘플러에도 적용할 수 있다. 우리는 그것을 COCO 캡션의 하위 집합[5]에서 벤치마킹한다. 시각적 충실도의 손실 없이, 그것은 사용된 장치의 수에 비례하여 계산*을 감소시키면서 원래의 안정 확산 XL(SDXL) [47]의 성능을 반영한다. 또한, 단일 영상을 생성하기 위한 SDXL U-Net의 지연시간을 2, 4, 8개의 A100 GPU를 사용하여 최대 1.8\\(\\times\\), 3.4\\(\\times\\), 6.1\\(\\times\\)까지 줄였다. 분류기 없는 안내[12]를 위해 배치 분할을 결합했을 때, 각각 4개의 A100 GPU와 8개의 A100 GPU를 사용하여 총 3.6\\(\\times\\) 및 6.6\\(\\times\\)의 속도 향상을 달성했다. 우리 방법의 몇 가지 예는 그림 1을 참조하십시오.\n' +
      '\n' +
      '도 2: (a) 단일 장치에서 실행되는 원본 확산 모델. (b) 2개의 GPU에 걸쳐 2개의 패치로 이미지를 약하게 분할하는 것은 패치에 걸친 상호 작용의 부재로 인해 경계에서 명백한 이음매가 있다. (c) 디스트리퓨전은 첫 번째 단계에서 패치 상호작용을 위해 동기 통신을 사용한다. 이후 비동기 통신을 통해 이전 단계의 활성화를 재사용한다. 이러한 방식으로, 통신 오버헤드는 계산 파이프라인에 숨겨질 수 있다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**확산 모델.**확산 모델은 콘텐츠 생성의 경관을 상당히 변형시켰다[2, 13, 42, 47]. 그 핵심에서, 이 모델들은 반복적인 잡음 제거 과정을 통해 콘텐츠를 합성한다. 이러한 반복적 접근은 콘텐츠 생성을 위한 전례 없는 능력을 산출하지만, 실질적으로 더 많은 계산 자원을 필요로 하고 더 느린 생성 속도를 초래한다. 이 문제는 고해상도 [9, 14] 또는 \\(360^{\\circ}\\) 이미지 [76]과 같은 고차원 데이터의 합성으로 심화된다. 연구자들은 확산 모델을 가속화하기 위해 다양한 관점을 조사했다. 첫 번째 라인은 더 효율적인 노이즈 제거 프로세스를 설계하는 데 있습니다. Rombach _et al_. [55] 및 Vahdat _et al_. [67] 고해상도 이미지를 저해상도 잠재 표현으로 압축하고 잠재 공간에서의 확산 모델을 학습하도록 제안한다. 또 다른 라인은 효율적인 훈련 없는 샘플링 알고리즘을 설계하여 샘플링을 개선하는 데 있다. 이 선을 따라 큰 범주의 작업은 확산 모델과 미분 방정식[63] 간의 연결에 기초하고 잘 확립된 지수 적분기[33, 74, 75]를 활용하여 수치 정확도를 유지하면서 샘플링 단계를 줄인다. 세 번째 전략은 미리 훈련된 확산 모델에서 더 빠른 생성 모델을 증류하는 것을 포함한다. 이 지역에서 상당한 진전이 있었음에도 불구하고 이러한 신속 발전기와 확산 모델 사이에 품질 격차가 지속된다[20, 37, 58]. 상기 방식들 외에도, 몇몇 연구들은 확산 모델들에 대한 신경 추론을 최적화하는 방법을 조사한다[24, 26, 27]. 본 연구에서는 다중 디바이스에서 신경망에 병렬성을 활용하여 확산을 가속화하기 위한 새로운 패러다임을 탐색한다.\n' +
      '\n' +
      '**병렬성.** 기존 작업은 데이터, 파이프라인[15, 28, 39], 텐서[17, 40, 72, 73, 79], 및 제로-중복 병렬성[48, 51, 52, 78]을 포함하는 대형 언어 모델들(LLM)의 트레이닝 및 추론을 가속화하기 위한 다양한 병렬성 전략들을 탐색하였다. 특히 텐서 병렬성은 LLM 추론이 메모리 바인딩되는 경향이 있기 때문에 LLM을 가속화하기 위해 널리 채택되었다[29]. 이러한 시나리오에서 텐서 병렬에 의해 도입된 통신 오버헤드는 증가된 메모리 대역폭에 의해 야기되는 실질적인 레이턴시 이점에 비해 상대적으로 작다. 그러나 컴퓨팅 바인딩인 확산 모델에서는 상황이 다릅니다. 확산 모델의 경우 텐서 병렬성에 의한 통신 오버헤드가 중요한 요소가 되어 실제 계산 시간을 무색하게 한다. 결과적으로, 지금까지 확산 모델 서빙을 위해 데이터 병렬성만이 사용되었으며, 이는 대기 시간 개선을 제공하지 않는다. 유일한 예외는 Picard 반복을 사용하여 여러 단계를 병렬로 실행하는 ParaDiGMS[60]이다. 그러나 이 샘플러는 많은 계산량을 낭비하는 경향이 있으며 생성된 결과는 원래 확산 모델과 상당한 편차를 나타낸다. 제안된 방법은 패치 병렬성을 기반으로 하여, 입력을 작은 패치로 분할함으로써 여러 장치에 걸쳐 계산을 분배한다. 이러한 방식은 텐서 병렬에 비해 독립성이 우수하고 통신 요구량이 줄어든다. 또한 데이터 상호 작용을 위해 올리듀스보다 올가더 사용을 선호하여 오버헤드를 크게 낮춥니다(전체 비교는 섹션 5.3 참조). 병렬 컴퓨팅에서 비동기 통신의 성공으로부터 영감을 끌어내는 [68], 우리는 이전 단계의 특징을 현재 단계의 컨텍스트로 재사용하여 통신과 계산을 겹치게 하는데, 이는 _displaced patch parallelism_이다. 이는 텐서 병렬과 같은 전통적인 기법의 많은 통신 비용을 피하면서 확산 모델의 순차적 특성에 맞춘 첫 번째 병렬 전략을 나타낸다.\n' +
      '\n' +
      '**희소 계산.** 희소 계산은 가중치[10, 16, 22, 32], 입력[54, 65, 66] 및 활성화[7, 18, 24, 25, 43, 53, 59]를 포함한 다양한 도메인에서 광범위하게 연구되었다. 활성화 도메인에서는 온-하드웨어 속도 향상을 촉진하기 위해 여러 연구에서 구조화된 희소성을 사용할 것을 제안한다. SBNet[53]은 3D 객체 검출을 가속화하기 위한 활성화를 희박화하기 위해 공간 마스크를 사용한다. 이 마스크는 사전 문제 지식 또는 보조 네트워크로부터 도출될 수 있다. 이미지 생성의 맥락에서 SIGE[24]는 사용자 편집의 고도로 구조화된 희소성을 활용하여 편집된 영역에서 선택적으로 계산을 수행하여 GAN[8] 및 확산 모델을 가속화한다. MCUNetV2[30]은 이미지 분류 및 검출을 위한 메모리 사용량을 줄이기 위해 패치 기반 추론을 채택한다. 또한 작업에서 입력을 서로 다른 장치에 의해 처리되는 패치로 분할한다. 그러나, 본 논문에서는 이미지 생성을 위한 병렬성(parallelism)에 의해 지연시간을 줄이는 것에 초점을 맞추고 있다. 각 장치는 장치당 계산을 줄이기 위해 할당된 영역을 단독으로 처리합니다.\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      '고화질 영상을 생성하기 위해 확산 모델은 잡음 예측 신경망 모델(_e.g_., U-Net[56]) \\(\\epsilon_{\\theta}\\)을 학습시키는 경우가 많다. 순수 가우시안 잡음 \\(\\mathbf{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)에서 시작하여, 최종 깨끗한 이미지 \\(\\mathbf{x}_{0}\\)을 얻기 위해 수십에서 수백 단계의 반복적인 잡음 제거 단계를 포함하며, 여기서 \\(T\\)은 총 단계 수이다. 구체적으로, 시간 단계에서의 잡음 영상\\(\\mathbf{x}_{t}\\)을 고려할 때, 모델\\(\\epsilon_{\\theta}\\)은 입력으로서 \\(\\mathbf{x}_{t}\\), \\(t\\) 및 추가 조건\\(c\\)(_e.g_, 텍스트)을 취하여 \\(\\mathbf{x}_{t}\\) 내에서 해당 잡음\\(\\epsilon_{t}\\)을 예측한다. 각각의 잡음 제거 단계에서, \\(\\mathbf{x}_{t-1}\\)는 다음의 수학식으로부터 유도될 수 있다:\n' +
      '\n' +
      '\\[\\mathbf{x}_{t-1}=\\text{Update}(\\mathbf{x}_{t},t,\\epsilon_{t}),\\quad\\epsilon_{t}=\\epsilon_{\\theta}(\\mathbf{x}_{t},t,c). \\tag{1}\\t.\n' +
      '\n' +
      '여기서, \'Update\'는 대표적으로 요소별 덧셈과 곱셈을 포함하는 샘플러별 함수를 의미한다. 따라서 이 과정에서 대기시간의 주된 원천은 모델 \\(\\epsilon_{\\theta}\\)을 통과하는 순방향이다. 예를 들어, 안정적인 확산 XL[47]은 \\(1024\\times 1024\\) 이미지를 생성하기 위해 한 단계당 6,763개의 GMAC가 필요하다. 이러한 계산 요구는 해상도가 증가함에 따라 2차보다 더 증가하며, 단일 고해상도 이미지를 생성하기 위한 대기 시간을 실제 애플리케이션에 대해 비실용적으로 높게 만든다. 또한 \\(\\mathbf{x}_{t-1}\\)이 \\(\\mathbf{x}_{t}\\)에 의존한다는 점에서 \\(\\epsilon_{t}\\)과 \\(\\epsilon_{t-1}\\)의 병렬 연산은 어렵다. 따라서 여러 개의 유휴 GPU를 사용하더라도 단일 고해상도 이미지 생성을 가속화하는 것은 여전히 까다롭다. 최근, Shih _et al_. 도입된 ParaDiGMS [60]은 디노이징 단계를 데이터 병렬 방식으로 병렬화하기 위해 피카드 반복을 사용한다. 그러나 ParaDiGMS는 품질 임계값을 실패하는 추측 추측에 대한 계산을 낭비한다. 또한 다중 GPU 데이터 병렬성을 활용하기 위해 많은 총 단계 수\\(T\\)에 의존하여 잠재적인 응용 프로그램을 제한한다. 또 다른 종래의 방법은 다수의 장치에 모델을 샤딩하고 추론을 위해 텐서 병렬성을 사용하는 것이다. 그러나 이 방법은 참을 수 없는 통신 비용으로 인해 실제 응용 프로그램에는 비실용적이다. 이 두 가지 방식 외에도 단일 이미지 생성이 여러 장치에서 무료 점심 속도 향상을 즐길 수 있도록 여러 GPU 장치에 워크로드를 배포하는 대체 전략이 있습니까?\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '디스트리퓨전(DistriFusion)의 핵심 아이디어는 이미지를 패치로 분할하여 장치 간 연산을 병렬화하는 것이다. 순차적으로, 이것은 (1) 패치들을 독립적으로 컴퓨팅하고 이들을 함께 스티칭하거나, (2) 패치들 사이의 중간 활성화들을 동기적으로 통신함으로써 행해질 수 있다. 그러나, 첫 번째 접근법은 이들 사이의 상호 작용의 부재로 인해 각 패치의 경계에서 가시적인 불일치를 초래한다(도 1 및 도 2(b) 참조). 반면에 두 번째 접근법은 과도한 통신 오버헤드를 발생시켜 병렬 처리의 이점을 무효화한다. 이러한 문제를 해결하기 위해 우리는 확산 모델의 순차적 특성을 활용하여 통신과 계산을 중첩시키는 새로운 병렬성 패러다임인 _displaced patch parallelism_을 제안한다. 본 논문의 핵심적 통찰력은 패치간의 상호작용을 용이하게 하기 위해 이전 확산 단계로부터의 \'tale\' 활성화(activation displacement_)를 재사용하는 것이다. 이는 연속적인 잡음 제거 단계에 대한 입력이 비교적 유사하다는 관찰에 기초하며, 계층에서 각 패치의 활성화를 계산하는 것은 다른 패치의 새로운 활성화(activation)에 의존하지 않기 때문에 후속 계층의 계산 내에 통신이 숨겨질 수 있다. 다음으로 알고리즘과 시스템 설계의 각 측면에 대한 자세한 분석을 제공할 것입니다.\n' +
      '\n' +
      '도 3에 도시된 바와 같이, \\(\\epsilon_{\\theta}(\\mathbf{x}_{t})\\)을 예측할 때(단순화를 위해 여기서 timestep\\(t\\) 및 조건\\(c\\)의 입력을 생략), 먼저 \\(\\mathbf{x}_{t}\\)을 다중 패치 \\(\\mathbf{x}_{t}^{(1)},\\mathbf{x}_{t}^{(2)},\\ldots,\\mathbf{x}_{t}^{(N)}}}으로 분할하고, 여기서 \\(N)은 장치의 수이다. 예를 들어, 그림 3에서 \\(N=2\\)을 사용한다. 각 장치는 모델 \\(\\epsilon_{\\theta}\\)의 복제가 있으며 단일 패치를 독립적으로 병렬로 처리할 것이다.\n' +
      '\n' +
      '주어진 층 \\(l\\)에 대해 \\(i\\)번째 소자의 입력 활성화 패치를 \\(A_{t}^{l, (i)}\\)으로 표기한다. 이 패치는 먼저 이전 단계인 \\(A_{t+1}^{l}\\)에서 해당 공간 위치에서 오래된 활성화로 산란된다. (A_{t+1}^{l}\\을 얻기 위한 방법은 나중에 논의될 것이다). 여기서, \\(A_{t+1}^{l}\\)은 전체 공간 형상이다. 산란 출력에서, \\(A_{t}^{l,(i)}\\)이 배치된 \\(\\frac{1}{N}\\) 영역만이 신선하고 재계산이 필요하다. 그런 다음 레이어 연산 \\(F_{l}\\)(선형, 컨볼루션 또는 주의)을 이러한 신선한 영역에 선택적으로 적용하여 해당 영역에 대한 출력을 생성한다. 이러한 과정은 각 층마다 반복된다. 마지막으로, 모든 층으로부터의 출력은 근사 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t})\\으로 동기된다. 이 방법론을 통해 각 장치는 전체 계산의 \\(\\frac{1}{N}\\)만을 담당하므로 효율적인 병렬화가 가능하다.\n' +
      '\n' +
      '이전 단계에서 오래된 활성화를 얻는 방법에 대한 문제가 여전히 남아 있다. 도 3에 도시된 바와 같이, 각 타임스텝 \\(t\\)에서, 디바이스 \\(i\\)가 \\(A_{t}^{l,(i)}\\)을 획득하면, 그것은 그 후 활성화들을 다른 모든 디바이스들에 브로드캐스트하고 올가더 동작을 수행할 것이다. 현대의 GPU들은 종종 비동기 통신 및 계산을 지원하는데, 이는 이 AllGather 프로세스가 진행중인 계산을 차단하지 않는다는 것을 의미한다. 다음 타임스텝에서 레이어\\(l\\)에 도달할 때까지 각 장치는 이미 \\(A_{t}^{l}\\)의 복제를 받았어야 한다. 이러한 접근법은 그림 4와 같이 계산 단계 내에서 통신 오버헤드를 효과적으로 숨긴다. 그러나, 예외가 있다: 바로 첫 번째 단계((, \\(\\mathbf{x}_{T}\\)) 이 시나리오에서, 각 디바이스는 단순히 표준 동기 통신을 실행하고 다음 단계를 위한 중간 활성화를 캐싱한다.\n' +
      '\n' +
      '**희소 연산** 각 계층 \\(l\\)에 대해 원래의 연산자 \\(F_{l}\\)을 수정하여 새로운 영역에서 선택적으로 희소 연산을 가능하게 한다. 구체적으로, \\(F_{l}\\)이 컨볼루션, 선형 또는 교차-어텐션 계층인 경우, 전체 특징 맵이 아닌 새로 리프레시된 영역에만 연산자를 적용한다. 이것은 산란 출력에서 신선한 섹션을 추출하여 \\(F_{l}\\)에 공급함으로써 달성할 수 있다. \\(F_{l}\\)이 자기-어텐션 층인 층들에 대해, 우리는 그것을 SIGE [24]와 유사하게 교차-어텐션 층으로 변환한다. 이 설정에서, 신선한 영역들로부터의 질의 토큰들만이 디바이스 상에 보존되는 반면, 키 및 값 토큰들은 여전히 전체 특징 맵(산포 출력)을 포괄한다. 따라서, \\(F_{l}\\)에 대한 계산 비용은 신선 영역의 크기에 정확히 비례한다.\n' +
      '\n' +
      '**Corrected asynchronous GroupNorm.** Diffusion 모델들은 종종 네트워크에서 그룹 정규화(GN) [41, 69] 계층들을 채택한다. 이러한 레이어는 공간 차원을 가로질러 정규화되어 전체 공간 모양을 복원하기 위한 활성화의 집합이 필요하다. 섹션 5.3에서 우리는 갓 준비한 패치만 정규화하거나 오래된 기능을 재사용하면 이미지 품질이 저하된다는 것을 발견했다. 그러나, 모든 정규화 통계를 집계하는 것은 동기 통신으로 인해 상당한 오버헤드가 발생할 것이다. 이 문제를 해결하기 위해 우리는 오래된 통계에 수정항을 추가로 도입한다. 구체적으로, 주어진 단계 \\(t\\)에서 각 디바이스 \\(i\\)에 대해, 모든 GN 레이어는 \\(\\mathbb{E}[\\mathbf{A}_{t}^{(i)}]]로 표시된 새로운 패치 \\(\\mathbf{A}_{t}^{(i)}})의 그룹별 평균을 계산할 수 있다. 간단히 하기 위해, 여기서는 계층 인덱스 \\(l\\)을 생략하였다. 또한 이전 단계에서 로컬 평균 \\(\\mathbb{E}[\\mathbf{A}_{t+1}^{(i)}]\\)과 종합 글로벌 평균 \\(\\mathbb{E}[\\mathbf{A}_{t+1}]\\)을 캐싱하였다. 그런 다음 현재 단계 장치\\(i\\)에 대한 근사화된 전역 평균 \\(\\mathbb{E}[\\mathbf{A}_{t}]\\)을 다음과 같이 계산할 수 있다.\n' +
      '\n' +
      '\\mathbbb{E}[\\mathbf{A}_{t}]\\approx\\underbrace{\\mathbb{E}[\\mathbf{A}_{t+1}]}_{text{stale global mean}+\\underbrace{(\\mathbb{E}[\\mathbf{A}_{t}^{(i}]]-\\mathbbb{E}[\\mathbf{A}_{t+1}^{(i}}}_{\\text{correction}}}_tag{2}\\text{2}}}+\\underbrace{(\\mathbbb{E}[\\mathbf{A}_{t+1}}}}{{t+1}}}{{t+1}}{{t+1}}+\\underbrace{(\\mathbbb{E}[\\mathbf{A}_{t+1}}}{{e}[\\mathbf{A}_{t+1}}}}{{t+1}\n' +
      '\n' +
      '우리는 같은 기법을 사용하여 \\(\\mathbb{E}[(\\mathbf{A}_{t})^{2}]\\을 근사한 후, 분산을 \\(\\mathbb{E}[(\\mathbf{A}_{t})^{2}] -\\mathbb{E}[\\mathbf{A}_{t}]^{2}\\)으로 근사할 수 있다. 그런 다음 이러한 근사 통계량을 GN 계층에 사용하고 그 동안 국부 평균과 분산을 집계하여 비동기 통신을 사용하여 정확한 통계를 계산한다. 따라서, 통신 비용은 또한 계산에 파이프라인화될 수 있다. 우리는 이 방법이 직접 동기 집계에 필적하는 결과를 얻는다는 것을 경험적으로 발견했다. 그러나 근사 분산이 음수인 경우는 드물다. 이러한 음의 분산 그룹의 경우 새 패치의 국소 분산을 사용하기 위해 후퇴할 것이다.\n' +
      '\n' +
      '**웜업 단계.** eDiff-I[2] 및 FastComposer[70]에서 관찰된 바와 같이 확산 합성의 거동은 잡음 제거 공정 전반에 걸쳐 정성적 변화를 겪는다. 구체적으로, 샘플링의 초기 단계들은 공간 레이아웃 및 전체 시맨틱스와 같은 이미지의 저주파 측면들을 주로 형상화한다. 샘플링이 진행됨에 따라, 포커스는 국부적인 고주파 세부사항들을 복구하는 것으로 이동한다. 따라서, 특히 스텝 수가 감소된 샘플러에서 이미지 품질을 향상시키기 위해 워밍업 단계를 채택한다. 첫 번째 단계 후에 변위된 패치 병렬성을 직접 사용하는 대신 표준 동기 패치 병렬성을 예비 단계 또는 _warm-up_로 여러 번 반복한다. 섹션 5.3에 자세히 설명된 바와 같이 워밍업 단계의 이러한 통합은 성능을 크게 향상시킨다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '먼저 벤치마크 데이터 세트, 기준선 및 평가 프로토콜을 포함한 실험 설정을 설명한다. 그런 다음 품질과 효율성 모두에 관한 주요 결과를 제시한다. 마지막으로, 각 설계 선택을 확인하기 위해 일부 절제 연구를 추가로 보여준다.\n' +
      '\n' +
      '### _Setups_\n' +
      '\n' +
      '**모델.** 우리의 방법은 기성품 사전 훈련된 확산 모델만을 필요로 하기 때문에, 우리는 주로 최첨단 공개 텍스트-이미지 모델 안정 확산 XL(SDXL)에 대한 실험을 수행한다[47]. SDXL은 먼저 미리 학습된 오토 인코더를 사용하여 이미지를 더 작은 잠재 표현으로 압축한 다음 이 잠재 잠재에서 확산 모델을 적용한다.\n' +
      '\n' +
      '도. 4: \\(\\epsilon_{\\theta}(\\mathbf{x}_{t})\\) 예측시 각 장치에 대한 타임라인 시각화 Comm._ 통신이란 계산과 비동기적인 통신을 의미합니다. 전체 수집 오버헤드는 계산 내에 완전히 숨겨져 있습니다.\n' +
      '\n' +
      '도. 3: 디스트리퓨전 개요. 단순화를 위해 \\(t\\)과 \\(c\\)의 입력을 생략하고 \\(N=2\\) 장치를 예로 들어 설명한다. 위 첨자 \\({}^{(1)}\\)와 \\({}^{(2)}\\)는 각각 첫 번째 패치와 두 번째 패치를 나타낸다. 이전 단계의 줄기 활동이 어두워졌다. 각 단계 \\(t\\)에서 우리는 먼저 입력 \\(\\mathbf{x}_{t}\\)을 \\(N\\) 패치 \\(\\mathbf{x}_{t}^{(1)},\\dots,\\mathbf{x}_{t}^{(N)}\\)으로 나눈다. 각 층(\\(l\\)과 디바이스 \\(i\\)에 대해, 입력 활성화 패치 \\(\\mathbf{A}_{t}^{l,(i)}\\)을 얻을 때, 두 동작을 비동기적으로 처리한다. 먼저, 디바이스 \\(i\\)에서, \\(\\mathbf{A}_{t}^{l,(i)}\\)은 이전 단계로부터 다시 스테일 활성화 \\(\\mathbf{A}_{t+1}^{l}\\)으로 산란된다. 그런 다음 이 산란 연산의 출력은 희소 연산자 \\(F_{l}\\)(선형, 컨볼루션 또는 주의 계층)에 공급되며, 이는 신선한 영역에 대해서만 계산을 수행하고 대응하는 출력을 생성한다. 한편, 다음 단계를 위해 전체 활성화\\(\\mathbf{A}_{t}^{l,(i)}\\)를 준비하기 위해 \\(\\mathbf{A}_{t}^{l}\\)에 걸쳐 AllGather 연산을 수행한다. 우리는 각 계층에 대해 이 절차를 반복한다. 그 다음, 최종 출력들은 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t})\\)을 근사하기 위해 함께 집계되며, 이는 \\(\\mathbf{x}_{t-1}\\)을 계산하는 데 사용된다. \\(\\epsilon_{\\theta}(\\mathbf{x}_{t})\\)를 예측하기 위한 각 장치의 타임라인 시각화는 그림 4에 나와 있다.\n' +
      '\n' +
      '공간. 또한 텍스트 컨디셔닝을 용이하게 하기 위해 다수의 교차 주의 계층을 통합한다. 원래의 안정 확산[55]에 비해 SDXL은 훨씬 더 많은 주의 계층을 채택하여 계산 집약적인 모델을 생성한다.\n' +
      '\n' +
      '**Datasets.** 우리는 COCO Captions 2014 [5] 데이터세트의 HuggingFace 버전을 사용하여 우리의 방법을 벤치마킹한다. 이 데이터세트는 COntext(COCO) 데이터세트에서 Microsoft Common Objects로부터의 이미지에 대한 인간 생성 캡션을 포함한다[31]. 평가를 위해, 우리는 이미지당 하나의 캡션이 있는 5K 이미지를 포함하는 검증 세트에서 하위 집합을 무작위로 샘플링한다.\n' +
      '\n' +
      '**기준.** 품질 및 효율성 측면에서 디스트리퓨전을 다음 기준들과 비교한다:\n' +
      '\n' +
      '* _Naive Patch_. 각각의 반복에서, 입력은 행-와이즈 또는 열-와이즈 교대로 분할된다. 그런 다음 이러한 패치는 모델 간에 상호 작용 없이 독립적으로 처리됩니다. 출력은 이후에 함께 연결된다.\n' +
      '\n' +
      '그림 5: 질적 결과. FID는 지상-진실 영상에 대해 계산된다. 우리의 디스트리퓨전은 시각적 충실도를 유지하면서 사용되는 장치의 수에 따라 대기 시간을 줄일 수 있다.\n' +
      '\n' +
      '* [leftmargin=*]\n' +
      '* _ParaDiGMS[60]_는 다중 단계를 병렬로 디노이징하여 미리 학습된 확산 모델을 가속화하는 기법이다. 피카드 반복을 사용하여 미래 단계의 해를 추측하고 수렴할 때까지 반복적으로 정제한다. 우리는 ParaDiGMS에 대해 배치 크기 8을 사용하여 원래 논문의 표 4와 정렬한다[60]. 우리는 이 설정이 품질과 지연 모두에서 최고의 성능을 제공한다는 것을 경험적으로 발견했다.\n' +
      '\n' +
      '**Metrics.** 이전 작업 [23, 24, 38, 44]에 이어서, 우리는 표준 메트릭으로 이미지 품질을 평가한다: Peak Signal Noise Ratio (PSNR, higher is better), LPIPS (lower is better) [77], Frechet Inception Distance (FID, lower is better) [11]+. 우리는 벤치마킹된 방법의 출력과 원래 확산 모델 출력 사이의 사소한 수치 차이를 정량화하기 위해 PSNR을 사용한다. LPIPS는 지각적 유사성을 평가하는 데 사용된다. 또한, FID 점수는 방법의 출력과 원본 출력 또는 지상-진실 이미지 간의 분포 차이를 측정하는 데 사용된다.\n' +
      '\n' +
      '각주 †: 우리는 TorchMetrics를 사용하여 PSNR 및 LPIPS를 계산하고, CleanFID[45]를 사용하여 FID를 계산한다.\n' +
      '\n' +
      '**구현 상세.** 기본적으로 분류기 없는 안내 척도 5를 가진 50단계 DDIM 샘플러[62]를 채택하여 별도의 명시 사항이 없는 한 \\(1024\\times 1024\\) 이미지를 생성한다. 첫 번째 단계 외에도 워밍업 단계로 작용하는 또 다른 4단계 동기 패치 병렬화를 수행한다.\n' +
      '\n' +
      '우리는 PyTorch 2.2 [46]을 사용하여 본 방법의 속도 향상을 벤치마킹한다. 레이턴시를 측정하기 위해 먼저 전체 노이즈 제거 프로세스의 3회 반복으로 장치를 예열한 다음 10회 반복을 실행하고 가장 빠르고 느린 실행의 결과를 폐기하여 평균 레이턴시를 계산한다. 또한, CUDGraph를 사용하여 원본 모델과 방법 모두에 대한 커널 런칭 오버헤드를 최적화한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**품질 결과.** 도 5에서, 우리는 일부 정성적 시각적 결과를 보여주고 표 1에서 일부 정량적 평가를 보고한다. _with G.T._는 지상-진실 COCO[31] 이미지로 메트릭을 계산하는 것을 의미하는 반면, _w/ Orig._ 원래 모델의 출력으로 메트릭을 계산하는 것을 나타냅니다. PSNR의 경우, _w/ Orig._ 생성된 출력과 지상-진실 이미지 간의 상당한 수치 차이로 인해 _w/G.T._ 비교가 유익하지 않기 때문에 설정한다.\n' +
      '\n' +
      '표 1에 나타난 바와 같이, ParaDiGMS[60]은 미래의 잡음 제거 단계를 추측하는 데 상당한 계산 자원을 소비하여 총 MAC이 훨씬 더 높다. 또한, 성능 저하의 원인이 되기도 한다. 이와는 대조적으로, 우리의 방법은 단순히 다수의 GPU에 워크로드를 분배하여, 일정한 총 계산을 유지한다. 나이브 패치 기준선은 총 MAC이 낮지만 중요한 패치 간 상호 작용이 부족하여 단편화된 출력으로 이어진다. 이거.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{\\#Steps} & \\multirow{2}{*}{\\#Devices} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{PSNR (\\(\\uparrow\\))} & \\multicolumn{2}{c}{LPIPS (\\(\\downarrow\\))} & \\multicolumn{2}{c}{FID (\\(\\downarrow\\))} & \\multirow{2}{*}{MACs (T)} & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} Latency \\\\ Value (s) \\\\ \\end{tabular} } \\\\ \\cline{5-5} \\cline{7-10}  & & & & w/ G.T. & & w/ Orig. & & w/ G.T. & & Value (s) & Speedup \\\\ \\hline \\multirow{5}{*}{50} & \\multirow{2}{*}{1} & Original & – & 0.797 & – & 24.0 & – & 338 & 5.02 & – \\\\ \\cline{3-10}  & & & Naive Patch & 28.2 & 0.812 & 0.596 & 33.6 & 29.4 & **322** & **2.83** & **1.8\\(\\times\\)** \\\\ \\cline{3-10}  & & **Ours** & **31.9** & **0.797** & **0.146** & **24.2** & **4.86** & 338 & 3.35 & 1.5\\(\\times\\) \\\\ \\cline{3-10}  & & & Naive Patch & 27.9 & 0.853 & 0.753 & 125 & 133 & **318** & **1.74** & **2.9\\(\\times\\)** \\\\ \\cline{3-10}  & & & **Ours** & **31.0** & **0.798** & **0.183** & **24.2** & **5.76** & 338 & 2.26 & 2.2\\(\\times\\) \\\\ \\cline{3-10}  & & & Naive Patch & 27.8 & 0.892 & 0.857 & 252 & 259 & **324** & **1.27** & **4.0\\(\\times\\)** \\\\ \\cline{3-10}  & & & ParaDiGMS & 29.3 & 0.800 & 0.320 & 25.1 & 10.8 & 657 & 1.80 & 2.8\\(\\times\\) \\\\ \\cline{3-10}  & & & **Ours** & **30.5** & **0.799** & **0.211** & **24.4** & **6.46** & 338 & 1.77 & 2.8\\(\\times\\) \\\\ \\hline \\multirow{2}{*}{25} & \\multirow{2}{*}{1} & Original & – & 0.801 & – & 23.9 & – & 169 & 2.52 & – \\\\ \\cline{3-10}  & & & ParaDiGMS & 29.6 & 0.808 & 0.273 & 25.8 & 10.4 & 721 & 1.89 & 1.3\\(\\times\\) \\\\ \\cline{3-10}  & & & **Ours** & **31.5** & **0.802** & **0.161** & **24.6** & **5.67** & **169** & **0.93** & **2.7\\(\\times\\)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 정량적 평가_ MACs_는 단일 \\(1024\\times 1024\\) 이미지를 생성하기 위한 전체 잡음 제거 프로세스에 대해 모든 장치에 걸친 누적 계산을 측정한다. _ w/G.T._는 지상-진실 이미지로 메트릭을 계산하는 것을 의미하는 반면, _w/ Orig._ 평균은 원래 모델의 표본과 같습니다. PSNR을 위해, 우리는 _w/ Orig._ 설정 중. 이 방법은 전체 MAC을 유지하면서 모든 메트릭에 걸쳐 원래 모델의 결과를 반영한다. 또한 사용된 장치의 수에 비례하여 NVIDIA A100 GPU의 대기 시간을 줄입니다.\n' +
      '\n' +
      '그림 6: NVIDIA A100 GPU에서 서로 다른 해상도에 걸쳐 단일 이미지를 생성하기 위해 50단계 DDIM 샘플러 [62]로 디스트리퓨전의 총 대기 시간을 측정했다. 해상도를 높일 때 GPU 장치를 더 잘 활용합니다. 현저하게, \\(3840\\times 3840\\) 영상을 생성할 때, DistriFusion은 2, 4, 8 A100s에서 각각 1.8\\(\\times\\), 3.4\\(\\times\\) 및 6.1\\(\\times\\)의 속도를 달성한다.\n' +
      '\n' +
      '제한은 모든 평가 메트릭에 반영되는 것처럼 이미지 품질에 상당한 영향을 미칩니다. 우리의 디스트리퓨전은 상호 작용을 잘 보존할 수 있다. 8개의 장치를 사용할 경우에도 원래 모델과 유사한 PSNR, LPIPS 및 FID 점수를 달성한다.\n' +
      '\n' +
      '**속도 향상.** 이론적인 계산 감소에 비해, 온-하드웨어 가속은 실제 응용에 더 중요하다. 우리의 방법의 효과를 입증하기 위해, 우리는 또한 8개의 NVIDIA A100 GPU에 대한 표 1의 종단 간 대기 시간을 보고한다. 50단계 설정에서 ParaDiGMS는 화질 저하의 비용으로 본 방법과 동일한 \\(2.8\\times\\)의 속도를 달성한다(그림 5 참조). 더 일반적으로 사용되는 25단계 설정에서 ParaDiGMS는 과도한 추측으로 인해 한계 \\(1.3\\times\\)의 속도 향상만을 가지고 있으며, 이는 Shih _et al_. [60]에도 보고되어 있다. 그러나 본 논문에서 제안한 방법은 기존의 품질만을 반영하면서도 2.7\\(\\times\\)의 가속도를 얻을 수 있다.\n' +
      '\n' +
      '1024\\(1024\\times 1024\\) 영상을 생성할 때 SDXL의 낮은 GPU 이용률로 인해 속도 향상이 제한된다. 디바이스의 사용을 극대화하기 위해 그림 6의 해상도를 \\(2048\\times 2048\\)과 \\(3840\\times 3840\\)으로 확장하였으며, 이러한 더 큰 해상도에서는 GPU 디바이스가 더 잘 활용된다. 구체적으로, 디스트리퓨전은 2, 4, 8 A100s에서 각각 1.8\\(\\times\\), 3.4\\(\\times\\) 및 6.1\\(\\times\\)의 지연시간을 감소시켰다. 이러한 결과는 PyTorch로 벤치마킹된다. TVM[4] 및 텐서RT[1]과 같은 고급 컴파일러를 사용하면 SIGE[24]에서 볼 수 있듯이 훨씬 더 높은 GPU 활용률과 결과적으로 디스트리퓨전으로부터 더 뚜렷한 속도 향상을 기대할 수 있다. 실제 사용에서, 배치 크기는 분류기가 없는 안내[12]로 인해 종종 배가된다. 우리는 먼저 배치를 분할한 다음 각 배치에 디스트리퓨전을 별도로 적용할 수 있다. 이 방법은 4와 8 A100s에서 각각 3.6\\(\\times\\)과 6.6\\(\\times\\)의 전체 속도를 향상시키며, 단일 영상(3840\\times 3840\\)을 생성한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**통신 비용.** 표 2에서 동기 텐서 병렬성(_Sync. TP_)으로 지연 시간을 벤치마킹한다. 및 동기 패치 병렬성(_Sync. PP_) 그리고 해당 통신량을 보고합니다. TP와 비교하여 PP는 교차 주의 및 선형 계층 내에서 통신이 필요하지 않다. 컨볼루션 계층들의 경우, 전체 텐서의 최소 부분을 나타내는 패치 경계들에서만 통신이 요구된다. 또한 PP는 AllGather over AllReduce를 활용하여 통신 수요를 낮추고 컴퓨팅 자원을 추가로 사용하지 않습니다. 따라서 PP는 TP보다 통신량이 60\\(%\\) 적으며, 통신량이 1.6\\sim 2.1\\times\\ 더 빠르다. 우리는 또한 어떠한 통신도 없는 이론적인 PP 기준선을 포함한다(_No Comm._) _Sync에서 통신 오버헤드를 입증합니다. PP_ 및 디스트리퓨전. _Sync와 비교하여. PP_, DistriFusion은 이러한 오버헤드를 \\(50\\%\\) 이상 더 줄인다. 나머지 오버헤드는 주로 비동기 통신을 위한 NVIDIA 집합 통신 라이브러리(NCCL)의 현재 사용에서 비롯된다. NCCL 커널은 SMs(GPU의 컴퓨팅 리소스)를 사용하므로 중복 계산이 느려집니다. 원격 메모리 액세스를 사용하면 이 문제를 우회하고 성능 격차를 줄일 수 있습니다.\n' +
      '\n' +
      '**입력 유사성.** 변위된 패치 병렬성은 연속적인 잡음 제거 단계의 입력이 유사하다는 가정에 의존한다. 이 주장을 뒷받침하기 위해 50단계 DDIM 샘플러를 사용하여 모든 연속 단계에 걸친 모델 입력 차이를 정량적으로 계산한다. 평균 차이는 \\([-4,4]\\)(약 \\(0.3\\%\\))의 입력 범위 내에서 0.02에 불과하다. 도 7은 단계 9와 8(무작위로 선택됨) 사이의 입력 차이를 더 정성적으로 시각화한다. 그 차이는 거의 모두 0이며, 이웃 단계의 입력 사이에 높은 유사성에 대한 가설을 입증한다.\n' +
      '\n' +
      '**-단계 샘플링 및 워밍업 단계** 위에서 언급한 바와 같이, 우리의 접근법은 인접한 잡음 제거 단계가 유사한 입력, _i.e_., \\(\\mathbf{x}_{t}\\approx\\mathbf{x}_{t-1}\\)을 공유한다는 관찰에 달려 있다. 그러나 단계 크기를 증가시키고 그에 따라 단계 수를 줄임으로써 근사 오차가 증가하여 잠재적으로 방법의 효과를 손상시킬 수 있다. 그림 8에서는 10단계 DPM-Solver [33, 34]를 사용하여 결과를 제시한다. 10단계 구성은 트레이닝이 없는 샘플러가 화질을 유지하기 위한 임계값이다. 이러한 환경에서 준비되지 않은 순진한 디스트리퓨전은 이미지 품질을 보존하기 위해 고군분투한다. 그러나 추가 2단계 워밍업을 통합하면 지연 시간이 약간만 증가하여 성능이 크게 회복됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{\\(1024\\times 1024\\)} & \\multicolumn{3}{c}{\\(2048\\times 2048\\)} & \\multicolumn{3}{c}{\\(3840\\times 3840\\)} \\\\ \\cline{2-7}  & Comm. & Latency & Comm. & Latency & Comm. & Latency \\\\ \\hline Original & – & 5,02s & – & 23.7s & – & 140s \\\\ \\hline Sync. TP & 1.33G & 3.61s & 5.33G & 11.7s & 18.7G & 46.3s \\\\ Sync. PP & 0.42G & 2.21s & 1.48G & 5.62s & 5.38G & 24.7s \\\\\n' +
      '**DistriFusion (Ours)** & **0.42G** & **1.77s** & **1.48G** & **4.81s** & **5.38G** & **22.9s** \\\\ \\hline No Comm. & – & 1.48s & – & 4.14s & – & 21.3s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 상이한 해상도에 걸쳐 8 A100s와의 통신 비용 비교. _ 동기화 TP/PP_: 동기 텐서/패치 병렬성_ No Comm._ : 이상적 무통신 PP. _Comm._ 총 통신량을 측정합니다. PP는 TP에 비해 \\(\\frac{1}{3}\\) 이하의 통신량만을 필요로 한다. 우리의 디스트리퓨전은 통신 오버헤드를 \\(50\\sim 60\\%\\) 더 줄였다.\n' +
      '\n' +
      '그림 7: 9단계와 8단계의 입력과 그 차이를 시각화합니다. 모든 피쳐 맵은 채널별 평균화됩니다. 그 차이는 거의 모두 0으로 높은 유사성을 보인다.\n' +
      '\n' +
      '**GroupNorm.** 섹션 4에서 논의된 바와 같이, 정확한 그룹 정규화(GN) 통계를 계산하는 것은 이미지 품질을 보존하는 데 중요하다. 그림 9에서 우리는 네 가지 다른 GN 방식을 비교한다. 첫 번째 접근 방식 _Separate GN_은 온-디바이스 프레쉬 패치로부터의 통계를 사용한다. 이 접근법은 더 낮은 이미지 충실도의 비용으로 최상의 속도를 제공한다. 이러한 절충은 정확한 통계 추정을 위한 패치 크기가 충분하지 않기 때문에 많은 수의 중고 장치에 대해 특히 심각하다. 두 번째 방식 _Stale GN_은 오래된 액티베이션을 사용하여 통계를 계산한다. 그러나 이 방법은 오래된 활성화와 신선한 활성화 사이의 분포가 다르기 때문에 종종 안개 같은 노이즈 효과가 있는 이미지를 생성하기 때문에 품질 저하에 직면한다. 세 번째 접근 방식 _Sync. GN_는 정확한 통계를 집계하기 위해 동기화된 통신을 사용한다. 최상의 화질을 달성하지만, 동기화 오버헤드가 크다. 우리의 방법은 오래된 통계량과 새로운 통계량 사이의 분포 격차를 좁히기 위해 수정항을 사용한다. 그것은 _Sync와 동등한 수준의 화질을 달성한다. GN_이지만 동기 통신 오버헤드가 발생하지 않습니다.\n' +
      '\n' +
      '##6 결론 및 논의\n' +
      '\n' +
      '본 논문에서는 병렬화를 위해 다중 GPU를 사용하여 확산 모델을 가속화하기 위해 DistriFusion을 소개한다. 제안하는 방법은 이미지를 패치로 분할하여 각각 별도의 GPU에 할당한다. 우리는 패치 상호 작용을 유지하기 위해 이전 단계에서 사전 계산된 활성화를 재사용한다. 안정적인 확산 XL에서 8 NVIDIA A100s에서 최대 6.1\\(\\times\\)의 속도 향상을 보였다. 이러한 발전은 AI 생성 콘텐츠 생성의 효율성을 높일 뿐만 아니라 AI 애플리케이션을 위한 병렬 컴퓨팅에 대한 향후 연구의 새로운 벤치마크를 설정한다.\n' +
      '\n' +
      '**제한.** 계산 내의 통신 오버헤드를 완전히 숨기기 위해, NVLink는 속도 향상을 최대화하기 위해 디스트리퓨전에 필수적이다. 그러나 최근에는 NVLink가 널리 사용되고 있다. 더욱이, 양자화 [26]은 또한 우리의 방법에 대한 통신 워크로드를 감소시킬 수 있다. 또한, 디스트리퓨전(DistriFusion)은 장치의 활용도가 떨어짐에 따라 저해상도 영상에 대한 속도 향상이 제한적이다. 고급 컴파일러 [1, 4]는 장치를 활용하고 더 나은 속도를 달성하는 데 도움이 됩니다. 우리의 방법은 잡음 제거 상태의 급격한 변화로 인해 극히 소수의 단계 방법[35, 36, 37, 58, 64]에서는 작동하지 않을 수 있다. 그러나 예비 실험은 디스트리퓨전이 고품질 결과를 얻기에 약간 더 많은 단계(예: 10)가 충분하다는 것을 시사한다.\n' +
      '\n' +
      '**사회적 영향** 본 논문에서는 다중 GPU를 활용하여 확산 모델을 가속화하는 새로운 방법을 제안하여 사용자에게 보다 반응적인 상호 작용을 가능하게 한다. 지연 시간을 줄임으로써, 우리의 발전은 이미지 편집과 같은 응용 프로그램에서 더 원활한 창의적인 프로세스로 이어집니다.\n' +
      '\n' +
      '그러나 많은 생성 모델과 마찬가지로 오용 가능성이 있다. 우리는 사려 깊은 통치의 필요성을 인정한다. 이전 작업 [24, 38]의 지침에 따라 잠재적인 피해를 완화하기 위해 라이선스에 승인된 사용을 명확하게 지정할 것이다. 이 기술이 발전함에 따라 대중의 관심을 염두에 둠으로써 책임 있는 혁신을 추진하면서 창의성과 접근성을 장려하고자 한다.\n' +
      '\n' +
      '#### Acknowledgments\n' +
      '\n' +
      '우리는 그들의 유익한 토론과 소중한 피드백에 준얀 주와 리굵 주에게 감사한다. 이 프로젝트는 MIT-IBM 왓슨 AI 랩, 아마존, MIT 사이언스 허브, 국립과학재단이 지원한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]_NVIDIA/TensorRT_. 2023년\n' +
      '* [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji\n' +
      '\n' +
      '그림 8: 준비 단계가 다른 10단계 DPM-Solver [33, 34]에 대한 질적 결과. LPIPS는 전체 COCO [5] 데이터 세트에 걸쳐 원래 SDXL의 샘플에 대해 계산된다. 준비 단계가 없는 순진한 디스트리퓨전은 명백한 품질 저하를 가지고 있다. 2단계 워밍업을 추가하면 높은 대기 시간 상승을 피하면서 성능이 크게 향상됩니다.\n' +
      '\n' +
      '그림 9: 8 A100s를 가진 다양한 GN 계획의 정성적 결과. LPIPS는 전체 COCO[5] 데이터세트에서 원본 샘플에 대해 계산된다. _ GN_은 온-디바이스 패치의 통계만을 이용한다. _ Stale GN_은 오래된 통계를 재사용한다 그들은 품질 저하를 겪는다. _ 동기화 GN_는 추가 오버헤드의 비용으로 정확한 통계를 보장하기 위해 데이터를 동기화한다. 우리의 수정된 비동기 GN은 오래된 통계를 수정함으로써 동기화의 필요성을 피하고 품질을 효과적으로 복원한다.\n' +
      '\n' +
      'Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with ensemble of expert denoisers. _ arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntana Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science. [https://cdn.openai.com/papers/dall-e-3.pdf_](https://cdn.openai.com/papers/dall-e-3.pdf_), 2023.\n' +
      '* [4] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In _OSDI_, 2018.\n' +
      '* [5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.\n' +
      '* [6] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimany Dubey, et al. Emu: Enhancing image generation models using pathogenic needles in a haystack. _arXiv preprint arXiv:2309.15807_, 2023.\n' +
      '* [7] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated network with less inference complexity. In _CVPR_, 2017.\n' +
      '* [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _NeurIPS_, 2014.\n' +
      '* [9] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and Navdeep Jaitly. Matryoshka diffusion models. _arXiv preprint arXiv:2310.15111_, 2023.\n' +
      '* [10] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. _NeurIPS_, 2015.\n' +
      '* [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _NeurIPS_, 2017.\n' +
      '* [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.\n' +
      '* [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _NeurIPS_, 2020.\n' +
      '* [14] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. _arXiv preprint arXiv:2301.11093_, 2023.\n' +
      '* [15] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. _NeurIPS_, 2019.\n' +
      '* [16] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. In _BMVC_, 2014.\n' +
      '* [17] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep neural networks. _MLSys_, 2019.\n' +
      '* [18] Patrick Judd, Alberto Delmas, Sayeh Sharify, and Andreas Moshovos. Cnvlutin2: Ineffectual-activation-and-weight-free deep neural network computing. _arXiv preprint arXiv:1705.00125_, 2017.\n' +
      '* [19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _NeurIPS_, 2022.\n' +
      '* [20] Gwanghyun Kim and Jong Chul Ye. Diffusionclip: Text-guided image manipulation using diffusion models. _arXiv preprint arXiv:2110.02711_, 2021.\n' +
      '* [21] Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In _ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models_, 2021.\n' +
      '* [22] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. _ICLR_, 2016.\n' +
      '* [23] Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architectures for interactive conditional gans. In _CVPR_, 2020.\n' +
      '* [24] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. In _NeurIPS_, 2022.\n' +
      '* [25] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and Xiaoou Tang. Not all pixels are equal: Difficulty-aware semantic segmentation via deep layer cascade. In _CVPR_, 2017.\n' +
      '* [26] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. _arXiv preprint arXiv:2302.04304_, 2023.\n' +
      '* [27] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. _NeurIPS_, 2023.\n' +
      '* [28] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, D. Song, and I. Stoica. Terapipe: Token-level pipeline parallelism for training large-scale language models. _ICML_, 2021.\n' +
      '* [29] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Z. Chen, Hao Zhang, Joseph E. Gonzalez, and I. Stoica. Alpaserve: Statistical multiplexing with model parallelism for deep learning serving. _USENIX Symposium on Operating Systems Design and Implementation_, 2023.\n' +
      '* [30] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song Han. Mcunetv2: Memory-efficient patch-based inference for tiny deep learning. In _Annual Conference on Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '\n' +
      '* [32] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In _CVPR_, 2015.\n' +
      '* [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.\n' +
      '* [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.\n' +
      '* [35] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. _arXiv preprint arXiv: 2310.04378_, 2023.\n' +
      '* [36] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. _arXiv preprint arXiv: 2311.05556_, 2023.\n' +
      '* [37] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. _arXiv preprint arXiv:2210.03142_, 2022.\n' +
      '* [38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [39] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In _SOSP_, 2019.\n' +
      '* [40] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, J. Bernauer, Bryan Catanzaro, Amar Phanishayee, and M. Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. _International Conference for High Performance Computing, Networking, Storage and Analysis_, 2021.\n' +
      '* [41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _ICML_, 2021.\n' +
      '* [42] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _ICML_, 2022.\n' +
      '* [43] Bowen Pan, Wuwei Lin, Xiaolin Fang, Chaoqin Huang, Bolei Zhou, and Cewu Lu. Recurrent residual module for fast inference in videos. In _CVPR_, 2018.\n' +
      '* [44] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In _CVPR_, 2019.\n' +
      '* [45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in GAN evaluation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 11400-11410. IEEE, 2022.\n' +
      '* [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an imperative style, high-performance deep learning library. In _NeurIPS_, 2019.\n' +
      '* [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In _ICLR_, 2024.\n' +
      '* [48] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. _Sc20: International Conference For High Performance Computing, Networking, Storage And Analysis_, 2019.\n' +
      '* [49] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, 2021.\n' +
      '* [50] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [51] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3505-3506, 2020.\n' +
      '* [52] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In _2021 USENIX Annual Technical Conference, USENIX ATC 2021, July 14-16, 2021_, pages 551-564. USENIX Association, 2021.\n' +
      '* [53] Mengye Ren, Andrei Pokrovsky, Bin Yang, and Raquel Urtasun. Sbnet: Sparse blocks network for fast inference. In _CVPR_, 2018.\n' +
      '* [54] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In _CVPR_, 2017.\n' +
      '* [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer, 2015.\n' +
      '* [57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcuk Karagol Ayan, Tim Salimans, et al. Photoelastic text-to-image diffusion models with deep language understanding. _NeurIPS_, 2022.\n' +
      '* [58] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2021.\n' +
      '* [59] Shaohuai Shi and Xiaowen Chu. Speeding up convolutional neural networks by exploiting the sparsity of rectifier units. _arXiv preprint arXiv:1704.07724_, 2017.\n' +
      '\n' +
      '* [60] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari. Parallel sampling of diffusion models. _NeurIPS_, 2023.\n' +
      '* [61] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [62] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2020.\n' +
      '* [63] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2020.\n' +
      '* [64] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\n' +
      '* [65] Haotian Tang, Zhijian Liu, Xiuyu Li, Yujun Lin, and Song Han. Torchsparse: Efficient point cloud inference engine. In _MLSys_, 2022.\n' +
      '* [66] Haotian Tang, Shang Yang, Zhijian Liu, Ke Hong, Zhongming Yu, Xiuyu Li, Guohao Dai, Yu Wang, and Song Han. Torchsparse++: Efficient training and inference framework for sparse convolution on gpus. In _MICRO_, 2023.\n' +
      '* [67] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. 34:11287-11302, 2021.\n' +
      '* [68] Leslie G. Valiant. A bridging model for parallel computation. _Commun. ACM_, 33(8):103-111, 1990.\n' +
      '* [69] Yuxin Wu and Kaiming He. Group normalization. In _ECCV_, 2018.\n' +
      '* [70] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. _arXiv_, 2023.\n' +
      '* [71] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In _ICLR_, 2022.\n' +
      '* [72] Yuanzhong Xu, HyoukJong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. Gspmd: General and scalable parallelization for ml computation graphs. _arXiv preprint arXiv: 2105.04663_, 2021.\n' +
      '* [73] Jinhui Yuan, Xinqi Li, Cheng Cheng, Juncheng Liu, Ran Guo, Shenghang Cai, Chi Yao, Fei Yang, Xiaodong Yi, Chuan Wu, Haoran Zhang, and Jie Zhao. Oneflow: Redesign the distributed deep learning framework from scratch. _arXiv preprint arXiv: 2110.15032_, 2021.\n' +
      '* [74] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In _ICLR_, 2022.\n' +
      '* [75] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. 2022.\n' +
      '* [76] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In _CVPR_, 2023.\n' +
      '* [77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.\n' +
      '* [78] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. _arXiv preprint arXiv:2304.11277_, 2023.\n' +
      '* [79] Liammin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter- and {Intra-Operator} parallelism for distributed deep learning. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 559-578, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>