<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#아리아 일상활동 데이터세트\n' +
      '\n' +
      '조양 Lv, 니콜라스 샤론, 피에르 물롱, 알렉산더 가미노, 청 펭, 크리스 스위니, 에드워드 밀러, 휘촨 탕, 제프 마이스너, 징 동, 키란 소마순다람, 루이스 페스케이라, 마크 슈웨신저, 옴카 파키, 차오구, 렌초 데 나디, 샹이 청, 스티브 사린엔, 비제이 바이야, 유양 주, 리처드 뉴콤, 자콥 줄리안 엥겔, 샤칭 판, 칼 렌\n' +
      '\n' +
      '메타현실연구소\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 프로젝트 아리아 안경을 사용하여 기록된 자기중심적 멀티모달 오픈 데이터세트인 아리아 에브리데이 활동(AEA) 데이터세트를 제시한다. AEA는 지리적으로 다양한 5개의 실내 위치에서 여러 착용자가 기록한 143개의 일일 활동 시퀀스를 포함한다. 기록물 각각은 프로젝트 아리아 안경을 통해 기록된 멀티모달 센서 데이터를 포함한다. 또한, AEA는 높은 주파수 전역 정렬된 3D 궤적, 장면 포인트 클라우드, 프레임당 3D 눈 시선 벡터 및 시간 정렬된 음성 전사를 포함하는 기계 인식 데이터를 제공한다. 본 논문에서는 신경망 장면 재구성 및 신속한 분할을 포함하여 이 데이터 세트에 의해 활성화된 몇 가지 예시적인 연구 응용 프로그램을 시연한다. AEA는 Projectaria.com에서 다운로드할 수 있는 오픈 소스 데이터세트이며, 프로젝트 아리아 툴에서 데이터세트를 사용하는 방법에 대한 오픈 소스 구현 및 예를 제공하고 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '증강 현실(AR) 장치와 개인용 웨어러블 AI 장치가 미래에 어디에나 있을 것이라는 약속은 사람들의 삶에 심오한 영향을 미칠 새로운 기술을 개발할 기회를 만든다. 착용자와 동일한 자기 중심적 관점에서 항상 멀티모달 데이터를 캡처하는 새로운 AR 및 AI 장치는 뚜렷한 새로운 데이터 기회와 도전을 제공한다. 대용량 언어 모델과 같은 기계 학습의 발전과 함께 이러한 자기 중심적 장치의 지속적인 맥락 데이터를 활용함으로써 착용자의 마음에 확장으로 작용할 수 있는 진정한 개인화 및 맥락화된 AI 어시스턴트를 구축할 수 있을 것이다.\n' +
      '\n' +
      '오늘날의 멀티모달 AI 어시스턴트는 GPT-4v[18], 제미니[26], 라바[15]와 같은 이미지 또는 짧은 비디오로 인터리빙된 텍스트에서 컨텍스트를 활용하여 지능형 기능을 보여주지만, 공공 인터넷 데이터에 대한 액세스만 가능하고 사용자가 의식적으로 입력하도록 프롬프트가 있다. AI 비서의 미래는 사용자에 대한 훨씬 더 많은 컨텍스트 데이터에 액세스할 수 있어야 하며 이 데이터로 추론할 수 있어야 한다고 믿습니다. 첫째, 인공지능은 공간 오디오, 모션 정보 및 장기간 비디오를 포함하여 향후 우리가 기대하는 센서와 하드웨어를 사용하여 착용자와 환경에 대한 모든 풍부한 센서 데이터를 충분히 활용할 수 있어야 한다. 둘째, 인공지능은 근본적인 글로벌 좌표 공간에서 시공간적 맥락을 지속적으로 추론할 수 있어야 한다. 셋째, AI는 사람의 시선 움직임이나 손을 이용한 물리적 상호작용을 관찰하여 착용자의 의도를 파악해야 한다.\n' +
      '\n' +
      '이러한 분야의 연구 및 응용 프로그램을 강화하기 위해서는 진정으로 대표적이며 새로운 기능을 측정하고 훈련하기에 충분한 컨텍스트 데이터를 포함하는 데이터 세트가 필요하다. 기존의 자기 중심 데이터 세트는 주로 전통적인 비디오 카메라를 사용하여 캡처되며 이 연구에 필요한 모든 센서 양식을 포함하지 않는다. 그들은 일반적으로 미래의 AR 안경(공간 오디오, 관성 데이터와 같은)으로부터 예상되는 원시 데이터, 정밀한 3D 위치 데이터, 모달리티들 및 디바이스들에 걸친 시간 동기화, 및 착용자의 동작 또는 의도를 추론하기 위한 추가적인 개인 컨텍스트가 부족하다.\n' +
      '\n' +
      '본 논문에서는 아리아 에브리데이 액티비티(AEA, Aria Everyday Activities) 데이터세트, 4D 데이터세트, AI 및 AR 연구를 위한 멀티모달 감각 정보 및 최첨단 기계 인식 출력의 풍부한 세트를 소개한다. AEA 데이터 세트는 비뚤어진 폼 팩터에서 항상 켜져 있는 자기 중심 데이터를 제공하기 위한 타의 추종을 불허하는 능력을 가진 센서 플랫폼인 프로젝트 아리아 장치[7]를 사용하여 캡처되었다. 아리아 데이터는 다음과 같은 원시 센서 데이터를 포함한다: 고해상도 RGB 비디오, 위치 추적을 위한 저해상도 글로벌 셔터 흑백 비디오, 접안 비디오, 두 개의 IMU 데이터 스트림, 여러 마이크로폰으로부터의 공간 오디오, 자력계 데이터 및 기압계 데이터. 최첨단 기계 인식 서비스(MPS) 1의 결과를 추가로 제공한다. MPS 데이터는 모델에 대한 상황 입력으로 활용되거나 더 엄격한 입력 제약 조건을 가진 목표를 측정하기 위해 의사 지상 진실로 사용될 수 있는 매우 정확하고 신뢰할 수 있는 결과를 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '각주 1: 기계 인식 서비스(MPS) 문서 페이지AEA는 모든 일일 활동 기록이 시공간적으로 정렬된 4D 종단 데이터 세트이다. AEA는 하루에 다인 활동을 대표하는 5개 위치에서 다인 착용자가 기록한 143개의 활동을 포함한다. 6DoF 포즈를 제공하는 온-디바이스 고주파 궤적에 더하여, AEA는 동일한 위치에서 모든 관찰에 대해 전역적으로 정렬된 3D 컨텍스트를 포함한다. 각 위치에서 서로 다른 착용자의 모든 기록에는 정렬된 궤적과 동일한 3D 좌표계에 전역 포인트 클라우드가 포함되어 있다. 각 착용자의 의도를 나타내기 위해, 우리는 시선 추적 카메라 스트림으로부터 계산된 처리된 시선을 제공한다. 우리는 녹음에서 연설의 시간 정렬된 전사를 추가로 제공한다. 다인칭 활동의 경우, 모든 기록은 외부 시간 코드와 정확하게 시간 동기화되며, 이는 모든 상이한 관찰이 공간뿐만 아니라 시간 도메인에서도 정확하게 동기화되도록 보장한다.\n' +
      '\n' +
      'AEA 데이터셋은 최초로 공개된 아리아 데이터셋인 아리아 Pilot 데이터셋(APD) [17]의 Everyday Activities 섹션의 업데이트 버전이다. 이 데이터 세트는 다양한 연구 분야에서 프로젝트 아리아 데이터를 실험하는 연구자들을 끌어들이는 첫 번째 목적지가 되었다. APD의 출범 이후 프로젝트 아리아는 아리아 디지털 트윈 데이터세트[19], 아리아 합성 환경 데이터세트, Ego-Exo4D 데이터세트[10] 등 여러 새로운 공개 데이터세트에 사용되고 있다. 이러한 오픈 데이터 릴리스에 따라 프로젝트 아리아는 MPS와 이를 제공하는 오픈 소스 툴링을 개선했다[7].\n' +
      '\n' +
      'AEA 데이터 세트에서는 원래 파일럿 데이터 세트에 대한 세 가지 주요 업데이트를 제공한다. 첫째, 새로운 오픈 소스 툴링과 일치하도록 데이터 형식을 업데이트하여 모든 아리아 공개 데이터 세트에 통합된 툴링 세트를 사용하여 데이터를 로드하고 쿼리할 수 있다. 둘째, 가장 최근의 Machine Perception Services로 원시 아리아 레코딩을 재처리하여 (1) 보다 정밀한 6DoF 포즈 정보, (2) 새롭게 생성된 2차원 관측을 이용한 반밀집 포인트 클라우드, (3) 보다 정밀한 3차원 방향 벡터를 이용한 시선 보정, (4) 프레임당 센서 외부 및 내부 보정 등의 개선점을 제공하였다. 셋째, C++와 Python에서 보다 쉬운 데이터 로딩 기능으로 오픈 소스 툴링을 확장하고, 본 논문에서 논의된 실습 사례를 실행할 수 있는 도구를 만들었다. 기존 프로젝트 아리아 데이터셋 중 AEA는 모든 녹음과 기계 인식 맥락이 동일하게 정렬된 상태에서 연구자가 자기중심적인 일상 활동을 연구할 수 있는 고유한 데이터셋 역할을 계속할 것이다.\n' +
      '\n' +
      '그림 1: 위치 1에 기록된 일부 예제 활동을 사용하는 아리아 에브리데이 활동(AEA) 데이터 세트의 개요. 오른쪽 열에서, 우리는 두 착용자가 한 활동에서 서로 대화하는 시간 동기화 스냅샷을 강조하며, 다음 정보는 (1) 그들의 고주파 6DoF 클로즈 루프 궤적, (2) 관찰된 포인트 클라우드, (3) RGB 카메라 뷰 절두체, (4) 흑백 장면 카메라, (5) 접안 카메라, (6) 세 카메라 스트림 모두에 대한 투영된 눈 시선 및 (7) 전사된 음성을 나타낸다. 또한 왼쪽에는 RGB 스트림에 투영된 눈동자(녹색 점)를 사용하여 다양한 활동(예: 식사, 빨래하기, 옷 접기, 요리)을 강조한다. 모든 기록은 환경 점군(반밀도 점)에 공간적으로 정렬된 근접 루프 궤적(흰색 선)을 포함한다.\n' +
      '\n' +
      '공간 및 시간 좌표 프레임입니다.\n' +
      '\n' +
      '연구에서 AEA 데이터 세트의 기능을 추가로 설명하기 위해 우리는 신경 재구성 및 신속한 분할에 대한 예시적인 응용 프로그램을 제공했다. 신경 장면 재구성에서, 우리는 가우시안 스플래팅[11]의 최근 발전을 사용하여 단일 기록 또는 다중 기록으로부터 관찰된 장면을 재구성하기 위해 폐쇄 루프 궤적과 전역 포인트 클라우드를 활용하는 방법을 보여준다. 마지막으로, 강력한 기반 모델 SAM[12]의 증류 변형인 Efficient-SAM[29], 접지-DINO[16]을 사용하여 눈 시선과 음성을 사용하여 착용자의 입력으로 기반 모델을 쉽게 프롬프트할 수 있는 두 가지 프롬프트 세그먼트화 예를 제공했다. 우리는 이러한 예시적인 응용 프로그램이 연구자들이 이러한 풍부한 맥락에서 더 강력한 AI를 탐구하도록 영감을 줄 수 있기를 바란다.\n' +
      '\n' +
      '요약하면, 우리는 프로젝트 아리아 안경을 사용하여 기록된 다인 일상 활동 데이터 세트인 AEA 데이터 세트를 멀티모달 감각 데이터와 4D 시공간 정렬된 기계 인식 정보와 함께 제시한다. 이 기술 보고서에서는 먼저 Sec.2에서 관련 작업에 대해 논의하고 Sec.3에서 이 데이터 세트의 주요 측면을 설명하고 Sec.4에서 오픈 소스 도구를 제공할 것이며 Sec.5에서는 이 데이터 세트를 사용하여 몇 가지 예시적인 연구 응용 프로그램을 제공할 것이다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이 섹션에서는 자기 중심 AI 및 3D 멀티모달 연구 커뮤니티를 제공한 데이터 세트에 특히 초점을 맞춘 이전 작업을 검토한다.\n' +
      '\n' +
      '자기 중심 데이터 세트: 1인칭 비전(FPV) 및 자기 중심 데이터 세트[13, 1, 6, 9]는 자기 중심 비디오 이해 및 AI에서 연구 개발을 주도하는 중요한 역할을 했다. 에픽키친[5, 6] 및 Ego4D[9]의 두 가지 정성적 데이터 세트는 광범위한 자기중심적 비디오 및 주석과 쌍을 이루는 다수의 자기중심적 이해 벤치마크를 설정했다. 이러한 데이터 세트는 주로 착용자의 머리에 장착된 비디오 기록 장치, 예를 들어 고프로 헤드 장착 카메라 리그에 의해 캡처되었다. 그들은 3D 정보 또는 착용자의 의도를 보다 정확하게 추론하는 데 필요한 다른 중요한 센서 모달리티가 부족하다. 자기중심적 비디오에서 빠른 헤드 모션의 특성으로 인해 모든 비디오에서 3D 정보를 정확하고 안정적으로 추정하는 것은 여전히 매우 어려운 연구 문제이다. Ego4D 비디오의 작은 하위 집합은 사전 스캔된 3D 환경을 사용하여 재위치화에 의존한다. EpicFields[27]은 COLMAP[22]와 신경장을 이용하여 3차원 환경과 궤적의 일부를 복원하였다. AEA 데이터세트에서는 최첨단 시각관성 오도메트리(VIO)와 SLAM(simultaneous location and mapping) 시스템을 사용하여 프로젝트 아리아 장치의 기본 능력에 의해 생성된 모든 레코딩에 대해 자기중심 관측기의 연속 6DoF 궤적에 근사하는 6DoF 1KHz 궤적을 제공한다. AEA는 또한 모든 레코딩에 대해 보정된 눈 시선 정보를 포함한다. 기존 데이터세트 GTEA[14], EGTEa[13] 및 Ego4D 데이터세트의 작은 부분집합은 시선 정보를 포함하지만 다른 3D 정보는 포함하지 않아 3D 환경에서 착용자의 의도를 맥락화할 때 장애물을 생성할 수 있다.\n' +
      '\n' +
      '프로젝트 아리아 데이터세트:AEA는 아리아 파일럿 데이터세트의 에브리데이 액티비티 시퀀스를 기반으로 업데이트된 데이터세트이다[17]. 아리아 파일럿 데이터 세트가 출시된 이후, 다른 기능과 작업을 특징으로 하는 프로젝트 아리아 장치를 사용하여 더 많은 데이터 세트가 기록되었다. 아리아 디지털 트윈(ADT) [19]는 정확한 객체와 착용자 포즈로 완전히 디지털화된 두 환경에서 아리아 착용자가 캡처한 실제 활동을 포함한다. 모든 아리아 레코딩에 대해, ADT는 동일한 원시 센서 정보를 포함하고, 디지털화된 장면과 정렬된 모션 캡처 시스템(MCS)으로부터 렌더링되는 추가적인 그라운드 트루스를 포함한다. 또한 데이터 세트에는 MCS를 활성화하기 위한 일부 시각적 아티팩트가 있다. 이러한 데이터 세트를 만들려면 확장하기 어려울 수 있는 광범위한 노력과 리소스가 필요합니다.\n' +
      '\n' +
      '대조적으로, AEA는 현장에서 다른 요구 사항이 없는 5개의 다양한 완전히 동의된 환경에서 기록되었다. 다중 일상 활동을 기록하고 프로젝트 아리아의 기계 인식 서비스(MPS)를 사용하여 파생된 기계 인식 데이터를 획득했다. AEA는 광범위한 자기 중심 데이터 캡처를 필요로 하는 연구를 위해 쉽게 확장될 수 있는 프로젝트 아리아 레코딩의 예시 역할을 한다.\n' +
      '\n' +
      'Ego-Exo4D 데이터세트[10]의 최근 노력은 프로젝트 아리아를 자기중심 기록장치로 사용하는 가장 큰 멀티모달 자기중심 데이터세트를 특징으로 한다. 1400시간 내에 축적된 자기중심적 장치와 자기중심적 장치를 모두 포함하고 있습니다. 데이터세트는 또한 MPS를 사용하여 기록당 기계 인식 데이터를 생성한다. AEA와 Ego-Exo4d의 주요 차이점은 고려 중인 활동의 유형과 환경에 있다. Ego-Exo4d는 절차적 활동에 초점을 맞추고, AEA는 가정 위치에서 하루 종일 종단적 일상 활동에 초점을 맞춘다. AEA에서는 모든 카메라로부터 관측 가능한 제한된 3차 공간에서의 외심 캡쳐와 관련하여 다인칭 동기화된 자기중심 캡쳐와 한 집에 정렬된 다궤적 캡쳐를 제공하는 반면, Ego-Exo4d는 자기중심 비디오 간에 동기화된 캡쳐를 제공한다. 우리는 AEA가 자연 3D 환경과 장거리 시공간 정렬된 비디오를 필요로 하는 AI 연구를 위한 중요한 파일럿 자기 중심 데이터 세트 역할을 할 수 있다고 믿는다.\n' +
      '\n' +
      '다른 3D 멀티모달 데이터세트: 최근에는 더 번거로운 폼 팩터로 다른 헤드 마운트 장치를 사용하여 기록된 몇 가지 다른 데이터세트가 도입되었다. Assembly101[23]은 메타 오큘러스 퀘스트 장치를 사용하며, 멀티뷰 데스크탑 환경 하에서 일련의 조립 활동을 운용한다. 홀로애시스트[28]는 홀로렌즈 헤드셋을 사용하여 여러 개의 두 사람 간의 협업 작업을 특징으로 한다. 이러한 데이터 세트에 비해 AEA는 눈에 띄지 않는 프로젝트 아리아 폼 팩터의 이점과 함께 실내 위치에서 더 자연스러운 인간 활동을 특징으로 한다.\n' +
      '\n' +
      '멀티모달 연구 연구는 또한 매우 다른 시나리오와 센서 양식을 제공하는 자율 주행 데이터 세트[3, 8, 24]를 광범위하게 사용한다. 개인 AI에서 멀티모달 학습의 미래는 미래의 AR 안경과 유사한 기기 형태 인자를 사용하여 수집된 데이터 세트와 일상 생활에서 인간의 활동과 밀접하게 유사한 데이터 세트를 필요로 할 것이다. 우리는 AEA가 이 도메인에서 연구를 추진하기 위한 모범 데이터 세트 역할을 할 것이라고 믿는다.\n' +
      '\n' +
      '##3 데이터세트 설명\n' +
      '\n' +
      '아리아 에브리데이 액티비티(AEA) 데이터 세트는 아리아 파일럿 데이터세트(APD) 내의 에브리데이 액티비티 시퀀스의 업데이트된 버전이다. APD는 프로젝트 아리아 장치를 사용하여 수집된 최초의 공개 데이터 세트였다. 이것은 공간-시간적으로 정렬된 메타데이터가 있는 항상 켜져 있는 폼 팩터 안경에 의해 캡처된 모든 일 활동 기록과 함께 4D 세로 데이터 세트를 소개한다. 새로운 AEA 데이터셋의 경우, 모든 녹음에 대해 가장 최근의 MPS(Machine Perception Services)를 사용하여 모든 기계 인식 데이터를 업데이트하여 가능한 최고의 3D 정보를 제공하였다. 우리는 아래 AEA 데이터 세트의 세부 사항을 설명한다.\n' +
      '\n' +
      '데이터셋 통계:AEA는 5개의 실내 위치에서 여러 착용자가 수집한 일상 활동 기록 143개를 포함한다. 총 100만 프레임 이상으로 구성되어 있으며 누적 녹화 시간은 7.3시간이다. RGB 컬러 이미지와 흑백 장면 이미지를 설명하는 이미지 프레임은 200만 개가 넘는다. 탭 도 1은 각 위치에서의 착용자 수, 레코딩 및 프레임 번호에 대한 데이터 통계를 도시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Location & Recording Scripts & Wearers & Recordings & Image Frames & RGB Frames & Total Duration (hours) \\\\ \\hline Location 1 & Scripts 1-5 & 1-2 Wearers & 29 & 230,487 & 115,235 & 1.6 \\\\ Location 2 & Scripts 1-5 & 1-2 Wearers & 43 & 339,650 & 169,824 & 2.3 \\\\ Location 3 & Scripts 1-5 & 1-2 Wearers & 38 & 259,027 & 129,514 & 1.7 \\\\ Location 4 & Scripts 1-5 & 1-2 Wearers & 19 & 94,029 & 47,015 & 0.6 \\\\ Location 5 & Scripts 4-5 & 1 Wearer & 14 & 168,428 & 84,214 & 1.1 \\\\ \\hline In total & Scripts 1-5 & 1-2 Wearers & 143 & 1,091,621 & 545,802 & 7.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 각 위치에서의 녹음에 대한 데이터 통계량. 녹음 스크립트에 대한 자세한 내용은 부록 C를 참조하십시오. 전체 이미지 프레임의 수는 모든 RGB 이미지 프레임과 흑백 장면 카메라 프레임의 조합을 설명한다.\n' +
      '\n' +
      '그림 2: 프로젝트 아리아 장치의 구성 요소에 대한 개요이며, 각각의 예시적인 센서 구성을 가지고 있다. 이 데이터셋에서는 WiFi, Bluetooth & GNSS를 제외한 장치의 모든 센서를 포함하는 Profile9를 사용한다. 오른쪽에 있는 녹음에 있는 모든 센서의 스냅샷을 시각화합니다.\n' +
      '\n' +
      '데이터 집합 수집 프로세스: 1~2명의 프로젝트 아리아 장치 착용자에 대해 상시 감지로 관찰할 수 있는 하루 종일 종단 활동을 나타낼 수 있도록 착용자를 위한 안내 스크립트를 만들었다. 각 스크립트에는 사람들이 하루를 보내는 이야기를 들려주는 여러 시나리오가 포함되어 있습니다. 대본은 즉흥적인 시나리오에 대한 일반적인 안내만 제공했다. 착용자들은 제공된 프롬프트를 따랐고 각각의 녹음에서 그들에게 가장 자연스러웠던 것을 따랐다. 스크립트는 자기중심적 활동에 대한 개방형 어휘 설명으로 사용될 수 있다. 대본 시나리오 중 일부는 요리, 일일 청소, 운동, 식사, 독서, 게임, 친구들과 시간을 보내는 것입니다. 우리는 스크립트가 부록 C의 특정 작업에 어떻게 대응하는지에 대한 세부 정보를 제공한다.\n' +
      '\n' +
      '아리아 원시 센서 데이터: 프로젝트 아리아 데이터 수집 구성에서 제공하는 _recording Profile 9_를 사용하였으며, \\(1408\\times 1408\\) 해상도에서 20 fps RGB 카메라, \\(640\\times 480\\) 해상도에서 10 fps 흑백 장면 카메라, \\(320\\times 240\\) 해상도에서 10 fps 시선 추적 카메라 및 기타 모든 센서를 기본 설정으로 구성하였다. 원시 센서 설명에 대한 자세한 내용은 부록 A를 참조하십시오.\n' +
      '\n' +
      '기계 인식 데이터: 이것은 아리아 파일럿 데이터 세트의 기록과 비교하여 AEA 데이터 세트의 주요 기능 업데이트이다. AEA는 프로젝트 아리아 학술 파트너에게 제공되는 현재 기계 인식 서비스(MPS)를 사용한다. 데이터 형식은 프로젝트 아리아 툴스에서 사용되는 새로운 공식 프로젝트 아리아 데이터 형식 관습, 새로운 오픈 소스 툴링 및 문서와 일치하도록 업데이트되었습니다.\n' +
      '\n' +
      '그림 3: 5개 위치마다 다중 녹음 활동을 위한 공유된 3D 전역 폐루프 궤적과 반밀도 포인트 클라우드의 시각화. 각 색상은 이 위치에 기록된 한 시퀀스의 고주파 궤적을 나타낸다. 각 위치에서 점 구름은 모든 기록에서 집계됩니다. 위치 3과 5는 다층 시나리오를 강조하기 위해 옆으로 표시된다.\n' +
      '\n' +
      '옵션 리포지토리. 아리아 파일럿 데이터세트와 비교하여 MPS 데이터에 대한 두 가지 주요 업데이트가 있으며,\n' +
      '\n' +
      '***3D 위치**: 각 레코딩에 대해 가시적인 글로벌 포인트 클라우드와 정렬된 6DoF 궤적을 제공했으며, 이는 동일한 위치의 다른 모든 레코딩과 동일한 좌표계를 공유한다. 전역 포인트 클라우드는 환경의 정적 부분의 추적된 특징점을 사용하는 재구성이다. 우리는 MPS에서 3D 포인트 클라우드를 _semi-dense point_라고 부르고 2D 추적된 포인트를 _semi-dense observations_라고 부른다. 초기 아리아 파일럿 데이터셋에서 각 기록에 대해 1Khz 궤적만 제공했다. 이 업데이트에서 우리는 전역 좌표 공간에서 각 궤적에 정렬된 반밀집점을 추가했으며, 이를 _close-loop 궤적_이라고 한다. 도. 도 3은 각 위치에서의 모든 공간 정렬된 기록물의 시각화를 도시한다. 우리는 궤도와 포인트 클라우드가 정확한지 확인하기 위해 전역 정렬된 좌표의 모든 기록에 대해 수동으로 검사했다.\n' +
      '3D 방향 벡터를 갖는 ***눈 시선**: AEA는 계산된 3D 눈 시선을 중앙 동공 프레임에 고정된 프레임당 3D 광선으로서 제공한다. 이는 RGB 카메라에서 2D 재투사 시점의 초기 릴리즈에 비해 3D 환경에서 더 많은 정보를 제공한다. 또한 디바이스 캘리브레이션을 이용하여 2D 시선 획득을 위한 코드 예제를 프로젝트 아리아 툴에 제공하였다.\n' +
      '\n' +
      '우리는 또한 온-디바이스 오도메트리(on-device odometry)를 제공했는데, 이것은 우리가 _open-loop trajectory_와 온라인 디바이스 캘리브레이션이라고 부른다. 파일럿 데이터 세트를 공개할 때 이 두 가지 기능을 모두 사용할 수 없었다. 기계 인식 데이터와 MPS에 대한 자세한 내용은 부록 B를 참조하시기 바랍니다.\n' +
      '\n' +
      '시간 동기화:AEA는 두 사람이 대화를 하는 것과 같은 여러 개의 다중 사람 활동을 포함한다. 다인 시나리오에서 동기화된 타임스탬프로 녹음을 캡처했습니다. 서로 근접하여 동작하는 다수의 프로젝트 아리아 디바이스(\\(<100m\\))는 SMPTE LTC 타임코드를 레버리지하여 서브 밀리초 정확도로 동기화된 타임 클럭을 수신할 수 있다. VRS 파일에서는 각 장치의 캡처 시간 외에도 다중 장치 캡처에 걸쳐 공유되는 동기화된 타임코드 타임스탬프를 제공했다. 동시에 캡처된 상이한 기록들은 디바이스 타임코드를 사용하여 연관될 수 있다. 또한 동기화된 녹음을 시각화하기 위한 툴킷을 제공하며, 이는 Sec. 4에서 설명할 것이다.\n' +
      '\n' +
      '음성 2 텍스트 전사: 우리는 APD에서 출시된 자동 음성 인식(ASR)에 의해 생성된 동일한 텍스트 전사를 사용했다. 텍스트의 모든 문자는 디바이스 타임스탬프와 정렬되고, 각각의 출력은 또한 신뢰 등급과 연관된다. ASR 주석은 프로젝트 아리아 MPS의 일부가 아닌 독점 서비스를 사용했다. [21, 4]와 같은 오픈 소스 ASR 솔루션을 통해 유사한 결과를 얻을 수 있다. 공개된 음성 전사가 시간 정렬된 텍스트 입력을 필요로 하는 연구 및 응용 프로그램을 가속화할 수 있기를 바란다.\n' +
      '\n' +
      '개인 정보 보호 약속: 우리는 모든 데이터 수집 및 처리에 대한 메타의 책임 있는 혁신 원칙을 엄격하게 준수합니다. 모든 녹음은 개인 식별 정보가 없는 완전히 동의된 실내 환경에서 캡처되었다. 가장 높은 익명화 품질을 보장하기 위해 RGB 및 흑백 장면 카메라 스트림에서 모든 이미지의 모든 얼굴 및 라이선스에 수동으로 주석을 달고 흐리게 했다. 우리는 여러 라운드의 품질 분석으로 주석 결과를 주의 깊게 확인했다. 품질 검사를 통과하지 못한 프레임으로 플래그된 녹화는 데이터 세트에서 완전히 제외되었다. 그림 4는 RGB 및 흑백 센서 스트림에 대한 익명화의 예를 보여준다.\n' +
      '\n' +
      '##4 데이터세트 도구\n' +
      '\n' +
      '이 데이터 세트의 릴리스를 지원하기 위해 아리아 데이터 작업을 위한 오픈 소스 데이터 유틸리티를 제공하는 프로젝트 아리아 도구를 업데이트했다. 이러한 업데이트는 데이터 제공자들이 다수의 디바이스들로부터 멀티모달 감각 데이터 및 그들의 페어링된 머신 지각 데이터를 소비하도록 제공한다. 모든 데이터 모달리티 및 메타데이터는 그들의 디바이스 _timestamp_에 의해 검색될 수 있다. 1인 또는 다인 활동을 쉽게 조작할 수 있습니다. 프로젝트 아리아 툴은 또한 다양한 센서들 사이의 2D/3D 포인트 프로젝션 및 재투영뿐만 아니라 6DoF 포즈 변환을 조작하기 위해 디바이스 및 카메라 캘리브레이션을 사용하기 위한 편리한 API들을 제공한다.\n' +
      '\n' +
      '프로젝트 아리아 툴은 시각화 도구를 제공하여 사용자가 시간적 스크러빙으로 각 시퀀스 또는 다인 시나리오를 빠르게 탐색할 수 있도록 지원한다. 도. 도 5는 일 예를 도시한 도면\n' +
      '\n' +
      '도 4: RGB 컬러 비디오(오른쪽) 및 두 개의 흑백 장면 카메라 비디오(왼쪽 두 이미지에 크롭됨) 모두에서 모든 인간 얼굴을 수동으로 흐리게 했다.\n' +
      '\n' +
      '3D 궤적, 글로벌 포인트 클라우드 환경 및 시선 벡터 시각화와 함께 두 개의 시간 동기화된 녹화를 하나의 공유 좌표에서 모두 시각화한다. 또한 장치 시간에 따라 질의되는 다른 시간 데이터의 예로 음성에서 동기화된 텍스트를 표시한다. 이것은 개발 및 디버깅을 위한 추가 사용자 메타데이터 또는 더 많은 레코딩을 디스플레이하기 위해 쉽게 확장될 수 있다.\n' +
      '\n' +
      '## 5 Applications\n' +
      '\n' +
      '이 섹션에서 AEA 데이터 세트의 두 가지 주요 연구 응용 프로그램인 3D 신경 장면 재구성 및 신속한 분할을 제공한다. 이러한 예제 응용 프로그램은 AEA 다중 모드 자기 중심 관찰 및 기계 인식 데이터를 사용하기 위한 강력한 사용 사례를 보여준다. 우리는 그들이 이 데이터 세트를 사용하여 연구를 엿볼 수 있기를 바랍니다.\n' +
      '\n' +
      '##### 3D 신경 장면 재구성\n' +
      '\n' +
      '관찰 가능한 영역에 대한 장면을 재구성하는 것은 AR/VR 애플리케이션에서 몰입형 사진 사실적 가상 메모리를 생성하기 위한 중요한 문제 중 하나이다. 기존의 신경 복원 방법들은 대개 2단계 접근법이 필요하다. 하나의 비디오 시퀀스가 주어지면, 그들은 COLMAP[22]와 같은 SfM(structure-from-motion) 툴로부터 6DoF 카메라 궤적을 획득하고, 이어서 포즈된 이미지들에 대해 신경 재구성을 실행한다. 이것은 레코딩이 SfM을 실행할 목적으로 주의 깊게 촬영되고 큐레이션되지 않는 자기중심적 활동에 도전적일 수 있다. 빠른 머리 움직임 또는 덜 관찰 가능한 영역은 쉽게 SfM 또는 유사한 국소화 알고리즘의 실패로 이어질 수 있으며 추가 재구성에 실패할 수 있다. 동일한 환경에서 여러 개의 긴 기록의 경우 공유 좌표계에서 모든 관찰을 공동으로 국소화하는 것도 어려울 수 있다.\n' +
      '\n' +
      'AEA 데이터 세트는 활동 중심적이라는 점에 주목할 필요가 있다. 모든 녹음은 다양한 자기중심적 동작을 포함하고 있으며, 다인칭 활동에서 움직이는 인간을 관찰할 수도 있다. 모든 궤적은 객체 또는 정적 장면 지향 재구성 목적으로 만들어진 전통적인 데이터 세트와 크게 다른 자유 시점 모션이다. 이러한 어려움에도 불구하고, 우리는 하나 또는 다수의 녹음에 걸쳐 지속되는 장면을 성공적으로 재구성할 수 있음을 입증한다. 동적인 움직임이 있을 때 재구성 품질을 향상시키거나 완전히 몰입하는 환경에서 활동 및 장면 이해도를 향상시킬 수 있는 향후 연구를 고무할 수 있기를 바란다.\n' +
      '\n' +
      '도 5: 동기화된 정류된 RGB 스트림을 그들의 디바이스 궤적으로 보여주는 AEA 데이터세트 뷰어(복수의 사람 활동)의 스냅샷(각각의 암적색 및 녹색), 눈 시선 벡터 방향(각각의 디바이스 절두체로부터의 적색 벡터), RGB 이미지 상의 투영된 눈 시선(각각의 이미지 내의 적색 도트), 각각의 기록(백색)에 대한 집성된 3D 반-밀도 포인트 클라우드 및 전사된 스피치 문장(각각의 RGB 이미지 상에 중첩됨)을 도시한다. 프로젝트 영역 도구에서 이 뷰어를 제공합니다.\n' +
      '\n' +
      '이 섹션에서는 실행 중인 신경 재구성을 시연하고 AEA 데이터 세트, 특히 폐루프 궤적 및 반밀도 포인트 클라우드의 3D 기계 인식 데이터를 사용하여 앞서 언급한 문제를 해결한다. 우리는 가우시안 스플래팅 [11]을 기본 베이스라인으로 사용하고 완전한 평가를 제공한다.\n' +
      '\n' +
      '1.1 가우스 스플래팅#### 5.1.1 가우스 스플래팅\n' +
      '\n' +
      '구현: 다중 시퀀스 재구성을 지원하기 위해 몇 가지 사소한 변경을 제외하고는 가우스 스플래팅의 공식 구현을 따랐다. 메모리에 직접 로드할 수 없는 긴 궤적을 재구성했기 때문에 런타임에 프레임 배치를 무작위로 찾기 위해 배치 처리를 사용했다. 프로젝트 아리아 툴을 사용하여 기본 핀홀 래스터라이저의 요구 사항에 맞는 모든 RGB 이미지를 보정했습니다. AEA 반밀집점을 이용하여 포인트 클라우드를 초기화하고, 고주파 폐루프 궤적으로부터 가장 가까운 타임스탬프를 갖는 포즈를 이용하여 프레임당 포즈를 획득하였다. 다중 기록 재구성을 위해 우리는 모든 프레임과 포인트 클라우드를 함께 연결한다. 단일 기록 재구성을 위해 30K 반복을 실행하고 다중 서열 재구성을 위해 100K 반복을 실행했다. 모든 재구성은 가지치기 및 치밀화를 위한 동일한 하이퍼 매개 변수와 전략을 공유했다. 단일 A100 80GB GPU에서 단일 레코딩을 재구성하는 데는 한 공유 위치에서 여러 레코딩에 대해 약 3시간, 10시간이 소요되었다.\n' +
      '\n' +
      '단일 기록 결과: 이 방법은 높은 동적 모션 또는 최소 헤드 모션이 있는 일부 기록에도 불구하고 각 기록에서 개별적으로 성공적으로 수렴되었다. 도. 도 6은 3D 공간에서 가장 가까운 뷰를 갖는 몇몇 렌더링 예들을 도시한다. 결과에서, 우리는 잘 관찰된 모든 정적 영역에 대해 양호한 품질 재구성을 보았다. 가우시안으로부터 렌더링된 흐릿한 영역은 원래 관찰에서 동적 또는 관찰되지 않은 영역을 나타낸다.\n' +
      '\n' +
      '다중 기록 결과: 각 위치에서 집계된 모든 기록에서 가우스 스플래팅을 추가로 실행했다. 기록 및 프레임의 수는 탭.1에서 찾을 수 있다. 예를 들어, 위치 2는 총 길이가 2.3h이고, 170K RGB 프레임에 가깝고, 반-밀집 포인트 클라우드로부터 집계된 5M 초기 포인트로 축적된 가장 긴 기록을 포함한다. 100K 반복 재구성 후, 위치 2는 약 627K 가우시안 포인트로 수렴하였다. 도. 도 7은 몇 개의 렌더링된 뷰를 도시한다. 한 위치에 관찰이 풍부하기 때문에 잘 관찰된 생활 영역을 재구성하고 각 위치에 동적 움직임이 특이치로 있는 이미지 영역을 제거할 수 있다.\n' +
      '\n' +
      '정량적 평가:탭. 도 2는 단일 기록(SR) 및 다중 기록(MR) 설정에서, 각 위치에 대해 평가된 정량적 재구성 품질을 도시한다. 우리는 모든 보류된 테스트 프레임에 대해 평균 PSNR 수를 보고했다. 테스트 프레임은 5개의 연속 프레임 중 마지막 프레임이었다.\n' +
      '\n' +
      'NeRFstudio를 이용한 3차원 재구성\n' +
      '\n' +
      'NeRFstudio[25]는 MPS에서 제공하는 궤적과 전역 포인트 클라우드를 이용하여 아리아 원시 데이터를 처리하기 위한 아리아 맞춤형 통합을 제공한다. NeRF 기준선, 예를 들어 NeRFacto는 아리아 폐루프 궤적 및 원시 RGB 이미지를 입력으로 사용한다. Gaussian-splatting 기준선(일명 Splatfacto)은 또한 3D 가우시안들을 초기화하고 우리가 증명한 바와 같이 RGB 이미지들을 교정하기 위해 반-밀집 포인트 클라우드 데이터를 사용한다. AEA에 대한 최상의 결과를 달성하기 위해 기록의 길이를 수용하기 위해 현재 구현의 일부 조정이 필요했다. 우리는 프로젝트 아리아 데이터를 이용한 장면 재구성의 오픈 소스 구현을 지속적으로 지원할 것이며, 향후 작업에서 NeRFstudio에서 베이스라인으로 AEA 데이터셋을 재구성할 계획이다.\n' +
      '\n' +
      '### Prompted Segmentation\n' +
      '\n' +
      '3D 환경에서 객체 인스턴스를 인식하고 분할하는 기능은 상황별 AI 응용 프로그램의 기본 구성 요소 중 하나이다. 최근 2D 기반 모델의 발전은 CLIP[20], DINO[2], SAM[12]와 같이 일반화가 잘 된 제로샷 AI 기능을 가능하게 하는 유망한 경로를 보여준다. 이 섹션에서는 여러 센서 모달리티가 AI 연구에 도움이 될 수 있음을 보여주기 위해 세그먼테이션 모든 모델(SAM)을 예로 사용한다. SAM의 변형인 EfficientSAM[29]은 RGB 프레임마다 실행할 때 정확도와 실시간 성능 사이의 균형을 잘 맞춰주기 때문에 사용한다.\n' +
      '\n' +
      '이 예에서는 기계 인식 데이터에서 두 가지 다른 프롬프트로 EfficientSAM을 연결하는 두 가지 응용 프로그램을 보여준다. 먼저, 눈의 시선을 착용자의 의도의 프롬프트로 사용하여 객체를 분할하는 방법을 보여준다. 그런 다음 EfficientSAM을 GroundingDino[16]와 연결하여 음성 접지 분할을 시연한다. 상기 ap\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Location & PSNR (SR) & PSNR (MR) \\\\ \\hline Location 1 & 25.79 & 22.85 \\\\ Location 2 & 25.19 & 21.93 \\\\ Location 3 & 24.85 & 22.06 \\\\ Location 4 & 23.70 & 21.83 \\\\ Location 5 & 26.27 & 24.18 \\\\ Locations all & 25.12 & 22.35 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 보유된 테스팅 프레임들(5개 프레임들 중 마지막 프레임)에 대한 가우시안-스플래팅의 정량적 평가. 단일 녹화(SR) 또는 다중 녹화(MR) 시나리오에서 모든 프레임에 대해 평균화된 PSNR을 보고한다.\n' +
      '\n' +
      '도 6: 단일 기록**로부터의 관찰 **을 사용하여 가우시안-스플래팅 재구성(상부) 및 그들의 대응하는 최근접 이웃 뷰들(하부)의 새로운 렌더링 뷰들.\n' +
      '\n' +
      '도 7: 공유 위치**에서의 모든 기록으로부터의 관찰 **을 사용하여 가우시안-스플래팅 재구성의 새로운 렌더링 뷰. 대응하는 가장 가까운 이웃 뷰가 두 번째 행에 표시됩니다.\n' +
      '\n' +
      '복제본은 프로젝트 아리아 툴에서 구현 예로서 제공된다.\n' +
      '\n' +
      '눈 응시 프롬프트된 세그멘테이션:그림 7(a)는 하나의 RGB 이미지에서 투영된 눈 응시를 프롬프트로서 사용하는 세그멘테이션의 시각화를 도시한다. 눈 시선 방향 벡터와 이 벡터를 따라 가상 깊이가 주어지면, 디바이스 캘리브레이션을 사용하여 2D RGB 이미지 스트림에 투영한 다음, 2D 재투영된 눈 시선을 위치 프롬프트로서 사용하여 EfficientSAM을 갖는 분할 마스크를 생성하였다. 이 예에서는 가상 깊이를 1.0미터로 설정합니다. 향후 작업은 보다 정확한 눈 시선 깊이 추정기를 통합함으로써 신속한 정확도를 향상시킬 수 있다.\n' +
      '\n' +
      '음성 접지 분할:그림 7(b)는 음성에서 언급된 물체의 시각적 접지를 보여준다. 시간 정렬된 음성 문장이 주어지면 먼저 GroundingDino[16]를 사용하여 텍스트를 이미지와 정렬하고 문장 내의 객체를 감지한다. 객체가 존재하는 경우, 대응하는 바운딩 박스가 박스 프롬프트로서 사용되었고 EfficientSAM[29]은 그 안에서 세그먼트화 마스크를 생성하였다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '연구 커뮤니티를 위한 아리아 에브리데이 액티비티(AEA) 데이터셋(Dataset)을 소개하고, 시공간적 정렬 맥락을 갖는 현실 세계의 개인 종단적 활동을 통해 멀티모달 AI 연구를 탐구한다. 이 데이터 세트는 아리아 파일럿 데이터 세트의 에브리데이 액티비티 시퀀스의 업데이트된 버전이다. 우리는 프로젝트 아리아 팀에서 제공하는 가장 최근의 기계 인식 서비스를 사용하여 이전 릴리스를 업데이트했으며 기계 인식 데이터에 의해 활성화된 몇 가지 연구 애플리케이션을 시연했다. 또한 오픈 소스 프로젝트 아리아 툴을 이 데이터 세트와 호환되도록 업데이트했으며 몇 가지 연구 응용 사례를 제공했다. 우리는 AEA가 연구자들이 항상 멀티모달 상황 AI 연구를 탐구할 수 있도록 하는 유용한 자원으로 작용할 수 있다고 믿는다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '아리아 에브리데이 활동 데이터 세트는 메타 리얼리티 랩 연구소의 많은 기여자들의 집단적 노력이다. 데이터 세트의 성공은 프로젝트 아리아 팀 전체와 기계 인식 서비스의 모든 작업과 성과에 달려 있다. 이러한 노력을 가능하게 해주신 모든 팀원들과 파트너들께 진심으로 감사드립니다. 또한 데이터 수집에 참여하고 이 데이터 세트에 대한 품질 분석을 수행하는 데이비드 부이, 토마스 소아레스, 마이클 라우돈 및 매덜린 보웬에게 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Siddhant Bansal, Chetan Arora, and C.V. Jawahar. My view is the best view: Procedure learning from egocentric videos. In _European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [3] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argoverse: 3d tracking and forecasting with rich maps, 2019.\n' +
      '* [4] Seamless Communication, Loic Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Dupenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Hahaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotorison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hananti, Ela Kalbassi, Amanda Kallet, Artym Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussa, Maha Elbayad, Hongyu Gong, Francisco Guzman, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Pelquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, and Mary Williamson. Seamless: Multilingual expressive and streaming speech translation, 2023.\n' +
      '* [5] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In _European Conference on Computer Vision (ECCV)_, 2018.\n' +
      '\n' +
      '도 8: 착용자가 보고 있거나 이야기하고 있는 것을 강조하기 위해 멀티모달 프롬프트된 세그먼트화를 입증한다. 눈 시선 투영은 (a)의 EfficientSAM[29]에 대한 포인트 프롬프트로서 사용되고 스피치는 (b)의 EfficientSAM을 사용하여 객체를 검출하고 이들을 추가로 세그먼트아웃하기 위해 GroundingDino[16]를 프롬프트하는 데 사용된다.\n' +
      '\n' +
      '* [6] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. _International Journal of Computer Vision (IJCV)_, 130:33-55, 2022.\n' +
      '* [7] Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talatof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera, Ji Luo, Jing Dong, Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington, Sachin Kulkarni, Sagar Migliani, Santosh Gondi, Saransh Solanki, Sean Diener, Shangyi Cheng, Simon Green, Steve Saarinen, Suvam Patra, Tasso Mourikis, Thomas Whelan, Tripti Singh, Vasileios Balntas, Vijay Baiyya, Wilson Dreewes, Xiaqing Pan, Yang Lou, Yipu Zhao, Yusuf Mansour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan, Carl Ren, Renzo De Nardi, and Richard Newcombe. Project aria: A new tool for egocentric multi-modal ai research, 2023.\n' +
      '* [8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. _International Journal of Robotics Research (IJRR)_, 2013.\n' +
      '* [9] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeg Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruy Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreeslaise, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Yamsi Krishna Intapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video, 2022.\n' +
      '* [10] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu- Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Crishtian Foriguba, Abrham Gebreeslaise, Sanjay Haresh, Jing Huang, Md Mohaimimul Islam, Suygo Jain, Rawal Khirokar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatuninin Uguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chen Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, and Michael Wray. Ego4d: Understanding skilled human activity from first- and third-person perspectives, 2023.\n' +
      '* [12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), July 2023.\n' +
      '* [13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.\n' +
      '* [14] Yin Li, Miao Liu, and James M. Rehg. In the eye of beholder: Joint learning of gaze and actions in first person video. In _Proceedings of the European Conference on Computer Vision (ECCV)_, September 2018.\n' +
      '* [15] Yin Li, Zhefan Ye, and James M Rehg. Delving into egocentric actions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 287-295, 2015.\n' +
      '* [16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.\n' +
      '* [17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.\n' +
      '* [18] Zhaoyang Lv, Edward Miller, Jeff Meissner, Luis Pesqueira, Chris Sweeney, Jing Dong, Lingni Ma, Pratik Patel, Pierre Moulon, Kiran Somasundaram, Omkar Parkhi, Yuyang Zou, Nikhil Raina, Steve Saarinen, Yusuf M Mansour, Po-Kang Huang, Zijian Wang, Anton Troynikov, Raul Mur Artal, Daniel DeTone, Daniel Barnes, Elizabeth Argall, AndreyLobanovskiy, David Jaeyun Kim, Philippe Bouttefroy, Julian Straub, Jakob Julian Engel, Prince Gupta, Mingfei Yan, Renzo De Nardi, and Richard Newcombe. Aria pilot dataset. [https://about.facebook.com/realitylabs/projectaria/datasets](https://about.facebook.com/realitylabs/projectaria/datasets), 2022.\n' +
      '* [18] OpenAI, ;. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Ballestcu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Breding, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madeline Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eleundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fullford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Lukasz Rondraciuk, Andrew Konfach, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishu Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovi, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reitichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parasandolo, Joel Parish, Emy Paraparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Issca Shieph, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntaing Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023.\n' +
      '* [10] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Carl Yuheng Ren. Aria digital twin: A new benchmark dataset for egocentric 3d machine perception, 2023.\n' +
      '* [11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\n' +
      '* [12] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In _International Conference on Machine Learning_, pages 28492-28518. PMLR, 2023.\n' +
      '* [13] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [14] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao. Assembly101: A large-scale multi-view video dataset for understanding procedural activities. _CVPR 2022_, 2022.\n' +
      '* [15] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.\n' +
      '* [16] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: A modular framework for neural radiance field development. In _ACM SIGGRAPH 2023 Conference Proceedings_, SIGGRAPH \'23, 2023.\n' +
      '* [26] Gemini Team. Gemini: A family of highly capable multimodal models, 2023.\n' +
      '* [27] Vadim Tschernezki, Ahmad Darkhail, Zhifan Zhu, David Fouhey, Iro Larina, Diane Larlus, Dima Damen, and Andrea Vedaldi. EPIC Fields: Marrying 3D Geometry and Video Understanding. In _Proceedings of the Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [28] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, Neel Joshi, and Marc Pollefeys. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 20270-20281, October 2023.\n' +
      '* [29] Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, and Vikas Chandra. Efficientsam: Leveraged masked image pre-training for efficient segment anything. 2023.\n' +
      '\n' +
      '## 부록 A 프로젝트 아리아 원시 센서 데이터 기술\n' +
      '\n' +
      '프로젝트 아리아 디바이스들은 사용자에 의해 선택적으로 런타임에 구성될 수 있는 자기중심 멀티모달 데이터를 기록하는 다양한 센서들을 통합한다. 프로젝트 아리아 디바이스들에 대한 보다 포괄적인 설명을 위해, 독자들은 프로젝트 아리아 백서 [7]의 디바이스 설명을 참조할 수 있다.\n' +
      '\n' +
      '일상적인 시나리오에서 멀티모달 연구에 완전히 권한을 부여하기 위해 실내 환경에 적합한 풍부한 센서 세트를 제공하는 녹음 구성을 선택했다. 원시 데이터를 생성하는 데 사용된 센서는 다음과 같다.\n' +
      '\n' +
      'RGB 카메라: 이것은 수평 시야(HFoV)를 갖는 롤링 셔틀 카메라이다. 우리는 1408x1408 픽셀의 해상도로 20fps에서 RGB 스트림을 캡처했다.\n' +
      '\n' +
      '단색 장면/SLAM 카메라: 이러한 카메라는 안경의 좌우측에 배치된 \\(150^{\\circ}\\)HFoV를 갖는 글로벌 셔터 흑백 카메라이다. 이 두 카메라는 주로 시각적 SLAM에 유용하며 손 추적에도 도움이 될 수 있다. 각 흑백 비디오 스트림을 640x480 픽셀의 해상도로 10fps로 기록했다.\n' +
      '\n' +
      '눈 추적 카메라: 눈 추적 목적으로 사용되는 IR 조명이 있는 두 개의 흑백 내향 카메라가 있습니다. 그들은 320x240 픽셀의 해상도와 80^{\\circ}\\ 대각선 시야(DFoV)의 해상도를 가지고 10 fps로 기록되었다.\n' +
      '\n' +
      'IMU: 안경의 각 측면에는 두 개의 관성 측정 장치(IMU)가 있다. 좌측 IMU는 4g(가속도계)와 \\(500^{\\circ}/s\\)의 포화 한계로 800 Hz에서 구동된다. 우측 IMU는 8g(가속기) 및 \\(1000^{\\circ}/s\\)의 포화 한계를 갖는 1000 Hz에서 샘플을 샘플링한다.\n' +
      '\n' +
      '마이크: 안경에 7개의 마이크가 있고, 5개는 전면 플러스 1개입니다. 우리는 48kHz의 샘플 속도를 사용하여 7개의 채널로 공간 오디오를 캡처했다.\n' +
      '\n' +
      '다른 센서: 자력계는 10 Hz의 샘플 속도에서 \\(0.1T\\)의 해상도로 주변 자기장을 측정했다. 기압계 센서는 50 Hz의 샘플 속도에서 각각 \\(0.66Pa\\) 및 \\(0.005C\\)의 해상도에서 국소 기압과 온도를 캡처했다. 녹음 중에는 GPS나 WiFi 센서를 켜지 않았습니다.\n' +
      '\n' +
      '## 부록 B 기계 인식 서비스(MPS)\n' +
      '\n' +
      '원시 센서 데이터 외에도 센서에서 파생된 신뢰할 수 있는 3D 및 시선 정보를 필요로 하는 다운스트림 애플리케이션에 더 많은 권한을 부여하는 다양한 기계 인식 기능을 제공했다. 기존 기성 오픈소스 솔루션에 비해 정확도 및 견고성이 우수한 프로젝트 아리아에서 제공하는 머신 인텔리전스 서비스(MPS: Machine Perception Services)를 이용하여 위치 및 시선 정보를 획득한다. 음성 전사를 위해 사내 음성 인식 모델을 사용했다. 기계 인식 서비스에 대한 자세한 내용은 [7]을 참조하십시오.\n' +
      '\n' +
      '개루프 궤적: 각 기록에 대해 실시간 오도메트리 추정에서 고주파(1kHz) 6DoF 궤적을 제공했다. 이 궤적은 온-디바이스 6DoF 포즈 정보를 필요로 하는 임의의 실시간 애플리케이션을 모방하는데 사용하기에 적합하다.\n' +
      '\n' +
      '클로즈-루프 궤적: 이것은 또한 고주파(1 kHz) 6 DoF 궤적이지만, 후처리를 통해 획득된다. 개루프 궤적과 달리 이 궤적은 동일한 위치에서 기록된 다른 모든 시퀀스로부터 동일한 참조 프레임을 공유한다. 이를 통해 여러 착용자가 동일한 위치에서 여러 활동을 함으로써 서로 다른 기록의 모든 궤적을 정렬할 수 있다.\n' +
      '\n' +
      '준밀점:클로즈 루프 궤적 외에도 환경의 정적 부분을 부분적으로 재구성한 동일한 전역 참조 프레임을 사용하여 준밀도의 트랙과 포인트 클라우드를 제공했다. 도. 도 3은 하루 동안 동일한 위치에서 한 사용자에 의해 수행되는 모든 상이한 활동으로부터의 폐루프 궤적과 함께 시각화된 포인트 클라우드를 도시한다.\n' +
      '\n' +
      '시선 추적: 프로젝트 아리아 장치의 시선 추적 카메라에서 계산된 3D 시선을 제공했습니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c} \\hline \\hline\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>