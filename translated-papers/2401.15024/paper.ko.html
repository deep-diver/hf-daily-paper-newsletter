<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '트래블러즈 및 콜라보팅에 의한 컨슈어 모델.\n' +
      '\n' +
      'Saleh Ashkboos\n' +
      '\n' +
      'ETH Zurich\n' +
      '\n' +
      'saleh.ashkboos@inf.ethz.ch\n' +
      '\n' +
      '&맥시밀리아 L. CrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCrociCCrociCCrociCCrociCCrociCCrociCCrociCCrociCCCCrociCrociCCrociCCrociCCCrociCCCCCCrociCCCCCCCCCCCCCCCCCCCCCCCCCCrociCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      't-mcroci@microsoft.com\n' +
      '\n' +
      '나스투미투, 마르셀로 제나리.\n' +
      '\n' +
      'Microsoft\n' +
      '\n' +
      'marceloge@microsoft.com\n' +
      '\n' +
      '&Torsten Hoefler\n' +
      '\n' +
      'ETH Zurich\n' +
      '\n' +
      'torsten.hoefler@inf.ethz.ch\n' +
      '\n' +
      '&James Hensman\n' +
      '\n' +
      'Microsoft Research\n' +
      '\n' +
      'jameshensman@microsoft.com\n' +
      '\n' +
      '컨소시엄에서 인턴으로 완성된 작업입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델은 자연어 처리의 초석이 되었지만, 그들의 사용은 계산 및 메모리 자원 측면에서 상당한 비용을 제공한다. 파생화는 이러한 자원 제약을 완화하기 위한 솔루션을 제공하며, 최근의 연구는 훈련된 모델이 사후적으로 유출될 수 있음을 보여주었다. 기존의 유출 기법은 추가적인 데이터 구조가 필요하고 현재 하드웨어로 제약된 속도를 제공하기 때문에 도전에 직면한다. 본 논문에서는 각 가중치 행렬을 더 작은(센스) 매트릭스로 대체하는 새로운 사후 학습 유출 방식인 슬리스GPT를 제시하여 네트워크의 임베딩 차원을 감소시킨다. 광범위한 실험을 통해 슬리스GPT가 울창한 모델의 99%, 99%, 90% 제로 샷 태스크 성능을 유지하면서 램마-2 70B, OPT 66B 및 Phi-2 모델에 대한 모델 매개변수( 임베딩 포함)의 최대 25%를 제거할 수 있음을 보여준다. 당사의 슬라이스된 모델은 더 적은 GPU에서 실행되고 추가 코드 최적화 없이 더 빠르게 실행되며, 24GB 소비자 GPU에서 우리는 라마-2 70B에 대한 추론을 위한 총 계산량을 조밀한 모델의 64%로 감소시키며, 40GB A100 GPU에서 66%로 감소시킨다. 슬리스GPT를 가능하게 하는 변압기 네트워크에서 새로운 통찰력, 계산 불변성을 제공하며, 사전 훈련된 모델에 대한 메모리 및 계산 요구를 줄일 수 있는 미래를 고취하고 가능하게 하기를 바랍니다. 코드는 [https://github.com/마이크로소프트/트랜스포머 억제](https://github.com/마이크로소프트/트랜스포머 압축)에서 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 수십억 개의 매개변수를 가진 신경망으로 토큰의 트릴라이온(Zhao et al., 2023)에 대해 훈련되었다. LLM을 훈련하는 비용은 다중 과제인 _foundation 모델_ 패러다임에 대해 사전 훈련된 모델을 재활용하기 위한 전환을 야기했다. LLM의 크기는 미리 학습된 모델을 배치하는 데 비용이 많이 드는 작업을 한다. 많은 모델은 예측을 계산할 수 있도록 여러 개의 GPU를 필요로 하며, 모델이 자기회귀적이기 때문에 텍스트 응답을 생성하기 위해 뉴럴 네트워크의 다중 전방 통과가 필요하다. 따라서 일반적으로 _m모델 압축_이라고 하는 훈련 후 기술을 통해 수행된 이러한 모델의 계산 요구 사항을 줄이는 것은 광범위한 관심이다.\n' +
      '\n' +
      '모형 압축 기술의 대부분은 증류, 텐서 분해(저 순위 요인화), 프루닝 및 양자화(호플러 et al, 2021; 글라미 et al, 2021, Zhu et al, 2023; Gupta 및 Agrawal, 2021)의 네 가지 범주 중 하나에 속한다. 이 작업에서 우리는 우리의 방법론이 다른 영역에 대한 미래 작업에 영향을 미칠 수 있기를 바랍니다. 경우에 따라 프루닝 방법이 오래 전부터 있었지만, 많은 접근 방식은 성능을 유지하기 위해 프루닝 후 회수 미세 조정(RFT)을 요구하여 전반적인 과정이 비싸고 어려운 작업으로 만든다. 슬리스GPT를 사용하면 단 몇 시간 만에 단일 GPU를 사용하여 대형 모델을 압축하고 RFT가 없어도 생성 및 하류 작업에 대한 경쟁 성능을 유지한다.\n' +
      '\n' +
      '피팅 방법은 가중치 행렬의 일부 요소를 LLM에서 0으로 설정하고, (선택적으로) 매트릭스의 주변 요소를 업데이트하여 보상함으로써 작동한다. 그 결과는 신경망의 전방 패스에서 요구되는 매트릭스 곱셈에서 일부 부동 포인트 연산을 스킵할 수 있다는 것을 의미하는 희박한 패턴이다. 동작의 상대적인 속도는 유출 수준 및 유출 패턴에 따라 달라지며, 보다 구조화된 유출은 더 많은 계산 이득과 관련이 있다. 다른 프루닝 방법과 달리 슬리스GPT는 웨이트 매트릭스의 전체 행 또는 열들(오프오프)을 제거한다. 슬라이싱 전에 예측이 불변하는 네트워크를 단일 변환하지만 슬라이싱이 작은 효과만 가질 수 있도록 한다.\n' +
      '\n' +
      '그 결과는 가중치 행렬이 더 작으며, 신경망의 블록들 사이에 통과하는 신호들도 작다는 것인데, 우리는 신경망의 _벨딩 차원_을 감소시킨다.\n' +
      '\n' +
      '그림 1은 우리의 접근법과 기존 유출 방법을 비교한다. 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. _ 컴퓨터 불변_의 개념을 소개하고, 우리는 모델을 변경하지 않고 변압기에서 각 가중치 행렬에 직교 행렬 변환을 적용할 수 있음을 보여준다.\n' +
      '2. 우리는 이것을 사용하여 변압기 아키텍처에서 각 블록을 편집하여 블록 간의 신호 행렬1을 자체 주성분에 투사하고 있다. 모델 크기를 줄이기 위해 변환된 가중치 매트릭스의 열 또는 행을 제거합니다. 우리는 가중치 슬리스GPT의 변형 및 제거라고 부른다. 발주 1: 신호 행렬을 활성화 행렬로 지칭하기도 한다.\n' +
      '3. 우리는 OPT(Zhang et al., 2022), Llama-2(Touvron et al., 2023) 및 LLM에 대한 다중 실험을 수행하여 SliceGPT가 미술 2:4 방식의 상태로 우수한 퍼플렉시로 이러한 모델을 최대 30% 압축할 수 있음을 보여준다. 하류 작업에서도 Phi-2를 실험하고 조밀한 성능의 90% 이상을 유지하면서 모든 모델을 최대 30%까지 슬라이스할 수 있음을 보여준다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '이 섹션에서는 먼저 변압기 아키텍처에 대한 몇 가지 필요한 배경을 설명하며, 이는 우리가 주요 결과를 입증하기 위해 사용할 표기법을 도입할 수 있다. 그런 다음 이러한 아키텍처를 압축하기 위한 유출에 관한 관련 작업을 설명한다.\n' +
      '\n' +
      '그림 1: 신호 \\(\\mathbf{X}\\)의 Matrix 곱셈과 다양한 유형의 유출도에서 가중치 매트릭스 \\(\\mathbf{W}\\)를 보여준다. **Left**: 비구조화된 sparsity는 \\(\\mathbf{W}\\)의 일부 요소가 0이고 \\(\\mathbf{X}\\)는 조밀하다. ***중간**: 2:4 구조화된 스파시트로 4개의 가중치 매트릭스 엔트리 각각의 블록에는 2개의 제로가 포함되어 있으며 \\(\\mathbf{X}\\)는 조밀하다. ** 우**: 슬리스GPT는 형질전환 \\(\\mathbf{Q}\\)를 도입한 후, 모든 가락이 \\(\\mathbf{W}\\)의 하단 행으로 배열되고, 이에 상응하는 \\ 컬럼(\\mathbf{X}\\)이 제거된다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Transformer Networks\n' +
      '\n' +
      '트랜스포머 네트워크(Vaswani et al, 2017)는 언어 모델링을 포함한 광범위한 작업에서 효과적인 것으로 나타난 신경 네트워크 부류이다. 변압기 아키텍처는 일련의 층으로 구성되어 있으며, 각각은 멀티 헤드 자기 의도 블록과 피드 포워드 네트워크 블록으로 구성된다. 각 블록 사이에는 플레이어 정규(Ba et al., 2016)(또는 RMS 정상(Zhang and Sennrich, 2019) 블록이 있다. 그림 2는 변압기 네트워크의 일부분, 즉 Layer 정규 블록을 통해 FFN 블록에 연결된 주의 블록은 잔차 연결을 나타낸다. 다음은 각 구성요소의 동작(사후 학습을 적용하지 않는 중도 탈락 정렬)을 설명한다.\n' +
      '\n' +
      '배아(D\\)는 변압기의 임베딩 차원, \\(N\\)는 시퀀스 길이이다. 변압기 모델은 토큰 ID와 위치 ID의 시퀀스를 입력함에 따라 취하여 임베딩 행렬을 인덱싱하여 형태 \\(N\\tcer D\\)로 초기 신호 \\(\\mathbf{X}\\)를 생성한다. 이하에서는 일반성의 손실 없이 입력 서열 \\(\\mathbf{W}_{\\text{embd}}\\)에 의해 인덱싱된 단일 임베딩 매트릭스 \\(\\mathbf{s}\\)를 고려한다.\n' +
      '\n' +
      '일반적으로 임베딩 후, 신호 행렬은 행렬의 각 행에서 평균을 차감하고 행을 표준 편차로 나누고(열별), 리칼레스(열별) 오프셋을 추가하는 Layer 정규 연산을 통과한다. 우리는 LayerNorm 블록을 그대로 작성합니다.\n' +
      '\n' +
      '}(\\math{f{})\n' +
      '\n' +
      'H\\(\\text{RMSNorm}(\\mathbf{X})는 2\\(\\mathbf{x}\\mathbf{x}\\lelearrow\\mathbf{x}/\\|\\mathbf{x}\\|\\)를 \\(\\mathbf{X}\\)의 각 행에 적용한다. 벡터 매개변수 \\(\\mathbf{\\alpha}\\) 및 오프셋(벡터) 매개변수 \\(\\mathbf{\\beta}\\)는 각 플레이어 정규 사례에서 독립적으로 학습된다. 상시적인 매트릭스 \\(\\mathbf{M}=\\mathbf{I}-\\frac{1}{D}\\mathbf{1}\\mathbf{1}^{\\top}\\)는 \\(D\\tcer D\\) 매트릭스로 \\(\\mathbf{X}\\)의 각 행에서 평균을 차감한다.\n' +
      '\n' +
      '부프트 2: 일부 구현예에서, RMS 정규 블록은 스케일 파라미터들을 포함할 수 있다. 우리는 이것들을 정상들의 특별한 사례들로 간주하고 이에 따라 처리합니다.\n' +
      '\n' +
      '시청 블록에는 네 가지 행렬이 있는데, \\(\\mathbf{W}_{k},\\mathbf{W}_{q},\\mathbf{W}_{v}_{v}\\)와 \\(\\mathbf{W}_{o}_{o}\\)의 4가지 행렬이 있다. 블록에 들어가는 입력 신호는 키(\\(\\mathbf{X}\\mathbf{W}_{k}\\)), 퀀리(\\(\\mathbf{X}\\mathbf{W}_{q}\\) 및 밸류(\\(\\mathbf{f{W}_{q}\\)로 투영되어 다중 _헤드_로 분할된다. 신호들이 결합되기 전에 각 머리에 비선형 동작이 인가되고 출력 가중치 행렬 \\(\\mathbf{W}_{o}\\)에 곱된다. 첫 번째 세 가지 가중치 행렬이 입력들에 별도로 적용되므로, 우리는 그것들을 연결하여 단일 행렬 곱셈(그림 2의 이들 행렬 주변의 화이트 박스에 의해 표시)을 수행할 수 있다. 우리는 이러한 매트릭스의 연결된 것을 단일 선형 층으로 고려할 수 있으며, 이는 \\(\\mathbf{W}_{\\text{in}\\)를 나타낸다. 우리는 또한 출력 행렬을 \\(\\mathbf{W}_{\\text{out}\\)로 지칭한다. 우리는 주의 블록을 \\(\\sathbf{X}\\mathbf{W}_{\\text{in}}+\\mathbf{b}}_{\\text{b}}})\\mathbf{W}_{\\text {out}}+\\mathbf{b}_{\\text{b}_{\\text{b}_{\\text{b}}_{\\text{b}_{\\text{b}}_{\\text{b}}_{\\text{b}_{\\text{b}}}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}_{\\text{b}}}_{\\text{b}_{\\text{b}_{\\text{b}\n' +
      '\n' +
      '기준 3: 여기 및 이 논문 전체에 표기법을 쉽게 하기 위해 표기법을 약간 악용하고 시퀀스 길이 차원을 가로질러 편향 용어의 방송을 생략한다. 피처 블록 운영을 위한 완전한 표기법은\\(Imathbf{X}_{\\text{in}+\\mathbf{b}_{\\text{in}_{\\text{top})이다.\n' +
      '\n' +
      'FFN블록은 변압기 아키텍처에 나타나는 다른 유형의 블록이 피드 포워드 네트워크(FFN) 블록이다. 이는 선형층 \\(\\mathbf{W}_{1}\\)으로 구성된 다중층 페셉트론(MLP)으로, 그 뒤를 이어 두 번째 선형층 \\(\\mathbf{X}\\mathbf{W}_{1}_{2}_{2}\\mathbf{b}_{2}\\)으로 구성된 다층 페셉트론이다. 일부 아키텍처는 추가 매트릭스가 사용되는 게이팅 포맷을 채택했으며, 작동은 \\(}\\mathbf{X}\\mathbf{W}_{1}+\\mathbf{b}_{1})\\ 회로(\\mathbf{W}_{3}}\\mathbf{W}_{3}\\\\)}\\mathbf{W}_{3}\\\\)}\\-wise 생성물이다. 우리는 주의 모듈에서 처음 세 개의 선형 레이어와 유사하게 \\(\\mathbf{W}_{1}\\)와 \\(\\mathbf{W}_{2}\\)의 연결된 것을 단일 선형 동작으로 고려하고, 이를 \\(\\mathbf{W}_{\\text{in}}\\)로 나타낼 수 있다. 따라서 우리는 MLP 또는 게이팅된 FFN 층의 작동을 \\(\\sigma(\\mathbf{X}\\mathbf{W}_{\\text{in}})\\mathbf{W}_{\\text{out}})로 나타낼 수 있으며, 여기서 \\(\\sigma\\)는 주의사항과 다른 의미를 갖는다.\n' +
      '\n' +
      '본 논문에서 슬리스GPT를 적용한 언어 모형화(LM) 네트워크들은 모두 (라드포드 등, 2018)에 따른 디코더 전용 구조를 가지고 있으며, 교류 주의력 및 FFN 블록을 적용한 후, 트레이닝 및 배치에 대한 토큰 예측 동안 손실을 계산하는 데 사용되는 헤드 블록 컴파일 로그츠이다. 헤드 동작은 \\(\\mathbf{X}\\mathbf{f{W}_{\\text{헤드}}+\\mathbf{b}_{\\text{헤드}}}})로, 여기서 \\(\\mathbf{X}\\)는 마지막 변압기 블록의 출력이다.\n' +
      '\n' +
      '모델이 트레이닝되고 모든 파라미터들이 설정되면, 예측들을 생성하기 위해 변압기 네트워크에서 요구되는 계산들은 헤드 노드가 도달될 때까지 하나의 블록으로부터 다음 블록으로 신호 행렬들을 통과하는 것을 포함한다. 우리는 형식 \\(\\mathbf{W}_{\\text{in}}+\\mathbf{bf{b}}_{\\text{b}})의 FFN 및 주의 블록을 정의할 수 있기 때문에,\\(\\sigma\\)가 지점별 또는 다중-의도 비선형성을 나타내는 것으로 이해할 수 있다.\n' +
      '\n' +
      '### Related work\n' +
      '\n' +
      '가장 간단한 설정에서 크기 기반 유출을 사용할 수 있으며, 이는 모델에서 가장 작은 가중치를 0으로 설정(한 et al, 2016; Zhu 및 Gupta, 2017; Gale et al., 2019)할 수 있다. 크기 유출은 확장성이 있지만 LLM에 대한 적용은 성능(Frantar and Alistarh, 2023)에서 너무 강한 저하를 제공한다. 보다 정교한 방법인 최적의 뇌수건(OBS)(Hassibi et al., 1993; LeCun et al., 1989)은 손실 기능에 가장 큰 영향을 미치는 가중치를 체계적으로 제거한다. 이 방법은 헤센 매트릭스의 역을 사용하여 분사되지 않은 가중치를 업데이트함으로써 가중치 제거로 도입된 오차를 보상한다. 불행히도 OBS는 헤센 매트릭스의 역수를 계산하고 저장할 필요성으로 인해 몇 백만 개의 매개변수를 가진 모델에 대해 비실용적이다. OBS가 제기하는 계산 제한을 해결하기 위해 최근 연구는 우드피셔(싱히 및 알리스타, 2020)와 같은 헤센 매트릭스의 역수를 근사화하거나 계층별 프루닝으로 알려진 OBC, 프랑타르 및 알리스타, 2022년 알리스타르(OBC, Alistarh)와 같은 각 계층에 별도로 적용하는 두 가지 접근법을 조사했다. 이러한 기술은 중형 네트워크에 효과적인 것으로 입증되었지만 개별 계층 가중치 행렬이 일반적으로 \\(10^{8}\\) 매개변수를 포함하는 큰 언어 모델에 대해서는 실용적이지 않다.\n' +
      '\n' +
      'GPTQ(Frantar et al., 2022)는 컬럼별 방식을 사용하여 LLM의 가중치 매트릭스를 정량화하고 다음 열에서 모든 비열계수 가중치를 업데이트함으로써 이 문제를 해결했다. SparseGPT(Frantar and Alistarh, 2023)는 비정형 및 반구조화된 프루닝을 사용하여 LLM을 프루닝하고 스파싱하는 것과 Hessian의 대각선만을 사용하여 아이디어를 단순화했다. 비구조화된 프루닝(Mishra et al, 2021)을 통한 엔드 투 엔드 속도 개선 달성은 까다로운 과제이기 때문에 2:4 및 4:8(Mishra et al., 2021)과 같은 반구조화된 패턴으로 유출을 유도하는 유사한 기법도 시도했다. 그러나 이러한 구조를 구현하는 것은 모델의 정확도를 유지하지 못한다.\n' +
      '\n' +
      '그림 2: 변압기 네트워크에서 단일 레이어를 보여준다. 네트워크의 이전 블록에서 발생하는 신호(입력)는 그림 하단에 도착하여 주의력, 플레이어 정규, FFN을 통과하였다. 관점과 FFN 블록 모두 텍스트에서 \\(\\mathbf{W}_{\\text{in}},\\mathbf{W}_{\\text{out}}\\)로 나타내는 입력 및 출력 선형 동작(\\(\\mathbf{blue}\\))을 갖는다. Layer 정규 \\(\\mathbf{\\bar{M}}\\) 및 \\(\\mathbf{diag}(\\boldsymbol{\\alpha})의 선형 작업이 강조된다. 이 수치와 후속 수치는 편향을 나타내지 않는다.\n' +
      '\n' +
      '압축에 대한 또 다른 접근법은 낮은 순위 근사치이며, 각 가중치 매트릭스가 더 작은 내부 차원으로 두 매트릭스의 생성물로 대체되며 일반적으로 미세 조정 단계(Hu et al, 2021, 마하바디 et al., 2021, 노흐 및 골드버그, 2020, 투칸 et al. 2020)가 뒤따른다. 압축을 달성하기 위해서는 내부 차원이 원래 차원의 절반보다 작아야 한다. 대조적으로, 우리의 방법은 각 가중치 매트릭스를 단일 더 작은 매트릭스로 대체하여 미세 조정 없이 임베딩 차원을 감소시킨다.\n' +
      '\n' +
      '우리는 컨템넷 문헌에서 필터와 채널의 프루닝과 유사한 무게 행렬의 행과 열을 삭제할 것을 제안한다. 거기에는 회분식 요인(Liu et al, 2017), 또는 네트워크 구조(Huang and Wang, 2018)에 유출 유도 규칙화가 추가되고, 네트워크가 훈련되거나 미세 조정되어 네트워크의 채널이나 부분의 프루닝이 발생한다. 아마도 우리와 가장 유사한 방법은 ThiNet(Luo et al, 2017; He et al, 2017)이며, 이는 계층들 간의 선형 동작들을 적용(우리가 가질 것)하며, 규칙화로 더 미세 조정으로 인터리빙된다. 이 문헌에서 모델 크기는 보통 LLM보다 몇 배 작은 크기이며, 예를 들어 VGG16 네트워크는 138M 매개변수를 가지고 있으며, 이는 우리가 고려하는 가장 작은 OPT 모델과 비슷하다. LLM의 거대한 크기는 특히 정규화 매개변수를 선택하기 위해 외부 루프가 필요할 때 광범위한 미세 조정되지 않는 방법을 만든다.\n' +
      '\n' +
      '최근 LLM에 구조화된 프루닝을 적용한 일부 작품이 제안되고, 이어서 손실된 공연을 회복하기 위한 지속적인 훈련(또는 미세 조정)이 이루어졌다. 예를 들어, LLM-pruner(Ma et al., 2023)는 추가 훈련 전에 LLM에서 연결된 구조를 제거한다. 우리의 작업과 동시적으로, LLM 수리건(반데르 오우데라라 등, 2023)은 프루닝과 함께 미세 조정 회복을 회복한다. 우리는 슬리스GPT에 대한 결과를 단일 샷 방법 및 사후 복원 복구 미세 조정으로 제공한다.\n' +
      '\n' +
      '## 3 SliceGPT\n' +
      '\n' +
      '당사의 슬리스GPT 방법은 변압기 아키텍처에 내재된 계산 불변성에 의존한다. 이를 통해, 우리는 하나의 구성요소의 출력에 직교 변환을 적용할 수 있다는 것을 의미하므로, 다음 부분에서는 그렇지 않은 한이다. 우리의 핵심 통찰력은 네트워크의 블록들 사이에서 수행되는 RMS 정규 동작이 변형: 작업 출퇴근에 영향을 미치지 않는다는 것이다. 이 섹션에서는 먼저 RMS 정규 연결된 변압기 네트워크에서 불변성이 어떻게 발생하는지 설명하고, 이후 Layer 정규 연결로 훈련된 네트워크가 RMS 정규으로 변환될 수 있는 방법에 주목한다. 다음으로, 블록 간의 신호가 주성분에 투영되도록 주성분 분석(PCA)을 사용하여 각 레이어에서 변환을 계산하는 방법을 설명한다. 마지막으로, 우리는 작은 주성분들을 삭제하는 것이 변형된 네트워크의 슬라이싱 행들 또는 열들에 어떻게 대응하는지를 설명한다.\n' +
      '\n' +
      '변압기 네트워크에서 컴퓨팅 불변.\n' +
      '\n' +
      '\\(\\mathbf{Q}\\)는 직교 매트릭스를 나타내며, 우리는 \\(\\mathbf{Q}^{\\top}\\mathbf{Q}\\mathbf{Q}=\\mathbf{Q}\\mathbf{Q}^mathbf{Q}^{\\top}=\\mathbf{I}=\\mathbf{I}\\)를 가지고 있다. \\(\\|\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\)는 벡터 \\(\\sqrt{\\bf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\ing}\\mathbf{Q}\\ing}\\mathbf{Q}\\ing}\\mathbf{Q}\\mathbf{Q}\\ing}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\f{Q}\\ing}\\mathbf{Q}\\d{Q}\\ 이 작업에서 \\(\\mathbf{Q}\\)의 치수는 항상 변압기 \\(D\\)의 임베딩 차원과 일치하게 된다.\n' +
      '\n' +
      'r\\(\\mathbf{X}_{\\ell}\\)가 RMS 정규 처리된 변압기의 하나의 블록의 출력인 다음, 후속 블록에 \\(\\text{RMSNorm}(\\mathbf{X}_{\\ell})로 입력했다고 가정하자. 직교 행렬 \\(\\text{RMSNorm}\\) 전과 \\(\\text{RMSQ{Q}\\)로 선형 층을 삽입하면(\\text{RMS 정규}\\) 후에 네트워크(\\mathbf{Q}^\\}\\) 네트워크가 변하지 않고 있으며, 이는 신호 행렬의 각 행이 \\(\\mathbf{Q}\\)에 곱되어 정규화되고 \\(\\mathbf{Q}^{Q}\\)를 곱하기 때문에 네트워크에는 변하지 않고 있다. 우리는 가지고 있습니다.\n' +
      '\n' +
      '\\[\\text{RMSNorm}(\\mathbf{X}_{\\ell}_{\\mathbf{Q}_{\\ell}:{\\bf{Q})\\mathbf{Q}}=\\text{RMSNorm}( \\mathbf{X}_{\\ell})\\,\\{RMSNorm}=\\text{RMSNorm}(\\mathbf{X}_{\\}:\\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{RMSNorm} \\{Rf{X} < <{f{X}_{\\x{X}:{f{X}_{\\:{f{X}_{\\:{f{X}_{\\:{f{X}_{\\s{X}_\n' +
      '\n' +
      '이 관계에 대한 증거는 부록 A.1에서 나타나는데, 네트워크의 각 주의 또는 FFN 블록은 입력 및 출력 모두에 선형 동작이 있기 때문에 추가 동작 \\(\\mathbf{Q}\\)를 블록의 선형 층으로 흡수할 수 있다. 네트워크에는 잔차 연결이 포함되어 있기 때문에 \\(\\mathbf{Q}\\)도 모든 이전 레이어의 출력(모든 방식으로 임베딩)과 모든 후속 레이어(LM 헤드까지의 모든 방식)에 적용해야 한다.\n' +
      '\n' +
      '상기 _invariant_ 함수는 상기 입력에 대한 변환이 상기 출력으로의 변화를 초래하지 않는 것이다. 우리의 경우, 결과를 변경하지 않고 변압기의 가중치에 직교 변환 \\(\\mathbf{Q}\\)를 적용할 수 있으므로, 어떤 형질전환 상태에서든 _computation_을 수행할 수 있다. 이를 _ 컴퓨터 불변_이라고 하며, 다음과 같은 정리로 정의한다.\n' +
      '\n' +
      'r\\(\\math{b}}{\\b}}{\\{b}}{\\f}}{\\d}}{\\ell}) 및 \\(\\math{f}}{\\d}}{\\d}}\\)의 선형 레이어의 가중치 매트릭스가 될 수 있다. \\(\\mathbf{Q}\\)는 치수 \\(D\\)의 직교 행렬이 된다. 그렇다면 다음 네트워크는 원래 변압기 네트워크___에 해당한다.\n' +
      '\n' +
      '}\\mathbf{f}}_{\\mathbf{f}}}\\math{f{f}}\\,^math{f}}}\\[\\math{f{f}}} <\\tath{{f}}} <\\math{f{f}}} <\\math{f{f}} <\\{f{f{f{f}} <\\{\\]{\\,\\:\\{\\]}} <\\{f{f{f{f{f{f{f{f{f{f{f}}}}\\[\\math{f{f{f{f{f{f{f{f{f{\\,\\)\\,\\]}}}}}}}}}<\\:\\:\\－\\.\n' +
      '\n' +
      '입력 및 머리 편향_는 \\(\\tild{\\mathbf{b}}_{\\text{b}}}=^{\\ell}=\\mathbf{b}_{\\text{b}_{\\text{b}}_{\\text{b}}_{\\text{b}})로 복제되어 있다.\n' +
      '\n' +
      '2번 라인에서 일반적으로 2개의 네트워크 일치(\\text{RMSC{mathbf{X}})의 작동을 계산한다는 것을 알 수 있다. 4호선 위에 비선형성(\\tild{\\mathbf{X}}\\tild{\\mathbf{f{W}}_{\\text{f}}} =^{{\\ell}=\\mathbf{f}_{\\text{in}_{\\text{in}}}}}})을 사용하여\\(\\tilde{\\mathbf{f{f{f{f{f{f})를 적용하며, 이는 수학식 4를 사용하여 \\(\\tilde{\\mathbf{\\ell}:\\mathbf{\\ell})를 사용한다. 잔류 연결 5라인에서 우리는 \\((CStild{\\mathbf{X}}+\\tathbf{mathbf{Z}}) =(\\mathbf{Z}+\\mathbf{Z})\\mathbf{Q}\\)를 가지며, RMS 정규 결과를 적용하여 \\(\\tilde{\\mathbf{X}=\\mathbf{X}=\\mathbf{X}=\\tathbf{X} <\\tathbf{mathbf{X} <\\tathbf{mathbf{X} <\\mathbf{X} <\\tathbf{X} <\\mathbf{Q <\\mathbf{X}=\\mathbf{X}=\\mathbf{X}=\\mathbf{X})를 할당한다. 이는 루프의 끝까지 이어진다. 마지막으로, 라인 7에서 형질전환은 식 7을 사용하여 \\(\\mathbf{XW}_{\\text{헤드}}=\\tilde{\\mathbf{X}}\\tilde{\\mathbf{W}}_{\\text{헤드}}})로 미톤이다.\n' +
      '\n' +
      '평균 변환기는 RMS 정상으로 전환될 수 있다.\n' +
      '\n' +
      '변압기 네트워크의 계산 불변성은 RMS 정규 연결 네트워크에만 적용된다. Layer 정규자를 작업하기 전에 Layer 정규의 선형 블록을 인접한 블록으로 흡수하여 네트워크를 RMS 정규으로 변환한다. 그림 3은 변압기 네트워크상의 이러한 변형을 보여준다(그림 2 참조). 각 블록에서 출력 매트릭스 \\(\\mathbf{W}_{\\text{out}}\\)를 평균 하위 분획 매트릭스 \\(\\mathbf{M}\\)에 의해 곱하여 후속 Layer 정규에서 발생할 평균 뺄셈을 설명한다. 입력 행렬 \\(\\mathbf{W}_{\\text{in}}\\)는 선행 레이어 정규 블록의 스케일에 의해 미리 추출된다. 임베딩 매트릭스 \\(\\mathbf{W}_{\\text{embd}}\\)는 평균 추출되어야 하며, \\(\\mathbf{W}_{\\text{헤드}}}\\)는 마지막 부분 척도로 재측정되어야 한다. 이는 동작 순서의 단순한 변화로서 네트워크 출력에 영향을 미치지 않는다.\n' +
      '\n' +
      '블록당 A 변환.\n' +
      '\n' +
      '이제 변압기의 모든 계층이 RMS 정규으로 전환되었기 때문에 모델을 수정하기 위해 모든 \\(\\mathbf{Q}\\)를 선택할 수 있다. 우리의 초기 계획은 모델에서 신호를 수집하고 이러한 신호를 사용하여 직교 행렬을 구성하고 네트워크의 일부를 삭제하는 것이었다. 네트워크의 다른 블록에서의 신호가 정렬되지 않고 각 블록, \\(\\mathbf{Q}_{\\ell}\\)에서 서로 다른 직교 행렬을 적용해야 함을 빠르게 보았다.\n' +
      '\n' +
      '각 블록에서 사용되는 직교 행렬을 다르게 허용하는 것은 모델을 남기는 것으로 보일 수 있다.\n' +
      '\n' +
      '그림 3: Layer 정규에서 RMS 정규으로의 변압기 네트워크를 변환한다: 스케일 매트릭스 \\(\\text{diag}(\\mathbf{\\alpha})는 후속 매트릭스 \\(\\overline{\\mathbf{W}_{\\text{in}}}})에 흡수된다. 그림은 결합된 색상의 블록을 보여준다. 우리는 불균일을 위해 \\((\\mathbf{\\alpha})\\)를 사용한다. 평균 하위 추출 매트릭스 \\(차등선{\\mathbf{M}}\\)는 각 매트릭스 \\(\\overline{\\mathbf{W}_{\\text{out}}\\)에 적용된다. 레이어 정규은 RMS 정규, 최대 일정한 \\(\\sqrt{D}\\)가 된다(미도시). 여기서 스케일링 \\((\\mathbf{\\alpha}^{\\prime})\\)는 이전 블록으로부터 나온다.\n' +
      '\n' +
      '정리 1과 동일한 증명을 이용하여 소명하였다.\n' +
      '\n' +
      '알고리즘 1의 5호선을 제외하고, 우리는 블록의 잔차 연결과 출력이 동일한 회전을 가져야 함을 알 수 있다. 이를 해결하기 위해 선형 형질전환 \\(\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}\\mathbf{Q}_{\\ell}\\\\ell}\\)를 잔차에 적용하여 잔류 연결을 수정한다. 그림 4는 잔류 연결에서 추가 선형 작동으로 서로 다른 블록에 서로 다른 회전들을 적용할 수 있는 방법을 보여준다. 가중치 행렬에 대한 수정과 달리 이러한 추가 작업은 미리 계산될 수 없으며 모델에 작은(D\\tcer D\\)) 오버헤드를 추가한다. 그럼에도 불구하고, 모델(섹션 3.4)을 슬라이싱할 수 있도록 하기 위해서는 필요하다(섹션 4). 전반적으로 실제 속도를 확인할 수 있다.\n' +
      '\n' +
      '행렬 \\(\\mathbf{Q}_{\\ell}\\)를 계산하기 위해 PCA를 사용한다. 트레이닝 세트로부터 보정 데이터 세트를 선택하고, 모델을 통해 실행(Layer 정규 연산을 RMS 정규으로 변환 후)하고, 레이어의 직교 행렬을 추출한다. 우리는 변환된 네트워크의 출력을 사용하여 다음 층의 직교 행렬을 계산한다. 좀 더 정확하게,\\(\\mathbf{X}_{\\text{th}}\\)가 보정 데이터셋에서 \\(\\mathbf{X}_{\\ell,i}\\)에 대한 RMS 정규 블록의 출력이라면(\\ell^{\\text{{\\text{th}}\\)을 계산한다.\n' +
      '\n' +
      '\\[\\mathbf{C}_{\\ell}=\\sum_{i}\\mathbf{X}_{\\ell,i}^{\\top}\\mathbf{X}_{\\ell,i} \\tag{8}\\]\n' +
      '\n' +
      'r\\(\\mathbf{Q}_{\\ell}\\)는 고유값을 감소시켜 분류된 \\(\\mathbf{C}_{\\ell}\\)의 고유 벡터이다.\n' +
      '\n' +
      '### Slicing\n' +
      '\n' +
      '구성 요소 분석의 목표는 보통 데이터 매트릭스 \\(\\mathbf{X}\\)를 취하고 하위 차원 표현 \\(\\mathbf{Z}\\)를 구성하며 대략적인 재구성 \\(\\tilde{\\mathbf{X}\\)을 계산하는 것이다.\n' +
      '\n' +
      '\\[\\mathbf{Z}=\\mathbf{X}\\mathbf{Q}\\mathbf{Q}\\mathbf{D}\\,\\qquad\\mathbf{D}\\,\\qquad\\tilde{\\mathbf{D}} =\\mathbf{D}\\{9}\\mathbf{D}\\.\n' +
      '\n' +
      '\\(\\mathbf{Q}\\)는 \\(\\mathbf{\\top}\\mathbf{f{X}\\\\)의 고유 벡터로서, \\(D\\tcer D_{\\text{small}}\\) 삭제 매트릭스(D\\(D\\)의 삭제 매트릭스(D\\(D_{\\text{small}\\) 열 중 일부를 왼쪽으로 제거하는 \\(D_{\\ill{\\:D\\) 결실 매트릭스 함유 \\(D\\, D_{\\) 결실 매트릭스)인 \\(D\\, D_{\\) 결실 매트릭스(D\\, D_{\\) 결실 매트릭스(D\\(D\\, D_{\\) 결실 매트릭스 함유 \\(D\\, D_{\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\, D\\) 말단 D\\, D\\, D\\) 재건축은 \\(\\mathbf{Q}\\mathbf{D}\\)가 \\(\\|\\\\bf{X}-\\|mathbf{X}}\\|^{{2}\\)를 최소화하는 선형 매핑이라는 의미에서 \\(L_{2}\\) 최적이다.\n' +
      '\n' +
      '블록들 사이에 신호 매트릭스 \\(\\mathbf{X}\\)에 PCA를 적용할 때, 우리는 \\(N\\tcer D\\) 신호 매트릭스를 재료화하지 않지만, 우리는 삭제 매트릭스 \\(\\mathbf{D}\\)를 이전의 동작에 적용하고 이미 \\(\\mathbf{Q}\\)를 곱한 해당 매트릭스의 구성을 승계한다. 우리는 \\(\\mathbf{W}_{\\text{in}}\\)의 행과 \\(\\mathbf{W}_{\\text{out}}}})의 열을 삭제하고 \\(\\mathbf{W}_{\\text{embd}}})과 \\(\\mathbf{W}_{\\text{embd}_{\\text{embd}_{\\)의 열을 삭제한다. 우리는 또한 잔차 연결에 삽입된 매트릭스 \\(\\mathbf{Q}_{\\ell-1}^{\\top}\\mathbf{Q}_{\\ell}\\ell}\\ell}\\)의 두 행(<그림 4> 참조)을 모두 삭제한다.\n' +
      '\n' +
      '4개.\n' +
      '\n' +
      '세트업에는 HuggingFace Transformers(Wolf et al., 2019)를 사용하여 PyTorch(Paszke et al., 2019)로 코드를 구현합니다. i\\(\\mathbf{Q}\\)의 계산은 80GB 메모리로 단일 H100 GPU에서 수행되며, 램마-2 70B 모델에 대해 완료하는 데 약 3.5시간이 소요된다. PCA 계산 동안 공분산 매트릭스의 고유벡터를 컴퓨팅하기 위해 이중 정밀도를 사용한다. 우리는 우리는 우리는 우리는 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가 우리가\n' +
      '\n' +
      '그림 4: RMS 정규으로 변환된 네트워크(그림 3 참조)와 함께 계산-분산 아이디어를 적용한다. 입력 가중치 매트릭스 \\(\\operatoral{\\overline{diag}}(\\boldsymbol{\\alpha})\\mathbf{W}_{\\text{in}}\\)는 \\(\\mathbf{Q}^{\\top}\\)에 의해 미리 추출된다. 출력 행렬 \\(\\mathbf{W}_{\\text{out}}\\mathbf{M}\\)는 \\(\\mathbf{Q}\\)에 의해 다중화된 후이다. 스킵 연결에서 새로운 선형 레이어는 \\(\\mathbf{Q}_{\\ell}^{\\top}\\mathbf{Q}_{\\ell\\in\\mathbm{1}\\\\)를 추가한다. 이러한 수정 후에 행렬을 슬라이스(해치 영역)할 수 있다.\n' +
      '\n' +
      'PyTorch에서 고유벡터 계산에 단일 정밀도를 사용하는 것은 부록 A.2에 자세히 설명된 바와 같이 최종 정확도의 불일치로 이어진다.\n' +
      '\n' +
      '위키Text-2 학습 데이터셋(Merity et al, 2016)의 샘플 1024개와 알라카 트레이닝 데이터셋(Taori et al, 2023)의 샘플 5000개 등 두 가지 다른 보정 세트를 실험한다. 서열 길이는 각 언어 모델의 최대값으로 선택됩니다. 교정 설정 크기와 서열 길이에 대한 절제 연구는 부록 A.3에 나와 있다.\n' +
      '\n' +
      '모델, 플라스크 및 GPU는 OPT(Zhang et al., 2022), 라마-2(Touvron et al., 2023) 모델 계열에 대한 모든 실험을 평가하고 Phi-2(0샷 태스크에서) 실험을 추가로 평가한다. 더 작은 라마-2 모델에 의해 능가되기 때문에 OPT 175B를 제외한다. 그럼에도 불구하고, 우리는 더 큰 모델이 일반적으로 압축에 더 유망한 기회를 제공하기 때문에 이 더 큰 모델이 개선된 결과를 산출할 것으로 예상한다(제4.1절 참조). 우리는 인기 있는 제로 샷 작업뿐만 아니라 두 언어 생성에 대한 우리의 계획을 평가한다. 슬리스GPT가 달성한 포괄적인 속도를 입증하기 위해 우리는 24GB의 메모리를 가진 쿼드로 RTX6000 GPU를 소비자 수준의 GPU의 대표적인 예로 사용하고, 40GB A100 및 80GB H100을 사용하여 인접 수준의 벤치마크를 제공한다.\n' +
      '\n' +
      '기준 세트업은 처음에 프루닝된 열(또는 행)과 가장 작은 규범의 결과를 비교할 계획이었으나 이 기준선이 매우 불량하다는 것을 발견했으며 모델의 불균형이 몇 기둥만 프루닝한 후 1000대로 치솟았다. 대신 2:4 스파시리티 비율을 사용하는 SparseGPT(Frantar 및 Alistarh, 2023)에 대한 슬리스GPT를 비교하여 스피드업을 달성하는 유일한 스파시시티 방식(Mishra et al, 2021)이다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '생산 태스크는 위키Text-2 데이터 세트를 사용하여 우리의 결과를 보여줌으로써 시작된다. 이러한 맥락에서 우리는 이 데이터 세트에 적용될 때 다양한 크기에 걸쳐 OPT와 라마-2 모델 계열의 성능을 평가한다. 표 1은 슬라이싱 수준을 달리하여 얻은 퍼플리티를 보여준다. SliceGPT는 이러한 모델의 스펙트럼 분석(토론을 위해 부록 A.4 참조)에서 직관과 일치하는 라마-2 모델에 비해 OPT 모델에 적용될 때 우수한 성능을 나타낸다. 슬리스GPT의 성능은 모델 크기가 증가함에 따라 향상된다. 슬리스GPT와 SparseGPT를 비교하면 SparseGPT 2:4가 모든 Llama-2 모델에서 \\(25\\%\\) 슬라이싱과 슬리스GPT보다 더 나쁘다는 것을 알 수 있다. OPT의 경우 \\(30\\%\\) 슬라이스 모델이 2.7B를 제외한 모든 모델 크기에 대해 2:4 스파시티를 꺾었다는 것을 알 수 있다.\n' +
      '\n' +
      '제로 샷 플라스크는 PIQA(Bisk et al., 2020), 위노 그란데(사카구치 et al., 2021), 헬라 스위그(Z펠러 et al., 2019), ARC-e 및 ARC-c(Clark et al., 2018) 5가지 잘 알려진 제로 샷 작업에 걸쳐 슬리스GPT를 평가한다. 유사한 작업(Frantar and Alistarh, 2023; Dettmers et al, 2022; Frantar et al, 2022; Dettmers et al, 2023)에 따라 우리는 평가에서 기본 매개변수와 함께 LM 평가 하네스(Gao et al., 2021)를 사용한다.\n' +
      '\n' +
      '그림 5는 이러한 과제에 걸쳐 슬라이스된 모델에 의해 달성된 평균 점수를 보여준다. 플롯의 상단 행은 캘리브레이션에 위키Text-2를 사용할 때 평균 정확도를 나타내고, 하단 행은 알카카가 캘리브레이션에 사용될 때 정확도를 나타낸다. 우리는 결과의 생성 과제와 유사한 패턴을 관찰하는데, OPT 모델은 라마-2 모델보다 압축에 더 적합하며 정확도 감소는 더 큰 모델에서 덜 두드러진다. 여기에는 Phi-2 모델도 포함됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\hline \\multirow{3}{*}{} & \\multirow{3}{*}{Sparsity} & \\multicolumn{8}{c|}{OPT} & \\multicolumn{2}{c|}{Llama-2} \\\\  & & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & \\(0\\%\\) & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.11 & 4.57 & 3.11 \\\\ \\hline SparseGPT & 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.15 & 6.63 & 4.70 \\\\ \\hline \\multirow{3}{*}{SliceGPT} & \\(10\\%\\) & 29.44 & 15.08 & 12.80 & 11.00 & 10.26 & 9.63 & 9.41 & 5.47 & 4.88 & 3.46 \\\\  & \\(20\\%\\) & 34.02 & 16.37 & 13.76 & 11.54 & 10.64 & 9.86 & 9.57 & 6.16 & 5.46 & 4.00 \\\\  & \\(25\\%\\) & 37.94 & 17.43 & 14.60 & 11.97 & 10.94 & 10.05 & 9.70 & 6.70 & 5.90 & 4.35 \\\\  & \\(30\\%\\) & 44.17 & 19.05 & 15.91 & 11.36 & 11.36 & 10.29 & 9.87 & 7.51 & 6.52 & 4.77 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: OPT와 램마-2 퍼플렉시티 결과는 위키Text2. OPT. 캘리브레이션 세트 크기는 512이고 서열 길이는 2048이며 라마-2는 캘리브레이션 세트 크기는 1024이고 서열 길이는 4096이다.\n' +
      '\n' +
      '우리는 Phi-2 모델의 슬라이스된 버전이 라마-2 7B 모델의 슬라이스된 버전과 비슷하다는 것을 안다. 가장 큰 OPT 및 라마-2 모델은 66B OPT 모델의 30%를 제거할 때 몇 퍼센트 포인트 손실만으로 매우 효과적으로 압축될 수 있다.\n' +
      '\n' +
      '여기에서 회복 미세 조정(RFT)을 추가로 실험한다. M et al.(2023a)의 아이디어에 따라 LoRA(Hu et al., 2021)를 사용하여 슬라이스된 라마-2 및 Phi-2 모델에 소량의 RFT를 적용한다. 위키Text-2로 슬라이스된 모델의 경우 우리는 5k를 사용하는 알카카 데이터 세트로 슬라이스된 모델에 대해 약 1k 서열을 사용한다. 모든 RFT의 경우 \\(플로라\\_r=32\\), \\(플로라\\_alpha=10\\) 및 서열 길이 1024를 사용하고 HuggingFace PEFT 패키지(Mangrwalkar et al, 2022)의 다른 모든 하이퍼파라미터에 디폴트를 사용한다.\n' +
      '\n' +
      '그림 6은 그 결과를 보여준다. 우리는 WikiText-2와 Alpaca 데이터 세트의 RFT 사이의 현저한 차이를 보고 알카카 데이터 세트는 훨씬 더 높은 수행 모델을 제공한다. 우리는 이 차이를 알카카와 벤치마크 과제 간의 유사성에 귀속시킨다. 가장 큰 라마-2 70B 모델의 경우 30%로 슬라이스되었으며 알파카의 RFT는 조밀한 모델의 76.6%에 비해 평균 74.3%의 정확도를 달성할 수 있다. 슬라이스된 모델은 약 51.6B 매개변수를 가지며 나중에 입증하기 때문에 처리량이 상당히 향상되었다.\n' +
      '\n' +
      '우리는 Phi-2가 위키Text-2 데이터세트만을 사용하여 슬라이싱에서 정확도의 감소를 복구할 수 없지만 알파카를 사용하여 몇 퍼센트 포인트를 회수할 수 있음을 안다. 25% 슬라이싱 및 RFT가 있는 Phi-2의 평균 정확도는 65.2%로 조밀한 모델의 72.2%에 비해 높다. 슬라이스된 모델은 약 2.2B 매개변수를 가지며 2.8B 모델의 정확도의 90.3%를 유지한다. 이것은 작은 LM조차도 훈련 후 프루닝으로부터 이익을 얻을 수 있음을 보여준다. 각 작업에 걸친 정확도의 표는 부록 A.5에 제공된다.\n' +
      '\n' +
      '골드(\\mathbf{W}_{\\text{in}}\\) 및 \\(\\mathbf{W}_{\\text{out}}}\\)에서 스파시티를 도입하는 기존의 스파시리티 방법과 달리 슬리스GPT는 \\(\\mathbf{X}\\)의 전체 컬럼(\\mathbf{X}\\)이 슬라이스되어 임베딩 차원을 감소시킨다. 이것은 압축된 모델 내의 계산 복잡성(플롭들)과 데이터 이동을 모두 향상시킨다.\n' +
      '\n' +
      '25%와 50%로 슬라이스된 모델의 토큰 처리량은 80GB H100 GPU의 조밀한 모델과 비교된다. 우리는 서열 길이를 128으로 설정하고 GPU가 기억력이 떨어지거나 처리량이 떨어질 때까지 배치 크기를 두 배로 증가시켜 최대 처리량을 구한다. 25% 슬라이스 모델은 조밀한 모델에 비해 최대 1.55\\(표본) 처리량 개선을 달성합니다. 가장 큰 모델을 50% 슬라이싱하려면 2개 대신 1개의 GPU만 필요하며 처리량은 3.13\\(종량) 및 1.87\\(종량)가 크게 증가한다. 이는 고정된 수의 GPU에 대해 이러한 모델이 조밀한 모델의 6.26\\(종량) 및 3.75\\(종량) 처리량을 달성한다는 것을 의미한다. 우리는 50%에서 슬리스GPT의 위키Text2 퍼플릭스가 SparseGPT 2:4보다 더 나쁘지만 처리량은 \\(\\mathbf{X}\\)를 슬라이스하지 않는 희소 방법으로 달성할 수 있는 것보다 훨씬 높다는 점에 주목한다. 크기 13B의 모델에 대해, 배치 크기 증가에 따른 성능 증가는 덜 두드러진다.\n' +
      '\n' +
      '그림 5: 보정을 위한 위키Text-2(톱) 및 알파카(바닥) 데이터셋으로 슬라이싱한 후 여러 작업에 걸쳐 OPT, 라마-2 및 Phi-2에 대한 Mean 제로 샷 정확도가 그림 5:였다.\n' +
      '\n' +
      '모델들이 GPU 메모리를 거의 차지하지 않기 때문입니다. 소비자 등급 GPU(기억이 적은 경우)에서 이러한 더 작은 모델에 대한 처리량은 향상될 가능성이 높다. 전체 세부 정보는 부록 A.6을 확인할 수 있습니다.\n' +
      '\n' +
      '참조 시간 다음으로 슬리스GPT로 압축된 모델의 종료 종료 런타임에 대해 연구합니다. 표 2는 쿼드로 RTX6000 및 A100 GPU에서 OPT 66B 및 라마-2 70B 모델에서 단일 토큰을 생성하는 시간을 비교한다. 25% 슬라이싱을 사용할 때 RTX6000 GPU에서 16-17%, A100에서 11-13%의 속도를 관찰했다. 두 경우 모두 사용된 GPU의 수를 줄여 조밀한 모델의 배치에 비해 에너지와 비용 절감 효과를 제공합니다. 라미-2 70B의 경우 RTX6000 GPU를 사용하는 데 필요한 계산은 1764 GPU에서 1075 GPU4로 64%로 감소하며, 이러한 개선은 가중치 매트릭스를 더 작은 것으로 대체하고 다른 프루닝 시스템과 호환 가능한 압축 모델에서 조밀한 커널을 사용하는 접근법으로 결정된다.\n' +
      '\n' +
      '부츠 4: 우리 휴깅페이스 기반 테스트는 연속 배치나 모델 쉐딩을 누리지 않습니다. 이는 추론 시간 측면에서 GPU 측면에서 슬라이스된 모델보다 조밀한 모델을 더 향상시킬 수 있음을 의미한다. 그럼에도 불구하고, 우리의 측정 _do_는 이러한 배치에서 토큰당 에너지 사용량을 반영한다.\n' +
      '\n' +
      '종단 간 성능 이득은 쓰기 시점에 기준 SparseGPT 2:4에서 실현 가능하지 않다. 대신, 변압기 층에 관련된 각 동작의 상대적인 타이밍을 비교하여 슬리스GPT와 SparseGPT 2:4를 비교한다. 우리는 슬리스GPT(25%)가 큰 모델에 대한 속도 및 엄격성 측면에서 SparseGPT(2:4)와 경쟁적이라는 것을 발견했다. 자세한 내용은 부록 A.7을 참조하십시오.\n' +
      '\n' +
      '분쟁 비용은 모든 램마-2, OPT 및 Phi-2 모델이 1~3시간 내에 단일 GPU에서 슬라이스될 수 있다. 회수 미세 조정으로 표 3과 같이 총 1~5시간 내에 모든 LM을 압축합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|} \\hline \\hline \\multicolumn{1}{|c|}{GPU Type} & Slicing & \\multicolumn{2}{c|}{OPT 66B} & \\multicolumn{2}{c|}{Llama-2 70B} \\\\ \\hline \\multirow{3}{*}{A100 (40GB)} & Dense & 114ms on 4 GPUs & 456 GPUs & 125ms on 4 GPUs & 500 GPUs \\\\ \\cline{2-5}  & \\(25\\%\\) & 102ms on 3 GPUs & 306 GPUs & 110ms on 3 GPUs & 330 GPUs \\\\ \\hline \\multirow{3}{*}{Quadro RTX6000} & Dense & 237ms on 6 GPUs & 1422 GPUs & 252ms on 7 GPUs & 1764 GPUs \\\\  & (24GB) & \\(25\\%\\) & 204ms on 5 GPUs & 1020 GPUs & 215ms on 5 GPUs & 1075 GPUs \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 길이 128(배치 크기 1)의 서열을 생성할 때 조밀한 모델에 걸쳐 슬리스GPT의 평균당 평균 추론 시간은 표 2이다. 각각의 경우 ms에서 취한 시간, 필요한 GPU의 수 및 GPU의 총 계산량을 보여준다.\n' +
      '\n' +
      '그림 6: 라마-2 및 Phi-2에 대한 메언 제로 샷 정확도는 슬라이싱 및 회수 미세 조정(RFT) 후 여러 작업에 걸쳐 있다. 좌표: 위키Text-2는 캘리브레이션 및 RFT에 사용된다. 맞습니다: 캘리브레이션 및 RFT에 사용되는 알파카입니다. 광범위한 검색에도 불구하고 OPT 모델에서 향상된 성능을 가능하게 하는 RFT 매개변수를 찾을 수 없었다.\n' +
      '\n' +
      '5배제 및 미래 근무.\n' +
      '\n' +
      '우리는 큰 언어 모델을 위한 구조화된 프루닝이 가능한 슬리스GPT를 도입했다. 우리는 40GB A100 GPU에서 Llama-2 70B의 추론 비용을 추가 코드 최적화가 없는 조밀한 모델의 66%로 감소시켜 SparseGPT 2:4보다 더 나은 퍼플렉스(4~3)를 유지하면서 더 적은 GPU(4~3)를 필요로 하며 24GB RTX6000 GPU, 추론 비용은 64%로 감소하여 2개의 GPU(7~5개)가 필요하다. 0샷 다운스트림 작업에서 OPT 66B, 라마-2 70B 및 Phi-2는 25%로 밀도가 높은 성능의 99%, 96% 및 87%를 유지한다. 회수 미세 조정 25% 리폴라-2 70B 및 Phi-2는 각각 99% 및 90%로 증가한다.\n' +
      '\n' +
      '기회는 우리의 방법에 따라 구축되어야 합니다. 소규모이지만 조밀한 LM은 13B 매개변수 이하가 유사한 크기로 프루닝된 LM보다 더 나은 성능을 발휘하지만 이것이 오랫동안 계속 유지될 것으로 예상하지는 않는다. 우리의 프루닝된 모델은 SparseGPT로 프루닝된 모델보다 더 많은 매개변수를 갖지만 우리의 방법은 더 큰 배치 크기를 GPU 메모리에 로딩할 수 있으며, 아마도 결합된 방법이 둘 다의 최고를 얻을 수 있는 유출 구조에 대한 오버헤드가 없다. 컴퓨팅 \\(\\mathbf{Q}\\)의 다른 방법은 결과를 개선할 수 있다. 추론 시간과 GPU 수를 더 줄이기 위해 양자화(Xiao et al, 2023; Dettmers et al, 2022; Ashkboos et al, 2023; Dettmers et al, 2023; Frantar et al, 2022) 및 구조적 프루닝(예: Ma et al., 2023b)을 포함한 보완 방법을 사용할 수 있다.\n' +
      '\n' +
      '계산 불변성에 대한 우리의 관찰이 딥러닝 모델의 효율성을 향상시키는 데 향후 연구에 도움이 될 수 있기를 바라며 아마도 새로운 이론적 통찰력을 고취시킬 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 아슈보스 등은 (2023) 세일레 아슈코보스, 일리아 마르코프, 엘리사 프란타르, 투링수안 중, 신청 왕, 지렌, 토르스텐 회플러, 단 알리스타르 등을 들 수 있다. 생성 대 언어 모델에 대한 종단 대 엔드 4 비트 추론은 __ 생성 대 언어 모델에 대한 종단 대 엔드 4 비트 추론이다. arXiv 프리프린트 arXiv:2310.09259_, 2023.\n' +
      '* Ba et al. (2016) 짐미 레이 Ba, 제이미 라이언 키오스, 거프리 에힌턴. 선수 정규화 _Layer 정규화 _Layer 정규화 __Layer 정규화. _Layer 정규화. arXiv 프리프린트 arXiv:1607.06450_ 2016.\n' +
      '* Bisk et al.(2020) 연가탄 Bisk, 로완 자셀러, Ronan Le Bras, Jianfeng Gao 및 예진 최입니다. 피카: 자연어의 물리적 커먼즈에 대한 이유. 유엔 인공지능_2020년 제35차 제4차 AAAI 콘퍼런스에서.\n' +
      '* 클라크 등 (2018) 피터 클라크, 아이사크 코위, 오렌 에지니, 투샤르 킬, 아슈시 사바왈, 카리사 소초닉, 오이빈드 타프조드 등이다. 질문 답변을 해결하셨다고 생각하시나요? i2 추론 도전. __ ai2 추론 도전. ArXiv_, abs/1803.05457, 2018. URL[https://apisemanticscholar.org/CorpusID:3922816](https://apisemanticscholar.org/CorpusID:3922816)\n' +
      '* 디트리머 등은 (2022) 팀 디트머, 마이크 루이스, 예제 벨카다, 루크 제트렘거 등이 있다. LLM. 블렌트8(): 스케일에서의 변압기에 대한 8비트 행렬 곱셈. __. arXiv 프리프린트 arXiv:2208.07339_, 2022.\n' +
      '* 디트머스는 (2023) 팀 디트머스, 루슬란 스바스체프스키, 비지 이통자리아, 데니스 쿠즈네델레프, 엘리사 프란타르, 세일라스 애슈보스, 알렉산더 보즈노프, 토르스텐 회플러, 단 알리스타르 등을 들었다. Spqr: 거의 무손실 LLM 가중치 압축에 대한 희소 계량 표현. _2. arXiv 프리프린트 arXiv:2306.03078_, 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{SliceGPT 30\\%} & \\multicolumn{2}{c|}{Recovery fine-tuning} & \\multirow{2}{*}{Total} \\\\  & \\multicolumn{1}{c|}{Time} & \\multicolumn{1}{c|}{GPUs} & \\multicolumn{1}{c|}{Time} & \\multicolumn{1}{c|}{GPUs} \\\\ \\hline Llama-2 7B & 0h44m & 1xH100 80GB & 0h23m & 1xH100 80GB & 1h07m \\\\ Llama-2 13B & 1h08m & 1xH100 80GB & 0h44m & 1xH100 80GB & 1h52m \\\\ Llama-2 70B & 3h31m & 1xH100 80GB & 1h35m & 4xH100 80GB & 5h06m \\\\ \\hline Phi-2 & 0h49m & 1xV100 32GB & 1h59m & 1xV100 32GB & 2h48m \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 슬리스GPT로 30% 슬라이싱하고 알카카 데이터셋을 사용하여 회수 미세 조정 작업을 수행하는 분쟁 비용은 표 3이다. 여기에서 우리는 라마-2 모델의 경우 1024, Phi-2의 경우 2048의 보정 세트 크기를 사용한다.\n' +
      '\n' +
      '* 프랑타르와 알리스타르(2022) 에리아스 프란타르, 단 알리스타르. 최적의 뇌 압박: 정확한 사후 학습 양자화 및 프루닝을 위한 A 프레임워크. __ 최적의 뇌 압박. 신경 정보 처리 시스템_, 2022년 35:4475-4488의 발전입니다.\n' +
      '* 프란타르와 알리스타르(2023) 에리아스 프란타르, 단 알리스타르. SparseGPT: 마사지 언어 모델은 원샷으로 정확하게 프루닝될 수 있습니다. 2023.\n' +
      '* 프랑타르 등 (2022) 에리오스 프란타르, 세일링 애슈보스, 토스트렌 회플러, 단 알리스타르 등이다. GPTQ: 생성 사전 훈련된 변압기에 대한 정확한 사후 학습 양자화. __ 생성 사전 훈련된 변압기에 대한 정확한 사후 학습 양자화이다. arXiv 프리프린트 arXiv:2210.17323_ 2022년입니다.\n' +
      '* 게일 등은 (2019) 트레보르 게일, 에리히 엘센, 사라 홋커 등이 있다. 2019년 심층 신경망의 유출 상태.\n' +
      '* 가오 등은 (2021) 레오 가오, 조나단 토우, 스텔라 비둘란, 스텔라 바이들만, 시드 블랙, 앤서니 디포피, 찰스 포스터, 로렌스 구팅, 제프리 허, 카일 맥도넬, 니클라스 무니엘로프 등 몇 가지 샷 언어 모델 평가를 위한 프레임워크. 2021년 9월_. 9.0.1.Version v.0.0.1.\n' +
      '* 가홀라미 등은 (2021) 아미르 가홀라미, 세훈 김, 선동, 저장웨이 야오, 마이클 W. 마이어와 커트 게이저. 효율적인 신경망 추론을 위한 양자화 방법에 대한 설문 조사 __ 효율적인 신경망 추론을 위한 양자화 방법을 조사하였다. CoRR_, abs/2103.13630, 2021년 URL[https://arxiv.org/abs/2103.13630](https://arxiv.org/abs/2103.13630])이다.\n' +
      '* Gupta & Agrawal (2021) Manish Gupta 및 Puneet Agrawal. 텍스트를 위한 딥러닝 모델의 억제: 2021년 조사.\n' +
      '* 한 등은 (2016) 송한, 후지 마오, 윌리엄 J. Dally. 딥 압축: 딥 뉴럴 네트워크를 프루닝, 훈련된 양자화 및 커프만 코딩, 2016.\n' +
      '* 하시비 등은 (1993) 바박 하시비, 다비드 가스트크, 그레고리 주 월프 등이 있다. 최적의 뇌외과 의사 및 일반 네트워크 프루닝. 신경망_, pp. 293-299. IEEE. 1993에 대한 _IEEE 국제 회의.\n' +
      '* He et al.(2017) 이희하이, 샹유 장, 지안 선. 매우 깊은 신경망을 가속화하기 위한 채널 프루닝입니다. 컴퓨터 비전_, pp 1389-1397에 대한 IEEE 국제 회의의 _검토에서 2017년.\n' +
      '* 호플러 등은 (2021) 토스트렌 회플러, 단 알리스타, 탈벤-닌, 니콜리 드라이덴, 알렉산드라 페어가 있다. 딥러닝의 비교: 신경 네트워크에서 효율적인 추론 및 훈련을 위한 프루닝 및 성장: 신경 네트워크에서의 효율적인 추론 및 학습을 위한 프루닝 및 성장이다. CoRR_, abs/2102.00554, 2021년 URL[https://arxiv.org/abs/2102.00554] (https://arxiv.org/abs/2102.00554)\n' +
      '* Hu et al.(2021) 에드워드 J Hu, 예롱 심, 필립 월리스, 지유안 알렌 주, 원히 리, 시안 왕, 루 왕, 위즈후 첸 등이다. 로라: 2021년 대형 언어 모델의 낮은 순위 적응입니다.\n' +
      '*황앤왕(2018) 자오황과 나이옌 왕. 심층 신경망에 대한 데이터 구동 희소 구조 선택이 필요하다. 2018년 컴퓨터 비전(ECCV)_, pp 304-320에 대한 유럽 회의의 _검토에서.\n' +
      '* LeCun et al. (1989) Yann LeCun, 존 덴커, Sara Solla. 최적의 뇌 손상 __최적의 뇌 손상. __최적의 뇌 손상. 신경 정보 처리 시스템_, 1989년 2.\n' +
      '* 류 등은 (2017) 주앙류, 지구고 리, 지히창 선, 가오황, 샹멍 옌, 창희 장 등이 있다. 네트워크 슬림화를 통해 효율적인 컨볼루션 네트워크를 학습합니다. 컴퓨터 비전_, pp. 2736-2744에 대한 IEEE 국제 회의의 _검토에서 2017.\n' +
      '* 루오 등은 (2017) 지안하오 루오, 지앙신 우, 웨야오 린. 딥 뉴럴 네트워크 압축을 위한 필터 레벨 프루닝 방법: 컴퓨터 비전_, pp. 5058-5066에 대한 IEEE 국제 회의의 _발표에서 2017년.\n' +
      '* 마 등은 (2023a) 신닌 마, 공판 포, 사치오 왕이다. 큰 언어 모델의 구조적 프루닝: __Llm-pruner: 큰 언어 모델의 구조적 프루닝이다. arXiv 프리프린트 arXiv:2305.11627_, 2023a. URL[https://arxiv.org/pdf/2305.11627.pdf](https://arxiv.org/pdf/2305.11627.pdf)\n' +
      '* 마 등은 (2023b) 신닌 마, 공판 포, 사치오 왕이다. LLM-pruner: 대형 언어 모델의 구조적 프루닝, 2023b.\n' +
      '* 마하바디 등은 (2021) 라비히 카라미 마하바디, 제임스 헨더슨, 세바스티안 루더 등이 있다. 특성: 2021년 효율적인 저 순위 과복합 어댑터 레이어입니다.\n' +
      '* 망룰카 등 (2022) 소라브 망룰카르, 실베인 구거, 리칸드르 데부, 예네스 벨카다, 사약 폴, 벤자민 보산 등이 있다. Peft: 최첨단 매개변수 효율적인 미세 조정 방법[플랫폼://github.com/huggingface/peft]]]. 2022년 (https://github.com/huggingface/peft)입니다.\n' +
      '\n' +
      '* 메르티 등은 (2016) 스티븐 머티티, 클레임 시온그, 제임스 브래드베리, 리처드 소셔 등이 있다. 포인터 센티넬 혼합물 모델 __Pointer 센티넬 혼합물 모델. __Pointer 센티넬 혼합물 모델. arXiv 프리프린트 arXiv:1609.07843_ 2016.\n' +
      '* 미슈라 등은 (2021) 아스이트 미슈라, 조르게 알베르시오 라이터레, 제프 풀, 다코 스토시, 다산 스토시, 가네히 베네켄시, 충유, 폴리스 미키비우스 등이다. 가속성 희소 깊은 신경 네트워크 _가속성 희소 심층 신경 네트워크. _가속성 희소 심층 신경 네트워크. arXiv 프리프린트 arXiv:2104.08378_ 2021.\n' +
      '* 노흐&골드버그(2020) 마탄 벤 노흐와 요바 골드버그. 기질 분해로 사전 훈련된 언어 모델을 압축합니다. 2020년 12월 중국 수저우 주 천연 언어 처리_ pp 884-889, 컴퓨터 통계 협회의 제1차 아시아 태평양 위원회 회의 및 제10차 국제 공동 컨퍼런스 \'자연 언어 처리_, pp 884-889\'가 개최되었다. URL[https://aclanthology.org/2020.aacl-main.88](https://aclanthology.org/2020.aacl-main.88).\n' +
      '* 파스케 등(2019) 아담 파스케, 샘 Gross, 샌프란시스코 매사, 아담 라더러, 제임스 브래드버리, 그레고리 찬안, 트레보리 킬렌, 제밍 린, 나탈리아 김싱헤인, 나탈리아 김스헴, 루카 안티치 등 필수 스타일, 고성능 딥러닝 라이브러리. PyTorch: 필수 스타일, 고성능 딥 러닝 라이브러리. 신경 정보 처리 시스템_, 2019년 32.\n' +
      '* Radford et al. (2018) Alec Radford, Karthik Narasimhan, 팀 살리만스, Ilya Sutskever 등은 생성적 사전 학습을 통해 언어 이해를 향상시킨다. 2018.\n' +
      '* 사카구치 등은 알(2021) 키스키 사카구치, 로난 르 브루스, 샹드라 바하바틀라, 예진 최씨 등이 있다. 위노그란데: 저울에서 적대적 윈도그라드 스키마 도전. __ 측근적 윈도그라드 스키마 도전. 2021년 ACM_, 64(9):99-106의 통신.\n' +
      '* 싱앤알리스타(2020) 시닥 팔 싱, 단 알리스타르. 우드셔: 신경망 압축에 대한 효율적인 2차 근사치. __신경망 압축에 대한 효율적인 2차 근사치. 신경 정보 처리 시스템_, 2020년 33:18098-18109의 발전.\n' +
      '* 선 등은 (2023) 명지선, 주앙 류, 안나 베어, 제이 지코 콜터 등이 있다. 대형 언어 모델에 대한 간단하고 효과적인 프루닝 접근 방법 __ 대형 언어 모델에 대한 간단하고 효과적인 프루닝 접근 방식. arXiv 프리프린트 arXiv:2306.11695_, 2023.\n' +
      '* 타오리 등 (2023) 로한태리, 이하안 굴라자니, 톈이 장, 야나두아, 잔첸 리, 카를로스 게스트린, 페시 리앙, 타쓰노리 바슈미모토. 안시스트란포드 alpaca: 아지스트 인스티튜트.com/tatsu-lab/스탄포드_alpaca]]]는 라파카 모델. [내셔널://github.com/tatsu-lab/스탄포드_alpaca]]. (https://github.com/tatsu-lab/스탄포드_alpaca) 2023.\n' +
      '오케네 아흐네르, 아말리아누에, 아네네르스, 아네네우, 아네우우, 아네우우, 아레누 아와네, 카네우, 라흐네르, 아레누 아, 다케네르, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누아, 아레누, 나우우우, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누 아, 아레누 아, 아라, 아레누 아, 마티네, 우와, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누, 라마 2: 오픈 파운데이션과 미세 조정 채팅 모델 2023.\n' +
      '* 투칸 등은 (2020) 무라드 투칸, 알라 마알루프, 마탄 위크러, 단펠트만 등이 있다. 압축된 심층 네트워크, 즉 굿바이 SVD, 안부 강력한 저 순위 근사치. _굿바이 SVD, 안부 강력한 저 순위 근사치. arXiv 프리프린트 arXiv:2009.05647_ 2020.\n' +
      '* 밴 데르 오데라 등 (2023) 티초 FA 반 데르 오우다라, 마르쿠스 나가엘, 마르 마르 반 바알렌, 유키 마카노, 티먼 블란케보르트 등이다. llm 외과 의사는 __ llm 외과 의사이다. arXiv 프리프린트 arXiv:2312.17244_, 2023.\n' +
      '* 바스완이 등은 (2017) 아샤시 바소와이, 노암 샤제리, 니키 파마르, 작노 우즈코레이트, 리온 존스, 디단 노 고메즈, 루카즈 카이저, 일리아 폴로숙신 등이 있다. 필요한 것이 전부입니다. __ 주의가 필요합니다. __ 주목된다. 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* 울프 등(2019) 토마스 볼프, 리만드르 데부트, 빅토르 산하, 진리엔 차믈론, 코멘 델랑에, 앤서니 모이, 피에르 시스트락, 팀 루프, 리미 루프, 모건 푸토위츠 등은 최첨단 자연어 처리인 Huggingface의 변압기이다. arXiv 프리프린트 arXiv:1910.03771_ 2019.\n' +
      '\n' +
      '* 샤오 등은 (2023) 광수안 샤오, 지린, 미카엘 세즈앙, 하오우, 진리엔 데머스, 송한 등이 있다. 스모쿼트: 대규모 언어 모델에 대한 정확하고 효율적인 사후 학습 양자화입니다. 기계 학습_ 국제 회의에서 pp 38087-38099 PMLR, 2023.\n' +
      '*자셀러는 (2019) 로완 자셀러, 아리 홀츠만, 연가탄 위험, 알리 파하디, 예진 최씨 등이 있다. 헬라스와그: 기계가 당신의 문장을 정말 마무리할 수 있습니까? arXiv 프리프린트 arXiv:1905.07830_ 2019.\n' +
      '* 장&세니히(2019) 비오 장과 르코 세니히가 있다. 뿌리 평균 제곱층 정규화는 __Root 평균 제곱층 정규화를 의미한다. 신경정보처리시스템_, 2019년 32.\n' +
      '*장 등은 24일(2022) 수산장, 스티븐 롤러, 남안 고열, 미켈 아르테크스, 모야 첸, 샤오의 첸, 크리스토퍼 듀란, 몬가 디바, 시안 리, 시 빅토리아 린 등의 사전 훈련된 변압기 언어 모델. Opt: 오픈 사전 훈련된 변압기 모델. arXiv 프리프린트 arXiv:2205.01068_, 2022.\n' +
      '* 자오 등은 (2023) 웨인신자오, 건주주, 준이리, 톈이당, 샤오레이 왕, 요펑호우, 유펑호우, 유키안 민, 비셸 장, 준지 장, 지케동 등을 대상으로 한 설문 조사 결과, 대형 언어 모델을 조사했다. arXiv 프리프린트 arXiv:2303.18223_, 2023.\n' +
      '*Zhu & Gupta (2017) 마이클 주와 수요그 구파. 사구까지는 모델 압축을 위한 프루닝의 효과, 즉 2017년을 탐색한다.\n' +
      '*주 등은 (2023) 잔유 주, 지안 리, 용 류, 칸 마, 웨핑 왕이다. 대형 언어 모델에 대한 모델 압축 조사 __ 대형 언어 모델에 대한 모델 압축 조사. __ arXiv 프리프린트 arXiv:2308.07633_, 2023.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '식 2는 식 2와 같다.\n' +
      '\n' +
      '직교 행렬 \\(\\mathbf{Q}\\)는 관계 \\(\\mathbf{Q}^{\\top}^mathbf{Q}\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}^{\\top}=\\mathbf{I}=\\mathbf{I}\\mathbf{I}\\)를 만족하는 정사각형 매트릭스이다. 벡터의 규범은 원소의 제곱합의 제곱근이다: \\(\\|\\|\\|mathbf{x}\\|=\\sqrt{\\sum_{i}\\mathbf{x}_{i}^{x}}=\\sqrt{\\mathbf{x}^{\\mathbf{x}^{\\bf{x}\\mathbf{x}\\\\-f{x}\\)이다. \\(\\|\\mathbf{Q}\\mathbf{Q}\\|,\\mathbf{x}\\|=\\sqrt{\\mathbf{x}\\mathbf{x}^{\\top}^mathbf{Q}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\|}\\mathbf{x}\\|}\\mathbf{x}\\mathbf{x}\\|}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\nf{x}\\mathbf{x}\\)}\\mathbf{x}\\mathbf{x}\\df{x}\\ing}\\\n' +
      '\n' +
      'RMS 정규 동작은 입력 행렬 \\(\\mathbf{X}\\)의 각 행을 그 규범으로 분할한다. 선형 대수의 기본 규칙에 의해, \\(\\mathbf{x}\\)가 \\(\\mathbf{X}\\)의 행이라면, \\(\\mathbf{X}^{\\top}^mathbf{x}\\)는 \\(\\mathbf{x}\\)의 해당 행이다. RNase를 \\(\\mathbf{X}\\mathbf{Q}\\\\)에 적용하면 행은 이제 \\(\\frac{1}{\\|\\|\\mathbf{x}\\|}\\mathbf{Q}^{\\top}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\mathbf{x}\\)와 동일할 것이라고 말했다. M\\(\\mathbf{Q}^{\\top}\\) 이후, 우리는 이제 \\(\\frac{1}{\\|\\|\\mathbf{x}\\|}\\mathbf{x}\\mathbf{Q}\\mathbf{Q}\\mathbf{x}\\|}\\|}\\mathbf{x}\\o)에 의해 증식할 수 있다. 따라서 우리는 관계가 있다.\n' +
      '\n' +
      '\\[\\text{RMSNorm}(\\mathbf{X}\\mathbf{f{Q}\\mathbf{Q})\\mathbf{Q}}:^{RMSNorm}=\\text{RMSNorm}(\\mathbf{ X})\\,\\f{RMSNorm}(\\mathbf{ X})\\\n' +
      '\n' +
      '정밀 Eigen는 계산합니다.\n' +
      '\n' +
      '앞서 섹션 4에서 언급한 바와 같이 PCA 알고리즘을 수행할 때 이중 정밀도를 사용한다. 이러한 선택은 슬리스GPT에서 직교 행렬의 계산 중에 발생할 수 있는 잠재적 수치 오차를 완화하기 위해 이루어진다. 그럼에도 불구하고 PCA 계산에 대한 더 낮은 정밀도를 사용하는 것이 궁극적인 정확도에 미치는 영향을 조사하는 것은 흥미롭다.\n' +
      '\n' +
      '표 4는 우리가 FP32 PCA를 계획에 적용할 때 모든 모델의 엄격성을 보여준다. PCA 계산 단계에서 더 큰 모델의 정확도가 수치 오류에 영향을 받을 수 있음을 보여준다. 우리는 고유벡터와 고유값을 계산하기 위해 PyTorch(torch.linalg)를 사용한다는 점에 유의해야 한다.\n' +
      '\n' +
      '###. 캘리브레이션 설정 크기 및 서열 길이에 민감합니다.\n' +
      '\n' +
      '위키텍스트-2 보정 세트의 역할을 조사하기 위한 절제 연구를 제시한다. 우리는 OPT 6.7B 및 라마-2 7B 모델을 사용하여 25% 가연성 생성 작업에 중점을 둔다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\hline  & \\multirow{2}{*}{Sparsity} & \\multicolumn{5}{c|}{OPT} & \\multicolumn{5}{c|}{Llama-2} \\\\  & & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & \\(0\\%\\) & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.47 & 4.88 & 3.32 \\\\ \\hline SparseGPT & 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.69 & 7.07 & 4.98 \\\\ \\hline  & \\(10\\%\\) & 29.48 & 15.15 & 12.83 & 11.05 & 10.28 & 9.68 & 9.45 & 6.51 & 5.64 & 4.20 \\\\ SliceGPT & \\(20\\%\\) & 34.12 & 16.51 & 13.87 & 11.64 & 10.73 & 9.94 & 9.80 & 7.30 & 6.07 & 5.82 \\\\  & \\(25\\%\\) & 38.25 & 17.67 & 14.78 & 12.14 & 11.08 & 10.15 & 9.81 & 8.52 & 6.65 & 7.01 \\\\  & \\(30\\%\\) & 44.17 & 19.33 & 16.20 & 12.82 & 11.53 & 10.43 & 9.99 & 10.41 & 7.49 & 8.75 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'FP32 PCA 계산을 사용하여 위키Text2에 대한 표 4: OPT 및 라마-2 퍼플렉스 결과이다. 모든 경우의 시퀀스 길이는 2048입니다.\n' +
      '\n' +
      '그림 7: 캘리브레이션 세트 크기와 서열 길이가 위키Text2의 심장에 미치는 영향.\n' +
      '\n' +
      '그림 7(왼쪽)은 각막에 설정된 보정 크기를 달리한 결과를 보여준다. 최소 128의 샘플 크기가 캘리브레이션 세트에 대해 민감하게 선택한다는 것을 보여준다.\n' +
      '\n' +
      '다음으로 보정 세트에 서로 다른 서열 길이 \\(N\\)를 사용하는 효과를 탐구한다. 고정 수의 \\(B\\) 샘플을 감안할 때 PCA 입력 매트릭스는 \\(NB\\) 임베딩 벡터를 사용하여 계산되며, 더 큰 \\(B\\) 또는 더 큰 \\(N\\)를 갖는 간의 트레이드오프를 이해하는 것이 흥미롭다. 그림 7(오른쪽)은 보정 세트에서 서열 길이를 128에서 4096로 달리한 결과를 보여주고 있으며, 우리는 더 큰 서열 길이를 갖는 것이 더 나은 엄격성을 초래할 수 있다고 결론지었다.\n' +
      '\n' +
      '이러한 통찰력을 사용하여 주요 실험에서 라마-2의 경우 512, 라마-2의 경우 1024, OPT의 경우 2048, 라마-2의 경우 4096의 보정 세트 크기를 사용한다(표 1). 아래 표 5에서 우리는 더 작은 보정 세트 크기와 더 짧은 보정 시퀀스 길이를 갖는 위키트워드-2의 OPT 및 라마-2 모델의 경피성을 평가하고, 이는 모든 모델과 크기에 걸쳐 불균일성을 감소시키는 경향을 확인시켜준다.\n' +
      '\n' +
      '라마-2 및 OPT 모델의 스펙트럼 분석##\n' +
      '\n' +
      '아래 그림은 OPT 6.7B 및 라마-2 7B 모델에 대한 고유값 분포를 보여준다. 두 모델 모두 비교 가능한 매개변수 수를 가지고 있지만 라마-2 모델은 임베딩 스펙트럼에서 더 단단히 압축된 분포를 갖는다. 이 관찰은 훨씬 더 많은 정보를 가진 지배적인 주성분이 존재하지 않아 이러한 구성 요소의 프루닝이 더 어려운 작업임을 보여준다.\n' +
      '\n' +
      '슬라이싱 레벨을 선방향으로 특정하는 대신 각 PCA 계산 동안 폐기하기 위해 총 분산의 분율을 설정했으며, 이는 각 매트릭스에서 슬라이스하기 위해 행과 열의 수를 설정한다. 각 모델에 대해 25%에 가까운 네트워크에서 총 감소를 얻기 위해 다양한 표적 분산을 사용하여 세 가지 실험을 실행한다.\n' +
      '\n' +
      '그림 8: 64개의 샘플을 사용하여 MLP 입력(log 스케일)의 정규화된(최대) 스펙트럼을 사용한다. 라미-2 모델에서 첫 번째 층을 제외하고 두 모델에 대한 고유값 분포는 이후 모델에 비해 초기 층에서 더 빠른 붕괴를 보여준다. 이는 이러한 초기 층에서 직교 변환 후에 더 많은 양의 슬라이싱이 적용될 수 있음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{Method} & \\multicolumn{8}{c|}{OPT} & \\multicolumn{3}{c|}{Llama-2} \\\\  & 125M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 7B & 13B & 70B \\\\ \\hline Dense & 27.65 & 14.63 & 12.47 & 10.86 & 10.13 & 9.56 & 9.34 & 5.47 & 4.88 & 3.32 \\\\ \\hline SparseGPT 2:4 & 45.07 & 29.61 & 14.90 & 13.00 & 11.80 & 10.53 & 10.22 & 8.69 & 7.07 & 4.98 \\\\ \\hline SliceGPT (\\(10\\%\\)) & 29.50 & 15.16 & 12.82 & 11.07 & 10.29 & 9.65 & 9.43 & 5.94 & 5.31 & 3.79 \\\\ SliceGPT (\\(20\\%\\)) & 34.10 & 16.51 & 13.89 & 11.60 & 10.71 & 9.92 & 9.60 & 6.84 & 6.06 & 4.48 \\\\ SliceGPT (\\(25\\%\\)) & 38.24 & 17.67 & 14.76 & 12.10 & 11.04 & 10.13 & 9.75 & 7.55 & 6.63 & 4.89 \\\\ SliceGPT (\\(30\\%\\)) & 44.23 & 19.31 & 16.13 & 12.73 & 11.49 & 10.39 & 9.94 & 8.59 & 7.44 & 5.44 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 위키Text2에 대한 OPT 및 라마-2 퍼플렉스 결과. 모든 모델은 교정 설정 크기가 128이고 서열 길이는 2048이다.\n' +
      '\n' +
      '결과는 아래 표 6에 나와 있다. 계층별 슬라이싱 수준을 변화시키면 OPT 모델에서 위키텍스트-\\(2\\) 퍼플리티가 향상되지만 라마-\\(2\\) 모델에서 반대 효과가 있다.\n' +
      '\n' +
      '제로샷 결과.\n' +
      '\n' +
      '본 절에서는 우리가 논문에서 제시한 제로샷 과제의 상세한 결과를 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|} \\hline \\hline Model & WikiText-2 PPL at 25\\% & WikiText-2 ppl varying slicing & PPL improvement \\\\  & constant slicing & by layer & \\\\ \\hline OPT 6.7B & 12.10 & 11.94, 24.7\\% total slicing & 0.16 \\\\ OPT 13B & 11.04 & 10.76, 24.2\\% total slicing & 0.28 \\\\ OPT 30B & 10.13 & 9.95, 24.8\\% total slicing & 0.18 \\\\ OPT 66B & 9.75 & 9.63, 24.1\\% total slicing & 0.12 \\\\ \\hline Llama-\\(2\\) 7B & 6.84 & 7.63, 24.1\\% total slicing & -0.79 \\\\ Llama-\\(2\\) 13B & 6.00 & 6.17, 23.3\\% total slicing & -0.17 \\\\ Llama-\\(2\\) 70B & 4.44 & 4.63, 25.5\\% total slicing & -0.19 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: 계층별 슬라이싱 수준이 달라지는 효과를 평가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\hline \\multirow{4}{*}{OPT-1.3B} & Dense & 72.42 & 59.27 & 53.72 & 50.97 & 29.52 & 53.18 \\\\  & \\(20\\%\\) & 65.34 & 54.85 & 45.39 & 46.04 & 26.96 & 47.72 \\\\  & \\(25\\%\\) & 62.30 & 53.83 & 42.91 & 45.45 & 27.22 & 46.34 \\\\  & \\(30\\%\\) & 60.77 & 54.70 & 39.81 & 43.90 & 25.77 & 44.99 \\\\ \\hline \\multirow{4}{*}{OPT-2.7B} & Dense & 74.81 & 61.01 & 60.58 & 54.42 & 31.14 & 56.39 \\\\  & \\(20\\%\\) & 68.23 & 57.93 & 51.38 & 51.81 & 28.50 & 51.57 \\\\  & \\(25\\%\\) & 65.29 & 57.22 & 47.85 & 49.79 & 27.99 & 49.63 \\\\  & \\(30\\%\\) & 62.35 & 57.22 & 44.18 & 46.72 & 27.05 & 47.50 \\\\ \\hline \\multirow{4}{*}{OPT-6.7B} & Dense & 76.39 & 65.19 & 67.16 & 60.14 & 34.64 & 60.70 \\\\  & \\(20\\%\\) & 72.74 & 61.09 & 61.04 & 55.89 & 30.80 & 56.31 \\\\  & \\(25\\%\\) & 70.35 & 60.62 & 58.15 & 52.78 & 29.52 & 54.28 \\\\  & \\(30\\%\\) & 68.61 & 60.69 & 54.56 & 52.15 & 29.01 & 53.00 \\\\ \\hline \\multirow{4}{*}{OPT-13B} & Dense & 76.82 & 64.80 & 69.81 & 61.87 & 35.67 & 61.79 \\\\  & \\(20\\%\\) & 74.48 & 64.96 & 65.42 & 60.90 & 35.24 & 60.20 \\\\  & \\(25\\%\\) & 73.67 & 64.25 & 63.28 & 60.52 & 34.64 & 59.27 \\\\  & \\(30\\%\\) & 71.82 & 62.90 & 60.66 & 58.80 & 32.94 & 57.42 \\\\ \\hline \\multirow{4}{*}{OPT-3OB} & Dense & 78.07 & 68.19 & 72.27 & 65.24 & 38.23 & 64.40 \\\\  & \\(20\\%\\) & 76.50 & 66.61 & 70.61 & 64.18 & 35.75 & 62.73 \\\\  & \\(25\\%\\) & 75.30 & 66.61 & 69.42 & 63.55 & 35.67 & 62.11 \\\\  & \\(30\\%\\) & 74.97 & 65.04 & 68.15 & 63.55 & 34.64 & 61.27 \\\\ \\hline \\multirow{4}{*}{OPT-66B} & Dense & 79.82 & 68.90 & 74.85 & 67.21 & 40.02 & 66.16 \\\\  & \\(20\\%\\) & 78.73 & 67.88 & 73.79 & 68.81 & 39.51 & 65.74 \\\\  & \\(25\\%\\) & 78.40 & 67.09 & 73.33 & 67.89 & 39.16 & 65.17 \\\\  & \\(30\\%\\) & 77.42 & 66.30 & 72.62 & 66.90 & 37.97 & 64.24 \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 69.42 & 65.11 & 59.04 & 59.76 & 37.54 & 58.18 \\\\  & \\(25\\%\\) & 66.87 & 63.38 & 54.16 & 58.46 & 34.56 & 55.48 \\\\  & \\(30\\%\\) & 63.55 & 61.33 & 49.62 & 51.77 & 31.23 & 51.50 \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 71.87 & 69.38 & 63.04 & 69.87 & 43.09 & 63.45 \\\\  & \\(25\\%\\) & 68.55 & 67.48 & 58.10 & 62.50 & 37.88 & 58.90 \\\\  & \\(30\\%\\) & 66.10 & 65.11 & 52.69 & 56.82 & 35.07 & 55.16 \\\\ \\hline \\multirow{4}{*}{Llama-2 7OB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 76.61 & 76.40 & 72.98 & 80.51 & 55.20 & 72.34 \\\\  & \\(25\\%\\) & 74.92 & 75.37 & 68.84 & 77.90 & 51.71 & 69.75 \\\\  & \\(30\\%\\) & 72.31 & 73.56 & 63.69 & 73.40 & 47.61 & 66.11 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 71.87 & 67.80 & 57.76 & 58.00 & 35.32 & 58.15 \\\\ \\cline{1-1}  & \\(25\\%\\) & 69.21 & 65.35 & 52.40 & 53.70 & 31.66 & 54.46 \\\\ \\cline{1-1}  & \\(30\\%\\) & 65.94 & 63.14 & 47.56 & 53.03 & 30.29 & 51.99 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: OPT, 라마-2 및 Phi-2 모델에 대한 위키Text2 데이터셋을 사용하여 슬라이싱할 때 다운스트림 태스크 성능은 표 7:였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\hline \\multirow{4}{*}{OPT-1.3B} & Dense & 72.42 & 59.27 & 53.72 & 50.97 & 29.52 & 53.18 \\\\  & \\(20\\%\\) & 69.91 & 55.49 & 47.88 & 49.66 & 27.05 & 50.00 \\\\  & \\(25\\%\\) & 69.37 & 55.72 & 45.82 & 48.70 & 26.62 & 49.25 \\\\  & \\(30\\%\\) & 68.55 & 55.33 & 43.92 & 47.26 & 26.45 & 48.30 \\\\ \\hline \\multirow{4}{*}{OPT-2.7B} & Dense & 74.81 & 61.01 & 60.58 & 54.42 & 31.14 & 56.39 \\\\  & \\(20\\%\\) & 71.87 & 58.09 & 54.98 & 54.04 & 29.44 & 53.68 \\\\  & \\(25\\%\\) & 70.95 & 58.09 & 52.62 & 53.03 & 29.61 & 52.86 \\\\  & \\(30\\%\\) & 69.64 & 56.43 & 49.45 & 51.81 & 28.33 & 51.13 \\\\ \\hline \\multirow{4}{*}{OPT-6.7B} & Dense & 76.39 & 65.19 & 67.16 & 60.14 & 34.64 & 60.70 \\\\  & \\(20\\%\\) & 74.54 & 62.67 & 62.84 & 59.18 & 33.36 & 58.52 \\\\  & \\(25\\%\\) & 73.78 & 62.59 & 60.99 & 59.01 & 33.70 & 58.01 \\\\  & \\(30\\%\\) & 73.34 & 61.80 & 58.93 & 58.33 & 32.85 & 57.05 \\\\ \\hline \\multirow{4}{*}{OPT-13B} & Dense & 76.82 & 64.80 & 69.81 & 61.87 & 35.67 & 61.79 \\\\  & \\(20\\%\\) & 76.01 & 65.19 & 66.15 & 61.57 & 34.73 & 60.73 \\\\  & \\(25\\%\\) & 74.65 & 64.64 & 65.02 & 60.65 & 35.07 & 60.00 \\\\  & \\(30\\%\\) & 74.86 & 63.46 & 63.16 & 61.36 & 34.56 & 59.48 \\\\ \\hline \\multirow{4}{*}{OPT-3OB} & Dense & 78.07 & 68.19 & 72.27 & 65.24 & 38.23 & 64.40 \\\\  & \\(20\\%\\) & 78.35 & 66.61 & 70.64 & 65.19 & 37.46 & 63.65 \\\\  & \\(25\\%\\) & 77.48 & 65.82 & 69.58 & 65.91 & 37.63 & 63.28 \\\\  & \\(30\\%\\) & 76.93 & 64.96 & 68.66 & 65.70 & 37.12 & 62.67 \\\\ \\hline \\multirow{4}{*}{OPT-66B} & Dense & 79.82 & 68.90 & 74.85 & 67.21 & 40.02 & 66.16 \\\\  & \\(20\\%\\) & 79.49 & 68.19 & 73.69 & 67.26 & 39.25 & 65.58 \\\\  & \\(25\\%\\) & 79.11 & 68.35 & 73.30 & 67.00 & 38.74 & 65.30 \\\\  & \\(30\\%\\) & 79.05 & 68.75 & 72.62 & 66.29 & 38.31 & 65.00 \\\\ \\hline \\multirow{4}{*}{LLama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.50 & 65.51 & 65.20 & 69.99 & 41.21 & 63.68 \\\\  & \\(25\\%\\) & 74.21 & 64.01 & 60.55 & 66.88 & 38.91 & 60.91 \\\\  & \\(30\\%\\) & 72.25 & 59.83 & 55.86 & 63.93 & 37.80 & 57.93 \\\\ \\hline \\multirow{4}{*}{LLama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 77.97 & 68.90 & 69.64 & 74.71 & 45.99 & 67.44 \\\\  & \\(25\\%\\) & 76.88 & 67.40 & 65.85 & 72.52 & 44.54 & 65.44 \\\\  & \\(30\\%\\) & 74.10 & 65.82 & 60.91 & 68.43 & 42.41 & 62.34 \\\\ \\hline \\multirow{4}{*}{LLama-2 7OB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 81.99 & 76.87 & 78.93 & 80.26 & 54.10 & 74.43 \\\\  & \\(25\\%\\) & 80.69 & 77.98 & 76.97 & 79.67 & 52.65 & 73.59 \\\\  & \\(30\\%\\) & 79.33 & 77.27 & 73.11 & 77.44 & 51.19 & 71.67 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 76.17 & 68.75 & 61.95 & 72.18 & 45.48 & 64.90 \\\\ \\cline{1-1}  & \\(25\\%\\) & 75.68 & 64.88 & 58.19 & 70.41 & 43.43 & 62.52 \\\\ \\cline{1-1}  & \\(30\\%\\) & 74.05 & 62.12 & 53.31 & 67.26 & 39.42 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: OPT, 라마-2 및 Phi-2 모델에 대한 알파카 데이터 세트를 사용하여 슬라이싱할 때 다운스트림 태스크 성능은 표 8:였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.55 & 65.59 & 68.26 & 71.84 & 45.05 & 65.46 \\\\  & \\(25\\%\\) & 75.79 & 63.22 & 65.12 & 68.22 & 42.83 & 63.04 \\\\  & \\(30\\%\\) & 74.59 & 61.64 & 63.06 & 66.54 & 40.87 & 61.34 \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 79.27 & 68.27 & 73.21 & 74.37 & 49.83 & 68.99 \\\\  & \\(25\\%\\) & 78.84 & 67.64 & 71.21 & 73.57 & 49.66 & 68.18 \\\\  & \\(30\\%\\) & 76.11 & 68.03 & 68.58 & 71.42 & 47.10 & 66.35 \\\\ \\hline \\multirow{4}{*}{Llama-2 70B} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 81.94 & 77.74 & 79.39 & 81.57 & 58.45 & 75.82 \\\\  & \\(25\\%\\) & 81.88 & 77.11 & 79.04 & 81.36 & 58.70 & 75.62 \\\\  & \\(30\\%\\) & 80.30 & 75.85 & 77.13 & 80.05 & 58.19 & 74.30 \\\\ \\hline \\multirow{4}{*}{Phi-2} & Dense & 79.11 & 75.77 & 73.83 & 78.32 & 54.18 & 72.24 \\\\  & \\(20\\%\\) & 77.42 & 72.14 & 65.33 & 74.20 & 49.91 & 67.80 \\\\  & \\(25\\%\\) & 76.17 & 68.75 & 63.39 & 70.45 & 47.44 & 65.24 \\\\  & \\(30\\%\\) & 75.24 & 65.59 & 60.10 & 70.16 & 46.25 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 10>은 라마-2 및 Phi-2 모델에 알파카 데이터셋을 사용하여 미세 조정 및 회수 시 다운스트림 태스크 성능을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\hline Model & Slicing & PIQA & WinoGrande & HellaSwag & ARC-e & ARC-c & Avg. Score \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 79.11 & 69.06 & 75.99 & 74.58 & 46.25 & 69.00 \\\\  & \\(20\\%\\) & 76.55 & 65.59 & 68.26 & 71.84 & 45.05 & 65.46 \\\\  & \\(25\\%\\) & 75.79 & 63.22 & 65.12 & 68.22 & 42.83 & 63.04 \\\\  & \\(30\\%\\) & 74.59 & 61.64 & 63.06 & 66.54 & 40.87 & 61.34 \\\\ \\hline \\multirow{4}{*}{Llama-2 TB} & Dense & 80.47 & 72.22 & 79.39 & 77.48 & 49.23 & 71.76 \\\\  & \\(20\\%\\) & 79.27 & 68.27 & 73.21 & 74.37 & 49.83 & 68.99 \\\\  & \\(25\\%\\) & 78.84 & 67.64 & 71.21 & 73.57 & 49.66 & 68.18 \\\\  & \\(30\\%\\) & 76.11 & 68.03 & 68.58 & 71.42 & 47.10 & 66.35 \\\\ \\hline \\multirow{4}{*}{Llama-2 TBB} & Dense & 82.70 & 77.98 & 83.84 & 80.98 & 57.34 & 76.57 \\\\  & \\(20\\%\\) & 77.08 & 68.75 & 63.39 & 70.45 & 47.44 & 65.24 \\\\ \\cline{1-1}  & \\(30\\%\\) & 75.24 & 65.59 & 60.10 & 70.16 & 46.25 & 63.47 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 10>은 라마-2 및 Phi-2 모델에 알파카 데이터셋을 사용하여 미세 조정 및 회수 시 다운스트림 태스크 성능을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      'SparseGPT에 대한 슬리스GPT의 결정.\n' +
      '\n' +
      '우리는 CuSparseLT 0.5 라이브러리를 사용하여 80GB A100 GPU에서 희소 매트릭스 다중 신호를 실행하여 3개의 다른 크기의 Llama-2 모델에서 매트릭스-매트릭스 곱셈의 크기를 복제한다. 우리는 PyTorch를 사용하여 조밀한 등가물에 대해 유사한 매트릭스 곱셈을 실행했으며 슬리스GPT(즉, 단순한 조밀한 매트물이지만 작음)에 대해 실행했다. 우리는 2048의 시퀀스 길이를 선택하고 휴깅페이스 구성 파일에서 매트릭스 크기를 취했다. 우리는 \\(10^{3}\\) 시도를 통해 중간 런타임을 취했다.\n' +
      '\n' +
      '각 라마-2 레이어는 하나의 상향 투영, 하나의 다운 투영 및 게이팅 투영이 있는 게이팅 FFN을 필요로 한다. 관점에서 모델의 아키텍처는 질의 행렬 곱셈이 키 및 값 행렬 곱셈과 다른 크기임을 의미한다. 다음 표는 모델에서 각 행렬 곱셈을 실행시키기 위해 ms에서 취한 시간과 "총" 시간과 상대 속도를 보여준다.\n' +
      '\n' +
      '우리는 또한 같은 방식으로 OPT 구조를 벤치마킹했다. 이 경우 키, 밸류, 퀀리, 아웃의 행렬 곱셈은 모두 동일한 크기이며 MLP 구간(FC1, FC2)에는 2개의 매트릭스 곱셈만이 존재한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|l|} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{PPL} & \\multicolumn{4}{c|}{Operation (ms)} & \\multicolumn{2}{c|}{Total in ms} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{FC2} & \\multicolumn{1}{c|}{FC1} & \\multicolumn{1}{c|}{K,V,Q,Out} & \\multicolumn{1}{c|}{(speedup)} \\\\ \\hline \\multirow{4}{*}{OPT 13B} & Dense & 10.13 & 1.89 & 1.89 & 0.52 & 7.75 \\\\  & SparseGPT 2:4 & 11.80 & 1.18 & 1.50 & 0.31 & 5.42 (1.43\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 10.94 & 1.50 & 1.45 & 0.38 & 5.92 (1.31\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 15.39 & 0.96 & 0.99 & 0.26 & 3.98 (1.95\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{OPT 30B} & Dense & 9.56 & 10.29 & 1.28 & 0.52 & 5.93 \\\\  & SparseGPT 2:4 & 10.53 & 0.81 & 0.95 & 0.31 & 3.95 (1.50\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 10.05 & 1.03 & 0.98 & 0.39 & 4.55 (1.30\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 12.47 & 0.68 & 0.67 & 0.26 & 3.06 (1.94\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{OPT 66B} & Dense & 9.34 & 4.63 & 4.27 & 0.21 & 14.01 \\\\  & SparseGPT 2:4 & 10.22 & 2.87 & 3.69 & 0.14 & 10.81 (1.30\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (25\\%) & 9.70 & 3.40 & 3.26 & 0.16 & 10.56 (1.33\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (50\\%) & 11.39 & 2.28 & 2.34 & 0.15 & 7.56 (1.85\\(\\times\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: OPT 모델의 각 계층에 관여하는 행렬 다중화 타이밍의 결과. 더 큰 모델의 경우 슬리스GPT(25%)는 SparseGPT 2:4보다 약간 더 나은 스피드를 제공하고 더 나은 퍼플리티를 제공한다. 더 작은 모델의 경우 SparseGPT 2:4는 더 나쁜 엄격함에도 더 나은 속도를 제공한다. 50%로 이동하는 것은 훨씬 더 큰 속도를 위해 부족함이 없습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|l|l|l|l|l|l|l|l|} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{PPL} & \\multicolumn{4}{c|}{Operation (ms)} & \\multicolumn{2}{c|}{Total in ms} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\  & & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\  & & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\\\ \\hline \\multirow{4}{*}{Llama-2 7B} & Dense & 5.11 & 0.89 & 0.87 & 0.34 & 0.34 & 0.34 & 3.99 \\\\  & SparseGPT 2:4 & 8.15 & 0.56 & 0.61 & 0.23 & 0.23 & 0.23 & 2.70 (1.48\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 6.70 & 0.67 & 0.64 & 0.26 & 0.25 & 0.27 & 2.99 (1.33\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 17.17 & 0.46 & 0.44 & 0.18 & 0.18 & 0.18 & 2.06 (1.94\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{Llama-2 13B} & Dense & 4.58 & 1.29 & 1.28 & 0.52 & 0.52 & 0.52 & 5.93 \\\\  & SparseGPT 2:4 & 6.63 & 0.81 & 0.95 & 0.31 & 0.31 & 0.31 & 3.95 (1.50\\(\\times\\)) \\\\  & SliceGPT (25\\%) & 5.90 & 1.03 & 0.98 & 0.39 & 0.39 & 0.41 & 4.57 (1.30\\(\\times\\)) \\\\  & SliceGPT (50\\%) & 13.71 & 0.68 & 0.67 & 0.26 & 0.27 & 0.30 & 3.11 (1.91\\(\\times\\)) \\\\ \\hline \\multirow{4}{*}{Llama-2 70B} & Dense & 3.12 & 4.63 & 4.27 & 0.21 & 1.27 & 1.27 & 16.13 \\\\  & SparseGPT 2:4 & 4.70 & 2.87 & 3.69 & 0.14 & 0.84 & 0.83 & 12.20 (1.32\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (25\\%) & 4.35 & 3.4 & 3.26 & 0.16 & 0.96 & 1.00 & 12.20 (1.32\\(\\times\\)) \\\\ \\cline{1-1}  & SliceGPT (50\\%) & 8.86 & 2.28 & 2.34 & 0.15 & 0.69 & 0.68 & 8.63 (1.87\\(\\times\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 12>는 라마-2 모형의 각 계층에 관여하는 행렬 곱셈 타이밍의 결과이다. 더 큰 모델의 경우 슬리스GPT(25%)는 SparseGPT 2:4와 동일한 속도를 제공하지만 더 나은 불균일성을 제공한다. 더 작은 모델의 경우 SparseGPT 2:4는 더 나쁜 엄격함에도 더 나은 속도를 제공한다. 50%로 이동하는 것은 훨씬 더 큰 속도를 위해 부족함이 없습니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>