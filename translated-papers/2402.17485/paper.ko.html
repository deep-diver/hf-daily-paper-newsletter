<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '비디오 전반에 걸쳐 동일성 보존으로 표현력이 높고 실제와 같은 애니메이션이 탄생했습니다. 실험 결과는 EMO가 설득력 있는 말하기 동영상뿐만 아니라 다양한 스타일로 노래하는 동영상을 제작할 수 있다는 것을 입증하며 표현성과 사실성 측면에서 기존의 최첨단 방법론을 크게 능가한다.\n' +
      '\n' +
      '키워드: 확산 모델비디오 생성\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안 이미지 생성 분야는 확산 모델[4, 8, 16, 20, 25]의 출현과 성공에 크게 기인하여 놀라운 발전을 목격했다. 이러한 모델은 대규모 이미지 데이터 세트에 대한 광범위한 훈련과 점진적인 생성 접근 방식보다 고품질 이미지를 생성하는 능력을 자랑한다. 이 혁신적인 방법론은 생성 모델의 영역에서 새로운 벤치마크를 설정하면서 비교할 수 없는 세부 사항과 사실성을 가진 이미지를 생성할 수 있게 한다. 확산 모델의 적용은 정지 영상에만 국한되지 않았다. 비디오 생성에 대한 급격한 관심은 역동적이고 설득력 있는 시각적 내러티브를 만드는 데 있어 이러한 모델의 잠재력을 탐구하게 했다[6, 9]. 이러한 선구적인 노력은 비디오 생성 분야에서 확산 모델의 방대한 잠재력을 강조한다.\n' +
      '\n' +
      '일반적인 비디오 합성을 넘어, 인간 중심 비디오의 생성은 말하는 머리 등 연구의 초점이 되어 왔다. 대화 헤드의 목적은 사용자가 제공하는 오디오 클립으로부터 얼굴 표정을 생성하는 것이다. 이러한 표현을 조작하는 것은 인간의 얼굴 움직임의 미묘함과 다양성을 포착하는 것을 포함하며, 비디오 합성에서 중요한 도전을 제시한다. 전통적인 접근법들은 종종 이 작업을 단순화하기 위해 최종 비디오 출력에 제약을 부과한다. 예를 들어, 일부 방법은 얼굴 키포인트를 제한하기 위해 3D 모델을 사용하는 반면, 다른 방법은 전체 동작을 안내하기 위해 베이스 비디오로부터 머리 움직임의 시퀀스를 추출한다. 이러한 제약들은 비디오 생성의 복잡성을 감소시키지만, 또한 결과적인 얼굴 표정의 풍부함과 자연스러움을 제한하는 경향이 있다.\n' +
      '\n' +
      '본 논문에서는 미묘한 미시적 표현을 포함한 넓은 스펙트럼의 사실적인 표정을 캡처하고 자연스러운 머리 움직임을 용이하게 하여 생성된 머리 비디오를 비교할 수 없는 수준의 표현성으로 삽입하도록 설계된 혁신적인 대화 헤드 프레임워크를 확립하는 것을 목표로 한다. 이러한 목적을 달성하기 위해 본 논문에서는 주어진 영상과 오디오 클립으로부터 캐릭터 헤드 영상을 직접 합성할 수 있는 확산 모델의 생성력을 활용하는 방법을 제안한다. 이 접근법은 오디오 입력에 존재하는 뉘앙스와 밀접하게 정렬된 높은 정도의 시각적 및 정서적 충실도를 나타내는 대화 헤드 비디오의 생성을 간소화하는 중간 표현 또는 복잡한 전처리의 필요성을 제거한다. 오디오 신호는 얼굴 표정과 관련된 정보가 풍부하여 이론적으로 모델이 다양한 표현적 얼굴 움직임을 생성할 수 있다. 그러나 오디오와 Diffusion Models을 통합하는 것은 오디오와 얼굴 표정의 매핑에 내재된 모호성으로 인해 쉬운 작업이 아니다. 이 문제는 모델에 의해 생성된 비디오의 불안정성으로 이어질 수 있으며, 비디오 프레임 간의 얼굴 왜곡 또는 지터링으로 나타나며 심각한 경우 비디오의 완전한 붕괴를 초래할 수도 있다. 이 문제를 해결하기 위해, 우리는 생성 과정에서 안정성을 향상시키기 위해 속도 제어기와 얼굴 영역 제어기와 같은 안정적인 제어 메커니즘을 모델에 통합했다. 이 두 컨트롤러는 하이퍼파라미터로 기능하여 최종 생성된 비디오의 다양성과 표현력을 손상시키지 않는 미묘한 제어 신호로 작용한다. 또한, 생성된 비디오의 캐릭터가 입력 참조 이미지와 일관되게 유지되도록 하기 위해, 우리는 비디오 전체에 걸쳐 캐릭터의 신원을 보존하기 위한 유사한 모듈인 FrameEncoding을 설계함으로써 ReferenceNet의 접근 방식을 채택 및 향상시켰다.\n' +
      '\n' +
      '마지막으로, 모델을 훈련하기 위해 250시간 이상의 영상과 1억 5천만 개 이상의 이미지를 축적하는 방대하고 다양한 오디오 비디오 데이터 세트를 구성했다. 이 확장된 데이터 세트는 연설, 영화 및 텔레비전 클립, 노래 공연을 포함한 광범위한 콘텐츠를 포함하며 중국어 및 영어와 같은 여러 언어를 포함한다. 풍부한 다양한 말하기와 노래 비디오는 우리의 훈련 자료가 인간의 표현과 발성 스타일의 광범위한 스펙트럼을 포착하여 EMO 발전을 위한 견고한 기반을 제공한다. 우리는 HDTF 데이터 세트에 대해 광범위한 실험과 비교를 수행했으며, 여기서 접근 방식은 FID, SyncNet, F-SIM 및 FVD와 같은 여러 메트릭에 걸쳐 드림톡, Wav2Lip 및 SadTalker를 포함한 현재 최첨단(SOTA) 방법을 능가했다. 정량적인 평가 외에도 종합적인 사용자 연구와 정성적인 평가도 수행했는데, 우리의 방법은 매우 자연스럽고 표현력 있는 대화와 심지어 노래하는 비디오를 생성할 수 있어 현재까지 최고의 결과를 얻을 수 있음을 보여주었다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**확산 모델**확산 모델은 이미지 합성[4, 8], 이미지 편집[10, 24], 비디오 생성[6, 9] 및 심지어 3D 콘텐츠 생성[12, 17]을 포함한 다양한 도메인에 걸쳐 현저한 능력을 입증했다. 그 중 대표적인 사례로는 SD(Stable Diffusion)[20]이 두드러지며, UNet 아키텍처를 사용하여 대규모 텍스트-이미지 데이터셋에 대한 광범위한 훈련에 이어 주목할만한 텍스트-이미지 기능을 가진 이미지를 반복적으로 생성한다[23]. 이러한 사전 훈련된 모델은 다양한 이미지 및 비디오 생성 작업에서 광범위한 적용을 발견했다[6, 9]. 또한, 최근 일부 연구에서는 UNet을 Transformer를 이용하여 시간 모듈 및 3D Convolutions로 변경하여 대규모 데이터 및 모델 파라미터를 지원할 수 있는 DiT(Diffusion-in-Transformer) [16]을 채택하였다. 전체 텍스트 대 비디오 모델을 처음부터 교육함으로써, 우수한 비디오 생성 결과를 달성한다[14]. 또한, 몇 가지 노력은 대화 헤드 생성을 위한 확산 모델을 적용하는 데 파고들어 실제 대화 헤드 비디오를 만드는 데 이러한 모델의 능력을 강조하는 유망한 결과를 산출했다[15, 27].\n' +
      '\n' +
      '**오디오 기반 대화 헤드 생성**오디오 기반 대화 헤드 생성은 크게 두 가지 접근 방식으로 분류될 수 있는데, 비디오 기반 방법[5, 18, 30]과 단일 이미지[15, 28, 33]이다. 비디오 기반 대화 헤드 생성은 입력 비디오 세그먼트에 대한 직접 편집을 허용한다. 예를 들어, Wav2Lip[18]은 오디오-립 싱크를 위한 판별기를 사용하여 오디오를 기반으로 비디오에서 입술 움직임을 재생성한다. 그 한계는 베이스 비디오에 의존하여 고정된 머리 움직임으로 이어지고 입 움직임만 발생하여 현실감을 제한할 수 있다. 단일 이미지 대화 헤드 생성을 위해, 참조 사진이 사진의 외관을 반영하는 비디오를 생성하기 위해 활용된다. [28] 블렌드 쉐이프와 헤드 포즈를 학습하여 헤드 모션과 얼굴 표정을 독립적으로 생성하도록 제안합니다. 그런 다음 이들은 최종 비디오 프레임 생성을 안내하는 중간 표현으로서 기능하는 3D 얼굴 메시를 생성하는 데 사용된다. 유사하게, [33]은 대화 헤드 비디오를 생성하기 위한 중간 표현으로서 3D Morphable Model(3DMM)을 채용한다. 이러한 방법의 일반적인 문제는 생성된 비디오의 전체적인 표현성과 사실성을 제한하는 3D 메쉬의 제한된 표현 능력이다. 추가적으로, 두 방법 모두 비확산 모델에 기초하고 있으며, 이는 생성된 결과의 성능을 더욱 제한한다. [15] 말하는 머리 생성을 위해 확산 모델을 사용하려는 시도이지만 이미지 프레임에 직접 적용하는 대신 3DMM에 대한 계수를 생성하기 위해 사용한다. 드림토크는 앞의 두 가지 방법에 비해 결과가 다소 개선되지만 여전히 높은 자연스러운 얼굴 영상 생성에는 미치지 못한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '캐릭터 초상화의 단일 참조 이미지가 주어지면, 제안된 방법은 입력 음성 오디오 클립과 동기화된 비디오를 생성할 수 있으며, 제공된 음성 오디오의 톤 분산과 조화롭게 자연스러운 머리 움직임과 생생한 표현을 보존할 수 있다. 캐스케이드 비디오의 끊김 없는 시리즈를 생성함으로써, 본 모델은 일관성 있는 아이덴티티와 일관성 있는 모션으로 긴 시간 동안 대화하는 인물 비디오의 생성을 용이하게 하며, 이는 현실적인 응용에 매우 중요하다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '본 연구의 방법론으로는 SD(Stable Diffusion)를 기반으로 한다. SD는 LDM(Latent Diffusion Model)[20]에서 진화하는 널리 활용되는 T2I(Text-to-Image) 모델이다. 오토인코더 Variational Autoencoder(VAE)[11]을 이용하여 원본 영상 특징 분포 \\(x_{0}\\)를 잠재 공간 \\(z_{0}\\)으로 매핑하고, 영상을 \\(z_{0}=\\mathbf{E}(x_{0})\\)으로 인코딩하고 잠재 특징을 \\(x_{0}=\\mathbf{D}(z_{0})\\으로 재구성한다. 이 아키텍처는 높은 시각적 충실도를 유지하면서 계산 비용을 줄일 수 있는 이점을 제공한다. DDPM(Denoising Diffusion Probabilistic Model)[8] 또는 DDIM(Denoising Diffusion Implicit Model)[26] 접근법에 기초하여 SD는 특정 타임스텝(t\\)에서 잡음이 있는 잠재 잠재(z_{0}\\)를 생성하기 위해 가우시안 잡음(\\epsilon\\)을 잠재 잠재(z_{t}\\)에 도입한다. 추론 과정에서 SD는 잠재 \\(z_{t}\\)에서 잡음 \\(\\epsilon\\)을 제거하고 텍스트 특징을 통합하여 원하는 결과를 얻기 위해 텍스트 제어를 통합한다. 이러한 잡음 제거 과정에 대한 훈련 목표는 다음과 같이 표현된다:\n' +
      '\n' +
      '\\mathcal{L}=\\mathbb{E}_{t,c,z_{t},\\epsilon}\\left[||\\epsilon-\\epsilon_{\\theta}(z_{t},t,c||^{2}\\right] \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(c\\)는 텍스트 특징들을 나타내며, 이는 CLIP [19] ViT-L/14 텍스트 인코더를 통해 토큰 프롬프트로부터 얻어진다. SD에서, \\(\\epsilon_{\\theta}\\)는 수정된 UNet [21] 모델을 통해 실현되며, 교차 주의 메커니즘을 사용하여 잠재 특징과 융합한다.\n' +
      '\n' +
      '### Network Pipelines\n' +
      '\n' +
      '우리의 방법의 개요는 그림 2에 나와 있다. 우리의 **백본 네트워크**는 다중 프레임 잡음 잠재 입력을 얻고, 각 시간 단계 동안 연속적인 비디오 프레임에 대해 그것들을 잡음 제거하려고 시도한다. 백본 네트워크는 유사한 것을 갖는다.\n' +
      '\n' +
      '그림 2: 제안된 방법의 개요. 우리의 프레임워크는 주로 두 단계로 구성된다. 프레임 인코딩이라는 초기 단계에서 참조 이미지 및 모션 프레임으로부터 특징을 추출하기 위해 참조넷이 배치된다. 이어서, 확산 프로세스 단계 동안, 사전 훈련된 오디오 인코더는 오디오 임베딩을 처리한다. 얼굴 영역 마스크는 얼굴 이미지의 생성을 제어하기 위해 다중 프레임 노이즈와 통합된다. 그 다음에는 노이즈 제거 작업을 용이하게 하기 위해 백본 네트워크를 사용한다. 백본 네트워크 내에서, 두 가지 형태의 주의 메커니즘이 적용된다: 참조-주의 및 오디오-주의. 이러한 메커니즘은 캐릭터의 정체성을 보존하고 캐릭터의 움직임을 조절하는 데 각각 필수적이다. 또한, 시간적인 차원을 조작하고 움직임의 속도를 조절하기 위해 시간 모듈을 사용한다.\n' +
      '\n' +
      '원래의 SD 1.5를 사용한 UNet 구조 구성 1) 이전 작업과 유사하게 생성된 프레임 간의 연속성을 보장하기 위해 백본 네트워크는 **시간 모듈**을 내장한다. 2) 생성된 프레임에서 인물상의 ID 일관성을 유지하기 위해 Backbone과 평행하게 **ReferenceNet**이라는 UNet 구조를 배치하고, 참조 이미지를 입력하여 참조 특징을 얻는다. 3) 캐릭터 말하기 동작을 구동하기 위해, **오디오 레이어**가 음성 특징을 인코딩하기 위해 활용된다. 4) 말하는 캐릭터의 동작을 제어가능하고 안정되게 하기 위해, 약한 조건들을 제공하기 위해 **얼굴 로케이터** 및 **속도 레이어**를 사용한다.\n' +
      '\n' +
      '#####3.2.1 백본 네트워크.\n' +
      '\n' +
      '따라서 본 논문에서는 SD 1.5 UNet 구조의 cross-attention layer를 reference-attention layer에 적응시켰다. 이러한 수정된 도면층은 이제 텍스트 임베딩이 아닌 입력으로서 ReferenceNet의 참조 기능을 취한다.\n' +
      '\n' +
      '####3.2.2 오디오 레이어.\n' +
      '\n' +
      '음성에서의 발음과 톤은 생성된 문자의 모션에 대한 주 구동 부호이다. 사전 훈련된 wav2vec[22]의 다양한 블록들에 의해 입력 오디오 시퀀스로부터 추출된 특징들을 연결하여 \\(f_{th}\\) 프레임에 대한 오디오 표현 임베딩 \\(A^{(f)}\\)을 산출한다. 그러나 움직임은 말하기 전에 입을 열고 흡입하는 것과 같은 미래/과거 오디오 세그먼트에 의해 영향을 받을 수 있다. 이를 해결하기 위해, 주변 프레임의 특징을 연결하여 생성된 각 프레임의 음성 특징을 정의한다. \\(A^{(f)}_{gen}=\\oplus\\{A^{(f-m)},...A^{(f)},...A^{(f+m)}\\}\\), \\(m\\)은 한 측면에서 추가된 특징의 수이다. 음성 특징을 생성 과정에 주입하기 위해 Backbone Network에서 각 ref-attention 계층 후에 잠재 코드와 \\(A_{gen}\\) 사이의 cross attention 메커니즘을 수행하는 오디오-attention 계층을 추가한다.\n' +
      '\n' +
      '#### 3.2.3 ReferenceNet.\n' +
      '\n' +
      'ReferenceNet은 Backbone Network와 동일한 구조를 가지고 있으며 입력 영상에서 세부적인 특징을 추출하는 역할을 한다. ReferenceNet과 Backbone Network가 모두 동일한 원래의 SD 1.5 UNet 아키텍처에서 유래한다는 점을 감안할 때, 특정 계층에서 이 두 구조에 의해 생성된 특징 맵은 유사성을 나타낼 가능성이 높다. 결과적으로, 이것은 참조넷에 의해 추출된 특징들의 백본 네트워크의 통합을 용이하게 한다. 선행 연구 [9, 35]는 대상 객체의 정체성의 일관성을 유지하는 데 유사한 구조를 활용하는 데 지대한 영향을 강조하고 있다. 본 연구에서 ReferenceNet과 Backbone Network는 모두 원래의 SD UNet으로부터 가중치를 상속받는다. 타겟 캐릭터의 이미지는 레퍼런스넷에 입력되어 셀프 어텐션 레이어에서 출력되는 레퍼런스 특징맵을 추출한다. 백본 잡음 제거 절차 동안, 대응하는 계층들의 특징들은 추출된 특징 맵들과 함께 참조-어텐션 계층들을 겪는다. 레퍼런스넷은 주로 개별 이미지를 처리하도록 설계되었기 때문에 백본에서 발견되는 시간적 계층이 부족하다.\n' +
      '\n' +
      '시간 모듈.대부분의 작업들은 연속적인 비디오 프레임들 사이의 시간 관계에 대한 이해와 인코딩을 용이하게 하기 위해 미리 훈련된 텍스트-이미지 아키텍처에 시간 혼합 계층을 삽입하려고 시도한다. 이렇게 함으로써, 향상된 모델은 프레임들에 걸쳐 연속성 및 일관성을 유지할 수 있고, 그 결과 매끄럽고 일관된 비디오 스트림들을 생성할 수 있다. AnimateDiff의 건축적 개념에 기초하여, 우리는 프레임 내의 특징들에 자기 주의 시간 계층들을 적용한다. 구체적으로, 입력 특징 맵 \\(x\\in\\mathbb{R}^{b\\times c\\times f\\times h\\times w}\\)을 모양 \\((b\\times h\\times w)\\times f\\times c\\)으로 재구성한다. 여기서, \\(b\\)은 배치 크기를 의미하며, \\(h\\)과 \\(w\\)은 특징 맵의 공간 차원을 나타내며, \\(f\\)은 생성된 프레임의 개수, \\(c\\)은 특징 차원을 나타낸다. 특히, 비디오의 동적 콘텐츠를 효과적으로 캡처하기 위해 시간적 차원 \\(f\\)에 걸쳐 자기 주의를 지시한다. 시간 계층은 백본 네트워크의 각 해상도 계층에서 삽입됩니다. 현재의 대부분의 확산-기반 비디오 생성 모델들은 미리 결정된 수의 프레임들을 생성하도록 그들의 설계에 의해 본질적으로 제한되며, 이에 의해 확장된 비디오 시퀀스들의 생성을 제약한다. 이러한 제한은 의미 있는 말의 조음을 위해 충분한 지속 시간이 필수적인 대화 헤드 비디오의 적용에서 특히 영향을 미친다. 일부 방법론은 연결된 세그먼트에 걸쳐 원활한 전환을 유지하는 것을 목표로 후속 세대의 초기 프레임으로서 선행 클립의 끝으로부터의 프레임을 사용한다. 이에 착안하여, 기존의 클립에서 \'움직임 프레임\'이라고 하는 마지막 \\(n\\) 프레임을 통합하여 크로스 클립 일관성을 향상시킨다. 구체적으로, 이러한 \\(n\\) 모션 프레임들은 참조넷에 공급되어 다중 해상도 모션 특징 맵들을 사전 추출한다. 백본 네트워크 내의 잡음 제거 과정 동안, 프레임 차원을 따라 정합 해상도의 미리 추출된 움직임 특징과 시간 계층 입력을 병합한다. 이 간단한 방법은 다양한 클립 간의 일관성을 효과적으로 보장합니다. 첫 번째 비디오 클립 생성을 위해 모션 프레임을 제로 맵으로 초기화한다.\n' +
      '\n' +
      '백본 네트워크가 잡음이 많은 프레임들을 잡음제거하기 위해 여러 번 반복될 수 있지만, 타겟 이미지 및 모션 프레임들은 연결되고 참조넷에 한 번만 입력된다는 것에 유의해야 한다. 결과적으로, 추출된 특징들은 프로세스 전반에 걸쳐 재사용되어, 추론 동안 계산 시간의 실질적인 증가가 없음을 보장한다.\n' +
      '\n' +
      '####4.2.2 페이스 로케이터 및 스피드 레이어.\n' +
      '\n' +
      '시간 모듈들은 생성된 프레임들의 연속성과 비디오 클립들 사이의 끊김 없는 전이를 보장할 수 있지만, 독립적인 생성 프로세스로 인해 생성된 캐릭터의 모션의 일관성과 안정성을 보장하기에는 불충분하다. 기존의 연구들은 골격[9], 블렌드쉐이프[33], 또는 3DMM[28]과 같은 캐릭터 동작을 제어하기 위해 일부 신호를 사용하지만, 이러한 제어 신호를 사용하는 것은 제한된 자유도로 인해 생동감 있는 표정 및 동작을 생성하는 데 좋지 않을 수 있으며, 훈련 단계 동안 부적절한 라벨링은 얼굴 동역학의 전체 범위를 포착하기에 불충분하다. 또한, 동일한 제어 신호는 서로 다른 문자 간의 불일치를 초래하여 개별 뉘앙스를 설명하지 못할 수 있다. 제어 신호들의 생성을 가능하게 하는 것은 실행 가능한 접근법(28)일 수 있지만, 실제와 같은 동작을 생성하는 것은 여전히 과제로 남아 있다. 따라서, 우리는 "약한" 제어 신호 접근법을 선택한다.\n' +
      '\n' +
      '구체적으로, 그림 2와 같이 비디오 클립의 얼굴 경계 박스 영역을 포함하는 얼굴 영역으로 마스크 \\(M=\\bigcup_{i=0}^{f}M^{i}\\)을 활용한다. 경계 박스 마스크를 인코딩하도록 설계된 경량 컨볼루션 레이어로 구성된 Face Locator를 사용한다. 그 후, 결과 인코딩된 마스크는 백본으로 공급되기 전에 잡음 잠재 표현에 추가된다. 이러한 방식으로, 우리는 마스크를 사용하여 캐릭터 얼굴이 생성되어야 하는 위치를 제어할 수 있다.\n' +
      '\n' +
      '그러나, 클립들 사이에 일관되고 매끄러운 모션을 생성하는 것은 별개의 생성 프로세스들 동안 헤드 모션 주파수의 변동으로 인해 어렵다. 이 문제를 해결하기 위해 목표 헤드 모션 속도를 세대에 통합합니다. 더 정확하게는 프레임에서의 머리 회전 속도\\(w^{f}\\)를 고려하고 속도 범위를 각각 다른 속도 레벨을 나타내는\\(d\\) 이산 속도 버킷으로 나눈다. 각 버킷은 중심값 \\(c^{d}\\)과 반지름 \\(r^{d}\\)을 갖는다. 우리는 벡터 \\(S=\\{s^{d}\\}in\\mathbb{R}^{d}\\)에 \\(w^{f}\\)을 재표적화하는데, 여기서 \\(s^{d}=\\tanh((w^{f}-c^{d})/r^{d}*3)\\이다. 오디오 레이어에서 사용되는 방법과 유사하게 각 프레임에 대한 헤드 회전 속도 임베딩은 \\(S^{f}=\\oplus\\{S^{(f-m)},\\dots,S^{(f)},\\dots,S^{(f+m)}\\}, 그리고 \\(S^{f}\\in\\mathbb{R}^{b\\times f^{c^{speed}}\\)에 의해 후속적으로 처리되어 속도 특징을 추출한다. 시간 계층 내에서 우리는 \\(S^{f}\\)의 모양을 \\(b\\times h\\times w\\times f\\times c^{speed}\\)으로 반복하고, 시간 차원 \\(f\\)에 걸쳐 속도 특징과 형상화된 특징 맵 사이에서 동작하는 교차 주의 메커니즘을 구현한다. 이렇게 하고 목표 속도를 지정함으로써 생성된 캐릭터의 머리의 회전 속도와 빈도를 서로 다른 클립에 걸쳐 동기화할 수 있다. 페이스 로케이터에 의해 제공되는 얼굴 위치 제어와 결합하여, 결과적인 출력은 안정적이고 제어가능할 수 있다.\n' +
      '\n' +
      '또한, 특정된 얼굴 영역 및 할당된 속도는 강력한 제어 조건을 구성하지 않는다는 점에 유의해야 한다. 얼굴 로케이터의 맥락에서, \\(M\\)은 전체 비디오 클립의 결합 영역이기 때문에, 얼굴 움직임이 허용되는 크기 영역을 나타내고, 이에 의해 머리가 정적 자세로 제한되지 않도록 보장한다. 속도 계층과 관련하여, 데이터세트 라벨링에 대한 인간 머리 회전 속도를 정확하게 추정하는 것이 어렵다는 것은 예측된 속도 시퀀스가 본질적으로 잡음이 있다는 것을 의미한다. 결과적으로, 생성된 헤드 모션은 지정된 속도 레벨에만 근사할 수 있다. 이러한 제한은 스피드 버킷 프레임워크의 설계에 동기를 부여합니다.\n' +
      '\n' +
      '### Training Strategies\n' +
      '\n' +
      '훈련 과정은 세 단계로 구성되어 있다. 첫 번째 단계는 이미지 사전 훈련으로, 백본 네트워크, 참조넷 및 얼굴 로케이터가 훈련에 토큰이 되며, 이 단계에서 백본은 입력으로 단일 프레임을 취하는 반면 참조넷은 동일한 비디오 클립에서 무작위로 선택된 별개의 프레임을 처리한다. 백본과 참조넷은 모두 원래 SD에서 가중치를 초기화한다. 두 번째 단계에서는 시간 모듈과 오디오 레이어가 통합된 비디오 트레이닝을 소개하고, 비디오 클립에서 연속 프레임들을 샘플링하고, 시작 프레임들은 모션 프레임들이다.\n' +
      '\n' +
      '시간 모듈들은 AnimateDiff[6]로부터 가중치들을 초기화한다. 마지막 단계에서는 속도 계층을 통합하고, 이 단계에서는 시간 모듈과 속도 계층만 학습한다. 이 전략적 결정은 훈련 과정에서 의도적으로 오디오 계층을 생략합니다. 말하는 캐릭터의 표정, 입 움직임, 머리 움직임의 빈도는 주로 오디오에 의해 영향을 받기 때문이다. 결과적으로, 이들 요소들 사이에 상관관계가 있는 것으로 보이며, 모델은 오디오가 아닌 속도 신호에 기초하여 캐릭터의 동작을 구동하도록 프롬프트될 수 있다. 실험 결과는 속도와 오디오 레이어의 동시 훈련이 캐릭터의 모션에 대한 오디오의 구동 능력을 약화시킨다는 것을 시사한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementations\n' +
      '\n' +
      '우리는 인터넷에서 약 250시간의 대화 헤드 비디오를 수집하고 이를 HDTF[34] 및 VFHQ[31] 데이터 세트로 보완하여 모델을 훈련했다. VFHQ 데이터셋은 오디오가 부족하기 때문에 첫 번째 훈련 단계에서만 사용된다. 얼굴 경계 박스 영역을 획득하기 위해 MediaPipe 얼굴 검출 프레임워크[13]를 적용한다. 얼굴 랜드마크를 사용하여 각 프레임에 대한 6DoF 머리 포즈를 추출한 다음 연속 프레임 간의 회전도를 계산하여 머리 회전 속도를 레이블링했다.\n' +
      '\n' +
      '데이터 세트로부터 샘플링된 비디오 클립은 \\(512\\times 512\\)으로 크기 조정되고 크롭된다. 첫 번째 훈련 단계에서는 비디오 클립에서 기준 영상과 목표 프레임을 분리하여 샘플링하고, Backbone Network와 ReferneceNet을 48의 배치 크기로 훈련하였으며, 두 번째와 세 번째 단계에서는 생성된 비디오 길이로 \\(f=12\\)을 설정하고, 모션 프레임 수를 \\(n=4\\)으로 설정하여 4의 배스 크기를 훈련에 적용하였다. 추가 특징 수 \\(m\\)는 Diffused Heads[27] 다음에 2로 설정된다. 모든 단계에 대한 학습률은 1e-5로 설정하였으며, 추론 과정에서 DDIM의 샘플링 알고리즘을 사용하여 40단계 동안 비디오 클립을 생성하고 프레임 생성마다 일정한 속도 값을 할당한다. 제안된 방법의 시간 소모는 한 회분(\\(f=12\\) 프레임)에 대해 약 15초이다.\n' +
      '\n' +
      '### Experiments Setup\n' +
      '\n' +
      '방법 비교를 위해 HDTF 데이터 세트를 분할하고 테스트 세트로 10%를 할당하고 교육을 위해 나머지 90%를 예약했다. 우리는 이 두 부분 집합 사이에 문자 ID가 중복되지 않도록 예방 조치를 취했다.\n' +
      '\n' +
      '우리는 Wav2Lip[18], SadTalker[33], DreamTalk[15]를 포함한 몇몇 이전 작업들과 우리의 방법들을 비교한다. 또한, Diffused Heads [27]에서 릴리즈된 코드를 사용하여 결과를 생성하였으나, 그린 배경만을 포함하는 CREMA [1] 데이터셋에 대해 모델을 학습하고, 생성된 결과는 최적이 아니다. 또한 생성된 프레임에 걸친 오류 누적에 의해 결과가 손상되었다. 따라서 우리는 확산 헤드 접근법과 정성적 비교만 수행한다. 드림톡의 경우 원저자가 규정한 대화 스타일 매개변수를 활용한다.\n' +
      '\n' +
      '제안된 방법의 우수성을 입증하기 위해, 우리는 몇 가지 정량적 메트릭으로 모델을 평가한다. 생성된 프레임의 품질을 평가하기 위해 FID(Frechet Inception Distance)[7]을 사용한다[32]. 또한, 생성된 프레임들과 참조 영상간의 얼굴 특징을 추출하여 비교함으로써 얼굴 유사도(F-SIM)를 계산하였다. 단일, 가변적이지 않은 참조 이미지를 사용하는 것은 기만적으로 완벽한 F-SIM 점수를 초래할 수 있다는 점에 유의하는 것이 중요하다. 특정 방법 [18]은 입 영역만 생성하여 나머지 프레임을 참조 이미지와 동일하게 하여 결과를 왜곡할 수 있다. 따라서 우리는 F-SIM을 모집단 참조 지수[27]로 취급하며 해당 Ground truth(GT) 값에 더 가까운 근사치는 더 나은 성능을 나타낸다. 비디오 수준 평가를 위해 FVD(Frechet Video Distance) [29]를 추가로 사용했다. SyncNet[2] 점수는 말하는 머리 응용 프로그램의 중요한 측면인 입술 동기화 품질을 평가하는 데 사용되었다. 생성된 동영상에서 얼굴 표정의 표현성을 평가하기 위해 표정-FID(E-FID) 메트릭의 사용을 소개한다. 이것은 [3]에 설명된 바와 같이 얼굴 재구성 기술을 통해 표현 매개변수를 추출하는 것을 포함한다. 그런 다음, 합성 비디오의 표현과 Ground truth 데이터 세트의 표현 사이의 발산을 정량적으로 측정하기 위해 이러한 표현 매개변수의 FID를 계산한다.\n' +
      '\n' +
      '### Qualitative Comparisons\n' +
      '\n' +
      '그림 3은 이전 접근법과 함께 우리의 방법의 시각적 결과를 보여준다. Wav2Lip은 일반적으로 흐릿한 입 영역을 합성하고 단일 참조 이미지가 입력으로 제공될 때 정적 머리 포즈 및 최소 눈 움직임을 특징으로 하는 비디오를 생성하는 것이 관찰 가능하다. 드림토크[15]의 경우 저자가 제공한 스타일 클립은 원래 얼굴을 왜곡할 수 있고 얼굴 표정과 머리 움직임의 역동성을 제약할 수 있다. SadTalker와 DreamTalk와는 달리, 제안된 방법은 더 넓은 범위의 머리 움직임과 더 역동적인 얼굴 표정을 생성할 수 있다. 우리는 문자 모션을 제어하기 위해 직접 신호, 예를 들어 블렌드 쉐이프 또는 3DMM을 사용하지 않기 때문에 이러한 모션은 오디오에 의해 직접 구동되며, 이는 다음 쇼케이스에서 자세히 논의될 것이다.\n' +
      '\n' +
      '다양한 초상화 스타일에 걸쳐 말하는 헤드 비디오의 생성을 더 탐구합니다. 그림 4에 도시된 바와 같이, 시비타이에서 조달된 참조 이미지는 다양한 스타일의 캐릭터, 즉 사실적, 애니메이션 및 3D를 포함하는 이질적인 텍스트 대 이미지(T2I) 모델에 의해 합성된다. 이러한 캐릭터들은 동일한 보컬 오디오 입력들을 사용하여 애니메이션화되어, 상이한 스타일들에 걸쳐 대략 균일한 입술 동기화를 초래한다. 비록 우리의 모델은 사실적인 비디오에 대해서만 훈련되지만, 그것은 다양한 초상화 유형에 대해 말하는 머리 비디오를 생산하는 능숙함을 보여준다.\n' +
      '\n' +
      '도 5는 우리의 방법이 뚜렷한 톤으로 오디오를 프로세싱할 때 더 풍부한 표정 및 움직임을 생성할 수 있음을 입증한다\n' +
      '\n' +
      '특징. 예를 들어, 세 번째 행의 예는 고음의 성조가 등장인물들로부터 더 강렬하고 애니메이션적인 표현들을 이끌어낸다는 것을 드러낸다. 또한, 모션 프레임을 활용하여 생성된 비디오의 확장을 가능하게 하며, 입력 오디오의 길이에 따라 지속 시간이 긴 비디오를 생성할 수 있다. 그림 5와 그림 6에서 볼 수 있듯이, 우리의 접근법은 실질적인 움직임 속에서도 확장된 시퀀스에 비해 캐릭터의 정체성을 보존한다.\n' +
      '\n' +
      '### Quantitative Comparisons\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Method & FID\\(\\downarrow\\) & SyncNet\\(\\uparrow\\) & F-SIM & FVD\\(\\downarrow\\) & E-FID\\(\\downarrow\\) \\\\ \\hline Wav2Lip [18] & 9.38 & **5.79** & 80.34 & 407.93 & 0.693 \\\\ SadTalker [33] & 10.31 & 4.82 & 84.56 & 214.98 & 0.503 \\\\ DreamTalk [15] & 58.80 & 3.43 & 67.87 & 619.05 & 2.257 \\\\ GT & - & 7.3 & 77.44 & - & - \\\\ Ours & **8.76** & 3.89 & **78.96** & **67.66** & **0.116** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 여러 대화 헤드 생성과의 정량적 비교가 작동한다.\n' +
      '\n' +
      '그림 3: 여러 대화 헤드 생성과의 질적 비교가 작동합니다.\n' +
      '\n' +
      '표 1에서 볼 수 있듯이, 우리의 결과는 더 낮은 FVD 점수에 의해 입증된 바와 같이 비디오 품질 평가에서 상당한 이점을 보여준다. 또한, 제안된 방법은 개선된 FID 점수로 나타낸 바와 같이 개별 프레임 품질 측면에서 다른 방법보다 우수하다. SyncNet 메트릭에서 가장 높은 점수를 얻지 못했음에도 불구하고, 우리의 접근법은 E-FID에 의해 보여지는 바와 같이 생동감 있는 표정을 생성하는 데 탁월하다.\n' +
      '\n' +
      '## 5 Limitation\n' +
      '\n' +
      '우리의 방법에는 몇 가지 제한 사항이 있다. 첫째, 확산 모델에 의존하지 않는 방법에 비해 시간이 많이 소요된다. 둘째, 캐릭터의 동작을 제어하기 위해 어떠한 명시적인 제어 신호도 사용하지 않기 때문에, 손과 같은 다른 신체 부위의 부주의한 생성을 초래하여 비디오의 아티팩트로 이어질 수 있다. 이 문제에 대한 한 가지 잠재적인 해결책은 특히 신체 부위에 대한 제어 신호를 사용하는 것이다.\n' +
      '\n' +
      '그림 4: **다른 초상화 스타일**을 기반으로 한 우리의 방법의 질적 결과. 여기에서 우리는 14개의 생성된 비디오 클립을 시연하며, 이 비디오 클립에서 캐릭터는 동일한 보컬 오디오 클립에 의해 구동된다. 생성된 각 클립의 지속 시간은 약 8초입니다. 공간 제약으로 인해 각 클립에서 4개의 프레임만 샘플링합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A., Verma, R.: Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing **5**(4), 377-390 (2014)\n' +
      '* [2] Chung, J.S., Zisserman, A.: Out of time: automated lip sync in the wild. In: Computer Vision-ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13. pp. 251-263. Springer (2017)\n' +
      '* [3] Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In: IEEE Computer Vision and Pattern Recognition Workshops (2019)\n' +
      '* [4] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis (2021)\n' +
      '* [5] Fan, Y., Lin, Z., Saito, J., Wang, W., Komura, T.: Faceformer: Speech-driven 3d facial animation with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022)\n' +
      '\n' +
      '그림 5: ** 긴 기간**에 걸쳐 ** 강한 톤 품질**을 갖는 음성 오디오에 대한 우리의 방법에 의해 생성된 결과. 각각의 클립에서, 캐릭터는 강한 톤 품질, 예를 들어 노래하는 오디오에 의해 구동되며, 각각의 클립의 지속기간은 대략 1분이다.\n' +
      '\n' +
      '도 6: Diffused Heads와의 비교 [27], 생성된 클립의 지속 시간은 6초이고, Diffused Heads의 결과는 낮은 해상도를 가지며 생성된 프레임에 걸친 에러 누적에 의해 손상된다.\n' +
      '\n' +
      '6] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., Dai, B.: Animatediff: 특정 튜닝 없이 개인화된 텍스트-투-이미지 확산 모델들을 애니메이션한다. arXiv preprint arXiv:2307.04725 (2023)\n' +
      '* [7] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems **30** (2017)\n' +
      '* [8] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [9] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117 (2023)\n' +
      '* [10] Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani, M.: Imagic: Text-based real image editing with diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6007-6017 (2023)\n' +
      '* [11] Kingma, D.P., Welling, M.: Auto-Encoding Variational Bayes. In: 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings (2014)\n' +
      '* [12] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Fidler, X.H.K.K.S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation\n' +
      '* [13] Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., Zhang, F., Chang, C.L., Yong, M., Lee, J., Chang, W.T., Hua, W., Georg, M., Grundmann, M.: Mediapipe: A framework for building perception pipelines (06 2019)\n' +
      '* [14] Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.F., Chen, C., Qiao, Y.: Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048 (2024)\n' +
      '* [15] Ma, Y., Zhang, S., Wang, J., Wang, X., Zhang, Y., Deng, Z.: Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767 (2023)\n' +
      '* [16] Peebles, W., Xie, S.: Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748 (2022)\n' +
      '* [17] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [18] Prajwal, K.R., Mukhopadhyay, R., Namboodiri, V.P., Jawahar, C.: A lip sync expert is all you need for speech to lip generation in the wild. In: Proceedings of the 28th ACM International Conference on Multimedia. p. 484-492. MM \'20, Association for Computing Machinery, New York, NY, USA (2020). [https://doi.org/10.1145/3394171.3413532](https://doi.org/10.1145/3394171.3413532), [https://doi.org/10.1145/3394171.3413532](https://doi.org/10.1145/3394171.3413532)\n' +
      '* [19] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [20] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* MICCAI 2015. pp. 234-241. Springer International Publishing, Cham (2015)\n' +
      '22] Schneider, S., Baevski, A., Collobert, R., Auli, M.: wav2vec: 음성 인식을 위한 비감독 사전 훈련. pp. 3465-3469 (09 2019). [https://doi.org/10.21437/Interspeech.2019-1873] (https://doi.org/10.21437/Interspeech.2019-1873)\n' +
      '* [23] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: Laion-5b: An open large-scale dataset for training next generation image-text models (2022)\n' +
      '* [24] Shi, Y., Xue, C., Pan, J., Zhang, W., Tan, V.Y., Bai, S.: Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435 (2023)\n' +
      '* [25] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. pp. 2256-2265. PMLR (2015)\n' +
      '* [26] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: International Conference on Learning Representations (2021), [https://openreview.net/forum?id=St1giafGHLP](https://openreview.net/forum?id=St1giafGHLP)\n' +
      '* [27] Stypulkowski, M., Vougioukas, K., He, S., Zieba, M., Petridis, S., Pantic, M.: Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation. In: [https://arxiv.org/abs/2301.03396](https://arxiv.org/abs/2301.03396) (2023)\n' +
      '* [28] Sun, X., Zhang, L., Zhu, H., Zhang, P., Zhang, B., Ji, X., Zhou, K., Gao, D., Bo, L., Cao, X.: Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior. arXiv preprint arXiv:2312.01841 (2023)\n' +
      '* [29] Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Fvd: A new metric for video generation (2019)\n' +
      '* [30] Wen, X., Wang, M., Richardt, C., Chen, Z.Y., Hu, S.M.: Photorealistic audio-driven video portraits. IEEE Transactions on Visualization and Computer Graphics **26**(12), 3457-3466 (2020). [https://doi.org/10.1109/TVCG.2020.3023573](https://doi.org/10.1109/TVCG.2020.3023573)\n' +
      '* [31] Xie, L., Wang, X., Zhang, H., Dong, C., Shan, Y.: Vfhq: A high-quality dataset and benchmark for video face super-resolution. In: The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2022)\n' +
      '* [32] Ye, Z., Zhong, T., Ren, Y., Yang, J., Li, W., Huang, J., Jiang, Z., He, J., Huang, R., Liu, J., et al.: Real3d-portrait: One-shot realistic 3d talking portrait synthesis. arXiv preprint arXiv:2401.08503 (2024)\n' +
      '* [33] Zhang, W., Cun, X., Wang, X., Zhang, Y., Shen, X., Guo, Y., Shan, Y., Wang, F.: Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8652-8661. IEEE Computer Society, Los Alamitos, CA, USA (jun 2023). [https://doi.org/10.1109/CVPR52729.2023.00836](https://doi.org/10.1109/CVPR52729.2023.00836), [https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00836](https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.00836)\n' +
      '* [34] Zhang, Z., Li, L., Ding, Y., Fan, C.: Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3661-3670 (2021)\n' +
      '* [35] Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M., Kemelmacher-Shlizerman, I.: Tryondiffusion: A tale of two unets. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4606-4615 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>