<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DiLightNet: 세립형 조명제어\n' +
      '\n' +
      '확산 기반 이미지 생성을 위한\n' +
      '\n' +
      '청종종({}^{1,2}\\) Yue Dong\\({}^{2}\\) Pieter Peers\\({}^{3}\\) Youkang Kong\\({}^{4,2}\\) Hongzhi Wu\\({}^{1}\\) Xin Tong\\({}^{2}\\)\n' +
      '\n' +
      'CAD와 CG의 State Key Lab, 저장대학교 \\({}^{2}\\)Microsoft Research Asia\n' +
      '\n' +
      '청화대학교 윌리엄&메리대학\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문은 텍스트 기반 확산 기반 이미지 생성 시 세밀한 조명 제어를 수행하는 새로운 방법을 제안한다. 기존의 확산 모델들은 이미 어떠한 조명 조건에서도 이미지를 생성할 수 있는 능력을 가지고 있지만, 추가적인 안내 없이 이러한 모델들은 이미지 콘텐츠와 조명을 상관시키는 경향이 있다. 또한 텍스트 프롬프트에는 상세한 조명 설정을 설명하는 데 필요한 표현력이 부족합니다. 콘텐츠 제작자에게 이미지 생성 시 조명에 대한 세밀한 제어를 제공하기 위해, 텍스트 프롬프트에 광채 힌트의 형태로 상세 조명 정보, 즉 대상 조명 아래에 균질한 표준 재료를 사용한 장면 기하학의 시각화를 확대한다. 그러나 광휘 힌트를 생성하는 데 필요한 장면 기하학은 알려져 있지 않다. 우리의 주요 관찰은 확산 과정을 안내하기만 하면 되므로 정확한 복사 힌트는 필요하지 않으며 확산 모델을 올바른 방향으로 가리키기만 하면 된다. 이러한 관찰을 바탕으로 영상 생성 시 조명을 제어하기 위한 3단계 방법을 소개한다. 첫 번째 단계에서는 제어되지 않는 조명 하에서 임시 이미지를 생성하기 위해 표준 사전 훈련된 확산 모델을 활용한다. 다음으로, 두 번째 단계에서는 임시 영상에서 추론된 전경 객체의 거친 모양에 대해 계산된 복사 힌트를 이용하여 목표 조명을 DiLightNet이라는 정제된 확산 모델에 전달하여 생성된 영상에서 전경 객체를 재합성하고 정제한다. 텍스처의 세부 정보를 유지하기 위해, 우리는 DiLightNet에 전달하기 전에 임시 합성 이미지의 신경 인코딩과 광도 힌트를 곱한다. 마지막으로, 세 번째 단계에서는 전경 객체의 조명과 일치하도록 배경을 재합성한다. 다양한 텍스트 프롬프트와 조명 조건에서 조명 제어 확산 모델을 시연하고 검증한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델[12, 21, 23, 26]과 같은 텍스트 기반 생성 기계 학습 방법은 간단한 텍스트 프롬프트로부터 환상적으로 상세한 이미지를 생성할 수 있다. 그러나 확산 모델도 편향으로 구축되었다. 예를 들어, Liu _et al_. [2023] 확산 모델은 이미지를 생성할 때 특정 시점을 선호하는 경향이 있음을 보여준다. 그림 2와 같이 이전에 보고되지 않은 또 다른 바이어스는 생성된 이미지에서의 조명이다. 더욱이, 이미지 콘텐츠와 조명은 높은 상관 관계를 갖는다. 확산\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '2023; Voynov et al.2023b] including non-rigid semantic edit [Cao et al.2023; Kawar et al.2023], modifying the identity and gender of subjects [Kim et al.2022], capturing the data distribution of underrepresented attributes [Cong et al.2023], and material properties [Sharma et al.2023]. 그러나, 연금술사 [Sharma et al.2023]을 제외하고, 이러한 방법들은 단지 중간 및 높은 레벨의 의미 제어를 제공한다. 연금술사와 유사하게, 우리의 방법은 사용자가 낮은 수준의 음영 특성을 제어할 수 있도록 권한을 부여하는 것을 목표로 한다. 투광성 및 광택과 같은 물질적 특성에 대한 상대적 제어를 제공하는 연금술사에게 보완적으로, 본 방법은 생성된 이미지에서 입사 조명에 대한 세밀한 제어를 제공한다.\n' +
      '\n' +
      '(스케치, 깊이, 또는 스트로크) 이미지들에 기초하여 합성 프로세스 동안 공간 제어를 제공하기 위해 대안적인 안내 메커니즘들이 도입되었다[Voynov et al.2023a; Ye et al.2023; Meng et al.2022], 아이덴티티[Ma et al.2023; Xiao et al.2023; Ruiz et al.2023b], 포토-컬렉션[Ruiz et al.2023a], 및 중간 레벨 정보[Ho and Salimans 2021; Zhang et al.2023b; Mou et al.2023]. 그러나, 이러한 방법들 중 어느 것도 입사 조명에 대한 제어를 제공하지 않는다. 우리는 유사한 과정을 따르고 제어넷[Zhang et al.2023b]을 통해 이미지의 신경 인코딩 버전에 의해 변조된 복사 힌트를 확산 모델에 주입한다.\n' +
      '\n' +
      '2D 확산 모델들은 또한 시점을 변경하거나 3D 모델들을 생성하기 위해 레버리지되었다[Liu et al.2023; Zhang et al.2023a; Watson et al.2022; Xiang et al.2023]. 그러나 이러한 방법은 입사 조명에 대한 제어를 제공하지 않으며 시점 간의 일관된 조명을 보장하지 않는다. Paint3D[Zeng et al.2023]는 주어진 메시의 UV 도메인에서 확산 알베도 텍스처를 직접 생성한다. Fantasia3D [Chen et al.2023] 및 MatLaber [Xu et al.2023]은 텍스트 대 이미지 2D 확산 모델을 활용하고 증류를 점수화하여 모양 및 공간적으로 변하는 BRDF의 형태로 더 풍부한 반사율 특성 세트를 생성한다. 확산 기반 SVBRDF 추정[Sartor and Peers2023; Vecchio et al.2023] 및 확산 기반 고유 분해[Kocsis et al.2023]도 텍스트 프롬프트가 아닌 사진으로부터도 풍부한 반사율 특성을 생성한다. 그러나, 이러한 모든 방법들은 간접 조명 및 그림자를 포함하는 외관을 시각화하기 위한 렌더링 알고리즘을 필요로 한다. 이와는 대조적으로, 본 방법은 샘플링 과정에서 조명을 직접 제어하여 확산 모델에 의해 내장된 그럴듯한 이미지 외관의 공간을 활용한다.\n' +
      '\n' +
      '단일 이미지 재조명은 구별되지만, 본 방법은 단일 이미지로부터의 재조명과 관련이 있으며, 이는 매우 구속되지 않는 문제이다. 추가적인 제약들을 제공하기 위해, 기존의 단일 이미지 방법들은 오직 어느 하나의 실외 장면들[Wu 및 Saito2017; Ture et al.2021; Yu et al.2020; Liu et al.2020a; Griffiths et al.2022], 얼굴들[Peers et al.2007; Wang et al.2008; Shu et al.2017; Sun et al.2019; Nestmeyer et al.2020; Pandey et al.2021; Han et al.2023; Ranjan et al.2023] 또는 인체 [Kanamori and Endo2018; Lagunas et al.2021; Ji et al.2022]에 초점을 맞춘다. 이와는 대조적으로, 우리의 방법은 일반 물체들의 세밀한 조명 제어를 제공하는 것을 목표로 한다. 또한, 기존의 방법은 입력으로서 기존 장면의 캡처된 사진을 기대하는 반면, 중요한 것은 우리의 방법이 아마도 믿을 수 없는 생성된 이미지에 대해 작동한다는 것이다. 대부분의 이전 단일 이미지 재조명 방법은 다양한 구성 요소에서 이미지를 명확하게 분해하고 조명을 변경한 후 재결합한다. 대조적으로, Sun _et al_. [2019]와 유사하다. 우리는 얽히지 않은 구성요소에서 입력 장면의 명시적 분해를 포기한다. 그러나 Sun_et al_와 달리, 우리는 특수하게 훈련된 인코더-디코더 모델을 사용하지 않고, 현실적인 재조명 이미지를 생성하기 위해 일반적인 생성 확산 모델에 의존한다. 또한, 기존의 단일 영상 재조명 기법들은 대부분 구형 고조파 부호화를 이용한 입사 조명을 나타낸다. 주목할만한 예외는 음영 이미지로 입사 조명을 나타내는 방법입니다. 그리피스 _et al_. [2022] 코사인 가중 섀도우 맵(정규 및 주 조명 방향과 함께)을 실외 장면에 대한 재조명 네트워크에 통과시킵니다. 마찬가지로, 카나모리 _et al_. [2018] and Ji _et al_. [2022] 쉐이딩 및 주변 폐색 맵들을 신경 렌더링 네트워크로 전달한다. 정반사를 더 잘 모델링하기 위해 Pandey _et al_. [2021] 및 라구나스 _et al_. [2021] 확산 쉐이딩 이미지 외에도, 인간 얼굴 및 전신 각각의 신경 재조명을 위한 하나 이상의 스페큘러 쉐이딩 이미지도 통과시킨다. 우리는 유사한 전략을 따르고 목표 조명을 확산 모델로 전달하고 (4) 정반사 광도는 이미지를 조건으로 확산 모델에 전달한다.\n' +
      '\n' +
      '확산 모델딩_et al_. [2023]을 이용한 재조명 표면 정상, 알베도 및 단일 사진에 피팅된 확산 음영 3D 변형 모델로부터 CGI-to-real 매핑을 학습함으로써 조명, 포즈 및 얼굴 표정을 변경한다[Feng et al.2021]. 입력된 사진에서 피사체의 신원을 보존하기 위해, 확산 모델은 피사체의 사진들의 작은 컬렉션(\\(\\sim\\)20) 상에서 정제된다. Ponglerntapakorn _et al_. [2023] leverage off-the-shelf 추정기 [Feng et al.2021; Deng et al.2019; Yu et al.2018] for the lighting, a 3D morphable model, the subject\'s identity, camera parameters, foreground mask, and cast-shadows to training the conditional diffusion network that take diffuse rendered model under the novel lighting (blended on the estimated background), and addition the identity, camera parameters, and target shadows to generate the relit image of the subject. 우리는 유사한 전체 전략을 따르지만 우리의 방법은 세 가지 임계점에서 다르다. 첫째, 본 논문에서 제안하는 방법은 얼굴에만 국한되지 않으며, 모양, 카메라, 조명 등의 엉킨 표현을 추출하지 않아도 된다. 둘째, 확산 과정에서 조명을 제어하기 위한 다중 복사 힌트(diffuse and specular)를 제공한다. 마지막으로, 제안하는 방법은 텍스트 프롬프트를 통해 생성된 이미지에서 순수하게 동작하며, 실제 촬영된 입력 사진을 요구하지 않는다.\n' +
      '\n' +
      '## 3 Overview\n' +
      '\n' +
      '제안된 방법은 원하는 이미지 내용을 설명하는 텍스트 프롬프트, 목표 조명, 모양과 질감의 변화를 제어하는 내용-시드, 빛-물질 상호작용의 변화를 제어하는 외관-시드를 입력으로 한다. 결과적인 출력은 텍스트 프롬프트에 대응하고 타겟 조명과 일치하는 생성된 이미지이다. 우리는 이미지에 고립된 전경 객체가 포함되어 있고 배경 내용이 표적 조명에 의해 암시적으로 묘사된다고 가정한다. 우리는 목표 조명에 대한 가정을 하지 않으며, 임의의 조명 조건을 지원한다. 마지막으로, 합성된 콘텐츠(예: 환상적인 짐승)의 사실성에 어떠한 제약도 부과하지 않지만, 물리적 기반 빛-물질 상호작용을 묘사하는 이미지 스타일을 가정한다(예: 셀-차광 또는 초현실적 이미지와 같은 예술적 스타일에서 조명 제어를 지원하지 않는다).\n' +
      '\n' +
      '조명 제어 프롬프트 구동 이미지 합성을 위한 우리의 파이프라인은 세 개의 개별 단계(그림 3)로 구성된다.\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. _Provisional Image Generation:_1단계에서 사전 학습된 확산 모델을 이용하여 텍스트-프롬프트와 내용-시드가 주어진 제어되지 않은 조명으로 임시 이미지를 생성한다[20]. 이 단계의 목표는 전경 객체의 모양과 질감을 결정하는 것이다. 선택적으로, 우리는 전경 검출을 용이하게 하기 위해 텍스트 프롬프트에 _"백색 배경"_를 추가한다.\n' +
      '3. _Synthesis with Radiance Hints:_ 2단계(섹션 4)에서는 먼저 임시 이미지와 목표 조명이 주어진 Radiance 힌트를 생성한다. 다음으로, 복사 힌트는 임시 이미지의 신경 인코딩 버전과 곱해지고 텍스트 프롬프트 및 외관 시드와 함께 DiLightNet으로 전달된다. 이 두 번째 단계의 결과는 일관된 조명을 가진 전경 객체이다.\n' +
      '4. _Background Inpainting:_ 3단계(섹션 5)에서는 타겟 조명과 일치하도록 배경을 인페인팅한다.\n' +
      '\n' +
      '##4 Radiance 힌트를 이용한 합성\n' +
      '\n' +
      '우리의 목표는 임시 이미지와 동일한 전경 객체를 가진 이미지를 합성하는 것이지만, 그 외관은 주어진 목표 조명과 일치합니다. 우리는 ControlNet[13]을 통해 목표 조명을 고려하여 임시 이미지를 생성하는 데 사용되는 동일한 확산 모델을 미세 조정할 것이다. ControlNet은 픽셀당 제어 신호를 가정하므로 환경 맵이나 구형 고조파 인코딩과 같은 조명의 직접적인 표현을 사용하여 확산 모델을 직접 안내할 수 없다. 대신, 우리는 각 픽셀의 나가는 광도에 대한 목표 조명의 _effect_를 광도 힌트를 사용하여 인코딩한다.\n' +
      '\n' +
      '반짝이 힌트 생성\n' +
      '\n' +
      '광선 힌트는 타겟 조명 아래의 타겟 형상의 시각화이며, 여기서 오브젝트의 재료는 균질한 프록시 재료(예를 들어, 균일한 확산)로 대체된다. 그러나 우리는 전경 객체의 모양에 접근할 수 없다. 이 문제를 피하기 위해 우리는 ControlNet이 일반적으로 매우 정확한 정보를 필요로 하지 않으며 스케치와 같은 희소 신호에서 잘 작동하는 것으로 나타났음을 관찰한다. 따라서, 우리는 모양에 대한 대략적인 추정으로부터 계산된 대략적인 광도 힌트가 충분하다고 주장한다.\n' +
      '\n' +
      '전경 객체의 형태를 추정하기 위해, 먼저 임시 영상에서 외곽의 두드러진 객체 검출 네트워크를 이용하여 전경 객체를 분할한다. 실제적으로 U2Net(14)은 속도와 정확성 사이에서 좋은 트레이드 오프를 제공하기 때문에, U2Net이 깨끗한 전경 분할을 제공하지 못하는 드문 경우에 SAM(15)으로 되돌아간다. 다음으로, 분할된 전경 객체에 다른 기성 깊이 추정 네트워크(ZoeDepth[1])를 적용한다. 추정된 깊이 맵은 후속적으로 메쉬에서 삼각측량되고 프록시 재료로 타겟 조명 아래에서 렌더링된다. 그러나, 단일 영상 깊이 추정은 어려운 문제이고, 결과적인 삼각화된 깊이 맵들은 완벽과는 거리가 멀다. 경험적으로 우리는 ControlNet이 결과 음영에서 저주파 오류에 덜 민감한 반면 음영에서 고주파 오류는 인공물로 이어질 수 있음을 발견했다. 따라서 고주파 불연속의 영향을 줄이기 위해 메쉬 위에 라플라스 평활 필터를 적용한다.\n' +
      '\n' +
      'NeRF [19]의 위치 인코딩에서 영감을 얻은 우리는 또한 별도의 광도 힌트로 전경 모양의 모양에 대한 표적 조명의 다른 주파수의 영향을 인코딩한다. BRDF가 입사 조명에 대역 통과 필터 역할을 한다는 사실을 이용하여, 디즈니 BRDF 모델 [1]로 모델링된 다른 재료(순수한 확산 재료 1개와 거칠기가 \\(0.34\\), \\(0.13\\), \\(0.05\\)로 설정된 세 개의 정재질로 각각 렌더링된 \\(5\\)의 복사 힌트를 생성한다. 블렌더의 사이클 경로 추적기로 그림자와 간접 조명을 포함한 광채 힌트를 렌더링합니다.\n' +
      '\n' +
      '###조명조건제어넷\n' +
      '\n' +
      '앞서 언급한 바와 같이, 우리는 제어넷을 사용하여 복사 힌트 이미지와 임시 이미지를 생성하는 데 사용된 원본 텍스트 프롬프트 및 외관 시드( appearance-seed)를 통합하기 위해 확산 모델을 미세 조정한다. 그러나 모델을 미세화함으로써 가영상과 같은 모양과 질감을 갖는 전경 객체를 생성할 수 있다는 보장은 없다. 따라서, 우리는 잠정 이미지를 확산 과정에 포함시키고자 한다. 그러나, 임시 이미지 내의 텍스처 및 형상 정보는 첫 단계부터 알려지지 않은 조명과 얽혀 있다. 먼저 가영상(분할 마스크에 알파 채널을 설정)을 인코딩하여 관련 텍스쳐와 모양 정보를 분리한다. 인코더는 Gao _et al_의 [2020] 지연 신경 재조명 아키텍처를 따르지만 메모리 사용을 제한하기 위해 채널 수를 줄였다. 또한, 임시 영상의 \\(12\\) 채널 부호화된 특징맵과 \\(4\\times 3\\) 채널 복사 힌트 사이의 채널별 곱셈을 포함하여 제어넷에 전달한다. 인코더 아키텍처는 그림 4에 요약되어 있다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'DiLightNet을 훈련하기 위해 조명, 기하학 및 재료 분포를 정확하게 제어할 수 있는 합성 3D 훈련 세트를 선택합니다. 합성 훈련 세트는 다양한 모양, 재료 및 조명을 포함하는 것이 중요하다.\n' +
      '\n' +
      'Shape and Material DiversityWe select the LVIS category in Objaverse dataset [Deitke et al.2022] also have a roughness map, normal map, or both, resulting the initial subset of \\(13K\\) objects. 또한 Objaverse 데이터세트(LVIS 카테고리)에서 확산 텍스쳐 맵만을 포함하는 \\(4K\\) 객체를 선택하고 \\([0.02,0.5]\\) 및 \\(1.0\\)으로 설정된 거칠기 로그-균일하게 선택된 균질한 정면 BRDF를 할당한다. 정제된 확산 모델이 균질한 물질을 가진 물체를 보았는지 확인하기 위해 LVIS 범주에서 추가로 \\(4K\\) 물체를 선택하고 이전과 같이 균질한 확산 알베도와 균질한 정면을 무작위로 할당한다.\n' +
      '\n' +
      '경험적으로, 우리는 Objaverse 데이터 세트에서 공간적으로 변하는 세부 자료의 다양성이 제한적이라는 것을 발견했다. 따라서 각 LVIS 범주에서 가장 "좋아요"(옵저버스 데이터 세트에 의해 제공되는 통계)가 많은 모양을 가진 데이터 세트를 추가로 보강한다. 이 선택된 각 모양에 대해 블렌더를 사용하여 UV 좌표를 자동으로 생성하고(이 단계가 실패한 모양(\\(17\\))을 제거하고 INRIA-Highres SVBRDF 데이터세트[Deschaintre et al.2020]에서 무작위로 선택된 공간적으로 변하는 물질을 할당하여 모양당 \\(4\\) 합성 객체를 생성하여 향상된 물질로 총 \\(4K\\)의 추가 객체를 생성한다.\n' +
      '\n' +
      '전체적으로, 우리의 훈련 세트는 다양한 모양과 재료를 가진 \\(25K\\) 합성 물체를 포함한다. 우리는 각 물체의 경계 구가 반지름이 0.5m인 원점을 중심으로 축척되고 변환된다.\n' +
      '\n' +
      '조명 다양성 5가지 조명 범주를 고려합니다.\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. _Point Light Source_random sampling with \\(0\\leq\\theta\\leq 60^{\\circ}\\) with the upper hemisphere and \\([4m,5m]\\) and the power with uniformly chosen \\([500W,1500W]\\) 점광이 물체 뒤에 위치할 때 완전히 검은 이미지를 피하기 위해 균일한 흰색도 추가한다.\n' +
      '\n' +
      '도 4: 임시 이미지 인코더 아키텍처. 인코더의 출력은 생성된 \\(12\\)-채널 특징맵을 ControlNet에 전달하기 전에 복사 힌트와 채널 단위로 곱해진다.\n' +
      '\n' +
      '그림 3: 조명 제어 프롬프트 구동 이미지 합성을 위한 파이프라인의 개요: (1) 텍스트 프롬프트 및 내용-시드가 주어진 제어되지 않은 조명 하에서 사전 훈련된 확산 모델을 사용하여 _프로비저널 이미지_를 생성하는 것으로 시작한다. (2) 다음으로, 임시 이미지의 콘텐츠를 유지하면서 목표 조명과 일치하도록 이미지를 재합성할 DiLightNet에 외관-시드, 임시 이미지 및 복사 힌트 세트(목표 조명 및 깊이의 거친 추정치로부터 계산됨)를 전달한다. (3) 마지막으로 전경 객체 및 목표 조명과 일치하도록 배경을 인페인팅한다.\n' +
      '\n' +
      '환경은 총 전력\\(1W\\)으로 빛납니다.\n' +
      '2. _Multiple Point Light Sources:_ 환경 조명을 포함하는 단일 광원 케이스와 동일한 방식으로 샘플링된 세 개의 광원.\n' +
      '3. _Environment Lighting_ sampled from the collection of \\(679\\) environment maps from Polyhaven.com.\n' +
      '4. _모노크롬 환경 조명_은 환경 조명 카테고리의 휘도 전용 버전이다. 이 범주를 포함하면 환경 조명의 전체 색상 분포에서 잠재적인 고유한 편향이 퇴치됩니다.\n' +
      '5. _Area Light Source_ 큰 라이트 박스로 스튜디오 설정을 시뮬레이션합니다. 우리는 물체를 겨냥한 물체(점광원과 유사한)를 둘러싸는 반구에 영역 광원을 무작위로 배치함으로써 이것을 달성하는데, 크기는 \\([5m,10m]\\)의 범위에서 무작위로 선택되고 총 전력은 \\([500W,1500W]\\)으로 샘플링된다. 점 조명과 유사하게, 우리는 \\(1W\\)의 균일한 백색 환경 조명을 추가한다.\n' +
      '\n' +
      '렌더링은 반구에 반지름이 균일하게 샘플링된 4개의 시점([0.8m,1.1m]\\)과 (10^{\\circ}\\theta\\leq 90^{\\circ}\\)의 합성물체를 각각 렌더링하고, (25^{\\circ},30^{\\circ}]\\)에서 샘플링된 시야가 있는 물체를 지향하고, 점광원 조명, 다중 점광원, 환경도, 흑백 환경도, 영역 광원에 대해 각각 \\(3:1:3:2:3\\)의 상대적 비율로 선택된 \\(12\\)의 조명 조건으로 조명한다. 렌더링된 각 시점에 대해 해당 복사 힌트도 필요합니다. 그러나, _evaluation_ 시간에서, 복사 힌트는 추정된 깊이 맵들로부터 구성될 것이다; _training_ 동안 지상 진리 기하학 및 정규들을 사용하여 도메인 갭을 도입할 것이다. 우리는 깊이에서 파생된 광도 힌트가 두 가지 유형의 근사치를 포함한다는 것을 관찰한다. 첫째, 평활화된 정규식으로 인해 결과 음영도 평활화되고 복잡한 기하학적 세부 사항으로 인한 음영 효과가 손실되며, 즉 국부적으로 광도 힌트에 영향을 미친다. 둘째, 단일 영상으로부터 깊이 추정의 모호성으로 인해 기하학 누락과 전역적 변형으로 인해 부정확한 그림자 즉, 비국소적인 효과가 발생한다. 우리는 확산 모델이 전자를 그럴듯하게 수정할 수 있는 반면 후자는 더 모호하고 수정하기 어렵다고 주장한다. 따라서, 우리는 국부 음영에 대한 근사치만을 도입하기 위한 훈련 광도 힌트를 원한다. 이는 수정된 음영 정규선과 함께 지상 진리 형상을 사용하여 달성된다. 음영 정규식에 대해 두 가지 다른 근사식을 고려하고, (1) 기하학적 정규식을 사용하고 물체의 재료 모델에서 임의의 음영 정규식을 무시하거나 (2) 평활화된 삼각 측량 깊이에서 해당 정규식을 사용한다 (계산 비용을 줄이기 위해, \\(9\\) 샘플링된 조명 조건 각각에 대해 대신 균일한 백색 조명 하에서 각 시점에 대해 각 합성 물체의 깊이를 추정한다).\n' +
      '\n' +
      'Training Dataset은 훈련 시간에 입력-출력 쌍을 동적으로 구성한다. 우리는 먼저 합성 물체를 선택하고 균일하게 본다. 다음으로, 입력 및 출력 영상에 대한 조명을 선택한다. 입력된 학습 영상에 대한 조명 조건을 선택하기 위해 확산 모델로 생성된 영상은 주의 깊게 흰색 균형을 이루는 경향이 있음을 주목한다. 따라서, 우리는 (색상) 환경 조명 하에서 렌더링된 입력 이미지를 제외한다. 출력 영상의 경우, 미리 계산된 렌더(컬러 환경 조명으로 렌더링된 렌더 포함) 중 임의의 렌더(12\\)를 무작위로 선택한다. 우리는 평활화된 깊이 추정 정규식과 기하 정규식이 있는 복사 힌트에 대해 1:9 비율로 출력에 해당하는 복사 힌트를 선택한다. 색채 조명에 대한 강인성을 향상시키기 위해 RGB 컬러 채널을 랜덤하게 셔플링하여 출력 영상에 추가 컬러 증강을 적용하고, 출력 영상과 그에 대응하는 광도 힌트에 대해 동일한 컬러 채널 순열을 사용한다.\n' +
      '\n' +
      '## 5 배경인페인팅\n' +
      '\n' +
      '환경 기반 인페인팅은 환경 맵에 의해 목표 조명이 지정되면, 광도 힌트와 동일한 카메라 구성을 사용하여 배경 이미지를 직접 렌더링할 수 있다. 우리는 이전에 계산된 분할 마스크를 사용하여 마스크 에지를 매끄럽게 하기 위해 \\(3\\times 3\\) 평균 필터로 필터링한 후 배경의 전경을 합성한다.\n' +
      '\n' +
      '확산 기반 인페인팅 다른 모든 조명 조건에 대해 사전 훈련된 확산 기반 인페인팅 모델[10](즉, _stable-diffusion-2-inpainting_ 모델[16])을 사용한다. 합성된 전경 이미지를 (역)분할 마스크와 함께 입력하며 원문 프롬프트를 통해 일관된 배경으로 전경 이미지를 완성한다.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'PyTorch[22]에서 DiLightNet을 구현하고, 기본 사전 훈련된 확산 모델로 _stable diffusion v2.1_[16]을 사용하여 정제하였다. 본 논문에서는 AdamW[17]을 이용하여 임시영상 인코더와 ControlNet을 공동으로 학습하였으며, 학습률은 \\(10^{-5}\\(다른 하이퍼파라미터는 모두 기본값으로 유지됨)\\(64\\)의 배치 크기를 사용하여 \\(150K\\) 반복한다. 훈련은 \\(8\\times\\) NVidia V100 GPU를 사용하여 약 30시간 소요되었다. 학습 데이터는 Blender\'s Cycles path tracer [1]을 이용하여 \\(512\\times 512\\) 해상도에서 픽셀당 \\(4096\\)개의 샘플로 렌더링된다.\n' +
      '\n' +
      '그림 5: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 기준으로 보여주는 반면, 마지막 다섯 개의 열은 서로 다른 사용자 지정 조명 조건(포인트 조명(열 2-3) 및 환경 조명(열 4-6))에서 생성된다. 마지막 두 예에 대한 임시 이미지는 보다 복잡한 프롬프트를 더 잘 처리하기 위해 _stable diffusion v2.1_ 대신 _DALL-E3_로 생성된다.\n' +
      '\n' +
      '일치 조명 제어 그림 5는 5가지 다른 프롬프트에 대해 \\(5\\) 다른 조명 조건(점광(2열 및 3열) 및 3가지 다른 환경 맵: 유칼립투스 그로브(4열), 주방(5열), 그레이스 대성당(마지막 열))에서 생성된 5가지 장면(임시 이미지를 참조용으로 첫 번째 열에 표시)을 보여준다. 각 프롬프트는 높은 정재(1행), 풍부한 기하학적 상세(2행), 다수의 균질한 재료를 갖는 물체(3행), 비실감적 기하(4행), 공간적으로 변하는 재료(마지막 행)와 같은 상이한 재료 및 기하학적 특성을 다루는 우리의 방법의 능력을 입증하기 위해 선택되었다. 마지막 두 행의 임시 이미지는 보다 복잡한 프롬프트를 더 잘 모델링하기 위해 _stable diffusion v2.1_ 대신 _DALL-E3_로 생성된다. 우리는 DiLightNet이 그럴듯한 결과를 생성하고 다른 프롬프트에 대해 동일한 목표 조명 아래에서 모양이 일관된다는 것을 관찰한다. 또한 각 프롬프트에 걸쳐 조명 변화가 그럴듯합니다. 추가 결과는 첨부 자료를 참고하시기 바랍니다. 정교하게 시행되지는 않았지만, 우리는 또한 DiLightNet이 합성된 세부 사항에 대해 약간의 반짝임이 눈에 띄면서 변화하는 조명 하에서 양호한 전체 시간적 안정성을 나타내는 것을 관찰한다(보충 비디오를 참조하세요).\n' +
      '\n' +
      '추가 사용자 제어 3단계 솔루션의 장점은 사용자가 임시 이미지에서 재료의 해석을 수정하기 위해 2단계에서 외관 시드를 변경할 수 있다는 것이다. 그림 6은 다양한 외관 종자가 생성된 결과에 어떻게 영향을 미치는지 보여준다. 외관-종자를 변경하면 임시 이미지의 외관에 대한 대체 설명이 생성된다. 반대로, 동일한 외관-씨드를 사용하면 상이한 제어된 조명 조건(도 5에서 입증된 바와 같이)에서 일관된 외관을 생성한다.\n' +
      '\n' +
      '외관 시드 외에도 재료 특성에 대한 추가 지침을 제공하기 위해 첫 번째 단계와 두 번째 단계 사이의 텍스트 프롬프트를 추가로 전문화할 수 있다. 도 7은 초기 프롬프트(_"장난감 로봇"_)의 네 가지 전문화를 부가한 것으로, _"종이 제조"_, _"플라스틱"_, _"종기 광택 금속성"_, 및 _"거울 광택 금속성"_이다. 이러한 결과로부터 모든 변형이 동일한 조명 하에서 일관되지만 더 제한된 재료 외관(즉, 하이라이트가 없는 확산, 확산 및 정면의 혼합물, 다른 거칠기를 갖는 두 금속 표면)을 갖는다는 것을 알 수 있다.\n' +
      '\n' +
      '## 7 Ablation Study\n' +
      '\n' +
      '우리는 방법을 구성하는 다양한 구성 요소의 영향을 더 잘 이해하기 위해 일련의 정성적 및 정량적 절제 연구를 수행한다. 정량적 평가를 위해 \'Staff Picked\' 레이블과 _no_ LVIS 레이블이 있는 Objavverse 데이터셋에서 객체를 선택하여 합성 테스트 세트를 생성하여 훈련과 테스트 세트 사이에 중복이 없도록 한다. 고품질의 합성물체를 보장하기 위해 단일물체 및/또는 조명효과에서 구워진 낮은 품질의 스캔 질감을 갖는 물체에 국한되지 않는 장면을 수동으로 제거하여 \\(50\\) 고품질 합성물체의 테스트 세트를 생성한다. 각 시험장면을 \\(3\\) 시점과 \\(6\\) 조명조건에 대해 렌더링한다. 그림 8은 테스트 세트의 대표적인 예를 보여준다. 우리는 PSNR, SSIM 및 LPIPS [22] 메트릭으로 오류를 정량화한다. 외관-시드는 사용자 제어 파라미터이기 때문에, 우리는 사용자가 가장 그럴듯한 결과를 생성하는 외관-시드를 선택할 것이라고 가정한다. 이 과정을 시뮬레이션하기 위해, 우리는 \\(4\\)개의 다른 외관-씨드로 생성된 렌더에서 가장 낮은 LPIPS 오류를 생성하는 각 장면/뷰/조명 조합에 대한 오류를 보고한다.\n' +
      '\n' +
      '임시 이미지 인코딩디라이트넷은 (인코딩된) 임시 이미지에 복사 힌트를 곱합니다. 우리는 인코딩과 곱셈이 모두 좋은 결과를 얻는 데 중요하다는 것을 발견했다. 도 9는 DiLightNet 대 두 개의 대체 아키텍처의 비교를 도시한다:\n' +
      '\n' +
      '1. _Direct ControlNet_는 상기 임시 이미지를 곱하는 대신 (라디언스 힌트에 추가하여) 추가 채널로서 직접 통과시키고, ControlNet에 대해 16개의 채널(상기 임시 이미지에 대해 3-채널, 더하기 (\\(4\\times 3\\))-채널, 및 마스크에 대해 \\(1\\)채널) 입력을 산출하며; 및\n' +
      '2. _Non-encoded Multiplication_ of the provisional image (with encoding) with the radiance hints.\n' +
      '\n' +
      '두 변형 모두 만족스러운 결과를 생성하지 않는다. 이러한 정성적 결과는 표 1(행 1-3)에서 추가로 정량적으로 확인된다.\n' +
      '\n' +
      '라디언스 힌트 수의 영향 표 1(행 4-6)은 (특정)라디언스 힌트의 수를 변경할 때의 영향을 비교합니다. 모든 변형에는 확산 라디언스 힌트가 포함됩니다. 광채도 힌트 변형에는 거칠기가 \\(0.13\\)인 \\(2\\) 정광채도 힌트와 \\(0.34\\)이 포함되어 있다. 4\\(4\\) 광도 힌트 변형에는 조도\\(0.05\\)을 갖는 하나의 추가적인 정광 광도 힌트가 포함된다. 마지막으로, \\(5\\) 광도 힌트 변형에는 거칠기가 \\(0.02\\)인 추가적인 (날카로운 정경) 힌트가 포함된다. 표 1의 정량적 결과로부터 \\(4\\) 광채 힌트가 가장 잘 수행됨을 알 수 있다. 결과를 자세히 살펴보면 간단한 모양과 간단한 재료를 나타내는 장면에 대해서는 거의 차이가 없음을 알 수 있다. 그러나 보다 복잡한 형상을 가진 장면의 경우, \\(3\\) 광도의 힌트가 빛과 물질의 상호작용을 정확하게 모델링하기에 불충분하다는 것을 발견했다. 복잡한 재료가 있는 장면의 경우, 너무 많은 복사 힌트를 제공하는 것도 (매끄러운) 깊이 추정 정규식의 제한된 품질로 인해 해로울 수 있음을 발견했다.\n' +
      '\n' +
      '포그라운드 마스킹DiLightNet은 포그라운드 마스크를 추가 입력으로 사용한다. 마스크 포함의 영향을 더 잘 이해하기 위해 마스크를 추가 채널로 사용하지 않고 변형도 교육합니다. 대신 임시 이미지에서 배경을 검은색 픽셀로 채웁니다. 훈련하는 동안 참조 이미지의 배경도 제거합니다. 결과적으로, DiLightNet은 검은 배경을 생성하는 법을 배울 것이다. 절제의 경우 전경 픽셀에 대한 오류만 계산합니다. 표 1(행 7-8)에 나타낸 바와 같이, 마스크 없이 훈련된 변형은 특히 복잡한 형상 또는 재료를 갖는 경우에 더 큰 에러를 생성한다.\n' +
      '\n' +
      '훈련 증강 훈련 세트로부터 세 가지 증강 각각을 제거하여 그들의 영향을 더 잘 측정한다(표 1, 행 9-12):\n' +
      '\n' +
      '* ** Normal Augmentation이 없는 경우:** 이 변종은 Ground truth shad로 렌더링된 Radiance 힌트를 사용하여 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline Variant & PSNR & SSIM & LPIPS \\\\ \\hline\n' +
      '**Our Network** & **22.97** & **0.8249** & **0.1165** \\\\ Direct ControlNet & 22.82 & 0.8216 & 0.1212 \\\\ Non-Encoded Multiplication & 22.40 & 0.8174 & 0.1232 \\\\ \\hline\n' +
      '3 Radiance Hint & 22.92 & 0.8197 & 0.1188 \\\\\n' +
      '**4 Radiance Hints** & **22.97** & **0.8249** & **0.1165**\n' +
      '5 Radiance Hints & 22.79 & 0.8200 & 0.1176 \\\\ \\hline\n' +
      '**w/ Mask** & **22.97** & **0.8249** & **0.1165** \\\\ w/o Mask & 22.23 & 0.8148 & 0.1184 \\\\ \\hline\n' +
      '**Full Augmentation** & **22.97** & **0.8249** & **0.1165** \\\\ w/o Material Augmentation & 22.90 & 0.8235 & 0.1178 \\\\ w/o Smoothed Normal & 21.88 & 0.7974 & 0.1314 \\\\ w/o Color Augmentation & 22.54 & 0.8161 & 0.1223 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: DiLightNet(행 1-3), 복사 힌트의 수(행 4-6), 분할 마스크를 포함하는 영향(행 7-8) 및 상이한 트레이닝 데이터 증강 방식(행 9-12)에 대한 통과 복사 힌트의 상이한 변형의 정량적 비교.\n' +
      '\n' +
      '그림 8: 모양 및/또는 재료의 복잡성이 다른 합성 테스트에서 완전성을 위한 다양한 ID를 가진 대표적인 예이다.\n' +
      '\n' +
      '도 6: 외관-씨앗의 변화의 영향. 텍스트 프롬프트에 의해 충분히 구속되지 않는 경우, 생성된 임시 이미지(왼쪽)는 DiLightNet이 객체의 정확한 재료를 결정하기 위해 충분한 정보를 제공하지 않을 수 있다. 외관-종자를 변경하면 DiLightNet이 임시 이미지에서 빛-물질 상호작용에 대한 다른 해석을 샘플링하도록 지시한다. 이 예에서, 외관-종자를 변경하는 것은 가죽 장갑의 광택 및 매끄러움에 대한 해석의 변화를 유도한다.\n' +
      '\n' +
      '도 7: DiLightNet에서의 신속한 전문화의 영향. 외관-시드를 변경하는 대신에, 사용자는 또한 2단계에서 추가 재료 정보로 프롬프트를 전문화할 수 있다. 이 예에서 초기 프롬프트(_"장난감 로봇"_)는 (점등)을 고정 상태로 유지하면서 추가 재료 설명으로 증강된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '우리의 방법은 제한이 없는 것이 아니다. 텍스트 프롬프트들로 이미지 콘텐츠를 특정하는 제한들로 인해, 사용자는 장면 내의 재료들에 대한 제한된 제어만을 갖는다. 결과적으로 재료-광 상호작용은 신속한 엔지니어의 의도를 따르지 않을 수 있다. DiLightNet은 외관 시드를 통해 텍스트 프롬프트를 넘어 일부 간접 제어를 가능하게 한다. 알케미스트 [2]와 같은 물질 인식 확산 모델을 통합하면 잠재적으로 물질-광 상호작용에 대한 더 나은 제어로 이어질 수 있다. 또한, 전경 객체의 대략적인 깊이 맵과 분할 마스크를 추정하기 위해 다수의 기성 해법에 의존한다. 우리의 방법은 깊이 맵의 일부 에러들에 강인하지만, 일부 타입들의 에러들(예를 들어, 베이스-완화 모호성)은 불만족스러운 결과들을 초래할 수 있다. 흥미로운 대안 파이프라인은 (예를 들어, "_stable-diffusion-2-depth"_와 같은 깊이 조건화된 확산 모델을 사용하여) 입력으로서 기준 깊이 맵을 취함으로써, 깊이 및 마스크를 추정할 필요성을 우회한다. 도 11에서 입증된 바와 같이, 기준 깊이 맵으로 입력을 증강하는 것은 결과의 품질을 더욱 증가시킨다.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      '본 논문에서는 확산 기반 텍스트-이미지 생성에서 조명을 제어하기 위한 새로운 방법을 소개한다. 제안된 방법은 (1) 기존의 텍스트-이미지 방법을 사용하여 제어되지 않은 조명 하에서 임시 이미지 합성, (2) 전경 객체의 광도에 의해 조절된 새로운 DiLightNet을 사용하여 전경 객체의 재합성, 그리고 마지막으로 (3) 목표 조명과 일치하는 배경의 인페인팅의 세 단계로 구성된다. 본 논문에서 제안하는 방법의 핵심은 DiLightNet이다. DiLightNet은 밝기 힌트를 곱한 임시 이미지의 인코딩 버전(모양과 질감 정보를 유지하기 위해)을 취한 ControlNet의 변형이다. 제안된 방법은 텍스트 프롬프트와 목표 조명에 일치하는 이미지를 생성할 수 있다. 향후 연구를 위해 DiLightNet을 적용하여 단일 사진에서 반사율 특성을 추정하고 풍부한 재료 특성을 가진 텍스트 대 3D 생성에 적용하고자 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Arvanami, D. Lischinski, and O. 튀김(2022) 자연 이미지의 텍스트 기반 편집을 위한 블렌디드 확산. CVPR, pp. 18208-18218. Cited by: SS1.\n' +
      '* S. 파루크 바트 Birkl, D. Woffe, P. Wonka, M. Muller (2023)Zoedepth: 상대 깊이와 메트릭 깊이를 조합하여 제로 샷 전송을 한다. ArXiv:2302.12288. 인용: SS1.\n' +
      '* B. Foundation (2011)Blender Cycles. 참고: [https://github.com/blender/cycles](https://github.com/blender/cycles) Cited by: SS1.\n' +
      '*T. Brooks, A. Holynski, and A. A. Efros (2023)Instruct-pix2pix: 이미지 편집 지시를 따르는 것을 배우는 것. CVPR, pp. 18392-18402. Cited by: SS1.\n' +
      '* B. Burley(2012) Physically-based shading at disney. ACM Siggraph Courses, Vol. 2012. 인용: SS1.\n' +
      '* M. 조욱 왕주영 기영 섀, X Xie, Y. Zheng(2023)MasaCtrl: 일관된 이미지 합성 및 편집을 위한 튜닝 프리 상호 자체 주의 제어. ArXiv:2304.08465. 인용: SS1.\n' +
      '*R. 천영 천남 쟈오와 K Jia(2023)Fantasia3D: 고품질 텍스트-3d 콘텐츠 생성을 위한 디엔탠글링 기하학 및 외관. ICCV에서 인용: SS1.\n' +
      '*Y. Cong, M. R. Min, L. E. Li, B. Rosenhahn, and M. Y. Yang(2023)Attribute-centric compositional text-to-image generation. ArXiv:2301.01413. 인용: SS1.\n' +
      '* M. Deitke, D. Schwenk, J. Salvador, L. 위스오 미셸, E 밴더빌트, L 슈미트 Ehsani, A. Kembhavi, 및 A. Farhadi (2022)ObiJaverse: 주석이 달린 3d 물체들의 우주. ArXiv:2212.08051. 인용: SS1.\n' +
      '* J. Deng, J. Guo, N. 슈와 S Zafeiriou (2019)ArcFace: 딥 페이스 인식을 위한 부가적인 각도 여백 손실. CVPR, pp. 4690-4699. Cited by: SS1.\n' +
      '* V. Deschaintre, G. Drettakis, and A. Bousseau(2020)는 대규모 물질 이송을 위한 미세 조정을 안내하였다. Comp. 그래프에서. Forum, Vol. 39, pp. 1-105. Cited by: SS1.\n' +
      '*Z. 딩, 엑스 장장 시아락 제비 Tu, X Zhang (2023)DiffusionRig: 얼굴 외형 편집을 위한 개인화된 전적을 학습하는 것. CVPR, pp. 12736-12746. Cited by: SS1.\n' +
      '*Y. 펑, H. 펑, M. J. 블랙, T. 볼카트(2021)는 야생의 이미지로부터 애니메이션이 가능한 상세한 3D 얼굴 모델을 학습한다. ACM Trans. Graph.40 (4), Article 88 (jul 2021). 인용: SS1.\n' +
      '* D. Gao, G. Chen, Y. 동평피어스 Xu, X. Tong(2020)Deferred neural lighting: Free-viewpoint relighting from unstructured 사진들. ACM Transactions on Graphics (TOG)39 (6). 인용: SS1.\n' +
      '* S. 지태 Park, J. Zhu and J. Huang (2023) Expressive text-to-image generation with rich text. CVPR, pp. 7545-7556. Cited by: SS1.\n' +
      '* D. 그리피스, T. Ritschel, and J. Philip (2022)OutCast: 야외 싱글 이미지 재조명, 캐스트 섀도우. Computer Graphics Forum41(2), pp. 179-193. Cited by: SS1.\n' +
      '*Y. 한진 Wang, and F. Xu(2023)는 저비용 데이터로부터 3D 변형 가능한 얼굴 반사율 모델을 학습한다. CVPR, pp. 8598-8608. Cited by: SS1.\n' +
      '* A. Hertz, R. Mokady, J. Tenenbaum, K. A. Herman, Y. Pritch, D. Cohen-Or(2022) cross attention control을 이용한 Prompt-to-prompt 이미지 편집. ArXiv:2208.01626. 인용: SS1.\n' +
      '* J. Ho and T. 살리만(2021) 분류기 없는 확산 안내. NeurIPS에서 인용: SS1.\n' +
      '*C. Ji, T. 유광 궈, 제이 류, Y. Liu(2022)Geometry-aware single-image full-body human relighting. ECCV에서, pp. 388-405. 인용: SS1.\n' +
      '*Y. 카나모리와 Y. Endo(2018)Relighting human: occlusion-aware inverse rendering for full-body human image. ACM Trans. Graph.37(6). 인용: SS1.\n' +
      '*T. 카라스 아티탈라 아일라, 그리고 S Laine(2022)은 확산 기반 생성 모델의 설계 공간을 설명한다. NeurIPS에서 인용: SS1.\n' +
      '*Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic : 확산 모델을 이용한 텍스트 기반의 실사 이미지 편집. _CVPR_에서. 6007-6017\n' +
      '* 김 등(2022) 김광현, 권태성, 예종철. 2022. 확산 클립: 강건한 이미지 조작을 위한 텍스트-유도 확산 모델. _CVPR_에서. 2426-2435\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 2023. Segment Anything _ICCV_에서. 4015-4026.\n' +
      '* Kocsis et al. (2023) Peter Kocsis, Vincent Sitzmann, and Matthias Niessner. 2023. 단일 시점 물질 추정을 위한 내재적 영상 확산_ arXiv preprint arXiv:2312.12274_(2023).\n' +
      '* DL-only Track_.\n' +
      '* Liu et al. (2020a) Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, and Noah Snavely. 2020a. 도시를 인수분해하고 재조명하는 법을 배우는 것. _ECCV_에서. 스프링거, 544-561\n' +
      '* Liu et al. (2023) Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot one image to 3d object. _ICCV_에서. 9298-9309\n' +
      '* Liu et al. (2020b) Xihui Liu, Zhe Lin, Jianming Zhang, Hanong Zhao, Quan Tran, Xiaogang Wang, and Hongsheng Li. 2020b. 오픈 에디트: 오픈 어휘 명령어를 사용한 오픈 도메인 이미지 조작. _ECCV_에서. 스프링거, 89-106\n' +
      '* Loshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization. _ICLR_에서.\n' +
      '* Ma et al. (2023) Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. 2023. Subject-Diffusion:Test-time Fine-tuning 없이 Open Domain Personalized Text-to-Image Generation arXiv preprint arXiv:2307.11410_(2023).\n' +
      '* Meng et al. (2022) Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2022. SDEdit: 유도 이미지 합성 및 확률적 미분 방정식으로 편집. _ICLR_에서.\n' +
      '* Mildenhall et al. (2020) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: 장면들을 뷰 합성을 위한 신경 복사 필드로 표현. _ ECCV_(2020), 405-421.\n' +
      '* Mokady et al. (2023) Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. 2023. 유도 확산 모델들을 이용하여 실제 이미지들을 편집하기 위한 널-텍스트 반전. _CVPR_에서. 6038-6047\n' +
      '* Mou et al. (2023) Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023. T2i-어댑터: 텍스트 대 이미지 확산 모델에 대한 보다 제어가능한 능력을 발굴하기 위한 학습 어댑터들 _ arXiv preprint arXiv:2302.08453_(2023).\n' +
      '* Nestmeyer et al.(2020) Thomas Nestmeyer, Jean-Francois Lalonde, Iain Matthews, and Andreas Lehrmann. 2020. Learning physics-guided face relighting under directional light. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 5124-5133.\n' +
      '* Nichol et al. (2022) Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: 텍스트 유도 확산 모델로 광실사 이미지 생성 및 편집을 향한다. _ICML_에서. 16784-16804\n' +
      '* Paiss et al. (2023) Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. 2023. 클립을 세어 열까지 가르친다. _ arXiv preprint arXiv:2302.12066_(2023).\n' +
      '* Pandey et al. (2021) Rohit Pandey, Sergio Ots Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Fanello. 2021. 전체 재조명: 배경 교체를 위한 초상화 재조명 학습_ ACM Trans. Graph.__ 40, 4(2021).\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Kimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, highperformance deep learning library. _ NeurIPS_32(2019).\n' +
      '* Peers et al. (2007) Pieter Peers, Naoki Tamura, Wojciech Matusik, and Paul Debevec. 2007. Post-production facial performance relighting using reflectance transfer. _ ACM Trans. Graph.__ 26, 3(2007).\n' +
      '* Ponglentmapakorn et al. (2023) Puntawat Ponglentmapakorn, Nontawat Tirtrong, and Supasorn Suwajanakorn. 2023. DiFaRel: 확산면 재조명_ arXiv preprint arXiv:2304.09479_(2023).\n' +
      '* Qin et al. (2020) Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. 2020. U2-Net: salient object detection을 위한 nested U-structure로 더 깊이 들어가기 _ 패턴인식_106(2020), 107404.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. CLIP Latents를 갖는 계층적 텍스트-조건부 이미지 생성_ arXiv preprint arXiv:2204.06125_(2022).\n' +
      '* Ranjan et al. (2023) Anurag Ranjan, Kwang Moo Yi, Jen-Hao Rick Chang, and Oncel Tuzel. 2023. FaceLit: Neural 3D Relightable Faces. _CVPR_에서. 8619-8628\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Loenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. _CVPR_에서. 10684-10695\n' +
      '* Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aherman. 2023a. 드림부스: 피사체 중심 생성을 위한 텍스트 대 이미지 확산 모델을 미세 조정합니다. _CVPR_에서. 22500-22510.\n' +
      '* Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aherman. 2023b. HyperDreamBooth: 텍스트-이미지 모델의 빠른 개인화를 위한 하이퍼네트워크 arXiv preprint arXiv:2307.06949_(2023).\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. _ NeurIPS_35(2022), 36479-36494.\n' +
      '* Sartor and Peers (2023) Sam Sartor and Pieter Peers. 2023. MafFusion: SVBRDF 캡쳐를 위한 생성적 확산 모델. The _SIGGRAPH Asia 2023 Conference Papers_. 1-10\n' +
      '* Sharma et al. (2023) Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. 프리먼과 마크 매튜스요 2023. 연금술사: 확산모델을 이용한 재료물성의 파라메트릭 제어 arXiv preprint arXiv:2312.02970_(2023).\n' +
      '* Shu et al. (2017) Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, and Dimitris Samaras. 2017. Neural face editing with intrinsic image disentangling. _CVPR_에서. 5541-5550.\n' +
      '* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. 확률적 미분방정식을 통한 점수 기반 생성 모델링 _ICLR_에서.\n' +
      '* Inpainting. [https://huggingface.co/stabilityai/stable-diffusion-2-inpainting] (https://huggingface.co/stabilityai/stable-diffusion-2-inpainting).\n' +
      '* [Sutabity2020] Stabily AI. 2020. Stable Diffusion V2.1. [https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1)\n' +
      '* [Sun et al.2019] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. 2019. 단일 이미지 인물 재조명. _ ACM Trans. Graph.__ 38, 4 (2019).\n' +
      '* [Tumanyan et al.2023] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. 텍스트 기반 이미지 대 이미지 변환을 위한 플러그 앤 플레이 확산 기능 _CVPR_에서. 1921-1930년\n' +
      '* [Ture et al.2021] Murat Ture, Mustafa Ege Ciklabakkal, Aykut Erdem, Erkut Erdem, Pinar Satlims, and Ahmet Oguz Akyuz. 2021. 정오부터 일몰까지: 태양 위치 수정에 의한 경관 사진의 인터랙티브 렌더링, 재조명 및 레콜링. In _Comp. 그래프. Forum_, Vol. 40. 500-515.\n' +
      '*[Vecchio et al.2023] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. 2023. ControlMat: Controlled Generative Approach to Material Capture. _ arXiv preprint arXiv:2309.01700_ (2023).\n' +
      '*[Voynov et al.2023a] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2023a. 스케치 가이드 텍스트 대 이미지 확산 모델입니다. In _ACM SIGGRAPH 2023 Conference Proceedings_. 55조 11쪽\n' +
      '*[Voynov et al.2023b] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. 2023b. P+: 텍스트-이미지 생성에서의 확장된 텍스트 컨디셔닝_ arXiv preprint arXiv:2303.09522_(2023).\n' +
      '* [Wang et al.2008] Yang Wang, Lei Zhang, Zicheng Liu, Gang Hua, Zhen Wen, Zhengyou Zhang, and Dimitris Samaras. 2008. Face relighting from single image under arbitrary unknown lighting conditions. _ IEEE PAMI_31, 11(2008), 1968-1984.\n' +
      '* [Watson et al.2022] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2022. 확산 모델과의 새로운 뷰 합성 _ arXiv preprint arXiv:2210.04628_(2022).\n' +
      '* [Wu and Saito2017] 중후안 Wu and Suguru Saito. 2017. Interactive relighting in single low-dynamic range images. _ ACM Trans. Graph.__ 36, 2 (2017).\n' +
      '*[Xiang et al.2023] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 2023. 2D 확산 모델을 이용한 3D 인식 영상 생성_ arXiv preprint arXiv:2303.17905_(2023).\n' +
      '*[Xiao et al.2023] Guangxuan Xiao, Tianwei Yin, William T. 프리먼, 프레도 듀랜드, 송한 2023. FastComposer: Localized Attention을 갖는 Tuning-Free Multi-Subject Image Generation arXiv preprint arXiv:2305.10431_(2023).\n' +
      '* [Xu et al.2023] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. 2023. Matlaber: Material-aware text-to-3d via latent brdf auto-encoder. _ arXiv preprint arXiv:2308.09278_(2023).\n' +
      '* [Ye et al.2023] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-어댑터 : 텍스트 대 이미지 확산 모델을 위한 텍스트 호환 이미지 프롬프트 어댑터 _ arXiv preprint arXiv:2308.06721_(2023).\n' +
      '* [Yu et al.2018] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. 2018. Bisenet: 실시간 시맨틱 세분화를 위한 양방향 세분화 네트워크. _ECCV_에서. 325-341\n' +
      '*[Ye et al.2020] Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. 2020년, 자체 감독 야외 장면 재조명 _ECCV_에서. 84-101\n' +
      '*[Zeng et al.2023] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. 2023. Paint3D: Lighting-Less Texture Diffusion Models으로 Anything 3D 도색 arXiv preprint arXiv:2312.13913_(2023).\n' +
      '* [Zhang et al.2023a] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. 2023a. DreamFace: Text Guidance에 의한 Animatable 3D 얼굴의 점진적 생성 ACM Trans. Graph.__ 제42조 제4항 제138조 (줄 2023)\n' +
      '* [Zhang et al.2023b] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. 텍스트 대 이미지 확산 모델에 조건부 제어를 추가합니다. _CVPR_에서. 3836-3847\n' +
      '*[Zhang et al.2018] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. _CVPR_에서. 586-595\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '**마스크 절제:** 도 12는 DiLightNet에 마스크를 통과시키는 시각적 영향을 나타낸다. 우리는 마스크가 없으면 네트워크가 어두운 전경 픽셀과 배경을 구별할 수 없기 때문에 잘못된 정반경 하이라이트가 더 많이 발생한다는 것을 관찰한다.\n' +
      '**Radiance 힌트의 수:** 도 13은 다른 수의 Radiance 힌트를 사용하는 시각적 효과를 나타낸다. 3개의 광채 힌트를 사용하면 종종 누락되거나 흐릿한 하이라이트가 발생합니다. 복사 힌트를 너무 많이 사용하면 복사 힌트를 렌더링하는 데 사용되는 (평활) 깊이 추정 정규식의 제한된 정확성으로 인해 결과에 부정적인 영향을 미치는 경향이 있습니다.\n' +
      '**Radiance 힌트의 예:** 도 14는 _"가죽 장갑"_에 대한 입사 조명을 제어하기 위해 DiLightNet에 의해 사용되는 Radiance 힌트를 도시한다.\n' +
      '**추가 결과:** 그림 15, 16, 17, 18, 19, 20 및 21은 동일한 텍스트 프롬프트를 사용하여 내용-시드를 변경하는 영향을 포함하여 추가 결과를 보여준다. 모든 예에 대해 3가지 다른 조명 조건에 대한 결과를 보여준다.\n' +
      '\n' +
      '도 12: 추가 입력 채널로서 마스크를 통과하지 않는 것은 부정확한 경면 하이라이트들의 더 많은 발생들을 초래할 것이다.\n' +
      '\n' +
      '그림 13: 다른 수의 복사 힌트를 사용하는 절제 비교. 단지 _3 래디언스 힌트_로, DiLightNet은 일부 경면 하이라이트를 놓치는 반면, 너무 많은 힌트(_5 래디언스 힌트_)는 또한 경면 래디언스 힌트를 생성하는 데 사용되는 깊이 추정치의 부정확성으로 인해 결과에 악영향을 미칠 수 있다. 우리의 구현에서 우리는 시각적으로 더 그럴듯한 결과를 생성하는 _4 복사 힌트를 사용하도록 선택한다.\n' +
      '\n' +
      '도 14: _"가죽 장갑"_에 대한 광채 힌트의 예시 시각화. DeLightNet은 확산 모델에 내장된 이미지의 학습된 공간을 활용하여 광도 힌트에 인코딩된 평활화된 음영 정보로부터 풍부한 음영 세부 정보를 생성한다는 점에 유의한다.\n' +
      '\n' +
      '도 12: 추가 입력 채널로서 마스크를 통과하지 않는 것은 부정확한 경면 하이라이트들의 더 많은 발생들을 초래할 것이다.\n' +
      '\n' +
      '그림 13: 다른 수의 복사 힌트를 사용하는 절제 비교. 단지 _3 래디언스 힌트_로, DiLightNet은 일부 경면 하이라이트를 놓치는 반면, 너무 많은 힌트(_5 래디언스 힌트_)는 또한 경면 래디언스 힌트를 생성하는 데 사용되는 깊이 추정치의 부정확성으로 인해 결과에 악영향을 미칠 수 있다. 우리의 구현에서 우리는 시각적으로 더 그럴듯한 결과를 생성하는 _4 복사 힌트를 사용하도록 선택한다.\n' +
      '\n' +
      '그림 16: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다.\n' +
      '\n' +
      '그림 17: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다.\n' +
      '\n' +
      '그림 18: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다.\n' +
      '\n' +
      '그림 19: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다.\n' +
      '\n' +
      '그림 20: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다. 임시 이미지는 보다 복잡한 프롬프트를 더 잘 처리하기 위해 _stable diffusion v2.1_ 대신 _DALL-E3_로 생성된다.\n' +
      '\n' +
      '그림 21: 조명 제어로 텍스트 대 이미지 생성 결과. 첫 번째 열은 임시 이미지를 참조로 보여주는 반면 마지막 세 열은 사용자가 지정한 환경 조명 조건에서 생성된다. 임시 이미지는 보다 복잡한 프롬프트를 더 잘 처리하기 위해 _stable diffusion v2.1_ 대신 _DALL-E3_로 생성된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>