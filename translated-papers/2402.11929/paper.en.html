<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# DiLightNet: Fine-grained Lighting Control\n' +
      '\n' +
      'for Diffusion-based Image Generation\n' +
      '\n' +
      'Chong Zeng\\({}^{1,2}\\) Yue Dong\\({}^{2}\\) Pieter Peers\\({}^{3}\\) Youkang Kong\\({}^{4,2}\\) Hongzhi Wu\\({}^{1}\\) Xin Tong\\({}^{2}\\)\n' +
      '\n' +
      '\\({}^{1}\\)State Key Lab of CAD and CG, Zhejiang University \\({}^{2}\\)Microsoft Research Asia\n' +
      '\n' +
      '\\({}^{3}\\)College of William & Mary \\({}^{4}\\)Tsinghua University\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Text-driven generative machine learning methods, such as diffusion models [12, 21, 23, 26], can generate fantastically detailed images from a simple text prompt. However, diffusion models also have built in biases. For example, Liu _et al_. [2023] demonstrate that diffusion models tend to prefer certain viewpoints when generating images. As shown in Figure 2, another previously unreported bias is the lighting in the generated images. Moreover, the image content and lighting are highly correlated. While diffusion\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '2023; Voynov et al.2023b], including non-rigid semantic edits [Cao et al.2023; Kawar et al.2023], modifying the identity and gender of subjects [Kim et al.2022], capturing the data distribution of underrepresented attributes [Cong et al.2023], and material properties [Sharma et al.2023]. However, with the exception of Alchemist [Sharma et al.2023], these methods only offer mid and high level semantic control. Similar to Alchemist, our method aims to empower the user to control low level shading properties. Complementary to Alchemist which offers relative control over material properties such as translucency and gloss, our method provides fine-grained control over the incident lighting in the generated image.\n' +
      '\n' +
      'Alternative guidance mechanisms have been introduced to provide spatial control during the synthesis process based on (sketch, depth, or stroke) images [Voynov et al.2023a; Ye et al.2023; Meng et al.2022], identity [Ma et al.2023; Xiao et al.2023; Ruiz et al.2023b], photo-collections [Ruiz et al.2023a], and by directly manipulating mid-level information [Ho and Salimans2021; Zhang et al.2023b; Mou et al.2023]. However, none of these methods provide control over the incident lighting. We follow a similar process and inject radiance hints modulated by a neural encoded version of the image into the diffusion model via a ControlNet [Zhang et al.2023b].\n' +
      '\n' +
      '2D diffusion models have also been leveraged to change viewpoint or generate 3D models [Liu et al.2023; Zhang et al.2023a; Watson et al.2022; Xiang et al.2023]. However, these methods do not offer control over incident lighting, nor guarantee consistent lighting between viewpoints. Paint3D [Zeng et al.2023] directly generates diffuse albedo textures in the UV domain of a given mesh. Fantasia3D [Chen et al.2023] and MatLaber [Xu et al.2023] generate a richer set of reflectance properties in the form of shape and spatially-varying BRDFs by leveraging text-to-image 2D diffusion models and score distillation. Diffusion-based SVBRDF estimation [Sartor and Peers2023; Vecchio et al.2023] and diffusion-based intrinsic decomposition [Kocsis et al.2023] also produce rich reflectance properties, albeit from a photograph instead of a text-prompt. However, all these methods require a rendering algorithm to visualize the appearance, including indirect lighting and shadows. In contrast, our method directly controls the lighting during the sampling process, leveraging the space of plausible image appearance embedded by the diffusion model.\n' +
      '\n' +
      'Single Image RelightingWhile distinct, our method is related to relighting from a single image, which is a highly underconstrained problem. To provide additional constraints, existing single image methods focus exclusively on either outdoor scenes [Wu and Saito2017; Ture et al.2021; Yu et al.2020; Liu et al.2020a; Griffiths et al.2022], faces [Peers et al.2007; Wang et al.2008; Shu et al.2017; Sun et al.2019; Nestmeyer et al.2020; Pandey et al.2021; Han et al.2023; Ranjan et al.2023], or human bodies [Kanamori and Endo2018; Lagunas et al.2021; Ji et al.2022]. In contrast, our method aims to offer fine-grained lighting control of general objects. Furthermore, existing methods expect a captured photograph of an existing scene as input, whereas, importantly, our method operates on, possibly implausible, generated images. The vast majority of prior single image relighting methods explicitely disentangle the image in various components, that are subsequently recombined after changing the lighting. In contrast, similar to Sun _et al_. [2019], we forego explicit decomposition of the input scene in disentangled components. However, unlike Sun _et al_., we do not use a specially trained encoder-decoder model, but rely on a general generative diffusion model to produce realistic relit images. Furthermore, the vast majority of prior single image relighting methods represents incident lighting using a Spherical Harmonics encoding. Notable exceptions are methods that represent the incident lighting by a shading image. Griffiths _et al_. [2022] pass a cosine weighted shadow map (along with normals and the main light direction) to a relighting network for outdoor scenes. Similarly, Kanamori _et al_. [2018] and Ji _et al_. [2022] pass shading and ambient occlusion maps to a neural rendering network. To better model specular reflections, Pandey _et al_. [2021] and Lagunas _et al_. [2021] pass, in addition to a diffuse shading image, also one or more specular shading images for neural relighting of human faces and full bodies respectively. We follow a similar strategy and pass the target lighting as a diffuse and (four) specular radiance hint images as conditions to a diffusion model.\n' +
      '\n' +
      'Relighting using Diffusion ModelsDing _et al_. [2023] alter lighting, pose, and facial expression by learning a CGI-to-real mapping from surface normals, albedo, and a diffuse shaded 3D morphable model fitted to a single photograph [Feng et al.2021]. To preserve the identity of the subject in the input photograph, the diffusion model is refined on a small collection (\\(\\sim\\)20) of photographs of the subject. Ponglerntapakorn _et al_. [2023] leverage off-the-shelf estimators [Feng et al.2021; Deng et al.2019; Yu et al.2018] for the lighting, a 3D morphable model, the subject\'s identity, camera parameters, a foreground mask, and cast-shadows to train a conditional diffusion network that takes a diffuse rendered model under the novel lighting (blended on the estimated background), in addition to the identity, camera parameters, and target shadows to generate a relit image of the subject. While we follow a similar overall strategy, our method differs on three critical points. First, our method is not limited to faces, and therefore does not require to extract a disentangled representation of the shape, camera, and lighting. Second, we provide multiple radiance hints (diffuse and specular) to control the lighting during the diffusion process. Finally, our method operates purely on an image generated via a text-prompt and our method does not require a real-world captured input photograph.\n' +
      '\n' +
      '## 3 Overview\n' +
      '\n' +
      'Our method takes as input a text prompt describing the desired image content, the target lighting, a content-seed that controls variations in shape and texture, and an appearance-seed that controls variations in light-material interactions. The resulting output is a generated image corresponding to the text prompt and that is consistent with the target lighting. We assume that the image contains an isolated foreground object, and that the background content is implicitly described by the target lighting. We make no assumption on the target lighting, and support arbitrary lighting conditions. Finally, while we do not impose any constraint on the realism of the synthesized content (e.g. fantastic beasts), we assume an image style that depicts physically-based light-matter interactions (e.g., we do not support lighting control in artistic styles such as cell-shading or surrealistic images).\n' +
      '\n' +
      'Our pipeline for lighting-controlled prompt-driven image synthesis consists of three separate stages (Figure 3):\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. _Provisional Image Generation:_ In the first stage, we generate a provisional image with uncontrolled lighting given the text-prompt and the content-seed using a pre-trained diffusion model [20]. The goal of this stage is to determine the shape and texture of the foreground object. Optionally, we add _"white background"_ to the text-prompt to facilitate foreground detection.\n' +
      '3. _Synthesis with Radiance Hints:_ In the second stage (section 4), we first generate radiance hints given the provisional image and target lighting. Next, the radiance hints are multiplied with a neural encoded version of the provisional image, and passed to DiLightNet together with the text-prompt and appearance-seed. The result of this second stage is the foreground object with consistent lighting.\n' +
      '4. _Background Inpainting:_ In the third stage (section 5), we inpaint the background to be consistent with the target lighting.\n' +
      '\n' +
      '## 4 Synthesis with Radiance Hints\n' +
      '\n' +
      'Our goal is to synthesize an image with the same foreground object as in the provisional image, but with its appearance consistent with the given target lighting. We will finetune the same diffusion model used to generate the provisional image to take in account the target lighting via a ControlNet [13]. A ControlNet assumes a control signal per pixel, and thus we cannot directly guide the diffusion model using a direct representation of the lighting such as an environment map or a spherical harmonics encoding. Instead, we encode the _effect_ of the target lighting on each pixel\'s outgoing radiance using radiance hints.\n' +
      '\n' +
      '### Radiance Hint Generation\n' +
      '\n' +
      'A radiance hint is a visualization of the target shape under the target illumination, where the material of the object is replaced by a homogeneous proxy material (e.g. uniform diffuse). However, we do not have access to the shape of the foreground object. To circumvent this challenge, we observe that ControlNet typically does not require very precise information and it has been shown to work well on sparse signals such as sketches. Hence, we argue that an approximate radiance hint computed from a coarse estimate of the shape suffices.\n' +
      '\n' +
      'To estimate the shape of the foreground object, we first segment the foreground object from the provisional image using an off-the-shelf salient object detection network. Practically, we use U2Net [14] as it offers a good trade-off between speed and accuracy; we revert to SAM [15] for the rare cases where U2Net fails to provide a clean foreground segmentation. Next, we apply another off-the-shelf depth estimation network (ZoeDepth [1]) on the segmented foreground object. The estimated depth map is subsequently triangulated in a mesh and rendered under the target lighting with the proxy materials. However, single-image depth estimation is a challenging problem, and the resulting triangulated depth maps are far from perfect. Empirically we find that ControlNet is less sensitive to low-frequency errors in the resulting shading, while high-frequency errors in the shading can lead to artifacts. We therefore apply a Laplace smoothing filter over the mesh to reduce the impact of high-frequency discontinuities.\n' +
      '\n' +
      'Inspired by the positional encoding in NeRFs [19], we also encode the impact of different frequencies in the target lighting on the appearance of the foreground shape in separate radiance hints. Leveraging the fact that a BRDF acts as a band-pass filter on the incident lighting, we generate \\(5\\) radiance hints, each rendered with a different material modeled with the Disney BRDF model [1] (one pure diffuse material and three specular materials with roughness set to \\(0.34\\), \\(0.13\\), and \\(0.05\\) respectively). We render the radiance hints, inclusive of shadows and indirect lighting, with Blender\'s Cycles path tracer.\n' +
      '\n' +
      '### Lighting Conditioned ControlNet\n' +
      '\n' +
      'As noted before, we finetune a diffusion model to incorporate the radiance hint images using ControlNet, as well as the original text prompt used to generate the provisional image, and the appearance-seed. However, as we finetune the model, there is no guarantee that it will generate a foreground object with the same shape and texture as in the provisional image. Therefore, we want to include the provisional image into the diffusion process. However, the texture and shape information in the provisional image is entangled with the unknown lighting from the first stage. We disentangle the relevant texture and shape information by first encoding the provisional image (with the alpha channel set to the segmentation mask). Our encoder follows Gao _et al_.\'s [2020] deferred neural relighting architecture, but with a reduced number of channels to limit memory usage. In addition, we include a channel-wise multiplication between the \\(12\\)-channel encoded feature map of the provisional image and the \\(4\\times 3\\)-channel radiance hints, which is subsequently passed to ControlNet. The encoder architecture is summarized in Figure 4.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'To train DiLightNet, we opt for a synthetic 3D training set that allows us to precisely control the lighting, geometry, and the material distributions. It is critical that the synthetic training set contains a wide variety of shapes, materials, and lighting.\n' +
      '\n' +
      'Shape and Material DiversityWe select synthetic objects from the LVIS category in the Objaverse dataset [Deitke et al.2022] that also have either a roughness map, a normal map, or both, yielding an initial subset of \\(13K\\) objects. In addition, we select \\(4K\\) objects from the Objaverse dataset (from the LVIS category) that only contain a diffuse texture map and assign a homogeneous specular BRDF with a roughness log-uniformly selected in \\([0.02,0.5]\\) and specular tint set to \\(1.0\\). To ensure that the refined diffusion model has seen objects with homogeneous materials, we select an additional \\(4K\\) objects (from the LVIS category) and randomly assign a homogeneous diffuse albedo, and a homogeneous specular sampled as before.\n' +
      '\n' +
      'Empirically, we found that the diversity of detailed spatially varying materials in the Objaverse dataset is limited. Therefore, we further augment the dataset with the shapes with the most "likes" (a statistic provided by the Objaverse dataset) from each LVIS category. For each of these selected shapes we automatically generate UV coordinates using Blender (we eliminate the shapes (\\(17\\)) for which this step failed), and create \\(4\\) synthetic objects per shape by assigning a randomly selected spatially varying material from the INRIA-Highres SVBRDF dataset [Deschaintre et al.2020], yielding a total of \\(4K\\) additional objects with enhanced materials.\n' +
      '\n' +
      'In total, our training set contains \\(25K\\) synthetic objects with a wide variety of shapes and materials. We scale and translate each object such that its bounding sphere is centered at the origin with a radius of 0.5m.\n' +
      '\n' +
      'Lighting DiversityWe consider five different lighting categories:\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. _Point Light Source_ random uniformly sampled on the upper hemisphere (with \\(0\\leq\\theta\\leq 60^{\\circ}\\)) surrounding the object with radius sampled in \\([4m,5m]\\), and with the power uniformly chosen in \\([500W,1500W]\\). To avoid completely black images when the point light is positioned behind the object, we also add a uniform white\n' +
      '\n' +
      'Figure 4: Provisional image encoder architecture. The output of the encoder is channel-wise multiplied with the radiance hints before passing the resulting \\(12\\)-channel feature map to a ControlNet.\n' +
      '\n' +
      'Figure 3: Overview of our pipeline for lighting-controlled prompt-driven image synthesis: (1) We start by generating a _provisional image_ using a pretrained diffusion model under uncontrolled lighting given a text prompt and a content-seed. (2) Next, we pass an appearance-seed, the provisional image, and a set of radiance hints (computed from the target lighting and a coarse estimate of the depth) to DiLightNet that will resynthesize the image such that becomes consistent with the target lighting while retaining the content of the provisional image. (3) Finally, we inpaint the background to be consistent with foreground object and the target lighting.\n' +
      '\n' +
      'environment light with a total power of \\(1W\\).\n' +
      '2. _Multiple Point Light Sources:_ three light source sampled in the same manner as the single light source case, including the environment lighting.\n' +
      '3. _Environment Lighting_ sampled from a collection of \\(679\\) environment maps from Polyhaven.com.\n' +
      '4. _Monochrome Environment Lighting_ are the luminance only versions of the environment lighting category. Including this category combats potential inherent biases in the overall color distribution in the environment lighting.\n' +
      '5. _Area Light Source_ simulates studio setups with large light boxes. We achieve this by randomly placing an area light sources on the hemisphere surrounding the object (similar to point light sources) aimed at the object, with a size randomly chosen in the range \\([5m,10m]\\) and total power sampled in \\([500W,1500W]\\). Similar to the point lighting, we add a uniform white environment light of \\(1W\\).\n' +
      '\n' +
      'RenderingWe render each of the \\(25K\\) synthetic objects from four viewpoints uniformly sampled on the hemisphere with radius uniformly sampled from \\([0.8m,1.1m]\\) and \\(10^{\\circ}\\leq\\theta\\leq 90^{\\circ}\\), aimed at the object with a field of view sampled from \\([25^{\\circ},30^{\\circ}]\\), and lit with \\(12\\) different lighting conditions, selected with a relative ratio of \\(3:1:3:2:3\\) for point source lighting, multiple point sources, environment maps, monochrome environment maps, and area light sources respectively. For each rendered viewpoint, we also require corresponding radiance hints. However, at _evaluation_ time, the radiance hints will be constructed from estimated depth maps; using the ground truth geometry and normals during _training_ would therefore introduce a domain gap. We observe that depth-derived radiance hints include two types of approximations. First, due to the smoothed normals, the resulting shading will also be smoothed and shading effects due to intricate geometrical details are lost; i.e., it locally affects the radiance hints. Second, due to the ambiguities in estimating depth from a single image, missing geometry and global deformations cause incorrect shadows; i.e., a non-local effect. We argue that diffusion models can plausibly correct the former, whereas the latter is more ambiguous and difficult to correct. Therefore, we would like the training radiance hints to only introduce approximations on the local shading. This is achieved by using the ground truth geometry with modified shading normals. We consider two different approximations for the shading normals, and randomly select at training time which one to use: (1) we use the geometric normals and ignore any shading normals from the object\'s material model, or (2) we use the corresponding normals from the smoothed triangulated depth (to reduce computational costs, we estimate the depth for each synthetic object for each viewpoint under uniform white lighting instead for each of the \\(9\\) sampled lighting conditions).\n' +
      '\n' +
      'Training DatasetAt training time we dynamically compose the input-output pairs. We first select a synthetic object and view uniformly. Next, we select the lighting for the input and output image. To select the lighting condition for the input training image, we note that images generated with diffusion models tend to be carefully white balanced. Therefore, we exclude the input images rendered under (colored) environment lighting. For the output image, we randomly select any of the \\(12\\) precomputed renders (including those rendered with colored environment lighting). We select the radiance hints corresponding to the output with a 1:9 ratio for the radiance hints with smoothed depth-estimated normals versus geometric normals. To further improve robustness with respect to colored lighting, we apply an additional color augmentation to the output images by randomly shuffling their RGB color channels; we use the same color channel permutation for the output image and its corresponding radiance hints.\n' +
      '\n' +
      '## 5 Background Inpainting\n' +
      '\n' +
      'Environment-based InpaintingWhen the target lighting is specified by an environment map, we can directly render the background image using the same camera configuration as for the radiance hints. We composite the foreground on the background using the previously computed segmentation mask filtered with a \\(3\\times 3\\) average filter to smooth the mask edges.\n' +
      '\n' +
      'Diffusion-based InpaintingFor all other lighting conditions, we use a pretrained diffusion-based inpainting model [10] (i.e., the _stable-diffusion-2-inpainting_ model [16]). We input the synthesized foreground image along with the (inverse) segmentation mask, as well as the original text prompt, to complete the foreground image with a consistent background.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      'We implemented DiLightNet in PyTorch [22] and use _stable diffusion v2.1_[16] as the base pretrained diffusion model to refine. We jointly train the provisional image encoder as well as the ControlNet using AdamW [17] with a \\(10^{-5}\\) learning rate (all other hyper-parameter are kept at the default values) for \\(150K\\) iterations using a batch size of \\(64\\). Training took approximately \\(30\\) hours using \\(8\\times\\) NVidia V100 GPUs. The training data is rendered using Blender\'s Cycles path tracer [1] at \\(512\\times 512\\) resolution with \\(4096\\) samples per pixel.\n' +
      '\n' +
      'Figure 5: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last five columns are generated under different user-specified lighting conditions (point lighting (columns 2-3) and environment lighting (columns 4-6)). The provisional images for the last two examples are generated with _DALL-E3_ instead of _stable diffusion v2.1_ to better handle the more complex prompt.\n' +
      '\n' +
      'Consistent Lighting ControlFigure 5 shows five generated scenes (the provisional image is shown in the first column for reference) under \\(5\\) different lighting conditions (point light (2nd and 3rd column), and 3 different environment maps: Eucalyptus Grove (4th column), Kitchen (5th column), and Grace Cathedral (last column)) for five different prompts. Each prompt was chosen to demonstrate our method\'s ability to handle different material and geometric properties such high specular materials (1st row), rich geometrical details (2nd row), objects with multiple homogeneous materials (3rd row), non-realistic geometry (4th row), and spatially-varying materials (last row). The provisional image in the last two rows are generated with _DALL-E3_ instead of _stable diffusion v2.1_ to better model the more complex prompt. We observe that DiLightNet produces plausible results and that the appearance is consistent under the same target lighting for different prompts. Furthermore, the lighting changes are plausible over each prompt. Please refer to the supplemental material for additional results. While not explicitely enforced, we also observe that DiLightNet exhibits good overall temporal stability under changing lighting, with some minor shimmering noticeable for synthesized details (please refer to the supplemental video).\n' +
      '\n' +
      'Additional User ControlOne advantage of our three step solution is that the user can alter the appearance-seed in the second stage to modify the interpretation of the materials in the provisional image. Figure 6 showcases how different appearance-seeds affect the generated results. Altering the appearance-seed yields alternative explanations of the appearance in the provisional image. Conversely, using the same appearance-seed produces a consistent appearance under different controlled lighting conditions (as demonstrated in Figure 5).\n' +
      '\n' +
      'In addition to the appearance-seed, we can further specialize the text prompt between the first and second stage to provide additional guidance on the material properties. Figure 7 shows four specializations of an initial prompt (_"toy robot"_) by adding: _"paper made"_, _"plastic"_, _"specular shinny metallic"_, and _"mirror polished metallic"_. From these results we can see that all variants are consistent under the same lighting, but with a more constrained material appearance (i.e., diffuse without a highlight, a mixture of diffuse and specular, and two metallic surfaces with a different roughness).\n' +
      '\n' +
      '## 7 Ablation Study\n' +
      '\n' +
      'We perform a series of qualitative and quantitative ablation studies to better understand the impact of the different components that comprise our method. For quantitative evaluation, we create a synthetic test set by selecting objects from the Objavverse dataset that have the \'Staff Picked\' label and _no_ LVIS label, ensuring that there is no overlap between the training and test set. To ensure high quality synthetic objects, we manually remove scenes that are not limited to a single object and/or objects with low quality scanned textures with baked in lighting effects, yielding a test set of \\(50\\) high quality synthetic objects. We render each test scene for \\(3\\) viewpoints and \\(6\\) lighting conditions. Figure 8 shows representative examples from the test set. We quantify errors with the PSNR, SSIM, and LPIPS [22] metrics. Because the appearance-seed is a user controlled parameter, we assume that the user would select the appearance-seed that produces the most plausible result. To simulate this process, we report the errors for each scene/view/lighting combination that produces the lowest LPIPS errors on renders generated with \\(4\\) different appearance-seeds.\n' +
      '\n' +
      'Provisional Image EncodingDiLightNet multiplies the (encoded) provisional image with the radiance hints. We found that both the encoding, as well as the multiplication is critical for obtaining good results. Figure 9 shows a comparison of DiLightNet versus two alternate architectures:\n' +
      '\n' +
      '1. _Direct ControlNet_ passes the provisional image directly as an additional channel (in addition to the radiance hints) instead of multiplying, yielding 16 channels input for ControlNet (3-channels for the provisional image, plus (\\(4\\times 3\\))-channels for the radiance hints, and \\(1\\) channel for the mask); and\n' +
      '2. _Non-encoded Multiplication_ of the provisional image (without encoding) with the radiance hints.\n' +
      '\n' +
      'Neither of the variants generates satisfactory results. This qualitative result is further quantitatively confirmed in Table 1 (rows 1-3).\n' +
      '\n' +
      'Impact of Number of Radiance HintsTable 1 (rows 4-6) compares the impact of changing the number of (specular) radiance hints; all variants include a diffuse radiance hint. The \\(3\\) radiance hint variant includes \\(2\\) specular radiance hints with roughness \\(0.13\\), and \\(0.34\\). The \\(4\\) radiance hint variant includes one additional specular radiance hint with roughness \\(0.05\\). Finally, the \\(5\\) radiance hint variant includes an additional (sharp specular) hint with roughness \\(0.02\\). From the quantitative results in Table 1 we can see that \\(4\\) radiance hints perform best. Upon closer inspection of the results, we observe that there is little difference for scenes that exhibit a simple shape and simple materials. However, for scenes with a more complex shape we find that the \\(3\\) radiance hints are insufficient to accurately model the light-matter interactions. For scenes with complex materials, we found that providing too many radiance hints can also be detrimental due to the limited quality of the (smoothed) depth-estimated normals.\n' +
      '\n' +
      'Foreground MaskingDiLightNet takes the foreground mask as additional input. To better understand the impact of including the mask, we also train a variant without taking the mask as an additional channel. Instead we fill the background with black pixels in the provisional image. During training we also remove the background in the reference images. As a consequence, DiLightNet will learn to generate a black background. For the ablation, we only compute the errors over the foreground pixels. As shown in Table 1 (rows 7-8), the variant trained without a mask produces larger errors especially on cases with either complex shape or materials.\n' +
      '\n' +
      'Training AugmentationWe eliminate each of the three augmentations from the training set to better gauge their impact (Table 1, rows 9-12):\n' +
      '\n' +
      '* **Without Normal Augmentation:** This variant is trained using radiance hints rendered with the ground truth shad\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline Variant & PSNR & SSIM & LPIPS \\\\ \\hline\n' +
      '**Our Network** & **22.97** & **0.8249** & **0.1165** \\\\ Direct ControlNet & 22.82 & 0.8216 & 0.1212 \\\\ Non-Encoded Multiplication & 22.40 & 0.8174 & 0.1232 \\\\ \\hline\n' +
      '3 Radiance Hints & 22.92 & 0.8197 & 0.1188 \\\\\n' +
      '**4 Radiance Hints** & **22.97** & **0.8249** & **0.1165** \\\\\n' +
      '5 Radiance Hints & 22.79 & 0.8200 & 0.1176 \\\\ \\hline\n' +
      '**w/ Mask** & **22.97** & **0.8249** & **0.1165** \\\\ w/o Mask & 22.23 & 0.8148 & 0.1184 \\\\ \\hline\n' +
      '**Full Augmentation** & **22.97** & **0.8249** & **0.1165** \\\\ w/o Material Augmentation & 22.90 & 0.8235 & 0.1178 \\\\ w/o Smoothed Normal & 21.88 & 0.7974 & 0.1314 \\\\ w/o Color Augmentation & 22.54 & 0.8161 & 0.1223 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative comparison of different variants of passing radiance hints to the DiLightNet (rows 1-3), the number of radiance hints (rows 4-6), impact of including the segmentation mask (row 7-8) and different training data augmentation schemes (rows 9-12).\n' +
      '\n' +
      'Figure 8: Representative examples, with Obiverse ID for completeness, from the synthetic test with different complexities in shape and/or material.\n' +
      '\n' +
      'Figure 6: Impact of changing the appearance-seed. If not sufficiently constrained by the text prompt, the generated provisional image (left) might not provide sufficient information for DiLightNet to determine the exact materials of the object. Altering the appearance-seed directs DiLightNet to sample a different interpretation of light-matter interaction in the provisional image. In this example, altering the appearance-seed induces changes in the interpretation of the glossiness and smoothness of the leather gloves.\n' +
      '\n' +
      'Figure 7: Impact of prompt specialization in DiLightNet. Instead of altering the appearance-seed, the user can also specialize the prompt with additional material information in the 2nd stage. In this example the initial prompt (_“toy robot”_) is augmented with additional material descriptions while keeping the (point lighting) fixed.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      'LimitationsOur method is not without limitations. Due to the limitations of specifying the image content with text prompts, the user only has limited control over the materials in the scene. Consequently, the material-light interactions might not follow the intention of the prompt-engineer. DiLightNet enables some indirect control, beyond text prompts, through the appearance-seed. Integrating material aware diffusion models, such as Alchemist [2], could potentially lead to better control over the material-light interactions. Furthermore, our method relies on a number of off-the-shelf solutions for estimating a rough depth map and segmentation mask of the foreground object. While our method is robust to some errors in the depth map, some types of errors (e.g., the bass-relief ambiguity) can result in non-satisfactory results. An interesting alternative pipeline takes a reference depth map as input (e.g., using a depth conditioned diffusion model such as "_stable-diffusion-2-depth"_), thereby bypassing the need to estimate the depth and mask. As demonstrated in Figure 11, augmenting the input with a reference depth map, further increases the quality of the results.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'In this paper we introduced a novel method for controlling the lighting in diffusion-based text-to-image generation. Our method consists of three stages: (1) provisional image synthesis under uncontrolled lighting using existing text-to-image methods, (2) resynthesis of the foreground object using our novel DiLightNet conditioned by the radiance hints of the foreground object, and finally (3) inpainting of the background consistent with the target lighting. Key to our method is DiLightNet, a variant of ControlNet that takes an encoded version of the provisional image (to retain the shape and texture information) multiplied with the radiance hints. Our method is able to generate images that match both the text prompt and the target lighting. For future work we would like to apply DiLightNet to estimate reflectance properties from a single photograph and for text-to-3D generation with rich material properties.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Arvanami, D. Lischinski, and O. Fried (2022)Blended diffusion for text-driven editing of natural images. In CVPR, pp. 18208-18218. Cited by: SS1.\n' +
      '* S. Farooq Bhat, R. Birkl, D. Woffe, P. Wonka, and M. Muller (2023)Zoedepth: zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288. Cited by: SS1.\n' +
      '* B. Foundation (2011)Blender Cycles. Note: [https://github.com/blender/cycles](https://github.com/blender/cycles) Cited by: SS1.\n' +
      '* T. Brooks, A. Holynski, and A. A. Efros (2023)Instruct-pix2pix: learning to follow image editing instructions. In CVPR, pp. 18392-18402. Cited by: SS1.\n' +
      '* B. Burley (2012)Physically-based shading at disney. In ACM Siggraph Courses, Vol. 2012. Cited by: SS1.\n' +
      '* M. Cao, X. Wang, Z. Qi, Y. Shan, X. Xie, and Y. Zheng (2023)MasaCtrl: tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465. Cited by: SS1.\n' +
      '* R. Chen, Y. Chen, N. Jiao, and K. Jia (2023)Fantasia3D: disentangling geometry and appearance for high-quality text-to-3d content creation. In ICCV, Cited by: SS1.\n' +
      '* Y. Cong, M. R. Min, L. E. Li, B. Rosenhahn, and M. Y. Yang (2023)Attribute-centric compositional text-to-image generation. arXiv preprint arXiv:2301.01413. Cited by: SS1.\n' +
      '* M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi (2022)ObiJaverse: a universe of annotated 3d objects. arXiv preprint arXiv:2212.08051. Cited by: SS1.\n' +
      '* J. Deng, J. Guo, N. Xue, and S. Zafeiriou (2019)ArcFace: additive angular margin loss for deep face recognition. In CVPR, pp. 4690-4699. Cited by: SS1.\n' +
      '* V. Deschaintre, G. Drettakis, and A. Bousseau (2020)Guided fine-tuning for large-scale material transfer. In Comp. Graph. Forum, Vol. 39, pp. 1-105. Cited by: SS1.\n' +
      '* Z. Ding, X. Zhang, Z. Xia, L. Jebe, Z. Tu, and X. Zhang (2023)DiffusionRig: learning personalized priors for facial appearance editing. In CVPR, pp. 12736-12746. Cited by: SS1.\n' +
      '* Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021)Learning an animatable detailed 3d face model from ln-the-wild images. ACM Trans. Graph.40 (4), Article 88 (jul 2021). Cited by: SS1.\n' +
      '* D. Gao, G. Chen, Y. Dong, P. Peers, K. Xu, and X. Tong (2020)Deferred neural lighting: free-viewpoint relighting from unstructured photographs. ACM Transactions on Graphics (TOG)39 (6). Cited by: SS1.\n' +
      '* S. Ge, T. Park, J. Zhu, and J. Huang (2023)Expressive text-to-image generation with rich text. In CVPR, pp. 7545-7556. Cited by: SS1.\n' +
      '* D. Griffiths, T. Ritschel, and J. Philip (2022)OutCast: outdoor single-image relighting with cast shadows. Computer Graphics Forum41 (2), pp. 179-193. Cited by: SS1.\n' +
      '* Y. Han, Z. Wang, and F. Xu (2023)Learning a 3d morphable face reflectance model from low-cost data. In CVPR, pp. 8598-8608. Cited by: SS1.\n' +
      '* A. Hertz, R. Mokady, J. Tenenbaum, K. A. Herman, Y. Pritch, and D. Cohen-Or (2022)Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626. Cited by: SS1.\n' +
      '* J. Ho and T. Salimans (2021)Classifier-free diffusion guidance. In NeurIPS, Cited by: SS1.\n' +
      '* C. Ji, T. Yu, K. Guo, J. Liu, and Y. Liu (2022)Geometry-aware single-image full-body human relighting. In ECCV, pp. 388-405. Cited by: SS1.\n' +
      '* Y. Kanamori and Y. Endo (2018)Relighting humans: occlusion-aware inverse rendering for full-body human images. ACM Trans. Graph.37 (6). Cited by: SS1.\n' +
      '* T. Karras, M. Aittala, T. Aila, and S. Laine (2022)Elucidating the design space of diffusion-based generative models. In NeurIPS, Cited by: SS1.\n' +
      '*Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing with diffusion models. In _CVPR_. 6007-6017.\n' +
      '* Kim et al. (2022) Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. 2022. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _CVPR_. 2426-2435.\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 2023. Segment Anything. In _ICCV_. 4015-4026.\n' +
      '* Kocsis et al. (2023) Peter Kocsis, Vincent Sitzmann, and Matthias Niessner. 2023. Intrinsic Image Diffusion for Single-view Material Estimation. _arXiv preprint arXiv:2312.12274_ (2023).\n' +
      '* DL-only Track_.\n' +
      '* Liu et al. (2020a) Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A Efros, and Noah Snavely. 2020a. Learning to factorize and relight a city. In _ECCV_. Springer, 544-561.\n' +
      '* Liu et al. (2023) Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot one image to 3d object. In _ICCV_. 9298-9309.\n' +
      '* Liu et al. (2020b) Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, and Hongsheng Li. 2020b. Open-edit: Open-domain image manipulation with open-vocabulary instructions. In _ECCV_. Springer, 89-106.\n' +
      '* Loshchilov and Hutter (2018) Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization. In _ICLR_.\n' +
      '* Ma et al. (2023) Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. 2023. Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning. _arXiv preprint arXiv:2307.11410_ (2023).\n' +
      '* Meng et al. (2022) Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2022. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In _ICLR_.\n' +
      '* Mildenhall et al. (2020) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. _ECCV_ (2020), 405-421.\n' +
      '* Mokady et al. (2023) Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-text inversion for editing real images using guided diffusion models. In _CVPR_. 6038-6047.\n' +
      '* Mou et al. (2023) Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_ (2023).\n' +
      '* Nestmeyer et al. (2020) Thomas Nestmeyer, Jean-Francois Lalonde, Iain Matthews, and Andreas Lehrmann. 2020. Learning physics-guided face relighting under directional light. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 5124-5133.\n' +
      '* Nichol et al. (2022) Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In _ICML_. 16784-16804.\n' +
      '* Paiss et al. (2023) Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. 2023. Teaching clip to count to ten. _arXiv preprint arXiv:2302.12066_ (2023).\n' +
      '* Pandey et al. (2021) Rohit Pandey, Sergio Ots Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Fanello. 2021. Total relighting: learning to relight portraits for background replacement. _ACM Trans. Graph._ 40, 4 (2021).\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. _NeurIPS_ 32 (2019).\n' +
      '* Peers et al. (2007) Pieter Peers, Naoki Tamura, Wojciech Matusik, and Paul Debevec. 2007. Post-production facial performance relighting using reflectance transfer. _ACM Trans. Graph._ 26, 3 (2007).\n' +
      '* Ponglentmapakorn et al. (2023) Puntawat Ponglentmapakorn, Nontawat Tirtrong, and Supasorn Suwajanakorn. 2023. DiFaRel: Diffusion Face Relighting. _arXiv preprint arXiv:2304.09479_ (2023).\n' +
      '* Qin et al. (2020) Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. 2020. U2-Net: Going deeper with nested U-structure for salient object detection. _Pattern recognition_ 106 (2020), 107404.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. _arXiv preprint arXiv:2204.06125_ (2022).\n' +
      '* Ranjan et al. (2023) Anurag Ranjan, Kwang Moo Yi, Jen-Hao Rick Chang, and Oncel Tuzel. 2023. FaceLit: Neural 3D Relightable Faces. In _CVPR_. 8619-8628.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Loenz, Patrick Esser, and Bjorn Ommer. 2022. High-Resolution Imaging Synthesis With Latent Diffusion Models. In _CVPR_. 10684-10695.\n' +
      '* Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aherman. 2023a. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _CVPR_. 22500-22510.\n' +
      '* Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aherman. 2023b. HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. _arXiv preprint arXiv:2307.06949_ (2023).\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_ 35 (2022), 36479-36494.\n' +
      '* Sartor and Peers (2023) Sam Sartor and Pieter Peers. 2023. MafFusion: A Generative Diffusion Model for SVBRDF Capture. In _SIGGRAPH Asia 2023 Conference Papers_. 1-10.\n' +
      '* Sharma et al. (2023) Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. Freeman, and Mark Matthews. 2023. Alchemist: Parametric Control of Material Properties with Diffusion Models. _arXiv preprint arXiv:2312.02970_ (2023).\n' +
      '* Shu et al. (2017) Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, and Dimitris Samaras. 2017. Neural face editing with intrinsic image disentangling. In _CVPR_. 5541-5550.\n' +
      '* Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through Stochastic Differential Equations. In _ICLR_.\n' +
      '* Inpainting. [https://huggingface.co/stabilityai/stable-diffusion-2-inpainting](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting).\n' +
      '* [Sutabity2020] Stabily AI. 2020. Stable Diffusion V2.1. [https://huggingface.co/stabilityai/stable-diffusion-2-1](https://huggingface.co/stabilityai/stable-diffusion-2-1).\n' +
      '* [Sun et al.2019] Tiancheng Sun, Jonathan T Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. 2019. Single image portrait relighting. _ACM Trans. Graph._ 38, 4 (2019).\n' +
      '* [Tumanyan et al.2023] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation. In _CVPR_. 1921-1930.\n' +
      '* [Ture et al.2021] Murat Ture, Mustafa Ege Ciklabakkal, Aykut Erdem, Erkut Erdem, Pinar Satlims, and Ahmet Oguz Akyuz. 2021. From Noon to Sunset: Interactive Rendering, Relighting, and Recolouring of Landscape Photographs by Modifying Solar Position. In _Comp. Graph. Forum_, Vol. 40. 500-515.\n' +
      '* [Vecchio et al.2023] Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy Boubekeur. 2023. ControlMat: A Controlled Generative Approach to Material Capture. _arXiv preprint arXiv:2309.01700_ (2023).\n' +
      '* [Voynov et al.2023a] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2023a. Sketch-Guided Text-to-Image Diffusion Models. In _ACM SIGGRAPH 2023 Conference Proceedings_. Article 55, 11 pages.\n' +
      '* [Voynov et al.2023b] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. 2023b. P+: Extended Textual Conditioning in Text-to-Image Generation. _arXiv preprint arXiv:2303.09522_ (2023).\n' +
      '* [Wang et al.2008] Yang Wang, Lei Zhang, Zicheng Liu, Gang Hua, Zhen Wen, Zhengyou Zhang, and Dimitris Samaras. 2008. Face relighting from a single image under arbitrary unknown lighting conditions. _IEEE PAMI_ 31, 11 (2008), 1968-1984.\n' +
      '* [Watson et al.2022] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2022. Novel view synthesis with diffusion models. _arXiv preprint arXiv:2210.04628_ (2022).\n' +
      '* [Wu and Saito2017] Jung-Hsuan Wu and Suguru Saito. 2017. Interactive relighting in single low-dynamic range images. _ACM Trans. Graph._ 36, 2 (2017).\n' +
      '* [Xiang et al.2023] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 2023. 3D-aware Image Generation using 2D Diffusion Models. _arXiv preprint arXiv:2303.17905_ (2023).\n' +
      '* [Xiao et al.2023] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fredo Durand, and Song Han. 2023. FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention. _arXiv preprint arXiv:2305.10431_ (2023).\n' +
      '* [Xu et al.2023] Xudong Xu, Zhaoyang Lyu, Xingang Pan, and Bo Dai. 2023. Matlaber: Material-aware text-to-3d via latent brdf auto-encoder. _arXiv preprint arXiv:2308.09278_ (2023).\n' +
      '* [Ye et al.2023] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models. _arXiv preprint arXiv:2308.06721_ (2023).\n' +
      '* [Yu et al.2018] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. 2018. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In _ECCV_. 325-341.\n' +
      '* [Ye et al.2020] Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. 2020. Self-supervised outdoor scene relighting. In _ECCV_. 84-101.\n' +
      '* [Zeng et al.2023] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. 2023. Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models. _arXiv preprint arXiv:2312.13913_ (2023).\n' +
      '* [Zhang et al.2023a] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. 2023a. DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance. _ACM Trans. Graph._ 42, 4, Article 138 (jul 2023).\n' +
      '* [Zhang et al.2023b] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding conditional control to text-to-image diffusion models. In _CVPR_. 3836-3847.\n' +
      '* [Zhang et al.2018] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_. 586-595.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '* **Mask Ablation:** Figure 12 shows the visual impact of passing the mask to DiLightNet. We observe that without a mask, there are more occurrences of incorrect specular highlights as the network is unable to differentiate between dark foreground pixels and background.\n' +
      '* **Number of Radiance Hints:** Figure 13 shows the visual effect of using a different number of radiance hints. Using 3 radiance hints often results in missed or blurred highlights. Using too many radiance hints also tends to adversely affect the results due to the limited accuracy of the (smoothed) depth-estimated normals used for rendering the radiance hints causing sharp specular highlights to be incorrectly placed.\n' +
      '* **Example of Radiance Hints:** Figure 14 shows the radiance hints used by DiLightNet to control the incident lighting for a _"leather glove"_.\n' +
      '* **Additional Results:** Figure 15, 16, 17, 18, 19, 20, and 21 show additional results, including the impact of changing the content-seed using the same text prompt. For all examples, we show the results for 3 different lighting conditions.\n' +
      '\n' +
      'Figure 12: Not passing the mask as an extra input channel will result in more occurences of incorrect specular highlights.\n' +
      '\n' +
      'Figure 13: Ablation comparison of using a different number of radiance hints. With only _3 radiance hints_, DiLightNet misses some specular highlights, while too many hints (_5 radiance hints_) can also adversely affect results due to the inaccuracies in the depth estimates used to generate the specular radiance hints. In our implementaion we opt for using _4 radiance hints_ which produces visually more plausible results.\n' +
      '\n' +
      'Figure 14: Example visualizations of the radiance hints for a _“leather glove”_. Note that DeLightNet leverages the learned space of images embedded in the diffusion model to generate rich shading details from the smoothed shading information encoded in the radiance hints.\n' +
      '\n' +
      'Figure 12: Not passing the mask as an extra input channel will result in more occurences of incorrect specular highlights.\n' +
      '\n' +
      'Figure 13: Ablation comparison of using a different number of radiance hints. With only _3 radiance hints_, DiLightNet misses some specular highlights, while too many hints (_5 radiance hints_) can also adversely affect results due to the inaccuracies in the depth estimates used to generate the specular radiance hints. In our implementaion we opt for using _4 radiance hints_ which produces visually more plausible results.\n' +
      '\n' +
      'Figure 16: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions.\n' +
      '\n' +
      'Figure 17: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions.\n' +
      '\n' +
      'Figure 18: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions.\n' +
      '\n' +
      'Figure 19: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions.\n' +
      '\n' +
      'Figure 20: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions. The provisional images are generated with _DALL-E3_ instead of _stable diffusion v2.1_ to better handle the more complex prompt.\n' +
      '\n' +
      'Figure 21: Text-to-image generated results with lighting control. The first column shows the provisional image as a reference, whereas the last three columns are generated under different user-specified environment lighting conditions. The provisional images are generated with _DALL-E3_ instead of _stable diffusion v2.1_ to better handle the more complex prompt.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>