<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# How Easy is It to Fool Your Multimodal LLMs?\n' +
      '\n' +
      'An Empirical Analysis on Deceptive Prompts\n' +
      '\n' +
      ' Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan\n' +
      '\n' +
      'Apple\n' +
      '\n' +
      '{yusugian,haotian.zhang2,yinfeiy,zhe.gan}@apple.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling _deceptive_ information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench,1 a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models\' resilience against deceptive prompts.\n' +
      '\n' +
      'Footnote 1: Short for **M**ultimodAI **D**eception Benchmark.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent advancements in Multimodal Large Language Models (MLLMs) Liu et al. (2023, 2023); Wang et al. (2023); You et al. (2024); Bai et al. (2023); Liu et al. (2024); Zhu et al. (2024), exemplified by models like GPT-4V(ision) (OpenAI, 2023) and Gemini Team (2023), mark a significant milestone in the evolution of AI, extending the capabilities of large language models to the realm of visual understanding and interaction.\n' +
      '\n' +
      'However, the sophistication of MLLMs brings with it unique challenges, notably, _hallucination_. Current studies Liu et al. (2024); Lee et al. (2023); Yin et al. (2023) have been actively exploring solutions to mitigate hallucination, especially when the model tries to generate long responses. However, there still remains a notable gap in the literature: no work has yet been conducted to focus on comprehensively studying the robustness of MLLMs when confronted with deceptive information in the prompts.2 Our work aims to fill in this gap. This issue is particularly critical, as it pertains to the reliability and trustworthiness of these models in real-world applications Liu et al. (2023), and holds substantial importance for the ongoing development and deployment of such AI systems.\n' +
      '\n' +
      'Footnote 2: LRV-Instruction Liu et al. (2023) is the pioneering work in this direction, while we aim to provide a more _comprehensive_ evaluation with hard negative instructions. Please see Section 2 for a more detailed discussion on related work.\n' +
      '\n' +
      'To this end, we present\n' +
      '\n' +
      'Figure 1: How easy is it to _fool_ your multimodal LLMs? Our study found that multimodal LLMs, such as LLaVA-1.5 Liu et al. (2023), can be easily deceived by prompts with incorrect information (the 2nd question in each subfigure, marked in red with Hard Negative Instruction).\n' +
      '\n' +
      'fully curated benchmark that contains 850 image-prompt pairs spanning across six deception categories, to systematically examine how MLLMs resolve the conflicts when facing inconsistencies between text prompts and images. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V (OpenAI, 2023), Gemini-Pro (Team, 2023), to open-sourced models, such as LLaVA-1.5 (Liu et al., 2023) and CogVLM (Wang et al., 2023). The evaluation is fully automated via the use of GPT-4. Results shed light on how vulnerable MLLMs are in handling deceptive instructions. For example, Figure 1 illustrates how sensitive LLaVA-1.5 (Liu et al., 2023) is to the _factualness_ of the input prompt and its consistency with the image. When asked "is there a cat in the image?", LLaVA-1.5 can successfully identify there is no cat; but when prompted with "what color is the cat in the image?", the model will imagine there is a cat inside. Empirically, we observe that GPT-4V suffers much less when compared with all the other MLLMs; however, the performance is still not ideal (GPT-4V vs. others: 75% vs. 5%-35% accuracy). Further, previous models that aim to mitigate hallucinations, such as LRV-Instruction (Liu et al., 2024) and LLaVA-RLHF (Sun et al., 2023), are not effective on this new benchmark.\n' +
      '\n' +
      'Finally, we provide a simple remedy to boost performance, which was surprisingly found to be effective to double the models\' accuracy. Specifically, we carefully design a system prompt in the form of a long paragraph to be prepended to the existing prompt, to encourage the model to think carefully before answering the question. This simple approach boosts the accuracy of LLaVA-1.5 from 10.42% to 20.56% (similar boosts for other models); however, the absolute numbers are still too low to be satisfactory. Further research is needed to study how to match GPT-4V\'s performance (75.02%).\n' +
      '\n' +
      'Our contributions are summarized as follows. (\\(i\\)) We construct MAD-Bench, a new benchmark to comprehensively evaluate MLLMs on their capability to resist deceiving information in the prompt. (\\(ii\\)) We provide a detailed analysis of popular MLLMs, and list some common causes for incorrect responses. (\\(iii\\)) We provide a simple remedy to boost performance via the careful design of a system prompt. MAD-Bench will be open-sourced, and we hope this benchmark can serve as a useful resource to stimulate further research to enhance models\' resilience against deceptive prompts.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Multimodal Large Language Models (MLMs).** MLLM has become an increasingly hot research topic. Early models primarily focused on large-scale image-text pre-training (Wang et al., 2022, 2022; Chen et al., 2022, 2023; Li et al., 2023; Driess et al., 2023; Huang et al., 2023; Awadalla et al., 2023; Laurencon et al., 2023). Among them, Flamingo (Alayrac et al., 2022) pioneered the integration of a CLIP image encoder with LLMs through gated cross-attention blocks, showcasing emergent multimodal in-context few-shot learning capabilities, via pre-training over millions of image-text pairs and interleaved image-text datasets (Zhu et al., 2023).\n' +
      '\n' +
      'On the other hand, recent research has focused on visual instruction tuning (Zhu et al., 2024; Li et al., 2023; Ye et al., 2023; Li et al., 2023; Chen et al., 2023). Prominent examples include LLaVA(-1.5) (Liu et al., 2023, 20), Instruct-BLIP (Dai et al., 2023), Qwen-VL (Bai et al., 2023), CogVLM (Wang et al., 2023), Emu2 (Sun et al., 2023), SPHINX (Lin et al., 2023), to name a few. Besides text response generation, recent works have also enabled MLLMs for referring and grounding (Peng et al., 2023; Chen et al., 2023; You et al., 2024; Wang et al., 2023), image segmentation (Lai et al., 2023; Zhang et al., 2023), image editing (Fu et al., 2023), image generation (Koh et al., 2023; Sun et al., 2023), _etc_.\n' +
      '\n' +
      'The release of proprietary systems like GPT-4V (OpenAI, 2023) and Gemini (Team, 2023) has elevated the research of MLLMs to new heights. Since GPT-4V\'s release, researchers have been exploring its capabilities as well as weaknesses (Zhou et al., 2023; Li et al., 2023; Liu et al., 2023; Yang et al., 2023; Cui et al., 2023). As MLLMs become stronger, the development of more challenging benchmarks is essential to push the boundaries of what these models can achieve. In this work, we aim to design a new benchmark to evaluate MLLMs\' resilience against deceptive prompts.\n' +
      '\n' +
      '**Hallucination in MLLMs.** Below, we first discuss hallucination in LLMs, and then focus on hallucination in MLLMs.\n' +
      '\n' +
      'Existing work on mitigating hallucination in LLMs can be roughly divided into two categories: (\\(i\\)) prompt engineering (Si et al., 2023; Cheng et al., 2023; Ji et al., 2023; Jones et al., 2023; Mundler et al., 2023; Vu et al., 2023), and (\\(ii\\)) model enhancement (Li et al., 2023; Chuang et al., 2023;Shi et al., 2023; Elaraby et al., 2023; Tian et al., 2024; Qiu et al., 2023; Leng et al., 2023). These studies laid solid foundations for understanding the causes of hallucinations, such as over-reliance on context, or training data biases.\n' +
      '\n' +
      'Similarly, hallucination in MLLMs is also growing to be an important research topic (Liu et al., 2024). There are various categories of hallucinations, such as describing objects that are non-existent in the input image, misunderstanding the spatial relationship between objects in the image, and counting objects incorrectly (Liu et al., 2023). The two main causes of hallucination in MLLMs found in existing work apart from the potential issues with training data include (\\(i\\)) limitations in correctly understanding input images, and (\\(ii\\)) language model bias (Wang et al., 2023). Various methods have been proposed to mitigate hallucination in MLLMs (Lee et al., 2023; Yin et al., 2023; Sun et al., 2023; Wang et al., 2023; Liu et al., 2024; Zhai et al., 2023; Zhou et al., 2024; Gunjal et al., 2024; Liu et al., 2023).\n' +
      '\n' +
      'Furthermore, various benchmarks have been proposed to evaluate hallucination in MLLMs. Specifically, POPE (Li et al., 2023), M-HalDetect (Gunjal et al., 2024), and GAVIE (Liu et al., 2024) evaluated object hallucination. HallusionBench (Guan et al., 2023) evaluated both visual and language hallucination. MMHal-Bench (Sun et al., 2023) evaluated hallucination in more aspects including relations, attributes, environments, _etc._ Bingo (Cui et al., 2023) studied hallucination in terms of bias and interference in GPT-4V (OpenAI, 2023).\n' +
      '\n' +
      'In this work, we aim to study how easy it is to use deceptive prompts that contain information inconsistent with the image to mislead MLLMs to generate responses with hallucination. Note, that we are not the first to study this. A similar model behavior is called "sycophancy" in the LLM literature (Sharma et al., 2023). Fu et al. (2023) and Liu et al. (2023) also constructed prompts with deceiving information to test model robustness. Deceptive prompts are termed "negative instructions" in LRV-Instruction (Liu et al., 2023) and "text-to-image interference" in the Bingo benchmark (Cui et al., 2023). Different from them, we comprehensively study MLLMs\' ability to handle deceptive prompts in multiple categories. Unlike previous studies (Fu et al., 2023; Liu et al., 2023) which primarily used "Is/Are/Can" questions, we found that it is relatively easy for state-of-the-art MLLMs to counter deceptive information in such formats. Consequently, we shifted our focus to questions beginning with "What", "How", "Where", _etc._, to provide a more challenging and insightful evaluation.\n' +
      '\n' +
      '## 3 MAD-Bench\n' +
      '\n' +
      'In this section, we present MAD-Bench, introduce how we collect3 deceptive image-prompt pairs, as\n' +
      '\n' +
      'Figure 2: Examples of deceptive prompts used in the proposed MAD-Bench with example model responses.\n' +
      '\n' +
      'well as our evaluation method.\n' +
      '\n' +
      '### Deception Categories\n' +
      '\n' +
      'MAD-Bench encompasses six distinct categories of 850 image-prompt pairs designed to test the resilience of MLLMs against deceptive prompts. Table 1 provides the statistics of each category, and Figure 2 shows examples of deceptive prompts. The selected categories are partly inspired by Liu et al. (2023). Below, we detail each category.\n' +
      '\n' +
      '**Count of Object.** This category intentionally cites an incorrect quantity of visible objects in the image. A response fails this test if it asserts the presence of \\(m\\) instances of an object \'A\' when, in reality, a different number \\(n\\) of object \'A\' is present -- \\(n\\) being distinct from \\(m\\) and not zero. The images for this and the subsequent four categories are sourced from COCO 2017 (Lin et al., 2015). Using a public dataset sometimes brings concerns about data leakage. In our case, given the special nature of our deceptive prompts to be introduced in the next section, this will not be a problem. An accurate response would either challenge the prompt\'s inconsistency with the visual data and abstain from speculating on absent information, or seek further clarification to resolve any uncertainties.\n' +
      '\n' +
      '**Non-existent Object.** Here, the prompts query about objects absent from the image. Failure occurs when a response acknowledges these non-existent objects as present.\n' +
      '\n' +
      '**Object Attribute.** This category includes prompts that inaccurately describe visible objects\' attributes. A response fails if it attributes these incorrect characteristics to the actual objects in the image.\n' +
      '\n' +
      '**Scene Understanding.** This category involves prompts that inaccurately describe the scene encapsulating the objects in the image. A response that falls into error here can be one that accurately identifies the actions of the objects but misconstrues the scene or setting in alignment with the deceptive prompt.\n' +
      '\n' +
      '**Spatial Relationship.** This category presents prompts that incorrectly specify the spatial dynamics between objects that do indeed exist within the image. A misstep in this category arises when a response correctly recognizes the objects but mis-represents their spatial relations.\n' +
      '\n' +
      '**Visual Confusion.** This category is different from the previous ones by employing both the prompts and the images as instruments of deception, often deceptive even to the human eye. This category includes three types of images: (\\(i\\)) those depicting naked-eye 3D paintings or screens, (\\(ii\\)) visual dislocation photography, and (\\(iii\\)) mirror reflections.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline Deception Category & Count & Image Source \\\\ \\hline Count of Object & 188 & COCO 2017 \\\\ Non-existent Object & 244 & COCO 2017 \\\\ Object Attribute & 136 & COCO 2017 \\\\ Scene Understanding & 122 & COCO 2017 \\\\ Spatial Relationship & 132 & COCO 2017 \\\\ Visual Confusion & 28 & In the Wild \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Statistics of the 850 image-prompt pairs in MAD-Bench.\n' +
      '\n' +
      'Figure 3: Examples of image-prompt pairs in the Visual Confusion category of MAD-Bench.\n' +
      '\n' +
      'Figure 3 shows an example image-prompt pair 4 in each category. Here, the prompts paired with the 3D paintings or screens aim to deceive the MLLMs by portraying the objects in the two-dimensional artwork as three-dimensional entities. With visual dislocation photography, the prompts reinforce the optical illusions present in the images. Lastly, the prompts associated with mirror reflections attempt to deceive the MLLMs into interpreting reflections as tangible objects.\n' +
      '\n' +
      'Footnote 4: Photo credit to Braga last1 and Tiago Silva.\n' +
      '\n' +
      '### Prompt Generation Method\n' +
      '\n' +
      'The process of creating deceptive prompts was automated by employing GPT-4, leveraging the ground-truth captions from the COCO dataset (Lin et al., 2015). We chose not to use GPT-4V for this task, as we later also evaluated GPT-4V on this benchmark, and empirically, employing GPT-4 is already enough for this task. To guide GPT-4 in generating questions that would intentionally mislead MLLMs within the specified categories, we crafted tailored prompts. These guiding prompts are provided in Appendix A.2, from Figure 16 to 20. The process is illustrated in Figure 4, using an example in the _non-existent object_ category. Bounding box information is not used as part of the prompt sent to GPT-4, as empirically, we observed that it does not contribute to further improving the quality of generated prompts in our deceptive categories. Following the generation of these deceptive questions, a rigorous manual filtering process is followed to ensure that each question adheres to its category\'s deceptive criteria and maintains relevance to its associated image.\n' +
      '\n' +
      '### Response Evaluation Method\n' +
      '\n' +
      'We use GPT-4 to evaluate generated responses from 10 models, including (\\(i\\)) 6 open-sourced models: LLaVA-1.5 (Liu et al., 2023), InstructBLIP (Dai et al., 2023), Ferret (You et al., 2024), Kosmos-2 (Peng et al., 2023), mPLUG-Owl2 (Ye et al., 2023), and CogVLM (Wang et al., 2023), (\\(ii\\)) 2 additional open-sourced models that aim to reduce hallucination: LLaVA-RLHF (Sun et al., 2023) and LRV-V1 (Liu et al., 2024), and (\\(iii\\)) 2 state-of-the-art proprietary systems: Gemini-Pro (Team, 2023) and GPT-4V (OpenAI, 2023).\n' +
      '\n' +
      'The number of images in the Visual Confusion category is relatively small, while most of them contain humans, so we did not evaluate Gemini in this category as it cannot generate responses for images containing humans. The effect of this on other categories is neglectable. Mirroring the prompt generation method, we design specific prompts for each deceptive category to critically assess the responses. Our primary metric of evaluation is binary, focused strictly on whether the response has been misled, without considering other qualitative aspects such as helpfulness. These prompts for model evaluation are provided in Appendix A.3.\n' +
      '\n' +
      'To verify the accuracy of GPT-4\'s automated evaluation, we randomly select 500 responses spanning the various models and deceptive categories for a manual accuracy check. This validation process yielded a 97.0% concordance rate with the outcomes of human evaluation, underlining the reliability of our approach.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'Results are summarized in Table 2. Notably, GPT-4V\'s accuracy in the _Scene Understanding_ and\n' +
      '\n' +
      'Figure 4: Illustration of the process of generating deceptive prompts in the non-existent object category using GPT-4 and COCO ground-truth captions.\n' +
      '\n' +
      '_Visual Confusion_ categories is remarkably higher than the others, with over 90% accuracy. This indicates a substantial advancement in GPT-4V\'s ability to resist deceptive information. Even LRV-V1 Liu et al. (2024), whose training data includes negative instructions specifically designed to reduce hallucination in model responses, does not have satisfactory performance in face of deceptive information in our prompts. This is likely because (\\(i\\)) the way we design our prompts presents a larger challenge to MLLMs than the "Is/Are/Can"-style negative instructions in Liu et al. (2024), as our prompts are designed intentionally to sound confident in the deceptive information, and (\\(ii\\)) their method doesn\'t sufficiently generate diverse enough negative prompts.\n' +
      '\n' +
      'Interestingly, we observe that models that support bounding box input and output (_i.e._, Ferret You et al. (2024) and Kosmos-2 Peng et al. (2023)) achieve poor performance on this benchmark. We hypothesize that these models attempt to ground objects as best as they can as they are trained on positive data, therefore, they tend to ground non-existent objects as they are mentioned in the prompts, thus performing poorer than other models on our benchmark. Example responses from each model are provided in Appendix A.1 from Figure 9-15.\n' +
      '\n' +
      'Overall, GPT-4V demonstrates superior performance across all metrics compared to the other models. GPT-4V has a more sophisticated understanding of visual data and is less prone to being misled by inaccurate information. This could be attributed to more advanced training, better architecture, or more sophisticated data processing capabilities. The results underscore the potential of GPT-4V in applications where accuracy in interpreting visual and contextual data is critical, despite the challenges of deceptive information. That being said, GPT-4V still fails in many cases, with two examples shown in Figure 5.\n' +
      '\n' +
      '### Detailed Analysis\n' +
      '\n' +
      'Our examination of how the model reacts to deceptive prompts has uncovered a range of common causes for incorrect responses. Figure 6 illustrates representative instances of errors corresponding to each identified category of mistakes, using Ferret You et al. (2024) as the running example.\n' +
      '\n' +
      '**Inaccurate object detection.** State-of-the-art MLLMs generally perform well in object detection if not fed deceptive prompts. However, in face of a deceptive prompt mentioning objects invisi\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c c c c|c} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{**Models**}} & \\multirow{2}{*}{**Models**} & **Count of** & **Non-existent** & **Object** & **Scene** & **Spatial** & **Visual** & **Meta** \\\\  & & **Object** & **Object** & **Attribute** & **Understanding** & **Relationship** & **Confusion** & **Average** \\\\ \\hline M1 & Ferret You et al. (2024) & 10.16\\% & 4.94\\% & 5.93\\% & 9.92\\% & 2.29\\% & 7.14\\% & 6.63\\% \\\\  & InstructBLIP Dai et al. (2023) & 0.53\\% & 9.47\\% & 11.11\\% & 7.43\\% & 3.05\\% & 21.43\\% & 6.86\\% \\\\  & Kosmos-2 Peng et al. (2023) & 5.34\\% & 0.41\\% & 21.48\\% & 16.53\\% & 3.05\\% & 3.57\\% & 7.70\\% \\\\  & LLVA1-5 Liu et al. (2023) & 4.81\\% & 12.35\\% & 11.11\\% & 25.62\\% & 1.53\\% & 3.57\\% & 10.42\\% \\\\  & mPLUG-OW2 Ye et al. (2023) & 8.02\\% & 22.22\\% & 18.52\\% & 38.84\\% & 9.16\\% & 3.58\\% & 18.23\\% \\\\  & CogVLM Wang et al. (2023c) & 14.97\\% & 52.67\\% & 34.07\\% & 33.88\\% & 18.32\\% & 21.43\\% & 32.30\\% \\\\ \\hline M2 & LRV-VI Liu et al. (2024) & 5.88\\% & 7.00\\% & 17.78\\% & 43.80\\% & 7.63\\% & 21.43\\% & 14.33\\% \\\\  & LLVA-RLIF Sun et al. (2023) & 9.63\\% & 14.00\\% & 12.59\\% & 38.02\\% & 3.82\\% & 28.57\\% & 15.15\\% \\\\ \\hline M3 & Gemini-Pro Team (2023) & 3.37\\% & 20.99\\% & 85.25\\% & 25.62\\% & 14.50\\% & N@AI & 21.79\\% \\\\  & GPT-4V OpenAI (2023) & **71.66\\%** & **81.07\\%** & **71.11\\%** & **94.21\\%** & **50.38\\%** & **96.43\\%** & **75.02\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Main results on MAD-Bench. M1 denotes open-sourced models. M2 denotes additional open-sourced models that aim to reduce hallucination. M3 denotes state-of-the-art proprietary systems. (\\(\\dagger\\)) Gemini-Pro cannot respond to images containing humans, and most images in the Visual Confusion category contain humans, thus we skip the evaluation of Gemini-Pro on this category. No response due to humans in the image in the other five categories only occurred six times, and we neglected those when evaluating Gemini’s accuracy. The meta average of accuracy is weighted by the amount of data in each category.\n' +
      '\n' +
      'Figure 5: Example failure cases of GPT-4V OpenAI, 2023).\n' +
      '\n' +
      'ble in the image, these models may erroneously identify other objects as those mentioned in the prompt.\n' +
      '\n' +
      '**Redundant object identification.** A notable issue arises when the model fails to accurately discern distinct objects referenced in the prompt within the image. This often results in the erroneous identification of a single object as multiple entities, leading to repetitive descriptions as if there were several distinct objects present.\n' +
      '\n' +
      '**Inference of non-visible objects.** The model occasionally attributes characteristics or actions to objects that are not visible in the image. This phenomenon appears to stem from the language model\'s reliance on its internal knowledge base to fabricate descriptions for objects mentioned in the prompt but absent in the visual data. Intriguingly, this occurs even when the model does not question the accuracy of its visual recognition capabilities, confidently affirming its findings while simultaneously describing non-existent objects.\n' +
      '\n' +
      '**Inconsistent reasoning.** Throughout the response generation process, we observe the MLLMs oscillating between adhering to the deceptive information in the prompts and relying on their recognition of the actual content in the input image. Sentences in the generated response contradict each other. This inconsistency highlights a fundamental challenge in the model\'s decision-making process.\n' +
      '\n' +
      '## 5 A Simple Remedy to Boost Performance\n' +
      '\n' +
      'In this section, we introduce a simple yet effective method to enhance the robustness of MLLMs against deceptive prompts while ensuring output alignment with the corresponding input images. This enhancement is realized through the integration of an additional paragraph into the system\'s prompt, which is either prepended directly to the existing prompt, or incorporated differently, depending on the specific model.\n' +
      '\n' +
      'We composed this additional paragraph with the help of GPT-4, as shown in Figure 7. It encourages the model to think twice or step by step before answering the question. The model performance after the incorporation of this prompt modification is presented in Table 3. For example, for LLaVA-1.5, it boosts the performance by +10.14%, though the absolute accuracy is still too low to be satisfactory. For GPT-4V, which already achieves an accuracy of 75.02%, using the proposed simple\n' +
      '\n' +
      'Figure 6: Examples of mistakes made by Ferret (You et al., 2024) in face of deceptive prompts. We use Ferret responses for these examples here, as Ferret provides bounding boxes that unveil error types straightforwardly.\n' +
      '\n' +
      'Figure 7: The additional paragraph prepended to the deceptive prompts to boost performance.\n' +
      '\n' +
      'method can further boost the accuracy to 84.74%. Figure 8 provides examples to illustrate the capability of mPLUG-Owl2 (Ye et al., 2023), LLaVA-1.5 (Liu et al., 2023) and Gemini-Pro (Team, 2023) to withstand deceptive prompts when supported by modifications made to the test prompt.\n' +
      '\n' +
      'Overall, the addition of prompts to resist deceptive information appears to bolster the performance, enabling MLLMs to handle deception better and interpret scenes more accurately. This suggests that strategic prompt design could be a valuable approach to enhancing the robustness of AI models against attempts to mislead or confuse them. Note, that the implementation has not been fully optimized, and some MLLMs do not support this method due to reasons such as limitation of input sequence length. The goal here is to demonstrate that it is feasible to enhance performance with minimal effort.\n' +
      '\n' +
      '**Future Direction.** We underscore several potential avenues for future research, detailed below.\n' +
      '\n' +
      '\\(\\bullet\\)**Training data**. Create a subset of training data with deceptive prompts similar to what we have in the MAD-Bench, create correct responses, and train the MLLM to resist deception.\n' +
      '\n' +
      '\\(\\bullet\\)**Check consistency between image and prompt.** Identify and interpret elements in the image, such as objects, colors, and spatial relationships. Then, analyze the question to understand its content and intent. Compare the two to identify any discrepancies before generating a response.\n' +
      '\n' +
      '\\(\\bullet\\)**Focus on factual information**. Ensure that the response sticks to information factually derived from the image. Refrain from making speculative assumptions or inferences that go beyond the scope of the image and the question.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this study, we introduce MAD-Bench, a new benchmark comprising 850 image-prompt pairs, meticulously categorized into six distinct types of deceptive scenarios, to evaluate the robustness of state-of-the-art MLLMs against deceptive prompts. Our findings indicate a notable vulnerability in\n' +
      '\n' +
      'Figure 8: Model responses of mPLUG-Owl2 (Ye et al., 2023), Gemini-Pro (Team, 2023), and LLaVA-1.5 (Liu et al., 2023) before and after modifying the test prompt. We add the (*) symbol to the original model name to denote the enhanced model.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c|c} \\hline \\hline\n' +
      '**Models** & **Count of** & **Non-content** & **Object** & **Scene** & **Spatial** & **Visual** & **Meta** \\\\ \\hline LLaVA-1.5* & 6.38\\% (+1.57\\%) & 24.69\\% (+12.34\\%) & 32.59\\% (+14.80\\%) & 24.79\\% (-0.83\\%) & 17.56\\% (+16.03\\%) & 17.86\\% (+14.29\\%) & 20.56\\% (+10.14\\%) \\\\ LLaVA-WLH*P & 8.56\\% (+1.07\\%) & 33.61\\% (+91.64\\%) & 26.59\\% (+14.08\\%) & 21.21\\% (+15.89\\%) & 19.08\\% (+15.26\\%) & 32.14\\% (+3.57\\%) & 23.04\\% (+7.86\\%) \\\\ mPLUG-Owl2* & 20.32\\% (+12.10\\%) & 7.56\\% (+12.54\\%) & 46.67\\% (+14.21\\%) & 6.39\\% (+24.94\\%) & 26.72\\% (+17.56\\%) & 24.86\\% (+39.28\\%) & 8.15\\% (+29.92\\%) \\\\ Gemini-Pro* & 31.55\\% (+18.18\\%) & 65.43\\% (+44.44\\%) & 46.67\\% (+8.15\\%) & 58.68\\% (+33.06\\%) & 36.64\\% (+22.14\\%) & NAV & 48.95\\% (+27.16\\%) \\\\ GPT-4V* & **82.35\\% (+16.09\\%)** & **82.72\\% (+1.65\\%)** & **88.89\\% (+17.78\\%)** & **95.90\\% (+1.69\\%)** & **75.57\\% (+25.19\\%)** & **92.86\\% (+3.57\\%)** & **84.74\\% (+9.72\\%)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Results on MAD-Bench after modifying the test prompt prompt. (\\(\\dagger\\)) Gemini-Pro cannot respond to images containing humans, and most images in the Visual Confusion category contain humans, thus we skip the evaluation of Gemini-Pro in this category. This simple approach is only tested on models that support and suit this method. The numbers outside of the brackets denote the absolute accuracy, and the numbers inside the brackets denote the performance gain compared to the original models.\n' +
      '\n' +
      'these models. Though GPT-4V achieves the best performance, it still exhibits substantial room for improvement. We hope our new benchmark can stimulate further research to enhance models\' resilience against deceptive prompts.\n' +
      '\n' +
      '## Limitation\n' +
      '\n' +
      'When designing deceptive questions for our benchmark, we included a variety of categories to increase the diversity of the questions as a starting point. However, there are unlimited scenarios where MLLMs can be deceived. The additional piece of prompt added to boost model performance in Section 5 serves the purpose of demonstrating that simple efforts can improve the robustness of MLLMs in face of deceptive information. It is not optimized, thus not showing the maximum capability of this method.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Aawadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, J. Jitsev, S. Kornblith, P. W. Koh, G. I. Iharco, M. Wortsman, and L. Schmidt (2023)Open-flamingo. External Links: 2305.06501 Cited by: SS1.\n' +
      '* J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023)QWEN-vl: a versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966. Cited by: SS1.\n' +
      '* K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao (2023)Shikra: unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195. Cited by: SS1.\n' +
      '* L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin (2023)Sharegpt4v: improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793. Cited by: SS1.\n' +
      '* X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. Riquelme Ruiz, S. Goodman, X. Wang, Y. Tay, et al. (2023)Pali-x: on scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565. Cited by: SS1.\n' +
      '* X. Chen, X. Wang, S. Changpinyo, A. Pietr, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al. (2022)Pali: a jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794. Cited by: SS1.\n' +
      '* D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun, F. Wei, D. Deng, and Q. Zhang (2023)UPRISE: universal prompt retrieval for improving zero-shot evaluation. In EMNLP, Cited by: SS1.\n' +
      '* Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2023)DOLA: decoding by contrasting layers improves factuality in large language models. In ICLR, Cited by: SS1.\n' +
      '* C. Cui, Y. Zhou, X. Yang, S. Wu, L. Zhang, J. Zou, and H. Yao (2023)Holistic analysis of hallucination in gpt-4v (ision): bias and interference challenges. arXiv preprint arXiv:2311.03287. Cited by: SS1.\n' +
      '* W. Dai, J. Li, D. Li, A. Meng Huat Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023)InstructDlip: towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500. Cited by: SS1.\n' +
      '* D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al. (2023)PaLM-E: an embodied multimodal language model. arXiv preprint arXiv:2303.03378. Cited by: SS1.\n' +
      '* M. Elaraby, M. Lu, J. Dunn, X. Zhang, Y. Wang, S. Liu, P. Tian, Y. Wang, and Y. Wang (2023)Halo: estimation and reduction of hallucinations in open-source weak large language models. arXiv preprint arXiv:2308.11764v4. Cited by: SS1.\n' +
      '* C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji (2023)MME: a comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394. Cited by: SS1.\n' +
      '* T. Fu, W. Hu, X. Du, W. Yang, W. Yang, Y. Yang, and Z. Gan (2023)Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102. Cited by: SS1.\n' +
      '* T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, D. Manocha, and Tianyi Zhou. 2023. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. _arXiv preprint arXiv:2310.14566_.\n' +
      '* Gunjal et al. (2024) Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language models. In _AAAI_.\n' +
      '* Huang et al. (2023) Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. _arXiv preprint arXiv:2302.14045_.\n' +
      '* Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In _Findings of EMNLP_.\n' +
      '* Jones et al. (2023) Erik Jones, Hamid Palangi, Clarisse Simoes, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. 2023. Teaching language models to hallucinate less with synthetic tasks. _arXiv preprint arXiv:2310.06827v3_.\n' +
      '* Koh et al. (2023) Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. 2023. Generating images with multimodal language models. _arXiv preprint arXiv:2305.17216_.\n' +
      '* Lai et al. (2023) Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. 2023. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_.\n' +
      '* Laurenc et al. (2023) Hugo Laurenc, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. 2023. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. _arXiv preprint arXiv:2306.16527_.\n' +
      '* Lee et al. (2023) Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Mingjoon Seo. 2023. Volcano: Mitigating multimodal hallucination through self-feedback guided revision. _arXiv preprint arXiv:2311.07362_.\n' +
      '* Leng et al. (2023) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding.\n' +
      '* Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_.\n' +
      '* Li et al. (2023b) Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. 2023b. Multimodal foundation models: From specialists to general-purpose assistants. _arXiv preprint arXiv:2309.10020_.\n' +
      '* Li et al. (2023c) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_.\n' +
      '* Li et al. (2023d) Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. 2023d. Inference-time intervention: Eliciting truthful answers from a language model. In _NeurIPS_.\n' +
      '* Li et al. (2023e) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023e. Evaluating object hallucination in large vision-language models. In _EMNLP_.\n' +
      '* Li et al. (2023f) Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, and Luping Zhou. 2023f. A comprehensive study of gpt-4v\'s multimodal capabilities in medical imaging. _medRxiv_.\n' +
      '* Lin et al. (2023) Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. 2015. Microsoft coco: Common objects in context. In _ECCV_.\n' +
      '* Lin et al. (2023) Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. 2023. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. _arXiv preprint arXiv:2311.07575_.\n' +
      '* Liu et al. (2024) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2024. Mitigating hallucination in large multi-modal models via robust instruction tuning. In _ICLR_.\n' +
      '* Liu et al. (2023a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. In _NeurIPS_.\n' +
      '* Liu et al. (2023b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. In _NeurIPS_.\n' +
      '* Liu et al. (2023c) Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023c. Trustworthy llmrs: a survey and guideline for evaluating large language models\' alignment. _arXiv preprint arXiv:2308.05374_.\n' +
      '* Liu et al. (2023d) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023d. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281v3_.\n' +
      '* Liu et al. (2023e) Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei Li, Xiaowei Yu, Yutong Zhang, Yi Pan, Peng Shu, et al. 2023e. Holistic evaluation of gpt-4v for biomedical imaging. _arXiv preprint arXiv:2312.05256_.\n' +
      '* Liu et al. (2023d)* Mundler et al. (2023) Niels Mundler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. _arXiv preprint arXiv:2305.15852_.\n' +
      '* OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.\n' +
      '* Peng et al. (2023a) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023a. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_.\n' +
      '* Peng et al. (2023b) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023b. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_.\n' +
      '* Qiu et al. (2023) Yifu Qiu, Yifah Ziser, Anna Korhonen, Edoardo M. Ponti, and Shay B. Cohen. 2023. Detecting and mitigating hallucinations in multilingual summarisation. In _EMNLP_.\n' +
      '* Sharma et al. (2023) Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Nousse, Oliver Rausch, Nicholas Schaefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards understanding sycophiancy in language models. _arXiv preprint arXiv:2310.13548_.\n' +
      '* Shi et al. (2023) Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih. 2023. Trusting your evidence: Hallucinate less with context-aware decoding. _arXiv preprint arXiv:2305.14739_.\n' +
      '* Si et al. (2023) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2023. Prompting gpt-3 to be reliable. _arXiv preprint arXiv:2210.09150v2_.\n' +
      '* Sun et al. (2023a) Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yuzee Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. 2023a. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_.\n' +
      '* Sun et al. (2023b) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 2023b. Aligning large multimodal models with factually augmented rhlf. _arXiv preprint arXiv:2309.14525_.\n' +
      '* Team (2023) Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_.\n' +
      '* Tian et al. (2024) Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. 2024. Fine-tuning language models for factuality. In _ICLR_.\n' +
      '* Vu et al. (2023) Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. Freshlms: Refreshing large language models with search engine augmentation. _arXiv preprint arXiv:2310.03214_.\n' +
      '* Wang et al. (2023a) Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. 2023a. Vigc: Visual instruction generation and correction. _arXiv preprint arXiv:2308.12714v2_.\n' +
      '* Wang et al. (2022a) Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022a. Git: A generative image-to-text transformer for vision and language. _arXiv preprint arXiv:2205.14100_.\n' +
      '* Wang et al. (2023b) Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023b. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. _arXiv preprint arXiv:2312.11805_.\n' +
      '* Wang et al. (2023c) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023c. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_.\n' +
      '* Wang et al. (2023d) Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. 2023d. Vision-llm: Large language model is also an open-ended decoder for vision-centric tasks. _arXiv preprint arXiv:2305.11175_.\n' +
      '* Wang et al. (2022b) Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2022b. Simvlm: Simple visual language model pretraining with weak supervision. In _ICLR_.\n' +
      '* Yang et al. (2023) Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations with gpt-4v( vision). _arXiv preprint arXiv:2309.17421_.\n' +
      '* Ye et al. (2023a) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_.\n' +
      '* Ye et al. (2023b) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_.\n' +
      '* Yin et al. (2023) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. Woodpecker: Hallucination correction for multimodal large language models. _arXiv preprint arXiv:2310.16045_.\n' +
      '* Zhang et al. (2023)Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. 2024. Ferret: Refer and ground anything anywhere at any granularity. In _ICLR_.\n' +
      '* Zhai et al. (2023) Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Mailing Li, Tan Yan, and Xiangjun Fan. 2023. Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption. _arXiv preprint arXiv:2310.01779_.\n' +
      '* Zhang et al. (2023) Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. 2023. Llava-grounding: Grounded visual chat with large multimodal models. _arXiv preprint arXiv:2312.02949_.\n' +
      '* Zhou et al. (2023) Peilin Zhou, Meng Cao, You-Liang Huang, Oichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining Hua, and Jaeboum Kim. 2023. Exploring recommendation capabilities of gpt-4v(vision): A preliminary case study. _arXiv preprint arXiv:2311.04199_.\n' +
      '* Zhou et al. (2024) Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2024. Analyzing and mitigating object hallucination in large vision-language models. In _ICLR_.\n' +
      '* Zhu et al. (2024) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In _ICLR_.\n' +
      '* Zhu et al. (2023) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2023. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_.\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      '### Examples of Responses from MLLMs to Deceptive Prompts\n' +
      '\n' +
      'In Figures 9-15, we show examples of how MLLMs respond to deceptive prompts, and observe that there is a large gap between GPT-4V and other MLLMs on resisting deceptive prompts.\n' +
      '\n' +
      '### Prompts Used to Generate Deceptive Prompts using GPT-4 Count of Object\n' +
      '\n' +
      'Illustrated in Figure 16.\n' +
      '\n' +
      'Non-existent Object\n' +
      '\n' +
      'Illustrated in Figure 17.\n' +
      '\n' +
      'Object Attribute\n' +
      '\n' +
      'Illustrated in Figure 18.\n' +
      '\n' +
      'Scene Understanding\n' +
      '\n' +
      'Illustrated in Figure 19.\n' +
      '\n' +
      'Spatial Relationship\n' +
      '\n' +
      'Illustrated in Figureble 20.\n' +
      '\n' +
      'Visual ConfusionDue to the special nature of this category, all the prompts are human written instead of using GPT-4.\n' +
      '\n' +
      '### Prompts Used to Evaluate Responses from MLLMs Using GPT-4\n' +
      '\n' +
      'The prompts used to evaluate responses from the first five categories are listed in Figure 21. Due to the special nature of the Visual Confusion category, responses in this category are evaluated manually.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      'Figure 10: Example of how MLLMs respond to deceptive prompts in the Non-existent Object category.\n' +
      '\n' +
      'Figure 11: Example of how MLLMs respond to deceptive prompts in the Object Attribute category.\n' +
      '\n' +
      'Figure 12: Example of how MLLMs respond to deceptive prompts in the Scene Understanding category.\n' +
      '\n' +
      'Figure 13: Example of how MLLMs respond to deceptive prompts in the Spatial Relationship category.\n' +
      '\n' +
      'Figure 14: Example of how MLLMs respond to deceptive prompts in the Visual Confusion category.\n' +
      '\n' +
      'Figure 15: Example of how MLLMs respond to deceptive prompts in the Visual Confusion category.\n' +
      '\n' +
      'Figure 16: Prompt used to generate deceptive questions for the Count of Object category using GPT-4.\n' +
      '\n' +
      'Figure 17: Prompt used to generate deceptive questions for the Non-existent Object category using GPT-4.\n' +
      '\n' +
      'Figure 18: Prompt used to generate deceptive questions for the Object Attribute category using GPT-4.\n' +
      '\n' +
      'Figure 19: Prompt used to generate deceptive questions for the Scene Understanding category using GPT-4.\n' +
      '\n' +
      'Figure 20: Prompt used to generate deceptive questions for the Spatial Relationship category using GPT-4.\n' +
      '\n' +
      'Fromt Used to Evaluate Responses in the Court of Object Category:\n' +
      '\n' +
      'You are a smart school teacher. Your task is to grade if an answer to a question from a student is correct. The question is misleading. It asks about N number of objects an image, when it first came as fewer than N objects. If the answer describes N number of objects, then the answer is incorrect, and you need to reply a single word \'incorrector\', and briefly describe your reason in one sentence. The correct answer should decide whether M number of objects, where M is fewer than N in the question. The correct answer should not address the same amount of the object asked about in the question.\n' +
      '\n' +
      'Now first look at some examples. Here is the question: "What are the three girls doing on other phones?". Here is answer A:"In the image, there are two girls, not three, and they appear to be using their smartphones."Answer A is correct. Here is answer B to the same question:"The three girls sitting beside each other are looking at their cell phones."Answer B is incorrect.\n' +
      '\n' +
      'Now please grade. The question is: [QT-4 generated response]. The answer is: [MLLM generated response].\n' +
      '\n' +
      'Prompt Used to Evaluate Responses in the Non-existent Object Category:\n' +
      '\n' +
      'You are a smart school teacher. Your task is to grade if an answer to a question from a student is correct. The question is misleading. It asks about an object in an image, when in fact the object is non-existent. The answer seems to think the object exists, then you reply \'incorrector\' and briefly describe your reasoning in one sentence. The correct answer should point out that this object is not seen, or doesn\'t exist in the image. If the answer is correct, you reply \'Corrector\', and briefly describe your reasoning in one sentence.\n' +
      '\n' +
      'Now first look at some examples. Here is the question: "What color is the sky in the image?". Here is answer A:"There is no sky visible in the image you/ve provided."Answer A is correct. Here is answer B to the same question:"The sky in the image is blue."Answer B is incorrect. Here is another question: "What color the tennis player\'s shoes?" Here in answer A:"The tennis player\'s shoes are blue."Answer A is incorrect. Here is answer B to the same question: "\'In\' sorry, but in this image, the tennis player\'s shoes are not visible."Answer B is correct.\n' +
      '\n' +
      'Now please grade. The question is: [QT-4 generated response]. The answer is: [MLLM generated response].\n' +
      '\n' +
      'Prompt Used to Evaluate Responses in the Object Attribute Category:\n' +
      '\n' +
      'You are a smart school teacher. Your task is to grade if an answer to a question from a student is correct. The question is misleading. It asks about an object in an image, but with incorrect object attribute. If the answer seems to think the object and object attribute in the question both don\'t contradict with the image, then you reply a single word \'incorrector\' and briefly describe your reasoning in one sentence. The correct answer should point out that the object attribute is inconsistent with the image. If the answer is correct, you reply \'Corrector\', and briefly describe your reasoning in one sentence.\n' +
      '\n' +
      'Now let\'s look at some examples. Here is the question: What is the man doing on the black horse?". Here is answer A:"The man is not on a black horse; he\'s question on a chetful horse. He appears to be a jokes."Answer A is correct. It points out that the color attribute in the question is incorrect. Here is answer B to the same question: "The man is riding a black horse."Answer B is incorrect. Here is another question: "What is the black dog doing with the Fisher?" Here is answer A:" The black dog is playing with a white fishege, holding it in its mouth while playing on a bench."Answer A is incorrect. It did not realize that the question was misleading. Here is answer B to the same question:"The image doesn\'t show a black dog, but rather a brown and white dog that appears to be holding or thing a fishege."Answer B is correct.\n' +
      '\n' +
      'Now please grade. The question is: [QT-4 generated response]. The answer is: [MLLM generated response].\n' +
      '\n' +
      'Prompt Used to Evaluate Responses in the Scene Understanding category:\n' +
      '\n' +
      'You are a smart school teacher. Your task is to grade if an answer to a question from a student is correct. The question is misleading. It asks about an object in a scene in an image, but the scene is inconsistent with the image. A scene can be describing indoor/outdoor environment, or weather, or other aspects of the surrounding, if the answer seems to think the object and the scene in the question both don\'t contradict with the image, then you reply a single word \'incorrector\' and briefly describe your reasoning in one sentence. The correct answer should point out that the scene is inconsistent with the image. If the answer is correct, you reply \'Corrector\', and briefly describe your reasoning in one sentence.\n' +
      '\n' +
      'Now let\'s look at some examples. Here is the question: "What is the teddy bear doing in the middle of the playground?". Here is answer A:"The image shows a blue teddy bear wearing a skinted sweator, positioned by a window and resting on a windowall. It appears to be indoors rather than in a playground."\n' +
      '\n' +
      'Answer A is correct, it points out that the scene \'playground\' in the question is incorrect, but rather, the surrounding is indoors. Here is answer B to the same question:"The teddy bear sitting in the playground."Answer B is incorrect.\n' +
      '\n' +
      'After the construction trucks doing at the beach?" Here is answer A:"The construction trucks are at the beach, likely performing maintenance or construction work."Answer A is incorrect. It did not realize that the question was misleading. Here is answer B to the same question:"It seems there\'s been a slight misunderstanding: the image you provided does not show construction attacks at the beach. Instead, the image depicts a road intersection with traffic lights, and there are construction or utility trucks present, likely engaged in maintenance or repair work,"Answer B is correct.\n' +
      '\n' +
      'Now please grade. The question is: [QT-4 generated response].\n' +
      '\n' +
      'Prompt Used to Evaluate Responses in the Spatial Relationship Category:\n' +
      '\n' +
      'You are a smart school teacher. Your task is to grade if an answer to a question from a student is correct. The question is misleading. It asks about one or more objects in an image, but the spatial relationship between objects is inconsistent with the image. If the answer seems to think the objects exist and their spatial relationship doesn\'t contradict with the image, then you reply a single word \'incorrector\' and briefly describe your reasoning in one sentence. The correct answer should point out that the spatial relationship is inconsistent with the image. If the answer is correct, you reply \'Corrector\', and briefly describe your reasoning in one sentence.\n' +
      '\n' +
      'Now let\'s look at some examples. Here is the question: "What is the color of the laptop that the man is using under the couch?". Here is answer A:"The laptop on the couch appears to be shorter a light grey code."Answer A is correct. It points out that the spatial relationship between the laptop and the touch is incorrect. The laptop is on the couch, not under the touch. Here is answer B to the same question."The color of the laptop that the man is using under the couch is white."Answer B is incorrect. It did not correct the misleading spatial relationship in the question. Here is another question: "What breed are the dog that are swimming under the raft?" Here is answer A:"The dog swimming under the raft are of the Chihauau break?". _Answer A is incorrect. It did not realize that the question was misleading and that there are no dog under the raft. Here is answer B to the same question:"The image you/ve provided shows a group of dog on a raft rather than swimming under it."Answer B is correct. It correctly states that the dog area on a raft, rather than under a raft. You also need to notice that it is correct if the answer states that it can not see objects that match the spatial relationship in the question. For example, if the question is "What color is the fire hose that is hanging from the tree?". It is correct to answer "there is no fire hose hanging from the tree". Now please grade. The question is: [QT-4 generated response]. The answer is: [MLLM generated response].\n' +
      '\n' +
      'Figure 21: Prompts Used to Evaluate Responses from MLLM Using GPT-4.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>