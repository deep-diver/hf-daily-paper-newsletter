<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# TexDreamer: Zero-Shot High-Fidelity 3D Human Texture 생성을 위한\n' +
      '\n' +
      'Yufei Liu\n' +
      '\n' +
      '중국 상하이 텐센트 유튜 연구소.1 상하이 대학교에서 인턴십을 하는 동안 작업이 수행됩니다.\n' +
      '\n' +
      'Junwei Zhu\n' +
      '\n' +
      '중국 상하이대학교 1텐센트청소년연구소\n' +
      '\n' +
      'Junshu Tang\n' +
      '\n' +
      '중국 상하이대학교\n' +
      '\n' +
      'Shijie Zhang\n' +
      '\n' +
      '중국 상하이 4푸단대학교\n' +
      '\n' +
      'Jiangning Zhang\n' +
      '\n' +
      '중국 상하이대학교 2텐센트청소년연구실\n' +
      '\n' +
      'Weijian Cao\n' +
      '\n' +
      '중국 상하이대학교 2텐센트청소년연구실\n' +
      '\n' +
      'Chengjie Wang\n' +
      '\n' +
      '중국 상하이대학교 2텐센트청소년연구실\n' +
      '\n' +
      'Yunsheng Wu\n' +
      '\n' +
      '중국 상하이대학교 2텐센트청소년연구실\n' +
      '\n' +
      'Dongjin Huang\n' +
      '\n' +
      '중국 상하이 텐센트 유튜 연구소.1 상하이 대학교에서 인턴십을 하는 동안 작업이 수행됩니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '시맨틱 UV 맵으로 3D 인간을 텍스처링하는 것은 합리적으로 펼쳐진 UV를 획득하는 어려움으로 인해 여전히 과제로 남아 있다. 최근 텍스트-투-이미지(T2I) 모델을 이용한 멀티뷰 렌더링의 감독에서 텍스트-투-3D 발전에도 불구하고, 생성 속도, 텍스트 일관성 및 텍스처 품질에 문제가 지속되어 기존 데이터 세트 간의 데이터 부족이 발생한다. 첫 번째 제로샷 멀티모달 고충실도 3D 인간 텍스처 생성 모델인 **TexDreamer**를 소개합니다. 효율적인 텍스쳐 적응 미세화 전략을 이용하여, 기존의 일반화 능력을 유지하면서 큰 T2I 모델을 시맨틱 UV 구조에 적용한다. 새로운 특징 변환기 모듈을 활용하여, 훈련된 모델은 몇 초 이내에 텍스트 또는 이미지로부터 고 충실도의 3D 인간 텍스처를 생성할 수 있다. 또한, 본 논문에서는 50k의 고충실도 텍스처를 포함하는 가장 큰 고해상도(1,024\\times 1,024) 3차원 인간 텍스처 데이터셋인 **ArT**icu**L**ated \\(\\text{hum}\\textbf{An}\\) 텍스처**S**(ATLAS)를 소개한다.\n' +
      '\n' +
      '키워드: 인간 텍스처 멀티모달 텍스처 합성\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '3D 인간 질감은 매력적인 3D 인간 모델을 만드는 데 중요하고 필수적인 역할을 한다. UV 맵은 왜곡, 중첩 및 스트레칭을 최소화하여 3D 모델의 매끄럽고 정확한 텍스처링을 가능하게 한다. UV는 영화 제작, 게임, 가상현실 등 다양한 산업 분야에서 널리 사용되고 있다. 그러나 UV가 합리적으로 펼쳐진 고품질 질감을 얻는 것은 지루하고 시간이 많이 걸리는 작업일 수 있다. 현대 그래픽 제작에서 3D 인간 텍스처의 생성은 주로 경험 많은 텍스처 페인팅 아티스트와 함께 비싼 3D 스캐너에 의존한다. 스캐닝 프로세스는 다중 카메라 어레이와 구조화된 광으로 구축된 캡처 시스템을 필요로 한다. 텍스처 페인팅은 DCC 소프트웨어, 예를 들어 물질 페인터, ZBrush 및 포토샵을 사용하는 데 능숙한 훈련된 아티스트의 전문 지식을 요구한다. 잘 구조화된 인간 UV 지도는 종종 몇 주간의 헌신적인 노력이 필요하다.\n' +
      '\n' +
      '최근 텍스트-이미지 분야에서 중요한 업적은 3D 전적을 사용하여 텍스트 설명으로부터 3D 인간 모델을 직접 생성하는 것을 가능하게 했다. 그러나, 인간 지향 최적화 방법들[5, 18, 24, 26, 69, 29, 66]은 시간이 많이 소요되고 렌더링 해상도 제약으로 인해 제한된 텍스처 품질을 겪는다. 더욱이, 이러한 방법들을 실제로 사용하는 것은 마칭 큐브[34]와 같은 메쉬 추출 알고리즘을 필요로 하며, 이는 UV 레이아웃 및 메쉬 토폴로지를 보존하는 데 어려움을 나타내어 수정 프로세스를 매우 불편하게 만든다. 비최적화 텍스쳐 생성 방법은 주로 객체, TEXTure[46], Latent-Paint[38], Text2Tex[9]와 같은 접근법에 초점을 맞추고, LDM(Latent Diffusion Model)[47]을 사용하여 주어진 기하학의 다시점 텍스쳐를 완성한다. 그러나 복잡한 입력 모델을 다룰 때 불일치 및 공백이 발생할 수 있다.\n' +
      '\n' +
      '텍스트 외에도 2D 이미지는 3D 인간 텍스처의 매개체 역할도 할 수 있다. 단일 이미지로부터 텍스처를 예측하는 것은 주로 두 가지 과제에 직면한다. 가시 부분의 경우 UV 매핑은 픽셀 대 표면 대응 추정의 정확도에 영향을 받는다. 보이지 않는 부분의 경우 UV 결과는 모델의 인페인팅 능력에 의존한다. 효율적인 고품질 데이터가 없으면 아티팩트로 이어질 수 있습니다. 비디오 데이터 세트는 보이지 않는 부분의 질감을 추정하는 데 도움이 되는 다시점 정보를 제공한다. 그러나, 이 접근법은 프레임들에 걸친 픽셀-대-표면 대응 추정에서 더 높은 수준의 정밀도를 요구한다. 또한, 비디오 데이터 세트는 종종 양이 제한된다.\n' +
      '\n' +
      '그림 1: **Left: ATLAS 데이터 세트의 개요.** ATLAS는 지금까지 실제 아이덴티티와 허구 아이덴티티를 모두 포함하는 텍스트 설명과 쌍을 이루는 가장 큰 고해상도 \\((1,024\\times 1,024)\\) 3D 인간 텍스처 데이터 세트이다. **Right: Text2Cramer.**의 기본 구조 텍스트와 이미지 입력을 모두 지원하는 최초의 제로샷 고충실도 인간 텍스처 생성 방법.\n' +
      '\n' +
      ' 이러한 문제를 해결하기 위해, 우리는 3D 인간 텍스처 생성을 위한 갭을 메우는 최초의 제로-샷 멀티모달 고충실도 인간 텍스처 생성 방법인 텍스드리머를 소개한다. 우리의 방법은 가장 쉽게 구할 수 있는 원시 데이터, 텍스트 및 이미지 중 두 가지를 처리할 수 있다. 이러한 다양성으로 인해 우리의 방법은 다양한 사용 사례에 더 유연하고 적응할 수 있다. 먼저 텍스트-투-UV(Text-to-UV, T2UV)에 대한 효율적인 텍스처 적응 미세 조정을 수행한다. T2UV는 새로운 2단계 텍스처 프로젝션 프로세스에 의해 획득된 고품질 샘플 텍스처로 트레이닝되며, 원래 T2I 모델의 일반화 능력을 보존하면서 특정 UV 구조의 의미 및 위치 정보를 처리할 수 있다. 이미지-투-UV(Image-to-UV, I2UV)의 경우, DensePose[16]에 의해 추출된 부분 텍스처의 보이지 않는 부분을 예측하는 대신, 보다 의미적인 잠재 공간에서 이미지와 UV를 연결하는 것을 목표로 한다. 영상에서 추출된 시각적 특징을 T2UV의 텍스트 특징 공간으로 번역하기 위해 특징 번역기를 구축한다. 실제 이미지와 합성 인간 이미지 모두 420만 개로 훈련된 I2UV는 가장 높은 질감 품질과 텍스트 일관성을 보여준다. 또한, 가장 큰 고해상도(\\(1,024\\times 1,024\\)) 3D 인간 텍스처 데이터세트인 ATLAS(ArTicuLated humAn textureS) 데이터세트를 제안한다. ATLAS는 SMPL UV 공간에 적합한 50k 고충실도 인간 텍스처를 포함한다. 각각의 텍스처는 상세한 텍스트 설명과 쌍을 이룬다. 그림 3의 몇 가지 예를 참조하십시오. 1, 우리의 ALTAS 데이터 세트는 높은 충실도와 다양한 캐릭터 아이덴티티로 구별된다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약할 수 있다:\n' +
      '\n' +
      '* 효율적인 텍스쳐 적응 미세화 전략과 특징 변환기 모듈 설계를 통해 달성된 최초의 제로샷 멀티모달 고충실도 3D 인간 텍스쳐 생성 방법인 텍스드리머를 소개한다.\n' +
      '* 우리는 가장 큰 고해상도 3D 인간 텍스처 데이터세트인 ATLAS를 제안하여, 고충실도 3D 인간 텍스처의 공백을 메운다.\n' +
      '* 광범위한 실험은 우리의 방법이 두 양식 모두에 대한 텍스트 일관성 및 UV 품질에 관한 기존 접근법을 능가한다는 것을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**인간 관련 데이터 세트.** ATLAS와 기존 인간 관련 데이터 세트의 비교가 탭에 나와 있다. 1. 3D 스캔은 가장 높은 정밀도를 갖지만 획득하는데 가장 어렵고 시간이 많이 소요된다. [70] 전신 인체 스캔을 캡처하기 위해 맞춤 제작된 다중 카메라 능동 스테레오 시스템을 사용합니다. [78] 조밀한 DLSR 장비로 THuman을 구축하고, 그 이후의 [68]은 더 높은 해상도로 500개의 스캔을 제공한다. 의류를 별도로 예측하기 위해 의류 데이터 세트[35; 51; 51; 3]도 있다. 그러나 스캔 데이터는 보통 많은 정점(종종 수백만 개)과 구조화되지 않은 그리드를 가지고 있다. 추가 프로세스 없이, 스캔에서 텍스처 맵을 얻는 것은 어렵다. 복잡한 하드웨어와 높은 가격으로부터 자유롭기 위해, 일련의 연구 [1; 3; 12; 23; 31]는 신경망이 3D 전과, _e.g_로 단안 RGB 비디오로부터 3D 인간을 직접 재구성할 수 있다는 것을 증명했다. 파라메트릭 인체 모델 SMPL[33]. 인간 비디오 데이터세트[1; 23; 31]는 일반적으로 실제 인간 A-포즈 회전 비디오로서 나타난다. 이러한 종류의 데이터는 일반적으로 3D 정보를 포함하지 않는다. 일부 작업은 자산 사용성과 효율성을 향상시키기 위해 2D 이미지[61; 71]를 직접 애니메이션하지만 많은 작업은 데이터 세트[25, 32, 76]를 활용하는 단일 이미지[20, 49, 53, 60, 77]에서 3D 사람을 재구성하는 데 중점을 둔다. 획득 어려움으로 인해 일부 데이터 세트[1, 6, 27, 56]만이 UV 텍스처를 포함한다. 검색 등록, SURREAL [56]은 921개의 UV 텍스처를 제공합니다. 그러나, 프라이버시 정책 때문에, SURREAL UV 텍스처들은 모두 동일한 평균 얼굴을 갖는다. Lazova _et al_. [27] [54, 55]의 장비를 사용하여 스캔을 획득하고 상업적 데이터 세트 [2, 45]에서 구매하여 텍스처 데이터 세트를 제공하는데, 이는 힘들고 비용이 많이 든다. 우리의 텍스처 생성 방법에 더 가까운 것은 SMPLitex[6], 그들은 [1, 27]에서 미세 조정 안정 확산 모델[47]까지 10개의 UV 텍스처를 사용하여 텍스트 설명과 함께 100개의 UV 텍스처를 제공한다. 그러나 신분과 의류의 다양성이 부족합니다. 우리가 아는 한, 기존의 인간 데이터 세트 중 어느 것도 우리와 동일한 고품질 질감과 풍부한 정보를 가지고 있지 않다.\n' +
      '\n' +
      '**Texture Generation from Text.** text-to-image의 상당한 발전은 text-to-3D generation[10, 22, 30, 40, 43] 분야에서 상당한 관심을 끌었다. 이전에 SMPL을 사용한 인간 지향 최적화 방법 [5, 18, 24, 26, 29, 69]은 텍스트로 3D 아바타를 생성할 수 있는 큰 잠재력을 보여준다. AvatarCLIP[18]은 렌더링 이미지 상에서 감독하는 CLIP 점수[44]와 함께 [58]을 사용하여 메쉬 외관을 정제한다. Reveraging Score Distillation Score (SDS) Loss from DreamFusion [43], AvatarCraft [24]는 Instant-NGP [41]과 결합된 NeuS [58]을 사용하여 표준 공간에서 최적화한다. 제로 샷 추론 방법[9, 10, 38, 46, 39]은 텍스처링 3D 객체에서 큰 발전을 보여준다. PBR 재료[37]를 사용하여, Fantasia3D[10]은 현실적인 외관 모델링을 달성한다. Latent-NeRF[38]은 LDM[47]의 잠재 공간에 SDS 손실을 배치한다. TEXTure[46] 및 Text2Tex[9]는 3D 메쉬 텍스처 상에 다수의 뷰포인트들 및 인페인트를 업데이트한다. 10개의 훈련 데이터만을 사용하여, SMPLitex [6]은 일반화된 능력이 부족하고 결함이 있는 텍스처를 생성할 수 있다.\n' +
      '\n' +
      '**이미지로부터의 텍스처 생성.** 작업 그룹 [6, 7, 15, 63, 74]은 텍스처 생성을 지시하는 데 전념한다. 이미지 대 이미지 생성 접근법[7, 8, 21]은 보통 UV를 생성하기 위해 GAN 기반 네트워크를 사용한다. exformer[63]는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & 3D Shape UV & Textures Texture Resolution Text & Description \\\\ \\hline BUFF [70] & ✓ & \\(12^{*}\\) & N/A & ✗ \\\\ CAPE [35] & ✓ & \\(15^{*}\\) & N/A & ✗ \\\\ X-Human [50] & ✓ & \\(20^{*}\\) & N/A & ✗ \\\\ THuman [78] & ✓ & \\(200^{*}\\) & N/A & ✗ \\\\ THuman2.0 [68] & ✓ & \\(526^{*}\\) & N/A & ✗ \\\\ Digital Wardrobe [3] & ✓ & \\(256^{*}\\) & N/A & ✗ \\\\ \\hline iPER [31] & ✗ & ✗ & N/A & ✗ \\\\ People-Snapshot [1] & ✓ & 24 & 1,000\\(\\times\\)1,000 & ✗ \\\\ SelfRecon [23] & ✗ & ✗ & N/A & ✗ \\\\ \\hline SMPLitex [6] & ✗ & 100 & 512\\(\\times\\)512 & ✓ \\\\ SURREAL [56] & ✓ & 921 & 512\\(\\times\\)512 & ✗ \\\\ \\hline\n' +
      '**ATLAS (Ours)** & ✓ & **50k** & **1,024\\(\\times\\)1,024** & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ATLAS와 기존 인간 데이터 세트의 비교. \\ ({}^{*}\\)는 3D 스캔에서 얻을 수 있는 잠재적인 UV 질감을 나타내며, 질감 획득은 설정에 따라 달라지기 때문에 UV 분해능은 N/A로 유지된다.\n' +
      '\n' +
      '2D 인체 분할을 SMPL UV 텍스처와 정렬하기 위한 트랜스포머 기반 네트워크. Zhao _et al._[74]는 부분 기반 분할을 추가하고 교차 뷰 일관성을 강화한다. 유형자[15]는 은닉된 부분을 재구성하기 위해 GAN의 분리 잠재 공간을 도입하지만 불완전한 생성 모델로 인해 종종 불합리한 결과를 생성한다. 확산 모델을 기반으로 SMPLitex[6]은 안정적인 확산 모델을 유도하기 위해 Densepose의 부분 분할을 조건으로 사용한다. 다른 작업 라인[52, 27]은 이 작업을 인페인팅 문제로 간주한다. DensePose 부분 분할 및 텍스처에 기초하여, [27]은 텍스처 맵 및 변위 맵을 완성하기 위해 GAN 기반 네트워크를 사용한다. DINAR[52]은 StyleGAN2를 사용하여 입력 이미지를 신경 텍스처로 변환한다. 2D 이미지 분할에 의존하는 기존의 방법과는 달리, 우리는 잠재 공간에서 인간 이미지와 UV 텍스처 특징을 정렬하기 위해 특징 변환기를 구축한다.\n' +
      '\n' +
      '## 3 ATLAS 데이터세트\n' +
      '\n' +
      '합리적으로 펼쳐진 UV로 대규모 인간 텍스처를 생산하는 것은 훈련을 위해 이러한 데이터를 획득하는 것이 어렵기 때문에 본질적으로 어렵다. 이 섹션에서는 ArTicuLated humAn textureS(ATLAS) 데이터 세트를 제시하고 샘플 텍스처 획득 Sec. 3.1 및 다양한 텍스처 인간 합성 Sec. 3.2를 포함하여 텍스드리머 훈련을 위한 데이터 생성 전략을 설명한다. 그림 2의 ATLAS 비주얼 파이프라인을 참조하라.\n' +
      '\n' +
      '### 샘플 질감 획득\n' +
      '\n' +
      '잘 구조화된 인간 UV 텍스처를 얻는 것은 전통적으로 아티스트에 의한 스캔 데이터 또는 페인트에서 등록해야 한다. 우리는 두 가지를 모두 능가하고 먼저 UV 프로젝션을 사용하여 다중 뷰 이미지에서 거친 인간 UV를 최적화한 다음 프로젝트 페인팅으로 정제할 것을 제안한다. Fig.의 왼쪽을 보세요. 시각적 과정에 대한 2.\n' +
      '\n' +
      '도 2: 합성 데이터 생성을 위한 파이프라인. 왼쪽: 샘플 텍스처 획득. 우리는 먼저 미분 가능한 렌더링을 사용하여 다중 뷰 이미지에서 UV를 최적화한 다음 투영 페인팅으로 UV를 추가로 정제한다. 프롬프트가 있는 획득한 샘플 텍스처는 텍스드리머에서 T2UV를 훈련하는 데 사용된다. 맞아, 다양한 질감의 인간 합성이지 ChatGPT의 도움으로 T2UV를 사용하여 50k 인간 질감을 생성한다. 인간 이미지는 애니메이션 시퀀스, 배경 이미지, HDR 조명 및 투시 카메라로 렌더링된다. 주황색 별은 ATLAS 데이터 세트에 포함된 데이터를 나타낸다.\n' +
      '\n' +
      'UV 투영의 핵심 아이디어는 지상-진실 프레임과 렌더링된 프레임 간의 차이를 최소화하는 것이다. 분할 후, CLIFF[28]을 이용하여 전역 회전, 관절 포즈 및 3D 형상을 추정하고, 마스킹된 프레임으로부터 카메라 파라미터를 추정한다. 초기 UV 맵은 미분 가능한 렌더링을 통해 최적화될 수 있다. 그러나, 추정된 포즈와 실제 포즈 사이에는 편차가 존재한다. 우리는 CGI 생산에 일반적으로 사용되는 텍스처 페인팅 기법인 UV 품질을 개선하기 위해 프로젝트 페인팅을 추가로 수행한다. UV 텍스처 품질은 복수의 뷰 앵글로부터 상이한 SMPL UV 맵을 교번하고 수정함으로써 개선될 수 있다. 획득된 UV 데이터는 TexDreamer T2UV를 훈련하는 데 사용되며, Sec. 4.2의 상세한 훈련 방법을 참조한다.\n' +
      '\n' +
      '샘플 텍스처 다양성을 높이고 T2UV 과적합을 피하기 위해 실제 이미지와 생성된 멀티뷰 이미지를 모두 사용한다. 실제 인간의 질감을 위해, 우리는 People-Snapshot[1] 및 iPER[31]의 비디오를 사용한다. 허구 캐릭터는 원하는 아이덴티티의 멀티뷰 이미지를 생성하기 위해 미리 훈련된 LDM[47]과 함께 ControlNet[72] 및 DWpose[64]에 의존한다. 각 문자(회전각: \\(0,\\pm 45,\\pm 90,\\pm 135,180\\))에 대한 8개의 SMPL A-포즈를 사용하여 LDM의 ID 일관성 문제를 해결한다. 구체적으로, 우리는 긍정적인 것과 부정적인 것 모두를 세대를 제약하기 위해 대응하는 방향 설명을 추가한다. 예를 들어, 뒷면 세대와 관련하여 우리는 "뒷면, 뒷면"을 긍정적인 프롬프트로 사용하고 이에 대응하는 부정적인 프롬프트는 "얼굴, 앞면"이다.\n' +
      '\n' +
      '다양한 질감의 인간 합성\n' +
      '\n' +
      'I2UV 트레이닝을 위한 아이덴티티와 함께 다양한 텍스처 인간 이미지를 합성하기 위해 T2UV 생성 텍스처를 애니메이션, 배경 및 HDR 조명과 합성한다. Fig.의 오른쪽을 보세요. 2는 시각적 파이프라인에 대한 것이다.\n' +
      '\n' +
      '**Texture Generation.** T2UV를 이용하여 UV 데이터셋을 생성하려면 많은 수의 해당 텍스트 설명이 필요하다. 아바타CLIP[18] 분류를 확장하여 상세한 설명, 허구 캐릭터, 유명인 및 일반 설명의 네 가지 범주로 설명을 묘사한다. 각각 설계된 구조를 가지고 있습니다. 상세한 설명을 위해 먼저 인종이나 국가에 대한 무작위 설명이 있는 사람의 모습을 설명하고 성별, 옷, 헤어스타일, 나이가 그 뒤를 잇는다. 허구 캐릭터와 유명인을 위해, 우리는 그 사람의 이름과 그들의 일반적인 옷을 묘사합니다. 연예인들은 허구적인 인물들의 구조에 헤어스타일을 더한다. 일반적인 설명에 있어서, 각각의 프롬프트는 카테고리를 나타내기 위해 하나의 단어 또는 구문을 포함한다. 보충에서 더 빠른 설계를 참조하십시오. ChatGPT[42]를 활용하여 총 50k 프롬프트를 생성했다. 우리는 무작위로 20%의 세대를 ATLAS 테스트 세트로 선택한다.\n' +
      '\n' +
      '**Composite Rendering.** 진정성을 높이기 위해 HDR 이미지 조명과 PBR 휴먼 소재 쉐이더가 있는 블렌더[4]를 이용하여 휴먼 이미지를 합성한다[37]. HDR 조명은 이미지 기반 조명(Image Based Lighting, IBL) 과정[11]에 의해 개발되었으며, 이 과정은 360^{\\circ}\\ 파노라마 영상에서 빛을 샘플링하고 전체 CG 장면을 재조명하기 위해 재사용된다. IBL은 실제 장면을 시뮬레이션하고 균일한 조명을 보장할 수 있으므로 HDR 이미지를 "햇빛"으로 사용하여 조명을 풍부하게 한다. 인체의 경우, 양방향 반사율 분포 함수(BSDF)를 사용하는데, 이는 PBR 쉐이더라고도 알려진 디즈니 원리 모델의 변형이다[37]. 보충에 있는 자세한 설정을 참조하십시오.\n' +
      '\n' +
      '인간 자세의 다양화는 300명 이상의 피험자에 걸쳐 40시간 이상의 모션 데이터를 포함하여 가장 큰 인간 모션 캡처 데이터 세트인 AMASS[36]에 의해 달성된다. 모든 움직임 속도는 렌더 프레임 속도와 동일한 24로 설정되어 830만 개 이상의 렌더링 프레임을 생성한다. 모든 모션 시퀀스는 전역적 변환을 갖는다. 각 사람의 움직임을 보다 완벽하게 포착하기 위해 렌더링 카메라에 제약을 설정한다. 각 투시 카메라는 "골반" 관절의 움직임을 추적하며 80mm 초점 거리로 메쉬 앞 5m에 위치한다. 각 픽셀에 대한 렌더링 샘플은 64로 설정된다.\n' +
      '\n' +
      '배경들을 통합하는 것은 야생에서 실제 인간 이미지들의 더 가까운 유사성에 기여하고 합성 이미지들의 풍부함을 증가시킨다. 이전 합성 데이터 세트 SURREAL은 LSUN[67]의 주방, 거실, 침실 및 식당 범주를 사용한다. LSUN 영상 해상도는 매우 낮다(\\(256\\times 256\\)). 실감성을 높이기 위해 자연 장면, 도시 거리, 실내 설정, 추상적인 질감, 담백한 색상 등 펙셀[13]의 로열티 없는 이미지를 사용한다. 입력된 "알파" 채널과 레이어 다중 렌더링 채널을 계산하기 위해 후처리 기법을 적용하여 텍스처링된 모션 시퀀스를 배경 이미지와 결합할 수 있다.\n' +
      '\n' +
      '##4 Zero-Shot Human Texture Generation\n' +
      '\n' +
      '일률적이고 의미적인 UV 레이아웃으로 대규모의 사실적인 인간 텍스처를 생성하는 것은 효율적인 훈련 데이터를 획득하는 어려움으로 인해 본질적으로 어렵다. 우리는 소수의 샘플 텍스처를 사용하고 사전 훈련된 대규모 T2I 모델의 생성 및 일반화 기능을 활용하여 공통 문자 생성과 해당 UV 구성 요소 간의 연결을 설정하는 것을 목표로 한다. 본 절에서는 첫 번째 제로샷 멀티모달 고충실도 3D 인간 텍스처 생성 방법인 텍스드리머에 대한 상세한 설명을 제공한다. 2단계 학습 전략인 Text-to-UV(T2UV) Sec. 4.2와 Image-to-UV(I2UV) Sec. 4.3을 수행하였고, 먼저 텍스트로부터 텍스쳐를 생성할 수 있도록 텍스쳐 적응 미세 조정을 통해 T2UV 모듈을 학습한다. 그런 다음 ATLAS 데이터 생성에서 합성된 렌더링 이미지와 함께 T2UV를 활용하여 새로운 특징 변환기를 사용하여 I2UV를 학습한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '드림부트[48]는 LDM에서 전체 파라미터를 미세화하고 새로운 체크포인트를 생성한다. 드림부스는 인상적인 결과를 낼 수 있지만 크기 면에서 비용이 든다. 반면, 텍스트 역산[14]은 텍스트 임베딩 공간에서 새로운 "단어"를 통해 제공된 개념을 표현하는 방법을 학습하기 때문에 더 빠르다. 그러나, 그들은 오직 한 명 또는 소수의 과목을 위해 일한다. 위의 두 가지 방법과 달리, 저순위 적응(Low-Rank Adaption; LoRA) [19] 방법은 모델에 새로운 가중치 세트를 추가하며, 이는 범용 제어에 사용될 수 있다. LoRA는 초기에 대규모 언어 모델(LLM)을 미세 조정하기 위해 제안되며, 트랜스포머 교차 주의 계층에 추가 계층을 추가하여 가중치를 학습하고 저순위 행렬을 사용하여 파라미터의 오프셋을 학습한다. 이 기술은 LDM에서도 적용될 수 있다.\n' +
      '\n' +
      '### Text-to-UV\n' +
      '\n' +
      'T2UV 교육을 위해 효율적인 텍스처 적응 미세 조정을 수행합니다. Fig.의 녹색 흐름을 참조한다. T2UV의 시각적 훈련 과정을 위한 도 3. 구체적으로, 각 주의 계층에서 훈련 가능한 몇 가지 매개변수를 추가하고 LoRA 미세 조정을 통해 작은 데이터 세트의 특정 공통 개념을 학습하도록 모델을 훈련한다. 모든 피네튠 방법 4.1 Sec. LoRA는 훈련 효율성과 특정 개념을 생성하기 위해 모델을 미세 조정하는 능력 사이에 좋은 균형을 이룬다. 전형적으로, 조밀한 층들 내의 가중치 매트릭스들은 전체 랭크를 갖는다. LoRA는 가중치에 대한 업데이트가 적응 동안 낮은 "내재 순위"를 갖는다는 것을 보여준다. LDM(W_{\\phi_{unet}}\\in\\mathbb{R}^{d\\times k}\\)의 사전 훈련된 가중치 행렬에 대해, LoRA는 낮은 순위 분해로 후자를 표현함으로써 업데이트를 제약한다. 이 경우, \\(W_{\\phi_{unet}+\\Delta W=W_{\\phi_{unet}+BA\\), 여기서 \\(B\\in\\mathbb{R}^{d\\times r},A\\in\\mathbb{R}^{r\\times k}\\) 및 순위 \\(r\\ll\\min(d,k)\\. 훈련 중 훈련 가능한 매개변수는 \\(A\\) 및 \\(B\\)이다. 이미지-텍스트 입력 잠재 \\(s\\), \\(\\tilde{s}=W_{\\phi_{tenc-unet}s\\)에 대해, 수정된 순방향 패스 수율:\n' +
      '\n' +
      '\\[\\tilde{s}=W_{\\phi_{tenc-unet}}s+\\Delta Ws=W_{\\phi_{tenc-unet}}s+BAs. \\tag{1}\\\n' +
      '\n' +
      'LoRA는 A에서는 랜덤 가우시안 초기화를 사용하고 B에서는 0을 사용하여 \\(W_{\\phi_{tenc-unet}s\\)을 \\(\\frac{\\alpha}{r}\\)으로 스케일링하고, 여기서 \\(\\alpha\\)은 \\(r\\)의 상수이다. 입력된 GT UV 영상\\(x\\)은 SD 영상 인코더\\(\\mathcal{E}\\)로 인코딩되고 입력 텍스트는 \\(\\phi_{t-enc}\\)으로 인코딩된다. LDM의 하이퍼 파라미터들의 수가 많기 때문에, 트레이닝 동안 고정된 보편적인 구성이 없다. Sec. 3.1에서 얻은 샘플 텍스처와 프롬프트 \\(c\\)를 사용하여 T2UV를 훈련한다.\n' +
      '\n' +
      '=\\mathbb{E}_{\\mathcal{E}(x),c,c\\sim\\mathcal{N}(0,1),t}\\left[\\left\\|\\epsilon-\\phi_{unet}\\left(z_{t},t,\\phi_{t-enc}(c\\right)\\right\\|_{2}^{2} \\right], \\tag{2}\\t.\n' +
      '\n' +
      '도 3: 텍스드리머의 구조. 우리는 두 가지 훈련 단계를 수행합니다. T2UV(green)의 경우, 텍스트 인코더와 U-Net을 최적화하기 위해 LDM 데노이즈 손실\\(\\mathcal{L}_{1}\\)을 사용한다. I2UV(blue)의 경우, 특징 변환기는 \\(\\phi_{i2t}\\)에 의해 인코딩된 입력 이미지 특징을 조건부 특징 \\(f_{i2t}\\)에 매핑한다. 우리는 \\(\\phi_{t-enc}\\)와 \\(\\phi_{i-enc}\\)을 \\(\\mathcal{L}_{2}\\)으로 최적화하여 I2UV를 훈련한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '이어서, 다음과 같이 수식화된다:\n' +
      '\n' +
      '\\phi_{i2t}:=\\phi_{i-dec}\\left(\\phi_{MLP}\\left(f_{voken}\\right),q\\right)\\in\\mathbff{R}^{L\\times\\hat{d}, \\tag{3}\\}}\n' +
      '\n' +
      '여기서 \\(L\\)은 텍스트 인코더 \\(\\phi_{t-enc}\\)의 최대 입력 길이이고 \\(\\hat{d}\\)은 LDM 인코더 \\(\\mathcal{E}\\) 출력 특징의 차원이다. 우리의 경우, \\(f_{i2t}\\in\\mathbb{R}^{77\\times 1,024}\\)이다.\n' +
      '\n' +
      '입력 영상에 T2UV 생성을 제약하기 위해, 매핑된 특징 \\(f_{i2t}\\)은 생성 과정에서 조건으로 기능한다. 학습 시 생성된 UV 텍스처는 LDM 영상 인코더를 통해 먼저 잠재 특징 \\(z_{0}\\)으로 인코딩된다. 잡음 특성 \\(z_{t}\\)은 \\(z_{0}\\)에 잡음 \\(\\epsilon\\)을 더하여 구한다. \\(f_{i2t}\\)는 본질적으로 텍스트 특징이며, \\(\\phi_{t-enc}(c)\\)와 유사하며, 훈련에 직접 사용될 수 있으며, LDM 잡음 손실로 이미지 인코더 \\(\\phi_{i-enc}\\) 및 특징 변환기 \\(\\phi_{i2t}\\)을 최적화한다:\n' +
      '\n' +
      '\\phi_{i-enc}(y)),\\\\&L_{2}:=\\mathbb{E}_{\\mathcal{E}(x),y,\\epsilon\\sim\\mathcal{N}(0,1),t}\\left[\\left\\|\\epsilon-\\phi_{unet}\\left(z_{t},t,f_{i2t}\\right}\\right\\|_{2}^{2}\\right].\\end{split}\\tag{4}\\t}\\t.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '구현 상세.** T2UV를 훈련하기 위해 안정확산-2-1과 clip-vit-large-patch14-336을 사용하였으며, 각 훈련은 회분식 8을 사용하며, 총 2,000개의 훈련단계를 가진다. 최적화기는 AdamW이며, 학습률이 0.0001로 설정되고 100개의 워밍업 단계가 있는 일정한 스케줄러이다. 훈련 효율을 높이기 위해 [17]을 따르고 SNR-\\(\\gamma\\)을 5로 설정하였으며, 추론 시 T2UV의 가중치를 1.0으로 설정하고 32단계의 결과를 사용하였다. 학습용 I2UV의 경우 T2UV를 기반으로 동일한 배치 크기를 사용하지만 학습 단계를 20,000으로 추가하고 학습률을 \\(1e-5\\)으로 변경하고 정규화 시 가중치 감소를 0.01로 설정하며 모든 학습은 단일 Nvidia A100 GPU에서 수행된다.\n' +
      '\n' +
      '**평가 메트릭** T2UV의 경우 CLIP 점수[44]를 사용하여 생성된 질감과 입력 텍스트 간의 일관성을 측정한다. 모든 계산에 대해 동일한 투시 카메라, 조명 및 재료를 사용하여 Pytorch3D를 사용하여 T-포즈에서 생성된 질감을 SMPL 중립체로 렌더링한다. I2UV의 경우, 이전 방법들[57, 62, 63, 74]은 주로 렌더링들과 지상-진실 이미지들 사이에서 SSIM[59] 및 LPIPS[73]을 사용한다. 그러나, 인간의 포즈 추정의 정확도에 영향을 받아, 이러한 메트릭들은 재구성된 텍스처 품질을 완전히 측정할 수 없다. 우리는 텍스쳐 그라운드 트루스와 그에 대응하는 텍스트를 가지고 있기 때문에, 텍스쳐 품질과 텍스트 일관성을 평가하기 위해 MSE(Mean Squared Error)와 CLIP 점수를 사용할 것을 제안한다.\n' +
      '\n' +
      '### Qualitative Comparison\n' +
      '\n' +
      '**T2UV.** Text2Tex[9], TEXTure[46], Latent-Paint[38], Fantasia3D[10], SMPLitex[6]를 포함하여 TexDreamer T2UV를 최첨단 텍스쳐 생성 방법과 비교한다. 도 1에 도시된 바와 같다. 5, 우리 세대는 최고의 전반적인 품질과 최고의 얼굴 디테일을 달성합니다. 인간 중심의 최적화 방법은 시간이 오래 걸리므로 탭(Tab)을 참조하십시오. 2, 우리는 비교할 첫 번째 [18]과 가장 최근의 진보된 [24] 오픈 소스 방법을 선택한다. 둘 다 안면 부위에 추가 최적화 단계가 있습니다. 우리는 그림의 왼쪽에 있는 그들의 쇼케이스에서 신분을 사용한다. 도 6은 질감 색상과 얼굴 특징에 대한 사실성이 결여된 결과를 보여준다. 보충에서 텍스드리머의 더 많은 생성 결과를 참조하십시오.\n' +
      '\n' +
      '**I2UV.** 선도 방법 텍스포머 [63] 및 SMPLitex [6]과 이미지-to-uv를 비교한다. 우리는 텍스포머 훈련 데이터세트 Market-1501[76]과 ATLAS 테스트 세트 모두에서 평가한다. 그림 7의 시각적 비교를 참조하십시오. 우리의 방법은 현저하게 충실한 정체성과 뛰어난 질감 사실성을 달성합니다.\n' +
      '\n' +
      '### Quantitative Comparison\n' +
      '\n' +
      '**T2UV.** 텍스트로부터 텍스처링된 3D 인간의 렌더링은 참조 뷰에서 입력 텍스트와 밀접하게 유사해야 하며, 새로운 뷰 하에서 참조와 일관된 의미론을 입증해야 한다. CLIP를 사용하여 이 두 가지 측면을 평가합니다.\n' +
      '\n' +
      '그림 5: 텍스트로부터의 질감 생성의 질적 비교. TexDreamer를 Text2Tex[9], TEXTure[46], Latent-Paint[38], Fantasia3D[10] 등 최신 텍스쳐 생성 방법과 비교하였다. 우리의 결과는 최고의 얼굴 디테일과 최고의 전반적인 품질을 분명히 달성합니다. 더 나은 보기를 보려면 확대하십시오.\n' +
      '\n' +
      '그림 6: 왼쪽: AvatarCLIP [18] 및 AvatarCraft [24]와의 질적 비교. 우리의 방법은 더 현실적인 헤드 아바타를 가지고 있다. 예: 텍스처 편집입니다. 텍스드리머는 텍스트를 사용하여 생성된 질감 세부 정보, 예를 들어__, 의류 스타일, 색상 및 액세서리를 편집할 수 있다.\n' +
      '\n' +
      '점수[44]는 신규 뷰와 참조 사이의 의미론적 유사성을 계산한다. T2UV 텍스트 일관성을 보다 종합적으로 평가하기 위해 Sec. 5.1의 SMPL의 기본 렌더링 외에도 세 개의 뷰(azim = 0, 90, 180, 270)를 추가한다. 시간과 자원의 소모가 많기 때문에, 우리는 아바타 크래프트와 효율성만을 비교한다. 탭에 표시된 대로입니다. 둘째, 본 논문에서 제안한 방법이 가장 효율적이며, 텍스트 일관성이 가장 우수하다.\n' +
      '\n' +
      '**I2UV.** 지상 진리로 생성 및 수집된 텍스처를 사용하여 먼저 MSE를 사용하여 I2UV와 고급 이미지 대 UV 방법 텍스포머 [63] 및 스플라이트텍스 [6]을 비교한다. MSE는 예측된 값과 실제 값 사이의 제곱의 평균을 계산한다. MSE가 작을수록 모델의 질감 생성 품질이 높아진다. ATLAS 테스트 세트에서 무작위로 두 프레임을 입력으로 추출한다. 또한, 페어드 렌더링과 텍스트 기술을 사용하여 CLIP 점수를 사용하여 각 텍스쳐의 텍스트 일관성을 평가했다. 탭 도 4는 TexDreamer I2UV가 두 측정 모두에서 최상의 결과를 달성함을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & GPU (GiB) & Time (mins) \\(\\downarrow\\) CLIP Score \\(\\uparrow\\) \\\\ \\hline Text2Tex [9] & 20.31 & \\(\\sim\\) 14.35 & 29.962 \\\\ TEXTure [46] & 12.05 & \\(\\sim\\) 2.38 & 27.298 \\\\ Latent-Paint [38] & 11.46 & \\(\\sim\\) 13.95 & 26.378 \\\\ Fantasia3d [10] & 12.42 & \\(\\sim\\) 14.50 & 30.557 \\\\ SMPLitex [6] & 7.77 & \\(\\sim\\) 0.31 & 22.998 \\\\ \\hline AvatarCILP [18] & 37.74 & \\(\\sim\\) 360 & 29.422 \\\\ AvatarCraft\\({}^{*}\\)[24] & 26.65 & \\(\\sim\\) 480 & - \\\\ \\hline\n' +
      '**Ours-T2UV** & **5.71** & \\(\\sim\\)**0.17** & **31.297** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 텍스트로부터 인간 텍스처를 생성하는 정량적 비교. "-T"는 TexDreamer T2UV를 의미하며, 우리는 추론 방법(위)과 최적화 방법(중간) 결과를 비교한다. 아바타크래프트의 CLIP 점수는 시간과 자원의 소비가 높기 때문에 보고되지 않는다.\n' +
      '\n' +
      '그림 7: 이미지로부터 UV 생성과의 질적 비교. 우리는 ATLAS 데이터셋(왼쪽)과 Market-1501[76](오른쪽)에서 고급 텍스포머[63] 및 SMPLitex[6]와 비교한다. 질감 완성도와 품질을 비교하려면 확대하십시오.\n' +
      '\n' +
      '사용자 연구.우리는 텍스트를 사용하여 텍스처링 3D 인간을 평가하기 위해 사용자 연구를 추가로 수행한다. 탭 도 3은 본 방법의 선호도가 가장 높고 해당 텍스트와 가장 유사함을 나타낸다. 동일한 렌더링 뷰 이미지를 사용하고 14명의 참가자를 초대하여 "전체 텍스처의 품질" 및 "텍스트 설명과의 일관성"에 대해 1-5의 척도로 평가한다. 각 참가자는 동일한 양의 비교로 무작위로 할당된다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '텍스트로부터 학습 텍스처 생성을 위해 각 주의 계층에서 학습 가능한 파라미터를 몇 개 추가하고, 랭크의 약간의 변화 및 LoRA의 \\(\\alpha\\)가 생성 결과에 큰 영향을 미칠 수 있다. U-Net과 텍스트 인코더의 \\(r\\)과 \\(\\alpha\\)에 대한 절제 실험을 수행하였다. 절제 목적으로, 우리는 Sec. 5.1 설정에서 훈련 단계를 줄이고, 다른 것들은 그대로 유지한다. 탭 도 5는 우리의 T2UV가 가장 높은 텍스트 일관성을 가지고 있음을 보여준다. I2UV의 경우 ATLAS 테스트 세트에서 고정 텍스트 인코더와 전체 I2UV 모듈을 비교하며, Tab. 4. 전체 모델은 이미지 유사도와 텍스트 일관성이 가장 높다.\n' +
      '\n' +
      '## 6 Applications\n' +
      '\n' +
      'Texture Editing.TexDreamer는 3D Human appearance generation, details to clothing style(상하의복 모두), accessories, _etc._ 그림 6을 참조하십시오. 이를 통해 아티스트가 디자인한 캐릭터의 빠르고 정확한 변경이 가능하여 영화 또는 게임 산업에서 컨셉 캐릭터 디자인에 더 유연하고 적응할 수 있습니다. 시각적 결과는 또한 우리의\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & MSE \\(\\downarrow\\) CLIP Score \\(\\uparrow\\) \\\\ \\hline Texformer [63] & 0.1148 & 21.811 \\\\ SMPLitex [6] & 0.0783 & 22.488 \\\\ Ours-I2UV (fixed \\(\\phi_{i-enc}\\)) & 0.0632 & 26.138 \\\\\n' +
      '**Ours-I2UV (full)** & **0.0442** & **27.334** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline \\(\\phi_{unet}r\\) & \\(\\phi_{unet}\\alpha\\) & \\(\\phi_{t-enc}r\\) & \\(\\phi_{t-enc}\\alpha\\) CLIP Score \\(\\uparrow\\) \\\\ \\hline\n' +
      '128 & 128 & 8 & 8\\\\\n' +
      '**128** & **128** & **16** & **29**\n' +
      '128 & 128 & 32 & 32 & 28.36 \\\\ \\hline\n' +
      '64 & 64 & 16 & 28.20 \\\\\n' +
      '192 & 192 & 16 & 16 & 29.19 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: TexDreamer I2UV의 정량 비교 및 절제 연구. “fixed \\(\\phi_{i-enc}\\)”는 I2UV에서 이미지 인코더를 훈련시키지 않는다는 것을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Text Consistency \\(\\uparrow\\) Image Quality \\(\\uparrow\\) \\\\ \\hline Text2Tex [9] & 1.919 & 1.641 \\\\ TEXTure [46] & 2.003 & 1.744 \\\\ Latent-Paint [38] & 1.878 & 1.456 \\\\ Fantasia3D [10] & 2.089 & 1.904 \\\\ AvatarCLIP [18] & 1.752 & 1.341 \\\\\n' +
      '**TexDreamer (Ours)** & **4.019** & **4.244** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 텍스트로부터 텍스처 생성에 대한 사용자 연구. 우리의 결과는 가장 높은 화질과 테스트 일관성을 가지고 있다.\n' +
      '\n' +
      '방법은 신분을 보존하면서 다른 옷으로 인간의 질감을 생성할 수 있으며, 이는 또한 빠른 가상 트라이온을 위한 문을 열어준다.\n' +
      '\n' +
      '**텍스처링 드레싱 아바타.** 더 복잡한 기하학을 위해 생성된 텍스처의 적용을 추가로 조사했다. 도 1에 도시된 바와 같다. 도 8을 참조하면, 생성된 텍스처는 복잡한 인간 메쉬와 통합될 수 있고 보다 진정한 인간 유사 캐릭터를 생성할 수 있다. 구체적으로, 우리는 가장 진보된 텍스트-투-3D-아바타 생성 방법 TADA를 활용한다[29]. 메쉬 초기화 과정을 수정하여 텍스드리머의 합성 텍스처를 적용하여 원래 UV 정보를 보존하면서 메쉬를 밀도화할 수 있다. 이 응용 프로그램은 텍스드리머가 사용자에게 개인화된 캐릭터를 쉽게 생성할 수 있는 권한을 부여하며, 이는 전통적인 3D 모델링 기술에 어려움을 줄 수 있음을 보여준다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '본 논문에서는 제로샷 멀티모달 고충실도 3차원 인간 텍스처 생성 모델인 TexDreamer를 제안한다. 효율적인 텍스처 적응 미세 조정과 새로운 특징 변환기로 독특한 UV 구조에 큰 T2I 모델 생성 능력을 적응시킨 텍스드리머는 텍스트나 이미지를 사용하여 3D 인간의 텍스처링을 위한 충실한 아이덴티티와 의류를 나타낸다. 보다 다양한 인간 유사 아바타 생성을 가능하게 한다. 또한, 가장 광범위한 고해상도(1,024\\(\\times\\)1,024) 3차원 인간 텍스처 데이터 세트인 ATLAS를 균일하고 의미적인 UV 레이아웃으로 구성하여 고품질 인간 UV 데이터의 부재를 채운다. 광범위한 실험을 통해 제안된 방법이 텍스트 일관성 및 UV 품질 측면에서 기존 접근법을 능가한다는 것을 입증한다.\n' +
      '\n' +
      '**제한 및 사회적 영향** 텍스드리머는 유망한 결과를 보여주지만 여전히 몇 가지 한계를 가지고 있다. I2UV는 덴스포즈 세분화를 기반으로 하지 않으며, 실제 사례에 적용할 경우 일부 출력이 입력 의류 패턴과 엄격하게 정렬되지 않을 수 있다. 사실적인 인간 질감을 생산하는 텍스포머는 가상 인간 산업에 영향을 미칠 가능성이 있다. 그러나 이 기술은 잠재적으로 딥페이크를 만드는 데 사용될 수 있기 때문에 윤리적 및 개인 정보 보호 문제도 제기한다.\n' +
      '\n' +
      '그림 8: 옷을 입은 아바타를 텍스쳐링합니다. 본 논문에서 제안한 텍스쳐는 텍스트-투-3d 방법에 의해 생성된 복잡한 복장 메쉬에 적용될 수 있다. TexDreamer에 의해 생성된 합성 UV 텍스처와 함께 TADA[29]에 의해 생성된 몇 가지 예를 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Video based reconstruction of 3d people models. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8387-8397 (2018)\n' +
      '* [2] AXYZ: 4D Scanned People Character Animation Software. [https://secure.axyz-design.com/](https://secure.axyz-design.com/) (2023)\n' +
      '* [3] Bhatnagar, B.L., Tiwari, G., Theobalt, C., Pons-Moll, G.: Multi-garment net: Learning to dress 3d people from images. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 5420-5430 (2019)\n' +
      '* 3D 모델링 및 렌더링 패키지: [https://www.blender.org/](https://www.blender.org/)(2023)\n' +
      '* [5] Cao, Y., Cao, Y.P., Han, K., Shan, Y., Wong, K.Y.K.: Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. arXiv preprint arXiv:2304.00916 (2023)\n' +
      '* [6] Casas, D., Trinidad, M.C.: Smplictex: A generative model and dataset for 3d human texture estimation from single image. arXiv preprint arXiv:2309.01855 (2023)\n' +
      '* [7] Cha, S., Seo, K., Ashtari, A., Noh, J.: Generating texture for 3d human avatar from a single image using sampling and refinement networks. In: Computer Graphics Forum. vol. 42, pp. 385-396. Wiley Online Library (2023)\n' +
      '* [8] Chang, S., Cho, J., Oh, S.: Texture generation using dual-domain feature flow with multi-view hallucinations. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 203-211 (2022)\n' +
      '* [9] Chen, D.Z., Siddiqui, Y., Lee, H.Y., Tulyakov, S., Niessner, M.: Text2tex: Text-driven texture synthesis via diffusion models. arXiv preprint arXiv:2303.11396 (2023)\n' +
      '* [10] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023)\n' +
      '* [11] Debevec, P.: Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. In: Acm siggraph 2008 classes, pp. 1-10 (2008)\n' +
      '* [12] Feng, Y., Yang, J., Pollefeys, M., Black, M.J., Bolkart, T.: Capturing and animation of body and clothing from monocular video. In: SIGGRAPH Asia 2022 Conference Papers. pp. 1-9 (2022)\n' +
      '* [13] Free Stock Photos, Royalty Free Stock Images amd Copyright Free Pictures: Pexels, [https://www.pexels.com/](https://www.pexels.com/)\n' +
      '* [14] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022)\n' +
      '* [15] Grigorev, A., Iskakov, K., Iainina, A., Bashirov, R., Zakharkin, I., Vakhitov, A., Lempitsky, V.: Stylepeople: A generative model of fullbody human avatars. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5151-5160 (2021)\n' +
      '* [16] Guler, R.A., Neverova, N., Kokkinos, I.: Densepose: Dense human pose estimation in the wild. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7297-7306 (2018)\n' +
      '* [17] Hang, T., Gu, S., Li, C., Bao, J., Chen, D., Hu, H., Geng, X., Guo, B.: Efficient diffusion training via min-snr weighting strategy. arXiv preprint arXiv:2303.09556 (2023)* [18] Hong, F., Zhang, M., Pan, L., Cai, Z., Yang, L., Liu, Z.: Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535 (2022)\n' +
      '* [19] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n' +
      '* [20] Huang, Y., Yi, H., Xiu, Y., Liao, T., Tang, J., Cai, D., Thies, J.: TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. In: International Conference on 3D Vision (3DV) (2024)\n' +
      '* [21] Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1125-1134 (2017)\n' +
      '* [22] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 867-876 (2022)\n' +
      '* [23] Jiang, B., Hong, Y., Bao, H., Zhang, J.: Selfrecon: Self reconstruction your digital avatar from monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5605-5615 (2022)\n' +
      '* [24] Jiang, R., Wang, C., Zhang, J., Chai, M., He, M., Chen, D., Liao, J.: Avatar-craft: Transforming text into neural human avatars with parameterized shape and pose control. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 14325-14336 (2023), [https://api.semanticscholar.org/CorpusID:257834153](https://api.semanticscholar.org/CorpusID:257834153)\n' +
      '* [25] Jiang, Y., Yang, S., Qiu, H., Wu, W., Loy, C.C., Liu, Z.: Text2human: Text-driven controllable human image generation. ACM Transactions on Graphics (TOG) **41**(4), 1-11 (2022)\n' +
      '* [26] Kolotouros, N., Alldieck, T., Zanfir, A., Bazavan, E.G., Fieraru, M., Sminchisescu, C.: Dreamhuman: Animatable 3d avatars from text. arXiv preprint arXiv:2306.09329 (2023)\n' +
      '* [27] Lazova, V., Insafutdinov, E., Pons-Moll, G.: 360-degree textures of people in clothing from a single image. In: 2019 International Conference on 3D Vision (3DV). pp. 643-653. IEEE (2019)\n' +
      '* [28] Li, Z., Liu, J., Zhang, Z., Xu, S., Yan, Y.: Cliff: Carrying location information in full frames into human pose and shape estimation. In: European Conference on Computer Vision. pp. 590-606. Springer (2022)\n' +
      '* [29] Liao, T., Yi, H., Xiu, Y., Tang, J., Huang, Y., Thies, J., Black, M.J.: TADA! Text to Animatable Digital Avatars. In: International Conference on 3D Vision (3DV) (2024)\n' +
      '* [30] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [31] Liu, W., Piao, Z., Min, J., Luo, W., Ma, L., Gao, S.: Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5904-5913 (2019)\n' +
      '* [32] Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X.: Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1096-1104 (2016)33] Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: A skinned multi-person linear model. In: Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pp. 851-866 (2023)\n' +
      '* [34] Lorensen, W.E., Cline, H.E.: Marching cubes: A high resolution 3d surface construction algorithm. In: Seminal graphics: pioneering efforts that shaped the field, pp. 347-353 (1998)\n' +
      '* [35] Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black, M.J.: Learning to dress 3d people in generative clothing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6469-6478 (2020)\n' +
      '* [36] Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 5442-5451 (2019)\n' +
      '* [37] McAuley, S., Hill, S., Hoffman, N., Gotanda, Y., Smits, B., Burley, B., Martinez, A.: Practical physically-based shading in film and game production. In: ACM SIGGRAPH 2012 Courses, pp. 1-7 (2012)\n' +
      '* [38] Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf for shape-guided generation of 3d shapes and textures. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12663-12673 (2023)\n' +
      '* [39] Michel, O., Bar-On, R., Liu, R., Benaim, S., Hanocka, R.: Text2mesh: Text-driven neural stylization for meshes. In: CVPR (2022)\n' +
      '* [40] Mohammad Khalid, N., Xie, T., Bellilovsky, E., Popa, T.: Clip-mesh: Generating textured meshes from text using pretrained image-text models. In: SIGGRAPH Asia 2022 conference papers. pp. 1-8 (2022)\n' +
      '* [41] Muller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG) **41**(4), 1-15 (2022)\n' +
      '* [42] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems **35**, 27730-27744 (2022)\n' +
      '* [43] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [44] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [45] Renderpeople: Over 4,000 Scanned 3D People Models. [https://renderpeople.com/](https://renderpeople.com/) (2023)\n' +
      '* [46] Richardson, E., Metzer, G., Alaluf, Y., Giryes, R., Cohen-Or, D.: Texture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023)\n' +
      '* [47] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [48] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22500-22510 (2023)* [49] Saito, S., Simon, T., Saragih, J., Joo, H.: Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 84-93 (2020) 4\n' +
      '* [50] Shen, K., Guo, C., Kaufmann, M., Zarate, J.J., Valentin, J., Song, J., Hilliges, O.: X-avatar: Expressive human avatars. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16911-16921 (2023) 4\n' +
      '* [51] Su, Z., Yu, T., Wang, Y., Liu, Y.: Deepcloth: Neural garment representation for shape and style editing. IEEE Transactions on Pattern Analysis and Machine Intelligence **45**(2), 1581-1593 (2022) 3\n' +
      '* [52] Svitov, D., Gudkov, D., Bashirov, R., Lempitsky, V.: Dinar: Diffusion inpainting of neural textures for one-shot human avatars. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7062-7072 (2023) 5\n' +
      '* [53] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior pp. 22819-22829 (October 2023) 4\n' +
      '* [54] treedy\'s: 3D body scanning technology. [https://www.treedys.com/](https://www.treedys.com/) (2023) 4\n' +
      '* [55] Twindom: Full Body 3D Scanners for 3D Printed Figurines, 3D Portraits and 3D Selfies. [https://web.twindom.com/](https://web.twindom.com/) (2023) 4\n' +
      '* [56] Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M.J., Laptev, I., Schmid, C.: Learning from synthetic humans. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 109-117 (2017) 4\n' +
      '* [57] Wang, J., Zhong, Y., Li, Y., Zhang, C., Wei, Y.: Re-identification supervised texture generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11846-11856 (2019) 10\n' +
      '* [58] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021) 4\n' +
      '* [59] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing **13**(4), 600-612 (2004) 10\n' +
      '* [60] Xiu, Y., Yang, J., Cao, X., Tzionas, D., Black, M.J.: Econ: Explicit clothed humans optimized via normal integration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 512-523 (2023) 4\n' +
      '* [61] Xu, C., Zhu, J., Zhang, J., Han, Y., Chu, W., Tai, Y., Wang, C., Xie, Z., Liu, Y.: High-fidelity generalized emotional talking face generation with multi-modal emotion space learning. In: CVPR (2023) 3\n' +
      '* [62] Xu, X., Chen, H., Moreno-Noguer, F., Jeni, L.A., De la Torre, F.: 3d human pose, shape and texture from low-resolution images and videos. IEEE transactions on pattern analysis and machine intelligence **44**(9), 4490-4504 (2021) 10\n' +
      '* [63] Xu, X., Loy, C.C.: 3d human texture estimation from a single image with transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 13849-13858 (2021) 4, 10, 11, 12, 13\n' +
      '* [64] Yang, Z., Zeng, A., Yuan, C., Li, Y.: Effective whole-body pose estimation with two-stages distillation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4210-4220 (2023) 6\n' +
      '* [65] Yang, Z., Cai, Z., Mei, H., Liu, S., Chen, Z., Xiao, W., Wei, Y., Qing, Z., Wei, C., Dai, B., Wu, W., Qian, C., Lin, D., Liu, Z., Yang, L.: Synbody: Synthetic dataset with layered human models for 3d human perception and modeling (2023) 3* [66] Youwang, K., Ji-Yeon, K., Oh, T.H.: Clip-actor: Text-driven recommendation and stylization for animating human meshes. In: European Conference on Computer Vision. pp. 173-191. Springer (2022)\n' +
      '* [67] Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., Xiao, J.: Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 (2015)\n' +
      '* [68] Yu, T., Zheng, Z., Guo, K., Liu, P., Dai, Q., Liu, Y.: Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5746-5756 (2021)\n' +
      '* [69] Zeng, Y., Lu, Y., Ji, X., Yao, Y., Zhu, H., Cao, X.: Avatarbooth: High-quality and customizable 3d human avatar generation. arXiv preprint arXiv:2306.09864 (2023)\n' +
      '* [70] Zhang, C., Pujades, S., Black, M.J., Pons-Moll, G.: Detailed, accurate, human shape estimation from clothed 3d scan sequences. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4191-4200 (2017)\n' +
      '* [71] Zhang, J., Zeng, X., Wang, M., Pan, Y., Liu, L., Liu, Y., Ding, Y., Fan, C.: Freenet: Multi-identity face reenactment. In: CVPR (2020)\n' +
      '* [72] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [73] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 586-595 (2018)\n' +
      '* [74] Zhao, F., Liao, S., Zhang, K., Shao, L.: Human parsing based texture transfer from single image to 3d human via cross-view consistency. Advances in Neural Information Processing Systems **33**, 14326-14337 (2020)\n' +
      '* [75] Zheng, K., He, X., Wang, X.E.: Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239 (2023)\n' +
      '* [76] Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-identification: A benchmark. In: Proceedings of the IEEE international conference on computer vision. pp. 1116-1124 (2015)\n' +
      '* [77] Zheng, Z., Yu, T., Liu, Y., Dai, Q.: Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. IEEE transactions on pattern analysis and machine intelligence **44**(6), 3170-3184 (2021)\n' +
      '* [78] Zheng, Z., Yu, T., Wei, Y., Dai, Q., Liu, Y.: Deephuman: 3d human reconstruction from a single image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7739-7749 (2019)\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '이 부록에서 부록 0.A는 ATLAS 데이터 세트 구성에 관한 더 많은 정보를 제공한다. 부록 0.B는 텍스드리머 훈련에 대한 더 많은 분석을 제시한다. 부록 0.C는 텍스트 대 UV 및 이미지 대 UV를 모두 포함하여 텍스드리머의 더 정성적 결과를 보여준다. 또한 ATLAS 데이터 세트에 포함된 더 많은 인간 질감을 제시한다.\n' +
      '\n' +
      '## 부록 0.ATLAS 건설에 관한 자세한 내용\n' +
      '\n' +
      '**텍스트 증강** 허구 캐릭터에 대한 멀티뷰 이미지를 획득하기 위해, 텍스트 증강을 사용하여 생성된 캐릭터 아이덴티티의 일관성을 촉진한다. 탭 도 6은 뷰 관련 및 다른 설명 프롬프트에 대한 세부사항을 제공한다. 양성 프롬프트 \\(T_{pos}\\)를 사용하여 주 생성 방향을 조정한다. 우리는 캐릭터 아이덴티티를 갖는 \\(T_{pos}\\)을 설명하고, 포즈 \\(T_{pose}\\) 및 다른 설명 \\(T_{other}\\)을 생성한다.\n' +
      '\n' +
      '**ChatGPT 프롬프트 구조** 아바타 설명을 세부 설명, 허구 캐릭터, 유명인, 일반 설명 등 네 가지 범주로 나눕니다. 각 범주에 대해 고유한 생성 템플릿을 설계합니다. 그림 1의 각 범주에 대해. 도 9에 도시된 바와 같이, "[ ]" 컨텐츠는 모든 프롬프트와 함께 포함되며, "( ))"는 컨텐츠가 랜덤하게 나타나는 것을 나타낸다.\n' +
      '\n' +
      '**재료.** 질감 있는 사람을 렌더링하기 위한 더 많은 재료 설정을 제공합니다. 정통적인 인간-유사 재료를 달성하기 위해, "유전체 정반사"를 0.1로 설정하고, "거칠기"를 0.6으로 증가시킨다. 또한, "Sheen Tint"는 0.5, "Clearcoat Roughness"는 0.03, "투과를 위한 굴절 지수"(IOR)는 1.45이고, "Alpha" 채널은 1로 유지된다.\n' +
      '\n' +
      '## 부록 0.B TexDreamer 추가 분석\n' +
      '\n' +
      '**트레이닝 샘플 사이즈.** 트레이닝 샘플 사이즈가 다르면 모델 생성 능력에 다른 영향을 미칠 수 있다. FID 점수를 사용하여 다양한 훈련 크기를 평가한다. FID 점수는 생성된 이미지의 분포와 실제 이미지의 분포 사이의 유사성을 측정한다. 낮은 FID 값 평균\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline \\hline  & \\multicolumn{2}{c|}{\\(T_{pos}\\)} & \\(T_{neg}\\) \\\\ \\hline \\multirow{3}{*}{view} & front & front side, from front, the & \\multirow{3}{*}{backside} \\\\  & & front of \\(T_{id}\\) & \\\\ \\cline{1-1} \\cline{3-3}  & back & backside, from back, the & & front, face, head \\\\ \\cline{1-1}  & back & backside of \\(T_{id}\\) & & front, back \\\\ \\cline{1-1} \\cline{2-3}  & left & left side, from left & & front, back \\\\ \\cline{1-1} \\cline{2-3}  & right & right side, from right & & front, back \\\\ \\hline \\multirow{3}{*}{other} & black background, diffuse rendering, & \\multirow{3}{*}{overexposed, nude, layman work, worst quality, teeth, smile, open mouth, eyes closed} \\\\ \\cline{1-1}  & & daylight & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: ATLAS 샘플 텍스처 이미지 생성에 사용한 프롬프트 개요.\n' +
      '\n' +
      '더 나은 화질과 다양성. 텍스트, T2UV에서 인간 질감 생성에 대해 샘플 크기 범위는 10에서 300까지 20 간격으로 실험하며 결과는 그림 1에 나와 있다. 10. 각 테스트 세트와 트레인 세트 사이의 FID 점수는 100개 이상의 트레이닝 샘플에 걸쳐 점진적으로 일관되기 시작하는 경향이 있음을 발견했다. 이는 모델이 주어진 학습 데이터로 포화 상태에 도달했음을 나타낸다. 즉, 모델은 약 100개의 텍스처를 사용하여 UV 구조를 학습했으며, 더 많은 트레이닝 샘플을 추가하는 것은 모델의 새롭고 다양하며 고품질의 텍스처를 생성하는 능력을 향상시키는 데 계속 기여하지 않을 것이다.\n' +
      '\n' +
      '## 부록 0.C 보다 질적인 결과\n' +
      '\n' +
      '우리는 텍스드리머와 ATLAS의 보다 질적인 결과를 보여준다. 텍스트에서 질감을 생성하기 위해 그림 1에서 사실적인 인간과 허구적 문자를 모두 포함하는 더 많은 결과를 보여준다. 도 11을 참조하면, 도 12 및 도 12를 참조하여 설명한다. 13. Meshes는 텍스트-투-아바타 방법 TADA로 생성되며, Mixamo로 가상의 캐릭터를 애니메이션화한다. 도. 도 14는 또한 이미지들로부터 생성된 텍스처들로 더 많은 결과들을 도시한다. 또한 ATLAS 데이터 세트에 포함된 더 많은 인간 질감을 표시한다. 도 15를 참조하면, 도 16 및 도 6을 참조하여 설명한다. 17.\n' +
      '\n' +
      '도 10: 트레이닝 샘플 크기와 텍스처 FID 점수 사이의 상관 관계.\n' +
      '\n' +
      '도 9: ATLAS 구축을 위한 프롬프트 구조.\n' +
      '\n' +
      '도 11: 텍스드리머가 있는 텍스트로부터 생성된 사실적인 인간 텍스처들, 각각은 동일한 메시로 렌더링된다.\n' +
      '\n' +
      '도 12: 텍스드리머가 있는 텍스트로부터 생성된 사실적인 인간 텍스처들, 각각은 동일한 메시로 렌더링된다.\n' +
      '\n' +
      '도 13: 텍스드리머가 있는 텍스트로부터 생성된 픽션 캐릭터 텍스처.\n' +
      '\n' +
      '도 14: 텍스드리머가 있는 이미지들로부터 생성된 텍스처.\n' +
      '\n' +
      '그림 15: ALTAS 데이터 세트의 인간 텍스처, 예제 세트 1.\n' +
      '\n' +
      '그림 16: ALTAS 데이터 세트의 인간 텍스처, 예제 세트 2.\n' +
      '\n' +
      '그림 17: ALTAS 데이터 세트의 인간 텍스처, 예제 세트 3.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>