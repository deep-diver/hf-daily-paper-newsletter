<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Rethinking Interpretability in the Era of Large Language Models\n' +
      '\n' +
      'Chandan Singh\n' +
      '\n' +
      'Jeevana Priya Inala\n' +
      '\n' +
      'Michel Galley\n' +
      '\n' +
      'Rich Caruana\n' +
      '\n' +
      'Jianfeng Gao\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.\n' +
      '\n' +
      'In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.\n' +
      '\n' +
      'Machine Learning, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Machine learning (ML) and natural language processing (NLP) have seen a rapid expansion in recent years, due to the availability of increasingly large datasets and powerful neural network models. In response, the field of interpretable ML1 has grown to incorporate a diverse array of techniques and methods for understanding these models and datasets[1, 2, 3]. One part of this expansion has focused on the development and use of inherently interpretable models[4], such as sparse linear models, generalized additive models, and decision trees. Alongside these models, post-hoc interpretability techniques have become increasingly prominent, offering insights into predictions after a model has been trained. Notable examples include methods for assessing feature importance[5],[6], and broader post-hoc techniques, e.g., model visualizations[7],[8], or interpretable distillation[9],[10].\n' +
      '\n' +
      'Meanwhile, pre-trained large language models (LLMs) have shown impressive proficiency in a range of complex NLP tasks, significantly advancing the field and opening new frontiers for applications[11, 12, 13]. However, the inability to effectively interpret these models has debilitated their use in high-stakes applications such as medicine and raised issues related to regulatory pressure, safety, and alignment[14, 15, 16]. Moreover, this lack of interpretability has limited the use of LLMs (and other neural-network models) in fields such as science and data analysis[17, 18, 19]. In these settings, the end goal is often to elicit a trustworthy interpretation, rather than to deploy an LLM.\n' +
      '\n' +
      'Footnote 1: We use the terms interpretable, explainable, and transparent interchangeably.\n' +
      '\n' +
      'In this work, we contend that LLMs hold the opportunity to rethink interpretability with a more ambitious scope. LLMs can elicit more elaborate explanations than the previous generation of interpretable ML techniques. While previous methods have often relied on restricted interfaces such as saliency maps, LLMs can communicate directly in expressive natural language. This allows users to make targeted queries, such as _Can you explain your logic?_, _Why didn\'t you answer with (A)?_, or _Explain this data to me._, and get immediate, relevant responses. We believe simple questions such as these, coupled with techniques for grounding and processing data, will allow LLMs to articulate previously incomprehensible model behaviors and data patterns directly to humans in understandable text. However, unlocking these opportunities requires tackling new challenges, including hallucinated (i.e. incorrect or baseless) explanations, along with the immense size, cost, and inherent opaqueness of modern LLMs.\n' +
      '\n' +
      'Contributions and overviewWe evaluate LLM interpretation and highlight emerging research priorities, taking a broader scope than recent works, e.g., those focused on explaining LLM predictions[20], mechanistic interpretability[21], social science[19], or science more generally[17, 22, 23]. Rather than providing an exhaustive overview of methods, we highlight the aspects of interpretability that are unique toLLMs and showcase them with practically useful methods.\n' +
      '\n' +
      'Specifically, we begin with a background and definitions (Sec. 2) before proceeding to analyze the unique opportunities and challenges that LLMs present for interpretation (Sec. 3). We then ground these opportunities in two complementary categories for LLM-based interpretation (see Fig. 1). The first is generating explanations for _an existing LLM_ (Sec. 4), which is useful for auditing a model\'s performance, alignment, fairness, etc. The second is explaining _a dataset_ (Sec. 5); in this setting, an LLM is used to help analyze a new dataset (which can consist of either text or tabular features).\n' +
      '\n' +
      'Throughout the paper, we highlight dataset explanation and interactive explanation as emerging research priorities. Together, these two areas have great potential real-world significance in domains from science to statistics, where they can facilitate the process of scientific discovery, data analysis, and model building. Throughout, we focus on pre-trained LLMs, mostly applied to text data, but also applied to tabular data.\n' +
      '\n' +
      '## 2 Background: definitions and evaluation\n' +
      '\n' +
      'DefinitionsWithout context, _interpretability_ is a poorly defined concept. Precisely defining interpretability requires understanding the problem and audience an interpretation is intended to serve. In light of this imprecision, interpretable ML has largely become associated with a narrow set of techniques, including feature attribution, saliency maps, and transparent models. However, LLM interpretation is broader in scope and more expressive than these methods. Here, we paraphrase the definition of interpretable ML from a prior work [2] to define LLM interpretation as the _extraction of relevant knowledge from an LLM concerning relationships either contained in data or learned by the model_. We empha\n' +
      '\n' +
      'Figure 1: **Categorization of LLM interpretation research. (A) LLMs raise unique opportunities and challenges for interpretation (Sec. 3). (B) Explaining an LLM can be categorized into methods that seek to explain a single generation from an LLM (i.e. local explanation, Sec. 4.1) or the LLM in its entirety (i.e. global/mechanistic explanation, Sec. 4.2). Local explanation methods build on many techniques that were originally developed for interpreting non-LLM models, such as feature attribution methods. More recent local explanation techniques use LLMs themselves to yield interpretations, e.g., through post-hoc natural language (NL) explanations, asking an LLM to build explanations into its generation process, or through data grounding. Similar techniques have been developed and applied to global explanation, although it also includes unique types of explanations, e.g., analyzing individual attention heads or circuits inside an LLM. (C) Sec. 5 analyzes the emerging area that uses an LLM to aid in directly explaining a _dataset_. In this setting, an LLM is given a new dataset (which can consist of either text or tabular features) and is used to help analyze it. LLM-based techniques for dataset explanation are quite diverse, including helping to build interpretable models, generate NL explanations, generate chains of NL explanations, or construct data visualizations. (D) Common themes emerge among methods for local explanation, global explanation, and dataset explanation.**\n' +
      '\n' +
      'size that this definition applies to both interpreting an LLM and to using an LLM to generate explanations. Moreover, the definition relies on the extraction of _relevant_ knowledge, i.e., knowledge that is useful for a particular problem and audience. For example, in a code generation context, a relevant interpretation may help a user quickly integrate an LLM-generated code snippet. In contrast, a relevant interpretation in a medical diagnosis setting may inform a user whether or not a prediction is trustworthy.\n' +
      '\n' +
      'The term _large language model (LLM)_ is often used imprecisely. Here, we use it to refer to transformer-based neural language models that contain tens to hundreds of billions of parameters, and which are pre-trained on massive text data, e.g., PaLM[24], LLaMA[12], and GPT-4[13]. Compared to early pre-trained language models, such as BERT, LLMs are not only much larger, but also exhibit stronger language understanding, generation abilities, and explanation capabilities. After an initial computationally intensive pre-training stage, LLMs often undergo instruction finetuning and further alignment with human preferences to improve instruction following [25] or to improve interactive chat capabilities, e.g., the LLaMA-2 chat model[12]. They are sometimes also further adapted via supervised finetuning to improve performance in a specific domain, such as medicine[26].\n' +
      '\n' +
      'After undergoing these steps, LLMs are often used with _prompting_, the most common interface for applying LLMs (and our main focus in this paper). In prompting, a text prompt is directly fed to an LLM and used to generate subsequent output text. _Few-shot prompting_ is a type of prompting that involves providing an LLM with a small number of examples to allow it to better understand the task it is being asked to perform.\n' +
      '\n' +
      'Evaluating LLM interpretationsSince different interpretations are relevant to different contexts, the ideal way to evaluate an interpretation is by studying whether its usage in a real-world setting with humans improves a desired outcome[27]. In contrast, simply measuring human judgment of explanations is not particularly useful, as it may not translate into improvements in practice[28]. A recent meta-analysis finds that introducing NLP explanations into settings with humans yields widely varying utilities, ranging from completely unhelpful to very useful[29]. An important piece of this evaluation is the notion of complementarity[30], i.e., that explanations should help LLMs complement human performance in a team setting, rather than improve their performance in isolation.\n' +
      '\n' +
      'While human studies provide the most realistic evaluation, automated metrics (that can be computed without involving humans) are desirable to ease and scale evaluation, especially in mechanistic interpretability. An increasingly popular approach is to use LLMs themselves in evaluation, although great care must be taken to avoid introducing biases, e.g., an LLM systematically scoring its own outputs too positively[31]. One way to reduce bias is to use LLMs as part of a structured evaluation process tailored to a particular problem, rather than directly querying LLMs for evaluation scores. For example, one common setting is evaluating a natural-language interpretation of a given function (which may be any component of a pre-trained LLM). In this setting, one can evaluate an explanation\'s ability to simulate the function\'s behavior[32], the function\'s output on LLM-generated synthetic data[33], or its ability to recover a groundtruth function[34, 35]. In a question-answering setting, many automated metrics have been proposed for measuring the faithfulness of a natural-language explanation for an individual answer to a question[36, 37, 38].\n' +
      '\n' +
      'A final avenue for evaluating interpretations is through their ability to alter/improve model performance in useful ways. This approach provides strong evidence for the utility of an explanation, although it does not encompass all critical use cases of interpretability (particularly those directly involving human interaction). Model improvements can take various forms, the simplest of which is simply improving accuracy at downstream tasks. For example, few-shot accuracy was seen to improve when aligning an LLM\'s rationales with explanations generated using post-hoc explanation methods[39] or explanations distilled from large models[40]. Moreover, employing few-shot explanations during inference (not training) can significantly improve few-shot LLM accuracy, especially when these explanations are further optimized[41, 42]. Beyond general performance, explanations can be used to overcome specific shortcomings of a model. For example, one line of work identifies and addresses shortcuts/spurious correlations learned by an LLM[43, 44, 45]. Model editing, a related line of work, enables precise modifications to certain model behaviors, enhancing overall performance[46, 47, 48].\n' +
      '\n' +
      '## 3 Unique opportunities and challenges of LLM interpretation\n' +
      '\n' +
      'Unique opportunities of LLM interpretationFirst among LLM interpretation opportunities is the ability to provide _a natural-language interface_ to explain complex patterns. This interface is very familiar to humans, potentially ameliorating the difficulties that practitioners often face when using explainability techniques[50, 49]. Additionally, natural language can be used to build a bridge between humans and a range of other modalities, e.g., DNA, chemical compounds, or images[51, 52, 53], that may be difficult for humans to interpret on their own. In these cases, natural language allows for expressing complex concepts through explanations at different levels of granularity, potentially grounded in evidence or discussions of counterfactuals.\n' +
      '\n' +
      'A second major opportunity is the ability for LLMs to generate _interactive explanations_. Interactivity allows users to tailor explanations to their unique needs, e.g., by asking follow-up questions and performing analysis on related examples. Interviews with decision-makers, including physicians and policymakers, indicate that they strongly prefer interactive explanations, particularly in the form of natural-language dialogues[54]. Interactivity further allows LLM explanations to be decomposed into many different LLM calls, each of which can be audited independently. This can be enabled in different ways, e.g., having a user repeatedly chat with an LLM using prompting, or providing a user a sequence of LLM calls and evidence to analyze.\n' +
      '\n' +
      'Unique challenges of LLM interpretationThese opportunities bring new challenges. First and foremost is the issue of _hallucination_, i.e. incorrect or baseless explanations. Flexible explanations provided in natural language can quickly become less grounded in evidence, whether the evidence is present in a given input or presumed to be present in the knowledge an LLM has learned from its training data. Hallucinated explanations are unhelpful or even misleading, and thus techniques for identifying and combating hallucination are critical to the success of LLM interpretation.\n' +
      '\n' +
      'A second challenge is the _immunity and opaqueness_ of LLMs. Models have grown to contain tens or hundreds of billions of parameters[11, 12], and continue to grow in size. This makes it infeasible for a human to inspect or even comprehend the units of an LLM. Moreover, it necessitates efficient algorithms for interpretation, as even generating a single token from an LLM often incurs a non-trivial computational cost. In fact, LLMs are often too large to be run locally or can be accessed only through a proprietary text API, necessitating the need for interpretation algorithms that do not have full access to the model (e.g., no access to the model weights or the model gradients).\n' +
      '\n' +
      '## 4 Explaining an LLM\n' +
      '\n' +
      'In this section, we study techniques for explaining an LLM, including explaining a single generation from an LLM (Sec. 4.1) or an LLM in its entirety (Sec. 4.2). We evaluate both traditional interpretable ML techniques and LLM-based techniques for explaining an LLM.\n' +
      '\n' +
      '### Local explanation\n' +
      '\n' +
      'Local explanation, i.e., explaining a single generation from an LLM, has been a major focus in the recent interpretability literature. It allows for understanding and using LLMs in high-stakes scenarios, e.g., healthcare.\n' +
      '\n' +
      'The simplest approach for providing local explanations in LLMs provides feature attributions for input tokens. These feature attributions assign a relevance score to each input feature, reflecting its impact on the model\'s generated output. Various attribution methods have been developed, including perturbation-based methods[6], gradient-based methods[55, 56], and linear approximations[5]. Recently, these methods have been specifically adapted for transformer models, addressing unique challenges such as discrete token embeddings[57, 58] and computational costs[59]. Moreover, the conditional distribution learned by an LLM can be used to enhance existing attribution methods, e.g., by performing input marginalization[60]. Besides feature attributions, attention mechanisms within an LLM offer another avenue for visualizing token contributions to an LLM generation[61], though their faithfulness/effectiveness remains unclear[62]. Interestingly, recent work suggests that LLMs themselves can generate post-hoc attributions of important features through prompting[63]. This approach could be extended to enable eliciting different feature attributions that are relevant in different contexts.\n' +
      '\n' +
      'Beyond token-level attributions, LLMs can also generate local explanations directly in natural language. While the generation of natural-language explanations predates the current era of LLMs (e.g., in text classification[64, 65] or image classification[66]), the advent of more powerful models has significantly enhanced their effectiveness. Natural-language explanations generated by LLMs have shown the ability to elucidate model predictions, even simulating counterfactual scenarios[67], and expressing nuances like uncertainty[68, 69, 70]. Despite their potential benefits, natural language explanations remain extremely susceptible to hallucination or inaccuracies, especially when generated post-hoc[71, 72].\n' +
      '\n' +
      'One starting point for combating these hallucinations is integrating an explanation within the answer-generation process itself. Chain-of-thought prompting exemplifies this approach[73], where an LLM is prompted to articulate its reasoning step-by-step before arriving at an answer. This reasoning chain generally results in more accurate and faithful outcomes, as the final answer is more aligned with the preceding logical steps. The robustness of this method can be tested by introducing perturbations in the reasoning process and observing the effects on the final output[74, 75, 76]. Alternative methods for generating this reasoning chain exist, such as tree-of-thoughts[77], which extends chain-of-thought to instead generate a tree of thoughts used in conjunction with backtracking, graph-of-thoughts[78], and others[79, 80, 81]. All of these methods not only help convey an LLM\'s intermediate reasoning to a user, but also help the LLM to follow the reasoning through prompting, often enhancing the reliability of the output. However, like all LLM-based generations, the fidelity of these explanations can vary[76, 82].\n' +
      '\n' +
      'An alternative path to reducing hallucinations during generation is to employ retrieval-augmented generation (RAG). In RAG, an LLM incorporates a retrieval step in its decision-making process, usually by searching a reference corpus or knowledge base using text embeddings[83, 84] (see review[85]). This allows the information that is used to generate an output to be specified and examined explicitly, making it easier to explain the evidence an LLM uses during decision-making.\n' +
      '\n' +
      '### Global and mechanistic explanation\n' +
      '\n' +
      'Rather than studying individual generations, global / mechanistic explanations aim to understand an LLM as a whole. These explanations can help to audit a model for concerns beyond generalization, e.g., bias, privacy, and safety, helping to build LLMs that are more efficient / trustworthy, They can also yield mechanistic understanding about how LLMs function. To do so, researchers have focused on summarizing the behaviors and mechanisms of LLMs through various lenses. Generally, these works require access to model weights and do not work for explaining models that are only accessible through a text API, e.g., GPT-4[13].\n' +
      '\n' +
      'One popular method for understanding neural-network representations is probing. Probing techniques analyze a model\'s representation either by decoding embedded information, e.g., syntax[86], or by testing the model\'s capabilities through precisely designed tasks, e.g., subject-verb agreement[87, 88]. In the context of LLMs, probing has evolved to include the analysis of attention heads[89], embeddings[90], and different controllable aspects of representations[91]. It also includes methods that directly decode an output token to understand what is represented at different positions and layers[92, 93]. These methods can provide a deeper understanding of the nuanced ways in which LLMs process and represent information.\n' +
      '\n' +
      'In addition to probing, many works study LLM representations at a more granular level. This includes categorizing or decoding concepts from individual neurons[94, 95] or directly explaining the function of attention heads in natural language[32, 33, 96]. Beyond individual neurons, there is growing interest in understanding how groups of neurons combine to perform specific tasks, e.g., finding a circuit for indirect object identification[97], for entity binding[98], or for multiple shared purposes[99]. More broadly, this type of analysis can be applied to localize functionalities rather than fully explain a circuit, e.g., localizing factual knowledge within an LLM[100, 46]. A persistent problem with these methods is that they are difficult to scale to immense LLMs, leading to research in (semi)-automated methods that can scale to today\'s largest LLMs[101, 102].\n' +
      '\n' +
      'A complementary approach to mechanistic understanding uses miniature LLMs as a test bed for investigating complex phenomena. For example, examining a 2-layer transformer model reveals information about what patterns are learned by attention heads as a function of input statistics[103] or helps identify key components, such as induction heads or ngram heads that copy and utilize relevant tokens[104, 105]. This line of mechanistic understanding places a particular focus on studying the important capability of in-context learning, i.e., given a few input-output examples in a prompt, an LLM can learn to correctly generate an output for a new input[106, 107].\n' +
      '\n' +
      'A related area of research seeks to interpret an LLM by understanding the influence of its training data distribution. Unlike other methods we have discussed, this requires access to an LLM\'s training dataset, which is often unknown or inaccessible. In the case that the data is known, researchers can employ techniques such as influence functions to identify important elements in the training data[108]. They can also study how model behaviors arise from patterns in training data, such as hallucination in the presence of long-tail data[109], in the presence of repeated training data[110], or statistical patterns that contradict proper reasoning[111].\n' +
      '\n' +
      'All these interpretation techniques can be improved via LLM-based interactivity, allowing a user to investigate different model components via follow-up queries and altered prompts from a user. For example, one recent work introduces an end-to-end framework for explanation-based debugging and improvement of text models, showing that it can quickly yield improvements in text-classification performance[112]. Another work, Talk2Model, introduces a natural-language interface that allows users to interrogate a tabular prediction model through a dialog, implicitly calling many different model explainability tools, such as calculating feature importance[113].1 More recent work extends Talk2Model to a setting interrogating an LLM about its behavior[114].\n' +
      '\n' +
      'Footnote 1: Note that Talk2Model focuses on interpreting prediction models rather than LLMs.\n' +
      '\n' +
      'Finally, the insights gained from mechanistic understanding are beginning to inform practical applications, with current areas of focus including model editing[46], improving instruction following[115], and model compression[116]. These areas simultaneously serve as a sanity check on many mechanistic interpretations and as a useful path to enhancing the reliability of LLMs.\n' +
      '\n' +
      '## 5 Explaining a dataset\n' +
      '\n' +
      'As LLMs improve their context length and capabilities, they can be leveraged to explain an entire dataset, rather than explaining an LLM or its generations. This can aid with data analysis, knowledge discovery, and scientific applications. Fig. 2 shows an overview of dataset explanations at different levels of granularity, which we cover in detail below. We distinguish between tabular and text data, but note that most methods can be successfully applied to either, or both simultaneously in a multimodal setting.\n' +
      '\n' +
      'Tabular dataOne way LLMs can aid in dataset explanation is by making it easier to interactively visualize and analyze tabular data. This is made possible by the fact that LLMs can simultaneously understand code, text, and numbers by treating them all as input tokens. Perhaps the most popular method in this category is ChatGPT Code Interpreter3, which enables uploading datasets and building visualizations on top of them through an interactive text interface. This capability is part of a broader trend of LLM-aided visualization, e.g., suggesting automatic visualizations for dataframes [117], helping to automate data wrangling [118], or even conducting full-fledged data analysis [119]. These capabilities benefit from a growing line of work that analyzes how to effectively represent and process tabular data with LLMs [120, 121, 122].\n' +
      '\n' +
      'LLMs can also help explaining datasets by directly analyzing models that have been fit to tabular data Unlike mechanistic interpretability, where the goal is to understand the model, in dataset explanation, the goal is to understand patterns in the data through the model (although similar techniques can be used for both problems). For example, one recent work uses LLMs to analyze generalized additive models (GAMs) that are fit to tabular data [123]. GAMs are interpretable models that can be represented as a set of curves, each representing the contribution of a feature to the output prediction as a function of the feature\'s value. An LLM can analyze the fitted model (and thereby the underlying dataset) by processing each curve as a set of numerical tokens and then detecting and describing patterns in each curve. The authors find that LLMs can identify surprising characteristics in the curves and the underlying data, largely based on their prior knowledge of a domain. Rather than using an interpretable GAM model, another approach is to distill dataset insights by analyzing classifier predictions. For example, MaNtLE generates natural-language descriptions of a classifier\'s rationale based on the classifier\'s predictions, and these explanations are found to identify explainable subgroups that contain similar feature patterns [124].\n' +
      '\n' +
      'Footnote 3: [https://openai.com/blog/chatgpt-plugins#code-interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter)\n' +
      '\n' +
      'Text dataText data poses different challenges for dataset explanation than tabular data because it is sparse, high-dimensional, and modeling it requires many high-order interactions. As a result, interpretable models that have been successful in the tabular domain (e.g., sparse linear models [125, 126], GAMs [127, 128, 129], decision trees [130, 131, 132], and others [133, 134]), have struggled to accurately model text. One recent line of work addresses this issue by using LLMs to help build fully interpretable text models, such as linear models or decision trees [135]; the resulting models are surprisingly accurate, often outperforming even much larger LLM models. These interpretable models can help explain a dataset by showing which features (i.e. words or ngrams) are important for predicting different outcomes. Similar methods, e.g., CHiLL [136] use LLMs to build interpretable representations for text classification tasks.\n' +
      '\n' +
      'Going beyond fully interpretable models, LLMs also help in building partially interpretable text models. Partially interpretable text models often employ chains of prompts; these chains allow for decomposing an LLM\'s decision-making process to analyze which dataset patterns a model learns. Prompt chains are usually constructed by humans or by querying a model to generate a chain of calls on-the-fly [137]. For dataset explanation, the most relevant chains are sequences of explanations that are generated by an LLM. For example, a model can generate a single tree of explanations that is shared across all examples in a dataset, a process that enables understanding hierarchical structures stored within a dataset [138]. Rather than a tree, a single chain of prompts can often help an LLM employ self-verification, i.e. the model itself checks its previous generations using a chain of prompts, a popular technique that often improves reliability [139, 140, 141]. As in local explanation, an LLM can incorporate a retrieval step in its decision-making process [85], and access to different tools can help make different steps (e.g., arithmetic) more reliable and transparent [142].\n' +
      '\n' +
      'Natural-language explanations hold the potential to produce rich, concise descriptions of patterns present in a dataset, but are prone to hallucination. One method, iPrompt [143], aims to avoid hallucination by searching for a dataset explanation in the form of a single prompt, and verifying that the prompt induces an LLM to accurately predict a pattern in the underlying dataset. Related methods use LLMs to provide descriptions that differentiate between groups in a dataset, followed by an LLM that verifies the credibility of the description [144, 145, 35]. In addition to a raw natural-language explanation, LLMs can aid in summarizing textual information, e.g., through explainable clustering of a text dataset [146] or creating prompt-based topic models [147].\n' +
      '\n' +
      '## 6 Future research priorities\n' +
      '\n' +
      'We now highlight research priorities surrounding LLM interpretation in three areas: explanation reliability, dataset explanation, and interactive explanations.\n' +
      '\n' +
      'Explanation reliabilityAll LLM explanations are bottleneck by reliability issues. This includes hallucinations [148], but encompasses a broader set of issues. For example, LLMs continue to be very sensitive to the nuances of prompt phrasing; minor variations in prompts can completely change the substance of an LLM output [149, 150]. Additionally, LLMsmay ignore parts of their context, e.g., the middle of long contexts [151] or instructions that are difficult to parse [115].\n' +
      '\n' +
      'These reliability issues are particularly critical in interpretation, which often uses explanations to mitigate risk in high-stakes settings. One work analyzing explanation reliably finds that LLMs often generate seemingly correct explanations that are actually inconsistent with their own outputs on related questions [71], preventing a human practitioner from trusting an LLM or understanding how its explanations apply to new scenarios. Another study finds that explanations generated by an LLM may not entail the model\'s predictions or be factually grounded in the input, even on simple tasks with extractive explanations[72]. Future work will be required to improve the grounding of explanations and develop stronger methods to test their reliability, perhaps through methods such as self-verification [139], iterative prompting [143], or automatically improving model self-consistency [152, 153, 154].\n' +
      '\n' +
      'Dataset explanation for knowledge discoveryDataset explanation using LLMs (Sec. 5) holds the potential to help with the generation and discovery of new knowledge from data [17, 22, 23], rather than simply helping to speed up data analysis or visualization. Dataset explanation could initially help at the level of brainstorming scientific hypotheses that can then be screened or tested by human researchers [155]. During and after this process, LLM explanations can help with using natural language to understand data from otherwise opaque domains, such as chemical compounds [156] or DNA sequences [51]. In the algorithms domain, LLMs have been used to uncover new algorithms, translating them to humans as readable computer programs [157]. These approaches could be combined with data from experiments to help yield new data-driven insights.\n' +
      '\n' +
      'LLM explanations can also be used to help humans better perform a task. Explanations from transformers have already begun to be applied to domains such as Chess, where their explanations can help improve even expert players [158]. Additionally, LLMs can provide explanations of expert human behavior, e.g. "Why did the doctor prescribe this medication given this information about the patient?", that are helpful in understanding, auditing, and improving human behavior [159].\n' +
      '\n' +
      'Interactive explanationsFinally, advancements in LLMs are poised to allow for the development of more user-centric, interactive explanations. LLM explanations and follow-up questions are already being integrated into a variety of LLM applications, such as interactive task specification [160], recommendation [161], and a wide set of tasks involving dialog. Furthermore, works like Talk2Model [113] enable users to interactively audit models in a conversational manner. This dialog interface could be used in conjunction with many of the methods covered in this work to help with new applications, e.g., interactive dataset explanation.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      'In this paper, we have explored the vast and dynamic landscape of interpretable ML, particularly focusing on the unique opportunities and challenges presented by LLMs. LLMs\' advanced natural language generation capabilities have opened new avenues for generating more elaborate and nuanced explanations, allowing for a deeper and more accessible understanding of complex patterns in data and model behaviors. As we navigate this terrain, we assert that the integration of LLMs into interpretative processes is not merely an enhancement of existing methodologies but a transformative shift that promises to redefine the boundaries of machine learning interpretability.\n' +
      '\n' +
      'Our position is anchored in the belief that the future of interpretable ML hinges on our ability to harness the full potential of LLMs. To this end, we outlined several key stances and directions for future research, such as enhancing explanation reliability and advancing dataset interpretation for knowledge discovery. As LLMs continue to improve rapidly, these explanations (and all the methods discussed\n' +
      '\n' +
      'Figure 2: **Dataset explanations at different levels of granularity.** Dataset explanation involves understanding a new dataset (consisting of either text or tabular features) using a pre-trained LLM. Low-level explanations are more faithful to the dataset but involve more human effort to extract meaningful insights. Many dataset interpretations use prediction models (classification or regression) as a means to identify and explain patterns between features.\n' +
      '\n' +
      'in this work) will advance correspondingly to enable new applications and insights. In the near future, LLMs may be able to offer the holy grail of interpretability: explanations that can reliably aggregate and convey extremely complex information to us all.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of interpretability. _arXiv preprint arXiv:1702.08608_, 2017. \\(\\sim\\)1.\n' +
      '* [2] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Aal, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. _Proceedings of the National Academy of Sciences of the United States of America_, 116(44):22071-22080, 2019. \\(\\sim\\)2.\n' +
      '* [3] Christoph Molnar. _Interpretable machine learning_. Lulu. com, 2019. \\(\\sim\\)1.\n' +
      '* [4] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. _arXiv preprint arXiv:2103.11251_, 2021. \\(\\sim\\)1.\n' +
      '* [5] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions and classifier. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1135-1144. ACM, 2016. \\(\\sim\\)1.\n' +
      '* [6] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In _Advances in Neural Information Processing Systems_, pages 4768-4777, 2017. \\(\\sim\\)1 and 4\n' +
      '* [7] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. _arXiv preprint arXiv:1506.06579_, 2015. \\(\\sim\\)1.\n' +
      '* [8] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and Antonio Torralba. GAN dissection: Visualizing and understanding generative adversarial networks. _arXiv preprint arXiv:1811.10597_, 2018. \\(\\sim\\)1.\n' +
      '* [9] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare: Auditing black-box models using transparent model distillation. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 303-310, 2018. \\(\\sim\\)1.\n' +
      '* [10] Wooseok Ha, Chandan Singh, Francois Lannse, Sirgoikul Upadhyavula, and Bin Yu. Adaptive wavelet distillation from neural networks through interpretations. _Advances in Neural Information Processing Systems_, 34:20669-20682, 2021. \\(\\sim\\)1.\n' +
      '* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prufalla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. \\(\\sim\\)1.\n' +
      '* [12] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahari, Yasmine Bahoei, Nikuloy Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhossale, et al. Llama 2: Open foundation and fine-tuned chut models. _arXiv preprint arXiv:2307.09288_, 2023. \\(\\sim\\)3 and 3\n' +
      '* [13] OpenAI. GPT-4 technical report, 2023. \\(\\sim\\)1., 3, and 5\n' +
      '* [14] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a" right to explanation". _arXiv preprint arXiv:1606.08813_, 2016. \\(\\sim\\)1.\n' +
      '* [15] Dario Amodel, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mand. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06555_, 2016. (Not cited)\n' +
      '* [16] Iason Gabriel. Artificial intelligence, values, and alignment. _Minds and machines_, 30(3):411-437, 2020. \\(\\sim\\)1.\n' +
      '* [17] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Lin, Payal Chandak, Shengchao Liu, Peter Van Katvyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 620(7972):47-60, 2023. \\(\\sim\\)1 and 7\n' +
      '* [18] Enkelejda Kasneci, Kathrin Soller, Stefan Koehmann, Maria Bannert, Daryna Demmetieux, Frank Fischer, Urs Gasser, Georg Groh, Stephan Gunnemann, Eyke Hullemeier, et al. ChatGPT for good? on opportunities and challenges of large language models for education. _Learning and individual differences_, 103:102272, 2023. (Not cited).\n' +
      '* [19] Cache Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _arXiv preprint arXiv:2305.03514_, 2023. \\(\\sim\\)1.\n' +
      '* [20] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huigi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. _arXiv preprint arXiv:2309.01029_, 2023. \\(\\sim\\)1.\n' +
      '* [21] Tilman Rauker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In _2021 IEEE Conference on Secure and Transworth Machine Learning (SaTML)_, pages 464-483. IEEE, 2023. \\(\\sim\\)1.\n' +
      '* [22] Abeba Birhane, Atosona Kasirzadeh, David Leslie, and Sandra Wachter. Science in the age of large language models. _Nature Reviews Physics_, pages 1-4, 2023. \\(\\sim\\)1 and 7\n' +
      '* [23] Luca Pon-Tonchini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert, W Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Hauko Wainwright, Ghanshyam Pianian, Benjamin Nachman, et al. Learning from learning machines: a new generation of AI technology to meet the needs of science. _arXiv preprint arXiv:2111.13786_, 2021. \\(\\sim\\)1 and 7\n' +
      '* [24] Aakanksha Chowdhury, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaur Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Pal.M: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(20):1-113, 2023. \\(\\sim\\)3.\n' +
      '* [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Cheng Zhang, Sandini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022. \\(\\sim\\)3.\n' +
      '* [26] Karan Singhal, Tao Tu, Jorai Gottweis, Rory Sayres, Ellery Walczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlenne Neal, et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023. \\(\\sim\\)3.\n' +
      '* [27] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernando Viegas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (teav). _arXiv preprint arXiv:1711.11279_, 2017. \\(\\sim\\)3.\n' +
      '* [28] Julius Adebayo, Justin Gilmer, Michael Moehlty, Ian Goodfellow, Moritz Hard, and Been Kim. Suitty checks for saliency maps. In _Advances in Neural Information Processing Systems_, pages 9505-9515, 2018. \\(\\sim\\)3.\n' +
      '* [29] Fateme Hashemi Chaleshotri, Atreya Ghosal, and Ana Marasovic. On evaluating explanation utility for Human-AI decision-making in NLP. In _NAI at Action: Trust, Present, and Future Applications_, 2023. \\(\\sim\\)3.\n' +
      '* [30] Gagan Bansal, Tonghuang Wu, Joyce Zhou, Raymond Fok, Besmin Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed its parts? the effect of AI explanations on complementary team performance. In _Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems_, pages 1-16, 2021. \\(\\sim\\)3.\n' +
      '* [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohuan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MFTbench and chattorid arena. In _Thirty-aeventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. \\(\\sim\\)3.\n' +
      '* [32] Steven Bills, Nick Cammrata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Golb, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models, 2023. \\(\\sim\\)3 and 5\n' +
      '* [33] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural language with language models. _arXiv preprint arXiv:2305.09863_, 2023. \\(\\sim\\)3 and * [34] Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torrallba. FIND: A function description benchmark for evaluating interpretability methods. _arXiv e-prints_, pages arXiv-2309, 2023. \\(\\dashdot\\)3.\n' +
      '* [35] Rusiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. _ArXiv_, abs/2302.14233, 2023. \\(\\dashdot\\)3 and 6.\n' +
      '* [36] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Gure Simonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. _arXiv preprint arXiv:2305.18029_, 2023. \\(\\dashdot\\)3.\n' +
      '* [37] Leittia Practalbescu and Anette Frank. On measuring faithfulness of natural language explanations. _arXiv preprint arXiv:2311.07466_, 2023. (Not cited.)\n' +
      '* [38] Hanjie Chen, Facez Brahman, Xiang Ren, Yangfeng Bi, Yejin Choi, and Swohba Sugayama.fpas. Rev: information-theoretic evaluation of free-text rationales. _arXiv preprint arXiv:2210.04982_, 2022. \\(\\dashdot\\)3.\n' +
      '* [39] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabulto Lakkaraju. Post hoc explanations of language models can improve language models. _arXiv preprint arXiv:2305.11426_, 2023. \\(\\dashdot\\)3.\n' +
      '* [40] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahai Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. _arXiv preprint arXiv:2306.02707_, 2023. \\(\\dashdot\\)3.\n' +
      '* [41] Andrew K Lampien, Ishita Dasgupta, Stephanie CY Chan, Kory Mathewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Feix Hill. Can language models learn from explanations in context? _arXiv preprint arXiv:2104.02329_, 2022. \\(\\dashdot\\)3.\n' +
      '* [42] Xi Ye and Greg Durrett. Explanation selection using unlabeled data for chain-of-thought prompting. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 619-637, 2023. \\(\\dashdot\\)3.\n' +
      '* [43] Mengnan Du, Fengxiang He, Na Zou, Ducheng Tao, and Xia Hu. Shortcut learning of large language models in natural language understanding. _Communications of the ACM (CACM)_, 2023. \\(\\dashdot\\)3.\n' +
      '* [44] Choongwoong Kang and Jaesik Choi. Impact of co-occurrence on factual knowledge of large language models. _arXiv preprint arXiv:2310.08256_, 2023. (Not cited.)\n' +
      '* [45] Isamjin Bastiangs, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Kajta Filippova. "Will you find these shortcuts?" a protocol for evaluating the faithfulness of input salience methods for text classification. _arXiv preprint arXiv:2111.07367_, 2021. \\(\\dashdot\\)3.\n' +
      '* [46] Kevin Meng, David Bau, Alex Andonian, and Yonatan Beilnikov. Locating and editing factual knowledge in GPT. _arXiv preprint arXiv:2202.05262_, 2022. \\(\\dashdot\\)3 and 5\n' +
      '* [47] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale, 2022. (Not cited.)\n' +
      '* [48] Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge representations in language models, 2023. \\(\\dashdot\\)3.\n' +
      '* [49] Harmampreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. Interpreting interpretability: understanding data scientists\' use of interpretability tools for machine learning. In _Proceedings of the 2020 CHI conference on human factors in computing systems_, pages 1-14, 2020. \\(\\dashdot\\)3.\n' +
      '* [50] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible. _Communications of the ACM_, 62(6):70-79, 2019. \\(\\dashdot\\)3.\n' +
      '* [51] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scilom, Anthony Harshman, Elvis Saravia, Andrew Paulton, Viktor Kerzlez, and Robert Stojnic. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022. \\(\\dashdot\\)3 and 7\n' +
      '* [52] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _ArXiv_, abs/2212.10789, 2022. (Not cited.)\n' +
      '* [53] Alce Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Ananda Asikel, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021. \\(\\dashdot\\)3.\n' +
      '* [54] Himabulto Lakkaraju, Dylan Slack, Yuxia Chen, Chenchao Tan, and Sameer Singh. Rethinking explainability as a dialogue: A practitioner\'s perspective. _arXiv preprint arXiv:2202.01875_, 2022. \\(\\dashdot\\)4.\n' +
      '* [55] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. _ICML_, 2017. \\(\\dashdot\\)4.\n' +
      '* [56] Gregoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Muller. Explaining nonlinear classification decisions with deep taylor decomposition. _Pattern Recognition_, 65:211-222, 2017. \\(\\dashdot\\)4.\n' +
      '* [57] Sandipan Sikidar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature interaction attribution for neural tlp models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 865-878, 2021. \\(\\dashdot\\)4.\n' +
      '* [58] Joseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language models. _arXiv preprint arXiv:2305.15853_, 2023. \\(\\dashdot\\)4.\n' +
      '* [59] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. Algorithms to estimate shapley value feature attributions. _Nature Machine Intelligence_, pages 1-12, 2023. \\(\\dashdot\\)4.\n' +
      '* [60] Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon. Interpretation of tlp models through input imaginalization. _arXiv preprint arXiv:2010.13984_, 2020. \\(\\dashdot\\)4.\n' +
      '* [61] Sarah Wiegeffe and Yuxil Winter. Attention is not not explanation. _arXiv preprint arXiv:1908.04626_, 2019. \\(\\dashdot\\)4.\n' +
      '* [62] Sarthak Jain and Byron C Wallace. Attention is not explanation. _arXiv preprint arXiv:1902.10186_, 2019. \\(\\dashdot\\)4.\n' +
      '* [63] Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Ching Agarwal, and Himabulto Lakkaraju. Are large language models post hoc explainers? _arXiv preprint arXiv:2310.05797_, 2023. \\(\\dashdot\\)4.\n' +
      '* [64] Oana-Maria Camburu, Tim Rocktaschel, Thomas Lukasiewicz, and Phil Blumson. \\(\\dashdot\\)snti: Natural language inference with natural language explanations. _Advances in Neural Information Processing Systems_, 31, 2018. \\(\\dashdot\\)4.\n' +
      '* [65] Nazneen Fatema Rajani, Bryan McCann, Caining Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. _arXiv preprint arXiv:1906.02361_, 2019. \\(\\dashdot\\)4.\n' +
      '* [66] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Berri Schiele, and Trevor Darrell. Generating visual explanations. In _European conference on computer vision_, pages 3-19. Springer, 2016. \\(\\dashdot\\)4.\n' +
      '* [67] Aritta Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. LLMs as counterfactual explanation modules: Can chatGPT explain black-box text classifiers? _arXiv preprint arXiv:2309.13340_, 2023. \\(\\dashdot\\)4.\n' +
      '* [68] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. _arXiv preprint arXiv:2306.13063_, 2023. \\(\\dashdot\\)4.\n' +
      '* [69] Sree Harh Tannner, Ching Agrawal, and Himabulto Lakkaraju. Quantifying uncertainty in natural language explanations of large language models. _arXiv preprint arXiv:2311.03553_, 2023. (Not cited.)\n' +
      '* [70] Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. Relying on the unreliable: The impact of language models\' reluctance to express uncertainty, 2024. \\(\\dashdot\\)4.\n' +
      '* [71] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? Counterfactual simulability of natural language explanations. _arXiv preprint arXiv:2307.08678_, 2023. \\(\\dashdot\\)4 and 7\n' +
      '* [72] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. _Advances in neural information processing systems_, 35:30378-30392, 2022. \\(\\dashdot\\)4 and 7* [73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brain icther, Fei Xia, Ed H Chi, Quoc V Le, and Demy Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022, \\(\\dashrightarrow\\)4.\n' +
      '* [74] Aman Madan and Amir Yadadukahsh. Text and patterns: For effective chain of thought, it takes two to tango. _arXiv preprint arXiv:2209.07686_, 2022. \\(\\dashrightarrow\\)4.\n' +
      '* [75] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-though prompting: An empirical study of what matters. _arXiv preprint arXiv:2212.10001_, 2022. (Not cited.)\n' +
      '* [76] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson E. Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, John Kernton, Kamile Lakovaitte, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Sistreifer, Oliver Rausch, Robin Larson, Samuel McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, T. J. Henighan, Timothy D. Maxwell, Timothy Telleleman-Avova, Tristan Hume, Zae Hatfield-Dodds, Jared Kaplan, Janina Braumer, Sam Bowman, and Ethan Perez. Measuring faithfulness in chain-of-though reasoning. _ArXiv_, abs/2307.13702, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [77] Shuryu Yao, Dian Yu, Jeffrey Zhao, Llark Shafran, Thomas I. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [78] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasaz Lehmann, Michal Podstawski, Hubert Niewidomski, Piotr Nyczyk, et al. Graph of Thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [79] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henry Michalewski, Jacob Austin, David Bieber, David Dolan, Aitor Lewkowycz, Maarten Bosma, David Lian, Charles Sutton, and Augustus Odena. Show your work: Scratchgrads for intermediate computation with language models. _ArXiv_, abs/2112.00114, 2021. \\(\\dashrightarrow\\)4.\n' +
      '* [80] Oif Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2022. (Not cited.)\n' +
      '* [81] Demy Zhou, Nathamal Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022. \\(\\dashrightarrow\\)4.\n' +
      '* [82] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Demy Zhou, et al. Larger language models to in-context learning differently. _arXiv preprint arXiv:2303.0846_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [83] Kelvin Guu, Kenton Lee, Zora Tung, Pampuong Pasquet, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. _ArXiv_, abs/2002.08909, 2020. \\(\\dashrightarrow\\)5.\n' +
      '* [84] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Quuyuan Huang, Lars Lien, Zhou Yu, Weiich Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback. _ArXiv_, abs/2302.12813, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [85] Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and Carlos Guestrin. Unifying corroborative and contributive attributions in large language models. _arXiv preprint arXiv:2311.12233_, 2023. \\(\\dashrightarrow\\)5 and 6\n' +
      '* [86] Alexis Conneau, German Kuszewski, Guillaume Lample, Lok Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. _arXiv preprint arXiv:1805.01070_, 2018. \\(\\dashrightarrow\\)5.\n' +
      '* [87] Frederick Liu and Besim Avei. Incorporating priors with feature attribution on text classification. _arXiv preprint arXiv:1906.08286_, 2019. \\(\\dashrightarrow\\)5.\n' +
      '* [88] Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. _arXiv preprint arXiv:1808.09031_, 2018. \\(\\dashrightarrow\\)5.\n' +
      '* [89] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look at? An analysis of bert\'s attention. _arXiv preprint arXiv:1906.04341_, 2019. \\(\\dashrightarrow\\)5.\n' +
      '* [90] John X Morris, Volodymyr Kuleshov, Vitaly Shmutikov, and Alexander M Rush. Text embeddings reveal (almost) as much as text. _arXiv preprint arXiv:2310.06816_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [91] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xawang Yin, Mantas Mazeika, Ann-Kahrin Dembrowski, Shishwatt Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basert, Samii Koyeib, Dawn Song, Martin Fredrikson, Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI transparency. _ArXiv_, abs/2310.01405, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [92] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. _arXiv preprint arXiv:2303.08112_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [93] Asma Ghandehariou, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. PatchScope: A unifying framework for inspecting hidden representations of language models, 2024. \\(\\dashrightarrow\\)5.\n' +
      '* [94] Jesse Ma and Jacob Andreas. Compositional explanations of neurons. _Advances in Neural Information Processing Systems_, 33:17153-17163, 2020. \\(\\dashrightarrow\\)5.\n' +
      '* [95] Wes Gurnee, Neil Nanda, Matthew Pauly, Katherine Harvey, Dmitri Troitski, and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. _arXiv preprint arXiv:2305.01610_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [96] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In _International Conference on Learning Representations_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [97] Kevin Wang, Alexandre Varienigen, Arthur Conmy, Buck Shleperis, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. _arXiv preprint arXiv:2211.00593_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [98] Jiahai Feng and Jacob Steinhardt. How do language models bind entities in context? _arXiv preprint arXiv:2310.17191_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [99] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse across tasks in transformer language models. _arXiv preprint arXiv:2310.08744_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [100] Damani Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Frun Wei. Knowledge neurons in pretrained transformers. _arXiv preprint arXiv:2104.08696_, 2021. \\(\\dashrightarrow\\)5.\n' +
      '* [101] Tom Lieberum, Matthew Raluz, Janos Kamar, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? Evidence from multiple choice capabilities in chinchilla. _arXiv preprint arXiv:2307.09458_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [102] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. _ArXiv_, abs/2305.08809, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [103] Nelson Ellhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1, 2021. \\(\\dashrightarrow\\)5.\n' +
      '* [104] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [105] Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms, 2024. \\(\\dashrightarrow\\)5.\n' +
      '* [106] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [107] Hattie Zhou, Arwen Bradley, Eizi Litwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. _arXiv preprint arXiv:2310.16088_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '* [145] Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: A framework for explaining distribution shifts in natural language. _arXiv preprint arXiv:2206.15007_, 2022. \\(\\dashrightarrow\\)6.\n' +
      '* [146] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clustering via language descriptions. _arXiv preprint arXiv:2305.13749_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [147] Chau Minh Pham, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. TopicGPT: A prompt-based topic modeling framework. _arXiv preprint arXiv:2311.01449_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [148] SM Tommoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadhu, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large language models. _arXiv preprint arXiv:2401.01313_, 2024. \\(\\dashrightarrow\\)6.\n' +
      '* [149] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\' sensitivity to spurious features in prompt design or. How i learned to start worrying about prompt formatting. _arXiv preprint arXiv:2310.11324_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [150] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language models don\'t always say what they think: Unfaithful explanations in chain-of-thought prompting. _arXiv preprint arXiv:2305.04388_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [151] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michelle Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _ArXiv_, abs/2307.03172, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [152] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, and Jianfeng Gao. Towards consistent natural-language explanations via explanation-consistency through2024. \\(\\dashrightarrow\\)7.\n' +
      '* [153] Xiang Lisa Li, Vishnavi Shrivastava, Sayan Li, Titsmori Hashimoto, and Percy Liang. Benchmarking and improving generate-validator consistency of language models. _arXiv preprint arXiv:2310.01846_, 2023. (Not cited.)\n' +
      '* [154] Afra Feyza Alyurek, Ekin Akyurek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deductive closure training of language models for coherence, accuracy, and updability. _arXiv preprint arXiv:2401.08574_, 2024. \\(\\dashrightarrow\\)7.\n' +
      '* [155] Zonglin Yang, Xinya Du, Junsian Li, Lie Zheng, Souliaya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. _arXiv preprint arXiv:2309.02726_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [156] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _Nature Machine Intelligence_, 5(12):1447-1457, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [157] Bernardino Romera-Paredes, Mohammad Bernstein, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. _Nature_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [158] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the Human-AI knowledge gap: Concept discovery and transfer in alphazero. _arXiv preprint arXiv:2310.16410_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [159] Tao Tu, Anil Palepu, Mike Schezkernmann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic ai. _arXiv preprint arXiv:2401.05654_, 2024. \\(\\dashrightarrow\\)7.\n' +
      '* [160] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. _arXiv preprint arXiv:2310.11589_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [161] Xi Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender AI agent: Integrating large language models for interactive recommendations. _arXiv preprint arXiv:2308.16505_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>