<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#대언어 모델 시대의 재해석 가능성\n' +
      '\n' +
      'Chandan Singh\n' +
      '\n' +
      '이날라 지바나 프리야\n' +
      '\n' +
      'Michel Galley\n' +
      '\n' +
      'Rich Caruana\n' +
      '\n' +
      'Jianfeng Gao\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '해석 가능한 기계 학습은 점점 더 큰 데이터 세트와 심층 신경망의 증가로 촉발된 지난 10년 동안 관심 영역으로 폭발했다. 동시에 대규모 언어 모델(LLM)은 다양한 작업에 걸쳐 놀라운 기능을 보여 해석 가능한 기계 학습에서 기회를 재고할 수 있는 기회를 제공했다. 특히 자연어로 설명할 수 있는 능력은 LLM이 인간에게 주어질 수 있는 패턴의 규모와 복잡성을 확장할 수 있게 한다. 그러나 이러한 새로운 역량은 환각적인 설명과 막대한 계산 비용과 같은 새로운 문제를 제기한다.\n' +
      '\n' +
      '본 논문에서는 LLM 해석의 새로운 분야(LLM을 해석하는 것과 설명을 위해 LLM을 사용하는 것 모두)를 평가하기 위한 기존 방법을 검토하는 것으로 시작한다. 우리는 그들의 한계에도 불구하고 LLMs가 LLMs 자체를 감사하는 것을 포함하여 많은 응용 프로그램에 걸쳐 보다 야심찬 범위로 해석 가능성을 재정의할 기회를 가지고 있다고 주장한다. LLM 해석을 위한 두 가지 새로운 연구 우선 순위를 강조합니다. LLM을 사용하여 새로운 데이터 세트를 직접 분석하고 대화형 설명을 생성합니다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '기계 학습(ML) 및 자연 언어 처리(NLP)는 점점 더 큰 데이터 세트와 강력한 신경망 모델의 가용성으로 인해 최근 몇 년 동안 급속한 확장을 보였다. 이에 대해 해석 가능한 ML1 분야는 이러한 모델과 데이터 세트를 이해하기 위한 다양한 기술과 방법을 통합하도록 성장했다[1, 2, 3]. 이 확장의 한 부분은 희소 선형 모델, 일반화된 가법 모델 및 결정 트리와 같이 본질적으로 해석 가능한 모델[4]의 개발 및 사용에 초점을 맞추었다. 이러한 모델과 함께 사후 해석성 기술이 점점 더 두드러져 모델이 훈련된 후 예측에 대한 통찰력을 제공한다. 주목할 만한 예는 특징 중요도[5],[6] 및 모델 시각화[7],[8] 또는 해석 가능한 증류[9],[10]과 같은 광범위한 사후 기술을 평가하는 방법을 포함한다.\n' +
      '\n' +
      '한편, 사전 훈련된 대규모 언어 모델(LLM)은 다양한 복잡한 NLP 작업에서 인상적인 숙련도를 보여 분야를 크게 발전시키고 응용 프로그램[11, 12, 13]을 위한 새로운 프론티어를 열었다. 그러나 이러한 모델을 효과적으로 해석할 수 없기 때문에 의약품과 같은 고위험 응용 프로그램에서 사용이 쇠약해지고 규제 압력, 안전 및 정렬과 관련된 문제가 제기되었다[14, 15, 16]. 더욱이, 이러한 해석성의 결여는 과학 및 데이터 분석(17, 18, 19)과 같은 분야에서 LLM(및 다른 신경망 모델)의 사용을 제한했다. 이러한 설정에서 최종 목표는 종종 LLM을 배포하기보다는 신뢰할 수 있는 해석을 도출하는 것이다.\n' +
      '\n' +
      '각주 1: 우리는 해석 가능한 용어, 설명 가능한 용어, 투명한 용어를 혼용하여 사용한다.\n' +
      '\n' +
      '이 작업에서 우리는 LLM이 더 야심찬 범위로 해석 가능성을 재고할 기회를 가지고 있다고 주장한다. LLM은 해석 가능한 ML 기술의 이전 세대보다 더 정교한 설명을 이끌어낼 수 있다. 이전 방법은 종종 현저성 맵과 같은 제한된 인터페이스에 의존했지만 LLM은 표현 가능한 자연 언어로 직접 통신할 수 있다. 이를 통해 사용자는 논리를 설명할 수 있습니까?__와 같은 대상 쿼리를 만들 수 있습니다. 왜 (A)로 대답하지 않았어?__ 이 데이터를 설명합니다.__ 즉시 관련 답변을 받을 수 있습니다. 이러한 간단한 질문과 데이터 접지 및 처리 기술이 결합되어 LLM이 이전에 이해할 수 없었던 모델 행동과 데이터 패턴을 이해할 수 있는 텍스트로 인간에게 직접 표현할 수 있다고 믿는다. 그러나 이러한 기회를 해제하려면 현대 LLM의 엄청난 크기, 비용 및 고유한 불투명성과 함께 환각(즉, 부정확하거나 근거 없는) 설명을 포함하여 새로운 문제를 해결해야 한다.\n' +
      '\n' +
      '기여도와 개요 LLM 해석을 평가하고 최근 연구 우선 순위를 강조하며, LLM 예측[20], 기계론적 해석 가능성[21], 사회 과학[19] 또는 과학[17, 22, 23]을 더 일반적으로 설명하는 데 중점을 둔 것과 같이 최근 연구보다 광범위한 범위를 취한다. 우리는 방법에 대한 철저한 개요를 제공하기보다는LLM에 고유한 해석 가능성의 측면을 강조하고 실제로 유용한 방법으로 보여준다.\n' +
      '\n' +
      '구체적으로, 우리는 배경과 정의(Sec. 2)로 시작한다. LLM이 해석하기 위해 제시하는 고유한 기회와 과제를 분석하기 전에. 그런 다음 LLM 기반 해석을 위해 두 가지 보완 범주로 이러한 기회를 설정했다(그림 1 참조). 첫 번째는 _an existing LLM_ (Sec. 4)에 대한 설명을 생성하는 것이다. 모델의 성능, 정렬, 공정성 등을 감사하는 데 유용합니다. 두 번째는 _a dataset_(Sec)를 설명하는 것이다. 5; 이 설정에서 LLM은 새로운 데이터 세트(텍스트 또는 표 피쳐로 구성될 수 있음)를 분석하는 데 사용됩니다.\n' +
      '\n' +
      '논문 전체에서 우리는 데이터 세트 설명과 대화형 설명을 새로운 연구 우선순위로 강조한다. 함께, 이 두 영역은 과학에서 통계에 이르기까지 영역에서 과학적 발견, 데이터 분석 및 모델 구축 과정을 촉진할 수 있는 잠재적인 실제 중요성을 가지고 있다. 우리는 대부분 텍스트 데이터에 적용되지만 표 데이터에도 적용되는 사전 훈련된 LLM에 중점을 둔다.\n' +
      '\n' +
      '##2 배경 : 정의 및 평가\n' +
      '\n' +
      'Context가 없는 정의, _interpretability_는 제대로 정의되지 않은 개념이다. 정확하게 해석 가능성을 정의하려면 문제를 이해해야 하며 관객은 해석을 위해 봉사해야 한다. 이러한 부정확성에 비추어, 해석 가능한 ML은 특징 속성, 현저성 맵 및 투명 모델을 포함한 좁은 기술 세트와 크게 연관되었다. 그러나 LLM 해석은 이러한 방법보다 범위가 더 넓고 표현력이 있다. 여기에서 우리는 LLM 해석을 데이터에 포함되거나 모델_에 의해 학습된 관계에 관한 LLM에서 관련 지식의 추출로 정의하기 위해 이전 작업 [2]에서 해석 가능한 ML의 정의를 패러프레이즈한다. 우리 엠파\n' +
      '\n' +
      '그림 1: **LLM 해석 연구의 범주화. (A) LLM은 해석을 위한 독특한 기회와 과제를 제기한다(Sec. 3). (B) LLM에 대한 설명은 LLM(즉, 로컬 설명, Sec. 4.1) 또는 LLM 전체에서 단일 세대를 설명하려는 방법(즉, 글로벌/기계 설명, Sec. 4.2)으로 분류할 수 있다. 로컬 설명 방법은 특징 속성 방법과 같이 비LLM 모델을 해석하기 위해 원래 개발된 많은 기술을 기반으로 한다. 보다 최근의 로컬 설명 기술은 LLM 자체를 사용하여 예를 들어 사후 자연 언어(NL) 설명을 통해, LLM에 생성 프로세스에 설명을 구축하도록 요청하거나 데이터 접지를 통해 해석을 산출한다. 유사한 기술이 개발되어 글로벌 설명에 적용되었지만, LLM 내부의 개별 주의 헤드 또는 회로를 분석하는 것과 같은 고유한 유형의 설명도 포함한다. (C) Sec. 도 5는 LLM을 사용하여 _dataset_를 직접 설명하는 데 도움이 되는 신흥 영역을 분석한다. 이 설정에서 LLM에는 텍스트 또는 표 피쳐로 구성될 수 있는 새 데이터 세트가 제공되며 이를 분석하는 데 사용됩니다. 데이터세트 설명을 위한LLM 기반 기술은 해석 가능한 모델을 구축하거나 NL 설명을 생성하거나 NL 설명의 사슬을 생성하거나 데이터 시각화를 구성하는 데 도움이 되는 등 매우 다양하다. (D) 로컬 설명, 글로벌 설명 및 데이터세트 설명을 위한 방법 중에서 공통 주제가 등장한다.**\n' +
      '\n' +
      '이 정의는 LLM을 해석하는 것과 설명을 생성하기 위해 LLM을 사용하는 것 모두에 적용된다. 더욱이, 정의는 _relevant_ 지식, 즉 특정 문제 및 청중에게 유용한 지식의 추출에 의존한다. 예를 들어, 코드 생성 컨텍스트에서, 관련 해석은 사용자가 LLM-생성 코드 스니펫을 신속하게 통합하는 것을 도울 수 있다. 이와 달리, 의료 진단 설정에서의 관련 해석은 사용자에게 예측이 신뢰할 수 있는지 여부를 알려줄 수 있다.\n' +
      '\n' +
      'LLM(large language model)_이라는 용어는 종종 부정확하게 사용된다. 여기서는 PaLM[24], LLaMA[12], GPT-4[13]와 같은 방대한 텍스트 데이터에 대해 미리 훈련된 수십억에서 수천억 개의 매개 변수를 포함하는 변압기 기반 신경 언어 모델을 참조하기 위해 사용한다. BERT와 같은 초기 사전 훈련된 언어 모델에 비해 LLM은 훨씬 클 뿐만 아니라 더 강한 언어 이해, 생성 능력 및 설명 능력을 나타낸다. 초기 계산 집약적인 사전-트레이닝 단계 후에, LLM은 종종 [25] 이후의 명령어를 개선하거나 대화식 채팅 능력, 예를 들어 LLaMA-2 채팅 모델[12]을 개선하기 위해 명령어 미세조정 및 인간 선호도와의 추가 정렬을 겪는다. 그들은 때때로 의학[26]과 같은 특정 영역에서 성능을 향상시키기 위해 감독된 미세 조정을 통해 추가로 적응된다.\n' +
      '\n' +
      '이러한 단계를 거친 후 LLMs는 LLMs를 적용하기 위한 가장 일반적인 인터페이스인 _prompting_와 함께 자주 사용된다(그리고 본 논문에서는 우리의 주요 초점). 프롬프트에서 텍스트 프롬프트가 LLM에 직접 공급되어 후속 출력 텍스트를 생성하는 데 사용됩니다. _ Few-shot prompting_는 LLM이 수행하도록 요청되고 있는 태스크를 더 잘 이해할 수 있도록 소수의 예를 갖는 LLM을 제공하는 것을 수반하는 프롬프트의 유형이다.\n' +
      '\n' +
      'LLM 해석을 평가하는 것은 다른 해석이 다른 맥락과 관련이 있기 때문에 해석을 평가하는 이상적인 방법은 인간이 있는 실제 환경에서 사용이 원하는 결과를 개선하는지 여부를 연구하는 것이다[27]. 대조적으로, 설명의 인간 판단을 단순히 측정하는 것은 실제로 개선으로 변환되지 않을 수 있기 때문에 특별히 유용하지 않다[28]. 최근 메타 분석에 따르면 NLP 설명을 인간이 있는 환경에 도입하면 완전히 도움이 되지 않는 것부터 매우 유용한 것까지 광범위하게 다양한 유틸리티가 산출된다. 이 평가의 중요한 부분은 상보성[30]의 개념이며, 즉 설명은 LLMs가 고립 상태에서 성능을 개선하기보다는 팀 환경에서 인간 성능을 보완하는 데 도움이 되어야 한다는 것이다.\n' +
      '\n' +
      '인간 연구가 가장 현실적인 평가를 제공하는 반면, 자동화된 메트릭(인간을 포함하지 않고 계산될 수 있음)은 특히 기계론적 해석 가능성에서 평가를 용이하게 하고 확장하는 것이 바람직하다. LLM이 자신의 산출물을 너무 긍정적으로 체계적으로 채점하는 것과 같은 편향을 도입하지 않도록 큰 주의를 기울여야 하지만 LLM 자체를 평가에 사용하는 것이 점점 더 인기 있는 접근법이다[31]. 편향을 줄이는 한 가지 방법은 평가 점수에 대해 LLMs를 직접 쿼리하는 것이 아니라 특정 문제에 맞춘 구조화된 평가 프로세스의 일부로 LLMs를 사용하는 것이다. 예를 들어, 하나의 공통 설정은 (미리 훈련된 LLM의 임의의 컴포넌트일 수 있는) 주어진 함수의 자연 언어 해석을 평가하는 것이다. 이 설정에서 함수의 동작을 시뮬레이션하는 설명[32], LLM 생성 합성 데이터에 대한 함수의 출력[33] 또는 근거 함수 복원 능력[34, 35]을 평가할 수 있다. 질문 응답 환경에서, 질문에 대한 개별 답변에 대한 자연 언어 설명의 충실성을 측정하기 위한 많은 자동화된 메트릭이 제안되었다[36, 37, 38].\n' +
      '\n' +
      '해석을 평가하기 위한 최종 방법은 유용한 방식으로 모델 성능을 변경/개선하는 능력을 통한 것이다. 이 접근법은 해석의 모든 중요한 사용 사례(특히 인간 상호 작용을 직접적으로 포함하는 경우)를 포괄하지는 않지만 설명의 유용성에 대한 강력한 증거를 제공한다. 모델 개선은 다양한 형태를 취할 수 있으며, 그 중 가장 간단한 것은 단순히 다운스트림 작업에서 정확도를 향상시키는 것이다. 예를 들어, LLM의 이론적 근거를 사후 설명 방법[39] 또는 대형 모델에서 증류된 설명과 정렬할 때 몇 가지 샷 정확도가 향상되는 것으로 나타났다. 더욱이, 추론(훈련이 아님) 동안 소수의 샷 설명을 사용하는 것은 특히 이러한 설명이 더 최적화될 때 소수의 샷 LLM 정확도를 상당히 향상시킬 수 있다[41, 42]. 일반적인 성능을 넘어 모델의 구체적인 단점을 극복하기 위해 설명이 사용될 수 있다. 예를 들어, 하나의 작업 라인은 LLM[43, 44, 45]에 의해 학습된 바로가기/가짜 상관 관계를 식별하고 해결한다. 관련 작업 라인인 모델 편집은 특정 모델 동작에 대한 정밀한 수정을 가능하게 하여 전반적인 성능을 향상시킨다[46, 47, 48].\n' +
      '\n' +
      '##3 LLM 해석의 독특한 기회와 과제\n' +
      '\n' +
      'LLM 해석 기회 중 LLM 해석의 독특한 기회는 복잡한 패턴을 설명하는 _a 자연어 인터페이스를 제공하는 능력이다. 이 인터페이스는 인간에게 매우 친숙하여 실무자들이 설명 가능성 기술을 사용할 때 종종 겪는 어려움을 잠재적으로 개선한다[50, 49]. 또한, 자연 언어는 인간과 DNA, 화학 화합물 또는 이미지[51, 52, 53]와 같은 다양한 다른 양식 사이의 다리를 구축하는 데 사용될 수 있으며, 이는 인간이 스스로 해석하기 어려울 수 있다. 이러한 경우, 자연 언어는 다양한 수준의 세분화에서 설명을 통해 복잡한 개념을 표현할 수 있도록 하며, 잠재적으로 증거 또는 반사실의 논의에 근거한다.\n' +
      '\n' +
      '두 번째 주요 기회는 LLM이 _상호작용 설명을 생성하는 능력이다. 상호 작용을 통해 사용자는 예를 들어 후속 질문을 하고 관련 예제에 대한 분석을 수행함으로써 고유한 요구에 맞게 설명을 조정할 수 있다. 의사 및 정책 입안자를 포함한 의사 결정자와의 인터뷰는 특히 자연 언어 대화의 형태로 대화형 설명을 강력하게 선호한다는 것을 나타낸다. 상호 작용성은 LLM 설명이 많은 다른 LLM 호출로 분해되도록 추가로 허용하며, 각각은 독립적으로 감사될 수 있다. 이는, 예를 들어, 사용자가 프롬프트를 사용하여 LLM과 반복적으로 채팅하게 하거나, 사용자에게 분석할 LLM 호출 및 증거의 시퀀스를 제공하는 것과 같은 상이한 방식으로 인에이블될 수 있다.\n' +
      '\n' +
      'LLM 해석의 독특한 도전, 이러한 기회는 새로운 도전을 가져온다. 무엇보다도 _hallucination_의 문제, 즉 부정확하거나 근거 없는 설명이다. 자연어로 제공되는 유연한 설명은 증거가 주어진 입력에 존재하는지 또는 LLM이 훈련 데이터에서 학습한 지식에 존재하는 것으로 추정되는지 여부에 관계없이 빠르게 증거에 덜 근거할 수 있다. 환각된 설명은 도움이 되지 않거나 심지어 오해의 소지가 있으므로 환각을 식별하고 퇴치하는 기술은 LLM 해석의 성공에 중요하다.\n' +
      '\n' +
      '두 번째 도전은 LLM의 _면역성 및 불투명성_이다. 모델은 수백억 또는 수천억 개의 매개 변수를 포함하도록 성장했으며[11, 12], 크기가 계속 커지고 있다. 이것은 인간이 LLM의 단위를 조사하거나 심지어 이해하는 것을 불가능하게 만든다. 또한, LLM으로부터 단일 토큰을 생성하는 것조차도 종종 사소한 계산 비용이 발생하기 때문에 해석을 위한 효율적인 알고리즘이 필요하다. 사실, LLM들은 종종 로컬로 실행되기에는 너무 크거나 독점적인 텍스트 API를 통해서만 액세스될 수 있어, 모델에 대한 완전한 액세스(예를 들어, 모델 가중치들 또는 모델 구배들에 대한 액세스 없음)를 갖지 않는 해석 알고리즘들의 필요성을 필요로 한다.\n' +
      '\n' +
      '##4 LLM 설명\n' +
      '\n' +
      '이 절에서는 LLM(Sec. 4.1) 또는 LLM 전체(Sec. 4.2)에서 단일 세대를 설명하는 것을 포함하여 LLM을 설명하는 기술을 연구한다. 우리는 LLM을 설명하기 위해 전통적인 해석 가능한 ML 기술과 LLM 기반 기술을 모두 평가한다.\n' +
      '\n' +
      '### Local explanation\n' +
      '\n' +
      '로컬 설명, 즉 LLM에서 단일 세대를 설명하는 것은 최근 해석 가능성 문헌에서 주요 초점이 되었다. 의료와 같은 고위험 시나리오에서 LLM을 이해하고 사용할 수 있습니다.\n' +
      '\n' +
      'LLM에서 로컬 설명을 제공하기 위한 가장 간단한 접근법은 입력 토큰에 대한 특징 속성을 제공한다. 이러한 특성 속성은 모델의 생성된 출력에 대한 영향을 반영하여 각 입력 특성에 관련성 점수를 할당한다. 섭동 기반 방법[6], 기울기 기반 방법[55, 56], 선형 근사[5] 등 다양한 속성 방법이 개발되었다. 최근 이러한 방법은 이산 토큰 임베딩[57, 58] 및 계산 비용[59]과 같은 고유한 문제를 해결하면서 변압기 모델에 특별히 적용되었다. 더욱이, LLM에 의해 학습된 조건부 분포는 입력 주변화를 수행함으로써 기존의 속성 방법을 향상시키는 데 사용될 수 있다[60]. 특징 속성 외에도 LLM 내의 주의 메커니즘은 LLM 세대[61]에 대한 토큰 기여를 시각화할 수 있는 또 다른 방법을 제공하지만 충실성/효과성은 불분명하다[62]. 흥미롭게도 최근 연구는 LLM 자체가 프롬프트를 통해 중요한 기능의 사후 귀인을 생성할 수 있음을 시사한다[63]. 이 접근법은 서로 다른 맥락에서 관련된 서로 다른 특징 속성을 이끌어낼 수 있도록 확장될 수 있다.\n' +
      '\n' +
      '토큰 수준의 속성 외에도 LLM은 자연어로 직접 현지 설명을 생성할 수도 있다. 자연어 설명의 생성은 LLM의 현재 시대(예: 텍스트 분류[64, 65] 또는 이미지 분류[66])보다 이전이지만, 보다 강력한 모델의 출현은 그 효과를 크게 향상시켰다. LLM에 의해 생성된 자연 언어 설명은 모델 예측을 설명하는 능력, 심지어 반사실적 시나리오[67]를 시뮬레이션하고 불확실성과 같은 뉘앙스를 표현하는 능력을 보여주었다[68, 69, 70]. 잠재적인 이점에도 불구하고 자연 언어 설명은 특히 사후에 생성될 때 환각이나 부정확성에 매우 취약하다[71, 72].\n' +
      '\n' +
      '이러한 환각을 퇴치하기 위한 하나의 출발점은 해답 생성 과정 자체에 대한 설명을 통합하는 것이다. 생각 사슬 프롬프트는 LLM이 해답에 도착하기 전에 그 추론을 단계별로 명확히 하도록 프롬프트되는 이 접근[73]을 예시한다. 이 추론 체인은 일반적으로 최종 답변이 이전의 논리적 단계와 더 일치하기 때문에 더 정확하고 충실한 결과를 초래한다. 이 방법의 견고성은 추론 과정에서 섭동을 도입하고 최종 출력에 미치는 영향을 관찰함으로써 테스트할 수 있다[74, 75, 76]. 이 추론 체인을 생성하기 위한 대안적인 방법들, 예를 들어, 역추적, 생각 그래프[78] 및 다른 것들[79, 80, 81]과 함께 사용되는 생각들의 트리를 생성하기 위해 생각 사슬을 확장하는 생각 트리[77]가 존재한다. 이러한 모든 방법은 LLM의 중간 추론을 사용자에게 전달하는 데 도움이 될 뿐만 아니라 프롬프트를 통해 LLM이 추론을 따르는 데 도움이 되어 종종 출력의 신뢰성을 향상시킨다. 그러나 모든 LLM 기반 세대와 마찬가지로 이러한 설명의 충실도는 다양할 수 있다[76, 82].\n' +
      '\n' +
      '세대 동안 환각을 줄이기 위한 대안적인 경로는 검색 증강 세대(RAG)를 사용하는 것이다. RAG에서 LLM은 의사 결정 과정에서 검색 단계를 통합하는데, 일반적으로 텍스트 임베딩[83, 84]을 사용하여 참조 코퍼스 또는 지식 베이스를 검색한다(검토[85] 참조). 이를 통해 출력을 생성하는 데 사용되는 정보를 명시적으로 지정하고 조사할 수 있으므로 LLM이 의사 결정 중에 사용하는 증거를 더 쉽게 설명할 수 있다.\n' +
      '\n' +
      '### Global and mechanistic explanation\n' +
      '\n' +
      '개별 세대를 연구하는 것보다 글로벌/기계론적 설명은 LLM 전체를 이해하는 것을 목표로 한다. 이러한 설명은 일반화, 예를 들어 편향성, 프라이버시 및 안전을 넘어서는 우려에 대한 모델을 감사하는 데 도움이 될 수 있으며, 보다 효율적이고 신뢰할 수 있는 LLM을 구축하는 데 도움이 되며, LLM이 어떻게 기능하는지에 대한 기계론적 이해도 얻을 수 있다. 이를 위해 연구자들은 다양한 렌즈를 통해 LLM의 행동과 메커니즘을 요약하는 데 중점을 두었다. 일반적으로 이러한 작업은 모델 가중치에 대한 액세스가 필요하며 텍스트 API, 예를 들어 GPT-4[13]을 통해서만 액세스할 수 있는 모델을 설명하는 데 작동하지 않는다.\n' +
      '\n' +
      '신경망 표현을 이해하기 위한 한 가지 인기 있는 방법은 프로빙이다. 프로빙 기법들은 임베딩된 정보, 예를 들어 구문[86]을 디코딩하거나, 또는 주제-동사 합의[87, 88]와 같은 정교하게 설계된 태스크들을 통해 모델의 능력을 테스트함으로써 모델의 표현을 분석한다. LLM의 맥락에서, 프로빙은 주의 헤드[89], 임베딩[90] 및 표현[91]의 다양한 제어 가능한 측면의 분석을 포함하도록 진화했다. 또한 출력 토큰을 직접 디코딩하여 서로 다른 위치 및 계층에서 표현되는 것을 이해하는 방법을 포함한다[92, 93]. 이러한 방법은 LLMs가 정보를 처리하고 표현하는 미묘한 방법에 대한 더 깊은 이해를 제공할 수 있다.\n' +
      '\n' +
      '프로빙 외에도 많은 작업이 보다 세분화된 수준에서 LLM 표현을 연구한다. 여기에는 개별 뉴런에서 개념을 분류하거나 해독하거나 자연 언어로 주의 머리 기능을 직접 설명하는 것이 포함된다[32, 33, 96]. 개별 뉴런을 넘어, 뉴런 그룹이 어떻게 결합하여 특정 작업을 수행하는지 이해하는 데 관심이 증가하고 있는데, 예를 들어 간접 객체 식별[97], 개체 결합[98] 또는 다중 공유 목적[99]을 위한 회로를 찾는 것이다. 보다 광범위하게, 이러한 유형의 분석은 LLM[100, 46] 내에서 사실적 지식을 국소화하는 것과 같은 회로를 완전히 설명하기보다는 기능을 국소화하는 데 적용될 수 있다. 이러한 방법의 지속적인 문제는 엄청난 LLMs로 확장하기 어렵다는 것이며, 이는 오늘날 가장 큰 LLMs[101, 102]로 확장할 수 있는 (반)자동화 방법의 연구로 이어진다.\n' +
      '\n' +
      '기계론적 이해에 대한 보완적 접근은 복잡한 현상을 조사하기 위한 테스트 베드로 축소 LLM을 사용한다. 예를 들어, 2-계층 변압기 모델을 검사하는 것은 입력 통계의 함수로서 주의 헤드들에 의해 어떤 패턴들이 학습되는지에 대한 정보를 드러내거나[103] 관련 토큰들을 복사하고 활용하는 유도 헤드들 또는 ngram 헤드들과 같은 주요 컴포넌트들을 식별하는 것을 돕는다[104, 105]. 이러한 기계론적 이해의 라인은 상황 내 학습의 중요한 능력, 즉 프롬프트에서 몇 가지 입력-출력 예가 주어지면, LLM은 새로운 입력에 대한 출력을 올바르게 생성하는 것을 학습할 수 있다[106, 107].\n' +
      '\n' +
      '관련 연구 영역은 LLM의 학습 데이터 분포의 영향을 이해하여 LLM을 해석하고자 한다. 우리가 논의한 다른 방법과 달리, 이것은 종종 알려지지 않았거나 접근할 수 없는 LLM의 훈련 데이터 세트에 대한 액세스를 요구한다. 데이터가 알려져 있는 경우 연구자는 영향 함수 등의 기술을 사용하여 학습 데이터에서 중요한 요소를 식별할 수 있다[108]. 그들은 또한 롱테일 데이터[109]가 있는 경우 환각, 반복된 학습 데이터[110]가 있는 경우 또는 적절한 추론[111]과 모순되는 통계적 패턴과 같은 학습 데이터의 패턴에서 모델 행동이 어떻게 발생하는지 연구할 수 있다.\n' +
      '\n' +
      '이러한 모든 해석 기술은 LLM 기반 상호작용성을 통해 개선될 수 있어, 사용자가 후속 쿼리 및 변경된 프롬프트를 통해 상이한 모델 구성요소를 조사할 수 있게 한다. 예를 들어, 최근 한 연구는 설명 기반 디버깅 및 텍스트 모델의 개선을 위한 종단간 프레임워크를 도입하여 텍스트 분류 성능의 개선을 신속하게 산출할 수 있음을 보여준다[112]. 또 다른 작업인 Talk2Model은 사용자가 대화 상자를 통해 표 모양의 예측 모델을 질문할 수 있는 자연 언어 인터페이스를 도입하여 특징 중요도 계산과 같은 다양한 모델 설명 도구를 암묵적으로 호출한다[113].1 보다 최근의 작업은 Talk2Model을 LLM이 자신의 행동에 대해 질문하는 설정으로 확장한다[114].\n' +
      '\n' +
      '각주 1: Talk2Model은 LLMs보다는 예측모형 해석에 초점을 맞추고 있음에 유의한다.\n' +
      '\n' +
      '마지막으로, 기계론적 이해에서 얻은 통찰력은 모델 편집[46], 다음 [115] 및 모델 압축[116]을 포함한 현재 초점 영역으로 실제 응용 프로그램을 알리기 시작했다. 이러한 영역은 많은 기계론적 해석에 대한 정상 확인과 LLM의 신뢰성을 향상시키는 유용한 경로 역할을 동시에 한다.\n' +
      '\n' +
      '##5 데이터셋 설명\n' +
      '\n' +
      'LLM은 컨텍스트 길이와 기능을 개선하므로 LLM 또는 그 세대를 설명하기보다는 전체 데이터 세트를 설명하는 데 활용할 수 있다. 이는 데이터 분석, 지식 발견 및 과학적 응용 프로그램에 도움이 될 수 있습니다. 도. 도 2는 우리가 아래에서 자세히 다루는 다양한 수준의 세분화에서 데이터세트 설명의 개요를 보여준다. 우리는 표와 텍스트 데이터를 구별하지만 대부분의 방법이 멀티모달 설정에서 둘 중 하나 또는 둘 모두에 성공적으로 적용될 수 있다는 점에 유의한다.\n' +
      '\n' +
      '표형 데이터 LLM이 데이터 세트 설명에 도움이 될 수 있는 한 가지 방법은 표형 데이터를 대화식으로 시각화하고 분석하는 것을 더 쉽게 만드는 것이다. 이는 LLM이 모두 입력 토큰으로 취급하여 코드, 텍스트, 숫자를 동시에 이해할 수 있다는 사실에 의해 가능하게 된다. 아마도 이 범주에서 가장 인기 있는 방법은 대화형 텍스트 인터페이스를 통해 데이터 세트를 업로드하고 그 위에 시각화를 구축할 수 있는 ChatGPT 코드 인터프리터3일 것이다. 이 기능은 LLM 지원 시각화의 광범위한 경향의 일부이며, 예를 들어 데이터 프레임에 대한 자동 시각화를 제안하거나[117], 데이터 논쟁을 자동화하거나[118], 심지어 본격적인 데이터 분석을 수행하는 데 도움이 된다[119]. 이러한 기능은 LLMs[120, 121, 122]로 표 형식의 데이터를 효과적으로 표현하고 처리하는 방법을 분석하는 증가하는 작업 라인에서 이익을 얻습니다.\n' +
      '\n' +
      'LLM은 또한 표형 데이터에 적합했던 모델을 직접 분석함으로써 데이터 세트를 설명하는 데 도움이 될 수 있다. 모델을 이해하는 것이 목표인 기계론적 해석 가능성과 달리 데이터 세트 설명에서는 모델을 통해 데이터의 패턴을 이해하는 것이 목표이다(두 문제 모두에 유사한 기술이 사용될 수 있지만). 예를 들어, 최근 한 작업은 LLM을 사용하여 표 데이터에 적합한 일반화된 가법 모형(GAM)을 분석합니다[123]. GAM은 곡선의 집합으로 나타낼 수 있는 해석 가능한 모델이며, 각각은 특징의 값의 함수로서 출력 예측에 대한 특징의 기여도를 나타낸다. LLM은 각 곡선을 수치 토큰의 집합으로 처리한 다음 각 곡선에서 패턴을 감지하고 설명함으로써 적합 모델(및 이에 따라 기본 데이터 세트)을 분석할 수 있다. 저자는 LLM이 주로 도메인에 대한 사전 지식을 기반으로 곡선과 기본 데이터에서 놀라운 특성을 식별할 수 있음을 발견했다. 해석 가능한 GAM 모델을 사용하기보다는 분류기 예측을 분석하여 데이터 세트 통찰력을 증류하는 또 다른 접근법이다. 예를 들어, MaNtLE는 분류기의 예측에 기초하여 분류기의 근거에 대한 자연 언어 설명을 생성하고, 이러한 설명은 유사한 특징 패턴을 포함하는 설명 가능한 하위 그룹을 식별하는 것으로 발견된다[124].\n' +
      '\n' +
      '각주 3: [https://openai.com/blog/chatgpt-plugins#code-interpreter](https://openai.com/blog/chatgpt-plugins#code-interpreter)\n' +
      '\n' +
      '텍스트 데이터 텍스트 데이터는 희소하고 고차원적이며 모델링하려면 많은 고차 상호 작용이 필요하기 때문에 테이블 데이터와 데이터 세트 설명에 대해 다른 문제를 제기한다. 그 결과, 표 영역에서 성공한 해석 가능한 모델(예: 희소 선형 모델[125, 126], GAM[127, 128, 129], 결정 트리[130, 131, 132] 및 기타 [133, 134])은 텍스트를 정확하게 모델링하기 위해 고군분투했다. 최근 한 작업 라인은 선형 모델 또는 결정 트리[135]와 같이 완전히 해석할 수 있는 텍스트 모델을 구축하는 데 도움이 되도록 LLM을 사용하여 이 문제를 해결한다. 결과 모델은 놀랍게도 정확하며 종종 훨씬 더 큰 LLM 모델보다 성능이 우수하다. 이러한 해석 가능한 모델은 어떤 특징(즉, 단어 또는 ngram)이 다른 결과를 예측하는 데 중요한지 보여줌으로써 데이터 세트를 설명하는 데 도움이 될 수 있다. 유사한 방법들, 예를 들어, CHiLL [136]은 텍스트 분류 태스크들에 대한 해석가능한 표현들을 구축하기 위해 LLM들을 사용한다.\n' +
      '\n' +
      '완전히 해석할 수 있는 모델을 넘어 LLM은 부분적으로 해석할 수 있는 텍스트 모델을 구축하는 데에도 도움이 된다. 부분적으로 해석 가능한 텍스트 모델은 종종 프롬프트의 체인을 사용합니다. 이러한 체인은 LLM의 의사 결정 프로세스를 분해하여 모델이 학습하는 데이터 세트 패턴을 분석할 수 있습니다. 프롬프트 체인은 보통 인간에 의해 또는 모델에 질의하여 온 더 플라이(on-the-fly) 호출 체인을 생성함으로써 구성된다[137]. 데이터세트 설명의 경우 가장 관련성이 높은 사슬은 LLM에 의해 생성되는 설명 시퀀스이다. 예를 들어, 모델은 데이터세트 내의 모든 예들에 걸쳐 공유되는 설명들의 단일 트리를 생성할 수 있으며, 프로세스는 데이터세트 내에 저장된 계층적 구조들을 이해할 수 있게 한다[138]. 트리보다는 프롬프트의 단일 체인은 종종 LLM이 자기 검증을 사용하는 것을 도울 수 있는데, 즉 모델 자체는 종종 신뢰성을 향상시키는 인기 있는 기법인 프롬프트의 체인을 사용하여 이전 세대들을 확인한다[139, 140, 141]. 로컬 설명에서와 같이, LLM은 그 의사 결정 프로세스에 검색 단계를 통합할 수 있고[85], 상이한 도구에 대한 액세스는 상이한 단계들(예를 들어, 산술)을 더 신뢰할 수 있고 투명하게 만드는 것을 도울 수 있다[142].\n' +
      '\n' +
      '자연 언어 설명은 데이터 세트에 존재하는 패턴에 대한 풍부하고 간결한 설명을 생성할 가능성을 가지고 있지만 환각을 일으키기 쉽다. 하나의 방법, iPrompt[143]은 단일 프롬프트의 형태로 데이터세트 설명을 검색하고, 프롬프트가 LLM이 기본 데이터세트 내의 패턴을 정확하게 예측하도록 유도한다는 것을 검증함으로써 환각을 회피하는 것을 목표로 한다. 관련 방법은 LLM을 사용하여 데이터 세트에서 그룹 간을 구별하는 설명을 제공하고, 이어서 설명의 신뢰성을 검증하는 LLM이 이어진다[144, 145, 35]. 원시 자연 언어 설명 외에도 LLM은 예를 들어 텍스트 데이터 세트의 설명 가능한 클러스터링[146] 또는 프롬프트 기반 토픽 모델 생성[147]을 통해 텍스트 정보를 요약하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '##6 미래 우선순위\n' +
      '\n' +
      '이제 우리는 설명 신뢰도, 데이터 세트 설명 및 대화형 설명의 세 가지 영역에서 LLM 해석을 둘러싼 연구 우선 순위를 강조한다.\n' +
      '\n' +
      '설명 신뢰도 모든 LLM 설명은 신뢰도 문제에 의한 병목 현상이다. 이것은 환각[148]을 포함하지만, 더 광범위한 문제들을 포함한다. 예를 들어, LLM은 프롬프트 표현의 뉘앙스에 계속 매우 민감하며, 프롬프트의 사소한 변형은 LLM 출력의 실체를 완전히 변경할 수 있다[149, 150]. 추가로, LLMsmay는 그들의 컨텍스트의 부분들, 예를 들어 긴 컨텍스트들(151)의 중간 또는 파싱하기 어려운 명령어들(115)을 무시한다.\n' +
      '\n' +
      '이러한 신뢰성 문제는 해석에서 특히 중요하며, 종종 높은 지분 설정에서 위험을 완화하기 위해 설명을 사용한다. 설명을 분석하는 한 작업은 LLM이 관련 질문에 대한 자체 출력[71]과 실제로 일치하지 않는 겉보기에 올바른 설명을 생성하는 경우가 많다는 것을 안정적으로 발견하여 인간 실무자가 LLM을 신뢰하거나 설명이 새로운 시나리오에 어떻게 적용되는지 이해하는 것을 방지한다. 또 다른 연구에 따르면 LLM에 의해 생성된 설명은 추출 설명이 있는 간단한 작업에서도 모델의 예측을 수반하지 않거나 입력에 사실적으로 근거할 수 있다[72]. 향후 연구는 설명의 근거를 개선하고 자기 검증[139], 반복 프롬프트[143] 또는 모델 자기 일치성[152, 153, 154]과 같은 방법을 통해 신뢰성을 테스트하기 위한 더 강력한 방법을 개발하는 것이 필요할 것이다.\n' +
      '\n' +
      '지식 발견을 위한 데이터세트 설명 LLMs를 이용한 데이터세트 설명(Sec. 5) 단순히 데이터 분석이나 시각화의 속도를 높이는 데 도움이 되기보다는 데이터[17, 22, 23]에서 새로운 지식의 생성과 발견을 도울 수 있는 잠재력을 보유한다. 데이터셋 설명은 처음에 인간 연구자들에 의해 스크리닝되거나 테스트될 수 있는 과학적 가설을 브레인스토밍하는 수준에서 도움이 될 수 있다[155]. 이 과정 중과 후에 LLM 설명은 자연어를 사용하여 화합물 [156] 또는 DNA 서열 [51]과 같은 불투명한 도메인의 데이터를 이해하는 데 도움이 될 수 있다. 알고리즘 영역에서 LLM은 새로운 알고리즘을 발견하는 데 사용되어 판독 가능한 컴퓨터 프로그램으로 인간에게 번역되었다[157]. 이러한 접근법은 실험의 데이터와 결합하여 새로운 데이터 기반 통찰력을 산출하는 데 도움이 될 수 있다.\n' +
      '\n' +
      'LLM 설명은 또한 인간이 작업을 더 잘 수행할 수 있도록 돕기 위해 사용될 수 있다. 변압기로부터의 설명은 이미 체스와 같은 도메인에 적용되기 시작했는데, 여기서 그들의 설명은 심지어 전문가 플레이어들을 개선하는 데 도움이 될 수 있다[158]. 또한 LLM은 인간의 행동을 이해하고 감사하고 개선하는 데 도움이 되는 "의사가 환자에 대한 이 정보를 제공한 이 약을 처방한 이유"와 같은 전문가의 인간 행동에 대한 설명을 제공할 수 있다[159].\n' +
      '\n' +
      '마지막으로 대화형 설명, LLM의 발전은 보다 사용자 중심의 대화형 설명을 개발할 수 있도록 준비가 되어 있다. LLM 설명 및 후속 질문은 대화형 작업 사양[160], 추천[161] 및 대화와 관련된 광범위한 작업 집합과 같은 다양한 LLM 응용 프로그램에 이미 통합되고 있다. 또한, Talk2Model[113]과 같은 작업을 통해 사용자는 대화식으로 모델을 감사할 수 있습니다. 이 대화 인터페이스는 대화형 데이터 세트 설명과 같은 새로운 응용 프로그램을 돕기 위해 이 작업에서 다루는 많은 방법과 함께 사용할 수 있다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '이 논문에서 우리는 특히 LLM이 제시한 독특한 기회와 도전에 초점을 맞춰 해석 가능한 ML의 광대하고 역동적인 환경을 탐구했다. LLM의 고급 자연 언어 생성 기능은 더 정교하고 미묘한 설명을 생성하기 위한 새로운 길을 열어 데이터 및 모델 행동에서 복잡한 패턴에 대한 더 깊고 접근 가능한 이해를 가능하게 했다. 이 지형을 탐색하면서 LLM을 해석 프로세스에 통합하는 것은 기존 방법론의 향상만이 아니라 기계 학습 해석 가능성의 경계를 재정의할 것을 약속하는 변형적 이동이라고 주장한다.\n' +
      '\n' +
      '우리의 입장은 해석 가능한 ML의 미래가 LLM의 잠재력을 최대한 활용하는 능력에 달려 있다는 믿음에 고정되어 있다. 이를 위해, 우리는 설명 신뢰도를 높이고 지식 발견을 위한 데이터 세트 해석을 발전시키는 것과 같은 향후 연구를 위한 몇 가지 주요 위치와 방향을 설명했다. LLM이 빠르게 계속 개선됨에 따라 이러한 설명(및 논의된 모든 방법)\n' +
      '\n' +
      '도 2: 상이한 레벨의 입상도에서의 **데이터세트 설명.**데이터세트 설명은 미리 트레이닝된 LLM을 사용하여 새로운 데이터세트(텍스트 또는 표상의 특징으로 구성됨)를 이해하는 것을 포함한다. 낮은 수준의 설명은 데이터 세트에 더 충실하지만 의미 있는 통찰력을 추출하기 위한 더 많은 인간의 노력을 포함한다. 많은 데이터 세트 해석은 특징 간의 패턴을 식별하고 설명하기 위한 수단으로 예측 모델(분류 또는 회귀)을 사용한다.\n' +
      '\n' +
      '이 작업에서) 새로운 애플리케이션과 통찰력을 가능하게 하기 위해 상응하게 발전할 것이다. 가까운 미래에 LLM은 해석 가능성의 성배를 제공할 수 있을 것이다: 우리 모두에게 매우 복잡한 정보를 안정적으로 집계하고 전달할 수 있는 설명.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Finale Doshi-Velez and Been Kim. A roadmap for a rigorous science of interpretability. _arXiv preprint arXiv:1702.08608_, 2017. \\(\\sim\\)1.\n' +
      '* [2] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Aal, and Bin Yu. Definitions, methods, and applications in interpretable machine learning. _Proceedings of the National Academy of Sciences of the United States of America_, 116(44):22071-22080, 2019. \\(\\sim\\)2.\n' +
      '* [3] Christoph Molnar. _Interpretable machine learning_. Lulu. com, 2019. \\(\\sim\\)1.\n' +
      '* [4] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Interpretable machine learning: Fundamental principles and 10 grand challenges. _arXiv preprint arXiv:2103.11251_, 2021. \\(\\sim\\)1.\n' +
      '* [5] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions and classifier. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1135-1144. ACM, 2016. \\(\\sim\\)1.\n' +
      '* [6] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In _Advances in Neural Information Processing Systems_, pages 4768-4777, 2017. \\(\\sim\\)1 and 4\n' +
      '* [7] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. _arXiv preprint arXiv:1506.06579_, 2015. \\(\\sim\\)1.\n' +
      '* [8] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and Antonio Torralba. GAN dissection: Visualizing and understanding generative adversarial networks. _arXiv preprint arXiv:1811.10597_, 2018. \\(\\sim\\)1.\n' +
      '* [9] Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou. Distill-and-Compare: Auditing black-box models using transparent model distillation. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, pages 303-310, 2018. \\(\\sim\\)1.\n' +
      '* [10] Wooseok Ha, Chandan Singh, Francois Lannse, Sirgoikul Upadhyavula, and Bin Yu. Adaptive wavelet distillation from neural networks through interpretations. _Advances in Neural Information Processing Systems_, 34:20669-20682, 2021. \\(\\sim\\)1.\n' +
      '* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prufalla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020. \\(\\sim\\)1.\n' +
      '* [12] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahari, Yasmine Bahoei, Nikuloy Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhossale, et al. Llama 2: Open foundation and fine-tuned chut models. _arXiv preprint arXiv:2307.09288_, 2023. \\(\\sim\\)3 and 3\n' +
      '* [13] OpenAI. GPT-4 technical report, 2023. \\(\\sim\\)1., 3, and 5\n' +
      '* [14] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a" right to explanation". _arXiv preprint arXiv:1606.08813_, 2016. \\(\\sim\\)1.\n' +
      '* [15] Dario Amodel, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mand. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06555_, 2016. (Not cited)\n' +
      '* [16] Iason Gabriel. Artificial intelligence, values, and alignment. _Minds and machines_, 30(3):411-437, 2020. \\(\\sim\\)1.\n' +
      '* [17] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Lin, Payal Chandak, Shengchao Liu, Peter Van Katvyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 620(7972):47-60, 2023. \\(\\sim\\)1 and 7\n' +
      '* [18] Enkelejda Kasneci, Kathrin Soller, Stefan Koehmann, Maria Bannert, Daryna Demmetieux, Frank Fischer, Urs Gasser, Georg Groh, Stephan Gunnemann, Eyke Hullemeier, et al. ChatGPT for good? on opportunities and challenges of large language models for education. _Learning and individual differences_, 103:102272, 2023. (Not cited).\n' +
      '* [19] Cache Ziems, William Held, Omar Shaikh, Jiao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? _arXiv preprint arXiv:2305.03514_, 2023. \\(\\sim\\)1.\n' +
      '* [20] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huigi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. _arXiv preprint arXiv:2309.01029_, 2023. \\(\\sim\\)1.\n' +
      '* [21] Tilman Rauker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent AI: A survey on interpreting the inner structures of deep neural networks. In _2021 IEEE Conference on Secure and Transworth Machine Learning (SaTML)_, pages 464-483. IEEE, 2023. \\(\\sim\\)1.\n' +
      '* [22] Abeba Birhane, Atosona Kasirzadeh, David Leslie, and Sandra Wachter. Science in the age of large language models. _Nature Reviews Physics_, pages 1-4, 2023. \\(\\sim\\)1 and 7\n' +
      '* [23] Luca Pon-Tonchini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert, W Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Hauko Wainwright, Ghanshyam Pianian, Benjamin Nachman, et al. Learning from learning machines: a new generation of AI technology to meet the needs of science. _arXiv preprint arXiv:2111.13786_, 2021. \\(\\sim\\)1 and 7\n' +
      '* [24] Aakanksha Chowdhury, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaur Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Pal.M: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(20):1-113, 2023. \\(\\sim\\)3.\n' +
      '* [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Cheng Zhang, Sandini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022. \\(\\sim\\)3.\n' +
      '* [26] Karan Singhal, Tao Tu, Jorai Gottweis, Rory Sayres, Ellery Walczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlenne Neal, et al. Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_, 2023. \\(\\sim\\)3.\n' +
      '* [27] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernando Viegas, and Rory Sayres. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (teav). _arXiv preprint arXiv:1711.11279_, 2017. \\(\\sim\\)3.\n' +
      '* [28] Julius Adebayo, Justin Gilmer, Michael Moehlty, Ian Goodfellow, Moritz Hard, and Been Kim. Suitty checks for saliency maps. In _Advances in Neural Information Processing Systems_, pages 9505-9515, 2018. \\(\\sim\\)3.\n' +
      '* [29] Fateme Hashemi Chaleshotri, Atreya Ghosal, and Ana Marasovic. On evaluating explanation utility for Human-AI decision-making in NLP. In _NAI at Action: Trust, Present, and Future Applications_, 2023. \\(\\sim\\)3.\n' +
      '* [30] Gagan Bansal, Tonghuang Wu, Joyce Zhou, Raymond Fok, Besmin Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. Does the whole exceed its parts? the effect of AI explanations on complementary team performance. In _Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems_, pages 1-16, 2021. \\(\\sim\\)3.\n' +
      '* [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohuan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MFTbench and chattorid arena. In _Thirty-aeventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. \\(\\sim\\)3.\n' +
      '* [32] Steven Bills, Nick Cammrata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Golb, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models, 2023. \\(\\sim\\)3 and 5\n' +
      '* [33] Chandan Singh, Aliyah R Hsu, Richard Antonello, Shailee Jain, Alexander G Huth, Bin Yu, and Jianfeng Gao. Explaining black box text modules in natural language with language models. _arXiv preprint arXiv:2305.09863_, 2023. \\(\\sim\\)3 and * [34] Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torrallba. FIND: A function description benchmark for evaluating interpretability methods. _arXiv e-prints_, pages arXiv-2309, 2023. \\(\\dashdot\\)3.\n' +
      '* [35] Rusiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. _ArXiv_, abs/2302.14233, 2023. \\(\\dashdot\\)3 and 6.\n' +
      '* [36] Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Gure Simonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. _arXiv preprint arXiv:2305.18029_, 2023. \\(\\dashdot\\)3.\n' +
      '* [37] Leittia Practalbescu and Anette Frank. On measuring faithfulness of natural language explanations. _arXiv preprint arXiv:2311.07466_, 2023. (Not cited.)\n' +
      '* [38] Hanjie Chen, Facez Brahman, Xiang Ren, Yangfeng Bi, Yejin Choi, and Swohba Sugayama.fpas. Rev: information-theoretic evaluation of free-text rationales. _arXiv preprint arXiv:2210.04982_, 2022. \\(\\dashdot\\)3.\n' +
      '* [39] Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabulto Lakkaraju. Post hoc explanations of language models can improve language models. _arXiv preprint arXiv:2305.11426_, 2023. \\(\\dashdot\\)3.\n' +
      '* [40] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahai Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. _arXiv preprint arXiv:2306.02707_, 2023. \\(\\dashdot\\)3.\n' +
      '* [41] Andrew K Lampien, Ishita Dasgupta, Stephanie CY Chan, Kory Mathewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Feix Hill. Can language models learn from explanations in context? _arXiv preprint arXiv:2104.02329_, 2022. \\(\\dashdot\\)3.\n' +
      '* [42] Xi Ye and Greg Durrett. Explanation selection using unlabeled data for chain-of-thought prompting. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 619-637, 2023. \\(\\dashdot\\)3.\n' +
      '* [43] Mengnan Du, Fengxiang He, Na Zou, Ducheng Tao, and Xia Hu. Shortcut learning of large language models in natural language understanding. _Communications of the ACM (CACM)_, 2023. \\(\\dashdot\\)3.\n' +
      '* [44] Choongwoong Kang and Jaesik Choi. Impact of co-occurrence on factual knowledge of large language models. _arXiv preprint arXiv:2310.08256_, 2023. (Not cited.)\n' +
      '* [45] Isamjin Bastiangs, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Kajta Filippova. "Will you find these shortcuts?" a protocol for evaluating the faithfulness of input salience methods for text classification. _arXiv preprint arXiv:2111.07367_, 2021. \\(\\dashdot\\)3.\n' +
      '* [46] Kevin Meng, David Bau, Alex Andonian, and Yonatan Beilnikov. Locating and editing factual knowledge in GPT. _arXiv preprint arXiv:2202.05262_, 2022. \\(\\dashdot\\)3 and 5\n' +
      '* [47] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model editing at scale, 2022. (Not cited.)\n' +
      '* [48] Evan Hernandez, Belinda Z. Li, and Jacob Andreas. Inspecting and editing knowledge representations in language models, 2023. \\(\\dashdot\\)3.\n' +
      '* [49] Harmampreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. Interpreting interpretability: understanding data scientists\' use of interpretability tools for machine learning. In _Proceedings of the 2020 CHI conference on human factors in computing systems_, pages 1-14, 2020. \\(\\dashdot\\)3.\n' +
      '* [50] Daniel S Weld and Gagan Bansal. The challenge of crafting intelligible. _Communications of the ACM_, 62(6):70-79, 2019. \\(\\dashdot\\)3.\n' +
      '* [51] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scilom, Anthony Harshman, Elvis Saravia, Andrew Paulton, Viktor Kerzlez, and Robert Stojnic. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022. \\(\\dashdot\\)3 and 7\n' +
      '* [52] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Anima Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _ArXiv_, abs/2212.10789, 2022. (Not cited.)\n' +
      '* [53] Alce Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Ananda Asikel, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021. \\(\\dashdot\\)3.\n' +
      '* [54] Himabulto Lakkaraju, Dylan Slack, Yuxia Chen, Chenchao Tan, and Sameer Singh. Rethinking explainability as a dialogue: A practitioner\'s perspective. _arXiv preprint arXiv:2202.01875_, 2022. \\(\\dashdot\\)4.\n' +
      '* [55] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. _ICML_, 2017. \\(\\dashdot\\)4.\n' +
      '* [56] Gregoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Muller. Explaining nonlinear classification decisions with deep taylor decomposition. _Pattern Recognition_, 65:211-222, 2017. \\(\\dashdot\\)4.\n' +
      '* [57] Sandipan Sikidar, Parantapa Bhattacharya, and Kieran Heese. Integrated directional gradients: Feature interaction attribution for neural tlp models. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 865-878, 2021. \\(\\dashdot\\)4.\n' +
      '* [58] Joseph Enguehard. Sequential integrated gradients: a simple but effective method for explaining language models. _arXiv preprint arXiv:2305.15853_, 2023. \\(\\dashdot\\)4.\n' +
      '* [59] Hugh Chen, Ian C Covert, Scott M Lundberg, and Su-In Lee. Algorithms to estimate shapley value feature attributions. _Nature Machine Intelligence_, pages 1-12, 2023. \\(\\dashdot\\)4.\n' +
      '* [60] Siwon Kim, Jihun Yi, Eunji Kim, and Sungroh Yoon. Interpretation of tlp models through input imaginalization. _arXiv preprint arXiv:2010.13984_, 2020. \\(\\dashdot\\)4.\n' +
      '* [61] Sarah Wiegeffe and Yuxil Winter. Attention is not not explanation. _arXiv preprint arXiv:1908.04626_, 2019. \\(\\dashdot\\)4.\n' +
      '* [62] Sarthak Jain and Byron C Wallace. Attention is not explanation. _arXiv preprint arXiv:1902.10186_, 2019. \\(\\dashdot\\)4.\n' +
      '* [63] Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Ching Agarwal, and Himabulto Lakkaraju. Are large language models post hoc explainers? _arXiv preprint arXiv:2310.05797_, 2023. \\(\\dashdot\\)4.\n' +
      '* [64] Oana-Maria Camburu, Tim Rocktaschel, Thomas Lukasiewicz, and Phil Blumson. \\(\\dashdot\\)snti: Natural language inference with natural language explanations. _Advances in Neural Information Processing Systems_, 31, 2018. \\(\\dashdot\\)4.\n' +
      '* [65] Nazneen Fatema Rajani, Bryan McCann, Caining Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. _arXiv preprint arXiv:1906.02361_, 2019. \\(\\dashdot\\)4.\n' +
      '* [66] Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Berri Schiele, and Trevor Darrell. Generating visual explanations. In _European conference on computer vision_, pages 3-19. Springer, 2016. \\(\\dashdot\\)4.\n' +
      '* [67] Aritta Bhattacharjee, Raha Moraffah, Joshua Garland, and Huan Liu. LLMs as counterfactual explanation modules: Can chatGPT explain black-box text classifiers? _arXiv preprint arXiv:2309.13340_, 2023. \\(\\dashdot\\)4.\n' +
      '* [68] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. _arXiv preprint arXiv:2306.13063_, 2023. \\(\\dashdot\\)4.\n' +
      '* [69] Sree Harh Tannner, Ching Agrawal, and Himabulto Lakkaraju. Quantifying uncertainty in natural language explanations of large language models. _arXiv preprint arXiv:2311.03553_, 2023. (Not cited.)\n' +
      '* [70] Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. Relying on the unreliable: The impact of language models\' reluctance to express uncertainty, 2024. \\(\\dashdot\\)4.\n' +
      '* [71] Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and Kathleen McKeown. Do models explain themselves? Counterfactual simulability of natural language explanations. _arXiv preprint arXiv:2307.08678_, 2023. \\(\\dashdot\\)4 and 7\n' +
      '* [72] Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning. _Advances in neural information processing systems_, 35:30378-30392, 2022. \\(\\dashdot\\)4 and 7* [73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brain icther, Fei Xia, Ed H Chi, Quoc V Le, and Demy Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022, \\(\\dashrightarrow\\)4.\n' +
      '* [74] Aman Madan and Amir Yadadukahsh. Text and patterns: For effective chain of thought, it takes two to tango. _arXiv preprint arXiv:2209.07686_, 2022. \\(\\dashrightarrow\\)4.\n' +
      '* [75] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-though prompting: An empirical study of what matters. _arXiv preprint arXiv:2212.10001_, 2022. (Not cited.)\n' +
      '* [76] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson E. Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, John Kernton, Kamile Lakovaitte, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Sistreifer, Oliver Rausch, Robin Larson, Samuel McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, T. J. Henighan, Timothy D. Maxwell, Timothy Telleleman-Avova, Tristan Hume, Zae Hatfield-Dodds, Jared Kaplan, Janina Braumer, Sam Bowman, and Ethan Perez. Measuring faithfulness in chain-of-though reasoning. _ArXiv_, abs/2307.13702, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [77] Shuryu Yao, Dian Yu, Jeffrey Zhao, Llark Shafran, Thomas I. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [78] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasaz Lehmann, Michal Podstawski, Hubert Niewidomski, Piotr Nyczyk, et al. Graph of Thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [79] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henry Michalewski, Jacob Austin, David Bieber, David Dolan, Aitor Lewkowycz, Maarten Bosma, David Lian, Charles Sutton, and Augustus Odena. Show your work: Scratchgrads for intermediate computation with language models. _ArXiv_, abs/2112.00114, 2021. \\(\\dashrightarrow\\)4.\n' +
      '* [80] Oif Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2022. (Not cited.)\n' +
      '* [81] Demy Zhou, Nathamal Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022. \\(\\dashrightarrow\\)4.\n' +
      '* [82] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Demy Zhou, et al. Larger language models to in-context learning differently. _arXiv preprint arXiv:2303.0846_, 2023. \\(\\dashrightarrow\\)4.\n' +
      '* [83] Kelvin Guu, Kenton Lee, Zora Tung, Pampuong Pasquet, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. _ArXiv_, abs/2002.08909, 2020. \\(\\dashrightarrow\\)5.\n' +
      '* [84] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Quuyuan Huang, Lars Lien, Zhou Yu, Weiich Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback. _ArXiv_, abs/2302.12813, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [85] Theodora Worledge, Judy Hanwen Shen, Nicole Meister, Caleb Winston, and Carlos Guestrin. Unifying corroborative and contributive attributions in large language models. _arXiv preprint arXiv:2311.12233_, 2023. \\(\\dashrightarrow\\)5 and 6\n' +
      '* [86] Alexis Conneau, German Kuszewski, Guillaume Lample, Lok Barrault, and Marco Baroni. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. _arXiv preprint arXiv:1805.01070_, 2018. \\(\\dashrightarrow\\)5.\n' +
      '* [87] Frederick Liu and Besim Avei. Incorporating priors with feature attribution on text classification. _arXiv preprint arXiv:1906.08286_, 2019. \\(\\dashrightarrow\\)5.\n' +
      '* [88] Rebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. _arXiv preprint arXiv:1808.09031_, 2018. \\(\\dashrightarrow\\)5.\n' +
      '* [89] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does BERT look at? An analysis of bert\'s attention. _arXiv preprint arXiv:1906.04341_, 2019. \\(\\dashrightarrow\\)5.\n' +
      '* [90] John X Morris, Volodymyr Kuleshov, Vitaly Shmutikov, and Alexander M Rush. Text embeddings reveal (almost) as much as text. _arXiv preprint arXiv:2310.06816_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [91] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xawang Yin, Mantas Mazeika, Ann-Kahrin Dembrowski, Shishwatt Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basert, Samii Koyeib, Dawn Song, Martin Fredrikson, Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI transparency. _ArXiv_, abs/2310.01405, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [92] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. _arXiv preprint arXiv:2303.08112_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [93] Asma Ghandehariou, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. PatchScope: A unifying framework for inspecting hidden representations of language models, 2024. \\(\\dashrightarrow\\)5.\n' +
      '* [94] Jesse Ma and Jacob Andreas. Compositional explanations of neurons. _Advances in Neural Information Processing Systems_, 33:17153-17163, 2020. \\(\\dashrightarrow\\)5.\n' +
      '* [95] Wes Gurnee, Neil Nanda, Matthew Pauly, Katherine Harvey, Dmitri Troitski, and Dimitris Bertsimas. Finding neurons in a haystack: Case studies with sparse probing. _arXiv preprint arXiv:2305.01610_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [96] Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. Natural language descriptions of deep visual features. In _International Conference on Learning Representations_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [97] Kevin Wang, Alexandre Varienigen, Arthur Conmy, Buck Shleperis, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. _arXiv preprint arXiv:2211.00593_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [98] Jiahai Feng and Jacob Steinhardt. How do language models bind entities in context? _arXiv preprint arXiv:2310.17191_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [99] Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse across tasks in transformer language models. _arXiv preprint arXiv:2310.08744_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [100] Damani Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Frun Wei. Knowledge neurons in pretrained transformers. _arXiv preprint arXiv:2104.08696_, 2021. \\(\\dashrightarrow\\)5.\n' +
      '* [101] Tom Lieberum, Matthew Raluz, Janos Kamar, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? Evidence from multiple choice capabilities in chinchilla. _arXiv preprint arXiv:2307.09458_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [102] Zhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in Alpaca. _ArXiv_, abs/2305.08809, 2023. \\(\\dashrightarrow\\)5.\n' +
      '* [103] Nelson Ellhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1, 2021. \\(\\dashrightarrow\\)5.\n' +
      '* [104] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [105] Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms, 2024. \\(\\dashrightarrow\\)5.\n' +
      '* [106] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022. \\(\\dashrightarrow\\)5.\n' +
      '* [107] Hattie Zhou, Arwen Bradley, Eizi Litwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. _arXiv preprint arXiv:2310.16088_, 2023. \\(\\dashrightarrow\\)5.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '* [145] Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: A framework for explaining distribution shifts in natural language. _arXiv preprint arXiv:2206.15007_, 2022. \\(\\dashrightarrow\\)6.\n' +
      '* [146] Zihan Wang, Jingbo Shang, and Ruiqi Zhong. Goal-driven explainable clustering via language descriptions. _arXiv preprint arXiv:2305.13749_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [147] Chau Minh Pham, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. TopicGPT: A prompt-based topic modeling framework. _arXiv preprint arXiv:2311.01449_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [148] SM Tommoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadhu, and Amitava Das. A comprehensive survey of hallucination mitigation techniques in large language models. _arXiv preprint arXiv:2401.01313_, 2024. \\(\\dashrightarrow\\)6.\n' +
      '* [149] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\' sensitivity to spurious features in prompt design or. How i learned to start worrying about prompt formatting. _arXiv preprint arXiv:2310.11324_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [150] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language models don\'t always say what they think: Unfaithful explanations in chain-of-thought prompting. _arXiv preprint arXiv:2305.04388_, 2023. \\(\\dashrightarrow\\)6.\n' +
      '* [151] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michelle Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. _ArXiv_, abs/2307.03172, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [152] Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, and Jianfeng Gao. Towards consistent natural-language explanations via explanation-consistency through2024. \\(\\dashrightarrow\\)7.\n' +
      '* [153] Xiang Lisa Li, Vishnavi Shrivastava, Sayan Li, Titsmori Hashimoto, and Percy Liang. Benchmarking and improving generate-validator consistency of language models. _arXiv preprint arXiv:2310.01846_, 2023. (Not cited.)\n' +
      '* [154] Afra Feyza Alyurek, Ekin Akyurek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deductive closure training of language models for coherence, accuracy, and updability. _arXiv preprint arXiv:2401.08574_, 2024. \\(\\dashrightarrow\\)7.\n' +
      '* [155] Zonglin Yang, Xinya Du, Junsian Li, Lie Zheng, Souliaya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. _arXiv preprint arXiv:2309.02726_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [156] Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. Multi-modal molecule structure-text model for text-based retrieval and editing. _Nature Machine Intelligence_, 5(12):1447-1457, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [157] Bernardino Romera-Paredes, Mohammad Bernstein, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. _Nature_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [158] Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, and Been Kim. Bridging the Human-AI knowledge gap: Concept discovery and transfer in alphazero. _arXiv preprint arXiv:2310.16410_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [159] Tao Tu, Anil Palepu, Mike Schezkernmann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, et al. Towards conversational diagnostic ai. _arXiv preprint arXiv:2401.05654_, 2024. \\(\\dashrightarrow\\)7.\n' +
      '* [160] Belinda Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. _arXiv preprint arXiv:2310.11589_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '* [161] Xi Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender AI agent: Integrating large language models for interactive recommendations. _arXiv preprint arXiv:2308.16505_, 2023. \\(\\dashrightarrow\\)7.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>