<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# BeTAIL: 인간 경주 게임 플레이로부터의 행동 트랜스포머 적대적 모방 학습\n' +
      '\n' +
      'Catherine Weaver\\({}^{1}\\), Chen Tang\\({}^{1,2}\\), Ce Hao\\({}^{1,3}\\), Kenta Kawamoto\\({}^{4}\\), Masayoshi Tomizuka\\({}^{1}\\), Wei Zhan\\({}^{1}\\)\n' +
      '\n' +
      '미국 캘리포니아 버클리 대학교 기계공학과 Contact: catherine22@berkeley.edu\\({}^{2}\\)Department of Texas, Austin, USA.\\ ({}^{3}\\) Computing School of Singapore, National University of Singapore, ({}^{4}\\)Sony Research, Tokyo, Japan.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '모방 학습은 손으로 디자인한 보상 기능을 요구하지 않고 시연으로부터 정책을 학습한다. 자율 주행 경주와 같은 많은 로봇 작업에서 모방된 정책은 복잡한 환경 역학 및 인간의 의사 결정을 모델링해야 한다. 시퀀스 모델링은 복잡한 모션 시퀀스 패턴을 캡처하는 데 매우 효과적이지만 실제 로봇 작업에서 흔히 볼 수 있는 새로운 환경이나 분포 이동에 적응하기 위해 고군분투한다. 대조적으로, 적대적 모방 학습(AIL)은 이러한 효과를 완화할 수 있지만, 샘플 비효율성과 복잡한 모션 패턴을 처리하는 데 어려움을 겪는다. 따라서 본 논문에서는 인간의 시연으로부터 BeT( Behavior Transformer) 정책을 온라인 AIL과 결합한 BeTAIL: Behavior Transformer Adversarial Imitation Learning을 제안한다. BeTAIL은 AIL 잔차 정책을 BeT 정책에 추가하여 인간 전문가의 순차적 의사 결정 프로세스를 모델링하고 유통 외 상태 또는 환경 역학의 변화를 수정한다. 우리는 Gran Turliano Sport에서 실제 인간 게임 플레이의 전문가 수준의 시연을 통해 세 가지 도전에 대해 BeTAIL을 테스트합니다. 제안된 잔여 BeTAIL은 BeT가 다운스트림 학습보다 다른 트랙에서 사전 훈련된 경우에도 환경 상호 작용을 줄이고 레이싱 성능과 안정성을 향상시킨다. 사용 가능한 비디오 및 코드는 [https://sites.google.com/berkeley.edu/BeTAIL/home](https://sites.google.com/berkeley.edu/BeTAIL/home]이다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '자율 주행 레이싱은 차량 취급의 한계에서 제어기 설계를 알리고 인간 운전자와의 레이싱에 대한 안전한 대안을 제공하기 위한 관심이 증가하고 있다. 자율 레이서의 운전 스타일은 경주의 서면 및 사회적 규칙[1, 2]과 일치하는 안전하고 예측 가능한 방식으로 행동하기 위해 인간과 같은 경주 행동과 유사해야 한다. 심층 강화 학습(RL)은 전문가 인간 플레이어보다 우수하지만 밀도 있는 보상 정책 선택의 반복적인 튜닝을 요구한다[1]. 사실, RL에서의 보상 성형은 종종 임시 시행착오로 수행된다[3]. 모방 학습(IL)은 오프라인 시연을 이용한 전문가의 행동을 모방한 잠재적 해결책이다[4]. 본 연구에서는 인간 레이싱 게임플레이에서 IL의 문제점을 살펴보고, 마르코프적이지 않은 의사결정을 포착하고 그랜 투리스모 스포츠(GTS) 레이싱 게임에서 높은 성과를 보이는 온라인 정책을 배울 수 있는 새로운 솔루션을 제안한다.\n' +
      '\n' +
      '환경 역학 및 인간 의사 결정을 모델링하는 것은 인간 경주 전략을 학습하는 데 필수적이다[5]. 그러나 마르코프 정책의 성과는 인간의 시위로 악화될 수 있다[6]. 언어 모델[9]에서 현저하게 성공적인 시퀀스 기반 트랜스포머 아키텍처[7, 8]는 미세 조정 정책에 대한 환경 보상에 의존하면서도 인간 사고[10]의 복잡한 역학을 정확하게 모델링한다. 보상을 필요로 하지 않는 Behavior Transformer(BeT)[11], Trajectory Transformer[8]은 과거를 무심코 조건화하여 장기 환경 동역학 모델링[8]을 더 잘한다. 정책은 오프라인 데이터 세트에서 궤적의 가능성을 자동으로 최대화하기 위해 지도 학습을 통해 훈련된다. 따라서, 정책들은 데이터세트 품질[12]에 의해 제한되고, 데이터세트 분포 밖의 환경 역학 및 상태들의 변동에 민감하다.\n' +
      '\n' +
      '적대적 모방 학습(AIL) [13]은 적대적 훈련과 강화 학습을 활용하여 오프라인 학습으로 이러한 문제를 극복한다. 훈련된 판별자 네트워크는 에이전트가 온라인 롤아웃 및 전문가 궤적의 상태 점유율과 일치하도록 권장하여 보이지 않는 상태[4] 또는 환경 역학의 사소한 변화에 직면할 때 분포 이동 문제에 대한 민감성을 감소시킨다. 그러나 AIL은 처음부터 정책을 훈련할 때 광범위한 환경 상호 작용이 필요하며 인간 시연에서 학습할 때 성능이 크게 저하될 수 있다[6]. 따라서 레이싱 정책에 대한 AIL 훈련은 불안정하고 표본이 비효율적이다. 결과적인 정책은 또한 흔들리는 조향 행동을 나타내며 정책이 인간의 비마코브적 의사 결정 전략을 포착하지 못하기 때문에 궤도에서 벗어나기 쉽다.\n' +
      '\n' +
      '본 연구에서는 오프라인 순차 모델링과 온라인 점유일치 미세조정을 모두 1.)로 활용하는 행동변압기 적대적 모방학습(BeTAIL)을 제안한다. 먼저, BeT 정책은 오프라인의 인간 시연으로부터 학습된다. 그런 다음 시위의 국가 점유와 일치하도록 정책을 미세 조정하기 위한 AIL 메커니즘을 검토한다. BeTAIL은 BeT 액션 예측에 잔여 정책, 예를 들어 [14]를 추가하며, 잔여 정책은 BeT에 의해 예측된 액션 근처에 남아 있는 동안 에이전트의 액션을 정제한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 행동변압기 적대적 모방학습(BeTAIL, Behavior Transformer Adversarial Imitation Learning)을 제안한다.\n' +
      '2. 그란 투리스모 스포츠에서 실제 인간 게임 플레이를 통해 레이싱 정책을 배울 때, BeTAIL이 인간 시연에서 비마코브 패턴을 밀접하게 일치시키면서 BeT 또는 AIL 단독보다 우수하다는 것을 보여준다.\n' +
      '\n' +
      '3. 단일 시범 궤적으로 보이지 않는 트랙에서 미세 조정할 때 샘플 효율성과 성능을 향상시키기 위해 다중 트랙에서 시범 라이브러리에서 사전 훈련할 때 BeTAIL을 보여준다.\n' +
      '\n' +
      '이하에서는 먼저 예비(섹션 III)에 대해 논의한 후, 우리의 BeTAIL 방법(섹션 IV)을 소개한다. 섹션 VI에서 우리는 GT 스포츠에서 인간 선수를 모델링하기 위한 세 가지 실험 과제를 설명하고 섹션 VII에서 결론을 내린다.\n' +
      '\n' +
      '## II 관련 작품들\n' +
      '\n' +
      '### _Behavior Modeling_\n' +
      '\n' +
      '우리의 BeTAIL 방법은 인간 시연에서만 통제 정책을 학습하여 행동 모델링과 관련이 있다. 행동 모델링은 인간에 근접하여 작동하는 로봇 및 자동화 차량에 중요한 인간 시연자의 행동을 포착하는 것을 목표로 한다[15]. 행동 모델링을 위한 AIL은 행동 복제 및 모수 모델과 같은 전통적인 기술로 계단식 오류 문제를 극복한다[16]. 잠재적인 가변 공간은 연구자들이 다수의 별개의 운전 행동들[17, 18, 19, 20]을 모델링할 수 있게 한다. 그러나 샘플 효율성과 훈련 안정성은 AIL 정책을 처음부터 배우는 데 일반적인 문제이며, 이는 인간 시연[6] 또는 자율 경주[1]와 같은 복잡한 환경을 사용할 때 악화된다. 증강 보상[17, 19] 또는 부정적인 시연[21]은 훈련을 가속화할 수 있지만 RL에서 보상 형성과 동일한 함정을 공유할 수 있다. BeTAIL은 AIL 잔류 정책의 다운스트림 학습을 안내하기 위해 인간 시연에 대한 BeT 정책을 사전 훈련함으로써 AIL의 샘플 효율성과 훈련 안정성 함정을 극복한다.\n' +
      '\n' +
      '###_교육과정 학습 및 안내학습_\n' +
      '\n' +
      'RL에서는 처음부터 정책을 훈련하기보다는 구조화된 훈련 요법이 정책 학습을 가속화할 수 있다. 교육과정 학습은 점차 과제의 난이도를 높이고[22], 과제의 단계로 자동화할 수 있으며, 이는 점차 밀집된 모방 기반 보상을 희박한 환경 보상으로 전환한다[23]. "교사 정책"은 전문가 행동이 아닌 좋은 행동을 제공함으로써 RL을 가속화할 수 있다. 정책 개입은 안전하지 않은 행동을 방지한다[24, 25]. 안내된 정책 검색[26]에서 교사는 에이전트의 목적을 증명하고 안내하므로, 대규모 오프라인 정책은 경량 RL 정책으로 증류될 수 있다[10]. BeTAIL은 잔여 RL 정책[27, 14, 28]을 채택하고 있는데, 이는 교사에게 도우미 정책을 추가하여 과제의 변이에 적응할 수 있다. 따라서, 에이전트의 액션은 작은 잔여 액션 공간을 갖는 원래의 제어기에 의해 예측된 액션에 가깝게 유지될 수 있다[29, 30].\n' +
      '\n' +
      '### _Sequence Modeling_\n' +
      '\n' +
      '오프라인 RL 방법에서 시퀀스 기반 모델링은 상태, 동작 및 선택적으로 목표를 포함하는 궤적 시퀀스에서 다음 동작을 예측한다. 일반적으로 목표는 복귀, 즉 그 시간 단계[7, 8]로부터의 미래의 보상의 합으로 설정되지만, 어드밴티지 컨디셔닝은 확률적 환경에서 성능을 향상시킨다[31]. 목표 조절 정책은 자동 목표 레이블링을 활용하여 온라인 궤적을 미세 조정할 수 있다[12]. 시퀀스 모델은 환경 보상[10]의 안내와 함께 오프라인 RL 알고리즘을 사용하여 온라인 롤아웃 동안 경량 정책으로 증류될 수 있다. 모방 학습(11, 8)에서와 같이 목표 또는 보상이 이용 가능하지 않을 때, 지도 학습으로 훈련된 오프라인 모델의 성능은 배포 이동 및 열악한 데이터세트 품질(12) 하에서 고통받을 수 있다. BeTAIL은 환경 보상 또는 구조 목표 상태를 요구하지 않고, 모방 학습을 가속화하기 위해 행동 트랜스포머[11] 시퀀스 모델을 활용한다.\n' +
      '\n' +
      '## III Preliminaries\n' +
      '\n' +
      '### _Problem Statement_\n' +
      '\n' +
      '학습과제는 상태\\(s\\in\\mathcal{S}), 행동\\(a\\in\\mathcal{A}\\), 전이확률\\(T(s_{t},a_{t},s_{t+1}):\\mathcal{S}\\times\\mathcal{S}\\mapsto[0,1]\\)으로 정의되는 마르코프 프로세스(MP)로 모델링한다. RL과 달리 환경 보상에 액세스할 수 없습니다. 우리는 일련의 궤적(\\tau_{E}=(\\tau_{0}^{E},\\tau_{1}^{E}...,\\tau_{M}^{E}))으로 구성된 훈련 환경에서 인간의 전문가 시연에 접근할 수 있다. 기본적인 전문가 정책인 \\(\\pi_{E}\\)은 알려져 있지 않다. 전문가 정책(\\pi_{E}\\)에 가장 근사한 에이전트의 정책(\\pi\\)을 배우는 것이 목표이다. 각 전문가 궤적\\(\\tau^{E}\\)은 상태 및 동작 쌍으로 구성된다.\n' +
      '\n' +
      '\\[\\tau^{E}=(s_{0},a_{0},s_{1},a_{1},\\dots,s_{N},a_{N}). \\tag{1}\\]\n' +
      '\n' +
      '전문가의 인간 의사 결정 과정은 알려져 있지 않고 비마코비안일 가능성이 있으며, 따라서 모방 학습 성능은 인간 궤적과 함께 악화될 수 있다[6].\n' +
      '\n' +
      '###_Unimodal Decision Transformer_\n' +
      '\n' +
      'BeT( Behavior Transformer)는 궤적\\(\\tau_{E}\\)을 상태 및 동작의 2가지 입력들의 시퀀스로 처리한다. 원래 BeT 구현[11]은 가우시안 혼합을 사용하여 다중 모드 동작을 가진 데이터 세트를 모델링했다. 단순화와 계산 부담을 줄이기 위해, 우리는 대신 결정 변환기에 의해 원래 사용된 것과 유사한 결정론을 사용하는 유니모달 BeT를 사용한다[7]. 그러나, 잔류 정책은 블랙-박스 정책들에 추가될 수 있기 때문에[14], BeTAIL의 잔류 정책은 원래의 BeT 구현에 존재하는 k-모드들에 쉽게 추가될 수 있다.\n' +
      '\n' +
      '타임스텝 \\(t\\)에서 BeT는 마지막 \\(K\\) 타임스텝의 토큰을 사용하여 액션 \\(a_{t}\\)을 생성하는데, 여기서 \\(K\\)은 컨텍스트 길이라고 한다. 특히, 평가 동안의 컨텍스트 길이는 훈련에 사용되는 것보다 짧을 수 있다. BeT는 결정론적 정책 \\(\\pi_{\\text{BeT}}(a_{t}|\\mathbf{s}_{-K,t})\\)을 학습하며, 여기서 \\(\\mathbf{s}_{-K,t}\\)은 과거 상태 \\(\\(K\\) 과거 상태 \\(\\mathbf{s}_{\\max(1,t-K+1):t}\\)의 시퀀스를 나타낸다. 정책은 예측된 액션 시퀀스에서 자기회귀 구조를 강제하기 위해 인과 마스크를 적용하는 minGPT 아키텍처[33]를 사용하여 매개변수화된다.\n' +
      '\n' +
      'BeT의 주목할 만한 강점은 정책이 비마코비안 행동을 모델링할 수 있다는 것이다; 즉, 행동 확률을 \\(P(a_{t}|s_{t})\\)로 모델링하기보다는, 정책은 확률 \\(P(a_{t}|s_{t},s_{t-1},...,s_{t-h+1})\\)을 모델링한다. 그러나, 정책은 오프라인 데이터세트 _only_를 사용하여 트레이닝되며, 트레이닝 데이터와 평가 환경 사이의 사소한 차이조차도 정책의 성능에 큰 편차를 초래할 수 있다[4].\n' +
      '\n' +
      '## IV 행동변압기-보조 적대적 모방학습\n' +
      '\n' +
      '이제 그림 1에 요약된 행동 변압기 지원 적대적 모방 학습(BeTAIL)을 제시한다. 먼저, 인간 전문가의 순차적 의사 결정을 포착하기 위해 오프라인 시연에서 인과적 행동 변압기(BeT) [11]의 단일 모드 버전을 훈련한다. 그런 다음, BeT 정책은 적대적 모방 학습(AIL) 미세 조정을 위한 결정론적 기반 정책으로 사용된다. BeTAIL은 AIL을 사용하여 잔류 정책을 훈련하고[29], 이는 기본 정책의 액션에 추가되어 에이전트의 정책을 형성한다. 따라서 BeTAIL은 오프라인 순차 모델링과 온라인 점유 매칭 미세 조정을 결합하여 인간 시연자의 의사 결정 과정을 포착하고 유통 외 상태 또는 환경 변화에 맞게 조정한다.\n' +
      '\n' +
      '### _BeT(Behavior Transformer) Pretraining_\n' +
      '\n' +
      '유니모달 BeT 정책\\(\\hat{a}=\\pi_{\\mathrm{BeT}(a_{t}|\\mathbf{s}_{-K,t})\\)은 기본 작용\\(\\hat{a}\\)을 예측하는 데 사용된다. [7, 12]에 이어서, 시연들에서 길이 \\(K\\)의 서브-궤적이 균일하게 샘플링되고, BeT에 의해 예측된 액션과 시연 시퀀스에서의 다음 액션 사이의 차이를 최소화하도록 트레이닝된다. BeT가 예측한 행동, \\(\\hat{a}\\)은 이전 상태 이력 \\(\\mathbf{s}_{-K,t}\\)을 고려하지만, 확률적 환경에서는 행동이 이상적이지 않을 수 있다[31]. 따라서, 에이전트가 환경에서 얼마나 잘 수행하는지에 기초하여 베이스 액션을 보정할 수 있는 잔여 액션[14]을 추가하여 베이스 액션을 보정한다. 환경 보상은 제공되지 않으며 인간 시연을 복제하고 싶기 때문에 잔류 정책을 훈련하기 위해 AIL을 사용합니다.\n' +
      '\n' +
      '온라인 미세조정을 위한###_잔차정책 학습\n' +
      '\n' +
      '에이전트의 액션은 BeT에 의해 지정된 액션과 잔여 정책으로부터의 액션의 합이다[14]. 잔여 정책은 BeT 기본 정책으로부터의 동작을 수정하거나 개선한다[14]. 먼저, 증강 MP(\\tilde{\\mathcal{M}=\\tilde{\\mathcal{S},\\tilde{\\mathcal{A},\\tilde{T}\\})를 정의한다. 상태공간\\(\\tilde{\\mathcal{S}\\)은 기본작용으로 증강된다.\\(\\tilde{s}_{t}\\doteq\\begin{bmatrix}s_{t}&a_{t}\\end{bmatrix}^{\\intercal}\\. 작용공간인 \\(\\tilde{A}\\)은 잔류작용 \\(\\tilde{a}\\)의 공간이다. 전이 확률인 \\(\\tilde{T}(\\tilde{s}_{t},\\tilde{a}_{t},\\tilde{s}_{t+1})\\)는 원래 환경의 동역학, 즉 \\(T(s_{t},a_{t},s_{t+1})\\)과 기본 작용 \\(\\hat{a}_{t}=\\pi_{\\mathrm{BeT}(\\cdot|\\mathbf{s}_{-K,t})\\)을 모두 포함한다.\n' +
      '\n' +
      '잔차정책으로 가우시안 \\(\\hat{a}\\sim f_{\\mathrm{res}(\\hat{a}|s_{t},\\hat{a}_{t})=\\mathcal{N}(\\mu,\\sigma)\\)를 정의한다. 잔차 정책은 현재 상태 및 기본 정책에 의해 예측된 액션을 사용하여 잔차 액션을 선택한다. 그런 다음 이 잔여 액션이 기본 액션에 추가됩니다. 이 합, \\(a_{t}\\)는 환경에서 취해지는 작용이다:\n' +
      '\n' +
      '\\[a_{t}=\\hat{a}_{t}+\\mathrm{clip}(\\tilde{a}_{t},-\\alpha,\\alpha). \\tag{2}\\]\n' +
      '\n' +
      '잔여정책은 \\([-\\alpha,\\alpha]\\) 사이로 제한되며, 이는 환경작용 \\(a_{t}\\)이 기본작용 \\(\\hat{a}\\)[29,14]에서 얼마나 벗어날 수 있는지에 대한 제약을 허용한다. \\(\\alpha\\)이 작은 경우, 환경 작용은 BeT가 예측한 기본 작용인 \\(\\hat{a}_{t}\\)에 여전히 근접해야 한다.\n' +
      '\n' +
      '\\(f_{\\theta}(\\cdot|s_{t},\\hat{a}_{t})\\)의 작용 공간이 \\([-\\alpha,\\alpha]\\)의 범위로 제한된다고 가정하면, 우리는 정책 \\(\\pi_{A}(a|\\mathbf{s}_{-k,t})\\)을 정의할 수 있다:\n' +
      '\n' +
      '\\\\[\\pi_{A}(a|\\mathbf{s}_{-k,t})= \\tag{3}\\] \\[\\tilde{a}_{t}|_{\\hat{a}_{t}\\sim f_{\\mathrm{res}(\\cdot|s_{t}, \\pi_{\\mathrm{BeT}}(\\cdot|s_{-k,t}))}+\\pi_{\\mathrm{BeT}(\\cdot|\\mathbf{s}_{-k,t})}[\\tilde{a}_{t}|_{\\hat{a}_{t}\\sim f_{\\mathrm{res}(\\cdot|s_{t}, \\pi_{\\mathrm{BeT}}(\\cdot|s_{-k,t}))}+\\pi_{\\mathrm{BeT}}(\\cdot|\\mathbf{s}_{-k,t}}}}(\\cdot|s_{-\n' +
      '\n' +
      'Eq. 도 3은 [34]의 표기법을 따른다. BeT 정책은 비마코비안이기 때문에 에이전트의 정책 \\(\\pi_{A}\\)도 비마코비안이다. 그러나, 증강 상태의 정의인 \\(\\tilde{s}\\)을 사용하여 AIL 잔차 정책에 대한 입력은 현재 상태 및 BeT에 의해 예측되는 베이스 액션이다. 따라서, \\(\\pi_{A}\\)는 증강 상태에 대해 사실상 마코비안이다 \\(\\tilde{s}\\):\n' +
      '\n' +
      '\\[\\pi_{A}(a|s_{t},\\hat{a}_{t})=\\tilde{a}_{t}|_{\\hat{a}_{t}\\sim f_{\\mathrm{res}(\\cdot|s_{t},\\hat{a}_{t}}+\\hat{a}_{t}.\\tag{4}\\t}\n' +
      '\n' +
      '기존 연구[14, 34]에 이어 온라인 교육 시 롤아웃 시 에이전트의 정책은 (4)에서 \\(\\pi_{A}\\)으로 주어진다. 즉, 에이전트의 정책은 기본 정책\\(\\pi_{\\mathrm{BeT}\\)에 의해 주어진 액션과 잔여 정책\\(f_{\\mathrm{res}\\)에 의해 주어진 액션의 합이다. (3). 초기 교육(IV-A절) 후, 결정론적 기반 정책인 \\(\\pi_{\\mathrm{BeT}}\\)은 온라인 교육 중에 동결되고, 에이전트 정책은 잔류 정책 \\(f_{\\mathrm{res}}\\)을 AIL로 업데이트함으로써 개선된다.\n' +
      '\n' +
      '###_AIL_을 이용한 잔여 정책 훈련\n' +
      '\n' +
      '잔여 정책을 훈련하기 위해, 우리는 온라인 롤아웃에서 에이전트의 정책이 AIL 목표를 해결하도록 적대적 모방 학습(AIL) [35]를 적용한다. 그러나, AIL은 일반적으로 기본 정책에 추가되지 않는 단일 마코비아 정책의 관점에서 정의된다[13]. 이 섹션에서는 AIL을 사용하여 잔차 정책\\(f_{\\mathrm{res}}\\)을 업데이트하는 AIL 공식 업데이트에 대해 자세히 설명한다. [\\(\\pi_{A}\\)(4)의 정의를 고려할 때, 잔차 정책 학습을 위한 AIL 목표를 다음과 같이 정의한다.\n' +
      '\n' +
      '\\[\\underset{f_{\\mathrm{res}}{\\mathrm{minimize}}\\quad D_{\\mathrm{JS}\\left(\\rho_{\\pi_{A}},\\rho_{\\pi_{E}}\\right)-\\lambda H(f_{\\mathrm{res}}). \\tag{5}\\\n' +
      '\n' +
      '표준 AIL 목표 [35]와 유사하게, 우리는 여전히 Eq에서 각각 \\(\\rho_{\\pi_{E}}\\) 및 \\(\\rho_{\\pi_{A}}\\)으로 표시된 전문가와 에이전트의 정책 점유 측정 사이의 거리를 최소화하는 것을 목표로 한다. (5). (f_{\\mathrm{res}}\\)에 대한 최소화는 Eq이므로 유효하다. (4) 단순히 표준 단일 정책 AIL보다 더 제한적인 정책 클래스를 정의한다. 그때부터\n' +
      '\n' +
      '도. 1: BeTAIL 롤아웃 컬렉션. 사전 훈련된 BeT는 마지막 상태 행동으로부터 행동\\(\\hat{a}_{t}\\)을 예측한다. 그리고 잔여 정책은 현재 상태로부터 \\(\\tilde{a}_{t}\\)와 \\(\\hat{a}_{t}\\)을 지정하고, 에이전트는 환경에서 \\(a_{t}=\\hat{a}_{t}+\\tilde{a}_{t}\\)을 실행한다.\n' +
      '\n' +
      '기본 정책의 기여도는 결정론적이며 잔차 정책의 엔트로피만을 사용하여 문제를 정규화하고 정책 업데이트 단계는 다음과 같이 대체된다.\n' +
      '\n' +
      '\\tilde{R}^{E}\\tilde{\\tau}\\sim f_{\\text{res}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\Bigg{(}\\tilde{s}_{t},\\tilde{a}_{t}\\right)+\\lambda H\\left(\\cdot\\mid\\tilde{s}_{t}\\right)+\\gamma^{t}\\Bigg{(}\\tilde{R}^{E}\\fty}\\gamma^{t}\\Bigg{(}\\mid\\tilde{s}_{t}\\right),\\tag{6}\\tw}\\tw}\\tw}\\tw}\n' +
      '\n' +
      '여기서 \\(\\tilde{\\tau}\\)는 \\(\\tilde{\\mathcal{M}\\)에 \\(f\\text{res}\\)을 사용하여 수집된 궤적 \\(\\tilde{\\tau}=(\\tilde{s}_{0},\\tilde{a}_{0},...)\\)을 나타낸다. 따라서, 증강된 상태와 잔여 액션을 이용하여 잔여 정책 \\(f_{\\text{res}}\\)을 갱신한다. 우리는 \\(\\tilde{R}^{E}\\)을 \\(\\tilde{M}\\)에 대한 프록시 환경 보상으로 정의한다. RL에 대한 잔차 정책 학습을 사용한 이전 작업에 이어 [14] 환경에서 취한 조치를 사용하여 대리 보상을 계산한다. AIL의 프록시 환경 보상을 유사하게 적용하기 위해 정의합니다.\n' +
      '\n' +
      '\\[\\tilde{R}^{E}(\\tilde{s}_{t},\\tilde{a}_{t})=-\\log\\left(1-D_{\\omega}^{E}(s,\\hat{a}+\\tilde{a})\\right), \\tag{7}\\t}\n' +
      '\n' +
      '여기서 \\(D_{\\omega}^{E}(s,\\hat{a}+\\tilde{a}))는 최소화를 위해 훈련된 이진 분류기이다.\n' +
      '\n' +
      '\\mathcal{L}_{D,\\mathcal{D}_{E}(\\omega)& =-E_{\\tau_{E}\\sim\\mathcal{D}_{E}}\\left[\\log\\left(D_{\\omega}^{E}(s,a)\\right)\\right]\\\\\\\\qquad-E_{\\tilde{\\tau}\\sim f_{text{res}}\\left[\\log\\left(1-D_{\\omega}^{E}(s,\\hat{a}+\\tilde{a}\\right)\\right],\\end{split}\\tag{8}\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\\wit\n' +
      '\n' +
      '와 동등하다.\n' +
      '\n' +
      '\\mathcal{L}_{D,\\mathcal{D}_{E}}(\\omega)& =-E_{\\tau_{E}\\sim\\mathcal{D}_{E}}\\left[\\log\\left(D_{\\omega}^{E}(s,a)\\right)\\right]\\\\\\\\\\qquad-E_{\\tau\\sim\\pi_{a}}\\left[\\log\\left(1-D_{\\omega}^{E}(s,a)\\right]\\end{split}\\tag{9}\\right]\n' +
      '\n' +
      'Eq. (7) 환경보상은 상태-행동 쌍이 전문가 정책에서 나올 확률(s,a)=(s,\\hat{a}+\\tilde{a}))이다. 따라서, 정책 학습 목표(6)와 판별자 손실(9)을 반복함으로써 AIL은 (5)를 최소화하고, 전문가(\\pi_{E}\\)의 점유 측정치와 밀접하게 일치할 수 있는 \\(f_{text{res}\\)을 결정한다.\n' +
      '\n' +
      '## V Imitit of Human Racing Gameplay\n' +
      '\n' +
      '우리는 3가지 IL 도전에 대한 상태 특징, 환경 및 훈련 체제를 포함하여 GTS 레이싱 게임에서 인간 게임 플레이로부터 배우는 방법을 설명한다.\n' +
      '\n' +
      '###_상태 특징 추출 및 Actions_\n' +
      '\n' +
      '선택한 기능에는 이전에 RL에서 중요한 것으로 나타났으며 시연에서 사용할 수 있는 기능이 포함되어 있습니다. [36]에 기재된 바와 같이 다음과 같은 상태특징을 정확히 포함한다. 직선속도, \\(\\mathbf{v}_{t}\\in\\mathbbb{R}^{3}\\) 및 선형가속 \\(\\hat{\\mathbf{v}}_{t}\\in\\mathbbb{R}^{3}\\)은 차량의 관성 프레임에 대한 것이다. 2.) 수평면에서의 에이전트의 회전을 정의하는 2D 벡터와 투영점에서의 중심선에 접하는 단위 접선 벡터 사이의 오일러각 \\(\\theta_{t}\\in(-\\pi,\\pi]\\)은 벽접촉을 나타내는 2차원 플래그를 갖는다. 3.) N 샘플링된 곡률 측정치 (\\(w_{t}=1\\)는 벽접촉을 나타낸다. 4.) N 샘플링된 곡률 측정치 (\\mathbf{c}_{t}\\in\\mathbbb{R}^{N}\\).\n' +
      '\n' +
      '또한, [1]:5.) 차량의 현재 헤딩의 코사인 및 사인, \\(\\cos(\\psi)\\) 및 \\(\\sin(\\psi)\\).6.) 차량의 현재 위치로부터 좌측까지의 상대적인 2D 거리, \\(\\mathbf{e}_{LA,l}\\), 우측, \\(\\mathbf{e}_{LA,r}\\) 및 중앙, \\(\\mathbf{e}_{LA,c}\\)에서 사용된 특징들과 유사한 특징들을 5"룩-어헤드 포인트들(도 2의 청색 도트들)에서 선택한다. 차량이 현재 속도를 유지한다고 가정할 때, 2초의 룩-어헤드 시간을 사용하여 균등하게 배치되며, 즉 트랙의 다음 2초에 걸쳐 균등하게 이격된다. 따라서 전체 상태는 \\(s_{t}=[\\mathbf{v}_{t},\\hat{\\mathbf{v}}_{t},\\theta_{t},w_{t},\\mathbf{c}_{t}, \\cos(\\psi),\\sin(\\psi),\\mathbf{e}_{LA,l},\\mathbf{e}_{LA,r},\\mathbf{e}_{LA,c}]으로 구성된 벡터이다. 마지막으로, 시연에서 각 상태 특징의 평균 및 표준 편차를 사용하여 상태를 정규화한다.\n' +
      '\n' +
      'GTS는 조향 명령 \\(\\delta\\in[-\\pi/6,\\pi/6]\\) rad와 스로틀-브레이크 신호 \\(\\omega_{r}\\in[-1,1]\\)을 수신하며, 여기서 \\(\\omega_{r}=1\\)은 풀 스로틀을 나타내고 \\(\\omega_{r}=-1\\)은 풀 브레이크 [36,1]을 나타낸다. 모든 기준선 비교에 대해, 시연에서의 조향 명령은 \\([-1,1]\\) 사이에 있도록 스케일링된다. 이 에이전트는 GTS로 전송되기 전에 \\(\\delta\\in[-\\pi/6,\\pi/6]\\)으로 축소된 \\([-1,1]\\) 사이의 조향 동작을 지정한다.\n' +
      '\n' +
      '잔차 정책을 학습한 경우(다음 절의 BeTAIL과 BCAIL 참조) 잔차 정책 네트워크는 \\(\\tilde{a}\\in[-1,1]\\)을 예측한 다음 \\(\\tilde{a}\\)을 \\([-\\alpha,\\alpha]\\)이 되도록 스케일링한 후 \\(a=\\hat{a}+\\tilde{a}\\)에 추가한다. \\(\\hat{a}\\in[-1,1]\\)이므로 \\(a\\)이 \\([-1,1]\\의 범위를 벗어날 수 있으므로 GTS로 작업을 보내기 전에 \\(a\\)을 클리핑한다. 초매개변수\\(\\alpha\\)의 선택은 잔류작용의 최대 크기를 결정한다. 과제와 기본 정책의 강도에 따라 잔류 정책이 나쁜 행동을 교정할 수 있도록 \\(\\alpha\\)이 상대적으로 클 수 있거나 [34] 정책이 기본 정책에서 크게 벗어나지 않도록 \\(\\alpha\\)이 상대적으로 작을 수 있다[14, 30]. 본 연구에서는 비마코프 레이싱 행태를 포착하는 오프라인 BeT에 최대한 근접하여 정책이 유지될 수 있도록 각 태스크에 대한 높은 성능을 달성하기 위해 \\(\\alpha\\)을 가능한 작게 설정하였다.\n' +
      '\n' +
      '각주 1: [-1,1] 사이의 잔여 정책을 스케일링하는 선택은 SAC[37]와 함께 스케일링된 액션 스페이스를 사용하는 관행에 기초하여 이루어진다.\n' +
      '\n' +
      '###_환경 및 데이터 수집_\n' +
      '\n' +
      '#### V-B1 그란 투리스모 스포츠 레이싱 시뮬레이터\n' +
      '\n' +
      '본 논문에서는 Polyphony Digital, Inc.가 개발한 고충실도 플레이스테이션(PS) 게임 Gran Turismo Sport (GTS) ([https://www.gran-turismo.com/us/](https://www.gran-turismo.com/us/))에서 실험을 수행하였다. GTS는 스로틀/브레이킹과 스티어링의 두 가지 연속 입력을 사용합니다. 차량 위치, 속도, 가속도 및 자세가 관찰된다. 에이전트와 시연자의 상태 피쳐와 제어 입력이 동일합니다. Deep RL은 최근 GTS[1]에서 인간 상대를 상대로 초인적 퍼포먼스를 달성하였다. 그러나, 이러한 성공은 레이싱 에이전트가 인간 반대자들과 "잘" 행동하기 위해 조정되고 조밀한 보상 기능을 필요로 했다. 리워드 기능을 세심하게 제작하기보다는 전문가 인간 운전자의 시연이 최고 성능의 에이전트를 알릴 수 있는지 탐구한다.\n' +
      '\n' +
      '####V-B2 에이전트 롤아웃 및 테스트 환경\n' +
      '\n' +
      '롤아웃 및 평가 에피소드를 수집하기 위해 GTS 시뮬레이터는 플레이스테이션에서 실행되고 에이전트는 별도의 데스크톱 컴퓨터에서 실행됩니다. 데스크톱 컴퓨터는 [36]에 설명된 것과 유사한 이더넷 연결을 통해 전용 API를 통해 GTS와 통신한다. API는 최대 20대의 시뮬레이션된 자동차의 현재 상태를 제공하고 다음 명령이 수신될 때까지 활성화되는 자동차 제어 명령을 수락한다. GTS 상태가 60Hz마다 업데이트되는 동안, 우리는 RL[36, 1]에서 이전 작업에 따른 PS 및 데스크탑 컴퓨터의 부하를 줄이기 위해 에이전트의 제어 주파수를 10Hz로 제한한다. 단일 정책 네트워크를 학습하고 롤아웃을 병렬로 수집한 이전 작업과 달리, 본 프레임워크는 BeT 정책과 잔차 정책의 두 가지 네트워크를 가지고 있기 때문에 데스크탑 PC의 계산 부담을 줄이기 위해 롤아웃을 순차적으로 학습하고 수집한다.\n' +
      '\n' +
      '### _ 모방 학습을 위한 학습 기법_\n' +
      '\n' +
      '그림 3의 상단에 표시된 대로 시연 가용성 및 환경 설정이 다른 세 가지 문제에 대해 BeTAIL을 평가한다. 각 문제에 대한 보다 자세한 설명은 섹션 VI의 해당 부분을 참조한다.\n' +
      '\n' +
      '####V-C1 알고리즘과 기준선\n' +
      '\n' +
      '각 챌린지에 대해, BeT 정책인 \\(\\pi_{\\text{BeT}}(\\cdot|\\mathbf{s}_{-K,t})\\)는 하나 이상의 트랙으로부터 오프라인 인간 시연에서 사전 훈련된다. 그리고 AIL을 이용하여 다운스트림 환경 및 인간 시연을 통해 부가적 잔차 정책인 \\(f_{\\text{res}}\\)을 학습하여 **BeTAIL** 미세 조정을 제어한다. 잔여정책은 \\([-\\alpha,\\alpha]\\) 사이에서 제한되며, 여기서 \\(\\alpha\\)은 각 도전마다 개별적으로 지정된다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 2(a)-c, 적색은 BeT 정책에 대한 시연에 해당하고 \\(\\mathrm{green}\\)은 하류 환경에 해당하고 잔류 AIL 정책에 대한 시연에 해당한다. *BeT** 기준선은 사전 훈련 시연을 빨간색과 함께 BeT 정책인 \\(\\pi_{\\text{BeT}}(\\cdot|\\mathbf{s}_{-K,t})\\을 사용한다. 반대로 **AIL** 기준선은 \\(\\pi_{A}(a|s)\\)의 다운스트림 시연 및 환경만 사용하여 AIL을 통해 훈련된 단일 정책, \\(\\mathrm{green}\\)으로 구성된다. *BCAIL** 기준선은 오프라인 시연(빨간색)에서 BC 정책, \\(\\pi_{A}(a|s)\\)을 훈련한 다음 BeTAIL과 동일한 훈련 방식에 따라 다운스트림 환경(\\(\\mathrm{green}\\))에서 잔류 정책을 미세 조정한다. 그러나 첫 번째 도전에서 BC와 BCAIL의 낮은 성능을 감안할 때 다른 더 어려운 도전에서 BCAIL의 비교를 제외한다.\n' +
      '\n' +
      '에이전트는 적대적 학습(즉, AIL, BCAIL, BeTAIL)에서 강화 학습 알고리즘을 위해 소프트 액터-크리틱(soft actor-critic, SAC)[37]을 사용한다. SAC는 오프 정책이기 때문에, 이것은 리플레이 버퍼가 이전의 다수의 반복으로부터의 이력 롤아웃을 포함할 수 있음을 의미한다. 그러나, 판별기 네트워크, 따라서 상태-액션 쌍과 연관된 보상이 변경되었을 수 있다. 따라서, 정책 및 Q-네트워크 트레이닝을 위해 리플레이 버퍼로부터 데이터를 샘플링할 때, 현재 판별기 네트워크를 사용하여 연관된 상태-행동 쌍에 대한 보상을 재계산한다. 마지막으로, 판별자 과적합은 판별자가 너무 강력하면 AIL이 의미 있는 정책을 배우지 못하게 할 수 있다. 모든 AIL 방법에 대해 판별자를 훈련할 때 기울기 패널티와 판별자 엔트로피 정규화[6]라는 두 개의 조절자를 사용한다.\n' +
      '\n' +
      '###### V-C2 시범 개요\n' +
      '\n' +
      '모든 시연은 아우디 TT 컵 차량과의 실시간 게임 플레이, "시범" 경쟁에 참여하는 다른 인간 플레이어 GTS에서 녹음되었다. 모든 궤적은 고품질이며 전문가 수준의 레이서가 수행합니다. 각 시연은 트랙의 단일 랩 주위에서 상태와 액션의 전체 궤적을 포함한다. 궤적은 60Hz의 주파수로 기록되었으며, 이를 10Hz(에이전트의 제어 주파수)로 다운샘플링했다. 각각의 궤적(즉, 랩)은 대략 7000개의 타임스테프를 포함한다. 도 1에 도시된 바와 같다. 2(a)-c, BeTAIL이 프리트레이닝과 동일한 트랙에서 성능을 향상시키고, 단일 트랙 BeT 정책을 새로운 트랙으로 전달하고, 다중 트랙 BeT 정책을 보이지 않는 트랙으로 전달할 수 있는지 테스트하기 위해 세 가지 훈련 스킴을 테스트한다(자세한 내용은 섹션 VI 참조).\n' +
      '\n' +
      '## VI 레이싱 실험 결과\n' +
      '\n' +
      '우리는 BeTAIL 제형을 테스트하기 위해 뚜렷한 사전 훈련 시연과 다운스트림 환경에 대한 세 가지 문제를 소개한다. 평가를 위해 20대의 자동차가 트랙에 무작위로 고르게 배치됩니다. 각 자동차의 초기 속도는 시연 궤적에서 가장 가까운 위치의 전문가의 속도로 설정됩니다. 각 자동차는 전체 바퀴를 완성하는 데 500초가 걸립니다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 2(d)-f, 우리는 훈련 동안 두 가지 평가 메트릭을 제공한다: 랩(상단)을 마치는 자동차의 비율과 마치는 자동차의 평균 랩 시간. 오차 막대는 20대의 자동차(평가 에피소드)와 3개의 씨앗에 걸친 총 표준 편차를 보여준다. 도. 2(g)-i는 RL 정책이 인간 전문가에 존재하지 않는 바람직하지 않은 흔들리는 조향 행동[1]을 나타낼 수 있기 때문에 최상의 정책의 랩 타임의 평균\\(\\pm\\) 표준 편차와 조향 \\(|\\delta_{t}-\\delta_{t-1}|\\)의 절대 변화를 제공한다. 궤적 및 GTS 게임 환경의 비디오는 [https://sites.google.com/berkeley.edu/BeTAIL/home](https://sites.google.com/berkeley.edu/BeTAIL/home]에서 제공된다. 아래에서는 각 도전의 설정을 자세히 설명하고 결과를 분석한다.\n' +
      '\n' +
      '### _Lago Maggiore Challenge: BeTAIL_와 같은 환경에서 미세 조정\n' +
      '\n' +
      '첫 번째 실험은 BeT 정책이 다운스트림 환경과 동일한 트랙에서 시연으로부터 학습될 때 BeTAIL이 BeT 정책을 얼마나 잘 미세 조정할 수 있는지를 테스트한다(Fig). 2(a)). BeTAIL(0.05)은 동일한 환경에서 BeT 정책을 사전 훈련하기 때문에 작은 잔차 정책인 \\(\\alpha=0.05\\)을 사용한다. 절제법 BeTAIL(1.0)은 큰 잔차 정책인 \\(\\alpha=1.0\\)을 사용한다. 라고 마지오레 트랙에는 다양한 인간 선수들로부터 49건의 시위가 있다. BeT 정책은 49개의 궤적에 대해 훈련된 다음 시연과 동일한 49개의 궤적 및 동일한 환경에서 BeTAIL을 실행한다. BCAIL은 동일한 훈련 방식을 따른다(\\(\\alpha=0.05\\)). AIL은 동일한 49개의 궤적에 대해 훈련된다.\n' +
      '\n' +
      '도. 도 2(d)는 훈련 중 각 에이전트의 평가를 나타낸다. BeTAIL은 모든 방법 중 최고를 수행하고, 풀 랩을 일관되게 마무리하고 최저 랩 시간을 달성할 수 있는 정책을 빠르게 학습한다. AIL 베이스라인은 결국 트랙을 항해하고 유사하게 낮은 랩 타임을 달성하는 정책을 학습할 수 있지만, 훈련이 끝날 무렵에도,\n' +
      '\n' +
      '도. 2: Lago Maggiorethe AIL 정책에 대한 Track Lookahead Points on Lago Maggiorethe AIL 정책은 일관성이 낮고 그림 상단의 완료 랩 수의 더 높은 표준 편차에서 알 수 있듯이 여러 자동차가 평가 중에 전체 랩을 완료하지 못할 수 있습니다. d)d. 나머지 기준선은 이 작업에서 제대로 작동하지 않습니다. 잔여 BC 정책(BCAIL)을 사용하면 실제로 성능이 악화되는데, 이는 분포 이동으로 인해 보이지 않는 환경에서 BC 정책이 제대로 수행되지 않기 때문일 수 있다. 인상적이게도, BeT 정책은 오프라인 데이터에 대해서만 교육을 받았음에도 불구하고 일부 랩을 마칠 수 있지만, 온라인 롤아웃을 사용하여 정책의 문제를 수정할 수 있는 BeTAIL 및 AIL에 비해 랩 시간이 부족하다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. (g)g, BeTAIL(1.0)이 BeTAIL(0.05)보다 더 좋은 랩타임을 얻을 수 있는데, 이는 \\(\\alpha\\)이 클수록 잔차 정책이 BeT 기본 정책에 더 많은 수정을 적용할 수 있기 때문이다. 그러나 BeTAIL(1.0)은 타임스텝 간 조향 명령의 편차가 클수록 앞뒤로 조향을 진동시키는 경향이 있다. 우리는 그것이 \\(\\alpha\\)이 커질수록 비마르코프 인간 유사 행동이 감소하기 때문이라고 가정한다. 마코비아 AIL 정책도 조종명령에 극단적인 변화를 가져오는 경향이 있다는 관측이 뒷받침하고 있다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 4, 우리는 AIL 정책의 극단적인 조향 속도가 견고성에 어떻게 해를 끼치는지에 대한 추가 통찰력을 제공한다. 우리는 고의적으로 AIL과 BeTAIL을 위한 코너 앞에 있는 인간 시위대보다 훨씬 낮은 속도로 경주용 자동차를 초기화한다. 우리는 보이지 않는 상태에서 안정성을 모니터링하여 학습된 에이전트의 견고성을 테스트하는 것을 목표로 한다. AIL 에이전트의 헤딩은 앞뒤로 빠르게 조향하는 경향으로 인해 진동하여 AIL 에이전트의 제어를 잃게 한다.\n' +
      '\n' +
      '도. 3: 세 가지 레이싱 챌린지에 대한 실험 결과. (a) 라고 마지오레 챌린지는 동일한 시연 및 다운스트림 환경에서 BeT를 사전 훈련한다. (b) 드래곤 테일은 BeTAIL 미세조정으로 BeT 정책을 새로운 트랙으로 이전한다. (c) Mount Panorama 챌린지는 4 트랙의 라이브러리에서 BeT를 미리 훈련시키고, 보이지 않는 트랙에서 BeTAIL 미세선을 훈련시킨다. (d)-(f) 랩 타임의 평균(std) 성공률 및 평균(std)의 평가. (g)-(i) Best policy\'s mean \\(\\pm\\) std lap time and change in steering from previous time step.\n' +
      '\n' +
      '그 차량은 코너와 충돌한다. 반대로 베테일은 부드럽게 가속하고 코너로 브레이크를 밟는다.\n' +
      '\n' +
      '### _Dragon Tail Challenge: BeTAIL_\n' +
      '\n' +
      '두 번째 실험은 다운스트림 환경이 BeT 시연과 다를 때 BeTAIL이 BeT 정책을 얼마나 잘 조정할 수 있는지 테스트한다(그림). 2(b)). 라고 마지오레 챌린지에서도 동일한 BeT 정책이 사용되지만 하류 환경은 다른 트랙이며 새로운 드래곤 트레일 트랙에 대한 시연은 12개에 불과하다. BeTAIL(0.10)의 경우 하류 환경이 BeT 사전 훈련 시연과 다르기 때문에 잔류 정책이 약간 더 큰 \\(\\alpha=0.10\\)이 되도록 한다. 차량 역학은 변하지 않습니다.\n' +
      '\n' +
      '드래곤 꼬리 트랙의 정책을 훈련하고 평가하기 위한 결과는 그림 1에 나와 있다. 2(e)/h. 다시, BeTAIL은 정책 학습을 안내하기 위해 BeT를 채용할 수 있다; 그 결과, BeTAIL은 고속으로 트랙을 일관되게 탐색하는 것을 빠르게 학습한다. 또한, 작은 \\(\\alpha\\)는 비마코프 사람의 행동이 보존되어 부드러운 조향을 할 수 있다. 반대로 AIL은 낮은 랩 시간을 달성할 수 있는 정책을 배울 수 있지만 스티어링의 급격한 변화에 훨씬 더 취약하고 랩을 완료하지 못할 가능성이 훨씬 더 높다. 2(e)). 다른 트랙에서 시연으로 훈련된 사전 훈련된 BeT는 어떤 랩도 완료할 수 없다.\n' +
      '\n' +
      'BeTAIL__Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: 다중 트랙 BeT 정책을 학습하고 BeTAIL___Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Panorama 챌린지: BeTAIL__Mount Pan\n' +
      '\n' +
      '마지막으로, BeT 정책은 Lago Maggiore GP(38바퀴), Autodromo de Interlagos(20바퀴), Dragon Tail - Seaside(28바퀴), Brands Hatch GP(20바퀴)의 네 가지 트랙에 대한 궤적 라이브러리에 대해 훈련된다. BeTAIL 미세 조정을 위해, BeTAIL은 마운트 파노라마 서킷에서 단일 시연으로 훈련된다. 궤적의 가용성으로 인해, 다른 타이어(레이싱 하드)의 사용으로 인해 처음 두 가지 도전으로부터 차량 역학에 약간의 변화가 있으며, 다운스트림 훈련 및 평가에서의 차량 역학은 궤적 라이브러리와 동일한 레이싱 하드 타이어를 사용한다. 마운트 파노라마 회로는 프리트레이닝 트랙보다 더 많은 언덕과 날카로운 회전이 있는 코스 형상이 상당히 다르므로, 오프라인 BeT 정책인 \\(\\alpha=0.2\\)의 오류를 보정하기 위해 잔류 정책을 상대적으로 크게 할 수 있다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 2(f), BeTAIL(4-track)은 BeT가 궤적의 라이브러리 상에서 트레이닝되는 것을 나타낸다(Fig). 2(c))와 \\(\\alpha=0.2\\). 절제술로 우리는 또한 사전 훈련된 BeT 정책이 그림 1에서 사용되는 BeTAIL(1-트랙)과 \\(\\알파=0.2\\)을 비교한다. 2(a)/b는 하류 환경에서 레이싱 하드 타이어보다 마찰 계수가 낮은 레이싱 미디엄 타이어로 수집된 시연이다.\n' +
      '\n' +
      '이전의 도전과 마찬가지로, BeTAIL(1-트랙)과 BeTAIL(4-트랙)은 모두 AIL 단독보다 훨씬 더 빠르게 트랙을 일관되게 탐색하는 것을 학습하는 정책을 학습할 수 있다. BeTAIL(4-track)은 AIL보다 더 빠른 랩 타임과 더 부드러운 조향을 달성하여 트랙 라이브러리에서 BeT 사전 훈련이 새로운 트랙을 만날 때 도움이 될 수 있음을 나타낸다. BeTAIL(1-track)은 BeTAIL(4-track)에 비해 서로 다른 차량 동역학 사이에서 이동할 때 약간의 성능 저하를 보인다. 그러나 여전히 BeTAIL(1-track)은 AIL 단독보다 빠른 높은 성공률을 달성하기 위해 훈련을 가속화한다. 비디오는 당사 웹사이트에서 제공됩니다. 모든 에이전트가 트랙의 시작 부분에서 고군분투하는 동안 BeTAIL과 AIL은 복잡한 트랙 기하학과 언덕에도 불구하고 트랙을 빠르게 탐색할 수 있다. 그러나, AIL은 여전히 코너로 이어지는 바람직하지 않은 흔들림 거동을 나타내는 반면, BeTAIL은 각 코너를 매끄럽고 최고 속도로 항해한다.\n' +
      '\n' +
      '도전의 결과는 BeTAIL이 AIL로 학습을 가속화하기 위한 강력한 방법임을 나타낸다. 모든 도전과 절제에 대해, BeTAIL은 훈련 초기에 높은 완성 랩 성공률을 일관되게 달성할 수 있다. 이것은 그림 1의 상단에서 평균의 큰 편차와 감소로 표시되는 바와 같이 더 느리고 불안정한 AIL의 훈련과 대조된다. 2(d)-f. 따라서, BeTAIL은 BeT 정책이 동일한 트랙 또는 많은 트랙들의 라이브러리 상에서 트레이닝될 때 AIL 및 BeT 단독의 성능을 능가할 수 있다.\n' +
      '\n' +
      '## VII Conclusion\n' +
      '\n' +
      '잔여적 적대적 모방 학습 정책을 지원하는 행동 트랜스포머인 BeTAIL을 사용하여 시퀀스 모델링과 온라인 모방 학습을 활용하여 인간 시연에서 레이싱 정책을 학습할 수 있도록 제안하였다. 고충실도 레이싱 시뮬레이터 그란 투리스모 스포츠의 세 가지 실험에서 BeTAIL이 단일 트랙에서 시연과 트랙 라이브러리를 모두 활용하여 제한된 시연으로 보이지 않는 트랙에서 다운스트림 AIL을 가속화할 수 있음을 보여준다. BeTAIL 정책은 인간의 조향 행동을 연상시키는 보다 부드러운 조향을 나타내며, 또한 레이싱 랩을 안정적으로 완성하는 일관된 정책을 낳는다. 또한, 작은 절제에서 BeTAIL이 다운스트림 잔류 AIL보다 다른 타이어와 트랙으로 BeT를 훈련할 때 사소한 역학 이동에서도 학습을 가속화할 수 있음을 보여준다.\n' +
      '\n' +
      'BeTAIL은 온라인 모조 학습을 별도의 사전 학습된 BeT 네트워크로 공식화한다\n' +
      '\n' +
      '도. 4: 라고 마지오레 챌린지에 대한 에이전트 궤적. 우리는 AIL과 BeTAIL을 의도적으로 초기화하여 인간 시연보다 낮은 초기 속도로 시작한다. 자동차 드로잉은 차량의 위치에 배치되고 0.4초마다 향합니다. 애니메이션 버전은 웹사이트를 참조하십시오.\n' +
      '\n' +
      '각각이 개별 훈련을 필요로 하는 잔여 AIL 정책 네트워크를 포함하는, 시스템. 또한, 잔차 AIL 공식은 BeT에서 시퀀스 모델링을 활용하기보다는 잔차 정책과 판별자 네트워크가 마르코프 방식으로 단일 시간 단계에 적용된다고 가정한다. 향후 작업에서는 온라인 롤아웃을 사용하여 BeT 액션 예측 자체를 개선하는 대체 이론적 프레임워크에 관심이 있다. 또한, 단일 단계 상태 행동 점유 측정 대신 정책 및 전문가의 궤적 시퀀스와 일치하도록 시퀀스 모델링이 AIL 프레임워크에 어떻게 도입될 수 있는지 관심이 있다. BeTAIL은 학습을 가속화하고 기존 방법에 비해 낮은 랩 타임을 달성하지만, 여전히 BeTAIL의 랩 타임과 전문가 시연에서 달성된 랩 타임 사이에는 작은 격차가 존재한다. 향후 연구에서는 개선된 제형 또는 더 긴 훈련 및 증가된 환경 상호 작용으로 이 격차를 줄이는 방법을 탐구할 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]A. A. Barham, M. 샤르마 Rhinehart, and K. M. Kitani (2018) Directed-info gail: learning hierarchical policies from unsegmented demonstration using directed information. ArXiv:1810.01266. 인용: SSII-A.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'a, A. Zhou, P. Abbeel, S. Levine (2018) Soft actor-critic: off-policy maximum entropy deep reinforcement learning with stochastic actor. Intl에서. Conf. Machine Learning, pp. 1861-1870. Cited by: SSII-A.\n' +
      '*[39]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. Adv. 신경 정보입니다 Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '*[40]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. Adv. 신경 정보입니다 Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '*[41]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. Adv. 신경 정보입니다 Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '*[42]T. B. Brown, D. Hoornaert, and M. 카카모(2020) 자율 주행 자동차 차량 제어를 위한 잔여 정책 학습. ArXiv:2302.07035. 인용: SSII-A.\n' +
      '*[43]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. Adv. 신경 정보입니다 Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '*[44]T. B. Brown, D. Hoornaert, and M. 카카모(2020) 자율 주행 자동차 차량 제어를 위한 잔여 정책 학습. ArXiv:2302.07035. 인용: SSII-A.\n' +
      '*[45]T. B. Brown(2017) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. ArXiv:1709.01177. 인용: SSII-A.\n' +
      '*[46]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. Adv. 신경 정보입니다 Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '*[47]T. B. Brown, D. Hoornaert, and M. 카카모(2020) 자율 주행 자동차 차량 제어를 위한 잔여 정책 학습. ArXiv:2302.07035. 인용: SSII-A.\n' +
      '*[48]T. B. Brown(2017) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. ArXiv:1709.01177. 인용: SSII-A.\n' +
      '*[49]T. B. Brown(2018) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. ArXiv:1812.06298. 인용: SSII-A.\n' +
      '*[50]T. B. Brown, D. Hoornaert, and M. 카카모(2020) 자율 주행 자동차 차량 제어를 위한 잔여 정책 학습. ArXiv:2302.07035. 인용\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>