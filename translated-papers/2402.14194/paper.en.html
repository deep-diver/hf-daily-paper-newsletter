<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay\n' +
      '\n' +
      'Catherine Weaver\\({}^{1}\\), Chen Tang\\({}^{1,2}\\), Ce Hao\\({}^{1,3}\\), Kenta Kawamoto\\({}^{4}\\), Masayoshi Tomizuka\\({}^{1}\\), Wei Zhan\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Department of Mechanical Engineering, University of California Berkeley, CA, USA. Contact: catherine22@berkeley.edu\\({}^{2}\\)Department of Computer Science, University of Texas, Austin, TX, USA.\\({}^{3}\\) School of Computing, National University of Singapore, Singapore\\({}^{4}\\)Sony Research, Tokyo, Japan.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environment dynamics. We test BeTAIL on three challenges with expert-level demonstrations of real human gameplay in Gran Turliano Sport. Our proposed residual BeTAIL reduces environment interactions and improves racing performance and stability, even when the BeT is pretrained on different tracks than downstream learning. Videos and code available at: [https://sites.google.com/berkeley.edu/BeTAIL/home](https://sites.google.com/berkeley.edu/BeTAIL/home).\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'Autonomous racing is of growing interest to inform controller design at the limits of vehicle handling and provide a safe alternative to racing with human drivers. An autonomous racer\'s driving style should resemble _human-like racing behavior_ in order to behave in safe and predictable ways that align with the written and social rules of the race [1, 2]. Deep reinforcement learning (RL) outperforms expert human players but requires iterative tuning of dense rewards policy selection [1]. In fact, reward shaping in RL is often performed with ad hoc trial and error [3]. Imitation learning (IL) is a potential solution that mimics the behavior of experts using offline demonstrations [4]. In this work, we examine the challenges with IL in human racing gameplay, and we propose a novel solution to capture non-Markovian decision-making and learn a high-performing online policy in the Gran Turismo Sport (GTS) racing game.\n' +
      '\n' +
      'Modeling environmental dynamics and human decision-making is essential for learning human racing strategies [5]. However, the performance of Markovian policies can deteriorate with human demonstrations [6]. Sequence-based transformer architectures [7, 8], which are remarkably successful in language models [9], accurately model the complex dynamics of human thought [10] while still relying on the environment reward for finetuning policies. The Behavior Transformer (BeT) [11], and Trajectory Transformer [8], which do not require rewards, are casually conditioned on the past, so they are better at modeling long-term environment dynamics [8]. Policies are trained via supervised learning to autoregressively maximize the likelihood of trajectories in the offline dataset. Thus, policies are limited by dataset quality [12] and are sensitive to variations in environment dynamics and states out of the dataset distribution.\n' +
      '\n' +
      'Adversarial Imitation Learning (AIL) [13] overcomes these issues with offline learning by leveraging adversarial training and reinforcement learning. A trained discriminator network encourages the agent to match the state occupancy of online rollouts and expert trajectories, reducing susceptibility to distribution shift problems when encountering unseen states [4] or minor variations in environment dynamics. However, AIL requires extensive environmental interactions when training a policy from scratch, and its performance can deteriorate significantly when learning from human demonstrations [6]. Thus, AIL training of racing policies is unstable and sample inefficient. The resulting policies also exhibit shaky steering behavior and are prone to spinning off the track, as the policy does not capture the non-Markovian decision-making strategy of humans.\n' +
      '\n' +
      'In this work, we propose Behavior Transformer Adversarial Imitation Learning (BeTAIL), which leverages both offline sequential modeling and online occupancy-matching fine-tuning to 1.) capture the sequential decision-making process of human demonstrators and 2.) correct for out-of-distribution states or minor shifts in environment dynamics. First, a BeT policy is learned from offline human demonstrations. Then, we examine an AIL mechanism to finetune the policy to match the state occupancy of the demonstrations. BeTAIL adds a residual policy, e.g. [14], to the BeT action prediction; the residual policy refines the agent\'s actions while remaining near the action predicted by the BeT.\n' +
      '\n' +
      'Our contributions are as follows:\n' +
      '\n' +
      '1. We propose Behavior Transformer Adversarial Imitation learning (BeTAIL) to pre-train a BeT and fine-tune it with a residual AIL policy to learn complex, non-Markovian behavior from human demonstrations.\n' +
      '2. We show that when learning a racing policy from real human gameplay in Gran Turismo Sport, BeTAIL outperforms BeT or AIL alone while closely matching non-Markovian patterns in human demonstrations.\n' +
      '\n' +
      '3. We show BeTAIL when pre-training on a library of demonstrations from multiple tracks to improve sample efficiency and performance when fine-tuning on an unseen track with a single demonstration trajectory.\n' +
      '\n' +
      'In the following, we first discuss preliminaries (Section III) and then introduce our BeTAIL method (Section IV). In Section VI, we describe our three experimental challenges to model human players in GT Sport, followed by the conclusion in Section VII.\n' +
      '\n' +
      '## II Related Works\n' +
      '\n' +
      '### _Behavior Modeling_\n' +
      '\n' +
      'Our BeTAIL method relates to Behavior Modeling by learning a control policy solely from human demonstrations. Behavior modeling aims to capture the behavior of human demonstrators, which is important for robots and automated vehicles that operate in close proximity to humans [15]. AIL for behavior modeling overcomes the problem of cascading errors with traditional techniques like behavioral cloning and parametric models [16]. Latent variable space further allows researchers to model multiple distinct driving behaviors [17, 18, 19, 20]. However, sample efficiency and training stability are common problems with learning AIL policies from scratch, which is exacerbated when using human demonstrations [6] or complicated environments such as autonomous racing [1]. Augmented rewards [17, 19] or negative demonstrations [21] can accelerate training but share the same pitfalls as reward shaping in RL. BeTAIL overcomes the sample efficiency and training stability pitfalls in AIL by pretraining a BeT policy on human demonstrations to guide downstream learning of an AIL residual policy.\n' +
      '\n' +
      '### _Curriculum Learning and Guided Learning_\n' +
      '\n' +
      'In RL, rather than training a policy from scratch, structured training regimens can accelerate policy learning. Curriculum learning gradually increases the difficulty of tasks [22] and can be automated with task phasing, which gradually transitions a dense imitation-based reward to a sparse environment reward [23]. "Teacher policies" can accelerate RL by providing good but not expert actions. Policy intervention prevents unsafe actions [24, 25]. In guided policy search [26], the teacher demonstrates and guides the objective of the agent; thus, large offline policies can be distilled into lightweight RL policies [10]. BeTAIL employs residual RL policies [27, 14, 28], which can adapt to variations in the task by adding a helper policy to the teacher. Thus, the agent\'s action can be kept close to the action predicted by the original controller with a small residual action space [29, 30].\n' +
      '\n' +
      '### _Sequence Modeling_\n' +
      '\n' +
      'Sequence-based modeling in offline RL method predicts the next action in a trajectory sequence, which contains states, actions, and optionally goals. Commonly, goals are set to the return-to-go, i.e. the sum of future rewards from that time step [7, 8], but advantage conditioning improves performance in stochastic environments [31]. Goal-conditioned policies can be fine-tuned on online trajectories by exploiting automatic goal labeling [12]. Sequence models can be distilled during online rollouts into lightweight policies by using offline RL algorithms with guidance from the environment reward [10]. When goals or rewards are not available, such as in imitation learning [11, 8], the performance of offline models trained with supervised learning can suffer under distribution shift and poor dataset quality [12]. BeTAIL leverages a Behavior Transformer [11] sequence model to accelerate imitation learning, without requiring an environment reward or structure goal states.\n' +
      '\n' +
      '## III Preliminaries\n' +
      '\n' +
      '### _Problem Statement_\n' +
      '\n' +
      'We model the learning task as a Markov Process (MP) defined by \\(\\{\\mathcal{S},\\mathcal{A},T\\}\\) of states \\(s\\in\\mathcal{S}\\), actions \\(a\\in\\mathcal{A}\\), and the transition probability \\(T(s_{t},a_{t},s_{t+1}):\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\mapsto[0,1]\\). Note that, unlike RL, we do not have access to an environment reward. We have access to human expert demonstrations in the training environment consisting of a set of trajectories, \\(D_{E}=(\\tau_{0}^{E},\\tau_{1}^{E}...,\\tau_{M}^{E})\\), of states and actions at every time step \\(\\tau=(s_{t},a_{t},...)\\). The underlying expert policy, \\(\\pi_{E}\\), is unknown. The goal is to learn the agent\'s policy \\(\\pi\\), that best approximates the expert policy \\(\\pi_{E}\\). Each expert trajectory \\(\\tau^{E}\\), consists of states and action pairs:\n' +
      '\n' +
      '\\[\\tau^{E}=(s_{0},a_{0},s_{1},a_{1},\\dots,s_{N},a_{N}). \\tag{1}\\]\n' +
      '\n' +
      'The human decision-making process of the expert is unknown and likely non-Markovian [32]; thus, imitation learning performance can deteriorate with human trajectories [6].\n' +
      '\n' +
      '### _Unimodal Decision Transformer_\n' +
      '\n' +
      'The Behavior Transformer (BeT) processes the trajectory \\(\\tau_{E}\\) as a sequence of 2 types of inputs: states and actions. The original BeT implementation [11] employed a mixture of Gaussians to model a dataset with multimodal behavior. For simplicity and to reduce the computational burden, we instead use an unimodal BeT that uses a deterministic similar to the one originally used by the Decision Transformer [7]. However, since residual policies can be added to black-box policies [14], BeTAIL\'s residual policy could be easily added to the k-modes present in the original BeT implementation.\n' +
      '\n' +
      'At timestep \\(t\\), the BeT uses the tokens from the last \\(K\\) timesteps to generate the action \\(a_{t}\\), where \\(K\\) is referred to as the context length. Notably, the context length during evaluation can be shorter than the one used for training. The BeT learns a deterministic policy \\(\\pi_{\\text{BeT}}(a_{t}|\\mathbf{s}_{-K,t})\\), where \\(\\mathbf{s}_{-K,t}\\) represents the sequence of \\(K\\) past states \\(\\mathbf{s}_{\\max(1,t-K+1):t}\\). The policy is parameterized using the minGPT architecture [33], which applies a causal mask to enforce the autoregressive structure in the predicted action sequence.\n' +
      '\n' +
      'A notable strength of BeT is that the policy can model non-Markovian behavior; in other words, rather than modeling the action probability as \\(P(a_{t}|s_{t})\\), the policy models the probability \\(P(a_{t}|s_{t},s_{t-1},...,s_{t-h+1})\\). However, the policy is trained using _only_ the offline dataset, and even minor differences between the training data and evaluation environment can lead to large deviations in the policies\' performance [4].\n' +
      '\n' +
      '## IV Behavior Transformer-Assisted Adversarial Imitation Learning\n' +
      '\n' +
      'We now present Behavior Transformer-Assisted Adversarial Imitation Learning (BeTAIL), summarized in Fig. 1. First, an unimodal version of a causal Behavior Transformer (BeT) [11] is trained on the offline demonstrations to capture the sequential decision-making of the human experts. Then, the BeT policy is used as the deterministic base policy for Adversarial Imitation Learning (AIL) fine-tuning. BeTAIL uses AIL to train a residual policy [29], which is added to the base policy\'s action to form the agent\'s policy. Thus, BeTAIL combines offline sequential modeling and online occupancy-matching fine-tuning to capture the decision-making process of human demonstrators and adjust for out-of-distribution states or environment changes.\n' +
      '\n' +
      '### _Behavior Transformer (BeT) Pretraining_\n' +
      '\n' +
      'A unimodal BeT policy \\(\\hat{a}=\\pi_{\\mathrm{BeT}}(a_{t}|\\mathbf{s}_{-K,t})\\) is used to predict a base action \\(\\hat{a}\\). Following [7, 12], sub-trajectories of length \\(K\\) in the demonstrations are sampled uniformly and trained to minimize the difference between the action predicted by the BeT and the next action in the demonstration sequence. While the action predicted by the BeT, \\(\\hat{a}\\), considers the previous state history \\(\\mathbf{s}_{-K,t}\\), the action may not be ideal in stochastic environments [31]. Thus, we correct the base action by adding a residual action [14] that can correct the base action based on how well the agent performs in the environment. Since an environment reward is not available, and we wish to replicate the human demonstrations, we employ AIL to train the residual policy.\n' +
      '\n' +
      '### _Residual Policy Learning for Online Fine-tuning_\n' +
      '\n' +
      'The agent\'s action is the sum of the action specified by the BeT and the action from a residual policy [14]. The residual policy corrects or improves the actions from the BeT base policy [14]. First, we define an augmented MP: \\(\\tilde{\\mathcal{M}}=\\{\\tilde{\\mathcal{S}},\\tilde{\\mathcal{A}},\\tilde{T}\\}\\). The state space \\(\\tilde{\\mathcal{S}}\\) is augmented with the base action: \\(\\tilde{s}_{t}\\doteq\\begin{bmatrix}s_{t}&a_{t}\\end{bmatrix}^{\\intercal}\\). The action space, \\(\\tilde{A}\\), is the space of the residual action \\(\\tilde{a}\\). The transition probability, \\(\\tilde{T}(\\tilde{s}_{t},\\tilde{a}_{t},\\tilde{s}_{t+1})\\), includes both the dynamics of the original environment, i.e. \\(T(s_{t},a_{t},s_{t+1})\\), and also the dynamics of the base action \\(\\hat{a}_{t}=\\pi_{\\mathrm{BeT}}(\\cdot|\\mathbf{s}_{-K,t})\\).\n' +
      '\n' +
      'We define the Gaussian \\(\\hat{a}\\sim f_{\\mathrm{res}}(\\hat{a}|s_{t},\\hat{a}_{t})=\\mathcal{N}(\\mu,\\sigma)\\) as the residual policy. The residual policy uses the current state and the action predicted by the base policy to select the residual action. Then, this residual action is added to the base action. This sum, \\(a_{t}\\), is the action that is taken in the environment:\n' +
      '\n' +
      '\\[a_{t}=\\hat{a}_{t}+\\mathrm{clip}(\\tilde{a}_{t},-\\alpha,\\alpha). \\tag{2}\\]\n' +
      '\n' +
      'The residual policy is constrained to be between \\([-\\alpha,\\alpha]\\), which allows constraint on how much the environment action \\(a_{t}\\) is allowed to deviate from the base action \\(\\hat{a}\\)[29, 14]. In the case where \\(\\alpha\\) is small, the environment action must still be close to the base action, \\(\\hat{a}_{t}\\), predicted by the BeT.\n' +
      '\n' +
      'Assuming the action space of \\(f_{\\theta}(\\cdot|s_{t},\\hat{a}_{t})\\) is restricted to the range \\([-\\alpha,\\alpha]\\), we can define the policy \\(\\pi_{A}(a|\\mathbf{s}_{-k,t})\\):\n' +
      '\n' +
      '\\[\\pi_{A}(a|\\mathbf{s}_{-k,t})= \\tag{3}\\] \\[\\tilde{a}_{t}|_{\\hat{a}_{t}\\sim f_{\\mathrm{res}}(\\cdot|s_{t}, \\pi_{\\mathrm{BeT}}(\\cdot|s_{-k,t}))}+\\pi_{\\mathrm{BeT}}(\\cdot|\\mathbf{s}_{-k,t}).\\]\n' +
      '\n' +
      'Eq. 3 follows the notation of [34]. Because the BeT policy is non-Markovian, the agent\'s policy \\(\\pi_{A}\\) is also non-Markovian. However, using the definition of the augmented state, \\(\\tilde{s}\\), the input to the AIL residual policy is the current state and the base action predicted by the BeT. Thus, \\(\\pi_{A}\\) is in fact Markovian with respect to the augmented state \\(\\tilde{s}\\):\n' +
      '\n' +
      '\\[\\pi_{A}(a|s_{t},\\hat{a}_{t})=\\tilde{a}_{t}|_{\\hat{a}_{t}\\sim f_{\\mathrm{res} }(\\cdot|s_{t},\\hat{a}_{t})}+\\hat{a}_{t}. \\tag{4}\\]\n' +
      '\n' +
      'Following previous works with residual policies [14, 34], during online training, the agent\'s policy during rollouts is given by \\(\\pi_{A}\\) in (4). Namely, the agent\'s policy is the sum of the action given by the base policy \\(\\pi_{\\mathrm{BeT}}\\) and the action given by the residual policy \\(f_{\\mathrm{res}}\\) according to Eq. (3). After initial training (Section IV-A), the deterministic base policy, \\(\\pi_{\\mathrm{BeT}}\\), is frozen during online training, and the agent\'s policy is improved by updating the residual policy \\(f_{\\mathrm{res}}\\) with AIL.\n' +
      '\n' +
      '### _Residual Policy Training with AIL_\n' +
      '\n' +
      'To train the residual policy, we adapt Adversarial Imitation Learning (AIL) [35] so that the agent\'s policy in online rollouts solves the AIL objective. However, AIL is typically defined in terms of a single Markovian policy that is not added to a base policy [13]. In this section, we detail our updates to the AIL formulation to employ AIL to update the residual policy \\(f_{\\mathrm{res}}\\). Given the definition of \\(\\pi_{A}\\) (4), we define the AIL objective for residual policy learning as\n' +
      '\n' +
      '\\[\\underset{f_{\\mathrm{res}}}{\\mathrm{minimize}}\\quad D_{\\mathrm{JS}}\\left( \\rho_{\\pi_{A}},\\rho_{\\pi_{E}}\\right)-\\lambda H(f_{\\mathrm{res}}). \\tag{5}\\]\n' +
      '\n' +
      'Similar to the standard AIL objective [35], we still aim to minimize the distance between the occupancy measures of the expert\'s and agent\'s policies, denoted as \\(\\rho_{\\pi_{E}}\\) and \\(\\rho_{\\pi_{A}}\\) respectively in Eq. (5). The minimization with respect to \\(f_{\\mathrm{res}}\\) is valid since Eq. (4) simply defines a more restrictive class of policies than standard single-policy AIL. Since\n' +
      '\n' +
      'Fig. 1: BeTAIL rollout collection. The pre-trained BeT predicts action \\(\\hat{a}_{t}\\) from the last \\(H\\) state-actions. Then the residual policy specifies action \\(\\tilde{a}_{t}\\) from the current state and \\(\\hat{a}_{t}\\), and the agent executes \\(a_{t}=\\hat{a}_{t}+\\tilde{a}_{t}\\) in the environment.\n' +
      '\n' +
      'the contribution from the base policy is deterministic, we regularize the problem using only the entropy of the residual policy, and the policy update step is replaced with\n' +
      '\n' +
      '\\[\\max_{f_{\\text{res}}}\\,\\mathop{\\mathbb{E}}_{\\tilde{\\tau}\\sim f_{\\text{res}}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^{t}\\Bigg{(}\\tilde{R}^{E}\\left(\\tilde{s}_{t},\\tilde{a}_ {t}\\right)+\\lambda H\\left(f_{\\text{res}}\\left(\\cdot\\mid\\tilde{s}_{t}\\right) \\right)\\Bigg{)}\\right], \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\tilde{\\tau}\\) represents the trajectory \\(\\tilde{\\tau}=(\\tilde{s}_{0},\\tilde{a}_{0},...)\\) collected using \\(f_{\\text{res}}\\) on \\(\\tilde{\\mathcal{M}}\\). Thus, the residual policy \\(f_{\\text{res}}\\) is updated using the augmented state and residual action. We define \\(\\tilde{R}^{E}\\) as the proxy environment reward on \\(\\tilde{M}\\). Following previous works with residual policy learning for RL [14], the proxy reward is calculated using the action taken in the environment. To similarly adapt AIL\'s proxy environment reward, we define\n' +
      '\n' +
      '\\[\\tilde{R}^{E}(\\tilde{s}_{t},\\tilde{a}_{t})=-\\log\\left(1-D_{\\omega}^{E}(s,\\hat{ a}+\\tilde{a})\\right), \\tag{7}\\]\n' +
      '\n' +
      'where \\(D_{\\omega}^{E}(s,\\hat{a}+\\tilde{a})\\) is a binary classifier trained to minimize\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{D,\\mathcal{D}_{E}}(\\omega)& =-E_{\\tau_{E}\\sim\\mathcal{D}_{E}}\\left[\\log\\left(D_{\\omega}^{E}(s,a )\\right)\\right]\\\\ &\\qquad-E_{\\tilde{\\tau}\\sim f_{\\text{res}}}\\left[\\log\\left(1-D_{ \\omega}^{E}(s,\\hat{a}+\\tilde{a})\\right)\\right],\\end{split} \\tag{8}\\]\n' +
      '\n' +
      'which is equivalent to\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{D,\\mathcal{D}_{E}}(\\omega)& =-E_{\\tau_{E}\\sim\\mathcal{D}_{E}}\\left[\\log\\left(D_{\\omega}^{E}(s,a)\\right)\\right]\\\\ &\\qquad-E_{\\tau\\sim\\pi_{a}}\\left[\\log\\left(1-D_{\\omega}^{E}(s,a )\\right)\\right].\\end{split} \\tag{9}\\]\n' +
      '\n' +
      'Eq. (7) implies that give the pair \\((\\tilde{s},\\tilde{a})\\), the environment reward is the probability that the state-action pair \\((s,a)=(s,\\hat{a}+\\tilde{a})\\) comes from the expert policy, according to the discriminator. Thus, by iterating between the policy learning objective (6) and the discriminator loss (9), AIL minimizes (5) and determines the \\(f_{\\text{res}}\\) that allows \\(\\pi_{A}\\) to closely match the occupancy measure of the expert \\(\\pi_{E}\\).\n' +
      '\n' +
      '## V Imitation of Human Racing Gameplay\n' +
      '\n' +
      'We describe the method to learn from human gameplay in the GTS racing game, including the state features, environment, and training regime for three IL challenges.\n' +
      '\n' +
      '### _State Feature Extraction and Actions_\n' +
      '\n' +
      'Our selected features include features that were previously shown as important in RL and are available in the demonstrations. We include the following state features exactly as described in [36]: 1.) The linear velocity, \\(\\mathbf{v}_{t}\\in\\mathbb{R}^{3}\\), and linear acceleration \\(\\hat{\\mathbf{v}}_{t}\\in\\mathbb{R}^{3}\\) with respect to the inertial frame of the vehicle. 2.) The Euler angle \\(\\theta_{t}\\in(-\\pi,\\pi]\\) between the 2D vector that defines the agent\'s rotation in the horizontal plane and the unit tangent vector that is tangent to the centerline at the projection point. 3.) A binary flag with \\(w_{t}=1\\) indicating wall contact 4.) N sampled curvature measurement of the course centerline in the near future \\(\\mathbf{c}_{t}\\in\\mathbb{R}^{N}\\).\n' +
      '\n' +
      'Additionally, we select features similar to those used in [1]: 5.) The cosine and sine of the vehicle\'s current heading, \\(\\cos(\\psi)\\) and \\(\\sin(\\psi)\\). 6.) The relative 2D distance from the vehicle\'s current position to the left, \\(\\mathbf{e}_{LA,l}\\), right, \\(\\mathbf{e}_{LA,r}\\), and center, \\(\\mathbf{e}_{LA,c}\\), of the track at 5 "look-ahead points." The look-ahead points (blue dots in Fig. 2) are placed evenly using a look-ahead time of 2 seconds, i.e., they are spaced evenly over the next 2 seconds of track, assuming the vehicle maintains its current speed. Thus the full state is a vector composed of \\(s_{t}=[\\mathbf{v}_{t},\\hat{\\mathbf{v}}_{t},\\theta_{t},w_{t},\\mathbf{c}_{t}, \\cos(\\psi),\\sin(\\psi),\\mathbf{e}_{LA,l},\\mathbf{e}_{LA,r},\\mathbf{e}_{LA,c}]\\). Finally, the state is normalized using the mean and standard deviation of each state feature in the demonstrations.\n' +
      '\n' +
      'GTS receives the steering command \\(\\delta\\in[-\\pi/6,\\pi/6]\\) rad and a throttle-brake signal \\(\\omega_{r}\\in[-1,1]\\) where \\(\\omega_{r}=1\\) denotes full throttle and \\(\\omega_{r}=-1\\) denotes full brake [36, 1]. For all baseline comparisons, the steering command in the demonstrations is scaled to be between \\([-1,1]\\). The agent specifies steering actions between \\([-1,1]\\), which are scaled to \\(\\delta\\in[-\\pi/6,\\pi/6]\\) before being sent to GTS.\n' +
      '\n' +
      'In the case where a residual policy is learned (see BeTAIL and BCAIL in the next section), the residual policy network predicts \\(\\tilde{a}\\in[-1,1]\\), and then \\(\\tilde{a}\\) is scaled1 to be between \\([-\\alpha,\\alpha]\\) and then added to \\(a=\\hat{a}+\\tilde{a}\\). Since \\(\\hat{a}\\in[-1,1]\\), it is possible for \\(a\\) to be outside the bounds of \\([-1,1]\\), so we clip \\(a\\) before sending the action to GTS. The choice of the hyperparameter \\(\\alpha\\) determines the maximum magnitude of the residual action. Depending on the task and the strength of the base policy, \\(\\alpha\\) can be relatively large to allow the residual policy to correct for bad actions [34], or \\(\\alpha\\) can be relatively small to ensure the policy does not deviate significantly from the base policy [14, 30]. In this work, we set \\(\\alpha\\) as small as possible to achieve high performance for each task so that the policy remains as close as possible to the offline BeT, which captures non-Markovian racing behavior.\n' +
      '\n' +
      'Footnote 1: The choice to scale the residual policy between [-1,1] is made based on the practice of using scaled action spaces with SAC [37]\n' +
      '\n' +
      '### _Environment and Data Collection_\n' +
      '\n' +
      '#### V-B1 Gran Turismo Sport Racing Simulator\n' +
      '\n' +
      'We conduct experiments in the high-fidelity PlayStation (PS) game Gran Turismo Sport (GTS) ([https://www.gran-turismo.com/us/](https://www.gran-turismo.com/us/)), developed by Polyphony Digital, Inc. GTS takes two continuous inputs: throttle/braking and steering. The vehicle positions, velocities, accelerations, and pose are observed. The agent\'s and demonstrator\'s state features and control inputs are identical. Deep RL recently achieved super-human performance against human opponents in GTS [1]. However, this success required tuned, dense reward functions in order for the racing agent to behave "well" with human opponents. Rather than carefully crafting a reward function, we explore whether demonstrations from expert human drivers can inform a top-performing agent.\n' +
      '\n' +
      '#### V-B2 Agent Rollout and Testing Environments\n' +
      '\n' +
      'To collect rollouts and evaluation episodes, the GTS simulator runs on a PlayStation, while the agent runs on a separate desktop computer. The desktop computer communicates with GTS over a dedicated API via an Ethernet connection similar to the one described in [36]. The API provides the current state of up to 20 simulated cars and accepts car control commands, which are active until the next command is received. While the GTS state is updated every 60Hz, we limit the control frequency of our agent to 10Hz to reduce the load on the PS and the desktop computers following previous works in RL [36, 1]. Unlike previous works that trained a single policy network and collected rollouts in parallel, we opt to train and collect rollouts sequentially to reduce the computational burden on the desktop PC, given our framework has two networks: the BeT policy and the residual policy.\n' +
      '\n' +
      '### _Training Scheme for Imitation Learning_\n' +
      '\n' +
      'We evaluate BeTAIL on three challenges with different demonstration availability and environment setups, as indicated in the top of Fig. 3. For a more detailed description of each challenge, see the corresponding part in Section VI.\n' +
      '\n' +
      '#### V-C1 Algorithms and Baselines\n' +
      '\n' +
      'For each challenge, the BeT policy, \\(\\pi_{\\text{BeT}}(\\cdot|\\mathbf{s}_{-K,t})\\), is pretrained on offline human demonstrations from one or more tracks. Then our **BeTAIL** fine-tunes control by training an additive residual policy, \\(f_{\\text{res}}\\), with AIL on the downstream environment and human demonstrations. The residual policy is constricted between \\([-\\alpha,\\alpha]\\), where \\(\\alpha\\) is specified individually for each challenge. In Fig. 2(a)-c, red corresponds to demonstrations for BeT policy and \\(\\mathrm{green}\\) corresponds to the downstream environment and demonstrations for the residual AIL policy. The **BeT** baseline uses the BeT policy, \\(\\pi_{\\text{BeT}}(\\cdot|\\mathbf{s}_{-K,t})\\), with the pretraining demonstrations red. Conversely, the **AIL** baseline consists of a single policy, \\(\\pi_{A}(a|s)\\), trained via AIL using only the downstream demonstrations and environment in \\(\\mathrm{green}\\). The **BCAIL** baseline trains a BC policy, \\(\\pi_{A}(a|s)\\) on the offline demonstrations (in red) and then fine-tunes a residual policy in the downstream environment (in \\(\\mathrm{green}\\)) following the same training scheme as BeTAIL. However, given the underperformance of both BC and BCAIL in the first challenge, we exclude the comparison of BCAIL in the other, more difficult challenges.\n' +
      '\n' +
      'Agents use soft actor-critic (SAC) [37] for the reinforcement learning algorithm in adversarial learning (i.e. for AIL, BCAIL, and BeTAIL). Since SAC is off-policy, this implies that the replay buffer can include historical rollouts from multiple iterations prior. However, the discriminator network, and thus the reward associated with a state-action pair may have changed. Thus, when sampling data from the replay buffer for policy and Q-network training, we re-calculate the reward for the associated state-action pair using the current discriminator network. Finally, discriminator overfitting can cause AIL to fail to learn a meaningful policy if the discriminator is too powerful. For all AIL methods, we employ two regulators when training the discriminator: gradient penalty and discriminator entropy regularization [6].\n' +
      '\n' +
      '#### V-C2 Demonstration Overview\n' +
      '\n' +
      'All demonstrations were recorded from _different_ human players GTS participating in real gameplay, "time-trial" competitions with the Audi TT Cup vehicle. All trajectories are high-quality and performed by an expert-level racer. Each demonstration contains a full trajectory of states and actions, around a single lap of the track. The trajectories were recorded at a frequency of 60Hz, which we downsample to 10Hz (the agent\'s control frequency). Each trajectory (i.e. lap) contains approximately 7000 timesteps. As shown in Fig. 2(a)-c, we test three training schemes to test if BeTAIL can improve performance on the same track as pretraining, transfer a single-track BeT policy to a new track, and transfer a multi-track BeT policy to an unseen track (see Section VI for more information).\n' +
      '\n' +
      '## VI Racing Experiment Results\n' +
      '\n' +
      'We introduce three challenges with distinct pretraining demonstrations and downstream environments to test our BeTAIL formulation. For evaluation, 20 cars are randomly placed evenly on the track. Each car\'s initial speed is set to the expert\'s speed at the nearest position on a demonstration trajectory. Each car has 500 seconds to complete a full lap. In Fig. 2(d)-f, we provide two evaluation metrics during training: the proportion of cars that finish a lap (top) and the average lap times of cars that finish. Error bars show the total standard deviation across the 20 cars (evaluation episodes) and 3 seeds. Fig. 2(g)-i provide the mean\\(\\pm\\)standard deviation of the best policy\'s lap time and absolute change in steering \\(|\\delta_{t}-\\delta_{t-1}|\\), as RL policies can exhibit undesirable shaky steering behavior [1] not present in human experts. Videos of the trajectories and GTS game environment are provided at [https://sites.google.com/berkeley.edu/BeTAIL/home](https://sites.google.com/berkeley.edu/BeTAIL/home). In the following, we detail the setup of each challenge and analyze the results.\n' +
      '\n' +
      '### _Lago Maggiore Challenge: Fine-tuning in the same environment with BeTAIL_\n' +
      '\n' +
      'The first experiment tests how well BeTAIL can fine-tune the BeT policy when the BeT policy is learned from demonstrations on the same track as the downstream environment (Fig. 2(a)). Since the BeT policy is pretrained on the same environment, BeTAIL(0.05) employs a small residual policy, \\(\\alpha=0.05\\). The ablation BeTAIL(1.0) uses a large residual policy, \\(\\alpha=1.0\\). There are 49 demonstrations from different human players on the Lago Maggiore track. The BeT policy is trained on the 49 trajectories, then we run BeTAIL on the same 49 trajectories and the same environment as the demonstrations. BCAIL follows the same training scheme (\\(\\alpha=0.05\\)). AIL is trained on the same 49 trajectories.\n' +
      '\n' +
      'Fig. 2(d) shows the evaluation of each agent during training. BeTAIL performs the best of all the methods and rapidly learns a policy that can consistently finish a full lap and achieve the lowest lap time. The AIL baseline can eventually learn a policy that navigates the track and achieves a similarly low lap time; however, even towards the end of the training,\n' +
      '\n' +
      'Fig. 2: Track Lookahead Points on Lago Maggiorethe AIL policies are less consistent, and multiple cars may fail to finish a full lap during evaluation, as evidenced by the higher standard deviation in the number of finished laps in the top of Fig. (d)d. The remaining baselines perform poorly on this task. Using a residual BC policy (BCAIL) actually worsens performance, likely because the BC policy performs poorly in an unseen environment due to distribution shifts. Impressively, the BeT policy can finish some laps even though it is trained exclusively on offline data; however, the lap time is poor compared to BeTAIL and AIL, which are able to use online rollouts to correct any issues in the policy.\n' +
      '\n' +
      'In Fig. (g)g, BeTAIL(1.0) can achieve a better lap time than BeTAIL(0.05), since larger \\(\\alpha\\) allows the residual policy to apply more correction to the BeT base policy. However, BeTAIL(1.0) is prone to oscillating steering back and forth, as indicated by the higher deviation in the steering command between timesteps. We hypothesize that it is because the non-Markovian human-like behavior diminishes as \\(\\alpha\\) becomes larger. It is supported by the observation that the Markovian AIL policy also tends to have extreme changes in the steering command. In Fig. 4, We provide further insight into how the extreme steering rates of the AIL policy harm its robustness. We deliberately initialize the race cars at a significantly lower speed than the human demonstrator before the corner for AIL and BeTAIL. We aim to test the robustness of learned agents by monitoring their stability at unseen states. The heading of the AIL agent oscillates due to its tendency to rapidly steer back and forth, which causes the AIL agent to lose control of\n' +
      '\n' +
      'Fig. 3: Experimental results on three racing challenges. (a) Lago Maggiore challenges pretrains the BeT on the same demonstrations and downstream environments. (b) Dragon Tail transfers the BeT policy to a new track with BeTAIL finetuning. (c) The Mount Panorama challenge pretrains the BeT on a library of 4 tracks, and BeTAIL finetunes on an unseen track. (d)-(f) evaluation of mean (std) success rate to finish laps and mean (std) of lap times. (g)-(i) Best policy’s mean \\(\\pm\\) std lap time and change in steering from previous time step.\n' +
      '\n' +
      'the vehicle and collide with the corner. Conversely, BeTAIL smoothly accelerates and brakes into the corner.\n' +
      '\n' +
      '### _Dragon Tail Challenge: Transferring the BeT policy to another track with BeTAIL_\n' +
      '\n' +
      'The second experiment tests how well BeTAIL can fine-tune the BeT policy when the downstream environment is different from the BeT demonstrations (Fig. 2(b)). The same BeT policy is used in the Lago Maggiore Challenge; however, the downstream environment is a different track, and there are only 12 demonstrations for the new Dragon Trail track. For BeTAIL(0.10), we allow the residual policy to be slightly larger, \\(\\alpha=0.10\\), since the downstream environment is different than the BeT pretraining demonstrations. The vehicle dynamics are unchanged.\n' +
      '\n' +
      'The results for training and evaluating the policies on the Dragon Tail track are given in Fig. 2(e)/h. Again, BeTAIL can employ BeT to guide policy learning; as a result, BeTAIL quickly learns to consistently navigate the track at a high speed. Additionally, small \\(\\alpha\\) can ensure that non-Markovian human behavior is preserved, resulting in smooth steering. Conversely, AIL can learn policies that are capable of achieving low lap times; however, they are significantly more prone to rapid changes in steering and much more prone to fail to finish a lap (top in Fig. 2(e)). The pretrained BeT, which was trained from demonstrations on a different track, is unable to complete any laps.\n' +
      '\n' +
      '_Mount Panorama Challenge: Learning a multi-track BeT policy and solving an unseen track with BeTAIL_\n' +
      '\n' +
      'Finally, a BeT policy is trained on a library of trajectories on four different tracks: Lago Maggiore GP (38 laps), Autodromo de Interlagos (20 laps), Dragon Tail - Seaside (28 laps), and Brands Hatch GP (20 laps). For BeTAIL fine-tuning, BeTAIL is trained on a single demonstration on the Mount Panorama Circuit. Due to the availability of trajectories, there is a slight change in vehicle dynamics from the first two challenges due to the use of different tires (Racing Hard); the vehicle dynamics in downstream training and evaluation employ the same Racing Hard tires as the library of trajectories. The Mount Panorama circuit has a significantly different course geometry with more hills and sharp banked turns than the pretraining tracks; thus, we allow the residual policy to be relatively larger to correct for errors in the offline BeT policy, \\(\\alpha=0.2\\). In Fig. 2(f), BeTAIL(4-track) indicates the BeT is trained on the library of trajectories (Fig. 2(c)) with \\(\\alpha=0.2\\). As an ablation, we also compare BeTAIL(1-track) with \\(\\alpha=0.2\\), where the pretrained BeT policy is the one used in Fig. 2(a)/b with demonstrations collected with Racing Medium tires, which have a lower coefficient of friction than the Racing Hard tires in the downstream environment.\n' +
      '\n' +
      'As with previous challenges, both BeTAIL(1-track) and BeTAIL(4-track) can learn a policy that learns to consistently navigate the track significantly faster than AIL alone. BeTAIL(4-track) achieves faster lap times and smoother steering than AIL, indicating that BeT pre-training on a library of tracks can assist when encountering new tracks. BeTAIL(1-track) exhibits a slight performance drop when transferring between different vehicle dynamics compared to BeTAIL(4-track). However, BeTAIL(1-track) still accelerates training to achieve a high success rate faster than AIL alone. Videos are provided on our website. While all agents struggle at the beginning of the track, BeTAIL and AIL can navigate the track rapidly despite the complicated track geometry and hills. However, AIL still exhibits undesirable shaking behavior leading into corners while BeTAIL navigates each corner smoothly and with the highest speed.\n' +
      '\n' +
      'The results of the challenges indicate that BeTAIL is a powerful method for accelerating learning with AIL. For all challenges and ablations, BeTAIL can consistently achieve high finished laps success rates early in training. We contrast this to AIL\'s training, which is slower and unstable as indicated by the large deviations and drops in mean in tops of Fig. 2(d)-f. Thus, BeTAIL can outperform AIL and BeT alone when the BeT policy is trained on the same track or a library of many tracks.\n' +
      '\n' +
      '## VII Conclusion\n' +
      '\n' +
      'We proposed BeTAIL, a Behavior Transformer that is assisted with a residual Adversarial Imitation Learning policy, to leverage sequence modeling and online imitation learning to learn racing policies from human demonstrations. In three experiments in the high-fidelity racing simulator Gran Turismo Sport, we show that BeTAIL can leverage both demonstrations on a single track and a library of tracks to accelerate downstream AIL on unseen tracks with limited demonstrations. BeTAIL policies exhibit smoother steering, which is reminiscent of human steering behavior, and additionally result in consistent policies that reliably complete racing laps. Furthermore, in a small ablation, we demonstrate that BeTAIL can even accelerate learning under minor dynamics shifts when the BeT is trained with different tires and tracks than the downstream residual AIL.\n' +
      '\n' +
      '_Limitations and Future Work:_ BeTAIL formulates the online imitation learning as a separate pre-trained BeT network\n' +
      '\n' +
      'Fig. 4: Agent trajectories on Lago Maggiore Challenge. We deliberately initialize AIL and BeTAIL to start at a lower initial speed than the human demonstration. Car drawing is placed at the vehicle’s location and heading every 0.4s. See the website for the animated version.\n' +
      '\n' +
      'and a residual AIL policy network, which each require individual training. Further, residual AIL formulation assumes that the residual policy and the discriminator network are applied to single-time steps in a Markovian fashion, rather than exploiting the sequence modeling in the BeT. In future work, we are interested in an alternate theoretical framework that improves the BeT action predictions themselves using online rollouts. Additionally, we are interested in how sequence modeling could be introduced into AIL frameworks to match the policy\'s and expert\'s trajectory sequences instead of single-step state-action occupancy measures. While BeTAIL accelerates learning and achieves lower lap times compared to existing methods, there is still a small gap between BeTAIL\'s lap times and the lap times achieved in the expert demonstrations. Future work will explore how to narrow this gap with improved formulations or longer training and increased environment interactions.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]A. A. Barham, M. Sharma, N. Rhinehart, and K. M. Kitani (2018) Directed-info gail: learning hierarchical policies from unsegmented demonstrations using directed information. arXiv preprint arXiv:1810.01266. Cited by: SSII-A.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      'a, A. Zhou, P. Abbeel, and S. Levine (2018) Soft actor-critic: off-policy maximum entropy deep reinforcement learning with stochastic actor. In Intl. Conf. Machine Learning, pp. 1861-1870. Cited by: SSII-A.\n' +
      '* [39]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '* [40]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '* [41]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '* [42]T. B. Brown, D. Hoornaert, and M. Caccamo (2020) Residual policy learning for vehicle control of autonomous racing cars. arXiv preprint arXiv:2302.07035. Cited by: SSII-A.\n' +
      '* [43]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '* [44]T. B. Brown, D. Hoornaert, and M. Caccamo (2020) Residual policy learning for vehicle control of autonomous racing cars. arXiv preprint arXiv:2302.07035. Cited by: SSII-A.\n' +
      '* [45]T. B. Brown (2017) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. arXiv preprint arXiv:1709.01177. Cited by: SSII-A.\n' +
      '* [46]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Adv. Neural Inform. Processing Syst.33, pp. 1877-1901. Cited by: SSII-A.\n' +
      '* [47]T. B. Brown, D. Hoornaert, and M. Caccamo (2020) Residual policy learning for vehicle control of autonomous racing cars. arXiv preprint arXiv:2302.07035. Cited by: SSII-A.\n' +
      '* [48]T. B. Brown (2017) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. arXiv preprint arXiv:1709.01177. Cited by: SSII-A.\n' +
      '* [49]T. B. Brown (2018) Benchmarking the 2017-1000 challenge: a survey of deep reinforcement learning. arXiv preprint arXiv:1812.06298. Cited by: SSII-A.\n' +
      '* [50]T. B. Brown, D. Hoornaert, and M. Caccamo (2020) Residual policy learning for vehicle control of autonomous racing cars. arXiv preprint arXiv:2302.07035. Cited by\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>