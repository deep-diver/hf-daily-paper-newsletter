<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VisionGPT-3D: 일반화된 멀티모달 에이전트\n' +
      '\n' +
      '향상된 3D 비전 이해를 위해\n' +
      '\n' +
      'Chris Kelly\n' +
      '\n' +
      '_Stanford University_\n' +
      '\n' +
      'ckelly24@stanford.edu\n' +
      '\n' +
      'Tu Tian\n' +
      '\n' +
      '_Harvard University_\n' +
      '\n' +
      'ytian11@meei.harvard.edu\n' +
      '\n' +
      'Luhui Hu\n' +
      '\n' +
      '_Seeking AI_\n' +
      '\n' +
      '미국 캘리포니아 대학 로스앤젤레스_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_캘리포니아_대학교\n' +
      '\n' +
      'greenlake@ucla.edu\n' +
      '\n' +
      'Jiavin Hu\n' +
      '\n' +
      '미국 캘리포니아 대학 로스앤젤레스_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_LA_캘리포니아_대학교\n' +
      '\n' +
      'greenlake@ucla.edu\n' +
      '\n' +
      'Yu Tian\n' +
      '\n' +
      '_Harvard University_\n' +
      '\n' +
      'ytian11@meei.harvard.edu\n' +
      '\n' +
      'Cindy Yang\n' +
      '\n' +
      '워싱턴 DC, 시애틀_시애틀__University of Washington, Seattle_\n' +
      '\n' +
      'cindyy@uw.edu\n' +
      '\n' +
      'Deshun Yang\n' +
      '\n' +
      '_Seeking AI_\n' +
      '\n' +
      '_Seeking AI_\n' +
      '\n' +
      'Zaoshan Huang\n' +
      '\n' +
      '_Seeking AI_\n' +
      '\n' +
      '1i981354@seeking.ai\n' +
      '\n' +
      'Yuexian Zou\n' +
      '\n' +
      '_Peking University_\n' +
      '\n' +
      'zouyx@pku.edu.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트의 시각적 구성요소로의 진화는 이미지, 텍스트로부터 비디오를 생성하고 이미지 내에서 원하는 요소를 식별하는 것과 같은 사람들의 일상을 용이하게 한다. 최근 멀티모달 능력을 포함하는 컴퓨터 비전 모델은 잘 정의된 객체를 기반으로 한 이미지 검출, 분류에 초점을 맞추고 있다. 대형 언어 모델(LLM)은 자연 언어에서 시각적 객체로의 변환을 도입하며, 텍스트 컨텍스트에 대한 시각적 레이아웃을 제시한다. OpenAI GPT-4는 LLM의 정점으로 떠오른 반면, 컴퓨터 비전(CV) 도메인은 2D 이미지를 3D 표현으로 변환하는 SOTA(최첨단) 모델과 알고리즘을 과도하게 자랑한다. 그러나, 이 문제를 갖는 알고리즘들 간의 미스매칭은 원하지 않는 결과를 초래할 수 있다. 이러한 문제에 대응하여, 우리는 최첨단 비전 모델을 통합하여 비전 지향 AI의 개발을 용이하게 하기 위해 통합된 VisionGPT-3D 프레임워크를 제안한다. VisionGPT-3D는 멀티모달 기반 모델의 장점을 기반으로 다용도 멀티모달 프레임워크 구축을 제공한다. 다양한 SOTA 비전 모델을 원활하게 통합하고, SOTA 비전 모델 선택에 자동화를 가져오며, 2D 깊이 맵 분석에 대응하는 적합한 3D 메쉬 생성 알고리즘을 식별하고, 텍스트 프롬프트와 같은 다양한 멀티모달 입력을 기반으로 최적의 결과를 생성한다.\n' +
      '\n' +
      '키워드:VisionGPT-3D 비전 이해 멀티모달 에이전트\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간됨은 생생한 이야기를 언어로 묘사하고 다른 사람들이 마음속에 활기찬 환상을 개괄하게 만드는 것이 쉽다고 느낀다. AI가 해당 텍스트에서 시각적 컨텍스트를 생성하는 것은 어려울 수 있다. GPT-4와 같은 모델이 LLM에 대한 놀라운 벤치마크를 확립하고 SORA는 3D 시각적 구성요소 생성의 확장을 입증하지만 멀티모달 CV 모델 분야는 잠재력이 풍부한 끊임없이 변화하는 프론티어이다.\n' +
      '\n' +
      'VisionGPT-3D는 영상 내에서 객체를 분할하거나 잘라낼 수 있는 SAM(segment anything model), 영상 내 객체를 검출할 수 있는 YOLO(You only look once) 모델, 영상 표현 학습을 위해 제안된 자가 지도 학습 프레임워크인 DINO(self-attention을 통한 영상 표현 증류)와 같은 다수의 대규모 모델을 통합한다. 각 모델에는 장단점이 있습니다. DINO는 이미지의 서로 다른 부분 사이의 관계를 캡처할 수 있고 세밀한 객체 검출에서 좋은 성능을 얻을 수 있다. SAM은 이미지의 중요한 영역을 강조하기 위해 공간적 주의를 집중한다. YOLO는 객체 탐지 벤치마크에서 강력한 성능을 보여준다. DINO와 같은 자체 감독 모델은 계산 비용이 많이 들 수 있다. SAM은 모델 성능을 다루지 않을 수 있습니다. YOLO는 세밀한 객체 검출 작업에서도 잘 수행하지 않을 수 있다. DINO, SAM 및 YOLO 사이의 선택은 자기 지도 학습, 공간적 주의 또는 실시간 객체 검출과 같은 특정 작업에 의존할 수 있다.\n' +
      '\n' +
      'VisionGPT-3D는 여러 모델의 조인트로 최적화된 솔루션을 생성하거나 작업 유형에 따라 모델을 선택한다. 본 논문에서는 다시점 스테레오를 이용하여 3차원 영상을 VisionGPT-3D로 재구성하는 방법을 제안한다. 다시점 스테레오는 깊이 맵으로부터 조밀한 포인트 클라우드를 생성하고, 조밀한 포인트 클라우드로부터 생성된 표면 메쉬를 이용하여 삼각형을 형성하여 장면의 기하구조를 표현한다. 모션으로부터 구조는 다른 시점에서 촬영된 2차원 영상들의 집합으로부터 장면의 3차원 구조를 추정한다; 스테레오로부터의 깊이는 근접거리 물체들에 효과적인 스테레오 영상 쌍의 대응점들 간의 디스패리티를 비교한다; 포토메트릭 스테레오는 조명 방향의 변화에 따른 이미지 강도의 변화로부터 깊이를 추정하며, 이는 반사율은 다양하지만 조명은 제어된 조명이 필요하다; 광 검출 및 레인징 사용자들은 야외 장면에 대해 높은 정밀도를 제공하지만 값비싼 하드웨어를 필요로 하는 거리들을 측정하기 위해 레이저 광을 측정한다; 비행 카메라는 광이 물체까지 이동하는데 걸리는 시간을 측정하지만 반짝이는 표면에는 정확도가 떨어진다. VisionGPT-3D 프레임워크에 특징을 구현하기 위해, 영상으로부터 깊이 맵을 획득하는 단계 (1)을 탐색한다. (2) 포인트 클라우드를 생성한다. (3) 포인트 클라우드로부터 메쉬를 생성한다. (4) 컨텍스트로부터 비디오를 생성한다.\n' +
      '\n' +
      '## 2 Method\n' +
      '\n' +
      '### 깊이 맵 생성\n' +
      '\n' +
      '깊이 맵은 뷰어 또는 카메라에 대한 장면 내의 객체들의 거리 또는 깊이에 관한 정보를 제공한다. 도 1은 토끼를 나타내는 깊이 맵의 일 예를 나타낸다.\n' +
      '\n' +
      '이것은 2D 이미지에서 캡처된 장면의 3D 구조의 표현이다. 깊이 맵 내의 각각의 픽셀은 장면 내의 특정 포인트에 대응한다. 픽셀 강도는 해당 지점의 깊이를 나타낸다. 높은 픽셀 값들은 더 멀리 떨어져 있는 객체들에 대응하는 반면, 낮은 값들은 더 가까운 객체들에 대응한다. 깊이 맵은 상이한 위치들로부터 촬영된 이미지들에서 대응하는 포인트들 사이의 디스패리티들로부터 생성될 수 있다. 또한 적응 샘플링, DepthNet, MonoDepth 모델을 사용하여 단안 깊이 추정과 같은 신경망을 실시간으로 장면 이해하여 추정할 수 있다. 깊이 추정에는 전이 학습 및 사전 훈련된 모델도 사용된다. 딥러닝 모델을 활용하여 관련 작업을 사전 훈련하고 깊이 추정을 위해 미세 조정한다. 아래의 그림 2는 서로 다른 각도에서 촬영한 영상의 깊이 정보를 보여준다.\n' +
      '\n' +
      '깊이 맵을 얻으려면 여러 개의 이미지가 필요합니다. 때때로, 우리는 물체를 나타내는 많은 이미지들을 가지고 있지 않다. 단일 2차원 영상으로부터 깊이 맵을 추정하기 위해, MiDas(Monocular Depth Estimation in Real-Time with Adaptive Sampling)를 학습시킨다.\n' +
      '\n' +
      '도 1 : 이미지 깊이 맵\n' +
      '\n' +
      '그림 2: 서로 다른 각도에서 2D 이미지의 깊이 값은 이미지 내 연상 시각적 특징을 깊이 값으로 학습한다. MiDaS는 일반적으로 깊이 추정을 위해 컨볼루션 신경망을 활용한다. 단안 이미지 쌍과 깊이 맵을 포함하는 데이터 세트에 대해 학습된다. 모델은 훈련에서 예측치와 지상진실 깊이 값의 차이를 최소화하여 깊이 예측을 학습한다. MiDas는 이미지 구조를 기반으로 자원을 할당하기 위해 적응적 샘플링을 사용한다. 복잡한 구조는 단순한 지역보다 더 많은 자원을 얻는다. 복잡한 구조는 이미지 수율에서 가장 먼 부분이 될 수 있고 덜 중요한 부분이 될 수 있다. 본 논문에서는 영상의 주요 점에 초점을 맞춘 새로운 샘플링 방법을 제안한다. 그림 3의 연한 녹색 원은 이미지의 핵심 포인트를 보여준다.\n' +
      '\n' +
      '샘플링 알고리즘들은 키 포인트들의 밀도에 기초하여 자원을 할당한다. 알고리즘은 아래에서 설명한다.\n' +
      '\n' +
      '```\n' +
      '1 : K-Means 클러스터 설정\n' +
      '2:for\\(2D-Images\\)do\n' +
      '3: 키 포인트를 자신의 코디네이터에 기초하여 K개의 클러스터로 그룹화\n' +
      '4:endfor\n' +
      '5 : 각 그룹 내 키 포인트의 개수 산출\n' +
      '6: 각 그룹 내 키 포인트의 개수를 기준으로 리소스 할당\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 키 포인트 기반의 자원 할당\n' +
      '\n' +
      '미리 훈련된 MiDaS 모델을 통해 사용자는 처음부터 훈련 없이 깊이 맵을 생성할 수 있다. 다양한 이미지 및 장면을 처리할 때 보다 나은 모델 예측 정밀도 성능을 얻기 위해 요구사항에 따라 모델을 수정하고 미세 조정하며 사용자에게 가해지는 노력을 최소화한다.\n' +
      '\n' +
      '깊이 맵에서 점 구름 만들기\n' +
      '\n' +
      '포인트 클라우드는 3D 재구성 파이프라인의 기본 단계 역할을 한다. 그것은 장면의 구조와 레이아웃에 대한 상세한 설명을 제시한다. 카메라 공간에서 2차원 픽셀 좌표를 3차원 광선으로 변환하고, 깊이 맵으로부터의 깊이 값을 해당 픽셀과 연관시킴으로써, 최종적으로 2차원-3차원 연관을 외재성을 이용하여 세계 좌표로 변환한다. 포인트 클라우드는 PLY와 같은 공통 포맷 파일로 저장되며 3D로 시각화됩니다.\n' +
      '\n' +
      '도 3 : 2D 이미지 키 포인트\n' +
      '\n' +
      '매트플로틀립, 오픈3D와 같은 시각화 도구. 포인트 클라우드는 Open3D 라이브러리를 이용하여 깊이 맵으로부터 생성될 수 있다. 2D 깊이 맵으로부터 포인트 클라우드를 일반화하기 위해, 신경망은 깊이 맵을 입력으로 하고 각 픽셀에 대해 예측된 3D 좌표를 출력한다. 신경망은 주요 깊이 영역 분석, 객체 경계 식별, 필터 노이즈, 객체 세그먼트, 표면 법선 계산, 깊이 기울기 분석 등의 구성요소로 구성된다. 영상을 처리하기 전에 잡음을 필터링하여 학습 결과를 향상시킬 수 있다. 노이즈는 후속 처리 단계들에 영향을 미칠 수 있다. 장면을 이해하고 주요 깊이 영역을 파악하기 위해서는 깊이 값의 범위를 확인하고 깊이 값의 분포를 분석할 필요가 있다. 키 깊이 영역을 찾아냄으로써, 키 깊이 영역의 밀집도를 기반으로 해당 자원을 배치하는 것이 효율적이다. 객체 경계를 식별하면 일관성 학습을 위해 객체에 속한 핵심 깊이 영역을 그룹화하는 데 도움이 된다. 깊이의 급격한 변화는 종종 물체 경계 또는 폐색에 대응한다. 객체 분할은 의미 있는 객체 또는 영역을 탐색하고 그 구조에 기초하여 이미지를 분할한다. 프로그램을 통해 주변을 인식하고 정보에 입각한 결정을 내릴 수 있습니다. VisionGPT-3D는 깊이에 기초하여 오브젝트에 응답할 수 있다. 장면 내에서 특정 객체를 선택적으로 조작할 수 있습니다. VisionGPT-3D가 자율 주행 차량, 드론 또는 산업 조립체에 사용될 때 충돌 회피에 필수적이다. 객체의 경계를 깊이 있게 아는 것은 경로 계획과 장애물 회피에 도움이 된다. 표면 법선은 지표면의 로컬 지오메트리를 간결하게 표현하고 특정 지점에서 지표면에 수직인 방향을 나타냅니다. 그것들은 3D 장면 내의 표면들의 형상 및 배향을 이해하는데 사용된다. 점 구름 처리에서 특징 추출 및 클러스터링에 필수적입니다. 기하학적 특성이 다른 객체를 구별하여 분류 및 식별 작업에 도움이 됩니다. 깊이 구배는 이미지에서 깊이 값의 변화율에 대한 정보를 제공한다. 이들은 깊이 불연속성과 객체 경계를 식별하는데 도움을 주어 보다 정확한 분할에 기여한다. 3D 객체 인식 및 라벨링에 유용합니다. 이전의 맥락에서, 우리는 VisionGPT-3D에서 사용자를 위한 응답으로서 라벨링된 2D 이미지를 3D 표현으로 변환할 것을 제안했다. 사용자가 다른 답변 또는 스타일 응답을 원할 때 관련 3D 이미지를 생성하도록 모델을 훈련시키기 위해 3D 이미지의 객체를 훈련 데이터세트로 추가로 레이블링했다. 전체 네트워크 아키텍처는 아래의 그림 4에 설명되어 있다.\n' +
      '\n' +
      '도 4: AI 기반 2D-3D 영상 변환기 파이프라인\n' +
      '\n' +
      '본 논문에서는 3차원 영상 내에서 객체 분할의 혁신에 초점을 맞춘다. 깊이 맵에서 객체 분할을 위해 많은 기존 알고리즘과 방법이 사용될 수 있다. 깊이 맵을 정확하게 분석하는 것은 성공적인 포인트 클라우드를 생성하는 데 도움이 될 수 있다. 효율적인 객체 분할은 깊이 맵으로 장면에 대한 더 나은 이해로 이어진다. 컴퓨터 비전 처리에서는 임계화, 유역 분할, 그래프 컷 분할, 평균 이동 분할, 슈퍼픽셀 기반 분할, 영역 확장과 같은 객체 분할 알고리즘이 일반적이다. 분할 알고리즘들의 선택은 깊이 맵의 특정 특성들, 장면의 복잡성 및 원하는 레벨의 분할 세부사항들에 의존한다. 임계값은 영역이 특정 임계값에 의해 분리되는 기본 기술이다. 영역 성장은 시드 픽셀에서 시작하여 깊이 값이 유사하면 이웃 픽셀을 영역에 추가한다. 시드 선택은 알고리즘의 성능과 결과에 영향을 미칠 수 있다. 유역 분할은 유역 분지에 픽셀을 할당하여 깊이 불연속성을 기반으로 서로 다른 객체를 분리한다. 부드러운 영역에서 과분할 또는 약한 강도 차이가 있는 경우 과소분할로 이어질 수 있다. 그래프 컷 분할은 깊이 맵을 나타내는 그래프를 구성하고 그래프 컷 알고리즘을 적용하여 최적의 분할을 찾는다. 계산 복잡도는 특히 큰 데이터 세트 또는 고차원 특징 공간의 경우 비용이 많이 들 수 있다. 그래프 컷 방법은 종종 픽셀 단위의 정보에 의존하며, 유사한 외관을 갖는 영역은 효과적으로 분리되지 않을 수 있다. Mean-shift 클러스터링은 유사한 깊이 값을 갖는 픽셀들을 그룹화하고 별개의 객체들을 형성한다. 그것은 잠재적으로 유사한 컬러를 갖는 영역들을 병합한 후에 세부사항들의 손실로 이어질 수 있는 반면, 영역들은 균질한 컬러를 갖는 상이한 영역들을 갖는다. 슈퍼픽셀 기반 분할은 깊이 맵을 컴팩트한 이미지 영역으로 분해한다. 클러스터링 방법을 사용하여 깊이 정보를 기반으로 슈퍼픽셀을 생성한다. 이는 계산 복잡도를 감소시키지만 복잡한 영역에서 과도한 분할로 이어질 수 있다. 알고리즘을 수동으로 선택하고 이미지를 분할하는 것은 비효율적이다. 본 논문에서는 영상 특성을 기반으로 객체 분할을 위한 알고리즘을 선택하기 위한 인공지능 기반 접근 방법을 제안한다. 입력은 깊이 영상이고 출력은 결과와 함께 최적화된 분할 알고리즘이다. 트레이닝은 동일한 이미지를 기반으로 한 모델 출력으로 그라운드 트루스 알고리즘 선택 사이의 오류를 최소화한다. 컨벌루션 네트워크를 기본 아키텍처로 사용하여 이미지를 처리하고 순위화 방법을 포함하여 상위 k개의 해당 알고리즘을 반환하고 상위 상대 알고리즘을 적용하여 이미지를 분할한다. 이 모델은 대규모 데이터 세트를 확장하고 처리할 수 있습니다. 이러한 접근법으로, 객체 분할 효율 및 정확성이 개선된다. 포인트 클라우드는 Open3D 또는 OpenCV와 같은 툴들에 의해 분할된 깊이 맵에 기초하여 생성될 수 있다.\n' +
      '\n' +
      '### 포인트 클라우드에서 메쉬 생성\n' +
      '\n' +
      '점 구름으로부터 메쉬를 생성하는 것은 점들을 삼각형들과 연결하여 표면 표현을 생성하는 것을 포함한다. 점군으로부터 메쉬를 생성하기 위해 사용되는 일반적인 알고리즘은 Delaunay Triangulation, Surface reconstruction, alpha shape, marching cubes, ball pivoting algorithm, power crust, screened Poisson reconstruction, ball and triangle growing, depth map 기반 기법, Poisson surface reconstruction, greedy projection triangulation, Voronoi diagram 기반 메쉬를 포함한다. Delaunay 삼각 측량법은 메쉬 내의 모든 삼각형의 최소 각도를 최대화하여 작은 삼각형으로 인한 수치 계산을 줄이고 계산 효율을 향상시킨다. 결점은 점 분포가 불규칙한 영역에서 평평하고 긴 삼각형을 생성할 수 있다는 것이다. 알파 모양은 볼록하지 않은 모양을 가진 물체를 나타내는 점 구름을 다룰 때 오목한 선체를 생성할 수 있다. 그러나 매개변수 알파의 선택에 따라 다릅니다. 알파 모양 계산은 복잡한 모양에 제한이 될 수 있다. 포아송 표면 재구성은 포아송 방정식을 기반으로 기본 표면을 나타내는 스칼라 필드를 재구성한다. 매끄럽고 연속적인 표면을 생성하는 경향이 있으며 기하학에서 날카로운 특징을 포착하기 어렵다. 3차원 포아송 방정식의 기울기 및 라플라시안 연산자는 아래와 같이 정의된다. 포아송 방정식은 스칼라장의 라플라시안과 밀도함수의 차이를 최소화하는 객체함수를 정의하여 최적화된 문제로 수정하였으며, 밀도함수는 입력점군을 기반으로 구성하였다. 아래와 같은 공식을 바탕으로 표면이 매끄러워야 하는 스칼라장의 편미분을 계산하려고 한다. VisionGPT-3D는 정확도 향상을 위한 알고리즘 정의 분석을 기반으로 해당 알고리즘을 선택한다.\n' +
      '\n' +
      '[\\nabla\\Phi=\\left(\\frac{\\partial x}\\mbox{, }\\frac{\\partial y}\\mbox{, }\\frac{\\partial\\Phi}{\\partial z}\\right)\\\\[\\nabla^{2}\\Phi=\\frac{\\partial x^{2}\\Phi}{\\partial y^{2}}+\\frac{\\partial z^{2}\\Phi}{\\partial z^{2}}+\\frac{\\partial z^{2}\\Phi}\\frac{\\partial z^{2}\\frac{\\partial y}\\mbox{, }\\frac{\\partial z^{2}\\frac{\\partial y}\\mbox{, }\\frac{\\partial z^{2}\\frac{\\partial z^{2}\\frac{\\partial z^{2}\\frac{\\partial z^{2}\\frac{\\partial z^{2}\\frac{\\partial z^{\n' +
      '\n' +
      '최소 자승은 다항식 곡면을 이용한 국부 근사법으로, 재구성된 곡면의 날카로운 특징을 보다 잘 보존할 수 있는 방법이다. MLS는 근사화 방법을 사용하므로 포아송 표면 재구성보다 더 효율적일 수 있다. 이전 섹션에서는 깊이 맵 내의 깊이 기울기를 분석하여 픽셀 값의 급격한 변화로부터 객체 에지를 찾을 수 있다. 그것은 메쉬 생성에서 매끄럽거나 날카로운 표면을 처리하기 위해 적합한 알고리즘을 선택하는 데 도움이 된다. 볼 피봇 알고리즘과 볼 성장 알고리즘은 이웃한 볼들을 연결하여 삼각형을 형성한다. 볼 피벗팅 알고리즘은 곡률이 변하는 곡면에 적응하는 데 어려움을 겪을 수 있지만 볼 성장 알고리즘은 곡률이 변하는 곡면에 잘 어울린다. 그러나 볼 피벗 알고리즘은 다른 알고리즘에 비해 성능이 우수하다. 그들은 서로 다른 연결점을 가지고 있으며 삼각형을 형성하여 다양한 곡률을 처리할 수 있는 적응력이 다르다. 볼 피벗에서 고정된 크기의 볼은 다양한 곡률을 다루기 위한 제한이 될 수 있다. 볼 성장 알고리즘은 포인트 밀도가 다른 영역에 적응하여 더 나은 표면 재구성을 보장할 수 있다. 이전 섹션에서는 핵심 포인트를 식별하여 포인트 밀도를 식별하거나 깊이 맵 내에서 픽셀 값을 분석할 수 있다. 깊이 맵 분석 결과는 이미지 내의 특정 영역에 대해 최적화된 메쉬 생성 알고리즘을 적응시키기 위해 사용된다.\n' +
      '\n' +
      '### 유효메시지의 정확성\n' +
      '\n' +
      '메쉬 생성 알고리즘의 정확성을 검증하는 것은 결과 메쉬가 기본 형상을 정확하게 나타내는 것을 보장하기 위한 중요한 단계이다. 일반적으로, 사람들은 생성된 메쉬를 3D 시각화 도구를 사용하여 검사한다. 부드러움, 연속성 및 기본 형상의 중요한 피쳐를 캡처할지 여부를 확인합니다. 이 방법은 직관적인 이해도를 제공하지만 정량적 평가에는 충분하지 않을 수 있다. 그림 5의 잘못된 생성 결과와 같은. VisionGPT-3D는 학습된 모델에 의해 생성된 메시를 검증한다.\n' +
      '\n' +
      '도 5: 부정확하게 생성된 메시\n' +
      '\n' +
      '비교 분석으로부터. 이전 결과가 실패하면 메시를 재생성합니다. VisionGPT-3D는 메쉬 검증에 사용되는 일반적인 알고리즘을 기반으로 결과를 검증하기 위해 최적화된 검증 방법을 선택한다. 육안 검사 외에도 표면 편차 분석, 에지 길이 분석, 체적 보존, 메쉬 품질 측정, 수렴 분석과 같은 다른 검증 방법이 있다. 표면 편차 분석은 해당 지점에서 표면 편차를 계산하여 생성된 메쉬를 참조 모델 또는 그라운드 트루스와 비교한다. 하우스도르프 거리 또는 점 대 표면 거리와 같은 메트릭을 사용하여 편차를 정량화할 수 있다. 더 작은 편차는 더 나은 정확도를 나타낸다. 모서리 길이 분석은 생성된 메시에서 모서리 길이의 분포를 분석합니다. 그것은 메쉬가 높은 곡률 또는 디테일의 영역에서 적절하게 정제되는 것을 보장하는 것을 돕는다. 볼륨 보존은 메쉬로 둘러싸인 볼륨이 원래 객체의 볼륨과 일치하는지 확인합니다. 검증 방법은 정확성을 확인하기 위해 진실과 잘못된 결과를 비교하는 유사한 요소를 가지고 있다.\n' +
      '\n' +
      '### 정적 프레임에서 비디오 생성\n' +
      '\n' +
      '때때로 사람들은 연속적인 시나리오를 나타내는 컨텍스트를 설명할 수 있다. 예를 들어, 매 한 마리가 눈 내리는 하늘을 날고 있습니다. 3D 이미지로부터 비디오를 생성하는 것은 모션의 착시를 생성하기 위해 상이한 프레임들에 걸쳐 이미지 데이터를 시각화하는 것을 수반한다. 프레임이 모두 표시되는 것은 어렵지 않습니다. 우리는 그것들을 생성하기 위해 ffmpeg와 같은 라이브러리를 사용할 수 있다. 단계들은 (1) 미래 및 3D 축을 생성한다. (2) 상기 초기 3D 영상을 셋업하는 단계를 포함하는 것을 특징으로 하는 영상 처리 방법. (3) 각 프레임에 대한 3D 플롯을 업데이트한다. (4) 프레임 수를 설정하여 애니메이션을 생성한다. 비디오에 대한 모든 프레임이 없다면 더 복잡할 수 있습니다. 우리는 그것을 하기 위해 대안적인 접근법을 사용한다. 프레임에 초점을 맞추는 대신 런타임에 객체를 장면에 배치하고 라우팅합니다. 구체적인 과정을 설명하면 다음과 같다. 언어 이해 모델은 단어를 먼저 세그먼트화합니다. 장면에서 장면이나 사물을 나타내는 단어들을 찾아내려고 한다. 기존의 분석에서는 영상 내부의 물체들이 깊이별로 분석될 수 있음을 알 수 있었고, 그 사이의 충돌을 이해할 수 있었다. 우리는 장면과 충돌할 객체와 장면을 구별하기 위해 이미지에 라벨을 붙일 수 있다. 우리는 또한 우리가 관찰한 물체에 대해 이미지를 여러 레벨로 필터링할 수 있다. 이러한 정보를 바탕으로 충돌을 고려하여 물체를 장면에 배치할 수 있습니다. VisionGPT-3D가 물체를 위치시키고 경로화할 때, 기본 물리 법칙에 기초하여 물체를 움직이게 하려고 한다. 일반적인 접근법들에서, 모델들은 통계에 기초하여 트레이닝되고, 경험들에 기초하여 객체들을 배치한다. 그러나 통계가 객체 움직임을 정확하게 설명하지 못할 수 있습니다. 본 논문에서는 영상에서 획득한 충돌 정보를 기반으로 런타임에서 이동 경로를 생성한다.\n' +
      '\n' +
      '###유효성 생성 비디오 정확성\n' +
      '\n' +
      '검증은 나쁜 시각적 구성 요소가 다른 텍스트 기반 구성 요소보다 사람들의 마음에 더 직접적으로 부정적인 영향을 미치기 때문에 시각적 컨텍스트 처리에서 중요한 단계이다. 그림 6에서 보는 바와 같이 영상 내부의 가장 먼 위치에 있는 사찰은 정면 객체의 배경 장면과 깊이에 비해 부정확한 크기 비율로 생성된다.\n' +
      '\n' +
      '그림 7은 잘못된 생성 이미지를 보여준다.\n' +
      '\n' +
      '검증 프로세스에서, 프레임워크는 컬러 정확도를 체크하고, 비디오 내의 컬러들이 정확하고 일관성이 있는지 검증할 필요가 있을 것이다; 프레임 레이트, 비디오가 머신 러닝 모델 트레이닝을 위한 특징들로서 의도된 프레임 레이트 및 다른 검증 포인트들에서 재생됨을 확인한다.\n' +
      '\n' +
      '도 6: 이미지 내부의 잘못된 객체\n' +
      '\n' +
      '도 7 : 잘못된 이미지\n' +
      '\n' +
      '## 3 Summary\n' +
      '\n' +
      '본 논문에서는 사용자의 요구사항을 예측하기 위해 머신러닝 모델을 기반으로 최적화된 메쉬 생성, 깊이 맵 분석 알고리즘을 선택할 수 있는 통합 비전GPT-3D 프레임워크를 제안하였다. 전통적인 비전 처리 방법과 AI 모델을 통합 시스템으로 결합하여 시각적 애플리케이션 변환의 능력을 극대화합니다. 자기 지도 학습을 활용하여 모델을 학습하고 2D 이미지로부터 3D 재구성의 각 단계에서 적합한 알고리즘을 선택한다. 작업 중 비GPU 환경에서 작업하는 동안 일부 라이브러리는 사용할 수 없거나 성능이 낮은 한계를 발견한다. 모델 학습 비용을 더 줄이고 효율성, 예측 정밀도를 향상시키기 위해 자체 설계 저비용 일반화된 칩셋을 기반으로 알고리즘 최적화를 작업할 계획이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Chris Kelly and Luhui Hu et al., "UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework," Nov. 2023.\n' +
      '* [2] Deyao Zhu and Jun Chen et al., "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models," Apr. 2023.\n' +
      '* [3] Chen-Hsuan Lin and Jun Gao et al., "Magic3D: High-Resolution Text-to-3D Content Creation," Mar. 2023.\n' +
      '* [4] Junyoung Seo and Wooseok Jang et al., "LET 2D DIFFUSION MODEL KNOW 3D-CONSISTENCY FOR ROBUST TEXT-TO-3D GENERATION," Feb. 2024.\n' +
      '* [5] Junshu Tang and Tengfei Wang et al., "Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior," Apr. 2023.\n' +
      '* [6] Junnan Li and Dongxu Li et al., "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation," Jan. 2022.\n' +
      '* [7] Qiuhong Shen and Xingyi Yang et al., "Anything-3D: Towards Single-view Anything Reconstruction in the Wild," Apr. 2023.\n' +
      '* [8] Rui Huang and Songyou Peng et al., "Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels," Dec. 2023.\n' +
      '* [9] Jiale Li and Hang Dai et al., "MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving," Mar. 2023.\n' +
      '* [10] Deshun Yang and Luhui Hu and Yu Tian et al., "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs," Mar. 2024.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>