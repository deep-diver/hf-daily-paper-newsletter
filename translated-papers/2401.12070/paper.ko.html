<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '비안대로 기계 생성 텍스트 LLM의 제로 블록 검출:\n' +
      '\n' +
      'Abhimanyu Hans\n' +
      '\n' +
      '메릴랜드의 대학교.\n' +
      '\n' +
      '&Avi Schwarzschild\n' +
      '\n' +
      '>멜론대.\n' +
      '\n' +
      '&Valerilia Cherepanova\n' +
      '\n' +
      '메릴랜드의 대학교.\n' +
      '\n' +
      '&Hamid Kazemi\n' +
      '\n' +
      '메릴랜드의 대학교.\n' +
      '\n' +
      '&Aniruddha Saha\n' +
      '\n' +
      '메릴랜드의 대학교.\n' +
      '\n' +
      '&Micah Goldblum\n' +
      '\n' +
      '뉴욕대.\n' +
      '\n' +
      '&Jonas Geiping\n' +
      '\n' +
      '지능 시스템&MPI\n' +
      '\n' +
      '방방형 AI 센터 방 방 방 방 방 방 방 방 방TubTuben AI 센터TubTuben AI 센터TubTTubenTTuberererererererererererererererererererererererererererererererererererererererererererererererererererererererererTubenTubenTubenTubenTubenTubenTubenTubenTubenTubenTub\n' +
      '\n' +
      '동일한 기여. 한스1@umd.edu에 대한 반응.\n' +
      '\n' +
      '[https://github.com/ahans30/B 아르헨티나](https://github.com/ahans30/B dosesulars)에서 사용할 수 있다.\n' +
      '\n' +
      'Tom Goldstein\n' +
      '\n' +
      '메릴랜드의 대학교.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '현대 대형 언어 모델에 의해 생성된 텍스트를 검출하는 것은 LLM과 인간 모두 광범위한 복잡한 행동을 나타낼 수 있기 때문에 어려울 것으로 판단된다. 그러나, 우리는 두 개의 밀접하게 관련된 언어 모델을 대조하는 것에 기초한 점수가 인간 생성 및 기계 생성 텍스트를 분리하는 데 매우 정확하다는 것을 발견했다. 이 메커니즘을 기반으로 미리 학습된 LLM 한 쌍을 사용하여 간단한 계산만 필요로 하는 새로운 LLM 검출기를 제안한다. i_B 쌍안경_이라 불리는 방법은 학습 데이터 없이 최첨단 정확도를 달성합니다. 모델별 수정 없이 다양한 현대 LLM에서 기계 텍스트를 스팟팅할 수 있습니다. 우리는 다수의 텍스트 소스와 다양한 상황에서 _B 쌍안경__을 종합적으로 평가한다. 광범위한 문서 유형에서 _B 쌍안경은 ChatGPT 데이터에서 훈련되지 않았음에도 불구하고 ChatGPT(및 기타 LLM)에서 생성된 샘플의 90% 이상을 0.01%의 허위 비율로 검출한다.\n' +
      '\n' +
      '그림 1: ** ChatGPT에서 기계 생성 텍스트 검출이다. i_B 쌍안경_를 사용한 검출 접근법은 \\(0.01\\%\\)의 잘못된 양성률을 갖는 _News__News_, _Creative Writing_ 및 _risent Essay_ 데이터셋에서 기계 생성 및 인간 표지 샘플을 분리하는 데 매우 정확하다. 핀셋링이 없는 오픈 소스 팔콘 모델을 기반으로 하는 쌍안경_은 GPTZero와 같은 상업적 검출 시스템과 강력한 오픈 소스 검출기를 능가하며, 이 두 바젤 모두 ChatGPT(Verma et al., 2023; 톈, 2023a)를 감지하기 위해 구체적으로 조정된다. 우리의 접근법은 완전히 제로 샷 설정에서 작동하며 특히 ChatGPT를 검출하도록 튜닝되지 않았거나 훈련되지 않았다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '우리는 LLM 소스로부터 훈련 예시가 사용되지 않는 제로 샷 설정에서 작동하는 LLM 생성 텍스트를 검출하는 방법을 제시한다. 이러한 엄격한 제한에도 불구하고 우리의 계획은 ChatGPT 검출을 위한 모든 오픈 소스 방법을 여전히 능가하고 ChatGPT(Mitchell et al., 2023; Verma et al., 2023)의 훈련 샘플을 사용하여 경쟁자에도 불구하고 상업용 API와 경쟁적이거나 더 우수한다. 동시에, 검출기의 제로 샷 특성 때문에, 매우 동일한 검출기는 높은 정확도로 여러 다른 LLM을 스팟할 수 있으며, 기존 모든 솔루션은 하지 못한다.\n' +
      '\n' +
      '제로 샷 설정에서 LLM을 검출하는 능력은 중요성이 증가하는 문제를 다룬다. 학문 표절 퇴치에 대한 선행 연구는 간단하고 접근할 수 있는 인터페이스 때문에 ChatGPT에 강력하게 고정되었다. 그러나 더 정교한 배우들은 LLM API를 사용하여 보트를 운영하고 가짜 제품 리뷰를 만들고 소셜 미디어 플랫폼에서 잘못된 정보를 대규모로 퍼뜨린다. 이 배우들은 단지 ChatGPT를 넘어 다양한 LLM을 사용할 수 있으며, 소셜 미디어 모델 및 플랫폼 무결성 보증에 중요한 제로샷, 모델 진단 검출(Crothers et al, 2022, Bail et al., 2023)을 만든다. 당사의 제로 샷 능력은 모델 특정 학습 데이터에 의존하며 종종 새로운 모델로 전송하지 못하는 기존 검출기에서 출발하는 것입니다.\n' +
      '\n' +
      '제안된 _B 쌍안경_이라 불리는 디텍터는 두 개의 렌즈를 통해 텍스트를 보고 작동합니다. 먼저, 우리는 "서버" LLM을 사용하여 문제의 텍스트의 \\(\\log\\) 불균형을 계산한다. 그런 다음 "퍼포넌저" LLM이 스트링 내의 각 위치에서 만들 것이라는 모든 차세대 예측을 계산하고 관찰자에 따라 그들의 불균일성을 계산합니다. 만약 끈이 기계에 의해 쓰여진다면, 우리는 이 두 가지 불균형이 유사할 것으로 예상해야 한다. 만약 그것이 인간이 쓴 것이라면, 그것들은 달라야 한다. 우리는 먼저 이 간단한 관찰에 동기를 부여한 다음 여러 텍스트 영역에서 광범위하게 스트레스 테스트를 하는 강력한 제로 샷 검출기를 구축하기에 충분하다는 것을 보여준다.\n' +
      '\n' +
      '2개의 LLM이 감지됩니다.\n' +
      '\n' +
      '생성 AI에 대한 위해 감소에 대한 일반적인 첫 번째 단계는 검출이다. 구체적으로 텍스트 기원을 문서화하고 추적하는 것(Biderman and Raff, 2022)부터 스팸 및 가짜 뉴스 캠페인(Zellers et al., 2019)을 조사하고 훈련 데이터 체인을 분석하는 것까지 다양한 응용 프로그램은 텍스트가 인간 또는 기계 생성되었는지 여부를 정량화하는 신호(Bender et al, 2021, 크로테르 등, 2022, 미스키 등)로부터의 혜택을 모두 얻는다.\n' +
      '\n' +
      '기계 생성 텍스트를 스팟링하기 위한 성공적인 노력은 생성 출력이 설득력 있는 인간이 아닌 초기 모델에 대한 약속을 보여준다. 그러나 언어 모델링을 위한 변압기 모델(라드포드 et al., 2019; 브라운 et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023)이 상승함에 따라 기계 생성 텍스트를 검출하기 위한 원시 메커니즘이 쓸모없게 된다. 한 가지 접근법은 (Krishna et al, 2023) 또는 워터마크가 생성된 모든 텍스트(키르첸바우어 등, 2023)를 기록하는 것이지만 이러한 _선명 검출_ 접근법은 생성 모델에 대한 완전한 제어만으로 구현될 수 있다.\n' +
      '\n' +
      '대신, 최근 기계 생성 텍스트, 특히 ChatGPT를 통해 확산된 텍스트는 모델 소유자의 협력 없이 기계 텍스트를 검출하는 데 사용할 수 있는 _포스트-hoc 검출_ 접근법에 대한 작업으로 이어진다. 이 검출기는 두 개의 주요 그룹으로 분리될 수 있다. 첫 번째는 전처리된 언어 모델 백본이 검출의 이진 분류 과제에 대해 피검출된 검출 모델(솔라이만 등, 2019; 자셀러 등, 2019; 유 et al, 2023; 자한 등은 2023), 역학적 훈련(Hu et al., 2023), 기권(Tian et al., 2023)과 같은 기술을 포함한다. 대안적으로, 전체 백본을 피팅하는 대신, 선형 분류기는 상업용 API 출력(Verma et al, 2023)을 포함할 수 있는 냉동 학습된 특징 위에 적합할 수 있다.\n' +
      '\n' +
      '두 번째 범주의 접근 방식은 기계 생성 텍스트의 특징인 통계적 서명을 포함한다. 이러한 접근 방식은 훈련 데이터를 전혀 요구하지 않거나 거의 요구하지 않는 장점이 있으며 새로운 모델 가족(Pu et al, 2022)에 쉽게 적응할 수 있다. 예로는 경화성(Tian, 2023; Vasilatos et al., 2023; Vasilatos et al., 2023), 경화성 곡률(Mitchell et al., 2023), 로그 순위(Su et al., 2023), 생성된 텍스트의 고유 차원(Tulchinskii et al., 2023), 무그램 분석(양 et al., 2023)을 기반으로 하는 검출기가 있다. 경관의 적용 범위는 치명적이지 않으며 최근 조사 Ghosal et al.(2023); 탕 et al.(2023); Dhanini et al.(2023); 구오 등은 추가 세부 사항에 대해 참조한다.\n' +
      '\n' +
      '이론적인 관점에서 볼트니(2020), 헬름 등 알(2023), 사다시반(2023) 등은 모두 검출 한계에 대해 논의한다. 이러한 작업은 일반적으로 언어의 완전히 범용적인 모델이 정의상 감지할 수 없다는 것에 동의한다. 그러나 Chakraborty et al.(2023)는 이 최적에 임의로 가까운 모델조차도 충분한 수의 샘플을 감안할 때 기술적으로 감지할 수 있다는 점에 주목한다. 실제로, 우리가 이 작업에서 제안하고 분석하는 것과 같은 검출 접근법의 상대적 성공은 현재의 언어 모델이 인간 글쓰기의 불완전한 표현이라는 건설적 증거를 제공하며, 따라서 감지할 수 있다. 마지막으로 탐지를 우회하려는 공격용 감지기의 견고성은 최악의 경우(Bhat and Parthasarathy, 2020; 월프 및 월프, 2022; 리옌지 및 부스칼디, 2023)에서 신뢰성에 대한 더 강한 실질적인 한계를 제공할 수 있다.\n' +
      '\n' +
      'LLM 검출에 대한 작업이 얼마나 존재하는지에 대한 이해로 중요한 질문이 발생하는데, 우리는 어떻게 적절하게 그리고 철저히 검출기를 평가합니까? 많은 작업은 제안된 분류기의 균형 잡힌 테스트 세트 및 AUC에 대한 정확성에 초점을 맞추고 있지만 이러한 메트릭은 검출의 고열량 문제에 대해 잘 달성되지 않는다. 궁극적으로 인간 필기 텍스트의 광범위한 분포에 걸쳐 거짓 양성률이 낮은 검출기만 실제로 피해를 감소시킨다. 또한, 리앙 et al.(2023)는 검출기가 종종 학습 데이터에 반영되는 비교적 쉬운 데이터셋에서만 평가된다는 점에 주목한다. 도메인 외 샘플에 대한 그들의 성능은 종종 마모된다. 예를 들어, 비천연 영어 스피커가 작성한 TOEFL 에세이는 상용 검출기(Liang et al., 2023)에 의해 시간의 48~76%의 기계 생성으로 잘못 표기되었다.\n' +
      '\n' +
      '3절에서는 접근에 동기를 부여하고, 특히 ChatGPT 세계에서 언어 모델 텍스트를 검출하는 것이 어려운 이유에 대해 논의한다. 이 작업에서 우리의 강조점은 사후, 도메인 외(제로샷), 블랙박스 검출 시나리오 내에서 기능하는 기저부로 향하고 있다. 우리는 최신 오픈 소스 검출기 고스트버스(Verma et al, 2023), 상업적으로 배치된 GPTZero,1 및 DetectGPT(Mitchell et al, 2023)를 사용하여 4절에서 다양한 데이터 세트를 가로질러 검출 성능을 비교하며, 5절에서는 _B 쌍안경_에 기반한 검출기가 고려해야 하는 에지 사례 및 중요한 배치 행동을 구성하는 다양한 환경에서 _B 쌍안경_의 신뢰성을 평가한다.\n' +
      '\n' +
      '부타주 1: [국경://gpt 0.me/] (https://gpt 0.me/) (https://gpt 0.me/)\n' +
      '\n' +
      '어떻게 작동하는지 3 _B dosesulars.\n' +
      '\n' +
      '우리의 접근 방식인 _B 쌍안경_는 두 가지 다른 언어 모델의 렌즈를 통한 입력을 살펴봄으로써 이름 붙인다. 기계/인간 분류에 대한 일반적인 기준선 - 자체적으로는 불충분하여 통계적 서명을 기반으로 하는 풍미 접근에 대한 사전 작업을 주도한다는 것은 잘 알려져 있다. 그러나 우리는 두 점수의 비율을 사용하여 제안하는데, 여기서 하나는 불균일 측정이고 다른 하나는 _cross-perplexity_이며, 이는 하나의 모델의 다음 토큰 예측이 다른 모델에 얼마나 놀라운지에 대한 개념이다. 이 두 모델 메커니즘은 우리의 일반적이고 정확한 검출기의 기본이며 이 메커니즘이 _B 쌍안경_에 의해 사용되는 두 모델과 관련이 없는 경우에도 많은 큰 언어 모델을 검출할 수 있음을 보여준다.\n' +
      '\n' +
      '백그라운드 부인.\n' +
      '\n' +
      '인물 \\(s\\)의 문자열은 토큰으로 파싱될 수 있고 토큰화기 \\(T\\)에 의해 토큰 인덱스 \\(\\vec{x}\\)의 리스트로 나타낼 수 있다. r\\(x_{i}\\)는 LLM 어휘 \\(V=\\{1,2.n\\}\\)의 엔트리를 지칭하는 \\(i\\)-th 토큰의 토큰 ID를 나타낸다. 토큰 시퀀스를 입력으로 보아 언어 모델 \\(\\mathcal{M}\\)는 어휘에 대한 확률 분포를 출력함으로써 다음 토큰을 예측한다.\n' +
      '\n' +
      '\\[\\mathcal{M}(T))=\\mathcal{M}(s))=\\mathcal{M}(\\vec{x})=Y \\tag{1}\\] \\[Y_{ij}=P(v_{j}|x_{0:i-1})\\[Y_{ij}=P(v_{j}|x_{0:i-1})\\[Y_{ij}.\\[Y_{ij}.\\[Y_{ij}.\\]:v_{ij]:v_{ij_{ij}:v_{ij]:v_{j}.\\[Y_{ij]:v_{j}|x_{ij_{ij_{ij]:{j}.\\]:{j}.\\]:{j} <{j}.\\]:{ij_{ij]:{j}.\\_{ij]:{j}.\\]:{j}.\\]:{j}{0:i-1:i-1:i-1}\n' +
      '\n' +
      '우리는 표기를 악용하고 \\(\\mathcal{M}(T(s))\\를 \\(\\mathcal{M})로 축약할 것이다. 우리의 목적을 위해 우리는 로그 투과성인 \\(\\log\\\\operatoral{PPL}\\)를 주어진 시퀀스에서 모든 토큰의 평균 음의 로그 가능성으로 정의한다. 형식적으로, 형식적으로, 형식적으로, 그자.\n' +
      '\n' +
      '(들)=\\log\\\\\\u_{i{M}} <\\log\\sum_{i =1}{frac{L}} <\\log_{ix_{i}}> \\tag{L} \\log(Y_{ix_{i}}), \\tag{2}\\[\\text{x}] \\[\\text{x}=T)(Y=Y=\\mathcal{L} \\tag{L} \\tag{L} \\tag{L} \\\\{i} \\\\{i} \\\\{i]\\{i} \\\\{i} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\{x} \\ 직관적으로 로그 성능은 문자열을 언어 모델에 어떻게 "항쟁"하는지를 측정한다. 위에서 언급한 바와 같이, 인간은 LLM보다 더 놀라운 텍스트를 생성하기 때문에 경피성은 LLM을 검출하는 데 사용되었다. 이는 \\(\\log\\mathrm{PPL}\\)도 생성 LLM을 훈련시키는 데 사용되는 손실 함수이며 모델이 자체 산출물을 항쟁으로 점수를 매길 가능성이 높기 때문에 합리적이다.\n' +
      '\n' +
      '우리의 방법은 또한 하나의 모델의 출력이 다른 모델에 얼마나 놀라운지 측정한다. 우리는 두 가지 모델과 끈을 주장하는 _cross-perplexity_를 그 논거로 정의한다. \\(\\log\\mathcal{L}_{\\mathcal{M}_{\\mathcal{M}_{1},\\mathcal{M}_{2}}(들)\\)은 \\(\\mathcal{M}_{1}\\)의 토큰화를 작동시킬 때 \\(\\mathcal{M}_{2})와 \\(\\mathcal{M}_{mathcal{M}_{2}) 두 모델의 출력, \\) 및 \\(\\)의 토큰화에 동작할 때 \\(\\mathcal{M}_{mathcal{M}_{2}_{mathcal{M}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}_{2}) 및 \\(\\)의 토큰화(s\\)과 \\(\\)의 토큰화를 조작할 때 \\(\\.2}_{2}_{2}_\n' +
      '\n' +
      '뿌리 2: 이는 \\(\\mathcal{M}_{1}\\) 및 \\(\\mathcal{M}_{2}\\)가 토큰화기를 공유하도록 요구한다.\n' +
      '\n' +
      'MS{M}_{\\mathcal{M}_{1},\\mathcal{M}_{mathcal{M}},\\mathcal{M}}(s)=-\\frac{1}}(s)=-\\frac{1}{L} \\sum_{i =1} \\mathcal{L}}, \\mathcal{M}:\\mathcal{M} <{2} <\\mathcal{M}:\\mathcal{M}_{2} <\\mathcal{1} <\\mathcal{1}:\\mathcal{1})} <\\mathcal{1}{L} <\\mathcal{1}{L}{L} <\\mathcal{L}{L} <\\mathcal{L}{L}{L} <\\frac{L}{L}{L}{L}{L}{L}{L}{L} <\\sum_{L}{L}{L} <\\sum_{L}{L}{\n' +
      '\n' +
      'i\\(\\cdot\\)는 두 개의 벡터-평가된 양 사이의 점 생성물을 나타낸다.\n' +
      '\n' +
      '###은 탐지를 어렵게 하는 것은 무엇입니까? 카피바라 문제에 대한 프라이머입니다.\n' +
      '\n' +
      '왜 난간과 교차 성능의 측정이 필요합니까? 당연히 LLM은 LLM에 만족하지 않는 텍스트를 생성하는 경향이 있다. 한편, 인간은 기계와 다르기 때문에, 인간 텍스트는 LLM 관찰자에 따라 더 높은 주위성을 갖는다. 이러한 이유로 LLM 검출을 위해 원시 주위성을 사용하려는 유혹이 있는데, 이는 높은 경막성이 인간 저자의 강한 신호이기 때문이다.\n' +
      '\n' +
      '불행히도, 이 직관은 손으로 만든 프롬프트가 개입될 때 깨집니다. 프로파트는 다운스트림 텍스트에 강한 영향을 미치며 프롬프트는 일반적으로 검출기에 알려지지 않았다. 한편, 프롬프트 \'1, 2, 3\'은 매우 낮은 불균일 완성 \'4, 5, 6\'을 초래할 수 있으며, 다른 한편으로는 \'아토피주의자인 카피바라에 대해 몇 가지 문장을 쓸 수 있는가\'라는 프롬프트가 더 이상해 보이는 응답을 얻을 수 있을 것이다. 프롬프트가 있는 경우, 응답은 부정하지 않을 수 있다(낮은 혼탁도). 그러나 프롬프트가 없는 상태에서 같은 문장에 호기심 있는 단어인 "카피바라"와 "위식주의"를 포함하는 응답은 격막성이 높아서 그 텍스트가 인간이 쓴다는 잘못된 판단을 초래하여 그림 2의 예를 볼 수 있으며, 특정 맥락은 저작자가 인간인지 기계인지에 관계없이 매우 엄격하고 다른 맥락이 낮을 것이다. 우리는 이 딜레마를 "카피바라 문제"라고 지칭하며, 신속한 기간이 없을 때 LLM 검출은 어렵고 순진한 불균일 기반 검출이 실패해 보인다.\n' +
      '\n' +
      '검출 스코어입니다.\n' +
      '\n' +
      '_B 쌍안경_은 프롬프트에 의해 유도된 기준선 주위성을 추정하는 메커니즘을 제공함으로써 카피바라 문제를 해결한다. 관찰된 텍스트의 불균일성을 이 예상 기준선과 비교함으로써, 우리는 치열한 개선된 LLM 검출을 얻는다.\n' +
      '\n' +
      '그림 2: 이 인용문은 ChatGPT(GPT-4)의 LLM 출력이며, 이 인용문은 "아식주의자인 카피바라에 대해 몇 개의 문장을 쓸 수 있는가?"라고 프롬콘 LLM은 이 샘플을 인간 및 기계 데이터 모두에 대한 평균보다 훨씬 높은 퍼플로우티(2.20)를 할당한다. 이 문제에도 불구하고, 우리의 검출기는 0.73의 _B 쌍안경_ 점수를 올바르게 할당하고, 이는 0.901의 글로벌 임계치보다 훨씬 낮아서 높은 신뢰도로 정확한 분류를 초래한다. 참고로 디텍트GPT는 0.14의 점수를 잘못 할당하는데, 이는 0.17의 최적 임계치 이하이며 텍스트를 인간으로 분류한다. GPTZero는 이 텍스트가 AI에 의해 생성된다는 49.71% 점수를 부여한다.\n' +
      '\n' +
      '*** 모티베이션** 언어 모델은 인간에게 비해 낮은 성능의 텍스트를 생성하는 것으로 알려져 있으므로 불균등 임계값 분류기가 명백한 검출 방식을 만든다. 그러나 LLM 시대에 생성된 텍스트는 특정된 프롬프트(표 2의 "카피바라 문제"(Capbyara Problem)에 따라 높은 엄격도 점수를 나타낼 수 있다. 높은 성능의 생성을 생성하는 프롬프트에 대해 보정하기 위해 우리는 두 모델의 다음 함수 예측의 불균일 수준을 직관적으로 인코딩하는 정규화 요인으로 _cross-perplexity_ 도입 식 (3)을 사용한다.\n' +
      '\n' +
      '원시 퍼플렉스 점수를 조사하는 것보다 대신에 스트링에 나타나는 토큰이 동일한 스트링_에 작용하는 LLM의 기준선 퍼플에 놀라운 _상대인지 여부를 측정하는 것을 제안한다. 끈은 모든 에이전트, 기계 또는 인간에게 완료될 때 높은 엄격도를 초래하는 특성을 가질 수 있다. 그러나 우리는 인간의 다음 놀라운 선택이 기계보다 훨씬 더 혼란스러울 것으로 예상한다. 동일한 텍스트에 작용하는 기계의 예상 퍼플리티에 의해 관찰된 퍼플리티를 정규화하여, 우리는 프롬프트에 상당히 불변하는 검출 메트릭에 도착할 수 있다.\n' +
      '\n' +
      '우리는 _B 쌍안경_ 점수 \\(B\\)를 일종의 정상화 또는 불균형의 재향상으로 제안한다. 특히 교차성과의 불균일성 비율을 살펴본다.\n' +
      '\n' +
      '}({\\mathcal{M}_{\\mathcal{M}_{\\mathcal{M}_{fcal{M})\n' +
      '\n' +
      '여기서, 숫자란 단순히 흉막이며, 이는 문자열이 \\(\\mathcal{M}_{1}\\)에 얼마나 놀라운지를 측정하는 것이다. 분모는 \\(\\mathcal{M}_{2}\\)의 토큰 예측이 \\(\\mathcal{M}_{1}\\)에 의해 관찰될 때 얼마나 놀라운지를 측정한다. 직관적으로, 우리는 인간(\\mathcal{M}_{1}\\)보다 \\(\\mathcal{M}_{2}_{2}\\)에서 더 많이 분기될 것으로 기대한다(\\mathcal{M}_{1}\\) LLMs(\\mathcal{M}_{1}\\) 및\\(\\mathcal{M}_{1}\\)는 인간보다 서로 더 유사할 것으로 예상한다.\n' +
      '\n' +
      'i_B 쌍안경_ 점수는 기계 텍스트의 통계적 서명을 포착하는 일반적인 메커니즘이다. 아래 섹션에서 우리는 \\(\\mathcal{M}_{1}\\) 및 \\(\\mathcal{M}_{2}\\)의 가장 명백한 선택에 대해 _B Nightulars_가 단독보다 기계와 인간 텍스트를 훨씬 더 잘 분리한다는 것을 보여준다. 중요하게도, 제3 모델에 의해 생성된 일반 기계-텍스트를 모두 검출할 수 있다.\n' +
      '\n' +
      '흥미롭게도 약한 모델과 강한 모델의 차이를 대략 극대화하는 텍스트를 생성하여 고품질 텍스트 완성도를 생성하는 것을 목표로 하는 대조적 디코딩(Li et al, 2023)과 같은 두 가지 강력한 언어 모델을 대비하는 다른 접근법과 약간의 연결을 도출할 수 있다. 누적 디코딩은 유사하며(Chen et al., 2023; Leviathan et al., 2023) 더 약한 모델을 사용하여 완성도를 계획한다. 두 접근법 모두 매우 약한 2차 모델과 강한 모델을 페어링할 때 가장 잘 기능한다. 그러나 아래에서 알 수 있듯이 우리의 접근법은 성능에서 서로 매우 가까운 두 모델에 가장 적합하다. 이 연구의 나머지 부분에서 개방형 소스 모델 Falcon-7b 모델(\\(\\mathcal{M}_{1}\\)) 및 Falcon-7b 구성(\\(\\mathcal{M}_{2}\\))(Almathcal{M}_{2}\\))를 사용한다. 사용된 채점 모델의 전체 조합 세트는 부록의 표 2에서 찾을 수 있다.\n' +
      '\n' +
      '4개의 인증된 제로-샵 검출 기능이 있습니다.\n' +
      '\n' +
      '이 섹션에서는 제안된 점수를 평가하고 이를 사용하여 제로샷 LLM 검출기를 구축한다. i_B 쌍안경_를 사용하면 여러 영역에서 기계 생성 텍스트를 스팟팅할 수 있다. 실험 평가에서 우리는 검출 메커니즘에 대한 고려 없이 공통 사용 사례에서 생성된 것처럼 현대적인 LLM에서 기계 생성 텍스트를 검출하는 문제 설정에 중점을 둔다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리는 LLM 검출 문헌에 설명된 여러 데이터 세트를 사용하여 실험을 시작한다. 우리가 비교하는 가장 최근의 기준선은 고스트버스이다. 이 방법을 제안하는 버마 등(2023)은 또한 우리가 연구에 포함하는 3개의 데이터 세트를 소개하는데, _W 쓰기 프롭트_, _News_ 및 _학생 Essay_이다. 이들은 동일한 수의 인간 샘플 및 기계 샘플을 갖는 균형 잡힌 데이터 세트이다. 기계 샘플은 ChatGPT.3.3에 의해 작성된다.\n' +
      '\n' +
      '3: 다음에서는 GPT-4의 채팅 버전이 아닌 GPT-3.5-(트루보)의 채팅 버전에 항상 ChatGPT를 단기로 사용할 것이다.\n' +
      '\n' +
      '우리는 또한 ChatGPT를 제외하고 다른 언어 모델을 검출하는 능력을 평가하기 위해 자신의 여러 데이터 세트를 생성한다. CC뉴스(함보라트 알, 2017), PubMed(맨 et al, 2008), CNN(허만 et al., 2015)의 인간 필기 텍스트 샘플을 채취하여 LLaMA-2-7B 및 Falcon-7B를 사용하여 대체 기계 생성 완성도를 생성한다. 그러기 위해 우리는 각 인간 샘플의 처음 50개의 토큰을 벗겨내고 최대 512개의 기계 출력 토큰을 생성하는 프롬프트로 사용한다. 그런 다음 인간 프롬프트를 생성에서 제거하고 순수하게 기계 생성 텍스트를 기계-텍스트 데이터 세트에서만 사용합니다. 또한, GPT-3 및 GPT-4의 채팅 버전에서 기계 생성 완성으로 몇백만 개의 지시 프롬프트를 제공하는 Orca 데이터셋(Lian et al., 2023)을 사용하는데, 이 데이터셋은 명령어-튜닝 모델을 검출할 때 제안된 방법의 신뢰성을 확인하고 GPT-3과 GPT-4 사이의 검출 차이를 정량화할 수 있다.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '검출기는 이진 분류기이기 때문에 이진 분류 메트릭의 표준 스위트룸이 관련이 있다. 특히 ROC 곡선을 살펴보고 곡선 아래 영역(AUC)을 성능 메트릭으로 사용하는 것이 포괄적이라고 보는 경우가 많다. 실제로 버마(2023)와 미첼(2023) 등은 AUC와 F1 점수로 측정한 성능만 보고한다. 우리는 LLM 검출 정확도를 측정할 때 이러한 측정치만으로는 부적절하다고 주장한다.\n' +
      '\n' +
      '고열 감지 환경에서 가장 우려되는 해악은 종종 _허세 양성_, 즉 인간 텍스트가 기계 생성으로 잘못 표시된 경우에 발생한다. 이러한 이유로 낮은 허위율(FPR)에서 진정한 양성률(TPR)에 초점을 맞추고 \\(0.01\\%\\.4)의 표준 FPR 임계값을 채택하며 이전 출판물과 비교할 때만 F1 점수와 AUC 값을 제시하지만 낮은 FPR 값의 TPR 값에 핵심 메트릭으로 초점을 맞추는 것을 선호한다. 독자는 FPR이 \\(1\\%\\) 미만인 경우 AUC 점수가 종종 TRP@FPR과 관련이 없음을 관찰할 수 있다.\n' +
      '\n' +
      '부츠 4: 가장 작은 임계값은 계산 자원으로 충분한 통계적 유의성에 포괄적으로 평가할 수 있다.\n' +
      '\n' +
      '### Benchmark Performance\n' +
      '\n' +
      '손의 데이터 세트를 사용하여 _B 쌍안판의 AUC 및 TPR을 고스트버스(V Verma et al, 2023), GPTZero(Tian, 2023a), 디텍트GPT(LLaMA-2-13B 사용)를 사용하여 곡률을 점수화하는 것(Mitchell et al, 2023)과 비교한다. ChatGPT의 기계 샘플에 대한 이러한 비교가 GPTZero 및 Ghostbuster의 _in 호의_이고, 이러한 검출기가 ChatGPT 출력을 검출하기 위해 조정되었으며 LLaMA 모델의 샘플을 사용한 비교는 동일한 이유로 DetectGPT의 _in 호의_임을 강조한다.\n' +
      '\n' +
      '우리의 점수 기반 검출기는 하나의 단일 튜닝 가능한 파라미터만 가지고 있으며, 참조 데이터를 사용하여 미리 설정된 기계 및 인간 텍스트를 분리하기 위한 임계값이 있다. 우리는 ChatGPT를 사용하여 생성된 버마 등(2023년)의 뉴스, 창의 쓰기 및 학생 에세이 데이터셋의 모든 참조 데이터셋에서 훈련 스플릿의 조합을 사용하여 임계값을 설정했다. 또한 CC 뉴스, CNN 및 PubMed 데이터 세트의 프롬프트와 텍스트를 생성한 LLaMA-2-13B 및 Falcon-7B의 검출기를 비교한다. 이 모든 데이터 세트는 동일한 수의 인간 및 기계 생성 텍스트 샘플을 가지고 있다. 이 데이터 세트를 사용하여 전 세계적으로 임계값을 최적화하고 고정합니다. 한 가지 예외로서, 우리의 성능을 고스트버스터와 비교할 때 "도메인 아웃"의 기스트버스 정의를 충족하는지 확인하기 위해 임계 결정에 ChatGPT 데이터세트(뉴스, 창의 작성 및 스튜던트 Essay)를 포함하지 않으며 CC 뉴스, CNN 및 PubMed(LLaMA 및 팔콘을 통해 생성된)의 샘플만 사용하여 문턱을 선택하도록 한다.\n' +
      '\n' +
      '***Ghostbuster Datasets.** Ghostbuster 검출기는 ChatGPT에서 출력을 검출하기 위해 튜닝된 최근 검출기이다. 버마 등(2023년)에 의해 원래 작업에서 도입하고 조사한 동일한 3개의 데이터 세트를 사용하여 그림 1의 0.01% FPR에서 TPR을 비교(그림 11의 F1-Score 및 부록의 F1-Score)하여 "도메인 아웃" 설정에서 _B 쌍안형 Ghostbuster가 있음을 보여준다. 이 설정은 가장 현실적이며, 고스트버터의 학습 데이터 이외의 데이터셋에 대한 평가를 포함한다. 검출기를 위한 바람직한 속성은 더 많은 정보를 가진 것이 더 강하다는 것이다. 그림 3은 _B 쌍안경_과 고스트버터가 모두 이러한 특성을 가지고 있으며 _B 쌍안경_의 장점이 소수의 정권에 훨씬 더 명확하다는 것을 보여준다.\n' +
      '\n' +
      '*** 개방 소스 언어 모델*** 우리는 검출기가 부록에서 그림 4와 같이 LLaMA 및 팔콘과 같은 여러 LLM의 출력을 검출할 수 있음을 보여준다. 여기에서 우리는 또한 고스트버터가 실제로 ChatGPT를 검출할 수 있을 뿐이며 LLaMA 생성 텍스트를 확실하게 감지하지 못한다는 것을 관찰했다. 그림 4의 상세한 ROC 도표는 모든 방법에 대한 임계치 간의 성능을 비교한다.\n' +
      '\n' +
      '야생류의 5가지 신뢰성.\n' +
      '\n' +
      '야생에서 마주치는 시나리오에 직면했을 때 _B Nightulars_는 얼마나 잘 작동합니까? 이 섹션 전체에 걸쳐 강조하고자 하는 핵심 테두리는 _B 쌍안경_, 즉 식 (4)의 기초가 되는 점수가 _machine-텍스트 검출기_라는 것이다. 직관적으로, 이는 주어진 텍스트 조각이 유사한 언어 모델에 의해 생성되었을 가능성이 얼마나 될 수 있는지 예측한다는 것을 의미한다. 이것은 암기된 샘플, 비천연 스피커의 텍스트, 수정된 프롬프트 전략 및 에지 케이스에 관한 많은 중요한 의미를 가지며, 모두 이 섹션에서 종합적으로 평가한다.\n' +
      '\n' +
      'Varied T decree factor는 Varied T On On)\n' +
      '\n' +
      '자연 영어 이외의 추가 환경에서 감지기 성능을 탐색하여 정밀 조사를 시작합니다. 이를 위해 멀티 생성자, 멀티 도메인 및 멀티 수정자(M4) 검출 데이터세트(왕 등은 2023)를 조사한다. 이 샘플은 아렉시브, 레드디트, 위키오트 및 위키피디아 출처에서 유래하며 우르두, 러시아, 불가리아 및 아랍어와 같은 다양한 언어의 예를 포함한다. 이 데이터 세트의 기계 텍스트 샘플은 ChatGPT를 통해 생성된다. 그림 5에\n' +
      '\n' +
      '그림 4: ** LLaMA-2-13B 세대 검출**_B 쌍안경_는 경쟁 기저장보다 낮은 FPR에 대해 더 높은 TPR을 달성한다.\n' +
      '\n' +
      '그림 3: ** 프로브 성능에 대한 문서화의 영향. 도표는 다양한 문서 크기에 걸쳐 0.01% FPR에서 TPR을 표시한다. x축은 관측된 문서의 토큰 수를 나타내는 반면, y축은 해당 검출 성능을 나타내며, 이는 낮은 수의 토큰으로 검출할 수 있는 _B 쌍안경_ 능력을 강조한다.\n' +
      '\n' +
      '우리는 _B Nightulars_ 및 4개의 다른 기저부의 정밀도와 리콜을 보여줌으로써 우리의 방법이 도메인과 언어 전반에 걸쳐 일반화되는 것을 보여준다. M4 Datasets로 공개된 LLM 텍스트(LiBE et al., 2019), LLM 텍스트(Liu et al., Soliman et al., 2019)를 검출하기 위해 미세 회귀(LR GLTR)(Gehrmann et al., 2019)는 특성, 단어 및 문장 수준에서 신문의 특징을 사용하는 특정 토큰 분포, 스타일리즘(Li et al., 2014)에서 샘플링된 특징을 생성하는 로브RTa(Lihrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al.,Gehrmann et al., 2014)에서 예측을 생성하는 함수(Li et al., 강조한다. 우리는 참조를 위해 벤치마크에서 이 결과를 다시 결정합니다. 더 많은 소스 모델을 사용한 결과는 그림 6에 나와 있다.\n' +
      '\n' +
      '### Other languages\n' +
      '\n' +
      '공통 크로슬 데이터(표준 LLM 전처리 데이터)에서 잘 나타내지 않는 언어의 샘플에서 _B 쌍안경__를 평가할 때, 우리는 허위 비율이 낮게 유지된다는 것을 발견하며, 이는 위해 감소 관점에서 매우 바람직하다. 그러나 이러한 저자원 언어의 기계 텍스트는 종종 인간으로 분류된다. 그림 7은 우리가 실제로 합리적인 정밀도를 갖지만 이러한 환경에서 리콜이 좋지 않음을 보여준다. 이 점수 주문은 무해한 승리가지만, 왜 다국어 텍스트 검출이 제한되나요?\n' +
      '\n' +
      '위에서 서술한 바와 같이, _B 쌍안경은 기계-텍스트 검출기로서, 유사한 언어 모델에서 텍스트가 생성되었을 수 있는지 여부를 검출한다. 실험에서 사용하는 팔콘 모델은 이러한 저자원 언어로 텍스트를 생성하는 능력이 매우 제한적이기 때문에 ChatGPT 생성 텍스트는 기계 생성 가능성이 거의 없다. 더 강한 다국어 모델 쌍이 이러한 언어로 ChatGPT 생성 텍스트를 더 효과적으로 스팟할 수 있는 _B 쌍으로 이어질 것이라고 가정한다.\n' +
      '\n' +
      '리앙(2023) 등이 제기한 바와 같이 LLM 검출기는 무생성 영어 스피커(ESL)가 글을 기계 생성 초과로 분류하는 비천연 영어 스피커(ESL)에 대해 전적으로 편향되어 있다는 것은 LLM 검출기가 비생산 스피커(2023)가 작성한 텍스트에 대해 보편적으로 편향되어 있다는 것이다.\n' +
      '\n' +
      '그림 5: ** M4 Dataset** B 쌍안경으로부터의 다양한 도메인에서 ChatGPT 생성 텍스트의 검출은 검출을 위해 글로벌 임계치(도메인 외)를 사용하여 4개 도메인보다 높은 정밀도를 유지한다. 우리는 왕 등이 보고한 도메인 외 성능 메트릭의 평균을 사용한다(2023).\n' +
      '\n' +
      '그림 6: 다양한 생성 모델의 샘플에서 _B 쌍안경_의 성능을 나타낸다.\n' +
      '\n' +
      '그림 7: _B 쌍안경은 불가리아와 우르두에서 높은 정밀도로 작동하지만 4개 언어 모두에서 낮은 리콜을 받는다.\n' +
      '\n' +
      '정말 자주. 이를 테스트하기 위해 ESL 학생들의 학업 쓰기 개선 웹 페이지인 _EssayForum_의 에세이(EssayForum, 2022)를 분석한다. 이 데이터 세트에는 원본 에세이와 문법 보정 버전이 모두 포함되어 있습니다. 원본 및 문법 보정 샘플에 걸쳐 _B 쌍안경_ 점수의 분포를 비교한다. 흥미로운 사실은 리앙 등(2023)이 유사한 데이터 세트에 대해 조사한 상업용 검출기에 비해 _B 쌍안경은 보정 및 수정되지 않은 논술 데이터 세트 모두에 대해 99.67%에서 동일한 정확도를 나타낸다(그림 8 참조). 우리는 또한 비천연 영어 화자의 텍스트에 대한 _B 쌍안경_ 점수 분포가 동일한 에세이의 문법 보정 버전과 매우 중복되어 _B 쌍안경을 통한 검출이 이러한 유형의 이동에 둔감함을 보여준다.\n' +
      '\n' +
      '### Memorization\n' +
      '\n' +
      '각질 기반 검출의 하나의 공통 특징(또는 버그?)은 고도로 암기된 예를 기계 생성으로 분류한다는 것이다. 예를 들어, 훈련 데이터에 여러 번 나타나는 유명한 인용문은 이러한 현에 과적합성을 갖는 관찰자 모델에 따라 비정상성이 낮을 가능성이 있다. 여러 사례를 살펴보면, 이러한 유형의 데이터에 대해 _B 쌍안경_가 어떻게 수행되는지 살펴본다.\n' +
      '\n' +
      '본 연구에서 평가한 모든 유명한 텍스트에 대해 부록 A.3의 표 4를 참조한다. 먼저 미국 헌법 - 현대 LLM에서 대부분 외우는 문서에 대해 질문한다. 이 예는 0.76의 _B 쌍안경_ 점수가 있으며, 기계 범위에 잘 있다. 우리가 연구하는 11개의 유명한 텍스트 중 가장 낮은 점수(최소한 _machine-y_)였다. 11개 중 3개는 임계값의 기계 측면에 속한다.\n' +
      '\n' +
      '이 행동은 놀라운 일이 될 수 있지만 배치에 신중한 고려가 필요한 것은 기계-텍스트 검출기와 완전히 일치한다는 점에 유의하는 것이 중요하다. 실화 텍스트는 인간 작가들이 작성한 텍스트와 LLM에 의해 생성될 가능성이 높은 텍스트이다. 생성된 기계로서 암기된 텍스트를 분류하는 것은 일부 애플리케이션(예: 표절 검출), 또는 다른 애플리케이션(예: 훈련 코퍼스로부터 LLM 생성 텍스트를 제거하는 것)에서 허용되거나 심지어 바람직할 수 있다.\n' +
      '\n' +
      '전략의 최적화를 모델화했습니다.\n' +
      '\n' +
      '오픈 오르카 데이터 세트는 광범위한 작업(Lian et al., 2023)에 대해 GPT-3 및 GPT-4의 기계 세대를 포함한다. 이것은 이러한 현대 고성능 LLM 모두에서 _B 쌍안경_를 측정하기 위한 다양한 테스트 베드 역할을 한다. 임의로, _B 쌍안경은 GPT-3 샘플의 92%, GPT-4 샘플의 89.57%를 전 세계 임계값(기준 데이터세트로부터)을 사용할 때 검출한다. 참고로, 우리는 이것이 기계 생성 텍스트 세트 이상이기 때문에 정확성만을 보고한다. 이 데이터 세트는 또한 민감한 _B 쌍안경_가 프롬프트 수정에 어떻게 민감한지 탐색할 수 있도록 하는 프롬프트를 제공한다.\n' +
      '\n' +
      '단순 탐지 계획은 때때로 전략 마련에 대한 간단한 변화에 속아 표준 출력 분포에서 벗어난 스타일화된 텍스트를 생성할 수 있다. 이를 염두에 두고 LLaMA-2-13B-chat을 사용하고 출력의 스타일을 조정하도록 설계된 프롬프트를 사용합니다. 구체적으로, 표준 시스템에 적용함으로써 3개의 다른 시스템 프롬프트로 LLaMA2-13B-chat을 촉발하여 카를 사간의 음성에 기록하거나 기계식 또는 로봇 소리 단어 또는 해적처럼 작성 요청을 촉발했다.\n' +
      '\n' +
      '일반적으로 이러한 양식적 변화가 _B 쌍안경_의 정확도에 유의한 영향을 미치지 않는다는 것을 발견했다. 우리가 관찰한 가장 큰 영향은 해적 경계 출력을 요청할 때 발생하며, 이는 민감도(허위 음률 증가)만을 \\(1\\%\\)만큼 감소시키며, 그림 9. 1은 지정된 프롬프트 전략을 사용하는 샘플 프롬프트에 기초한 세대들을 기록한다.\n' +
      '\n' +
      '### Randomized Data\n' +
      '\n' +
      '다음으로, 임의의 실수, 해시코드 또는 다른 종류의 랜덤(및 랜덤 보일링) 스트링이 거짓 양성에 대한 우리의 모델을 편향하는지 테스트하고 싶습니다. 무작위성의 영향을 테스트하기 위해 우리는 팔콘 토큰라이저로부터 토큰의 무작위 서열을 생성하고 평소처럼 _B 쌍안경_으로 점수를 매긴다. 그림 10에서 이 분포의 히스토그램을 도표한다. 우리는 Falcon(인간은 약 \\(1.35\\)의 평균 점수가 약 \\(1\\)인 이 분포를 인간으로 자신 있게 점수를 매긴다는 것을 발견했다. 이는 훈련된 LLM이 강력한 언어 모델이며 어떤 상황에서도 토큰의 이러한 완전히 무작위 서열을 생성할 가능성이 매우 낮기 때문에 예상된다. 특히, 이러한 무작위 서열의 생성은 우연히 완벽한 인간 필기 텍스트의 생성보다 훨씬 덜 가능성이 있다.\n' +
      '\n' +
      '6가지 토론과 임장.\n' +
      '\n' +
      '모델에서 데이터가 검출되지 않는 제로 샷 사례에서 LLM을 검출하는 방법인 _B 쌍안경_을 제시한다. 우리의 전달 가능한 검출기는 생성 또는 그로부터의 예시 데이터에 사용되는 특정 모델에 액세스하지 않고 제로 샷 설정에서 작동한다. 우리는 우리가 생각하는 것 같아.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt}} \\hline \\hline \\multirow{2}{*}{Prompt} & Write an article based on this “Use an adjustable jump. Create your jump withinder blocks and plywood. Use natural outdoor obstacles as jumps. Set down a text on the far side of the jump. Practice at the lowest level until your dog is comfortable. Raise the bar gradually. Use the same method to teach your dog to jump through hoops.” \\\\ \\hline \\hline \\multirow{4}{*}{Default} & A well-trained dog is not only a default to be around, but it also provides a sense of pride and accomplishment to the owner. One capturing and useful skill that you may want to teach your dog is jumping. Whether you’re doing it for fun, agility competitions, or simply to keep your framy companion physically engaged, teaching your dog how to jump can be a bonding and rearving experience. In this article, we will discuss various steps and techniques that will guide you through the process of training your dog to confidently jump over obstacles and through hoops. \\\\ \\hline \\multirow{2}{*}{Carl Sagan} & Ab, my dear friend, we stand at the precipe of a most worntowns adventure, an odyssey of the canine kind. Our quest, to teach our loyal conclusions, those tiny wonders out, to some through the air with grave and agility. The tool at our disposal, the adjustable jump. This murred of jump stimulus allows to create a series of obstacles, each one more challenging than the last, and to guide our canine companions through them with precision and patience. \\\\ \\hline \\multirow{4}{*}{Non-Robotic} & Hey there, follow dog lovers? Are you looking for a fun and challenging way to keep your furry friend active and engaged? Look no further than an adjustable jump! Not only is it a great way to provide your dog with physical service, but it’s also a great way to bond with your pup. Plus, it’s super easy to set up and can be customized to fit your dog skill level. \\\\ \\hline \\multirow{4}{*}{Pirate} & Arrr, me heartiest: Ye landlubtese be wantini’ yer pouch ta learn i jump, eh? Well, fear not! It be herent to help ye teach sex per ferry first male it a jump over obstacles like a proper salty sex dog! First thing first, ye need to create the jump. Now, I know what ye be thinking! “Cinider blocks and plywood?” That be a bit, raused, don’t ye think?” But never fear, me heartiest: These materials be starved and easy to work with. Ye can build a jump that be adjustable, so ye can raise the bar it as sint yet pock\'s skill level. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '다양한 시스템 프롬프트가 있는 Open-Orca 샘플 프롬프트를 사용하여 LLaMA-2-13B-chat 세대로부터의 표 1: Excerpts. 이러한 양식화된 출력을 생성하는 프롬프트에 대한 완전한 수정을 위해 부록 A.5를 볼 수 있다.\n' +
      '\n' +
      '그림 10: 무작위 토큰 서열은 쌍안 임계값의 인간 측면에 엄격하게 떨어진다.\n' +
      '\n' +
      '이 전달 가능성은 모두 거의 동일한 변압기 성분을 사용하고 유사한 기간에서 대부분 공통 크로를 포함하는 데이터 세트 상에서 훈련될 가능성이 높기 때문에 현대 LLM 간의 유사성에서 발생한다. 오픈 소스 LLM의 수가 급격히 증가함에 따라 단일 검출기로 다중 LLM을 검출하는 능력은, 예를 들어 플랫폼 수정에 사용할 때 _B 쌍안경_의 주요 장점이다.\n' +
      '\n' +
      '우리의 연구는 여러 가지 한계를 가지고 있다. 제한된 GPU 메모리로 인해 검출기에서 더 큰(30B+) 오픈 소스 모델에 대한 광범위한 연구를 수행하지 않는다. 또한, 우리는 정상적인 사용에서 기계 생성 텍스트를 검출하는 문제 설정에 초점을 맞추고 검출을 우회하기 위한 명시적인 노력을 고려하지 않는다. 마지막으로, 본 연구에서 조사하지 않은 소스 코드와 같은 다른 비대화 텍스트 도메인이 있다.\n' +
      '\n' +
      '## Reproducibility Statement\n' +
      '\n' +
      '사용된 모든 데이터 세트와 이 작업의 본체에 사용하는 정확한 방법에 대한 세부 정보를 제공합니다. 또한, 이 작업의 보충 재료로 검출 점수 구현을 정확하게 복제할 수 있는 코드를 제공합니다. GPTZero와 같은 상업용 검출 API와의 비교는 2023년 9월부터 API 평가를 기반으로 하며 향후 재현 가능하지 않을 수 있으므로 투명하고 오픈 소스 솔루션의 중요성을 강조한다.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      '언어 모델 검출은 피해 감소, 인터넷 플랫폼 및 소셜 미디어에서 기계 생성 텍스트를 모니터링할지, 필터 학습 데이터 또는 채팅 애플리케이션에서 응답을 식별하는 핵심 기술일 수 있다. 그럼에도 불구하고, 검출 메커니즘이 증식하거나 증가시키는 대신 실제로 피해를 줄이기 위해 주의를 기울여야 한다. 우리는 제5절에서 제안된 _B 쌍안경_ 메커니즘에 대한 광범위한 신뢰도 조사를 제공하고, 예를 들어 비천연 스피커가 작성한 텍스트와 같은 영역을 고려할 때 신뢰성 측면에서 중요한 단계라고 믿는다. 그러나 이 분석에서는 LLM 검출 전략을 전개하는 과정에서 첫 번째 단계일 뿐이며 이러한 응용 프로그램의 개발자가 시스템에 미치는 영향을 신중하게 검증하지 않는다는 점에 주목한다. 우리는 특히 LLM 검출기의 존재가 이를 사용하는 것이 모든 시나리오에서 가치가 있음을 의미하지 않는다는 것을 주의한다.\n' +
      '\n' +
      '또한, 우리는 공통 사용 사례에서 LLM에서 생성된 것처럼 "자연적으로" 발생하는 기계 생성 텍스트를 검출하는 작업을 고려한다는 것을 명시적으로 강조한다. 우리는 감지기가 완벽하지 않고 동기부여가 시스템을 속이기 위해 노력하는 환경에서 어떤 성능도 보장하지 않는다는 것을 이해합니다. 다양한 테스트 소스에 걸쳐 철저한 평가를 제시하지만 종종 신경 네트워크에 의존하는 시스템의 경우와 같이 분류기를 우회하려는 지시 시도가 가능할 수 있음을 유지한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '이 작업은 ONR MURI 프로그램과 AFOSR MURI 프로그램에 의해 가능했다. 상업적 지원은 자본 원뱅크, 아마존 연구상 프로그램, 오픈 필란트로피 등이 제공하였다. 국가과학재단(IIS-2212182), NSF TRAILS연구소(2229885)에서 추가 지원이 이루어졌다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '(2023) 에바타암 알마즈로에이, 하바자 알로비치, 압둘라즈 알샤미시, 아리스탄드로 카펠리, 루산드라 코조카루, 메누네 데바, 에티엔 고피네, 다니엘 헤센 라운네, 퀀타인 말입자, 바레디네 네운테, 부티리스테 판니에, 구일헤르메 페데노 등이 있다. Falcon-40B: 최첨단 성능인 2023년 URL[https://falconllm.tii.ae/] (https://falconllm.tii.ae/)로 대형 언어 모델을 개방한다.\n' +
      '* 베일 등은 (2023) 크리스토퍼 베일, 리사 피니히로, 지미 로또 등이 있다. AI 콘텐츠 Poses Legal Challenges를 검출하는 데 어려움이 있습니다. __ AI 콘텐츠 Poses Legal Challenges. 법360_, 2023년 4월.\n' +
      '* 베일 등 (2021) 정말 M. 벤더, 팀니트 게브루, 엔젤리나 맥밀란 메이저, 샤마가렛 샬트셸. 스모스틱 파라당당인 캔어모델 베투 빅에서? 2021년 ACM 공정성, 회계성 및 투명성_검토에서 FAccT \'21, pp 610-623, 뉴욕, NY, 2021년 3월 컴퓨팅 기계 협회\'. ISBN 978-1-4503-8309-7. 도이: 10.1145/3442188.3445922 URL[https://doi/10.1145/3442188.3445922] (https://doi/10.1145/3442188.3445922)\n' +
      '* Bhatt와 Parthasarathy (2020) Meghana Moorthy Bhat 및 Srinivasan 파라사르병증이 있다. 기계 디펜드 어인스트 기계 생성 페이크 뉴스는 얼마나 효과적인 사용 가능합니까? 경험적 연구입니다. NLP_, pp. 48-53, 온라인, 2020년 11월 컴퓨터 동역학 협회의 네거티브 결과에 대한 인벤토리에 관한 제1워크숍의 _검토. 도이: 10.18653/v1/2020.7 URL[https://aclanthology.org/2020.인치권리-1.7](https://aclanthology.org/2020.insights-1.7).\n' +
      '* 바이데르만과 라프(2022) 스텔라 비데르만과 에드워드 라이프가 있다. 레트로닉된 언어 모델을 사용한 MOSS 검출을 풀링합니다. 제31회 ACM 정보 지식 관리 국제 회의의 _검토에서 CIKM \'22, pp. 2933-2943, 뉴욕, NY, 2022년 10월 컴퓨팅 기계 협회. ISBN 978-1-4503-9236-5. 도이: 10.1145/3511808.3557079 URL[https://dl.org/doi/10.1145/doi/10.1145/3511808.3557079] (https://dl.acg/doi/10.1145/3511808.3557079)\n' +
      '베냐민 만, 니키 수비야, 나르드 카플란, 펠라 다하리왈, 아빈드 네힐란탄, 아반드 네오아칸트, 아칸다 시스트리, 아미다 아카이왈, 샌델 헤베르트-바르왈, 그레첸 크라우거, 톰 헨하안, 아디티야 라메스, 다니엘 M. 지글러, 제프리 우, 크렘센 동계, 크리스토퍼 헤세, 마크 크런, 에릭 시글러, 메테우스즈 리트윈, 스콧 그레이, 벤자민 체스, 잭 클락, 크리스토퍼 버너, 삼맥커워드, 알레크 라드포드, 아이리사 세이츠케버, 다리오 아모디 등이 동계이다. 언어 모델은 Few-Shot Learner입니다. Nural 정보 처리 시스템(NeurIPS 2020)__34차 URL[2020/한화/2020/hash/1457c0d6bfcd967418bfb8ac142f64a-Abstract.hml] (https:// homose.cc/종이/1457bfb8ac142f64a-Abstract.hat.\n' +
      '* 차크라비 등은 (2023) 소우라디프 차크라비, 아미트 싱메디, 시정주, 방안, 딘만노차, 푸롱황 등이다. AI 생성 텍스트 검출의 가능성 __에서 AI 생성 텍스트 검출의 가능성. xiv:2304.04736[cs]_, 2023년 4월 도이: 10.48550/arXiv.2304.04736 URL[http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736])이다.\n' +
      '* 첸 등은 (2023) 찰리 첸, 세바스티안 보게우드, 거프리 어빙, 장-베티스트 레스피아우, 로랑 시프레, 존 주커퍼 등이다. 수혜적 샘플링으로 증폭된 대용량 언어 모델 증폭 __가속화 대어 모델 샘플링. xiv:2302.01318[cs]_, 2023. 2. 도이: 10.48550/arXiv.2302.01318 URL[http://arxiv.org/abs/2302.01318](http://arxiv.org/abs/2302.01318])이다.\n' +
      '바흐네 아네르드니 시우네, 다윗 자르코, 다케나 세바니 오세네, 다케네, 다케네, 다케네, 다케르네, 다케네, 다케네, 다케르네, 다케르네, 다케네, 다케르네, 다케르네, 다케네, 다케네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네, 다케르네르네 말리아야 모레카야, 마리 페랄라, 르네르카야에라, 르네르카야 모레로프, 에워사노프, 오랑크산드르 폴로조프, 캐서린 리, 자롱웨이 저우, 제르히 왕, 크렌난 사타, 마크 디아, 오르한 피라트, 미하클 카라스타, 미하슨 카라스타, 제이슨 웨이, 쿠슨 웨이, 쿠슨 마에이, 제프 델리스, 제피, 제프, 제아비, 제라 피에타, 쁘라 피에타, 미들캐스타, 미스 웨이, 제라, 제라 피에타, 미들 피르타, 미들 피르타, 미들 피르타, 미들 피르타, 미레 카라스 웨이, 제이슨 카스타, 제이슨 웨이, 쿠슨 웨이, 쿠슨 웨이, 쿠슨 웨이, 쿠슨 웨이, 쿠슨 웨이, 쿠슨 마에리, 제아스 메이스트린, 제슨 마에리, 제아스 메리, 제슨 마에리, 제아스 아이, 제이, 제아스 PaLM: path 고속도로와 함께 모델링하는 스칼링 언어 모델. __athing 언어 모델. arXiv:2204.02311[cs]_, 2022년 4월 URL[http://arxiv.org/abs/2204.02311](http://arxiv.org/abs/2204.02311])이다.\n' +
      '* 크로테르 등 (2022) 에반 크로테스, 나탈리 자프코위즈, 헤르나 비코르 등이 있다. 기계 유전 텍스트: 호흡 모델 및 검출 방법에 대한 종합 조사. __ 처리 방법의 종합 조사. 2210.07321[cs]_, 2022년 11월, 10.48550/arXiv.2210.07321: URL[http://arxiv.org/abs/2210.07321](http://arxiv.org/abs/221010.07321])이다.\n' +
      '* 다히니 등은 (2023) 마흐디 다히니, 웨셀 펠만, 에게 에르도안 등이다. ChatGPT를 검출하는 것: ChatGPT를 검출하는 국가 조사. _ ChatGPT 생성 텍스트를 검출하는 국가의 조사. xiv:2309.07689[cs]_, 2023. 9. 도이: 10.48550/arXiv.2309.07689 URL[http://arxiv.org/abs/2309.07689](http://arxiv/abs/2309.07689])이다.\n' +
      '2022년 9월 하그깅 페이스에서* 다타셋. URL[https://huggingface.co/dataset/n989/Essay a-Dataset] (https://huggingface.co/dataset/iv989/Essay.\n' +
      '세바스티안 게르만, 헨드리크 스트로벨트, 알렉산더 러쉬. GLTR: 통계 검출 및 생성된 텍스트의 시각화. 마타 R. 코스타-주사 및 엔리케 알폰세아(종), 컴퓨터 통계 협회의 제57차 연례 회의 : 시스템 시범_, pp 111-116, 피렌체, 이탈리아 7월 컴퓨터 통계 협회의 _수익이다. 도이: 10.18653/v1/P19-3019 URL[https://aclanthology.org/P19-3019](https://aclanthology.org/P19-3019).\n' +
      '* Ghosal 등은 (2023) Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha 및 Amrit Singh Bedi. 아 생성 텍스트 검출의 가능성 및 불가능 가능성: A 조사: a a-생성 텍스트 검출의 가능성 및 불가능성. arXiv 프리프린트 arXiv:2310.15264_, 2023.\n' +
      '* 구오 등은 (2023) 비양구오, 신장 장, 지유안 왕, 미니 장, 진란 니, 유시아안 딩, 지안웨이 유, 유펑 우 등이 있다. Close는 휴먼 전문가에게 어떻게 ChatGPT가 있습니까? __비교법인, 평가 및 검출. _비교법인, 평가 및 검사. __비교법인, 평가 및 검출. xiv:2301.07597[cs]_, 2023. 1. 도이: 10.48550/arXiv.2301.07597 URL[http://arxiv.org/abs/2301.07597](http://arxiv.org/abs/2301.07597])이다.\n' +
      '*햄보그 등 (2017) 펠릭스 함보그, 노먼 메수쉬케, 코린나 브라이밍거, 벨라 기프 등이 있다. 뉴스-파악: A 제네릭 뉴스가 크롤러와 추출기입니다. 제15차 국제 정보 과학 심포지엄의 _검토에서 2017년 3월 pp. 218-223, 도이: 10.5281/제노도.4120316.\n' +
      '* 헬름 등 (2023) 하든 헬름, 케미 에리베, 웨이웨이 양. 유전적 모달에 대한 통계 투링 테스트. __ 유전체 모델을 위한 통계 튜링 테스트. xiv:2309.08913[cs]_, 2023. 9. 도이: 10.48550/arXiv.2309.08913 URL[http://arxiv.org/abs/2309.08913](http://arxiv.org/abs/2309.08913])이다.\n' +
      '2015년 12월 MIT 프레스. MIT 프레스.\n' +
      '* 호른 등은 (2019) 벤자민 D. 호른, 제페 노레가드 및 시벨 아달리. 시간 및 공격에 따른 가짜 뉴스 탐지는 시간 및 공격에 따른 잘못된 가짜 뉴스 검출. __로스트 가짜 뉴스 감지. ACM 전달. 인텔. 시스터. ____ 기술.___ 기술.__ 기술. 기술.__ 기술. 기술. 기술. 2019년 ISSN 2157-6904. 도이: 10.1145/3363818 URL[https://doi/10.1145/3363818](https://doi/10.1145/336381818)\n' +
      '* Hu et al.(2023) 샤오멍후, 핀유첸, 타성이호. RADAR: 아보투스 학습을 통한 로부스 AI-텍스트 검출. __ xiv:2307.03838[cs]_, 2023년 7월 10.48550/arXiv.2307.03838 URL[http://arxiv.org/abs/2307.03838](http://arxiv.org/abs/2307.03838])이다.\n' +
      '* 커첸바우어 등은 존 키르첸바우어, 조나스 거핑, 유신 위엔, 조나단 카츠, 이안 미지에스, 톰 골드슈타인 등이 있다. 큰 언어 모델을 위한 와테르마크를 사용합니다. 기계학습_, pp 17061-17084. PMLR, 2023년 7월 URL[https://propress/v202/kirchenbauer23a.html](https://proing/v202/kirchenbauer23a.html].\n' +
      '* 크리슈나 등은 (2023) 칼페스 케리샤나, 요시아오 송, 마젠가 카핀스카, 존 위엣잉, 모하이트 이이어 등이 있다. AI 생성 텍스트의 파라프레이싱은 AI 생성 텍스트의 검출기를 회피하지만, 검색은 효과적인 방어이다. 2303.13408[cs]_, 2023년 3월 도이: 10.48550/arXiv.2303.13408 URL[http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408])이다.\n' +
      '* 레비아탄 등은 (2023) 옌비 레비아탄, 마탄 칼만, 요시 마티아스 등이 있다. 누적 탈코딩을 통한 트랜스포머의 마지막 해결. 기계 학습_, pp 19274-19286. PMLR, 2023년 7월 URL[https://propress/v202/leviathan23a.html] (https://proing/v202/leviathan23a.html)에 대한 제40차 국제 회의의 _검토에서.mlr.press/v202/leviathan23a.html].ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.ml.\n' +
      '* Li 등은 (2014) 제니 S. 리, 존 V. 모나코, 리치우 첸, 찰스 C. 티퍼트. 소셜 네트워킹 사이트의 짧은 메시지를 이용한 인증. e-비즈니스 엔지니어링_, pp. 314-319, 2014. 도이: 10.1109/ICEBE.2014.61에 대한 _2014 IEEE 11차 국제 회의.\n' +
      '*리 등은 알(2023) 샤앙리사 리, 아리 홀츠만, 다니엘 프리드, 퍼시 리앙, 제이슨 에세너, 타쓰노리 하시모토, 루크 제트렘요어, 마이크 루이스 등이 있다. 대조적 탈코딩: 오픈 수정 교과서 세대는 최적화이다. 2012년 7월 캐나다 토론토 컴퓨터통계협회의 제61차 컴퓨터통계학회 연례회의(1: 롱 파이어스)_, pp 12286-12312, pp 12286-12312) 개최. 10.18653/v1/2023.687 URL[https://aclanthology.org/2023.acl-장기.687](https://aclanthology.org/2023.acl-오랜.687).\n' +
      '\n' +
      '* Lian et al. (2023) Wing Lian, Bleys굿손, 유진펜틀랜드, 오스틴 쿡, 찬비베드 발레 및 "Teknium"입니다. 오픈러카: gpt 증강 플란 추론 흔적의 오픈 데이터 세트입니다. https://[https://huggingface.co/개방-Orca/개방Orca](https://huggingface.co/개방-Orca/개방OrOrca) 2023.\n' +
      '* 리앙 등은 알(2023) 웨신 류, 마트 육세곤울, 유닝 마오, 에릭 우, 제임스 주 등이 있다. GPT 검출기는 비천연 영어 작가에게 편향되어 있다. __GPT 검출기가 편향되어 있다. 2304.02819[cs]_, 2023년 4월 도이: 10.48550/arXiv.2304.02819 URL[http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819])이다.\n' +
      '* 류 등은 (2019) 예인산 류, 미일 오트, 남안 고살, 징페이 듀, 만다르 조시, 다치 첸, 오머 레비, 마이크 루이스, 루케 제트렘요어, 베셀린 스토야노프 등이다. 로버타: 강건하게 최적화된 바트 사전 실행 접근, 2019.\n' +
      '* 리옌아지와 부스칼디(2023) 비진이 리옌아지와 다빈드 부스칼디. 예술적 유전 학문의 검출: 대어모델의 인간 활용 감수성의 의미. 엘리사베스 메아이스에서는 블리사베스 메자아니, 비자야 수쓰마네란, 워런 마누닝, 스테판 리프 마가니크(종), _자연어 처리 및 정보 시스템_, 컴퓨터 과학, pp. 558-565, Cham,스프링어 네이처 스위스의 렉큐어 네테. ISBN 978-3-031-35320-8. 도이: 10.1007/978-3-031-35320-8-35320-8.42.\n' +
      '* 미라스키 등은 알(2022) 예브라 데몬티스, 아미브라 데몬티스, 지드립 코탁, 라 샹카라, 덩게리, 류양, 샤우유 장, 모라 핀토, 원케 리, 유발 엘로비치, 바티스타 빅지오 등이 있다. 조직화에 대한 연방 AI의 위협. __ 조직화에 대한 공공 AI의 위협. DB://www.com/과학/입자/pii/S0167404822003984](https://www.scenced.com/과학/입자/pii/S01674022003984), 124:103006, 2023. 1. ISSN 0167-4048.\n' +
      '* 미첼 등은 에릭 미첼, 윤호, 알렉산더 카자츠키, 크리스토퍼 디 마닝, 첼시 핀 등이 있다. 프로벤트 GPT: 제로-쇼트 기계 생성 텍스트 검출이 가능성 커브리티시(Probability Curvature)를 사용한다. 기계 학습_, pp 24950-24962. PMLR, 2023년 7월 URL[https://propress/v202/mitchell23a.html] (https://proing/v202/mitchell23a.html)의 제40차 국제 컨퍼런스에서.mlr.press/v202/mitchell23a.html].\n' +
      '* 푸 등은 알(2022) 지슈 푸, 지이황, 야동 시, 구안 첸, 위지 첸, 루정 장 등을 들 수 있다. 기계 유전 텍스트에서 아티큘러스의 미스터리를 풀어보세요. 10차 언어 자원 평가 회의_의 _검토에서 pp 6889-6898, 프랑스 마르세유, 2022년 6월 유럽 언어 자원 협회. URL[https://aclanthology.org/2022.lrec-1.744](https://aclanthology.org/2022.lrec-1.744)\n' +
      '* 라드포드 등은 (2019) 알레크 라드포드, 제프리 우, 레원 아이, 데이비드 루안, 다리오 아모디, 아이라이아 세이츠케버 등이 있다. 언어 모델들은 언어화된 멀티태스펜션 학습자들이다. _ 언어 모델들은 언어렉션 멀티태스크 학습자들이다. 오픈AI_, pp. 24, 2019.\n' +
      '* 사다시반 등은 (2023) 비누 산카르 사디반, 아운논 쿠마르, Sriram Balasubramanian, Wenxiao 왕, 소헤일 Feizi 등이다. AI 생성 텍스트가 확실히 감지될 수 있습니까? __ AI 생성 텍스트가 확실히 감지될 수 있습니까? xiv:2303.11156[cs]_, 2023년 3월 도이: 10.48550/arXiv.2303.11156 URL[http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156])이다.\n' +
      '*센 등은 (2008) 프리스비르자센, 갈릴레오 나나타, 무스타파 빌리프, 리이즈 게터, 브라이언 갤러레이어, 티나 에리아시래드 등이다. 네트워크 데이터의 집합 분류. __ 네트워크 데이터의 집합 분류. __ 군집 분류. AI 매거진_, 29(3):93-93, 2008년 9월 ISSN 2371-9621. 도이: 10.1609/임마진/인디파진/입자/관/2157.\n' +
      '*솔라이만 등은 (2019) 이렌 솔라이만, 마일스 브룬다, 잭 클라크, 아란다 허버트-보스, 아리엘 헤베르트-보스, 제프 우, 알레크 라드퍼드, 그레첸 크루거, 종욱 김, 사라 크레이프, 마일스 매크레스, 알렉스 뉴하우스, 제이슨 블러키스, 크리스 맥고피, 자스민 왕. 언어모달의 해제전략과 사회영향은 __전산전략과 언어모달의 사회적 영향이다. 1908.09203[cs]_, 2019. 11. 도이: 10.48550/arXiv.1908.09203. URL[http://arxiv.org/abs/1908.09203](http://arxiv.org/abs/1908.09203])이다.\n' +
      '* 수 등은 (2023) 진안수, 테리 유에 주오, 디왕, 프레슬라브 나코프 등이다. 타이펙트LLM: 머신 생성 텍스트의 제로-쇼트 검출에 대한 레버리징 Log Rank 정보. __기계 생성 텍스트이다. xiv:2306.05540[cs]_, 2023. 5. 도이: 10.48550/arXiv.2306.05540 URL[http://arxiv.org/abs/2306.05540](http://arxiv.org/abs/2306.05540])이다.\n' +
      '* 탕 등 (2023) 루닉시앙 당, 유펑 추앙, 샤후. LLM 생성 텍스트를 검출하는 과학. __ LLM 생성 텍스트를 검출하는 과학입니다. xiv:2303.07205[cs]_, 2023년 3월. 도이: 10.48550/arXiv.2303.07205. URL[http://arxiv.org/abs/2303.07205](http://arxiv.org/abs/2303.07205])이다.\n' +
      '\n' +
      '* 톈(2023a) 에드워드 톈. Gpt제로 업데이트 v1, 2023a. 1. URL[https://gpt제로.substack.com/p/gpt제로-업데이트-v1](https://gpt제로.substack.com/p/gpt제로-업데이트-v1)\n' +
      '* 톈(2023b) 에드워드 톈. 새해, 새로운 기능, 새로운 모델, 2023년 1월. URL[https://gpt제로.substack.com/p/새 연도 새로운 특징 모델](https://gpt 0.substack.com/p/새 연도 새로운 특징 모델)\n' +
      '* 톈 등은 (2023)유구안 톈, 한팅첸, 수타오 왕, 저장안 바이, 청화 장, 루피펑 리, 차오 주, 윤허 왕이다. AI 생성 텍스트의 다단계 양성 표지 표지 프로브 __AI 생성 텍스트의 다단계 양성 표지 표지 검출이다. xiv:2305.18149[cs]_, 2023. 6. 도이: 10.48550/arXiv.2305.18149. URL[http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149])이다.\n' +
      '오케네 아흐네르, 아말리아누에, 아네네 루위, 아네네우, 아네우우, 아네우우, 아흐네르와, 아흐네르네, 아레누 아, 라미우우, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누아, 아레누, 아레누, 아라, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누 아, 아레누, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아레누 아, 아, 아 라마2: 오픈 재단과 파인테드 차트 모델 __라마2: 오픈 재단과 파인-테드 차트 모델. xiv:2307.09288[cs]_, 2023. 7. 도이: 10.48550/arXiv.2307.09288 URL[http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288])이다.\n' +
      '* 툴킨스코이 등은 (2023) 에스타드 툴킨스크리, 크리스티아 쿠즈네소비, 라이다 쿠슈나레바, 다니일 체르니아브스키, 세르히에이 바만니코프, 이리나 피오니코프스카야, 세르게이 니들렌코 및 에베젠이 베르나에프 등이다. AI 생성 텍스트의 로보스트 탐지를 위한 외재적 Dimension Estimation. __ AI 생성 텍스트. xiv:2306.04723[cs]_, 2023. 6. 도이: 10.48550/arXiv.2306.04723 URL[http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723])이다.\n' +
      '* 투린(2020) 투린 이네.com URL[https://www.turnitin.com/](https://www.turnitin.com/]).\n' +
      '* 바셜니 등 (2020) 라브 R. 바셜니, 니티시 시리쉬 칼샤르, 리처드 소셔. 대규모-스케일 언어 모델들에 의해 유전된 텍스트들을 검출하는 한계들. _2020 정보 이론 및 신청 워크샵(ITA)_, pp 1-5, 2020년 2월 도이: 10.1109/ITA50056.2020.9245012.\n' +
      '* 바실라토스 등은 (2023) 크리스토포로스 바실라토스, 마나라 알람, 탈랄 라완, 야시르 자키, 미즈마 마나타코 등이 있다. 어떻게kGPT: 콘텍스트-Aware Perplexity 분석을 통해 ChatGPT 생성 대학 학생 가정 작업의 검출을 조사하는 __. xiv:2305.18226[cs]_, 2023. 6. 도이: 10.48550/arXiv.org/abs/2305.18226](http://arxiv.org/abs/2305.18226])(http://arxiv.org/abs/2305.18226).\n' +
      '* 버마 등은 (2023) 비브크 버마, 에브 플리시그, 니콜라스 톰린, 단클린 등이 있다. 고스트버스터: 대형 언어 모델 __고스트버터: 대형 언어 모델들에 의해 작성된 텍스트 고스트를 검출하는 것이다. xiv:2305.15047[cs]_, 2023. 5. 도이: 10.48550/arXiv.2305.15047 URL[http://arxiv.org/abs/2305.15047](http://arxiv.org/abs/2305.15047])이다.\n' +
      '* 왕 등은 (2023) 유시아 왕, 조니비키 만수로프, 페타르 이바노프, 진옌 수, 아르엠 셸만노프, 아림 치비건, 첸시 화이트하우스, 오사마 모하메드 아프리카잘, 타레크 마흐무드, 알햄 피키 아리, 프레슬라브 나코프 등이다. M4:멀티 생성기, 멀티 도메인 및 다중 수정 블랙-박스 기계 생성 텍스트 검출. __수정 블랙-박스 기계 생성 과정. xiv:2305.14902[cs]_, 2023. 5. 도이: 10.48550/arXiv.2305.14902 URL[http://arxiv.org/abs/2305.14902](http://arxiv.org/abs/2305.14902])이다.\n' +
      '* 월프와 월프(2022) 맥스 월프와 스투르트 월프입니다. 신경 텍스트 디텍터 __공격 신경 교과 디텍터. _공격 신경 교과 디텍터. 2002.11768[cs]_, 2022. 1. 도이: 10.48550/arXiv.2002.11768 URL[http://arxiv.org/abs/2002.11768] (http://arxiv.org/abs/2002.11768])이다.\n' +
      '* 양 등은 (2023) 시안준양, 웨이청, 린다 페트폴드, 윌리엄 양왕, 하이펑첸 등이 있다. DNAGPT: GPT 생성 텍스트의 훈련 없는 검출을 위한 Divergent N-Gram 분석. _. xiv:2305.17359[cs]_, 2023. 5. 도이: 10.48550/arXiv.2305.17359 URL[http://arxiv.org/abs/2305.17359](http://arxiv.org/abs/2305.17359])이다.\n' +
      '*유 등은 알(2023) 샤오유, 유앙기, 키장 첸, 구취앙 첸, 시양, 펑유안 주, 웨이밍 장, 니그라이 유. GPT 인식 테스트: GPT 유전자 상속을 사용한 GPT 유전자 유전자 검출: GPT 유전자 검사이다. 2305.12519[cs]__, 2023년 5월 도이: 10.48550/arXiv.2305.12519 URL[http://arxiv.org/abs/2305.12519](http://arxiv.org/abs/2305.12519])이다.\n' +
      '\n' +
      '로완 자셀러, 아리 홀츠만, 한나 라시킨, 연가탄 Bisk, 알리 파하디, 프랑지스카 로제너, 예진 최씨 등이다. 에인스트 신경페이크 뉴스를 허용합니다. 신경 정보 처리 시스템_의 _Advances.\n' +
      '* 자한 등은 (2023) 해올란 자한, 시안리 허, 키오그카이 주, 유시앙 우, 폰투스 스트렌토르프 등이다. G3 검출기 __G3 검출기: 일반 GPT 생성 텍스트 검출기: 일반 GPT 생성 텍스트 검출기이다. 2305.12680[cs]__, 2023년 5월 도이: 10.48550/arXiv.org/abs/2305.12680] URL[http://arxiv.org/abs/2305.12680](http://arxiv.org/abs/2305.12680).\n' +
      '\n' +
      '부록 A.\n' +
      '\n' +
      '### Benchmark Performance\n' +
      '\n' +
      '***ChatGPT 텍스트 검출*** F1-점수는 (V Verma et al., 2023)에서 방출된 ChatGPT 데이터 세트에 있다. 제로-쇼트 기준 방법에 대한 숫자는 동일한 작업에서 취해진다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '오더 스코터링 모델 박람회*********************와 비교됩니다.\n' +
      '\n' +
      '*** 스트링 레인** _B dosesulars_ 점수와 시퀀스 길이 사이에는 상관관계가 있는가? 이러한 상관관계는 특정 길이에 대한 잘못된 결과에 대한 편향을 생성할 수 있다. 그림 12에서 토큰 시퀀스 길이와 _B 쌍안_ 점수의 공동 분포를 보여준다. 서열 길이는 수업 회원에 대한 정보를 거의 제공하지 않는다.\n' +
      '\n' +
      '**Score Components** Perplexity는 분리 시 많은 검출 제형에 의해 사용된다. 우리는 그림 13에 퍼플리티와 교차 투과성이 모두 분리에서 효과적인 검출기가 아님을 보여준다. 표 3은 PPL과 X-PPL을 서로 다른 모델 계열 viz. LLaMA-2 및 Falcon으로 계산하는 결과를 보여준다.\n' +
      '\n' +
      '유명 텍스트.\n' +
      '\n' +
      '밥 딜란에 의한 두 곡은 이 행동을 추가로 입증한다. __밥 딜란에 의한 두 곡은 이 행동을 추가로 보여준다. Blowin\' The Wind_에서 유명한 Dylan 트랙은 미개척곡 _~Fall In Love With_(logPPL 값 각각 1.11과 3.30)보다 훨씬 낮은 Falcon 퍼플렉시티를 가지고 있다. 그러나 쌍안경_은 이 두 샘플을 모두 인간 샘플(각각 0.92점 및 1.01점)으로 자신 있게 표지한다.\n' +
      '\n' +
      '안내하세요.\n' +
      '\n' +
      '우리는 방정식 (4)에서 동일한 \\(\\mathcal{M}_{1}\\) 및 \\(\\mathcal{M}_{1}\\) 모델을 사용할 때 쌍안경의 성능을 검사한다. 우리는 Falcon-7B 및 Falcon-7B 구성 모델을 사용하고 두 공연을 Falcon-7B 및 Falcon-7B 구성 모델과 비교하고 두 공연을 비교한다.\n' +
      '\n' +
      'ChatGPT 생성 텍스트의 검출을 위한 그림 11:F1 점수는 여러 검출기가 유사하게 수행함을 나타낸다. 우리는 이 메트릭이 낮은 FPR에서 성능의 좋지 않은 지표가 될 수 있는 방법에 대해 아래에 논의한다.\n' +
      '\n' +
      '그림 12: A는 고스트버스 뉴스 데이터셋의 시퀀스 길이 측면에서 점수들의 실제 분포를 자세히 살펴본다.\n' +
      '\n' +
      '그림 15의 데이터셋(Verma et al., 2023)에 대한 쌍안경 스코어. 바닐라 쌍안경 점수가 3개 도메인에 가장 우수하지만 Falcon-7B를 입력 모델로 사용하는 것은 경쟁력이 있다.\n' +
      '\n' +
      '변형된 시스템 프로펩티드입니다.\n' +
      '\n' +
      '우리는 여러 가지 신속한 전략에 대해 섹션 5.4에서 쌍안경과 유사한 기저부의 성능을 테스트한다. 우리는 Open-Orca 데이터 세트의 샘플로 LLaMA-2-13B-chat을 촉발했다. 기본 샘플 특정 프롬프트 외에도 시스템 프롬프트에 지침을 부팅하는 3가지 다른 버전을 사용합니다. 여기에는 카를 사간의 스타일로 쓸 수 있는 지시를 비생물적으로 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline PPL Score (\\(\\mathcal{M}_{1}\\)) & X-Cross PPL Scores (\\(\\mathcal{M}_{1}^{\\prime}\\), \\(\\mathcal{M}_{2}\\)) & TPR & at TPR & at & F1-Score & AUC \\\\  & & \\(0.01\\%\\) FPR & \\(0.1\\%\\) FPR & & & \\\\ \\hline Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 100.0000 & 100.0000 & 1.0000 & 1.0000 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 99.6539 & 99.6539 & 0.9982 & 0.9999 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 99.3079 & 99.3079 & 0.9965 & 0.9998 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.3549 & 98.3549 & 0.9913 & 0.9997 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 98.7200 & 99.1600 & 0.9953 & 0.9996 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 94.9200 & 99.4000 & 0.9963 & 0.9996 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 95.8441 & 97.5757 & 0.9922 & 0.9996 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.6400 & 99.0400 & 0.9953 & 0.9995 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.8000 & 99.2800 & 0.9959 & 0.9995 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.1600 & 98.6000 & 0.9937 & 0.9992 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.4000 & 98.7200 & 0.9943 & 0.9992 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 94.1125 & 97.9220 & 0.9926 & 0.9992 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 93.5000 & 93.5000 & 0.9875 & 0.9990 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 92.0000 & 92.0000 & 0.9918 & 0.9990 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 94.0000 & 94.0000 & 0.9850 & 0.9989 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.0000 & 98.0000 & 0.9956 & 0.9988 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 72.6957 & 72.7857 & 0.9908 & 0.9988 \\\\ Llama-2-13B & Llama-2-13B, Llama-2-13B & 97.8750 & 97.8750 & 0.9931 & 0.9987 \\\\ Llama-2-13B-Chat & Llama-2-13B, Llama-2-13B-Chat & 71.3199 & 82.6799 & 0.9846 & 0.9986 \\\\ Llama-2-13B, Llama-2-13B & 97.5000 & 97.5000 & 0.9875 & 0.9985 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 97.5778 & 97.5778 & 0.9930 & 0.9983 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 23.3076 & 48.3732 & 0.9842 & 0.9975 \\\\ Llama-2-13B & Llama-2-13B & 0.3200 & 32.0800 & 0.9840 & 0.9968 \\\\ Llama-2-13B-Chat & Llama-2-13B, Llama-2-13B-Chat & 20.9172 & 60.0671 & 0.9763 & 0.9968 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 47.1476 & 69.2953 & 0.9747 & 0.9964 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 본체에 설명된 참조 데이터 세트에 대해 평가된 점수 모델의 다른 조합.\n' +
      '\n' +
      '그림 13: 퍼플렉시스 및 크로스-퍼플렉스성은 스스로 강한 검출기가 아니다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:19]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline \\multirow{2}{*}{Human Sample} & \\multirow{2}{*}{PPL (Falcon 7B Instruct)} & Cross PPL (Falcon 7B, Falcon 7B Instruct) & \\multirow{2}{*}{Binoculars Score} & Predicted as Human-Written \\\\ \\hline US Constitution & 0.6680 & 0.8789 & 0.7600 & ✘ \\\\ “I have a dream speech” & 1.0000 & 1.2344 & 0.8101 & ✘ \\\\ Snippet from Cosmos series & 2.3906 & 2.8281 & 0.8453 & ✘ \\\\ Blowin’ In the Wind (song) & 1.1172 & 1.2188 & 0.9167 & ✓ \\\\ Oscar Wilde’s quote & 2.9219 & 3.0781 & 0.9492 & ✓ \\\\ Snippet from White Night & 2.6875 & 2.8125 & 0.9556 & ✓ \\\\ Wish You Were Here & 2.5000 & 2.5938 & 0.9639 & ✓ \\\\ Snippet from Harry Potter book & 2.5938 & 2.6875 & 0.9651 & ✓ \\\\ First chapter of A Tale of Two Cities & 2.7188 & 2.7500 & 0.9886 & ✓ \\\\ Snippet from Crime and Punishment & 2.8750 & 2.9063 & 0.9892 & ✓ \\\\ To Fall In Love With You (song) & 3.2969 & 3.2656 & 1.0096 & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 4>는 LLM으로 외울 가능성이 있는 교과서 표본 사례학이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Prompting Strategy & Instruction appended to the default system prompt \\\\ \\hline Carl Sagan & Write in the voice of Carl Sagan. \\\\  & Write your response in a way that doesn’t sound pretentious or overly formal. \\\\ Non-Robotic & Don’t use robotic-sounding words like ‘logical’ and ‘execute.’ Write in the casual style of a normal person. \\\\ Pirate & Write in the voice of a pirate. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 시스템에 적용된 지침은 3가지 다른 전략을 촉발했다.\n' +
      '\n' +
      '그림 15: **AUC 큐브 쌍안경은 Falcon-7B 및 Falcon-7B-강제를 사용하여 동일한 \\(\\mathcal{M}_{1}\\) 및 \\(\\mathcal{M}_{2}\\) 모델을 사용한 동일한 \\(\\mathcal{M}_{2}\\) 모델을 사용한 점수와 Falcon-7B 및 Falcon-7B-강제를 사용한 **AUC 큐브 쌍안경 점수를 사용한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>