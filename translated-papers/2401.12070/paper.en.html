<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text\n' +
      '\n' +
      'Abhimanyu Hans\n' +
      '\n' +
      'University of Maryland\n' +
      '\n' +
      '&Avi Schwarzschild\n' +
      '\n' +
      'Carnegie Mellon University\n' +
      '\n' +
      '&Valerilia Cherepanova\n' +
      '\n' +
      'University of Maryland\n' +
      '\n' +
      '&Hamid Kazemi\n' +
      '\n' +
      'University of Maryland\n' +
      '\n' +
      '&Aniruddha Saha\n' +
      '\n' +
      'University of Maryland\n' +
      '\n' +
      '&Micah Goldblum\n' +
      '\n' +
      'New York University\n' +
      '\n' +
      '&Jonas Geiping\n' +
      '\n' +
      'ELIS Institute & MPI for Intelligent Systems,\n' +
      '\n' +
      'Tubingen AI Center\n' +
      '\n' +
      'Equal Contribution. Correspondence to ahans1@umd.edu.\n' +
      '\n' +
      'Code available at [https://github.com/ahans30/Binoculars](https://github.com/ahans30/Binoculars).\n' +
      '\n' +
      'Tom Goldstein\n' +
      '\n' +
      'University of Maryland\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called _Binoculars_, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate _Binoculars_ on a number of text sources and in varied situations. Over a wide range of document types, _Binoculars_ detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.\n' +
      '\n' +
      'Figure 1: **Detection of Machine-Generated Text from ChatGPT. Our detection approach using _Binoculars_ is highly accurate at separating machine-generated and human-written samples from _News_, _Creative Writing_ and _Student Essay_ datasets with a false positive rate of \\(0.01\\%\\). _Binoculars_, based on open-source Falcon models with no finetuning, outperforms commercial detection systems, such as GPTZero, as well as strong open-source detectors – even though both of these baselines are specifically tuned to detect ChatGPT (Verma et al., 2023; Tian, 2023a). Our approach operates entirely in a zero-shot setting and has not been tuned nor trained to detect ChatGPT in particular.**\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'We present a method for detecting LLM-generated text that works in the zero-shot setting in which no training examples are used from the LLM source. Even with this strict limitation, our scheme still out-performs all open-source methods for ChatGPT detection and is competitive with or better than commercial APIs, despite these competitors using training samples from ChatGPT (Mitchell et al., 2023; Verma et al., 2023). At the same time, because of the zero-shot nature of our detector, the very same detector can spot multiple different LLMs with high accuracy - something that all existing solutions fail to do.\n' +
      '\n' +
      'The ability to detect LLMs in the zero-shot setting addresses issues of growing importance. Prior research on combating academic plagiarism (TurmitIn.com) has fixated strongly on ChatGPT because of its simple and accessible interface. But more sophisticated actors use LLM APIs to operate bots, create fake product reviews, and spread misinformation on social media platforms at a large scale. These actors have a wide range of LLMs available to them beyond just ChatGPT, making zero-shot, model-agnostic detection critical for social media model and platform integrity assurance (Crothers et al., 2022; Bail et al., 2023). Our zero-shot capability is a departure from existing detectors that rely on model-specific training data and often fail to transfer to new models.\n' +
      '\n' +
      'Our proposed detector, called _Binoculars_, works by viewing text through two lenses. First, we compute the \\(\\log\\) perplexity of the text in question using an "observer" LLM. Then, we compute all the next-token predictions that a "performer" LLM would make at each position in the string, and compute their perplexity according to the observer. If the string is written by a machine, we should expect these two perplexities to be similar. If it is written by a human they should be different. We first motivate this simple observation, and then show that it is sufficient to build a strong zero-shot detector, which we extensively stress-test in a number of text domains.\n' +
      '\n' +
      '## 2 The LLM Detection Landscape\n' +
      '\n' +
      'A common first step in harm reduction for generative AI is detection. Specifically, applications that range from documenting and tracing text origins (Biderman and Raff, 2022) to investigating spam and fake news campaigns (Zellers et al., 2019) and to analyzing training data corpora, all benefit from signals that quantify whether text is human- or machine-generated (Bender et al., 2021; Crothers et al., 2022; Mirsky et al., 2023).\n' +
      '\n' +
      'Successful efforts to spot machine-generated text show promise on early models whose generation output is not convincingly human. However, with the rise of transformer models for language modeling (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023), primitive mechanisms to detect machine-generated text are rendered useless. While one approach is to record (Krishna et al., 2023) or watermark all generated text (Kirchenbauer et al., 2023), these _preemptive detection_ approaches can only be implemented with full control over the generative model.\n' +
      '\n' +
      'Instead, the recent spread of machine-generated text, especially via ChatGPT, has lead to a flurry of work on _post-hoc detection_ approaches that can be used to detect machine text without cooperation from model owners. These detectors can be separated into two main groups. The first is trained detection models, where a pretrained language model backbone is finetuned for the binary classification task of detection (Solaiman et al., 2019; Zellers et al., 2019; Yu et al., 2023; Zhan et al., 2023), including techniques like adversarial training (Hu et al., 2023) and abstention (Tian et al., 2023). Alternatively, instead of finetuning the whole backbone, a linear classifier can be fit on top of frozen learned features, which allows for the inclusion of commercial API outputs (Verma et al., 2023).\n' +
      '\n' +
      'The second category of approaches comprises statistical signatures that are characteristic of machine-generated text. These approaches have the advantage of requiring none or little training data and they can easily be adapted to newer model families (Pu et al., 2022). Examples include detectors based on perplexity (Tian, 2023; Vasilatos et al., 2023; Wang et al., 2023), perplexity curvature (Mitchell et al., 2023), log rank (Su et al., 2023), intrinsic dimensionality of generated text (Tulchinskii et al., 2023), and no-gram analysis (Yang et al., 2023). Our coverage of the landscape is non-exhaustive, and we refer to recent surveys Ghosal et al. (2023); Tang et al. (2023); Dhanini et al. (2023); Guo et al. (2023) for additional details.\n' +
      '\n' +
      'From a theoretical perspective, Varshney et al. (2020), Helm et al. (2023), and Sadasivan et al. (2023) all discuss the limits of detection. These works generally agree that fully general-purpose models of language would be, by definition, impossible to detect. However, Chakraborty et al. (2023) note that even models that are arbitrarily close to this optimum are technically detectable given a sufficient number of samples. In practice, the relative success of detection approaches, such as the one we propose and analyze in this work, provides constructive evidence that current language models are imperfect representations of human writing - and thereby detectable. Finally, the robustness of detectors to attacks attempting to circumvent detection can provide stronger practical limits on reliability in the worst case (Bhat and Parthasarathy, 2020; Wolff and Wolff, 2022; Liyanage and Buscaldi, 2023).\n' +
      '\n' +
      'With an understanding of how much work exists on LLM detection, a crucial question arises: How do we appropriately and thoroughly evaluate detectors? Many works focus on accuracy on balanced test sets and/or AUC of their proposed classifiers, but these metrics are not well-suited for the high-stakes question of detection. Ultimately, only detectors with low false positive rates across a wide distribution of human-written text, truly reduce harm. Further, Liang et al. (2023) note that detectors are often only evaluated on relatively easy datasets that are reflective of their training data. Their performance on out-of-domain samples is often abysmal. For example, TOEFL essays written by non-native English speakers were wrongly marked as machine-generated 48-76% of the time by commercial detectors (Liang et al., 2023).\n' +
      '\n' +
      'In Section 3, we motivate our approach and discuss why detecting language model text, especially in the ChatGPT world, is difficult. In this work, our emphasis is directed toward baselines that function within post-hoc, out-of-domain (zero-shot), and black-box detection scenarios. We use the state-of-the-art open source detector Ghostbuster (Verma et al., 2023), the commercially deployed GPTZero,1 and DetectGPT (Mitchell et al., 2023) to compare detection performance across various datasets in Section 4. In Section 5, we evaluate the reliability of _Binoculars_ in various settings that constitute edge cases and crucial deployment behaviors that a detector based on _Binoculars_ has to take into account.\n' +
      '\n' +
      'Footnote 1: [https://gptzero.me/](https://gptzero.me/)\n' +
      '\n' +
      '## 3 _Binoculars_: How it works\n' +
      '\n' +
      'Our approach, _Binoculars_, is so named as we look at inputs through the lenses of two different language models. It is well known that perplexity - a common baseline for machine/human classification - is insufficient on its own, leading prior work to disfavor approaches based on statistical signatures. However we propose using a ratio of two scores, where one is a perplexity measurement and the other is _cross-perplexity_, a notion of how surprising the next token predictions of one model are to another model. This two-model mechanism is the basis for our general and accurate detector, and we show that this mechanism is able to detect a number of large language models, even when they are unrelated to the two models used by _Binoculars_.\n' +
      '\n' +
      '### Background & Notation\n' +
      '\n' +
      'A string of characters \\(s\\) can be parsed into tokens and represented as a list of token indices \\(\\vec{x}\\) by a tokenizer \\(T\\). Let \\(x_{i}\\) denote the token ID of the \\(i\\)-th token, which refers to an entry in the LLMs vocabulary \\(V=\\{1,2...,n\\}\\). Given a token sequence as input, a language model \\(\\mathcal{M}\\) predicts the next token by outputting a probability distribution over the vocabulary:\n' +
      '\n' +
      '\\[\\mathcal{M}(T(s))=\\mathcal{M}(\\vec{x})=Y \\tag{1}\\] \\[Y_{ij}=P(v_{j}|x_{0:i-1})\\text{ for all }j\\in V.\\]\n' +
      '\n' +
      'We will abuse notation and abbreviate \\(\\mathcal{M}(T(s))\\) as \\(\\mathcal{M}(s)\\) where the tokenizer is implicitly the one used in training \\(\\mathcal{M}\\). For our purposes, we define \\(\\log\\operatorname{PPL}\\), the log-perplexity, as the average negative log-likelihood of all tokens in the given sequence. Formally, let\n' +
      '\n' +
      '\\[\\log\\operatorname{PPL}_{\\mathcal{M}}(s)=-\\frac{1}{L}\\sum_{i=1}^{L} \\log(Y_{ix_{i}}), \\tag{2}\\] \\[\\text{ where }\\vec{x}=T(s),Y=\\mathcal{M}(\\vec{x})\\text{ and }L= \\text{ number of tokens in }s.\\]Intuitively, log-perplexity measures how "surprising" a string is to a language model. As mentioned above, perplexity has been used to detect LLMs, as humans produce more surprising text than LLMs. This is reasonable, as \\(\\log\\mathrm{PPL}\\) is also the loss function used to train generative LLMs, and models are likely to score their own outputs as unsurprising.\n' +
      '\n' +
      'Our method also measures how surprising the output of one model is to another. We define the _cross-perplexity_, which takes two models and a string as its arguments. Let \\(\\log\\mathrm{X}\\mathrm{-PPL}_{\\mathcal{M}_{1},\\mathcal{M}_{2}}(s)\\) measure the average per-token cross-entropy between the outputs of two models, \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\), when operating on the tokenization of \\(s\\).2\n' +
      '\n' +
      'Footnote 2: This requires that \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\) share a tokenizer.\n' +
      '\n' +
      '\\[\\log\\mathrm{X}\\mathrm{-PPL}_{\\mathcal{M}_{1},\\mathcal{M}_{2}}(s)=-\\frac{1}{L} \\sum_{i=1}^{L}\\mathcal{M}_{1}(s)_{i}\\cdot\\log\\left(\\mathcal{M}_{2}(s)_{i} \\right). \\tag{3}\\]\n' +
      '\n' +
      'Note that \\(\\cdot\\) denotes the dot product between two vector-valued quantities.\n' +
      '\n' +
      '### What makes detection hard? A primer on the capybara problem.\n' +
      '\n' +
      'Why do we require measurements of both perplexity and cross-perplexity? Unsurprisingly, LLMs tend to generate text that is unsurprising to an LLM. Meanwhile, because humans differ from machines, human text has higher perplexity according to an LLM observer. For this reason, it is tempting to use raw perplexity for LLM detection, as high perplexity is a strong sign of a human author.\n' +
      '\n' +
      'Unfortunately, this intuition breaks when hand-crafted prompts are involved. Prompts have a strong influence over downstream text, and prompts are typically unknown to the detector. On the one hand, the prompt "1, 2, 3," might result in the very low perplexity completion "4, 5, 6." On the other hand, the prompt "Can you write a few sentences about a capybara that is an astrophysicist?" will yield a response that seems more strange. In the presence of the prompt, the response may be unsurprising (low perplexity). But in the absence of the prompt, a response containing the curious words "capybara" and "astrophysicist" in the same sentence will have high perplexity, resulting in the false determination that the text was written by a human, see the example in Figure 2. Clearly, certain contexts will result in high perplexity and others low perplexity, regardless of whether the author is human or machine. We refer to this dilemma as "the capybara problem" - in the absence of the prompt, LLM detection seems difficult and naive perplexity-based detection fails.\n' +
      '\n' +
      '### Our Detection Score\n' +
      '\n' +
      '_Binoculars_ solves the capybara problem by providing a mechanism for estimating the baseline perplexity induced by the prompt. By comparing the perplexity of the observed text to this expected baseline, we get fiercely improved LLM detection.\n' +
      '\n' +
      'Figure 2: This quote is LLM output from ChatGPT (GPT-4) when prompted with “Can you write a few sentences about a capybara that is an astrophysicist?” The Falcon LLM assigns this sample a high perplexity (2.20), well above the mean for both human and machine data. Despite this problem, our detector correctly assigns a _Binoculars_ score of 0.73, which is well below the global threshold of 0.901, resulting in a correct classification with high confidence. For reference, DetectGPT wrongly assigns a score of 0.14, which is below it’s optimal threshold of 0.17, and classifies the text as human. GPTZero assigns a 49.71% score that this text is generated by AI.\n' +
      '\n' +
      '**Motivation.** Language models are known for producing low-perplexity text relative to humans and thus a perplexity threshold classifier makes for an obvious detecting scheme. However, in the LLM era, the generated text may exhibit a high perplexity score depending on the prompt specified (see the "Capbyara Problem" in Table 2). To calibrate for prompts that yield high-perplexity generation, we use _cross-perplexity_ introduced Equation (3) as a normalizing factor that intuitively encodes the perplexity level of next-token predictions from two models.\n' +
      '\n' +
      'Rather than examining raw perplexity scores, we instead propose measuring whether the tokens that appear in a string are surprising _relative to the baseline perplexity of an LLM acting on the same string_. A string might have properties that result in high perplexity when completed by any agent, machine or human. Yet, we expect the next-token choices of humans to be even higher perplexity than those of a machine. By normalizing the observed perplexity by the expected perplexity of a machine acting on the same text, we can arrive at a detection metric that is fairly invariant to the prompt; see Table 2.\n' +
      '\n' +
      'We propose the _Binoculars_ score \\(B\\) as a sort of normalization or reorientation of perplexity. In particular we look at the ratio of perplexity to cross-perplexity.\n' +
      '\n' +
      '\\[B_{\\mathcal{M}_{1},\\mathcal{M}_{2}}(s)=\\frac{\\log\\mathrm{PPL}_{\\mathcal{M}_{ 1}}(s)}{\\log\\text{X-PPL}_{\\mathcal{M}_{1},\\mathcal{M}_{2}}(s)} \\tag{4}\\]\n' +
      '\n' +
      'Here, the numerator is simply the perplexity, which measures how surprising a string is to \\(\\mathcal{M}_{1}\\). The denominator measures how surprising the token predictions of \\(\\mathcal{M}_{2}\\) are when observed by \\(\\mathcal{M}_{1}\\). Intuitively, we expect a human to diverge from \\(\\mathcal{M}_{1}\\) more than \\(\\mathcal{M}_{2}\\) diverges from \\(\\mathcal{M}_{1}\\), provided the LLMs \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\) are more similar to each other than they are to a human.\n' +
      '\n' +
      'The _Binoculars_ score is a general mechanism that captures a statistical signature of machine text. In the sections below, we show that for most obvious choices of \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\), _Binoculars_ does separate machine and human text much better than perplexity alone. Importantly, it is capable of detecting generic machine-text generated by a third model altogether.\n' +
      '\n' +
      'Interestingly, we can draw some connection to other approaches that contrast two strong language models, such as contrastive decoding (Li et al., 2023), which aims to generate high-quality text completions by generating text that roughly maximizes the difference between a weak and a strong model. Speculative decoding is similar (Chen et al., 2023; Leviathan et al., 2023), it uses a weaker model to plan completions. Both approaches function best when pairing a strong model with a very weak secondary model. However, as we show below, our approach works best for two models that are very close to each other in performance. In the remainder of this work, we use the open-source models Falcon-7b model (\\(\\mathcal{M}_{1}\\)) and the Falcon-7b-instruct (\\(\\mathcal{M}_{2}\\)) (Almazrouei et al., 2023). The full set of combinations of scoring models used can be found in Table 2 in the appendix.\n' +
      '\n' +
      '## 4 Accurate Zero-Shot Detection\n' +
      '\n' +
      'In this section we evaluate our proposed score, and build a zero-shot LLM detector with it. With _Binoculars_, we are able to spot machine-generated text in a number of domains. In our experimental evaluation, we focus on the problem setting of detecting machine-generated text from a modern LLM, as generated in common use cases without consideration for the detection mechanism.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'We start our experiments with several datasets described in the LLM detection literature. The most recent baseline to which we compare is Ghostbuster. Verma et al. (2023), who propose this method, also introduce three datasets that we include in our study: _Writing Prompts_, _News_, and _Student Essay_. These are balanced datasets with equal numbers of human samples and machine samples. The machine samples are written by ChatGPT.3\n' +
      '\n' +
      'Footnote 3: In the following we will always use ChatGPT as short-hand for the chat versions of GPT-3.5-(turbo), not for the chat versions of GPT-4.\n' +
      '\n' +
      'We also generate several datasets of our own to evaluate our capability in detecting other language models aside from ChatGPT. Drawing samples of human-written text from CCNews (Hamborget al., 2017), PubMed (Sen et al., 2008), and CNN (Hermann et al., 2015), we generate alternative, machine-generated completions using LLaMA-2-7B and Falcon-7B. To do so, we peel off the first 50 tokens of each human sample and use it as a prompt to generate up to 512 tokens of machine output. We then remove the human prompt from the generation and only use the purely machine-generated text in our machine-text datasets. Further, we use the Orca dataset (Lian et al., 2023), which provides several million instruction prompts with their machine-generated completions from chat versions of GPT-3 and GPT-4. This dataset allows us to check the reliability of the proposed method when detecting instruction-tuned models, and allows us to quantify detection differences between GPT-3 and GPT-4.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'Since detectors are binary classifiers, the standard suite of binary classification metrics are relevant. In particular, it is often considered comprehensive to look at ROC curves and use the area under the curve (AUC) as a performance metric. In fact, Verma et al. (2023) and Mitchell et al. (2023) only report performance as measured by AUC and F1 scores. We argue that these metrics alone are inadequate when measuring LLM detection accuracy.\n' +
      '\n' +
      'In high-stakes detection settings, the most concerning harms often arise from _false positives_, i.e., instances when human text is wrongly labeled as machine-generated. For this reason, we focus on true-positive rates (TPR) at low false-positive rates (FPR), and adopt a standard FPR threshold of \\(0.01\\%\\).4 We will present F1 scores and AUC values only for comparison to prior publications, but we prefer to focus on TPR values at low FPR as a key metric. The reader may observe that AUC scores are often uncorrelated with TRP@FPR when the FPR is below \\(1\\%\\).\n' +
      '\n' +
      'Footnote 4: The smallest threshold we can comprehensively evaluate to sufficient statistical significance with our compute resources.\n' +
      '\n' +
      '### Benchmark Performance\n' +
      '\n' +
      'Using a handful of datasets, we compare the AUC and TPR of _Binoculars_ to Ghostbuster (V Verma et al., 2023), GPTZero (Tian, 2023a), and DetectGPT (using LLaMA-2-13B to score curvature) (Mitchell et al., 2023). We highlight that these comparisons on machine samples from ChatGPT are _in favor_ of GPTZero and Ghostbuster, as these detectors have been tuned to detect ChatGPT output, and comparisons using samples from LLaMA models are _in favor_ of DetectGPT for the same reason.\n' +
      '\n' +
      'Our score-based detector has just one single tunable parameter; a threshold to separate machine and human text, which we preset using reference data. We set the threshold using the combination of training splits from all of our reference datasets: News, Creative Writing, and Student Essay datasets from Verma et al. (2023), which are generated using ChatGPT. We also compare detectors on LLaMA-2-13B and Falcon-7B generated text with prompts from CC News, CNN, and PubMed datasets. All of these datasets have an equal number of human and machine-generated text samples. We optimize and fix our threshold globally using these datasets. As one exception, to be sure that we meet the Ghostbuster definition of "out-of-domain," when comparing our performance with Ghostbuster we do not include the ChatGPT datasets (News, Creative Writing, and Student Essay) in the threshold determination, and only use samples from CC News, CNN, and PubMed (generated via LLaMA and Falcon) to choose our threshold.\n' +
      '\n' +
      '**Ghostbuster Datasets.** The Ghostbuster detector is a recent detector tuned to detect output from ChatGPT. Using the same three datasets introduced and examined in the original work by Verma et al. (2023), we compare TPR at 0.01% FPR in Figure 1 (and F1-Score in Figure 11 in Appendix) to show that _Binoculars_ outperforms Ghostbuster in the "out-of-domain" setting. This setting is the most realistic, and includes evaluation on datasets other than Ghostbuster\'s training data. A desirable property for detectors is that with more information they get stronger. Figure 3 shows that both _Binoculars_ and Ghostbuster have this property, and that the advantages of _Binoculars_ are even clearer in the few-token regime.\n' +
      '\n' +
      '**Open-Source Language Models.** We show that our detector is capable of detecting the output of several LLMs, such as LLaMA as shown in Figure 4 and Falcon as shown in Figure 14 in the appendix. Here we also observe that Ghostbuster is indeed only capable of detecting ChatGPT,and it fails to reliably detect LLaMA generated text. The detailed ROC plots in Figure 4 compare performance across thresholds for all methods.\n' +
      '\n' +
      '## 5 Reliability in the Wild\n' +
      '\n' +
      'How well does _Binoculars_ work when faced with scenarios encountered in the wild? The key takeaway that we want to highlight throughout this section is that the score underlying _Binoculars_, i.e. Equation (4) is a _machine-text detector_. Intuitively, this means that is predicts how likely it is that the given piece of text could have been generated by a similar language model. This has a number of crucial implications regarding memorized samples, text from non-native speakers, modified prompting strategies, and edge cases, all of which we comprehensively evaluate in this section.\n' +
      '\n' +
      '### Varied Text Sources\n' +
      '\n' +
      'We start our in-the-wild investigation by exploring detector performance in additional settings outside of natural English language. To this end we investigate the Multi-generator, Multi-domain, and Multi-lingual (M4) detection datasets (Wang et al., 2023). These samples come from Arxiv, Reddit, Wikihow, and Wikipedia sources, and include examples in varied languages, such as Urdu, Russian, Bulgarian, and Arabic. Machine text samples in this dataset are generated via ChatGPT. In Figure 5,\n' +
      '\n' +
      'Figure 4: **Detecting LLaMA-2-13B generations.**_Binoculars_ achieves higher TPRs for low FPRs than competing baselines.\n' +
      '\n' +
      'Figure 3: **Impact of Document Size on Detection Performance. The plot displays the TPR at 0.01% FPR across varying document sizes. The x-axis represents the number of tokens of the observed document, while the y-axis indicates the corresponding detection performance, highlighting the _Binoculars_ ability to detect with a low number of tokens.**\n' +
      '\n' +
      'we show the precision and recall of _Binoculars_ and four other baselines, showing that our method generalizes across domains and languages. These baselines, released with the M4 Datasets, include RoBERTa (Liu et al., 2019) fine-tuned to detect LLM text (Zellers et al., 2019; Solaiman et al., 2019), Logistic Regression over Giant Language Model Test Room (LR GLTR) (Gehrmann et al., 2019) which generates features assuming predictions are sampled from a specific token distribution, stylistic (Li et al., 2014) which employs syntactic features at character, word, and sentence level, News Landscape classifiers (NELA) (Horne et al., 2019) which generates and leverages semantic and structural features for veracity classification. We reprint this result from the benchmark for reference. Results with more source models appear in Figure 6.\n' +
      '\n' +
      '### Other languages\n' +
      '\n' +
      'When evaluating _Binoculars_ on samples from languages that are not well represented in Common Crawl data (standard LLM pretraining data), we find that false-positives rates remain low, which is highly desirable from a harm reduction perspective. However, machine text in these low-resource languages is often classified as human. Figure 7 shows that we indeed have reasonable precision but poor recall in these settings. While this ordering of scores is a win for harmlessness, why is multilingual text detection limited?\n' +
      '\n' +
      'As we outline above, _Binoculars_ is a machine-text detector, detecting whether text may have been generated from a similar language model. As the Falcon models we use in our experiments are highly limited in their capability of generating text in these low-resource languages, the ChatGPT-generated text is unlikely to be machine-generated, according to our score. We hypothesize that a stronger multilingual pair of models would lead to a version of _Binoculars_ that could spot ChatGPT-generated text in these languages more effectively.\n' +
      '\n' +
      '**False-Positive Rates on text written by non-native speakers.** A significant concern about LLM detection algorithms, as raised by Liang et al. (2023), is that LLM detectors are inadvertently biased against non-native English speakers (ESL) classifying their writing as machine-generated exceed\n' +
      '\n' +
      'Figure 5: **Detection of ChatGPT-generated text in various domains from M4 Dataset.** Binoculars maintain high precision over 4 domains using the global threshold (tuned out-of-domain) for detection. We use the mean of out-of-domain performance metrics reported by Wang et al. (2023)\n' +
      '\n' +
      'Figure 6: Performance of _Binoculars_ on samples from various generative models.\n' +
      '\n' +
      'Figure 7: _Binoculars_ operates at high precision in Bulgarian and Urdu, but with low recall in all four languages.\n' +
      '\n' +
      'ingly often. To test this, we analyze essays from _EssayForum_, a web page for ESL students to improve their academic writing (EssayForum, 2022). This dataset contains both the original essays, as well as grammar-corrected versions. We compare the distribution of _Binoculars_ scores across the original and the grammar-corrected samples. Interestingly, and in stark comparison to commercial detectors examined by Liang et al. (2023) on a similar dataset, _Binoculars_ attains equal accuracy at 99.67% for both corrected and uncorrected essay datasets (see Figure 8). We also point out that the _Binoculars_ score distribution on non-native English speaker\'s text highly overlaps with that of grammar-corrected versions of the same essays, showing that detection through _Binoculars_ is insensitive to this type of shift.\n' +
      '\n' +
      '### Memorization\n' +
      '\n' +
      'One common feature (or bug?) of perplexity-based detection is that highly memorized examples are classified as machine-generated. For example, famous quotes that appear many times in the training data likely have low perplexity according to an observer model that has overfit to these strings. By looking at several examples, we examine how _Binoculars_ performs on this type of data.\n' +
      '\n' +
      'See Table 4 in Appendix A.3 for all famous text evaluated in this study. First, we ask about the US Constitution - a document that is largely memorized by modern LLMs. This example has a _Binoculars_ score of 0.76, well into the machine range. Of the 11 famous texts we study, this was the lowest score (most _machine-y_). Three of the 11 fall on the machine-side of our threshold.\n' +
      '\n' +
      'It is important to note that while this behavior may be surprising, and does require careful consideration in deployment, it is fully consistent with a machine-text detector. Memorized text is both text written by human writers, and text that is likely to be generated by an LLM. Classification of memorized text as machine generated may be acceptable or even desirable in some applications (e.g., plagiarism detection), or undesirable in others (e.g., removal of LLM-generated text from a training corpus).\n' +
      '\n' +
      '### Modified Prompting Strategies\n' +
      '\n' +
      'The Open Orca dataset contains machine generations from both GPT-3 and GPT-4 for a wide range of tasks (Lian et al., 2023). This serves as a diverse test bed for measuring _Binoculars_ on both of these modern high-performing LLMs. Impressively, _Binoculars_ detects 92% of GPT-3 samples and 89.57% of GPT-4 samples when using the global threshold (from reference datasets). Note, we only report accuracy since this is over a set of machine-generated text only. This dataset also provides prompts that allow us to explore how sensitive _Binoculars_ is to modifying prompts.\n' +
      '\n' +
      'Simple detection schemes are sometimes fooled by simple changes to prompting strategies, which can produce stylized text that deviates from the standard output distribution. With this in mind, we use LLaMA-2-13B-chat and prompts designed to tweak the style of the output. Specifically, we prompt LLaMA2-13B-chat with three different system prompts by appending to the standard system prompt a request to write in Carl Sagan\'s voice or without any mechanical or robotic sounding words or like a pirate.\n' +
      '\n' +
      'In general, we find that these stylistic changes do not significantly impact the accuracy of _Binoculars_. The biggest impact we observe arises when asking for pirate-sounding output, and this only decreases the sensitivity (increases the false negative rate) by \\(1\\%\\); see Figure 9. Table 1 records generations based on a sample prompt employing the specified prompting strategies.\n' +
      '\n' +
      '### Randomized Data\n' +
      '\n' +
      'Next, we also want to test whether arbitrary mistakes, hashcodes, or other kinds of random (and random seeming) strings bias our model towards false positives. To test the impact of randomness, we generate random sequences of tokens from the Falcon tokenizer, and score them with _Binoculars_ as usual. We plot a histogram of this distribution in Figure 10. We find that _Binoculars_ confidently scores this as human, with a mean score around \\(1.35\\) for Falcon (humans have a mean of around \\(1\\)). This is expected, as trained LLMs are strong models of language and exceedingly unlikely to ever generate these completely random sequences of tokens in any situation. In particular, the generation of these random sequences is even less likely than the generation of perfect human-written text by chance.\n' +
      '\n' +
      '## 6 Discussion and Limitations\n' +
      '\n' +
      'We present _Binoculars_, a method for detecting LLMs in the zero-shot case in which no data is available from the model being detected. Our transferable detector works in the zero-shot setting, without access to the particular model used for generation or example data from it. We speculate\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt} p{142.3pt}} \\hline \\hline \\multirow{2}{*}{Prompt} & Write an article based on this “Use an adjustable jump. Create your jump withinder blocks and plywood. Use natural outdoor obstacles as jumps. Set down a text on the far side of the jump. Practice at the lowest level until your dog is comfortable. Raise the bar gradually. Use the same method to teach your dog to jump through hoops.” \\\\ \\hline \\hline \\multirow{4}{*}{Default} & A well-trained dog is not only a default to be around, but it also provides a sense of pride and accomplishment to the owner. One capturing and useful skill that you may want to teach your dog is jumping. Whether you’re doing it for fun, agility competitions, or simply to keep your framy companion physically engaged, teaching your dog how to jump can be a bonding and rearving experience. In this article, we will discuss various steps and techniques that will guide you through the process of training your dog to confidently jump over obstacles and through hoops. \\\\ \\hline \\multirow{2}{*}{Carl Sagan} & Ab, my dear friend, we stand at the precipe of a most worntowns adventure, an odyssey of the canine kind. Our quest, to teach our loyal conclusions, those tiny wonders out, to some through the air with grave and agility. The tool at our disposal, the adjustable jump. This murred of jump stimulus allows to create a series of obstacles, each one more challenging than the last, and to guide our canine companions through them with precision and patience. \\\\ \\hline \\multirow{4}{*}{Non-Robotic} & Hey there, follow dog lovers? Are you looking for a fun and challenging way to keep your furry friend active and engaged? Look no further than an adjustable jump! Not only is it a great way to provide your dog with physical service, but it’s also a great way to bond with your pup. Plus, it’s super easy to set up and can be customized to fit your dog skill level. \\\\ \\hline \\multirow{4}{*}{Pirate} & Arrr, me heartiest: Ye landlubtese be wantini’ yer pouch ta learn i jump, eh? Well, fear not! It be herent to help ye teach sex per ferry first male it a jump over obstacles like a proper salty sex dog! First thing first, ye need to create the jump. Now, I know what ye be thinking! “Cinider blocks and plywood?” That be a bit, raused, don’t ye think?” But never fear, me heartiest: These materials be starved and easy to work with. Ye can build a jump that be adjustable, so ye can raise the bar it as sint yet pock\'s skill level. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Excerpts from LLaMA-2-13B-chat generations using an Open-Orca sample prompt with varying system prompts. For the full modifications to the prompts to generate these stylized outputs, see Appendix A.5.\n' +
      '\n' +
      'Figure 10: Random token sequences fall strictly on the human side of the Binoculars threshold.\n' +
      '\n' +
      'that this transferability arises from the similarity between modern LLMs, as they all use nearly identical transformer components and are likely trained on datasets comprising mostly Common Crawl (commoncrawl.org) data from similar time periods. As the number of open source LLMs rapidly increases, the ability to detect multiple LLMs with a single detector is a major advantage of _Binoculars_ when used in practice, for example for platform moderation.\n' +
      '\n' +
      'Our study has a number of limitations. Due to limited GPU memory, we do not perform broader studies with larger (30B+) open-source models in our detector. Further, we focus on the problem setting of detecting machine-generated text in normal use, and we do not consider explicit efforts to bypass detection. Finally, there are other non-conversational text domains, such as source code, which we have not investigated in this study.\n' +
      '\n' +
      '## Reproducibility Statement\n' +
      '\n' +
      'We provide details on all datasets used, and on the exact method that we employ in the main body of this work. Additionally, we provide code to exactly replicate our detection score implementation with the supplementary material of this work. We note that comparisons to commercial detection APIs, such as GPTZero are based on API evaluations from September 2023, and may not be reproducible in the future, underscoring the importance of transparent, open-source solutions.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'Language model detection may be a key technology to reduce harm, whether to monitor machine-generated text on internet platforms and social media, filter training data, or identify responses in chat applications. Nevertheless, care has to be taken so that detection mechanisms actually reduce harm, instead of proliferating or increasing it. We provide an extensive reliability investigation of the proposed _Binoculars_ mechanisms in Section 5, and believe that this is a significant step forward in terms of reliability, for example when considering domains such as text written by non-native speakers. Yet, we note that this analysis is only a first step in the process of deploying LLM detection strategies and does not absolve developers of such applications from carefully verifying the impact on their systems. We especially caution that the existence of LLM detectors does not imply that using them is worthwhile in all scenarios.\n' +
      '\n' +
      'Also, we explicitly highlight that we consider the task of detecting "naturally" occurring machine-generated text, as generated by LLMs in common use cases. We understand that no detector is perfect and we do not guarantee any performance in settings where a motivated adversary tries to fool our system. We present a thorough evaluation across a wide variety of test sources, but we maintain that directed attempts to bypass our classifier might be possible, as is often the case for systems that rely on neural networks.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This work was made possible by the ONR MURI program and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023. URL [https://falconllm.tii.ae/](https://falconllm.tii.ae/).\n' +
      '* Bail et al. (2023) Christopher Bail, Lisa Pinheiro, and Jimmy Royer. Difficulty Of Detecting AI Content Poses Legal Challenges. _Law360_, April 2023.\n' +
      '* Bail et al. (2021)Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, FAccT \'21, pp. 610-623, New York, NY, USA, March 2021. Association for Computing Machinery. ISBN 978-1-4503-8309-7. doi: 10.1145/3442188.3445922. URL [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922).\n' +
      '* Bhatt and Parthasarathy (2020) Meghana Moorthy Bhat and Srinivasan Parthasarathy. How Effectively Can Machines Defend Against Machine-Generated Fake News? An Empirical Study. In _Proceedings of the First Workshop on Insights from Negative Results in NLP_, pp. 48-53, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.insights-1.7. URL [https://aclanthology.org/2020.insights-1.7](https://aclanthology.org/2020.insights-1.7).\n' +
      '* Biderman and Raff (2022) Stella Biderman and Edward Raff. Fooling MOSS Detection with Pretrained Language Models. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, CIKM \'22, pp. 2933-2943, New York, NY, USA, October 2022. Association for Computing Machinery. ISBN 978-1-4503-9236-5. doi: 10.1145/3511808.3557079. URL [https://dl.acm.org/doi/10.1145/3511808.3557079](https://dl.acm.org/doi/10.1145/3511808.3557079).\n' +
      '* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In _34th Conference on Neural Information Processing Systems (NeurIPS 2020)_, December 2020. URL [https://papers.nips.cc/paper/2020/hash/1457c0d6bfcd967418bfb8ac142f64a-Abstract.html](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcd967418bfb8ac142f64a-Abstract.html).\n' +
      '* Chakraborty et al. (2023) Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong Huang. On the Possibilities of AI-Generated Text Detection. _arxiv:2304.04736[cs]_, April 2023. doi: 10.48550/arXiv.2304.04736. URL [http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736).\n' +
      '* Chen et al. (2023) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling. _arxiv:2302.01318[cs]_, February 2023. doi: 10.48550/arXiv.2302.01318. URL [http://arxiv.org/abs/2302.01318](http://arxiv.org/abs/2302.01318).\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunjia Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shirvan Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with Pathways. _arXiv:2204.02311 [cs]_, April 2022. URL [http://arxiv.org/abs/2204.02311](http://arxiv.org/abs/2204.02311).\n' +
      '* Crothers et al. (2022) Evan Crothers, Nathalie Japkowicz, and Herna Viktor. Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods. _arxiv:2210.07321[cs]_, November 2022. doi: 10.48550/arXiv.2210.07321. URL [http://arxiv.org/abs/2210.07321](http://arxiv.org/abs/2210.07321).\n' +
      '* Dhaini et al. (2023) Mahdi Dhaini, Wessel Poelman, and Ege Erdogan. Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. _arxiv:2309.07689[cs]_, September 2023. doi: 10.48550/arXiv.2309.07689. URL [http://arxiv.org/abs/2309.07689](http://arxiv.org/abs/2309.07689).\n' +
      '* Datasets at Hugging Face, September 2022. URL [https://huggingface.co/datasets/nid989/EssayFrom-Dataset](https://huggingface.co/datasets/nid989/EssayFrom-Dataset).\n' +
      'Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visualization of generated text. In Marta R. Costa-jussa and Enrique Alfonseca (eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pp. 111-116, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-3019. URL [https://aclanthology.org/P19-3019](https://aclanthology.org/P19-3019).\n' +
      '* Ghosal et al. (2023) Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, and Amrit Singh Bedi. Towards possibilities & impossibilities of ai-generated text detection: A survey. _arXiv preprint arXiv:2310.15264_, 2023.\n' +
      '* Guo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. _arxiv:2301.07597[cs]_, January 2023. doi: 10.48550/arXiv.2301.07597. URL [http://arxiv.org/abs/2301.07597](http://arxiv.org/abs/2301.07597).\n' +
      '* Hamborg et al. (2017) Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news crawler and extractor. In _Proceedings of the 15th International Symposium of Information Science_, pp. 218-223, March 2017. doi: 10.5281/zenodo.4120316.\n' +
      '* Helm et al. (2023) Hayden Helm, Carey E. Priebe, and Weiwei Yang. A Statistical Turing Test for Generative Models. _arxiv:2309.08913[cs]_, September 2023. doi: 10.48550/arXiv.2309.08913. URL [http://arxiv.org/abs/2309.08913](http://arxiv.org/abs/2309.08913).\n' +
      '* Volume 1_, NIPS\'15, pp. 1693-1701, Cambridge, MA, USA, December 2015. MIT Press.\n' +
      '* Horne et al. (2019) Benjamin D. Horne, Jeppe Norregaard, and Sibel Adali. Robust fake news detection over time and attack. _ACM Trans. Intell. Syst. Technol._, 11(1), dec 2019. ISSN 2157-6904. doi: 10.1145/3363818. URL [https://doi.org/10.1145/3363818](https://doi.org/10.1145/3363818).\n' +
      '* Hu et al. (2023) Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. RADAR: Robust AI-Text Detection via Adversarial Learning. _arxiv:2307.03838[cs]_, July 2023. doi: 10.48550/arXiv.2307.03838. URL [http://arxiv.org/abs/2307.03838](http://arxiv.org/abs/2307.03838).\n' +
      '* Kirchenbauer et al. (2023) John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A Watermark for Large Language Models. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 17061-17084. PMLR, July 2023. URL [https://proceedings.mlr.press/v202/kirchenbauer23a.html](https://proceedings.mlr.press/v202/kirchenbauer23a.html).\n' +
      '* Krishna et al. (2023) Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. _arxiv:2303.13408[cs]_, March 2023. doi: 10.48550/arXiv.2303.13408. URL [http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408).\n' +
      '* Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via Speculative Decoding. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 19274-19286. PMLR, July 2023. URL [https://proceedings.mlr.press/v202/leviathan23a.html](https://proceedings.mlr.press/v202/leviathan23a.html).\n' +
      '* Li et al. (2014) Jenny S. Li, John V. Monaco, Li-Chiou Chen, and Charles C. Tappert. Authorship authentication using short messages from social networking sites. In _2014 IEEE 11th International Conference on e-Business Engineering_, pp. 314-319, 2014. doi: 10.1109/ICEBE.2014.61.\n' +
      '* Li et al. (2023) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive Decoding: Open-ended Text Generation as Optimization. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 12286-12312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL [https://aclanthology.org/2023.acl-long.687](https://aclanthology.org/2023.acl-long.687).\n' +
      '\n' +
      '* Lian et al. (2023) Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". Openorca: An open dataset of gpt augmented flan reasoning traces. https://[https://huggingface.co/Open-Orca/OpenOrca](https://huggingface.co/Open-Orca/OpenOrca), 2023.\n' +
      '* Liang et al. (2023) Weixin Liang, Mert Yuksegonul, Yining Mao, Eric Wu, and James Zou. GPT detectors are biased against non-native English writers. _arxiv:2304.02819[cs]_, April 2023. doi: 10.48550/arXiv.2304.02819. URL [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819).\n' +
      '* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\n' +
      '* Liyanage and Buscaldi (2023) Vijini Liyanage and Davide Buscaldi. Detecting Artificially Generated Academic Text: The Importance of Mimicking Human Utilization of Large Language Models. In Elisabeth Metais, Farid Meziane, Vijayan Sugumaran, Warren Manning, and Stephan Reiff-Marganiec (eds.), _Natural Language Processing and Information Systems_, Lecture Notes in Computer Science, pp. 558-565, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-35320-8. doi: 10.1007/978-3-031-35320-8.42.\n' +
      '* Mirsky et al. (2022) Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei, Liu Yang, Xiangyu Zhang, Maura Pintor, Wenke Lee, Yuval Elovici, and Battista Biggio. The Threat of Offensive AI to Organizations. _Computers & Security_, 124:103006, January 2023. ISSN 0167-4048. doi: 10.1016/j.cose.2022.103006. URL [https://www.sciencedirect.com/science/article/pii/S0167404822003984](https://www.sciencedirect.com/science/article/pii/S0167404822003984).\n' +
      '* Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. In _Proceedings of the 40th International Conference on Machine Learning_, pp. 24950-24962. PMLR, July 2023. URL [https://proceedings.mlr.press/v202/mitchell23a.html](https://proceedings.mlr.press/v202/mitchell23a.html).\n' +
      '* Pu et al. (2022) Jiashu Pu, Ziyi Huang, Yadong Xi, Guandan Chen, Weijie Chen, and Rongsheng Zhang. Unraveling the Mystery of Artifacts in Machine Generated Text. In _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pp. 6889-6898, Marseille, France, June 2022. European Language Resources Association. URL [https://aclanthology.org/2022.lrec-1.744](https://aclanthology.org/2022.lrec-1.744).\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. _OpenAI_, pp. 24, 2019.\n' +
      '* Sadasivan et al. (2023) Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can AI-Generated Text be Reliably Detected? _arxiv:2303.11156[cs]_, March 2023. doi: 10.48550/arXiv.2303.11156. URL [http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156).\n' +
      '* Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective Classification in Network Data. _AI Magazine_, 29(3):93-93, September 2008. ISSN 2371-9621. doi: 10.1609/aimagazine/index.php/aimagazine/article/view/2157.\n' +
      '* Solaiman et al. (2019) Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. Release Strategies and the Social Impacts of Language Models. _arxiv:1908.09203[cs]_, November 2019. doi: 10.48550/arXiv.1908.09203. URL [http://arxiv.org/abs/1908.09203](http://arxiv.org/abs/1908.09203).\n' +
      '* Su et al. (2023) Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text. _arxiv:2306.05540[cs]_, May 2023. doi: 10.48550/arXiv.2306.05540. URL [http://arxiv.org/abs/2306.05540](http://arxiv.org/abs/2306.05540).\n' +
      '* Tang et al. (2023) Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. The Science of Detecting LLM-Generated Texts. _arxiv:2303.07205[cs]_, March 2023. doi: 10.48550/arXiv.2303.07205. URL [http://arxiv.org/abs/2303.07205](http://arxiv.org/abs/2303.07205).\n' +
      '\n' +
      '* Tian (2023a) Edward Tian. Gptzero update v1, January 2023a. URL [https://gptzero.substack.com/p/gptzero-update-v1](https://gptzero.substack.com/p/gptzero-update-v1).\n' +
      '* Tian (2023b) Edward Tian. New year, new features, new model, January 2023b. URL [https://gptzero.substack.com/p/new-year-new-features-new-model](https://gptzero.substack.com/p/new-year-new-features-new-model).\n' +
      '* Tian et al. (2023) Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang, Ruifeng Li, Chao Xu, and Yunhe Wang. Multiscale Positive-Unlabeled Detection of AI-Generated Texts. _arxiv:2305.18149[cs]_, June 2023. doi: 10.48550/arXiv.2305.18149. URL [http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajijwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Ienya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martin, Teodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. _arxiv:2307.09288[cs]_, July 2023. doi: 10.48550/arXiv.2307.09288. URL [http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288).\n' +
      '* Tulchinskii et al. (2023) Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, and Evgeny Burnaev. Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. _arxiv:2306.04723[cs]_, June 2023. doi: 10.48550/arXiv.2306.04723. URL [http://arxiv.org/abs/2306.04723](http://arxiv.org/abs/2306.04723).\n' +
      '* Turin (2020) TurinItn.com. URL [https://www.turnitin.com/](https://www.turnitin.com/).\n' +
      '* Varshney et al. (2020) Lav R. Varshney, Nitish Shirish Keskar, and Richard Socher. Limits of Detecting Text Generated by Large-Scale Language Models. In _2020 Information Theory and Applications Workshop (ITA)_, pp. 1-5, February 2020. doi: 10.1109/ITA50056.2020.9245012.\n' +
      '* Vasilatos et al. (2023) Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and Michail Maniatakos. HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis. _arxiv:2305.18226[cs]_, June 2023. doi: 10.48550/arXiv.2305.18226. URL [http://arxiv.org/abs/2305.18226](http://arxiv.org/abs/2305.18226).\n' +
      '* Verma et al. (2023) Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting Text Ghostwritten by Large Language Models. _arxiv:2305.15047[cs]_, May 2023. doi: 10.48550/arXiv.2305.15047. URL [http://arxiv.org/abs/2305.15047](http://arxiv.org/abs/2305.15047).\n' +
      '* Wang et al. (2023) Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsivgun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Ari, and Preslav Nakov. M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection. _arxiv:2305.14902[cs]_, May 2023. doi: 10.48550/arXiv.2305.14902. URL [http://arxiv.org/abs/2305.14902](http://arxiv.org/abs/2305.14902).\n' +
      '* Wolff and Wolff (2022) Max Wolff and Stuart Wolff. Attacking Neural Text Detectors. _arxiv:2002.11768[cs]_, January 2022. doi: 10.48550/arXiv.2002.11768. URL [http://arxiv.org/abs/2002.11768](http://arxiv.org/abs/2002.11768).\n' +
      '* Yang et al. (2023) Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. DNAGPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. _arxiv:2305.17359[cs]_, May 2023. doi: 10.48550/arXiv.2305.17359. URL [http://arxiv.org/abs/2305.17359](http://arxiv.org/abs/2305.17359).\n' +
      '* Yu et al. (2023) Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and Nenghai Yu. GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance. _arxiv:2305.12519[cs]_, May 2023. doi: 10.48550/arXiv.2305.12519. URL [http://arxiv.org/abs/2305.12519](http://arxiv.org/abs/2305.12519).\n' +
      '\n' +
      'Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending Against Neural Fake News. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html).\n' +
      '* Zhan et al. (2023) Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. G3Detector: General GPT-Generated Text Detector. _arxiv:2305.12680[cs]_, May 2023. doi: 10.48550/arXiv.2305.12680. URL [http://arxiv.org/abs/2305.12680](http://arxiv.org/abs/2305.12680).\n' +
      '\n' +
      '## Appendix A Appendix\n' +
      '\n' +
      '### Benchmark Performance\n' +
      '\n' +
      '**ChatGPT Text Detection.** F1-scores on ChatGPT dataset released by (V Verma et al., 2023). The numbers for Zero-Shot baseline method are taken from the same work.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '**Comparison to Other Scoring Model Pairs.**\n' +
      '\n' +
      '**String Length.** Is there a correlation between _Binoculars_ score and sequence length? Such correlations may create a bias towards incorrect results for certain lengths. In Figure 12, we show the joint distribution of token sequence length and _Binocular_ score. Sequence length offers little information about class membership.\n' +
      '\n' +
      '**Score Components.** Perplexity is used by many detecting formulations in isolation. We show in Figure 13 that both perplexity and cross-perplexity are not effective detectors in isolation. Table 3 show the results where we compute PPL and X-PPL with different model families viz. LLaMA-2 and Falcon.\n' +
      '\n' +
      '### Other famous texts\n' +
      '\n' +
      'Two songs by Bob Dylan further demonstrate this behavior. _Blowin\' In The Wind_, a famous Dylan track has a much lower Falcon perplexity than his unreleased song _To Fall In Love With You_ (logPPL values are 1.11 and 3.30, respectively.) It might be reasonable for famous songs to get classified as machine text and they are more likely output than less famous songs. _Binoculars_, however, labels both of these samples confidently as human samples (with scores of 0.92, and 1.01, respectively).\n' +
      '\n' +
      '### Identical Scoring Model\n' +
      '\n' +
      'We inspect Binocular\'s performance when we choose to use identical \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{1}\\) models in equation (4). We use Falcon-7B and Falcon-7B-Instruct models and compare the two performances with\n' +
      '\n' +
      'Figure 11: F1 scores for detection of ChatGPT-generated text indicate that several detectors perform similarly. We discuss below how this metric can be a poor indicator of performance at low FPR.\n' +
      '\n' +
      'Figure 12: A closer look at the actual distribution of scores in terms of sequence length for the Ghostbuster news dataset.\n' +
      '\n' +
      'Binoculars Score over dataset by (Verma et al., 2023) in Figure 15. We observe although the vanilla Binoculars score is best over 3 domains, using Falcon-7B as input models is competitive.\n' +
      '\n' +
      '### Modified System Prompts\n' +
      '\n' +
      'We test Binoculars\' and comparable baselines\' performances in Section 5.4 on multiple prompting strategies. We prompt LLaMA-2-13B-chat with samples from the Open-Orca dataset. In addition to the default sample-specific prompt, we use 3 different versions in which we append instructions into the system prompt. These include instruction to write in the style of Carl Sagan, in a non-robotic\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline PPL Score (\\(\\mathcal{M}_{1}\\)) & X-Cross PPL Scores (\\(\\mathcal{M}_{1}^{\\prime}\\), \\(\\mathcal{M}_{2}\\)) & TPR & at TPR & at & F1-Score & AUC \\\\  & & \\(0.01\\%\\) FPR & \\(0.1\\%\\) FPR & & & \\\\ \\hline Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 100.0000 & 100.0000 & 1.0000 & 1.0000 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 99.6539 & 99.6539 & 0.9982 & 0.9999 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 99.3079 & 99.3079 & 0.9965 & 0.9998 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.3549 & 98.3549 & 0.9913 & 0.9997 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 98.7200 & 99.1600 & 0.9953 & 0.9996 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 94.9200 & 99.4000 & 0.9963 & 0.9996 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 95.8441 & 97.5757 & 0.9922 & 0.9996 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.6400 & 99.0400 & 0.9953 & 0.9995 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.8000 & 99.2800 & 0.9959 & 0.9995 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.1600 & 98.6000 & 0.9937 & 0.9992 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 98.4000 & 98.7200 & 0.9943 & 0.9992 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 94.1125 & 97.9220 & 0.9926 & 0.9992 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 93.5000 & 93.5000 & 0.9875 & 0.9990 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 92.0000 & 92.0000 & 0.9918 & 0.9990 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 94.0000 & 94.0000 & 0.9850 & 0.9989 \\\\ Llama-2-7B & Llama-7B, Llama-2-7B & 98.0000 & 98.0000 & 0.9956 & 0.9988 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 72.6957 & 72.7857 & 0.9908 & 0.9988 \\\\ Llama-2-13B & Llama-2-13B, Llama-2-13B & 97.8750 & 97.8750 & 0.9931 & 0.9987 \\\\ Llama-2-13B-Chat & Llama-2-13B, Llama-2-13B-Chat & 71.3199 & 82.6799 & 0.9846 & 0.9986 \\\\ Llama-2-13B, Llama-2-13B & 97.5000 & 97.5000 & 0.9875 & 0.9985 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 97.5778 & 97.5778 & 0.9930 & 0.9983 \\\\ Falcon-7B-Instruct & Falcon-7B, Falcon-7B-Instruct & 23.3076 & 48.3732 & 0.9842 & 0.9975 \\\\ Llama-2-13B & Llama-2-13B & 0.3200 & 32.0800 & 0.9840 & 0.9968 \\\\ Llama-2-13B-Chat & Llama-2-13B, Llama-2-13B-Chat & 20.9172 & 60.0671 & 0.9763 & 0.9968 \\\\ Llama-2-13B & Llama-13B, Llama-2-13B & 47.1476 & 69.2953 & 0.9747 & 0.9964 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Other combinations of scoring models, evaluated on our reference datasets as described in the main body.\n' +
      '\n' +
      'Figure 13: Perplexity and Cross-perplexity are not strong detectors on their own.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:19]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline \\multirow{2}{*}{Human Sample} & \\multirow{2}{*}{PPL (Falcon 7B Instruct)} & Cross PPL (Falcon 7B, Falcon 7B Instruct) & \\multirow{2}{*}{Binoculars Score} & Predicted as Human-Written \\\\ \\hline US Constitution & 0.6680 & 0.8789 & 0.7600 & ✘ \\\\ “I have a dream speech” & 1.0000 & 1.2344 & 0.8101 & ✘ \\\\ Snippet from Cosmos series & 2.3906 & 2.8281 & 0.8453 & ✘ \\\\ Blowin’ In the Wind (song) & 1.1172 & 1.2188 & 0.9167 & ✓ \\\\ Oscar Wilde’s quote & 2.9219 & 3.0781 & 0.9492 & ✓ \\\\ Snippet from White Night & 2.6875 & 2.8125 & 0.9556 & ✓ \\\\ Wish You Were Here & 2.5000 & 2.5938 & 0.9639 & ✓ \\\\ Snippet from Harry Potter book & 2.5938 & 2.6875 & 0.9651 & ✓ \\\\ First chapter of A Tale of Two Cities & 2.7188 & 2.7500 & 0.9886 & ✓ \\\\ Snippet from Crime and Punishment & 2.8750 & 2.9063 & 0.9892 & ✓ \\\\ To Fall In Love With You (song) & 3.2969 & 3.2656 & 1.0096 & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Case Studies of Text Samples likely to be memorized by LLMs.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Prompting Strategy & Instruction appended to the default system prompt \\\\ \\hline Carl Sagan & Write in the voice of Carl Sagan. \\\\  & Write your response in a way that doesn’t sound pretentious or overly formal. \\\\ Non-Robotic & Don’t use robotic-sounding words like ‘logical’ and ‘execute.’ Write in the casual style of a normal person. \\\\ Pirate & Write in the voice of a pirate. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Instructions appended in system prompts for 3 different strategies.\n' +
      '\n' +
      'Figure 15: **AUC Curve Binoculars score using identical \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\) models using Falcon-7B and Falcon-7B-Instruct.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>