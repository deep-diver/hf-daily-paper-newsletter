<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 하이에나 제거: 변압기를 긴 컨볼루션 모델로 증류하는 것\n' +
      '\n' +
      ' 토키니아나 라하리슨 랄람보니한타\n' +
      '\n' +
      'Shahrad Mohammadzadeh\n' +
      '\n' +
      '모하마드 사미누르 이슬람\n' +
      '\n' +
      'Wassim Jabbour\n' +
      '\n' +
      'Laurence Liang\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'GPT-4와 같은 아키텍처로 대표되는 대규모 언어 모델(LLM)의 급속한 진화는 자연 언어 처리의 풍경을 재형성했다. 본 논문은 LLM 사전 훈련과 관련된 효율성 문제를 해결하기 위한 선구적인 접근 방식을 소개하고 아키텍처 간 전달을 위한 지식 증류의 사용을 제안한다. 효율적인 하이에나 메커니즘의 통찰력을 활용하여, 이 방법은 2차 주의 메커니즘에 내재된 긴 상황 정보를 처리하는 문제에 직면하면서 전통적인 사전 훈련에 대한 비용 효율적인 대안을 제공하는 하이에나에 의한 변압기 모델의 주의 헤드를 대체한다. 기존의 압축 중심 기법과는 달리, 본 기법은 추론 속도를 향상시킬 뿐만 아니라 정확도와 효율성 측면에서 사전 학습을 능가한다. LLM이 진화하는 시대에 우리의 작업은 지속 가능한 AI 솔루션을 추구하는 데 기여하여 계산력과 환경 영향 사이의 균형을 이룬다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근, 자연어 처리 분야(NLP)는 Vaswani 등이 2017년에 도입한 트랜스포머 아키텍처와 함께 대용량 언어 모델(LLM)의 등장으로 혁명을 일으켰으며, 문헌에서 중요한 전환점을 맞이했다. LLM에 대해 보편적으로 받아들여지는 정의가 없음에도 불구하고, 이들은 다수의 자연어 처리 작업을 동시에 실행할 수 있는 강건한 기계 학습 모델로서 광범위하게 개념화될 수 있다. 2023년 Yang 등에 의해 기술되는 바와 같이, 이들 태스크는:\n' +
      '\n' +
      '1. 자연어 이해\n' +
      '2. 자연어 생성\n' +
      '3. 지식 집약적 과제\n' +
      '4. 추론 능력\n' +
      '\n' +
      '실제로, LLM(Large Language Models)의 풍경은 다양한 아키텍처 전략의 확산을 목격했다. 여기에는 인코더와 디코더를 모두 활용하는 모델, BERT와 같은 인코더만을 사용하는 모델, GPT-4와 같이 디코더만을 기반으로 하는 모델이 포함된다. GPT-4에 의해 예시되는 디코더 전용 모델은 특히 인코더 기반 대응물과 병치될 때 자연 언어 생성과 관련된 작업에서 우수한 성능을 입증하는 것이 관찰되었다. 이는 특히 자연어 생성 작업과 관련하여 향상된 성능을 추구하는 디코더 전용 모델에 대한 잠재적인 경향을 시사한다.\n' +
      '\n' +
      '전년도 오픈AI는 GPT-4 터보 모델을 도입했는데, 이는 성능 측면에서 전작보다 크게 발전한 것이다(오픈AI, 2023). 그러나 GPT-4 모델은 약 1조 7천억 개의 매개변수를 가지고 있어 사전 훈련에 필요한 상당한 에너지 자원에 대한 우려를 불러일으켰다. 이는 컴퓨팅 능력과 환경 영향의 균형을 맞추는 지속 가능한 AI 솔루션 개발의 중요성을 강조한다.\n' +
      '\n' +
      '본 연구는 새로운 아키텍처로 LLM(Large Language Models)을 훈련하기 위한 능숙한 방법론으로서 증류의 개념을 탐구한다. 이 접근법은 특히 미리 훈련된 다른 LLM의 지식이 활용될 수 있는 경우 새로운 아키텍처의 사전 훈련과 관련된 실질적인 전력 소비 및 재정적 지출을 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '특히, 본 연구는 전통적인 2차 다중 헤드 주의를 사용하는 LLM에 대한 지식을 대신 하위 2차 하이에나 연산자를 사용하는 등가 모델로 증류하는 것을 조사한다(Poli et al., 2023). 그런 다음 증류 결과를 처음부터 후자 모델을 훈련하는 것과 비교한다.\n' +
      '\n' +
      '우리의 작업은 또한 더 긴 컨텍스트 길이가 더 큰 모델 메모리 및 더 복잡한 모델 추론과 상관되기 때문에 긴 컨텍스트 길이를 효율적으로 처리하기 위한 모델의 필요성을 다룬다(Ding et al., 2023). 주의 메커니즘의 2차 특성은 전통적인 모델에서 근본적인 도전을 제기하여 긴 맥락 정보를 효과적으로 통합하는 능력을 제한한다. 의미 있는 시퀀스를 이해하고 생성하는 데 더 긴 컨텍스트를 활용하는 고유한 이점을 인식하면 2차 스케일링 문제를 극복하는 것이 중요해진다.\n' +
      '\n' +
      '전통적인 증류 접근법에서는 기존 모델을 동일한 아키텍처의 보다 컴팩트한 버전으로 압축하여 추론 속도를 향상시키는 데 중점을 둔다. 그러나 이 방법의 두드러진 단점은 모델의 언어 모델링 능력을 감소시키는 경향이다. 또한, 동일한 아키텍처를 유지하는 것이 긴 컨텍스트 문제를 해결하지 못하기 때문에, 이 접근법은 2차 스케일링 문제를 길이에서 다루지 않는다. **우리의 연구는 기존 변압기에서 지식을 효율적으로 긴 컨볼루션 모델로 전달하기 위해 지식 증류 방법을 사용하는 새로운 접근법을 제안함으로써 이러한 한계를 해결하며, 표준 사전 훈련 접근법과 비교할 때 컨텍스트 길이에 관한 개선된 스케일링 및 감소된 훈련 비용을 나타내는 모델을 생성한다.** 다음 점은 원하는 효율성을 달성하기 위한 주요 접근법을 설명한다:\n' +
      '\n' +
      '* 교차 아키텍처 이전을 위한 지식 증류: 본 연구는 모델 압축뿐만 아니라 기존 변압기에서 긴 컨볼루션 모델로 지식을 전달하는 지식 증류 기술을 사용하여 새로운 접근법을 개척한다.\n' +
      '* 사전 훈련 효율성을 능가하는 지식 증류: 우리의 연구는 정확성과 효율성 측면에서 전통적인 사전 훈련보다 우수한 증류 패러다임을 확립한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### 자기 주의 메커니즘\n' +
      '\n' +
      '변압기에서는 길이\\(L\\) 수열\\(u\\in\\mathbb{R}^{L\\times D}\\)에 대해 세 개의 학습 가능한 선형 투영(M_{q},M_{k},M_{v}\\in\\mathbb{R}^{D\\times D}\\)을 포함한다. 이러한 투영은 입력 시퀀스 \\(u\\)에 적용되어 쿼리((\\(Q\\)), 키((\\(K\\)), 및 값((\\(V\\)) 행렬을 계산한다:\n' +
      '\n' +
      '\\[Q=u\\cdot M_{q},\\;K=u\\cdot M_{k},\\;V=u\\cdot M_{v}.\\;\n' +
      '\n' +
      '어텐션 동작은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[A(u)=softmax\\left(\\frac{QK^{T}}{\\sqrt{D}}\\right),\\]\n' +
      '\n' +
      '여기서 SoftMax는 행 방향으로 적용된다. 자기 주의력 \\(y\\)의 출력은 주의력 가중치 \\(A(u)\\)와 값 행렬 \\(V\\):\n' +
      '\n' +
      '\\[y=\\text{SelfAttention}(u)=A(u)\\cdot V.\\]\n' +
      '\n' +
      '이 메커니즘은 모델이 입력 시퀀스에서 요소 간의 종속성을 캡처할 수 있게 하여 계산 중에 다른 요소에 다양한 중요성을 할당한다. 시퀀스의 관련 부분에 주의를 기울이는 것을 학습함으로써, 자기 주의는 순차적인 데이터를 효율적으로 처리하는 모델의 능력을 향상시킨다.\n' +
      '\n' +
      '### 부차적 주의력 대체\n' +
      '\n' +
      '표준 주의를 갖는 도전(Vaswani et al., 2017)은 입력 길이 \\(N\\)을 갖는 2차 스케일링에 있으며, 이는 하위 2차 대안의 탐색을 촉발한다. 주목할 만한 예는 어텐션 프리 트랜스포머(Zhai et al., 2021) 및 선형 어텐션(Katharopoulos et al., 2020)을 포함하며, 여기서 트랜스포머 아키텍처의 전체 무결성을 유지하면서 시간 복잡도가 감소된다.\n' +
      '\n' +
      '주의할 또 다른 대안은 차분 방정식을 통해 시스템의 동학을 포착하는 상태 공간 모델의 사용이다. 이들 모델은 입력 신호로부터 출력 신호로의 선형 매핑을 사용하며, 여기서 출력 신호 \\(y[n]\\)는 입력 신호 \\(u[n]\\) 및 상태 변수 \\(x[n]\\)의 함수이다:\n' +
      '\n' +
      '\\[x[n+1] =Ax[n]+Bu[n]\\] \\[y[n] =Cx[n]+Du[n]\\\n' +
      '\n' +
      '상태 공간 표현은 재발 관계를 통해 출력을 계산하는 직접적인 수단을 제공한다. 선형성과 시간 분산을 강제하면 시스템의 임펄스 응답 \\(h[n]\\)과 컨벌루션을 통해 출력 \\(y[n]\\)을 동등하게 계산할 수 있다:\n' +
      '\n' +
      '\\[y[n]=u[n]*h[n]=u[n]*(CA^{n}B+D\\delta[n])\\]\n' +
      '\n' +
      '여기서 \\(*\\)은 컨볼루션 연산을 나타내고, \\(\\delta\\)은 크로네커 델타 함수를 나타낸다. 이 컨볼루션 뷰는 고속 푸리에 변환 알고리즘(Brigham & Morrow, 1967)을 통해 \\(O(N(\\log N)^{2})\\)의 출력을 효율적으로 계산할 수 있다. 결과적으로, (Fu et al., 2022)에서 입증된 바와 같이 구조화된 매트릭스로서 \\(A,B,C,D\\)을 직접 파라미터화하도록 선택할 수 있다. 대안적으로, Hyena(Poli et al., 2023)는 암시적 긴 컨볼루션의 파라미터화를 갖는 새로운 접근법을 도입하며, 이는 이후 일정한 시간 추론을 위해 상태 공간 표현으로 증류될 수 있다(Massaroli et al., 2023).\n' +
      '\n' +
      ' \n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      '신경망에서의 지식 증류(Hinton et al., 2015)는 정보 손실을 최소화하면서 더 크고 복잡한 모델에서 더 작은 모델로 정보를 전달하는 것을 포함한다. 이 방법은 하나의 더 큰 모델을 압축하고 여러 모델(앙상블)의 통찰력을 단일 모델로 통합하는 것으로 확장한다.\n' +
      '\n' +
      '신경망의 지식 전달 방법인 증류는 온도 조정된 소프트맥스 확률을 활용한다. 초기에는 번거로운 모델이 소프트맥스에서 더 높은 온도를 적용하여 소프트 타겟을 생성하여 더 작은 증류 모델의 학습을 돕는다. 소프트 타겟을 모방하는 것 외에도, 정확한 라벨로 증류된 모델을 최적화하는 것은 학습을 더욱 향상시킨다.\n' +
      '\n' +
      '훈련은 두 가지 목적 함수의 가중 평균을 포함한다: 첫 번째 부분은 부드러운 목표(더 높은 온도에서)와의 컬백-라이블러 발산이다. 두 번째 부분은 (온도 1에서) 정확한 라벨이 있는 교차 엔트로피 손실이다.\n' +
      '\n' +
      '이 방법론을 통해 증류된 모델은 더 큰 모델과 정확한 지면 진리 레이블에 의해 생성된 부드러운 표적에 존재하는 미묘한 정보 모두에서 효과적으로 학습하여 더 컴팩트하면서도 지식이 풍부한 모델을 생성할 수 있다.\n' +
      '\n' +
      'LLM에서 증류의 한 가지 주목할 만한 예는 DistilBERT 모델이다: DistilBERT는 모 모델 BERT보다 40% 작고 모 모델보다 60% 빠르지만 BERT의 언어 능력의 97%를 유지한다. (Sanh et al., 2020)\n' +
      '\n' +
      '점진적 지식 전달\n' +
      '\n' +
      '증류가 대형 모델에 구현되면 교사와 학생 모델의 아키텍처 차이로 인해 교사 모델에서 학생 모델로 지식 전달이 최적으로 전달되지 않을 위험이 있다. 지식 전달을 최대화하기 위한 한 가지 접근법은 점진적 지식 전달이다: 학생 모델은 첫 번째 인코더 블록의 입력 및 출력에 대해서만 먼저 트레이닝되고, 학생 모델은 그 후에 이전 트레이닝된 블록을 동결하면서 다음 인코더 블록의 출력을 트레이닝한다. (Sun et al., 2020) 우리의 경우, 인코더 블록은 아키텍처가 자기회귀적이기 때문에 디코더로 대체된다. (도 2)\n' +
      '\n' +
      '## 3 Methods\n' +
      '\n' +
      '### Hyena Operator\n' +
      '\n' +
      'Hyena(Poli et al., 2023)는 주의 연산자에 대한 하위 이차적 대체로서 암시적 긴 컨볼루션의 사용을 제안한다. H3(Fu et al., 2022)와 같은 다른 상태 공간 모델에서와 같이 상태 공간 계수를 파라메트리화하는 대신, LTI 시스템의 임펄스 응답과 동등한 필터 \\(h:\\mathbb{N}\\rightarrow\\mathbb{R}^{d}\\)를 직접 파라메트리화하는 것을 선택한다. 필터는 먼저 위치 임베딩 \\(P_{e}:\\mathbb{N}\\rightarrow\\mathbb{R}^{d_{f}}\\) - 여기서 \\(d_{f}\\)는 임베딩 차원을 시간 인덱스에 적용하여 얻는다. 그런 다음 피드 포워드 신경망 \\(\\text{FFN}:\\mathbb{R}^{d_{f}}\\rightarrow\\mathbb{R}^{d_{m}\\) - 여기서 \\(d_{m}\\)은 모델의 차원이며, 필터를 얻기 위해 윈도우 함수를 곱한다.\n' +
      '\n' +
      '\\[h[n]\\coloneqq\\text{Window}(\\text{FFN}(P_{e}[n]))\\]\n' +
      '\n' +
      '하이에나 연산자 \\(H:\\mathbb{R}^{d_{m}}\\rightarrow\\mathbb{R}^{d_{m}}\\)는 하나의 필터 \\(h\\)를 사용하여 긴 컨텍스트 윈도우에 컨텍스트를 집계하고 곱셈 게이팅 메커니즘을 통해 비선형성을 추가한다. 첫 번째 단계는 매개변수\\(\\theta\\)를 갖는 프로젝션 연산\\(P(x,\\theta)\\)을 통해 세 개의 프로젝션\\(q,k,v\\)을 구하는 것이다. 투영 연산은 선형투영(W_{\\theta}\\)과 짧은 깊이-와이즈 컨볼루션(k_{\\theta}\\)으로 구성된다. 그런 다음 요소 현명한 곱셈 다음에 컨볼루션과 두 번째 요소 현명한 곱셈을 사용하여 하이에나 연산자의 출력을 계산한다:\n' +
      '\n' +
      '\\[P_{\\theta}(x)\\coloneqq k_{\\theta}*(x\\cdot W_{\\theta})\\]\\[H(x)\\coloneqq P(x;\\theta_{q})\\odot(h*(P(x;\\theta_{k})\\odot P(x;\\theta_{v}))\\\n' +
      '\n' +
      '여기서 \\(*\\)은 컨볼루션 연산이고 \\(\\odot\\)은 요소별 곱셈이다. 오퍼레이터는 상이한 수의 투영을 사용함으로써 더욱 일반화될 수 있다는 점에 유의한다(Poli et al., 2023).\n' +
      '\n' +
      '### Model\n' +
      '\n' +
      '실험을 수행하기 위해 사용된 모델의 관점에서, 우리는 몇 가지 주요 차이점을 제외하고, GPT-3의 아키텍처와 거의 일치하는 디코더 전용 트랜스포머 모델인 GPT-NeoX(Black et al., 2022)의 70M 파라미터 버전을 선택했다:\n' +
      '\n' +
      '* GPT 모델들에서 전통적으로 발견되는 위치 임베딩들은 회전 매트릭스를 사용하여 토큰들의 위치 정보를 인코딩하는 회전 위치 임베딩들(RoPE)로 스왑된다.\n' +
      '* 전통적인 GPT 모델에서 일반적으로 직렬로 발견되는 주의 및 피드 포워드 레이어는 대신 효율성 목적을 위해 병렬로 계산된다.\n' +
      '* 모든 피드-포워드 층은 GPT-3에서 조밀하고 희박한 층의 교번과 반대로 조밀하다.\n' +
      '\n' +
      'GPT-NeoX 아키텍처가 GPT-J의 아키텍처와 밀접하게 일치한다는 점에 주목하는 것이 유용하다. 그림 1은 모델의 아키텍처에 대한 상세 다이어그램을 보여준다. 본 논문의 목적은 주의 메커니즘을 그림 1에 표시된 대로 하이에나 메커니즘으로 대체하는 것이었다. 그러나 하이에나 버전의 모델은 하이에나 연산자가 입력 토큰에 대한 위치 정보를 이미 보유하고 있기 때문에 회전식 위치 임베딩을 포함하지 않는다는 점에 유의하는 것이 중요하다. 마지막으로, 오픈 소스 파일(Gao et al., 2020) 데이터세트에 대해 훈련된 전술한 모델의 Pythia(Biderman et al., 2023) 구현을 사용했다.\n' +
      '\n' +
      '### Distillation Procedure\n' +
      '\n' +
      '우리는 점진적 지식 전달(Sun et al., 2020)을 선택하여 학생 모델 \\(S(\\cdot;\\Theta_{s})\\)을 점진적으로 훈련한다. 각 레이어에 대해 먼저 토큰 데이터세트 \\(X\\)을 통해 교사 모델 \\(M(\\cdot;\\Theta_{t})\\)을 추론하여 증류 데이터세트 \\(D=\\{(x,y^{i}_{m})|x\\in X\\}\\)을 얻는다. 여기서 \\(x\\)은 토큰 인덱스의 시퀀스이고 \\(y^{i}\\)은 레이어 \\(i\\)에서 교사 모델의 출력이다. 그 후, 한 계층에서 학생 모델의 출력인 \\(y^{i}_{s}\\)에 대한 평균 제곱 오차 손실을 최소화한다. 마지막 계층의 경우 텍스트 데이터에 대한 비지도 학습을 수행하여 모델을 미세 조정할 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}^{i}(M(\\cdot;\\Theta_{m}),S(\\cdot;\\Theta_{s}))=\\mathbb{E}_{(x,y^{i})\\sim D}[MSE(y^{i}_{m},y^{i}_{s})]\\\n' +
      '\n' +
      '### 훈련 데이터세트 및 절차\n' +
      '\n' +
      '모든 언어 모델링 실험에는 OpenWebText(Gokaslan and Cohen, 2019)를 사용한다. 문맥 길이가 1024인 각 사전 훈련 사례를 오픈웹텍스트로부터 2M 예시를 무작위로 샘플링하여 토큰화된 사전 훈련 데이터 세트를 얻었다.\n' +
      '\n' +
      '도 1: **(A)** GPT NEO X 계층 구조: 70M GPT NEO X에서 적층된 어텐션 및 MLP의 6개 계층. **(B)** Hyena-Distilled NEO GPT X Layer Architecture: 증류 작업을 위해 Hyena 연산자에 의한 주의 헤드 교체. **(C)** (Vaswani et al., 2017)로부터 적응된, 어텐션 오퍼레이터의 시각적 표현. **(D)** (Poli et al., 2023)로부터 적응된, Hyena 연산자의 시각적 표현.\n' +
      '\n' +
      'training set and validation set with \\(0.1\\%\\) is reserved for validation. 증류 실험을 위해 각 층을 훈련하는 데 사용되는 증류 데이터 세트를 얻기 위해 훈련 세트에서 동일한 40M 토큰을 샘플링했다.\n' +
      '\n' +
      '모든 실험은 70M 교사 모델과 동일한 차원으로 동일한 6층 GPTNeoX 스타일 아키텍처를 사용한다. 우리는 먼저 Pythia(Biderman et al., 2023) 및 Hyena 모델(Poli et al., 2023)에 대한 하이퍼파라미터를 기반으로 1B 토큰에서 처음부터 모델을 사전 훈련한다. 우리는 사전 훈련을 무작위로 초기화된 모델로 시작하여 텍스트 데이터에 대한 비지도 학습을 수행하는 과정으로 정의한다. 또한, 우리는 비지도 학습(CE-intentune)을 모델 체크포인트로 시작하는 텍스트 데이터에 대한 비지도 학습을 수행하는 과정으로 정의한다. 사전 훈련 단계에서는 300개의 훈련 단계에 걸쳐 선형 웜업을 구현하고 2000번의 반복에 걸쳐 코사인 감쇠를 사용하여 학습 속도 감소를 구현한다. 이 붕괴는 우리가 최대 학습률의 10\\%\\에 도달할 때까지 계속되며, 이때 학습률은 일정하게 유지된다. 증류 과정에서도 마찬가지로 전체 학습 단계의 \\(2.5\\%\\) 이상의 선형 웜업을 통합한 후, 최대 학습 속도의 \\(10\\%\\)에 도달할 때까지 전체 단계의 붕괴를 통합한다. 우리는 증류(MSE)와 미세조정(CE-intentune)만을 시도하는데, 모든 실험은 RTX 3090에서 5시간 내에 실행되도록 설계되었다.\n' +
      '\n' +
      '## 4 언어 모델링 결과\n' +
      '\n' +
      '### Perplexity Scores\n' +
      '\n' +
      'OpenWebText의 경우 사전 훈련 데이터 세트와 동일한 방식으로 얻은 검증 세트를 사용하여 모든 모델에 대한 복잡성을 계산했다. WikiText(Merity et al., 2016)의 테스트 분할에서도 동일한 절차를 사용하였다. 위키텍스트와 오픈웹텍스트 모두에 대한 당혹성 점수는 1024개의 토큰의 컨텍스트 길이에 걸쳐 얻어졌다.\n' +
      '\n' +
      '### Language Evaluation\n' +
      '\n' +
      '관심 모델에는 (1) 주의력 드롭인 교체로 Hyena를 사용한 GPT 모델, (2) 주의력을 사용한 Pythia 70M 교사 모델, (3) 공동 지식 전달(JKT)을 사용하여 Hyena를 사용하고 증류한 Pythia 70M 학생 모델의 세 가지 자연어 과제를 적용했다.\n' +
      '\n' +
      '우리는 언어 모델 평가 하니스(lm_eval)(Gao et al., 2021)를 사용하여 이 세 가지 모델을 여러 다른 자연 언어 작업에 대해 벤치마킹했다. (표 2) 재현성을 확보하고 낮은 정밀도로 인한 기계 오차의 영향을 최소화하기 위해 모든 테스트에서 32비트 부동 소수점 정밀도를 사용하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Model** & **Wikitext** & **OpenWebText** \\\\ \\hline Pythia-70m (teacher) & 51.4 & 35.3 \\\\ \\hline Pre-trained & 230 & 64.9 \\\\ MSE & 155.8 & 63.5 \\\\ CE fine-tune & **121.2** & **49.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Pythia 70M 교사 모델, 사전 훈련된 Hyena 모델, MSE 손실로 증류된 Hyena 학생 모델 및 위에서 아래로 증류된 Hyena 학생 모델의 복잡도 점수.\n' +
      '\n' +
      '그림 2: 그것의 디코더 계층들 상의 피티아 모델 상에서의 점진적 지식 전달. (Sun et al., 2020).\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '### Analysis\n' +
      '\n' +
      '표 1에서 볼 수 있듯이, 우리의 실험 결과는 비교 가능한 GPU-시간 예산 내에서 달성된 모델 성능 측면에서 전통적인 사전 훈련 접근법에 비해 점진적인 지식 전달의 이점을 보여준다. 중요한 것은, 추가적인 비지도 학습 없이, 우리의 방법은 우수한 성능을 산출하며, 우리의 점진적인 지식 전달 전략의 효율성을 나타낸다.\n' +
      '\n' +
      '또한, 우리의 연구 결과는 비지도 학습 이전의 초기화 단계로서 증류의 가능성을 보여준다. 이 접근법은 순수 지식 전달뿐만 아니라 기존의 사전 훈련과 동일한 훈련 비용으로 향상된 성능을 제공한다. 이것은 우리의 지식 증류 접근법이 개선된 초기 성능을 제공할 뿐만 아니라 추가 교육 비용을 발생시키지 않고 추가 최적화를 허용함을 시사한다.\n' +
      '\n' +
      '우리의 결과에 대한 면밀한 조사는 지식 증류가 모델 일반화에 미치는 중요한 영향을 강조한다. 실제로 증류를 사용한 위키텍스트 복잡성 점수의 향상된 개선은 교사 모델의 지식으로 보이지 않는 데이터를 추론할 수 있는 모델의 능력을 향상시키는 데 있어 접근법의 효율성을 강조한다. 이는 특히 기존의 사전 훈련 전략과 비교할 때 기계 학습 시나리오에서 지식 증류의 광범위한 적용 가능성과 견고성에 대한 귀중한 통찰력을 제공한다.\n' +
      '\n' +
      '표 2는 하이에나를 사용하여 GPT 모델을 사전 훈련하는 것이 일반적으로 하이에나를 사용하는 피티아 70M 모델과 유사하지만 약간 낮은 정확도를 산출함을 시사한다. 이러한 결과는 하이에나를 사용하는 LLM이 일반적으로 주의 기반 LLM 모델뿐만 아니라 수행할 수 있음을 시사하며, 하이에나 기반 모델은 일반적으로 측정 성능이 약간 낮다. 우리는 학생 피티아 70M JKT 모델이 GPT Hyena 및 교사 모델보다 학생 모델의 정확도가 눈에 띄게 낮은 Sciq를 제외하고 모델 성능이 일반적으로 유사한 범위 내에 있지만 사전 훈련된 주의 기반 피티아 70M 모델에 비해 약간 낮은 성능을 갖는다는 것을 관찰한다. 그러나 아크챌린지 및 Wsc 태스크의 경우 피티아 70M 학생 모델이 다른 두 모델보다 약간 우수하고 눈에 띄게 우수하다.\n' +
      '\n' +
      '따라서 본 연구의 결과는 학생 하이에나 모델에 대한 공동 지식 전달이 일반적으로 교사 모델의 언어 능력을 보존하고, 학생 하이에나 모델이 경우에 따라 교사 모델을 능가할 수 있음을 시사한다. 하이에나는 직접 비교할 때 주의력보다 계산적으로 더 효율적이고, 공동 지식 전달이 전통적인 사전 훈련보다 계산적으로 더 효율적일 수 있기 때문에, 우리의 결과는 하이에나 학생 모델에 대한 공동 지식 전달이 사전 훈련 주의력 기반 대규모 언어 모델에 대한 계산적으로 효율적인 대안을 제공한다는 고무적인 징후를 보여준다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '**모델 크기:** 시간 제약과 제한된 접근으로 인해 더 큰 모델에 대한 접근 방식을 확장하는 것은 불가능했다. 결과적으로, 더 깊거나 더 넓은 모델에 대한 접근법의 일반화 가능성은 여전히 불분명하다. 따라서 본 연구의 방법의 실용성을 평가하기 위해 더 큰 모델에 대한 추가 실험이 수행되어야 한다.\n' +
      '\n' +
      '**트레이닝 시간:** 상기 제한과 유사하게, 획득된 보고된 결과에 대한 트레이닝 시간은 5h로 제한되었다. 따라서 정상적인 사전 훈련이 유리해지기 전에 최적의 증류 기간이 있는지 여부를 결정할 수 없었다.\n' +
      '\n' +
      '**Benchmarking:** lm_eval 테스트에 대해 다른 부동 소수점 정밀도 값을 사용하면 다른 결과가 나올 수 있음을 발견했다. 따라서 우리는 32비트 부동 소수점 정밀도를 사용하기로 선택했지만 기계 오류가 얼마나 존재하는지 직접 정량화하기는 어렵다. 람다 오픈AI 작업의 경우 일부 모델에서 매우 높은 당혹성 점수와 매우 낮은 정확도 점수를 보고했으며 이러한 이상값 결과의 근본적인 원인을 결정하기 위해서는 추가 조사가 필요하기 때문에 이러한 결과를 주요 결과에서 제외하기로 결정했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Task** & **Metric** & **GPT Hyena** & **Pythia 70M Teacher** & **Pythia 70M JKT Student** \\\\ \\hline Arc Challenge & Acc & \\(0.1775\\pm 0.0112\\) & \\(0.1749\\pm 0.0111\\) & \\(\\mathbf{0.1792\\pm 0.0112}\\) \\\\ Arc Easy & Acc & \\(\\mathbf{0.3998\\pm 0.0101}\\) & \\(0.3754\\pm 0.0099\\) & \\(0.3270\\pm 0.0096\\) \\\\ Logiqa & Acc & \\(0.1966\\pm 0.0156\\) & \\(\\mathbf{0.2104\\pm 0.0160}\\) & \\(0.1982\\pm 0.0156\\) \\\\ Piqa & Acc & \\(0.5832\\pm 0.0115\\) & \\(\\mathbf{0.5985\\pm 0.0114}\\) & \\(0.5408\\pm 0.0116\\) \\\\ Sciq & Acc & \\(0.5910\\pm 0.0156\\) & \\(\\mathbf{0.6400\\pm 0.0152}\\) & \\(0.3570\\pm 0.0152\\) \\\\ Winogrande & Acc & \\(0.5004\\pm 0.0141\\) & \\(\\mathbf{0.5296\\pm 0.0140}\\) & \\(0.4886\\pm 0.0140\\) \\\\ Wsc & Acc & \\(0.3750\\pm 0.0477\\) & \\(0.3654\\pm 0.0474\\) & \\(\\mathbf{0.5865\\pm 0.0485}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모델 성능 평가. 공동 지식 전달은 JKT로 약칭된다. 모든 결과는 32비트 부동 소수점 정밀도로 언어 모델 평가 하니스(Gao et al., 2021)를 사용하여 측정되었으며 첫 번째 값은 정확도이고 그 다음 표준 편차가 뒤따른다.\n' +
      '\n' +
      '##6 미래\n' +
      '\n' +
      '향후 연구에서는 차원성과 깊이를 줄이는 데 초점을 둔 현재 문헌의 초점을 넘어 교사 모델의 압축성을 보다 컴팩트한 상태 공간 모델로 탐구하는 것을 목표로 한다. 이것은 압축 동안 주의 메커니즘의 적응성에 대한 조사를 포함한다. 또한, 다양한 증류 접근법을 평가하여 증류 시간과 비지도 학습의 비율에 따라 성능 차이가 어떻게 확장되는지 분석할 계획이다. 모델 크기 및 훈련 시간과 관련된 한계를 해결하기 위해 향후 작업에는 더 큰 언어 모델에 대한 제안된 접근법을 평가하는 것이 포함될 것이다. 또한 다양한 2차 주의력 대체에 대한 증류를 평가하여 지식 증류 방법론의 적용 가능성과 확장성에 대한 보다 포괄적인 이해의 길을 열고자 한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 훈련 중 LLM의 계산 효율성을 향상시키기 위해 하이에나 연산자와 공동 지식 전달을 사용하는 효과(주의를 위한 드롭인 대체)를 평가했다. 그 결과 주의력이 있는 피티아 70M 모델을 교사 모델로 정의하고 주의력을 하이에나 연산자로 대체하여 피티아 70M 학생 모델에 대한 증류를 수행하였다. 오픈웹 텍스트 및 위키텍스트 데이터 세트에 대한 모델 복잡도 점수를 평가하여 점진적 지식 전달을 받은 피티아 70M 하이에나 모델이 사전 훈련된 피티아 70M 하이에나 모델보다 더 나은 성능을 보이는 것을 관찰했다. 또한 점진적 지식 전달 후 피티아 70M을 미세 조정하면 당혹성 점수가 눈에 띄게 감소하여 모델 성능이 더욱 향상되는 것을 관찰했다. 자연어 과제 측면에서 학생 하이에나 모델은 일반적으로 교사 모델보다 정확도가 낮았지만 두 가지 경우에서 학생 하이에나 모델은 교사 모델을 능가할 수 있었다. 이러한 초기 결과는 하이에나 학생 모델에 대한 공동 지식 전달이 교사 모델의 랭게이지 능력의 많은 부분을 보존할 수 있으므로 LLM을 훈련할 수 있는 실행 가능한 대안을 제공할 수 있다는 고무적인 징후를 보여준다. 그 결과, 하이에나를 주의력 드롭인 대체 수단으로 사용하는 LLM이 점진적 지식 전달과 함께 모델 훈련 동안 현재 주의력 기반 변압기에 비해 계산적으로 더 효율적이라는 유망한 징후를 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* S. 바이더만, H. 스쿨코프, Q. 앤서니, H 브래들리, K 오브라이언, E. 한라한, M. A. 칸, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. 수타위카, O. Van Der Wal(2023)Pythia: 훈련과 스케일링에 걸쳐 대규모 언어 모델을 분석하기 위한 스위트룸. In Proceedings of 40th International Conference on Machine Learning, ICML\'23, pp. 인용: SS1.\n' +
      '* S. 검은색 바이더만, E. 할라한, Q. 안소니 가오현희 맥도넬, J.팡, M. 미국 프라샨트 피엘러 푸로히트 L. 레이놀즈, J. 토우, B. 왕, S. Weinbach (2022)Gpt-neox-20b: 오픈소스 자기회귀 언어 모델. 외부 링크: 2202.0222 인용: SS1.\n' +
      '* E. O. Brigham and R. E. Morrow (1967) the fast Fourier transform. IEEE Spectrum4 (12), pp. 63-70. External Links: Document, ISSN 0018-9235 Cited by: SS1.\n' +
      '* J. Ding, S. 마락 동진 장승 황원 왕남 Zheng, F. Wei(2023)Longnet: 변압기를 1,000,000,000 토큰으로 스케일링합니다. 외부 링크: 2303.0001 인용: SS1.\n' +
      '* D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re(2022)H hungry hippos: toward language modeling with state space models. 외부 링크: 2102.14052 인용: SS1.\n' +
      '* L. 가오성 비더만 블랙 L. 골딩, T 호피, C. 포스터, J. Phang, H. He, A. Thite, N. 나베시마 Presser and C. Leahy(2020) the pile: 800gb dataset of various text for language modeling. 외부 링크: 2009.0002 인용: SS1.\n' +
      '* L. 가오진토우 비더만 블랙, A. 디포피 C. 포스터, L. 골딩재수 맥도넬 Muennighoff, et al.(2021)A framework for few-shot language model evaluation. Version v0. 0.1. Sept. 외부 링크: 2102.0210 인용: SS1.\n' +
      '* A. Gokaslan and V. Cohen(2019)OpenWebText corpus. 외부 링크: 인용된 링크: SS1입니다.\n' +
      '* G. E. Hinton, O. Vinyals, and J. Dean (2015) the knowledge in neural network. ArXivabs/1503.02531. External Links: 1503.02531 Cited by: SS1.\n' +
      '* A. Katharopoulos, A. Vyas, N. Pappas, 및 F. Fleuret(2020) 트랜스포머는 선형 주의를 갖는 고속 자기회귀 트랜스포머인 RNN이다. 외부 링크: 2006.16236 인용: SS1.\n' +
      '* Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. 포인터 센티넬 혼합 모델 2016년\n' +
      '* OpenAI(2023) OpenAI, Nov 2023. URL[https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday]).\n' +
      '*Poli et al. (2023) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and Re, C. Hyena hierarchy: Towards the larger convolutional language models, 2023.\n' +
      '* Sanh et al. (2020) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. 버트의 증류 버전인 Distilbert: 2020년, 더 작고, 빠르고, 더 싸고, 더 가볍다.\n' +
      '* Sun et al.(2020) Sun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D. Mobilebert: a compact task-agnostic bert for resource-limited devices, 2020.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention이면 된다. 2017. URL[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf).\n' +
      '* Yang et al. (2023) Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., Yin, B., and Hu, X. 실제로 llms의 힘을 이용하는 것: 2023년 채팅과 그 너머에 대한 조사.\n' +
      '* Zhai et al. (2021) Zhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., and Susskind, J. Attention Free Transformer. 2021. doi: 10.48550/ARXIV.2105.14103.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>