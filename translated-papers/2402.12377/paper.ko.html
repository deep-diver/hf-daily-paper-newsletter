<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 이진 불투명도 격자: 메쉬 기반 뷰 합성을 위한 미세 형상 상세 캡처\n' +
      '\n' +
      'CHRISTIAN REISER\n' +
      '\n' +
      '튜빙겐대학교\n' +
      '\n' +
      'Tubingen Al Center\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Germany\n' +
      '\n' +
      'STEPHAN GARBIN\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'United Kingdom\n' +
      '\n' +
      'PRATUL P. SRINIVASAN\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '미국\n' +
      '\n' +
      'DOR VERBIN\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '미국\n' +
      '\n' +
      'RICHARD SZELISKI\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '미국\n' +
      '\n' +
      'BEN MILDENHALL\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '미국\n' +
      '\n' +
      '조나단 T 바론\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '미국\n' +
      '\n' +
      'PETER HEDMAN\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'United Kingdom\n' +
      '\n' +
      'ANDREAS GEIGER\n' +
      '\n' +
      '튜빙겐대학교\n' +
      '\n' +
      'Tubingen Al Center\n' +
      '\n' +
      'Germany\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      '표면 기반 뷰 합성 알고리즘은 낮은 계산 요구 사항으로 인해 매력적이지만 종종 얇은 구조를 재현하는 데 어려움을 겪는다. 대조적으로, 장면의 기하학을 체적 밀도 필드(예: NeRF)로 모델링하는 더 비싼 방법은 미세한 기하 세부 사항을 재구성하는 데 탁월하다. 그러나 밀도 필드는 종종 "퍼지" 방식으로 기하학을 나타내며, 이는 표면의 정확한 국소화를 방해한다. 이 작업에서 우리는 밀도 필드를 수정하여 얇은 구조를 재구성하는 능력을 손상시키지 않으면서 표면을 향해 수렴하도록 유도한다. 먼저, 연속 밀도장 대신 이산 불투명도 격자 표현을 사용하여 불투명도 값이 표면에서 0에서 1로 불연속적으로 전이되도록 한다. 둘째, 반투명한 복셀을 사용하지 않고도 폐색 경계와 서브 픽셀 구조를 모델링할 수 있는 픽셀당 여러 개의 광선을 캐스팅하여 별명을 방지합니다. 셋째, 불투명도 값들의 이진 엔트로피를 최소화함으로써, 학습 종료 시점까지 불투명도 값들이 이진화되도록 유도함으로써 표면 기하학의 추출을 용이하게 한다. 마지막으로, 메쉬 단순화 및 외관 모델 피팅에 이어 융합 기반 메쉬링 전략을 개발한다. 본 논문에서 개발한 메쉬는 모바일 기기에서 실시간 렌더링이 가능하며, 기존의 메쉬 기반 방식에 비해 훨씬 높은 뷰 합성 품질을 얻을 수 있다. 추가 키 단어 및 구: 새로운 뷰 합성, 미분 렌더링, 신경 복사 필드, 멀티뷰-투-3D, 실시간 렌더링\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      '표면 렌더링은 일반적으로 볼륨 렌더링보다 더 효율적인 것으로 간주되는데, 이는 표면 렌더링이 이상적으로는 단일 3D 위치로부터 외관 데이터를 판독하는 것만을 필요로 하는 반면, 볼륨 렌더링은 각 광선을 따라 여러 지점에 걸쳐 색상 및 밀도를 집계하는 것을 필요로 하기 때문이다. 그럼에도 불구하고, 현재의 최고 품질의 뷰 합성 알고리즘들(Barron et al., 2023; Duckworth et al., 2023; Kerbl et al., 2023)은 모두 볼륨 렌더링을 사용한다. 이러한 알고리즘은 딱딱한 표면도 "퍼지" 볼륨으로 표현하는 경향이 있으며, 이는 높은 계산 비용을 초래한다. 이는 표면 촉진 레귤러라이저(Barron et al., 2022)를 적용할 때에도 성립한다(도 2 참조).\n' +
      '\n' +
      '최근 BakedSDF(Yariv et al., 2023)는 표면 기반 접근법으로도 정확한 뷰 합성이 가능함을 입증하였다. 그러나 체적 방법과 달리 BakedSDF는 미세한 기하학적 세부 사항을 복구하는 데 어려움을 겪는다. 이것의 한 가지 이유는 BakedSDF가 3D 재구성을 위한 현재 지배적인 패러다임을 채택하기 때문이며, 여기서 SDF는 트레이닝 동안 퍼지 볼륨으로 변환된다(Li et al., 2023; Wang et al., 2021; Yariv et al., 2021; Yu\n' +
      '\n' +
      '그림 1. 본 방법은 다시점 영상으로부터 삼각형 메쉬를 재구성하고, 나뭇잎, 가지, 잔디(왼쪽)와 같은 미세한 기하학적 디테일을 포착할 수 있다. 동시에 당사의 메쉬는 구글 픽셀 8 프로(오른쪽)에서 실시간 뷰 합성을 할 수 있을 만큼 충분히 컴팩트합니다.\n' +
      '\n' +
      'et al., 2022). SDF에서 체적 밀도로의 이러한 소프트 변환은 모델이 얇은 구조를 퍼지 방식으로 표현함으로써 "사기"할 수 있게 한다. 결과적으로, 메쉬하는 동안, 얇은 구조들은 종종 사라진다. 또한 훈련 중에 복구된 SDF의 유효성은 이전에 평활도로 작용하여 미세한 기하학적 세부 사항을 제거하는 경향이 있는 이코날 손실을 사용하여 보장되어야 한다.\n' +
      '\n' +
      'SDF 기반 승인들의 이러한 약점을 피하기 위해, 우리는 이코날 손실 또는 소프트 밀도 변환이 필요하지 않은 대체 전략을 조사한다. 우리는 훈련 중에 기하학을 연속적으로 "샤프"하는 볼륨 기반 표현을 사용한다. 우리는 기존의 최첨단 복사 필드 모델에 다음과 같은 세 가지 수정을 적용하여 이러한 표면 수렴을 달성한다(Barron et al., 2023). 먼저, 연속 밀도 필드 대신 이산 불투명도 그리드를 사용하여 불투명도 값이 표면에서 0에서 1로 불연속적으로 전이될 수 있도록 한다(Chen et al., 2023). 둘째, 반투명한 복셀을 사용하지 않고 반앨리어싱된 폐색 경계를 정확하게 재현할 수 있도록 픽셀당 여러 개의 광선을 주조한다. 셋째, 불투명도 값에 이진 엔트로피 손실을 적용하여 단단한 표면을 명시적으로 권장한다. 이는 그림 2와 같이 학습 수렴에 따라 불투명도 값이 0 또는 1로 이진화되며, 이는 표면 기하학의 추출을 가능하게 한다. 우리는 세 가지 요소 모두가 서브픽셀 구조의 정확한 재구성을 위해 필요하다는 것을 입증한다.\n' +
      '\n' +
      '또한 훈련 후 복구된 이진 불투명도 그리드를 삼각형 메쉬로 변환하기 위한 융합 기반 메쉬화 전략을 제시한다. 그런 다음 결과 메시는 기성 도구를 사용하여 여전히 얇은 구조를 보존하면서 실시간 렌더링에 적합한 복잡성으로 단순화될 수 있다. 마지막으로, 메쉬에 실시간 뷰어 애플리케이션에 적합한 경량 뷰 종속 외관 모델을 장착한다. UV 매핑의 표준 접근법은 우리의 매우 상세한 메쉬에 문제가 있기 때문에, 우리는 대안적인 외관 표현을 체계적으로 평가하고 바람직한 방법으로 저해상도 복셀 그리드를 갖는 삼중 평면의 조합을 찾는다(Reiser et al., 2023). 우리의 삼각형 메쉬와 외관 표현은 모바일 장치에서 실시간으로 렌더링될 수 있을 만큼 컴팩트하며 기존의 메쉬 기반 모델에 비해 훨씬 더 높은 뷰 합성 품질을 달성한다. 따라서, 우리의 작업은 표면 기반 뷰 합성 방법과 볼륨 기반 뷰 합성 방법 사이의 격차를 줄이는 단계를 나타낸다.\n' +
      '\n' +
      '##2. 관련업무\n' +
      '\n' +
      '이 섹션에서는 미분 가능한 렌더링을 사용하여 보정된 다시점 영상에 3차원 장면 표현을 맞추는 실시간 뷰 합성 방법을 검토한다. 이러한 방법들은 렌더링 공식에 따라 볼륨 기반, 표면 기반 및 하이브리드 방법으로 세분화될 수 있다.\n' +
      '\n' +
      '**Volume-based Methods.** 최고-품질 뷰 합성 방법들은 최적화 및 추론 동안 볼륨 렌더링을 사용한다(Barron et al., 2023). 이 카테고리의 많은 기술들은 신경 복사 필드(NeRFs) 패러다임을 따른다(Mildenhall et al., 2020). NeRF는 밀도 및 뷰에 의존하는 색상 값을 각각의 3D 포인트와 연관시키고, 재렌더링 대물렌즈를 갖는 멀티뷰 이미지들에 미분가능한 볼륨 렌더링을 사용하여 피팅될 수 있다. 원래의 NeRF는 장면을 나타내기 위해 MLP를 사용하며, 이는 느린 렌더링을 초래한다. 후속 작업은 복셀 그리드(Garbin et al., 2021; Hedman et al., 2021; Yan et al., 2023; Yu et al., 2021), 트라이플레인(Chen et al., 2022; Duckworth et al., 2023; Reiser et al., 2023) 또는 포인트 기반 표현(Kerbl et al., 2023; Kopanas et al., 2021; Ruckert et al., 2022a, b; Xu et al., 2022)과 같은 대체 표현을 사용하여 렌더링 속도를 향상시킨다.\n' +
      '\n' +
      '**표면 기반 방법.** SDF 또는 3DGS와 같은 최근의 볼륨 기반 방법은 실시간 렌더링이 가능하지만, Duckworth 등(2023)에서 입증된 바와 같이 BakedSDF와 같은 표면 기반 대안보다 느리다. 볼륨 렌더링에서, 픽셀의 값을 계산하는 것은 _multiple_ 샘플링 위치들(SMERF) 또는 프리미티브들(3DGS)로부터 컬러들을 합성하는 것을 필요로 하는 반면, 표면 렌더링(전형적인 조건들 하에서)은 _single_ 표면 위치로부터 외관 데이터를 판독하는 것만을 필요로 한다. 이는 하드웨어 가속 래스터화를 사용하여 효율적으로 렌더링할 수 있는 삼각형 메쉬를 사용하는 표면 기반 뷰 합성 방법의 경우이다.\n' +
      '\n' +
      '초기 시점 합성 방법은 다시점 스테레오(Jancossek and Pajdla, 2011; Schonberger et al., 2016)로부터의 표면 기하학을 이용하였고, 입력 영상들 간의 블렌딩에 의해 모델링된 외관(Debevec et al., 1998; Waechter et al., 2014; Wood et al., 2000)을 이용하였다. 이후의 방법들은 훈련된 신경망으로 표면의 외관을 예측함으로써 시각적 품질을 개선하였다(Philip et al., 2021; Riegler and Koltun, 2021; Thies et al., 2019). 그러나 이러한 접근법은 외관과 기하학에 대해 공동으로 최적화되지 않았기 때문에 재구성된 표면의 품질에 의해 제약을 받았다.\n' +
      '\n' +
      'MobileNeRF는 또한 훈련 동안 불투명도 기반 표현을 사용한다는 점에서 우리의 방법과 유사하다(Chen et al., 2023). 주요 차이점은 MobileNeRF는 바이너리 알파 마스크가 장착된 거친 프록시 메시를 출력하는 반면, 우리는 더 넓은 호환성을 가능하게 하는 전통적인 메시를 목표로 한다는 것이다. 이를 위해 MobileNeRF보다 훨씬 높은 복셀 그리드 해상도를 사용하고 컴팩트한 메쉬를 얻기 위해 단순화에 의존한다. 우리의 미세 그리드는 모바일네프(MobileNeRF)의 알파 텍스처 거친 메쉬보다 기하학적으로 더 표현되며, 이는 여러 장의 가까운 기하학 시트를 표현할 수 없다. 훈련 동안, 우리는 슈퍼샘플링과 엔트로피 정규화의 조합을 사용하여 표면 수렴을 달성하는 반면, MobileNeRF는 불투명도 값을 차별적으로 양자화한다.\n' +
      '\n' +
      'UNISURF는 또한 불투명도 기반 표현을 사용한다(Oechsle et al., 2021). 볼륨 렌더링에 더하여, UNISURF는 0.5 레벨 세트가 표면을 정의하는 두 번째 렌더링 포뮬레이션을 사용한다(Niemeyer et al., 2020; Yariv et al., 2020). 대신 불투명도 값을 정규화하여 단단한 표면을 얻는다.\n' +
      '\n' +
      '그림 2. 픽셀의 행(왼쪽, 분홍색)에 해당하는 광선들에 대한 볼륨 렌더링 가중치를 시각화한다. Zip-NeRF(Barron et al., 2023)와 같은 밀도 필드는 표면-촉진 규칙화제의 사용에도 불구하고 단단한 표면을 반투명 부피로서 나타내는 경향이 있다. 대조적으로, 우리의 불투명 그리드는 단단한 표면으로 수렴한다. 각 픽셀 열은 단일 광선의 볼륨 렌더링 가중치들을 시각화하기 때문에, 이 시각화의 갭들은 하부 표현에서의 홀들의 존재를 나타내지 않는다는 점에 유의한다.\n' +
      '\n' +
      'BakedSDF는 훈련 후 메쉬로 변환될 수 있는 부호화된 거리 함수(SDF)를 최적화하고 꼭지점 속성으로서 외관을 인코딩한다(Yariv et al., 2023). VolSDF(Yariv et al., 2021), NeuS(Wang et al., 2021), 또는 NeuralAngelo(Li et al., 2023)와 유사하게, BakedSDF는 최적화 동안 서명된 거리를 밀도 값으로 변환하고, 이러한 밀도는 볼륨 렌더링을 위해 사용된다. 유효한 SDF는 Eikonal 제약을 강제하기 위해 손실의 사용을 통해 권장되지만, 이 제약은 때때로 퍼지 방식으로 미세한 기하학적 세부 사항을 재구성하는 데 유리하게 위반됩니다. 결과적으로 얇은 구조는 이러한 SDF를 메쉬로 "굽는" 때 종종 사라진다. 이와는 대조적으로, 본 논문에서 제안한 방법은 혼탁도 값이 대부분 학습이 끝날 무렵에 이진수가 되기 때문에 최적화된 기하학과 추출된 기하학 사이에 높은 일치를 보인다.\n' +
      '\n' +
      '최근의 많은 논문들도 미세한 기하학적 세부사항에 초점을 맞추고 있다. NeRFMeshing 및 NeRF2Mesh는 둘 다 밀도 필드를 삼각형 메쉬로 변환한다(Rakotosaoa et al., 2023; Tang et al., 2023). 밀도 필드는 명확하게 정의된 표면을 갖지 않기 때문에, 이러한 방법들은 추가적인 최적화 단계로 손실 메시 변환을 보상한다. LoD-NeuS는 오차 유도 SDF 성장 전략을 사용하여 각 광선을 따라 원뿔형 프루스타를 재포화한다(Zhuang et al., 2023).\n' +
      '\n' +
      'DMTet 및 FlexiCubes는 훈련 중에 암시적 표현을 삼각형 메쉬로 차별적으로 변환한다. 우리의 방법과 유사하게, 최적화된 기하학과 구운 기하학 사이의 임의의 불일치는 회피되지만, 이러한 방법들은 각각의 전방 통과 동안 풀 그리드가 프로세싱될 것을 요구하기 때문에 높은 해상도로 스케일링되지 않는다(Liao et al., 2018; Munkberg et al., 2022; Shen et al., 2021, 2023).\n' +
      '\n' +
      '**Hybrid Methods.** 최근, 추론 동안 표면 및 볼륨 렌더링의 조합을 사용하는 일부 방법들이 등장하였다(Guo et al., 2023; Turki et al., 2023; Wang et al., 2023). 이러한 모델은 장면의 대부분을 표면 기하학으로 모델링하는 반면, "퍼지"처럼 보이는 장면의 작은 부분 집합은 볼륨으로 모델링하는 것을 목표로 한다. 우리의 목표는 장면에서 표면으로 표현될 수 있는 부분을 확장하는 것이며, 최적의 성능을 보장하기 위해 볼륨 렌더링을 가능한 한 적게 사용하는 것을 목표로 한다.\n' +
      '\n' +
      '##3. 이진 불투명 격자\n' +
      '\n' +
      '표면 기반 표현으로 얇은 구조를 캡처하기 위해 본 모델은 먼저 불투명도 기반 복셀 그리드 표현을 사용한다. 엔트로피 정규화기와 슈퍼샘플링을 사용하여, 우리의 불투명도 값은 훈련이 끝날 때까지 이진수(0 또는 1)가 된다. 이를 통해 복구된 모델을 삼각형 메쉬로 변환하는 데 필수적인 표면을 정확하게 찾을 수 있다.\n' +
      '\n' +
      '### Representation\n' +
      '\n' +
      '훈련 중에 부록 C에 기술된 바와 같이 3D 수축 함수를 사용하여 \\(\\mathrm{R}\\times\\mathrm{R}\\times\\mathrm{R}\\) 복셀 격자로 장면을 표현한다. 각 복셀에 대해 불투명도 값 \\(\\alpha\\in[0,1]\\)과 색상 값 \\(\\mathbf{c}\\in[0,1]^{3}\\)을 연관시키며, 이는 또한 뷰 방향에 따라 달라진다. 픽셀 렌더링을 위해 카메라 원점에서 픽셀의 중심을 통해 광선을 캐스팅하고 이 광선은 경로를 따라 모든 복셀과 교차한다. 각 교차 복셀에 대해 불투명도 값\\(\\alpha_{\\mathrm{K}}\\)과 색상 값\\(\\mathbf{c}_{\\mathrm{K}}\\)을 질의한다. 최종 픽셀 값 \\(\\mathbf{C}\\)은 프론트-투-백 알파 합성을 사용하여 계산된다:\n' +
      '\n' +
      '\\[\\mathbf{C}=\\sum_{k}\\alpha_{k}\\left(\\prod_{j=1}^{k-1}\\alpha_{j}\\right)\\mathbff{c}_{k}\\,.\\tag{1}\\\n' +
      '\n' +
      'MobileNeRF에 이어서, 우리는 샘플링 포인트들 사이의 거리를 사용하여 나중에 불투명도 값으로 변환되는 밀도 값들을 파라미터화하는 NeRF와 달리 \\([0,1]\\)에서 불투명도 값들을 직접 파라미터화한다(Mildenhall et al., 2020). 밀도 기반 볼륨 렌더링과 달리, 우리의 공식은 광선을 따라 복셀과 관련된 값에 대한 유한한 합이기 때문에 어떤 근사치도 포함하지 않는다. 우리 공식의 한 가지 장점은 모든 불투명도 값이 이진수일 때 표면이 하나의 불투명도 값으로 광선을 따라 첫 번째 복셀에 위치해야 한다는 것이다(Chen et al., 2023).\n' +
      '\n' +
      '얇은 구조를 표현하기 위해서는 높은 복셀 격자 분해능\\(\\mathrm{R}\\)이 필요하다. 이 크기의 복셀 그리드를 직접 최적화하는 것은 불투명도 값만 저장하려면 \\(>2\\)테라바이트의 메모리가 필요하기 때문에 실현 가능하지 않다. 대신, Muller et al.(2022)과 같이 다중 해상도 해시 인코딩이 장착된 MLP를 사용하여 그리드 값을 예측한다. MLP는 양자화된 위치에서만 질의되기 때문에 우리의 전체 표현은 본질적으로 여전히 이산적이라는 점에 유의한다(Reiser et al., 2023).\n' +
      '\n' +
      '광선과 교차하는 복셀의 수는 격자 해상도에 비례한다. 따라서, 높은 해상도로, 모든 교차 복셀에서 표현을 질의하는 것은 계산적으로 난해해진다. 이를 해결하기 위해 선행 연구는 빈 공간 스킵과 결합하여 거친-대-미세 전략을 채택했지만, 이는 초기 훈련 반복 동안 얇은 구조가 손실될 수 있다는 것이 관찰되었다(Liu et al., 2020; Muller et al., 2022). 표준 밀도 기반 NeRF의 경우, 이 문제는 "제안" MLP(Barron et al., 2022; Mildenhall et al., 2020)를 사용하여 계층적 샘플링으로 우회될 수 있다. 그러나 이 전략은 훈련 초기에 볼륨 렌더링 적분이 초기 볼륨이 다소 매끄럽기 때문에 무작위로 배치된 샘플로 잘 근사화될 수 있다는 가정에 의존한다. 불투명도 기반 렌더링은 샘플 포인트들 사이의 거리를 통합하지 않기 때문에, 식 (1)의 유한 합은 무작위로 배치된 적은 수의 샘플들만으로 잘 추정될 수 없다. 이를 우회하기 위해 먼저 Zip-NeRF를 훈련하여 장면의 거친 기하학을 인코딩하는 수렴된 제안 MLP를 생성한다(Barron et al., 2023). 모델을 훈련할 때 가중치가 고정된 미리 훈련된 제안 MLP에 의해 예측된 분포에서 고정된 수의 샘플에서만 표현을 쿼리한다. 이 샘플들은 실제 표면 위치들의 수퍼세트를 나타내며, 이는 식 (1)의 유한 합이 정확하게 계산된다는 것을 수반한다. 두 개 이상의 샘플링된 위치가 동일한 복셀 내에 속할 경우 첫 번째 위치만 사용되며, 이는 각 복셀이 최대 한 번만 기여함을 보장한다.\n' +
      '\n' +
      '### Training strategy\n' +
      '\n' +
      '표면을 위치시키고 불투명 격자로부터 삼각형 메쉬를 추출하는 것은 이진 불투명도 값을 필요로 하지만, 추가적인 정규화 없이 그리드의 불투명도 값을 최적화하는 것은 트레이닝이 끝날 때 자연스럽게 이진화된 값을 초래하지 않는다. 이진 불투명도 값을 유도하기 위해 0.5보다 작은 불투명도 값을 0으로 끌어당기는 엔트로피 손실과 0.5보다 큰 불투명도 값을 1로 끌어당기는 엔트로피 손실을 사용한다. 이 손실을 각 광선을 따라 샘플링된 모든 복셀들의 불투명도 값 \\(\\alpha_{\\mathrm{K}}\\)에 적용한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{ent}}=\\frac{1}{k}\\sum_{k}\\mathrm{H}(\\alpha_{k}), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\mathrm{H}\\)은 이진 엔트로피 함수이다:\n' +
      '\n' +
      '\\[\\mathrm{H}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p). \\tag{3}\\]\n' +
      '\n' +
      '그러나 이것만으로는 미세한 기하학적 세부 사항을 정확하게 재구성하기에 충분하지 않다. 이것은 (우리가 입력으로 사용하는 사진과 같은) 적절하게 안티앨리어싱된 이미지에서 각 픽셀의 값은 해당 픽셀과 연관된 원뿔 내의 모든 빛의 적분이기 때문이다.\n' +
      '\n' +
      '폐색 경계에서 "혼합 픽셀"의 경우를 고려하되, 픽셀의 값은 전경 객체와 배경 객체 둘 다로부터 방출되는 광에 의존한다. NeRF 또는 3DGS와 같은 체적 방법에서, 그러한 픽셀은 전경 객체의 반투명 영역을 재구성함으로써 모델링될 것이며, 이에 따라 주조되는 광선이 이를 부분적으로 관통하여 배경 객체로 진행한다. 이것은 전경 및 배경 객체 둘 다로부터의 기여들을 포함하는 재구성된 픽셀 값을 정확하게 산출한다. 그러나 이러한 반투명성의 사용은 우리의 모델에서 요구하는 이진 엔트로피 가정을 위반한다: 불투명도 값이 모두 이진인 경우, 픽셀의 중심을 통해 단일 광선을 주조하면 전경 또는 배경 객체가 타격을 받게 되고, 따라서 부정확하고 앨리어싱된 픽셀 강도(즉, "jaggies")가 산출될 것이다. 따라서 단일 광선이 각 픽셀에 대해 캐스팅된다고 가정할 때 이진 불투명도 값을 사용하여 이러한 혼합 픽셀을 정확하게 재구성하는 것은 불가능하다. 따라서 여러 표면의 기여도를 올바르게 명확하게 하기 위해 훈련 중에 픽셀당 _다중_광선을 주조한다. 보다 구체적으로, 우리는 각 픽셀의 발자국 내에서 16개의 서브선을 균일하게 샘플링한다. 각각의 서브-레이를 렌더링한 후, 최종 픽셀 값은 서브 픽셀 값들의 산술 평균으로서 계산된다. 우리는 슈퍼샘플링이 특히 단일 픽셀 미만을 커버하는 얇은 구조의 재구성과 관련하여 기하학적 품질의 상당한 개선을 생성한다는 것을 관찰한다.\n' +
      '\n' +
      '##4. 메쉬 변환\n' +
      '\n' +
      '최적화 후, 복구된 이진 불투명도 그리드를 컴퓨터 그래픽에서 기하학에 가장 편재하고 실용적인 표현인 삼각형 메쉬로 변환한다. 순진하게 수행되면, 이러한 변환은 수십억 개의 작은 큐브로 구성된 메쉬로 이어지며, 이는 실시간 렌더링에 대해 엄청나게 크다. 이를 완화하기 위해 기성 도구를 사용하여 단순화할 수 있는 메쉬를 출력하는 간단하고 확장 가능한 베이킹 파이프라인을 설계한다.\n' +
      '\n' +
      '이상치 제거를 위한### 체적 융합\n' +
      '\n' +
      '이진 점유 격자 표현을 삼각형 메쉬로 변환하기 위한 가장 기본적인 전략은 서로 상반된 불투명도 값을 갖는 모든 복셀 쌍 사이의 표면 사분면을 인스턴스화하는 것이다. 이것은 훈련 중에 이러한 복셀을 샘플링하지 않기 때문에 자유 공간에서 복셀의 불투명도 값이 완전히 제한되지 않기 때문에 제대로 작동하지 않는다. 마찬가지로, 폐색된 공간은 재렌더링 대물렌즈에 의해 제한되지 않으며, 이는 물체 내부의 임의의 불투명도 값으로 이어진다. 따라서 이 전략은 많은 무작위 표면을 생성하는데, 이는 객체 앞에서 부동 아티팩트를 산만하게 하거나 보이지 않지만 객체 내부에서는 계산적으로 낭비되는 의사 기하학을 초래한다.\n' +
      '\n' +
      '더 나은 전략은 훈련 목표에 의해 제약되는 불투명도 값을 인코딩하는 제안 MLP를 통합하는 것이다. 선행 작업은 제안 MLP(Reiser et al., 2023; Yariv et al., 2023)를 사용하여 모든 트레이닝 뷰를 렌더링하고, 그 다음 임의의 픽셀의 렌더링에 기여하는 복셀 부근의 표면만을 인스턴스화함으로써 이를 수행한다. 이는, 제안 MLP에 의해 가려지고 샘플링되지 않고 따라서 감독을 받는 장면의 부분들만이 메쉬를 위해 고려되기 때문에, 구속되지 않은 영역들의 필터링으로 이어진다. 그러나 모델에서는 그림 3의 왼쪽 하단 이미지에서 볼 수 있듯이 이 전략이 여전히 상당한 수의 부동 인공물을 생성한다. 이는 일부 복셀이 훈련 중 제안 MLP에 의해 일관되게 샘플링되지 않기 때문에 심하게 과소 구속되기 때문이다. 즉, 훈련 중에 복셀을 관찰하는 훈련 뷰의 일부에서만 일부 복셀이 샘플링된다. 이러한 구속되지 않은 복셀은 표면에서 멀리 떨어져 있음에도 불구하고 불투명도 값이 1로 잘못 할당될 수 있다. 이러한 복셀은 여전히 일부 훈련 뷰에서 샘플링되기 때문에 최종 메쉬에 잘못 추가된다. 이러한 거짓 양성을 필터링하기 위해 부록 A에 설명된 대로 체적 융합(Curless and Levoy, 1996)을 사용한다. 그림 3의 오른쪽 하단 이미지는 이상치를 제거하기 위한 체적 융합의 효과를 보여준다.\n' +
      '\n' +
      '체적 융합을 사용하기 위한 또 다른 중요한 동기는 장면의 조밀한 암시적 표현을 출력한다는 것이다. Curless and Levoy(1996)에 도시된 바와 같이, 이러한 암시적 표현은 홀-프리 메쉬로 변환될 수 있으며, 이는 홀-프리 메쉬로 변환된다.\n' +
      '\n' +
      '도 3. **서로 다른 메싱 전략들 간의 비교.** 좌측 하단 이미지는 Yariv 등(2023)으로부터의 메싱 전략을 우리의 표현에 적용함으로써 획득된 메쉬로부터 렌더링된 깊이 맵을 도시한다. 기하학은 _any_ 트레이닝 뷰에서 제안 MLP에 의해 샘플링되는 불투명도 값이 1인 모든 가시 복셀에서 인스턴스화된다. 이는 자유 공간에서 드물게 샘플링된 복셀이 훈련 손실에 의해 심각하게 과소 구속되기 때문에 수많은 부유 인공물로 이어진다. 오른쪽 하단은 이러한 구속되지 않은 복셀이 모델에서 렌더링된 깊이 맵에서 부피 융합을 실행하여 효과적으로 필터링될 수 있음을 보여준다. 이 필터링 단계는 또한 상부 이미지에서 볼 수 있는 바와 같이 얇은 구조를 완전히 보존한다.\n' +
      '\n' +
      '대부분의 메쉬 단순화 알고리즘. 마칭 큐브가 있는 메쉬로 변환하기 전에, 우리는 장면 바깥 부분의 기하학적 잡음을 제거하기 위해 \\(\\sigma=1\\)의 작은 가우시안 블러로 암시적 표현을 필터링한다.\n' +
      '\n' +
      '### 단순화 및 가시성 컬링\n' +
      '\n' +
      '더 컴팩트한 표현을 생성하기 위해, 우리는 4차 에지 붕괴 데시메이션(Garland and Heckbert, 1997)에 기초한 기성 도구로 메쉬를 단순화한다. 우리는 얇은 구조를 유지하면서 메쉬를 극적으로 단순화하기 위해 이 접근법을 발견했다. 우리는 부록 C에 설명된 대로 멀리 떨어진 영역에서 메쉬를 보다 공격적으로 단순화한다. 단순화 후, 어떤 훈련 카메라에서도 볼 수 없는 삼각형을 도태하여 삼각형의 수를 또 다른 현저한 감소로 이어진다. 시인성 추정을 위해 훈련 카메라의 포즈만을 사용하는 것은 새로운 뷰 합성 동안 명백해지는 메쉬의 구멍으로 이어진다. 이를 해결하기 위해, 가시성 추정에 사용되는 카메라 포즈 세트를 증강한다: 부록 C에 설명된 바와 같이 훈련 카메라의 포즈에 무작위로 샘플링된 오프셋과 회전을 추가하여 추가 포즈를 생성한다. 메쉬 단순화 방법이 컬링에 의해 도입된 수많은 작은 구멍에 견고하지 않은 경향이 있기 때문에 컬링 _after_ 단순화를 수행하는 것이 중요하다는 것을 발견한다.\n' +
      '\n' +
      '##5. 메쉬에 대한 뷰 의존적 외관\n' +
      '\n' +
      '뷰 합성을 가능하게 하려면 재구성된 메쉬에 대한 뷰 의존적 외관 모델이 필요하다. 이를 위해 실시간 렌더링에 적합한 옵션에 초점을 맞추어 뷰 종속 컬러에 대한 다수의 잠재적 표현 및 인코딩을 평가한다.\n' +
      '\n' +
      '### Spatial Parameterization\n' +
      '\n' +
      '우리는 메쉬 상의 위치를 외형을 인코딩하는 계수에 효율적으로 매핑하는 매개변수화를 탐색하는 것으로 시작한다.\n' +
      '\n' +
      '**UV 매핑.**UV 텍스처 맵은 외관을 위한 가장 유비쿼터스 표현이다. 그러나, 현재 UV 매핑 툴은 많은 미세한 기하학적 세부 사항을 포함하는 입력 메시의 복잡성을 잘 처리할 수 없다는 것을 발견했다(비록 Srinivasan et al.(2023)과 같은 동시 작업이 실행 가능한 경로를 제공할 수 있다).\n' +
      '\n' +
      '**Vertex Attributes.** Yariv et al.(2023)과 같은 이전의 메쉬 기반 뷰 합성 방법들은 메쉬 상의 꼭지점 속성들에 출현 계수들을 저장한 후, 각각의 얼굴에 걸쳐 이들을 보간한다. 불행하게도, 이것은 정점 밀도가 원하는 텍스처 밀도보다 더 높을 것을 요구하며, 이는 엄청나게 크고 값비싼 메시를 초래한다. 우리의 메쉬가 급격히 단순화되어 기하학적으로 단순한 영역에서 큰 삼각형으로 이어지기 때문에 이것은 우리에게는 그렇지 않다.\n' +
      '\n' +
      '** 볼륨 텍스쳐.** 3D 볼륨 텍스쳐를 이용하여 색상 값을 각 3D 위치와 직접 연관시킬 수 있다. 볼륨을 희소하게 인코딩하는 간단한 방법은 볼륨을 D\\({}^{3}\\) 복셀의 블록으로 세분화하고 비어 있지 않은 블록만을 저장하는 것이다(Hedman et al., 2021). 블록 크기 D의 선택은 트레이드-오프(trade-off)를 수반한다: 작은 블록 크기는 높은 압축성을 산출하지만, 열악한 데이터 지역성을 초래하여 느린 렌더링으로 이어진다. 단일 표면 인접 복셀을 포함하는 모든 블록이 할당되어야 하기 때문에, 큰 블록 크기는 높은 메모리 소비와 함께 제공된다. 이것은 또한 옥트리스(Benson and Davis, 2002) 또는 공간 해싱(Lefebvre and Hoppe, 2006)과 같은 대안적인 희소 데이터 구조에 대해 성립하는데, 이는 이들이 고속 액세스를 위한 차단에 동등하게 의존하기 때문이다.\n' +
      '\n' +
      '**트리플레인 및 저해상도 복셀 그리드.** 최근에, 볼륨 텍스처가 트리플레인 및 저해상도 복셀 그리드의 조합으로 콤팩트하게 인코딩될 수 있다는 것이 보여졌다(Reiser et al., 2023; Reiser et al., 2023). 트리플레인과 저해상도 복셀 그리드 모두 캐시 친화적이어서 빠른 랜덤 액세스가 가능합니다.\n' +
      '\n' +
      '표 1은 볼륨 텍스처가 가장 높은 품질을 산출하며, 3평면과 저해상도 복셀 그리드의 조합이 그 뒤를 잇고 있음을 보여준다. 정점 속성에 대한 간격이 더 두드러지며, 그림 4에서도 볼 수 있습니다. 정점 속성은 기하학적으로 단순한 영역에서 흐릿하게 보입니다. 마지막으로, 트라이플레인과 그리드 조합은 볼륨 텍스처보다 메모리를 덜 사용하고 훨씬 빠르지만, 이러한 표현들은 그림 4에서 거의 동일하게 보인다.\n' +
      '\n' +
      '### View-Dependence\n' +
      '\n' +
      '또한 실시간 뷰 합성 시스템에서 뷰 종속 컬러에 대한 확립된 포맷인 **구면 고조파** 및 **구면 가우시안**에 대한 여러 인코딩을 조사한다(Fridovich-Keil et al., 2022; Yariv et al., 2023; Yu et al., 2021),\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c}  & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) & VRAM \\(\\downarrow\\) & FPS \\(\\uparrow\\) \\\\ \\hline vertex attributes & 25.58 & 0.771 & 0.211 & **97** & 261 \\\\ volume textures & **26.25** & **0.820** & **0.143** & 4513 & 169 \\\\ triplane + voxel & 26.02 & 0.807 & 0.157 & 629 & **477** \\\\ \\hline offline & 26.86 & 0.830 & 0.135 & – & – \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. gardenvasse 상의 메쉬 모양에 대한 표현들 간의 비교. 격자 기반 표현으로 정점 속성을 대체하면 품질이 향상됩니다. 그러나, 희소 복셀 그리드 표현은 높은 메모리 소비(VRAM)로 이어진다. 질의 약간의 손실에서, "트리플레인 + 복셀" 옵션은 상당히 더 컴팩트하지만, 모든 대안들 중에서 가장 빠른 렌더링을 갖는다.\n' +
      '\n' +
      '도 4. ** 메쉬 외관에 대한 상이한 표현들 간의 비교. 정점 속성을 그리드 표현으로 대체하면 더 선명한 질감이 나타납니다. "복셀 그리드"와 더 저렴한 대체 "트리플레인 + 복셀" 사이에는 거의 차이가 없다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '베이스라인 MERF와 3DGS, 표면 기반 방식 BakedSDF를 구글 픽셀 8 프로 스마트폰, 맥북 M1 프로(2022) 노트북, 엔비디아 RTX 3090 그래픽 카드가 탑재된 데스크톱에 탑재했다. Mip-NeRF 360 데이터셋(Barron et al., 2022)의 실외 장면에 대한 초당 프레임 평균(FPS)을 보고한다. 렌더링 속도 측면에서 메쉬 기반 표현은 모든 볼륨 기반 기준선을 능가한다. 품질 메트릭 측면에서, 표 3에서 볼 수 있는 바와 같이, 우리의 방법은 여전히 가장 최근의 볼륨 기반 기준선보다 뒤처진다. 그러나, 표면 기반 방법과 볼륨 기반 방법 사이의 품질 갭은 특히 그림 5에서 볼 수 있는 바와 같이 얇은 구조의 재구성에 있어서 현저하게 감소된다.\n' +
      '\n' +
      '### Geometry Ablations\n' +
      '\n' +
      '기하학적 품질에 가장 크게 기여하는 요소를 조사하기 위해 mip-NeRF 360(Barron et al., 2022)의 야외 장면에 대한 절제 연구를 수행한다. 여기서는 메쉬의 품질을 결정하는 초기 불투명도 그리드의 훈련에 초점을 맞춘다.\n' +
      '\n' +
      '우리는 슈퍼샘플링(a) 없이 모델의 변형을 훈련한다. 이 경우 이진 점유 그리드를 훈련하는 동안에만 슈퍼샘플링을 비활성화하지만 메쉬 모양 모델을 피팅하고 품질 메트릭을 계산하기 위해 여전히 슈퍼샘플링을 사용한다. 이것은 슈퍼샘플링이 얻은 메쉬의 품질에 미치는 영향을 분리한다. 도 7의 상단 행에 도시된 바와 같이, 얇은 구조들은 트레이닝 동안 픽셀당 다수의 광선들을 주조하지 않고서는 잘 복구되기 어렵다.\n' +
      '\n' +
      '다음으로 엔트로피 손실(b) 없이 모델의 변형을 훈련한다. 이 모델의 경우, 많은 불투명도 값이 훈련 과정 동안 이진수가 되지 않기 때문에, 우리는 깊이를 불투명도 값이 0.5보다 큰 광선을 따라 첫 번째 복셀까지의 거리로 정의한다. (a)와 유사하게, 이것의 효과는 그림 7에 표시된 것과 같은 매우 얇은 구조에서 가장 두드러진다.\n' +
      '\n' +
      '마지막으로, 이진 불투명도 그리드(c)의 해상도 R을 감소시킨다. 이 실험을 위해 초기 이진 불투명도 그리드의 분해능만 감소시키지만, 전체 모델과 마찬가지로 메쉬 모양 피팅 동안 삼중 평면과 저해상도 복셀 그리드의 분해능을 동일하게 사용한다. 이는 기하학적 분해능이 메쉬 품질에 미치는 영향을 분리한다. 그림 7에서 볼 수 있듯이 얇은 구조물을 재구성하기 위해서는 고해상도가 중요하다. 이러한 절제에 대한 정량적 결과는 표 7에 나와 있다.\n' +
      '\n' +
      '### Storage Analysis\n' +
      '\n' +
      '마지막으로, 본 논문에서 제시하는 개별 컴포넌트들이 디스크 저장 및 메모리 소비에 어떻게 기여하는지 연구한다. 우리는 우리의 표현을 메쉬와 외관 모델로 나눈다. 우리가 실험적으로 보여준 바와 같이, 얇은 구조물을 재구성하는 것은 높은 그리드 해상도를 필요로 한다. 표 8에서 볼 수 있는 바와 같이, 더 이상의 처리 없이, 이는 수십억 개의 면과의 메쉬로 이어져 20GiB가 넘는 비실용적인 저장 요건을 초래한다. 그러나, 메쉬 단순화 및 도태를 사용하여, 메쉬의 크기는 100 내지 약 200 MiB의 배만큼 감소될 수 있다. 이는 표현의 전체 크기가 전체 저장소의 약 76%를 차지하는 외관 모델에 의해 지배되는 결과를 초래한다.\n' +
      '\n' +
      '###한계 및 미래업무\n' +
      '\n' +
      '트레이닝-시간 수퍼샘플링은 큰 계산 오버헤드를 추가한다. 장면의 구속되지 않은 배경의 재구성은 종종 매우 시끄럽고, 이는 우리의 메쉬의 크기를 상당히 증가시킨다. 이것은 평활도 규칙화기로 잠재적으로 완화될 수 있다. 동시 작업 누보는 우리와 같은 고-세부 메쉬에 적합한 UV 매핑 방법을 제시한다(Srinivasan et al., 2023). 외관 표현을 UV 텍스처로 대체하고 평활도 정규화를 사용하여 더 작은 메쉬를 얻으면 훨씬 더 빠른 속도와 메모리 절약으로 이어질 수 있다. 마지막으로, 우리는 우리의 접근법과 품질 차이를 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c}  & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) & \\#faces \\(\\downarrow\\) \\\\ \\hline BakedSDF & 22.47 & 0.585 & 0.349 & 40M \\\\ BakedSDF++ & 22.50 & 0.612 & 0.315 & 40M \\\\ Ours (SSAA) & **23.94** & **0.680** & **0.263** & **13M** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6. 개선된 버전의 BakedSDF(BakedSDF++)인 BakedSDF와 mip-NeRF 360(Barron et al., 2022)의 실외 장면에 대한 우리의 방법 간의 비교. 제안된 방법은 BakedSDF++보다 더 높은 뷰 합성 품질을 달성하며, 이는 우리의 소형 메쉬가 BakedSDF의 메쉬보다 뷰 합성에 더 적합함을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c}  & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline (a) No supersampling & 23.38 & 0.645 & 0.292 \\\\ (b) No entropy loss & 23.21 & 0.635 & 0.293 \\\\ (c) R = 2048 instead of R = 8192 & 22.44 & 0.582 & 0.343 \\\\ Ours (SSAA) & **23.94** & **0.680** & **0.263** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7. mip-NeRF 360 데이터세트(Barron et al., 2022)로부터의 실외 장면에 대한 기하학적 삭제에 대한 정량적 결과.\n' +
      '\n' +
      '그림 5. 우리의 방법은 얇은 구조의 재구성에 있어서 표면 기반 방법과 부피 기반 방법 사이의 품질 격차를 좁힌다.\n' +
      '\n' +
      '볼륨 기반 방법은 실내 장면에서 더 큽니다. 저용량 뷰 의존성 모델로 표면에서 캡처하기 어려운 이미지 간의 조명(예: 그림자)의 변화에 기인한다. 실제로, 우리는 이러한 장면들에서 상당히 더 높은 품질을 산출하기 위해 큰 오프라인 뷰-의존 네트워크를 발견했다. SMERF(Duckworth et al., 2023)로부터 보간된 뷰-의존성 네트워크는 유망한 실시간 대안으로 보인다.\n' +
      '\n' +
      '## 7. Conclusion\n' +
      '\n' +
      '본 논문에서는 초샘플링과 이진 엔트로피 손실을 결합한 고해상도 불투명도 그리드를 이용하여 입력 영상에서 서브 픽셀 구조를 재현할 수 있는 최초의 메쉬 기반 뷰 합성 알고리즘을 제시하였다. 볼륨 기반 대안과는 대조적으로, 우리의 방법은 저렴한 스마트폰에서 실시간으로 렌더링된다. 기존의 메쉬 기반 뷰 합성 방식인 BakedSDF에 비해 3배 더 컴팩트한 메쉬를 얻을 수 있으며, 야외 장면에서 1.46 dB 더 높은 PSNR을 얻을 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Barron et al. (2022) Jonathan T. 배런, 벤 밀덴홀, 도르 버빈, 프라툴 P. 스리니바산, 피터 헤드만. 2022. Mip-NetRF 300: Unbounded, Anti-Alsaceal Neural Radiance Fields _ CVPR_(2022).\n' +
      '* Barron et al. (2023) Jonathan T. 배런, 벤 밀덴홀, 도르 버빈, 프라툴 P. 스리니바산, 피터 헤드만. 2023. ZweBF: NetRF: Anti-Alsaceal Grid Based Neural Radiance Fields. _ ICCV_(2023).\n' +
      '* Benson and Davis (2002) David Benson and Joel Davis. 2002. Octree Textures. _ ACM Transactions on Graphics (TOG)_ (2002).\n' +
      '* Chen et al. (2022) Angel Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. 텐서프: 텐서피얼 복사 필드. 유럽 컴퓨터 비전 회의에서. 스프링거 333-350\n' +
      '* Chen et al. (2023) Zhiqun Chen, Thomas Funkhouser, Peter Hedman, and Andrea Taglassacchi. 2023. MobileNet: Mobile Architecture에서 효율적인 Neural Field Rendering을 위한 Polygon Masterization Pipeline 활용 CVPR_(2023).\n' +
      '* Curless and Levoy(1996) Brian Curless and Marc Levoy. 1996. Volumetric Method for Building Complex Models from Range Images. _ SIGGRAPH_(1996).\n' +
      '* Debeve et al.(1998) Paul Debeve, Yichou Yu, and George Borshkov. 1998. Projective texture-mapping을 이용한 효율적인 view-dependent image-based rendering. _ EGSR_(1998)\n' +
      '* Duckworth(1979) Claude D. 1979. Lanco filtering in one and two dimensions. _ Journal of Applied Meteorology and Climatology_ (1979).\n' +
      '* Duckworth et al. (2023) Daniel Duckworth, Peter Hedman, Christian Reiser, Peter Zhihain, Jean-Francois Thibibert, Marc Lucic, Richard Szeliski, and Jonathan T. 배런 2023. SMERF: 실시간 대용량 탐색을 위한 Streamable Memory Efficient Radiance Fields. _ ArXivcs.CV_/2312.07541\n' +
      '* Fridovich-Keel et al. (2022) Sara Fridovich-Keel, Alex Yu, Matthew Tancki, Qinhong Chen, Benjamin Recht, and Angelo Kozanzawa. 2022. 플레녹셀: 신경망이 없는 복사 필드. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 5501-5510\n' +
      '* Grablin et al. (2021) Stephan Grablin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. 2021. FastNeRF: 200fps에서의 고충실도 신경 렌더링 ICCV_(2021).\n' +
      '* Garland and Heckbert (1997) Michael Garland and Paul S. 헥버트 1997. 4차 오차 메트릭을 이용한 표면 단순화. _ SIGGRAPH_(1997).\n' +
      '* Guo et al. (2023) Yuan-Chen Guo, Yan-Pei Cao, Chen Wang, Yu He, Ying Shan, and Song-Hai Zhang. 2023. VRSA: 효율적인 뷰 합성을 위한 하이브리드 볼륨-메쉬 표현_ SIGGRAPH Asia_(2023).\n' +
      '* Hedman et al. (2021) Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. 배런과 폴 드베브 2021. 실시간 뷰 합성을 위한 Bagging Neural Radiance Fields. _ ICCV_(2021).\n' +
      '* Janockes and Jajla (2011) Michal Janockes and Tomas Jajla. 2011. 약하게 지지된 표면을 보존하는 다시점 재구성. _ CVPR_(2011).\n' +
      '* Karis (2014) Brian Karis. 2014. 고품질 시간 수퍼샘플링. ACM SIGGRAPH 과정: 게임에서의 실시간 렌더링의 발전\n' +
      '* Kershl et al. (2023) Bernhard Kershl, Georgios Koprans, Thomas Lemikuhler, and George Drettakis. 2023. 실시간 복사 필드 렌더링을 위한 3차원 가우시안 분할 _ ACM Transactions on Graphics_ (2023).\n' +
      '* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: stochastic optimization의 방법. _ arXiv_(2014).\n' +
      '* Keppans et al. (2021) Georgios Koppans, Julien Philip, Thomas Lemikuhler, and George Drettakis. 2021. Per-View 최적화를 이용한 점기반 신경망 렌더링. _Computer Graphics Forum_, Vol. 40. Wiley Online Library, 29-43.\n' +
      '* Lefebvre and Hoppe (2006) Sylvain Lefebvre and Hugues Hoppe. 2006. Perfect spatial hashhing. _ ACM Transactions on Graphics (TOG)_ (2006).\n' +
      '* Li et al. (2023) Zluoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. 2023. Neurallangel: High Fidelity Neural Surface Reconstruction. _ CVPR_(2023).\n' +
      '* Liao et al.(2018) Yiyi Liao, Simon Donne, and Andreas Geiger. 2018. 딥 마칭 큐브: 명시적 표면 표현 학습. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2916-2925.\n' +
      '* Li et al. (2020) Lingie Liu, Jianto Guo, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. 신경 희소 복셀 특징. _ NeurIPS_(2020).\n' +
      '* Mildenhall et al. (2020) Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancki, Jonathan T. Barron, Ravi Ramamoorthi, and Ren B. 2020. NeRF: Representing Semes as Neural Radiance Fields for View Synthesis. _ ECCV_(2020).\n' +
      '* Muller et al. (2022) Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. 다해상도 해시 인코딩을 갖는 인스턴트 뉴럴 그래픽 프리미티브들 _ ACM Trans. Graph_(2022).\n' +
      '* Munkberg et al. (2022) Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. 2022. 이미지에서 삼각형 3D 모델, 재료 및 조명 추출 CVPR_(2022년 6월)\n' +
      '* Niemeyer et al. (2020) Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger. 2020. Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision. _ CVPR_(2020).\n' +
      '* Oechle et al.(2021) Michael Oechle, Songyong Peng, and Andreas Geiger. 2021. UNIBUR: Multi-View Reconstruction을 위한 Neural Implicit Surface와 Radiance Field의 통합 ICCV_(2021).\n' +
      '* Philip et al. (2021) Julien Philip, Sebastien Morgenthaler, Michael Gharbi, and George Drettakis. 2021. 다시점 스테레오로부터의 자유 시점 실내 신경 재조명_ ACM Transactions on Graphics (TOG)_ (2021).\n' +
      '* Rakotosona et al.(2023) Marie-Julie Rakotosona, Fabian Manhardt, Diego Martin Arroyo, Michael Niemeyer, Abhijit Kundu, and Federico Tomblari. 2023. NeRFMembishing: Distilling Neural Radiance Fields into Geometrically-Accurately 3d Meshes _ 3DV_(2023).\n' +
      '* Rieser et al. (2023) Christian Rieser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. 2023. MERF: Unbounded Seemes에서의 실시간 시점 합성을 위한 메모리 효율적인 Radiance Fields. _ ACM Trans. Graph.__ (2023).\n' +
      '* Riedger and Koltun (2021) Gernot Riedger and Vladlen Koltun. 2021. 안정적인 시점 합성_ CVPR_(2021).\n' +
      '* Radivert et al.(2022) Darrius Radivert, Linus Franke, and Marc Stamminger. 2022a. Adop: 근사 미분가능한 일-픽셀 포인트 렌더링. _ ACM Transactions on Graphics (ToG)_41, 4, 2022.\n' +
      '* Ruckert et al. (2022) Darus Ruckert, Yuanhao Wang, Rui Li, Ramzi Houghi, and Wolfgang Heidrich. 2022b. Neet: 신경 적응 단층 촬영. _ ACM Transactions on Graphics_(TOG) 41, 4, 2022), 1-13.\n' +
      '* 사키(2016) 마르코 사키. 2016. A excursion in temporal supersampling. 게임 개발자 컨퍼런스\n' +
      '* Schonberger et al. (2016) Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. 2016. 비정형 멀티뷰 스테레오를 위한 픽셀뷰 선택_ ECCV_(2016).\n' +
      '* Shen et al.(2021) Tianchang Shen, Jun Gao, Kangcue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. 딥마칭 사면체: 고해상도 3차원 형상 합성을 위한 하이브리드 표현_ NeurIPS_(2021).\n' +
      '* Shen et al. (2023) Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangcue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Faller, Nicholas Sharp, and Jun Gao. 2023. Gradient-Based Mesh Optimization을 위한 유연한 Isosurface Extraction. _ ACM Trans. Graph.__ (2023).\n' +
      '* Srinivasan et al. (2023) Pratul P. Srinivasan, Stephan J. Garbin, Dor Verbin, Jonathan T. 배런과 벤 밀덴홀 2023. Nuvc: Unvuly 3D Representation을 위한 Neural UV Mapping. _ arXiv_(2023).\n' +
      '* Tang et al. (2013) Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshun Hu, Errui Ding, Jingdong Wang, and Gang Zeng. 2023. 적응적 표면 정제를 통한 NeelFi로부터의 중복 텍스처 메쉬 복구__ ICCV_(2023).\n' +
      '* Thies et al. (2019) Justus Thies, Michael Zollhofer, and Matthias Niedner. 2019. Defeered Neural Rendering: Neural Textures를 이용한 이미지 합성 _ ACM Transactions on Graphics (TOG)_ (2019).\n' +
      '* Turki et al. (2022) Haithen Turki, Vasa Agrawal, Samuel Rota Bulo, Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael Zollhofer, and Christian Richard. 2022. HybridNetRF: Adaptive Volumetric Surfaces를 통한 효율적인 Neural Rendering. arXivcs.CVE-2213.01360.\n' +
      '* Unke and Moenner (2024) Oliver T. 운키와 하트무트 모에너 2024. 52x: [G3-]\n' +
      '* Waeetheret et al. (2014) Michael Waeetheret, Nils Moenbride, and Michael Goesele. 2014년, 색깔이 있어야지! 3D 재구성의 대규모 텍스처링. _ ECCV_(2014).\n' +
      '* Wang et al. (2021) Peng Wang, Lingie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021. Neets: Multi-view Reconstruction을 위한 볼륨 렌더링에 의한 Neural Implicit Surfaces by Volume Rendering. _ NeurIPS_(2021).\n' +
      '* Wang et al. (2023) Zian Wang, Tianchang Shen, Merlin Nusser-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Miller, 및 Zan Gogic. 2023. 효율적인 신경 복사 필드 렌더링을 위한 적응적 쉘. _ ACM Trans. Graph.__ (2023).\n' +
      '* Wood et al. (2000) Daniel N. 대니얼 L. 우드 Anuma, Ken Adlinger, Brian Curless, Tom DuchampHan Yan, Celong Liu, Chao Ma, Xing Mei. 2023. PleuYDIB: 고속 훈련 및 렌더링을 위한 메모리 효율적인 VDIB 기반 복사 필드. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 88-96\n' +
      '* Liu et al.(2020) Lei Ying, Shiqai Liu, and Marco Salvi. 2020. Temporal Antialiasing Techniques에 대한 조사__ 컴퓨터 그래픽스 포럼_(2020).\n' +
      '* Yariv et al. (2021) Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. 신경 암시적 표면의 볼륨 렌더링. _ NeurIPS_(2021).\n' +
      '* Yariv et al. (2023) Izor Yariv, Peter Hedham, Christian Reiser, Dror Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. 배런과 벤 밀덴홀 2023. BakedSDf: 실시간 시점 합성을 위한 Meshing Neural SDFs _ SIGGRAPH_(2023).\n' +
      '* Yariv et al. (2020) Izor Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Disentangling geometry and appearance에 의한 다시점 신경 표면 재구성 _ 신경 정보 처리 시스템_33(2020), 2492-2502에서의 발전.\n' +
      '* Yu et al. (2021) Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021. 신경 복사 필드의 실시간 렌더링을 위한 PreDetorse. _ ICCV_(2021).\n' +
      '* Yu et al. (2022) Zehao Yu, Songru Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. MonoSDF: Neural Implicit Surface Reconstruction을 위한 Monocular Geometric Cues 탐사 _ NeurIPS_(2022).\n' +
      '* Zhuang et al. (2023) Yiying Zhuang, Qiang Zhang, Ying Feng, Hao Zhu, Yao Yao, Xiaoyu Li, Yan-Pei Cao, Ying Shan, 및 Xun Cao. 2023. Antialiased Neural Implicit Surfaces with Encoding Level of Detail. _ SIGGRAPH Asia_(2023).\n' +
      '\n' +
      '## 부록 A 융합 알고리즘\n' +
      '\n' +
      '훈련된 이진 불투명도 그리드를 삼각형 메쉬로 변환할 때, 훈련 중에 이러한 영역만 감독되었기 때문에 제안 MLP에 의해 샘플링된 영역의 기하학만 인스턴스화하는 것이 중요하다. 이는 제안 MLP를 사용하여 훈련 시점으로부터 깊이 맵을 렌더링하고 언프로젝션을 통해 표면 복셀을 생성함으로써 달성될 수 있다. 그러나 훈련 중에 일부 복셀은 뷰의 일부에서만 샘플링될 수 있으므로 부정확한 불투명도 값을 가질 수 있다. 이는 결과적인 메시에서 부동 아티팩트로 이어진다(본 논문의 도 3 참조).\n' +
      '\n' +
      '우리는 이러한 구속되지 않은 복셀을 필터링하기 위해 체적 융합을 사용한다. 구체적으로, 각 복셀에 대해, 복셀이 1) 자유 공간에서, 2) 표면에서 관찰되는 깊이 맵의 수를 계산한다. 매우 적은 수의 훈련 뷰에서 구속되지 않은 복셀이 나타나므로, 우리는 자유 공간이 아닌 표면에서 더 자주 관찰되는 복셀만 유지함으로써 탐지하고 폐기할 수 있다.\n' +
      '\n' +
      '체적 융합에 대한 또 다른 중요한 동기는 대부분의 단순화 알고리즘에서 선호되는 입력인 구멍이 없는 메쉬로 쉽게 변환될 수 있는 조밀한 암시적 표현을 출력한다는 것이다. 결과적으로, 우리는 우리의 융합 알고리즘이 관찰되지 않은 복셀을 "내부" 또는 "외부"로 라벨링하기를 원한다. 단순한 휴리스틱은 관찰되지 않은 복셀에 "내부"로 레이블을 지정하는 것이다[23]. 그러나, 드문 경우들에서, 제안 MLP는 결과의 메시에 큰 구멍들을 조각하는 물체의 표면을 샘플링하지 않는다. 우리는 복셀이 외부로 표시되기 전에 자유 공간에서 복셀을 관찰하기 위해 여러 개의 뷰를 요구함으로써 이를 해결한다.\n' +
      '\n' +
      '이것은 우리의 융합 알고리즘으로 이어진다. 각 복셀에 대해 카운트합니다:\n' +
      '\n' +
      '*\\(\\mathcal{S}\\): 표면에서 관찰되는 뷰의 수.\n' +
      '*\\(\\mathcal{F}\\): 그것이 표면 앞에서, 즉 자유 공간에서 관찰되는 뷰의 수.\n' +
      '*\\(\\mathcal{O}\\): 그것이 관측되는 뷰의 수.\n' +
      '\n' +
      '구체적으로, 깊이 값 \\(d\\)을 구하는 각 학습 뷰에 복셀을 투영한다. 그리고 복셀의 깊이가 \\(d\\)와 거의 같으면 \\(\\mathcal{S}\\)을 증가시킨다. 복셀의 깊이가 \\(d\\)보다 작으면 \\(\\mathcal{F}\\)을 증가시킨다. 복셀이 훈련용 카메라의 절두부 내에 있으면, 우리는 \\(\\mathcal{O}\\)을 증가시킨다. 마지막으로 다음 조건 중 하나가 충족되면 복셀을 "내부"로 표시한다.\n' +
      '\n' +
      '1. \\(s\\mathcal{S}>\\mathcal{F}\\)\n' +
      '2. \\(\\mathcal{F}<t(\\mathcal{O})\\)\n' +
      '3. \\(\\mathcal{S}=0\\) 및 \\(\\mathcal{F}=0\\)\n' +
      '4. \\(\\mathcal{O}<2\\)\n' +
      '\n' +
      '우리는 얇은 구조를 보존하는 데 도움이 되는 표면으로의 재구성을 편향시키기 위해 \\(\\mathcal{S}\\)과 \\(s>1\\)을 곱한다. 물체 경계에서의 복셀이 MLP에 의해 일관되게 샘플링되지 않기 때문에, 가중치 \\(\\mathcal{S}\\)와 \\(\\mathcal{F}\\)은 물체의 침식으로 이어진다. 규칙 (2)는 때때로 제안 MLP가 전체 객체를 완전히 놓치는 사실에 의해 동기 부여된다. 관찰되지 않은 물체의 내부에서는 \\(\\mathcal{S}\\)이 0과 같으나, \\(\\mathcal{F}\\)이 0이 아니기 때문에 복셀을 외부로 잘못 표시하게 된다. 규칙(2)은 복셀이 외부로 라벨링되기 위해 자유 공간에 놓이기 위해 복셀을 관찰해야 하는 최소 수의 뷰를 요구함으로써 이를 수정한다. 이 임계값은 복셀을 관찰하는 뷰 수에 따라 달라져야 하는데, 그렇지 않으면 드물게 관찰된 복셀이 항상 내부로 표시되기 때문이다. 규칙 (3)은 현장의 완전히 관찰되지 않은 부분들이 내부로 라벨링되는 것을 보장한다. 선택적 규칙(4)은 단일 카메라에 의해서만 관찰되는 장면의 부분들이 본질적으로 구속되지 않고 따라서 기하학적 잡음에만 기여하기 때문에 적어도 두 개의 뷰들에서 관찰되는 복셀들만이 고려되는 것을 보장한다. 본 논문의 그림 3에서 볼 수 있듯이, 이 알고리즘은 얇은 구조물을 보존하면서 부유 인공물을 효과적으로 제거한다.\n' +
      '\n' +
      '## 부록 B 청킹을 통한 스케일러블 메쉬 변환\n' +
      '\n' +
      '표면 기반 접근법으로 얇은 구조물을 포착하기 위해서는 81923의 매우 높은 격자 해상도가 필요하다. 이러한 고해상도 그리드를 하나의 패스로 처리하는 것은 너무 많은 메모리를 필요로 한다. 다행히도, 우리 파이프라인의 모든 단계(체적 융합, 필터링, 행킹 큐브 및 단순화)는 10243개의 청크로 실행될 수 있으며, 이는 임의의 해상도로 스케일링을 허용한다. 청크 경계에서 불연속을 방지하기 위해 경계 정점을 그대로 유지하도록 메쉬 단순화 알고리즘을 구성한다. 그런 다음 하위 메쉬를 연결하고 중복 경계 정점을 병합하여 최종 메쉬를 계산할 수 있다. 체적 융합 속도를 높이기 위해 표면의 초기 추정치에 충분히 가까운 복셀만 처리한다. 모든 입력 뷰의 깊이 맵에 포함된 깊이 값을 투영하지 않음으로써 표면의 초기 추정치를 얻는다. 그런 다음 그리드 분해능(81923)을 기반으로 생성된 3D 점을 양자화하여 표면 복셀 목록을 제공한다. 그런 다음 장면을 163개의 블록으로 세분화하고 각 블록에 대해 최대 \\(D=64\\) 복셀을 포함하는지 여부를 결정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c|c c c}  & & (a) Dense Mesh & (b) + Simpl. & (c) + Culling \\\\ \\hline Mesh & \\#vertices & 606M & 9M & **7M** \\\\ Mesh & \\#faces & 1208M & 18M & **10M** \\\\ Mesh & VRAM & 20.28 GiB & 0.30 GiB & **0.19 GiB** \\\\ Mesh & DISK & 21.40 GiB & 0.32 GiB & **0.20 GiB** \\\\ \\hline Appearance & VRAM & 0.75 GiB & 0.75 GiB & **0.75 GiB** \\\\ Appearance & DISK & 0.65 GiB & 0.65 GiB & **0.65 GiB** \\\\ \\hline \\hline Total & VRAM & 21.02 GiB & 1.05 GiB & **0.94 GiB** \\\\ Total & DISK & 22.05 GiB & 0.97 GiB & **0.85 GiB** \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8. 우리의 방법의 메모리 소비 및 저장 영향. (a) 단순화 전의 조밀한 메쉬, 단순화 후의 (b), 도태 후의 (c) 단순화와 도태는 실용적인 메쉬 크기를 달성하는 데 중요하다. 결과는 mipNeRF-360 [3]의 모든 장면에 걸쳐 평균화된다.\n' +
      '\n' +
      '이전에 계산된 관찰된 표면 복셀 목록의 복셀입니다. 융합하는 동안 우리는 살아있는 것으로 표시되지 않은 블록을 건너뛴다. 우리가 더 이상 암시적 표현을 조밀하게 계산하지 않기 때문에, 행진 큐브를 실행하면 대부분의 단순화 알고리즘에서 선호되는 입력인 구멍이 없는 메시가 생성된다는 것이 더 이상 보장되지 않는다. 그러나 우리는 \\(D=64\\)의 선택으로 적당한 수의 구멍만 도입됨을 알 수 있다. 이러한 구멍은 일반적으로 관찰되지 않으므로 재구성의 품질이 저하되지 않는다. 이 기술은 또한 약간 더 작은 메쉬로 이어진다.\n' +
      '\n' +
      '## 부록 C 구현 세부사항 및 하이퍼파라미터\n' +
      '\n' +
      '**Architecture.** 제안 MLPs 및 이진 불투명도 값들 및 뷰-의존적 컬러들을 예측하는 MLP에 대해, 우리는 다중-해상도 해시 인코딩(Muller et al., 2022)에 기초한 Zip-NeRF(Barron et al., 2023) 아키텍처를 밀접하게 따른다. 0에서 1 사이의 불투명도와 색상 값을 바인딩하기 위해 시그모이드 활성화 함수를 사용한다. Reiser et al.(2023)에 따라, 메쉬 외관 피팅 동안, 우리는 해시 그리드가 장착된 MLP를 갖는 3면 및 저해상도 복셀 그리드의 값들을 예측한다. 이 MLP의 경우 이진 불투명도 그리드를 매개변수화하는 MLP와 동일한 아키텍처를 사용한다.\n' +
      '\n' +
      '무한한 장면에서 멀리서만 관찰되는 영역은 낮은 해상도로 나타낼 수 있다. 장면 중심으로부터의 거리에 따라 부드럽게 감소하는 해상도를 얻기 위해, MLP를 질의하기 전에 MERF의 수축 함수를 각 위치 \\(\\mathbf{x}\\)에 적용한다:\n' +
      '\n' +
      'ff{x}\\begin{cases}x&\\text{if}\\|\\mathbf{x}\\|_{ \\infty}\\leq 1\\\\frac{x}{\\|\\mathbf{x}\\|\\neq\\|\\mathbf{x}\\|_{\\infty}>1\\\\left(2-\\frac{1}\\nfty}>1\\end{cases}\\frac{x}\\|\\x|}\\text{if}x\\mathbf{x}\\|_{\\infty}\\text{if}x\\neq\\|\\mathbf{x}\\|\\left(2-\\frac{1}\\nfty}>1\\end{cases}\\tag{4}\\text{if}x\\mathbf{x}\\|\\x}\\text{if}x\\neq\\|\\mathbf{x}\\|\\left(2-\\frac{1}\\nfty}>1\\end{cases}\\frac{\n' +
      '\n' +
      '수축 함수를 적용하기 전에 입력 좌표를 2.5의 배율로 스케일링하여 전경에 더 많은 표현력을 할당한다. 독립형 복셀 그리드에서는 \\(2048^{3}\\)의 해상도를 사용한다. 3면 격자와 저해상도 복셀 격자의 조합은 각각 \\(2048^{2}\\)과 \\(512^{3}\\)의 해상도를 사용한다.\n' +
      '\n' +
      '**최적화.** 이진 불투명도 그리드 최적화를 위해 초기 학습률 0.01, 최종 학습률 0.001, 25K 단계의 Adam(Kingma and Ba, 2014)을 사용한다. 메쉬 모양 최적화를 위해 초기 학습률 0.0005, 최종 학습률 0.00005 및 100K 단계의 Adam을 사용한다. 학습 속도는 2500단계 동안 워밍업됩니다. 이진 엔트로피 손실의 경우, 우리는 0.05의 가중치를 사용한다.\n' +
      '\n' +
      '** 단순화** 메쉬 단순화를 위해서는 원본 삼각형의 어떤 분율을 유지해야 하는지 제어하는 비율 \\(R\\)을 지정해야 한다. 우리는 정확한 뷰 합성을 위해 배경이 덜 중요하기 때문에 전경보다 더 공격적으로 배경을 단순화하고자 한다. 앞 절에서 자세히 설명했듯이 단순화는 청크 단위로 실행된다. 그 장면은 청크의 \\(8^{3}\\) 격자로 세분된다. 우리는 청크의 중심이 \\([-1,1]^{3}\\) 단위 큐브 바깥에 있다면 청크는 배경에 놓여 있는 것으로 정의한다. 전경 청크의 경우 \\(R\\)을 0.03으로 설정하고, \\(R\\)을 0.015로 설정함으로써 공격적으로 두 배의 배경 청크를 단순화하며, 그에 따라 \\(R\\)을 조정하여 최대 0.5M 얼굴의 배경 청크가 포함되도록 한다.\n' +
      '\n' +
      '**가시성 도태.** 가시성 도태를 위해, 우리는 트레이닝 이미지들의 카메라 포즈들을 사용할 뿐만 아니라, 원래의 포즈에 무작위 오프셋들 및 회전들을 추가함으로써 각각의 트레이닝 포즈들에 대해 6개의 추가 포즈들을 생성한다. 교육용 카메라의 기원을 \\(\\mathbf{o}\\)으로 하고, 교육용 카메라가 향하는 방향을 \\(\\mathbf{d}\\)으로 한다. 등방성 가우시안 잡음을 적용하여 새로운 원점\\(\\mathbf{o}\\)을 얻는다. 새로운 방향\\(\\hat{\\mathbf{d}\\)을 얻기 위해 E3X 라이브러리(Unke and Maennel, 2024)를 사용하여 방향벡터의 \\(\\epsilon\\)-이웃으로부터 균일한 샘플을 얻는다:\n' +
      '\n' +
      '\\sim\\mathcal{N}(\\mathbf{o},\\sigma^{2}\\mathbf{I}))\\,\\tag{6}\\hat{\\mathbf{d}\\sim\\mathcal{U}(\\{\\mathbf{v}\\in\\mathbbb{R}^{3}:||\\mathbf{v}-\\mathbf{d}||_{2}<\\epsilon,||\\mathbf{v}||_{2}=1\\})\\,\\tag{5}\\.\n' +
      '\n' +
      '우리는 추가 포즈가 최종 메쉬의 가시적인 구멍을 피하는 데 중요하다는 것을 발견했다.\n' +
      '\n' +
      '## 부록 D 시간별 앨리어싱 방지\n' +
      '\n' +
      '우리는 산업 모범 사례(Karis, 2014)에 따라 우리의 시간적 안티앨리어싱 전략(Yang et al., 2020)을 구현한다. 즉, 길이 16의 \\(\\text{Halton}(2,3)\\) 시퀀스로 프로젝션 행렬을 지터하고, 현재 깊이 버퍼를 이용하여 이전 프레임의 컬러를 재투영한다. 그리고 블렌드 팩터가 0.05인 지수 이동 평균을 사용하여 현재 프레임의 컬러로 재투영된 컬러를 평균화하고, 반복 재샘플링으로 인한 블러 현상을 줄이기 위해 반지름이 3인 Lanczos 커널(Duchon, 1979)을 사용하여 재투영한다. 마지막으로, 폐색된 콘텐츠에 대한 고스팅 아티팩트를 제한하기 위해 YCoCg 컬러 공간(Karis, 2014)에서 분산 박스(Salvi, 2016) 이웃 클램핑을 사용하여 재투영된 색상을 클립한다.\n' +
      '\n' +
      'TAA는 움직임 하에서 흐림을 일으키는 것으로 알려져 있기 때문에, 우리는 움직이는 카메라로 테스트 세트 이미지의 품질을 평가한다. 대상 카메라 자세가 주어졌을 때, 먼저 "상향" 벡터\\(\\mathbf{u}\\)와 "좌향" 벡터\\(\\mathbf{l}\\)을 추출한다. 그런 다음 고정된 프레임 수\\(T=100\\)에서 초기 위치\\(\\mathbf{p}+c(\\mathbf{u}+\\mathbf{l})\\)에서 목표 위치\\(\\mathbf{p}\\)으로 카메라를 변환한다:\n' +
      '\n' +
      '\\[\\mathbf{p}(t)=\\mathbf{p}+\\lambda c(\\mathbf{u}+\\mathbf{l}} \\tag{7}\\)\n' +
      '\n' +
      '여기서 \\(\\lambda=1-\\frac{t}{T-1}\\) 및 시간 단계 \\(t\\)는 0 내지 \\(T-1\\)의 범위이다. 요인\\(c=0.05\\)은 초기 위치가 목표 위치로부터 얼마나 멀리 떨어져 있는지를 제어한다. 카메라의 회전은 전체 궤적 동안 고정된 상태로 유지됩니다. 품질 메트릭을 계산하기 위해 사용되는 프레임은 카메라가 목표 위치에 도착했을 때 캡처된다.\n' +
      '\n' +
      '## 부록 E 프레임 비율 벤치마킹\n' +
      '\n' +
      '우리는 Duckworth et al.(2023)의 평가 프로토콜을 따르고, 장면의 테스트 세트 카메라 포즈에 대한 평균 프레임 레이트를 측정한다. Duckworth et al.(2023)에 이어서, 우리는 각각의 카메라 포즈 100 프레임에 대해 렌더링하고 평균 프레임 시간을 계산한다. SMERF와 유사하게, 브라우저 기반 뷰어 애플리케이션들(BakedSDF(Tariw et al., 2023), MERF(Reiser et al., 2023)) 및 우리의 방법에 대해, 디스플레이를 위해 스케줄링하기 전에 각각의 프레임 \\(k\\)을 드로잉함으로써 브라우저의 프레임 레이트 제한을 초과하는 프레임 레이트들을 측정한다. SMERF에 이어서, 우리는 프레임 레이트가 60 FPS 이하가 되도록 \\(k\\)에 대한 초기 값을 선택하는 \\(k\\)의 세 가지 다른 값으로 프레임 시간을 측정한다. 그런 다음 \\(k\\)에 대해 더 큰 값으로 두 가지 추가 측정을 수행한다. 마지막으로 각 프레임에 대해 \\(k\\) 이상의 최소 평균 프레임 시간을 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:11]\n' +
      '\n' +
      '그림 7: **기하학적 절제에 대한 정성적인 결과.** 도전적인 얇은 구조를 정확하게 재구성하는 우리 모델의 능력은 (a) 픽셀당 다중 광선을 주조하는 것, (b) 엔트로피 손실을 시행하는 것, (c) 고해상도 그리드를 사용하는 것에 크게 의존한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>