<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'AnimateLCM: Decoupled Consistency Learning을 이용한 개인화 확산 모델과 어댑터의 애니메이션 가속화\n' +
      '\n' +
      'Fu-Yun Wang\n' +
      '\n' +
      'MMLab, CUHK\n' +
      '\n' +
      'Avolution AI\n' +
      '\n' +
      '상하이 AI 연구소\n' +
      '\n' +
      'SenseTime Research\n' +
      '\n' +
      '{fywang@link, hsli@ee}.cuhk.edu.hk\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '비디오 확산 모델은 일관성이 있고 충실도가 높은 비디오를 생산하는 능력으로 인해 점점 더 주목을 받고 있다. 그러나 반복적인 잡음 제거 과정은 계산 집약적이고 시간이 많이 소요되므로 응용 프로그램을 제한한다. 최소 단계로 샘플링을 가속화하기 위해 사전 훈련된 이미지 확산 모델을 증류하는 일관성 모델(CM)과 조건부 이미지 생성에 대한 성공적인 확장 잠재 일관성 모델(LCM)에서 영감을 받아 최소 단계 내에서 고충실도 비디오 생성이 가능한 AnimateLCM을 제안한다. 원시 비디오 데이터셋에 대한 일관성 학습을 직접 수행하는 대신 이미지 생성 전과 동작 생성 전과 증류를 분리하여 학습 효율을 높이고 생성 시각적 품질을 향상시키는 분리 일관성 학습 전략을 제안한다. 또한, 안정적인 확산 커뮤니티에서 플러그 앤 플레이 어댑터의 조합이 다양한 기능(예: 제어 가능한 생성을 위한 ControlNet)을 달성할 수 있도록 한다. 우리는 샘플링 속도에 해를 끼치지 않고 기존의 어댑터를 증류된 텍스트 조건 비디오 일관성 모델에 적용하거나 처음부터 어댑터를 훈련하는 효율적인 전략을 제안한다. 본 논문에서 제안한 방법을 이미지 조건 비디오 생성과 레이아웃 조건 비디오 생성에서 검증한 결과, 모두 우수한 성능을 보였다. 실험 결과는 제안된 방법의 유효성을 검증한다. 코드 및 가중치\n' +
      '\n' +
      '공개될 것이다. 보다 자세한 내용은 [https://github.com/G-U-N/AnimateLCM._](https://github.com/G-U-N/AnimateLCM._)에서 확인할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델[3, 10, 11, 25]은 이미지 생성[11, 25] 및 비디오 생성[4, 12, 20, 37]에서 전례 없는 성공을 달성했다. 확산 모델에 의해 달성된 고품질 생성은 고차원 가우시안 잡음을 점진적으로 실제 데이터로 변환하는 반복적인 잡음 제거 프로세스에 의존한다. 이미지 생성을 위한 가장 대표적인 모델 중 하나는 가변 오토인코더(Variational Autoencoder, VAE)에 의존하여 실제 이미지와 다운 샘플링된 잠재 특징 사이의 매핑을 구축하여 생성 비용을 줄이고 텍스트 조건 이미지 생성을 달성하기 위한 교차 주의 메커니즘이다. 안정적인 확산에 기반하여, 더 혁신적인 기능들을 달성하기 위해 많은 플러그 앤 플레이 어댑터들(예를 들어, ControlNet)[38]이 개발되고 결합된다.\n' +
      '\n' +
      '그러나 반복 샘플링의 특성은 다른 생성 모델(예: GAN)[5, 36]보다 훨씬 느린 확산 모델의 느린 생성 및 높은 계산 부담을 초래한다. 최근, 일관성 모델[33] (CM)은 생성 프로세스의 고속화를 목표로 하는 유망한 대안으로 제안되었다. 사전 훈련된 확산 모델에 의해 유도된 PF-ODE 궤적에 대해 자기 일관성[33]을 유지하는 일관성 매핑을 학습함으로써 CM은 매우 적은 수의 단계로 고품질 이미지 생성을 허용하여 계산 집약적인 반복이 필요하지 않다. SD를 기반으로 구축된 잠재 일관성 모델 [19] (LCM)은 다양한 기능(예: 실시간 이미지 대 이미지 변환)을 달성하기 위해 기존 어댑터와 함께 커뮤니티에서 널리 채택된 웹-UI에 통합될 수 있다. 대조적으로, 비디오 확산 모델들이 많은 고무적인 진보[4, 7, 21, 28, 29]를 달성하고 있지만, 비디오 샘플링의 가속화는 비디오 생성의 높은 계산 비용으로 인해 여전히 훨씬 더 시급하다.\n' +
      '\n' +
      '본 논문에서는 최소의 스텝 수를 갖는 고충실도 비디오 생성을 위한 AnimateLCM을 제안한다. LCM에 따라 역확산 과정을 분류기 없는 유도(CFG) [9] 증강 확률 흐름 ODE (PF-ODE) [15, 19, 32]를 해결하는 것으로 취급하고, 잠재 공간에서 이러한 ODE의 해를 직접 예측하도록 모델을 훈련한다. 그러나 저화질 문제를 겪고 높은 훈련 자원을 필요로 하는 원시 비디오 데이터에 대해 직접 일관성 학습을 수행하는 대신, 이미지 생성 전과 동작 생성 전과의 일관성 증류를 분리하는 분리 일관성 학습 전략을 제안한다. 도 1에 도시된 바와 같다. 먼저, 이미지 베이스 확산 모델을 이미지 일관성 모델에 적응시키기 위해 일관성 증류를 수행하는데, 이는 매우 효율적으로 적응될 수 있다. 그런 다음 3D 비디오 기능을 수용하기 위해 이미지 확산 모델과 이미지 일관성 모델 모두에 3D 인플레이션을 수행한다. 결국, 비디오 데이터에 대한 일관성 증류를 수행하여 궁극적인 비디오 일관성 모델을 얻는다. 인플레이션 프로세스에 의한 잠재적인 피쳐 손상을 완화하기 위해 특별히 설계된 초기화 전략을 추가로 제안한다. 우리는 이 과정이 훈련 효율을 높일 뿐만 아니라 최종 생성 품질을 향상시킨다는 것을 경험적으로 보여준다.\n' +
      '\n' +
      '또한, SD에 AnimateLCM을 구축하고, 따라서 우리는 그림 1의 오른쪽과 같이 훈련된 비디오 일관성 모델의 공간 가중치를 공개적으로 이용 가능한 개인화된 이미지 확산 가중치로 대체함으로써 혁신적인 생성 결과를 얻을 수 있다. 또한, 커뮤니티 내의 대부분의 어댑터가 훈련된 비디오 일관성 모델과 직접 통합될 수 있지만, 세부 사항에 대한 통제를 잃거나 결과에 깜박임을 유발할 수 있음을 보여준다. 커뮤니티의 기존 어댑터에 더 잘 어울리거나 비디오 일관성 모델로 처음부터 특정 어댑터를 훈련하기 위해 특정 교사 모델을 요구하지 않고 어댑터를 "가속"할 수 있는 효과적인 전략을 제안한다. 전략의 효율성을 보이기 위해, 우리는 비디오 일관성 모델을 사용하여 이미지 인코더를 처음부터 훈련하고, 최소한의 단계로 고 충실도 이미지 대 비디오 생성을 추가로 달성한다. 기존의 레이아웃 조건 어댑터를 바닐라 이미지 확산 모델로 미리 훈련하여 최소한의 단계로 더 나은 호환성과 더 나은 제어 가능한 비디오 생성을 달성한다. 우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 우리는 빠르고 높은 충실도의 비디오 생성을 허용하는 AniamteLCM을 제안한다. 본 논문에서는 영상 생성 전과 동작 전과를 분리하여 보다 우수한 학습 효율과 생성 품질을 얻을 수 있는 디커플드 증류 전략을 제안한다.\n' +
      '* 안정적인 확산 가중치에 기반하여 다양한 스타일에서 고화질의 영상 생성을 달성하기 위해 개인화된 영상 확산 가중치와 좋은 호환성을 보인다. 또한 기존의 어댑터를 일관성 모델에 더 잘 맞게 조정하거나 샘플링 속도에 해를 끼치지 않고 처음부터 특정 어댑터를 훈련하는 효율적인 전략을 제안한다. 이를 바탕으로 고충실도 영상 조건 영상 생성과 레이아웃 조건 영상 생성을 추가적으로 달성한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '이 섹션에서는 관련 확산 모델과 샘플링 속도 전략에 대해 논의한다.\n' +
      '\n' +
      '### Diffusion Models\n' +
      '\n' +
      '점수 기반 생성 모델[15, 32]로도 알려진 확산 모델[10, 11, 25]은 이미지 생성에서 큰 성공을 거두었다. 반복 샘플링 진행은 잡음-훼손된 데이터를 점진적으로 잡음제거하기 위해 점수 방향에 의해 안내된다. 현재의 성공적인 일반화된 비디오 확산 모델[4, 28, 29, 37]은 통상적으로 미리 트레이닝된 이미지 확산 모델[25]에 응답하고, 이들에 시간 계층을 추가함으로써 트레이닝된다. 그들 대부분은 이미지-비디오 조인트 튜닝으로 훈련되거나[3, 29] 단순히 공간 가중치들을 동결시킨다[4].\n' +
      '\n' +
      '### Sampling Acceleration\n' +
      '\n' +
      '확산 모델에서 느린 생성 문제를 해결하기 위해. 샘플링 속도를 개선하기 위한 초기 작업은 개선된 ODE 솔버[15, 17, 18, 39]에 초점을 맞춘다. 증류 기반 가속 방법은 정제된 스케줄러 또는 아키텍처로 원래의 확산 가중치를 튜닝함으로써 보다 유망한 가속 속도를 보여준다[22, 27]. 일관성 모델[33]은 자기 일치성 속성을 강제하여 훈련된 새로운 버전 모델이다. 잠재적 일관성 모델[19]은 잠재 공간에서의 조건부 이미지 생성에 대한 안정적인 확산에 아이디어를 성공적으로 적응시킨다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '이 섹션에서는 확산 모델과 일관성 모델에 대한 고수준의 검토를 제공한다. 우리의 모델은 DDPM의 확장된 버전인 안정적인 확산을 기반으로 하기 때문에 [10] 다음 논의에서 관련 표기법을 적용한다. 즉, 이산 순방향 확산 과정을 연속 시간 분산 보존 SDE로 취급한다[15]. 논의는 SDE의 정의의 다른 형식으로 더 일반화될 수 있다.\n' +
      '\n' +
      '확산 모델.** DDPM에서 학습 데이터 포인트 \\(\\mathbf{x}_{0}\\sim p_{\\mathrm{data}(\\mathbf{x})\\)는 섭동 커널 \\(p(\\mathbf{x}_{i}\\mid\\mathbf{x}_{i-1})=\\mathcal{N}(\\mathbf{x}_{i};\\sqrt{1-\\beta_{i}}\\mathbf{x}_{i-1},\\beta_{i}\\mathbf{I})\\), \\(i=1,2,\\ldots,N\\)을 갖는 이산 마르코프 체인에 의해 점진적으로 섭동된다. 이러한 방식으로, 상이한 타임스텝에서의 잡음 데이터의 분포는 분포\\(p_{i}(\\mathbf{x}_{i}\\mid\\mathbf{x}_{0})=(\\mathbf{x}_{i};\\sqrt{\\alpha_{i}}\\mathbf{x}_{0},(1-\\alpha_{i})\\mathbf{I})\\), 여기서 \\(\\alpha_{i}:=\\Pi_{j=1}^{i}(1-\\beta_{i})\\). \\(N\\to\\infty\\)로서, 이산 마르코프 체인은 다음의 SDE에 수렴하고,\n' +
      '\n' +
      '\\mathrm{d}\\mathbf{x}=\\mathbf{f}(\\mathbf{x},t)\\mathrm{d}t+g(t)\\mathrm{d}\\mathbf{w}, \\tag{1}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{f}(\\mathbf{x},t)=-\\frac{1}{2}\\mathbf{x}\\beta(t)\\) 및 \\(g(t)=\\sqrt{\\beta(t)\\이다.\n' +
      '\n' +
      '이 SDE의 두드러진 특성은 확률흐름(PF) ODE[32]라고 불리는 역시간 상미분 방정식(ODE)의 존재이며, 시간 t에서의 해 궤적이 여전히 잡음 분포\\(p_{t}(\\mathbf{x})\\을 따르는 지점이다.\n' +
      '\n' +
      '\\mathbf{x}=\\left[\\mathbf{f}(\\mathbf{x},t)-g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}\\right]\\mathrm{d}t. \\tag{2}\\t.\n' +
      '\n' +
      'DDPM에서는 잡음 예측 신경망(\\mathbf{\\epsilon}_{\\theta}(\\mathbff{x}_{t},t)\\)이 현재 데이터 포인트에서 잡음을 제거하도록 학습되는데, 그 방향은 스코어 함수의 반대 방향인\\(\\nabla_{\\mathbff{x}_{t}\\log p_{t}(\\mathbff{x}_{t})\\approx-\\frac{1}\\approx-\\frac{1}{\\sqrt{1-\\alpha_{i}}\\mathbf{\\epsilon}_{\\theta}(\\mathbff{x}_{t},t)\\macrt{1-\\alpha_{i}}\\mathbf{\\epsilon}_{\\theta}(\\mathbff{x}_{t},t)\\macrt{1-\\alpha_{i}}\\mathbf{x}_{t}(\\mathbff{x}_{t},t)\n' +
      '\n' +
      '일관성 모델.** 위에서 논의된 PF ODE의 위의 해 궤적\\(\\{\\mathbf{x}_{t}\\}_{t\\in[\\epsilon,T]}\\)이 주어지면, 일관성 모델은 PF ODE의 해를 직접 예측하기 위해 동일한 궤적에 있는 모든 데이터 포인트를 허용하려고 한다[33]. 그러나, 직접 해를 목표 예측으로 설정하는 대신, 자기 일관성 속성을 따르도록 궤적의 점들을 강제하는 것을 혁신적으로 제안한다. 즉, 동일한 PF ODE 궤적에 속하는 \\((\\mathbf{x}_{t},t)\\)의 임의의 쌍, 즉 모든 \\(t,t^{\\prime}\\in[\\epsilon,T]\\에 대해 \\(\\mathbf{f}(\\mathbf{x}_{t},t)=\\mathbf{f}(\\mathbf{x}_{t^{\\prime},t^{\\prime})\\)에 대해 출력이 동일하게 강제된다.\n' +
      '\n' +
      '일관함수\\(\\mathbf{f}(\\cdot,\\cdot)\\)에 대한 경계조건[33]을 보장하기 위해 우리는 \\(\\mathbf{f}(\\mathbf{x}_{\\epsilon},\\epsilon)=\\mathbf{x}_{\\epsilon}\\, 즉 \\(\\mathbf{f}(\\cdot,\\epsilon)\\)이 아이덴티티 함수이다. 일관성 모델들은 전형적으로 스킵 연결, 즉,\n' +
      '\n' +
      '\\[\\mathbf{f}_{\\theta}(\\mathbf{x}_{t},t)=c_{\\text{skip}(t)\\mathbf{x}_{t}+c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x},t), \\tag{3}\\t)\n' +
      '\n' +
      '여기서 \\(c_{\\text{skip}}(t)\\) 및 \\(c_{\\text{out}(t)\\)은 \\(c_{\\text{skip}}(\\epsilon)=1\\) 및 \\(c_{out}(\\epsilon)=0\\인 미분 함수이다.\n' +
      '\n' +
      '결국, 자기일관성을 강화하기 위해 지수이동평균(EMA)으로 갱신된 목표모델 \\(\\theta^{-}\\) 즉, \\(\\theta^{-}=\\mu\\theta^{-}+(1-\\mu)\\theta\\)을 일관성 증류에 추가로 유지한다.\n' +
      '\n' +
      '\\mathcal{L}(\\theta,\\theta^{-};\\Phi)=\\mathbbbb{E}_{\\mathbf{x},t}\\left[d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1},t_{n+1}}),\\mathbf{f}_{\\theta^{-}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},t_{n})\\right)\\right], \\tag{4}\\tag{4}\\mathbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n' +
      '\n' +
      '여기서 \\(d(\\cdot,\\cdot)\\)는 거리 측정 함수이고, \\(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}\\)는 \\(\\phi\\)로 파라미터화된 사전 훈련된 확산 모델을 갖는 수치 ODE 솔버의 하나의 이산화 단계에 의해 획득되고,\n' +
      '\n' +
      '\\[\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}=\\mathbff{x}_{t_{n+1}}+(t_{n}-t_{n+1})\\Phi(\\mathbff{x}_{t_{n+1}},t_{n+1};\\phi. \\tag{5}\\tag{5}}}\n' +
      '\n' +
      '## 4 Methodology\n' +
      '\n' +
      'AnimateLCM은 안정적인 확산 기반 비디오 모델을 자기 일치 특성을 따르도록 길들여 최소한의 단계 내에서 고충실도 비디오 생성을 달성한다. 도. 도 2는 효과적인 일관성 학습을 위한 디커플링된 일관성 학습 전략과 효과적인 어댑터 훈련/피네튜닝을 위한 무교사 적응으로 구성된 AnimateLCM의 전반적인 훈련 패러다임을 나타낸다. 먼저 Sec. 4.1에서 잠재 확산 모델의 기본 적응을 잠재 일관성 모델에 적용한 것을 소개한다. 이후, Sec. 4.2에서 이미지 생성 전위와 모션 전위의 증류를 분리하여 비디오 데이터에 대한 단순 일관성 학습보다 더 나은 학습 효율과 생성 품질을 달성하는 _decoupled consistency learning_ 전략을 소개한다. Sec. 4.3에서는 샘플링 속도를 해치지 않고 어댑터에 더 잘 어울리거나 처음부터 훈련하여 빠르고 고품질 이미지 조건 생성 및 레이아웃 조건 생성을 달성하는 _교사 없는 적응_ 전략을 설명한다.\n' +
      '\n' +
      'DM에서 CM까지\n' +
      '\n' +
      '여기서는 안정 확산 모델(DM)을 일관성 모델(CM)에 적응시키는 것을 소개하고, 기본적으로 잠재 일관성 모델(LCM)의 설계에 따른다[19].\n' +
      '\n' +
      '**\\(\\mathbf{\\epsilon}\\)-예측에서\\(\\mathbf{x}_{0}\\)-예측까지 안정확산모델은 일반적으로 주어진 샘플에 부가되는 잡음을 예측하는데, 이는 \\(\\mathbf{\\epsilon}\\)-예측 모델이다. 그러나 일관성 모델은 PF-ODE 궤적의 해 \\(\\mathbf{x}_{0}\\)를 직접 예측하는 것을 목표로 한다. 우리는 \\(\\mathbf{x}_{t}\\sim\\mathcal{N}(\\sqrt{\\alpha_{i}\\mathbf{x}_{0},1-\\alpha_{i}\\mathbf{I})\\\\(\\mathbf{f}_{\\theta}\\)을 다음과 같이 파라메트리화할 수 있다.\n' +
      '\n' +
      '\\mathbf{f}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)=c_{\\text{skip}(t)\\mathbf{x}_{t}+c_{\\text{out}(t)\\mathbff{x}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t}\\,, \\tag{6}\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{c}\\)는 텍스트 조건 임베딩이고, \\(\\mathbf{x}_{\\theta}=\\left(\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\alpha_{t}\\mathbf{\\epsilon}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)}{\\sqrt{\\alpha_{t}\\right)\\.\n' +
      '\n' +
      '**Classifier-free guidance augmented ODE solver.** parameter \\(\\phi\\)를 갖는 안정적인 확산 모델에서 분류기-free guidance는 고품질 생성을 위한 필수적인 전략, 즉,\n' +
      '\n' +
      '\\mathbf{\\epsilon}_{\\phi}(\\mathbf{\\epsilon}_{t},\\mathbf{c},t,w)=(1+w)\\mathbff{\\epsilon}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t)-w\\mathbf{\\epsilon}_{\\phi}(\\mathbf{x}_{t},\\varnothing,t). \\tag{7}\\ta}(\\mathbf{x}_{t},\\mathbf{c},t)-w\\mathbf{\\epsilon}_{\\phi}(\\mathbf{x}_{t},\\varnothing,t).\n' +
      '\n' +
      'PF ODE 솔버를 정방향 SDE 프로세스와 정렬하는 \\(\\Phi(\\mathbf{x}_{t_{a}},\\mathbf{c},t_{n},t_{n+1};\\phi)\\)로 표시한다고 가정하면, 실제로 용액 궤적을 풀기 위한 안정적인 확산의 실제 PF ODE 솔버는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\Phi(\\Phi_{w}(\\mathbf{x}_{t_{n}},\\mathbf{c},t_{n},t_{n+1};\\phi)=(1+w)\\Phi(\\mathbf{x}_{t_{n}},\\mathbf{c},t_{n},t_{n+1};\\phi\\]\\[-w\\Phi(\\mathbf{x}_{t_{n}},\\varnothing,t_{n},t_{n+1};\\phi)\\tag{8}\\fi(\\mathbf{x}_{n}},t_{n},t_{n+1};\\fi)=(1+w)\\Phi(\\mathbf{x}_{n}},t_{n},t_{n+1};\\fi)=(1+w)\\fi(\\mathbf{x}_{n}},t\n' +
      '\n' +
      '이러한 방법으로, 우리는 동일한 궤적으로 인접한 쌍을 샘플링하기 위해 분류기 없는 유도 증강 ODE 솔버를 적용한다.\n' +
      '\n' +
      '**sample sparse timesteps from dense timesteps.** 이전 작업 [19, 22]는 궤적 \\(N\\)의 이산점 수가 훈련 효율성과 품질에 큰 영향을 미친다는 것을 보여준다. 전형적으로, 더 작은 N은 트레이닝을 가속화하고, 반대로, 더 큰 N은 트레이닝에서 더 적은 바이어스를 도입한다. 안정 확산 모델은 일반적으로 상대적으로 너무 큰 1000개의 타임스테프를 보유한다. 훈련의 효율성을 높이기 위해, 우리는 훈련을 위해 전체 1000개의 타임스텝에서 50개의 타임스텝을 균일하게 샘플링한다.\n' +
      '\n' +
      '연립일관성 학습\n' +
      '\n' +
      '정합성 증류 공정의 경우 정합성 모델의 최종 생성 품질이 학습에 사용된 데이터에 크게 영향을 받는다는 것이 중요한 관찰이다. 그러나, 현재 공개된 데이터 세트는 일반적으로 저해상도, 워터마크, 및 지나치게 간략하거나 모호한 캡션을 포함하는 다양한 문제들을 겪는다. 게다가, 고해상도 비디오에 대한 직접적인 교육은 자원이 많이 소모되며 대부분의 연구자들에게는 저렴하지 않다. 현재 사용 가능한 필터링된 고품질 이미지 데이터 세트가 많다는 점을 고려하여 이미지 생성 사전 및 모션 사전의 증류를 분리할 것을 제안한다. 구체적으로, 우리는 먼저 안정적인 확산 모델을 높은 해상도의 필터링된 고품질 이미지 텍스트 데이터 세트에서 이미지 일관성 모델로 증류한다. 안정 확산의 가중치를 동결하기 위해, 우리는 안정 확산 모델의 층에서 광 LoRA[13] 가중치를 훈련한다. LoRA 가중치는 조정되고, 다용도 가속 모듈로 작동하며, 안정적인 확산 커뮤니티에서 다른 개인화된 모델과 잘 호환되는 것으로 나타났다. 추론을 위해,\n' +
      '\n' +
      '그림 2: AnimateLCM의 높은 수준의 개요. 왼쪽: 일관성 속성을 따르도록 비디오 생성 모델을 길들이기 위한 전반적인 훈련 패러다임. 상위 우측: 교사 없는 적응 전략은 cfg-증강된 오드 솔버를 1단계 MCMC 근사치로 대체한다. 오른쪽 하단: 안정적인 확산 커뮤니티에서 이미지 대 비디오 어댑터를 처음부터 교육하거나 기존 어댑터를 길들이기 위한 설계.\n' +
      '\n' +
      'LoRA 가중치들은 추론 속도를 손상시키지 않고 원래의 가중치들과 병합된다.\n' +
      '\n' +
      '이미지 생성 수준에서 일관성 모델을 얻은 후 안정적인 확산의 가중치와 LoRA 가중치를 동결한다. 비디오 생성을 위한 일관성 모델을 학습하기 위해, 먼저 의사-3D 컨볼루션 커널(예를 들어, \\(3\\times 3\\))에 2D 컨볼루션 커널(예를 들어, \\(1\\times 3\\times 3\\))을 팽창시키고, 제로 초기화 및 블록 레벨 잔차 연결과 함께 추가적인 시간적 계층을 추가한다.\n' +
      '\n' +
      '\\[\\mathbf{z}^{\\prime}=T_{\\text{zero}}(\\mathbf{z})+\\mathbf{z}, \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(T\\)은 시간적 층을 나타내고, 0은 제로-초기화됨을 의미한다. 따라서, 첫 번째 훈련에서의 모델의 출력은 영향을 받지 않을 것이며, 이는 훈련을 가속화하기 위해 많이 검증되었다[29, 38]. 그런 다음 안정적인 확산 모델에서 확장된 오픈 소스 비디오 확산 모델의 지침에 따라 시간적 계층을 훈련한다.\n' +
      '\n' +
      '**초기화 전략** 그러나, 공간적 LoRA 가중치는 시간적 모델링을 고려하지 않고 샘플링을 가속화하도록 훈련되고 시간적 모듈은 바닐라 확산 방식으로 훈련되므로, 이들을 직접 결합하는 것은 훈련 초기에 표현 손상을 야기하는 경향이 있다. 이러한 방식으로, 어떻게 그것들을 더 적은 갈등과 효과적이고 효율적으로 조합할 것인가는 자명하지 않은 도전을 제기한다. 공간 LoRA 가중치로부터 일관성 전개를 차용할 수 있을 뿐만 아니라 직접 결합의 부정적인 영향을 완화할 수 있는 효과적인 초기화 전략을 경험적으로 발견한다. 구체적으로, 도 1에 도시된 바와 같다. 도 3을 참조하면, 일관성 훈련 초기에는 목표 일관성 모델에 삽입하지 않고 사전 학습된 공간 LoRA 가중치만을 온라인 일관성 모델에 삽입한다. 이러한 방식으로, 온라인 모델에 대한 학습 "강사"로서 작동하는 목표 일관성 모델은 온라인 모델을 손상시키는 손상된 예측을 생성하지 않을 것이다. 트레이닝 동안, LoRA 가중치는 지수 이동 평균(EMA)을 통해 목표 일관성 모델에 점진적으로 누적되고, 특정 반복 후에 원하는 가중치에 도달할 것이다. 우리는 이 전략이 훈련 효율성을 크게 향상시킨다는 것을 경험적으로 보여준다. 일반적으로, 우리는 손실을 다음과 같이 적용한다.\n' +
      '\n' +
      '\\mathcal{L}(\\theta,\\theta^{-};\\Phi)\\]\\[=\\mathbbb{E}_{\\mathbf{x},t}\\left[\\lambda_{n}d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},\\mathbf{x},t_{n+1}),\\mathbf{f}_{\\theta^{-}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi},\\mathbf{c},t_{n}\\right)\\right]\\\\tag{10}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\tt{n}\\t\n' +
      '\n' +
      '여기서 \\(\\lambda_{n}\\)는 서로 다른 타임스테프에 대한 가중치 함수이다. 더 작은 타임스텝(t_{n}\\)에서의 예측이 더 큰 타임스텝(t_{n+1}\\)의 타겟으로 작용한다는 점을 감안할 때, 우리는 더 작은 타임스텝에서의 학습에 더 많은 우선순위를 부여해야 한다. 따라서, \\(\\lambda\\)가 성장함에 따라 \\(\\lambda\\)을 감쇠시키고 \\(\\lambda_{n}=(1-\\delta\\frac{n}{N})^{\\gamma}\\)을 설정하며, 여기서 \\(\\delta\\)과 \\(\\gamma\\)은 감쇠속도를 조절하는 스케일링 인자이다. Huber loss는 hyper-parameter \\(c\\)를 갖는 거리 메트릭 \\(d(\\mathbf{x},\\mathbf{y})=\\sqrt{\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}+c^{2}}-c\\)으로 적용되며, 이는 더 부드러운 기울기를 생성하고 이상치에 대해 더 나은 견고성을 갖는다.\n' +
      '\n' +
      '### Teacher-Free Adaptation\n' +
      '\n' +
      '플러그 앤 플레이 어댑터는 안정적인 확산 커뮤니티에서 중요한 역할을 한다. 그러나 이미지 확산 모델로 훈련된 많은 어댑터가 여전히 일부 확장되어 작동하지만 일반적으로 그림 6과 같이 세부적으로 통제력을 상실한다는 것을 관찰한다. 이 섹션에서는 더 나은 호환성을 위해 어댑터를 처음부터 훈련하거나 기존 어댑터를 수용하는 간단하면서도 효과적인 전략을 제안하며, 이는 교사 모델을 요구하지 않고 최소한의 단계로 이미지 대 비디오 생성 및 제어 가능한 비디오 생성을 달성한다.\n' +
      '\n' +
      '비디오 일관성 모델의 학습은 일반적으로 사전 학습된 비디오 확산 모델\\(\\epsilon_{\\theta}(\\mathbf{x},\\mathbf{c},t)\\)을 사용하여 점수\\(\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})를 근사화한다. 송과 다리왈[31]의 영감을 받아 악보를 편견 없이 추정할 수 있었다.\n' +
      '\n' +
      '=-\\mathbbb{E}_{\\mathbf{x}_{t}(\\nabla_{\\mathbf{x}_{t}}\\log p_{t}(\\frac{\\mathbf{x}_{t}-\\sqrt{\\alpha_{t}}\\mathbf{x}_{t}}{1-\\alpha_{t}}\\bigg{|}\\mathbf{x}_{t}\\right], \\tag{11}\\tqrt\\left[\\frac{\\mathbf{x}_{t}\\tqrt{\\alpha_{t}\\bigg{|}\\mathbf{x}_{t}\\tq}\\tq}\\tq}\\tq}\\tq}\n' +
      '\n' +
      '를 포함하는, 안정한 확산 모델. 이러한 방법으로 주어진 \\(\\mathbf{x}\\)에 대해 우리는 \\(\\frac{\\mathbf{x}_{t}-\\sqrt{\\alpha_{t}\\mathbf{x}{1-\\alpha_{t}\\)으로 \\(\\mathbf{x}_{t}\\)에서의 점수를 간단히 근사할 수 있다. 실제 점수에 대한 1단계 MCMC 근사치이지만, 실제 비디오 생성에서 잘 작동함을 실험을 통해 알 수 있다. 또한, 서로 다른 시간 간격에서 동일한 섭동 잡음을 공유하는 두 개의 순방향 SDE를 사용하여 실제로 추정이 수행될 수 있음을 보여준다. 소음을 \\(\\mathbf{\\epsilon}_{0}\\)으로 표기하고 \\(\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}\\mathbf{x}+\\sqrt{1-\\alpha_{t}\\mathbf{\\epsilon}_{0}\\)으로 가정하면, \\(\\frac{\\mathbff{x}_{t}-\\sqrt{\\alpha_{t}\\mathbf{x}{1-\\alpha_{t}\\)의 점수근사를 \\(\\mathbf{\\epsilon}_{0}\\)으로 계산하면 잡음 예측도 \\(\\mathbf{\\epsilon}_{0}\\(\\mathbf{\\epsilon}_{0}\\)임을 알 수 있다. 널리 사용되는 ODE 솔버에 이어\n' +
      '\n' +
      '그림 3: 초기화 전략. 우리는 처음에 온라인 모델에 공간 LoRA 가중치만 삽입하고 EMA를 통해 LoRA 가중치를 점차적으로 대상 모델에 전파한다.\n' +
      '\n' +
      'DDIM[30] 우리는 볼 수 있었다.\n' +
      '\n' +
      'x}{t-1}&=\\sqrt{\\alpha_{t-1}}\\frac(\\frac{\\mathbf{x}_{t}-\\sqrt{1-\\alpha_{t}}\\sqrt{1-\\alpha_{t-1}}\\sqrt{x}+\\sqrt{t-1}\\mathbf{x}+\\sqrt{t-1}\\mathbf{x}+\\sqrt{t-1}\\sqrt{t-1}\\mathbf{x}+\\sqrt{t-1}\\sqrt{t-1}\\mathbf{x}+\\sqrt{t-1}\\sqrt{t-1}\\mathbf{x}\\sqrt{t-1}\\sqrt{t-1}\\sqrt{t-1}\\mathbf{x}\\sqrt{t-1}\\sqr\n' +
      '\n' +
      '따라서 위의 전략에 따라 일정 정도의 편견을 견디는 교사 없는 일관성 학습을 달성할 수 있었다.\n' +
      '\n' +
      '**이미지-투-비디오.** 이미지-투-비디오 생성을 달성하기 위해, 가장 중요한 부분은 입력 이미지들을 전처리하여 이미지 컨텍스트들을 추출하는 방법 및 이미지 컨텍스트들을 일관성 모델들에 통합하는 방법이다.\n' +
      '\n' +
      '영상 전처리를 위해 미리 학습된 클립 영상 인코더, 픽셀 공간에서의 라이트 다운샘플 블록, 잠재 공간에서의 라이트 블록을 비교한다. 우리의 관찰은 클립 이미지 인코더가 입력 이미지의 세부 사항을 무시하는 경향이 있으며 원래 문자의 세부 기능에 맞지 않는 프레임을 생성할 수 있다는 것이다. 픽셀 공간의 다운샘플 블록의 경우, 생성된 프레임은 일반적으로 모양과 윤곽선을 잘 따르지만 색상 일관성에서는 실패한다는 것을 발견한다. 이미지들이 VAE 인코더에 의해 처음 인코딩되는 잠재 공간의 광 블록들은 부분적으로 잠재 공간에서 수행되는 생성 프로세스로 인해 최상의 결과들을 달성한다. 또한, U-Net 블록은 서로 다른 블록에서 다양한 공간 차원을 가지므로, U-Net과 더 잘 정렬하기 위해 조명 블록의 특징을 다운샘플링한다.\n' +
      '\n' +
      '이미지 컨텍스트를 통합하기 위해 일반적인 방법은 교차 주의 또는 마스킹 모델링을 시도한다. 그러나, 교차 주의는 원 영상의 공간 정보를 손상시켜 입력 영상의 재생에 실패하고 느린 수렴을 직면하는 경향이 있음을 알 수 있다. 마스킹 모델링을 위해 이전 연구들은 대상 영상과 동일한 조건을 구성하고 첫 번째 프레임은 주어진 영상으로, 나머지 프레임은 0으로 설정하였다. 그러나, 이 방법은 일반적으로 다음 프레임에서 원본 이미지의 동일성을 유지하지 못하며, 이는 잠재적으로 시간적 계층이 첫 번째 프레임의 정보를 다른 프레임에 정렬하기에 적절하지 않을 수 있음을 나타낸다. 우리는 매우 간단하면서도 효과적인 방법을 통해 이미지 대 비디오를 달성합니다. 입력 이미지를 통합하기 위해 먼저 이미지를 잠재 공간에 인코딩한 다음 시간적 차원에서 잠재된 이미지를 반복하여 비디오 생성 프레임 수를 정렬한다. 그런 다음, 추출된 특징들은 다른 해상도로 특징들을 추출하기 위해 라이트 블록들에 공급되고, 그 다음 대응하는 특징들은 U-Net 내의 대응하는 레이어들에 직접 추가된다. 생성 결과를 더 정제하기 위해 VAE 디코더로 디코딩한 후 생성된 비디오를 HSV 공간에서 입력 이미지와 추가로 정렬한다.\n' +
      '\n' +
      '**제어 가능한 비디오 생성.** T2I-어댑터 [23], ControlNet [38] 등을 포함하는 레이아웃 제어 어댑터가 놀랍게도 발견되었다. 이미지 확산 모델에 대한 훈련은 제어 가능한 비디오 생성을 달성하기 위해 비디오 일관성 모델과 직접 통합될 수 있다. 그러나, 우리가 청구하고 그림과 같이. 6, 그들은 세부 사항에 대한 통제력을 잃거나 결과를 깜박이게 할 수 있다. 운 좋게도 기존 어댑터에 LoRA 레이어를 튜닝하여 더 나은 호환성을 얻을 수 있는 간격을 효율적으로 좁힐 수 있음을 확인했다. 그림 6에서 보는 바와 같이, 무교사 적응 과정을 거친 후, 우리는 분명히 우수한 통제 안정성과 시각적 품질을 달성한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**구현 상세.** 대부분의 실험을 위한 기본 모델로 안정적인 확산 v1-5를 적용한다. 훈련에 DDIM ODE 솔버를 적용합니다. 잠재 일관성 모델에 따라 훈련을 위해 50개의 타임스테프를 균일하게 샘플링한다. 교사 비디오 확산 모델로서 개방 소스 모션 가중치[6]를 사용하여 안정적인 확산 v1-5를 적용한다. 제어 가능한 비디오 생성을 제외한 모든 실험은 증강 또는 추가 데이터 없이 공개적으로 사용 가능한 WebVid-2M [2] 데이터 세트에서 수행된다. 제어 가능한 비디오 생성을 위해 우리는 BLIP[16]로 캡션된 간략 텍스트 프롬프트와 함께 틱톡 데이터 세트[14]에서 모델을 훈련한다.\n' +
      '\n' +
      '**Benchmarks.** 우리의 접근법을 평가하기 위해, 우리는 검증을 위해 액션 인식 작업을 위해 큐레이션된 분류된 비디오 데이터 세트인 널리 사용되는 UCF-101 [34]를 활용하여 이전 작업을 따른다. 우리는 GPT-4 [1]에 각 범주에 대해 매우 간단한 캡션을 생성하도록 요청한다. 각 카테고리별로 16 프레임 해상도의 \\(512\\times 512\\)의 \\(24\\times 101\\) 비디오를 생성하여 총합으로 \\(24\\times 101\\) 비디오를 생성한다. 우리는 검증 메트릭으로 FVD[35]와 CLIP-SIM[8]을 적용하여 이전 비디오 생성 작업을 따른다. FVD의 경우 UCF-101 데이터셋에서 무작위로 \\(2048\\)개의 비디오와 생성된 \\(24\\times 101\\)개의 비디오를 각각 선택한다. CLIPSIM의 경우, 우리는 CLIP ViT-H/14 LAION-2B[24]에 의존하여 짧은 캡션의 유사성과 비디오의 모든 프레임의 평균값을 계산한다. 비디오 생성 가속을 위한 선구적인 작업으로 테스트 및 비교를 위해 사용할 수 있는 개방형 가중치가 없다. 따라서, 우리는 DDIM[30]과 DPM-Solver++[18]을 사용하여 AnimateLCM과 교사 모델을 비교한다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '도. 도 7은 텍스트-비디오 생성, 이미지-비디오 생성, 제어 가능한 비디오 생성에서 본 방법의 4단계 생성 결과를 실제, 2D 애니메이션, 3D 애니메이션의 스타일을 포함한 다양한 개인화된 스타일 모델로 보여준다. 모두 만족스러운 결과를 얻습니다. 또한 그림 7의 다양한 함수 평가(NFE)에서 생성 결과를 보여준다. 생성 결과는 우리의 방법이 유사한 스타일과 동작을 유지하면서 추론 단계가 다른 일관성 특성을 잘 따른다는 것을 보여준다. 우리는 2개의 추론 단계만으로 부드러운 움직임뿐만 아니라 좋은 시각적 품질을 보여준다. NFE가 증가함에 따라 그에 따라 생성 품질이 증가하여 25단계 및 50단계의 추론 단계로 교사 모델과의 경쟁적 성과를 달성한다.\n' +
      '\n' +
      '### Quantitative Experiments\n' +
      '\n' +
      '표 1은 AnimateLCM 및 강력한 기준선 방법 DDIM[30], 및 DPM++[17, 18]에 대한 정량적 메트릭 비교를 나타낸다. AnimateLCM은 특히 낮은 단계 체제(1\\(\\sim\\)4)에서 기준 방법을 크게 능가한다. 또한, AnimateLCM의 이러한 모든 메트릭은 분류기 없는 안내(CFG)를 필요로 하지 않고 평가된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{5}{c|}{FVD \\(\\downarrow\\)} & \\multicolumn{5}{c}{CLIPSIM \\(\\uparrow\\)} \\\\ \\cline{2-10}  & 1 Step & 2 Steps & 4 Steps & 8 Steps & 1 Step & 2 Steps & 4 Steps & 8 Steps \\\\ \\hline DDIM (Pretrained) [30] & 4940.83 & 3218.74 & 1944.82 & 1209.88 & 4.43 & 5.26 & 14.87 & 24.38 \\\\ DPM++ (Pretrained) [18] & 2731.37 & 2093.47 & 1043.82 & 932.43 & 10.48 & 18.04 & 26.82 & 29.50 \\\\ AnimateLCM & 1256.50 & 1081.26 & 925.71 & 910.34 & 22.16 & 25.99 & 28.89 & 30.03 \\\\ AnimateLCM (Realistic) & 1071.50 & 790.99 & 929.79 & 1081.72 & 25.41 & 29.39 & 30.62 & 30.71 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: UCF-101 상의 제로 샷 텍스트-투-비디오 생성. 우리의 접근법은 FVD 및 CLIPSIM 메트릭들 모두에서 최첨단 성능을 달성한다. AnimateLCM(Realistic)은 원래 SD의 공간 레이어 가중치를 사실적인 스타일 개인화 모델로 대체한다는 것을 의미한다.\n' +
      '\n' +
      '그림 4: 질적 생성 결과. 본 논문에서는 텍스트-비디오 생성, 이미지-비디오 생성, 제어 가능한 비디오 생성을 포함한 AnimateLCM의 생성 결과를 보인다.\n' +
      '\n' +
      '다른 기준선에 7.5 CFG 강도를 적용하여 추론 피크 메모리 비용과 추론 시간의 절반을 절약한다. 또한, 공간 가중치를 세대 다양성과 충실도 사이의 균형이 좋은 공개적으로 이용 가능한 개인화된 사실적 스타일 모델로 대체하여 AnimateLCM의 성능을 검증하여 성능을 더욱 향상시킨다. 애니암테LCM과 개인화된 확산 모델의 좋은 호환성을 보여준다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '**분리 일관성 학습의 효과.** 공간 LoRA 초기화를 위한 특수 설계로 제안된 분리 학습 전략의 유효성을 검증한다. 우리는 "w/o decouple"과 "w/o init"라는 두 가지 기준선과 비교한다. "w/o decouple"은 고품질 이미지 데이터 세트에서 공간 LoRA 계층을 사전 훈련하지 않고 비디오 데이터에 비디오 일관성 학습을 직접 적용하는 것을 의미하며, "w/o init"는 디커플링된 일관성 학습을 적용하지만 그림 3의 초기화 전략을 적용하지 않는다. 융합 속도의 공정한 비교를 위해 8 A800 GPU 노드에서 4시간 동안 공간 LoRA 가중치를 훈련한다. 본 논문에서 제안하는 방법은 비디오 데이터 셋에서 "w/o decouple"을 8시간 동안 학습하고, 그림 5와 같이 "w/o decouple"의 기준선을 크게 능가하는 우수한 성능을 보이며, 분리 일관성 학습의 효과를 보여준다. **교사가 없는 적응의 효과** 도 6은 이미지 확산 모델에서 T2I-어댑터[23]를 직접 사용하거나 제안된 무교사 적응으로 미세 조정한 제어 가능한 생성 결과를 보여준다. 도 1에 도시된 바와 같다. 6, 교사 없는 적응 과정 후, 우리는 분명히 우수한 통제 안정성과 시각적 품질을 달성한다.\n' +
      '\n' +
      '**개인화된 모델의 속성.** 원래의 안정적인 확산 모델은 방대한 텍스트-이미지 쌍에 대해 훈련되어 이미지 생성 이전을 얻는다. 그러나, 이들은 더 이상 인간-정렬되지 않고, 따라서 전형적으로 양호한 미적 점수를 갖는 이미지들을 생성하지 못한다. 많은 개인화된 모델은 고품질 데이터 세트에서 미세 조정되거나 서로 다른 기본 가중치와 병합되어 얻어진다[13, 26]. 그들은 일반적으로 특정 영역에 대해 더 높은 미적 점수를 가진 이미지를 생성한다. 도 1에 도시된 바와 같다. 본 논문에서 제안한 AnimateLCM은 실감형, 2D 애니메이션, 3D 애니메이션 등 다양한 스타일의 고품질 비디오 생성을 위해 직접 가중치를 적용할 수 있다. 일반적으로 개인화된 모델의 가중치를 적용하여 시각적 품질뿐만 아니라 움직임의 부드러움과 일관성을 향상시킬 수 있음을 알 수 있다. 본 논문에서는 Civitai에 대한 Realistic Vision V5.0의 개인화된 가중치와 원래의 안정적인 확산-v1.5의 성능을 검증하고, 그 결과를 표 1에 나타내었다. 특히 저단계 레짐에서 FVD와 CLIPSIM 모두에서 일관된 메트릭 증가를 관찰한다. 그러나 추론 단계를 증가시킬 때 명백한 시각적 품질 향상을 관찰하더라도, FVD 메트릭은 4단계 및 8단계에서 악화되며, 이는 개인화된 모델과 UCF101 사이의 특정 도메인 갭에 의해 발생할 수 있다. 예를 들어, 개인화된 모델은 선명하고 고품질의 비디오를 생성하는 경향이 있는 반면 UCF101은 흐릿하고 퍼지 콘텐츠를 포함한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 비디오 생성 가속의 혁신적인 발전인 AnimateLCM을 제시한다. 애니메이트LCM은 훈련 효율성과 생성 품질에 대한 좋은 균형을 달성하기 위해 두 단계 사이의 간격을 완화하기 위해 특정 초기화 설계로 이미지 생성 전 및 모션 생성 전의 일관성 증류를 분리한다는 점에서 분리 일관성 학습 전략에 의존한다. 또한 안정적인 확산 커뮤니티에서 기존 어댑터를 더 잘 수용하거나 처음부터 훈련하여 보다 혁신적인 생성 결과를 얻을 수 있는 간단하면서도 효과적인 교사 없는 적응 전략을 제안한다. 광범위한 실험이 효과를 검증한다. **제한:** AnimateLCM은 한 단계에서 샘플을 생성하는 데 한계에 직면하여 흐릿하거나 아티팩트가 풍부한 결과를 초래할 수 있다.\n' +
      '\n' +
      '그림 5: 제안된 분리 일관성 학습과 특정 초기화 전략에 대한 정량적 절제 연구.\n' +
      '\n' +
      '그림 6: 제어 가능한 세대 w/와 무교사 적응의 비교. 세대들의 얼굴이 흐릿하다.\n' +
      '\n' +
      '도 7: 상이한 수의 추론 단계(NFE)에서 AnimateLCM 및 교사 모델에 대한 생성 결과 비교.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, pages 1728-1738, 2021.\n' +
      '* [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Macie Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.\n' +
      '* [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, pages 22563-22575, 2023.\n' +
      '* [5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _NeurIPS_, 2014.\n' +
      '* [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatedff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [7] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. _arXiv preprint arXiv:2310.07702_, 2023.\n' +
      '* [8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.\n' +
      '* [9] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _NeurIPS_, 33:6840-6851, 2020.\n' +
      '* [11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv:2204.03458_, 2022.\n' +
      '* [13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [14] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In _CVPR_, pages 12753-12762, 2021.\n' +
      '* [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _NeurIPS_, 35:26565-26577, 2022.\n' +
      '* [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _NeurIPS_, 35:5775-5787, 2022.\n' +
      '* [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022.\n' +
      '* [19] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. _arXiv preprint arXiv:2310.04378_, 2023.\n' +
      '* [20] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. _arXiv e-prints_, pages arXiv-2303, 2023.\n' +
      '* [21] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. _arXiv preprint arXiv:2304.01186_, 2023.\n' +
      '* [22] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In _CVPR_, pages 14297-14306, 2023.\n' +
      '* [23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763. PMLR, 2021.\n' +
      '* [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10684-10695, 2022.\n' +
      '* [26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.\n' +
      '* [27] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. _arXiv preprint arXiv:2202.00512_, 2022.\n' +
      '* [28] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasono Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. _arXiv preprint arXiv:2401.15977_, 2024.\n' +
      '\n' +
      '* [29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [30] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [31] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. _arXiv preprint arXiv:2310.14189_, 2023.\n' +
      '* [32] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.\n' +
      '* [34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* [35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '* [36] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, pages 10459-10469, 2023.\n' +
      '* [37] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [38] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '* [39] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-corrector framework for fast sampling of diffusion models. _arXiv preprint arXiv:2302.04867_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>