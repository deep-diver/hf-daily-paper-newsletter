<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '주요 언어 모델들의 다중 언어 적응.\n' +
      '\n' +
      '피치킨 린\\({}^{{*,1,6}\\), 호리히 슈테즈\\({}^{{*,1,2}\\), 샤오니오니아 지\\({}^{*,3}\\), 조르그 타이데만\\({}^{3}\\), 안드레 F.\n' +
      '\n' +
      '정보 및 언어 처리를 위한 LMU 뮌헨({}^{1}\\)\n' +
      '\n' +
      '헬싱키\\({}^{2}\\) 기계 학습 센터({}^{3}\\)의 대학교입니다.\n' +
      '\n' +
      '\\({}^{4}\\) 인큐투토 슈퍼루타 테코(Lisbon ELLIS Unit)\n' +
      '\n' +
      '카누토 데텔레콤의\\({}^{5}\\)는 라벨이 없다({}^{6}\\).\n' +
      '\n' +
      'linpq@cis.lmu.de, shaoxiong.ji@helsinki.fi\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델은 자연어 처리에서 예술의 상태를 발전시켰다. 그러나 영어에 대한 우세한 디자인이나 제한된 언어 세트는 저자원 언어에 대한 효과에 상당한 격차를 일으킨다. 이러한 격차를 해소하기 위해 광범위한 범위의 534개 언어를 커버하도록 설계된 새로운 대형 언어 모델인 마LA-500을 소개합니다. 마LA-500을 훈련시키기 위해 어휘 확장을 사용하고 Glot500-c로 LLaMA 2를 계속 전처리했다. SIB-200에 대한 우리의 실험은 마LA-500이 최첨단 텍스트 학습 결과를 달성한다는 것을 보여준다. 우리는 [https://huggingface.co/MaLA-LM](https://huggingface.co/MaLA-LM)에서 마LA-500을 방출한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM), 예를 들어 LLaMA(Touvron et al., 2023, 2024), ChatGPT,1은 자연어 이해와 생성에서 놀라운 성능을 보여주었다. 후속 연구(방 et al., 2023; Lai et al., 2023; Ahja et al., 2023; Ahuja et al., 2023)는 이러한 영어 중심 LLM이 주로 영어 데이터를 학습 데이터로 하는 LLaMA와 같은 일부 고자원 비영어 언어를 처리할 수 있음을 관찰하여 사전 실습 동안 비영어 언어 데이터를 포함함으로써 혜택을 받을 수 있다. 그러나, 모델 크기 측면에서 데이터 부족 및 모델 능력으로 인해 저자원 언어에 대한 적용 가능성은 여전히 제한적이다.\n' +
      '\n' +
      '부시 1: [부시 나노오픈이.com/blog/chatqpt] (https://openai.com/blog/chatqpt)\n' +
      '\n' +
      '최근 XGLM(Lin et al, 2021), mGPT(Shliazhko et al., 2022), BLOOM(Scao et al., 2022) 등 여러 생성 다중 언어 LLM이 등장했다. 특히, 이러한 LLM에 대한 현재 언어 적용 범위는 최대 60개 언어로 제한되며, 이는 대량 다국어 LLM에 대한 추가 작업의 남은 필요성을 강조한다.\n' +
      '\n' +
      'ImaniGooghari et al.(2023)는 소규모 다국어 언어 모델인 XLM-R(Conneau et al., 2020)의 언어 적용 범위를 100개 언어로 278M 매개변수가 인상적인 534개 언어로 확장하고 395M 매개변수가 있는 확장 모델인 Glot500-m를 도입하여 대규모 언어 적응 영역에서 상당한 이정표를 달성했다. ImaniGooghari et al.(2023)은 47개 언어가정 534개 언어에 걸쳐 있는 Glot500c 체인을 소개하는 후 어휘 연장을 적용하고 Glot500m를 만드는 척을 계속하였다. Glot500-c의 도입은 저자원 언어에 대한 데이터 부족의 도전을 상당히 완화시킨다. 더욱이, 적응 방법은 더 적은 계산 자원을 필요로 하고 더 작은 탄소 발자국을 방출하기 때문에 처음부터 훈련보다 더 유리한다. 이러한 성공은 LLM의 대규모 언어 적응에 대한 우리의 탐구에 강한 동기부여 역할을 한다.\n' +
      '\n' +
      '이 작업은 LLM의 능력을 확장하여 더 넓은 범위의 언어를 포함하는 것을 목표로 한다. 사전 훈련된 모델의 언어 적응에 관한 기존 작업은 넓은 언어 스펙트럼에 걸쳐 확장된 적용 범위를 제공하지만 상대적으로 작은 모델 크기로 제한되며 대부분 백억 규모이다. 우리의 연구는 최대 10B까지의 모델 매개변수 스케일링으로 LLM에 대한 언어 적응 기술을 탐색하여 경계를 밀어냅니다. 우리의 조사는 모델 매개변수가 크게 증가한 생성 LLM과 다양한 언어에 걸쳐 맥락적 및 언어적 관련성을 향상시키는 텍스트 내 학습 능력에 대해 설명한다. 우리는 데이터 유출, 도메인 특이적 어휘 및 언어적 다양성과 같은 저자원 언어에 LLM을 적응시키는 문제를 다룬다. 우리는 오픈 LLM(즉, LLaMA 2(Touvron et al., 2023), 어휘 확장 및 적응 기술(즉, LoRA 저 순위 재직경화(Hu et al.,2022)의 전처리를 계속했다. 다양한 영역에서 500개 이상의 언어를 다루는 마LA-500을 훈련하여 출시합니다. 우리는 SIB-200에서 MaLA-500을 평가하고 결과는 MaLA-500이 거의 또는 약간 더 큰 모델 크기의 기존 오픈 LLM을 능가한다는 것을 보여준다. 이 작업은 LLM의 접근성을 넓히며, 특히 저자원 언어에 대한 보다 다양한 언어별 사용 사례 세트에 가치가 있으며, 많은 언어, 특히 기존 LLM이 다루는 과소 표현 언어에 대한 언어 장벽을 제거하여 평등 문제를 해결하는 데 유용하다.\n' +
      '\n' +
      '2개의 매생어 적응.\n' +
      '\n' +
      '대형 언어 모델의 대규모 언어 적응 원리는 대량 다국어 코퍼스(섹션 2.1), 강한 베이스 LLM(섹션 2.2), 효과적인 언어 적응 기술을 수용한다(섹션 2.3과 2.4).\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '534개 언어2를 포함하는 Glot500-c ImaniGooghari et al.(2023)을 데이터 양으로 언어 목록에 대한 MaLA-500의 학습 데이터로 사용한다. 우리는 어휘 확장을 위해 \\(\\alpha=0.3\\)와 함께 다항 분포에 따라 불균형 데이터 세트의 언어를 샘플링하고 계속 전처리했다.\n' +
      '\n' +
      '유도 2: 대응하는 필기 스크립트와 결합된 ISO 639-3 코드를 사용하여 언어를 정의한다. 예를 들어, "eng_Latin"은 라틴 대본에 적힌 영어를 나타낸다.\n' +
      '\n' +
      '### Model\n' +
      '\n' +
      '우리는 지속적인 교육을 시작하기 위해 LLaMA 2 Touvron et al.(2023)을 선택한다. 모델 가중치가 공개적으로 공개된 라마 시리즈 모델 투브론(2023)이 연구계에서 인기를 끌었다. 다국어 대응물에 비해 영어 중심성이 있음에도 불구하고 복수의 언어 자오 등(2024)에 주목할 만한 능력을 보였다. 우리는 2조 토큰으로 훈련된 최신 LLaMA 2를 뛰어난 언어 용량에서 혜택을 받기 위해 기본 모델로 선택한다. 우리의 연구는 32개의 변압기 층이 있는 7B 모델을 선택한다.\n' +
      '\n' +
      '### Vocabulary Extension\n' +
      '\n' +
      '우리는 처음에 샘플링된 Glot500-c의 어휘와 샘플링된 Glot500-c에 대한 다중 언어 토큰화기를 Glot500-c로 훈련하고 어휘를 25만-c로 인코딩하여 어휘를 확장한다.\n' +
      '\n' +
      '우리는 언어별 분할 길이 감소를 분석하여 어휘 확장이 Glot500-c의 개발 세트에 미치는 영향을 측정한다. 결과는 어휘 확장 효과가 8% 영어, eng_Latin)에서 88% Oriya, ori_Orya까지 다양함을 나타낸다. 당연히 어휘 연장은 라틴 대본에 있는 언어보다 비라틴 대본으로 작성된 언어에 더 큰 영향을 미친다. 그러나 라틴 대본, 예를 들어 카비예(kbp_Latin)와 베트남어(vie_Latin)에 쓰여진 일부 저자원 언어에 대해서는 분할 길이가 50% 정도 단축된다.\n' +
      '\n' +
      '### Continued Pretraining\n' +
      '\n' +
      '우리는 컴퓨팅 자원의 한계를 감안할 때 매개변수 효율적인 교육을 가능하게 하기 위해 낮은 순위 적응 LoRA, Hu et al.(2022)로 언어 적응을 위한 지속적인 전술을 사용한다. LoRA는 하위 순위가 있는 큰 가중치 행렬에 근사하는 훈련 가능한 순위 분해 행렬을 전처리된 모델 가중치에 주입한다. 계산 복잡도를 크게 감소시켜 훈련 비용을 절약합니다. 우리는 사전 조작된 모델의 모델 가중치를 동결하면서 순위 구성 행렬을 업데이트하기 위해 캐주얼 언어 모델을 지속적으로 훈련하여 이전 언어 능력을 완전히 잃지 않고 새로운 언어로 새로운 데이터에서 지속적으로 훈련된 언어 모델을 학습할 수 있게 한다. 대형 언어 모델의 지속적인 훈련은 실질적인 계산 자원을 필요로 한다. 트레이닝 과정을 실현 가능하게 하기 위해 슈퍼컴퓨터에 효율적인 분산형 트레이닝 세트를 채택합니다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '하드웨어와 소프트웨어는 GPU 노드에서 2개의 펫타플롭스의 이론적 피크 성능으로 컴퓨팅 클러스터에 모델을 훈련시킨다. 우리는 24개의 Nvidia Ampere A100 GPU에 분산 훈련을 배치했다. 소프트웨어는 휴깅페이스 트랜스포머 울프(2020), PEFT 직경의 효율적인 파인-트닝(Tuning)3 및 딥스피드(Rasley et al., 2020)를 사용한다. 우리는 ZeRO 중복 최적화기 Rajbhandari et al.(2020)를 사용하고 각 GPU의 메모리에 맞는 배치 크기를 최대화한다. 우리는 bfloat16 형식을 사용하여 혼합 고정 훈련을 사용한다.\n' +
      '\n' +
      '폐지 3: [전기신경부진 표면.코/docs/peft/index](https://huggingface.co/docs/docs/peft/index)\n' +
      '\n' +
      '학습률은 2e-4로 설정되며 체중 붕괴율은 0.01이 적용되어 큰 가중치를 처벌하고 과적합성을 완화한다. 트레이닝 가능한 LoRA 모듈은 언어 모델 헤드를 제외한 모든 선형 레이어를 대상으로 한다. 우리의 설정에서 최종 모델은 2B 매개변수가 훈련 가능한 총 10B 매개변수를 가지고 있다. LoRA 모듈은 순위 8, 알파 값 32, 드롭아웃 속도 0.05로 통합되어 훈련 중 모델의 적응성과 규칙화에 기여한다. 컨텍스트 창은 2k입니다. 우리는 배치 크기를 메모리에 맞게 최대화하여 384의 글로벌 배치 크기를 만들고 모델은 3개의 훈련 에포크를 겪는다. 체크포인트는 500단계마다 저장되며, 하류 업무에서 가장 유리한 평균 성능을 나타내는 체크포인트를 선택하기 위해 조기 중단을 사용한다. 이 모델 공개에서 우리는 주제 분류 과제만을 고려한다.\n' +
      '\n' +
      '환경영향력은 CSC의 탄소 중립 데이터 센터에 모델을 훈련시키고 재생 가능한 수력,4로 생성된 모든 전기를 지구 난방에 사용하여 CO2 발자국을 추가로 감소시킨다.\n' +
      '\n' +
      '부츠 4: [https://www.fi/지속 가능한 개발] (https://www.fi/s 지속가능 개발) (https://www.fi/s 지속가능 개발)\n' +
      '\n' +
      '부츠 5: [https://www.fi/지속가능성-1] (https://www.fi/지속가능성-1) (https://www.ci/지속가능성-1)].\n' +
      '\n' +
      '## 3 Evaluation\n' +
      '\n' +
      '마LA-500의 소수의 샷 학습 능력을 평가하고 SIB-200 아델라니 등(2023)의 토픽 분류 데이터셋에서 다른 LLM과 비교한다. 분류 과제는 과학/기술, 여행, 정치, 스포츠, 건강, 오락, 지리 등 7개 수업을 포함한다. 우리의 평가는 SIB-200 및 Glot500-c의 언어 세트와 교차하여 얻은 다양한 177개 언어 세트에 걸쳐 있다. 이 평가에서 우리는 각 훈련 세트로부터 무작위로 샘플링된 샷과 함께 각 언어에 대한 99개의 샘플의 정확도를 계산한다.\n' +
      '\n' +
      '평가된 LLM은 구조화된 프롬프트를 수신하며, 이는 소수의 샷 예와 예측을 위해 의도된 샘플의 연결이다. 몇 가지 샷 예와 샘플을 모두 예측할 수 있는 포맷은 다음과 같이 정의된다.\n' +
      '\n' +
      '토픽 분류: 과학/기술, 여행, 정치, 스포츠, 건강, 오락, 지리이다.\n' +
      '\n' +
      '[센트]의 표지는 [라벨]이다.\n' +
      '\n' +
      '[젠트]가 주제 분류를 위한 문장이고, [라벨]은 근거 진실이다. 샘플이 몇 샷 예시로서 역할을 하거나 예측하고자 하는 샘플이면 생략되는 경우[라벨]이 포함된다. 그런 다음 구성된 프롬프트는 LLM에 대한 입력으로 사용된다. 이어서, 평가된 LLM은 제공된 프롬프트에 기초하여 라벨 세트에 대한 레이블의 확률을 추정하도록 프롬프트된다. 이 평가 과정은 lm-평가-성숙도.6을 이용하여 구현된다.\n' +
      '\n' +
      '폐경 6: [https://github.com/esutherAI/lm 평가-harness] (https://github.com/ 증가하게utherAI/lm-평가-harness)\n' +
      '\n' +
      'LLM 전체에 걸쳐 비교된다.\n' +
      '\n' +
      '마LA-500을 LLaMA 2-7B, mGPT-13B, BLOOM-7B1 및 XGLM-7.5B와 비교하여 언어 전반에 걸쳐 3샷 인텍스트 학습 매크로 평균 정확도에 초점을 맞추고 있다. 표에서 보는 바와 같이. 1, MaLA-500은 이전 LLM을 상당히 능가한다. 특히, MaLA-500은 우리의 기본 모델인 LLaMA 2에 비해 현저한 12.16% 개선을 나타내며, 이는 MLA-500이 LLM의 다중 언어 능력을 향상시키는 데 상당한 기여를 강조한다.\n' +
      '\n' +
      '그림. 1은 언어 전반에 걸쳐 상세한 성능 분석을 제공합니다. 특히, MaLA-500은 언어가 무작위 결과를 얻지 못하고(0-20%) 정확도가 매우 낮은 언어 수(20-40%)가 크게 감소한다는 의미에서 이전 LLM을 능가한다. 더욱이, 마LA-500은 60% 이상의 정확도를 달성하는 언어 수(65개 모두)에 적합하다.\n' +
      '\n' +
      '우리의 기여 요인에 대한 포괄적인 분석에서 어휘 확장 효과와 성능 이득 사이에 0.53의 유의한 피어슨 상관관계가 관찰된다. 이 관찰은 카나다(칸_Knda), 말야알람(말_Mlym), 티그린야(티르_에탄올i)와 이보(ibo_Latin) 및 요루바(yor_Latin)와 같은 라틴 대본과 같은 비-라틴 대본 모두의 언어에 대해 사실이다. 또한 언어의 코퍼스 크기는 성능 이득과 0.13의 약한 상관 관계를 나타낸다는 점에 주목한다. 대조적으로, 언어가 속한 언어 가족의 코퍼스 크기는 0.40의 중간 상관관계를 보여준다.\n' +
      '\n' +
      '골목 수에 대한 효과.\n' +
      '\n' +
      '그림. 2는 정확도와 텍스트 내 예들의 수(즉, 샷) 사이의 관계를 보여준다. 텍스트 내 샷의 수가 증가함에 따라, 그에 상응하는 정확도의 상승이 있다. 특히, 1샷만으로 정확도는 26.08%로 무작위성을 나타내며, 1샷은 과제 학습에 대한 제한된 정보를 제공한다. 1샷에서 2샷/3슛으로의 전환은 공연이 각각 20.97%와 28.02% 증가하는 등 상당한 개선을 초래한다. 이는 샷 수를 늘리는 효과를 강조한다. 마LA-500은 6-10 인텍스트 샷으로 약 60% 정확도로 피크 성능을 달성합니다. 이는 SIB-200 데이터셋의 다중 클래스 특성에 기인할 수 있으며, 복잡한 입력 라벨 매핑을 학습하기 위해 더 많은 샷이 필요할 수 있다.\n' +
      '\n' +
      '그림에서. 3, 결과들을 더 많이 묘사한 것은 그림 2에서 이루어진 관찰과 일치하며, 1샷 인-텍스트 학습의 영역에서는 약 40개의 언어가 불규칙적인 결과를 나타낸다. 샷 수가 증가함에 따라, 높은 정확도를 달성하는 코호트가 증가하는 것과 결합된 낮은 정확도(20-40%)를 달성하는 언어 수의 감소가 있다(60-80%) 특히, 10샷 인텍스트 학습의 경우 101개(57%) 언어가 60% 정확도의 임계값을 능가한다.\n' +
      '\n' +
      '개별 언어 경향에 대한 추가 조사는 일부 저자원 언어가 더 나은 성능(예: 페르시아용 pes_Arab for 페르시아용)을 달성하기 위해 더 많은 샷을 필요로 하거나 심지어 10개의 샷(예: Dzongkha용 dzo_Tibt 및 센트럴 Aymara용 ayr_Latin)으로 저조한 성능을 나타낸다는 것을 보여준다. 이와는 대조적으로 프랑스어 프렌치앤드 라틴과 같은 고자원 언어는 샷이 적더라도 인상적인 성능을 보이며, 샷 수를 늘리면 한계개선에 그치고 있다.\n' +
      '\n' +
      '4번 관련 작업.\n' +
      '\n' +
      '언어 모델.\n' +
      '\n' +
      '언어 모델 개발은 다국어 시나리오를 해결하기 위해 관련 언어의 범위를 넓히기 위해 노력했다. 퇴적된 다국어 모델은 최대 100개 이상의 언어를 수용할 수 있었다. 주목할 만한 예로는 104개 언어를 지원하는 mBERT(Devlin et al., 2019), XLM-R(Conneau et)을 포함하는 mBERT(Devlin et al., 2019)가 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline Model & Accuracy \\\\ \\hline LLaMA 2-7B & 41.94 \\\\ mGPT-13B & 39.96 \\\\ BLOOM-7B1 & 44.22 \\\\ XGLM-7.5B & 47.53 \\\\ \\hline MaLA-500 & **54.10** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '다른 LLM의 SIB-200에서 표 1:3샷 인텍스트 학습 매크로 평균 정확도(%)를 나타낸다. mGPT는 약 7B 매개변수가 있는 모델이 없기 때문에 13B 매개변수가 있는 더 큰 모델을 선택한다.\n' +
      '\n' +
      '그림 1: SIB-200 X축에 대한 3샷 인텍스트 학습 결과, 서로 다른 정확도 범위의 언어 수(%)\n' +
      '\n' +
      '그림 2: MaLA-500을 사용하여 서로 다른 수의 샷을 갖는 SIB-200에서 인-텍스트 학습 거시 평균 정확도(%)를 보여준다.\n' +
      '\n' +
      '100개 언어를 포함하는 mBART Liu et al.(2020), 100개 언어를 포함하는 mBART Liu et al.(2020), 101개 언어에 걸쳐 있는 mT5 Xue et al.(2021), 30개 언어에 걸쳐 XGLM Lin et al.(2021), 118개 언어(93%)를 덮는 GPT-3, 60개 언어를 수용하는 mGPT Shliazhko(2022), 60개 언어를 수용하는 mGPT Shliazhko(2022) 및 13개 프로그래밍 언어를 지원하는 GPT-3개 언어(93% 영어(93%) 및 BLOOM Scao et al.\n' +
      '\n' +
      '놀랍게도, 최근 두 개의 다국어 언어 모델은 400개 이상의 언어를 지원함으로써 기존의 한계를 뛰어넘었다. 517개 이상의 아프리카어와 언어 품종, 5.17개 이상의 언어를 지원함으로써 더 나아가 5.17개 이상의 언어와 5개의 다른 언어에 대한 교육을 지원하며, C.SERE.S.S.S.S.S.A.I.S.A.2(2023) 언어에 따라 검색어 전용 및 MAD.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.S.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A.A. 모델 아키텍처. 우리는 오픈 모델 아키텍처, 즉 LLaMA를 채택하는 반면 MADLAD는 UL2를 사용한다.\n' +
      '\n' +
      '### Language Adaptation\n' +
      '\n' +
      'LLM이 등장하기 전에 소규모 다국어 언어 모델을 새로운 언어에 적응시키기 위해 다양한 접근법이 사용된다. 이러한 방법에는 어댑터 알(2020), 어휘 확장 및 대체 차우(2020); 루블러 등(2020), 어휘 확장 및 대체 수단 알 알(2020), 피플러 등(2020), 파펠러 에브라히미(2023), 레버리징 체아 에브라히미(2021), 이중 언어 lexix Wang(2022) 등을 사용한다.\n' +
      '\n' +
      '언어 모델이 크게 확장되었지만, 그 적용 범위는 특정 언어 집합으로 제한된다. 이러한 제약을 해결하기 위해 광범위한 언어, 케이터링, 일반 목적 작업 및 기계 번역과 같은 특정 애플리케이션 모두에 걸쳐 이러한 대형 언어 모델(LLM)의 적용 가능성을 확장하기 위한 다양한 방법이 제시되었다. 이러한 방법에는 어휘 확장 체이(2023)도 포함되어 있으며, 어휘 확장 용(2022), 사전 및 지시 중단 용(2022), 코이 등은 알(2023), 커먼(2023), 평행 기업 착취 카히위자야(2023), 양 등은 알(2023), 주 등은 알(2023) 이러한 노력에도 불구하고 다양한 언어에 걸친 범용 과제에 대한 LLM의 대규모 언어 적응은 아직 철저히 탐구되지 않은 영역으로 남아 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'LLaMA 7B를 모델, MaLA-500으로 확장하는 데 중점을 두고 LLM에 대한 대규모 언어 적응에 선구적인 노력을 제시하며, 이 적응은 어휘 확장과 LoRA를 계속 전처리하는 것을 포함한다. 우리의 접근법은 SIB-200의 과제에 대해 입증된 바와 같이 마LA-500이 최첨단 텍스트 학습 능력을 달성하는 것으로 이어지며 향후 연구를 용이하게 하기 위해 모델 가중치를 공개적으로 공개한다. 이 작업은 다양한 언어에 LLM을 적용하는 데 상당한 발전을 나타낸다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '데이터는 더 많은 고자원 데이터를 수집하지 않고 대신 Glot500-c만을 사용하여 고자원 언어에 대한 데이터 수집에 만전을 기하지 않는다. 또한, 플로어200 기반 SIB-200 평가 세트는 Glot500-c가 플레스200을 포함하기 때문에 학습 데이터에 포함되지만 분류 라벨은 제공되지 않는다.\n' +
      '\n' +
      '모델 화면 연구에 따르면 최대 모델 크기는 10B 매개변수로 제한된다. 미래 작업에서 계획합니다.\n' +
      '\n' +
      '그림 3: MaLA-500을 사용하여 SIB-200에 대한 텍스트 내 학습 결과를 요약한 X축: 서로 다른 정확도 범위의 언어 수(%) Y축: 샷 수.\n' +
      '\n' +
      '더 큰 모델로 확장합니다.\n' +
      '\n' +
      '## Ethical Implications\n' +
      '\n' +
      'LLM은 학습 데이터에 존재하는 편향을 나타내는 것으로 알려져 있다. LLM을 저자원 언어로 확장할 때, 고자원 언어에서 과소 표현 언어로의 편향을 전파할 위험이 있다. 편향을 완화하고 데이터 수집 및 모델 훈련의 공정성을 확보하기 위해서는 세심한 주의가 필요하다. 이 논문은 LLM을 과소 대표되는 언어에 더 쉽게 접근할 수 있도록 하는 것을 목표로 한다. 여전히 제한된 기술 접근으로 인해 특정 커뮤니티가 배제될 경우 디지털 언어 분할을 만들 위험이 있다. 향후 작업은 훈련 데이터에 대한 편향 감사를 수행하고, 생성 중 모델을 디피싱하고, 지속적으로 모니터링 모델 출력을 수행하여 편향을 해결할 것이다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 작업은 유럽 연구 협의회(기업 #740516 및 #758969), EU의 호라이즌 유럽 연구 혁신 기능(UTTER, 계약 101070631), EU 호라이즌 유럽 연구 혁신 프로그램(EU 호라이즌 유럽 연구 및 혁신 프로그램이 영국 정부의 호라이즌 유럽 자금 보장[계약 번호 10052546]에 따라 교부 계약 No 101070350 및 영국 연구 혁신(UKRI)에 따라 자금을 지원받았다. 저자들은 관대한 계산 자원에 대해 핀란드의 CSC-IT 과학 센터를 인정하고자 한다. Shaoxiong 지와 Peiqin Lin은 ELISE(GA 951847 없음)의 여행 지원을 인정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. 엘마다야, M. Abdul-Mageed 및 A. 알바바 잉세이레(2022)SERENGETI: 아프리카를 위한 대량 다국어 언어 모델이다. arXiv 프리프린트 arXiv:2212.10785: SS1에 의해 계산된다.\n' +
      '* D. H. 아델라니, H. 류, X. 네, 네. 바실예프, J. 오알라비, Y. Mao, H. 가오, E. A. 이씨(2023)SIB-200: 200+ 언어와 방언에서 토픽 분류를 위한 간단하고 포괄적이며 큰 평가 데이터셋이다. CoRRabs/2309.07445. 외부 링크:링크, 2309.07445: SS1로 제공됩니다.\n' +
      '* K. 아유자, R. 하다, M. 오치펑, P. 자인, H. 디데이, S. 주아, T. 가누, S. 세갈, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 생성 AI에 대한 다국어 평가이다. CoRRabs/2303.12528. 외부 링크:링크, 2309.12528: SS1로 계산되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 오멍, R. 하다, P. 자인, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M. 알았어요, K. 발리, S. 시타람(2023)MEGA: 새로운 연구 시대를 향해. 외부 링크: SS1에 의해 링크되었습니다.\n' +
      'S*S. 아유자, D. 아가왈, V. 음마, 나는 와트, A. 사테, M.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '삼야암 라자베나리, 제프 라슬리, 올라틀지 루위즈, 유시온그 헤. 2020. ZeRO: 메모리 최적화는 1조 매개변수 모델을 교육한다. _SC20: 고성능 컴퓨팅, 네트워크링, 저장 및 분석_ 페이지 1-16. IEEE 국제 회의.\n' +
      '* 라슬리 등은 (2020) 제프 라슬리, 삼야암 라즈베나리, 올라툰지 루위즈, 유시온그 헤 등이 있다. 2020년 딥스피드: 시스템 최적화는 1,000억 개 이상의 매개변수를 가진 딥러닝 모델을 트레이닝할 수 있다. 제26회 ACM SIGKDD 국제 지식 발견 및 데이터 Mining_ 페이지 3505-3506에 관한 회의의 _발표에서.\n' +
      '* Le Scao et al.(2022) 테븐 르스카오, 앙헬라 팬, 크리스토퍼 아키키, 엘리 파블릭, 스즈나 아이솔, 다니엘 헤센, 로마 카스타그네, 알렉산드라 사샤 루키니, 프랑코이스 Yvon, 마티아스 갈레 등 2022. BLOOM: A 176b-파라미터 개방형 액세스 다중언어 언어 모델. arXiv 프리프린트 arXiv:2211.05100_.\n' +
      '* 샤리지코 등은 알(2022) 오레 시리아조코, 알레나 페노게노바, 마리아 타이카노바, 블라디슬라브 미하일로프, 안스타시아 코즐로바, 타티아나 샤브리나 등이다. 2022. mGPT: Few-shot 학습자가 다국어로 간다. __ew-shot 학습자는 다언어이다. CoRR_, 절대/2204.07580.\n' +
      '* 테이(2022) 이테이, 모스타파 데헤이니, 비네 큐 트란, 자비에르 가르시아, 제이슨 웨이, 쉬에히 왕, 형원정, 다라 바히, 탈슈머, 스티븐 정 등 2022. UL2: 언어 학습 패러다임의 유니징입니다. _Eleventh 국제 학습 발표회의_.\n' +
      '(2023) 허고 타브론, 티비어 라브론, 지케어 이자카드, 자비에르 마르티네, 마리에-안네 라흐크, 티모트희 라크로릭스, 벅시스트 라지어, 남안 고살, 에릭 함크로, 아우렐리엔 로드리게스, 아르만드 자울린, 에드우가르드 그레이브, 과닐라메드 그라네 등이 있다. 2023a. LLaMA: 오픈과 효율적인 기초 언어 모델 __LLaMA: 오픈과 효율적인 기초 언어 모델. CoRR_, 절대/2302.13971.\n' +
      '오케네 아흐네르, 아말리아누에, 아네네르스 루위, 아네네우, 아네우우, 아레누 아와네, 카네우우, 라흐네르, 아흐네우, 마흐네르네, 주니니 크라누, 후미, 다케나 스켈레미, 오랄라, 아레누에, 아레누에, 아레누, 아레누에, 아레누에, 아레누에, 나우우우우, 아흐네르네우우, 아흐네르네우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우, 아흐네르네, 아흐네르네, 아레누, 아레누, 아레누, 아레누, 아에로, 우에, 알루, 아에, 아레누, 아레누, 아레누, 아레누, 아레누, 아레누, 아 2023b. 라마2: 오픈 파운데이션과 미세 조정 채팅 모델 __라마2: 오픈 파운데이션 및 미세 조정 채팅 모델. CoRR_, 절대/2307.09288.\n' +
      '* 우스텐 등은 (2020) 아메 우스텐, 아라이언나 비사자, 고세 부마, 게르잔 밴 노어드 등이다. 2020. Ud캡터: 진정으로 보편적인 의존성을 위한 언어 적응. 자연 언어 처리에서 2020년 실증 방법에 관한 회의의 _검토에서 EMNLP 2020, 온라인, 11월 16-20, 2020_, 컴퓨터 동역학 협회의 2302-2315페이지이다.\n' +
      '* 왕 등 (2022) 신니 왕, 세바스티안 루더, 그레이엄 네비그. 2022. lexicon 기반 적응을 통해 수천 개의 더 많은 언어로 모델을 전처리했다. 컴퓨터통계학회 제60회 연례회의(1: 롱 파이어스)에서 ACL 2022, 더블린, 아일랜드, 2022년 5월 22-27일, 2022_5월 22-27일, 컴퓨터통계학회 863-877쪽이다.\n' +
      '* 왕 등은 (2020) 지한왕, 가티키옌 K, 스티븐 메이휴, 단 로드가 있다. 2020년 다국어 BERT를 저자원 언어로 확장합니다. 컴퓨팅 언어 협회의 _ 찾기에서 EMNLP 2020, 온라인 이벤트, 2020년 11월 16-20일, ACL_의 부피 EMNLP 2020, ACL_의 검색, 페이지 2649-2656.\n' +
      '* 울프 등(2020) 토마스 울프, 리만드르 데부트, 빅토르 산, 진리엔 차믈론, 코멘 델랑에, 앤서니 모이, 피에르 시스트락, 팀 루프, 리미 루프, 모건 푸토위츠 등 2020년 국가-첨단 자연어 처리이다. 자연어 처리에서의 경험적 방법에 관한 2020년 회의의 _검토에서 시스템 시연_ 페이지 38-45.\n' +
      '*Xu 등 (2023) 하로란 주, 영진 김, 암르 샤라프, 하야 하산 아와달라. 머신 번역의 패러다임 전환: 대형 언어 모델의 번역 성능, 즉 기계 번역의 패러다임의 변화. CoRR_, 절대/2309.11674.\n' +
      '* Xue et al.(2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua 및 Colin Raffel. 2021. mT5: A 질량적으로 다국어 사전 학습된 텍스트 대 텍스트 변압기이다. 2021년 북미 컴퓨터 통계 협회의 발표: 인간 언어 기술_ 페이지 483-498.\n' +
      '* 양 등은 (2023) 원양, 충리, 지준장, 청칭동 등이 있다. 100개 언어에 걸쳐 다중언어 번역 능력을 가진 대형 언어 모델을 100개 언어에 걸쳐 수정: __2023. 빅트랜즈: 큰 언어 모델을 다국어 번역 능력으로 작성한다. CoRR_, 절대/2305.18098.\n' +
      '*용 등은 정신용(2022) 정신용, 해일리 소켈크로프, 니클라스 무니니오프, 알햄 피키 아지, 데이비드 카놀루와 아델라니, 칼리드 알무바라크, M. 살아있는 바리, 린탄 수타와카, 중오 카사이, 아메드 바루와, 펜타 인디라 위나타, 스텔라 바이데르만, 드롭기르 라데프, 바실리나 니콜리나. 0샷 프롬프트를 위해 BLOOM에 언어 지원을 추가하는 __BLOOM+1: BLOOM에 언어 지원을 추가한다. CoRR_, 절대/2212.09535.\n' +
      '* 자오 등은 (2024) 준자오, 지하오 장, 기장, 도귀, 잔징황 등이 있다. 영어 이상의 2024. LLaMA: 언어 능력 전달에 대한 경험적 연구는 영어 이상의 __2024. LLaMA: 언어 능력 전달에 관한 경험적 연구이다. arXiv 프리프린트 arXiv:2401.01055_입니다.\n' +
      '\n' +
      '웬하오 주, 윤즈허 Lv, 칭시우 동, 피위안, 징징Xu, 슈젠황, 랑펑콩, 지준 첸, 레이 리가 있다. 언어를 정렬하여 큰 언어 모델을 비영어로 재배치하는 것 __2023. CoRR_, 절대/2308.04948.\n' +
      '\n' +
      'A 부동산에 해당합니다.\n' +
      '\n' +
      '각 언어에 대한 사용 가능한 문장 및 언어 가족 정보의 수로 마LA-500을 훈련시키는 데 사용되는 Glot500-c의 언어 목록은 표 2, 3 및 4에 나와 있다.\n' +
      '\n' +
      '신청자 B.\n' +
      '\n' +
      '몇 번의 샷 학습 평가에 대한 자세한 결과는 표 5와 표 6에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l|c c c|c c c} \\hline \\hline Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c|}{Family} & Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c|}{Family} & Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c}{Family} \\\\ \\hline hbs\\_Latn & 63411156 & indo1319 & hin\\_Deva & 7046700 & indo1319 & ton\\_Latn & 1216118 & aust1307 \\\\ mal\\_Mlym & 48098273 & drav1251 & kor\\_Hang & 6468444 & kore1284 & tah\\_Latn & 1190747 & aust1307 \\\\ aze\\_Latn & 46300705 & & orry\\_Ory\\_Orya & 6266475 & indo1319 & lat\\_Latn & 1179913 & indo1319 \\\\ guj\\_Gujar & 45738685 & indo1319 & urd\\_Arab & 6009594 & indo1319 & snr\\_Latn & 1172349 & indo1319 \\\\ ben\\_Beng & 43514870 & indo1319 & swa\\_Latn & 5989369 & & ewe\\_Latn & 1161605 & atla1278 \\\\ kan\\_Knda & 41836495 & drav1251 & sqi\\_Latn & 5526836 & indo1319 & ben\\_Latn & 111969 & atla1278 \\\\ tel\\_Telu & 41580525 & drav1251 & bel\\_Cyrl & 5319675 & indo1319 & efi\\_Latn & 1082621 & atla1278 \\\\ ml\\_Latn & 40654838 & afro1255 & afr\\_Latn & 5157787 & indo1319 & bis\\_Latn & 1070170 & indo1319 \\\\ fra\\_Latn & 39197581 & indo1319 & nno\\_Latn & 4899103 & indo1319 & om\\_Latn & 1067699 \\\\ spa\\_Latn & 37286756 & indo1319 & tat\\_Cyrl & 4708088 & turk1311 & haw\\_Latn & 1062491 & aust1307 \\\\ eng\\_Latn & 36122761 & indo1319 & ast\\_Latn & 4683554 & indo1319 & hmo\\_Latn & 1033636 & pidg1258 \\\\ fil\\_Latn & 33493255 & aust1307 & mon\\_Cyrl & 4616960 & mong1349 & kat\\_Geor & 1004297 & kart1248 \\\\ nob\\_Latn & 32869205 & indo1319 & hbs\\_Cyrl & 4598073 & indo1319 & pag\\_Latn & 983637 & aust1307 \\\\ rus\\_Cyrl & 31787973 & indo1319 & nau\\_Latn & 4368483 & afro1255 & loz\\_Latn & 964418 & atla1278 \\\\ deu\\_Latn & 31015993 & indo1319 & sna\\_Latn & 4019596 & atla1278 & fry\\_Latn & 957422 & indo1319 \\\\ tur\\_Latn & 29184662 & turk1311 & msa\\_Latn & 3929084 & & may\\_Mymt & 945180 & sino1245 \\\\ pan\\_Guru & 29052537 & indo1319 & som\\_Latn & 3916769 & norg1255 & ads\\_Latn & 944715 & indo1319 \\\\ mar\\_Deva & 28748897 & indo1319 & spr\\_Cyrl & 3864091 & indo1319 & run\\_Latn & 943828 & atla1278 \\\\ por\\_Latn & 27824391 & indo1319 & mlg\\_Latn & 3715802 & & pub\\_Arab & 899895 & indo1319 \\\\ nld\\_Latn & 25061426 & indo1319 & zul\\_Latn & 3580113 & atla1278 & rar\\_Latn & 894515 & aust1307 \\\\ ara\\_Arab & 24524122 & arz\\_Arab & 3488224 & afro1255 & fij\\_Latn & 887134 & aust1307 \\\\ zho\\_Hani & 24143786 & nya\\_Latn & 3409030 & atla1278 & wls\\_Latn & 882167 & aust1307 \\\\ ita\\_Latn & 23539857 & indo1319 & tam\\_Taml & 3388255 & drav1251 & ckb\\_Arab & 874441 & indo1319 \\\\ ind\\_Latn & 23018106 & aust1307 & hat\\_Latn & 3226932 & indo1319 & ven\\_Latn & 860249 & atla1278 \\\\ ell\\_Grek & 2203328 & indo1319 & zu\\_Latn & 3223485 & turk1311 & zsm\\_Latn & 859947 & aust1307 \\\\ bul\\_Cyrl & 21823004 & indo1319 & vol\\_Latn & 3205510 & atla1278 & ch\\_Cyrl & 859863 & turk1311 \\\\ swe\\_Latn & 20725883 & indo1319 & uz\\_Cyrl & 3029947 & turk1311 & lua\\_Latn & 854359 & atla1278 \\\\ ces\\_Latn & 20376340 & indo1319 & cos\\_Latn & 3015055 & indo1319 & que\\_Latn & 838486 \\\\ isl\\_Latn & 19547941 & indo1319 & als\\_Latn & 2954874 & indo1319 & sag\\_Latn & 771048 & atla1278 \\\\ pol\\_Latn & 19339945 & indo1319 & ann\\_Ehti & 2862985 & afro1255 & guw\\_Latn & 767918 & atla1278 \\\\ non\\_Latn & 19190217 & indo1319 & sun\\_Latn & 2586011 & aust1307 & bre\\_Latn & 748954 & indo1319 \\\\ dam\\_Latn & 19174573 & indo1319 & war\\_Latn & 2584810 & aust1307 & toi\\_Latn & 745385 & atla1278 \\\\ hun\\_Latn & 18800025 & ural1722 & div\\_Tha & 2416867 & indo1319 & pus\\_Arab & 731992 & indo1319 \\\\ tgk\\_Cyrl & 18659517 & indo1319 & yor\\_Latn & 2392359 & atla1278 & che\\_Cyrl & 728201 & nash1245 \\\\ spr\\_Latn & 18371769 & indo1319 & fao\\_Latn & 2365271 & indo1319 & pis\\_Latn & 714783 & indo1319 \\\\ fas\\_Arab & 18277593 & uzu\\_Cyrl & 2293672 & turk1311 & kon\\_Latn & 685194 \\\\ ceb\\_Latn & 18149215 & aust1307 & smo\\_Latn & 2290439 & aust1307 & oss\\_Cyrl & 683517 & indo1319 \\\\ heb\\_Hebr & 18128962 & afro1255 & bak\\_Cyrl & 2264196 & turk1311 & hbyw\\_Armn & 679819 & indo1319 \\\\ hrv\\_Latn & 17882932 & indo1319 & io\\_Latn & 2106531 & aust1307 & iso\\_Latn & 658789 & atla1278 \\\\ glg\\_Latn & 17852274 & indo1319 & uso\\_Latn & 2100708 & atla1278 & nan\\_Latn & 656389 & sino1245 \\\\ fin\\_Latn & 16730388 & ural1272 & rrmi\\_Latn & 2046850 & aust1307 & lub\\_Latn & 654390 & atla1278 \\\\ slv\\_Latn & 15719210 & indo1319 & hm\\_Latn & 1903898 & lim\\_Latn & 625078 & indo1319 \\\\ vie\\_Latn & 15697827 & aust1305 & asm\\_Beng & 1882353 & indo1319 & tuk\\_Latn & 649411 & turk1311 \\\\ mkd\\_Cyrl & 14717004 & indo1319 & hi\\_Latn & 1798875 & aust1307 & tir\\_Eti & 649117 & afro1255 \\\\ slk\\_Latn &\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:11]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c|c|c c c} \\hline \\hline Lang & [Sent] & Family & Lang & [Sent] & Family & Lang & [Sent] & Family \\\\ \\hline alt\\_Cyrl & 95148 & turk1311 & mmy\\_Lattn & 50581 & atla1278 & csy\\_Lattn & 34126 & sinon1245 \\\\ rmn\\_Grek & 94533 & indo1319 & gkp\\_Lattn & 50549 & mand1469 & azb\\_Arab & 33758 & turk1311 \\\\ miq\\_Lattn & 94343 & misu1242 & kat\\_Lattn & 50424 & kart1248 & csy\\_Lattn & 33743 & indo1319 \\\\ kaa\\_Cyrl & 88815 & turk1311 & bijn\\_Lattn & 49068 & ausu1307 & tpm\\_Lattn & 33517 & atla1278 \\\\ kos\\_Lattn & 88603 & ausu1307 & ac\\_Lattn & 48886 & maya1287 & quw\\_Lattn & 33449 & quec1387 \\\\ grn\\_Lattn & 87568 & & dtp\\_Lattn & 48468 & ausu1307 & rmy\\_Cyrl & 33351 & indo1319 \\\\ lhu\\_Lattn & 87255 & sino1245 & lam\\_Lattn & 46853 & atla1278 & ix\\_Lattn & 33289 & maya1287 \\\\ lzh\\_Hani & 86035 & sino1245 & bik\\_Lattn & 46561 & & mbb\\_Lattn & 33240 & ausu1307 \\\\ ajp\\_Arab & 83297 & afo1255 & bph\\_Lattn & 46454 & maya1287 & pfl\\_Lattn & 33148 & indo1319 \\\\ cmn\\_Hani & 80745 & sino1245 & phm\\_Lattn & 45862 & atla1278 & pcd\\_Lattn & 32867 & indo1319 \\\\ gcf\\_Lattn & 80737 & indo1319 & ntw\\_Lattn & 45716 & indo1319 & tpm\\_Lattn & 32863 & arti1236 \\\\ rmn\\_Cyrl & 79925 & indo1319 & quh\\_Lattn & 45566 & quec1387 & suz\\_Deva & 32811 & sinon1245 \\\\ kijh\\_Cyrl & 79262 & turk1311 & jtw\\_Cyrl & 45379 & indo1319 & gcr\\_Lattn & 32676 & indo1319 \\\\ rng\\_Lattn & 78177 & atla1278 & rue\\_Cyrl & 45369 & indo1319 & jbo\\_Lattn & 32619 & arti1236 \\\\ mgh\\_Lattn & 78117 & atla1278 & eml\\_Lattn & 44630 & indo1319 & tbz\\_Lattn & 32264 & atla1278 \\\\ xmv\\_Lattn & 77896 & ausu1307 & ac\\_Arab & 44505 & afro1255 & bam\\_Lattn & 32150 & mand1469 \\\\ ige\\_Lattn & 77114 & atla1278 & tob\\_Lattn & 44473 & guai1249 & prk\\_Lattn & 32085 & ausu1305 \\\\ rmy\\_Lattn & 76991 & indo1319 & ach\\_Lattn & 43974 & nilo1247 & jam\\_Lattn & 32048 & indo1319 \\\\ srm\\_Lattn & 76884 & indo1319 & vpe\\_Lattn & 43076 & ural1272 & twx\\_Lattn & 32028 & atla1278 \\\\ bak\\_Lattn & 76809 & turk1311 & npi\\_Deva & 43072 & indo1319 & nmf\\_Lattn & 31997 & sinon1245 \\\\ gur\\_Lattn & 76151 & atla1278 & tok\\_Lattn & 42820 & arti1236 & caq\\_Lattn & 31903 & ausu1305 \\\\ idu\\_Lattn & 75106 & atla1278 & sgs\\_Lattn & 42467 & indo1319 & rop\\_Lattn & 31889 & indo1319 \\\\ yom\\_Lattn & 74818 & atla1278 & lij\\_Lattn & 42447 & indo1319 & tca\\_Lattn & 31852 & tciu1244 \\\\ tdx\\_Lattn & 74430 & ausu1307 & myv\\_Cyrl & 42147 & ural1272 & yan\\_Lattn & 31775 & misu1242 \\\\ mzn\\_Arab & 73719 & indo1319 & ith\\_Lattn & 41873 & ausu1307 & xav\\_Lattn & 31765 & nucl1710 \\\\ cfm\\_Lattn & 70227 & sino1245 & tat\\_Lattn & 41640 & turk1311 & bih\\_Deva & 31658 & \\\\ zpa\\_Lattn & 69237 & otom1299 & lfn\\_Lattn & 41632 & arti1236 & cuk\\_Lattn & 31612 & chib1249 \\\\ kbd\\_Cyrl & 67914 & abk1242 & cgg\\_Lattn & 41196 & atla1278 & kj\\_Lattn & 31471 & maya1287 \\\\ lao\\_Lano & 66966 & tta\\_Lattn & 41188 & atla1278 & hne\\_Deva & 31465 & indo1319 \\\\ nap\\_Lattn & 65826 & indo1319 & gwp\\_Lattn & 41174 & ausu1307 & wbm\\_Lattn & 31394 & ausu1305 \\\\ qub\\_Lattn & 64973 & quec1387 & ile\\_Lattn & 40984 & arti1236 & zlm\\_Lattn & 31345 & ausu1307 \\\\ oke\\_Lattn & 64508 & atla1278 & ium\\_Lattn & 40683 & hmon1336 & tui\\_Lattn & 31161 & atla1278 \\\\ ote\\_Lattn & 64224 & otom1299 & teo\\_Lattn & 40203 & nilo1247 & ifb\\_Lattn & 30980 & ausu1307 \\\\ bsb\\_Lattn & 63634 & ausu1307 & kia\\_Lattn & 40035 & atla1278 & iz\\_Lattn & 30894 & atla1278 \\\\ ogo\\_Lattn & 61901 & atla1278 & crh\\_Cyrl & 39985 & turk1311 & rug\\_Lattn & 30857 & ausu1307 \\\\ abn\\_Lattn & 61830 & atla1278 & crh\\_Lattn & 39896 & turk1311 & aka\\_Lattn & 30704 & atla1278 \\\\ ldi\\_Lattn & 61827 & atla1278 & enm\\_Lattn & 39809 & indo1319 & pxm\\_Lattn & 30698 & book1242 \\\\ ay\\_Lattn & 61570 & ayma1253 & sat\\_Dck & 39614 & ausu1305 & kmm\\_Lattn & 30671 & sinon1245 \\\\ gom\\_Deva & 61140 & indo1319 & mrad\\_Lattn & 38993 & ausu1307 & mcm\\_Lattn & 30666 & afro1255 \\\\ bba\\_Lattn & 61123 & atla1278 & cac\\_Lattn & 38812 & maya1287 & ifa\\_Lattn & 30621 & ausu1307 \\\\ aln\\_Lattn & 60989 & indo1319 & nnj\\_Lattn & 38611 & hmon1336 & dln\\_Lattn & 30620 & sinon1245 \\\\ leh\\_Lattn & 59944 & atla1278 & ksh\\_Lattn & 38130 & indo1319 & ext\\_Lattn & 30605 & indo1319 \\\\ ban\\_Lattn & 59805 & ausu1307 & ik\\_Lattn & 38071 & atla1278 & ksd\\_Lattn & 30550 & ausu1307 \\\\ ace\\_Lattn & 59333 & ausu1307 & sba\\_Lattn & 38040 & cent2225 & mzh\\_Lattn & 30517 & mata1289 \\\\ pes\\_Arab & 57511 & indo1319 & zom\\_Lattn & 37013 & sino1245 & llb\\_Lattn & 30480 & atla1278 \\\\ skg\\_Lattn & 57228 & ausu1307 & bqc\\_Lattn & 36881 & manda1469 & hra\\_Lattn & 30472 & sinon1245 \\\\ ary\\_Arab & 56933 & afro1255 & bim\\_Lattn & 3635 & atla1278 & mwn\\_Lattn & 30432 & cent225 \\\\ hus\\_Lattn & 56176 & maya1287 & rue\\_Lattn &\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>