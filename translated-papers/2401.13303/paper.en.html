<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MaLA-500: Massive Language Adaptation of Large Language Models\n' +
      '\n' +
      'Peiqin Lin\\({}^{*,1,2}\\), Shaoxiong Ji\\({}^{*,3}\\), Jorg Tiedemann\\({}^{3}\\), Andre F. T. Martins\\({}^{4,5,6}\\), Hinrich Schutze\\({}^{1,2}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Center for Information and Language Processing, LMU Munich\n' +
      '\n' +
      '\\({}^{2}\\)Munich Center for Machine Learning \\({}^{3}\\)University of Helsinki\n' +
      '\n' +
      '\\({}^{4}\\)Instituto Superior Tecnico (Lisbon ELLIS Unit)\n' +
      '\n' +
      '\\({}^{5}\\)Instituto de Telecomunicacoes \\({}^{6}\\)Unbabel\n' +
      '\n' +
      'linpq@cis.lmu.de, shaoxiong.ji@helsinki.fi\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models have advanced the state of the art in natural language processing. However, their predominant design for English or a limited set of languages creates a substantial gap in their effectiveness for low-resource languages. To bridge this gap, we introduce MaLA-500, a novel large language model designed to cover an extensive range of 534 languages. To train MaLA-500, we employ vocabulary extension and continued pretraining on LLaMA 2 with Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves state-of-the-art in-context learning results. We release MaLA-500 at [https://huggingface.co/MaLA-LM](https://huggingface.co/MaLA-LM).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Language Models (LLMs), e.g., LLaMA (Touvron et al., 2023, 2024), and ChatGPT,1 have shown remarkable performance in natural language understanding and generation. Follow-up studies (Bang et al., 2023; Lai et al., 2023; Ahuja et al., 2023, 2023) observe that these English-centric LLMs, such as LLaMA with mainly English as the training data, are capable of handling some high-resource non-English languages, benefiting from the inclusion of non-English language data during pretraining. However, their applicability to low-resource languages is still limited due to data scarcity and model capability in terms of model size.\n' +
      '\n' +
      'Footnote 1: [https://openai.com/blog/chatqpt](https://openai.com/blog/chatqpt)\n' +
      '\n' +
      'Recently, several generative multilingual LLMs, such as XGLM (Lin et al., 2021), mGPT (Shliazhko et al., 2022), and BLOOM (Scao et al., 2022), have emerged. Notably, the current language coverage for these LLMs is limited to up to 60 languages, highlighting the remaining need for further work on massively multilingual LLMs.\n' +
      '\n' +
      'ImaniGooghari et al. (2023) have achieved a significant milestone in the realm of massive language adaptation by extending the language coverage of a small-scale multilingual language model, XLM-R (Conneau et al., 2020) - an auto-encoding model with 278M parameters, from 100 languages to an impressive number of 534 languages, and introducing an extended model, Glot500-m with 395M parameters. ImaniGooghari et al. (2023) introduce the Glot500-c corpora spanning 534 languages from 47 language families, and then apply vocabulary extension and continued pretraining to create Glot500-m. The introduction of Glot500-c significantly mitigates the challenge of data scarcity for low-resource languages. Moreover, the adaptation method is more favorable than training from scratch, as it requires fewer computational resources and emits a smaller carbon footprint. This success serves as a strong motivation for our exploration into the massive language adaptation of LLMs.\n' +
      '\n' +
      'This work aims to extend the capabilities of LLMs to encompass a wider range of languages. Existing works on language adaptation of pre-trained models provide extended coverage across a wide linguistic spectrum but are limited to relatively small model sizes - mostly at the hundred million scale. Our study pushes the boundaries by exploring language adaptation techniques for LLMs with model parameters scaling up to 10B. Our investigation delves into generative LLMs with a substantial increase in model parameters and their in-context learning capabilities that enhance contextual and linguistic relevance across a diverse range of languages. We address the challenges of adapting LLMs to low-resource languages, such as data sparsity, domain-specific vocabulary, and linguistic diversity. We study continued pretraining of open LLM (i.e., LLaMA 2 (Touvron et al., 2023)), vocabulary extension, and adaptation techniques (i.e., LoRA low-rank reparameterization (Hu et al.,2022)). We train and release MaLA-500 that covers more than 500 languages in various domains. We evaluate MaLA-500 on SIB-200 and the results show that MaLA-500 outperforms existing open LLMs of close or slightly larger model size. This work broadens the accessibility of LLMs, making them valuable for a more diverse set of language-specific use cases, especially for low-resource ones, and addressing the equality issue by removing language barriers for speakers of many languages, especially those underrepresented languages covered by existing LLMs.\n' +
      '\n' +
      '## 2 Massive Language Adaptation\n' +
      '\n' +
      'The principle of massive language adaption of large language models accommodates the utilization of a massively multilingual corpus (Section 2.1), the strong base LLM (Section 2.2), and the technique for effective language adaption (Sections 2.3 and 2.4).\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      'We use Glot500-c ImaniGooghari et al. (2023) covering 534 languages2 as the training data of MaLA-500. See SSA for the list of languages with their data amounts. We sample languages from the imbalanced dataset according to a multinomial distribution, with \\(\\alpha=0.3\\) for vocabulary extension and continued pretraining.\n' +
      '\n' +
      'Footnote 2: We define languages using the ISO 639-3 code combined with the corresponding written script. For example, “eng_Latin” represents English written in the Latin script.\n' +
      '\n' +
      '### Model\n' +
      '\n' +
      'We choose LLaMA 2 Touvron et al. (2023) to start continual training. LLaMA series models Touvron et al. (2023), with model weights released publicly, have gained popularity in the research community. Despite being English-centric compared to their multilingual counterparts, they have shown remarkable capacity for multiple languages Zhao et al. (2024). We choose the latest LLaMA 2, trained on 2 trillion tokens, as our base model to benefit from its outstanding language capacity. Our study chooses the 7B model with 32 transformer layers.\n' +
      '\n' +
      '### Vocabulary Extension\n' +
      '\n' +
      'The original LLaMA 2\'s 32,000 tokenizer covers English and a small fraction of other European languages using Latin or Cyrillic scripts. To enhance its capability and encoding efficiency for a broader range of languages, we extend the vocabulary with Glot500-c. Specifically, we initially train a multilingual tokenizer with SentencePiece Kudo and Richardson (2018) on the sampled Glot500-c with a vocabulary of 250,000. Subsequently, we merge the trained tokenizer with the original LLaMA 2 tokenizer by taking the union of their vocabularies. As a result, we obtain the MaLA-500\'s tokenizer with a vocabulary size of 260,164. After vocabulary extension and resizing the embedding layer, the model size becomes 8.6B.\n' +
      '\n' +
      'We measure the impact of vocabulary extension on the development set of Glot500-c by analyzing the reduction in segmentation length for each language. The results indicate that the effect of vocabulary extension varies, ranging from 8% English, eng_Latin) to 88% Oriya, ori_Orya). Unsurprisingly, vocabulary extension has a larger effect on languages written in non-Latin scripts than on those in the Latin script. However, for some low-resource languages written in the Latin script, e.g., Kabiye (kbp_Latin) and Vietnamese (vie_Latin), the segmentation length is shortened by around 50%.\n' +
      '\n' +
      '### Continued Pretraining\n' +
      '\n' +
      'We employ continued pretraining for language adaptation with low-rank adaptation LoRA, Hu et al. (2022) to enable parameter-efficient training, given the limitation of our computing resources. LoRA injects trainable rank decomposition matrices, which approximate the large weight matrices with a lower rank, to the pretrained model weights. It significantly reduces the computational complexity and thus saves the training cost. We continually train the casual language model to update the rank-decomposition matrices while freezing the model weights of pretrained models, allowing the continually trained language model to learn from new data in new languages without completely losing its previous language capacity. Continual training of large language models requires substantial computational resources. We adopt efficient distributed training setups on supercomputers to make the training process feasible.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'Hardware and SoftwareWe train our model on the computing cluster with the theoretical peak performance of 2 petaflops on GPU nodes. We deploy distributed training on 24 Nvidia Ampere A100 GPUs. As for software, we utilize the Huggingface Transformers Wolf et al. (2020), PEFT Parameter Efficient Fine-Tuning)3, and DeepSpeed (Rasley et al., 2020). We use the ZeRO redundancy optimizer Rajbhandari et al. (2020) and maximize the batch size that fits the memory of each GPU. We employ mixed-precision training using the bfloat16 format.\n' +
      '\n' +
      'Footnote 3: [https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)\n' +
      '\n' +
      'HyperparametersThe learning rate is set at 2e-4. A weight decay of 0.01 is applied to penalize large weights and mitigate overfitting. The trainable LoRA module targets all linear layers, excluding the language model head. In our setting, the final model has 10B parameters in total, in which 2B parameters are trainable. The LoRA module is incorporated with a rank of 8, an alpha value of 32, and a dropout rate of 0.05, contributing to the model\'s adaptability and regularization during training. The context window is 2k. We maximize the batch size to fit the memory, making a global batch size of 384. The model undergoes three training epochs. Checkpoints are saved every 500 steps, and we employ early stopping to select the checkpoint that exhibits the most favorable average performance on downstream tasks. In this model release, we only consider the topic classification task.\n' +
      '\n' +
      'Environmental ImpactsWe train our model on CSC\'s carbon-neutral data center, with all electricity generated with renewable hydropower,4 and the waste heat is utilized in district heating to further reduce CO2 footprint.5\n' +
      '\n' +
      'Footnote 4: [https://www.csc.fi/sustainable-development](https://www.csc.fi/sustainable-development)\n' +
      '\n' +
      'Footnote 5: [https://www.csc.fi/sustainability-1](https://www.csc.fi/sustainability-1)\n' +
      '\n' +
      '## 3 Evaluation\n' +
      '\n' +
      'We evaluate the few-shot learning capability of MaLA-500 and compare it with other LLMs on a topic classification dataset of SIB-200 Adelani et al. (2023). The classification task involves seven classes, namely science/technology, travel, politics, sports, health, entertainment, and geography. Our evaluation spans a diverse set of 177 languages, obtained by intersecting the language sets of SIB-200 and Glot500-c. In this evaluation, we compute the accuracy of 99 samples for each language, with shots randomly sampled from the respective training sets.\n' +
      '\n' +
      'The evaluated LLM receives a structured prompt, which is the concatenation of few-shot examples and the sample intended for prediction. The format for both a few-shot example and the sample to predict is defined as follows:\n' +
      '\n' +
      'Topic Classification: science/technology, travel, politics, sports, health, entertainment, geography.\n' +
      '\n' +
      'The label of [sent] is [label]\n' +
      '\n' +
      'where [sent] is the sentence for topic classification, and [label] is the ground truth. [label] is included if the sample serves as a few-shot example or omitted if it is the sample to predict. The constructed prompt is then used as input to the LLM. Subsequently, the evaluated LLM is prompted to estimate the probability of the label over the label set based on the provided prompt. This evaluation process is implemented using the lm-evaluation-harness.6\n' +
      '\n' +
      'Footnote 6: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '### Comparison across LLMs\n' +
      '\n' +
      'We compare MaLA-500 with LLaMA 2-7B, mGPT-13B, BLOOM-7B1, and XGLM-7.5B, focusing on the 3-shot in-context learning macro-average accuracy across languages. As shown in Table. 1, MaLA-500 outperforms previous LLMs significantly. Notably, MaLA-500 exhibits a remarkable 12.16% improvement compared to our base model, LLaMA 2, highlighting MaLA-500\'s substantial contribution to enhancing the multilingual capacity of LLMs.\n' +
      '\n' +
      'Fig. 1 provides a detailed performance analysis across languages. Notably, MaLA-500 surpasses previous LLMs in the sense that no language obtains random results (0-20%) and the number of languages with very low accuracy (20-40%) is significantly reduced. Moreover, MaLA-500 excels in the number of languages achieving 60% accuracy or higher (65 altogether).\n' +
      '\n' +
      'In our comprehensive analysis of contributing factors, a significant Pearson correlation of 0.53 is observed between the effect of vocabulary extension and the performance gain. This observation holds true for languages with both non-Latin scripts, such as Kannada (kan_Knda), Malayalam (mal_Mlym), and Tigrinya (tir_Ethi), as well as Latin scripts, such as Igbo (ibo_Latin) and Yoruba (yor_Latin). Furthermore, we note that the corpus size of a language exhibits a weak correlation of 0.13 with its performance gain. In contrast, the corpus size of the language family to which a language belongs demonstrates a moderate correlation of 0.40.\n' +
      '\n' +
      '### Effect of Number of Shots\n' +
      '\n' +
      'Fig. 2 illustrates the relationship between accuracy and the number of in-context examples (i.e., shots). As the number of in-context shots increases, there is a corresponding rise in accuracy. Notably, with just 1-shot, accuracy exhibits randomness at 26.08%, indicating 1-shot provides limited information for task learning. The transition from 1 shot to 2 shots/3 shots results in a significant improvement, with performances boosted by 20.97% and 28.02%, respectively. This highlights the effectiveness of increasing the number of shots. MaLA-500 achieves its peak performance at approximately 60% accuracy with 6-10 in-context shots. This may be attributed to the multi-class nature of the SIB-200 dataset, necessitating more shots for learning intricate input-label mappings.\n' +
      '\n' +
      'In Fig. 3, a more nuanced portrayal of results aligns with the observations made in Fig. 2. In the realm of 1-shot in-context learning, approximately 40 languages exhibit erratic results. As the number of shots increases, there is a reduction in the number of languages achieving low accuracy (20-40%), coupled with a growing cohort achieving high accuracy (60-80%). Notably, in the case of 10-shot in-context learning, 101 (57%) languages surpass the threshold of 60% accuracy.\n' +
      '\n' +
      'Further examination into individual language trends reveals that some low-resource languages require more shots to achieve better performance (e.g., pes_Arab for Persian) or even exhibit poor performance with 10 shots (e.g., dzo_Tibt for Dzongkha and ayr_Latin for Central Aymara). In contrast, high-resource languages, such as fra_Latin for French, demonstrate impressive performance even with fewer shots, and increasing the number of shots results in only marginal improvement.\n' +
      '\n' +
      '## 4 Related Work\n' +
      '\n' +
      '### Multilingual Language Models\n' +
      '\n' +
      'Language model development has endeavored to broaden the scope of pertaining languages to address multilingual scenarios. Pretrained multilingual models have been able to accommodate up to a hundred or more languages. Noteworthy examples include mBERT (Devlin et al., 2019), which supports 104 languages, XLM-R (Conneau\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline Model & Accuracy \\\\ \\hline LLaMA 2-7B & 41.94 \\\\ mGPT-13B & 39.96 \\\\ BLOOM-7B1 & 44.22 \\\\ XGLM-7.5B & 47.53 \\\\ \\hline MaLA-500 & **54.10** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: 3-shot in-context learning macro-average accuracy (%) on SIB-200 of different LLMs. mGPT has no model with around 7B parameters, so we choose a larger one with 13B parameters.\n' +
      '\n' +
      'Figure 1: Detailed results of 3-shot in-context learning on SIB-200. X-axis: the number of languages in different accuracy ranges (%).\n' +
      '\n' +
      'Figure 2: In-context learning macro-average accuracy (%) on SIB-200 with different number of shots using MaLA-500.\n' +
      '\n' +
      'et al., 2020) covering 100 languages, mBART Liu et al. (2020) designed for 25 languages, mT5 Xue et al. (2021) spanning 101 languages, XGLM Lin et al. (2021) across 30 languages, GPT-3 covering 118 languages (93% English), mGPT Shliazhko et al. (2022) accommodating 60 languages, and BLOOM Scao et al. (2022) supporting 46 languages and 13 programming languages.\n' +
      '\n' +
      'Surprisingly, two recent multilingual language models have surpassed the conventional limit by supporting more than 400 languages. Glot500-m ImaniGooghari et al. (2023) spans 511 languages through vocabulary extension and continued training based on XLM-R. SERENGETI Adebara et al. (2022) goes even further by supporting 517 African languages and language varieties, written in five different scripts, employing models inspired by both ELECTRA Clark et al. (2020) and XLM-R. MADLAD Kudugunta et al. (2023) covers 419 languages and trains an 8B language model from scratch with an adapted UL2 objective Tay et al. (2022). Our work is concurrent with the MADLAD400 language model. We distinguish it from: 1) language coverage. Our work covered more than 500 languages, a number comparable to that of encoder-only models and surpassing MADLAD-400 by an additional 100 languages. 2) training methods. We consider continual training to benefit from the learned knowledge of the original models. 3) model architecture. We adopt an open model architecture, i.e., LLaMA, while MADLAD uses UL2.\n' +
      '\n' +
      '### Language Adaptation\n' +
      '\n' +
      'Before the advent of LLMs, diverse approaches are employed to adapt small-scale multilingual language models to new languages. These methods include using adapters Pfeiffer et al. (2020); Ustun et al. (2020); Pfeiffer et al. (2020); Nguyen et al. (2021); Faisal and Anastasopoulos (2022); Yong et al. (2022), vocabulary extension and substitution Chau et al. (2020); Wang et al. (2020); Muller et al. (2020); Pfeiffer et al. (2021); Chen et al. (2023); Downey et al. (2023), leveraging monolingual corpora Ebrahimi and Kann (2021), and utilizing bilingual lexicons Wang et al. (2022).\n' +
      '\n' +
      'While language models have been scaled up significantly, their coverage is limited to a specific set of languages. To address this constraint, various methods have been proposed to expand the applicability of these Large Language Models (LLMs) across a broader range of languages, catering to both general-purpose tasks and specific applications like machine translation. These methods also involve vocabulary extension Cui et al. (2023), continued pretraining and instruction-tuning Yong et al. (2022); Cui et al. (2023); Chen et al. (2023), and parallel corpora exploitation Cahyawijaya et al. (2023); Yang et al. (2023); Zhu et al. (2023); Xu et al. (2023). Despite these efforts, massive language adaptation of LLMs for general-purpose tasks across diverse languages remains an area yet to be thoroughly explored.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We present a pioneering effort in massive language adaptation on LLMs, focusing on extending LLaMA 7B to our model, MaLA-500. This adaptation involves vocabulary extension and continued pretraining with LoRA. Our approach leads to MaLA-500 achieving state-of-the-art in-context learning capabilities, as demonstrated on the task of SIB-200. We release the model weights publicly to facilitate future research. This work marks a substantial advancement in applying LLMs to a diverse range of languages.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'DataWe do not collect more high-resource data and instead use only Glot500-c, which does not put full effort on collecting data for high-resource languages. In addition, the flores200-based SIB-200 evaluation set is included in the training data since Glot500-c includes flores200, but the classification labels are not provided.\n' +
      '\n' +
      'Model SizeOur study limits the maximum model size to 10B parameters. In our future work, we plan\n' +
      '\n' +
      'Figure 3: Detailed results of in-context learning on SIB-200 using MaLA-500. X-axis: the number of languages in different accuracy ranges (%). Y-axis: number of shots.\n' +
      '\n' +
      'to expand to a larger model.\n' +
      '\n' +
      '## Ethical Implications\n' +
      '\n' +
      'LLMs have been known to exhibit biases present in their training data. When extending LLMs to low-resource languages, there is a risk of propagating biases from high-resource languages to underrepresented ones. Careful attention must be paid to mitigate bias and ensure fairness in data collection and model training. The paper aims to make LLMs more accessible for underrepresented languages. Still, there is a risk of creating a digital language divide if certain communities are left out due to limited technological access. Future work would address biases by conducting bias audits on the training data, debiasing the models during generation, and continuously monitoring model outputs.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'This work was funded by the European Research Council (grants #740516 and #758969), EU\'s Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), and the European Union\'s Horizon Europe research and innovation programme under grant agreement No 101070350 and from UK Research and Innovation (UKRI) under the UK government\'s Horizon Europe funding guarantee [grant number 10052546]. The authors wish to acknowledge CSC - IT Center for Science, Finland, for generous computational resources. Shaoxiong Ji and Peiqin Lin acknowledge travel support from ELISE (GA no 951847).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Elmadany, M. Abdul-Mageed, and A. Alcoba Inciare (2022)SERENGETI: massively multilingual language models for africa. arXiv preprint arXiv:2212.10785. Cited by: SS1.\n' +
      '* D. H. Adelani, H. Liu, X. Shen, N. Vassilyev, J. O. Alabi, Y. Mao, H. Gao, and E. A. Lee (2023)SIB-200: a simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. CoRRabs/2309.07445. External Links: Link, 2309.07445 Cited by: SS1.\n' +
      '* K. Ahuja, R. Hada, M. Ochieng, P. Jain, H. Diddee, S. Maina, T. Ganu, S. Segal, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: multilingual evaluation of generative AI. CoRRabs/2303.12528. External Links: Link, 2309.12528 Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Ochieng, R. Hada, P. Jain, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M. Axmed, K. Bali, and S. Sitaram (2023)MEGA: towards a new era of research. External Links: Link Cited by: SS1.\n' +
      '* S. Ahuja, D. Aggarwal, V. Gumma, I. Watts, A. Sathe, M.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE.\n' +
      '* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3505-3506.\n' +
      '* Le Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_.\n' +
      '* Shliazhko et al. (2022) Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2022. mGPT: Few-shot learners go multilingual. _CoRR_, abs/2204.07580.\n' +
      '* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2022. UL2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lemple. 2023a. LLaMA: Open and efficient foundation language models. _CoRR_, abs/2302.13971.\n' +
      '* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Rezenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288.\n' +
      '* Ustun et al. (2020) Ahmet Ustun, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020. Udapter: Language adaptation for truly universal dependency parsing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 2302-2315. Association for Computational Linguistics.\n' +
      '* Wang et al. (2022) Xinyi Wang, Sebastian Ruder, and Graham Neubig. 2022. Expanding pretrained models to thousands more languages via lexicon-based adaptation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 863-877. Association for Computational Linguistics.\n' +
      '* Wang et al. (2020) Zihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. 2020. Extending multilingual BERT to low-resource languages. In _Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020_, volume EMNLP 2020 of _Findings of ACL_, pages 2649-2656. Association for Computational Linguistics.\n' +
      '* Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45.\n' +
      '* Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. A paradigm shift in machine translation: Boosting translation performance of large language models. _CoRR_, abs/2309.11674.\n' +
      '* Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498.\n' +
      '* Yang et al. (2023) Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. _CoRR_, abs/2305.18098.\n' +
      '* Yong et al. (2022) Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M. Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Dragomir Radev, and Vassilina Nikoulina. 2022. BLOOM+1: adding language support to BLOOM for zero-shot prompting. _CoRR_, abs/2212.09535.\n' +
      '* Zhao et al. (2024) Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. LLaMA beyond English: An empirical study on language capability transfer. _arXiv preprint arXiv:2401.01055_.\n' +
      '\n' +
      'Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Extrapolating large language models to non-english by aligning languages. _CoRR_, abs/2308.04948.\n' +
      '\n' +
      '## Appendix A Languages\n' +
      '\n' +
      'The list of languages of Glot500-c used to train MaLA-500 with the number of available sentences and language family information for each language is available in Tables 2, 3 and 4.\n' +
      '\n' +
      '## Appendix B Detailed Results\n' +
      '\n' +
      'Detailed results of few-shot learning evaluation are shown in Tables 5 and 6.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l|c c c|c c c} \\hline \\hline Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c|}{Family} & Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c|}{Family} & Lang & \\multicolumn{1}{c|}{|Sent} & \\multicolumn{1}{c}{Family} \\\\ \\hline hbs\\_Latn & 63411156 & indo1319 & hin\\_Deva & 7046700 & indo1319 & ton\\_Latn & 1216118 & aust1307 \\\\ mal\\_Mlym & 48098273 & drav1251 & kor\\_Hang & 6468444 & kore1284 & tah\\_Latn & 1190747 & aust1307 \\\\ aze\\_Latn & 46300705 & & orry\\_Ory\\_Orya & 6266475 & indo1319 & lat\\_Latn & 1179913 & indo1319 \\\\ guj\\_Gujar & 45738685 & indo1319 & urd\\_Arab & 6009594 & indo1319 & snr\\_Latn & 1172349 & indo1319 \\\\ ben\\_Beng & 43514870 & indo1319 & swa\\_Latn & 5989369 & & ewe\\_Latn & 1161605 & atla1278 \\\\ kan\\_Knda & 41836495 & drav1251 & sqi\\_Latn & 5526836 & indo1319 & ben\\_Latn & 111969 & atla1278 \\\\ tel\\_Telu & 41580525 & drav1251 & bel\\_Cyrl & 5319675 & indo1319 & efi\\_Latn & 1082621 & atla1278 \\\\ ml\\_Latn & 40654838 & afro1255 & afr\\_Latn & 5157787 & indo1319 & bis\\_Latn & 1070170 & indo1319 \\\\ fra\\_Latn & 39197581 & indo1319 & nno\\_Latn & 4899103 & indo1319 & om\\_Latn & 1067699 \\\\ spa\\_Latn & 37286756 & indo1319 & tat\\_Cyrl & 4708088 & turk1311 & haw\\_Latn & 1062491 & aust1307 \\\\ eng\\_Latn & 36122761 & indo1319 & ast\\_Latn & 4683554 & indo1319 & hmo\\_Latn & 1033636 & pidg1258 \\\\ fil\\_Latn & 33493255 & aust1307 & mon\\_Cyrl & 4616960 & mong1349 & kat\\_Geor & 1004297 & kart1248 \\\\ nob\\_Latn & 32869205 & indo1319 & hbs\\_Cyrl & 4598073 & indo1319 & pag\\_Latn & 983637 & aust1307 \\\\ rus\\_Cyrl & 31787973 & indo1319 & nau\\_Latn & 4368483 & afro1255 & loz\\_Latn & 964418 & atla1278 \\\\ deu\\_Latn & 31015993 & indo1319 & sna\\_Latn & 4019596 & atla1278 & fry\\_Latn & 957422 & indo1319 \\\\ tur\\_Latn & 29184662 & turk1311 & msa\\_Latn & 3929084 & & may\\_Mymt & 945180 & sino1245 \\\\ pan\\_Guru & 29052537 & indo1319 & som\\_Latn & 3916769 & norg1255 & ads\\_Latn & 944715 & indo1319 \\\\ mar\\_Deva & 28748897 & indo1319 & spr\\_Cyrl & 3864091 & indo1319 & run\\_Latn & 943828 & atla1278 \\\\ por\\_Latn & 27824391 & indo1319 & mlg\\_Latn & 3715802 & & pub\\_Arab & 899895 & indo1319 \\\\ nld\\_Latn & 25061426 & indo1319 & zul\\_Latn & 3580113 & atla1278 & rar\\_Latn & 894515 & aust1307 \\\\ ara\\_Arab & 24524122 & arz\\_Arab & 3488224 & afro1255 & fij\\_Latn & 887134 & aust1307 \\\\ zho\\_Hani & 24143786 & nya\\_Latn & 3409030 & atla1278 & wls\\_Latn & 882167 & aust1307 \\\\ ita\\_Latn & 23539857 & indo1319 & tam\\_Taml & 3388255 & drav1251 & ckb\\_Arab & 874441 & indo1319 \\\\ ind\\_Latn & 23018106 & aust1307 & hat\\_Latn & 3226932 & indo1319 & ven\\_Latn & 860249 & atla1278 \\\\ ell\\_Grek & 2203328 & indo1319 & zu\\_Latn & 3223485 & turk1311 & zsm\\_Latn & 859947 & aust1307 \\\\ bul\\_Cyrl & 21823004 & indo1319 & vol\\_Latn & 3205510 & atla1278 & ch\\_Cyrl & 859863 & turk1311 \\\\ swe\\_Latn & 20725883 & indo1319 & uz\\_Cyrl & 3029947 & turk1311 & lua\\_Latn & 854359 & atla1278 \\\\ ces\\_Latn & 20376340 & indo1319 & cos\\_Latn & 3015055 & indo1319 & que\\_Latn & 838486 \\\\ isl\\_Latn & 19547941 & indo1319 & als\\_Latn & 2954874 & indo1319 & sag\\_Latn & 771048 & atla1278 \\\\ pol\\_Latn & 19339945 & indo1319 & ann\\_Ehti & 2862985 & afro1255 & guw\\_Latn & 767918 & atla1278 \\\\ non\\_Latn & 19190217 & indo1319 & sun\\_Latn & 2586011 & aust1307 & bre\\_Latn & 748954 & indo1319 \\\\ dam\\_Latn & 19174573 & indo1319 & war\\_Latn & 2584810 & aust1307 & toi\\_Latn & 745385 & atla1278 \\\\ hun\\_Latn & 18800025 & ural1722 & div\\_Tha & 2416867 & indo1319 & pus\\_Arab & 731992 & indo1319 \\\\ tgk\\_Cyrl & 18659517 & indo1319 & yor\\_Latn & 2392359 & atla1278 & che\\_Cyrl & 728201 & nash1245 \\\\ spr\\_Latn & 18371769 & indo1319 & fao\\_Latn & 2365271 & indo1319 & pis\\_Latn & 714783 & indo1319 \\\\ fas\\_Arab & 18277593 & uzu\\_Cyrl & 2293672 & turk1311 & kon\\_Latn & 685194 \\\\ ceb\\_Latn & 18149215 & aust1307 & smo\\_Latn & 2290439 & aust1307 & oss\\_Cyrl & 683517 & indo1319 \\\\ heb\\_Hebr & 18128962 & afro1255 & bak\\_Cyrl & 2264196 & turk1311 & hbyw\\_Armn & 679819 & indo1319 \\\\ hrv\\_Latn & 17882932 & indo1319 & io\\_Latn & 2106531 & aust1307 & iso\\_Latn & 658789 & atla1278 \\\\ glg\\_Latn & 17852274 & indo1319 & uso\\_Latn & 2100708 & atla1278 & nan\\_Latn & 656389 & sino1245 \\\\ fin\\_Latn & 16730388 & ural1272 & rrmi\\_Latn & 2046850 & aust1307 & lub\\_Latn & 654390 & atla1278 \\\\ slv\\_Latn & 15719210 & indo1319 & hm\\_Latn & 1903898 & lim\\_Latn & 625078 & indo1319 \\\\ vie\\_Latn & 15697827 & aust1305 & asm\\_Beng & 1882353 & indo1319 & tuk\\_Latn & 649411 & turk1311 \\\\ mkd\\_Cyrl & 14717004 & indo1319 & hi\\_Latn & 1798875 & aust1307 & tir\\_Eti & 649117 & afro1255 \\\\ slk\\_Latn &\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:11]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c|c|c c c} \\hline \\hline Lang & [Sent] & Family & Lang & [Sent] & Family & Lang & [Sent] & Family \\\\ \\hline alt\\_Cyrl & 95148 & turk1311 & mmy\\_Lattn & 50581 & atla1278 & csy\\_Lattn & 34126 & sinon1245 \\\\ rmn\\_Grek & 94533 & indo1319 & gkp\\_Lattn & 50549 & mand1469 & azb\\_Arab & 33758 & turk1311 \\\\ miq\\_Lattn & 94343 & misu1242 & kat\\_Lattn & 50424 & kart1248 & csy\\_Lattn & 33743 & indo1319 \\\\ kaa\\_Cyrl & 88815 & turk1311 & bijn\\_Lattn & 49068 & ausu1307 & tpm\\_Lattn & 33517 & atla1278 \\\\ kos\\_Lattn & 88603 & ausu1307 & ac\\_Lattn & 48886 & maya1287 & quw\\_Lattn & 33449 & quec1387 \\\\ grn\\_Lattn & 87568 & & dtp\\_Lattn & 48468 & ausu1307 & rmy\\_Cyrl & 33351 & indo1319 \\\\ lhu\\_Lattn & 87255 & sino1245 & lam\\_Lattn & 46853 & atla1278 & ix\\_Lattn & 33289 & maya1287 \\\\ lzh\\_Hani & 86035 & sino1245 & bik\\_Lattn & 46561 & & mbb\\_Lattn & 33240 & ausu1307 \\\\ ajp\\_Arab & 83297 & afo1255 & bph\\_Lattn & 46454 & maya1287 & pfl\\_Lattn & 33148 & indo1319 \\\\ cmn\\_Hani & 80745 & sino1245 & phm\\_Lattn & 45862 & atla1278 & pcd\\_Lattn & 32867 & indo1319 \\\\ gcf\\_Lattn & 80737 & indo1319 & ntw\\_Lattn & 45716 & indo1319 & tpm\\_Lattn & 32863 & arti1236 \\\\ rmn\\_Cyrl & 79925 & indo1319 & quh\\_Lattn & 45566 & quec1387 & suz\\_Deva & 32811 & sinon1245 \\\\ kijh\\_Cyrl & 79262 & turk1311 & jtw\\_Cyrl & 45379 & indo1319 & gcr\\_Lattn & 32676 & indo1319 \\\\ rng\\_Lattn & 78177 & atla1278 & rue\\_Cyrl & 45369 & indo1319 & jbo\\_Lattn & 32619 & arti1236 \\\\ mgh\\_Lattn & 78117 & atla1278 & eml\\_Lattn & 44630 & indo1319 & tbz\\_Lattn & 32264 & atla1278 \\\\ xmv\\_Lattn & 77896 & ausu1307 & ac\\_Arab & 44505 & afro1255 & bam\\_Lattn & 32150 & mand1469 \\\\ ige\\_Lattn & 77114 & atla1278 & tob\\_Lattn & 44473 & guai1249 & prk\\_Lattn & 32085 & ausu1305 \\\\ rmy\\_Lattn & 76991 & indo1319 & ach\\_Lattn & 43974 & nilo1247 & jam\\_Lattn & 32048 & indo1319 \\\\ srm\\_Lattn & 76884 & indo1319 & vpe\\_Lattn & 43076 & ural1272 & twx\\_Lattn & 32028 & atla1278 \\\\ bak\\_Lattn & 76809 & turk1311 & npi\\_Deva & 43072 & indo1319 & nmf\\_Lattn & 31997 & sinon1245 \\\\ gur\\_Lattn & 76151 & atla1278 & tok\\_Lattn & 42820 & arti1236 & caq\\_Lattn & 31903 & ausu1305 \\\\ idu\\_Lattn & 75106 & atla1278 & sgs\\_Lattn & 42467 & indo1319 & rop\\_Lattn & 31889 & indo1319 \\\\ yom\\_Lattn & 74818 & atla1278 & lij\\_Lattn & 42447 & indo1319 & tca\\_Lattn & 31852 & tciu1244 \\\\ tdx\\_Lattn & 74430 & ausu1307 & myv\\_Cyrl & 42147 & ural1272 & yan\\_Lattn & 31775 & misu1242 \\\\ mzn\\_Arab & 73719 & indo1319 & ith\\_Lattn & 41873 & ausu1307 & xav\\_Lattn & 31765 & nucl1710 \\\\ cfm\\_Lattn & 70227 & sino1245 & tat\\_Lattn & 41640 & turk1311 & bih\\_Deva & 31658 & \\\\ zpa\\_Lattn & 69237 & otom1299 & lfn\\_Lattn & 41632 & arti1236 & cuk\\_Lattn & 31612 & chib1249 \\\\ kbd\\_Cyrl & 67914 & abk1242 & cgg\\_Lattn & 41196 & atla1278 & kj\\_Lattn & 31471 & maya1287 \\\\ lao\\_Lano & 66966 & tta\\_Lattn & 41188 & atla1278 & hne\\_Deva & 31465 & indo1319 \\\\ nap\\_Lattn & 65826 & indo1319 & gwp\\_Lattn & 41174 & ausu1307 & wbm\\_Lattn & 31394 & ausu1305 \\\\ qub\\_Lattn & 64973 & quec1387 & ile\\_Lattn & 40984 & arti1236 & zlm\\_Lattn & 31345 & ausu1307 \\\\ oke\\_Lattn & 64508 & atla1278 & ium\\_Lattn & 40683 & hmon1336 & tui\\_Lattn & 31161 & atla1278 \\\\ ote\\_Lattn & 64224 & otom1299 & teo\\_Lattn & 40203 & nilo1247 & ifb\\_Lattn & 30980 & ausu1307 \\\\ bsb\\_Lattn & 63634 & ausu1307 & kia\\_Lattn & 40035 & atla1278 & iz\\_Lattn & 30894 & atla1278 \\\\ ogo\\_Lattn & 61901 & atla1278 & crh\\_Cyrl & 39985 & turk1311 & rug\\_Lattn & 30857 & ausu1307 \\\\ abn\\_Lattn & 61830 & atla1278 & crh\\_Lattn & 39896 & turk1311 & aka\\_Lattn & 30704 & atla1278 \\\\ ldi\\_Lattn & 61827 & atla1278 & enm\\_Lattn & 39809 & indo1319 & pxm\\_Lattn & 30698 & book1242 \\\\ ay\\_Lattn & 61570 & ayma1253 & sat\\_Dck & 39614 & ausu1305 & kmm\\_Lattn & 30671 & sinon1245 \\\\ gom\\_Deva & 61140 & indo1319 & mrad\\_Lattn & 38993 & ausu1307 & mcm\\_Lattn & 30666 & afro1255 \\\\ bba\\_Lattn & 61123 & atla1278 & cac\\_Lattn & 38812 & maya1287 & ifa\\_Lattn & 30621 & ausu1307 \\\\ aln\\_Lattn & 60989 & indo1319 & nnj\\_Lattn & 38611 & hmon1336 & dln\\_Lattn & 30620 & sinon1245 \\\\ leh\\_Lattn & 59944 & atla1278 & ksh\\_Lattn & 38130 & indo1319 & ext\\_Lattn & 30605 & indo1319 \\\\ ban\\_Lattn & 59805 & ausu1307 & ik\\_Lattn & 38071 & atla1278 & ksd\\_Lattn & 30550 & ausu1307 \\\\ ace\\_Lattn & 59333 & ausu1307 & sba\\_Lattn & 38040 & cent2225 & mzh\\_Lattn & 30517 & mata1289 \\\\ pes\\_Arab & 57511 & indo1319 & zom\\_Lattn & 37013 & sino1245 & llb\\_Lattn & 30480 & atla1278 \\\\ skg\\_Lattn & 57228 & ausu1307 & bqc\\_Lattn & 36881 & manda1469 & hra\\_Lattn & 30472 & sinon1245 \\\\ ary\\_Arab & 56933 & afro1255 & bim\\_Lattn & 3635 & atla1278 & mwn\\_Lattn & 30432 & cent225 \\\\ hus\\_Lattn & 56176 & maya1287 & rue\\_Lattn &\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>