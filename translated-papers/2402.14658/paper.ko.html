<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _OpenCodeInterpreter_: 코드 생성과 통합\n' +
      '\n' +
      '실행 및 정제\n' +
      '\n' +
      ' Tianyu Zheng\\({}^{1}\\), Ge Zhang\\({}^{1,2}\\), Tianhao Shen\\({}^{1}\\), Xueling Liu\\({}^{1}\\),\n' +
      '\n' +
      '빌 유첸린\\({}^{3}\\), 제후\\({}^{1,4}\\), 원후첸\\({}^{1,2}\\), 상유\\({}^{1,5}\\)**\n' +
      '\n' +
      '({}^{1}\\)Multimodal Art Projection Research Community, \\({}^{2}\\)University of Waterloo,\n' +
      '\n' +
      'Allen Institute of Artificial Intelligence, \\({}^{4}\\)HKUST, \\({}^{5}\\)IN.AI Research\n' +
      '\n' +
      '{zhengtianyu0428, xiangyue.work}@gmail.com, ge.zhang@uwaterloo.ca\n' +
      '\n' +
      '[https://opencodeinterpreter.github.io](https://opencodeinterpreter.github.io)\n' +
      '\n' +
      '동등 기부금.교신작성자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델의 도입은 코드 생성을 상당히 발전시켰다. 그러나 오픈 소스 모델은 GPT-4 코드 해석기와 같은 고급 시스템의 실행 능력과 반복적인 개선이 부족한 경우가 많다. 이를 해결하기 위해 코드를 생성, 실행 및 반복 정제하기 위해 설계된 오픈 소스 코드 시스템 계열인 _OpenCodeInterpreter_를 소개한다. 68K 멀티턴 인터랙션을 특징으로 하는 데이터셋인 CodeFeedback에 의해 지원되는 _OpenCodeInterpreter_는 동적 코드 정제를 위해 실행과 휴먼 피드백을 통합한다. HumanEval, MBPP, EvalPlus의 향상된 버전과 같은 주요 벤치마크 전반에 걸친 _OpenCodeInterpreter_에 대한 포괄적인 평가는 탁월한 성능을 보여줍니다. 특히, _OpenCodeInterpreter_-33B는 HumanEval과 MBPP의 평균(및 플러스 버전)에서 83.2(76.4)의 정확도를 달성하여 GPT-4의 84.2(76.2)와 매우 유사하고 GPT-4로부터 합성된 인간 피드백으로 91.6(84.6)으로 더욱 상승한다. _OpenCodeInterpreter_는 GPT-4 코드 해석기와 같은 오픈소스 코드 생성 모델과 독점 시스템 간의 격차를 가져온다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '코드 생성은 수십 년 동안 컴퓨터 과학 내에서 중추적인 도전이었다. 최근, 코드 생성의 경관은 광범위한 코드 말뭉치[20, 18, 17, 16, 19, 21, 22, 23, 24]에서 사전 훈련된 대규모 언어 모델(LLM)의 출현으로 혁명을 일으켰다. 이러한 모델은 사용자 의도에 정확하게 일치하는 코드를 생성하는 놀라운 능력을 보여 소프트웨어 개발에 대한 실질적인 지원을 제공했다[14].\n' +
      '\n' +
      '미리 훈련된 코드 모델, 명령어-t의 기능을 실행하려면\n' +
      '\n' +
      '도 1: HumanEval에 대한 _OpenCodeInterpreter_ 및 그것의 pass@1 정확도에 대한 개요. 적절한 피드백으로 _OpenCodeInterpreter_-33B는 GPT-4 Code Interpreter에 필적하는 성능을 달성한다.\n' +
      '\n' +
      '발달. 예를 들어, CodeAlpaca Chaudhary(2023)는 21개의 시드 태스크들을 기초로서 활용하는, ChatGPT에 자기-인스트럭션 Wang 등(2023)을 적용함으로써 자동으로 생성되는 20K 코드 명령어들을 포함한다. LLMs Luo et al.(2023)의 코딩 숙련도를 더욱 향상시키기 위해 Code Evol-Instruct를 도입하는데, CodeAlpaca가 제공하는 데이터셋을 기반으로 초기 코드 명령어들의 복잡도를 풍부하게 하기 위해 다양한 휴리스틱을 적용하는 방법이다. 한편, MagicCoder Wei et al.(2023)은 다양한 범위의 오픈 소스 코드 스니펫으로부터 영감을 소싱하는 새로운 코딩 도전을 생성하기 위해 강력한 LLM을 채용한다. 추가적으로, WaveCoder Yu et al.(2023)은 코드 명령 데이터를 생성하기 위한 LLM 생성기-식별자 프레임워크를 구현하여, 데이터 생성 프로세스에 대한 커스터마이징 및 제어를 제공한다.\n' +
      '\n' +
      '이러한 발전에도 불구하고, 현재 코드 모델은 개선을 위해 피드백을 활용하는 능력에 의해 제약을 받는다. 본질적으로 피드백은 (1) 실행 출력 및 진단을 포함하는 _실행 피드백_와 (2) 사용자로부터의 후속 안내 또는 지시를 포함하는 _인간 피드백_의 두 가지 형태를 가질 수 있다. 실행 피드백은 모델이 구문 및 논리적 오류를 수정할 수 있도록 하는 데 중요한 역할을 하며, 인간 피드백은 모델을 사용자 명령을 더 잘 이해하는 데 도움이 되어 사용자 기대와 더 밀접하게 일치하는 솔루션의 생성을 촉진한다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 우리는 코드를 생성, 실행 및 반복적으로 정제하기 위해 설계된 오픈 소스 코드 시스템의 패밀리인 _OpenCodeInterpreter_를 제안한다. _ OpenCodeInterpreter_는 사용자, 코드 모델 및 컴파일러 간의 68K 멀티턴 상호 작용을 특징으로 하는 구축된 Code-Feedback 데이터셋에 대해 학습된다. _ OpenCodeInterpreter_는 실행과 인간 피드백을 모두 고유하게 통합하며, 컴파일러 진단을 사용하여 오류와 인간 통찰력을 수정하여 코드 생성을 개선한다. 이 접근법을 통해 _OpenCodeInterpreter_는 기술적으로 건전하고 사용자 요구 사항에 밀접하게 일치하는 솔루션을 생성하여 전체 성능을 크게 높일 수 있다.\n' +
      '\n' +
      'HumanEval Chen et al. (2021), MBPP Austin et al. (2021), 및 EvalPlus Liu et al. (2023)의 그들의 증강된 대응물들과 같은 널리 인식된 벤치마크들에 대한 _OpenCodeInterpreter_에 대한 우리의 철저한 평가는 품질 및 기능성의 예시적인 표준들을 달성하면서, 코드를 생성하고 반복적으로 정제하는 그것의 우수한 능력을 강조한다. 놀랍게도, _OpenCodeInterpreter_-33B는 HumanEval과 MBPP의 평균(및 플러스 버전)에서 83.2(76.4)의 인상적인 정확도를 확보하여 GPT-4의 84.2(76.2)와 동등한 성능을 보여준다. 또한, GPT-4로부터 합성된 인간 피드백으로 증강될 때 _OpenCodeInterpreter_의 성능은 91.6(84.6)으로 현저하게 증가한다. 따라서 OpenCodeInterpreter_는 코드 생성에서 새로운 벤치마크를 설정하여 GPT-4 Code Interpreter와 같은 오픈 소스 모델과 정교한 독점 시스템 간의 성능 격차를 효과적으로 줄인다.\n' +
      '\n' +
      '## 2 Code-Feedback\n' +
      '\n' +
      '이 섹션에서, 우리는 _OpenCodeInterpreter_를 트레이닝하도록 설계된 코드 명령어 튜닝 데이터세트, 코드-피드백(도 2)의 생성을 상세히 설명한다. 코드피드백은 특정 기준을 충족하도록 조작된다: **1) 다양하고 도전적인 실제 세계 쿼리:** 데이터 세트는 다양성과 복잡성을 모두 제시하면서 실제 세계 코딩 작업에서 파생된 광범위한 쿼리를 포함해야 한다. **2) 멀티턴 대화 구조:** 코드-피드백은 멀티턴 대화로 구성되며, 컴파일러의 출력 및 진단을 포함하는 실행 피드백과 사용자의 추가 안내 또는 지시로 구성된 인간 피드백의 두 가지 유형의 피드백을 통합한다. **3) 인터리빙된 텍스트 및 코드 응답:** 각각의 응답은 자연 언어 설명을 코드 스니펫과 혼합하는 응답을 제공하여 코딩 쿼리를 해결하는 전체론적 접근법을 제공할 것으로 예상된다.\n' +
      '\n' +
      '이러한 데시데라타를 충족하는 데이터 세트를 조립하기 위해 5가지 별개의 방법을 사용했다. 이 다섯 가지 범주의 예는 부록 E에서 찾을 수 있다. 우리의 쿼리의 출처는 두 가지 주요 범주로 나뉜다: _a 다양한 오픈 소스 데이터세트_ 및 _coding challenge from LeetCode_. 다음 하위 섹션에서는 두 데이터 소스에서 앞서 언급한 세 가지 기준을 충족하기 위해 데이터 구성 방법을 개발하는 방법에 대해 논의한다.\n' +
      '\n' +
      '오픈소스 데이터의### 코딩 질의\n' +
      '\n' +
      '본 논문에서는 Magicoder-OSS-Instruct1, Python code subset of ShareGPT2, Magicoder-Evol-Instruct3, Evol-Instruct-Code4의 4가지 오픈소스 코드 명령어 튜닝 데이터 집합으로부터 287k개의 쿼리를 취합하였다. LLM은 컴파일된 데이터 세트 내의 각 코드 쿼리와 해당 응답을 1에서 5까지의 복잡도 점수로 평가하며, 시드 세트에 대해 4 또는 5의 등급을 가진 가장 어려운 쿼리만 유지되어 가장 어려운 지침에 초점을 맞춘다. 선택의 견고성을 보장하기 위해, 이 필터링 동작은 두 개의 별개의 프롬프트(부록 A에 세부됨)로 반복되어 최종 쿼리 선택의 복잡성을 확고히 한다. 이 세심한 과정을 통해 156k의 고품질 단일 턴 코드 지침이 도전적인 쿼리 풀로 나타났다. 이 자료 편찬에 대한 자세한 통계는 부록 A에 나와 있다.\n' +
      '\n' +
      '이어서, 이 큐레이션된 단일 턴 데이터를 실행 및 인간 피드백이 모두 풍부한 다중 턴 대화로 변환하는 데 사용되는 세 가지 방법을 설명한다.\n' +
      '\n' +
      '**단일 턴 패킹.** 다중 턴 데이터를 만들기 위한 직접적인 접근법은 단일 턴 질의-응답 쌍을 다중 턴 포맷으로 그룹화하는 것이다. 관련 문서들 간의 종속성에 대한 모델 학습을 육성하기 위해 유사한 시퀀스를 통합하는 In-context pre-training 기법 Shi et al.(2023)에서 영감을 받아, 유사한 단일 턴 질의-응답 쌍을 병합하여 다중 턴 다이얼로그를 형성한다.\n' +
      '\n' +
      'BERT-base embedding Devlin et al.(2019)을 활용하여, 우리는 질의를 벡터화된 표현으로 변환한다. 각 질의에 대해 \\(k\\)-최근접 이웃 알고리즘을 사용하여 가장 가까운 4개의 대응점을 식별한다. 이 중에서 2~3개를 무작위로 선택하여 멀티턴 시퀀스를 조립한다. 데이터 고유성을 유지하기 위해, 질의가 이웃으로 선택되면, 이웃 질의로서 향후 선택으로부터 면제되어, 데이터 세트에 걸쳐 단일 명령이 반복되지 않도록 한다. 쿼리의 잠재적 이웃이 이전에 활용된 경우 해당 쿼리는 무시됩니다. 이 방법은 105K 단일 턴 인스턴스에서 파생된 16.6K 멀티 턴 인스턴스를 생성한다.\n' +
      '\n' +
      '**상호작용 시뮬레이션** 진정한 인간 상호작용 데이터를 수집하는 것은 중요한 과제를 제기한다. 실감 있는 코드 해석기 사용 시나리오를 재현하기 위해 GPT-3.5와 GPT-4를 이용한 시뮬레이터를 개발하였으며, 선택된 질의마다 먼저 GPT-3.5에서 코드 스니펫을 추출하여 실행하였다. 이 실행의 결과는 모든 컴파일러 진단과 함께 후속 반응을 이끌어내기 위해 GPT-4에 공급된다. 이 주기는 GPT-4가 올바른 해결책으로 간주되는 것을 전달할 때까지 또는 최대 세 번의 반복에 도달할 때까지 반복된다.\n' +
      '\n' +
      '그 후, 시뮬레이션된 인간 피드백을 상호 작용에 도입한다. 우리는 구문 및 형식, 효율성, 기능, 명확성, 버그, 보안, 호환성, 리소스 사용, 확장성 및 모범 사례와 관련된 문제를 포함하여 10개의 공통 피드백 범주를 사전 정의하며, 각 범주는 부록 B에 자세히 설명되어 있다. 그러면 GPT-4는 시나리오에 가장 적합한 피드백을 선택하고 해당 피드백 범주 내에서 적절한 응답을 생성하도록 프롬프트된다. 이 시뮬레이션된 피드백을 대화 이력에 통합함으로써 GPT-4는 복잡한 사용자 모델 교환을 모방하고 인간 입력에 대한 응답으로 자체 수정을 시연하면서 솔루션을 더욱 개선하도록 권장된다. 이 시뮬레이션 접근법을 통해 51K 사례를 구축했으며 효과적으로 캡을 닫았다.\n' +
      '\n' +
      '그림 2: 제안된 데이터셋의 요약 코드-피드백 구성 및 기존 코드 명령어 튜닝 데이터셋과의 비교. M.T: Multi Turn, E.F: Execute Feedback, H.F: Human Feedback.\n' +
      '\n' +
      '사용자 상호 작용 및 피드백 기반 솔루션 개선의 미묘한 역학을 조정합니다.\n' +
      '\n' +
      '**코드 수정** 모델의 오류 처리 기능을 높이기 위해 데이터 컴파일에서 500개의 특정 오류 수정 상호 작용을 생성하는 집중 단계를 포함합니다. 부록 B에 요약된 대로 GPT-4가 잘못된 코드 스니펫을 생성하도록 프롬프트하여 이를 시작한다. 그러면 모델은 이러한 스니펫을 실행하는 오류 메시지를 수정을 위한 큐로 사용한다. 이 접근법은 개발자가 지속적으로 코드를 디버깅하고 정제하는 실제 코딩 주기를 반영하므로 광범위한 오류 수정 예로 데이터 세트를 풍부하게 한다. 이후 잘못된 코드를 초래한 초기 프롬프트를 올바른 코드 출력 생성을 장려하는 프롬프트로 대체한다. 이 방법은 성공적인 코드 생성과 오류 식별 및 수정 모두에서 모델을 학습하여 문제 해결 능력과 디버깅 프로세스에 대한 이해도를 크게 향상시킨다.\n' +
      '\n' +
      'LeetCode의### 코딩 과제\n' +
      '\n' +
      '**LeetCode Similar Problem.** LeetCode 챌린지를 통해 그들의 스킬을 연마하는 프로그래머들 사이에서 연습으로부터 영감을 이끌어내고, 우리는 TACO 데이터세트로부터 유사한 LeetCode 질문 및 그들의 해결책을 수집한다(Li et al., 2023b). LeetCode5는 태그를 통해 관련 질문을 분류하여 연결된 문제의 추출을 용이하게 한다. TACO는 HumanEval 및 MBPP와 같은 다른 작업 데이터 세트에 의도하지 않은 영향을 방지하기 위해 LeetCode 데이터 세트가 정화되도록 보장합니다. 연관된 LeetCode 질문을 병합하여 303개의 멀티턴 인스턴스를 컴파일하여 다양한 코딩 도전으로 데이터 세트를 풍부하게 한다.\n' +
      '\n' +
      '각주 5: [https://leetcode.com/problemset/](https://leetcode.com/problemset/)\n' +
      '\n' +
      '**LeetCode Follow-up Question.** LeetCode 데이터셋을 더 파헤쳐 시간이나 공간 복잡도가 다르거나 다양한 프로그래밍 언어로 구현된 동일한 질문에 대한 솔루션을 분리합니다. 동일한 LeetCode 질문에 대한 다양한 솔루션을 집계하는 이 프로세스는 200개의 다중 라운드 인스턴스를 생성하여 대안적인 문제 해결 접근법을 보여준다.\n' +
      '\n' +
      '원래의 LeetCode 솔루션은 종종 포괄적인 자연어 설명이 부족하기 때문에 GPT-4를 사용하여 통합된 텍스트 설명 및 코드 스니펫으로 이러한 솔루션을 개선하여 모든 인스턴스를 일관된 형식으로 표준화한다. 이 농축 과정에서 GPT-4를 안내하는 데 사용되는 특정 프롬프트는 부록 C에 자세히 설명되어 응답의 명확성과 교육적 가치를 보장한다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '**Training Setup.** 두 가지 가능한 기본 모델 CodeLlama(Roziere et al., 2023) 및 DeepSeeKoder(Guo et al., 2024) 가변 용량을 선택하여 서로 다른 스케일(7B, 13B, 34B, 70B)에 걸쳐 데이터세트의 보편적인 적용 가능성 및 이점을 설명한다. 우리는 모든 모델에서 균일한 하이퍼파라미터 구성을 유지합니다. 우리는 3시대에 대한 기본 모델을 미세 조정한다. 학습률은 0.05 워밍업 비율과 코사인 스케줄러로 2e-5로 설정된다. 입력 크기의 일관성을 유지하기 위해 4096의 토큰 컷오프 길이를 부과한다.\n' +
      '\n' +
      '미세 조정 프로세스를 최적화하기 위해 위저드코더 110k 데이터 세트의 고품질 단일 턴 데이터와 코드피드백을 2:1의 비율로 전략적으로 결합하며 단일 턴 고품질 데이터와 혼합하면 코딩 능력이 더욱 향상될 수 있다. 이 블렌드는 신중하게 선택되고 더 자세한 내용은 표 2에서 논의된다.\n' +
      '\n' +
      '**Evaluation Setup.** 우리의 평가 프레임워크는 주로 HumanEval(Chen et al., 2021) 및 MBPP(Austin et al., 2021)를 활용하는데, 이는 코드 생성 능력에 대한 엄격한 테스트로 유명한 두 벤치마크이다. 모든 에지 케이스(Liu et al., 2023)를 커버하는 데 있어 그들의 원래 테스트 스위트의 한계를 인정하면서, 우리는 보다 포괄적인 평가를 위해 EvalPlus 프레임워크(Liu et al., 2023)를 사용하여 확장된 버전인 HumanEval+ 및 MBPP+를 추가로 통합한다.\n' +
      '\n' +
      '최근 연구에 요약된 모범 사례(Liu et al., 2023; Chen et al., 2023)에 따라, _OpenCodeInterpreter_\'의 솔루션은 그리디 디코딩을 통해 생성된다. GPT-3.5 Turbo(OpenAI, 2022)와 GPT-4 Turbo(OpenAI, 2023)의 비교를 위해 0의 온도 설정을 유지한다. EvalPlus의 통합 살균 도구는 이러한 솔루션을 후처리한 후 EvalPlus의 도구 세트를 사용하여 4개의 벤치마크에 걸쳐 평가된다.\n' +
      '\n' +
      '*single-turn** 코드 생성을 위해 기본 프롬프트를 캡슐화하는 간단한 명령어를 만들어 모델에 대한 새로운 입력을 형성한다. 정확한 프롬프트는 부록 D에 자세히 설명되어 있으며 EvalPlus의 지침에 따라 pass@1 메트릭을 사용하여 모델의 성능을 평가한다.\n' +
      '\n' +
      '우리의 분석은 반복 피드백을 통해 코드를 정제하는 _OpenCodeInterpreter_의 숙련도를 탐색하기 위해 **멀티턴** 패스 레이트로 확장된다. 평가의 이러한 측면은 실행 결과에 의존한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Model & Size & Type &\n' +
      '\\begin{tabular}{c} Open-source \\\\ Model \\\\ \\end{tabular} & HumanEval (+) & MBPP (+) & Average (+) \\\\ \\hline GPT-4 Turbo & & & & & 85.4 (81.7) & 83.0 (70.7) & 84.2 (76.2) \\\\ + Execution Feedback & - & - & \\(\\circ\\) & & **88.0 (84.2)** & **92.0 (78.2)** & **90.0 (81.2)** \\\\ GPT-3.5 Turbo & & & & 72.6 (65.9) & 81.7 (69.4) & 77.2 (67.7) \\\\ + Execution Feedback & - & - & \\(\\circ\\) & \\(\\circ\\) & 76.8 (70.7) & 87.0 (73.9) & 81.9 (72.3) \\\\ Gemini Pro (Google et al., 2023) & - & - & \\(\\circ\\) & \\(\\circ\\) & 63.4 (55.5) & 72.9 (57.9) & 68.2 (56.7) \\\\ \\hline \\multicolumn{6}{c}{\\(\\sim\\)7B Scale} \\\\ \\hline StarCoder (Li et al., 2023a) & 7B & Base & \\(\\star\\) & \\(\\star\\) & 24.4 (20.7) & 33.1 (28.8) & 28.8 (24.8) \\\\ CodeT5+ (Wang et al., 2023c) & 6B & Base & \\(\\star\\) & \\(\\star\\) & 29.3 (23.8) & 51.9 (40.9) & 40.6 (32.4) \\\\ CodeGen-Mono (Nijkamp et al., 2022) & 6B & Base & \\(\\star\\) & \\(\\star\\) & 29.3 (25.6) & 49.9 (42.1) & 39.6 (33.9) \\\\ Mistral (Jiang et al., 2023) & 7B & Base & \\(\\star\\) & \\(\\circ\\) & 28.7 (23.2) & 50.1 (40.9) & 39.4 (32.1) \\\\ OpenChat (Wang et al., 2023a) & 7B & Instruct & \\(\\star\\) & \\(\\star\\) & 72.0 (67.1) & 62.7 (52.9) & 67.4 (60.0) \\\\ CodeLlama-Python (Roziere et al., 2023) & 7B & Base & \\(\\star\\) & \\(\\circ\\) & 37.8 (34.1) & 57.6 (45.4) & 47.7 (39.8) \\\\ WizardCoder-CL (Luo et al., 2023) & 7B & Instruct & \\(\\circ\\) & 48.2 (40.9) & 56.6 (47.1) & 52.4 (44.0) \\\\ Magicoder-CL (Wei et al., 2023) & 7B & Instruct & \\(\\star\\) & 60.4 (55.5) & 64.2 (52.6) & 62.3 (54.1) \\\\ Magicoders-SL (Wei et al., 2023) & 7B & Instruct & \\(\\star\\) & 70.7 (66.5) & 68.4 (56.6) & 69.6 (61.6) \\\\ \\multicolumn{6}{c}{_OpenCodInterpreter-CL_} & \\multirow{3}{*}{7B} & \\multirow{3}{*}{Instruct} & \\multirow{3}{*}{\\(\\star\\)} & 75.6 (70.1) & 69.9 (60.7) & 72.8 (65.4) \\\\ DeepseeKoder & & & & & & 72.6 (67.1) & 69.9 (60.7) & 72.8 (65.4) \\\\ DeepseeKoder (Guo et al., 2024) & 6.7B & Base & \\(\\star\\) & \\(\\circ\\) & 47.6 (39.6) & 70.2 (56.6) & 58.9 (48.1) \\\\ DeepseeKoder-Instruct & & & & & 73.8 (70.1) & 73.2 (63.4) & 73.5 (66.8) \\\\ + Execution Feedback & & & & & 80.5 (75.6) & 79.9 (70.4) & 80.2 (73.0) \\\\ Magicoder-DS (Wei et al., 2023) & 6.7B & Instruct & \\(\\star\\) & \\(\\star\\) & 66.5 (60.4) & 75.4 (61.9) & 71.0 (61.2) \\\\ Magicoder-S-DS (Wei et al., 2023) & 6.7B & Instruct & \\(\\star\\) & \\(\\star\\) & 76.8 (70.7) & 75.7 (64.4) & 76.3 (67.6) \\\\ \\multicolumn{6}{c}{_OpenCodInterpreter-DS_} & \\multirow{3}{*}{6.7B} & \\multirow{3}{*}{Instruct} & \\multirow{3}{*}{\\(\\star\\)} & 77.4 (72.0) & 73.2 (62.4) & 75.3 (67.2) \\\\ + Execution Feedback & & & & & 76.2 (72.0) & 76.1 (71.9) & 61.7 (75.6) \\\\ + Execution Feedback & & & & & 81.1 (78.7) & 82.7 (72.4) & 81.9 (75.6) \\\\ + Synth. Human Feedback & & & & & 87.2 (**86.6)** & 86.2 (74.2) & 86.7 (80.4) \\\\ + Synth. Human Feedback (Oracle) & & & & & **89.7 (86.6)** & **87.2 (75.2)** & **88.5 (80.9)** \\\\ \\hline \\multicolumn{6}{c}{\\(\\sim\\)13B Scale} \\\\ \\hline CodeGen-Mono (Nijkamp et al., 2022) & 16B & Base & \\(\\star\\) & \\(\\star\\) & 32.9 (27.4) & 52.6 (43.6) & 42.8 (35.5) \\\\ StarCoder (Li et al., 2023a) & 15B & Base & \\(\\star\\) & \\(\\circ\\) & 34.1 (29.3) & 55.1 (46.1) & 44.6 (37.7) \\\\ CodeT5+ (Wang et al., 2023c) & 16B & Base & \\(\\star\\) & \\(\\circ\\) & 31.7 (26.2) & 54.6 (44.4) & 43.2 (35.3) \\\\ CodeLlama-Python (Roziere et al., 2023) & 13B & Base & \\(\\star\\) & \\(\\circ\\) & 42.7 (36.6) & 61.2 (50.9) & 52.0 (43.8) \\\\ \\multicolumn{6}{c}{_OpenCodInterpreter-CL_} & \\multirow{3}{*}{13B} & \\multirow{3}{*}{Instruct} & \\multirow{3}{*}{\\(\\star\\)} & 77.4 (73.8) & 70.7 (59.2) & 74.1 (66.5) \\\\ + Execution Feedback & & & & & **81.1 (76.8)** & **78.2 (67.2)** & **79.7 (72.0)** \\\\ \\hline \\multicolumn{6}{c}{\\(\\sim\\)34B Scale} \\\\ \\hline CodeLlama-Python (Roziere et al., 2023) & 34B & Base & \\(\\star\\) & \\(\\circ\\) & 51.8 (43.9) & 67.2 (52.9) & 59.5 (48.4) \\\\ Speechness-CL-v2.0 (speechness, 2023) & 34B & Instruct & \\(\\star\\) & \\(\\star\\) & 77.4 (71.3) & 72.4 (59.1) & 74.9 (65.2) \\\\ XwinCoder-CL (Xwin-LM, 2023) & 34B & Instruct & \\(\\star\\) & \\(\\star\\) & 75.6 (67.7) & 76.2 (62.4) & 75.9 (65.1) \\\\ Phind-CL-v2 (Phind, 2023) & 34B & Instruct & \\(\\star\\) & \\(\\circ\\) & 71.3 (67.1) & - & - \\\\ WizardCoder-CL (Luo et al., 2023) & 34B & Instruct & \\(\\star\\) & 73.2 (64.6) & 73.2 (59.9) & 73.2 (62.3) \\\\ \\multicolumn{6}{c}{_OpenCodInterpreter-CL_} & \\multirow{3}{*}{34B} & \\multirow{3}{*}{Instruct} & \\multirow{3}{*}{\\(\\star\\)} & 78.0 (72.6) & 73.4 (61.4) & 75.7 (67.0) \\\\ \\multicolumn{6}{c}{_+ Execution Feedback_} & \\multirow{3}{*}{34B} & \\multirow{3}{*}{Instruct} & \\multirow{3}{*}{\\(\\star\\)} & 78.1 (78.7) & 80.2 (67.9) & 81.0 (73.3) \\\\ \\multicolumn{6}{c}{_+ Execution Feedback_} & \\multirow{3}{*}{33B} & \\multirow{*}{Base}{\\(\\star\\)} & \\multirow{3}{*}{\\(\\circ\\)} & 51.2 (44.5) & - & - \\\\ DeepSeeKoder-Instruct & & & & & 81.1 (75.0) &and synthetic human feedback, generated by GPT-4 [20], to simulate real-world coding scenarios and interactions. Specifically, the multi-turn evaluation encompasses three scenarios, offering a holistic view of _OpenCodeInterpreter_\'s capabilities in dynamic code refinement:\n' +
      '\n' +
      '* **실행 피드백:** 여기서, _OpenCodeInterpreter_는 실행 결과 및 컴파일러 진단을 독립적으로 활용하여 오류를 정확히 지적하고 수정하며, 직접 실행 피드백을 기반으로 코드를 정제하는 개발자의 프로세스를 미러링한다.\n' +
      '***합성 인간 피드백:** 이 시나리오에서 GPT-4는 태스크 기술, 초기 모델 응답 및 임의의 실행 피드백을 고려하여 인간 입력을 모방하는 피드백을 생성한다. 이것은 실제 개발자 또는 사용자 상호 작용을 반영하여 미묘한 인간 유사 피드백에 대한 _OpenCodeInterpreter_의 적응성을 테스트한다.\n' +
      '***합성 인간 피드백(Oracle):** 이전 시나리오에서 빌딩, GPT-4는 또한 지상-진실 솔루션에 액세스하여 정확한 피드백에 의해 안내될 때 코드 정제에서 _OpenCodeInterpreter_의 최적 성능에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '각 작업에 대해 모델의 솔루션이 평가를 성공적으로 통과하거나 설정된 최대 2라운드에 도달할 때 코드 생성 및 평가 프로세스가 종료된다. 코드 샘플이 평가에 실패하면 솔루션과 테스트 결과가 모두 개선 프롬프트에 다시 통합됩니다. 평가는 비통과 결과에 대한 세 가지 주요 시나리오를 식별한다: 1) _Exception Handling_: 실행 중에 발생하는 예외 또는 오류를 오류 메시지로 캡처하고 릴레이하여 수정을 위한 직접적인 피드백을 제공한다. 2) _Not-Expected_: 출력들이 예상 결과들로부터 벗어나는 경우들에서, 모델은 테스트 입력들, 예상 출력들, 및 실제 출력들을 포함하는 피드백을 받아 불일치를 강조한다. 3) _Timeout Handling_: 과도한 또는 무한한 런타임들을 갖는 솔루션들로부터의 평가 지연들을 방지하기 위해 타임아웃 임계치를 구현한다. 이 임계값을 초과하면 "실행 시간 초과" 알림이 트리거됩니다.\n' +
      '\n' +
      '##4 주요 결과\n' +
      '\n' +
      '이 섹션에서는 단일 턴 및 다중 턴 코드 생성 설정에서 _OpenCodeInterpreter_ 및 기준선을 보고합니다. 결과는 표 1에 나와 있다.\n' +
      '\n' +
      '### 단회전 코드 생성 결과\n' +
      '\n' +
      '본 논문에서는 HTTP-3.5/4-Turbo[22, 2023], CodeLlama-Python[20], Wizard-Coder[19], Deepseek-Coder[24], CodeT5+[23]와 같은 상위 모델들과 각각 다른 스케일에서 _OpenCodeInterpreter_의 단일 턴 코드 생성 성능을 비교하고, 2024년 2월 10일 현재 EvalPlus 리더보드의 데이터를 활용하여 HumanEval과 MBPP 벤치마크에 대한 _OpenCodeInterpreter_의 성과와 상위 버전인 HumanEval+와 MBPP+에 대한 성과를 살펴본다. 간단한 비교를 위해 서로 다른 모델 척도에 걸친 결과를 하나의 테이블로 통합하여 각 모델 척도와 _OpenCodeInterpreter_의 각 변형 간의 직접적인 성능 비교를 용이하게 한다.\n' +
      '\n' +
      '우리의 실험 분석은 _OpenCodeInterpreter_의 강력한 성능을 보여주며, 몇 가지 구성이 주요 벤치마크와 일치하거나 능가한다. _OpenCodeInterpreter_-DS 33B 변종은 오픈 소스 모델 중 가장 높은 점수를 달성한다. 이러한 성취는 특히 초기 훈련 세트에서 저품질 또는 부정확한 데이터의 상당한 존재를 고려할 때 현저하다.\n' +
      '\n' +
      '### 멀티턴 코드 생성 결과\n' +
      '\n' +
      '이 섹션에서는 반복 정제, 해석기 진단 및 인간 통찰력을 활용하여 다중 회전 상호 작용에서 _OpenCodeInterpreter_의 숙련도를 평가한다.\n' +
      '\n' +
      '우리의 실험 평가는 작업 전반에 걸쳐 공정성과 일관성을 유지하기 위해 반복에 2라운드 제한을 부과한다. 일부 문제는 여러 가지 개선의 이점을 얻을 수 있지만 다른 문제는 더 적은 양을 필요로 합니다. 이 제한은 모델의 반복 기능에 대한 명확한 통찰력을 제공한다. 실행 피드백 시나리오에서 모든 척도에 걸쳐 제안된 모델들은 SOTA 벤치마크에 비해 우수한 성능을 보였으며, _OpenCodeInterpreter_33B 모델은 GPT-4 Turbo의 단일 라운드 점수로 패리티를 달성하여 평가된 코드 모델 중 새로운 SOTA 벤치마크를 수립하였다.\n' +
      '\n' +
      '예산 제약으로 인해, 우리의 인간 피드백 및 인간 피드백(Oracle) 평가는 _OpenCodeInterpreter_ 6.7B 및 _OpenCodeInterpreter_ 33B 모델에 집중한다. 그 결과 Human Feedback의 경우, _OpenCodeInterpreter_ 6.7B 모델이 GPT-4 Turbo의 단일 라운드 점수를 크게 능가하는 반면, Human Feedback(Oracle) 시나리오에서는 HumanEval/MBPP 벤치마크에서 _OpenCodeInterpreter_ 33B 모델의 평균 점수가 90 벤치마크를 현저하게 초과했음을 알 수 있다. 이러한 결과는 소프트웨어 개발 도구의 리더로서 _OpenCodeInterpreter_를 확립하면서 코드 생성 모델을 발전시키는 데 반복적인 피드백과 개선의 중요한 역할을 강조한다. 이러한 개선된 방법을 통해, _OpenCodeInterpreter_는 다양한 피드백에 기반한 뛰어난 적응성과 코드 정제성을 입증할 뿐만 아니라 향후 코드 생성 기술에 대한 새로운 벤치마크를 설정한다.\n' +
      '\n' +
      '### 데이터 소스의 요약\n' +
      '\n' +
      '본 절에서는 다양한 데이터 소스가 _OpenCodeInterpreter_의 성능에 미치는 영향을 체계적으로 탐색한다. 우리는 고품질 단일 턴 데이터와 다양한 다중 턴 피드백 메커니즘이 모델의 코드 생성, 디버깅 및 정제 능력에 미치는 영향을 평가하기 위해 일련의 절제 연구를 수행한다.\n' +
      '\n' +
      '** 고품질 단일 턴 데이터의 영향.** 고품질 단일 턴 데이터가 _OpenCodeInterpreter_의 효능에 미치는 영향을 평가하기 위해 구문 정확성과 논리적 일관성으로 유명한 위저드코더 110K 데이터 세트를 광범위한 다중 턴 데이터 세트에 통합한다. 이 통합은 정확한 단일 턴 코드 생성과 다중 턴 상호 작용에 의해 가능한 고급 반복 개선의 최적 혼합을 식별하려고 한다.\n' +
      '\n' +
      '우리의 실험은 멀티턴 데이터 세트에서 위저드코더 110K 데이터의 비율을 변경하는 6가지 구성에 걸쳐 소프트 타겟 미세 조정 전략을 사용한다. 이러한 구성은 딥시크코더-베이스-6.7B 및 딥시크코더-베이스-33B의 두 가지 버전에서 모델의 성능을 평가하는 위저드코더 데이터세트의 전체 통합에서 전체 제외에 걸쳐 있다.\n' +
      '\n' +
      '본 연구의 결과는 표 1에 나타나 있으며, 고품질 단일 턴 데이터(예: WizardCoder dataset)를 통합하면 모델의 다중 턴 성능이 크게 향상됨을 보여준다. 이러한 전략적 통합은 모델이 단일 턴 작업에 내재된 구문적 정확성과 논리적 일관성에서 이익을 얻도록 하여 후속 턴에서 미묘한 반복적 개선을 위한 능력을 풍부하게 한다. 보다 효과적인 멀티턴 코드 생성 및 정제를 위한 단계를 설정하는 데 있어 고품질 단일턴 입력의 중요한 역할을 드러낸다.\n' +
      '\n' +
      '**다양한 멀티턴 데이터 소스의 이점.** 위저드코더 데이터 세트를 완전히 통합하여 수립된 향상된 기준선에 이어 이 하위 섹션에서는 모델의 개선 및 디버깅 효능에 대한 다양한 데이터 소스의 이점을 조사한다. 단일 회전 패킹, 상호 작용 시뮬레이션 및 코드 수정 데이터를 포함한 다양한 데이터 소스를 개별적으로 및 조합하여 교육 요법에 추가한다.\n' +
      '\n' +
      '단일 턴 패킹, 인터렉션 시뮬레이션 및 코드 수정 데이터를 포함한 이러한 다중 턴 데이터 소스의 사용은 _OpenCodeInterpreter_의 디버깅 및 정제 기능을 개별적으로 및 조합하여 분명히 향상시킨다. 특히, 코드 수정 데이터를 포함하면 오류를 수정하는 데 있어 모델의 효율성이 크게 향상됩니다. 이것은 다양하고 표적화된 훈련 접근법이 정교한 코드 생성 모델의 능력을 발전시키는 데 미치는 심오한 영향을 강조한다. 이러한 접근법을 통해 이러한 모델은 복잡한 코딩 문제를 보다 효과적으로 해결하고 오류를 수정하며 광범위한 피드백 메커니즘을 통해 출력을 정제할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Ratio & E.F & HumanEval (+) & MBPP (+) & Average (+) \\\\ \\hline \\multirow{2}{*}{2:1} & ✗ & 76.2 (72.0) & 73.9 (63.7) & 75.1 (67.9) \\\\  & ✗ & **81.1 (78.7)** & **82.7 (72.4)** & **81.9 (75.6)** \\\\ \\multirow{2}{*}{1:1} & ✗ & **77.3 (72.6)** & 74.6 (62.6) & **76.0 (67.6)** \\\\  & ✗ & 78.0 (72.6) & 78.4 (65.9) & 78.2 (69.3) \\\\ \\multirow{2}{*}{1:2} & ✗ & 75.7 (71.9) & 72.9 (62.9) & 74.3 (67.4) \\\\  & ✗ & 78.7 (75.6) & 77.9 (65.9) & 78.3 (70.8) \\\\ \\multirow{2}{*}{1:3} & ✗ & 76.2 (72.0) & **75.4 (65.4)** & 75.8 (68.7) \\\\  & ✗ & 78.0 (75.0) & 79.2 (69.9) & 78.6 (72.5) \\\\ \\multirow{2}{*}{1:5} & ✗ & 70.7 (67.0) & 73.4 (63.1) & 72.1 (65.1) \\\\  & ✗ & 75.6 (70.7) & 79.2 (67.9) & 77.4 (69.3) \\\\ \\multirow{2}{*}{0:1} & ✗ & 73.8 (68.9) & 73.9 (62.9) & 73.9 (65.9) \\\\  & ✗ & 76.2 (71.3) & 66.7 (**76.6)** & 71.5 (74.0) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 단회전 데이터와 Code-Feedback의 데이터 혼합 비율을 갖는 _OpenCodeInterpreter_의 성능. "E.F"는 실행 피드백의 사용을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Datasets & E.F & Average (+) \\\\ \\hline \\multirow{2}{*}{Single-turn Packing} & ✗ & 75.0 (66.9) \\\\  & ✗ & 77.5 (69.5) \\\\ \\multirow{2}{*}{Interaction Simulation} & ✗ & 75.1 (66.9) \\\\  & ✗ & 78.5 (69.6) \\\\ \\multirow{2}{*}{Single-turn Packing} & ✗ & 74.7 (66.5) \\\\  & ✗ & 78.2 (70.1) \\\\ \\multirow{2}{*}{Single-turn Packing + Interaction} & ✗ & **75.2** (65.4) \\\\  & ✗ & 79.1 (71.3) \\\\ \\multirow{2}{*}{Code-Feedback (Full)} & ✗ & 75.1 (**67.9)** \\\\  & ✗ & **81.9 (75.6)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 증분 데이터 소스 통합과 함께 다른 설정에 걸친 모델의 성능 비교. "E.F"는 실행 피드백의 사용을 나타낸다.\n' +
      '\n' +
      '### 사례연구: 야생동물에서의 코딩 질의\n' +
      '\n' +
      '이 섹션에서는 "야생" 사용자 쿼리에 직면했을 때 _OpenCodeInterpreter_의 작동 역학을 입증하기 위해 세 가지 별개의 사례 연구를 조사한다. 이러한 사례 연구의 동기는 _OpenCodeInterpreter_의 실제 응용 프로그램을 보여주는 것이다.\n' +
      '\n' +
      '주목할 만한 성공 스토리(그림 A8)에서 우리는 1-100 범위 내의 모든 소수를 계산하는 함수를 개발하는 _OpenCodeInterpreter_를 작업했으며, 나중에 용액을 임의의 범위 x-y로 확장했다. 또 다른 추천 가능한 인스턴스(도 A9)는 정규식을 사용하여 IPv6 주소를 검증하기 위해 파이썬 함수를 구현하는 _OpenCodeInterpreter_를 포함하였다. 접근 방식을 반복적으로 개선하는 기능을 입증하는 _OpenCodeInterpreter_는 오류를 식별하고 수정했을 뿐만 아니라 인간의 피드백을 기반으로 솔루션을 향상시켰다. 이 두 가지 사례는 수학적 논리를 이해하고 지정된 기준에 맞게 알고리즘을 동적으로 조정하는 데 있어 _OpenCodeInterpreter_의 강도를 예시한다.\n' +
      '\n' +
      '어려운 경우(그림 A10)는 _OpenCodeInterpreter_가 두 입력 목록의 교차점을 식별하는 기능을 설계하도록 요청받았을 때 발생하여 두 목록에 존재하는 별개의 요소의 튜플을 발생 빈도와 함께 반환했다. 수정시도에 대한 _OpenCodeInterpreter_의 시도에도 불구하고, 오류는 점진적으로 해결되어 궁극적으로 최대 시도 횟수(3회)를 초과하였으며, 이 경우는 여러 도전 오류를 동시에 해결하는 _OpenCodeInterpreter_의 한계를 조명한다.\n' +
      '\n' +
      '이러한 사례 연구를 통해 우리는 _OpenCodeInterpreter_의 능력과 한계에 대한 귀중한 통찰력을 얻는다. 이러한 통찰력은 _OpenCodeInterpreter_에 대한 향후 개선을 안내하는 데 중요하다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '코드용 LLM.LM을 사전 훈련하기 위한 코드 데이터를 포함하는 것이 일반적인 관행이 된다. 예를 들어, PaLM사의 Chowdhery et al. (2023) 사전 트레이닝 데이터의 5%가 코드이고, LaMDA Thoppilan et al. (2022), Galactic Taylor et al. (2022), LLaMA Touvron et al. (2023), Gopher Rae et al. (2021), GPT-NeoX Black et al. (2022)에 대한 이 비율은 각각 13%, 7%, 5%, 3%, 및 8%이다.\n' +
      '\n' +
      '추가적으로, 코드 생성을 위해 전문화된 LLM들이 미리 훈련되었는데, 예를 들어, CodeGen Nijkamp et al. (2022), PanGu-Coder Christopoulou et al. (2022), CodeGeeX Zheng et al. (2023), CodeFuse Di et al. (2023), CodeT5+ Wang et al. (2023), AlphaCode Li et al. (2022), EncoderFried et al. (2022), StarCoder Li et al. (2023), DeepSeek-Coder Guo et al. (2024). 한편, 코드 LLMs는 범용 LLMs, 예를 들어 CodeLlama Roziere 등(2023), WizardCoder Luo 등(2023)으로부터 미세 조정될 수 있으며, 이는 우리가 여기서 취하는 접근법이다. 특화된 LLM에 비해 미세 조정 패러다임은 사전 훈련된 범용 LLM을 활용하여 코드 생성 능력을 향상시키는 방법을 탐색할 수 있게 하는데, 특히 이러한 LLM은 이미 방대한 양의 코드 데이터에 대해 훈련되었기 때문이다.\n' +
      '\n' +
      '반복 코드 생성 및 정제.많은 시퀀스 생성 태스크들에 대해, 종종 생성 품질을 향상시키기 위해 반복 접근법들이 취해진다. 예를 들어, 스크립트 생성 Tandon et al.(2021), 요약 Scheurer et al.(2022), 및 Madaan et al.(2022); Saunders et al.(2022). 특히 Self-Refine Madaan et al.(2023)에서 LLM은 초기 출력들을 생성한 후 피드백을 생성하고, LLM은 피드백으로 출력들을 반복적으로 업데이트한다. 범용 LLM 설정에 중점을 둔 반면 코드 생성 작업에 중점을 둡니다. LLMs를 이용한 코드 생성에 관하여, DebugBench Tian et al.(2024)은 런타임 피드백을 통합하는 것이 코드 LLMs의 디버깅 성능을 향상시킨다는 것을 관찰한다. 가장 최근의 관련 작업은 StepCoder Dou et al. (2024)이며, 컴파일러 피드백 Le et al. (2022); Shojaee et al. (2023)은 원래 탐색 문제를 더 쉬운 하위 작업의 시퀀스로 나눈다. 그러나 우리의 접근 방식은 강화 학습에 의존하지 않고 중간 세대에 접근할 수 있어 훈련이 더 쉽고 안정적이다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '결론적으로 _OpenCodeInterpreter_는 이전에 확인된 오픈소스 모델 간의 격차와 GPT-4 Code Interpreter와 같은 독점 시스템의 고급 기능을 연결하는 코드 생성 분야에서 상당한 발전을 나타낸다. 컴파일러 진단 및 인간 피드백을 반복적인 정제 프로세스에 통합함으로써 _OpenCodeInterpreter_는 기존의 일회성 생성 접근법을 능가할 뿐만 아니라 오픈 소스 모델에서 이전에 볼 수 없었던 적응성 및 정밀도 수준을 도입한다. 광범위한 멀티턴 상호작용과 함께 코드-피드백의 도입은 진화하는 사용자 의도 및 복잡한 코딩 작업에 대응하여 코드를 동적으로 정제하기 위해 _OpenCodeInterpreter_에 더 많은 권한을 부여한다.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'Code-Feedback의 사용과 함께 _OpenCodeInterpreter_의 개발 및 배포는 책임 있는 사용을 보장하기 위해 윤리적 고려 사항을 취한다. 우리는 데이터 세트가 편향되거나 불공정한 결과의 전파를 방지하기 위해 다양한 범위의 코딩 스타일, 문제 도메인 및 사용자 시나리오를 나타내도록 노력했다. _OpenCodeInterpreter_가 사용자 입력을 기반으로 코드를 생성하고 정제할 수 있다는 점을 감안할 때 민감한 정보를 노출하거나 보안 취약점을 만들지 않도록 데이터셋을 엄격하게 확인한다. _ OpenCodeInterpreter_는 비전문가 및 개발자의 진입 장벽을 낮추어 코딩을 민주화할 수 있는 잠재력을 가지고 있다. 접근성을 극대화하기 위해 모든 코드, 모델 및 데이터 세트를 오픈 소스합니다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '_OpenCodeInterpreter_는 자동화된 코드 생성에서 중요한 발전을 도입하지만, 시스템에 내재된 한계와 이를 지원하는 Code-Feedback을 인정하는 것이 중요하다. _OpenCodeInterpreter_는 다국어 코드 생성을 지원하고 광범위한 프로그래밍 컨텍스트를 이해하도록 설계되었지만, 그 성능은 상이한 언어 및 특정 도메인에 걸쳐 달라질 수 있다. _OpenCodeInterpreter_는 다양한 코딩 작업을 해석하고 응답하는 데 탁월하지만, 극도로 복잡하거나 모호한 사용자 의도와 함께 어려움을 겪을 수 있다. 이러한 의도를 정확하게 포착하고 해결할 수 있는 능력은 코드의 피드백에서 모델의 현재 이해와 데이터의 특수성에 의해 제한된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. _ ArXiv preprint_, abs/2108.07732.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang et al. 2023. Qwen technical report. _ ArXiv preprint_, abs/2309.16609.\n' +
      '* Workshop on Challenges & Perspectives in Creating Large Language Models_, pages 95-136, virtual+Dublin. 컴퓨터 언어학과의 연관성\n' +
      '* Chaudhary (2023) Sahil Chaudhary 2023. 코드 알파카: 코드 생성을 위한 명령어-후속 라마 모델. [https://github.com/sahil280114/codealpaca] (https://github.com/sahil280114/codealpaca). 접속: 2024-02-13\n' +
      '* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. 코드로 훈련된 대형 언어 모델들을 평가한다. _ ArXiv preprint_, abs/2107.03374.\n' +
      '* Chen et al. (2023) Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. 2023. 자가 디버그에 대용량 언어 모델을 가르치는 단계 _ ArXiv preprint_, abs/2304.05128.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Christopoulou et al. (2022) Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, et al. 2022. Pangu-coder: function-level 언어 모델링과의 프로그램 합성 _ ArXiv preprint_, abs/2207.11280.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. [Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association for Human Language Technologies, Volume 1(Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota. 컴퓨터 언어학과의 연관성\n' +
      '* Di et al. (2023) Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, et al. 2023. Codefuse-13b: 사전 훈련된 다국어 코드 대언어 모델 _ ArXiv preprint_, abs/2310.06266.\n' +
      '* Dou et al. (2024) Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, et al. 2024. Step-coder: 컴파일러 피드백으로부터의 강화 학습으로 코드 생성을 개선한다. _ ArXiv preprint_, abs/2402.01391.\n' +
      '*Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Inocoder : 코드 채우기 및 합성을 위한 생성 모델 _ ArXiv preprint_, abs/2204.05999.\n' +
      '\n' +
      '깃허브 2023. Github 부조종. [https://github.com/features/copilot] (https://github.com/features/copilot). 접속: 2024-02-14\n' +
      '* Google et al. (2023) Gemini Google, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: High capable multiimodal model의 가족. _ ArXiv preprint_, abs/2312.11805.\n' +
      '* Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024: Deepseek-codder: large language model meets programming-the rise of code intelligence. _ ArXiv preprint_, abs/2401.14196.\n' +
      '* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _ ArXiv preprint_, abs/2310.06825.\n' +
      '* Le et al. (2022) Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. 코더: 사전 훈련된 모델과 심층 강화 학습을 통한 마스터링 코드 생성_ 신경 정보 처리 시스템_, 35:21314-21328의 발전.\n' +
      '* Li et al. (2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023a. 스타코드: 출처가 당신과 함께 있기를! _ ArXiv preprint_, abs/2305.06161.\n' +
      '* Li 등(2023b) Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, 및 Ge Li. 2023b. 타코: 알고리즘 코드 생성 데이터 세트의 주제 _ ArXiv preprint_, abs/2312.14852.\n' +
      '* Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Kimeno, Agustin Dal Lago, et al. 2022. 경쟁-level code generation with alphacode. _ Science_, 378(6624):1092-1097.\n' +
      '* Liu et al. (2023) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. 채팅GPT에서 생성한 코드가 정말 맞습니까? 코드 생성을 위한 대규모 언어 모델에 대한 엄격한 평가. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xibo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. 위저드코더: 에볼루-인스트럭션으로 코드 대형 언어 모델들을 엠파워링하는 단계 _ ArXiv preprint_, abs/2306.08568.\n' +
      '* Madaan et al. (2022) Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang. 2022. 배포 후 GPT-3를 개선하기 위한 메모리 지원 프롬프트 편집. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2833-2861, 아랍에미리트 아부다비. 컴퓨터 언어학과의 연관성\n' +
      '* Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Self-feedback을 이용한 반복적 정제; _ ArXiv preprint_, abs/2303.17651.\n' +
      '*Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. 코데겐: 멀티턴 프로그램 합성을 갖는 코드에 대한 개방형 대형 언어 모델 _ ArXiv preprint_, abs/2203.13474.\n' +
      '* OpenAI(2022) OpenAI. 2022. ChatGPT: 대화용 언어 모델 최적화. [https://openai.com/blog/chatgpt/] (https://openai.com/blog/chatgpt/). 2월 14일에 접속했다. 2024년\n' +
      '* OpenAI(2023) OpenAI. 2023. Gpt-4 기술 보고서\n' +
      '* Phind(2023) Phind. 2023. Phind/phind-codellama-34b-v2.\n' +
      '* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis and insights from training gopher. _ ArXiv preprint_, abs/2112.11446.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: code에 대한 오픈 파운데이션 모델 _ ArXiv preprint_, abs/2308.12950.\n' +
      '* Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. 인간 평가자를 보조하기 위한 자기 평가 모델들. _ ArXiv preprint_, abs/2206.05802.\n' +
      '* Scheurer et al. (2022) Jeremy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, KyungHyun Cho, and Ethan Perez. 2022. 자연어 피드백을 갖는 언어 모델 학습. _ ArXiv preprint_, abs/2204.14146.\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 2023. 부정맥 사전 훈련 : 문서 경계를 넘어선 언어 모델링 _ ArXiv preprint_, abs/2310.10638.\n' +
      '* Shojaee et al. (2023) Parshin Shojaee, Aneesh Jain, Sindhu Tipirmeni, and Chandan K Reddy. 2023. 심층 강화 학습을 이용한 실행 기반 코드 생성. _ ArXiv preprint_, abs/2301.13816.\n' +
      '* 무음(2023) 무음. 2023. 말 없는 코델라마-34b-v2.0.\n' +
      '* Tandon et al. (2021) Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: 오류 피드백을 통한 스크립트의 상호 학습을 위한 데이터셋. _ ArXiv preprint_, abs/2112.07867.\n' +
      '* Taylor 등(2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: large language model for science. _ ArXiv preprint_, abs/2211.09085.\n' +
      '\n' +
      'Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: 대화 애플리케이션을 위한 언어 모델들 __ ArXiv preprint_, abs/2201.08239.\n' +
      '* Tian et al. (2024) Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. 디버그벤치: 대용량 언어모델의 디버깅 능력 평가_ ArXiv preprint_, abs/2401.04621.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _ ArXiv preprint_, abs/2302.13971.\n' +
      '* Wang et al. (2023a) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023a. 오픈챗: 양질의 데이터가 혼합된 오픈소스 언어 모델의 발전 ArXiv preprint_, abs/2309.11235.\n' +
      '* Wang et al. (2023b) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. 자가 지시: 언어 모델을 자가 생성 지시와 정렬합니다. _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, July 9-14, 2023_, pages 13484-13508.\n' +
      '* Wang et al. (2023c) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023c. Codel5+: 코드 이해와 생성을 위한 코드 대언어 모델을 개방한다. _ ArXiv preprint_, abs/2305.07922.\n' +
      '* Wang et al. (2023d) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023d. Codel5+: 코드 이해와 생성을 위한 코드 대언어 모델을 개방한다. _ ArXiv preprint_, abs/2305.07922.\n' +
      '* Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. 매직코더 : 소스코드만 있으면 된다. _ ArXiv preprint_, abs/2312.02120.\n' +
      '* Xwin-LM (2023) Xwin-LM. 2023. Xwin-lm.\n' +
      '* Yu et al. (2023) Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Quifeng Yin. 2023. 웨이브코더: 정제된 데이터 생성과 함께 광범위하고 다재다능한 향상된 명령어 튜닝_ ArXiv preprint_, abs/2312.14187.\n' +
      '* Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: Humananeval-x. _ ArXiv preprint_, abs/2303.17568.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      '아래는 초기 시드 데이터 세트에 적용된 데이터 필터링 프로세스의 개요이며 그림 A1은 각 필터링 단계 이후의 데이터 양을 요약한다.\n' +
      '\n' +
      '그림 A2의 파이 차트는 필터링 후 데이터 세트에서 프로그래밍 언어의 분포를 보여준다.\n' +
      '\n' +
      '그림 A1: 데이터 필터링 프로세스 개요 및 해당 데이터 수량\n' +
      '\n' +
      '그림 A2: 필터링 데이터셋에서 프로그래밍 언어의 분포\n' +
      '\n' +
      '데이터 수집을 위한 상호작용 시뮬레이션\n' +
      '\n' +
      '멀티턴 실행 피드백과 멀티턴 휴먼 피드백에 각각 사용되는 프롬프트를 설명한다.\n' +
      '\n' +
      '멀티턴 실행 피드백을 위한 시스템 프롬프트\n' +
      '\n' +
      '당신은 AI 코드 인터프리터입니다.\n' +
      '\n' +
      '파이썬 코드를 실행하여 사용자가 다양한 작업을 수행할 수 있도록 돕는 것이 목표입니다.\n' +
      '\n' +
      'You should:\n' +
      '\n' +
      '1. 사용자의 요구 사항을 주의 깊게 이해하고 편지에 따라 이해한다.\n' +
      '\n' +
      '2. 수행할 계획에 대한 간략한 설명을 제공하고 제공된 기능을 호출하여 코드를 실행합니다.\n' +
      '\n' +
      '3. 실행 출력에 기초한 결과 분석을 제공한다.\n' +
      '\n' +
      '4. 에러가 발생한 경우, 고치도록 한다.\n' +
      '\n' +
      '5. 사용자와 동일한 언어로 응답.\n' +
      '\n' +
      '멀티턴 휴먼 피드백을 위한 시스템 프롬프트\n' +
      '\n' +
      '최신 생성된 코드에 피드백을 제공하는 사용자입니다. 대화에서 사용 가능한 코드가 발견되지 않은 경우, 어시스턴트가 코드를 생성하도록 권장하기 위해 피드백을 제공해야 한다. 참고: 당신의 피드백은 2개의 짧은 문장 안에 있어야 합니다.\n' +
      '\n' +
      '다음과 같은 피드백 유형을 참고할 수 있다.\n' +
      '\n' +
      '1. **Syntax and Formatting**: 구문 오류 확인, 일관성 없는 포맷팅, 및 가독성 및 유지성에 대한 표준 코딩 스타일 준수를 제안한다.\n' +
      '\n' +
      '2. **효율**: 시간 복잡도를 감소시키고, 루프를 최적화하거나, 보다 효율적인 데이터 구조를 제안하는 것과 같이 더 나은 성능을 위해 최적화될 수 있는 코드의 부분들을 식별한다.\n' +
      '\n' +
      '3. **기능 향상**: 코드를 보다 기능적이거나 사용자 친화적으로 만들 수 있는 추가적인 특징들 또는 향상들을 제안함.\n' +
      '\n' +
      '4. **코드 명확성 및 문서화**: 코드를 보다 이해하기 쉽고 쉽게 유지하기 위해 코드 주석 및 문서의 개선을 권장한다.\n' +
      '\n' +
      '5. **버그 식별**: 코드에서 잠재적인 버그 또는 논리적 오류를 지적하고 이를 해결하는 방법을 제안한다.\n' +
      '\n' +
      '6. **보안 개선**: 코드의 보안 취약점을 강조 표시하고 보안을 강화하기 위한 모범 사례를 제안함.\n' +
      '\n' +
      '7. **호환성 및 테스트**: 코드를 상이한 환경 또는 플랫폼과 보다 호환되도록 하고 보다 포괄적인 테스트 시나리오를 제안하는 것에 관한 조언.\n' +
      '\n' +
      '8. **자원 최적화**: 코드가 필요한 것보다 더 많은 자원을 사용할 수 있는 영역을 식별하고(메모리 또는 CPU와 같이) 최적화를 제안한다.\n' +
      '\n' +
      '9. **확장성**: 더 큰 데이터 세트 또는 더 많은 사용자를 다루기 위해 코드가 어떻게 더 확장 가능하게 만들어질 수 있는지에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '10. **Best Practices에 대한 준수**: 코드를 보장하는 것은 사용되는 언어 또는 프레임워크에 특정한 베스트 프랙티스를 따른다.\n' +
      '\n' +
      '출력은 이와 같은 json 형식이어야 합니다.\n' +
      '\n' +
      ' [noitemsep]\n' +
      '\n' +
      '"만족": "발생된 코드에서 달성된 포인트들",\n' +
      '\n' +
      '"not_satisfied": "상기 포인트들은 생성된 코드에서 아직 달성되지 않은 것",\n' +
      '\n' +
      '"피드백": "실제 피드백. 당신의 피드백은 2 SHORT SENTENCES와 함께 있어야 한다. 피드백은 \'not_satisfied\' 필드에 포함된 포인트로부터 와야 한다. 이전 대화에서 사용 가능한 코드가 발견되지 않으면 여기서 어시스턴트에게 코드를 생성하도록 요청할 수 있다."는 AI 코드 해석자이다.\n' +
      '\n' +
      '파이썬 코드를 생성하고 실행하는 것이 목표입니다.\n' +
      '\n' +
      '코드에는 다음 오류 유형 중 하나 이상이 포함되어야 합니다.\n' +
      '\n' +
      '1. 구문 오류: 이러한 유형의 오류는 코드가 프로그래밍 언어의 문법 규칙을 위반할 때 발생한다. 예를 들어 괄호 또는 따옴표를 닫는 것을 잊거나 키워드 철자를 잘못 입력합니다.\n' +
      '\n' +
      '2. 논리적 오류: 이 오류들은 당신이 해결하고 있는 문제에 대한 오해가 있을 때 당신의 코드로 몰래 들어가, 코드가 충돌하지 않고 실행되었음에도 불구하고 잘못된 결과를 초래한다. 예를 들어, 숫자를 합산하여 숫자의 평균을 계산하지만 숫자의 수로 나누는 것을 잊는다.\n' +
      '\n' +
      '3. 타입 에러: 부적절한 타입의 오브젝트에 동작이 적용될 때 이러한 에러가 발생한다. 예를 들어, 먼저 정수를 문자열로 변환하지 않고 문자열과 정수를 연결하려고 시도합니다.\n' +
      '\n' +
      '4. 이름 오류: 코드가 정의되지 않은 변수 또는 함수 이름을 참조하려고 시도할 때 발생합니다. 예를 들어, 선언되지 않은 변수를 인쇄하려고 합니다.\n' +
      '\n' +
      '5. 타임아웃 오류: 이 오류는 논리 결함 또는 절대 거짓이 되지 않는 조건으로 인해 코드가 절대 끝나지 않는 루프에 고착될 때 발생합니다. 프로그래밍에서 이러한 오류는 응용 프로그램이 무기한 매달리게 하여 리소스를 소모하고 적절하게 처리되지 않으면 잠재적으로 충돌로 이어질 수 있습니다. 예를 들어, 특정 조건이 변경되기를 기다리는 루프를 기록하지만, 조건은 루프 내에서 결코 업데이트되지 않는다.\n' +
      '\n' +
      'NOTE:\n' +
      '\n' +
      '1. 생성된 코드에서 실수를 해야 합니다!\n' +
      '\n' +
      '2. 내부의 오류에 대해서는 설명하지 않는다. 그냥 당신의 생각과 코드를 평범하게 써라.\n' +
      '\n' +
      '3. 어떤 형태로든 (예를 들어, 텍스트/코드/주석으로) 잘못된 코드를 쓰고 있다고 말하지 마세요. 올바른 코드를 쓰고 있지만 여전히 오류를 인식하지 못하는 척하십시오.\n' +
      '\n' +
      '## 부록 C 자연어 설명 생성\n' +
      '\n' +
      '우리는 GPT-4를 사용하여 코드에 대한 설명을 생성하기 위해 다음 프롬프트를 사용한다.\n' +
      '\n' +
      '```\n' +
      '1GPT-4를 이용하여 자연어 설명을 생성하기 위한 프롬플.\n' +
      '2. 여기 사용자와 프로그래머 어시스턴트 사이의 일련의 대화들을 포함하는 리스트가 있다.\n' +
      '3. 이전 다이얼로그에 이어서, 사용자는 최신 문제를 제기하였다.\n' +
      '4. 어시스턴트는 이제 이전 대화 및 최신 문제에 기초하여 올바른 코드를 조작했다.\n' +
      '5. 프로그래머 어시스턴트라고 가정하면 코드 전에 텍스트를 추가하십시오.\n' +
      '6. 이 텍스트의 목적은 최신 문제에 대응하고 뒤따르는 코드를 도입하는 것이다.\n' +
      '7. 이 텍스트는: 코드에서 사용되는 언어, 코드에서 사용되는 알고리즘, 단계별 구현 개요, 및 다른 관련 콘텐츠를 포함할 수 있다.\n' +
      '8. @@Code를 참조하기 위해 "다음 코드", "마이 코드", "마이 솔루션"과 같은 문구를 사용할 수 있다.\n' +
      '9. 응답에는 코드 앞에 추가한 텍스트만 포함되어야 합니다.\n' +
      '10. 당신의 유일한 임무는 텍스트를 쓰는 것이고, 결코 코드를 수정하거나 무언가를 상기시키는 것이 아니다. 이전 대화와 문제를 다시 설명하지 마십시오.\n' +
      '11.\n' +
      '```\n' +
      '\n' +
      '```\n' +
      '@@Previous Dialogues{previous dialogues}\n' +
      '```\n' +
      '\n' +
      '```\n' +
      '@@RecentProblem: {recent problem}\n' +
      '```\n' +
      '\n' +
      '거기에 문자를 추가하세요.\n' +
      '\n' +
      '```\n' +
      '@@Code: {code}\n' +
      '```\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '우리는 피드백을 생성할 때 인간의 행동을 모방하기 위해 GPT 모델을 사용한다. GPT 모델에 제공된 프롬프트는 다음과 같이 표시된다.\n' +
      '\n' +
      '```\n' +
      '인간 피드백 진화를 모방하는 GPT 모델에 대한 프롬프트\n' +
      '```\n' +
      '\n' +
      '프로그래밍 문제에 대한 코드를 작성한 프로그래머에게 지침을 제공하는 작업을 수행합니다. 당신의 역할은 인간과 유사한 반응을 모방하고 표준 솔루션과 관찰된 실행 결과를 기반으로 코드를 수정하기 위한 제안을 제공하는 것입니다. @@캐노니컬 솔루션의 내용을 직접 공개하거나 "캐노니컬 솔루션"과 같은 용어를 언급해서는 안 되며, 코드를 직접 작성하는 것은 삼가야 한다. 기존 코드와 기능을 철저히 검토하는 것으로 시작하십시오. 제공된 @@캐노니컬 솔루션과 @@기존 코드를 비교합니다. 논리, 접근 방식 또는 구현의 불일치에 유의하십시오. @기존 코드를 실행하여 얻은 @@실행 결과를 분석합니다. 예상 출력의 오류, 예상치 못한 동작 또는 편차를 식별합니다. @@캐노니컬 솔루션 및 @@실행 결과 둘 다의 통찰력을 기반으로 잠재적인 에지 사례, 최적화 기회 또는 대안적 접근 방식을 고려하십시오. 각 제안의 이면에 있는 근거를 설명하면서 명확하고 이해할 수 있는 방식으로 지침을 제공한다. 실제 코드 솔루션을 제공하는 것을 자제하되, 대신 개념 수정이나 전략에 초점을 맞춥니다. 프로그래머가 코딩 기술을 향상시킬 수 있도록 건설적인 피드백을 제공합니다. 여러분의 역할은 솔루션을 직접 구현하지 않고 프로그래밍에 있어 인간과 같은 지침과 전문성을 시뮬레이션하는 것입니다. 3문장 이내로 답변 부탁드립니다.\n' +
      '\n' +
      '```\n' +
      '@@Problem{original prompt}@@Existing Code{sanitized code}@@Execution Result{execution results}@@Canonical Solution{canonical solution}@@Guidance\n' +
      '```\n' +
      '\n' +
      'Frompt for GPT models mimicking human feedback evolution```\n' +
      '프로그래밍 문제에 대한 코드를 작성한 프로그래머에게 지침을 제공하는 작업을 수행합니다. 당신의 역할은 인간과 유사한 반응을 모방하고 관찰된 실행 결과를 기반으로 코드를 수정하기 위한 제안을 제공하는 것입니다. 당신은 코드를 직접 쓰는 것을 삼가야 한다. 기존 코드와 기능을 철저히 검토하는 것으로 시작하십시오. @기존 코드를 실행하여 얻은 @@실행 결과를 분석합니다. 예상 출력의 오류, 예상치 못한 동작 또는 편차를 식별합니다. @@실행 결과의 통찰력을 기반으로 잠재적인 에지 사례, 최적화 기회 또는 대안적 접근 방식을 고려하십시오. 각 제안의 이면에 있는 근거를 설명하면서 명확하고 이해할 수 있는 방식으로 지침을 제공한다. 실제 코드 솔루션을 제공하는 것을 자제하되, 대신 개념 수정이나 전략에 초점을 맞춥니다. 프로그래머가 코딩 기술을 향상시킬 수 있도록 건설적인 피드백을 제공합니다. 여러분의 역할은 솔루션을 직접 구현하지 않고 프로그래밍에 있어 인간과 같은 지침과 전문성을 시뮬레이션하는 것입니다. 3문장 이내로 답변 부탁드립니다.\n' +
      '``@@기존 코드{sanitized code}@@Execution Result{execution result}@@Guidance```Data Collection에 사용된 방법의 예\n' +
      '\n' +
      '여기에서 우리는 데이터 수집 프로세스에서 유사한 쿼리 패킹, 오픈 소스 데이터의 코딩 쿼리에 대한 인간 피드백 시뮬레이션 및 코드 수정(섹션 2.1), LeetCode(섹션 2.2)의 코딩 문제에 대한 유사한 문제 패킹 및 후속 Q&A를 포함한 각 방법의 예를 나열했다.\n' +
      '\n' +
      '도 A3: 도전적인 쿼리 풀들에 대한 유사한 쿼리 패킹의 예시.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:20]\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      '도시의 **스카이라인**은 멀리서 보았을 때 그 도시의 모든 건물들이 이루는 실루엣의 외곽 윤곽이다. 모든 건물의 위치와 높이를 감안할 때 이러한 건물에 의해 형성된 **스카이라인**을 집합적으로 반환합니다.\n' +
      '\n' +
      '각 건물의 기하학적 정보는 \'건물[i] = [좌, 우, 높이]\'인 배열 \'건물\'에서 주어진다.\n' +
      '\n' +
      '*\'lefii\'는 \'ith\' 건물의 좌측 모서리의 x 좌표이다.\n' +
      '*\'right\'은 \'ith\' 건물의 오른쪽 모서리의 x 좌표이다.\n' +
      '* \'높이\'는 \'ith\' 건물의 높이이다.\n' +
      '\n' +
      '당신은 모든 건물들이 \'0\'의 높이에서 절대적으로 평평한 표면에 접지된 완벽한 직사각형이라고 가정할 수 있다.\n' +
      '\n' +
      '*skyline**은 "key points"의 리스트로 표현되어야 한다.\n' +
      '\n' +
      '**sorted by their x-coordinate** form[[X1,y1],[X2,y2],...]]. 각 키 포인트는 리스트의 마지막 포인트를 제외한 스카이라인 내의 일부 수평 세그먼트의 왼쪽 끝점이며, 이는 항상 y-coordinate \'0\'을 가지며 가장 오른쪽 빌딩이 끝나는 곳에 스카이라인의 종료를 표시하는 데 사용된다. 가장 왼쪽 건물과 가장 오른쪽 건물 사이의 모든 지면은 스카이라인의 윤곽의 일부여야 합니다.\n' +
      '\n' +
      '**주:** 출력 스카이라인에는 동일한 높이의 연속 수평선이 없어야 합니다. 예를 들어 [...[2 3], [4 5], [7], [11 5], [12 7] ,...]\'는 허용되지 않는다; 높이 5의 세 라인은 다음과 같이 최종 출력에서 하나로 병합되어야 한다: [...[2 3], [4 5], [12 7],...]\n' +
      '\n' +
      '[실시예는 잘린다]\n' +
      '\n' +
      '**Constraints:**\n' +
      '\n' +
      '*\'1 \\(<=\\) 건물. 길이 \\(<=\\) 104\'\n' +
      '\n' +
      '* \'빌딩\'은 감소하지 않는 순서로 \'lefii\'에 의해 정렬된다.\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '다음 코드에서 우리는 주어진 건물 집합에서 도시 스카이라인을 계산하는 문제를 다룬다. 각 건물은 x좌표 경계와 높이로 정의되며, 우리의 목표는 멀리서 집합적으로 관찰될 때 이러한 건물의 윤곽을 구성하는 것이다.\n' +
      '\n' +
      '사용되는 알고리즘은 다음의 단계로 요약될 수 있다:\n' +
      '\n' +
      '[스텝이 잘린다]\n' +
      '\n' +
      '내 솔루션은 입력을 효율적으로 처리하도록 설계되었으며 중복 수평선을 정렬하고 제거하는 문제와 관련된 문제의 제약 조건에 부합합니다. 스카이라인 문제를 해결하는 코드로 진행합시다.\n' +
      '\n' +
      '**python\n' +
      '\n' +
      '소트 컨테이너에서 정렬 목록을 가져옵니다.\n' +
      '\n' +
      'def getSkyline(buildings):\n' +
      '\n' +
      'points = []\n' +
      '\n' +
      'for l, r, h in buildings:\n' +
      '\n' +
      'points.append((l, -h))\n' +
      '\n' +
      'points.append((r, h))\n' +
      '\n' +
      'points.sort()\n' +
      '\n' +
      'heights = SortedList([0])\n' +
      '\n' +
      'prev = 0\n' +
      '\n' +
      '스카이라인 = []\n' +
      '\n' +
      'for x, h in points:\n' +
      '\n' +
      'if h\\(<\\)0:\n' +
      '\n' +
      'heights.add(h)\n' +
      '\n' +
      'else:\n' +
      '\n' +
      'heights.remove(h)\n' +
      '\n' +
      'cur = 높이[-1]\n' +
      '\n' +
      'if. prev:\n' +
      '\n' +
      'skyline.append([x, cur])\n' +
      '\n' +
      'prev = cur\n' +
      '\n' +
      '... 스카이라인을 반환하다.\n' +
      '\n' +
      '**Continued**\n' +
      '\n' +
      '2D 평면의 X축에 몇 개의 정사각형이 떨어져 있다.\n' +
      '\n' +
      '2D 정수 배열 \'위치\'가 주어지며, 여기서 \'위치[i] = [왼쪽, sideLength!]\'는 X 좌표 \'lefii\'와 정렬된 왼쪽 에지와 함께 드롭되는 \'ideLength\'의 측면 길이를 갖는 \'ith\' 정사각형을 나타낸다.\n' +
      '\n' +
      '각 사각형은 착륙한 사각형 위의 높이에서 한 번에 하나씩 떨어집니다. 그런 다음 **가 다른 사각형**의 측면에 놓이거나 x축**에 놓일 때까지 아래(음의 Y 방향)로 떨어집니다. 다른 정사각형의 왼쪽/오른쪽을 빗질하는 정사각형은 그 위에 착지하는 것으로 간주되지 않는다. 착륙하면 제자리에서 얼어서 이동할 수 없습니다.\n' +
      '\n' +
      '각 정사각형이 떨어진 후에는 현재 가장 높은 정사각형 스택의 ** 높이를 기록해야 합니다.\n' +
      '\n' +
      'Return _an integer array _ans\'_where _ans[i]\' _는 \'ith _square_\'를 드롭한 후 위에서 설명한 높이를 나타낸다[예들이 잘린다].\n' +
      '\n' +
      '**Constraints:**\n' +
      '\n' +
      '*\'1 \\(<=\\) 위치.length \\(<=\\\\) 1000\'\n' +
      '*\'1 \\(<=\\)left\\(<\\)108\'\n' +
      '*\'1 \\(<=\\) sideLength \\(<=\\) 106\'\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '제공된 코드 조각에서, 나는 각 정사각형을 2D 평면에 떨어뜨린 후 가장 높은 스택의 높이를 계산하는 솔루션을 구현했다. 알고리즘은 그들의 왼쪽 에지 및 측면 길이로 표현되는 사각형들의 리스트를 처리하고, 이러한 주요 단계들을 따른다:\n' +
      '\n' +
      '[스텝이 잘린다]\n' +
      '\n' +
      '**python\n' +
      '\n' +
      'def fallingSquares(positions):\n' +
      '\n' +
      'ans = []\n' +
      '\n' +
      '간격 = []\n' +
      '\n' +
      'for p in positions:\n' +
      '\n' +
      'L, size = p\n' +
      '\n' +
      'R = L + 크기\n' +
      '\n' +
      'h = 크기\n' +
      '\n' +
      'for l, R2 in intervals:\n' +
      '\n' +
      'if R2\\(>\\)L and R\\(>\\)R2:\n' +
      '\n' +
      'h = max(h, size + h2)\n' +
      '\n' +
      'maxHeight = max((h2 for h2, R2 in intervals),\n' +
      '\n' +
      'default=0)\n' +
      '\n' +
      'ans.append(max(max(Height, h))\n' +
      '\n' +
      'intervals.append((h, R))\n' +
      '\n' +
      'return ans\n' +
      '\n' +
      '...\n' +
      '\n' +
      '...\n' +
      '\n' +
      '도 A6: LeetCode 문제들에 대한 유사한 문제 패킹의 예시.\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      '이 문제를 해결해 주시기 바랍니다.\n' +
      '\n' +
      '멀리 떨어진 은하계에서는 공화국과 드로이드군 사이에 끊임없는 전투가 벌어진다. 드로이드 군대는 공화국에 대한 마지막 공격을 개시하기로 결정했다. N명의 우주비행사가 있습니다.\n' +
      '\n' +
      '처음에 Si({}^{+}\\)$ 전투기는 (x_{}_{i}, y_{}_{i}}}에 위치한다. 모든 우주 전투기는 양의 X 방향으로 등속 V 단위/sec로 움직인다.\n' +
      '\n' +
      '즉, (x_{{}_{i}, y_{{}_{i}})에서의 전투기는 1초 만에 (x_{{}_{i}+V, y_{{}_{i}}})로 이동한다. Si({}^{+}\\){th}$ 우주전투기는 주파수 f_{{}_{i}에서 적의 정보를 방송한다.\n' +
      '\n' +
      '그 공화국은 요다를 가지고 있기 때문에 인위적으로 지능화된 드로이드 세력을 두려워하지 않는다. 요다는 특별한 힘을 가지고 있다. 그는 언제든지 드로이드 군대의 영역을 선택할 수 있고 하나의 특정 주파수 F를 차단할 수 있다. 이 힘은 하나의 제약을 가지고 있다; 이것은 X축을 가로지르는 양쪽 방향을 향해 열려 있는 두 개의 측면 비경계 축 평행 직사각형 박스 형태로만 적용될 수 있다(명확성을 위해 아래 이미지를 참조). 주파수(F)가 차단되면 주파수(F)를 갖는 영역의 모든 우주 전투기들은 통신할 수 없다.\n' +
      '\n' +
      '우주 전투기의 초기 위치와 속도를 감안할 때 다음 형식의 질문에 답해야 합니다.\n' +
      '\n' +
      '유영태\n' +
      '\n' +
      '여기서 YU, YD는 YODA가 시간 T에서 주파수를 차단할 수 있는 y축의 경계이다.\n' +
      '\n' +
      '쿼리에 의해 기술된 영역에서, 시작으로부터 시간 T초 후에, 요다가 원하는 하나의 주파수(F)를 선택할 수 있다면, 그가 차단할 수 있는 최대 통신 횟수는 얼마인가?\n' +
      '\n' +
      'Input Format\n' +
      '\n' +
      '각 시험 사례는 다음과 같다. 첫 번째 행은 3개의 공간 분리된 정수 N - 공간-전투기의 수, Q - 답해야 하는 질의 수, V - 단일 공간으로 분리된 공간-전투기의 속도를 포함한다.\n' +
      '\n' +
      '각각 3개의 공간 분리된 정수 x_{{}_i}, y_{{}_{i}, f_{{}_{i}}를 포함하는 N개의 선이 뒤따르며, x좌표, y좌표 및 S\\({}^{+}}$)$ 선박이 각각 방송하는 주파수를 나타낸다. 다음 Q 라인들 각각은 YU, YD, T를 각각 나타내는 3개의 공간 분리된 정수를 포함한다. 더 명확하게 하려면 그림 참조\n' +
      '\n' +
      '참고: 경계 상의 점도 계산해야 합니다.\n' +
      '\n' +
      'Output Format\n' +
      '\n' +
      '각 쿼리에 대해 결과를 나타내는 단일 정수를 출력합니다.\n' +
      '\n' +
      'Constraints\n' +
      '\n' +
      '1 \\(<\\)N \\(<=\\) 50000\n' +
      '\n' +
      '1 \\(<\\) Q \\(<=\\) 30000\n' +
      '\n' +
      '1 \\(<\\)\\(\\sim\\) V \\(<=\\) 10000\n' +
      '\n' +
      'S-10\\({}^{\\circ}\\)\\(\\neq\\) Sc x_{{}_{i}\\(<=\\) S10\\(\\times\\){9} S\n' +
      '\n' +
      'S-10\\({}^{\\circ}\\)\\(\\neq\\) S\\(\\sim\\) y_{{}_{i}\\(<=\\) S10\\(\\times\\){9} S\n' +
      '\n' +
      '1 \\(<\\\\)\\(=\\) T_{{}_{i}\\(<=\\) S10\\({}^{\\circ}\\){9} S\n' +
      '\n' +
      'S-10\\({}^{\\circ}\\){9} S\\(<=\\) YU\\(<=\\) S10\\({}^{\\circ}\\){9} S\n' +
      '\n' +
      'S-10\\({}^{\\circ}\\){9} S\\(<=\\) YD\\(<=\\) S10\\({}^{\\circ}\\){9} S\n' +
      '\n' +
      '1 \\(<\\\\)\\(=\\)T \\(<\\)10000\n' +
      '\n' +
      'YU\\(>=\\) YD\n' +
      '\n' +
      '[샘플 입력, 출력 및 설명이 잘립니다]\n' +
      '\n' +
      '**(Continued) Assistant:**\n' +
      '\n' +
      '귀하의 요청을 해결하기 위해, 저는 이 성간 충돌 시나리오에 제시된 상황을 평가하는 파이썬 코드를 개발했습니다. 이 프로그램은 드로이드 군대의 우주전투기들의 초기 위치, 주파수, 속도 등에 대한 정보를 입력하고, 다중 질의를 처리하여 요다가 지정된 우주 영역 내에서 주어진 시간에 차단할 수 있는 최대 통신 횟수를 결정하도록 설계되었다.\n' +
      '\n' +
      '다음은 단계별 구현 개요입니다.\n' +
      '\n' +
      '[스텝이 잘린다]\n' +
      '\n' +
      '다음 코드는 위에서 자세히 설명한 로직을 캡슐화하고 필요한 쿼리를 실행하여 원하는 출력을 생성합니다.\n' +
      '\n' +
      '\\(\\cdots\\)python\n' +
      '\n' +
      'import bisect\n' +
      '\n' +
      '컬렉션에서 기본식을 가져오다\n' +
      '\n' +
      '(N, Q_)=지도(int, input(.split())\n' +
      '\n' +
      'a = 기본값(리스트)\n' +
      '\n' +
      'y = list()\n' +
      '\n' +
      'for - in range(N):\n' +
      '\n' +
      '(_, y_, freq)= map(int, input(.split())\n' +
      '\n' +
      'a[freq].append(y_)\n' +
      '\n' +
      'y_append(y_)\n' +
      '\n' +
      'a = {freq: sorted(y) for (freq, y) in a.items() if len(y) \\(>\\\\)1}\n' +
      '\n' +
      'y = sorted(y)\n' +
      '\n' +
      'res = []\n' +
      '\n' +
      'for - in range(Q):\n' +
      '\n' +
      '(y_max, y_min, T) = map(int, input(.split())\n' +
      '\n' +
      'res = 0\n' +
      '\n' +
      'index_start = bisect.bisect_left(y, y_min)\n' +
      '\n' +
      'if y[index_start] \\(<=\\) y_max:\n' +
      '\n' +
      'lres = 1\n' +
      '\n' +
      'for freq in a.\n' +
      '\n' +
      'index_start = bisect.bisect_left(a[freq],\n' +
      '\n' +
      'y_min)\n' +
      '\n' +
      'index_stop = bisect.bisect_right(a[freq],\n' +
      '\n' +
      'y_max)\n' +
      '\n' +
      'lres = max(lres, index_stop - index_start)\n' +
      '\n' +
      'res.append(lres)\n' +
      '\n' +
      'print(*res, sep="u")\n' +
      '\n' +
      '...\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      '이 질문의 어려움은 무엇입니까?\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '설명된 문제의 어려움은 HARD로 간주될 수 있다.\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      '이 문제를 해결할 다른 방법이 있나요?\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '주어진 문제에 대한 대안적인 해결책을 제공하기 위해, 범위 질의와 관련된 문제의 계산 복잡도를 최적화하기 위해 자주 사용되는 제곱근 분해 알고리즘을 활용했다.\n' +
      '\n' +
      '[나머지 컨텐츠 및 생성된 코드가 잘려 있음]\n' +
      '\n' +
      '그림 A7: LeetCode 문제에 대한 후속 질의응답의 예.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '그림 A9: 문자열이 정규식을 가진 유효한 IPv6 주소인지 확인하기 위해 파이썬 함수를 구현하도록 우리 모델에 요청한 좋은 사례. 실행 결과에서 제기된 오류에 영감을 받아 본 모델은 복잡한 정규식에서 작은 오류를 인식했다. 그런 다음, 우리의 모델은 이 정규 표현에 대한 설명을 성공적으로 생성하고 철저한 인라인 코멘트로 가독성을 개선했다.\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      '두 입력 목록의 교차점을 찾고, 두 목록에 각각 존재하는 별개의 요소와 그 발생 빈도를 포함하는 튜플 목록을 반환하는 함수를 설계한다. 이 작업의 경우 파이썬의 내장 목록 함수나 라이브러리를 사용하여 결과를 정렬하고 중복을 제거하지 마십시오. 또한 이 작업에 대해 자신만의 해시 테이블 데이터 구조를 구현합니다.\n' +
      '\n' +
      'def shared_elements[list1: list, list2: list]:\n' +
      '\n' +
      '"파이썬의 내장 목록 함수나 라이브러리를 사용하지 않고 오름차순으로 정렬된 입력 목록과 빈도 모두에 나타난 고유 요소의 튜플 목록을 반환합니다. 이 작업에 대한 자체 해시 테이블을 구현합니다.\n' +
      '\n' +
      '\\(>>>\\)shared_elements[[1, 4, 1, 3, 4, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121]]\n' +
      '\n' +
      '[[1, 2], (5, 3), (653, 2)]\n' +
      '\n' +
      '\\(>>>\\)shared_elements[[5, 3, 3, 2, 8], [3, 2]]\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '"python\n' +
      '\n' +
      'class HashMap:\n' +
      '\n' +
      '[코드가 잘린다]\n' +
      '\n' +
      'def shared_elements[list1, list2]:\n' +
      '\n' +
      '[코드가 잘린다]\n' +
      '\n' +
      'for i in range(hash_map1.size)\n' +
      '\n' +
      'if hash_map1.get(i)는 None이 아니고 hash_map2.get(i)는 None이 아니다:\n' +
      '\n' +
      'intersection_append((i, hash_map1.get(i)+ hash_map2.get(i))\n' +
      '\n' +
      'insertion_sort(intersection)\n' +
      '\n' +
      'return intersection\n' +
      '\n' +
      'def insertion_sort(lst):\n' +
      '\n' +
      'for i in range(len(lst):\n' +
      '\n' +
      '\\(j^{+}=\\)\n' +
      '\n' +
      'while j > 0 and lst[j - 1] > lst[j]:\n' +
      '\n' +
      '\\(\\text{lst}[j^{-}]\\), lst[j] - 1[j], lst[j^{-}], lst[j^{-}]\n' +
      '\n' +
      '\\(j-1\\)\n' +
      '\n' +
      '샘플 - [[1, 4, 1, 3, 4, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121]], [[5, 3, 3, 2, 8], [3, 2]]\n' +
      '\n' +
      '샘플의 샘플에 대해:\n' +
      '\n' +
      '\\(\\text{print(shared\\_elements(sample[0], sample[1]))}\\)\n' +
      '\n' +
      '**User:**\n' +
      '\n' +
      'Execution result:\n' +
      '\n' +
      'Cell ln[2] line 45\n' +
      '\n' +
      'for i in range(hash_map1.size)\n' +
      '\n' +
      'SyntaxError: 유효하지 않은 구문\n' +
      '\n' +
      '**Assistant:**\n' +
      '\n' +
      '죄송합니다, 작은 실수가 있는 것 같습니다. 내가 바로잡을게\n' +
      '\n' +
      'class HashMap:\n' +
      '\n' +
      '[코드가 잘린다]\n' +
      '\n' +
      'def shared_elements(lst1, list2):\n' +
      '\n' +
      '[코드가 잘린다]\n' +
      '\n' +
      'for i in range(hash_map1.size):\n' +
      '\n' +
      'if hash_map1.get(i) is not None and hash_map2.get(i) is not\n' +
      '\n' +
      'intersection_append((i, hash_map1.get(i)+ hash_map2.get(i))\n' +
      '\n' +
      'return intersection\n' +
      '\n' +
      'return intersection\n' +
      '\n' +
      'def insertion_sort(lst):\n' +
      '\n' +
      'for i in range(len(lst):\n' +
      '\n' +
      '\\(\\text{j}=\\)\n' +
      '\n' +
      'while j > 0 and lst[j - 1] > lst[j]:\n' +
      '\n' +
      '\\text{lst}[j]\\ - 1\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>