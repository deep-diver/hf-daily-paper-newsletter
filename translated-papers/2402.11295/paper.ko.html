<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OneBit : 극저비트의 대용량 언어모델을 중심으로\n' +
      '\n' +
      '유장 Xu\\({}^{1}\\), Xu Han\\({}^{1}\\), Zonghan Yang\\({}^{1}\\), Shuo Wang\\({}^{1}\\)\n' +
      '\n' +
      '청푸주({}^{2}\\), 지위안유({}^{1}\\), 위동유({}^{1}\\), 완샹체({}^{2,}\\)**\n' +
      '\n' +
      '중국 베이징 칭화대학교 컴퓨터과학과\n' +
      '\n' +
      '({}^{2}\\)소셜 컴퓨팅 및 정보 검색 연구 센터,\n' +
      '\n' +
      '중국 하얼빈공과대학\n' +
      '\n' +
      'xyz21thu@gmail.com, car@ir.hit.edu.cn\n' +
      '\n' +
      'Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '모델 정량화는 모델의 가중치 매트릭스를 나타내기 위해 낮은 비트 폭 값을 사용하며, 이는 매우 기대되는 LLM을 배포하는 스토리지 및 계산 오버헤드를 모두 줄이기 위한 유망한 접근법이다. 그러나 기존의 양자화 방법은 비트폭이 매우 줄어들 경우 성능 저하가 심하여 4비트 또는 8비트 값을 이용하여 모델을 양자화하는데 초점을 맞추고 있다. 본 논문은 LLM의 가중치 행렬을 1비트로 과감하게 정량화하여 LLM의 극히 낮은 비트 폭 배치를 위한 길을 열어준다. 이를 위해 1비트 양자화 인식 훈련(QAT: Quantization-aware Training) 프레임워크인 OneBit에 LLM을 더 잘 양자화하기 위한 새로운 1비트 파라미터 표현 방법과 QAT 프레임워크의 수렴 속도를 향상시키기 위한 행렬 분해에 기반한 효과적인 파라미터 초기화 방법을 소개한다. 충분한 실험 결과는 OneBit가 1비트 가중치 행렬만을 사용할 때 강건한 훈련 과정을 통해 우수한 성능(비양자화 성능의 최소 83%)을 달성함을 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '트랜스포머 Vaswani 등(2017)은 대규모 언어 모델(LLM)에서 중추적인 아키텍처로 등장하여, 딥러닝 Bubeck 등(2023); Touvron 등(2023); Bisk 등(2020). 트랜스포머 기반 LLM의 보급은 그 인기에도 불구하고 LLM의 파라미터가 점점 많아짐에 따라 계산 강도와 상당한 메모리 요구 사항으로 인해 상당한 어려움을 겪고 있다. 예를 들어, LLaMA-13B Touvron et al. (2023)과 같은 적당히 크기의 LLM들조차도 그것의 모든 파라미터들을 FP16 포맷으로 로딩하기 위해 약 26GB의 메모리를 필요로 한다. 이러한 오버헤드는 모바일 장치는 고사하고 A100과 같은 중간에서 고급 GPU를 넘어 LLM을 배치하는 것을 어렵게 만든다. 자원에 대한 높은 수요는 사용 비용을 증가시킬 뿐만 아니라 광범위한 적용을 제한한다.\n' +
      '\n' +
      '많은 노력 Dettmers et al.(2022); Frantar et al.(2022); Frantar and Alistarh (2023)은 여전히 대부분의 원래 모델 능력을 보존하면서 LLMs의 계산 및 메모리 오버헤드를 줄이는 데 전념해 왔다. 이러한 노력들 중에서 양자화는 광범위한 주목을 받았고, 특히 PTQ(Post-Training Quantization)는 낮은 전송 비용으로부터 이익을 얻었다. GPTQ Frantar et al. (2022), SpQR Dettmers et al. (2023), AWQ Lin et al. (2023)과 같은 정성적 연구는 LLMs의 주요 능력을 유지하면서 LLMs의 가중치 매트릭스를 4비트 값으로 성공적으로 압축했다. 효율적인 양자화는 모델 성능뿐만 아니라 시간과 공간 효율성 사이의 균형을 달성함으로써 LLM 최적화의 상당한 발전을 나타낸다.\n' +
      '\n' +
      '불행히도, PTQ의 효능은 그림 1에 도시된 바와 같이 양자화 비트-폭이 극도로 낮을 때 급격히 감소한다. 기존의 PTQ 방법들은 가중치 매트릭스들을 아래로 압축하도록 관리되었다\n' +
      '\n' +
      '도 1: Wikitext2 Merity et al.(2016)에 보고된, LLaMA-7B 상의 기존의 널리 사용되는 저-비트 양자화 방법들의 복잡성(낮은 스코어들은 더 나은 성능을 의미한다) 검사된 모든 이전 접근법은 모델을 2비트 값으로 양자화할 때 상당한 성능 저하를 겪는다. 우리의 1비트 양자화 방법은 이러한 2비트 기준선을 능가할 수 있다.\n' +
      '\n' +
      '적어도 3비트 디트머스 및 제틀모이어(2023). 최근 연구는 QAT(Quantization-Aware Training)를 활용하여 PTQ가 직면한 병목 현상을 극복하고자 한다. LLM-QAT Liu et al.(2023)은 양자화 과정에 몇 가지 학습 가능한 파라미터를 도입하여 주목할 만한 결과를 얻었다. 학습 가능한 등가 변환을 통합하는 OmniQuant Shao et al.(2023)은 2 비트 양자화에서 유망한 결과를 제시한다. 그러나 기존의 방법들은 모델 가중치를 1비트로 압축할 경우 성능이 저하되어 효과를 유지하는데 어려움을 겪고 있다. 이는 LLMs 내의 핵심 연산자인 선형 투영(\\(\\mathbf{W}\\mathbf{X}\\)에서 손실이 크게 증가하며, 가중치 매트릭스\\(\\mathbf{W}\\mathbf{X}\\)에서 매우 낮은 비트폭에서 급격한 정밀 손실에 기인한다.\n' +
      '\n' +
      '본 논문에서는 약 1비트 값을 이용하여 LLM을 표현하는 가중치 행렬에 대한 새로운 선형 계층 및 부호-값 독립 분해(Sign-Value-Independent Decomposition, SVID)를 제안한다. SVID에서, 각각의 원래의 고-비트 가중치 행렬은 하나의 부호 행렬(\\(\\pm 1\\))과 두 개의 값 벡터로 분해된다. 값 벡터는 적은 비용으로 선형 투영에서 필요한 부동 소수점 정밀도를 제공하고 모델이 쉽게 훈련될 수 있도록 돕는다. 부호 행렬은 적은 공간 비용으로 원래의 가중치 행렬의 높은 순위를 유지함으로써 높은 정보 용량을 보존한다. SVID는 1-비트 모델에 대해 더 나은 매개변수 초기화를 제공하며, 제안된 1-비트 대응물에 원래 모델의 기능을 전달하기 위해 양자화 인식 지식 증류를 사용한다. 실험을 통해 제안한 방법이 W1A16(1비트 가중치 및 16비트 활성화) 양자화 레벨에서 잘 수행됨을 보인다. 또한, 1비트 모델은 이전 작업보다 훈련 및 지식 전달에 더 적합하다. 요약하면, 이 작업의 기여는 3배이다:\n' +
      '\n' +
      '* 우리는 모델 추론 동안 시간과 공간 효율성을 모두 향상시킬 수 있는 LLM에 대한 새롭고 효율적인 1비트 모델 아키텍처를 제안한다. 또한 LLM을 정량화하는 동안 아키텍처가 더 안정적입니다.\n' +
      '* 우리는 1비트 아키텍처의 초기화에 필수적인 고비트 행렬을 저비트 행렬로 분해하는 SVID를 제안한다. 실험을 통해 SVID 기반 초기화가 모델 성능 및 수렴 속도를 향상시킬 수 있음을 보인다.\n' +
      '* 광범위한 실험은 우리의 방법이 OPT, LLaMA 및 LLaMA2에서 모델 크기 1.3B에서 13B까지 잘 작동하여 일반화 가능성을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '##### 대언어 모델 압축\n' +
      '\n' +
      '모델 압축을 위한 주요 방법은 정량화, 가지치기 및 지식 증류(KD)이다. 양자화는 모델 가중치를 저비트 값 Frantar et al.(2022); Lin et al.(2023); Dettmers et al.(2023)로 압축한다. 계산 및 메모리 감소에서의 데이터 유형 정렬을 위해, 또한 활성화 Dettmers et al.(2022); Xiao et al.(2023) 및 키-값 캐시 Shao et al.(2023)을 양자화하는 것을 포함한다. 프런닝은 중요하지 않은 가중치들 또는 모듈들을 제거함으로써 모델 복잡성을 단순화시키고, 이에 의해 원래의 더 큰 모델들 Frantar 및 Alistarh (2023); Sun et al. (2023); Ma et al. (2023). KD는 더 큰 교사 모델 Hsieh et al.(2023); Agarwal et al.(2023); Hsieh et al.(2023)의 안내 하에 더 작은 학생 모델을 훈련시키며, 더 큰 것을 압축하는 목적을 달성한다. 이러한 방법 외에도 저순위 인수분해는 원래 가중치 행렬 \\(\\mathbf{W}\\)을 두 개의 저순위 행렬 Xu et al.(2023)의 곱으로 근사하고 또한 유망한 결과를 얻는다. 우리의 작업은 원래 LLM에서 지식 전달을 위해 KD를 사용하고 극도로 낮은 비트 폭 양자화에 독특하게 초점을 맞춘 양자화에 속한다. 모델 압축에 대한 더 자세한 내용은 기존의 생존완 등(2023); Zhu 등(2023)을 참조할 수 있다.\n' +
      '\n' +
      '### 대용량 언어 모델 양자화\n' +
      '\n' +
      '본 논문에서는 매우 낮은 비트 LLM을 얻기 위해 LLM 양자화에 대한 자세한 내용을 소개한다. 양자화는 비교적 작은 손실로 상당한 압축비를 달성할 수 있는 모델 압축에 대중적이고 중요한 방법으로 자리 잡고 있다. 양자화 적용 시기에 따라 PTQ(Post-Training Quantization)와 QAT(Quantization-Aware Training)로 구분할 수 있다.\n' +
      '\n' +
      'PTQ는 추가적인 훈련 없이 정확한 해결기와 제한된 교정 데이터를 사용하여 훈련된 모델을 저비트 대응물로 직접 변환한다. 전형적으로, GPTQ Frantar et al.(2022)은 가중치 매트릭스들을 행-순으로 양자화하고, 양자화에 의해 야기된 정밀 손실을 보상하기 위해 나머지 가중치들을 조정하여 거의 무손실 4-비트 가중치 양자화를 달성한다. 더욱이, 수많은 연구들은 양자화 Dettmers et al. (2022); Kim et al. (2023); Lin et al., 2023에서 "outlier"의 효과를 관찰했다. LLM.int8() Dettmers et al.(2022)은 활성화에서 몇 개의 이상치의 정확도를 보장하기 위해 혼합 정밀도 분해를 제안한다. SmoothQuant Xiao et al.(2023)은 활성화의 이상치를 평활화함으로써 양자화의 어려움을 감소시킨다. SpQR Dettmers et al.(2023)은 민감한 가중치들을 식별하여 그들의 정밀도를 보장하는 한편, 다른 가중치들을 더 낮은 비트-폭으로 양자화한다.\n' +
      '\n' +
      'QAT는 모델 내에서 양자화 단계를 통합하여 훈련 또는 미세 조정 중에 적용한다. 이는 모델이 양자화에 의해 유도된 감소된 정밀도에 더 잘 적응할 수 있게 하여 PTQ에 비해 향상된 성능을 유도한다. LLM-QAT Liu et al.(2023)은 적은 수의 학습 가능한 파라미터를 양자화에 도입하고, 원래의 모델 자체에 의해 생성된 데이터를 사용하여 KD를 채용한다. OmniQuant Shao et al. (2023); 우리는 이를 QAT로 분류하며 학습 가능한 등가 변환을 추가로 도입하여 2비트 가중치 양자화에서 허용 가능한 결과를 달성한다. PEQA Kim et al. (2023) 및 QLoRA Dettmers et al. (2023)은 서브-4비트 가중치 양자화에 의해 야기되는 정밀 손실을 완화하기 위해 제한된 수의 여분의 파라미터들을 미세 조정하는 것에 초점을 맞춘다. 우리의 작업은 QAT와 밀접한 관련이 있지만 1비트 양자화에 의해 제기된 고유한 도전으로 인해 양자화된 가중치의 표현 및 초기화 방법은 기존의 작업과 구별된다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '이 섹션에서는 정량화될 선형 레이어의 1비트 아키텍처를 설명하고 지식 증류에서 더 나은 성능을 달성하기 위해 정량화된 모델을 초기화하는 방법에 대해 논의한다. 우리는 섹션 3.1에서 고전적인 가중치 양자화 방법에 대한 짧은 검토로 시작하여 섹션 3.2에서 섹션 3.4까지 우리의 OneBit를 자세히 공식화한다.\n' +
      '\n' +
      '### Background\n' +
      '\n' +
      '모델 양자화의 주요 아이디어는 FP32 또는 FP16 형식의 모델 내의 각 가중치 행렬 \\(\\mathbf{W}\\)을 저비트 대응물로 압축하는 것이다. 특히, 우리는 종종 트랜스포머에서 선형 레이어의 가중치 행렬을 8, 4, 심지어 2비트로 양자화한다.\n' +
      '\n' +
      '양자화 연구의 대부분은 RTN(round-to-nearest) 방법을 주로 사용하며, 이 방법은 양자화 그리드에서 가중치\\(w\\)를 가장 가까운 값으로 반올림한다. 와 같이 제형화될 수 있다.\n' +
      '\n' +
      '\\[\\hat{w}=\\mathrm{Clip}\\left(\\left\\lfloor\\frac{w}{s}\\right\\rceil+z,0,2^{N}-1 \\right), \\tag{1}\\hextrm{Clip}\\left(\\left\\lfloor\\frac{w}{s}\\right\\rceil+z,0,2^{N}-1 \\right), \\tag{1}\\hextrm{Clip}\\left(\\left\\lfloor\\frac{w}{s}\\right\\rceil+z,0,2^{N}-1 \\right)\n' +
      '\n' +
      '여기서 \\(s\\)은 양자화 스케일 파라미터를 나타내고, \\(z\\)은 영점 파라미터를 나타내며, \\(N\\)은 양자화 비트-폭을 나타낸다. \\(s\\)은 양자화 스케일 파라미터를 나타낸다. (\\mathrm{Clip}(\\cdot)\\)는 \\(0\\)에서 \\(2^{N}-1\\)의 범위에서 결과를 잘랐다. 비트-폭이 점점 낮아짐에 따라, 양자화 그리드도 더 희박해진다. LLM을 1비트 값으로 양자화할 때 양자화된 모델에서 선택할 수 있는 수는 2개뿐이다. 기존 연구 Dettmers and Zettlemoyer (2023)는 RTN 방법에 기반한 양자화가 4비트 수준에서 가장 좋은 성능을 얻을 수 있다고 지적한다. 2비트 값으로 모델을 양자화하는 것과 같이 추가로 압축하는 것은 도 1에 도시된 바와 같이 샤오 등(2023)의 실질적인 열화를 초래할 것이다.\n' +
      '\n' +
      '또한, \\(N\\)이 \\(1\\)일 때, RTN 방법에 기초한 양자화는 임계값을 설정하는 것과 본질적으로 동일하며, 그 양쪽에 있는 가중치 \\(w\\)는 대응하는 정수 값 \\(\\hat{w}\\)으로 변환된다. 이러한 시나리오에서, 매개변수 \\(s\\) 및 \\(z\\)은 Eq. (1) 실제적 의의를 효과적으로 상실한다. 결과적으로 가중치를 1비트로 양자화할 때 요소별 RTN 연산은 가중치 행렬 \\(\\mathbf{W}\\)의 정밀도를 크게 저하시켜 양자화된 모델의 성능을 저하시킨다.\n' +
      '\n' +
      '###1비트 선형 계층 구조\n' +
      '\n' +
      '1비트 가중치 양자화의 심각한 정밀 손실로 인해 선형 레이어의 가중치 매트릭스를 RTN을 기반으로 FP32/16에서 1비트 형식으로 직접 변환하는 것은 어렵다. Wang et al.(2023)은 순수하게 1비트 가중치 행렬의 능력을 연구하여, 처음부터 1비트 모델을 훈련시킴으로써 이러한 가능성을 탐구한다. W1A16 설정에서, 이들의 선형 층은 다음과 같이 설계된다.\n' +
      '\n' +
      '\\mathbf{W}-\\mathm{Sign}\\Big{[}\\mathbf{W}\\big{}=\\mathrm{Sign}\\Big{[\\begin{split}\\mathrm{Abs}\\Big{},\\\\eta=\\mathrm{Mean}\\Big{(}\\mathbf{W}\\big{(}\\mathbf{W}\\big{}\\Big{}}\\mathbf{Y}=\\eta\\cdot\\mathm{W}\\big{(}\\mathbf{X}\\big{}}\\mathbf{W}\\big{}}\\mathbf{W}_{\\pm 1}^{\\mathrm{T},\\end{split}\\tag{2}\\big{}\\mathbf{Y}=\\eta\\cdot\\mathm{W}\\big{\n' +
      '\n' +
      '여기서 \\(\\mathbf{W}\\)는 형상 \\(m\\times n\\)을 갖는 양자화된 가중치 매트릭스를 나타내고 \\(\\mathbf{W}_{\\pm 1}\\)은 1-비트 양자화된 매트릭스를 나타낸다. \\(\\mathbf{W}_{\\pm 1}\\) (\\mathbf{X}\\)는 선형층의 입력이고 \\(\\mathbf{Y}\\)는 출력이다. \\ (\\mathrm{Sign}(\\cdot)\\), \\(\\mathrm{Mean}(\\cdot)\\) 및 \\(\\mathrm{Abs}(\\cdot)\\) 함수는 부호 행렬, 평균 및 절대값 행렬을 반환한다. 불행하게도, 이 접근법은 계산 요구를 감소시키지만 또한 Wang 등(2023)의 현저한 성능 저하로 이어진다.\n' +
      '\n' +
      '또한 Wang et al. (2023)에 의해 추론된 가중치 행렬은 함수 \\(\\mathrm{Sign}(\\cdot)\\)을 이용하여 양자화하고, 양자화된 행렬의 요소는 +1 또는 -1로 설정된다. 또한, \\(\\mathbf{W}_{\\pm 1}\\)이 \\(\\mathbf{W}\\)의 높은 순위를 유지하지만, 부동 소수점 정밀도는 여전히 모델 성능을 파괴한다는 것을 알 수 있다. 따라서, 이전 작업과 달리 양자화 과정에서 정밀 손실을 줄이기 위해 FP16 형식을 가진 두 개의 값 벡터를 도입한다. 제안된 선형 레이어는 다음과 같이 설계된다.\n' +
      '\n' +
      'bf{W}_{\\pm 1}=\\mathrm{Sign}\\big{(}\\begin{split}\\mathbf{W}\\big{),\\\\mathbf{Y}=\\big{[}\\big{(}\\mathbf{X}\\odot\\mathbf{g}\\big{}\\mathbf{W}_{\\pm 1}^{\\mathrm{T}\\big{}\\odot\\mathbf{h},\\end{split}\\tag{3}\\big{}\\mathbf{Z}=\\mathrm{LayerNorm}\\big{(}\\mathbf{Y}\\big{}\\big{,\\end{split}\\tag{3}\\big{}\\big{}\\big{(}\\mathbf{X}\\odot\\mathbf{g}\\big{}\\mathbf{W}{\\pm 1}^{\\mathrm{T}\\big{}\\big{,\n' +
      '\n' +
      '여기서 \\(\\mathbf{g}\\) 및 \\(\\mathbf{h}\\)는 두 FP16 값 벡터이다. 우리는 식에서 괄호를 사용하여 계산 순서를 지정한다는 점에 유의한다. (3) 시간 및 공간 비용을 최소화하기 위한 방법. W Wang et al. (2023)과 OneBit의 주요 차이점은 추가 매개변수 \\(\\mathbf{g}\\)와 \\(\\mathbf{h}\\)이다. 추가 매개 변수를 가져오더라도 이점은 적은 비용보다 훨씬 큽니다. 예를 들어, 하나의 가중치 행렬을 모양 \\(4096\\times 4096\\)으로 양자화할 때, 양자화된 결과의 평균 비트폭은 _1.0073_이다. 자세한 내용은 A.6을 참조하십시오.\n' +
      '\n' +
      '### Sign-Value-Independent Decomposition\n' +
      '\n' +
      '제안된 1비트 구조에서 가중치 행렬\\(\\mathbf{W}\\)은 INT1 형식에서 하나의 부호 행렬\\(\\mathbf{W}_{\\pm 1}\\)과 FP16 형식에서 두 개의 값 벡터\\(\\mathbf{g}\\)/\\(\\mathbf{h}\\)으로 수학적으로 나뉜다. 완전 훈련된 가중치의 도움으로 1비트 모델을 초기화하기 위해, 가중치 행렬 \\(\\mathbf{W}\\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}\\odot\\mathbf{W}_{\\mathrm{value}\\)로 공식화될 수 있는 가중치 행렬 \\(\\mathbf{W}\\(\\mathbf{W}})의 Sign-Value-Independent Decomposition (SVID)을 소개한다. 여기 \\(\\mathbf{W}_{\\mathrm{value}}=|\\mathbf{W}|\\)와 \\(\\mathbf{W}_{\\mathrm{sign}}=\\mathrm{Sign}(\\mathbf{W})\\이 있다. 또한, \\(\\mathbf{W}_{\\mathrm{value}\\)의 경우, 우리는 이것을 두 벡터 \\(\\mathbf{a}\\)와 \\(\\mathbf{b}\\)의 외부곱으로 분해하는데, 이는 _rank-1 approximation_라고도 한다. 따라서, 제안된 행렬 분해 방법은 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\mathbf{W}\\approx\\mathbf{W}\\mathrm{sign}\\odot\\left(\\mathbf{a}\\mathbf{b}^{\\mathrm{T}\\right)\\tag{4}\\right)\n' +
      '\n' +
      '우리는 SVD(Beltrami, 1990)와 NMF(Paatero and Tapper, 1994)와 같이 랭크-1 근사를 수행하기 위해 널리 사용되는 행렬 분해 방법을 사용할 수 있다.\n' +
      '\n' +
      '**명제 1** 가중치 행렬 \\(\\mathbf{W}\\) 및 입력 \\(\\mathbf{X}\\)이 주어지면, 선형 층은 SVD에 따라 다음과 같이 재구성될 수 있다:\n' +
      '\n' +
      '\\mathbf{W}^{\\mathrm{T}}\\approx\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{b}^{\\mathrm{T}\\right)\\mathbf{W}_{\\mathrm{sign}^{\\mathrm{T}\\big{}\\odot\\mathbf{a}^{\\mathrm{T}}. \\tag{5}\\tig{\n' +
      '\n' +
      '우리는 부록 A.1에서 이 근사치를 증명한다. 이것은 양자화된 모델의 아키텍처와 원래 가중치 사이의 격차를 좁힌다. 이것은 \\(\\mathbf{W}_{\\mathrm{sign}\\)을 \\(\\mathbf{W}_{\\pm 1}\\)에, \\(\\mathbf{a}^{\\mathrm{T}\\)을 \\(\\mathbf{h}\\)에, \\(\\mathbf{b}^{\\mathrm{T}\\)을 \\(\\mathbf{g}\\)에 할당하면 양자화된 모델이 원래 모델의 근사 초기화임을 나타낸다. 또한, 원래의 행렬 \\(\\mathbf{W}\\)을 먼저 복원하는 것과 비교(예: Eq. (4)), 계산 순서는 Eq. (5) FP16(\\mathbf{W}\\)은 FP16(\\mathbf{W}\\)을 복원할 필요가 없기 때문에 메모리에서 FP16 포맷의 대략 하나의 행렬(\\mathbf{W}\\)을 저장한다.\n' +
      '\n' +
      'SVD의 주요 목적은 FP16 형식의 값 벡터에만 의존하기보다는 근사 행렬에 부호 행렬\\(\\mathbf{W}_{\\mathrm{sign}\\)을 포함시키는 것이다. 행렬근사에서 부호행렬\\(\\mathbf{W}_{\\mathrm{sign}\\)의 역할을 입증하기 위해 다음과 같은 명제를 제시한다.\n' +
      '\n' +
      '**명제 2** 주어진 행렬 \\(\\mathbf{W}\\) 및 \\(|\\mathbf{W}|\\), \\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}\\odot|\\mathbf{W}|\\) 이 행렬들을 \\(\\mathbf{W}=\\mathbf{a}\\mathbf{b}^{\\mathrm{T}+\\mathbf{E}{1}\\)과 \\(\\mathbf{W}|=\\tilde{\\mathbf{a}\\tilde{\\mathbf{b}}\\tilde{\\mathbf{b}}+\\mathbf{E}{2}\\)으로 분해하고, 여기서 \\(\\mathbf{E}_{i}\\)은 오차행렬을 나타낸다. 프로베니우스-norm의 관점에서, SVD는 원래의 행렬 \\(\\mathbf{W}\\)에 더 가깝다:\n' +
      '\n' +
      '\\mathbf{W}_{\\mathrm{sign}\\odot\\tilde{\\mathbf{a}\\tilde{\\mathbf{b}^{\\mathrm{T}\\right\\|_{\\mathrm{F}\\leq\\left\\|\\mathbf{W}-\\mathbf{b}^{\\mathrm{T}\\right\\|_{\\mathrm{F}^{2}\\text\\mathbf{b}^{\\mathrm{F}^{2}\\tag{6}\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\n' +
      '\n' +
      '또한 부록 A.1에서 이 명제를 증명한다. 이것은 행렬 근사법에서 부호 행렬 \\(\\mathbf{W}_{\\mathrm{sign}\\)의 실질적인 역할을 분명히 보여준다.\n' +
      '\n' +
      '대부분의 매개변수의 가장 낮은 정밀도를 감안할 때 상당히 어렵다.\n' +
      '\n' +
      '그림 2: 우리의 방법 OneBit의 주요 아이디어. 왼쪽은 원래 FP16 선형 계층으로 활성화\\(\\mathbf{X}\\)와 가중치 행렬\\(\\mathbf{W}\\)이 모두 FP16 형식이다. 오른쪽은 우리가 제안한 아키텍처입니다. 값 벡터 \\(\\mathbf{g}\\)와 \\(\\mathbf{h}\\)만이 FP16 형식이고 가중치 행렬은 \\(\\pm 1\\)으로 구성된다.\n' +
      '\n' +
      '가중치 행렬 \\(\\mathbf{W}\\)을 정확하게 근사한다. SVID는 원본 모델의 매개변수를 정확하게 복제하는 것이 아니라, 원본 모델의 광범위한 훈련을 활용하여 추가 훈련을 위한 효과적인 시작점을 제공하는 것을 목표로 한다. 원본 모델에서 양자화된 대응물로의 지식 전달에 대한 자세한 내용은 3.4절에 나와 있다.\n' +
      '\n' +
      '### Knowledge Transfer\n' +
      '\n' +
      '이를 위해 양자화 인식 지식 증류를 이용하여 원본 모델(즉, 교사 모델)에서 양자화된 모델(즉, 학생 모델)로 지식을 전달한다. 학생 모델에서는 행렬의 요소\\(\\mathbf{W}\\)와 벡터의 요소\\(\\mathbf{g/h}\\)을 Eq. (3)을 훈련한다. 양자화된 학생 모델을 지시하기 위해 교차 엔트로피 기반 로짓과 완전 정밀 교사 모델의 평균 제곱 오차 기반 은닉 상태를 사용한다(Sun et al., 2019). 언어 모델링 손실이 사용되지 않습니다. 상기 크로스 엔트로피는,\n' +
      '\n' +
      '[\\mathcal{L}_{\\mathrm{CE}=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}\\sum_{c}P_{c}^{\\mathcal{T}}\\left(\\mathbf{o}_{i}\\right)\\log P_{c}^{\\mathcal{S}}\\left(\\mathbf{ o}_{i}\\right), \\tag{7}\\w}\n' +
      '\n' +
      '여기서 \\(c\\)는 클래스의 수를 나타내고 \\(n_{s}\\)는 현재 배치에서 트레이닝 샘플의 수를 나타낸다. \\(n_{s}\\) (\\mathcal{T}\\)와 \\(\\mathcal{S}\\)은 각각 교사 모델과 학생 모델이다. 은닉 상태의 에러는,\n' +
      '\n' +
      '[\\mathcal{L}_{\\mathrm{MSE}=\\sum_{i=1}^{n_{s}}\\sum_{j=1}^{n_{l}}\\left\\|\\frac{ \\mathbf{q}_{i,j}^{\\mathcal{T}}{\\left\\|\\mathbf{q}_{i,j}^{\\mathcal{T}\\right\\|_{2}}-\\frac{\\mathbf{q}_{i,j}^{\\mathcal{S}}\\right\\|_{2}}, \\tag{8}\\tag{8}}}\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\text\\\n' +
      '\n' +
      '여기서 \\(n_{l}\\)는 레이어의 수를 나타내고 \\(\\mathbf{q}\\)는 히든 상태를 나타낸다. 따라서 최종 목적 함수는 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{KD}=\\mathcal{L}_{\\mathrm{CE}+\\alpha\\mathcal{L}_{\\mathrm{MSE}, \\tag{9}\\}\n' +
      '\n' +
      '여기서 \\(\\alpha\\)는 교차 엔트로피 손실의 중요성과 중간 계층에서의 특징들의 균형을 이루는 하이퍼-파라미터이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 논문에서는 1비트 가중치만을 이용한 양자화를 실험하고 16비트 활성화(W1A16)를 유지한다. OPT-1.3B/2.7B 모델, LLaMA-7B/13B 모델 및 LLaMA2-7B/13B 모델에 대한 실험을 수행하여 접근 방식을 평가하고 다양한 작업에 대한 결과를 제시한다.\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      '본 논문의 양자화 인식 지식 증류의 학습 데이터는 Liu et al. (2023)을 따라 원래의 교사 모델로부터 다음 토큰 생성을 이용하여 말뭉치를 합성한다. 어휘로부터 첫 번째 토큰을 랜덤화하고 <_EOS_> 토큰 또는 최대 길이에 도달할 때까지 다음 토큰을 반복적으로 생성한다. 특히, 상위 1개의 예측은 처음 3~5개의 토큰에 대해 결정적으로 선택되고, 나머지 토큰에 대해서는 확률적 샘플링이 뒤따른다. LLaMA-7B를 활용하여 각각 최대 길이가 2,048인 총 132k개의 데이터 항목을 생성했다.\n' +
      '\n' +
      '모든 KD 실험은 2048개의 토큰 세그먼트가 선택된 50개 에폭 이상의 학습 데이터를 학습한다. 우리는 SVID의 가중치 행렬을 분해하기 위해 scikit-learn 1에 NMF를 사용한다. 양자화된 학생 모델은 Adam(Kingma and Ba, 2014)에 의해 \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.98\\)으로 최적화된다. 모든 실험에 대한 학습률은 _cosine_ 전략에 의해 스케줄링된다. 우리는 NVIDIA A100 GPU를 사용하고 양자화된 모델을 훈련하는 동안 FP16 정밀도를 유지한다. 학습률 등 자세한 사항은 <표 1>을 참고하시기 바랍니다.\n' +
      '\n' +
      '각주 1: [https://scikit-learn.org/](https://scikit-learn.org/)\n' +
      '\n' +
      '우리가 아는 한, 지식 전달 관점에서 LLM의 1비트 양자화를 탐구하는 이전 작업은 없다. 이를 위해 본 논문에서는 W1A16 설정을 유지하면서 기준선의 양자화 비트폭을 2비트(W2A16)로 완화한다. 우리는 GPTQ (Frantar et al., 2022), LLM-QAT (Liu et al., 2023) 및 OmniQuant (Shao et al., 2023)와 우리의 방법을 비교한다. 공간 사용 측면에서 공정한 비교를 보장하기 위해 기준선은 그룹화된 양자화를 사용하지 않는다. 또한 FP16 정밀도를 가진 바닐라 변압기의 결과를 참조로 포함했다. 최근 작업인 BitNet(Wang et al., 2023)도 하나의 1비트 모델 아키텍처를 도입한 반면, 처음부터 모델 훈련에만 초점을 맞추었다. 우리도\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Models** & **learning rate** & \\(\\alpha\\) & **\\# GPUs** \\\\ \\hline OPT-1.3B & 4e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ OPT-2.7B & 2e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ LLaMA-7B & 4e-4 & 10.0 & 1 \\(\\times\\) 8 \\\\ LLaMA-13B & 2e-4 & 1.0 & 2 \\(\\times\\) 8 \\\\ LLaMA2-7B & 1e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ LLaMA2-13B & 2e-4 & 1.0 & 2 \\(\\times\\) 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 지식 증류에 대한 교육 세부 사항.\n' +
      '\n' +
      '부록 A.5의 원본 모델에서 지식을 전달할 수 있는 기능을 분석합니다.\n' +
      '\n' +
      '평가 메트릭은 기본적으로 검증 세트, 특히 WikiText2 Merity et al.(2016) 및 C4 Raffel et al.(2020)에 대한 복잡성을 테스트하여 양자화된 모델을 평가한다. 더 낮은 당혹감은 압축된 모델이 원래 모델의 출력 분포를 보존하는 데 더 낫다는 것을 나타낸다. 또한, Winograde Sakaguchi et al. (2021), HellaSwag Zellers et al. (2019), PIQA Bisk et al. (2020), BoolQ Clark et al. (2019), 및 ARC Clark et al. (2018)을 포함하는 제로 샷 태스크들의 정확도들이 또한 보고된다. 다운스트림 작업에 대한 원래 모델의 기능이 유지되는지 여부를 평가합니다. 오픈 소스 툴킷 "LM-평가-Harness"2를 활용하여 당혹성 테스트 및 모든 제로 샷 작업을 수행합니다.\n' +
      '\n' +
      '각주 2: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '표 2는 우리의 방법을 다른 모델에 대한 다른 전형적인 강력한 기준선과 비교한다. 공간 제약으로 인해 LLaMA2-7B/13B의 결과는 부록 A.3에 나열되어 있으며, 다양한 모델 크기에서 W2A16 설정에서 1비트 가중치 양자화 방법이 다른 방법보다 분명히 우수하다. 또한, QAT 기반 방법의 효과는 모델 크기가 증가함에 따라 일관되게 향상되는 반면, PTQ 방법의 결과인 GPTQ는 모델 크기가 증가할 때(예를 들어, LLaMA에서 7B에서 13B로) 저하될 수 있다. 이는 QAT 기반 방법이 극히 낮은 비트 양자화에서 안정적인 결과를 얻을 수 있음을 입증한다. 특히, 제안된 방법은 모델 크기가 증가함에 따라 FP16의 성능에 더 근접하게 접근한다. 예를 들어, LLaMA-7B에서 LLaMA-13B로 스케일링할 때 FP16 모델의 복잡성은 0.59만큼 감소하는 반면, 우리의 방법은 1.20의 감소를 본다.\n' +
      '\n' +
      '당혹감을 위해 우리의 방법만이 가장 강력한 FP16 기준선과 유사한 결과를 달성한다. 예를 들어, LLaMA-13B 모델의 Wiki2 데이터셋에서 9.18을 달성하고 FP16 기준선은 5.09로, 1비트 이상의 2비트 양자화를 사용함에도 불구하고 다른 방법의 성능 손실은 매우 크다. GPTQ와 LLM-QAT의 경우 양자화 후 성능 저하가 상당히 심하다. 옴니퀀트의 경우 W2A16 설정에서 가장 강력한 기준선임에도 불구하고 여전히 W1A16 설정에 비해 더 큰 성능 손실을 겪는다.\n' +
      '\n' +
      '제로 샷 정확도의 경우 모든 방법이 필연적으로 약간의 열화를 갖지만, 우리의 방법은 대부분의 모델 중에서 FP16 기준선에 가장 가까운 성능을 달성한다. OPT-1.3B/2.7B\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c|}{**Perplexity(\\(\\downarrow\\))**} & \\multicolumn{6}{c}{**Zero-shot Accuracy(\\(\\uparrow\\))**} \\\\  & & **Wiki2** & **C4** & **Winogrande** & **Hellaswag** & **PIQA** & **BoolQ** & **ARC-e** & **ARC-c** & **Avg.** \\\\ \\hline \\multirow{6}{*}{OPT-1.3B} & FP16 & 14.63 & 14.72 & 59.67 & 53.73 & 72.42 & 57.68 & 50.80 & 29.69 & 54.00 \\\\  & GPTQ & 9.5e3 & 3.8e3 & 49.33 & 25.57 & 52.07 & 39.60 & 26.68 & 23.63 & 36.15 \\\\  & LLM-QAT & 4.9e3 & 2.1e3 & 49.72 & 25.72 & 50.05 & 37.83 & 25.76 & **25.09** & 35.70 \\\\  & OmniQuant & 42.43 & 55.64 & **51.85** & 33.39 & 60.94 & 56.45 & 38.76 & 23.38 & 44.13 \\\\  & OneBit & **25.42** & **22.95** & 51.14 & **34.26** & **62.57** & **59.45** & **41.25** & 24.06 & **45.46** \\\\ \\hline \\multirow{6}{*}{OPT-2.7B} & FP16 & 12.47 & 13.17 & 60.93 & 60.59 & 74.81 & 60.28 & 54.34 & 31.31 & 57.04 \\\\  & GPTQ & 8.7e3 & 3.9e3 & 49.88 & 26.47 & 49.84 & 39.88 & 25.76 & **26.02** & 36.31 \\\\  & LLM-QAT & 3.7e3 & 1.4e3 & 52.09 & 25.47 & 49.29 & 37.83 & 24.92 & 25.60 & 35.87 \\\\  & OmniQuant & 30.25 & 41.31 & 51.62 & **38.21** & 62.19 & 54.25 & 40.82 & 24.74 & 45.31 \\\\  & OneBit & **21.86** & **20.76** & **51.67** & 38.18 & **63.87** & **54.28** & **43.39** & 24.40 & **45.97** \\\\ \\hline \\multirow{6}{*}{LLaMA-7B} & FP16 & 5.68 & 7.08 & 66.85 & 72.99 & 77.37 & 73.21 & 52.53 & 41.38 & 64.06 \\\\  & GPTQ & 1.9e3 & 7.8e2 & 49.41 & 25.63 & 49.95 & 43.79 & 25.84 & 27.47 & 37.02 \\\\ \\cline{1-1}  & LLM-QAT & 7.1e2 & 3.0e2 & 51.78 & 24.76 & 50.87 & 37.83 & 26.26 & 25.51 & 36.17 \\\\ \\cline{1-1}  & OmniQuant & 15.34 & 26.21 & 52.96 & 43.68 & 62.79 & 58.69 & 41.54 & 29.35 & 48.17 \\\\ \\cline{1-1}  & OneBit & **10.38** & **11.56** & **60.30** & **50.73** & **67.46** & **62.51** & **41.71** & **29.61** & **52.05** \\\\ \\hline \\multirow{6}{*}{LLaMA-13B} & FP16 & 5.09 & 6.61 & 70.17 & 76.24 & 79.05 & 68.47 & 59.85 & 44.54 & 66.39 \\\\ \\cline{1-1}  & GPTQ & 3.2e3 & 9.9e2 & 50.67 & 25.27 & 50.00 & 42.39 & 26.14 & 27.39 & 36.98 \\\\ \\cline{1-1}  & LLM-QAT & 1.8e3 & 1.2e3 & 51.62 & 25.40 & 50.33 & 37.83 & 27.02 & 26.87 & 36.51 \\\\ \\cline{1-1}  & OmniQuant & 13.43 & 19.33 & 53.83 & 54.16 & 68.99 & 62.20 & **45.50** & 30.38 & 52.51 \\\\ \\cline{1-1}  & OneBit & **9.18** & **10.25** & **62.90** & **56.78** & **70.67** & **64.16** & 44.53 & **32.00** & **55.17** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 평가 실험의 주요 결과. 우리는 당혹감과 제로 샷 정확도를 보고한다. "FP16"은 FP16 파라미터를 가진 트랜스포머이며 우리는 이를 모든 방법의 상한이라고 한다. *** 최고 점수입니다.\n' +
      '\n' +
      '모델, 제안된 방법은 PIQA 및 ARC-e와 같은 대부분의 태스크에서 더 작은 성능 손실을 보인다. 또한 다른 작업의 손실은 차선책인 옴니퀀트에 비해 무시할 수 있다. LLAMA-7B 모델에서, 우리의 방법은 ARC-e/ARC-c를 제외한 모든 작업에서 옴니퀀트를 현저하게 능가하여 전체적으로 약 4% 개선되었다.\n' +
      '\n' +
      '문제 해결 능력\n' +
      '\n' +
      '우리는 W1A16 설정에서 다른 대표적인 기준선과 비교하여 본 방법의 우수한 성능을 입증했다. 모든 방법은 필연적으로 1비트 가중치 양자화에서 성능 저하에 직면하지만, 모델 크기를 줄이기 위한 다양한 접근 방식 중 실제 문제를 해결하는 데 있어 우리의 방법이 어떻게 작용하는지 여전히 관심이 있다. 예를 들어, 더 작은 모델들을 직접 트레이닝하거나(Zhang et al., 2024), 또는 파라미터들의 수를 감소시키기 위해 낮은-순위 분해를 채용한다.\n' +
      '\n' +
      '이를 위해 우리는 LLM의 두 가지 중요한 능력인 상식 추론과 세계 지식을 고려한다. 상식추론은 4.2절에 기술된 6개의 태스크(Hellaswag 등)와 설정을 사용한다. 세계지식은 광범위한 영역과 지식을 포괄하는 벤치마크인 Massive Multi-task Language Understanding(MMLU; Hendrycks et al., 2021)을 사용하여 살펴본다. 우리는 다음의 4가지 모델을 비교한다:\n' +
      '\n' +
      '**Pythia-1.0B**(Biderman et al., 2023). 메모리 풋프린트가 7B 모델의 1.54배인 EleutherAI가 잘 훈련된 모델을 출시했습니다.\n' +
      '\n' +
      '**TinyLLAMA-1.1B**(Zhang et al., 2024). 지속적인 훈련을 받는 LLaMA 모델과 동일한 구조의 모델이다. 공정하게 비교하기 위해, 우리는 10k 훈련 단계에서 체크포인트를 사용하는데, 이것은 우리 모델의 2배이다.\n' +
      '\n' +
      '**LowRank LLaMA**(Noach and Goldberg, 2020). 선형 레이어의 모든 가중치 행렬을 두 개의 낮은 순위 행렬로 분해하고 원비트-7B의 동일한 설정에서 KD에 의해 원래의 LLaMA-7B 모델로부터 학습한다.\n' +
      '\n' +
      '**OneBit-7B** OneBit로 구축된 섹션 4.2에서 사용하는 모델입니다.\n' +
      '\n' +
      '그림 2(a)와 2(b)는 서로 다른 모델의 상식 추론 능력과 일반적인 세계 지식을 보여준다. 우리는 다른 모델이 우리 모델보다 더 많은 매개변수를 가지고 더 철저하게 훈련되었지만 우리의 모델은 여전히 상식 추론에 이점이 있음을 관찰할 수 있다. 이는 더 큰 7B 모형에서 승계되는 이익을 반영한다. 세계 지식 측면에서, 사회 과학의 상당한 손실에도 불구하고, 우리의 모델은 다른 영역에서 완전히 훈련된 피티아-1B를 능가한다. 이러한 결과는 OneBit의 실용성을 입증한다.\n' +
      '\n' +
      '##5 분석 및 논의\n' +
      '\n' +
      '### Efficiency\n' +
      '\n' +
      '가중치의 극히 낮은 비트 양자화는 메모리 풋을 상당히 감소시킬 수 있다는 것이 명백하다.\n' +
      '\n' +
      '그림 3: 모델 성능 및 압축 정도 비교.\n' +
      '\n' +
      '모델 자국이요 표 3에서 보는 바와 같이 모델 크기가 커질수록 실제 압축비는 증가한다. 이는 더 큰 모델에 특히 의미가 있어 하나의 GPU에 모델을 맞추는 것이 가능하다. 성능 손실이 있는 반면, 그림 4는 우리의 방법이 공간 점유와 모델 성능 사이에서 좋은 균형을 달성한다는 것을 보여준다. 예를 들어, 우리는 모델 공간의 0.2배만으로 FP16과 유사한 성능을 달성할 수 있다. 또한, \\(\\pm 1\\)으로 양자화하면 CPU에서 행렬 곱셈을 가속하는 데 도움이 된다. 두 행렬에서 원소의 부동 소수점 곱셈은 이러한 칩에서 훨씬 더 빠른 비트 연산으로 변환될 수 있기 때문이다. 따라서 메모리 오버헤드의 상당한 감소는 이러한 저비트 LLM을 PC 및 스마트폰에 배포하기 위한 요구 사항을 충족시킨다.\n' +
      '\n' +
      '### Robustness\n' +
      '\n' +
      '기존 연구 Wang et al.(2023)은 이미 QAT 내의 불안정성에 주목하였다. 극도로 낮은 비트 양자화는 트레이닝 프로세스를 학습 속도에 매우 민감하게 하여, 속도가 너무 작거나 너무 클 때 모델이 수렴하는 것을 어렵게 한다. 이는 주로 중량 요소가 +1과 -1 사이에서 변동함에 따라 발생하는 큰 구배의 크기로 인해 선형 레이어의 출력이 크게 변동하기 때문이다. 실험은 OneBit가 보다 안정적인 훈련 과정을 보이고 학습 속도에 민감하지 않다는 것을 보여준다. 자세한 내용은 부록 A.5를 참조하십시오.\n' +
      '\n' +
      '###성분에 따른 효과\n' +
      '\n' +
      '이 방법의 변수 구성 요소는 주로 Post-LayerNorm, 값 벡터 및 매개변수 초기화를 포함한다.\n' +
      '\n' +
      'Post-LayerNorm 우리는 모델이 QAT 프로세스 동안 부동 소수점 오버플로를 경험할 수 있음을 발견한다. 깊이가 증가함에 따라, 활성화는 점진적으로 더 커질 수 있다. 우리는 Pre-LayerNorm 대신 Post-LayerNorm을 사용하여 그것을 해결한다. 대조적으로, Pre-LayerNorm은 때때로 효과가 없을 수 있다.\n' +
      '\n' +
      'Value Vectors OneBit와 BitNet Wang 등(2023)의 주요 구조적 차이점은 두 개의 값 벡터로, 섹션 4.2에서 효과적인 것으로 입증된다. 비교에 대한 자세한 내용은 부록 A.5를 참조하라.\n' +
      '\n' +
      '제안된 SVID에서 매개변수 초기화는 NMF와 SVD를 모두 사용하여 \\(|\\mathbf{W}|\\)을 분해할 수 있으며 전자를 사용하는 것이 좋다. 이는 NMF가 훈련을 더 빠르게 수렴하도록 할 수 있다는 것을 발견하기 때문이다. 그림 5는 NMF에 의한 초기화가 더 나은 성능을 촉진한다는 것을 보여준다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 1비트 가중치 양자화를 위한 모델 구조와 이에 대응하는 파라미터 초기화 방법을 제안한다. 다양한 크기와 시리즈의 모델에 대한 광범위한 실험은 원비트가 대표적인 강력한 기준선에 비해 분명한 이점을 가지고 있으며 모델 크기와 성능 사이의 좋은 균형을 달성한다는 것을 보여준다. 우리는 이러한 극도로 낮은 비트 양자화 모델의 기능을 추가로 분석하고 향후 연구에 대한 지침을 제공한다.\n' +
      '\n' +
      '그림 4: 모델 크기와 복잡성 간의 트레이드오프.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Models** & **FP16 (GB)** & **OneBit (GB)** & **Ratio (\\%)** \\\\ \\hline LLaMA-7B & 13.5 & 1.3 & 90.4 \\\\ LLaMA-13B & 26.0 & 2.2 & 91.5 \\\\ LLaMA-30B & 65.1 & 4.9 & 92.5 \\\\ LLaMA-65B & 130.6 & 9.2 & 93.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LLaMA 모델의 압축비.\n' +
      '\n' +
      '도 5: OneBit-7B의 트레이닝 과정.\n' +
      '\n' +
      '### Limitation\n' +
      '\n' +
      '제안된 방법은 LLM의 메모리 풋프린트를 크게 감소시켜 효율적인 배치에 대한 희망을 가져오지만 여전히 몇 가지 제한 사항이 있다. 첫째, 원래의 모델과 비교하여, 우리의 극도로 낮은 비트 양자화는 필연적으로 성능 손실을 초래한다. 또한, 1비트 양자화된 모델의 최적 매개변수 뒤에 있는 수학적 원리를 아직 이해하지 못했기 때문에 능력 전달은 KD의 비용이 많이 드는 과정을 통해서만 달성할 수 있다. 다행히도 이 비용은 일회성 비용입니다. 또한, 1-비트 양자화의 고유한 특성 때문에, 우리의 방법은 더 높은 비트-폭으로 자연스럽게 확장될 수 없다. 마지막으로 활성화 양자화를 고려하지 않고 향후 작업으로 남겨두었다.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      '이 연구에서는 공개적으로 사용 가능하고 오픈 소스인 모델을 사용한다. 우리는 이러한 모델의 사용이 원래 의도된 목적과 일치함을 확인한다. 이러한 모델은 윤리적 지침을 준수하고 오픈 소스 라이선스의 준수를 보장하면서 학술 및 연구 기반 활동의 범위 내에서 엄격하게 활용되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. GKD: Automatic Regressive Sequence Model을 위한 Generalized Knowledge distillation. _ arXiv preprint arXiv:2306.13649_.\n' +
      '* 벨트라미(1990) E 벨트라미. 1990. Sulle funzioni bilineari, giomale di mathematicshe ad uso studenti delle uninerista. 11, 98-106.(d boley의 영어 번역은 미네소타 대학교, 컴퓨터과학과로 이용 가능하다.) 기술 보고서, 기술 보고서 90-37.\n' +
      '* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbic Bradley, Kyle O\'Brien, Eric Hallahan, Mohammad Afhal Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: 훈련 및 스케일링에 걸친 대규모 언어 모델을 분석하기 위한 스위트. _ICML_에서, 페이지 2397-2430.\n' +
      '* Bisk et al. (2020) 요나탄 비스크, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. PIQA: Reasoning about physical commonsense in natural language. _Proceedings of the AAAI_, volume 34, pages 7432-7439.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ NeurIPS_, 33:1877-1901의 발전.\n' +
      '* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_.\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구하는 것 _ arXiv preprint arXiv:1905.10044_.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018년 질문 답은 해결한 것 같아? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_.\n' +
      '* Dettmers et al.(2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 스케일에서 변압기에 대한 8비트 행렬 곱셈 _ arXiv preprint arXiv:2208.07339_.\n' +
      '* Dettmers et al.(2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023a. QLoRA: 양자화된 LLMs의 효율적인 미세 조정. In _Advances in NeurIPS_.\n' +
      '*Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuzmedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023b. SpQR: 거의 손실 없는 llm 가중치 압축을 위한 희소 양자화 표현 _ arXiv preprint arXiv:2306.03078_.\n' +
      '*Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. 4 비트 정밀도에 대한 경우: k 비트 추론 스케일링 법칙. _ICML_에서, 페이지 7750-7774.\n' +
      '* Frantar and Alistarh (2023) Ellias Frantar and Dan Alistarh. 2023. SparseGPT: 대규모 언어 모델은 원샷으로 정확하게 가지치기될 수 있다. _ICML_에서, 페이지 10323-10337.\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: 생성적 사전 훈련된 변압기에 대한 정확한 사후 훈련 양자화_ arXiv preprint arXiv:2210.17323_.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. 방대한 멀티태스크 언어 이해도 측정 _ICLR_에서.\n' +
      '* Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. 단계별로 증류하는 단계! 학습 데이터가 적고 모델 크기가 작을수록 더 큰 언어 모델을 능가합니다. ACL_의 _Findings에서, 페이지 8003-8017.\n' +
      '* 김 등(2023a) 김정훈, 이정현, 김성동, 박준석, 강민유, 세정권, 동수. 2023a. 서브-4 비트 정수 양자화를 통한 압축된 대형 언어 모델의 메모리 효율적인 미세 조정. _ arXiv preprint arXiv:2305.14152_.\n' +
      '\n' +
      '김세훈, 콜먼 후퍼, 아미르 골라미, 젠동, 시유 리, 셍셴, 마이클 마호니, 커트 커트 커처 등이다. 2023b. SqueezeLLM: Dense-and-sparse quantization. _ arXiv preprint arXiv:2306.07629_.\n' +
      '* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: stochastic optimization의 방법. _ arXiv preprint arXiv:1412.6980_.\n' +
      '* Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: llm 압축 및 가속을 위한 활성화 인식 가중치 양자화_ arXiv preprint arXiv:2306.00978_.\n' +
      '* Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: 대용량 언어 모델에 대한 데이터 프리 양자화 인식 훈련_ arXiv preprint arXiv:2305.17888_.\n' +
      '* Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: on the structural pruning of large language models. In _Advances in NeurIPS_.\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. 포인터 센티넬 혼합 모델_ arXiv preprint arXiv:1609.07843_.\n' +
      '* Noach and Goldberg (2020) Matan Ben Noach and Yoav Goldberg. 2020. 매트릭스 분해에 의해 미리 훈련된 언어 모델들을 압축하는 단계. AACL-IJCNLP_의 _Proceedings, pages 884-889.\n' +
      '* Paatero and Tapper (1994) Pentti Paatero and Unto Tapper. 1994. positive matrix factorization: non-negative factor model with optimal utilization of error estimates of data values. _ Environmetrics_, 5(2):111-126.\n' +
      '* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. 명령어 튜닝 with gpt-4. _arXiv preprint arXiv:2304.03277_.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스크 학습자들이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: 적대적 winograd schema challenge at scale. _ ACM_, 64(9):99-106의 통신.\n' +
      '* Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 및 Ping Luo. 2023. 옴니퀀트: 대형 언어 모델에 대한 옴니디방향 보정된 양자화_ arXiv preprint arXiv:2308.13137_.\n' +
      '* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. 대형 언어 모델들에 대한 간단하고 효과적인 프루닝 접근법. _ arXiv preprint arXiv:2306.11695_.\n' +
      '* Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. _Proceedings of the EMNLP-IJCNLP_, pages 4323-4332.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. 스탠포드 알파카: 명령어-추종 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의력만 있으면 됩니다 _ NeurIPS_, 30에 진전이 있습니다.\n' +
      '* Wan et al. (2023) Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. 2023. Efficient large language models: A survey. _ arXiv preprint arXiv:2312.03863_.\n' +
      '* Wang et al. (2023) Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. BitNet: 대형 언어 모델용 스케일링 1비트 트랜스포머 _ arXiv preprint arXiv:2310.11453_.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. SmoothQuant: 대형 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화. _ICML_에서, 페이지 38087-38099.\n' +
      '* Xu et al. (2023) Mingxue Xu, Yao Lei Xu, and Danilo P Mandic. 2023. 텐서플로우GPT: 텐서-트레인 분해에 기초한 llms 내의 임베딩 레이어의 효율적인 압축. _ arXiv preprint arXiv:2307.00526_.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, 요나탄 Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: 기계가 정말로 당신의 문장을 끝낼 수 있나요? _ arXiv preprint arXiv:1905.07830_.\n' +
      '* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: 오픈소스 소형 언어 모델 _ arXiv preprint arXiv:2401.02385_.\n' +
      '* Zhu et al.(2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. 대형 언어 모델에 대한 모델 압축에 관한 조사_ arXiv preprint arXiv:2308.07633_.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '### 명제의 증명\n' +
      '\n' +
      '본 절에서는 본 논문에서 제시한 명제에 대한 필요적이고 상세한 증명을 제시한다. 모든 기호는 본문과 동일한 정의를 가지고 있다.\n' +
      '\n' +
      '**명제 1** 가중치 행렬 \\(\\mathbf{W}\\) 및 입력 \\(\\mathbf{X}\\)이 주어지면, 선형 층은 SVID에 따라 다음과 같이 재구성될 수 있다:\n' +
      '\n' +
      '\\mathbf{W}^{\\mathrm{T}}\\approx\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{b}^{\\mathrm{T}\\right)\\mathbf{W}_{\\mathrm{sign}^{\\mathrm{T}\\big{}\\odot\\mathbf{a}^{\\mathrm{T}}}}}}}}\\mathrm{T}}}}}.\\mathbf{a}^{\\mathrm{T}}}}}\n' +
      '\n' +
      'Eq. (4) 우리는 \\(w_{ij}\\approx s_{ij}\\cdot a_{i}b_{j}\\을 가지며, 여기서 \\(s_{ij}\\)는 \\(\\mathbf{W}_{\\mathrm{sign}\\)의 원소이다. 그래서 우리는\n' +
      '\n' +
      'bf{W}x_{ik}w_{jk}^{\\mathrm{T}\\}=\\sum_{k}x_{j}a_{j}\\}[=\\sum_{k}\\left(\\mathbf{X}\\odot\\mathbf{W}{\\mathrm{T}}\\big}\\mathbf{W}{\\mathrm{T}}\\big}\n' +
      '\n' +
      '이 명제는 증명되었다.\n' +
      '\n' +
      '**Lemma 1** Let \\(\\sigma_{i}\\left(\\mathbf{W}\\right)\\)는 행렬 \\(\\mathbf{W}\\)의 _i_th 최대 특이값을 나타낸다. 다음의 부등식이 성립한다:\n' +
      '\n' +
      '\\[\\sigma_{1}\\left(\\left|\\mathbf{W}\\right|\\right)\\geq\\sigma_{1}\\left(\\mathbf{W} \\right).\\]\n' +
      '\n' +
      '유도된 규범의 정의에 따르면 다음과 같은 증거가 있다.\n' +
      '\n' +
      'bf{W}\\|_{1}\\left(\\mathbf{W}\\|_{2})=\\max_{\\mathbf{x}\\|_{2}=1}\\|\\mathbf{x}\\|_{2},\\\\sigma_{1}\\left(\\left|\\mathbf{W}\\|_{2})=\\|\\mathbf{W}\\|_{y},\\|\\mathbf{y}\\|_{2}=1}\\|\\mathbf{w}\\|\\mathbf{y}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\mathbf{w}\\|\\math\n' +
      '\n' +
      '주목할 점은 \\(\\forall\\mathbf{x}\\), \\(\\|\\mathbf{x}\\|_{2}=1\\) 그리고 우리는\n' +
      '\n' +
      'bf{x}\\|\\mathbf{w}\\|\\j}\\Bigg{(}\\sum_{j}|w_{j}|\\Bigg{(}\\geq\\sum_{i}\\Bigg{(}\\sum_{j}w_{ij}x_{j}|\\Bigg{}}^{2}\\Bigg{(}\\sum_{j}w_{j}|\\Bigg{(}\\sum_{j}\\Bigg{(}\\sum_{j}w_{j}\\Bigg{(}\\sum_{j}\\Bigg{W}\\mathbf{x}\\|_{2}^{2}}}\n' +
      '\n' +
      'Therefore\n' +
      '\n' +
      '\\[\\max_{\\mathbf{y},\\|\\mathbf{y}\\|_{2}=1}\\|\\mathbf{W}|\\mathbf{y}\\|_{2}\\geq\\max_{ \\mathbf{x},\\|\\mathbf{x}\\|_{2}=1}\\|\\mathbf{W}\\mathbf{x}\\|_{2}.\\]\n' +
      '\n' +
      '이 딜레마가 증명되었다.\n' +
      '\n' +
      '**명제 2** 주어진 행렬 \\(\\mathbf{W}\\) 및 \\(|\\mathbf{W}|\\), \\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}\\odot|\\mathbf{W}|\\) 이 행렬들을 \\(\\mathbf{W}=\\mathbf{a}\\mathbf{b}^{\\mathrm{T}+\\mathbf{E}{1}\\)과 \\(\\mathbf{W}|=\\tilde{\\mathbf{a}\\tilde{\\mathbf{b}}\\tilde{\\mathbf{b}}+\\mathbf{E}{2}\\)으로 분해하고, 여기서 \\(\\mathbf{E}_{i}\\)은 오차행렬을 나타낸다. 프로베니우스-norm의 관점에서, SVID는 원래의 행렬 \\(\\mathbf{W}\\)에 더 가깝다:\n' +
      '\n' +
      '\\mathbf{W}_{\\mathrm{sign}\\odot\\tilde{\\mathbf{a}\\tilde{\\mathbf{b}^{\\mathrm{T}\\right\\|_{\\mathrm{F}\\leq\\left\\|\\mathbf{W}-\\mathbf{b}^{\\mathrm{T}\\right\\|_{\\mathrm{F}^{2}\\leq\\left\\|\\mathbf{b}^{\\mathrm{F}^{2}}\\right\\|\\mathrm{F}}^{2}\\mathbf{b}^{\\mathrm{F}}^{2}\\mathm{a}\\mathm{f}\\right\\|\\mathrm{F}}^{2}\\mathm{a}\\mathbf{b}^{t}\\right\\|\\mathrm{F}}^{2}\\mathm{a}\\mathbf{b}\\right\\|\\mathrm{F}}^{2}\\mathm{a}\n' +
      '\n' +
      '여기서 우리는 그것을 증명하기 위해 SVD를 고려한다. SVD의 경우 rank-1 근사법에서 오차 행렬 \\(\\mathbf{E}\\)의 norm은 가장 큰 값을 제외한 모든 특이값의 제곱의 합이다. 우리에겐\n' +
      '\n' +
      '(\\mathbf{E}_{1}\\|_{F}^{2} =\\sum_{i=2}^{n}\\sigma_{i}^{2}\\left(\\mathbff{W}\\right),\\] \\[\\|\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i=2}^{n}\\sigma_{i}^{2}\\left(\\left|\\mathbff{W}\\right|\\right),\\\\[\\|\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i=2}^{n}\\sigma_{i}\\left(\\left|\\mathbf{W}\\right|\\right),\\\\[\\|\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i=2}\\sigma_{i}\\sigma_{i}\\left(\\left|\\mathbf\n' +
      '\n' +
      '\\(\\|\\mathbff{W}\\|_{F}^{2}=\\||\\mathbff{W}\\|_{F}^{2}\\)에 기초하여, 우리는\n' +
      '\n' +
      '\\[\\sum_{i=1}^{n}\\sigma_{i}^{2}\\left(\\mathbf{W}\\right)=\\sum_{i=1}^{n}\\sigma_{i}^{2 }\\left(\\left|\\mathbf{W}\\right|\\right).\\]\n' +
      '\n' +
      'Lemma 1에 따르면 우리는 결론을 내릴 수 있다.\n' +
      '\n' +
      '\\[\\|\\mathbf{E}_{2}\\|_{F}^{2}\\leq\\|\\mathbf{E}_{1}\\|_{F}^{2}.\\]\n' +
      '\n' +
      '이 명제의 방정식으로부터 우리는 공식화할 수 있다\n' +
      '\n' +
      '\\mathbf{W}_{\\mathrm{sign}\\odot|\\mathbf{W}|=\\mathbf{W}_{\\mathrm{sign}\\odot\\tilde{\\mathbf{a}\\tilde{\\mathbf{b}}^{\\mathrm{T}+\\mathbf{W}_{\\mathrm{sign}\\odot\\mathbf{E}_{2}.\\\n' +
      '\n' +
      '그래서 우리는\n' +
      '\n' +
      '\\[\\mathbf{W}-\\mathbf{W}_{\\mathrm{sign}}\\odot\\tilde{\\mathbf{a}}\\tilde{\\mathbf{b}} ^{\\mathrm{T}}=\\mathbf{W}_{\\mathrm{sign}}\\odot\\mathbf{E}_{2}.\\]\n' +
      '\n' +
      'Therefore\n' +
      '\n' +
      '\\odot\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i,j}s_{ij}^{2}e_{ij}^{2}=\\sum_{i,j}e_{ij}^{2}\\] \\[=\\|\\mathbf{E}_{2}\\|_{F}^{2}\\leq\\leq\\mathbf{E}_{1}\\|_{F}^{2},\\\\\n' +
      '\n' +
      '여기서 \\(s_{ij}=\\pm 1\\)은 \\(\\mathbf{W}_{\\mathrm{sign}}\\)의 원소이다. 그러므로 이 명제의 불공평이 증명되었다.\n' +
      '\n' +
      '기준선에 대한 자세한 내용\n' +
      '\n' +
      '이 하위 섹션에서는 이 작업에서 기준선의 필수 세부 정보를 제공합니다.\n' +
      '\n' +
      '* GPTQ(Frantar et al., 2022): 저자가 공개한 오픈소스 코드를 채용한다. OPT 모델과 LLaMA 모델 모두 양자화된 모델을 보정하기 위해 C4 데이터 세트에서 128개의 2048-토큰 샘플을 취한다. LLaMA 모델의 경우 코드의 추천에 따라 활성화 순서 휴리스틱을 적용한다.\n' +
      '* LLM-QAT (Liu et al., 2023): LLM-QAT가 2 비트 가중치 양자화를 위해 설계되지 않기 때문에, W2A16 설정을 적응시키기 위해 이 방법을 재구현한다. 우리는 또한 KV 캐시를 양자화하지 않는다. 선형 계층에서 가중치 행렬을 양자화할 때, 우리는 영점을 0으로 설정하는 대칭 MinMax 양자화를 사용한다. 훈련 하이퍼-파라미터는 우리의 것과 동일하다. 섹션 4.1의 교육 세부 정보를 참조하십시오.\n' +
      '* OmniQuant(Shao et al., 2023): 저자가 공개한 오픈 소스 코드를 채용한다. OPT 모델과 LLaMA 모델 모두 양자화된 모델을 보정하기 위해 위키Text2 데이터 세트에서 128개의 2048-토큰 샘플을 취한다. 학습 가능한 가중치 클리핑 및 등가 변환에 대한 학습률은 각각 5e-3 및 1e-2로 설정된다. 우리는 1의 배치 크기를 사용하고 각 모델에 대해 40개의 에포크를 훈련한다. OPT 모델의 경우 학습 가능한 가중치 클리핑과 등가 변환이 모두 활용된다. LLaMA 모델의 경우 학습 가능한 웨이트 클리핑만 사용됩니다.\n' +
      '\n' +
      '### LLaMA2 결과\n' +
      '\n' +
      '표 4는 LLaMA2-7B/13B에 대한 결과를 비교한다. 분명히, 우리의 방법은 당혹스러움과 제로 샷 정확도 모두에서 이점이 있다. 그것은 또한 우리의 방법의 장점이 더 큰 모델에서 더 두드러진다는 것을 반영한다. 예를 들어, LLaMA2-7B에서 LLaMA2-13B로 스케일링할 때 FP16 모델의 복잡도는 약 0.5만큼 감소하는 반면, 우리의 방법은 위키2 및 C4 데이터 세트 모두에서 약 1.0만큼 감소한다.\n' +
      '\n' +
      '##### 실행 추적 능력\n' +
      '\n' +
      '명령어 팔로잉은 LLMs(Radford et al., 2019; Brown et al., 2020; Peng et al., 2023)의 중요한 능력이다. 이전 모델 능력과 효율성에 대한 논의를 넘어 실제 사용성과 밀접한 관련이 있는 극저비트 모델의 명령어 추종 능력에도 초점을 맞춘다. 이 하위 섹션에서 우리는 양자화된 모델의 이 능력을 경험적으로 연구한다. 우리는 al-paca_en_52k 데이터세트와 alpaca 템플릿(Taori et al., 2023)을 사용하여 3개의 에폭에 대한 모델을 미세 조정한 다음 미세 조정 전후에 제로 샷 및 소수 샷 설정 모두에서 생성을 관찰한다. 트레이닝 동안, 학습 속도는 1e-7로 설정되고 배치 크기는 32로 설정된다. 다른 파라미터들은 섹션 4.1과 일치한다.\n' +
      '\n' +
      '표 5는 우리의 7B 모델의 콘텐츠 생성 및 지시에 따른 능력을 보여준다. 제로샷 설정 하에서 SFT가 없는 모델은 장황하고 반복적이며 품질이 낮은 텍스트를 생성했다. 그러나 SFT를 한번 경험하게 되면, 우리의 모델은 우수한 지시 추종 능력을 발휘하면서 고품질의 콘텐츠를 원활하게 출력할 수 있다. 소샷 설정의 경우, 본 모델은 SFT 전과 SFT 후에 모두 명령어 추적 기능을 발휘합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c|}{**Perplexity(\\(\\downarrow\\))**} & \\multicolumn{6}{c}{**Zero-shot Accuracy(\\(\\uparrow\\))**} \\\\  & & **Wiki2** & **C4** & **Winogrande** & **Hellawag** & **PIQA** & **BoolQ** & **ARC-e** & **ARC-c** & **Avg.** \\\\ \\hline \\multirow{8}{*}{LLaMA2-7B} & FP16 & 5.47 & 6.97 & 67.09 & 72.94 & 76.88 & 71.10 & 53.58 & 40.61 & 63.70 \\\\  & GPTQ & 7.763 & NAN & 50.28 & 26.19 & 49.46 & 42.97 & 26.77 & 28.58 & 37.38 \\\\  & LLM-QAT & 1.1e3 & 6.6e2 & 49.08 & 25.10 & 50.12 & 37.83 & 26.26 & 26.96 & 35.89 \\\\  & OmniQuant & 31.21 & 64.34 & 51.22 & 33.87 & 56.53 & 59.14 & 33.63 & 24.32 & 43.12 \\\\  & OneBit & **9.73** & **11.11** & **58.41** & **52.58** & **68.12** & **63.06** & **41.58** & **29.61** & **52.23** \\\\ \\hline \\multirow{8}{*}{LLaMA2-13B} & FP16 & 4.88 & 6.47 & 69.77 & 76.62 & 79.05 & 68.99 & 57.95 & 44.20 & 66.10 \\\\  & GPTQ & 2.1e3 & 3.2e2 & 51.85 & 25.67 & 51.74 & 40.61 & 25.46 & 27.30 & 37.11 \\\\ \\cline{1-1}  & LLM-QAT & 5.1e2 & 1.1e3 & 51.38 & 24.37 & 49.08 & 39.85 & 27.15 & 24.32 & 36.03 \\\\ \\cline{1-1}  & OmniQuant & 16.88 & 27.02 & 53.20 & 50.34 & 62.24 & 62.05 & 40.66 & 29.61 & 49.68 \\\\ \\cline{1-1}  & OneBit & **8.76** & **10.15** & **61.72** & **56.43** & **70.13** & **65.20** & **43.10** & **33.62** & **55.03** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: LLaMA2의 결과. **best** 점수를 굵게 표시하였다.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '마이크로소프트사가 개발한 여러 윈도우 운영체제를 나열하라.\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      '마이크로소프트 윈도우 10은 마이크로소프트가 개인용 컴퓨터와 태블릿에 사용할 수 있도록 개발한 무료 운영체제다. 그것은 2014년 4월 29일에 대중에게 처음 공개되었습니다.\n' +
      '\n' +
      '마이크로소프트 윈도우 10은 마이크로소프트가 개인용 컴퓨터와 태블릿에 사용할 수 있도록 개발한 무료 운영체제다. 2014년 4월 29일 처음 공개되었는데, 윈도우 10은 윈도우 8.1의 후속작이다.\n' +
      '\n' +
      '마이크로소프트 윈도우 10은 마이크로소프트가 개인용 컴퓨터와 태블릿에 사용할 수 있도록 개발한 무료 운영체제다. 2014년 4월 29일 처음 공개되었습니다. 윈도우 10은 윈도우 8.1의 후계자입니다. 윈도우 10은 윈도우 8.1의 후계자입니다.\n' +
      '\n' +
      '마이크로소프트 윈도우 10은 마이크로소프트가 개인용 컴퓨터와 태블릿에 사용할 수 있도록 개발한 무료 운영체제다.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '아래는 작업을 설명하는 지시사항입니다. 요청을 적절하게 완료하는 응답을 작성합니다.\n' +
      '\n' +
      '*** Instruction:\n' +
      '\n' +
      '마이크로소프트사가 개발한 여러 윈도우 운영체제를 나열하라.\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      '마이크로소프트는 윈도우 10, 윈도우 8, 윈도우 7, 윈도우 XP, 윈도우 2000, 윈도우 NT, 윈도우 ME, 윈도우 95, 윈도우 1 등 여러 윈도우 운영체제를 개발하였다. 윈도우 10은 윈도우 운영체제의 최신 버전이다. 윈도우는 2019년, 윈도우 8은 2011년, 윈도우 7은 2009년, 윈도우 XP는 2001년, 윈도우 NT는 1997년, 윈도우 ME는 1999년, 윈도우 95는 1995년, 윈도우 1은...\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '다음은 대학 생물학에 대한 객관식 질문이다.\n' +
      '\n' +
      '다음 중 유사분열 및 감수분열에서 염색체의 분리에 필요한 DNA 서열을 포함하는 것은? A. 텔로미어 B. 센트로미어 C. 뉴클레오솜 D. 스플리세오솜 Answer:\n' +
      '\n' +
      'B. 암컷 포유동물 세포 핵 내에서 발견되는 Barr body는 다음과 같은 예이다: A. euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. 극축에 비스듬한 각도로 발생하는 초기 세포 분열에서 발생하는 분열 대칭은 A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage Answer: C. 다음 중 어느 것이 대기 중 탄소의 공급원이 아님? A. Respiration B. Photosynthesis C. 박테리아 분해 D. Combustion of fossil fuels Answer: B.\n' +
      '\n' +
      'D. 다음 중 어느 것이 세포질로 칼슘 이온의 방출을 자극하는 두 번째 메신저인가? A. 프로스타글란딘 B. 칼모듈린 C. 사이클릭 AMP D. 이노시톨 삼인산 대답:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'D(정답)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline  & **Prompt:** \\\\  & List several Windows operating systems developed by Microsoft corporation. \\\\  & **Generation:** \\\\  & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. \\\\\n' +
      '**Zero-shot w/o SFT** & Microsoft Windows 10은 Microsoft가 개인용 컴퓨터 및 태블릿에 사용하기 위해 개발한 무료 운영체제이다. 2014년 4월 29일 처음 일반에 공개되었다. 윈도우 10은 윈도우 8.1의 후속작이다.\n' +
      '**Zero-shot w/o SFT** & Microsoft Windows 10은 Microsoft가 개인용 컴퓨터 및 태블릿에 사용하기 위해 개발한 무료 운영체제이다. 2014년 4월 29일 처음 일반에 공개되었다. 윈도우 10은 윈도우 8.1의 후속작이다.\n' +
      '**Zero-shot w/ SFT** & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. \\\\ \\hline\n' +
      '**Prompt:**\n' +
      '\n' +
      '아래는 작업을 설명하는 지시사항입니다. 요청을 적절하게 완료하는 응답을 작성합니다.\n' +
      '\n' +
      '*** Instruction:\n' +
      '\n' +
      '마이크로소프트사가 개발한 여러 윈도우 운영체제를 나열하라.\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      '마이크로소프트는 윈도우 10, 윈도우 8, 윈도우 7, 윈도우 XP, 윈도우 2000, 윈도우 NT, 윈도우 ME, 윈도우 95, 윈도우 1 등 여러 윈도우 운영체제를 개발하였다. 윈도우 10은 윈도우 운영체제의 최신 버전이다. 윈도우는 2019년, 윈도우 8은 2011년, 윈도우 7은 2009년, 윈도우 XP는 2001년, 윈도우 NT는 1997년, 윈도우 ME는 1999년, 윈도우 95는 1995년, 윈도우 1은...\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      '다음은 대학 생물학에 대한 객관식 질문이다.\n' +
      '\n' +
      '다음 중 유사분열 및 감수분열에서 염색체의 분리에 필요한 DNA 서열을 포함하는 것은? A. 텔로미어 B. 센트로미어 C. 뉴클레오솜 D. 스플리세오솜 Answer:\n' +
      '\n' +
      'B. 헤테로크로마틴 C. 시스트론 D. 슈도제네스 Answer: B. 극축에 비스듬한 각도로 발생하는 초기 세포 분열에서 발생하는 명확한 대칭은 다음과 같이 알려져 있다: A. 메로블라스틱 절단 B. 방사형 절단 C. 나선형 절단 D. 표면 절단 Answer: C. 다음 중 대기 중 탄소의 공급원이 아닌 것은? A. 호흡 B. 광합성 C. 박테리아 분해 D. 화석 연료의 연소: B. 점적 평형의 개념은: A. 진동 생태학적 천이 단계 B. 갑작스러운 환경 변화, 예를 들어 화재 C. 지속 포식자-먹이 관계에 의해 체포된 생태학적 천이 단계 B. 상대적으로 변하지 않는 계통을 말한다:\n' +
      '\n' +
      'D. 다음 중 어느 것이 세포질로 칼슘 이온의 방출을 자극하는 두 번째 메신저인가? A. 프로스타글란딘 B. 칼모듈린 C. 사이클릭 AMP D. 이노시톨 삼인산 대답:\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'D(정답)\n' +
      '\n' +
      '\\end{table}\n' +
      '표 5: SFT(Supervised Fine-Tuning) 전후의 제로 샷 및 소수 샷 설정에서의 경우에 따른 지시. 몇 안 되는 예는 MMLU 벤치마크에서 가져온 것입니다.\n' +
      '\n' +
      '### BitNet과의 비교\n' +
      '\n' +
      '최근 BitNet[23]은 1비트 모델 아키텍처를 도입하여 처음부터 모델을 훈련하는 데 적용하여 1비트 모델 구조의 실현 가능성과 적용 가치를 입증하고 있다. 본 논문에서는 LLaMA-7B 모델을 정량화하기 위해 1비트 양자화와 지식 증류를 결합하려고 한다. 불행히도 더 큰 학습률을 사용하라는 제안을 따르더라도 훈련 중에 행동은 여전히 불안정하다.\n' +
      '\n' +
      '그림 6은 비트넷의 훈련 과정이 지식 증류 과정에서 불안정성을 겪을 수 있음을 보여준다. 우리는 가중치 요소가 +1과 -1 사이에서 변동할 때 기울기가 상당히 커서 선형 레이어의 출력을 더욱 악화시키기 때문이라고 추측한다.\n' +
      '\n' +
      '보다 효과적인 측도로서, 양자화를 위해 제안하는 값 벡터는 필요한 부동 소수점 수치 정밀도를 보완할 뿐만 아니라 양자화 후 행렬 곱셈 결과의 변동 범위를 제한한다. 이것은 각각 순방향 및 역방향 계산으로부터 이해될 수 있다.\n' +
      '\n' +
      '정방향 안정성.양자화된 행렬 곱셈은 입력 활성화의 사소한 섭동에 대한 응답으로 FP16 대응보다 오버플로되기 쉽다. 이는 양자화된 행렬에서 원소의 크기, 특히 값 \\(\\pm 1\\)이 대부분의 FP16 행렬의 매개변수보다 훨씬 크기 때문이다. FP16 모델과 유사한 크기의 값 벡터를 곱함으로써 모델 출력 활성화의 변동 범위를 FP16 수준으로 복원할 수 있으며, Post-LayerNorm을 통해 활성화의 점점 더 커지는 "드리프트 현상"을 피할 수 있다.\n' +
      '\n' +
      '역방향 안정성.\\(\\mathrm{Sign}(\\cdot)\\) 함수는 미분할 수 없기 때문에 행렬의 원소가 변하면 그 기울기가 무한대가 될 수 있다. 순방향 안정성과 유사하게 두 개의 수치적으로 더 작은 값 벡터를 곱함으로써 기울기 역전파 동안 층별 축적 및 폭발을 방지한다. 또한 쌍곡선 탄젠트 함수의 미분을 이용하여 \\(\\mathrm{Sign}(\\cdot)\\)의 미분함수를 구현함으로써 모든 무게의 영점에서의 구배 폭발 문제를 피할 수 있다.\n' +
      '\n' +
      '### 선형 계층의 평균 비트 폭\n' +
      '\n' +
      '이 하위 섹션은 선형 레이어의 평균 비트 폭의 계산을 공식화한다. 그러한 레이어에 \\(4096\\times 4096\\)의 형상을 갖는 웨이트 매트릭스가 있다고 가정하면, 모든 컴포넌트에서의 비트 수는\n' +
      '\n' +
      '\\[1\\times 4096\\times 4096,\\] \\[16\\times 1\\times 4096\\times 2,\\]\n' +
      '\n' +
      '여기서 첫 번째는 1비트 양자화된 가중치 행렬에 대한 것이고 두 번째는 두 FP16 값 벡터에 대한 것이다. 따라서 전체 비트 수는 \\(16,908,288\\)이다. 또한 매개변수 수는 \\(4096\\times 4096+2\\times 4096\\times 1=16,785,408\\)이다. 따라서 이 선형층의 평균 비트폭은 \\(16,908,288\\div 16,785,408\\approx 1.0073\\)이다.\n' +
      '\n' +
      '그림 6: BitNet이 LLaMA-7B로부터 지식 증류를 수행할 때 다른 학습 속도 간의 훈련 비교. 여기에서 우리는 우리와 동일한 W1A16 설정을 선택한다. 비트넷의 가중치 행렬은 원래의 LLaMA-7B 모델에서 직접 복사된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>