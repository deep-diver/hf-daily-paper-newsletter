<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OneBit: Towards Extremely Low-bit Large Language Models\n' +
      '\n' +
      'Yuzhuang Xu\\({}^{1}\\), Xu Han\\({}^{1}\\), Zonghan Yang\\({}^{1}\\), Shuo Wang\\({}^{1}\\)\n' +
      '\n' +
      '**Qingfu Zhu\\({}^{2}\\), Zhiyuan Liu\\({}^{1}\\), Weidong Liu\\({}^{1}\\), Wanxiang Che\\({}^{2,}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)Department of Computer Science & Technology, Tsinghua University, Beijing, China\n' +
      '\n' +
      '\\({}^{2}\\)Research Center for Social Computing and Information Retrieval,\n' +
      '\n' +
      'Harbin Institute of Technology, Harbin, China\n' +
      '\n' +
      'xyz21thu@gmail.com, car@ir.hit.edu.cn\n' +
      '\n' +
      'Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non-quantized performance) with robust training processes when only using 1-bit weight matrices.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Transformer Vaswani et al. (2017) has emerged as the pivotal architecture in large language models (LLMs), fundamentally reshaping the approach to natural language processing in deep learning Bubeck et al. (2023); Touvron et al. (2023); Bisk et al. (2020). Despite their popularity, deploying transformer-based LLMs presents significant challenges due to their computational intensity and considerable memory requirements as the parameters of LLMs become more and more. For instance, even moderately-sized LLMs like LLaMA-13B Touvron et al. (2023) require around 26GB of memory to load its all parameters in FP16 format. Such overheads make deploying LLMs difficult beyond mid-to-high-end GPUs like the A100, let alone on mobile devices. The high demand for resources not only drives up usage costs, but also restricts their wider application.\n' +
      '\n' +
      'Numerous efforts Dettmers et al. (2022); Frantar et al. (2022); Frantar and Alistarh (2023) have been devoted to reducing the computational and memory overheads of LLMs, while still preserving most of their original model capabilities. Among these efforts, quantization has gained widespread attention, particularly Post-Training Quantization (PTQ), benefitted from its lower transferring costs. Seminal studies such as GPTQ Frantar et al. (2022), SpQR Dettmers et al. (2023), and AWQ Lin et al. (2023) successfully compress the weight matrices of LLMs to 4-bit values while maintaining the main abilities of LLMs. Efficient quantization represents significant advances in LLM optimization, by achieving a balance between time and space efficiency as well as model performance.\n' +
      '\n' +
      'Unfortunately, the efficacy of PTQ rapidly diminishes when the quantization bit-width is extremely low, as shown in Figure 1. Existing PTQ methods managed to compress weight matrices down\n' +
      '\n' +
      'Figure 1: The perplexity (lower scores mean better performance) of existing widely-used low-bit quantization methods on LLaMA-7B, reported on Wikitext2 Merity et al. (2016). All the examined previous approaches suffer from significant performance degradation when quantizing models to 2-bit values. Our 1-bit quantization method can outperform these 2-bit baselines.\n' +
      '\n' +
      'to at least 3-bit Dettmers and Zettlemoyer (2023). Recent researches hope to leverage Quantization-Aware Training (QAT) to overcome the bottlenecks faced by PTQ. LLM-QAT Liu et al. (2023) introduces a few learnable parameters into the quantization process, achieving notable results. OmniQuant Shao et al. (2023), integrating learnable equivalent transformation, presents promising results in 2-bit quantization. However, existing methods decline when compressing model weights to 1 bit, struggling to maintain effectiveness. This mainly stems from the drastic precision loss at extremely low bit-width in weight matrix \\(\\mathbf{W}\\), significantly increasing loss in linear projection \\(\\mathbf{W}\\mathbf{X}\\), which is the core operator within LLMs.\n' +
      '\n' +
      'In this paper, we propose a novel Linear layer and Sign-Value-Independent Decomposition (SVID) for weight matrices to represent LLMs using approximately 1-bit values. In SVID, each original high-bit weight matrix is decomposed into one sign matrix (\\(\\pm 1\\)) and two value vectors. The value vectors provide necessary floating-point precision in linear projection at little cost and help the model to be trained easily. The sign matrix maintains the high rank of the original weight matrix with a small space cost, thereby preserving high information capacity. SVID offers a better parameter initialization for 1-bit models and we employ quantization-aware knowledge distillation to transfer the capabilities of the original model to the proposed 1-bit counterpart. Experiments demonstrate that our method performs well at the W1A16 (1-bit weight and 16-bit activation) quantization level. Furthermore, our 1-bit model is more amenable to training and knowledge transfer than previous works. In summary, the contributions of this work are 3-fold:\n' +
      '\n' +
      '* We propose a novel and efficient 1-bit model architecture for LLMs, which can improve both the time and space efficiency during model inference. Moreover, our architecture is more stable during quantizing LLMs.\n' +
      '* We propose SVID to decompose high-bit matrices into low-bit ones, which is essential for the initialization of our 1-bit architecture. Experiments demonstrate that the SVID-based initialization can improve the model performance and convergence speed.\n' +
      '* Extensive experiments demonstrate that our method works well in model sizes from 1.3B to 13B in OPT, LLaMA, and LLaMA2, showcasing its generalizability.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Large Language Model Compression\n' +
      '\n' +
      'Quantization, pruning, and knowledge distillation (KD) are the mainstream methods for model compression. Quantization compresses model weights into low-bit values Frantar et al. (2022); Lin et al. (2023); Dettmers et al. (2023). For data type alignment in computation and reducing memory, it also involves quantizing activation Dettmers et al. (2022); Xiao et al. (2023) and key-value cache Shao et al. (2023). Pruning simplifies model complexity by removing unimportant weights or modules, thereby sparsifying the original larger models Frantar and Alistarh (2023); Sun et al. (2023); Ma et al. (2023). KD trains a smaller student model under the guidance of a larger teacher model Hsieh et al. (2023); Agarwal et al. (2023); Hsieh et al. (2023), achieving the purpose of compressing the larger one. Beyond these methods, low-rank factorization approximates the original weight matrix \\(\\mathbf{W}\\) with the product of two lower-rank matrices Xu et al. (2023) and also achieves promising results. Our work belongs to quantization, using KD for knowledge transfer from the original LLM and uniquely focusing on extremely low bit-width quantization. More details about model compression can refer to existing survies Wan et al. (2023); Zhu et al. (2023).\n' +
      '\n' +
      '### Large Language Model Quantization\n' +
      '\n' +
      'Since this paper aims to obtain extremely low-bit LLMs, here we thus introduce more details about LLM quantization. Quantization stands as a popular and crucial method for model compression, capable of achieving a significant compression ratio with a relatively small loss. It can be classified into Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) according to when quantization is applied.\n' +
      '\n' +
      'PTQ directly converts trained models into lower-bit counterparts using accurate solvers and limited calibration data without additional training. Typically, GPTQ Frantar et al. (2022) row-wisely quantizes weight matrices and adjusts remaining weights to compensate for the precision loss caused by quantization, achieving nearly lossless 4-bit weight quantization. Moreover, numerous studies observed the effect of "outliers" in quantization Dettmers et al. (2022); Kim et al. (2023);Lin et al., 2023). LLM.int8() Dettmers et al. (2022) suggests mixed-precision decomposition to ensure the accuracy of a few outliers in activations. SmoothQuant Xiao et al. (2023) reduces the difficulty of quantization by smoothing the outliers of activation. SpQR Dettmers et al. (2023) identifies sensitive weights to ensure their precision, while quantizing other weights to lower bit-width.\n' +
      '\n' +
      'QAT integrates quantization steps within the model, applying them during training or fine-tuning. It allows the model to better adapt to the reduced precision induced by quantization, leading to improved performance compared to PTQ. LLM-QAT Liu et al. (2023) introduces a small number of learnable parameters into quantization and employs KD using data generated by the original model itself. OmniQuant Shao et al. (2023); we classify it as QAT) further introduces learnable equivalent transformation, achieving acceptable results in 2-bit weight quantization. PEQA Kim et al. (2023) and QLoRA Dettmers et al. (2023) focus on fine-tuning a limited number of extra parameters to mitigate the precision loss caused by sub-4bit weight quantization. Our work is closely related to QAT, but due to the unique challenges posed by 1-bit quantization, our representation and initialization methods of quantized weights are distinct from any existing work.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      'This section demonstrates our 1-bit architecture of the Linear layer to be quantized and discuss how to initialize the quantized model to achieve better performance in knowledge distillation. We start with a short review of classical weight quantization methods in Section 3.1 and then formulate our OneBit from Section 3.2 to Section 3.4 in detail.\n' +
      '\n' +
      '### Background\n' +
      '\n' +
      'The main idea of model quantization is to compress each weight matrix \\(\\mathbf{W}\\) within models in FP32 or FP16 format to a low-bit counterpart. Specifically, we often quantize the weight matrices of Linear layers in transformer to 8, 4, and even 2 bits.\n' +
      '\n' +
      'The majority of quantization studies primarily employ the round-to-nearest (RTN) method, by which the weight \\(w\\) is rounded to the nearest value in the quantization grid. It can be formulated as\n' +
      '\n' +
      '\\[\\hat{w}=\\mathrm{Clip}\\left(\\left\\lfloor\\frac{w}{s}\\right\\rceil+z,0,2^{N}-1 \\right), \\tag{1}\\]\n' +
      '\n' +
      'where \\(s\\) denotes the quantization scale parameter, \\(z\\) denotes the zero point parameter, and \\(N\\) is the quantization bit-width. \\(\\mathrm{Clip}(\\cdot)\\) truncates the result in the range of \\(0\\) to \\(2^{N}-1\\). With the bit-width being lower and lower, the quantization grid also becomes sparser. When we quantize a LLM to 1-bit values, there are only 2 available numbers to be chosen in the quantized model. Existing study Dettmers and Zettlemoyer (2023) points out that quantization based on the RTN method may get their best performance at the 4-bit level. Further compressing like quantizing models to 2-bit values would result in a substantial degradation Shao et al. (2023) as shown in Figure 1.\n' +
      '\n' +
      'Furthermore, when \\(N\\) equals \\(1\\), quantization based on RTN method is essentially equivalent to setting a threshold, with weight \\(w\\) on either side of it being converted to corresponding integer value \\(\\hat{w}\\). In such a scenario, the parameters \\(s\\) and \\(z\\) in Eq. (1) effectively lose their practical significance. Consequently, when quantizing weights to 1 bit, the element-wise RTN operation drastically undermines the precision of the weight matrix \\(\\mathbf{W}\\), leading to poor performance of the quantized model.\n' +
      '\n' +
      '### 1-bit Linear Layer Architecture\n' +
      '\n' +
      'Due to the severe precision loss of 1-bit weight quantization, converting weight matrices in Linear layers directly from FP32/16 to 1-bit format based on RTN is challenging. Wang et al. (2023) explore this possibility by studying the capabilities of purely 1-bit weight matrices, training the 1-bit model from scratch. In the W1A16 setting, their Linear layers are designed as\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{W}_{\\pm 1}=\\mathrm{Sign}\\Big{[}\\mathbf{W}- \\mathrm{Mean}\\big{(}\\mathbf{W}\\big{)}\\Big{]},\\\\ \\eta=\\mathrm{Mean}\\Big{[}\\mathrm{Abs}\\Big{(}\\mathbf{W}- \\mathrm{Mean}\\big{(}\\mathbf{W}\\big{)}\\Big{)}\\Big{]},\\\\ \\mathbf{Y}=\\eta\\cdot\\mathrm{LayerNorm}\\big{(}\\mathbf{X}\\big{)} \\mathbf{W}_{\\pm 1}^{\\mathrm{T}},\\end{split} \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbf{W}\\) denotes the quantized weight matrix with the shape \\(m\\times n\\) and \\(\\mathbf{W}_{\\pm 1}\\) denotes the 1-bit quantized matrix. \\(\\mathbf{X}\\) is the input of Linear layer and \\(\\mathbf{Y}\\) is the output. \\(\\mathrm{Sign}(\\cdot)\\), \\(\\mathrm{Mean}(\\cdot)\\) and \\(\\mathrm{Abs}(\\cdot)\\) functions return the sign matrix, average and absolute value matrix. Unfortunately, this approach reduces computational demands but also leads to a marked decrease in performance Wang et al. (2023).\n' +
      '\n' +
      'Inspired by Wang et al. (2023), we also quantize the weight matrix using the function \\(\\mathrm{Sign}(\\cdot)\\), and the element of the quantized matrix is set to +1 or -1. Moreover, we also notice that although \\(\\mathbf{W}_{\\pm 1}\\) maintains a high rank of \\(\\mathbf{W}\\), the missedfloating-point precision still destroys the model performance. Therefore, different from previous work, we introduce two value vectors with an FP16 format to compromise the precision loss in the quantization process. Our proposed Linear layers are designed as\n' +
      '\n' +
      '\\[\\begin{split}\\mathbf{W}_{\\pm 1}=\\mathrm{Sign}\\big{(}\\mathbf{W} \\big{)},\\\\ \\mathbf{Y}=\\big{[}\\big{(}\\mathbf{X}\\odot\\mathbf{g}\\big{)}\\mathbf{W }_{\\pm 1}^{\\mathrm{T}}\\big{]}\\odot\\mathbf{h},\\\\ \\mathbf{Z}=\\mathrm{LayerNorm}\\big{(}\\mathbf{Y}\\big{)},\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathbf{g}\\) and \\(\\mathbf{h}\\) are the two FP16 value vectors. Note that we specify the calculation order using brackets in Eq. (3) for minimizing the time and space cost. The main difference between Wang et al. (2023) and OneBit is the extra parameter \\(\\mathbf{g}\\) and \\(\\mathbf{h}\\). Even if additional parameters are brought in, the benefits far outweigh its small cost. For instance, when we quantize one weight matrix with the shape \\(4096\\times 4096\\), the average bit-width of the quantized result is _1.0073_. See A.6 for the details.\n' +
      '\n' +
      '### Sign-Value-Independent Decomposition\n' +
      '\n' +
      'In our proposed 1-bit architecture, the weight matrix \\(\\mathbf{W}\\) is mathematically divided into two components: one sign matrix \\(\\mathbf{W}_{\\pm 1}\\) in INT1 format and two value vector \\(\\mathbf{g}\\)/\\(\\mathbf{h}\\) in FP16 format. To initialize the 1-bit model with the help of the fully trained weight, we introduce the Sign-Value-Independent Decomposition (SVID) of the weight matrix \\(\\mathbf{W}\\), which can be formulated as \\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}}\\odot\\mathbf{W}_{\\mathrm{value}}\\). Here we have \\(\\mathbf{W}_{\\mathrm{value}}=|\\mathbf{W}|\\) and \\(\\mathbf{W}_{\\mathrm{sign}}=\\mathrm{Sign}(\\mathbf{W})\\). For \\(\\mathbf{W}_{\\mathrm{value}}\\), we further approximately decompose it into the outer product of two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), which is also known as _rank-1 approximation_. Hence, our proposed matrix decomposition method can be represented as\n' +
      '\n' +
      '\\[\\mathbf{W}\\approx\\mathbf{W}_{\\mathrm{sign}}\\odot\\left(\\mathbf{a}\\mathbf{b}^{ \\mathrm{T}}\\right). \\tag{4}\\]\n' +
      '\n' +
      'We can employ some widely used matrix decomposition methods to perform the rank-1 approximation, such as SVD (Beltrami, 1990) and NMF (Paatero and Tapper, 1994).\n' +
      '\n' +
      '**Proposition 1** Given the weight matrix \\(\\mathbf{W}\\) and input \\(\\mathbf{X}\\), the Linear layer can be reformulated as the following according to SVD:\n' +
      '\n' +
      '\\[\\mathbf{X}\\mathbf{W}^{\\mathrm{T}}\\approx\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{ b}^{\\mathrm{T}}\\right)\\mathbf{W}_{\\mathrm{sign}}^{\\mathrm{T}}\\big{]}\\odot \\mathbf{a}^{\\mathrm{T}}. \\tag{5}\\]\n' +
      '\n' +
      'We prove this approximation in Appendix A.1. This bridges the gap between the architecture of the quantized model and its original weights. It indicates that if we assign \\(\\mathbf{W}_{\\mathrm{sign}}\\) to \\(\\mathbf{W}_{\\pm 1}\\), \\(\\mathbf{a}^{\\mathrm{T}}\\) to \\(\\mathbf{h}\\) and \\(\\mathbf{b}^{\\mathrm{T}}\\) to \\(\\mathbf{g}\\), the quantized model is an approximate initialization of the original model. Moreover, compared to restoring the original matrix \\(\\mathbf{W}\\) first (such as in Eq. (4)), the computational order in Eq. (5) saves approximately one matrix \\(\\mathbf{W}\\) in FP16 format in memory as there is no need to restore FP16 \\(\\mathbf{W}\\).\n' +
      '\n' +
      'The main objective of SVD is to involve the sign matrix \\(\\mathbf{W}_{\\mathrm{sign}}\\) in approximating matrix \\(\\mathbf{W}\\), rather than solely relying on value vectors in FP16 format. To substantiate the role of the sign matrix \\(\\mathbf{W}_{\\mathrm{sign}}\\) in matrix approximation, we present the following proposition.\n' +
      '\n' +
      '**Proposition 2** Given matrices \\(\\mathbf{W}\\) and \\(|\\mathbf{W}|\\), \\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}}\\odot|\\mathbf{W}|\\). We decompose these matrices in the way \\(\\mathbf{W}=\\mathbf{a}\\mathbf{b}^{\\mathrm{T}}+\\mathbf{E}_{1}\\) and \\(|\\mathbf{W}|=\\tilde{\\mathbf{a}}\\tilde{\\mathbf{b}}^{\\mathrm{T}}+\\mathbf{E}_{2}\\), where \\(\\mathbf{E}_{i}\\) denotes the error matrices. In terms of the Frobenius-norm, the SVD is closer to the original matrix \\(\\mathbf{W}\\):\n' +
      '\n' +
      '\\[\\left\\|\\mathbf{W}-\\mathbf{W}_{\\mathrm{sign}}\\odot\\tilde{\\mathbf{a}}\\tilde{ \\mathbf{b}}^{\\mathrm{T}}\\right\\|_{\\mathrm{F}}^{2}\\leq\\left\\|\\mathbf{W}-\\mathbf{ a}\\mathbf{b}^{\\mathrm{T}}\\right\\|_{\\mathrm{F}}^{2}. \\tag{6}\\]\n' +
      '\n' +
      'We also prove this proposition in Appendix A.1. It clearly demonstrates the practical role of the sign matrix \\(\\mathbf{W}_{\\mathrm{sign}}\\) in matrix approximation.\n' +
      '\n' +
      'Note that, given the predominantly low precision of most parameters, it is quite challenging\n' +
      '\n' +
      'Figure 2: The main idea of our method OneBit. The left is the original FP16 Linear Layer, in which both the activation \\(\\mathbf{X}\\) and the weight matrix \\(\\mathbf{W}\\) are in FP16 format. The right is our proposed architecture. Only value vectors \\(\\mathbf{g}\\) and \\(\\mathbf{h}\\) are in FP16 format and the weight matrix consists of \\(\\pm 1\\) instead.\n' +
      '\n' +
      'to approximate the weight matrix \\(\\mathbf{W}\\) accurately. SVID is not aimed to precisely replicate the original model\'s parameters, but to provide an effective starting point for further training, leveraging the extensive training of the original model. Details on transferring knowledge from the original model to the quantized counterpart are in Section 3.4.\n' +
      '\n' +
      '### Knowledge Transfer\n' +
      '\n' +
      'We employ quantization-aware knowledge distillation to transfer knowledge from the original model (i.e. teacher model) to the quantized one (i.e. student model). In the student model, the element in matrix \\(\\mathbf{W}\\) and vectors \\(\\mathbf{g/h}\\) in Eq. (3) will be trained. We use cross-entropy based logits and mean-square-error based hidden state of the full-precision teacher model to direct the quantized student model (Sun et al., 2019). Language modeling loss is not used. The cross-entropy is defined as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{CE}}=-\\frac{1}{n_{s}}\\sum_{i=1}^{n_{s}}\\sum_{c}P_{c}^{ \\mathcal{T}}\\left(\\mathbf{o}_{i}\\right)\\log P_{c}^{\\mathcal{S}}\\left(\\mathbf{ o}_{i}\\right), \\tag{7}\\]\n' +
      '\n' +
      'where \\(c\\) denotes the number of classes and \\(n_{s}\\) denotes the number of training samples in the current batch. \\(\\mathcal{T}\\) and \\(\\mathcal{S}\\) are the teacher model and student model, respectively. The error of hidden states is defined as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{MSE}}=\\sum_{i=1}^{n_{s}}\\sum_{j=1}^{n_{l}}\\left\\|\\frac{ \\mathbf{q}_{i,j}^{\\mathcal{T}}}{\\left\\|\\mathbf{q}_{i,j}^{\\mathcal{T}}\\right\\| _{2}}-\\frac{\\mathbf{q}_{i,j}^{\\mathcal{S}}}{\\left\\|\\mathbf{q}_{i,j}^{\\mathcal{ S}}\\right\\|_{2}}\\right\\|_{2}^{2}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(n_{l}\\) denotes the number of layers and \\(\\mathbf{q}\\) denotes the hidden state. Hence the final objective function can be formulated as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathrm{KD}}=\\mathcal{L}_{\\mathrm{CE}}+\\alpha\\mathcal{L}_{ \\mathrm{MSE}}, \\tag{9}\\]\n' +
      '\n' +
      'where \\(\\alpha\\) is the hyper-parameter that balances the importance of the cross-entropy loss and the features in the intermediate layers.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We experiment with 1-bit weight-only quantizaton and maintain 16-bit activation (W1A16) in this work. We evaluate our approach by performing experiments on OPT-1.3B/2.7B models, LLaMA-7B/13B models and LLaMA2-7B/13B models, and present results on various tasks.\n' +
      '\n' +
      '### Settings\n' +
      '\n' +
      'DataFor the training data of our quantization-aware knowledge distillation, we follow Liu et al. (2023) to synthesize corpus using next token generation from the original teacher model. It randomizes the first token from vocabulary and generates the next token iteratively until reaching either the <_EOS_> token or the maximum length. Specially, the top-1 predictions are selected deterministically for the first 3 to 5 tokens, followed by stochastic sampling for the remaining tokens. We utilized LLaMA-7B to generate a total of 132k data entries, each with a maximum length of 2,048.\n' +
      '\n' +
      'Training DetailsEvery KD experiment learns the training data over 50 epochs, from which 2048-token segments are selected. We employ NMF in scikit-learn 1 to decompose the weight matrices in SVID. The quantized student models are optimized by Adam (Kingma and Ba, 2014) with \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.98\\). The learning rate for all experiments is scheduled by _cosine_ strategy. We use NVIDIA A100 GPUs and maintain FP16 precision while training quantized models. For additional details such as learning rate, please refer to Table 1.\n' +
      '\n' +
      'Footnote 1: [https://scikit-learn.org/](https://scikit-learn.org/)\n' +
      '\n' +
      'BaselinesTo our knowledge, there is no previous work exploring the 1-bit quantization of LLMs from a knowledge transfer perspective. To this end, we relax the quantization bit-width of baselines to 2 bits (W2A16) while maintaining the W1A16 setting in our method. We compare our method with GPTQ (Frantar et al., 2022), LLM-QAT (Liu et al., 2023) and OmniQuant (Shao et al., 2023). To ensure a fair comparison in terms of space usage, baselines do not employ grouped quantization. Additionally, we included the results of vanilla transformers with FP16 precision as a reference. While the recent work BitNet (Wang et al., 2023) also introduced one 1-bit model architecture, it only focused on training models from scratch. We also\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Models** & **learning rate** & \\(\\alpha\\) & **\\# GPUs** \\\\ \\hline OPT-1.3B & 4e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ OPT-2.7B & 2e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ LLaMA-7B & 4e-4 & 10.0 & 1 \\(\\times\\) 8 \\\\ LLaMA-13B & 2e-4 & 1.0 & 2 \\(\\times\\) 8 \\\\ LLaMA2-7B & 1e-4 & 1.0 & 1 \\(\\times\\) 8 \\\\ LLaMA2-13B & 2e-4 & 1.0 & 2 \\(\\times\\) 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Training details of knowledge distillation.\n' +
      '\n' +
      'analyze its capability to transfer knowledge from the original models in Appendix A.5.\n' +
      '\n' +
      'Evaluation MetricsBasically, we evaluate quantized models by testing the perplexity on the validation set, specifically on WikiText2 Merity et al. (2016) and C4 Raffel et al. (2020). Lower perplexity indicates that the compressed model is better at preserving the output distribution of the original model. Furthermore, accuracies of zero-shot tasks including Winograde Sakaguchi et al. (2021), HellaSwag Zellers et al. (2019), PIQA Bisk et al. (2020), BoolQ Clark et al. (2019), and ARC Clark et al. (2018) are also reported. They evaluate if the capabilities of the original model on downstream tasks are retained. We utilize the open-sourced toolkit "LM-Evaluation-Harness"2 to perform the perplexity test and all zero-shot tasks.\n' +
      '\n' +
      'Footnote 2: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'Table 2 compares our method with other typical strong baselines on different models. Due to space limitations, results of LLaMA2-7B/13B are listed in Appendix A.3. In various model sizes, our 1-bit weight quantization method obviously outperforms others under the W2A16 setting. Moreover, the effectiveness of QAT based methods consistently improves as the model size increases, whereas the result of the PTQ method, GPTQ, may degrade when model size increases (e.g., from 7B to 13B on LLaMA). This demonstrates that QAT-based method can achieve stable results in extremely low-bit quantization. Specifically, our method approaches the performance of FP16 more closely as the model size increases. For instance, when scaling from LLaMA-7B to LLaMA-13B, the perplexity of the FP16 model decreases by only 0.59, whereas our method sees a reduction of 1.20.\n' +
      '\n' +
      'For perplexity, only our method achieves comparable results to the strongest FP16 baseline. For instance, our method achieves 9.18 in the Wiki2 dataset on LLaMA-13B model and the FP16 baseline is 5.09. The performance loss of other methods is significant, even though they use 2-bit quantization, which is more than our 1 bit. For GPTQ and LLM-QAT, the performance degradation after quantization is pretty severe. As for OmniQuant, even though it is the strongest baseline under the W2A16 setting, it still suffers greater performance loss compared to our W1A16 setting.\n' +
      '\n' +
      'For zero-shot accuracy, although all methods inevitably have some degradation, our method achieves the closest performance to the FP16 baseline among most models. On the OPT-1.3B/2.7B\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c|}{**Perplexity(\\(\\downarrow\\))**} & \\multicolumn{6}{c}{**Zero-shot Accuracy(\\(\\uparrow\\))**} \\\\  & & **Wiki2** & **C4** & **Winogrande** & **Hellaswag** & **PIQA** & **BoolQ** & **ARC-e** & **ARC-c** & **Avg.** \\\\ \\hline \\multirow{6}{*}{OPT-1.3B} & FP16 & 14.63 & 14.72 & 59.67 & 53.73 & 72.42 & 57.68 & 50.80 & 29.69 & 54.00 \\\\  & GPTQ & 9.5e3 & 3.8e3 & 49.33 & 25.57 & 52.07 & 39.60 & 26.68 & 23.63 & 36.15 \\\\  & LLM-QAT & 4.9e3 & 2.1e3 & 49.72 & 25.72 & 50.05 & 37.83 & 25.76 & **25.09** & 35.70 \\\\  & OmniQuant & 42.43 & 55.64 & **51.85** & 33.39 & 60.94 & 56.45 & 38.76 & 23.38 & 44.13 \\\\  & OneBit & **25.42** & **22.95** & 51.14 & **34.26** & **62.57** & **59.45** & **41.25** & 24.06 & **45.46** \\\\ \\hline \\multirow{6}{*}{OPT-2.7B} & FP16 & 12.47 & 13.17 & 60.93 & 60.59 & 74.81 & 60.28 & 54.34 & 31.31 & 57.04 \\\\  & GPTQ & 8.7e3 & 3.9e3 & 49.88 & 26.47 & 49.84 & 39.88 & 25.76 & **26.02** & 36.31 \\\\  & LLM-QAT & 3.7e3 & 1.4e3 & 52.09 & 25.47 & 49.29 & 37.83 & 24.92 & 25.60 & 35.87 \\\\  & OmniQuant & 30.25 & 41.31 & 51.62 & **38.21** & 62.19 & 54.25 & 40.82 & 24.74 & 45.31 \\\\  & OneBit & **21.86** & **20.76** & **51.67** & 38.18 & **63.87** & **54.28** & **43.39** & 24.40 & **45.97** \\\\ \\hline \\multirow{6}{*}{LLaMA-7B} & FP16 & 5.68 & 7.08 & 66.85 & 72.99 & 77.37 & 73.21 & 52.53 & 41.38 & 64.06 \\\\  & GPTQ & 1.9e3 & 7.8e2 & 49.41 & 25.63 & 49.95 & 43.79 & 25.84 & 27.47 & 37.02 \\\\ \\cline{1-1}  & LLM-QAT & 7.1e2 & 3.0e2 & 51.78 & 24.76 & 50.87 & 37.83 & 26.26 & 25.51 & 36.17 \\\\ \\cline{1-1}  & OmniQuant & 15.34 & 26.21 & 52.96 & 43.68 & 62.79 & 58.69 & 41.54 & 29.35 & 48.17 \\\\ \\cline{1-1}  & OneBit & **10.38** & **11.56** & **60.30** & **50.73** & **67.46** & **62.51** & **41.71** & **29.61** & **52.05** \\\\ \\hline \\multirow{6}{*}{LLaMA-13B} & FP16 & 5.09 & 6.61 & 70.17 & 76.24 & 79.05 & 68.47 & 59.85 & 44.54 & 66.39 \\\\ \\cline{1-1}  & GPTQ & 3.2e3 & 9.9e2 & 50.67 & 25.27 & 50.00 & 42.39 & 26.14 & 27.39 & 36.98 \\\\ \\cline{1-1}  & LLM-QAT & 1.8e3 & 1.2e3 & 51.62 & 25.40 & 50.33 & 37.83 & 27.02 & 26.87 & 36.51 \\\\ \\cline{1-1}  & OmniQuant & 13.43 & 19.33 & 53.83 & 54.16 & 68.99 & 62.20 & **45.50** & 30.38 & 52.51 \\\\ \\cline{1-1}  & OneBit & **9.18** & **10.25** & **62.90** & **56.78** & **70.67** & **64.16** & 44.53 & **32.00** & **55.17** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Main results of evaluation experiment. We report the perplexity and zero-shot accuracy. “FP16” is the transformer with FP16 parameters and we refer to it as the upper-bound of all the methods. The **best** score is bolded.\n' +
      '\n' +
      'model, our method shows smaller performance loss on most tasks such as PIQA and ARC-e. Additionally, the loss of other tasks is negligible compared with the second-best baseline, OmniQuant. On the LLAMA-7B model, our method notably outperforms OmniQuant in all tasks except ARC-e/ARC-c, averaging about a 4% improvement overall.\n' +
      '\n' +
      '### Problem Solving Ability\n' +
      '\n' +
      'We have demonstrated the superior performance of our method under the W1A16 setting, compared to other representative baselines. Although all methods inevitably face performance degradation in 1-bit weight quantization, it remains of interest how our method fares in solving practical problems among the various approaches to reducing model size. For instance, directly training smaller models (Zhang et al., 2024) or employing low-rank decomposition to reduce the number of parameters.\n' +
      '\n' +
      'To this end, we consider two crucial abilities of LLMs: commonsense reasoning and world knowledge. For commonsense reasoning, we use the 6 tasks (Hellaswag, etc.) and settings described in Section 4.2. For world knowledge, we examine it using the Massive Multi-task Language Understanding (MMLU; Hendrycks et al., 2021), a benchmark that covers a wide range of domains and knowledge. We compare the following 4 models:\n' +
      '\n' +
      '**Pythia-1.0B**(Biderman et al., 2023). A well-trained model released by EleutherAI whose memory footprint is 1.54x that of our 7B model.\n' +
      '\n' +
      '**TinyLLAMA-1.1B**(Zhang et al., 2024). A model with the same structure as the LLaMA models, which undergoes continued training. To compare fairly, we use the checkpoint at 10k training steps, which is 2x that of our model.\n' +
      '\n' +
      '**LowRank LLaMA**(Noach and Goldberg, 2020). Decompose every weight matrix in Linear layers to two low-rank matrices and learn from the original LLaMA-7B model by KD in the same setting of OneBit-7B.\n' +
      '\n' +
      '**OneBit-7B** The model that we use in Section 4.2, which is built with OneBit.\n' +
      '\n' +
      'Figure 2(a) and 2(b) demonstrate common sense reasoning ability and general world knowledge of different models. We can observe that, although other models have more parameters and are more thoroughly trained than ours, our model still has advantages in common sense reasoning. This reflects the benefits inherited from the larger 7B model. In terms of world knowledge, despite a significant loss in social sciences, our model outperforms the fully trained Pythia-1B in other domains. These results demonstrate the practical usability of OneBit.\n' +
      '\n' +
      '## 5 Analysis and Discussion\n' +
      '\n' +
      '### Efficiency\n' +
      '\n' +
      'It is evident that extremely low-bit quantization of weights can significantly reduce the memory foot\n' +
      '\n' +
      'Figure 3: Comparison of model capabilities and compressive degree.\n' +
      '\n' +
      'print of models. As shown in Table 3, the actual compression ratio increases as the model size increases. This is particularly meaningful for larger models, making it possible to fit the model into one GPU. While there is a performance loss, Figure 4 illustrates that our method achieves a good trade-off between space occupancy and model performance. For example, we can achieve comparable performance to FP16 with only 0.2x the model space. Furthermore, quantizing to \\(\\pm 1\\) also aids in accelerating matrix multiplication on CPUs. It is because the floating-point multiplication of elements in two matrices can be converted into much faster bit operations on these chips. Thus the substantial reduction in memory overhead makes these low-bit LLMs meet the requirements for deployment on PCs and smartphones.\n' +
      '\n' +
      '### Robustness\n' +
      '\n' +
      'Existing work Wang et al. (2023) has already noted the instability within QAT. Extremely low-bit quantization makes the training process highly sensitive to the learning rate, making it difficult for the model to converge when the rate is too small or too large. This is primarily due to the large magnitude of gradients generated as the weight elements fluctuate between +1 and -1, leading to substantial fluctuations in the output of Linear layers. Experiments demonstrate that OneBit shows more stable training process and is not sensitive to learning rates. Please refer to Appendix A.5 for more details.\n' +
      '\n' +
      '### Effect of Different Components\n' +
      '\n' +
      'The variable components in our method primarily include Post-LayerNorm, value vectors, and parameter initialization.\n' +
      '\n' +
      'Post-LayerNormWe discover that models might experience floating-point overflow during the QAT process. As depth increases, the activation can become progressively larger. We tackle it using Post-LayerNorm instead of Pre-LayerNorm. In contrast, Pre-LayerNorm may occasionally be ineffective.\n' +
      '\n' +
      'Value VectorsThe main structural difference between OneBit and BitNet Wang et al. (2023) is the two value vectors, which are demonstrated to be effective in Section 4.2. Please refer to Appendix A.5 for more details of comparison.\n' +
      '\n' +
      'Parameter InitializationIn our proposed SVID, both NMF and SVD can be used to decompose \\(|\\mathbf{W}|\\) and we recommend using the former. This is because we find that NMF may make the training more faster to converge. Figure 5 shows that initializing by NMF facilitates better performance.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'We propose a model structure for 1-bit weight quantization and a corresponding parameter initialization method. Extensive experiments on models of various sizes and series demonstrate that OneBit has clear advantages over representative strong baselines and achieves a good tradeoff between model size and performance. We further analyze the capabilities of such extremely low-bit quantized models and provide guidance for future research.\n' +
      '\n' +
      'Figure 4: Tradeoff between model size and perplexity.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Models** & **FP16 (GB)** & **OneBit (GB)** & **Ratio (\\%)** \\\\ \\hline LLaMA-7B & 13.5 & 1.3 & 90.4 \\\\ LLaMA-13B & 26.0 & 2.2 & 91.5 \\\\ LLaMA-30B & 65.1 & 4.9 & 92.5 \\\\ LLaMA-65B & 130.6 & 9.2 & 93.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Compression ratio of LLaMA models.\n' +
      '\n' +
      'Figure 5: Training process of OneBit-7B.\n' +
      '\n' +
      '### Limitation\n' +
      '\n' +
      'Although our proposed method significantly reduces the memory footprint of LLMs, bringing hope for efficient deployment of them, there are still some limitations. Firstly, compared to the original model, our extremely low-bit quantization inevitably incurs a performance loss. Additionally, we are yet to understand the mathematical principles behind the optimal parameters of the 1-bit quantized model, thus capability transfer can only be achieved through the costly process of KD. Fortunately, this cost is a one-time expense. Moreover, due to the unique nature of 1-bit quantization, our method can not be naturally extended to higher bit-width. Lastly, we have not considered the activation quantization and leave it as future work.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'In this study, we employ models that are publicly available and open source. We affirm that the use of these models aligns with their original intended purposes. These models have been utilized strictly within the scope of academic and research-based activities, adhering to ethical guidelines and ensuring compliance with open-source licenses.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. 2023. GKD: Generalized knowledge distillation for auto-regressive sequence models. _arXiv preprint arXiv:2306.13649_.\n' +
      '* Beltrami (1990) E Beltrami. 1990. Sulle funzioni bilineari, giomale di mathematiche ad uso studenti delle uninerista. 11, 98-106.(an english translation by d boley is available as university of minnesota, department of computer science). Technical report, Technical Report 90-37.\n' +
      '* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbic Bradley, Kyle O\'Brien, Eric Hallahan, Mohammad Afhal Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In _ICML_, pages 2397-2430.\n' +
      '* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. PIQA: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI_, volume 34, pages 7432-7439.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in NeurIPS_, 33:1877-1901.\n' +
      '* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_.\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_.\n' +
      '* Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit matrix multiplication for transformers at scale. _arXiv preprint arXiv:2208.07339_.\n' +
      '* Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023a. QLoRA: Efficient finetuning of quantized LLMs. In _Advances in NeurIPS_.\n' +
      '* Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuzmedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023b. SpQR: A sparse-quantized representation for near-lossless llm weight compression. _arXiv preprint arXiv:2306.03078_.\n' +
      '* Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The case for 4-bit precision: k-bit inference scaling laws. In _ICML_, pages 7750-7774.\n' +
      '* Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. SparseGPT: Massive language models can be accurately pruned in one-shot. In _ICML_, pages 10323-10337.\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: Accurate post-training quantization for generative pre-trained transformers. _arXiv preprint arXiv:2210.17323_.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In _ICLR_.\n' +
      '* Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In _Findings of the ACL_, pages 8003-8017.\n' +
      '* Kim et al. (2023a) Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2023a. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. _arXiv preprint arXiv:2305.14152_.\n' +
      '\n' +
      'Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023b. SqueezeLLM: Dense-and-sparse quantization. _arXiv preprint arXiv:2306.07629_.\n' +
      '* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.\n' +
      '* Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware weight quantization for llm compression and acceleration. _arXiv preprint arXiv:2306.00978_.\n' +
      '* Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. LLM-QAT: Data-free quantization aware training for large language models. _arXiv preprint arXiv:2305.17888_.\n' +
      '* Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. LLM-Pruner: On the structural pruning of large language models. In _Advances in NeurIPS_.\n' +
      '* Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_.\n' +
      '* Noach and Goldberg (2020) Matan Ben Noach and Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In _Proceedings of the AACL-IJCNLP_, pages 884-889.\n' +
      '* Paatero and Tapper (1994) Pentti Paatero and Unto Tapper. 1994. Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values. _Environmetrics_, 5(2):111-126.\n' +
      '* Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106.\n' +
      '* Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. OmniQuant: Omnidirectionally calibrated quantization for large language models. _arXiv preprint arXiv:2308.13137_.\n' +
      '* Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A simple and effective pruning approach for large language models. _arXiv preprint arXiv:2306.11695_.\n' +
      '* Sun et al. (2019) Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. In _Proceedings of the EMNLP-IJCNLP_, pages 4323-4332.\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in NeurIPS_, 30.\n' +
      '* Wan et al. (2023) Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. 2023. Efficient large language models: A survey. _arXiv preprint arXiv:2312.03863_.\n' +
      '* Wang et al. (2023) Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. BitNet: Scaling 1-bit transformers for large language models. _arXiv preprint arXiv:2310.11453_.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. SmoothQuant: Accurate and efficient post-training quantization for large language models. In _ICML_, pages 38087-38099.\n' +
      '* Xu et al. (2023) Mingxue Xu, Yao Lei Xu, and Danilo P Mandic. 2023. TensorFlowGPT: Efficient compression of the embedding layer in llms based on the tensor-train decomposition. _arXiv preprint arXiv:2307.00526_.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_.\n' +
      '* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An open-source small language model. _arXiv preprint arXiv:2401.02385_.\n' +
      '* Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language models. _arXiv preprint arXiv:2308.07633_.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '### Proofs of Propositions\n' +
      '\n' +
      'In this section, we provide the necessary and detailed proofs for the propositions presented in this paper. All symbols have the same definition as in the main text.\n' +
      '\n' +
      '**Proposition 1** Given the weight matrix \\(\\mathbf{W}\\) and input \\(\\mathbf{X}\\), the Linear layer can be reformulated as the following according to SVID:\n' +
      '\n' +
      '\\[\\mathbf{X}\\mathbf{W}^{\\mathrm{T}}\\approx\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{b} ^{\\mathrm{T}}\\right)\\mathbf{W}_{\\mathrm{sign}}^{\\mathrm{T}}\\big{]}\\odot \\mathbf{a}^{\\mathrm{T}}.\\]\n' +
      '\n' +
      'ProofFrom Eq. (4), we have \\(w_{ij}\\approx s_{ij}\\cdot a_{i}b_{j}\\), where \\(s_{ij}\\) is the element of \\(\\mathbf{W}_{\\mathrm{sign}}\\). Hence we have\n' +
      '\n' +
      '\\[\\left(\\mathbf{X}\\mathbf{W}^{\\mathrm{T}}\\right)_{ij} \\approx\\sum_{k}x_{ik}w_{kj}^{\\mathrm{T}}=\\sum_{k}x_{ik}w_{jk}\\] \\[=\\sum_{k}x_{ik}s_{jk}a_{j}b_{k}\\] \\[=\\sum_{k}x_{ik}b_{k}s_{jk}a_{j}\\] \\[=\\sum_{k}\\left(\\mathbf{X}\\odot\\mathbf{b}^{\\mathrm{T}}\\right)_{ik }s_{kj}^{\\mathrm{T}}a_{j}\\] \\[=\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{b}^{\\mathrm{T}}\\right) \\mathbf{W}_{\\mathrm{sign}}^{\\mathrm{T}}\\big{]}_{ij}a_{j}\\] \\[=\\Big{\\{}\\big{[}\\left(\\mathbf{X}\\odot\\mathbf{b}^{\\mathrm{T}} \\right)\\mathbf{W}_{\\mathrm{sign}}^{\\mathrm{T}}\\big{]}\\odot\\mathbf{a}^{\\mathrm{ T}}\\Big{\\}}_{ij}.\\]\n' +
      '\n' +
      'This proposition is proved.\n' +
      '\n' +
      '**Lemma 1** Let \\(\\sigma_{i}\\left(\\mathbf{W}\\right)\\) denote the _i_-th biggest singular value of matrix \\(\\mathbf{W}\\). The following inequality holds:\n' +
      '\n' +
      '\\[\\sigma_{1}\\left(\\left|\\mathbf{W}\\right|\\right)\\geq\\sigma_{1}\\left(\\mathbf{W} \\right).\\]\n' +
      '\n' +
      'ProofAccording to the definition of induced norm, there are\n' +
      '\n' +
      '\\[\\sigma_{1}\\left(\\mathbf{W}\\right)=\\|\\mathbf{W}\\|_{2}=\\max_{ \\mathbf{x},\\|\\mathbf{x}\\|_{2}=1}\\|\\mathbf{W}\\mathbf{x}\\|_{2},\\] \\[\\sigma_{1}\\left(\\left|\\mathbf{W}\\right|\\right)=\\|\\mathbf{W}\\|_{2 }=\\max_{\\mathbf{y},\\|\\mathbf{y}\\|_{2}=1}\\|\\|\\mathbf{W}|\\mathbf{y}\\|_{2}.\\]\n' +
      '\n' +
      'Note that for \\(\\forall\\mathbf{x}\\), \\(\\|\\mathbf{x}\\|_{2}=1\\) and we have\n' +
      '\n' +
      '\\[\\|\\mathbf{W}\\|\\mathbf{x}\\|\\|_{2}^{2} =\\sum_{i}\\Bigg{(}\\sum_{j}|w_{ij}||x_{j}|\\Bigg{)}^{2}\\] \\[\\geq\\sum_{i}\\Bigg{(}|\\sum_{j}w_{ij}x_{j}|\\Bigg{)}^{2}\\] \\[=\\sum_{i}\\Bigg{(}\\sum_{j}w_{ij}x_{j}\\Bigg{)}^{2}=\\|\\mathbf{W} \\mathbf{x}\\|_{2}^{2}.\\]\n' +
      '\n' +
      'Therefore\n' +
      '\n' +
      '\\[\\max_{\\mathbf{y},\\|\\mathbf{y}\\|_{2}=1}\\|\\mathbf{W}|\\mathbf{y}\\|_{2}\\geq\\max_{ \\mathbf{x},\\|\\mathbf{x}\\|_{2}=1}\\|\\mathbf{W}\\mathbf{x}\\|_{2}.\\]\n' +
      '\n' +
      'This lemma is proved.\n' +
      '\n' +
      '**Proposition 2** Given matrices \\(\\mathbf{W}\\) and \\(|\\mathbf{W}|\\), \\(\\mathbf{W}=\\mathbf{W}_{\\mathrm{sign}}\\odot|\\mathbf{W}|\\). We decompose these matrices in the way \\(\\mathbf{W}=\\mathbf{a}\\mathbf{b}^{\\mathrm{T}}+\\mathbf{E}_{1}\\) and \\(|\\mathbf{W}|=\\tilde{\\mathbf{a}}\\tilde{\\mathbf{b}}^{\\mathrm{T}}+\\mathbf{E}_{2}\\), where \\(\\mathbf{E}_{i}\\) denotes the error matrices. In terms of the Frobenius-norm, the SVID is closer to the original matrix \\(\\mathbf{W}\\):\n' +
      '\n' +
      '\\[\\left\\|\\mathbf{W}-\\mathbf{W}_{\\mathrm{sign}}\\odot\\tilde{\\mathbf{a}}\\tilde{ \\mathbf{b}}^{\\mathrm{T}}\\right\\|_{\\mathrm{F}}^{2}\\leq\\left\\|\\mathbf{W}- \\mathbf{a}\\mathbf{b}^{\\mathrm{T}}\\right\\|_{\\mathrm{F}}^{2}.\\]\n' +
      '\n' +
      'ProofHere we consider SVD to prove it. For SVD, the norm of the error matrix \\(\\mathbf{E}\\) in the rank-1 approximation is the sum of the squares of all singular values except for the largest one. We have\n' +
      '\n' +
      '\\[\\|\\mathbf{E}_{1}\\|_{F}^{2} =\\sum_{i=2}^{n}\\sigma_{i}^{2}\\left(\\mathbf{W}\\right),\\] \\[\\|\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i=2}^{n}\\sigma_{i}^{2}\\left(\\left|\\mathbf{W}\\right|\\right).\\]\n' +
      '\n' +
      'Based on \\(\\|\\mathbf{W}\\|_{F}^{2}=\\||\\mathbf{W}\\|_{F}^{2}\\), we have\n' +
      '\n' +
      '\\[\\sum_{i=1}^{n}\\sigma_{i}^{2}\\left(\\mathbf{W}\\right)=\\sum_{i=1}^{n}\\sigma_{i}^{2 }\\left(\\left|\\mathbf{W}\\right|\\right).\\]\n' +
      '\n' +
      'According to Lemma 1, we can conclude\n' +
      '\n' +
      '\\[\\|\\mathbf{E}_{2}\\|_{F}^{2}\\leq\\|\\mathbf{E}_{1}\\|_{F}^{2}.\\]\n' +
      '\n' +
      'From the equation in this proposition, we can formulate\n' +
      '\n' +
      '\\[\\mathbf{W}_{\\mathrm{sign}}\\odot|\\mathbf{W}|=\\mathbf{W}_{\\mathrm{sign}}\\odot \\tilde{\\mathbf{a}}\\tilde{\\mathbf{b}}^{\\mathrm{T}}+\\mathbf{W}_{\\mathrm{sign}} \\odot\\mathbf{E}_{2}.\\]\n' +
      '\n' +
      'Hence we have\n' +
      '\n' +
      '\\[\\mathbf{W}-\\mathbf{W}_{\\mathrm{sign}}\\odot\\tilde{\\mathbf{a}}\\tilde{\\mathbf{b}} ^{\\mathrm{T}}=\\mathbf{W}_{\\mathrm{sign}}\\odot\\mathbf{E}_{2}.\\]\n' +
      '\n' +
      'Therefore\n' +
      '\n' +
      '\\[\\|\\mathbf{W}_{\\mathrm{sign}}\\odot\\mathbf{E}_{2}\\|_{F}^{2} =\\sum_{i,j}s_{ij}^{2}e_{ij}^{2}=\\sum_{i,j}e_{ij}^{2}\\] \\[=\\|\\mathbf{E}_{2}\\|_{F}^{2}\\leq\\|\\mathbf{E}_{1}\\|_{F}^{2},\\]\n' +
      '\n' +
      'where \\(s_{ij}=\\pm 1\\) is the element of \\(\\mathbf{W}_{\\mathrm{sign}}\\). Hence the inequation in this proposition is proved.\n' +
      '\n' +
      '### Details on Baselines\n' +
      '\n' +
      'In this subsection, we provide the essential details of the baselines in this work:\n' +
      '\n' +
      '* GPTQ (Frantar et al., 2022): We employ the open-source code released by the author. Both OPT models and LLaMA models take 128 2048-token samples from the C4 dataset to calibrate the quantized model. For LLaMA models, we apply the activation order heuristic according to the recommendation from the code.\n' +
      '* LLM-QAT (Liu et al., 2023): We reimplement this method to adapt the W2A16 setting, as LLM-QAT is not designed for 2-bit weight quantization. We also do not quantize the KV Cache. When quantizing the weight matrix in Linear layer, we use symmetric MinMax quantization in which the zero-point is set to 0. The training hyper-parameters are the same as ours. Please refer to the training details in Section 4.1.\n' +
      '* OmniQuant (Shao et al., 2023): We employ the open-source code released by the author. Both OPT models and LLaMA models take 128 2048-token samples from the WikiText2 dataset to calibrate the quantized model. The learning rate for learnable weight clipping and equivalent transformation is set to 5e-3 and 1e-2, respectively. We use a batch size of 1 and train 40 epochs for each model. For OPT models, both learnable weight clipping and equivalent transformation are leveraged. For LLaMA models, only learnable weight clipping is used.\n' +
      '\n' +
      '### Results of LLaMA2\n' +
      '\n' +
      'Table 4 compares the results on LLaMA2-7B/13B. Obviously, our method has advantages in both perplexity and zero-shot accuracy. It also reflects that the advantages of our method are more pronounced in larger models. For instance, when scaling from LLaMA2-7B to LLaMA2-13B, the perplexity of the FP16 model decreases by around only 0.5, whereas our method reduces it by around 1.0 on both Wiki2 and C4 datasets.\n' +
      '\n' +
      '### Instrution Following Ability\n' +
      '\n' +
      'Instruction following is an important ability of LLMs (Radford et al., 2019; Brown et al., 2020; Peng et al., 2023). Beyond the discussion on model abilities and efficiency before, we also focus on the instruction following ability of extremely low-bit models, which is closely related to their practical usability. In this subsection, we empirically study this capability of our quantized model. We fine-tune the model for 3 epochs using the al-paca_en_52k dataset and alpaca templates (Taori et al., 2023), then observe the generation in both zero-shot and few-shot settings before and after fine-tuning. During training, the learning rate is set to 1e-7 and the batch size to 32. Other parameters are consistent with Section 4.1.\n' +
      '\n' +
      'Table 5 demonstrates the content generation and instruction following abilities of our 7B model. Under the zero-shot setting, the model without SFT produced verbose, repetitive, and low-quality text. However, once experienced to SFT, our model is able to smoothly output high-quality content, exhibiting excellent instruction following ability. For the few-shot setting, our model exhibits instruction following ability both before and after SFT.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c|c c c c c c} \\hline \\hline \\multirow{2}{*}{**Models**} & \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c|}{**Perplexity(\\(\\downarrow\\))**} & \\multicolumn{6}{c}{**Zero-shot Accuracy(\\(\\uparrow\\))**} \\\\  & & **Wiki2** & **C4** & **Winogrande** & **Hellawag** & **PIQA** & **BoolQ** & **ARC-e** & **ARC-c** & **Avg.** \\\\ \\hline \\multirow{8}{*}{LLaMA2-7B} & FP16 & 5.47 & 6.97 & 67.09 & 72.94 & 76.88 & 71.10 & 53.58 & 40.61 & 63.70 \\\\  & GPTQ & 7.763 & NAN & 50.28 & 26.19 & 49.46 & 42.97 & 26.77 & 28.58 & 37.38 \\\\  & LLM-QAT & 1.1e3 & 6.6e2 & 49.08 & 25.10 & 50.12 & 37.83 & 26.26 & 26.96 & 35.89 \\\\  & OmniQuant & 31.21 & 64.34 & 51.22 & 33.87 & 56.53 & 59.14 & 33.63 & 24.32 & 43.12 \\\\  & OneBit & **9.73** & **11.11** & **58.41** & **52.58** & **68.12** & **63.06** & **41.58** & **29.61** & **52.23** \\\\ \\hline \\multirow{8}{*}{LLaMA2-13B} & FP16 & 4.88 & 6.47 & 69.77 & 76.62 & 79.05 & 68.99 & 57.95 & 44.20 & 66.10 \\\\  & GPTQ & 2.1e3 & 3.2e2 & 51.85 & 25.67 & 51.74 & 40.61 & 25.46 & 27.30 & 37.11 \\\\ \\cline{1-1}  & LLM-QAT & 5.1e2 & 1.1e3 & 51.38 & 24.37 & 49.08 & 39.85 & 27.15 & 24.32 & 36.03 \\\\ \\cline{1-1}  & OmniQuant & 16.88 & 27.02 & 53.20 & 50.34 & 62.24 & 62.05 & 40.66 & 29.61 & 49.68 \\\\ \\cline{1-1}  & OneBit & **8.76** & **10.15** & **61.72** & **56.43** & **70.13** & **65.20** & **43.10** & **33.62** & **55.03** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Results of LLaMA2. We bold the **best** scores.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'List several Windows operating systems developed by Microsoft corporation.\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014.\n' +
      '\n' +
      'Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the successor to Windows 8.1.\n' +
      '\n' +
      'Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the successor to Windows 8.1. Windows 10 is the successor to Windows 8.1.\n' +
      '\n' +
      'Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets.\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n' +
      '\n' +
      '*** Instruction:\n' +
      '\n' +
      'List several Windows operating systems developed by Microsoft corporation.\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'Microsoft has developed several Windows operating systems, including Windows 10, Windows 8, Windows 7, Windows XP, Windows 2000, Windows NT, Windows ME, Windows 95, and Windows 1. Windows 10 is the latest version of the Windows operating system. It was released in 2019. Windows 8 was released in 2011. Windows 7 was released in 2009. Windows XP was released in 2001. Windows NT was released in 1997. Windows ME was released in 1999. Windows 95 was released in 1995. Windows 1 was released in...\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'The following are multiple choice questions (with answers) about college biology.\n' +
      '\n' +
      'Which of the following contain DNA sequences required for the segregation of chromosomes in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes Answer:\n' +
      '\n' +
      'B. The Barr body found within female mammalian cells\' nuclei is an example of: A. euchromatin B. heterochromatin C. a cistron D. pseudogenes Answer: B. Cleavage symmetry arising from early cell divisions that occur at an oblique angle to the polar axis is known as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The concept of punctuated equilibrium was sudden environmental changes, e.g., fire C. persistent predator-prey relationships in relatively stable environments D. bursts of speciation followed by relatively unchanging lineages Answer:\n' +
      '\n' +
      'D. Which of the following is a second messenger that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin C. Cyclic AMP D. Inositol triphosphate Answer:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'D (Correct answer)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline  & **Prompt:** \\\\  & List several Windows operating systems developed by Microsoft corporation. \\\\  & **Generation:** \\\\  & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. \\\\\n' +
      '**Zero-shot w/o SFT** & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the successor to Windows 8.1. \\\\\n' +
      '**Zero-shot w/o SFT** & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. It was first released to the public on 29 April 2014. Windows 10 is the successor to Windows 8.1. \\\\\n' +
      '**Zero-shot w/ SFT** & Microsoft Windows 10 is a free operating system developed by Microsoft for use on personal computers and tablets. \\\\ \\hline\n' +
      '**Prompt:**\n' +
      '\n' +
      'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n' +
      '\n' +
      '*** Instruction:\n' +
      '\n' +
      'List several Windows operating systems developed by Microsoft corporation.\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'Microsoft has developed several Windows operating systems, including Windows 10, Windows 8, Windows 7, Windows XP, Windows 2000, Windows NT, Windows ME, Windows 95, and Windows 1. Windows 10 is the latest version of the Windows operating system. It was released in 2019. Windows 8 was released in 2011. Windows 7 was released in 2009. Windows XP was released in 2001. Windows NT was released in 1997. Windows ME was released in 1999. Windows 95 was released in 1995. Windows 1 was released in...\n' +
      '\n' +
      '**Prompt:**\n' +
      '\n' +
      'The following are multiple choice questions (with answers) about college biology.\n' +
      '\n' +
      'Which of the following contain DNA sequences required for the segregation of chromosomes in mitosis and meiosis? A. Telomeres B. Centromeres C. Nucleosomes D. Spliceosomes Answer:\n' +
      '\n' +
      'B. heterochromatin C. a cistron D. pseudogenes Answer: B. Clearage symmetry arising from early cell divisions that occur at an oblique angle to the polar axis is known as: A. meroblastic cleavage B. radial cleavage C. spiral cleavage D. superficial cleavage Answer: C. Which of the following is NOT a source of atmospheric carbon? A. Respiration B. Photosynthesis C. Bacterial decomposition D. Combustion of fossil fuels Answer: B. The concept of punctuated equilibrium refers to: A. oscillating ecological succession stages B. ecological succession arrested by sudden environmental changes, e.g., fire C. persistent predator-prey relationships in relatively stable environments D. bursts of speciation followed by relatively unchanging lineages Answer:\n' +
      '\n' +
      'D. Which of the following is a second messenger that stimulates release of calcium ions into the cytoplasm? A. Prostaglandins B. Calmodulin C. Cyclic AMP D. Inositol triphosphate Answer:\n' +
      '\n' +
      '*** Response:\n' +
      '\n' +
      '**Generation:**\n' +
      '\n' +
      'D (Correct answer)\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 5: Instruction following cases in zero-shot and few-shot settings before and after Supervised Fine-Tuning (SFT). The few-shot examples are from MMLU benchmark.\n' +
      '\n' +
      '### Comparison with BitNet\n' +
      '\n' +
      'Recently, BitNet [23] introduces a 1-bit model architecture and applies the architecture to train models from scratch, demonstrating the feasibility and application value of the 1-bit model structure. In this paper, we attempt to combine 1-bit quantization with knowledge distillation to quantize the LLaMA-7B model. Unfortunately, despite following the suggestion to use larger learning rates, the behavior remains unstable during training.\n' +
      '\n' +
      'Figure 6 shows that the training process of BitNet may suffer from instability during knowledge distillation. We conjecture that it is because the gradient is pretty large when the weight elements fluctuate between +1 and -1, further aggravating the output of the Linear layer.\n' +
      '\n' +
      'As a more effective measure, the value vectors we propose for quantization not only supplement the necessary floating-point numerical precision but also limit the fluctuation range of the matrix multiplication results after quantization. This can be understood from forward and backward computation, respectively.\n' +
      '\n' +
      'Forward stability.Quantized matrix multiplication is more prone to overflow than FP16 counterparts in response to minor perturbations of input activations. This is because the magnitude of elements in quantized matrices, particularly the value \\(\\pm 1\\), is far greater than the parameters of most FP16 matrices. By multiplying by value vectors of a magnitude similar to that of the FP16 model, the range of variation in model output activations can be restored to the level of FP16. Furthermore, we also avoid the increasingly large "drift phenomenon" of activations through Post-LayerNorm.\n' +
      '\n' +
      'Backward stability.Since \\(\\mathrm{Sign}(\\cdot)\\) function is not differentiable, when the elements of the matrix change, their gradient may become infinite. Similar to forward stability, by multiplying two numerically smaller value vectors, we avoid layer-by-layer accumulation and explosion during gradient back-propagation. Moreover, we implement the derivative function of \\(\\mathrm{Sign}(\\cdot)\\) using the derivative of the hyperbolic tangent function, thereby avoiding the problem of gradient explosion at the zero point of every weight.\n' +
      '\n' +
      '### Average Bit-width of Linear Layer\n' +
      '\n' +
      'This subsection formulates the calculation of the average bit-width of Linear layers. Assume there is a weight matrix with a shape of \\(4096\\times 4096\\) in such a layer, the number of bits in every component is\n' +
      '\n' +
      '\\[1\\times 4096\\times 4096,\\] \\[16\\times 1\\times 4096\\times 2,\\]\n' +
      '\n' +
      'where the first is for the 1-bit quantized weight matrix and the second is for the two FP16 value vectors. Hence the overall number of bits is \\(16,908,288\\). Moreover, the number of parameters is \\(4096\\times 4096+2\\times 4096\\times 1=16,785,408\\). Therefore, the average bit-width of this Linear layer is \\(16,908,288\\div 16,785,408\\approx 1.0073\\).\n' +
      '\n' +
      'Figure 6: Training comparisons among different learning rates when BitNet performs knowledge distillation from LLaMA-7B. Here we choose the same W1A16 setting as ours. The weight matrices in BitNet are directly copied from the original LLaMA-7B model.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>