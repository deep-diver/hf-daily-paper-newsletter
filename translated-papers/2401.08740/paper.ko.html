<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '슬로 및 디퓨전 기반 유전체 모델# SiT\n' +
      '\n' +
      '사용 가능한 인터폴란트 전송 장치\n' +
      '\n' +
      '난예마 마크 골드슈타인  마이클 S. 알버고 니콜라스 M. 비비야.\n' +
      '\n' +
      '세이릭 반덴-에멘-에ijnden은 Xieric Vanden-Eijnden을 염색한다.\n' +
      '\n' +
      '뉴욕대.\n' +
      '\n' +
      'Code: [https://github.com/willsma/SiT](https://github.com/willsma/SiT)\n' +
      '\n' +
      'Equal advising.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 디확산 트랜스포머(DiT)의 백본에 구축된 생성 모델 계열인 스칼러블 인터폴란트 트랜스포머(SiT)를 제시한다. 표준 확산 모델보다 더 유연한 방식으로 두 분포를 연결할 수 있는 보간 프레임워크는 역동적 수송에 구축된 생성 모델에 영향을 미치는 다양한 설계 선택에 대한 모듈식 연구를 가능하게 한다. 지속적인 시간 학습, 모델이 학습할 목적을 결정하고, 분포를 연결하는 보간체를 선택하고, 결정론적 또는 확률적 샘플러를 배치한다. 위 성분을 조심스럽게 도입하여 SiT는 동일한 백본, 파라미터 수 및 GFLOP를 사용하여 조건부 이미지넷 256x256 벤치마크에서 모델 크기에 걸쳐 획일적으로 DiT를 능가한다. 학습과 별도로 조정 가능한 다양한 확산 계수를 탐색하여 SiT는 FID-50K 점수 2.06을 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지 생성의 일시적인 성공은 알고리즘 발전 및 모델 아키텍처의 개선 및 스케일링 신경망 모델 및 데이터의 진행의 조합에서 비롯되었다. 최첨단 확산 모델[25, 51]은 데이터를 이산 또는 연속 시간에 특정할 수 있는 반복 확률적 공정에 의해 규정된 가우시안 노이즈로 확장 변환함으로써 진행된다. 추상적인 수준에서 이러한 부패 과정은 원본 데이터 분포로부터 반복적으로 평활화되는 시간 의존적 분포를 표준 정규 분포로 정의하는 것으로 볼 수 있다. 확산 모델은 이러한 부패 과정을 역전시키고 이러한 연결을 따라 가우시안 노이즈를 뒤로 밀어 데이터 샘플을 얻기 위해 학습한다. 이러한 변형을 수행하기 위해 학습된 대상은 종래 부패 과정[25]에서 소음을 예측하거나 자료와 가우시안[62]를 연결하는 분포의 점수를 예측하는 것이지만 이러한 선택의 대안은 [27, 54]가 존재한다.\n' +
      '\n' +
      '이러한 객체들을 나타내는 데 사용되는 신경망 아키텍처는 다양한 작업들에 대해 잘 수행되는 것으로 나타났다. 확산 모델은 원래 U-Net 백본[25, 52]에 구축된 반면, 최근 연구는 비전 트랜스포머(ViT)[21]과 같은 비전의 건축 발전이 성능[48]을 개선하기 위해 표준 확산 모델 파이프라인에 통합될 수 있음을 강조했다. [48]의 목표는 알고리즘과 모델의 이중성의 모델 측면에 대한 개선을 추진하는 것이었다.\n' +
      '\n' +
      '오토돈적으로, 유의미한 연구 노력은 성취 과정의 구조를 탐구하는 데 들어갔는데, 이는 성과 혜택[32, 35, 36, 58]으로 이어지는 것으로 나타났다. 그러나 이러한 노력의 대부분은 지나가겠다는 개념을 지나서 움직이지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Params(M) & Training Steps & FID \\(\\downarrow\\) \\\\ \\hline DiT-S & 33 & 400K & 68.4 \\\\ SiT-S & 33 & 400K & **57.6** \\\\ \\hline DiT-B & 130 & 400K & 43.5 \\\\ SiT-B & 130 & 400K & **33.5** \\\\ \\hline DiT-L & 458 & 400K & 23.3 \\\\ SiT-L & 458 & 400K & **18.8** \\\\ \\hline DiT-XL & 675 & 400K & 19.5 \\\\ SiT-XL & 675 & 400K & **17.2** \\\\ \\hline DiT-XL & 675 & 7M & 9.6 \\\\ SiT-XL & 675 & 7M & **8.6** \\\\ \\hline DiT-XL (cfg=1.5) & 675 & 7M & 2.27 \\\\ SiT-XL (cfg=1.5) & 675 & 7M & **2.06** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **Scalable 인터폴란트 트랜스퍼러** 우리는 생성 모델의 다음과 같은 측면을 체계적으로 달리한다. 연속 시간**, 모델 예측, 보간, **ch******** **샘플러의***입니다. 생성된 Scalable 인터폴란트 트랜스포머(SiT) 모델은 동일한 훈련 계산 하에서 256x256 이미지넷 이미지를 생성하는 데 있어서 일관되게 디확산 트랜스포머(DiT)를 능가한다. 모든 모델은 2의 패치 크기를 사용하는데, 이 작업은 __성과 이득의 원천이 무엇인지 질문한다. 데이터와 가우시안 사이의 제한된 유형의 연결인 평형 분포를 갖는 확산 과정을 통한 데이터이다. 최근 도입된 _줄기간 보간제_[3]는 이러한 제약을 해제하고 소음-데이터 연결에서 더 많은 유연성을 도입한다. 본 논문에서는 대규모 영상 생성에서 그 성과를 더욱 탐색한다.\n' +
      '\n' +
      '직관적으로 _러닝 문제_의 난이도는 선택한 특정 연결과 학습되는 대상과 모두 관련될 수 있을 것으로 기대한다. 우리의 목표는 학습 문제를 단순화하고 성능을 향상시키기 위해 이러한 디자인 선택을 명확히 하는 것입니다. 학습 문제에서 잠재적인 이점이 발생하는 청소하기 위해, 우리는 학습 대상이 되는 (i)와 모범 관행을 밝히기 위해 보간되는 (ii)의 적응을 통해 덴오징 디퓨전 안정화 모델(DDPM)과 스윕으로 시작한다.\n' +
      '\n' +
      '학습 문제 외에도 추론 시간에 해결해야 할 _샘플링 문제_가 있다. 샘플링은 결정론적 또는 확률적 [61]일 수 있고, 샘플링 방법의 선택은 학습 과정 이후에 이루어질 수 있다는 것을 확산 모델에 대해 인정했다. 그러나 확률적 샘플링에 사용된 확산 계수는 일반적으로 일반적으로 정방향 산란 과정에 본질적으로 묶인 것으로 제시되며, 이는 일반적으로 경우에 해당되지 않는다.\n' +
      '\n' +
      '이 논문 전반에 걸쳐 보간제의 설계와 결과 모델을 결정론적 또는 확률적 샘플러로 사용하는 것이 성능에 어떤 영향을 미치는지 탐구한다. 우리는 디자인 공간에서 일련의 직교 단계를 취함으로써 전형적인 탈염 확산 모델에서 보간 모델로 점진적으로 전환한다. 진행됨에 따라 확산 모델에서 벗어나서 각각의 이동이 성능에 어떤 영향을 미치는지 주의 깊게 평가한다. 요약하면, **주 기여***가 있습니다.\n' +
      '\n' +
      '*는 **도입에서 연속시간**로 이동하여 모델 예측, **인터폴란트***, 샘플러***의 ** 선택으로 변경하면 디확산트랜스포머(DiT)에 비해 일관된 성능 향상을 관측한다.\n' +
      '* 우리는 이러한 요인을 하나씩 해결함으로써 이러한 개선이 발생하는 경우 체계적으로 연구하는데, 이 요인들은 연속 시간에 학습, _score_와 비교하여 _velocity_를 학습하며, 두 분포를 연결하는 보간체를 변경하고, 확산 계수의 특정 선택으로 SDE 샘플러에서 속도를 사용한다.\n' +
      '* 우리는 보간제에 대한 SDE가 속도 모델만을 사용하여 인스턴스화될 수 있음을 보여주며, 이는 이전 결과를 넘어 이러한 방법의 성능을 밀어내는 데 사용된다.\n' +
      '\n' +
      '그림 1: 이미지넷 [53]에서 cfg = 4.0으로 이미지넷(512\\시 512\\) 및\\(256\\시 256\\) 해상도로 훈련된 SiT-XL 모델의 선택된 샘플이다.\n' +
      '\n' +
      '사용 가능한 인터폴란트 변환기 2 SiT\n' +
      '\n' +
      '흐름 기반 및 확산 기반 생성 모델을 구축하기 위한 주요 성분을 회상하는 것으로 시작한다.\n' +
      '\n' +
      '꽃과 디퓨전, 디퓨전.\n' +
      '\n' +
      '최근 몇 년 동안 노이즈\\(\\mathbf{\\varepsilon}\\ason\\mathsf{N}(0,\\mathbf{I})\\를 데이터 \\(\\mathbf{x}_{*}\\ason p(\\mathbf{x})로 전환시키는 데 기반한 유연한 수준의 생성 모델이 도입되었다. 이 모델은 시간 의존적 프로세스를 사용한다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}+\\sigma_{t}\\mathbf{\\varepsilon}, \\tag{1}\\]\n' +
      '\n' +
      '\\(알파_{t}\\)가 \\(t\\)의 감소 함수이고 \\(\\sigma_{t}\\)는 \\(t\\)의 증가 함수이다. 알파_{0}=1\\), \\(알파_{1}=1\\), \\(\\sigma_{0}=0\\), \\(\\sigma_{0}=0\\) 및 \\(\\mathbf{x}_{t} <0\\)에서, \\(\\bf{x}_{0}=0\\)를 설정함으로써 \\(\\bf{x}. 대조적으로, 점수 기반 확산 모델[32, 36, 62]은 \\(\\alpha_{t}\\) 및 \\(\\sigma_{t}\\)를 모두 평형 분포로서 \\(\\mathsf{N}(0,\\mathbf{I})를 갖는 확률적 차등 방정식(SDE)의 상이한 제형을 통해 간접적으로 설정했다. 더욱이, 그들은 \\(\\mathbf{x}_{t}\\)가 가우시안 분포를 근사할 만큼 충분히 큰 \\(T\\)를 갖는 간격([0,T]\\)에서 프로세스 \\(\\mathbf{x}_{t}\\)를 고려한다.\n' +
      '\n' +
      '확률 흐름 및 점수 기반 확산 모델 모두에 대해 일반적으로는 프로세스 \\(\\mathbf{x}_{t}\\)가 SDE 또는 확률 흐름 일반 미분 방정식(ODE)을 사용하여 동적으로 샘플링될 수 있다는 관찰이다. (1)의\\(\\mathbf{x}_{t}\\)의 확률 흐름 ODE 분포와 보다 정확하게는 한계 확률 분포 \\(p_{t}(\\mathbf{x})와 일치한다.\n' +
      '\n' +
      '\\[\\dot{\\mathbf{X}}_{t}=\\mathbf{v}(\\mathbf{X}_{t},t), \\tag{2}\\]\n' +
      '\n' +
      '\\(\\mathbf{v},\\mathbf{x},t)는 조건부 기대치에 의해 주어진다.\n' +
      '\n' +
      '>\\mathbf{x}[\\mathbf{x} <\\mathbf{x} <\\mathbf{x} <\\mathbf{x} <\\mathbf{x} <\\mathbf{x} <\\mathbf{x} <\\mathbf{x}>} <\\mathbf{x}>} <\\mathbf{t} <\\mathbf{t}>} <\\mathbf{t}} <\\mathbf{t}} <\\mathbf{t} <{t}>} <\\mathbf{t}>} <\\mathbf{t}>} <\\mathbf{t} <{t}>} <\\mathbf{t} <{t}>} <\\mathbf{t}} <\\mathbf{t} <{t}>} <\\mathbf{t} <{t}>} <\\mathbf{t}} <\\mathb\n' +
      '\n' +
      '\\(\\mathbf{X}_{T}){T}=\\mathbf{\\varepsilon}\\mathsf{N}(0,\\mathbf{I})\\에서 시간이 지남에 따라 ODE(2)가 파생된다. 우리는 (2)를 _ 흐름 기반_ 생성 모델로 지칭한다.\n' +
      '\n' +
      '(\\mathbf{x}_{t}_{t})\\(\\mathbf{x}_{t})\\(p_{t}(\\mathbf{x})\\)의 시간 의존적 확률 분포(p_{t}(\\mathbf{x})도 역전 시간 SDE[5]시간 SDE의 분포와 일치한다.\n' +
      '\n' +
      '}} <\\mathbf{t>}(\\mathbf{f}_{t})\\mathbf{v}(\\mathbf{f}_{t})\n' +
      '\n' +
      '\\(w_{t}}<{t}>0\\)는 임의의 시간 의존적 확산 계수, \\(\\mathbf{v}) 즉,\\(\\mathbf{v}(\\mathbf{x},t)는 (3)에서 정의된 속도이며, 여기서 \\(\\mathbf{s}(\\mathbf{x},\\)는 \\(\\mathbf{v})는 \\(\\mathbf{f{v}(\\mathbf{v})는 \\(\\mathbf{v}(\\mathbf{v})는 (\\mathbf{v}(\\mathbf{v}(\\mathbf{v}(\\mathbf{v},\\mathbf{f{v},\\)는 \\)는 (\\mathbf{v}(\\mathbf{v}(\\mathbf{v}(\\mathbf{x},\\)는 (\\mathbf{v} \\(\\mathbf{v}}\\)와 유사하게 이 점수는 조건부 기대치에 의해 주어진다.\n' +
      '\n' +
      '\\[\\mathbf{s}(\\mathbf{x},t)=-\\sigma_{t}^{t}^{bb{E}[\\mathbf{\\bf{{E}[\\mathbf{\\varepsilon}|\\mathbf{f{x}_{t}=\\mathbf{x})]=\\mathbf{x} <\\mathbf{x}.\n' +
      '\n' +
      '이 방정식은 a\\(\\mathbf{X}_{T}=\\mathbf{\\bf{T}=\\mathbf{varepsilon}\\ason\\mathsf{N}(0,\\mathbf{I})\\에서 시간이 지남에 따라 파생된 것으로, 근사 데이터 분포 \\(p_{0}(\\mathbf{x})\\심 p(\\mathbf{x})\\(\\mathbf{x})\\(\\mathbf{X})\\(\\mathbf{x})\\(\\mathbf{x}(\\mathbf{x})\\)\\(\\mathbf{x}(\\mathbf{x}(\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x},\\mathbf{x})\\ 우리는 (2)를 _확산 기반_생성 모델로 지칭한다.\n' +
      '\n' +
      '디자인 선택 스코어 기반 확산 모델은 일반적으로 \\(\\alpha_{t}\\), \\(\\sigma_{t}\\), \\(\\sigma_{t}\\), \\(w_{t}\\)의 선택을 \\(\\mathbf{x}_{t}\\)를 생성하는 정방향 SDE에서 사용되는 드리프트 및 확산 계수에 (아래 (10 참조)로 묶는다. 확률적 인터폴란트 프레임워크는 정방향 SDE에서 \\(\\mathbf{x}_{t}\\)의 제형을 탈색하고 \\(\\alpha_{t}\\), \\(\\sigma_{t}\\), \\(w_{t}\\)의 선택에 더 유연성이 있음을 보여준다. 아래에서는 이러한 유연성을 활용하여 이미지 생성 작업에서 표준 벤치마크에 대한 점수 기반 확산 모델을 능가하는 생성 모델을 구성할 것이다.\n' +
      '\n' +
      '#####는 점수와 속도를 계산하면 점수와 속도를 계산합니다.\n' +
      '\n' +
      '확률 흐름 ODE(2) 및 역시간 SDE(4)를 생성 모델로 실용적인 사용은 속도 \\(\\mathbf{v}(\\mathbf{x},t) 및 점수 \\(\\mathbf{s}(\\mathbf{x},t)를 추정하는 능력에 의존한다. 점수 기반 확산 모델에서 이루어진 주요 관찰은 점수를 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t)로 모수적으로 추정할 수 있다는 것이다.\n' +
      '\n' +
      '\\{L}(\\mathcal{L}_{\\text{L})=\\mathbb}^{T}[\\|\\sigma_{t}[\\|\\sigma_{t}\\|\\sigma_{t}\\mathbf{s}_{ \\mathbf{s}}_{ \\mathbf{s}(\\mathbf{s}:\\mathbf{t},\\mathbf{t},\\mathbf{t},\\mathbf{t},\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t} <{t}:\\mathbf{t}:\\mathbf{f{f{s}_{t}\n' +
      '\n' +
      '이러한 손실은 조건부 기대의 표준 성질과 함께 (5)를 이용하여 도출할 수 있다. 유사하게, 유사하게,\n' +
      '\n' +
      '그림 2: **SiT는 모든 모델 크기에 걸쳐 FID의 개선을 관찰했으며*** DiT와 SiT 모두에 대한 훈련 반복보다 FID-50K를 보여준다. 모든 결과는 250개의 통합 단계를 사용하여 오일러 마누야마 샘플러에 의해 생성된다. 모든 모델 크기에 걸쳐 SiT는 훨씬 더 빠르게 수렴한다.\n' +
      '\n' +
      '(3)의 관측은 손실(\\mathbf{v}_{\\theta}(\\mathbf{x},t)을 통해 \\(\\mathbf{v}_{\\theta})로 모수적으로 추정할 수 있다.\n' +
      '\n' +
      '<\\|\\mathbf{v}>=\\|\\mathbf{v}_{\\mathbf{v}}[\\mathbf{v}}_{\\mathbf{v}}( \\mathbf{x}_{t)-\\dath{f}_{t}}_\\math{f{v}}_{\\mathbf{v}}_{\\mathbf{v}}_{\\mathbf{v}}_{\\mathbf{v}}_{\\mathbf{v}}_{\\mathbf{v}}}_{f{f}}_{f}}_{f{t}}_{f}}_{f}}}_{t)-\\mathbf{t}}}_{f{t}}}_{f}}}_{t)-\\dot{t}}_{t}}}}}_{t)-\\dot{t}}}}}_{t)-\\dot{\n' +
      '\n' +
      '우리는 시간 의존적 중량이 (6)과 (7) 모두에서 적분 아래 포함될 수 있다는 점에 주목한다. 이러한 가중치 요인은 \\(T\\)가 [35]가 클 때 점수 기반 모델의 맥락에서 핵심이며, 대조적으로 \\(T=1\\)는 편향 없이 확률적 보간제와 함께 이러한 가중치가 덜 중요하며 수치 안정성 문제(부록 B 참조)를 부과할 수 있다.\n' +
      '\n' +
      '모델 예측은 두 가지 양 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t)\\)과 \\(\\mathbf{v}_{\\theta}(\\mathbf{x},t)\\) 중 하나만 실제 추정해야 함을 관찰했다. 이는 제약 조건으로부터 직접적으로 뒤따른다.\n' +
      '\n' +
      '}.\n' +
      '\n' +
      '속도(3)의 측면에서 점수(5)를 재표현하기 위해 사용될 수 있는 속도(3)와 같이 속도(5)의 측면에서 점수(5)를 재표현하는 데 사용될 수 있다.\n' +
      '\n' +
      '<\\{t_{t}\\mathbf{t}>=\\mathbf{t}\\mathbf{t}}\\mathbf{v}}(\\mathbf{x},t)-\\mathbf{x}_{t},\\math{f{x}_{t}}}}{mathbf{t}:\\mathbf{t}.\n' +
      '\n' +
      '이 관계를 사용하여 ** 모델 예측**를 지정합니다. 반대로, 우리는 \\(\\mathbf{v}(\\mathbf{x},t)\\(\\mathbf{s}(\\mathbf{x},t)\\) 측면에서 \\(\\mathbf{v}(\\mathbf{x},t)\\도 표현할 수 있다. 우리의 실험에서 우리는 보통 속도 필드 \\(\\mathbf{v}(\\mathbf{x},t)를 배우고 샘플링을 위해 SDE를 사용할 때 점수 \\(\\mathbf{s}(\\mathbf{x},t)를 표현하는데 사용한다. 우리는 부록 A.4에 상세한 도출을 포함한다.\n' +
      '\n' +
      '우리의 정의(\\dot{\\alpha}_{t}<0\\)와 \\(\\dot{\\sigma}_{t}>0\\)에 의해 (9)의 분모가 결코 0이 아님을 주목한다. 그러나 \\(\\sigma_{t}\\)는 \\(t=0\\)에서 소실되어(9)에서 \\(\\sigma_{t}^{{-1}\\)가 특이점을 유발하는 것으로 보이며, 이는 (4)에서 이 특이점을 취소하기 위한\\(w_{t}=\\sigma_{t}={t}.3)를 제안한다.\n' +
      '\n' +
      '부츠 1:\\(\\mathbf{s}(\\mathbf{x},t)\\)는 데이터 분포 \\(p(\\mathbf{x})가 평활 밀도[3]를 갖는 경우 분석적으로 \\(t=0\\)에서 비단일화인 것으로 보일 수 있지만, 이러한 특이점은 일반적으로 수치 구현 및 손실에서 나타난다.\n' +
      '\n' +
      '###은 보간 공정을 유추한다.\n' +
      '\n' +
      '스카어 기반 확산 모델(SBDM)에서 (1)에서 \\(\\alpha_{t}\\) 및 \\(\\sigma_{t}\\)의 선택은 보통 이 과정을 정의하는 데 사용되는 정방향 SDE의 선택에 의해 결정되지만 최근 작업은 이 [32, 35]를 재고하려고 했다. 예를 들어, 표준 분산 보존(VP) SDE[62]을 사용하는 경우입니다.\n' +
      '\n' +
      '}{t}}{mathrm{d}.\n' +
      '\n' +
      '일부\\(\\beta_{t}>0\\)에 대해서는 (10)에 대한 솔루션이 (p_{t}(\\mathbf{x})과 동일한 확률 분포 (p_{t} (\\mathbf{x})를 가지고 있다는 것을 (부록 B 참조) 나타낼 수 있다.\n' +
      '\n' +
      'r\\{VP: \\text{VP:}\\text{t}=e^{{-\\frac{1}}{2}\\int_{0}^{t}\\beta_{t}\\beta_{s}\\qquad\\_{t}\\qquad\\_{t}}, \\qquad\\sigma_{t}\\{{t}\\{{t}\\{{t}\\{{t}\\{{t}\\{{t}\\{{t}\\beta_{t}\\beta_{t}\\beta_{t}\\{t}\\beta_{t}\\beta_{t}\\beta_{t}\\{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{t}\\beta_{\n' +
      '\n' +
      '(11)의 유일한 설계 유연성은 \\(\\alpha_{t}\\)와 \\(\\sigma_{t}\\)2를 모두 결정하기 때문에 \\(\\sigma_{t}\\)의 선택에서 비롯되며, 예를 들어 \\(\\beta_{t}=1\\)를 설정하면 \\(\\beta_{t}=e^{t}=e^{t}=e^{t}\\)와 \\(\\sigma_{t}\\) 및 \\(\\sigma_{t}<\\_{t}1\\)는 \\(\\_{t}1\\)는 \\(\\_{t}=1\\)는 \\(\\_{t}=1\\)는 \\(\\_{t}=e^_{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{t}=e^{ 이 선택은 \\(T\\_{t}\\)의 충분히 큰 선택[25] 또는 더 적절한 선택을 찾기 위해 \\(\\beta_{t}\\)[62, 58, 16]을 필요로 하며, 이는 SDE(10)에 대한 솔루션만이 \\(B\\boldsymbol{\\varepsilon}\\simchathsf{N}(0,\\mathbf{I})로 수렴한다는 사실만으로 수렴한다는 사실에 의해 유도된 편향을 감소시켜야 한다.\n' +
      '\n' +
      'Footnote 2: VP는 평형 분포[58]를 갖는 유일한 선형 스칼라 SDE이며, 보간체는 평형 분포의 요구 사항을 예견하여 \\(\\알파_{t}^{2}+\\sigma_{t}^{2}=1\\)를 넘어 확장된다.\n' +
      '\n' +
      '확률적 인터폴란트 프레임워크에서 프로세스(1)는 정방향 SDE에 대한 언급 없이 명시적으로 정의되어 \\(\\alpha_{t}\\) 및 \\(\\sigma_{t}\\) 선택에 더 많은 유연성을 생성한다. 구체적으로, 만족스럽지 못한 선택이 가능합니다.\n' +
      '\n' +
      '1. \\(t\\in[0,1]\\)에 대한\\(알파_{t}^{2}^{2}+\\sigma_{t}^{2}^{2}>0\\)\n' +
      '2. \\(알파_{t}\\) 및 \\(\\sigma_{t}\\)는 모든 \\(t\\in[0,1]\\)에 대해 다를 수 있다.\n' +
      '3. \\(알파_{1}=\\sigma_{0}=\\sigma_{0}=0\\), \\(\\alpha_{0}=\\sigma_{1}=1\\)\n' +
      '\n' +
      '\\(\\mathbf{x}_{t=0}=\\mathbf{x}_{*}_{*}\\)와 \\(\\mathbf{x}_{t=1}=\\boldsbol{\\varepsilon}\\) 사이의 편향 없이 보간되는 과정을 조절한다. 수치 실험에서 우리는 특히 이 디자인 유연성을 사용하여 테스트할 수 있습니다.\n' +
      '\n' +
      '},\\t{1}{t}=\\ar:\\qquad\\_{t}),\\qquad\\_{t}=\\sin(\\tfrac{2}{t),\\end{{fit}}.\n' +
      '\n' +
      'GVP가 동일한 분산을 갖는 모든 종말점 분포에 대해 시간에 걸쳐 일정한 분산을 갖는 일반화된 VP를 의미한다. 우리는 현장 \\(\\mathbf{v}(\\mathbf{x},t)}(\\mathbf{s})와 \\(\\mathbf{x},t)\\(\\mathbf{x},t)\\(2) 및 (4)로 들어가는 분야는 현재 학습3 전에 \\(\\alpha_{t}\\) 및 \\(\\mathbf{x}) 및 \\(\\mathbf{x}(\\mathbf{x}(\\mathbf{x}(\\mathbf{x},t)의 선택에 따라 달라지고(\\mathbf{x})\\)의 선택에 의존하며, B\\(\\mathbf{x}(\\)\\)의 선택에 따라 달라지고(\\)\\(\\)\\(\\)\\(\\)\\(\\)\\(\\)\\(\\)의 선택에 의존하며, a\\(\\)\\(\\)\\(\\)\\(\\)\\)를 지정해야 하며, B\\(\\)\\(\\)\\(\n' +
      '\n' +
      '부츠 3: 훈련 시간에 \\(\\alpha_{t},\\sigma_{t}\\)에 의해 지정된 하나의 경로 선택하에 학습 및 샘플을 배워야 할 요구 사항은 완화될 수 있으며 [2]에서 탐색된다.\n' +
      '\n' +
      '확산 계수 측정##\n' +
      '\n' +
      '앞서 밝힌 바와 같이 역 SDE(4)에서 사용되는 SBDM 확산 계수는 통상적으로 정방향 SDE(10)와 일치하도록 취해진다. 즉, 하나의 세트 \\(w_{t}=\\beta_{t}\\)이다. 확률적 보간제 프레임워크에서 이러한 선택은 다시 더 큰 유연성을 적용하며 _any_\\(w_{t}>0\\)를 사용할 수 있다. 흥미롭게도 이 선택은 속도 \\(\\mathbf{v}(\\mathbf{x},t) 또는 점수 \\(\\mathbf{s}(\\mathbf{x},t)\\에 영향을 미치지 않기 때문에 _후에_학습을 할 수 있다. 우리의 실험에서 우리는 표 2에 나열된 선택을 고려하여 이러한 유연성을 사용한다.\n' +
      '\n' +
      '시간 탈분화 및 DDPM과의 연결 및 연결.\n' +
      '\n' +
      '추론 과정에서 확률 흐름 ODE(2)와 역시간 SDE(4)를 풀 때 연속 시간 모델을 폐기해야 한다. 이를 통해 DDPM[25]과 링크를 만들 수 있습니다.\n' +
      '\n' +
      '각 격자점(0=t_{t}}} <\\mathbf{x} <\\mathbf{x}_{t_{i}>}=\\mathbf{x}_{t_{i})을 사용하여 시간을 판별할 수 있다고 가정한다.\n' +
      '\n' +
      '(\\mathb{i})\\math{i}} <\\math{i}} <\\math{i} <\\math{i} <\\math{i} <\\math{i}} <\\math{{i}_\\math{i}} <\\math{{i} <\\math{i} <\\math{{i}>} <\\math{t{i}} <\\math{t{i}} <\\math{t{i} <{t{i} <{t{i}>} <\\math{t{i} <{t{{i} <{t{{i}>}} <\\math{t{{i}} <{t{{i} <{t{{i}>}} <\\math{t{{i}>}} <{t{{i}>}} <\\math{s} <{{i}>}} <\\math{s} <{{i}>}} <\\math{s} <{{i}\n' +
      '\n' +
      '더욱이, 학습된 \\(\\mathbf{s}_{\\theta}(\\mathbf{x},t_{i}) 또는 \\(\\mathbf{v}_{\\theta}_{\\theta}(\\mathbf{x},t_{i})\\)만이 확률 흐름 ODE(2) 및 역시간 SDE(t_{i})를 동일한 그리드에서 통합하기 위해 필요하다. 우리가 격자에 반복적으로 \\(\\mathbf{x}_{t}\\)를 정의하는 결과 절차는 DDPM의 일반화이다. \\(i\\geq 0\\),}(\\mathbf{x}_{t_{0}=\\mathbf{x}_{*})를 시작했다.\n' +
      '\n' +
      '}}{t_{i}}\\mathbf{x}]{t_{i}.\n' +
      '\n' +
      'HH(h=t_{i+1}-t_{i}\\)와 그리드가 균일하다고 가정하는 곳. H\\(\\sqrt{1-h\\beta_{t_{i}}=1-\\frac{1}{2}h\\beta_{t_{i}}+o(h)\\)이기 때문에 (15)가 정방향 SDE(10)의 일관된 시간 분비물임을 쉽게 알 수 있다. 우리의 결과는 (15)를 사용하여 시간 폐기 프로세스 \\(\\mathbf{x}_{t_{i}}\\)를 특정할 필요가 없지만 대신 시간 격자에 직접 (1) 사용할 수 있음을 보여준다.\n' +
      '\n' +
      '인터폴란트 트랜스퍼는 아키텍처 개발\n' +
      '\n' +
      '생성 모델의 백본 아키텍처 및 용량은 또한 고품질 샘플을 생산하는 데 중요하다. 어떤 교란 요인을 제거하고 우리의 탐사에 초점을 맞추기 위해 표준 디퓨전 트랜스포머(DiT) [48] 및 그 구성을 엄격하게 따른다. 이렇게 하면 다양한 모델 크기에 걸쳐 모델의 확장성도 테스트할 수 있습니다.\n' +
      '\n' +
      '여기서 우리는 모델 설계를 간략하게 소개하고 있다. 확산 모델로 고해상도 이미지를 생성하는 것은 계산적으로 비쌀 수 있다. 렌트 확산 모델(LDMs)[51]은 인코더 \\(E\\)를 사용하여 먼저 더 작은 잠재 임베딩 공간으로 영상을 다운샘플링한 다음 \\(z=E(x)\\에서 확산 모델을 트레이닝함으로써 이를 다룬다. 모델에서 \\(z\\)를 샘플링하여 디코더 \\(x=D(z)\\를 이용하여 다시 이미지로 디코딩하여 새로운 이미지를 생성한다.\n' +
      '\n' +
      '유사하게, SiT는 또한 잠재 생성 모델이며 원래 Stable Diffusion[51]에서 사용되는 동일한 사전 훈련된 VAE 인코더 및 디코더 모델을 사용한다. SiT는 1차원이\\(T\\)의 선형적으로 내장된 토큰으로 \'패치화\'(T\\)함으로써 공간 입력 \\(z\\)(32\\t 32\\t 4\\)을 처리한다. 우리는 항상 최고의 샘플 품질을 달성하기 때문에 이 모델에서 2의 패치 크기를 사용한다. 그런 다음\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c} \\hline \\hline\n' +
      '**Expression for \\(w_{t}\\)** \\\\ \\hline \\(\\beta_{t}=-2\\sigma_{t}(\\dot{\\sigma}_{t}-\\frac{\\sigma_{t}\\dot{\\sigma}_{t}}{ \\alpha_{t}})\\) \\\\ \\(\\sigma_{t}\\) \\\\ \\(1-t\\) \\\\ \\(\\sin^{2}(\\pi t)\\) \\\\ \\((\\cos(\\pi t)+1)^{2}\\) \\\\ \\((\\cos(\\pi t)-1)^{2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>:< **D확산 계수.**>는 성능을 극대화하기 위해 학습 후 특정될 수 있으며, <표 2>:< **D확산 계수.**. \\는 학습 후 특정할 수 있다. 첫 번째 행의 (w_{t}=\\beta_{t}\\)은 SBDM(Eq 참조)에 해당한다. 전방 공정과 결합된 (11) 세부 도출은 부록 B. \\(w_{t}=\\sigma_{t}\\)에서 제공되며, 두 번째 행의 특이성은 Sec. 2.2의 끝에서 설명 후 \\(t=0\\)에서 특이점을 제거하기 위해 사용된다.\n' +
      '\n' +
      '그림 3: ** 상승 변압기 크기는 표본 품질*****. __<그림 3>: ** 증가 변압기 크기는 표본 품질****를 증가시킨다. 가장 크게 줌인_을 보았다. 동일한 잠재 소음 및 클래스 라벨을 사용하여 400K 훈련 단계 후에 SiT 모델(SiT-S, SiT-B, SiT-L 및 SiT-XL)의 모든 \\(4\\)에서 샘플을 채취했다.\n' +
      '\n' +
      '이 토큰에 대한 표준 ViT[21] 정현 위치 임베딩이 적용되었습니다. 우리는 각각 숨겨진 치수 \\(d\\)를 가진 일련의 \\(N\\) SiT 변압기 블록을 사용한다.\n' +
      '\n' +
      '우리의 모델 구성들(SiT-{S,B,L,XL})은 모델 크기(파라미터) 및 계산(플롭스)에서 변화하여 모델 스케일링 분석을 허용한다. 이미지넷에서 수업조건부 생성을 위해 AdaLN-Zero 블록[48]을 사용하여 추가적인 조건부 정보(시간 및 등급 라벨)를 처리한다. SiT 건축 세부 정보는 표 3에 나열되어 있다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '타브에서 제기된 질문에 대한 더 자세한 답변을 제공하기 위해요. Dt와 SiT를 1로 공정하게 비교하고 DiT 모델(탈환, 점수 예측, VP 보간제)에서 다음 4개의 하위 섹션에서 SiT 모델(연속, 속도 예측, 라인 인터폴란트)으로 점진적으로 전환하고 성능에 미치는 영향을 제시한다. 각 하위 섹션에서 실험 전반에 걸쳐 400K 훈련 단계에서 DiT-B 모델을 백본으로 사용한다. ODE(2)를 해결하기 위해 고정된 허네이터를 채택했으며 SDE(4)를 해결하기 위해 오일러 마누야마 통합기를 사용했다. 두 솔버 선택 모두 DiT에 사용된 샘플링 단계의 수와 일치하도록 기능 평가 수(NFE)를 \\(250\\)로 제한한다. 다음 섹션에 제시된 모든 숫자는 이미지넷256 훈련 세트에 대해 평가된 FID-50K 점수이다.\n' +
      '\n' +
      '날짜타임으로 옮겨요.\n' +
      '\n' +
      '연속 시간 대 이산 시간 모델의 역할을 이해하기 위해 점수를 추정하면서 연속 시간 SBDM-VP에 대해 이산 시간 DDPM을 연구했다. 결과는 표 4에 나와 있으며, 이산 시간 데노저에서 연속 시간 점수 분야로 갈 때 FID 점수의 한계 개선을 발견했다.\n' +
      '\n' +
      '### Model parameterizations\n' +
      '\n' +
      'SBDM-VP의 맥락에서 모델 파라미터화의 역할을 명확히 하기 위해 (i) 학습(i)을 (6), (ii) 가중 점수 모델 (부록 A.3 참조), 또는 (iii)를 이용하여 속도 모델을 비교한다. 결과는 가중 점수 모델 또는 속도 모델을 학습하여 유의한 성능 향상을 얻는 표 5에 나와 있다.\n' +
      '\n' +
      '도대체 보간물.\n' +
      '\n' +
      '섹션 2는 보간제(1)의 정의에서 \\(\\alpha_{t}\\)와 \\(\\sigma_{t}\\)의 선택을 달리하여 데이터 분포와 가우시안 사이의 연결을 구축할 수 있는 많은 방법이 있음을 강조한다. 이 선택의 역할을 이해하기 위해 우리는 이제 일반적으로 사용되는 SBDM-VP 설정에서 멀어지는 이점을 연구한다. 우리는 Gaussian와 데이터 분포 사이의 보간을 \\([0,1]\\)에서 정확하게 만드는 라인 및 GVP 보간제와 속도 모델 \\(\\mathbf{v}(\\mathbf{x},t)를 학습한다(12). 우리는 표 6의 SBDM-VP에 대해 이러한 모델을 벤치마킹하며, 여기서 우리는 GVP와 라인 인터폴렌트가 모두 상당히 향상된 성능을 얻는다는 것을 발견했다.\n' +
      '\n' +
      '이 관찰에 대한 한 가지 가능한 설명은 그림 1에 나와 있다. 4, SBDM-VP에서 GVP 또는 라인르로 변경될 때 경로 길이(수송 비용)가 감소한다는 것을 알 수 있다. 우리는 또한 SBDM-VP, \\(\\dot{\\sigma}_{t}=\\beta_{t}e^{{-\\int_{0}^{t}\\beta_{s}/(2\\sigma_{t})에서 특이하게 된다는 점에 주목한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & _Model_ & _Objective_ & FID \\\\ \\hline DDPM & Noise & \\(\\mathcal{L}_{\\mathrm{s}}^{N}\\) & 44.2 \\\\ SBDM-VP & Score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & **43.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **DDPM 대 표. SBDM*******SBDM*****.\n' +
      '\n' +
      '<그림 4> <\\mathbb{E> (v)=\\mathbb{E}[|\\mathbf{v}[|\\mathbf{v}[\\mathbf{x}_{t},t)|^{2}]는 SBDM(VP), 라인 및 GVP로 간주되는 다양한 모델에 대한 다양한 훈련 단계에서 속도 필드에서 발생하는 경로 길이 \\*Path{C}[\\mathbf{v}[\\mathbf{C}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}[\\mathbf{v}.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & Layers \\(N\\) & Hidden size \\(d\\) & Heads \\\\ \\hline SiT-S & 12 & 384 & 6 \\\\ SiT-B & 12 & 768 & 12 \\\\ SiT-L & 24 & 1024 & 16 \\\\ SiT-XL & 28 & 1152 & 16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ** SiT 모델의 상세 상세* (S), 베이스(B), 대형(L) 및 X 대형(XL) 모델 구성에 대한 DiT[48]을 따른다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '확산 모델의 훈련 및 샘플링은 [25, 59, 62]에서 발생했으며 변성 방법[28, 29, 57]과 밀접한 역사적 관계를 가지고 있다. DDPM[60] 및 SBDM[32, 61]의 맥락에서 이러한 방법 뒤에 샘플링 알고리즘을 개선하는 데 다양한 노력이 들어갔으며, 이는 또한 우리의 연구와 직교하며 향후 작업에서 더 나은 성능을 추진하기 위해 결합될 수 있다. 개선된 확산 ODE[71]는 또한 ODE를 샘플링하기 위한 모델 매개변수화(벨로시티 대 노이즈) 및 경로(VP 대 라인)의 여러 조합을 연구하며, 더 원활한 확률 흐름을 갖는 속도 모델에 대해 최상의 결과를 보고하며, 더 낮은 차원 실험, 가능성이 있는 벤치마크에 초점을 맞추고 SDE 샘플링을 고려하지 않는다. 우리의 연구에서 VP, 라인 및 GVP 인터폴트 간의 변화와 점수 및 속도 매개변수가 심층적으로 변화하는 효과를 탐색하고 이러한 선택이 더 큰 스케일 이미지넷256에 대한 성능을 개별적으로 개선하는 방법을 보여준다. 또한 확산 계수의 선택에 의해 인덱싱된 블랙박스 ODE 및 SDE를 포함한 샘플링 알고리즘의 계열과 관련하여 FID가 어떻게 변화하는지 문서화하고 최고의 계수 선택이 모델 및 보간체에 의존할 수 있음을 보여준다. 이는 [3]의 샘플링의 유연성과 절충에 대한 관찰을 실천으로 가져온다.\n' +
      '\n' +
      '라인바 인터폴트를 사용한 인터폴트와 흐름 매칭. 라인시티 필드 매개변수도 [39, 42]에서 연구되었으며 [6]의 매니폴드 설정으로 일반화되었다. 표적 분포와 모델 사이의 KL 분기에 대한 결합의 절충은 SDE 대 ODE로 샘플링을 고려할 때 발생하며 [3]은 이 작업에 제시된 목표를 최소화하면 SDE에 대한 KL을 제어하지만 ODE에는 조절하지 않는다는 것을 보여준다. 점수 기반 확산 모델을 사용한 SDE 기반 샘플링에 대한 오류 결합은 [13, 14, 37, 38]에서 연구되었다. ODE의 오류 결합도 [7, 15], [1]에 제공된 와세르스타인 결합 외에 탐색된다.\n' +
      '\n' +
      '다른 관련 작품들은 훈련 중 소음 및 데이터가 어떻게 샘플링되는지 변화시켜 개선한다[64, 50]]. 운송 비용 및 구배 분산을 줄이기 위해 가우시안 및 데이터 분포 사이의 분쟁 미니 배치 최적 결합, [4] 대신 컨디셔닝 변수에서 초해상도 및 인포팅과 같은 이미지 조건 작업을 위한 데이터로 직접 흐르는 방식으로 결합을 구축한다. 마지막으로 다양한 작업은 2개의 임의의 분포[41, 18, 56, 49]를 연결하는 확률교 학습을 고려한다. 이러한 방향은 우리의 조사와 호환되며, 그들은 모델 매개변수화, 보간 일정 및 샘플링 알고리즘의 선택을 변경할 수 있는 학습 문제를 지정한다.\n' +
      '\n' +
      '잠복 공간(65, 51])의 생성 모델링은 고차원 데이터를 모델링하기 위한 관능적 접근법이다. 속도 훈련된 모델에 대해 아직 탐구되고 유망한 응용 영역인 비디오 생성 [8]에 영상을 넘어 접근법을 적용했다. 전 훈련된 스테이블 디확산 VAE의 잠재 공간에서의 발소 열차 속도 모델이다. 그들은 최종 FID-50K가 4.46인 DiT-B 백본에 대한 유망한 결과를 보여주는데, 그들의 연구는 이러한 모델의 어떤 측면이 DiT보다 성능의 이익에 기여하는지에 관한 이 작업에서 조사에 대한 하나의 동기였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline _Interpolant_ & _Model_ & _Objective_ & \\(w_{t}=\\beta_{t}\\) & \\(w_{t}=\\sigma_{t}\\) & \\(w_{t}=\\sin^{2}(\\pi t)\\) \\\\ \\hline SBDM-VP & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & 37.8 & 38.7 & 39.2 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}_{\\mathrm{x}}}\\) & 35.7 & 37.1 & 37.7 \\\\ \\hline GVP & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & **32.9** & 33.4 & 33.6 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & 38.0 & 33.5 & 33.2 \\\\ \\hline Linear & velocity & \\(\\mathcal{L}_{\\mathrm{v}}\\) & 33.6 & 33.5 & 33.3 \\\\  & score & \\(\\mathcal{L}_{\\mathrm{s}}\\) & 41.0 & 35.3 & 34.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: ** 우리의 SDE 샘플러의 평가*** 표의 모든 결과는 400K 훈련 단계에서 SiT-B 모델에 의해 생성된 FID-50K 점수이다. 마지막 세 열은 Tab 2에 자세히 설명된 서로 다른 확산 계수 \\(w_{t}\\)를 명시하여 스코어를 학습할 때 SBDM-VP 경쟁력을 갖도록 하기 위해 아래 발언(7)에 따라 부록 A.4에서 주어진 가중 점수를 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Class-Conditional ImageNet 256x256** & & & & & \\\\ \\hline Model & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & IS\\(\\uparrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) \\\\ \\hline BigGAN-deep[10] & 6.95 & 7.36 & 171.4 & **0.87** & 0.28 \\\\ StyleGAN-XL[55] & 2.30 & **4.02** & 265.12 & 0.78 & 0.53 \\\\ \\hline Mask-GIT[12] & 6.18 & - & 182.1 & - & - \\\\ \\hline ADM[19] & 10.94 & 6.02 & 100.98 & 0.69 & 0.63 \\\\ ADM-G, ADM-U & 3.94 & 6.14 & 215.84 & 0.83 & 0.53 \\\\ \\hline CDM[26] & 4.88 & - & 158.71 & - & - \\\\ \\hline RIN[30] & 3.42 & - & 182.0 & - & - \\\\ \\hline Simple Diffusion(U-Net)[27] & 3.76 & - & 171.6 & - & - \\\\ Simple Diffusion(U-ViT, L) & 2.77 & - & 211.8 & - & - \\\\ \\hline VDM++[25] & 2.12 & - & 267.7 & - & - \\\\ \\hline DiT-XL(cfg = 1.5)[48] & 2.27 & 4.60 & 278.24 & 0.83 & 0.57 \\\\ \\hline\n' +
      '0.81&0.60 \\\\SiT-XL(cfg = 1.5, ODE)** & 2.15 & 0.09 & 0.60 \\\\-SiT-XL(cfg = 1.5, ODE)** 및 2.15 & 0.60 \\\\ 0.09 & 0.60 \\\\-XL(cfg = 1.5, ODE)*** 및 2.15 & 0.09 & 0.60 \\\\.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 & 0.09 및 0.60 \\SiT-XL(cfg = 1.60\n' +
      '**SiT-XL(cfg = 1.5, SDE:\\(\\sigma_{t}\\))** & **2.06** & 4.50 & 270.27 & 0.82 & 0.59 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **Bench마크 클래스-조건 이미지 생성은 ImageNet 256x256.** SiT-XL에서 샘플러, ODE 또는 SDE 기반일 때 FID에서 DiT-XL을 능가한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '이 작업에서 이미지 생성 작업을 위한 간단하고 강력한 프레임워크인 스칼러블 인터폴란트 트랜스포머를 제시했다. 프레임워크 내에서 연속 또는 이산 시간 모델의 선택, 보간제의 선택, 모델 예측의 선택 및 확산 계수의 선택이라는 다수의 주요 설계 선택 사이의 트레이드오프를 조사했다. 우리는 각 선택의 장점과 단점을 강조했고, 얼마나 신중한 결정이 상당한 성과 개선으로 이어질 수 있는지 보여주었다. 많은 동시 작품[23, 31, 40, 45]은 매우 다양한 하류 작업에서 유사한 접근법을 탐색하고 미래의 작업에 대해 SiT를 이러한 작업에 적용하는 것을 맡기게 된다.\n' +
      '\n' +
      '아키스토우스를 통해 애드리샤 이이어, 세이 샤이타 아쿨라, 프레드 루, 지타오 구, 에드윈 파이어 게버에게 도움이 되는 논의와 피드백에 감사드린다. 이 연구는 부분적으로 구글 TRC 프로그램에 의해 뒷받침된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michael S Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _ICLR_, 2023.\n' +
      '* [2] Michael S Albergo, Nicholas M Boffi, Michael Lindsey, and Eric Vanden-Eijnden. Multimarginal generative modeling with stochastic interpolants. _arXiv preprint arXiv:2310.03695_, 2023.\n' +
      '* [3] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023.\n' +
      '* [4] Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Rajesh Ranganath, and Eric Vanden-Eijnden. Stochastic interpolants with data-dependent couplings. _arXiv preprint arXiv:2310.03725_, 2023.\n' +
      '* [5] Brian D.O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 1982.\n' +
      '* [6] Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos, Aditya Grover, Maximilian Nickel, Ricky TQ Chen, and Yaron Lipman. Matching normalizing flows and probability paths on manifolds. In _ICML_, 2022.\n' +
      '* [7] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. _arXiv preprint arXiv:2305.16860_, 2023.\n' +
      '* [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, 2023.\n' +
      '* [9] Nicholas M Boffi and Eric Vanden-Eijnden. Deep learning probability flows and entropy production rates in active matter. _arXiv preprint arXiv:2309.12991_, 2023.\n' +
      '* [10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In _ICLR_, 2019.\n' +
      '* [11] Abel Chandra, Laura Tunnermann, Tommy Lofstedt, and Regina Gratz. Transformer-based deep learning for predicting protein properties in the life sciences. _Elife_, 12:e82819, 2023.\n' +
      '* [12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In _CVPR_, 2022.\n' +
      '* [13] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _ICML_, 2023.\n' +
      '* [14] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _ICLR_, 2023.\n' +
      '* [15] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers. In _ICML_, 2023.\n' +
      '* [16] Ting Chen. On the importance of noise scheduling for diffusion models. _arXiv preprint arXiv:2301.10972_, 2023.\n' +
      '* [17] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow matching in latent space. _arXiv preprint arXiv:2307.08698_, 2023.\n' +
      '* [18] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. In _NeurIPS_, 2021.\n' +
      '* [19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In _NeurIPS_, 2021.\n' +
      '* [20] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-based generative modeling with critically-damped langevin diffusion. In _ICLR_, 2022.\n' +
      '* [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [22] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. _arXiv preprint arXiv:2303.14389_, 2023.\n' +
      '* [23] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. _arXiv preprint arXiv:2312.06662_, 2023.\n' +
      '* [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_, 2020.\n' +
      '* [26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. _arXiv preprint arXiv:2106.15282_, 2021.\n' +
      '* [27] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In _ICML_, 2023.\n' +
      '* [28] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _JMLR_, 2005.\n' +
      '***[29] 아포 히바린넨. 파세 코드 수축: 최대 우도 추정에 의한 비 가우시안 데이터의 분해: 최대 우도 추정에 의한 비 가우시안 데이터의 수축이다. 신경 컴퓨팅_ 1999.\n' +
      '* [30] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In _ICML_, 2023.\n' +
      '* [31] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3D: Learning articulated 3d animals by distilling 2d diffusion. In _3DV_, 2024.\n' +
      '* [32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _NeurIPS_, 2022.\n' +
      '* [33] Patrick Kidger. _On Neural Differential Equations_. PhD thesis, University of Oxford, 2021.\n' +
      '* [34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.\n' +
      '* [35] Diederik P Kingma and Ruiqi Gao. Understanding the diffusion objective as a weighted integral of elbos. _arXiv preprint arXiv:2303.00848_, 2023.\n' +
      '* [36] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In _NeurIPS_, 2021.\n' +
      '* [37] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In _NeurIPS_, 2022.\n' +
      '* [38] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _ALT_, 2023.\n' +
      '* [39] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In _ICLR_, 2023.\n' +
      '* [40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _ICCV_, 2023.\n' +
      '* [41] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: Understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_, 2022.\n' +
      '* [42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In _ICLR_, 2023.\n' +
      '* [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In _NeurIPS_, 2022.\n' +
      '* [45] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_, 2022.\n' +
      '* [46] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _ICML_, 2021.\n' +
      '* [47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image Transformer. In _ICML_, 2018.\n' +
      '* [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _ICCV_, 2023.\n' +
      '* [49] Stefano Peluchetti. Non-denoising forward-time diffusions. In _ICLR_, 2022.\n' +
      '* [50] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky T. Q. Chen. Multisample flow matching: Straightening flows with minibatch couplings. In _ICML_, 2023.\n' +
      '* [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.\n' +
      '* [53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 2015.\n' +
      '* [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _ICLR_, 2022.\n' +
      '* [55] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In _SIGGRAPH_, 2022.\n' +
      '* [56] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. In _NeurIPS_, 2023.\n' +
      '* [57] Eero P. Simoncelli and Edward H. Adelson. Noise removal via bayesian wavelet coring. In _ICIP_, 1996.\n' +
      '* [58] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath. Where to diffuse, how to diffuse, and how to get back: Automated learning for multivariate diffusions. In _ICLR_, 2023.\n' +
      '* [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [60] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _ICLR_, 2021.\n' +
      '* [61] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In _NeurIPS_, 2021.\n' +
      '* [62] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _ICLR_, 2021.\n' +
      '* [63] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In _ICML_, 2023.\n' +
      '* [64] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.\n' +
      '* [65] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _NeurIPS_, 2021.\n' +
      '* [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [67] Ingrid von Glehn, James S. Spencer, and David Pfau. A Self-Attention Ansatz for Ab-initio Quantum Chemistry. In _ICLR_, 2023.\n' +
      '\n' +
      '* [68] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. In _ACL_, 2019.\n' +
      '* [69] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudu Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big Bird: Transformers for Longer Sequences. In _NeurIPS_, 2020.\n' +
      '* [70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. _arXiv preprint arXiv:2306.09305_, 2023.\n' +
      '* [71] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation for diffusion odes. In _ICML_, 2023.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '### Proofs\n' +
      '\n' +
      '아래 모든 증명에서 우리는 점 제품에 \\(\\cdot\\)를 사용하고 모든 과감한 미션(\\(\\mathbf{x}\\), \\(\\mathbf{\\varepsilon}\\)을 가정하며 \\(\\mathbb{R}^{d}\\)에서 실제 평가 벡터이다. 대부분의 증명은 알베르고 등[3]에서 파생된다.\n' +
      '\n' +
      'Eq의 속도를 갖는 확률 흐름 ODE(2)의### Proof. >3개.\n' +
      '\n' +
      'Eq에서 정의한 \\(\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}\\mathbf{x}_{*}+\\sigma_{t}\\mathbf{\\varepsilon}\\)의 시간 의존적 확률 밀도 함수(PDF) \\(p_{t}(p_{t},\\mathbf{t})\\-의존 확률 밀도 함수(p_{t},\\mathbf{t},\\mathbf{t})\\-의존 확률 밀도 함수(p_{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf{t}:\\mathbf (1) 정의에 따라\\(\\hat{p}_{t}}^mathbf{k}}\\mathbf{x})가 제공합니다.\n' +
      '\n' +
      '\\[\\hat{p}_{t}(\\mathbf{k})=\\mathbb{E}[e^{i\\mathbf{k}\\cdot\\mathbf{x}_{t}}] \\tag{16}\\]\n' +
      '\n' +
      'HH(\\mathbb{E}\\)는 \\(\\mathbf{x}_{*}\\) 및 \\(\\mathbf{\\varepsilon}\\)에 대한 기대를 나타낸다. 양측에서 시간 유도체를 취하며 조건부 기대의 타워 속성을 이용하여 양면의 시간 유도체를 취하고 있다.\n' +
      '\n' +
      '}}{dath{}\n' +
      '\n' +
      '{t}[\\mathbf{x}} <\\mathbf{t} <\\mathbf{t}]]\\mathbf{t}(\\mathbf{t} <\\mathb{t} <\\mathb{t} <\\mathb{t} <\\mathb{t} <\\math{f}>{t} <\\mathb{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\mathbf{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t} <\\math{f{t}>{t} <\\math{f{t} <\\math{f{t} <\\math{f{t}>{t} <\\math{f{t} <\\m >3개. 예외적으로 Eq. (21) 읽기 읽기 읽기 읽기 판독(21)(21)\n' +
      '\n' +
      '}}.\n' +
      '\n' +
      '우리가 그것을 알고 있다.\n' +
      '\n' +
      '}\\math{d}\\math{d}.\n' +
      '\n' +
      'r\\(\\nabla_{\\mathbf{x}}\\cdot[\\mathbf{v}p_{t}]=\\sum_{i =1}^{d}\\frac{TPd}{ \\frac{\\ial}{ \\frac{i}}}[v_{i}p_{t}]\\)는 분기 연산자이며 두 번째 평등을 얻기 위해 부품에 의한 통합을 활용하고 분산을 사용한다. 푸리에 변환의 특성에 따라 Eq. (23)은 \\(p_{t}(\\mathbf{x})가 수송식을 만족한다는 것을 의미한다(23).\n' +
      '\n' +
      '\\[\\부분_{t}p_{t}(\\mathbf{x})+\\nabla_{\\mathbf{x}}}\\cdot(\\mathbf{v},\\mathbf{v}, t)p_{t}(\\mathbf{x}))=0.\n' +
      '\n' +
      '이 식을 특성 방법으로 제거하는 것은 확률 흐름 ODE(2)로 이어진다.\n' +
      '\n' +
      'SPSE(4)의### Proof.\n' +
      '\n' +
      '우리는 SDE(4)가 한계 밀도 \\(p_{t}(\\mathbf{x})\\(w_{t}\\geq 0\\)를 가지고 있음을 보여준다. 이를 위해 SDE에 대한 솔루션을 리콜한다.\n' +
      '\n' +
      '\\[d\\mathbf{X}_{t}=[\\mathbf{v}(\\mathbf{X}_{t},t)+\\frac{1}{2}w_{t}\\mathbf{s}( \\mathbf{X}_{t},t)]dt+\\sqrt{w_{t}}d\\bar{\\mathbf{W}}_{t}\\]\n' +
      '\n' +
      '포커-플란크 방정식에 만족하는 PDF가 PDF이다.\n' +
      '\n' +
      '}\\mathbf{x} (\\mathbf{x})\\mathbf{v}(\\mathbf{x})+\\mathbf{x}(\\mathbf{x})\n' +
      '\n' +
      'r\\(\\Delta_{\\mathbf{x}})는 \\(\\Delta_{\\mathbf{x}=\\nabla_{\\mathbf{x}}},\\nabla_{\\mathbf{x}}},\\cdot\\natha_{\\mathbf{x}},\\cdot\\natha_{\\mathbf{x}}},\\cdot\\nathbf{mathbf{mathbf{mathbf{mathbf{mathbf{x}}}},\\cdot\\nathbf{mathbf{mathbf{matha_{mathbf{x}},\\cot\\natha_{mathbf{x}:{matha_{mathbf{x}_{mathbf{x}:{matha_{mathbf{x} <{mathbf{x}_{mathbf{x }}}(\\mathbf{x}) 식(\\mathbf{x})의 정의는\\nathbf{s}(\\mathbf{x},\\mathbf{x})=p_{t}(\\mathbf{x})를 가지고 있다.\n' +
      '\n' +
      '\\[\\mathbf{x}) =\\[\\mathbf{t}(\\mathbf{t}_{t}-\\underbabla_{\\mathbf{x}}}\\cdot[\\mathbf{v}(\\mathbf{v},t){f{v}(\\mathbf{x},\\mathbf{x})}. \\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},\\mathbf{v},t{x},t:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}:\\mathbf{v}} Lplace 연산자의 정의에 의해, 마지막 방정식은 모든 \\(w_{t}\\geq 0\\)에 대해 유지된다. i\\(w_{t}=0\\) 때, Fokker-Planck 방정식은 연속 방정식으로 감소하며, SDE는 ODE로 감소하므로 연결은 3배 증가한다.\n' +
      '\n' +
      'Eq의 점수에 대한 발현의### Proof. (5)\n' +
      '\n' +
      '우리는 \\(\\mathbf{s}(\\mathbf{x},t)=-\\sigma_{t}^{t}^{bb{E}[\\boldsbol{\\varepsilon}| \\mathbf{x}_{t}=\\mathbf{x}]\\)를 보여준다. 우리는\\(\\hat{f}\\mathbf{k},\\mathbf{k)를 포함한다.\n' +
      '\n' +
      '}\\nathbf}}\\mathbf{k}\\mathbf{k}\\mathbol{k}\\mathbf{k}\n' +
      '\n' +
      '\\(히스볼{\\varepsilon}\\sim\\mathsf{I},0,\\mathbf{I})를 얻기 위해 명시적으로 기대를 계산할 수 있다.\n' +
      '\n' +
      '}}\\nath{k}\\math{f}}^{t}^{f{.\n' +
      '\n' +
      '\\(\\mathbf{x}_{*}\\) 및 \\(\\bathbol{\\varepsilon}\\)는 독립적인 확률 변수이기 때문에 우리는 가지고 있다.\n' +
      '\n' +
      '}}{dath{f}.\n' +
      '\n' +
      'H\\(\\hat{p}_{t}(\\mathbf{k})\\)는 Eq에서 정의된 \\(\\mathbf{x}_{t}=\\alpha_{t}\\mathbf{x}_{*}_{*}+\\sigma_{t}\\boldsbol{\\varepsilon}\\)의 특징적인 기능이다. (16) 이 방정식의 왼손 측도 그대로 쓸 수 있다.\n' +
      '\n' +
      '}}{ml}.\n' +
      '\n' +
      '오른손측이 어디에 있든 오른손측이.\n' +
      '\n' +
      '}\\math{d}\\math{d}\\math{f}.\n' +
      '\n' +
      '세 번째 평등을 얻기 위해 부품을 통해 정리와 통합을 다시 사용한 곳, 그리고 다시 점수의 정의를 통해 마지막을 얻을 수 있었다. Eq를 비교해 보세요. (33)와 Eq. (37)은 \\(모험_{t} 헨네크 0\\)를 할 때 이를 추론한다.\n' +
      '\n' +
      '}\\mathbf{t} (\\mathpsilx})\\mathbf{x}=\\mathbf{x}]\n' +
      '\n' +
      '또한 Eq에서 \\(w_{t}\\)를 \\(\\sigma_{t}\\)로 설정한다. (4)는 (4)는 (4)는 (4)를 줄 수 있다.\n' +
      '\n' +
      '}{2}\\mathbf{x} <\\mathpsilx>} <\\mathbf{t}>\n' +
      '\n' +
      '모든 \\(t\\in[0,1]\\)에 대해. 이는 \\(\\sigma_{t}\\neq 0\\)의 제약을 우회하고 \\(t=0\\)에서 특이점을 효과적으로 제거한다.\n' +
      '\n' +
      '### Eq 프로포즈. (9)\n' +
      '\n' +
      '우리는 \\(\\mathbf{v}(\\mathbf{x},t)\\)와 \\(\\mathbf{s}(\\mathbf{x},t)\\ 사이에 간단한 연결이 존재한다는 점에 주목한다. Eq부터. (1), 우리는 (1), (1) 가지고 있다.\n' +
      '\n' +
      '}}\\math{{.\n' +
      '\n' +
      '우리가 정의된 대로\n' +
      '\n' +
      '\\[\\lambda_{t}=\\dot{\\sigma}_{t}-\\frac{\\dot{\\alpha}_{t}\\sigma_{t}}{\\alpha_{t}} \\tag{45}\\]\n' +
      '\n' +
      'Eq를 감안할 때. (44)는 \\(\\mathbf{s}\\) 측면에서 선형이며, 이를 되돌려 Eq로 이어질 것이다. (9)\n' +
      '\n' +
      'Eq를 꽂을 수도 있습니다. (44) Eq의 손실 \\(\\mathcal{L}_{\\mathbf{v}}\\)에 있다. (7)가 그것을 추론하기 위해(7)을 추론하기 위해 (7)\n' +
      '\n' +
      '}}{{.\n' +
      '\n' +
      '가중 점수 목표\\(\\mathcal{L}_{\\mathbf{s}_{1}}(\\theta)\\)를 정의한다. 이 관찰은 서로 다른 단조 가중치 함수를 가진 점수 목표가 서로 다른 모델 매개변수에 대한 손실과 일치한다는 킹마 및 가오[35]에서 이루어진 주장과 일치한다. 부록 B에서 \\(\\lambda_{t}\\)가 송 등 알에서 제안된 최대 가능성 가중치의 제곱에 해당한다는 것을 보여준다[61] 및 Vahdat 등[65].\n' +
      '\n' +
      '<##>는 Score 기반 디퓨전 B와 관련이 있다.\n' +
      '\n' +
      '송 등에 나타난 바와 같이, [62], Eq의 역시간 SDE. (10)는 (10)이고, (10)은 (10))\n' +
      '\n' +
      '}\\mathbf{d}}\\mathbf{t}}\\mathbf{t}.\n' +
      '\n' +
      '이 SDE는 Eq입니다. 특정 선택 \\(w_{t}=\\beta_{t}\\)에 대한 (4) 이를 위해 용액 \\(\\mathbf{X}_{t}\\)을 Eq에 통지한다. 고정된 것은 초기 조건 \\(\\mathbf{X}_{t=0}=\\mathbf{x}_{*}\\)에 대해 각각 주어진 평균과 분산으로 분포된 가우시안이다.\n' +
      '\n' +
      '}.\n' +
      '\n' +
      'Eq를 사용합니다. 따라서 점수 기반 확산 모델의 속도는 점수 기반 확산 모델의 속도(44)로 표현될 수 있으므로 점수 기반 확산 모델의 속도를 (44)로 나타낼 수 있다.\n' +
      '\n' +
      '}}\\math{d}\\math{2}\\math{d}\\math{.\n' +
      '\n' +
      '우리는 \\(2\\lambda_{t}\\sigma_{t}\\)가 정확하게 \\(\\beta_{t}\\)임을 알 수 있으며, \\(\\lambda_{t}\\)는 송(62]에서 제안된 최대 가능성 가중치의 제곱에 해당한다. 또한 Eq를 꽂으면요. (55) Eq로. (w_{t}=\\beta_{t}\\)가 있는 (4) Eq에 도착합니다. (51)\n' +
      '\n' +
      '속도 모델 대 잡음 모델에 대한 유용한 관찰은 속도 모델에서 모든 경로 의존적 용어(\\(\\alpha_{t}\\), \\(\\sigma_{t}\\))가 제곱 손실 내부에 있으며, 점수 모델에서 항을 빼내고(필요한 \\(\\sigma_{t}\\) 점수 매칭 손실에서 아파트(\\sigma_{t}\\) 규범에서 나와 스쿼드를 획득한다는 것을 알 수 있다. 따라서 더 안정적인 것은 보간제에 달려 있습니다. 우리는 SBDM-VP의 경우 \\(t=0\\) 근처에서 \\(\\dot{\\sigma}_{t}_{t}\\)의 송풍 행동으로 인해 \\(\\mathcal{L}_{\\mathrm{v}}\\)와 \\(\\mathcal{L}_{\\mathrm{s}_{\\mathrm{s}_{\\lambda}_{\\lambda}_{\\) 모두 불안정하다는 것을 알 수 있다.\n' +
      '\n' +
      '그러나 타브에 나와 있습니다. SBDM-VP에 대한\\(\\mathrm{L}_{\\mathrm{s}_{\\lambda}}\\)는 \\(t=0\\) 근처에 있는 \\(\\lambda_{t}\\)로 더 나은 성능을 관찰했는데, 여기서 \\(\\mathcal{L}_{\\mathrm{L}_{\\)는 단순히\\(\\dot{\\sigma}_{\\mathrm{L}_{\\mathrm{L}_{\\)는 \\(t=0\\)가 제곱된 규범 내부의 감소된 구배를 보상할 것이며, 이는 \\(\\mathcal{L}_{\\)를 사용하여 더 나은 성능을 관찰할 것이며, 이는 \\(\\mathcal{L}_{\\)는 \\(\\mathcal{L}_{\\/t=0\\)에서 비롯된 기울기 폭발을 경험하게 될 것이다. 불안정성의 원인이 \\(알파_{t}^{-1}\\)인 라인 및 GVP 인터폴란트에 대해서는 거동이 다르다(t=1\\). \\(\\mathcal{L}_{\\mathrm{v}}\\)는 \\(\\alpha_{t}^{{-1}\\)가 제곱된 규범 내부에서 취소되는 반면,\\(\\mathcal{L}_{\\mathrm{s}_{\\mathrm{s}_{\\lambda}}}}}\\)에서는 정규 외부의\\(\\lambda_{t}_{\\)에서 취소되기 때문에 안정적이다.\n' +
      '\n' +
      '도지도 C 샘플링.\n' +
      '\n' +
      '\\(p_{t})는 \\(p_{t}(\\mathbf{x}_{t}_{t}=\\alpha_{t}\\mathbf{x}_{t}\\mathbf{x}_{*}_{*}+\\sigma_{t}<\\mathbf{y}})\\)의 밀도이다. 부록 A.1에서 주어진 것과 유사한 주장을 통해 \\(p_{t}(\\mathbf{x}|\\mathbf{y})가 수송 방정식(화합물 Eq)을 만족시킨다는 것을 쉽게 알 수 있다. (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) ((24)) (24)) (((24)) (24)) (24)) (24)) ((((24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) ((((((((((((((((((24)) (24)) (24)) (24)) (24)) (24)) (24)) ((((((((((((((((((((((((24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) (24)) ((((((((((((((((((((((((((24)) (24)) (24)) (24)) (24))\n' +
      '\n' +
      '}}\\cdot(\\mathbf{v},\\mathbf{x})+\\nath_{t}(\\mathbf{y})\n' +
      '\n' +
      '그곳(화합물 Eq) (3))))((3))))((3))))))((3))))))((3))))))(((3))))))(((3))))))((((3))))))((((((3))))))(((((((((3))))))((((((((((((3))))))(((((((((((3))))((((((((((((((((((((((((((3))))))(((((((((((((((((((((3))))))(((((((((((((((((3))))))))((((((((((((3))))))))((((((((((((3))))))))))))(((((((((((3))))))))(((((((((((3))))))))))((((((((\n' +
      '\n' +
      '>\\mathb{x}\\mathf{x}\\mathf{x.\n' +
      '\n' +
      '부록 A.3 및 부록 A.4에서와 같이 점수 \\(\\mathbf{s}(\\mathbf{x},t|\\mathbf{y})=\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}| \\mathbf{y})가 제공된다는 것을 쉽게 알 수 있다. (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) ((((((((((((((((((((((((((((((((((((((((((((((5)))) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5)) (5))\n' +
      '\n' +
      't}^mathbf{x}\\mathpsilx}\\mathbf{x} <\\mathbf{y}>\n' +
      '\n' +
      '\\(\\mathbf{v},\\mathbf{x},t|\\mathbf{y})\\ 및 \\(\\mathbf{s}(\\mathbf{x},t|\\mathbf{y})\\)는 (법인 Eq)를 통해 관련이 있다. (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) (44)) ((((((((((((((((((((((((((((\n' +
      '\n' +
      '}}\\mathbf{t}}\\mathbf{x}\\mathbf{y}\n' +
      '\n' +
      'Consider now\n' +
      '\n' +
      '(\\mathlog p})\\mathf}(\\math{{f},\\math{{)\\mathf}(\\mathf})\n' +
      '\n' +
      '<p_{t}(\\mathbf{x}}\\mathbf{y}) \\mathbf{t}(\\mathbf{y})=\\mathbf{t}(\\mathbf{y})\\nathbf{t} <\\mathbf{t}} <\\mathbf{t}} <\\mathbf{t} <\\mathbf{t}} <\\mathbf{t} <\\mathbf{t}} <\\mathbf{t} <\\mathbf{t}} <\\mathbf{t}} <\\mathbf{t} <\\mathbf{t}} <\\mathbf{t}} <\\math{t}}{t}{t} <\\math{t}}{t}{t}{t} <\\math{t}} <\\math{t}}{t}{t}} <\\math{t}}{t}>{t} Eq. (\\mathbf{x}, a|\\mathbf{y}) 점수 혼합물인\\mathbf{s}(\\mathbf{x},\\mathbf{x},t)+\\mathbf{x}(1-\\mathbf{y})를 사용하는 것으로 나타났다.\n' +
      '\n' +
      '(\\mathf})\\math{x}(\\math{f},\\math{f},\\math{f},\\math{f})\\math{}.\n' +
      '\n' +
      '분류기 지침 [19]에 따라 강화 분포 \\(p_{t}(\\mathbf{x}_{t})p_{t}^{\\zeta}(\\mathbf{y}|\\mathbf{x}_{t})\\)를 샘플링하는 생성 모델을 구성하도록 할 것이다. p_{t}(\\mathbf{x})p(\\mathbf{t}}^{t}(\\mathbf{y})\\propto p_{t}^{\\o p_{t}^{\\zeta}( \\mathbf{t} \\mathbf{f{x})\\(\\mathbf{t}:\\mathbf{t}:\\mathbf{t})\\propto p_{t}.{t}^{f{t}d{t}^{t}^{t}^{f{t}^{t}^{f{t}^{t}^{t}^{f{t}^{f{t}^{t}^{f{t}^{\\zeta p_{t}^{t}^{\\zeta p_{t}^{\\zeta p_{t}^{\\zeta p_{t}^{\\mathbf{t}^{\\zeta}:^{\\ 실증적으로 Tab에서 볼 수 있듯이 분류기 무료 지침을 적용하여 상당한 성능 부스트를 관찰한다. 1, Tab. 9.\n' +
      '\n' +
      'ODE 및 SDE와 함께 실험할 수 있습니다.\n' +
      '\n' +
      '종이의 본체에서 우리는 Eq에서 ODE를 해결하기 위해 하이네이터를 사용했다. (2) 및 Eq에서 SDE를 해결하기 위한 오일러 마누야마 통합기이다. (4) 우리는 Tab의 모든 결과를 요약한다. 1, 그리고 하기 구현들을 제시한다.\n' +
      '\n' +
      '```\n' +
      '>\\math+}(\\math{d\\)\\.\n' +
      '```\n' +
      '\n' +
      '단골.\n' +
      '\n' +
      '```\n' +
      '(\\mathbf}})\\(\\mathf{t}},\\mathf{t})\\(\\mathf{t})\\(\\mathf{t},\\mathf{t})\\(\\mathf{t})\\(\\mathf{t})\\(\\mathf{t} <\\math{t},\\math{t},\\math{t}},\\math{t})\\. (\\math{_\\i})\\.\n' +
      '```\n' +
      '\n' +
      '원주 오일러-마누야마 샘플**\n' +
      '\n' +
      '속도 모델 \\(\\mathbf{v}_{\\theta}\\) 또는 점수 모델 \\(\\mathbf{s}_{\\theta}\\)를 상기 두 샘플러를 적용하는 데 사용할 수 있다. 만약 점수를 학습하면, 스코어를 학습하게 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Model & Training Steps(K) & FID\\(\\downarrow\\) & sFID\\(\\downarrow\\) & IS\\(\\uparrow\\) & Precision\\(\\uparrow\\) & Recall\\(\\uparrow\\) \\\\ \\hline SiT-S & 400 & 58.97 / 57.64 & 8.95 / 9.05 & 23.34 / 24.78 & 0.40 / 0.41 & 0.59 / 0.60 \\\\ \\hline SiT-B & 400 & 34.84 / 33.45 & 6.59 / 6.46 & 41.53 / 43.71 & 0.52 / 0.53 & 0.64 / 0.63 \\\\ \\hline SiT-L & 400 & 20.01 / 18.79 & 5.31 / 5.29 & 67.76 / 72.02 & 0.62 / 0.64 & 0.64 / 0.64 \\\\ \\hline SiT-XL & 400 & 18.04 / 17.19 & 5.17 / 5.07 & 73.90 / 76.52 & 0.63 / 0.65 & 0.64 / 0.63 \\\\ SiT-XL & 7000 & 9.35 / 8.61 & 6.38 / 6.32 & 126.06 / 131.65 & 0.67 / 0.68 & 0.68 / 0.67 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'ODE 및 SDE**에 의해 생성된 표 1: **FID-50K 점수는 모든 모델 크기에 걸쳐 ODE와 SDE 사이의 비교를 보여준다. 모든 통계는 분류기 무료 안내 없이 생산됩니다. 표의 각 셀은 [ODE 결과]/[SDE 결과]를 보여주고 있다. 모든 모델 크기에서 관찰된 SDE의 더 나은 성능은 [3]에서 주어진 결합과 일치하며 ODE는 그림과 같이 더 낮은 NFE 영역에서 이점이 있다는 점에 주목한다. \\(\\dot{\\sigma}_{t}\\), \\(\\alpha_{t}^{t}\\)에 잠재적인 수치 불안정성(\\alpha_{t}^{t}\\)이 존재하지만, 결정론적 하이틀러(\\lambda_{t}\\)을 사용하여\\(\\mathbf{v}{t}\\)은 항상 \\(\\mathbf}{t}.{t}<\\_{t}\\)에서 \\(\\bf{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}<\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\_{t}\\ 확률 샘플러의 경우 통합에서 \\(\\mathbf{v}_{\\theta}\\)와 \\(\\mathbf{s}_{\\theta}\\)를 모두 가져야 하므로 다른 하나를 얻기 위해서는 항상 하나의(학습 속도 또는 점수)에서 변환해야 한다. 이 시나리오에서 부록 A.4의 수치 문제는 \\(t=0\\) 근처에서 시간 간격을 클릭하기만 하면 피할 수 있다. 실증적으로 우리는 \\(h=0.04\\)에 의해 간격을 닫고 \\(t=0.04\\)에서 \\(0\\)까지 긴 마지막 단계를 하는 것을 발견했는데, 이는 성능에 큰 도움이 될 수 있다. 부록 E에는 샘플러 구성에 대한 상세한 요약이 제공된다.\n' +
      '\n' +
      '또한, 우리는 \\(\\mathbf{v}_{\\theta}\\)와 \\(\\mathbf{v}_{\\theta}_{\\theta}_{\\theta}\\)를 두 샘플러의 입력으로 대체하고 부록 C(\\mathbf{s}_{\\{\\zeta}_{\\i}_{\\)에 제시된\\(\\mathbf{v}_{\\bf{v}_{\\bf{\\s}_{\\bf{\\)와 \\(\\mathbf{v}_{\\theta}_{\\)와 \\(\\mathbf{\\dta}_{\\{\\ta}_{\\{\\) 및 \\(\\mathbf{\\FAFA}_{\\{\\ta}_{\\{\\ta}_{\\:\\)와 \\(\\mathbf{\\t}_{\\theta}_{\\{\\ta}_{\\{\\ta}^{\\ta}^{\\ta}^{\\ 지도는 단일 단계에서 조건부 모델과 무조건적인 모델 출력을 모두 평가해야 하므로 샘플링 시 계산 비용의 2배를 부과하게 된다.\n' +
      '\n' +
      'DDPM과 오일러-마누야마 사이의 비교는 주로 DDPM과 오일러-마누야마 샘플러 간의 성능 비교를 조사하고 보고한다. 우리는 평가 중에 오일러 샘플러의 단계 수를 DDPM과 일치하도록 250단계로 설정했다. 이러한 비교는 DDPM 방식이 폐기된 오일러의 방법에 해당하기 때문에 직접적이고 공정하게 이루어진다.\n' +
      '\n' +
      'DDIM과 Heun 간의 비교도 DiT와 모델 간의 결정론적 샘플러에 의해 생성된 성능 차이를 조사한다. 그림에서. 1, 우리는 DDIM으로 샘플링된 DiT 모델과 Heun로 샘플링된 SiT 모델 모두에 대한 FID-50K 결과를 보여준다. 우리는 DDIM이 첫 번째 주문 오일러의 방법의 폐기된 버전으로 볼 수 있는 반면, 우리는 연속 시간에 오일러의 방법과 큰 폐기 오류로 인해 SiT 모델을 샘플링하는 데 두 번째 주문 하이군의 방법을 사용할 수 있기 때문에 이것은 사과 대애플 비교가 아니라는 점에 주목한다. 그럼에도 불구하고 우리는 DDIM(250개의 샘플링 단계)과 Heun(250개의 NFE) 모두에 대해 NFE를 제어한다.\n' +
      '\n' +
      '적응적 결정성 도파관5 솔버와 제2 차수 확률형 허네플러[32]의 성능도 테스트된다. 도펀5의 경우 각각 아톨 및 rtol을 le-6 및 le-3으로 설정했으며 헤운의 경우 DDPM의 것과 일치하도록 NFE를 250으로 다시 유지한다. 두 용질 모두에서 성능 증가를 관찰하지 않으며, \\(\\zeta=1.5\\)의 CFG 규모 하에서 도파관5와 확률적 하운은 각각 2.15 및 2.07의 FID-50K를 제공한다.\n' +
      '\n' +
      '또한 우리의 모델이 확산 모델과 샘플링 증류[63, 54]에 특이적으로 조정되는 다른 샘플러[36, 44]와 호환된다는 점에 주목한다. 우리는 DDPM 모델과 사과 간 비교를 위해 작업에 그러한 방법에 대한 평가를 포함하지 않으며 잠재적 성능 개선에 대한 조사를 향후 작업에 맡긴다.\n' +
      '\n' +
      '자료 적용\n' +
      '\n' +
      '우리는 0T PyTorch 코드베이스에 이어 JAX에서 피블러와 Xie[48]4로 모델을 구현했으며 오일러 마누야마 샘플러 구현을 위해 Albergo et al. [3]5, 송 et al. [62]6 및 도쇼린 et al. [20]7을 참조했다. 헤운 샘플러의 경우 JAX 기반 수치 차등 방정식 해결 라이브러리인 확산기[33]8에서 직접 사용했다.\n' +
      '\n' +
      '유도 4: [https://github.com/페이스북리서치/DiT] (https://github.com/facebookresearch/DiT)\n' +
      '\n' +
      '폐경 5: [https://github.com/malbergo/stochastic-interpolants] (https://github.com/malbergo/oniabergo/stochastic-interpolants)\n' +
      '\n' +
      '폐경 6: [https://github.com/yang-송/스코어_sde](https://github.com/yang-송/score_sde)\n' +
      '\n' +
      '폐경 7: [https://github.com/nv-talabs/CLD-SGM] (https://github.com/nv-talabs/CLD-SGM)\n' +
      '\n' +
      '우리는 0T[48]에서 유지된 동일한 구조 및 하이퍼모수 후에 모든 모델을 훈련했다: [https://github.com/frick-kidger/diffrax]. 모든 모델의 최적지로 AdamW[34, 43]를 사용했다. 우리는 \\(1\\t10^{-4}\\)의 일정한 학습률과 \\(256\\)의 배치 크기를 사용한다. 데이터 증강에서 \\(0.5\\) 확률로 무작위 수평 플립을 사용했다. 학습률, 붕괴/지연 일정, AdamW 매개변수, 학습_ 동안 추가 데이터 증강 또는 구배 클램핑을 사용하지 않았다. 우리의 가장 큰 모델인 SiT-XL은 위의 구성에 따라 TPU v4-64 팟에서 약 \\(6.8\\) 계층/sec의 열차이다. 이 속도는 동일한 설정 하에서 \\(~{}6.4\\) 계층/sec에서 훈련하는 DiT-XL에 비해 약간 더 빠르다. 또한 다른 모델 크기의 훈련 속도를 수집하고 아래를 요약합니다.\n' +
      '\n' +
      '샘플링 실시예들은 \\(0.9999\\)의 붕괴로 훈련을 통해 모든 모델 가중치들의 지수 이동 평균(EMA)을 유지한다. 모든 결과는 EMA 체크포인트에서 샘플링되며, 이는 더 나은 성능을 생성하는 것으로 경험적으로 관찰된다. 우리는 각 \\(t_{0}\\) 및 \\(t_{N}\\)가 성능을 최적화하고 통합 중에 수치 불안정성을 피하기 위해 조심스럽게 조정된 아래 서로 다른 보간체를 갖는 결정론적 및 확률적 샘플러의 시작과 종점을 요약한다.\n' +
      '\n' +
      'FID 계산은 생성된 이미지(10K 또는 50K)와 이미지넷 훈련 데이터셋에서 사용 가능한 모든 실제 이미지 사이의 FID 점수를 계산한다. TPU 기반 FID 평가와 GPU 기반 FID 평가(ADM의 TensorFlow 평가 스위트[19]9) 간의 작은 성능 변화를 관찰한다. 바이너인 DiT와의 일관성을 보장하기 위해 GPU에서 모든 모델을 샘플링하고 ADM 평가 스위트룸을 사용하여 FID 점수를 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & S & B & L & XL \\\\ \\hline DiT & 20.0 & 19.8 & 9.3 & 6.4 \\\\ \\hline SiT & 19.7 & 20.8 & 9.3 & 6.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '모든 모델 크기에 걸쳐 ** 훈련 속도(액터/sec)는 TPU v4-64 족에서 측정되며,** 모든 훈련 속도는 TPU v4-64 족에서 측정된다. 우리는 훈련 속도가 하드웨어 상태에 의해 크게 영향을 받는다는 점에 주목한다.___\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline _Interpolant_ & _Model_ & _Objective_ & \\multicolumn{2}{c}{Heun} & \\multicolumn{2}{c}{Euler-Maruyama} \\\\  & & & \\(t_{0}\\) & \\(t_{N}\\) & \\(t_{0}\\) & \\(t_{N}\\) \\\\ \\hline SBDM-VP & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 1e-5 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}_{\\lambda}}\\) & 1 & 1e-5 & 1 & 4e-2 \\\\ \\hline GVP & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 0 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}}\\) & 1 - 1e-5 & 0 & 1 - 1e-3 & 4e-2 \\\\ \\hline Linear & velocity & \\(\\mathcal{L}_{\\text{v}}\\) & 1 & 0 & 1 & 4e-2 \\\\  & score & \\(\\mathcal{L}_{\\text{s}}\\) & 1 - 1e-5 & 0 & 1 - 1e-3 & 4e-2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 삼플러 구성표 3: 삼플러 구성표 3:\n' +
      '\n' +
      '## 부록 F 추가 평가 결과는 그림 4: ** 미결(512\\) SiT-XL 샘플***이다. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "로그헤드 거북이"(33) 그림 5: ** 미완료(512\\) SiT-XL 샘플***. 분류자가 없는 지도 척도 = 4.0클래스 레이블 = "풍선"(417)이다.\n' +
      '\n' +
      '그림 6: ** 폴리(512\\) SIT-XL 샘플***. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "붉은 판다"(387) 그림 7: ** 미공개(512\\·512\\) SIT-XL 샘플***이다. 분류기 없는 지도 척도 = 4.0클래스 라벨 = ‘게이저’(974)이다.\n' +
      '\n' +
      '그림 8: ** 폴리(256\\, 256\\) SIT-XL 샘플***. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "마카와" = (88) 그림 9: ** 미공개(256\\·256\\) SIT-XL 샘플***. 분류기 없는 지도 척도 = 4.0클래스 레이블 = ‘골든 리트리버’(207)이다.\n' +
      '\n' +
      '그림 10: ** 정제되지 않은\\(256\\) SIT-XL 샘플. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "이스 크림" = (928)*** 그림 11: ** 미공개 (256\\ 55\\) SIT-XL 샘플이다. 고전기 없는 지도 척도 = 분류기 없는 지도 척도 = 4.0클래스 라벨 = 4.0클래스 레이블=(972)***** <클립> = 4.0클래스 라벨이다.\n' +
      '\n' +
      '그림 12: ** 정제되지 않은\\(256\\) SIT-XL 샘플. 분류자가 없는 지도 척도 = 4.0클래스 라벨 = "후스키"(250)** 그림 13: ** 미공개(256\\·256\\) SIT-XL 샘플이다. 분류기 없는 지도 척도 = 분류기 없는 지도 척도 = 4.0클래스 라벨 = "발리" = (979)****=" 4.0클래스 라벨 = "발리"이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>