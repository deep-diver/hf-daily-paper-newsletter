<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#소라는 놀라운 기하학적 일관성을 가진 비디오를 생성한다.\n' +
      '\n' +
      'Xuanyi LI\n' +
      '\n' +
      'VCIP, Nankai University, Tianjin, 300350, China;\n' +
      '\n' +
      'Daquan ZHOU\n' +
      '\n' +
      'ByteDance Inc, Singapore;\n' +
      '\n' +
      'Chenxu ZHANG\n' +
      '\n' +
      'ByteDance Inc, Singapore;\n' +
      '\n' +
      'Shaodong WEI\n' +
      '\n' +
      'W우한대학교, 중국 우한 430079;\n' +
      '\n' +
      'Qibin HOU\n' +
      '\n' +
      'zhoudaguan21@gmail.com Project page: [https://sora-geometrical-consistence.github.io/](https://sora-geometrical-consistence.github.io/);\n' +
      '\n' +
      '처음 두 명의 저자는 동등하게 기여했다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근에 개발된 소라 모델 [1]은 비디오 생성에서 놀라운 능력을 보여 실제 현상을 시뮬레이션하는 능력과 관련하여 격렬한 토론을 불러일으켰다. 인기가 증가하고 있음에도 불구하고 실제 물리학에 대한 충실도를 정량적으로 평가할 수 있는 확립된 메트릭이 부족하다. 본 논문에서는 실제 물리 원리를 준수하여 생성된 비디오의 품질을 평가하는 새로운 벤치마크를 소개한다. 생성된 비디오를 3D 모델로 변환하는 방법을 사용하여 3D 재구성의 정확도가 비디오 품질에 크게 좌우된다는 전제를 활용한다. 3D 재구성의 관점에서, 생성된 비디오가 실제 물리 규칙을 준수하는 정도를 측정하기 위해 구축된 3D 모델이 만족하는 기하학적 제약의 충실도를 프록시로 사용한다.\n' +
      '\n' +
      '그림 1: 소라, 피카 및 Gen2 간의 비교. (a)는 Sec. 2에 정의된 5가지 메트릭에 걸친 정량적 평가를 보여준다. 더 자세한 내용은 독자가 탭 1을 참조할 수 있다. (b)는 우리가 설계한 지속 안정성 메트릭에 따라 다양한 방법의 성능을 제시한다. 두 그림 모두 기하학 일관성 측면에서 다른 기준선에 비해 소라의 상당한 이점을 볼 수 있다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '이미지 생성의 실질적인 성과를 바탕으로 텍스트-비디오 합성(Text-to-Video Synthesis, T2V) 분야는 생성 모델의 적용을 위한 새로운 프론티어로 확인되었다. 비디오 생성은 비디오 프레임에 걸쳐 일관된 공간적, 시간적 관계를 유지해야 하는 복잡한 요구로 인해 이미지 합성에 대한 진화를 필요로 한다. 이 복잡한 프로세스는 간략하고 종종 추상적인 비디오 캡션과 관련된 해석적 도전과 고품질 주석이 달린 비디오 텍스트 데이터 세트의 제한된 가용성으로 인해 더욱 복잡해진다.\n' +
      '\n' +
      '확산 모델의 발전으로 비디오 생성이 가능하게 된 새로운 시대가 도래하여 비디오 확산 모델 [2] 및 Imagen 비디오 [3]과 같은 독특한 프레임워크가 강조되었다. 이러한 선구적인 연구는 공간과 시간 모두에서 동영상의 일관된 확장을 촉진하는 혁신적인 조건부 샘플링 방법으로 길을 열었다. 이 영역에서 주목할 만한 진화는 매직비디오[4]의 도입이며, 이는 비디오 LDM[5]에 의해 더욱 완성된 방법론인 컴팩트한 잠재 공간 내에서 비디오 세그먼트를 합성함으로써 생성 프로세스를 실질적으로 개선했다. 개선된 교차 주의 메커니즘을 통해 텍스트 정렬 비디오 콘텐츠의 생성을 강화하는 MaskDiffusion[6]의 개발로 개선이 계속되고 있다. 이러한 발전에도 불구하고, 프레임 충실도, 모션 조화 및 텍스트-비디오 합동을 중심으로 한 종래의 메트릭은 결과적인 비디오의 기하학적 품질을 캡처하는 데 부족한다.\n' +
      '\n' +
      '최근 소라 모델[1]의 도입은 비디오 생성에서 역작임을 보여주었으며, 공간적 및 시간적 벡터 모두를 따라 뚜렷한 사실성과 일관되고 논리적인 콘텐츠를 가진 비디오를 생산하는 탁월한 능력으로 광범위한 표창을 얻었다. 소라 모델의 성과는 T2V 기술에서 중요한 진전이 이루어졌음을 나타내며, 생성 모델의 영역에서 지속적인 혁신의 중요성을 재확인한다. 공개된 비디오 클립은 런웨이의 SVD[7], Pika Labs[8], Gen-2[9]와 같은 이 분야의 이전 리더들과 비교했을 때 눈에 띄는 품질 향상을 보여준다. 소라의 두드러진 측면은 물리적 합리성이다: 때때로 잘못된 조치에도 불구하고, 많은 생성된 클립은 물리적 법칙을 준수하고 이전 모델에 의해 적절하게 캡처되지 않은 주목할만한 기하학적 특성을 유지한다.\n' +
      '\n' +
      '더욱이, 비디오 생성을 평가하기 위한 종래의 메트릭들, 예를 들어 FID(Frechet Inception Distance) [10], FVD(Frechet Video Distance) [11], IS(Inception Score) [12], 및 Aesthetic Score[13] 등이 명확하다. 특히 기하학적 측면에서 물리적 정확성의 차원을 포함하지 않는다. 이를 위해 영상 생성 품질 평가를 위해 3D 객체 품질 접근에 메트릭을 활용하는 것을 고려한다. 기존의 3차원 복원은 크게 SFM(Structural-from-Motion)과 MVS(Multi-View Stereo)의 두 가지 구성 요소로 구성된다. COLMAP[14] 및 OpenMVG[15]와 같은 도구로 표현되는 SFM은 희소 매칭 및 번들 조정을 활용하여 카메라 포즈 및 희소 포인트 클라우드를 추정한다. OpenMVS [16]과 같은 라이브러리로 예시된 MVS는 SFM의 결과를 입력으로 하고 3D 모델을 생성하기 위해 밀집 매칭, 메쉬 재구성 및 텍스처 매핑을 수행한다. 최근에는 Neural Radiance Fields(Nerf)[17] 및 3D Gaussian Splatting[18]과 같은 딥러닝 기반의 3D 복원 방법들이 등장하고 있다. 이러한 방법은 다양한 재료, 투명한 물체 및 수중 환경을 처리하는 데 탁월하여 렌더링 품질을 높이고 복잡한 장면을 미세하게 재구성할 수 있다.\n' +
      '\n' +
      '본 논문은 소라의 생성 동영상으로부터 유도된 3차원 객체 재구성의 품질을 기하학의 맥락에서 물리적 원리와의 정렬을 정량적으로 평가하기 위한 메트릭으로 활용하는 것을 탐구한다. 구체적으로, 우리는 소라가 만든 10개의 대표적인 동영상 클립을 인터넷상에서 공개적으로 수집한다. 그런 다음 피카 랩 [8] 및 Gen-2 [9]를 사용하여 동일한 텍스트 프롬프트가 있는 비디오를 생성합니다. 놀랍게도, Fig. 1, 우리는 소라로 생성된 비디오가 강력한 기준선을 통해 선택된 모든 메트릭에 걸쳐 상당한 이점이 있는 3D 재구성에 충분하다는 것을 관찰한다. 우리는 이 간단한 벤치마크가 비디오 생성 모델이 물리적 세계를 얼마나 잘 볼 수 있는지 확인하는 데 도움이 될 수 있기를 바란다.\n' +
      '\n' +
      '## 2 Method\n' +
      '\n' +
      '3D 재구성 과정.생성된 비디오의 특성을 수용할 수 있도록 원래의 COLMAP[14]과 Gaussian Splatting[18] 알고리즘을 수정하는 것을 삼간다. SfM[14]을 이용하여 카메라 포즈를 계산한 후 Gaussian Splatting을 이용하여 3차원 복원을 수행한다. 이 벤치마크에서 사용되는 세부 메트릭은 다음과 같다.\n' +
      '\n' +
      '계량 설계.SFM(Structure from Motion) [14] 및 3D 구축의 기본 원리는 멀티 뷰 기하학이며, 이는 모델의 품질이 두 가지 주요 요인에 의존한다는 것을 의미한다: 1) 가상 비디오의 관찰 카메라의 관점은 핀홀 카메라와 같은 물리적 특성을 충분히 충족해야 하며, 2) 비디오가 진행되고 관점이 변경됨에 따라 장면의 강성 부분은 물리적 및 기하학적 안정성을 유지하는 방식으로 변화해야 한다.\n' +
      '\n' +
      '또한, 다시점 기하학의 기본 단위는 2시점 기하학이다. AI 생성 비디오의 물리적 충실도가 높을수록 두 프레임이 에피폴라 기하학과 같은 이상적인 2시점 기하 제약 조건을 준수해야 한다. 구체적으로, 시퀀스 비디오에서 가상 시점들의 카메라 이미징이 더 이상적일수록, 장면의 물리적 특성들이 이미지들에서 더 충실하게 보존된다. 두 프레임이 이상적인 두 시점 기하학에 가까울수록, 그레이스케일 및 형상 측면에서 로컬 특징의 왜곡 및 뒤틀림이 작을수록 매칭 알고리즘에 의해 더 많은 매칭 포인트를 얻을 수 있다. 결과적으로, RANSAC[19] 이후에 더 많은 수의 고품질 매칭 포인트들이 유지된다. 따라서, 우리는 AI가 생성한 비디오에서 일정한 간격으로 두 개의 프레임을 추출하여 두 개의 뷰 이미지 쌍을 생성한다. 각 쌍에 대해 매칭 알고리즘을 사용하여 대응점을 찾고 기본 행렬(에피폴라 제약 조건)을 기반으로 RANSAC을 사용하여 잘못된 대응점을 제거한다.\n' +
      '\n' +
      '제거 후, 우리는 평균 정확한 초기 매칭 포인트 수, 평균 보유 포인트 수 및 평균 보유 비율을 계산한다. 따라서, 우리는 다음과 같은 메트릭을 갖는다: num_pts는 양안시에서의 초기 매칭 포인트들의 총 수를 지칭하고, num_inliers_F는 필터링 후에 보유된 매칭 포인트들의 총 수를 지칭한다. keep_ratio는 num_inliers_F 대 num_pts의 비율에 의해 구해진다. 또한, 각 영상 쌍에 대해 \\(F\\) 행렬의 점당 \\(N\\) 매칭점, \\(d(x,x^{\\prime})\\)에 대한 양방향 기하학적 재투영 오차를 계산한다. \\(x,x^{\\prime})\\ (x\\)와 \\(x^{\\prime}\\)은 RANSAC 후에 유지되는 정합점이고 \\(d\\)은 한 점으로부터 대응하는 에피폴라 선까지의 거리이다. 마지막으로 모든 데이터에 대한 전반적인 통계 분석을 수행하여 RMSE(Root Mean Square Error)와 MAE(Mean Absolute Error)를 계산한다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '실험에서는 보다 강건한 딥러닝 기반 매칭 알고리즘 대신 희소 매칭 모듈에 대해 기존의 알고리즘인 SIFT[20]을 선택하였다. 이 결정은 매칭 성능이 과도하게 강한 것을 방지하는 것을 목표로 하며, 이는 조명, 텍스처 및 재료 특성의 변화와 같은 이미지 품질의 근본적인 결함을 잠재적으로 마스킹할 수 있다. 유사하게, 밀집 매칭 모듈은 동일한 이유로 전통적인 SGBM 알고리즘[21]으로 구현된다. RANSAC 알고리즘은 OpenCV에서 사용할 수 있는 원본 버전을 사용하여 사용된다. 공식 홈페이지에서 직접 소라 영상을 입수합니다. 공정한 비교를 유지하기 위해 Gen2와 Pika(동일한 프롬프트 사용)의 이미지2 비디오 기능과 결합된 소라 비디오의 첫 번째 프레임을 사용하여 동일한 장면을 가진 비디오를 생성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c} \\hline \\hline Video name & Method & num\\_pts\\(\\uparrow\\) & num\\_inliers\\_F\\(\\uparrow\\) & keep\\_ratio\\(\\uparrow\\) & mean\\_error\\(\\downarrow\\) & rmse\\(\\downarrow\\) \\\\ \\hline \\hline \\multirow{3}{*}{Amalifi\\_coast} & Sora & **5441.18** & **3857.36** & **0.71** & **0.96** & **1.20** \\\\  & Gen2 & 2071.00 & 367.82 & 0.18 & 1.19 & 1.45 \\\\  & Pika & 1598.82 & 101.36 & 0.06 & 1.25 & 1.54 \\\\ \\hline \\multirow{3}{*}{Art\\_museum} & Sora & **5220.09** & **3463.91** & **0.66** & **0.86** & **1.12** \\\\  & Gen2 & 2480.36 & 987.00 & **0.40** & 1.10 & 1.35 \\\\  & Pika & 2249.91 & 527.73 & **0.23** & 1.27 & 1.53 \\\\ \\hline \\multirow{3}{*}{Big\\_sur} & Sora & **5044.55** & **3924.09** & **0.78** & **0.58** & **0.80** \\\\  & Gen2 & 1899.09 & 190.73 & 0.10 & 1.13 & 1.39 \\\\  & Pika & 1662.73 & 89.55 & 0.05 & 1.08 & 1.37 \\\\ \\hline \\multirow{3}{*}{Gold\\_rush} & Sora & **6198.55** & **4733.91** & **0.76** & **0.89** & **1.13** \\\\  & Gen2 & 1779.27 & 500.36 & 0.28 & 1.08 & 1.34 \\\\  & Pika & 1897.55 & 587.73 & 0.30 & 1.19 & 1.45 \\\\ \\hline \\multirow{3}{*}{Minecraft} & Sora & **4911.36** & **3048.18** & **0.62** & **0.92** & **1.15** \\\\  & Gen2 & 1445.82 & 201.64 & 0.14 & 1.23 & 1.48 \\\\  & Pika & 1196.91 & 91.64 & 0.08 & 0.95 & 1.25 \\\\ \\hline \\multirow{3}{*}{Santorini} & Sora & **3790.27** & **2608.00** & **0.69** & **0.99** & **1.24** \\\\  & Gen2 & 1739.91 & 365.91 & 0.21 & 1.26 & 1.52 \\\\ \\cline{1-1}  & Pika & 1287.18 & 60.27 & 0.05 & 1.07 & 1.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 6개의 소라 비디오에 대한 **정량적 비교.** 기호 \'\\(\\uparrow\\)/\\(\\downarrow\\)\'는 더 높은/낮은 점수가 더 우수함을 나타낸다. 가장 좋은 점수는 **볼드**로 표시된다.\n' +
      '\n' +
      '충실도 메트릭.샘플링 간격이 30 프레임이고 RANSAC 임계값이 3인 각 비디오의 총 프레임 수는 첫 번째 프레임부터 연속적으로 계산된다. 그 결과를 Tab. 1에 나타내었다. 그 결과는 소라가 다른 두 방법(최적의 방법)에 비해 유사한 매칭 오류를 나타내지만, 몇 배 더 정확한 매칭 포인트를 획득한다는 것을 나타낸다. 이는 생성된 이미지가 가장 진품이며 가장 높은 기하학적 일관성 품질을 가지고 있음을 시사한다.\n' +
      '\n' +
      '지속적인 안정성 메트릭.또한 프레임 샘플링 간격에 따른 앞서 언급한 \\(keep\\_ratio\\) 메트릭의 변화를 비교한다. 이 평가를 지속 안정성에 대한 평가라고 한다. 그림 1에 표시된 결과로부터. 도 1의 (b)를 참조하면, 프레임 간격이 증가함에 따라 소라는 정확한 매칭의 보존 비율이 느리게 감소하는 반면, 다른 두 방법은 급격한 감소를 나타내는 것을 관찰할 수 있다. 이것은 오랜 기간 동안 물리적, 이미징 및 기하학적 특징을 보존하는 데 있어 소라의 안정성과 일관성을 보여준다.\n' +
      '\n' +
      '시각화.먼저, SFM과 Gaussian Splatting 방법을 사용하여 3차원 재구성 과정을 시연하고, 서로 다른 접근법에 의해 생성된 포인트 클라우드와 가우시안 Splatting 재구성 결과를 보여준다. 도. 도 2는 6개의 다른 장면을 다루는 피카, 젠투, 소라가 제작한 비디오의 3D 재구성 결과를 보여준다. 그림은 점 구름과 가우스 스플래팅 시각화를 모두 보여준다. 특히, Sora가 획득한 재구성 품질은 Pika와 Gen2의 재구성 품질보다 월등히 우수하다. 이러한 성능 향상은 1) 더 많은 카메라 정보를 제공하는 더 긴 비디오를 생성하는 소라의 능력과 2) 선명하고 복잡한 3D 기하학적 구조의 재구성을 용이하게 하는 Sora 생성 비디오 내의 상이한 프레임들 간의 개선된 일관성에 기인할 수 있다. 또한 그림 3과 같이 다른 방법으로 생성된 비디오에서 얻은 희소 매칭 결과에 대한 시각적 분석을 수행하는데, 소라 방법으로 생성된 비디오는 필터링 후 가장 많은 수의 올바르게 매칭된 포인트를 나타낸다. 마지막으로, 보정된 스테레오 영상을 SGBM 매칭 알고리즘에 공급하고 그림 4와 같이 시각화를 통해 스테레오 매칭 결과의 품질을 직접 비교한다. 시각적 SGBM 스테레오 매칭 결과는 기하학적 일관성을 엄격하게 준수하는 뷰만이 SGBM 알고리즘을 통해 합리적으로 밀집된 매칭 결과를 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '도 2: 포인트 클라우드의 시각화 및 가우시안 스플래팅 렌더링. 이 그림은 피카, 젠투, 소라가 제작한 비디오의 3D 재구성 결과를 제시한다. Pika와 Gen2의 결과는 기하학적 구조와 질감이 좋지 않은 제한된 재구성 범위를 가지고 있다. 소라의 재구성 품질은 피카와 젠2의 재구성 품질을 크게 능가하는데, 이는 1) 소라가 더 긴 비디오를 생산하는 능력, 더 광범위한 카메라 정보를 제공하는 능력, 2) 소라 비디오에서 더 큰 프레임 간 일관성을 제공하여 더 명확하고 더 상세한 3D 재구성을 가능하게 하는 두 가지 주요 요인에 기인할 수 있다. (참고: Gen2의 비디오 중 하나는 카메라 원근법의 불충분한 변화로 인해 재구성될 수 없었다.)\n' +
      '\n' +
      '##4 미래 작업 토론\n' +
      '\n' +
      '소라와 같은 모델의 출현은 비디오 생성 작업에 대한 보다 정확하고 전체적인 평가 도구의 필요성을 강조했다. 본 연구는 생성된 비디오의 품질을 철저히 평가하기 위해 기하학적 특성을 조사하기 위한 3D 재구성 메트릭의 적용에 대한 초기 조사를 수행한다. 기하학을 넘어 질감 진정성, 모션 준수 및 장면 객체 간의 상호 작용 로직과 같은 추가 물리 기반 메트릭은 추가 고려를 보증한다. 현재 우리의 주요 목표는 기하학적 일관성을 평가하는 것이지만 이러한 기준의 중요성을 인정하고 향후 탐구를 위해 지정한다.\n' +
      '\n' +
      '그림 4: SGBM 스테레오 매칭 결과의 시각화는 기하학적 일관성을 엄격하게 준수하는 뷰만이 SGBM 알고리즘을 통해 합리적인 밀집 매칭 결과를 생성할 수 있음을 보여준다. 소라 방법에 의해 생성된 비디오는 가장 뛰어난 기하학적 일관성을 나타내는 것이 분명하다.\n' +
      '\n' +
      '도 3: 매칭 결과 비교. 이미지에서 녹색은 고품질 매칭 결과를 나타내고, 적색은 폐기된 매칭 결과를 나타낸다. 더 많은 녹색 고품질 일치가 존재한다는 것은 두 뷰 사이의 더 높은 수준의 기하학적 일관성을 나타낸다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] T. Brooks, B. Peebles, C. Homes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. W. Y. Ng, R. Wang, and A. Ramesh, "Video generation models as world simulators," 2024. [Online]. Available: [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)\n' +
      '* [2] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, "Video diffusion models," _arXiv:2204.03458_, 2022.\n' +
      '* [3] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet _et al._, "Imagen video: High definition video generation with diffusion models," _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [4] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng, "Magicvideo: Efficient video generation with latent diffusion models," _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [5] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, "Align your latents: High-resolution video synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 22 563-22 575.\n' +
      '* [6] Y. Zhou, D. Zhou, Z.-L. Zhu, Y. Wang, Q. Hou, and J. Feng, "Maskdiffusion: Boosting text-to-image consistency with conditional mask," _arXiv preprint arXiv:2309.04399_, 2023.\n' +
      '* [7] A. Blattmann, T. Dockhorn, S. Kulas, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts _et al._, "Stable video diffusion: Scaling latent video diffusion models to large datasets," _arXiv preprint arXiv:2311.15127_, 2023.\n' +
      '* Home," [https://pika.art/home](https://pika.art/home), 2023, access:2024-02-01.\n' +
      '* Gen2", [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2), 2023.\n' +
      '* [10] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [11] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, "Fvd: A new metric for video generation," 2019.\n' +
      '* [12] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, "Improved techniques for training gans," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [13] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 10 684-10 695.\n' +
      '* [14] J. L. Schonberger and J.-M. Frahm, "Structure-from-motion revisited," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016, pp. 4104-4113.\n' +
      '* [15] P. Moulon, P. Monasse, R. Perrot, and R. Marlet, "Openmrg: Open multiple view geometry," in _Reproducible Research in Pattern Recognition: First International Workshop, RRPR 2016, Cancun, Mexico, December 4, 2016, Revised Selected Papers 1_. Springer, 2017, pp. 60-74.\n' +
      '* [16] D. Cernea, "Openmvs: Multi-view stereo reconstruction library," _City_, vol. 5, no. 7, 2020.\n' +
      '* [17] Z. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu, "Nerf-: Neural radiance fields without known camera parameters," _arXiv preprint arXiv:2102.07064_, 2021.\n' +
      '* [18] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, "3d gaussian splatting for real-time radiance field rendering," _ACM Transactions on Graphics_, vol. 42, no. 4, 2023.\n' +
      '* [19] M. A. Fischler and R. C. Bolles, "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography," _Communications of the ACM_, vol. 24, no. 6, pp. 381-395, 1981.\n' +
      '* [20] P. C. Ng and S. Henikoff, "Sift: Predicting amino acid changes that affect protein function," _Nucleic acids research_, vol. 31, no. 13, pp. 3812-3814, 2003.\n' +
      '* [21] H. Hirschmuller, "Accurate and efficient stereo processing by semi-global matching and mutual information," in _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'05)_, vol. 2. IEEE, 2005, pp. 807-814.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>