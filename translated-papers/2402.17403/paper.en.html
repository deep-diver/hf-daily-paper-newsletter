<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Sora Generates Videos with Stunning Geometrical Consistency\n' +
      '\n' +
      'Xuanyi LI\n' +
      '\n' +
      'VCIP, Nankai University, Tianjin, 300350, China;\n' +
      '\n' +
      'Daquan ZHOU\n' +
      '\n' +
      'ByteDance Inc, Singapore;\n' +
      '\n' +
      'Chenxu ZHANG\n' +
      '\n' +
      'ByteDance Inc, Singapore;\n' +
      '\n' +
      'Shaodong WEI\n' +
      '\n' +
      'W Wuhan university, Wuhan 430079, China;\n' +
      '\n' +
      'Qibin HOU\n' +
      '\n' +
      'zhoudaguan21@gmail.com Project page: [https://sora-geometrical-consistence.github.io/](https://sora-geometrical-consistence.github.io/);\n' +
      '\n' +
      'First two authors contributed equally.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The recently developed Sora model [1] has exhibited remarkable capabilities in video generation, sparking intense discussions regarding its ability to simulate real-world phenomena. Despite its growing popularity, there is a lack of established metrics to evaluate its fidelity to real-world physics quantitatively. In this paper, we introduce a new benchmark that assesses the quality of the generated videos based on their adherence to real-world physics principles. We employ a method that transforms the generated videos into 3D models, leveraging the premise that the accuracy of 3D reconstruction is heavily contingent on the video quality. From the perspective of 3D reconstruction, we use the fidelity of the geometric constraints satisfied by the constructed 3D models as a proxy to gauge the extent to which the generated videos conform to real-world physics rules.\n' +
      '\n' +
      'Figure 1: Comparisons among Sora, Pika, and Gen2. (a) shows the quantitative evaluations across five metrics, which we define in Sec. 2. For more details, readers can refer to Tab. 1. (b) presents the performance of different methods under our designed Sustained Stability metric. For both figures, we can see a significant advantage of Sora over other baselines in terms of geometry consistency.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      'Building upon the substantial achievements in image generation, the burgeoning field of text-to-video synthesis (T2V) has been identified as the novel frontier for the application of generative models. Video generation necessitates an evolution over image synthesis due to the intricate requirement of maintaining consistent spatial and temporal relationships across video frames. This intricate process is further compounded by the interpretative challenges associated with brief and often abstract video captions, as well as the limited availability of annotated video-text datasets of high quality.\n' +
      '\n' +
      'A new era has dawned in video generation made possible by the advancements in diffusion models, highlighting distinctive frameworks such as Video Diffusion Models [2] and Imagen Video [3]. These pioneering studies have paved the way with innovative conditional sampling methods that facilitate the coherent expansion of videos in both space and time. A noteworthy evolution in this domain is the introduction of MagicVideo [4], which has substantially refined the generative process by synthesizing video segments within a compact latent space--a methodology further perfected by Video LDM [5]. Enhancements continue with the development of MaskDiffusion [6], which strengthens the generation of text-aligned video content through an improved cross-attention mechanism. Despite these advancements, conventional metrics centered on frame fidelity, motion harmony, and text-video congruence fall short in capturing the geometric quality of the resulting videos.\n' +
      '\n' +
      'The recent introduction of the Sora model [1] has shown itself to be a tour de force in video generation, garnering widespread commendation for its exceptional ability to produce videos with pronounced realism and consistent, logical content along both spatial and temporal vectors. The Sora model\'s performance is indicative of the significant strides being made in T2V technology, reaffirming the importance of continued innovation in the realm of generative models. The showcased video clips demonstrate remarkable improvements in quality when compared to the previous leaders in the field, such as SVD [7], Pika Labs [8], and Gen-2 [9] from Runway. A standout aspect of Sora is its physical rationality: Despite occasional missteps, many generated clips exhibit adherence to the physical laws and maintain notable geometric properties, which are not adequately captured by previous models.\n' +
      '\n' +
      'Furthermore, it is clear that conventional metrics for assessing video generation, such as Frechet Inception Distance (FID) [10], Frechet Video Distance (FVD) [11], Inception Score (IS) [12], and Aesthetic Score [13], etc. do not encompass the dimension of physical accuracy, especially in geometric terms. To this end, we consider utilizing the metrics in 3D object quality accessions for the evaluation of the video generation quality. Traditional 3D reconstruction consists primarily of two components: Structure-from-Motion (SFM) and Multi-View Stereo (MVS). SFM, represented by tools like COLMAP [14] and OpenMVG [15], leverages sparse matching and bundle adjustment to estimate camera poses and sparse point clouds. MVS, exemplified by libraries like OpenMVS [16], takes the results from SFM as input and performs dense matching, mesh reconstruction, and texture mapping to generate 3D models. In recent years, deep learning-based 3D reconstruction methods such as Neural Radiance Fields (Nerf) [17] and 3D Gaussian Splatting [18] have emerged. These methods excel in handling different materials, transparent objects, and underwater environments, resulting in higher rendering quality and enabling fine reconstruction of complex scenes.\n' +
      '\n' +
      'Our paper delves into utilizing the quality of 3D object reconstructions derived from Sora\'s generated videos as a metric to quantitatively evaluate their alignment with physical principles in the context of geometry. Specifically, we collect 10 representative video clips generated by Sora that are publicly available on the Internet. We then use Pika Labs [8] and Gen-2 [9] to generate videos with the same text prompts. Surprisingly, as shown in Fig. 1, we observe that the videos generated with Sora are good enough for 3D reconstruction, with significant advantages across all selected metrics over the strong baselines. We hope this simple benchmark could be helpful for video generation models to see how well they can see the physical world.\n' +
      '\n' +
      '## 2 Method\n' +
      '\n' +
      'Process of 3D reconstruction.We refrain from modifying the original COLMAP [14] and Gaussian Splatting [18] algorithms to accommodate the characteristics of the generated videos. We utilize Structure-from-Motion (SfM) [14] to compute camera poses and then employ Gaussian Splatting for 3D reconstruction. The detailed metrics used in this benchmark are described in the following.\n' +
      '\n' +
      'Metrics design.The foundational principle of SFM (Structure from Motion) [14] and 3D construction is multi-view geometry, meaning that the quality of the model relies on two main factors: 1) The perspectives of the virtual video\'s observation cameras must sufficiently meet physical characteristics, such as the pinhole camera; 2) As the video progresses and perspectives change, the rigid parts of the scene must vary in a manner that maintains physical and geometric stability.\n' +
      '\n' +
      'Furthermore, the fundamental unit of multi-view geometry is two-view geometry. The higher the physical fidelity of the AI-generated video, the more its two frames should conform to the ideal two-view geometry constraints, such as epipolar geometry. Specifically, the more ideal the camera imaging of the virtual viewpoints in the sequence video, the more faithfully the physical characteristics of the scene are preserved in the images. The closer the two frames adhere to ideal two-view geometry, and the smaller the distortion and warping of local features in terms of grayscale and shape, the more matching points can be obtained by the matching algorithm. Consequently, a higher number of high-quality matching points are retained after RANSAC [19]. Therefore, we extract two frames at regular intervals from the AI-generated videos, yielding pairs of two-view images. For each pair, we use a matching algorithm to find corresponding points and employ RANSAC based on the fundamental matrix (epipolar constraint) to eliminate incorrect correspondences.\n' +
      '\n' +
      'After elimination, we calculate the average number of correct initial matching points, the average number of retained points, and the average retention ratio. Therefore, we have the following metrics: num_pts refers to the total number of initial matching points in the binocular view, and num_inliers_F refers to the total number of matching points retained after filtering. The keep_ratio is obtained by the ratio of num_inliers_F to num_pts. Additionally, for each pair of images, we calculate the bidirectional geometric reprojection error for \\(N\\) matching points per point of the \\(F\\) matrix, \\(d(x,x^{\\prime})\\). \\(x\\) and \\(x^{\\prime}\\) are the matching points retained after RANSAC and \\(d\\) is the distance from a point to its corresponding epipolar line. Finally, we perform an overall statistical analysis of all data to calculate the RMSE (Root Mean Square Error) and MAE (Mean Absolute Error).\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      'In our experiments, a traditional algorithm, SIFT [20], was chosen for the sparse matching module instead of a more robust deep learning-based matching algorithm. This decision aims to prevent the matching performance from being excessively strong, which could potentially mask underlying deficiencies in image quality, such as changes in lighting, texture, and material properties. Similarly, the dense matching module is implemented with the traditional SGBM algorithm [21] for the same reason. The RANSAC algorithm is employed using the original version available in OpenCV. We obtain Sora videos directly from the official website. To maintain a fair comparison, we utilize the first frame of Sora videos combined with the image2video functionality of both Gen2 and Pika (using the same prompt) to generate videos with the same scene.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c} \\hline \\hline Video name & Method & num\\_pts\\(\\uparrow\\) & num\\_inliers\\_F\\(\\uparrow\\) & keep\\_ratio\\(\\uparrow\\) & mean\\_error\\(\\downarrow\\) & rmse\\(\\downarrow\\) \\\\ \\hline \\hline \\multirow{3}{*}{Amalifi\\_coast} & Sora & **5441.18** & **3857.36** & **0.71** & **0.96** & **1.20** \\\\  & Gen2 & 2071.00 & 367.82 & 0.18 & 1.19 & 1.45 \\\\  & Pika & 1598.82 & 101.36 & 0.06 & 1.25 & 1.54 \\\\ \\hline \\multirow{3}{*}{Art\\_museum} & Sora & **5220.09** & **3463.91** & **0.66** & **0.86** & **1.12** \\\\  & Gen2 & 2480.36 & 987.00 & **0.40** & 1.10 & 1.35 \\\\  & Pika & 2249.91 & 527.73 & **0.23** & 1.27 & 1.53 \\\\ \\hline \\multirow{3}{*}{Big\\_sur} & Sora & **5044.55** & **3924.09** & **0.78** & **0.58** & **0.80** \\\\  & Gen2 & 1899.09 & 190.73 & 0.10 & 1.13 & 1.39 \\\\  & Pika & 1662.73 & 89.55 & 0.05 & 1.08 & 1.37 \\\\ \\hline \\multirow{3}{*}{Gold\\_rush} & Sora & **6198.55** & **4733.91** & **0.76** & **0.89** & **1.13** \\\\  & Gen2 & 1779.27 & 500.36 & 0.28 & 1.08 & 1.34 \\\\  & Pika & 1897.55 & 587.73 & 0.30 & 1.19 & 1.45 \\\\ \\hline \\multirow{3}{*}{Minecraft} & Sora & **4911.36** & **3048.18** & **0.62** & **0.92** & **1.15** \\\\  & Gen2 & 1445.82 & 201.64 & 0.14 & 1.23 & 1.48 \\\\  & Pika & 1196.91 & 91.64 & 0.08 & 0.95 & 1.25 \\\\ \\hline \\multirow{3}{*}{Santorini} & Sora & **3790.27** & **2608.00** & **0.69** & **0.99** & **1.24** \\\\  & Gen2 & 1739.91 & 365.91 & 0.21 & 1.26 & 1.52 \\\\ \\cline{1-1}  & Pika & 1287.18 & 60.27 & 0.05 & 1.07 & 1.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Quantitative comparisons on six Sora videos.** The symbols ‘\\(\\uparrow\\)/\\(\\downarrow\\)’ indicate that higher/lower scores are better. The best scores are marked in **bold**.\n' +
      '\n' +
      'Fidelity metric.The total number of frames in each video, with a sampling interval of 30 frames, and a RANSAC threshold of 3 are computed successively starting from the first frame. The results are shown in Tab. 1. The results indicate that while Sora demonstrates similar matching errors compared to the other two methods (still optimal), it acquires several times more correct matching points. This suggests that its generated images were the most authentic, with the highest geometrical consistency quality.\n' +
      '\n' +
      'Sustained stability metric.We also compare the variations in the aforementioned \\(keep\\_ratio\\) metrics under different frame sampling intervals. This evaluation is referred to as the assessment of sustained stability. From the results shown in Fig. 1(b), it can be observed that as the frame interval increases, Sora shows a slow decrease in the preservation ratio of correct matches, while the other two methods exhibit a sharp decrease. This demonstrates the stability and consistency of Sora in preserving physical, imaging, and geometric features over long periods.\n' +
      '\n' +
      'Visualizations.First, we demonstrate the 3D reconstruction process using the SFM and Gaussian Splatting methods, showcasing the point clouds and Gaussian Splatting reconstruction results generated by different approaches. Fig. 2 depicts the 3D reconstruction results from videos produced by Pika, Gen2, and Sora, covering 6 different scenes. The figure presents both point clouds and Gaussian Splatting visualizations. Remarkably, the reconstruction quality achieved by Sora is significantly superior to that of Pika and Gen2. This enhanced performance can be ascribed to two principal reasons: 1) Sora\'s capability to generate longer videos, which provides a richer set of camera information, and 2) the improved consistency among different frames within Sora-generated videos, which facilitates the reconstruction of clear and intricate 3D geometric structures. Furthermore, we conduct a visual analysis of the sparse matching results obtained from the videos generated by different methods as shown in Fig. 3. The videos generated by the Sora method exhibit the highest number of correctly matched points after filtering. Finally, we feed the rectified stereo images into the SGBM matching algorithm and directly compare the quality of stereo matching results through visualization as shown in Fig. 4. The visual SGBM stereo-matching results reveal that only views strictly adhering to geometric consistency can produce reasonably dense matching results through the SGBM algorithm.\n' +
      '\n' +
      'Figure 2: Visualizations of point clouds and Gaussian Splatting renderings. This figure presents 3D reconstruction results from videos produced by Pika, Gen2, and Sora. The results of Pika and Gen2 have a limited reconstruction scope with poor geometry and texture. The quality of Sora’s reconstructions significantly surpasses that of Pika and Gen2, which can be attributed to two key factors: 1) Sora’s ability to produce longer videos, offering more extensive camera information, and 2) the greater frame-to-frame consistency in Sora videos, enabling clearer and more detailed 3D reconstructions. (Note: One of Gen2’s videos could not be reconstructed due to insufficient variation in the camera perspective.)\n' +
      '\n' +
      '## 4 Future Work Discussions\n' +
      '\n' +
      'The emergence of models like Sora has underscored the need for more precise and holistic assessment tools for video generation tasks. To thoroughly evaluate the quality of generated videos, this research undertakes an initial investigation into the application of 3D reconstruction metrics for examining geometric properties. Beyond geometry, additional physics-based metrics, such as texture authenticity, motion adherence, and interaction logic among scene objects, warrant further consideration. While our primary objective currently is to assess geometric consistency, we acknowledge the importance of these criteria and earmark them for future explorations.\n' +
      '\n' +
      'Figure 4: Visualizations of the SGBM stereo matching results reveals that only views strictly adhering to geometric consistency can produce reasonable dense matching results through the SGBM algorithm. It is evident that the videos generated by the Sora method exhibit the most outstanding geometric consistency.\n' +
      '\n' +
      'Figure 3: Matching result comparisons. In the image, green represents high-quality matching results, while red represents discarded matching results. The presence of more green high-quality matches indicates a higher level of geometric consistency between the two views.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] T. Brooks, B. Peebles, C. Homes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. W. Y. Ng, R. Wang, and A. Ramesh, "Video generation models as world simulators," 2024. [Online]. Available: [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)\n' +
      '* [2] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, "Video diffusion models," _arXiv:2204.03458_, 2022.\n' +
      '* [3] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet _et al._, "Imagen video: High definition video generation with diffusion models," _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [4] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng, "Magicvideo: Efficient video generation with latent diffusion models," _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [5] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, "Align your latents: High-resolution video synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 22 563-22 575.\n' +
      '* [6] Y. Zhou, D. Zhou, Z.-L. Zhu, Y. Wang, Q. Hou, and J. Feng, "Maskdiffusion: Boosting text-to-image consistency with conditional mask," _arXiv preprint arXiv:2309.04399_, 2023.\n' +
      '* [7] A. Blattmann, T. Dockhorn, S. Kulas, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts _et al._, "Stable video diffusion: Scaling latent video diffusion models to large datasets," _arXiv preprint arXiv:2311.15127_, 2023.\n' +
      '* Home," [https://pika.art/home](https://pika.art/home), 2023, accessed: 2024-02-01.\n' +
      '* Gen2," [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2), 2023.\n' +
      '* [10] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [11] T. Unterthiner, S. van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly, "Fvd: A new metric for video generation," 2019.\n' +
      '* [12] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, "Improved techniques for training gans," _Advances in neural information processing systems_, vol. 29, 2016.\n' +
      '* [13] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2022, pp. 10 684-10 695.\n' +
      '* [14] J. L. Schonberger and J.-M. Frahm, "Structure-from-motion revisited," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016, pp. 4104-4113.\n' +
      '* [15] P. Moulon, P. Monasse, R. Perrot, and R. Marlet, "Openmrg: Open multiple view geometry," in _Reproducible Research in Pattern Recognition: First International Workshop, RRPR 2016, Cancun, Mexico, December 4, 2016, Revised Selected Papers 1_. Springer, 2017, pp. 60-74.\n' +
      '* [16] D. Cernea, "Openmvs: Multi-view stereo reconstruction library," _City_, vol. 5, no. 7, 2020.\n' +
      '* [17] Z. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu, "Nerf-: Neural radiance fields without known camera parameters," _arXiv preprint arXiv:2102.07064_, 2021.\n' +
      '* [18] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis, "3d gaussian splatting for real-time radiance field rendering," _ACM Transactions on Graphics_, vol. 42, no. 4, 2023.\n' +
      '* [19] M. A. Fischler and R. C. Bolles, "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography," _Communications of the ACM_, vol. 24, no. 6, pp. 381-395, 1981.\n' +
      '* [20] P. C. Ng and S. Henikoff, "Sift: Predicting amino acid changes that affect protein function," _Nucleic acids research_, vol. 31, no. 13, pp. 3812-3814, 2003.\n' +
      '* [21] H. Hirschmuller, "Accurate and efficient stereo processing by semi-global matching and mutual information," in _2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'05)_, vol. 2. IEEE, 2005, pp. 807-814.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>