<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 레이아웃 학습을 이용한 Disentangled 3D 장면 생성\n' +
      '\n' +
      ' 데이브 엡스타인\n' +
      '\n' +
      'Ben Poole\n' +
      '\n' +
      'Ben Mildenhall\n' +
      '\n' +
      '알렉세이 에프로스\n' +
      '\n' +
      'Aleksander Holynski\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 컴포넌트 객체들로 분해된 3차원 장면 생성 방법을 소개한다. 이 분할은 사전 훈련된 대규모 텍스트 대 이미지 모델의 지식에만 의존하여 감독되지 않는다. 우리의 핵심 통찰력은 공간적으로 재배열될 때 여전히 동일한 장면의 유효한 구성들을 생성하는 3D 장면의 부분들을 발견함으로써 객체들이 발견될 수 있다는 것이다. 구체적으로, 본 논문에서 제안하는 방법은 여러 개의 NeRF(NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF: NeRF 그런 다음 이러한 합성 장면이 이미지 생성기에 따라 배포되도록 권장한다. 본 논문에서 제안하는 방법은 단순함에도 불구하고, 3D 장면들을 개별 객체들로 분해하여 텍스트-3D 콘텐츠 생성에서 새로운 기능을 가능하게 하는 것을 보인다. 결과 및 대화형 데모: [https://dave.ml/layoutlearning/](https://dave.ml/layoutlearning/)는 프로젝트 페이지를 참조하십시오.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '많은 볼 수 있는 유기체의 놀라운 능력은 물체 개별화(Piaget et al., 1952), 망막에 투사된 빛으로부터 분리된 물체를 식별할 수 있는 능력(Wertheimer, 1938). 실제로, 매우 어린 나이로부터, 인간과 다른 생물들은 그들이 지각하는 물리적 세계를 그것을 구성하는 3차원 실체들로 조직할 수 있다(Spelke, 1990; Wilcox, 1999; Hoffmann et al., 2011). 객체 발견의 유사한 태스크는 3D 장면들을 컴포넌트 객체들로 자율적으로 파싱할 수 있는 에이전트들이 그들의 주변 환경을 탐색하고 상호작용할 수 있기 때문에, 그 시작으로부터 인공 지능 커뮤니티의 주의를 사로잡았다(Roberts, 1963; Ohta et al., 1978).\n' +
      '\n' +
      '50년 후, 이미지들의 생성 모델들은 광분한 페이스로 발전하고 있다(Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Yu et al., 2022; Chang et al., 2023). 이러한 모델은 고품질 샘플을 생성할 수 있지만 내부 작업은 해석하기 어렵고 생성된 이미지를 구성하는 별개의 3D 개체를 명시적으로 나타내지 않는다. 그럼에도 불구하고, 이러한 모델들에 의해 학습된 사전들은 3D 추론(Hedlin et al., 2023; Ke et al., 2023; Liu et al., 2023; Luo et al., 2023; Wu et al., 2023)을 포함하는 다양한 작업들에 걸쳐 믿을 수 없을 정도로 유용한 것으로 입증되었으며, 이는 이들이 실제로 생성된 콘텐츠를 묘사된 기초 3D 객체들로 분해할 수 있을 수 있음을 시사한다.\n' +
      '\n' +
      '이러한 텍스트-이미지 네트워크의 특히 흥미로운 응용 중 하나는 확산 모델에 의해 학습된 풍부한 분포를 활용하여 3D 표현, _e.g_를 최적화하는 3D 생성이다. 렌더링된 뷰들이 이전으로부터의 샘플들과 유사하도록 신경 복사 필드(NeRF, Mildenhall 등, 2020). 이 기술은 어떠한 3D 감독 없이 텍스트-대-3D 생성을 허용하지만(Poole et al., 2022; Wang et al., 2023b), 대부분의 결과는 단지 하나 또는 두 개의 고립된 객체들을 묘사하는 간단한 프롬프트들에 초점을 맞춘다(Lin et al., 2023; Wang et al., 2023c).\n' +
      '\n' +
      '우리의 방법은 이 작업을 기반으로 그들이 포함하고 있는 객체들에 자동으로 얽히는 복잡한 장면들을 생성한다. 이를 위해, 우리는 하나의 장면 대신에 주어진 장면에 대해 _multiple NeRFs_를 인스턴스화하고 렌더링하여, 모델이 각각의 NeRF를 사용하여 별도의 3D 엔티티를 표현하도록 유도한다. 우리의 접근법의 핵심에는 장면을 "잘 형성"하면서 다른 사람들과 독립적으로 조작할 수 있는 장면의 일부로 물체를 직관적으로 정의하는 것이 있다(Biederman, 1981). 우리는 텍스트 프롬프트가 주어진 배포 중인 2D 이미지로 렌더링하는 합성 장면을 산출해야 하는 모든 NeRF의 3D 아핀 변환의 서로 다른 레이아웃 세트를 학습함으로써 이를 구현한다(풀 등, 2022).\n' +
      '\n' +
      '우리는 우리가 _layout learning_라고 부르는 이 가벼운 유도성 바이어스가 생성된 3D 장면들에서 놀랍도록 효과적인 객체 디엔탠먼트를 초래하여 텍스트-대-3D 파이프라인에서 객체-레벨 장면 조작을 가능하게 한다는 것을 발견한다(도 1). 우리는 관심 있는 3D 자산 주위에 장면을 구축하고, 주어진 자산 세트에 대해 서로 다른 그럴듯한 배열을 샘플링하고, 제공된 NeRF를 포함하는 객체로 구문 분석하는 것과 같은 여러 작업에 대한 레이아웃 학습의 유용성을 보여준다. 또한 보조 모델이나 예제별 인간 주석이 필요하지 않음에도 불구하고 레이아웃 학습을 통해 나타나는 객체 수준 분해가 의미 있고 기준선보다 우수하다는 것을 정량적으로 검증한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '그림 1: **레이아웃 학습은 텍스트 프롬프트와 사전 훈련된 텍스트 대 이미지 확산 모델이 주어졌을 때 얽히지 않은 3D 장면**을 생성한다. 우리는 서로 다른 객체를 나타내는 여러 개의 NeRF(오른쪽)로 구성되고 학습된 레이아웃에 따라 배열된 전체 3D 장면(왼쪽, 두 뷰에서 표면 정규 및 질감 없는 렌더)을 학습한다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같다:\n' +
      '\n' +
      '* 서로 독립적으로 조작될 수 있고 여전히 유효한 장면들을 생성할 수 있는 장면의 부분들로서 객체들에 대한 간단하고 다루기 쉬운 정의를 소개한다.\n' +
      '* 우리는 이 개념을 신경망의 아키텍처에 통합함으로써, 이러한 NeRF들에 대한 레이아웃들의 세트뿐만 아니라 NeRF들의 세트를 최적화함으로써 3D 장면들의 구성적 생성을 가능하게 한다.\n' +
      '* 우리는 레이아웃 학습을 다양한 새로운 3D 장면 생성 및 편집 작업에 적용하여 객체 레이블, 경계 상자, 미세 조정, 외부 모델 또는 기타 추가 감독이 필요하지 않음에도 불구하고 복잡한 데이터를 풀 수 있는 능력을 보여준다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '##### 신경 3D 표현\n' +
      '\n' +
      '3차원 장면을 출력하기 위해서는 뉴럴 래디언스 필드(NeRF, Mildenhall et al., 2020)와 같이 3차원 데이터를 모델링할 수 있는 아키텍처를 사용해야 한다. 본 논문에서는 3차원 공간상의 한 점으로부터 밀도\\(\\tau\\) 및 알베도\\(\\mathbf{\\rho}\\)로 매핑되는 MLP\\(f\\)을 이용하여 볼륨을 나타내는 MLP 기반 NeRFs(Barron et al., 2021)를 기반으로 한다.\n' +
      '\n' +
      '\\[(\\tau,\\mathbf{\\rho})=f(\\mathbf{\\mu};\\theta).\\]\n' +
      '\n' +
      '광선\\(\\mathbf{r}\\)을 장면에 캐스팅한 후, 광선을 따라 샘플링된 지점에서 밀도와 색상을 알파 합성하여 색상과 누적 알파 값을 생성함으로써 이 볼륨을 차별적으로 렌더링할 수 있다. 3D 복원을 위해 관측된 영상과 카메라 포즈에서 알려진 픽셀 값과 일치하도록 렌더링된 광선에 대한 색상을 최적화하지만, 3D 생성을 위해 무작위 카메라 포즈를 샘플링하고 해당 광선을 렌더링하고 생성된 이미지를 생성 모델을 사용하여 점수를 매긴다.\n' +
      '\n' +
      '2D 확산 모델을 이용한### 텍스트-3D\n' +
      '\n' +
      '우리의 연구는 2D 확산 사전(Poole et al., 2022)을 사용하여 텍스트 대 3D 생성을 기반으로 한다. 이러한 방법들은 확산 모델을 3D 표현의 파라미터들을 최적화하는데 사용될 수 있는 손실 함수로 변환한다. 초기 무작위 파라미터 집합이 주어졌을 때, 각 반복에서 우리는 무작위로 카메라를 샘플링하고 3D 모델을 렌더링하여 이미지\\(x=g(\\theta,c)\\)를 얻는다. 그런 다음 미리 훈련된 확산 모델 \\(\\hat{\\epsilon}(z_{t};y,t)\\)을 사용하여 잡음화된 이미지 \\(z_{t}=\\alpha_{t}x+\\sigma_{t}\\epsilon\\)의 점수 함수를 평가함으로써 일부 조건화 텍스트 \\(y\\)이 주어진 렌더링 이미지의 품질을 평가할 수 있다. 점수 증류를 사용하여 3D 표현의 매개변수를 업데이트한다:\n' +
      '\n' +
      '[\\nabla_{\\theta}\\mathcal{L}_{\\text{SDS}(\\theta)=\\mathbb{E}_{t,\\epsilon,c}\\left[w(t)(\\hat{\\epsilon}(z_{t};y,t)-\\epsilon)\\frac{\\partial x}{\\partial\\theta}\\right]\\tag{1}\\t.\n' +
      '\n' +
      '여기서 \\(w(t)\\)는 잡음-레벨 종속 가중치이다.\n' +
      '\n' +
      'SDS 및 관련 방법은 3D 표현의 구조를 알리기 위해 큰 텍스트 이미지 데이터 세트로부터 획득된 풍부한 2D 사전의 사용을 가능하게 한다. 그러나 높은 품질의 3D 모델을 생성하기 위해 초기화 및 하이퍼파라미터를 신중하게 조정해야 하는 경우가 많으며, 과거 작업은 객체 생성을 위해 최적화했다. NeRF는 원점에서 밀도의 가우시안 블롭으로 초기화되어, 3D 표현의 주변부의 스카이박스-유사 환경에 밀도를 배치하는 대신 중심에 있는 물체를 선호하도록 최적화 프로세스를 바이어싱한다. 또한 경계 구는 배경에서 밀도가 생성되는 것을 방지하기 위해 사용된다. 결과적인 3D 모델들은 고품질의 개별 객체들을 생성할 수 있지만, 종종 흥미로운 장면들을 생성하지 못하고, 결과적인 3D 모델들은 구성 엔티티들로 쉽게 분리될 수 없는 단일 표현이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '모놀리식 3D 표현에서 여러 객체가 있는 장면까지의 간격을 메우기 위해 보다 표현적인 3D 표현을 소개한다. 여기서, 우리는 3D 공간에서 이러한 NeRF들을 배열하기 위한 일련의 레이아웃들, _i.e._ 유효한 방법들과 함께 다수의 NeRF들을 학습한다. 이러한 레이아웃에 따라 NeRF를 변환하고 합성하여 이전에 텍스트 대 이미지로 SDS 손실에 의해 평가된 고품질 장면을 형성하도록 훈련한다. 이러한 구조는 합성 NeRF가 고품질 장면을 나타내는 것을 보장하면서 각각의 개별 NeRF가 상이한 객체를 나타내게 한다. 우리의 접근 방식에 대한 개요는 그림 2를 참조하십시오.\n' +
      '\n' +
      '### 다중 볼륨을 합성하는 단계\n' +
      '\n' +
      '우리는 아마도 3D 장면들을 별개의 실체들로 해체하는 가장 순진한 접근법을 고려하는 것으로 시작한다 우리는 단순히 NeRFs(\\(K\\)NeRFs\\(\\{f_{k}\\}\\)를 선언하는데, 각각의 NeRFs는 자신의 물체를 수용하도록 의도되고, 모든 NeRFs로부터 광선을 따라 공동으로 밀도를 축적하고, 합성 볼륨을 렌더링함으로써 정상으로 훈련을 진행한다. 이는 다른 맥락에서 널리 탐색된 설정-잠복 표현(Locatello et al., 2020; Jaegle et al., 2021; Jabri et al., 2023)에 대한 유추로 볼 수 있다. 이 경우, 하나의 3차원 표현을 질의하여 최종 알베도(\\mathbf{\\rho}\\)와 점(\\mathbf{\\mu}\\)의 밀도(\\tau\\)에 도달하기 보다는, 이러한 표현들을 질의하여 집합(\\(\\mathbf{\\rho}_{k},\\tau_{k}\\}_{k=1}^{K}\\)을 얻는다. (\\mathbf{\\mu}\\)에서의 최종 밀도는 \\(\\tau^{\\prime}=\\sum\\tau_{k}\\)이고, 최종 알베도는 밀도 가중 평균 \\(\\mathbf{\\rho}^{\\prime}=\\sum\\frac{\\tau_{k}{\\tau^{\\prime}\\mathbf{\\rho}_{k}\\)이다.\n' +
      '\n' +
      '이 제형은 몇 가지 잠재적인 이점을 제공합니다. 첫째, 단지 하나가 아니라 초기화 시에 변형할 \\(K\\) 별개의 3D 가우시안 밀도 구가 있기 때문에, 더 큰 객체 세트를 생성하기 위해 이 표현을 최적화하는 것이 더 쉬울 수 있다. 둘째, 많은 표현들은 묵시적으로 로컬 평활도 바이어스(Tancik et al., 2020)를 포함하며, 이는 객체들을 생성하는데 도움이 되지만 공간적으로 불연속적인 장면들은 생성하지 않는다. 따라서 우리의 표현은 공간적으로 매끄러운 개체, 즉 객체를 향해 각 표현을 할당하는 쪽으로 기울어질 수 있다.\n' +
      '\n' +
      '그러나 불규칙적인 잠복기의 집합이 종종 매우 해석할 수 없는 것처럼 NeRF의 산란 사례는 의미 있는 분해를 생성하지 않는다. 실제로, 우리는 각각의 NeRF가 종종 3D 공간의 랜덤 포인트-클라우드-유사 서브세트를 나타낸다는 것을 발견한다(도 3).\n' +
      '\n' +
      '각 3D 인스턴스가 3D 공간의 다른 부분이 아닌 일관된 객체를 표현하도록 유도하는 방법이 필요하다.\n' +
      '\n' +
      '### Layout learning\n' +
      '\n' +
      '우리는 모델의 잠재 공간 구조, _e.g. 질의-축 소프트맥스 어텐션(Locatello et al., 2020), 공간 타원체 특징 맵(Epstein et al., 2022) 및 대각선 헤시안 매트릭스(Peebles et al., 2020)에서 간단한 유도 편향 또는 정규화를 부과함으로써 작동하는 물체에 대한 다른 감독되지 않은 정의에서 영감을 받는다. 특히, Niemeyer and Geiger (2021)는 순방향 패스에서 다수의 NeRF 볼륨을 합성하는 3D 인식 GAN을 학습하는데, 여기서 잠재 코드는 각각의 NeRF의 출력에 대한 랜덤 아핀 변환을 포함한다. 이러한 구조를 통해 각 NeRF는 서로 다른 객체와 자신을 연관시키는 것을 학습하여 우리가 추구하는 일종의 탈엉킴을 촉진한다. 그러나, 이들의 접근법은 각 객체의 위치, 포즈 및 크기의 미리 지정된 독립적인 분포에 의존하여, 하나 또는 두 개의 객체가 있는 이미지의 좁은 데이터 세트를 넘어 스케일링을 방지하고 레이아웃의 변동을 최소화한다.\n' +
      '\n' +
      '우리의 설정에서, 원하는 출력은 수많은 오픈-어휘들, 임의의 객체들을 포함할 뿐만 아니라, 이들 객체들은 결과 장면이 유효하거나 "잘-형성됨"을 위해 특정 way_로 배열되어야 한다(Biederman et al., 1982). 왜 단순히 이 배열을 배우지 않았을까요?\n' +
      '\n' +
      '이를 위해 각각의 NeRF\\(f_{k}\\)에 학습 가능한 아핀 변환\\(\\mathbf{T}_{k}\\)을 장착하고, 모든 볼륨에 걸친 변환 집합을 레이아웃\\(\\mathbf{L}\\equiv\\{\\mathbf{T}_{k}\\{k}_{k=1}^{K}\\)으로 표현한다. 각 \\(\\mathbf{T}_{k}\\)에는 회전 \\(\\mathbf{R}_{k}\\in\\mathbb{R}^{3\\times 3\\) (실제로는 최적화 용이성을 위해 쿼터니온 \\(\\mathbf{q}\\in\\mathbbb{R}^{4}\\)을 통해 표현됨), 변환 \\(\\mathbf{t}_{k}\\in\\mathbb{R}^{3\\) 및 스케일 \\(s_{k}\\in\\mathbb{R}\\)이 있다. 이 아핀 변환을 카메라-투-월드 광선 \\(\\mathbf{r}\\)에 적용하여 \\(f_{k}\\)을 질의하기 위해 사용된 점을 샘플링한다. 이 구현은 간단하고, \\(f\\)의 기본 형태에 대한 가정을 하지 않으며, 광선을 따른 샘플링 및 임베딩 포인트가 완전히 미분가능함에 따라 표준 역전파(standard backpropagation)로 파라미터를 업데이트한다(Lin et al., 2021). 구체적으로, 원점\\(\\mathbf{o}\\)과 방향\\(\\mathbf{d}\\)을 갖는 광선\\(\\mathbf{r}\\)은 다음과 같은 변환을 통해 인스턴스별 광선\\(\\mathbf{r}_{k}\\)으로 변환된다.\n' +
      '\n' +
      'bf{o}_{k}=s_{k}\\left(\\mathbf{R}_{k}-\\mathbf{t}_{k}\\right) \\tag{2}\\] \\[\\mathbf{d}_{k}=s_{k}\\mathbf{R}_{k}\\mathbf{d}\\\\[\\mathbf{r}_{k}(t)=\\mathbf{o}_{k}+t\\mathbf{d}_{k}\\tag{4}\\]\n' +
      '\n' +
      '각 \\(f_{k}\\)에 서로 다른 \\(H\\times W\\)의 광선 격자를 입력하더라도 모두 같은 위치에 있는 것처럼 출력을 합성한다.\n' +
      '\n' +
      '도 2: **방법. 레이아웃 학습은 NeRF(NeRF)\\(f_{k}\\)와 이들을 위한 서로 다른 레이아웃 \\(\\mathbf{L}_{n}\\)을 최적화함으로써 작동하며, 각각은 NeRF 아핀 변환 \\(\\mathbf{T}_{k}\\)으로 구성된다. 매 반복마다 랜덤 레이아웃이 샘플링되고 모든 NeRF를 공유 좌표 공간으로 변환하는 데 사용된다. 생성된 부피는 퇴화 분해 및 기하학적 구조를 방지하기 위해 스코어 증류 샘플링(Poole et al., 2022) 뿐만 아니라 per-NeRF 정규화로 렌더링되고 최적화된다(Barron et al., 2022). 이러한 간단한 구조는 생성된 3D 장면들에서 객체 디펜던스가 나타나게 한다.**\n' +
      '\n' +
      '좌표 공간 - 예를 들어, \\(\\mathbf{\\mu}=\\mathbf{r}(t)\\)에서의 최종 밀도는 \\(\\mathbf{\\mu}_{k}=\\mathbf{r}_{k}(t)\\)에서의 모든 \\(f_{k}\\)에 의해 출력되는 밀도의 합이다.\n' +
      '\n' +
      '초기 밀도가 동일한 \\(K\\) 모델을 인스턴스화하는 순진한 공식과 비교하여 각 모델의 크기, 방향 및 위치를 학습하면 3D 공간의 다른 부분에 밀도를 더 쉽게 배치할 수 있다. 또한, 최적화의 고유한 확률성은 퇴화 솔루션을 더욱 만류할 수 있다.\n' +
      '\n' +
      '레이아웃 학습을 도입하면서 객체 이탈의 품질을 크게 증가시킨다(Tbl. 3b). 모델은 여전히 바람직하지 않은 방식으로 개별 NeRF를 인접하고 활용할 수 있다. 예를 들어, 레이아웃 학습 없이 여전히 K NeRF와 동일한 방식으로 객체 부분을 서로 옆에 배치할 수 있다.\n' +
      '\n' +
      '**여러 레이아웃을 학습합니다.** 우리는 배포 중인 이미지에 렌더링하는 장면을 형성하기 위해 객체가 "특정 방식으로 배열"되어야 한다는 진술로 돌아갑니다. 우리는 이미 현재 형태의 레이아웃 학습으로 이것을 가능하게 하지만, 한 가지 핵심 사실을 이용하지 않고 있다: 각 객체들이 동등하게 유효한 구성을 제공하는 일련의 객체들을 배열하는 많은 "특정 방식들"이 있다. 우리는 하나의 레이아웃만을 학습하는 것이 아니라, 레이아웃에 대한 분포(P(\\mathbf{L})\\) 또는 무작위로 초기화된 레이아웃의 집합(\\(N\\)을 학습한다. 후자를 선택하고, 각 훈련 단계에서 집합에서 변환 광선\\(\\mathbf{r}_{k}\\)을 산출하기 위해 \\(N\\) 레이아웃 중 하나를 샘플링한다.\n' +
      '\n' +
      '이것이 제자리에서, 우리는 목적성에 대한 우리의 최종 정의에 도달했다(도 2): **객체는 유효한 구도를 형성하기 위해 상이한 방식으로 배열될 수 있는 장면의 부분들이다.** 다중 볼륨을 통합함으로써 "부분", 다중 레이아웃 학습을 통해 "다른 방식으로 배열"을 갖는다. 이 간단한 접근법은 구현하기가 쉽다(도). 9)는 정확한 매개변수(\\(8NK\\)를 거의 추가하지 않고 미세 조정이나 수동 주석이 필요하지 않으며 텍스트 대 이미지 및 3D 모델 선택에 불가지론적이다. 4절에서는 레이아웃 학습이 복잡한 3D 장면의 생성과 탈엉킴을 가능하게 한다는 것을 검증한다.\n' +
      '\n' +
      '**규칙화.** 우리는 Mip-NeRF 360(Barron et al., 2022)을 우리의 3D 백본으로 구축하고, 렌더링의 시각적 품질을 향상시키고 아티팩트를 최소화하기 위해 그들의 배향, 왜곡 및 축적 손실을 상속한다. 그러나 최종 합성 장면에서 이러한 손실을 계산하기보다는 NeRF 기준으로 적용한다. 중요한 것은, 각 NeRF의 누적 밀도 \\(\\mathbf{\\alpha}_{\\text{bin}}\\)의 소프트 비나라이즈 버전을 정규화하여 캔버스의 최소 10%를 차지함으로써 퇴화된 빈 NeRF를 손실 벌점화하는 손실 벌점을 추가한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{empty}}=\\max\\left(0.1-\\bar{\\mathbf{\\alpha}}_{\\text{bin}},0\\right) \\tag{5}\\]\n' +
      '\n' +
      '우리는 \\(s\\sim\\mathcal{N}(1,0.3)\\), \\(\\mathbf{t}^{(i)}\\sim\\mathcal{N}(0,0.3)\\), \\(\\mathbf{q}^{(i)}\\sim\\mathcal{N}(\\mu_{i},0.1)\\)을 초기화하고, 여기서 \\(\\mu_{i}\\)은 마지막 원소의 경우 1이고 나머지 모든 원소의 경우 0이다. 우리는 레이아웃 파라미터를 학습하기 위해 \\(10\\times\\) 더 높은 학습률을 사용한다. 자세한 내용은 부록 A.1을 참조하십시오.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '광범위한 텍스트 프롬프트에 걸쳐 3D 장면을 생성하고 분해하는 레이아웃 학습의 능력을 조사한다. 우리\n' +
      '\n' +
      '그림 3: **분할 및 품질 평가.** 각각 3개의 객체를 포함하는 30개의 프롬프트 목록에 \\(K=3\\) NeRF를 사용하여 모델을 최적화합니다. 그런 다음 각 NeRF와 프롬프트의 객체 중 하나에 대한 설명을 자동으로 페어링하고 평균 NeRF-객체 CLIP 점수를 보고한다(자세한 내용은 텍스트를 참조). 또한 프롬프트 리스트로부터 각각 \\(30\\times 3=90\\)개의 객체를 개별적으로 생성하고, 해당 프롬프트와 랜덤한 다른 객체와의 점수를 계산하여 이 태스크의 성능을 위한 상한과 하한을 제공한다. 학습 \\(K\\) NeRF는 일부 분해를 제공하지만 대부분의 객체는 2개 또는 3개의 모델에 흩어져 있다. 하나의 레이아웃을 학습하면 이러한 문제 중 일부가 완화되지만, 여러 레이아웃에서만 강한 얽힘이 발생합니다. 우리는 이러한 차이를 시각화하기 위해 두 가지 대표적인 창발 객체의 예를 보여준다.\n' +
      '\n' +
      '먼저 절제 연구와 기준선 비교를 통해 본 방법의 유효성을 검증한 후 레이아웃 학습에 의해 활성화된 다양한 응용 프로그램을 시연한다.\n' +
      '\n' +
      '### Qualitative evaluation\n' +
      '\n' +
      '그림 1에서 우리는 레이아웃 학습과 함께 전체 시스템의 몇 가지 예를 보여준다. 각각의 장면에서, 합성 3D 생성은 고품질이고 텍스트 프롬프트와 일치하는 반면, 개별 NeRF는 장면 내의 객체에 대응하는 것을 배운다. 흥미롭게도, 우리의 접근 방식은 입력 프롬프트에 직접적으로 의존하지 않기 때문에, 우리는 부활절 계란으로 채워진 바구니, 요리사의 모자, 피크닉 테이블과 같이 텍스트에서 언급되지 않은 개체를 분리할 수 있다.\n' +
      '\n' +
      '### Quantitative evaluation\n' +
      '\n' +
      '텍스트-투-3D 생성의 품질을 측정하는 것은 지상 진실 데이터의 부족으로 인해 여전히 열린 문제로 남아 있으며, 주어진 프롬프트에 대응하는 "진실한" 장면은 없다. 유사하게, 특정 텍스트 설명에 대한 진정한 분할은 없다. Park et al.(2021); Jain et al.(2022); Poole et al.(2022)에 이어, 우리는 사전 훈련된 CLIP 모델(Radford et al., 2021; Li et al., 2017)로부터의 스코어들을 사용하여 이들 양상들 둘 모두를 캡처하려고 시도한다. 구체적으로, 3개의 객체를 포함하는 30개의 프롬프트 목록을 생성하고, 각 프롬프트에 \\(K=3\\) NeRF를 갖는 모델을 최적화한다. 우리는 각 NeRF에 대한 CLIP 점수의 3\\(\\times\\)3 행렬(\\(100\\times\\) 코사인 유사도)을 "[객체 1/2/3]의 DSLR 사진"으로 계산하고, 최적의 NeRF 대 객체 매칭을 찾고 세 객체 모두에 대한 평균 점수를 보고한다.\n' +
      '\n' +
      '또한, 목적물당 (30\\times 3=90\\) 프롬프트에 대한 SDS를 개별적으로 실행하고, 완전한 디펜탱글먼트(공정성을 위해 모든 모델에 걸쳐 파라미터 카운트를 균등화함) 하에서 달성 가능한 최대 CLIP 점수를 나타내는 점수를 계산한다.\n' +
      '\n' +
      '그림 4: **조건 최적화. 우리는 구조화된 표현을 활용하여 특정 고양이 또는 오토바이(a)와 같은 텍스트 프롬프트 외에 3D 자산이 주어진 장면을 학습할 수 있다. NeRF 가중치는 동결하되 레이아웃 가중치는 동결하지 않음으로써, 모델은 제공된 자산을 자신이 발견한 다른 객체의 맥락에서 배열하도록 학습한다(b). 우리는 모델이 (c)에서 생성하는 전체 합성 장면을 두 뷰에서 표면 정규 및 질감 없는 렌더링과 함께 보여준다.**\n' +
      '\n' +
      '저물 표시로 객체당 NeRF와 90 풀의 무작위 다른 프롬프트 사이의 점수를 계산한다.\n' +
      '\n' +
      '표 2(b)의 결과는 텍스쳐링된("컬러") 및 텍스쳐가 없고 지오메트리 전용("지오") 렌더링에서 모두 계산된 이러한 CLIP 점수를 보여준다. 레이아웃 학습의 최종 변형은 가장 큰 CLIP 모델을 오라클로 사용할 때 감독된 객체당 렌더링에서 불과 0.1 포인트 떨어진 경쟁 성능을 달성하여 객체 얽힘과 외관의 높은 품질을 나타낸다. 전체 프롬프트 목록 및 자세한 내용은 부록 A.3을 참조하십시오.\n' +
      '\n' +
      '**절제.** 간단한 \\(K\\) NeRF 모음에서 시작하여 최종 아키텍처까지 구축함으로써 레이아웃 학습의 다양한 변형을 평가하여 섹션 3에 제시된 설계 결정의 순서를 정당화한다. 단순한 설정은 일부 자명하지 않은 분리(그림 2(a))로 이어지지만 객체의 일부는 NeRFs에 걸쳐 무작위로 분포되며--CLIP 점수는 무작위보다 상당히 높지만 상한보다 훨씬 낮다. 정규화 손실을 추가하면 점수가 다소 향상되지만 레이아웃 학습을 도입한 다음 다른 배열을 공동 학습함으로써 가장 큰 이점이 있어 접근 방식을 검증한다.\n' +
      '\n' +
      '### 레이아웃 학습의 응용\n' +
      '\n' +
      '세대를 넘어 레이아웃 학습으로 주어지는 디엔탱글먼트의 효용성을 부각시키기 위해 다양한 3D 편집 작업에 적용한다. 먼저 그림 4에서 객체 분리에 대한 추가 결과를 보여주지만, 관심 객체를 포함하기 위해 하나의 NeRF가 동결되고 그 주변에 나머지 장면이 구성되어야 하는 시나리오에서. 이 객체의 레이아웃 파라미터는 예를 들어 특정 위치 또는 크기가 원하는 경우 동결될 수도 있다. 레이아웃 매개변수도 배워야 하는 더 어려운 설정을 조사하고 심술궂은 고양이와 녹색 오토바이를 다양한 상황에 통합한 결과를 보여준다. 우리의 모델은 제공된 자산을 장면에 통합하기 위해 그럴듯한 변환을 학습하는 동시에 프롬프트를 완료하는 데 필요한 다른 객체를 발견한다.\n' +
      '\n' +
      '그림 5에서 우리는 단일 훈련 실행에서 학습된 다양한 레이아웃을 시각화한다. 발견된 레이아웃의 변화는 유의미하며, 이는 우리의 공식이 장면 내의 객체들의 다양한 의미 있는 배열들을 찾을 수 있음을 나타낸다. 이를 통해 우리 방법의 사용자는 자신이 생성하는 장면에서 동일한 콘텐츠의 서로 다른 순열을 탐색할 수 있다.\n' +
      '\n' +
      '이에 영감을 받아, 그리고 레이아웃 파라미터들로의 구배 흐름을 테스트하기 위해, 우리는 또한 우리의 방법이 기성품, 동결된 3D 자산들을 의미론적으로 유효한 구성들로 배열하는데 사용될 수 있는지 여부를 조사한다(도 6). 랜덤 위치, 크기 및 배향으로부터 시작하여, 레이아웃은 이미지 모델로부터 역전파된 신호를 사용하여 업데이트된다. 이것은 고무 오리가 욕조 안에서 수축하고 움직이는 것과 샤워 헤드가 위쪽으로 이동하여 욕조 안으로 들어가는 것과 같은 합리적인 변형을 배운다.\n' +
      '\n' +
      '마지막으로, 우리는 객체당 감독 없이 다수의 엔티티를 포함하는 기존의 NeRF를 풀기 위해 레이아웃 학습을 사용한다(도 8). 우리는 무작위로 새로운 모델을 초기화하고 대상 NeRF를 설명하는 캡션을 사용하여 훈련함으로써 이를 수행한다. RGB 공간에서 목표 NeRF를 충실히 재구성하는 장면을 생성하기 위해 첫 번째 레이아웃 \\(\\mathbf{L}_{1}\\)을 요구하므로 다른 모든 레이아웃이 자유롭게 변할 수 있다. 우리는 레이아웃 학습이 재구성해야 하는 장면의 합리적인 분해에 도달한다는 것을 발견한다.\n' +
      '\n' +
      '##5 관련사항\n' +
      '\n' +
      '**객체 인식 및 발견** 장면에 존재하는 객체를 식별하는 주된 방법은 광범위한 수동 주석(Kirillov et al., 2023; Li et al., 2022; Wang et al., 2023)을 사용하여 2차원 이미지를 분할하는 것이지만, 인간 감독에 의존하는 것은 3D 데이터에 대한 도전과 스케일을 제대로 도입하지 못한다. 대안으로서, _unsupervised_object discovery에 대한 광범위한 작업 라인(Russell et al., 2006; Rubinstein et al., 2013; Oktay et al., 2018; Henaff et al., 2022; Smith et al., 2022; Ye et al., 2022; Monnier et al., 2023)은 장면 내의 객체들의 인식을 장려하는 상이한 유도 편향들(Locatello et al., 2019)을 제안한다. 그러나, 이러한 접근법들은 2D 이미지들 또는 제약된 3D 데이터들(Yu et al., 2021; Sajjadi et al., 2022) 중 어느 하나에 크게 제한되어, 복잡한 3D 장면들에 대한 적용성을 제한한다. 동시에, 대규모 텍스트 대 이미지 모델은 엔티티에 대한 이해를 암시적으로 인코딩하는 것으로 나타났다.\n' +
      '\n' +
      '도 5: **레이아웃 다양성.** 우리의 방법은 객체들에 대한 상이한 그럴듯한 배열들을 발견한다. 여기서는 각 예제를 \\(N=4\\) 레이아웃에 걸쳐 최적화하고, 내부와 내부를 헤엄치는 플라밍고 등 합성 장면에서 차이를 보인다. 연못 옆에, 그리고 스누커 테이블 주위에 다른 포즈를 취한 고양이들.\n' +
      '\n' +
      'Enternals Epstein et al.(2023)은 명시적 객체 분리의 어려운 문제에 대한 그들의 사용에 동기를 부여한다.\n' +
      '\n' +
      '**구성 3D 생성.** 단순히 더 나은 제어를 넘어 객체들로 분리된 3D 장면들을 생성하는 것에는 많은 이점들이 있다. 예를 들어, 한 번에 하나씩 객체를 생성하고 이들을 합성하는 것은 도 1의 "일치하는 의상의 개들" 또는 도 4의 오토바이의 핸들바를 들고 있는 사자와 같은 외관 또는 포즈의 호환성에 대한 보장을 제공하지 않는다. 이전 및 동시 작업은 이 영역을 탐색하지만, 둘 중 하나는 사용자가 3D 바운딩 박스 및 객체별 라벨 코헨-바 등(2023); Po 및 웨츠스타인(2023) 또는 LLM과 같은 외부 감독을 사용하여 객체 및 레이아웃 양 등(2023); Zhang 등(2023)은 생성 프로세스를 상당히 늦추고 품질을 저해한다. 우리는 사전 훈련된 이미지 생성기에서 제공되는 신호를 사용하여 별도의 모델이나 레이블 없이 이 전체 프로세스를 해결할 수 있음을 보여준다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '본 논문에서는 텍스트 프롬프트(text prompt)를 이용하여 복잡한 3차원 장면들을 생성하기 위한 간단한 방법인 레이아웃 학습을 제안한다. 여러 레이아웃에 걸쳐 유효한 장면을 형성하기 위해 여러 NeRF를 최적화함으로써, 각 NeRF가 자신의 객체를 포함하도록 장려한다. 이 접근법은 추가 감독이나 보조 모델이 필요하지 않지만 꽤 잘 수행한다. 객체들로 분해된 장면들을 생성함으로써, 우리는 텍스트-투-3D 시스템 사용자들에게 블랙-박스 신경망에 의해 출력된 복잡한 창작물들에 대한 보다 세분화된 로컬 제어를 제공한다.\n' +
      '\n' +
      '레이아웃 학습은 매우 다양한 텍스트 프롬프트에 놀라울 정도로 효과적이지만, 3D에서의 객체 얽힘의 문제는 본질적으로 비포즈적이며, 객체에 대한 우리의 정의는 간단하다. 결과적으로, 우리가 제기하는 제약들을 만족시키는 많은 바람직하지 않은 해결책들이 존재한다.\n' +
      '\n' +
      '우리의 최선의 노력에도 불구하고, 우리 모델이 출력하는 구성 장면은 때때로 실패로 고통받는다(그림). 7)과 같은 오버- 또는 언더-세그먼트화 및 "Janus 문제"(여기서, 객체들은 모든 뷰들로부터 두드러진 특징들이 나타나도록 묘사되는 경우,_예를 들어, 그의 머리 뒤에 얼굴을 갖는 동물) 뿐만 아니라 다른 바람직하지 않은 기하학적 구조들을 포함한다. 또한, 레이아웃은 높은 표준 편차로 초기화되고 학습률이 증가하여 훈련되지만, 때때로 거의 동일한 값으로 수렴하여 본 방법의 효과를 최소화한다. 일반적으로, 우리는 풀림 실패가 시각적 품질의 전반적인 감소를 동반한다는 것을 발견한다.\n' +
      '\n' +
      '도 6: **레이아웃 최적화.** 제공된 3D 자산들의 세트를 동결하는 동안 그라디언트들이 레이아웃 파라미터들로만 흐르도록 허용하는 것은 텍스트 컨디셔닝에서 그러한 안내들이 제공되지 않았음에도 불구하고, 스파게티가 위에 있는 테이블 안으로 집어넣어진 의자와 같은 합리적인 객체 구성들을 초래한다.\n' +
      '\n' +
      '그림 7: **제한.** 레이아웃 학습은 이상하게 교차하는 외부 벽을 가진 캐빈의 잘못된 형상과 같은 SDS로부터 고장 모드를 상속합니다 **(a)**. 또한 말과 그 라이더와 같은 **(b)**와 많은 작은 물체를 생성하는 특정 프롬프트에 대해 **(c)**를 항상 함께 움직이는 물체를 원하지 않게 그룹화할 수 있으며, \\(K\\)을 올바르게 선택하는 것은 어렵고 엉킴을 해친다. 경우에 따라 **(d)**, 서로 다른 초기 값에도 불구하고 레이아웃은 매우 유사한 최종 구성으로 수렴한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '도르 버빈, 루이키 가오, 루시 차이, 허민영 등이 도움의 댓글을 달았고, 아서 브루시가 NGP 구현을 도와준 것에 대해 감사드린다. DE는 PD 소로스 펠로우십에 의해 부분적으로 지원되었다. DE는 ONR MURI 보조금의 추가 자금으로 구글에서 이 연구의 일부를 수행했다.\n' +
      '\n' +
      '## Impact statement\n' +
      '\n' +
      '생성 모델은 데이터 귀속, 사악한 응용 및 장기적인 사회적 영향에 대한 많은 윤리적 문제를 제시한다. 이미지와 캡션과 관련하여 제거하기 위해 필터링된 데이터에 대해 학습된 텍스트 대 이미지 모델을 기반으로 하지만 최근 연구에 따르면 인기 있는 데이터 세트에는 모델 가중치로 누출될 수 있는 바람직하지 않은 내용1에 대한 위험한 묘사가 포함되어 있다.\n' +
      '\n' +
      '각주 1: [https://crsreports.congress.gov/product/pdf/R/R47569](https://crsreports.congress.gov/product/pdf/R/R47569)\n' +
      '\n' +
      '또한, 이미지 생성기에 의해 학습된 분포를 증류하기 때문에 원본 모델에 의해 활성화된 잠재적인 음의 사용 사례를 상속한다. 보다 복잡하고 구성적인 3D 장면의 생성을 촉진함으로써 텍스트-3D 기술과 관련된 잠재적인 문제의 범위를 확장할 수 있다. 윤리적으로 조달되고 잘 정제된 데이터를 사용하여 생성 모델의 잠재적인 유해한 배치를 최소화하도록 주의하는 것은 우리 분야가 규모와 영향력이 계속 증가함에 따라 가장 중요하다.\n' +
      '\n' +
      '또한, 3D 장면들을 객체들로 분해하기 위한 비감독 방법을 도입함으로써, 우리는 자동화된 증가를 통해 비디오 게임 자산 설계자들과 같은 창조적인 작업자들의 변위에 기여할 수 있다. 그러나 동시에 우리가 제안하는 것과 같은 방법은 아티스트의 처분에 따라 가치 있는 도구가 될 가능성이 있으며, 출력에 대한 훨씬 더 많은 제어를 제공하고 새롭고 더 매력적인 형태의 콘텐츠를 만드는 데 도움이 된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Barron et al. (2021) Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., and Srinivasan, P. P. Mip-nerf: a multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 5855-5864, October 2021.\n' +
      '* Barron et al. (2022) Barron, J. T., Mildenhall, B., Verbin, D., Srinivasan, P. P., and Hedman, P. Mip-nerf 360: Unbounded anti-aliased neural radiance field. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 5470-5479, June 2022.\n' +
      '* Biederman(1981) Biederman, I. 한 장면을 한 눈에 볼 수 있는 의미론 In _Perceptual organization_, pp. 213-253. Routledge, 1981.\n' +
      '* Biederman et al. (1982) Biederman, I., Mezzanotte, R. J., and Rabinowitz, J. C. Scene perception: 관계적 위반을 겪고 있는 객체들을 탐지하고 판단한다. _ Cognitive psychology_, 14(2):143-177, 1982.\n' +
      '* Chang et al. (2023) Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M. - H., Murphy, K., Freeman, W. T., Rubinstein, M., et al. Muse: text-to-image generation via masked generative transformer. 2023년 _ICML_에서\n' +
      '* Cohen-Bar et al. (2023) Cohen-Bar, D., Richardson, E., Metzer, G., Giryes, R., and Cohen-Or, D. Set-the-scene: controllable nerf scene 생성을 위한 Global-local training. _ICCV_, 2023.\n' +
      '* Epstein et al. (2022) Epstein, D., Park, T., Zhang, R., Shechtman, E., and Efros, A. A. Blobgan: Spatially disentangled scene representations. In _European Conference on Computer Vision_, pp. 616-635. Springer, 2022.\n' +
      '* Epstein et al. (2023) Epstein, D., Jabri, A., Poole, B., Efros, A. A., and Holynski, A. Diffusion self-guidance for controllable image generation. In _Advances in Neural Information Processing Systems_, 2023.\n' +
      '* Gupta et al. (2018) Gupta, V., Koren, T., and Singer, Y. 샴푸: 사전 조절된 확률 텐서 최적화. 2018년 _ICML_에서.\n' +
      '* Hedlin et al. (2023) Hedlin, E., Sharma, G., Mahajan, S., Isack, H., Kar, A., Tagliasacchi, A., and Yi, K. M. Unsupervised semantic correspondence using stable diffusion. _ arXiv preprint arXiv:2305.15581_, 2023.\n' +
      '* Henaff et al. (2022) Henaff, O. J., Koppula, S., Shelhamer, E., Zoran, D., Jaegle, A., Zisserman, A., Carreira, J., and Arandjelovic, R. 객체 검색 및 표현 네트워크입니다. In _European Conference on Computer Vision_, pp. 123-143. Springer, 2022.\n' +
      '* Hoffmann et al. (2011) Hoffmann, A., Ruttler, V., and Nieder, A. Ontogeny of object permanence and object tracking in carrion crow, corvus corone. _ Animal behavior_, 82(2):359-367, 2011.\n' +
      '* Jabri et al.(2023) Jabri, A., Fleet, D., and Chen, T. 반복 생성을 위한 확장 가능한 적응 계산 2023년 _ICML_에서\n' +
      '* Jaegle et al. (2021a) Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et al. Perceiver io: structured input & output을 위한 일반적인 아키텍처. _ arXiv preprint arXiv:2107.14795_, 2021a.\n' +
      '* Jaegle et al. (2021b) Jaegle, A., Kimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pp. 4651-4664. PMLR, 2021b.\n' +
      '* Jain et al. (2022) Jain, A., Mildenhall, B., Barron, J. T., Abbeel, P., and Poole, B. Zero-shot text-guided object generation with dream fields. 2022년 _CVPR_에서.\n' +
      '* Jain et al. (2022)* Ke et al. (2023) Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R. C., and Schindler, K. 단안 깊이 추정을 위한 확산 기반 이미지 생성기의 용도 변경. _ arXiv preprint arXiv:2312.02145_, 2023.\n' +
      '* Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W. - Y., et al. Segment anything. _ arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* Li 등 (2017) Li, A., Jabri, A., Joulin, A., and van der Maaten, L. 웹 데이터에서 시각적 n-그램을 학습합니다. 2017년 _ICCV_에서.\n' +
      '* Li 등(2022) Li, Y., Mao, H., Girshick, R., and He, K. 객체 탐지를 위한 일반 비전 변압기 백본 탐색 In _European Conference on Computer Vision_, pp. 280-296. Springer, 2022.\n' +
      '* Lin et al. (2021) Lin, C.-H., Ma, W. - C., Torralba, A., and Lucey, S. 바프: 번들 조정 신경 복사 필드입니다. 2021년 _ICCV_에서.\n' +
      '* Lin et al. (2023) Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M. - Y., and Lin, T. -Y. 매직3d: 고해상도 텍스트-3d 콘텐츠 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 300-309, 2023.\n' +
      '* Liu et al. (2023) Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., and Vondrick, C. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 9298-9309, 2023.\n' +
      '* Locatello et al. (2019) Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Scholkopf, B., and Bachem, O. 교란된 표상의 지도되지 않은 학습에서 공통적인 가정을 도전하는 것. In _international conference on machine learning_, pp. 4114-4124. PMLR, 2019.\n' +
      '* Locatello et al. (2020) Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf, T. 슬롯 주의력을 갖는 객체 중심 학습. _ 신경 정보 처리 시스템_, 33:11525-11538, 2020에서의 발전.\n' +
      '* Luo et al. (2023) Luo, G., Dunlap, L., Park, D. H., Holynski, A., and Darrell, T. 확산 하이퍼 특징: 의미적 대응을 위한 시간과 공간을 통한 검색. _ arXiv preprint arXiv:2305.14334_, 2023.\n' +
      '* Mildenhall et al. (2020) Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: 장면들을 뷰 합성을 위한 신경 래디언스 필드들로서 표현하는 것. 2020년 _ECCV_에서.\n' +
      '* Monnier et al. (2023) Monnier, T., Austin, J., Kanazawa, A., Efros, A. A., and Aubry, M. 미분 가능한 블록 월드: 렌더링 프리미티브에 의한 정성적 3D 분해. _Neural Information Processing Systems_, 2023.\n' +
      '* Muller et al. (2022) Muller, T., Evans, A., Schied, C., and Keller, A. Instant neural graphics primitives with multiresolution hash encoding. _ ACM Trans. Graph.__ , 41(4):102:1-102:15, 2022년 7월. doi:10.1145/3528223.3530127. URL[https://doi.org/10.1145/3528223.3530127](https://doi.org/10.1145/3528223.3530127).\n' +
      '*Nichol et al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. 글라이드: 텍스트 유도 확산 모델을 사용한 사실적 이미지 생성 및 편집을 위한 _ arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* Niemeyer & Geiger (2021) Niemeyer, M. 그리고 Geiger, A. Giraffe: 장면들을 구성 생성 신경 특징 필드들로 표현하는 것. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 11453-11464, 2021.\n' +
      '* Ohta et al. (1978) Ohta, Y. -i., Kanade, T., and Sakai, T. 하위 구조를 갖는 객체를 포함하는 장면에 대한 분석 시스템. In _Proceedings of the Fourth International Joint Conference on Pattern Recognitions_, pp. 752-754, 1978.\n' +
      '* Oktay et al. (2018) Oktay, D., Vondrick, C., and Torralba, A. Counterfactual image networks, 2018. URL[https://openreview.net/forum?id=SyYPRg0-](https://openreview.net/forum?id=SyYPRg0-).\n' +
      '* Park et al. (2021) Park, D. H., Azadi, S., Liu, X., Darrell, T., and Rohrbach, A. Benchmark for compositional text-to-image synthesis. 2021년 제30차 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙(라운드 1) 회의에서.\n' +
      '* Peebles et al. (2020) Peebles, W., Peebles, J., Zhu, J.-Y., Efros, A., and Torralba, A. The Hessian penalty: A weak prior for supervised disentanglement. 2020년 _ECCV_에서.\n' +
      '* Piaget et al. (1952) Piaget, J., Cook, M., et al. _The origin of intelligence in children_, volume 8. International Universities Press New York, 1952.\n' +
      '* Po & Wetzstein (2023) Po, R. and Wetzstein, G. Compositional 3d scene generation using locally conditioned diffusion. _ arXiv preprint arXiv:2303.12218_, 2023.\n' +
      '* Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. _ arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. 자연 언어 감독으로부터 전이 가능한 시각적 모델들을 학습한다. 2021년 _ICML_에서.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '\n' +
      '* Roberts(1963) Roberts, L. G. _Machine perception of three-dimensional solids_. 1963년 매사추세츠 공대 박사 학위 논문\n' +
      '* Rubinstein et al. (2013) Rubinstein, M., Joulin, A., Kopf, J., and Liu, C. Unsupervised joint object discovery and segmentation in internet images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 1939-1946, 2013.\n' +
      '* Russell et al. (2006) Russell, B. C., Freeman, W. T., Efros, A. A., Sivic, J., and Zisserman, A. Multiple segmentations using objects and their extent in image collection. In _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'06)_, volume 2, pp. 1605-1614. IEEE, 2006.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 35:36479-36494, 2022에서의 발전.\n' +
      '* Sajjadi et al. (2022) Sajjadi, M. S., Duckworth, D., Mahendran, A., van Steenkiste, S., Pavetic, F., Lucic, M., Guibas, L. J., Greff, K., and Kipf, T. 객체 장면 표현 트랜스포머. _ 신경 정보 처리 시스템_, 35:9512-9524, 2022에서의 발전.\n' +
      '* Smith et al. (2022) Smith, C., Yu, H.-X., Zakharov, S., Durand, F., Tenenbaum, J. B., Wu, J., and Sitzmann, V. 무감독 발견과 물체 빛 필드의 구성 arXiv preprint arXiv:2205.03923_, 2022.\n' +
      '* Spelke(1990) Spelke, E. S. 객체 인식의 원리_ Cognitive science_, 14(1):29-56, 1990.\n' +
      '* Tancik et al. (2020) Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., and Ng, R. 푸리에 특징을 통해 네트워크는 저차원 영역에서 고주파수 함수를 학습할 수 있다. _Neural Information Processing Systems_, 2020.\n' +
      '* Wang et al. (2023a) Wang, C.-Y., Bochkovskiy, A., and Liao, H.-Y. M. Yolov7: Trainable bag-of-freebies set new state-of-the-artive for real-time object detector. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 7464-7475, 2023a.\n' +
      '* Wang et al. (2023b) Wang, H., Du, X., Li, J., Yeh, R. A., and Shakhnarovich, G. Score jacobian chaining: Lifting prerained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12619-12629, 2023b.\n' +
      '* Wang et al. (2023c) Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. _ arXiv preprint arXiv:2305.16213_, 2023c.\n' +
      '* Wertheimer(1938) Wertheimer, M. 지각적 형태의 조직법. 1938년\n' +
      '* Wilcox(1999) Wilcox, T. 개체 개성: 영아의 모양, 크기, 패턴, 색상 사용 Cognition_, 72(2):125-166, 1999.\n' +
      '* Wu et al. (2023) Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan, P. P., Verbin, D., Barron, J. T., Poole, B., et al. Reconfusion: 3d reconstruction with diffusion priors. _ arXiv preprint arXiv:2312.02981_, 2023.\n' +
      '* Yang et al. (2023) Yang, Y., Sun, F.-Y., Weihs, L., VanderBilt, E., Herrasti, A., Han, W., Wu, J., Haber, N., Krishna, R., Liu, L., et al. Holodeck: Language guided generation of 3d embodied ai environments. _ arXiv preprint arXiv:2312.09067_, 2023.\n' +
      '* Ye et al. (2022) Ye, V., Li, Z., Tucker, R., Kanazawa, A., and Snavely, N. 감독되지 않은 비디오 분해를 위해 변형 가능한 스프릿입니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2657-2666, 2022.\n' +
      '* Yu et al. (2021) Yu, H.-X., Guibas, L. J., and Wu, J. Unsupervised discovery of object radiance fields. _ arXiv preprint arXiv:2107.07905_, 2021.\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. _ arXiv preprint arXiv:2206.10789_, 2(3):5, 2022.\n' +
      '* Zhang et al. (2023) Zhang, Q., Wang, C., Siarohin, A., Zhu, P., Xu, Y., Yang, C., Lin, D., Zhou, B., Tulyakov, S., and Lee, H.-Y. Scenewiz3d: 텍스트 유도 3d 장면 구성에 대한 _ arXiv preprint arXiv:2312.08885_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '사전 훈련된 대조적 텍스트-이미지 모델(Radford et al., 2021)에 의해, 구성 생성의 품질에 대한 인간 판단과 상관관계가 있는 것으로 나타났다(Park et al., 2021). 그러나 정밀도나 재현율과 같은 검색 기반 메트릭을 계산하기 보다는 코사인 유사성의 원(\\(100\\times\\) 업스케일링(cosine similarity)을 보고한다. 더 세분화된 메트릭일 뿐만 아니라, 이것은 테스트 세트의 크기 및 난이도(전형적으로 단지 수백 개의 텍스트 프롬프트들)에 대한 검색의 의존성을 방지한다.\n' +
      '\n' +
      '우리는 30개의 프롬프트 목록을 고안한다(도 11). 각각 동물에서 음식, 스포츠 장비, 악기 등에 이르기까지 광범위한 데이터에 걸쳐 있는 세 가지 객체를 나열한다. 섹션 4에 설명된 대로 우리는 \\(K=3\\) NeRF와 레이아웃 학습으로 모델을 훈련하고 각 NeRF가 프롬프트에서 언급된 다른 객체를 포함하는지 테스트한다. 각 NeRF에 대한 CLIP 점수를 질의 프롬프트 "[A/B/C]의 DSLR 사진"으로 계산하여 \\(3\\times 3\\)의 점수 매트릭스를 생성한다. NeRF-prompt CLIP 점수를 계산하기 위해, 각 30도 간격으로 12개의 균일하게 샘플링된 뷰에 대해 (-30^{\\circ}\\) 고도에서 텍스트 이미지 유사성을 평균화한다. 그런 다음 가장 좋은 NeRF-프롬프트 할당(가능한 선택이 \\(3!=6\\)이기 때문에 무력을 사용)을 선택하고 이 과정을 3개의 다른 종자에 걸쳐 실행하여 평균 NeRF-프롬프트 점수가 가장 높은 종자를 선택한다.\n' +
      '\n' +
      '도 11: CLIP 평가에 사용되는 ** 프롬프트.** 각 프롬프트는 템플릿 "{prompt}의 DSLR 사진, 일반 단색 배경"에 주입된다. 개별 객체를 생성하기 위해, 각각의 프롬프트 내의 3개의 객체는 3개의 새로운 프롬프트로 분리되고 독립적으로 최적화된다.\n' +
      '\n' +
      '그림 10: **빈 NeRF 정규화를 위한 의사 코드,** 여기서 soft_bin_acc는 수학식 5에서 \\(\\tilde{\\alpha}_{\\text{bin}\\)을 계산한다.\n' +
      '\n' +
      '그림 9: 레이아웃 학습을 위한 **의사 코드,** 이전 작업에서 상속된 세그먼트가 함수로 추상화됨.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>