<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ReGAL: Refactoring Programs to Discover Generalizable Abstractions\n' +
      '\n' +
      ' Elias Stengel-Eskin\n' +
      '\n' +
      'Archiki Prasad\n' +
      '\n' +
      'Mohit Bansal\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)UNC Chapel Hill. Correspondence to: Elias Stengel-Eskin \\(<\\)esteng@cs.unc.edu\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose **R**efactoring for **G**eneralizable **A**bstraction **L**earning (ReGAL), a gradient-free method for learning a library of _reusable_ functions via code _refactorization_, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs _easier to predict_ across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text-game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy increases of \\(11.5\\%\\) on graphics, \\(26.1\\%\\) on date understanding, and \\(8.1\\%\\) on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals ReGAL\'s abstractions encapsulate frequently-used subroutines as well as environment dynamics.1\n' +
      '\n' +
      'Footnote 1: Code available at [https://github.com/esteng/regal_program_learning](https://github.com/esteng/regal_program_learning)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'An increasing range of tasks can be tackled by using a large language model (LLM) to generate an executable program for a given query; this paradigm has been applied in computer vision (Suris et al., 2023; Gupta et al., 2018; Cho et al., 2023), robotics (Ahn et al., 2022; Singh et al., 2023), tool use (Schick et al., 2023; Lu et al., 2023; Qin et al., 2023), and general reasoning (Lyu et al., 2023). In all these cases, the overall program generation framework is the same: an individual query is given (along with an instructive prompt) to an LLM, which produces a program that, when executed, yields the desired result. Crucially, each program is generated _independently_ (as shown in Fig. 1), with no reference to other queries or programs, and is composed of _primitive_ operations, i.e. the domain language\'s built-in operations. This approach has two major and related limitations:\n' +
      '\n' +
      '**1) Lack of Reusability**: Each program is designed as a one-off script to solve a given example but is not reused by other examples. This increases redundancy and can result in unnecessary errors: for two examples requiring a shared subroutine, the model might correctly generate the subroutine in one example and make a mistake in the other. For instance, in Fig. 1 (top) although the "primitive-only" model had previously generated nonagons, it uses the wrong interior angle (40.5). ReGAL\'s draw_small_ggon() function, on the other hand, executes correctly.\n' +
      '\n' +
      '**2) Lack of Abstraction**: Shared abstractions can improve accuracy by making skills more accessible to the model. When generating from primitives alone, the model must interpret the query and generate the correct mapping from the query to multiple primitives, requiring more reasoning. The model\'s _overall_ task becomes easier when it uses interpretable abstractions, as it is choosing a function name from a library instead of reasoning from scratch. In Fig. 1 (bottom) a model augmented with abstractions can match the sub-query _"a small 9 gon"_ to draw_small_9gon(); with this part of the task simplified, the model reasons correctly about the remaining code, while the primitive-only model fails to correctly embed the shape in a loop.\n' +
      '\n' +
      'Figure 1: ReGAL trains by refactoring primitive-only programs into abstractions that are verified and stored. These abstractions have two benefits: **Reusability**: Rewriting the same code multiple times leads to errors; **Abstraction**: ReGAL makes prediction easier by allowing matching between the query and the abstractions.\n' +
      '\n' +
      'Both limitations can be traced to a _lack of global context_ as the model sees each example separately, so it lacks a mechanism for developing reusable abstractions. This differs greatly from how humans write code: generally, developers might start solving individual tasks with one-off solutions, but quickly begin to develop a library of shared abstractions and code snippets for related problems, thereby reducing the redundancy of their code, promoting efficiency and readability (McConnell, 2004; Downey, 2012). Furthermore, functions can be verified: once we have tested a function, we can rely on it in the future - something that is harder to do for ever-changing one-off code snippets. Such abstraction and verification is only sensible if the code synthesis process takes place over the course of multiple examples. In other words, if presented with a single, one-off task, there is no reason not to write a one-off script.\n' +
      '\n' +
      'While abstraction offers numerous benefits, it comes with the risk of over-fitting, where a function tailored to a specific example loses its generalizability. For instance, a function like draw_ggon_snowflake() may perfectly match one example but fails to generalize. Conversely, draw_small_ggon() is a more versatile function applicable in various contexts. The ability to produce novel programs using primitive operations needs to be balanced with the benefits of encoding subroutines into reusable abstractions (O\'Donnell, 2015). A similar balance between flexibility and efficiency appears in a variety of domains, including language (O\'Donnell, 2015; Yang, 2016), biology (Futuyma & Moreno, 1988), manufacturing (Flynn & Jacobs, 1987), and programming (Ellis et al., 2021). To strike this balance in LLM-based program synthesis, we propose **Re**factoring for **G**eneralizable **A**bstraction **L**earning (ReGAL). ReGAL refines abstractions iteratively by refactoring programs as well as verifying, correcting, and pruning abstractions such that overly specific or incorrect programs are improved upon or removed. ReGAL relies on two key elements: a small set of programs using primitive operations (i.e. _primitive programs_) and an execution environment (e.g., Python). Importantly, we show ReGAL can learn from LLM-generated programs without requiring any human annotations.\n' +
      '\n' +
      'ReGAL follows a familiar train-test paradigm: during ReGAL\'s modular training phase (see Fig. 2), it iteratively refactors a small set of (_query, program_) examples to produce a library of useful abstractions. ReGAL uses an LLM to write helper functions for a batch of examples, which are verified against the expected result; successful helper functions are added to the library and the refactored program serves as an example of the function\'s usage. ReGAL can take success feedback into account to correct and debug errors, and it periodically edits the helper functions to make them more generalizable or - if they cannot be made more generic - prunes functions that are overly specific. Note that the training is gradient-free, relying on a frozen LLM to refactor programs. In the testing phase, an LLM agent is tasked with predicting programs for test queries. The agent has access to ReGAL\'s library of helper functions and demonstrations of how to use them.\n' +
      '\n' +
      'We demonstrate the broad applicability of ReGAL by testing it on three diverse datasets: LOGO (Ellis et al., 2021; Wong et al., 2021), a program induction task; a date reasoning task (Srivastava et al., 2022) known to challenge LLMs (Suzgun et al., 2022); and TextCraft (Prasad et al., 2023), a text-based game for crafting Minecraft objects. Across these tasks, ReGAL significantly improves the accuracy of the predicted programs from various LLMs - especially open-source LLMs - over a baseline that predicts primitive programs (i.e. programs without ReGAL\'s abstractions). For instance, CodeLlama-13B\'s (Roziere et al., 2023) accuracy increases by \\(11.5\\%\\), \\(26.1\\%\\), and \\(8.1\\%\\) on LOGO, Date, and TextCraft respectively, surpassing larger models like GPT-3.5 (cf. Sec. 5). In Sec. 6, we show that ReGAL\'s abstractions are reusable across examples, encapsulate key domain functionalities, and we include an error analysis further highlighting the features making ReGAL effective. Sec. 6.3 reveals that ReGAL can improve over the baseline with minimal training examples, yielding major improvements even with a \\(50\\%\\) reduced training set.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Program Induction.Program induction involves learning a symbolic and programmatic mapping of inputs to outputs. Humans are adept at this kind of "rule-learning" (Marcus et al., 1999; Furnkranz et al., 2012). ReGAL aims to learn a set of general functions that can be used to map inputs to outputs, i.e. a form of program induction. Ellis et al. (2021) present DreamCoder, a method for combining program induction and synthesis that uses a wake-sleep Bayesian learning method to learn programs. Wong et al. (2021) extend this work to incorporate language, using an alignment model as part of the joint model. Like Ellis et al. (2021), Grand et al. (2023) adopt a similar symbolic search procedure, but use an LLM to document abstractions. The symbolic search procedure used by this line of past work has relied on data structures for search that assume the domain language is \\(\\lambda\\)-calculus (Lake et al., 2015; Ellis et al., 2021; Wong et al., 2021; Grand et al., 2023), which is not typically used for software development. In contrast, ReGAL uses an LLM-based search procedure, allowing us to use more flexible languages like Python, which are more commonly used by developers and better represented in pre-training data.\n' +
      '\n' +
      'Program Synthesis and Tool Use.Tool use by LLMs (Schick et al., 2023; Mialon et al., 2023) refers to a form of program synthesis or semantic parsing where an LLM generates API calls to external tools (e.g. calculators, search functions, etc.). This formulation has also been applied to reasoning tasks (Lyu et al., 2023; Chen et al., 2022) as well as other domains such as computer vision (Suris et al., 2023; Gupta and Kembhavi, 2023; Cho et al., 2023), summarization (Saha et al., 2022), and robotics (Ahn et al., 2022; Singh et al., 2023; Huang et al., 2022; 2023). Past work has attempted to induce tools from examples. Cai et al. (2023) induce tools using an LLM for reasoning tasks from BigBench (Srivastava et al., 2022); unlike our work, their system generates one tool per task. While this can offer benefits for homogenous reasoning tasks (e.g. sorting words alphabetically), heterogenous tasks like the ones we explore require multiple functions. More akin to our work, Yuan et al. (2023) and Qian et al. (2023) induce multiple tools for vision and math tasks using an LLM-based framework which also includes retrieval-based parsing. In addition to focusing on different domains, we place an emphasis on efficiency, highlighting ReGAL\'s performance on open-source LLMs (both only consider proprietary models like GPT-3.5). We also differ in our focus on refactoring, and in the amount of information we provide to the refactoring model: unlike Yuan et al. (2023) and Qian et al. (2023), we do not provide in-context examples of the kinds of tools we want the model to create, investigating instead what abstractions the model builds without domain-specific guidance.\n' +
      '\n' +
      'Induction in Interactive Domains.Wang et al. (2023) also induce functions in a Minecraft domain; however, theirs are written and stored based on one iteration. In contrast, our work refactors programs in a group and tests and refines them across the training process, showing generalization in multiple domains. Other prior work learns a library of abstractions for planning in embodied domains (Wong et al., 2023; Majumder et al., 2023). While we share a similar motivation, ReGAL operates in the space of generating executable programs instead of PDDL operators (Wong et al., 2023) or causal textual feedback (Majumder et al., 2023). Similarly, our work aligns with prior efforts in LLM-based task decomposition (Khot et al., 2023; Prasad et al., 2023), where skills are reused across multiple task instances. However, these approaches manually identify atomic skills and require the LLM to repeatedly execute skills from scratch. In contrast, ReGAL provides a way of automatically discovering such abstractions and reusing them via helper functions.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      'In this section, we describe the overall pipeline of our method: **Re**factoring for **G**eneralizable **A**bstraction **L**earning (ReGAL). ReGAL consists of two phases: the _training_ or induction stage where abstractions (i.e., helper functions) are learned, and the _testing_ or synthesis stage, where abstractions are used to generate programs for test queries. During training, as illustrated in Fig. 2, ReGAL discovers reusable abstractions by generating candidate helper functions, validating their correctness, and debugging via editing and pruning of ineffective helper functions.\n' +
      '\n' +
      'Given a set of demonstrations \\((q,p)\\) of queries \\(q\\) and gold primitive programs \\(p\\), we first preprocess the data to cluster examples based on query similarity, described in Sec. 3.1. The training stage then builds abstractions by refactoring primitive programs in batches (Sec. 3.2), while the testing stage solves new queries by generating programs that glue together the learned abstractions with primitive operations (Sec. 3.3). We use GPT-3.5 for all our training prompts; at test time we experiment with a range of LLMs, focusing on freely available open-source LLMs.\n' +
      '\n' +
      '### Preprocessing\n' +
      '\n' +
      'Before training, we preprocess queries and programs \\((q,p)\\) by (optionally) adding comments, clustering them into related batches, and sorting them by approximate difficulty.\n' +
      '\n' +
      'Adding Comments.We optionally add comments to align the query with the primitive program, enabling the model to generate the correct abstractions. We present each \\((q,p)\\) pair to GPT-3.5 independently, with a prompt asking the model to comment \\(p\\) based on \\(q\\); we then verify that the commented code executes to the same result.\n' +
      '\n' +
      'Clustering Data.In order to form abstractions that are _shared_ between examples, the refactoring LLM requires a multi-instance scope, i.e. it must receive a batch of related \\((q,p)\\) tuples at a time. We implement this by clustering examples using an embedding of the query \\(q\\). Specifically, we embed each query using OpenAI\'s Ada embedding model (OpenAI, 2022) and hierarchically cluster the embeddings using Ward\'s agglomerative clustering algorithm (Ward Jr, 1963), implemented via Scikit-Learn (Pedregosa et al., 2011). This gives us a tree of related examples, which we topologically sort and group into batches of \\(k\\) related examples, where \\(k\\) is a hyperparameter (see Appendix C for all hyperparameter values).\n' +
      '\n' +
      'Curriculum.Intuitively, shorter and easier programs should contain abstractions that can be reused in harder, more compositional programs, so we sort examples into a curriculum (Bengio et al., 2009). To approximate difficulty, we sort the batches based on the average length (in tokens) of their queries. See Appendix A.1 for preprocessing details.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'ReGAL\'s training data consists pairs of queries \\(q\\) and primitive programs \\(p\\). The training phase outputs: 1) the _Code Bank_ (\\(C\\)): the library of helper functions abstracted out during training and 2) the _Demo Bank_ (\\(D\\)): examples of the functions being used. As shown in Fig. 2, the training phase is an iterative process where the LLM receives as input a batch of queries and primitive programs and then proposes helper functions that can be abstracted (Stage 1). For each batch, candidate helper functions are evaluated based on the correctness of the refactored programs that occur in (Stage 2a). After verification, failed examples are isolated into a second batch and re-attempted (Stage 2b). To improve the quality of the resultant Code Bank, we periodically edit helper functions after a fixed number of batches to improve their pass rate (over unit tests) and prune ineffective helper functions (Stage 3). At the end of training, the resultant library of helper functions - the code bank \\(C\\) - is stored for further use during testing along with successful demonstrations of refactored programs using helper functions - the demo bank \\(D\\). Note that the training process can be repeated over multiple epochs. Below we describe each stage in detail, with the overall algorithm detailed in Algorithm 1.\n' +
      '\n' +
      'Stage (1): Refactoring Examples (refactorBatch).The main module of the refactoring process takes as input a batch of examples, a set of instructions, and the current set of helper functions in the code bank (if any). It prompts the refactoring LLM for a set of new helper functions \\(H^{new}\\) along with refactored versions of each program that uses helpers from \\(H^{new}\\) when appropriate.\n' +
      '\n' +
      'Stage (2a): Verification (verify).To avoid introducing errors, we need to verify the helper functions and refactored programs generated by the LLM by executing them and comparing the results to the original, i.e. determining if \\(\\hat{p}(0)=p()\\). The refactored program \\((q,\\hat{p})\\) is stored as a demonstration for future use by the agent (cf. Sec. 3.3) if it passes verification. Only helper functions that pass verification are added to \\(C\\). We also store a record of programs that _failed_ verification, as these will be crucial in editCodeBank() and pruneCodeBank(), which improve existing functions and prune functions leading to failures.\n' +
      '\n' +
      'Stage (2b): Feedback-based Retrial (retry).If a program fails to pass verification, we optionally retry the refactoring process. In a follow-up prompt, we present failed programs and their helper functions. We also include environment feedback from execution (i.e. the output or error message produced).2 The refactoring LLM then produces a new version of each failed program; these are verified and their helpers are added to \\(C\\) if correct.\n' +
      '\n' +
      'Footnote 2: We do not include the output for LOGO as it is an image.\n' +
      '\n' +
      'Stage (3a): Editing Code Bank (editCodeBank).From the verify() module, some helper functions fail to pass all unit tests because they contain incorrect abstractions; for example, a function like draw_triangle() might start with a hardcoded value for a small size, leading it to fail on medium triangles. To update such functions, we construct a prompt for each function in \\(D\\) that shows the LLM passing and failing unit tests and asks it to propose edits to the function; this occurs once every editEvery iterations, where editEvery is a hyperparameter. We replace a function if it passes more unit tests after editing.\n' +
      '\n' +
      'Stage (3b): Pruning Code Bank (pruneCodeBank).In this module, we prune helper functions added to \\(C\\) that fail a majority of unit tests and cannot be improved further via editing. For each function, we derive a score based on the success rate of programs using the function; we set a threshold below which functions are pruned (shared by all domains). See Appendix A.2 for further details.\n' +
      '\n' +
      'We use the dev set to select hyperparameter values, reported in Appendix C. All prompts can be found in Appendix D.\n' +
      '\n' +
      '### Testing\n' +
      '\n' +
      'At test time, we deploy a program synthesis - or semantic parsing - _agent_ that makes predictions for test examples, one at a time. Unlike related work on using learned tools\n' +
      '\n' +
      'Figure 2: ReGAL starts by refactoring a batch of primitive programs to develop a set of modified programs and helper functions (**Stage 1**). It then verifies the results of refactored programs, optionally retrying failed programs according to environment feedback. Useful helper functions are added to the Code Bank along with example usage added to the Demo Bank (**Stage 2**). Periodically, we edit and prune the Code Bank to improve its functions (**Stage 3**). At test time, ReGAL agent has access to the Code Bank, the Demo Bank, and the remaining original programs. It is compared against a baseline agent which has access to a larger number of original programs.\n' +
      '\n' +
      '(Yuan et al., 2023; Qian et al., 2023; Wong et al., 2023), we explore a variety of open-source LLMs, in addition to a black-box LLM (GPT-3.5). Following effective strategies in semantic parsing and in-context learning (Shin and Van Durme, 2022; Roy et al., 2022; Bogin et al., 2023; Liu et al., 2022; Yasunaga et al., 2023), for each test example, the agent constructs a prompt with in-context learning (ICL) examples retrieved from a training corpus, followed by a test query. The examples are retrieved from the training data using vector similarity between the training queries and the test query. Further details in Appendix A.3.\n' +
      '\n' +
      '**ReGAL-augmented Agent.** Our agent has access to the _training data_ and _code bank_, as well as examples of refactored programs in the _demo bank_. The ICL budget (10 examples for all experiments) is split between primitive training examples and refactored ones.3 In addition to these demonstrations, the augmented agent retrieves up to 20 relevant helper functions from the code bank, where relevance is measured by the similarity between the query and the function name and description. These helper functions are concatenated into the prompt. The final input is a prompt containing the instructions, the retrieved helper functions, the mixed ICL examples, and the test query. To encourage the model to use helper functions, we include a ReAct-style prompt (Yao et al., 2023) that first asks the model to _think_ about which functions might be relevant based on the query and then generate the code.4\n' +
      '\n' +
      'Footnote 3: We found it necessary to keep some primitive programs as ICL examples, as not all test queries can be handled by helper functions. We treat this ratio of primitive and refactored programs as a hyperparameter.\n' +
      '\n' +
      'Footnote 4: Without these additional “thought” statements, we found the augmented agent rarely uses any helper functions.\n' +
      '\n' +
      '## 4 Experimental Setup\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'We explore three datasets: LOGO, Date understanding, and TextCraft. A common thread through these datasets is that they contain heterogenous problems requiring multiple helper functions as opposed to problems like sorting, which are challenging for LLMs but can be solved with a single function (Dziri et al., 2023). Statistics for the datasets are given in Table 4. Note that the number of primitives reflects functions not built into Python - all models also have access to all Python functions. See Appendix A.5 for further details about each dataset and its primitive operations.\n' +
      '\n' +
      '**LOGO.** LOGO is based on the Logo Turtle graphics domain-specific language (Abelson and DiSessa, 1986), with which basic graphics can be drawn by controlling a pen (the "turtle") that draws as it moves through space, using commands like \\(\\texttt{forward}(\\texttt{dist})\\) and \\(\\texttt{left}(\\texttt{theta})\\). The data we use is based on the Ellis et al. (2021)\'s LOGO dataset, re-annotated by Wong et al. (2021). For easier prediction by LLMs, we parse the data into abstract syntax trees and write a set of rules for translating these into Python; we release this rewritten data. We use the "small" train/test splits (200/111) from Wong et al. (2021) and take \\(100\\) dev examples from the "large" train set.\n' +
      '\n' +
      '**Date.** We use the date understanding task from BigBench-Hard (Srivastava et al., 2022; Suzgun et al., 2022), which consists of short word problems requiring date understanding. We obtain silver programs from Lyu et al. (2023)\'s predictions. Specifically, we split their predicted programs from GPT-3.5 into train, dev, and test splits (66/113/180) and filter the train split by correctness.\n' +
      '\n' +
      '**TextCraft.** To explore the utility of ReGAL in LLMs-as-agent settings (Liu et al., 2023), we use the TextCraft dataset (Prasad et al., 2023) that requires an agent to craft Minecraft items within a text-only environment (Cote et al., 2019). Each task instance in TextCraft consists of a goal (query) and a series of 10 crafting commands that contain recipes for related items including distractors. Unlike Prasad et al. (2023), who use TextCraft in an interactive setting where the LLM agent receives textual feedback from the environment at each step, we ask the agent to generate a single Python program for executing the entire task at once, making the task _more challenging_. We evaluate on the depth 2 split of the test set used in Prasad et al. (2023) while using a subset of depth 1 recipe examples for our dev set, giving us a train/dev/test split of 190/50/77.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**Baselines from Prior Work.** We compare ReGAL against relevant external baselines from past work. However, note that multiple methodological differences in our work, like the use of ICL examples and the format of the output programming language, give our agent an inherent advantage over these baselines. Thus, we refer to these numbers primarily to highlight the strength of our baseline agent. For LOGO, we use the "offline synthesis" numbers reported by Grand et al. (2023), which resemble our train/test setting; however, we note that Grand et al. (2023) predict programs in their original Lisp format and use a different agent model. For the Date dataset, we run Lyu et al. (2023)\'s Faithful-CoT method on our custom test split using gpt-3.5-turbo. While the output format and models used are the same, both our baseline and ReGAL use retrieved examples for in-context learning, while Lyu et al. (2023) do not. Furthermore, our ICL examples are based on filtered programs generated by Lyu et al. (2023), leading to better performance even from our baseline agent. Finally, for TextCraft we rerun Prasad et al. (2023)\'s baseline - based on ReAct (Yao et al., 2023) - on the depth-2 test set of TextCraft. Here, we use gpt-3.5-turbo-instruct, as Prasad et al. (2023) found it to outperform gpt-3.5-turbo.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'across multiple test instances. We argue that ReGAL improves over this paradigm by learning _shared_ abstractions. The results in Table 2 indicate that ReGAL offers large improvements over a baseline agent that lacks abstractions. Here, we verify that the abstractions learned are reusable, i.e. shared. Fig. 3 shows the number of times the top-5 most common ReGAL functions are called in test programs produced by the CodeLlama-13B agent. Across all datasets, we see that the helper functions learned by ReGAL are commonly reused, with the most relative reuse in TextCraft. Appendix B.1 shows examples of these common functions.\n' +
      '\n' +
      '## 6 Analysis\n' +
      '\n' +
      '### What kinds of programs are discovered?\n' +
      '\n' +
      'To further examine what kinds of helper functions are discovered, we examine the most frequent helper functions for each domain from Fig. 3, summarizing the results below. Refer to Appendix B.1 for function code. We find that distinct trends emerge across domains.\n' +
      '\n' +
      'For LOGO, ReGAL discovers functions that encapsulate different types of shapes. This is expected, as the LOGO data was generated with these functionalities in mind, i.e. the larger shapes are composed of objects like semicircles, pentagons, and circles. For Date, ReGAL tends to encapsulate single operations, prioritizing interpretability with names like get_date_one_year_ago(). While seemingly less complex than LOGO\'s functions, this approach aligns with the importance of function naming in synthesis procedures, as highlighted by Grand et al. (2023). In TextCraft, the functions uncovered by ReGAL are more complex and reflect the dynamics of the game. Specifically, the functions include conditional statements for checking ingredients, reflecting the fact that in TextCraft, having the correct crafting ingredients is a prerequisite for crafting an object (see craft_and_get_ingredient() in Fig. 2 and Table 10, which is taken from the learned code bank \\(C\\)).\n' +
      '\n' +
      '### Error Analysis\n' +
      '\n' +
      'To better understand how ReGAL aids program generation and also examine cases where it does not help, we perform a two-part error analysis. First, we examine cases where the ReGAL-augmented program was correct and the baseline agent\'s primitive program was incorrect. We then examine the opposite set of cases, where the baseline program was correct but the ReGAL program was incorrect.\n' +
      '\n' +
      'Fig. 1 shows the first kind of comparison on LOGO using the CodeLlama-13B model, where we qualitatively show an actual example that highlights the benefits of reuse and abstraction. The baseline program makes an error in calculating the polygon\'s interior angle when generating the program from scratch. This is avoided by the ReGAL agent, which simply uses a verified helper to generate the polygon correctly. The example also illustrates the importance of abstraction: as queries become more complex, generating a solution from scratch becomes more challenging. The baseline program reasons incorrectly about code outside of the shape, failing to use embed() correctly. Meanwhile, the augmented program offloads reasoning about the shape to an easily-matched function, and is able to correctly use embed(). To quantify these trends, we manually inspect the output of the baseline CodeLlama-13B on LOGO on the 25 cases where the ReGAL agent was correct, categorizing them into errors involving reasoning (first example in Fig. 1) and shape-internal errors (second example); we find 16 reasoning and 9 shape errors. We also examine ReGAL\'s failure modes by manually inspecting all cases where the augmented agent failed and the baseline succeeded, again using CodeLlama-13B on LOGO; there are 13 such cases. We categorize them into three types:\n' +
      '\n' +
      '* **Incorrect connector code**: (7 exs.) the program fails due to mistakes in the primitive operations or control flow.\n' +
      '* **Incorrect/undefined function**: (4 exs.) the code refers to non-existent functions, or incorrectly calls a function similar to the correct one.\n' +
      '* **Verification failure**: (2 exs.) the program was correct but the verification function gives a false negative.5 Footnote 5: For example, for “_a small square next to a small 6 gon_” the agent generates the hexagon to the left of the square, where in the reference it is to the right.\n' +
      '\n' +
      'Thus, the most common error is a failure to predict primitive operations; here, the ReGAL agent is at a disadvantage w.r.t. the baseline agent, since both have the same ICL budget. The baseline agent sees 10 ICL examples with _only_ primitive code, while the ReGAL agent sees 5 primitive examples and 5 Demo Bank examples.\n' +
      '\n' +
      'Figure 3: Function usage for CodeLlama-13B for the top-5 most common helpers. Functions are reused across programs.\n' +
      '\n' +
      '### Sample Efficiency\n' +
      '\n' +
      'As mentioned in Sec. 4.2, both baseline and ReGAL agents rely on demonstrations of queries and gold programs (\\(X\\)) to retrieve most similar ICL examples. Additionally, ReGAL uses the same demonstrations to learn helper functions in the code bank \\(C\\). We now study how the performance of both agents scales with the size of annotated gold programs in train set \\(X\\) using the CodeLlama-13B model on TextCraft. From Fig. 4, we observe that the ReGAL agent consistently outperforms the baseline agent as we vary the number of training examples. Notably, helper functions learned by ReGAL yield a \\(2.56\\%\\) improvement with as few as 25 demonstrations and an \\(8.45\\%\\) improvement with nearly half the size of the train set used in Sec. 5. Additionally, the performance of both the baseline and ReGAL agents improves as the number of demonstrations increases. This is expected as both agents benefit from the retrieval of demonstrations similar to the test query as the training set becomes larger and consequently more diverse.\n' +
      '\n' +
      '## 7 Discussion and Conclusion\n' +
      '\n' +
      'Fixed vs. Variable Costs.In Sec. 5, ReGAL was especially effective for open-source LLMs like CodeLlama. This result is encouraging, as it indicates that we can bring freely available and open-source models up to at least the same performance as a proprietary, closed-source model (if not more) using ReGAL abstractions. Thus, we can convert a variable cost - running an LLM on test data, which scales linearly with the size of the test set - into the fixed cost of running ReGAL to learn a library of helper functions.\n' +
      '\n' +
      'Connections to Semantic Parsing.Executable semantic parsing (Winograd, 1972; Zelle & Mooney, 1996) typically involves mapping queries to a domain-specific language (DSL); the DSL specifies a set of abstractions that are useful for a particular goal, e.g. SQL operations for querying databases, or spatial abstractions for robotics. These DSLs are typically defined by a human; one way to view ReGAL is as a way of _learning_ a DSL on top of an extremely general set of primitives. One of the benefits of ReGAL is its generality: on three different domains, it is able to learn useful abstractions without human intervention, while, in a standard semantic parsing setting, these abstractions would need to be manually specified.\n' +
      '\n' +
      'Connections to Hierarchical Reinforcement Learning.One way to view the functions discovered by ReGAL is as low-level policies - or skills - composed of primitive actions. In this view, our system is similar to hierarchical reinforcement learning (HRL; Barto & Mahadevan, 2003), where tasks are factorized into skill selection and skills themselves. In hierarchical frameworks, there is typically a high-level controller policy that selects lower-level policies. In our setting, the agent LLM acts as a controller, selecting from a set of skills, while ReGAL\'s training stage is responsible for discovering a useful set of skills; this is akin to option discovery (Sutton et al., 1999), where closed-loop policies for specific skills are learned from a reward signal. While ReGAL has a similar hierarchical structure, it differs from HRL in that ReGAL\'s skills are symbolic, interpretable, and editable, as opposed to HRL skill policies, which typically have none of these features.\n' +
      '\n' +
      'Limitations.As mentioned in connection to HRL, the functions ReGAL learns are code-based. This can make them less flexible than functions parameterized by neural networks (e.g. Andreas et al. (2016)), especially in domains where the environment can change dynamically. For instance, consider a navigation domain where an agent needs to get to the kitchen in various homes; depending on the primitive actions, a function that succeeds in one home will likely fail in another. However, ReGAL\'s verification-based pruning means that no function would be discovered for this navigation task. Relatedly, not every domain has reusable abstractions, and not every example stands to benefit from them. In many cases, the primitives for a domain are the right level of abstraction for that domain, e.g. if they already form a DSL. The abstractions are also not always ideal; in Appendix B.1 we see that ReGAL\'s abstractions are not necessarily the same as those a human would choose, e.g. a human would likely write a function like draw_5gon(size) rather than draw_small_5gon().\n' +
      '\n' +
      'Conclusion.We introduce ReGAL, a gradient-free approach to learning abstractions from a small set of examples. Our experimental results show that abstractions from ReGAL improve the accuracy of programs predicted by a variety of LLMs across three diverse domains. Furthermore, ReGAL abstractions are reusable and general, allowing them to be applied across examples for a given task. In our analysis, we find that the functions learned by ReGAL codify commonly-used subroutines as well as task dynamics. Our error analysis indicates that ReGAL\'s improvements come both from function reuse as well as simplification of the reasoning involved in program prediction.\n' +
      '\n' +
      'Figure 4: ReGAL programs yield a higher success rate (accuracy) compared to primitive programs on TextCraft for different sizes of training set \\(X\\) using CodeLlama-13B.\n' +
      '\n' +
      '## 8 Broader Impacts\n' +
      '\n' +
      'Our work aims to learn symbolic functions given a set of demonstrations; this has the potential to improve LLM predictions not only in terms of accuracy but also in terms of interpretability and trustworthiness. Unlike the mechanisms of an LLM itself, a Python function is natively interpretable by a human and can be debugged. Furthermore, results obtained by executing such a function are inherently faithful, in that we can identify the exact trace of operations that generated the result (Lyu et al., 2023). Our work does not have more potential for negative use than typical LLM-based systems and is subject to the biases inherent to these models and the datasets they are trained on (Weidinger et al., 2021). As with any system generating code, particular caution should be taken before executing snippets with the potential to damage the execution environment (Ruan et al., 2023).\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We thank Yichen Jiang, Justin Chen, Jaehong Yoon, and Swarnadeep Saha for their valuable feedback on the paper. This work was supported by NSF-AI Engage Institute DRL-2112635, DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, and Accelerate Foundation Models Research program. The views contained in this article are those of the authors and not of the funding agency.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abelson & DiSessa (1986) Abelson, H. and DiSessa, A. _Turtle geometry: The computer as a medium for exploring mathematics_. MIT press, 1986.\n' +
      '* Ahn et al. (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse, J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can and not as i say: Grounding language in robotic affordances. In _arXiv preprint arXiv:2204.01691_, 2022.\n' +
      '* Andreas et al. (2016) Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. Neural module networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 39-48, 2016.\n' +
      '* Barto & Mahadevan (2003) Barto, A. G. and Mahadevan, S. Recent advances in hierarchical reinforcement learning. _Discrete event dynamic systems_, 13(1-2):41-77, 2003.\n' +
      '* Bengio et al. (2009) Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In _Proceedings of the 26th annual international conference on machine learning_, pp. 41-48, 2009.\n' +
      '* Bogin et al. (2023) Bogin, B., Gupta, S., Clark, P., and Sabharwal, A. Leveraging code to improve in-context learning for semantic parsing. _arXiv preprint arXiv:2311.09519_, 2023.\n' +
      '* Bowers et al. (2023) Bowers, M., Olaussson, T. X., Wong, L., Grand, G., Tenenbaum, J. B., Ellis, K., and Solar-Lezama, A. Top-down synthesis for library learning. _Proceedings of the ACM on Programming Languages_, 7(POPL):1182-1213, 2023.\n' +
      '* Cai et al. (2023) Cai, T., Wang, X., Ma, T., Chen, X., and Zhou, D. Large language models as tool makers. _arXiv preprint arXiv:2305.17126_, 2023.\n' +
      '* Chen et al. (2022) Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _arXiv preprint arXiv:2211.12588_, 2022.\n' +
      '* Cho et al. (2023) Cho, J., Zala, A., and Bansal, M. Visual programming for text-to-image generation and evaluation. _Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* Cote et al. (2019) Cote, M.-A., Kadar, A., Yuan, X., Kybartas, B., Barnes, T., Fine, E., Moore, J., Hausknecht, M., El Asri, L., Adada, M., et al. Textworld: A learning environment for text-based games. In _Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7_, pp. 41-75. Springer, 2019.\n' +
      '* Downey (2012) Downey, A. _Think python_. " O\'Reilly Media, Inc.", 2012.\n' +
      '* Dziri et al. (2023) Dziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., et al. Faith and fate: Limits of transformers on compositionality. _arXiv preprint arXiv:2305.18654_, 2023.\n' +
      '* Ellis et al. (2021) Ellis, K., Wong, L., Nye, M., Sable-Meyer, M., Morales, L., Hewitt, L., Cary, L., Solar-Lezama, A., and Tenenbaum, J. B. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In _Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation_, pp. 835-850, 2021.\n' +
      '* Flynn & Jacobs (1987) Flynn, B. B. and Jacobs, F. R. Applications and implementation: an experimental comparison of cellular (group technology) layout with process layout. _Decision Sciences_, 18(4):562-581, 1987.\n' +
      '* Furnkranz et al. (2012) Furnkranz, J., Gamberger, D., and Lavrac, N. _Foundations of rule learning_. Springer Science & Business Media, 2012.\n' +
      '\n' +
      '* Futuyma & Moreno (1988) Futuyma, D. J. and Moreno, G. The evolution of ecological specialization. _Annual review of Ecology and Systematics_, 19(1):207-233, 1988.\n' +
      '* Grand et al. (2023) Grand, G., Wong, L., Bowers, M., Olaussson, T. X., Liu, M., Tenenbaum, J. B., and Andreas, J. Learning interpretable libraries by compressing and documenting code. In _Intrinsically-Motivated and Open-Ended Learning Workshop@ NeurIPS2023_, 2023.\n' +
      '* Gupta et al. (2018) Gupta, S., Shah, R., Mohit, M., Kumar, A., and Lewis, M. Semantic parsing for task oriented dialog using hierarchical representations. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 2787-2792, 2018.\n' +
      '* Gupta & Kembhavi (2023) Gupta, T. and Kembhavi, A. Visual programming: Compositional visual reasoning without training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14953-14962, 2023.\n' +
      '* Huang et al. (2022) Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. _arXiv preprint arXiv:2201.07207_, 2022.\n' +
      '* Huang et al. (2023) Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L. Voxposer: Composable 3d value maps for robotic manipulation with language models. In _Conference on Robot Learning_, pp. 540-562. PMLR, 2023.\n' +
      '* Khot et al. (2023) Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solving complex tasks. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Lake et al. (2015) Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. _Science_, 350(6266):1332-1338, 2015.\n' +
      '* Liu et al. (2022) Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good in-context examples for GPT-3? In Agirre, E., Apidianaki, M., and Vulic, I. (eds.), _Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures_, pp. 100-114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics.\n' +
      '* Liu et al. (2023) Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. _arXiv preprint arXiv:2308.03688_, 2023.\n' +
      '* Lu et al. (2023) Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-play compositional reasoning with large language models. _arXiv preprint arXiv:2304.09842_, 2023.\n' +
      '* Lyu et al. (2023) Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M., and Callison-Burch, C. Faithful chain-of-thought reasoning. _arXiv preprint arXiv:2301.13379_, 2023.\n' +
      '* Majumder et al. (2023) Majumder, B. P., Mishra, B. D., Jansen, P., Tafjord, O., Tandon, N., Zhang, L., Callison-Burch, C., and Clark, P. Clin: A continually learning language agent for rapid task adaptation and generalization. _arXiv preprint arXiv:2310.10134_, 2023.\n' +
      '* Marcus et al. (1999) Marcus, G. F., Vijayan, S., Bandi Rao, S., and Vishton, P. M. Rule learning by seven-month-old infants. _Science_, 283(5398):77-80, 1999.\n' +
      '* McConnell (2004) McConnell, S. _Code complete_. Pearson Education, 2004.\n' +
      '* Mialon et al. (2023) Mialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. _arXiv preprint arXiv:2302.07842_, 2023.\n' +
      '* O\'Donnell (2015) O\'Donnell, T. J. _Productivity and reuse in language: A theory of linguistic computation and storage_. MIT Press, 2015.\n' +
      '* OpenAI (2022) OpenAI. New and improved embedding model, 2022. URL [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model).\n' +
      '* Pedregosa et al. (2011) Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.\n' +
      '* Prasad et al. (2023) Prasad, A., Koller, A., Hartmann, M., Clark, P., Sabharwal, A., Bansal, M., and Khot, T. Adapt: As-needed decomposition and planning with language models. _arXiv preprint arXiv:2311.05772_, 2023.\n' +
      '* Qian et al. (2023) Qian, C., Han, C., Fung, Y., Qin, Y., Liu, Z., and Ji, H. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 6922-6939, 2023.\n' +
      '* Qin et al. (2023) Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. ToollIm: Facilitating large language models to master 16000+ real-world apis. _arXiv preprint arXiv:2307.16789_, 2023.\n' +
      '* Roy et al. (2022) Roy, S., Thomson, S., Chen, T., Shin, R., Pauls, A., Eisner, J., and Van Durme, B. Benchmarkp: A benchmark for evaluating language models on semantic parsing. _arXiv preprint arXiv:2206.10668_, 2022.\n' +
      '\n' +
      '* Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.\n' +
      '* Ruan et al. (2023) Ruan, Y., Dong, H., Wang, A., Pitis, S., Zhou, Y., Ba, J., Dubois, Y., Maddison, C., and Hashimoto, T. Identifying the risks of LM agents with an LM-emulated sandbox. In _NeurIPS 2023 Foundation Models for Decision Making Workshop_, 2023.\n' +
      '* Saha et al. (2022) Saha, S., Zhang, S., Hase, P., and Bansal, M. Summarization programs: Interpretable abstractive summarization with neural modular trees. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.\n' +
      '* Shin & Van Durme (2022) Shin, R. and Van Durme, B. Few-shot semantic parsing with language models trained on code. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 5417-5425, 2022.\n' +
      '* Singh et al. (2023) Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. ProPrompt: Generating situated robot task plans using Large Language Models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 11523-11530. IEEE, 2023.\n' +
      '* Srivastava et al. (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.\n' +
      '* Suris et al. (2023) Suris, D., Menon, S., and Vondrick, C. ViperGPT: Visual inference via Python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.\n' +
      '* Sutton et al. (1999) Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Artificial intelligence_, 112(1-2):181-211, 1999.\n' +
      '* Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Wang et al. (2023) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.\n' +
      '* Ward Jr (1963) Ward Jr, J. H. Hierarchical grouping to optimize an objective function. _Journal of the American statistical association_, 58(301):236-244, 1963.\n' +
      '* Weidinger et al. (2021) Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.\n' +
      '* Winograd (1972) Winograd, T. Understanding natural language. _Cognitive psychology_, 3(1):1-191, 1972.\n' +
      '* Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 38-45, Online, October 2020. Association for Computational Linguistics.\n' +
      '* Wong et al. (2021) Wong, L., Ellis, K. M., Tenenbaum, J., and Andreas, J. Leveraging language to learn program abstractions and search heuristics. In _International Conference on Machine Learning_, pp. 11193-11204. PMLR, 2021.\n' +
      '* Wong et al. (2023) Wong, L., Mao, J., Sharma, P., Siegel, Z. S., Feng, J., Korneev, N., Tenenbaum, J. B., and Andreas, J. Learning adaptive planning representations with natural language guidance. _arXiv preprint arXiv:2312.08566_, 2023.\n' +
      '* Yang (2016) Yang, C. _The price of linguistic productivity: How children learn to break the rules of language_. MIT press, 2016.\n' +
      '* Yao et al. (2023) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Yasunaga et al. (2023) Yasunaga, M., Chen, X., Li, Y., Pasupat, P., Leskovec, J., Liang, P., Chi, E. H., and Zhou, D. Large language models as analogical reasoners. _arXiv preprint arXiv:2310.01714_, 2023.\n' +
      '* Yuan et al. (2023) Yuan, L., Chen, Y., Wang, X., Fung, Y. R., Peng, H., and Ji, H. Craft: Customizing llms by creating and retrieving from specialized toolsets. _arXiv preprint arXiv:2309.17428_, 2023.\n' +
      '* Zelle & Mooney (1996) Zelle, J. M. and Mooney, R. J. Learning to parse database queries using inductive logic programming. In _Proceedings of the national conference on artificial intelligence_, pp. 1050-1055, 1996.\n' +
      '\n' +
      '## Appendix A Methods\n' +
      '\n' +
      '### Preprocessing\n' +
      '\n' +
      '**Adding comments.** To add comments, we first use a zero-shot prompt to break the query down into its constituent parts; for example, a LOGO query like _"Place 4 small semicircles in a row"_ is broken down into _"1. place semicircles 2. small semicircles 3. in a row 4. 4 semicircles._ We then include this decomposition in a prompt asking the model to add comments to the code. After adding comments, we verify the code first with exact match (excluding comment strings) and then use execution accuracy if exact match fails.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'Code Bank Editing.Our Code Bank editing prompt asks the model to produce a CoT-style output, first specifying _why_ the failing unit tests failed and then proposing an edit for the function. We then execute the stored demonstrations for that function with the new version; if there are more passing cases after refactoring, we replace the function. If the new function\'s signature differs from the old, we use a simple prompt to refactor the unit tests to accommodate the new function.\n' +
      '\n' +
      'Code Bank Pruning.For each function, given a set of passing programs \\(P\\) and failing programs \\(F\\), we compute a score \\(s=|P|-\\sum_{p\\in F}1/n_{p}\\), where \\(n_{p}\\) is the number of functions used in \\(p\\). In other words, the function receives \\(+1\\) for each passing program it participates in, and a negative score inversely proportional to the number of functions in the program (since naively, the failure could be attributed to any of the functions). Functions are pruned if they have been used a sufficient number of times and \\(s\\) falls below a threshold \\(\\theta\\) (set to 0 for all experiments).\n' +
      '\n' +
      '### Testing\n' +
      '\n' +
      'In our test-time agent, we use ChromaDB for indexing and retrieval6 with OpenAI Ada embeddings. ICL examples are retrieved from the training data and from the Demo Bank using query similarity. We limit the number of Code Bank functions to 20, using the similarity between the query and the function name for retrieval. The Code Bank is pruned once before testing.\n' +
      '\n' +
      'Footnote 6: [https://github.com/chroma-core/chroma/](https://github.com/chroma-core/chroma/)\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      'For GPT-3.5, we use the gpt-3.5-turbo version (0613). All CodeLlama models use the CodeLlama- * -Instruct-hf versions, and we use the lemur-70b-v1 version of Lemur. For the latter two open-source models, we use the checkpoints from HuggingFace (Wolf et al., 2020).\n' +
      '\n' +
      '```\n' +
      'Input:\\(X=(q,p)\\) // Training data: (query, program) Params: editEvery, pruneEvery, threshold \\(\\theta\\) Output: CodeBank \\(C\\), DemoBank \\(D\\) \\(C\\leftarrow\\varnothing,D\\leftarrow\\varnothing\\) // Initialization, i.e., no refactoring // Preprocessing data via clustering and sorting by difficulty \\(\\mathcal{P}\\leftarrow\\texttt{preprocessAndGroupData}(X)\\) for index\\(g\\), batch\\(\\mathcal{G}\\in\\mathcal{P}\\)do //Refactor programs in group \\(\\mathcal{G}\\) based on the current CodeBank \\(C\\). Returns new programs and helper functions. \\((p_{1}^{new},h_{1}),\\ldots,(p_{n}^{new},h_{k})\\) =\\(\\texttt{refactorBatch}(\\mathcal{G},C)\\) \\(H^{new}\\leftarrow\\{h_{1},\\cdots,h_{k}\\}\\) // Set of new helper functions \\(H\\) // Verifying that the gold program and the refactored program yield the same result when executed via indicator \\(\\delta^{new}\\). \\(\\delta^{new}_{1:k}\\leftarrow\\texttt{verify}(H,C,\\{p_{i}^{new}\\}_{i=1}^{k},\\{p_{ i}\\}_{i=1}^{k})\\) for\\(i\\in\\{i:\\delta^{new}_{i}=False\\}\\)do \\((p_{i}^{retry},h_{i}^{retry})\\leftarrow\\texttt{retry}(p_{i},p_{i}^{new},C)\\) \\(\\delta^{new}_{i}\\leftarrow\\texttt{verify}(h^{retry}_{i}\\cup H,C,p^{new},p)\\) if\\(\\delta^{new}_{i}\\)=\\(True\\)then// Update if retry succeeds \\(p_{i}^{new}\\gets p_{i}^{retry}\\) \\(H^{new}[i]\\gets h_{i}^{retry}\\) // update CodeBank \\(C\\) with successful helper functions \\(C\\gets C+H^{new}[i]\\) for\\(i\\in\\{i:\\delta^{new}_{i}=True\\}\\) // update DemoBank \\(D\\) with refactored programs for\\(i\\in\\{1,\\ldots,k\\}\\)do \\(D\\gets D+(p_{i}^{new},\\delta^{new}_{i})\\) // edit and prune CodeBank if\\(g\\pmod{\\text{editEvery}}=0\\)then \\(C\\leftarrow\\texttt{editCodeBank}(C,D)\\) if\\(g\\pmod{\\text{pruneEvery}}=0\\)then \\(C,D\\leftarrow\\texttt{pruneCodeBank}(C,D,\\theta)\\) return\\(C,D\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1**ReGAL: Training Algorithm\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '**Input:**\\(X=(q,p)\\) // Training data: (query, program) Params: editEvery, pruneEvery, threshold \\(\\theta\\) Output: CodeBank \\(C\\), DemoBank \\(D\\) \\(C\\leftarrow\\varnothing,D\\leftarrow\\varnothing\\) // Initialization, i.e., no refactoring // Preprocessing data via clustering and sorting by difficulty \\(\\mathcal{P}\\leftarrow\\texttt{preprocessAndGroupData}(X)\\) for index\\(g\\), batch\\(\\mathcal{G}\\in\\mathcal{P}\\)do //Refactor programs in group \\(\\mathcal{G}\\) based on the current CodeBank \\(C\\). Returns new programs and helper functions. \\((p_{1}^{new},h_{1}),\\ldots,(p_{n}^{new},h_{k})\\) =\\(\\texttt{refactorBatch}(\\mathcal{G},C)\\) \\(H^{new}\\leftarrow\\{h_{1},\\cdots,h_{k}\\}\\) // Set of new helper functions \\(H\\) // Verifying that the gold program and the refactored program yield the same result when executed via indicator \\(\\delta^{new}_{1:k}\\leftarrow\\texttt{verify}(H,C,\\{p_{i}^{new}\\}_{i=1}^{k},\\{p_{ i}\\}_{i=1}^{k})\\) for\\(i\\in\\{i:\\delta^{new}_{i}=False\\}\\)do \\((p_{i}^{retry},h_{i}^{retry})\\leftarrow\\texttt{retry}(p_{i},p_{i}^{new},C)\\) \\(\\delta^{new}_{i}\\leftarrow\\texttt{verify}(h^{retry}_{i}\\cup H,C,p^{new},p)\\) if\\(\\delta^{new}_{i}\\)=\\(True\\)then// Update if retry succeeds \\(p_{i}^{new}\\gets p_{i}^{retry}\\) \\(H^{new}[i]\\gets h_{i}^{retry}\\) // Update CodeBank \\(C\\) with successful helper functions \\(C\\gets C+H^{new}[i]\\) for\\(i\\in\\{i:\\delta^{new}_{i}=True\\}\\) // update DemoBank \\(D\\) with refactored programs for\\(i\\in\\{1,\\ldots,k\\}\\)do \\(D\\gets D+(p_{i}^{new},\\delta^{new}_{i})\\) // edit and prune CodeBank if\\(g\\pmod{\\text{editEvery}}=0\\)then \\(C\\leftarrow\\texttt{editCodeBank}(C,D)\\) if\\(g\\pmod{\\text{pruneEvery}}=0\\)then \\(C,D\\leftarrow\\texttt{pruneCodeBank}(C,D,\\theta)\\)  return\\(C,D\\) ```\n' +
      '\n' +
      '**Algorithm 2**ReGAL: Training Algorithm\n' +
      '\n' +
      '#### a.5.1 Logo\n' +
      '\n' +
      'LOGO data is generated from a synchronous text-code grammar, and pairs procedurally-generated language commands like _"three small triangles in a row"_ with a corresponding Turtle graphics program; however, the original LOGO dataset is expressed in a Lisp-style functional syntax. While\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline\n' +
      '**Dataset** & **Train** & **Dev** & **Test** & **\\# Primitives** \\\\ \\hline LOGO & 200 & 100 & 111 & 9 \\\\ Date & 66 & 113 & 180 & 4 \\\\ TextCraft & 190 & 50 & 77 & 3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Dataset statistics. We list the number of primitive functions in the programs (aside from built-in Python functions).\n' +
      '\n' +
      'this facilitates the application of helpful data structures for efficient code search (Ellis et al., 2021; Bowers et al., 2023), object-oriented languages like Python are far more common in practice. As a result, they are represented more in LLM pretraining data, which has been shown to contribute to parsing performance (Bogin et al., 2023). To account for this, we write an AST-based parsing script to translate the LOGO dataset into Python.\n' +
      '\n' +
      'Primitives.Table 5 describes the primitives available in the LOGO library. Note that these are in addition to all Python primitives. We also provide all agents with several hard-coded values for long loops and small steps so that they can draw round shapes. HALF_INF is the number of steps required to draw a semicircle. EPS_DIST is a small distance, and EPS_ANGLE is a small angle.\n' +
      '\n' +
      '#### a.5.2 Date understanding\n' +
      '\n' +
      'Date understanding involves both mathematical reasoning and parsing. Each question poses a word problem that requires reasoning about dates and times. For example, problems ask questions like: _"On May 9th, 2017 Jane bought 40 eggs. She ate one per day. Today she ran out of eggs. What is the date 10 days ago in MM/DD/YYYY?"_. These kinds of questions are especially hard for LLMs to answer directly. Lyu et al. (2023) approached this task as a program prediction task, wherein an LLM predicts a Python script that gives the answer to the question when executed. This paradigm is especially helpful for Date, as there are existing Python libraries that can perform math on dates, such as datetime and dateutil. While predicting programs with these libraries results in strong performance as compared to LLM-based reasoning, Lyu et al. (2023) method predicts programs one at a time, leaving the benefits of shared subroutines on the table. We use the programs predicted by Lyu et al. (2023) as a starting point for our refactoring process. Table 6 describes the Python libraries called by Lyu et al. (2023)\'s programs, which we treat as the primitives for Date.\n' +
      '\n' +
      '#### a.5.3 TextCraft\n' +
      '\n' +
      'TextCraft consists of goal queries paired with crafting recipes. Recipes are presented with distractors, making the parsing task challenging. Furthermore, the agent must reason about preconditions, as items can only be crafted when the requisite ingredients have been collected.\n' +
      '\n' +
      'The queries ask for a particular item to be crafted. For example the query can be "_craft behive_" along with crafting commands:\n' +
      '\n' +
      '_craft 4 oak planks using 1 oak log craft 1 honeycomb block using 4 honeycomb\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline\n' +
      '**Primitive** & **Description** \\\\ \\hline date() & returns a date object \\\\ \\hline time() & returns a time object \\\\ \\hline relativelydelta(time) & performs addition/sub- \\\\  & traction of time, which can be days, weeks, months, or years. \\\\ \\hline strftime(format) & prints the date in the specified format \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Date Primitives\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline\n' +
      '**Input:**\\(Q\\), \\(C\\), \\(D\\), \\(X\\) / Test queries \\(Q\\), Code Bank C, Demo Bank D, Training data \\(X=\\) (query, program) \\\\\n' +
      '**Params:** ICL Budget \\(M\\), ICL Percentage \\(r\\) \\\\\n' +
      '**Output:** Predicted programs \\(\\hat{P}\\) \\\\ \\(M^{demo}\\gets r*M\\) \\\\ \\(M^{train}\\gets M-M_{demo}\\) \\\\ \\(\\hat{P}\\leftarrow\\varnothing\\) \\\\\n' +
      '**for \\(q\\in X\\)**do** \\\\  \\(H\\leftarrow\\)**retrieve(\\(q,C,20\\))** / retrieve up to 20 helper \\\\  _functions conditioned on the query_ \\\\  _\\(X^{demo}\\leftarrow\\)retrieve(\\(q,D,M^{demo}\\))_ / retrieve helper \\\\  _demos from \\(D\\)_ \\\\  \\(X^{train}\\leftarrow\\)retrieve(\\(q,X,M^{train}\\))_ / retrieve primitive \\\\  _demos from \\(X\\)_ \\\\  \\(I\\leftarrow\\)createPrompt(\\(H,X^{demo},X^{train}\\))_ \\\\  \\(\\hat{p}\\gets LLM(I)\\) / generate program \\\\  \\(\\hat{P}\\leftarrow\\hat{P}+\\hat{p}\\) \\\\\n' +
      '**return**\\(\\hat{P}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: LOGO Primitives\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      'Please rewrite the following two programs to be more efficient. {primitive description string} The resulting programs MUST execute to the same results the original programs. Start by writing helper functions that can reduce the size of the code. You can also choose from the following helper functions: {codebank function definitions} {// repeated for all in batch QUERY {i}: {query} PROGRAM {i}: {program} Please formatyouranswer as: // repeated for i NEW PROGRAM {i}: // once at end NEW HELPERS:  Donotincludeanytext that is not valid Python code.  Recall that no matter what, your program MUST be formatted in the following fashion: // repeated fori NEW PROGRAM {i}: # Thoughts: # 1. The query asks for: <query intention>  2. <query> can be solved by <components>.  # 3. I will use helper function <function> to <goal>.  <code for program {i}> Try to make your new programs as short as possible by introducing shared helper functions.  Helper function parameters should be as general as possible and helper functions  should be informatively named.  {logo_special_instr} ```\n' +
      '``` logo_special_instructions= If the original functionuses \'embed\', you will likely need to use \'embed\' in your version . All code to be repeated needs to be included within the triple quotes passed to embed.\n' +
      '```\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} You are an expert coder. For each query below, decompose it intits parts.  Example:  Query: Do some action 5 times and then do another action  Query (decomposed):  The query asks: Do some action and then doanother action  This can be decomposed into: 1. repeat an action 2. some action 3. another action \\\\ Query: {query}  Query (decomposed): \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Query decomposition prompt. Output is used by the comment prompt in Table 14\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} Please add comments to the following program to explain what each chunk of code does with respect to the query. \\\\ First, decompose the query intoparts. Then comment the code with the queryparts. Example: \\\\ Query: Do someaction and then doanother action \\\\ Code: \\\\ do_some_action() \\\\ do_another_action() \\\\ Query: Do someaction 5 times and then doanother action \\\\ Query (decomposed): \\\\ The query asks: Do someaction and then doanother action \\\\ This can be decomposed into: \\\\\n' +
      '1. repeat an action \\\\\n' +
      '2. some action \\\\\n' +
      '3. another action \\\\ Commented code: \\\\\n' +
      '# repeat an action \\\\ for i inrange(5): \\\\\n' +
      '# do some action \\\\ do_some_action() \\\\\n' +
      'do another action \\\\ do_another_action() \\\\ \\{primitives description\\} \\\\ Query: \\{query\\} \\\\ Code: \\\\ \\{program\\} \\\\ Query (decomposed): \\\\ \\{output from decompose prompt\\} \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: Prompt to add comments to primitive programs. Takes output of Table 13 as input.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline Your tasks is tosolve simple word problems by creating Python programs. \\{codebank\\_str\\} \\\\ \\hline You will be given a query and have to produce a program. \\{thought\\_str\\} \\\\ Examples: \\{icl\\_string\\} \\\\ Please generate ONLY the code to produce the answer and nothing else. \\\\ Query: \\{query\\} \\\\ \\{thought\\_and\\}Program: \\\\ \\hline \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 16: Agent prompt for Python tasks like Date understanding. Note that the baseline and ReGAL agent use the same prompt, but \\(\\{codebank\\_str\\}\\) is empty for the baseline agent, and the ReGAL sees some demonstrations from the Demo Bank in \\(\\{\\)icl\\_string\\}.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline \\hline \\multicolumn{1}{l}{\\begin{tabular}{l} \\hline \\hline \\multicolumn{1}{l}{\n' +
      '\\begin{tabular}{l} \\hline \\hline \\end{tabular} } \\\\ \\hline \\hline \\end{tabular} \\\\ \\hline \\hline \\end{tabular} \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 15: Prompt for editCodeBank.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} YourtaskistodrawsimplifiguresusingpythonTurtlegraphics. \\\\ Youwilluseacustomturtlelibrary,similartothebuilt-inlibrary,whichissufficient \\\\ foralltasks. \\\\ Here’sadescriptionofthecustomlibrary: \\\\ -forward(x):moveforwardxpixels \\\\ -left(theta):rotateleftbythetageres \\\\ -right(theta):rotaterightbythetageres \\\\ -penup():stopdrawing \\\\ -pendow():startdrawing \\\\ -teleport(x,y,theta):movetoposition(x,y)withangletheta \\\\ -heading():getthecurrentangledoftheturtle \\\\ -isdown():checkifthepenisdown \\\\ -embed(program,local_vars):runsthecodeinprogramusingthecurrentcontextand \\\\ teleportsbacktotheoriginalposition.Allowsyoutoensetprograms. \\\\ Implementationally,embedgetstheturtlestate(is_down,x,y,heading),executes \\\\ program,thenreturnstotheoriginalstate. \\\\ -save(path):savethepicturetofile \\\\ \\(\\{\\)codebank_str\\} \\\\ Youwillbegivenaqueryandhavetoproducaprogram.Beginyourprogramwithacomment \\\\ thatexplainsyourreasoning.Forexample,youmightwrite:\\n#Thought:thequery \\\\ asksforaline,soIwillusetheforward()function. \\\\ Examples: \\\\ \\(\\{\\)icl_string\\} \\\\ PleasegenerateONLYthecodetopoducetheanswerandnothingelse. \\\\ Query:\\(\\{\\)query\\(\\}\\) \\\\ ThoughtandProgram: \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 17: Prompt for the LOGO agent.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>