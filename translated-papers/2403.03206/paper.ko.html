<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '고해상도 영상 합성을 위한 #스케일링 정류 유동변압기\n' +
      '\n' +
      ' 패트릭 에셔\n' +
      '\n' +
      '동등한 기여도. \\ (<\\)first.last\\(>\\)@stability.ai.\n' +
      '\n' +
      'Sumith Kulal\n' +
      '\n' +
      'Andreas Blattmann\n' +
      '\n' +
      'Rahim Entezari\n' +
      '\n' +
      'Jonas Muller\n' +
      '\n' +
      'Harry Saini\n' +
      '\n' +
      'Yam Levi\n' +
      '\n' +
      'Dominik Lorenz\n' +
      '\n' +
      'Axel Sauer\n' +
      '\n' +
      'Frederic Boesel\n' +
      '\n' +
      'Dustin Podell\n' +
      '\n' +
      'Tim Dockhorn\n' +
      '\n' +
      'Zion English\n' +
      '\n' +
      'Kyle Lacey\n' +
      '\n' +
      'Alex Goodwin\n' +
      '\n' +
      'Yannik Marek\n' +
      '\n' +
      'Robin Rombach\n' +
      '\n' +
      '동등한 기여도. \\ (<\\)first.last\\(>\\)@stability.ai.\n' +
      '\n' +
      'Stability AI\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 데이터의 전방 경로를 노이즈 쪽으로 반전시켜 노이즈로부터 데이터를 생성하고 이미지 및 비디오와 같은 고차원, 지각 데이터에 대한 강력한 생성 모델링 기술로 부상했다. 정류 흐름은 데이터와 잡음을 일직선으로 연결하는 최근의 생성 모델 공식이다. 더 나은 이론적 특성과 개념적 단순성에도 불구하고 아직 표준 관행으로 결정적으로 확립되지 않았다. 본 연구에서는 정류된 흐름 모델을 지각적으로 관련된 척도로 편향시켜 훈련하기 위한 기존의 노이즈 샘플링 기법을 개선한다. 대규모 연구를 통해 고해상도 텍스트 대 이미지 합성을 위해 확립된 확산 공식과 비교하여 이 접근법의 우수한 성능을 입증한다. 또한, 두 가지 양식에 대해 별도의 가중치를 사용하고 이미지와 텍스트 토큰 간의 양방향 정보 흐름을 가능하게 하여 텍스트 이해도, 타이포그래피 및 인간 선호도 등급을 향상시키는 텍스트 대 이미지 생성을 위한 새로운 트랜스포머 기반 아키텍처를 제시한다. 우리는 이 아키텍처가 예측 가능한 스케일링 경향을 따르고 다양한 메트릭 및 인간 평가에 의해 측정된 개선된 텍스트-이미지 합성과 더 낮은 검증 손실을 상관시킨다는 것을 입증한다. 우리의 가장 큰 모델은 최신 모델을 능가하며 실험 데이터, 코드 및 모델 가중치를 공개적으로 사용할 수 있도록 할 것입니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델들은 노이즈로부터 데이터를 생성한다(Song et al., 2020). 이들은 데이터의 순방향 경로를 랜덤 노이즈 쪽으로 반전시키도록 트레이닝되고, 따라서, 신경망들의 근사화 및 일반화 속성들과 함께, 트레이닝 데이터에 존재하지 않지만 트레이닝 데이터의 분포를 따르는 새로운 데이터 포인트들을 생성하는데 사용될 수 있다(Sohl-Dickstein et al., 2015; Song and Ermon, 2020). 이러한 생성적 모델링 기법은 이미지와 같은 고차원, 지각적 데이터를 모델링하는데 매우 효과적인 것으로 입증되었다(Ho et al., 2020). 최근, 확산 모델은 인상적인 일반화 능력을 갖는 자연 언어 입력으로부터 고해상도 이미지 및 비디오를 생성하기 위한 사실적 접근법이 되었다(Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Podell et al., 2023; Dai et al., 2023; Esser et al., 2023; Blattmann et al., 2023; Betker et al., 2023; Blattmann et al., 2023a; Singer et al., 2022). 그들의 반복적 특성 및 관련 계산 비용, 및 추론 동안의 긴 샘플링 시간으로 인해, 이들 모델의 보다 효율적인 트레이닝 및/또는 더 빠른 샘플링을 위한 제형에 대한 연구가 증가하였다(Karras et al., 2023; Liu et al., 2022).\n' +
      '\n' +
      '데이터에서 노이즈로의 전진 경로를 지정하면 효율적인 훈련으로 이어지지만, 어떤 경로를 선택해야 하는지에 대한 질문도 제기한다. 이 선택은 표본 추출에 중요한 의미를 가질 수 있다. 예를 들어, 데이터로부터 모든 잡음을 제거하지 못하는 순방향 프로세스는 트레이닝 및 테스트 분포의 불일치를 초래할 수 있고 그레이 이미지 샘플들과 같은 아티팩트들을 초래한다(Lin et al., 2024). 중요한 것은, 순방향 프로세스의 선택은 또한 학습된 역방향 프로세스 및 따라서 샘플링 효율성에 영향을 미친다는 것이다. 곡선 경로는 프로세스를 시뮬레이션하기 위해 많은 통합 단계를 필요로 하지만 직선 경로는 단일 단계로 시뮬레이션할 수 있으며 오류 누적이 적다. 각 단계는 신경망의 평가에 해당하므로 샘플링 속도에 직접적인 영향을 미친다.\n' +
      '\n' +
      '순방향 경로에 대한 특정 선택은 데이터 및 노이즈를 직선으로 연결하는 소위 _Rectified Flow_(Liu et al., 2022; Albergo and Vanden-Eijnden, 2022; Lipman et al., 2023)이다. 비록 이 모형 수업이 더 나은 이론적 속성을 가지고 있지만, 아직 실무에서 결정적으로 정립된 것은 아니다. 지금까지 중소형 실험(Ma et al., 2024)에서 몇 가지 장점이 경험적으로 입증되었지만, 이들은 대부분 클래스-조건부 모델에 국한된다. 본 연구에서는 잡음 예측 확산 모델(Ho et al., 2020)과 유사하게 정류 유동 모델에 잡음 척도의 재가중치를 도입하여 이를 변경한다. 대규모 연구를 통해 새로운 제형을 기존 확산 제형과 비교하고 그 이점을 입증한다.\n' +
      '\n' +
      '우리는 고정된 텍스트 표현이 (예를 들어, 교차-어텐션(Vaswani et al., 2017; Rombach et al., 2022)) 모델에 직접 공급되는 텍스트-이미지 합성을 위해 널리 사용되는 접근법이 이상적이지 않다는 것을 보여주고, 이미지 및 텍스트 토큰 모두에 대해 학습 가능한 스트림을 통합하는 새로운 아키텍처를 제시하며, 이는 그들 사이의 정보의 양방향 흐름을 가능하게 한다. 이를 개선된 정류 흐름 공식과 결합하고 확장성을 조사합니다. 검증 손실에서 예측 가능한 스케일링 경향을 보여주고 낮은 검증 손실이 개선된 자동 및 인간 평가와 강한 상관관계가 있음을 보여준다.\n' +
      '\n' +
      '우리의 가장 큰 모델은 _SDXL_(Podell et al., 2023), _SDXL-Turbo_(Sauer et al., 2023), _Pixart-\\(\\alpha\\)_(Chen et al., 2023), DALL-E 3(Betker et al., 2023)과 같은 최신 오픈 모델보다 빠른 이해와 인간 선호 등급의 정량적 평가(Ghosh et al., 2023)에서 모두 우수하다.\n' +
      '\n' +
      '본 연구의 핵심 기여는 다음과 같다. (i) 다양한 확산 모델과 정류된 흐름 제형에 대한 대규모의 체계적인 연구를 수행하여 최상의 설정을 식별한다. 이를 위해 기존에 알려진 샘플러보다 성능이 향상된 정류 유동 모델을 위한 새로운 노이즈 샘플러를 소개한다. (ii) 우리는 네트워크 내에서 텍스트와 이미지 토큰 스트림 사이의 양방향 혼합을 허용하는 텍스트 대 이미지 합성을 위한 새롭고 확장 가능한 아키텍처를 고안한다. 우리는 UViT(Hoogeboom et al., 2023) 및 DiT(Peebles and Xie, 2023)와 같은 확립된 백본과 비교하여 그 이점을 보여준다. 마지막으로, 우리는 (iii) 모델의 스케일링 연구를 수행하고 예측 가능한 스케일링 경향을 따른다는 것을 입증한다. 우리는 낮은 검증 손실이 T2I-CompBench(Huang et al., 2023), GenEval(Ghosh et al., 2023) 및 인간 등급과 같은 메트릭을 통해 평가된 개선된 텍스트 대 이미지 성능과 강한 상관관계가 있음을 보여준다. 결과, 코드 및 모델 가중치를 공개적으로 사용할 수 있도록 합니다.\n' +
      '\n' +
      '##2 유동의 시뮬레이션이 없는 훈련\n' +
      '\n' +
      '노이즈 분포(p_{1}\\)에서 샘플(x_{1}\\)을 데이터 분포(p_{0}\\)에서 샘플(x_{0}\\)로 매핑하는 생성 모델을 상미분 방정식(ODE)의 관점에서 고려하고,\n' +
      '\n' +
      '\\[dy_{t}=v_{\\Theta}(y_{t},t)\\,dt\\, \\tag{1}\\]\n' +
      '\n' +
      '여기서 속도\\(v\\)는 신경망의 가중치\\(\\theta\\)에 의해 매개변수화된다. Chen et al.(2018)의 선행 연구는 미분 가능한 ODE 풀이기를 통해 식 (1)을 직접 풀이할 것을 제안하였다. 그러나, 이 프로세스는 특히 \\(v_{\\theta}(y_{t},t)\\을 파라미터화하는 대형 네트워크 아키텍처의 경우 계산 비용이 많이 든다. 더 효율적인 대안은 \\(p_{0}\\)와 \\(p_{1}\\) 사이의 확률 경로를 생성하는 벡터 필드 \\(u_{t}\\)을 직접 회귀하는 것이다. 이러한 \\(u_{t}\\)을 구성하기 위해 우리는 \\(p_{0}\\)과 \\(p_{1}=\\mathcal{N}(0,1)\\) 사이의 확률 경로 \\(p_{t}\\)에 해당하는 순방향 과정을 정의한다.\n' +
      '\n' +
      '\\[z_{t}=a_{t}x_{0}+b_{t}\\epsilon\\quad\\text{where }\\epsilon\\sim\\mathcal{N}(0,I. \\tag{2}\\\\ \\(a_{0}=1,b_{0}=0,a_{1}=0\\) 및 \\(b_{1}=1\\)에 대해, 여백은,\n' +
      '\n' +
      '\\[p_{t}(z_{t})=\\mathbb{E}_{e\\sim\\mathcal{N}(0,I)}p_{t}(z_{t}|\\epsilon)\\;, \\tag{3}\\]\n' +
      '\n' +
      '데이터 및 노이즈 분포와 일치합니다.\n' +
      '\n' +
      '\\(z_{t},x_{0}\\)과 \\(\\epsilon\\)의 관계를 표현하기 위해 \\(\\psi_{t}\\)과 \\(u_{t}\\)을 다음과 같이 소개한다.\n' +
      '\n' +
      '[\\psi_{t}(\\cdot|\\epsilon):x_{0}\\mapsto a_{t}x_{0}+b_{t}\\epsilon \\tag{4}\\] \\[u_{t}(z|\\epsilon)\\coloneqq\\psi_{t}^{\\prime}(\\psi_{t}^{-1}(z|\\epsilon)|\\epsilon) \\tag{5}\\]\n' +
      '\n' +
      'ODE\\(z_{t}^{\\prime}=u_{t}(z_{t}|\\epsilon)\\), 초기값 \\(z_{0}=x_{0}\\), \\(u_{t}(\\cdot|\\epsilon)\\)에 대한 해로서 \\(z_{t}^{\\prime}=u_{t}(z_{t}|\\epsilon)\\)을 쓸 수 있기 때문에, \\(u_{t}(\\cdot|\\epsilon)\\)는 \\(p_{t}(\\cdot|\\epsilon)\\)을 생성한다. 놀랍게도, 조건부 벡터장 \\(u_{t}(\\cdot|\\epsilon)\\)을 사용하여 한계 확률 경로 \\(p_{t}\\)(Lipman et al., 2023)(B.1 참조)을 생성하는 한계 벡터장 \\(u_{t}\\)을 구성할 수 있다:\n' +
      '\n' +
      '\\mathbb{E}_{e\\sim\\mathcal{N}(0,I)}u_{t}(z|\\epsilon)\\frac{p_{t}(z|\\epsilon)}{p_{t}(z}\\tag{6}\\]\n' +
      '\n' +
      '_Flow Matching_ objective로 \\(u_{t}\\)을 회귀하면서\n' +
      '\n' +
      '\\[\\mathcal{L}_{FM}=\\mathbb{E}_{t,p_{t}(z)}||v_{\\Theta}(z,t)-u_{t}(z)||_{2}^{2}. \\tag{7}\\]\n' +
      '\n' +
      '직접적으로는 수학식 6의 주변화로 인해 난해하며, _Conditional Flow Matching_(B.1 참조),\n' +
      '\n' +
      '\\mathcal{L}_{CFM}=\\mathbb{E}_{t,p_{t}(z|\\epsilon),p(\\epsilon)}||v_{\\theta}(z,t)-u_{t}(z|\\epsilon)||_{2}^{2}\\;, \\tag{8}\\;\n' +
      '\n' +
      '조건 벡터 필드 \\(u_{t}(z|\\epsilon)\\)를 사용하여 동등하지만 다루기 쉬운 목표를 제공한다.\n' +
      '\n' +
      '손실을 명시적인 형태로 변환하기 위해 \\(\\psi_{t}^{\\prime}(x_{0}|\\epsilon)=a_{t}^{\\prime}x_{0}+b_{t}^{\\prime}\\epsilon\\)과 \\(\\psi_{t}^{-1}(z|\\epsilon)=\\frac{z-b_{t}\\epsilon}{a_{t}\\)을 (5)에 삽입한다.\n' +
      '\n' +
      '\\frac{a_{t}^{\\prime}=u_{t}(z_{t}|\\epsilon)=\\frac{a_{t}^{\\prime}}{a_{t}}z_{t}-\\epsilon b_{t}(\\frac{a_{t}^{\\prime}}{a_{t}}-\\frac{b_{t}^{\\prime}}{b_{t}}}\\frac{a_{t}^{\\prime}}{a_{t}}}\\frac{b_{t}^{\\prime}}{b_{t}}\\frac{a_{t}^{\\prime}}}\\ftag{9}\\frac{a_{t}^{\\prime}}}\\frac{a_{t}^{\\prime}}}\\frac{a_{t}^{\\prime}}{b_{t}}\\frac{a_{t}^{\\prime}}\\frac{\n' +
      '\n' +
      '이제, _signal-to-noise ratio_\\(\\lambda_{t}:=\\log\\frac{a_{t}^{2}}{b_{t}^{2}}\\)을 고려한다. \\(\\lambda_{t}^{\\prime}=2(\\frac{a_{t}^{\\prime}}{a_{t}-\\frac{b_{t}^{\\prime}}{b_{t})\\)로 식 (9)를 다시 쓸 수 있다.\n' +
      '\n' +
      '\\frac{a_{t}^{\\prime}}{a_{t}}z_{t}-\\frac{b_{t}{2}\\lambda_{t}^{\\prime}\\epsilon\\tag{10}\\frac{a_{t}^{\\prime}}\n' +
      '\n' +
      '다음으로, 우리는 수학식 (10)을 사용하여 잡음-예측 목적으로서 수학식 (8)을 재매개변수화한다:\n' +
      '\n' +
      '\\mathcal{L}_{CFM} =\\mathbb{E}_{t,p_{t}(z|\\epsilon),p(\\epsilon)}||v_{\\theta}(z,t)-\\frac{a_{t}^{\\prime}}{a_{t}}z+\\frac{b_{t}{2}\\lambda_{t}^{\\prime}\\epsilon||_{t}(z|\\epsilon),p(\\epsilon)}\\left(-\\frac{b_{t}}^{2}}}\\tag{12}\\right)\\mathbb{E}_{t,p_{t}(z,t)-\\epsilon||_{t}^{2}}\\tag{12}\\right)\\mathbb{E}_{t,p_{t}(z|\\epsilon)\\left(-\\frac{b_{t}}^{2}}\\tag{12}\\right)\\mathb\n' +
      '\n' +
      '여기서 \\(\\epsilon_{\\theta}\\coloneq\\frac{-2}{\\lambda_{t}^{\\prime}b_{t}(v_{\\theta}-\\frac{a_{t}^{\\prime}{a_{t}z)\\)을 정의했다.\n' +
      '\n' +
      '상기 목적의 최적은 시간 종속적 가중을 도입할 때 변하지 않는다는 점에 유의한다. 따라서, 원하는 해를 향해 신호를 제공하지만 최적화 궤적에 영향을 미칠 수 있는 다양한 가중 손실 함수를 도출할 수 있다. 고전적인 확산 공식들을 포함한 상이한 접근법들의 통일된 분석을 위해, 우리는 다음 형태로 목적을 작성할 수 있다(Kingma and Gao(2023):\n' +
      '\n' +
      'r=-\\frac{L}_{w}(x_{0})\\mathbb{E}_{t\\sim\\mathcal{U}(t),\\epsilon\\sim\\mathcal{N}(0,I}\\left[w_{t}\\lambda_{t}^{\\prime}\\|\\epsilon_{\\theta}(z_{t}, t)-\\epsilon\\|^{2}\\right]\\;,\\epsilon\\sim\\mathcal{N}(0,I}\\left[w_{t}\\lambda_{t}^{\\prime}\\epsilon_{\\theta}(z_{t}, t)-\\epsilon\\|^{2}\\right]\\;,\\epsilon\\sim\\mathbb{E}_{t\\sim\\mathcal{U}(t),\\epsilon\\sim\\mathcal{N}(0,I}\\left[w_{t}\\lambda_{t}^{\\prime}\\epsilon_\n' +
      '\n' +
      '여기서 \\(w_{t}=-\\frac{1}{2}\\lambda_{t}^{\\prime}b_{t}^{2}\\)는 \\(\\mathcal{L}_{CFM}\\)에 해당한다.\n' +
      '\n' +
      '## 3 흐름 궤적\n' +
      '\n' +
      '이 작업에서 우리는 다음에서 간략하게 설명하는 위의 형식주의의 다양한 변형을 고려한다.\n' +
      '\n' +
      'Rectified FlowRectified Flows (RFs)(Liu et al., 2022; Albergo and Vanden-Eijnden, 2022; Lipman et al., 2023)는 순방향 프로세스를 데이터 분포와 표준 정규 분포 사이의 직선 경로, 즉, 로 정의한다.\n' +
      '\n' +
      '\\[z_{t}=(1-t)x_{0}+t\\epsilon\\;, \\tag{13}\\]\n' +
      '\n' +
      '그리고 \\(\\mathcal{L}_{CFM}\\)을 사용하여 \\(w_{t}^{\\text{RF}}=\\frac{t}{1-t}\\)에 해당한다. 네트워크 출력은 속도\\(v_{\\theta}\\)를 직접 매개변수화한다.\n' +
      '\n' +
      'EdmEDM(Karras et al., 2022)은 Form의 순방향 프로세스를 사용한다.\n' +
      '\n' +
      '\\[z_{t}=x_{0}+b_{t}\\epsilon \\tag{14}\\]\n' +
      '\n' +
      '여기서 (Kingma and Gao, 2023)\\(b_{t}=\\exp F_{\\mathcal{N}}^{-1}(t|P_{m},P_{s}^{2})\\)과 \\(F_{\\mathcal{N}}^{-1}\\)은 평균 \\(P_{m}\\)과 분산 \\(P_{s}^{2}\\)을 갖는 정규 분포의 분위 함수이다. 이 선택으로 인해\n' +
      '\n' +
      '\\[\\lambda_{t}\\sim\\mathcal{N}(-2P_{m},(2P_{s})^{2})\\quad\\text{for }t\\sim\\mathcal{U}(0,1)\\tag{15}\\]\n' +
      '\n' +
      '네트워크는 \\(\\mathbf{F}\\)-예측(Kingma and Gao, 2023; Karras et al., 2022)을 통해 매개변수화되고 손실은 \\(\\mathcal{L}_{w_{t}^{\\text{EDM}}\\)과 함께 작성될 수 있다.\n' +
      '\n' +
      '\\[w_{t}^{\\text{EDM}=\\mathcal{N}(\\lambda_{t}|-2P_{m},(2P_{s})^{2})(e^{-\\lambda_{t}}+0.5^{2}) \\tag{16}\\]\n' +
      '\n' +
      '코사인(Nichol and Dhariwal, 2021)은 형식의 전진 과정을 제안했다.\n' +
      '\n' +
      '\\[z_{t}=\\cos(\\frac{\\pi}{2}t)x_{0}+\\sin(\\frac{\\pi}{2}t)\\epsilon\\;. \\tag{17}\\]\n' +
      '\n' +
      '이것은 \\(\\epsilon\\)-파라미터화 및 손실과 결합하여 가중치 \\(w_{t}=\\operatorname{sech}(\\lambda_{t}/2)\\에 해당한다. \\(\\mathbf{v}\\)-예측 손실(Kingma and Gao, 2023)과 결합하면, 가중치는 \\(w_{t}=e^{-\\lambda_{t}/2}\\)으로 주어진다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '두 개의 서열을 연결합니다. 그런 다음 DiT를 따르고 조절된 주의와 MLP의 시퀀스를 적용한다.\n' +
      '\n' +
      '텍스트와 이미지 임베딩은 개념적으로 상당히 다르기 때문에 두 모달리티에 대해 두 개의 개별 가중치 세트를 사용한다. 도 1(b)에 도시된 바와 같이, 이것은 각 모달리티에 대해 두 개의 독립적인 트랜스포머를 갖는 것과 동일하지만, 주의 동작을 위해 두 모달리티의 시퀀스를 결합함으로써, 두 표현 모두 자신의 공간에서 작동할 수 있으면서도 다른 한 표현을 고려할 수 있다.\n' +
      '\n' +
      '스케일링 실험을 위해, 은닉 크기를 \\(64\\cdot d\\) (MLP 블록에서 \\(4\\cdot 64\\cdot d\\) 채널로 확장), 주의 헤드 수를 \\(d\\)으로 설정하여 모델의 깊이 \\(d\\), 주의 블록 수 측면에서 모델의 크기를 매개변수화한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '정류 흐름 개선\n' +
      '\n' +
      '우리는 수학식 1과 같은 흐름을 정규화하는 시뮬레이션이 없는 훈련을 위한 접근법 중 어느 것이 가장 효율적인지 이해하는 것을 목표로 한다. 다양한 접근법에 걸쳐 비교를 가능하게 하기 위해 최적화 알고리즘, 모델 아키텍처, 데이터 세트 및 샘플러를 제어한다. 또한, 서로 다른 접근 방식의 손실은 비교할 수 없으며 출력 샘플의 품질과 반드시 상관관계가 있는 것은 아니므로 접근 방식 간의 비교를 허용하는 평가 메트릭이 필요하다. 우리는 ImageNet(Russakovsky et al., 2014) 및 CC12M(Changpinyo et al., 2021)에서 모델을 훈련하고, 다른 샘플러 설정(다른 유도 척도 및 샘플링 단계) 하에서 검증 손실, CLIP 점수(Radford et al., 2021; Hessel et al., 2021) 및 FID(Heusel et al., 2017)를 사용하여 훈련 동안 모델의 훈련 및 EMA 가중치를 모두 평가한다. 우리는 (Sauer et al., 2021)에 의해 제안된 CLIP 특징들에 대한 FID를 계산한다. 모든 메트릭은 COCO-2014 검증 분할(Lin et al., 2014)에서 평가된다. 교육 및 샘플링 하이퍼파라미터에 대한 자세한 내용은 부록 B.3에 나와 있다.\n' +
      '\n' +
      '#### 5.1.1 Results\n' +
      '\n' +
      '우리는 두 데이터 세트에서 각각 61개의 다른 제형을 훈련한다. 섹션 3의 다음 변형을 포함합니다.\n' +
      '\n' +
      '* \\(\\epsilon\\)- and \\(\\mathbf{v}\\)-prediction loss with linear (eps/linear, v/linear) and cosine (eps/cos, v/cos) schedule.\n' +
      '* \\(\\pi_{\\text{mode}}(t;s)\\) (rf/mode(s))와 \\(-1\\)과 \\(1.75\\) 사이에서 균일하게 선택된 \\(s\\)에 대한 7 값을 갖는 RF 손실, 및\n' +
      '\n' +
      '그림 2: **우리의 모델 아키텍처.** 연결은 \\(\\odot\\) 및 요소별 곱셈으로 표시된다. 훈련 실행을 안정화하기 위해 \\(Q\\) 및 \\(K\\)에 대한 RMS-Norm이 추가될 수 있다. 가장 잘 본 것은 확대되었다.\n' +
      '\n' +
      '또한, 균일 타임스텝 샘플링(rf/mode)에 해당하는 \\(s=1.0\\) 및 \\(s=0\\)에 대한 것이다.\n' +
      '*RF loss with \\(\\pi_{\\text{ln}}(t;m,s)\\)(rf/lognorm(m,s)) with 30 values for \\((m,s)\\) in the grid with \\(m\\)uniform between \\(-1\\) and \\(1\\) and \\(s\\) to \\(0.2\\) and \\(2.2\\)}\n' +
      '* \\(\\pi_{\\text{CostMap}}(t)\\)(rf/cosmap)을 갖는 RF 손실.\n' +
      '* EDM(\\(\\text{e}\\text{dm}\\,(P_{m},P_{s})\\))은 \\(-1.2\\)과 \\(1.2\\) 사이에서 균일하게 선택되고 \\(P_{s}\\)과 \\(0.6\\)과 \\(1.8\\) 사이에서 균일하게 선택된 \\(P_{m}\\)에 대해 15개의 값을 갖는다. \\(P_{m},P_{s}=(-1.2,1.2)\\)은 (Karras et al., 2022)의 파라미터에 대응한다는 점에 유의한다.\n' +
      '* rf(edm/rf)의 log-SNR 가중치와 v/cos(edm/cos)의 log-SNR 가중치와 일치하도록 스케줄을 갖는 EDM.\n' +
      '\n' +
      '각 실행에 대해 EMA 가중치로 평가할 때 최소 검증 손실로 단계를 선택한 다음 EMA 가중치가 있거나 없는 6가지 다른 샘플러 설정으로 얻은 CLIP 점수와 FID를 수집한다.\n' +
      '\n' +
      '샘플러 설정, EMA 가중치 및 데이터 세트 선택의 모든 24개 조합에 대해 비지배 정렬 알고리즘을 사용하여 다른 제형의 순위를 매긴다. 이를 위해 CLIP 및 FID 점수에 따라 파레토 최적인 변이체를 반복적으로 계산하고, 해당 변이체에 현재 반복 지수를 할당하고, 해당 변이체를 제거하고, 모든 변이체가 순위가 매겨질 때까지 나머지 변이체를 계속한다. 마지막으로 24개의 다른 제어 설정에서 해당 순위를 평균화합니다.\n' +
      '\n' +
      '결과를 탭에 표시합니다. 1에서 우리는 서로 다른 하이퍼파라미터로 평가된 변이체에 대해 가장 성능이 좋은 두 변이체만 보여준다. 또한 샘플러 설정에 대한 평균을 5단계와 50단계로 제한하는 순위를 보여준다.\n' +
      '\n' +
      '우리는 rf/lognorm(0.00, 1.00)이 일관되게 좋은 순위를 달성하는 것을 관찰한다. 균일한 타임스텝 샘플링(rf)으로 정류된 흐름 공식보다 우수하므로 중간 타임스텝이 더 중요하다는 가설을 확인한다. 모든 변형들 중에서, 수정된 타임스탬프 샘플링을 갖는 _only_ 정류된 흐름 제형은 이전에 사용된 LDM-Linear (Rombach et al., 2022) 제제(eps/linear)보다 더 잘 수행된다.\n' +
      '\n' +
      '또한 일부 변이체는 일부 설정에서는 잘 수행되지만 다른 설정에서는 더 좋지 않은 것으로 관찰되며 rf/대수(0.50, 0.60)는 50개의 샘플링 단계를 가진 가장 성능이 좋은 변이체이지만 5개의 샘플링 단계를 가진 훨씬 더 나쁜 변이체(평균 순위 8.5)이다. Tab. 2의 두 가지 메트릭에 대해 유사한 동작을 관찰한다. 첫 번째 그룹은 25개의 샘플링 단계를 가진 두 데이터 세트에서 대표적인 변형 및 메트릭을 보여준다. 다음 그룹은 최상의 CLIP 및 FID 점수를 달성하는 변형을 보여준다. rf/모드(1.75)를 제외하고, 이러한 변형들은 전형적으로 한 메트릭에서는 매우 잘 수행되지만 다른 메트릭에서는 상대적으로 나쁘게 수행된다. 대조적으로, rf/lognorm(0.00, 1.00)은 메트릭과 데이터 세트에서 좋은 성능을 달성한다는 것을 다시 한 번 관찰하며, 여기서 세 번째 베스트 점수는 네 번 중 두 번, 두 번째 베스트 성능은 한 번 얻는다.\n' +
      '\n' +
      '마지막으로, 우리는 제형의 다른 그룹(edm, rf, eps 및 v)에 대해 다른 색상을 사용하는 그림 3에서 다른 제형의 정성적 거동을 설명한다. 정류된 흐름 제형은 일반적으로 잘 수행되며 다른 제형에 비해 샘플링 단계 수를 줄일 때 성능이 덜 저하된다.\n' +
      '\n' +
      '양식별 표현력 향상\n' +
      '\n' +
      '정류 흐름 모델이 LDM-Linear(Rombach et al., 2022) 또는 EDM(Karras et al., 2022)과 같은 확립된 확산 제형과 경쟁할 수 있는 제형을 이전 섹션에서 발견했지만, 우리는 이제 고해상도 텍스트-이미지 합성에 대한 제형의 적용으로 돌아간다. AcAc\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\multicolumn{3}{c}{rank averaged over} \\\\ \\hline variant & all & 5 steps & 50 steps \\\\ \\hline rf/lognorm(0.00, 1.00) & 1.54 & 1.25 & 1.50 \\\\ rf/lognorm(1.00, 0.60) & 2.08 & 3.50 & 2.00 \\\\ rf/lognorm(0.50, 0.60) & 2.71 & 8.50 & 1.00 \\\\ rf/mode(1.29) & 2.75 & 3.25 & 3.00 \\\\ rf/lognorm(0.50, 1.00) & 2.83 & 1.50 & 2.50 \\\\ eps/linear & 2.88 & 4.25 & 2.75 \\\\ rf/mode(1.75) & 3.33 & 2.75 & 2.75 \\\\ rf/cosmap & 4.13 & 3.75 & 4.00 \\\\ edm(0.00, 0.60) & 5.63 & 13.25 & 3.25 \\\\ rf & 5.67 & 6.50 & 5.75 \\\\ v/linear & 6.83 & 5.75 & 7.75 \\\\ edm(0.60, 1.20) & 9.00 & 13.00 & 9.00 \\\\ v/cos & 9.17 & 12.25 & 8.75 \\\\ edm/cos & 11.04 & 14.25 & 11.25 \\\\ edm/rf & 13.04 & 15.25 & 13.25 \\\\ edm(-1.20, 1.20) & 15.58 & 20.25 & 15.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 변이체의 **글로벌 순위.** 이 순위에 대해 EMA 및 비-EMA 가중치, 두 데이터 세트 및 서로 다른 샘플링 설정에 대해 평균화된 비-지배 정렬을 적용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{ImageNet} & \\multicolumn{2}{c}{CC12M} \\\\ \\cline{2-5} variant & CLIP & FID & CLIP & FID \\\\ \\hline rf & 0.247 & 49.70 & 0.217 & 94.90 \\\\ edm(-1.20, 1.20) & 0.236 & 63.12 & 0.200 & 116.60 \\\\ eps/linear & 0.245 & 48.42 & 0.222 & _90.34_ \\\\ v/cos & 0.244 & 50.74 & 0.209 & 97.87 \\\\ v/linear & 0.246 & 51.68 & 0.217 & 100.76 \\\\ \\hline rf/lognorm(0.50, 0.60) & **0.256** & 80.41 & 0.233 & 120.84 \\\\ rf/mode(1.75) & _0.253_ & **44.39** & 0.218 & 94.06 \\\\ rf/lognorm(1.00, 0.60) & 0.254 & 114.26 & **0.234** & 147.69 \\\\ rf/lognorm(-0.50, 1.00) & 0.248 & 45.64 & 0.219 & **89.70** \\\\ \\hline rf/lognorm(0.00, 1.00) & 0.250 & _45.78_ & _0.224_ & 89.91 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 상이한 변이체에 대한 **Metrics.** 25개의 샘플링 단계를 갖는 상이한 변이체의 FID 및 CLIP 점수. **best**, second best, 그리고 _third best_ entries를 강조합니다.\n' +
      '\n' +
      '기록적으로, 본 알고리즘의 최종 성능은 훈련 공식뿐만 아니라 신경망을 통한 매개변수화 및 우리가 사용하는 이미지 및 텍스트 표현의 품질에 달려 있다. 다음 섹션에서는 섹션 5.3에서 최종 방법을 확장하기 전에 이러한 모든 구성요소를 개선하는 방법을 설명한다.\n' +
      '\n' +
      '개선된 오토인코더 5.2.1\n' +
      '\n' +
      '잠재 확산 모델은 입력 RGB\\(X\\in\\mathbb{R}^{H\\times W\\times 3}\\)을 저차원 공간\\(x=E(X)\\in\\mathbb{R}^{h\\times w\\times d}\\)으로 매핑하는 사전 훈련된 오토인코더(Rombach et al., 2022)의 잠재 공간에서 동작함으로써 높은 효율을 달성한다. 이 오토인코더의 재구성 품질은 잠재 확산 훈련 후 달성 가능한 이미지 품질에 대한 상한을 제공한다. Dai et al. (2023)과 유사하게, 잠재 채널 수 \\(d\\)가 증가하면 재구성 성능이 크게 향상된다는 것을 발견했으며, 표 3을 참조한다. 직관적으로, 더 높은 \\(d\\)으로 잠복기를 예측하는 것은 더 어려운 작업이며, 따라서 용량이 증가된 모델은 더 큰 \\(d\\)에 대해 더 잘 수행할 수 있어야 하며, 궁극적으로 더 높은 이미지 품질을 달성할 수 있어야 한다. 우리는 그림 10에서 이 가설을 확인하는데, 여기서 \\(d=16\\) 오토인코더가 샘플 FID 측면에서 더 나은 스케일링 성능을 보인다는 것을 알 수 있다. 이 논문의 나머지 부분에 대해서는 \\(d=16\\)을 선택한다.\n' +
      '\n' +
      '2.2 개선된 캡션\n' +
      '\n' +
      'Betker et al.(2023)은 합성적으로 생성된 캡션들이 스케일에서 트레이닝된 텍스트-이미지 모델들을 크게 개선할 수 있음을 입증하였다. 이것은 종종 대규모 이미지 데이터세트와 함께 오는 인간 생성 캡션의 단순성 때문에, 이는 이미지 주제에 지나치게 초점을 맞추고 일반적으로 장면의 배경 또는 구성을 설명하는 세부사항을 생략하거나, 적용 가능한 경우 디스플레이된 텍스트(Betker et al., 2023)를 포함한다. 우리는 그들의 접근법을 따르고 우리의 대규모 이미지 데이터세트에 대한 합성 주석을 생성하기 위해 기성품, 최첨단 비전 언어 모델인 _CogVLM_(Wang et al., 2023)을 사용한다. 합성 캡션은 텍스트 대 이미지 모델이 VLM 지식 코퍼스에 존재하지 않는 특정 개념을 잊게 할 수 있으므로 원본 50%와 합성 캡션 50%의 비율을 사용한다.\n' +
      '\n' +
      '이 캡션 믹스에 대한 훈련의 효과를 평가하기 위해, 두 개의 \\(d=15\\)_MM-DiT_ 모델을 250k 단계에 대해 훈련한다. 하나는 원본 캡션에서만 훈련하고 다른 하나는 50/50 믹스에서 훈련한다. 우리는 표 4의 GenEval 벤치마크(Ghosh et al., 2023)를 사용하여 훈련된 모델을 평가한다. 결과는 합성 캡션을 추가하여 훈련된 모델이 원본 캡션을 사용하는 모델보다 분명히 더 우수하다는 것을 보여준다. 따라서 우리는 이 작업의 나머지 동안 50/50 합성/원본 캡션 믹스를 사용한다.\n' +
      '\n' +
      '개선된 텍스트-이미지 백본 5.2.3\n' +
      '\n' +
      '이 섹션에서는 기존의 트랜스포머 기반 확산 백본과 섹션 4에 소개된 새로운 멀티모달 트랜스포머 기반 확산 백본인 _MM-DiT_의 성능을 비교한다. _MM-DiT_는 (2) 서로 다른 세트의 훈련 가능한 모델 가중치를 사용하여 텍스트 및 이미지 토큰과 같은 서로 다른 도메인을 처리하도록 특별히 설계되었다. 보다 구체적으로, 섹션 5.1의 실험 설정을 따르고 DiT, CrossDiT(DiT but with cross-attending with text tokens instead instead sequence-wise concatenation(Chen et al., 2023)) 및 우리의 _MM-DiT_의 CC12M에 대한 텍스트-대-이미지 성능을 비교한다. _MM-DiT_의 경우, 두 세트의 가중치 및 세 세트의 가중치로 모델을 비교하는데, 여기서 후자는 CLIP(Radford et al., 2021) 및 T5(Raffel et al., 2019) 토큰(_c.f_. Section 4)을 취급한다. 별도로. DiT(섹션 4에서와 같이 텍스트 및 이미지 토큰의 w/연결)는 _MM-DiT_의 특별한 경우로 해석될 수 있음에 유의한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline  & Original Captions & 50/50 Mix \\\\ \\cline{2-3}  & success rate [\\%] & success rate [\\%] \\\\ \\hline Color Attribution & 11.75 & 24.75 \\\\ Colors & 71.54 & 68.09 \\\\ Position & 6.50 & 18.00 \\\\ Counting & 33.44 & 41.56 \\\\ Single Object & 95.00 & 93.75 \\\\ Two Objects & 41.41 & 52.53 \\\\ \\hline Overall score & 43.27 & 49.78 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **개선된 캡션**. 합성(CogVLM(Wang et al., 2023)을 통해) 및 원본 캡션의 50/50 혼합 비율을 사용하면 텍스트 대 이미지 성능이 향상된다. GenEval(Ghosh et al., 2023) 벤치마크를 통해 평가된다.\n' +
      '\n' +
      '그림 3: **정류된 흐름은 샘플 효율적이다.**정류된 흐름은 더 적은 단계를 샘플링할 때 다른 제형보다 더 잘 수행된다. 25단계 이상의 단계에서 rf/lognorm(0.00, 1.00)만이 eps/linear에 대해 경쟁력을 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r} \\hline \\hline Metric & 4 chn & 8 chn & 16 chn \\\\ \\hline FID (\\(\\downarrow\\)) & 2.41 & 1.56 & **1.06** \\\\ Perceptual Similarity (\\(\\downarrow\\)) & 0.85 & 0.68 & **0.45** \\\\ SSIM (\\(\\uparrow\\)) & 0.75 & 0.79 & **0.86** \\\\ PSNR (\\(\\uparrow\\)) & 25.12 & 26.40 & **28.62** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **개선된 오토인코더.** 상이한 채널 구성에 대한 재구성 성능 메트릭. 모든 모델의 다운샘플링 인자는 \\(f=8\\)이다.\n' +
      '\n' +
      '모든 양식에 대한 가중치 세트를 공유합니다. 마지막으로 UViT (Hoogeboom et al., 2023) 아키텍처를 널리 사용되는 UNets와 변압기 변종 간의 하이브리드로서 고려한다.\n' +
      '\n' +
      '우리는 그림 4의 이러한 아키텍처의 수렴 거동을 분석한다: 바닐라 DiT는 UViT를 저성능으로 한다. 교차 주의 DiT 변형 CrossDiT는 UViT보다 더 나은 성능을 달성하지만, UViT는 초기에 훨씬 더 빨리 학습하는 것으로 판단된다. 우리의 _MM-DiT_ 변이체는 교차 주의 및 바닐라 변이체보다 상당히 우수하다. 우리는 두 개(매개변수 수 증가 및 VRAM 사용 비용) 대신 세 개의 매개변수 세트를 사용할 때 작은 이득만 관찰하므로 이 작업의 나머지 부분에 대해 전자의 옵션을 선택한다.\n' +
      '\n' +
      '척도에서의### 훈련\n' +
      '\n' +
      '확장하기 전에 데이터를 필터링하고 사전 인코딩하여 안전하고 효율적인 사전 교육을 보장합니다. 그런 다음 확산 제형, 아키텍처 및 데이터에 대한 모든 이전 고려 사항은 마지막 섹션에서 절정에 달하며 여기서 모델을 최대 8B 매개변수로 확장한다.\n' +
      '\n' +
      '####5.3.1 데이터 전처리\n' +
      '\n' +
      '사전 훈련 완화 훈련 데이터는 생성 모델의 능력에 상당한 영향을 미칩니다. 결과적으로, 데이터 필터링은 바람직하지 않은 능력을 제약하는 데 효과적이다(Nichol, 2022). 판매 교육 전에 우리는 다음 범주에 대한 데이터를 필터링한다. (i) 성 콘텐츠: 명시적 콘텐츠를 필터링하기 위해 NSFW 탐지 모델을 사용한다. (ii) 심미성: 평가 시스템이 낮은 점수를 예측하는 이미지를 제거한다. (iii) 역류: 우리는 훈련 데이터에서 지각적 및 의미적 중복을 제거하기 위해 클러스터 기반 중복 제거 방법을 사용한다; 부록 E.2를 참조한다.\n' +
      '\n' +
      '이미지 및 텍스트 임베딩 사전 계산 모델은 사전 훈련된 여러 냉동 네트워크의 출력을 입력(자동 인코더 래턴트 및 텍스트 인코더 표현)으로 사용합니다. 이러한 출력은 훈련 중에 일정하기 때문에 전체 데이터 세트에 대해 한 번 사전 계산한다. 부록 E.1에서 접근법에 대한 자세한 논의를 제공한다.\n' +
      '\n' +
      '높은 결심을 위한 파이너닝\n' +
      '\n' +
      'QK-Normalization은 일반적으로 크기\\(256^{2}\\) 픽셀의 저해상도 영상에서 모든 모델을 사전 학습한다. 다음으로, 우리는 혼합 종횡비로 더 높은 해상도로 모델을 미세 조정합니다(자세한 내용은 다음 단락 참조). 우리는 높은 해상도로 이동할 때 혼합 정밀 훈련이 불안정해질 수 있고 손실이 발산한다는 것을 발견했다. 이는 완전 정밀 훈련으로 전환하여 해결할 수 있지만, 혼합 정밀 훈련에 비해 \\(\\sim 2\\times\\)의 성능 저하를 동반한다. 보다 효율적인 대안이 (차별적인) ViT 문헌에 보고되어 있다: Dehghani et al. (2023)은 주의 엔트로피가 통제 불가능하게 성장하기 때문에 대형 비전 트랜스포머 모델의 훈련이 분기된다는 것을 관찰한다. 이를 피하기 위해 Dehghani et al.(2023)은 어텐션 동작 전에 Q와 K를 정규화하는 것을 제안한다. 본 논문에서는 MMDiT 구조의 두 스트림에서 학습 가능한 스케일을 갖는 RMSNorm(Zhang and Sennrich, 2019)을 모델들에 사용한다. 도 2를 참조한다. 도 5에서 증명된 바와 같이, 추가 정규화는 주의 로짓 성장 불안정성을 방지하며, Dehghani et al.(2023) 및 Wortsman et al.(2023)의 발견을 확인하고 AdamW(Loshchilov and Hutter, 2017) 최적화기에서 \\(\\epsilon=10^{-15}\\)과 결합될 때 bf16-혼합(Chen et al., 2019) 정밀도에서 효율적인 훈련을 가능하게 한다. 이 기법은 사전 훈련 시 qk 정규화를 사용하지 않은 사전 훈련된 모델에도 적용될 수 있다: 모델은 추가 정규화 레이어에 빠르게 적응하고 보다 안정적으로 훈련한다. 마지막으로, 우리는 이 방법이 일반적으로 대형 모델의 트레이닝을 안정화하는 데 도움이 될 수 있지만, 보편적인 레시피가 아니며 정확한 트레이닝 설정에 따라 적응될 필요가 있을 수 있음을 지적하고자 한다.\n' +
      '\n' +
      '고정된 \\(256\\times 256\\) 해상도로 훈련한 후, 우리는 (i) 해상도와 해상도를 높이고 (ii) 유연한 종횡비로 추론을 가능하게 하는 것을 목표로 한다. 2D 위치 프리를 사용하기 때문에\n' +
      '\n' +
      '그림 4: **모델 아키텍처의 훈련 역학. CC12M에서 _DiT_, _CrossDiT_, _UViT_ 및 _MM-DiT_의 비교 분석을 검증 손실, CLIP 점수 및 FID에 중점을 둔다. 제안된 _MM-DiT_는 모든 메트릭에서 양호하게 수행된다.**\n' +
      '\n' +
      '그림 5: **QK 정규화의 효과. 어텐션 매트릭스를 계산하기 전에 Q- 및 K-embeddings를 정규화하는 것은 어텐션-로짓 성장 불안정성(_left_)을 방지하며, 이는 어텐션 엔트로피를 붕괴(_right_)하게 하고, 판별 ViT 문헌에 이전에 보고된 바 있다(Dehghani et al., 2023; Wortsman et al., 2023). 이러한 기존 연구와 달리, 우리는 네트워크의 마지막 변압기 블록에서 이러한 불안정성을 관찰한다. 최대 주의력 로짓 및 주의력 엔트로피는 2B(d=24) 모델의 마지막 5개 블록에 대해 평균으로 표시된다.**\n' +
      '\n' +
      '우리는 해상도에 따라 그것들을 조정해야 한다. 다중 종횡비 설정에서, (도소비츠키 등, 2020)에서와 같은 임베딩들의 직접 보간은 측면 길이들을 정확하게 반영하지 않을 것이다. 대신에 우리는 주파수 임베딩된 확장 및 보간 위치 그리드의 조합을 사용한다.\n' +
      '\n' +
      'S^{2}\\ 픽셀의 목표 해상도는 버킷 샘플링(NovelAI, 2022; Podell et al., 2023)을 사용하여 각 배치가 균질한 크기\\(H\\times W\\), 여기서 \\(H\\cdot W\\approx S^{2}\\)의 이미지로 구성되도록 한다. 최대 및 최소 훈련 종횡비에 대해, 이는 마주칠 폭, \\(W_{\\text{max}}\\) 및 높이, \\(H_{\\text{max}}\\)에 대한 최대값을 초래한다. (h_{\\text{max}}=H_{\\text{max}}/16,w_{\\text{max}=W_{\\text{max}/16\\) 및 \\(s=S/16\\)을 패치 후 잠재 공간(요인 8)에 해당하는 크기(요인 2)로 한다. 이 값들을 바탕으로 수직 위치 격자를 구성하고, 수평 위치들에 대해 각각 \\((p-\\frac{h_{\\text{max}}-s}{2})\\cdot\\frac{256}{S})^{h_{\\text{max}-1}\\) 값을 갖는다. 그런 다음 임베딩하기 전에 생성된 위치 2d 그리드에서 중심 크롭을 생성한다.\n' +
      '\n' +
      '직관적으로 타임스텝 스케줄의 해상도 의존적 이동은 해상도가 높을수록 픽셀이 더 많기 때문에 신호를 파괴하기 위해 더 많은 노이즈가 필요하다. 우리가 \\(n=H\\cdot W\\) 픽셀을 갖는 해상도에서 작업하고 있다고 가정하자. 이제, "상수" 이미지, 즉 모든 픽셀이 값 \\(c\\)을 갖는 이미지를 고려한다. 정방향 공정은 \\(z_{t}=(1-t)c\\mathbbm{1}+t\\epsilon\\을 생성하며, 여기서 \\(\\mathbbm{1}\\)과 \\(\\epsilon\\in\\mathbbb{R}^{n}\\)을 모두 생성한다. 따라서, \\(z_{t}\\)은 \\(c\\)과 \\(\\mathbb{R}\\)에서 \\(Y=(1-t)c+t\\eta\\)의 확률변수 \\(n\\)의 관측치를 제공하며, \\(\\eta\\)은 표준정규분포를 따른다. 따라서 \\(\\mathbb{E}(Y)=(1-t)c\\) 및 \\(\\sigma(Y)=t\\). 따라서 우리는 \\(c=\\frac{1}{1-t}\\mathbb{E}(Y)\\)를 통해 \\(c\\)을 복원할 수 있으며, \\(c\\)와 그 표본 추정치 \\(\\hat{c=\\frac{1}{1-t}\\sum_{i=1}^{n}z_{t,i}\\) 사이의 오차는 \\(\\sigma(t,n)=\\frac{t}{1-t}\\sqrt{\\frac{1}}\\)의 표준편차를 갖는다. (Y\\(Y)에 대한 평균의 표준오차가 편차\\(\\frac{t{\\sqrt{n}\\)을 갖기 때문에 \\(c\\)의 오차는 \\(\\frac{t{\\sqrt{n}\\)을 갖는다.) 따라서 이미지\\(z_{0}\\)이 픽셀 전체에 걸쳐 일정하다는 것을 이미 알고 있다면, \\(\\sigma(t,n)\\)는 \\(z_{0}\\)에 대한 불확실성의 정도를 나타낸다. 예를 들어, 우리는 폭과 높이를 두 배로 늘리면 주어진 시간\\(0<t<1\\)에서 불확실성의 절반으로 이어진다는 것을 즉시 알 수 있다. 그러나 이제 asatz\\(\\sigma(t_{n},n)=\\sigma(t_{m},m)\\을 통해 같은 정도의 불확실성을 초래하는 분해능\\(n\\)에서 timestep\\(t_{n}\\)을 timestep\\(t_{m}\\)으로 매핑할 수 있다. \\(t_{m}\\)에 대한 풀이\n' +
      '\n' +
      '\\[t_{m}=\\frac{\\sqrt{\\frac{m}{n}}t_{n}}{1+(\\sqrt{\\frac{m}{n}}-1)t_{n}} \\tag{23}\\]\n' +
      '\n' +
      '우리는 그림 6에서 이러한 이동 함수를 시각화한다. 상수 이미지의 가정은 현실적이지 않다는 점에 유의한다. 추론 과정에서 이동 값\\(\\alpha\\coloneqq\\sqrt{\\frac{m}{n}\\)에 대한 좋은 값을 찾기 위해 해상도\\(1024\\times 1024\\)으로 훈련된 모델의 샘플링 단계에 적용하여 인간 선호도 연구를 수행한다. 그림 6의 결과는 시프트가 \\(1.5\\)보다 크지만 더 높은 시프트 값 사이에서 덜 급격한 차이를 갖는 샘플에 대한 강한 선호도를 보여준다. 따라서, 본 연구에서는 훈련과 샘플링 과정에서의 이동 값(\\(\\alpha=3.0\\)을 1024\\times 1024\\)에서 사용한다. 이러한 시프트가 있거나 없는 8k 트레이닝 단계들 이후의 샘플들 간의 정성적 비교는 그림 6에서 찾을 수 있다. 마지막으로, 수학식 23은 (Hoogeboom et al., 2023)과 유사한 \\(\\log\\frac{n}{m}\\)의 로그-SNR 시프트(log-SNR 시프트)를 함축한다는 점에 유의한다:\n' +
      '\n' +
      '\\[\\lambda_{t_{m}} = 2\\log\\frac{1-t_{n}{\\sqrt{\\frac{m}}t_{n}} \\tag{24}\\] \\[=\\lambda_{t_{n}}-2\\log\\alpha=\\lambda_{t_{n}}-\\log\\frac{m}{n}. \\tag{25}\\]\n' +
      '\n' +
      '도 6: 더 높은 해상도의 **Timestep shifting.**_Top Right:_Human Quality preference rating in applying the shifting based on Equation (23). _ 맨 아래 행:_A\\(512^{2}\\) 모델을 학습하고 \\(\\sqrt{m/n}=1.0\\)(_top_) 및 \\(\\sqrt{m/n}=3.0\\)(_bottom_)으로 샘플링했다. 섹션 5.3.2를 참조하십시오.\n' +
      '\n' +
      '도 7: **currrent closed and open SOTA 생성 이미지 모델들에 대한 인간 선호도 평가.** 우리의 8B 모델은 카테고리 _visual quality_, _prompt following_ 및 _typography generation_에 걸쳐 parti-prompts(Yu et al., 2022) 상에서 평가될 때 현재의 최신 텍스트-대-이미지 모델들에 대해 호의적인 비교를 한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '시간, 우리는 세 개의 텍스트 인코더의 임의의 부분 집합을 사용할 수 있다. 이는 향상된 메모리 효율을 위해 모델 성능을 트레이드오프하기 위한 수단을 제공하며, 이는 상당한 양의 VRAM을 필요로 하는 T5-XXL(Raffel et al., 2019)의 4.7B 파라미터와 특히 관련이 있다. 흥미로운 사실은 텍스트 프롬프트에 두 개의 CLIP 기반 텍스트 인코더만 사용하고 T5 임베딩을 0으로 대체할 때 제한된 성능 저하를 관찰한다. 우리는 그림 9에서 정성적 시각화를 제공한다. 장면에 대한 매우 상세한 설명이나 더 많은 양의 텍스트가 포함된 복잡한 프롬프트에 대해서만 세 개의 텍스트 인코더를 모두 사용할 때 상당한 성능 향상을 발견한다. 이러한 관찰은 도 7의 인간 선호도 평가 결과(_Ours w/o T5_)에서도 검증된다. T5를 제거하는 것은 심미적 품질 등급(\\(50\\%\\)의 승률)에는 영향을 미치지 않으며, 즉각적인 순응(\\(46\\%\\)의 승률)에는 영향을 미치지 않는 반면, 텍스트 생성 능력에 대한 기여도는 더 중요하다(\\(38\\%\\)의 승률).\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 텍스트-이미지 합성을 위한 정류 흐름 모델의 스케일링 분석을 제시하였다. 잠재 확산 모델에 대한 이전의 확산 훈련 공식보다 개선되고 소수 단계 샘플링 체제에서 정류 흐름의 유리한 특성을 유지하는 정류 흐름 훈련을 위한 새로운 타임스텝 샘플링을 제안했다. 또한 텍스트-이미지 태스크의 멀티모달 특성을 고려한 트랜스포머 기반 _MM-DiT_ 아키텍처의 장점을 증명하였다. 마지막으로, 8B 파라미터의 모델 크기까지 이 조합에 대한 스케일링 연구와 \\(5\\times 10^{22}\\) 훈련 FLOP를 수행하였다. 검증 손실 개선은 기존 텍스트 대 이미지 벤치마크뿐만 아니라 인간 선호도 평가와도 상관관계가 있음을 보여주었다. 이는 생성 모델링 및 확장 가능한 멀티모달 아키텍처의 개선과 함께 최첨단 독점 모델과 경쟁력 있는 성능을 달성합니다. 스케일링 추세는 포화 징후를 보이지 않으므로 향후 모델의 성능을 계속 개선할 수 있다고 낙관한다.\n' +
      '\n' +
      '그림 8: **스케일링의 정량적 효과.** 모델 크기가 성능에 미치는 영향을 분석하여 전체적으로 일관된 훈련 하이퍼파라미터를 유지한다. 예외는 깊이=38로, 발산을 방지하기 위해 \\(3\\times 10^{5}\\) 단계의 학습 속도 조정이 필요했다. (상단) 검증 손실은 이미지(열 1 및 2) 및 비디오 모델(열 3 및 4) 모두에 대한 모델 크기 및 트레이닝 단계 모두의 함수로서 부드럽게 감소한다. (하단) 검증 손실은 전체 모델 성능의 _강한 예측 변수입니다. 검증 손실과 전체적 이미지 평가 메트릭 사이에는 현저한 상관관계가 있는데, GenEval(Ghosh et al., 2023), 열 1, 인간 선호도, 열 2, 및 T2I-CompBench(Huang et al., 2023), 열 3. 비디오 모델에 대해 검증 손실과 인간 선호도 사이의 유사한 상관관계를 관찰한다. 열 4.\n' +
      '\n' +
      '도 9: **T5의 영향.** T5가 복잡한 프롬프트에 중요한 것으로 관찰되는데, 예를 들어 높은 정도의 세부사항 또는 더 긴 철자 텍스트(행 2 및 3)를 포함한다. 그러나 대부분의 프롬프트에 대해 추론 시간에 T5를 제거해도 여전히 경쟁적 성능을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '본 논문은 일반적으로 기계 학습 및 특히 이미지 합성 분야를 발전시키는 것을 목표로 하는 작업을 제시한다. 우리 작업에는 많은 잠재적인 사회적 결과가 있으며, 여기에서 특별히 강조되어야 한다고 느끼는 것은 없습니다. 확산 모델의 일반적인 영향에 대한 광범위한 논의를 위해 Po 등에 대한 관심 있는 독자를 지적한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ideogram v1.0 announcement (2024) Ideogram v1.0 announcement, 2024. URL[https://about.ideogram.ai/1.0](https://about.ideogram.ai/1.0).\n' +
      '* Playground v2.5 announcement (2024) Playground v2.5 announcement, 2024. URL[https://blog.playgroundai.com/playground-v2-5/](https://blog.playgroundai.com/playground-v2-5/)\n' +
      '* Albergo and Vanden-Eijnden (2022) Albergo, M. S. and Vanden-Eijnden, E. Building Normalizing Flow with stochastic interpolants, 2022.\n' +
      '* Atchison and Shen (1980) Atchison, J. and Shen, S. M. Logistic-normal distribution: 일부 properties and use. _ Biometrika_, 67(2):261-272, 1980.\n' +
      '*Autofaiss(2023) autofaiss. autofaiss, 2023. URL[https://github.com/criteo/autofaiss](https://github.com/criteo/autofaiss).\n' +
      '* Balaji et al. (2022) Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M. -Y. Ediff-i: Text-to-image diffusion model with ensemble of expert denoisers, 2022.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. 더 나은 캡션을 갖는 이미지 생성 개선. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3), 2023.\n' +
      '* Blattmann et al. (2023) Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: latent video diffusion models to large datasets. _ arXiv preprint arXiv:2311.15127_, 2023a.\n' +
      '* Blattmann et al. (2023) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. 잠복기 정렬: 고해상도 비디오 합성과 잠재 확산 모델, 2023b.\n' +
      '* Brooks et al. (2023) Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18392-18402, 2023.\n' +
      '* Carlini et al. (2023) Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramer, F., Balle, B., Ippolito, D., and Wallace, E. 확산 모델로부터 훈련 데이터를 추출하는 단계. In _32nd USENIX Security Symposium (USENIX Security 23)_, pp. 5253-5270, 2023.\n' +
      '* Carreira and Zisserman (2018) Carreira, J. and Zisserman, A. Quo vadis, action recognition? 새로운 모델 및 동역학 데이터세트, 2018.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P. K., Ding, N., and Soricut, R. 컨셉 12m: 롱테일 비주얼 컨셉을 인식하기 위해 웹 스케일 이미지-텍스트 사전 트레이닝을 푸시하는 단계. _ 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 3557-3567, 2021. URL[https://api.semanticscholar.org/Corp](https://api.semanticscholar.org/Corp) usID:231951742).\n' +
      '* Chen et al. (2019) Chen, D., Chou, C., Xu, Y., and Hseu, J. Bfloat16: The secret to high performance on cloud tpus, 2019. URL[https://cloud.google.com/blog/products/ai-machine-learning/bffloat16-the-secret-to-high-performance-on-cloud-tpus?hl=en](https://cloud.google.com/blog/products/ai-machine-learning/bffloat16-the-performance-on-cloud-tpus?hl=en)\n' +
      '* Chen et al. (2023) Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. 픽사르트-a: 실시간 텍스트-이미지 합성을 위한 확산 변압기의 빠른 훈련, 2023.\n' +
      '* Chen et al. (2018) Chen, T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. _Neural Information Processing Systems_, 2018. URL[https://api.semanticscholar.org/CorpusID:49](https://api.semanticscholar.org/CorpusID:49) 310446.\n' +
      '* Cherti et al. (2023) Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)_. IEEE, 2023. doi: 10.1109/cvpr52729.2023.00276. URL[http://dx.doi.org/10.1109/CVPR52729.2023.00276](http://dx.doi.org/10.1109/CVPR52729.2023.00276).\n' +
      '* Dai 등 (2023) Dai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M., Kadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y., Petrovic, V., Singh, M. K., Motwani, S., Wen, Y., Song, Y., Sumbaly, R., Ramanathan, V., He, Z., Vajda, P., and Parikh, D. Emu: Enhancing image generation models in a haystack, 2023.\n' +
      '* Dao et al. (2023) Dao, Q., Phung, H., Nguyen, B., and Tran, A. Flow matching in latent space, 2023.\n' +
      '* Dehghani et al. (2023) Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme, C., Minderer, M., Puigcerver, J., Evci, U., Kumar, M., van Steenkiste, S., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar, V., Kipf, T., Tay, Y., Molesnikov, A., Pavetic, F., Tran, D., Keysers, D., Harmsen, J., and Houlsby, N. F., M. 시력 변환기를 220억 개의 매개변수로 확장, 2023년\n' +
      '* Dhariwal and Nichol (2021) Dhariwal, P. and Nichol, A. Diffusion model beat gans on image synthesis, 2021.\n' +
      '* Dockhorn et al. (2021) Dockhorn, T., Vahdat, A., and Kreis, K. 임계 감쇠 란지빈 확산을 이용한 스코어 기반 생성 모델링. _ arXiv preprint arXiv:2112.07068_, 2021.\n' +
      '* Dockhorn et al. (2022) Dockhorn, T., Vahdat, A., and Kreis, K. 2022년, 고차 잡음 제거 확산 해결기\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ ICLR_, 2020.\n' +
      '* Esser et al. (2023) Esser, P., Chiu, J., Atighehchian, P., Granskog, J., and Germanidis, A. Structure and content-guided video synthesis with diffusion models, 2023.\n' +
      '* Euler(1768) Euler, L. _ 기관 미적분 적분 Bd번 1은 Institutionum calculi 적분이다. 임프. Acad. 임프. Saent., 1768. URL[https://book.google.de/book](https://book.google.de/book) s?id=Vg80AAAQAAJ.\n' +
      '* Fischer et al. (2023) Fischer, J. S., Gui, M., Ma, P., Stracke, N., Baumann, S. A., and Ommer, B. Boosting latent diffusion with flow matching. _ arXiv preprint arXiv:2312.07360_, 2023.\n' +
      '* Ghosh et al. (2023) Ghosh, D., Hajishirzi, H., and Schmidt, L. 유전자val: 텍스트 대 이미지 정렬을 평가하기 위한 객체 중심 프레임워크. _ arXiv preprint arXiv:2310.11513_, 2023.\n' +
      '* Gupta et al. (2023) Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., and Lezama, J. Photorealistic video generation with diffusion models, 2023.\n' +
      '* Hessel et al. (2021) Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and Choi, Y. 클립스코어: 이미지 캡셔닝을 위한 참조 없는 평가 메트릭. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.595. URL[http://dx.doi.org/10.18653/v1/2021.emnlp-main.595](http://dx.doi.org/10.18653/v1/2021.emnlp-main.595)\n' +
      '* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. 2개의 시간 척도 업데이트 규칙에 의해 훈련된 간스는 2017년 로컬 내시 균형(local nash equilibrium)으로 수렴한다.\n' +
      '* Ho & Salimans(2022) Ho, J. and Salimans, T. 분류기 없는 확산 안내 2022년\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020.\n' +
      '* Ho et al. (2022) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., and Salimans, T. Imagen video: High definition video generation with diffusion models, 2022.\n' +
      '* Hoogeboom et al. (2023) Hoogeboom, E., Heek, J., and Salimans, T. 단순 확산: 고해상도 이미지에 대한 종단간 확산, 2023.\n' +
      '* Huang et al. (2023) Huang, K., Sun, K., Xie, E., Li, Z., and Liu, X. T2i-compbench: 오픈 월드 작곡 텍스트 대 이미지 생성을 위한 포괄적인 벤치마크. _ arXiv preprint arXiv:2307.06350_, 2023.\n' +
      '* Hyvarinen(2005) Hyvarinen, A. score matching에 의한 nonnormalized statistical model의 추정 J 마흐 배워 Res._ , 6:695-709, 2005. URL[https://api.semanticschola](https://api.semanticschola) r.org/CorpusID:1152227.\n' +
      '* Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020.\n' +
      '* Karras et al. (2022) Karras, T., Aittala, M., Aila, T., and Laine, S. 확산 기반 생성 모델의 설계 공간을 명시합니다. _ ArXiv_, abs/2206.00364, 2022. URL[https://api.se](https://api.se) mancitscholar.org/CorpusID:249240415.\n' +
      '* Karras et al. (2023) Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. 확산 모델의 훈련 역학 분석 및 개선. _ arXiv preprint arXiv:2312.02696_, 2023.\n' +
      '* Kingma & Gao (2023) Kingma, D. P. and Gao, R. 확산 목표를 간단한 데이터 증강으로 엘보로 이해한다. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Lee et al. (2021) Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. 학습 데이터를 복제하면 언어 모델이 더 좋아집니다. _ arXiv preprint arXiv:2107.06499_, 2021.\n' +
      '* Lee et al. (2023) Lee, S., Kim, B., and Ye, J. C. Minimizing trajectory curvature of ode-based generative models, 2023.\n' +
      '* Lin et al. (2024) Lin, S., Liu, B., Li, J., and Yang, X. 일반적인 확산 노이즈 일람표 및 샘플 단계에 결함이 있습니다. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pp. 5404-5411, 2024.\n' +
      '* Lin et al. (2014) Lin, T. -Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. _Microsoft COCO: Common Objects in Context_, pp. 740-755. Springer International Publishing, 2014. ISBN 9783319106021. doi:10.1007/978-3-319-10602-1_48. URL[http://dx.doi.org/10.1007/978-3-319-10602-1_48](http://dx.doi.org/10.1007/978-3-319-10602-1_48)\n' +
      '* Lipman et al. (2023) Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. 생성 모델링을 위한 흐름 일치입니다. _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=PqvMRDCJT9t](https://openreview.net/forum?id=PqvMRDCJT9t).\n' +
      '* Liu et al. (2022) Liu, X., Gong, C., and Liu, Q. 곧고 빠른 흐름: 2022년 정류된 흐름으로 데이터를 생성하고 전송하는 학습.\n' +
      '* Liu et al.(2023) Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. 인스타플로: 고품질 확산 기반 텍스트-이미지 생성, 2023에 한 단계이면 충분합니다.\n' +
      '* Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. _ ArXiv_, abs/1711.05101, 2017. URL[https://api.semanticscholar.org/Corp](https://api.semanticscholar.org/Corp) usID:3312944.\n' +
      '* Lu et al. (2023) Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast Solver for guided sampling of diffusion probability models, 2023.\n' +
      '* Ma et al. (2024) Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., Vanden-Eijnden, E., and Xie, S. Sit: 확장 가능한 인터폴런트 트랜스포머를 사용하여 흐름 및 확산 기반 생성 모델을 탐색, 2024.\n' +
      '* Nichol(2022) Nichol, A. Dall-e 2 사전 훈련 완화. [https://openai.com/research/dall-e-2-사전 훈련 완화] (https://openai.com/research/dall-e-2-pre-training-mitigations), 2022.\n' +
      '* Nichol & Dhariwal(2021) Nichol, A. and Dhariwal, P. Improved denoising diffusion probability models, 2021.\n' +
      '* NovelAI(2022) NovelAI. Novelai improvement on stable diffusion, 2022. URL[https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10](https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10) d38db82ac.\n' +
      '* Peebles & Xie(2023) Peebles, W. 및 Xie, S. 변압기가 있는 확장 가능한 확산 모델입니다. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2023. doi: 10.1109/iccv51070.2023.00387. URL[http://dx.doi.org/10.1109/ICCV51070.2023.00387](http://dx.doi.org/10.1109/ICCV51070.2023.00387)\n' +
      '* Pernias et al. (2022) Pernias, P., Rampas, D., Richter, M. L., Pal, C. J., and Aubreville, M. Wuerstchen: 2023년 대규모 텍스트-이미지 확산 모델을 위한 효율적인 아키텍처.\n' +
      '* Pizzi et al. (2022) Pizzi, E., Roy, S. D., Ravindra, S. N., Goyal, P., and Douze, M. 이미지 복사 탐지를 위한 자체 감독 기술자. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14532-14542, 2022.\n' +
      '* Po et al. (2023) Po, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J. T., Bermano, A. H., Chan, E. R., Dekel, T., Holynski, A., Kanazawa, A., et al. State of the art on diffusion models for visual computing. _ arXiv preprint arXiv:2310.07204_, 2023.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선, 2023.\n' +
      '* Pooladian et al. (2023) Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., and Chen, R. T. Q. Multisample flow matching: Straightening flows with minibatch coupling, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\n' +
      '* Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. _ arXiv:2305.18290_, 2023.\n' +
      '* Raffel et al. (2019) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J., Exploring the limit of transfer learning with a unified text-to-text transformer, 2019.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 2022년, 클립 래턴트를 이용한 계층적 텍스트 조건 이미지 생성\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)_. IEEE, 2022. doi: 10.1109/cvpr52688.2022.01042. URL[http://dx.doi.org/10.1109/CVPR52688.2022.01042](http://dx.doi.org/10.1109/CVPR52688.2022.01042)\n' +
      '* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. _ U-Net: Convolutional Networks for Biomedical Image Segmentation_, pp. 234-241. Springer International Publishing, 2015. ISBN 9783319245744. doi: 10.1007/978-3-319-24574-4_28. URL[http://dx.doi.org/10.1007/978-3-319-24574-4_28](http://dx.doi.org/10.1007/978-3-319-24574-4_28)\n' +
      '* 252, 2014. URL[https://api.semanticscholar.org/CorpusID:29](https://api.semanticscholar.org/CorpusID:29) 30547.\n' +
      '* Raffel et al. (2015)* Saharia et al. (2022a) Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., and Norouzi, M. 팔레트: 이미지 대 이미지 확산 모델입니다. In _ACM SIGGRAPH 2022 Conference Proceedings_, pp. 1-10, 2022a.\n' +
      '* Saharia et al. (2022b) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. 심층 언어 이해도를 가진 사실적 텍스트-이미지 확산 모델, 2022b.\n' +
      '* Saharia et al. (2022c) Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., and Norouzi, M. 반복적인 정제를 통한 이미지 초해상화 IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4713-4726, 2022c.\n' +
      '* Sauer et al. (2021) Sauer, A., Chitta, K., Muller, J., and Geiger, A. Projected gans converge faster. _ 신경 정보 처리 시스템_, 2021의 발전.\n' +
      '* Sauer et al.(2023) Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. 적대적 확산 증류. _ arXiv preprint arXiv:2311.17042_, 2023.\n' +
      '* Sheynin et al. (2023) Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., and Taigman, Y. 에뮤 편집: 인식 및 생성 작업을 통해 이미지 편집을 정밀하게 수행 arXiv preprint arXiv:2311.10089_, 2023.\n' +
      '* Singer et al. (2022) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. 메이크-어-비디오: 텍스트-비디오 데이터가 없는 텍스트-투-비디오 생성, 2022.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. 비평형 열역학을 이용한 심층 비지도 학습 ArXiv_, abs/1503.03585, 2015. URL[https://api.semanticscholar.org/CorpusID:14888175](https://api.semanticscholar.org/CorpusID:14888175).\n' +
      '* Somepalli et al. (2023a) Somepalli, G., Singla, V., Goldblum, M., Geiping, J., and Goldstein, T. 확산 예술? 디지털 위조? 확산 모델에서 데이터 복제를 조사합니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6048-6058, 2023a.\n' +
      '* Somepalli et al. (2023b) Somepalli, G., Singla, V., Goldblum, M., Geiping, J., and Goldstein, T. 확산 모델의 복사 이해 및 완화 arXiv preprint arXiv:2305.20086_, 2023b.\n' +
      '* Song et al. (2022) Song, J., Meng, C., and Ermon, S. 2022년 확산 암시적 모델 잡음 제거\n' +
      '* Song and Ermon(2020) Song, Y. 및 Ermon, S. 2020년 데이터 분포의 기울기를 추정하여 생성 모델링을 수행합니다.\n' +
      '* Song et al. (2020) Song, Y., Sohl-Dickstein, J. N., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _ ArXiv_, abs/2011.13456, 2020. URL[https://api.semanticscholar.org/CorpusID:227209335](https://api.semanticscholar.org/CorpusID:227209335).\n' +
      '* Tong et al. (2023) Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. 미니 배치 최적 전송, 2023으로 흐름 기반 생성 모델을 개선하고 일반화한다.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need, 2017.\n' +
      '* Villani(2008) Villani, C. Optimal transport: Old and New. 2008. URL[https://api.semanticscholar.org/Corp](https://api.semanticscholar.org/Corp) usID:118347220.\n' +
      '* Vincent(2011) Vincent, P. A connection between score matching and denoising autoencoder. _ Neural Computation_, 23:1661-1674, 2011. URL[https://api.semanticscholar.org/CorpusID:5560643](https://api.semanticscholar.org/CorpusID:5560643).\n' +
      '* Wallace et al. (2023) Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. 직접 선호도 최적화를 이용한 확산 모델 정렬 arXiv:2311.12908_, 2023.\n' +
      '* Wang et al. (2023) Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual expert for prerained language models. _ arXiv preprint arXiv:2311.03079_, 2023.\n' +
      '* Wortsman et al. (2023) Wortsman, M., Liu, P. J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. 대규모 변압기 훈련 불안정성을 위한 소규모 프록시, 2023년\n' +
      '* Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. _ ARXiv:2206.10789_, 2022.\n' +
      '* Zhai et al. (2022) Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. 비전 트랜스포머를 확장합니다. _CVPR_, pp. 12104-12113, 2022.\n' +
      '* Zhang & Sennrich (2019) Zhang, B. and Sennrich, R. Root mean square layer normalization, 2019.\n' +
      '\n' +
      '**Supplementary**\n' +
      '\n' +
      '## 부록 배경\n' +
      '\n' +
      '확산 모델(Sohl-Dickstein et al., 2015; Song et al., 2020; Ho et al., 2020)은 데이터를 노이즈로 변환하는 확률적 순방향 프로세스에 역 ODE를 근사하여 데이터를 생성한다. 그들은 이미지의 생성 모델링을 위한 표준 접근법이 되었다(Dhariwal and Nichol, 2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Balaji et al., 2022) 및 비디오(Singer et al., 2022; Ho et al., 2022; Esser et al., 2023; Blattmann et al., 2023; Gupta et al., 2023). 이러한 모델은 음의 우도에 대한 변분 하한(Sohl-Dickstein et al., 2015) 및 점수 매칭(Hyvarinen, 2005; Vincent, 2011; Song and Ermon, 2020), 정방향 및 역방향 프로세스의 다양한 공식(Song et al., 2020; Dockhorn et al., 2021), 모델 매개변수화(Ho et al., 2020; Ho and Salimans, 2022; Karras et al., 2022), 손실 가중치(Ho et al., 2020; Karras et al., 2022) 및 ODE 솔버(Song et al., 2022; Lu et al., 2023; Dockhorn et al., 2022)를 통해 모두 도출될 수 있기 때문에, 많은 수의 상이한 훈련 목표 및 샘플링 절차가 야기되었다. 최근 Kingma and Gao (2023)와 Karras et al. (2022)의 정성적 연구는 통일된 공식을 제안하고 훈련에 대한 새로운 이론적, 실무적 통찰(Karras et al., 2022; Kingma and Gao, 2023)과 추론(Karras et al., 2022)을 도입하였다. 그러나, 이러한 개선에도 불구하고, 공통 ODE의 궤적은 부분적으로 상당한 양의 곡률을 수반하며(Karras et al., 2022; Liu et al., 2022), 이는 증가된 양의 솔버 단계를 필요로 하고, 따라서 빠른 추론을 어렵게 한다. 이를 극복하기 위해 우리는 제형이 직선 ODE 궤적을 학습할 수 있는 정류된 흐름 모델을 채택한다.\n' +
      '\n' +
      'Rectified Flow Models(Liu et al., 2022; Albergo and Vanden-Eijnden, 2022; Lipman et al., 2023)은 상미분 방정식(ODE)을 통해 두 분포 사이의 수송 지도를 구성하여 생성 모델링을 접근한다. 이 접근법은 확산 모델뿐만 아니라 연속 정규화 흐름(CNF)(Chen et al., 2018)에 밀접한 연결을 가지고 있다. CNF에 비해 Rectified Flow와 Stochastic Interpolants는 훈련 시 ODE의 시뮬레이션이 필요하지 않다는 장점이 있다. 확산 모델들과 비교하여, 이들은 확산 모델들과 연관된 확률 흐름 ODE(Song et al., 2020)보다 시뮬레이션하기에 더 빠른 ODE들을 초래할 수 있다. 그럼에도 불구하고, 이들은 최적의 수송 솔루션을 초래하지 않으며, 다수의 작업들은 궤적 곡률을 더 최소화하는 것을 목표로 한다(Lee et al., 2023; Tong et al., 2023; Pooladian et al., 2023). (Dao et al., 2023; Ma et al., 2024)는 클래스-조건부 이미지 합성을 위한 정류 흐름 제형의 실현 가능성을 입증하고, 잠재-공간 업샘플링을 위한 (Fischer et al., 2023), 및 (Liu et al., 2023)은 미리 훈련된 텍스트-투-이미지 모델을 증류하기 위해 (Liu et al., 2022)의 리플로우 절차를 적용한다(Rombach et al., 2022). 여기에서 우리는 샘플링 단계가 적은 텍스트 대 이미지 합성의 기초로서 정류된 흐름에 관심이 있다. 다양한 제형과 손실 가중치 간의 광범위한 비교를 수행하고 개선된 성능으로 정류 흐름 훈련을 위한 새로운 타임스텝 일정을 제안한다.\n' +
      '\n' +
      'Scaling Diffusion ModelsThe transformer architecture(Vaswani et al., 2017)는 NLP(Kaplan et al., 2020) 및 컴퓨터 비전 태스크(Dosovitskiy et al., 2020; Zhai et al., 2022)에서 스케일링 특성으로 잘 알려져 있다. 확산 모델의 경우, U-Net 아키텍처(Ronneberger et al., 2015)가 지배적인 선택이었다(Ho et al., 2020; Rombach et al., 2022; Balaji et al., 2022). 최근 일부 연구는 확산 변압기 백본(Peebles and Xie, 2023; Chen et al., 2023; Ma et al., 2024)을 탐구하는 반면 텍스트 대 이미지 확산 모델에 대한 스케일링 법칙은 아직 탐구되지 않았다.\n' +
      '\n' +
      '## 부록 B On Flow Matching\n' +
      '\n' +
      '시뮬레이션이 없는 흐름 훈련에 관한### 세부사항\n' +
      '\n' +
      '(Lipman et al., 2023), \\(u_{t}(z)\\)이 \\(p_{t}\\)을 생성한다는 것을 알기 위해, 우리는 연속성 방정식이 필요충분조건을 제공한다는 것에 주목한다(Villani, 2008):\n' +
      '\n' +
      'dt{d}{frac{d}p_{t}(x)+\\nabla\\cdot[p_{t}(x)v_{t}(x)]=0\\leftrightarrow v_{t}\\text{ generates probability density path }p_{t}. \\tag{26}\\text{\n' +
      '\n' +
      '따라서 본 연구의 결과는 다음과 같다.\n' +
      '\n' +
      'b{E}_{\\epsilon\\sim\\mathcal{N}(0,I)p_{t}(z|\\epsilon)\\frac{p_{t}(z|\\epsilon)}{p_{t}(z|\\epsilon)}{p_{t}(z|\\epsilon)}{N}(z|\\epsilon)}-\\nabla\\cdot[u_{t}(z|\\epsilon)p_{t}(z)]\\frac{n}(0,I)p_{t}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\\epsilon)\\frac{n}(z|\n' +
      '\n' +
      '여기서 우리는 \\(u_{t}(z|\\epsilon)\\)이 \\(p_{t}(z|\\epsilon)\\)을 생성하고, \\(u_{t}(z|\\epsilon)\\)와 \\(p_{t}(z|\\epsilon)\\)과 \\(27)의 식(6)의 정의를 줄 식(28)에서 식(29)에 대한 연속성 방정식 식(26)을 사용했다.\n' +
      '\n' +
      '목적의 동등성\\(\\mathcal{L}_{FM}\\leftrightarrows\\mathcal{L}_{CFM}\\)(Lipman et al., 2023)은 다음과 같다.\n' +
      '\n' +
      '\\mathcal{L}_{FM}(\\Theta) =\\mathbb{E}_{t,p_{t}(z)|||v_{t,p_{t}(z)||_{t}-2\\mathbb{E}_{t,p_{t}(z)||_{t}(z)||\\epsilon)||v_{t,p_{t}(z)||\\epsilon)||v_{t,p_{t}(z)|\n' +
      '\n' +
      '여기서 \\(c,c^{\\prime}\\)은 \\(\\theta\\)에 의존하지 않고, 라인 식 (31) 내지 라인 식 (32)는 다음과 같다:\n' +
      '\n' +
      'b{E}_{p_{t}(z|\\epsilon),p(\\epsilon)\\rangle v_{\\theta}(z,t)\\,|\\,u_{t}(z)\\rangle\\frac{p_{t}(z|\\epsilon)\\langle v_{\\theta}(z,t)\\,|\\,u_{t}(z)\\rangle v_{\\theta}(z,t)\\,|\\,u_{t}(z)\\rangle\n' +
      '\n' +
      '여기서 우리는 줄 식(35)에서 \\(\\frac{p_{t}(z)}{p_{t}(z)}\\)으로 확장하고 줄 식(35)에서 식(6)의 정의를 식(36)으로 사용했다.\n' +
      '\n' +
      '### 이미지 및 텍스트 표현에 대한 세부사항\n' +
      '\n' +
      '잠재적 이미지 표현** LDM(Rombach et al., 2022)을 따르고 미리 훈련된 오토인코더를 사용하여 더 작은 잠재 공간 \\(x=E(X)\\in\\mathbb{R}^{h\\times w\\times d}\\)에서 RGB 이미지 \\(X\\in\\mathbb{R}^{h\\times W\\times 3}\\)을 표현한다. 5.2.1절에서 \\(h=\\frac{H}{8}\\)과 \\(w=\\frac{W}{8}\\)의 공간 다운샘플링 팩터를 사용하고 \\(w=\\frac{W}{8}\\)에 대해 서로 다른 값을 실험한다. 우리는 항상 식 2로부터 포워드 프로세스를 잠재 공간에서 적용하고, 식 1을 통해 표현 \\(x\\)을 샘플링할 때, 그것을 디코더 \\(D\\)을 통해 다시 픽셀 공간 \\(X=D(x)\\)으로 디코딩한다. 우리는 Rombach et al.(2022)을 따르고 훈련 데이터의 하위 집합에 걸쳐 전역적으로 계산되는 잠복기를 평균 및 표준 편차로 정규화한다. 그림 10은 섹션 5.2.1에서 논의된 바와 같이 다른 \\(d\\)에 대한 생성 모델 훈련이 모델 용량의 함수로 어떻게 진화하는지 보여준다.\n' +
      '\n' +
      '**Text Representation** 잠재 표현들에 대한 이미지들의 인코딩과 유사하게, 우리는 또한 이전의 접근법들(Saharia et al., 2022; Balaji et al., 2022)을 따르고 사전 훈련된, 동결된 텍스트 모델들을 사용하여 텍스트 컨디셔닝 \\(c\\)을 인코딩한다. 특히, 모든 실험에 대해 CLIP(Radford et al., 2021) 모델과 인코더-디코더 텍스트 모델의 조합을 사용한다. 구체적으로 Radford 등의 CLIP L/14 모델(2021)과 Cherti 등의 OpenCLIP bigG/14 모델(2023)의 텍스트 인코더로 \\(c\\)을 인코딩한다. 벡터컨디셔닝(c_{\\text{vec}\\in\\mathbb{R}^{2048}\\)을 얻기 위해 각각 768\\과 1280\\ 크기의 풀링된 출력들을 연결한다. 또한 CLIPcontext conditioning(c_{\\text{crit}}^{\\text{CLIP}\\in\\mathbb{R}^{77\\times 2048}\\)에 채널별 두 번째 숨겨진 표현을 연결한다. 다음으로, T5-v1.1-XXL 모델(Raffel et al., 2019)의 인코더의 최종 은닉 표현인 \\(c_text{crit}}^{\\text{TS}\\in\\mathbb{R}^{77\\times 4096}\\)까지 인코딩한다. 마지막으로 채널축을 따라 0-pad\\(c_{\\text{crit}}^{\\text{CLIP}\\)을 4096\\ 차원에 맞추어 T5 표현과 일치시키고 시퀀스축을 따라 \\(c_{\\text{crit}}^{\\text{TS}\\)으로 연결하여 최종 문맥 표현 \\(c_{\\text{cxi}\\in\\mathbb{R}^{154\\times 4096}\\)을 얻는다. 이 두 캡션 표현, \\(c_{\\text{vec}}\\) 및 \\(c_{\\text{cxi}}\\)은 섹션 4에 설명된 바와 같이 두 가지 다른 방식으로 사용된다.\n' +
      '\n' +
      '### 섹션 5.1의 실험을 위한 예비.\n' +
      '\n' +
      '**데이터세트** 표준 텍스트 대 이미지 벤치마크의 결락을 설명하기 위해 두 개의 데이터세트를 사용합니다. 널리 사용되는 데이터셋으로서 이미지넷 데이터셋(Russakovsky et al., 2014)을 이미지에 "{클래스 이름}의 사진" 형태의 캡션을 추가하여 텍스트 대 이미지 모델에 적합한 데이터셋으로 변환하며, 여기서 {클래스 이름}은 이미지의 클래스 레이블에 대해 제공된 이름 중 하나에서 무작위로 선택된다. 보다 현실적인 텍스트 대 이미지 데이터셋으로 CC12M 데이터셋(Changpinyo et al., 2021)을 활용하여 학습한다.\n' +
      '\n' +
      '**최적화** 본 실험에서는 학습률 \\(10^{-4}\\) 및 1000 선형 워밍업 단계의 AdamW 최적화기(Loshchilov and Hutter, 2017)를 사용하여 1024의 전역 배치 크기를 사용하여 모든 모델을 훈련한다. 우리는 혼합 정밀 훈련을 사용하고 감쇠 계수 \\(0.99\\)를 사용하여 지수 이동 평균(EMA)으로 100개의 훈련 배치마다 업데이트되는 모델 가중치의 사본을 유지한다. 무조건 확산 유도(Ho and Salimans, 2022)를 위해 3개의 텍스트 인코더의 출력을 각각 0(46.4\\%\\)의 확률로 설정하여 모든 단계의 10\\%\\에서 무조건 모델을 대략적으로 학습시킨다.\n' +
      '\n' +
      '**평가** 섹션 5.1에 설명된 바와 같이, COCO-2014 검증 분할에 대한 훈련 동안 정기적으로 모델을 평가하기 위해 CLIP 점수, FID 및 검증 손실을 사용한다(Lin et al., 2014).\n' +
      '\n' +
      '손실 값은 시간 간격에 따라 크기와 분산이 크게 다르기 때문에 시간 간격 \\([0,1]\\)의 등간격 8개 값에 대해 계층화된 방식으로 평가한다.\n' +
      '\n' +
      '서로 다른 샘플러 설정에서 서로 다른 접근법이 어떻게 작용하는지를 분석하기 위해 각 샘플러에 대해 1000개의 샘플을 생산하며 샘플링 단계의 수와 지침 규모가 다르다. 우리는 CLIP L/14(Radford et al., 2021)를 사용하여 CLIP 점수로 이러한 샘플을 평가하고 또한 이러한 샘플의 CLIP L/14 이미지 특징과 검증 세트의 이미지 사이의 FID를 계산한다. 샘플링을 위해 우리는 항상 식 1의 오일러 이산화(오일러, 1768)와 6가지 다른 설정, 즉 분류기 없는 유도 척도가 1.0, 2.5, 5.0인 50단계와 분류기 없는 유도 척도가 5.0인 5, 10, 25단계를 사용한다.\n' +
      '\n' +
      '정류 모델을 위한 SNR 샘플러 개선\n' +
      '\n' +
      '섹션 2에 설명된 대로 정류된 흐름 모델을 훈련하는 데 사용하는 타임스텝에 대한 새로운 밀도\\(\\pi(t)\\)를 소개한다. 도 11은 섹션 3.1에서 소개된 **로짓-노멀 샘플러** 및 **모드 샘플러**의 분포를 시각화한다. 특히, 섹션 5.1에서 보여주듯이, **로짓-노멀 샘플러**는 고전적인 균일 정류 흐름 공식(Liu et al., 2022) 및 확립된 EDM(Karras et al., 2022) 및 LDM-Linear(Rombach et al., 2022)와 같은 확산 기준선들을 능가한다.\n' +
      '\n' +
      '도 10: 섹션 5.2.1에서 논의된 바와 같이 상이한 오토인코더들(4개의 잠재 채널들, 8개의 채널들 및 16개의 채널들)의 잠재 공간 상에서 상이한 크기들(그들의 깊이를 통해 파라미터화된)을 갖는 플로우 모델들을 트레이닝한 후의 FID 점수들. 예상대로, 16-채널 오토인코더 공간 상에서 트레이닝된 플로우 모델은 유사한 성능을 달성하기 위해 더 많은 모델 용량이 필요하다. 깊이\\(d=22\\)에서 8-chn과 16-chn 사이의 간격은 무시할 수 있다. 우리는 궁극적으로 훨씬 더 큰 모델 크기로 확장하는 것을 목표로 하기 때문에 16-chn 모델을 선택한다.\n' +
      '\n' +
      '그림 11: 훈련 시간표의 샘플링을 편향시키기 위해 탐색하는 모드(왼쪽) 및 로짓-정규(오른쪽) 분포.\n' +
      '\n' +
      '도 12: **스케일링의 정성적 효과.** 디스플레이됨은 스케일링 트레이닝 단계들(좌에서 우로: 50k, 200k, 350k, 500k) 및 모델 크기들(상단에서 아래로: 깊이=15, 30, 38)이 PartiPrompts에 미치는 영향을 입증하는 예들로서, 트레이닝 지속기간 및 모델 복잡성의 영향을 강조한다.\n' +
      '\n' +
      '직접 선호도 최적화(Direct Preference Optimization, DPO)(Rafailov et al., 2023)는 선호도 데이터로 LLMs를 미세화하는 기술이다. 최근에, 이 방법은 텍스트-이미지 확산 모델들의 선호 미세조정에 적응되었다(Wallace et al., 2023). 이 섹션에서는 모델도 선호도 최적화를 준수하는지 확인합니다. 특히, Wallace et al.(2023)에 소개된 방법을 우리의 2B 및 8B 파라미터 기반 모델에 적용한다. 전체 모델을 미세 조정하기보다는 일반적인 관행과 같이 모든 선형 계층에 대해 (순위 128의) 학습 가능한 낮은 순위 적응 행렬(LoRA)을 소개한다. 우리는 2B 및 8B 기본 모델에 대해 각각 4k 및 2k 반복에 대해 이러한 새로운 매개변수를 미세 조정한다. 그런 다음 Partiprompts 집합(Yu et al., 2022)의 128자막 부분집합(대략 프롬프트 및 비교당 3명의 유권자)을 사용하여 인간 선호도 연구에서 결과 모델을 평가한다. 그림 14는 우리의 기본 모델이 인간의 선호도에 따라 효과적으로 조정될 수 있음을 보여준다. 그림 13은 각 기본 모델과 DPO-finetuned 모델의 샘플을 보여준다.\n' +
      '\n' +
      '## 명령어 기반 이미지 편집을 위한 부록 D Finetuning\n' +
      '\n' +
      '트레이닝 명령 기반 이미지 편집 및 일반적인 이미지-대-이미지 확산 모델들을 위한 공통 접근법은, 입력을 U-Net에 공급하기 전에 채널 차원을 따라 확산 타겟의 잡음화된 래턴트에 입력 이미지의 래턴트를 연결하는 것이다(Brooks et al., 2023; Sheynin et al., 2023; Saharia et al., 2022; 20). 우리는 패치하기 전에 채널을 따라 입력 및 타겟을 연결하는 동일한 접근법을 따르고, 제안된 아키텍처에 동일한 방법이 적용 가능함을 입증한다. InstructPix2Pix 데이터셋(Brooks et al., 2023)의 분포와 유사한 이미지 대 이미지 편집 작업과 Emu Edit and Palette(Sheynin et al., 2023; Saharia et al., 2022)와 유사한 인페인팅, 세그멘테이션, 컬러화, 디블러링 및 컨트롤넷 작업으로 구성된 데이터셋에 2B 파라미터 기반 모델을 미세화한다. 도 15에 도시된 바와 같이, 우리는 결과적인 2B 편집 모델이 주어진 이미지에서 텍스트를 조작할 수 있는 능력을 갖는다는 것을 관찰한다.\n' +
      '\n' +
      '그림 13: 기본 모델과 DPO-finetuned 모델 간의 비교. DPO-피네튜닝은 일반적으로 더 나은 철자와 함께 더 미적으로 즐거운 샘플을 초래한다.\n' +
      '\n' +
      '그러나 텍스트 조작 작업은 훈련 데이터에 포함되지 않았다. 동일한 데이터에 대해 SDXL 기반(Podell et al., 2023) 편집 모델을 훈련할 때 유사한 결과를 재현할 수 없었다.\n' +
      '\n' +
      '## 대용량 텍스트-이미지 트레이닝을 위한 부록 E 데이터 전처리\n' +
      '\n' +
      '### 이미지 전처리와 텍스트 임베딩\n' +
      '\n' +
      '우리의 모델은 미리 훈련된 다수의 냉동 네트워크의 출력을 입력(자동 인코더 래턴트 및 텍스트 인코더 표현)으로 사용한다. 이러한 출력은 훈련 중에 일정하기 때문에 전체 데이터 세트에 대해 한 번 사전 계산한다. 이것은 두 가지 주요 이점들과 함께 온다: (i) 인코더들은 트레이닝 동안 GPU에서 이용가능할 필요가 없고, 필요한 메모리를 낮춘다. (ii) 순방향 인코딩 패스는 트레이닝 동안 건너뛰어, 첫 번째 에포크 이후에 시간 및 총 필요한 컴퓨트를 절약하고, 탭. 7을 참조한다.\n' +
      '\n' +
      '이 방법은 두 가지 단점이 있다. 첫째, 각 표본에 대한 모든 에폭에 대한 무작위 확대가 불가능하며 이미지 래턴트의 사전 계산 동안 정사각형 중심 절단을 사용한다. 더 높은 해상도로 모델을 미세 조정하기 위해 여러 개의 종횡비 버킷을 지정하고 가장 가까운 버킷으로 크기를 조정하고 크롭한 다음 해당 종횡비를 미리 계산합니다. 둘째, 텍스트 인코더의 조밀한 출력이 특히 커서, 추가 저장 비용 및 트레이닝 동안 더 긴 로딩 시간을 생성한다(탭. 7). 우리는 실제 성능 저하를 관찰하지 않기 때문에 언어 모델의 임베딩을 반 정밀도로 저장합니다.\n' +
      '\n' +
      '### 이미지 기억 방지\n' +
      '\n' +
      '생성 이미지 모델들의 맥락에서 트레이닝 샘플들의 암기는 다수의 이슈들로 이어질 수 있다(Sompalli et al., 2023; Carlini et al., 2023; Sompalli et al., 2023b). 학습된 모델에 의한 이미지의 버바텀 복사본을 피하기 위해, 우리는 학습 데이터 세트에 중복 예제를 주의 깊게 스캔하고 제거한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & Mem [GB] & FP [ms] & Storage [kB] & Delta [\\%] \\\\ \\hline VAE (Enc) & 0.14 & 2.45 & 65.5 & 13.8 \\\\ CLIP-L & 0.49 & 0.45 & 121.3 & 2.6 \\\\ CLIP-G & 2.78 & 2.77 & 202.2 & 15.6 \\\\ T5 & 19.05 & 17.46 & 630.7 & 98.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 동결된 입력 네트워크를 사전 인코딩하기 위한 **Key figures.** Mem은 GPU에 모델을 로드하는 데 필요한 메모리이다. FP[ms]는 장치당 배치 크기가 32인 순방향 패스에 대한 샘플당 시간이다. 저장소는 단일 샘플을 저장하기 위한 크기이다. Delta[\\%]는 2B MMDiT-Model(568ms/it)의 루프에 이를 추가할 때 훈련 단계가 얼마나 더 오래 걸리는지이다.\n' +
      '\n' +
      '그림 14: 기본 모델과 DPO-finetuned 모델 간의 인간 선호도 평가. 인간 평가자들은 신속한 후속 조치와 일반적인 품질 모두를 위해 DPO-최종 조정 모델을 선호한다.\n' +
      '\n' +
      'Deduplication에 관한 상세한 내용은 Carlini et al. (2023) 및 Sompalli et al. (2023a)에 의해 요약된 방법들에 따라, 우리는 중복제거 프로세스의 백본으로서 SSCD (Pizzi et al., 2022)를 선택한다. SSCD 알고리즘은 스케일에서 근접 중복 이미지를 탐지하는 최첨단 기술로 클러스터링 및 기타 다운스트림 작업에 사용할 수 있는 고품질 이미지 임베딩을 생성한다. 우리는 또한 Nichol (2022)을 따라 다수의 클러스터를 결정하기로 결정했다. 실험을 위해 \\(N=16,000\\)을 사용한다.\n' +
      '\n' +
      '클러스터링을 위해 Autofaiss(2023)를 이용한다. Autofaiss(2023)는 대규모 클러스터링 작업에 Faiss(Facebook AI Similarity Search)를 활용하는 과정을 단순화한 라이브러리이다. 특히 FAISS 인덱스 팩토리1 기능을 활용하여 미리 정의된 수의 중심점으로 사용자 지정 인덱스를 교육합니다. 이 접근법은 이미지 임베딩과 같은 고차원 데이터의 효율적이고 정확한 클러스터링을 허용한다.\n' +
      '\n' +
      '각주 1: [https://github.com/facebookresearch/faiss/wiki/The-index-factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n' +
      '\n' +
      '알고리즘 1은 중복 제거 접근 방식을 자세히 설명합니다. 그림 16b와 같이 서로 다른 SSCD 임계값에 의해 데이터가 얼마나 제거되는지 확인하기 위해 실험을 실행했다. 이러한 결과를 기반으로 최종 실행 그림 16a에 대해 4개의 임계값을 선택했다.\n' +
      '\n' +
      '도 15: 2B 편집 모델을 이용한 제로 샷 텍스트 조작 및 삽입\n' +
      '\n' +
      '중복제거 노력의 효과 평가\n' +
      '\n' +
      'Carlini et al.(2023)은 표준 접근법을 사용하여 이미지를 생성하는 2단계 데이터 추출 공격을 고안하고, 특정 멤버십 추론 스코어링 기준을 초과하는 것을 플래그한다. Carlini et al. (2023)은 이들의 검색이 중복되지 않은 예들보다 암기될 가능성이 더 크기 때문에, 그들의 검색을 중복 트레이닝 예들로 편향시킨다(Somepalli et al., 2023a;a; Lee et al., 2021).\n' +
      '\n' +
      'SSCD 기반 중복제거가 얼마나 잘 작동하는지 평가하기 위해, 우리는 칼리니 등(2023)을 따라 소형, 특히 이러한 목적으로 훈련된 모델로부터 암기된 샘플을 추출하고 중복제거 전후의 비교를 수행한다. 언급된 절차의 두 가지 주요 단계는: 1)을 포함한다. 표준 샘플링 방식으로 그리고 알려진 프롬프트와 함께 확산 모델을 사용하여 많은 예를 생성합니다. 2) 멤버십 추론을 수행하여 모델의 소설 세대와 암기된 훈련 사례인 세대를 분리한다. 알고리즘 2는 Carlini et al.(2023)을 기반으로 암기된 샘플을 찾는 단계를 보여준다. 이 기법은 SD-2.1 모델의 경우와 SD2.1 아키텍처를 기반으로 하지만 SSCD를 사용하여 제거된 정확한 중복 및 근접 중복에 대해 훈련된 모델의 경우 두 번 실행된다는 점에 유의한다(Pizzi et al., 2022).\n' +
      '\n' +
      '임계값이 0.5인 SSCD(Pizzi et al., 2022) 기반 학습 데이터셋에서 가장 중복된 350,000개의 예를 선택하고, 각 텍스트 프롬프트에 대해 500개의 후보 이미지를 생성하여 암기 가능성을 높인다. 직관은 확산모형에서 두 개의 다른 무작위 초기종자에 대해 높은 확률\\(Gen(p;r_{1})\\approx_{d}Gen(p;r_{2})\\)을 갖는 것이다. 한편, 일부 거리 측정 d에서 \\(Gen(p;r_{1})\\approx_{d}Gen(p;r_{2})\\)이 존재한다면, 이러한 생성된 샘플들은 암기된 예일 가능성이 높다. 두 영상간의 거리측도를 계산하기 위해 수정된 유클리드 거리(l_{2}\\)를 사용한다. 특히, 우리는 많은 세대가 종종 \\(l_{2}\\) 거리에 따라 매우 유사하다는 것을 발견했다(예를 들어, 모두 회색 배경을 가지고 있다). 따라서 각 영상을 16개의 겹치지 않는 128 \\(\\times\\) 128개의 타일로 분할하고, 두 영상 사이의 임의의 한 쌍의 영상 타일 사이의 \\(l_{2}\\) 거리의 최대값을 측정한다. 그림 17은 거의 중복되지 않은 샘플을 제거하기 위해 임계값이 0.5인 SSCD를 사용하기 전과 후의 암기된 샘플 수를 비교한 것이다. Carlini et al.(2023)은 암기된 샘플로서 10의 클릭 크기 내의 이미지들을 마킹한다. 여기에서 우리는 또한 파벌에 대한 다양한 크기를 탐색합니다. 모든 클리어 임계값에 대해 SSCD는 암기된 샘플의 수를 상당히 줄일 수 있다. 구체적으로, 클릭 크기가 10인 경우, SSCD(=0.5\\)에서 단절된 중복 제거된 훈련 표본에 대한 훈련된 SD 모델은 잠재적으로 암기된 예에서 \\(5\\times\\)의 감소를 보인다.\n' +
      '\n' +
      '```\n' +
      '1:vecs - 단일 클러스터 내의 벡터들의 리스트, 아이템들 - vecs에 대응하는 아이템 ID들의 리스트, 인덱스 - 클러스터 내의 유사성 검색을 위한 FAISS 인덱스, thresh - 중복을 결정하기 위한 임계값\n' +
      '2 : 출력 : dup - 중복 항목 ID 집합\n' +
      '3:dup \\(\\leftarrow\\) new set\\(()\\)\n' +
      '4:for\\(i\\gets 0\\)to\\(\\operatorname{length}(\\operatorname{vecs})-1\\)do\n' +
      '5:qs\\(\\leftarrow\\operatorname{vecs}[i]\\}{Current vector}\n' +
      '6:qid\\(\\leftarrow\\operatorname{items}[i]\\}Current item ID}\n' +
      '7:\\(\\operatorname{lims},D,I\\leftarrow\\operatorname{index.range.search}(\\text{qs,thresh})\\)\n' +
      '8:ifqid \\(\\in\\)dup then\n' +
      '9:continue\n' +
      '10:endif\n' +
      '11:start \\(\\leftarrow\\)lims\\([0]\\)\n' +
      '12:end \\(\\leftarrow\\)lims\\([1]\\)\n' +
      '13:duplicate_indices \\(\\leftarrow\\)\\(I[start:end]\\)\n' +
      '14:duplicate_ids \\(\\leftarrow\\) new list\\(()\\)\n' +
      '15:for\\(j\\in duplicate_indices do\n' +
      '16:if\\(\\operatorname{items}[j]\\neq\\)qid then\n' +
      '17:duplicate_ids.append\\((\\operatorname{items}[j])\\)\n' +
      '18:endif\n' +
      '19:endfor\n' +
      '20:dups.update\\((\\operatorname{duplicate\\_ids})\\)\n' +
      '21:endfor\n' +
      '22 : 리턴드업{Final set of duplicate ID}\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 클러스터에서 중복 항목 찾기\n' +
      '0: 프롬프트의 집합 \\(P\\), 프롬프트당 세대수 \\(N\\), 유사성 임계값 \\(\\epsilon=0.15\\), 기억 임계값 \\(T\\)\n' +
      '0 : 생성된 샘플에서 암기된 이미지의 검출\n' +
      '1: \\(D\\)을 가장 중복된 예들의 집합으로 초기화\n' +
      '2: 각 프롬프트 \\(p\\in P\\)do에 대해\n' +
      '3:for\\(i=1\\) to \\(N\\)do\n' +
      '4: 랜덤 시드 \\(r_{i}\\)을 갖는 이미지 \\(\\mathrm{Gen}(p;r_{i})\\) 생성\n' +
      '5:endfor\n' +
      '6:endfor\n' +
      '7: 생성된 이미지의 각 쌍에 대해 \\(x_{i},x_{j}\\)do\n' +
      '8:if distance \\(d(x_{i},x_{j})<\\epsilon\\)then\n' +
      '9: 그래프 \\(G\\)에서 \\(x_{i}\\)와 \\(x_{j}\\) 연결\n' +
      '10:endif\n' +
      '11:endfor\n' +
      '12: \\(G\\)do의 각 노드에 대해\n' +
      '13 : 노드를 포함하는 가장 큰 클릭 찾기\n' +
      '14 : 클릭 \\(\\geq T\\)then의 크기\n' +
      '15 : 클락 내의 이미지를 암기된 대로 표시\n' +
      '16:endif\n' +
      '17:endfor\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 생성영상에서 암기 검출\n' +
      '\n' +
      '도 16: 다양한 필터링 임계값에 대한 우리의 트레이닝 데이터 세트를 중복제거한 결과.\n' +
      '\n' +
      '도 17: **SSCD-기반 중복제거는 암기를 방지한다.** 우리의 SSCD-기반 중복제거가 얼마나 잘 작동하는지 평가하기 위해, 우리는 작은, 특히 이 목적을 위해 훈련된 모델로부터 암기된 샘플을 추출하고 중복제거 전후에 비교한다. 우리는 거의 복제된 샘플을 제거하기 위해 임계값이 0.5인 SSCD를 사용하기 전과 후의 암기된 샘플 수 간의 비교를 플로팅한다. Carlini et al.(2023)은 암기된 샘플로서 10의 클릭 크기 내의 이미지들을 마킹한다. 여기에서 우리는 또한 파벌에 대한 다양한 크기를 탐색합니다. 모든 클리어 임계값에 대해 SSCD는 암기된 샘플의 수를 상당히 줄일 수 있다. 구체적으로, 클릭 크기가 10인 경우, SSCD(=0.5\\)에서 단절된 중복제거 훈련 표본에 대한 모델들은 잠재적으로 암기된 예에서 \\(5\\times\\)의 감소를 보인다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>