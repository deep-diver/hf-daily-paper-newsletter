<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Direct-a-Video: 사용자 지향 카메라 움직임과 객체 움직임을 이용한 맞춤형 영상 생성\n' +
      '\n' +
      '시위안 양 1,3, 량후2, 하이빈황2, 청양마2,\n' +
      '\n' +
      '팽페이완2, 디장2, 샤오동천3, 징요원\n' +
      '\n' +
      '홍콩 1시립대학, 2과이슈기술, 3천진대학교\n' +
      '\n' +
      '[https://direct-a-video.github.io/](https://direct-a-video.github.io/)\n' +
      '\n' +
      '각주 1: 교신저자.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 텍스트-비디오 확산 모델은 인상적인 진전을 이루었다. 실제로, 사용자들은 종종 맞춤형 비디오 생성을 위해 객체 모션 및 카메라 움직임을 독립적으로 제어하는 능력을 원한다. 그러나, 현재 방법들은 객체 움직임과 카메라 움직임을 분리하여 제어하는데 초점을 맞추지 못하여 텍스트-비디오 모델의 제어성과 유연성을 제한한다. 본 논문에서는 사용자가 동영상을 연출하듯 하나 또는 다수의 객체 및/또는 카메라의 움직임에 대해 독립적으로 동작을 지정할 수 있는 시스템인 Direct-a-Video를 소개한다. 본 논문에서는 객체 움직임과 카메라 움직임의 비결합 제어를 위한 간단하면서도 효과적인 방법을 제안한다. 객체 모션은 모델의 고유한 이전을 사용하여 **공간 교차 주의** 변조를 통해 제어되므로 추가 최적화가 필요하지 않다. 카메라 움직임을 위해 정량적 카메라 움직임 매개변수를 해석하기 위해 새로운 **시간 교차 주의** 레이어를 도입한다. 우리는 소규모 데이터 세트에서 이러한 레이어를 자체 감독 방식으로 훈련하기 위해 증강 기반 접근법을 추가로 사용하여 명시적인 모션 주석이 필요하지 않다. 두 구성 요소는 독립적으로 작동하여 개별 또는 결합된 제어를 허용하고 개방형 도메인 시나리오로 일반화할 수 있다. 광범위한 실험을 통해 본 방법의 우수성과 유효성을 입증한다. 코드를 수락하면 사용할 수 있습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트-이미지(T2I) 확산 모델은 이미 이미지 생성 및 편집에서 놀랍도록 높은 품질과 다양성을 입증했다[23, 43, 45, 47, 63]. T2I 확산 모델의 급속한 발전은 또한 비디오 생성 및 편집을 위해 사전 훈련된 T2I 모델에서 일반적으로 확장되는 텍스트-비디오(T2V) 확산 모델[3, 22, 49, 53, 2]의 출현에 박차를 가했다. 한편, ControlNet[66], T2I-어댑터[39] 및 GLIGEN[33]과 같은 T2I 모델에서 제어 가능한 기술의 출현은 사용자가 스케치 맵, 깊이 맵 또는 경계 상자 등과 같은 조건을 통해 생성된 이미지의 공간 레이아웃을 지정할 수 있게 하여 T2I 모델의 공간 제어성을 크게 향상시켰다. 이러한 공간 제어 가능한 기술은 또한 비디오 생성을 위한 공간-시간 제어로 성공적으로 확장되었다. 이 영역의 대표적인 작품 중 하나는 VideoComposer[56]이며, 이는 스케치 또는 모션 벡터 맵의 시퀀스가 주어진 비디오를 합성할 수 있다.\n' +
      '\n' +
      '텍스트-비디오 합성의 성공에도 불구하고, 현재의 T2V 방법들은 종종 카메라 움직임 및 객체 움직임에 대한 사용자 정의 및 얽힘없는 제어에 대한 지원이 부족하며, 이는 비디오 모션 제어의 유연성을 제한한다. 비디오에서, 객체와 카메라 둘 다 각자의 동작을 나타낸다. 객체 움직임은 피사체의 활동에서 비롯되는 반면 카메라 움직임은 프레임 간의 전환에 영향을 미친다. 전체 비디오 모션은 카메라 모션과 객체 모션이 모두 결정될 때만 잘 정의된다. 예를 들어, 객체가 프레임 내에서 오른쪽으로 이동하는 비디오 클립을 생성하는 것과 같은 객체 모션에만 초점을 맞추는 것은 다수의 시나리오로 이어질 수 있다. 카메라는 객체 자체가 우측으로 이동하는 동안 정지 상태를 유지할 수 있거나, 또는 카메라가 좌측으로 이동하는 동안 객체가 정지 상태를 유지할 수 있거나, 또는 객체와 카메라 모두가 상이한 속도로 이동하고 있을 수 있다. 전체적인 비디오 모션에서 이러한 모호성이 발생할 수 있다. 따라서, 카메라 움직임 및 객체 움직임의 디커플링 및 독립적인 제어는 보다 많은 유연성을 제공할 뿐만 아니라 비디오 생성 과정에서 모호성을 감소시킨다. 그러나, 이러한 측면은 지금까지 제한된 연구 관심을 받았다.\n' +
      '\n' +
      'T2V 생성에서 카메라 움직임과 물체 움직임을 제어하기 위해, 간단한 접근법은 비디오 합성기와 같은 작업과 유사한 감독된 훈련 경로를 따르는 것이다[56]. 이러한 방식을 따르는 것은 카메라 및 객체 모션 정보 둘 다로 주석이 달린 비디오를 사용하여 조건부 T2V 모델을 트레이닝하는 것을 포함한다. 그러나, 이것은 다음의 과제들을 가져올 것이다: (1) 많은 비디오 클립들에서, 객체 모션은 그들의 고유한 상관관계로 인해 종종 카메라 모션들과 결합된다. 예를 들어, 전경 객체가 일부 방향으로 이동할 때, 카메라는 전형적으로 메인 피사체를 프레임의 중심에 유지하기 위한 선호도에 기인하여 동일한 방향으로 팬한다. 이러한 결합된 카메라 및 객체 모션 데이터에 대한 트레이닝은 모델이 카메라 움직임과 객체 모션을 구별하는 것을 어렵게 한다. (2) 프레임 단위의 객체 추적 및 카메라 포즈 추정을 수행하는 힘들고 비용이 많이 드는 특성으로 인해, 완전한 카메라 움직임 및 객체 움직임 주석을 갖는 대규모 비디오 데이터 세트를 획득하는 것은 어렵다. 추가적으로, 대규모 데이터세트 상에서 비디오 모델을 트레이닝하는 것은 계산적으로 비용이 많이 들 수 있다.\n' +
      '\n' +
      '본 연구에서는 사용자가 카메라 움직임과 하나 이상의 객체의 움직임을 독립적으로 지정할 수 있도록 하는 텍스트-비디오 프레임워크인 Direct-a-Video를 도입하여 마치 비디오를 연출하는 것처럼 원하는 움직임 패턴을 만들 수 있도록 한다(그림 1). 이를 위해 두 가지 직교 제어 메커니즘을 사용하여 카메라 움직임과 객체 움직임 제어를 분리하는 전략을 제안한다. 본질적으로, 우리는 자체 감독적이고 가벼운 훈련 접근법을 통해 카메라 움직임을 배운다. 반대로, 추론하는 동안, 우리는 객체 움직임을 제어하기 위해 훈련 없는 방법을 채택한다. 우리의 전략은 모션 주석 및 비디오 접지 데이터 세트의 집중적인 수집의 필요성을 능숙하게 피한다.\n' +
      '\n' +
      '카메라 이동 제어에서는 프레임 전환을 학습하기 위한 추가 모듈을 학습한다. 구체적으로, 우리는 텍스트 언어를 해석하는 데 있어 공간적인 교차 주의와 유사하게 기능하는 카메라 모듈이라고 알려진 새로운 시간적 교차 주의 레이어를 소개한다. 이 카메라 모듈은 "카메라 언어", 특히 카메라 패닝 및 줌 파라미터를 해석하여 카메라 이동에 대한 정밀한 제어를 가능하게 한다. 그러나, 카메라 움직임 주석으로 데이터 세트를 획득하는 것은 도전을 제기할 수 있다. 이 힘든 작업을 극복하기 위해 카메라 움직임 증강에 의존하는 자체 감독 훈련 전략을 사용한다. 이 접근법은 명시적인 모션 주석의 필요성을 제거한다. 중요하게도, 우리는 원래 모델 가중치를 보존하면서 이러한 새로운 레이어를 훈련하여 T2V 모델 내에 포함된 광범위한 사전 지식이 손상되지 않은 상태로 유지되도록 한다. 모델은 초기에 소규모 비디오 데이터 세트에 대해 훈련되었지만, 다양한 오픈 도메인 시나리오에서 카메라 움직임을 정량적으로 제어할 수 있는 능력을 획득한다.\n' +
      '\n' +
      '객체 모션 제어에서, 비디오에 대한 주석이 잘 부착된 접지 데이터 세트의 가용성으로부터 상당한 도전이 발생하며, 그러한 데이터 세트를 큐레이팅하는 것은 종종 노동 집약적인 프로세스이다. 이러한 문제를 우회하기 위해, 우리는 T2I 모델에서 이전의 주의 기반 이미지 레이아웃 제어 기술로부터 영감을 얻는다[31]. T2V 모델의 내부 사전을 공간 교차 어텐션 변조(spatial cross-attention modulation)를 통해 활용함으로써, 객체 움직임에 대한 접지 데이터 셋과 주석을 수집할 필요가 없다. 사용자 상호작용을 용이하게 하기 위해, 사용자는 중간 경로뿐만 아니라 첫 번째 프레임과 마지막 프레임에서 바운딩 박스를 그리면서 객체의 시공간적 궤적을 지정할 수 있게 한다. 이러한 상호작용은 이전의 픽셀-와이즈 제어 방법들에 비해 더 간단하고 사용자 친화적이다[56].\n' +
      '\n' +
      '우리의 접근 방식이 카메라 움직임과 객체 움직임을 독립적으로 제어함으로써 두 가지를 효과적으로 분리함으로써 사용자에게 비디오 생성에서 이러한 측면을 개별적으로 또는 동시에 조작할 수 있는 유연성을 제공한다.\n' +
      '\n' +
      '요약하면, 우리의 기여는 다음과 같다:\n' +
      '\n' +
      '* 카메라 움직임과 객체 움직임을 분리하여 사용자가 두 측면을 독립적으로 또는 공동으로 제어할 수 있도록 하는 제어 가능한 비디오 생성을 위한 통일된 프레임워크를 제안한다.\n' +
      '\n' +
      '* 카메라 이동의 경우 카메라 이동 컨디셔닝 전용 새로운 시간적 교차 주의 모듈을 소개합니다. 이 카메라 모듈은 자체 감독을 통해 훈련되어 사용자가 카메라의 수평 및 수직 패닝 속도와 줌 비율을 정량적으로 지정할 수 있다.\n' +
      '* 객체 모션의 경우, 우리의 접근법은 훈련 없는 공간 교차 주의 변조를 사용하여 사용자가 바운딩 박스를 드로잉함으로써 하나 이상의 객체에 대한 모션 궤적을 쉽게 정의할 수 있게 한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Text-to-Video Synthesis\n' +
      '\n' +
      '텍스트-투-이미지(T2I) 모델의 성공은 텍스트-투-비디오(T2V) 생성에 대한 잠재력을 드러냈다. T2V 모델은 종종 시간 계층을 통합하여 T2I 모델에서 진화한다. 초기 T2V 모델[25, 22, 49]은 픽셀 공간에서 확산 프로세스를 수행하며, 이는 고해상도 또는 더 긴 비디오를 생성하기 위해 다수의 캐스케이드 모델이 필요하여 높은 계산 복잡도를 초래한다. 최근의 T2V 모델들은 잠재 확산으로부터 영감을 끌어내고[45], 더 낮은 차원과 더 조밀한 잠재 공간[3, 69, 13, 53, 18]에서 작동한다. 가장 최근의 Stable VideoD 확산[2]은 큐레이션된 학습 데이터를 활용하며, 고품질의 동영상을 생성할 수 있다.\n' +
      '\n' +
      '한편, T2I 편집 기법[19, 38, 15, 46, 32]의 개발은 제로/퓨 샷 비디오 편집 작업을 용이하게 했다. 이러한 기술들은 주어진 소스 비디오를 가중치 미세 조정[59], 밀집 맵 컨디셔닝[64, 13, 67, 16], 희소 포인트 컨디셔닝[17, 51], 주의 특징 편집[42, 34, 4, 55], 및 표준 공간 프로세싱[41, 5, 30]과 같은 접근법들을 통해 타겟 비디오로 변환한다. 일부 작품들은 특히 소스 스켈레톤 시퀀스와 참조 초상화[6, 27, 62, 14, 54]를 사용하여 인간 댄스 비디오를 합성하는 데 중점을 두고 있으며, 이는 인상적인 결과를 낳았다.\n' +
      '\n' +
      '제어 가능한 움직임을 갖는### 비디오 생성\n' +
      '\n' +
      '동영상에서 모션이 중요한 요소이기 때문에 모션 제어를 통한 동영상 생성에 대한 연구가 주목을 받고 있다. 입력 매체의 종류에 따라 이미지 대 비디오, 비디오 대 비디오, 텍스트 대 비디오의 세 그룹으로 분류할 수 있다.\n' +
      '\n' +
      '이미지-투-비디오.일부 방법은 정적 이미지를 비디오로 변환하는 데 중점을 두고 있으며, 모션 제어를 위한 인기 있는 접근법은 키 포인트 드래그[65, 9, 12]를 통한 것이다. 이러한 상호 작용 방식은 직관적이고 사용자 친화적이지만, 핵심 사항의 지역적, 희소성으로 인해 한계가 있다. 결과적으로, 큰 입도에서 움직임을 제어하는 용량은 크게 제한된다.\n' +
      '\n' +
      '비디오-투-비디오.이들 작업은 주로 모션 트랜스퍼에 초점을 맞추는데, 이는 소스 비디오들로부터 특정 주제 동작을 학습하고 유사한 모션 패턴들을 갖는 참조 비디오들의 세트 상에서 모델을 미세 조정하거나[60, 68, 29, 58] 공간 특징들(예를 들어, 스케치, 깊이 맵들)을 차용하거나[56, 10] 또는 희소 특징들(예를 들어, DIFT 포인트 임베딩)을 소스 비디오들로부터 [17]을 포함하는 다양한 기법들을 사용하여 타겟 비디오들에 적용하는 것을 포함한다. 이러한 방법은 소스 비디오의 동작 전원에 크게 의존하지만, 항상 실제로 사용 가능한 것은 아니다.\n' +
      '\n' +
      '텍스트-투-비디오.소스 비디오를 사용할 수 없는 경우, 제어가능한 모션으로 텍스트로부터 비디오를 생성하는 것은 의미있지만 상대적으로 덜 탐색된 작업이다. 우리의 작업은 이 범주에 중점을 둡니다. 이 범주의 기존 접근법에는 특정 카메라 움직임을 가능하게 하기 위해 Ad-hoc 모션 LoRA 모듈[26]을 활용하는 AnimateDiff[18]가 포함된다. 그러나 카메라 움직임의 정량적 제어가 부족하고 객체 움직임 제어도 지원하지 않는다. 동시 작업인 피카부[28]는 주의 변조를 통해 객체의 궤적을 제어할 수 있지만 카메라 이동 제어는 지원하지 않는다. VideoComposer[56]는 픽셀-와이즈 모션 벡터들에 대한 컨디셔닝에 의해 전역 모션 안내를 제공한다. 그러나 VideoComposer에 의해 제공되는 조밀한 제어 방식은 직관적이지 않고 카메라 및 객체 모션을 명시적으로 디엔탱글링하지 못하여 사용자 상호 작용이 번거롭다. 또 다른 동시 작업인 MotionCtrl[57]은 희소 점 구동 객체 제어 및 궤적 구동 카메라 제어를 허용한다. 그러나, 훈련 과정은 노동 집약적이어서 이동 물체에 대한 마킹된 포인트들의 추적을 필요로 한다. 또한, 훈련 데이터로부터의 제약으로 인해 다양한 움직임 방향을 갖는 여러 개의 다른 객체를 제어하는데 어려움을 겪는다. 대조적으로, 우리의 접근법은 모션 주석을 필요로 하지 않으며 다양한 모션 방향 및 카메라 움직임을 갖는 다수의 객체에 대한 제어를 달성할 수 있다. 이것은 비디오 합성의 유연성과 유용성을 크게 향상시킨다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      'Task formulation.본 논문에서는 사용자 지향 카메라 움직임 및/또는 객체 움직임을 갖는 텍스트-비디오 생성에 초점을 둔다. 우선, 사용자는 선택적으로 하나 이상의 객체 단어 \\(O_{1},O_{2},...O_{N}\\)를 포함할 수 있는 텍스트 프롬프트를 제공해야 한다. 카메라의 움직임을 결정하기 위해 사용자는 x-pan ratio \\(c_{x}\\), y-pan ratio \\(c_{y}\\) 및 zoom ratio \\(c_{z}\\)을 지정할 수 있다. 본 논문에서 제안하는 시스템은 첫 번째 객체(\\(n)번째 객체(O_{n}\\)의 움직임을 결정하기 위해 시작 박스(\\mathbf{B}_{n}^{1}\\), 끝 박스(\\mathbf{B}_{n}^{L}\\)(L\\)와 중간 트랙(\\zeta_{n}\\)을 연결하며 보간을 통해 객체의 공간적-시간적 여정을 정의하여 박스([\\mathbff{B}_{n}^{1},...,\\mathbf{B}_{n}^{L}]\\)의 시퀀스를 생성한다.\n' +
      '\n' +
      '결과적으로, 본 모델은 규정된 카메라 움직임 및/또는 객체 움직임을 준수하는 비디오를 합성하여 사용자 정의되고 역동적인 시각적 내러티브를 생성한다.\n' +
      '\n' +
      '전체 파이프라인.우리의 전체 파이프라인은 그림 2에 예시되어 있다. 카메라 움직임은 훈련 단계에서 학습되고 객체 움직임은 추론 단계에서 구현된다. 훈련하는 동안 고정식 카메라로 촬영된 비디오 샘플을 사용하여 \\([c_{x},c_{y},c_{z}]\\)에 따라 카메라 움직임을 시뮬레이션하기 위해 증강된다. 증강 비디오는 U-Net에 대한 입력으로 후속적으로 사용된다. 추가로, 카메라 파라미터들은 또한 인코딩되고 카메라 움직임을 조정하기 위해 새롭게 도입된 훈련가능한 시간적 교차-어텐션 층에 주입된다(섹션 3.2에서 상세). 추론하는 동안, 트레이닝된 카메라 임베더 및 모듈로, 사용자는 그 움직임을 제어하기 위해 카메라 파라미터들을 지정할 수 있다. 동시에, 객체 움직임 제어를 훈련 없이 통합한다: 사용자의 프롬프트와 해당 상자에서 객체 단어들이 주어지면, 객체 공간-시간 크기 및 위치를 재지향하기 위해 프레임-방향 및 객체-방향 공간 교차 주의 맵을 변조한다(섹션 3.3에서 상세). 추론 단계에서 변조는 추가적인 최적화를 수반하지 않기 때문에 증분 시간 및 메모리 비용은 무시할 수 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '카메라의 움직임 제어\n' +
      '\n' +
      '카메라 움직임의 제어 신호 역할을 하기 위해 3중항\\(\\mathbf{c}_{\\mathrm{cam}}=[c_{x},c_{y},c_{z}]\\)으로 매개변수화된 수평 팬, 수직 팬, 줌의 세 가지 카메라 움직임을 선택한다. 이는 정량적 제어가 가능할 뿐만 아니라 사용자 친화적이다: 사용자는 텍스트 프롬프트를 타이핑하는 것만큼 간단한 트리플렛을 지정할 수 있다.\n' +
      '\n' +
      '데이터 구축 및 증강.기존 비디오에서 카메라 움직임 정보를 추출하는 것은 객체 움직임을 식별하고 필터링해야 하기 때문에 계산 비용이 많이 들 수 있다. 따라서 본 논문에서는 \\(\\mathbf{c}_{\\mathrm{cam}\\)에 의해 구동되는 카메라 증강을 이용하여 집중적인 움직임 주석의 필요성을 우회하는 자기 지도 학습 방법을 제안한다.\n' +
      '\n' +
      '우리는 먼저 카메라 움직임 파라미터를 공식적으로 정의합니다. \\ (c_{x}\\)는 x-pan 비를 나타내며, 프레임 폭에 대한 첫 번째 프레임에서 마지막 프레임까지의 프레임 중심의 총 x-shift, 패닝 우향에 대한 \\(c_{x}>0\\)(예를 들어, 반-폭 우향에 대한 \\(c_{x}=0.5\\)으로 정의된다. 마찬가지로 \\(c_{y}\\)은 프레임 높이에 대한 프레임 중심의 전체 y-시프트를 나타내는 y-pan 비율이며, 아래로 패닝하기 위한 \\(c_{y}>0\\)이다. \\(c_{y}>0\\) (c_{z}\\)는 줌-인을 위한 첫 번째 프레임에 대한 마지막 프레임의 스케일링 비율, \\(c_{z}>1\\)로 정의되는 줌 비율을 나타낸다. 일반적인 카메라 이동 범위를 커버하기에 충분한 \\(c_{x}\\), \\(c_{y}\\)을 \\([-1,1]\\), \\(c_{z}\\)을 \\([0.5,2]\\)으로 설정하였다.\n' +
      '\n' +
      '실제로 주어진 \\(\\mathbf{c}_{\\mathrm{cam}\\)에 대해 정지 카메라로 촬영된 비디오에서 크롭 윈도우에 시프트와 스케일링을 적용하여 카메라의 움직임을 시뮬레이션한다. 이 데이터 증강은 MovieShot[44]과 같이 쉽게 사용할 수 있는 데이터 세트를 이용한다. 을 포함하는, 이 프로세스의 추가 세부사항\n' +
      '\n' +
      '그림 2: Direct-a-Video의 전체 파이프라인. 카메라 움직임은 훈련 단계에서 학습되고 객체 움직임은 추론 단계에서 구현된다. **왼쪽**: 훈련 중에, 우리는 패닝 및 줌잉 파라미터를 사용하여 카메라 움직임을 시뮬레이션하기 위해 비디오 샘플에 증강을 적용한다. 이러한 파라미터들은 카메라 이동 컨디셔닝으로서 새롭게 도입된 시간적 교차-어텐션 층들에 임베딩되고 주입되어, 카메라 이동 어노테이션의 필요성을 제거한다. **Right**: 추론 동안, 카메라 움직임과 함께, 사용자는 객체 단어들 및 연관된 박스 궤적들을 포함하는 텍스트 프롬프트를 입력한다. 우리는 공간 교차 주의 변조를 사용하여 추가 최적화 없이 객체의 공간-시간적 배치를 안내한다. 우리의 접근 방식은 카메라 움직임과 물체 움직임을 독립적으로 제어함으로써 두 가지를 효과적으로 분리하여 개별 및 관절 제어를 모두 가능하게 한다는 점에 유의한다.\n' +
      '\n' +
      'pseudo code와 \\(\\mathbf{c}_{\\mathrm{cam}\\)의 sampling scheme이 부록에 제시되어 있다.\n' +
      '\n' +
      '카메라 임베딩.\\(\\mathbf{c}_{\\mathrm{cam}}\\)을 카메라 임베딩으로 인코딩하기 위해 퓨리에 임베딩기[37]와 두 개의 MLP를 포함하는 카메라 임베더를 사용한다. 하나의 MLP는 패닝 이동\\(c_{x}\\), \\(c_{y}\\)을 공동으로 인코딩하고, 다른 하나는 줌 이동\\(c_{z}\\)을 인코딩한다. 실험 결과, 팬닝과 주밍을 별도로 인코딩하면 두 가지 유형의 카메라 움직임을 효과적으로 구별할 수 있으며, 4.5절에서 이 모델을 검증한다. 임베딩 과정은 2.0의 시퀀스 길이를 갖는 \\(\\mathbf{e}_{xy}=\\mathrm{MLP}_{xy}(\\mathcal{F}([c_{x},c_{y}])\\),\\(\\mathbf{e}=\\mathrm{MLP}_{z}(\\mathcal{F}(c_{z}))\\),\\(\\mathbf{e}_{e}_{z}}=[\\mathbf{e}_{xy}) 및 \\(\\mathbf{e}_{z}]\\)으로 공식화될 수 있다.\n' +
      '\n' +
      '카메라 모듈.이제 카메라 임베딩을 어디에 주입할지 생각해보겠습니다. 이전 연구에서는 시간적 전환을 관리하는 데 시간적 계층의 역할을 강조했다[18, 68]. 따라서, 우리는 시간 계층을 통해 카메라 제어 신호를 주입한다. 본 논문에서는 T2V 모델의 각 U-Net 블록 내에 존재하는 시간적 자기 주의 계층 뒤에 추가된 카메라 모듈로 명명된 카메라 정보를 해석하기 위한 새로운 학습 가능한 시간적 자기 주의 계층을 그림 2와 같이 소개한다. 텍스트적 자기 주의와 유사하게 본 모듈에서는 시각적 프레임 특징(\\(\\mathbf{F}\\)으로부터 질의를 매핑하고, 패닝 임베딩(\\(\\mathbf{e}_{xy}\\)과 줌잉 임베딩(\\(\\mathbf{e}_{z}\\)으로부터 키와 값을 별도로 매핑한다. 시간적 교차 주의를 통해 카메라 움직임이 시각적 특징에 주입되고, 이후 게이트 잔차로 다시 추가된다. 우리는 이 과정을 다음과 같이 공식화한다.\n' +
      '\n' +
      '\\mathbf{F}=\\mathbf{F}+\\tanh(\\alpha)\\cdot\\text{TempCrossAttn}(\\mathbf{F}, \\mathbf{e}_{\\mathrm{cam}}) \\tag{1}\\text{\n' +
      '\n' +
      '\\mathbf{F},\\mathbf{e}_{\\mathrm{cam}})=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}[\\mathbf{K}_{xy},\\mathbf{K}_{z}]^{T}{\\sqrt{d}\\right}[\\mathbf{V}_{xy},\\mathbf{V}_{z}],\\tag{2}\\tag{2}}\\mathbf{Q}[\\mathbf{K}_{z}}}\\tqrt{d}\\right}[\\mathbf{V}_{xy},\\mathbf{V}_{z}}}\\tqrt{d}\\right}\\mathbf{Q}[\\mathbf{Q}[\\mathbf{K}_{z}}\\tqrt}\\right}\\tqrt}\\tqrt}\\tqrt}\\tqrt}\\tqrt}\n' +
      '\n' +
      '여기서 \\([,]\\)은 시퀀스 차원에서의 연접을 나타내고, \\(\\mathbf{K}_{xy}\\), \\(\\mathbf{K}_{z}\\)은 키 벡터이고, \\(\\mathbf{V}_{xy}\\), \\(\\mathbf{V}_{z}\\)은 각각 \\(\\mathbf{e}_{xy}\\), \\(\\mathbf{e}_{z}\\)으로부터 매핑되는 값 벡터이고, \\(d\\)은 \\(\\mathbf{Q}\\)의 특징 차원이고, \\(\\alpha\\)은 0으로 초기화된 학습 가능한 스칼라로서, 카메라 모션이 사전 훈련된 상태로부터 점진적으로 학습되도록 보장한다.\n' +
      '\n' +
      '모델의 사전 지식을 유지하면서 카메라 움직임을 학습하기 위해 원래 가중치를 동결하고 새로 추가된 카메라 임베더와 카메라 모듈만 학습한다. 이것은 카메라 움직임\\(\\mathbf{c}_{\\mathrm{cam}\\)과 비디오 캡션\\(c_{\\mathrm{txt}\\)을 조건으로 한다. 상기 트레이닝은 확산 잡음-예측 손실을 채용한다:\n' +
      '\n' +
      '\\mathcal{L}=\\mathbb{E}_{\\mathbf{x}_{0},\\mathbf{c}_{\\mathrm{cam},c_{\\mathrm{txt},t,\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,I}\\left[\\left\\|\\boldsymbol{\\epsilon}-\\boldsymbol{\\epsilon}_{\\theta}\\left(\\mathbf{x}_{t},\\mathbf{c}_{\\mathrm{cam},c_{\\mathrm{txt},t\\right}\\right\\|_{2}^{2}\\right], \\tag{3}\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\hext\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}_{0}\\)는 증강 입력 샘플이고, \\(t\\)은 확산 타임스탬프를 나타내고, \\(\\mathbff{x}_{t}=\\alpha_{t}\\mathbff{x}_{0}+\\sigma_{t}\\boldsymbol{\\epsilon}\\)은 \\(t\\), \\(\\alpha_{t}\\) 및 \\(\\sigma_{t}\\)은 시간 종속 DDPM 하이퍼 파라미터[23], \\(\\boldsymbol{\\epsilon}_{\\theta}\\)에 의해 파라미터화된 모델이다.\n' +
      '\n' +
      '물체 움직임 제어\n' +
      '\n' +
      '우리는 사용성과 접근성의 균형이 유리하기 때문에 객체 움직임에 대한 제어 신호로 바운딩 박스를 선택한다. 상자는 도면 기술이 필요하지 않기 때문에 조밀한 조건(예: 스케치 맵)보다 더 효율적이며, 희박한 조건(예: 키 포인트)이 부족한 특징인 객체의 크기에 대한 사양을 제공한다.\n' +
      '\n' +
      '이론적으로는 GLIGEN[33]과 유사한 box-conditioned T2V 모델을 훈련하는 것이 가능하다. 그러나 이미지와 달리 비디오에 대한 주석이 잘 지정된 접지 데이터 세트의 가용성에는 극명한 대조가 있으며, 이는 훨씬 적고 제한된 범주를 포함한다. 이 문제를 우회하기 위해 확산 프로세스를 원하는 결과로 스티어링하여 미리 훈련된 T2V 모델의 고유한 이전 정보를 완전히 활용하기로 결정했다. 이전의 T2I 작업은 교차 주의 지도[1, 31, 36, 8]를 편집하여 객체의 공간 위치를 제어하는 능력을 입증했다. 마찬가지로, 객체 모션 크래프팅을 위해 T2V 모델에서 공간 교차 주의 변조를 사용한다.\n' +
      '\n' +
      '교차 주의 계층에서 질의 특징 \\(\\mathbf{Q}\\)은 시각적 토큰으로부터 유도되고, 키 \\(\\mathbf{K}\\) 및 값 특징 \\(\\mathbf{V}\\)은 텍스트 토큰으로부터 매핑된다. \\(\\mathbf{Q}\\)은 텍스트 토큰으로부터 유도된다. (\\mathbf{Q}\\mathbf{K}^{\\top}\\)는 어텐션 맵을 구성하는데, 인덱스 \\([i,j]\\)에서의 값은 j번째 텍스트 토큰 특징에 대한 i번째 이미지 토큰 특징의 응답을 반영한다. 어텐션 맵\\(\\mathbf{Q}\\mathbf{K}^{\\top}\\)을 다음과 같이 변조한다.\n' +
      '\n' +
      '\\text{CrossAttnModulate}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}+\\lambda\\mathbf{S}{\\sqrt{d}\\right)\\mathbf{V},\\tag{4}\\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}+\\lambda\\mathbf{S}{\\sqrt{d}\\right)\\mathbf{V},\\tag{4}\\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}+\\lambda\\mathbf{S}{\\sqrt{d}\\right)\\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}+\\lambda\\math\n' +
      '\n' +
      '여기서 \\(\\lambda\\)은 변조 강도를 나타내고, \\(d\\)은 \\(\\mathbf{Q}\\)의 특징 치수이며, \\(\\mathbf{S}\\)은 \\(\\mathbf{Q}\\mathbf{K}^{\\top}\\)과 동일한 크기의 변조 항이다. 그것은 증폭과 억제의 두 가지 유형의 변조를 포함한다.\n' +
      '\n' +
      '어텐션 증폭은 바운딩 박스(\\mathbf{B}_{n}^{k}\\)로 둘러싸인 \\(k\\)번째 프레임에서 \\(n\\)번째 물체를 고려할 때, 이 영역에서 물체의 존재 확률을 높이는 것을 목표로 하기 때문에 영역 \\(\\mathbf{B}_{n}^{k}\\) 내에서 해당 물체 단어(인덱싱된 프롬프트에서 \\(\\mathbf{T}_{n}\\)에 대한 어텐션 값을 증폭할 수 있다. 만약 배경 단어가 존재한다면, 우리는 그것을 같은 방식으로 취급하고, 그 영역은 모든 물체의 영역의 결합의 보완에 해당한다. DenseDiff [31]의 결론에 따르면, 이 증폭의 스케일은 \\(\\mathbf{B}_{n}^{k}\\)의 면적과 반비례해야 하며, 즉 상자 면적이 작을수록 주의력이 더 크게 증가한다. 어텐션 증폭은 물체의 자연스러운 윤곽선과 일치하지 않는 박스 형태의 영역에서 수행되기 때문에, 초기 단계(타임스텝의 경우 \\(t\\geq\\tau\\), \\(\\tau\\)은 증폭 컷오프 타임스텝)으로 한정한다. \\(t<\\tau\\)의 경우 확산 과정을 완화하여 모양과 모양 세부 사항을 점진적으로 다듬을 수 있다.\n' +
      '\n' +
      '주의력 억제.특정 영역에 대한 무관한 단어들의 영향을 완화하고, 객체 특징들이 다른 영역으로 의도하지 않게 분산되는 것을 방지하기 위해, 우리는 매칭되지 않는 쿼리-키 토큰 쌍(시작 토큰과 종료 토큰을 제외하고, 비디오 품질이 손상될 수 있음)에 대한 주의력 값을 억제한다. 주의 증폭과 달리 주의 억제는 전체 샘플링 프로세스에 걸쳐 적용되어 상호 의미 간섭을 방지하며, 이는 한 객체의 의미론이 부주의하게 다른 객체로 출혈될 수 있는 다중 객체 생성 시나리오에서 잠재적인 문제이다. 우리는 절제 연구(섹션 4.5)에서 결과와 분석을 제시할 것이다.\n' +
      '\n' +
      '형식적으로 \\(k\\)번째 프레임 \\(\\mathbf{S}_{n}^{k}[i,j]\\)에서 \\(n\\)번째 물체에 대한 주의 변조 항은 다음과 같이 공식화된다.\n' +
      '\n' +
      'bf{S}_{n}^{k}[i,j}=\\begin{cases}1-\\frac{|\\mathbf{B}_{n}^{k}|},&\\text{if}i\\in\\mathbf{B}_{n}^{k}\\text{ and }j\\geq\\tau\\0,&\\text{if}i\\in\\mathbf{B}_{n}\\text{ and }t<\\tau\\\\infty,&\\text{otherwise\\text{cases}\\tag{5}\\text{\n' +
      '\n' +
      '여기서 \\(|\\mathbf{X}|\\)는 행렬 \\(\\mathbf{X}\\)에서 원소의 수를 나타낸다. 이러한 변조를 매 프레임마다 수행하여 완전한 시공간 객체 궤적을 결정할 수 있도록 한다. 이 변조가 각 프레임에서 독립적으로 수행되지만, 우리는 시간 연속성을 유지하는 사전 훈련된 시간 계층 덕분에 생성된 비디오가 연속성을 유지한다는 것을 관찰한다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '구현 세부사항.우리는 카메라 움직임 학습을 용이하게 하기 위해 제안된 훈련 가능한 카메라 임베더와 모듈을 통합하여 사전 훈련된 Zeroscope T2V 모델[53]을 기본 모델로 채택한다. 교육 내용은 부록을 참고하시기 바랍니다. 추론하는 동안, 우리는 DDIM 샘플러 [50]을 사용하여 \\(T=50\\) 샘플링 단계와 9 [24]의 분류기 없는 유도 척도를 사용한다. 기본 주의력 제어 가중치\\(\\lambda\\)와 cut-off timestep\\(\\tau\\)은 각각 25와 0.95T\\이다. 출력 비디오 크기는 320x512x24입니다.\n' +
      '\n' +
      '데이터세트.카메라 움직임 트레이닝을 위해, 우리는 MovieShot[44]의 서브세트를 사용한다. 이 서브세트는 22k개의 정적-샷 영화 트레일러를 포함하고, 즉, 카메라는 고정되지만 피사체는 이동하기에 유연하며, 이는 트레이닝 샘플들이 원래의 카메라 이동이 없다는 것을 보장한다. 훈련 샘플의 제한된 수와 범주에도 불구하고, 우리의 훈련된 카메라 모듈은 여전히 일반적인 장면에 적응할 수 있다. 카메라 제어 평가를 위해 [11]에서 제공하는 프롬프트 세트에서 200개의 장면 프롬프트를 수집했다. 객체 제어 평가를 위해 우리는 주로 자연 동물과 객체에 초점을 맞춘 프롬프트와 함께 다양한 상자 크기, 위치 및 궤적으로 구성된 200개의 상자 프롬프트 쌍의 벤치마크를 선별했다.\n' +
      '\n' +
      '계량학이야 (1) 비디오 생성 품질을 평가하기 위해 FID-vid[21]와 FVD[52]를 이용하였으며, 기준 세트는 카메라 움직임 태스크의 경우 MSRVTT[61]에서 2048개의 비디오와 객체 움직임 태스크의 경우 AnimalKingdom[40]에서 800개의 비디오이다. (2) 객체-박스 정렬을 측정하기 위해 비디오 샘플당 8 프레임을 균일하게 추출하고 박스 영역 내에서만 CLIP 이미지-텍스트 유사성(CLIP-sim) [20]을 계산하며, 템플릿 프롬프트 "[obj]의 사진"으로, 여기서 [obj]는 객체 문구에 해당한다. (3) 카메라 및 물체 움직임 정렬을 평가하기 위해 유동 오차를 도입한다. 구체적으로, 생성된 비디오로부터 플로우 맵을 추출하기 위해 최첨단 광학 흐름 모델인 VideoFlow[48]를 활용한다. 그 후, 이들은 (주어진 카메라 이동 및 물체 박스들로부터 도출되는) 지상 진실의 흐름과 비교된다. 흐름 오차는 카메라 움직임 평가를 위한 전체 프레임과 물체 움직임 평가만을 위한 박스 영역에서 계산된다.\n' +
      '\n' +
      '기준.최근 확산 기반 T2V 모델과 AnimateDiff[18] (카메라 이동용), Peekaboo[28] (객체 이동용), VideoComposer[56] (관절 제어용) 등 카메라 이동 또는 객체 움직임 제어성을 비교한다.\n' +
      '\n' +
      '카메라의 움직임 제어\n' +
      '\n' +
      '카메라 움직임 제어를 위해 AnimateDiff와 VideoComposer와의 비교를 수행한다. 애니메이트디프의 경우, 각각 특정 유형의 카메라 움직임에 전념하지만 정밀한 제어를 위한 지원이 부족한 공식 사전 훈련된 LoRA 모션 모듈을 사용한다. 비디오 합성기의 경우, 본 논문에서 증명된 바와 같이 카메라 움직임 파라미터를 기반으로 모션 벡터 맵을 수작업으로 제작한다.\n' +
      '\n' +
      '정성적 비교.우리는 그림 3에서 기준선과 나란히 시각적 비교를 제시한다. 볼 수 있듯이 모든 방법은 단일 유형의 카메라 움직임으로 비디오를 생성할 수 있지만 AnimateDiff는 로드된 모션 모듈이 한 유형의 카메라 움직임에만 전용이기 때문에 하이브리드 카메라 움직임(예: pan+zoom)을 지원하지 않는 반면, 우리의 방법과 VideoComposercan은 추가 모듈을 재로드할 필요 없이 모션 입력을 변경하여 카메라 움직임을 결합하거나 전환한다. 정확한 제어 측면에서, 본 논문에서 제안하는 방법과 VideoComposer는 카메라 속도를 정량적으로 제어할 수 있지만, VideoComposer는 공간 벡터 맵을 제공해야 하며, 이는 수작업으로 제작되거나 기존 동영상에서 추출되어야 하므로 사용 편의성이 떨어질 수 있다. 반면에 우리의 접근법은 사용자가 텍스트 프롬프트를 입력하는 것만큼 쉽게 카메라 속도를 지정할 수 있도록 더 사용자 친화적이다. 또한, 이 방법의 카메라 제어는 전경 물체에 어떠한 움직임 제약도 부과하지 않기 때문에 전경 물체에 영향을 미치지 않는다. 대조적으로, VideoComposer는 전역 움직임 벡터 맵을 사용하며, 이는 종종 배경 움직임과 함께 객체들을 묶는다. 그림 3의 세 번째 열에서 볼 수 있듯이, 우리의 결과에서 얼룩말은 카메라와 독립적인 움직임을 보이는 반면, VideoComposer의 결과(두 번째 열)에서는 얼룩말은 카메라의 움직임에 묶여 있다. 마지막으로, 우리의 결과는 또한 우리의 기본 모델의 우수성을 입증하는 더 높은 시각적 품질을 나타낸다.\n' +
      '\n' +
      '정량적 비교.표 1에서 FVD, FID-vid 및 Flow 오차를 보고한다. AnimateDiff는 정량적 제어가 부족하여 Flow 오차 비교에서 제외됨을 유의한다. 본 연구의 결과는 FVD 및 FID-vid 점수가 가장 우수하여 기준선에 비해 우수한 시각적 품질을 나타내며, 더 낮은 흐름 오차로 입증되는 보다 정밀한 카메라 제어를 보여준다.\n' +
      '\n' +
      '물체 움직임 제어\n' +
      '\n' +
      '객체 움직임 제어를 위해 주로 VideoComposer와 비교하는데, 이는 움직임 벡터 맵이 박스 시퀀스를 통해 객체 움직임을 생성하는데 사용될 수 있기 때문이다. 이를 위해 먼저 객체 박스 시퀀스를 조밀한 흐름 맵으로 변환하고, 이를 입력 포맷과 호환되는 움직임 벡터 맵으로 처리한다. 우리는 또한 동시 작업 피카부[28]와 정성적으로 비교하는데, 그 코드는 작성 시 사용할 수 없기 때문에 공식 웹 사이트의 샘플을 사용한다.\n' +
      '\n' +
      '정성적 비교.그림 4에서 관련 기준선과 시각적 비교를 제시한다. 정적 객체 생성을 위해 VideoComposer는 원하는 위치에 객체를 생성하지 못하며(첫 번째 열의 팬더 예제 참조), 모션 힌트 없이 바닐라 T2V 모델처럼 작동한다. 모든 방법들이 단일 이동 객체를 생성할 수 있는 반면, 다수의 이동 객체 시나리오들에서 도전들이 발생한다. 까꿍은 관련 사례가 없어 이번 비교에서 제외된다. 비디오 합성기는 우리의 방법과 달리 각 객체에 대한 개별 동작을 지정하는 것을 지원하지 않는다(7번째 열의 상어 및 해파리 예제 참조, 예상 위치에 나타나지 않음). 또한, 객체와 동작 사이의 명시적 바인딩이 부족하면 의미적 혼합과 부재라는 두 가지 추가 문제가 발생한다. 의미 혼합은 한 물체의 의미론과 다른 물체의 의미론의 혼합을 의미한다. 이것은 호랑이의 질감이 곰으로 새어 들어가는 9번째 기둥에 예시되어 있다. 의미 부재는 물체가 예상대로 나타나지 않을 때 발생하며, 이는 T2I/T2V 모델 [7]에서 알려진 문제이다. 예를 들어, 11번째 칼럼에서 예상되는 낙타는 미스\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline  & FVD \\(\\downarrow\\) & FID-vid\\(\\downarrow\\) & Flow error\\(\\downarrow\\) \\\\ \\hline AnimateDiff [18] & 1685.40 & 82.57 & - \\\\ VideoComposer [56] & 1230.57 & 82.14 & 0.74 \\\\ Direct-a-Video (ours) & **888.91** & **48.96** & **0.46** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 카메라 움직임 제어 평가를 위한 정량적 비교.\n' +
      '\n' +
      '그림 3: 관련 기준선과의 카메라 이동 제어에 대한 질적 비교. 세 번째 열의 결과는 두 번째 열의 비디오 합성기에 의한 결과와 달리 객체 움직임(노란색 선)이 카메라의 움직임(시안 선)과 독립적일 수 있음을 보여준다.\n' +
      '\n' +
      '지프차로 대체해 이와는 대조적으로, 본 방법은 각 객체에 대한 애드혹 어텐션 변조를 통해 이러한 문제를 효과적으로 해결하여 여러 객체의 움직임을 보다 쉽게 제어할 수 있다.\n' +
      '\n' +
      '정량적 비교.표 2에서 FVD, FID-vid, CLIP-sim, Flow error를 보고하였으며, 통계량을 통해 제안한 방법이 영상 합성기에 비해 생성품질과 객체 움직임 제어 측면에서 우수함을 알 수 있었다.\n' +
      '\n' +
      '### 카메라 움직임과 물체 움직임의 관절 제어\n' +
      '\n' +
      '카메라 움직임과 객체 움직임의 제어를 동시에 지원하기 위한 Direct-a-Video 특징은 그림 5에서 이러한 기능을 입증한다. 동일한 박스 시퀀스를 갖는 경우, 전경-배경 움직임의 다양한 조합으로 비디오를 생성할 수 있다. 예를 들어, 도 5의 (a)는 정적 박스가 정적 객체를 항상 암시하는 것은 아님을 나타내며, 상이한 카메라 움직임을 설정함으로써, 우리의 시스템은 정지(2열), 보행 우측(3열), 또는 보행 좌측(4열)의 얼룩말의 비디오를 생성할 수 있다. 유사하게, 도 5의 (b)는 움직이는 박스가 물체 자체가 움직임 중임을 반드시 나타내는 것은 아니며, 카메라가 움직이는 동안(마지막 열) 그 위치에 정지될 수 있음을 시사한다. 객체에만 초점을 맞춘 기존 연구들은 객체 고유의 움직임과 카메라 움직임에 의해 유도되는 겉보기 움직임을 구분하지 못하는 경우가 많다. 이와는 대조적으로, 본 방법은 사용자가 카메라 움직임과 객체 움직임을 모두 명확하게 지정할 수 있게 하여 전체적인 움직임 패턴을 정의하는 데 향상된 유연성을 제공한다. 더 많은 예들이 도 10 및 프로젝트 페이지에 제공된다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '어텐션 증폭.이는 객체 로컬화에 중요하며, 어텐션 증폭이 없으면 모델이 접지 능력을 상실하게 되며, 즉 그림 6의 첫 번째 행에서 볼 수 있듯이 객체가 상자를 따르지 않으며, 표 3에서 CLIP-심 점수의 감소와 흐름 오차의 증가가 있다.\n' +
      '\n' +
      '주의력 억제.이는 특히 객체가 유사한 특성을 공유할 때 다중 객체 시나리오에서 의도하지 않은 의미적 혼합을 완화하기 위해 도입된다. 이는 초기 단계에서만 주의 증폭이 적용되고 이후 이러한 제약이 완화된다는 설계에서 비롯된다. 억제 없이, 객체 A의 프롬프트 특징은 객체 B의 영역에도 참석할 수 있어, 의미적 중첩으로 이어진다. 그림 6의 두 번째 줄에서 볼 수 있듯이 호랑이의 질감이 곰의 몸에 잘못 나타난다. 세 번째 행은 주의 억제를 가능하게 함으로써 이 문제를 해결할 수 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline  & FVD \\(\\downarrow\\) & FID-vid\\(\\downarrow\\) & CLIP-sim\\(\\uparrow\\) & Flow error \\(\\downarrow\\) \\\\ \\hline VideoComposer[56] & 1620.83 & 90.57 & 27.35 & 1.53 \\\\ Direct-a-Video (ours) & **1300.86** & **43.55** & **27.63** & **1.11** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 객체 동작 제어 평가를 위한 정량적 비교.\n' +
      '\n' +
      '그림 4: 관련 기준선과의 객체 움직임 제어에 대한 질적 비교. 우리의 방법은 하나 이상의 객체를 포함하는 경우를 처리하는 데 탁월합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Attn amp. & Attn sup. & CLIP-sim \\(\\uparrow\\) & Flow error \\(\\downarrow\\) \\\\ \\hline \\(\\times\\) & ✓ & 25.82 & 2.08 \\\\ ✓ & \\(\\times\\) & 27.49 & 1.19 \\\\ ✓ & ✓ & **27.63** & **1.11** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 주의 증폭 및 억제에 대한 정량적 평가.\n' +
      '\n' +
      '카메라 임베딩 설계.3.2절에 자세히 설명된 대로 카메라 제어에서 패닝(\\(c_{x}\\), \\(c_{y}\\)) 및 줌(\\(c_{z}\\)) 움직임을 별도로 인코딩하는 효과를 평가하기 위해 이를 조인트 인코딩 접근법과 대조한다. 여기서, \\([c_{x},c_{y},c_{z}]\\)는 공유 MLP를 이용하여 하나의 카메라 임베딩 벡터로 인코딩된 후, 카메라 모듈에서 공유 키-값 투영 행렬이 뒤따른다. 동일한 설정으로 모델을 학습하고 평가하였으며, 흐름오차가 0.46에서 1.68로 증가함에 따라 카메라 움직임 제어 능력이 감소함을 확인하였다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '이 방법은 물체 및 카메라 모션에 대한 얽힘없는 제어를 제공하는 반면, 때때로 입력에서 충돌이 발생할 수 있다. 예를 들어, 도 7의 상단 행에서, 우리는 카메라를 왼쪽으로 동시에 패닝하면서 정적 박스 내에 정적 객체(집)를 유지하려고 시도한다. 이러한 상충되는 신호를 감안할 때, 우리의 방법은 결국 비현실적인 움직이는 집을 생성한다. 이를 위해서는 신중하고 합리적인 사용자 상호 작용이 필요하다. 충돌하는 상자를 처리할 때 또 다른 문제가 발생합니다. 두 상자가 겹치는 그림 7의 맨 아래 줄에 묘사된 것과 같은 시나리오에서 한 물체(곰)의 의미론은 다른 물체(호랑이)를 방해할 수 있다. 이 문제는 적응적인 자동 분할 영역에 대한 주의를 조절함으로써 완화될 수 있다.\n' +
      '\n' +
      '도 5: 객체 움직임 및 카메라 움직임의 관절 제어. 동일한 박스 시퀀스가 주어졌을 때, 서로 다른 카메라 움직임 파라미터를 설정함으로써, 우리의 접근법은 전경 움직임(노란색 선)과 배경 움직임(시안 선)의 다양한 조합을 나타내는 비디오를 합성할 수 있다. 사용자는 본 논문에서 제안한 방법을 이용하여 객체 움직임과 카메라 움직임 모두를 명확하게 지정하여 잘 정의된 전체 비디오 움직임을 생성할 수 있다.\n' +
      '\n' +
      '도 6: 주의 증폭 및 억제의 효과. 증폭 없이(첫 번째 줄), 물체들은 상자에 달라붙지 않는다; 억제 없이(두 번째 줄), 호랑이의 질감은 실수로 곰의 몸으로 새어 들어간다. 이 문제는 둘 다 사용 가능(세 번째 행)으로 해결됩니다.\n' +
      '\n' +
      '확산 샘플링 프로세스는 초기 상자 영역에 의존하기보다는. 마지막으로, 본 논문의 데이터 증강 방법은 현실감 있는 3D 카메라 움직임을 생성하는 시스템의 능력을 제한한다. 이러한 제약을 극복하기 위해 우리는 향후 작업에서 보다 정교한 증강 알고리즘의 채택을 예상한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 카메라 움직임과 객체 움직임에 대한 독립적이고 사용자 지향적인 제어를 위해 이전에 충족되지 않은 요구를 해결하는 텍스트-비디오 프레임워크인 Direct-a-Video를 제안한다. 본 논문에서 제안하는 방법은 카메라 움직임 제어를 위해 조정된 시간 교차 주의 계층에 대한 자체 감독 훈련 스킴과 객체 움직임 제어 전용 공간 교차 주의에 대한 훈련 없는 변조를 통합하여 이 두 요소를 효과적으로 분리한다. 실험적 평가는 카메라 움직임과 물체 움직임의 분리 및 공동 제어에서 접근법의 능력을 보여준다. 이것은 Direct-a-Video를 맞춤형 모션으로 창의적인 비디오 생성을 위한 효율적이고 유연한 도구로 포지셔닝한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voletti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. _arXiv preprint arXiv:2311.15127_, 2023.\n' +
      '* [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22563-22575, 2023.\n' +
      '* [4] Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23206-23217, 2023.\n' +
      '* [5] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-video: Text-driven consistency-aware diffusion video editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 23040-23050, 2023.\n' +
      '* [6] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. _arXiv preprint arXiv:2311.12052_, 2023.\n' +
      '* [7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-10, 2023.\n' +
      '* [8] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 5343-5353, 2024.\n' +
      '* [9] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. _arXiv preprint arXiv:2304.14404_, 2023.\n' +
      '* [10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. _arXiv preprint arXiv:2305.13840_, 2023.\n' +
      '* [11] Iya Chivileva, Philip Lynch, Tomas Ward, and Alan Smeaton. Text prompts and videos generated using 5 popular Text-to-Video models plus quality metrics including user quality assessments. 10 2023.\n' +
      '* [12] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. _arXiv preprint arXiv:2312.02216_, 2023.\n' +
      '* [13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7346-7356, 2023.\n' +
      '* [14] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, et al. Dreamoving: A human video generation framework based on diffusion models. _arXiv e-prints_, pages arXiv-2312, 2023.\n' +
      '* [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. _arXiv preprint arXiv:2307.10373_, 2023.\n' +
      '* [17] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. _arXiv preprint arXiv:2312.02087_, 2023.\n' +
      '* [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\n' +
      '\n' +
      '그림 7: 우리 방법의 한계. **Top**: 상충되는 입력은 비현실적인 결과로 이어질 수 있습니다 - 이사하는 집입니다. **아래**: 겹친 상자는 물체 간섭으로 이어질 수 있습니다 - 호랑이는 곰 머리를 가지고 있습니다.\n' +
      '\n' +
      '특정 튜닝 없이 개인화된 텍스트-이미지 확산 모델. _ arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.\n' +
      '* [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv:2204.03458_, 2022.\n' +
      '* [26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [27] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. _arXiv preprint arXiv:2311.17117_, 2023.\n' +
      '* [28] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via masked-diffusion. _arXiv preprint arXiv:2312.07509_, 2023.\n' +
      '* [29] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. _arXiv preprint arXiv:2312.00845_, 2023.\n' +
      '* [30] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. _ACM Transactions on Graphics (TOG)_, 40(6):1-12, 2021.\n' +
      '* [31] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7701-7711, 2023.\n' +
      '* [32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [33] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22511-22521, 2023.\n' +
      '* [34] Shaoteng Liu, Yucheon Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. _arXiv preprint arXiv:2303.04761_, 2023.\n' +
      '* [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* [36] Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and Thomas Leung. Directed diffusion: Direct control of object placement through attention guidance. _arXiv preprint arXiv:2302.13153_, 2023.\n' +
      '* [37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [38] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.\n' +
      '* [39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [40] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. Animal kingdom: A large and diverse dataset for animal behavior understanding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19023-19034, June 2022.\n' +
      '* [41] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. _arXiv preprint arXiv:2308.07926_, 2023.\n' +
      '* [42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. _arXiv preprint arXiv:2303.09535_, 2023.\n' +
      '* [43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [44] Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and Dahua Lin. A unified framework for shot type classification based on subject centric lens. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI 16_, pages 17-34. Springer, 2020.\n' +
      '* [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [46] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [47] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [48] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [49] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [50] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [51] R. Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '\n' +
      '* [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [47] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* [48] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasono Li, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. _arXiv preprint arXiv:2303.08340_, 2023.\n' +
      '* [49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [51] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. _arXiv preprint arXiv:2306.03881_, 2023.\n' +
      '* [52] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '* [53] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [54] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. _arXiv preprint arXiv:2307.00040_, 2023.\n' +
      '* [55] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. _arXiv preprint arXiv:2303.17599_, 2023.\n' +
      '* [56] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [57] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. _arXiv preprint arXiv:2312.03641_, 2023.\n' +
      '* [58] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. _arXiv preprint arXiv:2312.04433_, 2023.\n' +
      '* [59] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7623-7633, 2023.\n' +
      '* [60] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn a motion pattern for few-shot-based video generation. _arXiv preprint arXiv:2310.10769_, 2023.\n' +
      '* [61] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5288-5296, 2016.\n' +
      '* [62] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. _arXiv preprint arXiv:2311.16498_, 2023.\n' +
      '* [63] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, and Ping Luo. Towards photo-realistic virtual try-on by adaptively generating-preserving image content. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7850-7859, 2020.\n' +
      '* [64] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation. _arXiv preprint arXiv:2306.07954_, 2023.\n' +
      '* [65] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragunwa: Fine-grained control in video generation by integrating text, image, and trajectory. _arXiv preprint arXiv:2308.08089_, 2023.\n' +
      '* [66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [67] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. _arXiv preprint arXiv:2305.17098_, 2023.\n' +
      '* [68] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. _arXiv preprint arXiv:2310.08465_, 2023.\n' +
      '* [69] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n' +
      '## 부록 구현 세부사항\n' +
      '\n' +
      '### 카메라 증강 세부 정보\n' +
      '\n' +
      '실세계 비디오들로부터 카메라 움직임 파라미터들을 추출하는 것은 계산적으로 집약적이며, 종종 카메라 움직임으로부터 물체 움직임을 분리하는 번거로운 프로세스를 요구한다. 이러한 문제를 우회하기 위해 고정 카메라의 영상을 알고리즘적으로 조작하여 카메라의 움직임을 시뮬레이션하는 카메라 증강 방법을 제안한다. 간단히 말해서, 카메라 증강은 정지 카메라에 의해 캡처된 비디오 시퀀스에 걸쳐 계산된 크로핑 윈도우를 변경함으로써 구현되며, 이에 의해 계산적으로 효율적인 방식으로 카메라 이동의 효과를 시뮬레이션한다. 이 과정의 상세한 의사코드는 그림 9에 예시되어 있다.\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      '카메라 이동 매개변수 샘플링.훈련 중에 카메라 이동 매개변수\\(\\mathbf{c}_{\\text{cam}}=[c_{x},c_{y},c_{z}]\\)에 대해 다음과 같은 샘플링 방식을 채택한다.\n' +
      '\n' +
      '{cases}0,\\text{with probability}\\frac{1}{3},\\text{with probability}\\frac{2}{3},\\end{cases}\\sim\\begin{cases}0,\\text{with probability}\\frac{1}{3},\\text{with probability}\\frac{1}(-1,1),\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{1}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\frac{2}{3},\\text{with probability}\\\n' +
      '\n' +
      '각 구성 요소는 독립적으로 샘플링됩니다.\n' +
      '\n' +
      '훈련 기법.우리는 사전 훈련된 Zeroscope T2V 모델[53]을 기본 모델로 채택한다. 사전 훈련된 상태를 유지하면서 카메라 이동 학습을 용이하게 하기 위해, 새로 추가된 레이어들만이 훈련가능하며, 이는 카메라 임베더 및 카메라 모듈을 포함한다. 학습의 속도를 높이기 위해 먼저 100k 반복에 대해 크기\\(256\\times 256\\times 8\\)(높이 x 너비 x 프레임)의 비디오를 학습한 다음 50k 반복에 대해 크기\\(320\\times 512\\times 16\\) 및 크기\\(320\\times 512\\times 24\\)의 비디오를 다시 학습한다. 트레이닝은 타임스텝\\([400,1000]\\)에서 균일하게 샘플링된 타임스텝\\(t\\)을 갖는 DDPM 잡음 스케줄러[23]를 사용하여 수행되며, 이러한 높은 타임스텝에 대한 선호도는 시간적 천이를 이해하는데 필수적이지 않은 것으로 간주되는 낮은 레벨의 세부사항들에 과적합되는 것을 방지하는 데 도움이 된다. 우리는 8개의 NVIDIA Tesla V100 GPU에서 5e-5의 학습률과 8의 배치 크기를 갖는 AdamW 최적화기[35]를 사용한다.\n' +
      '\n' +
      '## 부록 B 추가 절제 연구\n' +
      '\n' +
      '어텐션 증폭을 위한 어떤 레이어?어텐션 증폭을 적용할 레이어를 결정하기 위해 U-Net을 인코더(E), 중간 레이어(M) 및 디코더(D)의 세 부분으로 나눈다. 이 세 가지 조합의 다양한 조합에 주의 증폭을 적용하고 CLIP 점수와 흐름 오류에 미치는 영향을 평가했다. 결과는 표 4에 제시되어 있다. 우리는 디코더가 더 높은 CLIP 점수로 표시된 바와 같이 더 나은 의미 반응성을 산출하는 경향이 있는 반면, 인코더는 흐름 오류를 줄이는 데 더 효과적이라는 것을 관찰했다. 중간 계층은 상대적으로 작은 영향력을 가지고 있어, 중간 계층을 통합해도 눈에 띄는 통계량 변화를 가져오지 않는다. 결과적으로 가장 높은 CLIP-sim 점수와 가장 낮은 흐름 오류에 대해 모든 층에 주의 증폭을 적용한다.\n' +
      '\n' +
      'Attention amplification hyper-parameter.Attention amplification에서 strength\\(\\lambda\\)와 cut-off timestep\\(\\tau\\)은 두 개의 hyper-parameter이다. 일반적으로, 낮은 \\(\\tau\\)과 높은 \\(\\lambda\\)은 주의 증폭 강도를 증가시킬 것이다. 하이퍼-파라미터의 적절한 선택을 결정하기 위해, 우리는 \\(\\lambda\\)와 \\(\\tau\\)의 다른 조합으로 테스트를 수행한다. 시각적인 예는 그림 8에 나와 있으며, 물체 반응이 \\(\\lambda\\)보다 \\(\\tau\\)의 값에 더 민감하다는 것을 관찰했다. 첫 번째 행과 두 번째 행에서 알 수 있듯이 상자 영역에서 과반응은 보통 \\(\\tau<0.9\\)에 대해 발생한다. 확산 모델에서의 초기 샘플링 단계는 출력 이미지 또는 비디오의 거친 레이아웃을 결정하는 데 중요한 역할을 하기 때문에, 연장된 기간 동안 증폭을 적용하는 것은 박스 영역에서 과잉 반응을 초래하기 때문이다. 경험적으로, 우리는 \\(\\tau\\in[0.9T,0.95T]\\)과 \\(\\lambda\\in[10,25]\\)이 대부분의 경우에 일반적으로 적합하다는 것을 발견했다.\n' +
      '\n' +
      'functionaug_with_cam_motion(src_video, cx, cy, cx, h, w):\n' +
      '\n' +
      ' # 파라미터: # src_video: 소스 비디오, 차원을 갖는 텐서[프레임(f), 3, src_height, src_width] #cx: 수평 변환비(-1~1) #cy: 수직 변환비(-1~1) #cx: 줌비(0.5~2) #h: 생성된 비디오의 높이 #w: 생성된 비디오의 폭\n' +
      '\n' +
      ' # src_video로부터 소스 프레임 번호, 폭 및 높이를 얻는다  f, src_h, src_w = src_video.shape[0], src_video.shape[2], src_video.shape[3]\n' +
      '\n' +
      ' # frame cropping용 카메라 박스 초기화  cam_boxes = zeros(f,4)#f frame, 4: [x1,y1,x2,y2]\n' +
      '\n' +
      ' # 각 프레임에 대한 동적 크로핑 상대 좌표를 계산한다 # 첫 번째 프레임 좌표는 항상 [0,0,1,1]인 기준이다. cam_boxes[; 0] = linspace(0, cx + (1 - 1/cz)/2, f)#x1, 상단-좌측 x cam_boxes[; 1] = linspace(0, cy + (1 - 1/cz)/2, f)#y1, 상단-좌측 y cam_boxes[; 2] = linspace(1, cx + (1 + 1/cz)/2, f)#x2, 하단-우측 x cam_boxes[; 3] = linspace(1, cy + (1 + 1/cz)/2, f)#y2, 하단-우측 y_boxes[; 3] = linspace(1, cy + (1 + 1/cz)/2, f)#y2, 하단-우측 y_boxes[; 1]\n' +
      '\n' +
      ' # 최소 및 최대 상대 좌표  min_x = min(cam_boxes[:, 0:2])  max_x = max(cam_boxes[:, 0:2])  min_y = min(cam_boxes[:, 1:2])  max_y = max(cam_boxes[:, 1:2])\n' +
      '\n' +
      ' # cameraaboxes  normalized_boxes = zeros_like(cam_boxes)  normalized_boxes[:, 0::2] = (cam_boxes[:, 0::2]- min_x)/(max_x - min_x)  normalized_boxes[:, 1::2] = (cam_boxes[:, 1::2]- min_y)/(max_y - min_y)\n' +
      '\n' +
      ' # new frame  augmented_frames = zeros(f, 3, h, w)에 대한 atensor 초기화\n' +
      '\n' +
      ' # 범위(f)에서 i에 대한 각 프레임  를 처리:  # 실제 크로핑 좌표  x1, y1, x2, y2 = normalized_boxes[i]*텐서 ([src_w, src_h, src_w, src_h])를 계산\n' +
      '\n' +
      ' # 좌표  Crop  Crop = src_video[i][:, int(y1):int(y2), int(x1):int(x2)]에 따라 프레임을 Crop\n' +
      '\n' +
      ' # 크롭된 프레임의 크기를 조정하고  augmented_frames[i] = 보간(crop, size=(h, w), mode=\'bilinear\')\n' +
      '\n' +
      ' return augmented_frames\n' +
      '\n' +
      '도 9: 카메라 증강 기능을 위한 의사-코드.\n' +
      '\n' +
      '도 10: 카메라 움직임 제어 및 물체 움직임 제어의 추가 결과.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>