<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '웹사이트 데이터셋을 이용한 웹 스크린샷의 HTML 코드 변환 해제\n' +
      '\n' +
      ' 휴고 로런\n' +
      '\n' +
      'Leo Tronchon\n' +
      '\n' +
      'Victor Sanh\n' +
      '\n' +
      'Hugging Face\n' +
      '\n' +
      '웹 개발에서 비젼 언어 모델(VLM: Vision-language Model)을 사용하여 효율성을 높이고 코드 없는 솔루션을 차단할 수 있는 유망한 전략을 제시한다. 즉, UI의 스크린샷이나 스케치를 제공함으로써, VLM은 HTML과 같은 언어로 코드를 재생산할 수 있다. 다양한 작업에 대한 VLM의 발전에도 불구하고, 스크린샷을 대응하는 HTML로 변환하는 구체적인 과제는 최소한으로 탐구되었다. 우리는 이것이 주로 적합한 고품질 데이터 세트의 부재 때문이라고 가정한다. 이 연구는 200만 쌍의 HTML 코드와 해당 스크린샷으로 구성된 합성 데이터 세트인 웹사이트를 소개한다. 우리는 데이터 세트에서 기본 VLM을 미세 조정하고 웹페이지 스크린샷을 기능적 HTML 코드로 변환하는 데 능숙함을 보여준다. 이 분야의 연구를 가속화하기 위해 우리는 오픈 소스 웹사이트를 사용한다.\n' +
      '\n' +
      'Dataset: [https://huggingface.co/datasets/HuggingFaceM4/WebSight](https://huggingface.co/datasets/HuggingFaceM4/WebSight)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '현재 비전 언어 모델(VLMs)의 발전은 그들의 능력을 상당히 향상시켜, 이미지 캡션, 질문 응답, 및 광학 문자 인식(OCR)을 포함한 다양한 작업을 마스터할 수 있게 했다(OpenAI 등, 2023; Team 등, 2023; Hong 등, 2023; Liu 등, 2024a). 이러한 성과에도 불구하고 웹 사이트 또는 웹 구성 요소의 스크린샷을 사용 가능한 HTML 코드로 변환하는 작업은 웹 개발자에게 매우 가치 있는 프로세스이며 특히 오픈 소스 커뮤니티에서는 상대적으로 탐구되지 않은 상태로 남아 있다. 이러한 변환이 가능한 모델의 개발 및 오픈 소스 릴리스는 UI 개발자를 위한 새로운 AI 기반 도구를 잠금 해제하여 피그마와 같은 설계 도구를 위한 무코드 모듈 및 플러그인을 쉽게 만들 수 있다. 예를 들어, 디자인 스케치를 기능적 UI 구성요소 및 코드로 빠르게 변환하는 기능은 UI 개발자의 반복 속도를 크게 증가시킬 수 있다.\n' +
      '\n' +
      '우리는 VLM이 이 특정 작업에서 숙련도를 달성하기 위한 일차적인 도전이 작업 자체의 고유한 어려움에서 비롯되지 않는다고 가정한다. 오히려 주요 장애물을 제기하는 것은 HTML 코드 쌍과 관련 스크린샷의 크고 고품질 데이터 세트가 없다는 것이다. 사실 VLMs\n' +
      '\n' +
      '그림 1: 왼쪽의 원본 웹 페이지(입력)와 오른쪽의 모델 - 투시자 - (출력)에서 생성된 코드의 렌더링 비교. 원본 웹페이지의 시각적 측면을 맞추기 위해, 모델은 이미지-텍스트 쌍들의 웹-스케일 데이터세트들(Schuhmann et al., 2022; Gadre et al., 2023) 또는 멀티모달 웹 문서들(Laurencon et al., 2023; Zhu et al., 2023)에 대해 일반적으로 트레이닝되는 www.unsplash.comare로부터 적합한 이미지 배경을 선택하였다. 개방형 및 액세스 가능한 인공물로 스크린샷-HTML 쌍의 데이터 세트를 갖는 것은 커뮤니티가 데이터를 검사하고 데이터 세트에 대한 한계를 개선하고 개선할 수 있게 함으로써 이 분야의 연구를 상당히 가속화할 것이다. 결과적으로, 우리의 초기 초점은 이 작업을 위한 VLM의 미세 조정에 유용한 데이터 세트를 개발하는 것이다. 이를 달성하기 위해 몇 가지 전략을 고려할 수 있다.\n' +
      '\n' +
      '1. _기존 웹페이지 및 그 HTML 코드 활용._ 온라인에서 사용할 수 있는 HTML 파일의 방대한 저장소(그리고 흔히 커먼 크롤과 같은 웹 크롤에서 캡처됨)는 단순히 HTML을 렌더링하고 출력을 캡처함으로써 스크린샷 및 대응하는 HTML 코드 쌍을 생성하기 위한 유혹적인 리소스를 제시한다. 그러나, 이 접근법은 상당한 도전을 제기한다. 웹에서 발견되는 HTML 파일은 종종 주석, 스크립트 또는 데이터와 같은 노이즈로 가득 차 있으며, 매우 많은 수의 토큰을 포괄하는 과도하게 길 수 있으며, 때로는 대부분의 현재 모델의 최대 시퀀스 길이를 초과하기도 한다. 이러한 복잡성은 스크린샷의 내용과 기본 HTML 구문 간의 상관 관계를 정확하게 학습하는 모델의 능력을 방해한다. 추가적으로, HTML 코드는 종종 외부 자바스크립트(JS) 또는 캐스케이딩 스타일 시트(CSS) 스크립트에 대한 참조를 통합하거나, 별도의 디렉토리에 위치한 파일에 의존한다. 이러한 종속성은 의도된 디자인을 스크린샷에서 충실하게 재현하는 자체적인 HTML 파일의 생성을 복잡하게 한다. 이러한 장애물을 감안할 때, 우리는 더 통제된 접근 방식을 선호하기 위해 이 방법을 포기하기로 결정했다.\n' +
      '2. HTML 코드를 LLM(Large Language Models)과 합성하는 단계._ 가장 최근의 대규모 언어 모델, 특히 프로그래밍 언어에 대해 광범위하게 훈련된 모델은 웹사이트 개발을 포함한 다양한 도메인에 적용할 수 있는 고품질 코드를 생성하는 데 현저한 숙련도를 보여준다. 이 기능은 코딩에 특화된 LLM을 사용하여 방대한 HTML 코드 코퍼스를 인위적으로 만들 수 있는 문을 열어 지침을 따르도록 더욱 미세 조정했다. 프롬프트들을 적응시킴으로써, 우리는 토픽, 텍스트 길이 또는 웹사이트들에서의 이미지 배치를 제어하는 것과 같은 코드 생성 프로세스에 특정 제약들을 도입할 수 있다. 이러한 제어 수준은 관련 HTML 코드의 생산을 보장할 뿐만 아니라 모델이 효과적으로 훈련될 수 있는 더 깨끗하고 간결하며 구조화된 데이터를 모델에 제공함으로써 VLM에 더 적합하도록 한다. 우리의 연구는 이 접근법을 채택한다.\n' +
      '\n' +
      '식별된 격차에 대응하여 우리는 해당 스크린샷과 쌍을 이루는 HTML 코드의 200만 개의 예를 포함하는 포괄적인 합성 데이터 세트인 웹사이트를 개발한다. 이 데이터 세트를 활용하여 특수 모델 Sightseer를 얻기 위해 강력한 OCR 기능에 의해 특히 향상된 80억 매개변수의 향후 기본 VLM을 미세 조정한다. 이 미세 조정 프로세스는 웹 페이지 스크린샷을 기능적 HTML 코드로 변환하는 모델의 숙련도를 보여주면서 유망한 결과를 산출한다. 놀랍게도, 모델은 또한 필기 스케치를 기능적 HTML 코드로 변환하는 것과 같은 훈련되지 않은 시나리오에 적응하기 위한 범용성을 나타낸다. 이 방향으로 발전을 가속화하기 위해 우리는 웹사이트를 오픈합니다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'Nguyen and Csallner(2015)는 컴퓨터 비전과 광학 문자를 가진 인터페이스 요소 인식(이미지, 텍스트, 컨테이너 등)의 고전적인 파이프라인을 사용하고, 휴리스틱을 사용하여 이러한 검출에 대한 코드를 생성한다. 저자들은 모바일 UI에 대한 이 접근법의 효과를 보여준다. Beltrametli(2017)는 딥러닝을 이용하여 GUI(Graphic User Interface) 스크린샷으로부터 컴퓨터 코드를 생성하는 end-to-end 방법을 소개한다. 훈련된 엔드 투 엔드 모델은 단일 입력 이미지로부터 상이한 플랫폼들(iOS, 안드로이드, 및 웹)에 대한 코드를 생성할 수 있다. 합성곱 신경망과 순환 신경망을 사용하여 GUI 스크린샷을 해석하고 해당 코드를 생성한다. Lee et al.(2023)에서 저자들은 웹 페이지의 마스킹된 스크린샷을 단순화된 HTML로 변환하기 위해 VLM을 사전 훈련하고, 다양한 다운스트림 작업으로 잘 전달되는 기반 VLM을 사전 훈련하기 위한 이 훈련 목표의 유효성을 보여준다. Sightseer와 유사하게, 그들의 모델은 다양한 해상도의 이미지를 입력으로 받아들인다.\n' +
      '\n' +
      '웹사이트-v0.1의 최근 베타 릴리스에서는 823K 합성 스크린샷 쌍과 관련 HTML + 전통적인 CSS 코드가 있는 데이터 세트를 제공했다. 본 논문에서 논의된 웹사이트의 현재 버전(v0.2)에서는 상당한 개선을 소개한다. 먼저 WebSight-v0.2는 WebSight-v0.1에서 이미지 자리 표시자로 사용된 컬러 직사각형을 웹사이트의 내용과 일치하는 실제 이미지로 대체한다. 또한 테일윈드 CSS를 채택하여 코드를 간소화하고 시각적으로 매력적인 디자인을 쉽게 만들 수 있습니다. 다른 주목할 만한 업그레이드에는 데이터 세트 크기 2.5배, 고해상도 스크린샷 제공, 더 풍부한 메타데이터 제공이 포함됩니다.\n' +
      '\n' +
      'WebSight-v0.1은 이미 유용한 자원임이 입증되었다. Design2Code(Si et al., 2024)에서, 저자들은 스크린샷이 주어진 HTML 코드를 생성할 때 VLM을 평가하기 위한 벤치마크를 생성한다. 그들은 또한 합성 예제로 훈련된 모델이 더 길고 복잡한 실제 코드 데이터로 훈련된 모델을 능가하는 것을 관찰한 후 WebSight-v0.1에서 18B 매개변수 VLM을 미세 조정한다.\n' +
      '\n' +
      '##3 데이터셋 구축\n' +
      '\n' +
      '우리의 합성 HTML 코드 생성 프로세스에 대한 개요는 다양성과 품질을 극대화하기 위한 두 가지 핵심 단계를 포함한다. 첫째, 다양한 웹사이트 테마와 디자인을 생성하기 위해 더 작은 언어 모델을 사용합니다. 이러한 창의적인 출력은 다음 단계의 토대가 되며, 주로 코드 데이터에 대해 훈련된 더 큰 언어 모델의 프롬프트에 공급된다. 그런 다음 이 LLM은 최종 HTML 코드를 생성하여 데이터 세트가 고품질 코드를 생성하면서 광범위한 스타일을 포함하도록 한다.\n' +
      '\n' +
      '다양한 웹사이트 개념들을 생성하기 위해 우리는 Mistral-7B-Instruct(Jiang et al., 2023)를 채용하여 수백만의 고유한 웹사이트 개념들 및 설계들을 프롬프트와 함께 생성한다:\n' +
      '\n' +
      '도 2: WebSight에 존재하는 합성 웹 페이지의 예.\n' +
      '\n' +
      '각각 고유한 디자인 요소를 사용하여 서로 다른 회사를 위한 다양한 웹사이트 레이아웃 아이디어를 생성합니다. 예를 들어, 왼쪽 기둥이 있는 자동차 회사 사이트, 중앙에 로고가 있는 웹페이지 바닥글 등이 있습니다. 색상, 위치 및 회사 분야의 다양성을 탐색합니다. 요청을 이해했다는 설명이나 인정을 하지 말고 10가지 아이디어 목록을 하나씩 줄 바꿉니다.\n' +
      '\n' +
      '여기 생성된 개념들의 5개의 예들이 있다:\n' +
      '\n' +
      '1) 패션 브랜드: 전체 너비의 회전식 이미지 캐러셀이 최신 컬렉션을 선보이는 시각적으로 놀라운 레이아웃, 굵고 중앙에 정렬된 로고, 하단 내비게이션 메뉴입니다. 컬러 팔레트는 최신 패션 트렌드에서 영감을 받았습니다.\n' +
      '\n' +
      '2) 레스토랑 체인: 인기 있는 요리의 군침 도는 헤더 이미지를 가진 디자인, 위치, 메뉴, 온라인 주문을 위한 페이지가 있는 상단 네비게이션 메뉴. 바닥글에는 소셜 미디어 아이콘, 뉴스레터 등록 양식 및 연락처 정보가 표시됩니다.\n' +
      '\n' +
      '3) 컨설팅 회사: 전체 너비의 이미지와 태그라인이 있는 깨끗하고 전문적인 디자인, 메인 내비게이션 메뉴의 왼쪽 열, 주요 직원과 서비스 오퍼링을 특징으로 하는 오른쪽 열입니다. 음소거된 색상 팔레트는 신뢰와 전문성을 불러일으킵니다.\n' +
      '\n' +
      '4) 부동산 중개소: 매물 이미지 및 검색 기능을 특징으로 하는 헤더가 있는 사용자 친화적인 디자인, 다양한 매물 목록에 대한 링크가 있는 상단 탐색 메뉴. 페이지는 이미지를 나열하기 위한 그리드 레이아웃, 연락처 정보 및 소셜 미디어 아이콘이 있는 바닥글을 포함한다.\n' +
      '\n' +
      '5) 교육 플랫폼: 넓고 영웅적인 이미지, 중심적인 로고, 코스, 가격, 회사에 대한 링크를 특징으로 하는 탑 내비게이션 메뉴를 갖춘 디자인입니다. 이 사이트에는 학생들이 후기와 성공담을 공유할 수 있는 특집 코너가 포함되어 있다. 따뜻하고 접근하기 쉬운 색상 팔레트는 자신감과 참여를 장려하는 데 사용됩니다.\n' +
      '\n' +
      '시각적으로 다양하고 매력적인 디자인을 생성하는 전통적인 CSS보다 테일윈드 CSS를 선택하는 것은 순수한 HTML 이상을 필요로 한다. 그러나 VLM의 학습 과정을 단순화하기 위해서는 독립형 코드를 사용하는 것이 별도의 파일을 관리하는 것보다 바람직하다. 이러한 맥락에서 테일윈드 CSS는 이상적인 해결책으로 등장한다. 이 유틸리티 우선 프레임워크는 광범위한 유틸리티 클래스를 제공함으로써 고유한 디자인을 생성할 수 있게 하고, HTML 문서 내에서 직접 스타일링을 가능하게 하며, 외부 스타일 파일의 필요성을 제거한다. 테일윈드 CSS는 다양한 CSS 속성을 반영하는 사전 정의된 클래스의 광범위한 배열을 제공합니다. 이러한 유틸리티 클래스를 HTML 요소에 통합함으로써 웹 페이지를 효율적으로 스타일링하여 VLM에서 더 쉽게 배울 수 있는 간결한 코드를 생성할 수 있다.\n' +
      '\n' +
      '최종 HTML 코드를 생성하기 위해 코드 특화 LLM을 사용하여 HTML 코드를 생성한다. 우리는 Deepseek-Coder-33b-instruct(Guo et al., 2024)를 활용한다. Deepseek-Coder-33b-instruct(Guo et al., 2024)는 대부분 코드 데이터에 대해 훈련되고 지시를 따르도록 미세 조정되는 최첨단 언어 모델이다. 우리는 프롬프트를 사용한다:\n' +
      '\n' +
      'HTML 및 Tailwind CSS에서 양호한 디자인으로 완전한 웹사이트를 코드화: {concept}\n' +
      '\n' +
      '태그 안에 코드를 써라\\(<\\)body\\(>\\).\n' +
      '\n' +
      '사업에 대해 실제적이고 긴 문장을 쓰세요. 로렘으로 시작하는 문장을 절대 사용하지 마\n' +
      '\n' +
      'ipsum, NEVER.\n' +
      '\n' +
      '이미지를 포함할 필요는 없지만, 포함할 경우 이 소스만 사용\n' +
      '\n' +
      '*[https://source.unsplash.com/random/WxH/?keyword](https://source.unsplash.com/random/WxH/?keyword)*, URL 내 \'W\'와 \'H\'를 원하는 가로와 세로로, \'?keyword\'는 그림을 설명하는 키워드로 대체한다.\n' +
      '300x200 크기의 체육관에 대한 이미지에 대해 *[https://source.unsplash.com/random/300x200/?gym](https://source.unsplash.com/random/300x200/?gym) 또는\n' +
      '*[https://source.unsplash.com/random/100x200/?cake](https://source.unsplash.com/random/100x200/?cake)*인, 사이즈 100x200의 케이크 이미지.\n' +
      '\n' +
      '초기 과제는 많은 이미지를 포함하는 실제 웹사이트와 대조되는 출력물의 텍스트 전용 특성이었다. HTML 코드에 이미지를 통합하는 작업은 특히 웹 페이지의 컨텍스트와 관련된 이미지를 찾으려고 할 때 어려워 보이지만, 키워드를 기반으로 동적으로 이미지를 생성할 수 있는 기능을 제공하는 [https://source.unsplash.com/](https://source.unsplash.com/)와 같은 사진 스톡을 통해 효과적인 솔루션을 발견하여 특정 주제에 맞는 이미지를 제공할 수 있다.\n' +
      '\n' +
      '웹사이트의 주제에 맞지 않는 텍스트, 일반 콘텐츠 또는 이미지가 부족한 웹 페이지를 폐기하는 필터링 단계를 거쳐 최종적으로 200만 개의 웹 페이지로 마무리되었다.\n' +
      '\n' +
      '스크린샷 캡처 프로세스는 플레이라이트1을 사용하여 생성된 HTML 코드의 출력을 시각화하고 캡처한다. 우리는 스크린샷이 길이에 관계없이 전체 웹 페이지를 포함하는지 확인합니다. 결과적으로, 우리의 데이터 세트는 광범위한 해상도의 스크린샷을 특징으로 합니다. 이러한 이미지 크기와 형식의 다양성은 모델의 견고성을 향상시키는 데 유용하다.\n' +
      '\n' +
      '각주 1: [https://www.sceenshot.com/](https://www.sceenshot.com/)\n' +
      '\n' +
      '웹사이트 예제의 시각화는 웹사이트에 존재하는 다섯 가지 예가 그림 2에 나와 있다.\n' +
      '\n' +
      '##4 웹사이트 기반 시각언어 모델 미세조정\n' +
      '\n' +
      '웹 페이지 변환을 위한 모델 전제 조건은 웹 페이지 스크린샷을 HTML 코드로 정확하게 변환하기 위해 몇 가지 기능을 필요로 한다. 여기에는 이미지에서 텍스트를 전사하는 고급 OCR, 페이지에 요소를 배열하는 공간 이해, 위에서 설명한 전략으로 입력의 이미지와 유사한 이미지를 복제하는 객체 인식 능력이 포함된다.\n' +
      '\n' +
      '우리는 우리의 다가오는 기초 VLM을 기본 모델로 사용한다. Mistral-7B(Jiang et al., 2023)와 SigLIP-SO400M(Zhai et al., 2023)에 기반을 두고 있으며, Patch n\' Pack 전략(Dehghani et al., 2023)을 사용하여 입력 영상의 원래 종횡비를 보존하고 있으며, 각 면에 대해 최대 980픽셀의 해상도를 가지고 있다.\n' +
      '\n' +
      '이 기본 모델은 대부분 OBELICS(Laurencon et al., 2023), 이미지/텍스트 쌍 데이터 세트의 합성 캡션 및 OCR 데이터 세트의 조합에 대해 훈련되었다(Biten et al., 2022).\n' +
      '\n' +
      '모델의 아키텍처 및 교육 프로세스에 대한 추가 통찰력은 출시 시 자세히 설명됩니다.\n' +
      '\n' +
      'WebSightFor the fine-tuning on WebSightFor the fine-tuning, the fine-tuning, 대신, 안정적인 훈련을 위해 학습률을 크게 낮추어야 하는 모든 가중치들을 언프리징하는 대신에, 파라미터 효율적인 DoRA 방법(Liu et al., 2024b)을 64등급으로 사용한다. 우리는 사전 훈련 동안 선택된 학습률인 \\(10^{-4}\\)을 사용하면서, 반복당 2016개의 예를 보는 동안, 총 1100번의 반복에 대해, 하나의 에폭보다 약간 작은 값을 나타낸다.\n' +
      '\n' +
      '우리는 검증 손실이 훈련된 모델의 좋은 지표가 아니며, 특히 실제 사례에서 생성된 코드의 품질이 좋지 않다는 것을 발견했다. 결과적으로, 검증 손실에 의존하지 않고 생성된 샘플을 수동으로 검사하여 체크포인트 선택을 수행한다. 검증 손실이 여러 시대에 걸쳐 지속적으로 크게 감소함에도 불구하고, 훈련 데이터 세트의 것과 다른 웹 사이트로 일반화하는 능력 증가로 변환되지 않았다.\n' +
      '\n' +
      '그림 3: 왼쪽의 원본 웹 페이지(입력)와 오른쪽의 모델인 Sightseer, (출력)에 의해 생성된 코드의 렌더링 비교.\n' +
      '\n' +
      'Qualitative evaluation\n' +
      '\n' +
      '### 여러 스크린샷의 결과\n' +
      '\n' +
      '그림 3은 간단한 웹사이트 디자인을 제공했을 때 Sightseer의 다양한 출력을 보여줍니다. 특히, 입력이 제한된 양의 텍스트를 포함하는 경우들에서, 이 텍스트는 출력에서 정확하게 보존되는 경향이 있다.\n' +
      '\n' +
      '놀랍게도, Sightseer는 때때로 자필 웹사이트 스케치를 기능적 HTML 코드로 변환함으로써 입증된 바와 같이, 자신의 훈련 데이터 세트를 넘어 외관이 크게 다른 웹사이트로 일반화하는 능력을 나타낸다.\n' +
      '\n' +
      '### Failure cases\n' +
      '\n' +
      '우리의 분석에서, Sightseer는 복잡한 웹사이트 레이아웃, 과도한 텍스트 또는 학습 데이터와 상당히 다른 디자인에 어려움을 겪는다.\n' +
      '\n' +
      '일부 예에서, 생성된 코드는 렌더링 시 나타나지 않는 이미지, 텍스트 또는 버튼과 같은 요소를 포함한다. 이것은 배경이나 잘못된 구문 사용과 동일하게 착색된 텍스트와 같은 문제로 인해 발생할 수 있으며, 이는 Sightseer가 HTML + Tailwind CSS 구문을 완전히 마스터하지 않았음을 시사한다.\n' +
      '\n' +
      '도 4: 시선 생성 코드가 의도된 설계를 복제하는데 부족한 예.\n' +
      '\n' +
      '이 모델은 시각적으로 더 매력적인 웹사이트를 생성하지만 때때로 테일윈드 CSS 대신 전통적인 CSS를 사용한 WebSight-v0.1에서 훈련된 초기 모델2에서 관찰되지 않는 오류를 생성한다. 전통적인 CSS보다 최근의 프레임워크로서 테일윈드 CSS는 기본 LLM의 사전 훈련 데이터에서 덜 빈번한 발생을 가지며 LLM의 구문을 완전히 마스터하는 데 더 큰 문제가 있다고 가정한다. 우리는 데이터의 혼합물에서 텍스트 전용 HTML + 테일윈드 CSS로 사전 훈련된 기본 VLM으로 시작하여 Sightsseer의 번역 정확도를 크게 향상시킬 수 있다고 가정하고 이러한 개선을 달성하기 위해 관련 전략을 탐색하고 있다.\n' +
      '\n' +
      '각주 2: [https://huggingface.co/HuggingFaceM4/VLM_WebSight_finetuned](https://huggingface.co/HuggingFaceM4/VLM_WebSight_finetuned)\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 연구에서는 웹 페이지 스크린샷의 HTML 코드 변환 자동화에 기여하기 위해 200만 쌍의 HTML 코드와 해당 렌더링으로 구성된 대형 합성 데이터셋인 WebSight와 WebSight에서 OCR 기능을 미세 조정한 비전 및 언어 모델인 Sightsseer를 소개한다. 합성 데이터 생성을 활용하고 데이터 세트에서 대용량 기반 VLM을 미세 조정함으로써 점점 더 강력한 AI 기반 도구로 UI 개발 작업을 가속화하고 노코드 솔루션을 향상시킬 수 있는 실행 가능한 경로를 보여준다. 웹사이트를 오픈소싱함으로써, 우리는 이 분야의 추가적인 혁신과 연구를 육성하는 것을 목표로 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 벨트라멜리(2017) 토니 벨트라멜리. pix2code: 그래픽 사용자 인터페이스 스크린샷으로부터 코드를 생성하는 단계. _ ArXiv:1705.07962_, 2017.\n' +
      '* Biten et al. (2022) Ali Furkan Biten, Ruben Tito, Lluis Gomez, Ernest Valveny, and Dimosthenis Karatzas. Ocr-idl: 산업 문서 라이브러리 데이터세트에 대한 Ocr 주석, 2022.\n' +
      '* Dehghani et al. (2023) Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lucic, and Neil Houlsby. 패치 n\' 팩: 네빗, 임의의 종횡비 및 해상도를 위한 비전 트랜스포머, 2023.\n' +
      '*Gadre et al. (2023) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, 요나탄 Bitton, Kalyani Marathe, Stephen Mencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt. Datacomp: 차세대 멀티모달 데이터 세트를 찾기 위해, 2023.\n' +
      '* 코드 인텔리전스의 상승, 2024.\n' +
      '* Hong et al. (2023) Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, 및 Jie Tang. Cogagent: A visual language model for gui agent, 2023.\n' +
      '* Jiang et al. (2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 데 라스 카사스, 플로리안 브레산드, 지아나 령겔, 기욤 옴플, 루실 사울니에, 릴리오 레나르 라바우, 마리안 라초, 피에르 스톡, 테븐 르 스카오, 티보트 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 사예드. 미스트랄 7b, 2023\n' +
      '\n' +
      '* Laurecon et al. (2023) Hugo Laurecon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: 인터리빙된 이미지-텍스트 문서들의 오픈 웹-스케일 필터링된 데이터세트. IMT-2000 3GPP-30-7차 신경정보처리시스템 데이터세트 및 벤치마크 Track_에서, 2023. URL[https://openreview.net/forum?id=SKN2hf1BIZ](https://openreview.net/forum?id=SKN2hf1BIZ)\n' +
      '* Lee et al. (2023) Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2Struct: 시각적 언어 이해를 위한 사전 훈련으로서 스크린샷 파싱. Andreas Krause, Emma Brunskill, Kyungyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, Editors, _Proceedings of 40th International Conference on Machine Learning_, Volume 202 of Machine Learning Research_, pages 18893-18912. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/lee23g.html](https://proceedings.mlr.press/v202/lee23g.html).\n' +
      '* Liu et al. (2024a) Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: 개선된 추론, ocr 및 세계 지식, 2024년 1월. URL[https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* Liu et al. (2024b) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 도라: 체중 분해 저순위 적응, 2024b.\n' +
      '* 응우옌과 칼너(2015) 투안 안 응우옌과 크리스토프 칼너. 레마우이(t)와의 엔지니어링 모바일 애플리케이션 사용자 인터페이스를 반전시킨다. _ 2015 제30회 IEEE/ACM International Conference on Automated Software Engineering (ASE)_, pages 248-259, 2015. URL[https://api.semanticscholar.org/CorpusID:7499368](https://api.semanticscholar.org/CorpusID:7499368)\n' +
      '* et al. (2023) OpenAI et al. Gpt-4 technical report, 2023.\n' +
      '* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: 차세대 이미지-텍스트 모델을 훈련하기 위한 개방형 대규모 데이터세트, 2022.\n' +
      '* Si et al. (2024) Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 디자인2코드: 프론트엔드 엔지니어링을 자동화하는데 얼마나 걸리나요? 2024년\n' +
      '* et al. (2023) Gemini Team et al. Gemini: A family of highly capable multimodal models, 2023.\n' +
      '* Zhai et al. (2023) Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 언어 이미지 사전 훈련에 대한 시그모이드 손실, 2023.\n' +
      '* Zhu et al. (2023) Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 멀티모달 c4: 텍스트와 인터리빙된 열린 10억 규모의 이미지 코퍼스. IMT-2000 3GPP-30-7차 신경정보처리시스템 데이터세트 및 벤치마크 Track_에서, 2023. URL[https://openreview.net/forum?id=t0d8rSjcWz](https://openreview.net/forum?id=t0d8rSjcWz)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>