<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 이미지 내 제목 위치 지정\n' +
      '\n' +
      '왕이카이, 천지조, 차올동, 이판리 옌웨이푸\n' +
      '\n' +
      '푸단대학교 데이터과학부\n' +
      '\n' +
      '{yikaiwang19, yanweifu}@fudan.edu.cn\n' +
      '\n' +
      '[https://yikai-wang.github.io/seele](https://yikai-wang.github.io/seele)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '현재 이미지 조작은 주로 이미지 내의 특정 영역을 대체하거나 전체 스타일을 변경하는 것과 같은 정적 조작에 중점을 둔다. 본 논문에서는 혁신적인 동적 조작 과제인 주제 재배치(Subject Regositioning)를 소개한다. 이 작업은 이미지의 충실도를 유지하면서 사용자가 지정한 피사체를 원하는 위치로 재배치하는 것을 포함한다. 연구 결과, 재위치 주체에 의해 남겨진 공백을 메우고, 주체의 가려진 부분을 재구성하고, 주체를 주변 지역과 일치하도록 혼합하는 것을 포함하는 주제 재위치의 근본적인 하위 작업은 통일되고 신속한 인페인팅 작업으로 효과적으로 재구성될 수 있음을 보여준다. 결과적으로, 본 논문에서 제안한 태스크 역산 기법을 통해 학습한 다양한 태스크 프롬프트를 이용하여 이러한 서브 태스크를 해결하기 위해 단일 확산 생성 모델을 사용할 수 있다. 또한, 전처리 기술과 후처리 기술을 통합하여 피사체 재배치 품질을 더욱 향상시킨다. 이러한 요소들은 함께 SEELE(segment-gEnerate-and-bLEnd) 프레임워크를 형성한다. 주제 재배치에서 SEELE의 효율성을 평가하기 위해 ReS라고 하는 실제 주제 재배치 데이터 세트를 조립한다. ReS에 대한 우리의 결과는 재배치된 이미지 생성의 품질을 보여준다.\n' +
      '\n' +
      '피사체 재배치 작업은 사용자가 지정한 피사체를 단일 이미지 내에서 재배치하는 것을 목표로 합니다. ** 이상의 비교에서 우리는 구글 매직 에디터와 비교하여 SEELE 모델에 의해 달성된 주제 재배치 결과를 평가한다. 우리는 구글의 도입 웹페이지에서 구글의 결과를 얻었다. **아래는 주제 재배치에 의해 포괄되는 생성 하위 태스크를 예시한다: i) 일관성을 유지하고 새로운 무작위 주제를 생성하는 것을 피하기 위해 주제를 이동할 때 생성된 공백을 채워야 한다. ii) 이동된 피사체의 폐색된 부분을 완성하는 것이 필요하다. iii) 재배치된 대상의 외관은 주변 영역들과 블렌딩되어야 한다. SEELE는 단일 확산 생성 모델에 의해 구동되는 통합된 프롬프트 유도 인페인팅 작업 내에서 생성 하위 작업을 효과적으로 해결한다. 이러한 결과는 SEELE가 다루는 하위 작업을 설명하는 반면, SEELE를 실행하는 포괄적인 결과는 부록의 그림 13에 나와 있다.\n' +
      '\n' +
      '그림 1: 피사체 재배치 작업은 단일 이미지 내에서 사용자가 지정한 피사체를 재배치하는 것을 목표로 한다. ** 이상의 비교에서 우리는 구글 매직 에디터와 비교하여 SEELE 모델에 의해 달성된 주제 재배치 결과를 평가한다. 우리는 구글의 도입 웹페이지에서 구글의 결과를 얻었다. **아래는 주제 재배치에 의해 포괄되는 생성 하위 태스크를 예시한다: i) 일관성을 유지하고 새로운 무작위 주제를 생성하는 것을 피하기 위해 주제를 이동할 때 생성된 공백을 채워야 한다. ii) 이동된 피사체의 폐색된 부분을 완성하는 것이 필요하다. iii) 재배치된 대상의 외관은 주변 영역들과 블렌딩되어야 한다. SEELE는 단일 확산 생성 모델에 의해 구동되는 통합된 프롬프트 유도 인페인팅 작업 내에서 생성 하위 작업을 효과적으로 해결한다. 이러한 결과는 SEELE가 다루는 하위 작업을 설명하는 반면, SEELE를 실행하는 포괄적인 결과는 부록의 그림 13에 나와 있다.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '2023년 5월 Google Photos는 사용자가 이미지1 내에서 주제를 재배치할 수 있는 획기적인 AI 편집 기능을 도입했지만, 안타깝게도 수반되는 기술 문서의 부족은 이 기능의 내부 작업을 거의 조사하지 못했다. 딥러닝 시대에 앞서, Iizuka et al.(2014)은 이미지의 이해를 돕기 위해 지면 영역, 객체의 바운딩 박스 및 그림자 영역의 사용자 입력으로 객체 재배치라는 유사한 문제를 탐색하였다. 딥러닝이 빠르게 발전함에 따라 많은 사용자 행동을 학습 모델로 대체할 수 있는 잠재력과 이미지에 대한 고급 이해가 대두되어 강력한 딥러닝 모델의 렌즈를 통한 주제 재배치 문제에 대한 포괄적인 재평가가 필요하다. 본 논문의 주요 목적은 이미지 내에서 피사체를 재배치하기 위해 구글 포토스의 최신 AI 기능과 동등하거나 능가할 수 있는 독창적인 프레임워크를 소개하는 것이다.\n' +
      '\n' +
      '각주 1: [https://blog.google/products/photos/google-photos-magic-editor-pixel-io-2023/](https://blog.google/products/photos/google-photos-magic-editor-pixel-io-2023/]\n' +
      '\n' +
      '학문적인 관점에서, 이러한 특징은 이미지 조작의 영역 내에 속한다는 것이 명백하다(Gatys et al., 2016; Isola et al., 2017; Zhu et al., 2017; Wang et al., 2018; El-Nouby et al., 2019; Fu et al., 2020; Zhang et al., 2021). 이 지역은 주로 대규모 생성 모델의 발전으로 인해 최근 몇 년 동안 관심이 급증했다. 이러한 생성 모델은 생성적 적대 모델(Goodfellow et al., 2014), 변량 자동 인코더(Kingma & Welling, 2014), 자동 회귀 모델(Vaswani et al., 2017), 및 특히 확산 모델(Sohl-Dickstein et al., 2015)을 포함하는 다양한 기술을 포함한다. 모델 아키텍처들 및 트레이닝 데이터세트들 둘 모두가 계속해서 확장됨에 따라, 이들 생성 모델들은 이미지 조작에서 현저한 능력들을 나타낸다(Rombach et al., 2022; Kawar et al., 2022; Chang et al., 2023).\n' +
      '\n' +
      '그러나 현재의 이미지 조작 접근법은 주로 "정적" 변경으로 설명될 수 있는 것을 강조한다는 점에 유의하는 것이 중요하다. 이러한 방법들은 종종 자연 언어, 스케치, 스트로크, 또는 레이아웃과 같은 다양한 큐들에 의해 안내되는 이미지의 특정 영역들을 수정하도록 설계된다(El-Nouby et al., 2019; Zhang et al., 2021; Fu et al., 2020). 조작의 또 다른 차원은 실제 사진을 애니메이션 스타일의 그림으로 변환하거나 그림을 그리거나 특정 영화의 고유한 미학을 모방하는 것과 같은 작업을 포괄하는 이미지의 전체적인 스타일의 변환을 중심으로 한다(Chen et al., 2018; Wang et al., 2018; Jiang et al., 2021). 일부 접근법들은 심지어 이러한 조작 기법들을 비디오들의 영역으로 확장시켰다(Kim et al., 2019; Xu et al., 2019; Fu et al., 2022). 여기서 목적은 시간에 따라 스타일 또는 피사체들을 동적으로 조작하는 것이다. 대조적으로, 피사체 재배치 개념은 이미지의 나머지를 변경하지 않고 유지하면서 선택된 피사체를 재배치하는 데 특정한 초점을 두고 단일 이미지의 동적 조작을 파고든다.\n' +
      '\n' +
      '텍스트 대 이미지 확산 모델(Nichol et al., 2022; Ho et al., 2022; Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022)이 오늘날 가장 강력한 생성 모델 중 하나로 부상함에 따라, 주제 재배치를 위해 이들을 적응시키는 것은 흥미로운 기회를 제공한다. 그럼에도 불구하고, 텍스트-대-이미지 확산 모델들은 전형적으로 태스크-특정 명령들이 아닌 이미지 캡션 프롬프트를 사용하여 트레이닝되기 때문에, 중요한 도전은 이 태스크에 적합한 텍스트 프롬프트를 찾는 것에 있다. 더욱이, 최상의 텍스트 프롬프트들은 종종 이미지 의존적이며 다른 이미지들로 일반화하기 어려워, 사용자 친화성을 우선시하고 사용자 노력을 최소화하는 실제 애플리케이션들에 비실용적이다. 한편, 지역 인페인팅(Zeng et al., 2020; Zhao et al., 2021; Li et al., 2022; Suvorov et al., 2022; Dong et al., 2022), 주제 완성(Zhan et al., 2020) 및 지역 조화(Xu et al., 2017; Zhang et al., 2020; Tsai et al., 2017)와 같은 주제 재배치의 특정 측면을 다루기 위해 특화된 모델이 개발되었지만, 우리의 연구는 흥미로운 질문을 제기한다: "_우리는 단일 생성 모델?_"을 사용하여 이러한 모든 하위 작업을 달성할 수 있다.\n' +
      '\n' +
      '넓게는, 우리는 이 다면적인 작업을 몇 가지 별개의 하위 작업으로 해체할 수 있다. 우리는 이러한 하위 작업을 비생성 작업과 생성 작업으로 대별한다. 비생성 하위 작업은 사용자 지정 주제를 분할하고 주제 간의 폐색 관계를 추정하는 것과 같은 활동을 포함한다. 이 논문에서 우리는 주로 사전 훈련된 모델을 사용하여 비생성 측면을 다루면서 생성 하위 작업에 집중한다.\n' +
      '\n' +
      '주제 재배치에 필수적인 생성 하위 작업은 다음과 같은 핵심 요소를 포함한다:\n' +
      '\n' +
      'i) **피험자 제거**:피험자가 재배치된 후 공백이 남게 된다. 생성 모델의 과제는 새로운 요소의 도입을 피하면서 주변 배경을 이용하여 이 공백을 일관되게 채우는 것이다. ii) **주제 완료**: 재배치된 주제가 부분적으로 가려질 때, 생성 모델은 그 무결성을 유지하기 위해 주제를 완료해야 한다. iii) **피험자 조화**: 재배치된 피험자의 외관은 주변 영역들과 매끄럽게 어우러져야 한다.\n' +
      '\n' +
      '이러한 모든 하위 작업은 조작을 위한 이미지와 조작할 영역을 나타내는 마스크를 입력으로 취하지만 뚜렷한 생성 능력을 요구한다. 또한, 이러한 작업 지침을 동결 텍스트-이미지 확산 모델에 대한 캡션 스타일 프롬프트로 변환하는 것은 어렵다.\n' +
      '\n' +
      '다행히도, 확산 모델에서 사용되는 텍스트 프롬프트의 임베딩 공간은 단순히 캡션을 표현하는 것보다 훨씬 더 다재다능하다. 텍스트 역산(Gal et al., 2022)은 텍스트 프롬프트의 임베딩 공간 내에서 언어를 통해 전달하기 어려운 텍스트 및 양식 정보를 포함하는 사용자 지정 개념을 표현하는 방법을 배울 수 있음을 밝혔다. 또한, 신속한 튜닝(Lester et al., 2021; Liu et al., 2021)은 특정 도메인에 적응하기 위해 변압기에 효과적으로 사용되어 작업 수준에서 텍스트 역산을 적용하도록 영감을 주었다. 이러한 접근 방식은 확산 모델이 따라야 하는 특정 작업 지침을 나타내기 위해 텍스트 조건에서 잠재 임베딩을 학습하도록 영감을 준다. 이 태스크 레벨 반전 설계로, 단순히 태스크 레벨 "텍스트" 프롬프트를 수정함으로써 다양한 태스크에 확산 모델을 적용할 수 있다.\n' +
      '\n' +
      '본 논문에서는 주제 재위치 문제를 정형적으로 해결하기 위해 전처리, 조작, 후처리 파이프라인으로 주제 재위치 문제를 해결하는 SEgment-gEnerate-and-bLEnd (SEELE) 프레임워크를 제안한다. i) 전처리 단계에서 SEELE는 SAM(Kirillov et al., 2023)을 사용하여 사용자 지정 포인트, 경계 박스 또는 텍스트 프롬프트를 입력하여 주제 재위치 문제를 분할하고, 사용자 지정 이동 방향에 따라 주제 이동 및 주제 간의 정확한 폐색 관계를 추적한다. ii) 조작 단계에서 SEELE는 학습된 태스크 프롬프트에 의해 안내된 단일 사전 훈련된 확산 모델을 사용하여 주제 제거 및 주제 완료를 처리한다. iii) SEELE는 후처리 단계에서 재배치된 피사체를 조화시켜 인접 영역과 매끄럽게 혼합되도록 한다.\n' +
      '\n' +
      '주제 재배치 알고리즘을 평가하기 위해 ReS라는 실제 주제 재배치 데이터 세트를 조립했다. 이 데이터 세트는 재배치된 피사체를 특징으로 하는 100개의 실제 이미지 쌍으로 구성된다. 다양한 장면과 다양한 시간에 이미지를 수집하여 다양성을 높였다. SAM과 수동 개선을 사용하여 재배치된 피험자의 마스크에 주석을 달았다. 페어링된 영상에서 마스크의 중심점을 기준으로 이동 방향을 추정하였다. 폐색된 피험자를 위한 아모달 마스크도 제공합니다. 우리가 아는 한, 이것은 주제 재배치를 위한 첫 번째 데이터 세트이며 귀중한 벤치마크 평가 데이터 세트가 되기를 바란다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 본 논문은 주제 재배치 작업을 특수 이미지 조작 도전으로 기술하고, 이를 몇 가지 별개의 하위 과제로 분해하며, 각각은 고유한 과제를 제시하고 특정 학습 모델 요구 사항을 필요로 한다.\n' +
      '* 본 논문은 단일 확산 모델을 사용하여 주제 재배치에서 여러 생성 하위 작업을 다루는 SEELE(segment-gEnerate-and-bLEnd) 프레임워크를 제안한다.\n' +
      '* 본 논문은 혁신적인 과제 역산 기법을 탐구하여, 과제 지시사항을 표현하기 위해 텍스트 조건을 재형성할 수 있음을 증명한다. 이 탐사는 확산 모델을 특정 작업에 적용할 수 있는 새로운 가능성을 열어준다.\n' +
      '* 논문은 재배치된 주제를 특징으로 하는 이미지 쌍의 실제 집합인 ReS 데이터세트를 큐레이션한다. ReS는 주제 재배치 알고리즘을 평가하기 위한 귀중한 벤치마크 역할을 한다.\n' +
      '\n' +
      '##2 주제 재배치\n' +
      '\n' +
      '### 과제 정의와 과제\n' +
      '\n' +
      '피사체 재배치에는 사용자가 지정한 피사체를 이미지 내에서 이동하는 작업이 포함됩니다. 이 겉보기에 간단한 작업은 실제로 상당히 도전적이며, 여러 하위 작업의 조정이 필요하다.\n' +
      '\n' +
      '**사용자 입력** 주제 재배치 작업은 단일 이미지에 초점을 맞춥니다. 대화식 접근법으로서, 주제 재배치는 주제를 식별하고, 원하는 위치로 이동하고, 주제를 완성하고 재배치된 주제의 격차를 해결하기 위한 사용자 의도를 따른다. 특히, 사용자는 포인팅, 바운딩 박스, 또는 텍스트 프롬프트를 통해 관심 있는 주제를 식별하기 위한 시스템에 대한 입력으로서 식별한다. 그리고, 사용자는 드래그 또는 재위치 방향을 제공하여 원하는 재위치 위치를 제공한다. 시스템은 또한 사용자가 시각적 차이를 최소화하기 위한 특정 후처리 알고리즘을 실행하는지 여부뿐만 아니라 완성을 위해 피험자의 폐색된 부분을 표시할 것을 요구한다. 사용자 입력의 예시가 그림 3에 나와 있다.\n' +
      '\n' +
      '이 작업을 해결하기 위해 그림 2와 같은 SEELE(SegmentgEnerate-and-bLEnd) 프레임워크를 소개한다. 구체적으로 SEELE는 작업을 전처리 단계, 조작 단계, 후처리 단계의 세 단계로 나눈다.\n' +
      '\n' +
      'i) _preprocessing_는 피사체가 단일 객체, 객체의 일부, 또는 사용자의 의도에 의해 식별된 객체들의 그룹일 수 있다는 점을 고려하여, 최소의 사용자 입력으로 특정된 피사체를 정확하게 위치시키는 방법을 다루고; 식별된 피사체를 원하는 위치로 재배치하고; 또한 기하학적 일관성을 유지하기 위해 교합 관계들을 식별한다. 또한 전체 구성 내에서 원근 관계를 유지하기 위해 피험자의 크기를 조정하는 것이 필요할 수 있다.\n' +
      '\n' +
      'ii) _manipulation_ stage는 이미지를 향상시키기 위해 피사체 재배치에서 새로운 요소를 생성하는 주요 작업을 다룬다. 특히, 이 단계는 재배치된 피험자의 좌측 빈 공간 상의 빈 공간을 채우는 피험자 제거 단계를 포함한다. 추가로, 피사체 완성 단계는 피사체가 완전히 형성되도록 임의의 가려진 부분들을 재구성하는 것을 포함한다.\n' +
      '\n' +
      'iii) _postprocessing_ stage는 재배치된 주제와 그 새로운 주변 환경 사이의 시각적 차이를 최소화하는 것에 초점을 맞춘다. 여기에는 부자연스러운 경계 혼합, 조명 통계 정렬, 때로는 사실성을 추가하기 위한 사실적인 그림자를 만드는 것을 포함하여 외관과 기하학의 불일치를 고정하는 것이 포함됩니다.\n' +
      '\n' +
      '다음 섹션에서는 Sec. 2.2의 SEELE 파이프라인을 검토하는 것으로 시작할 것이다. 특히 생성 하위 작업을 해결하기 위해 Sec. 2.3의 작업 역전을 설명한다. Sec. 2.4에서는 확산 모델을 변경하지 않고 태스크 반전 기법을 사용하여 서로 다른 조작 하위 태스크를 훈련하는 방법을 보여준다. 마지막으로 Sec. 2.5의 큐레이트된 ReS 데이터 세트에 대한 자세한 소개를 제공한다.\n' +
      '\n' +
      '### Seele\n' +
      '\n' +
      '앞서 언급한 바와 같이 SEELE는 세 단계로 구성된다. 전처리 단계는 일반적으로 비생성 태스크를 포함하는 반면, 조작 및 후처리 단계는 생성 능력을 필요로 한다. SEELE에서는 모든 생성 하위 작업에 대해 통일된 확산 모델을 사용하고 비생성 하위 작업에 대해 사전 훈련된 모델을 사용한다. 우리는 다음 단계에서 각 단계에 대한 세부 정보를 제공합니다.\n' +
      '\n' +
      '그림 3: 각 단계에서 사용자 입력의 일러스트레이션.\n' +
      '\n' +
      '그림 2: SEELE는 피사체 재배치를 위한 전처리, 조작 및 후처리 파이프라인을 사용한다. 전처리 단계 동안, SEELE는 세그먼트화 모델을 사용하여 피험자를 식별하고, 사용자 제공 조건들에 의해 안내되고, 피험자들 사이의 교합 관계들을 온전하게 유지한다. 조작 단계에서 SEELE는 이미지를 조작하여 왼쪽 틈을 채웁니다. 또한, SEELE는 사용자가 지정한 불완전 마스크로 가려진 피사체를 보정한다. 후처리 단계에서 SEELE는 재배치된 주체와 새로운 주변 환경 간의 격차를 해결한다.\n' +
      '\n' +
      '**전처리** 피사체를 식별하기 위한 포인트 및 바운딩 박스 입력의 경우, 사용자 상호작용을 위해 SAM(Kirillov et al., 2023)을 활용하고 복잡한 구조를 갖는 피사체를 세그먼트화하는 품질을 향상시키기 위해 SAM-HQ(Ke et al., 2023)를 채용한다. 텍스트 입력을 가능하게 하기 위해, 우리는 텍스트-유도 SAM 모드를 간접적으로 구현하기 위해 SeMani(Wang et al., 2023)를 따른다. 구체적으로, 먼저 SAM을 사용하여 전체 이미지를 별개의 피사체로 분할한다. 이후 각 피험자를 입력 텍스트와 비교하여 마스크 적응 CLIP 모델을 사용하여 가장 유사한 피험자를 식별한다(Liang et al., 2022).\n' +
      '\n' +
      '피사체를 식별한 후, SEELE는 사용자 직관을 따라 피사체를 원하는 위치로 재배치한 다음, 조작 단계에서 원래 피사체를 재도색을 위한 보이드로 마스킹한다.\n' +
      '\n' +
      '우리의 SEELE는 이동된 피사체와 이미지의 다른 요소 사이의 잠재적 폐색을 처리한다. 원하는 위치에 존재하는 다른 피험자가 있는 경우, SEELE는 피험자 간의 교합 관계를 식별하기 위해 단안 깊이 추정 알고리즘 MiDaS(Ranftl et al., 2020)를 사용한다. 그 후, 사용자가 이러한 교합 관계들을 보존하기를 원한다면, SEELE는 피험자의 교합된 부분들을 적절하게 마스킹할 것이다. MiDaS는 또한 기하학적 일관성을 유지하기 위해 피험자 간의 원근 관계를 추정하고 그에 따라 피험자의 크기를 조정하는데 사용된다. 경계가 모호한 피험자의 경우, SEELE는 주변 영역과 더 잘 합성하기 위해 ViTMatte 매트팅 알고리즘(Yao et al., 2023)을 통합한다.\n' +
      '\n' +
      '\'조작\' 이 단계에서 SEELE는 피험자를 재배치하여 조작하는 일차적 과제를 다루고 있다. 그림 2에 예시된 바와 같이, 그것은 피사체 제거 및 피사체 완성 단계를 갖는다. 비판적으로, 그러한 두 단계는 두 단계의 마스킹된 영역이 주변 영역과 일치하도록 채워져야 하기 때문에 단일 생성 모델에 의해 효과적으로 해결될 수 있다. 그러나, 이 두 개의 서브 태스크는 서로 다른 정보와 마스크의 종류를 필요로 한다. 특히, 피사체 제거를 위해, 전형적인 객체 형상 마스크를 사용하여, 마스킹되지 않은 영역들로부터 _non-semantic_inpainting이 균일하게 적용된다. 이것은 종종 허위로 인해 구멍 내에 새로운 무작위 피험자가 생성된다. 한편, 피사체 완성은 _semantic-rich_inpainting을 포함하며, 마스킹된 영역의 대부분을 피사체의 일부로서 통합하는 것을 목표로 한다. 본 논문에서는 위의 서브 태스크에 필요한 서로 다른 생성 방향에 동일한 확산 모델을 적용하기 위해 SEELE의 태스크 역산 기법을 제안한다. 이 기법은 특정 작업 지침에 따라 확산 모델을 안내한다. 따라서, 학습된 _remove-prompt_ 및 _complete-prompt_와 함께, SEELE는 제거된 피사체 및 피사체 완성을 하나의 생성 모델로 결합한다.\n' +
      '\n' +
      '**후처리** 마지막 단계에서 SEELE는 아래 두 가지 과제를 해결함으로써 재배치된 주제와 주변 환경을 조화롭게 혼합한다.\n' +
      '\n' +
      'i) _Local harmonization_는 경계 및 조명 통계에서 자연스러운 외관을 보장한다. EELE는 다른 이미지 부분에 영향을 미치지 않도록 재배치된 피사체에 이 프로세스를 제한합니다. 피사체의 위치를 다시 지정하는 이미지와 마스크를 입력으로 사용합니다. 그러나, 안정적인 확산 모델은 초기에 마스킹된 영역 내에서 새로운 개념을 생성하도록 훈련되며, 마스킹된 영역과 그 주변에서의 일관성만을 보장한다는 우리의 목표와 상충된다. 이를 해결하기 위해 SEELE는 _harmonize-prompt_를 학습하고 LoRA 어댑터를 사용하여 마스킹된 영역을 안내함으로써 모델을 적응시킨다. 이러한 국부적 조화는 또한 새롭게 제안된 설계와 함께 조작 단계에서 사용되는 동일한 확산 모델에 통합될 수 있다.\n' +
      '\n' +
      'ii) _Shadow generation_는 재배치된 피사체에 대한 사실적인 그림자를 생성하는 것을 목표로 하여 현실감을 높인다. 다양한 피사체의 고해상도 이미지에서 고충실도 그림자를 생성하는 것은 여전히 어려운 일이다. SEELE는 그림자 생성을 위한 안정 확산 모델을 사용하며, 두 가지 시나리오를 다룬다: (1) 피사체가 이미 그림자를 가지고 있는 경우, 우리는 그림자를 확장하기 위해 피사체 완성을 위한 _complete-prompt_를 사용한다. (2) 그림자가 없는 피험자의 경우 사용자가 지정한 마스크를 기반으로 예비 그림자를 생성한다. 그런 다음 이 작업은 LoRA 어댑터 Hu 등과 함께 _harmonize-prompt_를 활용하여 현실적인 그림자 생성을 위한 로컬 조화 프로세스로 변환된다(2021).\n' +
      '\n' +
      '### Task inversion\n' +
      '\n' +
      '피사체 재배치에서 생성 서브-태스크는 원큐 접근법으로 이미지와 마스크를 입력한다:\n' +
      '\n' +
      '* 피험자 제거는 새로운 피험자를 생성하지 않고 공백을 채운다.\n' +
      '* 피사체 완료는 마스킹된 영역 내에서 1차 피사체를 완료한다.\n' +
      '* 주제 조화는 새로운 요소를 도입하지 않고도 일관성을 보장한다.\n' +
      '\n' +
      '이러한 요구 사항은 서로 다른 생성 경로로 이어집니다. 대조적으로, 우리의 목표는 높은 수준의 작업 지침에 의해 안내된 이미지 조작을 위한 텍스트 대 이미지 확산 인페인팅 모델을 향상시키는 것이다.\n' +
      '\n' +
      '이를 해결하기 위해 작업 역산을 도입하고, 백본을 고정한 상태에서 확산 모델을 안내하기 위한 훈련 프롬프트를 소개한다. 전통적인 텍스트 프롬프트 대신에, 우리는 "주제 완성"과 같은 지시 프롬프트로서 작용하는 적응가능한 표현들을 활용한다. 결과적으로, 작업 역산은 안정적인 확산을 사용하여 주제 재배치를 위한 다양한 생성 하위 작업의 원활한 통합을 허용한다. 이러한 통합은 새로운 생성 모델을 도입하거나 광범위한 모듈 또는 매개변수를 추가할 필요 없이 발생하여 작업 역전의 플러그 앤 플레이 특성을 강조한다.\n' +
      '\n' +
      '작업 역산은 확산 모델의 원래 훈련 목표를 준수합니다. 구체적으로 학습 이미지는 \\(\\mathbf{x}\\), 로컬 마스크는 \\(\\mathbf{m}\\), 학습 가능한 태스크 프롬프트는 \\(\\mathbf{z}\\)으로 표시한다. 우리의 목표는\n' +
      '\n' +
      '=\\mathbb{E}_{\\mathbf{\\varepsilon}\\sim\\mathcal{N}(0,1),t\\sim\\mathcal{U}(0,1)}\\big{[}\\|\\mathbf{\\varepsilon}-\\mathbf{\\varepsilon}_{\\theta}([\\mathbf{x}_{t},\\mathbf{m},\\mathbf{x}\\odot(1-\\mathbf{m}),t,\\mathbf{z}\\|_{\\mathrm{F}^{2}], \\tag{1}\\mathbf{z}\\|_{\\mathrm{F}^{2},\\tag{1}\\mathbf{\\varepsilon}-\\mathbf{\\varepsilon}_{\\theta}([\\mathbf{x}_{t},\\mathbf{m},\\mathbf{x}\\odot(1-\\mathbf{m}),t,\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\varepsilon}\\)은 랜덤 잡음이고; \\(\\mathbf{\\varepsilon}_{\\theta}\\)은 확산 모델이고, \\(t\\)은 정규화된 잡음 레벨이고; \\(\\mathbf{x}_{t}\\)은 잡음화된 이미지이고, \\(\\odot\\)은 요소별 곱셈이고; 그리고 \\(\\|\\cdot\\|_{\\mathrm{F}}\\)은 프로베니우스 규범이다. Eq로 훈련할 때. (1) 조건화 모델\\(c\\)과 확산 모델\\(\\mathbf{\\varepsilon}_{\\theta}\\)은 동결된 반면 임베딩 모델\\(\\mathbf{z}\\)은 학습 가능한 유일한 매개변수이다.\n' +
      '\n' +
      '우리의 과제 역산은 기존의 다양한 작업들에 의해 영향을 받지만 분명한 차이를 가진 독특한 접근법이다. 작업 역전에 대해 언급된 지시 프롬프트는 텍스트가 이미지의 내용을 설명하는 훈련 데이터의 범위를 넘어 잠재적으로 원하는 생성 결과에 영향을 미칠 수 있다. 최근 텍스트 역산의 발전(Gal et al., 2022)은 임베딩 공간 내에서 사용자가 지정한 개념을 이해할 수 있는 가능성을 강조한다. 대조적으로, 프롬프트 튜닝(Lester et al., 2021; Liu et al., 2021)은 입력들에 학습가능한 토큰들을 도입함으로써 특정 도메인들에 대한 적응을 향상시킨다. 시각적 이해를 위해 몇 개의 토큰을 훈련시키는 텍스트 반전과 달리, 우리의 작업 프롬프트는 전체 작업 지시를 포함한다. 우리의 작업 역전은 프롬프트 튜닝이 새로운 토큰을 추가하는 반면, 우리의 접근법은 텍스트 조건 입력을 대체한다는 점에서 프롬프트 튜닝을 달리한다. 우리는 확산 모델을 안내하기 위해 텍스트 입력에 의존하지 않고, 대신 학습을 위해 모든 토큰을 사용한다. 구별은 그림 4를 참조하십시오.\n' +
      '\n' +
      '#### : 학습과제 역산\n' +
      '\n' +
      '기존의 텍스트-이미지 확산 인페인팅 모델은 다양한 시나리오에서 일반화하기 위해 무작위로 생성된 마스크로 학습된다. 대조적으로, 태스크 역산은 트레이닝 동안 태스크-특정 마스크를 생성하는 것을 수반하여, 모델이 특수화된 태스크 프롬프트를 학습할 수 있게 한다.\n' +
      '\n' +
      'i)_피사체 제거를 위한 마스크들을 생성하는 것_: 피사체 재배치에서 좌측 보이드에 대한 마스크는 피사체의 형상을 미러링하지만, 우리의 목표는 마스크 내에서 피사체를 생성하는 것이 아니다. 이 시나리오에 대한 학습 데이터를 생성하기 위해 각 이미지에 대해 피험자와 마스크를 무작위로 선택한다. 다음으로, 우리는 그림 5의 중앙에 있는 소녀의 마스크에 의해 보여지는 바와 같이 마스크를 이동시킨다. 이것은 마스킹된 영역이 마스크의 모양과 무관한 무작위 부분들을 포함하는 이미지를 초래한다. 이는 원래 피사체 위치를 나타내는 마스크와 함께 피사체 제거의 대상이 된다.\n' +
      '\n' +
      'ii) 피사체 완성을 위한 마스크들을 생성하는 단계_: 이 단계에서, SEELE는 피사체를 효과적으로 완성하는 것을 목표로 피사체가 부분적으로 가려지는 시나리오들을 다룬다. 이 사전 정보를 태스크 프롬프트에 통합하기 위해 학습 데이터를 다음과 같이 생성한다. 각 이미지에 대해 피험자를 무작위로 선택하고 마스크를 추출한다. 그런 다음 연속 부분을 무작위로 선택합니다.\n' +
      '\n' +
      '그림 4: 작업 반전은 서로 다른 입력, 목표를 다루고 서로 다른 작업을 제공합니다.\n' +
      '\n' +
      '도 5: 피험자 제거 및 피험자 완성을 훈련시키기 위한 마스크 생성.\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 마스크 입력 방법. 사용자 지정 마스크는 일반적으로 부정확하기 때문에 마스크 내에 인접한 영역을 포함하도록 무작위 확장을 도입한다. 도 5의 우측의 우산 마스크에 의해 예시된 바와 같이, 이러한 마스크는 피사체 완료에 사용되는 마스크에 대한 추정치로서 작용한다.\n' +
      '\n' +
      '**학습 과목 조화** SEELE에서는 확산 모델의 대상을 변경하여 주제 조화를 달성한다. 이것은 마스킹된 이미지 상태를 조화롭지 않은 이미지로 대체하는 것 및 재구성 대상을 조화로운 이미지를 예측하는 것으로 대체하는 것을 포함한다. 작업 역산은 주로 교차 주의 계층에 영향을 미친다. 확산 모델에 자기 주의력을 적용하여 외형을 조화롭게 하면서 마스킹된 영역의 질감을 보존하기 위해 LoRA 어댑터를 소개한다(Hu et al., 2021). 이러한 어댑터는 주제 조화 과제를 학습하는 데 도움이 된다:\n' +
      '\n' +
      '=\\mathbbb{E}_{\\mathbf{e}\\sim\\mathcal{N}(0,1),t\\sim\\mathcal{U}(0,1}[\\\\mathbf{\\varepsilon}+\\mathbf{x}-\\mathbf{x}^{*}-\\mathbf{\\varepsilon}_{\\theta}([\\mathbf{x}_{t},\\mathbf{m},\\mathbf{x}],t,\\mathbf{z}\\|_{\\mathrm{F}^{2}], \\tag{2}\\mathbf{x}+\\mathbf{x}-\\mathbf{x}^{*}-\\mathbf{x}^{t},\\mathbf{m},\\mathbf{t},\\mathbf{z}\\|_{\\mathrm{F}^{2},\\tag{2}\\mathbf{x}+\\mathbf{x}-\\mathbf{\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}^{*}\\)는 타겟 조화 영상을 나타내고, \\(\\mathbf{x}\\)는 입력 영상을 나타낸다. 훈련 목표를 수정하는 동안 확산 모델의 생성 과정은 변하지 않는다. 이를 통해 학습된 조화 프롬프트 및 LoRA 파라미터와 함께 사전 학습된 안정 확산 모델을 여전히 활용하고 다른 모듈과 원활하게 통합할 수 있다. 자세한 내용은 부록의 Sec A.10을 참조하십시오.\n' +
      '\n' +
      '### ReS dataset\n' +
      '\n' +
      '주제 재배치 알고리즘의 효율성을 평가하기 위해 ReS라는 벤치마크 데이터 세트를 선별했다. 이 데이터 세트는 각각 4032\\(\\times\\)3024의 차원을 갖는 100개의 쌍을 이루는 이미지를 포함하며, 여기서 한 이미지는 재배치된 피사체를 특징으로 하고 다른 요소는 일정하게 유지된다. 이 이미지는 50개 이상의 범주에서 피험자를 보여주는 20개 이상의 실내 및 실외 장면에서 수집되었다. 이러한 다양성은 실제 개방형 어휘 애플리케이션의 효과적인 시뮬레이션을 가능하게 한다. 따라서 우리의 데이터 세트는 우리의 SEELE 모델을 평가할 만큼 충분히 다양하다.\n' +
      '\n' +
      '재위치된 피험자에 대한 마스크는 처음에 SAM을 사용하여 생성되었고 여러 전문가에 의해 정제되었다. 대상 완료를 돕기 위해 폐색된 마스크도 제공되었다. 각 이미지 쌍에서 마스크의 중심점 사이의 거리를 측정하여 재배치 방향을 추정했다.\n' +
      '\n' +
      '데이터 세트의 각 쌍을 이루는 이미지에 대해 한 이미지에서 다른 이미지로 그리고 반대로 피사체 재배치 성능을 평가하여 총 200개의 테스트 예를 얻을 수 있다. 그림 6은 ReS 데이터 세트를 보여준다. ReS 데이터 세트는 [https://github.com/Yikai-Wang/ReS](https://github.com/Yikai-Wang/ReS)에서 사용할 수 있다.\n' +
      '\n' +
      '##3 결과 및 분석\n' +
      '\n' +
      '**주제 재배치**의 예. 본 논문에서는 그림 7의 SEELE를 이용하여 \\(1024^{2}\\) 영상에 대한 피사체 재배치 결과를 제시한다.\n' +
      '\n' +
      'ReS**에 피험자 재배치 현재 피험자 재배치용으로 특별히 설계된 공개 모델들이 없기 때문에, 우리는 주로 원래의 안정 확산 인페인팅 모델(SD)과 비교한다. 우리는 텍스트 프롬프트, 단순 프롬프트 및 복합 프롬프트 없이 SD를 채택한다. 사용된 프롬프트는 부록의 Sec. A.3에 제공된다. 또한, 피험자 이동과 완료 서브 태스크의 마스크를 하나의 마스크에 결합함으로써 SEELE에 대안적인 인페인팅 알고리즘을 통합할 수 있다. 구체적으로, SEELE에 LaMa(Suvorov et al., 2021), MAT(Li et al., 2022), MAE-FAR(Cao et al., 2022), ZITS++(Cao et al., 2023)을 통합한다. 이 실험에서 SEELE는 사전 처리 또는 사후 처리 기술을 활용하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '우리는 부록에서 더 큰 버전이 그림 14인 그림 8에 정성적 비교 결과를 제시한다. <그림 15>와 <표 1>에서 더 많은 결과를 부록에서 확인할 수 있다. 오렌지를 넣는다\n' +
      '\n' +
      '도 6: ReS 데이터세트의 예. 이동 방향은 파란색으로 표시되어 있습니다. 사용자가 지정한 가시 부분의 마스크와 완성된 피사체는 주황색으로 표시됩니다.\n' +
      '\n' +
      '상기 입력 영상에서 피사체 제거 마스크 및 청색 피사체 완료 마스크 중 적어도 하나를 포함하는 것을 특징으로 하는 영상 처리 방법. 우리의 정성적 분석은 SEELE가 무작위 부품을 추가하지 않고 우수한 피사체 제거 능력을 나타내며 피사체 완료에 탁월함을 나타낸다. 이동된 피사체가 좌측 보이드와 겹치면 SD는 피사체를 안내한 보이드를 채운다. 대조적으로, SEELE는 그림 8의 상단 행에 도시된 바와 같이 피사체의 영향을 회피한다. 마스크가 정확하지 않으면, 네 번째 행에서 볼 수 있는 바와 같이, 불분명한 에지들의 충격을 감소시키고 빈 공간을 평활화함으로써 SEELE가 다른 방법들보다 더 잘 작동한다. 또한 두 번째 행에서 볼 수 있듯이 SEELE는 일반적인 인페인팅 알고리즘보다 주제 완료에서 발휘된다. SEELE는 후처리 단계를 통해 더욱 향상될 수 있음에 유의한다.\n' +
      '\n' +
      '** 제안된 태스크-역전의 효과** 제안된 과제 역산을 검증하기 위해 표준 학습 및 평가 원칙에 따라 표준 인페인팅 및 아웃페인팅 과제에 대한 실험을 수행한다. <표 2>와 <그림 16>에서 인페인팅, <표 3>과 <그림 17>에서 아웃페인팅 결과를 찾을 수 있는 부록의 Sec. A.5에서 분석을 제공한다.\n' +
      '\n' +
      '**SEELE w/X.** 전처리 및 후처리 단계 모두에서 SEELE 내의 다양한 구성요소의 효과를 평가한다. 우리는 부록의 그림 9와 같이 이러한 구성 요소의 활용 유무에 따라 SEELE의 결과를 정성적으로 비교하는 반면, 각 구성 요소에 대한 자세한 분석은 부록의 Sec. A.4에 제공된다.\n' +
      '\n' +
      '그림 8: ReS에 대한 주제 재배치의 질적 비교.\n' +
      '\n' +
      '그림 7: SEELE를 사용하여 \\(1024^{2}\\) 이미지 상의 피사체 재배치. 그림 13의 더 큰 버전을 참조하십시오.\n' +
      '\n' +
      '##4 관련 작품\n' +
      '\n' +
      '**이미지 및 비디오 조작**은 사용자가 지정한 안내에 따라 이미지 및 비디오를 조작하는 것을 목적으로 한다. 이러한 안내 중 자연어 안내는 선행 연구에서 제시된 바와 같이(Dong et al., 2017; Nam et al., 2018; Li et al., 2020; Ma et al., 2021; Karras et al., 2019; El-Nouby et al., 2019; Zhang et al., 2021; Fu et al., 2020; Chen et al., 2018; Wang et al., 2018; Jiang et al., 2021) 적응성 및 사용자 친화성으로 인해 특히 매력적으로 두드러진다. 일부 연구 노력은 이미지 대 이미지 번역 작업으로 개념화할 수 있는 시각적 조건의 사용도 탐구했다. 이러한 조건은 스케치 기반(Yu et al., 2019; Jo and Park, 2019; Chen et al., 2020; Kim et al., 2020; Chen et al., 2021; Richardson et al., 2021; Zeng et al., 2022), 레이블 기반(Park et al., 2019; Zhu et al., 2020; Richardson et al., 2021; Lee et al., 2020), 라인 기반(Li et al., 2019), 및 레이아웃 기반(Liu et al., 2019) 조건을 포함한다. 이미지 조작과는 대조적으로, 비디오 조작(Kim et al., 2019; Xu et al., 2019; Fu et al., 2022)은 상이한 프레임들에 걸쳐 시간적 일관성을 보장하는 추가적인 도전을 도입하며, 신규한 시간적 아키텍처의 개발을 필요로 한다(Bar-Tal et al., 2022). 이미지 조작은 주로 정적 이미지를 수정하는 것을 중심으로 이루어지는 반면, 비디오 조작은 다수의 피사체가 움직이는 동적 장면을 다룬다. 대조적으로, 본 논문은 한 피사체가 재배치되는 반면 나머지 이미지는 변경되지 않는 피사체 재배치에만 초점을 맞춘다.\n' +
      '\n' +
      '**Textual inversion**(Gal et al., 2022)는 사용자 지정 개념에 따라 텍스트 대 이미지 확산 모델을 개인화하도록 설계되었다. 다른 모든 매개변수를 고정한 채 텍스트 조건의 임베딩 공간 내에서 새로운 개념을 학습함으로써 이를 달성한다. Null-text inversion(Mokady et al., 2022)은 모델 용량을 향상시키기 위해 상이한 잡음 레벨에서 별개의 임베딩을 학습한다. 추가적으로, 일부 미세 조정(Ruiz et al., 2022) 또는 적응(Zhang and Agrawala, 2023; Mou et al., 2023) 기술들은 텍스트-대-이미지 확산 모델들에 시각적 조건들을 주입한다. 이러한 접근 방식은 이미지 패턴에 중점을 두는 반면, SEELE는 확산 모델을 안내하는 작업 지침에 중점을 둔다.\n' +
      '\n' +
      '** 프롬프트 튜닝**(Lester et al., 2021; Liu et al., 2021; Ma et al., 2021; Ma et al., 2021)는 트랜스포머 모델에 대한 추가 입력으로서 특정 토큰을 학습하도록 모델을 트레이닝하는 것을 수반하여, 모델을 미세 조정하지 않고 특정 도메인에 대한 모델 적응을 가능하게 한다. 이 기술은 비전 언어 모델(Radford et al., 2021; Yao et al., 2021; Ge et al., 2022)에서 널리 사용되었다. 이 개념은 텍스트 조건을 조정하여 텍스트 대 이미지 확산 모델을 작업 대 이미지 확산 모델로 변환하도록 영감을 주었다.\n' +
      '\n' +
      '**영상 합성**(Niu et al., 2021)는 전경과 배경을 결합하여 고화질의 영상을 생성하는 과정이다. 전경과 배경 요소의 특성 차이로 인해 외관, 기하 또는 의미 측면에서 불일치가 발생할 수 있다. 외관 불일치는 부자연스러운 경계와 조명 격차를 포함합니다. 세그멘테이션(Kirillov et al., 2023), 매팅(Xu et al., 2017) 및 블렌딩(Zhang et al., 2020) 알고리즘을 사용하여 경계 우려를 해결할 수 있는 반면, 이미지 조화(Tsai et al., 2017) 기술은 조명 불일치를 완화할 수 있다. 기하학적 불일치는 교합 및 불균형 스케일링, 객체 완성(Zhan et al., 2020) 및 객체 배치(Tripathi et al., 2019) 방법을 각각 필요로 한다. 의미적 불일치는 피험자와 배경 사이의 부자연스러운 상호 작용에 관한 것이며 본 논문의 범위를 벗어난다. 이미지 구도의 각 측면이 특정 초점을 가지고 있지만, 가장 중요한 목표는 높은 충실도의 이미지를 생성하는 것이다. 본 논문에서 SEELE는 단일 생성 모델 내에서 조화 기능을 향상시키는 데 중점을 둔다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 피사체의 충실도를 유지하면서 피사체가 원하는 위치에 위치하도록 입력 영상을 조작하는 것을 특징으로 하는 피사체 재배치라는 혁신적인 과제를 소개한다. 주제 재배치 문제를 해결하기 위해, 우리는 제안된 작업 반전 기술을 통해 생성 하위 작업을 해결하기 위해 단일 확산 모델을 활용하는 프레임워크인 SEELE를 제시한다. 여기에는 주체 제거, 주체 완성, 주체 조화, 그림자 생성 등의 과제가 포함된다. 비생성 하위 작업의 경우 사전 학습된 모델을 활용합니다. 주제 재배치 효과를 평가하기 위해 ReS라고 하는 실제 데이터 세트를 선별했다. ReS에 대한 우리의 실험은 이 작업을 수행하는 SEELE의 숙련도를 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Abid, A. Abdalla, A. Abid, D. Khan, A. Alfozan, and J. Zou(2019)Grado: Hassle-free sharing and testing of ml models in wild. ArXiv:1906.02569. 인용: SS1.\n' +
      '*O. 바탈, D. 오프리 아마르, R. 프리드먼 캐스턴과 T Dekel(2022)Text2live: text-driven layered image and video editing. 유럽 Conference on Computer Vision, pp. 707-723. Cited by: SS1.\n' +
      '* C. Cao, Q. 동, Y Fu(2022) Learning prior feature and attention enhanced image inpainting. 유럽 Conference on Computer Vision, pp. 306-322. Cited by: SS1.\n' +
      '* C. Cao, Q. 동, Y Fu(2023)Zits++: 구조 전위의 증분 변압기를 개선하여 이미지 인페인팅. IEEE의 패턴분석 및 기계지능에 관한 연구 인용: SS1.\n' +
      '* H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. 장민 양광 머피, W. T. 프리먼, M. Rubinstein, et al. (2023)Muse: 마스킹된 생성 트랜스포머를 통한 텍스트-이미지 생성. ArXiv:2301.00704. 인용: SS1.\n' +
      '*J. 첸, Y. Shen, J. Gao, J. Liu, 및 X. Liu (2018)Language-based image editing with recurrent attentive models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8721-8729. Cited by: SS1.\n' +
      '* S. 천원 수룡 가오성 Xia, and H. Fu(2020)DeepFaceDrawing: 스케치로부터 얼굴 이미지의 심층 생성. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2020)39 (4), pp. 72:1-72:16. Cited by: SS1.\n' +
      '* S. 천필유 Lai, P. L. Rosin, C. Li, H. Fu, L. Gao(2021)DeepFaceediting: Disentangled geometry와 appearance control을 이용한 deep face 생성 및 편집. ArXiv:2105.08935. 인용: SS1.\n' +
      '*Y. 청철현 Tulyakov와 M. 양(2022) 인아웃: 간 역산을 통한 다양한 이미지 아웃페인팅. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11431-11440. Cited by: SS1.\n' +
      '*W. 콩재장 니우락 류종 링원 리, L. Zhang(2020)Dovenet: 도메인 검증을 통한 딥 이미지 조화. CVPR에서 인용됨: SS1.\n' +
      '*H. Dong, S 유창우, 유영 Guo(2017) 적대적 학습을 통한 의미론적 이미지 합성. In Proceedings of the IEEE International Conference on Computer Vision, pp. 5706-5714. Cited by: SS1.\n' +
      '*Q. 동철조 Fu (2022)Incremental transformer structure enhanced image inpainting with masking position encoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11358-11368. Cited by: SS1.\n' +
      '* A. El-Nouby, S. 샤르마, H. 슐츠, D. Hjelm, L. E. Asri, S. 에브라히미 카후, Y. Bengio, and G. W. Taylor (2019)Tell, Draw, and repeat: Generating and modifying images based on continual linguistic instruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10304-10312. Cited by: SS1.\n' +
      '*T. 후, 엑스 왕승 Grafton M. Eckstein, and W. Y. Wang (2020) Iterative language-based image editing via self-supervised counterfactual reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4413-4422. Cited by: SS1.\n' +
      '*T. Fu, X. E. Wang, S. T. Grafton, M. P. Eckstein, 및 W. Y. Wang(2022)M3l: 멀티모달 멀티 레벨 트랜스포머를 통한 언어 기반 비디오 편집. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10513-10522. Cited by: SS1.\n' +
      '*R. 갈영 알라루프 아츠몬, 오 파타슈닉, A. H. 버마노, G. 체칙, D. 코헨-또는 (2022) 이미지는 텍스트 역산을 사용하여 텍스트 대 이미지 생성을 개인화하는 하나의 단어 가치가 있다. ArXiv:2208.01618. 인용: SS1.\n' +
      '\n' +
      '* Gatys et al. (2016) Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 컨볼루션 신경망을 이용한 이미지 스타일 전송 In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2414-2423, 2016.\n' +
      '* Ge et al. (2022) Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, and Gao Huang. 프롬프트 학습을 통한 도메인 적응 arXiv preprint arXiv:2202.06687_, 2022.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 생성적 적대적 그물 신경 정보 처리 시스템_, 27, 2014의 발전.\n' +
      '* Ho et al. (2022) Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. 고충실도 이미지 생성을 위한 캐스케이드 확산 모델. _ J 마흐 배워 Res._ 2022년 23시 47분 1초\n' +
      '* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: 대형 언어 모델의 낮은 랭크 적응. _ arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* Iizuka et al. (2014) Satoshi Iizuka, Yuki Endo, Masaki Hirose, Yoshihiro Kanamori, Jun Mitani, 및 Yukio Fukui. 단일 이미지에서 원근법에 따라 객체를 재배치합니다. In _Computer Graphics Forum_, volume 33, pp. 157-166. Wiley Online Library, 2014.\n' +
      '* Isola et al. (2017) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 조건부 적대적 네트워크를 사용한 이미지 대 이미지 변환 In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 1125-1134, 2017.\n' +
      '* Jiang et al. (2021) Wentao Jiang, Ning Xu, Jiayun Wang, Chen Gao, Jing Shi, Zhe Lin, and Si Liu. 교차 모드 순환 메커니즘을 통한 언어 유도 전역 이미지 편집 In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2115-2124, 2021.\n' +
      '* 조앤박(2019) 조영주, 박종열. Sc-fegan: 사용자의 스케치 및 색상으로 생성적 적대 네트워크를 얼굴 편집합니다. In _The IEEE International Conference on Computer Vision (ICCV)_, October 2019.\n' +
      '* Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처입니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 4401-4410, 2019.\n' +
      '* Kawar et al. (2022) Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: 확산 모델을 이용한 텍스트 기반의 실사 이미지 편집. _ ArXiv:2210.09276_, 2022.\n' +
      '* Ke et al. (2023) Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. 고품질로 모든 것을 분할합니다. _ arXiv preprint arXiv:2306.01567_, 2023.\n' +
      '* 김 등(2019) 다훈 김, 우상현, 이준영, 인소권. 딥 비디오 인페인팅. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5792-5801, 2019.\n' +
      '*김 등(2020) 김준호, 김민재, 강현우, 이광희. U-gat-it: 이미지 대 이미지 변환을 위한 적응적 계층 인스턴스 정규화를 갖는 비감독 생성 주의 네트워크. _International Conference on Learning Representations_, 2020. URL[https://openreview.net/forum?id=BJ125ySKPH](https://openreview.net/forum?id=BJ125ySKPH).\n' +
      '* Kingma and Welling (2014) D. P. Kingma and M. 잘 지내요 자동 인코딩 변량 베이지안 ICLR_, 2014.\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 뭐든 구분해 봐 arXiv:2304.02643_, 2023.\n' +
      '* Lee et al. (2020) Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: 다양하고 상호작용적인 얼굴 이미지 조작을 향하여. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* Lee et al. (2019)Brian Lester, Rami Al-Rfou, and Noah Constant. 매개변수 효율적인 프롬프트 조정을 위한 축척의 검정력 arXiv preprint arXiv:2104.08691_, 2021.\n' +
      '* Li et al. (2020a) Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. 매니건: 텍스트 안내 이미지 조작입니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 7880-7889, 2020a.\n' +
      '* Li 등(2020b) Bowen Li, Xiaojuan Qi, Philip Torr, and Thomas Lukasiewicz. 텍스트 유도 이미지 조작을 위한 가벼운 생성적 적대 네트워크 _ 신경 정보 처리 시스템_, 33, 2020b에서의 발전.\n' +
      '* Li et al. (2022) Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, 및 Jiaya Jia. 매트: 대형 홀 이미지 인페인팅을 위한 마스크 인식 변압기. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10758-10768, 2022.\n' +
      '* Li 등(2019) Yuhhang Li, Xuejin Chen, Feng Wu, and Zheng-Jun Zha. Linestofacephoto: 조건부 자기 주의 생성적 적대 네트워크가 있는 라인에서 얼굴 사진 생성. In _Proceedings of the 27th ACM International Conference on Multimedia_, pp. 2323-2331, 2019.\n' +
      '* Liang et al. (2022) Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. 마스크가 적용된 클립을 이용한 개방형 어휘 의미 분할 ARXiv 프리프린트 arXiv:2210.04150_, 2022.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 마이크로소프트 코코: 맥락상 흔한 물건들. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* Liu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, 및 Jie Tang. P-튜닝 v2: 프롬프트 튜닝은 스케일 및 태스크에 걸쳐 보편적으로 미세-튜닝에 필적할 수 있다. _ arXiv preprint arXiv:2110.07602_, 2021a.\n' +
      '* Liu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, 및 Jie Tang. Gpt도 이해해 arXiv preprint arXiv:2103.10385_, 2021b.\n' +
      '* Liu et al. (2019) Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, et al. Learning to predict layout-to-image conditional convolutions for semantic image synthesis. _ Neural Information Processing Systems_, 32, 2019에서의 발전\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Mokady et al. (2022) Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. 유도 확산 모델을 이용하여 실제 영상을 편집하기 위한 Null-text inversion _ arXiv preprint arXiv:2211.09794_, 2022.\n' +
      '* Mou et al. (2023) Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-어댑터: 텍스트 대 이미지 확산 모델에 대한 보다 제어 가능한 능력을 발굴하기 위한 학습 어댑터; _ arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* 남 등(2018) 성현남, 김윤지, 김선주. 텍스트 적응형 생성적 적대 네트워크: 자연어로 이미지를 조작하는 것. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, pp. 42-51, 2018.\n' +
      '* Nichol et al. (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 글라이드: 텍스트 유도 확산 모델을 사용하여 실제 이미지 생성 및 편집을 진행합니다. 2022년 _ICML_에서\n' +
      '*Niu et al. (2021) Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, 및 Liqing Zhang. 이미지를 다시 실제화: 심층 이미지 구성에 대한 포괄적인 조사입니다. _ arXiv preprint arXiv:2106.14490_, 2021.\n' +
      '* Park et al. (2019) Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu. 공간 적응 정규화를 이용한 의미론적 영상 합성 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2337-2346, 2019.\n' +
      '* Zhang et al. (2019)Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ ArXiv:2204.06125_, 2022.\n' +
      '* Ranftl et al. (2020) Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 견고한 단안 깊이 추정을 위한: 제로샷 교차 데이터세트 전송을 위한 데이터세트 혼합. _ IEEE transaction on pattern analysis and machine intelligence_, 44(3):1623-1637, 2020.\n' +
      '* Richardson et al. (2021) Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 스타일 인코딩: 이미지 간 변환을 위한 스타일간 인코더입니다. 2021년 6월 _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_에서.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10684-10695, 2022.\n' +
      '* Ruiz et al. (2022) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: 피사체 중심 생성을 위한 텍스트-이미지 확산 모델의 미세 조정 arXiv preprint arXiv:2208.12242_, 2022.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Hwang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _ 신경 정보 처리 시스템_, 35:36479-36494, 2022에서의 발전.\n' +
      '* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 평형 열역학을 이용한 심층 비지도 학습 In _International Conference on Machine Learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Suvorov et al. (2021) Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 푸리에 컨볼루션을 사용한 해상도 강화 대형 마스크 인페인팅 arXiv preprint arXiv:2109.07161_, 2021.\n' +
      '* Suvorov et al. (2022) Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 푸리에 컨볼루션을 사용한 해상도 강화 대형 마스크 인페인팅 In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 2149-2159, 2022.\n' +
      '* Tripathi et al. (2019) Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M Rehg, and Visesh Chari. 합성을 통해 합성 데이터를 생성하는 학습. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 461-470, 2019.\n' +
      '* Tsai et al. (2017) Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. 깊은 이미지 조화입니다. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 3789-3797, 2017.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목해 주세요 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wang et al. (2018) Hai Wang, Jason D Williams, and SingBing Kang. 텍스트 설명으로 이미지를 전역적으로 편집하는 방법을 배우는 중입니다. _ arXiv preprint arXiv:1810.05786_, 2018.\n' +
      '* Wang et al. (2023) Yikai Wang, Jianan Wang, 관송 Lu, Hang Xu, Zhenguo Li, Wei Zhang, and Yanwei Fu. 개체 수준 텍스트 유도 이미지 조작입니다. _ arXiv preprint arXiv:2302.11383_, 2023.\n' +
      '* Xia et al. (2021) Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: 텍스트 유도 다양한 얼굴 이미지 생성 및 조작. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2256-2265, 2021.\n' +
      '*Xu et al.(2020)*Xu et al.(2017) Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. 딥 이미지 매팅입니다. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 2970-2979, 2017.\n' +
      '* Xu et al. (2019) Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy. 딥 플로우 가이드 비디오 인페인팅입니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3723-3732, 2019.\n' +
      '* Yao et al. (2023) Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. 비트매트: 미리 훈련된 평시변압기를 이용한 영상매팅 부스팅 arXiv preprint arXiv:2305.15272_, 2023.\n' +
      '* Yao et al. (2021) Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: 사전 훈련된 비전 언어 모델에 대한 컬러풀 프롬프트 튜닝. _ arXiv preprint arXiv:2109.11797_, 2021.\n' +
      '* Yu et al. (2019) Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. 게이티드 컨벌루션으로 자유로운 형태의 이미지를 인페인팅합니다. In _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 4471-4480, 2019.\n' +
      '* Zeng et al. (2020) Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, and Huchuan Lu. 반복적인 신뢰 피드백 및 가이드 업샘플링을 통한 고해상도 이미지 인페인팅. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIX 16_, pp. 1-17. Springer, 2020.\n' +
      '* Zeng et al.(2022) Yu Zeng, Zhe Lin, and Vishal M Patel. 스케치: 부분 스케치를 사용한 마스크 없는 로컬 이미지 조작. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 5951-5961, 2022.\n' +
      '* Zhan et al. (2020) Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, and Chen Change Loy. 자체 감독 장면 폐색 제거 In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 3784-3792, 2020.\n' +
      '* Zhang et al. (2020) Lingzhi Zhang, Tarmily Wen, and Jianbo Shi. 딥 이미지 블렌딩. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 231-240, 2020.\n' +
      '* Zhang & Agrawala (2023) Lvmin Zhang and Maneesh Agrawala. 텍스트 대 이미지 확산 모델에 조건부 컨트롤 추가. _ arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '* Zhang et al. (2021) Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, and Irfan Essa. 신경 연산자로서의 텍스트: 텍스트 명령에 의한 이미지 조작. In _Proceedings of the 29th ACM International Conference on Multimedia_, pp. 1893-1902, 2021.\n' +
      '* Zhao et al. (2021) Shen규 Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. 상호 변조된 생성적 적대 네트워크를 통한 대규모 이미지 완성. _ arXiv preprint arXiv:2103.10428_, 2021.\n' +
      '* Zhou et al. (2017) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 장소: 장면 인식을 위한 천만 개의 이미지 데이터베이스. _ IEEE transaction on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.\n' +
      '* Zhu et al.(2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 주기적으로 일치하는 적대적 네트워크를 사용하여 이미지 대 이미지 변환이 쌍을 이루지 않습니다. In _Proceedings of the IEEE international conference on computer vision_, pp. 2223-2232, 2017.\n' +
      '* Zhu et al. (2020) Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: 의미 영역 적응 정규화를 이용한 이미지 합성. 2020년 6월 _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_에서.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '### Additional Examples\n' +
      '\n' +
      '본 절에서는 먼저 그림 13의 SEELE를 이용하여 크기 \\(1024\\times 1024\\)의 이미지에 대한 피사체 재배치 결과를 제시하고, 그림 14의 논문에서 그림 8의 더 큰 시각화를 제공하고, 그림 15의 ReS 데이터셋에서 보여지는 바와 같이 SEELE와 경쟁자를 이용한 피사체 재배치 사례를 추가로 제시한다.\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      'SEELE는 SD 2.0 기반에서 미세 조정된 텍스트 유도 인페인팅 모델에 기반하며, 태스크 반전 기법을 사용하여 50개의 학습 가능한 토큰으로 각 태스크 프롬프트를 학습하고, 태스크 명령에서 텍스트 설명으로 초기화된다. 각 태스크에 대해 학습률 \\(8.0e-5\\), 무게 감소 \\(0.01\\), 배치 크기 32의 AdamW 최적화기(Loshchilov and Hutter, 2017)를 활용하였으며, 9,000단계 이상의 2개의 A6000 GPU를 대상으로 훈련하여 홀드아웃 검증 세트를 기반으로 최적의 체크포인트를 선정한다.\n' +
      '\n' +
      '주제 이동 및 완료를 다룰 때, 객체 마스크를 제공하는 MSCOCO 데이터세트(Lin et al., 2014)를 사용한다. 이미지 조화를 위해, iHarmony4 데이터세트(Cong et al., 2020)가 활용되어, 피사체-대-조화 마스크들과 함께 비조화-조화 이미지 쌍들을 제공한다. MSCOCO는 80k 트레이닝 이미지와 40k 테스팅 이미지로 구성되며, iHarmony4는 65k 트레이닝 이미지와 7k 테스팅 이미지로 구성된다. 이러한 다양성은 훈련 작업 프롬프트의 견고성을 보장하여 특정 이미지의 오버피팅을 방지합니다.\n' +
      '\n' +
      '**비용 분석** SEELE의 핵심 구성요소는 사전 훈련된 안정적인 확산 인페인팅 모델로 UNet 백본 내에서 8억 6,6593만 개의 매개변수를 자랑한다. 이를 위해 50\\(\\times\\)1024의 크기와 0.5백만 개의 훈련 가능한 매개 변수를 갖는 세 가지 별개의 작업 프롬프트를 통합한다. 로컬 조화 작업을 위해 훈련 가능한 매개 변수 512만 개를 포함하는 LoRA 어댑터를 소개합니다. 이러한 새로 추가된 매개변수는 가볍고 안정적인 확산 백본과 비교할 때 추가 추론 지연을 도입하지 않는다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '### ReS의 정량적 비교\n' +
      '\n' +
      '피험자 재위치의 효율성을 평가하기 위해, 우리는 PSNR(Peak Signal-to-Noise Ratio), SSIM(Structural Similarity Index Measure), LPIPS(Learned Perceptual Image Patch Similarity) 등 다양한 메트릭을 사용하여 새롭게 도입된 ReS 데이터셋에 대한 실험을 수행했다. 표준 인페인팅 알고리즘에 맞게 최소 측면 길이가 512인 모양으로 이미지를 조정합니다. 유감스럽게도 구글 포토의 매직 에디터 기능은 현재 대중에게 제공되지 않으며, 우리는 그것에 접근할 수 없습니다. 따라서 우리는 ReS에서 SEELE와 매직 에디터를 비교할 수 없다.\n' +
      '\n' +
      '기준선 안정 확산 모델과 비교하여 SEELE는 PSNR에 특히 중점을 두고 모든 메트릭에 걸쳐 조작된 이미지의 품질이 크게 향상되었음을 보여준다. SEELE로 안정적인 확산 백본을 사용하면 LPIPS 점수가 향상되는 반면 표준 인페인팅 알고리즘과 결합된 SEELE는 우수한 PSNR 결과를 산출한다.\n' +
      '\n' +
      'SD 프롬프트입니다 프롬프트가 없는 SD의 경우 텍스트 조건에 대해 ""로 입력한다. SD에 사용되는 간단한 프롬프트는 "인페인팅"과 "피사체 완성"이며, 피사체 이동 및 완성 작업에 대해 각각 "인페인팅을 통해 제공된 이미지에 시각적으로 응집력 있고 고충실도 배경과 질감을 통합"과 "결측된 영역에 시각적으로 응집력 있고 고충실도 배경과 질감을 채워 주어 피사체를 완성"과 같은 복잡한 프롬프트이다.\n' +
      '\n' +
      'SEELE에서 X의### 분석\n' +
      '\n' +
      '여기서는 SEELE에 사용된 각 구성 요소에 대한 분석을 제공한다.\n' +
      '\n' +
      'i) Occlusion_에 대한 _Depth 추정은 사용자들이 포어그라운드에서 백그라운드로 피사체를 이동시키고자 할 때 중요해진다. 폐색된 부분을 추정하고 수정하는 데 도움이 되며, 재배치된 피사체가 장면에 매끄럽게 혼합되도록 합니다. 그림 9의 첫 번째 행에서 알 수 있듯이 이 깊이 추정은 잎 뒤에 있는 탑이나 자동차 뒤에 있는 사람과 같은 물체를 재배치하는 데 중추적인 역할을 한다. 교합 관계를 무시하는 것은 부자연스럽게 보이는 재배치된 피사체 및 이미지 충실도의 상당한 손실을 초래할 수 있다.\n' +
      '\n' +
      'ii) perspective_에 대한 _Depth 추정은 사용자가 재위치하는 동안 주체의 크기를 비례적으로 조정하기를 원할 때 실행된다. 이러한 측면을 간과하면 피험자의 크기는 고정된 상태로 유지되어 사용자의 기대와 모순될 수 있다.\n' +
      '\n' +
      'iii) _Matting_는 주로 SAM에 의해 제공되는 부정확한 마스크로부터 발생하는 문제, 특히 모호한 경계를 갖는 주제를 다룰 때 문제를 다룬다. 정확하지 않은 마스킹은 최종 출력에서 정보가 누출될 수 있기 때문에 매우 중요합니다. 예를 들어, 그림 9에서 부정확한 마스킹은 틈새를 부추겨 부자연스러운 개 털을 생성할 수 있다.\n' +
      '\n' +
      'iv) _Shadow generation_는 SEELE 내의 생성 모델을 재사용함으로써 처리된다. 그림 9의 왼쪽 부분과 같이 피사체가 그림자를 포함하는 경우, 우리는 이를 피사체 완성 과제로 접근한다. 그림자 자체가 주체가 되고 우리는 학습 완료 프롬프트를 사용하여 안내합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & PSNR(\\(\\uparrow\\)) & SSIM(\\(\\uparrow\\)) & LPIPS(\\(\\downarrow\\)) \\\\ \\hline SD (no prompt) & 20.038 & 0.664 & 0.157 \\\\ SD (simple prompt) & 20.039 & 0.664 & 0.157 \\\\ SD (complex prompt) & 20.029 & 0.664 & 0.157 \\\\ \\hline SEELE & 20.100 & 0.666 & 0.156 \\\\ SEELE(ZITS+) & 20.330 & 0.679 & 0.176 \\\\ SEELE(MAE-FAR) & 20.453 & 0.680 & 0.172 \\\\ SEELE(LaMa) & 20.320 & 0.678 & 0.163 \\\\ SEELE(MAT) & 20.199 & 0.675 & 0.163 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 피험자 재위치의 정량적 비교.\n' +
      '\n' +
      '도 9: SEELE에서 성분 X를 사용하는 절제.\n' +
      '\n' +
      '확산 모델. 반대로 피사체에 그림자가 부족할 경우 SEELE의 조화 모델을 활용하여 그림자를 생성함으로써 로컬 조화 작업으로 변환할 수 있다.\n' +
      '\n' +
      'v) _Local harmonization_는 외관 불일치의 도전을 해결한다. 피사체 재배치 후 조명 통계가 변할 때, 피사체의 질감을 유지하면서 피사체의 외형을 조정하는 것이 필수적이다. 그림 9에서 볼 수 있듯이 SEELE는 이 지역 조화 작업에서 탁월하여 새로운 환경으로의 원활한 통합을 보장합니다.\n' +
      '\n' +
      '###표준 이미지 인페인팅 및 아웃페인팅\n' +
      '\n' +
      '**이미지 인페인팅** 제안된 태스크 역산 접근법은 특정 태스크에 대한 인페인팅 모델을 전문화할 뿐만 아니라 표준 인페인팅 기능을 향상시킨다. 우리는 Places2 데이터세트(Zhou et al., 2017)에서 수행된 실험을 통해 이 주장을 입증하며, 여기서 표준 인페인팅 프롬프트를 사용하여 SEELE를 훈련하고 다른 인페인팅 알고리즘과 성능을 비교한다. 결과는 표 2에 나와 있으며, 또한 그림 16의 결과를 시각적으로 표현하여 환각 인공물을 줄이는 SEELE의 이점을 보여준다.\n' +
      '\n' +
      '**이미지 아웃페인팅** 일반적으로 사용되는 또 다른 조작 작업은 이미지를 원래 콘텐츠 이상으로 확장하는 것을 포함한다. 이 접근법은 주제 완성도와 유사한 개념을 공유하지만 전체 이미지를 향상시켜 보다 총체적인 관점을 취한다. 또한 아웃페인팅 과제에 대한 실험을 수행하여 과제 역산의 효과를 입증하였다. 실험은 Flickr-Scenery 데이터셋(Cheng et al., 2022)을 이용하여 수행되었으며, 그 결과는 표 3의 안정적인 확산과 비교되었으며, 그 결과는 SEELE에 사용된 작업 역산의 우수성을 나타낸다. 또한, 우리는 그림 17의 질적 평가를 위한 시각적 예를 제공한다.\n' +
      '\n' +
      '다른 하위 태스크에 대한 반대 태스크 프롬프트 사용의### 제거\n' +
      '\n' +
      '소개된 태스크별 프롬프트는 서로 다른 생성 방향을 안내합니다. 따라서 서로 다른 하위 태스크에 대해 반대 태스크 프롬프트를 사용하면 반대 방향이 확산 모델로 이어져 불합리한 결과를 생성할 수 있다. 이를 보이기 위해 그림 10에서 서로 다른 태스크 프롬프트를 사용하는 것에 대한 정성적인 비교를 수행하는데, 분명히 반대 태스크 프롬프트는 해당 하위 태스크의 실패로 이어질 것이다.\n' +
      '\n' +
      '첫 번째 예에서는 불완전 분할 마스크를 사용하여 서로 다른 프롬프트의 서로 다른 동작을 강조 표시합니다. 주변 영역에 의해 균일하게 안내된 구멍을 다시 페인트칠하도록 훈련된 제거 프롬프트는 마스킹된 영역에 정확하게 꽃을 생성한다. 한편, 인접한 피험자를 완성하도록 훈련된 완전 프롬프트는 마스크를 파리의 일부로 식별하여 마스킹된 영역에서 파리를 생성하고 꽃의 영향을 줄이려고 한다. 두 번째 예에서는 새 머리를 완성하고 싶을 때입니다. 제거 프롬프트는 단순히 주변에 의해 안내되는 물을 생성하는 반면, 완전 프롬프트는 새의 일부로서 마스크의 의미를 정확하게 캡처하고 새 머리를 생성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Methods & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & FID\\(\\downarrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline Co-Mod (Zhao et al., 2021) & 21.091 & 0.843 & 30.041 & 0.1664 \\\\ MAT (Li et al., 2022) & 20.680 & 0.838 & 23.439 & 0.1650 \\\\ SD (no prompt) & 20.353 & 0.839 & 29.632 & 0.1603 \\\\ SD (“background”) & 20.591 & 0.844 & 29.313 & 0.1564 \\\\ \\hline SEELE & **21.982** & **0.869** & **24.401** & **0.1295** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 장소2에 대한 인페인팅에 대한 정량적 결과(Zhou et al., 2017)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Methods & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & FID\\(\\downarrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline SD (no prompt) & 14.476 & 0.693 & 53.523 & 0.3475 \\\\ SD (“background”) & 14.601 & 0.696 & 46.582 & 0.3429 \\\\ SEELE & **15.989** & **0.731** & **29.056** & **0.3131** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Flickr-Scenery에 대한 아웃페인팅에 대한 정량적 결과(Cheng et al., 2022).\n' +
      '\n' +
      '### 국소조화 제거\n' +
      '\n' +
      '로컬 조화 하위 작업을 해결하기 위해 LoRA 매개변수와 함께 조화 프롬프트를 학습한다. 각 모듈의 효과를 보이기 위해 그림 11에서 정성적 절제 연구를 수행하는데, LoRA 파라미터를 비활성화하면 안정적인 확산 모델을 위해 마스킹되지 않은 이미지 조건으로 비조화 이미지를 사용하므로 모델이 큰 수정 없이 이미지를 복사하는 경향이 있다. LoRA 파라미터만을 사용하는 경우, 국부적 조화를 수행하기 위해 무조건 확산 모델처럼 작동하지만, 일반적으로 오버- 또는 언더-조화를 수행한다. 그러한 태도는 어느 정도 효과가 있지만, 학습된 하모니 프롬프트로 향상될 수 있다.\n' +
      '\n' +
      '### SAM Everything Mode의 시각화\n' +
      '\n' +
      '사용자가 텍스트를 입력할 때 SAM에서 모든 모드를 사용하여 주제를 선택하므로, 여기서는 편의상 SAM 모든 모드의 분할 결과를 시각화할 수 있도록 한다. SAM 모든 모드에서 피사체 레벨의 마스크를 추출하기 위해, 생성된 마스크들을 더 큰 마스크를 보존하는 선호도로 필터링하고, 마스킹된 영역의 특징을 CLIP 텍스트 모델과 비교하여 가장 유사한 부분을 식별한다. SAM은 객체-레벨 마스크들을 직접 생성할 수 없기 때문에, 실제로, 사용자 안내에 따른 반복적인 개선이 일반적으로 요구된다.\n' +
      '\n' +
      'Train SEELE에 대한 데이터 세트별 활용의 필요성\n' +
      '\n' +
      'SEELE 모델의 학습은 지상-진실 객체 분할 마스크를 제공하는 COCO와 국부 조화 작업을 위한 쌍을 이루는 이미지를 제공하는 iHarmony4의 두 가지 데이터 세트만 활용했다. 공공 가용성을 위해 선택된 이러한 데이터 세트는 다양한 생성 하위 작업의 다양한 요구 사항을 적절하게 충족한다. 주제 이동과 완성을 모두 포함하는 우리의 훈련 접근 방식은 통일된 과제 역산 기법을 사용한다. 지역적 조화가 마스킹된 영역에 새로운 세부 사항을 도입하지 않는 것에 초점을 맞추고 있다는 점을 감안할 때, 마스킹된 영역의 특성을 통합하기 위해 확산 모델을 수정하여 태스크의 특정 요구와 일치하도록 했다.\n' +
      '\n' +
      '### Integrating LoRA\n' +
      '\n' +
      'LoRA 어댑터가 훈련되면 냉동 안정 확산 모델과 함께 로드한다. LoRA는 원래 레이어와 함께 추가 레이어로 구현됩니다. 예를 들어, 입력 \\(x_{i}\\)과 출력 \\(x_{i+1}\\)이 있는 특정 계층 \\(f\\)을 가정하자. 상기 원래의 안정한 확산은\n' +
      '\n' +
      '도 10: 상이한 서브-작업들에 대해 반대 태스크 프롬프트를 사용하는 것의 제거.\n' +
      '\n' +
      '그림 11: 국소 조화의 절제.\n' +
      '\n' +
      '도 12: SAM everything 모드의 시각화.\n' +
      '\n' +
      '(f(x_{i})\\), LoRA는 \\(x_{i+1}=f(x_{i})+\\text{LoRA}(x_{i})\\)를 수행하도록 훈련되고 \\(\\text{LoRA}(\\cdot)\\)만 학습하면서 \\(f(\\cdot)\\)을 동결한다. 그런 다음 훈련된 모델 \\(x_{i+1}=f(x_{i})+\\text{cLoRA}(x_{i})\\)에 대한 스케일 하이퍼-파라미터를 도입할 수 있다. SEELE이 조작 과정에서 하위 작업을 수행할 때 안정적인 확산의 원래 출력을 보존하기 위해 로라 스케일을 \\(c=0\\)으로 설정한다. 국부조화 과정에서 국부조화를 수행하기 위해 로라 스케일을 \\(c=1\\)으로 설정하였다. 이와 관련하여, 우리는 동일한 안정적인 확산 백본을 사용할 수 있고 상이한 서브-태스크 프롬프트들(및 LoRA 파라미터들)을 사용하여 상이한 서브-태스크들을 수행할 수 있다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'SEELE의 한 가지 중요한 한계는 시스템이 차선책으로 수행할 때 결과를 향상시키기 위해 수동 사용자 개입이 필요하다는 것이다. 예를 들어, 세그먼테이션에 실패한 경우 세그먼트 마스크를 수동으로 수정해야 합니다. 유사하게, 피험자가 폐색될 때, 사용자들은 피험자를 완성하기 위해 잠재적인 영역들의 마스크를 제공해야 한다. 이전 문제는 분할 모델의 개선을 통해 잠재적으로 완화될 수 있다. 그러나 후자의 과제는 개방형 어휘 아모달 마스크 생성의 문제를 해결하기 위한 새로운 모델의 개발이 필요하다(Zhan et al., 2020). 현재 개방형 어휘 아모달 마스크 생성을 지원할 수 있는 기반 모델이 부족하고 이러한 모델을 훈련하기에 적합한 대규모 데이터 세트가 부족하다. 이러한 측면은 향후 연구를 위한 잠재적인 수단으로 간주된다.\n' +
      '\n' +
      '### 웹 사용자 인터페이스(Web-UI)\n' +
      '\n' +
      '이 섹션에서는 사용자가 SEELE를 사용할 때 상호 작용하는 SEELE의 프론트 엔드 사용자 인터페이스(UI)에 대한 개요를 제공한다. 이러한 웹 기반 UI는 Gradio(Abid et al., 2019)를 기반으로 설계되었으며, 도 18에 묘사되어 있다.\n' +
      '\n' +
      '도 13: 크기 \\(1024\\times 1024\\)의 이미지 상의 SEELE.\n' +
      '\n' +
      '그림 14: ReS에서 주제 재배치에 대한 질적 비교.\n' +
      '\n' +
      '그림 15: ReS에서 주제 재배치에 대한 더 많은 질적 비교.\n' +
      '\n' +
      '그림 16: 인페인팅에 대한 질적 비교.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '도 18: SEELE용 Web-UI.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>