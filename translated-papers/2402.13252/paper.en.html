<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Improving Robustness for Joint Optimization of Camera Poses\n' +
      '\n' +
      'and Decomposed Low-Rank Tensorial Radiance Fields\n' +
      '\n' +
      ' Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu\n' +
      '\n' +
      'National Yang Ming Chiao Tung University\n' +
      '\n' +
      'tomas1999.ee06@nycu.edu.tw, walon@cs.nctu.edu.tw, yulunliu@cs.nycu.edu.tw\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization. The source code is available at [https://github.com/Nemo1999/Joint-TensoRF](https://github.com/Nemo1999/Joint-TensoRF).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, neural rendering has become a widely-used method for high-quality novel view synthesis. NeRF as a pioneer work [11] represents a 3D radiance field as an implicit continuous function built upon multilayer perceptrons (MLPs) which is trained with differentiable volume rendering. While achieving excellent synthesis quality, NeRF suffers from training/inference inefficiency due to dense evaluation of the computationally expensive MLPs.\n' +
      '\n' +
      'To this end, voxel-based methods built upon the explicit scene representation of 3D voxel grid [22, 14, 15] are proposed to achieve faster training and provide better rendering quality than the original MLP-based NeRF, hence becoming the more preferred choices for downstream applications.\n' +
      '\n' +
      'Nevertheless, maintaining a dense 3D voxel grid is in turn memory intensive, thus still restricting wider applications of voxel-based methods. Fortunately, TensoRF [14] proposes to tackle such memory-intensive issue of the voxel grid via replacing the dense 3D grid with _decomposed low-rank tensor_. TensoRF achieves a high data compression ratio and low computational cost at the same time while also achieving state-of-the-art performance. Providing a win-win situation on memory usage and computational efficiency, the decomposed low-rank tensor architecture has been widely adopted in many recent works [23, 13, 14, 15, 16, 17, 18, 19].\n' +
      '\n' +
      'On the other hand, the effectiveness of NeRF (and most of the aforementioned works) hinges on precise camera poses of input images, which are often calculated using Structure-from-Motion (SfM) algorithms like COLMAP [12]. While some works [22, 13, 14] aim to bypass the slow and occasionally inaccurate COLMAP process by optimizing camera pose and scene representation jointly on the original MLP-based NeRF, their success is often tied to the spectral bias [20] of the MLP architecture which ensures the smoothness of 3D radiance field early in training. Voxel-based methods, however, lack such properties and can overemphasize sharp edges, making naive joint optimization problematic as getting trapped in local optima (Fig. 2 (a)).\n' +
      '\n' +
      'In this work, we present simple yet effective methods for refining the camera pose and the 3D scene using decomposed low-rank tensors (cf. Fig. 1). We identify that\n' +
      '\n' +
      'Figure 1: **Robust joint pose refinement on decomposed tensor. Our method enables joint optimization of camera poses and decomposed voxel representation by applying efficient _separable component-wise convolution_ of Gaussian filters on 3D tensor volume and 2D supervision images.**controlling the frequency spectrum is vital for pose alignment, while directly realizing such control in a dense 3D grid could be nontrivial/challenging as well as computationally demanding. To this end, we introduce an efficient 3D filtering method using _component-wise separable convolution_ for enabling the spectral control, which is more efficient than the traditionally well-known trick of _separable convolution kernel_ as we additionally utilize the separability of the input signal. To further ensure stability in the optimization process, we propose several techniques, including _smoothed 2D supervision_, _randomly scaled kernel paramter_, and the _edge-guided loss mask_. These techniques are experimentally proven crucial for successful pose refinement in our ablation studies. In results, our proposed method requires only 50k training iterations, where all the previous methods typically needs 200k iterations (e.g. the overall training time is reduced to 25%, compared to previous MLP-based methods). The main reason behind this advantage is not only based on property of voxel-based architecture, but also relies on our carefully designed efficient spectral filtering algorithm that requires only single reusable voxel grid (please refer to Sec. 4.3). Moreover, our method performs favorably against state-of-the-art methods on novel view synthesis. Our contributions are three-fold:\n' +
      '\n' +
      '* With 1D pilot study, we provide insights into the impact of spectral property of 3D scene on the convergence of joint optimization beyond the coarse-to-fine heuristic discussed in prior research, and propose a learning strategy built upon specially designed efficient component-wise convolution algorithm.\n' +
      '* To enhance the robustness of our joint optimization, we introduce techniques of smoothed 2D supervision, scaled kernel parameters, and the edge-guided loss mask.\n' +
      '* Training time drops by 25% versus existing MLP-based methods, with requiring only 50k iterations against 200k of previous methods. Results show state-of-the-art performance in novel view synthesis with unknown pose.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Accelerating Neural Rendering.** As the seminal work of neural rendering, NeRF adopts MLPs to construct the implicit representation of the 3D scene, providing high-quality view synthesis but having a time-consuming training process due to the computational demands of MLPs. For addressing such issue, different variants of NeRF are proposed to use custom spatial data structures where the scene information is distributed only locally thus aiding faster training and rendering, in which those spatial data structures include _point cloud_[23, 24], _space partitioning tree_[25, 26], _triangular mesh_[10, 27], and _voxel grid_[26, 27, 28, 15]. Among these variants, the voxel grid has become more popular due to its easy implementation and quality reconstruction. However, as scene dimensions grow, the memory usage of the voxel grid becomes inefficient. To address this, [26] recommends compressing the grid via hash encoding, while [10, 28] suggest adopting tensor decomposition for 3D feature compression, in which our method is mainly based on [10] but can be adaptable to other tensor decomposition-based voxel structures like K-Planes [11].\n' +
      '\n' +
      '**Joint Pose Estimation on MLP-based NeRFs.**[25] is one of the first NeRF-based attempts to tackle the joint problem of estimating camera poses and learning 3D scene representation by directly adjusting camera pose using gradient propagation on neural radiance fields. The robustness of such joint optimization is further enhanced by [12, 26], where they propose various methods to smooth the pose gradient derived from the underlying MLP. [10] further increases the noise tolerance by a specially designed local-global joint alignment approach. Our method also tackles joint problems but is specifically designed for the voxel-based NeRF built upon the decomposed low-rank tensor architecture.\n' +
      '\n' +
      '**Pose Estimation on Decomposed Low-rank Tensors.** There do exist works that optimize camera pose on decomposed low-rank tensor [13, 28] but require rich additional geometry clues (e.g., depth map and optical flow). To our best knowledge, we are the first attempt to jointly optimize the camera pose and the _decomposed low-rank tensor_ using only 2D image supervision.\n' +
      '\n' +
      '**Pose Estimation on Multi-Resolution Hash Encoding.** Aside from decomposed low-rank tensor, _multi-resolution hash encoding_ is another compressed voxel-based architecture proposed by [24]. Along with such a choice of architecture, recently [1] has proposed to address the joint optimization of camera pose and multi-resolution hash encoding. They suggest a new interpolation scheme that provides smooth gradients hence preventing gradient fluctuation in the hash volume, along with\n' +
      '\n' +
      'Figure 2: **Comparison of naive joint pose optimization and our proposed method on voxel-based NeRFs.** (a) Naively applying joint optimization on voxel-based NeRFs leads to dramatic failure as premature high-frequency signals in the voxel volume would curse the camera poses to stuck in local minima. (b) We propose a computationally effective manner to directly control the spectrum of the radiance field by performing _separable component-wise convolution_ of Gaussian filters on the decomposed tensor. The proposed training scheme allows the joint optimization to converge successfully to a better solution.\n' +
      '\n' +
      'a curriculum learning scheme that controls the learning rate of the hash table at each resolution level. Although achieving impressive results on joint optimization, the effectiveness of their method is limited to multi-resolution hash encoding and is not applicable to _decomposed low-rank tensor_, While our proposed separable component-wise 3D convolution (and randomly scaled kernel) is specifically designed for _decomposed low-rank tensor_ and not directly applicable to _multi-resolution hash encoding_, in which these two representations have their respective pros and cons.\n' +
      '\n' +
      '## 3 Our Proposed Method\n' +
      '\n' +
      '### Joint Refinement of 3D Scenes and Poses\n' +
      '\n' +
      '**Volume Rendering for Radiance Field Reconstruction.** Based on the setting of neural volume rendering in NeRF, the radiance fields respective for geometry and appearance for a 3D scene are represented via two functions (implemented by MLPs): \\(F_{\\sigma}:\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{1}\\) and \\(F_{c}:\\mathbb{R}^{6}\\rightarrow\\mathbb{R}^{3}\\), where \\(F_{\\sigma}\\) returns the volume density of an input 3D coordinate, while \\(F_{c}\\) outputs the color at an input 3D coordinate given a 3D viewing direction. For rendering a pixel on 2D coordinate \\(u\\) with its homogeneous form \\(\\bar{u}=[u;1]^{\\top}\\), we first sample a sequence of \\(N\\) 3D-coordinates \\(\\{s_{n}\\}_{n=1\\cdots N}\\) along the camera ray defined by the camera center \\(\\vec{c}\\in\\mathbb{R}^{3}\\) and the ray direction \\(\\vec{d_{u}}=K^{-1}\\bar{u}\\),\n' +
      '\n' +
      '\\[\\{s_{n}\\}_{n=1\\cdots N}=s(\\vec{c},\\vec{d_{u}})=\\{\\vec{c}+t_{n}\\cdot\\vec{d_{u} }\\}_{n=1\\cdots N}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(K\\) is the intrinsic camera matrix and \\(\\{t_{n}\\}_{n=1\\cdots N}\\) are \\(N\\) samples equidistantly distributed along the depth axis in between the near and far planes of the view frustum. The resultant color of the pixel is obtained by integrating through the density field \\(F_{\\sigma}\\) and color field \\(F_{c}\\) using the volume rendering equation [13, 12], where we denote the _discretized volume rendering integral_ by a function \\(\\mathbf{V}\\):\n' +
      '\n' +
      '\\[\\mathbf{V}(F_{\\sigma},F_{c},s(\\vec{c},\\vec{d_{u}}))=\\sum_{s_{n}\\in s(\\vec{c}, \\vec{d})}T_{n}\\cdot\\alpha_{n}\\cdot\\mathbf{C}_{n}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(T_{n}=exp(-\\sum_{j=1}^{n}\\delta_{j}F_{\\sigma}(s_{j}))\\) represents accumulated transmittance prior to \\(s_{n}\\), \\(\\alpha_{n}=1-exp(-\\delta_{n}F_{\\sigma}(s_{n}))\\) represents the opacity of sample \\(s_{n}\\), and \\(\\mathbf{C}_{n}=F_{c}(s_{n},\\ \\vec{d_{u}}\\ )\\) represents the color of sample \\(s_{n}\\), and \\(\\delta_{j}=\\|s_{j}-s_{j-1}\\|\\) is the euclidean distance between two adjacent samples.\n' +
      '\n' +
      'In the typical setting of NeRF, given a set of \\(L\\) 2D-images \\(\\mathbf{I}=\\left\\{I_{1},\\cdots,I_{L}\\right\\}\\) with their corresponding camera poses \\(\\mathbf{P}=\\left\\{P_{1},\\cdots,P_{L}\\right\\}\\in\\mathfrak{se}(3)\\) Lie algebra (parametrizing rigid 3D transformation as \\(\\mathfrak{se}(3)\\) is a very common technique in robotics, here we follow the usage of [10]) as input, we aim to reconstruct the 3D scene represented by \\(F_{\\sigma}^{*}\\) and \\(F_{c}^{*}\\), via minimizing the loss \\(\\mathcal{L}_{\\text{rec}}\\) of 2D photometric reconstruction with the gradient-based optimization algorithm, in which\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{rec}}(F_{\\sigma},F_{c})=\\sum_{i=1}^{L}\\sum_{u\\in\\mathbf{U} }\\|\\mathbf{V}(F_{\\sigma},F_{c},\\mathcal{W}_{\\text{id}}(P_{i},s(\\vec{0},\\vec{ d_{u}})))-I_{iu}\\|, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathbf{U}\\) is the set of all possible 2D coordinates in the input images, \\(I_{iu}\\in\\mathbb{R}^{3}\\) is the RGB color of pixel location \\(u\\) on training image \\(I_{i}\\), warping function \\(\\mathcal{W}_{\\text{3d}}(P,\\_):\\mathbb{R}^{3}\\rightarrow\\mathbb{R}^{3}\\) performs rigid 3D transformation parameterized by \\(P\\in\\mathfrak{se}(3)\\) Lie algebra, and \\(\\mathcal{W}_{\\text{3d}}(P,s(\\vec{0},\\vec{d_{u}}))\\) maps each sample 3D coordinate in canonical ray (\\(\\vec{c}=\\vec{0}\\)) into a 3D sample coordinate of camera ray with pose \\(P\\). Note that this is an ill-posed reconstruction problem that suffers from shape-radiance ambiguity [13].\n' +
      '\n' +
      '**3D Joint Optimization.** When it comes to jointly estimating camera poses (where the camera poses \\(\\mathbf{P}\\) are also unknown) and learning scene representation [10, 11, 12, 13, 14], the problem is even more ill-defined with the objective now being extended from Eq. 3 and defined as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{joint}}(F_{\\sigma},F_{c},\\mathbf{P})=\\sum_{i=1}^{L}\\sum_{u\\in \\mathbf{U}}\\|\\mathbf{V}(F_{\\sigma},F_{c},\\mathcal{W}_{\\text{3d}}(P_{i},s( \\vec{0},\\vec{d_{u}})))-I_{iu}\\|. \\tag{4}\\]\n' +
      '\n' +
      'Such joint optimization is highly influenced by the structural bias of the underlying representation of \\(\\{F_{\\sigma},F_{c}\\}\\), which we will conduct a pilot study with a simpler 1D case in Sec. 3.2.\n' +
      '\n' +
      '### Gaussian Filter on 1D Signal Alignment\n' +
      '\n' +
      'Here we aim to analyze the effect of the signal spectrum (spectrum of \\(F_{c}\\), \\(F_{\\sigma}\\), and \\(\\mathbf{I}\\) in Eq. 4) on the joint optimization process. We begin by reducing 3D joint optimization of camera pose and scene reconstruction into a simpler 1D counterpart of signal alignment.\n' +
      '\n' +
      '**1D Signal Alignment.** Let us consider a target ground truth 1D signal \\(f_{GT}\\) (assuming the signal to be continuous, bounded, and have finite support), which we aim to reconstruct and align with. We are given randomly translated versions \\(f_{1}\\), \\(f_{2}\\) of the ground truth signal \\(f_{GT}\\), where \\(f_{1}=\\mathcal{W}_{\\text{1d}}(f_{GT},p_{1}),f_{2}=\\mathcal{W}_{\\text{1d}}(f_{GT},p_{2})\\) with having \\(\\mathcal{W}_{\\text{1d}}\\) a signal translation operation defined as \\(\\mathcal{W}_{\\text{1d}}(f,p)(x)=f(x-p)\\), and \\(p_{1},p_{2}\\) are the translation values.\n' +
      '\n' +
      'Although the reconstruction is trivial in such a 1D setting, in order to mimic the case of 3D joint optimization, we attempt to estimate a signal \\(g\\) as well as the translation values \\(q_{1}\\) and \\(q_{2}\\) via adopting the iterative gradient-based optimization on the reconstruction loss.\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{\\text{1d}}(g,q_{1},q_{2})& =\\sum_{i\\in[1,2]}\\int\\|\\mathcal{W}_{\\text{ld}}(g,q_{i})(x)-f_{i}(x)\\|^{2}dx \\\\ &=\\sum_{i\\in[1,2]}\\int\\|g(x)-f_{GT}(x-p_{i}+q_{i})\\|^{2}dx.\\end{split} \\tag{5}\\]\n' +
      '\n' +
      'Note that Eq. 5 and Eq. 4 are analogous in terms of their structure/formulation, where the difference only lies in the dimensionality. And \\(\\mathcal{L}_{\\text{1d}}\\) achieves the optimum whenever \\(q_{1}-q_{2}=p_{1}-p_{2}\\) and \\(g\\) = \\(\\mathcal{W}_{\\text{1d}}(f_{GT},p_{1}-q_{1})\\). Please check Figure 3(a) for a simple visual representation of Equation 5, where \\(f_{1}\\) and \\(f_{2}\\) are connected to \\(g\\) by the reconstruction loss \\(\\mathcal{L}_{\\text{1d}}\\) (i.e blue arrows), whose gradients are used to update \\(g\\) and the translation values \\(\\{q_{1},q_{1}\\}\\).\n' +
      '\n' +
      '**Connection between 1D Signal Alignment and 3D Joint Optimization.** The formulation of 1D signal alignment effectively simulates the "local phenomenon" of joint camera pose alignment and 3D scene reconstruction on a 2D cross-section: As shown in Figure 3(c), where we consider two neighboring camera poses as well as a cross-section in the 3D space passing through both camera planes and intersecting with each camera plane on a projected straight line, the RGB color values on such projected lines correspond to the 1D shifted ground truth signals \\(f_{1}\\), \\(f_{2}\\) in Equation 5, and the value of the radiance field on the cross-section corresponds to reconstructed signal \\(g\\) in Equation 5. Similar to the loss \\(\\mathcal{L}_{\\text{1d}}\\) in Equation 5, the projected lines on the camera planes and the corresponding cross-section in the 3D radiance field are connected by the volume rendering function \\(V\\) and reconstruction loss \\(\\mathcal{L}_{\\text{joint}}\\) in Equation 4. As a result, the complete 3D joint optimization can be intuitively viewed as simultaneously performing many 1D signal analyses on the superposition of all possible combinations of camera poses and cross-sections.\n' +
      '\n' +
      '**Spectrum Analysis and Effect of Gaussian Filtering on 1D Signal Alignment.** First we transform the problem into a simpler form with a assumption that is reflected by the fast convergent property of voxel grids (cf. our supplement for detailed derivation of the theorem):\n' +
      '\n' +
      '**Theorem 1**: _If we assume rapid convergence of signal \\(g\\) (which means \\(g\\) achieves local optima \\(g^{*}\\) w.r.t current \\(q_{1},q_{2}\\) whenever we update \\(q_{1},q_{2}\\).), we find that the problem in Eq.5 is equivalent to pure alignment between two ground-truth signals, that is_\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{\\text{1d}}(g,q_{1},q_{2})& =\\mathcal{L}_{\\text{1d}}(g^{*},q_{1},q_{2})\\\\ &=\\mathcal{L}_{\\text{1d}}(u)=\\int\\lVert f_{GT}(x)-f_{GT}(x+u) \\rVert^{2}dx,\\end{split} \\tag{6}\\]\n' +
      '\n' +
      '_where \\(u=(p_{1}-p_{2})-(q_{1}-q_{2})\\) is the shift between two ground truth signals, which has an initial value of \\(p_{1}-p_{2}\\)_\n' +
      '\n' +
      'We aim for \\(u\\) to reach \\(0\\) with gradient-based optimization.\n' +
      '\n' +
      'Next, by analyzing the relationship between \\(f_{GT}\\) and the optimization gradient \\(\\frac{d}{du}\\mathcal{L}_{\\text{1d}}\\) in terms of their spectral properties, we get the following result (cf. our supplement for detailed derivation of the theorem):\n' +
      '\n' +
      '**Theorem 2**: \\[\\frac{d}{du}\\mathcal{L}_{\\text{1d}}=\\int\\lVert\\mathfrak{F}[f_{GT}]\\rVert^{2} \\cdot H(u,k)\\;dk,\\] (7)\n' +
      '\n' +
      '_where \\(H(u,k)=4\\pi k\\;sin(2\\pi ku)\\), \\(\\mathfrak{F}[f_{GT}]\\) is Fourier transform of \\(f_{GT}\\), and \\(k\\) is the wavenumber in frequency domain._ Particularly, we are interested in the sign of \\(\\frac{d}{du}\\mathcal{L}_{\\text{1d}}\\) which determines the direction of our iterative optimization. We plot the value of \\(H(u,k)\\) in Fig. 3(b)(_Top_), where we can observe that the sign of \\(H\\) is well-behaved when the magnitude of \\(k\\) is small (here well-behaving means the direction of the gradient is able to let \\(u\\) descend to \\(0\\), i.e., being positive when \\(u>0\\) and negative when \\(u<0\\)). However, when \\(k\\) increases, the sign of \\(H\\) quickly begins to alternate, and the magnitude increases, which causes the gradient to be large and noisy. Hence high-frequency signals with a spreading spectrum can easily lead the optimization process to get stuck in the local optima.\n' +
      '\n' +
      'To this end, we demonstrate that applying a Gaussian filter on the signal \\(f_{GT}\\) effectively mitigates the sign-alternating issue of the original \\(H\\) function. Specifically, we show that filtering the input signal is equivalent to modulating \\(H\\) by a Gaussian window (cf. our supplement for derivation):\n' +
      '\n' +
      '**Theorem 3**: _Let \\(\\tilde{\\mathcal{L}}_{\\text{1d}}\\) denotes \\(\\mathcal{L}_{\\text{1d}}\\) calculated with Gaussian convoluted signal \\(\\mathcal{N}*f_{GT}\\), and \\(\\mathfrak{F}[\\mathcal{N}]\\) denotes the Fourier transform of the Gaussian kernel \\(\\mathcal{N}\\), then we have_\n' +
      '\n' +
      '\\[\\frac{d}{du}\\tilde{\\mathcal{L}}_{\\text{1d}}=\\int\\lVert\\mathfrak{F}[f_{GT}] \\rVert^{2}\\cdot\\tilde{H}(u,k)\\;dk, \\tag{8}\\]\n' +
      '\n' +
      '_where \\(\\tilde{H}(u,k)=\\parallel\\mathfrak{F}[\\mathcal{N}]\\parallel^{2}\\cdot H(u,k)\\)._\n' +
      '\n' +
      'In Fig. 3(b)(_Bottom_), we plot the modulated \\(\\tilde{H}(u,k)\\), with observing that the misbehave region is suppressed (note that we set the variance of \\(\\mathcal{N}\\) to \\(4\\) here). The gradient descent will likely converge to \\(u=0\\) once the initial magnitude of \\(u\\) is less than \\(6.0\\). The region where \\(\\frac{d}{du}\\tilde{\\mathcal{L}}_{\\text{1d}}\\) does well- behave is _quasi-convex_ and is guaranteed to converge to global optima given suitable learning rate that prevents us from getting stuck on saddle points. Our analysis agrees with the motivation behind the coarse-to-fine training schedule of (Lin et al., 2021) and (Hee et al., 2023). Specifically, observing that the well-behaved region in \\(H(u,k)\\) grows wider as \\(u\\) approaches \\(0\\) (cf. Fig. 3(b)(_Top_)), which means that we can loosen the filtering strength of Gaussian kernel as \\(u\\) approaches \\(0\\), leading to larger and more accurate gradient.\n' +
      '\n' +
      '### 2D Planar Image Alignment\n' +
      '\n' +
      'In addition to the 3D joint optimization problem, previous works (Lin et al., 2021; Chng et al., 2022) also consider a\n' +
      '\n' +
      'Figure 3: **Spectrum analysis and effect of Gaussian filtering on 1D signal alignment.** (a) 1D signal alignment comparison: noisy signals can get trapped in local optima without Gaussian filtering. (b)(_Top_) Visualization of \\(H(u,k)\\) in Eq. 7, which shows alternating signs as \\(k\\) departs from \\(0\\), causing misdirection in gradient-based optimization if there has too much high-frequency energy in the signal. (b)(_Bottom_) Visualization of \\(\\tilde{H}(u,k)\\) in Eq. 8, which is the modulated version of \\(H(u,k)\\) with the help of Gaussian filter \\(\\mathcal{N}\\). (c) 1D alignment relates to 3D joint optimization in Eq. 4, where effective pose refinement stems from the 1D alignment in specific cross-sections, with the red lines in 3D scene correlating to horizontal shifts (blue arrows) and rotations (green arrows).\n' +
      '\n' +
      '2D image patches alignment task as a simpler example of joint optimization, in which there are \\(L_{24}\\) overlapping image patches \\(\\mathbf{I}_{\\text{2d}}=\\{I_{1},\\cdots,I_{L_{\\text{2d}}}\\}\\) cropped from a single ground truth image \\(I_{gt}\\) before being transformed by 2D homography. The homography transforms are parameterized by \\(\\mathbf{P}_{\\text{2d}}=\\{P_{1},\\cdots,P_{L_{\\text{2d}}}\\}\\in\\mathfrak{sl}(3)\\) and initialized as \\(\\vec{0}\\) (here we also follow from [11] the usage Lie algebra to parameterize 2D homography transform). Analogously to Equation 4, our objective is to jointly optimize the 2D image content \\(F_{\\text{2d}}:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{2}\\) and per-patch homography warps \\(\\mathbf{P}_{\\text{2d}}\\) by the reconstruction loss. Joint optimization can be formulated as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{2d}}(F_{\\text{2d}},\\mathbf{P}_{\\text{2d}})=\\sum_{i=1}^{L} \\sum_{u\\in\\text{U}_{\\text{2d}}}\\|F_{\\text{2d}}(\\mathcal{W}_{\\text{2d}}(P_{i}, u))-I_{iu}\\|^{2}, \\tag{9}\\]\n' +
      '\n' +
      'where \\(\\mathbf{U}_{\\text{2d}}\\) is the set of all possible 2D coordinates in the image patches, \\(I_{iu}\\) is the color of pixel at location \\(u\\) on input image patch \\(I_{i}\\), warp function \\(\\mathcal{W}_{\\text{2d}}(P_{i},.):\\mathbb{R}^{2}\\rightarrow\\mathbb{R}^{2}\\) performs 2D homography transformation parameterized by \\(P_{i}\\in\\mathfrak{sl}(3)\\) Lie algebra, and \\(\\mathcal{W}_{\\text{2d}}(P_{i},u)\\) maps 2D coordinate \\(u\\) on \\(I_{gt}\\) into a transformed 2D coordinate on patch \\(I_{i}\\). Notice the strong structural correspondence among Eq. 5 (1D alignment), Eq. 9 (2D alignment), and Eq. 4 (3D alignment), the three problems share similar computational property.\n' +
      '\n' +
      'We parameterize \\(F_{\\text{2d}}\\) by a 2D decomposed low-rank tensor \\(\\mathcal{T}_{\\text{2d}}\\in\\mathbb{R}^{h\\times w}\\), where \\(w,h\\) are the dimensions of the image. Motivated by our analysis in Section 3.2, we filter \\(\\mathcal{T}_{\\text{2d}}\\) with 2D Gaussian kernel to avoid overfitting.\n' +
      '\n' +
      '\\[F_{\\text{2d}}(\\mathbf{x})=(\\mathcal{N}_{\\text{2d}}*_{\\text{2d}}\\mathcal{T}_{ \\text{2d}})(\\mathbf{x})=(\\mathcal{N}_{\\text{2d}}*_{\\text{2d}}(\\sum_{r=1}^{R} \\mathbf{v}_{r}^{X}\\otimes\\mathbf{v}_{r}^{Y}))(\\mathbf{x}), \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\mathbf{x}\\in\\mathbb{R}^{2}\\) is 2D pixel coordinates, \\(\\mathcal{N}_{\\text{2d}}\\) is 2D gaussian kernel, \\(*_{\\text{2d}}\\) is the convolution operator, and \\(\\otimes\\) denotes outer product between the 1D vector components \\(\\mathbf{v}_{r}^{X}\\in\\mathbb{R}^{w},\\mathbf{v}_{r}^{Y}\\in\\mathbb{R}^{h}\\). "\\(\\mathbf{(x)}\\)" at the end of the expressions means bilinearly interpolating the preceding discrete 2D volume with continuous coordinate \\(\\mathbf{x}\\). Our method outperforms the naive tensor method and previous methods [11, 12], experiment results are shown at Sec. 4.1.\n' +
      '\n' +
      'The width of Gaussian kernel \\(\\mathcal{N}_{\\text{2d}}\\) is controlled by an exponential coarse-to-fine training schedule that changes continuously (cf. our supplement for details of such kernel schedule). In order to support continuous changing width on a discrete Gaussian kernel, the kernel is generated by the following rule:\n' +
      '\n' +
      '\\[\\mathcal{N}_{\\text{1d}}(\\sigma) =\\begin{cases}\\bigoplus_{x=-L_{\\mathcal{N}}/2}^{L_{\\mathcal{N}}/2} min(1,\\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{\\pi^{2}}{2\\sigma^{2}}})&\\text{if }\\sigma>0.0001\\\\ \\bigoplus_{x=-L_{\\mathcal{N}}/2}^{L_{\\mathcal{N}}/2}\\delta[x]&\\text{ otherwise},\\end{cases} \\tag{11}\\] \\[\\mathcal{N}_{\\text{2d}}(\\sigma) =\\mathcal{N}_{\\text{1d}}(\\sigma)\\otimes\\mathcal{N}_{\\text{1d}}( \\sigma),\\]\n' +
      '\n' +
      'where \\(L_{\\mathcal{N}}\\) is the size of the discrete kernel, 1D kernel \\(\\mathcal{N}_{\\text{1d}}(\\sigma)\\in\\mathbb{R}^{L_{\\mathcal{N}}}\\) is discretely sampled from continuous Gaussian distribution and clamped to a max value of \\(1.0\\) before being concatenated into a vector by \\(\\oplus\\) operator. To avoid numerical instability, when \\(\\sigma<0.001\\), we assign \\(\\mathcal{N}_{\\text{1d}}(\\sigma)\\) to be discrete impulse function \\(\\delta\\). 2D kernel \\(\\mathcal{N}_{\\text{2d}}(\\sigma)\\in\\mathbb{R}^{L_{\\mathcal{N}}\\times L_{\\mathcal{N}}}\\) is generated by outer product of two 1D kernels.\n' +
      '\n' +
      '### Decomposed Low-Rank Tensor\n' +
      '\n' +
      'This section describes the decomposed low-rank tensor proposed by TensoRF [11] which is the scene representation that our proposed method is built upon. While there are two different types of tensor decomposition considered in [11]: CP-decomposition and VM-decomposition, in our discussion we mainly focus on _VM-decomposition_, although our method is also naturally applicable to CP-decomposition.\n' +
      '\n' +
      'To represent the 3D density field \\(F_{\\sigma}\\), we store the information in a 3D tensor \\(\\mathcal{T}_{\\sigma}\\in\\mathbb{R}^{I\\times J\\times K}\\), in which now \\(F_{\\sigma}\\) is defined simply as component-wise interpolation of \\(\\mathcal{T}_{\\sigma}\\).\n' +
      '\n' +
      '\\[\\mathcal{T}_{\\sigma}=\\sum_{r=1}^{\\mathbf{R}}\\mathbf{v}_{\\sigma,r}^{X}\\otimes \\mathbf{M}_{\\sigma,r}^{Y,Z}+\\mathbf{v}_{\\sigma,r}^{Y}\\otimes\\mathbf{M}_{ \\sigma,r}^{X,Z}+\\mathbf{v}_{\\sigma,r}^{Z}\\otimes\\mathbf{M}_{\\sigma,r}^{X,Y}, \\tag{12}\\]\n' +
      '\n' +
      'where \\(\\mathbf{R}\\) is the number of components in the decomposition, \\((\\mathbf{v}_{r}^{X},\\mathbf{v}_{r}^{Y},\\mathbf{v}_{r}^{Z})\\in(\\mathbb{R}^{I}, \\mathbb{R}^{J},\\mathbb{R}^{K})\\) are 1D vector-components for axes \\((X,Y,Z)\\) respectively, \\((\\mathbf{M}_{r}^{Y,Z},\\mathbf{M}_{r}^{X,Z},\\mathbf{M}_{r}^{X,Y})\\in(\\mathbb{R} ^{J\\times K},\\mathbb{R}^{I\\times K},\\mathbb{R}^{I\\times J})\\) are 2D matrix-components for axes \\((Y\\)-\\(X\\), \\(X\\)-\\(Z\\), \\(X\\)-\\(Y\\)) repectively, operator \\(\\otimes\\) denotes the outer product between vector and matrix.\n' +
      '\n' +
      'To represent the 3D color field \\(F_{c}\\), the information \\(\\mathcal{T}_{c}(\\mathbf{x})\\in\\mathbb{R}^{G}\\) queried from 3D feature tensor \\(\\mathcal{T}_{c}\\in\\mathbb{R}^{I\\times J\\times K\\times G}\\) is decoded by a small MLP \\(S\\) into RGB color value (\\(G\\) is the input feature dimension of \\(S\\)). The implementation can be formulated as\n' +
      '\n' +
      '\\[F_{c}(\\mathbf{x},\\vec{d})=\\mathbf{S}(\\mathcal{T}_{c}(\\mathbf{x}),\\vec{d}) \\tag{13}\\]\n' +
      '\n' +
      '\\(\\mathcal{T}_{c}=\\sum_{r=1}^{\\mathbf{R}}\\mathbf{v}_{c,r}^{X}\\otimes\\mathbf{M}_{ \\sigma,r}^{Y,Z}\\otimes\\mathbf{b}_{r}^{X}+\\)\n' +
      '\n' +
      '\\(\\mathcal{T}_{c}(\\mathbf{x})\\) denotes the component-wise linear-interpolation of tensor volume \\(\\mathcal{T}_{c}\\) on 3D coordinate \\(\\mathbf{x}\\). \\(\\vec{d}\\)\'s the viewing direction of the current ray. \\(\\mathbf{v}_{c,r}\\) and \\(\\mathbf{M}_{\\sigma,r}\\) have the same shape as their \\(\\mathbf{v}_{\\sigma,r}\\) and \\(\\mathbf{M}_{\\sigma,r}\\) counterparts, \\(\\mathbf{b}_{r}^{X},\\mathbf{b}_{r}^{Y},\\mathbf{b}_{r}^{X}\\in\\mathbb{R}^{G}\\) are feature components to expand the feature axis of \\(\\mathcal{T}_{c}\\).\n' +
      '\n' +
      '### Separable Component-Wise Convolution\n' +
      '\n' +
      'As theoretically analyzed in Sec. 3.2 and empirically shown in Fig. 2(a), naively applying low-rank decomposed tensor (which lacks internal bias that limits the spectrum of learned signal, hence corresponds to the top raw of Fig. 3) to joint camera pose optimization results in suboptimal reconstruction quality and inaccurate poses. Therefore, we propose to limit the spectrum of the radiance field \\(F_{\\sigma}\\) and \\(F_{c}\\) with a coarse-to-fine training schedule.\n' +
      '\n' +
      'If we naively convolve the 3D Gaussian kernel with our 3D volume \\(\\mathcal{T}_{\\sigma}\\), (as in the 2D planar case of Eq. 10), we would have to reconstruct the whole 3D tensor before applying convolution, destroying the space compression advantage of decomposed low-rank tensor, see Eq. 14.\n' +
      '\n' +
      '\\[F_{\\sigma}(x,y,z)=(\\mathcal{N}_{\\text{3d}}*_{\\text{3d}}\\mathcal{T}_{\\sigma})(x,y,z), \\tag{14}\\]\n' +
      '\n' +
      'where \\(*_{\\text{3d}}\\) denotes 3D convolution, \\(\\mathcal{N}_{\\text{3d}}\\) is the 3D Gaussian filter defined by \\(\\mathcal{N}_{\\text{1d}}\\otimes\\)\\(\\mathcal{N}_{\\text{2d}}\\). Under this setting, the time complexity and the space complexity are \\(O(I\\cdot J\\cdot K\\cdot L_{\\mathcal{N}}^{3})\\) and \\(O(I\\cdot J\\cdot K)\\) respectively, where \\(L_{\\mathcal{N}}\\) is the size of 3D Gaussian kernel in each dimension.\n' +
      '\n' +
      'To achieve computationally efficient convolution on the 3D decomposed low-rank tensor volume, we perform our proposed _separable component-wise convolution_, by taking advantage of the following identity (whose correctness will be proven in the supplementary material).\n' +
      '\n' +
      '**Theorem 4**: \\[\\tilde{\\mathcal{T}}_{\\sigma}=\\sum_{r=1}^{\\mathbf{R}}\\tilde{\\mathbf{v}}_{\\sigma, r}^{X}\\otimes\\tilde{\\mathbf{M}}_{\\sigma,r}^{Y,Z}+\\tilde{\\mathbf{v}}_{\\sigma,r}^{ Y}\\otimes\\tilde{\\mathbf{M}}_{\\sigma,r}^{X,Z}+\\tilde{\\mathbf{v}}_{\\sigma,r}^{Z} \\otimes\\tilde{\\mathbf{M}}_{\\sigma,r}^{X,Y},\\] (15)\n' +
      '\n' +
      'where \\(\\tilde{\\mathcal{T}}_{\\sigma}=(\\mathcal{N}_{\\text{3d }}\\ast_{\\text{3d }}\\mathcal{T}_{\\sigma})\\) denotes the 3D Gaussian convoluted tensor volume, \\(\\tilde{\\mathbf{v}}_{\\sigma,r}=(\\mathcal{N}_{\\text{1d }}\\ast_{\\text{1d }}\\tilde{\\mathbf{v}}_{\\sigma,r})\\) denotes the 1D Gaussian convoluted vector component, and \\(\\tilde{\\mathbf{M}}_{\\sigma,r}=(\\mathcal{N}_{\\text{2d }}\\ast_{\\text{2d }}\\tilde{\\mathbf{M}}_{\\sigma,r})\\) denotes the 2D Gaussian convoluted matrix component. In other words, **the 3D convoluted tensor can be expressed as the composition of individually convoluted components**, which allows us to distribute the 3D Gaussian convolution across the individual components of the decomposed low-rank tensor. Similar to Sec. 3.4, the value of the density field is component-wised linearly sampled from the Gaussian convoluted components, i.e., \\(\\tilde{F_{\\sigma}}(\\mathbf{x})=\\tilde{\\mathcal{T}}_{\\sigma}(\\mathbf{x})\\). Similarly, the spectral restricted version of the color field \\(F_{c}\\) can be obtained as\n' +
      '\n' +
      '\\[\\tilde{F_{c}}(\\mathbf{x},\\tilde{d})=\\mathbf{S}(\\tilde{\\mathcal{T} }_{c}(\\mathbf{x}),\\tilde{d}) \\tag{16}\\] \\[\\tilde{\\mathcal{T}}_{c}=\\sum_{r=1}^{\\mathbf{R}}\\tilde{\\mathbf{v} }_{\\sigma,r}^{X}\\otimes\\tilde{\\mathbf{M}}_{c,r}^{Y,Z}\\otimes\\mathbf{b}_{r}^{X}+\\] \\[\\tilde{\\mathbf{v}}_{c,r}^{Y}\\otimes\\tilde{\\mathbf{M}}_{c,r}^{X,Z }\\otimes\\mathbf{b}_{r}^{Y}+\\tilde{\\mathbf{v}}_{\\sigma,r}^{Z}\\otimes\\tilde{ \\mathbf{M}}_{c,r}^{X,Y}\\otimes\\mathbf{b}_{r}^{Z}.\\]\n' +
      '\n' +
      'With _separable component-wise convolution_, the time complexity required is \\(O(I\\cdot J\\cdot L_{\\mathcal{N}}+J\\cdot K\\cdot L_{\\mathcal{N}}+K\\cdot I\\cdot L _{\\mathcal{N}})\\) for computing convoluted components (assuming that we separate 2D Gaussian convolution on matrix components into 1D Gaussian convolutions), and \\(O(\\mathbf{R})\\) for each query sample (same as the original decomposed tensor in [1]), drastically reducing the computation required for filtering 3D radiance fields \\(F_{\\sigma}\\) and \\(F_{c}\\).\n' +
      '\n' +
      'We stress here that our proposed component-wise convolution is **different** from traditional technique of separated kernel convolution in signal processing literature, in the sense that the common separated kernel technique only separates the 3D kernel without utilizing the separability of the input signal itself, and hence requires sequentially performing three 1D convolution operation on 3D volume, the time complexity of traditional technique would be \\(O(I\\cdot J\\cdot K\\cdot L_{\\mathcal{N}})\\), and also requires a 3-dimensional memory with space complexity of \\((I\\cdot J\\cdot K)\\) to store convolution result.\n' +
      '\n' +
      '### Techniques for Increasing Pose Robustness\n' +
      '\n' +
      'Here we summarize our improvements on naive decomposed low-rank tensors that improve joint camera pose optimization and radiance field reconstruction.\n' +
      '\n' +
      '**Coarse-to-Fine 3D schedule.** Using efficient 3D convolution algorithm in Sec. 3.5. During training, we apply a coarse-to-fine schedule on the 3D radiance field \\(\\tilde{F}_{\\sigma},\\tilde{F}_{c}\\) by controlling the kernel parameter (\\(\\sigma\\) of Eq. 11) of the Gaussian kernel, which is exponentially reduced to 0 at 10k iterations and remains 0 afterward (for detailed settings of \\(\\sigma\\), please refer to the supplement).\n' +
      '\n' +
      '**Smoothed 2D Supervision.** Inspired by the analysis in Sec. 3.2, we discovered that blurring the 2D training image with a parallel set of scheduled 2D Gaussian kernels also helps the joint optimization. On the one hand, smoothed supervision images produce smoothed image gradients and stabilize the camera alignment. On the other hand, smoothed training image also helps to restrict the spectrum of the learned 3D scene. The Gaussian schedule for smoothing 2D training images is similar to that of the 3D radiance fields.\n' +
      '\n' +
      '**Randomly Scaled Kernel Parameter and Edge Guided Loss.** From the previous spectral analysis in Sec. 3.2, one may have the impression that a larger kernel leads to stronger modulation, and hence always results in more robust pose registration. However, this is not always true, because the magnitude of \\(H(u,k)\\) decreases linearly as \\(k\\) approaches \\(0\\). Notice that in Fig. 3(b) the magnitude of modulated \\(\\tilde{H}\\) is weaker than that of \\(H\\), which means that \\(\\frac{d}{du}\\tilde{\\mathcal{L}}_{\\text{1d}}\\) weaker than \\(\\frac{d}{du}\\mathcal{L}_{\\text{1d}}\\) and therefore is more easily influenced by noise. In the 3D case, this _weak and noisy gradient problem_ caused by overly aggressive filtering corresponds to the excessive blur effect that destroys important edge signals in the training images, causing pose alignment to fail. See Fig. 4(b) for a visualization of the image blurred by an over-strength kernel, in which the thin edge information is eliminated, causing the camera pose to randomly drift.\n' +
      '\n' +
      'Based on the effect of _weak and noisy gradient problem_, when applying only _coarse-to-fine 3D schedule_ and _smoothed 2D supervision_, we found that it is insufficient to use a single-size kernel on different real-world scene structures (in which the same kernel may be overly aggressive in one scene, but overly gentle in another scene). Therefore, we introduce _randomly scaled kernel_, which randomly scales the kernel by a factor uniformly sampled from \\([0,1]\\). Random scales are sampled independently among 3D Gaussian kernels (for the radiance field) and 2D Gaussian kernels (for training images), allowing combinations of different-sized kernels to guide the joint optimization. See Fig. 4(c) for a visualization of the same input image filtered by a range of randomly sampled kernels. We observe that the training schedule becomes more robust when we alternate between these randomly sampled kernel scales.\n' +
      '\n' +
      'Another way to mitigate the weak and noisy gradient problem is the _edge guided loss_, in which we increase the learning rate by 1.5x (and hence amplify the gradient signal) on the pixels in the edge region, from which the learning signal for pose alignment mainly comes. See visualization in Fig. 4 (d), where we color the edge area that is detected using the Sobel filter [1] on the filtered 2D images in yellow. Edge-guided rendering loss helps the joint optimization focuses more on the edge area of the training images, resulting in more robust pose optimization. Empirically we apply this edge-guided scale alternately on every other training iteration.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:8]\n' +
      '\n' +
      'rough attempts to add a positional encoding schedule into the MLP decoder input to simulate the setting of BARF or replace the decoder with a GARF network. We conduct experiments on four randomly chosen scenes in the LLFF dataset. The results are shown in Tab. 5, which demonstrate the efficacy and pertinency of our proposed method to achieve successful training.\n' +
      '\n' +
      'Using Other Low-Pass Filters.As we would like to have identical filtering strength along all spatial directions, we adopt the Gaussian filter in our method as it is the only kernel that is both circularly symmetric and separable (a well-known property in signal processing). Nevertheless, we experiment with other low-pass filters. We report in Tab. 6 the performance of using the box filter (i.e., a representative low-pass filter) on the LLFF Fortress scene, in which we clearly observe the benefits of using the Gaussian filter.\n' +
      '\n' +
      'Applying _Randomly Scaled Kernel Parameter._**and _Edge Guided Loss_ on Synthetic Scenes.Although the two techniques are originally proposed to improve the robustness of complex real-world scenes, they do not harm the performance of synthetic ones and even slightly boost the pose estimation, as shown in Tab. 7.\n' +
      '\n' +
      'Sensitivity w.r.t. Pose Initialization.We adopt the Chair scenes in the Blender dataset to conduct sensitivity analysis upon pose initialization via varying variance \\(\\sigma\\) of Gaussian noise. The result is shown in Tab. 8, which demonstrates that both BARF and our proposed method show certain robustness against the noisy initialization of camera poses.\n' +
      '\n' +
      '### Time Complexity\n' +
      '\n' +
      'In Fig. 7, we compare with previous methods on average PSNR and training iterations in the _Synthetic NeRF_ dataset. The figure shows two advantages of our method: (1) rapid convergence and (2) high-quality novel view synthesis.\n' +
      '\n' +
      'The early-stage blurry supervision can hinder detailed structure reconstruction later in the optimization, impacting the final result quality. Our method resolves this problem by applying 3D filters with directly controllable kernel parameters, which enables smooth and rapid transition (by continuous exponential kernel schedule) of the 3D content across the spectrum domains, as opposed to previous methods that use indirect methods (e.g., learning rate in [11]), encoding magnitude in [10]) to influence learned 3D scene spectral property. Furthermore, our method is carefully designed to use a single voxel grid, which is trained only once in the coarse-to-fine schedule controlled by our proposed efficient component-wised convolution algorithm, thus leading to faster convergence; in comparison, [11], which also uses voxel-based representation, requires sequential curriculum learning upon multiple voxel grids of different resolutions, resulting in four times more training iterations than ours.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'Our contributions is three fold: 1) _Theoretically_, we provide insights into the impact of 3D scene properties on the convergence of joint optimization beyond the coarse-to-fine heuristic discussed in prior research (e.g., BARF, Heo _et al._2023), thus offering a filtering strategy for improving the joint optimization of camera pose and 3D radiance field. 2) _Algorithmically_, we introduce (and prove the equivalence of) an effective method for applying the pilot study\'s filtering strategy on the decomposed low-rank tensor, notice that the proposed _separable component-wise convolution_ is more efficient than the traditionally well-known trick of _separable convolution kernel_ as we additionally utilize the separability of the input signal. Furthermore, we also propose other techniques such as _randomly-scaled kernel parameter_, _blurred 2D supervision_, and _edge-guided loss mask_ to help our proposed method better perform in complex real-world scenes. 3) Comprehensive evaluations demonstrate our proposed framework\'s state-of-the-art performance and rapid convergence without known poses.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Filter & Rot. \\(\\downarrow\\) & Trans. \\(\\downarrow\\) & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) & LPIPS \\(\\downarrow\\) \\\\ \\hline Box filter & 9.98 & 0.06 & 20.18 & 0.387 & 0.165 \\\\ Gaussian filter & 0.46 & 0.004 & 29.49 & 0.874 & 0.063 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Ablation On Low-Pass Filters.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c} \\hline \\hline  & \\(\\sigma\\) & \\(0.125\\) & \\(0.15\\) & \\(0.175\\) & \\(0.2\\) \\\\ \\hline \\multirow{2}{*}{BARF} & Rotation \\(\\downarrow\\) & 0.094 & 0.068 & 0.100 & 0.108 \\\\  & Translation \\(\\downarrow\\) & 0.004 & 0.004 & 0.005 & 0.005 \\\\ \\hline \\multirow{2}{*}{Ours} & Rotation \\(\\downarrow\\) & 0.07 & 0.062 & 0.072 & 0.066 \\\\  & Translation \\(\\downarrow\\) & 0.003 & 0.003 & 0.003 & 0.002 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **Ablation on Applying _Randomly Scaled Kernel Parameter_ and _Edge Guided Loss_ in Synthetic Scenes**\n' +
      '\n' +
      'Figure 7: **PSNR and training iterations comparison.**\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'This work is supported by National Science and Technology Council (NSTC) 111-2628-E-A49-018-MY4, 112-2221-E-A49-087-MY3, 112-2222-E-A49-004-MY2, and Higher Education Sprout Project of the National Yang Ming Chiao Tung University, as well as the Ministry of Education (MoE), Taiwan. In particular, Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MoE in Taiwan.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su (2022)Tensorf: tensorial radiance fields. In Proceedings of the European Conference on Computer Vision (ECCV), Cited by: SS1, SS2.\n' +
      '* Y. Chen, X. Chen, X. Wang, Q. Zhang, Y. Guo, Y. Shan, and F. Wang (2023)Local-to-global registration for bundle-adjusting neural radiance fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* Z. Chen, T. Funkhouser, P. Hedman, and A. Tagliasacchi (2023)MobileNeRF: exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* S. Chng, S. Ramasinghe, J. Sherrah, and S. Lucey (2022)Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), Cited by: SS1, SS2.\n' +
      '* S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa (2023)K-planes: explicit radiance fields in space. Time, and Appearance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa (2023)K-planes: explicit radiance fields in space. Time, and Appearance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa (2022)Plenoxels: radiance fields without neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* R. Goel, S. Dhawal, S. Saini, and P. J. Narayanan (2022)StyleTRF: stylizing tensorial radiance fields. In Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing, Cited by: SS1, SS2.\n' +
      '* K. Han and W. Xiang (2023)Multiscale tensor decomposition and rendering equation encoding for view synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* P. Hedman, P. P. Srinivasan, B. Mildenhall, J. T. Barron, and P. Debevec (2021)Baking neural radiance fields for real-time view synthesis. Proceedings of the IEEE International Conference on Computer Vision (ICCV), Cited by: SS1, SS2.\n' +
      '* H. Heo, T. Kim, J. Lee, J. Lee, S. Kim, H. J. Kim, and J. Kim (2023)Robust camera pose refinement for multi-resolution hash encoding. In Proceedings of the International Conference on Machine Learning (ICML), Cited by: SS1, SS2.\n' +
      '* T. Hu, X. Xu, R. Chu, and J. Jia (2023)TriVol: point cloud rendering via triple volumes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* J. T. Kajiya and B. P. Von Herzen (1984)Ray tracing volume densities. ACM SIGGRAPH computer graphics. Cited by: SS1, SS2.\n' +
      '* N. Kanopoulos, N. Vasanthavada, and R. L. Baker (1988)Design of an image edge detection filter using the sobel operator. IEEE Journal of solid-state circuits. Cited by: SS1, SS2.\n' +
      '* J. Kulhanek and T. Sattler (2023)Tetra-neRF: representing neural radiance fields using tetrahedra. arXiv preprint arXiv:2304.09987. Cited by: SS1, SS2.\n' +
      '* C. Lin, W. Ma, A. Torralba, and S. Lucey (2021)BARF: bundle-adjusting neural radiance fields. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Cited by: SS1, SS2.\n' +
      '* L. Liu, J. Gu, K. Z. Lin, T. Chua, and C. Theobalt (2020)Neural sparse voxel fields. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1, SS2.\n' +
      '* Y. Liu, C. Gao, A. Meuleman, H. Tseng, A. Saraf, C. Kim, Y. Chuang, J. Kopf, and J. Huang (2023)Robust dynamic radiance fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* A. Meuleman, Y. Liu, C. Gao, J. Huang, C. Kim, M. H. Kim, and J. Kopf (2023)Progressively optimized local radiance fields for robust view synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* B. Mildenhall, P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng (2020)NeRF: representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), Cited by: SS1, SS2.\n' +
      '* T. Muller, A. Evans, C. Schied, and A. Keller (2022)Instant neural graphics primitives with a multiresolution hash encoding. ACM Transactions on Graphics (TOG). Cited by: SS1, SS2.\n' +
      '* J. L. Schonberger and J. Frahm (2016)Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu (2023)Tensor4D: efficient neural 4D decomposition for high-fidelity dynamic reconstruction and rendering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* C. Sun, M. Sun, and H. Chen (2022)Direct voxel grid optimization: super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Cited by: SS1, SS2.\n' +
      '* J. Tang, X. Chen, J. Wang, and G. Zeng (2022)Compressible-composable neRF via rank-residual decomposition. Advances in Neural Information Processing Systems. Cited by: SS1, SS2.\n' +
      '* L. Wang, J. Zhang, X. Liu, F. Zhao, Y. Zhang, Y. Zhang, M. Wu, J. Yu, and L. Xu (2022)Fourier plenoctrees.\n' +
      '\n' +
      'for dynamic radiance field rendering in real-time. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Wang et al. (2021) Wang, Z.; Wu, S.; Xie, W.; Chen, M.; and Prisacariu, V. A. 2021. NeRF---: Neural Radiance Fields Without Known Camera Parameters. _arXiv preprint arXiv:2102.07064_.\n' +
      '* Xu et al. (2022) Xu, Q.; Xu, Z.; Philip, J.; Bi, S.; Shu, Z.; Sunkavalli, K.; and Neumann, U. 2022. Point-nerf: Point-based neural radiance fields. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Xu et al. (2023) Xu, Y.; Wang, L.; Zhao, X.; Zhang, H.; and Liu, Y. 2023. AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-Aware Neural Voxels. In _ACM SIGGRAPH 2023 Conference Proceedings_.\n' +
      '* Yu et al. (2021) Yu, A.; Li, R.; Tancik, M.; Li, H.; Ng, R.; and Kanazawa, A. 2021. PlenOctrees for Real-time Rendering of Neural Radiance Fields. _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_.\n' +
      '* Yuce et al. (2022) Yuce, G.; Ortiz-Jimenez, G.; Besbinar, B.; and Frossard, P. 2022. A structured dictionary perspective on implicit neural representations. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.\n' +
      '* Zhang et al. (2020) Zhang, K.; Riegler, G.; Snavely, N.; and Koltun, V. 2020. Nerf++: Analyzing and improving neural radiance fields. _arXiv preprint arXiv:2010.07492_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>