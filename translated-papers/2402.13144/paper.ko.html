<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '도. (p<0.05). i) 신경망 학습과 확산 모델의 역과정 모두 랜덤 잡음/초기화에서 특정 분포로의 전이로 간주될 수 있다. ii) 고품질 이미지 및 고성능 파라미터는 또한 다중 잡음 추가를 통해 가우시안 분포와 같은 간단한 분포로 열화될 수 있다.\n' +
      '\n' +
      '위의 관찰을 바탕으로 새로운 매개변수 집합을 합성하기 위해 표준 잠재 확산 모델을 사용하는 신경망 확산(**p-diff**, p는 매개변수)이라는 새로운 매개변수 생성 접근법을 소개한다. 그것은 확산 모델이 주어진 무작위 분포를 특정 분포로 변환할 수 있는 능력을 가지고 있다는 사실에 의해 동기가 부여된다. 본 논문에서 제안하는 방법은 오토인코더와 표준 잠재확산모델로 구성되어 성능이 좋은 파라미터들의 분포를 학습한다. 먼저, SGD 최적화기에 의해 트레이닝된 모델들의 파라미터들의 서브세트에 대해, 오토인코더는 이러한 파라미터들에 대한 잠재 표현들을 추출하도록 트레이닝된다. 그런 다음, 우리는 랜덤 노이즈로부터 잠재 표현을 합성하기 위해 표준 잠재 확산 모델을 활용한다. 마지막으로, 합성된 잠재 표현들은 훈련된 오토인코더의 디코더를 통과하여 새로운 고성능 모델 파라미터들을 산출한다.\n' +
      '\n' +
      '제안된 방법은 다음과 같은 특징을 가진다. i) SGD 옵티마이저에 의해 훈련된 모델인 학습 데이터보다 몇 초 이내에 여러 데이터 세트와 아키텍처에 걸쳐 지속적으로 유사하고 향상된 성능을 달성한다. ii) 생성된 모델은 훈련된 모델과 큰 차이가 있으며, 이는 우리의 접근법이 훈련 샘플을 암기하는 대신 새로운 매개변수를 합성할 수 있음을 보여준다. 우리의 연구가 확산 모델의 적용을 다른 영역으로 확장하는 데 새로운 통찰력을 제공할 수 있기를 바란다.\n' +
      '\n' +
      '##2 Nerer Network Diffusion\n' +
      '\n' +
      '### 확산 모델의 예비\n' +
      '\n' +
      '확산 모델은 일반적으로 타임스텝에 의해 인덱싱된 다단계 연쇄에서 정방향 및 역방향 프로세스로 구성된다. 우리는 다음에서 이 두 가지 과정을 소개한다.\n' +
      '\n' +
      '*Forward process.** 샘플 \\(x_{0}\\sim q(x)\\)이 주어지면, 순방향 프로세스는 \\(T\\) 단계에 대해 점진적으로 가우시안 잡음을 추가하고 \\(x_{1},x_{2},\\cdots,x_{T}\\)을 얻는다. 이러한 공정의 제형은 다음과 같이 기재될 수 있으며,\n' +
      '\n' +
      '\\mathcal{N}(x_{t}|x_{t-1})&=\\mathcal{N}(x_{t}; \\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\textbf{I}),\\\\q(x_{1:T}|x_{0}&=\\prod_{t=1}^{T}q(x_{t}|x_{t-1}),\\end{split}\\tag{1}\\textbf{I})\n' +
      '\n' +
      '여기서 \\(q\\) 및 \\(\\mathcal{N}\\)은 순방향 과정을 나타내고 \\(\\beta_{t}\\)에 의해 파라미터화된 가우시안 잡음을 추가하며, **I**는 아이덴티티 매트릭스이다.\n' +
      '\n' +
      '**역방향 프로세스.** 순방향 프로세스와 달리 역방향 프로세스는 \\(x_{t}\\)에서 잡음을 재귀적으로 제거하기 위해 잡음 제거 네트워크를 훈련시키는 것을 목표로 한다. t가 \\(T\\)에서 0으로 감소함에 따라 다중 단계 체인 상에서 뒤로 이동한다. 수학적으로, 역방향 프로세스는 다음과 같이 공식화될 수 있고,\n' +
      '\n' +
      '{split}p_{\\theta}(x_{t-1}|x_{t})&=\\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t)),\\\\p_{\\theta}(x_{0:T}&=p(x_{T})\\prod_{t=1}^{T}p_{\\theta}(x_{t-1}|x_{t}),\\end{split}\\tag{2}\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t)),\\p_{\\theta}(x_{0:T}&=p(x_{T})\\prod_{t=1}^{T}p_{\\theta}(x_{t-1}|x_{t}),\\end{split}\\tag{2}\\mu_{\n' +
      '\n' +
      '여기서 \\(p\\)는 역과정을 나타내며, \\(\\mu_{\\theta}(x_{t},t)\\) 및 \\(\\Sigma_{\\theta}(x_{t},t))\\)는 잡음제거 네트워크 매개변수 \\(\\theta\\)에 의해 추정된 가우시안 평균 및 분산이다. 역과정에서의 잡음 제거 네트워크는 표준 음의 로그 우도에 의해 최적화된다:\n' +
      '\n' +
      '\\[L_{\\mathrm{dm}}=\\mathcal{D}_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{\\theta}(x_{t-1}|x_{t})), \\tag{3}\\\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}_{KL}(\\cdot||\\cdot)\\)는 두 분포의 차이를 계산하는 데 일반적으로 사용되는 쿨백 라이블러(KL) 발산을 나타낸다.\n' +
      '\n' +
      '**훈련 및 추론 절차.** 훈련 확산 모델의 목표는 각 시간 단계 \\(t\\)에서 순방향 천이의 가능성을 최대화하는 역방향 천이를 찾는 것이다. 실제로, 훈련은 동등하게 변분 상한을 최소화하는 것으로 구성된다. 추론 과정은 최적화된 잡음 제거 파라미터\\(\\theta^{*}\\)와 역 과정에서 다단계 체인을 통해 랜덤 잡음으로부터 새로운 샘플을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '본 논문에서는 랜덤 잡음으로부터 고성능 파라미터를 생성하는 것을 목표로 하는 신경망 확산(p-diff)을 제안한다. 도 1에 도시된 바와 같다. 두 번째, 본 논문에서 제안하는 방법은 파라미터 오토인코더와 생성이라는 두 가지 과정으로 구성된다. 훈련된 고성능 모델 세트가 주어지면 먼저 이러한 매개변수의 하위 집합을 선택하고 1차원 벡터로 평평하게 만든다. 이어서, 이 벡터들로부터 잠재 표현들을 추출하기 위한 인코더를 소개하고, 잠재 표현들로부터 파라미터들을 재구성하는 디코더를 함께 소개한다. 그런 다음, 랜덤 잡음으로부터 잠재 표현들을 합성하도록 표준 잠재 확산 모델이 트레이닝된다. 학습 후 p-diff를 이용하여 랜덤 잡음(\\(\\rightarrow\\) 역과정(\\(\\rightarrow\\) 훈련된 디코더(\\(\\rightarrow\\)의 매개 변수를 생성하였다.\n' +
      '\n' +
      '### Parameter autoencoder\n' +
      '\n' +
      '**오토인코더를 훈련시키기 위한 데이터를 준비하는 것.** 본 논문에서는 모델 파라미터의 부분 집합을 합성하는 것을 기본으로 한다. 따라서 오토인코더에 대한 학습 데이터를 수집하기 위해 모델을 처음부터 학습하고 마지막 에폭에서 체크포인트를 조밀하게 저장한다. SGD 최적화기를 통해 선택된 매개변수 하위 집합만 업데이트하고 모델의 남아 있는 매개변수를 수정한다는 점에 주목할 가치가 있다. 오토인코더를 학습하기 위해 저장된 파라미터들의 부분집합 \\(S=[s_{1},\\dots,s_{k},\\dots,s_{K}]\\)을 사용하였으며, 여기서 \\(K\\)은 학습 샘플의 수이다. 대규모 데이터 세트에 대해 훈련된 일부 대규모 아키텍처의 경우 처음부터 훈련하는 비용을 고려하여 사전 훈련된 모델의 매개변수 하위 집합을 미세 조정하고 미세 조정된 매개변수를 훈련 샘플로 조밀하게 저장한다.\n' +
      '\n' +
      '매개변수 자동인코더.** 훈련 매개변수 \\(S\\)을 1차원 벡터 \\(V=[v_{1},\\dots,v_{k},\\dots,v_{K}]\\)으로 평평하게 만들고, 여기서 \\(V\\in\\mathbb{R}^{K\\times D}\\) 및 \\(D\\)은 부분집합 매개변수의 크기이다. 그 후, 오토인코더는 이들 파라미터 \\(V\\)를 재구성하도록 트레이닝된다. 오토인코더의 강인성과 일반화를 향상시키기 위해 입력 파라미터와 잠재 표현에서 랜덤 잡음 증강을 동시에 도입한다. 인코딩 및 디코딩 프로세스들은 다음과 같이 공식화될 수 있다,\n' +
      '\n' +
      '{split} Z&=[z_{1}^{0},\\dots,z_{k}^{0},\\dots,z_{K}^{0}]=\\underbrace{f_{\\text{encoder}(V+\\xi_{V},\\sigma}}_{\\text{encoding};\\\\V^{\\prime}&=[v_{1}^{\\prime},\\cdots,v_{k}^{\\prime},\\cdots,v_{K}^{\\prime}=\\underbrace{f_{\\text{decoder}(Z+\\xi_{Z},\\rho}}_{tag{4}\\cplit}},\\end{split}\\tag{4}\\cplit}}\n' +
      '\n' +
      '여기서 \\(f_{\\text{encoder}}(\\cdot,\\sigma)\\) 및 \\(f_{\\text{decoder}}(\\cdot,\\rho)\\)는 각각 \\(\\sigma\\) 및 \\(\\rho\\)에 의해 파라미터화된 인코더 및 디코더를 나타낸다. \\\\ (Z\\)는 잠재표상을 나타내고, \\(\\xi_{V}\\) 및 \\(\\xi_{Z}\\)는 입력변수 \\(V\\) 및 잠재표상에 부가되는 랜덤잡음을 나타내며, \\(V^{\\prime}\\)은 재구성된 매개변수이다. 우리는 4계층 인코더와 디코더가 있는 오토인코더를 사용하는 것을 기본으로 한다. 일반적인 오토인코더 훈련과 동일하게, 우리는 다음과 같이 \\(V^{\\prime}\\)와 \\(V\\) 사이의 평균제곱오차(MSE) 손실을 최소화한다.\n' +
      '\n' +
      '\\[L_{text{MSE}=\\frac{1}{K}\\sum\\limits_{1}^{K}\\lVert v_{k}-v_{k}^{\\prime} \\rVert^{2}, \\tag{5}\\\\tag{1}\\sum\\limits_{1}^{K}\\lVert v_{k}-v_{k}^{\\prime} \\rVert^{2},\n' +
      '\n' +
      '여기서 \\(v_{k}^{\\prime}\\)는 \\(k\\)번째 모델의 재구성된 매개변수이다.\n' +
      '\n' +
      '### Parameter generation\n' +
      '\n' +
      '가장 직접적인 전략 중 하나는 확산 모델을 통해 새로운 매개변수를 합성하는 것이다. 그러나, 이 동작의 메모리 비용은, 특히 \\(V\\)의 치수가 초대형일 때 너무 무겁다. 이러한 고려를 바탕으로, 우리는 기본적으로 잠재 표현들에 확산 과정을 적용한다. 파라미터 오토인코더로부터 추출된 \\(Z=[z_{1}^{0},\\cdots,z_{k}^{0},\\cdots,z_{K}^{0}]\\)에 대해, 다음과 같이 DDPM(Ho et al., 2020)의 최적화를 사용하고,\n' +
      '\n' +
      '\\theta\\leftarrow\\theta-\\nabla_{\\theta}||\\epsilon-\\epsilon_{\\theta}(\\sqrt{\\widetilde{\\alpha}_{t}}z_{k}^{0}+\\sqrt{1-\\widetilde{\\alpha}_{t}\\epsilon,t}||^{2}, \\tag{6}\\theta}\n' +
      '\n' +
      '여기서 \\(t\\)은 \\(1\\)과 \\(T\\) 사이에서 균일하고, 하이퍼파라미터 \\(\\widetilde{\\alpha}_{t}\\)의 시퀀스는 각 단계에서의 잡음 강도를 나타내며, \\(\\epsilon\\)은 가우스 잡음, \\(\\epsilon_{\\theta}(\\cdot)\\)은 \\(\\theta\\)에 의해 파라미터화된 잡음 제거 네트워크를 나타낸다. 파라미터 생성의 학습을 마친 후 랜덤 노이즈를 직접 공급했다.\n' +
      '\n' +
      '그림 2: 우리의 접근법은 파라미터 자동 인코더와 생성이라는 두 가지 프로세스로 구성된다. 파라미터 오토인코더는 잠재 표현들을 추출하고 디코더를 통해 모델 파라미터들을 재구성하는 것을 목표로 한다. 추출된 표현들은 표준 잠재 확산 모델(LDM)을 훈련시키는데 사용된다. 추론에서, 랜덤 잡음은 생성된 파라미터들을 획득하기 위해 LDM 및 트레이닝된 디코더에 공급된다.\n' +
      '\n' +
      '상기 역 프로세스 및 상기 트레이닝된 디코더를 통해, 새로운 고-수행 파라미터 세트를 생성하는 단계를 포함하는, 방법. 이러한 생성된 매개변수는 평가를 위한 새로운 모델을 형성하기 위해 남아 있는 모델 매개변수와 연결된다. 신경망 파라미터 및 이미지 픽셀은 데이터 유형, 치수, 범위 및 물리적 해석을 포함한 여러 주요 측면에서 상당한 격차를 나타낸다. 이미지와는 달리 신경망 파라미터는 대부분 공간적 관련성이 없기 때문에 파라미터 오토인코더와 파라미터 생성 과정에서 2차원 컨볼루션을 1차원 컨볼루션으로 대체한다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '이 절에서는 먼저 재생을 위한 설정을 소개한다. 그런 다음 결과 비교 및 절제 연구를 보고한다.\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '**Datasets and architectures.** 우리는 MNIST (LeCun et al., 1998), CIFAR-10/100 (Krizhevsky et al., 2009), ImageNet-1K (Deng et al., 2009), STL-10 (Coates et al., 2011), Flowers (Nilsback and Zisserman, 2008), Pets (Parkhi et al., 2012), 및 F-101 (Bossard et al., 2014)을 포함한 광범위한 데이터 세트에 걸쳐 우리의 접근법의 유효성을 연구한다. 우리는 주로 ResNet-18/50 (He et al., 2016), ViT-Tiny/Base (Dosovskiy et al., 2020), ConvNeXt-T/B (Liu et al., 2022)에 대한 실험을 수행한다.\n' +
      '\n' +
      '**훈련 상세.** 오토인코더 및 잠재 확산 모델은 모두 4-레이어 1D CNNs-기반 인코더 및 디코더를 포함한다. 우리는 모든 아키텍처에 대해 200개의 교육 데이터를 수집하는 것을 기본으로 한다. ResNet-18/50의 경우 처음부터 모델을 교육합니다. 마지막 에포크에서, 우리는 마지막 두 정규화 레이어를 계속 트레이닝하고 다른 파라미터들을 고정한다. 우리는 마지막 에포크에 200개의 체크포인트, 즉 원본 모델을 저장합니다. ViT-Tiny/Base 및 ConvNeXt-T/B의 경우, 우리는 timm 라이브러리(Wightman, 2019)에서 릴리스된 모델의 마지막 두 정규화 매개변수를 미세 조정한다. 0.001과 0.1의 진폭을 갖는 가우시안 잡음인 \\(\\xi_{V}\\)과 \\(\\xi_{Z}\\)은 대부분 단일 Nvidia A100 40G GPU에서 1~3시간 이내에 오토인코더와 잠재 확산 훈련을 완료할 수 있다.\n' +
      '\n' +
      '** 추론 상세.** 랜덤 노이즈를 잠재 확산 모델 및 훈련된 디코더에 공급하여 100개의 신규 파라미터를 합성한다. 그런 다음 이러한 합성된 매개변수를 앞서 언급한 고정 매개변수와 연결하여 생성된 모델을 형성한다. 이렇게 생성된 모델 중에서 학습 세트_에서 가장 성능이 좋은 모델을 선택한다. 그 후, 검증 세트에 대한 정확도를 평가하고 결과를 보고한다. 그것은 SGD 최적화를 사용하여 훈련된 모델과 공정한 비교를 하는 것을 고려한 것이다. 이를 통해 학습 집합에 대한 성능이 테스트 모델을 선택하는 데 적합하다는 것을 경험적으로 확인하였다.\n' +
      '\n' +
      '**기준.**1) 원본 모델 중 가장 우수한 검증 정확도를 \'원본\'으로 표기한다. 2) 오리지널 모델들의 평균 가중치 앙상블(Krogh and Vedelsby, 1994; Wortsman et al., 2022)을 \'ensemble\'로 표시한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '탭 도 1은 8개의 데이터 세트와 6개의 아키텍처에 걸쳐 2개의 기준선과의 결과 비교를 보여준다. 그 결과를 바탕으로 다음과 같은 몇 가지 관찰 결과를 얻었다. i) 대부분의 경우, 우리의 방법은 두 개의 기준선보다 유사하거나 더 나은 결과를 달성한다. 이를 통해 본 논문에서 제안한 방법이 높은 성능의 파라미터 분포를 효율적으로 학습하고 랜덤 노이즈로부터 우수한 모델을 생성할 수 있음을 보인다. ii) 우리의 방법은 다양한 데이터 세트에서 일관되게 잘 수행되며, 이는 우리의 방법의 좋은 일반성을 나타낸다.\n' +
      '\n' +
      '### 절제 연구 및 분석\n' +
      '\n' +
      '이 섹션에서는 우리의 방법의 특성을 설명하기 위해 광범위한 절제 연구가 수행된다. 우리는 CIFAR-100에서 ResNet-18을 훈련하는 것을 기본으로 하며 (달리 명시되지 않은 경우) 최상의, 평균 및 중간 정확도를 보고한다.\n' +
      '\n' +
      '**훈련 모델 수** 탭 도 2의 (a)는 학습 데이터의 크기, 즉 원본 모델의 개수를 가변한다. 우리는 서로 다른 수의 원본 모델들 사이에서 최상의 결과들의 성능 격차가 작다는 것을 발견한다. 다양한 수의 훈련 데이터가 성능 안정성에 미치는 영향을 종합적으로 조사하기 위해 생성된 모델의 안정성 메트릭으로 평균(avg) 및 중위수(med.) 정확도도 보고한다. 특히, 적은 수의 훈련 인스턴스로 생성된 모델의 안정성은 더 큰 설정에서 관찰된 것보다 훨씬 더 나쁘다. 이것은 확산 모델의 학습 원리에 의해 설명될 수 있다: 확산 프로세스는 만약 만약 확산 프로세스가 타겟 분포를 잘 모델링하기 어려울 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c c c c} \\hline \\hline Network\\(\\backslash\\)Dataset & MNIST & CIFAR-10 & CIFAR-100 & STL-10 & Flowers & Pets & F-101 & ImageNet-1K \\\\ \\hline ResNet-18 & 99.2 / 99.2 / **99.3** & 92.5 / 92.5 / **92.7** & 76.7 / 76.7 / **76.9** & 75.5 / 75.5 / 75.4 & 49.1 / **49.7** & **40.9 / 60.8 / 61.1** & 71.2 / 71.3 / 71.3 & 78.7 / 78.5 / 78.7 \\\\ ResNet-50 & 99.4 / 93.9 / 94.1 & 93.1 / 91.4 / 91.3 & 71.6 / 71.5 / **71.7** & 69.2 / 61.0 / 69.2 & 37.3 / 33.7 / 33.8 & 58.0 / 58.0 / 58.0 / 58.6 & 68.6 / 65.5 / 66.6 & 79.2 / 79.2 / **79.3** \\\\ ViT-Tiny & 99.5 / 99.5 / 99.5 & 96.8 / 96.8 / 96.8 & 86.7 / 86.8 / 86.7 / 86.7 & 93.7 / 93.7 / 93.7 / 85.7 / 85.7 / 85.7 & 93.9 / 89.3 / 89.3 & 75.8 / 74.7 / 85.5 & 73.7 / 73.7 / 74.1 \\\\ ViT-Base & 99.5 / 99.4 / 99.5 & 98.7 / 98.7 / 98.7 / 98.1 & 91.5 / 91.4 / **91.7** & 99.1 / 99.0 / **99.2** & 98.3 / 98.3 & 91.6 / 91.5 / **91.7** & 83.4 / 83.4 / 83.4 / 84.5 / **84.7** \\\\ ConvNeXt-T & 99.3 / **99.4** & 97.6 / 97.6 / 97.7 & 87.0 / 87.0 / **87.1** & 98.2 / 98.0 / 98.2 & 70.0 / 70.0 / 70.0 / **70.5** & 92.9 / **78.9** & 76.1 / 76.1 / 76.2 & 82.1 / 82.1 / **82.3** \\\\ ConvNeXt-B & 99.3 / 99.3 / **99.4** & 98.1 / 98.1 / 98.8 & 83.8 / 88.4 / 88.4 & 98.8 / 98.8 / **98.9** & 88.4 / 88.4 / 88.5 & 94.1 / 94.0 / 94.1 & 81.4 / 81.4 / **81.6** & 83.8 / 83.7 / **83.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: \'원본/앙상블/p-diff\'의 형식으로 결과를 제시한다. 우리의 방법은 기준선보다 유사하거나 심지어 더 높은 성능을 얻는다. p-diff의 결과는 세 번의 실행에서 평균이다. **굵은 항목**이 가장 좋은 결과입니다.\n' +
      '\n' +
      '훈련에는 몇 개의 입력 샘플만 사용됩니다.\n' +
      '\n' +
      '**where to apply p-diff.** we default to synthesizing the parameters of the last two normalization layer. 정규화 층의 다른 깊이에 대한 p-diff의 효과를 조사하기 위해 다른 얕은 층 매개변수를 합성하는 성능도 탐구한다. 동일한 수의 BN 매개변수를 유지하기 위해 깊이가 다른 레이어 사이에 있는 세 세트의 BN 레이어에 대한 접근법을 구현한다. 탭에 표시된 대로입니다. 2(b)는 BN 레이어 설정의 모든 깊이에서 본 논문에서 제안한 방법이 기존 모델보다 우수한 성능(최상의 정확도)을 나타냄을 경험적으로 알 수 있다. 또 다른 발견은 깊은 층을 합성하는 것이 얕은 층을 생성하는 것보다 더 나은 정확도를 달성할 수 있다는 것이다. 이는 얕은 층 파라미터를 생성하는 것이 깊은 층 파라미터를 생성하는 것보다 순방향 전파 동안 에러를 축적할 가능성이 더 높기 때문이다.\n' +
      '\n' +
      '**잡음 증강.**잡음 증강은 오토인코더의 트레이닝의 견고성 및 일반화를 향상시키도록 설계된다. 이 증강을 입력 매개변수와 잠재 표현에서 각각 적용하는 효과를 제거한다. 절제 결과는 탭에 나와 있다. 2(c). 몇 가지 관찰은 다음과 같이 요약될 수 있다: i) 소음 증가는 안정적이고 성능이 높은 모델을 생성하는 데 중요한 역할을 한다. ii) 잠재 표현들에서 잡음 증강을 적용하는 성능 이득들은 입력 파라미터들에서보다 더 크다. iii) 파라미터 및 표현에서 잡음 증강을 공동으로 사용하는 우리의 디폴트 설정은 최상의 성능(최고, 평균 및 중간 정확도를 포함함)을 획득한다.\n' +
      '\n' +
      '**전체 모델 매개변수에 대한 일반화.** 지금까지 우리는 모델 매개변수의 하위 집합인 _즉,_배치 정규화 매개변수를 합성하는 데 접근법의 유효성을 평가했다. _ 전체 모델 매개변수를 합성하는 것은 어떻습니까?_ 이를 평가하기 위해 MLP-3(3개의 선형 계층과 ReLU 활성화 함수를 포함)과 ConvNet-3(3개의 컨볼루션 계층과 1개의 선형 계층을 포함)의 두 가지 작은 아키텍처로 접근 방식을 확장한다. 앞서 언급한 훈련 데이터 수집 전략과 달리 우리는 200개의 다른 무작위 시드로 이러한 아키텍처를 처음부터 개별적으로 훈련한다. CIFAR-10을 예로 들어 이 두 아키텍처의 세부 사항(컨볼루션 계층: 커널 크기\\(\\times\\) 커널 크기, 채널 수; 선형 계층: 입력 차원, 출력 차원)을 다음과 같이 보여준다.\n' +
      '\n' +
      'ConvNet-3: conv1. 3\\(\\times\\)3, 32, conv2. 3\\(\\times\\)3, 32, conv3. 3\\(\\times\\)3, 32, 선형층. 2048, 10\n' +
      '\n' +
      '\\(\\bullet\\) MLP-3: 선형층1. 3072, 50, 선형층2. 50, 25, 선형층3. 25, 10.\n' +
      '\n' +
      '본 논문에서는 Tab. 3의 ConvNet-3과 CIFAR-10/100의 ConvNet-3과 MNIST 데이터 셋의 MLP-3의 비교와 파라미터 수를 제시한다. 이러한 실험은 전체 모델 매개변수를 합성하는 데 있어 접근법의 효과 및 일반화를 입증하며, 즉 유사하거나 심지어 개선된 결과를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 2: **p-diff 주요 절제 실험.** 원본 모델의 수\\(K\\), 접근법을 적용하는 위치 및 노이즈 증강의 효과를 절제한다. 기본 설정은 \\(K=200\\), 심층 BN 매개변수(계층 16에서 18 사이)에 p-diff를 적용하고 입력 매개변수와 잠재 표현에서 노이즈 증강을 사용한다. 기본값은 회색으로 표시됩니다. **굵은 항목**이 가장 좋은 결과입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: 전체 모델 매개변수 설정을 종합하여 원본, 앙상블 및 p-diff의 결과 비교를 제시한다. 이 방법은 ConvNet-3과 MLP-3에 대해 좋은 일반화를 보여준다. **굵은 엔트리**가 가장 좋은 결과이다.\n' +
      '\n' +
      '기준선을 초과하는 성능 이러한 결과는 제안된 방법의 실제 적용 가능성을 시사한다. 그러나, 우리는 ResNet, ViT 및 ConvNeXt 시리즈와 같은 대형 아키텍처의 전체 파라미터를 합성할 수 없다. GPU 메모리의 한계로 인해 주로 제약을 받는다.\n' +
      '\n' +
      '**원래 모델의 매개변수 패턴.** 실험 결과 및 절제 연구는 신경망 매개변수를 생성하는 데 있어 본 방법의 효과를 입증한다. 이 배경의 본질적인 이유를 탐구하기 위해 3개의 무작위 시드를 사용하여 ResNet-18 모델을 처음부터 훈련하고 그림 3의 매개변수를 시각화하고 다양한 레이어에서 최소 최대 정규화를 통해 매개변수 분포의 히트 맵을 개별적으로 시각화한다. 컨볼루션(Conv)의 파라미터의 시각화에 기초한다. -layer2) 및 완전 연결(FC-layer18) 층들, 실제로 이들 층들 사이에 특정 파라미터 패턴들이 존재한다. 이러한 패턴들의 학습에 기초하여, 우리의 접근법은 고성능의 신경망 파라미터들을 생성할 수 있다.\n' +
      '\n' +
      '##4는 P-diffing Only Memorizing인가요?\n' +
      '\n' +
      '본 절에서는 주로 원본 모델과 생성된 모델의 차이를 조사한다. 먼저 유사도 메트릭을 제안한다. 그런 다음 접근법의 특성을 설명하기 위해 몇 가지 비교 및 시각화를 수행한다.\n' +
      '\n' +
      '**질문 및 실험 설계** 여기서 우리는 먼저 다음과 같은 질문을 한다: 1) p-diff는 훈련 세트의 원래 모델의 샘플을 그냥 외우나요? 2) 노이즈를 추가하거나 원래 모델과 우리가 접근하여 생성한 모델 간에 차이가 있습니까? 본 논문에서 우리는 p-diff가 원래 모델과 다르게 작동하는 몇 가지 새로운 매개변수를 생성할 수 있기를 바란다. 이를 검증하기 위해 예측과 시각화를 비교하여 원본, 노이즈 추가, 미세 조정 및 p-diff 모델의 차이를 연구하는 실험을 설계한다.\n' +
      '\n' +
      '**Similarity metric.** 기본 설정 하에서 ResNet-18(He et al., 2016)으로 CIFAR-100(Krizhevsky et al., 2009)에 대한 실험을 수행하며, _i.e._ 마지막 두 배치 정규화 레이어의 파라미터만 생성한다. 우리는 두 모델의 잘못된 예측에 대해 IoU(Intersection over Union)를 계산하여 두 모델 간의 유사성을 측정한다. 상기 IoU는 다음과 같이 공식화될 수 있고,\n' +
      '\n' +
      '\\[\\mathrm{IoU}=|P_{1}^{\\mathrm{wrong}}\\cap P_{2}^{\\mathrm{wrong}}|/|P_{1}^{\\mathrm{wrong}}\\cup P_{2}^{\\mathrm{wrong}|, \\tag{7}\\cup\n' +
      '\n' +
      '여기서 \\(P^{\\mathrm{wrong}}\\)은 검증 세트에서 잘못된 예측의 인덱스를 나타내고, \\(\\cap\\) 및 \\(\\cup\\)은 결합 및 교차 연산을 나타낸다. IoU가 높을수록 두 모델의 예측 간에 더 큰 유사성을 나타낸다. 이제부터, 우리는 IoU를 우리의 논문에서 유사성 메트릭으로 사용한다. 실험에서 성능 대비의 영향을 완화하기 위해 기본적으로 76.5% 이상의 성능을 수행하는 모델을 선택한다.\n' +
      '\n' +
      '**예측의 유사성.** 원본 모델과 p-diff 모델 간의 유사성을 평가한다. 각 모델에 대해 IoU를 다른 모델과 평균하여 유사성을 얻는다. 1) 원본 모델 간의 유사성, 2) p-diff 모델 간의 유사성, 3) 원본 모델과 p-diff 모델 간의 유사성, 4) 원본 모델과 p-diff 모델 간의 최대 유사성(최근접 이웃)의 네 가지 비교를 소개한다. 위의 4가지 비교에서 모든 모델에 대한 IoU를 계산하고 그림 1에 평균 값을 보고한다. 4(a)\n' +
      '\n' +
      '생성된 모델 간의 차이가 원래 모델 간의 차이보다 훨씬 크다는 것을 알 수 있다. 또 다른 발견은 원본과 생성된 모델 간의 최대 유사성조차도 원본 모델 간의 유사성보다 낮다는 것이다. 그것은 우리의 p-diff가 그들의 트레이닝 데이터(_i.e._ 원본 모델들)와 다르게 수행하는 새로운 파라미터들을 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '우리는 또한 우리의 접근 방식을 미세 조정 및 노이즈 추가 모델과 비교한다. 구체적으로, 생성된 모델을 무작위로 선택하고, 원본 모델에서 가장 가까운 이웃(_i.e. max 유사도)을 검색한다. 그런 다음 가장 가까운 이웃에서 랜덤 노이즈를 미세 조정하고 추가하여 해당 모델을 얻는다. 그 후, 미세 조정 및 노이즈 추가 모델을 사용하여 원본의 유사성을 각각 계산한다. 마지막으로 이 작업을 50회 반복하고 분석을 위해 평균 IoU를 보고한다. 이 실험에서 우리는 또한 모든 모델의 성능을 제한하는데, 여기서는 시각화의 편향을 줄이기 위해 좋은 모델만 사용한다. 우리는 실질적인 성능 저하를 방지하기 위해 0.01에서 0.1 범위의 랜덤 노이즈의 진폭을 경험적으로 설정했다.\n' +
      '\n' +
      '그림 1의 결과를 기반으로 한다. 4(b) 우리는 미세 조정 및 노이즈 추가 모델의 성능이 어렵다는 것을 발견했다.\n' +
      '\n' +
      '도 3: 컨벌루션(Conv)의 파라미터 분포를 시각화하는 것. -layer2) 및 완전히 연결된(FC-layer18) 층들. 상이한 층으로부터의 파라미터들은 변형 패턴들을 나타내는 반면, 동일한 층으로부터의 이들 파라미터들은 유사한 패턴들을 나타낸다. 레이어의 인덱스는 표준 ResNet-18과 정렬된다.\n' +
      '\n' +
      '기존 모델을 능가합니다. 또한 미세 조정 또는 노이즈 추가 모델과 원래 모델 간의 유사성이 매우 높기 때문에 이 두 작업은 신규하지만 성능이 높은 모델을 얻을 수 없음을 나타낸다. 그러나 생성된 모델은 원본 모델에 비해 다양한 유사성과 우수한 성능을 달성한다.\n' +
      '\n' +
      '**잠재 표현의 비교.** 예측과 더불어, t-SNE(Van der Maaten et al., 2008)를 이용하여 원본 및 생성된 모델에 대한 잠재 표현의 분포를 평가한다. 우리의 접근법과 원래 모델의 잠재 표현에 노이즈를 추가하는 작업 간의 차이를 식별하기 위해 그림 1의 비교로 노이즈 추가 작업도 포함한다. 4(c). 추가된 잡음은 0.1의 진폭을 갖는 랜덤 가우시안 잡음으로, p-diff는 새로운 잠재 표상을 생성할 수 있고, 잡음은 원래 모델들의 잠재 표상을 중심으로 보간을 할 수 있다는 것을 알 수 있다.\n' +
      '\n' +
      '**p-diff 과정의 궤적.** 추론 단계에서 서로 다른 시간 단계의 생성된 매개변수를 플로팅하여 궤적을 형성하여 생성 과정을 탐색한다. 5개의 궤적(5개의 랜덤 노이즈에 의해 초기화됨)이 그림에 나와 있다. 5(a) 또한 원래 모델의 평균 모수와 표준 편차(std)를 표시합니다. 시간 단계가 증가함에 따라 생성된 매개변수는 전체적으로 원래 모델에 가깝다. 시각화를 위해 좁은 성능 범위 제약을 유지하지만 궤적의 끝점(주황색 삼각형)과 평균 매개변수(5점 별) 사이에는 여전히 일정한 거리가 있다. 또 다른 발견은 5개의 궤적이 다양하다는 것이다.\n' +
      '\n' +
      '** 암기에서 새로운 매개변수를 생성하기 위해.** 원본 모델 수(\\(K\\))가 생성된 모델의 다양성에 미치는 영향을 조사하기 위해 그림에서 서로 다른 \\(K\\)으로 원본 모델과 생성된 모델 간의 최대 유사성을 시각화한다. 5(b). 특히, 50개의 모델이 모든 경우에 76.5% 이상의 성능을 발휘할 때까지 지속적으로 매개변수를 생성한다. 생성된 모델들은 좁은 유사도 범위와 높은 값에서 알 수 있듯이 \\(K=1\\)일 때 거의 원래의 모델을 기억한다. 이렇게 생성된 모델의 유사성 범위는 \\(K\\)이 증가함에 따라 더 커지며, 본 논문에서 제안한 방법은 원래 모델과 다르게 수행하는 매개변수를 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '**확산 모델.**확산 모델은 시각적 생성에서 현저한 결과를 달성했다. 이들 방법(Ho et al., 2020; Dhariwal and Nichol, 2021; Ho et al., 2022; Peebles and Xie, 2022; Hertz et al., 2023; Li et al., 2023)은 비평형 열역학(Jarzynski, 1997; Sohl-Dickstein et al., 2015)에 기초하고, 그 경로는 GAN(Zhu et al., 2017; Isola et al., 2017; Brock et al., 2018), VAE(Kingma and Welling, 2013; Razavi et al., 2019) 및 흐름 기반 모델(Dinh et al., 2014; Rezende and Mohamed, 2015)과 유사하다. 확산 모델은 크게 세 가지로 분류할 수 있다. 첫 번째 분기는 DALL-E 2(Ramesh et al., 2022), Imagen(Saharia et al., 2022) 및 Stable Diffusion(Rombach et al., 2022)과 같은 모델로 예시된 확산 모델의 합성 품질을 향상시키는 데 중점을 둔다. 두 번째 지점은 DDIM(Song et al., 2021), Analytic-DPM(Bao et al., 2022), DPM-Solver(Lu et al., 2022) 등 샘플링 속도 향상을 목표로 하고 있다. 최종 브랜치는 디푸를 재평가하는 것을 포함한다\n' +
      '\n' +
      '그림 4: 유사성은 두 모델 사이의 잘못된 예측에 대한 연합의 교차(IoU)를 나타낸다(a)는 원본 모델과 p-diff 모델 간의 유사성, 원본 모델과 p-diff 모델 간의 유사성, 원본 모델과 p-diff 모델 간의 최대 유사성(최근접 이웃)의 네 가지 경우의 비교를 보여준다. (b) 미세 조정, 노이즈 부가, p-diff 모델의 정확도와 최대 유사도를 표시한다. 모든 최대 유사성은 원래 모델로 계산됩니다. (c)는 원본 모델들, p-diff 모델들, 및 추가 잡음 연산에 대한 잠재 표현들의 t-SNE(Van der Maaten et al., 2008)를 제시한다.\n' +
      '\n' +
      'sion models from continuous perspective, like score-based models (Song & Ermon, 2019; Feng et al., 2023).\n' +
      '\n' +
      '**Parameter generation.** HyperNet (Ha et al., 2017)은 가변 아키텍처를 가진 모델의 가중치를 동적으로 생성한다. Smash(Brock et al., 2018)는 다양한 범위의 아키텍처를 정의할 수 있는 메모리 판독-기록에 기초한 유연한 스킴을 소개한다. (Peebles et al., 2023)은 2,300만 개의 체크포인트를 수집하고 변압기 기반 확산 모델을 통해 조건부 발전기를 훈련시킨다. MetaDiff(Zhang & Yu, 2023)는 소수 샷 학습을 위한 확산 기반 메타 학습 방법을 소개하며, 여기서 레이어는 확산 U-Net으로 대체된다(Ronneberger et al., 2015). HyperDiffusion (Erkoc et al., 2023)은 새로운 신경 암시적 필드들을 생성하기 위해 MLP들 상의 확산 모델을 직접 이용한다. 이와는 달리 이미지와 파라미터 간의 내재적 차이를 분석하고, 해당 모듈을 설계하여 성능이 높은 파라미터의 분포를 학습한다.\n' +
      '\n' +
      '**확률적 및 베이지안 신경망.** 우리의 접근법은 훈련된 확산 모델로 표현되는 네트워크 매개 변수를 통해 사전 학습을 하는 것으로 볼 수 있다. 신경망에 대한 학습 파라미터 사전은 고전 문헌에서 연구되어 왔다. 확률 신경망(SNNs)(Sompolinsky et al., 1988; Bottou et al., 1991; Wong, 1991; Schmidt et al., 1992; Murata et al., 1994)은 또한 신경망의 견고성 및 일반화를 개선하기 위해 랜덤성을 도입함으로써 그러한 사전들을 학습한다. 베이지안 신경망(Neal, 2012; Kingma & Welling, 2013; Rezende et al., 2014; Kingma et al., 2015; Gal & Ghahramani, 2016)은 신경망에 대한 확률 분포를 모델링하여 과적합을 완화하고, 작은 데이터 세트로부터 학습하고, 모델 예측의 불확실성을 평가하는 것을 목표로 한다. (Graves, 2011)는 신경망에 대한 베이지안 추론의 실용적인 근사치로 쉽게 구현할 수 있는 확률적 변분법을 제안한다. 그들은 네트워크 가중치의 수를 줄이기 위해 휴리스틱 프루너를 도입하여 일반화를 개선한다. (Welling & Teh, 2011) Langevin 역학을 SGD와 결합하여 그라디언트에 앞서 가우시안(Gaussian)을 통합한다. 이것은 SGD 최적화를 샘플링 프로세스로 변환합니다. _ Bayes by Backprop_(Blundell et al., 2015)는 신경망의 가중치들에 대한 확률 분포를 미리 학습한다. 이러한 방법은 대부분 소규모 설정에서 작동하는 반면 p-diff는 실제 아키텍처에서 그 효과를 보여준다.\n' +
      '\n' +
      '##6 토론 및 결론\n' +
      '\n' +
      '신경망은 지도 학습(Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016; Dosovitskiy et al., 2020), 자가 지도 학습(Devlin et al., 2018; Brown et al., 2020; He et al., 2020, 2022) 등과 같은 몇 가지 인기 있는 학습 패러다임을 갖는다. 본 연구에서는 확산 모델이 고성능 및 새로운 신경망 파라미터를 생성하는 데 사용될 수 있음을 관찰하여 그 우수성을 입증한다. 신경망 파라미터 업데이트를 위해 확산 단계를 사용하는 것은 딥 러닝에서 잠재적으로 새로운 패러다임을 보여준다.\n' +
      '\n' +
      '그러나 우리는 이미지/비디오 및 매개변수가 서로 다른 특성의 신호임을 인정하며 이러한 구별은 주의 깊게 다루어져야 한다. 또한 확산 모델이 이미지/비디오 생성에서 상당한 성공을 거두었음에도 불구하고 매개변수에 대한 적용은 상대적으로 미해결 상태로 남아 있다. 이들은 신경망 확산에 대한 일련의 과제를 제기한다. 우리는 이러한 문제 중 일부를 해결하기 위한 초기 접근법을 제안한다. 그럼에도 불구하고, 대형 아키텍처의 전체 파라미터를 생성하기 위한 메모리 제약, 구조 설계의 효율성 및 성능 안정성을 포함하여 여전히 해결되지 않은 과제가 있다.\n' +
      '\n' +
      '그림 5: (a)는 우리의 접근법의 매개변수 궤적과 t-SNE를 통한 원래 모델 분포를 보여준다. (b)는 서로 다른 \\(K\\) 설정에서 생성된 모델과 원래 모델 사이의 최대 IoU를 나타낸다. 심 유사성을 나타냅니다.\n' +
      '\n' +
      '찬성합니다.** 카임잉 허, 디안보 류, 밍지아 시, 정 주, 보 자오, 지아웨이 류, 용 류, 정 진, 장웨이 정, 이판 장, 상유 펑, 홍얀 장, 다비드 인, 데이브 젠유 첸, 아흐마드 사제디, 조지 카제나베트에게 귀중한 토론과 피드백에 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bao et al. (2022) Bao, F., Li, C., Zhu, J., and Zhang, B. Analytic-DPM: a analytic estimate of the optimal reverse variance in diffusion probability models. _ICLR_, 2022. URL[https://openreview.net/forum?id=0xiJLKH-ufZ](https://openreview.net/forum?id=0xiJLKH-ufZ)이다.\n' +
      '* Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural network. _ICML_에서. PMLR, 2015년\n' +
      '* Bossard et al. (2014) Bossard, L., Guillaumin, M., and Van Gool, L. 푸드-101 채굴 랜덤 포레스트가 있는 차별적 구성 요소. _ECCV_에서. 2014년 스프링거\n' +
      '* Bottou et al.(1991) Bottou, L. et al. Stochastic gradient learning in neural networks. _ Proceedings of Neuro-Nimes_, 91(8), 1991.\n' +
      '* Brock et al. (2018a) Brock, A., Donahue, J., and Simonyan, K. 고충실도 자연 영상 합성을 위한 대규모 갠 트레이닝 _ arXiv preprint arXiv:1809.11096_, 2018a.\n' +
      '* Brock et al. (2018b) Brock, A., Lim, T., Ritchie, J., and Weston, N. SMASH: 하이퍼네트워크를 통한 원샷 모델 아키텍처 검색. _ICLR_, 2018b에서. URL[https://openreview.net/forum?id=rydeCEhs-](https://openreview.net/forum?id=rydeCEhs-)\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ NeurIPS_, 33, 2020\n' +
      '* Coates et al. (2011) Coates, A., Ng, A., and Lee, H. An analysis of single-layer networks in nonsupervised feature learning. [Proceedings of the 14th international conference on artificial intelligence and statistics_]. JMLR Workshop and Conference Proceedings, 2011.\n' +
      '* Cristianini et al. (2000) Cristianini, N., Shawe-Taylor, J., et al. _An introduction to support vector machines and other kernel-based learning methods_. 케임브리지 대학 기자 2000년\n' +
      '* Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L. -J., Li, K., and Fei-Fei, L. Imagenet: 대규모 계층 이미지 데이터베이스. _CVPR_에서. 2009년\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 깊은 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Dhariwal & Nichol (2021) Dhariwal, P. and Nichol, A. Diffusion model beat gans on image synthesis. _ NeurIPS_, 34, 2021.\n' +
      '* Dinh et al. (2014) Dinh, L., Krueger, D., and Bengio, Y. 나이스: 비선형 독립성분 추정. _ arXiv preprint arXiv:1410.8516_, 2014.\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* Erkoc et al. (2023) Erkoc, Z., Ma, F., Shan, Q., Niessner, M., and Dai, A. Hyper-diffusion: Weight-space diffusion을 갖는 implicit neural fields를 생성하는 단계; _ arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* Feng et al. (2023) Feng, B. T., Smith, J., Rubinstein, M., Chang, H., Bouman, K. L., and Freeman, W. T. Score-based diffusion models as principalipled priors for inverse imaging. _ arXiv preprint arXiv:2304.11751_, 2023.\n' +
      '* Gal & Ghramani (2016) Gal, Y. 그리고 Ghramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. _ICML_에서. PMLR, 2016년\n' +
      '* Graves(2011) Graves, A. Practical Variational inference for neural networks. _ NeurIPS_, 24, 2011.\n' +
      '*Ha et al.(2017) Ha, D., Dai, A. M., and Le, Q. V. Hypernetworks. _ICLR_, 2017. URL[https://openreview.net/forum?id=rkpACellx](https://openreview.net/forum?id=rkpACellx)에서,\n' +
      '* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. 2016년 _CVPR_에서.\n' +
      '* He et al. (2020) He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. 지도하지 않은 시각적 표현 학습을 위한 운동량 대비. 2020년 _CVPR_에서.\n' +
      '* He et al. (2022) He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. 마스크 자동 인코더는 확장 가능한 비전 학습자입니다. 2022년 _CVPR_에서.\n' +
      '* Hertz et al. (2023) Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-or, D. Prompt-to-prompt image editing with cross-attention control. _ICLR_, 2023. URL[https://openreview.net/forum?id=_CDixzkzeyb](https://openreview.net/forum?id=_CDixzkzeyb)에서,\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probability models. _ NeurIPS_, 33, 2020\n' +
      '* Ho et al. (2022) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. _ arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '\n' +
      '* Isola et al. (2017) Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. Image-to-image translation with conditional adversarial networks. 2017년 _CVPR_에서.\n' +
      '* Jarzynski (1997) Jarzynski, C. Equilibrium free-energy differences from nonquilibrium measurements: master-equation approach. _ 물리적 검토 E_, 56(5), 1997.\n' +
      '* Kingma & Welling (2013) Kingma, D. P. and Welling, M. 자동 인코딩 변량 베이지안 arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* Kingma et al. (2015) Kingma, D. P., Salimans, T., and Welling, M. 변이 드롭아웃과 국부 재매개변수화 기법 NeurIPS_, 28, 2015.\n' +
      '* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layer of features from tiny images. 2009년\n' +
      '* Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. _ NeurIPS_, 25, 2012.\n' +
      '* Krogh & Vedelsby (1994) Krogh, A. and Vedelsby, J. Neural Network ensembles, cross validation, active learning. _ NeurIPS_, 7, 1994.\n' +
      '* LeCun et al. (1998) LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. _ Proceedings of the IEEE_, 86(11), 1998.\n' +
      '* Li et al. (2023) Li, A. C., Prabhudesai, M., Duggal, S., Brown, E., and Pathak, D. Your diffusion model is secretly a zero-shot classifier. _ arXiv preprint arXiv:2303.16203_, 2023.\n' +
      '* Lin et al. (2014) Lin, T. -Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* Liu et al. (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. 2020년대를 위한 경마장 2022년 _CVPR_에서.\n' +
      '* Long et al. (2015) Long, J., Shelhamer, E., and Darrell, T. 시맨틱 세분화를 위한 완전한 컨볼루션 네트워크. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3431-3440, 2015.\n' +
      '* Lu et al. (2022) Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-solver: A fast ODE solver for diffusion probabilistic model sampling in 약 10step. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _NeurIPS_, 2022. URL[https://openreview.net/forum?id=2uAaGW1P_V](https://openreview.net/forum?id=2uAaGW1P_V).\n' +
      '* Murata et al. (1994) Murata, N., Yoshizawa, S., and Amari, S. -i. 네트워크 정보 기준-인공 신경망 모델에 대한 은닉 유닛의 개수를 결정하는 단계. _ IEEE transactions on neural networks_, 5(6), 1994.\n' +
      '* Neal(2012) Neal, R. M. _Bayesian learning for neural networks_, volume 118. Springer Science & Business Media, 2012.\n' +
      '*Nichol et al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. 글라이드: 텍스트 유도 확산 모델을 사용한 사실적 이미지 생성 및 편집을 위한 _ arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* Nilsback & Zisserman (2008) Nilsback, M. - E. 그리고 Zisserman, A. 많은 수의 클래스에 걸쳐 꽃 분류를 자동화했다. 2008년 제6차 인도 컴퓨터 비전 회의, 그래픽 및 이미지 처리. IEEE, 2008.\n' +
      '* Parkhi et al. (2012) Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. _CVPR_에서. IEEE, 2012.\n' +
      '* Peebles & Xie(2022) Peebles, W. 및 Xie, S. 변압기가 있는 확장 가능한 확산 모델 _ arXiv preprint arXiv:2212.09748_, 2022.\n' +
      '* Peebles et al. (2023) Peebles, W., Radosavovic, I., Brooks, T., Efros, A. A., and Malik, J. Learning to learn with generatingative models of neural network checkpoints, 2023. URL[https://openreview.net/forum?id=JXkz3zm8gJ](https://openreview.net/forum?id=JXkz3zm8gJ).\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. 클립 래턴트를 사용한 계층적 텍스트 조건 이미지 생성. _ arXiv preprint arXiv:2204.06125_, 1(2), 2022.\n' +
      '* Razavi et al. (2019) Razavi, A., Van den Oord, A., and Vinyals, O. vq-vae-2. _NeurIPS_, 32, 2019로 다양한 고충실도 이미지를 생성한다.\n' +
      '* Ren et al. (2015) Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. _ 신경 정보 처리 시스템_, 28, 2015의 발전.\n' +
      '* Rezende & Mohamed (2015) Rezende, D. and Mohamed, S. 흐름을 정규화하는 변형 추론입니다. _ICML_에서. PMLR, 2015년\n' +
      '* Rezende et al. (2014) Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. _ICML_에서. PMLR, 2014년\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. 2022년 _CVPR_에서.\n' +
      '* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. U-net: 생체 의학 영상 분할을 위한 컨볼루션 네트워크. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Saharia et al. (2015) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _ NeurIPS_, 35, 2022년\n' +
      '* Schmidt et al. (1992) Schmidt, W. F., Kraaijveld, M. A., Duin, R. P., et al. Feed forward neural networks with random weights. _ICPR_에서. IEEE Computer Society Press, 1992.\n' +
      '* Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional networks for large scale image recognition. _ arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 _ICML_에서. PMLR, 2015년\n' +
      '* Sompolinsky et al. (1988) Sompolinsky, H., Crisanti, A., and Sommers, H.-J. 무작위 신경망에서의 혼돈 물리적 검토 문자_, 61(3), 1988.\n' +
      '* Song et al. (2021) Song, J., Meng, C., and Ermon, S. 확산 암시적 모델의 잡음 제거 _ICLR_, 2021. URL[https://openreview.net/forum?id=St1girCHLP](https://openreview.net/forum?id=St1girCHLP)이다.\n' +
      '*Song & Ermon(2019) Song, Y. 및 Ermon, S. 데이터 분포의 기울기를 추정하여 생성 모델링. _ NeurIPS_, 32, 2019.\n' +
      '* Tian et al. (2020) Tian, Z., Shen, C., Chen, H., and He, T. Fcos: 간단하고 강한 앵커가 없는 물체 검출기 _ IEEE T-PAMI_, 44(4):1922-1933, 2020.\n' +
      '* Van der Maaten et al. (2008) Van der Maaten, L., Hinton, G., and Van der Maaten, L. t-sne을 이용하여 데이터를 시각화하는 단계; _ JMLR_, 9(11), 2008.\n' +
      '* Welling & Teh(2011) Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient langevin dynamics. 2011년 _ICML_에서.\n' +
      '* Wightman(2019) Wightman, R. 파이토치 이미지 모델. [https://github.com/rwightman/pytorch-image-models] (https://github.com/rwightman/pytorch-image-models), 2019.\n' +
      '* Wong(1991) Wong, E. Stochastic Neural Networks. _ Algorithmica_, 6(1-6), 1991.\n' +
      '* Wortsman et al. (2022) Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: 다중 미세 조정 모델의 가중치를 평균화하는 것은 추론 시간을 증가시키지 않으면서 정확도를 향상시킨다. In _ICML_, pp. 23965-23998. PMLR, 2022.\n' +
      '* Zhang & Yu(2023) Zhang, B. and Yu, D. Metadiff: few-shot learning을 위한 조건부 확산을 갖는 Meta-learning. _ arXiv preprint arXiv:2307.16424_, 2023.\n' +
      '* Zhu et al. (2017) Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017년 _ICCV_에서.\n' +
      '\n' +
      '## 부록 A 실험 설정\n' +
      '\n' +
      '이 절에서는 상세한 실험 설정, 데이터 세트 및 재생용 코드의 지침을 소개한다.\n' +
      '\n' +
      '### Training recipe\n' +
      '\n' +
      'Tab. 4. CIFAR-100 데이터셋으로 ResNet-18의 설정을 기반으로 기본 학습 레시피를 제공하고 있다. 우리는 일반적인 훈련 하이퍼파라미터, 오토인코더 및 잠재 확산 모델에 대한 세부 사항을 각각 소개한다. 다른 데이터 세트에 대한 학습 속도 및 훈련 반복에 대한 조정이 필요할 수 있다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리는 8개의 데이터 세트에 대한 p-diff의 효과를 평가한다. 구체적으로, **CIFAR-10/100**(Krizhevsky et al., 2009). CIFAR 데이터 세트는 각각 10개 클래스와 100개 클래스로 분류된 차원(32\\times 32\\)의 컬러 자연 이미지로 구성된다. 각 데이터 세트는 훈련을 위한 50,000개의 이미지와 테스트를 위한 10,000개의 이미지로 구성된다. **ImageNet-1K**(Deng et al., 2009) 더 큰 ImageNet-21K 데이터셋에서 파생된, ImageNet-1K는 1,000개의 카테고리를 특징으로 하는 큐레이션된 서브세트이다. 1,281,167개의 훈련 이미지와 50,000개의 검증 이미지를 포함합니다. **STL-10**(Coates et al., 2011)는 10개의 상이한 객체 카테고리에 걸쳐 \\(96\\times 96\\) 컬러 이미지를 포함한다. 이미지 분류 및 객체 인식을 포함한 다양한 컴퓨터 비전 작업을 위한 다목적 리소스 역할을 합니다. **꽃**(Nilsback & Zisserman, 2008)는 102개의 별개의 꽃 범주를 포함하는 데이터 세트이며, 각 범주는 영국에서 발견되는 일반적으로 발생하는 꽃 종을 나타낸다. **Pets**(Parkhi et al., 2012)는 37개의 카테고리를 갖는 약 7000개의 이미지를 포함한다. 이미지는 스케일, 포즈 및 조명에 큰 변화를 가지고 있습니다. **F-101**(Bossard et al., 2014)는 Food-101 분류법을 이용하여 Google, Bing, Yelp, TripAdvisor로부터 크롤링되는 365K개의 이미지로 구성된다.\n' +
      '\n' +
      '부록에서는 객체 검출, 의미론적 분할 및 이미지 생성 작업에서 p-diff를 확장한다. 따라서 우리는 또한 다음에서 추가로 사용되는 데이터 세트를 소개한다. **COCO**(Lin et al., 2014)는 80개의 객체 카테고리가 있는 복잡한 장면을 특징으로 하는 200,000개 이상의 이미지로 구성된다. 객체 검출 및 분할 작업에 널리 사용된다. 우리는 CIFAR-10에서 이미지 생성 작업을 구현한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} Training Setting & Configuration \\\\ \\hline \\(K\\), _i.e._, the number of original models & 200 \\\\ batch size & 200 \\\\ \\hline \\hline Autoencoder & \\\\ \\hline optimizer & AdamW \\\\ learning rate & 1e-3 \\\\ training iterations & 30, 000 \\\\ optimizer momentum & betas=(0.9, 0.999) \\\\ weight decay & 2e-6 \\\\ \\(\\xi_{V}\\), _i.e._, noise added on the input parameters & 0.001 \\\\ \\(\\xi_{Z}\\), _i.e._, noise added on the latent representations & 0.1 \\\\ \\hline \\hline Diffusion & \\\\ \\hline optimizer & AdamW \\\\ learning rate & 1e-3 \\\\ training iterations & 30, 000 \\\\ optimizer momentum & betas=(0.9, 0.999) \\\\ weight decay & 2e-6 \\\\ ema \\(\\beta\\) & 0.9999 \\\\ betas start & 1e-4 \\\\ betas end & 2e-2 \\\\ betas schedule & linear \\\\ \\(T\\), _i.e._, maximum time steps in the training stage & 1000 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: CIFAR100 데이터세트 및 ResNet-18 백본을 기반으로 한 우리의 기본 훈련 레시피.\n' +
      '\n' +
      '###코드에 대한 지침\n' +
      '\n' +
      '우리는 재생산을 위해 \'p-diff.zip\'로 명명된 지퍼 파일에 소스 코드를 보충 자료로 제출했다. 코드를 실행하기 위한 명령어에 대해서도 README가 포함된다.\n' +
      '\n' +
      '## 부록 B 디자인 탐색 및 전략\n' +
      '\n' +
      '이 절에서는 접근법의 설계 및 전략에 대한 이유를 소개한다.\n' +
      '\n' +
      '왜 1D CNN이죠?\n' +
      '\n' +
      '시각 데이터와 신경망 매개변수 간의 큰 차이를 고려하여 매개변수 자동 인코더 및 생성에서 1D CNN을 사용하는 것을 기본값으로 한다. 1D CNN의 상세한 설계는 다음에서 찾을 수 있다. 1D CNN의 각 계층은 정규화 계층 및 활성화 계층을 갖는 2개의 1D 컨볼루션 계층을 포함한다. 1D CNN에 대한 자세한 내용은 코드 zip 파일에서 _코어/모듈/모듈_에서 찾을 수 있다.\n' +
      '\n' +
      '여기서 자연스럽게 의문이 제기된다: 1D CNN에 대한 대안이 있는가? 우리는 순수 완전 연결(FC) 층을 대안으로 사용할 수 있다. 이 질문에 답하기 위해 FC 레이어와 1D CNN의 성능을 비교한다. 실험은 ConvNet-3을 백본으로 하여 MNIST에서 수행된다. Tab의 실험 결과를 기반으로 합니다. 5, 1D CNN은 모든 아키텍처에서 일관되게 FC를 능가합니다. 한편, 1D CNN의 메모리 점유율은 FC보다 작다.\n' +
      '\n' +
      '변분 오토인코더가 우리의 접근법에 대한 대안인가요?\n' +
      '\n' +
      'VAE(Variational Autoencoder)(Cristianini et al., 2000)는 확률적 생성 모델로 간주될 수 있으며, 생성 영역에서 많은 주목할 만한 결과를 얻을 수 있다. 또한 신경망 파라미터를 생성하기 위해 VAE를 구현한다. 우리는 먼저 우리의 실험에서 VAE의 세부 사항을 소개한다. 공정한 비교를 위해 p-diff에서 오토인코더의 동일한 백본을 사용하여 바닐라 VAE를 구현한다. 다른 \\(K\\)의 경우에 VAE 발생기를 평가하고 p-diff 생성 모델과 가장 우수한 성능, 평균 및 중간 성능을 비교한다. 탭의 결과를 기준으로 합니다. 7, 우리의 접근법은 모든 경우에 큰 마진으로 VAE를 능가한다. 또 다른 흥미로운 발견은 VAE 생성 모델의 평균 성능이 원래 모델의 수가 증가함에 따라 감소한다는 것이다.\n' +
      '\n' +
      '어떤 정규화 전략이 적합한가요?\n' +
      '\n' +
      '이미지와 신경망 매개변수의 본질적인 차이를 고려하여 서로 다른 정규화 전략의 영향을 탐구한다. CIFAR-10, MNIST 및 CIFAR-100에서 각각 배치 정규화(BN), 그룹 정규화(GN) 및 인스턴스 정규화(IN)를 제거한다. 우리는 또한 추가 정규화 없이 방법을 구현합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 6: 우리의 접근법에서 배치 정규화, 그룹 정규화 및 인스턴스 정규화를 사용하는 것의 비교. 또한 정상화 없이 결과를 보고한다. ‘norm.’은 정규화를 의미한다. 기본 설정은 회색으로 표시됩니다. **굵은 항목**이 가장 좋은 결과입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 5: 1D CNN과 완전 연결(FC) 레이어를 사용한 비교. 1D CNN은 특히 메모리와 시간에서 FC 레이어보다 더 좋은 성능을 보인다.\n' +
      '\n' +
      '비교. 100개의 생성 모델 중 가장 우수한 성능, 평균 및 중간 성능을 Tab. 6에 보고하였으며, 그 결과를 바탕으로 다음과 같은 관찰 결과를 얻었다. 1) BN은 세 가지 메트릭 모두에서 최악의 전체 성능을 얻었다. BN은 배치차원에서 동작하고 모델 파라미터들 사이에 원하지 않는 상관관계를 도입하기 때문에, GN과 IN은 정규화가 없는 것보다 더 좋은 성능을 나타내므로, Tab. 6에서 _i.e._\'no norm\'은 성능에 많은 영향을 미치는 일부 이상치 파라미터들에 의해 설명될 수 있다. 3) 본 논문에서 제안하는 방법은 GN과 IN과 같은 채널별 정규화 연산 중에서 일반화가 잘 이루어짐을 보인다.\n' +
      '\n' +
      '## 부록 C 더 많은 정리\n' +
      '\n' +
      '이 섹션에서는 우리의 방법에 대한 더 많은 절제 연구를 소개한다. 본 논문과 동일하게 달리 명시되지 않은 경우 CIFAR-100에서 ResNet-18을 훈련하는 것을 기본화하고 최상의, 평균 및 중간 정확도를 보고한다.\n' +
      '\n' +
      '### 입력 파라미터에 첨가되는 잡음의 강도\n' +
      '\n' +
      '본 논문에서는 추가된 잡음의 효과를 입력 파라미터로 축소한다. 여기서는 이 잡음의 세기가 미치는 영향을 연구한다. 구체적으로, 우리는 4가지 수준의 노이즈 강도를 탐색하고 탭에서 최고, 평균 및 중간 결과를 보고한다. 8(a) 우리는 우리의 기본 강도가 최고의 전체 성능을 달성한다는 것을 찾을 수 있습니다. 너무 큰 노이즈 강도와 너무 작은 노이즈 강도 모두 좋은 결과를 얻지 못한다. 그것은 너무 큰 노이즈가 파라미터의 원래 분포를 파괴할 수 있는 반면 너무 작은 노이즈는 증강의 충분한 효과를 제공할 수 없다는 것으로 설명될 수 있다.\n' +
      '\n' +
      '잠재적 표현에 부가된 잡음의 강도\n' +
      '\n' +
      'Sec. C.1과 유사하게, 우리는 또한 잠재 표현들에 추가된 잡음 강도를 제거한다. 탭에 표시된 대로입니다. 도 8의 (b)에 도시된 바와 같이, 생성된 모델들의 성능 안정성은 잡음 강도가 증가함에 따라 더 양호해진다. 그러나, 너무 큰 잡음은 또한 원래의 잠재 표현들의 분포를 깨뜨린다.\n' +
      '\n' +
      '### 다른 유형의 파라미터에 대한 일반화\n' +
      '\n' +
      '본 논문에서는 정규화 매개 변수를 생성하는 데 있어 접근법의 효율성을 조사한다. 또한 선형, 컨볼루션 및 바로 가기 계층과 같은 다른 유형의 매개변수에 대한 접근 방식을 평가한다. 여기, 우리는 세부 사항을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 7: VAE와 제안된 p-diff 간의 비교. VAE는 특히 평균 및 중간 정확도의 메트릭에서 우리의 접근법보다 더 나쁜 성능을 수행한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 8: 입력 잡음 \\(\\xi_{V}\\)과 잠재 잡음 \\(\\xi_{Z}\\)의 강도에 대한 어블레이션을 설계하여 변수의 변형 유형을 생성한다. ‘para.’는 파라미터를 나타낸다. 기본 설정은 회색으로 표시됩니다. **굵은 항목**이 가장 좋은 결과입니다.\n' +
      '\n' +
      '위의 세 가지 유형의 레이어 중 1) 선형 레이어: ResNet-18의 마지막 선형 레이어. 2) 컨볼루션 레이어: ResNet-18의 첫 번째 컨볼루션 레이어. 3) 바로 가기 레이어: ResNet-18의 7번째와 8번째 레이어 사이의 바로 가기 레이어. 훈련 데이터 준비는 본 논문에서 언급한 것과 동일하다. 탭에 표시된 대로입니다. 도 8을 참조하면, 본 논문에서 제안한 방법은 기존 모델들에 비해 지속적으로 유사하거나 향상된 성능을 보인다.\n' +
      '\n' +
      '## 부록 D 오픈 익스플로런스\n' +
      '\n' +
      '1000단계 확산 모델을 훈련해야 하나요?\n' +
      '\n' +
      '우리는 1000개의 시간 단계에서 무작위 샘플링을 통해 잠재 확산 모델을 훈련하는 것을 기본으로 한다. 훈련 단계에서 시간 스텝 수를 줄일 수 있을까요? 시간 단계의 영향을 연구하기 위해 절제를 수행하고 그 결과를 탭 9에 보고한다. 1) 너무 작은 시간 단계는 안정성이 좋은 고성능 모델을 생성할 만큼 충분히 강하지 않을 수 있다. 2) 최대 시간 단계를 100.3)으로 설정하여 최상의 안정성 성능을 얻는다. 최대 시간 단계를 1000에서 2000으로 늘리는 것은 성능을 향상시킬 수 없다. 우리는 이 탐색을 기반으로 디자인을 더욱 업그레이드할 것입니다.\n' +
      '\n' +
      '### Potential applications\n' +
      '\n' +
      '신경망 확산은 다음과 같은 잠재적 연구 영역을 활용하거나 도움을 줄 수 있다. 1) **파라미터 초기화**: 우리의 접근법은 높은 성능의 초기화된 파라미터를 생성할 수 있다. 따라서 최적화를 가속화하고 전체 교육 비용을 줄일 수 있습니다. 2) **도메인 적응**: 우리의 접근법은 도메인 적응 영역에서 세 가지 이점을 가질 수 있다. 먼저, 확산 과정을 직접 사용하여 서로 다른 도메인 데이터에 의해 훈련된 잘 수행된 모델을 학습할 수 있다. 둘째, 우리의 접근법으로 몇 가지 어려운 적응을 달성할 수 있다. 셋째, 적응 효율이 크게 향상될 수 있다.\n' +
      '\n' +
      '## 부록 E 기타 검색 및 비교 결과\n' +
      '\n' +
      '### 생성된 파라미터를 선택하는 방법?\n' +
      '\n' +
      'P-diff는 수많은 고성능 모델을 빠르게 생성할 수 있다. 우리는 이 모델들을 어떻게 평가하나요? 두 가지 주요 전략이 있습니다. 첫 번째는 _validation set_에서 직접 테스트하고 가장 성능이 좋은 모델을 선택하는 것입니다. 두 번째는 모델을 선택하기 위해 _training set_ 상의 Ground truth와 비교하여 모델 출력의 손실을 계산하는 것이다. 우리는 서로 다른 간격으로 성능 분포를 가진 100개의 모델 매개변수를 생성하고 그림의 훈련 및 검증 세트 모두에 정확도 곡선을 표시했다. 6(a). 실험 결과는 p-diff가 훈련과 검증 세트 사이에 높은 수준의 일관성을 나타냄을 나타낸다. 기준선 방법과 공정한 비교를 제공하기 위해 훈련 세트에서 최상의 결과를 수행하는 모델을 선택하고 기준선과 비교하는 것을 기본값으로 한다.\n' +
      '\n' +
      '### Parameter visualization\n' +
      '\n' +
      '보다 직관적인 이해를 제공하기 위해, 우리는 우리의 접근법, SGD 최적화(원본) 및 무작위 초기화에 의해 생성된 파라미터를 비교한다. ResNet-18을 예로 들어 그림 1의 CIFAR-100에 대한 훈련의 정규화 계층 매개변수의 평균, std, 정확도(acc.), IoU를 보고한다. 6(b). 본 논문에서 제안한 방법에 의해 생성된 파라미터들과 랜덤하게 초기화된 파라미터들(mean: 0.37 vs 0.36, std: 0.22 vs 0.21) 사이에는 상당한 차이가 있다. 본 논문에서 제안한 방법과 SGD 사이의 IoU는 0.87이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 9: 훈련 단계에서 최대 시간 스텝의 영향을 탐색한다. CIFAR-10, MNIST 및 CIFAR-100 데이터 세트에 대해 각각 실험을 수행한다. **굵은 항목**이 가장 좋은 결과입니다.\n' +
      '\n' +
      '고성능 파라미터와 랜덤 노이즈로부터 새로운 좋은 모델을 생성합니다. 더 중요한 것은 생성된 모델이 낮은 IoU 값에 반영되는 원래 모델에 비해 큰 행동 대비를 가지고 있다는 것이다.\n' +
      '\n' +
      '### 파라미터 생성의 효율성\n' +
      '\n' +
      '제안된 방법의 생성 효율을 평가하기 위해, 1) CIFAR-100 상의 ResNet-18과의 파라미터 확산, 2) ImageNet-1K의 ViT-Base와의 파라미터 확산의 경우에 대한 검증 정확도 곡선과 SGD 트레이닝을 비교한다. 우리는 공정한 비교를 위해 방법과 SGD를 위해 무작위 초기화된 매개변수를 활용한다. 도 1에 도시된 바와 같다. 도 7을 참조하면, 제안된 방법은 성능 저하 없이 SGD에 비해 최소 15\\(\\times\\)의 속도를 낼 수 있다. ImageNet-1K에서 우리는 바닐라 SGD 최적화와 비교할 때 44\\(\\times\\)의 속도를 높일 수 있으며, 이는 우리의 접근법을 대규모 훈련 데이터 세트에 적용할 때 더 중요한 잠재력을 보여준다.\n' +
      '\n' +
      '## 기타 작업에 대한 부록 F 일반화\n' +
      '\n' +
      '본 논문에서는 객체 검출, 의미론적 분할, 이미지 생성과 같은 다른 시각적 작업을 위한 방법을 구현한다. 실험 결과는 다양한 작업에 일반화하는 방법의 능력을 보여준다.\n' +
      '\n' +
      '### Object detection\n' +
      '\n' +
      'Faster R-CNN(Ren et al., 2015)은 객체 검출 태스크 상에서 Fast R-CNN을 개선하기 위해 전체 이미지 컨볼루션 특징들을 검출 네트워크와 공유하는 영역 제안 네트워크(RPN)를 이용한다. FCOS (Fully Convolutional One)\n' +
      '\n' +
      '그림 6: P-diff는 원래 모델과 비교하여 훈련 및 검증 세트 대비 모두에 대해 큰 일관성을 갖는 모델을 생성할 수 있다. (a)는 100개의 p-diff 모델에서 훈련 및 검증 세트의 정확도 분포를 나타낸다. (b) ResNet-18에서 정규화 레이어의 초기 SGD 훈련된 p-diff 생성 파라미터의 히트 맵을 표시한다.\n' +
      '\n' +
      '그림 7: 우리는 세 가지 경우에 대한 우리의 방법과 SGD의 정확도 곡선을 비교한다. (a): CIFAR-100 상의 ResNet-18. (b): ImageNet-1K 상의 ViT-Base. 우리의 접근 방식은 표준 SGD 프로세스보다 최소 15\\(\\times\\)의 속도를 높인다.\n' +
      '\n' +
      '스테이지)(Tian et al., 2020) 모델은 앵커 박스들의 필요성을 제거함으로써 검출 프로세스를 단순화하는 단일 스테이지 객체 검출 모델이다. 객체 검출 태스크에서는 토치/토치비전을 기반으로 COCO(Lin et al., 2014) 데이터셋에 ResNet-50 백본으로 Faster R-CNN(Ren et al., 2015)과 FCOS(Tian et al., 2020)를 구현한다. p-diff에 대한 데이터의 시간 비용을 고려하여 사전 훈련된 파라미터를 첫 번째 훈련 데이터로 직접 사용한 다음 미세 조정하여 다른 훈련 데이터를 얻는다. 권투 예측 변수의 매개변수는 p-diff에 의해 생성됩니다. 우리는 그 결과를 탭에 보고한다. 10. 본 논문에서 제안하는 방법은 초 단위로 기존 모델보다 유사하거나 더 우수한 성능을 가지는 모델들을 얻을 수 있다.\n' +
      '\n' +
      '### Semantic segmentation\n' +
      '\n' +
      'Fully Convolutional Network (FCN)(Long et al., 2015)은 픽셀 수준에서 이미지를 효율적으로 처리하고 분석하기 위해 설계되었으며, 이미지 내에서 객체의 의미론적 분할이 가능하다. 객체 검출의 접근법에 이어서, 우리는 Pascal VOC 데이터 세트에 존재하는 20개의 카테고리들에 대해 COCO val2017의 서브세트를 평가하기 위해 ResNet-50 백본과 함께 FCN(Long et al., 2015)을 사용하여 시맨틱 세분화 태스크를 구현한다. 우리는 백본의 매개변수의 하위 집합을 생성하고 그 결과를 탭에 보고한다. 11. 우리의 접근법은 시맨틱 세분화 태스크에서 고성능 신경망 파라미터를 생성할 수 있다.\n' +
      '\n' +
      '### Image generation\n' +
      '\n' +
      'DDPM(Ho et al., 2020)은 이미지 생성에서 확산 기반 방법으로서, UNet(Ronneberger et al., 2015)이 노이즈를 모델링하는데 사용된다. 이미지 생성 작업에서 우리는 p-diff를 사용하여 UNet의 모델 매개변수의 하위 집합을 생성한다. 비교를 위해 CIFAR-10 데이터 세트에서 p-diff 모델의 FID 점수를 평가하고 결과를 탭에 보고한다. 12. 가장 좋은 p-diff 생성 UNet은 원래 모델과 유사한 성능을 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline model/performance & best original mAP & best p-diff mAP \\\\ \\hline Faster R-CNN & 36.9 & 37.0 \\\\ FCOS & 39.1 & 39.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 객체 검출 태스크에서의 P-diff. 우리는 가장 좋은 원본 모델과 가장 좋은 p-diff 생성 모델의 mAP를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline model/performance & \\multicolumn{2}{c}{original} & \\multicolumn{2}{c}{p-diff} \\\\  & mean IoU & pixelwise acc. & mean IoU & pixelwise acc. \\\\ \\hline FCN & 60.5 & 91.4 & 60.7 & 91.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 시맨틱 세분화 태스크에서의 P-diff. 우리는 최상의 원본 모델과 최상의 p-diff 모델의 평균 IoU 및 픽셀별 정확도를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} \\hline \\hline model/performance & original FID & p-diff FID \\\\ \\hline DDPM UNet & 3.17 & 3.19 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 이미지 생성 태스크에서의 P-diff. 우리는 CIFAR-10 데이터 세트에 대한 FID 점수를 보고한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>