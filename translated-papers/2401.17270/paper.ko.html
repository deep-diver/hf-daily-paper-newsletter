<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 욜로-월드: 실시간 오픈 어휘 객체 검출\n' +
      '\n' +
      'Tianheng Cheng\\({}^{3,2,*}\\), Lin Song\\({}^{1,*,*}\\), Yixiao Ge\\({}^{1,2,\\dagger}\\), Wenyu Liu\\({}^{3}\\), Xinggang Wang\\({}^{3,*}\\), Ying Shan\\({}^{1,2}\\)\n' +
      '\n' +
      '({}^{*}\\)equal contribution \\({}^{*}\\)equal contribution \\({}^{\\dagger}\\) project lead \\({}^{*}\\) corresponding author\n' +
      '\n' +
      '({}^{1}\\)텐센트 AI Lab \\({}^{2}\\) ARC Lab, 텐센트 PCG\n' +
      '\n' +
      '화중과학기술대학교 EIC학부\n' +
      '\n' +
      'Code & Models: YOLO-World\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'YOLO(You Only Look Once) 시리즈의 탐지기는 효율적이고 실용적인 도구로 자리매김했습니다. 그러나 사전에 정의되고 훈련된 객체 범주에 대한 의존성은 개방형 시나리오에서의 적용 가능성을 제한한다. 이러한 한계를 해결하기 위해 우리는 비전 언어 모델링과 대규모 데이터 세트에 대한 사전 교육을 통해 개방형 어휘 탐지 기능으로 YOLO를 향상시키는 혁신적인 접근법인 YOLO-월드를 소개한다. 구체적으로, 우리는 시각 정보와 언어 정보 사이의 상호 작용을 용이하게 하기 위해 새로운 Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN)과 영역 텍스트 대비 손실을 제안한다. 제안하는 방법은 높은 효율로 제로 샷 방식으로 넓은 범위의 물체를 탐지하는 데 탁월하다. 도전적인 LVIS 데이터 세트에서 YOLO-World는 V100에서 52.0 FPS로 35.4 AP를 달성하여 정확도와 속도 면에서 많은 최첨단 방법을 능가한다. 또한, 미세 조정된 YOLO-World는 객체 검출 및 개방형 어휘 인스턴스 분할을 포함한 여러 다운스트림 태스크에서 현저한 성능을 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '객체 검출은 이미지 이해, 로봇 공학 및 자율 주행 차량에서 수많은 응용 프로그램으로 컴퓨터 비전 분야에서 오래되고 근본적인 과제였다. 대단한 작품[15, 26, 40, 42]은 심층 신경망의 발전으로 객체 검출에 있어 상당한 돌파구를 달성했다. 이러한 방법의 성공에도 불구하고 COCO [25] 데이터 세트에서 고정된 어휘인 _예: 80개의 범주로 객체 탐지만 처리하므로 제한적이다. 일단 객체 카테고리들이 정의되고 라벨링되면, 트레이닝된 검출기들은 단지 그러한 특정 카테고리들을 검출할 수 있고, 따라서 개방 시나리오들의 능력 및 적용가능성을 제한한다.\n' +
      '\n' +
      '최근 연구들[7, 12, 49, 547]은 언어 인코더에서 어휘 지식을 증류하여 개방형 어휘 탐지[54]를 다루기 위해 널리 퍼진 비전 언어 모델[18, 36]을 탐색했다. 그러나 이러한 증류 기반 방법은 48개의 기본 범주를 포함하는 제한된 어휘의 다양성, 즉 OV-COCO[54]를 가진 훈련 데이터의 부족으로 인해 매우 제한적이다. 여러 방법[23, 29, 52, 55, 55]은 객체 검출 훈련을 영역 수준의 비전 언어 사전 훈련으로 재구성하고 개방형 어휘 객체 검출기를 척도로 훈련한다. 그러나, 이러한 방법들은 (1) 무거운 계산 부담과 (2) 에지 디바이스들에 대한 복잡한 배치라는 두 가지 측면에서 고통받는 실제 시나리오들에서 검출을 위해 여전히 어려움을 겪는다. 이전 작품[23, 29, 52, 55, 55]은\n' +
      '\n' +
      '그림 1: **속도-정확도 곡선.** YOLO-World를 속도와 정확도의 측면에서 최근의 개방형 어휘 방법과 비교한다. 모든 모델은 LVIS 미니벌에서 평가되고 추론 속도는 하나의 NVIDIA V100 w/o 텐서RT에서 측정된다. 원의 크기는 모형의 크기를 나타낸다.\n' +
      '\n' +
      '개방형 인식 기능을 제공하기 위해 소형 탐지기를 사전 훈련하는 동안 대형 탐지기를 사전 훈련하는 유망한 성능을 입증했습니다.\n' +
      '\n' +
      '본 논문에서는 고효율 개방형 어휘 객체 탐지를 목표로 하는 YOLO-World를 제시하고, 기존의 YOLO 탐지기를 새로운 개방형 어휘 세계로 끌어올리기 위한 대규모 사전 훈련 기법을 탐색한다. 제안된 YOLO-World는 기존의 방법에 비해 추론 속도가 빠르고 다운스트림 응용에 쉽게 적용할 수 있는 장점이 있다. 구체적으로, YOLO-World는 표준 YOLO 아키텍처[19]를 따르고, 미리 트레이닝된 CLIP[36] 텍스트 인코더를 활용하여 입력 텍스트들을 인코딩한다. 또한 보다 나은 시각적 의미 표현을 위해 텍스트 특징과 이미지 특징을 연결하는 Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN)을 제안한다. 추론 동안, 텍스트 인코더는 제거될 수 있고, 텍스트 임베딩들은 효율적인 배치를 위해 RepVL-PAN의 가중치들로 재-파라미터화될 수 있다. 또한 탐지 데이터, 접지 데이터 및 이미지 텍스트 데이터를 영역-텍스트 쌍으로 통합하는 대규모 데이터 세트에 대한 영역-텍스트 대조 학습을 통해 YOLO 탐지기의 개방형 어휘 사전 훈련 방식을 조사한다. 풍부한 영역-텍스트 쌍을 갖는 사전 훈련된 YOLO-World는 큰 어휘 검출을 위한 강력한 능력을 보여주고 더 많은 데이터를 훈련시키면 개방 어휘 능력의 더 큰 개선으로 이어진다.\n' +
      '\n' +
      '또한, 실제 시나리오에서 개방형 어휘 객체 검출의 효율성을 더욱 향상시키기 위해 _prompt-then-detect_ 패러다임을 탐색한다. 도 1에 도시된 바와 같다. 도 2를 참조하면, 전통적인 객체 검출기[15, 19, 22, 38, 39, 40, 48]는 미리 정의되고 트레이닝된 카테고리를 갖는 고정-어휘(근접-세트) 검출에 집중한다. 이전의 개방형 어휘 검출기[23, 52, 55, 29]가 텍스트 인코더로 온라인 어휘에 대한 사용자의 프롬프트를 인코딩하고 객체를 검출한다. 특히, 이러한 방법들은 개방형 어휘 용량을 증가시키기 위해 무거운 백본, 예를 들어 Swin-L[31]을 갖는 대형 검출기를 사용하는 경향이 있다. 대조적으로, _prompt-then-detect_ paradigm(Fig. 도 2(c))는 먼저 오프라인 어휘를 구축하기 위해 사용자의 프롬프트를 인코딩하고 어휘는 상이한 요구에 따라 변한다. 그러면, 효율적인 검출기는 프롬프트를 재인코딩하지 않고 즉석에서 오프라인 어휘를 추론할 수 있다. 실제 응용의 경우, 일단 디텍터 _i.e_., YOLO-World를 훈련시킨 후, 프롬프트 또는 카테고리를 사전 인코딩하여 오프라인 어휘를 구축한 다음 디텍터에 원활하게 통합할 수 있다.\n' +
      '\n' +
      '우리의 주요 기여는 세 가지로 요약될 수 있다:\n' +
      '\n' +
      '* 실세계 응용을 위한 고효율의 최첨단 개방형 어휘 객체 검출기인 YOLO-World를 소개한다.\n' +
      '* 우리는 YOLO-World를 위한 비전과 언어 특징을 연결하는 Re-parameterizable Vision-Language PAN과 개방형 어휘 영역-텍스트 대조적 사전 학습 기법을 제안한다.\n' +
      '* 제안된 YOLO-World 사전 훈련된 대규모 데이터 세트는 강력한 제로 샷 성능을 보여주며 52.0 FPS로 LVIS에서 35.4 AP를 달성한다. 미리 훈련된 YOLO-World는 다운스트림 작업들, 예를 들어, 개방 어휘 인스턴스 분할 및 참조 객체 검출에 쉽게 적응될 수 있다. 또한, YOLO-World의 사전 훈련된 가중치 및 코드는 보다 실제적인 응용을 용이하게 하기 위해 오픈소싱될 것이다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '전통적 객체 검출\n' +
      '\n' +
      '공유 객체 검출 연구는 고정된 어휘(close-set) 검출에 중점을 두고 있으며, 객체 검출기는 미리 정의된 카테고리인 _e.g_., COCO 데이터세트[25] 및 객체365 데이터세트[43]를 가진 데이터세트에 대해 훈련된 다음 고정된 카테고리 세트 내의 객체를 검출한다. 지난 수십 년 동안 전통적인 객체 검출 방법은 단순히 영역 기반 방법, 픽셀 기반 방법 및 쿼리 기반 방법의 세 그룹으로 분류할 수 있다. Faster R-CNN[41]과 같은 영역 기반 방법[10, 11, 15, 26, 41]은 제안 생성[41]과 RoI-wise(Region-of-Interest) 분류 및 회귀를 위한 2단계 프레임워크를 채택한다. 픽셀 기반 방법[27, 30, 39, 45, 57]은 미리 정의된 앵커 또는 픽셀에 대해 분류 및 회귀를 수행하는 1단계 디텍터인 경향이 있다. DETR[1]은 먼저 변압기[46]를 통해 물체 탐지를 탐색하고 광범위한 질의 기반 방법[60]을 고무한다. 추론 속도 측면에서 Redmon _et al_. 실시간 객체 검출을 위해 간단한 컨볼루션 아키텍처를 이용하는 YOLO[37, 38, 39]를 제시한다. 여러 작업[22, 32, 48, 51]은 경로 집합 네트워크[28], 교차 단계 부분 네트워크[47], 및 재-파라미터화[6]를 포함하는 YOLO에 대한 다양한 아키텍처 또는 설계를 제안하며, 이는 속도와 정확도 모두를 더욱 향상시킨다. 본 논문의 YOLO-World는 기존의 YOLO와 비교하여 일반화 능력이 강한 고정 어휘를 넘어 물체를 탐지하는 것을 목표로 한다.\n' +
      '\n' +
      '### Open-Vabulary Object Detection\n' +
      '\n' +
      '개방 어휘 객체 검출(Open-Vocabulary Object Detection; OVD)[54]은 미리 정의된 범주를 넘어 객체를 검출하는 것을 목표로 하는 현대 객체 검출의 새로운 트렌드로 부상했다. 초기 작품[12]은 기본 클래스에 대한 탐지기를 훈련시키고 새로운(미지의) 클래스를 평가함으로써 표준 OVD 설정[54]을 따른다. 그럼에도 불구하고, 이러한 개방형 어휘 설정은 새로운 객체를 검출하고 인식하는 검출기의 능력을 평가할 수 있지만, 개방형 시나리오에 대해서는 여전히 제한적이며 제한된 데이터 세트 및 어휘에 대한 훈련으로 인해 다른 도메인에 대한 일반화 능력이 부족하다.\n' +
      '\n' +
      ' 비전 언어 사전 훈련[18, 36]에서 영감을 얻은 최근 작품 [7, 21, 49, 58, 59]는 개방형 어휘 객체 탐지를 이미지-텍스트 매칭으로 공식화하고 대규모 이미지-텍스트 데이터를 활용하여 훈련 어휘를 규모별로 증가시킨다. GLIP [23]은 어구 접지를 기반으로 개방형 어휘 검출을 위한 사전 훈련 프레임워크를 제시하고 제로 샷 설정에서 평가한다. 접지 DINO[29]는 접지된 사전 훈련[23]을 교차 모달리티 융합이 있는 검출 변압기[56]에 통합한다.\n' +
      '\n' +
      '여러 방법[52, 53, 24, 55]은 영역-텍스트 매칭과 대규모 이미지-텍스트 쌍을 갖는 사전 훈련 검출기를 통해 검출 데이터세트와 이미지-텍스트 데이터세트를 통합하여 유망한 성능과 일반화를 달성한다. 그러나 이러한 방법은 종종 Swin-L[31]을 백본으로 하는 ATSS[57] 또는 DINO[56]과 같은 무거운 검출기를 사용하여 높은 계산 요구 및 배치 문제를 초래한다. 이에 반해 YOLO-World는 실시간 추론과 보다 쉬운 다운스트림 애플리케이션 배포를 통해 효율적인 개방형 어휘 객체 탐지를 목표로 한다. 또한 언어 모델 정렬을 통해 YOLO와 함께 개방형 어휘 탐지[54]를 탐구하는 ZSD-YOLO[50]와 달리 YOLO-월드는 효과적인 사전 훈련 전략을 가진 새로운 YOLO 프레임워크를 도입하여 개방형 어휘 성능 및 일반화를 향상시킨다.\n' +
      '\n' +
      '도 3: **YOLO-World의 전체 아키텍처.** 전통적인 YOLO 검출기와 비교하여, 개방 어휘 검출기로서의 YOLO-World는 텍스트를 입력으로 채택한다. _Text Encoder_는 먼저 입력 텍스트 입력 텍스트 임베딩을 인코딩한다. 이후 _Image Encoder_는 입력 영상을 다중 스케일 영상 특징으로 인코딩하고, 제안된 _RepVL-PAN_는 영상과 텍스트 특징 모두에 대해 다중 레벨 교차 모달리티 융합을 이용한다. 마지막으로 YOLO-World는 입력 텍스트에 나타난 카테고리 또는 명사를 매칭하기 위한 회귀 바운딩 박스와 객체 임베딩을 예측한다.\n' +
      '\n' +
      '도 2: ** Detection Paradigms.****(a) Traditional Object Detector**: 이러한 Object Detector는 훈련 데이터세트, _e.g._, 80 카테고리의 COCO 데이터세트에 의해 미리 정의된 고정 어휘 내의 오브젝트만을 검출할 수 있다[25]. 고정된 어휘는 열린 장면에 대한 확장을 제한한다. **(b) 이전의 Open-Vocabulary Detector:** 이전의 방법들은 직관적으로 강한 용량을 갖는 Open-Vocabulary 검출을 위한 크고 무거운 검출기를 개발하는 경향이 있다. 또한, 이러한 검출기는 예측을 위한 입력으로서 이미지 및 텍스트를 동시에 인코딩하며, 이는 실제 적용에 시간이 많이 소요된다. **(c) YOLO-World:** 경량 검출기, _e.g._, YOLO 검출기 [19, 39]의 강력한 개방형 어휘 성능을 입증하며, 이는 실제 응용에 큰 의미를 갖는다. 온라인 어휘를 사용하는 것이 아니라 사용자가 필요에 따라 일련의 프롬프트를 생성하고 프롬프트가 오프라인 어휘로 인코딩되는 효율적인 추론을 위한 _prompt-then-detect_ 패러다임을 제시한다. 그런 다음 배치 및 추가 가속을 위한 모델 가중치로 다시 매개변수를 지정할 수 있습니다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### 사전 훈련 공식화: 영역-텍스트 쌍\n' +
      '\n' +
      'YOLO-series [19]를 포함한 기존의 객체 검출 방법은 경계 박스(\\{B_{i}\\)와 카테고리 레이블(\\{c_{i}\\}\\)로 구성된 인스턴스 주석(\\(\\Omega=\\{B_{i},c_{i}\\}_{i=1}^{N}\\)으로 학습된다. 본 논문에서는 인스턴스 주석을 영역-텍스트 쌍 \\(\\Omega=\\{B_{i},t_{i}\\}_{i=1}^{N}\\)으로 재구성하고, 여기서 \\(t_{i}\\)은 영역 \\(B_{i}\\)에 해당하는 텍스트이다. 구체적으로, 텍스트 \\(t_{i}\\)는 카테고리 이름, 명사구 또는 객체 설명일 수 있다. 또한 YOLO-World는 이미지\\(I\\)와 텍스트\\(T\\)(명사 집합)을 입력으로 채택하고 예측 박스\\(\\hat{B}_{k}\\}\\)과 해당 객체 임베딩\\(\\{e_{k}\\}\\)(\\(e_{k}\\in\\mathbb{R}^{D}\\))을 출력한다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '제안된 YOLO-World의 전체 아키텍처는 그림 1에 나와 있다. 도 3을 참조하면, _YOLO detector_, _Text Encoder_ 및 _Re-parameterizable Vision-Language Path Aggregation Network_(RepVL-PAN)로 구성된다. 입력된 텍스트가 주어지면, YOLO-World의 텍스트 인코더는 텍스트를 텍스트 임베딩으로 인코딩한다. YOLO 검출기의 영상 부호화기는 입력 영상으로부터 다중 스케일 특징을 추출한다. 그런 다음 RepVL-PAN을 활용하여 이미지 특징과 텍스트 임베딩 간의 교차 모달리티 융합을 활용하여 텍스트와 이미지 표현을 모두 향상시킨다.\n' +
      '\n' +
      'YOLO Detector.YOLO-World는 주로 YOLOv8[19]을 기반으로 개발되며, 영상 인코더로서 다크넷 백본[19, 40], 다중 스케일 피쳐 피라미드를 위한 경로 집합 네트워크(PAN), 바운딩 박스 회귀 및 객체 임베딩을 위한 헤드를 포함한다.\n' +
      '\n' +
      '텍스트 인코더.텍스트\\(T\\)이 주어지면 CLIP[36]에 의해 미리 훈련된 트랜스포머 텍스트 인코더를 채택하여 해당 텍스트 임베딩 \\(W\\!=\\!\\!\\!=\\!\\! texttt{TextEncoder}(T)\\!\\ in\\\\ mathb{R}^{C\\!\\times\\!D} )에서 \\(C\\)은 명사의 수이고 \\(D\\)은 임베딩 차원이다. CLIP 텍스트 인코더는 텍스트 전용 언어 인코더에 비해 시각적 객체를 텍스트와 연결하는 데 더 나은 시각적 의미 기능을 제공한다[5]. 입력된 텍스트가 자막 또는 참조 표현인 경우, 간단한 n-gram 알고리즘을 채택하여 명사구를 추출한 후 텍스트 인코더에 공급한다.\n' +
      '\n' +
      '텍스트 대조 헤드(Text Contrastive Head.19)에 이어, 경계 박스(\\{b_{k}\\}_{k=1}^{K}\\)와 객체 임베딩(\\{e_{k}\\}_{k=1}^{K}\\)을 회귀하기 위해 두 개의 (3\\times 3\\) convs를 갖는 디커플드 헤드를 채택하였으며, 여기서 \\(K\\)은 객체의 수를 나타낸다. 본 논문에서는 객체-텍스트 유사도 \\(s_{k,j}\\)를 얻기 위한 텍스트 대조 헤드를 제시한다.\n' +
      '\n' +
      '\\[s_{k,j}=\\alpha\\cdot\\texttt{L2-Norm}(e_{k})\\cdot\\texttt{L2-Norm}(w_{j})^{ \\top}+\\beta, \\tag{1}\\]\n' +
      '\n' +
      '여기서, L2-Norm\\((\\cdot)\\)는 L2 정규화이고, \\(w_{j}\\in W\\)는 \\(j\\)번째 텍스트 임베딩이다. 또한 학습 가능한 스케일링 팩터 \\(\\alpha\\)와 이동 팩터 \\(\\beta\\)으로 아핀 변환을 추가한다. L2 규범과 아핀 변환은 모두 지역 텍스트 훈련을 안정화하는 데 중요하다.\n' +
      '\n' +
      '온라인 어휘를 이용한 학습.학습 중 4개의 이미지가 포함된 모자이크 샘플마다 온라인 어휘\\(T\\)을 구성한다. 구체적으로 모자이크 이미지에 포함된 모든 긍정 명사를 샘플링하고 해당 데이터셋에서 일부 부정 명사를 무작위로 샘플링한다. 각 모자이크 샘플에 대한 어휘는 최대 \\(M\\) 명사를 포함하고 \\(M\\)은 기본값으로 80으로 설정된다.\n' +
      '\n' +
      '오프라인 어휘를 이용한 추론.추론 단계에서는 보다 효율적인 오프라인 어휘를 이용한 _prompt-then-detect_ 전략을 제시한다. 도 1에 도시된 바와 같다. 도 3을 참조하면, 사용자는 캡션 또는 카테고리를 포함할 수 있는 일련의 사용자 지정 프롬프트를 정의할 수 있다. 그런 다음 텍스트 인코더를 사용하여 이러한 프롬프트를 인코딩하고 오프라인 어휘 임베딩을 얻는다. 오프라인 어휘는 각 입력에 대한 계산을 피할 수 있게 하고 필요에 따라 어휘를 조정할 수 있는 유연성을 제공한다.\n' +
      '\n' +
      '### 재매개변수화 가능한 시각언어 PAN\n' +
      '\n' +
      '도. 도 4는 제안된 RepVL-PAN의 구조를 [19, 28]에서 하향 및 하향 경로를 따라 다중 스케일 이미지 특징을 갖는 특징 피라미드 \\(\\{P_{3},P_{4},P_{5}\\}\\)을 설정하는 구조를 보여준다. 또한, 개방형 어휘 능력을 위한 시각적 의미 표현을 향상시킬 수 있는 이미지 특징과 텍스트 특징의 상호 작용을 더욱 향상시키기 위해 텍스트 유도 CSPLayer(T-CSPLayer)와 이미지 풀링 어텐션(I-Pooling Attention)을 제안한다. 추론 동안, 오프라인 어휘 임베딩들은 배치를 위해 컨볼루션 또는 선형 계층들의 가중치들로 재-파라미터화될 수 있다.\n' +
      '\n' +
      '텍스트 유도 CSPLayer. 도 4는 크로스-스테이지 부분 층들(CSPLayer)이 탑-다운 또는 바텀-업 융합 후에 활용되는 것을 예시한다. 텍스트 안내를 다중 스케일 이미지 피쳐에 통합하여 [19]의 CSPLayer(C2f라고도 함)를 확장하여 텍스트 안내 CSPLayer를 구성한다. 구체적으로, 텍스트 임베딩과 이미지 특징\\(X_{l}\\in\\mathbb{R}^{H\\times W\\times D}\\)(\\(l\\in\\{3,4,5\\}\\)), 마지막 어두운 병목 블록 뒤에 _max-sigmoid attention_를 적용하여 텍스트 특징을 이미지 특징으로 통합한다.\n' +
      '\n' +
      '\\[X_{l}^{\\prime}=X_{l}\\cdot\\delta(\\max_{j\\in\\{1..C\\}}(X_{l}W_{j}^{\\top}))^{\\top}, \\tag{2}\\]\n' +
      '\n' +
      '여기서, 업데이트된 \\(X_{l}^{\\prime}\\)은 출력으로서 크로스-스테이지 특징들과 연결된다. \\(\\delta\\)는 시그모이드 함수를 나타낸다.\n' +
      '\n' +
      '이미지-풀링 어텐션.이미지 인식 정보로 텍스트 임베딩을 향상시키기 위해 이미지-풀링 어텐션을 제안하여 텍스트 임베딩을 갱신하기 위한 이미지 특징을 통합한다. 이미지 특징에서 교차 주의력을 직접 사용하는 것이 아니라 다중 스케일 특징에서 max pooling을 이용하여 \\(3\\times 3\\) 영역을 얻음으로써 총 27개의 패치 토큰 \\(\\tilde{X}\\in\\mathbb{R}^{27\\times D}\\)이 생성되었다. 그 다음, 텍스트 임베딩들은 다음과 같이 업데이트된다:\n' +
      '\n' +
      '\\[W^{\\prime}=W+\\texttt{MultiHead-Attention}(W,\\tilde{X},\\tilde{X}) \\tag{3}\\]\n' +
      '\n' +
      '### Pre-training Schemes\n' +
      '\n' +
      '이 섹션에서는 대규모 탐지, 접지 및 이미지 텍스트 데이터 세트에 대해 YOLO-World를 사전 훈련하기 위한 훈련 방법을 제시한다.\n' +
      '\n' +
      '영역-텍스트 대비 손실 학습. 모자이크 샘플 \\(I\\)과 텍스트 \\(T\\)이 주어지면 YOLO-World 출력 \\(K\\) 객체 예측 \\(\\{B_{k},s_{k}\\}_{k=1}^{K}\\)과 주석 \\(\\Omega=\\{B_{i},t_{i}\\}_{i=1}^{N}\\)과 함께 \\(\\{B_{k},s_{k}\\}_{k=1}^{K}\\) 객체 예측 [19]와 레버리지 작업 정렬 레이블 할당[8]을 따라 예측을 지상-진실 주석과 일치시키고 텍스트 인덱스를 분류 레이블로 사용하여 각 긍정적인 예측을 할당한다. 이 어휘를 기반으로 객체-텍스트(영역-텍스트) 유사도와 객체-텍스트 할당 사이의 교차 엔트로피를 통해 영역-텍스트 쌍으로 영역-텍스트 대조 손실\\(\\mathcal{L}_{\\text{con}}\\)을 구성한다. 또한 바운딩 박스 회귀를 위해 IoU 손실과 분산 초점 손실을 채택하였으며, 총 훈련 손실은 \\(\\mathcal{L}(I)=\\mathcal{L}_{\\text{con}+\\lambda_{I}\\cdot(\\mathcal{L}_{\\text{iou}+\\mathcal{L}_{\\text{dil})\\)으로 정의하였으며, 여기서 \\(\\lambda_{I}\\)은 입력 영상\\(I\\)이 검출 또는 접지 데이터일 때 1로 설정되고 영상-텍스트 데이터일 때 0으로 설정된다. 이미지 텍스트 데이터 세트에 잡음이 있는 상자를 고려하여 정확한 경계 상자를 가진 샘플에 대한 회귀 손실만 계산한다.\n' +
      '\n' +
      '이미지-텍스트 데이터를 이용한 의사 레이블링 사전 학습에 이미지-텍스트 쌍을 직접 사용하기보다는 영역-텍스트 쌍을 생성하는 자동 레이블링 방법을 제안한다. 구체적으로, 레이블링 방법은 (1) 명사구 추출_: 먼저 n-gram 알고리즘을 이용하여 텍스트로부터 명사구를 추출하는 단계; (2) _pseudo labeling_: 미리 학습된 오픈 어휘 검출기 _e.g_., GLIP [23]을 채택하여 각 이미지에 대해 주어진 명사구에 대한 의사 박스를 생성하여 거친 영역-텍스트 쌍을 제공하는 단계를 포함한다. (3) _filtering_: 이미지-텍스트 쌍들 및 영역-텍스트 쌍들의 관련성을 평가하기 위해 사전 훈련된 CLIP[36]을 채용하고, 저-관련 의사 주석들 및 이미지들을 필터링한다. 비최대 억제(Non-Maximum Suppression, NMS)와 같은 방법을 추가하여 중복 경계 상자를 필터링한다. 자세한 접근 방법은 독자들이 부록을 참조할 것을 제안한다. 위의 접근법으로 CC3M[44]에서 821k 의사 주석이 있는 246k 이미지를 샘플링하고 레이블링한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 제안된 YOLO-World를 대규모 데이터 세트에서 사전 훈련하고 LVIS 벤치마크와 COCO 벤치마크 모두에서 제로 샷 방식으로 YOLO-World를 평가함으로써 제안된 YOLO-World의 효과를 입증한다(Sec. 4.2). 또한 객체 검출을 위한 COCO, LVIS에서 YOLO-World의 미세 조정 성능을 평가한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'YOLO-World는 MMYOLO 툴박스[3]와 MMDetection 툴박스[2]를 기반으로 개발되었다. [19]에 이어, 우리는 다양한 지연 요구 사항(예: 소형(S), 중형(M) 및 대형(L)에 대한 YOLO-World의 세 가지 변형을 제공한다. 입력 텍스트를 인코딩하기 위해 사전 훈련된 가중치를 갖는 오픈 소스 CLIP[36] 텍스트 인코더를 채택한다. 명시되지 않는 한, 우리는 추가 가속 메커니즘인 _g_, FP16 또는 텐서RT 없이 하나의 NVIDIA V100 GPU에서 모든 모델의 추론 속도를 측정한다.\n' +
      '\n' +
      '### Pre-training\n' +
      '\n' +
      '실험적 설정, 사전 학습 단계에서는 초기 학습률 0.002, 가중치 감소 0.05인 AdamW 최적화기[33]를 채택하였고, YOLO-World는 총 배치 크기 512인 32개의 NVIDIA V100 GPU에서 100 에폭에 대해 사전 학습하였으며, 사전 학습 단계에서는 이전 연구 [19]를 따라 4개의 이미지로 컬러 증강, 랜덤 아핀, 랜덤 플립, 모자이크를 채택하였다. 사전 교육 중에 텍스트 인코더가 동결됩니다.\n' +
      '\n' +
      '도 4: RepVL-PAN.**의 **Illustration.** 제안된 RepVL-PAN은 이미지 특징에 언어 정보를 주입하는 _Text-Guided CSPLayer_(T-CSPLayer)와 이미지 인식 텍스트 임베딩을 향상시키는 _Image Pooling Attention_(I-Pooling Attention)을 채택한다.\n' +
      '\n' +
      'Pre-training Data.Pre-training YOLO-World의 경우, Tab. 1에 명시된 바와 같이 Objects365(V1)[43], GQA[16], Flickr30k[35]를 포함하는 탐지 또는 접지 데이터 셋을 주로 채택한다. [23]에 이어서 GoldG[20](GQA 및 Flickr30k)의 COCO 데이터 셋에서 이미지를 제외한다. 사전 훈련에 사용되는 탐지 데이터 세트의 주석에는 경계 상자와 범주 또는 명사 구가 모두 포함되어 있다. 또한 3.4절에서 논의된 의사 레이블링 방법을 통해 246k개의 이미지를 레이블링한 이미지-텍스트 쌍인 _i.e_, CC3M\\({}^{\\dagger}\\)[44]로 사전 학습 데이터를 확장한다.\n' +
      '\n' +
      'Zero-shot Evaluation.Pre-training 후 LVIS 데이터셋 [13]에서 제안한 YOLO-World를 zero-shot 방식으로 직접 평가한다. LVIS 데이터세트에는 1203개의 객체 카테고리가 포함되어 있으며, 이는 사전 훈련 탐지 데이터세트의 카테고리보다 훨씬 많으며 대용량 어휘 탐지에 대한 성능을 측정할 수 있다. 이전 작업 [20, 23, 52, 53]에 이어 LVIS 미니볼[20]을 주로 평가하고 비교를 위해 _Fixed AP_[4]를 보고한다. 최대 예측 횟수는 1000으로 설정된다.\n' +
      '\n' +
      'LVIS 객체 탐지에 대한 주요 결과.탭에서. 도 2를 참조하면, 제안된 YOLO-World를 LVIS 벤치마크에서 제로 샷 방식으로 최근의 최신 방법[20, 29, 52, 53, 55]과 비교한다. 계산 부담과 모델 매개변수를 고려하여, 우리는 주로 더 가벼운 백본, 즉 Swin-T[31]를 기반으로 한 방법과 비교한다. 놀랍게도, YOLO-World는 제로 샷 성능 및 추론 속도 면에서 이전의 최신 방식보다 우수하다. 더 많은 데이터(_e.g_., Cap4M(CC3M+SBU[34])를 통합한 GLIP, GLIPv2, Grounding DINO와 비교하여 O365 & GolG에서 사전 훈련된 YOLO-World는 더 적은 모델 파라미터로도 더 나은 성능을 얻는다. YOLO-World는 DetCLIP와 비교하여 추론 속도의 \\(20\\times\\) 증가를 얻으면서 비슷한 성능(35.4 _v.s._ 34.4)을 달성한다. _ 실험 결과는 또한 13M 파라미터를 갖는 작은 모델들(예를 들어, YOLO-World-S)이 비전 언어 사전 훈련에 사용될 수 있고 강력한 개방 어휘 능력을 얻을 수 있음을 보여준다.\n' +
      '\n' +
      '### Ablation Experiments\n' +
      '\n' +
      '우리는 YOLO-World를 사전 훈련과 아키텍처의 두 가지 주요 측면에서 분석하기 위해 광범위한 절제 연구를 제공한다. 명시되지 않는 한, 우리는 LVIS 미니벌에 대한 제로 샷 평가와 함께 YOLO-World-L 및 사전 훈련 객체365를 기반으로 절제 실험을 주로 수행한다.\n' +
      '\n' +
      '데이터 사전 교육.탭에서. 셋째, 다양한 데이터를 사용하여 YOLO-World 사전 훈련의 성능을 평가한다. Objects365에서 훈련된 기준선과 비교하여 GQA를 추가하면 LVIS에서 8.4 AP 이득으로 성능을 크게 향상시킬 수 있다. 이러한 개선은 GQA 데이터세트가 제공하는 보다 풍부한 텍스트 정보에 기인할 수 있으며, 이는 큰 어휘 객체를 인식하는 모델의 능력을 향상시킬 수 있다. CC3M 샘플의 일부(전체 데이터 세트의 8%)를 추가하면 희귀 객체에 대해 1.3 AP로 0.5 AP 이득을 추가로 가져올 수 있다. 탭 도 3은 더 많은 데이터를 추가하는 것이 대규모 어휘 시나리오에서 탐지 능력을 효과적으로 향상시킬 수 있음을 입증한다. 또한 데이터의 양이 증가함에 따라 성능이 계속 향상되어 훈련을 위해 더 크고 다양한 데이터 세트를 활용하는 이점이 강조된다.\n' +
      '\n' +
      'RepVL-PAN.Tab의 어블레이션. 도 4는 제로 샷 LVIS 검출을 위한 텍스트 유도 CSPLayers 및 이미지 풀링 어텐션을 포함하는 YOLO-World의 제안된 RepVL-PAN의 유효성을 입증한다. 구체적으로, 우리는 (1) O365에 대한 사전 훈련과 (2) O365 & GQA에 대한 사전 훈련의 두 가지 설정을 채택한다. 카테고리 주석만 포함하는 O365에 비해 GQA는 특히 명사구 형태의 풍부한 텍스트를 포함한다. 탭에 표시된 대로입니다. 도 4를 참조하면, 제안된 RepVL-PAN은 LVIS에서 기준선(YOLOv8-PAN[19])을 1.1 AP 향상시켰으며, 검출 및 인식이 어려운 LVIS의 희귀 범주(AP\\({}_{r}\\)) 측면에서 개선 효과가 두드러진다. 또한, YOLO-World가 GQA 데이터셋으로 사전 학습될 때 개선점이 더 크게 나타나며, 실험을 통해 제안된 RepVL-PAN이 풍부한 텍스트 정보와 함께 더 잘 작동함을 알 수 있다.\n' +
      '\n' +
      '문자 인코더.탭에 있습니다. 도 5를 참조하면, 서로 다른 텍스트 인코더들, _i.e_., BERT-base[5] 및 CLIP-base(ViT-base)[36]을 사용하는 성능을 비교한다. 본 논문에서는 사전학습(Pre-training, _i.e_, frozen and fine-tuned)의 두 가지 설정을 이용하였으며, 텍스트 인코더의 학습률은 기본 학습률의 \\(0.01\\times\\) 요소이다. 탭으로 도 5에 도시된 바와 같이, CLIP 텍스트 인코더는 이미지-텍스트 쌍으로 사전 트레이닝되고 비전-중심 임베딩을 위한 더 나은 능력을 갖는 BERT(LVIS에서 희귀 카테고리에 대해 +10.1 AP)보다 우수한 결과를 획득한다. 사전 훈련 동안 BERT를 미세 조정하는 것은 상당한 개선(+3.7 AP)을 가져오는 반면 CLIP를 미세 조정하는 것은 심각한 성능 저하를 초래한다. 우리는 그것으로 낙하를 돌린다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline Dataset & Type & Vocab. & Images & Anno. \\\\ \\hline Objects365V1 [43] & Detection & 365 & 609k & 9,621k \\\\ GQA [16] & Grounding & - & 621k & 3,681k \\\\ Flickr [35] & Grounding & - & 149k & 641k \\\\ CC3M\\({}^{\\dagger}\\)[44] & Image-Text & - & 246k & 821k \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **사전 훈련 데이터. O365에서 YOLO-World.** 미세 조정에 사용되는 데이터 세트의 사양은 365개 범주만 포함하고 풍부한 텍스트 정보가 부족한 사전 훈련된 CLIP의 일반화 능력을 저하시킬 수 있다.\n' +
      '\n' +
      '### Fine-tuning YOLO-World\n' +
      '\n' +
      '이 섹션에서는 사전 교육의 효과를 입증하기 위해 COCO 데이터 세트 및 LVIS 데이터 세트에서 근접 세트 객체 검출을 위해 YOLO-World를 추가로 미세 조정한다.\n' +
      '\n' +
      '실험적 설정.사전 훈련된 가중치를 이용하여 YOLO-World를 초기화하여 미세 조정을 수행한다. 모든 모델은 AdamW 최적화기를 사용하여 80시대를 대상으로 미세 조정하였으며, 초기 학습률은 0.0002로 설정하였으며, LVIS 데이터 셋의 경우 기존 연구 [7, 12, 59]와 LVIS 기반(common & frequent)에서 YOLO-World를 미세 조정한 후 LVIS-novel(희귀)에서 평가하였다.\n' +
      '\n' +
      'COCO 객체 검출.Tab.6의 사전 훈련된 YOLO-World와 기존의 YOLO 검출기 [19, 22, 48]를 비교하였으며, COCO 데이터 셋의 미세 조정을 위해 COCO 데이터 셋의 어휘 크기가 작음을 고려하여 추가 가속을 위해 제안된 RepVL-PAN을 제거하였다. 탭에서 도 6에 도시된 바와 같이, 본 논문에서 제안한 방법은 COCO 데이터 셋에서 우수한 제로샷 성능을 얻을 수 있음을 알 수 있으며, 이는 YOLO-World가 강력한 일반화 능력을 가지고 있음을 나타낸다. 또한, COCO 트레인 2017의 미세 조정 후 욜로월드가 보여주고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{GQA} & T\\(\\rightarrow\\)I & I\\(\\rightarrow\\)T & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline ✗ & ✗ & ✗ & 22.4 & 14.5 & 20.1 & 26.0 \\\\ ✗ & ✓ & ✗ & 23.2 & 15.2 & 20.6 & 27.0 \\\\ ✗ & ✓ & ✓ & 23.5 & 16.2 & 21.1 & 27.0 \\\\ \\hline ✓ & ✗ & ✗ & 29.7 & 21.0 & 27.1 & 33.6 \\\\ ✓ & ✓ & ✓ & **31.9** & **22.5** & **29.9** & **35.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **Re-parameterizable Vision-Language Path Aggregation Network 상의 Ablations.** 제안된 Vision-Language Path Aggregation Network의 LVIS 상의 제로샷 성능을 평가한다. T\\(\\rightarrow\\)I와 I\\(\\rightarrow\\)T는 각각 텍스트 유도 CSPLayers와 Image-Pooling Attention을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Method & Backbone & Params & Pre-trained Data & FPS & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline MDETR [20] & R-101 [14] & 169M & GoldG & - & 24.2 & 20.9 & 24.3 & 24.2 \\\\ GLIP-T [23] & Swin-T [31] & 232M & O365,GoldG & 0.12 & 24.9 & 17.7 & 19.5 & 31.0 \\\\ GLIP-T [23] & Swin-T [31] & 232M & O365,GoldG,Cap4M & 0.12 & 26.0 & 20.8 & 21.4 & 31.0 \\\\ GLIPv2-T [55] & Swin-T [31] & 232M & O365,GoldG & 0.12 & 26.9 & - & - & - \\\\ GLIPv2-T [55] & Swin-T [31] & 232M & O365,GoldG,Cap4M & 0.12 & 29.0 & - & - & - \\\\ Grounding DINO-T [29] & Swin-T [31] & 172M & O365,GoldG & 1.5 & 25.6 & 14.4 & 19.6 & 32.2 \\\\ Grounding DINO-T [29] & Swin-T [31] & 172M & O365,GoldG,Cap4M & 1.5 & 27.4 & 18.1 & 23.3 & 32.7 \\\\ DetCLIP-T [52] & Swin-T [31] & 155M & O365,GoldG & 2.3 & 34.4 & 26.9 & 33.9 & 36.3 \\\\ \\hline YOLO-World-S & YOLOv8-S & 13M (77M) & O365,GoldG & 74.1 (19.9) & 26.2 & 19.1 & 23.6 & 29.8 \\\\ YOLO-World-M & YOLOv8-M & 29M (92M) & O365,GoldG & 58.1 (18.5) & 31.0 & 23.8 & 29.2 & 33.9 \\\\ YOLO-World-L & YOLOv8-L & 48M (110M) & O365,GoldG & 52.0 (17.6) & 35.0 & 27.1 & 32.8 & 38.3 \\\\ YOLO-World-L & YOLOv8-L & 48M (110M) & O365,GoldG,CC3M\\({}^{\\dagger}\\) & 52.0 (17.6) & 35.4 & 27.6 & 34.1 & 38.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: LVIS에 대한 **제로샷 평가.** LVIS 미니벌 [20]에 대한 YOLO-World를 제로샷 방식으로 평가한다. 우리는 최근의 방법들과 공정한 비교를 위해 _Fixed AP_[4]를 보고한다. \\ ({}^{\\dagger}\\)는 246k 샘플을 포함하는 설정에서 의사 표지된 CC3M을 나타낸다. FPS는 하나의 NVIDIA V100 GPU w/o TensorRT에서 평가된다. YOLO-World의 파라미터와 FPS는 재매개변수화 버전(w/o bracket)과 원본 버전(w/ bracket) 모두에 대해 평가된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Text Encoder & Frozen? & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline BERT-base & Frozen & 14.6 & 3.4 & 10.7 & 20.0 \\\\ BERT-base & Fine-tune & 18.3 & 6.6 & 14.6 & 23.6 \\\\ \\hline CLIP-base & Frozen & **22.4** & **14.5** & **20.1** & **26.0** \\\\ CLIP-base & Fine-tune & 19.3 & 8.6 & 15.7 & 24.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **YOLO-World 내의 텍스트 인코더.** 제로 샷 LVIS 평가를 통해 YOLO-World 내의 상이한 텍스트 인코더를 절제한다.\n' +
      '\n' +
      '처음부터 훈련된 이전 방법에 비해 성능이 더 높습니다.\n' +
      '\n' +
      'LVIS 객체 탐지.탭에서. 7, 우리는 표준 LVIS 데이터 세트에서 YOLO-World의 미세 조정 성능을 평가한다. 첫째, 전체 LVIS 데이터셋에 대해 훈련된 오라클 YOLOv8[19]와 비교하여 YOLO-World는 특히 더 큰 모델인 _g_, YOLO-World-L이 YOLOv8-L보다 7.2 AP 및 10.2 AP\\({}_{r}\\)의 성능 향상을 달성한다. 개선된 결과는 대규모 어휘 탐지를 위한 제안된 사전 훈련 전략의 효과를 입증할 수 있다. 또한, YOLO-World는 효율적인 1단계 검출기로서 추가 설계, 학습 가능한 프롬프트[7] 또는 영역 기반 정렬[12] 없이 전체 성능에서 이전의 최첨단 2단계 방법[7, 12, 21, 49, 59]보다 우수하다.\n' +
      '\n' +
      '### Open-Vocabulary 인스턴스 분할\n' +
      '\n' +
      '이 섹션에서는 개방형 어휘 설정 하에서 객체를 세그먼트화하기 위해 YOLO-World를 추가로 미세 조정하는데, 이는 개방형 어휘 인스턴스 세그먼트화(OVIS)라고 할 수 있다. 이전 방법[17]은 새로운 객체에 의사 레이블링을 사용하여 OVIS를 탐색했다. 이와는 달리 YOLO-World가 강력한 전송 및 일반화 기능을 가지고 있다는 점을 고려하여 마스크 주석이 있는 데이터의 하위 집합에서 YOLO-World를 직접 미세 조정하고 대규모 어휘 설정에서 세그먼트화 성능을 평가한다. 구체적으로, 두 가지 설정 하에서 개방형 어휘 인스턴스 분할을 벤치마킹한다:\n' +
      '\n' +
      '*(1) _COCO에서 LVIS_ 설정으로, 우리는 마스크 주석을 사용하여 COCO 데이터세트(80개 카테고리 포함)에서 YOLO-World를 미세 조정하며, 여기서 모델은 80개 카테고리에서 1203개 카테고리(\\(80\\~1203\\))로 전송해야 한다.\n' +
      '* (2) _LVIS-base to LVIS_ setting, we fine-tune YOLO-World on LVIS-base (866 categories, common & frequent 포함) with mask annotations, under the models need to transfer to 866 categories to 1203 categories (\\(866\\to 1203\\))\n' +
      '\n' +
      '우리는 337개의 희귀 범주가 보이지 않고 개방형 어휘 성능을 측정하는 데 사용할 수 있는 1203개의 범주로 표준 LVIS val2017의 미세 조정 모델을 평가한다.\n' +
      '\n' +
      '결과, 탭 도 8은 개방형 어휘 인스턴스 분할을 위한 YOLO-World의 확장 실험 결과를 나타낸다. 구체적으로, 우리는 두 가지 미세 조정 전략을 채택한다: (1) 분할 헤드만 미세 조정하고 (2) 미세 조정\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Pre-train & AP & AP\\({}_{50}\\) & AP\\({}_{75}\\) & FPS \\\\ \\hline \\multicolumn{5}{l}{_Training from scratch._} \\\\ YOLOv6-S [22] & ✗ & 43.7 & 60.8 & 47.0 & 442 \\\\ YOLOv6-M [22] & ✗ & 48.4 & 65.7 & 52.7 & 277 \\\\ YOLOv6-L [22] & ✗ & 50.7 & 68.1 & 54.8 & 166 \\\\ YOLOv7-T [48] & ✗ & 37.5 & 55.8 & 40.2 & 404 \\\\ YOLOv7-L [48] & ✗ & 50.9 & 69.3 & 55.3 & 182 \\\\ YOLOv7-X [48] & ✗ & 52.6 & 70.6 & 57.3 & 131 \\\\ YOLOv8-S [19] & ✗ & 44.4 & 61.2 & 48.1 & 386 \\\\ YOLOv8-M [19] & ✗ & 50.5 & 67.3 & 55.0 & 238 \\\\ YOLOv8-L [19] & ✗ & 52.9 & 69.9 & 57.7 & 159 \\\\ \\hline \\multicolumn{5}{l}{_Zero-shot transfer._} \\\\ YOLO-World-S & O+G & 37.6 & 52.3 & 40.7 & - \\\\ YOLO-World-M & O+G & 42.8 & 58.3 & 46.4 & - \\\\ YOLO-World-L & O+G & 44.4 & 59.8 & 48.3 & - \\\\ YOLO-World-L & O+G+C & 45.1 & 60.7 & 48.9 & - \\\\ \\hline \\multicolumn{5}{l}{_Fine-tuned w/ RepVL-PAN._} \\\\ YOLO-World-S & O+G & 45.9 & 62.3 & 50.1 & - \\\\ YOLO-World-M & O+G & 51.2 & 68.1 & 55.9 & - \\\\ YOLO-World-L & O+G+C & 53.3 & 70.1 & 58.2 & - \\\\ \\hline \\multicolumn{5}{l}{_Fine-tuned w/ RepVL-PAN._} \\\\ YOLO-World-S & O+G & 45.7 & 62.3 & 49.9 & 373 \\\\ YOLO-World-M & O+G & 50.7 & 67.2 & 55.1 & 231 \\\\ YOLO-World-L & O+G+C & 53.3 & 70.3 & 58.1 & 156 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **YOLO와의 비교 on COCO Object Detection.** We fine-tune the YOLO-World on COCO train2017 and evaluate on COCO val2017. The results of YOLOv7[48] and YOLOv8[19] is obtained from MMYOLO[3]. \'O\', \'G\', \'C\'는 각각 Objects365, GoldG, CC3M을 사용하는 것을 의미한다. FPS는 하나의 NVIDIA V100 w/TensorFlowRT에서 측정된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline ViLD [12] & 27.8 & 16.7 & 26.5 & 34.2 \\\\ RegionCLIP [58] & 28.2 & 17.1 & - & - \\\\ Detic [59] & 26.8 & 17.8 & - & - \\\\ FVLM [21] & 24.2 & 18.6 & - & - \\\\ DetPro [7] & 28.4 & 20.8 & 27.8 & 32.4 \\\\ BARON [49] & 29.5 & 23.2 & 29.3 & 32.5 \\\\ \\hline YOLOv8-S & 19.4 & 7.4 & 17.4 & 27.0 \\\\ YOLOv8-M & 23.1 & 8.4 & 21.3 & 31.5 \\\\ YOLOv8-L & 26.9 & 10.2 & 25.4 & 35.8 \\\\ \\hline YOLO-World-S & 23.9 & 12.8 & 20.4 & 32.7 \\\\ YOLO-World-M & 28.8 & 15.9 & 24.6 & 39.0 \\\\ YOLO-World-L & 34.1 & 20.4 & 31.1 & 43.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **LVIS에서 Open-Vocabulary Detector와의 비교.** LVIS 기반(공통 및 빈발 포함)에서 YOLO-World를 훈련하여 _bbox AP_를 보고한다. YOLO-v8은 클래스 균형 샘플링과 함께 전체 LVIS 데이터 세트(베이스 및 신규 포함)에 대해 훈련된다.\n' +
      '\n' +
      '모든 모듈. 전략 (1)에 따라 미세 조정 욜로-월드는 사전 훈련 단계에서 획득한 제로 샷 기능을 여전히 유지하여 추가 미세 조정 없이 보이지 않는 범주로 일반화할 수 있다. 전략(2)은 YOLO-World가 LVIS 데이터 세트에 더 잘 적합하도록 하지만, 제로 샷 능력의 저하를 초래할 수 있다.\n' +
      '\n' +
      '탭 도 8은 다른 설정(COCO 또는 LVIS 기반) 및 다른 전략(미세 조정 헤드 또는 미세 조정 모두)과의 미세 조정 욜로-월드의 비교를 도시한다. 첫째, LVIS 기반에서 미세 조정은 COCO 기반에 비해 더 나은 성능을 얻는다. 그러나 AP와 AP({}_{r}\\) 사이의 비율(AP\\({}_{r}\\)/AP)은 거의 변하지 않았으며, COCO와 LVIS 기반에서 YOLO-World의 비율은 각각 76.5%와 74.3%였다. 검출기가 동결된 것을 고려할 때, 성능 격차는 LVIS 데이터 세트가 세분화 헤드를 학습하는 데 유익한 보다 상세하고 조밀한 세분화 주석을 제공한다는 사실에 기인한다. 모든 모듈을 미세 조정할 때 YOLO-World는 LVIS에서 현저한 개선을 얻으며, _e.g_, YOLO-World-L은 9.6 AP 이득을 얻는다. 그러나 미세조정은 개방어휘 성능을 저하시키고, YOLO-World-L에 대한 0.6 박스 AP\\({}_{r}\\) 드롭으로 이어질 수 있다.\n' +
      '\n' +
      '### Visualizations\n' +
      '\n' +
      '세 가지 설정 하에서 미리 훈련된 YOLO-World-L의 시각화 결과를 제공한다: (a) LVIS 카테고리로 제로 샷 추론을 수행하고, (b) 속성이 있는 세밀한 카테고리로 사용자 지정 프롬프트를 입력하고, (c) 검출을 참조한다. 시각화는 또한 YOLO-World가 참조 능력과 함께 개방형 어휘 시나리오에 대한 강력한 일반화 능력을 가지고 있음을 보여준다.\n' +
      '\n' +
      'LVIS에 대한 제로샷 추론. 도 5는 사전 훈련된 YOLO-World-L에 의해 제로-샷 방식으로 생성되는 LVIS 카테고리들에 기초한 시각화 결과들을 도시한다. 미리 훈련된 YOLO-World는 강력한 제로 샷 전송 능력을 나타내며, 이미지 내에서 가능한 많은 객체를 검출할 수 있다.\n' +
      '\n' +
      '사용자 어휘에 대한 추론. 6, 정의된 범주로 욜로-월드의 탐지 기능을 탐구한다. 시각화 결과는 미리 훈련된 YOLO-World-L이 또한 (1) 세립 검출(_i.e_., 하나의 객체의 부분을 검출) 및 (2) 세립 분류(_i.e_., 객체의 상이한 하위 카테고리를 구별)에 대한 능력을 나타냄을 입증한다.\n' +
      '\n' +
      '물체 검출을 설명한다. 도 7에서, 우리는 모델이 주어진 입력과 일치하는 이미지에서 영역 또는 객체를 찾을 수 있는지 여부를 탐색하기 위해 입력, 예를 들어 서 있는 사람으로서 일부 서술적(차별적) 명사 구를 활용한다. 시각화 결과는 문구와 해당 경계 상자를 표시하여 사전 훈련된 YOLO-World가 참조 또는 접지 기능을 가지고 있음을 보여준다. 이러한 능력은 대규모 훈련 데이터로 제안된 사전 훈련 전략에 기인할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 실시간 실시간 개방형 어휘 검출기인 YOLO-World(YOLO-World)를 제안한다. YOLO-World(YOLO-World)는 실제 응용에서 효율성과 개방형 어휘 능력을 향상시키는 것을 목표로 한다. 본 논문에서는 개방형 어휘 사전 학습 및 탐지를 위한 비전 언어 YOLO 아키텍처로 널리 사용되는 YOLO를 재구성하고, 네트워크와 비전 및 언어 정보를 연결하고 효율적인 배치를 위해 재매개변수화할 수 있는 RepVL-PAN을 제안한다. 또한, YOLO-World에 개방형 어휘 탐지를 위한 강력한 기능을 부여하기 위해 탐지, 접지 및 이미지 텍스트 데이터를 이용한 효과적인 사전 훈련 방법을 제시한다. 실험은 YOLO-World의 속도 및 개방형 어휘 성능 측면에서 우수성을 입증할 수 있으며, 향후 연구에 통찰력 있는 소형 모델에 대한 비전 언어 사전 훈련의 효과를 나타낼 수 있다. 우리는 욜로-월드가 실제 개방형 어휘 탐지를 해결하기 위한 새로운 벤치마크가 되기를 바란다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _ECCV_, pages 213-229, 2020.\n' +
      '* [2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. _arXiv preprint arXiv:1906.07155_, 2019.\n' +
      '* [3] MMYOLO Contributors. MMYOLO: OpenMMLab YOLO series toolbox and benchmark. [https://github.com/open-mmlab/mmyolo](https://github.com/open-mmlab/mmyolo), 2022.\n' +
      '* [4] Achal Dave, Piotr Dollar, Deva Ramanan, Alexander Kirillov, and Ross B. Girshick. Evaluating large-vocabulary object detectors: The devil is in the details. _CoRR_, abs/2102.01066, 2021.\n' +
      '* [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, pages 4171-4186, 2019.\n' +
      '* [6] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In _CVPR_, pages 13733-13742, 2021.\n' +
      '* [7] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In _CVPR_, pages 14064-14073, 2022.\n' +
      '* [8] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott, and Weilin Huang. TOOD: task-aligned one-stage object detection. In _ICCV_, pages 3490-3499. IEEE, 2021.\n' +
      '* [9] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLO: exceeding YOLO series in 2021. _CoRR_, abs/2107.08430, 2021.\n' +
      '* [10] Ross B. Girshick. Fast R-CNN. In _ICCV_, pages 1440-1448, 2015.\n' +
      '* [11] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In _CVPR_, pages 580-587, 2014.\n' +
      '* [12] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In _ICLR_, 2022.\n' +
      '* [13] Agrim Gupta, Piotr Dollar, and Ross B. Girshick. LVIS: A dataset for large vocabulary instance segmentation. In _CVPR_, pages 5356-5364, 2019.\n' +
      '* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.\n' +
      '* [15] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross B. Girshick. Mask R-CNN. In _ICCV_, pages 2980-2988, 2017.\n' +
      '* [16] Drew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _CVPR_, pages 6700-6709, 2019.\n' +
      '* [17] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan Elhamifar. Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling. In _CVPR_, pages 7010-7021, 2022.\n' +
      '* [18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, pages 4904-4916, 2021.\n' +
      '* [19] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics yolov8. [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics), 2023.\n' +
      '* [20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\n' +
      '\n' +
      '그림 5: **LVIS에 대한 제로샷 추론에 대한 시각화 결과.** 사전 훈련된 YOLO-World-L을 채택하고 COCO val2017에 LVIS 어휘(1203 범주를 포함)로 추론한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l c c c c c c} \\hline \\hline Model & Fine-tune Data & Fine-tune Modules & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) & AP\\({}^{b}\\) & AP\\({}^{b}_{r}\\) \\\\ \\hline YOLO-World-M & COCO & _Seg Head_ & 12.3 & 9.1 & 10.9 & 14.6 & 22.3 & 16.2 \\\\ YOLO-World-L & COCO & _Seg Head_ & 16.2 & 12.4 & 15.0 & 19.2 & 25.3 & **18.0** \\\\ \\hline YOLO-World-M & LVIS-base & _Seg Head_ & 16.7 & 12.6 & 14.6 & 20.8 & 22.3 & 16.2 \\\\ YOLO-World-L & LVIS-base & _Seg Head_ & 19.1 & 14.2 & 17.2 & 23.5 & 25.3 & **18.0** \\\\ \\hline YOLO-World-M & LVIS-base & _All_ & 25.9 & 13.4 & 24.9 & 32.6 & 32.6 & 15.8 \\\\ YOLO-World-L & LVIS-base & _All_ & **28.7** & **15.0** & **28.3** & **35.2** & **36.2** & 17.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: **Open-Vocabulary Instance Segmentation.** 두 설정 하에서 Open-Vocabulary instance segmentation에 대한 YOLO-World를 평가한다. 분할 헤드 또는 YOLO-World의 모든 모듈을 미세 조정하고 비교를 위해 _Mask AP_를 보고한다. AP\\({}^{b}\\)는 박스 AP를 나타낸다.\n' +
      '\n' +
      '신테베, 이산 미스라, 니콜라스 캐리온 MDETR - 엔드 투 엔드 멀티모달 이해를 위한 변조된 검출. _ICCV_, 페이지 1760-1770, 2021.\n' +
      '* [21] Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni, and Anelia Angelova. F-VLM: open-vocabulary object detection upon frozen vision and language models. _CoRR_, abs/2209.15639, 2022.\n' +
      '* [22] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and Xiaolin Wei. Yolov6: A single-stage object detection framework for industrial applications. _CoRR_, abs/2209.02976, 2022.\n' +
      '* [23] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In _CVPR_, pages 10955-10965, 2022.\n' +
      '* [24] Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning object-language alignments for open-vocabulary object detection. In _ICLR_, 2023.\n' +
      '* [25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 740-755, 2014.\n' +
      '* [26] Tsung-Yi Lin, Piotr Dollar, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie. Feature pyramid networks for object detection. In _CVPR_, pages 936-944, 2017.\n' +
      '* [27] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _ICCV_, pages 2999-3007, 2017.\n' +
      '* [28] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In _CVPR_, pages 8759-8768, 2018.\n' +
      '\n' +
      '그림 6: **Visualization Results on User\'s Vocabulary.** We define the custom vocabulary for each input image and YOLO-World can detect accurate regions to the vocabulary. 이미지는 COCO val2017에서 얻는다.\n' +
      '\n' +
      '그림 7: **참조 객체 검출에 대한 시각화 결과.** 기술 명사 구로 객체를 검출하는 사전 훈련된 YOLO-World의 능력을 탐구한다. 이미지는 COCO val2017에서 얻는다.\n' +
      '\n' +
      '* [29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: marraying DINO with grounded pre-training for open-set object detection. _CoRR_, abs/2303.05499, 2023.\n' +
      '* [30] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. SSD: single shot multibox detector. In _ECCV_, pages 21-37, 2016.\n' +
      '* [31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, pages 9992-10002, 2021.\n' +
      '* [32] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, and Shilei Wen. PP-YOLO: an effective and efficient implementation of object detector. _CoRR_, abs/2007.12099, 2020.\n' +
      '* [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [34] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In _NeurIPS_, pages 1143-1151, 2011.\n' +
      '* [35] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. _Int. J. Comput. Vis._, pages 74-93, 2017.\n' +
      '* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.\n' +
      '* [37] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In _CVPR_, pages 6517-6525, 2017.\n' +
      '* [38] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. _CoRR_, abs/1804.02767, 2018.\n' +
      '* [39] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _CVPR_, pages 779-788, 2016.\n' +
      '* [40] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _CVPR_, pages 779-788, 2016.\n' +
      '* [41] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1137-1149, 2017.\n' +
      '* [42] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1137-1149, 2017.\n' +
      '* [43] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, pages 8429-8438, 2019.\n' +
      '* [44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, pages 2556-2565, 2018.\n' +
      '* [45] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: fully convolutional one-stage object detection. In _ICCV_, pages 9626-9635, 2019.\n' +
      '* [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, pages 5998-6008, 2017.\n' +
      '* [47] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of CNN. In _CVPRW_, pages 1571-1580, 2020.\n' +
      '* [48] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. In _CVPR_, pages 7464-7475, 2023.\n' +
      '* [49] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In _CVPR_, pages 15254-15264, 2023.\n' +
      '* [50] Johnathan Xie and Shuai Zheng. ZSD-YOLO: zero-shot YOLO detection using vision-language knowledgedistillation. _CoRR_, 2021.\n' +
      '* [51] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua Lai. PP-YOLOE: an evolved version of YOLO. _CoRR_, abs/2203.16250, 2022.\n' +
      '* [52] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. In _NeurIPS_, 2022.\n' +
      '* [53] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment. In _CVPR_, pages 23497-23506, 2023.\n' +
      '* [54] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In _CVPR_, pages 14393-14402, 2021.\n' +
      '* [55] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. In _NeurIPS_, 2022.\n' +
      '* [56] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. DINO: DETR with improved denoising anchor boxes for end-to-end object detection. In _ICLR_, 2023.\n' +
      '* [57] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In _CVPR_, pages 9756-9765, 2020.\n' +
      '\n' +
      '* [58] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Lianian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image pretraining. In _CVPR_, pages 16772-16782, 2022.\n' +
      '* [59] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _ECCV_, pages 350-368, 2022.\n' +
      '* [60] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In _ICLR_, 2021.\n' +
      '\n' +
      '## 부록 추가 세부사항\n' +
      '\n' +
      'RepVL-PAN을 위한### Re-parameterization\n' +
      '\n' +
      '오프라인 어휘를 추론하는 동안, 우리는 더 빠른 추론 속도와 배치를 위해 RepVL-PAN을 위한 재모수화를 채택한다. 먼저, 텍스트 인코더를 통해 텍스트 임베딩(W\\in\\mathbb{R}^{C\\times D}\\)을 미리 계산한다.\n' +
      '\n' +
      'Re-parameterize T-CSPLayer.RepVL-PAN 내의 각 T-CSPLayer에 대해, 텍스트 임베딩 \\(W\\in\\mathbb{R}^{C\\times D\\times 1\\times 1}\\)을 \\(1\\times 1\\) 컨볼루션 레이어(또는 선형 레이어)의 가중치로 재형성함으로써 텍스트 안내를 추가하는 과정을 다음과 같이 재매개변수와 단순화할 수 있다.\n' +
      '\n' +
      '\\[X^{\\prime}=X\\odot\\texttt{Sigmoid}(\\texttt{max}(\\texttt{Conv}(X,W),\\texttt{ dim=1})), \\tag{4}\\.\n' +
      '\n' +
      '여기서 \\(X\\times\\in\\mathbb{R}^{B\\times D\\times H\\times W}\\) 및 \\(X\\prime\\in\\mathbb{R}^{B\\times D\\times H\\times W}\\)은 입력 및 출력 이미지 특징들이다. \\ (\\odot\\)는 재구성 또는 전치된 행렬 곱셈이다.\n' +
      '\n' +
      '재-파라미터화 I-Pooling Attention.I-Pooling Attention은 재-파라미터화 또는 단순화될 수 있다:\n' +
      '\n' +
      '\\[\\tilde{X}=\\texttt{cat}(\\texttt{MP}(X_{3},3),\\texttt{MP}(X_{4},3),\\texttt{MP}(X_{5},3)),\\tag{5}\\texttt{cat}(\\texttt{MP}(X_{3},3))\n' +
      '\n' +
      '여기서 cat는 농도이고 MP(-, 3)은 \\(3\\times 3\\) 출력 피쳐에 대한 최대 풀링을 나타낸다. \\ (\\{X_{3},X_{4},X_{5}\\}\\)는 RepVL-PAN에서 다중 스케일 특징이다. \\\\ (\\tilde{X}\\)는 평평하고, \\(B\\times D\\times 27\\)의 형상을 갖는다. 그런 다음 텍스트 임베딩을 업데이트합니다.\n' +
      '\n' +
      '\\[W^{\\prime}=W+\\texttt{Softmax}(W\\odot\\tilde{X}),\\texttt{dim=-1})\\odot W, \\tag{6}\\.\n' +
      '\n' +
      '### Fine-tuning Details.\n' +
      '\n' +
      'YOLO-World를 COCO[25] 객체 검출로 이전할 때 RepVL-PAN의 모든 T-CSPLayers와 Image-Pooling Attention을 제거하는데, 이는 80개의 카테고리만을 포함하고, 시각적 언어 상호작용에 대한 의존도가 상대적으로 낮다. 미세 조정 시 미리 훈련된 가중치를 사용하여 욜로월드를 초기화한다. 미세조정의 학습률은 0.0002, 가중치 감쇠는 0.05로 설정하였으며, 미세조정을 수행한 후, 주어진 COCO 카테고리로 클래스 텍스트 임베딩을 미리 계산하고 분류 계층의 가중치로 임베딩을 저장한다.\n' +
      '\n' +
      '## 부록 B 대규모 이미지 텍스트 데이터에 대한 자동 레이블링\n' +
      '\n' +
      '이 섹션에서는 영역-텍스트 쌍을 대규모 이미지-텍스트 데이터 _e.g_., CC3M [44]로 레이블링하기 위한 세부 절차를 추가한다. 전체 라벨링 파이프라인이 그림 1에 나와 있다. 8은 주로 _i.e_., (1) 객체 명사 추출, (2) 의사 레이블링, (3) 필터링의 세 가지 절차로 구성된다. Sec. 3.4에서 논의된 바와 같이, 우리는 캡션으로부터 명사를 추출하기 위해 간단한 n-gram 알고리즘을 채택한다.\n' +
      '\n' +
      '영역-텍스트 제안.첫 번째 단계에서 객체 명사 집합 \\(T=\\{t_{k}\\}^{K}\\)을 구한 후, 사전 훈련된 개방형 어휘 검출기인 _i.e_, GLIP-L [23]을 활용하여 신뢰 점수 \\(\\{c_{i}\\}\\}\\)과 함께 의사 박스 \\(\\{B_{i}\\}\\}\\)을 생성한다.\n' +
      '\n' +
      '\\[\\{B_{i},t_{i},c_{i}\\}_{i=1}^{N}=\\texttt{GLIP-Labeler}(I,T), \\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(\\{B_{i},t_{i},c_{i}\\}_{i=1}^{N}\\)은 거친 영역-텍스트 제안들이다.\n' +
      '\n' +
      'CLIP 기반 Re-scoring & Filtering.We presented the region-text proposals including much noise, we presented restored and filtering pipeline with the pre-trained CLIP[36]. 입력 영상\\(I\\), 자막\\(T\\) 및 거친 영역-텍스트 제안\\(\\{B_{i},t_{i},c_{i}\\}_{i=1}^{N}\\)이 주어지면, 특정 파이프라인은 다음과 같이 나열된다:\n' +
      '\n' +
      '* (1) Image-Text Score 계산: 자막이 있는 이미지\\(I\\)을 CLIP에 전달하고 이미지-Text similarity score\\(s^{img}\\)을 얻는다.\n' +
      '*(2) 영역-텍스트 점수 계산: 입력 영상에서 영역 박스 \\(\\{B_{i}\\}\\}\\)에 따라 영역 이미지를 크롭한다. 그리고 잘라낸 이미지를 텍스트와 함께 CLIP에 전달하여 영역-텍스트 유사도 \\(S^{r}=\\{s_{i}^{r}\\}_{i=1}^{N}\\)을 얻는다.\n' +
      '*(3) [선택사항] 재-라벨링: 우리는 모든 명사가 있는 각각의 크롭된 이미지를 포워딩하고 최대 유사도를 갖는 명사를 할당할 수 있으며, 이는 GLIP에 의해 잘못 라벨링된 텍스트를 정정하는 데 도움이 될 수 있다.\n' +
      '(4) Reccoring: 신뢰도 점수 \\(\\tilde{c_{i}}=\\sqrt{c_{i}*s_{i}^{r}}\\)를 복원하기 위해 영역-텍스트 유사성 \\(S^{r}\\)을 채택한다.\n' +
      '*(5) Region-level Filtering: 먼저 영역-텍스트 제안들을 텍스트에 따라 서로 다른 그룹으로 나눈 후 NMS(non-maximum suppression)를 수행하여 중복 예측을 필터링한다(NMS threshold는 0.5로 설정). 그런 다음 낮은 신뢰 점수로 제안을 필터링합니다(임계값은 0.3으로 설정됨).\n' +
      '* (6) Image-level Filtering: Image-level region-text score \\(s^{region}\\)을 보존 영역-text score를 평균하여 계산한다. 그리고 이미지 레벨 신뢰 점수를 \\(s=\\sqrt{s^{img}*s^{region}}\\)으로 구하고, 0.3보다 큰 점수를 갖는 이미지를 유지한다.\n' +
      '\n' +
      '위에서 언급한 임계값은 라벨링된 결과의 부분에 따라 경험적으로 설정되며 전체 파이프라인은 인간 검증 없이 자동이다. 마지막으로, 라벨이 붙은\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Method & Pre-trained Data & Samples & AP & AP\\({}_{r}\\) & AP\\({}_{c}\\) & AP\\({}_{f}\\) \\\\ \\hline YOLO-World-S & O365 & 0.61M & 16.3 & 9.2 & 14.1 & 20.1 \\\\ YOLO-World-S & O365+GoldG & 1.38M & 24.2 & 16.4 & 21.7 & 27.8 \\\\ \\hline YOLO-World-S & O365+CC3M-245k & 0.85M & 16.5 & 10.8 & 14.8 & 19.1 \\\\ YOLO-World-S & O365+CC3M-520k & 1.13M & 19.2 & 10.7 & 17.4 & 22.4 \\\\ YOLO-World-S & O365+CC3M-750k & 1.36M & 18.2 & 11.2 & 16.0 & 21.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: LVIS에 대한 **제로샷 평가.** 우리는 이미지 텍스트 데이터인 다른 양의 데이터로 YOLO-World-S를 사전 훈련하는 성능을 평가한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>