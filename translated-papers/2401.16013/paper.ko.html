<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '그리고 채택을 제한하는 알고리즘 _per se_의 제한보다는 이 설계 공간을 탐색하는 것이 과제이다. RL 알고리즘의 구현에서 세부 사항이 알고리즘의 특정 선택만큼 중요할 수 있다는 것은 해당 분야의 실무자들에 의해 종종 인정된다. 또한, 실제 학습은 보상 사양, 환경 재설정 구현, 샘플 효율성, 준수 및 안전한 제어 및 이 문제에 훨씬 더 많은 스트레스를 주는 기타 어려움과 함께 추가 과제를 제시한다. 따라서, 실제 로봇 RL에 대한 채택 및 추가 연구 진행은 새로운 알고리즘 혁신보다는 _구현_에서 병목 현상이 발생할 수 있다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 본 논문의 목적은 실제 로봇 공학에서 RL의 광범위한 채택을 촉진하는 것을 목표로 하는 오픈 소스 소프트웨어 프레임워크인 **S**샘플-**E**효율 **R**로봇 강화 **L**earning(SERL)을 제공하는 것이다. SERL은 다음과 같은 구성요소로 구성된다: (1) 실세계 로봇 학습을 지향하고 이미지 관찰 및 시연을 지원하는 고품질 RL 구현; (2) 분류기 및 적대적 훈련을 포함하는 이미지 관찰과 호환되는 여러 보상 사양 방법의 구현; (3) 시험들 사이에 태스크를 자동으로 리셋할 수 있는 "전진-후진" 제어기들의 학습을 지원하는 지원(Eysenbach et al., 2018); (4) 전술한 RL 컴포넌트를 임의의 로봇 조작기에 원칙적으로 연결할 수 있는 소프트웨어 패키지; 및 (5) 접촉이 풍부한 조작 태스크들을 처리하는데 특히 효과적인 임피던스 제어기 설계 원리. 본 논문의 목적은 새로운 알고리즘이나 방법론을 제안하는 것이 아니라 커뮤니티가 로봇 RL에 대한 향후 연구와 로봇 RL을 서브루틴으로 사용할 수 있는 다른 방법을 위해 로봇 전문가에게 잘 설계된 기초를 제공할 수 있는 자원을 제공하는 것이다. 그러나, 우리의 프레임워크를 평가하는 과정에서, 우리는 또한 과학적으로 흥미로운 경험적 관찰을 한다: 신중하게 조작된 소프트웨어 패키지에서 적절하게 구현되었을 때, 현재의 샘플 효율적인 로봇 RL 방법은 비교적 적당한 훈련 시간으로 매우 높은 성공률을 달성할 수 있다. 평가의 작업은 그림 1에 나와 있다. 1: 동적 접촉, 복잡한 동역학을 갖는 변형 가능한 물체 조작, 로봇이 수동으로 설계된 리셋 없이 학습해야 하는 물체 재배치를 포함하는 정밀한 삽입 작업. 이러한 각 작업에 대해 SERL은 이미지 관찰에서 작동하는 학습 정책에도 불구하고 정책당 훈련(총 벽 시계 시간 측면에서) 15~60분 이내에 효과적으로 학습할 수 있으며 거의 완벽한 성공률을 달성한다. 이 결과는 특히 딥 네트워크 및 이미지 입력을 갖는 RL이 종종 매우 비효율적인 것으로 간주되기 때문에 중요하다. 우리의 결과는 이 가정에 도전하여 잘 설계된 컨트롤러와 보상 사양 및 재설정을 위해 신중하게 선택된 구성 요소와 결합된 기존 기술의 신중한 구현을 제안함으로써 실제 사용에 충분히 효율적인 전체 시스템을 제공할 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '우리의 프레임워크는 기존의 RL 방법을 완전한 로봇 학습 시스템에 결합하지만, 부품의 특정 조합은 실제 세계에서 직접 효율적이고 즉각적인 강화 학습을 제공하도록 신중하게 설계되며, 우리의 실험에서 보여지는 바와 같이 광범위한 작업에서 우수한 결과를 달성한다. 여기서는 관련 선행 방법과 시스템을 모두 요약한다.\n' +
      '\n' +
      '**실세계 RL에 대한 알고리즘:**실세계 로봇 RL은 샘플 효율이 높고, 온보드 인식을 활용할 수 있으며, 쉽게 지정된 보상 및 재설정을 지원하는 알고리즘을 요구한다. 다수의 알고리즘들은 현실 세계에서 매우 효율적으로 직접 학습하는 능력을 보여주었다(Riedmiller et al., 2009; Westenbroek et al., 2022; Yang et al., 2020; Zhan et al., 2021; Hou et al., 2020; Tebbe et al., 2021; Popov et al., 2017; Luo et al., 2019; Zhao et al., 2022; Hu et al., 2023; Johannink et al., 2018; Schoettler et al., 2020), **use variants** of off-policy RL(Kostrikov et al., 2023; Hu et al., 2023b), **model-based** RL(Hester and Stone, 2013; Wu et al., 2022; Nagabandi et al., 2021; Luo et al., 2018) 및 on-policy RL(Zhu et al., 2019). 이러한 발전은 성공 분류기를 통해 원시 시각 관찰로부터 보상을 추론하는 진보(Fu et al., 2018; Li et al., 2021), 기초 모델 기반 보상(Du et al., 2023; Mahmoudieh et al., 2022; Fan et al., 2022) 및 비디오로부터의 보상(Ma et al., 2023b)과 쌍을 이루었다. 추가적으로, 자율적 훈련을 가능하게 하기 위해, 최소한의 인간 개입으로 자율적 훈련을 가능하게 하는 리셋-프리 학습(Gupta et al., 2021; Sharma et al., 2021; Zhu et al., 2020; Xie et al., 2022; Sharma et al., 2023)에서 다수의 알고리즘적 발전이 있었다. 이러한 알고리즘적 발전이 중요하지만, 본 연구에서 기여하는 바는 다양한 작업에 대해 잘 작동할 수 있는 방법을 미리 선택하여 실제 세계에서 샘플 효율적인 강화 학습을 가능하게 하는 프레임워크와 소프트웨어 패키지를 제공하는 것이다. 그렇게 함으로써, 우리는 새로운 연구자들이 실제 세계에서 로봇 학습을 위한 더 나은 알고리즘과 훈련 방법론을 구축하기 위해 진입 장벽을 낮추기를 바란다.\n' +
      '\n' +
      '**RL을 위한 소프트웨어 패키지들:** RL을 위한 다수의 패키지들(Seno and Imai, 2022; Nair and Pong; Hill et al., 2018; Guadarrama et al., 2018)이 있지만, 우리가 아는 한, 어느 것도 실제-세계 로봇 RL을 구체적으로 직접적으로 해결하는 것을 목표로 하지 않는다. SERL은 업데이트 대 데이터 비율이 높은 오프 정책 RL 알고리즘인 최근 제안된 RLPD 알고리즘을 기반으로 한다. SERL은 시뮬레이션에서 에이전트를 훈련시키기 위한 RL 알고리즘의 라이브러리는 아니지만, 그렇게 적응될 수 있다. 오히려 SERL은 로보트 제어를 위한 풀 스택 파이프라인을 제공하며, RL 알고리즘을 사용하여 비동기적이고 효율적인 훈련을 위한 인터페이스에서 리세트 없이 보상 및 훈련을 추론하기 위한 추가 기계로 이동한다. 그렇게 함으로써, SERL은 많은 방법들의 구현들을 제공하는 것을 목표로 하는 이전의 라이브러리들과는 달리, 비전문가들이 실제 세계에서 그들의 물리적 로봇들을 훈련시키기 위해 RL을 사용하기 시작하는 것을 돕기 위한 기성 패키지를 제공한다 - 즉, SERL은 컴포넌트들의 완전한 "수직적" 통합을 제공하는 반면, 이전의 라이브러리들은 "수평적"에 초점을 맞춘다. SERL은 또한 (Yu et al., 2019; James et al., 2020; Mittal et al., 2023)과 같은 RL 벤치마크 패키지가 아니다. SERL은 사용자가 실제 세계에서 직접 자신의 작업 및 성공 메트릭을 정의할 수 있도록 하여 이러한 작업에서 로봇 조작기를 실제로 제어하고 훈련시키기 위한 소프트웨어 인프라를 제공한다.\n' +
      '\n' +
      '**실세계 RL을 위한 소프트웨어:**실세계 RL을 위한 인프라스트럭처를 제안한 몇몇 이전의 패키지들이 있었다: 손재주 조작을 위한(Ahn et al., 2019), 탁상용 가구 어셈블리(Hoe et al., 2023), 레그드 로모션(Kostrikov et al., 2023), 및 페그 삽입(Levine et al., 2016). 이러한 패키지는 명시적 추적(Levine et al., 2016; Ahn et al., 2019) 또는 순수 고유 감각(Kostrikov et al., 2023)과 같은 특권 정보 또는 훈련 설정을 사용하거나, 또는 모방 학습으로 제한되는 좁은 상황에서 효과적이다. SERL에서는 사전 작업에서와 같이 훈련 설정의 특권을 요구하지 않고 매우 다양한 로봇 조작 작업에 사용할 수 있는 풀 스택 시스템을 보여준다.\n' +
      '\n' +
      '##3 예비 및 문제 진술\n' +
      '\n' +
      '로봇 강화 학습 작업은 MDP\\(\\mathcal{M}=\\{S,\\mathcal{A},\\rho,\\mathcal{P},r,\\gamma\\}\\)을 통해 정의될 수 있으며, 여기서 \\(\\mathsf{s}\\in\\mathcal{S}\\)은 상태 관찰(예를 들어, 현재 엔드-에펙터 위치와 결합된 이미지), \\(\\mathsf{c}\\in\\mathcal{A}\\)은 동작(예를 들어, 원하는 엔드-에펙터 포즈), \\(\\mathbf{s}_{0})은 초기 상태에 대한 분포이고, \\(r\\,:\\,S\\times\\mathcal{P}\\to\\mathbb{R}\\)은 시스템 역학에 의존하는 미지의 확률적 전이 확률이고, \\(r\\,:\\,S\\times\\mathcal{P}\\to\\mathbb{R}\\)은 시스템 역학에 의존하는 미지의 확률적 전이 확률이다. 최적의 정책\\(\\pi\\)은 보상의 누적 기대값, 즉 \\(E[\\sum_{t=0}^{\\infty}\\gamma^{t}r(\\mathbf{s}_{t},\\mathbf{a}_{t})]\\을 최대화하는 정책이다.\n' +
      '\n' +
      'RL 태스크의 사양은 간결하고 간단하지만, 실제 로봇 학습 문제를 RL 문제로 전환하려면 주의가 필요하다. 첫째, 학습을 위한 알고리즘의 샘플 효율성이 가장 중요하다: 실제 세계에서 학습이 이루어져야 할 때, 매분 및 시간마다 비용이 발생한다. 샘플 효율은 효과적인 오프-정책 RL 알고리즘(Konda and Tsitsiklis, 1999; Haarnoja et al., 2018; Fujimoto et al., 2018)을 사용함으로써 향상될 수 있지만, 또한 가장 빠른 트레이닝 시간을 달성하는 데 중요한 사전 데이터 및 시연(Rajeswaran et al., 2018; Ball et al., 2023; Nair et al., 2020)을 통합함으로써 가속화될 수 있다.\n' +
      '\n' +
      '또한, 로봇 RL에 대한 많은 도전들은 단지 \\(\\pi\\)을 최적화하기 위한 핵심 알고리즘을 넘어서 있다. 예를 들어, 보상 함수 \\(r\\)는 이미지 관찰에 의존할 수 있으며 사용자가 수동으로 지정하기 어려울 수 있다. 또한, 실험들 사이에 로봇이 초기 상태\\(\\mathbf{s}_{0}\\sim\\rho(\\mathbf{s}_{0})로 재설정되는 에피소드 작업들에 대해, 실제로 로봇(및 그 환경)을 이러한 초기 상태들 중 하나로 재설정하는 것은 어떻게든 자동화되어야 하는 기계적인 작업이다.\n' +
      '\n' +
      '더욱이, MDP 액션들(예를 들어, 엔드-이펙터 포즈들)을 실제 저-레벨 로봇 제어들에 인터페이스하는 제어기 층은 또한, 특히 로봇이 환경 내의 객체들과 물리적으로 상호작용하는 접촉-풍부 태스크들에 대해 큰 주의를 필요로 한다. 이 컨트롤러는 정확해야 할 뿐만 아니라 RL 알고리즘이 훈련 중에 무작위 동작으로 탐색할 수 있을 만큼 충분히 안전해야 한다.\n' +
      '\n' +
      'SERL은 사전 데이터를 통합할 수 있는 샘플 효율적인 오프 정책 RL 방법의 고품질 구현, 보상 기능 사양을 위한 몇 가지 선택, 학습을 위한 순방향 재설정 알고리즘 및 환경에서 로봇이나 객체를 손상시키지 않고 접촉이 풍부한 작업을 학습하기에 적합한 컨트롤러를 사용하여 이러한 각 문제에 대한 기성 솔루션을 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '##4 실세계에서의 샘플 효율적인 로봇 강화 학습\n' +
      '\n' +
      'SERL(Sample-Efficient Robotic Reinforced Learning)이라고 부르는 소프트웨어 패키지는 이전 섹션에서 자세히 설명한 문제에 대한 기성 솔루션을 제공하여 실제 세계의 로봇 RL을 액세스할 수 있도록 하는 것을 목표로 한다. 여기에는 자율 학습을 위한 이러한 학습 알고리즘을 지원하는 데 필요한 인프라와 효율적인 비전 기반 강화 학습 알고리즘을 제공하는 것이 포함된다. 이러한 노력의 목적은 새로운 알고리즘이나 도구를 제안하는 것이 아니라 복잡한 설정 절차와 라이브러리 간의 고통스러운 통합 없이 누구나 로봇 학습에 쉽게 사용할 수 있는 소프트웨어 패키지를 개발하는 것이다.\n' +
      '\n' +
      '코어 강화 학습 알고리즘은 RLPD(Ball et al., 2023)로부터 유도되며, 그 자체는 소프트 액터-크리틱(Haarnoja et al., 2018): 효율적인 학습을 위해 사전 데이터(차선의 데이터 또는 시연 중 하나)를 리플레이 버퍼에 쉽게 통합할 수 있는 오프 정책 Q-기능 액터-크리틱 방법이다. 보상 함수들은 이진 분류기 또는 VICE(Fu et al., 2018)로 특정될 수 있으며, 이는 정책으로부터의 추가적인 네거티브들로 RL 트레이닝 동안 분류기를 업데이트하는 방법을 제공한다. 보상 함수는 또한 로봇 상태가 성공을 평가하기에 충분한 경우(예를 들어, 우리의 PCB 보드 조립 작업에서와 같이)에 손으로 특정될 수 있다. 리셋들은 순방향-백워드 아키텍처(Sharma et al., 2021)를 통해 제공될 수 있으며, 여기서 알고리즘은 태스크를 수행하는 순방향 정책 및 환경을 초기 상태로 다시 리셋하는 역방향 정책의 두 가지 정책을 동시에 트레이닝한다. 또한 로봇 시스템 측면에서는 임의의 로봇에 본 방법을 인터페이스하기 위한 범용 어댑터와 접촉이 풍부한 조작 작업에 특히 적합한 임피던스 컨트롤러를 제공한다.\n' +
      '\n' +
      '### Core RL 알고리즘 : RLPD\n' +
      '\n' +
      '강화 학습 알고리즘이 이 설정에 적용되기 위해서는 (1) 효율적이고 시간 단계마다 여러 개의 그래디언트 업데이트를 할 수 있어야 하며, (2) 이전 데이터를 쉽게 통합한 다음 추가 경험으로 계속 개선할 수 있어야 하며, (3) 새로운 사용자를 위해 디버깅하고 빌드하는 것이 간단해야 한다. 이를 위해 최근 제안된 RLPD(Ball et al., 2023) 알고리즘을 기반으로 샘플 효율적인 로봇 학습에 대한 강력한 결과를 보여주었다. RLPD는 소프트-액터 비평가(Haarnoja et al., 2018)와 같은 시간차 알고리즘의 성공을 기반으로 하는 오프 정책 행위자-비판 강화 학습 알고리즘이지만, 위의 데시데라타를 만족시키기 위해 몇 가지 핵심 수정을 한다. RLPD는 (i) 높은 업데이트-대-데이터 비율 트레이닝(UTD), (ii) 사전 데이터와 온-정책 데이터 사이의 대칭 샘플링, 각각의 배치의 절반이 사전 데이터로부터 그리고 온라인 리플레이 버퍼로부터의 절반, 및 (iii) 트레이닝 동안 계층-규범 정규화의 세 가지 주요 변화를 만든다. 이 방법은 처음부터 훈련하거나, 부트스트랩 학습을 위해 사전 데이터(예를 들어, 시연)를 사용할 수 있다. 알고리즘의 각 단계는 각각의 손실 함수의 기울기에 따라 매개변수 Q-함수\\(Q_{\\phi}(\\mathbf{s},\\mathbf{a})\\)와 배우\\(\\pi_{\\theta}(\\mathbf{a}|\\mathbf{s})\\)의 매개변수를 업데이트한다:\n' +
      '\n' +
      '[\\mathcal{L}_{Q}(\\phi) =\\!F_{\\mathbf{s},\\mathbf{a},\\mathbf{s}^{\\prime}\\!\\left[\\left(Q_{\\phi}(\\mathbf{s},\\mathbf{a})\\!-\\!\\left(r(\\mathbf{s},\\mathbf{a})\\!+\\!rE_{\\mathbf{a}^{\\prime}\\!-\\!\\pi_{\\theta}[Q_{\\phi}(\\mathbf{s}^{\\prime},\\mathbf{a}^{\\prime})]\\right]\\\\[\\ma{s},\\mathbf{a}^{\\prime}\\right]\\!\\left(Q_{\\phi}(\\mathbf{s}^{\\prime},\\mathbf{a}}^{\\prime})\\!\\left(Q_{\\phi}(\\math thcal{L}_{\\pi}(\\theta) =\\! -\\!E_{\\mathbf{s}\\left[E_{\\mathbf{a}\\!\\! pi_{\\theta}(\\mathbf{a})}[Q_{\\phi}(\\mathbf{s},\\mathbf{a})]+\\alpha H(\\pi_{\\theta}(\\cdot|\\mathbf{s}} \\right],\\\n' +
      '\n' +
      '여기서 \\(Q_{\\phi}\\)는 _target network_(Mnih et al., 2013)이고, 액터 손실은 엔트로피 정규화를 사용하여 적응적으로 조정된 가중치 \\(\\alpha\\)(Haarnoja et al., 2018)을 사용한다. 각 업데이트 단계는 각 기대치의 샘플 기반 근사치를 사용하며, 샘플 중 절반은 이전 데이터(예를 들어, 시연)로부터 추출되고, 절반은 _replay buffer_(Mnih et al., 2013)로부터 추출된다. 효율적인 학습을 위해, 업데이트-대-데이트(UTD: Update-to-date) 비율이라고 지칭되는 환경에서 시간 단계당 다수의 업데이트 단계가 수행되고, 레이어 정규화로 비평기를 정규화하는 것은 더 높은 UTD 비율을 허용하고 따라서 더 효율적인 트레이닝을 허용한다(Ball et al., 2023).\n' +
      '\n' +
      '분류기를 이용한### 보상 명세\n' +
      '\n' +
      '보상 함수는 로봇이 일반적으로 작업이 성공적으로 수행되었는지 확인하기 위해 일종의 인식 시스템을 필요로 하기 때문에 이미지 관찰을 통해 학습할 때 손으로 지정하기 어렵다. 그림의 PCB 보드 조립 작업과 같은 일부 작업이 있다. 도 1을 참조하면, 엔드 이펙터의 위치에 기초하여 손-지정 보상을 수용할 수 있다(객체가 그리퍼에 견고하게 유지된다는 가정 하에), 대부분의 태스크들은 이미지들로부터 추론될 보상을 요구한다. 이 경우, 보상 함수는 상태 관찰 \\(\\mathbf{s}\\)을 취하고 성공적인 완료에 대응하는 이진 "이벤트" \\(e\\)의 확률을 출력하는 이진 분류기에 의해 제공될 수 있다. 그리고 나서 \\(r(\\mathbf{s})=\\log p(e|\\mathbf{s})\\에 의해 보상이 주어진다.\n' +
      '\n' +
      '이 분류기는 손으로 지정된 포지티브 및 네거티브 예를 사용하여 또는 VICE(Fu et al., 2018)라는 적대적 방법을 통해 훈련될 수 있다. 후자는 분류기 기반 보상을 사용하여 학습할 때 발생할 수 있는 보상 착취 문제를 해결하고, 분류기 훈련 세트에서 부정적인 예제의 필요성을 제거한다: RL 알고리즘이 보상 \\(r(\\mathbf{s})=\\log p(e|\\mathbf{s})\\을 최적화할 때, 잠재적으로 높은 확률을 잘못 출력하는 분류기 \\(p(e|\\mathbf{s})\\)를 속이는 "적대적" 상태를 발견할 수 있다. VICE는 네거티브 라벨들을 갖는 분류기에 대한 트레이닝 세트에 정책에 의해 방문된 모든 상태들을 추가하고, 각각의 반복 후에 분류기를 업데이트함으로써 이 문제를 해결한다. 이러한 방식으로, RL 프로세스는 생성적 적대 네트워크(GAN)(Goodfellow et al., 2014)와 유사하며, 정책은 생성기로 작용하고 보상 분류기는 판별기로 작용한다. 따라서 우리의 프레임워크는 세 가지 유형의 보상을 모두 지원합니다.\n' +
      '\n' +
      'Forward-Backward Controller를 이용한 Reset Free 훈련\n' +
      '\n' +
      '일화 작업을 학습할 때 로봇은 작업 시도 간의 환경을 재설정해야 합니다. 예를 들어, 도 1에서 객체 재배치 작업을 학습할 때 로봇이 객체를 목표 빈으로 성공적으로 이동시킬 때마다 이를 꺼내어 다시 초기 빈에 배치해야 한다. "리셋"에서 인간의 노력의 필요성을 제거하기 위해 SERL은 순방향 및 역방향 제어기를 사용하여 "리셋-프리" 훈련을 지원한다(Han et al., 2015; Gupta et al., 2021a). 이 설정에서 두 정책은 각각 자체 정책, Q-함수 및 보상 함수를 갖는 두 개의 독립적인 RL 에이전트를 사용하여 동시에 훈련된다(이전 섹션의 방법을 통해 지정됨). _forward_ 에이전트는 태스크를 수행하도록 학습하고, _backward_ 에이전트는 초기 상태(들)로 복귀하도록 학습한다. 보다 복잡한 리셋-프리 트레이닝 절차들이 또한 가능할 수 있지만(Gupta et al., 2021a), 우리는 이 간단한 레시피가 그림 1에서 재배치 스킬과 같은 객체 조작 태스크들을 학습하기에 충분하다는 것을 발견한다.\n' +
      '\n' +
      '### Software Components\n' +
      '\n' +
      '**환경 어댑터:** SERL은 많은 로봇 환경에 쉽게 사용할 수 있도록 하는 것을 목표로 한다. 프랑카 암에 대한 짐 환경 포장지 및 로봇 환경 세트를 스타터 가이드로 제공하지만 사용자는 자신의 기존 환경을 사용하거나 적합하다고 생각하는 새로운 환경을 개발할 수도 있다. 따라서, 라이브러리는 도 2에 도시된 바와 같이 Gym-like(Brockman et al., 2016)인 한 로봇 환경에 추가적인 제약을 부과하지 않는다. 다른 로봇 및 작업에 대해 쉽게 배치가능한 환경 포장지에 대한 지원을 확장하기 위한 커뮤니티의 기여를 환영한다.\n' +
      '\n' +
      '**배우 및 학습자 노드:** SERL은 그림 2와 같이 몇 줄의 코드로 액션을 추론하고 정책을 업데이트하기 위해 병렬로 훈련하고 행동하는 옵션을 포함한다. 우리는 이것이 높은 UTD 비율을 갖는 샘플 효율적인 실제 학습 문제에서 유익하다는 것을 발견했다. SERL은 두 개의 다른 스레드에서 배우와 학습자를 분리함으로써 제어 빈도를 고정된 속도로 보존할 수 있으며, 이는 변형 가능한 물체 및 접촉이 풍부한 조작과 같이 즉각적인 피드백과 반응이 필요한 작업에 매우 중요하며 실제 세계에서 훈련하는 총 벽-시계 시간을 줄인다.\n' +
      '\n' +
      '접촉이 풍부한 작업을 위한### 임피던스 제어기\n' +
      '\n' +
      '비록 우리의 패키지는 Sec에 설명된 대로 모든 OEM 로봇 컨트롤러와 호환되어야 하지만. 4, 제어기의 선택이 최종 성능에 큰 영향을 미칠 수 있음을 알 수 있었다. 이것은 접촉이 풍부한 조작에서 더 두드러진다. 예를 들어, 도 6의 PCB 삽입 작업에서. 1, 지나치게 뻣뻣한 제어기는 깨지기 쉬운 핀을 구부리고 삽입을 어렵게 하는 반면, 지나치게 순응적인 제어기는 물체를 신속하게 위치로 이동시키는 데 어려움을 겪을 수 있다.\n' +
      '\n' +
      '로봇 RL을 위한 일반적인 설정은 2-계층 제어 계층을 사용하며, 여기서 RL 정책은 다운스트림 실시간 제어기보다 훨씬 낮은 주파수에서 설정점 동작을 생성한다. RL 제어기는 물리적으로 바람직하지 않은 결과들을 야기하기 위해 로우-레벨 제어기에 대한 타겟들을 설정할 수 있다. 이를 설명하기 위해 Fig.에 제시된 계층적 제어기 구조를 고려해보자. 4에서, 하이-레벨 RL 제어기 \\(\\pi(\\mathbf{a}|\\mathbf{s})\\)는 로우-레벨 임피던스 제어기가 1K HZ에서 추적하기 위해 10HZ에서 제어 타겟들을 송신하므로, RL로부터 하나의 타임스텝은 로우-레벨 제어기의 100 타임스텝을 실행할 것이다. 이 제어기의 전형적인 임피던스 제어 목적은\n' +
      '\n' +
      '\\[F=k_{p}\\cdot e+k_{d}\\cdot\\dot{e}+F_{ff}+F_{cor},\\]\n' +
      '\n' +
      '여기서 \\(e=p-p_{ref}\\), \\(p\\)는 측정된 포즈이고, \\(p_{ref}\\)는 업스트림 제어기에 의해 계산된 타겟 포즈이고,\n' +
      '\n' +
      '그림 2: 소프트웨어 아키텍처 및 실제 로봇 훈련 예제 코드. SERL은 행위를 선택하는 행위자와 실제로 훈련 코드를 실행하는 학습자 노드, 행위자로부터 행위를 실행하고 학습자에게 데이터를 다시 기여하는 로봇 환경으로 구성된 3개의 병렬 프로세스를 실행한다.\n' +
      '\n' +
      'F_{ff}\\(F_{ff}\\)는 피드포워드 힘, \\(F_{cor}\\)는 코리올리 힘이며, 이 목표는 자코비안 전치 및 오프셋에 널스페이스 토크를 곱하여 관절 공간 토크로 변환된다. 이는 탄성계수가 \\(k_{p}\\)이고 감쇠계수가 \\(k_{d}\\)인 평형점(p_{ref}\\)을 중심으로 스프링-댐퍼 시스템처럼 작용한다. 전술한 바와 같이, 이 시스템은 \\(p_{ref}\\)이 현재 포즈로부터 멀리 떨어져 있으면 큰 힘을 산출할 것이며, 이는 팔이 무언가에 접촉될 때 단단한 충돌 또는 손상을 초래할 수 있다. 따라서 이로 인해 발생하는 상호 작용력을 제한하는 것이 중요하다. 그러나 이득을 직접 줄이면 컨트롤러의 정확도가 저하될 수 있습니다. 따라서, 우리는 \\(|e|\\leq\\Delta\\)가 되도록 \\(e\\)을 결합해야 하며, 그 후 스프링-댐퍼 시스템에서 발생하는 힘은 \\(k_{p}\\cdot|\\Delta|+2k_{d}\\cdot|\\Delta|\\cdot f\\), \\(f\\)이 제어 주파수이다.\n' +
      '\n' +
      'RL 정책에 의해 액션 출력을 직접 클립해야 하는지 궁금할 수 있습니다. 이것은 합리적으로 보일 수 있지만 일부 시나리오에서는 비실용적일 수 있다: PCB 보드와 같은 일부 오브젝트는 보통 마이크로미터 정도의 매우 작은 \\(\\Delta\\)의 상호작용력을 필요로 할 수 있다; RL 정책이 마이크로미터 단위로만 이동하도록 허용된다면, 에피소드가 장거리(예를 들어, 삽입 지점에 접근)를 통해 이동할 수 있는 충분한 시간 단계가 필요하기 때문에 매우 긴 학습 프로세스 또는 매우 불안정한 훈련을 초래할 수 있다. 그러나 실시간 계층에서 직접 클립하면 RL 정책을 작은 액션으로 제한할 필요 없이 문제를 크게 완화할 수 있습니다. 도 4에서와 같이, \\(M\\cdot|\\Delta|\\geq|a|_{max}\\)만큼 RL 정책의 자유 공간 이동을 차단하지 않을 것이며, 여기서 \\(M\\)은 블록 내부의 제어 시간-스텝 수이다. 이 값은 일반적으로 크다(예를 들어, \\(M=100\\)). 동시에, 우리는 접촉할 때마다 실시간 수준에서 기준 제약을 엄격하게 시행한다. 외력/토크 센서를 사용하여 동일한 결과를 얻을 수 있는지 궁금할 수도 있습니다. 이것은 여러 가지 이유로 바람직하지 않을 수 있다. (1) 힘/토크 센서는 상당한 노이즈를 가질 수 있고 올바른 하드웨어와 교정을 얻는 것은 어려울 수 있다. (2) 이러한 임계값을 얻더라도 힘 제약에 따를 뿐만 아니라 정책 학습을 수용하기 위해 로봇 동작을 설계하는 것은 간단하지 않다. 실제로 우리는 이러한 방식으로 참조를 클리핑하는 것이 간단하지만 매우 효과적이며 RL 기반 연락처가 풍부한 조작 작업을 가능하게 하는 데 중요하다는 것을 발견했다. 우리는 프랑카 판다 로봇에서 컨트롤러를 테스트했으며 패키지와 함께 프랑카 판다 구현을 포함했다. 그러나, 이러한 원리는 임의의 토크 제어 로봇 상에서 용이하게 구현될 수 있다. 제안된 제어기의 실제 성능을 검증하기 위해 그림과 같이 자유 공간에서 테이블 표면에 접촉하여 로봇을 이동하는 실제 추적 성능을 보고한다. 3, 컨트롤러가 접촉할 때마다 실제로 기준을 클램핑하는 반면 자유 공간에서 빠른 이동을 허용한다.\n' +
      '\n' +
      '### 상대 관측 및 액션 프레임\n' +
      '\n' +
      '행동 공간의 선택은 RL 훈련 과정의 용이성과 테스트 시간에 섭동에 일반화하는 학습된 정책의 능력 모두에 특히 중요하다. SERL은 표준 RL 환경 인터페이스를 통해 다양한 액션 표현에서 작동할 수 있지만, 우리는 상대 좌표계에서 관찰과 액션을 표현하는 편리한 메커니즘을 발견했다.\n' +
      '\n' +
      '동적 표적에 적응할 수 있는 에이전트를 개발하기 위해, 우리는 물리적 이동이 필요 없이 움직이는 표적을 시뮬레이션하는 훈련 절차를 제안한다. 타겟, 예를 들어, PCB 삽입 소켓 홀들은 로봇 베이스 프레임에 대해 고정되고, 보상은 Sec. 4.2에 제공된 표준 방법들 중 임의의 것을 사용하여 특정될 수 있다. 각 열차의 초기에\n' +
      '\n' +
      '도 4: 로보틱스 RL을 위한 전형적인 제어기 계층구조. RL 정책으로부터의 출력은 다운스트림 컨트롤러에 의해 시간 블록 내에 추적된다.\n' +
      '\n' +
      '그림 3: 엔드 이펙터의 z축에 대해 서로 다른 움직임으로 명령할 때 로봇에서 컨트롤러 로그의 시각화. 주황색 선은 명령 대상(RL의 출력)이고, 적색은 실시간 제어기로 보내진 평활화된 대상, 청색은 클립된 대상, 녹색은 이 제어기를 실행한 후의 로봇 위치이다. **왼쪽:** 로봇 엔드 이펙터는 단단한 표면과 접촉하도록 이동하고 접촉에도 불구하고 이동을 계속하라는 명령을 받았다. 기준 제한 메커니즘은 단단한 충돌을 피하기 위해 타겟을 클리핑했다. **Right:** 명령은 빠른 자유 공간 이동이며, 우리의 기준 제한 메커니즘은 _not_ 블록을 수행하여 타겟으로 빠른 움직임을 허용한다.\n' +
      '\n' +
      '에피소드, 로봇의 엔드 이펙터의 포즈는 작업 공간의 미리 정의된 영역 내에서 균일하게 무작위화되었다. 로봇의 고유수용성 정보는 엔드-이펙터의 초기 포즈의 프레임에 대해 표현되며, 정책(6D 트위스트)으로부터 출력된 액션은 현재 엔드-이펙터 프레임에 대해 상대적이다. 이는 엔드-이펙터에 부착된 프레임으로부터 상대적으로 볼 때 타겟을 물리적으로 이동시키는 것과 동일하다. 더 자세한 내용은 부록 7에 설명되어 있으며, 그 결과 객체가 움직이거나 일부 실험에서와 같이 에피소드 중간에 교란되더라도 정책이 성공할 수 있다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리의 실험 평가는 우리 시스템이 접촉이 풍부한 작업, 변형 가능한 물체 조작, 자유 부동 물체 조작을 포함한 다양한 로봇 조작 작업을 얼마나 효율적으로 학습할 수 있는지 연구하는 것을 목표로 한다. 이러한 실험은 SERL의 적용 가능성과 효율성의 폭을 보여준다. 우리는 클로즈인 뷰를 얻기 위해 엔드 이펙터에 부착된 프랑카 판다 팔과 두 개의 손목 카메라를 사용합니다. 자세한 내용은 [https://serl-robot.github.io/](https://serl-robot.github.io/)에서 확인할 수 있다. 정책 네트워크를 위한 비전 백본으로 ImageNet pre-trained ResNet-10 (He et al., 2015)을 사용하여 \\(2\\)-layer MLP에 연결한다. 관찰에는 엔드 이펙터 포즈, 비틀림, 힘 및 토크와 같은 카메라 이미지 및 로봇 고유수용성 정보가 포함된다. 정책은 현재 포즈로부터 6D 엔드-이펙터 델타 포즈를 출력하며, 이는 로우-레벨 제어기에 의해 추적된다. 평가 과제는 그림 1에 나와 있다. 도 5 및 하기에 기술됨:\n' +
      '\n' +
      '**PCB 삽입:** PCB 보드에 커넥터를 삽입하려면 서브 밀리미터 정밀도로 미세하고 접촉이 풍부한 조작이 필요합니다. 이러한 접촉이 풍부한 상호 작용을 시뮬레이션하고 전달하는 것은 어려울 수 있기 때문에 이 작업은 실제 교육에 이상적입니다. 각 훈련 및 평가 에피소드의 시작에서 초기 엔드 이펙터 포즈는 표 2에 설명된 대로 시작 영역으로부터 균일하게 샘플링된다.\n' +
      '\n' +
      '**케이블 라우팅:** 이 작업은 변형 가능한 케이블을 클립의 꼭 맞는 슬롯에 라우팅하는 것을 포함한다. 이 작업은 로봇이 케이블을 인식하고 다른 위치에 고정한 상태에서 클립에 맞도록 조심스럽게 조작해야 한다. 이것은 모델 기반 제어에 의존하거나, 변형 가능한 물체의 시각적 지각과 취급이 모두 접근에 필수적이기 때문에 단단한 물체 가정을 하는 어떤 방법에도 특히 어렵다. 이러한 종류의 작업은 종종 제조 및 유지 관리 시나리오에서 발생한다. PCB 작업과 유사하게, 초기 엔드 이펙터 포즈는 표 2에 설명된 바와 같이 시작 영역 내에서 균일하게 샘플링된다.\n' +
      '\n' +
      '**객체 재배치:** 이 작업은 빈들 사이에서 자유 부유 객체를 이동시켜야 하므로 파지 및 재배치가 필요하다. 보상 추론과 재설정 없는 훈련의 복잡성은 이러한 자유 부동 객체의 조작에서 특히 두드러진다. ♪ 정의해 ♪\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Package** & **Task** & **Training time** & **Success rate** & **Demos** & **Shaping?** & **Vision?** & **Open-sourced?** \\\\ \\hline Guided Policy Search(Levine et al., 2016) & Peg insertion & 3 hours & 70\\% & 0 & Yes & Yes & Yes \\\\ DDPGID (Vecerik et al., 2018) & Peg/clip insertion & 1.5-2.5 hours & 97\\% / 77\\% & 30 & No & Yes & No \\\\ Visual Residual RL (Schoettler et al., 2019) & Connector insertion & Not mentioned & 52\\% \\(\\sim\\) 100\\% & 0 & Yes & Yes & No \\\\ SHELD (Luo et al., 2021) & Connector insertion & 1.5 hours & 99.8\\% & 25 & No & Yes & No \\\\ InsertionNet (Spector and Castro, 2021) & Connector insertion & 40 mins & 78.5\\% - 100\\% & 0 & Yes & Yes & No \\\\\n' +
      '**SERL (Ours)** & **PCB Insertion** & **20 mins** & **100\\%** & **20** & **No** & **Yes** & **Yes** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 선행 작업에서 유사한 작업에 대해 보고된 결과와의 비교. 우리의 방법에 대한 전반적인 성공률은 일반적으로 더 높고 훈련 시간은 이전 결과에 비해 일반적으로 더 낮다. 또한 그림 1에 표시된 PCB 보드 조립 작업은 이전 작업에서 연구된 더 거친 페그 및 커넥터 삽입 작업보다 상당히 조이는 매우 엄격한 공차를 가지고 있다.\n' +
      '\n' +
      '그림 5: 우리의 방법으로 각 작업을 수행하는 로봇의 그림: PCB 삽입(왼쪽 상단), 케이블 라우팅(오른쪽 상단), 객체 재배치 - 전진(왼쪽 하단), 객체 재배치 - 후진(오른쪽 하단) 녹색 박스는 로봇이 작업을 완료하는 것에 대한 높은 보상을 받는 상태를 나타낸다.\n' +
      '\n' +
      '상기 전방 태스크는 오른쪽의 빈에서 객체를 픽업하여 왼쪽에 배치하는 반면, 상기 후방 태스크는 상기 객체를 다시 시작 빈으로 이동시켜 상기 전방 태스크를 취소하는, 방법.\n' +
      '\n' +
      '각 작업에 대해 우리는 스페이스 마우스를 사용하여 **20** 원격 조작 시연에서 RL 교육을 초기화한다. 시연만으로는 과제를 해결하기에 불충분하다는 것을 확인하기 위해, 우리는 RL이 수렴할 때 RL 재생 버퍼의 총 데이터 양과 대략 일치하는 **100** 고품질 **전문가** 원격 조작 시연을 사용하는 행동 복제(BC) 기준선을 포함한다. 이것은 우리의 방법에 의해 제공된 양보다 5배 더 많은 시연이라는 점에 유의한다. RL 및 BC 시연은 모두 표 2에 설명된 초기 엔드 이펙터 무작위화 스킴을 사용하여 수집되며 모든 훈련은 단일 Nvidia RTX 4090 GPU에서 수행되었다.\n' +
      '\n' +
      '**결과:** 표 2의 결과를 보고하고 그림 5의 예제 실행을 보여준다. 섹션 5에 자세히 설명된 것과 동일한 조건 및 프로토콜에서 BC 및 RL 정책을 모두 평가했으며 우리의 RL 정책은 100개의 모든 시험에서 세 가지 작업 모두에서 완벽한 성공률을 달성했다. PCB 삽입 및 케이블 라우팅 작업의 경우, 우리의 RL 정책은 모든 계산, 재설정 및 의도된 정지를 포함하는 30분 미만의 실제 교육에서 수렴한다. 자유 부동 객체 재배치 작업은 두 가지 정책(전진 및 후진)을 학습하며 총 시간은 정책_당 1시간 미만이다. 케이블 라우팅 작업과 PCB 삽입 작업의 경우, 우리의 정책은 BC보다 5배 적은 시연으로 훈련했음에도 불구하고 BC 기준선을 크게 능가하여 데모만으로는 충분하지 않음을 시사한다. 그림 1에서 성공률과 사이클 시간 측면에서 결과를 보고한다. 도 6 및 도 7. 학습된 RL 정책은 성공률 측면에서 BC 정책을 10배 이상 능가할 뿐만 아니라 초기 인간 시위의 사이클 시간에서 최대 3배까지 개선했다.\n' +
      '\n' +
      '**이전 시스템과의 비교:** 설정의 수많은 차이, 일관된 오픈 소스 코드의 부족 및 기타 불일치로 인해 우리의 결과를 이전 시스템의 결과와 직접 비교하는 것은 어렵지만 표 2의 PCB 보드 삽입 작업과 가장 유사한 작업에 대해 보고된 훈련 시간 및 성공률의 요약을 제공한다. 이전 작업에서 유사한 삽입 또는 조립 작업이 연구되었으며 이러한 작업은 종종 정밀도, 준수 제어 및 샘플 효율성의 문제를 제시하기 때문에 이 작업을 선택했다. 이러한 이전 작업과 비교하여, 우리의 실험은 작은 양의 시연 데이터(이는)를 활용하지만 광범위한 엔지니어링이 필요할 수 있는 정형화된 보상을 사용하지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline\n' +
      '**Task** & **\\# of Demos** & **Image Input** & **Random Reset** & **Reward Specification** & **Bin Size** & **Training Time** \\\\ \\hline\n' +
      '**PCB 부품 삽입** & 20 & 2 손목 카메라 & True & Ground Truth & 10cm\\(\\times\\) 10cm\\(\\times\\) 20분\n' +
      '**Cable Routing** & 20 & 2 wrist camera & True & Binary Classifier & 20cm\\(\\times\\) 20cm\\(\\times\\) 20cm&31 mins\\\\\\2\n' +
      '**Object Relocation (Forward-Backward)** & 20 & 1 wrist, 1 side camera & False & Binary Classifier & 20cm \\(\\times\\) 30cm & 105 mins \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 작업 매개변수: BC 및 RL 모두에 대한 데모 수집과 온라인 교육 동안, 각 에피소드의 초기 엔드-이펙터 포즈는 PCB 및 Cable 작업에 대해 고정된 영역 내에서 랜덤하게 균일하게 재설정되는 반면, 자유 부동 객체 재배치 작업은 각 빈의 중심 위에서 재설정된다.\n' +
      '\n' +
      '그림 6: 성공률 비교: 작업당 100번의 시행에 대해 평가했을 때, 학습된 RL 정책은 큰 마진, 객체 재배치는 **by 1.7x**, 케이블 라우팅은 **5x**, PCB 삽입은 **10x**에서 BC 정책을 능가했다.\n' +
      '\n' +
      '그림 7: 주기 시간 비교: 로봇이 각 작업에서 성공하는 데 걸리는 평균 시간을 기록했다. RL 정책은 세 가지 작업 모두에 대해 100개의 고품질 인간 원격 조작 시위로 훈련된 BC 정책보다 적어도 **2x** 더 빠르다.\n' +
      '\n' +
      '일부 이전 작업들은 피한다 이러한 이전 작업에서 보고된 결과는 일반적으로 더 낮은 성공률 또는 더 긴 훈련 시간 또는 둘 다를 가지며, 이는 샘플 효율적인 RL 일치의 구현을 제안하거나 적어도 이러한 유형의 작업에 대해 문헌에서 최첨단 방법의 성능을 초과한다. Spector et al.(Spector and Castro, 2021)의 작업에서 우리와 가장 근접한 성능은 삽입에 특정한 다수의 설계 결정 및 귀납적 편향을 포함하는 반면, 우리의 방법은 일반적이며 최소한의 작업별 가정을 수행한다. 우리 시스템의 구성 요소는 모두 (최근) 이전 작업을 기반으로 하지만, 이 조합의 최첨단 성능은 우리의 주요 논문을 보여준다: 얼마나 깊은 RL 방법이 구현되는지에 대한 세부 사항은 큰 차이를 만들 수 있다.\n' +
      '\n' +
      '재현성: SERL의 핵심 임무는 설정 장벽을 낮추고 다른 로봇 시스템에서 재현 가능한 로봇 RL을 장려하는 것이다. 이를 위해, 우리는 다른 기관에서 운영되는 로봇 암에 SERL 소프트웨어 제품군을 성공적으로 통합했음을 보여준다.\n' +
      '\n' +
      '워싱턴 대학의 연구원들은 Functional Manipulation Benchmark(Luo et al., 2024)의 3D 인쇄 부품을 사용하여 Peg 삽입 작업을 설정하고 이 어려운 작업을 해결하기 위해 SERL을 사용했다. 관련 하드웨어 및 소프트웨어 설정을 포함한 전체 준비 시간은 3시간 미만입니다. 이 정책은 19분 만에 수렴되었고 20번의 초기 인간 시연으로 100/100 성공률을 달성하여 우리의 결과를 성공적으로 재현했다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '우리는 연구자와 실무자 모두에게 실제 RL을 보다 쉽게 접근할 수 있도록 하는 것을 목표로 하는 로봇 강화 학습을 위한 시스템 및 소프트웨어 패키지를 설명했다. 당사의 소프트웨어 패키지는 샘플 효율적인 RL을 위해 신중하게 설계된 성분 조합, 보상 설계 자동화, 순방향 컨트롤러로 환경 재설정 자동화, 연락처가 풍부한 조작 작업에 특히 적합한 컨트롤러 프레임워크를 제공합니다. 또한, 본 프레임워크의 실험적 평가는 적은 수의 시연과 함께 제공될 때 정책당 1시간 미만의 훈련으로 다양한 조작 작업을 매우 효율적으로 학습할 수 있음을 보여준다. 이러한 결과는 문헌의 조작에 대한 RL의 최신 결과와 질적으로 잘 비교되며, 이는 본 프레임워크의 특정 선택이 이미지 관찰에서도 매우 우수한 실제 결과를 얻는 데 적합함을 나타낸다. 우리의 프레임워크에는 여러 가지 제한 사항이 있습니다. 첫째, 우리는 모든 가능한 RL 방법을 포함하는 포괄적인 라이브러리를 제공하는 것을 목표로 하지 않으며, 일부 작업 및 설정은 프레임워크(예: 조작하지 않는 작업)를 벗어날 수 있다. 둘째, 보상 사양의 전체 범위와 재설정 없는 학습 과제는 로봇 RL 연구에서 여전히 열린 문제를 구성한다. 우리의 분류기 기반 보상 및 순방향 제어기는 모든 설정에서 적절하지 않을 수 있다. 로봇 RL을 보다 광범위하게 적용하려면 이러한 주제에 대한 추가 연구가 필요하다. 그러나, 우리는 우리의 소프트웨어 패키지가 실제 RL 방법으로 실험하기를 원하는 연구자와 실무자 모두에게 합리적인 "기본" 출발점을 제공하기를 바란다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '본 연구는 IIS-2150826 산하 내재혁신 LLC, 국립과학재단, ARO W911NF-21-1-0097의 일부 지원을 받았으며, 이 프로젝트에 참여한 레한 아흐마드에게 감사드린다.\n' +
      '\n' +
      '## 7 Appendix\n' +
      '\n' +
      '상대적 관찰과 액션 프레임에 관한### 세부사항\n' +
      '\n' +
      '로봇의 기본 프레임을 \\(\\{s\\}\\); 정책을 실행하는 \\(i\\)번째 에피소드에 대해, 우리는 \\(\\{b_{i}^{(i)}\\}\\)을 엔드 이펙터 프레임으로 표현하였다. 여기서 \\(1\\leq i\\leq M\\), \\(0\\leq t\\leq N\\). 각 에피소드에 대해 무작위화 영역을 지정하는 균일 분포에서 \\(\\{b_{0}^{(i)}\\}\\)을 샘플링한다. 우리는 이러한 고유수용성 정보를 \\(\\{b_{0}^{(i)}\\}\\)에 대하여 표현하고자 한다. 따라서, 이 정책은 로봇의 엔드-이펙터와 타겟 사이의 상대적인 공간 거리가 일관성을 유지하는 한 새로운 위치에 적용될 것이다. 이 방법은 기준 프레임 내에서 특정 전역 위치에 과적합되는 것을 방지한다. 우리는 다음과 같은 동질 변환을 적용하여 이것을 달성한다:\n' +
      '\n' +
      '\\[T_{b_{0}^{(i)}b_{i}^{(i)}}=T_{b_{0}^{(i)}}^{-1}\\cdot T_{b_{i}^{(i)}}\\]\n' +
      '\n' +
      '그림 8: 워싱턴 대학의 페그 삽입 작업\n' +
      '\n' +
      '여기서 우리는 \\(T_{ab}\\)을 사용하여 프레임 \\(\\{a\\}\\)과 \\(\\{b\\}\\) 사이의 균질 변환 행렬을 나타낸다. 우리는 \\(T_{b_{0}^{(0)}b_{1}^{(0)}}\\)에서 추출한 위치와 회전 정보를 정책에 공급한다. 여기서 우리는 다음과 같이 정의된 프레임 \\(\\{a\\}\\)과 \\(\\{b\\}\\) 사이의 균질 변환 행렬을 나타내기 위해 \\(T_{ab}\\)을 사용한다.\n' +
      '\n' +
      '\\[T_{ab}=\\begin{bmatrix}R_{ab}&P_{ab}\\\\0_{1\\times 3}&1\\end{bmatrix}.\\\\\n' +
      '\n' +
      '이 정책은 6자유도(6 DoF) 트위스트 액션을 생성하는데, 이는 현재 관측치를 수신하는 기준 프레임, 즉 \\(\\{b_{t}^{(0)}\\}\\)에서 표현된다. 수학적으로 6 DoF twist action \\(\\mathcal{V}_{t}^{(0)}\\)는 timestep \\(t\\)에서 frame \\(\\{b_{t}^{(i)}\\}\\}\\)으로 표현되었다. 기본 프레임\\(\\{s\\}\\)에서 표현되는 동작\\(\\mathcal{V}_{t}^{(0)^{\\prime}}\\)을 기대하는 로봇의 제어 소프트웨어와 인터페이스하기 위해 Adjoint 매핑을 적용한다.\n' +
      '\n' +
      '\\[\\mathcal{V}_{t}^{(0)^{\\prime}}=[\\text{Ad}_{t}^{(i)}]^{-1}\\mathcal{V}_{t}^{(i)}\\]\n' +
      '\n' +
      '여기서 \\([\\text{Ad}_{t}^{(i)}]\\)는 다음과 같이 정의된 균질 변환 \\(T_{b_{0}^{(i)}}\\)의 함수이다:\n' +
      '\n' +
      '[\\text{Ad}_{t}^{(i)}]=\\begin{bmatrix}R_{b_{t}^{(i)}&0_{3\\times3}\\\\left[p_{b_{t}^{(i)}}\\right]\\timesR_{b_{t}^{(i)}&R_{b_{t}^{(i)}}\\end{bmatrix}.\\]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*11월 1일, 2019, Proceedings_, Volume 100 of _Proceedings of Machine Learning Research_, pages 1300-1313. PMLR, 2019. URL[http://proceedings.mlr.press/v100/ahn20a.html](http://proceedings.mlr.press/v100/ahn20a.html).\n' +
      '* Ball et al. (2023) Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. 오프라인 데이터로 효율적인 온라인 강화 학습. _ arXiv preprint arXiv:2302.02948_, 2023.\n' +
      '* Brockman et al. (2016) Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 오페나이 체육관 ArXiv:1606.01540_, 2016.\n' +
      '* Buchler et al. (2022) Dieter Buchler, Simon Guist, Roberto Calandra, Vincent Berenz, Bernhard Scholkopf, and Jan Peters. 근육 로봇을 사용하여 탁구를 처음부터 배우는 것. _ IEEE Trans. Robotics_, 38(6):3850-3860, 2022. doi: 10.1109/TRO.2022.3176207. URL[https://doi.org/10.1109/TRO.2022.3176207](https://doi.org/10.1109/TRO.2022.3176207).\n' +
      '* Du et al. (2023) Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan Cabi. 성공 탐지기로서의 비전 언어 모델. _ arXiv preprint arXiv:2303.07280_, 2023.\n' +
      '* 5월 3일, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL[https://openreview.net/forum?id=SivuO-bCW](https://openreview.net/forum?id=SivuO-bCW).\n' +
      '* Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedjo: 인터넷 규모의 지식을 가진 개방형 구체화된 에이전트를 구축합니다. _NeurIPS_, 2022. URL[http://papers.nips.cc/paper_files/paper/2022/hash/74a672685cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html](http://papers.nips.cc/paper_files/paper/2022/hash/74a672685cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html)에서,\n' +
      '* Fu et al. (2018) Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. 이벤트를 갖는 가변 역 제어: 데이터 기반 보상 정의를 위한 일반적인 프레임워크. _ 신경 정보 처리 시스템_, 31, 2018의 발전.\n' +
      '* Fujimoto et al.(2018) Scott Fujimoto, Herke van Hoof, and David Meger. 액터-비평가 방법들에서 함수 근사 오차를 어드레싱하는 것. Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1582-1591. PMLR, 2018. URL[http://proceedings.mlr.press/v80/fujimoto18a.html](http://proceedings.mlr.press/v80/fujimoto18a.html).\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 생성적 적대적 그물 신경 정보 처리 시스템_, 27, 2014의 발전.\n' +
      '* Guadarrama et al. (2018) Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, Gabor Bartok, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. TF-에이전트: 텐서플로우에서 강화학습을 위한 라이브러리. [https://github.com/tensorflow/agents] (https://github.com/tensorflow/agents), 2018. URL[https://github.com/tensorflow/agents](https://github.com/tensorflow/agents). [온라인; 25-6월-2019 접속]\n' +
      '* 6월 5일, 2021_, pages 6664-6671. IEEE, 2021a. doi: 10.1109/ICRA48506.2021.9561384. URL[https://doi.org/10.1109/ICRA48506.2021.9561384](https://doi.org/10.1109/ICRA48506.2021.9561384)\n' +
      '* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.\n' +
      '* Han et al. [2015] Weiqiao Han, Sergey Levine, and Pieter Abbeel. Learning compound multi-step controllers under unknown dynamics. In _2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 6435-6442. IEEE, 2015.\n' +
      '* He et al. [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n' +
      '* Heo et al. [2023] Minho Heo, Youngwoon Lee, Doohyun Lee, and Joseph J. Lim. Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, _Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023_, 2023. doi: 10.15607/RSS.2023.XIX.041. URL [https://doi.org/10.15607/RSS.2023.XIX.041](https://doi.org/10.15607/RSS.2023.XIX.041).\n' +
      '* 헤스터와 스톤[2013] 토드 헤스터와 피터 스톤. 탐구: 로봇을 위한 실시간 샘플-효율 강화 학습. _ Machine learning_, 90:385-429, 2013.\n' +
      '* Hill et al. [2018] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines), 2018.\n' +
      '* Hou et al. [2020] Zhimin Hou, Jiajun Fei, Yuelin Deng, and Jing Xu. Data-efficient hierarchical reinforcement learning for robotic assembly control applications. _IEEE Transactions on Industrial Electronics_, 68(11):11565-11575, 2020.\n' +
      '* Hu et al. [2023a] Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, and Sergey Levine. 재부팅: 효율적인 실세계 능숙한 조작을 부트스트래핑하기 위해 데이터를 재사용합니다. Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, _Proceedings of the 7th Conference on Robot Learning_, volume 229 of _Proceedings of Machine Learning Research_, pages 1930-1949. PMLR, 06-09 Nov 2023a. URL[https://proceedings.mlr.press/v229/hu23a.html](https://proceedings.mlr.press/v229/hu23a.html).\n' +
      '* Hu et al. [2023b] Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, and Sergey Levine. REBOOT: 효율적인 실세계 손재주 조작을 부트스트래핑하기 위한 재사용 데이터 _ CoRR_, abs/2309.03322, 2023b. doi: 10.48550/arXiv.2309.03322. URL[https://doi.org/10.48550/arXiv.2309.03322](https://doi.org/10.48550/arXiv.2309.03322)\n' +
      '* James et al. [2020] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment. _IEEE Robotics Autom. Lett._, 5(2):3019-3026, 2020. doi: 10.1109/LRA.2020.2974707. URL [https://doi.org/10.1109/LRA.2020.2974707](https://doi.org/10.1109/LRA.2020.2974707).\n' +
      '* Johannink et al. [2018] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash Kumar, Matthias Loskyll, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. Residual reinforcement learning for robot control. _CoRR_, abs/1812.03201, 2018. URL [http://arxiv.org/abs/1812.03201](http://arxiv.org/abs/1812.03201).\n' +
      '* Kalashnikov et al. [2021] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. _CoRR_, abs/2104.08212, 2021. URL [https://arxiv.org/abs/2104.08212](https://arxiv.org/abs/2104.08212).\n' +
      '* Konda and Tsitsiklis [2021] Vijay R. 콘다와 존 N. 쯔시클리스 배우 비평 알고리즘 새라 A. 솔라, 토드 K. Leen, and Klaus Robert Muller, editors, _Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999]_, pages 1008-1014. The MIT Press, 1999. URL[http://papers.nips.cc/paper/1786-actor-critic-algorithms](http://papers.nips.cc/paper/1786-actor-critic-algorithms).\n' +
      '* Kostrikov et al. [2023] Ilya Kostrikov, Laura M. Smith, and Sergey Levine. Demonstrating A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. In Kostas E. Bekris, Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, editors, _Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023_, 2023. doi: 10.15607/RSS.2023.XIX.056. URL [https://doi.org/10.15607/RSS.2023.XIX.056](https://doi.org/10.15607/RSS.2023.XIX.056).\n' +
      '* Levine et al. [2016a] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 깊은 시각 운동 정책에 대한 종단 간 교육입니다 J 마흐 배워 Res._ , 17:39:1-39:40, 2016a. URL[http://jmlr.org/papers/v17/15-522.html](http://jmlr.org/papers/v17/15-522.html).\n' +
      '* Levine et al. [2016b] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 깊은 시각 운동 정책에 대한 종단 간 교육입니다 The Journal of Machine Learning Research_, 17(1):1334-1373, 2016b.\n' +
      '* Levine et al. [2018] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. _Int. J. Robotics Res._, 37(4-5):421-436, 2018. doi: 10.1177/0278364917710318. URL [https://doi.org/10.1177/0278364917710318](https://doi.org/10.1177/0278364917710318).\n' +
      '* Li et al. [2021] Kevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H. Pong, Aurick Zhou, Justin Yu, and Sergey Levine. MURAL: meta-learning uncertainty-aware rewards for outcome-driven reinforcement learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 6346-6356. PMLR, 2021. URL [http://proceedings.mlr.press/v139/l12lg.html](http://proceedings.mlr.press/v139/l12lg.html).\n' +
      '* Luo et al. [2018] Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, and Alice M Agogino. Deep reinforcement learning for robotic assembly of mixed deformable and rigid objects. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2062-2069. IEEE, 2018.\n' +
      '* Luo et al. [2021] Jianlan Luo, Eugen Solowjow, Chengtao Wen, Juan Aparicio Ojea, Alice M Agogino, Aviv Tamar, and Pieter Abbeel. Reinforcement learning on variable impedance controller for high-precision robotic assembly. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 3080-3087. IEEE, 2019.\n' +
      '* Luo et al. [2021] Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wenzhao Lian, Chang Su, Mel Vecerik, Ning Ye, Stefan Schaal, and Jonathan Scholz. Robust Multi-Modal Policies for Industrial Assembly via Reinforcement Learning and Demonstrations: A Large-Scale Study. In _Proceedings of Robotics: Science and Systems_, Virtual, July 2021. doi: 10.15607/RSS.2021.XVII.088.\n' +
      '* Luo et al. [2024] Jianlan Luo, Charles Xu, Fangchen Liu, Liam Tan, Zipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey Levine. Emb: a functional manipulation benchmark for generalizable robotic learning, 2024.\n' +
      '* Ma et al. [2023] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. LIV: language representations and rewards for robotic control. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 23301-23320. PMLR, 2023a. URL [https://proceedings.mlr.press/v202/ma23b.html](https://proceedings.mlr.press/v202/ma23b.html).\n' +
      '* Ma et al. [2023b] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: 가치 암시적 사전 훈련을 통해 보편적인 시각적 보상과 표현을 지향한다. _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023b. URL[https://openreview.net/pdf?id=VJ%2wetJ2](https://openreview.net/pdf?id=VJ%2wetJ2).\n' +
      '* Mahler et al. [2017] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics. In Nancy M. Amato, Siddhartha S. Srinivasa, Nora Ayanian, and Scott Kuindersma, editors, _Robotics: Science and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017_, 2017. doi: 10.15607/RSS.2017.XIII.058. URL [http://www.roboticsproceedings.org/rss13/p58.html](http://www.roboticsproceedings.org/rss13/p58.html).\n' +
      '* Mahmoudieh et al. [2017] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell. Zero-shot reward specification via grounded natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 14743-14752. PMLR, 2022. URL [https://proceedings.mlr.press/v162/mahmoudieh22a.html](https://proceedings.mlr.press/v162/mahmoudieh22a.html).\n' +
      '* Mittal et al. (2023) Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Pooria Poorsaru Tehrani, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: 대화형 로봇 학습 환경을 위한 통합 시뮬레이션 프레임워크, 2023.\n' +
      '* Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 심층 강화 학습으로 아타리 연주 arXiv preprint arXiv:1312.5602_, 2013.\n' +
      '*11월 1일, 2019, Proceedings_, Volume 100 of _Proceedings of Machine Learning Research_, pages 1101-1112. PMLR, 2019. URL[http://proceedings.mlr.press/v100/nagabandi20a.html](http://proceedings.mlr.press/v100/nagabandi20a.html).\n' +
      '* Nair & Pong (2020) Ashvin Nair and Vitchyr Pong. rlkit. _ Github_ URL[https://github.com/rail-berkeley/rlkit](https://github.com/rail-berkeley/rlkit)\n' +
      '* Nair et al. (2020) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. 오프라인 데이터셋으로 온라인 강화 학습을 가속화, 2020.\n' +
      '* Popov et al. (2017) Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. 손쉬운 조작을 위한 데이터 효율적인 심층 강화 학습. _ ArXiv:1704.03073_, 2017.\n' +
      '* Rafailov et al. (2017) Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. 잠재 공간 모델을 가진 이미지로부터 오프라인 강화 학습. Ali Jadbabaie, John Lygeros, George J. Pappas, Pablo A. Parrilo, Benjamin Recht, Claire J. Tomlin, Melanie N. Zeilinger, editors, _Proceedings of 3rd Annual Conference on Learning for Dynamics and Control, L4DC 2021, 7-8 June 2021, Virtual Event, Switzerland_, volume 144 of _Proceedings of Machine Learning Research_, pages 1154-1168. PMLR, 2021. URL[http://proceedings.mlr.press/v144/rafailov21a.html](http://proceedings.mlr.press/v144/rafailov21a.html).\n' +
      '* Rajeswaran et al. (2018) Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. 심층 강화 학습과 시연으로 복잡한 능숙한 조작을 학습합니다. In _Proceedings of Robotics: Science and Systems_, Pittsburgh, Pennsylvania, June 2018. Doi: 10.15607/RSS.2018.XIV.049.\n' +
      '* Riedmiller et al. (2009) Martin Riedmiller, Thomas Gabel, Roland Hafner, and Sascha Lange. 로봇 축구를 위한 강화 학습. _ Autonomous Robots_, 27:55-73, 2009.\n' +
      '* Schoettler et al. (2019) Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. 시각적 입력과 자연스러운 보상이 있는 산업 삽입 작업을 위한 심층 강화 학습, 2019.\n' +
      '* Schoettler et al. (2020) Gerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar Bahl, Juan Aparicio Ojea, Eugen Solowjow, and Sergey Levine. 시각적 입력과 자연스러운 보상으로 산업 삽입 작업을 위한 심층 강화 학습입니다. _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)_에서, 페이지 5548-5555, 2020. doi: 10.1109/IROS45743.2020.9341714.\n' +
      '* Seno & Imai (2022) Takuma Seno and Michita Imai. d3rlpy: 오프라인 심층 강화 학습 라이브러리_ Journal of Machine Learning Research_, 23(315):1-20, 2022. URL[http://jmlr.org/papers/v23/2-0017.html](http://jmlr.org/papers/v23/2-0017.html)이다.\n' +
      '* Sharma et al. (2021) Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey Levine, and Chelsea Finn. 자율 강화 학습: 벤치마킹과 형식주의. _ arXiv preprint arXiv:2112.09605_, 2021.\n' +
      '* Sharma et al. (2023) Archit Sharma, Ahmed M. 아메드, 레한 아흐마드 첼시 핀 자율 개선 로봇: End-to-End 자율 점운동 강화 학습. _ CoRR_, abs/2303.01488, 2023. doi: 10.48550/arXiv.2303.01488. URL[https://doi.org/10.48550/arXiv.2303.01488](https://doi.org/10.48550/arXiv.2303.01488).\n' +
      '* 삽입을 위한 확장 가능한 솔루션, 2021.\n' +
      '* Tebbe et al. (2019) Jonas Tebbe, Lukas Krauch, Yapeng Gao, and Andreas Zell. 로봇 탁구에서의 샘플 효율적인 강화 학습. _2021 IEEE 국제 회의 on robotics and automation (ICRA)_, pages 4171-4178. IEEE, 2021.\n' +
      '* Vecerik et al. (2018) Mel Vecerik, Oleg Sushkov, David Barker, Thomas Rothorl, Todd Hester, and Jon Scholz. 딥 강화 학습, 2018을 사용하여 가변 소켓 위치를 갖는 삽입에 대한 실용적인 접근법이다.\n' +
      '* Westenbroek et al. (2022) Tyler Westenbroek, Fernando Castaneda, Ayush Agrawal, Shankar Sastry, and Koushil Sreenath. 강력하고 효율적인 로봇 강화 학습을 위한 라야푸노프 설계 _ arXiv preprint arXiv:2208.06721_, 2022.\n' +
      '* Wu et al. (2022) Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. 공상가: 물리적 로봇 학습을 위한 세계 모델들. Karen Liu, Dana Kulic, and Jeffrey Ichnowski, Editors, _Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand_, volume 205 of _Proceedings of Machine Learning Research_, pages 2226-2240. PMLR, 2022. URL[https://proceedings.mlr.press/v205/wu23c.html](https://proceedings.mlr.press/v205/wu23c.html).\n' +
      '* Xie et al. (2022) Annie Xie, Fahim Tajwar, Archit Sharma, and Chelsea Finn. 도움을 요청할 때: 자율 강화 학습에 대한 사전적 개입. _NeurIPS_, 2022. URL[http://papers.nips.cc/paper_files/paper/2022/hash/6b82c56a438baa8be65a70-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/6b82c56a5fa0287c438baa8be65a70-Abstract-Conference.html)에서,\n' +
      '* Yang et al. (2020) Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani. 다리형 로봇을 위한 데이터 효율적인 강화 학습. _Conference on Robot Learning_, pages 1-10. PMLR, 2020.\n' +
      '*11월 1일, 2019, Proceedings_, Volume 100 of _Proceedings of Machine Learning Research_, pages 1094-1100. PMLR, 2019. URL[http://proceedings.mlr.press/v100/yu20a.html](http://proceedings.mlr.press/v100/yu20a.html).\n' +
      '* Zhan et al. (2021) Albert Zhan, Ruihan Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. 효율적인 로봇 조작을 위한 프레임워크입니다. In _Deep RL Workshop NeurIPS 2021_, 2021.\n' +
      '* Zhao et al.(2022) Tony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevcevicuite, Nicolas Heess, Jon Scholz, Stefan Schal, and Sergey Levine. 산업체 삽입을 위한 오프라인 메타 강화 학습. _2022 국제 로봇 및 자동화 회의(ICRA)_에서, 페이지 6386-6393, 2022. doi: 10.1109/ICRA46639.2022.9812312.\n' +
      '* Zhu et al. (2019) Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, and Vikash Kumar. 심층 강화 학습을 통한 익스트림 조작: 효율적이고 일반적이며 비용이 저렴합니다. _International Conference on Robotics and Automation, ICRA 2019, Montreal, QC, Canada, May 20-24, 2019_, pages 3651-3657. IEEE, 2019. doi: 10.1109/ICRA.2019.8794102. URL[https://doi.org/10.1109/ICRA.2019.8794102](https://doi.org/10.1109/ICRA.2019.8794102).\n' +
      '* Zhu et al. (2020) Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, and Sergey Levine. 현실 세계 로봇 강화 학습의 구성 요소입니다. _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL[https://openreview.net/forum?id=rJe2syrtvS](https://openreview.net/forum?id=rJe2syrtvS).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>