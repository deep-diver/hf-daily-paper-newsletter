<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PokELLMon: A Human-Parity Agent for Pokemon Battles\n' +
      '\n' +
      'with Large Language Models\n' +
      '\n' +
      ' Sihao Hu, Tiansheng Huang, Ling Liu\n' +
      '\n' +
      'Georgia Institute of Technology\n' +
      '\n' +
      'Atlanta, GA 30332, United States\n' +
      '\n' +
      '{sihaohu, thuang, ling.liu}@gatech.edu\n' +
      '\n' +
      '[https://poke-llm-on.github.io/](https://poke-llm-on.github.io/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce PokELLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokELLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the _panic switching_ phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokELLMon\'s human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: [https://github.com/git-disl/PokeLLMon](https://github.com/git-disl/PokeLLMon).\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Generative AI and Large Language Models (LLMs) have shown unprecedented success on NLP tasks (Ouyang et al., 2022; Brown et al., 2020; Xi et al., 2023; Wang et al., 2023b). One of the forthcoming advancements will be to explore how LLMs can autonomously act in the physical world with extended generation space from text to action, representing a pivotal paradigm in the pursuit of Artificial General Intelligence (Goertzel and Pennachin, 2007; Goertzel, 2014).\n' +
      '\n' +
      'Games are suitable test-beds to develop LLM-embodied agents (Duan et al., 2022; Batra et al., 2020) to interact with the virtual environment in a way resembling human behavior. For example, Generative Agents (Park et al., 2023) conducts a social experiments with LLMs assuming various roles in a "The Sims"-like sandbox, where agents exhbit behavior and social interactions mirroring humans. In Mincraft, decision-making agents (Wang et al., 2023a;c; Singh et al., 2023) are designed to explore the world and develop new skills for solving tasks and making tools.\n' +
      '\n' +
      'Compared to existing games, tactical battle games (Ma et al., 2023) are better suited for benchmarking the game-playing ability of LLMs as the win rate can be directly measured and consistent opponents like AI or human players are always available. Pokemon battles, serving as a mechanism that evaluates the battle abilities of trainers in the well-known Pokemon games, offer several unique advantages as the first attempt for LLMs to play tactical battle games:\n' +
      '\n' +
      '(1) The state and action spaces are discrete and can be translated into text losslessly. **Figure** 1 is an illustrative example for a Pokemon battle: At each turn, the player is requested to generate an action to perform given the current state of Pokemon from each side. The action space consists of four moves and five possible Pokemon to switch; (2) The turn-based format eliminates the demands of intensive gameplay, alleviating the stress on the inference time cost for LLMs, making performance hinges solely on the reasoning abilities of LLMs; (3) Despite its seemingly simple mechanism, Pokemon battle is strategic and complex: an experienced player takes various factors into consideration, including species/type/ability/stats/item/moves of all the Pokemon on and off the field. In a random battle, each Pokemon is\n' +
      '\n' +
      'Figure 1: At each turn, the player is requested to decide which action to perform, _i.e._, whether to let _Dragonite_ to take a move or switch to another Pokemon off the field.\n' +
      '\n' +
      'randomly selected from a large candidate pool (more than 1,000) with distinct characteristics, demanding the players both the Pokemon knowledge and reasoning ability.\n' +
      '\n' +
      '**Scope and Contributions:** The scope of this paper is to develop an LLM-embodied agent that mimics the way a human player engages in Pokemon battles. The objective is to explore the key factors that make the LLM-embodied agent a good player and to examine its strengths and weaknesses in battles against human players. To enable LLMs play game autonomously, we implement an environment that can parse and translate battle state into text description, and deliver generated action back to the server. By evaluating existing LLMs, we identify the presence of _hallucination_, and the _panic switching_ phenomenon.\n' +
      '\n' +
      '_Hallucination_: The agent can mistakenly send out Pokemon at a type disadvantage or persist in using ineffective moves against the opponent. As a result, the most advanced LLM, GPT-4, achieves a win rate of 26% when playing against a heuristic bot, compared to 60% win rate of human players. To combat hallucination, we introduce two strategies: (1) In-context reinforcement learning: We provide the agent with text-based feedback _instantly_ derived from the battle, serving as a new form of "reward" to _iteratively_ refine the action generation policy without training; (2) Knowledge-augmented generation: We equip the agent with Pokedex, an encyclopaedia in Pokemon games that provides external knowledge like type advantage relationship or move/ability descriptions, simulating a human player searching for the information of unfamiliar Pokemon.\n' +
      '\n' +
      '_Panic switching_: We discover that when the agent encounters a powerful Pokemon, it tends to panic and generates inconsistent actions like switching different Pokemon in consecutive turns to elude the battle, a phenomenon that is especially pronounced with Chain-of-Thought (Wei et al., 2022) reasoning. Consistent action generation alleviates the issue by voting out the most consistent action without overthinking. This observation mirrors human behavior, where in stressful situations, overthinking and exaggerating difficulties can lead to panic and impede acting.\n' +
      '\n' +
      'Online battles demonstrate PokELMon\'s human-competitive battle abilities: it achieves a 49% win rate in the Ladder competitions and a 56% win rate in the invited battles. Furthermore, we reveal its vulnerabilities to human players\' attrition strategies and deceptive tricks.\n' +
      '\n' +
      'In summary, this paper makes four original contributions:\n' +
      '\n' +
      '* We implement and release an environment that enables LLMs to autonomously play Pokemon battles.\n' +
      '* We propose in-context reinforcement learning to instantly and iteratively refine the policy, and knowledge-augmented generation to combat hallucination.\n' +
      '* We discover that the agent with chain-of-thought experiences panic when facing powerful opponents, and consistent action generation can mitigate this issue.\n' +
      '* PokELMon, to the best of our knowledge, is the first LLM-embodied agent with human-parity performance in tactical battle games.\n' +
      '\n' +
      '## 2 LLMs as Game Players\n' +
      '\n' +
      '**Communicative games:** Communicative games revolve around communication, deduction and sometimes deception between players. Existing studies show that LLMs demonstrate strategic behaviors in board games like Werewolf (Xu et al., 2023), Avalane (Light et al., 2023), WorldWar II (Hua et al., 2023) and Diplomacy (Bakhtin et al., 2022).\n' +
      '\n' +
      '**Open-ended games:** Open-ended games allow players to freely explore the game world and interact with others. Generative Agent (Park et al., 2023) showcases that LLM-embodied agents exhibit behavior and social interactions mirroring human-like patterns. In MineCraft, Voyager (Wang et al., 2023) employs curriculum mechanism to explore the world and generates and executes code for solving tasks. DEPS (Wang et al., 2023) proposes an approach of "Describe, Explain, Plan and Select" to accomplish 70+ tasks. Planing-based frameworks like AutoGPT (Significant Gravitas) and MetaGPT (Hong et al., 2023) can be adopted for the exploration task as well.\n' +
      '\n' +
      '**Tactic battle games:** Among various game types, tactical battle games (Akata et al., 2023; Ma et al., 2023) are particularly suitable for benchmarking LLMs\' game-playing ability, as the win rate can be directly measured, and consistent opponents are always available. Recently, LLMs are employed to play StarCraft II (Ma et al., 2023) against the built-in AI with a text-based interface and a chain-of-summarization approach. In comparison, PokELMon has several advantages: (1) Translating Pokemon battle state into text is lossless; (2) Turn-based format eliminates real-time stress given the inference time cost of LLMs; (3) Battling against disciplined human players elevates the difficulty to a new height.\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      '### Pokemon\n' +
      '\n' +
      '**Species:** There are more than 1,000 Pokemon species (bul, 2024c), each with its unique ability, type(s), statistics (stats) and battle moves. **Figure**2 shows two representative Pokemon: _Charizard_ and _Venusaur_.\n' +
      '\n' +
      '**Type:** Each Pokemon species has up to two elemental types, which determine its advantages and weaknesses. **Figure**3 shows the advantage/weakness relationship between 18 types of attack moves and attacked Pokemon. For ex ample, fire-type moves like "Fire Blast" of _Charizard_ can cause double damage to grass-type Pokemon like _Venusaur_, while _Charizard_ is vulnerable to water-type moves.\n' +
      '\n' +
      '**Stats:** Stats determine how well a Pokemon performs in battles. There are four stats: (1) Hit Points (HP): determines the damage a Pokemon can take before fainting; (2) Attack (Atk): affects the strength of attack moves; (3) Defense (Def): dictates resistance against attacks; (4) Speed (Spe): determines the order of moves in battle.\n' +
      '\n' +
      '**Ability:** Abilities are passive effects that can affect battles. For example, _Charizard_\'s ability is "Blaze", which enhances the power of its fire-type moves when its HP is low.\n' +
      '\n' +
      '**Move:** A Pokemon can learn four battle moves, categorized as attack moves or status moves. An attack move deals instant damage with a power value and accuracy, and associated with a specific type, which often correlates with the Pokemon\'s type but does not necessarily align; A status move does not cause instant damage but affects the battle in various ways, such as altering stats, healing or protect Pokemon, or battle conditions, _etc_. There are 919 moves in total with distinctive effect (bul, 2024b).\n' +
      '\n' +
      '### Battle Rule\n' +
      '\n' +
      'In one-to-one random battles (Wikipedia, 2023), two battlers face off, each with six randomly selected Pokemon. Initially, each battle sends out one Pokemon onto the field, keeping the others in reserve for future switches. The objective is to make all the opponent\'s Pokemon faint (by reducing their HP to zero) while ensuring that at least one of own Pokemon remains unfainted. The battle is turn-based: at the start of each turn, both players choose an action to perform. Actions fall into two categories: (1) taking a move, or (2) switching to another Pokemon. The battle engine executes actions and updates the battle state for the next step. If a Pokemon fails after a turn and the battler has other Pokemon unfainted, the battle engine forces a switch, which does not count as the player\'s action for the next step. After a forced switch, the player can still choose a move or make another switch.\n' +
      '\n' +
      '## 4 Battle Environment\n' +
      '\n' +
      '**Battle Engine:** The environment interacts with a battle engine server called Pokemon showdown (pok, 2024), which provides a web-based GUI for human players, as well as web APIs for interacting with message in defined formats.\n' +
      '\n' +
      '**Battle Environment:** We implement a battle environment based on (Sahovic, 2023a) to support LLMs autonomously play Pokemon battles. **Figure**4 illustrates how the entire framework works. At the beginning of a turn, the environment get an action-request message from the server, including the execution result from the last turn. The environment first parses the message and update local state variables, and then translates the state variables into text. The text description primarily consists of four parts: (1) Own team information, including the attributes of Pokemon both on-the-field and off-the-field; (2) Opponent team information including the attributes of opposing Pokemon on-the-field and off-the-field (some are unknown); (3) Battle field information like the weather, entry hazard and terrain; (4) Historical turn log information, including previous actions of both side Pokemon, which is stored in a log queue. LLMs take the translated state as input and output an action for the next step. The action is sent to the server and executed alongside the action chosen by the human player.\n' +
      '\n' +
      '## 5 Preliminary Evaluation\n' +
      '\n' +
      'To gain insights into the challenges associated with Pokemon battles, we evaluate the abilities of existing LLMs, including GPT-3.5 (Ouyang et al., 2022), GPT-4 (Achiam et al., 2023), and LLaMA-2 (Touvron et al., 2023),\n' +
      '\n' +
      '### Pokemon Battles\n' +
      '\n' +
      'Placing LLMs in direct competitions against human players is time-consuming as human needs time to think (4 minutes for 1 battle in average). To save time, we adopt a heuris\n' +
      '\n' +
      'Figure 3: Type advantage/weakness relationship. “\\(+\\)” denotes super-effective (2x damage); “\\(-\\)” denotes ineffective (0.5x damage); “\\(\\times\\)” denotes no effect (0x damage). Unmarked is standard (1x) damage.\n' +
      '\n' +
      'Figure 2: Two representative Pokemon: _Charizard_ and _Venusaur_. Each Pokemon has type(s), ability, stats and four battle moves.\n' +
      '\n' +
      'tic bot (Sahovic, 2023b) to initially battle against human players in the Ladder competitions, and then use the bot to benchmark existing LLMs. The bot is programmed to use status boosting moves, set entry hazards, selecting the most effective actions by considering the stats of Pokemon, the power of moves, and type advantages/weaknesses.\n' +
      '\n' +
      'The statistic results are presented in **Table**1, where the battle score is defined as the sum of the numbers of the opponent\'s fainted Pokemon and the player\'s unfainted Pokemon at the end of a battle. Consequently, the opponent player\'s battle score is equal to 12 minus the player\'s battle score. Random is a simple strategy that randomly generates an action every time, and MaxPower chooses the move with the highest power value. Obviously, GPT-3.5 and LLaMA-2 are just slightly better than Random and even GPT-4 cannot beat the bot, let along well-disciplined human players from the Ladder competitions.\n' +
      '\n' +
      'By observing LLMs play battles and analyzing the explanations generated with their actions, we identify the occurrence of hallucination (Rawte et al., 2023; Cabello et al., 2023): LLMs can mistakenly claim non-existent type-advantage relationships or, even worse, reverse the advantage relationships between types like sending a grass-type Pokemon to face with a fire-type Pokemon. A clear understanding of type advantage/weakness is crucial in Pokemon battles, as choosing a Pokemon with a type advantage can result in dealing more damage and sustaining less.\n' +
      '\n' +
      '### Test of Hallucination\n' +
      '\n' +
      'To assess hallucination in the outputs of LLMs, we construct the task of type advantage/weakness prediction. The task involves asking LLMs to determine if an attack of a certain type is A. super-effective (2x damage), B. standard (1x damage), C. ineffective (0.5x damage) or D. no effect (0x damage) against a certain type of Pokemon. The 324 (18x18) testing pairs are constructed based on **Figure**3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline\n' +
      '**Player** & **Win rate \\(\\uparrow\\)** & **Score \\(\\uparrow\\)** & **Turn \\#** & **Battle \\#** \\\\ \\hline Human & 59.84\\% & 6.75 & 18.74 & 254 \\\\ Random & 1.2\\% & 2.34 & 22.37 & 200 \\\\ MaxPower & 10.40\\% & 3.79 & 18.11 & 200 \\\\ LLaMA-2 & 8.00\\% & 3.47 & 20.98 & 200 \\\\ GPT-3.5 & 4.00\\% & 2.61 & 20.09 & 100 \\\\ GPT-4 & 26.00\\% & 4.65 & 19.46 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Performance of LLMs in battles against the bot.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c|c c c|c c c} \\hline \\hline\n' +
      '**Model** & \\multicolumn{3}{c|}{**LLaMA-2**} & \\multicolumn{3}{c|}{**GPT-3.5**} & \\multicolumn{3}{c}{**GPT-4**} \\\\ \\hline Class & A & B & C & D & A & B & C & D & A & B & C & D \\\\ \\hline A & 5 & 46 & 0 & 0 & 0 & 0 & 49 & 2 & 37 & 8 & 5 & 1 \\\\ B & 25 & 179 & 0 & 0 & 2 & 16 & 185 & 11 & 0 & 1885 & 17 & 2 \\\\ C & 15 & 46 & 0 & 0 & 0 & 2 & 87 & 2 & 3 & 24 & 22 & 2 \\\\ D & 1 & 7 & 0 & 0 & 0 & 7 & 1 & 0 & 0 & 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Confusion matrices for type advantage prediction.\n' +
      '\n' +
      'Figure 4: The framework that enables LLMs to battle with human players: It parses the messages received from the battle server and translates state logs into text. LLMs take these state descriptions and historical turn logs as input and generates an action for the next step. The action is then sent to the battle server and executed alongside the action chosen by the opponent player.\n' +
      '\n' +
      '## 6 PokELLMon\n' +
      '\n' +
      '**Overview:** The overall framework of PokELLMon is illustrated in **Figure**5. In each turn, PokELLMon uses previous actions and corresponding text-based feedback to _iteratively_ refine the policy, and also augments the current state information with external knowledge, such as type advantage/weakness relationships and move/ability effects. Given above information as input, it independently generates multiple actions and selects the most consistent ones as the final output for execution.\n' +
      '\n' +
      '### In-Context Reinforcement Learning (ICRL)\n' +
      '\n' +
      'Human players make decisions based not only on the current state but also on the (implicit) feedback from previous actions, such as the change in a Pokemon\'s HP over two consecutive turns following an attack by a move. Without feedback provided, the agent can continuously stick to the same erroneous action. As illustrated in **Figure**6, the agent uses "Crabhammer", a water-type attack move against the opposing _Toxicroak_, a Pokemon with the ability "Dry Skin", which can nullify damage from water-type moves. The "Immune" message displayed in the battle animation can prompt a human player to change actions even without knowledge of "Dry Skin", however, is not included in the state description. As a result, the agent repeats the same action, inadvertently giving the opponent two free turns to triple _Toxicroak_\'s attack stats, leading to defeat.\n' +
      '\n' +
      'Reinforcement Learning (Schulman et al., 2017; Mnih et al., 2016; Hafner et al., 2023) requires numeric rewards to evaluate actions for refining policy. As LLMs can understand languages and distinguish what is good and bad, text-based feedback description provides a new form of "reward". By incorporating text-based feedback from the previous turns into the context, the agent is able to refine its "policy" _iteratively_ and _instantly_ during serving, namely In-Context Reinforcement Learning (ICRL).\n' +
      '\n' +
      'In practice, we generate four types of feedback: (1) The change in HP over two consecutive turns, which reflects the actual damage caused by an attack move; (2) The effectiveness of attack moves, indicating whether they are super-effective, ineffective, or have no effect (immunity) due to type advantages or ability/move effects; (3) The priority of move execution, providing a rough estimate of speed, as precise stats for the opposing Pokemon are unavailable; (4) The actual effects of executed moves: both status and certain attack moves can cause outcomes like stat boosts or debuffs, recover HP, inflict conditions such as poison, burns, or freezing, _etc_. **Figure**4 presents several instances of generated text-based feedback for ICLR.\n' +
      '\n' +
      '### Knowledge-Augmented Generation (KAG)\n' +
      '\n' +
      'Although ICRL can mitigate the impact of hallucination, it can still cause fatal consequences before the feedback is received. For example, if the agent sends out a grass-type Pokemon against a fire-type Pokemon, the former is likely be defeated in a single turn before the agent realize it is a bad decision. To further reduce hallucination, Retrieval-Augmented Generation (Lewis et al., 2020; Guu et al., 2020; Patil et al., 2023) employ external knowledge to augment\n' +
      '\n' +
      'Figure 5: PokELLMon is equipped with three strategies: (1) ICRL that leverages instant feedbacks from the battle to iteratively refine generation; (2) KAG that retrieves external knowledge to combat hallucination and to act timely and properly; (3) Consistent Action Generation to prevent the panic switching problem.\n' +
      '\n' +
      'Figure 6: The agent repeatedly uses the same attack move but has zero effect to the opposing Pokemon due to its ability “Dry Skin.”\n' +
      '\n' +
      'Figure 7: In turn 3, the agent uses “Psyshock”, which cause zero damage to the opposing Pokémon. With ICRL, the agent switch to another Pokémon.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline\n' +
      '**Player** & **Win rate \\(\\uparrow\\)** & **Score \\(\\uparrow\\)** & **Turn \\#** & **Battle \\#** \\\\ \\hline Human & 59.84\\% & 6.75 & 18.74 & 254 \\\\ Origin & 26.00\\% & 4.65 & 19.46 & 100 \\\\ ICRL & 36.00\\% & 5.25 & 20.64 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Performance of ICRL in battles against the bot.\n' +
      '\n' +
      'generation. In this section, we introduce two types of external knowledge to fundamentally mitigate hallucination.\n' +
      '\n' +
      '**Type advantage/weakness relationship:** In the original state description in **Figure**4, we annotate all the type information of Pokemon and moves to let the agent infer the type advantage relationship by itself. To reduce the hallucination contained in the reasoning, we explicitly annotate the type advantage and weakness of the opposing Pokemon and our Pokemon with descriptions like "_Charizard_ is strong against grass-type Pokemon yet weak to the fire-type moves".\n' +
      '\n' +
      '**Move/ability effect:** Given the numerous moves and abilities with distinct effects, it is challenging to memorize all of them even for experienced human players. For instance, it\'s difficult to infer the effect of a status move based solely on its name: "Dragon Dance" can boost the user\'s attack and speed by one stage, whereas "Haze" can reset the boosted stats of both Pokemon and remove abnormal statuses like being burnt. Even attack moves can have additional effects besides dealing damage.\n' +
      '\n' +
      'We collect all the effect descriptions of moves, abilities from Bulbapedia (bul, 2024b; a) and store them into a Pokedex, an encyclopaedia in Pokemon games. For each Pokemon on the battlefield, its ability effect and move effects are retrieved from the Pokedex and added to the state description.\n' +
      '\n' +
      '**Table**4 shows the results of generations augmented with two types of knowledge, where type advantage relationship (KAG[Type]) significantly boosts the win rate from 36% to 55%, whereas, Move/ability effect descriptions also enhance the win rate by 4 AP. By combining two of them, KAG achieves a win rate of 58% against the heuristic bot, approaching a level competitive with human.\n' +
      '\n' +
      'With external knowledge, we observe that the agent starts to use very special moves at proper time. As an example shown in **Figure**8, a steel-type _Klefki_ is vulnerable to the ground-type attack of the opposing _Rhydon_, a ground-type Pokemon. Usually in such a disadvantage, the agent will choose to switch to another Pokemon, however, it chooses to use the move "Magnet Rise", which levitates the user to make it immune to ground-type moves for five turns. As a result, the ground-type attack "Earthquake" of the opposing _Rhydon_ becomes invalid.\n' +
      '\n' +
      '### Consistent Action Generation\n' +
      '\n' +
      'Existing studies (Wei et al., 2022; Yao et al., 2022; Shinn et al., 2023; Bommasani et al., 2021; Hu et al., 2023) show that reasoning and prompting can improve the ability of LLMs on solving complex tasks. Instead of generating a one-shot action, we evaluate existing prompting approaches including Chain-of-Thought (Wei et al., 2022) (CoT), Self-Consistency (Wang et al., 2022) (SC) and Tree-of-Thought (Yao et al., 2023) (ToT). For CoT, the agent initially generates a thought that analyzes the current battle situation and outputs an action conditioned on the thought. For SC (k=3), the agent generates three times of actions and select the most voted answer as the output. For ToT (k=3), the agent generates three action options and picks out the best one evaluated by itself.\n' +
      '\n' +
      'As introduced in Section 3.2, for each turn there is single action can be taken, which means if the agent chooses to switch yet the opponent choose to attack, the switch-in Pokemon will sustain the damage. Usually switching happens when the agent decides to leverage the type advantage of an off-the-battle Pokemon, and thus the damage taken is sustainable since the switch-in Pokemon is typically type-resistant to the opposing Pokemon\'s moves. However, when the agent with CoT reasoning faces a powerful opposing Pokemon, its actions become inconsistent by switching to different Pokemon in consecutive turns, which we call _panic switching_. _panic switching_ wastes chances of taking moves and leading to the defeat. An illustrative example is shown\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline\n' +
      '**Player** & **Win rate \\(\\uparrow\\)** & **Score \\(\\uparrow\\)** & **Turn \\#** & **Battle \\#** \\\\ \\hline Human & 59.84\\% & 6.75 & 18.74 & 254 \\\\ Origin & 36.00\\% & 5.25 & 20.64 & 100 \\\\ KAG[Type] & 55.00\\% & 6.09 & 19.28 & 100 \\\\ KAG[Effect] & 40.00\\% & 5.64 & 20.73 & 100 \\\\ KAG & 58.00\\% & 6.53 & 18.84 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Performance of KAG in battles against the bot.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline\n' +
      '**Player** & **Win rate \\(\\uparrow\\)** & **Score \\(\\uparrow\\)** & **Turn \\#** & **Battle \\#** \\\\ \\hline Human & 59.84\\% & 6.75 & 18.74 & 254 \\\\ Origin & 58.00\\% & 6.53 & 18.84 & 100 \\\\ CoT & 54.00\\% & 5.78 & 19.60 & 100 \\\\ SC (k=3) & **64.00**\\% & 6.63 & 18.86 & 100 \\\\ ToT (k=3) & 60.00\\% & 6.42 & 20.24 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Performance of prompting approaches in battles against the bot.\n' +
      '\n' +
      'Figure 8: The agent understands the move effect and uses it properly: _Klefki_ is vulnerable to the ground-type attack of _Rhydon_. Instead of switching, the agent uses “Magnet Rise”, a move that protects itself from the ground-type attack for five turns, invalidating the ground-type attack “Earthquake” of the opposing _Rhydon_.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:7]\n' +
      '\n' +
      'derstanding of moves like "Toxic", "Recover" and "Protect", as well as the right timing for their use (such as when there\'s no type-weakness or when having high defense). An example with battle animation can be found at: [https://poke-llm-on.github.io](https://poke-llm-on.github.io).\n' +
      '\n' +
      '**Weakness:** PokELMon tends to take actions that can achieve short-term benefits, therefore, making it vulnerable to human players\' attrition strategy that requires long-term effort to break. As shown in the two battles in **Figure**11, after many turns, the agent\'s entire team is defeated by the human players\' Pokemon, which have significantly boosted defense and engage in frequent recovery. **Table**8 reports the performance of PokELMon in battles where human players either use the attrition strategy or not. Obviously, in battles without the attrition strategy, it outperforms Ladder players, while losing the majority of battles when human play the attrition strategy.\n' +
      '\n' +
      'The "Recover" move recovers 50% HP in one turn, which means if an attack cannot cause the opposing Pokemon more than 50% HP damage in one turn, it will never faint. The key to breaking the dilemma is to firstly boost a Pokemon\'s attack to a very high stage and then attack to cause unrecoverable damage, which is a long-term goal that requires joint efforts across many turns. PokELMon is weak to the long-term planing because current design does not keep a long-term plan in mind across many timesteps, which will be included in the future work.\n' +
      '\n' +
      'Finally, we observe that experienced human players can misdirect the agent to bad actions. As shown in **Figure**12, our _Zygarde_ has one chance to use an enhanced attack move. At the end of turn 2, the opposing _Mawile_ is fainted, leading to a forced switch and the opponent choose to switch in _Kyuren_. This switch is a trick that lures the agent uses a dragon-type move in turn 3 because _Kyuren_ is vulnerable to dragon-type attacks. In turn 3, the opponent switches in _Tapu Bulu_ at the beginning, a Pokemon immune to dragon-type attacks, making our enhanced attack chance wasted. The agent is fooled because it makes decision only based on the current state information, while experienced players condition on not only the state information, but also the opponent\'s next action prediction.\n' +
      '\n' +
      'Seeing through tricks and predicting the opponent\'s next action require the agent being disciplined in the real battle environment, which is the future step in our work.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      'In this paper, we enable LLMs to autonomously play the well-known Pokemon battles against human. We introduce PokELMon, the first LLM-embodied agent that achieves human-competent performance in tactical battle games. We introduce three key strategies in the design of PokELMon: (i) In-Context Reinforcement Learning, which consumes the text-based feedback as "reward" to _iteratively_ refine the action generation policy without training; (ii) Knowledge-Augmented Generation that retrieves external knowledge to combat hallucination and ensures the agent act timely and properly; (iii) Consistent Action Generation that prevents the _panic switching_ issue when encountering powerful opponents. The architecture of PokELMon is _general_ and can be adapted for the design of LLM-embodied agents in many other games, addressing the problems of hallucination and action inconsistency.\n' +
      '\n' +
      'Online battles show that PokELMon demonstrates human-like battle ability and strategies, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Furthermore, we uncover its vulnerabilities to human players\' attrition strategies and deception tricks, which are considered as our future work.\n' +
      '\n' +
      'Figure 11: PokELMon suffers from attrition strategies: the opponent players frequently recover high-defense Pokémons. Breaking the dilemma requires joint effects across many turns.\n' +
      '\n' +
      'Figure 12: An experienced human player misdirects the agent to use a dragon-type attack by firstly sending out a dragon-type Pokémon and immediately switch to another Pokémon immune to the dragon-type attack.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c} \\hline \\hline\n' +
      '**Ladder** & **Win rate \\(\\uparrow\\)** & **Score \\(\\uparrow\\)** & **Turn \\#** & **Battle \\#** \\\\ \\hline w. Attrition & 18.75\\% & 4.29 & 33.88 & 16 \\\\ w/o Attrition & 53.93\\% & 6.02 & 15.95 & 89 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Battle performance impacted by the attrition strategy \n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* List of abilities (2024) List of abilities, 2024a. URL [https://bulbapedia.bulbagarden.net/wiki/Ability#List_of_Abilities](https://bulbapedia.bulbagarden.net/wiki/Ability#List_of_Abilities).\n' +
      '* List of moves (2024b) List of moves, 2024b. URL [https://bulbapedia.bulbagarden.net/wiki/List_of_moves](https://bulbapedia.bulbagarden.net/wiki/List_of_moves).\n' +
      '* List of pokemon by national pokedex number (2024c). URL [https://bulbapedia.bulbagarden.net/wiki/List_of_Pokmon_by_National_Pokdex_number](https://bulbapedia.bulbagarden.net/wiki/List_of_Pokmon_by_National_Pokdex_number).\n' +
      '* Pokemon showdown (2024) Pokemon showdown, 2024. URL [https://play.pokemonshowdown.com](https://play.pokemonshowdown.com).\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Akata et al. (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. Playing repeated games with large language models. _arXiv preprint arXiv:2305.16867_, 2023.\n' +
      '* Bakhtin et al. (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. _Science_, 378(6624):1067-1074, 2022.\n' +
      '* Batra et al. (2020) Batra, D., Chang, A. X., Chernova, S., Davison, A. J., Deng, J., Koltun, V., Levine, S., Malik, J., Mordatch, I., Mottaghi, R., et al. Rearrangement: A challenge for embodied ai. _arXiv preprint arXiv:2011.01975_, 2020.\n' +
      '* Bommasani et al. (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* Cabello et al. (2023) Cabello, L., Li, J., and Chalkidis, I. Pokemonchat: Auditing chatgpt for pok\\\'emon universe knowledge. _arXiv preprint arXiv:2306.03024_, 2023.\n' +
      '* Duan et al. (2022) Duan, J., Yu, S., Tan, H. L., Zhu, H., and Tan, C. A survey of embodied ai: From simulators to research tasks. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 6(2):230-244, 2022.\n' +
      '* Goertzel (2014) Goertzel, B. Artificial general intelligence: concept, state of the art, and future prospects. _Journal of Artificial General Intelligence_, 5(1):1, 2014.\n' +
      '* Goertzel & Pennachin (2007) Goertzel, B. and Pennachin, C. _Artificial general intelligence_, volume 2. Springer, 2007.\n' +
      '* Guu et al. (2020) Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In _International conference on machine learning_, pp. 3929-3938. PMLR, 2020.\n' +
      '* Hafner et al. (2023) Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* Hong et al. (2023) Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. Metagpt: Meta programming for multi-agent collaborative framework. _arXiv preprint arXiv:2308.00352_, 2023.\n' +
      '* Hu et al. (2023) Hu, S., Huang, T., Ilhan, F., Tekin, S. F., and Liu, L. Large language model-powered smart contract vulnerability detection: New perspectives. _arXiv preprint arXiv:2310.01152_, 2023.\n' +
      '* Hua et al. (2023) Hua, W., Fan, L., Li, L., Mei, K., Ji, J., Ge, Y., Hemphill, L., and Zhang, Y. War and peace (waragent): Large language model-based multi-agent simulation of world wars. _arXiv preprint arXiv:2311.17227_, 2023.\n' +
      '* Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.\n' +
      '* Light et al. (2023) Light, J., Cai, M., Shen, S., and Hu, Z. From text to tactic: Evaluating llms playing the game of avalon. _arXiv preprint arXiv:2310.05036_, 2023.\n' +
      '* Ma et al. (2023) Ma, W., Mi, Q., Yan, X., Wu, Y., Lin, R., Zhang, H., and Wang, J. Large language models play starcraft ii: Benchmarks and a chain of summarization approach. _arXiv preprint arXiv:2312.11865_, 2023.\n' +
      '* Mnih et al. (2016) Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In _International conference on machine learning_, pp. 1928-1937. PMLR, 2016.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* Ouyang et al. (2023)Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.\n' +
      '* Patil et al. (2023) Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. _arXiv preprint arXiv:2305.15334_, 2023.\n' +
      '* Rawte et al. (2023) Rawte, V., Sheth, A., and Das, A. A survey of hallucination in large foundation models. _arXiv preprint arXiv:2309.05922_, 2023.\n' +
      '* Sahovic (2023a) Sahovic, H. Poke-env: poekemon ai in python, 2023a. URL [https://github.com/hsahovic/poke-env](https://github.com/hsahovic/poke-env).\n' +
      '* Sahovic (2023b) Sahovic, H. poke-env: Heuristicbot, 2023b. URL [https://github.com/hsahovic/poke-env/blob/master/src/poke_env/player/baselines.py](https://github.com/hsahovic/poke-env/blob/master/src/poke_env/player/baselines.py).\n' +
      '* Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n' +
      '* Shinn et al. (2023) Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and self-reflection. _arXiv preprint arXiv:2303.11366_, 2023.\n' +
      '* Significant Gravitas (2023) Significant Gravitas. AutoGPT. URL [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).\n' +
      '* Singh et al. (2023) Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progrpompt: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 11523-11530. IEEE, 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on large language model based autonomous agents. _arXiv preprint arXiv:2308.11432_, 2023b.\n' +
      '* Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_, 2022.\n' +
      '* Wang et al. (2023c) Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. _arXiv preprint arXiv:2302.01560_, 2023c.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* Wikipedia (2023) Wikipedia. Gameplay of poekemon, 2023. URL [https://en.wikipedia.org/wiki/Gameplay_of_Pok%C3%A9mon](https://en.wikipedia.org/wiki/Gameplay_of_Pok%C3%A9mon).\n' +
      '* Xi et al. (2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.\n' +
      '* Xu et al. (2023) Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu, Y. Exploring large language models for communication games: An empirical study on werewolf. _arXiv preprint arXiv:2309.04658_, 2023.\n' +
      '* Yao et al. (2022) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.\n' +
      '* Yao et al. (2023) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>