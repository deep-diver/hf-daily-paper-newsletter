<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 확산 세계 모델\n' +
      '\n' +
      'Zihan Ding\n' +
      '\n' +
      'Amy Zhang\n' +
      '\n' +
      'Yuandong Tian\n' +
      '\n' +
      'Qinqing Zheng\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '다단계 미래 상태와 보상을 동시에 예측할 수 있는 조건부 확산 모델인 확산 월드 모델(DWM)을 소개한다. 전통적인 1단계 역학 모델과 달리 DWM은 단일 전진 패스에서 긴 수평선 예측을 제공하여 재귀적 요구 사항이 필요하지 않다. 우리는 DWM을 모델 기반 가치 추정(Feinberg et al., 2018)에 통합하며, 여기서 단기 수익률은 DWM에서 샘플링된 미래 궤적에 의해 시뮬레이션된다. 오프라인 강화학습의 맥락에서 DWM은 생성적 모델링을 통한 보수적 가치 정례화로 볼 수 있다. 또는 합성 데이터로 오프라인 Q-러닝을 가능하게 하는 데이터 소스로 볼 수 있다. D4RL(Fu et al., 2020) 데이터 세트에 대한 우리의 실험은 긴-수평 시뮬레이션에 대한 DWM의 견고성을 확인한다. 절대 성능 측면에서 DWM은 \\(44\\%\\)의 성능 이득을 갖는 1단계 동역학 모델을 크게 능가하고, 최첨단 성능을 달성한다.\n' +
      '\n' +
      '머신러닝, 확산 월드 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '강화 학습(RL) 알고리즘은 크게 모델 기반(MB) 알고리즘과 모델 프리(MF) 알고리즘의 두 가지 클래스로 분류할 수 있다. MB 알고리즘의 기초는 종종 _world model_(Ha and Schmidhuber, 2018)로 지칭되는 환경 피드백의 예측 모델이다. 월드 모델은 실제 환경의 시뮬레이터 역할을 하며, 액션 검색(Schrittwieser et al., 2020; Ye et al., 2021), 이러한 시뮬레이터 내의 정책 최적화(Sutton, 1991; Dean et al., 2020; Feinberg et al., 2018; Hafner et al., 2019), 또는 양자의 조합(Hansen et al., 2022; 2023; Chitnis et al., 2023)을 통해 이들로부터 정책을 도출할 수 있다.\n' +
      '\n' +
      '세계 모델의 예측 정확도는 MB 접근법의 성능에 매우 중요하다. 실제로, 불가피한 모델링 오류로 인해, MB 방법은 일반적으로 MF 방법에 비해 더 나쁜 성능을 나타내며, 이는 실제 환경과 상호 작용함으로써 정책을 직접 학습한다. 그럼에도 불구하고, MB 방법은 샘플 효율의 이점을 갖는다(Deisenroth et al., 2013; Dean et al., 2020). 그들은 보통 훨씬 적은 환경 상호 작용으로 적절한 성능을 달성하므로 실제 문제를 처리하는 데 더 적합하다. 이것은 모델 기반 RL 영역에서 가장 근본적인 문제 중 하나를 강조한다:_어떻게 세계 모델링 오류를 효과적으로 줄일 수 있습니까?_\n' +
      '\n' +
      '전통적인 세계 모델들은 현재 상태 및 액션에 기초하여 보상 및 다음 상태를 예측하는 1단계 역학 모델들이다(Kaiser et al., 2019; Janner et al., 2019; Yu et al., 2020; Kidambi et al., 2020; Hansen et al., 2022; Hafner et al., 2019; 2020; 2023). 미래에 여러 단계를 계획할 때 이러한 모델이 재귀적으로 호출되어 긴 수평 롤아웃에 대한 오류와 신뢰할 수 없는 예측이 빠르게 축적된다. 그림 1.1은 1단계 역학 모델을 사용한 MB 접근법의 성능을 보여준다. 롤아웃 길이가 증가함에 따라 리턴은 빠르게 붕괴되어, 그러한 모델들에 대한 _compounding error_의 이슈를 강조한다(Asadi et al., 2019; Lambert et al., 2022; Xiao et al., 2019). 최근 다양한 연구에서 볼 수 있듯이 의사 결정 문제를 해결하기 위해 시퀀스 모델링 기법을 활용하는 것에 대한 관심이 높아지고 있다(Chen et al., 2021; Janner et al., 2021; Zheng et al., 2022; Ajay et al., 2022; Micheli et al., 2022; Robine et al., 2023; Zheng et al., 2023). 이것은 우리 논문이 답하고자 하는 흥미로운 질문을 제기한다:\n' +
      '\n' +
      '_Can 시퀀스 모델링 툴은 긴 지평선 예측의 오차를 효과적으로 감소시키고, 시퀀스 모델링 툴의 성능을 향상시킨다.\n' +
      '\n' +
      '그림 1.1: 확산 세계 모델과 1단계 역학 모델을 사용하여 훈련된 TD3+BC의 반환.\n' +
      '\n' +
      '### MBRL algorithms?\n' +
      '\n' +
      '우리는 긴 지평선 결과를 예측하기 위해 설계된 확산 확률 모델인 _Diffusion World Model_ (DWM)을 소개한다. 현재 상태, 액션 및 예상 수익에 대한 조건화, 확산 세계 모델은 다단계 미래 상태와 보상을 동시에 예측하여 윌 모델의 재귀적 쿼리가 없기 때문에 오류 축적의 소스를 제거한다. 그림 1.1과 같이 확산 세계 모델은 시뮬레이션 지평선\\(31\\)에도 성능이 저하되지 않는 긴 지평선 시뮬레이션에 강인하다. 자세한 실험은 섹션 4를 참조하십시오.\n' +
      '\n' +
      '본 논문에서는 온라인 상호 작용 없이 정적 데이터 세트로부터 정책을 학습하는 것을 목적으로 하는 오프라인 RL 설정을 특히 고려한다. 온라인 교육에서 벗어나는 것은 탐구의 부작용을 우회하고 세계 모델의 품질을 철저히 조사할 수 있게 한다. 본 논문에서는 일반적인 Dyna-type (Sutton, 1991) 모델 기반 프레임워크를 제안한다. 간단히 말해서, 먼저 오프라인 데이터셋을 이용하여 확산 세계 모델을 학습한 후, 확산 세계 모델에 의해 생성된 상상된 데이터를 이용하여 행위자-비판적 방식으로 정책을 학습한다. 특히, 비평가 훈련을 위한 목표값을 생성하기 위해, 선택된 지평선까지의 회귀를 시뮬레이션하기 위해 미래 궤적을 생성하는 확산 세계 모델을 사용하는 _Diffusion Model Value Expansion(Diffusion-MVE)_를 소개한다. 나중에 자세히 설명하겠지만 _Diffusion-MVE는 생성 모델링을 통해 오프라인 RL에 대한 값 정규화로 해석될 수 있으며, 대안적으로 합성 데이터로 오프라인 Q-러닝을 수행하는 방법으로 해석될 수 있다._\n' +
      '\n' +
      '우리의 프레임워크는 MF actor-critic RL 선택 방법을 유연하게 수행할 수 있으며, 세계 모델이 액션 생성에 개입하지 않기 때문에 추론 시간에 출력 정책이 효율적이다.\n' +
      '\n' +
      '우리는 D4RL 벤치마크로부터 9개의 이동 작업에 대한 확산 기반 및 전통적인 1단계 세계 모델을 벤치마킹한다(Fu et al., 2020). 우리는 확산 모델이 트랜스포머 아키텍처로 대체되는 접근법의 변형을 추가로 고려한다(Vaswani et al., 2017). 이 모든 작업은 지속적인 작업과 관찰 공간에 있습니다. 본 연구의 결과는 두 시퀀스 수준 세계 모델이 1단계 모델보다 성능이 우수하다는 것을 확인하며, 확산 세계 모델은 1단계 모델보다 \\(44\\%\\)의 성능 향상을 달성한다. 더욱이, 최근 오프라인 RL 방법의 발전은 MF 알고리즘(Kumar et al., 2020; Kostrikov et al., 2021; Wang et al., 2022; Garg et al., 2023; Ding and Jin, 2023)에 집중되었고, 여기서 몇몇 작업은 MB와 MF 방법 사이의 경계를 흐리게 했다(Chen et al., 2021; Janner et al., 2021, 2022; Ajay et al., 2022; Zheng et al., 2023). 본 논문에서 제안하는 방법은 MB 알고리즘과 MF 알고리즘 간의 차이를 제거함으로써, SOTA(state-of-the-art) 성능을 달성한다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '우리는 \\((\\mathcal{S},\\mathcal{A},R,P,p_{0},\\gamma)\\), \\(\\mathcal{S}\\)은 상태공간, \\(\\mathcal{A}\\)은 행동공간으로 정의되는 무한지평선 마르코프 결정과정(MDP)을 고려한다. \\(\\Delta(\\mathcal{S})\\)를 상태공간의 확률심플렉스라 하자. \\\\ (R:\\mathcal{S}\\times\\mathcal{A}\\mapsto\\mathbb{R}\\)는 결정론적 보상함수이고, \\(P:\\mathcal{S}\\times\\mathcal{A}\\mapsto\\Delta(\\mathcal{S})\\)는 전이의 확률분포를 정의하고, \\(p_{0}:\\mathcal{S}\\mapsto\\Delta(\\mathcal{S})\\)는 초기상태 \\(s_{0}\\)의 분포를 정의하며, \\(\\gamma\\in(0,1)\\)은 할인함수이다. RL의 과제는 수익률\\(J(\\pi)=\\mathbb{E}_{s_{0}\\sim p_{0}(s),a_{t}\\sim\\pi(:|s_{t}),s_{t+1}\\sim P(:|s_{ t},a_{t})}\\)[\\(\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})]을 최대화하는 정책\\(\\pi:\\mathcal{S}\\mapsto\\mathcal{A}\\)을 학습하는 것이다. 궤적\\(\\tau=\\{s_{0},a_{0},r_{0},\\ldots,s_{|\\tau|},a_{|\\tau|},a_{|\\tau|},r_{|\\tau|}\\})이 주어지면, 여기서 \\(|\\tau|\\)은 전체 타임스텝 수이고, timestep \\(t\\)에서의 복귀-to-go (RTG)는 \\(g_{t}=\\sum_{t^{\\prime}=t^{|\\tau|}\\gamma^{\\prime}^{t^{-t}}^{t^{^{t^{prime}}}^{t^{^{t^{prime}}\\)이다. 오프라인 RL에서는 알려지지 않은 특정 정책에 의해 생성된 정적 데이터 세트에서만 정책을 학습하도록 제한됩니다. 본 논문에서는 오프라인 데이터 분포를 나타내기 위해 \\(\\mathcal{D}_{\\text{offline}\\)을 사용하고, 오프라인 데이터 집합을 나타내기 위해 \\(D_{\\text{offline}\\)을 사용한다.\n' +
      '\n' +
      '**Diffusion Model.** Diffusion Probabilistic Model(Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020)은 반복적 잡음 제거 과정에 의해 잡음으로부터 샘플을 생성하는 생성 모델이다. 데이터 포인트 \\(x^{(0)}\\)에서 시작하여 \\(x^{(k)}\\)에 가우시안 노이즈를 반복적으로 추가하는 _forward_ 또는 _diffusion process_라고 하는 고정된 마르코프 체인을 정의한다:\n' +
      '\n' +
      '(k+1)}|x^{(k)})=\\mathcal{N}\\left(\\sqrt{1-\\beta_{k}}x^{(k)},\\beta_{k}\\mathbf{I}\\right),\\;0\\leq k\\leq K-1.\\tag{1}\\t.\n' +
      '\n' +
      '확산 단계의 수가 \\(K\\rightarrow\\infty\\)일 때, \\(x^{(K)}\\)는 본질적으로 랜덤 노이즈가 된다. 랜덤 노이즈를 데이터 포인트로 변환하는 해당 _reverse process_를 학습한다:\n' +
      '\n' +
      '\\[p_{\\theta}(x^{(k-1)}|x^{(k)})=\\mathcal{N}\\left(\\mu_{\\theta}(x^{(k)}),\\Sigma_{\\theta}(x^{(k)})\\right),\\;1\\leq k\\leq K.\\tag{2}\\heq\n' +
      '\n' +
      '확산 모델로부터 샘플링은 먼저 랜덤 잡음 \\(x^{(K)}\\sim\\mathcal{N}(0,\\mathbf{I})\\)을 샘플링한 후 역과정을 수행한다. 역과정을 학습하기 위해 한계우도\\(p_{\\theta}(x^{(0):(K)})\\)의 변분 하한을 최적화한다. 하한을 최적화하는 데에는 여러 가지 등가의 방법이 있으며(Kingma et al., 2021), 우리는 다음과 같이 잡음 예측 경로를 취한다. (x^{(k)}=\\sqrt{\\bar{\\alpha}_{k}x^{(0)}+\\sqrt{1-\\bar{\\alpha}_{k}\\varepsilon\\(\\bar{\\alpha}_{k}}\\varepsilon\\(\\bar{\\alpha}_{k}=\\prod_{k^{\\prime}=1}^{K}(1-\\beta_{k^{\\prime}})\\) 및 \\(\\varepsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\)는 \\(x^{(k)}\\)에 대해 주입된 잡음이다. 그리고 \\(\\varepsilon_{\\theta}(x^{(k)},k)\\)에 주입되는 \\(\\varepsilon\\)을 예측하기 위해 신경망 \\(\\varepsilon_{\\theta}(x^{(k)},k)\\)을 매개변수화한다. 또한, 조건변수 \\(y\\)는 각각 \\(q(x^{(k+1)}|x^{(k)},y)\\)와 \\(p_{\\theta}(x^{(k-1)}|x^{(k)},y)\\)을 수식화함으로써 두 공정에 쉽게 추가될 수 있다. 우리는 조건부 정보를 촉진하기 위해 분류기 없는 안내(Ho and Salimans, 2022)를 추가로 배포하며, 이는 본질적으로 조건부 및 조건부 잡음 예측기 모두를 학습한다. 보다 정확하게는 다음과 같은 손실 함수를 최적화한다:\n' +
      '\n' +
      'b{E}_{(x^{(0)},y),k,\\varepsilon,b}\\big{\\|}\\varepsilon_{\\theta}\\Big{(}x^{(k)}(x^{(0)},\\varepsilon),k,(1-b)\\cdot y+b\\cdot\\varnothing\\Big{}-\\varepsilon\\big{\\|}_{2}^{2}, \\tag{3}\\big{\\|}\\barepsilon_{\\theta}\\Big{(}x^{(k)}(x^{(0)},\\varepsilon),k,(1-b)\\cdot y+b\\cdot\\varnothing\\Big{}-\\varepsilon\\big{\\|}_{2}, \\tag{3}\\big{\\|}\\barepsilon_{\\theta}\\Big{(x^{(0)},\\varepsilon,k,\n' +
      '\n' +
      '여기서 \\(x^{(0)}\\)과 \\(y\\)은 실제 데이터 점 및 데이터 분포로부터 샘플링된 조건부 정보이고, \\(\\varepsilon\\sim\\mathcal{N}(0,\\mathbf{I})\\)은 주입된 잡음이고, \\(k\\)은 \\(1\\)과 \\(K\\) 사이에 균일하게 샘플링된 확산 단계이며, \\(b\\sim\\text{Bernoulli}(p_{\\text{uncond})\\)는 우리가 null 조건을 사용할지 여부를 나타내기 위해 사용되며, 마지막으로 \\(x^{(k)}=\\sqrt{\\bar{\\alpha}_{k}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{k}\\varepsilon\\(k))은 null 조건을 사용할지 여부를 나타내기 위해 사용된다. 알고리즘 A.1은 유도 확산 모델에서 샘플링하는 방법을 자세히 설명한다. 섹션 3에서는 오프라인 RL의 맥락에서 \\(x^{(0)}\\) 및 \\(y\\)의 형태를 소개하고, 계획을 용이하게 하기 위해 확산 모델을 활용하는 방법에 대해 논의한다.\n' +
      '\n' +
      '## 3 확산 세계 모델\n' +
      '\n' +
      '이 절에서는 확산 세계 모델을 가진 모델 기반 오프라인 RL을 위한 일반적인 레시피를 소개한다. 우리의 프레임워크는 각각 섹션 3.1 및 섹션 3.2에서 자세히 설명하는 두 가지 훈련 단계로 구성된다. 첫 번째 단계에서는 확산 모델을 학습하여 미래 상태 및 보상, 현재 상태에 대한 조건화, 행동 및 목표 반환의 순서를 예측한다. 다음으로, 행위자-비판 방법을 사용하여 오프라인 정책을 훈련하고, 모델 기반 가치 추정에 사전 훈련된 확산 모델을 활용한다. 알고리즘 3.1은 이 프레임워크를 결정론적 오프라인 정책을 가정하는 지연된 업데이트가 있는 간단한 행위자 비판 알고리즘으로 제시한다. 우리의 프레임워크는 다양한 방식으로 쉽게 확장될 수 있다. 첫째, 확률적 정책을 설명하기 위해 일반화할 수 있습니다. 더욱이, 우리가 제시하는 행위자 비평 알고리즘은 가장 단순한 형태이다. 기존의 다양한 오프라인 학습 알고리즘과 결합할 수 있도록 확장할 수 있다. 4절에서는 TD3+BC(Fujimoto and Gu, 2021), IQL(Kostrikov et al., 2021), Q-learning with 비관적 보상(Yu et al., 2020)을 각각 내장하는 알고리즘 3.1의 세 가지 인스턴스화에 대해 논의한다.\n' +
      '\n' +
      '### 조건부 확산 모델\n' +
      '\n' +
      '길이-\\(T\\) 부분궤적에 대한 회귀-조건 확산 모델 \\(p_{\\theta}\\)을 훈련한다. 여기서 조건 변수는 부분궤적의 RTG이다. 즉, \\(y=g_{t}\\) 및 \\(x^{(0)}=(s_{t},a_{t},r_{t},s_{t+1},r_{t+1},\\ldots,s_{t+T-1},r_{t+T-1})\\이다. 섹션 2에 소개된 바와 같이, 우리는 RTG의 역할을 촉진하기 위해 분류기 없는 지침을 사용한다. 알고리즘 3.1의 1단계는 훈련 절차를 상세히 설명한다. 2단계 파이프라인에서 학습된 확산 모델의 실제 사용을 위해 목표 RTG(g_{\\text{eval}}\\)와 현재 상태\\(s_{t}\\) 및 액션\\(a_{t}\\)을 기반으로 미래(T-1\\) 상태 및 보상을 예측한다. 이러한 예측 상태 및 보상은 정책 훈련에서 가치 추정을 용이하게 하기 위해 사용되며, 섹션 3.2를 참조한다. 향후 조치가 필요하지 않기 때문에 우리는 세계 모델에서 이를 모델링하지 않는다.\n' +
      '\n' +
      '(s_{t}\\) 및 (a_{t}\\)의 컨디셔닝을 가능하게 하기 위해, 우리는 표준 샘플링 절차(알고리즘 A.1)를 약간 조정하며, 여기서 역 프로세스에서 모든 잡음 제거 단계에 대해 \\(s_{t}\\) 및 \\(a_{t}\\)을 수정한다. 알고리즘 A.2를 참조한다.\n' +
      '\n' +
      '```\n' +
      '// 제1단계: 월드 모델 훈련\n' +
      '1Hyperparameters: 확산 단계 수\\(K\\), null conditioning probability\\(p_{\\text{uncond}}\\), noise parameters\\(\\bar{\\alpha}_{k}\\)\n' +
      '2whilenot convergeddo\n' +
      '3 \\(D_{\\text{offline}}\\)에서 샘플 길이 -\\(T\\) 부궤적 \\(x^{(0)}=(s_{t},a_{t},r_{t},s_{t+1},r_{t+1},\\ldots,s_{t+T-1},r_{t+T-1})\\)\n' +
      '4 계산 RTG\\(g_{t}\\leftarrow\\sum_{h=0}^{T-1}\\gamma^{h}r_{t+h}\\)//최적화\\(\\theta\\)via 식 (3) Sample \\(\\varepsilon\\sim\\mathcal{N}(0,I)\\) 및 \\(k\\in[K]\\)을 균일하게\n' +
      '5 Compute\\(x^{(k)}\\leftarrow\\sqrt{\\alpha_{k}}x^{(0)}+\\sqrt{1-\\bar{\\alpha_{k}}\\)\\(y\\leftarrow\\varnothing\\ with probability \\(p_{\\text{uncond}}\\), 그렇지 않으면 \\(y\\gets g_{t}\\)\n' +
      '6 \\(\\nabla_{\\theta}\\left\\lVert\\varepsilon_{\\theta}(x^{(k)},k,y)-\\varepsilon\\right\\rVert_{2}^{2}\\)에 기울기 단계를 취한다\n' +
      '7//단계 2 : 오프라인 정책 교육\n' +
      '8Hyperparameters: rollout length\\(H\\), conditioning RTG\\(g_{\\text{eval}}\\), guidance parameter\\(\\omega\\), target network update frequency\\(n\\)\n' +
      '9 배우와 비평가 네트워크를 초기화한다 \\(\\pi_{\\psi}\\), \\(Q_{\\phi}\\)\n' +
      '10 수렴될 때까지 목표 네트워크의 가중치를 \\(\\bar{\\psi}\\leftarrow\\psi\\), \\(\\bar{\\phi}\\leftarrow\\phi\\)for\\(i=1,2,\\ldots\\)til convergencedo\n' +
      '(D_{\\text{offline}})//확산 모델값 확장 샘플\\(\\hat{\\tau}}\\), \\(\\hat{\\tau}_{i+1}\\), \\(\\ldots\\), \\(\\hat{\\tau}_{i+T-1}\\), \\(\\hat{\\tau}_{i+T-1}\\), \\(\\hat{\\tau}_{i+T-1}\\), \\(\\hat{\\tau}_{i+T-1}\\)\\(\\sim\\)\\(p_{\\theta}(\\cdot|s_{t},a_{t},g_{\\text{eval}}))\\(\\omega\\)\n' +
      '12: 목표 \\(Q\\)값 \\(y=\\sum_{h=0}^{H-1}\\gamma^{h}\\hat{\\tau}_{t+h}+\\gamma^{H}Q_{\\bar{\\phi}(\\hat{\\tau}_{i+H},\\pi_{\\bar{\\psi}(\\hat{\\tau}_{i+H}))을 계산/비평가 \\(y=\\sum_{h=0}^{H-1}\\gamma^{h}\\hat{\\tau}_{t+h}+\\gamma^{H}Q_{\\bar{\\phi}}(\\hat{\\tau}_{i+H},\\pi_{\\bar{\\psi}}(\\hat{\\tau}_{i+H}))//비평가 \\(y=\\sum_{h=0}^{H-1}\\gamma^{h}\\hat{\\tau}_{i+H})\n' +
      '13\\(\\phi\\leftarrow\\phi-\\eta\\nabla_{\\psi}\\left\\lVert Q_{\\phi}(s_{t},a_{t})-y\\right\\rVert_{2}^{2}\\)//배우 업데이트\n' +
      '14 actor network update: \\(\\psi\\leftarrow\\psi+\\eta\\nabla_{\\psi}Q_{\\phi}(s_{t},\\pi_{\\psi}(s_{t}))// target network ifi mod\\(n\\)then\n' +
      '15\\(\\bar{\\phi}\\leftarrow\\bar{\\phi}+w(\\phi-\\bar{\\phi})\\)\\(\\bar{\\psi}\\leftarrow\\bar{\\psi}+w(\\psi-\\bar{\\psi})\\)\n' +
      '```\n' +
      '\n' +
      '확산 월드 모델을 이용한 오프라인 모델 기반 RL을 위한 알고리즘 3.1**A 일반 액터-크리틱 프레임워크\n' +
      '\n' +
      '확산 월드 모델을 이용한### 모델 기반 RL\n' +
      '\n' +
      '섹션 2는 세계 모델의 사용 범위를 소개한다. 우리는 결과 정책이 모델이 없으므로 추론 시간에 빠르게 작동할 수 있기 때문에 _data augmentation_ 전략에 특히 관심이 있다. 본 논문에서는 확산 모델에 의해 생성된 합성 데이터에 대해 비평가가 훈련되는 행위자-비평가 알고리즘을 제안한다. 간단히 말해서, 우리는 시간차(TD) 학습을 통해 학습된 프록시 \\(Q\\) 함수로 추정된 DWM으로 시뮬레이션된 단기 수익률과 장기 수익률의 합으로 \\(Q\\)-값을 추정한다.\n' +
      '\n' +
      '**Definition 3.1**(\\(H\\)**-step Diffusion Model Value Expansion)**.: \\((s_{t},a_{t})\\)을 state-action pair로 한다. 확산 모델로부터 샘플\\(\\hat{\\tau}_{t},\\hat{s}_{t+1},\\hat{\\tau}_{t+1},\\ldots,\\hat{s}_{t+T-1},\\hat{\\tau}_{t+T-1})\\(p_{\\theta}(\\cdot|s_{t},a_{t},g_{text{eval}})\\). \\(H\\)을 시뮬레이션 지평선이라고 하자. 여기서 \\(H<T\\) \\(H\\)-step _diffusion model value expansion_ estimate of \\((s_{t},a_{t})\\)에 의해 주어진다.\n' +
      '\n' +
      '\\hat{Q}_{\\text{diff}}^{H}(s_{t},a_{t})=\\sum_{h=0}^{H-1}\\gamma^{h}\\hat{\\tau}_{t+h}+\\gamma^{H}\\widehat{Q}(\\hat{s}_{t+H},\\hat{a}_{t+H}), \\tag{4}\\gamma^{h}\\hat{Q}(\\hat{s}_{t+H},\\hat{a}_{t+H})\n' +
      '\n' +
      '여기서 \\(\\widehat{a}_{t+H}=\\pi(\\widehat{s}_{t+H})\\) 및 \\(\\widehat{Q}\\)은 프록시 값 함수이다.\n' +
      '\n' +
      'TD 학습에서 목표값을 계산하기 위해 이 확장법을 사용하며 알고리즘 3.1을 참조한다. 이 메커니즘은 알고리즘의 성공의 핵심이며 몇 가지 매력적인 특성을 가지고 있다.\n' +
      '\n' +
      '1. 표준 모델 기반 값 확장(MVE, Feinberg et al. (2018))을 전개함에 있어서, 오차 누적의 근본 원인인 1단계 동역학 모델 \\(f_{\\theta}(s_{t+1},r_{t}|s_{t},a_{t})\\을 재귀적으로 질의하여 상상 궤적을 도출한다. MVE에 비해 우리의 DWM은 전체적으로 상상된 궤적(액션 없이)을 생성한다.\n' +
      '2. 흥미롭게도 MVE는 질의할 때 정책 예측 행동\\(\\widehat{a}_{t}=\\pi(\\widehat{s}_{t})\\)를 사용한다. 이는 모의 환경에서 \\(\\pi\\)의 온 정책 가치 추정으로 볼 수 있다. 반면에 Diffusion-MVE는 샘플링 과정에 영향을 미치지 않기 때문에 오프 정책 방식으로 동작한다. 섹션 4에서 탐색할 바와 같이 오프 정책 확산-MVE는 오프라인 RL에서 탁월하여 1단계-MVE의 성능을 크게 능가한다. 우리는 이제 각각 독특한 관점에서 이것에 대한 두 가지 해석을 파헤칠 것이다\n' +
      '\n' +
      '**(a)**: 우리의 접근법은 정책 평가(라인 13-16)와 정책 개선(라인 17) 단계를 번갈아 가면서, 정책 반복 알고리즘으로 볼 수 있다. 여기서 \\(\\widehat{Q}\\)은 정책값 함수 \\(Q^{\\pi}\\)의 추정치이다. 오프라인 RL의 맥락에서 TD 학습은 종종 \\(Q^{\\pi}\\)(Thrun and Schwartz, 2014; Kumar et al., 2020)의 과대평가로 이어진다. 이것은 \\(\\pi\\)이 분배외 행동을 일으켜 \\(\\widehat{Q}\\)에 대한 잘못된 값을 초래할 수 있기 때문에 \\(\\widehat{Q}\\)을 최대화하기 위해 정책을 정의하기 때문이다. 이러한 과대 평가는 온라인에 배포될 때 결과 정책의 일반화 능력에 부정적인 영향을 미친다. 이를 완화하기 위해, 광범위한 오프라인 RL 방법들은 값 함수에 다양한 형태의 정규화를 적용한다(Kumar et al., 2020; Kostrikov et al., 2021; Garg et al., 2023). DWM이 오프라인 데이터에 대해 배타적으로 학습됨에 따라 오프라인 데이터셋을 생성하는 행위 정책의 합성으로 볼 수 있다. 즉, 확산-MVE는 생성 모델링_을 통해 오프라인 RL에 대한 _value 정규화의 유형을 도입한다.\n' +
      '\n' +
      '더욱이, 우리의 접근법은 기존의 가치 비관론과는 상당히 다르다. 오프라인 RL의 한 가지 과제는 오프라인 데이터 세트를 생성하는 행동 정책이 종종 낮은-중간 품질의 것이므로 결과 데이터 세트는 낮은-중간 수익을 갖는 궤적만을 포함할 수 있다는 것이다. 결과적으로, 오프라인 RL을 위해 도입된 많은 정규화 기법들은 종종 _overly pessimistic_(Ghasemipour et al., 2022; Nakamoto et al., 2023)이다. 이 문제를 해결하기 위해 일반적으로 DWM에서 샘플링할 때 \\(g_{\\text{eval}}\\)의 큰 OOD 값을 조건으로 한다. 다르게 말하면, 우리는 DWM에 _optimistic goal_ 하에서 상상된 궤적을 출력하도록 요청한다.\n' +
      '\n' +
      '(b)**: 또는 오프라인 Q-러닝 알고리즘(Watkins and Dayan, 1992)으로도 볼 수 있는데, 여기서 \\(\\widehat{Q}\\)은 오프 정책 데이터를 사용하여 최적 값 함수 \\(Q^{\\text{s}\\)을 추정하고 있다. 다시, 오프 정책 데이터는 OOD RTG 값을 조정하는 확산 모델에 의해 생성된다. 본질적으로, 우리의 접근법은 합성 데이터_에서 _offline\\(Q\\)-학습으로 특징지어질 수 있다.\n' +
      '\n' +
      '**Transformer-based World Models과의 비교.** Curious reader는 DMW와 기존의 Transformer-based World Models(Chen et al., 2022; Micheli et al., 2022; Robine et al., 2023) 사이의 주요 차이에 대해 궁금해할 수 있다. 이 모델들은 현재 상태\\(s_{t}\\)와 행동\\(a_{t}\\)이 주어지면 트랜스포머의 자기회귀구조를 이용하여 과거 정보를 통합하여 예측한다. 미래에 여러 단계를 예측하려면 반복 예측을 수행해야 합니다. 대조적으로, DWM은 단일 쿼리에서 긴 수평선 예측을 한다. 우리의 작업에서 확산 모델을 트랜스포머로 대체하는 것이 전적으로 가능하며 섹션 4.2에서 설계 선택을 정당화하는 것은 주목할 가치가 있다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리의 실험은 두 가지 목적을 가진 설계이다. (1) 먼저, MBRL에 대한 컴파운딩 에러를 감소시키는데 있어서 DWM의 유효성을 조사하고자 한다. (2) 두 번째로, 제안된 알고리즘 3.1의 성능을 평가하고, 1단계 동역학 모델과 다른 SOTA MF 접근법을 사용하여 상대와 비교하고자 한다.\n' +
      '\n' +
      '이를 위해 TD3+BC(Fujimoto and Gu, 2021), IQL(Kostrikov et al., 2021), Q-learning with 비관적 보상(PQL)을 프레임워크에 통합하는 알고리즘 3.1의 3가지 인스턴스화를 2단계 오프라인 RL 알고리즘으로 고려한다. 이러한 알고리즘은 각각 액션(TD3+BC), 가치 함수(IQL) 및 보상(PQL)에 정의된 서로 다른 보수적 개념을 가지고 있다. 구체적으로, PQL 알고리즘은 MOPO 알고리즘 Yu et al.(2020)에서 영감을 얻었으며, 여기서 우리는 그 예측의 불확실성에 의해 예측된 세계 모델에 보상을 벌한다. 그럼에도 불구하고, 그것은 비평가 학습에서 MOPO와 구별된다. MOPO는 모델 생성 전이에 대해 표준 TD 학습을 사용하는 반면, 값 추정에는 MVE 또는 Diff-MVE를 사용한다. 후속편에서 우리는 알고리즘을 각각 DWM-TD3BC, DWM-IQL 및 DWM-PQL이라고 한다. DWM-IQL의 경우, \\(\\lambda\\)-return 기법(Schulman et al., 2015)을 기반으로 Diff-MVE의 변형을 사용하여 성능 향상을 관찰하였으므로, 이를 기본 특징으로 통합한다. 이러한 알고리즘에 대한 자세한 설명은 부록 C로 미루어진다.\n' +
      '\n' +
      '**기준.** 우리는 DWM을 1단계 역학 모델로 대체하고 O-TD3BC, O-IQL 및 O-PQL로 명명된 표준 MVE를 각각 사용하는 알고리즘의 3가지 변형을 고려한다. MF 기준선의 경우, TD3+BC, IQL, Decision Diffuser(DD, Ajay et al. (2022))의 SOTA 성능을 갖는 3가지 인기 있는 접근 방식을 고려한다.\n' +
      '\n' +
      '**Benchmark.** D4RL(Fu et al., 2020) 벤치마크로부터 9개의 운동 태스크 데이터셋에 대한 실험을 수행하고, 획득된 정규화된 리턴(0-1 with 1 with expert performance)을 보고한다. 랜덤 시드가 다른 5개의 인스턴스에 대해 각 알고리즘을 훈련하고 10개의 에피소드에 대해 평가한다. 보고된 모든 값은 5개의 무작위 종자에 걸쳐 집계된 평균 및 표준 편차이다.\n' +
      '\n' +
      '**Hyperparameters.** DWM의 시퀀스 길이를 \\(T=8\\)으로 설정한다(Sec. 4.1에서 논의). 확산 단계의 수는 훈련을 위해 \\(K=5\\)이다. DWM 추론을 위해 4.2절에 자세히 설명된 것처럼 확산 단계 수 \\(N=3\\)가 감소된 가속 추론 기법이 적용된다. DWM의 훈련 및 샘플링 세부사항은 부록 A를 참조하고, 각 오프라인 알고리즘의 훈련 세부사항은 부록 D를 참조한다.\n' +
      '\n' +
      '### Offline RL\n' +
      '\n' +
      '주요 결과**MB 알고리즘의 경우 시뮬레이션 지평선 \\(H\\in\\{1,3,5,7\\}\\)과 평가 RTG 값 세트를 스윕한다. 우리가 검색하는 RTG 값은 환경에 따라 다르며 표 D.2에 명시한다. 우리는 표 4.1의 각 알고리즘에 대해 최상의 결과를 보고한다. 우리가 발견한 주요 경향은 다음과 같다.\n' +
      '\n' +
      '제안된 DWM은 1단계 대비 우수한 성능을 보였으며, 현저한 \\(44\\%\\)의 성능 향상을 보였다. 흥미롭게도 MB 설정에서 적용할 경우, TD3+BC와 O-TD3BC, IQL 및 O-IQL DWMs의 비교에서 볼 수 있듯이 1단계 역학 모델은 일반적으로 해당 MF 알고리즘의 성능을 저하시키지만 IQL 및 DWM-IQL의 비교에서 볼 수 있듯이 MB 설정에서 적용할 때 기존 MF 알고리즘을 능가하는 성능을 유지할 수 있다. 전반적으로 DWM 알고리즘은 SOTA MF 알고리즘과 동등한 성능을 달성한다._\n' +
      '\n' +
      '이는 확산 모델의 강한 표현성과 전체 시퀀스의 예측이 한 번에 이루어지기 때문이며, 이는 전통적인 1단계 역학 모델의 다단계 롤아웃에서 합성 오류 문제를 우회하기 때문이다. 그 점은 다음 단락으로 시뮬레이션 길이에 대한 연구에서 더 논의될 것이다.\n' +
      '\n' +
      'DWM을 사용한 Long Horizon Planning.** 긴 시뮬레이션 지평에 대한 다양한 세계 모델의 반응을 탐색하기 위해 정책 훈련에 사용되는 시뮬레이션 지평이 변경될 때 성능 DWM 방법(DWM-TD3BC 및 DWM-IQL)과 1단계 대응물(O-TD3BC 및 O-IQL)을 비교한다. DWM 모델의 한계를 탐색하기 위해, 더 긴 시퀀스 길이\\(T=32\\)의 다른 DWM 세트를 훈련하고, \\(H\\in\\{1,3,7,15,31\\}\\)에 대한 다운스트림 RL 알고리즘의 성능을 조사한다. 1단계 동역학 모델을 사용한 알고리즘은 시뮬레이션 지평이 1에서 5까지이며 그림 4.1은 9개의 작업에 대한 결과를 보여준다. O-IQL과 O-TD3BC는 시뮬레이션 지평선이 증가함에 따라 뚜렷한 성능 저하를 보인다. 대부분의 작업에서 성능은 한두 개와 같이 비교적 짧은 시뮬레이션 지평으로 최고조에 달한다. 이것은 1단계 역학 모델을 사용한 더 긴 모델 기반 롤아웃이 심각한 복합 오류로 고통받고 있음을 시사한다. 반대로 DWM-TD3BC와 DWM-IQL은 지평선 길이 31을 사용하더라도 큰 성능 저하 없이 비교적 높은 수익률을 유지한다.\n' +
      '\n' +
      '또한 시퀀스 길이 \\(T=8\\)와 \\(T=32\\)으로 훈련된 DWM과 알고리즘의 성능을 비교한다. 표 4.2는 9개의 작업(RTG 값 및 시뮬레이션 지평선 \\(H\\))에 걸쳐 평균 최상의 수익을 나타낸다. 비록 DWM이 롱-호라이즌 시뮬레이션에 강인하고, 어떤 경우에는 최적의 \\(H\\)이 \\(8\\)보다 크더라도, \\(T=8\\)을 초과하는 시퀀스 길이는 성능을 더 향상시키지 못한다. 따라서 본 논문에서는 실험을 위해 \\(T=8\\)을 선택한다. 자세한 결과는 표 E.1에 나와 있다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '이 섹션에서는 제안된 알고리즘에 대한 설계 선택을 DWM으로 논의하고 삭제한다.\n' +
      '\n' +
      '**트랜스포머 v.s. Diffusion Model.** 알고리즘 3.1은 트랜스포머(Vaswani et al., 2017)를 비롯한 다양한 형태의 시퀀스 모델을 수용할 수 있으며, 이 중 하나인 트랜스포머(Vaswani et al., 2017)는\n' +
      '\n' +
      '그림 4.1: 정책 훈련에 사용되는 시뮬레이션 지평이 다른 MBRL 방법의 수행. x축은 로그 스케일로 \\([1,31]\\)의 범위를 갖는다.\n' +
      '\n' +
      '가장 성공적인 시퀀스 모델입니다. 그러나 1단계 동역학 모델의 컴파운딩 오차 문제와 유사하게 트랜스포머는 자기회귀적 구조로 인해 내재된 오차 누적의 대상이 된다. 따라서 본 논문에서는 트랜스포머가 저성과를 보일 것이라는 가설을 세우고 확산 모델을 선택한다.\n' +
      '\n' +
      '이 가설을 검증하기 위해, 제안된 알고리즘에서 확산 모델을 트랜스포머로 교체하고, 결과적인 성능을 DWM 방법과 비교한다. 우리는 특히 TD3+BC 및 IQL과의 조합을 고려하며, 여기서 얻은 알고리즘을 T-TD3BC 및 T-IQL이라고 한다. T-TD3BC와 T-IQL을 DWM 방법과 동일한 시뮬레이션 수평선(H\\in\\{1,3,5,7\\}\\)에 걸쳐 매개변수 스위핑으로 테스트한다. 평가 RTG를 위해 Decision Transformer(Chen et al., 2021)에서 사용되는 값을 취하여 학습 데이터로 정규화한다. 나머지 실험 설정은 섹션 4.1과 동일한 구성을 따르며 표 4.3의 비교 결과를 통해 DWM이 오프라인 RL 알고리즘 인스턴스화 및 환경에서 트랜스포머보다 일관되게 우수하다는 것을 알 수 있다. 실험 세부사항은 부록 E.2를 참조한다.\n' +
      '\n' +
      '성능에서 T-IQL이 O-IQL과 일치하는 반면, T-TD3BC는 O-TD3BC의 성능을 초과한다는 점에 주목한다.\n' +
      '\n' +
      '**확산 단계 및 추론 단계 비율.** 훈련 확산 단계 \\(K\\)의 수는 모델링 품질에 큰 영향을 미칠 수 있으며, 여기서 \\(K\\)의 더 큰 값은 일반적으로 더 나은 성능으로 이어진다. 동시에, 확산 모델로부터의 샘플링은 \\(K\\) 내부 잡음 제거 단계를 포함하므로 느린 절차로 인식된다. 샘플링 과정을 가속화하기 위해 _stride sampling_ 기법(Nichol and Dhariwal, 2021)을 적용하였으며, 자세한 내용은 부록 A를 참조한다. 그러나 샘플링 속도는 품질에 따라 달라집니다. 추론 속도와 예측 정확도 사이의 균형을 맞추는 것이 중요하다. 우리는 모델 성능을 희생시키지 않고 샘플링을 상당히 가속화하기 위해 \\(K\\)와 \\(N\\)의 수를 선택하는 방법을 조사한다.\n' +
      '\n' +
      'DWM은 서로 다른 확산 단계(K\\in\\{5,10,20,30,50,100\\}\\)를 가지며, 여기서 시퀀스 길이는 \\(T=8\\)이다. 우리는 4개의 추론 단계 비율 \\(r_{\\text{infer}}\\in\\{0.2,0.3,0.5,1.0\\}\\)을 설정하고 보폭 샘플링에서 \\(N=\\lceil r_{\\text{infer}}\\cdot K\\rceil\\) 내부 단계를 사용한다. 그림 4.2는 식 (17)에 정의된 관찰 및 보상 시퀀스 모두에 대한 DMW의 예측 오차를 보고한다. 예측 오차는 평가 RTG에 의존하며, 여러 값에 걸쳐 가장 좋은 결과를 보고하며, 표 D.2를 참조한다. 중요한 관찰은 추론 단계가 다른 성능을 구별하는 데 중요한 융선이며, 여기서 \\(r_{\\text{infer}}=0.5\\)은 예측 정확도를 크게 손상시킨다. 또한, 영역\\(r_{\\text{infer}}\\geq 0.5\\) 내에서 작은 확산 단계\\(K=5\\)는 큰 값과 거의 동일한 값을 수행한다. 따라서 본 연구의 주요 실험으로 \\(K=5\\)과 \\(r_{\\text{infer}}=0.5\\)을 선택하여 샘플링 단계의 수를 \\(N=3\\)으로 유도한다.\n' +
      '\n' +
      '우리는 또한 더 긴 서열 길이\\(T=32\\)의 DWM에 대해 위의 실험을 반복했다. 결과는 또한 \\(r_{\\text{infer}}=0.5\\)을 지지하지만 \\(K=10\\)을 선호하며 부록 E.3을 참조한다.\n' +
      '\n' +
      '**OOD 평가 RTG 값.** 평가 RTG 값이 알고리즘의 성능을 결정하는 데 중요한 역할을 한다는 것을 발견했다. 궤적 전조에 대한 예비 실험은 분포 평가에서 RTG가 OOD RTG보다 성능이 낮음을 시사했다. 그림 4.3은 부록 E.4.2를 참조하십시오. 그림 4.3은 3가지 작업에 걸쳐 DWM-IQL 및 DWM-TD3BC의 반환을 보고하며, 다른 값은 다음과 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline Env. & **T-TD3BC** & \\multicolumn{3}{c}{DWM-IQL (w/o \\(\\lambda\\))} \\\\ T=8 & T=32 & T=8 & T=32 \\\\ \\hline \\hline\n' +
      '0.68 \\(\\pm\\) 0.10 & 0.60 \\(\\pm\\) 0.12 & 0.57 \\(\\pm\\) 0.09 & 0.61\\(\\pm\\) 0.10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4.2: 시퀀스 길이가 다른 DWM을 사용하여 9개의 작업에 걸친 DWM 알고리즘의 평균 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c|c c|c c} \\hline \\hline  & \\multicolumn{3}{c|}{**Model-Free**} & \\multicolumn{6}{c}{**Model-Based**} \\\\ Env. & TD3+BC & IQL & DD & O-TD3BC & O-IQL & O-POL & DWM-TD3BC & DWM-IQL & DWM-POL \\\\ \\hline hopper-m & 0.58 \\(\\pm\\) 0.11 & 0.48 \\(\\pm\\) 0.08 & 0.49 \\(\\pm\\) 0.07 & 0.39 \\(\\pm\\) 0.04 & 0.45 \\(\\pm\\) 0.05 & 0.63 \\(\\pm\\) 0.12 & **0.65 \\(\\pm\\) 0.10** & 0.54 \\(\\pm\\) 0.11 & 0.50 \\(\\pm\\) 0.09 \\\\ walker2d-m & 0.77 \\(\\pm\\) 0.09 & 0.75 \\(\\pm\\) 0.15 & 0.67 \\(\\pm\\) 0.16 & 0.39 \\(\\pm\\) 0.15 & 0.52 \\(\\pm\\) 0.24 & 0.74 \\(\\pm\\) 0.14 & 0.70 \\(\\pm\\) 0.15 & 0.76 \\(\\pm\\) 0.05 & **0.79 \\(\\pm\\) 0.08** \\\\ halfcheetah-m & 0.47 \\(\\pm\\) 0.01 & 0.46 \\(\\pm\\) 0.07 & **0.49 \\(\\pm\\) 0.01** & 0.44 \\(\\pm\\) 0.05 & 0.44 \\(\\pm\\) 0.03 & 0.45 \\(\\pm\\) 0.01 & 0.46 \\(\\pm\\) 0.01 & 0.44 \\(\\pm\\) 0.01 & 0.44 \\(\\pm\\) 0.01 \\\\ hopper-mr & 0.53 \\(\\pm\\) 0.19 & 0.25 \\(\\pm\\) 0.02 & **0.66 \\(\\pm\\) 0.15** & 0.26 \\(\\pm\\) 0.05 & 0.25 \\(\\pm\\) 0.03 & 0.32 \\(\\pm\\) 0.03 & 0.53 \\(\\pm\\) 0.09 & 0.61 \\(\\pm\\) 0.13 & 0.39 \\(\\pm\\) 0.03 \\\\ walker2d-mr & **0.75 \\(\\pm\\) 0.19** & 0.48 \\(\\pm\\) 0.23 & 0.44 \\(\\pm\\) 0.26 & 0.23 \\(\\pm\\) 0.13 & 0.24 \\(\\pm\\) 0.07 & 0.62 \\(\\pm\\) 0.22 & 0.46 \\(\\pm\\) 0.19 & 0.35 \\(\\pm\\) 0.14 & 0.35 \\(\\pm\\) 0.13 \\\\ halfcheetah-mr & **0.43 \\(\\pm\\) 0.01** & 0.44 \\(\\pm\\) 0.01 & 0.38 \\(\\pm\\) 0.06 & **0.43 \\(\\pm\\) 0.01** & 0.42 \\(\\pm\\) 0.02 & 0.42 \\(\\pm\\) 0.01 & **0.43 \\(\\pm\\) 0.01** & 0.41 \\(\\pm\\) 0.01 & **0.43 \\(\\pm\\) 0.01** \\\\ hopper-me & 0.90 \\(\\pm\\) 0.28 & 0.86 \\(\\pm\\) 0.22 & **1.06 \\(\\pm\\) 0.11** & 0.31 \\(\\pm\\) 0.18 & 0.39 \\(\\pm\\) 0.19 & 0.43 \\(\\pm\\) 0.18 & 1.03 \\(\\pm\\) 0.14 & 0.90 \\(\\pm\\) 0.25 & 0.80 \\(\\pm\\) 0.18 \\\\ walker2d-me & 1.08 \\(\\pm\\) 0.01 & 1.09 \\(\\pm\\) 0.00 & 0.99 \\(\\pm\\) 0.15 & 0.60 \\(\\pm\\) 0.25 & 0.57 \\(\\pm\\) 0.18 & 0.61 \\(\\pm\\) 0.22 & **1.10 \\(\\pm\\) 0.00** & 1.04 \\(\\pm\\) 0.10 & **1.10 \\(\\pm\\) 0.01** \\\\ halfcheetah-m & 0.73 \\(\\pm\\) 0.16 & 0.60 \\(\\pm\\) 0.23 & **0.91 \\(\\pm\\) 0.01** & 0.27 \\(\\pm\\) 0.12 & 0.61 \\(\\pm\\) 0.22 & 0.61 \\(\\pm\\) 0.22 & 0.75 \\(\\pm\\) 0.16 & 0.71 \\(\\pm\\) 0.14 & 0.69 \\(\\pm\\) 0.13 \\\\ \\hline \\multirow{2}{*}{Average} & **0.69** & 0.61 & **0.68** & 0.37 & 0.43 & 0.54 & **0.68** & 0.64 & 0.61 \\\\ \\cline{2-2}  & \\multicolumn{3}{c|}{0.660} & 0.447 & & & & 0.643 & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4.1: D4RL 데이터세트 상의 상이한 MF 및 MB 방법의 비교: 정규화된 리턴(mean \\(\\pm\\) std).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline  & **T-TD3BC** & \\multicolumn{3}{c}{**T-IQL**} & **DWM-TD3BC** & **DWM-IQL** \\\\ T=8 & T=32 & T=8 & T=32 \\\\ \\hline \\hline\n' +
      '0.68 \\(\\pm\\) 0.10 & 0.60 \\(\\pm\\) 0.12 & 0.57 \\(\\pm\\) 0.09 & 0.61 \\(\\pm\\) 0.10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4.3: DWM 및 트랜스포머 월드 모델을 사용하는 알고리즘 3.1의 상이한 인스턴스화의 성능.\n' +
      '\n' +
      '실험 결과 1, 3, 5, 7에 대해 평균화된 RTG 값은 작업마다 다르지만 모두 OOD이다. 부록 E.4.1은 각 작업에 대한 훈련 RTG의 분포를 보여준다. 결과는 실제 수익률이 지정된 \\(g_{\\text{eval}}\\)과 항상 일치하는 것은 아니라는 것을 보여준다. 이것은 Return-conditioned RL 방법(Emmons et al., 2021; Zheng et al., 2022; Nguyen et al., 2022)의 잘 알려진 문제이다. 그럼에도 불구하고 OOD 평가 RTG는 일반적으로 잘 수행된다. 그림 4.3은 DWM-TD3BC와 DWM-IQL이 모두 OOD 평가 RTG에 견고함을 보여준다. 보고된 반환은 표 4.1에 보고된 최고 성능이 더 높은 시뮬레이션 지평을 달리하는 훈련 사례에 대해 평균임을 강조한다. 우리의 직관은 확산 모델이 현재 상태에 대한 미래 수익을 낙관적으로 볼 수 있도록 장려하는 것이다. 한편, 평가 RTG는 지나치게 높을 수 없다. 작업 _halfcheetah-mr_에서 볼 수 있듯이 RTG\\(q_{\\text{eval}}>0.4\\)을 증가시키면 두 방법 모두 실제 성능이 더욱 감소한다. 최적 RTG 값은 작업마다 다르며, 완전한 실험 결과는 부록 E.4에 나와 있다.\n' +
      '\n' +
      '각주 1: 우리는 반환과 RTG가 다른 방식으로 정규화된다는 점에 주목한다: D4RL 벤치마크에 의해 계산된 반환은 하나의 SAC 정책의 성능에 의해 계산되지 않고 정규화되는 반면, 훈련에서 사용하는 RTG는 손으로 선택한 상수에 의해 할인되고 정규화된다.\n' +
      '\n' +
      '\\(\\lambda\\)**-Return Value Estimation.** Dreamer series of work(Hafner et al., 2019, 2020, 2023)는 \\(\\lambda\\)-return Technique(Schulman et al., 2015)을 적용하여 가치추정을 하였으며, 상상된 궤적을 사용하였다. 이 기법은 표준 Diff-MVE의 수정으로서 우리의 프레임워크에 매끄럽게 내장될 수 있다. 보다 정확하게는, 오프라인 데이터세트로부터 샘플링된 상태-행동 쌍 \\((s_{t},a_{t})\\)이 주어지면, \\(h=H,\\ldots,0\\)에 대한 \\(\\lambda\\)-목표값을 재귀적으로 계산한다:\n' +
      '\n' +
      '\\widehat{Q}_{s_{t+k}}^{\\widehat{\\lambda}_{t+1}+\\gamma\\begin{cases}(\\widehat{s}_{t+k+1},\\overline{\\varphi}(\\widehat{s}_{t+k+1}}(\\widehat{s}_{t+1}}),\\overline{Q}_{t+k+1}^{\\lambda}&\\text{if}h=H\\end{cases}{t+k+1}(\\widehat{s}_{t+B},\\overline{\\varphi}(\\widehat{s}_{t+B})}+\\lambda\\widehat{Q}_{t+k+1}^{\\lambda}&\\text{if}h=H\\end{cases}{t+k+1}(\\widehat{s}_{t+B},\\overline{\\v\n' +
      '\n' +
      'DWM을 이용하여 예측 상태\\(\\{\\widehat{s}_{t+1}\\}_{h=0}^{H}\\)와 보상\\(\\{\\widehat{\\tau}_{t}\\}_{h=0}^{H}\\)을 구하였다. TD 학습의 목표 값인 \\(Q\\(\\widehat{Q}_{\\epsilon}^{\\lambda}\\)을 알고리즘 3.1의 15선 수정으로 사용할 수 있다. 또한 IQL과 같이 상태 전용 값 함수를 학습하는 알고리즘의 경우 \\(Q_{\\bar{\\psi}}\\) 함수를 V\\(V_{\\bar{\\psi}}) 함수로 대체할 수 있다. 주목할 가치가 있는 식 (5)는 \\(\\lambda=1\\)일 때 바닐라 Diff-MVE로 감소한다.\n' +
      '\n' +
      'DWM-TD3BC와 DWM-IQL에 대한 vanilla diff-MVE와 \\(\\lambda\\)-return 변이체를 \\(\\lambda=0.95\\)을 이용하여 비교하는 실험을 수행하였다. 우리는 RTG 값(부록 표 D.2에 명시됨)과 시뮬레이션 지평 \\(1,3,5,7\\)을 검색한다. 결과는 표 4.4에 요약되어 있다. \\(\\lambda\\)-return 기법은 DWM-IQL에는 유익하지만 DWM-TD3BC에는 유해하다. 우리는 식 (5)가 \\(Q_{\\bar{\\psi}}\\) 또는 \\(V_{\\bar{\\psi}}\\) 함수를 반복적으로 불러오기 때문에 더 정확한 값 추정이 가능한 접근법을 선호한다고 추측한다. IQL은 값 함수를 정규화하는 반면 TD3+BC는 정책 정규화만 있으며 실험에서 값 과대 추정 문제에 더 취약한 것으로 나타났다. 이러한 결과를 바탕으로 우리는 DWM-IQL에 \\(\\lambda\\)-return 기법을 통합했지만 DWM-TD3BC는 바닐라 Diff-MVE를 사용하도록 했다. 우리는 알고리즘 단순화를 위해 DWM-PQL이 바닐라 Diff-MVE를 사용하도록 했다.\n' +
      '\n' +
      '추가 실험.우리는 또한 재표지된 RTG로 DWM을 미세 조정하는 효과를 조사한다. 이 기술은 유용성이 제한적이기 때문에 최종 설계에서 제외한다는 것을 발견했으며 부록 E.5의 자세한 결과를 참조하십시오.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      'Model-Based RLOne 인기 있는 MB 기법은 액션 검색이다. 세계 모델을 사용하여, 제안 분포들 또는 정책 사전들로부터 샘플링되는 후보 액션들의 결과들을 시뮬레이션한다(Nagabandi et al., 2018);\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c|}{**DWM-TD3BC**} & \\multicolumn{2}{c}{**DWM-IQL**} \\\\ Env. & w/o \\(\\lambda\\) & w/ \\(\\lambda\\) & w/o \\(\\lambda\\) & w/ \\(\\lambda\\) \\\\ \\hline hopper-m & 0.65 \\(\\pm\\) 0.10 & **0.68 \\(\\pm\\) 0.13** & 0.50 \\(\\pm\\) 0.08 & 0.54 \\(\\pm\\) 0.11 \\\\ walker2d-m & 0.70 \\(\\pm\\) 0.15 & 0.74 \\(\\pm\\) 0.08 & 0.62 \\(\\pm\\) 0.19 & **0.76 \\(\\pm\\) 0.05** \\\\ halfcheetah & **0.46 \\(\\pm\\) 0.01** & 0.40 \\(\\pm\\) 0.01 & **0.46 \\(\\pm\\) 0.01** & 0.44 \\(\\pm\\) 0.01 \\\\ hopper-m & 0.53 \\(\\pm\\) 0.09 & 0.50 \\(\\pm\\) 0.23 & 0.29 \\(\\pm\\) 0.04 & **0.61 \\(\\pm\\) 0.13** \\\\ walker2d-mr & **0.46 \\(\\pm\\) 0.19** & 0.23 \\(\\pm\\) 0.10 & 0.27 \\(\\pm\\) 0.09 & 0.35 \\(\\pm\\) 0.14 \\\\ halfcheetah-mr & **0.43 \\(\\pm\\) 0.01** & 0.39 \\(\\pm\\) 0.02 & **0.43 \\(\\pm\\) 0.01** & 0.41 \\(\\pm\\) 0.01 \\\\ hopper-m & 1.03 \\(\\pm\\) 0.14 & **1.05 \\(\\pm\\) 0.16** & 0.78 \\(\\pm\\) 0.24 & 0.90 \\(\\pm\\) 0.25 \\\\ walker2d-me & **1.10 \\(\\pm\\) 0.00** & **0.89 \\(\\pm\\) 0.13 & 1.08 \\(\\pm\\) 0.03 & 1.04 \\(\\pm\\) 0.10 \\\\ halfcheetah-me & **0.75 \\(\\pm\\) 0.16** & 0.71 \\(\\pm\\) 0.22 & 0.73 \\(\\pm\\) 0.14 & 0.74 \\(\\pm\\) 0.16 \\\\ \\hline Avg. & **0.68** & 0.62 & 0.57 & 0.64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4.4: 바닐라 Diff-MVE와 \\(\\lambda\\)-return 변이체를 이용한 DWM 방법의 성능 비교.\n' +
      '\n' +
      '그림 4.2: 추론 스텝 비율 \\(r_{\\text{ratio}}\\)이 변화함에 따라 확산 스텝 \\(K\\)으로 훈련된 DWM에 대한 평균 관찰 및 보상 예측 오차(9개의 작업 및 시뮬레이션 지평선 \\(H\\in[7]\\))이다.\n' +
      '\n' +
      'Williams et al., 2015), 및 최적의 것을 탐색한다. 이러한 접근 방식은 Atari and Go(Schrittwieser et al., 2020; Ye et al., 2021)와 같은 게임과 픽셀 관찰에 대한 지속적인 제어 문제에 성공적으로 적용되었다(Hafner et al., 2019). 또는 세계 모델과의 상호 작용을 통해 정책을 최적화할 수 있습니다. 이 아이디어는 원래 다이나 알고리즘(서튼, 1991)에서 나온 것이다. 이 체제에서 작업 간의 주요 차이점은 모델 생성 데이터의 사용에 있다. 예를 들어, Dyna-Q(Sutton, 1990) 및 MBPO(Janner et al., 2019)는 세계 모델 생성 천이에 의해 진정한 환경 데이터를 증강한 다음, 증강된 또는 생성된 데이터세트 중 어느 하나에 MF 알고리즘을 수행한다. 파인버그 외(2018)는 세계 모형 내에서 정책을 일정 지평까지 언롤링하여 가치 추정을 개선할 것을 제안하고 있다. Dreamer series of work Hafner et al.(2019, 2020, 2023)은 롤아웃 데이터를 가치 추정과 정책 학습 모두에 사용한다. 보다 최근에, Hansen et al. (2022, 2023); Chitnis et al. (2023)은 연속 제어 문제를 해결하기 위해 두 기술을 결합한다. 모든 MB 접근 방식을 검토할 수 없기 때문에, 우리는 독자들을 Wang 등(2019); Amos 등(2021)에게 더 포괄적인 검토와 벤치마크를 참조한다.\n' +
      '\n' +
      '앞서 언급한 대부분의 접근법은 간단한 1단계 세계 모델 \\(f(r_{t},s_{t+1}|s_{t},a_{t})\\에 의존한다. 드리머 시리즈 작업(Hafner et al., 2019, 2020, 2023)은 다음 상태를 예측하기 위한 과거 정보에 관여하기 위해 순환 신경망(RNN)을 사용한다. 최근, Robine et al. (2023); Micheli et al. (2022); Chen et al. (2022)는 독립적으로 RNN의 대체로서 Transformer-based World 모델을 제안했다. Janner et al.(2020)은 하나의 전진 패스로 긴-수평 롤아웃을 수행할 수 있는 미래 상태에 대한 점유 측정을 학습하기 위해 생성 모델을 사용한다.\n' +
      '\n' +
      '**오프라인 RL**: 오프라인 RL에 온라인 RL 방식을 직접 적용하는 것은 보통 저조한 성능으로 이어진다. 고장들은 전형적으로 외삽 오차에 기인한다(Fujimoto et al., 2019). 이 문제를 해결하기 위해 정책을 오프라인 데이터와 가깝게 유지하도록 장려하기 위해 많은 보수주의 개념이 도입되었다. 모델 프리 방법의 경우, 이러한 개념은 가치 함수(Kumar et al., 2020; Kostrikov et al., 2021; Garg et al., 2023) 또는 정책(Wu et al., 2019; Jaques et al., 2019; Kumar et al., 2019; Fujimoto and Gu, 2021)에 적용된다. 보수주의는 또한 수정된 MDP를 통해 MB 기술에 통합되었다. 예를 들어, MOPO(Yu et al., 2020)는 MBPO를 기반으로 하고 트랜지션을 생성할 때 예측된 보상을 재라벨링한다. 예측된 보상에서 세계 모델의 예측의 불확실성을 빼서 불확실성이 낮은 상태-행동 쌍을 부드럽게 촉진한다. 유사한 맥락에서, MOReL(Kidambi et al., 2020)은 단말 상태를 갖는 구성된 비관적 MDP를 사용하여 정책을 훈련시킨다. 에이전트는 세계 모델의 예측 불확실성이 높을 경우 단말 상태로 이동하게 되며, 패널티로서 부정적인 보상을 받게 된다.\n' +
      '\n' +
      '**RL에 대한 시퀀스 모델링**: 시퀀스 모델링 툴을 RL 문제에 적용하는 것에 대한 최근의 연구 관심이 급증하고 있다. Chen et al. (2021); Janner et al. (2021)은 먼저 오프라인 궤적을 자기회귀 시퀀스로 간주하고 트랜스포머 아키텍처(Vaswani et al., 2017)를 사용하여 모델링한다. 이것은 Meng et al.(2021); Lee et al.(2022)을 포함한 일련의 후속 연구에 영감을 주었다. 확산 모델(Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020), 흐름 매칭(Lipman et al., 2022) 및 일관성 모델(Song et al., 2023)도 다양한 RL 알고리즘에 통합되었으며, 예를 들어 Wang et al. (2022); Chi et al. (2023); Hansen-Estruch et al. (2023); Jia et al. (2023); Ding and Jin (2023); Du et al. (2023); Xu et al. (2023); Mishra and Chen (2023). 이 중 Diffuser(Janner et al., 2022)와 Decision Diffuser(DD)(Ajay et al., 2022)는 미래의 궤적도 예측하므로 우리의 작업과 가장 가깝다. 그러나 생성된 궤적의 사용은 크게 다르다. 디퓨저는 세계 모델과 정책 모두의 이중 역할을 합니다. 그것은 미래의 행동과 상태를 동시에 예측하고 예측된 후속 행동을 실행한다. 결정 확산기는 상태 및 동작의 예측을 분리한다. 확산 모델은 목표 수익에 대한 미래 상태 시퀀스 컨디셔닝을 단독으로 모델링하는 반면, 액션은 현재 상태와 예측된 다음 상태가 주어진 역 역학 모델에 의해 예측된다.\n' +
      '\n' +
      '##6 결론 및 향후 과제\n' +
      '\n' +
      '우리는 오프라인 RL의 맥락에서 확산 모델을 세계 모델로 활용하는 일반적인 프레임워크를 제시한다. 이 프레임워크는 온라인 교육을 수용하기 위해 쉽게 확장될 수 있다. 구체적으로, 모델 기반 값 추정을 위해 DWM 생성 궤적을 활용한다. 실험 결과는 이 방법이 MBRL의 컴파운딩 오류를 효과적으로 감소시킨다는 것을 보여준다. 우리는 각각 생성된 상상 궤적을 사용하여 3가지 다른 유형의 오프라인 RL 알고리즘을 훈련하여 전통적인 1단계 동역학 모델에 대해 DWM을 벤치마킹했다. DWM은 주목할 만한 성능 이득을 보여주고 가장 진보된 MF 접근법과 동등한 SOTA 성능을 달성한다. 하지만 저희 업무에도 한계가 있습니다. 현재, DWM은 각각의 개별 환경에 대해 트레이닝되고, 태스크-애널리시브이다. 향후 연구를 위한 흥미로운 방법은 DWM을 다중 환경 및 다중 작업 설정으로 확장하는 것이다. 또한, 탐사의 부작용을 피하기 위해 오프라인 RL 설정에서 DWM만 조사한다. 이것은 온라인 환경에서 DWM의 성능에 관한 흥미로운 질문을 제기한다. 마지막으로 가장 중요한 것은 추론을 가속화하기 위해 보폭 샘플링 기법을 채택했지만 DWM의 계산 요구량은 여전히 높다. 샘플링 프로세스를 가속화하기 위한 추가 향상은 더 큰 규모의 문제를 해결하기 위해 DWM의 향후 사용에 중요할 수 있다.\n' +
      '\n' +
      '## Impact Statement\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리의 작업에는 많은 잠재적인 사회적 결과가 있으며, 우리가 특별히 강조해야 한다고 느끼는 것은 없다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ajay et al. (2022) Ajay, A., Du, Y., Gupta, A., Tenenbaum, J., Jaakkola, T., and Agrawal, P. Is conditional generative modeling all you need to decision decision? _ ARXiv 프리프린트 arXiv:2211.15657_, 2022.\n' +
      '* Amos et al. (2021) Amos, B., Stanton, S., Yarats, D., and Wilson, A. G. On the model-based stochastic value gradient for continuous reinforcement learning. In _Learning for Dynamics and Control_, pp. 6-20. PMLR, 2021.\n' +
      '* Asadi et al. (2019) Asadi, K., Misra, D., Kim, S., and Littman, M. L. Combating the compounding-error problem with multi-step model. _ ArXiv preprint arXiv:1905.13320_, 2019.\n' +
      '* Chen et al. (2022) Chen, C., Wu, Y. - F., Yoon, J., and Ahn, S. 트랜스드리머: 트랜스포머 월드 모델을 사용한 강화 학습. _ arXiv preprint arXiv:2202.09481_, 2022.\n' +
      '* Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Sequence Modeling을 통한 강화학습. _30-Fifth Conference on Neural Information Processing Systems_, 2021. URL[https://openreview.net/forum?id=a7APmM4B9d](https://openreview.net/forum?id=a7APmM4B9d).\n' +
      '* Chi et al. (2023) Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B., and Song, S. 확산 정책: 행동 확산을 통한 Visuomotor 정책 학습. _ arXiv preprint arXiv:2303.04137_, 2023.\n' +
      '* Chitnis et al. (2023) Chitnis, R., Xu, Y., Hashemi, B., Lehnert, L., Dogan, U., Zhu, Z., and Delalleau, O. Iql-td-mpc: 계층적 모델 예측 제어를 위한 내재적 q-learning _ arXiv preprint arXiv:2306.00867_, 2023.\n' +
      '* Dean et al. (2020) Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S. 선형 이차 레귤레이터의 샘플 복잡도에 대해. _ Foundations of Computational Mathematics_, 20(4):633-679, 2020.\n' +
      '* Deisenroth et al. (2013) Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey on policy search for robotics. _ 2013년 Robotics_, 2(1-2):1-142의 기초 및 동향(r).\n' +
      '* Ding & Jin(2023) Ding, Z. and Jin, C. Consistency models as a rich and efficient policy class for reinforcement learning. _ arXiv preprint arXiv:2309.16984_, 2023.\n' +
      '* Du et al. (2023) Du, Y., Yang, M., Dai, B., Dai, H., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D., and Abbeel, P. Learning universal policies via text-guided video generation. _ arXiv preprint arXiv:2302.00111_, 2023.\n' +
      '* Emmons et al. (2021) Emmons, S., Eysenbach, B., Kostrikov, I., and Levine, S. Rvs: 지도 학습을 통한 오프라인 rl에 필수적인 것은? _ arXiv preprint arXiv:2112.10751_, 2021.\n' +
      '* Feinberg et al. (2018) Feinberg, V., Wan, A., Stoica, I., Jordan, M. I., Gonzalez, J. E., and Levine, S. 효율적인 모델 프리 강화 학습을 위한 모델 기반 가치 추정. _ arXiv preprint arXiv:1803.00101_, 2018.\n' +
      '* Fu et al. (2020) Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: 심층 데이터 기반 강화 학습을 위한 데이터셋; _ arXiv preprint arXiv:2004.07219_, 2020.\n' +
      '* Fujimoto & Gu (2021) Fujimoto, S. 및 구상 오프라인 강화 학습에 대한 미니멀리즘 접근 방식입니다. _30-Fifth Conference on Neural Information Processing Systems_, 2021. URL[https://openreview.net/forum?id=Q32U7dzWXpc](https://openreview.net/forum?id=Q32U7dzWXpc).\n' +
      '* Fujimoto et al. (2018) Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pp. 1587-1596. PMLR, 2018.\n' +
      '* Fujimoto et al. (2019) Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In _International Conference on Machine Learning_, pp. 2052-2062. PMLR, 2019.\n' +
      '* Garg et al. (2023) Garg, D., Hejna, J., Geist, M., and Ermon, S. Extreme q-learning: 엔트로피가 없는 Maxent rl. _ arXiv preprint arXiv:2301.02328_, 2023.\n' +
      '* Ghasemipour et al. (2022) Ghasemipour, K., Gu, S. S., and Nachum, O. 왜 그렇게 비관적이야? 앙상블을 통해 오프라인 rl에 대한 불확실성을 추정하고, 이들의 독립성이 중요한 이유를 추정함. _ 신경 정보 처리 시스템_, 35:18267-18281, 2022에서의 발전.\n' +
      '* Ha & Schmidhuber (2018) Ha, D. and Schmidhuber, J. World models. _ arXiv preprint arXiv:1803.10122_, 2018.\n' +
      '* Hafner et al. (2019) Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. 통제하는 꿈: 잠재된 상상력에 의한 학습 행동 arXiv preprint arXiv:1912.01603_, 2019a.\n' +
      '* Hafner et al. (2019) Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In _International conference on machine learning_, pp. 2555-2565. PMLR, 2019b.\n' +
      '* Hafner et al. (2020) Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mastering atari with discrete world models. _ arXiv preprint arXiv:2010.02193_, 2020.\n' +
      '* Hutter et al. (2019)* Hafner et al. (2023) Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. 세계 모델을 통해 다양한 도메인을 마스터합니다. _ arXiv preprint arXiv:2301.04104_, 2023.\n' +
      '* Hansen et al. (2022a) Hansen, N., Lin, Y., Su, H., Wang, X., Kumar, V., and Rajeswaran, A. Modem: Accelerating visual model-based reinforcement learning with demonstrations. _ arXiv preprint arXiv:2212.05698_, 2022a.\n' +
      '* Hansen et al. (2022b) Hansen, N., Wang, X., and Su, H. Temporal Difference Learning for model predictive control. _ arXiv preprint arXiv:2203.04955_, 2022b.\n' +
      '* Hansen et al. (2023) Hansen, N., Su, H., and Wang, X. Td-mpc2: 연속 제어를 위한 스케일러블, 로버스트 월드 모델 _ arXiv preprint arXiv:2310.16828_, 2023.\n' +
      '* Hansen-Estruch et al. (2023) Hansen-Estruch, P., Kostrikov, I., Janner, M., Kuba, J. G., and Levine, S. Idql: 확산 정책을 갖는 행위자-비판적 방법으로서 내재적 q-learning. _ arXiv preprint arXiv:2304.10573_, 2023.\n' +
      '* Ho & Salimans(2022) Ho, J. and Salimans, T. 분류자가 없는 확산 안내. _ ArXiv:2207.12598_, 2022.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probability models. _ 신경 정보 처리 시스템_, 33:6840-6851, 2020에서의 발전.\n' +
      '* Janner et al. (2019) Janner, M., Fu, J., Zhang, M., and Levine, S. 모델을 신뢰할 때: 모델 기반 정책 최적화. _ 신경 정보 처리 시스템_, 32, 2019의 발전.\n' +
      '* Janner et al. (2020) Janner, M., Mordatch, I., and Levine, S. gamma-models: Generative temporal difference learning for infinite-horizon prediction. _ 2020년, 신경망 정보 처리 시스템_, 33:1724-1735의 발전.\n' +
      '* Janner et al. (2021) Janner, M., Li, Q., and Levine, S. 하나의 큰 시퀀스 모델링 문제로서 오프라인 강화학습이 있다. _30-Fifth Conference on Neural Information Processing Systems_, 2021. URL[https://openreview.net/forum?id=wgeK563QgSw](https://openreview.net/forum?id=wgeK563QgSw).\n' +
      '* Janner et al. (2022) Janner, M., Du, Y., Tenenbaum, J. B., and Levine, S. 융통성 있는 거동 합성을 위한 확산 계획. _ ArXiv:2205.09991_, 2022.\n' +
      '* Jaques et al. (2019) Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. 대화 상자에서 암시적 인간 선호도에 대한 잘못된 정책 배치 심층 강화 학습 ArXiv:1907.00456_, 2019.\n' +
      '* Jia et al. (2023) Jia, Z., Liu, F., Thumuluri, V., Chen, L., Huang, Z., and Su, H. Chain-of-thought predictive control. _ arXiv preprint arXiv:2304.00776_, 2023.\n' +
      '* Kaiser et al. (2019) Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al. Model-based reinforcement learning for atari. _ ArXiv preprint arXiv:1903.00374_, 2019.\n' +
      '* Kidambi et al. (2020) Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. Morel: Model-based offline reinforcement learning. _ 신경 정보 처리 시스템_, 33:21810-21823, 2020에서의 발전.\n' +
      '* Kingma et al. (2021) Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. _ 신경 정보 처리 시스템_, 34:21696-21707, 2021에서의 발전.\n' +
      '* Kostrikov et al. (2021) Kostrikov, I., Nair, A., and Levine, S. 암묵적 q-러닝을 활용한 오프라인 강화학습, 2021년.\n' +
      '* Kumar et al. (2019) Kumar, A., Fu, J., Tucker, G., and Levine, S. 부트스트래핑 오류 감소를 통한 오프 정책 q-러닝 안정화 ArXiv preprint arXiv:1906.00949_, 2019.\n' +
      '* Kumar et al. (2020) Kumar, A., Zhou, A., Tucker, G., and Levine, S. 오프라인 강화학습을 위한 보수적 q-learning. _ arXiv preprint arXiv:2006.04779_, 2020.\n' +
      '* Lambert et al.(2022) Lambert, N., Pister, K., and Calandra, R. 학습된 역학 모델에서 컴파운딩 예측 오류를 조사합니다. _ arXiv preprint arXiv:2203.09637_, 2022.\n' +
      '* Lee et al. (2022) Lee, K. -H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama, S., Fischer, I., Jang, E., Michalewski, H., et al. Multi-game decision transformerers. _ arXiv preprint arXiv:2205.15241_, 2022.\n' +
      '* Lipman et al. (2022) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. 생성 모델링을 위한 흐름 일치 _ ArXiv:2210.02747_, 2022.\n' +
      '* Meng et al. (2021) Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W., Wen, Y., Zhang, H., Wang, J., and Xu, B. Offline prerained multi-agent decision transformer: 하나의 큰 시퀀스 모델이 모든 starcrafii 태스크를 정복한다. _ arXiv preprint arXiv:2112.02845_, 2021.\n' +
      '* Micheli et al. (2022) Micheli, V., Alonso, E., and Fleuret, F. Transformers is sample efficient world models. _ arXiv preprint arXiv:2209.00588_, 2022.\n' +
      '* Mish(2019) Mish, M. D. A self regularized non-monotonic activation function[j]. _ ArXiv preprint arXiv:1908.08681_, 2019.\n' +
      '* Mishra & Chen (2023) Mishra, U. A. and Chen, Y. Reorientdiff: 객체 조작을 위한 확산 모델 기반 재배향. _ arXiv preprint arXiv:2303.12700_, 2023.\n' +
      '* Nagabandi et al. (2018) Nagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. 모델이 없는 미세 조정을 통한 모델 기반 심층 강화 학습을 위한 신경망 역학. In _2018_IEEE international conference on robotics and automation (ICRA)_, pp. 7559-7566. IEEE, 2018.\n' +
      '* Nakamoto et al. (2023) Nakamoto, M., Zhai, Y., Singh, A., Mark, M. S., Ma, Y., Finn, C., Kumar, A., and Levine, S. Cal-ql: 효율적인 온라인 미세 조정을 위한 보정된 오프라인 rl 사전 훈련. _ arXiv preprint arXiv:2303.05479_, 2023.\n' +
      '* Nguyen et al. (2022) Nguyen, T., Zheng, Q., and Grover, A. Conserveighttive behavioral cloning for reliable offline reinforcement learning. _ arXiv preprint arXiv:2210.05158_, 2022.\n' +
      '* Nichol & Dhariwal (2021) Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probability models. In _International Conference on Machine Learning_, pp. 8162-8171. PMLR, 2021.\n' +
      '* Peng et al. (2019) Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advante-weighted regression: Simple and scalable off-policy reinforcement learning. _ ArXiv preprint arXiv:1910.00177_, 2019.\n' +
      '* Robine et al. (2023) Robine, J., Hoffmann, M., Uelwer, T., and Harmeling, S. 트랜스포머 기반 세계 모델은 100k 상호 작용에 만족합니다. _ arXiv preprint arXiv:2303.07109_, 2023.\n' +
      '* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. U-net: 생체 의학 영상 분할을 위한 컨볼루션 네트워크. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Schrittwieser et al. (2020) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with learned model. _ Nature_, 588(7839):604-609, 2020.\n' +
      '* Schulman et al. (2015) Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. Highdimensional continuous control using generalized advantage estimation. _ arXiv preprint arXiv:1506.02438_, 2015.\n' +
      '* Silver et al. (2014) Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. 결정론적 정책 기울기 알고리즘입니다. In _International conference on machine learning_, pp. 387-395. Pmlr, 2014.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _ arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* Song et al. (2023) Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. _ arXiv preprint arXiv:2303.01469_, 2023.\n' +
      '* Sutton(1990) Sutton, R. S. 근사 동적 프로그래밍을 기반으로 학습, 계획 및 반응을 위한 통합 아키텍처. In _Machine Learning proceedings 1990_, pp. 216-224. Elsevier, 1990.\n' +
      '* 서튼(1991) 서튼, R. S. Dyna, learning, planning, reaction을 위한 통합 아키텍처. _ ACM Sigart Bulletin_, 2(4):160-163, 1991.\n' +
      '* Thrun & Schwartz (2014) Thrun, S. 및 Schwartz, A. Issues to use function approximation for reinforcement learning. In _Proceedings of the 1993 connectionist models summer school_, pp. 255-263. Psychology Press, 2014.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wang et al. (2019) Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba, J. Benchmarking model-based reinforcement learning. _ ArXiv preprint arXiv:1907.02057_, 2019.\n' +
      '* Wang et al. (2022) Wang, Z., Hunt, J. J., and Zhou, M. 오프라인 강화 학습을 위한 표현적 정책 클래스로서의 확산 정책. _ ArXiv:2208.06193_, 2022.\n' +
      '* Watkins & Dayan (1992) Watkins, C. J. and Dayan, P. Q-learning. _ Machine learning_, 8:279-292, 1992.\n' +
      '* Williams et al. (2015) Williams, G., Aldrich, A., and Theodorou, E. Model predictive path integral control using covariance variable importance sampling. _ arXiv preprint arXiv:1509.01149_, 2015.\n' +
      '* Wu & He(2018) Wu, Y. 및 He, K. 그룹 정규화 In _Proceedings of the European conference on computer vision (ECCV)_, pp. 3-19, 2018.\n' +
      '* Wu et al. (2019) Wu, Y., Tucker, G., and Nachum, O. 행동 규칙화된 오프라인 강화 학습. _ ArXiv preprint arXiv:1911.11361_, 2019.\n' +
      '* Xiao et al.(2019) Xiao, C., Wu, Y., Ma, C., Schuurmans, D., and Muller, M. 모델 기반 강화 학습에서 컴파운딩 오류에 대처하는 학습. _ arXiv preprint arXiv:1912.11206_, 2019.\n' +
      '*Xu et al. (2023) Xu, Y., Li, N., Goel, A., Guo, Z., Yao, Z., Kasaei, H., Kasaei, M., and Li, Z. 뉴럴 오드로 기저의 동적 시스템을 학습하여 제어 가능한 비디오 생성 arXiv preprint arXiv:2303.05323_, 2023.\n' +
      '\n' +
      '야마가타, T., 칼릴, A., 산토스-로드리게스, R. Q-learning 결정 트랜스포머: 오프라인 rl에서 조건부 시퀀스 모델링을 위한 동적 프로그래밍 활용 In _International Conference on Machine Learning_, pp. 38989-39007. PMLR, 2023.\n' +
      '* Ye et al. (2021) Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. 제한된 데이터로 아타리 게임을 마스터합니다. _ 신경 정보 처리 시스템_, 34:25476-25488, 2021에서의 발전.\n' +
      '* Yu et al. (2020) Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. 모포: 모델 기반 오프라인 정책 최적화. _ 신경 정보 처리 시스템_, 33:14129-14142, 2020에서의 발전.\n' +
      '* Zheng et al. (2022) Zheng, Q., Zhang, A., and Grover, A. Online decision transformer. _ arXiv preprint arXiv:2202.05607_, 2022.\n' +
      '* Zheng et al. (2023a) Zheng, Q., Henaff, M., Amos, B., and Grover, A. Semi-supervised offline reinforcement learning with action-free trajectory. In _International Conference on Machine Learning_, pp. 42339-42362. PMLR, 2023a.\n' +
      '* Zheng et al. (2023b) Zheng, Q., Le, M., Shaul, N., Lipman, Y., Grover, A., and Chen, R. T. Guided flows for generative modeling and decision decisions. _ arXiv preprint arXiv:2311.13443_, 2023b.\n' +
      '\n' +
      '확산 세계 모델의 구현 세부사항\n' +
      '\n' +
      '우리는 실험에 사용된 아키텍처와 하이퍼파라미터를 요약한다. 모든 실험에 대해 다음 코드베이스의 영향을 많이 받는 자체 파이토치 구현을 사용한다.\n' +
      '\n' +
      '결정 Difffuser(Ajay et al., 2022) [https://github.com/anuragajay/decision-diffuser](https://github.com/anuragajay/decision-diffuser)\n' +
      '\n' +
      '디퓨저(Janner et al., 2022) [https://github.com/jannemr/diffuser/](https://github.com/jannemr/diffuser/]\n' +
      '\n' +
      'SSORL (Zheng et al., 2023a) [https://github.com/facebookresearch/ssorl/](https://github.com/facebookresearch/ssorl/]\n' +
      '\n' +
      '3.1절에 소개된 바와 같이, 본 논문에서 사용된 확산 세계 모델 \\(p_{\\theta}\\)은 길이-\\(T\\) 서브트라제코티브 \\((s_{t},a_{t},r_{t},s_{t+1},r_{t+1},\\ldots,s_{t+T-1},r_{t+T-1})\\을 모델링하기 위해 선택된다. 추론 시간에는 초기 상태-작용 쌍에 대한 조건화(s_{t},a_{t})\\) 및 목표 RTG\\(y=g_{t}\\)의 후속 서브트라제코트리(T-1) 단계를 예측한다:\n' +
      '\n' +
      '\\[\\widehat{r}_{t},\\widehat{s}_{t+1},\\widehat{r}_{t+1},\\ldots,\\widehat{s}_{t+T-1},\\widehat{r}_{t+T-1},\\widehat{r}_{t+T-1}\\sim p_{\\theta}(\\cdot|s_{t},a_{t},y=g_{t}). \\tag{6}\\tag{6}\\\n' +
      '\n' +
      '우리가 시퀀스에서 미래의 행동을 모델링하지 않기로 선택한 두 가지 이유가 있다. 먼저, 제안된 확산 모델 값 확장(Definition 3.1)은 향후 단계를 위한 액션 정보를 필요로 하지 않는다. 둘째, 이전 연구에서는 확산을 통한 연속적인 행동을 모델링하는 것이 덜 정확하다는 것을 발견했다(Ajay et al., 2022).\n' +
      '\n' +
      '본 논문에서는 길이\\(T=8\\)의 상태-보상 시퀀스에 대한 유도확산 모델을 학습한다. 확산 단계의 수는 \\(K=5\\)이다. null conditioning \\(p_{\\text{uncond}}\\)의 확률은 \\(0.25\\)으로 설정되고 배치 크기는 \\(64\\)이다. 우리는 Nichol과 Dhariwal(2021)이 제안한 코사인 잡음 스케줄을 사용한다. 할인계수는 \\(\\gamma=0.99\\)이며, 할인된 RTG는 Hopper의 경우 \\(400\\), Walker의 경우 \\(550\\), Halfcheetah의 경우 \\(1200\\)의 과제별 보상척도로 정규화하였다.\n' +
      '\n' +
      'Ajay et al.(2022)에 이어, 우리의 잡음 예측기 \\(\\varepsilon_{\\theta}\\)는 6개의 반복 잔차 블록으로 구성된 시간적 U-넷(Janner et al., 2022; Ronneberger et al., 2015)이며, 각 블록은 2개의 시간적 컨볼루션으로 구성되며 그룹 노말(Wu and He, 2018)과 최종 Mish 비선형 활성화(Mish, 2019)가 뒤따른다. 확산 단계 \\(k\\)는 먼저 정현파 위치 인코딩으로 변환되고 2-레이어 MLP를 통해 잠재 공간으로 투영되며 RTG 값은 3-레이어 MLP를 통해 잠재 임베딩으로 변환된다. 확산 세계 모델에서는 추가 조건으로서 초기 작용 \\(a_{t}\\)도 3-층 MLP를 통해 잠재 임베딩으로 변환되고 확산 단계 및 RTG의 임베딩과 추가로 연결된다.\n' +
      '\n' +
      '모든 데이터 세트에 대해 학습률\\(1\\times 10^{-4}\\)을 갖는 Adam 최적화기에 의해 모델을 최적화한다. 최종 모델 매개변수\\(\\bar{\\theta}\\)는 훈련 과정에서 얻은 매개변수의 지수 이동 평균(EMA)이다. 모든 반복에 대해 지수 감쇠 매개변수 \\(\\beta=0.995\\)인 \\(\\bar{\\theta}=\\beta\\bar{\\theta}+(1-\\beta)\\theta\\를 갱신한다. 우리는 \\(2\\times 10^{6}\\) 반복에 대한 확산 모델을 훈련한다.\n' +
      '\n' +
      '**Sampling with Guidance.** 확산 모델로부터 샘플링하기 위해서는 먼저 랜덤 잡음 \\(x(K)\\sim\\mathcal{N}(0,\\mathbf{I})\\)을 샘플링한 후 역과정을 수행해야 한다. 알고리즘 A.1은 분류기가 없는 지침에 따라 훈련된 확산 모델에서 샘플링하는 일반적인 프로세스를 제시한다.\n' +
      '\n' +
      '오프라인 RL의 맥락에서 확산 세계 모델은 현재 상태\\(s_{t}\\), 현재 동작\\(a_{t}\\) 및 목표 복귀\\(g_{text{eval}}\\)을 기반으로 미래 상태 및 보상을 생성한다. 따라서 샘플링 과정은 3절을 참조하며, 초기 상태 및 초기 동작을 각각 \\(s_{t}\\) 및 \\(a_{t}\\)으로 제한해야 하므로 알고리즘 A.1과 약간 다르다. 적응된 알고리즘은 알고리즘 A.2에 요약되어 있다.\n' +
      '\n' +
      'Ajay et al.(2022)에 이어 확산 모델에 저온 샘플링 기법을 적용한다. 가우시안(\\mathcal{N}(\\widehat{\\theta},\\alpha^{2}\\widehat{\\Sigma}_{\\theta})으로부터 확산 단계별로 샘플링하기 위한 온도는 \\(\\alpha=0.5\\)으로 설정되었으며, \\(\\widehat{\\theta}\\) 및 \\(\\widehat{\\Sigma}{\\theta}\\)이 예측 평균 및 공분산이다.\n' +
      '\n' +
      '**Accelerated Inference.** 알고리즘 A.1 및 A.2는 전체 역 프로세스를 실행하고, 그 위에 빌딩, 우리는 샘플링 프로세스를 가속화하기 위해 Nichol 및 Dhariwal(2021)에서와 같은 스트라이드 샘플링 기술을 추가로 적용한다. 형식적으로 완전 역과정에서는 \\(k=K\\)에서 \\(k=1\\)까지 \\(x^{(k-1)}\\)을 \\(x^{(k)}\\)씩 생성한다.\n' +
      '\n' +
      '[x^{(k-1)}=\\frac{\\sqrt{\\bar{\\alpha}_{k-1}\\beta_{k}{1-\\bar{\\alpha}_{k}\\widehat{x}^{(0)}+\\frac{\\sqrt{\\alpha_{k}(1-\\bar{\\alpha}_{k-1}}{1-\\bar{\\alpha}_{k}}x^{(k)}\\sigma_{k}\\varepsilon,\\tag{7}\\mathcal{N}(0,\\mathbf{I}),\\tag{7}\\frac{\\sqrt{\\alpha_{k}}{x^{(k)}+\\sigma_{k}\\varepsilon\\sim\\mathcal{N}(0,\\mathbf{I}),\\tag{7}\\frac{\\sqrt{\\alpha_{k}}{x^{(k)}+\\sigma_{k}\\vareps\n' +
      '\n' +
      '여기서 \\(\\widehat{x}^{(0)}\\)은 참 데이터 점(알고리즘 A.1의 라인 5), \\(\\sigma_{k}=\\sqrt{\\frac{\\beta_{k}(1-\\bar{\\alpha}_{k-1})}{1-\\bar{\\alpha}_{k}}\\)은 단계 \\(k\\)(알고리즘 A.1의 라인 8)에서의 잡음의 표준편차이다. [\\(\\bar{\\alpha}_{k}=\\prod_{k^{\\prime}=1}^{K}(1-\\beta_{k^{\\prime})\\)에서 잡음 스케줄 \\(\\{\\beta_{k}\\}_{k=1}^{K}\\)이 미리 정의되어 있음을 주목하고, 섹션 2 및 부록 A를 참조한다.\n' +
      '\n' +
      '완전한 역과정을 실행하는 것은 시간 소모적인 식 (7)을 \\(K\\) 시간 동안 평가하는 것에 해당한다. 샘플링을 빠르게 하기 위해 \\(1\\)과 \\(K\\) 사이의 확산 단계, 즉 \\(\\tau_{1},\\dots,\\tau_{N}\\)을 선택하는데, 여기서 \\(\\tau_{N}=K\\이다. 그런 다음 선택한 단계 \\(\\tau_{1},\\dots,\\tau_{N}\\)에 대해 식 (7)을 평가한다. 이것은 추론 시간을 원본의 대략 \\(N/K\\)으로 효과적으로 줄인다. 실험에서 우리는 \\(K=5\\) 확산 단계와 \\(N=3\\) 추론 단계로 확산 모델을 훈련하고 이 수의 정당화를 위해 섹션 4.2를 참조한다.\n' +
      '\n' +
      '```\n' +
      '1Input: 훈련된 잡음 예측 모델 \\(\\varepsilon_{\\theta}\\), 조절 파라미터 \\(y\\), 유도 파라미터 \\(\\omega\\), 확산 단계 수 \\(K\\)\n' +
      '2\\(x^{(K)}\\sim\\mathcal{N}(0,\\mathbf{I})\\)\n' +
      '3for\\(k=K,\\dots,1\\)do\n' +
      '4\\(\\widehat{\\varepsilon}\\leftarrow\\omega\\cdot\\varepsilon_{\\theta}(x^{(k)},k,y)+(1- \\omega)\\cdot\\varepsilon_{\\theta}(x^{(k)},k,\\varnothing)\\)//estimatetruedatapoint\\(x^{(0)}\\)\n' +
      '5\\(\\widehat{x}^{(0)}\\leftarrow\\frac{1}{\\sqrt{\\tilde{\\alpha}_{k}}}\\big{(}x^{(k)}- \\sqrt{1-\\tilde{\\alpha}_{t}}\\widehat{\\varepsilon}\\big{)}\\)//Samplefromtheposteriordistribution\\(q(x^{(k-1)}|x^{(k)},x^{(0)})\\)//SeeEquation(6)and(7)ofHoetal.(2020)\n' +
      '6\\(\\widehat{\\mu}leftarrow\\frac{\\sqrt{\\tilde{\\alpha}_{k-1}\\beta_{k}{1-\\tilde{\\alpha}_{k}\\widehat{x}^{(0)}+\\frac{\\sqrt{\\alpha_{k}(1-\\tilde{\\alpha}_{k-1}}{1-\\tilde{\\alpha}_{k}}x^{(k)}}\n' +
      '7\\(\\widehat{\\Sigma}\\leftarrow\\frac{\\beta_{k}(1-\\tilde{\\alpha}_{k-1})}{1-\\tilde{ \\alpha}_{k}}\\mathbf{I}\\)\n' +
      '8\\(x^{(k-1)}\\sim\\mathcal{N}(\\widehat{\\mu},\\widehat{\\Sigma})\\)\n' +
      '9Output:\\(x^{(0)}\\)\n' +
      '```\n' +
      '\n' +
      '**안내 확산 모델로부터의 알고리즘 A.1**샘플링\n' +
      '\n' +
      '```\n' +
      '1Input: 훈련된 잡음 예측 모델\\(\\varepsilon_{\\theta}\\), 초기 상태\\(s_{t}\\), 초기 행동\\(a_{t}\\), 목표 복귀\\(g_{\\text{eval}}\\), 유도 파라미터\\(\\omega\\), 확산 단계 수\\(K\\)\n' +
      '2\\(x^{(K)}\\sim\\mathcal{N}(0,\\mathbf{I})\\)//applyconditioningof\\(s_{t}\\)and\\(a_{t}\\)\n' +
      '3\\(x^{(K)}[0:\\text{dim}(s_{t})+\\text{dim}(a_{t})]\\leftarrow\\text{concatenate}(s_{t},a_{t})\\)\n' +
      '4for\\(k=K,\\dots,1\\)do\n' +
      '5\\(\\widehat{\\varepsilon}\\leftarrow\\omega\\cdot\\varepsilon_{\\theta}(x^{(k)},k,g_{ \\text{eval}})+(1-\\omega)\\cdot\\varepsilon_{\\theta}(x^{(k)},k,\\varnothing)\\)//estimatetruedatapoint\\(x^{(0)}\\)\n' +
      '6\\(\\widehat{x}^{(0)}\\leftarrow\\frac{1}{\\sqrt{\\tilde{\\alpha}_{k}}}\\big{(}x^{(k)}- \\sqrt{1-\\tilde{\\alpha}_{t}}\\widehat{\\varepsilon}\\big{)}\\)//Samplefromtheposteriordistribution\\(q(x^{(k-1)}|x^{(k)},x^{(0)})\\)//SeeEquation(6)and(7)ofHoetal.(2020)\n' +
      '7\\(\\widehat{\\mu}\\leftarrow\\frac{\\sqrt{\\tilde{\\alpha}_{k-1}\\beta_{k}{1-\\tilde{\\alpha}_{k}\\widehat{x}^{(0}+\\frac{\\sqrt{\\alpha_{k}}(1-\\tilde{\\alpha}_{k-1}}{1-\\tilde{\\alpha}_{k}}x^{(k}})}\n' +
      '8\\(\\widehat{\\Sigma}\\leftarrow\\frac{\\beta_{k}(1-\\tilde{\\alpha}_{k-1})}{1-\\tilde{ \\alpha}_{k}}\\mathbf{I}\\)\n' +
      '9\\(x^{(k-1)}\\sim\\mathcal{N}(\\widehat{\\mu},\\widehat{\\Sigma})\\)//applyconditioningof\\(s_{t}\\)and\\(a_{t}\\)\n' +
      '10\\(x^{(k-1)}[0:\\text{dim}(s_{t})+\\text{dim}(a_{t})]\\leftarrow\\text{concatenate}(s_{t},a_{t})\\)\n' +
      '11\n' +
      '12Output:\\(x^{(0)}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 A.2**확산 세계 모델 샘플링\n' +
      '\n' +
      '## 부록 B 1단계 동역학 모델 구현 세부사항\n' +
      '\n' +
      '전통적인 1단계 동역학 모델 \\(f_{\\theta}(s_{t+1},r_{t}|s_{t},a_{t})\\)은 일반적으로 상태 및 보상 공간에 대한 매개변수화된 확률 분포로 표현되며 단일 단계 천이의 로그 우도 최대화를 통해 최적화된다:\n' +
      '\n' +
      '\\[\\max_{\\theta}\\mathbb{E}_{(s_{t},a_{t},r_{t},s_{t+1})\\sim\\mathcal{D}_{\\text{ offline}}\\left[\\log f_{\\theta}(s_{t+1},r_{t}|s_{t},a_{t})\\right], \\tag{8}\\]where \\((s_{t},a_{t},r_{t},s_{t+1})\\)는 오프라인 데이터 분포 \\(\\mathcal{D}_{\\text{offline}}\\)로부터 샘플링된다. Kidambi et al.(2020)에서와 같이, 우리는 평균 \\(\\mu_{\\theta},\\Sigma_{\\theta})\\(f_{\\theta}\\)을 가우시안 분포 \\(\\mathcal{N}(\\mu_{\\theta},\\Sigma_{\\theta})\\)로 모델링하고, 여기서 평균 \\(\\mu_{\\theta}\\)과 대각 공분산 행렬 \\(\\Sigma_{\\theta}\\)은 계층당 256개의 은닉 단위를 갖는 두 개의 4-계층 MLP 신경망에 의해 파라미터화된다. 우리는 은닉층에 ReLU 활성화 함수를 사용한다. Sigma_{\\theta}\\(\\Sigma_{\\theta}\\)의 마지막 계층은 SoftPlus 함수에 의해 활성화되어 유효성을 보장한다. 우리는 학습률\\(1\\times 10^{-4}\\)을 갖는 Adam 최적화기를 사용하여 \\(1\\times 10^{-6}\\) 반복에 대한 동역학 모델을 학습한다.\n' +
      '\n' +
      '## 부록 C 확산 월드 모델 기반 오프라인 RL 방법\n' +
      '\n' +
      '섹션 4에서는 TD3+BC, IQL, Q-러닝과 비관적 보상(PQL)을 프레임워크에 통합하는 알고리즘 3.1의 3가지 인스턴스화를 고려한다. 이러한 알고리즘은 오프라인 RL을 위해 특별히 설계되었으며, 액션(TD3+BC), 가치 함수(IQL) 및 보상(PQL)에 대해 각각 _conservatism_ 개념이 정의되어 있다. 후속편에서, 우리는 우리의 인스턴스화를 DWM-TD3BC, DWM-IQL 및 DWM-PQL로 지칭한다. 이들에 대한 구체적인 구현은 아래에서 소개될 것이다.\n' +
      '\n' +
      '### DWM-TD3BC : 확산 월드 모델을 갖는 TD3+BC\n' +
      '\n' +
      'TD3 알고리즘(Fujimoto et al., 2018), TD3+BC(Fujimoto and Gu, 2021)는 결정론적 정책을 학습하기 위해 명시적 행동 복제 규칙화를 사용한다. 알고리즘은 다음과 같이 동작한다.\n' +
      '\n' +
      '비평가 훈련은 TD3 알고리즘을 정확히 따른다. 우리는 TD 학습을 통해 두 개의 비평가 네트워크\\(Q_{\\phi_{1}}\\)와 \\(Q_{\\phi_{2}}\\)을 이중 Q-학습(Fujimoto et al., 2018)으로 학습한다. transition \\((s,a,r,s^{\\prime}))에 대한 TD 학습의 목표값은 다음과 같다.\n' +
      '\n' +
      '\\[y=r+\\gamma\\min_{i\\in\\{1,2\\}}Q_{\\phi_{i}}\\big{(}s^{\\prime},a^{\\prime}=\\text{Clip}(\\pi_{\\bar{\\psi}}(s^{\\prime})+\\varepsilon,-C,C)\\big{}, \\tag{9}\\big{(}s^{\\prime},a^{\\prime}=\\text{Clip}(\\pi_{\\bar{\\psi}}(s^{\\prime})+\\varepsilon,-C,C)\\big{}, \\tag{9}\\big{(}s^{\\prime},a^{\\prime}=\\text{Clip}(\\pi_{\\bar{\\psi}}(s^{\\prime})+\\varepsilon,-C,C)\\big{}, \\tag{9}\\big{(s^{\\prime},a^\n' +
      '\n' +
      '여기서 \\(\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)\\)은 랜덤 잡음이고, \\(\\pi_{\\bar{\\psi}\\)은 목표 정책이고 Clip(\\(\\cdot\\))은 각 차원에 대해 \\([-C,C]\\) 내에 있는 액션 벡터의 값을 제한하는 연산자이다. 평균 제곱 오차(MSE)를 최소화함으로써 \\(Q_{\\phi_{1}}\\)와 \\(Q_{\\phi_{2}}\\) 모두 목표 값 \\(y\\)으로 회귀할 것이며, 이는 다음과 같은 문제를 해결하는 데 해당한다.\n' +
      '\n' +
      '\\sim\\mathbb{E}_{(s,a,r,s^{\\prime}\\mathcal{D}_{\\text{offline}}\\left[\\left(y(r,s^{\\prime})-Q_{\\phi_{i}}(s,a\\right)^{2}\\right],\\;i\\in\\{1,2\\}. \\tag{10}\\text{offline}}\n' +
      '\n' +
      '정책을 훈련시키기 위해 TD3+BC는 다음과 같은 정규화된 문제를 최적화한다:\n' +
      '\n' +
      '\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal{D}_{{text{offline}}\\left[\\lambda Q _{\\phi_{1}}(s,\\pi_{\\psi}(s)-\\left\\|a-\\pi_{\\psi}(s)\\right\\|^{2}\\right], \\tag{11}\\tag{1}}\n' +
      '\n' +
      '이는 오프라인 데이터 분포이다. 행동 복제 규칙화 항 \\(\\left\\|a-\\pi_{\\psi}(s)\\right\\|^{2}\\)이 없으면, 위의 문제는 결정론적 정책 기울기 정리에 해당하는 목표로 줄어든다(Silver et al., 2014). \\(\\pi\\)는 항상 하나의 고정된 프록시 값 함수를 최대화하려고 한다는 점에 유의한다.\n' +
      '\n' +
      'TD3+BC에서는 목표 정책(\\pi_{\\bar{\\psi}\\)과 목표 비평가 네트워크(Q_{\\bar{\\phi}_{i}\\)의 업데이트가 모두 지연된다. 전체 알고리즘은 알고리즘 C.1에 요약되어 있다.\n' +
      '\n' +
      '### DWM-IQL : 확산 월드 모델을 갖는 IQL\n' +
      '\n' +
      'IQL(Kostrikov et al., 2021)은 오프라인 데이터셋 상에서 비관적 가치 추정을 적용한다. TD3+BC에 사용된 이중 Q 함수 외에도 IQL은 추가 상태 값 함수 \\(V_{\\xi}(s)\\)를 활용하며, 이는 기대 회귀를 통해 추정된다:\n' +
      '\n' +
      '\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal}\\left[L^{\\tau}\\left(\\min_{i\\in\\{1,2\\}}Q_{\\bar{\\phi}_{i}}(s,a)-V_{\\xi}(s\\right)\\right], \\tag{12}\\tau}\\left[L^{\\tau}\\left(\\min_{i\\in\\{1,2\\}}Q_{\\bar{\\phi}_{i}}(s,a)-V_{\\xi}(s\\right)\\right], \\tag{12}\\tau}\n' +
      '\n' +
      '여기서 \\(L^{\\tau}(u)=|\\tau-\\mathbb{1}_{u<0}|u^{2}\\) with hyperparameter \\(\\tau\\in(0.5,1)\\), As\\(\\tau\\to 1\\), \\(V_{\\xi}(s)\\)는 기본적으로 \\(Q(s,a)\\)의 최대값을 추정하고 있다. 이는 명시적인 정책 없이 암묵적으로 정책 개선 단계를 수행하는 것으로 볼 수 있다. 하이퍼파라미터 \\(\\tau<1\\)을 사용하면 (암시적 정책의) 값 추정을 정규화하여 \\(Q\\) 함수의 과대평가 문제를 완화한다. 또한 \\(Q\\) 함수는 Eq를 사용하여 업데이트된다. (10) 그러나 목표 \\(y=r+\\gamma V_{\\xi}(s^{\\prime})\\을 갖는다. 마지막으로 \\(Q\\) 및 \\(V\\) 함수를 고려할 때, 정책은 Advantage Weighted Regression (Peng et al., 2019)에 의해 추출되며, 즉 해결이다.\n' +
      '\n' +
      '\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal{D}_{(s,a)\\sim\\mathcal{D}_{\\text{ offline}}\\left[\\exp\\left(\\beta(Q_{\\phi}(s,a)-V_{\\xi}(s))\\right)\\log\\pi_{\\psi}(a|s\\right]\\tag{13}\\right]\n' +
      '\n' +
      '대상 비평가 네트워크(Q_{\\bar{\\phi}_{i}})의 업데이트는 IQL에서 지연된다. 전체 알고리즘은 알고리즘 C.2에 요약되어 있다.\n' +
      '\n' +
      '```\n' +
      '1Inputs: offline dataset \\(\\mathcal{D}_{\\text{offline}}\\), prerained diffusion world model \\(p_{\\theta}\\), simulation horizon \\(H\\), conditioning RTG \\(g_{\\text{eval}}\\), target network update frequency \\(n\\), coefficient \\(\\lambda\\), action perturbation and clipping parameter: \\(\\sigma\\), \\(C\\)\n' +
      '2배우와 비평가 네트워크를 초기화한다. \\(\\pi_{\\psi}\\), \\(Q_{\\phi_{1}}\\), \\(Q_{\\phi_{2}}\\)\n' +
      '3 목표 네트워크 가중치\\(\\bar{\\psi}\\leftarrow\\psi\\), \\(\\bar{\\phi}_{1}\\leftarrow\\phi_{1}\\), \\(\\bar{\\phi}_{2}\\leftarrow\\phi_{2}\\)를 초기화한다.\n' +
      '4for\\(i=1,2,\\ldots\\)until convergencedo\n' +
      '5 \\(\\mathcal{D}_{\\text{offline}})//확산 모델 값 확장 샘플 \\(\\widehat{r}_{t},\\widehat{s}_{t+1},\\widehat{r}_{t+1},\\ldots,\\widehat{s}_{t+T-1},\\widehat{r}_{t+T-1},\\widehat{r}_{t+T-1}\\sim p_{\\theta}(\\cdot|s_{t},a_{t},g_{\\text{eval})})\n' +
      '6개의 Sample \\(\\varepsilon\\sim\\mathcal{N}(0,\\sigma^{2}I)\\)\\(\\widehat{a}_{t+H}^{\\varepsilon\\leftarrow\\text{Clip}(\\pi_{\\bar{\\psi}(\\widehat{s}_{t+H}) +\\varepsilon,-C,C)\\) : 목표 \\(Q\\)값 \\(y=\\sum_h=0}^{H-1}\\gamma^{h}\\gamma^{h}\\min_{i}(\\widehat{s}_{t+H}+\\gamma^{h}\\gamma^{h}\\min_{i}(\\widehat{s}_{t+H}+\\gamma^{h}\\barepsilon})\\(y=\\sum_h=0}^{H-1}\\gamma^{h}\\gamma^{h}\n' +
      '7\\(\\phi_{1}\\leftarrow\\phi_{1}-\\eta\\nabla_{\\phi_{1}}\\left|Q_{\\phi_{1}}(s_{t},a_{t})- y\\right|_{2}^{2}\\)\n' +
      '8\\(\\phi_{2}\\leftarrow\\phi_{2}-\\eta\\nabla_{\\phi_{2}}\\left|Q_{\\phi_{2}}(s_{t},a_{t})-y\\right|_{2}^{2}\\)//배우를 업데이트한다.\n' +
      '9 배우 네트워크 업데이트: \\(\\psi\\leftarrow\\psi+\\eta\\nabla_{\\psi}\\left(\\lambda Q_{\\phi_{1}}\\Big{(}s_{t}, \\pi_{\\psi}(s_{t})\\Big{)}-\\left|a_{t}-\\pi_{\\psi}(s_{t})\\right|^{2}\\right))// 타겟 네트워크 업데이트\n' +
      '10if\\(i\\)\\(mod\\)\\(n\\)then\n' +
      '11\\(\\bar{\\phi}_{1}\\leftarrow\\bar{\\phi}_{1}+w(\\phi-\\bar{\\phi}_{1})\\)\\(\\bar{\\phi}_{2}\\leftarrow\\bar{\\phi}_{2}+w(\\phi-\\bar{\\phi}_{2})\\)\\(\\bar{\\psi}\\leftarrow\\bar{\\psi}+w(\\psi-\\bar{\\psi})\\)\n' +
      '12\n' +
      '13Output:\\(\\pi_{\\psi}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm C.1**DWM-TD3BC\n' +
      '\n' +
      '### DWM-PQL: 확산 월드 모델을 이용한 비관적 Q-학습\n' +
      '\n' +
      '기존의 오프라인 RL 알고리즘인 MOPO(Yu et al., 2020)는 보수성 개념을 보상함수에 직접 적용하였으며, 이를 비관적 Q-learning(PQL)이라고 하였다. 구체적으로, Yu et al.(2020)이 제안한 원래 알고리즘은 \\(m\\) 원스텝 동역학 모델 \\(\\{p_{\\theta_{i}}\\}_{i\\in[m]}\\)의 앙상블을 학습하고 수정된 보상을 사용한다.\n' +
      '\n' +
      '\\[\\tilde{r}(s,a)=\\widehat{r}(s,a)-\\kappa u(s,a|p_{\\theta_{1},\\ldots,p_{\\theta_{m}}) \\tag{14}\\.\n' +
      '\n' +
      '\\(Q\\) 함수를 학습하기 위해, 여기서 \\(\\widehat{r}\\)은 앙상블로부터의 평균 예측이고, \\(u(s,a|p_{\\theta_{1}},\\ldots,p_{\\theta_{m}})\\)은 앙상블을 사용하여 예측 불확실성을 측정한다.\n' +
      '\n' +
      'Yu et al.(2020)은 가우시안 분포에 의해 각 동역학 모델을 파라미터화하고, 각 공분산 행렬의 Frobenious norm의 최대값을 이용하여 예측 불확실성을 측정한다. 확산 모델은 이러한 파라미터화가 없고, 확산 모델들의 앙상블을 트레이닝하는 것은 계산적으로 벅차기 때문에, MoRel(Kidambi et al., 2020)에서 사용되는 것과 유사한 대안적인 불확실성 측정을 제안한다.\n' +
      '\n' +
      '주어진 \\((s_{t},a_{t})\\)와 \\(g_{\\text{eval}}\\)은 DWM에서 무작위로 \\(m\\)개의 시퀀스를 샘플링한다.\n' +
      '\n' +
      '\\[\\widehat{r}_{t}^{i},\\widehat{s}_{t+1}^{i},\\widehat{r}_{t+1}^{i},\\dots,\\widehat{ s}_{t+T-1}^{i},\\widehat{r}_{t+T-1}^{i},\\\\i\\in[m]. \\tag{15}\\tag{15}\\\n' +
      '\n' +
      '그런 다음 수정된 보상을 사용하여 첫 번째 샘플을 DWM 출력으로 취한다.\n' +
      '\n' +
      '{r}_{t^{m}\\frac{r}_{t^{m}\\prime}}^{i}-\\kappa\\max_{i\\in[m],j\\in[m}\\left(\\left\\\\widehat{r}_{t^{prime}}^{i}-\\widehat{r}_{t^{prime}}^{j}\\right\\prime}+\\left\\widehat{s}_{t^{\\prime}+1}^{j}\\right\\prime}_{2}\\right),\\t^{dots,t+T-2.\\tag{16}\\t^{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{m}\\frac{\n' +
      '\n' +
      '이는 확산 예측 궤적을 따라 각 타임스텝에 대한 불확실성-페널레이션된 보상을 구성하는 효율적인 방법을 제공한다. 이는 마지막 타임스테이프에 대해 예측된 보상에 적용되지 않는다는 점에 유의한다. 나머지 알고리즘은 IQL을 따르지만 값 네트워크를 업데이트하기 위해 기대 손실 대신 MSE 손실을 사용한다.\n' +
      '\n' +
      'DWM-PQL 알고리즘은 알고리즘 C.3에 요약되어 있다.\n' +
      '\n' +
      '1단계 동역학 모델을 이용한 기초선 방법의 경우, 1단계 동역학 모델(\\(f_{\\theta}(\\cdot|s,a)\\)과 정책(\\(\\pi_{\\psi}(\\cdot|s)\\):(\\widehat{\\tau}(s_{t},a_{t})\\sim(f_{\\theta}\\circ\\pi_{\\psi})^{H-1}(s_{t},a_{t})\\sim(f_{\\theta}\\circ\\pi_{\\psi})^{H-1}(s_{t},a_{t})\\sim(f_{\\theta}\\circ\\pi_{\\psi})^{H-1}(s_{t},a_{t})\\sim(f_{\\theta}\\circ\\pi}(s_{t},a_{t})\\sim(f_{\\theta}\\circ\\pi_{\\psi})^{H-1}(s_{t},a_{t 나머지는 위와 동일하게 유지하여 1단계 동역학, 즉 O-IQL, O-TD3BC 및 O-PQL을 갖는 MBRL 방법을 생성한다.\n' +
      '\n' +
      '```\n' +
      '1Inputs: offline dataset\\(\\mathcal{D}_{\\text{offline}}\\), prerained diffusion world model\\(p_{\\theta}\\), simulation horizon\\(H\\), conditioning RTG\\(g_{\\text{eval}}\\), target network update frequency\\(n\\), expectile loss parameter\\(\\tau\\)\n' +
      '2액터, 비평가 및 가치 네트워크를 초기화한다. \\(\\pi_{\\psi}\\), \\(Q_{\\phi_{1}}\\), \\(Q_{\\phi_{2}}\\), \\(V_{\\xi}\\)\n' +
      '3 목표 네트워크 가중치\\(\\bar{\\phi}_{1}\\leftarrow\\phi_{1}\\), \\(\\bar{\\phi}_{2}\\leftarrow\\phi_{2}\\)를 초기화한다.\n' +
      '4for\\(i=1,2,\\dots\\)until convergencedo\n' +
      '[D}_text{offline})//확산 모델 값 확장 샘플 \\(\\widehat{r}_t+1},\\widehat{r}_t+T-1},\\widehat{t+T-1},\\widehat{r}\\leftarrow\\phi_{i}(s_t},a_{t})-y\\right\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\wa\\w\n' +
      '6 배우 네트워크 업데이트: \\(\\psi\\leftarrow\\psi+\\eta\\nabla_{\\psi}\\exp\\left(\\beta(\\min_{i\\in\\{1,2\\}}Q_{\\phi_{i}}(s,a)-V_{\\xi}(s))\\right)\\log\\pi_{\\psi}(a|s)\\// 타겟 네트워크 업데이트\n' +
      '7ifi mod\\(n\\)then\n' +
      '8\\(\\bar{\\phi}_{1}\\leftarrow\\bar{\\phi}_{1}+w(\\phi-\\bar{\\phi}_{1})\\)\\(\\bar{\\phi}_{2}\\leftarrow\\bar{\\phi}_{2}+w(\\phi-\\bar{\\phi}_{2})\\)\n' +
      '9\n' +
      '10\n' +
      '11Output:\\(\\pi_{\\psi}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm C.2**DWM-IQL\n' +
      '\n' +
      '## 부록 D 훈련 및 오프라인 RL 알고리즘의 평가 세부사항\n' +
      '\n' +
      '### Common Settings\n' +
      '\n' +
      '우리는 데이터 정규화를 위한 모범 사례를 선택하기 위해 TD3+BC 및 IQL에 대한 1차 테스트를 수행한다. 그 결과를 바탕으로 TD3+BC, O-TD3BC 및 DWM-TD3BC는 관찰 정규화를 적용하고, 다른 알고리즘(O-PQL, DWM-PQL, IQL, O-IQL 및 DWM-IQL)은 관찰 정규화와 보상 정규화를 모두 적용한다.\n' +
      '\n' +
      '모든 알고리즘은 사전 훈련된 동역학 모델(1단계 및 확산)의 고정된 세트를 사용하여 128의 배치 크기로 훈련된다. 할인계수는 모든 자료에 대해 \\(\\gamma=0.99\\)으로 설정하였다.\n' +
      '\n' +
      '```\n' +
      '1Inputs: offline dataset\\(\\mathcal{D}_{\\text{offline}}\\), prerained diffusion world model\\(p_{\\theta}\\), simulation horizon\\(H\\), conditioning RTG\\(g_{\\text{eval}}\\), target network update frequency\\(n\\), pessimism coefficient\\(\\lambda\\), uncertainty estimation을 위한 sample수\\(m\\\\)\n' +
      '2 배우, 비평가 및 가치 네트워크를 초기화한다. \\(\\pi_{\\psi}\\), \\(Q_{\\phi_{1}}\\), \\(Q_{\\phi_{2}}\\), \\(V_{\\xi}\\)\n' +
      '3 목표 네트워크 가중치 \\(\\bar{\\phi}_{1}\\leftarrow\\phi_{1}\\), \\(\\bar{\\phi}_{2}\\leftarrow\\phi_{2}\\)를 초기화\n' +
      '4for\\(i=1,2,\\ldots\\)until convergencedo\n' +
      '5 \\(\\mathcal{D}_{\\text{offline}}\\)//확산 모델값 확장 샘플 \\(m\\) subtrajectories \\(\\tilde{r}^{j}_{t},\\tilde{s}^{j}_{t+1},\\hat{r}^{j}_{t+1},\\ldots,\\tilde{s}^{j}_{t+T-1},\\hat{r}^{j}_{t+T-1},\\hat{r}^{j}_{t+T-1},\\hat{r}^{j}_{t+T-1},\\hat{r}^{j}_{t+T-1}}^{m}_{j=1}\\sim p_{\\theta}(\\cdot|s_{t},a_{t},g_{text{eval}})\n' +
      '6 식과 같이 첫 번째 하위 궤적의 보상을 수정한다. (16): \\(\\tilde{r}_{t}\\), \\(\\hat{s}_{t+1},\\tilde{r}_{t+1},\\ldots,\\hat{s}_{t+T-1},\\hat{r}_{t+T-1})\n' +
      '7 목표 \\(Q\\)값 \\(y=\\sum_{h=0}^{H-1}\\gamma^{h}\\tilde{r}_{t+h}+\\gamma^{H}V_{\\xi}(\\tilde{s}^{1}_{t+H})\\)// \\(V\\)-값 네트워크를 갱신한다.\n' +
      '8\\(\\xi\\leftarrow\\xi-\\eta\\nabla_{\\xi}||\\min_{i\\in\\{1,2\\}}Q_{\\bar{\\phi}_{1}}(s,a)-V_{\\xi}(s))||^{2}_{2}\\)//비평가(\\(Q\\)-값 네트워크)를 업데이트한다.\n' +
      '9\\(\\phi_{1}\\leftarrow\\phi_{1}-\\eta\\nabla_{\\phi_{1}}\\left|Q_{\\phi_{1}}(s_{t},a_{t})- y\\right|^{2}_{2}\\)\n' +
      '10\\(\\phi_{2}\\leftarrow\\phi_{2}-\\eta\\nabla_{\\phi_{2}}\\left|Q_{\\phi_{2}}(s_{t},a_{t})-y\\right|^{2}_{2}\\)//배우를 업데이트한다.\n' +
      '11 배우 네트워크 갱신: \\(\\psi\\leftarrow\\psi+\\eta\\nabla_{\\psi}\\exp\\left(\\beta(\\min_{i\\in\\{1,2\\}}Q_{\\phi_{i}}(s,a)-V_{\\xi}(s))\\right)\\log\\pi_{\\psi}(a|s)\\// 타겟 네트워크 업데이트\n' +
      '12if\\(mod\\ n\\)then\n' +
      '13\\(\\bar{\\phi}_{1}\\leftarrow\\bar{\\phi}_{1}+w(\\phi-\\bar{\\phi}_{1})\\)\\(\\bar{\\phi}_{2}\\leftarrow\\bar{\\phi}_{2}+w(\\phi-\\bar{\\phi}_{2})\\)\n' +
      '14\n' +
      '15Output:\\(\\pi_{\\psi}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm C.3**DWM-PQL\n' +
      '\n' +
      '### MF Algorithms\n' +
      '\n' +
      'TD3+BC와 IQL은 \\(1\\times 10^{6}\\) 반복을 위해 학습되며, 배우, 비평가 및 가치 네트워크를 위한 학습률 \\(3\\times 10^{-4}\\)을 갖는다. 액터, 비평가 및 가치 네트워크는 모두 계층당 256개의 숨겨진 단위를 갖는 3-계층 MLP에 의해 매개변수화된다. 우리는 각 은닉층에 대해 ReLU 활성화 함수를 사용한다. IQL은 Tanh-Normal 분포를 출력하는 확률적 정책을 학습하고, TD3+BC는 Tanh 출력 활성화를 갖는 결정론적 정책을 학습한다. TD3+BC 및 IQL에 대한 하이퍼파라미터는 표 D.1에 나와 있다.\n' +
      '\n' +
      '베이스라인 DD(Ajay et al., 2022) 알고리즘은 시퀀스 길이\\(T=32\\)와 확산 단계 수\\(K=5\\)로 훈련된 확산 모델을 사용한다. 액션 예측을 위한 역동역학 모델(IDM)을 추가로 훈련해야 하는데, 이 모델은 은닉층별 1024개의 은닉 단위를 갖는 3-계층 MLP와 ReLU 활성화 함수에 의해 파라미터화된다. MLP의 탈락률은 0.1이며, IDM은 각 환경에 대한 \\(2\\times 10^{6}\\) 반복에 대해 학습된다. 다른 DWM 방법과 공정하게 비교하기 위해 DD는 DWM으로 \\(N=3\\) 내부 샘플링 단계를 사용한다. 우리는 DD와 다른 DWM 방법에 대해 동일한 범위의 평가 RTG\\(g_{\\text{eval}}\\)를 검색한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c} \\hline \\hline TD3+BC & & IQL & \\\\ \\hline policy noise & 0.2 & expectile & 0.7 \\\\ noise clip & 0.5 & \\(\\beta\\) & 3.0 \\\\ policy update frequency & 2 & max weight & 100.0 \\\\ target update frequency & 2 & policy update frequence & 1 \\\\ \\(\\alpha\\) & 2.5 & advantage normalization & False \\\\ EMA \\(w\\) & 0.005 & EMA \\(w\\) & 0.005 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 D.1: TD3+BC 및 IQL을 훈련시키기 위한 하이퍼파라미터.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '변압기 기반 세계 모델을 이용한### 알고리즘 3.1\n' +
      '\n' +
      'DWM과 동일한 프로토콜에 따라, 트랜스포머 모델은 초기 상태-액션 쌍 상에서 컨디셔닝된 미래의 상태-보상 시퀀스들을 예측하도록 트레이닝된다. Zheng et al.(2022)과 유사한 4개의 어텐션 헤드를 갖는 4-레이어 트랜스포머 아키텍처를 사용한다. 특히, 첫 번째 동작을 제외한 모든 동작은 상태-행동-보상 시퀀스에서 0으로 마스킹된다. 손실 함수가 액션 예측 에러만을 포함하는 원래의 DT(Chen et al., 2021)와는 구별되며, 여기서 트랜스포머는 상태 및 보상 예측 손실로 트레이닝된다. 트랜스포머는 ODT(Zheng et al., 2022)에 따라 최적화기 및 하이퍼파라미터로 트레이닝된다. 트랜스포머에 대한 RTG는 호퍼, 워커2d 및 하프치타 환경에 대해 각각 9.0,5000/550\\approx 9.1,6000/1200\\5.0\\의 값을 취한다. 상기 제어부는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline  & \\multicolumn{3}{c}{Return (mean\\(\\pm\\)std)} \\\\ \\cline{2-4} Env. & Simulation Horizon & DWM-IQL. & DWM-TD3BC \\\\ \\hline \\multirow{4}{*}{**hopper-medium-v2**} & 1 & \\(0.54\\pm 0.11\\) & \\(0.68\\pm 0.12\\) \\\\  & 3 & \\(0.55\\pm 0.10\\) & \\(0.63\\pm 0.11\\) \\\\  & 7 & \\(0.56\\pm 0.09\\) & \\(0.66\\pm 0.13\\) \\\\  & 15 & \\(0.58\\pm 0.12\\) & \\(0.77\\pm 0.15\\) \\\\  & 31 & \\(0.61\\pm 0.11\\) & \\(0.79\\pm 0.15\\) \\\\ \\hline \\multirow{4}{*}{**walker2d-medium-v2**} & 1 & \\(0.65\\pm 0.23\\) & \\(0.56\\pm 0.13\\) \\\\  & 3 & \\(0.74\\pm 0.11\\) & \\(0.74\\pm 0.13\\) \\\\  & 7 & \\(0.71\\pm 0.13\\) & \\(0.74\\pm 0.11\\) \\\\  & 15 & \\(0.66\\pm 0.15\\) & \\(0.73\\pm 0.13\\) \\\\  & 31 & \\(0.67\\pm 0.20\\) & \\(0.75\\pm 0.12\\) \\\\ \\hline \\multirow{4}{*}{**halfcheetah-medium-v2**} & 1 & \\(0.44\\pm 0.01\\) & \\(0.35\\pm 0.03\\) \\\\  & 3 & \\(0.44\\pm 0.01\\) & \\(0.39\\pm 0.01\\) \\\\  & 7 & \\(0.44\\pm 0.01\\) & \\(0.40\\pm 0.01\\) \\\\  & 15 & \\(0.44\\pm 0.02\\) & \\(0.40\\pm 0.01\\) \\\\  & 31 & \\(0.44\\pm 0.01\\) & \\(0.40\\pm 0.01\\) \\\\ \\hline \\multirow{4}{*}{hopper-medium-replay-v2} & 1 & \\(0.18\\pm 0.06\\) & \\(0.52\\pm 0.21\\) \\\\  & 3 & \\(0.37\\pm 0.18\\) & \\(0.44\\pm 0.23\\) \\\\  & 7 & \\(0.39\\pm 0.14\\) & \\(0.52\\pm 0.28\\) \\\\  & 15 & \\(0.37\\pm 0.18\\) & \\(0.67\\pm 0.25\\) \\\\  & 31 & \\(0.37\\pm 0.15\\) & \\(0.59\\pm 0.22\\) \\\\ \\hline \\multirow{4}{*}{**walker2d-medium-replay-v2**} & 1 & \\(0.32\\pm 0.15\\) & \\(0.13\\pm 0.02\\) \\\\  & 3 & \\(0.27\\pm 0.24\\) & \\(0.19\\pm 0.10\\) \\\\  & 7 & \\(0.25\\pm 0.20\\) & \\(0.22\\pm 0.14\\) \\\\  & 15 & \\(0.26\\pm 0.19\\) & \\(0.22\\pm 0.10\\) \\\\  & 31 & \\(0.27\\pm 0.19\\) & \\(0.17\\pm 0.12\\) \\\\ \\hline \\multirow{4}{*}{**halfcheetah-medium-replay-v2**} & 1 & \\(0.38\\pm 0.05\\) & \\(0.02\\pm 0.00\\) \\\\  & 3 & \\(0.39\\pm 0.02\\) & \\(0.17\\pm 0.05\\) \\\\  & 7 & \\(0.39\\pm 0.02\\) & \\(0.22\\pm 0.03\\) \\\\  & 15 & \\(0.38\\pm 0.03\\) & \\(0.26\\pm 0.03\\) \\\\  & 31 & \\(0.37\\pm 0.03\\) & \\(0.26\\pm 0.05\\) \\\\ \\hline \\multirow{4}{*}{**hopper-medium-expert-v2**} & 1 & \\(0.86\\pm 0.25\\) & \\(0.88\\pm 0.17\\) \\\\  & 3 & \\(0.90\\pm 0.19\\) & \\(0.94\\pm 0.22\\) \\\\  & 7 & \\(0.88\\pm 0.28\\) & \\(0.93\\pm 0.24\\) \\\\  & 15 & \\(0.85\\pm 0.20\\) & \\(0.91\\pm 0.19\\) \\\\  & 31 & \\(0.84\\pm 0.23\\) & \\(0.93\\pm 0.23\\) \\\\ \\hline \\multirow{4}{*}{**walker2d-medium-expert-v2**} & 1 & \\(0.80\\pm 0.22\\) & \\(0.74\\pm 0.21\\) \\\\  & 3 & \\(1.02\\pm 0.09\\) & \\(0.89\\pm 0.13\\) \\\\ \\cline{1-1}  & 7 & \\(0.98\\pm 0.2\\) & \\(0.82\\pm 0.19\\) \\\\ \\cline{1-1}  & 15 & \\(1.06\\pm 0.05\\) & \\(0.84\\pm 0.14\\) \\\\ \\cline{1-1}  & 31 & \\(1.05\\pm 0.06\\) & \\(0.87\\pm 0.03\\) \\\\ \\hline \\multirow{4}{*}{**halfcheetah-medium-expert-v2**} & 1 & \\(0.60\\pm 0.18\\) & \\(0.39\\pm 0.01\\) \\\\ \\cline{1-1}  & 3 & \\(0.52\\pm 0.14\\) & \\(0.43\\pm 0.07\\) \\\\ \\cline{1-1}  & 7 & \\(0.63\\pm 0.13\\) & \\(0.44\\pm 0.03\\) \\\\ \\cline{1-1}  & 15 & \\(0.66\\pm 0.14\\) & \\(0.50\\pm 0.08\\) \\\\ \\cline{1-1}  & 31 & \\(0.65\\pm 0.17\\) & \\(0.49\\pm 0.09\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 E.1: DWM-TD3BC 및 DWM-IQL에 대해 상이한 시뮬레이션 지평을 갖는 정규화된 리턴들의 비교. 보고된 값은 서로 다른 RTG 값(표 D.2에 나열됨)에서 가장 우수한 성능이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n' +
      '### 추가 실험: RTG Relabeling과 모델 미세조정\n' +
      '\n' +
      '전통적인 RL의 동적 프로그래밍과 달리 확산 모델 및 DT와 같은 순차적 모델링 방법은 차선책 궤적을 스티칭하지 못하는 것으로 의심된다. RTG 재라벨링은 훈련 데이터세트로부터 RTG \\(g\\)를 반복적으로 재라벨링함으로써 DT에 대한 이러한 문제를 완화하기 위해 제안된다(Yamagata 등, 2023).\n' +
      '\n' +
      '\\[\\tilde{g}_{t}=r_{t}+\\gamma\\max(g_{t+1},\\widehat{V}(s_{t+1}))=\\max(g_{t},r_{t}+\\widehat{V}(s_{t+1}), \\tag{18}\\]\n' +
      '\n' +
      '그림 E.4: RTG가 다른 각 예측 타임스텝에서 DWM의 고장 예측 오류. DWM은 \\(T=32\\)과 \\(K=10\\)으로 훈련된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:26]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:27]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c c c|c c c|c c c|} \\hline \\multicolumn{6}{|c|}{**Diffusion World Model**} \\\\ \\hline \\hline \\multicolumn{6}{|c|}{**hopper-medium-v2**} & \\multicolumn{3}{|c|}{**hopper-medium-replay-v2**} & \\multicolumn{3}{|c|}{**hopper-medium-expert-v2**} \\\\ \\hline Simulation Horizon & RTG & Return (Mean \\(\\pm\\) Std) & Simulation Horizon & RTG & Return (Mean \\(\\pm\\) Std) & Simulation Horizon & RTG & Return (Mean \\(\\pm\\) Std) \\\\ \\hline\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:29]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>