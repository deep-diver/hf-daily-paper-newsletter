<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GiT: Universal Language Interface를 통한 Generalist Vision Transformer의 방향\n' +
      '\n' +
      'Haiyang Wang\n' +
      '\n' +
      '동등한 기부금 1Peking University 2Max Planck Informatics 연구소\n' +
      '\n' +
      '({}^{3}\\) 중국 홍콩대학, 선전\n' +
      '\n' +
      '({}^{4}\\) ETH Zurich 5. 홍콩 중국대학\n' +
      '\n' +
      '{wanghaiyang@stu, tanghao@stu, wanglw@cis}.pku.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 하탕\n' +
      '\n' +
      '동등한 기부금 1Peking University 2Max Planck Informatics 연구소\n' +
      '\n' +
      '({}^{3}\\) 중국 홍콩대학, 선전\n' +
      '\n' +
      '({}^{4}\\) ETH Zurich 5. 홍콩 중국대학\n' +
      '\n' +
      '{wanghaiyang@stu, tanghao@stu, wanglw@cis}.pku.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 리장\n' +
      '\n' +
      'Shaoshuai Shi\n' +
      '\n' +
      'Junshil@cuhk.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 무하마드 페르자드 내엠\n' +
      '\n' +
      'Hongsheng Li\n' +
      '\n' +
      '동등한 기부금 1Peking University 2Max Planck Informatics 연구소\n' +
      '\n' +
      '({}^{3}\\) 중국 홍콩대학, 선전\n' +
      '\n' +
      '({}^{4}\\) ETH Zurich 5. 홍콩 중국대학\n' +
      '\n' +
      '{wanghaiyang@stu, tanghao@stu, wanglw@cis}.pku.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 번트 쉴레\n' +
      '\n' +
      'Junshil@cuhk.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 왕리위\n' +
      '\n' +
      'junshil@cuhk.edu.cn\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 1: 각주:\n' +
      '\n' +
      '각주 2: 각주:\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이 논문은 바닐라 ViT만으로 다양한 비전 작업에 동시에 적용할 수 있는 간단하면서도 효과적인 프레임워크인 GiT를 제안한다. 대규모 언어 모델(LLM)에서 널리 사용되는 Multi-layer Transformer Architecture(_e.g._, GPT)의 보편성에 착안하여, 강력한 비전 기반 모델(VFM)의 역할을 할 수 있도록 그 범위를 넓히고자 한다. 그러나 언어 모델링과 달리 시각적 작업은 일반적으로 검출을 위한 바운딩 박스 헤드 및 세그먼트화를 위한 픽셀 디코더와 같은 특정 모듈을 요구하여 비전 도메인에서 강력한 다층 변압기의 적용을 크게 방해한다. 이를 해결하기 위해, 우리는 이미지 수준 이해(_e.g._ captioning), 희소 인식(_e.g._ detection), 밀집 예측(_e.g._ segmentation)에 이르기까지 다양한 시각적 작업을 능숙하게 통합할 수 있도록 성공적인 자동 회귀 디코딩을 지원하는 범용 언어 인터페이스를 설계한다. 위의 디자인을 기반으로 전체 모델은 특별한 추가 없이 ViT만으로 구성되어 놀라운 아키텍처 단순화를 제공한다. GiT는 다중 작업 시각적 모델로, 작업별 미세 조정 없이 5개의 대표적인 벤치마크에 걸쳐 공동으로 훈련된다. 흥미롭게도, 우리의 GiT는 일반주의 성과에서 새로운 벤치마크를 구축하고, 과제 전반에 걸쳐 상호 향상을 촉진하여 고립된 훈련에 비해 상당한 개선을 이끌었다. 이는 LLM에서 관찰된 유사한 영향을 반영한다. 27개의 데이터 세트로 교육을 더욱 풍부하게 하는 GiT는 다양한 작업에 걸쳐 강력한 제로샷 결과를 달성합니다. 이 패러다임은 단순한 설계로 인해 시각과 언어의 구조적 격차를 좁힐 수 있는 가능성을 가지고 있다. 코드 및 모델은 [https://github.com/Haiyang-W/GiT](https://github.com/Haiyang-W/GiT)에서 사용할 수 있다.\n' +
      '\n' +
      '키워드: 통합 비주얼 모델링 멀티태스크 학습\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간의 의도에 부합하는 다양한 과제를 완성할 수 있는 보편적 모델을 개발하는 것은 머신 러닝의 오랜 목표이다. 언어 처리에서 LLMs[1, 69, 82, 102]의 출현은 최소한의 프롬프트로 적응 가능한 작업 관리를 위해 여러 개의 적층된 변압기 층만을 사용하는 유망한 경로를 열어준다. 본 논문에서는 시각 모델링에서 단순 다층 트랜스포머[84] 아키텍처를 탐색하고, 수많은 비전 작업을 범용 언어 인터페이스와 원활하게 통합하여 시각과 언어의 아키텍처 격차를 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '머신 러닝 커뮤니티는 방대한 데이터에 대해 훈련된 기초 모델(_예: GPT[9], BERT[43], DALL-E[71])의 등장으로 패러다임의 변화를 겪고 있으며, 개념 지식의 공유가 가능하고 다양한 다운스트림 작업에 대한 원활한 적응성을 제공한다. 언어 모델 [9, 43, 82]는 균질화된 표현(_i.e._, 입력 및 출력이 토큰 시퀀스로서 균일하게 표현됨) 덕분에, 최근에 이것으로부터 큰 혜택을 받았다. GPT4[65], LLaMA[82], PaLM2[1] 및 Gemini[81]과 같은 최첨단 모델은 인간의 지시를 따르고 개방형 작업을 해결하는 전례 없는 능력을 보여주었다. 그들의 성공 덕분에, 이 아키텍처는 잠재적으로 NLP를 넘어 다른 기계 학습 작업에 대한 일반적인 프레임워크로 간주됩니다.\n' +
      '\n' +
      '이 기회를 통해 커뮤니티는 오픈 소스 LLMs[80, 70, 82]의 외국어로 비전 기능[30, 38]을 활용하여 LLaVA[55], Unified-IO[59] 및 OFA[88]와 같은 여러 대형 비전 모델을 개발했다. 그러나, 이러한 진행은 다양한 시각적 인코더[88, 104], 지각 헤드[49], RPN[49] 및 특정 목표 표현[59]을 포함하는 태스크-특정 설계를 여전히 유지했다. 작업별 모듈은 모델이 해결해야 하는 각 작업에 대해 복잡한 설계를 요구하여 잠재적으로 일반적인 비전 모델로의 진행을 방해한다. 더욱이, 이러한 태스크-특정 설계들은 전형적으로 다수의 개별 트레이닝 스테이지들[89]을 수반하여, 상이한 태스크들에 걸친 모델 스케일링을 복잡하게 한다. 우리는 대체 범용 프레임워크가 보다 보편적인 입력-출력 인터페이스를 통해 경량 컴포넌트를 사용할 수 있으며 대부분의 모델 리소스를 이러한 작업에 걸쳐 일반 모델을 학습하는 데 할당할 수 있다고 주장한다.\n' +
      '\n' +
      '대형 시각적 모델링에 대한 이전의 시도[3, 7, 28, 51, 55, 90, 104]는 주로 이미지 수준의 비전 언어 도메인에 초점을 맞췄는데, 이는 주로 이미지를 외국어로 봄으로써 LLM에 쉽게 통합되었기 때문이다. 이 접근법은 종종 검출 및 분할과 같은 고전적 인식 과제의 통합을 간과한다. 기본적인 시각 인식을 위한 통합된 프레임워크를 개발하는 것은 모델이 서로 다른 형식을 가진 여러 출력을 병렬로 예측해야 하기 때문에 상당히 어려운 것으로 입증되었으며, 주석은 거친 이미지 레벨에서 미세한 픽셀 레벨에 이르기까지 표현에서 광범위하게 다양하다. 예를 들어, 검출은 바운딩 박스의 가변 수를 산출하고, 세그먼테이션은 이진 마스크를 생성하고, 이미지 캡셔닝은 텍스트 답변을 생성한다. 이러한 단점들은 모든 시각적 작업들에 걸쳐 동시에 적용가능한 단일 모델을 설계하는 것을 어렵게 한다.\n' +
      '\n' +
      'LLM[4, 9, 64, 65]의 최근 발전은 트랜스포머[84]가 범용 계산 아키텍처로서의 가능성을 보여주었다. 이에 영감을 받아 다양한 비전 중심 업무를 처리할 수 있는 비전 기반 모델인 GiT를 소개합니다. 그림 1에서 볼 수 있듯이 이전 통합 모델 [88, 59, 89]와 비교하여 본 방법은 패치 투영 레이어 이외의 비전별 추가 없이 몇 개의 트랜스포머 레이어만 포함하는 미니멀리즘 설계를 특징으로 하며 LLM 아키텍처와 밀접하게 정렬된다. 언어 모델링과 유사하게 모든 시각적 작업은 보편적인 언어 인터페이스를 통해 자동 회귀 프레임워크로 구성된다. 구체적으로, 우리의 타겟들은 여분의 토큰들을 포함하지 않고 표준 어휘에만 의존하여 통일된 표현을 사용하여 토큰 시퀀스들로 표현된다[72, 89]. 다양한 지각 척도에 걸쳐 다양한 시각적 작업과 호환될 수 있도록 작업 통합을 위한 유연한 다중 작업 템플릿을 소개합니다. 전체 영상을 그리드 샘플링에 의해 \\(N\\)개의 부영역으로 분할하고 효율적인 병렬 복호화를 통해 각 부영역을 동시에 처리한다.\n' +
      '\n' +
      '위의 설계는 작업별 미세 조정 없이 5개의 대표적인 벤치마크에 걸쳐 모델의 다중 작업 교육을 용이하게 한다. 표 3 및 4에서 볼 수 있듯이 공유 매개변수와 표현을 활용하여 우리의 모델은 강력한 일반주의적 결과를 달성하고 LLMs[4]의 다중 작업 기능을 반영한다. 중복 능력이 있는 업무는 상호 보완하여 별도의 교육을 통해 상당한 이득을 얻을 수 있다(더 많은 분석은 SS5.2 참조). 일반화 가능성을 더욱 향상시키기 위해 27개의 표준 시각 데이터 세트를 훈련에 통합(표 11 참조)하여 보이지 않는 데이터에 대한 강력한 제로 및 소수 샷 성능을 생성했다.\n' +
      '\n' +
      '특히, 우리의 작업은 다음과 같은 기여를 한다:\n' +
      '\n' +
      '*_Unified Visual Modeling을 위한 Baseational framework._ 간단한 다층 변압기를 사용하여 간단한 시각적 모델링 패러다임을 도입하여 모델 설계를 크게 단순화합니다. 우리의 모델은 다양한 비전 중심 작업, 특히 종종 무시되는 객체 및 픽셀 레벨 작업을 효율적인 범용 언어 인터페이스를 통해 통합한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c c c|c c c} \\hline \\hline  & \\multicolumn{2}{c|}{Example} & \\multicolumn{4}{c|}{Size} & \\multicolumn{2}{c|}{Input Modalities} & \\multicolumn{2}{c}{Output Modalities} \\\\  & \\multicolumn{2}{c|}{Sources} & \\multicolumn{2}{c|}{Dataset} & Size & \\multicolumn{2}{c|}{Percent} & Weight & \\multicolumn{1}{c}{Text} & \\multicolumn{1}{c|}{Image} & \\multicolumn{1}{c}{Text} & \\multicolumn{1}{c|}{Sparse} & \\multicolumn{1}{c}{Dense} \\\\ \\hline\n' +
      '**Image-Level** &\\multicolumn{2}{c|}{**10**} & **11.4m:** & **67.1** & **40** & ✓ & ✓ & ✓ & ✓ & \\(\\cdot\\\\) Image Captioning & _CC12M [14], VG [46], SBU [66] & 5 & 11.3m & 66 & 30 & \\(\\cdot\\\\) Visual Grounding & _Re/COCO [100], Flex:Mk [68] & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ &\n' +
      '*ObjectLevel** &\\multicolumn{2}{**11**} & **5.2m:** & **30.9** & **40** & - & ✓ & ✓ & ✓ & ✓ \\ Object Detection & _Object-365 [75], COCO [54] & 8 & 3.8m & 22.6 & 20 & ✓ & ✓ & ✓ & ✓ & ✜ 인스턴스 세그멘테이션 & _OpenImages [88], LYS [35] & 4 & 1.4m & 7.9 & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ &\n' +
      '***Pixel-Level** &\\multicolumn{2}{**6**} & **6** & **322k** & **2.0** & - & ✓ & - & ✓\\Semantic Segmentation & _COCOSM [12], ADE20K [103 & **6** & 322k & 2.0 & 20 & ✓ & ✓ & ✓ & ✓ & ✓ &\n' +
      '**All Tasks** & \\multicolumn{2}{c|}{**27**} & **17m** & **100** & **100** & - & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 왼쪽에서 오른쪽까지의 열은 작업 소스 예제, 데이터 세트 카운트, 총 샘플, 백분율 및 다중 작업 샘플링 속도를 표시한 다음 작업 양식을 표시합니다. 하이라이트된 행은 유사한 작업 그룹에 대한 통계를 요약합니다. 전체 목록은 부록을 참조하십시오.\n' +
      '\n' +
      '* _LLMs._와 같은 멀티태스크 능력 가중치 공유 및 통합 학습 목표는 LLM에서 관찰된 바와 같이 다중 작업 능력을 얻을 수 있게 하여 5가지 벤치마크에 걸쳐 최고 및 상호 향상된 일반주의적 성능을 달성한다.\n' +
      '* _Strong generalizability._ LLM에서 사용되는 1단계 공동 훈련 전략을 완전히 수용하여, 우리의 모델은 27개의 공개적으로 사용 가능한 데이터 세트에 대해 훈련되어 다양한 작업에 걸쳐 강력한 제로 및 소수 샷 성능을 달성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '** 다층 트랜스포머**[84]는 보편적인 학습 아키텍처로 등장하여 대부분의 LLM 프레임워크의 초석이 되었다. GPT 시리즈[69, 4, 64, 65, 4, 9]뿐만 아니라 LLaMA[82], PaLM[1] 및 OPT[102]와 같은 주목할만한 LLM은 이 도메인에서 상당한 발전을 이루었다. 언어 외에도, 일반 변압기는 ViT[30], DSVT[85], UniTR[86]의 멀티모달 이미징을 통한 2D 비전에서도 효과적인 것으로 입증되었다. 성공에도 불구하고, 이러한 간단한 변압기는 종종 특징 인코딩에 제한되고 작업별 모듈을 요구하여 일반 학습자로의 진행을 크게 방해한다. 이를 해결하기 위해 기존의 인코더 전용 기능을 넘어 LLM과 같은 시각적 모델링으로 넘어 다층 트랜스포머의 범위를 넓히는 것을 목표로 한다. 이 모델은 범용 언어 인터페이스를 가진 다양한 시각적 작업을 위해 여러 개의 트랜스포머 레이어를 사용하여 시각과 언어 사이의 구조적 격차를 좁힌다.\n' +
      '\n' +
      '**비전 기초 모델**은 통일된 아키텍처 프레임워크 내에서 다양한 시각적 작업을 처리하는 데 탁월합니다. NLP에서 seq2seq 모델의 성공에 힘입어 OFA[88], 플라밍고[3], LLaVA[55] 및 가토[72]와 같은 혁신은 시퀀스 생성 문제로 비전 언어 작업을 재구성했으며, 이는 더 많은 작업에 걸쳐 공간 정보를 처리하기 위해 Unified-IO[59], Pix2Seq v2[22], VisionLLM[89]에 의해 추가로 개발되었다. 그러나, 이러한 접근법들은 비-병렬 디코딩(22)으로부터의 비효율적인 추론 또는 비전-특정 추가들의 복잡성(49, 59, 89)과 같은 도전들에 직면하여, 범용 비전 모델로의 진행을 늦춘다. 더욱이, 그들은 종종 LLM의 다중 작업 능력이 부족하며, 여기서 합동 훈련은 개별 훈련에 비해 우수한 성능을 산출한다.\n' +
      '\n' +
      '## 3 유니버설 언어 인터페이스\n' +
      '\n' +
      '이 섹션에서는 이미지, 오버 오브젝트에서 픽셀 레벨까지 5가지 기본 시각적 작업을 통합하는 간단한 범용 언어 인터페이스를 제안한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c|c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Multi-Modal Tokinavers} & \\multicolumn{2}{c|}{Multi-layer} & \\multicolumn{1}{c|}{\\multirow{2}{*}{Layer}} & \\multirow{2}{*}{Total} \\\\  & & & Image & Out-of-vocabulary & & Transfer & Number & Parameter \\\\ \\hline GPT \\({}_{\\text{max}}\\) & 0 & 0.4\\% & 1.8\\% & 97.8\\% & 18 (12:6) & 131M \\\\ GT \\({}_{\\text{Low}}\\) & 0 & 0.2\\% & 1.1\\% & 98.7\\% & 30 (24:6) & 387M \\\\ GT \\({}_{\\text{HST}}\\) & 0 & \\(<\\) 0.1\\% & 0.8\\% & 99.1\\% & 38 (32:6) & 756M \\\\ \\hline \\end{tabular} \\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Multi-Modal Tokinavers} & \\multicolumn{3}{c|}{Multi-layer} & \\multirow{2}{*}{Layer} & \\multirow{2}{*}{Total} \\\\  & & & Image & Out-of-vocabulary & & Transfer & Number & Parameter \\\\ \\hline GPT \\({}_{\\text{max}}\\) & 0 & 0.4\\% & 1.8\\% & 97.8\\% & 18 (12:6) & 131M \\\\ GT \\({}_{\\text{Low}}\\) & 0 & 0.2\\% & 1.1\\% & 98.7\\% & 30 (24:6) & 387M \\\\ GT \\({}_{\\text{HST}}\\) & 0 & \\(<\\) 0.1\\% & 0.8\\% & 99.1\\% & 38 (32:6) & 756M \\\\ \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c} \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Multi-Modal Tokinavers} & \\multicolumn{3}{c|}{Multi-layer} & \\multirow{2}{*}{Layer} & \\multirow{2}{*}{Total} \\\\  & & & Image & Out-of-vocabulary & & Transfer & Number & Parameter \\\\ \\hline GPT \\({}_{\\text{max}}\\) & 0 & 0.\n' +
      '\n' +
      '성공적인 자동 회귀 프레임워크에 삽입합니다. 우리의 모든 목표는 통일된 표현(SS3.1)을 통해 토큰 시퀀스로 표현된 다음 일반 다중 작업 템플릿(SS3.2)으로 구성되며, 이는 세밀한 시각적 인식을 일련의 병렬 디코딩된 하위 문제로 분할한다. 도 2는 세 가지 태스크, 즉 이미지 캡션화(이미지-레벨 태스크, 좌측), 객체 검출(객체-레벨 태스크, 중간) 및 시맨틱 세분화(픽셀-레벨 태스크, 우측)에 대한 멀티-태스크 입력 템플릿을 예시한다. 추가적인 기술적 세부사항들이 아래에 제공된다.\n' +
      '\n' +
      '### 통합 입력 및 출력 표현\n' +
      '\n' +
      '이미지, 언어, 경계 상자, 마스크, _etc_와 같은 다양한 모달리티를 지원하기 위해서는 이들을 통일된 공간에서 표현하는 것이 필수적이다. 이를 위해 입력 이미지와 텍스트를 패치 및 언어 토큰 임베딩에 직접 투영한다. 이에 따라, 모든 타겟들은 범용 언어 인터페이스를 통해 표현되고, 전적으로 표준 어휘에 기초하여 토큰화된다[92].\n' +
      '\n' +
      '**텍스트 표현**시각 언어 작업은 종종 이미지 캡션과 같은 텍스트 처리를 필요로 하며, 여기서 자연어 설명은 주어진 이미지에 기초하여 생성된다. 이를 처리하기 위해 BERT[43]의 관행을 따르며, 텍스트는 워드피스[92] 서브워드로 변환되고, 토큰 어휘는 \\(\\sim\\)30,000이며, 룩업 테이블을 통해 학습 가능한 임베딩 공간으로 임베딩된다. 위치 인코딩은 시간 단계 내의 로컬 위치를 나타내기 위해 추가된다.\n' +
      '\n' +
      '**out-of-vocabulary representation.** Visual perception은 일반적으로 객체 검출에 사용되는 카테고리 이름 및 수치인 "_traffic light_" 및 "_2047_"와 같은 다수의 조각으로 구성된 복잡한 텍스트 개념에 의존한다. [52; 89]에서 논의된 바와 같이, 이들을 나타내기 위해 다수의 토큰을 사용하는 것은 비효율적이다. 1) 카테고리를 식별하기 위해 \\(<\\)/c\\(>\\)"_트래픽 라이트_"\\(<\\)/c\\(>\\)과 같은 분리기를 추가하면 시퀀스 길이가 연장되며, 특히 밀집 예측 작업에 비실용적이다. 2) 다중 조각 단어에 대한 토큰 길이의 변화는 일관성 없는 디코딩 단계를 초래하여 신뢰할 수 있는 결과를 얻기 위해 복잡하고 규칙 기반 후처리를 필요로 한다. 이 문제를 해결하기 위해, 일부 솔루션[59; 72; 89]은 토큰 용량 제약을 고려할 때 도전에 직면하면서 카테고리 및 숫자 항의 새로운 토큰을 도입한다. 어휘를 확장하는 대신 멀티피스 개념을 연속 텍스트로 취급하여 다음과 같이 하나의 토큰으로 압축하고,\n' +
      '\n' +
      '그림 2: 태스크-레벨 커스터마이제이션은 이미지-에서 객체- 및 픽셀-레벨에 걸쳐 있으며, 실제 구현에서는 \\(N\\)을 1, 625(25\\(\\times\\)25) 및 1764(42\\(\\times\\)42)로 설정한다. 적색점은 격자점에서 이미지 쌍선형 보간법에 의해 생성된 국부화된 시각적 토큰을 의미한다. 작업 프롬프트는 텍스트, 텍스트 및 어휘 외 표현을 통해 토큰으로 변환됩니다.\n' +
      '\n' +
      '여기서 주의력(\\(\\cdot\\))은 단층 주의력, \\(\\text{TE}(\\cdot)\\) 및 \\(\\text{PE}(\\cdot)\\)은 텍스트 및 위치 임베딩 함수이다. 우리의 접근 방식은 기본 어휘를 확장하지 않고 어휘 외 용어를 처리할 수 있는 대체 솔루션을 제공하여 효과적인 인식을 달성하기 위한 후처리를 크게 단순화한다.\n' +
      '\n' +
      '**희소 표현.** 다양한 카테고리 및 위치 표현들(예를 들어, 바운딩 박스들 및 인스턴스 마스크들)을 생성하는 객체 검출[13, 34] 및 인스턴스 분할[37]과 같은 희소 객체-레벨 인식들의 맥락에서, 우리는 표준화된 출력 포맷을 제안한다. 이 형식은 tuple \\((C,P)\\)로 정의되며, 여기서 \\(C\\)은 카테고리 레이블을 나타내고, \\(P\\)=\\(\\{x_{i},y_{i}\\}_{i=1}^{N}\\)은 물체의 위치를 식별하는 \\(N\\) 점들의 집합을 나타낸다. 언어적 토큰의 형식에 맞게 클래스 및 위치 표적이 모두 사전 텍스트 및 어휘 외 표상에 의해 토큰화된다. VisionLLM[89]에 이어서, 연속 좌표는 [-범위, 범위] 내의 정수로 균일하게 이산화된다. 경계 상자는 좌상단 좌표와 우하단 좌표를 나타내는 \\(\\{x_{1},y_{1},x_{2},y_{2}\\}\\)의 4개의 점으로 공식화되고 인스턴스 마스크는 경계를 따라 여러 점을 통해 세립 영역을 정의한다[93, 94].\n' +
      '\n' +
      '**Dense representation.** 시맨틱 세분화[58, 74]와 같은 다양한 지각적 작업들은 종종 픽셀당 예측들을 수반하는 조밀한 출력들을 생성하기 위해 모델들을 필요로 한다. 이러한 작업을 처리하기 위해 픽셀당 레이블을 통일된 토큰으로 변환하는 것으로 시작합니다. 예를 들어, 의미 클래스[54]는 먼저 텍스트 및 어휘 외 표현에 의해 토큰화된다. 그런 다음, 이러한 조밀한 라벨은 iGPT[20]와 유사하게 자동으로 축소되는 래스터 순서로 1D 시퀀스로 평평해진다.\n' +
      '\n' +
      '** 이미지 표현** 이미지는 래스터 순서로 겹치지 않는 16 \\(\\times\\) 16 패치 시퀀스로 변환된 다음 ViT[30]에서 수행된 바와 같이 훈련 가능한 선형 투영 및 학습 가능한 위치 임베딩을 갖는 토큰에 임베딩된다.\n' +
      '\n' +
      '병렬 디코딩을 이용한### 멀티태스크 템플릿\n' +
      '\n' +
      '템플릿을 구성하기 전에 먼저 2D 시각적 이해를 지각적 입도와 출력 표현으로 정의되는 세 가지 별개의 범주로 나눈다. 우리의 초점은 훈련 및 분석을 위한 5가지 핵심 작업: 1) 이미지 캡션 및 시각적 접지로 예시된 _Image-level_ 작업들을 포함한다.\n' +
      '\n' +
      '그림 3: 우리의 다중 작업 공식화는 이미지 패치, 지시 언어 토큰 및 \\(N\\) 병렬 점 기반 하위 프로세스의 네 가지 사용자 입력을 처리하는 것으로 광범위하게 설명되며, 각각은 효율적인 병렬 시각적 예측을 위해 보간된 로컬 이미지 특징과 작업 식별자를 갖는다. 언어 인터페이스는 기본 어휘, 현재 태스크에서 요구하는 특정 어휘 목록, 태스크-진단적 어휘 외 모듈(§3.1)을 사용하여 각 태스크에 대한 어휘 세트를 동적으로 생성한다.\n' +
      '\n' +
      '2) Object Detection and Instance Segmentation과 같은 _Object-Level_ 태스크, 3) Semantic Segmentation과 같은 _Pixel-Level_ 태스크이다. 그런 다음 순수 시각에서 언어를 포함하는 다양한 작업 공식을 매끄럽게 통합하여 유연한 작업 커스터마이징을 가능하게 하는 통합 seq2seq 프레임워크를 소개한다.\n' +
      '\n' +
      '**일반 수식.** 잘 정립된 언어 모델에 영감을 받아 LLM의 널리 받아들여지는 명령어 템플릿을 비전 커뮤니티(예:_e.g._, 비전 언어 및 공간 인식 시각 인식)에 적용한다. 도 2 및 도 3에 도시된 바와 같이, 수업용 템플릿은 다음과 같이 정의되며,\n' +
      '\n' +
      '<\\text{Image}><\\text{Instruction}>}_{\\text{LocalFeature}_{1}><\\text{Task}_{1}>:<\\text{Response}_{1}>}_{\\vdots:\\vdots:\\vdots:\\underbrace{<\\text{LocalFeature}_{N}><\\text{Task}_{N}>:<\\text{Response}_{N}}}_{\\text{multi-track local observations and responses}}\\tag{2}}}\n' +
      '\n' +
      '템플릿에서 사용자 입력은 네 개의 세그먼트로 구성된다. 첫 번째는 ViT에서 수행된 이미지 패치로 구성된다. 두 번째는 시각적 접지에 사용되는 언어 표현과 같은 명령어 입력을 포함한다. 세 번째 세그먼트와 네 번째 세그먼트에 대해, 기존의 객체 검출에서와 같이 다수의 경계 박스를 동시에 예측하는 것과 같은 효율적인 객체 및 픽셀 수준의 시각적 인식을 대상으로, 그림 2와 같이 그리드 샘플링에 의해 태스크를 \\(N\\) 병렬 로컬 서브 프로세스로 분할한다. 각 서브 프로세스는 그리드 포인트 위치를 기반으로 이미지 특징을 쌍선형 보간하여 생성된 로컬 이미지 토큰과 텍스트 및 어휘 외 표현을 통해 단일 토큰으로 변환된 순수 텍스트 태스크 식별자와 함께 작동한다. 비젼 언어 태스크의 경우 \\(N\\)을 1로 설정하는 반면, 검출 및 분할과 같은 비전 중심 태스크의 경우 필요한 예측 해상도에 맞게 \\(N\\)을 조정할 수 있다. 이러한 설계를 통해 우리의 방법은 거의 모든 2D 비전 작업을 유연하게 처리할 수 있다. 특히, 일부 세그먼트들은 선택적으로 상이한 태스크들에 의해 요구되며, _예를 들어, 이미지 캡셔닝은 단지 이미지 입력들 및 태스크 프롬프트를 필요로 한다.\n' +
      '\n' +
      '기존의 인코더 및 디코더 설정과 달리, 우리는 토큰 표현 컨텍스트를 결정하기 위해 다양한 마스크 매트릭스를 사용한다. 도 3에 도시된 바와 같이, 우리의 방법은 일반적인 인코더와 유사하게 양방향 자기-어텐션을 적용하여 입력들(_i.e._, 이미지 및 명령어)을 처리한다. 중요하게도, 우리는 텍스트-컨디셔닝 이미지 프로세싱의 능력을 향상시키기 위해 이미지-대-텍스트 주의를 가능하게 한다(표 7 참조). 로컬 및 태스크 프롬프트를 계산하고 각 하위 프로세스의 목표 예측을 위해 디코더 전용 자기회귀 접근법에 따라 인과 관계를 모델링하기 위해 왼쪽에서 오른쪽으로의 단방향 주의를 사용한다.\n' +
      '\n' +
      '**이미지 레벨.** 이미지 캡션 및 시각적 접지와 같은 이미지 레벨 작업에 대한 정의는 간단하여 NLP 작업을 밀접하게 미러링한다. 이전 비전 언어 방법에 따라 \\(N\\)을 1로 설정하고 이미지 캡셔닝의 토큰 시퀀스를 {\\(<\\)image\\(>\\)"_image captioning_": \\(<\\)text\\(>\\)}, 시각적 접지를 {\\(<\\)image\\(>\\)instruction\\(>\\)"_visual grounding_": \\(<\\)bbox\\(>\\})으로 구성한다.\n' +
      '\n' +
      '**객체-레벨.** 객체 검출 및 인스턴스 분할을 포함하는 고전적인 객체-레벨 인식 태스크들을 능숙하게 관리하는 생성 프레임워크를 개발하는 것은 중요한 과제를 제시한다. 모든 경계 상자와 마스크를 동시에 생성할 수 있는 모델이 필요하다. 이를 해결하기 위해 그림 2와 같이 시각적 프롬프트 인식을 위해 설계된 점 기반 병렬 디코딩 프레임워크를 소개한다. 그것은 1120\\(\\times\\)1120 이미지에 대해 25\\(\\times\\) 25 샘플링 해상도에 해당하는 \\(N\\) 포인트의 그리드를 이미지 전체에 걸쳐 샘플링하는 것으로 시작한다. 이후, {\\(<\\)image\\(>\\)\\(<\\)local feature\\(>\\)\\(<\\)task identifier\\(>\\): \\(<\\)sparse response\\(>\\\\)}. \\(<\\)image\\(>\\)\\(<\\)local feature\\(>\\)task identifier\\(>\\): \\(<\\)sparse response\\(>\\\\)}. (<\\)image\\(>\\)는 모든 그리드 서브 프로세스가 공유하는 패치 토큰이다. \\ (<\\)sparse response\\(>\\)는 SS3.1에 자세히 설명된 대로 우리가 선택한 객체 수준 희소 표현을 나타내며, 특히 점이 음의 부분에 있으면 \\(<\\)background\\(>\\) 토큰이 예측된다.\n' +
      '\n' +
      '격자점 검출의 예로는 {\\(<\\)image\\(>\\)\\(<\\)local feature\\(>\\)"_object detection_": \\(<\\)c\\(>\\)x\\({}_{1}\\)\\(>\\)y\\({{1}\\)\\(>\\)y\\((<\\)x\\({}_{2}\\)\\(>\\)y\\((<\\)y\\({{2}\\)\\(>\\)를 나타내며, 여기서 \\(<\\)x\\(<\\)y\\({({}_{2}\\)은 클래스 레이블이고, (\\(<\\)x\\(<\\)y\\({({}_{2}\\)은 클래스 레이블이고, (\\(<\\)x\\(<\\)y\\({({}_{2}\\)은 클래스 레이블이고, (\\(<\\)x\\(<\\)y\\({(<\\)y\\({(<\\)y\\({2}\\)\\(>\\)\n' +
      '\n' +
      '**Pixel-Level.** 자동 회귀 디코딩 패러다임[9, 65, 69]은 특히 단일 시퀀스에서 모든 픽셀 의미 범주를 계산하는 것과 같은 경우에 고차원 출력에 어려움을 겪으며 상당한 계산 오버헤드를 발생시킨다. 이전의 노력들[59, 63]은 VQ-VAE[83]를 통해 압축된 토큰들을 사용하여 이를 완화하려고 시도했다. 그러나 이 접근법은 순수한 언어 인터페이스를 손상시키고 복잡한 모듈을 도입했다. 이 문제를 해결하기 위해 그림 4와 같이 픽셀당 레이블을 언어적 토큰으로 변환하고 이미지를 객체 수준 작업과 마찬가지로 \\(N\\) 균일한 하위 영역으로 더 나눈다. 구체적으로, 분할 작업의 경우, 672\\(\\times\\)672 크기의 영상에 대해 42\\(\\times\\)42 지각 해상도를 달성하기 위해 \\(N\\)을 1764로 설정하였으며, 각 하위 프로세스는 독립적으로 순차적인 픽셀 수준 예측을 병렬적으로 수행하여 효율성을 높였다.\n' +
      '\n' +
      '16개의 디코딩 단계를 갖는 단일 트랙에 대한 의미론적 분할의 예: {\\(<\\)image\\(>\\)\\(<\\)local feature\\(>\\)"_semantic segmentation_": \\(<\\)c\\({}_{1}\\)\\(>\\)\\(<\\)c\\({}_{2}\\)\\(>\\)\\(<\\)c\\({}_{2}\\)\\(>\\)\\(\\cdots\\)\\(<\\)c\\({}_{15}\\)\\(>\\))\\(>\\)c\\(<\\)c\\({}_{16}\\)\\(>\\) 이며, 여기서 \\(<\\)c\\({}_{i}\\)은 각 서브 영역의 \\(i\\)번째 클래스 토큰이다.\n' +
      '\n' +
      '## 4 Training\n' +
      '\n' +
      '###구조 : 다층변압기\n' +
      '\n' +
      '범용 언어 인터페이스를 사용하여 이산 입력 및 출력 토큰의 시퀀스로 다양한 2D 비전 태스크 배열을 공식화한다. 상기 방법은\n' +
      '\n' +
      '도 4: 픽셀-레벨 다중 병렬 디코딩의 예시. 16개의 패치로 분할된 64\\(\\times\\)64 이미지를 고려한다. 각 패치는 16\\(\\times\\)16이고, 서브 프로세스마다 16의 디코딩 단계를 거쳐 각 그리드 포인트는 하나의 패치를 커버하여 4\\(\\times\\)4 시맨틱 맵을 예측하고, 최종 결과를 위해 4\\(\\times\\)4의 시맨틱 맵을 원래의 크기로 업샘플링한다.\n' +
      '\n' +
      '대규모 언어 모델의 성공적인 아키텍처(예: 다층 트랜스포머 [9, 69, 84])를 통합된 시각적 모델링으로 확장할 수 있는 길을 열었습니다.\n' +
      '\n' +
      '시각적 기초를 기반으로 언어적 시퀀스와 고해상도 이미지 모두에 대해 SAM[45]에서 사용된 시각적 인코더와 동일한 윈도우 기반 ViT[30, 53]의 구조를 활용한다. 몇 개의 전역 주의 블록이 특징 전파를 위해 모델에 고르게 통합된다. 특히, 윈도우 주의 계층 내에서, 각각의 패치 토큰은 동일한 윈도우에 위치된 그리드 포인트들과만 상호작용한다. 우리의 접근 방식은 아키텍처 변경 없이 이러한 공통 구조(_i.e., ViT)를 기반으로 구축되어 프레임워크의 보편성을 향상시킬 수 있다.\n' +
      '\n' +
      '위의 설계로 인해, 우리의 아키텍처는 표 2와 같이 다양한 모달리티 입력에 대해 몇 개의 경량 모듈로 보완된 일반적인 추론에 대부분의 계산 매개변수(\\(>98\\%\\))를 할당할 수 있다.\n' +
      '\n' +
      '### 멀티태스크 및 유니버설 트레이닝\n' +
      '\n' +
      'GiT는 다양한 작업과 데이터셋에 걸쳐 합동 훈련을 받는다. 우리의 목표는 여러 작업을 동시에 처리할 수 있는 통합 모델의 능력을 평가하는 것입니다. 따라서, 우리는 과제 수행을 향상시킬 수 있는 가능성을 입증하는 이전 연구에도 불구하고 과제별 미세 조정을 삼간다.\n' +
      '\n' +
      '**다양한 작업 및 데이터세트.** 다양한 지각 및 V&L 작업에 대한 단일 통합 모델을 구축하기 위해 이미지 수준에서 픽셀 수준 시각적 이해에 걸쳐 이전에 식별된 5가지 기본 작업에 걸쳐 가장 대표적인 데이터세트로 구성된 분석 가능한 다중 작업 벤치마크를 구성한다. 모델의 적응성을 향상시키기 위해 표 11에 나열된 16개의 공개적으로 액세스할 수 있는 데이터 소스에서 27개의 데이터 세트를 통합하여 벤치마크를 강화한다.\n' +
      '\n' +
      '**공동 다중 작업 훈련.** 우리는 이러한 데이터 세트의 샘플을 혼합하여 위의 다중 작업 벤치마크에서 GiT를 공동으로 훈련한다. 표 11에 자세히 설명된 바와 같이, 공동 훈련 동안 더 작은 데이터로 가려지는 작업을 방지하고 잠재적인 성능 저하를 방지하기 위해 데이터 크기에 관계없이 모든 작업(1/5)에서 균일하게 샘플링한다. 작업이 여러 도메인에 걸쳐 있는 보편적인 설정에서 각 작업 내부의 샘플링은 일상 생활, 실내 및 실외와 같은 시나리오에 걸쳐 균형을 이룬다. 이러한 도메인 내에서 데이터 세트는 크기에 비례하여 샘플링된다.\n' +
      '\n' +
      '학습 목표와 관련하여, 다른 작업들은 별개의 어휘들을 필요로 한다. 예를 들어, 시각적 접지는 숫자 좌표를 사용하는 반면, 세그먼테이션은 의미 개념을 포함한다. 이 문제를 해결하기 위해 그림 3과 같이 태스크별 어휘를 사용하면서 표준 크로스엔트로피 손실을 사용하여 모든 태스크를 다음 토큰 생성 문제로 접근한다. 이를 통해 훈련 및 추론 단계 모두에서 각 작업의 고유한 요구 사항에 적응하면서 어휘 세트를 동적으로 제어할 수 있다.\n' +
      '\n' +
      '**스케일링 모델.** 우리는 SAM[45]과 유사한 ViT[30]의 변형을 채택하며, 비시각적 모달리티 프로세싱을 개선하기 위해 BERT[43]에서 사용된 6개의 여분의 트랜스포머 층 및 텍스트 임베딩으로 증강된다(표 9 참조). 모델 축척에 대한 성능 의존성을 연구하기 위해 ViT-B, -L, -H에 구축된 모델의 세 가지 크기를 표 2에 자세히 설명된 131M에서 756M 범위의 매개변수로 소개하고 초기 계층은 SAM으로 사전 훈련된 매개변수를 상속하고 새로운 계층은 무작위 초기화로 시작한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '**Multi-Task Datasets.** 심층 분석과 공정한 평가를 용이하게 하기 위해 분석 가능한 Multi-Task 벤치마크를 구축하여 각 태스크별로 가장 대표적인 데이터셋 중 하나를 선택하였다. 일관성을 보장하고 VisionLLM[89]과의 비교를 가능하게 하기 위해 객체 검출 및 인스턴스 분할을 위한 COCO2017[54], 이미지 캡셔닝을 위한 COCO 캡션[23], 시각적 접지를 위한 RefCOCO 시리즈[60, 100]의 4가지 비전 중심 작업에 사용된 동일한 데이터 세트를 유지했다. VisionLLM에 포함되지 않은 의미론적 분할을 위해 널리 사용되는 ADE20K 데이터셋[103]을 사용했다.\n' +
      '\n' +
      '**확장된 데이터세트.** 통합 프레임워크의 보편성을 보여주기 위해 시각 언어 및 시각 지각에서 더 표준적이고 공개적으로 이용 가능한 데이터세트를 통합하여 다중 작업 벤치마크를 강화했다(SS4.2 참조).\n' +
      '\n' +
      '**훈련 및 평가 세부사항** 모델의 유연성과 유효성을 설명하기 위해 단일 작업, 다중 작업 및 범용 설정의 세 가지 훈련 패러다임을 설정했다. 단일 작업 교육에서는 성능 최적화에 중점을 둡니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c} \\hline \\hline Methods &\n' +
      '\\begin{tabular}{c} Specific Modules \\\\ Examples \\\\ \\end{tabular} & \\multirow{2}{*}{\\(\\ast\\)Param} & \\multicolumn{3}{c|}{Object Detection} & \\multicolumn{3}{c|}{Instance Seg} & \\multicolumn{3}{c|}{Semantic Seg} & \\multicolumn{3}{c|}{Captaining} & \\multicolumn{1}{c}{Crounding} \\\\  & & & AP & AP\\({}_{30}\\) & AP\\({}_{71}\\) & AP & AP\\({}_{30}\\) & AP\\({}_{71}\\) & mIoUS & BLEU-4 CIDE & Acc00.5 \\\\ \\hline \\multicolumn{11}{l}{_Specialist Models_} \\\\ Faster R-CNN-FPN [73] & ResNet,RPN & 5 & 42M & 40.3 & 61.0 & 44.0 & - & - & - & - & - & - \\\\ DETH-DCS [13] & ResNet,Encoder & 5 & 42M & 43.3 & 63.1 & 45.9 & - & - & - & - & - & - \\\\ Deformable-DETR [106] & ResNet,Encoder & 5 & 40M & 43.4 & 64.7 & 49.0 & - & - & - & - & - & - \\\\ PaiZSeq [21] & ResNet,Encoder & 3 & 37M & 43.0 & 61.0 & 45.6 & - & - & - & - & - & - \\\\ Mask R-CNN [86] & ResNet,RPN & 6 & 44M & 41.0 & 61.7 & 44.9 & 37.1 & 58.4 & 401.1 & - & - & - & - \\\\ Polar Mask [93] & ResNet,FPN & 5 & 53M & - & - & - & 30.5 & 52.0 & 31.1 & - & - & - & - \\\\ Mask2FFormer [25] & ResNet,Decoder & 5 & 44M & - & - & - & 43.7 & - & - & 47.2 & - & - & - \\\\ VL-T5 [28] & Faster R-CNN & 3 & 440M & - & - & - & - & - & - & - & 34.5 & 116.5 & - \\\\ UNITER [28] & Faster R-CNN & 4 & 303M & - & - & - & - & - & - & - & - & - & 81.4 \\\\ MDETR [40] & RoBERTa,DETR & 6 & 188M & - & - & - & - & - & - & - & - & - & 86.8 \\\\ \\hline \\multicolumn{11}{l}{_Generalist Models (Pre-training + MultiTask-Training)_} \\\\ UnTish [92] & Encoders & 4 & 188M & - & - & - & - & - & - & - & - & 115.8 & 88.6 \\\\ PaiZSeq v2 [22] & VLT,Decoder & 2 & 132M & 46.5 & \\(\\star\\) & 38.2 & \\(\\star\\) & \\(\\star\\) & - & 34.9 & \\(\\star\\) & \\(\\star\\) \\\\ Unified-IO\\({}_{x}\\)[50] & VQ-VAE & 4 & 2.9B & - & - & - & - & - & - & \\(\\star\\) & 122.3 & \\(\\star\\) \\\\ Shixa-13B [17] & ViVi,Vetuma & 3 & 13B & - & - & - & - & - & - & - & \\(\\star\\) & 117.5 & 87.8 \\\\ \\hline \\multicolumn{11}{l}{_Generalist Models (Multi-Task-Training)_} \\\\ Uni-Percive [107] & None & 1 & 124M & - & - & - & - & - & - & - & 32.0 & \\(\\star\\) & \\(\\star\\) \\\\ Uni-Percive-Me [105] & None & 1 & 167M & - & - & - & - & - & - & 33.2 & \\(\\star\\) & \\(\\star\\) \\\\ Uni-Percive-V2 [49] & Mask ImSDv,Swin & 8 & 308M & 58.6 & \\(\\star\\) & \\(\\star\\) & 50.6 & \\(\\star\\) & - & - & 35.4 & 116.9 & \\(\\star\\) \\\\ VisionLLM-R50 [89] & Deform-DETR & 6 & 7B & 44.6 & 64.0 & 48.1 & 25.1 & 50.0 & 22.4 & - & 31.0 & 112.5 & 80.6 \\\\ \\hline GT-Baseline [108] & None & 1 & 131M & 45.1 & 62.7 & 40.1 & 31.4 & 54.8 & 31.2 & 47.7 & 33.7 & 107.9 & 83.3 \\\\ GT-Baseline [108] & None & 1 & 131M & 46.7 & 64.2 & 50.7 & 31.9 & 56.4 & 31.4 & 47.8 & 35.4 & 112.6 & 85.8 \\\\ \\hline \\multicolumn{11}{l}{_Improvement_} \\\\ GT-Baseline [108] & None & 1 & **7.6** & **47.5** & **45.6** & **45.6** & **45.6** & **45.2** & **40.1** & **47.7** & **47.7** & **47.7** & **42.5** \\\\ \\hline GT-Baseline [108] & None & 1 & 387M & 51.3 & 69.2 & 55.9 & 35.1 & 61.4 & 34.7 & 50.6 & 35.7 & 116.0 & 88.4 \\\\ GIT-Baseline [108] & None & 1 & 756M & **52.9** & **71.0** & **57.8** & **35.8** & **62.6** & **35.6** & **52.4** & **36.2** & **118.2** & **89.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 표준 시력 중심 벤치마크에 대한 결과. "단일 작업"은 각 작업에 대해 개별적으로 훈련된 모델을 말하는 반면, "다중 작업"은 선택된 모든 벤치마크에 걸쳐 공동으로 훈련된 모델을 나타낸다. “\\(\\star\\)”는 어떤 숫자도 보고되지 않았지만 모델이 작업을 수행할 수 있음을 나타낸다. "-"는 그 특정 업무에서 무능함을 의미한다. "\\(\\dagger\\)"는 일반주의 모델이 성능을 향상시키기 위해 이전의 과제별 모델을 내장했음을 나타낸다. GiT는 나열된 모든 비전 과제를 지원하는 최초의 일반주의 모델로 눈에 띄며, 과제별 적응 없이 경쟁 결과를 제공한다. [17, 49]에 이어, 태스크-특정 미세-조정이 있는 결과만을 보고하는 일부 일반 모델들, _e.g._, OFA[88] 및 X-디코더[108]는 포함되지 않는다. 우리는 1단계 다과제 일반 모델의 상위 1개 항목과 **굵은 글꼴로 합동 훈련 개선을 강조한다. 특정 모듈 카운트는 인덱스 기반 텍스트 토큰라이저와 같이 컴퓨팅이 아닌 것을 제외합니다.\n' +
      '\n' +
      '개별 벤치마크에서. 한편, 다중과제 훈련은 선택된 5개의 데이터셋에 걸쳐 일반 학습자의 발달을 목표로 한다. Uni-Perceiver v2[49]의 통찰력으로부터, 우리는 더 빠르고 안정적인 훈련을 위해 혼합되지 않은 샘플링 전략(_i.e._, 반복당 하나의 작업 샘플링)을 채택하지만, 우리의 프레임워크는 최근 연구에서 제안된 바와 같이 배치 내 혼합 전략[59, 107]과도 호환된다. 보편적 학습은 SS4.2에 소개된 27개의 포괄적인 벤치마크를 통합하기 위해 우리의 접근법을 확장한다. 모든 모델은 코사인 어닐링 스케줄이 있는 AdamW[44] 최적화기를 사용하여 초기 학습률을 0.0002로 설정하고 가중치 감쇠를 0.05로 설정하며, 범용적 설정의 가장 큰 모델은 320k 반복에 대해 96개의 NVIDIA A100 GPU에서 학습된다.\n' +
      '\n' +
      '모든 실험은 표준 프로토콜과 테스트 분할을 사용하여 선택된 데이터 세트에 대해 평가된다. 제한된 공간 때문에 더 자세한 내용은 부록에 나와 있습니다.\n' +
      '\n' +
      '### In-distribution Benchmarking\n' +
      '\n' +
      '다양한 비전 중심 과제에 대한 모델의 배포 내 성능을 평가하며, 이를 과제에 특화된 모델과 고급 일반 모델 모두와 비교한다. 그것은 오직 적층형 다층 변압기에만 의존하며, 명령 및 후처리 변경을 통해서만 다양한 작업에 적응한다.\n' +
      '\n' +
      '**전문 모델과의 비교.** 표 4의 잘 정립된 전문가 기준선과 단일 작업 모델을 비교한다. 본 모델은 동일한 프레임워크 내에서 다양한 비전 중심 작업을 개별적으로 수행할 수 있는 능력을 보여 전문 모델과의 성능 격차를 줄인다. 대부분의 태스크들에서 비교가능한 결과들을 달성한다(_e.g._, 검출: Deformable-DETR[106]의 45.1 대 45.4], 시맨틱 세분화: Mask2Former[25]의 47.7 대 47.2). 인스턴스 세그먼트화에서는 약간 성능이 떨어집니다. 폴리곤 기반 방법은 마스크 방식보다 결과가 낮은 경우가 많습니다. 이 모델은 폴리곤 기반 대표적인 방법인 PolarMask[93]에 대해 +0.9 개선되었습니다.\n' +
      '\n' +
      '특히, 보편적인 인터페이스를 유지하기 위해 우리의 방법은 최신 향상 기술을 사용하지 않고 기본 레이블 할당만 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{Specific Modulus} & \\multirow{2}{*}{\\(\\bullet\\) Param.} & Object Detection & Instance Seg & \\multicolumn{2}{c|}{Semantic Seg} & Captioning \\\\  & \\multicolumn{2}{c|}{Examples} & \\multicolumn{1}{c|}{Num} & & \\multicolumn{1}{c|}{Cityscapes [27]} & Cityscapes [27] & Cityscapes [27] & SUN RGB-D [78] & \\multicolumn{1}{c}{acaps [2]} \\\\ \\hline _Supervised_ & \\multicolumn{2}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\\\ Faster R-CNN-FPN [73] & ResNet,RPN & 5 & 42M & 40.3 & - & - & - & - \\\\ Mask R-CNN [36] & ResNet,RPN & 6 & 46M & 40.9 & 36.4 & - & - & - \\\\ DeepLabv3-[19] & ResNet,Decoder & 3 & 63M & - & - & 80.9 & * & - \\\\ Mask2Former [25] & ResNet,Decoder & 5 & 44M & - & - & 80.4 & * & - \\\\ TokenFusion [91] & Sugformer,YOLOS & 4 & - & - & - & * & 48.1 & - \\\\ \\hline \\multicolumn{7}{l}{_Zero-Shot Transfer_} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ GLIP-T [52] & Swin-Dread & 5 & 156M & 28.1\\({}^{\\dagger}\\) & - & - & - & - \\\\ Grounding DINO-T [56] & Swin-DINO & 6 & 174M & 31.5\\({}^{\\dagger}\\) & - & - & - & - \\\\ BLIP-2 (129M) [50] & VIT-G,Qformer & 4 & 12.1B & - & - & - & - & **15.8** \\\\ ReCo [71] & DeiT-SIN & 4 & 46M & - & - & 24.2 & * & - \\\\ XDcode-T [108] & FocalNet,Encoder & 4 & 165M & - & 16.0 & 47.3 & 34.5 & * \\\\ \\hline GT-B\\({}_{\\text{model}}\\) & None & 1 & 131M & 21.8 & 14.3 & 34.4 & 30.9 & 9.2 \\\\ \\hline GT-B\\({}_{\\text{model}}\\) & None & 1 & 131M & 29.1 & 17.9 & 56.2 & 37.5 & 10.6 \\\\ GT-L\\({}_{\\text{model}}\\) & None & 1 & 387M & 32.3 & **20.3** & 58.0 & 39.9 & 11.6 \\\\ GT-H\\({}_{\\text{model}}\\) & None & 1 & 766M & **34.1** & 18.7 & **61.8** & **42.5** & 12.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 제로 샷 결과. “\\(\\star\\)” 및 “-”는 표 4를 따른다. \\(\\dagger\\)는 mmdetection을 기반으로 재현된 성능이다[16]. "유니버사"는 §4.2에 자세히 설명된 바와 같이 더 광범위한 데이터 세트 배열을 포함하여 다중 작업 설정을 확장한다.\n' +
      '\n' +
      '실적 향상을 위한 거대한 방 예를 들어, 검출에 사용되는 라벨 할당은 Deformable-DETR[106]을 밀접하게 미러링한다. DINO의 대조적인 잡음 제거[101]와 같은 보다 진보된 전략을 채택하면 우리의 결과를 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '**일반주의 모델과의 비교** 일부 일반주의 모델 [17, 22, 59, 88]은 2단계 훈련 프로세스를 채택하여 처음에는 이미지-텍스트 쌍 또는 다양한 인식 데이터와 같은 대규모 작업 관련 데이터 세트를 활용하고 동일한 프레임워크 내에서 단일 또는 다중 작업 다운스트림 튜닝을 수행하여 성능을 향상시킨다. 우리의 GiT는 작업별 적응 없이 통합 모델링에 이어 직접 다운스트림 평가를 위한 모든 데이터를 혼합하는 LLM에서 대중화된 보다 도전적인 1단계 공동 훈련을 완전히 수용한다.\n' +
      '\n' +
      '표 4는 우리의 모델이 조밀한 예측을 능숙하게 관리할 뿐만 아니라 50\\(\\times\\) 더 적은 매개변수와 훨씬 간단한 프레임워크로 모든 작업에서 이전의 선두 일반 모델인 VisionLLM[89]보다 우수하다는 것을 보여준다.\n' +
      '\n' +
      '표 4,5,6은 모델을 스케일링하는 것이 멀티태스크, 제로 및 소수 샷 성능을 크게 향상시키며 때로는 지도 접근법과 일치하기도 함을 보여준다.\n' +
      '\n' +
      '다중 작업 용량에 대한 논의.** 표 4는 GiT-B\\({}_{text{multi-task}\\)이 GiT-B\\({}_{text{single-task}\\)보다 우수하다는 것을 보여주며, 5개의 표준 데이터 세트에 대한 합동 훈련 후 각 작업에서 눈에 띄는 개선을 보여준다. 표 3에서 관찰된 바와 같이, 멀티-태스크 트레이닝은 일반적으로 태스크들이 동일한 능력들을 공유하지만 그렇지 않으면 덜 효과적일 때 성능을 향상시킨다. 이러한 패턴은 검출, 시각적 접지 및 인스턴스 세그먼테이션에 걸친 공유 로컬화 능력에서 명확하게 관찰된다. 반대로, 시멘틱 세그멘테이션의 세밀한 밀집 예측과 인스턴스 세그멘테이션의 폴리곤 기반 회귀와 같은 전문 기술은 다중 작업으로 인해 큰 이득을 보지 못한다.\n' +
      '\n' +
      '### Out-of-distribution Analysis\n' +
      '\n' +
      '**Zero-Shot Transfer.** 대규모 다중 작업 훈련 후 GiT는 다양한 새로운 데이터 소스에 대해 쉽게 평가된다. 이 기능을 입증하기 위해 5가지 구성에 걸쳐 3개의 확립된 데이터 세트에 대해 제로 샷 평가를 수행하여 시각적 접지를 넘어 4개의 비전 작업을 해결했다. 이러한 평가는 SUN RGB-D[78]와 같은 실내 환경에서 다양한 컨텍스트에 걸쳐 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c|c|c|c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{Specific Modules} & \\multicolumn{2}{c|}{Medical Imaging@mDice} & \\multicolumn{2}{c|}{Remote Sensing@mIoU} & \\multicolumn{2}{c}{Human Centric@mAP} \\\\  & Examples & Num & DRIVE [79] & LowDA [87] & Potsdam [39] & WIDERFace [96] & DeepFashion [57] \\\\ \\hline \\hline \\multicolumn{1}{l|}{_Supervised_} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c}{} & \\\\ U-Net [74] & None & 1 & 81.4 & * & * & - & - \\\\ AerialFormer [95] & Encoder.Stem & 3 & - & 54.1 & 89.1 & - & - \\\\ RetinaFace [29] & ResNet,FPN & 5 & - & - & - & 52.3 & - \\\\ Mask R-CNN [36] & ResNet,RPN & 6 & - & - & - & * & 59.9 \\\\ \\hline \\hline \\multicolumn{10}{l}{_Few-Shot Transfer_} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c}{} \\\\ Faster RCNN [73] & ResNet,RPN & 4 & - & - & - & 25.4\\({}^{\\dagger}\\) & 14.9\\({}^{\\dagger}\\) \\\\ DeepLabV3 [18] & ResNet,APP & 3 & 32.1\\({}^{\\dagger}\\) & 20.3\\({}^{\\dagger}\\) & 24.2\\({}^{\\dagger}\\) & - & - \\\\ \\hline GIT-B\\({}_{\\text{multi-task}}\\) & None & 1 & 34.3 & 24.9 & 19.1 & 17.4 & 23.0 \\\\ \\hline GIT-B\\({}_{\\text{multi-task}}\\) & None & 1 & 51.1 & 30.8 & 30.6 & 31.2 & 38.3 \\\\ GIT-Luc\\({}_{\\text{multi-task}}\\) & None & 1 & 55.4 & 34.1 & 37.2 & 33.4 & 49.3 \\\\ GIT-B\\({}_{\\text{multi-task}}\\) & None & 1 & **57.9** & **35.1** & **43.4** & **34.0** & **52.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 아웃-분산 도메인의 샷 결과는 거의 없다. 범용 단계에서 미리 훈련된 가중치를 기반으로 이 실험을 수행한다. “\\(\\star\\)”, “-” 및 \\(\\dagger\\)는 표 5를 따른다.\n' +
      '\n' +
      '도시 풍경과 같은 야외 풍경[27], 노캡과 같은 일상 생활[2]. 시맨틱 분할 및 캡셔닝을 위한 mIoU와 SPICE[5], 객체 검출 및 인스턴스 분할을 위한 mAP를 보고한다.\n' +
      '\n' +
      '표 5에서 볼 수 있듯이 우리의 보편적인 모델은 거의 모든 작업에서 최상의 결과를 달성한다. 비교 가능한 파라미터로 GiT-B\\({}_{text{universal}}\\)는 시맨틱 분할에서 Cityscapes(+8.9)와 SUN RGB-D(+3.0)의 X-Decoder[108]를 능가하며, 인스턴스 분할과 객체 검출에서도 유사한 장점을 보인다. 모델을 확장하면 제로 샷 기능이 더욱 향상되어 감독 성능에 가깝습니다. BLIP-2 [50]은 미리 훈련된 언어 모델 및 광범위한 훈련 데이터(129M)와의 통합에 기인할 가능성이 있는 노캡에서 GiT-H보다 우수하다. 특히, 우리가 아는 한, GiT는 다양한 영역과 작업에 걸쳐 제로 샷 성능을 달성한 최초의 일반 모델이다.\n' +
      '\n' +
      '**Few-Shot Transfer.** GiT는 유통 외 데이터 소스에 대한 신속한 적응을 보여준다. 의료영상(_i.e._, DRIVE[79]), 원격탐사(_i.e._, LoveDA[87] 및 ISPRS[39]) 및 인간 중심 시나리오(_i.e._, WIDERFace[96] 및 DeepFashion[57])에서 5개의 데이터셋에 대한 종합적인 수샷 평가를 수행했다. 우리의 접근법은 N-way K-shot[32] 설정(_i.e._, K=5)을 따르고 지원 세트 [10]에서 미리 훈련된 모델을 직접 미세 조정한다.\n' +
      '\n' +
      '분할 분석에서 우리는 다중 작업 변형을 훈련하는 데 사용되는 데이터 세트(_i.e._, ADE20K)와 일치하는 DeeplabV3를 기준선으로 선택한다. 우리는 GiT 멀티태스크와 DeeplabV3 모두 수샷 설정에서 성능이 좋지 않다는 것을 관찰했다. 그러나, 대규모 범용 훈련 후, GiT-B\\({}_{text{universal}}\\)은 상당히 개선된 일반화를 보여준다. 이러한 경향은 탐지 작업에 반영되어 우리의 보편적인 모델 구조와 훈련 접근법이 일반화 능력을 크게 향상시킨다는 것을 강조한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**디코더 전용 아키텍처.** 우리의 모델은 인코더-디코더 프레임워크에 비해 장점이 잘 탐구되지 않았지만 GPT의 디코더 전용 설계를 따른다. GiT-B의 초기 12개 레이어를 대상 토큰을 제외한 이미지와 텍스트용 인코더로 변환했다. 표 8은 인코더-디코더 패러다임이 특히 -0.9 드롭을 갖는 시맨틱 세분화에서 다섯 가지 작업 모두에서 디코더 전용 모델을 저성능으로 수행함을 보여준다. 이는 대상 토큰을 처리하기 위해 더 많은 계층(18 대 6)을 할당하는 디코더 전용 모델 때문일 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c} \\hline \\hline Modality Experts & \\multicolumn{1}{c|}{Total Conditioning} & \\multicolumn{1}{c|}{Detection@AP} & \\multicolumn{1}{c|}{Ins Seg@AP} & \\multicolumn{1}{c|}{Sem Seg@mIoU(SS)} & \\multicolumn{1}{c|}{Caption@CIDE:} & \\multicolumn{1}{c}{Grounding@Acc(0.5)} \\\\ \\hline \\multirow{2}{*}{\\(\\checkmark\\)} & & 46.1 & 31.4 & 47.8 & 111.8 & 78.6 \\\\  & & 46.2 & 31.6 & 47.7 & 112.2 & 78.7 \\\\  & \\(\\checkmark\\) & 46.7 & 31.9 & 47.8 & 112.6 & 85.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 시각적 접지에서 멀티모달 학습 및 이미지 대 텍스트 어텐션을 위해 다중 FFN을 사용하는 GiT-B\\({}_{\\text{multi-task}}\\) 상의 모달리티 전문가의 제거 및 텍스트 컨디셔닝.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c} \\hline \\hline Methods & Enc Layer & \\multicolumn{1}{c|}{Dec Layer} & \\multicolumn{1}{c|}{Detection@AP} & \\multicolumn{1}{c|}{Ins Seg@AP} & \\multicolumn{1}{c|}{Sem Seg@mIoU(SS)} & \\multicolumn{1}{c|}{Caption@CIDE:} & \\multicolumn{1}{c}{Grounding@Acc(0.5)} \\\\ \\hline GiT-B\\({}_{\\text{multi-task}}\\) & 12 & 6 & 46.3 & 31.6 & 46.9 & 110.8 & 84.8 \\\\ GiT-B\\({}_{\\text{multi-task}}\\) & 0 & 18 & 46.7 & 31.9 & 47.8 & 112.6 & 85.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 인코더-디코더와 디코더 전용 아키텍처 사이의 절제 연구.\n' +
      '\n' +
      '**New Layers의 수.** 표 9는 단지 하나의 새로운 레이어를 추가하는 것이 이미지 입력과 언어 타겟 사이의 차이로 인해 mAP를 2.6만큼 향상시키면서 성능을 크게 향상시킬 수 있음을 보여준다. 더 많은 레이어를 포함하면 6개 레이어 이후에 이득이 평준화되는 등 계속해서 결과가 개선됩니다.\n' +
      '\n' +
      '**모달 전문가.** 모달 전문가로서 다중 FFN을 사용하는 것은 멀티모달 처리를 위해 일반적으로 사용되는 관행[6, 105]이지만, 표 7은 우리의 접근법에서 주목할 만한 성능 이득을 나타내지 않으며, 이는 증가된 파라미터 및 추론 지연으로 인해 이 설계를 배제하도록 유도한다.\n' +
      '\n' +
      '**텍스트 컨디셔닝.** 이미지 및 텍스트 입력을 통한 시각적 접지 작업에서 네트워크 포워딩 동안 이미지 대 텍스트 주의를 가능하게 한다. 표 7은 이 방법이 검출과 시각적 접지 작업 사이의 향상된 차별화 때문에 다중 작업 설정에서 성능을 현저하게 향상시킨다는 것을 보여준다. 이 두 작업은 별개의 이미지 스케일(_i.e._, 1120 및 224)에서 기능하며, 여기서 전자는 다수의 박스를 식별하는 것을 수반하고, 후자는 텍스트에 의해 안내되는 단일 박스를 생성하는 것을 수반한다. 더욱이, 이 접근법은 모델이 이미지-텍스트 관계를 캡처하는 것을 도울 수 있어서, 명령어-팔로우잉의 능력을 높일 수 있다.\n' +
      '\n' +
      '**축척법 분석** 그림 6은 매개변수 수에 대한 범용 모델의 배포 내 성능을 제시하여 모델 용량을 확장하면서 잠재적인 개선 사항에 대한 통찰력을 제공한다. 우리는 모든 작업의 주요 메트릭을 평균화하는 합성 점수를 기반으로 세 가지 모델 크기에 대한 성능 진행을 표시하며 일관된 토큰 수에서 규모가 증가함에 따라 상당한 이득을 보여준다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 범용 언어 인터페이스를 통해 다양한 시각적 작업을 통합하기 위해 바닐라 ViT만을 활용하는 단순하면서도 강력한 비전 기반 모델인 GiT를 소개한다. LLM에서 관찰된 바와 같이 다중 작업 능력을 미러링하는 GiT는 일반주의적 성능에서 새로운 벤치마크를 설정한다. 27개의 데이터 세트에 걸친 훈련으로, GiT는 공유 매개변수를 사용하여 다양한 도메인에 걸쳐 제로 및 소수의 샷 작업에서 탁월한 최초의 일반 모델이 되어 컴퓨터 비전에서 다층 변압기의 기본 역할을 보여준다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Aakanksha, C., Sharan, N., Jacob, D., Maarten, B., Gaurav, M., Adam, R., Paul, B., Won, C.H., Charles, S., Sebastian, G., et al.: Palm: Scaling language modeling with pathways. JMLR (2023)\n' +
      '* [2] Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., Anderson, P.: Nocaps: Novel object captioning at scale. In: ICCV (2019)\n' +
      '* [3] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. In: NeurIPS (2022)\n' +
      '* [4] Alec, R., Jeffrey, W., Rewon, C., David, L., Dario, A., Ilya, S., et al.: Language models are unsupervised multitask learners. OpenAI blog (2019)\n' +
      '* [5] Anderson, P., Fernando, B., Johnson, M., Gould, S.: Spice: Semantic propositional image caption evaluation. In: ECCV (2016)\n' +
      '* [6] Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O.K., Aggarwal, K., Som, S., Piao, S., Wei, F.: Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. In: NeurIPS (2022)\n' +
      '* [7] Ravishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., Tasirlar, S.: Introducing our multimodal models (2023), [https://www.adept.ai/blog/fuyu-8b](https://www.adept.ai/blog/fuyu-8b)\n' +
      '* [8] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)\n' +
      '* [9] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. In: NeurIPS (2020)\n' +
      '* [10] Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixe, L., Cremers, D., Van Gool, L.: One-shot video object segmentation. In: CVPR (2017)\n' +
      '* [11] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Long, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous driving. In: CVPR (2020)\n' +
      '* [12] Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context. In: CVPR (2018)\n' +
      '* [13] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: ECCV (2020)\n' +
      '* [14] Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In: CVPR (2021)\n' +
      '* [15] Chen, C., Borgeaud, S., Irving, G., Lespiau, J.B., Sifre, L., Jumper, J.: Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318 (2023)\n' +
      '* [16] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C.C., Lin, D.: MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)\n' +
      '* [17] Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)* [18] Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for semantic image segmentation. CVPR (2017)\n' +
      '* [19] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: ECCV (2018)\n' +
      '* [20] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., Sutskever, I.: Generative pretraining from pixels. In: ICML (2020)\n' +
      '* [21] Chen, T., Saxena, S., Li, L., Fleet, D.J., Hinton, G.: Pix2seq: A language modeling framework for object detection. In: ICLR (2022)\n' +
      '* [22] Chen, T., Saxena, S., Li, L., Lin, T.Y., Fleet, D.J., Hinton, G.E.: A unified sequence interface for vision tasks. NeurIPS (2022)\n' +
      '* [23] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)\n' +
      '* [24] Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: Uniter: Universal image-text representation learning. In: ECCV (2020)\n' +
      '* [25] Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention mask transformer for universal image segmentation. In: CVPR (2022)\n' +
      '* [26] Cho, J., Lei, J., Tan, H., Bansal, M.: Unifying vision-and-language tasks via text generation. In: ICML (2021)\n' +
      '* [27] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene understanding. In: CVPR (2016)\n' +
      '* [28] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning. NeurIPS (2023)\n' +
      '* [29] Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: Retinaface: Single-shot multi-level face localisation in the wild. In: CVPR (2020)\n' +
      '* [30] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)\n' +
      '* [31] Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. IJCV (2010)\n' +
      '* [32] Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation of deep networks. In: ICML. PMLR (2017)\n' +
      '* [33] Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale adversarial training for vision-and-language representation learning. NeurIPS (2020)\n' +
      '* [34] Girshick, R.: Fast r-cnn. In: ICCV (2015)\n' +
      '* [35] Gupta, A., Dollar, P., Girshick, R.: Lvis: A dataset for large vocabulary instance segmentation. In: CVPR (2019)\n' +
      '* [36] He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)\n' +
      '* [37] He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask r-cnn. In: ICCV (2017)\n' +
      '* [38] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)\n' +
      '* [39] III/4, I.W.: ISPRS 2D Semantic Labeling Contest, [https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx](https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx)\n' +
      '* [40] Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr-modulated detection for end-to-end multi-modal understanding. In: ICCV (2021)\n' +
      '* [41] Karpathy, A., Fei-Fei, L.: Deep visual-semantic alignments for generating image descriptions. In: CVPR (2015)* [42] Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to objects in photographs of natural scenes. In: EMNLP (2014)\n' +
      '* [43] Kenton, J.D.M.W.C., Toutanova, L.K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: NAACL-HLT (2019)\n' +
      '* [44] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2015)\n' +
      '* [45] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: ICCV (2023)\n' +
      '* [46] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV (2017)\n' +
      '* [47] Kuhn, H.W.: The hungarian method for the assignment problem. NRL (1955)\n' +
      '* [48] Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., et al.: The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV (2020)\n' +
      '* [49] Li, H., Zhu, J., Jiang, X., Zhu, X., Li, H., Yuan, C., Wang, X., Qiao, Y., Wang, X., Wang, W., et al.: Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. In: CVPR (2023)\n' +
      '* [50] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ICML (2023)\n' +
      '* [51] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: ICML (2022)\n' +
      '* [52] Li, L.H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J.N., et al.: Grounded language-image pre-training. In: CVPR (2022)\n' +
      '* [53] Li, Y., Mao, H., Girshick, R., He, K.: Exploring plain vision transformer backbones for object detection. In: ECCV (2022)\n' +
      '* [54] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)\n' +
      '* [55] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. NeurIPS (2023)\n' +
      '* [56] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)\n' +
      '* [57] Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X.: Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In: CVPR (2016)\n' +
      '* [58] Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR (2015)\n' +
      '* [59] Lu, J., Clark, C., Zellers, R., Mottaghi, R., Kembhavi, A.: UNIFIED-IO: A unified model for vision, language, and multi-modal tasks. In: ICLR (2023)\n' +
      '* [60] Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Generation and comprehension of unambiguous object descriptions. In: CVPR (2016)\n' +
      '* [61] Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A.: The role of context for object detection and semantic segmentation in the wild. In: CVPR (2014)\n' +
      '* [62] Neuhold, G., Ollmann, T., Rota Bulo, S., Kontschieder, P.: The mapillary vistas dataset for semantic understanding of street scenes. In: ICCV (2017)\n' +
      '* [63] Ning, J., Li, C., Zhang, Z., Wang, C., Geng, Z., Dai, Q., He, K., Hu, H.: All in tokens: Unifying output space of visual tasks via soft token. In: ICCV (2023)\n' +
      '* [64] OpenAI: Chatgpt (2022), [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n' +
      '* [65] OpenAI: Gpt-4 technical report (2023)* [66] Ordonez, V., Kulkarni, G., Berg, T.: Im2text: Describing images using 1 million captioned photographs. NeurIPS **24** (2011)\n' +
      '* [67] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. NeurIPS **35** (2022)\n' +
      '* [68] Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: ICCV (2015)\n' +
      '* [69] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training (2018)\n' +
      '* [70] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR (2020)\n' +
      '* [71] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: ICML (2021)\n' +
      '* [72] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T., et al.: A generalist agent. TMRL (2022)\n' +
      '* [73] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. NeurIPS (2015)\n' +
      '* [74] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: MICCAI (2015)\n' +
      '* [75] Shao, S., Li, Z., Zhang, T., Peng, C., Yu, G., Zhang, X., Li, J., Sun, J.: Objects365: A large-scale, high-quality dataset for object detection. In: ICCV (2019)\n' +
      '* [76] Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: ACL (2018)\n' +
      '* [77] Shin, G., Xie, W., Albanie, S.: Reco: Retrieve and co-segment for zero-shot transfer. In: NeurIPS (2022)\n' +
      '* [78] Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding benchmark suite. In: CVPR (2015)\n' +
      '* [79] Staal, J., Abramoff, M.D., Niemeijer, M., Viergever, M.A., Van Ginneken, B.: Ridge-based vessel segmentation in color images of the retina. TMI (2004)\n' +
      '* [80] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B.: Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) (2023)\n' +
      '* [81] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [82] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n' +
      '* [83] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. In: NeurIPS (2017)\n' +
      '* [84] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\n' +
      '* [85] Wang, H., Shi, C., Shi, S., Lei, M., Wang, S., He, D., Schiele, B., Wang, L.: Dsvt: Dynamic sparse voxel transformer with rotated sets. In: CVPR (2023)\n' +
      '* [86] Wang, H., Tang, H., Shi, S., Li, A., Li, Z., Schiele, B., Wang, L.: Unitr: A unified and efficient multi-modal transformer for bird\'s-eye-view representation. In: ICCV (2023)* [87] Wang, J., Zheng, Z., Ma, A., Lu, X., Zhong, Y.: Loveda: A remote sensing land-cover dataset for domain adaptive semantic segmentation. In: NeurIPS (2021)\n' +
      '* [88] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In: ICML (2022)\n' +
      '* [89] Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. NeurIPS (2023)\n' +
      '* [90] Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.K., Singhal, S., Som, S., et al.: Image as a foreign language: Beit pretraining for all vision and vision-language tasks. CVPR (2023)\n' +
      '* [91] Wang, Y., Chen, X., Cao, L., Huang, W., Sun, F., Wang, Y.: Multimodal token fusion for vision transformers. In: CVPR (2022)\n' +
      '* [92] Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al.: Google\'s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016)\n' +
      '* [93] Xie, E., Sun, P., Song, X., Wang, W., Liu, X., Liang, D., Shen, C., Luo, P.: Polarmask: Single shot instance segmentation with polar representation. In: CVPR (2020)\n' +
      '* [94] Xu, W., Wang, H., Qi, F., Lu, C.: Explicit shape encoding for real-time instance segmentation. In: ICCV (2019)\n' +
      '* [95] Yamazaki, K., Hanyu, T., Tran, M., Garcia, A., Tran, A., McCann, R., Liao, H., Rainwater, C., Adkins, M., Molthan, A., et al.: Aerialformer: Multi-resolution transformer for aerial image segmentation. arXiv preprint arXiv:2306.06842 (2023)\n' +
      '* [96] Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark. In: CVPR (2016)\n' +
      '* [97] Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Unitab: Unifying text and box outputs for grounded vision-language modeling. In: ECCV (2022)\n' +
      '* [98] You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. In: ICLR (2024)\n' +
      '* [99] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)\n' +
      '* [100] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: ECCV. Springer (2016)\n' +
      '* [101] Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.Y.: Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In: ICLR (2022)\n' +
      '* [102] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022)\n' +
      '* [103] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: CVPR (2017)\n' +
      '* [104] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)* [105] Zhu, J., Zhu, X., Wang, W., Wang, X., Li, H., Wang, X., Dai, J.: Uni-perceiver-moe: Learning sparse generalist models with conditional moes. NeurIPS (2022)\n' +
      '* [106] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. ICLR (2020)\n' +
      '* [107] Zhu, X., Zhu, J., Li, H., Wu, X., Li, H., Wang, X., Dai, J.: Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In: CVPR (2022)\n' +
      '* [108] Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang, J., Yuan, L., et al.: Generalized decoding for pixel, image, and language. In: CVPR (2023)In our supplementary, we provide detailed information including model design specifics in SSA, dataset summaries in SSB, along with in-depth training, inference and evaluation procedures in SSC and D. Additional ablation experiments are included in SSE. SSF details specific modules used in comparative methods. Qualitative results across different datasets and tasks are in SSG. Lastly, limitations, negative societal impacts, and a comparison with Fuyu-8B are in SSH.\n' +
      '\n' +
      '## 부록 0.구현 상세사항\n' +
      '\n' +
      '**윈도우 어텐션.** 윈도우 어텐션은 ViT[30]의 SAM[45] 변형으로부터 적응된다. SAM에 이어 패치 임베딩 후 16의 인수로 이미지를 다운샘플링하고 윈도우는 14\\(\\times\\)14의 크기로 정의한다. 원본과 일차적인 구별은 그리드-와이즈 프롬프트(_i.e._, 로컬 이미지 토큰, 태스크 식별자)와 그 출력과 같은 병렬 훈련 단계에서 다중 트랙 로컬 관찰 및 응답을 처리하는 방법에 있다. 이러한 다중 트랙 요소를 관리하기 위해, 우리는 그것들을 시퀀스로 병합하고 공유된 관찰 후에 추가한다. 결과적으로, 윈도우 어텐션에 대한 입력은 도 7에 상세히 설명된 바와 같이, 자기회귀 예측을 가능하게 하면서 그리드 독립성을 보장하기 위해 맞춤화된 어텐션 마스크를 필요로 하는 다수의 부분들로 구성된다. 각각의 서브프로세스 그룹(_i.e._, 동일한 그리드와 연관된 것들) 내에서, 상호작용들은 좌우 단방향 어텐션이다. 더욱이, 서로 다른 하위 프로세스에 속하는 토큰은 고립되어 서로의 정보에 접근하는 것을 방지한다.\n' +
      '\n' +
      '**Global Attention.** 객체- 및 픽셀-레벨 분석을 필요로 하는 태스크들에서, 많은 수의 로컬 예측들은 특히 모든 그리드 포인트들에 걸친 프로세싱 어텐션이 불필요하고 비효율적일 수 있는 글로벌 어텐션 계층들에서 상당한 메모리 및 계산 부담을 생성한다. 따라서, 이러한 작업에 대해, 우리는 공유된 전역 관찰(_i.e._, 입력 이미지 및 텍스트)에만 집중하도록 전역 주의 계층을 최적화함으로써, 각 그리드에 대한 타겟을 계산할 필요가 없다. 표 10은 이 전략이 성능에 약간 영향을 미치지만 계산 시간을 크게 감소시킨다는 것을 보여준다. 그러나, 하나의 윈도우와 단일 전역 응답을 포함하는 224개의 이미지 크기를 갖는 캡션 및 시각적 접지에 있어서, 이러한 최적화는 불필요하다.\n' +
      '\n' +
      '**out-of-vocabulary Representation.** 멀티피스 out-of-vocabulary 개념을 단일 토큰으로 인코딩한다. 이는 절대 위치 인코딩과 결합된 하나의 주의 계층만을 활용하는 간소화된 접근법을 통해 달성된다. As\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline \\hline Global Attention & mIoU & Training Time \\\\ \\hline Normal & 47.9 & 51h \\\\ Accelerated & 47.7 & 35h \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 우리의 가속화된 글로벌 관심을 갖는 단일 작업 훈련에 의한 의미론적 분할의 성능. 약간의 성능 저하로 계산 비용을 크게 줄입니다.\n' +
      '\n' +
      '도 8에 도시된 바와 같이, "트래픽 콘"은 \\(<\\)트래픽\\(>\\)\\(<\\)콘\\(>\\)으로 토큰화된다. 위치 인코딩으로 증강된 해당 텍스트 임베딩은 주의 계층으로 입력되어 각 단어가 나머지 단어와 상호 작용할 수 있다. 우리는 "트래픽 콘"과 같은 다중 단어 개념에 대한 최종 표현으로 첫 번째 출력 토큰을 선택한다. 단일 단어 개념의 경우 원본 텍스트 임베딩을 직접 사용합니다.\n' +
      '\n' +
      '**Background Representation.** 각 데이터셋에 뚜렷한 양의 클래스와 음의 클래스가 포함되어 있음을 감안할 때, 음의 클래스를 나타내기 위해 \\(<\\)background\\(>\\)와 같은 텍스트 레이블을 사용하면 여러 데이터셋에 걸쳐 훈련할 때 모호성이 발생할 수 있다. 따라서, 우리는 배경 클래스에 대해 고유한 인코딩 접근법을 사용했고,\n' +
      '\n' +
      '\\[\\mathcal{F}_{\\text{background}}=-\\sum_{i=0}^{N-1}\\mathcal{F}_{i}/N \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\mathcal{F}_{i}\\)는 \\(i\\)번째 양의 클래스를 나타내고 \\(N\\)은 전체 범주의 수를 나타낸다. 이 접근법은 포지티브 클래스의 토큰과 백그라운드 클래스에 할당된 토큰 사이의 코사인 유사성을 일반적으로 네거티브로 만든다. 제로 샷 시나리오에서 우수한 성능은 그 효과를 강조합니다.\n' +
      '\n' +
      '본 논문에서는 객체 검출 및 인스턴스 분할을 위해 1120\\(\\times\\)1120 픽셀, 시맨틱 분할을 위해 672\\(\\times\\) 672 픽셀, 이미지 캡션 및 시각적 접지를 위해 224\\(\\times\\) 224 픽셀의 특정 작업에 맞춘 다양한 이미지 해상도를 사용한다. 공간 위치를 이산 토큰으로 인코딩하기 위해 이미지 좌표를 설정된 간격 수로 이산화한다. 구체적으로, 입력 영상의 해상도의 두 배가 되도록 이러한 간격의 수를 결정한다. 예를 들어, 224(\\times\\) 224 픽셀의 입력 영상으로 좌표 공간을 448개의 이산 구간으로 나눈다.\n' +
      '\n' +
      '## 부록 0.B 확장 데이터세트\n' +
      '\n' +
      '### In-distribution Datasets\n' +
      '\n' +
      '유니버설 트레이닝 동안, 16개의 공개적으로 액세스 가능한 데이터 소스로부터 총 27개의 데이터 세트가 사용되며, 크기 및 가중치는 표 11에 상세히 기재되어 있다. 웹-소싱 캡션 데이터 세트(CC3M[76], CC12M[14], SBU 캡션[66])에서의 실제 양은 비활성 링크로 인해 보고된 원래 수보다 적음에 유의한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline \\hline Dataset & Size & Percent (\\%) & Group ID & Weight (\\%) \\\\ \\hline\n' +
      '**Object Detection** & 3.8M & 22.55 & - & 20.00\\Objects365 [75 & 1.7M & 9.98 & 0 & 3.22\\OpenImages [48] & 1.7M & 9.98 & 0 & 3.22\\LVIS [35] & 164K & 0.96 & 0.23\\nuImages [11 & 93K & 0.55 & 1 & 6.66\\Pascal VOC 2007 [31 & 10K & 0.06 & 2 & 0.37\\Pascal VOC 2012 [54 & 11K & 0.06 & 2 & 0.07\\\\Pascal VOC 2012 [54 & 11K & 0.06 & 2 & 6.07\\\\LVIS [35] & 164K & 0.96 & 0.22\\LVIS [35] & 0.96 & 0.23\\nuImages [11 & 0.23\\nuImages [11 & 93K & 0.55 & 1 & 6.66\\Pascal VOC 2007 [31] & 10K & 0.06 & 2 & 0.37\\Pascal VOC 2012 [\n' +
      '**Instance Segmentation** & 1.4M & 8.34 & - & 20.00 \\ LVIS [35] & 164K & 0.96 & 3 & 0.76 \\ OpenImages [48] & 1M & 5.87 & 3 & 5.90 \\\\ nuImages [11] & 93K & 0.55 & 4 & 6.66 \\ COCO 2017 [54] & 164K & 0.96 & 5 & 6.66 \\\\ COCO\n' +
      '**Semantic Segmentation** & 322K & 1.89 & - & 20.00 \\COCO-Stuff [12 & & 164K & 0.96 & 6 & 6.28 \\Pascal Context[61 & 10K & 0.06 & 6 & 0.38 \\nuImages [11 & 93K & 0.55 & 7 & 4.84 \\BDD100K [99 & 10K & 0.06 & 7 & 0.52 \\ Mapillary Vistas [62 & 20K & 0.15 & 7 & 1.30 \\ADE20K [103 & 20K & 0.12 & 8 & 6.67 \\\\ADE20K [103] & 10K & 0.68 \\\\ADE20K [103] & 10K & 0.68 \\\\ADE20K [103] & 10K & 0.68 \\\\ADE20K [103] & 10K & 0.68 \\\\ADE20K [66 & 10K & 0.68 \\\\ADE20K [66 & 10K & 0.68 \\\\ADE20K\n' +
      '**Image Caption** & 11.3M & 66.54 & - & 20.00\\CC3M [76 & 1.8M & 10.57 & 9 & 1.74\\CC12M [14 & 7.8M & 45.79 & 9 & 6.96\\SBU Captions [66 & 800K & 4.70 & 9 & 0.58\\Visual Genome [46 & 770K & 4.52 & 9 & 0.71\\COCO Caption [23 & 164K & 0.96 & 10.00\\CC3M & 10.57 & 9 & 1.74\\CC12M [14 & 7.79 & 9 & 6.96\\SBU Captions [66 & 800K & 4.70 & 9 & 0.58\\Visual Genome [46 & 770K & 4.52 & 9 & 0.71\\COCO Caption [23 & 10 & 10.00\\CC12M]\n' +
      '**Visual Grounding** & 115K & 0.68 & - & 20.00\\RefCOCOCO[42 & 20K & 0.12 & 11 & 4.00\\RefCOCO+[42 & 20K & 0.12 & 4.00\\RefCOCOg[60 & 25K & 0.15 & 11 & 4.00\\RefCLEF[42 & 20K & 0.12 & 12 & 4.00\\Flickr30K [68 & 30K & 0.18 & 13 & 4.00\\\\flickr30K [68 & 30K & 0.18 & 13 & 4.00\\\\flickr30K & 0.18 & 13 & 4.00\\\\refCOCO+[42 & 20K & 0.12 & 25K & 0.15 & 11 & 4.00\\RefCOCOg[60 & 25K & 0.15 & 11 & 4.00\\RefCLEF[42 & 20K & 0.12 & 12 & 4.00\\flickr30K] & 30K & 0.18 & 13 & 4.00\\\\flickr30K [68] &\n' +
      '**All** & 17M & 100 & - & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 유니버설 트레이닝 데이터세트 세부사항. 왼쪽부터 오른쪽까지의 열은 데이터 세트 크기, 총 데이터에 대한 비율, 할당된 그룹 번호 및 샘플링 가중치를 나타낸다. 무게는 작업 전반에 걸쳐 고르게 분포되어 있습니다. 각 태스크(_e.g._, 일상 생활, 자율 주행) 내의 상이한 시나리오는 동일한 가중치를 갖는 개별 그룹을 생성한다. 그룹 내 샘플링 가중치는 데이터 세트 크기에 기초하여 설정된다.\n' +
      '\n' +
      '**COCO.** MS COCO 데이터세트, 또는 Microsoft Common Objects in Context[54]는 객체 검출, 분할, 키-포인트 검출, 및 캡셔닝을 위한 포괄적인 데이터세트이다. 여기에는 330K 이상의 이미지가 포함되어 있으며 220K 이상에 대한 주석이 있으며 80개 범주에 걸쳐 150만 개의 객체가 있다. 각 이미지에는 5개의 문장 설명이 있으며 250K 보행자에게 키포인트가 주석이 달려 있다. 2014년 초기 릴리스에는 훈련(83K), 검증(41K) 및 테스트(41K) 세트의 164K 이미지가 있다. 2017년에는 훈련/검증 분할이 118K/5K로 변경되었다. **Objects365.**Objects365[75]는 365개의 객체 범주를 포함하고 3천만 개의 주석이 달린 경계 상자와 함께 200만 개 이상의 훈련 이미지를 자랑하는 방대한 객체 탐지 데이터 세트이다. 이 데이터 세트는 다양한 시나리오에서 다양한 객체를 제시하여 도전적인 객체 탐지 작업에 대한 강력한 벤치마크를 제공한다. **OpenImage.**Open Images[48]는 약 900만 개의 이미지를 가진 데이터셋으로, 각각 이미지 레벨 레이블, 객체 경계 상자, 세그멘테이션 마스크, 시각적 관계, 로컬화된 내러티브 및 포인트 레벨 레이블로 주석을 달았다. 2만 638개의 이미지 레벨 라벨, 1600만 개의 경계 상자가 있는 600개의 객체 클래스, 280만 개의 세분화 마스크를 커버하며 컴퓨터 비전에서 귀중한 자원으로 자리 잡고 있다. **LVIS.** LVIS[35] (Large Vocabulary Instance Segmentation)은 인스턴스 분할 작업에 맞춰진 데이터세트로서, 164,000개의 이미지들의 데이터세트 내의 1000개 이상의 엔트리 레벨 오브젝트 카테고리들에 걸쳐 약 200만 개의 고품질 분할 마스크들을 제공한다. 이 데이터 세트는 자연 이미지에서 일반적으로 관찰되는 Zipf 분포를 다루기 위해 만들어졌으며, 이는 사물의 큰 어휘를 다루는 인스턴스 분할 작업을 수행하는 연구자와 개발자에게 귀중한 자원이 되었다. **파스칼 VOC 2007.** 파스칼 VOC 2007 [31] 데이터 세트는 20개의 객체 클래스를 특징으로 하는 실제 객체 인식을 위한 중요한 리소스 역할을 한다. 균형 잡힌 훈련/검증 및 테스트를 위해 신중하게 분할된 9,963개의 사진과 24,640개의 레이블이 지정된 샘플을 사용하여 분류, 탐지, 분할 및 사람 레이아웃을 포함한 다양한 작업을 지원하는 다목적 데이터 집합으로 간주한다. **파스칼 VOC 2012.**파스칼 VOC 2012 [31]은 실세계 설정에서 객체를 인식하기 위한 귀중한 데이터세트이다. 20개의 객체 클래스를 포함하며 27,450개의 ROI 태그 객체와 6,929개의 세그먼테이션이 있는 11,530개의 이미지를 포함하여 컴퓨터 비전에서 중요한 벤치마크 역할을 한다. **nuImages.**nuImages[11] 데이터셋은 과거 및 미래의 타임스탬프로부터 120만 개의 카메라 이미지와 함께 93,000개의 2D 주석이 달린 이미지를 제공함으로써 자율 주행을 위한 nuScenes[11]을 보완한다. 그것은 nuScenes 생태계의 일부이며 판옵틱 및 다중 주석 측면에 중점을 둔다. 데이터 세트는 비, 눈, 밤과 같은 다양한 조건을 포함하여 다양한 운전 시나리오를 다룬다. 또한 2Hz 간격 이미지로 시간적 역학을 제공합니다. 주석들은 인스턴스 마스크들을 갖는 80만 개의 전경 객체들 및 100,000 개의 시맨틱 세분화 마스크들을 포함한다. **ADE20K.** ADE20K [103] 시맨틱 세분화 데이터세트는 객체 및 객체 부분 모두에 대해 픽셀 레벨에서 세심하게 주석이 달린 20,000개의 장면 중심 이미지를 포함한다. 150개의 의미 범주를 포함하여 하늘, 도로, 사람, 자동차, 침대와 같은 특정 객체를 포함한다. 데이터 세트는 20,210개의 훈련, 2,000개의 유효성 검사 및 3,000개의 테스트 이미지로 나뉜다. **COCO-Stuff.** COCO-stuff [12] 데이터 세트는 시맨틱 세분화, 객체 검출, 이미지 캡션화 등과 같은 다양한 장면 이해 작업에 대해 중요성을 갖는다. 처음에 객체 주석을 우선시했던 원래의 COCO 데이터 세트를 보완함으로써 파생되며, 그것은 재료 주석의 감독을 다룬다. 164,000개의 이미지에 걸쳐 있는 COCO-stuff 데이터 세트에는 172개의 범주가 포함되어 있으며, 80개 항목, 91개 항목 및 1개의 레이블이 지정되지 않은 클래스를 통합한다.\n' +
      '\n' +
      '**Pascal Context.** PASCAL Context[61] 데이터세트는 모든 트레이닝 이미지에 대해 픽셀-와이즈 라벨을 제공함으로써 PASCAL VOC 2010[31] 검출 챌린지를 확장한다. PASCAL VOC 분할의 원래 20개 클래스를 포함하는 400개 이상의 클래스를 포함하여 이러한 클래스는 객체, 물건 및 잡종으로 분류된다. 많은 객체 범주의 희소성을 해결하기 위해 일반적인 관행은 자주 발생하는 59개 클래스의 하위 집합을 사용하는 것을 포함한다.\n' +
      '\n' +
      '**BDD100K.**BDD100K[99]는 100K 비디오가 포함된 대용량 데이터셋으로 1,000시간 이상의 운전 경험과 1억 프레임 이상을 제공합니다. 도로 객체, 차선 표시, 주행 가능 영역 및 세부 인스턴스 세그먼테이션에 대한 주석을 포함합니다. 도로 객체 검출 및 주행 가능한 영역 분할 문제의 경우, 70,000개의 훈련 및 10,000개의 검증 이미지가 있다. 풀-프레임 시맨틱 세그먼테이션의 경우, 7,000개의 트레이닝 및 1,000개의 검증 이미지가 존재한다.\n' +
      '\n' +
      '**Mapillary Vistas.**Mapillary Vistas[62]는 25,000개의 고해상도 이미지를 갖는 대규모 거리 레벨 이미지 데이터세트이다. 37개 클래스에 대한 인스턴스별 레이블을 포함하여 66개 객체 카테고리에 대한 주석이 특징인 폴리곤을 사용하여 조밀하고 세밀한 주석 스타일을 채택합니다. 데이터세트는 주로 시맨틱 이미지 분할 및 인스턴스별 이미지 분할에 중점을 두고 있으며, 시각적 로드-씬 이해의 향상을 목표로 한다.\n' +
      '\n' +
      '**CC3M.** CC3M[76]으로 알려진 개념 캡션은 각각 설명 캡션과 세심하게 쌍을 이루는 약 330만 개의 이미지를 광범위하게 수집하는 것이 특징이다. 웹 이미지와 관련된 Alt-text HTML 속성에서 추출된 이러한 캡션은 품질 보증을 위해 자동화된 파이프라인을 거친다. 이는 데이터세트를 매우 다양하게 만들어 다양한 자연어 처리 및 이미지 이해 작업에 적합합니다.\n' +
      '\n' +
      '**CC12M.** 개념 12M[14](CC12M)은 비전 및 언어 사전 훈련을 위해 특별히 만들어진 데이터셋이다. 상당한 1,200만 개의 이미지-텍스트 쌍으로 구성되어 있습니다. 제한적인 요구 사항을 가진 일부 다른 데이터 세트와 달리 CC12M은 데이터 수집 파이프라인을 완화하여 데이터 세트 규모와 다양성을 향상시킨다. 시각 및 언어 과제, 특히 긴 꼬리 시각 인식에서 최첨단 결과를 제공하는 것으로 나타났으며, 이는 이 분야의 연구 및 개발에 귀중한 자원이 된다.\n' +
      '\n' +
      '**SBU Captions.** SBU Captions 데이터셋 [66]은 100만 개의 이미지 및 플릭커에서 조달한 관련 캡션의 집합으로, 주로 이미지 캡션 모델 훈련에 사용된다. 다양한 실세계 이미지와 텍스트 설명을 제공하여 컴퓨터 비전 및 자연어 처리 연구에 귀중한 자원이 된다.\n' +
      '\n' +
      '**Visual Genome.**Visual Genome[46]은 108,077개의 이미지를 가진 포괄적인 데이터 세트이며 540만 개의 영역 설명, 170만 개의 시각적 질문 답변, 380만 개의 객체 인스턴스, 280만 개의 속성 및 230만 개의 관계가 풍부한 주석을 달았다. 이 데이터 세트는 객체, 속성 및 이미지 간의 관계를 포함하여 이미지에 대한 자세한 정보를 제공하도록 설계되었다.\n' +
      '\n' +
      'COCO Caption.COCO Captions [23]은 33만 개의 이미지에 대해 150만 개의 캡션으로 구성되며, 훈련 및 검증 세트에서 각 이미지에 대해 5개의 캡션이 있다. 안드레이 카르파시가 만든 이 데이터 세트의 널리 사용되는 하위 집합인 "카르파시 분할"은 원시 데이터 세트에서 열차와 발 세트를 병합하고 원래 발 세트에서 5,000개의 이미지를 선택하여 새로운 검증 세트를 생성하고 추가 5,000개의 이미지를 사용하여 테스트 세트를 구성한다.\n' +
      '\n' +
      '**RefCOCO.** RefCOCO[42], RefCOCO+[42], RefCOCOg[60] 데이터 세트는 한 참가자가 자연어를 사용하여 이미지에서 분할된 객체를 설명하고 다른 참가자는 올바른 객체를 식별하는 두 플레이어 게임인 ReferitGame을 통해 생성되었다. RefCOCO에서는 표현 참조에 대한 언어 제한이 없는 반면, RefCOCO+에서는 위치 단어가 금지된다. 이 데이터 세트는 원근법에 의존하는 것보다 "노란 물방울 무늬 셔츠를 입은 남자"와 같은 외모 기반 설명에 중점을 둔다. RefCOCO는 19,994개의 이미지에서 50,000개의 객체에 대해 142,209개의 참조 표현식을 포함하고, RefCOCO+는 19,992개의 이미지에서 49,856개의 객체에 대해 141,564개의 표현식을 포함한다.\n' +
      '\n' +
      'ReferIt로도 알려진**RefCLEF.**RefCLEF[42]는 IAPR TC-12 데이터세트로부터 소싱된 20,000개의 이미지들로 구성되며, SAIAPR-12 데이터세트로부터 분할된 이미지 영역들이 수반된다. 데이터 세트는 훈련 및 검증을 위해 지정된 10,000개의 이미지가 있는 섹션과 테스트를 위해 10,000개의 이미지가 있는 섹션으로 균등하게 분할된다. 트레이닝 및 검증 부분은 각각 이미지, 바운딩 박스 및 설명으로 구성된 총 59,976개의 엔트리를 포함한다. 테스트 세트는 약간 더 크며 동일한 유형의 데이터를 가진 60,105개의 항목을 특징으로 합니다.\n' +
      '\n' +
      '**Flickr30K.**Flickr30K[68]은 문장 기반 이미지 설명에 사용되는 널리 인식되는 데이터셋이다. 일상적인 활동과 사건을 묘사하는 31,783개의 이미지가 각각 설명 캡션을 동반합니다. 이 데이터 세트는 언어적 표현과 시각적 미디어의 관계를 연구하기 위한 표준 벤치마크 역할을 한다.\n' +
      '\n' +
      '### Out-distribution Datasets\n' +
      '\n' +
      '**도시 풍경.**도시 풍경 [27]은 도시 장면을 이해하기 위한 대규모 데이터 세트이며, 8개의 범주로 그룹화된 30개 클래스에 걸쳐 의미론적, 인스턴스적, 픽셀 수준 주석을 특징으로 한다. 그것은 약 5,000개의 미세 주석이 달린 이미지와 20,000개의 거칠게 주석이 달린 이미지로 구성되어 있으며, 다양한 조건에서 다양한 도시에 기록된다. 이 데이터 세트는 도시 장면 분석과 관련된 작업에 가치가 있다.\n' +
      '\n' +
      '**SUN RGB-D.** SUN RGB-D 데이터세트[78]는 각각 깊이 및 분할 맵을 갖는 룸 장면의 10,335개의 RGB-D 이미지를 포함한다. 700개의 객체 범주에 대해 주석이 달리고 각각 5,285개 및 5,050개의 이미지가 있는 훈련 및 테스트 세트로 나뉜다. 이 데이터세트는 장면 이해 작업을 위한 대규모 3D 주석 및 메트릭의 필요성을 해결한다. 여기에는 2D 및 3D 객체 경계, 방향, 룸 레이아웃 및 장면 범주에 대한 광범위한 주석이 있는 4개의 센서의 데이터가 포함되어 있어 고급 알고리즘 훈련 및 교차 센서 편향 연구가 가능하다.\n' +
      '\n' +
      '**nocaps.**nocaps [2] 데이터세트는 다양한 데이터 출처로부터 더 넓은 범위의 시각적 개념을 파악하기 위해 이미지 캡션 모델을 푸시한다. 오픈이미지에서 조달된 15,100개의 이미지에 대해 166,100개의 인간 생성 캡션을 포함하는, 데이터세트는 COCO 이미지-캡션 쌍 및 오픈이미지의 라벨 및 바운딩 박스를 포함하는 상이한 트레이닝 데이터를 객체 설명에 특정 중점을 두고 통합한다.\n' +
      '\n' +
      '**DRIVE.** 망막 혈관 분할에 사용되는 DRIVE[79] 데이터 세트는 비정상적인 병리를 표시하는 7개를 포함하여 40개의 컬러 안저 이미지로 구성된다. 네덜란드의 당뇨병 망막병증 스크리닝 중에 캡처된 이 이미지는 45도 시야가 특징인 캐논 CR5 카메라로 촬영되었다. 데이터세트는 각각 원형 시야(FOV) 마스크를 수반하는 트레이닝 세트(20개의 이미지) 및 테스팅 세트(20개의 이미지)로 분할된다. 전문가 수동 세그먼테이션은 훈련 세트에서 평가를 위해 제공되는 반면, 테스트 세트는 두 개의 관찰자 기반 세그먼테이션을 포함하며, 첫 번째 관찰자의 결과는 평가를 위한 지상 진리로 간주된다.\n' +
      '\n' +
      '**LoveDA.** LoveDA [87] 데이터 세트는 난징, 창저우 및 우한의 도시 및 농촌 지역의 5987개의 고해상도 원격 감지 이미지(0.3m)로 구성된다. 시맨틱 세분화 및 도메인 적응 작업을 대상으로 하며, 다양한 지리적 환경을 해결하는 것을 목표로 하는 다중 스케일 객체, 복잡한 배경, 일관성 없는 클래스 분포와 같은 과제를 제공한다.\n' +
      '\n' +
      '**ISPRS Potsdam.** ISPRS Potsdam [39] 데이터세트는 5 cm 지상 샘플링 거리를 갖는 TOP(true orthophotos) 및 DSM(digital surface model)을 갖는 38개의 패치로 구성된다. TOP 이미지는 다양한 채널 구성(IRRG, RGB, RGBIR)으로 이용 가능하며 DSM 파일은 높이를 나타내는 32비트 플로트 값을 포함한다. 일부 패치는 DSM을 정규화하여 지형 위의 높이를 나타낸다. 접지 진리 레이블은 데이터의 일부에 제공되며 나머지는 벤치마크 테스트를 위해 예약된다.\n' +
      '\n' +
      '**WIDER Face.** WIDER Face[96] 데이터세트는 포괄적인 얼굴 검출 벤치마크 데이터세트로서, 393,703개의 라벨링된 얼굴의 다양한 범위를 갖는 32,203개의 이미지로 구성된다. 이러한 이미지는 스케일, 포즈 및 폐색에서 변화를 나타낸다. 데이터 세트는 61개의 이벤트 클래스로 분류되며, 훈련의 경우 40%, 검증의 경우 10%, 테스트의 경우 50%이다. 평가는 PASCAL VOC 데이터 세트 메트릭을 따른다.\n' +
      '\n' +
      '**DeepFashion.** DeepFashion[57] 데이터셋은 광범위한 주석과 함께 약 80만 개의 패션 이미지를 종합적으로 모은 것이다. 이러한 주석에는 46개의 패션 카테고리, 1,000개의 설명 속성, 경계 상자 및 랜드마크 정보가 포함된다. 데이터 세트는 잘 포스팅된 제품 사진에서 실제 소비자 스냅샷에 이르기까지 광범위한 패션 이미지를 다룬다.\n' +
      '\n' +
      '## 부록 0.C 훈련\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**트레이닝 스킴.** 단일 태스크 트레이닝의 경우, GiT-B\\({}_{\\text{single-task}}\\)는 통상적으로 코사인 어닐링 스케줄에 따라 8개의 NVIDIA A100 GPU(40GB) 상에서 120,000번의 반복에 대해 24의 배치 크기를 사용하여 트레이닝된다. 5개의 데이터 집합에 대한 다중 작업 합동 훈련에서 GiT-B\\({}_{text{multi-task}}\\)은 더 많은 반복을 위해 동일한 배치 크기와 GPU 번호로 훈련을 받는다. 대형 및 거대한 모델 변형들은 트레이닝을 위해 더 많은 GPU 메모리를 필요로 하고, 따라서 각각 12개 및 24개의 GPU들에 대해 트레이닝된다. 대규모 범용 트레이닝을 위해, 우리는 320,000번의 반복에 걸쳐 96의 배치 크기를 사용하여 모든 모델을 트레이닝한다. 이 과정은 32개, 48개, 96개 GPU의 설정에 대해 수행되어 각각 3일, 5일, 7일의 총 교육 시간이 발생한다.\n' +
      '\n' +
      '**맞춤형 학습율.** 프리트레이닝이 없는 레이어에 대해서는 표준 베이스 학습율을 적용하였습니다. 대조적으로, 사전 훈련된 레이어는 점진적으로 증가하는 학습률을 사용했다. 이 전략은 첫 번째 사전 훈련 계층에 대한 기본 비율의 0.1배인 학습률로 시작하여 최종 사전 훈련 계층에 의해 점차 기본 비율의 1.0배로 확대된다. 우리는 이 방법이 사전 훈련된 가중치와 새로 훈련된 가중치의 통합을 향상시켜 모델의 전반적인 성능을 향상시켰다고 주장한다.\n' +
      '\n' +
      '**그리드 생성 및 샘플링.** 각 태스크에서 요구하는 상세 수준에 따라 그리드 크기를 조정합니다. 객체 검출과 인스턴스 분할을 위해 각 윈도우에 5\\(\\times\\)5개의 그리드를 배치하고, 시맨틱 분할을 위해 그리드 크기를 14\\(\\times\\)14로 증가시킨다. 예를 들어, 객체 검출에서는 1120\\(\\times\\)1120 픽셀의 입력 영상을 25\\(\\times\\)25개의 그리드로 표현하고, 시맨틱 분할에서는 672\\(\\times\\)672 픽셀을 42\\(\\times\\)42개의 그리드로 표현한다. 이러한 그리드의 모든 점에 대한 계산 손실은 특히 의미론적 분할을 위해 과도한 계산 자원을 요구할 것이다. 이를 관리하기 위해 훈련 중 특정 그리드 포인트를 샘플링하고 양성 샘플을 포함하는 데 중점을 두고 미리 결정된 수의 포인트를 선택하고 필요에 따라 음성 샘플로 보충하는 전략을 사용한다. 구체적으로, 객체 검출 및 인스턴스 분할을 위해, 각 윈도우에서 25점 중 10점을 선택하고, 시맨틱 분할을 위해, 196점 중 32점을 선택한다. 표 12에 도시된 바와 같이, 이 방법은 상당한 성능 저하 없이 계산 비용을 효과적으로 감소시킨다.\n' +
      '\n' +
      '### Label Assignment\n' +
      '\n' +
      '**객체 검출.** 우리의 접근법은 라벨 할당 계산을 위해 잘 확립된 헝가리 매칭 알고리즘[47]을 사용한다. 각 격자점에 대해 모든 상자의 중심까지의 정규화된 L1 거리를 매칭 비용으로 계산한다.\n' +
      '\n' +
      '**인스턴스 세그멘테이션.** 객체 검출과 유사하게, 인스턴스 세그멘테이션 타겟들은 바운딩 박스 센터들 및 그리드 포지션들 사이의 L1 거리를 컴퓨팅함으로써 결정된다. PolarMask[93]에서 영감을 얻은 24개의 광선을 갖는 극좌표가 마스크 표현을 위해 사용된다. 객체의 질량 중심은 주석이 달린 폴리곤 경계를 사용하여 계산됩니다. 양수로 분류된 그리드 점은 객체 범주, 경계 상자, 중심 및 질량 중심에서 경계점까지의 거리를 정확하게 예측해야 합니다.\n' +
      '\n' +
      '*Semantic Segmentation.** Expanding upon ViT, we generate the patch features (42 \\(\\times\\) 42) by downsampling the image (672 \\(\\times\\) 672) via factor 16. The dense prediction nature of semantic segmentation, given the grid point size with the patch feature size. 계산 부하를 줄이기 위해 다운샘플\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} \\hline Sample Number & mAP & Training Time \\\\ \\hline\n' +
      '625 & 45.3 & 47h\\\\\n' +
      '250 & 45.1 & 20h \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 25\\(\\times\\)25 그리드 해상도의 객체 검출에 대한 그리드 샘플링의 성능.\n' +
      '\n' +
      '원본 마스크 주석(672\\(\\times\\) 672)이 4배 증가하여 그리드 크기보다 4배 더 큰 크기 168\\(\\times\\) 168의 주석이 생성되었다. 그 후, 각 그리드 포인트는 중심 4\\(\\times\\)4 제곱 내에서 16개의 위치에 대한 분할 주석을 자율적으로 예측한다.\n' +
      '\n' +
      '**이미지 캡션.** 이미지 캡션 프로세스에서 각 캡션을 20개의 토큰의 고정 길이 시퀀스로 토큰화한다. 캡션 길이가 20 토큰보다 짧은 경우, 균일성을 보장하기 위해 종료 심볼로 패딩한다.\n' +
      '\n' +
      '**시각적 접지.**시각적 접지 작업에서, 각각의 쿼리는 특정 바운딩 박스를 직접 타겟팅하며, 박스들을 그리드 포인트들과 정렬할 필요성을 제거한다.\n' +
      '\n' +
      '### Data Augmentation\n' +
      '\n' +
      '**객체 검출 및 인스턴스 분할.**객체 수준 인식 작업에 대해, 이미지는 전처리 단계를 거친다. 초기에는 이미지가 0.5 확률로 수평으로 뒤집힙니다. 이어서, 고정된 입력 크기를 달성하기 위해 두 가지 방법이 사용된다. 첫 번째 방법은 원래의 종횡비를 무시하고 1120 \\(\\times\\)1120의 치수로 이미지를 직접 리사이징하는 것이다. 두 번째 방법은 원래의 종횡비를 보존하면서 이미지를 (400, 4200), (500, 4200), 또는 (600, 4200)의 세 개의 크기 쌍 중 하나로 랜덤하게 조정한다. 리사이징 후, 이미지는 (384, 600)의 크기로 크롭된 다음, 다시 1120 \\(\\times\\) 1120 픽셀들로 리사이징된다.\n' +
      '\n' +
      '**시맨틱 세그멘테이션.** 시맨틱 세그멘테이션에서, 특정 전처리 단계들은 이미지들에 적용되어 그 크기가 표준화되고 다양성을 증가시키는 것을 보장한다. 초기 영상 획득은 672\\(\\times\\) 672 픽셀의 크기를 가지며, 두 방법 사이에 무작위 선택을 사용한다. 첫 번째 방법은 원래 화면 비율을 무시하고 이미지를 672\\(\\times\\)672로 직접 조정한다. 두 번째 방법은 원래의 종횡비를 보존하지 않고 이미지를 672의 100% 내지 200% 범위의 크기로 스케일링하는 것을 포함한다. 이 후, 이미지 크기가 672\\(\\times\\) 672 픽셀로 유지되도록 랜덤 크롭을 적용한다. 또한, 영상의 다양성을 높이기 위해 수평 방향 플리핑과 측광 왜곡의 50% 확률로 두 가지 추가 연산을 수행한다. 이러한 단계는 집합적으로 세분화 작업에 대한 보다 강력한 데이터 세트에 기여한다.\n' +
      '\n' +
      '**이미지 캡션.** 이 작업에 대해, 우리는 원본 이미지와 관련하여 [0.08, 1.0]의 크기 비율과 [3/4, 4/3]의 종횡비를 변화시키는 동적 크롭으로 전처리를 시작한다. 이 작물에 따라 이미지의 크기는 224\\(\\times\\)224 크기로 조정됩니다. 추가적으로, 추가 증강을 위해 이미지를 수평으로 뒤집을 확률이 50%이다.\n' +
      '\n' +
      '**시각적 접지**시각적 접지 증강은 색상 조정을 포함한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c c|c|c c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multirow{2}{*}{\\#Params} & \\multicolumn{2}{c|}{Object Detection} & \\multicolumn{2}{c|}{Instance Seg} & \\multicolumn{2}{c|}{Semantic Seg} & \\multicolumn{2}{c|}{ Captioning} & \\multicolumn{1}{c}{Grounding} \\\\  & & AP & AP\\({}_{50}\\) & AP\\({}_{75}\\) & AP & AP\\({}_{50}\\) & AP\\({}_{75}\\) & mIoU(SS) & BLEU-4 CIDEr & Acc00.5 \\\\ \\hline GT\\(\\cdot\\)Business & 131M & 44.4 & 61.2 & 48.1 & 30.3 & 53.0 & 30.0 & 44.6 & 33.6 & 108.3 & 84.2 \\\\ GT\\(\\cdot\\)L\\({}_{\\text{initial}}\\) & 387M & 50.2 & 67.6 & 54.6 & 33.1 & 58.4 & 32.7 & 48.1 & 36.2 & 117.5 & 86.0 \\\\ GT\\(\\cdot\\)Business & 756M & 53.3 & 71.2 & 58.3 & 35.9 & 62.6 & 36.1 & 53.0 & 37.7 & 124.2 & 88.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 5개의 표준 시력 중심 벤치마크에 대한 보편적 훈련 후 모델의 평가 결과.\n' +
      '\n' +
      '50% 확률로 밝기, 대비, 채도 및 색조의 변화를 가능하게 합니다. 이어서, 이미지는 원래 크기의 (0.8, 0.8)의 상대적인 범위 내에서 랜덤 크롭을 겪는다. 마지막으로, 원래의 종횡비를 유지하지 않고 영상의 크기를 224\\(\\times\\)224로 조정한다.\n' +
      '\n' +
      '## 부록 0.D 평가\n' +
      '\n' +
      '### Auto-regressive Decoding\n' +
      '\n' +
      '우리는 태스크 템플릿을 기반으로 다양한 태스크에 대한 고유한 디코딩 규칙을 조정한다. 예를 들어, 객체 검출에서 템플릿 \\(<\\)c\\(>\\)\\(<\\)x\\({}_{1}\\)\\(>\\)\\(<\\)y\\({}_{1}\\)\\(>\\)\\(<\\)x\\({(<\\)x\\({({}_{2}\\)\\(>\\))\\(<\\)y\\({({}_{2}\\))\\(>\\)을 이용하여 데이터세트의 모든 카테고리를 포함하는 어휘로부터 카테고리를 추출하여 첫 번째 위치에서 디코딩한다. 이후의 4개의 위치는 수치 값을 해독하여 이산화된 위치의 어휘에서 그림을 그린다. 표 16은 이미지 캡셔닝을 제외하고 필요한 터미네이터 토큰이 없는, 모든 태스크들에 대한 고정된 디코딩 단계 번호를 예시한다. 이미지 캡션에서, 추론 동안 종결자 이후의 예측은 무시된다.\n' +
      '\n' +
      '### Inference Speed\n' +
      '\n' +
      '표 17에서, 우리는 배치 크기가 1인 단일 NVIDIA A100 GPU에서 측정된 5개의 태스크에 걸친 GiT-B의 추론 속도를 제시한다. NLP에서 흔히 볼 수 있는 자동 회귀 디코딩 패러다임을 준수하기 때문에 느린 추론 속도의 단점을 계승한다. 이러한 한계는 픽셀당 예측을 필요로 하는 고해상도 객체 레벨 및 시맨틱 세분화 작업에서 더욱 두드러진다. 그러나, 우리는 다중 병렬 디코딩을 활용하는 것이 우리의 방법의 속도를 상당히 향상시켜 허용 가능한 수준으로 끌어올렸다고 주장한다. 표 18에 나타난 바와 같이, 우리의 접근법은 SAM과 유사한 분할 속도를 보여준다. 구조 및 예측 접근법이 LLM과 밀접하게 일치한다는 점을 감안할 때 LLM에 사용된 추론 가속 기법[15]도 우리의 방법을 향상시킬 가능성이 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c|c|c|c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{4}{c}{Object Detection\\(\\Delta\\)F} & \\multicolumn{4}{c}{Grounding\\(\\Delta\\)Acc} & \\multicolumn{2}{c}{Instance Seq\\(\\Delta\\)F} \\\\ \\cline{2-10}  & Object305 [73] & OmnImages [48] & LVIS [9] & VOC121 [21] & miluang [11] & ReCOCO [9] & ReMo20K [8] & ReCLEF [42] & LVIS [25] \\\\ \\hline GiT-B & 17.7 & 63.4 & 12.3 & 79.0 & 44.5 & 72.5 & 76.9 & 71.0 & 72.2 & 8.4 \\\\ GiT-B & 25.5 & 51.6 & 17.3 & 83.6 & 47.2 & 73.9 & 78.9 & 72.7 & 74.5 & 11.4 \\\\ GT-H & 31.9 & 57.7 & 21.7 & 84.9 & 50.0 & 78.3 & 80.7 & 77.5 & 75.8 & 14.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: 검출, 인스턴스 분할 및 시각적 접지 데이터 세트에 대한 범용 트레이닝 평가 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & COCO-Stuff [12] & Pascal Context [61] & BDD100K [99] & Mapillary Vistas [62] \\\\ \\hline GiT-B\\({}_{\\text{universal}}\\) & 42.6 & 56.8 & 57.8 & 23.0 \\\\ GiT-Luversal & 46.0 & 60.4 & 59.3 & 25.4 \\\\ GiT-H\\({}_{\\text{universal}}\\) & 49.1 & 63.3 & 61.5 & 28.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 15: 모든 결과가 mIoU 메트릭을 사용하여 측정된, 세그먼테이션 데이터 세트에 대한 범용 트레이닝의 평가.\n' +
      '\n' +
      '### Benchmarking Setup\n' +
      '\n' +
      '**다중 작업 학습.** 다중 작업 데이터 세트에 대해 COCO 캡션[12]을 제외하고 검증 세트에 대한 평가를 수행했으며 테스트 세트에 대한 평가를 위해 카파시 분할[41]을 사용했다.\n' +
      '\n' +
      '**유니버설 러닝.** 여러 주요 데이터 세트에 대한 범용 모델을 평가합니다. 표 13은 5가지 작업에 대한 대표 데이터 세트에 대한 성능을 나타낸다. 그러나 범용 트레이닝 동안 이러한 분석 가능한 다중 작업 데이터 세트의 샘플링이 덜 빈번하기 때문에, 그들의 성능은 다중 작업 벤치마크에서 트레이닝된 모델보다 약간 지연된다. 다른 데이터 세트에 대한 추가 성능 통찰력은 표 14 및 15를 참조하며, 특히 이미지 캡셔닝의 경우 COCO 캡션을 제외한 모든 데이터 세트가 훈련에 완전히 사용되어 추가 평가의 필요성을 제거한다.\n' +
      '\n' +
      '**Few-shot Learning.** 고전적인 N-way K-shot [32] 설정을 채택하여 few-shot 평가를 위한 지원 세트를 생성한다. 이 설정에서 데이터 세트의 각 클래스에 대해 해당 클래스로 표시된 k개의 샘플을 추출하여 N\\(\\times\\)K 샘플을 선택한다. 기본적으로 K는 5로 설정되며 표 19에 표시된 대로 각 데이터 세트의 범주 수에 따라 다양한 양의 지원 세트를 샘플링한다. 각 실험은 기본적으로 지원 세트에서 100회 반복됩니다. 그러나 WIDERFace[96]에 설정된 지원의 제한된 크기로 인해 반복 횟수를 50회로 줄여 과적합의 위험을 완화한다. 수 샷 훈련은 모두 2e-4의 고정 학습률로 진행된다.\n' +
      '\n' +
      '비교 기준선으로 두 가지 고전적인 방법인 Faster R-CNN[73]과 DeepLabV3[19]를 선정한다. Faster R-CNN의 경우 COCO [54] 데이터셋에서 미리 학습된 가중치를 활용하여 ResNet-50을 백본으로 하는 버전을 사용한다. DeepLabV3의 경우 ADE20K [103] 데이터 세트에 대한 사전 교육을 활용하여 ResNet-101을 백본으로 사용하는 버전을 선택한다.\n' +
      '\n' +
      '## 부록 0.E 더 많은 절제 연구\n' +
      '\n' +
      '**텍스트 컨디셔닝** 시각적 접지에서 네트워크 포워딩 동안 이미지 대 텍스트 주의를 통합하여 감지와 시각적 접지의 작업 차별성을 강화한다. 표 20은 텍스트 컨디셔닝을 통합하는 것이 훈련될 때 시각적 접지에서 +0.6의 적당한 개선을 초래한다는 것을 입증한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Task & \\multicolumn{1}{c|}{Object Detection} & Instance Segmentation & Semantic Segmentation & Image Captioning & Visual Grounding \\\\ \\hline Decoding Step & 5 & 31 & 16 & 20 & 4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 16: 다섯 가지 작업 모두에 대한 디코딩 단계.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Task & Resolution & Grid Number & Decoding Step & FPS \\\\ \\hline Object Detection & 1120 \\(\\times\\) 1120 & 625 & 5 & 2.5 \\\\ Instance Segmentation & 1120 \\(\\times\\) 1120 & 625 & 31 & 0.7 \\\\ Semantic Segmentation & 672 \\(\\times\\) 672 & 1764 & 16 & 1.5 \\\\ Image Captioning & 224 \\(\\times\\) 224 & 1 & 20 & 3.2 \\\\ Visual Grounding & 224 \\(\\times\\) 224 & 1 & 4 & 8.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 17: A100 상의 GiT-B의 추론 속도.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Method (ADE20K) & Resolution & \\#Params & FPS \\\\ \\hline SAM-B [41] & 672 \\(\\times\\) 672 & 90M & 1.6 \\\\ GiT-B & 672 \\(\\times\\) 672 & 131M & 1.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 18: 시맨틱 세분화 태스크에 대한 SAM과의 잠복성 비교.\n' +
      '\n' +
      '독립적으로요 그러나 그 영향은 다중 작업 훈련에서 더 크게 증가하여 +7.2의 현저한 향상을 보여 우리의 가설과 일치한다.\n' +
      '\n' +
      '**빔 탐색.** 표 21은 빔 탐색에서 상이한 빔 크기에 따라 성능이 어떻게 달라지는지를 보여준다. 우리는 빔 크기가 1에서 2로 증가함에 따라 개선되는 것을 관찰하지만 CIDEr의 약간의 감소만으로 성능이 2에서 5 사이에서 안정화된다. 더 큰 빔 크기가 더 긴 추론 시간으로 이어진다는 점을 감안할 때, 우리는 2의 기본 빔 크기를 선택했다.\n' +
      '\n' +
      '**Mass Center and Ray Number.** 표 22는 인스턴스 분할 설정의 삭제를 제시한다. 매스 중심을 사용하면 상자 중심보다 더 나은 결과가 나오는데, 아마도 상자 중심이 객체 외부에 떨어질 수 있기 때문일 것이다. 36개의 광선을 사용하면 성능이 약간 향상되지만 상당한 훈련 시간이 소요됩니다.\n' +
      '\n' +
      '## 부록 0.F 비교방법별 특정모듈\n' +
      '\n' +
      '표 23에서 우리는 방법 비교를 위해 사용되는 특정 모듈과 매개변수 양을 요약한다. 전문가 모델이든 일반 모델이든 상관없이 많은 방법은 작업에 특화된 모듈과 모달리티 특화 인코더를 설계에 통합한다. 대조적으로, 우리의 접근법은 그러한 복잡한 디자인에 의존하지 않기 때문에 단순성이 특징이다.\n' +
      '\n' +
      '## 부록 0.G 가시화\n' +
      '\n' +
      '**Task Visualization.** 그림 9에서 각 작업에 대한 예를 시각화하여 이미지 입력, 텍스트 형식의 예측 및 예측 결과의 시각화를 왼쪽에서 오른쪽으로 보여줍니다. 단순화를 위해 모델에 의해 예측된 로컬 응답의 몇 가지 예를 선택하고 해당 텍스트 형식 예측을 나열했다.\n' +
      '\n' +
      '**Zero-shot Visualization.** 그림 10에서 GiT-H\\({}_{\\text{universal}}\\)에 의해 만들어진 제로-shot 데이터셋에 대한 예측의 정성적인 예를 보여준다. 특히, 우리의 모델은 일부 경우에 누락된 주석을 정확하게 예측한다. 예를 들어, 도시 풍경에서\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline Model & Text Conditioning & Acc@0.5 \\\\ \\hline GiT-B\\({}_{\\text{single-task}}\\) & & 82.7 \\\\ GiT-B\\({}_{\\text{single-task}}\\) & ✓ & 83.3 \\\\ GiT-B\\({}_{\\text{multi-task}}\\) & & 78.6 \\\\ GiT-B\\({}_{\\text{multi-task}}\\) & ✓ & 85.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 20: 시각적 접지 작업에 대한 텍스트 컨디셔닝의 절제.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline \\hline Dataset & Size & Category Number & Support Set Size & Training Iters \\\\ \\hline DRIVE [79] & 40 & 2 & 10 & 100 \\\\ LoveDA [87] & 5,987 & 7 & 35 & 100 \\\\ ISPRS Potsdam [39] & 5,472 & 6 & 30 & 100 \\\\ WIDERFace [96] & 32,203 & 1 & 5 & 50 \\\\ DeepFashion [57] & 800,000 & 15 & 75 & 100 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 19: 소수의 샷 데이터 세트.\n' +
      '\n' +
      '검출, 저조도 조건에서도 주석이 없는 자전거와 차량을 정확하게 식별합니다. SUN RGB-D 분할에서도 유사한 정확도가 관찰되며, 여기서 모델은 두 개만 주석이 달렸지만 모든 의자를 감지한다. 시티스케이프 분할에서 자체 소유 차량을 주석에서 제외하는 데이터 세트의 편향에도 불구하고, 본 모델은 최소 정보에 의존하고 데이터 세트별 미세 조정 없이 이러한 차량을 올바르게 분류함으로써 예외적인 일반화를 보여준다.\n' +
      '\n' +
      '**Few-shot Visualization.** 그림 11은 GiT-H\\({}_{\\text{universal}}\\)가 소수의 샷 데이터 세트에 대해 수행한 정성적 예측의 시각적 표현을 제공한다. 이러한 예는 데이터가 제한된 상황에서 우리 모델의 놀라운 성능을 강조하여 다양한 도메인에 걸친 응용 가능성을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Box Center & Mass Center & Ray Number & mAP & Training Time \\\\ \\hline ✓ & & 24 & 29.0 & 32h \\\\ ✓ & & 36 & 29.2 & 49h \\\\  & ✓ & 24 & 31.4 & 32h \\\\  & ✓ & 36 & 31.7 & 49h \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 22: 인스턴스 분할 설정에 대한 절제.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c} \\hline \\hline Methods & \\multicolumn{3}{c|}{Specific Modules} & Num & \\#Params \\\\ \\hline _Specialist Models_ & \\multicolumn{3}{c|}{} \\\\ Faster R-CNN-FPN [73] & ResNet,FPN,RPN,ClassificationHead,RegressionHead & 5 & 42M \\\\ DETR-DCS [13] & ResNet,Encoder,Decoder,ClassificationHead,RegressionHead & 5 & 41M \\\\ Deformable-DETR [106] & ResNet,Encoder,Decoder,ClassificationHead,RegressionHead & 5 & 40M \\\\ Mask R-CNN [36] & ResNet,FPN,RPN,RPN,ClassificationHead,RegressionHead & 6 & 46M \\\\ Polar Mask [90] & ResNet,FPN,ClassificationHead,CartnessHead,RegressionHead & 5 & 53M \\\\ Mask2Former [25] & ResNet,FinalDecoder,TransformerDecide,ClassificationHead,MaskHead & 5 & 44M \\\\ Pri\n' +
      '\n' +
      '## 부록 0.H 토론\n' +
      '\n' +
      '**푸유-8B와의 비교.** 잘 탐색된 비전 언어 작업에 초점을 맞춘 푸유-8B[7]에 비해, 우리의 GiT는 범용 언어 인터페이스를 가진 자주 간과되는 객체 및 픽셀 수준 작업으로 다층 변압기의 범위를 확장한다. 이를 위해 다양한 지각적 스케일에 걸쳐 태스크 통합을 위한 포인트 프롬프트를 사용하여 유연한 병렬 디코딩 템플릿을 설계한다. 또한 지역 이미지 프롬프트를 도입하여 세립 인식 능력을 향상시킵니다.\n' +
      '\n' +
      '**어댑터 기반 방법과 비교.** 우리의 방법은 LVM에 대한 대체 솔루션을 제공한다. LLM으로 기존 미세 조정 노력과 달리 시각과 언어의 구조적 격차를 줄이는 것을 목표로 합니다. 또한, 우리의 GiT는 모듈별 설계 없이 간단한 종단간 구현을 가능하게 하여 트레이닝 프로세스 및 모델 스케일링을 크게 단순화한다.\n' +
      '\n' +
      '**제한.** 비교적 간단한 작업 프롬프트가 있는 선택된 5개의 작업으로 제한된 훈련 데이터에 의해 구속된 GiT는 제로 샷 설정에서 완전히 새로운 작업으로 일반화하기 위해 고군분투한다. 능력 있는 LLM에 대해서도 작업 수준의 제로샷은 여전히 도전적입니다. GiT는 그것과 밀접하게 정렬되고 이 제한을 상속한다. 그러나 우리의 GiT는 관련 데이터를 통합함으로써 잠재적으로 다양한 다른 작업을 지원할 수 있는 작업 통일에서 강력한 확장성을 보여준다.\n' +
      '\n' +
      '**Negative Societal Impact.** 당사의 가장 큰 모델은 96 A100 GPU에 대한 7일간의 교육을 필요로 하여 상당한 탄소 배출로 이어집니다. 또한 생성된 콘텐츠는 인간의 선호도와의 정렬 부족으로 인한 훈련 데이터의 편향을 반영할 수 있다.\n' +
      '\n' +
      '그림 9: 5가지 표준 시각 중심 태스크의 시각화.\n' +
      '\n' +
      '그림 10: 제로샷 데이터셋에 대한 질적 결과. 확대하여 더 잘 볼 수 있도록 합니다.\n' +
      '\n' +
      '그림 11: 소수의 샷 데이터 세트에 대한 질적 결과. 확대하여 더 잘 볼 수 있도록 합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>