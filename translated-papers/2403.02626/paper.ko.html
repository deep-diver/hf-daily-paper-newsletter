<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Collaborator 모델링: LLM Tool-Use를 통한 인간의 최소노력을 이용한 주관적 시각 분류\n' +
      '\n' +
      'Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal, Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou, Ranjay Krishna, Ariel Fuxman, Tom Duerig\n' +
      '\n' +
      '워싱턴대학교 구글리서치\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '콘텐츠 조절에서 야생 동물 보호에 이르기까지 미묘한 또는 주관적인 시각적 개념을 인식하기 위해 모델이 필요한 응용 프로그램이 증가하고 있다. 전통적으로 이러한 개념에 대한 분류기를 개발하려면 훈련에 필요한 데이터를 식별하고 주석을 달기 위해 시간, 일 또는 심지어 몇 달 단위로 측정된 상당한 수동 노력이 필요하다. 이미지 분류기의 신속한 부트스트래핑을 가능하게 하는 최근 제안된 애자일 모델링 기법에도 불구하고, 사용자는 단일 분류기를 훈련시키기 위해 여전히 30분 이상의 단조롭고 반복적인 데이터 레이블링을 필요로 한다. 피스크의 인지 미저 이론을 바탕으로 인간의 레이블링을 자연어 상호작용으로 대체하여 개념을 정의하는 데 필요한 총 노력을 줄이는 새로운 프레임워크를 제안한다. 우리의 프레임워크는 대화와 훈련 데이터 포인트의 자동 레이블링을 통해 개념 공간을 조각하기 위해 대규모 언어 모델과 비전 언어 모델 모두 기반 모델의 최근 발전을 활용한다. 가장 중요한 것은 우리의 프레임워크가 크라우드소싱 주석이 필요하지 않다는 것이다. 또한, 본 프레임워크는 궁극적으로 비용에 민감한 시나리오에서 배포할 수 있는 경량 분류 모델을 생성한다. 15개의 주관적 개념과 2개의 공공 이미지 분류 데이터 세트에서 훈련된 모델은 ALIGN, CLIP, CuPL과 같은 최첨단 제로 샷 분류 모델 및 PaLI-X와 같은 대형 시각적 질문 응답 모델뿐만 아니라 전통적인 애자일 모델링을 능가한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '컴퓨터 비전 분야는 주로 개, 고양이 또는 자동차와 같이 객관적으로 합의된 개념을 인식하는 데 중점을 두었다[11, 28, 30]. 세립 인식(예: "검은 발 알바트로스" 및 구성 개념(예: "오토바이 옆에 있는 빨간색 자동차")에 대한 연구조차도 보편적인 합의[22, 27, 32, 35]를 가지고 있다. 그러나 많은 실제 비전 응용 프로그램은 종종 개인 간의 상당한 불일치로 고통받는 주관적인 개념을 인식하는 것을 포함한다. 애플리케이션은 감정 예측, 미적 호소력 측정, 또는 콘텐츠 조절(10, 25, 26, 45)을 포함한다. 콘텐츠 중재자는 식품 비평가인 _unsafe_에 해당하는 것에 대한 정의에 따라 안전하지 않은 콘텐츠를 식별하기 위한 모델이 필요하다.\n' +
      '\n' +
      '그림 1: 우리는 모델링 협력자: 자연 언어 상호 작용과 최소한의 노력을 사용하여 누구나 비전 모델을 훈련시킬 수 있는 프레임워크를 소개한다. 우리는 오늘날 최고의 모델(예: PaLI-X[6])이 미식가 참치와 같은 주관적인 개념을 분류할 때 프롬프트에 따라 답변을 변경한다는 것을 보여준다. 한편, 모델링 협업자는 LLM과 도구 사용을 사용하여 사용자와 상호작용하여 컨셉 공간을 개척함으로써 비전 모델을 훈련한다.\n' +
      '\n' +
      '참치 샌드위치는 미식가라고 생각하지 않는 반면 다른 것들은 미식가라고 생각할 수 있다(도 1). 이러한 응용 프로그램을 운영하기 위해서는 누구나 주관적 시각 모델을 훈련할 수 있는 사용자 중심의 훈련 프레임워크가 필요하다.\n' +
      '\n' +
      '최근 애자일 모델링은 사용자-인-루프 프레임워크를 통해 모든 시각적 개념을 비전 모델로 전환하는 프로세스를 공식화했다[51]. 그들의 작업은 크라우드 워커들이 사용자의 개념 정의와 일치하는 라벨을 생산하기 위해 고군분투했다고 결론지었다. 대신, 그들은 능동적 학습 알고리즘을 제안했는데, 여기서 사용자는 일련의 훈련 이미지 자체에 반복적으로 라벨을 붙인다. 불행하게도, 이 과정은 지루하고, 반복적이며, 노동 집약적이다; 사용자는 이진 분류기를 훈련시키는데 평균 30분이 소요된 \\(\\sim 2000\\) 이미지에 라벨을 붙여야 했다.\n' +
      '\n' +
      '기존 프로세스는 인간이 보유한 핵심 능력을 활용하지 않기 때문에 부족합니다. 사람들은 1차 논리를 적용하여 복잡한 주관적 개념을 보다 다루기 쉽고 객관적인 구성요소로 분해하는 데 능숙하다[14, 36]. 이러한 능력은 Susan Fiske의 Cognitive Miser Theory: 사람들은 높은 인지 부하를 피하기 위해 복잡한 작업을 분해한다[13]. 사람들은 "안전하지 않다"와 "미식가"와 같은 복잡한 개념을 정의하기 위해 동일한 프로세스를 적용한다. 예를 들어, 한 음식 비평가는 "미식가"라는 주관적인 개념을 적어도 "참치"를 포함해야 하는 이미지로 분해할 수 있으며, "아히 참치"인 경우 미식가일 가능성이 높으며, "통조림"인 경우 미식가일 가능성이 낮고, "샌드위치"인 경우 여전히 미식가가 아니다. 주제 개념 "미식가"를 객관적인 개념 "아히 참치", "통조림", "샌드위치"의 연결 조항으로 분해하는 것은 단순한 _non_-aborious, 인지적으로 쉬운 변환이다.\n' +
      '\n' +
      '이 접지를 통해 사용자가 수동 노력을 최소화하면서 분류기를 구축할 수 있는 **모델링 협업자**를 제공합니다. 사용자가 수천 개의 이미지에 주석을 달도록 요구하는 대신 [51] 모델링 협력자는 주관적인 개념을 구성 하위 구성요소로 분해하는 데 도움이 되는 몇 가지 자연 언어 상호 작용과 함께 \\(100\\)을 필요로 한다. 모델링 협업자를 활성화하기 위해 대형 언어 모델(LLM)[2, 3, 9, 12, 37]의 발전, 특히 비전 언어 모델(VLM)[6, 7, 8] 및 기타 도구[19]를 사용하는 능력을 활용합니다. 사용자가 개념을 염두에 두고 모델링 공동 작업자를 사용할 때 LLM을 사용하며, LLM은 개념을 VQA(Visual Question Answering) 모델에 대해 소화 가능한 질문으로 나눈다[8]. 그런 다음 LLM은 VQA 모델에서 제공한 답변을 요약하고 생각 사슬[57]을 통해 추론을 수행하여 새로운 이미지를 개념의 긍정적 또는 부정적 사례로 분류한다. 사용자는 작은 \\(100\\) 이미지 유효성 검사 세트에 수동으로 레이블을 지정해야 합니다. 마지막으로, 모델링 공동작업자는 온라인에서 사용할 수 있는 많은 양의 레이블이 지정되지 않은 이미지에 레이블을 지정하고 이를 증류 데이터로 사용하여 경량 배치 준비 비전 모델을 훈련한다.\n' +
      '\n' +
      '제안된 방법은 기존의 제로샷 방법(CLIP[43], CuPL[41] 및 PaLI-X[6])을 능가하는 것으로 나타났으며, 특히 더 단단한 주관적 개념에서 더 우수한 성능을 보였다. 기존의 애자일 모델링[51]과 비교할 때, 우리의 시스템은 하드 컨셉에 대한 크라우드레이터의 품질을 능가하는 동시에 수동 사용자 제공 지상 진리의 필요성을 수십 배 줄인다. 분류 모델을 개발하는 데 필요한 수동 노력과 그에 따른 비용의 장벽을 줄임으로써 사용자가 아이디어를 신속하게 현실로 전환할 수 있도록 권한을 부여할 것이다. 이것은 차례로 최종 사용자 애플리케이션의 새로운 물결을 안내할 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '우리의 작업은 VLM 및 LLM의 발전을 기반으로 하며 최근에 도입된 애자일 모델링 문제에 대한 개선된 솔루션을 제공한다.\n' +
      '\n' +
      '**애자일 모델링.**애자일 소프트웨어 개발에 영감을 받아, 애자일 모델링[51]은 이미지 분류 모델의 신속한 개발에 중점을 두고 있다. 애자일 모델링은 속도 외에도 주관적인 시각 모델이 제기하는 문제를 해결하는 것을 목표로 한다. 분류 작업이 더 미묘해짐에 따라 사용자 상호 작용이 점점 더 중요해지고 있다. 그러나, 인간-인-루프 접근법은 지속적인 인간 관여 및 전문 지식의 필요성으로 인해 비용이 많이 들 수 있다는 점에 유의하는 것이 중요하다. 이 작업은 사용자가 분류 모델을 조정하는 데 소비하는 시간을 줄이는 것을 목표로 하지만, 파이프라인의 일부를 자동화하고 군중 측정기의 관여를 제거하는 보조 방법을 제안한다.\n' +
      '\n' +
      '**VLMs.**VLM의 빠르게 진화하는 도메인에서 대조 모델과 생성 모델의 두 가지 기본 스트림이 나타났다. CLIP[43] 및 ALIGN[23]과 같은 대조 모델은 대규모 데이터 세트를 활용하여 원시 텍스트에서 시각적 개념을 직접 학습하여 개방형 어휘[11, 17]에 대한 높은 정확도의 제로 샷 분류를 가능하게 한다. PaLI[6, 7, 8, 56] 및 GPT-V[37, 38]과 같은 생성 모델은 시각적 및 텍스트 입력의 조합으로부터 텍스트를 생성하는 것에 초점을 맞춘다. 예를 들어, 다양한 언어의 방대한 이미지-텍스트 쌍 집합에 대해 훈련된 PaLI는 다양한 비전 및 언어 작업에 걸쳐 최고의 성능을 달성한다. 유사하게, GPT-V는 이미지 입력들의 처리를 허용함으로써, 멀티모달 태스크들에 대한 언어 모델들의 적용성을 향상시킨다. CoCa[54, 63]과 같은 다른 방법은 생성적 목표와 대조적 목표를 동시에 학습하는 하이브리드 접근법을 제안했다. 강력함에도 불구하고 VLM은 시각적 데이터 의미를 포착하여 종종 미묘한 시각적 신호보다 두드러진 이미지 특징을 우선시한다. 예를 들어, CLIP 임베딩은 그것의 가장 두드러진 주제를 캡슐화하기 위해 의도적으로 압축된다[49]. 또한, PaLI는 훈련 데이터가 주로 상세한 주석이 부족하기 때문에 수많은 객체를 가진 복잡한 장면에 대한 상세한 설명을 제공하는 데 어려움을 겪을 수 있다. 대조적으로, 제안된 방법은 그림 1에서 관찰된 바와 같이 질문 표현에 더 안정적이고 덜 민감하다.\n' +
      '\n' +
      '**대형 언어 모델(LLM) 및 도구 사용**대형 언어 모델(LLM)은 특히 자연어 처리(NLP) 및 인지 추론 분야에서 인공 지능[1, 3, 12, 40, 55]의 풍경에 혁명을 일으켰다. 사상 연쇄 추론[57], 소수 샷 학습[4, 39], 도구 사용[21, 46]과 같은 고급 방법론을 활용함으로써 이러한 모델은 광범위한 다운스트림 작업[44]에서 탁월한 성능을 보여준다. 그들은 추가 훈련 없이도 높은 성능을 유지하면서 다양한 모달리티와 광범위한 응용 분야에 걸쳐 작동할 수 있습니다. 최근 외부 도구를 LLM[5, 20, 21, 29, 62]과 통합하는 과정에서 툴포머[46]와 같은 시스템이 생성되었습니다. 이 접근법은 어떤 API를 호출할지, 타이밍, 통과된 주장 및 결과를 미래의 토큰 예측으로 후속 동화시키는지에 대해 지능적인 결정을 내린다. 이는 다양한 작업에 걸쳐 제로샷 성능을 향상시켜 LLM이 고유한 기능을 넘어 작동할 수 있는 견고한 기반을 구축합니다. 세밀한 VQA를 위해 AVIS [20]은 자율 정보 추구 메커니즘을 도입한다. 외부 도구와 함께 LLM을 동적으로 활용함으로써 조합 검색 공간을 능숙하게 횡단합니다. 이는 LLM의 전략적 의사 결정을 안내하는 전이 그래프를 만들어 인간의 의사 결정 과정을 모방하는 독특한 접근 방식을 통해 달성된다. 또 다른 도구 사용 가능 LLM 시스템은 시각 쿼리를 다루는 혁신적인 접근법을 구현하는 ViperGPT[52]이다. 파이썬 코드 생성을 통해 비전과 언어 모델의 원활한 통합을 가능하게 하는 코드 생성 전략을 활용한다. 이 방법은 다른 유사한 방법(MMReact[61], HuggingGPT[50], Chameleon[34], Visual ChatGPT[58])과 함께 확장된 훈련의 필요성을 우회하고 다양한 시각적 작업 세트에 걸쳐 회복력을 보장한다. 종합적으로, 이러한 시스템은 LLM과 외부 도구 사용 사이의 급증하는 시너지 효과를 강조하여 LLM이 달성할 수 있는 것의 국경을 밀어붙입니다. 본 연구에서는 주관적 이미지 분류의 문제를 해결하기 위해 이러한 접근법에서 아이디어를 채택하고 확장한다.\n' +
      '\n' +
      '**언어 모델을 통한 맞춤형 프롬프트.**언어 모델(CuPL)을 통한 맞춤형 프롬프트[41]은 CLIP의 능력[43]을 활용하여 제로샷 이미지 분류를 달성한다. CuPL은 이미지와 각 시각적 클래스 간의 유사도를 측정하여 분류를 수행한다. 전형적으로, 클래스들은 클래스 새에 대한 "새의 사진"과 같은 템플릿 내의 CLIP의 텍스트 인코더로 전달된다. CuPL은 GPT[3]를 사용하여 CLIP에 공급하기 전에 각 클래스에 대해 보다 포괄적인 텍스트 설명을 생성한다. 이 간단하고 제로 샷 접근법은 다양한 제로 샷 이미지 분류 벤치마크에 걸쳐 향상된 정확도를 산출한다. 그러나 그 평가는 미묘한 시각 분류 또는 주관적인 시각 분류 작업이 아닌 객관적인 분류 작업에 국한되어 왔다. 데이터에 자동으로 주석을 달기 위한 이 접근법은 CLIP 시 개선되지만 작업에 비해 동일한 한계를 겪는다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '본 논문에서는 기존의 분류기 개발 방법의 한계를 해결하기 위해 미묘한 시각적 개념을 위한 분류기 개발을 간소화하는 종단간 시스템을 제안한다. 시스템은 다음 하위 섹션에 자세히 설명된 세 가지 핵심 구성 요소로 구성됩니다. (a) 데이터 min\n' +
      '\n' +
      '도 2: 공동작업자 주석기 시스템 모델링. 주어진 이미지, 개념 이름 및 설명에 대해 주석기는 양수 또는 음수 레이블을 출력합니다. 개념의 이름과 설명을 기반으로 LLM은 VQA 모델(우리의 경우 PaLI VQA)을 질문하기 위해 관련 원자 질문을 생성한다(단계 A). 이러한 질문들은 일반적으로 예/아니오 단답을 출력하는 VQA 모델에 공급된다(단계 B). 추가적으로, 우리는 이미지로부터 가능한 한 상세한 내용을 캡처하는 상세한 설명을 생성하기 위해 PaLI(Step C)의 캡션 버전을 사용한다. 마지막으로 LLM은 사고 연쇄 추론 과정을 거쳐 결정과 근거를 출력한다(단계 D).\n' +
      '\n' +
      '(b) 주석, (c) 능동적 학습을 통한 모델 학습.\n' +
      '\n' +
      '새로운 개념에 대한 분류기를 구축하기 위해, 사용자는 먼저 개념 이름 및 선택적 설명을 제공한다. 그런 다음 시스템은 개념과 관련된 이미지를 자동으로 채굴하고 LLM(Large Language Models), VLM(Vision-Language Models) 및 VQA(Visual-Question-Answering) 모델의 혼합물을 사용하여 주석을 달 수 있다. 주석이 달린 이미지는 기본 분류 모델을 훈련하는 데 사용되며, 이는 여러 라운드의 능동적 학습을 통해 더욱 정제되어 매우 정확한 분류기를 생성한다.\n' +
      '\n' +
      '이 설정은 전통적인 분류기 개발의 작업 흐름을 반영하지만, 전통적인 방법에서 중요한 병목 현상인 비용이 많이 들고 시간이 많이 걸리는 인간 주석이 필요하지 않다. LLM 및 VLM에 의해 구동되는 모델링 공동작업자 주석기 컴포넌트는 제로 샷 이미지 라벨링을 가능하게 하고 사용자 주석에 대한 의존성을 획기적으로 최소화한다.\n' +
      '\n' +
      '### Data mining\n' +
      '\n' +
      '교육을 위한 품질 데이터를 마이닝하는 것은 전통적으로 노동 집약적인 프로세스였다. 이 과정은 개념에 대한 명확한 정의에서 시작하여 관련 이미지에 대한 검색으로 시작하여 이러한 각 이미지의 수동 주석[30, 11]에서 종료된다. 특히 미묘한 시각적 작업의 경우 데이터 수집 중에 특정 미묘한 시각적 패턴이 간과될 가능성이 있다. 결과적으로, 모든 시각적 패턴의 포괄적인 캡쳐를 보장하기 위해, 미세화의 다중 반복이 필요할 수 있다. 전통적인 애자일 모델링[51]에서 이 과제는 데이터에 주석을 달거나 더 많은 이미지 예를 찾기 위해 새로운 검색 쿼리를 생성하기 위해 _users_를 요청함으로써 해결된다. 각각의 쿼리는 공개 도메인(LAION Dataset)으로부터 주석을 위한 다른 유사한 포지티브 이미지 예들을 수집하기 위해 새로운 시맨틱 이미지 검색 알고리즘[43, 23]을 초래한다[47]. 사용자 개입에도 _user_ 쿼리는 필수 단서를 간과할 수 있으며, 이는 잠재적으로 특정 시각적 모드에서 하드 네거티브의 부족 또는 커버리지의 부족을 초래할 수 있다. 또한, 라벨은 사용자마다 다를 수 있어 잠재적인 인간 편향으로 이어질 수 있다.\n' +
      '\n' +
      '인간 편향을 해결하고 수동 노력을 최소화하기 위해 LLM 사고 연쇄 추론 기반 데이터 마이닝 알고리즘을 제안한다. LLM은 본질적으로 편향되지 않고[15] 훈련 데이터에 존재하는 편향을 반영할 수 있지만 광범위한 지식 기반에서 대규모로 더 넓은 범위의 개념을 평가할 수 있으므로 잠재적인 예제의 더 광범위한 배열을 보다 효율적으로 식별할 수 있다. 먼저, LLM이 개념의 이름과 설명을 기반으로 여러 개의 긍정 및 부정 쿼리를 생성하도록 촉구한다. 우리는 쿼리에 기초하여 이미지를 긍정 또는 부정으로 직접 할당하지 않으며, 오히려 목표는 긍정 및 하드-부정 예제 모두에 걸쳐 있는 대표적인 이미지를 얻는 것이다. 커버리지와 다양성을 높이기 위해 LLM에 다양한 _mutations_를 적용하도록 지시하여 질의를 확장한다. 예를 들어, 우리는 LLM에게 쿼리들의 더 넓거나 더 좁은 버전들을 반복적으로 생각해내도록 요청하거나, 쿼리들의 특정 부분들에 대한 변형들을 생각해낼 수 있다. 애자일 모델링과 평행선을 그리며 각 쿼리를 사용하여 공개 도메인에서 이미지 샘플을 추출합니다[47].\n' +
      '\n' +
      '### 모델링 공동작업자 주석기\n' +
      '\n' +
      '도. 도 2는 이미지 주석 프로세스를 설명한다. 본 시스템은 VLM 및 기타 도구를 호출하는 LLM의 기능을 활용하여 주석 프로세스를 효과적으로 조정합니다. 그것은 LLM, Captioning VLM[56] 및 VQA VLM[6]의 세 가지 기본 AI 구동 모듈로 구성된다. 자동화된 주석 프로세스는 다음과 같이 구성된다:\n' +
      '\n' +
      '**개념 초기화**: 초기에, 우리의 시스템은 개념 이름(예를 들어, 미식가 참치), 및 선택적으로 개념 설명을 수신한다. 개념 설명이 없는 경우 LLM은 초기 설명을 생성합니다. 이 템플릿은 사용자가 모든 사양 및 조각 작업을 포함하도록 수정할 수 있습니다.\n' +
      '\n' +
      '**속성 추출**: 컨셉 스펙에 기초하여, LLM은 컨셉과 연관된 객관적 속성들, 예컨대 "이미지가 참치를 포함한다", "참치 샌드위치이다", "참치 스테이크이다"를 식별한다.\n' +
      '\n' +
      '**속성 분해**: LLM은 복잡한 속성을 보다 세분화된 속성과 원자적인 속성으로 분해한다.\n' +
      '\n' +
      '**질문 생성**: LLM은 그 후 VQA 모델에 맞춘 일련의 질문을 공식화한다. 예를 들면, "이미지에 음식이 포함되어 있음", "음식이 참치임", "참치 스테이크임" 등이 있다.\n' +
      '\n' +
      '**시각적 평가**: 이미지가 입력되면 VQA 모델은 이러한 질문을 처리하여 각각에 대해 간결한 답변을 산출한다. 동시에 캡션 VLM은 이미지에 대한 포괄적인 설명을 제공합니다.\n' +
      '\n' +
      '**최종 주석**: VLMs로부터의 텍스트 데이터와 사용자의 초기 개념 명세를 가지고, LLM은 사상 연쇄 추론을 사용한다. 이미지에 긍정적이거나 부정적인 것으로 주석을 달며 의사 결정 과정에 대한 통찰력을 제공합니다.\n' +
      '\n' +
      '우리의 접근법은 VLM, VQA 및 LLM 모델의 장점을 활용하면서 동시에 단점을 피한다. 예를 들어 VLM은 능력에도 불구하고 분류 작업에서 미묘한 개념과 주관적인 개념으로 어려움을 겪는 경우가 많다. 그들의 성능은 훈련 데이터의 폭과 품질에 달려 있으며, 잠재적으로 이해의 편향이나 격차로 이어진다[53]. 언어의 모호성과 특정 질문의 고유한 주관성은 그들의 정확성에 더욱 도전할 수 있다[33]. 더욱이 현실 세계의 맥락과 경험적 이해가 부족한 이러한 모델은 더 깊은 문화적 또는 정서적 뉘앙스를 놓칠 수 있다[16]. 따라서 강력하지만 VLM은 복잡하거나 주관적인 시각적 언어 작업을 해결하는 데 고유한 한계를 가지고 있다. 도. 도 1은 프롬프트에 대한 예시적인 VLMs\'(PaLI-X[6]) 감도를 도시한다.\n' +
      '\n' +
      'VLMs는 광고 기반 LLMs[33, 42, 53, 59]의 전형적인 깊은 사고 연쇄 추론을 수행하기보다는 시각적 콘텐츠와 관련된 질문을 이해하고 답변하기 위해 주로 설계되었다. VLM은 이미지에 대한 더 간단한 질문을 이해할 수 있지만 일반적으로 단일 샷 방식으로 작동하여 확장된 추론 없이 즉각적인 시각적 및 텍스트 입력을 기반으로 답변을 제공한다. 반면 LLM 질문 응답 품질은 확장 텍스트에 걸쳐 일관된 사고 라인을 유지하면서 사고 연쇄 추론을 통해 크게 향상될 수 있다. 프롬프트 체인링과 같은 다른 기법들은 지속적인 대화 또는 반복적인 추론을 시뮬레이션하는, 후속 입력의 일부로서 모델의 출력을 사용하는 것을 포함한다. 또한 더 깊은 통찰력을 추출하기 위해 사용자는 모델에 단계별[60]을 생각하도록 요청하거나 장단점을 따져 보다 의도적인 추론 프로세스를 시뮬레이션하는 것과 같은 특정 지침으로 LLM을 안내할 수 있다[3].\n' +
      '\n' +
      '### 훈련 및 능동적 학습\n' +
      '\n' +
      '주석을 모델로 직접 사용할 수 있지만 추론 비용이 높기 때문에 많은 시나리오에서 금지된다. 이러한 이유로 모델 학습과 능동적 학습을 위해 [51]과 유사한 접근법을 채택한다. 구체적으로, 먼저 기초 비전 모델(CLIP 또는 ALIGN)로부터 이미지 특징을 추출한다[23, 24]. 그런 다음, 주어진 개념에 대한 이진 분류를 수행하기 위해 계층 크기\\((128,128,128)\\)를 갖는 얕은 다층 퍼셉트론(MLP)을 훈련한다. 이는 LLM 기반 주석을 교사 모델로 사용하는 학생-교사 증류[18]로도 볼 수 있다. 학습률은 \\(3\\times 10^{-4}\\), 배치 크기는 \\(512\\)이며 AdamW [31]을 사용하여 최적화한다.\n' +
      '\n' +
      '초기 모델이 훈련된 후, 우리는 여러 라운드의 능동 학습을 수행한다. 각 능동 학습 반복은 세 단계로 구성된다. 먼저, 라벨링되지 않은 이미지의 대규모 데이터베이스에 경량 분류 모델을 적용한다(LAION[47]) 그런 다음 계층화된 샘플링을 수행하여 추가 AL 라운드에 대한 후보 이미지를 획득한다[51]. 정밀도와 회상을 각각 끌어올릴 하드 네거티브와 하드 포지티브를 포착하겠다는 취지다. 둘째, LLM 기반 주석은 선택된 이미지에 자율적으로 적용되어 추가적인 훈련 지상진실을 제공한다. 셋째, 학생 분류기는 현존하는 모든 라벨링된 데이터를 활용하여 재학습된다. 이 능동 학습 단계에서 예를 채굴하기 위해 마진 샘플링과 계층화된 샘플링 기술[48]을 모두 실험한다. 따라서 전체 시스템은 탐색(텍스트 검색 쿼리 및 확장을 통한 데이터 마이닝을 통해 달성)과 활용(모델 불확실성을 줄이는 시각적 모드를 채굴하는 능동 학습을 통해 달성) 사이의 균형을 맞춘다.\n' +
      '\n' +
      '### Implementation details\n' +
      '\n' +
      '큰 언어 모델로서, 우리는 다양한 다른 작업에 대해 훈련된 PaLM 2 [2, 9]를 사용하는데, 이는 모두 PaLM 2가 언어의 다른 측면을 학습하는 데 도움이 된다. 또한, 우리는 PaLI-X[6]의 VQA 및 MMIT(다중 모드 명령어 조정 [56]) 변형을 모두 사용한다. 기초 모델의 특별한 선택은 작성 당시의 SOTA 성능에 기초한다. 이 모델들은 이 작업에서 더 이상 훈련되거나 미세 조정되지 않았다.\n' +
      '\n' +
      '도 3: 미식가 참치(첫 번째 행) 및 정지 부호(두 번째 행) 개념에 대한 모델링 협력자 주석기 예. LAION 데이터 세트에서 마이닝된 하드 네거티브는 시각적 개념에 대한 일부 실제 포지티브와 함께 표시된다. 모델링 공동작업자 주석기는 이미지를 긍정 또는 부정으로 레이블 지정할 수 있을 뿐만 아니라 근거를 제공할 수 있습니다. 어떤 경우에는 VQA 응답의 오류 또는 LLM의 환각으로 인해 근거가 잘못(빨간색으로 강조)될 수 있다. 간결함을 위해 일부 이유들이 잘렸다\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 세 번의 테이크아웃으로 실험 설정과 결과를 제시한다. 먼저, 모델링 협업기 Annotator가 다른 zero-shot 방법(CLIP[43], CuPL[41] 및 PaLI-X[6])보다 우수함을 보인다. 둘째, Modeling Collaborator Annotator는 쉬운 개념과 어려운 개념 모두에서 최첨단 방법을 능가할 수 있지만, 더 어렵고 주관적인 개념에서 훨씬 더 큰 이득을 볼 수 있습니다. 마지막으로 엔드 투 엔드 시스템을 사용할 때 최소 _user_ 주석(100개의 주석 대 2,000개의 전통적인 애자일 모델링)을 사용하여 경쟁력 있는 품질의 배포 가능한 모델을 생성할 수 있다.\n' +
      '\n' +
      '**Datasets.** 우리 시스템에서 데이터 마이닝에 사용되는 LAION 데이터셋 외에도 공개 Hateful Memes 데이터셋에서 우리의 방법을 평가한다[26]. 평가 및 사용자 연구를 위해 LAION 데이터셋에서 마이닝된 긍정 이미지와 부정 이미지가 각각 포함된 14개의 개념으로 구성된 애자일 모델링 데이터셋[51]을 사용한다. 이 데이터셋은 [51]에서 설명한 바와 같이 CLIP를 사용하여 각 개념에 대한 제로샷 성능에 따라 _easy_와 _hard_ 개념으로 분할된다.\n' +
      '\n' +
      '**Models.** 우리는 생성 VQA 모델로 CLIP[43], CuPL[41], PaLI-X(55B)[6]의 최신 제로 샷 및 개방형 어휘 분류기에 대한 모델링 협력자 주석기를 벤치마킹한다. 개념의 이름을 임베딩하고 각 이미지 임베딩에 대한 코사인 유사도를 측정하여 CLIP를 평가한다. CuPL은 동일한 기술을 사용하지만 개념명을 직접 임베딩하는 대신 LLM에 의해 생성된 개념에 대한 설명을 임베딩한다. GPT3 및 PaLM 2 모델은 모두 실험되었지만 우수한 결과를 생성했기 때문에 PaLM 2를 선택했다. CLIP와 CuPL의 경우 훈련 세트의 하위 집합에서 F1 점수를 최대화하는 그리드 검색을 사용하여 작동 지점을 선택한다. 우리는 PaLI-X VQA 변종을 분류기로 사용하여 "_Is this is a image of \\(\\mathcal{X}\\)?"__에 대한 답변을 기반으로 긍정 또는 부정 예측을 할당한다.\n' +
      '\n' +
      '**주석자 적응.** 시스템을 테스트하는 동안 주석자에서 개념 의존적 변동성의 일부 양을 관찰했다. 예를 들어, "cat"과 같은 간단한 개념의 경우 VLM은 이미 최첨단 성능을 가질 수 있으며 이러한 경우 시스템은 품질을 저하시킬 수도 있다. 이를 해결하기 위해 6가지 다른 Annotator _strategies_를 구현했다. 특정 개념에 대한 분류기를 개발하는 동안, 우리는 개념 소유자가 100개의 이미지의 현장 검증 세트를 구축하고, 이 세트를 사용하여 특정 개념에 대한 최상의 수행 전략을 선택한다. 이러한 구성을 설명하는 다른 매개변수는 부록에 설명되어 있다.\n' +
      '\n' +
      '**사용자, 크라우드 및 모델링 공동작업자.** 크라우드 및 자동 주석 방법 모두에 대해 _user_와의 일치/정렬을 측정합니다. _user_는 지상-진실의 원천이며 테스트 세트에 수동으로 주석을 달 수 있는 사람이다. _ 크라우드_ 어노테이션자들은 _user_에 의해 설명 및 예들이 주어지고 더 큰 스케일로 이미지들을 어노테이션하도록 요청된다. _ Collaborator_ Annotator 모델링은 자율성으로 인해 주석 프로세스를 더욱 확장할 수 있으며 시각적 모드에서 더 높은 다양성의 이미지 세트를 캡슐화할 수 있다. 인간과 기계 주석이 달린 데이터에 대해 학습된 증류 모델에 대한 성능(auPR)을 비교하여 주석자 정렬을 측정한다.\n' +
      '\n' +
      '### 모델링 공동작업자 주석기\n' +
      '\n' +
      '**협력자 주석기 모델링은 다른 제로 샷 방법보다 우수합니다.** 이 실험의 결과를 Tab. 1에서 보여주며, 일치 점수(정밀도, 재현율, F1)를 사용하여 애자일 모델링 데이터 세트의 보류된 테스트 세트에서 _user_와의 정렬을 측정한다. CLIP 및 CuPL 대조 모델은 높은 재현율을 선호하여 매우 낮은 정밀도로 인해 어려움을 겪는다. PaLI-X는 대조 모델보다 우수하여 제안된 주석기의 기준선으로 더 적합하다. **우리는 덜 주관적인 (쉬운) 개념에 대해 동등한 성능을 유지하면서 주관적인 (하드) 개념에 대해 상당한 이득을 얻는다.** 탭. 1은 개념 개선의 상당한 왜곡을 보여준다: 25% 이상의 개념은 15%에서 혐오 밈[26], 6%에서 건강한 밈, 5%에서 정지 표시를 포함하여 4% 이상의 F1 점수 이득을 보여 더 주관적인 분류가 필요한 영역에서 상당한 개선을 나타낸다. 이러한 경향은 우리의 모델이 복잡하거나 주관적인 개념에 특히 효과적이지만 PaLI-X가 이미 잘하는 개념에 대해 한계 이점만 제공할 수 있음을 나타낸다. 그럼에도 불구하고, PaLI-X에 대한 시스템을 비교한 F1 점수에 대한 Wilcoxon Signed-Rank 테스트는 모든 개념에서 통계적으로 유의한 개선을 보였다 (\\(p<0.01\\)). 분류 외에도 우리의 시스템은 그림 3에 표시된 근거를 출력한다.\n' +
      '\n' +
      '### Human-machine alignment\n' +
      '\n' +
      '**모델링 협업자는 최소한의 사용자 주석으로 경쟁 품질의 배포 가능한 모델을 생성할 수 있다.** 탭에서 다양한 수준의 인간 및 자동화된 주석을 사용하는 효과를 측정한다. 2. 본 모델은 증류된 사용자 모델 성능(100% 정확한 주석에서 증류됨)을 초과할 수 없지만, 크라우드-레이더를 능가할 수 있다는 점에 주목한다. 우리의 주석기 시스템은 더 어려운 미묘한 개념(6%의 차이)에서 군중 비율보다 훨씬 뛰어나다. 반면 쉬운 개념에서는 약간 성능이 떨어집니다. 이는 인간이 더 나은 성능을 보이는 자동화된 VQA 모델(PaLI-X)의 예측 오류 때문일 수 있다. 다른 최신 개방형 어휘 제로 샷 주석기(CLIP, CuPL 및 PaLI-X)를 사용하는 것과 비교하여 본 시스템은 쉽고 단단한 개념 모두에서 이러한 방법을 능가한다. 우리의 완전 자동화 시스템은 _user_의 출력에서 2% 마진 이내의 성능으로 고전적인 애자일 모델링으로 제작된 모델의 품질과 일치하는 증류 모델을 성공적으로 생성했다. 도. 도 4는 crowd-annotator와 Modeling Collaborator Annotator가 모두 _user_ feedback이 최소인 경우에도 증류된 모델의 성능을 향상시킬 수 있음을 보여준다. 그러나 Modeling Collaborator Annotator는 완전 자동화되어 더 많은 수의 예제로 확장할 수 있다는 장점이 있다.\n' +
      '\n' +
      '**협력자 및 기타 제로샷 및 고전적 방법을 모델링하는 것은 복잡한 이해와 추론이 필요한 복잡한 시각적 작업에서 실패한다.** 탭에서 입증된 바와 같이 혐오스러운 밈을 식별하는 데 대한 본 방법의 효과[26]이다. 3은 라벨이 붙은 데이터에 의존하지 않고 완전히 훈련된 모델을 일치시키는 능력에 의해 더욱 강조된다. 둘 다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{**PaLI-X [6]**} & \\multicolumn{3}{c}{**CLIP [43]**} & \\multicolumn{3}{c}{**CuPL [41]**} & \\multicolumn{3}{c}{**Ours**} \\\\ \\hline\n' +
      '**Concept** & **Pre** & **Rec** & **F1** & **Pre** & **Rec** & **F1** & **Pre** & **Rec** & **F1** & **Pre** & **Rec** & **F1** \\\\ \\hline \\hline\n' +
      '**Easy concepts** & & & & & & & & & & & & \\\\ arts-and-crafts & 0.71 & 0.97 & 0.82 & 0.68 & 0.86 & 0.76 & 0.68 & 0.90 & 0.77 & 0.96 & 0.75 & 0.84 \\\\ dance & 0.57 & 0.87 & 0.69 & 0.51 & 0.95 & 0.66 & 0.52 & 0.89 & 0.66 & 0.67 & 0.95 & 0.79 \\\\ emergency-service & 0.67 & 0.88 & 0.76 & 0.53 & 0.87 & 0.65 & 0.54 & 0.91 & 0.67 & 0.88 & 0.73 & 0.76 \\\\ hair-coloring & 0.76 & 0.97 & 0.85 & 0.70 & 0.99 & 0.82 & 0.70 & 0.99 & 0.82 & 0.76 & 0.97 & 0.85 \\\\ in-ear-headphones & 0.70 & 0.96 & 0.81 & 0.43 & 0.95 & 0.59 & 0.44 & 0.96 & 0.60 & 0.82 & 0.86 & 0.82 \\\\ pie-chart & 0.80 & 0.96 & 0.88 & 0.52 & 0.80 & 0.63 & 0.50 & 0.92 & 0.65 & 0.80 & 0.96 & 0.88 \\\\ single-sneaker & 0.65 & 0.92 & 0.76 & 0.51 & 0.99 & 0.67 & 0.51 & 1.00 & 0.67 & 0.70 & 0.88 & 0.78 \\\\ \\hline\n' +
      '**Easy concepts average** & 0.69 & 0.93 & 0.80 & 0.55 & 0.92 & 0.68 & 0.56 & **0.94** & 0.69 & **0.80** & 0.87 & **0.82** \\\\ \\(\\Delta\\) & & & & & & & & & & +11\\% & -6\\% & +2\\% \\\\ \\hline\n' +
      '**Hard concepts** & & & & & & & & & & & & & \\\\ astronaut & 0.61 & 0.87 & 0.71 & 0.40 & 0.95 & 0.56 & 0.42 & 0.95 & 0.58 & 0.72 & 0.79 & 0.72 \\\\ block-tower & 0.45 & 0.97 & 0.62 & 0.38 & 0.99 & 0.55 & 0.37 & 0.98 & 0.54 & 0.89 & 0.68 & 0.66 \\\\ gourmet-tuna & 0.52 & 0.95 & 0.67 & 0.29 & 1.00 & 0.45 & 0.29 & 1.00 & 0.45 & 0.52 & 0.95 & 0.67 \\\\ hand-pointing & 0.56 & 0.99 & 0.71 & 0.39 & 0.87 & 0.54 & 0.39 & 0.94 & 0.55 & 0.89 & 0.79 & 0.74 \\\\ healthy-dish & 0.38 & 1.00 & 0.55 & 0.37 & 0.99 & 0.54 & 0.38 & 1.00 & 0.55 & 0.84 & 0.61 & 0.61 \\\\ home-fragrance & 0.57 & 0.51 & 0.54 & 0.40 & 0.95 & 0.56 & 0.40 & 0.96 & 0.57 & 0.57 & 0.51 & 0.54 \\\\ stop-sign & 0.61 & 0.99 & 0.76 & 0.48 & 1.00 & 0.65 & 0.49 & 0.99 & 0.65 & 0.83 & 0.83 & 0.81 \\\\ \\hline\n' +
      '**Hard concepts average** & 0.53 & 0.90 & 0.65 & 0.39 & 0.96 & 0.55 & 0.39 & **0.97** & 0.56 & **0.75** & 0.74 & **0.68** \\\\ \\(\\Delta\\) & & & & & & & & & & & +22\\% & -16\\% & +3\\% \\\\ \\hline\n' +
      '**Overall average** & 0.61 & 0.92 & 0.72 & 0.47 & 0.94 & 0.62 & 0.47 & **0.96** & 0.62 & **0.78** & 0.79 & **0.74** \\\\ \\(\\Delta\\) & & & & & & & & & & & +17\\% & -13\\% & +2\\% \\\\ \\hline \\hline\n' +
      '**Hateful memes [26]** & **0.66** & 0.42 & 0.51 & 0.49 & **0.98** & 0.66 & 0.50 & 0.87 & 0.64 & 0.58 & 0.77 & **0.66** \\\\ \\(\\Delta\\) & & & & & & & & & & & -8\\% & +35\\% & +15\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 교사 수행도(Precision, Recall, F1 score). Collaborator 모델링은 CLIP, CuPL, PaLI-X(visual query answering models)를 포함한 최첨단 제로샷 기법보다 우수하다. 밑줄 친 결과는 우리의 성능이 (델타)와 비교되는 기준선(PaLI-X)을 나타낸다. 우리는 쉬운 개념, 단단한 개념 및 Hateful memes 데이터 세트를 위해 최고의 정밀도, 재현율 및 F1을 굵게 표현했습니다.\n' +
      '\n' +
      '도 4: 군중-주석자에 의해 주석이 달린 점점 더 많은 훈련 예제의 기여도를 비교하는 것 대. 공동작업자 주석기 모델링(완전 자동화) y축은 최종 증류 모델의 성능을 보여준다. _user_ 피드백이 최소일 때(100개의 주석이 달린 예제), 더 많은 크라우드-어노테이션 예제는 잡음 예측에도 불구하고 최종 증류 모델을 개선한다. 모델링 공동작업자 주석기는 인간 상호 작용 없이 유사한 성능 향상을 제공하며 자율성으로 인해 훨씬 더 많은 예에 주석을 달기 위해 더 잘 확장될 수 있다.\n' +
      '\n' +
      '교사와 학생 모델은 훈련 데이터 세트를 사용하지 않고 전통적인 훈련 접근법을 능가한다. 그러나 성능은 여전히 낮아 우리의 접근 방식의 한계를 보여준다.\n' +
      '\n' +
      '## 5 Limitations\n' +
      '\n' +
      '우리의 시스템은 LLM과 VLM의 오케스트레이션이기 때문에 원자 구성요소(PaLM 2, PaLI-X 및 CLIP)의 일부 한계를 겪을 수 있다. 예를 들어, 우리는 단순한 개념(고양이나 개 등)에 대한 장황하고 지나치게 복잡한 설명을 제공하는 것이 단순히 PaLI-X를 사용하는 것에 비해 실제로 성능을 저하시킬 수 있음을 관찰했다. 또 다른 문제는 특정 개념의 경우 CLIP 기능이 잘못된 증류 모델 품질로 이어질 수 있다는 것이다. 한 가지 예는 정지 표지판(여기서 정지 표지판은 트래픽에서 실제 정지 표지판이 될 것으로 예상됨)이며, 여기서 CLIP 기능은 정지 표지판의 전체 의미를 캡처할 수 있지만 물리적 인스턴스와 묘사를 쉽게 구별할 수 없다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 주관적, 미묘한 시각적 개념을 위한 분류기 개발에 필요한 수동적 노력을 완화하는 새로운 프레임워크인 Modeling Collaborator를 제시하였다. 본 프레임워크는 대용량 언어 모델(LLM)과 비전 언어 모델(VLM)의 발전을 활용하여 대화를 통해 개념 공간을 만들고 훈련 데이터 포인트에 자동으로 레이블을 지정한다. 실험들을 통해 본 프레임워크의 효용성을 입증함으로써, 미묘한 개념에 대한 시각적 분류기를 빠르게 구축할 수 있고 전통적인 애자일 모델링과 최첨단 제로 샷 분류 모델 모두를 능가할 수 있음을 보여준다. 우리의 작업은 콘텐츠 조정 및 미적 분류를 포함한 광범위한 응용 프로그램을 위한 분류기를 개발하는 데 필요한 시간과 노력을 크게 줄일 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{Human Annotators} & \\multicolumn{3}{c}{Machine Annotators} \\\\\n' +
      '**Concept** & **User** & **Crowd** & **Crowd** & **CuPL** & **PaLI-X** & **Ours** \\\\ Dataset size (per concept) & \\(\\sim\\)600 & \\(\\sim\\)600 & \\(\\sim\\)3000 & \\(\\sim\\)3000 & \\(\\sim\\)3000 & \\(\\sim\\)3000 \\\\ \\hline\n' +
      '**Easy concepts** & & & & & & \\\\ arts-and-crafts & 0.77 & 0.73 & 0.86 & 0.78 & 0.77 & 0.78 \\\\ dance & 0.69 & 0.70 & 0.81 & 0.72 & 0.68 & 0.68 \\\\ emergency-service & 0.75 & 0.71 & 0.78 & 0.59 & 0.66 & 0.72 \\\\ hair-coloring & 0.85 & 0.85 & 0.83 & 0.77 & 0.58 & 0.80 \\\\ in-ear-headphones & 0.73 & 0.66 & 0.67 & 0.65 & 0.73 & 0.72 \\\\ pie-chart & 0.77 & 0.76 & 0.76 & 0.72 & 0.82 & 0.82 \\\\ single-sneaker & 0.74 & 0.64 & 0.68 & 0.51 & 0.61 & 0.56 \\\\ \\hline\n' +
      '**Easy concepts average** & 0.76 & 0.72 & 0.77 & 0.68 & 0.69 & **0.73 (+1\\%)** \\\\ \\hline\n' +
      '**Hard concepts** & & & & & & \\\\ astronaut & 0.67 & 0.71 & 0.66 & 0.60 & 0.65 & 0.65 \\\\ block-tower & 0.59 & 0.58 & 0.45 & 0.48 & 0.49 & 0.50 \\\\ gourmet-tuna & 0.50 & 0.51 & 0.35 & 0.54 & 0.52 & 0.52 \\\\ hand-pointing & 0.50 & 0.56 & 0.58 & 0.56 & 0.81 & 0.81 \\\\ healthy-dish & 0.59 & 0.49 & 0.47 & 0.42 & 0.45 & 0.53 \\\\ home-fragrance & 0.62 & 0.60 & 0.69 & 0.56 & 0.53 & 0.53 \\\\ stop-sign & 0.70 & 0.57 & 0.55 & 0.62 & 0.51 & 0.64 \\\\ \\hline\n' +
      '**Hard concepts average** & 0.60 & 0.57 & 0.54 & 0.54 & 0.57 & **0.60 (+3\\%)** \\\\ \\hline\n' +
      '**Overall average** & 0.68 & 0.65 & 0.65 & 0.61 & 0.63 & **0.66 (+1\\%)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 최종 증류 모델 성능(auPR)을 이용한 상이한 주석자(또는 교사 모델)의 품질 비교. 개념 소유자는 미묘한 개념에 대한 깊은 이해 때문에 최고 품질의 주석을 제공한다. 공동작업자 주석기 모델링은 크라우드 평가자의 노동 집약적인 주석과 다른 자동화된 방법에 비해 더 나은 품질의 레이블을 제공합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline\n' +
      '**Method** & **Labeler** & **\\# Ex.** & **F1** & **Acc** & **Pre** & **Rec** \\\\ \\hline Ours (Teacher) & - & - & 0.66 & 0.61 & 0.58 & 0.77 \\\\ \\hline \\hline CLIP [43] & - & - & **0.57** & 0.53 & 0.51 & 0.65 \\\\ CuPL [41] & - & - & 0.51 & **0.64** & 0.50 & **0.87** \\\\ PaLI-X [6] & - & - & 0.51 & 0.61 & **0.66** & 0.42 \\\\ Ours (Student) & MC & 7K & 0.56 & 0.52 & 0.50 & 0.64 \\\\ \\hline CLIP+MLP & Human & 8.5K & 0.48 & 0.60 & 0.65 & 0.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Hateful Memes[26] 공개 데이터 세트에 대한 우리의 방법(주석기 및 증류기 모델 둘 다)의 성능. 비교를 위해 Zero-shot 방법과 VQA 방법을 사용한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ebtesam Almazrouei, Hamza Alobeidi, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.\n' +
      '* [2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _NeurIPS_, 33:1877-1901, 2020.\n' +
      '* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _NeurIPS_, 33:1877-1901, 2020.\n' +
      '* [5] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Tianyu Liu, and Baobao Chang. Towards end-to-end embodied decision making with multimodal large language model: Explorations with gpt4-vision and beyond. In _NeurIPS 2023 Foundation Models for Decision Making Workshop_, 2023.\n' +
      '* [6] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. _arXiv preprint arXiv:2305.18565_, 2023.\n' +
      '* [7] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al. Pali-3 vision language models: Smaller, faster, stronger. _arXiv preprint arXiv:2310.09199_, 2023.\n' +
      '* [8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. In _ICLR_, 2022.\n' +
      '* [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* [10] Tee Connie, Mundher Al-Shabi, and Michael Goh. Smart content recognition from images using a mixture of convolutional neural networks. In _IT Convergence and Security 2017: Volume 1_, pages 11-18. Springer, 2017.\n' +
      '* [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, pages 248-255, 2009.\n' +
      '* [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, pages 4171-4186, 2019.\n' +
      '* [13] Susan T Fiske and Shelley E Taylor. _Social cognition_. McGraw-Hill Book Company, 1991.\n' +
      '* [14] Gottlob Frege et al. Begriffsschrift, a formula language, modeled upon that of arithmetic, for pure thought. _From Frege to Godel: A source book in mathematical logic_, 1931:1-82, 1879.\n' +
      '* [15] Isabel O Gallegoos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjin, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models: A survey. _arXiv preprint arXiv:2309.00770_, 2023.\n' +
      '* [16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _CVPR_, pages 6904-6913, 2017.\n' +
      '* [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.\n' +
      '* [18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.\n' +
      '* [19] Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models. _arXiv preprint arXiv:2308.00675_, 2023.\n' +
      '* [20] Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, and Alireza Fathi. Avis: Autonomous visual information seeking with large language models. _arXiv preprint arXiv:2306.08129_, 2023.\n' +
      '* [21] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In _CVPR_, pages 23369-23379, 2023.\n' +
      '* [22] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In _CVPR_, pages 10236-10247, 2020.\n' +
      '* [23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, pages 4904-4916, 2021.\n' +
      '* [24] Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. Graph-rise: Graph-regularized image semantic embedding. _arXiv preprint arXiv:1902.10814_, 2019.\n' +
      '* [25] Aditya Khosla, Akhil S Raju, Antonio Torralba, and Aude Oliva. Understanding and predicting image memorability at a large scale. _ICCV_, pages 2390-2398, 2015.\n' +
      '\n' +
      '* [26] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. _NeurIPS_, 33:2611-2624, 2020.\n' +
      '* [27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 123(1):32-73, 2017.\n' +
      '* [28] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. _ICCV_, 128:1956-1981, 2020.\n' +
      '* [29] Yunxin Li, Baotian Hu, Xinyu Chen, Lin Ma, and Min Zhang. Lmeye: An interactive perception network for large language models. _arXiv preprint arXiv:2305.03701_, 2023.\n' +
      '* [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, pages 740-755, 2014.\n' +
      '* [31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2018.\n' +
      '* [32] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. In _ECCV_, pages 852-869, 2016.\n' +
      '* [33] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. _NeurIPS_, 29, 2016.\n' +
      '* [34] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. _NeurIPS_, 36, 2024.\n' +
      '* [35] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In _CVPR_, pages 10910-10921, 2023.\n' +
      '* [36] George A Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing information. _Psychological review_, 63(2):81, 1956.\n' +
      '* [37] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [38] OpenAI. Gpt-4v(ision) system card. [https://cdn.openai.com/papers/GPTV_System_Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf), 2023. Accessed: 2023-11-15.\n' +
      '* [39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _NeurIPS_, 35:27730-27744, 2022.\n' +
      '* [40] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-dli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_, 2023.\n' +
      '* [41] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi. What does a platypus look like? generating customized prompts for zero-shot image classification. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15691-15701, 2023.\n' +
      '* [42] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. _arXiv preprint arXiv:2210.03350_, 2022.\n' +
      '* [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.\n' +
      '* [44] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1:9, 2019.\n' +
      '* [45] Arpita Roy, Anamika Paul, Hamed Pirsiavash, and Shimei Pan. Automated detection of substance use-related social media posts based on image and text analysis. In _2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI)_, pages 772-779. IEEE, 2017.\n' +
      '* [46] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.\n' +
      '* [47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _NeurIPS_, 35:25278-25294, 2022.\n' +
      '* [48] Burr Settles. Active learning literature survey. 2009.\n' +
      '* [49] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? In _ICLR_, 2021.\n' +
      '* [50] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Huggingpt: Solving ai tasks with chatgpt and its friends in hugging face. _NeurIPS_, 36, 2024.\n' +
      '* [51] Oilia Stretcu, Edward Vendrow, Kenji Hata, Krishnamurthy Viswanathan, Vittorio Ferrari, Sasan Tavakkol, Wenlei Zhou, Aditya Avinash, Enming Luo, Neil Gordon Alldrin, et al. Agile modeling: Image classification with domain experts in the loop. _ICCV_, 2023.\n' +
      '* [52] Didac Suris, Sachit Menon, and Carl Vondrick. Vibergpt: Visual inference via python execution for reasoning. _arXiv preprint arXiv:2303.08128_, 2023.\n' +
      '* [53] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_. Association for Computational Linguistics, 2019.\n' +
      '\n' +
      '* [54] Imad Eddine Toubal, Yi-Ting Chen, Krishnamurthy Viswanathan, Daniel Salz, Ye Xia, and Zhongli Ding. Multimodal dual-tower architectures for entity retrieval from image and text. In _CVPRW_, 2023.\n' +
      '* [55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhagava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [56] Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang, Geoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo Pang, Michael Bendersky, et al. Non-intrusive adaptation: Input-centric parameter-efficient fine-tuning for versatile multimodal modeling. _arXiv preprint arXiv:2310.12100_, 2023.\n' +
      '* [57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _NeurIPS_, 35:24824-24837, 2022.\n' +
      '* [58] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023.\n' +
      '* [59] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* [60] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.\n' +
      '* [61] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. _arXiv preprint arXiv:2303.11381_, 2023.\n' +
      '* [62] Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A Ayyubi, Kai-Wei Chang, and Shih-Fu Chang. Idealgtpt: Iteratively decomposing vision and language reasoning via large language models. _arXiv preprint arXiv:2305.14985_, 2023.\n' +
      '* [63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.\n' +
      '\n' +
      '## 부록 개념명 및 설명\n' +
      '\n' +
      '###신속한 모델링 데이터셋 개념\n' +
      '\n' +
      '**예술과 공예**: 이미지에는 예술과 공예가 포함되어야 합니다.\n' +
      '\n' +
      '**우주비행사**: 그림, 클립아트 등이라도 우주비행사를 보여주는 모든 그림. 우주비행사는 그들이 우주비행사가 되는 것과 관련이 있다는 것을 분명히 보여주어야 한다 - 보통 우주복이나 NASA 점프슈트로 표시된다.\n' +
      '\n' +
      '**블록 타워**: 이미지에는 레그 또는 나무로 만들어진 장난감 블록 타워가 포함되어야 합니다.\n' +
      '\n' +
      '춤추는 사람들 사진\n' +
      '\n' +
      '**응급 서비스**: 이미지에는 응급 서비스, 구급대원, 소방관, 경찰 또는 구조대가 포함되어야 합니다.\n' +
      '\n' +
      '**미식가 참치**: 참치가 포함되어야 하는 미식가 요리(즉, 화려하고 우아함)의 사진. 여기에는 초밥, 회, 구운 참치, 화려한 아히 참치 샐러드가 포함됩니다. 여기에는 참치 통조림, 참치 샌드위치, 생선 참치 자체의 사진이 포함되지 않습니다.\n' +
      '\n' +
      '**손 포인팅**: 검지 손가락만 뻗은 채로 손 포인팅이 보여지는 그림. 엄지손가락을 올린 사진이나 검지 손가락만 뻗은 손 사진은 포함하지 않는다. 화면을 가리키거나 두드리는 직선 손가락이 있는 그림이 포함되어 있습니다.\n' +
      '\n' +
      '**헤어 컬러링**: 헤어 컬러링 과정 중 또는 사진 촬영 전후, 바로 후, 사람에게 초점을 맞추는 사진. 부정적인 것들: 미용사 앞쪽에, 염색약 상자들을 보관하세요.\n' +
      '\n' +
      '**건강식** : 탄수화물이 적은 건강식이 들어간 요리의 사진\n' +
      '\n' +
      '**홈 향수** : 가정용 향수, 가정용 방향제, 향초, 에센셜 오일 등 주택에 사용되는 모든 종류의 향수의 사진\n' +
      '\n' +
      '**인 이어 헤드폰**: 귀 안쪽에 착용하는 모든 헤드폰은 가리기보다는 가립니다. 이러한 유형의 헤드폰은 외이도에 삽입됩니다. 인이어 헤드폰이 사진에 있는 한 유효합니다.\n' +
      '\n' +
      '**파이 차트**: 원형 통계 그래픽인 파이 차트가 있는 모든 이미지는 숫자 비율을 나타내기 위해 슬라이스로 분할된다.\n' +
      '\n' +
      '**흰색 배경의 단일 스피커**: 흰색 또는 중립색 배경(예: 베이지색)에 단일 운동화를 묘사한 이미지. 이것은 운동화의 부분적 뷰(예: 발바닥만 또는 운동화의 절반이 뷰)일 수 있지만, 단지 부분(예: 신발 끈만)일 수는 없다. 네거티브에는 하나 이상의 신발을 가진 이미지, 다른 색상의 배경을 가진 이미지 또는 다른 스타일의 신발이 포함됩니다.\n' +
      '\n' +
      '정지 표지판: 이것은 현실 세계의 공식 정지 표지판의 사진을 포함한다. 자율 주행 자동차에 대한 이러한 정지 표지판을 감지하고 싶다고 상상해 보세요. 양성은 건설에 포함되거나 건설 노동자가 손에 쥐고 있는 임시 사진을 포함하여 모든 정지 표지판 사진을 포함한다. 배너나 광고 포스터에 정지 표지판이 있으면, 차가 막히더라도 부정적일 것이다. (우리는 자율주행차가 거기서 멈추는 것을 원하지 않는다.) 클립 아트 또는 실내 정지 표시는 음수입니다.\n' +
      '\n' +
      '##### 공공 데이터 집합 개념\n' +
      '\n' +
      '해롭거나 인종차별적이거나 성차별적인 말\n' +
      '\n' +
      '## 부록 B 검색 질의\n' +
      '\n' +
      '다음은 우리 시스템의 데이터 마이닝 과정에서 LAION[47] 데이터 세트에서 후보 이미지를 마이닝하는 데 사용되는 검색 쿼리 세트이다. 이러한 모든 검색 쿼리들은 후보 이미지들을 검색하기 위해 LLM(PaLM-2[2]) 및 조인트 CLIP[43] 임베딩 공간을 사용하여 인코딩된다.\n' +
      '\n' +
      '**예술 및 공예** 공예실, 공예책, 초보자용 공예품, 공예 튜토리얼, 예술 및 공예품, 공예품 보관소, 공예품 박람회, 공예 프로젝트, 판매용 공예품, 공예품, 어린이용 공예품, 공예품, 공예기술, 공예용품, 다이, 공예잡지, 성인용 공예품, 수공예품\n' +
      '\n' +
      '***우주 비행사, 우주정거장 모듈 속의 우주 비행사, 우주 장갑 속의 우주 비행사, 우주 깃발 속의 우주 비행사, 우주 셔틀 조종석 속의 우주 비행사, 우주 정장 속의 우주 비행사, 우주 배낭 속의 우주 비행사, 우주 정거장 에어록 속의 우주 비행사, 우주 정거장 속의 우주 비행사, 우주 정거장 속의 우주 비행사, 우주 정거장 속의 우주 비행사, 우주 셔틀 속의 우주 비행사, 우주 정거장 쿠폴라 속의 우주 비행사, 우주 정거장 쿠폴라 속의 우주 비행사, 우주 정거장 속의 우주 비행사, 우주 정거장 속의 우주 비행사\n' +
      '\n' +
      '**블록 타워** 블록의 타워, 레고 타워, 톨 레고 타워, 톨 토이 타워, 레고 타워, 장난감의 타워, 우뚝 솟은 나무, 우뚝 솟은 소년, 블록 타워, 우뚝 솟은 다리, 톨 토이 타워, 블록의 탑, 톨 나무 타워, 나무 타워, 완구 타워, 나무의 탑\n' +
      '\n' +
      '***스트릿 댄스, 플라멩코, 발레, 모던 댄스, 바차타, 볼룸 댄스, 주크, 삼바, 사람들이 춤을 추고, 벨리 댄스, 살사, 메렝게, 댄스 공연, 라인 댄스, 탭 댄스, 힙합, 댄서, 포크 댄스\n' +
      '\n' +
      '** 응급 서비스** 경찰 응급, 직장의 경찰관, 구조 보트, 응급 대응, 경찰차, 소방차, 직장의 구조대원, 응급요원, 의료 응급, 소방관, 직장의 구급대원, 구조팀, 소방 구조, 경찰, 응급 서비스, 구조 운영, 직장의 소방대원, 구조 헬리콥터, 응급 차량, 구급차, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원, 구급대원\n' +
      '\n' +
      '**미식 참치** 참치 초밥, 참치 회, 참치 샐러드, 썰은 참치, 아히 참치, 고메 참치, 참치 타르타르, 아히 참치 스테이크, 화려한 참치\n' +
      '\n' +
      '**핸드 포인팅**, 핸드 포인팅 핑거 확장, 핸드 포인팅 핑거, 핸드 포인팅 핑거 스트레이트, 누군가에게 핸드 포인팅 핑거 스트레이트, 스크린에서 핸드 포인팅 핑거 스트레이트, 핸드 검지 핑거 확장, 무언가에 핸드 포인팅 핑거 스트레이트, 사람에게 핸드 포인팅 핑거 스트레이트, 나에게 핸드 포인팅 핑거 스트레이트, 우리에게 핸드 포인팅 핑거 스트레이트, 당신에게 핸드 포인팅 핑거 스트레이트, 스크린에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 핸드 포인팅 스크린, 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물에서 핸드 포인팅 핑거 스트레이트, 사물\n' +
      '\n' +
      '**헤어 컬러링**헤어 컬러링, 헤어 컬러 살롱, 헤어 컬러 전후의 헤어 컬러, 헤어 컬러 영감, 헤어 컬러 공포 스토리, 헤어 컬러 미스캡, 헤어 컬러 튜토리얼, 헤어 컬러 팁, 헤어 염색, 헤어 컬러 스타일리스트, 헤어 컬러 페일, 헤어 컬러 페일, 가정에서의 헤어 컬러, 헤어 컬러 프로세스, 헤어 컬러 실수, 헤어 컬러 재난, 헤어 컬러 잘못, 헤어 컬러 아이디어, 헤어 컬러, 헤어 컬러 불량, 헤어 컬러\n' +
      '\n' +
      '**건강한 요리** 건강한 점심, 건강한 샌드위치, 건강한 요리, 건강한 버거, 건강한 식사, 건강한 음식, 건강한 음식, 저탄수화물 요리, 건강한 샐러드, 건강한 생선, 건강한 피자, 건강한 저녁, 건강한 채식, 건강한 채식, 건강한 아침, 건강한 파스타, 건강한 치킨, 건강한 수프, 건강한 디저트\n' +
      '\n' +
      '**홈향기**홈향기, 홈향기디퓨저, 향초, 홈향기, 향초, 홈향기, 에센셜오일, 홈방향제, 방향제, 홈정유, 홈향기디퓨저, 홈향기디퓨저, 홈방스프레이, 홈향기디퓨저, 홈향기, 홈향기, 향기디퓨저, 홈향기, 룸스프레이\n' +
      '\n' +
      '**인 이어헤드폰**인 이어폰, 인 이어헤드폰, 인 이어헤드폰, 인 이어버드, 인 이어, 헤드폰\n' +
      '\n' +
      '***: 파이 차트 작성기, 파이 차트 데이터, 파이 차트 튜토리얼, 파이 차트 백분율, 파이 차트 일러스트레이션, 파이 차트 디자인, 파이 차트 템플릿, 파이 차트 차트, 파이 차트 인포그래픽, 파이 차트 그래픽, 파이 차트 차트 차트 차트, 파이 차트 작성기, 파이 차트 그래프, 파이 차트 예제, 파이 차트 생성기, 파이 차트 툴, 파이 차트 소프트웨어\n' +
      '\n' +
      '**흰색 바탕 위의 단일 운동화** 밝은 베이지색 바탕 위의 운동화, 바탕 위의 운동화, 중립색 바탕 위의 운동화, 밝은 바탕 위의 운동화, 밝은 바탕 위의 운동화, 타이트한 바탕 위의 운동화, 기차 바탕 위의 운동화, 기차 바탕 위의 운동화, 흰색 바탕 위의 운동화, 회색 바탕 위의 운동화\n' +
      '\n' +
      '**스톱 사인** : 도로상 정지 표지판, 도로상 정지 표지판, 지주 정지 표지판, 사람이 소지한 정지 표지판, 교통 정지 표지판, 교통중 정지 표지판, 도시내 정지 표지판, 인터스테이트상 정지 표지판, 정지 표지판, 고속도로상 정지 표지판, 건설중 정지 표지판, 건설중 정지 표지판, 실제 정지 표지판, 고속도로상 정지 표지판, 농촌지역에서의 정지 표지판, 공식 정지 표지판, 주차장에서의 정지 표지판, 손에 든 정지 표지판\n' +
      '\n' +
      '## 부록 C LLM 프롬프트\n' +
      '\n' +
      '아래 공동작업자 주석기 모델링에 사용된 예제 프롬프트 세트를 나열합니다. 주어진 개념에 대해 설명을 사용할 수 없을 때, 우리는 구조화된 설명을 자동 생성하기 위해 다음 프롬프트를 사용한다:\n' +
      '\n' +
      '시각적 개념 이름이 지정되었습니다.\n' +
      '\n' +
      '다음 단계를 따르세요: <단계 1> 당신은 전문가 \\(\\implies\\) 언어가로 일해야 합니다. 이러한 시각적인 \\(\\implies\\) 개념에 대해 주어진 이미지가 범위 내 또는 범위 외인 경우 \\(\\implies\\)을 결정해야 하는 인간 \\(\\implies\\) 주석자가 있다. 주석이 주어진 시각적 개념에 대해 범위 내 또는 범위 외 이미지를 결정하는 데 사용할 수 있는 시각적 개념에 대한 설명을 생성하는 작업을 수행합니다. </step1>\n' +
      '\n' +
      '<단계 2> 이 시각적 이미지의 개념 정의를 몇 문장으로 제공한다. </step2>\n' +
      '\n' +
      '<단계 3>이 시각적 개념에 대한 범위 내이기 위해 이미지가 가져야 하는 모든 이미지 속성을 제공한다. </step3>\n' +
      '\n' +
      '<단계 4> 단계 2와 단계 3에서 발견되는 각 속성은 장황하고 독립적이며 자기 설명적이며 의미 있어야 한다. </step4>\n' +
      '\n' +
      '<단계 7> 사용자 친화적이고 판독 가능한 형식으로 응답을 작성:\n' +
      '\n' +
      '시각적 개념 정의:\n' +
      '\n' +
      '<여기에 시각 개념의 2-3선 개념 정의를 추가>\n' +
      '\n' +
      '이미지는 이 시각적 개념에 대한 범위 내에 있기 위해 다음과 같은 속성을 가져야 한다.\n' +
      '\n' +
      '<여기서 세부사항을 글머리말 포인트로 추가.></step7>\n' +
      '\n' +
      '<visualConceptName>{CONCEPT_NAME}</\n' +
      '\n' +
      '<visualConceptName>\n' +
      '\n' +
      '(이미지 데이터베이스로부터 후보 포지티브 이미지들을 인출하는데 사용되는) 시각적 개념에 기초하여 포지티브 검색 쿼리들을 생성하기 위한 프롬프트:\n' +
      '\n' +
      '당신의 임무는 찾는 데 도움이 되는 것입니다.\n' +
      '\n' +
      '시각적 개념에 대한 양성(범위 내) 이미지입니다. 시각적 개념의 이름과 설명이 주어집니다. 설명\n' +
      '\n' +
      '이 시각적 개념의 범위를 만드는 이미지의 속성을 설명합니다. 또한 이러한 시각적 개념에 대해 범위를 벗어난 이미지의 속성을 설명한다.\n' +
      '\n' +
      '다음 단계를 따릅니다.\n' +
      '\n' +
      '<단계 1>이 시각적 개념에 대한 범위를 만드는 이미지의 모든 속성을 나열합니다. </step1>\n' +
      '\n' +
      '<단계 2> 각 속성은 객관적이고 완전하며 자명해야 한다. </step2>\n' +
      '\n' +
      '<단계 3>찾은 속성이 설명에 언급된 모든 범위 내 속성 또는 시나리오를 포함하는지 확인합니다. 그렇지 않은 경우 누락된 범위 내 속성을 추가합니다.\n' +
      '\n' +
      '<단계 4> 단계 3에서 확인한 모든 범위 내 속성을 기반으로 20개의 구글 검색 키워드를 생성하여 해당 범위 내 속성을 가진 다양한 이미지를 찾기 위해 구글 이미지 검색을 수행할 수 있다. </step4>\n' +
      '\n' +
      '<단계 5> 구글 검색 키워드가 설명에 언급된 모든 유형의 범위 내 이미지를 포함하는지 확인하십시오. 그렇지 않은 경우 Google 검색 키워드를 추가하여 해당 유형의 범위 내 이미지를 찾습니다. </step5>\n' +
      '\n' +
      '<단계 6> 이것은 중요한 단계이다. 지금까지 선택한 키워드 중 일부는 범위 외 이미지를 검색하는 데 적합할 수 있습니다. 범위 외 이미지를 검색할 키워드를 식별합니다. 응답에서 해당 구글 검색 키워드를 제거합니다. </단계 6>\n' +
      '\n' +
      '<단계 7> 각 검색 쿼리는 인터넷 이미지 검색을 위해 3-4개의 단어 길이, 독립, 자기 설명 및 의미가 있어야 한다. </step7>\n' +
      '\n' +
      '<단계 8> 이러한 질의들 중 어느 것이 4단어들보다 긴 경우, 3-4단어로 요약한다. </step8>\n' +
      '\n' +
      '<step9> 다음의 xml 형식으로 응답을 작성한다. 당신의 응답은 프로그램적으로 분석될 것이기 때문에, 당신의 응답은 이 형식을 엄격하게 따라야 한다:\n' +
      '\n' +
      '"xml\n' +
      '\n' +
      '<google_search_keywords>\n' +
      '\n' +
      '<keyword></keyword>\n' +
      '\n' +
      '...\n' +
      '\n' +
      '</google_search_keywords>\n' +
      '\n' +
      '""\n' +
      '\n' +
      '</step9>\n' +
      '\n' +
      '<단계 10> 응답에 xml만 유지하고 다른 텍스트는 제거한다. </step10>\n' +
      '\n' +
      '<concept>{CONCEPT_NAME}</concept>\n' +
      '\n' +
      '</description><</description></description></description></description></description></description></description></description></description></description></description></description></description></description></description>\n' +
      '\n' +
      '당신은 전문 언어학자로 일해야 한다. 이미지 분류를 위해 시각적 개념 이름과 그 설명이 제공됩니다. 설명에는 조각이 거의 없을 수 있습니다. 카브아웃은 이미지를 범위를 벗어난 것으로 분류해야 하는 몇 가지 특별한 상황입니다. 당신의 임무는 설명서에서 세부 사항을 추출하는 것입니다.\n' +
      '\n' +
      '다음 단계를 따릅니다. <단계> 설명에 조각이 포함되어 있지 않은 경우 다음 형식으로 응답을 작성하고 다음 단계를 모두 건너뜁니다. "...xml\n' +
      '\n' +
      '<carveOutsInDescription> <carveOut>NOT_FOUND</carveOut> </carveOutsInDescription> </carveOutsInDescription> <> </carveOutsInDescription> <> </step1> <step2> 설명이 이러한 시각적 개념에 대한 범위외 이미지들에 대한 세부사항들을 제공한다면, 언급된 그 조각-아웃 상황들의 리스트를 출력한다. </step2><step3> 다음 xml 형식으로 출력합니다. 당신의 응답은 프로그램적으로 파싱될 것이기 때문에, 당신의 응답은 "\'xml<carveOutsInDescription><carveOut></carveOut>... </carveOutsInDescription>\'... </step3> <step4> 응답에 xml만 유지하고 다른 텍스트는 제거한다. </step4>\n' +
      '\n' +
      '<concept>(CONCEPT_NAME)</concept><description>(CONCEPT_DESCRIPT)</<description>\n' +
      '\n' +
      '개념의 긍정적인 속성들을 생성하기 위한 프롬프트 템플릿:\n' +
      '\n' +
      '당신의 임무는 이미지 분류를 위한 시각적 개념의 범위를 이해하는 것이다. 시각적 개념 이름과 설명이 제공됩니다.\n' +
      '\n' +
      '설명은 이 시각적 개념에 대한 범위를 만드는 이미지의 속성을 설명합니다. 또한 이러한 시각적 개념에 대해 범위를 벗어난 이미지의 속성을 설명한다.\n' +
      '\n' +
      '다음 단계를 따릅니다.\n' +
      '\n' +
      '<단계 1>이 시각적 개념에 대한 범위를 만드는 이미지의 모든 속성을 나열합니다. </step1>\n' +
      '\n' +
      '<단계 2> 각 속성은 객관적이고, 모호하지 않으며, 상세하고, 장황하며, 자기 설명적이어야 한다. </단계2>\n' +
      '\n' +
      '<단계 3>찾은 속성이 설명에 언급된 모든 긍정적인 속성을 포함하는지 확인합니다. 그렇지 않은 경우 누락된 속성을 추가합니다. </step3>\n' +
      '\n' +
      '<단계 4> 다음 xml 형식으로 응답을 작성하십시오. 당신의 응답은 프로그램적으로 분석될 것이기 때문에, 당신의 응답은 이 형식을 엄격하게 따라야 한다:\n' +
      '\n' +
      '"\'xml<positiveAttributes><attribute></attribute>...</positiveAttributes>... </step4> <step5> 응답에 xml만 유지하고 다른 텍스트는 제거한다. </step5><concept>{CONCEPT_NAME}</concept><description>\n' +
      '\n' +
      '개념의 네거티브 속성들을 생성하기 위한 프롬프트 템플릿:\n' +
      '\n' +
      '당신은 전문 언어학자로 일해야 한다. 이미지 분류를 위해 시각적 개념 * 이름과 그 설명이 주어집니다. 설명에는 조각이 거의 없을 수 있습니다. 카브아웃은 이미지를 범위를 벗어난 것으로 분류해야 하는 몇 가지 특별한 상황입니다. 당신의 임무는 설명서에서 세부 사항을 추출하는 것입니다.\n' +
      '\n' +
      '다음 단계를 따릅니다.\n' +
      '\n' +
      '<단계 1> 설명에 조각이 포함되어 있지 않은 경우 다음 형식으로 응답을 작성하고 다음 단계를 모두 건너뜁니다.\n' +
      '\n' +
      '\'\'xml <carveOutsInDescription> <carveOut>NOT_FOUND</carveOut> </carveOutsInDescription> </carveOutsInDescription> <</step1> <step2> 설명이 이러한 시각적 개념에 대한 범위외 이미지들에 대한 세부사항들을 제공한다면, 언급된 그러한 조각-아웃 상황들의 리스트를 출력한다. </step2><step3> 다음 xml 형식으로 출력합니다. 당신의 응답은 프로그램적으로 파싱될 것이기 때문에, 당신의 응답은 엄격하게 이 포맷을 따라야 한다: "xml<carveOutsInDescription><carveOut></carveOut></carveOutsInDescription></carveOutsInDescription>"</step3><step4> 응답에 xml만 유지하고 다른 텍스트를 제거한다. </step4>\n' +
      '\n' +
      '<concept>{CONCEPT_NAME}</concept><description>{CONCEPT_DESCRIPTION}</description>\n' +
      '\n' +
      '여기서, CONCEPT_NAME 및 CONCEPT_DESCRIPT는 주관적 개념 명칭 및 설명이다.\n' +
      '\n' +
      '이미지에 대한 최종 주석 결정을 위해 다음 프롬프트를 PaLM-2에 전달합니다.\n' +
      '\n' +
      '시각적 개념의 이름과 설명이 제공됩니다. 우리는 그 이미지를 평가자들에게 보여주었고 그 이미지에 대해 많은 질문을 했고 그들은 답을 주었다. 질문 및 답변도 아래에 제공됩니다. 당신의 임무는 이미지에 대한 몇 가지 질문에 답하는 것입니다. 다음 단계를 따릅니다.\n' +
      '\n' +
      '<단계 1>다음 단계에서 텍스트 응답은 평가자의 응답에 제공된 답변을 엄격하게 기반으로 해야 한다. </step1>\n' +
      '\n' +
      '<단계 2> 이미지에 존재하는 범위 외 속성을 제공한다. </step2>\n' +
      '\n' +
      '<단계 3> 이미지에 존재하는 범위 내 속성을 제공한다. </step3>\n' +
      '\n' +
      '<단계 4> 이미지에 누락된 범위 내 속성을 제공합니다. </step4>\n' +
      '\n' +
      '<단계 5> 다음의 규칙에 의거하여 이미지를 분류한다. 규칙은 주어진 순서대로 따라야 한다.\n' +
      '\n' +
      '<classificationRules>\n' +
      '\n' +
      '<규칙1> 이미지가 범위 외 속성의 EVEN ONE을 갖는 경우, 이러한 시각적 개념에 대해 부정적으로 분류되어야 한다. </rule1>\n' +
      '\n' +
      '<rule2> 이미지는 이 시각적 개념에 대해 이미지를 긍정적인 것으로 분류하기 위해 필요한 모든 긍정적인 속성을 가져야 한다. 이미지에 필요한 모든 긍정적인 속성이 있는 경우 이미지를 긍정적인 것으로 분류합니다. 그렇지 않으면 부정으로 분류합니다. </rule2>\n' +
      '\n' +
      '<rule3> 다른 모든 경우에는 영상을 음수로 분류한다. </rule3></classificationRules></step5>\n' +
      '\n' +
      '<단계 6>이 형식으로 엄격하게 응답에 다음 세부 정보를 추가: 결정: "긍정" 또는 "부정" 이유: <이 이미지가 긍정 또는 부정인 이유 목록 제공> </단계 6>\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '그림 6: Modeling Collaborator와 전문가 협업이 증류 모델의 성능에 미치는 영향. 개념당 총 4,000개의 훈련 사례를 사용하였다. x축은 예제 없음(0%)에서 2,000예제(50%) 범위의 전문가(개념 소유자)에 의해 라벨링된 이러한 예제 중 몇 개를 나타낸다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>