<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Prismatic VLMs: 디자인 공간 탐색\n' +
      '\n' +
      '시각적 조건 언어 모델\n' +
      '\n' +
      'Siddharth Karamcheti\n' +
      '\n' +
      'Suraj Nair\n' +
      '\n' +
      'Ashwin Balakrishna\n' +
      '\n' +
      'Percy Liang\n' +
      '\n' +
      'Thomas Kollar\n' +
      '\n' +
      'Dorsa Sadigh\n' +
      '\n' +
      'github.com/TRI-ML/prismatic-vlms\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '시각 조건 언어 모델(VLM)은 시각적 대화, 장면 이해 및 로봇 작업 계획과 같은 응용 분야에서 채택이 증가하고 있으며, LLaVa, Instruct-BLIP, PaLI-3과 같은 많은 새로운 모델을 부채질하고 있다. 새로운 릴리스의 양에도 불구하고, 이미지 전처리, 아키텍처 및 최적화를 둘러싼 주요 설계 결정이 과소 탐색되어 모델 성능을 설명하는 요소가 무엇인지 이해하는 데 어려움이 있으며, 객관적이고 일관된 평가가 부족하여 더욱 복잡해진다. 이러한 격차를 해결하기 위해 먼저 시각적 질문 응답, 언어로부터의 객체 현지화 및 환각과 같은 특성을 조사하는 표적 챌린지 세트에 걸쳐 표준화된 평가 세트를 컴파일하며, VLM 기능에 대한 보정되고 세밀한 통찰력을 제공하는 평가이다. 둘째, 미리 훈련된 시각적 표현과 베이스 대 사용의 절충점을 정량화하는 것을 포함하여 주요 설계 축을 따라 VLM을 엄격하게 조사한다. 명령어 조정 언어 모델, 그 중에서도. 우리는 (1) VLM을 평가하기 위한 통합된 프레임워크, (2) VLM 훈련을 위한 최적화된 유연한 코드, (3) 오픈 소스 VLM에서 최첨단인 InstructBLIP 및 LLaVa v1.5를 엄격하게 능가하는 7-13B 규모의 VLM 패밀리를 포함한 모든 모델에 대한 체크포인트의 세 가지 자원 기여와 분석을 결합한다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '만약 당신이 공중에서 카스트들을 만들었다면, 당신의 일은 잃을 필요가 없다; 그것이 그들이 있어야 할 곳이다. 이제 그 밑에 파운데이션을 놓으세요.\n' +
      '\n' +
      '헨리 데이비드 소로\n' +
      '\n' +
      '시각-조건 언어 모델들(VLMs)은 이미지 입력 및 텍스트 프롬프트들로부터 자연 언어 응답들을 생성하고, 애플리케이션들의 성장하는 스펙트럼에 대한 일반적이고 표현적인 인터페이스를 제공하는, 접지된 채팅(Li 등, 2023; Gong 등, 2023), 시각적 프로그래밍(Sur\'is 등, 2023; Subramanian 등, 2023), 로봇 제어(Driess 등, 2023; Brohan 등, 2023) 등을 포함한다. 이 광범위한 채택은 VLM을 개발하는 방법에서 최근 _패러다임 시프트_에 의해 촉진되며, 이전의 복잡한 아키텍처 및 훈련 목표를 피한다.\n' +
      '\n' +
      '그림 1: **프리스마틱 VLMs.**1 시각 조건 언어 모델(VLMs)의 설계 공간을 탐색하는 엄격한 실험을 통해 훈련을 개선하는 통찰력을 식별한다. 데이터 및 스케일을 제어할 때, 우리의 모델들(주황색)은 11개의 다양한 태스크들에 걸쳐 최신 LLaVa v1.5(회색; Liu et al., 2023)를 능가하는 반면, 트레이닝 컴퓨트_의 30% 이상을 절약한다.\n' +
      '\n' +
      'work Tan and Bansal (2019); Li et al. (2022); Li et al. (2022); 2023b), 새로운 VLM은 간단한 접근법을 채택하고, 사전 훈련된 시각적 표현들로부터 패치 특징들을 처리한다(예를 들어, CLIP; Radford et al. (2021) as _tokens_ as the input space of a language model (LM) 이러한 "patch-as-token" 접근법은 간단한 목표-다음-토큰 예측으로 훈련을 가능하게 하고, Llama-2 및 Mistral Touvron et al.(2023); Jiang et al.(2023)과 같은 강력한 LMs의 생태계를 효율적으로 훈련시키기 위한 도구들과 함께 이용할 수 있게 한다(예를 들어, FSDP; Zhao et al.(2023). 이러한 조합은 미리 훈련된 컴포넌트들, 데이터, 또는 최적화 절차 Liu 등(2023); Chen 등(2023)의 선택과 같은 개별 성분들을 변화시키면서, 동일한 기본 레시피_를 채택하는 LLaVa v1.5, 및 PALI-3과 같은 모델들의 신속한 개발 및 방출을 촉진시켰다.\n' +
      '\n' +
      '불행히도 기존 접근법은 다운스트림 기능에 대한 주어진 선택의 영향을 철저히 평가하지 않고 VLM을 구축하고 훈련하는 주변 설계 공간의 일부만 다룬다. 이것은 이 작업의 핵심 질문을 동기 부여합니다: _VLM 기능과 다운스트림 사용에 영향을 미치는 핵심 설계 결정은 무엇입니까?_ 이 질문에 대한 답을 제공하기 위해서는 먼저 주어진 모델의 장단점을 철저히 평가할 수 있는 방법이 필요하다. 이를 효과적으로 수행하기 위해서는 다양하고 객관적인 작업으로 구성된 표준화된 평가 제품군을 컴파일해야 하며, 결정적으로 이러한 작업은 공간 추론, 유통 외 일반화 및 상식 이해와 같은 특정 기능을 조사할 수 있어야 한다. 둘째, 구체적인 권장 사항 세트를 구축할 뿐만 아니라 개별 선택을 다운스트림 성능과 연결하려면 다양한 VLM 설계 축을 엄격하게 탐색해야 한다.\n' +
      '\n' +
      '이 작업은 네 가지 기여를 통해 이러한 축을 다룬다. 먼저, VLM 능력에 대한 세밀한 통찰력을 제공하기 위해, 시각 질문 응답에 걸쳐 있는 네 가지 작업(Bigham et al., 2010; Goyal et al., 2017; Hudson and Manning, 2019; Singh et al., 2019)에 걸쳐 있는 네 가지 작업, Kazemzadeh et al. (2014); Yu et al. (2016); Wang et al. (2021) 및 세밀한 공간 추론, 강건성 및 환각을 평가하는 세 가지 도전 작업(Acharya et al. (2018); Liu et al. (2022); Li et al. (2023)을 포함하는, 시각 및 언어 문헌으로부터 11개의 벤치마크로 구성된 표준화된 평가 스위트**를 컴파일한다. 세밀한 공간 추론, 강건성 및 환각을 평가하는 세 가지 도전 작업(도 2; 오른쪽)은 훈련 시간 및 데이터를 중심으로 유연성을 강조하는 VLM 훈련을 위한 최적화되고 모듈화된 코드베이스**를 개발한다. 우리는 많은 통찰력을 식별한다. 예를 들어, 기존 작업에 의해 채택된 다단계 훈련 절차가 성능에 영향을 미치지 않고 제거되어 계산 비용이 20-25% 감소한다는 것을 발견한다. 또한 CLIP Radford et al. (2021) 및 DINov2 (Oquab et al., 2023)와 같은 상이한 백본들로부터의 특징들을 병합하는 _fused_ 비주얼 백본들이 보드 전반에 걸쳐 더 많은 수행성 VLM들을 유도한다는 것을 발견한다. 마지막으로, 우리는 연구 결과를 통합하고 InstructBLIP 및 LLaVa v1.5.1과 같은 최첨단 개방형 VLMs**를 엄격하게 능가하는 7B/13B 규모에서 오픈 소스 모델 패밀리인 프리즘스를 효율적으로 훈련한다.\n' +
      '\n' +
      '각주 1: 이 작업의 일부로 훈련된 모든 모델에 대해 최적화된 훈련 코드베이스, 평가 제품군 및 _체크포인트를 릴리스합니다.\n' +
      '\n' +
      '그림 2: **VLM 설계 축 탐색.** VLM 개발을 위한 네 가지 주요 설계 축을 탐색한다: 1) 최적화 절차, 2) 이미지 처리 및 사전 훈련된 시각적 표현, 3) 언어 모델 및 4) 훈련 시간 및 데이터 주변 스케일링 속성(**left**) 이 탐사를 가능하게 하기 위해, 우리는 VLM(**right**)을 효율적으로 훈련시키기 위한 오픈 소스, 유연한 코드베이스라는 핵심 자원 기여를 한다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '분석을 위해 1) VLM 모델 아키텍처, 2) 사전 훈련 데이터 및 3) 훈련 구현이 필요하다.\n' +
      '\n' +
      '**Model Architecture.** LLaVa, Qwen-VL, 및 PaLI-3과 같은 많은 최근의 VLM에 의해 사용되는 일반적인 아키텍처를 채택한다(Liu et al., 2023; Bai et al., 2023; Chen et al., 2023). 이러한 아키텍처는 입력 이미지를 LM의 임베딩 공간 내로 개별적으로 투영되는 패치 피처들의 시퀀스에 매핑하기 위해 (미리 훈련된) 시각적 백본을 사용한다. 형식적으로, VLM은 임의의 시퀀스 길이를 갖는 이미지\\(x_text{img}\\in\\mathbb{R}^{H\\times W}\\)와 텍스트 프롬프트 토큰\\(u_text{prompt}\\)을 입력으로서 취한다. 이 입력들은 1) 시각적 표현 백본, 2) 시각적 언어 프로젝터, 3) 구성 요소에 공급된다. 언어 모델\n' +
      '\n' +
      '_Visual Representation. 본 논문에서는 먼저 시각적 표현 백본(V\\(V\\{\\omega}\\)의 대상이 되는 \\(p\\text{img}\\in\\mathbb{R}^{L\\times h\\text{train}}\\)을 처리하고, 여기서 \\(p\\text{img}=V\\{\\omega}(x\\text{img})\\). 예로서, \\(p_{\\text{img}}\\)는 비전 트랜스포머(ViT; Dosovitskiy et al., 2021)에 의해 출력된 패치 피처들일 수 있다.\n' +
      '\n' +
      '_Vision-Language Projector.__Vision-Language Projector.__ 다음으로 학습한 프로젝터를 통해 학습한 프로젝터를 통해 학습한 프로젝터를 통해 학습한 프로젝터를 학습한 프로젝터를 통해 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝터를 학습한 프로젝\n' +
      '\n' +
      '_Language Model.____ 마지막으로, 텍스트 프롬프트 임베딩(e_{\\text{prompt}}=\\text{embed}(u_{\\text{prompt}})\\)과 시퀀스(e_{\\text{img}}\\)를 연결하여 언어 모델에 결과를 전달한다. 언어 모델은 출력 텍스트\\(u_{\\text{gen}}=\\text{LM}_{\\theta}([e_{\\text{img};e_{\\text{prompt}])를 생성한다.\n' +
      '\n' +
      '구성 \\(\\text{LM}_{\\theta}([F_{\\psi}(V_{\\omega}(a_{\\text{rgb})));\\text{embed}(u_{\\text{prompt}})])])은 VLM을 정의한다. 학습 중 트리플((x_{\\text{img},u_{\\text{prompt},\\tilde{u}_{\\text{gen}))이 주어지면 기울기 하강을 통해 손실(\\mathcal{L}(\\omega,\\psi,\\theta)=-\\log p(\\tilde{u}_{\\text{gen}\\mid x_{\\text{img},u_{\\text{prompt}))을 최소화한다.\n' +
      '\n' +
      '**Pretraining Dataset.** 우리는 사전 훈련 데이터의 선택을 완전히 오픈 소스(예: 허용 연구 라이선스)이고 이전 작업에 사용된 데이터 세트로 제한한다. 구체적으로, LLaVa v1.5 데이터 혼합을 사용하며, 이는 다단계 훈련 파이프라인에 사용되는 두 개의 하위 집합으로 구성된다. 첫 번째 서브세트는 다양한 캡션 데이터 세트로부터 소싱된 예들의 558K 샘플 혼합물(예를 들어, Conceptual Captions, LAION Sharma et al., 2018; Schuhmann et al., 2021)로 구성되고, 두 번째 서브세트는 Liu et al. (2023)에서 생성된 합성 데이터로 구성된 튜닝 예_ 및 기존의 비전-언어 트레이닝 세트로부터의 예들(예를 들어, GQA, TextCaps; Hudson and Manning, 2019; Sidorov et al., 2020)로 구성되며, 특히, ShareGPT(ShareGPT, 2023)로부터의 언어 전용 데이터의 샘플로 구성된다. 우리는 SSA.1에서 사전 훈련 데이터 혼합물에 대한 포괄적인 분석을 제공한다.\n' +
      '\n' +
      '**Training Implementation & Verification.** SS1에 열거된 설계 축들을 조사하기 위해, 우리는 _efficient_ 및 _flexible_인 VLM 트레이닝을 위한 코드가 필요하다; 비판적으로, 우리는 비전 및 LM 백본을 쉽게 스와핑하고 임의의 최적화 절차들(예를 들어, 트레이닝 동안 비전 백본을 동결하는 것)을 처리하는 능력이 필요하다. 이러한 요구 사항으로, 우리는 Fully Sharded Data Parallel (FSDP; Zhao et al., 2023) 및 BF16 혼합 정밀도를 사용하여 PyTorch에서 우리의 트레이닝 코드 베이스를 구현한다. FSDP는 개별 모델 컴포넌트(예를 들어, 비전 백본의 경우 FP16, LM의 경우 BF16)에 대한 정밀도를 지정할 수 있게 하고, 상이한 하드웨어에 대한 휴대성을 가능하게 하며, 최소한의 구현 오버헤드를 제공한다. 이전 작업(Karamcheti et al., 2021; Biderman et al., 2023)의 재현성 관행에 따라, 우리는 훈련 동안 초기화 무작위성을 고정하고 배치 순서를 수정한다. 우리는 TIMM(Wightman, 2019)과 Hugging Face Transformers(Wolf et al., 2019)를 활용하여 사전 훈련된 모델을 제공한다.\n' +
      '\n' +
      '코드를 검증하기 위해 7B 및 13B 매개변수 척도 모두에서 LLaVa v1.5(Liu et al., 2023)의 사과 대 사과 복제를 실행한다. 성공적인 재생산 결과는 그림 4(왼쪽)에 나와 있다. 본 논문의 구현은 참조 LLaVa v1.5 훈련 구현보다 상당히 효율적이라는 것을 발견한다: 동일한 하드웨어(8개의 A100 GPU를 갖는 AWS p4de.24xlarge 노드)에서 벤치마킹될 때, LLaVa가 잘 최적화된 DeepSpeed ZeRO 라이브러리를 활용함으로써 주목할 만한 이득인 FSDP 지원 구현으로 20% 더 빠른 걸음 시간을 관찰한다(Rasley et al., 2020).\n' +
      '\n' +
      '**우리는 이 작업의 핵심 기여 중 하나로 이 오픈 소스 교육 코드 베이스를 강조한다. 다른 개방형 코드 베이스와 달리 최소한의 코드 변경으로 모델 구성 요소, 최적화 절차 및 데이터를 쉽게 지정하거나 추가할 수 있는 모듈식 및 표현 인터페이스를 제공합니다(그림 2; 오른쪽). _efficient_ 및 _easily extensible_ framework를 제공함에 있어서, 우리는 높은 표준의 재현성 및 제어된 실험을 유지하면서, 새로운 평가 설계, 새로운 VLM 개발 및 훈련, 다양한 다운스트림 애플리케이션에 대한 기존 모델의 미세 조정 또는 기타 적응에 관한 향후 연구를 가능하게 한다.\n' +
      '\n' +
      '##3 평가 스위트\n' +
      '\n' +
      '이 작업의 첫 번째 기여는 주어진 VLM의 능력에 대한 _fine-grained insight_를 제공하는 통합 평가 제품군이다. VLM을 평가하는 최근의 작업은 GPT-4(OpenAI et al., 2023)와 같은 강력한 LM을 사용하여 _relative and subjective performance_(Liu et al., 2023; Yu et al., 2023)를 판단하는 자동화된 평가에 의존하는 경향이 있어 주어진 설계 변경의 절대적 영향을 측정하기 어렵다. 대신, 우리는 다음 세 가지 영역에 걸쳐 잘 정의된 메트릭을 사용한 평가에 중점을 둔다.\n' +
      '\n' +
      '_Open-Ended Visual Question Answering.__Open-Ended Visual Question Answering.___ 우리는 VizWiz (Bigham et al., 2010), VQAv2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), 및 TextVQA (Singh et al., 2019)에 대해 평가한다. VizWiz와 VQAv2는 모두 일반적인 시각적 추론을 평가한다; VizWiz는 또한 일련의 _unanswerable_ 질문을 포함한다. GQA는 공간 추론을 평가하는 반면, TextVQA는 이미지에 존재하는 텍스트(예: 레이블, 사이니지) 주변의 추론을 평가한다.\n' +
      '\n' +
      '_Localization.___ 사전 훈련 데이터 혼합물(SS2로부터)의 일부는 언어로 표현된 참조 표현이 주어진 정규화된 바운딩 박스 좌표를 예측하는 예들을 포함한다. 이와 같이 RefCOCO, RefCOCO+, RefCOCOg(Kazemzadeh et al., 2014; Yu et al., 2016), OCID-Ref(Wang et al., 2021)에 대한 바운딩 박스 예측 정확도를 평가한다. RefCOCO는 공간 앵커를 사용한 짧은 설명에 초점을 맞추고, RefCOCO+는 엄격하게 외형 기반 설명에 초점을 맞추고, RefCOCOg는 길고 풍부한 설명에 초점을 맞추고, OCID-Ref는 분산 외 일반화를 조사하는 로봇 데이터 세트이며, 클러터 내의 객체를 국소화하는 데 중점을 둔다.\n' +
      '\n' +
      '_Challenge Sets(Closed-Set Prediction)__ 우리는 Visual Spatial Reasoning (VSR; Liu et al., 2022), TallyQA (Acharya et al., 2018), POPE (Li et al., 2023)에 대해 평가한다. VSR은 다양한 장면(예: "케이크가 식탁의 가장자리에 있다")에서 개별 공간 관계에 대한 도전적인 True/False 질문으로 구성되며, 이는 특히 도전적인 작업이며 대부분의 기존 모델이 다수 클래스 기준선(51%)을 능가하지 못한다. TallyQA는 복잡성이 다양한 표현과 함께 언어로 기술된 객체를 계수하는 VLM의 능력을 평가하는 질문으로 구성된다. 마지막으로 POPE는 VLM의 환각 성향을 평가하는 목표 Yes/No 질문으로 구성된다.\n' +
      '\n' +
      '우리는 GQA(여기서 추천된 테스트-dev 분할을 사용하는 경우), VSR(여기서 제로샷 테스트 분할을 사용하는 경우), POPE(여기서 단일 평가 분할만 있는 경우)를 제외한 모든 벤치마크에 대해 _validation_set을 사용한다. 우리는 Appx. B에 있는 평가 프로토콜에 대해 더 자세히 제공한다.\n' +
      '\n' +
      '##4 실험 - 설계축 탐색\n' +
      '\n' +
      '두 번째 기여는 (SS4.1) 최적화 절차, (SS4.2) 이미지 처리 및 시각적 표현, (SS4.3) 언어 모델 및 (SS4.4) 훈련 시간 및 데이터 다양성과 같은 스케일링 속성을 따라 VLM 설계 공간을 탐색하는 일련의 표적 실험이다.\n' +
      '\n' +
      '**실험 설계: Protocols & Drawing Conclusions.** 우리는 먼저 LLAa v1.5(SS2 참조)를 재현함으로써 우리의 VLM 트레이닝 구현을 검증하고, 원작의 디자인 선택들을 채택하며 - 이미지를 처리하기 위해 많은 다른 최근의 VLM들에 의해 사용되는 동일한 선택들 - 패치의 크기가 14이고 입력 해상도가 336px인 CLIP ViT-Large(CLIP ViT-L/14 @ 336px; Radford et al., 2021)를 시각적 표현으로, Vicuna v1.5를 LM 백본으로, 그리고 SS2에 기술된 두 데이터 서브세트를 사용하여 2-스테이지 트레이닝 파이프라인에서 7B 및 13B 스케일 모두에서 성공적인 재생 결과를 나타낸다. 4(좌측). 번식의 충실도와 이러한 설계 선택의 보급을 모두 감안할 때 이 매개변수화를 중심으로 분석을 고정한다. 비판적으로, SS4.2, SS4.3 및 SS4.4의 각 실험은 이 기본 아키텍처의 _단일 단계 변경으로 공식화되고 다른 모든 선택은 상수_로 유지된다.\n' +
      '\n' +
      'SS3의 각 평가는 서로 다른 척도를 사용하여 직접 비교가 어렵다. 우리는 각 모델에 대한 정규화된 Z-점수와 평가(모든 모델에 걸친 평균 및 표준 편차를 사용)를 계산하여 이를 해결한다. 이러한 점수는 통계적 유의성(SSB.2의 추가 세부 사항)을 계산하고 각 레이더 플롯의 상대적 척도를 설정하는 데 사용된다(완전성을 위해 색상 및 굵은 레이블로 절대 메트릭도 제공한다.\n' +
      '\n' +
      '도 3: **평가 스위트 개요.** 시각적 질문 응답, 현지화 및 도전 과제(예: 계수, 공간 관계, 환각 성향 평가)에 걸쳐 있는 다수의 확립된 벤치마크를 컴파일한다. 이 평가 스위트는 모든 분석의 중추를 형성하여 개별 VLM 설계 선택의 영향에 대한 세밀한 통찰력을 제공한다.\n' +
      '\n' +
      '### Optimization Procedure\n' +
      '\n' +
      '이 절에서는 SS2에 설명된 세 가지 구성 요소를 각각 초기화하고 훈련하는 데 사용되는 최적화 절차를 중심으로 설계 선택에 초점을 맞추고, 구체적으로 훈련 시 서로 다른 VLM 구성 요소가 서로 다른 지점에서 동결되는 다단계 훈련의 효과를 조사한다.\n' +
      '\n' +
      '**Multi-Stage Training.** 많은 VLMs(Chen et al., 2023; Ye et al., 2023)에 의해 채택된 널리 퍼진 설계 선택들 중 하나는 2단계 트레이닝 파이프라인을 포함하는 것이다: (1) 임의의 초기화된 프로젝터 \\(F_{\\psi}\\)를 격리시키고, 다른 모든 컴포넌트들을 동결시킴으로써 시각 및 언어 특징들을 정렬하기 위한 _alignment_ 스테이지. 4, 우측) 및 (2) _finetuning_ 스테이지는 프로젝션과 LM이 모두 트레이닝되는 동안 시각적 표현만이 동결된다.\n' +
      '\n' +
      '다단계 교육을 채택하면 구현이 복잡해지고 교육 비용이 추가되므로 초기 실험으로 표적 절제술을 통해 이 첫 단계의 필요성을 평가한다. 우리는 기본 2단계 훈련 절차와 finetuning \\(F_{\\psi}\\) 및 LM으로 직접 건너뛰는 _single-stage_ 접근법을 비교한다. 우리는 (그림)을 찾는다. 4; 왼쪽) 명시적 프로젝터 사전 훈련 단계를 포함하는 것은 _불필요_이며, 단일 단계 훈련은 총 성능(\\(p=0.007\\))을 향상시킨다. 이 첫 번째 단계를 제거하는 것은 훈련 비용의 20-25%를 절약하고, 추가적인, 스테이지-특정 데이터(예를 들어, SS2로부터의 캡션 서브세트)의 필요성을 제거한다. _ 이러한 변화가 성능과 효율성을 엄격하게 개선함에 따라 다음 모든 실험에 대해 단일 단계 훈련을 채택한다._\n' +
      '\n' +
      '**Full Finetuning through Visual Backbones.** 미리 훈련된 시각적 표현들을 레버리지하는 기존의 VLM들에서 또 다른 인기 있는 설계 선택은 훈련의 전체 동안 시각적 백본 _frozen_을 떠나는 것이다(Liu et al., 2023; Driess et al., 2023; Li et al., 2023). 그러한 선택은 훈련 과정 동안 언어 생성에 도움이 되는 개선된 시각적 표현을 학습할 가능성을 제한한다. 따라서 우리는 -__시각 백본을 포함하여 전체 모델을 미세 조정함으로써 VLM 성능을 개선할 가능성이 있습니까?_ 우리는 (그림)을 찾는다. 특히 RefCOCO와 OCID-Ref와 같은 세밀한 공간 추론이 필요한 작업에서 시각적 백본을 미세 조정하면 성능_(\\(p=0.00407\\))이 크게 저하된다.\n' +
      '\n' +
      '_Remark_. 전체 미세 조정에서 성능이 저하된 것은 우리가 훈련하는 비전 언어 데이터의 규모와 다양성에서부터 학습 목적(vs. 세립 지각 특징의 학습을 장려하는 목적)으로서 언어 생성에 이르기까지 여러 가지 이유일 수 있다. 특히 폐쇄 소스 모델의 존재를 감안할 때\n' +
      '\n' +
      '도 4: **Reproducing LLaVa v1.5 & Exploring Optimization Procedures.** 우리의 훈련 코드베이스(§2)를 검증하기 위해, 우리는 LLaVa v1.5(녹색)를 재현하고, 우리의 모델은 Liu et al.(2023)(회색)에서 보고된 성능을 재현한다. 그런 다음 값비싼 다단계 교육(**right**)의 필요성을 조사하는 첫 번째 실험(§4.1)을 실행한다. 단일 단계 훈련은 다중 단계 모델_(주황색)을 유지하거나 능가하는 VLM을 생성하므로 상당한 계산량을 절약할 수 있으며, 그 결과 이 변화를 미래의 모든 실험으로 전달한다.\n' +
      '\n' +
      '도 5: **Visual Backbones를 통한 full Finetuning.** 훈련 시 프로젝터와 언어 모델 외에 (전통적으로 동결된) visual backbone의 Finetuning의 영향을 탐색한다. 단일 및 다단계 패러다임 모두에서 비전 백본을 미세 조정하면 특히 현지화 작업에서 거의 모든 벤치마크에서 성능이 크게 저하된다는 것을 알 수 있다.\n' +
      '\n' +
      '이 패러다임을 큰 성공으로 채택하는 Fuyu-8B(AI, 2023)로서, 우리는 VLM 훈련 동안 (예를 들어, 보조 목표를 통해) 이러한 특징 붕괴를 방지하는 방법을 식별하는 것이 향후 작업에 대한 풍부한 방향이 될 것이라고 믿는다.\n' +
      '\n' +
      '### 이미지 처리 및 시각적 표현\n' +
      '\n' +
      '**Pretrained Vision Representation.** CLIP(Radford et al., 2021)를 선택하는 것은, 다양한 데이터 소스들에 대해 트레이닝된 풍부한 시각적 표현들에도 불구하고, 거의 모든 VLM들에 대한 시각적 표현을 위한 디폴트 선택이 되었다. 본 실험에서는 CLIP, SigLIP(Zhai et al., 2023), DINOv2(Oquab et al., 2023) 및 분류를 위해 미리 훈련된 표준 비전 트랜스포머(ImageNet-21K, Finetuned on ImageNet-1K; Dosovitskiy et al., 2021; Steiner et al., 2021) 간의 일대일 비교를 수행하고, 공정한 비교를 위해 ViT-Large2를 사용한다(Fig. (p=8.88\\)e-8).\n' +
      '\n' +
      '각주 2: 모든 표현(224px)에 공통적인 이미지 해상도에 대해 평가하기 위해, 우리는 400M 파라미터(vs 307M)에서 ViT-Large보다 약간 큰 _shape-optimized_ SigLIP 모델(ViT-SO Alabdulmohsin et al., 2023)을 사용한다.\n' +
      '\n' +
      '_Remark_. 비전 언어 대비 목표는 CLIP와 SigLIP의 강점에 대한 하나의 설명이지만, 또 다른 가능한 설명은 훈련 이미지 분포 중 하나이다. CLIP와 SigLIP는 모두 ImageNet이나 DINOv2 사전 훈련 데이터에 없는 인터넷 소스 이미지(예: 스케치, 다이어그램, 애니메이션 그래픽 등)를 포함한다.\n' +
      '\n' +
      '**시각적 백본에 걸친 이미지 처리** 대부분의 이미지는 해상도 및 종횡비가 매우 다양하지만 대부분의 시각적 백본은 고정된 크기에서 정사각형 이미지를 기대하며, 이를 조정하기 위해 압도적인 기본값은 이미지를 크기에 맞게 "크기 조정 및 자르기"하는 것이다. 이는 분류와 같은 응용 분야에서 잘 작동하는 경향이 있지만 이미지의 일부를 잘라내는 것은 전체 장면 추론이 필요한 작업에 특히 해롭다. 이 실험에서 우리는 세 가지 다른 이미지 처리 방식, 즉 기본 "리사이즈 & 크롭" 방식, 비정방 이미지를 정사각형으로 패딩하는 LLaVa v1.5에서 사용하는 "레터박스 패딩" 방식, 원래 이미지 종횡비를 왜곡하거나 이미지를 정사각형으로 스퀴징 또는 스트레칭하는 "네이브 리사이즈" 방식을 평가한다. 우리의 연구 결과(도 6; 중간) 크롭은 분명히 차선책이지만 "순진한 크기 조정" 계획은 CLIP에 대해 가장 성능이 좋다. SigLIP의 경우 "순수 크기"와 "문자 상자 패딩" 모두 유사하게 수행됩니다. 일반적으로, 우리의 결과는 "letterbox padding"보다 "naive resizing"을 선호하지만 통계적으로 유의한 개선을 지배할 수는 없다(\\(p=0.0148\\).\n' +
      '\n' +
      '_Remark_. 패딩에 비해 이미지의 크기를 순진하게 조정하기 위한 두 가지 추측적 주장은 "죽은 픽셀"과 분포 이동을 최소화하는 것이다. 정사각형에 패딩되는 16:9 종횡비를 갖는 이미지는 많은 양의 비정보 픽셀(40% 초과)을 도입하며; 종횡비를 워핑하는 것은 아마도 시프트의 감소일 수 있다. 비전 트랜스포머의 선천적 패치 차원(\\(d=1024\\)과 함께 (16\\times 16\\) 픽셀 패치의 경우) 이미지 크기를 순진하게 조정하면 다운스트림 작업에 필요한 특성을 추출하기에 다운스트림 LM(7B+ 파라미터)에 대한 충분한 정보를 보존할 수 있다.\n' +
      '\n' +
      '**스케일링 이미지 해상도.** 최근 VLM에서의 또 다른 경향은 다운스트림 성능을 향상시키는 세밀한 세부사항들을 캡처할 희망으로 입력 이미지 해상도를 증가시키고 있다(Liu et al., 2023; Li et al., 2023). 우리의 연구 결과(Fig. 6; 오른쪽) 이 가설을 336px 또는 384px로 스케일링하여 상당한 개선을 제공한다(\\(p=5.66\\)e-4).\n' +
      '\n' +
      '_Remark_. 이미지 해상도를 확장하는 것이 명백한 승리인 것처럼 보이지만, 개별 ViT 패치를 LM의 임베딩 공간에 투영하는 VLM에 대한 계산 복잡도가 크게 증가한다는 점에 주의한다.\n' +
      '\n' +
      '도 6: **Image Processing & Visual Representations**. 우리는 §4.2에서 이미지 처리와 시각적 표현을 둘러싼 선택을 탐색한다. 구체적으로, 서로 다른 시각적 표현의 영향(**left**), 이미지 처리 전략의 기능에 따라 성능이 어떻게 달라지는지(**middle**), 입력 이미지 해상도 증가의 영향(**right**)을 조사한다.\n' +
      '\n' +
      '고정된 패치 입도를 가정하면, 입력 해상도를 두 배로 하면 LM에 공급되는 입력 패치_의 수의 4배가 된다. 시퀀스 길이의 함수로서 전통적인 트랜스포머 주의의 2차 비용과 함께, 이것은 시간 복잡도가 16배 증가한 것이다(메모리 요구 사항에서 유사한 폭발과 함께).\n' +
      '\n' +
      '**Ensembling Different Visual Representations.** 비전에서 이전 작업의 풍부한 바디는 상이한 유도 편향으로 훈련된 상이한 유형의 시각적 표현들이 광범위한 애플리케이션들에 대한 향상된 성능을 초래할 수 있음을 식별한다(Kobayashi et al., 2022; Karamcheti et al., 2023). 이에 동기 부여되어, 우리는 이러한 동일한 경향이 VLM 훈련에 대해 사실인지 묻는다 - 구체적으로 CLIP 및 SigLIP로부터의 비전-언어 대조 특징을 갖는 DINov2 특징을 앙상블하는 것이 Kerr 등(2023)에서 취한 접근법에 따라 향상된 성능으로 이어질 수 있는지 여부 - 이를 효율적으로 구현하기 위해 각 패치에 대한 채널 차원을 따라 서로 다른 백본에서 패치 피쳐를 연결하면 피쳐 차원이 두 배인 동일한 수의 입력 패치 임베딩이 생성된다. 이를 조정하기 위해, 우리는 적은 비용으로 프로젝터 \\(F_{\\psi}\\)(2-layer MLP)의 입력 치수를 증가시킨다. 우리는 (그림)을 찾는다. DINov2 및 SigLIP 기능을 융합하는 것은 전반적으로 상당한 이득을 제공한다(\\(p=0.00162\\)), DINov2 + CLIP 모델(\\(p=0.4066\\)), DINov2 기능을 결합하는 것은 텍스트VQA에 특히 해로운 것으로 보인다. 나머지 결과를 살펴보면, 현지화 및 도전 과제에서 특히 5-10%의 인상적인 이득을 볼 수 있으며, 일반적으로 DINov2 + SigLIP 융합 표현은 실제로 추가된 매개변수가 없는 가장 성능이 좋은 시각적 표현이다.\n' +
      '\n' +
      '_Remark_. Kerr et al.(2023)의 가설과 유사한 작업에 따라, 우리는 DINov2 특징이 이미지의 낮은 수준의 공간 속성을 캡처하는 특징을 제공하여 비전 언어 대조 모델에 의해 캡처된 더 높은 수준의 "의미" 속성을 증가시킨다고 믿는다. 우리는 이 결론이 DINO 스타일 백본을 넘어 일반화될 수 있다는 점에 주목한다; 우리가 이미지넷과 CLIP/SigLIP 백본의 융합을 평가하지 않는 유일한 이유는 패치 입도의 불일치 때문이다(이미지넷 백본은 \\(16\\times 16\\) 대 패치의 입도를 사용한다). 다른 모든 백본들이 사용하는 \\(14\\times 14\\)의 입도. 우리는 VLM에 대한 이러한 유형의 융합된 다중 해상도 기능에 대한 영향을 추가로 탐구하는 것이 향후 작업을 위한 강력한 방법이라고 믿는다.\n' +
      '\n' +
      '### 언어 모델 통합\n' +
      '\n' +
      '**베이스 vs. Instruct-Tuned LMs.** Instruct 튜닝(또는 채팅 튜닝; Ouyang et al., 2022; Chung et al., 2022)은 광범위한 애플리케이션에 대한 자연스러운 입력/출력 인터페이스를 제공하는, 대화 에이전트로서 행동하기 위해 베이스 LMs(다음-토큰 예측을 위해 트레이닝됨)를 미세 조정하는 방법이다. 결과적으로, Vicuna(Zheng et al., 2023)와 같은 명령 튜닝된 모델들은 VLM들에 대한 디폴트 백본이 되었다. 불행하게도, 명령 튜닝은 성능에서 편향 및 퇴행을 도입하는 단점이 있다(Ouyang et al., 2022). 따라서 본 실험에서는 베이스 LM(Llama-2; Touvron et al., 2023)과 지시 조정 변형(Vicuna v1.5) 간의 정면 비교를 통해 지시 조정 LM 백본이 다운스트림 VLM 성능에 미치는 영향을 평가한다. 우리는 (그림)을 찾는다. 7 - 오른쪽) 명령어 조정 LMs_는 기본 LMs_(\\(p=0.373\\))에 비해 통계적으로 유의미한 성능 향상을 얻지 못하지만, 정성적 성능_에서는 차이가 있다. 특히, 우리는 지시 조정된 LM이 더 장황하고 환각에 취약하며 일반적으로 응답에서 덜 구체적인 VLM을 유도한다는 것을 관찰한다(도 11).\n' +
      '\n' +
      '**언어 전용 안전 데이터에 대한 공동 훈련.** 우리가 훈련에 사용하는 LLaVa v1.5 사전 훈련 데이터 세트는 ShareGPT(ShareGPT, 2023)에서 조달한 언어 전용 데이터의 40K 예로 구성되며, 이 데이터는 OpenAI의 ChatGPT와 사용자 업로드 대화의 다양한 세트로 구성된다. cru\n' +
      '\n' +
      '도 7: **상이한 시각적 표현 및 베이스 vs. 지시 조정 LMs.** DINov2 및 CLIP/SigLIP 모델의 시각적 기능을 융합하는 방법을 탐색하고, 그렇게 하면 현지화 및 도전 평가(**left**)에서 성능이 크게 향상된다는 것을 발견했다. 우리는 베이스(Llama-2; 오렌지)와 지시 조정(Vicuna v1.5; 회색) 언어 모델(**right**) 간의 차이를 추가로 평가하며, Llama-2가 환각에 덜 취약하면서도 유사한 정량적 성능을 제공한다는 것을 발견했다(그림 11).\n' +
      '\n' +
      '이 데이터세트의 일반적으로 많은 예는 독성, 부적절하거나 그렇지 않으면 안전하지 않은 입력, _ 및 ChatGPT_로부터의 대응하는 "보호된" 응답을 포함한다(예를 들어, "AI로서, 나는...에 대해 논평할 수 없음). 이 실험에서는 시각적 추론과 관련이 없는 언어 전용 데이터를 추가하는 것이 멀티모달 데이터에 대한 훈련에 비해 성능을 손상시키는 경우 이해의 목표로 이 언어 전용 데이터가 다운스트림 성능에 미치는 영향을 완화한다. 우리는 (그림)을 찾는다. 8; 좌측) 언어 전용 데이터만을 제거하는 것은 성능_(\\(p=\\) 0.194)을 약간 향상시킨다.\n' +
      '\n' +
      '그러나 언어 전용 데이터가 미세 조정 동안 "안전" 데이터의 유일한 출처라는 점을 감안할 때, 이 데이터가 VLM 출력에 대한 안전 장치를 유도하는 데 얼마나 중요한지 평가하기 위해 직접 공격 및 독성 프롬프트로 VLM을 명시적으로 조사한다. 적대적 테스트에서 특히 Llama-2와 같은 기본 LM에서 훈련된 VLM의 경우 이 공동 훈련 데이터를 포함하는 것이 최소 세이프가드 세트를 유도하는 데 중요하다는 것을 발견했다. 도 8은 직접적인 인종 차별적 의도가 있는 질문으로 촉발될 때 VLM 세대에 대한 공동 훈련의 중요성을 보여준다.\n' +
      '\n' +
      '_Remark_.: 우리는 주로 인종 차별, 외국인 혐오, 그리고 성 편견에 대한 불안전한 반응에 초점을 맞춘다. 이러한 편향은 언어에서도 널리 퍼져 있으며 ShareGPT 공동 훈련 데이터에 명시적으로 표시된다. 우리는 광범위한 영향에 대한 논의에서 VLM 특유의 피해를 다룬다.\n' +
      '\n' +
      '### 스케일링 특성: 훈련 시간 및 데이터\n' +
      '\n' +
      '이 섹션에서는 훈련 시간과 추가 데이터가 있는 기존 VLMs 척도가 어떻게 되는지 조사하고, 비판적으로 훈련 시간의 선택(모델을 훈련 중인 경우)과 다양한 데이터 세트를 추가하는 것이 다운스트림 성능에 미치는 영향을 조사한다.\n' +
      '\n' +
      '**훈련 중입니까?** 훈련 시간의 영향을 훈련 시대의 함수로 탐색합니다. 최대 단일 에폭에서 수행하는 PaLI 또는 LLaVa와 같은 기존 VLM과 달리 서로 다른 수의 에폭에서 훈련할 때의 성능을 비교한다. 놀랄 것도 없이, 우리는 (무화과)를 찾는다. 10; 중간) 수행이 안정될 때 두 시간까지 (특히 RefCOCO와 같은 구조화된 출력을 요구하는 작업에 대해) 꾸준한 개선과 함께 단일 에폭으로 심각한 언더피팅의 증거. 우리는 두 개의 에폭에 대한 훈련이 한 개의 에폭에 대한 훈련보다 개선의 상당한 증가를 가져온다는 것을 발견했다 (\\(p=\\) 0.00440).\n' +
      '\n' +
      '**시각 언어 데이터 추가** 우리는 부가된 데이터가 다운스트림 성능에 영향을 미치는지 조사한다. 우리는 최근에 제안된 두 개의 데이터세트들: LVIS-Instruct-4V(Wang et al., 2023), GPT-4V가 LVIS로부터 소싱된 이미지들로부터 풍부한 합성 예들을 생성하도록 프롬프트함으로써 획득된, LVIS-Instruct(Gupta et al., 2019) 및 LRV-Instruct(Liu et al., 2023)를 식별하며, 이는 기존 데이터세트들(예를 들어, 차트들, 과학 다이어그램들, 및 뉴스 인쇄들 추가)에 비해 이미지 다양성에 대해 구체적으로 최적화한다. 우리는 (그림)을 찾는다. 10; 오른쪽) 두 데이터 세트를 추가하면 성능이 향상되지만(\\(p=\\) 0.0138), LRV-인스트럭트는 결과적인 성능 이득의 대부분을 제공하므로 다양한 이미지를 소싱하는 것이 향후 VLM을 스케일링하는 데 점점 더 중요해질 것임을 나타낸다.\n' +
      '\n' +
      '## 5 프리즘 - 증류키 통찰력\n' +
      '\n' +
      '우리는 VLM 훈련을 단순화하고 다운스트림 성능을 향상시키는 일련의 개별 통찰력을 식별한다:\n' +
      '\n' +
      '1) _Optimization Procedure_: Single-stage training은 다운스트림 성능을 해치지 않으면서 컴퓨팅 비용을 감소시킨다.\n' +
      '\n' +
      '2) _Image Processing and Visual Representations_: Fused DINOv2 and SigLIP backbones with _high resolution images_ and _naive image resizing_는 강한 성능을 나타낸다.\n' +
      '\n' +
      '3) _Language Models_: Llama-2와 같은 Base LMs는 명령어-튜닝된 LMs의 성능을 일치시키거나 초과하며, 언어 전용 데이터_에 대한 _co-training은 안전을 위해 중요하다.\n' +
      '\n' +
      '그림 8: **[경고 – 인종 차별] 언어 전용 공동 훈련 데이터 제거. 우리는 훈련 중 언어 전용 데이터를 제거하는 것이 벤치마크(왼쪽)에 거의 영향을 미치지 않으며, 기본 LM으로 훈련된 VLM의 안전성에 부정적인 영향을 미친다는 것을 발견했다. 이 예(오른쪽)에서 라마-2에서 파생된 VLM은 명확한 인종 차별적 행동을 나타내는 반면 공동 훈련은 안전 장치를 유도한다.**\n' +
      '\n' +
      '4) _Scaling Properties_: 다양한 데이터를 추가하고 훈련 시간을 연장하면 성능이 크게 향상됩니다.\n' +
      '\n' +
      '마지막 단계로 이러한 통찰력을 결합하여 7B 및 13B 매개변수 척도에서 VLM - 프리즘 -의 새로운 패밀리를 알린다. 우리는 DINOv2 + SigLIP 프리즘 모델을 그림 9의 InstructBLIP 및 LLaVa v1.5와 비교한 결과를 제시한다. 추가로 LLaVa v1.5에 대해 정면 비교를 실행하여 모델 - 프리즘(제어됨)을 학습하고 동일한 데이터와 학습 예산을 제공한다. 프리즘 모델의 두 세트는 강력한 정성적 성능으로 평가 제품군에 걸쳐 큰 마진으로 두 기준선을 균일하게 능가한다(그림 9, 오른쪽).\n' +
      '\n' +
      '##6 한계와 미래 업무\n' +
      '\n' +
      '이 작업은 VLM 설계 선택을 엄격하게 연구하는 데 중점을 두고 있지만 우리의 접근법에는 두 가지 핵심 한계가 있다. 주요 관심사는 모델 아키텍처의 일반성이다; SS2에서 정의하는 세 가지 구성 요소 아키텍처는 기존 VLM의 대부분을 반영하지만, 본 연구에서 현재 포착하지 못하는 다른 아키텍처 혁신 및 최적화 절차가 있다; 주목할 만한 예로서, 인터리브 이미지-텍스트 트레이닝을 위해 플라밍고 또는 IDEFICS(Alayrac et al., 2022; Laurencon et al., 2023)와 같은 모델이 사용하는 Perceiver 기반 아키텍처를 연구하지 않는다. 많은 테이크아웃이 일반적이지만(예: 이러한 모델은 CLIP 및 자기회귀 LM과 같은 대조적인 백본도 사용하지만), 연구 결과가 다른 아키텍처 또는 더 큰 규모(예: 70B LM)로 전달되는 정도에 대해서는 아직 미해결 질문이 남아 있다.\n' +
      '\n' +
      '별도의 한계는 평가의 한계이다; 우리는 객관적인 측정 기준과 함께 표준화된 평가에 초점을 맞추기 위해 이 작업에서 의도적인 선택을 한다. 이를 통해 세밀한 기능을 조사할 수 있지만 기존 VLM이 제공하는 쌍방향 상호 작용의 범위를 포착하지 못하며, 시각적 컨텍스트에 기반을 둔 주제에 맞는 확장 대화를 계속할 수 있다. SS3에서 논의된 자동 평가 중 일부는 이러한 개방형 행동을 평가하기 위한 초기 단계를 제공하지만 향후 작업은 이러한 평가를 더 길고 풍부한 컨텍스트로 확장하는 방법을 조사할 것이다. 이와 관련하여 광범위하게 가능한 VLM 위에 구축된 다운스트림 애플리케이션 - 로봇 제어 정책을 학습하거나 시각적 프로그래밍을 위해 VLM을 사용하는 것과 같은 애플리케이션(Brohan et al., 2023; Sur\'is et al., 2023); 향후 작업을 위한 강력한 방법은 업스트림 VLM 및 다운스트림 시스템 모두의 성능을 향상시키기 위해 다운스트림 애플리케이션과 VLM을 공동 설계하는 방법을 이해하는 것이다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 시각 조건 언어 모델의 설계 공간에 대한 엄격한 조사를 제시하여 미래 모델 교육을 위한 핵심 통찰력을 증류한다. 이 조사는 두 가지 주요 자원 기여에 의해 가능하게 된다: 1) VLM의 능력에 대한 _fine-grained insight_를 가능하게 하는 평가 제품군, 2) 최적화 절차, 이미지 처리 및 시각적 표현, 언어 모델 및 스케일링에 대한 _flexibility_에 중점을 둔 VLM을 훈련하기 위한 최적화된 확장 가능한 코드베이스. 우리의 통찰력을 통해 InstructBLIP 및 LLaVa-v1.5와 같은 최첨단 개방형 VLM을 능가하는 VLM-프리스M 계열을 훈련할 수 있지만 이러한 모델은 이 작업의 중심 목표인 VLM_훈련 및 평가에서 향후 작업의 기초를 확립한다. 우리는 우리의 조사와 자원이 시작점이 되기를 바란다; 광범위하게 유능한 차세대 VLM을 개발하는 데 무엇이 중요한지에 대한 추론을 위한 템플릿이다.\n' +
      '\n' +
      '그림 9: **프리즘: 훈련 VLM을 위한 통찰력 결합.** §4에서 실험 결과를 훈련 VLM을 위한 일련의 핵심 통찰력으로 증류한다. VLM의 결과 계열인 프리즘은 1) 단일 단계 훈련 파이프라인, 2) 순진한 이미지 크기 조정과 융합된 DINOv2 및 SigLIP 표현, 3) 기본 LM 및 4) 두 시대에 대해 여러 데이터 소스에서 훈련한다.\n' +
      '\n' +
      '## Broader Impacts\n' +
      '\n' +
      '개방형 데이터, 개방형(및 효율적인) 훈련 코드 및 개방형 평가 코드를 사용하여 개방형 언어 모델 모델을 구축하는 것이 광범위한 기계 학습 커뮤니티 및 대중에게 엄격하게 유익하다는 확립된 입장을 취한다(Zellers et al., 2019; Touvron et al., 2023). 투명하고 우리의 작업이 모든 이해 관계자가 접근할 수 있도록 보장하는 것은 위험을 완화하고 VLM의 긍정적인 사용에 권한을 부여하는 핵심이다. 이를 위해 우리는 몇 가지 오픈 소스 자원 기여를 하는 것 외에도 다음 단락에 걸쳐 작업의 해악과 VLM을 더 광범위하게 논의한다. (1) 효율적이고 최적화된 VLM 훈련을 위한 코드베이스. (2) 세립형 VLM 능력을 평가하기 위한 평가 제품군. (3) 이 작업에서 훈련된 _all_VLM에 대한 사전 훈련된 모델 체크포인트의 전체 세트 - 그림 8의 _알려진 인종 차별 및 독성 행동을 가진 사람들을 포함한다.\n' +
      '\n' +
      '### 위험과 알려진 편견\n' +
      '\n' +
      '시각-조건 언어 모델들은 언어 모델들과 연관된 모든 위험들 및 편향들을 상속한다(Touvron et al., 2023; Brown et al., 2020), _as 뿐만 아니라, 기본 비전 모델들 및 대응하는 사전 훈련 데이터세트들(Schuhmann et al., 2021; Lin et al., 2014).\n' +
      '\n' +
      '**독성 및 안전하지 않은 출력** 도 1에 도시된 바와 같다. 도 8에 도시된 바와 같이, VLM은 독성 및 안전하지 않은 함량을 생성할 수 있다. 이것은 제자리에 "안전 장치"가 있거나 없는 경우(예: 안전 조정 데이터)입니다. SS4.3에서 언급했듯이 이 작업에 대한 우리의 탐구는 피상적이지만 인종 차별, 성차별, 학대 및 기타 안전하지 않은 언어를 생성할 가능성을 보여준다. 안전 튜닝 데이터를 훈련 믹스에 포함시키는 것은 독성 콘텐츠 생성의 용이성(작업에서 보여주는 것처럼 성능에 최소한의 비용)을 방지하기 위한 하나의 낮은 노력 방식이지만 충분하지 않다. VLM은 특히 부주의하게 안전하지 않은 출력을 트리거할 수 있는 적대적 또는 심지어 아웃-오브-분포 _이미지 입력_에 취약하다(Qi et al., 2023; Liu et al., 2023). 우리는 우리의 훈련 코드와 모델의 접근성이 그러한 문제를 완화하기 위한 향후 연구를 가능하게 하기를 바란다.\n' +
      '\n' +
      '**웨스턴 바이어스 & (미국) 영국 바이어스**. 이 작업에 사용된 데이터와 사전 훈련된 언어 모델은 미국 영어와 그에 상응하는 문화적 규범에 대한 심한 편견을 반영한다. 이 작업에서 사용하는 LM은 일부 다국어 데이터에 노출되지만(VLM이 스페인어, 프랑스어 및 중국어와 같은 언어로 간단한 구문을 처리하는 몇 가지 능력을 보여주므로), 중요한 제한 사항은 _visual_ 데이터 다양성에 있다. 우리의 사전 훈련 이미지는 COCO(Lin et al., 2014)와 같은 데이터 세트로부터 소스되고, 주로 Flickr의 (영어) 서브세트로부터 소스된다.\n' +
      '\n' +
      '**사실성, 환각, 신뢰** LL과 VLM의 알려져 있는 한계는 사실성과 환각의 한계이며, VLM의 경우 모델이 후속 상호 작용에 대해 강화되는 장면의 객체 또는 특성을 "상상"하는 경향이 있기 때문에 특히 문제가 된다. 이러한 이유로, 우리는 VizWiz(Bigham et al., 2010) 및 POPE(Li et al., 2023)를 우리의 평가 스위트에 모두 포함한다; VizWiz는 모델 신뢰도를 조사하기 위해 명시적으로 사용되는 일련의 커먼센스 질문 및 _unanswerable question_를 갖는다. POPE는 다양한 어려움에서 환각을 평가하기 위해 특별히 생성된 벤치마크이다(예를 들어, 이미지에 묘사된 장면의 유형과 강한 동시 발생을 갖는 적대적 객체, 일반적으로 인기 있는 객체 등에 대해 질문할 때). 이러한 작업을 평가 스위트의 일부로 포함시킴으로써 미래의 VLM은 환각 감소 및 신뢰도 향상(및 그 반대)으로 이어지는 설계 선택을 하는 방향으로 이동하기를 바란다.\n' +
      '\n' +
      '### 혜택과 잠재적 기회\n' +
      '\n' +
      'SS1 및 SS6에서는 VLM이 이미 긍정적인 영향을 미치고 있어 로봇 공학, 시각적 프로그래밍 등과 같은 분야에서 연구를 가속화하는 응용 프로그램에 대해 논의한다. 여기에서 우리는 우리의 작업, 특히 자원 기여가 광범위한 연구 커뮤니티에 제공하는 혜택과 기회에 대해 구체적으로 이야기한다.\n' +
      '\n' +
      '**훈련 및 파인튜닝 접근성** VLM 훈련 코드베이스의 주요 이점 중 하나는 7B 매개변수 VLM(예: 프리즘 7B(제어됨))을 완전히 훈련시키는 효율성이다. 9)는 8개의 A100 GPU에서 9시간 미만이 소요되며 개별 GPU(또는 CPU)에서 미세 조정 및 평가가 가능하며 이는 훨씬 덜 효율적인 VLM 훈련을 위한 기존 코드베이스와 극명한 대조를 이룬다. VLM 개발을 중심으로 새로운 아이디어를 시도하기 위한 진입 장벽을 줄이는 것은 다운스트림 애플리케이션에 대한 위험 완화, 강력한 평가 및 통합의 진행을 가능하게 하는 핵심이다. 또한, 우리의 트레이닝 코드베이스의 _flexibility_는 더 작고, 더 컴퓨트-효율적인 컴포넌트들(예를 들어, 1B 스케일에서의 새로운 LMs)에서의 스와핑을 가능하게 한다.\n' +
      '\n' +
      '평가실을 확장합니다. 우리의 평가 스위트는 새로운 VLM을 추가하고 평가하기 쉽고 새로운 작업을 추가하는 방식으로 작성된다. 새로운 평가(특히 편향, 독성, 환각 및 기타 안전하지 않거나 바람직하지 않은 행동을 조사하는 것)로 제품군을 지속적으로 확장할 계획입니다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '도요타 연구소("TRI")는 이 작업을 지원하기 위해 자금을 제공했다. 싯타르스 카람체티는 열린 자선사업 프로젝트 AI 펠로우십의 지원을 받는 것에 감사한다. 마지막으로 아드리안 가이돈, 라레스 암브루스, 아찰 데이브, 블레이크 울프, 마샤 이티나, 장 메르카트, 이고르 바실예비치, 세드릭 케, 쿠샬 아로라, 존 틱스턴, 데이비드 홀 등 이 작품의 초기 개발 과정에서 보여준 통찰력과 조언에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Acharya et al. (2018) Acharya, M., Kafle, K., and Kanan, C. TallyQA: Answering complex counting questions. In _Association for the Advancement of Artificial Intelligence (AAAI)_, 2018.\n' +
      '* AI(2023) AI, A. Fuyu-8b: A multimodal architecture for AI agent, 2023.\n' +
      '* Alabdulmohsin et al. (2023) Alabdulmohsin, I. M., Zhai, X., Kolesnikov, A., and Beyer, L. ViT의 모양을 만드는 방법: 계산-최적 모델 설계를 위한 척도법 _ arXiv preprint arXiv:2305.13035_, 2023.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Vinyals, O., Zisserman, A., and Simonyan, K. 플라밍고: 몇 개의 샷 학습을 위한 시각적 언어 모델. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* Bai et al. (2023) Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _ arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* Biderman et al. (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: 훈련 및 스케일링에 걸쳐 큰 언어 모델을 분석하기 위한 스위트. In _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* Bigham et al. (2010) Bigham, J. P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R. C., Miller, R., Tatarowicz, A., White, B., White, S., and Yeh, T. 비즈위즈: 시각적 질문에 대한 거의 실시간 답변. _User Interface Software and Technology(UIST)_, pp. 333-342, 2010.\n' +
      '* Brohan et al.(2020) Brohan, A., Brown, N., Carbajal, J., Choromanski, K., Ding, T., Driess, D., Finn, C., Arenas, M. G., Herzog, A., Hsu, J., Ichter, B., Irpan, A., Joshi, N. J., Julian, R. C., Rao, K., Reymann, K., Ryoo, M. S., Salazar, G., Siao, T., Yu, T., Zitkovich, V. H., Wahid, A., Welker, S., Wohlhart, P. arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* Brown 등 (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Grey, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D. 언어 모델은 소수의 학습자를 포함한다. arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* Chen et al. (2023a) Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llms referential dialogue magic. _ arXiv preprint arXiv:2306.15195_, 2023a.\n' +
      '* Chen et al. (2023b) Chen, X., Wang, X., Beyer, L., Kolesnikov, A., Wu, J., Voigtlaender, P., Mustafa, B., Goodman, S., Alabdulmohsin, I. M., Padlewski, P., Salz, D. M., Xiong, X., Vlasic, D., Pavetic, F., Rong, K., Yu, T., Keysers, D., Zhai, X. - Q., and Soricut, R. PaL-I-3 비전 언어 모델: Smaller, faster, stronger. _ arXiv preprint arXiv:2310.09199_, 2023b.\n' +
      '* Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, W., Zhao, V., Huang, Y., Dai, A. M., Yu, H., Petrov, S., hsin Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., Wei, J. Scaling instruction-finetuned 언어 모델. ArXiv:2210.11416_, 2022.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B. A., Fung, P., and Hoi, S. C. H. InstructBLIP: Towards generalpurpose vision-language models with instruction tuning. _ arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* Dosovitskiy et al.(2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 이미지는 16x16 단어의 가치가 있습니다: 스케일에서 이미지 인식을 위한 트랜스포머입니다. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q. H., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. R. Palm-e: embodied multimodal language model. In _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '*Gao et al. (2022) Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y. J.\n' +
      '\n' +
      'Llama-adapter v2: Parameterefficient visual instruction model. _ arXiv preprint arXiv:2304.15010_, 2023.\n' +
      '* Gong et al.(2023) Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., and Chen, K. 멀티모달 GPT: 인간과의 대화를 위한 비전과 언어 모델. _ ArXiv_, 0, 2023.\n' +
      '* Goyal et al. (2017) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. _Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* Gupta et al. (2019) Gupta, A., Dollar, P., and Girshick, R. B. LVIS: A dataset for large vocabulary instance segmentation. In _Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* Hendrycks & Gimpel (2016) Hendrycks, D. and Gimpel, K. 가우시안 오차 선형 단위(gelus) _ ArXiv:1606.08415_, 2016.\n' +
      '* Hudson & Manning (2019) Hudson, D. A. and Manning, C. D. GQA: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. In _Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de Las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M. -A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* 재현 가능한 언어 모델 교육을 향한 여정, 2021.\n' +
      '* Karamcheti et al. (2023) Karamcheti, S., Nair, S., Chen, A. S., Kollar, T., Finn, C., Sadigh, D., and Liang, P. Language-driven representation learning for robotics. _Robotics: Science and Systems (RSS)_, 2023.\n' +
      '* Kazemzadeh et al. (2014) Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. ReferItGame: 자연 장면의 사진에 있는 물체를 참조하세요. In _Empirical Methods in Natural Language Processing (EMNLP)_, pp. 787-798, 2014.\n' +
      '* Kerr et al. (2023) Kerr, J., Kim, C. M., Goldberg, K., Kanazawa, A., and Tancik, M. LERF: 언어 내장 복사 필드. In _International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* Kobayashi et al. (2022) Kobayashi, S., Matsumoto, E., and Sitzmann, V. 특징 필드 증류를 통해 편집을 위한 너프를 분해하는 단계; _ arXiv preprint arXiv:2205.15585_, 2022.\n' +
      '* Krishna et al. (2017) Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidi, Y., Li, L. - J., Shamma, D. A., Bernstein, M. S., and Li, F.-F. 시각 게놈: 크라우드소싱된 밀집 이미지 주석을 사용하여 언어와 시각을 연결하는 것. _ International Journal of Computer Vision_, 123:32-73, 2017.\n' +
      '* Laurencon et al. (2023) Laurencon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A. M., Kiela, D., Cord, M., and Sanh, V. OBELICS: 인터리빙된 이미지-텍스트 문서들의 오픈 웹-스케일 필터링된 데이터세트. _Neural Information Processing Systems Track on Datasets and Benchmarks(NeurIPS Datasets and Benchmarks)_, 2023.\n' +
      '* Li 등(2023a) Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., and Liu, Z. Oterhd: 고해상도 멀티 모달리티 모델. _ arXiv preprint arXiv:2311.04219_, 2023a.\n' +
      '* Li et al. (2022) Li, J., Li, D., Xiong, C., and Hoi, S. C. H. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning (ICML)_, 2022.\n' +
      '* Li 등(2023b) Li, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: Bootstrapping language-image pre-training with frozen image encoder and large language models. In _International Conference on Machine Learning(ICML)_, 2023b.\n' +
      '*Li 등(2023c) Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. 비디오 채팅: 채팅 중심의 비디오 이해. _ ArXiv_, 0, 2023c.\n' +
      '* Li&Liang(2021) Li, X. L. and Liang, P. Prefix-tuning: 생성을 위한 연속 프롬프트를 최적화한다. In _Association for Computational Linguistics (ACL)_, 2021.\n' +
      '* Li 등(2023d) Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and rong Wen, J. Evalating object hallucination in large vision-language models. _Empirical Methods in Natural Language Processing (EMNLP)_, 2023d.\n' +
      '* Lin et al. (2014) Lin, T. -Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. In _European Conference on Computer Vision (ECCV)_, pp. 740-755, 2014.\n' +
      '* Liu et al. (2022) Liu, F., Emerson, G. E. T., and Collier, N. 시각적 공간 추론 Transactions of the Association for Computational Linguistics (TACL)_, 11:635-651, 2022.\n' +
      '*Liu et al.(2023a) Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L. 강력한 명령어 튜닝을 통해 대형 멀티모달 모델의 환각을 완화합니다. _ arXiv preprint arXiv:2306.14565_, 2023a.\n' +
      '*Liu et al. (2023b)*Liu et al. (2023) Liu, H., Li, C., Li, Y., and Lee, Y. J. improved baseelines with visual instruction tuning. _ arXiv preprint arXiv:2310.03744_, 2023b.\n' +
      '*Liu et al. (2023) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023c.\n' +
      '*Liu et al. (2023) Liu, X., Zhu, Y., Lan, Y., Yang, C., and Qiao, Y. 질의 관련 이미지가 대형 멀티모달 모델을 탈옥시킵니다. _ arXiv preprint arXiv:2311.17600_, 2023d.\n' +
      '*Liu et al. (2023) Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. MMBench: your multi-modal model is all-around player? _ arXiv preprint arXiv:2307.06281_, 2023e.\n' +
      '* Marino et al. (2019) Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. OK-VQA: 외부 지식이 필요한 시각적 질문 응답 벤치마크. In _Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* Mishra et al. (2019) Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A. OCR-VQA: Visual question answering by reading text in images. In _International Conference on Document Analysis and Recognition (ICDAR)_, 2019.\n' +
      '* OpenAI 등 (2019) OpenAI, Achiam, J., Adler, S., Agarwal, S., Anadkat, L., Akkaya, I., Balaji, S., Balcom, L., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brundage, K., Cai, T., Chen, S., Greene, R., G., Ecoffet, A., Eloundou, J., Felix, N., Doorhes, J., Forte, J., Gu. arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Oquab et al. (2023) Oquab, M., Darcet, T., Moutakanni, T., Vo, H. Q., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.-Y. B., Li, S. -W., Misra, I., Rabbat, M. G., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatt, P., Joulin, A., and Bojanowski, P. DINOV2: Learning robust visual features without supervision. _ TMLR(Transactions of Machine Learning Research)_, 2023.\n' +
      '* Ordonez et al. (2011) Ordonez, V., Kulkarni, G., and Berg, T. L. Im2Text: 100만장의 캡션 사진을 이용하여 이미지를 기술하는 것. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2011.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. _ arXiv_, 2022.\n' +
      '* Qi et al.(2019) Qi, X., Huang, K., Panda, A., Wang, M., and Mittal, P.\n' +
      '\n' +
      '시각적 적대적 예제 탈옥은 대규모 언어 모델을 정렬했다. _ arXiv preprint arXiv:2306.13213_, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. 자연 언어 감독으로부터 전이 가능한 시각적 모델들을 학습한다. In _International Conference on Machine Learning(ICML)_, volume 139, pp.8748-8763, 2021.\n' +
      '* Rasley et al. (2020) Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. 딥스피드: 시스템 최적화는 1,000억 개 이상의 매개 변수를 가진 딥 러닝 모델을 훈련할 수 있게 한다. 2020년 KDD(Knowledge Discovery and Data Mining) 국제회의에서\n' +
      '* Schuhmann et al. (2021) Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs. _ arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Schwenk et al. (2022) Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-OKVQA: 세계지식을 이용한 시각적 질의응답의 벤치마크. _ arXiv preprint arXiv:2206.01718_, 2022.\n' +
      '* ShareGPT(2023) ShareGPT. 공유 GPT, 2023년\n' +
      '* Sharma et al. (2018) Sharma, P., Ding, N., Goodman, S., and Soricut, R. 개념 캡션: 자동 이미지 캡션을 위한 세척, 하이퍼니밍, 이미지 알트 텍스트 데이터 세트. In _Association for Computational Linguistics (ACL)_, 2018.\n' +
      '* Sidorov et al. (2020) Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. TextCaps: a dataset for image captioning with reading comprehension. _European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* Singh et al. (2019) Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. 읽을 수 있는 VQA 모델을 향합니다. In _Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '*Steiner et al. (2021) Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., and Beyer, L. VIT를 어떻게 훈련시키죠? data, augmentation and regularization in vision transformer. _ TMLR(Transactions of Machine Learning Research)_, 2021.\n' +
      '* Subramanian et al. (2023) Subramanian, S., Narasimhan, M. G., Khangaonkar, K., Yang, K., Nagrani, A., Schmid, C., Zeng, A., Darrell, T., and Klein, D. Modular visual question answering via code generation. In _Association for Computational Linguistics (ACL)_, 2023.\n' +
      '* Sur\'is et al. (2023) Sur\'is, D., Menon, S., and Vondrick, C. ViperGPT: Visual inference via Python execution for reasoning. In _International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* Tan & Bansal(2019) Tan, H. H. and Bansal, M. LXMERT: 트랜스포머로부터 교차-모달리티 인코더 표현들을 학습하는 단계. _Empirical Methods in Natural Language Processing (EMNLP)_, 2019.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K. R., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esibou, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A. S., Hosseini, S., Hardas, M., Kerkez, V., Khabsa, M., Kloumann, I. M., Korenev, A. V., Koura, P. S., Lachaux, M. - A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zambadur, M., Fan, A., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wang et al. (2023) Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., and Jiang, Y. - G. 보는 것은 믿는 것이다: 더 나은 시각적 지시 튜닝을 위해 GPT-4V를 프롬프트하는 것 _ arXiv preprint arXiv:2311.07574_, 2023.\n' +
      '* Wang et al. (2021) Wang, K. - J., 류영 - H., Su, H.-T., Wang, J.-W., Wang, Y. - S., Hsu, W. H., and Chen, W. - C. OCID-Ref: 클러터 장면 접지를 위한 체화된 언어를 갖는 3d 로봇 데이터세트. In _Association for Computational Linguistics (ACL)_, 2021.\n' +
      '* Wightman(2019) Wightman, R. 파이토치 이미지 모델. [https://github.com/rwightman/pytorch-image-models] (https://github.com/rwightman/pytorch-image-models), 2019.\n' +
      '* Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., and Brew, J. HuggingFace\'s Transformers: State-of-the-art 자연어 처리. _ ArXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* Ye et al. (2023) Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., Li, C., Xu, Y., Chen, H., Tian, J., Qi, Q., Zhang, J., and Huang, F. mPLUG-Owl: Modularization empowers large language models with multimodality. _ arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. The _EuropeanConference on Computer Vision (ECCV)_, 2016.\n' +
      '* Yu et al. (2023) Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. MM-Vet: 통합 기능에 대한 대규모 멀티모달 모델 평가 arXiv preprint arXiv:2308.02490_, 2023.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Rashkin, H., Bisk, Y., Farhadi, A., Roesner, F., and Choi, Y. 신경 가짜 뉴스로부터 방어하는 것. In _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 9054-9065, 2019.\n' +
      '* Zhai et al. (2023) Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. 언어 이미지 사전 훈련에 대한 시그모이드 손실 In _International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* Zhao et al. (2023) Zhao, Y., Gu, A., Varma, R., Luo, L., chin Huang, C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Nguyen, B., Chauhan, G., Hao, Y., and Li, S. PyTorch FSDP: 완전히 샤드된 데이터를 병렬로 스케일링하는 데 대한 경험. _Very Large Data Bases (VLDB)_, 2023.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W. L., Sheng, Y., Zhu, S., Wu, Z., Zang, Y., Lin, Z., Li, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. LLM-as-a-판정을 MT-벤치와 챗봇 아레나로 판단한다. _ arXiv preprint arXiv:2306.05686_, 2023.\n' +
      '\n' +
      '도 11: **정성적 예제 - 지시-조정 대. 기본 LMs. 우리는 기본 LMs(예: 라마-2)가 지시 조정된 LMs(예: 비쿠냐 v1.5)에 비해 약간 더 나은 질적 성능을 가지고 있음을 발견했다. 놀랄 것도 없이, 지시 조정된 LM은 때때로 더 장황한 출력을 생성한다. 이 장황함은 비쿠냐 v1.5 모델이 남성이 칼을 들고 있다는 것을 잘못 나타내는 왼쪽 원숭이 예에서와 같이 _hallucinations_로 이어질 수 있다. 우리는 BLIP-2(Li et al., 2022) 논문(오른쪽)의 예제에 대해 두 모델을 추가로 평가한다. 우리는 베이스 라마-2 모델이 중국의 만리장성을 정확하게 식별하는 것과 같이 더 정확한 응답을 제공한다는 것을 발견하여 추가 배경 정보를 제공한다.**\n' +
      '\n' +
      '그림 10: **추가 LM & 스케일링 결과. 본 논문에서는 §4.3과 §4.4의 연구를 보완한 추가 결과를 제시한다. 첫째, 13B 매개변수 척도에서 기본 LMs가 지시 조정 모델(왼쪽)과 비교 가능하게 수행됨을 발견한다. 다음으로 훈련 시간이 모델 성능을 향상시키는 방법에 대해 연구합니다. 우리는 단일 에폭에서 심각한 _underfitting_의 증거를 발견하며, 두 에폭에 도달할 때까지 성능이 개선되며, 이 시점에서 성능이 안정된다(중간). 마지막으로, 스케일 업 데이터의 영향을 연구한다; 데이터를 추가하는 것은 이미지 다양성(오른쪽)의 증가로 인해 더 큰 영향을 갖는 LRV-Instruct 데이터세트(Liu et al., 2023)로 집계 성능을 향상시킨다.**\n' +
      '\n' +
      '시각적 조건 언어 모델 학습\n' +
      '\n' +
      '다음 섹션에서는 작업 전반에 걸쳐 사용하는 LLaVa v1.5 사전 훈련 데이터 세트에 대한 확장된 논의, 가장 중요한 VLM 아키텍처의 각 구성 요소에 대한 구체적인 구현 세부 사항 및 VLM 훈련을 위한 하이퍼 파라미터를 포함하여 VLM 훈련 절차에 대한 추가 세부 사항을 제공한다. 다음의 모든 정보는 또한 우리의 VLM 훈련 코드베이스에 명시되어 있다.\n' +
      '\n' +
      '### 사전 훈련 데이터 세트 구성\n' +
      '\n' +
      'SS2에 설명된 대로 대부분의 실험을 위해 LLaVa v1.5(Liu et al., 2023) 사전 훈련 데이터 세트를 사용한다. 데이터세트는 SS4.1에 기술된 다단계 훈련 절차에 사용되는 두 개의 고유한 하위 집합으로 구성되며, 첫 번째 단계("시각 언어 정렬") 동안 프로젝터\\(F_{\\psi}\\)만 훈련되어 시각적 표현과 LM의 가중치를 동결한다. 두 번째 단계("멀티모달 명령 튜닝") 동안, \\(F_{\\psi}\\) 및 LM이 모두 트레이닝된다.\n' +
      '\n' +
      '**시각 언어 정렬** 첫 번째 서브세트는 LAION(Schuhmann et al., 2021), Conceptual Captions(CC; Sharma et al., 2018), SBU Captions(SBU; Ordonez et al., 2011)로부터 획득된 이미지들로 구성되며, 캡셔닝에 최적화된 초기 VLM인 BLIP(Li et al., 2022)로부터 합성적으로 생성된 캡션들로 증강된다. 이 훈련의 첫 번째 단계의 목표는 단순히 프로젝터를 초기화하는 것이기 때문에, 훈련은 간단하다: 입력으로서 이미지만을 주어(예를 들어, 언어 프롬프트가 없음\\(u_{\\text{prompt}}\\)) 그리고 대응하는 캡션을 생성하고, LM을 통해 기울기(gradient)를 전파한다(가중치를 동결). 이 데이터 세트는 총 558K(이미지, 캡션) 쌍으로 구성되며, 여기서 캡션은 문장보다 길지 않다.\n' +
      '\n' +
      '**멀티모달 명령어 튜닝** 제2 서브세트는 튜닝 예_를 지시하는 665K _멀티모달로 구성된다. 채팅-유사 행동을 유도하고 VLM이 특정 태스크들을 수행할 수 있게 하기 위해, Liu 등(2023)은 혼합물 내의 각각의 데이터세트에 대한 "트리거 프롬프트들" \\(u_{\\text{prompt}}\\)의 세트를 식별한다; 이러한 트리거 프롬프트들은 명령(예를 들어, "이미지를 묘사하라." 또는 "이 문장이 묘사하는 영역에 대한 바운딩 박스 좌표 제공...")의 형태를 취한다. 멀티모달 명령 튜닝 예들은 다음과 같이 소싱된다:\n' +
      '\n' +
      '_LLaVa Synthetic Data_(158K). GPT-4(OpenAI et al., 2023)를 COCO(Lin et al., 2014)로부터 이미지 캡션들 및 객체 바운딩 박스들로 프롬프트함으로써 소싱된 Liu et al.(2023)로부터의 대화들, 세밀한 설명들, 및 질문-응답 데이터의 합성적으로 생성된 데이터세트. 이 데이터 세트는 위의 "명령" 형식에 따라 명시적으로 생성되었기 때문에 별도의 트리거 프롬프트를 정의할 필요가 없다.\n' +
      '\n' +
      '_Standard VQA Data_(224K). VQAv2(일반 질문 응답; Goyal et al., 2017), GQA(공간 및 구성 추론; Hudson and Manning, 2019), OK-VQA(외부 지식을 필요로 하는 추론; Marino et al., 2019) 및 OCR-VQA(이미지 내 텍스트/로고스에 대한 추론; Mishra et al., 2019)의 훈련 세트로부터 소싱된 시각적 질문 응답 데이터의 조합이다. VLM이 적절한 형식의 응답을 생성하도록 장려하기 위해 LLaVa v1.5는 다음과 같은 트리거 프롬프트를 정의한다.\n' +
      '\n' +
      '_Multiple Choice VQA Data_ (50K). A-OKVQA(다양한 외부 지식을 필요로 한다; Schwenk et al., 2022). 이는 객관식 과제이기 때문에 LLaVa v1.5는 다음과 같은 트리거 프롬프트를 정의한다. "\\(\\langle\\)Question) → A. \\(\\langle\\)Option A\\(\\rangle\\)B. \\(\\langle\\)Option B\\(\\rangle\\)… 주어진 선택사항에서 직접 옵션의 문자로 답한다."\n' +
      '\n' +
      '_Captioning Data_(22K). TextCaps(Text/logos를 가진 이미지들; Sidorov et al., 2020)로부터 소싱된 이미지들 및 캡션들. LLaVa v1.5는 다음과 같은 트리거 프롬프트를 정의한다: "제공된 이미지에 대해 원 문장 캡션을 제공"\n' +
      '\n' +
      '_expression Data_(116K)를 나타낸다. RefCOCO(Kazemzadeh et al., 2014; Yu et al., 2016) 및 Visual Genome(Krishna et al., 2017)으로부터 소싱된 표현 접지(경계 박스 예측) 및 영역 캡션 데이터를 참조한다. 경계 박스 예측(국소화)을 위해, 모델은 _정규화된 경계 박스 좌표_(자연어 스트링으로서)를 생성하는 태스크를 갖는다. 로컬라이제이션 태스크를 위해, LLaVa v1.5는 다음과 같은 트리거 프롬프트를 정의한다: "\\(\\langle\\)Referring Expression\\(\\rangle\\) Provide the bounding box coordinates of the region of this sentence describes." 역 태스크(region caption)를 위해, LLaVa v1.5는 별도의 트리거 프롬프트를 정의한다: "Provide the bounding box coordinates of the region of this sentence describes."\n' +
      '\n' +
      '_ShareGPT(Language-Only)_(40K). ShareGPT(ShareGPT, 2023)에서 조달한 언어 전용 공동 학습 데이터는 ChatGPT와의 사용자 업로드 대화로 구성된다. 위에서 설명한 LLaVa 합성 데이터와 유사하게 이 데이터는 이미 예상된 "명령어" 형식으로 별도의 트리거 프롬프트가 필요하지 않습니다.\n' +
      '\n' +
      '### 구현 - 아키텍처 구성요소 및 최적화\n' +
      '\n' +
      '우리는 훈련 코드베이스를 PyTorch에서 구현하며, GPU에 걸쳐 훈련을 분배하기 위해 그 네이티브 Fully Sharded Data Parallel (FSDP; Zhao et al., 2023) 구현을 활용한다. 우리는 BF16 혼합 정밀도로 모든 모델을 훈련한다. 다음 섹션에서는 SS2에 설명된 대로 VLM의 개별 구성 요소 각각에 대한 추가 세부 정보를 제공한다.\n' +
      '\n' +
      '**이미지 처리 및 시각적 표현** 우리는 Torchvision과 PyTorch Image Models(TIMM; Wightman, 2019)에서 제공하는 기본 이미지 변환을 사용하여 모든 이미지 처리 로직을 구현한다. SS4.2에서 평가하는 다양한 방식에 의해 적용되는 크기 조정 로직 외에도 미리 훈련된 각 백본(종종 전통적인 이미지넷 기본값)에 의해 정의된 기본값을 사용하여 픽셀 값을 정규화한다.\n' +
      '\n' +
      '본 연구에서 평가하는 모든 시각적 표현 \\(V_{\\omega}\\)에 의해 사용되는 기본 백본은 Vision Transformer(ViT; Dosovitskiy et al., 2021); LLAa v1/v1.5(Liu et al., 2023)에서 정의된 연습에 따라 ViT의 _penultimate_ 레이어로부터 패치 특징을 추출한다.\n' +
      '\n' +
      '**시각 언어 프로젝터** 프로젝터\\(F_{\\psi}\\)는 임의의 복잡도를 가질 수 있지만, 각 패치를 LM 임베딩 공간에 독립적으로 투사하는 간단한 2-레이어 GELU MLP(Hendrycks and Gimpel, 2016)를 초기화한다.\n' +
      '\n' +
      '**언어 모델** F_{\\psi}\\(F_{\\psi}\\)에서 출력되는 투영 패치 "embeddings"를 언어 프롬프트 임베딩(E_{\\phi}(\\text{u}}_{\\text{prompt})과 결합하기 위해 간단한 시퀀스 연결(sequence-wise concatenation)을 수행하여 프롬프트 임베딩의 "왼쪽"에 패치 임베딩을 삽입한다. 이는 많은 선행 VLM들(Liu et al., 2023; Ye et al., 2023; Gao et al., 2023)에 의한 프로세스를 따르고, 프리픽스 튜닝(Li and Liang, 2021)과 유사하며, 여기서 패치 임베딩들은 랜덤하게 초기화된 프리픽스 임베딩들을 대신한다.\n' +
      '\n' +
      '프롬프트 기지 대 명령어 조정 LMs**. 지시 조정된 LMs(예: Vicuna v1.5) 및 기본 LMs(예: Llama-2)를 수용하기 위해 서로 다른 프롬프트를 사용한다. Vicuna v1.5의 경우 시스템 프롬프트와 특수 포맷 "USER" 및 "ASSISTANT" 블록으로 구성된 예상 채팅 형식을 사용합니다. 우리는 LLAaVa v1.5에서 채택된 동일한 시스템 프롬프트를 사용한다 - "호기심 많은 사용자와 인공지능 어시스턴트 사이의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 예의바른 답변을 제공한다." 그리고 프롬프트 포맷팅을 위한 템플릿은 다음과 같다:\n' +
      '\n' +
      '<\\)s\\(>\\)USER: {Input 1} ASSISTANT: {Response} \\(<\\)\\(\\backslash\\)s\\(>\\)\n' +
      '\n' +
      '기본 LMs(예: Llama-2)의 경우 시스템 프롬프트를 완전히 제외합니다. 예제를 다음과 같이 포맷합니다.\n' +
      '\n' +
      '\\(<\\)s\\(>\\)In: {Input 1} Out: {Response} \\(<\\)\\(\\backslash\\)s\\(>\\)\n' +
      '\n' +
      '### Training Hyperparameters\n' +
      '\n' +
      '우리는 (7B 및 13B 모두에 대한) 단일 단계 실험에 대해 표 1의 하이퍼파라미터를 채택한다. 다단계 사전 훈련(예: 그림 4의 실험에 대해서만)의 경우 비전 언어 정렬을 위해 프로젝터\\(F_{\\psi}\\)을 훈련할 때 배치 크기를 256으로 늘리고 학습 속도를 1e-3으로 늘리며 다른 모든 하이퍼파라미터를 동일하게 유지한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline\n' +
      '**Hyperparameter** & **Value** \\\\ \\hline Batch Size & 128 \\\\ Max Gradient Norm & 1.0 \\\\ Weight Decay & 0.1 \\\\ Learning Rate & 2e-5 \\\\ Optimizer & AdamW \\\\ Scheduler & Warmup \\& Cosine Decay \\\\ Warmup Ratio & 0.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **훈련용 하이퍼파라미터**\n' +
      '\n' +
      '## 부록 B 평가 프로토콜\n' +
      '\n' +
      '우리는 평가 작업에 대해 VLM을 프롬프트하는 방법, 각 평가 작업에 대한 메트릭을 계산하는 방법, 마지막으로 결론을 도출할 때 통계적 유의성을 계산하는 방법에 대해 추가 세부 정보를 제공하는 등 평가 절차 주변에 추가 세부 정보를 제공한다. 이러한 절차는 또한 평가 코드베이스에 명시되어 있다.\n' +
      '\n' +
      '### Evaluation Procedures\n' +
      '\n' +
      '**응답 발생** 결정론적 평가를 실행하고 서로 다른 모델을 공정하게 비교하기 위해, 우리는 _greedy decoding_를 통해 출력을 생성하는데, 이것은 일관성을 보장하지만 핵 샘플링 또는 빔 검색과 같은 다른 LM 생성 전략을 사용하는 것과 비교하여 더 나쁜 품질의 출력을 초래할 수 있다는 점에 주목한다.\n' +
      '\n' +
      '** 개별 작업에 대한 VLM 요청** Appx에 의해 입증된 바와 같이. A, 상이한 "트리거 프롬프트들"은 모델들이 특정 구조의 출력들(예를 들어, VQAv2와 같은 시각적 질문 응답 평가들을 위한 짧은 문구들)을 생성하도록 유도한다. 모델 간의 비교에서 사전 훈련 데이터 세트 또는 원래 작업에서 정의된 트리거 프롬프트를 사용합니다. 구체적으로, 우리는 Appx에서 트리거 프롬프트를 사용한다. A in evaluating our model and LLaVa v1.5, and those defined in Dai et al.(2023) for InstructBLIP.\n' +
      '\n' +
      '**컴퓨팅 평가 메트릭** 모든 개방형 시각적 질문 응답 작업(VQAv2, TextVQA, GQA, TextVQA)에 대해 공식 평가 스크립트에 의해 계산된 _accuracy_를 보고한다. TextVQA의 경우 OCR 시스템에 의해 파싱된 입력 토큰으로 VLM이 추가로 프롬프트되는 평가의 변형도 실행합니다. 이러한 숫자 _는 부록_(표 2)의 끝에만 보고되며, 공식 LLaVa v1/v1.5 및 InstructBLIP에서 사용되는 평가 절차와 일치할 뿐이다. 본문 본문의 TextVQA는 이미지와 질문에 대한 액세스(OCR 시스템 입력 없이)만을 가정하여 실행된다.\n' +
      '\n' +
      '국산화 작업의 경우 공식 평가에서 정의된 특정 IoU 임계값에서 정확도를 보고하며, RefCOCO/RefCOCO+/RefCOCOg의 경우 0.5 IoU(Yu et al., 2016)인 반면 OCID-Ref의 경우 0.25 IoU(Wang et al., 2021)이다.\n' +
      '\n' +
      '마지막으로, 챌린지 태스크의 경우, 각각의 예를 객관식 질문 및 보고 정확도로 포맷한다; VSR 및 POPE의 경우, 이것은 두 가지 옵션(각각 True/False 및 Yes/No)을 의미하고, TallyQA의 경우, 16개(숫자 0 - 15, 포함)를 의미한다.\n' +
      '\n' +
      '### 모델 성능 비교 - 유의성 검정\n' +
      '\n' +
      'SS4에서 다루어진 바와 같이 각 평가 작업은 서로 다른 상대 척도와 함께 서로 다른 메트릭을 사용하여 직접적인 비교가 어렵다. 각 모델에 대한 정규화된 Z-점수와 평가(모든 모델에 걸친 평균 및 표준 편차를 사용)를 계산하고 12개의 벤치마크 모두에 걸쳐 평균을 내어 전역 점수를 계산함으로써 이를 해결한다. 주어진 설계 선택의 영향에 대한 결론을 도출하기 위해 비교를 위한 두 가지 모델 세트를 정의한다. 기본 집합은 기본 구성과 함께 귀무 가설을 반영하는 반면 대체 집합은 새 설계 선택을 반영합니다. 기본 및 대체 집합에 걸쳐 각 모델 쌍에 대해 정규화된 성능 차이를 계산하고 유의성을 계산하기 위해 1-측 피셔 T-검정을 수행한다.\n' +
      '\n' +
      '### Exhaustive Results\n' +
      '\n' +
      '완전성을 위해 이 작업에서 훈련된 모든 모델에 대한 평가 결과를 표로 작성한다. 개방형 VQA 결과는 표 2, 로컬라이제이션 결과는 표 3, 챌린지 세트 결과는 표 4이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Model & VQAv2 & GOA & VizWiz & TextVQA+OCR & TextVQA \\\\ \\hline\n' +
      '**Official Models** & & & & & \\\\ LL1AvA v1.5 7B & 76.54 & 61.58 & 54.24 & 58.25 & 46.13 \\\\ LLAvA v1.5 13B & 78.13 & 63.17 & 56.66 & 61.47 & 48.99 \\\\ InstructBell I7B & 76.12 & 48.41 & 32.02 & 28.01 & 33.54 \\\\ InstructBell I7B & 59.46 & 42.92 & 30.65 & 33.24 & 27.90 \\\\ \\hline\n' +
      '**Reproduction \\& Optimization Procedure** & & & & \\\\ LL1AvA v1.5 7B (Reproduction) & 76.80 & 62.28 & 51.26 & 57.91 & 46.44 \\\\ LL1AvA v1.5 13B (Reproduction) & 77.78 & 62.91 & 54.83 & 59.60 & 48.74 \\\\ Single-Stage 7B & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ Single-Stage 13B & 77.96 & 63.17 & 56.37 & 59.30 & 48.03 \\\\ Frozen WT (Single-Stage) & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ Finuent WT (Multi-Stage) & 74.36 & 60.08 & 57.27 & 56.56 & 44.40 \\\\ Finuent WT (Single-Stage) & 73.53 & 59.65 & 55.26 & 53.81 & 38.33 \\\\ \\hline\n' +
      '**Visual Representation** & & & & \\\\ IN1K VIT-2.224px & 68.26 & 56.82 & 49.61 & 44.54 & 12.31 \\\\ IDNO2 VIT-2.1224px & 66.29 & 55.64 & 48.37 & 44.70 & 12.62 \\\\ CLIP VIT-2.224px & 75.32 & 61.58 & 54.52 & 53.89 & 36.61 \\\\ SigLIP VIT-SO 224px & 76.32 & 62.15 & 58.82 & 55.75 & 40.50 \\\\ \\hline\n' +
      '**Image Preprocessing** & & & & & \\\\ CLIP VIT-3.365ms (Letterbox) & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ CLIP VIT-3.365ms (Revine Crop) & 77.07 & 62.29 & 58.15 & 58.06 & 48.83 \\\\ CLIP VIT-3.365px (Navie Resize) & 77.86 & 63.48 & 56.03 & 59.09 & 49.66 \\\\ SigLIP VIT-SO 384px (Letterbox) & 78.61 & 63.39 & 56.88 & 60.33 & 52.71 \\\\ SigLIP VIT-SO 384px (Resize Crop) & 77.57 & 62.23 & 58.10 & 58.40 & 50.41 \\\\ SigLIP VIT-SO 384px (Navie Resize) & 78.81 & 63.60 & 57.47 & 61.06 & 54.87 \\\\ \\hline\n' +
      '**Visual Resolution Scaling** & & & & & \\\\ CLIP VIT-2.224px & 75.32 & 61.58 & 54.52 & 53.89 & 36.61 \\\\ SigLIP VIT-SO 224px & 76.32 & 62.15 & 58.82 & 55.75 & 40.50 \\\\ CLIP VIT-3.365px & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ SigLIP VIT-SO 384px & 78.61 & 63.39 & 56.88 & 60.33 & 52.71 \\\\ \\hline\n' +
      '**EmEmembling Visual Features** & & & & & \\\\ CLIP 365px (Navie Resize) & 77.86 & 63.48 & 56.03 & 59.09 & 49.66 \\\\ IDNO2 + CLIP 336px (Letterbox) & 75.66 & 62.89 & 53.88 & 46.28 & 15.16 \\\\ IDNO3 + CLIP -336px (Navie Resize) & 75.90 & 63.57 & 55.31 & 46.20 & 15.67 \\\\ SigLIP 384px (Navie Resize) & 78.81 & 63.60 & 57.47 & 61.06 & 54.87 \\\\ IDNO3 + SigLIP 384px (Lettebox) & 78.66 & 63.81 & 59.00 & 58.77 & 50.11 \\\\ IDNO2 + SigLIP 384px (Navie Resize) & 79.18 & 64.33 & 61.06 & 60.31 & 52.18 \\\\ \\hline\n' +
      '**Base vs. Instruct Tuned LMs** & & & & & \\\\ Vicuna v1.5 7B & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ Vicuna v1.5 13B & 77.96 & 63.17 & 56.37 & 59.30 & 48.03 \\\\ Llama-2 7B & 77.08 & 62.44 & 55.98 & 55.24 & 44.92 \\\\ Llama-2 13B & 78.07 & 63.12 & 57.55 & 58.42 & 47.60 \\\\ \\hline\n' +
      '**Co-training on Language Safety Data** & & & & & \\\\ Vicuna v1.5 7B (No Co-training) & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ Vicuna v1.5 7B (No Co-training) & 77.08 & 62.90 & 44.81 & 57.59 & 44.55 \\\\ Llama-2 7B & 77.08 & 62.44 & 55.98 & 55.24 & 44.92 \\\\ Llama-2 7B (No Co-training) & 77.10 & 62.94 & 43.60 & 56.04 & 45.45 \\\\ \\hline\n' +
      '**Scaling Train Time** & & & & & \\\\  I Epoch & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\  I Epochs & 77.30 & 62.70 & 57.28 & 57.22 & 45.44 \\\\  I Epochs & 77.54 & 62.75 & 56.37 & 56.42 & 45.63 \\\\  I Epochs & 77.79 & 63.50 & 55.20 & 56.12 & 46.08 \\\\ \\(3\\) Epochs & 77.17 & 62.96 & 56.20 & 54.01 & 45.69 \\\\ \\hline\n' +
      '**Scaling Data** & & & & & \\\\ Base & 77.09 & 62.57 & 54.33 & 56.87 & 44.45 \\\\ Base + LEV & 77.58 & 63.13 & 55.76 & 57.23 & 45.67 \\\\ Base + LVIS-4V & 77.96 & 62.43 & 55.91 & 57.55 & 45.99 \\\\ Base + LVIS-4V + LRV & 78.33 & 63.60 & 56.01 & 59.06 & 46.86 \\\\ \\hline\n' +
      '**Prism** & & & & & \\\\ Prism-CLIP 7B (Controlled) & 77.87 & 63.65 & 56.10 & 58.40 & 50.31 \\\\ Prism-CLIP 7B & 79.67 & 64.56 & 53.34 & 57.72 & 51.12 \\\\ Prism-SigLIP 7B (Controlled) & 79.12 & 63.98 & 58.99 & 60.11 & 55.79 \\\\ Prism-SigLIP 7B & 80.67 & 64.32 & 53.70 & 62.14 & 58.01 \\\\ Prism-DIDNOsigLIP 7B (Controlled) & 79.05 & 64.16 & 59.82 & 58.69 & 51.78 \\\\ Prism-DIDNOsigLIP 7B & 80.97 & 65.27 & 52.82 & 59.71 & 55.64 \\\\ \\hline\n' +
      '**Prism 13B** & & & & & \\\\ Prism-CLIP 13B (Controlled) & 78.83 & 64.10 & 57.09 & 61.10 & 52.22 \\\\ Prism-CLIP 13B & 80.38 & 65.07 & 56.47 & 61.56 & 53.40 \\\\ Prism-SigLIP 13B (Controlled) & 78.52 & 63.24 & 57.29 & 58.50 & 50.61 \\\\ Prism-SigLIP 13B & 80.68 & 64.56 & 57.63 & 60.09 & 54.28 \\\\ Prism-DIDNOsigLIP 13B (Controlled) & 80.07 & 65.14 & 56.61 & 61.20 & 54.10 \\\\ Prism-DIDNOsigLIP 13B & 81.66 & 66.13 & 58.01 & 62.89 & 57.08 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: VQA 벤치마크에 대한 모든 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & ReFOCOC & RefCOCO+ & RefCOCOg & OCIDRef \\\\ \\hline\n' +
      '**Official Models** & & & & \\\\ LLAV v1.5 7B & 55.12 & 49.47 & 50.92 & 35.07 \\\\ LLAV v1.5 13B & 66.75 & 61.36 & 60.85 & 45.56 \\\\ InstructBLIP 7B & N/A & N/A & N/A & N/A \\\\ InstructBLIP 13B & N/A & N/A & N/A & N/A \\\\ \\hline\n' +
      '**Reproduction \\& Optimization Procedure** & & & \\\\ LLAv v1.5 7B (Reproduction) & 60.54 & 54.34 & 56.31 & 41.75 \\\\ LLAV v1.5 13B (Reproduction) & 64.79 & 59.32 & 59.33 & 44.48 \\\\ Single-Stage 7B & 64.08 & 58.19 & 58.03 & 44.58 \\\\ Single-Stage 13B & 68.98 & 63.59 & 61.64 & 44.89 \\\\ Frozen ViT (Single-Stage) & 64.08 & 58.19 & 58.03 & 44.58 \\\\ Finetune ViT (Multi-Stage) & 19.24 & 17.48 & 23.12 & 16.35 \\\\ Finetune ViT (Single-Stage) & 42.56 & 37.89 & 41.05 & 33.42 \\\\ \\hline\n' +
      '**Visual Representations** & & & & \\\\ IN1K ViT-L 224px & 43.24 & 35.40 & 36.05 & 19.58 \\\\ DINOVO2 ViT-L 224px & 28.65 & 20.72 & 24.75 & 8.33 \\\\ CLIP ViT-L 224px & 59.88 & 53.69 & 53.37 & 37.16 \\\\ SigLIP ViT-SO 224px & 57.94 & 51.90 & 53.31 & 37.42 \\\\ \\hline\n' +
      '**Image Preprocessing** & & & & \\\\ CLIP ViT-L 336px (Leastrebox) & 64.08 & 58.19 & 58.03 & 44.58 \\\\ CLIP ViT-L 336px (Resize Crop) & 54.31 & 49.14 & 49.43 & 40.82 \\\\ CLIP ViT-L 336px (Naive Resize) & 65.28 & 58.79 & 59.93 & 44.20 \\\\ SigLIP ViT-SO 348px (Leastrebox) & 63.09 & 56.24 & 58.17 & 45.50 \\\\ SigLIP ViT-SO 384px (Istevize Crop) & 53.29 & 47.63 & 50.18 & 39.27 \\\\ SigLIP ViT-SO 384px (Naive Resize) & 61.38 & 55.76 & 56.84 & 41.49 \\\\ \\hline\n' +
      '**Visual Resolution Scaling** & & & & \\\\ CLIP ViT-L 224px & 59.88 & 53.69 & 53.37 & 37.16 \\\\ SigLIP ViT-SO 224px & 57.94 & 51.90 & 53.31 & 37.42 \\\\ CLIP ViT-X 336px & 64.08 & 58.19 & 58.03 & 44.58 \\\\ SigLIP ViT-SO 384px & 63.09 & 56.24 & 58.17 & 45.50 \\\\ \\hline\n' +
      '**Ensembling Visual Features** & & & & \\\\ CLIP ViT-L 336px (Naive Resize) & 65.28 & 58.79 & 59.93 & 44.20 \\\\ DINOVO2 + CLIP 336px (Letterbox) & 72.44 & 65.84 & 64.32 & 47.41 \\\\ DINOVO2 + CLIP 336px (Naive Resize) & 71.07 & 64.77 & 65.26 & 47.66 \\\\ SigLIP ViT-SO 384px (Naive Resize) & 61.38 & 55.76 & 56.84 & 41.49 \\\\ DINOVO2 + SigLIP 384px (Letterbox) & 72.10 & 65.42 & 64.69 & 50.37 \\\\ DINOVO2 + SigLIP 384px (Naive Resize) & 73.86 & 67.29 & 67.85 & 52.82 \\\\ \\hline\n' +
      '**Base Ns Instred Tands** & & & & \\\\ Vicuna v1.5 7B & 64.08 & 58.19 & 58.03 & 44.58 \\\\ Vicuna v1.5 13B & 68.98 & 63.59 & 61.64 & 44.89 \\\\ Llama-2 7B & 65.24 & 59.47 & 58.78 & 43.89 \\\\ Llama-2 13B & 68.53 & 62.97 & 61.70 & 45.75 \\\\ \\hline\n' +
      '**Co-training on Language Safety Data** & & & & \\\\ Vicuna v1.5 7B & 64.08 & 58.19 & 58.03 & 44.58 \\\\ Vicuna v1.5 7B (No Co-training) & 63.94 & 57.51 & 57.88 & 44.11 \\\\ Llama-2 7B & 65.24 & 59.47 & 58.78 & 43.89 \\\\ Llama-2 7B (No Co-training) & 64.26 & 59.30 & 57.99 & 42.17 \\\\ \\hline\n' +
      '**스케일링 트레이닝 타임** & & &\n' +
      '1 Epoch & 64.08 & 58.19 & 58.03 & 44.58\\\\\n' +
      '1.25 Epoch & 67.02 & 61.37 & 60.01 & 46.45\\\\\n' +
      '1.5 Epoch & 68.62 & 62.81 & 61.21 & 47.23 \\\\\n' +
      '2 Epoch & 71.23 & 65.40 & 63.32 & 46.32 \\\\\n' +
      '3 Epochs & 71.79 & 66.94 & 63.87 & 46.25 \\\\ \\hline\n' +
      '**Scaling Data** & & & & \\\\ Base & 64.08 & 58.19 & 58.03 & 44.58 \\\\ Base + LRV & 65.62 & 59.77 & 59.82 & 46.21 \\\\ Base + LVIS-4V & 63.91 & 58.82 & 58.91 & 43.83 \\\\ Base + LVIS-4V + LRV & 64.94 & 58.91 & 58.19 & 43.73 \\\\ \\hline\n' +
      '**Prism 7B** & & & & \\\\ Prism-CLIP 7B (Controlled) & 66.42 & 60.14 & 60.56 & 44.12 \\\\ Prism-CLIP 7B & 71.98 & 66.96 & 66.18 & 44.65 \\\\ Prism-SigLIP 7B (Controlled) & 64.74 & 58.58 & 60.56 & 43.63 \\\\ Prism-SigLIP 7B & 70.92 & 65.73 & 65.46 & 48.08 \\\\ Prism-DIMOSigLIP 7B (Controlled) & 73.62 & 67.85 & 66.34 & 50.56 \\\\ Prism-DIMOSigLIP 7B & 77.78 & 73.08 & 71.04 & 54.12 \\\\ \\hline\n' +
      '**Prism 13B** & & & & \\\\ Prism-CLIP 13B (Controlled) & 70.92 & 65.95 & 65.03 & 47.32 \\\\ Prism-CLIP 13B & & 73.37 & 68.71 & 69.06 & 48.98 \\\\ Prism-SigLIP 13B (Controlled) & 59.21 & 53.33 & 54.66 & 40.44 \\\\ Prism-SigLIP 13B & & 69.69 & 64.99 & 64.81 & 44.31 \\\\ Prism-DIMOSigLIP 13B (Controlled) & 76.64 & 71.41 & 70.87 & 53.60 \\\\ Prism-DIMOSigLIP 13B & & 79.39 & 75.55 & 72.73 & 54.62 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 위치 결정 벤치마크에 대한 모든 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & VSR & POPE & TallyQA \\\\ \\hline\n' +
      '**Official Models** & & & \\\\ LLaVa v1.5 7B & 51.47 & 86.57 & 62.06 \\\\ LLaVa v1.5 13B & 69.07 & 87.10 & 64.83 \\\\ InstructibleDT 7B & 58.92 & 84.30 & 15.51 \\\\ InstructibleID 13B & 63.91 & 84.49 & 49.73 \\\\ \\hline\n' +
      '**Reproduction \\& Optimization Procedure** & & \\\\ LLaVa v1.5 7B (Reproduction) & 52.95 & 86.57 & 60.87 \\\\ LLaVa v1.5 13B (Reproduction) & 65.38 & 86.94 & 64.13 \\\\ Single-Stage 7B & 51.47 & 86.57 & 61.63 \\\\ Single-Stage 13B & 70.05 & 87.00 & 63.26 \\\\ Frozen Vf (Single-Stage) & 51.47 & 86.57 & 61.63 \\\\ Finentune Vf (Multi-Stage) & 57.20 & 82.70 & 59.15 \\\\ Finentune Vf (Single-Stage) & 51.47 & 83.82 & 59.53 \\\\ \\hline\n' +
      '**Visual Representations** & & & \\\\ IN1K VIT-2.224px & 51.47 & 82.08 & 52.95 \\\\ DINOVO2 VIT-2.224px & 51.47 & 84.84 & 57.12 \\\\ CLIP VIT-2.224px & 51.47 & 85.80 & 59.09 \\\\ SigL-IP VIT-SO 224px & 51.47 & 85.07 & 63.02 \\\\ \\hline\n' +
      '**Image Preprocessing** & & & \\\\ CLIP VIT-3.369x (Letters) & 51.47 & 86.57 & 61.63 \\\\ CLIP VIT-3.369x (Revise Crop) & 51.47 & 85.42 & 61.24 \\\\ CLIP VIT-3.369x (Naive Resize) & 51.47 & 87.01 & 62.90 \\\\ SigL-IP VIT-SO 3849x (Letters) & 51.47 & 86.78 & 64.83 \\\\ SigL-IP VIT-SO 3849x (Reise Crop) & 51.47 & 84.62 & 62.94 \\\\ SigL-IP VIT-SO 3849x (Naive Resize) & 51.47 & 86.52 & 65.47 \\\\ \\hline\n' +
      '**Visual Resolution Scaling** & & & \\\\ CLIP VIT-1.224px & 51.47 & 85.80 & 59.09 \\\\ SigL-IP VIT-SO 224px & 51.47 & 85.07 & 63.02 \\\\ CLIP VIT-T.369x & 51.47 & 86.57 & 61.63 \\\\ SigL-IP VIT-SO 384px & 51.47 & 86.78 & 64.83 \\\\ \\hline\n' +
      '**Ensembling Visual Features** & & & \\\\ CLIP 336px (Naive Resize) & 51.47 & 87.01 & 62.90 \\\\ DINOVO2 + CLIP 336px (Lettersbox) & 51.47 & 87.70 & 63.99 \\\\ DiNOVO2 + CLIP 336px (Naive Resize) & 51.47 & 87.29 & 65.02 \\\\ SiL-IP 384px (Naive Resize) & 51.47 & 86.52 & 65.47 \\\\ DiNOVO2 + SigLIP 384px (Leittersbox) & 51.47 & 87.89 & 67.19 \\\\ DiNOVO2 + SigLIP 384px (Naive Resize) & 51.55 & 88.30 & 67.63 \\\\ \\hline\n' +
      '**Base vs. Instruct Tuned LMs** & & & \\\\ Vicuna v1.5 7B & 51.47 & 86.57 & 61.63 \\\\ Vicuna v1.5 7B & 70.05 & 87.00 & 63.26 \\\\ Lima-2 7B & 63.67 & 86.74 & 59.22 \\\\ Lima-2 13B & 65.71 & 86.91 & 62.54 \\\\ \\hline\n' +
      '**Co-training on Language Safety Data** & & & \\\\ Vicuna v1.5 7B & 51.47 & 86.57 & 61.63 \\\\ Vicuna v1.5 7B (No Co-training) & 53.68 & 87.27 & 63.31 \\\\ Lima-2 7B & 63.67 & 86.74 & 59.22 \\\\ Lima-2 7B (No Co-training) & 67.18 & 86.88 & 57.17 \\\\ \\hline\n' +
      '*Scaling Training Time** & & &\\\\1 Epoch & 51.47 & 86.57 & 61.63\\1.25 Epochs & 51.80 & 86.80 & 61.69\\1.5 Epochs & 51.55 & 87.78 & 61.67\\2 Epochs & 53.93 & 87.03 & 62.52\\\\\n' +
      '3 Epochs & 54.99 & 86.86 & 62.30 \\\\ \\hline\n' +
      '**Scaling Data** & & & \\\\ Base & & & \\\\ Base + LRV & 64.08 & 86.84 & 65.54 \\\\ Base + LVIS-4V & 51.47 & 86.98 & 62.60 \\\\ Base + LVIS-4V + LRV & 54.91 & 87.27 & 63.74 \\\\ \\hline\n' +
      '**Prism 7B** & & & \\\\ Prism-CLIP 7B (Controlled) & 66.61 & 86.83 & 60.86 \\\\ Prism-CLIP 7B & 57.77 & 87.30 & 66.00 \\\\ Prism-SigLIP 7B (Controlled) & 65.14 & 87.07 & 64.54 \\\\ Prism-SigLIP 7B & 56.79 & 87.30 & 66.46 \\\\ Prism-DINOogiLIP 7B (Controlled) & 66.28 & 88.28 & 65.07 \\\\ Prism-DINOogiLIP 7B & 59.57 & 88.12 & 66.70 \\\\ \\hline\n' +
      '**Prism 13B** & & & \\\\ Prism-CLIP 13B (Controlled) & 65.96 & 86.96 & 65.71 \\\\ Prism-CLIP 13B & 71.85 & 87.23 & 69.37 \\\\ Prism-SigLIP 13B (Controlled) & 62.85 & 86.82 & 62.90 \\\\ Prism-SigLIP 13B & 64.57 & 87.50 & 68.95 \\\\ Prism-DINOosiLIP 13B (Controlled) & 71.85 & 88.50 & 66.09 \\\\ Prism-DINOosiLIP 13B & 72.18 & 88.07 & 70.41 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 챌린지 벤치마크에 대한 모든 결과\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>