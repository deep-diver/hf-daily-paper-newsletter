<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Trajectory Consistency Distillation\n' +
      '\n' +
      'Jianbin Zheng\n' +
      '\n' +
      'Minghui Hu\n' +
      '\n' +
      'Zhongyi Fan\n' +
      '\n' +
      'Chaoyue Wang\n' +
      '\n' +
      'Changxing Ding\n' +
      '\n' +
      'Dacheng Tao\n' +
      '\n' +
      'Tat-Jen Cham\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency distillation technique to achieve impressive performance in accelerating text-to-image synthesis. However, we observed that LCM struggles to generate images with both clarity and detailed intricacy. To address this limitation, we initially delve into and elucidate the underlying causes. Our investigation identifies that the primary issue stems from errors in three distinct areas. Consequently, we introduce Trajectory Consistency Distillation (TCD), which encompasses _trajectory consistency function_ and _strategic stochastic sampling_. The trajectory consistency function diminishes the distillation errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE. Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model. Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Score-based generative models (SGMs), also commonly known as Diffusion Models (Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Song et al., 2020; Ho et al., 2020), have demonstrated their proficiency in various generative modeling domains such as image (Dhariwal and Nichol, 2021; Ramesh et al., 2022; Rombach et al., 2022), video (Ho et al., 2020; Wu et al., 2023; Guo et al., 2023), and audio (Kong et al., 2020; Chen et al., 2020; Popov et al., 2021), particularly in text-to-image synthesis (Nichol et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Podell et al., 2023). An noteworthy aspect of SGMs is the utilisation of stochastic differential equations (SDEs) and corresponding marginal-preserving ordinary differential equations (ODEs) to iteratively perturb data and eliminate noise (Song et al., 2020). This facilitates an effective trade-off between generation cost and sampling quality, but they are also constrained by slow inference speed, requiring a substantial number of function evaluations (NFEs) to obtain satisfactory results.\n' +
      '\n' +
      'To overcome this limitation, Song et al. (2023) proposed Consistency Models (CMs), an emerging class of powerful generative models capable of generating high-quality data with single-step or few-step sampling without the need for adversarial training. CMs are inextricably connected with SGMs in their underlying mathematical underpinnings, aiming to enforce the self-consistency property by mapping arbitrary points on the trajectory of the same Probability Flow Ordinary Differential Equation (PF ODE) to the trajectory\'s origin (Song et al., 2020). CMs can be trained with consistency distillation or treated as standalone generative models. Song et al. (2023) have demonstrated their superiority through extensive experiments in the pixel space. Latent Consistency Models (LCMs) (Luo et al., 2023) further incorporate Latent Diffusion Models (LDMs) (Rombach et al., 2022) with CMs, achieving remarkable success in swiftly synthesizing high-resolution images conditioned on text. Moreover, LCM-LoRA (Luo et al., 2023) improves the training efficiency of LCMs and converts them into a universal neural PF ODE solver by introducing LoRA (Hu et al., 2021) into the distillation process of LCMs. It is noteworthy that all these Consistency-Type Models still allow for striking a balance between computation and sample quality using Multistep Consistency Sampling (Song et al., 2023). In particular, allocating additional compute for more iterations can theoretically yield samples of higher quality.\n' +
      '\n' +
      'Despite the introduction of the guided distillation method and skipping-step technique (Luo et al., 2023) by LCMs for effectively distilling knowledge from pre-trained diffusion models, the quality of images generated by LCMs in a single step or even with minimal steps (4\\(\\sim\\)8) still lags significantly behind the convergence of its teacher model. Our investigation revealed that, in practice, increasing the number of inference iterations diminishes the visual complexity and quality of the results, as illustrated in Figure 1. This renders LCMs less capable of synthesizing samples that are perceptually comparable to those of their teacher model. Recent findings by EDM (Karras et al., 2022) identified that an excessive Langevin-like addition and removal of random noise result in a gradual loss of detail in the generated images. Additionally, Li et al. (2023) showed evidence that during the process of multi-step sampling, discrete errors accumulate over iterations, ultimately causing the generated images to deviate from the target distribution.\n' +
      '\n' +
      'Drawing inspiration from the observation, we first meticulously examine the training procedure and delve into the multistep consistency sampling procedure to figure out the root cause. Our investigation revealed that the issue stems from the cumulative errors inherent in multi-step sampling processes. These errors predominantly originate from three sources: 1) _the estimation errors in the original score matching model_, 2) _the distillation errors in the consistency model_, and 3) _the discretisation errors accumulated during the sampling phase_. Collectively, these errors significantly undermine the efficacy of the multistep sampling consistency model, resulting in a performance that falls substantially short of expectations.\n' +
      '\n' +
      'To suppress these errors, we introduce Trajectory Consistency Distillation (TCD), as summarized in Figure 2, which comprises two key elements: _trajectory consistency function_ and _strategic stochastic sampling_. Specifically, inspired by the form of exponential integrators, the trajectory consistency function (TCF) diminishes distillation errors by expanding the boundary conditions of the consistency model and enabling seamless transitions at any point along the trajectory governed by the PF ODE, as demonstrated in Fig\n' +
      '\n' +
      'Figure 1: Comparison between TCD and other state-of-the-art methods. TCD delivers exceptional results in terms of both quality and speed, completely surpassing LCM. Notably, LCM experiences a notable decline in quality at high NFEs. In contrast, TCD maintains superior generative quality at high NFEs, even exceeding the performance of DPM-Solver++(2S) with origin SDXL.\n' +
      '\n' +
      'ure 2a. Furthermore, strategic stochastic sampling (SSS) suppresses the accumulated discretisation error and estimation error according to the narrowed bi-directional iterations as shown in Figure 2b.\n' +
      '\n' +
      'Experiments show that TCD can significantly enhance the quality of images generated by LCM, surpassing it in performance. Furthermore, TCD is capable of outperforming the teacher model (SDXL with DPMSolver++) when sampling with sufficient iterations (20 NFEs).\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '### Diffusion Models\n' +
      '\n' +
      'Diffusion Models (DMs) start with a predefined forward process \\(\\{\\mathbf{x}_{t}\\}_{t\\in[0,T]}\\) indexed by a continuous time variable \\(t\\) with \\(T>0\\), which progressively adds noise to data via Gaussian perturbations. The forward process can be modeled as a widely used stochastic differential equation (SDE) (Song et al., 2020; Karras et al., 2022):\n' +
      '\n' +
      '\\[\\text{d}\\mathbf{x}_{t}=\\mu(t)\\mathbf{x}_{t}\\text{d}t+\\nu(t)\\text{d}\\mathbf{w}_{t}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{w}_{t}\\) denotes the \\(d\\)-dimensional standard Brownian motion and \\(\\mu(t)\\colon\\mathbb{R}\\to\\mathbb{R}\\) and \\(\\nu(t)\\colon\\mathbb{R}\\to\\mathbb{R}\\) are the drift and diffusion coefficients, respectively, where \\(d\\) is the dimensionality of the dataset. Denote the marginal distribution of \\(\\mathbf{x}_{t}\\) following the forward process as \\(p_{t}(\\mathbf{x}_{t})\\) and, such an Ito SDE gradually perturbs the empirical data distribution \\(p_{0}(\\mathbf{x})=p_{\\text{data}}(\\mathbf{x})\\) towards the prior distribution \\(p_{T}(\\mathbf{x})\\approx\\pi(\\mathbf{x})\\) approximately, where \\(\\pi(\\mathbf{x})\\) is a tractable Gaussian distribution.\n' +
      '\n' +
      'Remarkably, Song et al. (2020) proved that there exists an ordinary differential equation (ODE) dubbed the _probability flow_ (PF) ODE, whose trajectories share the same marginal probability densities \\(\\{p_{t}(\\mathbf{x})\\}_{t\\in[0,T]}\\) as the forward SDE,\n' +
      '\n' +
      '\\[\\frac{\\text{d}\\mathbf{x}_{t}}{\\text{d}t}=\\mu(t)\\mathbf{x}_{t}-\\frac{1}{2}\\nu(t)^{2} \\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x}_{t}). \\tag{2}\\]\n' +
      '\n' +
      'As for sampling, the ground truth score in Equation (2) is approximated with the learned score model \\(\\mathbf{s}_{\\mathbf{\\theta}}(\\mathbf{x},t)\\approx\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})\\) via score matching (Hyvarinen et al., 2009; Song and Ermon, 2019; Ho et al., 2020). This yields an empirical estimate of the PF ODE, referred to as the _empirical PF ODE_:\n' +
      '\n' +
      '\\[\\frac{\\text{d}\\tilde{\\mathbf{x}}_{t}}{\\text{d}t}=\\mu(t)\\tilde{\\mathbf{x}}_{t}-\\frac{1} {2}\\nu(t)^{2}\\mathbf{s}_{\\mathbf{\\theta}}(\\tilde{\\mathbf{x}}_{t},t). \\tag{3}\\]\n' +
      '\n' +
      'Then samples can be drawn by solving the empirical PF ODE from \\(T\\) to 0. There already exist off-the-shelf ODE solvers (Song et al., 2020;a; Karras et al., 2022) or efficient numerical solvers (Lu et al., 2022;b; Zhang and Chen, 2022) that can be directly applied to approximate the exact solution.\n' +
      '\n' +
      '### Consistency Models\n' +
      '\n' +
      'Solving Equation (3) typically involves numerous neural network evaluations to generate decent samples. Thus, consistency models are proposed to directly map any points along the trajectory \\(\\{\\mathbf{x}_{t}\\}_{t\\in[0,T]}\\) of the PF ODE to the origin of its trajectory, thereby facilitating generation in just a few steps. The associated mapping can be formulated as follows:\n' +
      '\n' +
      '\\[\\mathbf{f}(\\mathbf{x}_{t},t)=\\mathbf{x}_{0}\\quad\\forall t\\in[0,T], \\tag{4}\\]\n' +
      '\n' +
      'with the boundary condition \\(\\mathbf{f}(\\mathbf{x}_{0},0)=\\mathbf{x}_{0}\\). It is worth noting that Equation (4) is equivalent to the _self-consistency_ condition:\n' +
      '\n' +
      '\\[\\mathbf{f}(\\mathbf{x}_{t},t)=\\mathbf{f}(\\mathbf{x}_{t}^{\\prime},t^{\\prime})\\quad\\forall t,t^{ \\prime}\\in[0,T]. \\tag{5}\\]\n' +
      '\n' +
      'A parametric model \\(\\mathbf{f}_{\\mathbf{\\theta}}\\) is constructed to estimate the consistency function \\(\\mathbf{f}\\) by enforcing the self-consistency property. Typically, \\(\\mathbf{f}\\) can be distilled from a pretrained diffusion model \\(F_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t)\\) and parameterized as:\n' +
      '\n' +
      '\\[\\mathbf{f}_{\\mathbf{\\theta}}(\\tilde{\\mathbf{x}}_{t},t)=\\begin{cases}\\mathbf{x}_{0},&t=0\\\\ \\texttt{Solver}(F_{\\mathbf{\\theta}}(\\tilde{\\mathbf{x}}_{t},t),t,0;\\mathbf{\\theta}),&t \\in(0,T]\\end{cases}\\]\n' +
      '\n' +
      'Figure 2: The comparative overview of the baseline Consistency Distillation (Song et al., 2023) and the proposed Trajectory Consistency Distillation, includes Trajectory Consistency Function (TCF) for training and Strategic Stochastic Sampling (SSS) for inference.\n' +
      '\n' +
      'The \\(\\texttt{Solver}(\\cdot,t,0;\\theta)\\) is the update function of an ODE solver used to estimate \\(\\mathbf{x}_{0}\\) when given the output \\(F_{\\theta}(\\tilde{\\mathbf{x}}_{t},t)\\) from a pretrained model at timestep \\(t\\). For training consistency models, the objective of Consistency Distillation (CD) is defined as minimizing:\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}^{N}_{\\mathrm{CD}}(\\mathbf{\\theta},\\mathbf{\\theta}^ {-};\\mathbf{\\phi}):=\\\\ \\quad\\quad\\quad\\quad\\quad\\quad\\mathbb{E}\\left[\\lambda(t_{n})\\left\\| \\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})-\\mathbf{f}_{\\mathbf{\\theta}^{-}}(\\hat{ \\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n})\\right\\|_{2}^{2}\\right],\\end{split} \\tag{6}\\]\n' +
      '\n' +
      'where \\(0=t_{1}<t_{2}\\cdots<t_{N}=T\\), \\(n\\) uniformly distributed over \\(\\{1,2,\\cdots,N-1\\}\\), \\(\\lambda(\\cdot)\\in\\mathbb{R}^{+}\\) is a positive weighting function, and the expectation is taken with respect to \\(\\mathbf{x}\\sim p_{\\mathrm{data}}\\). \\(\\mathbf{x}_{t_{n+1}}\\) can be sampled using SDE 1 and \\(\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}}\\) is calculated by \\(\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}}:=\\Phi(\\mathbf{x}_{t_{n+1}},t_{n+1},t_{n};\\mathbf{\\phi})\\), where \\(\\Phi(\\cdots;\\mathbf{\\phi})\\) represents the update function of a one-step ODE solver applied to the empirical PF ODE 3. Moreover, \\(\\mathbf{\\theta}^{-}\\) is introduced to stabilize the training process and updated by an exponential moving average (EMA) strategy, _i.e._, given \\(0\\leq\\mu<1\\), \\(\\mathbf{\\theta}^{-}\\leftarrow\\texttt{SG}(\\mu\\mathbf{\\theta}^{-}+(1-\\mu)\\mathbf{\\theta})\\).\n' +
      '\n' +
      'Besides the distillation strategy that needs an existing score model, Song et al. (2023) also introduced a way to train without any pre-trained models called Consistency Training (CT). In this paper, our primary focus is on the CD objective.\n' +
      '\n' +
      '## 3 Elucidating Errors in Consistency Models\n' +
      '\n' +
      'In this section, we elucidate various types of errors that occurred in multistep sampling of consistency models, which motivates us to propose corresponding solutions aimed at refining them. Our analysis reveals that the error is primarily composed of three components: _distillation error_ of consistency models, _estimation error_ of score matching models, and accumulated _discretisation error_ (also known as _truncation error_) during solving the ODEs.\n' +
      '\n' +
      '### Consistency Distillation Error\n' +
      '\n' +
      'With a well-trained model \\(\\mathbf{\\theta}^{\\star}\\) such that \\(\\mathcal{L}^{N}_{\\mathrm{CD}}(\\mathbf{\\theta}^{\\star},\\mathbf{\\theta}^{\\star};\\mathbf{ \\phi})=0\\), and considering that the ODE solver \\(\\Phi(\\cdots;\\mathbf{\\phi})\\) has the local discretisation error, Theorem 1 in (Song et al., 2023) shows that the consistency distillation error is bounded:\n' +
      '\n' +
      '\\[\\sup_{n,\\mathbf{x}}\\|\\mathbf{f}_{\\mathbf{\\theta}^{\\star}}(\\mathbf{x},t_{n}),\\mathbf{f}(\\mathbf{x},t_{ n};\\mathbf{\\phi})\\|_{2}=\\mathcal{O}\\left((\\Delta t)^{p}\\right), \\tag{7}\\]\n' +
      '\n' +
      'with \\(\\Delta t\\) and \\(p\\) as defined in (Song et al., 2023).\n' +
      '\n' +
      '### Error Bound in Multistep Consistency Sampling\n' +
      '\n' +
      'Theoretically, with a well-trained consistency model \\(\\mathbf{f}_{\\mathbf{\\theta}^{\\star}}\\), one can generate samples with just one forward pass through the consistency model. However, the one-step sampling yields suboptimal results (Luo et al., 2023; Song et al., 2023). Thus, multistep consistency sampling has been introduced in (Song et al., 2023) to enhance sample quality through alternating denoising and noise injection steps. We term it as _multistep sampling_ in this manuscript.\n' +
      '\n' +
      'For simplicity, we consider VP SDEs in this paper; thus, the drift and diffusion coefficients \\(\\mu(t)\\) and \\(\\nu(t)\\) in SDE (Equation (1)) can be written as:\n' +
      '\n' +
      '\\[\\mu(t)=\\frac{\\mathrm{d}\\log\\alpha_{t}}{\\mathrm{d}t},\\quad\\nu(t)=\\sqrt{\\frac{ \\mathrm{d}\\sigma_{t}^{2}}{\\mathrm{d}t}-2\\frac{\\mathrm{d}\\log\\alpha_{t}}{ \\mathrm{d}t}\\sigma_{t}^{2}}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\alpha_{t}\\) and \\(\\sigma_{t}\\) specify the noise schedule in the perturbation kernels,\n' +
      '\n' +
      '\\[q_{0t}(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t}|\\alpha_{t}\\mathbf{x}_{0}, \\sigma_{t}^{2}\\mathbf{I}). \\tag{9}\\]\n' +
      '\n' +
      'Given a sequence of \\(N\\) sampling timesteps \\(T=\\tau_{1}>\\tau_{2}>\\cdots>\\tau_{N}\\) and an initial value \\(\\tilde{\\mathbf{x}}_{\\tau_{1}}\\sim\\mathcal{N}(0,\\mathbf{I})\\), the generating procedure of the \\(n\\)-th step can be written as:\n' +
      '\n' +
      '\\[\\begin{split}&\\mathbf{x}_{\\tau_{1}\\to 0}\\leftarrow\\mathbf{f}_{\\mathbf{\\theta}^{ \\star}}(\\tilde{\\mathbf{x}}_{\\tau_{1}},T),\\\\ \\text{Diffuse:}&\\tilde{\\mathbf{x}}_{\\tau_{n}}\\leftarrow \\alpha_{\\tau_{n}}\\mathbf{x}_{\\tau_{(n-1)}\\to 0}+\\sigma_{\\tau_{n}}\\mathbf{z},\\quad\\mathbf{z}\\sim \\mathcal{N}(0,\\mathbf{I})\\\\ \\text{Denoise:}&\\mathbf{x}_{\\tau_{n}\\to 0}\\leftarrow\\mathbf{f}_{\\mathbf{ \\theta}^{\\star}}(\\tilde{\\mathbf{x}}_{\\tau_{n}},\\tau_{n}),\\end{split} \\tag{10}\\]\n' +
      '\n' +
      'this process is detailed in Algorithm 3. We derive a further corollary from Corollary 7 in (Lyu et al., 2023) as follows:\n' +
      '\n' +
      '**Corollary 3.1**.: _With the sampling process defined in Equation (10) and denoting the distribution of \\(\\mathbf{x}_{\\tau_{n}\\to 0}\\) as \\(q_{\\mathbf{\\theta}^{\\star},n}\\), we have single-step sampling result, \\(q_{\\mathbf{\\theta}^{\\star},1}=\\mathbf{f}_{\\mathbf{\\theta}^{\\star},T}\\sharp\\mathcal{N}(0,\\mathbf{ I})\\) and multistep sampling result, \\(q_{\\mathbf{\\theta}^{\\star},N}=\\mathbf{f}_{\\mathbf{\\theta}^{\\star},\\tau_{N}}\\sharp\\mathcal{N}( \\alpha_{\\tau_{N}}\\mathbf{x}_{\\tau_{(N-1)}\\to 0},\\sigma_{\\tau_{N}}\\mathbf{I})\\) with \\(N>1\\). The Total Variational (TV) distance between \\(q\\) and \\(p_{\\mathrm{data}}\\) is_\n' +
      '\n' +
      '\\[\\begin{split}& TV(q_{\\mathbf{\\theta}^{\\star},1},p_{\\mathrm{data}})= \\mathcal{O}\\left(T(\\varepsilon_{cd}+\\mathcal{L}_{f}\\varepsilon_{se})\\right)\\\\ & TV(q_{\\mathbf{\\theta}^{\\star},N},p_{\\mathrm{data}})=\\mathcal{O}\\left( 2^{-N}T(\\varepsilon_{cd}+\\mathcal{L}_{f}\\varepsilon_{se})\\right),\\end{split}\\]\n' +
      '\n' +
      '_where \\(\\sharp\\) is a push-forward operator associated with a measurable map, \\(\\mathcal{L}_{f}\\) is the Lipschitz constant of consistency model, and \\(\\varepsilon_{cd}\\), \\(\\varepsilon_{se}\\) represent the error of consistency distillation in Equation (7) and score matching estimation, respectively._\n' +
      '\n' +
      'The detailed proof can be found in Appendix D.1. From Corollary 3.1, we observe that multistep sampling has a lower error bound than a single step. This observation aligns with the empirical findings presented in (Karras et al., 2022): the local truncation error scales superlinearly with respect to step size, thus increasing \\(N\\) improves the accuracy of the solution. Moreover, it illustrates that the model performance tends to underperform single-step generation methods (Sauer et al., 2023; Yin et al., 2023) without the help of additional supervision.\n' +
      '\n' +
      '### Accumulated Error in Multistep Sampling\n' +
      '\n' +
      'However, due to the error of estimation and discretisation in practice, the total error will be accumulated in every sampling step. Denote the distribution of \\(\\mathbf{x}_{\\tau_{n}\\to 0}\\) output by the true consistency function \\(\\mathbf{f}(\\cdot,\\cdot;\\mathbf{\\phi})\\) as \\(p_{n}\\), Theorem 2 in (Chen et al., 2022) demonstrates that the TV error between \\(q_{\\mathbf{\\theta}^{*},n}\\) and \\(p_{n}\\) is\n' +
      '\n' +
      '\\[TV(q_{\\mathbf{\\theta}^{*},n},p_{n})=\\mathcal{O}(\\sqrt{\\tau_{n}}), \\tag{11}\\]\n' +
      '\n' +
      'the error in each \\(n\\)-th step will accumulate over \\(N\\) sampling steps, resulting in the defined accumulated error\n' +
      '\n' +
      '\\[TV(q_{\\mathbf{\\theta}^{*},N},p_{N})=\\mathcal{O}\\left(\\sum_{n=1}^{N}\\sqrt{\\tau_{n} }\\right). \\tag{12}\\]\n' +
      '\n' +
      'The above result is a special case of Theorem 4.2, and its proof is presented in Appendix D.3. As a consequence, there is a significant reduction in image details at higher NFEs, as shown in Figure 3.\n' +
      '\n' +
      '## 4 Trajectory Consistency Distillation\n' +
      '\n' +
      '### Trajectory Consistency Function\n' +
      '\n' +
      'Definition.The distillation error from the consistency model can be reduced by expanding the original boundary conditions to encompass the entire trajectory. To facilitate this, we introduce the Trajectory Consistency Function (TCF), designed to enable comprehensive tracking along the full trajectory.\n' +
      '\n' +
      '\\[\\mathbf{f}(\\mathbf{x}_{t},t,s)\\mapsto\\mathbf{x}_{s}. \\tag{13}\\]\n' +
      '\n' +
      'The trajectory consistency function possesses the property of _trajectory consistency_, enhancing the original self-consistency property in an endpoint-unrestricted trajectory. Specifically, its output remains consistent for arbitrary sets \\((\\mathbf{x}_{t},t)\\) with the given \\(s\\) that belong to the same PF ODE trajectory, where \\(0\\leqslant s\\leqslant t\\leqslant T\\),\n' +
      '\n' +
      '\\[\\mathbf{f}(\\mathbf{x}_{t},t,s)=\\mathbf{f}(\\mathbf{x}_{t}^{\\prime},t^{\\prime},s)\\quad\\forall t,t^{\\prime},s\\in[0,T]. \\tag{14}\\]\n' +
      '\n' +
      'Parameterisation.The _semi-linear_ structure of the empirical PF-ODE revealed by (Lu et al., 2022; Zhang and Chen, 2022) motivates us to parameterise the trajectory consistency function using the exponential integrators form, as shown in Equation (15).\n' +
      '\n' +
      '\\[\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t,s)=\\frac{\\sigma_{s}}{\\sigma_{t}}\\mathbf{x}_{t}+ \\sigma_{s}\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{\\lambda}\\hat{\\mathbf{x}}_{\\mathbf{\\theta} }(\\hat{\\mathbf{x}}_{\\lambda},\\lambda)\\text{d}\\lambda, \\tag{15}\\]\n' +
      '\n' +
      'where \\(\\lambda_{t}:=\\log(\\alpha_{t}/\\sigma_{t})\\) is the log-SNR, and \\(\\mathbf{x}_{\\mathbf{\\theta}}\\) is a trainable network with parameter \\(\\mathbf{\\theta}\\).\n' +
      '\n' +
      'For \\(k\\geqslant 1\\), one can take the (\\(k\\)-1)-th Taylor expansion at \\(\\lambda_{t}\\) for \\(\\mathbf{x}_{\\mathbf{\\theta}}\\) w.r.t \\(\\lambda\\in[\\lambda_{s},\\lambda_{t}]\\), we have:\n' +
      '\n' +
      '\\[\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t,s)=\\frac{\\sigma_{s}}{\\sigma_{t} }\\mathbf{x}_{t}+\\] \\[\\sigma_{s}\\sum_{n=0}^{k-1}\\mathbf{x}_{\\mathbf{\\theta}}^{(n)}(\\hat{\\mathbf{x}} _{\\lambda_{t}},\\lambda_{t})\\int_{\\lambda_{t}}^{\\lambda_{s}}e^{\\lambda}\\frac{( \\lambda-\\lambda_{t})^{n}}{n!}\\text{d}\\lambda+\\mathcal{O}(h^{k+1}), \\tag{16}\\]\n' +
      '\n' +
      'where \\(h=\\lambda_{s}-\\lambda_{t}\\) and \\(\\mathbf{x}_{\\mathbf{\\theta}}^{(n)}(\\cdot,\\cdot)\\) is the \\(n\\)-th order total derivatives of \\(\\mathbf{x}_{\\mathbf{\\theta}}\\) w.r.t \\(\\lambda_{\\mathbf{\\theta}}\\).\n' +
      '\n' +
      'Here, we consider 1st-order and 2nd-order estimations, omitting high-order terms \\(\\mathcal{O}(h^{k+1})\\).\n' +
      '\n' +
      '\\[\\text{TCF(1):}\\quad\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t,s)=\\frac{\\sigma_{s}}{ \\sigma_{t}}\\mathbf{x}_{t}-\\alpha_{s}(e^{-h}-1)\\hat{\\mathbf{x}}_{\\mathbf{\\theta}}(\\mathbf{x}_{t },t). \\tag{17}\\]\n' +
      '\n' +
      'For the \\(2\\)nd-order expansion, we can write the trajectory consistency function as:\n' +
      '\n' +
      '\\[\\text{TCF(2):}\\quad\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t}, t,s)=\\frac{\\sigma_{s}}{\\sigma_{t}}\\mathbf{x}_{t}-\\alpha_{s}(e^{-h}-1)\\] \\[\\left((1-\\frac{1}{2r})\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_{t},t)+\\frac{ 1}{2r}\\hat{\\mathbf{x}}_{\\theta}(\\hat{\\mathbf{x}}_{u},u)\\right), \\tag{18}\\]\n' +
      '\n' +
      'where \\(u\\) is the mid-timestep w.r.t. \\(t>u>s\\) and \\(r:=(\\lambda_{u}-\\lambda_{t})/h\\).\n' +
      '\n' +
      'We also propose a modified network \\(F_{\\mathbf{\\theta}}\\) with additional parameters for conditioning on \\(s\\) to directly estimate the exponentially weighted integral of \\(\\mathbf{x}_{\\mathbf{\\theta}}\\) without omitting residual term when \\(k=1\\) in Equation (16):\n' +
      '\n' +
      '\\[\\text{TCF(S+):}\\quad\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t,s)=\\frac{\\sigma_{s}}{ \\sigma_{t}}\\mathbf{x}_{t}-\\alpha_{s}(e^{-h}-1)F_{\\mathbf{\\theta}}(\\mathbf{x}_{t},t,s). \\tag{19}\\]\n' +
      '\n' +
      'Broadening the Boundary Condition.The boundary condition in the CM is limited to intervals extending from\n' +
      '\n' +
      'Figure 3: Synthesis results across various NFEs. Due to accumulated errors in multistep sampling, LCM experiences a loss of image detail, leading to a degradation in performance, whereas TCD addresses this issue. Additional samples are available in Appendix E.1.\n' +
      '\n' +
      'any start point on the solution trajectory to the origin. Conversely, TCF mitigates this constraint, allowing the model to handle any interval along the PF-ODE trajectory and preventing trivial solutions \\(\\mathbf{f_{\\theta}}(\\mathbf{x}_{t},t,s)\\equiv 0\\) from arising in TCD training. Consequently, we broaden the boundary condition to encompass a more comprehensive range of trajectory intervals,\n' +
      '\n' +
      '\\[\\mathbf{f_{\\theta}}(\\mathbf{x}_{s},s,s)=\\mathbf{x}_{s}, \\tag{20}\\]\n' +
      '\n' +
      'it is evident that all three types of our parametrisation satisfy the broad boundary condition effortlessly.\n' +
      '\n' +
      'Training.Considering \\(0=t_{1}<t_{2}\\cdots<t_{N}=T\\) and given the one-step update function of a trained PF ODE solution \\(\\Phi(\\cdots;\\mathbf{\\phi})\\) parameterised by \\(\\mathbf{\\phi}\\), we can obtain an accurate estimation \\(\\mathbf{x}_{t_{n}}\\) from \\(\\mathbf{x}_{t_{n+k}}\\) by executing \\(k\\) discretisation steps with \\(\\Phi^{(k)}(\\cdots;\\mathbf{\\phi})\\),\n' +
      '\n' +
      '\\[\\hat{\\mathbf{x}}_{t_{n}}^{\\phi,k}=\\Phi^{(k)}(\\mathbf{x}_{t_{n+k}},t_{n+k},t_{n};\\mathbf{ \\phi}). \\tag{21}\\]\n' +
      '\n' +
      'Thus, we could express the object of trajectory distillation in alignment with reconstruction:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{TCD}}^{N}( \\mathbf{\\theta}, \\mathbf{\\theta}^{-};\\mathbf{\\phi}):=\\mathbb{E}[\\omega(t_{n},t_{m}) \\tag{22}\\] \\[\\|\\mathbf{f_{\\theta}}(\\mathbf{x}_{t_{n+k}},t_{n+k},t_{m})-\\mathbf{f_{\\theta ^{-}}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi,k},t_{n},t_{m})\\|_{2}^{2}],\\]\n' +
      '\n' +
      'where \\(n\\sim\\mathcal{U}[1,N-1]\\), \\(m\\sim\\mathcal{U}[1,n]\\), \\(\\mathbf{\\theta}^{-}\\) can be either updated by EMA: \\(\\mathbf{\\theta}^{-}\\leftarrow\\texttt{sg}(\\mu\\mathbf{\\theta}^{-}+(1-\\mu)\\mathbf{\\theta})\\) or stop the gradient without updating: \\(\\texttt{sg}(\\mathbf{\\theta})\\), \\(\\omega(\\cdots)\\) is a positive weighting function, and we find \\(\\omega(t_{n},t_{m})\\equiv 1\\) performs well in our experiments. We also employ the skipping-step method proposed in (Luo et al., 2023a) to accelerate convergence. The detailed training process is outlined in Algorithm 1.\n' +
      '\n' +
      'Below, we offer a theoretical justification based on asymptotic analysis to explain how trajectory consistency distillation optimizes the distillation error.\n' +
      '\n' +
      '**Theorem 4.1**.: _Let \\(\\Delta t:=\\max_{n\\in\\llbracket 1,\\text{N}-1\\rrbracket}\\{|\\text{t}_{n+1}- \\text{t}_{n}|\\}\\), and \\(\\mathbf{f}(\\cdot,\\cdot,\\cdot;\\phi)\\) be the trajectory consistency function of the empirical PF ODE in Equation (3). Assume \\(\\mathbf{f_{\\theta}}\\) satisfies the Lipschitz condition, that is, there exists \\(L>0\\) such that for all \\(t\\in[0,T]\\), \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), we have \\(\\left\\|\\mathbf{f_{\\theta}}(\\mathbf{x},t,s)-\\mathbf{f_{\\theta}}(\\mathbf{y},t,s)\\right\\|_{2} \\leqslant L\\left\\|\\mathbf{x}-\\mathbf{y}\\right\\|_{2}\\). Assume further that for all \\(n\\in\\llbracket 1,N-1\\rrbracket\\), the \\(p\\)-th order ODE solver called at \\(t_{n+1}\\) has local error uniformly bounded by \\(\\mathcal{O}((t_{n+1}-t_{n})^{p+1})\\) with \\(p\\geqslant 1\\). Then, if there is a \\(\\mathbf{\\theta^{*}}\\) so that \\(\\mathcal{L}_{\\text{TCD}}^{N}(\\mathbf{\\theta^{*}},\\mathbf{\\theta^{*}};\\mathbf{\\phi})=0\\), for any \\(n\\in\\llbracket 1,N-1\\rrbracket\\) and \\(m\\in\\llbracket 1,n\\rrbracket\\), we have_\n' +
      '\n' +
      '\\[\\sup_{n,m,\\mathbf{x}}\\|\\mathbf{f_{\\theta^{*}}}(\\mathbf{x},t_{n},t_{m}),\\mathbf{f }(\\mathbf{x}, t_{n},t_{m};\\phi)\\|_{2}\\] \\[=\\mathcal{O}\\left((\\Delta t)^{p}\\right)(t_{n}-t_{m}).\\]\n' +
      '\n' +
      'Proof.: The proof is provided in Appendix D.2. \n' +
      '\n' +
      'Theorem 4.1 implies that the distillation error of TCF is upper bound by that of CD presented in Equation (7).\n' +
      '\n' +
      '### Strategic Stochastic Sampling\n' +
      '\n' +
      'The proposed trajectory consistency function not only optimizes the distillation loss but also enables the model to access non-origin destinations along the PF ODE. This capability allows Strategic Stochastic Sampling (SSS) to further reduce discretisation errors and estimation errors introduced in each sampling step.\n' +
      '\n' +
      'Specifically, every sampling step in SSS includes the _denoise sub-step_ according to the ODE solver and the _diffuse sub-step_ based on Langevin SDE. In comparison with multistep consistency sampling, where the endpoint and noise level are fixed, SSS introduces the additional parameter \\(\\gamma\\) to control the destination point for the denoise step and allows for the adjustment of random noise level for the diffuse step, as detailed in Figure 1(b) and Algorithm 4. This parameter is referred to as the _stochastic parameter_.\n' +
      '\n' +
      'In the _denoising sub-step_, our focus is on reducing the errors that arise from the \\(n\\)-th step in Equation (11) to \\(\\mathcal{O}(\\sqrt{\\tau_{n}-(1-\\gamma)\\tau_{(n+1)}})\\) when \\(n\\in\\llbracket 1,N-1\\rrbracket\\), by the prediction of the non-origin, and ultimately optimize the accumulated error as shown in Theorem 4.2.\n' +
      '\n' +
      '**Theorem 4.2**.: _As the strategic stochastic sampling process defined in Algorithm 4, and denoting the \\(N>1\\) steps sampling results with trained trajectory consistency model \\(\\mathbf{f_{\\theta^{*}}}\\) as \\(\\mathbf{q_{\\theta^{*},N}}=\\mathbf{f_{\\theta^{*},\\tau_{N}}}\\texttt{N}(\\alpha_{\\tau_{N} }\\mathbf{x}_{\\tau_{(N-1)}\\to 0},\\sigma_{\\tau_{N}}\\mathbf{I})\\), the results with exact trajectory consistency function \\(\\mathbf{f_{\\theta}}\\) as \\(p_{N}\\), then the accumulated error in SSS is defined as:_\n' +
      '\n' +
      '\\[TV(\\mathbf{q_{\\theta^{*},N}},p_{N})=\\mathcal{O}\\left(\\sum_{n=1}^{N-1}\\sqrt{\\tau_{n }-(1-\\gamma)\\tau_{n+1}}+\\sqrt{\\tau_{N}}\\right), \\tag{23}\\]\n' +
      '\n' +
      '_where \\(\\gamma\\in[0,1]\\) is the parameter controlling the destination in each denoising step._\n' +
      '\n' +
      'Proof.: The proof is provided in Appendix D.3. \n' +
      '\n' +
      'While sufficient stochasticity in _diffuse sub-step_ helps reduce both the estimation errors and discretisation accumulated in earlier sampling steps and drives the sample towards the desired marginal distribution, a similar finding is also observed in (Karras et al., 2022; Xu et al., 2023b). In SSS, we encourage the step size of the _diffuse sub-step_ to be smaller than that of _the denoise sub-step_, which is contrary to (Xu et al., 2023b). It is also worth noting that when \\(\\gamma\\) is low, the estimation error plays a more important role. Hence, the optimal value of \\(\\gamma\\) should be determined empirically, as we show in Section 5.3.\n' +
      '\n' +
      'Figure 4: Qualitative comparison. For each prompt, images are generated using the same random seed for every model, without any cherry-picking. More results are provided in Appendix E.3.\n' +
      '\n' +
      'Figure 5: Qualitative effects of stochastic parameter \\(\\gamma\\) with same NFEs. Images under the same prompt but with different \\(\\gamma\\) applied during sampling. The leftmost image is sampled from LCM (Luo et al., 2023b). More samples can be found in Appendix E.2.\n' +
      '\n' +
      '### Extension to Large Text Conditional Models\n' +
      '\n' +
      'Conditional models frequently outperform their unconditional counterparts and exhibit a wider range of use cases (Bao et al., 2022; Dhariwal and Nichol, 2021; Ho and Salimans, 2022). Particularly, text conditional models have recently garnered substantial attention, showcasing remarkable results (Nichol et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Podell et al., 2023). The trajectory consistency function can be seamlessly integrated into conditional models by introducing an additional input, \\(\\mathbf{c}\\), to accommodate conditioning information, such as text. This results in the transformation of the trajectory function to \\(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t},\\mathbf{c},t,s)\\), and the guided distillation method proposed by (Meng et al., 2023; Luo et al., 2023) can be directly integrated into it, as detailed in Algorithm 2.\n' +
      '\n' +
      'Trajectory consistency distillation can directly occur on top of the parameters of a pre-trained diffusion model as the fine-tuning process. For scaling TCD to larger models (e.g., SDXL) with significantly reduced memory consumption, we incorporate Low-Rank Adaptation (LoRA) (Hu et al., 2021), a parameter-efficient fine-tuning method, into the distillation process. Additionally, the parameters of LoRA can be identified as a versatile acceleration module applicable to different fine-tuned models or LoRAs sharing the same base model without the need for additional training, aligning with the observations in (Luo et al., 2023).\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'We selected SDXL (Podell et al., 2023), a widely recognized diffusion model, as our backbone. By default, we employ TCF(1) as the parameterisation and set the stochastic parameter \\(\\gamma\\) as 0.2. The influence of \\(\\gamma\\) and parameterisation type is left to be explored in the ablation studies (Section 5.3). For detailed implementation information, please refer to Appendix C.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      'To illustrate the effectiveness and superiority of our methods, we perform qualitative and quantitative comparisons with prior works, including Euler (Karras et al., 2022), efficient numerical ODE solvers like DDIM (Song et al., 2020) and DPM-Solver++(2S) (Lu et al., 2022), and a relevant work LCM (Luo et al., 2023) as baseline.\n' +
      '\n' +
      'Qualitative Results.As illustrated in Figure 4, prior efficient numerical methods produce suboptimal images with 4 steps, while LCM can generate relatively better images. Our TCD further enhances visual quality. With an increased number of function evaluations (20 steps), the quality of samples generated by DDIM or DPM-Solver++(2S) improves rapidly. However, the improvement in LCM is not as noticeable, resulting in smoother and less detailed images due to the accumulated error in multistep sampling. In contrast, TCD addresses this flaw, producing more detailed\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{METHOD} & \\multicolumn{5}{c|}{FID \\(\\downarrow\\)} & \\multicolumn{5}{c}{Image Complexity Score \\(\\uparrow\\)} \\\\ \\cline{2-9}  & 2 STEPS & 4 STEPS & 8 STEPS & 20 STEPS & 2 STEPS & 4 STEPS & 8 STEPS & 20 STEPS \\\\ \\hline Euler (Karras et al., 2022) & 104.73 & 44.31 & 18.20 & 14.72 & 0.4251 & 0.3639 & 0.4151 & 0.4489 \\\\ DDIM (Song et al., 2020) & 105.98 & 44.86 & 17.62 & 13.60 & 0.4456 & 0.3633 & 0.4148 & 0.4481 \\\\ DPM++(2S) (Lu et al., 2022) & 46.08 & 18.50 & **12.49** & **12.15** & 0.2876 & 0.4496 & 0.4788 & 0.4679 \\\\ LCM (Luo et al., 2023) & 16.15 & 15.03 & 16.93 & 18.13 & 0.4300 & 0.4364 & 0.4260 & 0.4057 \\\\ TCD (Ours) & **14.66** & **12.68** & 13.64 & 13.56 & **0.4701** & **0.5095** & **0.5336** & **0.5563** \\\\ \\hline \\hline \\multicolumn{9}{c}{ImageReward \\(\\uparrow\\)} & \\multicolumn{5}{c}{PickScore \\(\\uparrow\\)} \\\\ \\cline{2-9}  & 2 STEPS & 4 STEPS & 8 STEPS & 20 STEPS & 2 STEPS & 4 STEPS & 8 STEPS & 20 STEPS \\\\ \\hline Euler (Karras et al., 2022) & -227.77 & -189.41 & 12.59 & 65.05 & 16.75 & 18.71 & 21.32 & 22.21 \\\\ DDIM (Song et al., 2020) & -227.75 & -189.96 & 13.45 & 66.14 & 16.74 & 18.68 & 21.31 & 22.16 \\\\ DPM++(2S) (Lu et al., 2022) & -169.21 & -1.27 & 67.58 & **75.8** & 19.05 & 20.68 & 21.9 & 22.33 \\\\ LCM (Luo et al., 2023) & 18.78 & 52.72 & 55.16 & 49.32 & 21.49 & 22.2 & 22.32 & 22.25 \\\\ TCD (Ours) & **34.58** & **68.49** & **73.09** & 74.96 & **21.51** & **22.31** & **22.5** & **22.36** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '* The best scores are highlighted in **bold**, and the runner-ups are underlined.\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative comparison on the COCO validation set.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Para Type & FID \\(\\downarrow\\) & IC Score \\(\\uparrow\\) & ImageReward \\(\\uparrow\\) & PickScore \\(\\uparrow\\) \\\\ \\hline TCF(1) & 12.68 & 0.5095 & 68.49 & 22.31 \\\\ TCF(2) & 13.35 & 0.5037 & 58.13 & 22.07 \\\\ TCF(S+) & 13.03 & 0.4176 & 57.96 & 22.01 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative ablation on the TCF parameterisation type.\n' +
      '\n' +
      'Figure 6: Quantitative ablation on different stochastic parameter \\(\\gamma\\).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'Papercut XL 2, Depth ControlNet 3, Canny ControlNet 4, and IP-Adapter 5. The results shown in Figure 7 imply that TCD can be directly applied to various models to accelerate image generation with high quality in only 2-8 steps. Additional samples can be found in Appendix E.4.\n' +
      '\n' +
      'Footnote 2: _Papercut_: [https://civitai.com/models/122567/papercut-sdxl](https://civitai.com/models/122567/papercut-sdxl)\n' +
      '\n' +
      'Footnote 3: _Depth ControlNet_: [https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0](https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0)\n' +
      '\n' +
      'Footnote 4: _Canny ControlNet_: [https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0)\n' +
      '\n' +
      'Footnote 5: _IP-Adapter_: [https://github.com/tencent-ailab/IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this work, we introduce TCD, a novel distillation method that reduces inherent errors present in consistency models, including TCF for training and SSS for sampling. The TCF is proposed to diminish distillation errors and enable the model to track the trajectory along the PF ODE. Furthermore, SSS is proposed to reduce accumulated errors by the bijective traversal. Remarkably, TCD outperforms LCM across all sampling steps and exhibits superior performance compared to numerical methods of teacher model. We believe that TCD can provide novel perspectives for fast and high-quality image generation, while certain characters of TCD also contribute valuable insights to downstream applications, _e.g._, enhanced details for super-resolution and a better intermediate manifold for editing.\n' +
      '\n' +
      'Limitations.In our experiments, we observed instability in high-order TCF and poor convergence in TCF(S+). Further analysis is necessary to ascertain the stability of the high-order function and TCF(S+). Additionally, it is worth investigating an improved design to achieve fewer steps generation, _e.g._, single step.\n' +
      '\n' +
      '## Impact Statements\n' +
      '\n' +
      'While our advancements in sample quality and speed can help reduce inference costs, they also have the potential to amplify negative societal effects, such as disseminating disinformation. In the future, it is encouraged to implement adequate guardrails and detection techniques to minimize the risk of misuse.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agahanyan et al. (2020) Aghahanyan, A., Zettlemoyer, L., and Gupta, S. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. _arXiv preprint arXiv:2012.13255_, 2020.\n' +
      '* Balaji et al. (2022) Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., et al. ediff: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* Bao et al. (2022) Bao, F., Li, C., Sun, J., and Zhu, J. Why are conditional generative models better than unconditional ones? _arXiv preprint arXiv:2212.00362_, 2022.\n' +
      '* Chen et al. (2020) Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. _arXiv preprint arXiv:2009.00713_, 2020.\n' +
      '* Chen et al. (2022) Chen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _arXiv preprint arXiv:2209.11215_, 2022.\n' +
      '* De Bortoli et al. (2021) De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.\n' +
      '* Dhariwal and Nichol (2021) Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.\n' +
      '* Dieleman (2022) Dieleman, S. Guidance: a cheat code for diffusion models, 2022. URL [https://benanne.github.io/2022/05/26/guidance.html](https://benanne.github.io/2022/05/26/guidance.html).\n' +
      '* Feng et al. (2022) Feng, T., Zhai, Y., Yang, J., Liang, J., Fan, D.-P., Zhang, J., Shao, L., and Tao, D. Ic9600: A benchmark dataset for automatic image complexity assessment. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.\n' +
      '* Guo et al. (2023) Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* Ho & Salimans (2022) Ho, J. and Salimans, T. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pp. 2790-2799. PMLR, 2019.\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '\n' +
      'Hyvarinen, A., Hurri, J., Hoyer, P. O., Hyvarinen, A., Hurri, J., and Hoyer, P. O. Estimation of non-normalized statistical models. _Natural Image Statistics: A Probabilistic Approach to Early Computational Vision_, pp. 419-426, 2009.\n' +
      '* Jolicoeur-Martineau et al. (2021) Jolicoeur-Martineau, A., Li, K., Piche-Taillefer, R., Kachman, T., and Mitliagkas, I. Gotta go fast when generating data with score-based models. _arXiv preprint arXiv:2105.14080_, 2021.\n' +
      '* Karras et al. (2022) Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.\n' +
      '* Kim et al. (2023) Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. _arXiv preprint arXiv:2310.02279_, 2023.\n' +
      '* Kirstain et al. (2023) Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-pic: An open dataset of user preferences for text-to-image generation. _arXiv preprint arXiv:2305.01569_, 2023.\n' +
      '* Kong & Ping (2021) Kong, Z. and Ping, W. On fast sampling of diffusion probabilistic models. _arXiv preprint arXiv:2106.00132_, 2021.\n' +
      '* Kong et al. (2020) Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. Diffwave: A versatile diffusion model for audio synthesis. _arXiv preprint arXiv:2009.09761_, 2020.\n' +
      '* Lee et al. (2023) Lee, H., Lu, J., and Tan, Y. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pp. 946-985. PMLR, 2023.\n' +
      '* Li et al. (2023) Li, Y., Qian, Z., and van der Schaar, M. Do diffusion models suffer error propagation? theoretical analysis and consistency regularization. _arXiv preprint arXiv:2308.05021_, 2023.\n' +
      '* Lu et al. (2022a) Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022a.\n' +
      '* Lu et al. (2022b) Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. _arXiv preprint arXiv:2211.01095_, 2022b.\n' +
      '* Luhman & Luhman (2021) Luhman, E. and Luhman, T. Knowledge distillation in iterative generative models for improved sampling speed. _arXiv preprint arXiv:2101.02388_, 2021.\n' +
      '* Luo et al. (2023a) Luo, S., Tan, Y., Huang, L., Li, J., and Zhao, H. Latent consistency models: Synthesizing high-resolution images with few-step inference. _arXiv preprint arXiv:2310.04378_, 2023a.\n' +
      '* Luo et al. (2023b) Luo, S., Tan, Y., Patil, S., Gu, D., von Platen, P., Passos, A., Huang, L., Li, J., and Zhao, H. Lcm-lora: A universal stable-diffusion acceleration module. _arXiv preprint arXiv:2311.05556_, 2023b.\n' +
      '* Lyu et al. (2023) Lyu, J., Chen, Z., and Feng, S. Convergence guarantee for consistency models. _arXiv preprint arXiv:2308.11449_, 2023.\n' +
      '* Meng et al. (2023) Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14297-14306, 2023.\n' +
      '* Nichol et al. (2022) Nichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pp. 16784-16804. PMLR, 2022.\n' +
      '* Podell et al. (2023) Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* Popov et al. (2021) Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-tts: A diffusion probabilistic model for text-to-speech. In _International Conference on Machine Learning_, pp. 8599-8608. PMLR, 2021.\n' +
      '* Prasad (1990) Prasad, D. An introduction to numerical analysis. _Mathematics and Computers in Simulation_, pp. 319, May 1990. doi: 10.1016/0378-4754(90)90206-x. URL [http://dx.doi.org/10.1016/0378-4754](http://dx.doi.org/10.1016/0378-4754)(90)90206-x.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '\n' +
      '* Salimans & Ho (2022) Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. _arXiv preprint arXiv:2202.00512_, 2022.\n' +
      '* Sauer et al. (2023) Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. _arXiv preprint arXiv:2311.17042_, 2023.\n' +
      '* Schuhmann (2022) Schuhmann, C. Clip+mlp aesthetic score predictor. [https://github.com/christophschuhmann/improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor), 2022.\n' +
      '* Schuhmann et al. (2022) Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Song et al. (2020a) Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020a.\n' +
      '* Song & Ermon (2019) Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* Song et al. (2020b) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020b.\n' +
      '* Song et al. (2023) Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.\n' +
      '* Wu et al. (2023) Wu, J. Z., Ge, Y., Wang, X., Lei, S. W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., and Shou, M. Z. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7623-7633, 2023.\n' +
      '* Xu et al. (2023a) Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagenreward: Learning and evaluating human preferences for text-to-image generation. _arXiv preprint arXiv:2304.05977_, 2023a.\n' +
      '* Xu et al. (2023b) Xu, Y., Deng, M., Cheng, X., Tian, Y., Liu, Z., and Jaakkola, T. Restart sampling for improving generative processes. _arXiv preprint arXiv:2306.14878_, 2023b.\n' +
      '* Yin et al. (2023) Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. _arXiv preprint arXiv:2311.18828_, 2023.\n' +
      '* Zhang & Chen (2022) Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_, 2022.\n' +
      '\n' +
      'A Related Works\n' +
      '\n' +
      'Diffusion Models.Diffusion Models (DMs), also known as Score-based generative models, have shown superior performance in various generative fields. Chen et al. (2022) provides theoretical convergence guarantees, implying that DMs can efficiently sample from essentially any realistic data distribution under minimal data assumptions. ADM (Dhariwal and Nichol, 2021) firstly shows the potential of DMs to outperform GANs. EDM (Karras et al., 2022) further elucidates the design space of GMs, clearly separating the concrete design choices, deriving best practices for the sampling process, and improving the training dynamics, thereby drastically improving the results.\n' +
      '\n' +
      'Text-conditional Diffusion Models.DMs have specially achieved great success in the area of text-to-image synthesis (Nichol et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Balaji et al., 2022; Podell et al., 2023). To reduce computational cost, diffusion models typically operate within the latent space (Rombach et al., 2022; Podell et al., 2023) or include separate super-resolution steps (Ramesh et al., 2022; Saharia et al., 2022; Balaji et al., 2022). The integration of classifier-free diffusion guidance (Ho and Salimans, 2022; Dieleman, 2022) during the sampling process dramatically improves samples produced by conditional diffusion models at almost no additional cost.\n' +
      '\n' +
      'Fast Sampling of DMs.DMs exhibit great generative capabilities but are bottlenecked by their slow sampling speed. Various numerical methods driven by the SDE mathematical underpinnings underlying DMs have been proposed for accelerating DM sampling. DDIM (Song et al., 2020) has originally shown promise for few-step sampling. Other works involve predictor-corrector samplers (Song et al., 2020; Karras et al., 2022), exponential integrators (Lu et al., 2022; Zhang and Chen, 2022; Lu et al., 2022), and automated methods to adjust solvers (Kong and Ping, 2021; Jolicoeur-Martineau et al., 2021). Despite these methods having achieved great improvement in fast sampling, they are limited by the inherent discretisation error present in all solvers (De Bortoli et al., 2021). Ultimately, the sample quality obtained with few NFEs is limited. Another series of research works exemplified by distillation techniques (Luhman and Luhman, 2021; Salimans and Ho, 2022; Meng et al., 2023). These methods distill knowledge from pretrained models into few-step samplers, representing an efficient solution for few NFEs. However, they may experience a lengthy and costly process that requires huge amounts of data and suffer from slow convergence.\n' +
      '\n' +
      'Consistency Models.To overcome the limitations of current fast samplers, Song et al. (2023) proposes Consistency Models (CMs) learning a direct mapping from noise to data built on the top of the trajectory of PF ODE, achieving one-step generation while allowing multi-step sampling to trade compute for quality. Lyu et al. (2023) provide the first convergence guarantees for CMs under moderate assumptions. Kim et al. (2023) proposes a universal framework for CMs and DMs. The core design is similar to ours, with the main differences being that we focus on reducing error in CMs, subtly leverage the semi-linear structure of the PF ODE for parameterization, and avoid the need for adversarial training. Latent Consistency Models (LCMs) (Luo et al., 2023) integrate consistency distillation with latent diffusion models (Rombach et al., 2022) to achieve impressive performance in accelerating text-to-image synthesis. LCM-LoRA (Luo et al., 2023) further improves the training efficiency and versatility of LCMs by introducing LoRA (Hu et al., 2021) into the distillation process. However, due to inherent accumulated errors in multistep consistency samplings, LCM experiences a notable decline in quality at high NFEs. In contrast, our TCD leverages trajectory consistency and bijective traversal to eliminate this defect.\n' +
      '\n' +
      'Parameter-Efficient Fine-Tuning.Training a diffusion model is highly resource-intensive and environmentally unfriendly. Fine-tuning such a model can also be challenging due to the vast number of parameters involved (Aghajanyan et al., 2020). Thus, Parameter-Efficient Fine-Tuning (PEFT) (Houlsby et al., 2019) was proposed to enable the fine-tuning of pretrained models with a limited number of parameters required for training. Among these techniques, Low-Rank Adaptation (LoRA) (Hu et al., 2021) has demonstrated transcendental performance. LoRA\'s strategy involves freezing the pretrained model weights and injecting trainable rank decomposition matrices, which succinctly represent the required adjustments in the model\'s weights for fine-tuning. With this strategy, LoRA significantly reduces the volume of parameters to be modified, thereby substantially decreasing both computational load and storage demands.\n' +
      '\n' +
      '## Appendix B Algorithmic Details\n' +
      '\n' +
      '### Algorithm Details of Trajectory Consistency Distillation\n' +
      '\n' +
      'Due to space limitations, we omitted some implementation details in the main body, but we provided a detailed algorithm for trajectory consistency distillation during training as Algorithm 1 and a guided version for conditional models as Algorithm 2.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '## Appendix C Implementation Details\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      'We trained TCD on the LAION5B High-Res dataset (Schuhmann et al., 2022). Images with an aesthetic score (Schuhmann, 2022) lower than 5.8 were filtered out. For training, all images were initially resized to 1024 pixels by the shorter side and subsequently randomly cropped to dimensions of \\(1024\\times 1024\\).\n' +
      '\n' +
      '### Hyper-parameters\n' +
      '\n' +
      'In our experiments, we utilized the AdamW optimizer with \\(\\beta_{1}=0.9,\\beta_{2}=0.999\\), and a weight decay of 0.01. For TCF(2) parameterization, the itemidinate timestep \\(u\\) in Equation (18) was set as \\(u:=t_{\\lambda}(\\lambda_{s}+\\lambda_{t})\\), where \\(t_{\\lambda}\\) is the inverse function of \\(\\lambda_{t}\\). For TCF(S+) parameterization, to encode the ending time \\(s\\) into the TCD model, we create sinusoidal timestep embeddings matching the implementation in (Ho et al., 2020). Subsequently, we project it with a simple MLP, integrating it into the TCD backbone by adding the projected \\(s\\)-embedding to the start time \\(t\\)-embedding, following the approach in the original Stable Diffusion models (Rombach et al., 2022). We choose the DDIM (Song et al., 2020) as the ODE solver and set the skipping step \\(k=20\\) in Equation (21). During training, we initialize the trajectory consistency function with the same parameters as the teacher diffusion model. The guidance scale for guided distillation (Meng et al., 2023) was set to \\([2,14]\\). We did not utilize the EDM\'s skip scale and output scale, as we found it did not provide any benefit. For TCF(1) and TCF(2), the batch size was set to 256, and the learning rate was set to 4.5e-6, and the LoRA rank was 64. We did not use EMA for TCF(1) and TCF(2). For TCF(S+), the batch size was set to 192, the learning rate was set to 4.0e-6. We did not use LoRA for TCF(S+) but directly fine-tuned the U-Net, and the EMA decay rate was set to 0.95.\n' +
      '\n' +
      '### Hardware and Efficiency\n' +
      '\n' +
      'All our models were trained using 8 80G A800 units. TCF(1) was trained for 3000 iterations, taking 15 hours, demonstrating high training efficiency. TCF(2) employed the same parameterization method as TCF(1) during training. TCF(S+) was trained for 43,000 iterations, spanning 5 days and 20 hours. The low convergence rate of TCF(S+) may be attributed to the introduction of additional parameters, resulting in low distillation efficiency of teacher information. We defer the exploration of improved design for enhancing the efficiency of TCF(S+) to future work. The inference time in Figure 1 was tested on an A800, averaged over 16 prompts.\n' +
      '\n' +
      '## Appendix D Proofs\n' +
      '\n' +
      '### Proof of Corollary 3.1\n' +
      '\n' +
      'Before we prove the corollary 3.1, we assume the following mild conditions on the data distribution \\(p_{\\mathrm{data}}\\)(Lee et al., 2023; Chen et al., 2022):\n' +
      '\n' +
      '**Assumption D.1**.: The data distribution has finite second moment, that is, \\(\\mathbb{E}_{\\mathbf{x}_{0}\\sim p_{\\mathrm{data}}}[\\|\\mathbf{x}_{0}\\|_{2}^{2}]=m_{2}^{2}<\\infty\\).\n' +
      '\n' +
      '**Assumption D.2**.: The score function \\(\\nabla\\log p_{t}(\\mathbf{x})\\) is Lipschitz on the variable \\(\\mathbf{x}\\) with Lipschitz constant \\(L_{s}\\geq 1\\), \\(\\forall t\\in[0,T]\\).\n' +
      '\n' +
      'We further assume bounds on the score estimation error and consistency error (Song et al., 2023; Lyu et al., 2023):\n' +
      '\n' +
      '**Assumption D.3**.: Assume \\(\\mathbb{E}_{\\mathbf{x}_{t_{n}}\\sim p_{t_{n}}}[\\|\\mathbf{s}_{\\mathbf{\\phi}}(\\mathbf{x}_{t_{n}},t_{n})-\\nabla\\log p_{t_{n}}(\\mathbf{x}_{t_{n}})\\|_{2}^{2}]\\leq\\varepsilon_{ \\mathrm{sc}}^{2},\\forall n\\in[\\![1,N]\\!]\\).\n' +
      '\n' +
      '**Assumption D.4**.: Assume \\(\\mathbb{E}_{\\mathbf{x}_{t_{n}}\\sim p_{t_{n}}}[\\|\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n +1}},t_{n+1})-\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{\\hat{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n})\\|_{ 2}^{2}]\\leq\\varepsilon_{\\mathrm{cm}}^{2}(t_{n+1}-t_{n})^{2},\\forall n\\in[\\![1, N-1]\\!]\\)\n' +
      '\n' +
      '**Assumption D.5**.: The consistency model \\(\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x},t_{n})\\) is Lipschitz on the variable \\(\\mathbf{x}\\) with Lipschitz constant \\(L_{f}>1,\\ \\forall n\\in[\\![1,N]\\!]\\).\n' +
      '\n' +
      'We also follow the (Lyu et al., 2023) to assume a discrete schedule:\n' +
      '\n' +
      '**Assumption D.6**.: Assume the discretisation schedule \\(0<\\delta=t_{1}<t_{2}<\\cdots<t_{N}=T\\), \\(h_{k}=t_{k+1}-t_{k}\\) to Equation (3) is divided into two stages:\n' +
      '\n' +
      '1. \\(h_{k}\\equiv h\\) for all \\(k\\in[\\![N_{1},N-1]\\!]\\), and \\((N-N_{1}-1)h<T\\leq(N-N_{1})h\\);\n' +
      '2. \\(h_{k}=2^{-(N_{1}-k)}h=\\frac{h_{k+1}}{2}\\) for \\(k\\in[\\![1,N_{1}-1]\\!]\\), \\(N_{1}\\) satisfies \\(h_{2}=2^{-(N_{1}-2)}h\\leq 2\\delta\\).\n' +
      '\n' +
      'Then we introduce some properties and a lemma borrowed from (Lyu et al., 2023), which shows that TV error can be bounded after a small time OU regularisation.\n' +
      '\n' +
      'The forward OU process can be defined as follows:\n' +
      '\n' +
      '\\[\\mathrm{d}\\mathbf{x}_{t}=-\\mathbf{x}_{t}\\mathrm{d}t+\\sqrt{2}\\mathrm{d}\\mathbf{w}_{t}, \\tag{26}\\]\n' +
      '\n' +
      'and denote the Markov kernel \\(P^{s}_{\\text{OU}}\\) to be defined by Equation (26), that is, if \\(\\mathbf{x}_{t}\\sim p\\) for some \\(p\\) be a distribution over \\(\\mathbb{R}^{d}\\), then \\(\\mathbf{x}_{t+s}\\sim pP^{s}_{\\text{OU}}\\).\n' +
      '\n' +
      'Under Assumptions D.1-D.6, when \\(T>L_{s}^{-1}\\), the one-step generating error is bounded as follows:\n' +
      '\n' +
      '\\[W_{2}(\\mathbf{f}_{\\mathbf{\\theta},T}\\sharp\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d}),p_{\\text{data }})\\lesssim(d^{\\frac{1}{2}}\\lor m_{2})L_{f}e^{-T}+T(\\varepsilon_{\\text{cm}}+L _{f}\\varepsilon_{\\text{sc}}+L_{f}L_{s}^{\\frac{3}{2}}h)+(d^{\\frac{1}{2}}\\lor m _{2})\\delta^{\\frac{1}{2}}. \\tag{27}\\]\n' +
      '\n' +
      '**Lemma D.7**.: _For any two distributions \\(p\\) and \\(q\\), running the OU process in Equation (26) for \\(p,q\\) individually with time \\(\\tau>0\\), the following TV distance bound holds,_\n' +
      '\n' +
      '\\[\\text{TV}(pP^{\\tau}_{\\text{OU}},qP^{\\tau}_{\\text{OU}})\\lesssim\\frac{1}{\\sqrt{ \\tau}}W_{1}(p,q)\\leq\\frac{1}{\\sqrt{\\tau}}W_{2}(p,q)\\]\n' +
      '\n' +
      'Proof.: Denote \\(\\psi_{\\sigma^{2}}(\\mathbf{y})\\) as the density function to the normal distribution \\(\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{d})\\). We write the \\(\\text{TV}(pP^{\\tau}_{\\text{OU}},qP^{\\tau}_{\\text{OU}})\\) into integral form as:\n' +
      '\n' +
      '\\[\\text{TV}(pP^{\\tau}_{\\text{OU}},qP^{\\tau}_{\\text{OU}}) =\\frac{1}{2}\\int_{\\mathbb{R}^{d}}|(pP^{\\tau}_{\\text{OU}})(\\mathbf{x}) -(qP^{\\tau}_{\\text{OU}})(\\mathbf{x})|\\mathrm{d}\\mathbf{x}\\] \\[=\\frac{1}{2}\\int_{\\mathbb{R}^{d}}\\left|\\int_{\\mathbb{R}^{d}}p(\\bm {y})\\psi_{1-e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{y})\\mathrm{d}\\mathbf{y}-\\int_{\\mathbb{ R}^{d}}q(\\mathbf{z})\\psi_{1-e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{z})\\mathrm{d}\\mathbf{z} \\right|\\mathrm{d}\\mathbf{x}. \\tag{28}\\]\n' +
      '\n' +
      'Take a coupling \\(\\gamma\\in\\Gamma(p,q)\\), then\n' +
      '\n' +
      '\\[\\int_{\\mathbb{R}^{d}}\\gamma(\\mathbf{y},\\mathbf{z})\\mathrm{d}\\mathbf{z} =p(\\mathbf{y}),\\] \\[\\int_{\\mathbb{R}^{d}}\\gamma(\\mathbf{y},\\mathbf{z})\\mathrm{d}\\mathbf{y} =q(\\mathbf{z}), \\tag{29}\\]\n' +
      '\n' +
      'we have\n' +
      '\n' +
      '\\[\\text{TV}(pP^{\\tau}_{\\text{OU}},qP^{\\tau}_{\\text{OU}}) =\\frac{1}{2}\\int_{\\mathbb{R}^{d}}\\left|\\int_{\\mathbb{R}^{d\\times d }}\\gamma(\\mathbf{y},\\mathbf{z})[\\psi_{1-e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{y})-\\psi_{1-e^ {-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{z})]\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{z}\\right| \\mathrm{d}\\mathbf{x}.\\] \\[\\leq\\frac{1}{2}\\int_{\\mathbb{R}^{d}}\\int_{\\mathbb{R}^{d\\times d}} \\gamma(\\mathbf{y},\\mathbf{z})\\left|\\psi_{1-e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{y})-\\psi_{1- e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{z})\\right|\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{z} \\mathrm{d}\\mathbf{x}\\] \\[=\\int_{\\mathbb{R}^{d\\times d}}\\gamma(\\mathbf{y},\\mathbf{z})\\left(\\frac{1} {2}\\int_{\\mathbb{R}^{d}}|\\psi_{1-e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{y})-\\psi_{1- e^{-2\\tau}}(\\mathbf{x}-e^{-\\tau}\\mathbf{z})|\\mathrm{d}\\mathbf{x}\\right)\\mathrm{d}\\mathbf{y} \\mathrm{d}\\mathbf{z}\\] \\[=\\int_{\\mathbb{R}^{d\\times d}}\\gamma(\\mathbf{y},\\mathbf{z})\\text{TV}(\\psi _{1-e^{-2\\tau}}(\\cdot-e^{-\\tau}\\mathbf{y}),\\psi_{1-e^{-2\\tau}}(\\cdot-e^{-\\tau}\\mathbf{ z}))\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{z}\\] \\[\\leq\\int_{\\mathbb{R}^{d\\times d}}\\gamma(\\mathbf{y},\\mathbf{z})\\sqrt{\\frac{ 1}{2}\\text{KL}(\\psi_{1-e^{-2\\tau}}(\\cdot-e^{-\\tau}\\mathbf{y})\\|\\psi_{1-e^{-2\\tau}} (\\cdot-e^{-\\tau}\\mathbf{z}))\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{z}}\\] \\[=\\int_{\\mathbb{R}^{d\\times d}}\\gamma(\\mathbf{y},\\mathbf{z})\\frac{1}{2} \\sqrt{\\frac{e^{-2\\tau}}{1-e^{-2\\tau}}\\|\\mathbf{y}-\\mathbf{z}\\|_{2}^{2}}\\mathrm{d}\\mathbf{y} \\mathrm{d}\\mathbf{z}\\] \\[=\\frac{1}{2\\sqrt{e^{2\\tau}-1}}\\int_{\\mathbb{R}^{d\\times d}}\\gamma( \\mathbf{y},\\mathbf{z})\\|\\mathbf{y}-\\mathbf{z}\\|_{2}\\mathrm{d}\\mathbf{y}\\mathrm{d}\\mathbf{z} \\tag{30}\\]\n' +
      '\n' +
      'Noting \\(\\frac{1}{2\\sqrt{e^{2\\tau}-1}}\\leq\\frac{1}{2\\sqrt{2\\tau}}\\), and taking \\(\\gamma\\) over all coupling \\(\\Gamma(p,q)\\), we have\n' +
      '\n' +
      '\\[\\text{TV}(pP^{\\tau}_{\\text{OU}},qP^{\\tau}_{\\text{OU}})\\lesssim\\frac{1}{\\sqrt{ \\tau}}W_{1}(p,q)\\leq\\frac{1}{\\sqrt{\\tau}}W_{2}(p,q)\\]Let \\(q\\) be the output of consistency models, either the one step consistency sampling result or the \\(k\\)-th multistep consistency sampling result. To control the TV error, we smooth the generated sample by the forward OU process with a small time that is the same as the early stopping time \\(\\delta\\), and then we can get the TV distance between \\(q_{k}P_{\\text{OU}}^{\\delta}\\) and \\(p_{\\text{data}}\\).\n' +
      '\n' +
      '**Proof of Corollary 3.1**.: According to the triangular inequality, Lemma D.7 and Equation (27),\n' +
      '\n' +
      '\\[\\text{TV}(q_{1}P_{\\text{OU}}^{\\delta},p_{\\text{data}}) \\leq\\text{TV}(q_{1}P_{\\text{OU}}^{\\delta},p_{\\delta}P_{\\text{OU }}^{\\delta})+\\text{TV}(p_{\\delta}P_{\\text{OU}}^{\\delta},p_{\\text{data}})\\] \\[\\lesssim\\frac{1}{\\sqrt{\\delta}}W_{2}(q_{1},p_{\\delta})+\\text{TV} (p_{2\\delta},p_{\\text{data}})\\] \\[\\lesssim\\frac{1}{\\sqrt{\\delta}}\\left((d^{\\frac{1}{2}}\\lor m_{2}) L_{f}e^{-T}+T(\\varepsilon_{\\text{cm}}+L_{f}\\varepsilon_{\\text{sc}}+L_{f}L_{s}^{ \\frac{3}{2}}d^{\\frac{1}{2}}h)\\right)+\\text{TV}(p_{2\\delta},p_{\\text{data}}). \\tag{31}\\]\n' +
      '\n' +
      'Note that if we take \\(\\delta\\asymp\\frac{\\varepsilon^{2}}{L_{s}^{2}(d\\lor m_{2}^{2})}\\), then by Lemma 6.4, (Lee et al., 2023), \\(\\text{TV}(p_{2\\delta},p_{\\text{data}})\\leq\\varepsilon\\), this concludes that\n' +
      '\n' +
      '\\[\\text{TV}(q_{1}P_{\\text{OU}}^{\\delta},p_{\\text{data}})\\lesssim\\frac{L_{s}(d^{ \\frac{1}{2}}\\lor m_{2})}{\\varepsilon}[(\\log L_{f}+\\frac{T}{2^{k}})(\\varepsilon _{\\text{cm}}+L_{f}\\varepsilon_{\\text{sc}}+L_{f}L_{s}^{\\frac{3}{2}}d^{\\frac{1} {2}}h)+\\frac{(d^{\\frac{1}{2}}\\lor m_{2})L_{f}}{2^{k}e^{T}}]+\\varepsilon.\\]\n' +
      '\n' +
      'Similarly for \\(q_{k}\\), if we take \\(\\delta\\asymp\\frac{\\varepsilon^{2}}{L_{s}^{2}(d\\lor m_{2}^{2})}\\)\n' +
      '\n' +
      '\\[\\text{TV}(q_{k}P_{\\text{OU}}^{\\delta},p_{\\text{data}}) \\tag{32}\\] \\[\\leq \\text{TV}(q_{k}P_{\\text{OU}}^{\\delta},p_{\\delta}P_{\\text{OU}}^{ \\delta})+\\text{TV}(p_{\\delta}P_{\\text{OU}}^{\\delta},p_{\\text{data}})\\] \\[\\lesssim \\frac{L_{s}(d^{\\frac{1}{2}}\\lor m_{2})}{\\varepsilon}[(\\log L_{f}+ \\frac{T}{2^{k}})(\\varepsilon_{\\text{cm}}+L_{f}\\varepsilon_{\\text{sc}}+L_{f}L_{ s}^{\\frac{3}{2}}d^{\\frac{1}{2}}h)+\\frac{(d^{\\frac{1}{2}}\\lor m_{2})L_{f}}{2^{k}e^{T}}]+\\varepsilon. \\tag{33}\\]\n' +
      '\n' +
      '### Proof of Theorem 4.1\n' +
      '\n' +
      'The proof closely follows Song et al. (2023) and relies on induction, drawing parallels with the classic proof of global error bounds for numerical ODE solvers (Prasad, 1990).\n' +
      '\n' +
      'Proof.: For simplicity of notation, we denote \\(\\mathbf{\\theta}^{\\star}\\) as \\(\\mathbf{\\theta}\\). From \\(\\mathcal{L}_{\\text{TCD}}^{N}(\\mathbf{\\theta},\\mathbf{\\theta};\\mathbf{\\phi})=0\\), we have\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{TCD}}^{N}=\\mathbb{E}\\left[\\omega(t_{n},t_{m})\\left\\|\\mathbf{f}_ {\\mathbf{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1},t_{m})-\\mathbf{f}_{\\mathbf{\\theta}}(\\hat{\\mathbf{x }}_{t_{n}}^{\\mathbf{\\phi}},t_{n},t_{m})\\right\\|_{2}^{2}\\right]=0. \\tag{34}\\]\n' +
      '\n' +
      'According to the definition, it follows that \\(p_{t_{n}}(\\mathbf{x}_{t_{n}})>0\\) for every \\(x_{t_{n}}\\) and \\(1\\leqslant n\\leqslant N\\). Therefore, Equation (34) entails\n' +
      '\n' +
      '\\[\\omega(t_{n},t_{m})\\left\\|\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1},t_{m} )-\\mathbf{f}_{\\mathbf{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n},t_{m})\\right\\|_ {2}^{2}\\equiv 0. \\tag{35}\\]\n' +
      '\n' +
      'Because \\(\\lambda(\\cdot)>0\\) and \\(\\left\\|\\mathbf{x},\\mathbf{y}\\right\\|_{2}^{2}=0\\Leftrightarrow\\mathbf{x}=\\mathbf{y}\\), this further implies that\n' +
      '\n' +
      '\\[\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1},t_{m})\\equiv\\mathbf{f}_{\\mathbf{\\theta}} (\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n},t_{m}). \\tag{36}\\]\n' +
      '\n' +
      'Let \\(\\mathbf{e}_{n}^{s}\\) represent the error vector that arises when predicting \\(x_{s}\\) at \\(t_{n}\\), which is defined as\n' +
      '\n' +
      '\\[\\mathbf{e}_{n,m}:=\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n}},t_{n},t_{m})-\\mathbf{f}(\\mathbf{x}_{t _{n}},t_{n},t_{m};\\mathbf{\\phi}).\\]\n' +
      '\n' +
      'We can easily derive the following recursion relation\n' +
      '\n' +
      '\\[\\mathbf{e}_{n+1,m} =\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1},t_{m})-\\mathbf{f}(\\mathbf{ x}_{t_{n+1}},t_{n+1},t_{m};\\mathbf{\\phi}) \\tag{37}\\] \\[=\\mathbf{f}_{\\mathbf{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n},t_{m })-\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n}},t_{n},t_{m})+\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_ {t_{n}},t_{n},t_{m})-\\mathbf{f}(\\mathbf{x}_{t_{n}},t_{n},t_{m};\\mathbf{\\phi})\\] \\[=\\mathbf{f}_{\\mathbf{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{\\phi}},t_{n},t_{m })-\\mathbf{f}_{\\mathbf{\\theta}}(\\mathbf{x}_{t_{n}},t_{n},t_{m})+\\mathbf{e}_{n,m}.\\]Because \\(\\mathbf{f}_{\\theta}(\\cdot,t_{n},t_{m})\\) has Lipschitz constant \\(L\\), we have\n' +
      '\n' +
      '\\[\\|\\mathbf{e}_{n+1,m}\\|_{2} \\leqslant\\|\\mathbf{e}_{n,m}\\|_{2}+L\\left\\|\\hat{\\mathbf{x}}_{t_{n}}^{\\mathbf{ \\phi}}-\\mathbf{x}_{t_{n}}\\right\\|_{2}\\] \\[=\\|\\mathbf{e}_{n,m}\\|_{2}+L\\cdot\\mathcal{O}((t_{n+1}-t_{n})^{p+1})\\] \\[=\\|\\mathbf{e}_{n,m}\\|_{2}+\\mathcal{O}((t_{n+1}-t_{n})^{p+1}.\\]\n' +
      '\n' +
      'In addition, we observe that \\(\\mathbf{e}_{m,m}=\\mathbf{0}\\), because\n' +
      '\n' +
      '\\[\\mathbf{e}_{m,m} =\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{m}},t_{m},t_{m})-\\mathbf{f}(\\mathbf{x}_{t_{m} },t_{m},t_{m};\\mathbf{\\phi})\\] \\[\\overset{(i)}{=}\\mathbf{x}_{t_{m}}-\\mathbf{f}(\\mathbf{x}_{t_{m}},t_{m},t_{m}; \\mathbf{\\phi})\\] \\[\\overset{(i)}{=}\\mathbf{x}_{t_{m}}-\\mathbf{x}_{t_{m}}\\] \\[=\\mathbf{0},\\]\n' +
      '\n' +
      'here \\((i)\\) is true because trajectory consistency distillation restricts the boundary condition for the parameterized model such that \\(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{m}},t_{m},t_{m})=\\mathbf{x}_{t_{m}}\\), and \\((ii)\\) is entailed by the definition of trajectory function \\(\\mathbf{f}(\\cdot,\\cdot,\\cdot;\\mathbf{\\phi})\\) in Equation (13). This allows us to perform induction on the recursion formula Equation (37) to obtain\n' +
      '\n' +
      '\\[\\|\\mathbf{e}_{n,m}\\|_{2} \\leqslant\\|\\mathbf{e}_{m,m}\\|_{2}+\\sum_{k=m}^{n-1}\\mathcal{O}((t_{k+ 1}-t_{k})^{p+1}\\] \\[=\\sum_{k=m}^{n-1}\\mathcal{O}((t_{k+1}-t_{k})^{p+1}\\] \\[=\\sum_{k=m}^{n-1}(t_{k+1}-t_{k})\\mathcal{O}((t_{k+1}-t_{k})^{p}\\] \\[\\leqslant\\sum_{k=m}^{n-1}(t_{k+1}-t_{k})\\mathcal{O}((\\Delta t)^{p}\\] \\[=\\mathcal{O}((\\Delta t)^{p}\\sum_{k=m}^{n-1}(t_{k+1}-t_{k})\\] \\[=\\mathcal{O}((\\Delta t)^{p}(t_{n}-t_{m}),\\]\n' +
      '\n' +
      'which completes the proof. \n' +
      '\n' +
      '### Proof of Theorem 4.2\n' +
      '\n' +
      'In this section, our derivation mainly borrows the proof from (Kim et al., 2023; Chen et al., 2022).\n' +
      '\n' +
      'For DDPM (Chen et al., 2022),\n' +
      '\n' +
      '**Theorem D.8**.: _Suppose that Assumptions D.2, D.1, and D.3 hold. Let \\(q_{T}\\) be the output of the DDPM algorithm at time \\(T\\), and suppose that the step size \\(h:=T/N\\) satisfies \\(h\\lesssim 1/L_{s}\\), where \\(L_{s}\\geq 1\\) is the Lipschitz constant of score function. Then, it holds that_\n' +
      '\n' +
      '\\[\\text{TV}(q_{T},p_{\\text{data}})\\lesssim\\underbrace{\\sqrt{D_{\\text{KL}}(p_{ \\text{data}}\\|\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d}))}\\exp(-T)}_{\\text{convergence of forward process}}+\\underbrace{\\left(L_{s}\\sqrt{dh}+L_{s}m_{2}h\\right)\\sqrt{T}}_{\\text{ discretization error}}+\\underbrace{\\epsilon_{\\text{sc}}\\sqrt{T}}_{\\text{score estimation error}}.\\]\n' +
      '\n' +
      'For simplicity, we assume the optimal TCD, \\(f_{\\mathbf{\\theta}^{*}}\\equiv f\\) with a well-learned \\(\\mathbf{\\theta}^{*}\\), which recovers the true \\(f(\\cdot,t,s)\\) function. We establish that the density propagated by this optimal TCD model from any time \\(t\\) to a subsequent time \\(s\\) aligns with the predefined density determined by the fixed forward process.\n' +
      '\n' +
      'We now present the proposition ensuring the alignment of the transited density.\n' +
      '\n' +
      '**Proposition D.9**.: _Let \\(\\{p_{t}\\}_{t=0}^{T}\\) be densities defined by the diffusion process in Equation (1), where \\(p_{0}:=p_{data}\\). Denote \\(\\mathcal{T}_{t\\to s}(\\cdot):=f(\\cdot,t,s):\\mathbb{R}^{D}\\to\\mathbb{R}^{D}\\) for any \\(t\\geq s\\). Suppose that the score \\(\\nabla\\log p_{t}\\) satisfies that there is a function \\(L(t)\\geq 0\\) so that \\(\\int_{0}^{T}|L(t)|\\mathrm{d}t<\\infty\\) and_\n' +
      '\n' +
      '1. _Linear growth:_ \\(\\|\\nabla\\log p_{t}(\\mathbf{x})\\|_{2}\\leq L(t)(1+\\|\\mathbf{x}\\|_{2})\\)_, for all_ \\(\\mathbf{x}\\in\\mathbb{R}^{D}\\)__\n' +
      '2. _Lipschitz:_ \\(\\|\\nabla\\log p_{t}(\\mathbf{x})-\\nabla\\log p_{t}(\\mathbf{y})\\|_{2}\\leq L(t)\\| \\mathbf{x}-\\mathbf{y}\\|_{2}\\)_, for all_ \\(\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{D}\\)_._\n' +
      '\n' +
      '_Then for any \\(t\\in[0,T]\\) and \\(s\\in[0,t]\\), \\(p_{s}=\\mathcal{T}_{t\\to s}\\sharp p_{t}\\)._\n' +
      '\n' +
      'This theorem guarantees that by learning the optimal TCD, which possesses complete trajectory information, we can retrieve all true densities at any time using TCD.\n' +
      '\n' +
      'Define \\(\\mathcal{T}_{t\\to s}\\) as the oracle transition mapping from \\(t\\) to \\(s\\) via the diffusion process in Equation (1). Let \\(\\mathcal{T}_{t\\to s}^{\\boldsymbol{\\theta}^{*}}(\\cdot)\\) represent the transition mapping from the optimal TCD model, and \\(\\mathcal{T}_{t\\to s}^{\\boldsymbol{\\phi}}(\\cdot)\\) represent the transition mapping from the empirical probability flow ODE. Since all processes start at point \\(T\\) with an initial probability distribution \\(p_{T}\\) and \\(\\mathcal{T}_{t\\to s}^{\\boldsymbol{\\theta}^{*}}(\\cdot)=\\mathcal{T}_{t\\to s}^{ \\boldsymbol{\\phi}}(\\cdot)\\), Theorem D.8 and \\(\\mathcal{T}_{T\\to t}\\sharp p_{T}=p_{t}\\) from Proposition D.9 tell us that for \\(t>s\\)\n' +
      '\n' +
      '\\[\\text{TV}\\left(\\mathcal{T}_{t\\to s}\\sharp p_{t},\\mathcal{T}_{t \\to s}^{\\boldsymbol{\\theta}^{*}}\\sharp p_{t}\\right)=\\text{TV}\\left(\\mathcal{T }_{t\\to s}\\sharp p_{t},\\mathcal{T}_{t\\to s}^{\\boldsymbol{\\phi}}\\sharp p_{t} \\right)=\\mathcal{O}(\\sqrt{t-s}). \\tag{38}\\]\n' +
      '\n' +
      '\\[\\text{TV}\\left(\\mathcal{T}_{t\\to 0}\\mathcal{T}_{(1-\\gamma)t\\to t} \\mathcal{T}_{T\\to(1-\\gamma)t}\\sharp p_{T},\\mathcal{T}_{t\\to 0}^{ \\boldsymbol{\\theta}^{*}}\\mathcal{T}_{(1-\\gamma)t\\to t}\\mathcal{T}_{T \\to(1-\\gamma)t}^{\\boldsymbol{\\theta}^{*}}\\sharp p_{T}\\right)\\] \\[\\overset{(a)}{\\leq}\\] \\[+\\text{TV}\\left(\\mathcal{T}_{t\\to 0}^{\\boldsymbol{\\theta}^{*}} \\mathcal{T}_{(1-\\gamma)t\\to t}\\mathcal{T}_{T\\to(1-\\gamma)t}\\sharp p_{T}, \\mathcal{T}_{t\\to 0}^{\\boldsymbol{\\theta}^{*}}\\mathcal{T}_{(1-\\gamma)t\\to t} \\mathcal{T}_{T\\to(1-\\gamma)t}^{\\boldsymbol{\\theta}^{*}}\\sharp p_{T}\\right)\\] \\[\\overset{(b)}{=}\\] \\[\\overset{(c)}{=}\\] \\[\\overset{(d)}{=} \\mathcal{O}(\\sqrt{t})+\\mathcal{O}(\\sqrt{T-(1-\\gamma)t}).\\]\n' +
      '\n' +
      'Here (a) is obtained from the triangular inequality, (b) and (c) are due to \\(\\mathcal{T}_{(1-\\gamma)t\\to t}\\mathcal{T}_{T\\to(1-\\gamma)t}=\\mathcal{T}_{T \\to t}\\) and \\(\\mathcal{T}_{T\\to t}\\sharp p_{T}=p_{t}\\) from Proposition D.9, and (d) comes from Equation (38).\n' +
      '\n' +
      '## Appendix E Additional Results\n' +
      '\n' +
      '### More Samples with Different NFEs\n' +
      '\n' +
      'In Figure 8, we present additional samples synthesized by LCM and TCD with varying NFEs. These samples consistently demonstrate that the details of images generated by LCM tend to vanish as the NFEs increase, whereas TCD either maintains or enhances image details at higher NFEs.\n' +
      '\n' +
      '### More Samples with Different Stochastic Parameter \\(\\gamma\\)\n' +
      '\n' +
      'In Figure 9, we showcase additional samples generated by TCD with varying stochastic parameter \\(\\gamma\\), demonstrating that the visual complexity and quality of the image gradually improve as \\(\\gamma\\) increases.\n' +
      '\n' +
      '### More Comparisons Results\n' +
      '\n' +
      'In Figure 10 to Figure 14, we present additional examples for comparison with state-of-the-art methods. These samples illustrate that our TCD can generate high-quality images with few NFEs, outperforming LCM. Additionally, it can produce more detailed and high-definition images with sufficient NFEs, even surpassing the performance of DPM-Solver++(2S).\n' +
      '\n' +
      '### A.4 More samples from TCD\'s Versatility Testing\n' +
      '\n' +
      'To ascertain the versatility of TCD, we test it on a wide range of models, including the popular community model Animagine XL V3 6, LoRA Papercut XL 7, Depth ControlNet 8, Canny ControlNet 9, and IP-Adapter 10. In Figure 15 to Figure 19, we observe that TCD can be directly applied to all these models, accelerating their sampling with only _4 steps_. It is worth noting that, in this experiment, all models share _the same TCD LoRA parameters_.\n' +
      '\n' +
      'Footnote 6: The checkpoint of _Animagine_ can be found in [https://civitai.com/models/260267/animagine-xl-v3](https://civitai.com/models/260267/animagine-xl-v3)\n' +
      '\n' +
      'Footnote 7: The checkpoint of _Papercut_ can be found in [https://civitai.com/models/122567/papercut-sdxl](https://civitai.com/models/122567/papercut-sdxl)\n' +
      '\n' +
      'Footnote 8: _Depth ControlNet:_[https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0](https://huggingface.co/diffusers/controlnet-depth-sdxl-1.0)\n' +
      '\n' +
      'Footnote 9: _Canny ControlNet:_[https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0](https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0)\n' +
      '\n' +
      'Footnote 10: _IP-Adapter:_[https://github.com/tencent-ailab/IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)\n' +
      'Figure 8: More samples with different NFEs.\n' +
      '\n' +
      'Figure 9: More samples with different stochastic parameter \\(\\gamma\\).\n' +
      '\n' +
      'Figure 10: More comparisons results.\n' +
      '\n' +
      'Figure 11: More comparisons results.\n' +
      '\n' +
      'Figure 12: More comparisons results.\n' +
      '\n' +
      'Figure 13: More comparisons results.\n' +
      '\n' +
      'Figure 14: More comparisons results.\n' +
      '\n' +
      'Figure 15: Qualitative results of TCD using different base models: SDXL and Animagine XL V3. It is worth noting that we employed _the same TCD parameters_ for both models. All samples were generated using _4 steps_. In each subfigure, the top row corresponds to TCD + SDXL, and the bottom row corresponds to TCD + Animagine XL V3.\n' +
      '\n' +
      'Figure 16: Qualitative results of TCD with and without Papercut XL LoRA. We used _the same TCD parameters_. All samples are generated using _4 steps_. In each subfigure, the top row corresponds to TCD without Papercut LoRA, and the bottom row corresponds to TCD with Papercut LoRA. The Lora scale of Papercut was set to 1.0 in the experiments.\n' +
      '\n' +
      'Figure 19: Qualitative results of TCD with IP-Adapter. All samples are generated using _4 steps_.\n' +
      '\n' +
      'Figure 17: Qualitative results of TCD with Depth ControlNet. All samples are generated using _4 steps_.\n' +
      '\n' +
      'Figure 18: Qualitative results of TCD with Canny ControlNet. All samples are generated using _4 steps_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>