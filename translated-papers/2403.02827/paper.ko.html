<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '고충실도 영상-비디오 생성을 위한 # 튜닝-프리 잡음 보정\n' +
      '\n' +
      '위제리, 리퉁공, 이란주, 판다판, 비아오왕, 티정게, 보정\n' +
      '\n' +
      '알리마마테크 알리바바그룹\n' +
      '\n' +
      'Beijing, China\n' +
      '\n' +
      '{weijie.lwj0, gonglitong.glt, yizhu.zyr, fanda.ffd}\n' +
      '\n' +
      'eric.wb, tiezhenggtz, bozheng}@alibaba-inc.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'I2V(Image-to-video) 생성 작업은 항상 열린 도메인에서 높은 충실도를 유지하는 데 어려움을 겪는다. 전통적인 이미지 애니메이션 기법은 주로 얼굴이나 사람의 포즈와 같은 특정 도메인에 초점을 맞추고 있어 오픈 도메인으로 일반화하기 어렵다. 확산 모델을 기반으로 한 최근 I2V 프레임워크는 오픈 도메인 이미지에 대한 동적 콘텐츠를 생성할 수 있지만 충실도를 유지하지 못한다. 낮은 충실도의 두 가지 주요 요인은 노이즈 제거 과정에서 이미지 세부 정보의 손실과 노이즈 예측 편향임을 발견했다. 이를 위해 주류 동영상 확산 모델에 적용할 수 있는 효과적인 방법을 제안한다. 이 방법은 보다 정밀한 영상 정보와 잡음 정류를 보완하여 높은 충실도를 달성한다. 구체적으로, 지정된 영상이 주어졌을 때, 제안하는 방법은 먼저 입력 영상에 더 많은 세부 사항을 유지하기 위해 잠재된 잡음을 추가하고, 잡음 예측 편향을 완화하기 위해 적절한 정류와 함께 잠재된 잡음을 제거한다. 우리의 방법은 튜닝이 없고 플러그 앤 플레이입니다. 실험 결과는 생성된 비디오의 충실도를 향상시키는 데 우리의 접근법의 효과를 보여준다. 더 많은 이미지 대 비디오 생성 결과를 보려면 프로젝트 웹사이트 [https://noise-rectification.github.io/](https://noise-rectification.github.io/]를 참조하십시오.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '정교한 이미지 생성[7, 36, 37, 39, 40]에서 확산 모델의 놀라운 돌파구로 연구자들은 보다 일관된 비디오 생성을 달성하기 위해 확산 모델의 추가 잠재력을 탐색하고 있다. 일부 최근 작업[15, 16, 43, 49, 51]은 텍스트-투-비디오(T2V) 태스크에서 입력 텍스트에 정렬되는 비디오를 생성하기 위해 점진적인 진전을 이루었다. 그러나, 텍스트 설명은 다양한 상상 가능한 비디오에 대응할 수 있으며, 이는 반드시 사람들의 특정 기대를 충족시키지 않을 수 있다. 따라서, 기준 이미지는 주어진 이미지와 밀접하게 정렬되거나 심지어 정지 이미지로부터 엄격하게 시작하는 비디오를 생성하는 것을 목표로 비디오 생성 프로세스를 안내하기 위해 제안되며, 이는 이미지-투-비디오(I2V) 작업이라고 불린다.\n' +
      '\n' +
      '이미지 대 비디오의 개념은 참신하지 않고 얼굴 애니메이션[42, 50], 신체 모션 합성[33], 자연 구동 애니메이션[17, 28], 비디오 예측[9, 18, 24]과 같은 전통적인 컴퓨터 비전 작업에서 오랫동안 존재해 왔으며, 이는 모두 I2V 작업으로 간주될 수 있다. 그러나, 이러한 작업들은 특정 도메인들(얼굴들, 인간 포즈들, 및 단순한 자연 장면들과 같은)로 제한되거나 비교적 간단한 시나리오들(애니메이션 폰트들[9], 드로잉[44] 또는 움직이는 강성 객체들[19])에 초점을 맞추었다. 이러한 특정 작업에 대한 제안된 솔루션은 오픈 도메인 이미지에 적용하기 어렵다. 또한, 이전 연구 [18, 19, 23, 28, 48, 53]은 비디오 시퀀스를 생성하기 위해 자기회귀 접근법을 채택했으며, 이는 계산 비용이 많이 들고 복잡한 오픈 도메인 시나리오에서 여전히 어려움에 직면해 있다. 최근에 등장한 확산 모델은 노이즈로부터 데이터 분포를 학습함으로써 강력한 생성 능력과 상당한 확장성을 입증했다. 비디오는 상관도가 높은 이미지의 시간적 시퀀스(배치)로 간주될 수 있기 때문에 확산 모델을 사용하여 비디오를 배치로 처리하는 것이 가능하다. 결과적으로, 이미지 대 비디오 작업에 대한 확산 모델을 활용하는 데 초점을 맞추고 있으며, 연구와 산업 모두에서 상당한 관심을 받고 있다.\n' +
      '\n' +
      '그러나, 현재의 I2V 연구 [4, 11, 51, 59]는 주로 비디오 확산 모델을 안내하기 위해 이미지 신호의 감독을 강화하는 것에 의존한다. 결과적으로, 생성된 비디오들은 주어진 이미지와 유사할 수 있을 뿐이다. 우리가 보기에, 이러한 작업에서 기존의 비디오 확산 모델은 이미 동적 모션 생성을 위한 강력한 기능을 발휘하지만, 충실도를 유지하는데 어려움을 겪고 있는데, 이는 두 가지 주요 요인에 기인할 수 있다. 하나는 IP-어댑터 [54] 또는 ControlNet [58]을 채택하여 부분 이미지 표현만 추출하는 것과 같은 이미지 세부 정보의 손실이다. 다른 하나는 전체 이미지 정보가 주입되거나 연결된 경우에도 비디오 확산 모델을 훈련하는 데 있어 달성할 수 없는 완벽한 제로 손실로 인한 잡음 예측 편향이다. 최근 영상 편집 작업에서 중추적인 잡음 벡터를 이용한 전이 정제에서 영감을 얻어 잡음 제거 과정에서 초기 잡음의 방향을 중추적인 기준으로 설정하는 것을 제안한다[6, 29, 30]. 구체적으로, 생성된 비디오의 충실도를 향상시키기 위해 "잡음 및 정류 잡음 제거" 역 프로세스를 채택하는 이미지-비디오 파이프라인을 설계한다. 제안하는 방법은 프레임 간의 유창한 움직임을 생성하기 위해 미리 학습된 비디오 잠재 확산 모델(VLDM)을 이용한다. 추론하는 동안 우리는 먼저 주어진 이미지에 초기 노이즈를 추가하여 "잡음" 단계로 표시되는 세부 정보의 손실을 완화한다. 그리고 잡음 예측 편향을 완화하기 위해 역 잡음 타임스테프에서 중심 참조 잡음을 이용하여 예측 잡음을 적절히 보정하여 "정정 잡음 제거" 단계로 나타낸다. 또한, 참조 영상의 유지 정도를 제어하기 위해 노이즈 정정에 기반한 실용적인 단계 적응 중재 전략을 추가로 소개한다.\n' +
      '\n' +
      '일반적으로, 우리는 이미지 대 비디오 작업에 대해 기존의 사전 훈련된 비디오 확산 모델을 활용하는 효과적인 방법을 제안한다. 현재 공개 I2V 작업과 활성 커뮤니티에서 여러 I2V 시도와의 비교 실험은 더 높은 충실도를 가진 비디오를 생성하는 데 우리의 방법의 효과를 입증했다. 또한, 본 논문에서 제안하는 방법은 별도의 훈련이 필요 없고 구현이 간단하여 현재 미리 훈련된 오픈 도메인 비디오 확산 모델과 플러그 앤 플레이 방식으로 원활하게 통합되어 오픈 도메인에서 높은 충실도 I2V 생성이 가능하다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '확산 모델은 최근 몇 년 동안 생성 작업에서 큰 성공을 거두었다. 이미지와 비디오 양식 사이의 높은 상관관계로 인해, 현재 비디오 생성 작업의 많은 아이디어와 통찰력은 광범위한 이미지 생성 작업에서 영감을 받았다. 이에 본 논문에서는 영상 및 영상 생성에 관한 관련 연구를 소개한다.\n' +
      '\n' +
      '확산 모델을 이용한### 영상 생성\n' +
      '\n' +
      '기존의 GAN[10] 및 VAE[22] 기반 방법에 비해 확산 모델[40, 45, 46, 47, 36, 44, 45, 46, 47]은 사실적인 질감과 미세한 디테일을 가진 고품질 이미지를 생성하는 데 더 강력한 기능을 보여주었다. 어텐션 레이어를 갖는 U-Net[38]은 잡음을 예측하기 위해 이미지 확산 모델에서 널리 채택되는 구조이다. 계산 비용을 절약하기 위해, 안정 확산(Stable Diffusion, SD)[37]은 VAE[22]를 활용하여 이미지를 잠재 공간으로 인코딩하고 잠재 공간에 대한 확산 프로세스를 수행하는 잠재 확산 모델(L latent diffusion model, LDM)을 제안하였다. 제어성을 높이고 깊이, 기준 영상, 정상 지도, 캐니 지도 등 다양한 제어 조건을 지원하기 위해 ControlNet[58]과 T2I-Adapter[31]은 제어 생성을 위한 SD[37] 기반의 유연한 어댑터를 도입하였다. 최근에는 IP-Adapter[54]에서도 T2I 모델들을 위한 영상 프롬프트 어댑터를 제안하여 기준 영상으로 영상 생성을 안내하고 있다. 또한, SDEdit[29]는 입력 스트로크 영상에 노이즈를 추가하고 결과 영상을 점진적으로 잡음 제거하여 합성 영상의 사실성을 높였다. 이미지-투-비디오 작업에서, 우리는 T2I 작업과 유사한 팽창된 3D U-Net을 채택하고, 노이즈 보정은 또한 이미지 편집 작업에서 전환 개선으로부터 영감을 얻는다[6, 29, 30].\n' +
      '\n' +
      '확산 모델을 이용한### 비디오 생성\n' +
      '\n' +
      '텍스트-이미지 생성의 상당한 진전에 힘입어 비디오 생성도 텍스트-비디오(T2V) 작업에서 발전하기 시작했다. VDM[16]은 2D U-Net을 3D U-Net 구조로 확장한 선구적인 비디오 확산 모델을 도입하여 픽셀 공간의 이미지와 비디오 모두를 공동으로 훈련시켰다. 이후의 방법[1, 3, 12, 26, 49, 52, 57, 60]은 대부분 기억 요구량을 줄이고 훈련과 추론의 속도를 높이기 위해 잠재 공간을 채택했다. 비디오 생성에 필요한 실행 시간을 최적화하기 위해 대부분의 작업(Make-A-Video[43], ModelScopeT2V[49], Latent-Shift[1], AnimateDiff[12])을 미리 훈련된 T2I 모델에 구축하고 시간 모듈을 통합하여 모든 비디오 프레임을 동시에 배치 생성할 수 있다. 특히, AnimateDiff[12]는 다양한 개인화된 T2I 모델에 적응할 수 있는 모션 모듈만을 훈련시킨다. Text2Video-Zero[21]은 움직임 동역학을 풍부하게 하고 프레임 간 주의와 함께 시간적 일관성을 유지하기 위해 훈련 없는 샘플링 방법을 제안했다. 또한, 비디오 확산 모델의 캐스케이드 프레임워크는 고해상도 [3, 15, 52] 및 더 긴 비디오 [13, 56]를 생성하는 데에도 사용된다.\n' +
      '\n' +
      '영상 생성과 마찬가지로 영상 생성에 더 많은 제어 조건을 도입하는 것도 중요하다. 최근, 생성된 비디오들을 보다 제어가능하게 하기 위해, 최근의 작업은 깊이[8, 43], 포즈[20, 27], 궤적으로부터의 유도 동작[55], 스트로크 페인팅[5] 또는 주파수 분석[25]을 포함하는 다양한 조건들을 비디오 확산 모델들에 도입하였다. 이미지 조건과 관련하여, 기존의 비디오 생성 작업은 주로 이미지 생성 필드로부터의 경험들, 예를 들어, ControlNet[58] 및 IP-Adapter[54]를 사용하여 이미지 안내를 강화한다. 또한, Seer[11]은 3D U-Net의 시간 모듈(temporal module)에서 잡음 잠재(noise latent)에 잠재된 조건부 이미지를 연결하였으며, 이미지 대 비디오 작업을 위해 인과적 주의를 사용하였다. 비디오 합성기[51]는 특징 채널을 따라 잠재된 잡음과 이미지 임베딩을 연결시키고, 주어진 이미지의 스타일을 비디오 잠재 확산 모델(VLDM)로 포워딩하는 것을 지원하기 위해 제안되었다. 최근 VideoCrafter[4]는 영상 대 영상 작업을 위해 VLDM에 영상 특징을 추출하였다. 마찬가지로 I2VGen-XL[59]은 모두 입력 계층에 잠재된 잡음이 있는 이미지를 추가하고 글로벌 인코더를 구축하여 이미지 CLIP 특징을 VLDM에 추출했다. 그러나 이러한 영상-비디오 작업은 충실도가 제한적이거나 전체 VLDM을 미세 조정해야 한다. 이에 비해, 잡음 보정 방법은 튜닝이 불필요하며 높은 충실도를 유지한다.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      '### 이미지 대 비디오 작업 정의\n' +
      '\n' +
      '모든 비디오 생성 작업은 시각적 일관성과 논리적 움직임을 모두 유지하는 일관성 있는 프레임을 생성해야 한다. 구체적으로, I2V(Image-to-video) 태스크는 지정된 기준 이미지로부터 비디오를 생성하는 것으로 정의된다. 그 목표는 이미지의 정적 특성을 동적 시각적 표현으로 변환하여 콘텐츠에 움직임과 유동성을 추가하는 것이다. 텍스트-투-비디오(T2V) 태스크와 비교하여, I2V는 조건부 이미지와의 높은 충실도를 우선시하는 반면, 비디오 내의 동적 모션은 공통 사전 지식을 통해 학습되거나 텍스트 기술 또는 다른 데이터 형태와 같은 주어진 조건에 의해 구동될 수 있다. 본 논문에서는 텍스트-조건 이미지-비디오 태스크에 초점을 맞추고, 이 정의는 정지 이미지\\(I\\)와 텍스트 기술\\(c\\)이 주어지면 생성 시스템은 예측된 비디오\\(V^{0:L-1}=\\left\\{\\bar{I}^{0},\\dots,\\bar{I}^{L-1}\\right\\})을 출력하며, 여기서 \\(L\\)은 비디오 길이를 나타낸다. 목적은 주어진 초기 이미지\\(I\\)와 일치하는 외형을 유지하고 생성된 비디오가 텍스트 설명\\(c\\)과 일치하는지 확인하는 것이다.\n' +
      '\n' +
      '### 비디오 잠재 확산 모델\n' +
      '\n' +
      '확산 모델[45, 46, 47, 14]은 비평형 열역학에서 영감을 얻은 생성 모델의 한 종류로, 확산 과정에서 데이터를 노이즈로 교란시킨 다음 역 과정에서 노이즈를 데이터로 다시 변환하는 방법을 배운다. 공식적으로, 확산 과정에서 데이터 분포 \\(z_{0}\\sim q(z_{0})\\)이 주어지면, 순방향 마르코프 체인은 \\(T\\) 타임스테프에서 데이터 샘플에 점차적으로 가우시안 잡음을 추가하므로 전이 공식에 따라 \\(z_{0}\\)에 조건화된 잡음 데이터 \\(\\left\\{z_{1},z_{2},\\dots,z_{T}\\right\\}\\)의 시퀀스를 얻을 수 있으며, 이는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '[q(z_{1:T}|z_{0})=\\prod_{t=1}^{T}q(z_{t}|z_{t-1}), \\tag{1}\\] \\[q(z_{t}|z_{t-1})=\\mathcal{N}(z_{t};\\sqrt{\\alpha_{t}}z_{t-1}, (1-\\alpha_{t})\\mathbf{I}), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\left\\{\\alpha_{t}\\in\\left(0,1\\right)\\right\\}_{t=1}^{T}\\)는 스텝 크기를 제어하기 위한 분산 스케쥴이다. 역과정에서는 잡음의 사전(z_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})으로부터 모델\\(p_{\\theta}\\)을 학습하여 원하는 데이터를 점진적으로 반복해서 생성한다:\n' +
      '\n' +
      '\\[p_{\\theta}(z_{t-1}|z_{t})=\\mathcal{N}(z_{t-1};\\mu_{\\theta}(z_{t},t),\\Sigma_{\\theta}(z_{t},t)), \\tag{3}\\mathcal{N}(z_{t-1};\\mu_{\\theta}(z_{t},t))\n' +
      '\n' +
      '여기서 \\(\\theta\\)는 모델 매개변수이며 \\(\\mu_{\\theta}(z_{t},t)\\) 및 \\(\\Sigma_{\\theta}(z_{t},t)\\)는 모델에 의한 예측된 평균 및 분산을 나타낸다.\n' +
      '\n' +
      '이미지 생성 작업에서 노이즈 제거 모델은 일반적으로 U-Net 네트워크 아키텍처로 설계되고 목적 함수로 학습된다.\n' +
      '\n' +
      '\\[\\min_{\\theta}\\mathbb{E}_{z_{0}\\sim p_{data},t,e\\sim\\mathcal{N}(\\mathbf{0}, \\mathbf{I}}[\\left\\|\\epsilon-\\epsilon_{\\theta}(z_{t},c,t)\\right\\|_{2}^{2}], \\tag{4}\\}\n' +
      '\n' +
      '여기서 \\(\\epsilon\\) 및 \\(\\epsilon_{\\theta}\\)은 각각 실제 및 예측된 잡음이고, \\(c\\)은 텍스트, 이미지 또는 다른 제어 신호와 같은 다양한 조건을 나타낸다. 또한, 계산 복잡도를 줄이기 위해, 확산 모델들은 픽셀 공간이 아닌 저차원 잠재 공간에서 활용되며, 이는 잠재 확산 모델(37)로 표시된다.\n' +
      '\n' +
      '이미지 확산 생성과 유사하게, 비디오 확산 생성은 이미지들의 배치를 함께 다루는 것으로 간주될 수 있다(도 1 참조). 최근 비디오 잠재 확산 모델(VLDM)도 텍스트-이미지 생성 시 개발되었으며, 가우시안 잡음으로부터 비디오 데이터를 모델링하는 것을 목표로 앞서 언급한 확산 과정을 따랐다. 형식적으로, 주어진 비디오 데이터 \\(V^{0:L-1}\\in\\mathbb{R}^{L\\times 3\\times H\\times W\\)는 VAE 인코더[22]를 통해 잠재 표현 \\(z_{0}^{0:L-1}\\in\\mathbb{R}^{L\\times C\\times H^{{}^{prime}}\\times W^{{}^{prime}}\\)으로 변환될 것이며, 여기서 \\(C\\)은 특징 채널의 수이다. 게다가, 비디오에서의 시간적 일관성 및 콘텐츠 관련성 요건들로 인해, VLDM은 종종 추가적인 시간적 모듈 [1, 12, 16, 49]을 수반하므로, infl\n' +
      '\n' +
      '그림 1: 부풀려진 3D U-Net 구조를 가진 이미지 확산 모델과 비디오 확산 모델의 일반적인 프레임워크.\n' +
      '\n' +
      '그림 2: 이미지 대 비디오 생성에 관한 기존 연구 및 커뮤니티의 두 가지 기본 접근법.\n' +
      '\n' +
      '모델은 2D U-Net에서 3D U-Net까지입니다. 확산과정 \\(z_{t}^{0:L-1}=q(z_{0}^{0:L-1},t)\\)과 역과정 \\(z_{t-1}^{0:L-1}=p_{\\theta}(z_{t}^{0:L-1},t)\\)을 통해 최종적으로 잡음 제거된 비디오 잠재 \\(z_{0}^{0:L-1}\\)을 VAE 디코더를 통해 처리하여 비디오를 생성한다.\n' +
      '\n' +
      '주류 텍스트-비디오 프레임워크에서 영감을 받아 정지 영상으로부터 비디오를 생성하기 위해, 우리는 또한 부풀려진 3D U-Net[12]에서 시간적인 관심을 가지고 비디오 움직임을 모델링한다. 도 1에 도시된 바와 같다. 도 1(b)를 참조하면, 계산 효율을 향상시키기 위해, 비디오의 프레임 차원은 공간 모듈들의 전방에서 배치 축으로 취급되고, 비디오의 공간 차원은 시간 모듈들의 전방에서 배치 축으로 취급된다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '### 향상된 이미지 상태 분석\n' +
      '\n' +
      '텍스트-대-비디오 프레임워크는 비교적 일관된 움직임을 갖는 비디오 클립을 생성할 수 있지만, 생성된 비디오의 의미적 콘텐츠는 주로 거친-그레인드 레벨에서 주어진 텍스트 설명과 정렬된다. 생성된 영상과 기준 영상 사이의 콘텐츠 일관성을 제어하기 위해 기존 연구에서 주류 I2V가 작동하고 커뮤니티는 두 가지 기본 유형으로 요약될 수 있다(도 2 참조): 하나는 역과정의 초기에 이미지 조건을 통합하는 것이다. 이 접근법은 이미지 편집 작업[6, 29, 30]과 같은 img2img 작업과 같은 이미지 생성 분야에서 주로 영감을 받으며, 이 작업은 초기 노이즈 잠재에 잠재된 이미지를 주입하는 것이다. 이러한 방식으로, 역 잡음 제거 프로세스는 잠재 공간에 잠재된 이미지의 방향을 향해 암묵적으로 안내될 수 있다. 그러나, 이러한 접근법은 주어진 이미지에 대한 유사성만을 달성할 수 있고 여전히 높은 충실도에 대한 일정한 갭이 존재한다. 다른 방법은 더 미세한 세부사항들을 도입하기 위해 전체 깨끗한 이미지를 초기 노이즈와 연결하는 것을 포함한다[11, 51, 59]. 이 접근 방식은 충실도를 향상시키지만 전체 생성 프레임워크를 재교육해야 하므로 확장성이 낮고 문제가 발생할 수 있습니다.\n' +
      '\n' +
      '그림 3: 튜닝 프리 이미지 대 비디오 방법의 프레임워크입니다. (a)는 추론 파이프라인을 나타내며, 여기서 입력 이미지는 초기 잠재로 노이즈화되고 팽창된 3D U-Net의 예측된 노이즈는 노이즈 제거 프로세스 동안 보정될 것이다. (b)는 우리의 방법의 상세한 생성 과정을 예시한다. 중간 단계의 시각화는 우리의 방법이 잡음 제거 방향을 효과적으로 정제하여 중간 결과를 주어진 이미지에 더 가깝게 만들 수 있음을 보여준다.\n' +
      '\n' +
      'ControlNet[58]과 같은 기존 사전 훈련된 모듈과 통합합니다. 이미지 충실도를 향상시키기 위한 또 다른 방법은 다양한 제어넷[58] 및 IP-어댑터[54]를 사용하는 것과 같이, 각각의 타임스텝[4]에서 확산 모델의 내부 계산에 더 많은 이미지 특징 신호 및 조건을 도입한다. 이미지 기능은 충실도를 향상시키기 위한 강력한 감독 역할을 합니다. 그러나, 특징 추출은 필연적으로 이미지 디테일을 잃기 때문에, 이러한 접근법들은 원래의 이미지로부터 전체적인 스타일 또는 일반적인 레이아웃을 학습하는 경향이 있어서, 미세한 디테일의 관점에서 높은 충실도를 달성하기 어렵다.\n' +
      '\n' +
      '위의 모든 방법들은 충실도를 향상시키기 위해 비디오 생성에서 초기 이미지의 안내 및 제어를 향상시키는 것을 목표로 한다. 그러나, 도 3(b)에 도시된 바와 같이, 노이즈 제거 과정(회색 화살표로 표현됨)은 주어진 이미지에 노이즈를 추가하여 초기 노이즈 래턴트를 얻어도 주어진 이미지를 복원할 수 없으며(파란색 화살표로 표현됨), 이러한 방법 [4, 11, 51, 59]이 완벽한 충실도를 달성하지 못하는 이유는 노이즈 제거 과정 동안 누적된 노이즈 바이어스에 있으며, 생성된 프레임 래턴트가 잠재된 주어진 이미지에서 벗어나게 한다고 분석했다. 훈련 과정에서 예측 잡음을 초기 입력 잡음에 가깝게 만들기 위해 MSE 손실 함수를 활용하지만, 훈련 과정은 0의 완벽한 손실을 완전히 달성할 수 없다. 따라서, 예측 잡음과 실제 잡음 사이에는 항상 불일치가 있을 것이다. 충실도를 더욱 향상시키기 위해 잠재된 노이즈에서 영감을 도출하고 노이즈 제거 과정에서 노이즈 격차를 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '### 잡음정류 전략\n' +
      '\n' +
      '우리의 방법은 "잡음 및 교정 잡음 제거" 프로세스를 포함한다. [29]와 유사하게, 우리의 접근법은 초기 잡음에 잠재된 이미지를 주입하는 것으로 시작한다. 추가적인 조작을 도입하지 않고, 그러한 설정은 전체 스타일 및 레이아웃에서 주어진 이미지와 유사한 일관된 비디오를 생성할 수 있다. 다른 관점을 취하면, 잡음 제거 프로세스가 각 타임스텝에서 예측된 편향된 잡음보다는 알려진 초기 잡음을 채택한다면, 그것은 전적으로 충실하지만 모션 또는 역학도 결여된 비디오 시퀀스를 초래할 것이다. 따라서, 완전한 충실도와 동역학 사이의 균형을 맞추기 위해, 우리는 잡음 정류 방법을 제안한다. 우리의 추론 과정의 파이프라인은 그림 3(a)에 나와 있으며, 노이즈 제거 과정의 일부 중간 단계에서 알려진 초기 노이즈로 적응적으로 보상하여 예측된 노이즈를 보정하며, 이는 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\[\\widetilde{n}_{t}^{0:L-1}=Rectify(\\widetilde{n}_{t}^{0:L-1},n^{0:L-1},t,\\omega^{0:L-1},\\tau), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\widetilde{n}_{t}^{0:L-1}\\)은 \\(t^{th}\\) 타임스테프에서의 정류된 잡음을 나타내고, \\(\\widetilde{n}_{t}^{0:L-1}\\)은 3D-UNet의 예측된 잡음을 나타내고, \\(n^{0:L-1}\\)은 주어진 영상에 부가되는 초기 샘플링된 잡음을 나타내고, \\(\\omega^{0:L-1}\\) 및 \\(\\tau\\)은 정류 가중치와 타임스테프 주기를 나타낸다.\n' +
      '\n' +
      '```\n' +
      '0: 주어진 이미지 잠재\\(z^{0}\\), 선택적 텍스트 임베딩\\(c\\), 비디오 길이\\(L\\), 정류 가중치\\(\\omega^{0:L-1}\\) 및 타임스텝 주기\\(\\tau\\)\n' +
      '0: 생성된 동영상 잠재 \\(z^{0:L-1}_{0}\\)\n' +
      '1:\\(n^{0:L-1}\\sim\\mathcal{N}(0,\\mathbf{I})\\)\n' +
      '2:\\(z^{0:L-1}_{T}\\gets AddNoise(Repeat(z^{0}),n^{0:L-1},T)\\)\n' +
      '3:for\\(t=T,\\dots,1\\)do\n' +
      '4: 예측 잡음 \\(\\widetilde{n}_{t}^{0:L-1}=\\epsilon_{\\theta}(z^{0:L-1}_{t},c,t)\\)\n' +
      '5: Compute Noise gap \\(\\Delta_{t}^{0:L-1}=n^{0:L-1}-\\widetilde{n}_{t}^{0:L-1}\\)\n' +
      '6:if\\(t\\ in \\(\\tau\\)then\n' +
      '7: 정류 \\(\\widetilde{n}_{t}^{0:L-1}=\\widetilde{n}_{t}^{0:L-1}+\\omega^{0:L-1}\\cdot Repeat (\\Delta_{t}^{0})\\)\n' +
      '8:else\n' +
      '9:\\(\\widetilde{n}_{t}^{0:L-1}=\\widetilde{n}_{t}^{0:L-1}\\)\n' +
      '10:endif\n' +
      '11:\\(z^{0:L-1}_{t-1}\\gets Sample(z^{0:L-1}_{t},\\widetilde{n}_{t}^{0:L-1})\\)\n' +
      '12:endfor\n' +
      '13:return\\(z^{0:L-1}_{0}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 영상 대 비디오의 잡음 보정\n' +
      '\n' +
      '구체적으로, 우리의 잡음 보정 전략에서, 각 단계 \\(t\\)에서 U-Net이 예측한 잡음 \\(\\widetilde{n}_{t}^{0:L-1}\\)을 먼저 구한다:\n' +
      '\n' +
      '\\[\\widetilde{n}_{t}^{0:L-1}=\\epsilon_{\\theta}(z^{0:L-1}_{t},c,t), \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(z^{0:L-1}_{t}\\)는 단계 \\(t\\)에서의 입력 잠재 맵이고 \\(\\epsilon_{\\theta}(\\cdot)\\)는 데노이즈 모델(팽창된 3D U-Net)이다. \\(z^{0:L-1}_{t}\\)은 단계 \\(t\\)에서의 입력 잠재 맵이고 \\(\\epsilon_{\\theta}(\\cdot)\\)은 팽창된 3D U-Net이다. (c\\) 및 \\(L\\)은 각각 텍스트 임베딩 및 비디오 길이이다. 그런 다음 잡음 제거 과정에서 초기 샘플링된 잡음과 잡음 제거 과정에서 예측된 잡음 사이의 잡음 간격(dubbed \\(\\Delta_{t}^{0:L-1}\\))을 자연스럽게 계산할 수 있다.\n' +
      '\n' +
      '\\[\\Delta_{t}^{0:L-1}=n^{0:L-1}-\\widetilde{n}_{t}^{0:L-1}. \\tag{7}\\]\n' +
      '\n' +
      '우리는 우리의 방법의 핵심 절차인 예측된 편향된 잡음을 추가로 보정한다. 정류 가중치 인자\\(\\omega^{0:L-1}\\)를 도입하여 첫 번째 프레임 잡음 갭과 다음 프레임 잡음 갭의 균형을 맞추어 가중 정류 오프셋을 구하고, 이를 이용하여 원래 예측한 잡음을 프레임 단위로 갱신한다.\n' +
      '\n' +
      '[\\begin{split}\\widetilde{n}_{t}^{0:L-1}&=\\widetilde{n}_{t}^{0:L-1}+\\omega^{0:L-1}\\cdot Repeat(\\Delta_{t}^{0:L-1})\\cdot\\Delta_{t}^{0:L-1},\\end{split}\\tag{8}\\cdot Repeat(\\Delta_{t}^{0:L-1})\\cdot\\Delta_{t}^{0:L-1},\\end{split}\\tag{8}\\cdot Repeat(\\Delta_{t}^{0:L-1}+\\omega^{0:L-1})\\cdot\\Delta_{t}^{0:L-1},\\end{split}\\tag{8}\\cdot Repeat(\\Delta_{t}^{0:L-\n' +
      '\n' +
      '여기서 \\(Repeat(\\cdot)\\)는 시간적 차원을 정렬하기 위한 브로드캐스팅 동작이다.\n' +
      '\n' +
      '본 논문에서 제안하는 영상-영상 방법의 전체 과정은 알고리즘 1에 자세히 설명되어 있으며, 이러한 잡음 보정 방법은 간단하지만 효과적이다. 그림 3(b)에 예시된 바와 같이, 잡음 정류(녹색 화살표로 표현됨)를 통해, 축적 잡음 갭이 효과적으로 완화될 수 있었고, 따라서 생성된 프레임들의 잡음 잠재는 이미지 잠재에 더 가깝다. 이러한 방식으로, 기준 이미지의 세밀한 콘텐츠 세부사항은 생성된 비디오에서 잘 보존될 수 있다. 또한, 참조 영상의 유지 정도를 제어하기 위해 잡음 정정에 기반한 단계적 적응 중재 전략을 추가로 소개한다. 구체적으로, 정류 단계 \\(\\tau\\)와 가중치 \\(\\omega^{0:L-1}\\)의 매개변수를 조절함으로써 생성된 영상의 충실도를 조절할 수 있었다. 우리의 방법은 튜닝이 없으며 대부분의 현재 비디오 확산 모델에 적용될 수 있다는 점을 언급할 가치가 있다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Dataset.** 우리는 우리의 방법을 평가하기 위해 [41]에서 Web-Vid10M [2]와 LAION-Aesthetic 두 개의 공개 데이터 세트를 활용했다. WebVid10M 데이터 세트의 경우 검증 하위 집합의 다른 범주에 비례하여 1000개의 비디오 텍스트 쌍을 무작위로 샘플링했다. 정량적 평가를 위해 동영상 초반의 버퍼링 프레임을 피하기 위해 동영상의 텍스트 설명과 함께 동영상 생성을 위한 이미지 입력으로 10번째 프레임을 선택했다. LAION-Aesthetic 데이터셋은 정성적 평가를 위해 1000개의 이미지-텍스트 쌍을 무작위로 선택했다.\n' +
      '\n' +
      '**평가 메트릭.** 이미지-투-비디오 생성에 대한\n' +
      '\n' +
      '도 4: 현재 이미지 대 비디오 방법들과의 시각적 비교. 우리는 VLDM으로 AnimateDiff[12]를 사용한다. “Ctrl.R.O.”는 ControlNet Reference-Only 방법[58]을 의미한다. 제안된 방법은 주어진 영상을 이용하여 비디오 시퀀스에서 보다 높은 충실도를 얻을 수 있다.\n' +
      '\n' +
      '작업, 초점은 생성된 비디오의 충실도와 부드러움에 있습니다. 따라서, 생성된 비디오를 이미지 충실도, 시간적 일관성 및 비디오-텍스트 정렬의 세 가지 측면에서 평가한다. 구체적으로, 생성된 영상과 주어진 영상 간의 충실도를 평가하기 위해 생성된 프레임 및 주어진 영상 각각에 대한 CLIP[35] 영상 유사도를 계산한다. 비디오의 시간적 일관성을 고려하여 생성된 프레임 간의 CLIP 점수를 평가한다. 또한 텍스트 기술이 조건으로서 입력되기 때문에 생성된 비디오와 텍스트 기술 간의 의미적 관련성을 평가하기 위해 CLIP 텍스트 이미지 유사도를 계산한다.\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '**비교 방법.** 비교 방법을 그림 2와 같이 이 두 가지 유형으로 분류했다. 하나는 이미지 조건을 입력 레이어에 통합하는 것이다. (1) 의미적 이미지 변환 방법인 _SDEdit_[29]는 주어진 이미지에 단순히 잡음을 추가한 후 잡음 제거만 하면 I2V 작업에도 사용할 수 있다. (2) _ConcateImage_, 초기화 노이즈에 이미지 조건을 연결하기 위한 또 다른 간단한 베이스라인으로서, 주어진 이미지의 구조적 정보를 학습하기 위해 미세 조정될 필요가 있다. 또 다른 유형의 접근법은 VLDM의 각 층에서 이미지 조건 주입을 수행하는 것이다. (3) _ControlNet Reference-Only_[58], VLDM의 어텐션 레이어를 레퍼런스 이미지에 직접 링크하는 효과적인 방법. (4) _IP-Adapter_[54], 의미론적 및 구조적 보존을 달성하기 위해 이미지 프롬프트에 대한 추가적인 교차-어텐션 레이어를 사용한다. (5) IP-어댑터와 유사한 _VideoCrafter1-I2V_[4]는 VLDM으로의 이미지 프롬프트 주입의 또 다른 구현이다. 또한, (6) _VideoComposer_[51] 및 (7) _I2VGen-XL_[59]는 VLDM의 입력 및 중간 층 모두에서 이미지 주입을 위한 상기 두 가지 유형의 아이디어를 결합한다.\n' +
      '\n' +
      '플러그 앤 플레이 및 튜닝이 없는 특성으로 인해 본 방법은 위에서 언급한 다른 이미지 상태 향상 모듈과 결합할 수 있다. 직관적이고 공정한 비교를 위해 위의 두 가지 유형 모두에 대해 _Ours_ 및 _Ours+IP-Adapter_[54]로 표시되는 방법을 수행한다. 공정성을 위해 사전 훈련된 VLDM으로 AnimateDiff[12]를 선택한다.\n' +
      '\n' +
      '**정성적 비교.** 도 4에 도시된 바와 같이, 역방향 스테이지의 초기에 잠재된 잡음이 있는 이미지 조건만을 통합하는 방법 [29] 및 _ConcateImage_는 주어진 이미지의 유사한 스타일을 유지할 수 있을 뿐이다. 대조적으로, 모델의 중간 계산 프로세스에서 이미지 정보를 반복적으로 활용하는 이러한 방법[54, 51, 58, 59, 4, 51]은 주어진 이미지의 더 많은 시각적 특징을 보존할 수 있다. 이에 비해 본 논문에서 제안하는 방법은 보다 많은 시각적 세부 사항을 유지하고 입력 영상에 대한 높은 충실도를 달성한다. 더 선명한 비디오 샘플은 프로젝트 웹사이트를 참조하십시오.\n' +
      '\n' +
      '**정량적 비교.** 탭.1에 도시된 바와 같이, 우리의 잡음 정정 방법은 충실도를 효과적으로 향상시킨다. 추가적인 이미지 프롬프트 모듈과 결합하여 [54], 본 방법은 0.8042의 가장 높은 비디오-이미지 충실도 값과 0.9934의 시간적 일관성 값을 얻을 수 있다. 또한, 본 방법은 여전히 허용 가능한 비디오-텍스트를 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Methods & Image fidelity\\(\\uparrow\\) & Temporal coherence\\(\\uparrow\\) & Video-text alignment\\(\\uparrow\\) \\\\ \\hline VLDM [12] + SDEdit [29] & 0.7425 & 0.9888 & **0.2548** \\\\ VLDM [12] + ConcateImage & 0.6944 & 0.9427 & 0.2084 \\\\ VLDM [12] + Ctrl.R.O. [58] & 0.7689 & 0.9919 & 0.2466 \\\\ VLDM [12] + IP-Adapter [54] & 0.7650 & 0.9918 & 0.2287 \\\\ VideoComposer [51] & 0.7483 & 0.9352 & 0.2447 \\\\ VideoCrafter1-I2V [4] & 0.7695 & 0.9689 & 0.2206 \\\\ I2VGen-XL [59] & 0.7717 & 0.9560 & 0.2208 \\\\ \\hline\n' +
      '**s** & 0.7907 & 0.9882 & 0.2517\\\\\n' +
      '**Ours** + IP-Adapter [54] & **0.8042** & **0.9934** & 0.2405 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: WebVid 데이터셋 [2]에 대한 정량적 비교 결과.\n' +
      '\n' +
      '그림 5: 잡음 정정의 가중치에 대한 절제 연구. 정류 시간(\\tau\\)을 고정하고, 다른 프레임에 대해 정류 가중치(\\omega\\)를 변경한다.\n' +
      '\n' +
      '비록 우리는 주로 높은 충실도의 이미지 대 비디오 작업에 초점을 맞추고 있지만 정렬입니다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '알고리즘 1에 소개된 정류 가중치\\(\\omega\\)와 정류 타임스테프\\(\\tau\\)의 두 가지 조정 가능한 파라미터가 포함되어 있으므로, 이 두 파라미터에 대한 삭마 연구를 각각 수행한다. 구체적으로, \\(\\tau=[s,e]\\)는 전체 타임스테프의 \\(s\\) 비율에서 \\(e\\) 비율까지 잡음 정류가 수행됨을 나타낸다.\n' +
      '\n' +
      '정류량.**정류시간\\(\\tau=[0\\%,60\\%]\\)을 고정하고 다른 프레임에 대해 정류량\\(\\omega\\)을 변경한다. 비디오 시퀀스 위의 플롯은 \\(i^{th}\\) 프레임에 대한 정류 가중치 \\(\\omega^{i}\\)를 나타내는 \\(\\omega^{i}\\)에 대한 절제 결과는 그림 5에 나와 있다. 이는 \\(\\omega^{i}\\)이 후속 프레임의 충실도와 시간적 일관성에 영향을 미칠 수 있음을 알 수 있다. 예를 들어, 세 번째 또는 네 번째 열의 결과는 이미지 디테일 또는 모션 이펙트의 급격한 변화를 초래할 수 있다. 따라서, 우리는 높은 충실도와 자연스러운 움직임을 유지하기 위한 두 번째 열의 설정을 경험적으로 선택한다.\n' +
      '\n' +
      '**Rectification Timestep.**Rectification Timestep 주기 \\(\\tau\\)는 예측된 잡음이 어느 디노이징 단계들로 정정될 필요가 있는지를 결정한다. 그림 6과 같이 정류 가중치\\(\\omega\\)를 고정하고 녹색 패널은 정류 시작 및 종료 시간을 보여준다. 잡음 정정이 수행되지 않는 경우, 즉 첫 번째 열 \\(\\tau=[0\\%,0\\%]\\), 충실도\n' +
      '\n' +
      '그림 6: 노이즈 정정의 타임스텝에 대한 절제 연구. 정류량\\(\\omega\\)을 고정하고, 녹색 패널에 정류 시작 및 종료 시간\\(\\tau\\)을 나타내었다.\n' +
      '\n' +
      '그림 7: I2V를 실현하기 위한 현재 T2V 프레임워크에서 본 방법의 플러그 앤 플레이 확장이다. (a) 상이한 T2V 모델에 대한 텍스트-투-비디오 생성 결과. (b) 상이한 T2V 프레임워크들은 고 충실도 이미지-대-비디오 생성을 위한 우리의 방법과 결합된다.\n' +
      '\n' +
      '생성된 동영상의 품질이 좋지 않습니다. 초기 잡음제거부터 잡음정류 주기가 증가함에 따라 (\\(\\tau=[0\\%,20\\%]\\)에서 \\(\\tau=[0\\%,100\\%]\\)까지 충실도가 점진적으로 향상될 것이다. 그러나, 잡음제거 과정(\\(\\tau=[30\\,70\\%]\\) 또는 \\(\\tau=[60\\,100\\%]\\)에서만 정류가 이루어진다면, 생성된 비디오는 여전히 충실도가 떨어진다. 이러한 결과는 역방향 과정이 시작될 때 잡음을 정확하게 예측하는 것이 이미지 충실도를 유지하는 데 중요하다는 것을 나타낸다. 완벽한 충실도가 모션 강도를 손상시킬 수 있음을 고려하여, 우리는 모든 실험에 대해 \\(\\tau=[0\\%,60\\%]\\)을 설정하는 균형을 이룬다.\n' +
      '\n' +
      '**더 많은 VLDM으로의 확장.** 우리의 방법은 동적 모션을 모델링하기 위해 VLDM 이전의 모션을 활용하는데, 이는 실제로 튜닝이 없고 다른 비디오 확산 모델에 적응될 수 있다. 제안된 방법의 확장 성능을 평가하기 위해 최근 T2V 모델을 여러 개 선택하고 잡음 정류 방법을 적용하여 I2V를 구현했다. AnimateDiff[12] 외에도 ModelScopeT2V[49]는 확산 기반 텍스트-비디오 모델로 시공간 블록을 모델 역학에 활용한다. Hotshot-XL[32]는 SD-XL(Stable Diffusion XL) 모델[34]과 함께 작동하도록 개발된 개방형 소스 텍스트 대 GIF 모델이다. 이 세 가지 T2V 모델을 평가하고 플러그 앤 플레이 노이즈 정류 방법을 사용하여 I2V로 확장한다. 그림 7에 도시된 바와 같이, 사전 훈련된 동작 전을 기반으로, 우리의 방법은 높은 충실도와 일관된 애니메이션을 유지할 수 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 오픈 도메인에서 영상 대 영상 생성을 위한 간단하면서도 효과적인 잡음 보정 방법을 제안한다. 우리는 I2V의 문제를 깊이 분석하고 잡음 및 수정된 잡음 제거 프로세스를 통해 높은 충실도를 보장하기 위한 튜닝 프리 접근법을 제안한다. 제안된 방법은 플러그 앤 플레이이며, I2V를 구현하기 위해 다른 비디오 잠재 확산 모델에 적용될 수 있다. 실험 결과는 제안된 방법의 효율성을 입증한다. 우리는 우리의 방법이 비디오 합성 분야에서 충실도를 향상시키는 새로운 아이디어를 제공하기를 바란다. 특히, 본 논문에서 제안하는 방법은 움직임의 세기를 감소시키면서 보다 높은 충실도를 얻을 수 있다. 따라서 향후 탐구에서는 높은 충실도를 유지하면서 움직임 강도를 높이는 데 지속적으로 집중할 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv preprint arXiv:2304.08477_, 2023.\n' +
      '* [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, pages 1728-1738, 2021.\n' +
      '* [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _CVPR_, pages 22563-22575. IEEE, 2023.\n' +
      '* [4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. _arXiv preprint arXiv:2310.19512_, 2023.\n' +
      '* [5] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. _arXiv preprint arXiv:2304.14404_, 2023.\n' +
      '* [6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: conditioning method for denoising diffusion probabilistic models. In _ICCV_, pages 14347-14356. IEEE, 2021.\n' +
      '* [7] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In _NIPS_, pages 8780-8794, 2021.\n' +
      '* [8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _ICCV_, pages 7346-7356, 2023.\n' +
      '* [9] Jean-Yves Franceschi, Edouard Delasalles, Mickael Chen, Sylvain Lamprier, and Patrick Gallinari. Stochastic latent residual video prediction. In _ICML_, pages 3233-3246. PMLR, 2020.\n' +
      '* [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _NIPS_, 27, 2014.\n' +
      '* [11] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. _arXiv preprint arXiv:2303.14897_, 2023.\n' +
      '* [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _NIPS_, 2020.\n' +
      '* [15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruigi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [16] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _NIPS_, 2022.\n' +
      '* [17] Aleksander Holynski, Brian L Curless, Steven M Seitz, and Richard Szeliski. Animating pictures with eulerian motion fields. In _CVPR_, pages 5810-5819, 2021.\n' +
      '* [18] Tobias Hoppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. _arXiv preprint arXiv:2206.07696_, 2022.\n' +
      '*[*[19] Yaosi Hu, Chong Luo, and Zhenzhong Chen. 텍스트 설명으로 제어 가능한 이미지 대 비디오 생성으로 이동합니다. _CVPR_에서, 페이지 18219-18228, 2022.\n' +
      '* [20] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. _arXiv preprint arXiv:2304.06025_, 2023.\n' +
      '* [21] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [22] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In _ICLR_, 2014.\n' +
      '* [23] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: context-aware controllable video synthesis. _NIPS_, 34:14042-14055, 2021.\n' +
      '* [24] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal video prediction from still images. In _ECCV_, pages 600-615, 2018.\n' +
      '* [25] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image dynamics. _arXiv preprint arXiv:2309.07906_, 2023.\n' +
      '* [26] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. _arXiv preprint arXiv:2303.08320_, 2023.\n' +
      '* [27] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. _arXiv preprint arXiv:2304.01186_, 2023.\n' +
      '* [28] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable animation of fluid elements in still images. In _CVPR_, pages 3667-3676, 2022.\n' +
      '* [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In _ICLR_. OpenReview.net, 2022.\n' +
      '* [30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _CVPR_, pages 6038-6047. IEEE, 2023.\n' +
      '* [31] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.\n' +
      '* [32] John Mullan, Duncan Crawbuck, and Aakash Sastry. Hotshot-XL, 2023.\n' +
      '* [33] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In _CVPR_, pages 18444-18455, 2023.\n' +
      '* [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763. PMLR, 2021.\n' +
      '* [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '* [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10674-10685. IEEE, 2022.\n' +
      '* [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, pages 234-241. Springer, 2015.\n' +
      '* [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _CVPR_, pages 22500-22510, 2023.\n' +
      '* [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In _NIPS_, 2022.\n' +
      '* [41] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [42] Aliaksandr Siarohin, Stephane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. _NIPS_, 32, 2019.\n' +
      '* [43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In _ICLR_. OpenReview.net, 2023.\n' +
      '* [44] Harrison Jesse Smith, Qingyuan Zheng, Yifei Li, Somya Jain, and Jessica K Hodgins. A method for automatically animating children\'s drawings of the human figure. _arXiv preprint arXiv:2303.12741_, 2023.\n' +
      '* [45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, pages 2256-2265. PMLR, 2015.\n' +
      '* [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '\n' +
      '* [48] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. _NIPS_, 35:23371-23385, 2022.\n' +
      '* [49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint arXiv:2308.06571_, 2023.\n' +
      '* [50] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In _CVPR_, pages 10039-10049, 2021.\n' +
      '* [51] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [52] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. _arXiv preprint arXiv:2309.15103_, 2023.\n' +
      '* [53] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. _Entropy_, 25(10):1469, 2023.\n' +
      '* [54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arxiv:2308.06721_, 2023.\n' +
      '* [55] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. _arXiv preprint arXiv:2308.08089_, 2023.\n' +
      '* [56] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Ming Gong, Lijuan Wang, Zicheng Liu, Houqiang Li, and Nan Duan. NUWA-XL: diffusion over diffusion for extremely long video generation. In _ACL_, pages 1309-1320. Association for Computational Linguistics, 2023.\n' +
      '* [57] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, pages 3836-3847, 2023.\n' +
      '* [59] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2ygen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [60] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>