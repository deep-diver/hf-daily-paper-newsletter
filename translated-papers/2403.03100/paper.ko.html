<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _NaturalSpeech 3_: Zero-Shot Speech Synthesis\n' +
      '\n' +
      '요인화된 코덱 및 확산 모델과 함께\n' +
      '\n' +
      ' Zeqian Ju\\({}^{12}\\)1\n' +
      '\n' +
      'Yuancheng Wang\\({}^{3}\\)1\n' +
      '\n' +
      'Kai Shen\\({}^{41}\\)1\n' +
      '\n' +
      'Xu Tan\\({}^{1}\\)1\n' +
      '\n' +
      'Detai Xin\\({}^{1}\\)1\n' +
      '\n' +
      'Dongchao Yang\\({}^{1}\\)\n' +
      '\n' +
      'Yanqing Liu\\({}^{1}\\)\n' +
      '\n' +
      'Yichong Leng\\({}^{1}\\)\n' +
      '\n' +
      'Kaitao Song\\({}^{1}\\)\n' +
      '\n' +
      'Siliang Tang\\({}^{4}\\)\n' +
      '\n' +
      'Zhizheng Wu\\({}^{3}\\)\n' +
      '\n' +
      'Tao Qin\\({}^{1}\\)\n' +
      '\n' +
      'Xiang-Yang Li\\({}^{2}\\)\n' +
      '\n' +
      'Wei Ye\\({}^{6}\\)\n' +
      '\n' +
      'Shikun Zhang\\({}^{6}\\)\n' +
      '\n' +
      'Jiang Bian\\({}^{1}\\)\n' +
      '\n' +
      'Lei He\\({}^{1}\\)\n' +
      '\n' +
      'Jinyu Li\\({}^{1}\\)\n' +
      '\n' +
      'Sheng Zhao\\({}^{1}\\)\n' +
      '\n' +
      'Microsoft Research Asia & Microsoft Azure Speech\n' +
      '\n' +
      '중국과학기술대학교\n' +
      '\n' +
      '중국 홍콩대학, 선전.\n' +
      '\n' +
      '동경대학, \\({}^{6}\\)페킹대학, \\({}^{4}\\)저장대학, \\({}^{5}\\)도쿄대학, \\({}^{6}\\)페킹대학\n' +
      '\n' +
      '[https://aka.ms/speechresearch](https://aka.ms/speechresearch)\n' +
      '\n' +
      '각주 1: 처음 4명의 저자는 이 작업에 동등하게 기여했으며 그 이름은 무작위 순서로 나열된다. 교신저자 : Xu Tan, xuta@microsoft.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근의 대규모 텍스트 음성 변환(TTS) 모델은 상당한 진전을 이루었지만 여전히 음성 품질, 유사성 및 운율에는 미치지 못한다. 발화를 생성하기 위해 중요한 과제를 제기하는 다양한 속성들(예를 들어, 콘텐츠, 운율, 음색, 및 음향 세부사항)을 복잡하게 포괄하는 것을 고려하면, 자연스러운 아이디어는 발화를 상이한 속성들을 나타내는 개별 서브스페이스들로 인수분해하여 개별적으로 생성하는 것이다. 이에 착안하여, 우리는 제로 샷 방식으로 자연스러운 음성을 생성하기 위해 새로운 인수분해 확산 모델을 갖는 TTS 시스템인 _NaturalSpeech 3_를 제안한다. 구체적으로, 1) 음성 파형을 콘텐츠, 운율, 음색 및 음향 세부 사항의 부분 공간으로 분해하기 위해 요인화된 벡터 양자화(Factorized Vector Quantization, FVQ)를 갖는 신경 코덱을 설계하고, 2) 해당 프롬프트에 따라 각 부분 공간에 속성을 생성하기 위한 요인화된 확산 모델을 제안한다. 이 인수분해 설계로, NaturalSpeech 3은 분할 및 정복 방식으로 얽히지 않은 부분 공간을 갖는 복잡한 음성을 효과적이고 효율적으로 모델링할 수 있다. 실험은 NaturalSpeech 3이 품질, 유사성, 운율 및 명료도에서 최첨단 TTS 시스템보다 우수함을 보여준다. 또한, 1B 파라미터와 200K 시간의 훈련 데이터로 스케일링함으로써 더 나은 성능을 달성한다.\n' +
      '\n' +
      '도 1: (a) 음성 속성 인수분해를 위한 신경 음성 코덱과 인수분해 확산 모델을 갖는, NaturalSpeech 3의 개요. (b) NaturalSpeech 3의 데이터 및 모델 스케일링.\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '최근에는 TTS(Text-to-Speech) 합성에 있어서 상당한 진보가 이루어지고 있다. 전통적인 TTS 시스템[1; 2; 3; 4]은 일반적으로 스튜디오에 기록된 제한된 데이터 세트에 대해 트레이닝되며, 따라서 고품질의 제로-샷 음성 합성을 지원하지 못한다. 최근 논문 [5; 6; 7]은 코퍼스와 모델 크기를 크게 확장함으로써 제로샷 TTS에 상당한 진전을 이루었다. 그러나 이러한 대규모 TTS 시스템의 합성 결과는 음성 품질, 유사도, 운율 측면에서 만족스럽지 못하다.\n' +
      '\n' +
      '열등한 결과의 과제는 연설이 내용, 운율, 음색 및 음향 세부 사항과 같은 수많은 속성을 포함하기 때문에 연설에 포함된 복잡한 정보에서 비롯된다. 데이터 표현으로 원시 파형[8; 9]과 멜-스펙트로그램[1; 2; 10; 7; 11]을 사용하는 이전 작업은 음성 생성 동안 이러한 복잡한 복잡성을 겪는다. 자연스러운 아이디어는 말을 서로 다른 속성을 나타내는 얽히지 않은 부분 공간으로 인수분해하여 개별적으로 생성하는 것이다. 그러나, 이러한 종류의 얽힘이 없는 인수분해를 달성하는 것은 자명하지 않다. 이전 작업[12; 13; 6]은 잔여 벡터 양자화(RVQ)에 기초한 신경 오디오 코덱[14; 15]을 사용하여 음성을 다단계 이산 토큰들로 인코딩한다. 이 접근법은 음성을 상이한 계층 표현으로 분해하지만, 상이한 RVQ 레벨에 걸쳐 음성의 상이한 속성의 정보를 효과적으로 분리하지 못하고 여전히 복잡한 결합 정보를 모델링하는 데 어려움을 겪는다.\n' +
      '\n' +
      '더 나은 품질, 유사성 및 운율을 갖는 음성을 효과적으로 생성하기 위해, 우리는 제로 샷 방식으로 자연스러운 음성을 생성하기 위해 새로운 인수분해 확산 모델을 갖는 TTS 시스템을 제안한다. 구체적으로, 1) FACodec으로 명명된 인자화 벡터 양자화(Factorized Vector Quantization, FVQ)를 이용한 새로운 신경 음성 코덱을 도입하여 음성 파형을 콘텐츠, 운율, 음색 및 음향 세부 사항의 별개의 부분 공간으로 분해하고, 이러한 비얽힌 표현으로 음성 파형을 재구성하고, 정보 병목 현상을 활용하며[16; 17], 다양한 감독 손실 및 비얽힘 현상을 개선하기 위한 적대적 학습[18]을 제안한다. 2) 본 논문에서는 이에 대응하는 프롬프트를 기반으로 지속 시간, 콘텐츠, 운율 및 음향 세부 사항의 인자화 음성 표현을 생성하는 인자화 확산 모델을 제안한다. 이 디자인을 사용하면 서로 다른 속성을 제어하기 위해 서로 다른 프롬프트를 사용할 수 있습니다. 내추럴 스피치 3에 언급된 우리의 방법의 개요는 그림 1에 나와 있다.\n' +
      '\n' +
      '우리는 복잡한 음성을 다른 속성을 나타내는 부분 공간으로 분해하여 음성 표현의 모델링을 단순화한다. 이 방법은 다음과 같은 몇 가지 장점을 제공한다. 1) 인수분해 확산 모델은 이러한 얽힌 표현을 효율적으로 학습할 수 있고, 더 높은 품질의 음성 생성을 초래한다. 2) FACodec에서 음색 정보를 디엔탱글링함으로써, 직접 음색을 모델링하는 것을 피할 수 있게 한다. 이것은 학습 복잡도를 감소시키고 제로-샷 음성 합성을 개선시킨다; 3) 우리는 상이한 속성들을 제어하기 위해 상이한 프롬프트들을 사용할 수 있고, NaturalSpeech 3의 제어성을 향상시킨다.\n' +
      '\n' +
      '이러한 설계로 인해 자연 음성 3은 음성 품질, 유사성, 운율 및 명료도에서 상당한 개선을 달성했다. 구체적으로, 1) CMOS 측면에서 LibriSpeech 테스트 세트에서 지상-진실 음성과 비교하거나 더 나은 음성 품질을 달성한다; 2) 합성 음성과 프롬프트 음성 사이의 유사성에 대해 새로운 SOTA를 달성한다(Sim-O에서는 0.64\\(\\rightarrow\\) 0.67, SMOS에서는 3.69\\(\\rightarrow\\) 4.01), 3) 다른 TTS 시스템에 비해 운율이 크게 향상되었으며, 평균 MCD(낮은 것이 더 좋다), +\\(0.21 SMOS; 4) 명료도에 대해 SOTA를 달성한다(WER에서는 1.94\\(\\rightarrow\\) 1.81). 또한, 1B 파라미터와 200K 시간의 학습 데이터로 스케일링하여 NaturalSpeech 3의 확장성을 입증한다. 오디오 샘플은 [https://speechresearch.github.io/naturalspeech3](https://speechresearch.github.io/naturalspeech3)에서 찾을 수 있다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '이 섹션에서는 1) 제로 샷 TTS; 2) TTS에서의 음성 표현; 3) TTS에서의 생성 방법; 4) 음성 속성 이탈을 포함하는 TTS의 최근 진행 상황에 대해 논의한다.\n' +
      '\n' +
      '** Zero-shot TTS.** Zero-shot TTS는 보이지 않는 화자에 대한 음성을 음성 프롬프트로 합성하는 것을 목적으로 한다. 이 시스템은 데이터 표현과 모델링 방법에 따라 4개의 그룹으로 체계적으로 분류할 수 있다: 1) 이산토큰 + 자기회귀[6; 19; 20]; 2) 이산토큰 + 비자기회귀[13; 21; 22]; 3) 연속벡터 + 자기회귀[23]; 4) 연속벡터 + 비자기회귀[5; 11; 24; 25). 이산 토큰들은 전형적으로 신경 코덱으로부터 유도되는 반면, 연속 벡터들은 일반적으로 오디오 오토인코더 또는 코덱으로부터의 멜-스펙트로그램 또는 레이턴트들로부터 획득된다. 앞서 언급한 관점 외에도, 우리는 속성 디엔탱글먼트를 기반으로 음성 파형을 부분 공간으로 디엔탱글링하고, 디밴딩 앤 정커의 원리에 의해 동기화된 각 부분 공간 내의 속성을 생성하기 위한 인수분해 확산 모델을 제안한다. 한편, 우리는 자기 회귀 모델과 함께 이산 토큰을 사용하여 이전 방법을 재사용할 수 있다.\n' +
      '\n' +
      '** TTS에서의 음성 표현** 전통적인 작품들은 원시 파형[26; 27; 28] 또는 멜-스펙트로그램[29; 30; 3; 31]과 같은 사전-기반 음성 표현을 사용하여 제안한다. 최근, 대규모 TTS 시스템[6; 13; 5] 레버리지 데이터 구동 표현, 즉 이산 토큰 또는 연속 벡터 중 하나는 오토 인코더[14; 15; 32]를 형성한다. 그러나, 이러한 방법들은 음성이 다양한 복잡한 속성을 포함하고 있고 음성 생성 동안 복잡한 복잡성을 만난다는 것을 무시한다. 본 논문에서는 음성을 효과적으로 효율적으로 모델링할 수 있는 서로 다른 속성을 나타내는 개별 부분공간으로 인수분해한다.\n' +
      '\n' +
      '**TTS.**의 생성 방법.** 이전 작업은 NAR 기반 모델[3; 33; 34; 7; 5; 11]이 AR 기반 모델보다 더 나은 견고성과 생성 속도를 제공한다는 것을 입증했는데, 이는 NAR 기반 모델이 지속 시간을 명시적으로 모델링하고 모든 기능을 동시에 예측하기 때문이다. 대신 AR 기반 모델[2; 30; 6; 23; 35]은 암묵적으로 지속 시간 모델링 및 토큰 샘플링 전략으로 인해 NAR 기반 모델보다 다양성, 운율, 표현성 및 유연성이 더 우수하다. 본 연구에서는 NAR 모델링 방법을 채택하여 분해된 음성 표현을 지원하고 AR 모델링 접근법으로 확장하기 위한 인수분해 확산 모델을 제안한다. 이를 통해 내추럴 스피치 3은 안정성과 생성 속도를 유지하면서 더 나은 표현력을 얻을 수 있다.\n' +
      '\n' +
      '**Speech Attribute Disentanglement.** 선행 작업 [36; 37; 38]은 자기-감독된 사전-훈련된 모델들 [39; 40; 41], 기본 주파수, 및 음색으로부터의 스피치 콘텐츠와 같은 스피치 생성을 위한 디엔탱글 표현을 이용하지만, 스피치 품질은 만족스럽지 않다. 최근, 몇몇 연구들은 신경 음성 코덱에서 속성 분할을 탐구한다. SpeechTokenizer[42]는 시맨틱 증류를 위해 HuBERT[43]을 사용하며, 제1-레이어 RVQ 표현을 시맨틱 정보로서 렌더링하는 것을 목표로 한다. Disen-TF-Codec[44]은 콘텐츠 및 음색 표현과의 디엔탠먼트(disentanglement)를 제안하고, 제로-샷 음성 변환에 적용한다. 본 논문에서는 고품질 재구성을 보장하면서 콘텐츠, 운율, 음향 세부 사항 및 음색을 포함한 더 많은 음성 속성과 더 나은 분할을 달성한다. 본 논문에서는 이러한 엉킴이 제로샷 TTS 작업에서 상당한 개선을 가져올 수 있음을 검증한다.\n' +
      '\n' +
      '## 3 자연 발화 3\n' +
      '\n' +
      '### Overall Architecture\n' +
      '\n' +
      '이 절에서는 더 나은 음성 품질, 유사성 및 제어성을 가진 자연 및 제로 샷 텍스트 음성 합성을 위한 최첨단 시스템인 NaturalSpeech 3을 제시한다. 도 1에 도시된 바와 같이, NaturalSpeech 3은 1) 속성 분해를 위한 신경 음성 코덱(즉, FACodec); 2) 요인화된 음성 속성을 생성하는 요인화된 확산 모델로 구성된다. 음성 파형은 복잡하고 다양한 속성을 복잡하게 포함하기 때문에, 우리는 음성을 지속 시간, 운율, 내용, 음향 세부 사항 및 음색의 5가지 속성으로 인수분해한다. 구체적으로, 지속 시간은 운율의 한 측면으로 간주될 수 있지만, 우리는 비-자진적 음성 생성 설계로 인해 명시적으로 모델링하기로 선택한다. 우리는 내부 정렬 도구를 사용하여 음성과 음소를 정렬하고 음소 수준 지속 시간을 얻는다. 다른 속성의 경우, 우리는 분해된 음성 속성 부분 공간(즉, 콘텐츠, 운율, 음향 세부 정보 및 음색)을 학습하기 위해 인자화된 신경 음성 코덱을 암묵적으로 활용한다. 그런 다음 요인화된 확산 모델을 사용하여 각 음성 속성 표현을 생성한다. 마지막으로, 생성된 음성 속성으로 파형을 재구성하기 위해 코덱 디코더를 사용한다. 섹션 3.2의 FACodec과 섹션 3.3의 인수분해 확산 모델을 소개한다.\n' +
      '\n' +
      '속성 인수분해를 위한### FACodec\n' +
      '\n' +
      '###### 3.2.1 FACodec 모델 개요\n' +
      '\n' +
      '본 논문에서는 복잡한 음성 파형을 콘텐츠, 운율, 음색 및 음향 세부 사항의 음성 속성을 나타내는 비얽힌 부분 공간으로 변환하고 이로부터 고품질 음성 파형을 재구성하는 요인화 신경망 음성 코덱(FACodec2)을 제안한다.\n' +
      '\n' +
      '도 2에 도시된 바와 같이, 우리의 FACodec은 음성 인코더, 음색 추출기, 콘텐츠, 운율, 음향 세부 사항 및 음성 디코더를 위한 3개의 인수분해 벡터 양자화기(FVQ)로 구성된다. 음색 추출기는 16KHz 음성 데이터(즉, 12.5ms 음성 세그먼트에 해당하는 각 프레임)에 대해 200의 다운샘플 레이트를 갖는 음성 인코더를 위해 몇 개의 컨볼루션 블록을 채택하여 사전 양자화 잠재력(h\\)을 획득한다; 2) 음색 추출기는 음성 인코더의 출력을 음색 속성을 나타내는 전역 벡터(h_{t}\\)로 변환하는 트랜스포머 인코더; 3) 다른 속성(i\\)(운율, 내용 및 음향 상세에 대해 각각\\(i=p,c,d\\)(각각)에 대해 인자화된 벡터 양자화기(FVQ\\({}_{i}\\))를 사용하여 세밀한 음성 속성 표현을 캡처하고 대응하는 이산 토큰을 획득한다; 4) 음성 디코더는 음성 인코더의 구조를 반영하지만 훨씬 더 큰 파라미터 양으로 고품질 음성 재구성을 보장한다. 먼저 운율, 내용, 음향의 세부 사항을 함께 표현한 후 조건부 계층 정규화[45]를 통해 음색 정보를 융합하여 음성 복호화기에 대한 입력 \\(z\\)을 얻는다. 우리는 다음 섹션에서 더 나은 발화 속성 이탈을 달성하는 방법에 대해 논의한다.\n' +
      '\n' +
      '###### 3.2.2 속성 불엉킴\n' +
      '\n' +
      '말을 다른 부분 공간으로 직접 인수분해하는 것은 말의 엉킴을 보장하지 않는다. 이 섹션에서는 1) 정보 병목 현상, 2) 감독, 3) 그라디언트 역방향 및 4) 세부 드롭아웃과 같은 더 나은 음성 속성 불일치를 달성하기 위한 몇 가지 기술을 소개한다. 자세한 교육 내용은 부록 B.1을 참조하십시오.\n' +
      '\n' +
      '**Information Bottleneck.** [16, 17]에서 영감을 받아 모델이 불필요한 정보(콘텐츠 부분 공간에서의 운율 등)를 제거하도록 강제하고, 인코더 출력을 저차원 공간(즉, 8차원)으로 투영하여 운율, 콘텐츠 및 음향 세부사항 FVQ에서 정보 병목 현상을 구성하고 후속적으로 이 저차원 공간 내에서 양자화한다. 이 기술은 각각의 코드 임베딩이 더 적은 정보를 포함하는 것을 보장하여, 정보 분할을 용이하게 한다[32, 46]. 양자화 후, 우리는 양자화된 벡터를 원래의 차원으로 다시 투영할 것이다.\n' +
      '\n' +
      '**Supervision.** 고품질 음성 분할을 달성하기 위해 각 속성에 대한 보조 작업으로 Supervision을 도입합니다. 운율은 음정이 운율의 중요한 부분이기 때문에 우리는 음정 정보를 예측하기 위해 양자화 후 잠재 \\(z_{p}\\)을 취한다. 각 프레임에 대한 F0를 추출하고 정규화된 F0(z-score)를 표적으로 사용한다. 콘텐츠의 경우 음소 레이블을 대상으로 직접 사용한다(프레임 수준 음소 레이블을 얻기 위해 내부 정렬 도구를 사용한다). 음색은 화자 ID를 예측하여 \\(h_{t}\\)에 화자 분류를 적용한다.\n' +
      '\n' +
      '**경사 반전.** 정보 유출(콘텐츠의 운율 누출과 같은)을 피하면 엉킴을 개선할 수 있습니다. [47]에서 영감을 받아, 우리는 잠재 공간에서 원하지 않는 정보를 제거하기 위해 GRL(gradient reversal layer)[48]을 갖는 적대적 분류기를 채택한다. 구체적으로 운율의 경우 음소-GRL(즉, 음소 레이블을 예측하여 GRL 계층)을 적용하여 내용 정보를 제거하고, 콘텐츠의 경우 음정이 운율의 중요한 측면이기 때문에 단순화를 위해 운율 정보를 줄이기 위해 F0-GRL을 적용하고, 음향 세부 사항의 경우 음소-GRL과 F0-GRL을 모두 적용하여 내용 정보와 운율 정보를 모두 제거한다. 또한, 음색을 제거하기 위해 \\(z_{p},z_{c},z_{d}\\)의 합에 스피커-GRL을 적용한다.\n' +
      '\n' +
      '**세부 드롭아웃.** 우리는 다음과 같은 고려 사항을 가지고 있다: 1) 경험적으로, 우리는 코덱이 감독이 없기 때문에 음향 세부 부분 공간에서 원하지 않는 정보(예: 콘텐츠, 운율)를 보존하는 경향이 있다는 것을 발견한다; 2) 직관적으로, 음향 세부 사항 없이, 디코더는 음성만을 재구성해야 한다.\n' +
      '\n' +
      '그림 2: 속성 인수분해를 위한 FACodec의 프레임워크.\n' +
      '\n' +
      '저품질이지만 운율, 내용, 음색을 가지고 있습니다. 이들에 의해 동기화된 학습 과정에서 확률 \\(p\\)으로 \\(z_{d}\\)을 랜덤하게 마스킹하여 디테일 드롭아웃을 설계한다. 디테일 드롭아웃을 통해 디엔탱글먼트와 재구성 품질의 트레이드 오프를 달성한다: 1) 코덱은 낮은 퀄리티에도 불구하고 디커플 능력을 보장하기 위해 운율, 내용 및 음색 정보를 충분히 활용하여 음성을 재구성할 수 있다; 2) 음향 세부 정보가 주어지면 고품질 음성을 얻을 수 있습니다.\n' +
      '\n' +
      '### 인수분해 확산 모델\n' +
      '\n' +
      '###### 3.3.1 모델 개요\n' +
      '\n' +
      '우리는 더 나은 생성 품질을 위해 이산 확산으로 음성을 생성한다. 우리는 다음과 같은 고려 사항을 가지고 있다: 1) 우리는 음성을 지속 시간, 운율, 내용 및 음향 세부 사항과 같은 속성으로 인수분해하고 특정 조건에 따라 순차적으로 생성한다. 첫째, 섹션 3.1에서 언급했듯이 비자동 회귀 생성 설계로 인해 먼저 지속 시간을 생성한다. 둘째, 직관적으로 음향적 세부사항은 최종적으로 생성되어야 한다; 2) 음성 인수분해 설계에 따라 생성 모델에만 해당 속성 프롬프트를 제공하고 부분공간에서 이산 확산을 적용한다; 3) 확산 모델에서 문맥 내 학습을 용이하게 하기 위해 코덱을 활용하여 음성 프롬프트를 속성 프롬프트(내용, 운율 및 음향적 세부사항 프롬프트)로 인수분해하고 [49; 13]에 이어 부분 소음 메커니즘으로 목표 음성 속성을 생성한다. 예를 들어 운율 생성을 위해 우리는 운율 프롬프트(잡음 없음)와 표적 시퀀스(잡음 있음)를 직접 연결하고 점진적으로 표적 시퀀스로부터 운율 프롬프트와 잡음을 제거한다.\n' +
      '\n' +
      '이러한 생각을 바탕으로 그림 3과 같이 음소 인코더와 음성 속성(즉, 지속 시간, 운율, 내용 및 음향 세부 사항) 확산 모듈로 구성된 요인화 확산 모델을 제시한다. 1) 음소 인코더에서 인코딩된 지속 시간 프롬프트 및 음소 수준 텍스처 조건을 사용하여 지속 시간 확산을 적용하여 음성 지속 시간을 생성한다. 그리고 프레임 레벨 음소 조건\\(c_{ph}\\); 2) 운율 프롬프트와 음소 조건\\(c_{ph}\\)을 갖는 운율\\(z_{p}\\); 3) 콘텐츠 프롬프트와 함께 콘텐츠 운율\\(z_{c}\\)을 생성하고 생성된 운율\\(z_{p}\\)과 음소\\(c_{ph}\\; 4) 음향 세부 사항 프롬프트와 함께 음향 세부 사항\\(z_{d}\\)을 생성하고 생성된 운율, 콘텐츠 및 음소\\(z_{p},z_{c},c_{ph}\\)을 조건으로 사용한다. 구체적으로, 우리는 음색 속성을 명시적으로 생성하지 않는다. FACodec의 인수분해 설계로 인해 프롬프트에서 음색을 직접 얻을 수 있으며 생성할 필요가 없다. 마지막으로, 속성 \\(z_{p},z_{c},z_{d}\\)과 \\(h_{t}\\)을 결합하여 코덱 디코더로 디코딩하여 타겟 음성을 합성한다. 우리는 섹션 3.3.2에서 확산 제형에 대해 논의한다.\n' +
      '\n' +
      '###### 3.3.2 확산 제형\n' +
      '\n' +
      '*Forward Process.** Denote \\(\\mathbf{X}=[x_{i}]_{i=1}^{N}\\) 타겟 이산 토큰 시퀀스, 여기서 \\(\\mathbf{X}^{p}\\)는 프롬프트 이산 토큰 시퀀스이고, \\(\\mathbf{C}\\)는 조건이다. 시간에서의 순방향 프로세스는 \\(\\mathbf{M}_{t}=[m_{t,i}]_{i=1}^{N}\\)으로 수식화된 \\(\\mathbff{M}_{t}=[m_{t,i}]_{i=1}^{N}\\)에서 토큰의 서브세트를 마스킹하는 것으로 정의된다. token if \\(m_{t,i}=1\\) if \\(m_{t,i}=0\\) if \\(x_{i}\\) unmasked. \\(m_{t,i}\\overset{iid}{\\sim}\\text{Bernoulli}(\\sigma(t))\\) and \\(\\sigma(t)\\in(0,1]\\) is monotonically increasing function. 본 논문에서는 \\(\\sigma(t)=\\sin(\\frac{\\pi t}{2T}), t\\in(0,T]\\) 특히, 원본 토큰 시퀀스에 대해서는 \\(\\mathbf{X}_{0}=\\mathbf{X}\\)을, 완전 마스킹 시퀀스에 대해서는 \\(\\mathbf{X}_{T}\\)을 나타낸다.\n' +
      '\n' +
      '그림 3: 요인화 확산 모델의 프레임워크는 1) 음소 인코더, 2) 지속 시간 확산 및 길이 조절기, 3) 운율 확산, 4) 내용 확산, 5) 세부(음향 세부) 확산으로 구성된다. 모듈 2~5는 동일한 확산 제형을 공유한다는 점에 유의한다.\n' +
      '\n' +
      '역방향 프로세스.** 역방향 프로세스는 전체 마스킹된 시퀀스 \\(\\mathbf{X}_{T}\\)에서 시작하여 역방향 분포 \\(q(\\mathbf{X}_{t-\\Delta t}|\\mathbf{X}_{0},\\mathbf{X}_{t})\\에서 샘플링하여 \\(\\mathbf{X}_{0}\\)을 점진적으로 복원한다. 추론에서 \\(\\mathbf{X}_{0}\\)을 사용할 수 없기 때문에, 우리는 \\(\\theta\\)에 의해 매개변수화된 확산 모델 \\(p_{\\theta\\)을 사용하여 \\(\\mathbf{X}^{p}\\)과 \\(\\mathbf{C}\\(p_{\\theta}(\\mathbf{X}_{0}|\\mathbff{X}_{t},\\mathbf{X}^{p},\\mathbf{C})으로 표시된 \\(p_{\\theta\\)와 \\(\\mathbf{C}\\)의 마스킹된 토큰을 예측한다. 매개변수 \\(\\theta\\)는 마스킹된 토큰들의 음의 로그-우도를 최소화하도록 최적화된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{mask}=\\operatorname*{\\mathbb{E}_{\\mathbf{X}\\in\\mathcal{D},t\\in[0,T]}-\\sum_{i=1}^{N}m_{t,i}\\cdot\\log(p_{\\theta}(x_{i}|\\mathbf{X}_{t}, \\mathbf{X}^{p},\\mathbf{C})).\n' +
      '\n' +
      '그러면 우리는 역전이 분포를 얻을 수 있다:\n' +
      '\n' +
      '\\[p(\\mathbf{X}_{t-\\Delta t}|\\mathbf{X}_{t},\\mathbf{X}^{p},\\mathbf{C})=\\operatorname*{\\mathbbbb{E}_{\\mathbf{X}_{0}\\sim p_{\\theta}(\\mathbf{X}_{0}|\\mathbff{X}_{t},\\mathbf{X}^{p},\\mathbf{C}}q(\\mathbf{X}_{t-\\Delta t}|\\mathbf{\\hat{X}_{0},\\mathbf{X}_{t}}}}_{t},\\mathbf{C}}}_{\\mathbbb{E}_{\\mathbbb{E}_{\\mathbf{X}_{0}\\sim p_{\\theta}(\\mathbf{X}_{0}|\\mathbf{X}_{\n' +
      '\n' +
      '추론하는 동안 우리는 완전히 마스킹된 시퀀스 \\(\\mathbf{X}_{T}\\)에서 시작하여 \\(p(\\mathbf{X}_{t-\\Delta t}|\\mathbf{X}_{t},\\mathbf{X}^{p},\\mathbf{C})\\에서 반복적으로 샘플링함으로써 점진적으로 마스킹된 토큰을 대체한다. [50; 51; 52]에 의해, 우리는 먼저 \\(\\mathcal{X}_{t},\\mathbf{X}_{t},\\mathbf{x}_{i}|\\mathbf{x}_{t},\\mathbf{C})에서 \\(\\hat{x}_{i}|\\mathbf{x}_{t},\\mathbf{C})의 신뢰 점수를 \\(\\mathbf{x}_{t})에서 \\(\\hat{x}_{i}|\\mathbf{x}_{t},\\mathbf{C})의 신뢰 점수를 \\(\\mathbf{x}_{t})에서 \\(\\hat{x}_{i}|\\mathbf{x}_{t})으로 정의하면,\n' +
      '\n' +
      '**분류자-프리 안내.** 더욱이, 분류자-프리 안내 기법을 적용한다[53;54]. 구체적으로, 훈련에서 우리는 \\(p_{\\text{cfg}}=0.15\\)의 확률로 프롬프트를 사용하지 않는다. 추론에서는 유도 척도(g_{\\text{cond}}=g(\\mathbf{X}|\\mathbf{X}^{p})\\)에 의해 유도되고, 무조건 생성(g_{\\text{uncond}}=g(\\mathbf{X})으로부터 멀리 떨어진 조건 생성(g_{\\text{cfg}}=g_{\\text{cond}}+\\alpha\\cdot(g_{\\text{cond}-g_{\\text{uncond})\\)으로 유도되는 모델 출력을 실험 결과에 기초하여 유도 척도(\\alpha\\)로 추론한다. 그런 다음 [55]에 따라 \\(g_{\\text{final}}=\\text{std}(g_{\\text{cond})\\times g_{\\text{cfg}}/\\text{std}(g_{\\text{cfg}})\\text{std}(g_{\\text{cfg}})를 통해 재구성한다.\n' +
      '\n' +
      '내추럴 스피치 시리즈에 대한### 연결\n' +
      '\n' +
      '내추럴 스피치 3은 내추럴 스피치 시리즈의 고급 TTS 시스템이다. 이전 버전인 NaturalSpeech[4] 및 NaturalSpeech2[5]와 비교하여, NaturalSpeech3은 다음과 같은 연결 및 구별을 갖는다:\n' +
      '\n' +
      '* _Goal._ 내추럴 스피치 시리즈는 높은 품질과 다양성을 가진 자연스러운 스피치를 생성하는 것을 목표로 한다. 이 목표를 여러 단계로 접근한다: 1) 단일 화자 시나리오에서 고품질 음성 합성을 달성한다. 이를 위해, NaturalSpeech[4]는 인간의 녹음과 동등한 품질로 음성을 생성하고 단일 화자 녹음-스튜디오 데이터세트(예를 들어, LJSpeech)만을 다룬다. 2) 멀티 스타일, 멀티 스피커 및 멀티 언어 시나리오에서 고품질 및 다양한 음성 합성을 달성합니다. 내추럴 스피치 2[5]와 내추럴 스피치 3은 모두 대규모, 다중 화자 및 야생 데이터 세트를 기반으로 제로 샷 합성 능력을 탐색하여 음성 다양성에 중점을 둔다.\n' +
      '* _Architecture._ 내츄럴 스피치 시리즈는 비-자동 회귀 음성 생성을 위한 파형 재구성 및 지속 시간 예측을 위한 인코더/디코더와 같은 기본 구성 요소를 공유한다. 내츄럴 스피치 3은 플로우 기반 생성 모델을 활용하는 NaturalSpeech와 잠재 확산 모델을 활용하는 NaturalSpeech 2와는 달리, 각각의 인수분해된 음성 속성을 분할 및 정복 방식으로 생성하기 위해 인수분해된 확산 모델의 개념을 제안한다.\n' +
      '* _Speech Representations._ 음성 파형의 복잡성으로 인해, NaturalSpeech 시리즈는 인코더/디코더를 사용하여 고품질 음성 합성을 위해 잠재된 음성을 획득한다. 내츄럴 스피치는 나이브 VAE 기반 연속 표현을 활용하고, 내츄럴 스피치 2는 잔차 벡터 양자화기를 갖는 신경 오디오 코덱으로부터의 연속 표현을 활용하는 반면, 내츄럴 스피치 3은 복잡한 음성 신호를 비 얽힌 부분 공간(운율, 내용, 음향 세부 사항 및 음색)으로 변환하고 음성 모델링 복잡도를 감소시키는 새로운 FACodec을 제안한다.\n' +
      '\n' +
      '##4 실험 및 결과\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '이 하위 섹션에서는 요인화된 확산 모델에 대한 훈련, 추론 및 평가를 소개한다. 모델 구성은 부록 A.1을 참조하십시오.\n' +
      '\n' +
      '**구현 상세.** 우리는 라이브러리라잇[56]을 사용하는데, 라이브러리라잇[56]은 \\(60\\)K 시간의 \\(16\\)KHz 라벨이 없는 음성 데이터와 LibriVox 오디오북에서 약 7000명의 별개의 스피커를 훈련 세트로 포함한다. 지속시간 확산에서는 음소 수준 운율 코드를 조정함으로써 성능을 더욱 향상시킨다. 구체적으로, 사전 양자화된 벡터들에 대해 지속시간에 따른 음소 레벨 풀링을 수행하고, 이러한 음소 레벨 표현들을 코덱의 운율 양자화기에 전달하여 음소 레벨 운율 코드를 획득한다. 추론에서 이를 생성하기 위해 추가 이산 확산을 사용한다. 각 확산 과정에서 \\(4\\) 반복을 수행한다. 분류기 없는 안내 없이 지속시간을 생성하고, 분류기 없는 안내척도(\\(1.0\\)로 다른 것들을 생성한다. 이 방법은 음소 수준의 운율(4\\times 2\\), 지속시간(4\\times 4\\times 2\\), 운율(4\\times 2\\), 내용(내용) 및 음향 세부사항의 각 토큰 시퀀스에 대해 분류기 없는 안내로 인한 이중 계산으로 인해 총 60\\(60\\)의 순방향 통과를 초래한다. FACodec의 자세한 내용은 부록 B.1, 인수분해 확산 모델의 자세한 내용은 부록 A.2를 참조하십시오.\n' +
      '\n' +
      '**평가 데이터세트.** 우리는 두 개의 벤치마크 데이터세트를 사용한다: 1) 제로샷 TTS 작업에 널리 사용되는 테스트세트인 LibriSpeech[57] 테스트클리닝. 40명의 뚜렷한 연사와 5.4시간의 연설이 포함되어 있습니다. [5]에 이어 LibriSpeech 테스트 클린 벤치마크를 위해 각 화자에 대해 한 문장을 무작위로 선택한다. 구체적으로, 동일한 화자의 발화에서 프롬프트로 \\(3\\)-초 클립을 무작위로 선택한다. 2) 24명의 전문배우(여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여, 여 벤치마크는 8가지 다른 감정에 걸쳐 동일한 화자의 동일한 텍스트를 가진 음성 샘플을 제공한다.\n' +
      '\n' +
      '**Evaluation Metrics.** Objective Metrics: Librispeech test-clean benchmark에서 화자 유사도(SIM-O and SIM-R)와 강건도(WER)를 모두 평가한다. 특히, 1) SIM-O 및 SIM-R의 경우 WavLM-TDCNN3 화자 임베딩 모델을 사용하여 생성된 샘플과 프롬프트 간의 화자 유사성을 평가한다. 결과들은 원래 프롬프트(SIM-O) 및 재구성 프롬프트(SIM-R)와의 유사성 모두에 대해 보고된다. 2) 워드 에러율(WER)에 대해, 생성된 음성을 전사하기 위해 ASR 모델4를 사용한다. 이 모델은 라이브러리라잇에서 사전 훈련된 CTC 기반 HuBERT이며 LibriSpeech의 960시간 훈련 세트에 대해 미세 조정된다. RAVDESS 벤치마크에서는 운율 유사성(MCD와 MCD-Acc)을 평가한다. 특히, 1) [59] 다음에 생성된 샘플과 진리 샘플 간의 차이를 측정하여 운율 평가를 위해 멜-켑스트럴 왜곡(Mel-Cepstral Distortion, MCD)을 채택하고, 평균 결과와 함께 8가지 감정에 대한 결과를 보고한다. 2) MCD-Acc를 위해 RAVDESS 검증에서 생성된 음성의 최상위 감정 정확도를 평가한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & Training Data & Sim-O \\(\\uparrow\\) & Sim-R \\(\\uparrow\\) & WER\\(\\downarrow\\) & CMOS\\(\\uparrow\\) & SMOS\\(\\uparrow\\) \\\\ \\hline Ground Truth & - & 0.68 & - & 1.94 & +0.08 & 3.85 \\\\ \\hline VALL-E \\({}^{\\blacktriangledown}\\) & Librilight & - & 0.58 & 5.90 & - & - \\\\ VALL-E \\({}^{\\blacktriangledown}\\) & Librilight & 0.47 & 0.51 & 6.11 & -0.60 & 3.46 \\\\ NaturalSpeech 2\\({}^{\\blacktriangle}\\) & Librilight & 0.55 & 0.62 & 1.94 & -0.18 & 3.65 \\\\ Voicebox\\({}^{\\blacktriangle}\\) & Self-Collected (60kh) & 0.64 & 0.67 & 2.03 & -0.23 & 3.69 \\\\ Voicebox\\({}^{\\blacktriangle}\\) & Librilight & 0.48 & 0.50 & 2.14 & -0.32 & 3.52 \\\\ Mega-TTS 2\\({}^{\\blacktriangle}\\) & Librilight & 0.53 & - & 2.32 & -0.20 & 3.63 \\\\ UniAudio\\({}^{\\blacktriangle}\\) & Mixed (165kh) & 0.57 & 0.68 & 2.49 & -0.25 & 3.71 \\\\ StyleTTS 2\\({}^{\\blacktriangle}\\) & LT + V + LJ & 0.38 & - & 2.49 & -0.21 & 3.07 \\\\ HierSpeech++\\({}^{\\blacktriangle}\\) & LT + LL\\({}^{\\star}\\) + EX + MS + NI & 0.51 & - & 6.33 & -0.41 & 3.50 \\\\ \\hline NaturalSpeech 3 & Librilight & **0.67** & **0.76** & **1.81** & **0.00** & **4.01** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 자연음성 3에 대한 평가 결과 및 LibriSpeech test-clean에 대한 기준 방법. \\ ({}^{\\blacktriangle}\\)는 저자로부터 얻은 결과를 의미한다. \\ ({}^{\\blacktriangleown}\\)는 논문으로부터 직접 얻어진 결과를 의미한다. \\ ({}^{\\blacktriangle}\\)는 결과가 공식 검문소에서 영향을 받는다는 것을 의미한다. \\ ({}^{\\blacktriangleown}\\)는 재현된 결과를 의미한다. 약칭: LT(LibriTTS), V(VCTK), LJ(LJSpeech), LL\\({}^{\\star}\\)(Librilight Small, Medium), EX(Expresso), MS(MSSS Kor), NI(NIKL Kor).\n' +
      '\n' +
      '운율 유사성 측정을 위한 벤치마크입니다. 구체적으로 KNN(K-Nearest-Neighbors) 모델을 감정 분류기로 채택한다. 생성된 음성과 동일한 화자의 지상-진실 음성 사이의 MCD 거리를 8가지 다른 감정에 걸쳐 비교한다. 주관적 척도: 자연성과 유사성을 평가하기 위해 두 벤치마크 모두에 비교 평균 옵션 점수(CMOS)와 유사성 평균 옵션 점수(SMOS)를 사용한다.\n' +
      '\n' +
      '**평가 기준.** NaturalSpeech 3과 기준: 1) VALL-E [6]을 비교한다. 2) NaturalSpeech 2[5] 3) 보이스박스[11] 4) Mega-TTS 2[60]. 5) UniAudio[35] 6) StyleTTS 2[24]. 7) HierSpeech++[25]. 자세한 내용은 부록 A.3을 참조하시기 바랍니다.\n' +
      '\n' +
      'Zero-shot TTS에 대한 실험 결과\n' +
      '\n' +
      '이 부분에서는 자연음성 3을 기준선과 비교한다: 1) 4.2.1절에서 생성 품질, 2) 4.2.2절에서 생성 유사성, 3) 4.2.3절에서 견고성, 구체적으로 생성 유사성에 대해 1) 화자 유사성, 2) 운율 유사성 두 가지 측면에서 평가한다. 대기 시간 분석은 부록 A.4를 참조하십시오.\n' +
      '\n' +
      '######4.2.1 세대품질\n' +
      '\n' +
      '음성 품질을 평가하기 위해 CMOS 테스트를 수행하는데, 이 테스트는 \\(12\\) 네이티브를 판사로 한다. LibriSpeech test-clean과 RAVDESS 벤치마크에서 무작위로 \\(20\\)의 발화를 선택한다. <표 1>에서 볼 수 있듯이, 자연음성 3은 Librispeech test-clean에 대한 Ground-truth recording (\\(-0.08\\), RAVDESS에 대한 \\(-0.17\\)에 가깝다는 것을 알 수 있다. 이는 자연음성 3이 고품질 및 자연음성을 생성할 수 있음을 보여준다; 2) 자연음성 3이 기준선보다 상당한 마진으로 우수한 성능을 보여 인수분해를 통한 자연음성 3의 유효성을 검증한다.\n' +
      '\n' +
      '2.2 세대 유사도\n' +
      '\n' +
      '**Speaker Similarity.** 객관적 메트릭(Sim-O and Sim-R)과 주관적 메트릭(SMOS)으로 음성 유사도를 평가하며, \\(12\\)의 네이티브를 심사위원으로 한다. 우리는 SMOS 테스트를 위해 무작위로 \\(10\\)의 발화를 선택한다. 표 1에 나타난 바와 같이, 1) NaturalSpeech 3은 Sim-O에서 패리티를 달성하고, Ground truth와 함께 SMOS에서 \\(0.16\\) 증가를 보여주며, 이는 제안된 방법에 의해 달성된 큰 화자 유사성을 나타낸다; 2) NaturalSpeech 3은 객관적 메트릭과 주관적 메트릭 모두에서 모든 베이스라인을 능가하여 화자 유사성 측면에서 인수분해와 함께 우리의 방법의 우수성을 강조한다. 또한 Sim-O와 SMOS 사이에 일정한 불일치가 있음을 알 수 있다. 예를 들어, SMOS는 일부 부자연스러운 운율 때문에 보이스박스 모델의 SIM-O만큼 경쟁력이 없다.\n' +
      '\n' +
      '**Prosody Similarity.** RAVDESS 벤치마크에서 객관적 메트릭(MCD 및 MCD-Acc) 및 주관적 메트릭(SMOS) 둘 다로 운율 유사성을 평가한다. 우리는 SMOS 테스트를 위해 무작위로 \\(10\\)의 발화를 선택한다. <표 2>에서 보는 바와 같이 NaturalSpeech 3은 MCD avg, MCD-Acc, SMOS에서 괄목할 만한 마진만큼 꾸준히 기준선을 능가하고 있다. 내츄럴 스피치 3은 운율적 유사성 측면에서 상당한 개선을 달성함을 드러낸다. 감정에 걸친 MCD 점수는 부록 A.6을 참조하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Avg\\(\\downarrow\\) & Acc\\(\\uparrow\\) & CMOS\\(\\uparrow\\) & SMOS\\(\\uparrow\\) \\\\ \\hline Ground Truth & 0.00 & 1.00 & +0.17 & 4.42 \\\\ \\hline VALL-E \\(\\lx@notemark{\\bullet}\\) & 5.03 & 0.34 & -0.55 & 3.80 \\\\ NaturalSpeech 2\\(\\lx@notemark{\\bullet}\\) & 4.56 & 0.25 & -0.22 & 4.04 \\\\ Voicebox\\(\\lx@notemark{\\bullet}\\) & 4.88 & 0.34 & -0.34 & 3.92 \\\\ Mega-TTS 2\\(\\lx@notemark{\\bullet}\\) & 4.44 & 0.39 & -0.20 & 4.51 \\\\ StyleTTS 2\\(\\lx@notemark{\\bullet}\\) & 4.50 & 0.40 & -0.25 & 3.98 \\\\ HierSpeech++\\(\\lx@notemark{\\bullet}\\) & 6.08 & 0.30 & -0.37 & 3.87 \\\\ \\hline NaturalSpeech 3 & **4.28** & **0.52** & **0.00** & **4.72** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: NaturalSpeech 3에 대한 평가 결과 및 RAVDESS에 대한 기준선 방법. \\ (\\lx@notemark{\\bullet}\\)는 저자로부터 얻은 결과를 의미한다. \\\\ (\\lx@notemark{\\bullet}\\)는 공식 검문소에서 추론한 결과를 의미한다. \\\\ (\\lx@notemark{\\bullet}\\)는 재현된 결과를 의미한다. 약칭: Avg(평균 MCD), Acc(MCD-Acc).\n' +
      '\n' +
      '#### 4.2.3 Robustness\n' +
      '\n' +
      'LibriSpeech 테스트 클린 벤치마크에서 생성된 음성의 단어 오류율을 측정하여 제로 샷 TTS의 견고성을 평가한다. 표 1의 결과는 1) NaturalSpeech 3이 Ground truth보다 더 나은 WER을 달성하여 높은 명료도를 증명하고; 2) NaturalSpeech 3이 다른 기준선보다 상당한 마진만큼 우수한 성능을 보여 NaturalSpeech 3의 우수한 견고성을 보여준다.\n' +
      '\n' +
      '### 절제 연구 및 방법 분석\n' +
      '\n' +
      '1 절제 연구\n' +
      '\n' +
      '이 하위 섹션에서는 1) 인수분해, 2) 품격 없는 안내, 3) 운율 표현의 효과를 검증하기 위해 절제 연구를 수행한다. 또한 부록 A.5에서 지속 기간 확산 모델과 전통적인 지속 기간 예측 변수를 비교하기 위해 절제 연구를 수행한다.\n' +
      '\n' +
      '**Factorization.** 제안된 인수분해 방법을 검증하기 위해 코덱과 인수분해 확산 모델 모두에서 인수분해를 제거하여 제거한다. 구체적으로, 우리는 1) 소인수분해를 고려하지 않는 신경 코덱인 사운드스트림의 이산 토큰을 사용하고, 2) 소인수분해를 생성에서 고려하지 않는다. 표 3에 나타난 바와 같이, 우리는 소인수분해, Sim-O에서 \\(0.12\\), Sim-R에서 \\(0.15\\), WER에서 \\(0.68\\), CMOS에서 \\(0.25\\), SMOS에서 0.42의 감소 없이 상당한 성능 저하를 발견할 수 있었다. 이는 제안된 요인화 방법이 화자 유사성, 견고성 및 품질 측면에서 일관되게 성능을 향상시킬 수 있음을 나타낸다.\n' +
      '\n' +
      '**Classier-Free Guidance.** 분류기-free Guidance를 추론에 드롭하여 그 유효성을 검증함으로써 절제 연구를 수행한다. 우리는 공정한 비교를 위해 동일한 \\(60\\) 순방향 패스를 보장하기 위해 반복을 두 배로 한다. 표 3은 분류기 없는 안내 없이, Sim-O의 경우 \\(0.03\\), Sim-R의 경우 \\(0.04\\), CMOS의 경우 \\(0.06\\), SMOS의 경우 \\(0.21\\)의 현저한 저하를 보여주며, 분류기 없는 안내는 화자 유사도와 품질에 큰 도움이 될 수 있음을 증명한다.\n' +
      '\n' +
      '**Prosody Representation.** Zero-shot TTS 태스크에서 서로 다른 운율 표현을 비교한다. 특히, 우리는 손으로 조작한 운율 특징(예: 멜-스펙트로그램[7, 61, 62])을 기준선으로 선택한다. 운율 FVQ 모듈을 삭제하고 정규화된 F0 손실 없이 멜-스펙트로그램의 처음 20개의 빈을 직접 양자화한다. 표 4는 "Mel 20 Bins"를 운율 표현으로 사용하는 것이 코덱(평균 MCD 4.34 vs 4.28, MCD-Acc 0.46 vs 0.52)에서 학습된 운율 표현과 비교하여 운율 유사성 측면에서 열세를 보인다는 것을 보여준다.\n' +
      '\n' +
      '###### 4.3.2 방법 분석\n' +
      '\n' +
      '이 하위 섹션에서는 먼저 인수분해의 확장성에 대해 논의한다. 그런 다음 제로 샷 방식으로 음성 속성 조작의 적용을 소개한다.\n' +
      '\n' +
      '**확장성.** 자연 음성 3은 인수분해 설계를 가진 이산 토큰 생성을 위해 비자동 회귀 모델을 활용한다. 제안된 인수분해 방법의 확장성을 검증하기 위해 우리는 더 나아가\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & MCD Avg\\(\\downarrow\\) & MCD-Acc\\(\\uparrow\\) \\\\ \\hline NaturalSpeech 3 & **4.28** & **0.52** \\\\ Mel 20 Bins & 4.34 & 0.46 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: RAVDESS에 대한 운율 표현의 절제 연구. 멜-스펙트로그램에서 처음 20개의 빈을 운율 표현으로 사용하여 "멜 20개의 빈"을 노팅한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Sim-O / Sim-R \\(\\uparrow\\) & WER\\(\\downarrow\\) & CMOS\\(\\uparrow\\) & SMOS\\(\\uparrow\\) \\\\ \\hline NaturalSpeech 3 & **0.67 / 0.76** & **1.81** & **0.00** & **4.01** \\\\ \\hline - factorization & 0.55 / 0.61 & 2.49 & -0.25 & 3.59 \\\\ - cfg & 0.64 / 0.72 & **1.81** & -0.06 & 3.80 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LibriSpeech test-clean에 대한 인수분해 및 분류기 없는 안내(cfg)의 절제 연구.\n' +
      '\n' +
      '인수분해 프레임워크에서 이산 토큰 생성을 위한 자기회귀 생성 모델을 탐색한다. 우리는 검증을 위해 VALL-E를 활용한다. 먼저 운율 코드를 생성하기 위해 자기회귀 언어 모델을 사용하고, 나머지 콘텐츠 및 음향 세부 코드를 생성하기 위해 비자기회귀 모델을 사용한다. 이 접근법은 속성 생성의 일관된 순서를 유지하여 공정한 비교를 허용한다. VALL-E + F라고 명명한다. 표 6에 나타난 바와 같이, VALL-E + F는 모든 객관적 및 주관적 메트릭에서 VALL-E보다 상당한 마진만큼 일관되게 능가하며, 인수분해 설계가 음성 유사성, 품질 및 생성 견고성에서 VALL-E를 향상시킬 수 있음을 보여준다. 또한 제안된 인수분해 확산 모델에서 우리의 인수분해 패러다임이 제한되지 않으며 다른 생성 모델에서도 큰 잠재력을 가지고 있음을 보여준다. 우리는 미래의 일을 위해 그것을 남겨둔다.\n' +
      '\n' +
      '**Speech Attribute Manipulation.** 섹션 3.3에서 논의된 바와 같이, 우리의 인수분해 확산 모델은 상이한 스피치로부터 상이한 속성 프롬프트를 선택함으로써 속성 조작을 가능하게 한다. 우리는 콘텐츠 코드가 TTS의 텍스트에 의해 지시되고 음향 세부 정보가 의미 정보를 전달하지 않기 때문에 주로 지속 시간, 운율 및 음색을 조작하는 데 중점을 둔다. 내츄럴 스피치 3의 강한 컨텍스트 내 능력을 활용하여, 생성된 스피치는 대응하는 스피치 속성들을 효과적으로 반영한다. 예를 들어, 1) 다른 음성의 음색 프롬프트를 사용하여 다른 속성을 변경하지 않고 음색을 제어할 수 있다; 2) 지속 시간과 운율 사이의 상관 관계에도 불구하고, 우리는 여전히 속도를 조절하기 위해 지속 시간 프롬프트를 단독으로 조정할 수 있다; 3) 더 나아가 원하는 대로 이질적인 샘플로부터 다른 음성 속성을 결합할 수 있다. 이를 통해 다양한 운율과 음성 속도를 사용하면서 음색을 모방할 수 있습니다. 샘플은 데모 5페이지에 나와 있습니다.\n' +
      '\n' +
      '각주 5: [https://speechresearch.github.io/naturalspeech3](https://speechresearch.github.io/naturalspeech3)\n' +
      '\n' +
      'FACodec의 실험결과 4.3.3\n' +
      '\n' +
      '제안된 FACodec을 EnCodec[15], HiFi-Codec[63], Descript-Audio-Codec[32], 그리고 재생된 SoundStream[14]과 같은 강력한 베이스라인들과 재구성 품질 측면에서 비교한다. 표 5는 동일한 대역폭 설정(PESQ에서\\(0.44\\), STOI에서 \\(0.05\\), MSTFT에서 \\(0.14\\), MCD에서 \\(0.79\\))에서 우리의 코덱이 사운드스트림을 크게 능가한다는 것을 보여준다. 부록 B.2에서 자세한 내용을 확인하십시오. 다른 기준선과 비교하여 FACodec도 비슷한 성능을 얻습니다. 또한, 본 논문에서 제안하는 코덱은 음색 정보를 복호화하기 때문에 제로샷 음성 변환을 쉽게 할 수 있기 때문에 부록 B.3에서 상세한 내용과 실험 결과를 제공한다. 부록 B.4는 FACodec에 대한 몇 가지 절제 연구를 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Sim-O / R \\(\\uparrow\\) & WER\\(\\downarrow\\) & CMOS\\(\\uparrow\\) & SMOS\\(\\uparrow\\) \\\\ \\hline VALL-E + F & **0.57 / 0.65** & **5.60** & **+0.24** & **3.61** \\\\ VALL-E\\({}^{\\blacklozenge}\\) & 0.47 / 0.51 & 6.11 & 0.00 & 3.46 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: LibriSpeech test-clean에 대해 제안된 인수분해를 사용한 (VALL-E + F)와 사용하지 않은 (VALL-E)의 자기회귀 접근법의 비교. \\ ({}^{\\blacklozenge}\\)는 재현된 결과를 의미한다. 약칭: Sim-O/R(Sim-O/Sim-R)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Models & Sampling Rate & H & N & Bandwidth & PESQ \\(\\uparrow\\) & STOI \\(\\uparrow\\) & MSTFT \\(\\downarrow\\) & MCD \\(\\downarrow\\) \\\\ \\hline EnCodec\\({}^{\\blacklozenge}\\) & 24kHz & 320 & 8 & 6.0 kbps & 3.28 & 0.94 & 0.99 & 2.70 \\\\ HiFi-Codec\\({}^{\\blacklozenge}\\) & 16kHz & 320 & 4 & 2.0 kbps & 3.17 & 0.93 & 0.98 & 3.05 \\\\ DAC\\({}^{\\blacklozenge}\\) & 16kHz & 320 & 9 & 4.5 kbps & **3.52** & **0.95** & 0.97 & 2.65 \\\\ SoundStream\\({}^{\\blacklozenge}\\) & 16kHz & 200 & 6 & 4.8 kbps & 3.03 & 0.90 & 1.07 & 3.38 \\\\ \\hline FACodec & 16kHz & 200 & 6 & 4.8 kbps & 3.47 & **0.95** & 0.93 & **2.59** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 코덱의 재구성 품질 평가. \\ ({}^{\\blacklozenge}\\)는 결과가 외부의 검문소에서 영향을 받는다는 것을 의미한다. \\ ({}^{\\blacklozenge}\\)는 재현된 검문소를 의미한다. \\ ({}^{\\blacklozenge}\\)는 원본 논문의 구현 및 실험 설정에 따라 재현된 모델을 의미한다. 모든 모델은 \\(1024\\)의 코드북 크기를 사용한다. 가장 좋은 결과에 대해서는 과감하게, 두 번째로 좋은 결과에 대해서는 밑줄을 긋는다. 약칭: H(Hop Size), N(Codebook Number).\n' +
      '\n' +
      '### 데이터 및 모델 스케일링의 효과\n' +
      '\n' +
      '본 절에서는 제안된 인수분해 확산 모델에 대한 데이터 및 모델 스케일링의 효과를 연구한다. 우리는 공정한 비교를 위해 LibriLight 데이터 세트에 대해 훈련된 동일한 FACodec을 사용한다. 본 논문에서는 30\\(30\\) 오디오 클립으로 구성된 내부 테스트 세트에서 화자 유사성(Sim-O)과 강건성(WER) 측면에서 제로샷 TTS 성능을 평가한다.\n' +
      '\n' +
      '**Data Scaling.** 500M 매개변수의 고정 모델 크기로 1) Librilight 데이터셋에서 무작위로 추출한 1K 시간 하위 집합, 2) 60K 시간 Librilight 데이터셋, 3) 200K 시간 음성 내부 데이터셋의 세 가지 데이터셋에 대해 인수분해 확산 모델을 훈련시켰다. 표 7에서 우리는 1) 1K 시간의 음성 데이터에서도 Sim-O 점수(0.69\\)와 WER 점수(3.39\\)를 얻었다. 그것은 음성 인수분해를 통해 NaturalSpeech 3이 효과적으로 음성을 생성할 수 있음을 보여준다. 2) 훈련 데이터를 1K 시간에서 60K 시간으로 확장한 후 200K 시간으로 확장함에 따라 NaturalSpeech 3은 Sim-O 측면에서 \\(0.03\\)과 \\(0.04\\), WER 측면에서 \\(0.33\\)과 \\(0.56\\)의 개선으로 지속적으로 향상된 성능을 보여 데이터 스케일링의 이점을 확인하였다. 200K 시간에 훈련된 우리의 방법은 여전히 적합하지 않으며 더 긴 훈련은 더 나은 성능을 초래할 것이다.\n' +
      '\n' +
      '**모델 스케일링.** 내부 200K 시간 데이터 세트를 사용하여 모델 크기를 500M에서 1B 매개변수로 확장한다. 특히, 변압기 층의 수를 \\(12\\)에서 \\(24\\)으로 두 배로 늘렸다. 표 8의 결과는 Sim-O에서 화자 유사성(\\(0.02\\)과 WER에서 강건성(\\(0.21\\) 모두 증가되어 모델 스케일링의 유효성을 검증한다. 앞으로 더 나은 결과를 얻기 위해 모델 크기를 훨씬 더 크게 확장할 것입니다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 1) 음성 파형을 콘텐츠, 운율, 음향 상세 및 음색의 별개의 부공간으로 분해하기 위해 인자화된 벡터 양자화(FACodec)를 갖는 새로운 신경 음성 코덱과 2) 이산 확산의 부공간에 속성을 생성하여 음성을 합성하기 위한 새로운 인자화된 확산 모델로 구성된 TTS 시스템을 개발한다. 자연 음성 3은 음성 품질, 유사성, 운율 및 명료도에서 최첨단 TTS 시스템을 능가한다. 또한 내츄럴 스피치 3은 스피치 속성 프롬프트를 커스터마이징함으로써 스피치 속성 조작을 가능하게 할 수 있음을 보여준다. 또한, NaturalSpeech 3은 1B 파라미터와 200K 시간의 훈련 데이터로 스케일링함으로써 더 나은 성능을 달성함을 입증한다. 우리는 부록 C의 한계와 향후 작업을 나열한다.\n' +
      '\n' +
      '## 6 보더 임팩트\n' +
      '\n' +
      '우리의 모델은 화자 유사성이 큰 음성을 합성할 수 있기 때문에 음성 식별을 스푸핑하거나 특정 화자를 사칭하는 것과 같은 모델의 오남용에 잠재적인 위험을 초래할 수 있다. 음성 합성에서 사용자가 목표 화자가 되는 것에 동의한다는 가정 하에 실험을 수행하였다. 오남용을 방지하기 위해서는 강력한 합성 음성 검출 모델을 개발하고 개인이 오남용이 의심되는 경우 신고할 수 있는 시스템을 구축하는 것이 중요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline  & Sim-O\\(\\uparrow\\) & WER\\(\\downarrow\\) \\\\ \\hline\n' +
      '1K&0.69&3.39\\\\\n' +
      '60K&0.72&3.03\\\\\n' +
      '200K & **0.73** & **2.83** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 내부 테스트 세트에 대한 내츄럴 스피치 3의 성능, 500M 모델 크기 및 상이한 시간의 트레이닝 데이터를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline  & Sim-O\\(\\uparrow\\) & WER\\(\\downarrow\\) \\\\ \\hline\n' +
      '500M & 0.73 & 2.83 \\\\\n' +
      '1B & **0.75** & **2.62** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 200K 시간의 훈련 데이터와 다양한 모델 크기를 가진 내부 테스트 세트에 대한 NaturalSpeech 3의 성능.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. _Proc. Interspeech 2017_, pages 4006-4010, 2017.\n' +
      '* [2] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In _2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4779-4783. IEEE, 2018.\n' +
      '* [3] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech: Fast, robust and controllable text to speech. In _NeurIPS_, 2019.\n' +
      '* [4] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. NaturalSpeech: End-to-end text to speech synthesis with human-level quality. _arXiv preprint arXiv:2205.04421_, 2022.\n' +
      '* [5] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. _arXiv preprint arXiv:2304.09116_, 2023.\n' +
      '* [6] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. _arXiv preprint arXiv:2301.02111_, 2023.\n' +
      '* [7] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, et al. Mega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias. _arXiv preprint arXiv:2306.03509_, 2023.\n' +
      '* [8] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. _arXiv preprint arXiv:2106.06103_, 2021.\n' +
      '* [9] Dan Lim, Sunghee Jung, and Eesung Kim. Jets: Jointly training fastspeech2 and hifi-gan for end to end text to speech. _arXiv preprint arXiv:2203.16852_, 2022.\n' +
      '* [10] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: A diffusion probabilistic model for text-to-speech. _arXiv preprint arXiv:2105.06337_, 2021.\n' +
      '* [11] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. _arXiv preprint arXiv:2306.15687_, 2023.\n' +
      '* [12] Zalan Borosos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling approach to audio generation. _arXiv preprint arXiv:2209.03143_, 2022.\n' +
      '* [13] Zalan Borosos, Matt Sharifi, Damien Vincent, Eugene Kharitonov, Neil Zeghidour, and Marco Tagliasacchi. Soundstorm: Efficient parallel audio generation. _arXiv preprint arXiv:2305.09636_, 2023.\n' +
      '* [14] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2021.\n' +
      '* [15] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. _arXiv preprint arXiv:2210.13438_, 2022.\n' +
      '* [16] Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, and David Cox. Unsupervised speech decomposition via triple information bottleneck. In _International Conference on Machine Learning_, pages 7836-7846. PMLR, 2020.\n' +
      '\n' +
      '* [17] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-Johnson. AutoVC: Zero-shot voice style transfer with only autoencoder loss. In _International Conference on Machine Learning_, pages 5210-5219. PMLR, 2019.\n' +
      '* [18] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. _Advances in Neural Information Processing Systems_, 33, 2020.\n' +
      '* [19] Eugene Kharitonov, Damien Vincent, Zalan Boros, Raphael Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. _arXiv preprint arXiv:2302.03540_, 2023.\n' +
      '* [20] Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue Jiang, Chao Weng, Zhou Zhao, and Dong Yu. Make-a-voice: Unified voice synthesis with discrete representation. _arXiv preprint arXiv:2305.19269_, 2023.\n' +
      '* [21] Dongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and Dong Yu. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt. _arXiv preprint arXiv:2301.13662_, 2023.\n' +
      '* [22] Chenpeng Du, Yiwei Guo, Feiyu Shen, Zhijun Liu, Zheng Liang, Xie Chen, Shuai Wang, Hui Zhang, and Kai Yu. Unicats: A unified context-aware text-to-speech framework with contextual vq-diffusion and vocoding. _arXiv preprint arXiv:2306.07547_, 2023.\n' +
      '* [23] Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Lms with a voice: Spoken language modeling beyond speech tokens. _arXiv preprint arXiv:2305.15255_, 2023.\n' +
      '* [24] Yinghao Aaron Li, Cong Han, Vinay S Raghavan, Gavin Mischler, and Nima Mesgarani. Sflylets 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. _arXiv preprint arXiv:2306.07691_, 2023.\n' +
      '* [25] Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, and Seong-Whan Lee. Hierspeech++: Bridging the gap between semantic and acoustic representation of speech by hierarchical variational inference for zero-shot speech synthesis. _arXiv preprint arXiv:2311.12454_, 2023.\n' +
      '* [26] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.\n' +
      '* [27] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel WaveNet: Fast high-fidelity speech synthesis. In _International conference on machine learning_, pages 3918-3926. PMLR, 2018.\n' +
      '* [28] Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio. Char2wav: End-to-end speech synthesis. 2017.\n' +
      '* [29] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman, and John Miller. Deep Voice 3: 2000-speaker neural text-to-speech. _Proc. ICLR_, pages 214-217, 2018.\n' +
      '* [30] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with Transformer network. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 6706-6713, 2019.\n' +
      '* [31] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-TTS: A generative flow for text-to-speech via monotonic alignment search. _Advances in Neural Information Processing Systems_, 33, 2020.\n' +
      '* [32] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. _arXiv preprint arXiv:2306.06546_, 2023.\n' +
      '\n' +
      '* [33] Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, Ron Weiss, and Yonghui Wu. Parallel Tacotron: Non-autoregressive and controllable TTS. _arXiv preprint arXiv:2010.11439_, 2020.\n' +
      '* [34] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. DiffSinger: Singing voice synthesis via shallow diffusion mechanism. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 11020-11028, 2022.\n' +
      '* [35] Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foundation model toward universal audio generation. _arXiv preprint arXiv:2310.00704_, 2023.\n' +
      '* [36] Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee, Hoon Heo, and Kyogu Lee. Neural analysis and synthesis: Reconstructing speech from self-supervised representations. _Advances in Neural Information Processing Systems_, 34:16251-16265, 2021.\n' +
      '* [37] Hyeong-Seok Choi, Jinhyeok Yang, Juheon Lee, and Hyeongju Kim. Nansy++: Unified voice synthesis with neural analysis and synthesis. _arXiv preprint arXiv:2211.09407_, 2022.\n' +
      '* [38] Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations. _arXiv preprint arXiv:2104.00355_, 2021.\n' +
      '* [39] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In _2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)_, pages 244-250. IEEE, 2021.\n' +
      '* [40] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. _Advances in Neural Information Processing Systems_, 33:12449-12460, 2020.\n' +
      '* [41] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. _Proc. Interspeech 2019_, pages 3465-3469, 2019.\n' +
      '* [42] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models. _arXiv preprint arXiv:2308.16692_, 2023.\n' +
      '* [43] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 29:3451-3460, 2021.\n' +
      '* [44] Xue Jiang, Xiulian Peng, Yuan Zhang, and Yan Lu. Disentangled feature learning for real-time neural speech coding. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.\n' +
      '* [45] Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, sheng zhao, and Tie-Yan Liu. AdaSpeech: Adaptive text to speech for custom voice. In _International Conference on Learning Representations_, 2021.\n' +
      '* [46] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. _arXiv preprint arXiv:2110.04627_, 2021.\n' +
      '* [47] SiCheng Yang, Methawee Tantrawenith, Haolin Zhuang, Zhiyong Wu, Aolan Sun, Jianzong Wang, Ning Cheng, Huaizhen Tang, Xintao Zhao, Jie Wang, et al. Speech representation disentanglement with adversarial mutual information learning for one-shot voice conversion. _arXiv preprint arXiv:2208.08757_, 2022.\n' +
      '* [48] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In _International conference on machine learning_, pages 1180-1189. PMLR, 2015.\n' +
      '* [49] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. _arXiv preprint arXiv:2210.08933_, 2022.\n' +
      '\n' +
      '* [50] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. _arXiv preprint arXiv:2209.14687_, 2022.\n' +
      '* [51] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10696-10706, 2022.\n' +
      '* [52] Jose Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with token-critic. In _European Conference on Computer Vision_, pages 70-86. Springer, 2022.\n' +
      '* [53] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [54] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [55] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 5404-5411, 2024.\n' +
      '* [56] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazare, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: A benchmark for asr with limited or no supervision. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7669-7673. IEEE, 2020.\n' +
      '* [57] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an ASR corpus based on public domain audio books. In _2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5206-5210. IEEE, 2015.\n' +
      '* [58] Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english. _PloS one_, 13(5):e0196391, 2018.\n' +
      '* [59] Mohammed Salah Al-Radhi, Tamas Gabor Csapo, and Geza Nemeth. Nonparallel expressive tts for unseen target speaker using style-controlled adaptive layer and optimized pitch embedding. In _2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)_, pages 176-181. IEEE, 2023.\n' +
      '* [60] Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, et al. Mega-tts 2: Zero-shot text-to-speech with arbitrary length speech prompts. _arXiv preprint arXiv:2307.07218_, 2023.\n' +
      '* [61] Hyung-Seok Oh, Sang-Hoon Lee, and Seong-Whan Lee. Diffprosody: Diffusion-based latent prosody generation for expressive speech synthesis with prosody conditional adversarial training. _arXiv preprint arXiv:2307.16549_, 2023.\n' +
      '* [62] Yi Ren, Ming Lei, Zhiying Huang, Shiliang Zhang, Qian Chen, Zhijie Yan, and Zhou Zhao. Prosospeech: Enhancing prosody with quantized vector pre-training in text-to-speech. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 7577-7581. IEEE, 2022.\n' +
      '* [63] Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. Hifi-codec: Group-residual vector quantization for high fidelity audio codec. _arXiv preprint arXiv:2305.02765_, 2023.\n' +
      '* [64] Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Tao Qin, and Tie-Yan Liu. Token-level ensemble distillation for grapheme-to-phoneme conversion. In _INTERSPEECH_, 2019.\n' +
      '\n' +
      '* [65] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11315-11325, 2022.\n' +
      '* [66] Takaaki Saeki, Detai Xin, Wataru Nakata, Tomoki Koriyama, Shinnosuke Takamichi, and Hiroshi Saruwatari. Utmos: Utokyo-sarulab system for voicemos challenge 2022. _arXiv preprint arXiv:2204.02152_, 2022.\n' +
      '* [67] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. Bigvgan: A universal neural vocoder with large-scale training. _arXiv preprint arXiv:2206.04658_, 2022.\n' +
      '* [68] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech recognition. _arXiv preprint arXiv:2005.08100_, 2020.\n' +
      '* [69] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, pages 6309-6318, 2017.\n' +
      '* [70] Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Golge, and Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In _International Conference on Machine Learning_, pages 2709-2720. PMLR, 2022.\n' +
      '* [71] Zhichao Wang, Yuanzhe Chen, Lei Xie, Qiao Tian, and Yuping Wang. Lm-vc: Zero-shot voice conversion via speech generation based on language models. _arXiv preprint arXiv:2306.10521_, 2023.\n' +
      '\n' +
      '인수분해 확산 모델의 상세\n' +
      '\n' +
      '### Model Configuration\n' +
      '\n' +
      '음소 인코더는 [5]와 유사한 구조를 사용하며, 8개의 어텐션 헤드를 갖는 6-레이어 트랜스포머, 임베딩 차원, 1D 컨볼루션에 대한 필터 크기\\(2048\\) 및 커널 크기\\(9\\) 및 드롭아웃(0.1\\)으로 구성된다. 운율, 내용 및 음향 상세 확산에서, 우리는 1D 컨볼루션에 대한 \\(8\\) 주의 헤드, \\(1024\\) 임베딩 치수, 필터 크기 \\(2048\\) 및 커널 크기 \\(3\\) 및 드롭아웃이 \\(0.1\\)인 공유 \\(12\\) 레이어 트랜스포머를 채택한다. 확산 시간 입력을 지원하기 위해 각 트랜스포머 블록에서 조건부 계층 정규화를 추가로 사용한다. 음소 레벨 운율 및 지속시간 확산에서, 우리는 1D 컨볼루션에 대한 \\(8\\) 주의 헤드, \\(1024\\) 임베딩 차원, 필터 크기 \\(2048\\) 및 커널 크기 \\(3\\) 및 드롭아웃이 \\(0.1\\)인 6-계층 트랜스포머를 채택한다. 또한 확산 시간 입력을 지원하기 위해 모델에서 조건부 계층 정규화를 사용한다.\n' +
      '\n' +
      '### 훈련 및 추론 세부사항\n' +
      '\n' +
      '본 논문에서는 LibriVox 오디오북에서 추출된 7000명의 화자 및 16KHz의 레이블이 없는 음성 데이터를 포함하는 Librilight [56]을 훈련 세트로 사용한다. 우리는 내부 ASR 시스템을 사용하여 전사하고, 자소 대 음소 변환을 통해 전사본을 음소로 변환하고[64], 내부 정렬 도구로 지속 시간을 얻는다. 우리는 \\(1\\)M 단계를 위해 GPU당 잠재 벡터의 배치 크기가 \\(10\\)K인 \\(8\\) A100 80GB GPU를 사용한다. 우리는 역제곱근 학습 스케줄에 따라 \\(1e-4\\), \\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.98\\), \\(5\\)K 워밍업 단계의 학습률을 갖는 AdamW 최적화기를 사용한다.\n' +
      '\n' +
      '추론 과정에서 음소 수준 운율, 지속 시간, 운율, 내용 및 음향 세부 확산을 포함하여 각 확산 과정에서 \\(4\\) 반복을 수행한다. 우리는 분류기 없는 안내 없이 지속 시간을 생성하고, 분류기 없는 안내 척도는 \\(1.0\\)으로 다른 것들을 생성한다. 이 방법은 음소 단위의 운율(4\\times 2\\), 지속시간(4\\times 4\\times 2\\), 운율(4\\times 2\\), 내용 및 음향 세부사항의 각 토큰 시퀀스에 대해 분류기 없는 안내를 통한 이중 계산으로 인해 총 60\\(60\\)의 순방향 통과를 수행한다. 샘플링 온도 어닐링을 \\(1.5\\)에서 \\(0\\)으로 하여 \\(20\\)의 top-k를 사용한다. [65]에 이어 섹션 3.3.2에서 언급한 \\(q(\\mathbf{X}_{t-\\Delta t}|\\mathbf{\\hat{X}_{0},\\mathbf{X}_{t})\\)에서 어떤 위치를 재마스크할지 결정할 때 검벨 노이즈가 토큰 신뢰에 추가된다.\n' +
      '\n' +
      '### Evaluation Baselines\n' +
      '\n' +
      '내추럴 스피치 3과 강력한 제로 샷 TTS 기준선을 비교한다.\n' +
      '\n' +
      '* VALL-E[6] 이산형 토큰 생성을 위해 자기회귀 모델과 추가적인 비자기회귀 모델을 사용한다. 우리는 신문에서 직접 얻은 점수를 보고한다. 우리는 또한 그것을 Libraryilight에서 사운드스트림의 이산 토큰을 사용하여 재생산한다.\n' +
      '* NaturalSpeech 2[5] 연속 벡터 생성을 위해 비자동 회귀 모델을 사용한다. 우리는 저자와의 소통을 통해 샘플을 얻는다.\n' +
      '* 보이스박스[11] 연속 벡터 생성을 위해 비자동 회귀 모델을 사용한다. 우리는 저자와의 소통을 통해 샘플을 얻는다. 우리는 또한 그것을 Librilight의 mel-spectrogram을 사용하여 재생산한다.\n' +
      '*메가-TTS 2 [60] 연속 벡터 생성을 위해 비자동 회귀 모델을 사용한다. 우리는 저자와의 소통을 통해 샘플을 얻는다.\n' +
      '* UniAudio[35] 이산형 토큰 생성을 위해 자기회귀 모델을 사용한다. 우리는 저자와의 소통을 통해 샘플을 얻는다.\n' +
      '* StyleTTS 2[24]. 연속 벡터 생성을 위해 비자동 회귀 모델을 사용한다. 우리는 공식 코드와 체크포인트 6을 사용한다. 각주 6: [https://github.com/yl4579/StyleTTS2](https://github.com/yl4579/StyleTTS2)\n' +
      '* HierSpeech++[25]. 연속 벡터 생성을 위해 비자동 회귀 모델을 사용한다. 우리는 공식 코드와 체크포인트7을 사용하며, 공정한 비교를 위해 슈퍼 해상도 모델을 사용하지 않는다.\n' +
      '\n' +
      '각주 7: [https://github.com/sh-lee-prml/HierSpeechpp](https://github.com/sh-lee-prml/HierSpeechpp)\n' +
      '\n' +
      '### Latency Analysis\n' +
      '\n' +
      '이 하위 섹션에서는 자연 음성 3의 추론 지연 시간을 자기 회귀 방법(VALL-E) 및 비자기 회귀 방법(NaturalSpeech 2)과 비교한다. 또한 각 확산에서 반복 횟수를 4에서 1로 줄여 총 15번의 전진 통과를 초래하는 효과를 조사한다. 우리는 이 변종 NaturalSpeech 3을 1단계라고 부른다. Librispeech test-clean에 대한 성능을 화자 유사성(Sim-O/Sim-R)과 품질(UTMOS [66]8, CMOS의 대용 객관적 메트릭) 측면에서 평가한다. 대기 시간 테스트는 E5-2690 인텔 제온 CPU, 512GB 메모리, NVIDIA V100 GPU 1개를 갖춘 서버에서 수행된다. 그 결과는 표 9에 나와 있다. 그 결과로부터 우리는 몇 가지 관측치를 가지고 있다. 1) NaturalSpeech 3은 VALL-E보다 \\(15.27\\times\\)의 속도향상을, NaturalSpeech 2보다 \\(1.24\\times\\)의 속도향상을 달성하는 반면, 모든 메트릭에서 이러한 기준선을 지속적으로 능가한다. 이것은 NaturalSpeech 3이 효과적이고 효율적이라는 것을 보여준다. 2) 더 적은 확산 스텝을 사용할 때, NaturalSpeech 3은 여전히 Sim-O에서 (\\(-0.01\\), Sim-R에서 (\\(-0.01\\), UTMOS에서 (\\(-0.29\\)의 강인한 성능을 유지할 수 있으며, \\(4.41\\times\\) 더 빠른 속도로 확산 스텝의 강인성을 증명한다.\n' +
      '\n' +
      '각주 8: [https://github.com/tarepan/SpeechMOS](https://github.com/tarepan/SpeechMOS)\n' +
      '\n' +
      'Duration Diffusion Model을 이용한### Ablation 연구\n' +
      '\n' +
      '이 하위 섹션에서는 로그 도메인에서 지속 시간을 회귀하는 기존 지속 시간 예측 변수와 지속 시간 이산 확산 모델을 비교하기 위해 절제 연구를 수행한다. 절제 연구는 1) 세대: 다단계 세대 대 세대에 초점을 맞춘다. 1단계 생성. 2) 목적: 분류 기반 교차 엔트로피 손실 vs. 회귀 기반 L2 손실. 3) 컨디셔닝: with vs. 음소 수준의 운율 조절 없이. 4) 프롬프트:with vs. 기간 프롬프트가 없습니다. 이를 Librispeech test-clean을 이용하여 화자 유사성(Sim-O/Sim-R), 강건성(WER) 및 품질(UTMOS) 측면에서 평가한다. 표 10에 나타난 바와 같이, 1) 다단계 생성이 없는 경우, 유의한 성능 하락(-Sim-O에서 0.05, Sim-R에서 -0.03, UTMOS에서 -0.12)이 있음을 알 수 있다. 2) 교차 엔트로피 손실을 l2 손실로 대체하는 것은 성능에 영향을 주어 Sim-O에서 -0.05, Sim-R에서 -0.04, WER에서 0.44, UTMOS에서 -0.17의 감소를 유발한다. 3) 음소 수준 운율 조절은 화자 유사성(-Sim-O에서 0.05, Sim-R에서 -0.04), 강건성(WER에서 0.55) 및 품질(-UTMOS에서 0.19) 모두에 영향을 미칠 것이다. 4) 지속시간 프롬프트 메커니즘은 화자 유사성, 견고성 및 품질에 결정적이며, Sim-O에서 -0.06, Sim-R에서 -0.05, WER에서 0.89, UTMOS에서 -0.22의 변화가 있다. 이러한 결과는 지속시간 예측기의 각 설계 측면이 성능 향상에 기여함을 확인시켜준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Sim-O \\(\\uparrow\\) & Sim-R \\(\\uparrow\\) & WER\\(\\downarrow\\) & UTMOS\\(\\uparrow\\) \\\\ \\hline NaturalSpeech 3 & **0.67** & **0.76** & **1.94** & **4.30** \\\\ \\hline Generation ablation & 0.62 & 0.73 & **1.94** & 4.18 \\\\ Objective ablation & 0.62 & 0.72 & 2.38 & 4.13 \\\\ Conditioning ablation & 0.62 & 0.72 & 2.49 & 4.11 \\\\ Prompting ablation & 0.61 & 0.71 & 2.83 & 4.08 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: LibriSpeech 테스트-클리닝에 대한 지속시간 예측기 설계의 절제 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Models & NFE & RTF \\(\\downarrow\\) & Sim-O \\(\\uparrow\\) & Sim-R \\(\\uparrow\\) & UTMOS \\(\\uparrow\\) \\\\ \\hline NaturalSpeech 2 & 150 & 0.366 & 0.55 & 0.62 & 3.87 \\\\ VALL-E & - & 4.520 & 0.47 & 0.51 & 3.67 \\\\ \\hline NaturalSpeech 3 & 60 & 0.296 & **0.67** & **0.76** & **4.30** \\\\ NaturalSpeech 3 one-step & 15 & **0.067** & 0.66 & 0.75 & 4.01 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: LibriSpeech test-clean에 대한 잠복기 연구. 내츄럴 스피치 3 원 스텝은 각 확산 프로세스에서 원래의 \\(4\\) 대신 \\(1\\) 반복만을 사용하는 것을 나타낸다. 약어: NFE(기능 평가 횟수)\n' +
      '\n' +
      '### Proody Similarity Evaluation의 세부사항\n' +
      '\n' +
      '표 11에서 우리는 자연 음성 3과 RAVDESS 벤치마크의 기준선 방법을 비교하여 \\(8\\) 다른 감정에 대한 MCD를 제시한다. 내추럴 스피치 3은 \\(8\\) 감정 전반에 걸쳐 강건한 성능을 보여 운율 유사성 측면에서 효과성과 견고성을 검증한다.\n' +
      '\n' +
      '## 부록 B 세부사항 FACodec\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Model Architecture.** FACodec 인코더 및 디코더의 기본 아키텍처는 [32]를 따르고, SnakeBeta 활성화 함수를 채용한다[67]. 음색 추출기는 여러 개의 컨포머[68] 블록으로 구성된다. 3개의 FVQ(\\mathcal{Q}^{c},\\mathcal{Q}^{p},\\mathcal{Q}^{d}\\) 각각에 대한 양자화기의 개수로 \\(N_{q_{c}}=2,N_{q_{p}=1,N_{q_{d}=3\\)을 사용하며, 모든 양자화기의 코드북 크기는 1024이다.\n' +
      '\n' +
      '**Loss Functions.** 다중스케일 멜-재구성 손실\\(\\mathcal{L}_{\\text{rec}}\\)을 [32]에 자세히 설명되어 있다. 적대적 손실\\(\\mathcal{L}_{\\text{adv}\\)을 위해 [32]에서 제안한 다중주기 판별기(MPD)와 다중대역 다중스케일 STFT 판별기를 사용하였다. 또한, 상대 특징 매칭 손실(\\mathcal{L}_{\\text{feat}\\)을 통합한다. 코드북 학습을 위해 VQ-VAE[69]로부터 코드북 손실\\(\\mathcal{L}_{\\text{codebook}\\)과 몰입 손실\\(\\mathcal{L}_{\\text{commit}\\)을 사용한다. 학습 손실에는 전화 예측 손실\\(\\mathcal{L}_{\\text{ph}\\), 정규화된 F0 예측 손실\\(\\mathcal{L}_{\\text{f0}\\), 전화 예측의 기울기 역손실\\(\\mathcal{L}_{\\text{gr-ph}\\), 정규화된 F0 예측\\(\\mathcal{L}_{\\text{gr-f0}\\), 화자 분류\\(\\mathcal{L}_{\\text{gr-spk}\\)이 포함된다. 생성기에 대한 총 학습 손실은 다음과 같다. \\(\\lambda_{\\text{rec}}+\\lambda_{\\text{adv}}+\\lambda_{\\text{gr-spk}}), \\(\\lambda_{\\text{gr-f0}}\\(\\lambda_{\\text{gr-f0}}}), \\(\\lambda_{\\text{gr-f0}}\\(\\lambda_{\\text{gr-f0}}+\\lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{lambda_{\\text{l 본 논문에서는 이들 계수를 다음과 같이 설정하였다. \\(\\lambda_{\\text{rec}}=10.0\\), \\(\\lambda_{\\text{adv}}=2.0\\), \\(\\lambda_{\\text{feat}=2.0\\), \\(\\lambda_{\\text{codebook}=1.0\\), \\(\\lambda_{\\text{commit}=0.25\\), \\(\\lambda_{\\text{f0}=5.0\\), \\(\\lambda_{\\text{gr-f0}=5.0\\), \\(\\lambda_{\\text{gr-ph}=5.0\\), \\(\\lambda_{\\text{gr-spk}=1.0\\).\n' +
      '\n' +
      '**훈련 세부사항** 라이브러리라이트를 훈련 세트로 사용합니다. 8개의 NVIDIA TESLA V100 32GB GPU를 사용하여 8(800\\)K 단계의 GPU당 16000 프레임씩 32개의 음성 클립의 배치 크기를 사용하여 코드를 훈련한다. 학습률은 \\(2e-4\\), \\(\\beta_{1}=0.5\\), \\(\\beta_{2}=0.9\\)인 Adam 최적화기를 사용하였다.\n' +
      '\n' +
      '### 재구성 성능 비교\n' +
      '\n' +
      '본 논문에서는 음성 품질의 지각 평가(Perceptual Evaluation of Speech Quality, PESQ), 단시간 목표 명료도(Short-Time Objective Intelligibility, STOI), 다중 해상도 STFT 거리(Multi-Resolution STFT Distance, MSTFT), 멜-셉스트럴 왜곡(Mel-Cepstral Distortion, MCD)을 이용하여 재구성 성능을 평가한다. 이러한 메트릭은 원본 샘플과 재구성 샘플 간의 차이를 일괄적으로 측정한다. 우리는 다음을 선택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{MCD\\(\\downarrow\\)} & \\\\  & neutral & calm & happy & sad & angry & fearful & disgust & surprised \\\\ \\hline Ground Truth & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\\\ \\hline VALL-E \\({}^{\\blacklozenge}\\) & 3.97 & 4.75 & 4.83 & 5.51 & 5.19 & 5.29 & 5.45 & 5.29 \\\\ Voicebox\\({}^{\\blacklozenge}\\) & 3.93 & 4.90 & 4.96 & 4.93 & 5.01 & 5.03 & 5.34 & 4.89 \\\\ NaturalSpeech 2\\({}^{\\blacktriangle}\\) & **2.77** & **3.51** & 4.85 & 4.88 & 5.42 & 5.23 & 5.31 & 4.52 \\\\ Mega-TTS 2\\({}^{\\blacklozenge}\\) & 3.28 & 4.39 & 4.44 & 4.67 & **4.21** & 5.00 & 5.42 & **4.14** \\\\ StyleTTS 2\\({}^{\\blacktriangle}\\) & 3.41 & 4.38 & 4.40 & 4.64 & 4.80 & 4.69 & 5.10 & 4.57 \\\\ HierSpeech++\\({}^{\\blacklozenge}\\) & 5.54 & 6.55 & 5.78 & 5.84 & 6.37 & 6.17 & 6.74 & 5.62 \\\\ \\hline NaturalSpeech 3 & 3.23 & 4.32 & **4.26** & **4.41** & 4.64 & **4.25** & **4.80** & 4.45 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: MCD score on \\(8\\) different emotions of NaturalSpeech 3 and the baseline methods on RAVDESS. \\(8\\) ({}^{\\blacklozenge}\\)는 저자로부터 얻은 결과를 의미한다. \\ ({}^{\\blacklozenge}\\)는 공식 검문소에서 추론된 결과를 의미한다. \\ ({}^{\\blacklozenge}\\)는 재현된 결과를 의미한다. 우리는 가장 좋은 결과를 나타내기 위해 **굵은**을 사용하고 두 번째로 좋은 결과를 나타내기 위해 밑줄을 사용한다.\n' +
      '\n' +
      '오픈소스 코덱 모델은 EnCodec[15]9, HiFi-Codec[63]10, DAC(Descript-Audio-Codec)[32]11을 기본으로 하며, 본 논문의 구현 및 실험 설정에 따라 SoundStream[14]을 추가적으로 재생한다. 표 12는 1) FACodec이 동일한 대역폭 설정에서 사운드스트림(PESQ에서\\(0.44\\), STOI에서 \\(0.05\\), MSTFT에서 \\(0.14\\), MCD에서 \\(0.79\\)을 각각 크게 능가함을 보여준다. 또한, FACodec은 대역폭이 2배(PESQ에서는\\(0.02\\), STOI에서는 \\(0.01\\), MSTFT에서는 \\(-0.01\\), MCD에서는 \\(0.17\\)인 경우에도 SoundStream으로 On-par 성능을 달성한다. 공정한 비교를 위해 유사한 대역폭에서 FACodec을 다른 베이스라인과 비교한다. FACodec은 이러한 강력한 베이스라인보다 대부분의 메트릭에서 비교가능하거나 더 나은 결과를 달성하며, 이는 우리가 음성 속성을 디엔탱글링할 때 여전히 우수한 재구성 음성 품질을 달성할 수 있음을 의미한다.\n' +
      '\n' +
      '각주 9: [https://github.com/facebookresearch/encodec](https://github.com/facebookresearch/encodec)\n' +
      '\n' +
      '각주 10: [https://github.com/yangdongchao/AcademiCodec](https://github.com/yangdongchao/AcademiCodec)\n' +
      '\n' +
      '각주 11: [https://github.com/descriptinc/descript-audio-codec](https://github.com/descriptinc/descript-audio-codec)\n' +
      '\n' +
      '# Zero-shot 음성변환\n' +
      '\n' +
      '음성 변환은 음색을 변경하면서 콘텐츠를 보존하면서 소스 스피커에서 타겟 스피커의 음성으로 변환하는 것을 목표로 한다. 제로-샷 음성 변환은 소스 화자의 음성을 변환하기 위해 타겟 화자로부터의 프롬프트 음성 샘플을 이용함으로써 이를 달성한다. FACodec은 소스 음성으로부터 화자 임베딩 \\(h_{t}^{source}\\)을 대체하기 위해 프롬프트 음성으로부터 화자 임베딩 \\(h_{t}^{prompt}\\)을 추출하고, 소스 음성으로부터 콘텐츠 코드 \\(z_{c}^{source}\\), 운율 코드 \\(z_{c}^{source}\\), 디테일 코드 \\(z_{d}^{source}\\)을 활용하여 목표 음성 \\(\\mathcal{D}(z_{c}^{source},z_{p}^{source},z_{d}^{source},h_{t}^{prompt})을 재구성함으로써 제로샷 음성 변환을 달성한다. 기존 SOTA 모델인 YourITS[70], Make-A-Voice(VC)[20], LM-VC[71], UniAudio[35]와 FACodec을 비교한다. 비교를 위해 VCTK 데이터셋을 사용한다. 우리는 Sim-O12를 사용하여 기준선과 화자 유사성을 비교하고 화질의 평가를 위해 WER을 사용한다. 평가 결과를 표 13에 나타낸다. 실험 결과는 FACodec이 이 작업에 대한 추가 교육이 필요한 최첨단 제로 샷 VC 모델에 비해 유사한 유사성과 우수한 지능을 달성한다는 것을 보여준다. 이는 FACodec이 특히 음색에서 우수한 탈엉킴을 달성한다는 것을 의미한다.\n' +
      '\n' +
      '각주 12: [https://huggingface.co/microsoft/wavlm-base-plus-sv](https://huggingface.co/microsoft/wavlm-base-plus-sv)\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '이 하위 섹션에서는 1) 정보 병목 현상이 FACodec의 비엉킴에 미치는 영향; 2) 기울기 역전이 FACodec의 비엉킴에 미치는 영향; 3) 음향 세부 양자화기의 역할; 4) TTS 생성을 위한 서로 다른 운율 표현의 영향에 대해 연구한다.\n' +
      '\n' +
      '**Disentanglement에 대한 정보 병목현상**\n' +
      '\n' +
      '우리는 질적 분석을 통해 정보 병목 현상이 언어 혼란에 미치는 영향을 조사한다. 우리는 정보 병목 현상(낮은 차원 공간보다는 원래의 차원 공간에서 양자화)을 사용하지 않으면 불완전한 얽힘으로 이어질 수 있음을 발견한다. 예를 들어, 우리는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Models & Sampling Rate & H & N & Bandwidth & PESQ \\(\\uparrow\\) & STOI \\(\\uparrow\\) & MSTFT \\(\\downarrow\\) & MCD \\(\\downarrow\\) \\\\ \\hline EnCodec\\({}^{\\clubsuit}\\) & 24kHz & 320 & 8 & 6.0 kbps & 3.28 & 0.94 & 0.99 & 2.70 \\\\ EnCodec\\({}^{\\clubsuit}\\) & 16kHz & 320 & 10 & 5.0 kbps & 3.10 & 0.92 & 0.97 & 3.10 \\\\ HiFi-Codec\\({}^{\\clubsuit}\\) & 16kHz & 320 & 4 & 2.0 kbps & 3.17 & 0.93 & 0.98 & 3.05 \\\\ DAC\\({}^{\\clubsuit}\\) & 16kHz & 320 & 9 & 4.5 kbps & **3.52** & **0.95** & 0.97 & 2.65 \\\\ \\hline SoundStream\\({}^{\\clubsuit}\\) & 16kHz & 200 & 6 & 4.8 kbps & 3.03 & 0.90 & 1.07 & 3.38 \\\\ SoundStream\\({}^{\\clubsuit}\\) & 16kHz & 200 & 12 & 9.6 kbps & 3.45 & 0.94 & **0.92** & 2.76 \\\\ \\hline FACodec & 16kHz & 200 & 6 & 4.8 kbps & 3.47 & **0.95** & 0.93 & **2.59** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 코덱의 재구성 품질 평가. \\ ({}^{\\clubsuit}\\)는 결과가 외부의 검문소에서 영향을 받는다는 것을 의미한다. \\ ({}^{\\clubsuit}\\)는 재현된 검문소를 의미한다. \\ ({}^{\\clubsuit}\\)는 원본 논문의 구현과 실험 설정에 따라 재현된 모델을 의미한다. 모든 모델은 \\(1024\\)의 코드북 크기를 사용한다. 우리는 가장 좋은 결과를 나타내기 위해 **굵은**을 사용하고 두 번째로 좋은 결과를 나타내기 위해 밑줄을 사용한다. 약칭: H(Hop Size), N(Codebook Number).\n' +
      '\n' +
      '부록 B.3에서 언급한 바와 같이 정보 병목 현상이 없는 FACodec을 사용한 동일한 실험 설정에서 제로 샷 음성 변환은 변환된 음성의 음색이 소스와 타겟 사이의 보간으로 음색 불일치가 불량함을 나타낸다. 표 14는 정보 병목 현상이 없으면 제로샷 음성 변환의 화자 유사도가 0.13만큼 감소함을 보여준다.\n' +
      '\n' +
      '변형을 위한 기울기 반전\n' +
      '\n' +
      '우리는 정성적 분석을 통해 FACodec의 디엔탱글먼트에 대한 기울기 역전의 영향을 조사한다. 우리는 기울기 역전을 사용하지 않는 것이 FACodec의 디엔탱글링 능력을 감소시킨다는 것을 관찰한다. 예를 들어, 음향 세부 모듈로부터 콘텐츠 및 운율 구배 반전을 제거하면 일부 콘텐츠 및 운율 정보가 세부 음향으로 누출된다. 우리는 부분 내용과 음높이 변화를 들을 수 있는 세부 코드와 음색 임베딩을 사용하여 음성을 단독으로 재구성함으로써 이를 확인할 수 있다.\n' +
      '\n' +
      '음향 디테일 양자화기의 역할\n' +
      '\n' +
      '콘텐츠, 운율 및 음색 정보가 이미 대부분의 음성 정보를 포괄하지만, 표 15는 음향 상세 양자화기를 사용하는 것이 FACodec의 음성 재구성 품질을 향상시킨다는 것을 입증한다. 1) 음향 디테일 양자화기(3개의 코드북만을 사용함)를 사용하지 않고, FACodec은 3개의 코드북을 사용하는 사운드스트림과 비교하거나 더 나은 결과를 얻는데, 이는 콘텐츠 코드, 운율 코드 및 음색 임베딩이 이미 음성 재구성에 필요한 대부분의 정보를 포함하고 있음을 의미한다; 2) 음향 디테일을 추가하는 것은 더 나은 재구성 품질을 달성하며, 이는 음향 디테일 코드가 주로 고주파 디테일을 보충하는 역할을 한다는 것을 암시한다.\n' +
      '\n' +
      '## 부록 C 제한 및 향후 작업\n' +
      '\n' +
      '제안된 TTS 시스템이 큰 발전을 이루었음에도 불구하고, 우리는 여전히 다음과 같은 제한 사항을 가지고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Models & Sim-O \\(\\uparrow\\) & WER \\(\\downarrow\\) \\\\ \\hline Ground Truth & - & 3.25 \\\\ \\hline YourTTS & 0.72 & 10.1 \\\\ Make-A-Voice (VC) & 0.68 & 6.20 \\\\ LM-VC & 0.82 & 4.91 \\\\ UniAudio & **0.87** & 4.80 \\\\ \\hline FACodec & 0.86 & **3.46** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 이전의 SOTA 방법들을 갖는 FACodec에 대한 제로-샷 음성 변환 평가 결과. 우리는 가장 좋은 결과를 나타내기 위해 **굵은**을 사용하고 두 번째로 좋은 결과를 나타내기 위해 밑줄을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & Sim-O \\(\\uparrow\\) \\\\ \\hline w. information bottleneck & **0.86** \\\\ w.o. information bottleneck & 0.73 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: 정보 병목 현상을 이용하는 경우와 이용하지 않는 경우의 FACodec에 대한 제로샷 음성 변환 평가 결과의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Codebook Number & PESQ \\(\\uparrow\\) & STOI \\(\\uparrow\\) & MSTFT \\(\\downarrow\\) & MCD \\(\\downarrow\\) \\\\ \\hline FACodec & 6 & **3.47** & **0.95** & **0.93** & **2.59** \\\\ - acoustic details quantizers & 3 & 3.09 & 0.92 & 1.08 & 3.12 \\\\ \\hline SoundStream & 6 & 3.03 & 0.90 & 1.07 & 3.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 15: 음향 세부 양자화기를 사용하는 경우와 사용하지 않는 경우의 FACodec 간의 재구성 품질 비교.\n' +
      '\n' +
      '**Attribute Coverage.** 이 작업에서 우리는 음성 표현 및 생성을 위한 인수분해 설계를 제안하며, 음성을 내용, 운율, 지속 시간, 음향 세부 사항 및 음색으로 인수분해함으로써 상당한 개선을 달성했다. 그러나, 이러한 속성들은 모든 음성 측면들을 커버할 수 없다. 예를 들어, 우리는 배경음을 추출할 수 없는데, 이는 말의 엉킴을 풀기 위한 일반적인 도전이다. 향후에는 1. 에너지, 2. 배경음 등을 포함한 더 많은 속성을 탐색할 것이다.\n' +
      '\n' +
      '**Data Coverage.** 음성 품질, 유사성 및 견고성에 대한 제로 샷 음성 합성에 대해 현저한 개선을 달성했지만, NaturalSpeech 3은 LibriVox 오디오북의 영어 코퍼스에 대해 훈련된다. 따라서, 실제 단어 사람들의 다양한 음성을 커버할 수 없고 다국어 TTS를 지원할 수 없다. 향후, 우리는 더 큰 다양성을 가진 더 많은 음성 데이터를 수집함으로써 이러한 한계를 해결할 것이다.\n' +
      '\n' +
      '**Neural Speech Codec.** 우리의 FACodec은 음성을 속성으로 분해하고 고품질로 재구성할 수 있지만, 여전히 다음과 같은 한계를 가지고 있다: 1) 콘텐츠 감독을 위해 음소 전사가 필요하여 확장성이 제한되며, 2) 제로 샷 TTS 작업에서 잘못된 엉킴만 확인했다. 앞으로는 첫째, 특히 감독 없이 더 나은 분할을 달성하기 위해 보다 일반적인 방법을 탐색할 것이다. 둘째, 제로샷 음성 변환 및 자동 음성 인식과 같은 FACodec으로 더 많은 작업을 탐색하고자 합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>