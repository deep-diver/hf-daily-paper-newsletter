<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'S4D[23] 및 S4D[23]은 특히 장거리 의존성을 모델링하는 데 광범위한 작업 및 양식에 걸쳐 시퀀스 데이터를 처리하도록 제안된다. 컨볼루션 계산과 근선형 계산 때문에 긴 시퀀스에서 효율적입니다. 2-D SSM[2], SGConvNeXt[37], ConvSSM[52]는 SSM과 CNN 또는 트랜스포머 구조를 결합하여 2-D 데이터를 처리한다. 최근 작품인 만바[20]는 SSM에 시간 가변 파라미터를 통합하고 효율적인 훈련과 추론을 가능하게 하기 위해 하드웨어 인식 알고리즘을 제안한다. 삼바의 우수한 스케일링 성능은 언어 모델링에서 트랜스퍼에 대한 유망한 대안임을 나타낸다. 그럼에도 불구하고, 비전 작업에 대한 일반적인 순수-SSM 기반 백본은 조사되지 않았다.\n' +
      '\n' +
      '비전트랜스포머(ViT)는 대규모 자기 지도 사전 훈련과 하류 활동에 대한 높은 성능 모두에서 우수한 시각적 표현 학습에서 큰 성공을 거두었다. 컨볼루션 신경망과 비교하여 핵심 이점은 ViT가 자기 의사를 통해 각각의 이미지 패치에 데이터/패치 의존적 글로벌 컨텍스트를 제공할 수 있다는 데 있다. 이것은 모든 위치에 대해 동일한 매개변수인 _i.e._ 콘볼루션 필터를 사용하는 컨볼루션 네트워크와 다르다. 또 다른 장점은 이미지를 2D 유도 편향 없이 패치 시퀀스로 처리하여 모달-진단 모델링이며, 이는 다중 모드 애플리케이션(3, 36, 40])에 대한 선호 아키텍처로 만든다. 동시에, 트랜스퍼의 자기 의도 메커니즘은 장거리 시각적 의존도, _e.g.__를 다룰 때 속도와 메모리 사용 측면에서 도전을 제기한다.\n' +
      '\n' +
      '언어 모델링에서 몰바가 성공함으로써 이러한 성공을 언어에서 비전으로, _i.e._로 전달하여 첨단 SSM 방식으로 일반적이고 효율적인 시각적 백본을 설계할 수도 있다는 것을 호소한다. 그러나 삼바, _i.e._, 단방향 모델링 및 위치 인식 결여에 대한 두 가지 문제가 있다. 이러한 과제를 해결하기 위해 데이터 의존적 글로벌 시각적 컨텍스트 모델링 및 위치 인식 시각적 인식을 위한 위치 임베딩을 위한 양방향 SSM을 통합하는 비전앰바(Vim) 블록을 제안한다. 우리는 먼저 입력 이미지를 패치들로 분할하고 이를 Vim에 대한 벡터로 선형적으로 투사한다. 이미지 패치는 Vim 블록에서의 시퀀스 데이터로 처리되며, 이는 제안된 양방향 선택 상태 공간으로 시각적 표현을 효율적으로 압축한다. 나아가, Vim 블록에 위치 임베딩은 공간 정보에 대한 인식을 제공하며, 이는 Vim이 조밀한 예측 작업에서 더 강력해질 수 있게 한다. 현재 단계에서는 이미지넷 데이터셋을 사용하여 감독 이미지 분류 작업에 Vim 모델을 훈련시킨 다음 전처리된 Vim을 백본으로 사용하여 하류 조밀한 예측 작업, _i._i._ 의미 세분화, 객체 검출 및 인스턴스 분할에 대한 순차적 시각적 표현 학습을 수행한다. 트랜스포머와 마찬가지로 Vim은 더 나은 시각적 표현을 위해 대규모 비지도 시각적 데이터에 대해 전처리될 수 있다. 탐바의 더 나은 효율 덕분에 더 낮은 계산 비용으로 Vim의 대규모 전술을 달성할 수 있다.\n' +
      '\n' +
      '비전 작업을 위한 다른 SSM 기반 모델과 비교하여 Vim은 순-SSM 기반 방법 및 시퀀스 방식으로 모델 이미지이며, 이는 일반적이고 효율적인 백본에 더 유망하다. 양방향 압축 모델링을 위치 인식으로 압축한 덕분에 Vim은 조밀한 예측 작업을 처리하는 최초의 순수-SSM 기반 모델이다. 가장 설득력 있는 트랜스포머 기반 모델과 비교하여 _i.e._, DeiT[60], Vim은 이미지넷 분류에 대한 우수한 성능을 달성한다. 또한 Vim은 고해상도 이미지에 대한 GPU 메모리 및 추론 시간 측면에서 더 효율적이다. 메모리 및 속도 측면에서 효율성은 Vim이 DeiT보다 높은 정확도를 달성하면서 고해상도 시각 이해 작업을 위해 2D 제사(VirT 디코딩 [38])에서 2D 로컬 윈도우와 같이)에 의존하지 않고 순차적 시각적 표현 학습을 직접 수행하도록 한다.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* 우리는 데이터 의존적 글로벌 시각적 컨텍스트 모델링 및 위치 인식 시각적 이해를 위한 위치 임베딩을 위해 양방향 SSM을 통합하는 비전앰바(Vim)를 제안한다.\n' +
      '*는 주의 필요 없이 제안된 Vim은 ViT와 동일한 모델링 파워를 가지며, 서브퀄리티-타임 연산 및 선형 메모리 복잡성만을 가지고 있다. 구체적으로, 우리의 Vim은 DeiT보다 2.8\\(종량) 빠르고 배치 추론을 수행할 때 86.8% GPU 메모리를 절약하여 1248\\(종량)1248의 해상도에서 이미지에 대한 특징을 추출한다.\n' +
      '* 우리는 이미지넷 분류 및 조밀한 예측 다운스트림 작업에 대한 광범위한 실험을 수행한다. 결과는 Vim이 잘 정립되고 고도로 최적화 된 일반 비전 트랜스포머인 _i.e._, DeiT에 비해 우수한 성능을 달성한다는 것을 보여준다.\n' +
      '* 삼바의 효율적인 하드웨어 인식 설계에서 요약하면 Vim은 고해상도 컴퓨터 비전 작업, _e._e._, 비디오 분할, 공중 이미지 분석, 의료 이미지 세분화, 계산 병리학에 대해 자기 의도 기반 DeiT[60]보다 훨씬 더 효율적이다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '일반 비전 백본을 위한 아키텍처 초기 귀에서 ConvNet[34]은 컴퓨터 비전을 위한 디-계면 표준 네트워크 설계 역할을 한다. 많은 합성곱 신경망[25, 26, 33, 50, 51, 56, 57, 58, 63, 72]이 다양한 시각적 응용 프로그램의 비전 백본으로 제안되었다. 개척작인 비전트랜스포머(ViT)[14]는 경관을 변화시킨다. 이미지를 평탄화 된 2D 패치의 시퀀스로 취급하고 순수한 변환 아키텍처를 직접 적용한다. 이미지 분류 및 스케일링 능력에 대한 ViT의 놀라운 결과는 많은 후속 작업[16, 59, 61, 62]을 장려한다. 한 줄의 작품은 2D 콘볼루션 사제들을 ViT[9, 13, 15, 69]에 도입하여 하이브리드 아키텍처 설계에 초점을 맞추고 있다. PVT[66]은 피라미드 구조 트랜스포머를 제안한다. 스윈 트랜스포머[42]는 시프트 윈도우 내에서 자기 의사를 적용한다. 또 다른 작품 라인은 보다 발전된 설정[41, 67]이 있는 전통적인 2D ConvNets 개선에 중점을 둔다. ConvNeXt[43]은 설계 공간을 검토하고 ViT 및 그 변이체로 확장 가능한 순수한 ConvNets를 제안한다. RepLKNet[12]는 개선을 가져오기 위해 기존 ConvNets의 커널 크기를 스케일링할 것을 제안한다.\n' +
      '\n' +
      '이러한 지배적인 후속 작업은 2D 사제들을 소개함으로써 이미지넷[10]과 다양한 다운스트림 작업[39, 74]에서 우수한 성능과 더 나은 효율성을 보여주지만, 대규모 시각 사전 적용[1, 5, 17]과 다중 모델 적용[3, 29, 35, 36, 40, 49]이 급증하면서 바닐라 트랜스포머 스타일의 모델이 컴퓨터 비전 중심 단계로 돌아섰다. 더 큰 모델링 능력, 통일된 다중-모달성 표현의 장점은 자기 지도 학습에 친화적이어서 선호되는 아키텍처로 만든다. 그러나, 트랜스퍼의 2차 복잡성으로 인해 시각적 토큰의 수가 제한된다. 이 오래되고 두드러진 도전을 해결하기 위해 많은 작품[7, 8, 11, 32, 48, 55, 65]이 있지만 시각적 응용에 초점을 맞춘 작품은 거의 없다. 최근에 롱ViT[68]는 확장된 주의를 통해 계산 병리학 응용을 위한 효율적인 트랜스포머 구조를 구축했다. 롱ViT의 선형 계산 복잡성은 매우 긴 시각적 서열을 인코딩할 수 있게 한다. 본 연구에서는 Mambo[20]에서 영감을 도출하고 ViT의 순차적 모달리티 진단 모델링 메리트를 보존하면서 주목하지 않고 순수SSM 기반 모델을 일반 비전 백본으로 구축하고 탐색한다.\n' +
      '\n' +
      '긴 서열 모델링을 위한 상태 공간 모델[21][21]. CNN 또는 트랜스퍼러에 대한 새로운 대안인 구조화된 국가-공간 서열(S4) 모델을 제공하여 장거리 의존성을 모델링한다. 서열 길이의 선형 스케일링의 유망한 특성은 추가 탐색을 끌어낸다.[53] MIMO SSM과 효율적인 병렬 스캔을 S4 레이어에 도입하여 새로운 S5 레이어를 활성화한다[18][18] 언어 모델링에서 SSM과 트랜스포머 주의력의 성능 격차를 거의 채워주는 새로운 SSM 레이어인 H3를 디자인한다[46][46] 표현성을 향상시키기 위해 더 많은 게이팅 유닛을 도입하여 S4에 Gated 스테이트 스페이스 레이어를 제작한다. 최근 [20]은 데이터 의존적 SSM 레이어를 제안하고, 대규모 실제 데이터에 다양한 크기의 트랜스포머를 능가하고 서열 길이의 선형 스케일링을 즐기는 일반 언어 모델 백본인 마모바를 구축한다. 이 작품에서 우리는 마모바의 성공을 비전으로 옮기고, 주의를 기울이지 않고 순수 SSM에서 일반 비전 백본을 구축하는 것을 탐구한다.\n' +
      '\n' +
      '시각적 응용을 위한 상태 공간 모델[27][27]] 시각 애플리케이션을 위한 상태 공간 모델. 영상 분류를 위한 장거리 시간적 의존성을 처리하기 위해 1D S4를 1D S4로 구성하여 비디오 분류를 위한 장거리 시간적 의존도를 처리한다[47][47]. 2D 이미지 및 3D 비디오를 포함하는 다차원 데이터를 처리하기 위해 1D S4를 추가 연장한다[28][28] S4의 강점과 트란S4mer 모델 구축을 위한 자구력을 결합시켜 영화 장면 검출을 위한 최첨단 성능을 달성하였다.[64] S4에 새로운 선택성 메커니즘을 도입하여 훨씬 낮은 메모리 발자국과 함께 긴 형태의 비디오 이해에 대한 S4의 성능을 크게 개선한다[73]]. 저렴한 계산 하에서 고해상도 이미지를 생성하고 미세 구성 표현을 처리하기 위해 보다 확장 가능한 SSM 기반 백본으로 공급자 주의 메커니즘.[45] 하이브리드 CNN-SSM 아키텍처인 U-람바다는 생물학적 이미지 세분화의 장거리 의존성을 다루기 위해 촉진된다. 상기 작업은 SSM을 특정 시각 애플리케이션에 적용하거나 SSM을 컨볼루션 또는 주의와 결합하여 하이브리드 아키텍처를 구축한다. 그들과 다른, 우리는 일반적인 비전 백본으로 채택될 수 있는 순수-SSM 기반 모델을 구축한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '비전 마모바(Vim)의 목표는 첨단 상태 공간 모델(SSM), 마모보아[20]을 컴퓨터 비전에 도입하는 것이다. 이 섹션은 SSM의 예선에 대한 설명에서 시작된다. 그 다음은 Vim에 대한 개요가 뒤따른다. 그런 다음 Vim 블록이 입력 토큰 시퀀스를 처리하는 방법을 자세히 설명하고 Vim의 아키텍처 세부 사항을 설명하기 위해 진행한다. 섹션은 제안된 Vim의 효율성에 대한 분석으로 마무리된다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      'SSM 기반 모델, 구조화된 상태 서열 모델(S4) 및 마모나는 연속 시스템에 의해 영감을 받아 1D 기능 또는 서열 \\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\)을 숨겨진 상태 \\(h(t)\\in\\mathbb{R}^{\\bb{R}^{\\)를 통해 1-D 기능 또는 서열 \\(t)\\in\\mathbb{R}\\(t)\\in\\mathbb{R}\\(t)\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\)를 매핑하는 연속 시스템(t)\\in\\mathbb{R}^Mathbb{R}^Mathbb{R}^in\\in\\mathbb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R}^Mathb{R 이 시스템은 진화 매개변수로서\\(\\mathbbb{A}\\mathbb{R}\\mathbb{R}\\mathbb{N}\\mathbb{N}\\)를 사용하고 \\(\\mathbf{B}\\mathbb{B}\\in\\mathbb{B}\\in\\mathbb{B}\\mathb{B}\\mathbb{B}\\mathbb{B}\\mathbb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\in\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\mathb{B}\\in\\mathb{B}\\mathb{B}\\mathb{B}\\math\n' +
      '\n' +
      '\\[\\begin{split} h^{\\prime}(t)&=\\mathbf{A}h(t)+\\mathbf{A}h(t)+\\mathbf{B}}h(t),\\\\ y(t)&=\\mathbf{C}h(t)\n' +
      '\n' +
      'S4와 Mamboa는 연속 파라미터 \\(\\mathbf{\\Delta}\\)를 포함하여 연속 파라미터 \\(\\mathbf{A}\\), \\(\\mathbf{B}\\)를 이산 파라미터 \\(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\), \\(\\overline{\\mathbf{B}\\)로 변환한다. 일반적으로 사용되는 변환 방법은 제로차 홀드(ZOH)로 다음과 같이 정의된다.\n' +
      '\n' +
      '\\\\\\mathbf{B}}}(\\mathline{\\mathbf{B}\\mathbf{B}}\\mathline{\\mathbf{A}\\mathbf{A}})^{-1}(\\math{{ 및\\mathbf{}\\mathbf{}\\mathbf{}\\mathbf{}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\math{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f}\\mathbf{f{f}\\math{f{f{f}\\mathbf{f{f}\\math{f{f{f}\\math{f{f{f{f{f{f{f{f{f{}\\ HPA(\\overline{\\mathbf{A}}\\), \\(\\overline{\\mathbf{B}}\\)의 폐기 후 Eq의 폐기된 버전이다. 단계 크기 \\(\\mathbf{\\Delta}\\)를 사용하는 (1)을 재작성할 수 있다.\n' +
      '\n' +
      '\\[\\begin{split} h_{t}=\\오버라인{\\mathbf{A}}}h_{t-1}+ \\overline{\\mathbf{B}}}}x_{t},\\\\ y_{t}－=\\mathbf{C}h_{t}.\n' +
      '\n' +
      '마지막으로 모델은 글로벌 컨벌루션을 통해 출력을 계산합니다.\n' +
      '\n' +
      '}\\math{f}\\math{{}\\math{f}\\math{{f}\n' +
      '\n' +
      'r\\(\\mathtt{M}\\)가 입력 서열 \\(\\mathbf{x}\\)의 길이이고, \\(\\overline{\\mathbf{K}}\\in\\mathbb{R}\\in\\mathtt{M}}\\)는 구조화된 컨볼루션 커널이다.\n' +
      '\n' +
      '### Vision Mamba\n' +
      '\n' +
      '제안된 Vim의 개요는 그림 2에 나와 있으며 표준 만바는 1D 서열을 위해 설계되었다. rmath{R}\\math{P}\\math{P}\\math{C}}\\math{P}\\math{C}}\\\\math{C}}\\로 전환되며, 여기서 우리는 먼저 2-D 패치(\\math{H}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{F}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\)의 크기,\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C}\\math{C} 다음으로 우리는\\(\\mathtt{D}}) 크기의 벡터에\\(\\mathbf{f}_{\\mathbf{p}}\\)를 선형적으로 프로젝트하고 위치 임베딩(\\mathbf{E}_{pos}\\mathb{{{{{(\\mathtt{I}+1)을 추가한다.\n' +
      '\n' +
      't}_{mathtt{t}} <\\bf{f}> <\\mathbf}> <\\mathbf}> <\\mathbf{f}>.\n' +
      '\n' +
      '\\(\\mathbf{t}_{p}^{\\tt{j}}\\)는 \\(\\mathtt{j}}\\), \\(\\mathbf{W}\\mathbb{R}\\in\\mathbb{R}^{{{{(\\mathtt{P}^{P}^{P}\\)의\\(\\mathtt{j}}\\)의\\(\\mathtt{j}}\\) 패치(\\mathbf{f{t}\\), \\(\\mathbf{W}\\mathbf{W}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\in\\mathbbb{R}\\in\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\in\\mathbbb{R}\\in\\mathbb{R}\\in\\ ViT[14] 및 BERT[31]에 의해 영감을 받아 그룹 토큰을 사용하여 \\(\\mathbf{t}_{cls}\\)로 표시되는 전체 패치 서열을 나타낸다. 그런 다음 토큰 서열(\\(\\mathbf{T}_{1-1}\\))을 Vim 인코더의 \\(\\mathtt{l}\\)-제 레이어로 보내고 출력 \\(\\mathbf{T}_{1}\\)를 얻는다. 마지막으로 출력 클래스 토큰 \\(\\mathbf{T}_{\\mathtt{L}}^{0}\\)를 정상화하고 멀티 레이어 퍼셉트론(MLP) 헤드에 공급하여 최종 예측 \\(\\hat{p}\\)를 얻으세요.\n' +
      '\n' +
      '}(\\mathbf{T})\\mathbf{f}(\\math{f}:\\math{f}:\\math{f})+\\mathbf{f}.\n' +
      '\n' +
      '\\(\\mathbf{Vim}\\)가 제안된 비전 만바 블록, \\(\\mathtt{L}\\)는 층의 수이고, \\(\\mathbf{Norm}\\)는 정규화 계층이다.\n' +
      '\n' +
      '```\n' +
      '입력: 토큰 서열 \\(\\mathbf{T}_{l-1}\\math{T}}:\\math{B}:\\math{B},\\mathtt{M},\\mathtt{D})을 정규화한다.\n' +
      '1\\(\\mathbf{T}_{l-1}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{D})\\leftarrow\\mathbf{ Norm}(\\mathbf{T}_{l-1})\\)\n' +
      '2\\(\\mathtt{x}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{Linear}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\prime})\\)\n' +
      '3\\(\\mathtt{B},\\mathtt{M},\\mathtt{E})는 서로 다른 방향 */f}^{ \\mathbf{x}}(\\mathbf{T}_{l-1}^{\\ime})로 처리한다.\n' +
      'P{ 포워드, 백워드}도 4foro입니다.\n' +
      '5\\(\\mathbf{x}_{o}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow\\mathbf{ SiLU}(\\mathbf{Conv1d}_{o}(\\mathbf{x}))\\)\n' +
      '6\\(\\mathbf{B}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{N})\\leftarrow\\mathbf{Linear}_{o}^{ \\mathtt{B}}(\\mathbf{x}_{o}^{\\prime})\\)\n' +
      '(\\mathbf{C},\\mathtt{M},\\mathtt{M},\\mathtt{N}_{o}d\\mathtt{C}}(\\mathbf{x}_{o})\n' +
      '\\log(1+\\log,\\math{B},\\math{{f}},\\math{{\\math{{f}}:\\math{{\\math{{f}})입니다.\n' +
      '9\\(\\overline{\\mathbf{\\Delta}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{Parameter}_{o}^{\\mathbf{A}}\\)\n' +
      '10\\(\\overline{\\mathbf{B}}_{o}:(\\mathtt{B},\\mathtt{M},\\mathtt{E},\\mathtt{N}) \\leftarrow\\mathbf{\\Delta}_{o}\\bigotimes\\mathbf{B}_{o}\\)\n' +
      '}(\\mathtt{M},\\math{f})\\mathbf{SSM}(I\\overline{\\mathbf{\\Delta}}_{o}}\\mathbf{f}_{o})(\\mathbf{x}_{o})\n' +
      '12\n' +
      '② \\(\\mathbf{y}_{o}\\)\n' +
      '14\\(\\mathbf{y}_{forward}^{\\prime}:(\\mathtt{B},\\mathtt{M},\\mathtt{E})\\leftarrow \\mathbf{y}_{forward}\\bigodot\\mathbf{SiLU}(\\mathbf{z})\\)\n' +
      '15\\(\\mathbf{y}:\\mathtt{B},\\mathtt{M},\\mathtt{E}) /* 잔여 연결 */{backward\\mathbf{SiLU}(\\mathbf{{f})\n' +
      '}(\\math{T},\\math{M})}(\\mathbf{f}} <\\mathbf{f}_{backime})\n' +
      '```\n' +
      '\n' +
      '표 1*\n' +
      '\n' +
      '### Vim Block\n' +
      '\n' +
      '원래의 만바 블록은 1-D 시퀀스를 위해 설계되었으며, 이는 공간 인식 이해가 필요한 비전 작업에 적합하지 않다. 본 절에서는 비전 태스크에 대한 양방향 시퀀스 모델링을 포함하는 Vim 블록을 소개한다. 비임 블록은 그림 2에 나와 있다.\n' +
      '\n' +
      '구체적으로 알고 1에서 Vim 블록의 동작을 제시한다. 입력 토큰 서열 \\(\\mathbf{T}_{1-1}\\)는 정규화 계층에 의해 먼저 정규화된다. 다음으로, 우리는 정규화된 서열을 측정 크기 \\(E\\)를 갖는 \\(\\mathbf{x}\\) 및 \\(\\mathbf{z}\\)에 선형적으로 투영한다. 그런 다음, 우리는 전방 및 후방 방향에서 \\(\\mathbf{x}\\)를 처리한다. 각 방향에 대해 먼저 \\(\\mathbf{x}\\)에 1-D 컨볼루션을 적용하고 \\(\\mathbf{x}_{o}^{\\prime}\\)를 얻는다. 그런 다음\\(\\mathbf{x}_{o}^{\\prime}\\), \\(\\mathbf{B}_{o}_{o}\\), \\(\\mathbf{C}_{o}\\), \\(\\mathbf{\\Delta}_{o}_{o}\\)를 각각 선형적으로 프로젝트한다. 그런 다음 \\(\\mathbf{\\Delta}_{o}\\)를 사용하여 \\(\\overline{\\mathbf{A}}}_{o}\\), \\(\\overline{\\mathbf{B}}_{o}\\)를 각각 변환한다. 마지막으로 SSM을 통해 \\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)를 계산한다. r\\(\\mathbf{y}_{forward}\\) 및 \\(\\mathbf{y}_{backward}\\)는 \\(\\mathbf{z}\\)에 의해 게이팅되고 함께 추가되어 출력 토큰 서열 \\(\\mathbf{T}_{1}\\)을 얻는다.\n' +
      '\n' +
      '### Architecture Details\n' +
      '\n' +
      '요약하면, 우리의 건축의 하이퍼 파라미터는 다음과 같이 나열된다.\n' +
      '\n' +
      'L: 블록 수,\n' +
      '\n' +
      'D: 히든 상태 차원, D: 히든 상태 차원.\n' +
      '\n' +
      'E: 확장 상태 차원\n' +
      '\n' +
      'N: SSM 차원.\n' +
      '\n' +
      'VVT[14] 및 DeiT[61] 후, 우리는 먼저 16\\(표본)16 커널 크기 투영층을 사용하여 1D 시퀀스가 아닌 패치 임베딩을 얻는다. 이어서, 우리는 L Vim 블록을 직접 스택합니다. 기본적으로 블록 L의 수를 24, SSM 차원 N에서 16으로 설정하여 DeiT 시리즈의 모델 크기와 정렬하여 숨겨진 상태 차원 D를 192로 설정하고 작은 크기의 변이체에 대해 상태 차원 E를 384로 확장했다. 작은 크기의 변이체의 경우 D를 384 및 E에서 768으로 설정했다.\n' +
      '\n' +
      '### Efficiency Analysis\n' +
      '\n' +
      '전통적인 SSM 기반 방법은 Eq와 같이 컨볼루션 연산을 증가시키기 위해 빠른 푸리에 변환을 레버리지한다. (4) Mamba와 같은 데이터 의존적 방법의 경우 Algo 11호선 SSM 연산을 수행한다. 1은 더 이상 컨볼루션과 동등하지 않다. 이 문제를 해결하기 위해, 만바와 제안된 Vim은 효율성을 보장하기 위한 현대식 하드웨어 친화적인 방법을 선택한다. 이 최적화의 핵심 아이디어는 현대 하드웨어 가속기(GPU)의 IO-결합 및 메모리-결합을 피하기 위한 것이다.\n' +
      '\n' +
      'IO-효율성은 높은 대역폭 메모리(HBM)와 SRAM은 GPU의 두 가지 중요한 구성요소이다. 이 중 SRAM은 대역폭이 크고 HBM은 메모리 크기가 더 크다. HBM을 사용한 Vim의 SSM 동작의 표준 구현은 \\(O(\\texttt{BME})\\ 순서에 대한 메모리 IO의 수를 요구한다. 모바에서 영감을 받은 Vim은 느린 HBM에서 빠른 SRAM까지 메모리 \\(O(\\texttt{BME}+\\texttt{BME}+\\texttt{EN}}) 바이트((\\mathbf{\\Delta_{o}},\\mathbf{B_{o}},\\mathbf{C_{o}},\\mathbf{C_{o}})를 통해 먼저 판독한다. 그런 다음 Vim은 SRAM에서 이산 \\(\\overline{\\mathbf{A}_{o}}), \\(\\overline{\\mathbf{B}_{o}_{o}})를 얻는다. 마지막으로, Vim은 SRAM에서 SSM 연산을 수행하고 \\((텍스트tt{B},\\texttt{M},\\texttt{E}) 크기의 출력을 다시 HBM으로 기입한다. 이 방법은 IO를 \\(O(\\texttt{BME})\\에서 \\(O(\\texttt{BME})\\(O(\\texttt{BME}+\\texttt{EN})\\로 환원시키는 데 도움이 될 수 있다.\n' +
      '\n' +
      '메모리 효율성은 메모리 외 문제를 피하고 긴 서열을 다룰 때 더 낮은 메모리 사용을 달성하기 위해 Vim은 Mamba와 동일한 재입력 방법을 선택한다. 크기 \\((\\texttt{B},\\texttt{M},\\texttt{E},\\texttt{N})\\)의 중간 상태에 대해 구배를 계산하기 위해 Vim은 네트워크 백워드 패스에서 그것들을 재입력한다. 활성화 함수 및 컨볼루션의 출력과 같은 중간 액티베이션에 대해, Vim은 또한 활성화 값이 많은 메모리를 취하지만 재선택을 위해 빠르기 때문에 GPU 메모리 요구 사항을 최적화하기 위해 이를 재입력한다.\n' +
      '\n' +
      '비임 블록에서의 컴퓨터-효율성(알고 1의 라인 11)과 트랜스포머에 대한 자기 의도는 모두 적응적으로 글로벌 컨텍스트를 제공하는 데 중요한 역할을 한다. 시각적 서열 \\(\\mathbf{T}\\in R^{1\\in\\texttt{M}\\tcer\\texttt{D}}\\\\)와 디폴트 설정 \\(\\texttt{E}=\\texttt{2D}\\)을 감안할 때, 글로벌 자기 의도 및 SSM의 계산 복잡성은 마찬가지이다.\n' +
      '\n' +
      'SF{M}.\n' +
      '\n' +
      '자기 의도가 2차에서 서열 길이 M이고, SSM은 서열 길이 M(N은 고정 파라미터이고 디폴트로 16으로 설정됨)에 선형이다. 계산 효율은 시퀀스 길이가 큰 기가픽셀 애플리케이션에 대해 Vim 스케일링을 가능하게 한다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Image Classification\n' +
      '\n' +
      '세트 설정은 1,000개 범주의 1.28M 훈련 이미지와 50K 검증 이미지를 포함하는 이미지넷-1K 데이터세트[10]에 Vim을 벤치마킹한다. 모든 모델은 트레이닝 세트에 대해 훈련되며 검증 세트에 대한 상위 1의 정확도가 보고된다. 공정 비교를 위해 교육 설정은 주로 DeiT[61]를 따른다. 구체적으로, 우리는 데이터 증가에 따라 랜덤 크로핑, 랜덤 수평 플핑, 레이블-스모딩 정규화, 믹싱 및 랜덤 소거를 적용한다. I\\(224^{2}\\) 입력 이미지에 대해 훈련할 때, 우리는 모델 최적화를 위해 \\(0.9\\), 총 배치 크기 \\(1024\\) 및 중량 붕괴(0.05\\)의 운동량으로 AdamW[44]를 사용한다. 우리는 코사인 일정, \\(1\\, 10^{-3}\\) 초기 학습률 및 EMA를 사용하여 \\(300\\) epoch에 대한 Vim 모델을 훈련시킨다. 테스트하는 동안 우리는 작물 아웃(224^{2}\\) 이미지에 대한 검증 세트에 중심 작물을 적용한다. 실험은 8 A800 GPU에 대해 수행된다.\n' +
      '\n' +
      '그림 2: 제안된 Vim 모델의 개요. 먼저 입력 영상을 패치로 나눈 후 패치 토큰으로 투사합니다. 마지막으로 제안된 Vim 인코더에 토큰의 시퀀스를 보냅니다. 이미지넷 분류를 수행하기 위해 패치 토큰 시퀀스에 추가 학습 가능한 분류 토큰을 연결한다. 텍스트 서열 모델링을 위해 만바와는 다른 Vim 인코더는 토큰 시퀀스를 전후방향으로 처리한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '* 단방향 서킷. 훈련 중 시각적 순서를 무작위로 뒤집습니다. 이것은 데이터 증강과 같은 역할을 합니다.\n' +
      '* 비방향 블록. 우리는 적층된 블록을 페어링합니다. 각 쌍의 제1 블록은 전방 방향으로 시각적 시퀀스를 처리하고, 각 쌍의 제2 블록은 후방 방향으로 처리한다.\n' +
      '* 비방향 SSM. 우리는 시각 시퀀스를 후방 방향으로 처리하기 위해 각 블록에 대한 추가 SSM을 추가한다.\n' +
      '* 비방향 SSM + Conv1d. 비방향 SSM을 기반으로 후진 SSM 전에 후방 Conv1d를 추가한다(그림 2).\n' +
      '\n' +
      '타브에서 보는 바와 같이. 4, 만바 블록을 직접 채택하면 분류에서 좋은 성능을 얻을 수 있다. 그러나 부자연스러운 단방향 방식은 하류 조밀한 예측에서 문제를 야기한다. 구체적으로, 비방향블록을 사용하는 예비 양방향 전략은 분류에 대한 상위 1의 정확도가 낮은 7점을 달성한다. 그러나 의미 세분화에 대해 바닐라 단방향 만바 블록을 1.3 mIoU 능가한다. 추가 후방 SSM 및 Conv1d를 추가하여 유사한 분류 정확도(73.1 탑-1 acc _vs_ 73.2 탑-1 acc)를 달성했다. 특수 분할 우위와 예외적인 분할 우성(34.8 mIoU _vs_ 32.3 mIoU)이다. 우리는 바이방향 SSM + Conv1d의 전략을 Vim 블록에서 기본 설정으로 사용한다.\n' +
      '\n' +
      '5배제 및 미래 근무.\n' +
      '\n' +
      '우리는 일반 비전 백본으로 매우 최근의 효율적인 상태 공간 모델인 _i.e._, Mamba를 탐색하기 위해 비전 몰바(Vim)를 제안했다. 하이브리드 아키텍처 또는 등가 글로벌 2D 컨볼루션 커널을 사용하는 비전 태스크에 대한 사전 상태 공간 모델과 달리 Vim은 서열 모델링 방식으로 시각적 표현을 학습하고 이미지 특이적 유도 편향을 도입하지 않는다. 제안된 양방향 상태 공간 모델링 덕분에 Vim은 데이터 의존적 글로벌 시각적 컨텍스트를 달성하고, 연산 복잡도가 낮은 동시에 트랜스포머와 동일한 모델링 전력을 누린다. 탐바의 하드웨어 인식 설계에서 요약하면 고해상도 이미지를 처리할 때 Vim의 추론 속도 및 메모리 사용이 ViT보다 훨씬 우수하다. 표준 컴퓨터 비전 벤치마크에 대한 실험 결과는 Vim의 모델링력과 높은 효율을 검증하여 Vim이 차세대 비전 백본일 가능성이 크다는 것을 보여주었다.\n' +
      '\n' +
      '향후 작품에서 위치 임베딩이 있는 양방향 SSM 모델링이 있는 Vim은 마스크 이미지 모델링 사전 실습과 유사한 아키텍처와 같은 비지도 작업에 적합하며, Mamba와의 유사한 아키텍처는 CLIP 방식의 사전 트레이닝과 같은 복합 작업을 가능하게 한다. 사전 절제된 Vim 가중치를 바탕으로 하류의 작업으로 볼 수 있는 고해상도 의료 영상, 원격 감지 영상, 긴 영상 분석을 위한 Vim의 유용성을 탐색하는 것은 매우 간단하다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '초안에 대한 도움이 되는 피드백을 위해 톈정청, 유신포랑, 신정양, 보장, 진펑야오 등을 인정하고자 합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* [2] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: A general spatial layer for visual transformers. _arXiv preprint arXiv:2306.06635_, 2023.\n' +
      '* [3] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasirlar. Introducing our multimodal models, 2023.\n' +
      '* [4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation. _TPAMI_, 2019.\n' +
      '* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.\n' +
      '* [6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_, 2018.\n' +
      '\n' +
      '그림 5: Cascade Mask R-CNN [4] 프레임워크에서 DeiT-Ti[61]과 Vim-Ti의 표준화 비교는 그림 5이다. SSM의 장거리 맥락 학습 덕분에 DeiT-Ti 상대가 인지하지 못하는 영상에서 매우 큰 객체를 캡처할 수 있다.\n' +
      '\n' +
      '* [7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_, 2019.\n' +
      '* [8] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In _ICLR_, 2021.\n' +
      '* [9] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. _NeurIPS_, 34, 2021.\n' +
      '* [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [11] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. _arXiv preprint arXiv:2307.02486_, 2023.\n' +
      '* [12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In _CVPR_, 2022.\n' +
      '* [13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In _CVPR_, 2022.\n' +
      '* [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2020.\n' +
      '* [15] Stephane d\'Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In _ICML_, 2021.\n' +
      '* [16] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging local spatial information by manipulating messenger tokens. In _CVPR_, 2022.\n' +
      '* [17] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _CVPR_, 2023.\n' +
      '* [18] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hupps: Towards language modeling with state space models. In _ICLR_, 2023.\n' +
      '* [19] Golanz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In _CVPR_, 2021.\n' +
      '* [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [21] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.\n' +
      '* [22] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Re. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In _NeurIPS_, 2021.\n' +
      '* [23] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. In _NeurIPS_, 2022.\n' +
      '* [24] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In _NeurIPS_, 2022.\n' +
      '* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.\n' +
      '* [26] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _CVPR_, 2017.\n' +
      '* [27] Md Mohaimul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In _ECCV_, 2022.\n' +
      '* [28] Md Mohaimul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers. In _CVPR_, 2023.\n' +
      '* [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* [30] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n' +
      '* [31] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019.\n' +
      '* [32] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In _ICLR_, 2020.\n' +
      '* [33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _NeurIPS_, 2012.\n' +
      '* [34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.\n' +
      '* [35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.\n' +
      '* [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [37] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In _ICLR_, 2022.\n' +
      '* [38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _ECCV_, 2022.\n' +
      '\n' +
      '* [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [41] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Karkkainen, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. _arXiv preprint arXiv:2207.03620_, 2022.\n' +
      '* [42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.\n' +
      '* [43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _CVPR_, 2022.\n' +
      '* [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* [45] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. _arXiv preprint arXiv:2401.04722_, 2024.\n' +
      '* [46] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In _ICLR_, 2023.\n' +
      '* [47] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher Re. S4nd: Modeling images and videos as multidimensional signals with state spaces. In _NeurIPS_, 2022.\n' +
      '* [48] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In _NeurIPS_, 2023.\n' +
      '* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In _CVPR_, 2020.\n' +
      '* [51] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* [52] Jimmy TH Smith, Shalini De Mello, Jan Kautz, Scott Linderman, and Wonmin Byeon. Convolutional state space models for long-range spatiotemporal modeling. In _NeurIPS_, 2023.\n' +
      '* [53] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In _ICLR_, 2023.\n' +
      '* [54] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _ICCV_, 2021.\n' +
      '* [55] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023.\n' +
      '* [56] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _CVPR_, 2015.\n' +
      '* [57] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, 2019.\n' +
      '* [58] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _ICML_, 2021.\n' +
      '* [59] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. In _NeurIPS_, 2021.\n' +
      '* [60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _ICML_, 2021.\n' +
      '* [62] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaedin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al. Resmlp: Feedforward networks for image classification with data-efficient training. _TPAMI_, 2022.\n' +
      '* [63] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _TPAMI_, 2020.\n' +
      '* [64] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In _CVPR_, 2023.\n' +
      '* [65] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* [66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In _ICCV_, 2021.\n' +
      '* [67] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Intermimage: Exploring large-scale vision foundation models with deformable convolutions. In _CVPR_, 2023.\n' +
      '* [68] Wenhui Wang, Shuming Ma, Hanwen Xu, Naoto Usuyama, Jiayu Ding, Hoifung Poon, and Furu Wei. When an image is worth 1,024 x 1,024 words: A case study in computational pathology. _arXiv preprint arXiv:2312.03558_, 2023.\n' +
      '* [69] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In _ICCV_, 2021.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [71] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In _ECCV_, 2018.\n' +
      '* [72] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _CVPR_, 2017.\n' +
      '* [73] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models without attention. _arXiv preprint arXiv:2311.18257_, 2023.\n' +
      '* [74] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. _IJCV_, 2019.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>