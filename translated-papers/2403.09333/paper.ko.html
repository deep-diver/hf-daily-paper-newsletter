<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Griffon v2: 고해상도 스케일링과 시각적 언어 공동 참조를 이용한 멀티모달 인식 향상\n' +
      '\n' +
      'Yufei Zhan\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 과학원, 베이징, 중국 12 인공지능학부\n' +
      '\n' +
      '중국 북경 중국과학원\n' +
      '\n' +
      'Yousong Zhu\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 베이징 중국과학원\n' +
      '\n' +
      'Hongyin Zhao\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 베이징 중국과학원\n' +
      '\n' +
      'Fan Yang\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 과학원, 베이징, 중국 12 인공지능학부\n' +
      '\n' +
      '중국 베이징 중국과학원 중국선전 23펑청연구소 중국 우한 34우한 AI연구소\n' +
      '\n' +
      'Ming Tang\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 과학원, 베이징, 중국 12 인공지능학부\n' +
      '\n' +
      '중국 북경 중국과학원\n' +
      '\n' +
      'Jinqiao Wang\n' +
      '\n' +
      '자동화연구소 1기초모델연구센터\n' +
      '\n' +
      '중국 과학원, 베이징, 중국 12 인공지능학부\n' +
      '\n' +
      '중국 베이징 중국과학원 중국선전 23펑청연구소 중국 우한 34우한 AI연구소\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 비전 언어 모델은 세밀한 객체 인식을 달성했지만 이미지 해상도의 한계는 복잡하고 밀집된 시나리오에서 작업별 전문가의 성능을 능가하는 중요한 장애물로 남아 있다. 이러한 제한은 GUI 에이전트, 카운팅 및 _etc_와 같은 도메인에서 참조하는 미묘한 시각적 및 언어를 달성할 수 있는 모델의 잠재력을 더욱 제한한다. 이 문제를 해결하기 위해 우리는 통합된 고해상도 일반 모델인 그리폰 v2를 도입하여 시각적 및 텍스트 프롬프트를 참조하는 유연한 객체를 가능하게 한다. 이미지 해상도를 효율적으로 확장하기 위해, 대용량 언어 모델에서 입력 토큰의 제약을 극복하기 위해 간단하고 가벼운 다운 샘플링 프로젝터를 설계한다. 이 디자인은 본질적으로 완전한 맥락과 미세한 세부 사항을 보존하고 특히 작은 물체에 대한 멀티모달 인식 능력을 크게 향상시킨다. 이를 기반으로 플러그 앤 플레이 비주얼 토큰화기를 통해 모델을 시각 언어 공동 참조 기능을 추가로 갖추고 있습니다. 유연한 대상 이미지, 자유로운 형태의 텍스트, 심지어 좌표와도 사용자 친화적인 상호 작용을 가능하게 합니다. 실험 결과, 그리폰 v2는 시각적 및 텍스트적 참조를 통해 관심 객체를 로컬화하고 REC, 구문 접지 및 REG 작업에서 최첨단 성능을 달성하며 객체 탐지 및 객체 카운팅에서 전문가 모델을 능가할 수 있음을 보여준다. 데이터, 코드 및 모델은 [https://github.com/jefferyZhan/Griffon](https://github.com/jefferyZhan/Griffon)에서 공개될 것이다.\n' +
      '\n' +
      '키워드: 대규모 비전 언어 모델 멀티모달 인식 고해상도 구조 시각적 언어 공동 참조\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 비전 언어 모델(LVLMs, 26, 2, 13, 59)은 이미지-텍스트 이해(3, 12) 및 추론(16)의 돌파 후 참조 표현 이해(REC, Reference Expression Comprehension)와 같은 지역 수준 태스크에서 유망한 성능을 보여준다. 특히, 그리폰[55]과 같은 모델들은 객체 검출 작업에서 더 강력한 인식 능력을 입증했다. 이는 더 나은 사용자 상호 작용을 위해 텍스트 설명만을 넘어 객체의 유연한 참조 개발에 더욱 박차를 가했다.\n' +
      '\n' +
      '이러한 진전에도 불구하고, 현재의 LVLM은 이미지 해상도에 의해 걸려든 미세한 작업에서 작업별 전문가를 능가하는 병목 현상을 여전히 충족한다. 즉, 그들은 미묘한 시각적 세부 사항을 거의 포착할 수 없어 수많은 사실 충돌 환각을 초래한다. 지역 기반 질문에 근거 없이 대답하기[35], 문자 관련 과제에서 작은 단어에서 실패하기[30] 또는 잘못된 카운팅 결과를 제공하기[20]와 같은 저해상도 시나리오를 다룰 때 특히 분명하다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 최근 연구에서는 LVLM을 참조하여 해상도 향상 및 유연한 시각적 언어를 탐구했다. 한편으로, 이전의 방법들 [10, 11]은 점진적으로 해상도를 향상시키기 위한 점진적 트레이닝 접근법을 채택한다. 그러나, LLM(Large Language Models)의 최대 입력 길이는 더 높은 이미지 해상도를 달성하는 데 제약을 준다. 또한, 일부 접근법 [22, 25, 27]은 이미지를 더 작은 패치로 분할하고 줌인을 위해 별도로 인코딩한다. 이 분할 기반 설계는 패치의 컨텍스트 및 에지 디테일을 손실시키고 계산 복잡도를 증가시키는 차선의 멀티모달 인식 능력을 증명한다[22]. 한편, 선행 연구들은 저해상도 영상(_e.g._224\\(\\times\\)224 또는 448\\(\\times\\)448)을 기반으로 특정 영상 콘텐츠에 대한 이해를 향상시키기 위해 다양한 형태의 참고문헌[5, 8, 57]을 주로 연구해 왔다. 그러나 이러한 방법은 종종 눈에 띄는 것을 인식하는 데 탁월하고, 그리고\n' +
      '\n' +
      '그림 1: Griffon v2의 개요. Griffon v2는 멀티모달 모델에 대한 고해상도 입력 및 시각적 언어 공동 참조를 가능하게 한다. 참조 측면에서, 사용자는 좌표, 텍스트 설명, 스크린샷 및 크로스 이미지 모드를 통해 객체를 참조할 수 있다. 그리폰 v2는 임의의 객체를 로컬화하는 데 탁월하고 광범위한 시나리오에 걸쳐 공동 참조와 함께 설명을 생성한다.\n' +
      '\n' +
      '이미지 수준 객체이지만 세밀한 로컬 객체를 정확하게 로컬화하고 설명하는 데 부족합니다. 또한, 단일 시각적 또는 언어 프롬프트만으로는 대화 능력이 부족하거나 언어 설명에 의해 제약을 받으므로[15], 포괄적인 사용자 친화적인 대화형 경험을 제공하지 못한다.\n' +
      '\n' +
      '본 논문에서는 고해상도 악을 개방하고, 시각적 언어 공동 참조를 통해 관심 대상을 찾는 그리폰 v2를 제안한다. 고해상도 이미지를 더 작은 패치로 분할하는 대신 고해상도 비주얼 인코더를 사용하여 표현을 추출하고, 비주얼 토큰의 길이를 압축하기 위해 스트라이드 컨볼루션이 있는 간단하고 가벼운 다운 샘플링 프로젝터를 설계한다. 그런 다음 압축된 시각적 토큰은 텍스트 토큰과 정렬하도록 훈련되고 LLaVA[26]와 같은 추가 융합을 위해 LLM에 공급된다. 복잡한 리샘플러 구조[25] 및 다중 서브 이미지 모드에 비해 제안된 파이프라인은 매개변수 효율적이고 계산적으로 간결하다. 더 중요한 것은, 고해상도 입력에 대한 모델의 세밀한 인식을 향상시키기 위해 시각적 언어 공동 참조 패러다임을 구축하여 모델의 적용 가능성을 크게 확장한다는 것이다. 로컬 크롭 이미지, 텍스트 및 좌표 프롬프트를 지원하며, 대상 객체 또는 해당 텍스트 설명의 좌표를 출력하여 다양한 대화형 능력을 제공하여 특이 시각 프롬프트의 대화 결함 및 텍스트 프롬프트의 잠재적 표현 모호성을 완화한다. 마지막으로 사전 교육을 위한 12M 공개 현지화 데이터와 미세 조정을 위한 900K 명령어 데이터를 수집했다. REC 태스크, 프레이즈 접지 태스크, 참조 표현 생성(REG) 태스크에서 최신의 결과를 달성한다. 특히, 우리의 모델은 처음으로 객체 검출 및 객체 카운팅 전문가 모델보다 우수하다.\n' +
      '\n' +
      '요약하면, 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 영상의 분할 없이 1K까지의 해상도를 원천적으로 지원하는 지역 이해도를 높이기 위한 고해상도 멀티모달 인식 모델을 제안한다.\n' +
      '2. 모델의 적용 범위를 넓히고 다양한 상호작용 모드를 제공하는 시각적 언어 공동 참조 구조를 소개한다.\n' +
      '3. 다양한 국산화 관련 작업에 대한 실험을 수행하였고, REC, 프레이즈 접지 및 REG에 대한 최신 성능을 입증하였다. 우리는 객체 검출 작업 객체 계수에서 전문가 모델을 정성적으로 능가한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '위치추정을 이용한### 대형 시각언어 모델\n' +
      '\n' +
      'LVLM[1, 2, 13, 26]에서 상당한 발전이 이루어짐에 따라 LVLM을 사용하여 필수적인 기초 시각적 작업으로서 객체 위치를 탐색했다. 이러한 방법[23, 39, 49, 58]의 한 가지 분기는 API를 통해 탐지 전문가 모델을 호출하는 데 중점을 둔다. LVLM이 검출을 필요로 할 때, LLM은 검출 모델을 호출하고 추가 처리를 위해 결과를 수신하기 위한 명령을 발행한다. 이 접근법은 모델의 복잡성을 증가시키고 LVLM으로 통합된 구조를 즐길 수 없다. 방법들의 또 다른 세트 [4, 8, 11, 51]은 미세-그레인 단일-객체 로컬라이제이션, _i.e._ REC에 초점을 맞춘다. 이러한 방법들은 REC 태스크 데이터를 명령어 데이터로 변환하고 상이한 표현 접근법들을 갖는 좌표들을 인코딩함으로써 LVLM 자체가 로컬화 능력들을 소유할 수 있게 한다. 그리핀의 등장[55]까지 LVLM의 객체 위치 추정 능력은 다중 객체 및 임의의 입도로 확장된다. 본 논문에서는 기존의 분할 기반 해상도 향상 방법[22, 25]보다 더 효과적이고 효율적인 것으로 증명된 전문가 모델을 넘어 LVLM의 탐지 능력을 높이는 고해상도 구조를 제안한다.\n' +
      '\n' +
      '멀티모달 모델을 이용한### 객체 참조\n' +
      '\n' +
      '대형 멀티모달 모델에서 LLMs[1, 41]을 적용한 이후, 텍스트 기술은 객체 참조를 위한 가장 간단한 방법이 되었다. 사용자는 "이 고양이 앞에 무엇이 있는가"와 같이 특별한 주의조차 기울이지 않고 자연스럽게 이 방법을 활용하여 질문하는데 단순함에도 불구하고 수많은 유사 항목 중 특정 대상을 뚜렷하게 지칭하는 데 어려움을 겪으며 부정확한 결과를 산출할 수 있는 모호한 언급으로 이어진다. 지역 이해의 중요성이 커지면서 복잡하고 밀집된 장면에서 공간 참조에 대한 탐구를 촉발했다. 현재 연구에서는 텍스트 좌표[4, 7, 8], 학습 가능한 임베딩[33], 관심 영역 특징[57]을 갖는 공간 참조를 제안하였다. 일부 접근법은 [1, 5, 51]을 사용자 친화적인 관점에서 업그레이드하고, 화살표, 마크 등을 지원한다. 작업 및 세분성에 대한 초점의 한계로 인해 이러한 접근법은 고해상도도에 대한 추가 고려 없이 주로 단일 촬영장비 내에서 언급하는 데 중점을 둔다. 대조적으로, 우리의 모델은 서로 다른 입도에서 인식, 현지화 및 이해도를 동시에 고려한다. 따라서, 우리는 사용자 친화적이면서도 다양한 작업에서 정확한 참조를 가능하게 하기 위해 시각적 언어 공동 참조 접근법을 사용한다.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      '본 절에서는 그림 2와 같이 그리폰 v2에 대한 간략한 개요를 시작으로, 핵심 기반으로서 Sec. 3.2에서 고해상도 구조 설계를 설명하고 Sec. 3.3에서 시각적 언어 공동 참조와 Sec. 3.4에서 훈련 파이프라인을 나타낸다.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 2, 그리폰-V2는 자동 회귀 패러다임을 채택하여 통합 언어 모델링 목표 내에서 참조 및 접지 작업을 원활하게 통합한다. 주어진 입력 영상\\(X_{v}\\)에 대해, 사전 학습된 EVA2-CLIP-L/14@336[40] 모델로부터 쌍선형 보간법에 의해 고해상도에 적응된 시각적 인코더를 활용하여 고해상도 시각적 특징(_e.g. over 1K)을 추출한다. 동시에, 설계된 다운 샘플링 프로젝터는 이러한 이미지 특징을 시각적 임베딩 토큰 \\(H_{v}\\)으로 변환한다. 사용자가 스크린샷 또는 추가 대상 이미지로 시각적 프롬프트 \\(X_{q}\\)를 커스터마이징하는 경우, 시각적 토네이저를 사용하여 시각적 참조 이미지로부터 영역 임베딩 \\(H_{q}\\)을 유도한다. 한편, 우리는 토큰화 후 텍스트 임베딩에 투영되는 텍스트 설명 및 좌표와 같은 객체를 식별하기 위해 언어 참조인 \\(X_{ins}\\)을 본질적으로 지원한다. 이어서, 이미지 임베딩과 함께 시각적 프롬프트 또는 텍스트 임베딩은 LLM, 구체적으로 LLaMA2-13B[41]를 사용하여 프로세싱을 거쳐, 원하는 답변 \\(X_{a}\\)이 생성된다. 특히, 바운딩 박스의 표현은 특수 토큰 및 특수 헤드의 사용을 피하고 대신에 텍스트 수치를 통합함으로써 단순성을 유지한다.\n' +
      '\n' +
      '다운샘플 프로젝터를 이용한 고해상도 스케일링\n' +
      '\n' +
      '고해상도는 미묘한 시각적 특징의 정확한 해석을 향상시켜 시각적 작업과 시각 언어 작업 모두에 도움이 되는 것으로 경험적으로 입증되었다. 이전 방법[22]에서는 일반적으로 더 작은 패치로 분할되어 전체 해상도가 762 또는 896으로 확대되지만, 이러한 방법은 문맥 일관성 및 패치 간의 에지 상세 표현 측면에서 한계에 직면한다. 프로그레시브 스케일링 방법은 4096 입력 토큰의 제약 내에서 해상도를 784(패치 크기 = 14)로 확장할 수 있지만, 광범위한 긴 텍스트를 포함하는 출력을 처리하기에 부적합하고 계산 복잡도를 상당히 증가시킨다.\n' +
      '\n' +
      '그림 2: 시각 언어 공동 참조가 있는 제안된 고해상도 그리폰 v2의 구조.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 1K 이상의 해상도를 지원할 수 있는 다운 샘플링 프로젝터를 통합한 고해상도 구조를 소개한다. 임의의 치수의 입력 영상을 수신한 후, 미리 정의된 입력 높이 및 너비로 크기를 조정하여 \\(X_{v}\\in\\mathbb{R}^{H\\times W\\times 3}\\)으로 표시한다. 영상 분할을 사용하지 않고, 쌍선형 보간법에 의해 사전 훈련된 모델에서 적응된 훈련 가능한 고해상도 시각적 인코더를 사용하여 영상을 인코딩하여 시각적 특징\\(Z_{v}=f_{V}(X_{v})\\을 산출한다. 저해상도 고정 인코더와 달리, 우리의 접근법은 더 미세한 세부 사항을 포착하는 데 탁월합니다. 도 1의 좌측부에 도시된 바와 같다. 다음으로, 학습 목표의 안내에 따라 압축과 함께 필수적인 특징을 추상화하고 시각적 특징을 단어 임베딩 공간에 연결하기 위해 경량 다운 샘플링 프로젝터를 소개한다. 구체적으로, 스트라이드 컨볼루션 층(\\(Conv\\))은 단순히 \\(S\\)에 의해 특징들을 다운샘플링하기 위해 적용되고, 프로젝션 매트릭스 \\(W\\)은 \\(Z_{v}\\)을 시각적 임베딩 토큰들로 변환하기 위해 활용된다:\n' +
      '\n' +
      '\\[H_{v}=W\\cdot Conv(Z_{v}),\\ with\\Z_{v}=f_{V}(X_{v}). \\tag{1}\\\n' +
      '\n' +
      '비주얼 토큰의 최종 개수는 입력 크기 및 컨볼루션 스트라이드, _i.e. \\ (H,W\\) 및 \\(S\\). 설계 효과를 입증하기 위해 커널 크기가 3이고 보폭이 2인 컨볼루션 레이어를 사용하고 그림 3에서 여러 가지 다른 해상도를 조사한다. 도 3을 참조하면, 2-계층 선형투영[26]을 축소 없이 사용하는 700과 448의 해상도에서 700의 해상도와 우리의 구조를 비교한 결과, 더 높은 mAP와 더 적은 토큰을 보여준다. 이는 본 논문에서 제안하는 방법이 세부 정보를 보존하면서 특징을 추출할 수 있어 토큰 중복성을 줄일 수 있음을 나타낸다. 또한 해상도를 더 높이면 성능이 향상될 수 있다는 것이 관찰된다.\n' +
      '\n' +
      '### Visual-Language Co-Referring\n' +
      '\n' +
      '이전에 입증된 단수 시각적 또는 언어 참조의 한계, 대화 능력 부족 및 잠재적 모호성을 각각 인식하고,\n' +
      '\n' +
      '도 3: 상이한 해상도의 성능 및 시각적 토큰 번호 비교. 우리는 컨볼루션 레이어에 대해 2에서 보폭을 커널 크기 3으로 고정하고 분석을 위해 해상도를 변경한다. 448-해상도 모델은 2층 선형 프로젝터[26]를 사용하는 반면, 다른 모델은 다운 샘플링 프로젝터를 사용한다. 우리의 방법은 세부 사항을 보존하면서 특징을 추출할 수 있어 토큰 중복성을 줄일 수 있다.\n' +
      '\n' +
      '시각 언어 공동 참조를 용이하게 하기 위해 강력한 의미 특징을 추출하는 CLIP의 능숙성을 활용합니다. 이 접근법은 이러한 단점을 완화하고 고해상도 조건에서 더 광범위한 적용 가능성을 찾는다.\n' +
      '\n' +
      '**Visual Referring.** 다양한 기존의 방법들이 원시 이미지에 마킹하는 것부터 객체 마스킹에 이르기까지 다양한 시각적 참조 형태를 제안하지만, 이들 중 하나는 원시 이미지 콘텐츠를 변경하거나[1] 특별히 설계된 추출기를 필요로 한다[51]. 효율성과 참조 정확도 사이의 균형을 맞추기 위해 시각적 참조 이미지를 사용하여 관심 대상을 나타내는 간단한 방법을 선택한다. 구체적으로, 도 1의 중간에 도시된 바와 같다. 2는 사용자가 제공하는 참조 영상\\(X_{q}\\)이 주어지면 영역 인코더인 _i.e_를 사용한다. EVA-CLIP-L/14@224[40]는 시각적 프롬프트 특징 \\(Z_{q}\\)을 추출하기 위해 ViT를 사전 훈련했다. 시각적 참조에 대한 LLM의 이해를 텍스트 참조와 일치시키기 위해 영역 특징을 단어 임베딩 공간에 투영하여 시각적 프롬프트 토큰 \\(H_{q}\\)을 생성한다. 수십억 개의 인스턴스(36)의 데이터셋에서 사전 학습된 \\([CLS]\\) 토큰의 강건한 의미 표현 능력을 기반으로, 이전 응용 프로그램 이후에 다른 토큰을 버리고 특정 객체를 표현하기 위해 \\([CLS]\\) 토큰을 활용한다. 이 접근법을 사용하면 범주 수준에서 객체를 참조할 수 있어 픽셀 패치에서 과도하게 특정 정보를 제거하고 참조의 안정성을 유지할 수 있다. 다양한 대화 시나리오를 수용하기 위해 지침에 "<영역>"을 사용하여 그림과 같이 참조 대상을 나타낸다. 4(b). LLM에 입력하기 전에 이 자리 표시자를 참조된 객체의 임베딩으로 대체하여 최적의 일관성을 위해 매끄럽게 통합한다.\n' +
      '\n' +
      '**언어 참조** 텍스트 참조는 LLM 기반 멀티모달 모델의 고유한 기능입니다. 텍스트적 언급의 단순성에도 불구하고 모호한 언급으로 오답을 초래하는 도전에 직면해 있다. 이는 자연스럽게 텍스트와 영역을 연관시키는 것이 LLM이 텍스트-영역 참조에 기초하여 해당 객체를 이해할 수 있게 하여 작업 지침을 이행하고 단순성 이점을 유지할 수 있는지 여부를 고려하도록 자극했다. 따라서 그림과 같이 텍스트 좌표 참조를 추가로 지원한다. 4(a). 상세한 설명에 더하여, 객체는 또한 그것의 상단-좌측 및 하단-우측 좌표에 의해 참조될 수 있다. 이 경우, 도 2의 우측에 묘사된 \\(H_{ins}\\)와 같이 토큰화 및 단어 공간으로의 임베딩을 수반하는 텍스트 설명과 동일하게 처리된다. 명령어와의 훈련을 통해\n' +
      '\n' +
      '도 4: 대화에서의 시각적-언어적 공동-참조 사용의 예.\n' +
      '\n' +
      '객체 참조가 텍스트 좌표로 변환된 데이터에 따라 LLM이 객체의 텍스트 좌표를 기반으로 복잡하고 밀집된 장면에서 객체를 정확하게 참조할 수 있음을 보여준다. 텍스트와 비전을 모두 포함하는 협력적 참조를 통해, 우리의 접근법은 최적의 참조 성능과 유연한 상호 작용을 달성한다.\n' +
      '\n' +
      '### Training Pipeline\n' +
      '\n' +
      '다양한 사용자의 의도를 이해하고 수준 높은 결과로 다양한 작업을 완료하기 위해 [26, 55]에서 안내하는 다음과 같이 그리폰 v2에 대한 3단계 종단간 훈련 절차를 채택한다. 사용된 데이터 세트는 탭 1에 나열되어 있다.\n' +
      '\n' +
      '**단계 I: 고해상도 비전-언어 정렬.** 사전 훈련 전의 특징 정렬은 더 나은 훈련 효율을 달성하기 위해 널리 활용되었다. 우리는 [26]에 의해 명령어 추적 데이터로 변환된 558K 이미지-텍스트 쌍을 사용하여 고해상도 비주얼 인코더와 LLM을 연결하기 위해 이 전략을 채택한다. 고해상도 이미지 인코더 및 LLM 매개변수는 다운 샘플링 프로젝터만 훈련 가능한 상태로 동결 상태로 유지된다. 이러한 이미지-텍스트 쌍에서 시각적 프롬프트가 없으면, 시각적 프롬프트 토큰화기는 스테이지 I의 훈련에 참여하지 않는다.\n' +
      '\n' +
      '**단계 II: 공동 참조 다중 작업 사전 훈련.** 단계 I에서 달성된 시각적 콘텐츠에 대한 기본 이해도를 기반으로, 우리는 시각적 참조와 텍스트 참조를 모두 포함하는 12M 다중 작업 인스턴스로 구성된 다양한 데이터 세트로 전체 모델을 추가로 사전 훈련한다. 이 단계는 세밀한 인식 및 현지화 능력, 시각적 언어 참조 능력을 향상시키는 것을 목표로 한다. 탭에 표시된 대로입니다. 1, 텍스트 참조 데이터는 Visual Genome[19] 및 V3Det[42]와 같은 70K 이상의 객체 범주를 포함하는 다양한 작업 데이터 세트에서 큐레이션된다. 또한 항공 사진, 농업, 산업 등에 걸쳐 10개 도메인의 공개 및 자체 수집 객체 계수 데이터로 데이터 세트를 구축하여 시각적 참조 데이터를 생성한다. 다양한 텍스트 범주와 시각적 영역은 모델의 일반화 능력에 기여한다. 중간에\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Period & Data Vol. & Annotation Type & Sources \\\\ \\hline Stage I & 558K & Image-text & LLaVA \\\\ \\hline \\multirow{4}{*}{Stage II} & \\multirow{4}{*}{12M} & Object Detection & Objects 365, MSCOCO \\\\  & & REC/REG & Visual Genemo, RefCOCO series \\\\ \\cline{1-1}  & & Visual Grounding & V3Det, LVIS, Flickrs30K Entities \\\\ \\cline{1-1}  & & Object Counting & CA-44, OpenImages v4, Self-collected data \\\\ \\cline{1-1}  & & Non-existing Judging & LVIS \\\\ \\hline Stage III & 900K & Instruction-following & Build from stage 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 단계의 데이터 통계량. 우리는 다른 주석의 데이터를 프롬프트/명령어-답변 양식으로 변환합니다. 우리는 2단계와 3단계에서 자체 수집 및 가공된 데이터를 공개할 것이다. 자세한 내용은 보충제에서 확인할 수 있습니다.\n' +
      '\n' +
      '이 단계에서는 시각 프롬프트 토큰화기에서 영역 인코더를 동결 상태로 유지하면서 전체 네트워크를 훈련한다.\n' +
      '\n' +
      '**단계 III: Intent-Enhanced Instruction Tuning.** 단계 II의 사전 훈련에 이어, 모델은 자유 형식의 텍스트 및 유연하게 획득된 시각적 참조 이미지로 관심 대상을 찾고 설명하는 능력을 얻는다. 사용자가 맞춤형 및 강화하여 특정 작업을 기초 모델로 달성할 수 있습니다. 사용자 의도에 대한 이해 능력을 더욱 향상시키기 위해, 우리는 단계 II의 거의 900K 명령어 후속 데이터 구축과 보충에 자세히 설명된 다양한 작업에 대한 보다 다양한 명령어 프롬프트로 모델을 세분화한다. 또한, 정제된 시각적 특징 추출을 보존하고 망각을 방지하기 위해 고해상도 시각적 인코더와 영역 인코더를 동결하고 프로젝터와 함께 LLM을 훈련한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 먼저 Sec. 4.1에서 주요 구현 세부 사항을 소개하고, 이어서 REC, REG, 객체 탐지, 구문 접지 및 객체 계수에서 그리폰 v2와 선도 LVLM을 비교한다. 또한, Griffon v2의 모델 설계를 Sec. 4.4의 절제 연구를 통해 분석하였고, 정성적 분석 결과는 Sec. 4.5에 존재한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'ResNet[14]의 널리 사용되는 구성에 따라, 다운 샘플링 프로젝터의 컨볼루션 레이어를 커널 크기 3, 스트라이드 2, 패딩 1로 설정하였으며, 이미지 해상도의 선택은 사전 학습된 CLIP의 패치 크기와 LLaMA2의 제약 조건 하에서 데이터의 토큰 길이를 고려하여 최대 4096개의 입력 토큰을 지원한다. 기존의 L-size CLIP 모델을 이용하여 패치 크기가 보통 14 또는 16인 L-size CLIP 모델을 사용하였으며, 카운팅 데이터의 평균 텍스트 토큰 길이가 2500에 가까울수록 최대 달성 가능 해상도는 각각 1369 토큰 또는 1024 토큰에 해당하는 1022(패치 크기 = 14) 또는 비교 가능한 1024(패치 크기 = 16)이다. 탭에 표시된 대로입니다. 도 7을 참조하면, EVA-CLIP-L/14가 가장 우수한 성능을 가짐에 따라 해상도를 1022로 설정하였으며, 위치 임베딩 보간법을 이용하여 EVA2-CLIP-ViT-L/14@336을 적응시킨 비주얼 인코더를 초기화하고, LLaMA2를 이용한 LLM으로 다운 샘플링 프로젝터와 비주얼 토큰라이저의 프로젝터를 랜덤 초기화하였다. 보충제에서 더 자세한 내용을 확인할 수 있습니다.\n' +
      '\n' +
      '기본접지와 참고문헌의### 평가\n' +
      '\n' +
      '기본 접지 및 참조는 일반적으로 하나의 기존 객체를 포함하는 REC 태스크 및 REG 태스크, 및 제한된 수의 다중 타겟을 갖는 구문 접지 태스크를 주로 포함한다. 그리핀 v2는 이 세 가지 작업에 걸쳐 전문가 및 일반 모델과 체계적으로 비교된다.\n' +
      '\n' +
      '**REC.** REC 태스크는 텍스트 설명이 있는 단일 객체를 접지하는 기본 로컬화 태스크로서 LVLM에서 광범위하게 연구되었다. RefCOCOg[52], RefCOCO+[52], RefCOCOg[31]에 대한 실험을 수행하였다. 탭의 실험 결과입니다. 도 2는 본 모델이 여러 세트에서 최신 최신 모델보다 성능이 우수함을 보여준다. 이에 대해 자세히 설명하면, 본 모델은 작은 객체를 포함하여 여러 객체를 인식할 수 있고, 지시 대상에 해당하는 모든 객체를 찾을 수 있는 반면, 다른 모델은 이 작업에 대해 단일 출력만 생성한다는 것이다. 그러나 RefCOCO+와 같은 집합에서 불완전하거나 모호한 주석이 더 많이 관찰되어 객체에 고유하지 않은 속성을 추가하여 평가에 영향을 미친다. 보충제에 대해 자세히 설명합니다. 현재 선도 모델인 CogVLM과 비교하여 우리는 패러다임에 따라 모델을 훈련하고 그 결과는 거의 모든 세트에 걸쳐 우수한 성능을 나타내어 우리의 발전을 보여준다.\n' +
      '\n' +
      '**REG.**REG는 그들의 영역 위치들에 기초하여 특정된 객체들에 대한 간결한 설명을 생성하는 것을 목표로 한다. 우리는 RefCOCOg[31] val set에 대해 물체 참조와 테스트를 위한 텍스트 좌표를 입력하였다. 탭에 표시된 대로입니다. 셋째, 참조에 학습 가능한 임베딩을 사용하는 KOSMOS-2와 달리, 의미적 유사성에 중점을 두어 CIDEr에서 우수한 성능을 달성한 반면, Meteor은 단어의 정확성에 더 중점을 두어 LLM의 개방형 기술 생성에 적합하지 않다.\n' +
      '\n' +
      '**구 접지.**구 접지 작업은 REC 작업에 비해 더 큰 도전을 제공하며 Flickrs30K 엔터티에서 평가된다[34]. 두 개의 평가 프로토콜[17]이 사용되며, 여기에는 ANY-BOX 프로토콜과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Type} & \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{3}{c}{RefCOCOg} \\\\ \\cline{3-10}  & & val & testA & testB & val & testA & testB & val & test \\\\ \\hline \\multirow{5}{*}{\\begin{tabular}{c} \\end{tabular} } & MDETR [17] & 87.5 & 90.4 & 82.7 & 81.1 & 85.5 & 73.0 & 83.3 & 83.3 \\\\  & G-DINO-L [29] & 90.6 & 93.2 & 88.2 & 82.8 & 89.0 & 75.9 & 86.1 & 87.0 \\\\  & UNINEXT-L [48] & 91.4 & 93.7 & 88.9 & 83.1 & 87.9 & 76.2 & 86.9 & 87.5 \\\\  & Griffon-13B\\(\\dagger\\)[55] & 90.1 & 93.4 & 86.1 & 84.8 & 90.5 & 77.8 & 86.1 & 87.2 \\\\  & CogVLM [44] & 92.5 & 93.9 & 88.7 & 87.5 & 91.8 & 81.5 & 89.5 & 90.1 \\\\  & Griffon v2\\(\\dagger\\) & 92.3 & 94.0 & 89.5 & 88.7 & 92.3 & 82.8 & 90.2 & 90.2 \\\\ \\hline \\multirow{5}{*}{\n' +
      '\\begin{tabular}{c} \\end{tabular} } & OFA-L [43] & 80.0 & 83.7 & 76.4 & 68.3 & 76.0 & 61.8 & 67.8 & 67.5 \\\\  & KOSMOS-2 [33] & 52.3 & 57.4 & 47.3 & 45.5 & 50.7 & 42.2 & 60.6 & 61.7 \\\\  & Shikra-13B [8] & 87.8 & 90.6 & 80.2 & 82.9 & 87.8 & 74.4 & 82.6 & 83.2 \\\\  & Qwen-VL [4] & 89.4 & 92.3 & 85.3 & 83.1 & 88.3 & 77.2 & 85.6 & 85.5 \\\\  & Ferret-13B [51] & 89.5 & 92.4 & 84.4 & 82.8 & 88.1 & 75.2 & 85.8 & 86.3 \\\\  & Griffon-13B [55] & 88.0 & 92.1 & 81.9 & 81.5 & 88.2 & 73.3 & 82.9 & 84.3 \\\\ \\cline{1-1}  & \\multicolumn{2}{c}{**Griffon v2**} & **89.6** & **91.8** & **86.5** & **81.9** & **85.5** & **76.2** & **85.9** & **86.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 참조 표현 이해 결과. REC는 \\(ACC@0.5\\)의 메트릭을 이용한다. \\ (\\dagger\\)는 비교를 위해 접지 전용 데이터를 사용하여 CogVLM-17B의 패러다임을 따른 결과를 나타낸다.\n' +
      '\n' +
      'MERGE-BOXES 프로토콜입니다 Any-BOX 프로토콜은 각 인스턴스의 원자성에 초점을 맞추고, MERGE-BOXES 프로토콜은 모델이 병합된 상자로 참조된 모든 객체를 식별하는지 여부를 평가한다. 기존의 LVLM은 일반적으로 단일 지시 시나리오로 제한되며, 위상당 하나의 박스만을 예측하는 경향이 있으며, 이에 따라 MERGED-BOXES 프로토콜을 사용한다. 탭에 표시된 대로입니다. 도 4에 도시된 바와 같이, 그리폰 v2는 임의의-BOX 프로토콜에서 최첨단 결과를 달성하고, MERGE-BOX 프로토콜에서 대부분의 전문가 및 일반의를 능가하며, 보다 세밀한 박스들을 갖는다.\n' +
      '\n' +
      '복잡도 검출 및 카운팅의### 평가\n' +
      '\n' +
      '객체 검출 및 카운팅은 필수적인 시각적 인식 태스크를 나타내며, 다수의 카테고리 및 밀집된 객체 시나리오와 함께 중요한 과제를 제시한다. 우리는 첫 번째 LVLM으로 이 두 가지 작업에 대한 실험을 수행하고 복잡하고 밀집된 시나리오에서 세밀한 인식 능력을 보여준다.\n' +
      '\n' +
      '**객체 검출.** 객체 검출 태스크는 텍스트 참조 및 그리폰-13B의 프롬프트를 사용하여 MSCOCO val2017 [24] 상에서 평가된다. 각 이미지에 대해 모든 테스트 범주를 동시에 입력하고 [55]에 이어 각 예측에 대한 신뢰 점수를 계산한다. 탭에 표시된 대로입니다. 도 5를 참조하면, Faster RCNN(C4 and FPN)과 DAB-DETR을 포함한 기존의 전문가 모델보다 적은 학습 시간과 낮은 입력 해상도로 우수한 성능을 보인다. 또한 일반주의자 그리폰-13B와 비교하여 모든 메트릭에서 상당한 개선을 달성했다.\n' +
      '\n' +
      '**객체 카운팅.** 객체 카운팅은 시각적 참조로 수행되고 정확한 카운팅 및 일반화 비교를 용이하게 하는 것을 목표로 하는 FSCD-LVIS [32]의 보이지 않는 테스트 클래스 세트에 대해 테스트된다. 시각적 참조는 세트로부터 하나의 예제 박스를 랜덤하게 선택하고 이미지 내의 영역을 스크린샷함으로써 구성된다. 탭에 표시된 대로입니다. 6, MAE와 NAE가 낮은 기존 전문가 모델을 능가합니다. 특히, 우리의 접근 방식은 숫자를 출력할 뿐만 아니라\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Type & Model & ANY & MERGED \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & DDPN [54] & - & 73.5 \\\\  & VisualBert [21] & 71.3 & - \\\\  & MDETR [17] & 83.4 & 83.8 \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & UniTAB [50] & - & 79.6 \\\\  & Ferret-13B [51] & - & 84.8 \\\\ \\cline{1-1}  & Shikra-13B [8] & - & 78.4 \\\\ \\cline{1-1}  & Griffon-13B [55] & 84.2 & 82.8 \\\\ \\cline{1-1}  & **Griffon v2** & **84.8** & **83.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: Flickrs30K Entities[34] 테스트 세트에 대한 구 접지 결과. 스펙. 전문가를 나타내는 반면 젠을 나타냅니다. 은 일반주의자들을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Type & Model & CIDEr & Meteor \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & SLR [53] & 66.2 & 15.9 \\\\  & ASM [45] & 41.9 & 13.6 \\\\  & Grit [46] & 71.6 & 15.2 \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & KOSMOS-2 [33] & 60.3 & 12.2 \\\\ \\cline{1-1}  & **Griffon v2** & 72.5 & 12.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: RefCOCOg에 대한 참조 표현 생성 결과[31]. 스펙. 전문가를 나타내는 반면 젠을 나타냅니다. 는 Tab. 4에서와 동일한 일반론자를 나타낸다.\n' +
      '\n' +
      '또한 탐지된 객체의 경계 상자를 제공합니다. 이는 LVLM이 실제 전문가 수준의 탐지 및 계수를 달성하는 첫 번째 단계로 그리폰 V2의 우수성과 시각적 참조 접근법의 일반화 능력을 보여준다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '아래 절제 연구에서 기본적으로 MSCOCO val2017 [24]의 객체 탐지 작업에서 그리폰 v2를 주로 평가하고 특징 정렬 후 설정된 트레인 2017만으로 전체 모델을 훈련한다.\n' +
      '\n' +
      '####4.4.1 미리 훈련된 다른 비주얼 인코더.\n' +
      '\n' +
      '기존의 방법들은 사전 학습된 CLIP 모델로부터 시각적 인코더를 이용하며, 사전 학습된 모델들의 성능은 최적화 방법에 따라 달라진다. 우리는 EVA2-CLIP-L/14[40], 원본 CLIP-L/14[36] 및 SAM-CLIP-16[18]을 비교한다. 336에서 1022의 해상도를 얻기 위해 쌍선형 보간을 수행하는데, 이는 사전 훈련된 SAM-CLIP-16의 1024 해상도에 가깝고 탭의 결과와 같다. 도 7을 참조하면, EVA2-CLIP 기반 시각 인코더는 평균 2%의 개선으로 다른 두 모델보다 성능이 우수하다. 위치 위치 보간을 이용한 CLIP 기반 모델과 보간이 없는 SAM 기반 모델을 비교한 결과,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Type & Model & MAE(\\(\\downarrow\\)) & NAE(\\(\\downarrow\\)) \\\\ \\hline \\multirow{3}{*}{Specialists} & FamNet [37] & 68.5 & 2.3 \\\\  & FSDetView [47] & 29.0 & 0.8 \\\\ \\cline{1-1}  & Counting-DETR [32] & 23.5 & 0.6 \\\\ \\cline{1-1} \\cline{2-4} Generalist & **Griffon v2** & **20.3** & **0.5** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 밀집 시나리오에서의 객체 카운팅에서의 성능 비교. 개체 계수는 FSCD-LVIS 보이지 않는 테스트 클래스 세트[32]에서 평가된다. MAE는 평균 평균 오차를 나타내는 반면 NAE는 정규화된 상대 오차를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Type & Model & Res. Epochs & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AP_{S}\\) & \\(AP_{M}\\) & \\(AP_{L}\\) \\\\ \\hline \\multirow{6}{*}{Specialists} & FRCNN-FPN [38] & 1022 & 12 & 37.9 & 58.6 & 40.9 & 20.4 & 41.1 & 50.3 \\\\  & FRCNN-C4 [38] & 1022 & 12 & 35.6 & 55.7 & 37.8 & 17.0 & 40.6 & 50.3 \\\\ \\cline{1-1}  & DAB-DETR [28] & 1333 & 12 & 38.0 & 60.3 & 39.8 & 19.2 & 40.9 & 55.4 \\\\ \\cline{1-1}  & Pix2Seq [9] & 1333 & 300 & 43.0 & 61.0 & 45.6 & 25.1 & 46.9 & 59.4 \\\\ \\cline{1-1}  & DETR [56] & 1333 & 500 & 42.0 & 62.4 & 44.2 & 20.5 & 45.8 & 61.1 \\\\ \\hline \\multirow{2}{*}{Generalist} & Griffon-13B [55] & 448 & 1 & 24.8 & 40.6 & 25.1 & 5.9 & 25.5 & 48.7 \\\\ \\cline{1-1}  & **Griffon v2** & **1022** & 1 & 38.5 & 54.3 & 41.2 & 19.4 & 43.2 & **57.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MSCOCO val2017에 대한 객체 검출에서의 성능 비교[24].\n' +
      '\n' +
      '해상도를 높이기 위해 위치 보간을 사용하는 우리의 구조 설계도 훨씬 더 나은 성능과 함께 작동할 수 있다.\n' +
      '\n' +
      '#### 5.3.3 다운 샘플링 구조.\n' +
      '\n' +
      '기존 방법은 Flamingo[2]에서 제안한 리샘플러를 적용하여 학습 가능한 토큰으로 시각적 특징을 다운 샘플링하면서 해상도를 높인다. 본 논문에서 설계한 다운샘플링 프로젝터를 훈련 중 성능 및 메모리 측면에서 비교 분석하였다. 탭에 표시된 대로입니다. 도 8에 도시된 바와 같이, 본 모델은 메모리 소비가 상당히 적으면서 더 높은 정밀도를 달성하며, 이는 LVLM의 대규모 사전 훈련에 상당히 중요하다. 리샘플러는 과제를 이해하기 위한 의미 정보를 추출할 수 있지만 동일한 훈련 환경에서 인식 및 현지화 작업에 대한 세밀한 세부 정보를 캡처하는 데 부족하므로 더 많은 시간이 필요하다[6].\n' +
      '\n' +
      '#### 5.3.4 훈련 가능 또는 냉동 비주얼 인코더.\n' +
      '\n' +
      '대부분의 작업은 효능을 위해 미리 훈련된 시각적 인코더를 동결하도록 선택한다. 그러나, 냉동 비주얼 인코더가 제공하는 전역적 특징들이 객체 레벨 태스크에 최적이 아닌 것에 대한 우려가 있다. 이 두 가지 훈련 전략을 평가하기 위해 CLIP-L/14 및 700 해상도로 시각적 인코더를 동결할지 여부에 대한 절제 연구를 수행한다. 탭에 표시된 대로입니다. 도 9를 참조하면, 비주얼 인코더를 언프레이징할 때 \\(mAP\\)은 6.8%, \\(AR_{100}\\)은 8.9% 향상되었다. 따라서, 시각적 인코더를 언프레이징하는 것은 더 미묘한 특징들을 획득하는 것을 도울 수 있고, 평균 재현율을 향상시키고 전체 정밀도를 상당히 증가시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c c c c c} \\hline \\hline Pretrained & Res. & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AP_{S}\\) & \\(AP_{M}\\) & \\(AP_{L}\\) & \\(AR_{100}\\) \\\\ \\hline CLIP [36] & 1022 & 29.8 & 48.6 & 29.7 & 10.0 & 32.7 & **53.9** & 41.4 \\\\ SAM [18] & 1024 & 28.4 & 43.1 & 29.6 & 11.2 & 30.8 & 46.9 & 40.9 \\\\ EVA2-CLIP [40] & 1022 & **31.9** & **50.4** & **32.7** & **14.3** & **38.6** & 52.3 & **43.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 상이한 사전 훈련된 시각적 인코더에 대한 절제 연구.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Type & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & Mem. \\\\ \\hline Resampler & 9.6 & 18.4 & 8.8 & 416.0G \\\\ Down-sample projector (ours) & **28.4** & **43.1** & **29.6** & 461.3M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 다운 샘플링 기능을 가진 다른 프로젝터에 대한 절제 연구. 멤버 파라미터들 및 순방향/역방향 패스를 포함하는 이 블록의 메모리 소비를 기부한다.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      '또한 시각화 결과를 제시하여 5가지 태스크에 걸쳐 그리폰 v2의 성능을 평가한다. 도 1에 도시된 바와 같다. 도 5에 도시된 바와 같이, 그리폰 v2는 시각적 언어 공동 참조를 통해 관심 대상을 정확하게 찾고 정확한 설명을 생성하는 능력을 일관되게 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 연구에서는 최대 1K 해상도를 지원하고 시각적 언어 공동 참조를 용이하게 하는 혁신적인 고해상도 멀티모달 모델인 그리폰 v2를 제시한다. 설계된 고해상도 구조는 시각적 특징과 프로젝트를 직접 추출합니다.\n' +
      '\n' +
      '그림 5: 5가지 비전 및 비전 언어 작업에 걸친 그리폰 v2의 시각화 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Resolution & Unfreeze & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AR_{100}\\) \\\\ \\hline \\multirow{3}{*}{700} & \\(\\times\\) & 17.5 & 32.7 & 16.7 & 29.5 \\\\  & \\(\\checkmark\\)(ours) & 24.3 & 43.4 & 23.4 & 38.4 \\\\ \\cline{1-1}  & \\(\\Delta\\) & **6.8\\(\\uparrow\\)** & **10.7\\(\\uparrow\\)** & **6.7\\(\\uparrow\\)** & **8.9\\(\\uparrow\\)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 시각적 인코더 훈련 전략에 대한 절제 연구.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '12] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft coco captionions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)\n' +
      '* [13] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)\n' +
      '* [14] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [15] Jiang, Q., Li, F., Ren, T., Liu, S., Zeng, Z., Yu, K., Zhang, L.: T-rex: Counting by visual prompting. arXiv preprint arXiv:2311.13596 (2023)\n' +
      '* [16] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901-2910 (2017)\n' +
      '* [17] Kamath, A., Singh, M., LeCun, Y., Misra, I., Synnaeve, G., Carion, N.: Mdetr-modulated detection for end-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763 (2021)\n' +
      '* [18] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)\n' +
      '* [19] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision **123**, 32-73 (2017)\n' +
      '* [20] Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z.: Otterhd: A high-resolution multi-modality model. arXiv preprint arXiv:2311.04219 (2023)\n' +
      '* [21] Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)\n' +
      '* [22] Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607 (2023)\n' +
      '* [23] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L., Mao, S., et al.: Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint arXiv:2303.16434 (2023)\n' +
      '* [24] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)\n' +
      '* [25] Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)\n' +
      '* [26] Liu, H., et al: Visual instruction tuning. In: NeurIPS (2023)\n' +
      '* [27] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* [28] Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L.: Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329 (2022)\n' +
      '*[*[29] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)\n' +
      '* [30] Liu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu, M., Chen, M., Li, C., Jin, L., et al.: On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895 (2023)\n' +
      '* [31] Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for referring expression understanding. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14. pp. 792-807. Springer (2016)\n' +
      '* [32] Nguyen, T., Pham, C., Nguyen, K., Hoai, M.: Few-shot object counting and detection. In: European Conference on Computer Vision. pp. 348-365. Springer (2022)\n' +
      '* [33] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)\n' +
      '* [34] Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: Proceedings of the IEEE international conference on computer vision. pp. 2641-2649 (2015)\n' +
      '* [35] Qi, J., Ding, M., Wang, W., Bai, Y., Lv, Q., Hong, W., Xu, B., Hou, L., Li, J., Dong, Y., et al.: Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236 (2024)\n' +
      '* [36] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [37] Ranjan, V., Sharma, U., Nguyen, T., Hoai, M.: Learning to count everything. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n' +
      '* [38] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems **28** (2015)\n' +
      '* [39] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 (2023)\n' +
      '* [40] Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023)\n' +
      '* [41] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n' +
      '* [42] Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin, D.: V3det: Vast vocabulary visual detection dataset. In: The IEEE International Conference on Computer Vision (ICCV) (October 2023)\n' +
      '* [43] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In: International Conference on Machine Learning. pp. 23318-23340. PMLR (2022)\n' +
      '* [44] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)* [45] Wang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing, L., Chen, Z., Li, H., Zhu, X., Cao, Z., et al.: The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907 (2023)\n' +
      '* [46] Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 (2022)\n' +
      '* [47] Xiao, Y., Lepetit, V., Marlet, R.: Few-shot object detection and viewpoint estimation for objects in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence **45**(3), 3090-3106 (2022)\n' +
      '* [48] Yan, B., Jiang, Y., Wu, J., Wang, D., Luo, P., Yuan, Z., Lu, H.: Universal instance perception as object discovery and retrieval. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15325-15336 (2023)\n' +
      '* [49] Yang, H., Yue, S., He, Y.: Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224 (2023)\n' +
      '* [50] Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Unitab: Unifying text and box outputs for grounded vision-language modeling. In: European Conference on Computer Vision. pp. 521-539. Springer (2022)\n' +
      '* [51] You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704 (2023)\n' +
      '* [52] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69-85. Springer (2016)\n' +
      '* [53] Yu, L., Tan, H., Bansal, M., Berg, T.L.: A joint speaker-listener-reinforcer model for referring expressions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7282-7290 (2017)\n' +
      '* [54] Yu, Z., Yu, J., Xiang, C., Zhao, Z., Tian, Q., Tao, D.: Rethinking diversified and discriminative proposal generation for visual grounding. arXiv preprint arXiv:1805.03508 (2018)\n' +
      '* [55] Zhan, Y., Zhu, Y., Chen, Z., Yang, F., Tang, M., Wang, J.: Griffon: Spelling out all object locations at any granularity with large language models. arXiv preprint arXiv:2311.14552 (2023)\n' +
      '* [56] Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.Y.: Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605 (2022)\n' +
      '* [57] Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601 (2023)\n' +
      '* [58] Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., Kang, B.: Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023)\n' +
      '* [59] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>