<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring\n' +
      '\n' +
      'Yufei Zhan\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 12School of Artificial Intelligence,\n' +
      '\n' +
      'University of Chinese Academy of Sciences, Beijing, China 2\n' +
      '\n' +
      'Yousong Zhu\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 1\n' +
      '\n' +
      'Hongyin Zhao\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 1\n' +
      '\n' +
      'Fan Yang\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 12School of Artificial Intelligence,\n' +
      '\n' +
      'University of Chinese Academy of Sciences, Beijing, China 23Peng Cheng Laboratory, Shenzhen, China 34Wuhan AI Research, Wuhan, China\n' +
      '\n' +
      'Ming Tang\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 12School of Artificial Intelligence,\n' +
      '\n' +
      'University of Chinese Academy of Sciences, Beijing, China 2\n' +
      '\n' +
      'Jinqiao Wang\n' +
      '\n' +
      '1Foundation Model Research Center, Institute of Automation,\n' +
      '\n' +
      'Chinese Academy of Sciences, Beijing, China 12School of Artificial Intelligence,\n' +
      '\n' +
      'University of Chinese Academy of Sciences, Beijing, China 23Peng Cheng Laboratory, Shenzhen, China 34Wuhan AI Research, Wuhan, China\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model\'s potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and _etc_. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-referring capabilities through a plug-and-play visual tokenizer. It enables user-friendly interaction with flexible target images, free-form texts and even coordinates. Experiments demonstrate that Griffon v2 can localize any objects of interest with visual and textual referring, achieve state-of-the-art performance on REC, phrase grounding, and REG tasks, and outperform expert models in object detection and object counting. Data, codes and models will be released at [https://github.com/jefferyZhan/Griffon](https://github.com/jefferyZhan/Griffon).\n' +
      '\n' +
      'Keywords:Large Vision Language Models Multimodal perception High-resolution structure Visual-language co-referring\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Vision Language Models (LVLMs) [26, 2, 13, 59] show promising performance in region-level tasks like Referring Expression Comprehension (REC) after the breakthrough in image-text understanding [3, 12] and reasoning [16]. Inparticular, models like Griffon [55] have demonstrated more compelling perception capability in object detection task. This has further spurred the development of flexible references of objects beyond only textual descriptions for better user interaction.\n' +
      '\n' +
      'Despite these progresses, current LVLMs still meet the bottleneck in surpassing the task-specific experts in fine-grained tasks, tripped by the image resolution. In other words, they can hardly capture the nuanced visual details, leading to a plethora of fact-conflicting hallucinations. It is particularly evident when dealing with low-resolution scenarios such as answering region-based questions without basis [35], failing in small words in characters-related tasks [30], or providing incorrect counting results [20].\n' +
      '\n' +
      'To address this issue, recent works have explored resolution enhancement and flexible visual-language referring in LVLMs. On the one hand, previous methods [10, 11] adopt a progressive training approach to gradually improve the resolution. However, the maximum input length of Large Language Models (LLMs) imposes a constraint on achieving higher image resolutions. Additionally, some approaches [22, 25, 27] divide the image to smaller patches and encode them separately for zooming in. This division-based design proves sub-optimal multimodal perception ability, which losses the contexts and edge details of patches and increases computation complexity [22]. On the other hand, prior researches have predominantly studied various forms of references [5, 8, 57] to improve the understanding of specific image content based on low-resolution images (_e.g._ 224\\(\\times\\)224 or 448\\(\\times\\)448). However, these methods often excel at perceiving prominent, and\n' +
      '\n' +
      'Figure 1: Overview of Griffon v2. Griffon v2 enables high-resolution input and visual-language co-referring for multimodal models. In terms of referring, a user can refer to an object through coordinates, textual descriptions, screenshotting and cross-image modes. Griffon v2 excels in localizing arbitrary objects and generate descriptions with co-referring across a wide range of scenarios.\n' +
      '\n' +
      'image-level objects, but fall short in accurately localizing and describing fine-grained local objects. Additionally, singular visual or language prompts alone either lack conversational abilities or are constrained by linguistic descriptions [15], failing to provide a comprehensive user-friendly interactive experience.\n' +
      '\n' +
      'In this paper, we propose Griffon v2 to open the evil of high-resolution and endow it with locating any objects of interest with visual-language co-referring. Instead of partitioning the high-resolution image into smaller patches, we employ a high-resolution visual encoder to direct extract representations, and design a simple and lightweight down-sampling projector with strided convolution to compress the length of visual tokens. The compressed visual tokens are then trained to align with text tokens and fed into LLM for further fusion like LLaVA [26]. Compared to complex resampler structure [25] and multiple sub-images mode, the proposed pipeline is parameter-efficient and computationally concise. More importantly, we build a visual-language co-referring paradigm to enhance the model\'s fine-grained perception of high-resolution inputs, greatly expanding the model\'s applicability. It supports local cropped images, texts, and coordinates prompting, and outputs coordinates of target objects or corresponding text descriptions, providing various interactive abilities, thereby mitigating the conversational deficiencies of singular visual prompting and the potential expressive ambiguity of textual prompting. Finally, we collected 12M publicly available localization data for pre-training and 900K instruction data for fine-tuning. We achieve state-of-the-art results in REC task, phrase grounding task, Referring Expression Generation (REG) task. Notably, our model outperforms object detection and object counting expert models for the first time.\n' +
      '\n' +
      'In summary, our main contributions are:\n' +
      '\n' +
      '1. We propose a high-resolution multimodal perception model for better local understanding, which inherently supports resolution up to 1K without image division.\n' +
      '2. We introduce a visual-language co-referring structure, which broadens the model\'s scope of application and provide various interaction modes.\n' +
      '3. We have conducted experiments on a wide range of localization-related tasks and demonstrated state-of-the-art performance on REC, phrase grounding and REG. We quantitatively surpass expert models in object detection task object counting qualitatively.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Large Vision Language Models with Localization\n' +
      '\n' +
      'With the significant advancements made in LVLMs [1, 2, 13, 26], object localization as an essential foundation visual task has been explored with LVLMs. One branch of these methods [23, 39, 49, 58] focuses on invoking the detection expert models through an API. When the LVLM requires detection, the LLM issues instructions to call the detection model and receives the results for further processing. This approach increases the complexity of the model and can not enjoy the unified structure with LVLMs. Another set of methods [4, 8, 11, 51] focuses on fine-grained single-object localization, _i.e._ REC. These methods transform REC task data into instruction data and encode the coordinates with different representation approaches, enabling the LVLM itself to possess localization capabilities. Until the appearance of Griffon [55], the object localization capacity of LVLM is extended to multi-object and arbitrary granularity. Our model further proposes a high-resolution structure that elevates the detection capability of LVLM beyond expert models, which is also proved more effective and efficient than existing division-based resolution-enhanced methods [22, 25].\n' +
      '\n' +
      '### Object Referring in Multimodal Models\n' +
      '\n' +
      'Since the application of LLMs [1, 41] in large multimodal models, textual descriptions have become the most straightforward method for object referring. Users naturally utilize this method to ask questions without even paying special attention, such as "What is in front of this cat?" Despite its simplicity, it struggles to distinctly refer to a specific object among numerous similar items, leading to ambiguous referring that may yield inaccurate results. With the growing importance of region comprehension, it has sparked the exploration of spatial references in complex and dense scenes. Current works have proposed spatial referrals with textual coordinates [4, 7, 8], learnable embeddings [33], and Region of Interest features [57]. Some approaches [1, 5, 51] upgrade from the user-friendly perspective, supporting arrows, marks, and so on. Due to the limitations of their focus on tasks and granularity, these approaches primarily concentrate on referring within a single modality, also without further consideration for high resolution. In contrast, our model simultaneously considers perception, localization, and comprehension at different granularity. Therefore, we employ a visual-language co-referring approach to enable accurate references in various tasks while remaining user-friendly.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      'In this section, we start with a brief overview of our Griffon v2 as shown in Fig. 2. As the key foundation, we describe the high-resolution structure design in Sec. 3.2. Then, we represent the visual-language co-referring in Sec. 3.3 and the training pipeline in Sec. 3.4.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      'As depicted in Fig. 2, Griffon-V2 employs an auto-regression paradigm, seamlessly integrating referring and grounding tasks within a unified language modeling objective. For a given input image \\(X_{v}\\), we leverage a visual encoder, adapted to high-resolution by bilinear interpolation from the pretrained EVA2-CLIP-L/14@336 [40] model to extract high-resolution visual features (_e.g._ over 1K). Simultaneously, the designed down-sampling projector transforms these image features into visual embedding tokens \\(H_{v}\\). In cases where a user customizes a visual prompt \\(X_{q}\\) with a screenshot or an additional target image, region embeddings \\(H_{q}\\) are derived from the visual referring image using the visual tokenizer. Meanwhile, we inherently support language references, \\(X_{ins}\\), to identify objects, such as textual descriptions and coordinates, which are projected into text embeddings as \\(H_{ins}\\) after tokenization. Subsequently, the visual prompt or text embeddings together with image embeddings undergo processing using the LLM, specifically LLaMA2-13B [41], resulting in the generation of the desired answer \\(X_{a}\\). Notably, the representation of bounding boxes eschews the use of special tokens and specialized heads and instead incorporate textual numerical values, thereby maintaining simplicity.\n' +
      '\n' +
      '### High-Resolution Scaling with Down-Sample Projector\n' +
      '\n' +
      'High resolution has been empirically proven to enhance the accurate interpretation of nuanced visual features, thereby benefiting both visual tasks and vision-language tasks. In previous methods [22], images are commonly partitioned into smaller patches, enlarging their total resolution to 762 or 896. However, this approach faces limitations in terms of contextual coherence and the representation of edge details among patches. While progressive scaling methods can extend resolution to 784 (patch size = 14) within the constraint of 4096 input tokens, they are unsuitable for handling outputs involving extensive long texts and significantly increase the computation complexity.\n' +
      '\n' +
      'Figure 2: Structure of the proposed high-resolution Griffon v2 with visual-language co-referring.\n' +
      '\n' +
      'To address these challenges, we introduces a high-resolution structure, incorporating a down-sampling projector capable of supporting a resolution more than 1K. Upon receiving an input image of arbitrary dimensions, we resize it to a predefined input height and width, denoted as \\(X_{v}\\in\\mathbb{R}^{H\\times W\\times 3}\\). Instead of employing image division, we encode the image using trainable high-resolution visual encoder adapted from pretrained model by bilinear interpolation, yielding visual features \\(Z_{v}=f_{V}(X_{v})\\). In contrast to low-resolution fixed encoders, our approach excels in capturing finer details. As illustrated in the left part of Fig. 2, we subsequently introduce a lightweight down-sampling projector to abstract essential features with compression under the guidance of the training objective and connect visual features to word embedding space. Specifically, a strided convolution layer (\\(Conv\\)) is simply applied to down-sample the features by \\(S\\), and a projection matrix \\(W\\) is utilized to convert \\(Z_{v}\\) into visual embedding tokens:\n' +
      '\n' +
      '\\[H_{v}=W\\cdot Conv(Z_{v}),\\ with\\ Z_{v}=f_{V}(X_{v}). \\tag{1}\\]\n' +
      '\n' +
      'It can be inferred that the final number of visual tokens is quadratically related to the input size and the convolution stride, _i.e_. \\(H,W\\) and \\(S\\). To demonstrate the effectiveness of our design, we employ the convolution layer with kernel size at 3 and stride at 2, and investigate several different resolutions in Fig. 3. As depicted in Fig. 3, comparing the results at resolutions of 700 and 448 which use two-layer linear projection [26] without reduction, using the resolution of 700 with our structure shows a higher mAP and fewer tokens. This indicates that our method is capable of extracting features while preserving details, thereby reducing token redundancy. It is also observed that further increasing the resolution can lead to improved performance.\n' +
      '\n' +
      '### Visual-Language Co-Referring\n' +
      '\n' +
      'Recognizing the limitations of singular visual or language referring demonstrated before, lack of conversational ability and potential ambiguity respectively, we\n' +
      '\n' +
      'Figure 3: Performance and visual token number comparison of different resolution. We fix the stride at 2 with the kernel size at 3 for the convolution layer and vary the resolution for analysis. The 448-resolution model employs a two-layer linear projector [26], whereas the others utilize our down-sampling projector. our method is capable of extracting features while preserving details, thereby reducing token redundancy.\n' +
      '\n' +
      'capitalize on CLIP\'s adeptness in extracting robust semantic features to facilitate visual-language co-referring. This approach mitigates these drawbacks and finds broader applicability under high-resolution conditions.\n' +
      '\n' +
      '**Visual Referring.** While various existing methods propose diverse visual reference forms, ranging from marking on the raw image to object masking, either of them alter the raw image content [1] or require specially designed extractor [51]. To strike a balance between efficiency and referring accuracy, we opt for a straightforward method, using visual referring image to represent the target of interest. Specifically, as depicted in the middle of Fig. 2, given a user-provided referring image \\(X_{q}\\), we employ a region encoder, _i.e_. EVA-CLIP-L/14@224 [40] pretrained ViT to extract visual prompt features \\(Z_{q}\\). To align the LLM\'s understanding of visual references with textual references, we project the region features into the word embedding space, yielding the visual prompt token \\(H_{q}\\). Based on the robust semantic representation capability of \\([CLS]\\) token, pre-trained on a dataset of billions of instances [36], we discard other tokens following previous applications and utilize the \\([CLS]\\) token to represent the specific object. This approach allows us to refer to the object at the category level, eliminating overly specific information from pixel patches and maintaining the stability of reference. To accommodate diverse conversation scenarios, we use "<region>" in instructions to represent the referred object as shown in the Fig. 4 (b). Before inputting into the LLM, we replace this placeholder with the embedding of the referred object, seamlessly integrating it for optimal coherence.\n' +
      '\n' +
      '**Language Referring.** Textual referring is an inherent capability of LLM-based multimodal models. Despite the simplicity of textual referring, it faces the challenge of resulting in wrong answer with ambiguous referring. This has naturally prompted us to consider whether associating texts with regions could allow LLMs to comprehend the corresponding object based on textual-region reference and thereby fulfill task instructions and keep the simplicity advantage. Therefore, we further support textual coordinate references as illustrated in the Fig. 4 (a). In additional to the detailed description, an object can also be referred by its top-left and bottom-right coordinates. In this case, it is processed the same as textual descriptions, involving tokenization and embedding into the word space as \\(H_{ins}\\), depicted at the right side of Fig. 2. Through training with instruction\n' +
      '\n' +
      'Figure 4: Examples of visual-language co-referring usage in conversations.\n' +
      '\n' +
      'following data whose object references are transformed into textual coordinates, we demonstrate that LLMs can accurately refer to objects in complex and dense scenes based on textual coordinates of objects. Through collaborative referring involving both text and vision, our approach achieves optimal referring performance and flexible interaction.\n' +
      '\n' +
      '### Training Pipeline\n' +
      '\n' +
      'To understand different users\' intents and finish diverse tasks with high-quality results, we adopt a three-stage end-to-end training procedure for Griffon v2 as follows guided by [26, 55]. The dataset used are listed in Tab. 1.\n' +
      '\n' +
      '**Stage I: High-resolution Vision-Language Alignment.** Feature alignment before pretraining has been widely utilized to achieve better training efficiency. We adopt this strategy to connect the high-resolution visual encoder with the LLM using 558K image-text pairs, converted to instruction-following data by [26]. The high-resolution image encoder and LLM parameters remain frozen, with only the down-sampling projector trainable. Without visual prompts in these image-text pairs, the visual prompt tokenizer doesn\'t participate in the training of stage I.\n' +
      '\n' +
      '**Stage II: Co-referring Multi-Tasks Pretraining.** Building on the basic understanding of visual content achieved in Stage I, we further pretrain the entire model with a diverse dataset comprising 12M multi-task instances involving both visual and textual referring. This stage aims to enhance fine-grained perception and localization capabilities, and visual-language referring abilities. As shown in Tab. 1, our textual-referring data is curated from various task datasets covering over 70K object categories, such as Visual Genome [19] and V3Det [42]. Additionally, we build a dataset with public and self-collected object counting data from 10 different domains, spanning aerial photography, agriculture, industry, and more, to create our visual-referring data. The diverse textual categories and visual domains contribute to the model\'s generalization capability. During\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Period & Data Vol. & Annotation Type & Sources \\\\ \\hline Stage I & 558K & Image-text & LLaVA \\\\ \\hline \\multirow{4}{*}{Stage II} & \\multirow{4}{*}{12M} & Object Detection & Objects 365, MSCOCO \\\\  & & REC/REG & Visual Genemo, RefCOCO series \\\\ \\cline{1-1}  & & Visual Grounding & V3Det, LVIS, Flickrs30K Entities \\\\ \\cline{1-1}  & & Object Counting & CA-44, OpenImages v4, Self-collected data \\\\ \\cline{1-1}  & & Non-existing Judging & LVIS \\\\ \\hline Stage III & 900K & Instruction-following & Build from stage 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Data statistics of different stages. We convert the data of different annotations to Prompt/Instruction-Answer form. We will release the self-collected and processed data in Stage II and Stage III. More details can be found in the supplements.\n' +
      '\n' +
      'this stage, we train the entire network while keeping the region encoder in the visual prompts tokenizer frozen.\n' +
      '\n' +
      '**Stage III: Intent-Enhanced Instruction Tuning.** Following the pretraining of stage II, the model gains the abilities to locate and describe objects of interest with free-form texts and flexibly-obtained visual referring images. It can be customized and enhanced by users to achieve specific tasks as a foundation model. To further enhance its understanding ability of user\'s intents, we finetune the model with nearly 900K instruction-following data build from stage II and more diverse instruction prompts for different tasks detailed in supplements. Moreover, to preserve the refined visual features extraction and prevent forgetting, we keep the high-resolution visual encoder and region encoder frozen and train the LLM together with the projectors.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we first introduce the key implementation details in Sec. 4.1. Subsequently, we compare our Griffon v2 with leading LVLMs in REC, REG, object detection, phrase grounding, and object counting. In addition, we analyse the model design of Griffon v2 with ablation studies in Sec. 4.4. The qualitative analysis results are present in Sec. 4.5.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'Following the widely used configuration of ResNet [14], we set the convolution layer of the down-sampling projector with a kernel size of 3, a stride of 2, and padding of 1. For the chosen of image resolution, we consider from both the patch size of pretrained CLIP and the token length of our data under the constraint of LLaMA2, supporting maximum 4096 input tokens. We follow the previous methods to utilize L-size CLIP model, whose patch size is usually 14 or 16. As the average textual token length of counting data is approximate to 2500, the maximum achievable resolution is 1022 (patch size = 14) or comparable 1024 (patch size = 16), corresponding to 1369 tokens or 1024 tokens respectively. As shown in Tab. 7, as the EVA-CLIP-L/14 has the best performance, we set the resolution to 1022. We initialize the visual encoder with adapted EVA2-CLIP-ViT-L/14@336 by position embedding interpolation and LLM with LLaMA2, leaving the down-sampling projector and projector of visual tokenizer random initialized. More detailes are demonstrated in the supplements.\n' +
      '\n' +
      '### Evaluation on Basic Grounding and Referring\n' +
      '\n' +
      'Basic grounding and referring mainly includes the REC task and REG task, typically involving one pre-existing object, and phrase grounding task with a limit number of multiple targets. Griffon v2 is systematically compared with specialist and generalist models across these three tasks.\n' +
      '\n' +
      '**REC.** The REC task has been extensively researched in the LVLMs as a basic localization task grounding single object with textual description. We conduct experiments on the RefCOCOg [52], RefCOCO+ [52] and RefCOCOg [31]. The experimental results in Tab. 2 demonstrate that our model outperforms recent state-of-the-art models across multiple sets. We\'d like to elaborate on this, that our model is capable of perceiving multiple objects, including small ones, and locate all the objects corresponding to the referent, whereas other models only generate a single output for this task. However, incomplete or ambiguous annotations are more observed in sets like RefCOCO+ adding non-unique attributes to objects and affects the evaluation. We detail in the supplements. In comparison to the current leading model, CogVLM, we train our model following its paradigm, and the results indicate superior performance across almost all sets, showcasing our advancements.\n' +
      '\n' +
      '**REG.** REG aims to generate concise descriptions for specified objects based on their region locations. We input textual coordinates for object referring and tests on the RefCOCOg [31] val set. As illustrated in Tab. 3, in contrast to KOSMOS-2, which use learnable embeddings for referring, we achieve superior performance in CIDEr, concentrating on the semantic similarity, while the Meteor focuses more on the accuracy of wording, making it less suitable for the open-ended description generation of LLMs.\n' +
      '\n' +
      '**Phrase Grounding.** Phrase grounding task presents a greater challenge compared to the REC task and is evaluated on Flickrs30K Entities [34]. Two evaluation protocols [17] are employed, including the ANY-BOX protocol and\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Type} & \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{RefCOCO} & \\multicolumn{3}{c}{RefCOCO+} & \\multicolumn{3}{c}{RefCOCOg} \\\\ \\cline{3-10}  & & val & testA & testB & val & testA & testB & val & test \\\\ \\hline \\multirow{5}{*}{\\begin{tabular}{c} \\end{tabular} } & MDETR [17] & 87.5 & 90.4 & 82.7 & 81.1 & 85.5 & 73.0 & 83.3 & 83.3 \\\\  & G-DINO-L [29] & 90.6 & 93.2 & 88.2 & 82.8 & 89.0 & 75.9 & 86.1 & 87.0 \\\\  & UNINEXT-L [48] & 91.4 & 93.7 & 88.9 & 83.1 & 87.9 & 76.2 & 86.9 & 87.5 \\\\  & Griffon-13B\\(\\dagger\\)[55] & 90.1 & 93.4 & 86.1 & 84.8 & 90.5 & 77.8 & 86.1 & 87.2 \\\\  & CogVLM [44] & 92.5 & 93.9 & 88.7 & 87.5 & 91.8 & 81.5 & 89.5 & 90.1 \\\\  & Griffon v2\\(\\dagger\\) & 92.3 & 94.0 & 89.5 & 88.7 & 92.3 & 82.8 & 90.2 & 90.2 \\\\ \\hline \\multirow{5}{*}{\n' +
      '\\begin{tabular}{c} \\end{tabular} } & OFA-L [43] & 80.0 & 83.7 & 76.4 & 68.3 & 76.0 & 61.8 & 67.8 & 67.5 \\\\  & KOSMOS-2 [33] & 52.3 & 57.4 & 47.3 & 45.5 & 50.7 & 42.2 & 60.6 & 61.7 \\\\  & Shikra-13B [8] & 87.8 & 90.6 & 80.2 & 82.9 & 87.8 & 74.4 & 82.6 & 83.2 \\\\  & Qwen-VL [4] & 89.4 & 92.3 & 85.3 & 83.1 & 88.3 & 77.2 & 85.6 & 85.5 \\\\  & Ferret-13B [51] & 89.5 & 92.4 & 84.4 & 82.8 & 88.1 & 75.2 & 85.8 & 86.3 \\\\  & Griffon-13B [55] & 88.0 & 92.1 & 81.9 & 81.5 & 88.2 & 73.3 & 82.9 & 84.3 \\\\ \\cline{1-1}  & \\multicolumn{2}{c}{**Griffon v2**} & **89.6** & **91.8** & **86.5** & **81.9** & **85.5** & **76.2** & **85.9** & **86.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Referring Expression Comprehension results. REC utilizes the metric of \\(ACC@0.5\\). \\(\\dagger\\) indicates the result from following the paradigm of CogVLM-17B with grounding-only data for comparison.\n' +
      '\n' +
      'MERGE-BOXES protocol. The ANY-BOX protocol focuses on the atomicity of each instance, while the MERGE-BOXES protocol evaluates whether the model identifies all referred objects with a merged box. Existing LVLMs are typically limited to the single referent scenario, tending to predict only one box per phase, thereby employing the MERGED-BOXES protocol. As shown in Tab. 4, Griffon v2 achieves state-of-the-art results in the ANY-BOX protocol and surpasses most specialists and generalists in the MERGE-BOX protocol, with more fine-grained boxes.\n' +
      '\n' +
      '### Evaluation on Complex Detection and Counting\n' +
      '\n' +
      'Object detection and counting represent essential visual perception tasks, presenting significant challenges with multiple categories and dense object scenarios. We conduct experiments on these two tasks as the first LVLM and demonstrate our fine-grained perception ability in complex and dense scenarios.\n' +
      '\n' +
      '**Object Detection.** The object detection task is evaluated on MSCOCO val2017 [24] using textual references and Griffon-13B\'s prompt. We input all test categories simultaneously for each image, and calculate confidence score for each prediction following [55]. As illustrated in Tab. 5, we outperform existing expert models, including Faster RCNN (C4 and FPN) and DAB-DETR, with fewer training epochs and lower input resolution. Moreover, we achieve substantial improvements across all metrics compared to the generalist Griffon-13B.\n' +
      '\n' +
      '**Object Counting.** Object counting is conducted with visual references and tested on the unseen test classes set of FSCD-LVIS [32] aiming for accurate counting and facilitating generalization comparisons. The visual reference is constructed by randomly selecting one example box from the set and screenshotting the region in the image. As depicted in Tab. 6, we surpass existing expert models with lower MAE and NAE. Notably, our approach not only outputs the number\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Type & Model & ANY & MERGED \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & DDPN [54] & - & 73.5 \\\\  & VisualBert [21] & 71.3 & - \\\\  & MDETR [17] & 83.4 & 83.8 \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & UniTAB [50] & - & 79.6 \\\\  & Ferret-13B [51] & - & 84.8 \\\\ \\cline{1-1}  & Shikra-13B [8] & - & 78.4 \\\\ \\cline{1-1}  & Griffon-13B [55] & 84.2 & 82.8 \\\\ \\cline{1-1}  & **Griffon v2** & **84.8** & **83.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Phrase grounding results on Flickrs30K Entities [34] test set. Spec. represents specialists, while Gen. represents generalists.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Type & Model & CIDEr & Meteor \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & SLR [53] & 66.2 & 15.9 \\\\  & ASM [45] & 41.9 & 13.6 \\\\  & Grit [46] & 71.6 & 15.2 \\\\ \\hline \\multirow{4}{*}{FSCD-LVIS} & KOSMOS-2 [33] & 60.3 & 12.2 \\\\ \\cline{1-1}  & **Griffon v2** & 72.5 & 12.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Referring Expression Generation results on RefCOCOg [31]. Spec. represents the specialist, while Gen. represents the generalists, the same as in Tab. 4.\n' +
      '\n' +
      'but also provides the bounding boxes of detected objects. This marks the first time LVLMs achieve real expert-level detection and counting, showcasing the superiority of Griffon V2 and the generalization ability of our visual reference approach.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'In the ablation studies below, in default we mainly evaluate Griffon v2 in object detection task on MSCOCO val2017 [24] and train the whole model with only train2017 set after feature alignment.\n' +
      '\n' +
      '#### 4.4.1 Different pretrained visual encoders.\n' +
      '\n' +
      'Existing methods utilize the visual encoder from the pretrained CLIP model, and the performance of pretrained models varies with different optimization methods. We compare EVA2-CLIP-L/14 [40], original CLIP-L/14 [36] and SAM-CLIP-16 [18]. We perform bilinear interpolation to achieve a resolution of 1022 from 336, close to the 1024 resolution of the pretrained SAM-CLIP-16. As shown in the results in Tab. 7, the EVA2-CLIP-based visual encoder outperforms the other two models with an average improvement of 2%. Comparing the CLIP-based models using positional position interpolation and SAM-based model without interpolation, it indicates that with\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Type & Model & MAE(\\(\\downarrow\\)) & NAE(\\(\\downarrow\\)) \\\\ \\hline \\multirow{3}{*}{Specialists} & FamNet [37] & 68.5 & 2.3 \\\\  & FSDetView [47] & 29.0 & 0.8 \\\\ \\cline{1-1}  & Counting-DETR [32] & 23.5 & 0.6 \\\\ \\cline{1-1} \\cline{2-4} Generalist & **Griffon v2** & **20.3** & **0.5** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Comparison of performance in object counting in dense scenarios. Object counting is assessed on the FSCD-LVIS unseen test class set [32]. MAE denotes Mean Average Error, while NAE stands for Normalized Relative Error.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Type & Model & Res. Epochs & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AP_{S}\\) & \\(AP_{M}\\) & \\(AP_{L}\\) \\\\ \\hline \\multirow{6}{*}{Specialists} & FRCNN-FPN [38] & 1022 & 12 & 37.9 & 58.6 & 40.9 & 20.4 & 41.1 & 50.3 \\\\  & FRCNN-C4 [38] & 1022 & 12 & 35.6 & 55.7 & 37.8 & 17.0 & 40.6 & 50.3 \\\\ \\cline{1-1}  & DAB-DETR [28] & 1333 & 12 & 38.0 & 60.3 & 39.8 & 19.2 & 40.9 & 55.4 \\\\ \\cline{1-1}  & Pix2Seq [9] & 1333 & 300 & 43.0 & 61.0 & 45.6 & 25.1 & 46.9 & 59.4 \\\\ \\cline{1-1}  & DETR [56] & 1333 & 500 & 42.0 & 62.4 & 44.2 & 20.5 & 45.8 & 61.1 \\\\ \\hline \\multirow{2}{*}{Generalist} & Griffon-13B [55] & 448 & 1 & 24.8 & 40.6 & 25.1 & 5.9 & 25.5 & 48.7 \\\\ \\cline{1-1}  & **Griffon v2** & **1022** & 1 & 38.5 & 54.3 & 41.2 & 19.4 & 43.2 & **57.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Comparison of performance in object detection on MSCOCO val2017 [24].\n' +
      '\n' +
      'our structure design, using positional interpolation to increase resolution can also work with even better performance.\n' +
      '\n' +
      '#### 5.3.3 Down-sampling structures.\n' +
      '\n' +
      'Previous methods apply the resampler proposed in Flamingo [2] to down-sample the visual features with learnable tokens while increasing the resolution. We compare this approach with our designed down-sampling projector in terms of performance and memory of this module during training. As depicted in Tab. 8, our model achieves higher precision with quite less memory consumption, which is quite important for large-scale pretraining of LVLMs. While the resampler can extract semantic information for understanding task, it falls short in capturing fine-grained details for perception and localization tasks under the same training setting, necessitating more epochs [6].\n' +
      '\n' +
      '#### 5.3.4 Trainable or frozen visual encoder.\n' +
      '\n' +
      'Most works choose to freeze the pre-trained visual encoder for efficacy. However, researches have express concern about that the global features provided by the frozen visual encoder is sub-optimal for object-level tasks. To assess of these two training strategies, we conduct an ablation study on whether to freeze the visual encoder with CLIP-L/14 and 700-resolution. As illustrated in Tab. 9, when unfreezing the visual encoder, the \\(mAP\\) is improved by 6.8% and \\(AR_{100}\\) is improved by 8.9%. Therefore, unfreezing the visual encoder can help with obtaining more nuanced features, enhancing the average recall and significantly increasing the overall precision.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|c c c c c c c} \\hline \\hline Pretrained & Res. & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AP_{S}\\) & \\(AP_{M}\\) & \\(AP_{L}\\) & \\(AR_{100}\\) \\\\ \\hline CLIP [36] & 1022 & 29.8 & 48.6 & 29.7 & 10.0 & 32.7 & **53.9** & 41.4 \\\\ SAM [18] & 1024 & 28.4 & 43.1 & 29.6 & 11.2 & 30.8 & 46.9 & 40.9 \\\\ EVA2-CLIP [40] & 1022 & **31.9** & **50.4** & **32.7** & **14.3** & **38.6** & 52.3 & **43.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Ablation study on the different pretrained visual encoder.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Type & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & Mem. \\\\ \\hline Resampler & 9.6 & 18.4 & 8.8 & 416.0G \\\\ Down-sample projector (ours) & **28.4** & **43.1** & **29.6** & 461.3M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Ablation study on the different projectors with down-sampling function. Mem. donates the memory consumption of this block including the parameters and forward/backward pass.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      'We further evaluate Griffon v2\'s performance across five tasks by presenting visualization results. As depicted in Fig. 5, Griffon v2 consistently demonstrates its ability to precisely locate objects of interest and generate accurate descriptions through visual-language co-referring.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this study, we present Griffon v2, an innovative high-resolution multimodal model supporting resolutions up to 1K and facilitating visual-language co-referring. Our designed high-resolution structure directly extracts visual feature and projects\n' +
      '\n' +
      'Figure 5: Visualization results of Griffon v2 across five vision and vision-language tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Resolution & Unfreeze & \\(mAP\\) & \\(AP_{50}\\) & \\(AP_{75}\\) & \\(AR_{100}\\) \\\\ \\hline \\multirow{3}{*}{700} & \\(\\times\\) & 17.5 & 32.7 & 16.7 & 29.5 \\\\  & \\(\\checkmark\\)(ours) & 24.3 & 43.4 & 23.4 & 38.4 \\\\ \\cline{1-1}  & \\(\\Delta\\) & **6.8\\(\\uparrow\\)** & **10.7\\(\\uparrow\\)** & **6.7\\(\\uparrow\\)** & **8.9\\(\\uparrow\\)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Ablation study on the visual encoder training strategy.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '12] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)\n' +
      '* [13] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)\n' +
      '* [14] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [15] Jiang, Q., Li, F., Ren, T., Liu, S., Zeng, Z., Yu, K., Zhang, L.: T-rex: Counting by visual prompting. arXiv preprint arXiv:2311.13596 (2023)\n' +
      '* [16] Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2901-2910 (2017)\n' +
      '* [17] Kamath, A., Singh, M., LeCun, Y., Misra, I., Synnaeve, G., Carion, N.: Mdetr-modulated detection for end-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763 (2021)\n' +
      '* [18] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)\n' +
      '* [19] Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision **123**, 32-73 (2017)\n' +
      '* [20] Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z.: Otterhd: A high-resolution multi-modality model. arXiv preprint arXiv:2311.04219 (2023)\n' +
      '* [21] Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)\n' +
      '* [22] Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint arXiv:2311.06607 (2023)\n' +
      '* [23] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S., Ji, L., Mao, S., et al.: Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. arXiv preprint arXiv:2303.16434 (2023)\n' +
      '* [24] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740-755. Springer (2014)\n' +
      '* [25] Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)\n' +
      '* [26] Liu, H., et al: Visual instruction tuning. In: NeurIPS (2023)\n' +
      '* [27] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* [28] Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L.: Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329 (2022)\n' +
      '* [* [29] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)\n' +
      '* [30] Liu, Y., Li, Z., Li, H., Yu, W., Huang, M., Peng, D., Liu, M., Chen, M., Li, C., Jin, L., et al.: On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895 (2023)\n' +
      '* [31] Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for referring expression understanding. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14. pp. 792-807. Springer (2016)\n' +
      '* [32] Nguyen, T., Pham, C., Nguyen, K., Hoai, M.: Few-shot object counting and detection. In: European Conference on Computer Vision. pp. 348-365. Springer (2022)\n' +
      '* [33] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)\n' +
      '* [34] Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In: Proceedings of the IEEE international conference on computer vision. pp. 2641-2649 (2015)\n' +
      '* [35] Qi, J., Ding, M., Wang, W., Bai, Y., Lv, Q., Hong, W., Xu, B., Hou, L., Li, J., Dong, Y., et al.: Cogcom: Train large vision-language models diving into details through chain of manipulations. arXiv preprint arXiv:2402.04236 (2024)\n' +
      '* [36] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [37] Ranjan, V., Sharma, U., Nguyen, T., Hoai, M.: Learning to count everything. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n' +
      '* [38] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems **28** (2015)\n' +
      '* [39] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 (2023)\n' +
      '* [40] Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 (2023)\n' +
      '* [41] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n' +
      '* [42] Wang, J., Zhang, P., Chu, T., Cao, Y., Zhou, Y., Wu, T., Wang, B., He, C., Lin, D.: V3det: Vast vocabulary visual detection dataset. In: The IEEE International Conference on Computer Vision (ICCV) (October 2023)\n' +
      '* [43] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In: International Conference on Machine Learning. pp. 23318-23340. PMLR (2022)\n' +
      '* [44] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)* [45] Wang, W., Shi, M., Li, Q., Wang, W., Huang, Z., Xing, L., Chen, Z., Li, H., Zhu, X., Cao, Z., et al.: The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907 (2023)\n' +
      '* [46] Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 (2022)\n' +
      '* [47] Xiao, Y., Lepetit, V., Marlet, R.: Few-shot object detection and viewpoint estimation for objects in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence **45**(3), 3090-3106 (2022)\n' +
      '* [48] Yan, B., Jiang, Y., Wu, J., Wang, D., Luo, P., Yuan, Z., Lu, H.: Universal instance perception as object discovery and retrieval. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15325-15336 (2023)\n' +
      '* [49] Yang, H., Yue, S., He, Y.: Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224 (2023)\n' +
      '* [50] Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Unitab: Unifying text and box outputs for grounded vision-language modeling. In: European Conference on Computer Vision. pp. 521-539. Springer (2022)\n' +
      '* [51] You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704 (2023)\n' +
      '* [52] Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring expressions. In: Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69-85. Springer (2016)\n' +
      '* [53] Yu, L., Tan, H., Bansal, M., Berg, T.L.: A joint speaker-listener-reinforcer model for referring expressions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7282-7290 (2017)\n' +
      '* [54] Yu, Z., Yu, J., Xiang, C., Zhao, Z., Tian, Q., Tao, D.: Rethinking diversified and discriminative proposal generation for visual grounding. arXiv preprint arXiv:1805.03508 (2018)\n' +
      '* [55] Zhan, Y., Zhu, Y., Chen, Z., Yang, F., Tang, M., Wang, J.: Griffon: Spelling out all object locations at any granularity with large language models. arXiv preprint arXiv:2311.14552 (2023)\n' +
      '* [56] Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.Y.: Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605 (2022)\n' +
      '* [57] Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601 (2023)\n' +
      '* [58] Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., Kang, B.: Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581 (2023)\n' +
      '* [59] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>