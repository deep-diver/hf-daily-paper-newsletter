<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'to bidirectionally process latent poses, to enhance accurate motion generation within a temporal frame. Our proposed method achieves up to **50%** FID improvement and up to **4** times faster on the HumanML3D and KIT-ML datasets compared to the previous best diffusion-based method, which demonstrates strong capabilities of high-quality long sequence motion modeling and real-time human motion generation.\n' +
      '\n' +
      'Keywords:Human Motion Generation Selective State Space Models Latent Diffusion Models\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Human motion generation stands as a holy grail in generative computer vision, holding broad applications in computer animation, game development, and robot manipulation. To emulate human motion effectively, virtual characters must respond to the conditional context, exhibit natural movement, and perform motion accurately. Recent motion generation models are categorized into four main approaches: autoencoder-based [1, 10, 17, 36, 48, 55], utilizing transformers for latent space compression and motion synthesis; GAN-based [4, 20, 30], using discriminators to enhance the realism of generated motions; autoregressive models [24], treating motion sequences as languages with specialized codebooks; and diffusion-based [6, 49, 53], employing denoising steps for motion generation. Challenges vary across methods, with autoencoder models struggling to generate accurate motions from detailed descriptions due to textual information compression, GAN-based models facing training difficulties, especially in conditional tasks, and diffusion-based models relying on complex transformer-based architectures results in inefficient of motion prediction.\n' +
      '\n' +
      'Although diffusion-based models excel at generating motion with robust performance and often exhibit superior diversity, they encounter two limitations. 1) Convolutional or transformer-based diffusion methods exhibit limitations in generating _long-range motion sequences_. Previous transformer-based methodologies [6, 49, 53] have focused on modeling long-range dependencies and acquiring comprehensive global context information. Despite these advances, they are frequently associated with a substantial increase in computational requirements. Furthermore, transformer architectures are not intrinsically designed for temporal sequential modeling, which poses an inherent limitation. 2) The _efficiency_ of inference in transformer-based diffusion methods is constrained. Although prior research has attempted to leverage the Variational Autoencoder for denoising operations in the latent space [6], the inference speed remains adversely affected by the attention mechanism\'s quadratic scaling, leading to inefficient motion generation. Consequently, exploring a new architectural paradigm that accommodates long-range dependencies and maintains a linear computational complexity is crucial for sustaining motion generation tasks.\n' +
      '\n' +
      'Recent advances have sparked renewed interest in state space models (SSMs) [14, 15], a field that originated from the foundational classic state space model [25]. Modern versions of SSMs stand out due to their ability to effectively capture long-range dependencies, a capability greatly improved by the introduction of parallel training techniques. This evolution has led to various methodologies based on SSM, notably the linear state space layers (LSSL) [15], the structured state-space sequence model (S4) [14], the diagonal state space (DSS) [19], and S4D [13]. These methods have been carefully designed to handle sequential data across various tasks and modalities, paying special attention to modeling long-range dependencies. Their efficacy in managing long sequences is attributed to the implementation of convolutional computations [15] and near-linear computational strategies, such as mamba [12], marking a significant stride in sequentially oriented tasks, including large language model decoding and motion sequence generation.\n' +
      '\n' +
      'Adapting selective state space modules for motion generation tasks presents notable challenges, primarily due to the lack of specialized design in SSMs for capturing the sensitive motion details required for temporal representation and the complexities involved in aggregating latent space. In response to these challenges, we have meticulously developed a motion generation architecture, specifically tailored to address the intricacies of long-term sequence generation, while optimizing for computational efficiency with near-linear-time complexity. This innovation is embodied in the Motion Mamba model, a simple yet potent approach to motion generation. The Motion Mamba framework pioneers a diffusion-based generative system, incorporating two key components oriented toward SSM as shown in Figure. 2: (1) a **H**ierarchical **T**emporal **M**amba (HTM) block: This component is ingeniously crafted to arrange motion frames in sequential order, using hierarchically adjusted scanning. It is adept at identifying temporal dependencies at various depths, thereby facilitating a thorough comprehension of the dynamics inherent in motion sequences. (2) a **B**idirectional **S**patial **M**amba (BSM) block: This block is designed to unravel the structured latent skeleton by evaluating data from both forward and reverse directions. Its primary goal is to safeguard the continuity of information flow, significantly bolstering the model\'s capacity for precise motion generation through the retention of dense informational exchange.\n' +
      '\n' +
      'The Motion Mamba introduces a new approach to motion generation that strikes an exceptional trade-off between accuracy and efficiency, shown in Fig. 1. Our experimental results underscore the significant improvements brought about by Motion Mamba, showcasing a remarkable improvement in the Frechet Inception Distance (FID), with a reduction of up to 50% from the prior state-of-the-art metric of 0.473 to an impressive 0.281 on the HumanML3D dataset [17]. Furthermore, we emphasize our framework\'s unparalleled inference speed, which is four times faster than previous methods, achieving an average inference time of only 0.058 seconds per sequence compared to the 0.217 seconds required by the MLD [6] method per sequence. These outcomes unequivocally establish Motion Mamba\'s state-of-the-art performance, concurrently ensuring fast inference speeds for conditional human motion generation tasks.\n' +
      '\n' +
      'Our contributions to the field of motion generation can be summarized as:1. We introduce a simple yet effective framework, named _Motion Mamba_, which is a pioneering method integrates a selective scanning mechanism into motion generation tasks.\n' +
      '2. _Motion Mamba_ is comprised of two modules: Hierarchical Temporal Mamba (HTM) and Bidirectional Spatial Mamba (BSM), which are designed for temporal and spatial modeling, respectively. HTM blocks are tasked with processing temporal motion data, aiming to enhance motion consistency across frames. BSM blocks are engineered to bidirectionally capture the channel-wise flow of hidden information within the latent pose representations.\n' +
      '3. _Motion Mamba_ framework demonstrated exceptional performance on text-to-motion generation task, through experimental validation on the HumanML3D [17] and KIT-ML [38] datasets. Our methodology achieved state-of-the-art generation quality and significantly improved long-squence modeling, meanwhile optimizing inference speed.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '**Human Motion Generation.** Generating human motion is a fundamental task of computer vision, essential for various applications like 3D modeling and robot manipulation. Recently, the predominant method of achieving human motion generation, known as the _Text-to-Motion_ task, involves learning a common latent space for both language and motion.\n' +
      '\n' +
      'DVGAN [30] create the GAN [11] discriminator by densely validating at each time-scale and perturbing the discriminator input for translation invariance, enabling motion generation and completion. ERD-QV [20] enhances latent representations through two additive modifiers: a time-to-arrival embedding applied universally and an additive scheduled target noise vector used during extended transitions. It further improves transition quality by incorporating a GAN framework with two discriminators operating at different timescales. HP-GAN [4], trained with a modified version of the improved WGAN-GP [16], utilizes a custom loss function designed for human motion prediction. It learns a probability density function of future human poses conditioned on previous poses.\n' +
      '\n' +
      'Autoencoders [27, 43] are notable generative models known for their ability to represent data robustly by compressing high-dimensional data into a latent space, which is widely adopted in human motion generation. JL2P [1] uses RNN-based autoencoders [23] to learn a combined representation of language and pose. It restricts the direct mapping from text to motion to a one-to-one relationship. MotionCLIP [48] uses Transformer-based Autoencoders [51] to reconstruct motion while ensuring alignment with the corresponding text label in the CLIP [40] space. This alignment effectively integrates the semantic knowledge from CLIP into the human motion manifold. TEMOS [36] and T2M [17] combine a Transformer-based VAE [26] with a text encoder to generate distribution parameters that work within the VAE latent space. AttT2M [55] and TM2D [10] incorporate a body-part spatio-temporal encoder into VQ-VAE [50] for enhanced learning of a discrete latent space with increased expressiveness.\n' +
      '\n' +
      'Diffusion models [7, 22, 41, 47] have recently surpassed GANs and VAEs in generating 2D images. Developing a motion generation model based on diffusion models is obviously an attractive direction. MotionDiffuse [53] introduces the inaugural framework for text-driven motion generation based on diffusion models. It showcases several desirable properties, including probabilistic mapping, realistic synthesis, and multi-level manipulation. MDM [49] utilizes a classifier-free Transformer-based diffusion model for the human motion domain to predict sample rather than noise in each diffusion step. MLD [6] performs a diffusion process in latent motion space, rather than using a diffusion model to establish connections between raw motion sequences and conditional inputs.\n' +
      '\n' +
      '**State Space Models.** Recently, state space sequence models (SSMs) [14, 15], drawing inspiration from classical state-space models [25], have emerged as a promising architecture for sequence modeling. Mamba [12] introduces a selective SSM architecture, integrating time-varying parameters into the SSM framework, and proposes a hardware-aware algorithm to facilitate highly efficient training and inference processes. Some research works leverage SSM in computer vision to process 2D data. The 2D SSM [3] introduces an SSM block at the beginning of each transformer block [8, 51]. This approach aims to achieve efficient parameterization, accelerated computation, and a suitable normalization scheme. SGConvNeXt [29] presents a structured global convolution method inspired by ConvNeXt [32], incorporating multi-scale sub-kernels to achieve both parameterization efficiency and effective long sequence modeling. ConvSSM [46] integrates the tensor modeling principles of ConvLSTM [45] with SSMs, elucidating the utilization of parallel scans in convolutional recurrences. This approach enables subquadratic parallelization and rapid autoregressive generation. Vim [56] introduces a bidirectional SSM block for efficient and versatile visual representation learning, achieving performance comparable to established ViT [8] methods. VMamba [31] introduces a Cross-Scan Module (CSM) designed to traverse the spatial domain and transform any non-causal visual image into ordered patch sequences. This approach achieves linear complexity while preserving global receptive fields. There have also been attempts to utilize SSMs to handle higher-dimensional data. Mamba-ND [28] explores various combinations of SSM and different scan directions within the SSM block to adapt Mamba [12] to higher-dimensional tasks. Recent efforts have sought to replace the traditional transformer-based U-Net within the diffusion denoiser with the SSM block, with the aim of enhancing image generation efficiency. DiffuSSM [52] adeptly manages higher resolutions without relying on global compression, thus maintaining detailed image representation throughout the diffusion process.\n' +
      '\n' +
      '## 3 The Proposed Method\n' +
      '\n' +
      'In this section, we delineate the architecture and operational principles of the _Motion Mamba_ framework, designed for generating human motion over long ranges efficiently from textual descriptions. Initially, we discuss the foundational concepts underpinning our approach, including the Mamba Model [12] and the latent diffusion model [6]. Following this, we detail our uniquely crafted architecture that leverages the Mamba model to enhance motion generation efficiency. This architecture comprises two principal components: the Hierarchical Temporal Mamba (HTM) block, which addresses temporal aspects, and the Bidirectional Spatial Mamba (BSM) block, focusing on spatial dynamics.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**Selective Structured State Space Sequence Model.** SSMs particularly through the contributions of structured state space sequence models (S4) and Mamba, have demonstrated exceptional proficiency in handling long sequences. These models operationalize the mapping of a 1-D function or sequence, \\(x(t)\\in\\mathbb{R}\\mapsto y(t)\\in\\mathbb{R}\\), through a hidden state \\(h(t)\\in\\mathbb{R}^{N}\\), employing \\(\\mathbf{A}\\in\\mathbb{R}^{N\\times N}\\) as the evolution parameters, \\(\\mathbf{B}\\in\\mathbb{R}^{N\\times 1}\\) and \\(\\mathbf{C}\\in\\mathbb{R}^{1\\times N}\\) as the projection parameters, respectively. The continuous system dynamics are described by the ordinary differential equation (ODE):\n' +
      '\n' +
      '\\[h^{\\prime}(t) =\\mathbf{A}h(t)+\\mathbf{B}x(t), \\tag{1}\\] \\[y(t) =\\mathbf{C}h(t)+\\mathbf{D}x(t),\\]\n' +
      '\n' +
      'with \\(x(t)\\) representing a continuous input signal and \\(y(t)\\) a continuous output signal in the time domain.\n' +
      '\n' +
      'To adapt these continuous dynamics for practical computation, the S4 and Mamba models employ a discretization process, notably using the zero-order hold (ZOH) method, resulting in a transformation of continuous parameters into discrete ones:\n' +
      '\n' +
      '\\[\\overline{\\mathbf{A}} =\\exp{(\\mathbf{\\Delta A})}, \\tag{2}\\] \\[\\overline{\\mathbf{B}} =(\\mathbf{\\Delta A})^{-1}(\\exp{(\\mathbf{\\Delta A})}-\\mathbf{I}) \\cdot\\mathbf{\\Delta B}.\\]\n' +
      '\n' +
      'The discretized system can then be expressed as follows, incorporating a step size \\(\\mathbf{\\Delta}\\):\n' +
      '\n' +
      '\\[h_{t} =\\overline{\\mathbf{A}}h_{t-1}+\\overline{\\mathbf{B}}x_{t}, \\tag{3}\\] \\[y_{t} =\\mathbf{C}h_{t}.\\]\n' +
      '\n' +
      'This adaptation facilitates the computation of output through global convolution, leveraging a structured convolutional kernel \\(\\overline{\\mathbf{K}}\\), which encompasses the entire length \\(M\\) of the input sequence \\(\\mathbf{x}\\):\n' +
      '\n' +
      '\\[\\overline{\\mathbf{K}} =(\\mathbf{C}\\overline{\\mathbf{B}},\\mathbf{C}\\overline{\\mathbf{A} \\mathbf{B}},\\dots,\\mathbf{C}\\overline{\\mathbf{A}}^{M-1}\\overline{\\mathbf{B}}), \\tag{4}\\] \\[\\mathbf{y} =\\mathbf{x}*\\overline{\\mathbf{K}}.\\]\n' +
      '\n' +
      'Selective models like Mamba introduce time-varying parameters, deviating from the linear time invariance (LTI) assumption and complicating parallel computation. However, hardware-aware optimizations, such as associative scans, have been developed to address these computational challenges, highlighting the ongoing evolution and application of SSMs in modeling complex temporal dynamics.\n' +
      '\n' +
      '**Latent Motion Diffusion Model.** Diffusion probabilistic models offer a significant advancement in motion generation by gradually reducing noise from a Gaussian distribution to a target data distribution \\(p(x)\\) through a T-length learned Markov process [7, 22, 41, 44, 47, 49, 53], giving \\(\\{\\mathbf{x_{t}}\\}_{t=1}^{T}\\). In the motion generation, we define our trainable diffusion models with a denoiser \\(\\epsilon_{\\theta}\\left(x_{t},t\\right)\\) which anneal the random noise to motion sequence \\(\\{\\hat{x}_{t}^{1:N}\\}_{t=1}^{T}\\) iteratively. To address the inefficiencies of applying diffusion models directly to raw motion sequences, we employ a low-dimensional motion latent space for the diffusion process. Given an input condition c, such as a descriptive sentence \\(\\mathbf{w}^{1:N}=\\{w^{i}\\}_{i=1}^{N}\\), an action label \\(a\\) from a predefined set \\(\\mathcal{A}\\), or an empty condition \\(c=\\varnothing\\), and the motion representation that combines 3D joint rotations, positions, velocities, and foot contact as proposed in [17]. The frozen CLIP [40] text encoder \\(\\tau_{\\theta}^{w}\\) has been employed to obtain projected text embedding \\(\\tau_{\\theta}^{w}(w^{1:N})\\in\\mathbb{R}^{1\\times d}\\), thereby conditional denoiser comprised in term of \\(\\epsilon_{\\theta}(z_{t},t,\\tau_{\\theta}(c))\\). The latent diffusion model \\(\\epsilon_{\\theta}\\left(x_{t},t\\right)\\) aimed to generate the human motion sequence in terms of \\(\\hat{x}^{1:L}=\\{\\hat{x}^{i}\\}_{i=1}^{L}\\), where L denotes the sequence length or number of frames [34, 35, 37, 54]. Afterthat we reused the motion Variational AutoEncoder (VAE) \\(\\mathcal{V}=\\{\\mathcal{E},\\mathcal{D}\\}\\) proposed in MLD [6] to manipulate the motion sequence in latent space \\(z=\\mathcal{E}(x^{1:L})\\), and decompress the intermediate representation to motion sequence by \\(\\hat{x}^{1:L}=\\mathcal{D}(z)=\\mathcal{DE}(x^{1:L})\\)[26, 42, 51]. Finally, our latent diffusion model is trained with an objective focusing on minimization of MSE between true and predicted noise in the latent space, facilitating efficient and high-quality motion generation [2, 22].\n' +
      '\n' +
      'Figure 2: This figure illustrates the architecture of the proposed Motion Mamba model. Each of encoder and decoder blocks consists of a Hierarchical Temporal Mamba block (HTM) and a Bidirectional Spatial Mamba (BSM) block, which possess hierarchical scan and bidirectional scan within SSM layers respectively. This symmetric distribution of scans ensure a balanced and coherence framework across the encoder-decoder architecture.\n' +
      '\n' +
      '### Motion Mamba\n' +
      '\n' +
      'The architecture of the proposed _Motion Mamba_ framework is illustrated in Figure. 2. At its core, Motion Mamba utilizes a denoising U-Net architecture, which is distinguished for its effectiveness in modeling the continuous, temporal sequences of motion frames. This effectiveness is attributed to the inherent long-sequence modeling capacity of the Mamba model. The denoiser, denoted by \\(\\epsilon_{\\theta}\\), comprises \\(N\\) blocks including encoder \\(E_{1..N}\\) and decoder \\(D_{1..N}\\). Additionally, the architecture is enhanced with a transformer-based attention mixer block \\(M\\), designed to augment the model\'s ability to capture complex temporal dynamics.\n' +
      '\n' +
      '\\[\\epsilon_{\\theta}(x)\\in\\{E_{1...N},M,D_{1..N}\\}. \\tag{5}\\]\n' +
      '\n' +
      'The encoder blocks are represented as \\(E_{1..N}\\), arranged sequentially, and the decoder blocks as \\(D_{1..N}\\), configured in reverse order to facilitate effective bottom-up and top-down information flow. Given that selective operations have significantly lower computational complexity compared to attention-based methods, we have increased the number of scans to achieve higher quality generations. Concurrently, it is imperative to maintain a balance between the model\'s parameters and its efficiency. Thereby, a novel aspect of our model is the introduction of a hierarchical scan strategy, characterized by a sequence of scan numbers as,\n' +
      '\n' +
      '\\[K=\\{S^{2N-1},S^{2(N-1)-1},\\ldots,S^{1}\\}. \\tag{6}\\]\n' +
      '\n' +
      'This sequence specifies the number of scans allocated to each layer, in descending order of complexity. For instance, the uppermost encoder layer, \\(E_{1}\\), and the lowermost decoder layer, \\(D_{N}\\), are allocated \\(S^{2N-1}\\) scans, indicating the highest scanning complexity. Conversely, the lowest encoder layer, \\(E_{N}\\), and the uppermost decoder layer, \\(D_{1}\\), are assigned \\(S^{1}\\) scans, reflecting the lowest level ofscanning complexity.\n' +
      '\n' +
      '\\[E_{i}(S)=\\begin{cases}S^{2N-1}&\\text{for }i=1\\\\ S^{2(N-i)-1}&\\text{for }i=2,\\ldots,N-1\\\\ S^{1}&\\text{for }i=N\\end{cases} \\tag{7}\\]\n' +
      '\n' +
      '\\[D_{j}(S)=\\begin{cases}S^{2N-1}&\\text{for }j=N\\\\ S^{2(N-j)-1}&\\text{for }j=N-1,\\ldots,2\\\\ S^{1}&\\text{for }j=1\\end{cases} \\tag{8}\\]\n' +
      '\n' +
      'This hierarchical scanning approach ensures that processing capabilities are evenly distributed throughout the encoder-decoder architecture., facilitating a detailed and nuanced analysis of temporal sequences. Within this structured framework, each denoiser is equipped with a specialized Hierarchical Temporal Mamba (HTM) block, which serves to augment the model\'s ability to process temporal information effectively. Additionally, the proposed Motion Mamba incorporates an attention-based mixer block denoted as \\(M\\), strategically integrated to enhance conditional fusion.\n' +
      '\n' +
      '**Hierarchical Temporal Mamba (HTM)** block processes compressed latent representations, denoted as \\(z\\), with the dimensions \\((T,B,C)\\), of which procedure shown in Algorithm 1. Here, \\(T\\) signifies the temporal dimension, as specified in the Variational AutoEncoder (VAE) framework. Initially, the input \\(z\\) is subjected to a linear projection layer, producing transformed representations \\(x\\) and \\(z\\) with dimension \\(E\\). Our analysis revealed an increased density of motion within the lower-level feature spaces. Consequently, we developed a hierarchical scanning methodology that is executed at various depths of the network.\n' +
      '\n' +
      'This approach not only accommodates the diverse motion densities, but also significantly reduces computational overhead. This step utilizes a hierarchically structured set of scans, \\(K=\\{S^{2N_{n}-1},S^{2N_{n-1}-1},\\ldots,S^{1}\\}\\), in conjunction with a corresponding series of memory matrices \\(\\{A_{1},\\ldots,A_{k}\\}\\). Each sub-SSM scan first applies a 1-D convolution to \\(x\\), resulting in \\(x^{\\prime}_{o}\\). \\(x^{\\prime}_{o}\\) is then linearly projected to derive \\(B_{o}\\), \\(C_{o}\\), and \\(\\Delta_{o}\\). These projections \\(B_{o}\\), \\(C_{o}\\) use \\(\\Delta_{o}\\) to effect transformations in \\(\\overline{A}_{o}\\) and \\(\\overline{B}_{o}\\), respectively. After executing a sequence of SSM scans \\(\\{SSM_{A_{1},x},SSM_{A_{2},x},\\ldots,SSM_{A_{k},x}\\}\\), a set of outputs \\(\\{O_{1},\\ldots,O_{k}\\}\\) is compiled. This collection is subsequently aggregated via a linear projection to obtain the final output of the HTM block.\n' +
      '\n' +
      '#### 3.3.3 Bidirectional Spatial Mamba (BSM)\n' +
      '\n' +
      'block focuses on enhancing latent representation learning through a novel approach of dimension rearrangement and bidirectional scanning, of which the process is shown in Algorithm 2. Initially, it alters the original input dimensions from \\((T,B,C)\\) to \\((C,B,T)\\), effectively swapping the temporal and channel dimensions. After this rearrangement, the input, now denoted \\(z^{\\prime}\\), undergoes a linear projection after normalization, resulting in dimensions \\(x\\) and \\(z\\) of size \\(E\\). The process involves bidirectional scanning of the latent channel dimension, where \\(\\mathbf{x}\\) is subjected to a 1-D convolution, yielding \\(\\mathbf{x}^{\\prime}_{o}\\) for both forward and backward directions. Each \\(\\mathbf{x}^{\\prime}_{o}\\) is then linearly projected to obtain \\(B_{o}\\), \\(C_{o}\\), and \\(\\Delta_{o}\\), which are utilized to transform \\(\\overline{A}_{o}\\) and \\(\\overline{B}_{o}\\), respectively. The final output token sequence, \\(\\mathbf{z_{1}}\\), is computed by gating and summing the forward \\(y_{\\text{forward}}\\) and backward \\(y_{\\text{backward}}\\) output with \\(\\mathbf{z}\\). This component is engineered to decode the structured latent skeleton by analyzing data from both forward and reverse viewpoints. Its main objective is to ensure the seamless continuity of information flow, thereby substantially enhancing the model\'s ability to generate accurate motion. This is achieved through the maintenance of a dense informational exchange, which is critical for the model\'s performance.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'We evaluate our proposed Motion Mamba on two prominent _Text-to-Motion_ synthesis benchmarks as follows:\n' +
      '\n' +
      '**HumanML3D.** The HumanML3D [17] dataset aggregates 14,616 motions sourced from the AMASS [33] and HumanAct12 [18] datasets, with each motion accompanied by three textual descriptions, culminating in 44,970 scripts. This dataset spans a wide range of actions such as exercising, dancing, and acrobatics, presenting a rich motion-language corpus.\n' +
      '\n' +
      '**KIT-ML.** The KIT-ML dataset [38] is comprised of 3,911 motions paired with 6,278 textual descriptions, serving as a compact yet effective benchmark for evaluation. For both datasets, the pose representation adopted is derived from T2M [17], ensuring consistency in motion representation across evaluations.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      'We adapt the standard evaluation metrics on following aspects throughout our experiments, including: **Generation Quality.** We implement a Frechet inception distance (FID) [21] to quantify the realism and diversity of motion generated by models. Moreover, we use multi-modal distance (MM Dist) to measure the distance between motions and texts and assess motion-text alignment. **Diversity.** We use the diversity metric to measure motion diversity, which calculates variance in features extracted from the motions. Additionally, we employ multi-modality (MModality) to assess diversity within generated motions sharing the same text description.\n' +
      '\n' +
      '### Comparative Studies\n' +
      '\n' +
      'We evaluate our method against the state-of-the-art methods on the HumanML3D [17] and KIT-ML [38] datasets. We train our Motion Mamba with HTM arrangement strategy MM (\\(\\{S^{2N_{n}-1},\\dots,S^{1}\\}\\)), BSM bidirectional block strategy on the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{R Precision \\(\\uparrow\\)} & \\multicolumn{2}{c}{FID\\(\\downarrow\\)} & \\multicolumn{2}{c}{MM Dist\\(\\downarrow\\) Diversity\\(\\rightarrow\\)MModality\\(\\uparrow\\)} \\\\ \\cline{2-7}  & Top 1 & Top 2 & Top 3 & & & \\\\ \\hline Seq2Seq [39] & \\(0.180^{\\pm.002}\\) & \\(0.300^{\\pm.002}\\) & \\(0.396^{\\pm.002}\\) & \\(11.75^{\\pm.035}\\) & \\(5.529^{\\pm.007}\\) & \\(6.223^{\\pm.061}\\) & - \\\\ LJ2P [1] & \\(0.246^{\\pm.001}\\) & \\(0.387^{\\pm.002}\\) & \\(0.486^{\\pm.002}\\) & \\(11.02^{\\pm.046}\\) & \\(5.296^{\\pm.008}\\) & \\(7.676^{\\pm.058}\\) & - \\\\ T2G [5] & \\(0.165^{\\pm.001}\\) & \\(0.267^{\\pm.002}\\) & \\(0.345^{\\pm.002}\\) & \\(7.664^{\\pm.030}\\) & \\(6.030^{\\pm.008}\\) & \\(6.409^{\\pm.071}\\) & - \\\\ Hier [9] & \\(0.301^{\\pm.002}\\) & \\(0.425^{\\pm.002}\\) & \\(0.552^{\\pm.004}\\) & \\(6.532^{\\pm.024}\\) & \\(5.012^{\\pm.018}\\) & \\(8.332^{\\pm.042}\\) & - \\\\ TEMOS [37] & \\(0.424^{\\pm.002}\\) & \\(0.612^{\\pm.002}\\) & \\(0.722^{\\pm.002}\\) & \\(3.734^{\\pm.028}\\) & \\(3.703^{\\pm.008}\\) & \\(8.973^{\\pm.01}\\) & \\(0.368^{\\pm.018}\\) \\\\ T2M [17] & \\(0.457^{\\pm.002}\\) & \\(0.639^{\\pm.003}\\) & \\(0.740^{\\pm.003}\\) & \\(1.067^{\\pm.002}\\) & \\(3.340^{\\pm.008}\\) & \\(9.188^{\\pm.002}\\) & \\(2.090^{\\pm.083}\\) \\\\ MDM [49] & \\(0.320^{\\pm.005}\\) & \\(0.498^{\\pm.004}\\) & \\(0.611^{\\pm.007}\\) & \\(0.544^{\\pm.044}\\) & \\(5.566^{\\pm.027}\\) & \\(\\mathbf{95.59^{\\pm.086}}\\) & \\(\\mathbf{2.799^{\\pm.072}}\\) \\\\ MotionDiffuse [53] & \\(0.491^{\\pm.001}\\) & \\(0.681^{\\pm.001}\\) & \\(0.782^{\\pm.001}\\) & \\(0.630^{\\pm.001}\\) & \\(3.113^{\\pm.001}\\) & \\(9.410^{\\pm.049}\\) & \\(1.553^{\\pm.042}\\) \\\\ MLD [6] & \\(0.481^{\\pm.003}\\) & \\(0.673^{\\pm.003}\\) & \\(0.772^{\\pm.002}\\) & \\(0.473^{\\pm.013}\\) & \\(3.196^{\\pm.010}\\) & \\(9.724^{\\pm.082}\\) & \\(\\underline{2.413}^{\\pm.079}\\) \\\\ \\hline\n' +
      '**Motion Mamba (Ours)** & \\(\\mathbf{0.502^{\\pm.003}}\\) & \\(\\mathbf{0.693^{\\pm.002}}\\) & \\(\\mathbf{0.792^{\\pm.002}}\\) & \\(\\mathbf{0.281^{\\pm.009}}\\) & \\(\\mathbf{3.060^{\\pm.038}}\\) & \\(9.871^{\\pm.084}\\) & \\(2.294^{\\pm.058}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison of text-conditional motion synthesis on HumanML3D [17]. These metrics are evaluated by the motion encoder from [17]. Empty MModality indicates the non-diverse generation methods. We employ real motion as a reference and sort all methods by descending FIDs. The right arrow \\(\\rightarrow\\) means that the closer to the real motion, the better. **Bold** and underline indicate the best and second best result.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{R Precision \\(\\uparrow\\)} & \\multicolumn{2}{c}{FID\\(\\downarrow\\)} & \\multicolumn{2}{c}{MM Dist\\(\\downarrow\\) Diversity\\(\\rightarrow\\)MModality\\(\\uparrow\\)} \\\\ \\cline{2-7}  & Top 1 & Top 2 & Top 3 & & & \\\\ \\hline Real & \\(0.424^{\\pm.005}\\) & \\(0.649^{\\pm.006}\\) & \\(0.779^{\\pm.006}\\) & \\(0.031^{\\pm.004}\\) & \\(2.788^{\\pm.012}\\) & \\(11.08^{\\pm.007}\\) & - \\\\ \\hline Seq2Seq [39] & \\(0.103^{\\pm.003}\\) & \\(0.178^{\\pm.005}\\) & \\(0.241^{\\pm.006}\\) & \\(24.86^{\\pm.348}\\) & \\(7.960^{\\pm.031}\\) & \\(6.744^{\\pm.106}\\) & - \\\\ T2G [5] & \\(0.156^{\\pm.004}\\) & \\(0.255^{\\pm.004}\\) & \\(0.338^{\\pm.005}\\) & \\(12.12^{\\pm.183}\\) & \\(6.964^{\\pm.029}\\) & \\(9.334^{\\pm.079}\\) & - \\\\ LJ2P [1] & \\(0.221^{\\pm.005}\\) & \\(0.373^{\\pm.004}\\) & \\(0.483^{\\pm.005}\\) & \\(6.545^{\\pm.072}\\) & \\(5.147^{\\pm.030}\\) & \\(9.073^{\\pm.100}\\) & - \\\\ Hier [9] & \\(0.255^{\\pm.006}\\) & \\(0.432^{\\pm.007}\\) & \\(0.531^{\\pm.007}\\) & \\(5.203^{\\pm.107}\\) & \\(4.986^{\\pm.027}\\) & \\(9.563^{\\pm.072}\\) & \\(2.090^{\\pm.083}\\) \\\\latent dimension = 2 with 11 layers. We evaluate our model and previous works with suggested metrics in HumanML3D [17] and calculate 95% confidence interval by repeat evaluation 20 times. The results for the HumanML3D dataset are presented in Table 1. Our model outperforms other methods significantly across various evaluation metrics, including FID, R precision, multi-modal distance, and diversity. For instance, our Motion Mamba outperforms previous best diffusion based motion generation MLD by 40.5% in terms of FID, and up to 10% improvement on R Precision, we aslo obtained best MModality by 3.060. The results for the KIT-ML dataset are presented in Table 2. We have also outperformed other well-established methods in FID and multi-modal distance.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'We concluded the ablation studies including long sequence evaluation, hierarchical design with HTM, bidirectional design in the BSM, number of latent dimensions, and number of layers of our proposed motion mamba in Table 4.\n' +
      '\n' +
      '**Long Sequence Motion Generation**. The HumanML3D [17] dataset exhibits a long-tailed and right-skewed distribution with a significant proportion of long-sequence human motions, as shown in Figure 3. We suggest previous study overlooked the challenges in the long-sequence generation problem. Thus, we introduce a new dataset variant, _HumanML3D-LS_, comprising motion sequences longer than 190 frames extracted from the original test set. This addition allows us to showcase our capability in generating long-sequence motions. Subsequently, we evaluate the performance of our method on HumanML3D-LS and compare it with other diffusion-based motion generation approaches. The comparative results are presented in Table 3. Motion Mamba by leverage the benefits on long-range dependency modeling make it well suitable for long sequence motion generation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{R Precision \\(\\uparrow\\)} & \\multirow{2}{*}{FID\\(\\downarrow\\)} & \\multirow{2}{*}{MM Dist\\(\\downarrow\\)} & \\multirow{2}{*}{Diversity\\(\\rightarrow\\) MModality\\(\\uparrow\\)} \\\\ \\cline{2-2} \\cline{5-8}  & Top 1 & & & & & & \\\\ \\hline Real & \\(0.437^{\\pm.003}\\) & \\(0.622^{\\pm.004}\\) & \\(0.721^{\\pm.004}\\) & \\(0.004^{\\pm.000}\\) & \\(3.343^{\\pm.015}\\) & \\(8.423^{\\pm.000}\\) & - \\\\ \\hline MDM [49] & \\(0.368^{\\pm.005}\\) & \\(0.553^{\\pm.006}\\) & \\(0.672^{\\pm.005}\\) & \\(0.802^{\\pm.004}\\) & \\(3.860^{\\pm.025}\\) & \\(8.817^{\\pm.008}\\) & - \\\\ MotionDiffuse [53] & \\(0.367^{\\pm.004}\\) & \\(0.521^{\\pm.004}\\) & \\(0.623^{\\pm.004}\\) & \\(2.460^{\\pm.002}\\) & \\(3.789^{\\pm.005}\\) & \\(\\mathbf{8.707^{\\pm.143}}\\) & \\(1.602^{\\pm.013}\\) \\\\ MLD [6] & \\(0.403^{\\pm.005}\\) & \\(0.584^{\\pm.005}\\) & \\(0.690^{\\pm.005}\\) & \\(0.952^{\\pm.020}\\) & \\(3.580^{\\pm.016}\\) & \\(9.050^{\\pm.085}\\) & \\(\\mathbf{2.711^{\\pm.104}}\\) \\\\ \\hline\n' +
      '**Motion Mamba (Ours)** & \\(\\mathbf{0.417^{\\pm.003}}\\) & \\(\\mathbf{0.606^{\\pm.003}}\\) & \\(\\mathbf{0.713^{\\pm.004}}\\) & \\(\\mathbf{0.668^{\\pm.019}}\\) & \\(\\mathbf{3.435^{\\pm.015}}\\) & \\(9.021^{\\pm.070}\\) & \\(\\underline{2.373^{\\pm.084}}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: In order to evaluate the models’ capability in long sequence motion generation, we compared our method with an existing approach on the recently introduced HumanML3D-LS dataset. This dataset comprises motion sequences longer than 190 frames from the original evaluation set. Our model demonstrates superior performance compared to other methods.\n' +
      '\n' +
      'Figure 3: The figure shows a long tail distribution of the HumanML3D [17], which has a significant proportion of long-sequence human motions.\n' +
      '\n' +
      '**Hierarchical Design with HTM.** In our ablation studies, we observed a slight improvement upon reversing the scan order from a lower to a higher level, specifically transitioning from MM \\(\\{S^{1},\\dots,S^{N}\\}\\) to MM \\(\\{S^{N},\\dots,S^{1}\\}\\). This enhancement suggests a correlation with the increase in temporal motion density within the lower-level feature spaces. Furthermore, to achieve the optimal result, we introduce the hierarchical design to arrange the scanning frequency, resulting in the sequence MM \\(\\{S^{2N_{n}-1},\\dots,S^{1}\\}\\). This expansion in the number of scans led to a performance increase. We attribute this enhancement to the observation that individual selective scan operations significantly reduce the parameter count, especially when compared to the parameter-intensive constructs of self-attention and feedforward network blocks prevalent in transformer architectures.\n' +
      '\n' +
      '**Bidirectional Design in BSM.** We developed three distinct variations of latent scanning mechanisms, differentiated by their scanning directions. In the context of motion generation tasks, we posit that the flow of hidden information within the structured latent skeleton holds significance, an aspect previously underexplored. Our ablation study reveals that a _single scan_ across the latent dimension yields minimal improvement. Subsequently, we investigated both layer-based and block-based bidirectional scans. Our findings indicate that the block-based bidirectional scan achieves optimal performance. This suggests that spatial information flows are encoded within the latent spaces and that bidirectional scanning facilitates the exchange of this information, thereby enhancing the efficacy of motion generation tasks.\n' +
      '\n' +
      '**Architecture Design for Motion Mamba.** The proposed Motion Mamba which is grounded in a standardized motion latent diffusion system. We delved into the interplay between dimensional aspects and the module\'s capacity (mea\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{R Precision} & \\multirow{2}{*}{FID\\(\\downarrow\\)} & \\multirow{2}{*}{MM Dist.\\(\\downarrow\\) Diversity\\(\\rightarrow\\) MModality\\(\\uparrow\\)} \\\\  & & Top \\(\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{    }}}}}}}}}}}}}}\\) & & \\\\ \\hline Real & 0.797\\({}^{\\pm.002}\\) & 0.002\\({}^{\\pm.000}\\) & 2.974\\({}^{\\pm.008}\\) & 9.503\\({}^{\\pm.065}\\) & - \\\\ \\hline MM \\(\\{S^{1},\\dots,S^{N}\\}\\) & 0.673\\({}^{\\pm.003}\\) & 1.278\\({}^{\\pm.012}\\) & 3.802\\({}^{\\pm.041}\\) & 8.678\\({}^{\\pm.006}\\) & 3.127\\({}^{\\pm.024}\\) \\\\ MM \\(\\{S^{N},\\dots,S^{1}\\}\\) & 0.738\\({}^{\\pm.002}\\) & 0.962\\({}^{\\pm.011}\\) & 3.433\\({}^{\\pm.003}\\) & 9.180\\({}^{\\pm.071}\\) & 2.723\\({}^{\\pm.033}\\) \\\\ MM \\(\\{S^{1},\\dots,S^{2N_{n-1}}\\}\\) & 0.698\\({}^{\\pm.002}\\) & 0.856\\({}^{\\pm.008}\\) & 3.624\\({}^{\\pm.07}\\) & 9.229\\({}^{\\pm.067}\\) & 2.826\\({}^{\\pm.017}\\) \\\\\n' +
      '**MM** \\(\\{S^{2N_{n-1}},\\dots,S^{1}\\}\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ \\hline MM \\((SingleScan)\\) & 0.736\\({}^{\\pm.003}\\) & 1.063\\({}^{\\pm.010}\\) & 3.443\\({}^{\\pm.026}\\) & 9.180\\({}^{\\pm.067}\\) & 2.676\\({}^{\\pm.041}\\) \\\\ MM \\((BiScan,layer)\\) & 0.735\\({}^{\\pm.004}\\) & 0.789\\({}^{\\pm.007}\\) & 3.408\\({}^{\\pm.034}\\) & 9.374\\({}^{\\pm.059}\\) & 2.591\\({}^{\\pm.046}\\) \\\\\n' +
      '**MM** \\((BiScan,block)\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ \\hline MM \\((Dim,1)\\) & 0.706\\({}^{\\pm.003}\\) & 0.652\\({}^{\\pm.011}\\) & 3.541\\({}^{\\pm.072}\\) & 9.141\\({}^{\\pm.032}\\) & 2.612\\({}^{\\pm.055}\\) \\\\\n' +
      '**MM** \\((Dim,2)\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ MM \\((Dim,5)\\) & 0.741\\({}^{\\pm.008}\\) & 0.728\\({}^{\\pm.009}\\) & 3.307\\({}^{\\pm.027}\\) & 9.427\\({}^{\\pm.009}\\) & 2.314\\({}^{\\pm.062}\\) \\\\ MM \\((Dim,7)\\) & 0.738\\({}^{\\pm.004}\\) & 0.599\\({}^{\\pm.007}\\) & 3.359\\({}^{\\pm.068}\\) & 9.166\\({}^{\\pm.075}\\) & 2.488\\({}^{\\pm.037}\\) \\\\ MM \\((Dim,10)\\) & 0.715\\({}^{\\pm.003}\\) & 0.628\\({}^{\\pm.008}\\) & 3.548\\({}^{\\pm.043}\\) & 9.200\\({}^{\\pm.075}\\) & 2.884\\({}^{\\pm.096}\\) \\\\ \\hline MM (9 layers) & 0.755\\({}^{\\pm.002}\\) & 1.080\\({}^{\\pm.012}\\) & 3.309\\({}^{\\pm.057}\\) & 9.721\\({}^{\\pm.081}\\) & 2.974\\({}^{\\pm.039}\\) \\\\\n' +
      '**MM** **(11 layers)** & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ MM (27 layers) & 0.750\\({}^{\\pm.003}\\) & 0.975\\({}^{\\pm.008}\\) & 3.336\\({}^{\\pm.096}\\) & 9.249\\({}^{\\pm.071}\\) & 2.821\\({}^{\\pm.063}\\) \\\\ MM (37 layers) & 0.754\\({}^{\\pm.005}\\) & 0.809\\({}^{\\pm.010}\\) & 3.338\\({}^{\\pm.061}\\) & 9.355\\({}^{\\pm.062}\\) & 2.741\\({}^{\\pm.077}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Evaluation of text-based motion synthesis on HumanML3D [17]: we use metrics in Table 1 and provides real reference, we evaluate the various HTM and BSM design choices, the dimension of the latent input, the different number of layer of Motion Mamba model.\n' +
      '\n' +
      'sured by the number of layers) to ascertain their impact on system performance. Experimental results demonstrate that the Motion Mamba achieves superior performance at a latent dimension of 2, diverging from prior works where the optimal dimension was identified as 1. We attribute this discrepancy to our HTM, which necessitates multiple scans correlating with the sequence length, thus implicating dimensionality as a pivotal factor. A marginal increase in dimensionality enabled us to attain peak performance, simultaneously enhancing efficiency compared to models with a dimensionality of 10. Furthermore, we conducted experiments to determine the optimal layer count for Motion Mamba, inspired by the design of its selective scanning mechanism. Notably, a single Mamba layer achieves a parameter reduction of approximately 75% compared to a conventional transformer encoder block. By increasing the number of layers, we aim to uncover the relationship between model capacity and its performance. Our findings reveal that, through the integration of our specially designed HTM and BSM (Bidirectional Scanning Module) blocks, the Motion Mamba reaches its optimal performance with 11 layers. This represents a slight increase over the MLD [6] baseline. However, due to the reduced parameter count in each layer, Motion Mamba exhibits significantly greater efficiency than previous methodologies.\n' +
      '\n' +
      '### Inference Time\n' +
      '\n' +
      'Inference time remains a significant challenge for diffusion-based methods. To address this, we enhance the inference speed by incorporating the efficient Mamba block within a lightweight architecture. Compared to the previous strong baseline, such as the MLD model cited in [6], which reports an average inference time of 0.217 seconds, our Motion Mamba model achieves a notable reduction in computational overhead, as shown in Figure 4. Specifically, it requires four times less computational effort, thereby facilitating faster and real-time inference speeds.\n' +
      '\n' +
      '## 5 Discussion and Conclusion\n' +
      '\n' +
      'In this study, we introduced Motion Mamba, a novel framework designed for efficient and extended sequence motion generation. Our approach represents the inaugural integration of the Mamba model within the domain of motion generation, featuring significant advancements including the implementation of Hierarchical Temporal Mamba (HTM) blocks. These blocks are specifically engineered to enhance temporal alignment through hierarchically organized selective scanning. Furthermore, Bidirectional Spatial Mamba (BSM) blocks have been developed to\n' +
      '\n' +
      'Figure 4: The figure shows the average inference time per sentence (AIT) vs FID, our proposed motion Mamba obtained 0.058s AIT and 0.281 FID overall outperform previous methods. We evaluate all methods on a single V100 GPU.\n' +
      '\n' +
      'amplify the exchange of information flow within latent spaces, thereby augmenting the model\'s ability to bidirectionally capture skeleton-level density features with greater precision. Compared to previous diffusion-based motion generation methodologies that predominantly utilize transformer blocks, our Motion Mamba framework achieves SOTA performance, evidencing an improvement of up to 50% in FID scores and a quadrupled improvement in inference speed. Through comprehensive experimentation across a variety of human motion generation tasks, the effectiveness and efficiency of our proposed Motion Mamba model have been robustly demonstrated, marking a significant leap forward in the field of human motion generation.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose forecasting. In: 2019 International Conference on 3D Vision (3DV). pp. 719-728. IEEE (2019)\n' +
      '* [2] Bao, F., Li, C., Cao, Y., Zhu, J.: All are worth words: a vit backbone for score-based diffusion models. arXiv preprint arXiv:2209.12152 (2022)\n' +
      '* [3] Baron, E., Zimerman, I., Wolf, L.: 2-d ssm: A general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635 (2023)\n' +
      '* [4] Barsoum, E., Kender, J., Liu, Z.: Hp-gan: Probabilistic 3d human motion prediction via gan. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 1418-1427 (2018)\n' +
      '* [5] Bhattacharya, U., Rewkowski, N., Banerjee, A., Guhan, P., Bera, A., Manocha, D.: Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents. In: 2021 IEEE virtual reality and 3D user interfaces (VR). pp. 1-10. IEEE (2021)\n' +
      '* [6] Chen, X., Jiang, B., Liu, W., Huang, Z., Fu, B., Chen, T., Yu, G.: Executing your commands via motion diffusion in latent space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18000-18010 (2023)\n' +
      '* [7] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems **34**, 8780-8794 (2021)\n' +
      '* [8] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020)\n' +
      '* [9] Ghosh, A., Cheema, N., Oguz, C., Theobalt, C., Slusallek, P.: Synthesis of compositional animations from textual descriptions. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1396-1406 (2021)\n' +
      '* [10] Gong, K., Lian, D., Chang, H., Guo, C., Jiang, Z., Zuo, X., Mi, M.B., Wang, X.: Tm2d: Bimodality driven 3d dance generation via music-text integration. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9942-9952 (2023)\n' +
      '* [11] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems **27** (2014)\n' +
      '* [12] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)\n' +
      '* [13] Gu, A., Goel, K., Gupta, A., Re, C.: On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems **35**, 35971-35983 (2022)\n' +
      '* [14] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021)\n' +
      '* [15] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Re, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems **34**, 572-585 (2021)\n' +
      '* [16] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein gans. Advances in neural information processing systems **30** (2017)\n' +
      '* [17] Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating diverse and natural 3d human motions from text. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5152-5161 (2022)* [18] Guo, C., Zuo, X., Wang, S., Zou, S., Sun, Q., Deng, A., Gong, M., Cheng, L.: Action2motion: Conditioned generation of 3d human motions. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 2021-2029 (2020)\n' +
      '* [19] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems **35**, 22982-22994 (2022)\n' +
      '* [20] Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion in-betweening. ACM Transactions on Graphics (TOG) **39**(4), 60-1 (2020)\n' +
      '* [21] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems **30** (2017)\n' +
      '* [22] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems **33**, 6840-6851 (2020)\n' +
      '* [23] Hopfield, J.J.: Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences **79**(8), 2554-2558 (1982)\n' +
      '* [24] Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., Chen, T.: Motiongpt: Human motion as a foreign language. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [25] Kalman, R.E.: A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering **82**(1), 35-45 (1960)\n' +
      '* [26] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. stat **1050**, 1 (2014)\n' +
      '* [27] Kramer, M.A.: Nonlinear principal component analysis using autoassociative neural networks. AIChE journal **37**(2), 233-243 (1991)\n' +
      '* [28] Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-dimensional data. arXiv preprint arXiv:2402.05892 (2024)\n' +
      '* [29] Li, Y., Cai, T., Zhang, Y., Chen, D., Dey, D.: What makes convolutional models great on long sequence modeling? In: The Eleventh International Conference on Learning Representations (2022)\n' +
      '* [30] Lin, X., Amer, M.R.: Human motion modeling using dvgans. arXiv preprint arXiv:1804.10652 (2018)\n' +
      '* [31] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)\n' +
      '* [32] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11976-11986 (2022)\n' +
      '* [33] Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5442-5451 (2019)\n' +
      '* [34] Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas, D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single image. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)\n' +
      '* [35] Petrovich, M., Black, M.J., Varol, G.: Action-conditioned 3D human motion synthesis with transformer VAE. In: International Conference on Computer Vision (ICCV) (2021)\n' +
      '* [36] Petrovich, M., Black, M.J., Varol, G.: Temos: Generating diverse human motions from textual descriptions. In: European Conference on Computer Vision. pp. 480-497. Springer (2022)* [37] Petrovich, M., Black, M.J., Varol, G.: TEMOS: Generating diverse human motions from textual descriptions. In: European Conference on Computer Vision (ECCV) (2022)\n' +
      '* [38] Plappert, M., Mandery, C., Asfour, T.: The kit motion-language dataset. Big data **4**(4), 236-252 (2016)\n' +
      '* [39] Plappert, M., Mandery, C., Asfour, T.: Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. Robotics and Autonomous Systems **109**, 13-26 (2018)\n' +
      '* [40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [41] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [42] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234-241. Springer (2015)\n' +
      '* [43] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations by error propagation. Parallel Distributed Processing pp. 318-362 (1986)\n' +
      '* [44] Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022)\n' +
      '* [45] Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems **28** (2015)\n' +
      '* [46] Smith, J., De Mello, S., Kautz, J., Linderman, S., Byeon, W.: Convolutional state space models for long-range spatiotemporal modeling. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [47] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. pp. 2256-2265. PMLR (2015)\n' +
      '* [48] Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip: Exposing human motion generation to clip space. In: European Conference on Computer Vision. pp. 358-374. Springer (2022)\n' +
      '* [49] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-or, D., Bermano, A.H.: Human motion diffusion model. In: The Eleventh International Conference on Learning Representations (2022)\n' +
      '* [50] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. Advances in neural information processing systems **30** (2017)\n' +
      '* [51] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [52] Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without attention. arXiv preprint arXiv:2311.18257 (2023)\n' +
      '* [53] Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)\n' +
      '* [54] Zhang, Y., Black, M.J., Tang, S.: We are more than our joints: Predicting how 3d bodies move. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3372-3382 (2021)* [55] Zhong, C., Hu, L., Zhang, Z., Xia, S.: Attt2m: Text-driven human motion generation with multi-perspective attention mechanism. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 509-519 (2023)\n' +
      '* [56] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>