<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '잠재 포즈를 양방향으로 처리하여 시간적 프레임 내에서 정확한 모션 생성을 향상시킨다. 제안된 방법은 기존의 확산 기반 방법보다 HumanML3D 및 KIT-ML 데이터 셋에서 최대 **50%** FID 향상 및 최대 **4** 배의 빠른 성능 향상을 보였으며, 이는 고품질의 긴 시퀀스 모션 모델링 및 실시간 인간 모션 생성의 강력한 능력을 보여준다.\n' +
      '\n' +
      '키워드: 휴먼 모션 생성 선택적 상태 공간 모델 잠재 확산 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간의 동작 생성은 컴퓨터 애니메이션, 게임 개발, 로봇 조작 등의 광범위한 응용 분야를 보유하면서 생성 컴퓨터 비전 분야에서 신성한 성배로 자리 잡고 있다. 인간의 움직임을 효과적으로 모방하기 위해서는 가상 캐릭터가 조건부 맥락에 반응하고 자연스러운 움직임을 보이며 정확하게 움직임을 수행해야 한다. 최근 모션 생성 모델은 크게 오토인코더 기반 [1, 10, 17, 36, 48, 55], 잠재 공간 압축 및 모션 합성을 위한 트랜스포머 활용, 생성된 모션의 사실성을 높이기 위한 판별기 활용, 자동 회귀 모델[24], 모션 시퀀스를 전문 코드북이 있는 언어로 취급하기 위한 디노이징 단계 활용, 확산 기반 [6, 49, 53]의 네 가지 접근법으로 분류된다. 문제는 텍스트 정보 압축으로 인해 상세한 설명으로부터 정확한 동작을 생성하기 위해 고군분투하는 오토인코더 모델, 특히 조건부 작업에서 훈련 어려움에 직면한 GAN 기반 모델, 복잡한 변압기 기반 아키텍처에 의존하는 확산 기반 모델이 움직임 예측의 비효율성을 초래하는 등 방법에 따라 다양하다.\n' +
      '\n' +
      '확산 기반 모델은 강력한 성능으로 모션을 생성하는 데 탁월하고 종종 우수한 다양성을 나타내지만 두 가지 제한 사항에 직면한다. 1) Convolutional 또는 Transformer 기반의 확산 방법은 _long-range motion sequence_를 생성하는데 한계를 보인다. 기존의 트랜스포머 기반 방법론[6, 49, 53]은 장거리 의존성을 모델링하고 포괄적인 전역 컨텍스트 정보를 획득하는 데 중점을 두었다. 이러한 발전에도 불구하고, 이는 종종 계산 요구 사항의 상당한 증가와 관련이 있다. 또한, 변압기 아키텍처는 시간적 순차 모델링을 위해 본질적으로 설계되지 않아 본질적인 한계를 내포하고 있다. 2) 트랜스포머 기반 확산 방법에서 추론의 _efficiency_가 제약된다. 이전 연구에서는 잠재 공간에서의 잡음 제거 연산을 위해 Variational Autoencoder를 활용하려고 시도했지만[6], 추론 속도는 주의 메커니즘의 2차 스케일링에 의해 부정적인 영향을 받아 비효율적인 모션 생성으로 이어진다. 결과적으로, 장거리 의존성을 수용하고 선형 계산 복잡도를 유지하는 새로운 아키텍처 패러다임을 탐색하는 것은 모션 생성 작업을 지속하는 데 중요하다.\n' +
      '\n' +
      '최근의 발전은 기본 고전적인 상태 공간 모델[25]에서 비롯된 분야인 상태 공간 모델(SSM)[14, 15]에 대한 새로운 관심을 불러일으켰다. 근대 버전의 SSM은 장거리 의존성을 효과적으로 포착할 수 있는 능력으로 인해 두드러지며, 이는 병렬 훈련 기술의 도입으로 크게 향상된 능력이다. 이러한 진화는 SSM, 특히 선형 상태 공간 계층(LSSL)[15], 구조화된 상태 공간 시퀀스 모델(S4)[14], 대각 상태 공간(DSS)[19], 및 S4D[13]에 기초한 다양한 방법론으로 이어졌다. 이러한 방법은 다양한 작업 및 양식에 걸쳐 순차적 데이터를 처리하도록 신중하게 설계되었으며, 장거리 종속성을 모델링하는 데 특히 주의를 기울였다. 긴 시퀀스를 관리하는 그들의 효능은 컨볼루션 계산[15] 및 mamba[12]와 같은 거의 선형 계산 전략의 구현에 기인하며, 대규모 언어 모델 디코딩 및 모션 시퀀스 생성을 포함하여 순차적으로 배향된 태스크에서 상당한 보폭을 나타낸다.\n' +
      '\n' +
      '모션 생성 작업을 위한 선택적 상태 공간 모듈을 적용하는 것은 주로 시간적 표현에 필요한 민감한 모션 세부 사항을 캡처하기 위한 SSM의 특수 설계 부족과 잠재 공간을 집계하는 데 수반되는 복잡성으로 인해 주목할만한 과제를 제시한다. 이러한 문제에 대응하여, 우리는 모션 생성 아키텍처를 세심하게 개발했으며, 특히 장기 시퀀스 생성의 복잡성을 해결하기 위해 맞춤화하면서 거의 선형 시간 복잡성으로 계산 효율성을 최적화했다. 이 혁신은 모션 생성에 대한 간단하면서도 강력한 접근 방식인 모션 맘바 모델로 구현됩니다. 모션 맘바 프레임워크는 그림과 같이 SSM을 지향하는 두 가지 핵심 구성 요소를 통합하는 확산 기반 생성 시스템을 개척한다. 2: (1) **H** 계층적 **T** 시간적 **M**amba(HTM) 블록: 이 컴포넌트는 계층적으로 조정된 스캐닝을 사용하여 모션 프레임들을 순차적인 순서로 배열하도록 독창적으로 조작된다. 다양한 깊이에서 시간적 종속성을 식별하는 데 능숙하여 모션 시퀀스에 내재된 역학에 대한 철저한 이해를 촉진한다. (2) **B**방향 **S**patial **M**amba(BSM) 블록: 이 블록은 순방향 및 역방향 모두에서 데이터를 평가하여 구조화된 잠재 골격을 풀도록 설계된다. 주요 목표는 정보 흐름의 연속성을 보호하여 조밀한 정보 교환의 유지를 통해 모델의 정밀한 모션 생성 능력을 크게 강화하는 것이다.\n' +
      '\n' +
      '모션 맘바는 그림 1과 같이 정확도와 효율성 사이의 예외적인 균형을 이루는 모션 생성에 대한 새로운 접근법을 도입한다. 우리의 실험 결과는 모션 맘바가 가져온 상당한 개선을 강조하며, FID(Frechet Inception Distance)의 현저한 개선을 보여주며, HumanML3D 데이터 세트에서 이전의 최신 메트릭인 0.473에서 인상적인 0.281로 최대 50% 감소했다[17]. 또한, 기존 방법보다 4배 빠른 프레임워크의 비교할 수 없는 추론 속도를 강조하여 시퀀스당 MLD[6] 방법이 요구하는 0.217초에 비해 시퀀스당 평균 추론 시간이 0.058초에 불과하다는 것을 달성했다. 이러한 결과는 모션 맘바의 최첨단 성능을 명확하게 확립하여 조건부 인간 모션 생성 작업에 대한 빠른 추론 속도를 동시에 보장한다.\n' +
      '\n' +
      '동작 생성 분야에 대한 우리의 기여는 다음과 같이 요약될 수 있다. 본 논문에서는 선택적 스캐닝 메커니즘을 동작 생성 작업에 통합하는 선구적인 방법인 _Motion Mamba_라는 간단하면서도 효과적인 프레임워크를 소개한다.\n' +
      '2. _Motion Mamba_는 Hierarchical Temporal Mamba (HTM)와 Bidirectional Spatial Mamba (BSM)의 두 모듈로 구성되어 있으며, 이들은 각각 시간 및 공간 모델링을 위해 설계되었다. HTM 블록들은 프레임들에 걸쳐 모션 일관성을 향상시키는 것을 목표로 하여, 시간적 모션 데이터를 프로세싱하는 태스크를 갖는다. BSM 블록들은 잠재 포즈 표현들 내의 숨겨진 정보의 채널-방향 흐름을 양방향으로 캡처하도록 엔지니어링된다.\n' +
      '3. _Motion Mamba_ framework는 HumanML3D[17]와 KIT-ML[38] 데이터셋에 대한 실험 검증을 통해 텍스트-투-모션 생성 작업에서 우수한 성능을 보였다. 우리의 방법론은 최신 생성 품질을 달성하고 추론 속도를 최적화하는 동안 긴 제곱 모델링을 크게 개선했다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**인간 움직임 생성**인간 움직임 생성은 3D 모델링 및 로봇 조작과 같은 다양한 응용 분야에 필수적인 컴퓨터 비전의 기본 작업이다. 최근, _Text-to-Motion_ task로 알려진 인간의 동작 생성을 달성하는 주된 방법은 언어와 동작 모두에 대한 공통 잠재 공간을 학습하는 것을 포함한다.\n' +
      '\n' +
      'DVGAN[30]은 각 시간 스케일에서 조밀하게 검증하고 변환 불변성에 대한 판별기 입력을 교란하여 GAN[11] 판별기를 생성하여 움직임 생성 및 완성을 가능하게 한다. ERD-QV[20]은 보편적으로 적용되는 도달 시간 임베딩과 확장된 전이 동안 사용되는 가산 스케줄링된 타겟 잡음 벡터의 두 가지 가산 수식어를 통해 잠재 표현을 향상시킨다. 다른 시간 척도에서 작동하는 두 개의 판별기가 있는 GAN 프레임워크를 통합함으로써 전환 품질을 더욱 향상시킨다. 개선된 WGAN-GP[16]의 수정된 버전으로 훈련된 HP-GAN[4]는 인간의 움직임 예측을 위해 설계된 커스텀 손실 함수를 이용한다. 이전 포즈를 조건으로 한 미래 인간 포즈의 확률 밀도 함수를 학습한다.\n' +
      '\n' +
      '오토 인코더[27, 43]는 인간의 움직임 생성에 널리 채택되는 고차원 데이터를 잠재 공간으로 압축하여 데이터를 강력하게 표현하는 능력으로 알려져 있는 주목할 만한 생성 모델이다. JL2P[1]은 언어와 포즈의 결합된 표현을 학습하기 위해 RNN 기반 오토인코더[23]를 사용한다. 텍스트에서 모션으로의 직접 매핑을 일대일 관계로 제한한다. 모션CLIP[48]은 트랜스포머 기반 오토인코더[51]를 사용하여 CLIP[40] 공간에서 해당 텍스트 레이블과의 정렬을 보장하면서 모션을 재구성한다. 이 정렬은 CLIP의 의미 지식을 인간의 움직임 다양체에 효과적으로 통합한다. TEMOS[36] 및 T2M[17]은 트랜스포머 기반 VAE[26]를 텍스트 인코더와 결합하여 VAE 잠재 공간 내에서 작동하는 분포 파라미터를 생성한다. AttT2M[55]과 TM2D[10]은 표현성이 향상된 이산 잠재 공간의 학습을 향상시키기 위해 신체 부분 시공간 인코더를 VQ-VAE[50]에 통합한다.\n' +
      '\n' +
      '확산 모델[7, 22, 41, 47]은 최근 2D 이미지를 생성하는 데 있어 GAN 및 VAE를 능가했다. 확산 모델을 기반으로 한 모션 생성 모델을 개발하는 것은 분명히 매력적인 방향이다. MotionDiffuse[53]는 확산 모델을 기반으로 한 텍스트 기반 모션 생성을 위한 첫 번째 프레임워크를 소개한다. 확률적 매핑, 사실적 합성 및 다단계 조작을 포함한 몇 가지 바람직한 특성을 보여준다. MDM[49]은 각 확산 단계에서 잡음이 아닌 샘플을 예측하기 위해 인간 모션 도메인에 대해 분류기 없는 트랜스포머 기반 확산 모델을 이용한다. MLD[6]은 원시 모션 시퀀스들과 조건부 입력들 사이의 연결들을 확립하기 위해 확산 모델을 사용하는 것이 아니라 잠재 모션 공간에서 확산 프로세스를 수행한다.\n' +
      '\n' +
      '**State Space Models.** 최근, State Space Sequence Model(SSMs) [14, 15], 고전적인 State-space Model[25]에서 영감을 얻어 시퀀스 모델링을 위한 유망한 아키텍처로 부상하고 있다. Mamba[12]는 선택적 SSM 아키텍처를 도입하여 시변 파라미터를 SSM 프레임워크에 통합하고, 고효율의 훈련 및 추론 프로세스를 용이하게 하기 위한 하드웨어 인식 알고리즘을 제안한다. 일부 연구는 2D 데이터를 처리하기 위해 컴퓨터 비전에서 SSM을 활용합니다. 2D SSM[3]은 각 변압기 블록[8, 51]의 시작 부분에 SSM 블록을 도입한다. 이 접근법은 효율적인 매개변수화, 가속된 계산 및 적절한 정규화 방식을 달성하는 것을 목표로 한다. SGConvNeXt[29]는 매개변수화 효율과 효과적인 긴 시퀀스 모델링을 모두 달성하기 위해 다중 스케일 서브 커널을 통합하는 ConvNeXt[32]에서 영감을 얻은 구조화된 글로벌 컨볼루션 방법을 제시한다. ConvSSM[46]은 ConvLSTM[45]의 텐서 모델링 원리를 SSM과 통합하여 컨볼루션 재발에서 병렬 스캔의 활용을 설명한다. 이 접근법은 아이차 병렬화와 빠른 자기회귀 생성을 가능하게 한다. Vim[56]은 효율적이고 다재다능한 시각적 표현 학습을 위해 양방향 SSM 블록을 도입하여 확립된 ViT[8] 방법에 필적하는 성능을 달성한다. VMamba[31]은 공간 영역을 횡단하고 임의의 비인과적 시각적 이미지를 정렬된 패치 시퀀스로 변환하도록 설계된 크로스-스캔 모듈(CSM)을 도입한다. 이 접근법은 전역 수용 필드를 보존하면서 선형 복잡성을 달성한다. 고차원 데이터를 처리하기 위해 SSM을 활용하려는 시도도 있었다. Mamba-ND[28]은 Mamba[12]를 고차원 작업들에 적응시키기 위해 SSM 블록 내의 상이한 스캔 방향들과 SSM의 다양한 조합들을 탐색한다. 최근 영상 생성 효율을 높이기 위해 확산 디노이저(diffusion denoiser) 내의 전통적인 변압기 기반 U-Net을 SSM 블록으로 대체하려는 노력이 계속되고 있다. DiffuSSM[52]은 전역 압축에 의존하지 않고 더 높은 해상도를 능숙하게 관리하므로 확산 프로세스 전반에 걸쳐 상세한 이미지 표현을 유지한다.\n' +
      '\n' +
      '##3 제안된 방법\n' +
      '\n' +
      '이 섹션에서는 텍스트 설명으로부터 긴 범위에 걸쳐 인간의 움직임을 효율적으로 생성하기 위해 설계된 _Motion Mamba_ 프레임워크의 구조와 작동 원리를 설명한다. 먼저, 우리는 Mamba 모델[12]과 잠재 확산 모델[6]을 포함하여 우리의 접근법을 뒷받침하는 기초 개념에 대해 논의한다. 다음으로, 우리는 모션 생성 효율성을 향상시키기 위해 맘바 모델을 활용하는 독특하게 조작된 아키텍처를 자세히 설명합니다. 이 구조는 시간적인 측면을 다루는 Hierarchical Temporal Mamba (HTM) 블록과 공간 동학에 초점을 맞춘 Bidirectional Spatial Mamba (BSM) 블록의 두 가지 주요 구성 요소로 구성된다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**선택적 구조화된 상태 공간 시퀀스 모델.** SSM은 특히 구조화된 상태 공간 시퀀스 모델(S4) 및 맘바의 기여를 통해 긴 시퀀스를 다루는 데 탁월한 숙련도를 입증했다. 이 모델들은 1차원 함수 또는 수열, \\(x(t)\\in\\mathbbbb{R}\\mapsto y(t)\\in\\mathbbb{R}\\)의 매핑을 진화 매개변수로 \\(h(t)\\in\\mathbbbb{R}^{N}\\)을 사용하고, 진화 매개변수로 \\(\\mathbfbbbb{R}^{N}\\(\\mathbbbb{R}^{N}\\)과 프로젝션 매개변수로 \\(\\mathbbbb{R}^{C}\\in\\mathbb{R}^{1\\times N}\\)을 각각 사용한다. 연속 시스템 역학은 상미분 방정식(ODE)에 의해 설명된다:\n' +
      '\n' +
      '\\[h^{\\prime}(t)=\\mathbf{A}h(t)+\\mathbf{B}x(t), \\tag{1}\\] \\[y(t)=\\mathbf{C}h(t)+\\mathbf{D}x(t),\\\n' +
      '\n' +
      '는 연속 입력 신호를 나타내고 \\(x(t)\\)는 시간 영역에서 연속 출력 신호를 나타낸다.\n' +
      '\n' +
      '이러한 연속 역학을 실제 계산에 적용하기 위해 S4 모델과 Mamba 모델은 이산화 과정을 사용하며, 특히 영차 홀드(ZOH) 방법을 사용하여 연속 매개변수를 이산 매개변수로 변환한다.\n' +
      '\n' +
      '\\exp{(\\mathbf{\\Delta A}} =\\exp{(\\mathbf{\\Delta A}), \\tag{2}\\] \\[\\overline{\\mathbf{B}} =(\\mathbf{\\Delta A})^{-1}(\\exp{(\\mathbf{\\Delta A}})-\\mathbf{I})\\cdot\\mathbf{\\Delta B}.\\\n' +
      '\n' +
      '그 다음, 이산화된 시스템은 단계 크기\\(\\mathbf{\\Delta}\\)를 통합하여 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '\\[h_{t} =\\overline{\\mathbf{A}}h_{t-1}+\\overline{\\mathbf{B}}x_{t}, \\tag{3}\\]\\[y_{t} =\\mathbf{C}h_{t}.\\\n' +
      '\n' +
      '이러한 적응은 입력 시퀀스의 전체 길이\\(\\mathbf{x}\\)를 포함하는 구조화된 컨볼루션 커널\\(\\overline{\\mathbf{K}}\\)을 활용하여 전역 컨벌루션을 통한 출력 계산을 용이하게 한다:\n' +
      '\n' +
      '\\[\\overline{\\mathbf{K}} = (\\mathbf{C}\\overline{\\mathbf{B}},\\mathbf{C}\\overline{\\mathbf{A}\\mathbf{B}},\\dots,\\mathbf{C}\\overline{\\mathbf{A}}^{M-1}\\overline{\\mathbf{B}},\\tag{4}\\] \\[\\mathbf{y} =\\mathbf{x}*\\overline{\\mathbf{K}}}.\\dots,\\mathbf{C}\\overline{\\mathbf{A}}^{M-1}\\overline{\\mathbf{B}},\\tag{4}\\] \\[\\mathbf{y} =\\mathbf{x}*\\overline{\\mathbf{K}}}}.\\dots,\\mathbf{C}\\overline{\\mathbf{A}}}^{M-1}\\overline{\\mathbf{\n' +
      '\n' +
      '맘바와 같은 선택적 모델은 선형 시간 불변(LTI) 가정으로부터 벗어나고 병렬 계산을 복잡하게 하는 시변 매개변수를 도입한다. 그러나 이러한 계산 문제를 해결하기 위해 연관 스캔과 같은 하드웨어 인식 최적화가 개발되었으며 복잡한 시간 역학을 모델링하는 SSM의 지속적인 진화 및 적용을 강조한다.\n' +
      '\n' +
      '**잠재적 모션 확산 모델.** 확산 확률 모델은 T-길이 학습된 마르코프 과정[7, 22, 41, 44, 47, 49, 53]을 통해 가우시안 분포에서 목표 데이터 분포\\(p(x)\\)까지 점진적으로 잡음을 줄여 모션 생성에 상당한 진전을 제공한다. *\\(\\mathbf{x_{t}}\\}_{t=1}^{T}\\) 모션 생성에서 랜덤잡음을 모션 시퀀스(\\{\\hat{x}_{t}^{1:N}_{t=1}^{T}\\)에 어닐링하는 데노이저\\(\\epsilon_{\\theta}\\left(x_{t},t\\right)\\)로 학습 가능한 확산 모델을 정의한다. 확산 모델을 원시 모션 시퀀스에 직접 적용하는 비효율성을 해결하기 위해 확산 프로세스를 위해 저차원 모션 잠재 공간을 사용한다. 입력 조건 c가 주어지면, 서술형 문장 \\(\\mathbf{w}^{1:N}=\\{w^{i}_{i=1}^{N}\\), 미리 정의된 집합 \\(\\mathcal{A}\\)으로부터 액션 레이블 \\(a\\), 또는 빈 조건 \\(c=\\varnothing\\), 그리고 [17]에서 제안된 바와 같이 3D 관절 회전, 위치, 속도, 발 접촉을 결합한 모션 표현이 주어진다. 동결된 CLIP[40] 텍스트 인코더\\(\\tau_{\\theta}^{w}\\(\\tau_{\\theta}^{w}(w^{1:N})\\in\\mathbb{R}^{1\\times d}\\)을 사용하여 프로젝션된 텍스트 임베딩\\(\\epsilon_{\\theta}(z_{t},t,\\tau_{\\theta}(c))으로 구성된 조건부 디노이저를 얻었다. 잠재 확산 모델 \\(\\epsilon_{\\theta}\\left(x_{t},t\\right)\\)은 인간의 움직임 시퀀스를 \\(\\hat{x}^{1:L}=\\{\\hat{x}^{i}\\}_{i=1}^{L}\\)의 관점에서 생성하는 것을 목표로 하였으며, 여기서 L은 시퀀스 길이 또는 프레임 수[34, 35, 37, 54]를 나타낸다. MLD[6]에서 제안한 움직임 가변 자동 인코더(VAE)\\(\\mathcal{V}=\\{\\mathcal{E},\\mathcal{D}\\}\\}\\)를 재사용하여 잠재공간에서의 움직임 시퀀스를 조작하고, 중간표현을 움직임 시퀀스로 \\(\\hat{x}^{1:L})\\(\\hat{x}^{1:L}=\\mathcal{D}(z)=\\mathcal{DE}(x^{1:L})\\)으로 압축 해제하였다[26, 42, 51]. 마지막으로, 잠재공간에서 실제 잡음과 예측된 잡음 사이의 MSE 최소화에 초점을 맞춘 목표로 잠재확산모델을 학습하여 효율적이고 고품질의 움직임 생성을 용이하게 한다[2, 22].\n' +
      '\n' +
      '그림 2: 이 그림은 제안된 모션 맘바 모델의 아키텍처를 보여준다. 인코더와 디코더의 각 블록들은 SSM 계층 내에서 계층적 스캔과 양방향 스캔을 각각 갖는 Hierarchical Temporal Mamba 블록(HTM)과 BSM(Bidirectional Spatial Mamba) 블록으로 구성된다. 스캔의 이러한 대칭 분포는 인코더-디코더 아키텍처에 걸쳐 균형 잡힌 일관성 프레임워크를 보장한다.\n' +
      '\n' +
      '### Motion Mamba\n' +
      '\n' +
      '제안된_Motion Mamba_ framework의 구조는 그림 2에 예시되어 있다. Motion Mamba는 모션 프레임의 연속적이고 시간적인 시퀀스들을 모델링하는데 있어서 그 효과성으로 구별되는 디노이징 U-Net 구조를 사용한다. 이러한 효과는 맘바 모델의 고유한 장시퀀스 모델링 용량에 기인한다. 디노이저(\\(\\epsilon_{\\theta}\\)는 인코더(E_{1..N}\\)와 디코더(D_{1..N}\\)를 포함하는 \\(N\\) 블록으로 구성된다. 또한, 복잡한 시간역학을 포착할 수 있는 모델의 능력을 향상시키기 위해 트랜스포머 기반 어텐션 믹서 블록(M\\)을 사용하여 아키텍처를 향상시켰다.\n' +
      '\n' +
      '\\[\\epsilon_{\\theta}(x)\\in\\{E_{1...N},M,D_{1..N}\\}. \\tag{5}\\]\n' +
      '\n' +
      '인코더 블록들은 순차적으로 배열된 \\(E_{1..N}\\)으로 표현되고, 디코더 블록들은 효과적인 상향식 및 하향식 정보 흐름을 용이하게 하기 위해 역순으로 구성된 \\(D_{1..N}\\)으로 표현된다. 선택적 연산이 주의 기반 방법에 비해 계산 복잡도가 현저히 낮다는 점을 감안할 때, 우리는 더 높은 품질의 세대를 달성하기 위해 스캔 수를 증가시켰다. 동시에 모델의 매개변수와 효율성 사이의 균형을 유지하는 것이 필수적이다. 따라서, 우리 모델의 새로운 측면은 일련의 스캔 번호를 특징으로 하는 계층적 스캔 전략의 도입이며,\n' +
      '\n' +
      '\\[K=\\{S^{2N-1},S^{2(N-1)-1},\\ldots,S^{1}\\}. \\tag{6}\\]\n' +
      '\n' +
      '이 시퀀스는 복잡도의 내림차순으로 각 계층에 할당된 스캔 수를 지정합니다. 예를 들어, 최상위 인코더 계층인 \\(E_{1}\\)과 최하위 디코더 계층인 \\(D_{N}\\)에는 \\(S^{2N-1}\\) 스캔이 할당되어 가장 높은 스캔 복잡도를 나타낸다. 반대로, 가장 낮은 인코더 계층인 \\(E_{N}\\)과 가장 높은 디코더 계층인 \\(D_{1}\\)에는 가장 낮은 수준의 스캐닝 복잡도를 반영하는 \\(S^{1}\\) 스캔이 할당된다.\n' +
      '\n' +
      '[E_{i}(S)=\\begin{cases}S^{2N-1}&\\text{for }i=1\\\\S^{2(N-i)-1}&\\text{for }i=2,\\ldots,N-1\\\\S^{1}&\\text{for }i=N\\end{cases}\\tag{7}\\text{for }i=1\\\\S^{2(N-i)-1}&\\text{for }i=N\\end{cases}\\tag{7}\\text{for }i=\n' +
      '\n' +
      '[D_{j}(S)=\\begin{cases}S^{2N-1}&\\text{for }j=N\\\\S^{2(N-j)-1}&\\text{for }j=N-1,\\ldots,2\\\\S^{1}&\\text{for }j=1\\end{cases}\\tag{8}\\}\n' +
      '\n' +
      '이러한 계층적 스캐닝 접근법은 프로세싱 능력들이 인코더-디코더 아키텍처 전체에 걸쳐 고르게 분포되도록 보장하여, 시간적 시퀀스들의 상세하고 미묘한 분석을 용이하게 한다. 이러한 구조화된 프레임워크 내에서, 각 데노이저는 시간 정보를 효과적으로 처리하는 모델의 능력을 증가시키는 역할을 하는 전문화된 계층적 시간 암바(HTM) 블록을 장착한다. 또한, 제안된 모션맘바는 조건부 융합을 향상시키기 위해 전략적으로 통합된 \\(M\\)으로 표시된 주의 기반 믹서 블록을 통합한다.\n' +
      '\n' +
      '**Hierarchical Temporal Mamba (HTM)** 블록은 압축된 잠재 표현들을 \\(z\\)으로 표현하며, 그 차원들 \\((T,B,C)\\)은 알고리즘 1에 나타낸 절차이다. 여기서 \\(T\\)은 Variational AutoEncoder (VAE) 프레임워크에 명시된 바와 같이 시간적 차원을 의미한다. 초기에 입력 \\(z\\)은 선형 투영 레이어를 적용하여 차원 \\(E\\)으로 변환 표현 \\(x\\)과 \\(z\\)을 생성한다. 우리의 분석은 하위 레벨 특징 공간 내에서 증가된 움직임 밀도를 보여주었다. 그 결과, 네트워크의 다양한 깊이에서 실행되는 계층적 스캐닝 방법론을 개발하였다.\n' +
      '\n' +
      '이 접근법은 다양한 모션 밀도를 수용할 뿐만 아니라 계산 오버헤드를 상당히 감소시킨다. 이 단계는 계층적으로 구조화된 스캔 집합인 \\(K=\\{S^{2N_{n}-1},S^{2N_{n-1}-1},\\ldots,S^{1}\\}\\)을 사용하여 대응하는 일련의 메모리 행렬 \\(\\{A_{1},\\ldots,A_{k}\\}\\})과 함께 사용한다. 각각의 서브-SSM 스캔은 먼저 1-D 컨벌루션을 \\(x\\)에 적용하여 \\(x^{\\prime}_{o}\\)을 생성한다. \\ 그런 다음 (x^{\\prime}_{o}\\)을 선형 투영하여 \\(B_{o}\\), \\(C_{o}\\), \\(\\Delta_{o}\\)을 유도한다. 이 투영은 \\(B_{o}\\), \\(C_{o}\\)\\(\\Delta_{o}\\)을 사용하여 \\(\\overline{A}_{o}\\) 및 \\(\\overline{B}_{o}\\)의 변환을 각각 수행한다. SSM 스캔의 시퀀스를 실행한 후, 출력의 집합 \\(\\{SSM_{A_{1},x},SSM_{A_{2},x},\\ldots,SSM_{A_{k},x}\\})을 컴파일한다. 이 컬렉션은 HTM 블록의 최종 출력을 얻기 위해 선형 투영을 통해 후속적으로 집성된다.\n' +
      '\n' +
      '####3.3.3 양방향 공간맘바(BSM)\n' +
      '\n' +
      '블록은 차원 재배열과 양방향 스캐닝의 새로운 접근 방식을 통해 잠재 표현 학습을 향상시키는 데 중점을 두고 있으며, 그 과정은 알고리즘 2에 나와 있다. 초기 입력 차원을 \\((T,B,C)\\)에서 \\((C,B,T)\\)으로 변경하고, 시간적 차원과 채널 차원을 효과적으로 스와핑한다. 이 재배열 후, 이제 \\(z^{\\prime}\\)으로 표시된 입력은 정규화 후 선형 투영을 거쳐 치수 \\(x\\) 및 크기 \\(E\\)의 \\(z\\)이 된다. 이 과정은 잠재 채널 차원에 대한 양방향 스캐닝을 포함하며, 여기서 \\(\\mathbf{x}\\)은 1-D 컨벌루션을 수행하여 순방향과 역방향 모두에 대해 \\(\\mathbf{x}^{\\prime}_{o}\\)을 산출한다. 그리고 각 \\(\\mathbf{x}^{\\prime}_{o}\\)을 선형적으로 투영하여 \\(B_{o}\\), \\(C_{o}\\) 및 \\(\\Delta_{o}\\)을 얻으며, 이는 각각 \\(\\overline{A}_{o}\\) 및 \\(\\overline{B}_{o}\\)을 변환하는데 사용된다. 최종 출력 토큰 시퀀스인 \\(\\mathbf{z_{1}\\)는 \\(\\mathbf{z}\\)와 함께 순방향 \\(y_text{forward}\\) 및 역방향 \\(y_text{backward}\\) 출력을 게이팅 및 합산하여 계산된다. 이 컴포넌트는 순방향 및 역방향 관점들 양자 모두의 데이터를 분석함으로써 구조화된 잠재 골격을 디코딩하도록 조작된다. 주요 목적은 정보 흐름의 끊김 없는 연속성을 보장하여 정확한 움직임을 생성하는 모델의 능력을 실질적으로 향상시키는 것이다. 이는 모델의 성능에 중요한 밀집 정보 교환의 유지를 통해 달성된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '제안된 모션맘바를 다음과 같이 두 가지 두드러진 _Text-to-Motion_ 합성 벤치마크에서 평가한다:\n' +
      '\n' +
      '**HumanML3D.** HumanML3D[17] 데이터 세트는 AMASS[33] 및 HumanAct12[18] 데이터 세트에서 조달된 14,616개의 동작을 집계하며, 각 동작은 3개의 텍스트 설명을 동반하며 44,970개의 스크립트로 절정에 달한다. 이 데이터 세트는 운동, 춤, 곡예와 같은 광범위한 동작에 걸쳐 풍부한 모션 언어 말뭉치를 제공한다.\n' +
      '\n' +
      '**KIT-ML.** KIT-ML 데이터셋 [38]은 6,278개의 텍스트 설명과 쌍을 이루는 3,911개의 모션으로 구성되어 있으며, 평가를 위한 압축적이지만 효과적인 벤치마크 역할을 한다. 두 데이터세트 모두에 대해, 채택된 포즈 표현은 T2M[17]으로부터 도출되어, 평가에 걸쳐 모션 표현의 일관성을 보장한다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '우리는 실험 전반에 걸쳐 **생성 품질.** 모델에 의해 생성된 모션의 사실성과 다양성을 정량화하기 위해 프레쳇 인셉션 거리(FID) [21]을 구현하는 등 다음 측면에 대한 표준 평가 메트릭을 적용한다. 또한, 모션과 텍스트 사이의 거리를 측정하고 모션-텍스트 정렬을 평가하기 위해 다중 모드 거리(MM Dist)를 사용한다. **다양성.** 움직임으로부터 추출된 특징의 분산을 계산하는 움직임 다양성을 측정하기 위해 다양성 메트릭을 사용한다. 또한, 다중 모달리티(MModality)를 사용하여 동일한 텍스트 설명을 공유하는 생성된 모션 내의 다양성을 평가한다.\n' +
      '\n' +
      '### Comparative Studies\n' +
      '\n' +
      '본 논문에서는 HumanML3D[17]와 KIT-ML[38] 데이터 집합에 대한 최신 방법에 대해 평가한다. 우리는 모션맘바를 HTM 배치 전략 MM(\\(\\{S^{2N_{n}-1},\\dots,S^{1}\\})), BSM 양방향 블록 전략으로 훈련한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{R Precision \\(\\uparrow\\)} & \\multicolumn{2}{c}{FID\\(\\downarrow\\)} & \\multicolumn{2}{c}{MM Dist\\(\\downarrow\\) Diversity\\(\\rightarrow\\)MModality\\(\\uparrow\\)} \\\\ \\cline{2-7}  & Top 1 & Top 2 & Top 3 & & & \\\\ \\hline Seq2Seq [39] & \\(0.180^{\\pm.002}\\) & \\(0.300^{\\pm.002}\\) & \\(0.396^{\\pm.002}\\) & \\(11.75^{\\pm.035}\\) & \\(5.529^{\\pm.007}\\) & \\(6.223^{\\pm.061}\\) & - \\\\ LJ2P [1] & \\(0.246^{\\pm.001}\\) & \\(0.387^{\\pm.002}\\) & \\(0.486^{\\pm.002}\\) & \\(11.02^{\\pm.046}\\) & \\(5.296^{\\pm.008}\\) & \\(7.676^{\\pm.058}\\) & - \\\\ T2G [5] & \\(0.165^{\\pm.001}\\) & \\(0.267^{\\pm.002}\\) & \\(0.345^{\\pm.002}\\) & \\(7.664^{\\pm.030}\\) & \\(6.030^{\\pm.008}\\) & \\(6.409^{\\pm.071}\\) & - \\\\ Hier [9] & \\(0.301^{\\pm.002}\\) & \\(0.425^{\\pm.002}\\) & \\(0.552^{\\pm.004}\\) & \\(6.532^{\\pm.024}\\) & \\(5.012^{\\pm.018}\\) & \\(8.332^{\\pm.042}\\) & - \\\\ TEMOS [37] & \\(0.424^{\\pm.002}\\) & \\(0.612^{\\pm.002}\\) & \\(0.722^{\\pm.002}\\) & \\(3.734^{\\pm.028}\\) & \\(3.703^{\\pm.008}\\) & \\(8.973^{\\pm.01}\\) & \\(0.368^{\\pm.018}\\) \\\\ T2M [17] & \\(0.457^{\\pm.002}\\) & \\(0.639^{\\pm.003}\\) & \\(0.740^{\\pm.003}\\) & \\(1.067^{\\pm.002}\\) & \\(3.340^{\\pm.008}\\) & \\(9.188^{\\pm.002}\\) & \\(2.090^{\\pm.083}\\) \\\\ MDM [49] & \\(0.320^{\\pm.005}\\) & \\(0.498^{\\pm.004}\\) & \\(0.611^{\\pm.007}\\) & \\(0.544^{\\pm.044}\\) & \\(5.566^{\\pm.027}\\) & \\(\\mathbf{95.59^{\\pm.086}}\\) & \\(\\mathbf{2.799^{\\pm.072}}\\) \\\\ MotionDiffuse [53] & \\(0.491^{\\pm.001}\\) & \\(0.681^{\\pm.001}\\) & \\(0.782^{\\pm.001}\\) & \\(0.630^{\\pm.001}\\) & \\(3.113^{\\pm.001}\\) & \\(9.410^{\\pm.049}\\) & \\(1.553^{\\pm.042}\\) \\\\ MLD [6] & \\(0.481^{\\pm.003}\\) & \\(0.673^{\\pm.003}\\) & \\(0.772^{\\pm.002}\\) & \\(0.473^{\\pm.013}\\) & \\(3.196^{\\pm.010}\\) & \\(9.724^{\\pm.082}\\) & \\(\\underline{2.413}^{\\pm.079}\\) \\\\ \\hline\n' +
      '**Motion Mamba (Ours)** & \\(\\mathbf{0.502^{\\pm.003}}\\) & \\(\\mathbf{0.693^{\\pm.002}}\\) & \\(\\mathbf{0.792^{\\pm.002}}\\) & \\(\\mathbf{0.281^{\\pm.009}}\\) & \\(\\mathbf{3.060^{\\pm.038}}\\) & \\(9.871^{\\pm.084}\\) & \\(2.294^{\\pm.058}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: HumanML3D 상의 텍스트-조건부 모션 합성의 비교[17] 이러한 메트릭들은 [17]로부터 모션 인코더에 의해 평가된다. 빈 MModality는 다양한 생성 방법을 나타낸다. 우리는 실제 동작을 기준으로 사용하고 모든 방법을 내림차순 FID로 정렬한다. 오른쪽 화살표\\(\\rightarrow\\)는 실제 움직임에 가까울수록 좋다는 것을 의미한다. **굵은**와 밑줄은 가장 좋은 결과와 두 번째로 좋은 결과를 나타냅니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{R Precision \\(\\uparrow\\)} & \\multicolumn{2}{c}{FID\\(\\downarrow\\)} & \\multicolumn{2}{c}{MM Dist\\(\\downarrow\\) Diversity\\(\\rightarrow\\)MModality\\(\\uparrow\\)} \\\\ \\cline{2-7}  & Top 1 & Top 2 & Top 3 & & & \\\\ \\hline Real & \\(0.424^{\\pm.005}\\) & \\(0.649^{\\pm.006}\\) & \\(0.779^{\\pm.006}\\) & \\(0.031^{\\pm.004}\\) & \\(2.788^{\\pm.012}\\) & \\(11.08^{\\pm.007}\\) & - \\\\ \\hline Seq2Seq [39] & \\(0.103^{\\pm.003}\\) & \\(0.178^{\\pm.005}\\) & \\(0.241^{\\pm.006}\\) & \\(24.86^{\\pm.348}\\) & \\(7.960^{\\pm.031}\\) & \\(6.744^{\\pm.106}\\) & - \\\\ T2G [5] & \\(0.156^{\\pm.004}\\) & \\(0.255^{\\pm.004}\\) & \\(0.338^{\\pm.005}\\) & \\(12.12^{\\pm.183}\\) & \\(6.964^{\\pm.029}\\) & \\(9.334^{\\pm.079}\\) & - \\\\ LJ2P [1] & \\(0.221^{\\pm.005}\\) & \\(0.373^{\\pm.004}\\) & \\(0.483^{\\pm.005}\\) & \\(6.545^{\\pm.072}\\) & \\(5.147^{\\pm.030}\\) & \\(9.073^{\\pm.100}\\) & - \\\\ Hier [9] & \\(0.255^{\\pm.006}\\) & \\(0.432^{\\pm.007}\\) & \\(0.531^{\\pm.007}\\) & \\(5.203^{\\pm.107}\\) & \\(4.986^{\\pm.027}\\) & \\(9.563^{\\pm.072}\\) & \\(2.090^{\\pm.083}\\) \\\\latent dimension = 2 with 11 layers. We evaluate our model and previous works with suggested metrics in HumanML3D [17] and calculate 95% confidence interval by repeat evaluation 20 times. The results for the HumanML3D dataset are presented in Table 1. Our model outperforms other methods significantly across various evaluation metrics, including FID, R precision, multi-modal distance, and diversity. For instance, our Motion Mamba outperforms previous best diffusion based motion generation MLD by 40.5% in terms of FID, and up to 10% improvement on R Precision, we aslo obtained best MModality by 3.060. The results for the KIT-ML dataset are presented in Table 2. We have also outperformed other well-established methods in FID and multi-modal distance.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 표 4에서 제안된 모션 맘바의 긴 시퀀스 평가, HTM을 사용한 계층적 설계, BSM의 양방향 설계, 잠재 차원 수 및 레이어 수를 포함한 절제 연구를 결론지었다.\n' +
      '\n' +
      '**긴 시퀀스 모션 생성** HumanML3D [17] 데이터 세트는 그림 3과 같이 긴 시퀀스 인간 모션의 상당한 비율을 갖는 긴 꼬리 및 오른쪽 편향 분포를 나타낸다. 우리는 이전 연구에서 긴 시퀀스 생성 문제의 문제를 간과했음을 제안한다. 따라서, 우리는 원본 테스트 세트에서 추출된 190 프레임보다 긴 모션 시퀀스를 포함하는 새로운 데이터 세트 변형인 _HumanML3D-LS_를 소개한다. 이 추가를 통해 긴 시퀀스 모션을 생성하는 능력을 보여줄 수 있습니다. 그 후, HumanML3D-LS에서 제안한 방법의 성능을 평가하고 다른 확산 기반 움직임 생성 방법과 비교한다. 비교 결과는 표 3에 제시되어 있으며, 모션맘바는 장거리 의존성 모델링에 대한 이점을 활용하여 긴 시퀀스 모션 생성에 적합하도록 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{R Precision \\(\\uparrow\\)} & \\multirow{2}{*}{FID\\(\\downarrow\\)} & \\multirow{2}{*}{MM Dist\\(\\downarrow\\)} & \\multirow{2}{*}{Diversity\\(\\rightarrow\\) MModality\\(\\uparrow\\)} \\\\ \\cline{2-2} \\cline{5-8}  & Top 1 & & & & & & \\\\ \\hline Real & \\(0.437^{\\pm.003}\\) & \\(0.622^{\\pm.004}\\) & \\(0.721^{\\pm.004}\\) & \\(0.004^{\\pm.000}\\) & \\(3.343^{\\pm.015}\\) & \\(8.423^{\\pm.000}\\) & - \\\\ \\hline MDM [49] & \\(0.368^{\\pm.005}\\) & \\(0.553^{\\pm.006}\\) & \\(0.672^{\\pm.005}\\) & \\(0.802^{\\pm.004}\\) & \\(3.860^{\\pm.025}\\) & \\(8.817^{\\pm.008}\\) & - \\\\ MotionDiffuse [53] & \\(0.367^{\\pm.004}\\) & \\(0.521^{\\pm.004}\\) & \\(0.623^{\\pm.004}\\) & \\(2.460^{\\pm.002}\\) & \\(3.789^{\\pm.005}\\) & \\(\\mathbf{8.707^{\\pm.143}}\\) & \\(1.602^{\\pm.013}\\) \\\\ MLD [6] & \\(0.403^{\\pm.005}\\) & \\(0.584^{\\pm.005}\\) & \\(0.690^{\\pm.005}\\) & \\(0.952^{\\pm.020}\\) & \\(3.580^{\\pm.016}\\) & \\(9.050^{\\pm.085}\\) & \\(\\mathbf{2.711^{\\pm.104}}\\) \\\\ \\hline\n' +
      '**Motion Mamba (Ours)** & \\(\\mathbf{0.417^{\\pm.003}}\\) & \\(\\mathbf{0.606^{\\pm.003}}\\) & \\(\\mathbf{0.713^{\\pm.004}}\\) & \\(\\mathbf{0.668^{\\pm.019}}\\) & \\(\\mathbf{3.435^{\\pm.015}}\\) & \\(9.021^{\\pm.070}\\) & \\(\\underline{2.373^{\\pm.084}}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 긴 시퀀스 모션 생성에서 모델의 능력을 평가하기 위해, 우리는 최근 도입된 HumanML3D-LS 데이터 세트에 대한 기존 접근법과 우리의 방법을 비교했다. 이 데이터세트는 원래의 평가 세트로부터 190 프레임보다 긴 모션 시퀀스들을 포함한다. 우리의 모델은 다른 방법에 비해 우수한 성능을 보여준다.\n' +
      '\n' +
      '그림 3: 그림은 휴먼ML3D[17]의 긴 꼬리 분포를 보여주며, 이는 긴 시퀀스의 휴먼 모션의 상당한 비율을 갖는다.\n' +
      '\n' +
      'Hierarchical Design with HTM.** in our ablation studies, we observed a slight improvement of the scan order from lower to higher level, specifically transitioning of MM \\(\\{S^{1},\\dots,S^{N}\\}\\)에서 MM \\(\\{S^{N},\\dots,S^{1}\\}\\)으로 전환되었다. 이러한 향상은 하위 레벨 특징 공간 내에서 시간적 움직임 밀도의 증가와 상관 관계를 시사한다. 또한 최적의 결과를 얻기 위해 스캐닝 주파수를 배열하는 계층적 설계를 도입하여 MM(\\{S^{2N_{n}-1},\\dots,S^{1}\\}) 시퀀스를 생성한다. 이러한 검색 수의 확장은 성능 증가로 이어졌다. 이 개선은 특히 변압기 아키텍처에 널리 퍼져 있는 자체 주의 및 피드포워드 네트워크 블록의 매개변수 집약적 구성과 비교할 때 개별 선택적 스캔 작업이 매개변수 수를 크게 감소시킨다는 관찰에 기인한다.\n' +
      '\n' +
      '**BSM의 양방향 설계** 우리는 그들의 스캐닝 방향에 따라 구별되는 잠재 스캐닝 메커니즘의 세 가지 뚜렷한 변형을 개발했다. 동작 생성 태스크의 맥락에서, 우리는 구조화된 잠재 골격 내의 숨겨진 정보의 흐름이 이전에 미개척된 측면인 중요성을 갖는다고 가정한다. 우리의 절제 연구는 잠재 차원을 가로지르는 _단일 스캔_이 최소한의 개선을 산출한다는 것을 보여준다. 이후 레이어 기반 및 블록 기반 양방향 스캔을 모두 조사했다. 본 연구 결과는 블록 기반 양방향 스캔이 최적의 성능을 달성함을 나타낸다. 이는 공간 정보 흐름이 잠재 공간 내에 인코딩되고 양방향 스캐닝이 이 정보의 교환을 용이하게 하여 모션 생성 태스크의 효능을 향상시킨다는 것을 시사한다.\n' +
      '\n' +
      '**모션 맘바를 위한 아키텍처 설계.** 표준화된 모션 잠재 확산 시스템에 접지된 제안된 모션 맘바. 우리는 차원 측면과 모듈의 용량(메아) 사이의 상호 작용을 조사했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{R Precision} & \\multirow{2}{*}{FID\\(\\downarrow\\)} & \\multirow{2}{*}{MM Dist.\\(\\downarrow\\) Diversity\\(\\rightarrow\\) MModality\\(\\uparrow\\)} \\\\  & & Top \\(\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{ \\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{    }}}}}}}}}}}}}}\\) & & \\\\ \\hline Real & 0.797\\({}^{\\pm.002}\\) & 0.002\\({}^{\\pm.000}\\) & 2.974\\({}^{\\pm.008}\\) & 9.503\\({}^{\\pm.065}\\) & - \\\\ \\hline MM \\(\\{S^{1},\\dots,S^{N}\\}\\) & 0.673\\({}^{\\pm.003}\\) & 1.278\\({}^{\\pm.012}\\) & 3.802\\({}^{\\pm.041}\\) & 8.678\\({}^{\\pm.006}\\) & 3.127\\({}^{\\pm.024}\\) \\\\ MM \\(\\{S^{N},\\dots,S^{1}\\}\\) & 0.738\\({}^{\\pm.002}\\) & 0.962\\({}^{\\pm.011}\\) & 3.433\\({}^{\\pm.003}\\) & 9.180\\({}^{\\pm.071}\\) & 2.723\\({}^{\\pm.033}\\) \\\\ MM \\(\\{S^{1},\\dots,S^{2N_{n-1}}\\}\\) & 0.698\\({}^{\\pm.002}\\) & 0.856\\({}^{\\pm.008}\\) & 3.624\\({}^{\\pm.07}\\) & 9.229\\({}^{\\pm.067}\\) & 2.826\\({}^{\\pm.017}\\) \\\\\n' +
      '**MM** \\(\\{S^{2N_{n-1}},\\dots,S^{1}\\}\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ \\hline MM \\((SingleScan)\\) & 0.736\\({}^{\\pm.003}\\) & 1.063\\({}^{\\pm.010}\\) & 3.443\\({}^{\\pm.026}\\) & 9.180\\({}^{\\pm.067}\\) & 2.676\\({}^{\\pm.041}\\) \\\\ MM \\((BiScan,layer)\\) & 0.735\\({}^{\\pm.004}\\) & 0.789\\({}^{\\pm.007}\\) & 3.408\\({}^{\\pm.034}\\) & 9.374\\({}^{\\pm.059}\\) & 2.591\\({}^{\\pm.046}\\) \\\\\n' +
      '**MM** \\((BiScan,block)\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ \\hline MM \\((Dim,1)\\) & 0.706\\({}^{\\pm.003}\\) & 0.652\\({}^{\\pm.011}\\) & 3.541\\({}^{\\pm.072}\\) & 9.141\\({}^{\\pm.032}\\) & 2.612\\({}^{\\pm.055}\\) \\\\\n' +
      '**MM** \\((Dim,2)\\) & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ MM \\((Dim,5)\\) & 0.741\\({}^{\\pm.008}\\) & 0.728\\({}^{\\pm.009}\\) & 3.307\\({}^{\\pm.027}\\) & 9.427\\({}^{\\pm.009}\\) & 2.314\\({}^{\\pm.062}\\) \\\\ MM \\((Dim,7)\\) & 0.738\\({}^{\\pm.004}\\) & 0.599\\({}^{\\pm.007}\\) & 3.359\\({}^{\\pm.068}\\) & 9.166\\({}^{\\pm.075}\\) & 2.488\\({}^{\\pm.037}\\) \\\\ MM \\((Dim,10)\\) & 0.715\\({}^{\\pm.003}\\) & 0.628\\({}^{\\pm.008}\\) & 3.548\\({}^{\\pm.043}\\) & 9.200\\({}^{\\pm.075}\\) & 2.884\\({}^{\\pm.096}\\) \\\\ \\hline MM (9 layers) & 0.755\\({}^{\\pm.002}\\) & 1.080\\({}^{\\pm.012}\\) & 3.309\\({}^{\\pm.057}\\) & 9.721\\({}^{\\pm.081}\\) & 2.974\\({}^{\\pm.039}\\) \\\\\n' +
      '**MM** **(11 layers)** & 0.792\\({}^{\\pm.002}\\) & 0.281\\({}^{\\pm.009}\\) & 3.060\\({}^{\\pm.058}\\) & 9.871\\({}^{\\pm.084}\\) & 2.294\\({}^{\\pm.058}\\) \\\\ MM (27 layers) & 0.750\\({}^{\\pm.003}\\) & 0.975\\({}^{\\pm.008}\\) & 3.336\\({}^{\\pm.096}\\) & 9.249\\({}^{\\pm.071}\\) & 2.821\\({}^{\\pm.063}\\) \\\\ MM (37 layers) & 0.754\\({}^{\\pm.005}\\) & 0.809\\({}^{\\pm.010}\\) & 3.338\\({}^{\\pm.061}\\) & 9.355\\({}^{\\pm.062}\\) & 2.741\\({}^{\\pm.077}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: HumanML3D에 대한 텍스트 기반 모션 합성 평가 [17]: 표 1의 메트릭을 사용하고 실제 참조를 제공하며, 다양한 HTM 및 BSM 설계 선택, 잠재 입력의 차원, 모션 맘바 모델의 레이어 수를 평가한다.\n' +
      '\n' +
      ')가 시스템 성능에 미치는 영향을 확인하기 위해 계층 수에 따라 결정됩니다. 실험 결과, Motion Mamba는 잠재 차원 2에서 최적의 차원이 1로 식별된 이전 작업에서 분기되어 우수한 성능을 달성함을 알 수 있다. 이러한 불일치는 시퀀스 길이와 관련된 여러 스캔이 필요하기 때문에 차원성이 중추적인 요소임을 의미한다. 차원성의 한계 증가로 인해 최대 성능을 얻을 수 있었고, 동시에 10차원 모델에 비해 효율성을 향상시켰으며, 모션맘바의 선택적 스캐닝 메커니즘 설계에서 영감을 받아 최적의 레이어 수를 결정하기 위한 실험을 수행하였다. 특히, 단일 맘바 레이어는 종래의 트랜스포머 인코더 블록에 비해 약 75%의 파라미터 감소를 달성한다. 레이어 수를 증가시켜 모델 용량과 성능 간의 관계를 밝히는 것을 목표로 한다. 연구 결과는 특별히 설계된 HTM과 BSM(Bidirectional Scanning Module) 블록의 통합을 통해 모션맘바가 11개의 레이어로 최적의 성능을 달성한다는 것을 보여준다. 이것은 MLD [6] 기준선에 대한 약간의 증가를 나타낸다. 그러나, 각 계층에서 감소된 파라미터 카운트로 인해, 모션 맘바는 이전의 방법론들보다 상당히 큰 효율을 나타낸다.\n' +
      '\n' +
      '### Inference Time\n' +
      '\n' +
      '추론 시간은 확산 기반 방법에 대한 중요한 과제로 남아 있다. 이를 해결하기 위해, 효율적인 Mamba 블록을 경량 아키텍처 내에 통합함으로써 추론 속도를 향상시킨다. 평균 추론 시간 0.217초를 보고하는 [6]에서 인용된 MLD 모델과 같은 이전의 강한 베이스라인과 비교하여, 우리의 모션 맘바 모델은 그림 4와 같이 계산 오버헤드의 현저한 감소를 달성한다. 구체적으로, 4배 적은 계산 노력을 필요로 하여 더 빠르고 실시간 추론 속도를 용이하게 한다.\n' +
      '\n' +
      '##5 토론 및 결론\n' +
      '\n' +
      '본 연구에서는 효율적이고 확장된 시퀀스 모션 생성을 위해 설계된 새로운 프레임워크인 모션맘바를 소개한다. 우리의 접근 방식은 모션 생성 영역 내에서 Mamba 모델의 첫 번째 통합을 나타내며, 계층적 시간적 Mamba(HTM) 블록의 구현을 포함한 상당한 발전을 특징으로 한다. 이러한 블록들은 계층적으로 조직된 선택적 스캐닝을 통해 시간적 정렬을 향상시키도록 특별히 조작된다. 나아가, BSM(Bidirectional Spatial Mamba) 블록이 개발되어,\n' +
      '\n' +
      '그림 4: 그림은 문장당 평균 추론 시간(AIT) 대 FID를 보여주며, 제안된 모션 맘바는 0.058s AIT와 0.281 FID를 전체적으로 이전 방법을 능가한다. 우리는 단일 V100 GPU에서 모든 방법을 평가한다.\n' +
      '\n' +
      '잠재 공간 내에서 정보 흐름의 교환을 증폭하여 골격 수준 밀도 피쳐를 더 정확하게 양방향으로 캡처하는 모델의 능력을 향상시킵니다. 변압기 블록을 주로 사용하는 기존의 확산 기반 모션 생성 방법론과 비교하여, 모션 맘바 프레임워크는 SOTA 성능을 달성하여 FID 점수에서 최대 50%의 향상 및 추론 속도의 4배 향상을 입증한다. 다양한 인간 모션 생성 작업에 걸친 포괄적인 실험을 통해 제안된 모션 맘바 모델의 효과와 효율성이 강력하게 입증되어 인간 모션 생성 분야에서 상당한 도약을 나타낸다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Ahuja, C., Morency, L.P.: Language2pose: Natural language grounded pose forecasting. In: 2019 International Conference on 3D Vision (3DV). pp. 719-728. IEEE (2019)\n' +
      '* [2] Bao, F., Li, C., Cao, Y., Zhu, J.: All are worth words: a vit backbone for score-based diffusion models. arXiv preprint arXiv:2209.12152 (2022)\n' +
      '* [3] Baron, E., Zimerman, I., Wolf, L.: 2-d ssm: A general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635 (2023)\n' +
      '* [4] Barsoum, E., Kender, J., Liu, Z.: Hp-gan: Probabilistic 3d human motion prediction via gan. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 1418-1427 (2018)\n' +
      '* [5] Bhattacharya, U., Rewkowski, N., Banerjee, A., Guhan, P., Bera, A., Manocha, D.: Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents. In: 2021 IEEE virtual reality and 3D user interfaces (VR). pp. 1-10. IEEE (2021)\n' +
      '* [6] Chen, X., Jiang, B., Liu, W., Huang, Z., Fu, B., Chen, T., Yu, G.: Executing your commands via motion diffusion in latent space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18000-18010 (2023)\n' +
      '* [7] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems **34**, 8780-8794 (2021)\n' +
      '* [8] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2020)\n' +
      '* [9] Ghosh, A., Cheema, N., Oguz, C., Theobalt, C., Slusallek, P.: Synthesis of compositional animations from textual descriptions. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1396-1406 (2021)\n' +
      '* [10] Gong, K., Lian, D., Chang, H., Guo, C., Jiang, Z., Zuo, X., Mi, M.B., Wang, X.: Tm2d: Bimodality driven 3d dance generation via music-text integration. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9942-9952 (2023)\n' +
      '* [11] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems **27** (2014)\n' +
      '* [12] Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)\n' +
      '* [13] Gu, A., Goel, K., Gupta, A., Re, C.: On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems **35**, 35971-35983 (2022)\n' +
      '* [14] Gu, A., Goel, K., Re, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021)\n' +
      '* [15] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., Re, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems **34**, 572-585 (2021)\n' +
      '* [16] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein gans. Advances in neural information processing systems **30** (2017)\n' +
      '* [17] Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating diverse and natural 3d human motions from text. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5152-5161 (2022)* [18] Guo, C., Zuo, X., Wang, S., Zou, S., Sun, Q., Deng, A., Gong, M., Cheng, L.: Action2motion: Conditioned generation of 3d human motions. In: Proceedings of the 28th ACM International Conference on Multimedia. pp. 2021-2029 (2020)\n' +
      '* [19] Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems **35**, 22982-22994 (2022)\n' +
      '* [20] Harvey, F.G., Yurick, M., Nowrouzezahrai, D., Pal, C.: Robust motion in-betweening. ACM Transactions on Graphics (TOG) **39**(4), 60-1 (2020)\n' +
      '* [21] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems **30** (2017)\n' +
      '* [22] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems **33**, 6840-6851 (2020)\n' +
      '* [23] Hopfield, J.J.: Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences **79**(8), 2554-2558 (1982)\n' +
      '* [24] Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., Chen, T.: Motiongpt: Human motion as a foreign language. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [25] Kalman, R.E.: A New Approach to Linear Filtering and Prediction Problems. Journal of Basic Engineering **82**(1), 35-45 (1960)\n' +
      '* [26] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. stat **1050**, 1 (2014)\n' +
      '* [27] Kramer, M.A.: Nonlinear principal component analysis using autoassociative neural networks. AIChE journal **37**(2), 233-243 (1991)\n' +
      '* [28] Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multi-dimensional data. arXiv preprint arXiv:2402.05892 (2024)\n' +
      '* [29] Li, Y., Cai, T., Zhang, Y., Chen, D., Dey, D.: What makes convolutional models great on long sequence modeling? In: The Eleventh International Conference on Learning Representations (2022)\n' +
      '* [30] Lin, X., Amer, M.R.: Human motion modeling using dvgans. arXiv preprint arXiv:1804.10652 (2018)\n' +
      '* [31] Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)\n' +
      '* [32] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11976-11986 (2022)\n' +
      '* [33] Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of motion capture as surface shapes. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5442-5451 (2019)\n' +
      '* [34] Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A.A., Tzionas, D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single image. In: Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)\n' +
      '* [35] Petrovich, M., Black, M.J., Varol, G.: Action-conditioned 3D human motion synthesis with transformer VAE. In: International Conference on Computer Vision (ICCV) (2021)\n' +
      '* [36] Petrovich, M., Black, M.J., Varol, G.: Temos: Generating diverse human motions from textual descriptions. In: European Conference on Computer Vision. pp. 480-497. Springer (2022)* [37] Petrovich, M., Black, M.J., Varol, G.: TEMOS: Generating diverse human motions from textual descriptions. In: European Conference on Computer Vision (ECCV) (2022)\n' +
      '* [38] Plappert, M., Mandery, C., Asfour, T.: The kit motion-language dataset. Big data **4**(4), 236-252 (2016)\n' +
      '* [39] Plappert, M., Mandery, C., Asfour, T.: Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. Robotics and Autonomous Systems **109**, 13-26 (2018)\n' +
      '* [40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [41] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [42] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234-241. Springer (2015)\n' +
      '* [43] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning internal representations by error propagation. Parallel Distributed Processing pp. 318-362 (1986)\n' +
      '* [44] Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022)\n' +
      '* [45] Shi, X., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems **28** (2015)\n' +
      '* [46] Smith, J., De Mello, S., Kautz, J., Linderman, S., Byeon, W.: Convolutional state space models for long-range spatiotemporal modeling. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [47] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. pp. 2256-2265. PMLR (2015)\n' +
      '* [48] Tevet, G., Gordon, B., Hertz, A., Bermano, A.H., Cohen-Or, D.: Motionclip: Exposing human motion generation to clip space. In: European Conference on Computer Vision. pp. 358-374. Springer (2022)\n' +
      '* [49] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-or, D., Bermano, A.H.: Human motion diffusion model. In: The Eleventh International Conference on Learning Representations (2022)\n' +
      '* [50] Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning. Advances in neural information processing systems **30** (2017)\n' +
      '* [51] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [52] Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without attention. arXiv preprint arXiv:2311.18257 (2023)\n' +
      '* [53] Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)\n' +
      '* [54] Zhang, Y., Black, M.J., Tang, S.: We are more than our joints: Predicting how 3d bodies move. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3372-3382 (2021)* [55] Zhong, C., Hu, L., Zhang, Z., Xia, S.: Attt2m: Text-driven human motion generation with multi-perspective attention mechanism. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 509-519 (2023)\n' +
      '* [56] Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>