<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#비전 슈퍼얼라인먼트: 약대강 일반화\n' +
      '\n' +
      '비젼 기초 모델\n' +
      '\n' +
      ' 지안위안궈\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      '호주 시드니, 시드니대학교 공학부 컴퓨터과학부 대응: Yunhe Wang\\(<\\)yunhe.wang@huawei.com\\(>\\)\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      '중국 북경 화웨이 노아의 방주연구소\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      '동등한 기여도\\({}^{2}\\)컴퓨터과학부는 바둑 게임에서 가장 강한 인간 선수들의 기량을 훨씬 능가하는 능력을 발휘했다. 유사하게, GPT-유사 모델(Brown et al., 2020)은 다양한 시험에서 평균 인간 수준을 넘어서는 결과를 달성할 수 있다. 특히, 이러한 현상은 컴퓨터 비전 분야에서 더 일찍 나타났다. 이르면 2015년, 세심하게 설계된 합성곱 신경망(He et al., 2015)은 이미 인간의 성능을 능가하는 ImageNet과 같은 대규모 시각적 분류 작업에 대한 성과를 달성하고 있었다. 초인적인 업적의 이러한 경향은 연구 커뮤니티가 이러한 예외적으로 능력 있는 모델을 통제, 평가 및 최적화하는 방법에 초점을 맞추도록 하여 인공 지능에 대한 이해와 적용을 발전시키기 위해 보유하고 있는 엄청난 잠재력을 인식하게 했다.\n' +
      '\n' +
      '초인간 AI 모델을 감독하는 데 있어 인간의 전문 지식을 활용하는 복잡한 문제를 해결하기 위해 "초정렬" 개념이 도입되었다. 이 접근법은 인간 입력으로부터 그들의 학습을 최대화하는 방식으로 초인간 모델을 정렬하는 것을 목표로 한다. 이 분야의 중요한 작업은 Weak-to-Strong Generalization(WSG)의 개념이다(Burns et al., 2023). 이 연구는 더 약한 모델을 사용하여 더 강한 모델을 감독할 수 있는 가능성을 탐색하기 위한 흥미로운 유추를 제시한다. 이 조사 라인의 결과는 자극적이다: 이미 강력한 일반화 및 표현 능력이 부여된 강력한 모델은 이러한 약한 모델이 제공하는 간단한 감독을 통해 약한 상대방을 능가하는 성능을 달성할 수 있다. 이러한 감독에는 종종 불완전하거나 결함이 있는 레이블이 포함되지만 더 강력한 모델은 이러한 한계를 효과적으로 초월할 수 있다. 이러한 발견은 약대강 일반화의 실행 가능성을 긍정했을 뿐만 아니라 자연어 처리 및 강화 학습과 같은 분야에서 그 효능을 입증했다.\n' +
      '\n' +
      '본 논문에서는 비전 기반 모델의 맥락에서 특히 약대강 일반화(wak-to-Strong Generalization, WSG)의 적용 가능성을 조사하는 "비전 슈퍼 얼라인먼트" 주제를 탐구한다. 본 연구는 소수의 샷 학습, 전이 학습, 시끄러운 레이블 학습 및 전통 지식 증류 설정을 포함한 컴퓨터 비전의 여러 시나리오를 세심하게 설계하고 검토한다. 이러한 시나리오에서, 더 강한 모델은 더 약한 모델로부터 학습하도록 훈련된다. 상세한 검증과 비교 실험을 통해 시각적 영역에서 WSG의 타당성을 입증한다. 또한, WSG의 효능을 향상시키기 위해 개선되고 적응적인 신뢰 체계를 소개한다. 우리의 연구는 시각에서 WSG의 개념을 검증할 뿐만 아니라 다양한 AI 양식에 걸쳐 초정렬의 광범위한 추구에 크게 기여한다. 우리의 작업은 인간 수준의 전문 지식과 초인간 AI 능력 간의 상호 작용을 이해하고 최적화하는 데 실질적인 진전을 나타내며 잠재적으로 인공 지능의 획기적인 발전을 위한 길을 열어준다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '컴퓨터 비전에서 심층 신경망의 성능을 향상시키는 추구는 교사-학생 학습 패러다임의 발전으로 이어졌다(Hinton et al., 2015). 이 접근법은 일반적으로 더 약한 모델(학생)의 성능을 개선하는 더 강한 모델(교사)을 포함하며, 더 약한 모델의 능력을 최적화하는 데 초점을 맞춘 광범위한 연구가 있다. 이를 달성하기 위한 다양한 전략들이 제시되고 있다. 예를 들어, (Romero et al., 2014)는 출력 로짓 외에도 수퍼비전을 위한 중간 계층 특징을 통합하면 학생의 학습을 크게 향상시킬 수 있다고 제안한다. (Park et al., 2019)는 표본 간의 관계가 가치 있는 감독 정보로 작용할 수 있다고 가정한다.\n' +
      '\n' +
      '이 접근법의 추가 정제에서, (Zhao et al., 2022)는 고전 지식 증류(KD) 손실을 재정의하여, 이 두 유형의 정보의 전달의 균형을 보다 효과적으로 맞추기 위해 이를 표적-클래스 및 비-표적-클래스 증류로 분할한다. (허 등, 2019)는 특징 지식의 전달을 위한 개선된 방법에 도달하여 특징 증류의 세부 사항과 구성 요소를 자세히 설명한다. 한편, (Chen et al., 2021)은 종래의 동단계 특징 전달에 대한 대안으로 크로스-스테이지 특징 전달을 탐색한다. 이러한 방법은 강력하고 약한 일반화 시나리오에 효과적인 것으로 입증되었다.\n' +
      '\n' +
      '그러나 비전 기반 모델의 크기와 복잡성이 점차 증가함에 따라 약-강 일반화, 즉 약 모델이 강 모델을 개선할 수 있는 방식으로 초점이 이동했다. 이러한 맥락에서 (Furlanello et al., 2018)은 동일한 크기의 교사와 학생 간의 지식 증류를 조사하여 동일한 크기의 증류 모델의 실현 가능성을 보여준다. 이를 기반으로 (Xie et al., 2020)은 동일한 크기의 모델 중 지식 증류를 위해 추가 레이블이 지정되지 않은 데이터의 사용을 도입하여 특히 데이터 가용성이 풍부한 시나리오에서 강력 대 강력 일반화의 효과를 추가로 검증한다. 이 연구의 본체는 비전 기반 모델 분야에서 비교적 미지의 유망 영역인 약대강 일반화에 대한 우리의 탐구의 장을 마련한다.\n' +
      '\n' +
      '## 3 Vision Superalignment\n' +
      '\n' +
      '초인간 컴퓨터 비전 모델을 감독하고 최적화하는 방법을 조사하기 위해 우리는 비전 기반 모델에 대한 약대강 일반화 연구에 중점을 둔다. 이 섹션에서는 처음에 비전 기반 모델을 구성하는 요소를 조사하고 정의하는 것을 조사한다. 광범위한 용량과 다용도로 특징지어지는 이 모델들은 우리 연구의 중추 역할을 한다. 그 후, 우리는 약한 모델로부터의 출력 라벨의 부정확성과 같은 약한 대 강한 일반화 접근법에 내재된 중요한 문제를 다룬다. 이에 대응하여 혁신적인 솔루션인 적응형 신뢰 증류를 소개합니다. 이 방법은 약한 모델이 제공하는 안내를 효과적으로 활용하여 강한 모델의 학습 과정을 향상시키기 위해 고안되었다. 적응적 신뢰 증류는 정보가 올바르게 처리되고 적응된다면 약한 모델로부터의 불완전하거나 부분적으로 정확한 안내조차도 더 강한 모델에 대한 귀중한 학습 도구가 될 수 있다는 원리로 작동한다.\n' +
      '\n' +
      '비젼 기반 모델\n' +
      '\n' +
      '비전 기반 모델에 대한 약대강 일반화에 대한 우리의 탐색에서 먼저 이러한 모델을 구성하는 것을 정의하는 것이 중요하다. 비전 기반 모델을 나타내는 후보에는 몇 가지 잠재적인 범주가 있으며, 각각은 컴퓨터 비전 영역에서 고유한 능력과 접근 방식을 특징으로 한다.\n' +
      '\n' +
      '**Text-Visual Fusion Models:** 첫 번째 카테고리는 시각과 언어 작업을 통합하는 모델을 포함한다. 주목할 만한 예는 이미지-텍스트 사전 트레이닝 쌍을 통해 컴퓨터 비전 태스크를 언어 태스크와 정렬하여 기초 모델을 구축하는 Radford et al.(Radford et al., 2021)의 작업이다. 이 접근법은 텍스트 정보와 시각적 정보 사이의 격차를 해소하여 두 도메인에 대한 포괄적인 이해를 제공한다.\n' +
      '\n' +
      '**이미지 생성 모델:** 두 번째 카테고리는 이미지를 생성할 수 있는 모델에 초점을 맞추고 있으며, 이는 이미지 공간을 모델링하는 형태라고 볼 수 있다. Rombach et al.(Rombach et al., 2022)은 텍스트 설명으로부터 많은 이미지들을 생성하는 그들의 능력을 통해 이를 입증하여, 이미지 생성 모델들의 기초를 확립한다. 유사하게, Chen et al.(Chen et al., 2020)은 이미지에 대해 GPT-유사 사전-트레이닝 방법을 채용하여, 이미지 생성에서 상당한 일반화 능력을 갖는 생성 트랜스포머 모델을 생성한다.\n' +
      '\n' +
      '**일반 또는 제로 샷 비주얼 태스크를 위한 아키텍처:** 세 번째 범주는 일반적으로 또는 제로 샷 방식으로 다양한 비주얼 태스크를 해결할 수 있는 아키텍처를 개발하고자 한다. Bai et al.(Bai et al., 2023)은 일련의 이미지 태스크들을 순차적인 도전들로서 모델링함으로써, 시각적 문제들의 스펙트럼을 다루는 큰 비전 모델을 생성함으로써 이에 접근한다. 추가적으로, Kirillov et al.(Kirillov et al., 2023)은 "Segment Anything" 모델을 제안하여, 인상적인 제로-샷 분할 결과들을 달성한다.\n' +
      '\n' +
      '약대강 생성 작업에서 사용할 시각 기반 모델에 가장 적합한 후보를 식별하기 위해 시각 영역에서 범용성과 효율성에 초점을 맞춘 정의를 제안한다. 우리는 비전 기반 모델이 고품질 성능을 제공하면서 광범위한 시각적 작업에 적용되어야 한다고 가정한다.\n' +
      '\n' +
      '이 기준을 바탕으로 이미지넷에서 사전 훈련된 백본들이 비전 기반 모델로서 강력한 경쟁자들을 나타내는 것을 제안한다. 이 선택의 근거는 이중이다. 첫째, 이러한 백본은 분류, 탐지 및 분할과 같은 컴퓨터 비전의 핵심 작업에 매우 적응성이 높고 효과적인 것으로 입증되었다. 이러한 백본을 미세 조정함으로써 이러한 작업에서 최첨단(SOTA) 정확도를 달성할 수 있어 견고성과 범용성을 입증할 수 있다. 둘째, 이러한 모델(He et al., 2022)을 위해 특별히 개발된 광범위한 사전 훈련 알고리즘이 있으며, 이는 비전 작업에 대한 보편적인 사전 훈련 모델로 추가로 자격을 부여한다. 또한, 이러한 유형의 모델은 비전 언어 멀티모달 모델(Du et al., 2022)의 분기 중 하나로 자주 사용되며, 교차 모달 작업에서의 적용 가능성을 강조한다.\n' +
      '\n' +
      '따라서 실험 분석을 위해 비전 기반 모델의 대표로서 이러한 백본 모델에 초점을 맞추기로 선택한다. 우리는 이미지 분류의 기본 과제를 기준으로 약대강 일반화 분석을 수행하는 것을 목표로 한다. 이 접근법을 통해 통제되지만 포괄적인 방식으로 약세 세대의 능력과 잠재력을 철저히 평가하여 다른 보다 복잡한 비전 작업에 외삽할 수 있는 통찰력을 제공할 수 있다.\n' +
      '\n' +
      '적응적 신뢰증류\n' +
      '\n' +
      '이 하위 섹션에서는 비전 기반 모델에서 약대강 일반화를 구현하기 위한 방법론을 탐구한다. 우리가 다루는 핵심 질문은 약한 비전 기반 모델이 어떻게 더 강한 상대방을 효과적으로 감독할 수 있는가이다. (Burns et al., 2023)는 증강된 신뢰도 손실 접근법을 제안하며, 이는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[L_{\\text{conf}}(f)=(1-\\alpha)\\text{CE}(f(x),f_{w}(x))+\\alpha\\text{CE}(f(x), \\hat{f}(x)), \\tag{1}\\\n' +
      '\n' +
      '여기서 \\(f\\)는 최적화되어야 하는 강한 모델을 나타내고, \\(f_{w}\\)는 약한 모델을 나타내며, \\(\\hat{f}(x)\\)는 입력 이미지에 대해 강한 모델에 의해 예측된 하드 라벨을 나타낸다. 손실 함수는 교차 엔트로피 손실(CE)을 포함하고 하이퍼파라미터 \\(\\alpha\\)에 의해 균형을 이룬다. 이 공식에서 손실 함수의 첫 번째 항은 전통적인 지식 증류 손실과 유사하여 약한 모델로부터 강한 모델의 학습 과정을 나타낸다. 약한 모델에 의해 제공된 라벨이 항상 정확하지는 않을 수 있다는 점을 감안할 때 손실 함수의 두 번째 항은 강한 모델이 우수한 일반화 능력과 사전 지식을 활용하여 예측을 개선하도록 권장한다.\n' +
      '\n' +
      '이 접근법의 강점은 약한 모델로부터의 직접 학습과 시각적 데이터를 이해하고 해석하는 강한 모델의 내재적 능력의 균형을 맞추는 능력에 있다. 이 방법은 강력한 모델이 약한 모델의 한계를 능가할 수 있는 길을 열어주면서 후자의 지침을 활용하면서 동시에 고급 기능을 통해 예측을 향상시킨다.\n' +
      '\n' +
      '약한 모델이 제공하는 감독에 내재된 한계와 강한 모델의 자체 생성 하드 라벨의 부정확성을 해결하려면 이러한 라벨의 단순한 가중 조합을 넘어 보다 정교한 접근이 필요하다. 각 라벨의 정확도를 직접 식별하는 문제를 감안할 때 가장 가능성 있는 올바른 라벨을 선택하기 위한 메트릭으로 신뢰를 활용하는 것이 실행 가능한 솔루션으로 등장한다.\n' +
      '\n' +
      '우리는 소프트 라벨과 하드 라벨 간의 불일치를 모델의 신뢰도의 지표로 사용할 것을 제안한다. 기본적인 근거는 모델의 부드러운 라벨이 단단한 라벨과 밀접하게 정렬될 때 자체 판단에 대한 더 높은 신뢰를 제안한다는 것이다. 이 통찰력을 활용하기 위해 모델의 신뢰 수준에 따라 동적으로 조정하는 적응형 신뢰 손실을 도입한다. 이러한 손실의 구체적인 제형은 다음과 같다:\n' +
      '\n' +
      '\\[L_{\\text{AC}}(f)=(1-\\beta(x))\\text{CE}(f(x), f_{w}(x))+\\beta(x)), \\tag{2}\\] \\beta(x)=\\frac{\\text{exp}(\\text{CE}(f(x),\\hat{f}(x))}{\\text{exp}(\\text{CE}(f(x),\\hat{f}(x)))+\\text{exp}(\\text{CE}(f(x),\\hat{f}_{w}(x))}.\\]\n' +
      '\n' +
      '이 식에서 \\(\\beta(x)\\)는 신뢰 가중치를 계산하는 입력 이미지 \\(x\\)의 함수이고 \\(\\hat{f}_{w}(x)\\)는 약한 모델에서 \\(x\\)의 하드 레이블이다. 이 가중치는 약한 모델로부터의 학습과 강한 모델 자신의 예측에 의존하는 것 사이의 균형을 결정한다. 교차 엔트로피 손실(CE)은 두 구성 요소에 모두 사용되며, 첫 번째 용어는 약한 모델로부터의 학습에 초점을 맞추고 두 번째 용어는 강한 모델의 자기 감독을 강조한다.\n' +
      '\n' +
      '이러한 적응적 신뢰 손실은 약한 일반화에 대한 보다 미묘한 접근을 가능하게 한다. 신뢰 수준에 기초하여 가중치를 조정함으로써, 강한 모델은 약한 모델의 안내보다 언제 자신의 예측을 우선시할지를 식별할 수 있게 하고 그 반대의 경우도 마찬가지이다. 이러한 적응성은 두 모델의 부정확성과 한계를 극복하는 데 핵심적이며, 비전 기반 모델에서 보다 효과적인 학습과 향상된 성능으로 이어진다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '본 절에서는 기준선 및 유망 방법 등 다양한 과제에 대한 주요 실증 결과를 보고한다. 모든 구현 세부 정보는 보충 자료에 첨부되어 있습니다.\n' +
      '\n' +
      '### Tasks\n' +
      '\n' +
      '**이미지 분류.** 우리의 실험은 주로 두 개의 벤치마크 데이터 세트에 초점을 맞추고 있다. CIFAR-100 (Krizhevsky et al., 2009)은 100개의 카테고리에 걸쳐 32\\(\\times\\)32개의 픽셀 이미지를 포함하는 이미지 분류를 위해 널리 인식되는 데이터세트이며, 각각 50,000개 및 10,000개의 이미지를 포함하는 트레이닝 및 검증 세트가 있다. 반대로 ImageNet(Deng et al., 2009)은 1,000개의 클래스에 걸쳐 128만 개의 훈련 이미지와 50,000개의 검증 이미지를 포함하는 분류 작업을 위한 대규모 데이터세트이다. 또한 약한 교사 모델에 의해 생성된 부드러운 레이블만 훈련에 사용할 수 있는 시나리오를 탐색한다.\n' +
      '\n' +
      '**Few-shot learning.** ILSVRC-2012(Russakovsky et al., 2015)로부터 샘플링된 100개의 클래스를 포함하는 miniImageNet(Vinyals et al., 2016) 데이터셋에 걸친 few-shot learning을 탐색한다. 데이터 세트를 훈련, 검증 및 테스트 세트로 각각 64개, 16개 및 20개 클래스로 무작위로 분할했다. 그리고 각 클래스에 84\\(\\times\\)84 이미지 크기의 600개의 이미지가 있는지 확인합니다. 우리는 ResNet36을 활용하여 소수의 작업에서 약한 일반화 성능을 탐구한다. 약대강 일반화 성능을 입증하기 위해 Meta-Baseline을 따르고 분류기 단계 및 메타 단계에 대한 관련 실험을 수행한다.\n' +
      '\n' +
      '**Transfer learning.** 우리는 ImageNet(Deng et al., 2009) 및 iNaturalist 2018(Van Horn et al., 2018)의 두 벤치마크 데이터 세트에 걸친 전이 학습을 탐색하며, 후자는 8,142종에 걸쳐 분포된 437,513개의 훈련 이미지 및 24,426개의 테스트 이미지를 포함한다. 우리는 자체 감독 MAE(He et al., 2022b)를 사용하여 ImageNet 훈련 세트에서 사전 훈련된 ViT-B(Dosovitskiy et al., 2020)를 활용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c} \\hline \\hline \\multirow{2}{*}{Teacher} & ResNet20 & ResNet32 & ResNet8\\(\\times\\)4 & WRN-16-2 & WRN-40-1 & VGG8 \\\\  & 68.93 & 71.72 & 72.41 & 72.71 & 72.30 & 71.99 \\\\ \\multirow{2}{*}{Student} & ResNet56 & ResNet110 & ResNet32\\(\\times\\)4 & WRN-40-2 & WRN-40-2 & VGG13 \\\\  & 72.94 & 74.80 & 79.90 & 77.20 & 77.20 & 75.26 \\\\ \\hline KD (Hinton et al., 2015) & 73.81 & 76.45 & 79.32 & 78.25 & 77.97 & 76.41 \\\\ FitNet (Romero et al., 2014) & 70.51 & 73.15 & 77.65 & 76.71 & 76.12 & 76.39 \\\\ RKD (Park et al., 2019) & 72.98 & 75.62 & 80.10 & 77.27 & 77.76 & 76.20 \\\\ ReviewKD (Chen et al., 2021a) & 70.15 & 72.30 & 77.22 & 75.86 & 75.78 & 74.22 \\\\ DKD (Zhao et al., 2022) & 73.90 & 76.57 & 79.52 & 78.18 & 77.95 & 76.62 \\\\ AugConf (Burns et al., 2023) & 73.86 & 76.72 & 80.34 & 78.34 & 78.15 & 76.55 \\\\ \\hline AdaptConf (**Ours**) & **74.17** & **76.86** & **80.64** & **78.58** & **78.40** & **76.84** \\\\ \\(\\Delta\\) & +1.23 & +2.06 & +0.74 & +1.38 & +1.20 & +1.58 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: CIFAR-100 검증 세트에 대한 **결과. 교사들과 학생들은 같은 건축물에 있다. 그리고 \\(\\Delta\\)는 처음부터 훈련된 학생 모델에 비해 성능 향상을 나타낸다. 모든 결과는 3회 이상의 평균이다.** 접근, 레이블 없이 이미지 데이터만 활용. 우리의 결과는 각 벤치마크에서 약한 교사 모델의 지침에 따라 수행되는 미세 조정 단계에 대해 보고된다. 또한, 약한 교사 모델에 의해 생성된 소프트 라벨만이 훈련에 사용되는 시나리오를 조사한다.\n' +
      '\n' +
      '**잡음 레이블로 학습.** 모의 레이블 노이즈가 있는 두 데이터 세트, 구체적으로 CIFAR-10(Krizhevsky et al., 2009) 및 CIFAR-100(Krizhevsky et al., 2009)을 사용하여 우리의 접근법을 평가한다. 선행 연구(Li et al., 2020; Tanaka et al., 2018)와 일관되게, 우리는 대칭 및 비대칭의 두 가지 별개의 유형의 모의 잡음 라벨을 소개한다. 대칭 노이즈는 훈련 데이터의 일정 비율의 레이블을 다른 가능한 레이블에 균일하게 무작위로 대입하여 도입한다. 대조적으로, 비대칭 노이즈는 레이블을 밀접하게 관련된 클래스로 뒤집는 것과 같은 실제 오류를 모방하기 위해 체계적인 잘못된 레이블링을 포함한다. 예를 들어, CIFAR-10에서 _truck_는 _automobile_, _bird_는 _airplane_로 오라벨링되고, _cat_는 _dog_와 상호 교환된다. CIFAR-100의 경우, 유사한 오라벨링이 순환 방식으로 각 슈퍼 클래스 내에서 적용된다.\n' +
      '\n' +
      '**기준 방법.** 교사-학생 훈련 패러다임을 구현하기 위한 주된 프레임워크는 지식 증류(Hinton et al., 2015)이다. 이 접근법은 더 크고 복잡한 교사 네트워크가 더 컴팩트한 학생 네트워크의 훈련을 안내하는 방법을 설명한다. 그럼에도 불구하고, 번즈_et al._(번즈 et al., 2023)의 결과에 영감을 받아, 우리의 작업은 학생 네트워크가 시각적 능력에서 선생님을 능가하는 시나리오로 선회한다. 이러한 역할의 반전에도 불구하고 교사 네트워크에는 로짓이나 중간 표상 기능을 통해 학생에게 전달될 수 있는 귀중한 암흑 지식이 남아 있다. 우리의 실험을 벤치마킹하기 위해, 확립된 범위(Hinton et al., 2015; Romero et al., 2014; Park et al., 2019; Heo et al., 2019; Chen et al., 2021; Hao et al., 2023)와 최근 제안된(Zhao et al., 2022; Burns et al., 2023) 증류 기술을 기준 방법으로 사용한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '####4.2.1 이미지 분류.\n' +
      '\n' +
      '**CIFAR-100 이미지 분류.** CIFAR-100 데이터 세트에 대한 약한 대 강한 일반화(WSG)의 탐색으로 조사를 시작한다. 이 연구의 결과는 표 1과 표 2에 설명되어 있으며, 구체적으로 표 1은 교사와 학생 모델이 동일한 네트워크 아키텍처를 공유하는 시나리오를 보여준다. 우리는 ResNet (He et al., 2016), WRN (Zagoruyko & Komodakis, 2016), VGG (Simonyan & Zisserman, 2014)와 같은 널리 퍼진 비전 아키텍처를 조사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c} \\hline \\hline Teacher & ShuffleNet-V1 & ShuffleNet-V1 & MobileNet-V2 & MobileNet-V2 & ShuffleNet-V2 \\\\  & 72.40 & 72.40 & 66.85 & 66.85 & 74.44 \\\\ Student & ResNet32\\(\\times\\)4 & WRN-40-2 & VGG13 & ResNet50 & ResNet32\\(\\times\\)4 \\\\ \\hline KD (Hinton et al., 2015) & 77.92 & 76.45 & 72.13 & 73.32 & 78.27 \\\\ FitNet (Romero et al., 2014) & 75.74 & 74.03 & 70.57 & 71.45 & 76.42 \\\\ RKD (Park et al., 2019) & 76.59 & 75.70 & 70.28 & 72.06 & 77.84 \\\\ AugConf (Burns et al., 2023) & 78.25 & 76.37 & 72.51 & 74.48 & 78.81 \\\\ \\hline AdaptConf (**Ours**) & **78.48** & **76.66** & **72.93** & **74.67** & **79.04** \\\\ \\(\\Delta\\) & \\(+6.08\\) & \\(+4.26\\) & \\(+6.08\\) & \\(+7.82\\) & \\(+4.37\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: CIFAR-100 검증 세트에 대한 **결과. 선생님과 학생들은 서로 다른 아키텍처에 있습니다. 모든 결과는 3번의 시행에 걸친 평균이다.**우리는 제한된 능력 교사가 안내하는 더 큰 능력 학생의 잠재력을 평가하기 위해 다양한 KD 방법을 사용한다. 놀랍게도, KD 기반 접근법을 사용하는 거의 모든 경우에, 학생 모델은 처음부터 훈련된 모델을 능가한다. 또한 AugConf(Burns et al., 2023)와 제안된 AdapConf 방법은 모든 교사-학생 쌍에 걸쳐 이전의 모든 증류 기술을 능가한다. 이것은 단순히 약한 선생님을 모방하는 것이 가장 유리한 결과를 낳지 않는다는 것을 강조한다. 특히, AdapConf는 AugConf(Burns et al., 2023)에 비해 일관되게 우수한 성능을 달성하며, 우리의 동적 적응 신뢰 가중의 이점을 강조한다. 이 접근법은 약에서 강한 지식 전달을 촉진하기 위한 보다 세련된 메커니즘을 제공한다.\n' +
      '\n' +
      '표 2는 ShuffleNet(Zhang et al., 2018) 및 MobileNet(Sandler et al., 2018)과 같은 서로 다른 시리즈의 교사-학생 쌍에 대한 결과를 제시한다. 추가적으로, MobileNetV2-ResNet50 쌍을 예로 들면, 실험 결과는 교사 모델이 상당히 약할 때 _i.e_인 것을 드러낸다. 약한 교사 모델과 강한 학생 모델 사이에는 상당한 성과 격차가 존재하며, KD 기반 방법 중 AugConf와 AdapConf를 제외하고는 강한 학생의 성과를 효과적으로 향상시킬 수 있는 방법이 없었다. 가능한 이유는 이러한 방법들이 손실함수에 강한 학생의 예측을 포함하기 때문이다. 이는 (Lee et al., 2013)에 기술된 방법들과 유사한 자기 훈련 방법이 차선의 교사 모델로부터의 편향을 완화할 수 있음을 증명한다. FitNet(Romero et al., 2014)은 처음부터 훈련과 비교할 때 일관되게 저성과를 보인다는 점에 유의하는 것이 중요하다. 이것은 중간 특징에만 초점을 맞추고 있기 때문일 수 있으며, 이는 강한 학생이 부드러운 예측보다 배우는 데 더 오도할 수 있다(Hao et al., 2023). 전반적으로 AdapConf는 동일한 계열이든 다른 계열이든 평가된 모든 교사-학생 쌍에서 0.5%-2%의 개선을 달성한다.\n' +
      '\n' +
      '나아가 <표 2b>와 같이 교사의 산출물만 가능한 시나리오를 조사한다. 이러한 맥락에서 AugConf 및 AdapConf는 그라운드 트루스가 없을 때 다른 KD 기반 방법에 비해 더 큰 개선을 산출한다는 것이 분명해진다. 이 관찰은 지상 진실이 이용 가능하지 않은 더 극단적인 WSG 시나리오에 대한 신뢰 증류 접근법의 적합성을 강조한다.\n' +
      '\n' +
      '**ImageNet 이미지 분류.** 표 4는 ImageNet 데이터셋에서 이미지 분류에 대한 상위 1의 정확도를 제시한다. AdapConf 방법은 동일한 아키텍처를 사용하든 다른 아키텍처를 사용하든 두 WSG 시나리오에서 상당한 개선을 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} Teacher & ResNet18 & MobileNet-V1 \\\\  & 69.75 & 71.57 \\\\  & ResNet34 & ResNet50 \\\\  & 73.47 & 76.22 \\\\ \\hline KD (Hinton et al., 2015) & 73.68 & 76.52 \\\\ FitNet (Romero et al., 2014) & 70.93 & 73.61 \\\\ RKD (Park et al., 2019) & 73.65 & 76.45 \\\\ ReviewKD (Chen et al., 2021) & 72.99 & 75.28 \\\\ DKD (Zhao et al., 2022) & 73.74 & 76.72 \\\\ AugConf (Burns et al., 2023) & 73.80 & 76.64 \\\\ \\hline AdapConf (**Ours**) & **74.16** & **76.94** \\\\ \\(\\Delta\\) & \\(+0.69\\) & \\(+0.72\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ImageNet validation set에 대한 **Top-1 결과. \\ (\\Delta\\)는 처음부터 훈련된 학생 모델에 대한 성능 향상을 나타낸다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} Teacher: ResNet50 (80.36) & Teacher + GT & Teacher \\\\ Student: ViT-B (MAE pretrain) & 83.53 & - \\\\ \\hline KD (Hinton et al., 2015) & 83.62 & 82.32 \\\\ FitNet (Romero et al., 2014) & 82.48 & 81.02 \\\\ RKD (Park et al., 2019) & 82.19 & 80.98 \\\\ DKD (Zhao et al., 2022) & 83.68 & - \\\\ AugConf (Burns et al., 2023) & 83.70 & 82.38 \\\\ \\hline AdapConf (**Ours**) & **83.86** & **82.51** \\\\ \\(\\Delta\\) & \\(+0.33\\) & \\(+2.15\\) \\\\ \\hline AdapConf (**Ours**) & **76.03** & **71.99** \\\\ \\(\\Delta\\) & \\(+0.75\\) & \\(+4.57\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 분류 훈련 단계에서 miniImageNet 유효성 검사 세트에 대한 95% 신뢰 구간을 갖는 **평균 5-way 정확도(%). \\ (\\Delta\\)는 처음부터 훈련된 학생 모델에 비해 성능 향상을 나타낸다. 모든 결과는 3번의 시행에 대한 평균이다.**\n' +
      '\n' +
      'Few-shot 학습 4.2.2\n' +
      '\n' +
      '소샷 학습 과제는 분류(표 5)와 메타 학습(표 6) 단계에서 증류 실험을 별도로 진행한다. 우리는 크기가 다른 교사와 훈련했을 때 학생의 성과를 비교하고 평가한다. 분류 실험에서 RKD만 학생 모델의 성능 저하를 초래하는 반면 다른 방법의 사용은 다양한 개선 정도를 초래했다. 특히, 우리의 신뢰 기반 방법은 이전의 지식 증류 기반 방법보다 우수하다. 메타 학습 단계에서는 교사와 동일한 모델의 서로 다른 훈련 단계의 가중치를 사용한다. 실험 결과는 제안된 방법의 상당한 이점을 보여준다. 클래스-스테이지 가중치를 교사로 사용하더라도, 본 논문에서 제안한 방법은 약한 ResNet18(Class-stage) 교사 모델에 의해 설정된 기준선보다 +0.66% 향상된 성능을 보인다. 또한, 교사와 동일한 단계의 가중치를 사용할 때, 우리의 신뢰 기반 방법은 이전의 지식 증류 결과를 더 크게 능가한다.\n' +
      '\n' +
      '####4.2.3 전이학습\n' +
      '\n' +
      '표 3은 iNaturalist(Van Horn et al., 2018)와 ImageNet(Deng et al., 2009) 데이터셋을 이용한 전이학습의 효과를 살펴본다. 본 논문에서 제안한 방법은 ImageNet에서 Ground truth 레이블로 학습하였을 때, 83.5%의 높은 정밀도로 모델 상에서 Top-1 정확도에서 +0.33%의 향상된 성능을 보였다. 지상 진실 라벨이 없어도 우리의 접근법은 여전히 더 약한 ResNet50 교사 모델에 의해 설정된 기준선보다 +2.15% 개선을 보장한다. iNaturalist 데이터 세트에서 신뢰 기반 방법은 이전 지식 증류 결과를 상당한 차이로 능가한다.\n' +
      '\n' +
      '4.2.4 잡음 레이블을 이용한 학습\n' +
      '\n' +
      '표 7에서 두 개의 시뮬레이션된 노이즈 라벨 설정에서 CIFAR-10 및 CIFAR-100 데이터 세트를 사용하여 약대 강의 효과를 분석한다. 샘플 데이터 세트(CIFAR-10)에서 모델을 훈련할 때, 우리를 제외한 모든 방법은 이미 높은 정확도를 감안할 때 모델에 부정적인 영향을 미친다. 이것은 교사와 학생 모델 간의 성능 격차에 관계없이 우리의 방법의 견고성을 강조한다. CIFAR-100 데이터 세트에서, 본 방법은 비대칭 잡음 유형 설정 하에서 Top-1 정확도에서 0.81%의 성능 향상을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**신뢰 증류의 견고성.** 본 연구에서는 단순한 라벨의 가중치 조합을 넘어서는 방법을 고안할 필요성을 조사한다. Eq에 표시된 대로입니다. 도 1을 참조하면, 더 약한 모델로부터의 직접 학습을 더 강한 모델의 고유 용량과 통합하는 그것의 간단한 접근법에도 불구하고, AugConf(Burns et al., 2023)는 여전히\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline \\multirow{2}{*}{Teacher} & ResNet12 (Class-stage) & ResNet18 (Class-stage) & ResNet12 (Meta-stage) & ResNet18 (Meta-stage) \\\\  & 59.20 & 60.63 & 65.26 & 66.51 \\\\  & ResNet36 & ResNet36 & ResNet36 & ResNet36 \\\\ Student & 65.08 & 65.08 & 65.08 & 65.08 \\\\ \\hline KD (Hinton et al., 2015) & 63.43 & 65.04 & 66.08 & 65.93 \\\\ RKD (Park et al., 2019) & 64.79 & 65.42 & 65.96 & 65.46 \\\\ AugConf (Burns et al., 2023) & 65.15 & 65.59 & 65.9 & 65.78 \\\\ \\hline AdaptConf (**Ours**) & **65.38** & **65.74** & **66.08** & **65.95** \\\\ \\(\\Delta\\) & +0.30 & +0.66 & +1.00 & +0.87 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: Meta-Learning 단계에서 miniImageNet validation set에 대한 **평균 5-way accuracy. \\ (\\Delta\\)는 처음부터 훈련된 학생 모델에 비해 성능 향상을 나타낸다. 모든 결과는 3번의 시행에 대한 평균이다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline dataset & \\multicolumn{3}{c|}{CIFAR-10} & \\multicolumn{3}{c}{CIFAR-100} \\\\ \\hline noise type & \\multicolumn{2}{c}{asymmetric} & \\multicolumn{2}{c}{symmetric} & \\multicolumn{2}{c}{asymmetric} & \\multicolumn{2}{c}{symmetric} \\\\ \\hline \\multirow{2}{*}{Teacher} & PR18 & PR18 & PR18 & PR18 & PR18 \\\\  & 92.98 & 99.56 & 95.80 & 99.80 & 73.20 & 92.67 & 76.16 & 92.90 \\\\ \\multirow{2}{*}{Student} & PR34 & PR34 & PR34 & PR34 & & PR34 \\\\  & 93.69 & 99.61 & 96.13 & 99.77 & 74.80 & 92.94 & 78.20 & 93.77 \\\\ \\hline  & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\\\ \\hline KD (Hinton et al., 2015) & 93.54 & 99.84 & 95.90 & 99.84 & 75.49 & 93.67 & 77.61 & 93.74 \\\\ RKD (Park et al., 2019) & 92.42 & 99.75 & 95.99 & 99.85 & 74.20 & 93.54 & 76.92 & 93.09 \\\\ AugConf (Burns et al., 2023) & 92.60 & 99.75 & 95.10 & 99.83 & 74.99 & 93.72 & 78.34 & 94.02 \\\\ \\hline AdaptConf (**Ours**) & **93.69** & **99.84** & **96.13** & **99.87** & **75.61** & **93.78** & **78.64** & **94.03** \\\\ \\(\\Delta\\) & +0.00 & +0.23 & +0.00 & +0.10 & +0.81 & +0.84 & +0.44 & +0.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: CIFAR-10/CIFAR-100 노이즈 라벨 검증 세트에 대한 **Top-1 및 Top-5 결과. \\ (\\Delta\\)는 처음부터 훈련된 학생 모델에 비해 성능 향상을 나타낸다. 모든 결과는 3회 이상의 평균이다.** 두 가지 다른 목표의 비율의 균형을 맞추기 위해 하이퍼 매개변수 \\(\\알파\\)의 수동 조정이 필요하다. 다른 \\(\\alpha\\) 값의 설정은 상황에 따라 다양한 영향을 미칠 수 있다. 유사하게, 제안된 AdaptConf는 목표 비율의 균형을 맞추기 위해 \\(\\alpha\\)의 수동 조정이 필요하지 않지만, 기존의 증류법(Hinton et al., 2015)에 따라 교차 엔트로피 CE(-) 계산 동안 소프트 라벨의 확률 분포 정도를 제어하기 위해 온도 \\(T\\)을 조작할 수 있다. 따라서 우리는 최종 결과에 대한 서로 다른 하이퍼 매개변수 설정에서 이 두 가지 방법의 효과를 탐구한다. 전반적으로 KD, AugConf 및 AdaptConf의 성능은 다양한 아키텍처 설정에 걸쳐 순차적으로 개선됩니다. 또한 AugConf가 AdaptConf에 비해 결과의 변동이 더 큰 것을 관찰할 수 있으며, 이는 AugConf에 대한 \\(\\alpha\\)의 영향이 AdaptConf에 대한 \\(T\\)의 영향보다 더 크다는 것을 나타내며, 이는 우리의 AdaptConf가 우수한 견고성을 가지고 있음을 시사한다. 또한 AdaptConf가 달성한 평균 결과는 다른 하이퍼 매개변수 설정에서 AugConf보다 일관되게 높다.\n' +
      '\n' +
      '**신뢰 증류의 견고성.** 이 섹션에서 우리는 식에 설명된 동적 함수 \\(\\beta(x)\\)에 의해 결정된 신뢰 가중치의 정량적 분석을 수행한다. 2. 실험결과는 그림 3과 같으며, 4개의 훈련 단계에서 체크포인트를 선택하고 검증 세트에서 특정 \\(\\beta(x)\\) 값을 계산했다. 연수가 진행됨에 따라 \\(\\beta=0.5\\)인 표본의 비율이 증가하여 학생 모델의 성능이 향상되고 약한 교사의 올바른 분류와 정렬되고 있음을 관찰할 수 있다. 더 높은 온도 설정\\(T\\)은 교사와 학생 사이의 교차 엔트로피(CE) 불일치를 감소시켜 약한 교사의 지도와 강한 교사 사이의 보다 균일한 균형을 촉진한다.\n' +
      '\n' +
      '그림 3: Eq에서 \\(\\beta\\)(x)의 값에 대한 정량적 분석. CIFAR-100 데이터 세트의 2. 평가는 ShuffleNetV1-ResNet32x4 교사-학생 아키텍처 쌍을 기반으로 한다.\n' +
      '\n' +
      '그림 2: 신뢰 증류 결과에 대한 하이퍼-파라미터 변동의 영향을 조사하는 절제 연구. AugConf의 매개변수 \\(\\alpha\\)는 0.1에서 0.9 범위에 걸쳐 조정되고 AdaptConf의 온도 \\(T\\)는 0.1에서 8까지 조정된다.\n' +
      '\n' +
      '학생 자신의 예측. 결과적으로 훈련에 따라 \\(\\beta=0.5\\)의 표본수도 증가한다. 이러한 현상은 제안된 Adapt-Conf가 두 구성 요소 간의 학습 비율을 동적으로 조정할 수 있음을 집합적으로 검증한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 시각 기반 모델에 대한 약대강 일반화를 조사하고, 시각 영역에서 인공지능의 능력을 향상시킬 수 있는 유망한 방법을 제시한다. 혁신적인 적응형 신뢰 손실 메커니즘을 활용하여 더 약한 모델을 사용하여 더 강한 대응물을 감독하고 개선하는 것의 실현 가능성과 효과를 보여준다. 우리의 연구 결과는 약대강 일반화의 잠재력을 검증할 뿐만 아니라 AI 모델 성능의 추가 발전을 잠금 해제하는 것을 목표로 하는 향후 연구의 단계를 설정했다. 이 작업은 비전 작업에서 초인적 성능을 달성하는 데 있어 미묘한 감독 메커니즘의 중요성을 강조하면서 보다 정교하고 효율적이며 유능한 AI 시스템을 추구하는 데 상당한 진전에 기여한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*Bai et al. (2023) Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., Malik, J., and Efros, A. A. A. 순차적 모델링은 대형 비전 모델에 대한 확장 가능한 학습을 가능하게 한다. ArXiv:2312.00785. 인용: SS1.\n' +
      '*T. B. Mann, N., S. Subbiah, J. D., P. Dhariwal, A., P. Neelakantan, P., Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. 신경 정보 처리 시스템의 발전. 인용: SS1.\n' +
      '* C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. 가오, L, L. 천영 (주)에코펫 Joglekar, J. Leike, et al.(2023) Weak-to-strong Generalization: 약한 감독으로 강한 능력을 이끌어낸다. ArXiv:2312.09390. 인용: SS1.\n' +
      '* M. 천아래드포드 Child, J. Wu, H. Jun, D. Luan, and I. Sutskever (2020) Generative pretraining from pixels. 기계 학습에 관한 국제 회의에서, 인용: SS1.\n' +
      '* P. Chen, S. Liu, H. Zhao, 및 J. Jia(2021)는 지식 검토를 통해 지식을 증류한다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*Y. 천진 류현수 대럴, X Wang (2021) Meta-baseline: 몇 개의 샷 학습을 위한 간단한 메타 학습 탐색. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9062-9071. Cited by: SS1.\n' +
      '* J. Deng, W. 동록 소처 이경 리, L. Fei-Fei(2009) Imagenet: 대규모 계층적 이미지 데이터베이스. 2009년 IEEE Conference on computer vision and pattern recognition, Cited by: SS1.\n' +
      '* A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니 민더러, G. 헤이골드, S. Gelly, et al.(2020) 이미지는 16x16 단어들의 가치가 있다: 스케일에서 이미지 인식을 위한 트랜스포머들. ArXiv:2010.11929. 인용: SS1.\n' +
      '*Y. 두종 Liu, J. Li, and W. X. Zhao (2022) A survey of vision-language pre-trained models. ArXiv:2202.10936. 인용: SS1.\n' +
      '*T. 풀라넬로, 지 립톤 쯔샤넨 Itti, and A. Anandkumar (2018) Born again neural networks. In International Conference on Machine Learning, pp. 1607-1616. Cited by: SS1.\n' +
      '*Z. 하오정국 한희후, C. Xu, Y. Wang (2023) Vanillakd: 바닐라 지식 증류의 힘을 소규모에서 대규모로 재방문한다. ArXiv:2305.15781. 인용: SS1.\n' +
      '*Z. 하오정국 한영 탕희후 Wang, and C. Xu (2023) One-for-all: 지식 증류에서 이질적인 아키텍처 간의 격차를 메운다. ArXiv:2310.19444. 인용: SS1.\n' +
      '*K. 그, X 장승 Ren, and J. Sun (2015) Delving deep into rectifier: exceeding human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. Cited by: SS1.\n' +
      '*K. 그, X 천성호 시영 리 P. 달러, R. Girshick (2022) 마스킹 오토인코더는 확장 가능한 비전 학습자이다. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009. Cited by: SS1.\n' +
      '*K. 그, X 천성호 시영 리 P. 달러, R. Girshick (2022) 마스킹 오토인코더는 확장 가능한 비전 학습자이다. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009. Cited by: SS1.\n' +
      '*B. 허준 윤희남 곽, 및 J. Y. Choi(2019) 특징 증류의 포괄적인 정비. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Cited by: SS1.\n' +
      '* G. Hinton, O. Vinyals, and J. Dean (2015) the knowledge Distilling the neural network. ArXiv:1503.02531. 인용: SS1.\n' +
      '* J.\n' +
      '\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long shortterm memory. _ Neural computation_, 1997.\n' +
      '* Kaelbling et al. (1996) Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey _ Journal of artificial intelligence research_, 1996.\n' +
      '* Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W. - Y., et al. Segment anything. _ arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layer of features from tiny images. 2009년\n' +
      '* Lee et al. (2013) Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. 2013년 ICML_에서 표현 학습의 문제에 대한 워크샵에서.\n' +
      '* Li et al. (2020) Li, J., Socher, R., and Hoi, S. C. Dividemix: Learning with noisy label as semi-supervised learning. _ arXiv preprint arXiv:2002.07394_, 2020.\n' +
      '* Li 등 (2022) Li, S., Xia, X., Ge, S., and Liu, T. 시끄러운 레이블을 사용한 선택적 감독 대조 학습입니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 316-325, 2022.\n' +
      '* Lin et al. (2014) Lin, T. -Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, Proceedings, Part V 13_, 2014.\n' +
      '* Park et al. (2019) Park, W., Kim, D., Lu, Y., and Cho, M. 관계 지식 증류. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019.\n' +
      '* Paszke et al. (2019) Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Kimelshein, N., Antiga, L., et al. Pytorch: An imperative style, highperformance deep learning library. _ 신경 정보 처리 시스템_, 2019의 발전.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '*Romero et al. (2014) Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y. 핏넷: 얇은 깊은 그물에 대한 힌트. _ ArXiv:1412.6550_, 2014.\n' +
      '* Russakovsky et al. (2015) Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. _ International journal of computer vision_, 115:211-252, 2015.\n' +
      '* Sandler et al. (2018) Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L. - C. Mobilenetv2: 역 잔차 및 선형 병목 현상. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '* Silver et al. (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. _ nature_, 2016.\n' +
      '* Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional networks for large scale image recognition. _ arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* Tanaka et al. (2018) Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. 노이즈 라벨로 학습을 위한 공동 최적화 프레임워크입니다. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '* Van Horn et al. (2018) Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. 비자연주의 종 분류 및 탐지 데이터 세트입니다. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 2017의 발전.\n' +
      '* Vinyals et al. (2016) Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. _ 신경 정보 처리 시스템_, 29, 2016의 발전.\n' +
      '* Xie et al. (2020) Xie, Q., Luong, M. - T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10687-10698, 2020.\n' +
      '* Zagoruyko & Komodakis (2016) Zagoruyko, S. 및 코모다키스, N. 광범위한 잔여 네트워크. _ ArXiv:1605.07146_, 2016.\n' +
      '* Zhang et al. (2018) Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufflenet: a extremely efficient convolutional neural network for mobile devices. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '\n' +
      '* Zhao et al. (2022) Zhao, B., Cui, Q., Song, R., Qiu, Y., and Liang, J. Decoupled Knowledge distillation. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, 2022.\n' +
      '\n' +
      '## 부록 A 구현 세부사항.\n' +
      '\n' +
      '### ImageNet Classification\n' +
      '\n' +
      '**CIFAR-100.** 전통적인 증류지(Hao et al., 2023; Zhao et al., 2022)에 요약된 바와 같이 교사 및 학생 모델의 비전 아키텍처를 채택한다. 이전의 코드베이스(Zhao et al., 2022)는 1 GPU만을 사용하여 CIFAR-100에 대한 실험을 수행했다는 점에 유의해야 한다. 실험을 빠르게 하기 위해, 우리는 8개의 GPU에 대해 훈련하고 추론하기 위해 _distributed_ Pytorch 프레임워크(Paszke et al., 2019)를 활용한다. 결과적으로 일부 하이퍼파라미터 설정 및 결과가 이전 논문과 정확히 일치하지 않을 수 있다. 구체적으로 운동량 0.9인 SGD 최적화기를 사용하였으며, 학습률은 0.2에서 시작하여 코사인 어닐링 스케줄을 사용하여 최소 학습률 2e-3으로 쇠퇴한다. 8개의 GPU에 걸쳐 배치 크기가 512인 240개의 에폭에 대해 훈련하고 0.0005의 중량 감소를 적용하며 무작위 크기 조정 작물 및 수평 플립을 포함한 표준 데이터 증강 기술이 활용된다.\n' +
      '\n' +
      '**ImageNet.** 운동량 0.9의 SGD 최적화기를 사용합니다. 학습 속도는 0.1에서 시작하여 30 에폭마다 0.1의 속도로 쇠퇴합니다. 8개의 GPU에 걸쳐 배치 크기가 512인 100개의 에폭에 대해 훈련하고 0.0001의 가중치 감쇠를 적용하며 무작위 크기 조정 작물, 수평 플립 및 레이블 평활화를 포함한 표준 데이터 증강 기술을 활용한다.\n' +
      '\n' +
      '### Transfer learning.\n' +
      '\n' +
      'ImageNet과 iNaturalist에서 자체 지도 사전 훈련된 ViT-B를 미세 조정하기 위해 MAE(He et al., 2022)의 하이퍼파라미터 설정을 채택한다. 이를 위해 adamw 최적화기를 사용하였다. 학습률은 2e-3에서 시작하여 코사인 어닐링 스케줄을 활용하여 최소 학습률 1e-6으로 점차 쇠퇴한다. 우리는 8개의 GPU에 걸쳐 4096의 배치 크기를 사용하여 100개의 에폭에 대한 교육을 수행한다. 과적합을 완화하기 위해 0.05의 중량 감쇠가 적용된다. 미세 조정 프로세스는 자동 증강, 혼합, 컷믹스 및 확률적 드롭 경로를 포함한 강력한 데이터 증강 기술을 통합한다.\n' +
      '\n' +
      '### Few-shot Leearning\n' +
      '\n' +
      '우리는 ResNet12를 사용하여 miniImageNet 데이터셋에 (Chen et al., 2021)의 설정을 따르고, 원래 ResNet12에서 레이어의 수를 증가시켜 ResNet18과 ResNet36을 생성하였으며, 분류 훈련 단계에서는 모멘텀 0.9인 SGD 최적화기를 사용하여 학습률을 0.1부터 시작하여 감쇠 계수를 0.1로 설정하였으며, miniImageNet에서는 4개의 GPU에서 배치 크기가 128인 100 epoch를 학습하고, 90 epoch에서 학습률이 감쇠하고, 가중치 감쇠는 0.0005를 적용하여 무작위 크기 작물과 수평 플립을 포함한 표준 데이터 증강 전략을 적용하였다. 메타 학습 단계에서는 운동량 0.9인 SGD 최적화기를 사용하였으며, 학습률은 0.001로 고정하였으며, 각 학습 배치의 크기는 4, 즉, 각 학습 배치에는 평균 손실량을 계산하기 위해 4개의 소수의 태스크가 포함되어 있다. 코사인 스케일링 파라미터 \\(\\tau\\)를 10으로 초기화하고 지식증류를 위해 kd 손실 가중치를 1로, 온도를 10으로 설정하고 분류기 단계와 메타 단계는 각각 8과 0.25로 임계값을 사용한다.\n' +
      '\n' +
      '시끄러운 레이블을 이용한### 학습\n' +
      '\n' +
      'CIFAR-10/100 데이터셋의 경우, (Li 등, 2022) PreAct ResNet18 네트워크를 이용하여 PreAct ResNet12의 레이어 수를 증가시켜 PreAct ResNet34를 생성하고, 모멘텀 0.9, 가중치 감쇠 1e-4, 배치 크기 128의 SGD를 이용하여 모델을 학습하고, 250 에폭에 대해 네트워크를 학습하고 워밍업 에폭을 1 듀프린 학습 단계로 설정한다. 우리는 초기 학습률을 0.1로 설정하고 125 및 200 에폭 이후 10배 줄였다. Sel-CL+의 미세 조정 단계는 70 에폭으로 학습률이 0.001이고, Mixup 하이퍼파라미터는 1, 스칼라 온도는 0.1, 손실 가중치는 1로 설정하며, 모의 잡음 레이블은 대칭과 비대칭의 두 가지 설정을 시도한다. 상기 잡음비는 각각 0.2 및 0.4로 설정되는 것을 특징으로 하는 플라즈마 디스플레이 패널의 구동방법. 지식 증류의 경우 임계값을 0.5로 설정하고 지식 증류 손실에 1의 가중치를 할당한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>