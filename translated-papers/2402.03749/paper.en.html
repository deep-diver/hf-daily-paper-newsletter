<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Vision Superalignment: Weak-to-Strong Generalization\n' +
      '\n' +
      'for Vision Foundation Models\n' +
      '\n' +
      ' Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Chengcheng Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Kai Han\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Chang Xu\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Yunhe Wang\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Science, Faculty of Engineering, University of Sydney, Sydney, Sydney, Australia. Correspondence to: Yunhe Wang \\(<\\)yunhe.wang@huawei.com\\(>\\).\n' +
      '\n' +
      'Jianyuan Guo\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Huawei Noah\'s Ark Lab, Beijing, China\n' +
      '\n' +
      'Hanting Chen\n' +
      '\n' +
      'Equal contribution \\({}^{2}\\)School of Computer Sciencecased an ability in the game of Go that far exceeded the prowess of the strongest human players. Similarly, GPT-like models (Brown et al., 2020) are capable of achieving results beyond the average human level in a variety of exams. Notably, this phenomenon emerged even earlier in the field of computer vision. As early as 2015, meticulously designed convolutional neural networks (He et al., 2015) were already achieving results on large-scale visual classification tasks like ImageNet that surpassed human performance. This trend of superhuman achievements has driven the research community to focus on how to control, evaluate, and optimize these exceptionally capable models, recognizing the immense potential they hold for advancing our understanding and application of artificial intelligence.\n' +
      '\n' +
      'To address the intricate challenge of leveraging human expertise in supervising superhuman AI models, the concept of "superalignment" has been introduced. This approach aims to align superhuman models in a way that maximizes their learning from human input. A seminal work in this area is the concept of Weak-to-Strong Generalization (WSG) (Burns et al., 2023). This research presents an intriguing analogy to explore the feasibility of using weaker models to supervise stronger ones. The results from this line of inquiry are stimulating: strong models, already endowed with robust generalization and representational capabilities, can achieve performances that surpass their weaker counterparts through simple supervision provided by these weaker models. This supervision often involves incomplete or flawed labels, yet the stronger models can effectively transcend these limitations. Such findings have not only affirmed the viability of Weak-to-Strong Generalization but have also demonstrated its efficacy in fields like natural language processing and reinforcement learning.\n' +
      '\n' +
      'In this paper, we delve into the topic of "vision superalignment," specifically investigating the applicability of Weak-to-Strong Generalization (WSG) within the context of vision foundation models. Our study meticulously designs and examines multiple scenarios in computer vision, including few-shot learning, transfer learning, noisy label learning, and traditional knowledge distillation settings. In these scenarios, stronger models are trained to learn from weaker models. Through detailed validation and comparative experiments, we demonstrate the feasibility of WSG in the visual domain. Furthermore, we introduce an improved and adaptive confidence scheme to enhance the efficacy of WSG. Our study not only validates the concept of WSG in vision but also contributes significantly to the broader pursuit of superalignment across various AI modalities. Our work represents a substantial step forward in understanding and optimizing the interaction between human-level expertise and superhuman AI capabilities, potentially paving the way for groundbreaking advancements in artificial intelligence.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'The pursuit of enhancing the performance of deep neural networks in computer vision has led to the development of the teacher-student learning paradigm (Hinton et al., 2015). This approach typically involves a stronger model (teacher) improving the performance of a weaker model (student), with extensive research focusing on optimizing the capabilities of the weaker model. Various strategies have been proposed to achieve this. For instance, (Romero et al., 2014) suggests that in addition to the output logits, incorporating intermediate layer features for supervision can significantly boost the student\'s learning. (Park et al., 2019) posits that the relationships between samples can serve as valuable supervisory information.\n' +
      '\n' +
      'In a further refinement of this approach, (Zhao et al., 2022) redefines classical knowledge distillation (KD) loss, segmenting it into target-class and non-target-class distillation to balance the transfer of these two types of information more effectively. (Heo et al., 2019) delves into the details and components of feature distillation, arriving at an improved method for the transfer of feature knowledge. Meanwhile, (Chen et al., 2021) explores cross-stage feature transfer as an alternative to the conventional same-stage feature transfer. These methods have proven effective for strong-to-weak generalization scenarios.\n' +
      '\n' +
      'However, with the gradual increase in the size and complexity of vision foundation models, the focus has shifted towards weak-to-strong generalization, _i.e._, how a weak model can improve a strong model. In this context, (Furlanello et al., 2018) investigates knowledge distillation between teachers and students of equal size, demonstrating the feasibility of distilling models of the same size. Building upon this, (Xie et al., 2020) introduces the use of additional unlabeled data for knowledge distillation among models of equal size, further validating the effectiveness of strong-to-strong generalization, especially in scenarios with abundant data availability. This body of research sets the stage for our exploration into weak-to-strong generalization, a relatively uncharted yet promising domain in the field of vision foundation models.\n' +
      '\n' +
      '## 3 Vision Superalignment\n' +
      '\n' +
      'In order to investigate how to supervise and optimize superhuman computer vision models, our focus centers on the study of weak-to-strong generalization for vision foundation models. In this section, we initially delve into examining and defining what constitutes vision foundation models. These models, characterized by their extensive capacity and versatility, form the backbone of our research. Subsequently, we address a critical challenge inherent in the weak-to-strong generalization approach: the inaccuracy of output labels from weak models. In response to this, we introduce an innovative solution - adaptive confidence distillation. This method is designed to enhance the learning process of strong models by effectively utilizing the guidance provided by weak models. Adaptive confidence distillation operates on the principle that even imperfect or partially accurate guidance from a weak model can be a valuable learning tool for a stronger model, provided that the information is processed and adapted correctly.\n' +
      '\n' +
      '### Vision Foundation Models\n' +
      '\n' +
      'In our exploration of weak-to-strong generalization for vision foundation models, it is crucial to first define what constitutes these models. There are several potential categories of candidates that represent vision foundation models, each characterized by their unique capabilities and approaches in the realm of computer vision.\n' +
      '\n' +
      '**Text-Visual Fusion Models:** The first category includes models that integrate visual and linguistic tasks. A notable example is the work of Radford et al. (Radford et al., 2021), which constructs a foundational model by aligning computer vision tasks with language tasks through image-text pre-training pairs. This approach bridges the gap between textual and visual information, providing a comprehensive understanding of both domains.\n' +
      '\n' +
      '**Image Generation Models:** The second category focuses on models that are capable of generating images, which can be considered as a form of modeling the image space. Rombach et al.(Rombach et al., 2022) demonstrate this through their ability to generate a plethora of images from textual descriptions, establishing a basis for image generation models. Similarly, Chen et al.(Chen et al., 2020) employ a GPT-like pre-training method for images, resulting in a generative Transformer model with significant generalization capabilities in image creation.\n' +
      '\n' +
      '**Architecture for General or Zero-Shot Visual Tasks:** The third category seeks to develop architectures capable of solving a range of visual tasks, either generally or in a zero-shot manner. Bai et al.(Bai et al., 2023) approach this by modeling a series of image tasks as sequential challenges, creating a large vision model that addresses a spectrum of visual problems. Additionally, Kirillov et al.(Kirillov et al., 2023) propose the "Segment Anything" model, achieving impressive zero-shot segmentation results.\n' +
      '\n' +
      'In our quest to identify the most suitable candidates for vision foundation models to be used in weak-to-strong generation tasks, we propose a definition focused on versatility and effectiveness in the visual domain. We posit that vision foundation models should be applicable to a broad range of visual tasks while delivering high-quality performance.\n' +
      '\n' +
      'Based on this criterion, we suggest that backbones pre-trained on ImageNet represent strong contenders as vision foundation models. The rationale for this choice is twofold. Firstly, these backbones have proven to be highly adaptable and effective for key tasks in computer vision, such as classification, detection, and segmentation. By fine-tuning these backbones, state-of-the-art (SOTA) accuracies can be achieved in these tasks, demonstrating their robustness and versatility. Secondly, there is an extensive array of pretraining algorithms developed specifically for these models (He et al., 2022), which further qualifies them as universal pre-training models for vision tasks. Additionally, these types of models are often used as one of the branches in vision-language multimodal models (Du et al., 2022), highlighting their applicability in cross-modal tasks.\n' +
      '\n' +
      'Therefore, for our experimental analysis, we choose to focus on these backbone models as representatives of vision foundation models. We aim to conduct our weak-to-strong generalization analysis using the fundamental task of image classification as a baseline. This approach allows us to thoroughly assess the capabilities and potential of weak-to-strong generation in a controlled yet comprehensive manner, offering insights that could be extrapolated to other, more complex vision tasks.\n' +
      '\n' +
      '### Adaptive Confidence Distillation\n' +
      '\n' +
      'In this subsection, we explore the methodology for implementing weak-to-strong generalization in vision foundation models. The central question we address is how a weak vision foundation model can supervise a stronger counterpart effectively. (Burns et al., 2023) proposes an augmented confidence loss approach, which is formulated as:\n' +
      '\n' +
      '\\[L_{\\text{conf}}(f)=(1-\\alpha)\\text{CE}(f(x),f_{w}(x))+\\alpha\\text{CE}(f(x), \\hat{f}(x)), \\tag{1}\\]\n' +
      '\n' +
      'where \\(f\\) represent the strong model that needs to be optimized, and \\(f_{w}\\) denote the weak model, \\(\\hat{f}(x)\\) refers to the hard label predicted by the strong model for an input image \\(x\\). The loss function incorporates the cross-entropy loss (CE) and is balanced by a hyperparameter \\(\\alpha\\). In this formulation, the first term of the loss function resembles the traditional knowledge distillation loss, signifying the learning process of the strong model from the weak model. Given that the labels provided by the weak model may not always be accurate, the second term of the loss function encourages the strong model to leverage its superior generalization abilities and prior knowledge to refine its predictions.\n' +
      '\n' +
      'The strength of this approach lies in its ability to balance direct learning from the weak model with the strong model\'s intrinsic capacity for understanding and interpreting the visual data. This method paves the way for the strong model to surpass the limitations of the weak model, utilizing the latter\'s guidance while simultaneously enhancing its predictions through its advanced capabilities.\n' +
      '\n' +
      'Addressing the limitations inherent in the supervision provided by weak models and the inaccuracies of strong models\' self-generated hard labels, a more sophisticated approach is required beyond a simple weighted combination of these labels. Given the challenge in directly discerning the accuracy of each label, leveraging confidence as a metric for selecting the most probable correct label emerges as a viable solution.\n' +
      '\n' +
      'We propose to use the discrepancy between the soft label and the hard label as an indicator of the model\'s confidence. The underlying rationale is that when a model\'s soft label closely aligns with its hard label, it suggests a higher confidence in its own judgment. To capitalize on this insight, we introduce an adaptive confidence loss that dynamically adjusts based on the model\'s confidence level. The specific formulation of this loss is as follows:\n' +
      '\n' +
      '\\[L_{\\text{AC}}(f)=(1-\\beta(x))\\text{CE}(f(x),f_{w}(x))+\\beta(x)\\text{CE}(f(x), \\hat{f}(x)), \\tag{2}\\] \\[\\beta(x)=\\frac{\\text{exp}(\\text{CE}(f(x),\\hat{f}(x)))}{\\text{ exp}(\\text{CE}(f(x),\\hat{f}(x)))+\\text{exp}(\\text{CE}(f(x),\\hat{f}_{w}(x)))}.\\]\n' +
      '\n' +
      'In this formula, \\(\\beta(x)\\) is a function of the input image \\(x\\) that calculates the confidence weight and \\(\\hat{f}_{w}(x)\\) is the hard label of \\(x\\) in the weak model. This weight determines the balance between learning from the weak model and relying on the strong model\'s own predictions. The cross-entropy loss (CE) is used for both components, with the first term focusing on learning from the weak model and the second term emphasizing the strong model\'s self-supervision.\n' +
      '\n' +
      'This adaptive confidence loss enables a more nuanced approach to weak-to-strong generalization. By adjusting the weight based on confidence levels, it allows the strong model to discern when to prioritize its own predictions over the guidance of the weak model and vice versa. This adaptability is key to overcoming the inaccuracies and limitations of both models, leading to more effective learning and enhanced performance in vision foundation models.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      'In this section, we report our main empirical results on various tasks, including baselines and promising methods. All implementation details are attached in supplementary materials.\n' +
      '\n' +
      '### Tasks\n' +
      '\n' +
      '**Image Classification.** Our experiments are primarily focused on two benchmark datasets. CIFAR-100 (Krizhevsky et al., 2009) is a widely recognized dataset for image classification, comprising 32\\(\\times\\)32 pixel images across 100 categories, with training and validation sets containing 50,000 and 10,000 images, respectively. Conversely, ImageNet (Deng et al., 2009) is a large-scale dataset for classification tasks, encompassing 1.28 million training images and 50,000 validation images across 1,000 classes. Additionally, we explore scenarios where only soft labels generated by a weak teacher model are available for training.\n' +
      '\n' +
      '**Few-shot learning.** We explore few-shot learning across the miniImageNet (Vinyals et al., 2016) dataset which contains 100 classes sampled from ILSVRC-2012 (Russakovsky et al., 2015). We randomly split the dataset into 64, 16, and 20 classes as training, validation, and testing sets, respectively. And ensure that each class has 600 images of 84\\(\\times\\)84 image size. We utilize the ResNet36 to explore the weak-to-strong generalization performance in few-shot task. To demonstrate weak-to-strong generalization performance, we follow Meta-Baseline and conduct related experiments on classifier stage and meta stage.\n' +
      '\n' +
      '**Transfer learning.** We explore transfer learning across two benchmark datasets: ImageNet (Deng et al., 2009), and iNaturalist 2018 (Van Horn et al., 2018), the latter comprising 437,513 training images and 24,426 test images distributed across 8,142 species. We utilize the ViT-B (Dosovitskiy et al., 2020) that has been pretrained on the ImageNet training set using the self-supervised MAE (He et al., 2022b)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c} \\hline \\hline \\multirow{2}{*}{Teacher} & ResNet20 & ResNet32 & ResNet8\\(\\times\\)4 & WRN-16-2 & WRN-40-1 & VGG8 \\\\  & 68.93 & 71.72 & 72.41 & 72.71 & 72.30 & 71.99 \\\\ \\multirow{2}{*}{Student} & ResNet56 & ResNet110 & ResNet32\\(\\times\\)4 & WRN-40-2 & WRN-40-2 & VGG13 \\\\  & 72.94 & 74.80 & 79.90 & 77.20 & 77.20 & 75.26 \\\\ \\hline KD (Hinton et al., 2015) & 73.81 & 76.45 & 79.32 & 78.25 & 77.97 & 76.41 \\\\ FitNet (Romero et al., 2014) & 70.51 & 73.15 & 77.65 & 76.71 & 76.12 & 76.39 \\\\ RKD (Park et al., 2019) & 72.98 & 75.62 & 80.10 & 77.27 & 77.76 & 76.20 \\\\ ReviewKD (Chen et al., 2021a) & 70.15 & 72.30 & 77.22 & 75.86 & 75.78 & 74.22 \\\\ DKD (Zhao et al., 2022) & 73.90 & 76.57 & 79.52 & 78.18 & 77.95 & 76.62 \\\\ AugConf (Burns et al., 2023) & 73.86 & 76.72 & 80.34 & 78.34 & 78.15 & 76.55 \\\\ \\hline AdaptConf (**Ours**) & **74.17** & **76.86** & **80.64** & **78.58** & **78.40** & **76.84** \\\\ \\(\\Delta\\) & +1.23 & +2.06 & +0.74 & +1.38 & +1.20 & +1.58 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Results on the CIFAR-100 validation set. Teachers and students are in the same architectures. And \\(\\Delta\\) represents the performance improvement over the student model trained from scratch. All results are the average over 3 trials.**approach, leveraging only image data without labels. Our results are reported for the fine-tuning phase, which is conducted under the guidance of a weak teacher model on each benchmark. Furthermore, we investigate scenarios in which only soft labels produced by the weak teacher model are used for training.\n' +
      '\n' +
      '**Learning with noisy labels.** We evaluate our approach using two datasets with simulated label noise, specifically CIFAR-10 (Krizhevsky et al., 2009) and CIFAR-100 (Krizhevsky et al., 2009). Consistent with prior research (Li et al., 2020; Tanaka et al., 2018), we introduce two distinct types of simulated noisy labels: symmetric and asymmetric. Symmetric noise is introduced by randomly substituting the labels of a certain proportion of the training data with other possible labels uniformly. In contrast, asymmetric noise involves systematic mislabeling to mimic real-world errors, such as flipping the labels to closely related classes. For example, in CIFAR-10, _truck_ is mislabeled as _automobile_, _bird_ as _airplane_, and _cat_ is interchanged with _dog_. For CIFAR-100, similar mislabeling is applied within each of the super-classes in a circular fashion.\n' +
      '\n' +
      '**Baseline methods.** The predominant framework for implementing teacher-student training paradigms is knowledge distillation (Hinton et al., 2015). This approach outlines a method where a larger, more complex teacher network guides the training of a more compact student network. Nonetheless, inspired by the findings of Burns _et al._(Burns et al., 2023), our work pivots towards a scenario where the student network surpasses the teacher in visual capabilities. Despite this inversion of roles, there remains valuable dark knowledge in the teacher network that can be transferred to the student, either through logits or via intermediate representational features. To benchmark our experiments, we employ a range of established (Hinton et al., 2015; Romero et al., 2014; Park et al., 2019; Heo et al., 2019; Chen et al., 2021; Hao et al., 2023) and recently proposed (Zhao et al., 2022; Burns et al., 2023) distillation techniques as baseline methods.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '#### 4.2.1 Image Classification.\n' +
      '\n' +
      '**CIFAR-100 image classification.** We commence our investigation with an exploration of weak-to-strong generalization (WSG) on the CIFAR-100 dataset. The outcomes of this investigation are delineated in Tables 1 and 2. Specifically, Table 1 presents the scenarios in which both teacher and student models share the same network architectures. We examine a range of prevalent vision architectures such as ResNet (He et al., 2016), WRN (Zagoruyko & Komodakis, 2016), and VGG (Simonyan & Zisserman, 2014).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c} \\hline \\hline Teacher & ShuffleNet-V1 & ShuffleNet-V1 & MobileNet-V2 & MobileNet-V2 & ShuffleNet-V2 \\\\  & 72.40 & 72.40 & 66.85 & 66.85 & 74.44 \\\\ Student & ResNet32\\(\\times\\)4 & WRN-40-2 & VGG13 & ResNet50 & ResNet32\\(\\times\\)4 \\\\ \\hline KD (Hinton et al., 2015) & 77.92 & 76.45 & 72.13 & 73.32 & 78.27 \\\\ FitNet (Romero et al., 2014) & 75.74 & 74.03 & 70.57 & 71.45 & 76.42 \\\\ RKD (Park et al., 2019) & 76.59 & 75.70 & 70.28 & 72.06 & 77.84 \\\\ AugConf (Burns et al., 2023) & 78.25 & 76.37 & 72.51 & 74.48 & 78.81 \\\\ \\hline AdaptConf (**Ours**) & **78.48** & **76.66** & **72.93** & **74.67** & **79.04** \\\\ \\(\\Delta\\) & \\(+6.08\\) & \\(+4.26\\) & \\(+6.08\\) & \\(+7.82\\) & \\(+4.37\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Results on the CIFAR-100 validation set. Teachers and students are in the different architectures. All results are the average over 3 trials.**We employ various KD methods to assess the potential of larger-capacity students guided by limited-capacity teachers. Remarkably, in nearly all cases employing KD-based approaches, the student models outperform those trained from scratch. Furthermore, both AugConf (Burns et al., 2023) and our proposed AdapConf method surpasses all previous distillation techniques across all teacher-student pairs. This highlights that simply emulating a weak teacher does not yield the most favorable outcomes. Notably, AdapConf consistently achieves superior performance compared to AugConf (Burns et al., 2023), underscoring the advantage of our dynamic adaptive confidence weighting. This approach provides a more refined mechanism for facilitating weak-to-strong knowledge transfer.\n' +
      '\n' +
      'Table 2 presents the results for teacher-student pairs from different series, such as ShuffleNet (Zhang et al., 2018) and MobileNet (Sandler et al., 2018). Additionally, take the MobileNetV2-ResNet50 pair as an example, the experimental results reveal that when the teacher model is significantly weaker, _i.e_. a substantial performance gap exists between the weak teacher model and the strong student model, none of the KD-based methods were able to effectively enhance the strong student\'s performance, except for AugConf and AdapConf. The possible reason is that these methods include the predictions of the strong student in the loss function. This proves that self-training methods, akin to those described in (Lee et al., 2013), can mitigate the bias from a suboptimal teacher model. It is important to note that FitNet (Romero et al., 2014) consistently underperforms when compared to training from scratch. This could be attributed to its sole focus on intermediate features, which may be more misleading for the strong student to learn from than soft predictions, as suggested by (Hao et al., 2023). Overall, our AdapConf achieves an improvement of 0.5%-2% on all evaluated teacher-student pairings, whether they are from the same or different series.\n' +
      '\n' +
      'Furthermore, we investigate a scenario where only the teacher\'s output is available, as shown in Table 2b. In this context, it becomes evident that AugConf and AdapConf yields more significant improvements compared to other KD-based methods when ground truth is absent. This observation underscores the suitability of our confidence distillation approach for more extreme WSG scenarios where ground truth is not available.\n' +
      '\n' +
      '**ImageNet image classification.** Table 4 presents the top-1 accuracy for image classification on the ImageNet dataset. Our AdapConf method achieves significant improvements across both WSG scenarios, whether employing the same or different architectures.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} Teacher & ResNet18 & MobileNet-V1 \\\\  & 69.75 & 71.57 \\\\  & ResNet34 & ResNet50 \\\\  & 73.47 & 76.22 \\\\ \\hline KD (Hinton et al., 2015) & 73.68 & 76.52 \\\\ FitNet (Romero et al., 2014) & 70.93 & 73.61 \\\\ RKD (Park et al., 2019) & 73.65 & 76.45 \\\\ ReviewKD (Chen et al., 2021) & 72.99 & 75.28 \\\\ DKD (Zhao et al., 2022) & 73.74 & 76.72 \\\\ AugConf (Burns et al., 2023) & 73.80 & 76.64 \\\\ \\hline AdapConf (**Ours**) & **74.16** & **76.94** \\\\ \\(\\Delta\\) & \\(+0.69\\) & \\(+0.72\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Top-1 results on the ImageNet validation set. \\(\\Delta\\) represents the performance improvement over the student model trained from scratch.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c} Teacher: ResNet50 (80.36) & Teacher + GT & Teacher \\\\ Student: ViT-B (MAE pretrain) & 83.53 & - \\\\ \\hline KD (Hinton et al., 2015) & 83.62 & 82.32 \\\\ FitNet (Romero et al., 2014) & 82.48 & 81.02 \\\\ RKD (Park et al., 2019) & 82.19 & 80.98 \\\\ DKD (Zhao et al., 2022) & 83.68 & - \\\\ AugConf (Burns et al., 2023) & 83.70 & 82.38 \\\\ \\hline AdapConf (**Ours**) & **83.86** & **82.51** \\\\ \\(\\Delta\\) & \\(+0.33\\) & \\(+2.15\\) \\\\ \\hline AdapConf (**Ours**) & **76.03** & **71.99** \\\\ \\(\\Delta\\) & \\(+0.75\\) & \\(+4.57\\) \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Average 5-way accuracy (%) with 95% confidence interval on the miniImageNet validation set in Classification Training stage. \\(\\Delta\\) represents the performance improvement over the student model trained from scratch. All results are the average over 3 trials.**\n' +
      '\n' +
      '#### 4.2.2 Few-shot learning\n' +
      '\n' +
      'For the few-shot learning task, we conduct distillation experiments separately in the classification (Table 5) and meta-learning (Table 6) stages. We compare and evaluate the performances of student when trained with teachers of different sizes. In the classification experiments, only RKD results in a performance degradation of the student model, while the usage of other methods led to varying degrees of improvement. Notably, our confidence-based method outperforms previous knowledge distillation based ones. In the meta-learning stage, we employ weights from different training stages of the same model as the teacher. Experimental results demonstrate significant advantages of our proposed method. Even when using the Class-stage weight as the teacher, our approach achieves a +0.66% improvement over the baseline set by a weaker ResNet18(Class-stage) teacher model. Furthermore, when using the same stage weight as the teacher, our confidence-based method surpasses previous knowledge distillation results to a greater extent.\n' +
      '\n' +
      '#### 4.2.3 Transfer learning\n' +
      '\n' +
      'Table 3 examines the efficacy of transfer learning using the iNaturalist (Van Horn et al., 2018) and ImageNet (Deng et al., 2009) datasets. When our method is trained with ground truth labels on ImageNet, it demonstrates a notable enhancement, achieving an increase of +0.33% in top-1 accuracy on a model with a high precision of 83.5%. Even without ground truth labels, our approach still secures a +2.15% improvement over the baseline set by a weaker ResNet50 teacher model. On the iNaturalist dataset, our confidence-based method also surpasses previous knowledge distillation results by a considerable margin.\n' +
      '\n' +
      '#### 4.2.4 Learning with noisy labels\n' +
      '\n' +
      'In Table 7, we analyze the effectiveness of weak-to-strong using the CIFAR-10 and CIFAR-100 datasets under two simulated noisy label settings. When training the model on the sample dataset (CIFAR-10), all methods except ours, negatively impact the model given its already high accuracy. This underscores the robustness of our method, irrespective of the performance gap between the teacher and student models. On the CIFAR-100 dataset, our method demonstrates a performance improvement of 0.81% in top-1 accuracy under the asymmetric noise type setting.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Robustness of confidence distillation.** In this study, we investigate the necessity of devising a method that goes beyond a mere weighted combination of labels. As depicted in Eq. 1, despite its straightforward approach of integrating direct learning from a weaker model with the intrinsic capacity of a stronger model, AugConf (Burns et al., 2023) still\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline \\multirow{2}{*}{Teacher} & ResNet12 (Class-stage) & ResNet18 (Class-stage) & ResNet12 (Meta-stage) & ResNet18 (Meta-stage) \\\\  & 59.20 & 60.63 & 65.26 & 66.51 \\\\  & ResNet36 & ResNet36 & ResNet36 & ResNet36 \\\\ Student & 65.08 & 65.08 & 65.08 & 65.08 \\\\ \\hline KD (Hinton et al., 2015) & 63.43 & 65.04 & 66.08 & 65.93 \\\\ RKD (Park et al., 2019) & 64.79 & 65.42 & 65.96 & 65.46 \\\\ AugConf (Burns et al., 2023) & 65.15 & 65.59 & 65.9 & 65.78 \\\\ \\hline AdaptConf (**Ours**) & **65.38** & **65.74** & **66.08** & **65.95** \\\\ \\(\\Delta\\) & +0.30 & +0.66 & +1.00 & +0.87 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Average 5-way accuracy on the miniImageNet validation set at Meta-Learning stage. \\(\\Delta\\) represents the performance improvement over the student model trained from scratch. All results are the average over 3 trials.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c} \\hline \\hline dataset & \\multicolumn{3}{c|}{CIFAR-10} & \\multicolumn{3}{c}{CIFAR-100} \\\\ \\hline noise type & \\multicolumn{2}{c}{asymmetric} & \\multicolumn{2}{c}{symmetric} & \\multicolumn{2}{c}{asymmetric} & \\multicolumn{2}{c}{symmetric} \\\\ \\hline \\multirow{2}{*}{Teacher} & PR18 & PR18 & PR18 & PR18 & PR18 \\\\  & 92.98 & 99.56 & 95.80 & 99.80 & 73.20 & 92.67 & 76.16 & 92.90 \\\\ \\multirow{2}{*}{Student} & PR34 & PR34 & PR34 & PR34 & & PR34 \\\\  & 93.69 & 99.61 & 96.13 & 99.77 & 74.80 & 92.94 & 78.20 & 93.77 \\\\ \\hline  & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\\\ \\hline KD (Hinton et al., 2015) & 93.54 & 99.84 & 95.90 & 99.84 & 75.49 & 93.67 & 77.61 & 93.74 \\\\ RKD (Park et al., 2019) & 92.42 & 99.75 & 95.99 & 99.85 & 74.20 & 93.54 & 76.92 & 93.09 \\\\ AugConf (Burns et al., 2023) & 92.60 & 99.75 & 95.10 & 99.83 & 74.99 & 93.72 & 78.34 & 94.02 \\\\ \\hline AdaptConf (**Ours**) & **93.69** & **99.84** & **96.13** & **99.87** & **75.61** & **93.78** & **78.64** & **94.03** \\\\ \\(\\Delta\\) & +0.00 & +0.23 & +0.00 & +0.10 & +0.81 & +0.84 & +0.44 & +0.26 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **Top-1 and top-5 results on the CIFAR-10/CIFAR-100 noise label validation set. \\(\\Delta\\) represents the performance improvement over the student model trained from scratch. All results are the average over 3 trials.**requires manual tuning of a hyper-parameter \\(\\alpha\\) to balance the ratio of two different objectives. The setting of different \\(\\alpha\\) values can have varying impacts across different contexts. Similarly, although our proposed AdaptConf does not require manual adjustment of \\(\\alpha\\) to balance the proportions of objectives, we can manipulate the temperature \\(T\\) to control the degree of probability distribution in soft labels during the computation of the cross-entropy CE(-), following a conventional distillation method (Hinton et al., 2015). Therefore, we explore the effects of these two methods under different hyper-parameter settings on the final outcome. Overall, the performance of KD, AugConf, and AdaptConf improves sequentially across various architectural settings. Moreover, it can be observed that AugConf exhibits a larger fluctuation in results compared to AdaptConf, indicating that the influence of \\(\\alpha\\) on AugConf is more significant than the effect of \\(T\\) on AdaptConf, which suggests that our AdaptConf has superior robustness. Additionally, the average outcomes achieved by AdaptConf are consistently higher than those of AugConf under different hyper-parameter settings.\n' +
      '\n' +
      '**Robustness of confidence distillation.** In this section, we perform a quantitative analysis of the confidence weight determined by our dynamic function \\(\\beta(x)\\) as delineated in Eq. 2, with the findings illustrated in Figure 3. We selected checkpoints from four distinct training phases and calculated their specific \\(\\beta(x)\\) values on the validation set. It can be observed that as training progresses, the proportion of samples with \\(\\beta=0.5\\) increases, indicating that the student model\'s performance is improving and being aligned with the weak teacher\'s correct classifications. A higher temperature setting \\(T\\) reduces the cross-entropy (CE) discrepancy between the teacher and student, promoting a more uniform balance between the weak teacher\'s guidance and the strong\n' +
      '\n' +
      'Figure 3: Quantitative analysis about the value of \\(\\beta\\)(x) in Eq. 2 on the CIFAR-100 dataset. The evaluation is based on the ShuffleNetV1-ResNet32x4 teacher-student architecture pair.\n' +
      '\n' +
      'Figure 2: Ablation study examining the impact of hyper-parameter variation on confidence distillation results. The parameter \\(\\alpha\\) for AugConf is adjusted across a range from 0.1 to 0.9, while the temperature \\(T\\) for AdaptConf is scaled from 0.1 to 8.\n' +
      '\n' +
      'student\'s own predictions. Consequently, the number of samples where \\(\\beta=0.5\\) also increases with training. These phenomena collectively validate that our proposed Adapt-Conf can dynamically adjust the learning ratio between the two components.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we investigate weak-to-strong generalization for vision foundation models and unveil a promising avenue for enhancing the capabilities of artificial intelligence in the visual domain. By leveraging an innovative adaptive confidence loss mechanism, we demonstrate the feasibility and effectiveness of using weaker models to supervise and improve stronger counterparts. Our findings not only validate the potential of weak-to-strong generalization but also set the stage for future research endeavors aimed at unlocking further advancements in AI model performance. This work contributes a significant step forward in the pursuit of more sophisticated, efficient, and capable AI systems, emphasizing the importance of nuanced supervision mechanisms in achieving superhuman performance in vision tasks.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bai et al. (2023) Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., Malik, J., and Efros, A. A.. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785. Cited by: SS1.\n' +
      '* T. Brown, B. Mann, N., S. Subbiah, J. D., P. Dhariwal, A., P. Neelakantan, P., Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems. Cited by: SS1.\n' +
      '* C. Burns, P. Izmailov, J. H. Kirchner, B. Baker, L. Gao, L., L. Chen, Y. Ecoffet, M. Joglekar, J. Leike, et al. (2023) Weak-to-strong generalization: eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390. Cited by: SS1.\n' +
      '* M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever (2020) Generative pretraining from pixels. In International conference on machine learning, Cited by: SS1.\n' +
      '* P. Chen, S. Liu, H. Zhao, and J. Jia (2021) Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang (2021) Meta-baseline: exploring simple meta-learning for few-shot learning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9062-9071. Cited by: SS1.\n' +
      '* J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, Cited by: SS1.\n' +
      '* A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Cited by: SS1.\n' +
      '* Y. Du, Z. Liu, J. Li, and W. X. Zhao (2022) A survey of vision-language pre-trained models. arXiv preprint arXiv:2202.10936. Cited by: SS1.\n' +
      '* T. Furlanello, Z. Lipton, M. Tschannen, L. Itti, and A. Anandkumar (2018) Born again neural networks. In International Conference on Machine Learning, pp. 1607-1616. Cited by: SS1.\n' +
      '* Z. Hao, J. Guo, K. Han, H. Hu, C. Xu, and Y. Wang (2023) Vanillakd: revisit the power of vanilla knowledge distillation from small scale to large scale. arXiv preprint arXiv:2305.15781. Cited by: SS1.\n' +
      '* Z. Hao, J. Guo, K. Han, Y. Tang, H. Hu, Y. Wang, and C. Xu (2023) One-for-all: bridge the gap between heterogeneous architectures in knowledge distillation. arXiv preprint arXiv:2310.19444. Cited by: SS1.\n' +
      '* K. He, X. Zhang, S. Ren, and J. Sun (2015) Delving deep into rectifiers: surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034. Cited by: SS1.\n' +
      '* K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick (2022) Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009. Cited by: SS1.\n' +
      '* K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick (2022) Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 16000-16009. Cited by: SS1.\n' +
      '* B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y. Choi (2019) A comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, Cited by: SS1.\n' +
      '* G. Hinton, O. Vinyals, and J. Dean (2015) Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. Cited by: SS1.\n' +
      '* J.\n' +
      '\n' +
      '* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural computation_, 1997.\n' +
      '* Kaelbling et al. (1996) Kaelbling, L. P., Littman, M. L., and Moore, A. W. Reinforcement learning: A survey. _Journal of artificial intelligence research_, 1996.\n' +
      '* Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* Krizhevsky et al. (2009) Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n' +
      '* Lee et al. (2013) Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on challenges in representation learning, ICML_, 2013.\n' +
      '* Li et al. (2020) Li, J., Socher, R., and Hoi, S. C. Dividemix: Learning with noisy labels as semi-supervised learning. _arXiv preprint arXiv:2002.07394_, 2020.\n' +
      '* Li et al. (2022) Li, S., Xia, X., Ge, S., and Liu, T. Selective-supervised contrastive learning with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 316-325, 2022.\n' +
      '* Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, 2014.\n' +
      '* Park et al. (2019) Park, W., Kim, D., Lu, Y., and Cho, M. Relational knowledge distillation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019.\n' +
      '* Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 2019.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* Romero et al. (2014) Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_, 2014.\n' +
      '* Russakovsky et al. (2015) Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.\n' +
      '* Sandler et al. (2018) Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '* Silver et al. (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. _nature_, 2016.\n' +
      '* Simonyan & Zisserman (2014) Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.\n' +
      '* Tanaka et al. (2018) Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. Joint optimization framework for learning with noisy labels. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '* Van Horn et al. (2018) Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 2017.\n' +
      '* Vinyals et al. (2016) Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* Xie et al. (2020) Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V. Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10687-10698, 2020.\n' +
      '* Zagoruyko & Komodakis (2016) Zagoruyko, S. and Komodakis, N. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.\n' +
      '* Zhang et al. (2018) Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018.\n' +
      '\n' +
      '* Zhao et al. (2022) Zhao, B., Cui, Q., Song, R., Qiu, Y., and Liang, J. Decoupled knowledge distillation. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, 2022.\n' +
      '\n' +
      '## Appendix A Implementation details.\n' +
      '\n' +
      '### ImageNet Classification\n' +
      '\n' +
      '**CIFAR-100.** We adopt the vision architectures of the teacher and student models as outlined in the traditional distillation papers (Hao et al., 2023; Zhao et al., 2022). It should be noted that the previous codebase (Zhao et al., 2022) conducted experiments on CIFAR-100 using only 1 GPU. To expedite our experiments, we leverage the _distributed_ Pytorch framework (Paszke et al., 2019) to train and do inference on 8 GPUs. Consequently, some hyperparameter settings and results may not align exactly with the previous paper. Specifically, we employ the SGD optimizer with a momentum of 0.9. The learning rate starts at 0.2 and decays with a minimum learning rate of 2e-3 using a cosine annealing schedule. We train for 240 epochs with a batch size of 512 spread across 8 GPUs, and apply a weight decay of 0.0005. Standard data augmentation techniques, including random resized crop and horizontal flip are utilized.\n' +
      '\n' +
      '**ImageNet.** we employ the SGD optimizer with a momentum of 0.9. The learning rate starts at 0.1 and decays with a rate of 0.1 every 30 epochs. We train for 100 epochs with a batch size of 512 spread across 8 GPUs, and apply a weight decay of 0.0001. Standard data augmentation techniques, including random resized crop, horizontal flip and label smoothing are utilized.\n' +
      '\n' +
      '### Transfer learning.\n' +
      '\n' +
      'To fine-tune the self-supervised pretrained ViT-B on ImageNet and iNaturalist, we adopt the hyperparameter settings from MAE (He et al., 2022). The adamw optimizer is employed for this purpose. The learning rate begins at 2e-3 and gradually decays with a minimum learning rate of 1e-6, utilizing a cosine annealing schedule. We conduct training for 100 epochs, utilizing a batch size of 4096 across 8 GPUs. A weight decay of 0.05 is applied to mitigate overfitting. The fine-tuning process incorporates robust data augmentation techniques, including auto-augment, mixup, cutmix, and stochastic drop path.\n' +
      '\n' +
      '### Few-shot Leearning\n' +
      '\n' +
      'We use ResNet12 and follow the setting of (Chen et al., 2021) on miniImageNet dataset, and created ResNet18 and ResNet36 by increasing the number of layers in original ResNet12. For the classification training stage, we use the SGD optimizer with momentum 0.9. The learning rate starts from 0.1 and the decay factor is set to 0.1. On miniImageNet, we train 100 epochs with the batch size of 128 on 4 GPUs, the learning rate decays at 90 epoch, and the weight decay is 0.0005. Standard data augmentation strategies including random resized crop and horizontal flip are applied. For meta-learning stage, we use the SGD optimizer with momentum 0.9. The learning rate is fixed as 0.001. The batch size is set to 4, _i.e._, each training batch contains 4 few-shot tasks to compute the average loss. The cosine scaling parameter \\(\\tau\\) is initialized as 10. For knowledge distillation, the kd loss weight is set to 1, the temperature is set to 10. We use the threshold with 8 and 0.25 for classifier stage and meta stage, respectively.\n' +
      '\n' +
      '### Learning with noisy labels\n' +
      '\n' +
      'For CIFAR-10/100 datasets, we follow (Li et al., 2022) use a PreAct ResNet18 network, and created PreAct ResNet34 by increasing the number of layers in PreAct ResNet12. We train our models using SGD with a momentum of 0.9, a weight decay of 1e-4, and a batch size of 128. The network is trained for 250 epochs and the warm-up epoch is set to 1 dufring training stage. We set the initial learning rate as 0.1, and reduce it by a factor of 10 after 125 and 200 epochs. The fine-tuning stage of Sel-CL+ has 70 epochs, where the learning rate is 0.001. We always set the Mixup hyperparameter to 1, scalar temperature to 0.1, and loss weights to 1. We try two settings of simulated noisy labels: symmetric and asymmetric. And the noise ratio is set to 0.2 and 0.4, respectively. For knowledge distillation, we set the threshold to 0.5 and assign a weight of 1 to the knowledge distillation loss.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>