<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# EscherNet: A Generative Model for Scalable View Synthesis\n' +
      '\n' +
      'Xin Kong\\({}^{1}\\) Shikun Liu\\({}^{1}\\) Xiaoyang Lyu\\({}^{2}\\) Marwan Taher\\({}^{1}\\)\n' +
      '\n' +
      'Xiaojuan Qi\\({}^{2}\\) Andrew J. Davison\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Dyson Robotics Lab, Imperial College London \\({}^{2}\\)The University of Hong Kong\n' +
      '\n' +
      '\\({}^{*}\\)Corresponding Authors: {x.kong21, shikun.liu17}@imperial.ac.uk\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce EscherNet, a multi-view conditioned diffusion model for view synthesis. EscherNet learns implicit and generative 3D representations coupled with a specialised camera positional encoding, allowing precise and continuous relative control of the camera transformation between an arbitrary number of reference and target views. EscherNet offers exceptional generality, flexibility, and scalability in view synthesis -- it can generate more than 100 consistent target views simultaneously on a single consumer-grade GPU, despite being trained with a fixed number of 3 reference views to 3 target views. As a result, EscherNet not only addresses zero-shot novel view synthesis, but also naturally unifies single- and multi-image 3D reconstruction, combining these diverse tasks into a single, cohesive framework. Our extensive experiments demonstrate that EscherNet achieves state-of-the-art performance in multiple benchmarks, even when compared to methods specifically tailored for each individual problem. This remarkable versatility opens up new directions for designing scalable neural architectures for 3D vision. Project page: [https://kxhit.github.io/EscherNet](https://kxhit.github.io/EscherNet).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'View synthesis stands as a fundamental task in computer vision and computer graphics. By allowing the re-rendering of a scene from arbitrary viewpoints based on a set of reference viewpoints, this mimics the adaptability observed inhuman vision. This ability is not only crucial for practical everyday tasks like object manipulation and navigation, but also plays a pivotal role in fostering human creativity, enabling us to envision and craft objects with depth, perspective, and a sense of immersion.\n' +
      '\n' +
      'In this paper, we revisit the problem of view synthesis and ask: _How can we learn a general 3D representation to facilitate scalable view synthesis?_ We attempt to investigate this question from the following two observations:\n' +
      '\n' +
      '**i)** Up until now, recent advances in view synthesis have predominantly focused on training speed and/or rendering efficiency [12, 18, 30, 47]. Notably, these advancements all share a common reliance on volumetric rendering for scene optimisation. Thus, all these view synthesis methods are inherently _scene-specific_, coupled with global 3D spatial coordinates. In contrast, we advocate for a paradigm shift where a 3D representation relies solely on scene colours and geometries, learning implicit representations without the need for ground-truth 3D geometry, while also maintaining independence from any specific coordinate system. This distinction is crucial for achieving scalability to overcome the constraints imposed by scene-specific encoding.\n' +
      '\n' +
      '**ii)** View synthesis, by nature, is more suitable to be cast as a _conditional generative modelling problem_, similar to generative image in-painting [25, 59]. When given only a sparse set of reference views, a desired model should provide multiple plausible predictions, leveraging the inherent stochasticity within the generative formulation and drawing insights from natural image statistics and semantic priors learned from other images and objects. As the available information increases, the generated scene becomes more constrained, gradually converging closer to the ground-truth representation. Notably, existing 3D generative models currently only support a single reference view [20, 21, 22, 43, 23]. We argue that a more desirable generative formulation should flexibly accommodate varying levels of input information.\n' +
      '\n' +
      'Building upon these insights, we introduce EscherNet, an image-to-image conditional diffusion model for view synthesis. EscherNet leverages a transformer architecture [50], employing dot-product self-attention to capture the intricate relation between both reference-to-target and target-to-target views consistencies. A key innovation within EscherNet is the design of camera positional encoding (CaPE), dedicated to representing both 4 DoF (object-centric) and 6 DoF camera poses. This encoding incorporates spatial structures into the tokens, enabling the model to compute self-attention between query and key solely based on their relative camera transformation. In summary, EscherNet exhibits these remarkable characteristics:\n' +
      '\n' +
      '* **Consistency**: EscherNet inherently integrates view consistency thanks to the design of camera positional encoding, encouraging both _reference-to-target and target-to-target view consistencies_.\n' +
      '* **Scalability**: Unlike many existing neural rendering methods that are constrained by scene-specific optimisation, EscherNet decouples itself from any specific coordinate system and the need for ground-truth 3D geometry, without any expensive 3D operations (_e.g._ 3D convolutions or volumetric rendering), making it easier to _scale with everyday posed 2D image data_.\n' +
      '* **Generalisation**: Despite being trained on only a fixed number of 3 reference to 3 target views, EscherNet exhibits the capability to generate _any number of target views, with any camera poses, based on any number of reference views_. Notably, EscherNet exhibits improved generation quality with an increased number of reference views, aligning seamlessly with our original design goal.\n' +
      '\n' +
      'We conduct a comprehensive evaluation across both novel view synthesis and single/multi-image 3D reconstruction benchmarks. Our findings demonstrate that EscherNet not only outperforms all 3D diffusion models in terms of generation quality but also can generate plausible view synthesis given very limited views. This stands in contrast to these scene-specific neural rendering methods such as InstantNGP [30] and Gaussian Splatting [18], which often struggle to generate meaningful content under such constraints. This underscores the effectiveness of our method\'s simple yet scalable design, offering a promising avenue for advancing view synthesis and 3D vision as a whole.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Neural 3D RepresentationsEarly works in neural 3D representation learning focused on directly optimising on 3D data, using representations such as voxels [26] and point clouds [39, 40], for explicit 3D representation learning. Alternatively, another line of works focused on training neural networks to map 3D spatial coordinates to signed distance functions [34] or occupancies [28, 36], for implicit 3D representation learning. However, all these methods heavily rely on ground-truth 3D geometry, limiting their applicability to small-scale synthetic 3D data [2, 54].\n' +
      '\n' +
      'To accommodate a broader range of data sources, differentiable rendering functions [32, 45] have been introduced to optimise neural implicit shape representations with multi-view posed images. More recently, NeRF [29] paved the way to a significant enhancement in rendering quality compared to these methods by optimising MLPs to encode 5D radiance fields. In contrast to tightly coupling 3D scenes with spatial coordinates, we introduce EscherNet as an alternative for 3D representation learning by optimising a neural network to learn the interaction between multi-view posed images, independent of any coordinate system.\n' +
      '\n' +
      'Novel View SynthesisThe success of NeRF has sparked a wave of follow-up methods that address faster training and/or rendering efficiency, by incorporating different variants of space discretisation [3, 12, 14], codebooks [48], and encodings using hash tables [30] or Gaussians [18].\n' +
      '\n' +
      'To enhance NeRF\'s generalisation ability across diverse scenes and in a few-shot setting, PixelNeRF [58] attempts to learn a scene prior by jointly optimising multiple scenes, but it is constrained by the high computational demands required by volumetric rendering. Various other approaches have addressed this issue by introducing regularisation techniques, such as incorporating low-level priors from local patches [33], ensuring semantic consistency [16], considering adjacent ray frequency [56], and incorporating depth signals [9]. In contrast, EscherNet encodes scenes directly through the image space, enabling the learning of more generalised scene priors through large-scale datasets.\n' +
      '\n' +
      '3D Diffusion ModelsThe emergence of 2D generative diffusion models has shown impressive capabilities in generating realistic objects and scenes [15, 42]. This progress has inspired the early design of text-to-3D diffusion models, such as DreamFusion [38] and Magic3D [19], by optimising a radiance field guided by score distillation sampling (SDS) from these pre-trained 2D diffusion models. However, SDS necessitates computationally intensive iterative optimisation, often requiring up to an hour for convergence. Additionally, these methods, including recently proposed image-to-3D generation approaches [8, 27, 55], frequently yield unrealistic 3D generation results due to their limited 3D understanding, giving rise to challenges such as the multi-face Janus problem.\n' +
      '\n' +
      'To integrate 3D priors more efficiently, an alternative approach involves training 3D generative models directly on 3D datasets, employing representations like point clouds [31] or neural fields [4, 11, 17]. However, this design depends on 3D operations, such as 3D convolution and volumetric rendering, which are computationally expensive and challenging to scale.\n' +
      '\n' +
      'To address this issue, diffusion models trained on multi-view posed data have emerged as a promising direction, designed with no 3D operations. Zero-1-to-3 [21] stands out as a pioneering work, learning view synthesis from paired 2D posed images rendered from large-scale 3D object datasets [6, 7]. However, its capability is limited to generating a single target view conditioned on a single reference view. Recent advancements in multi-view diffusion models [20, 22, 43, 44, 57] focused on 3D generation and can only generate a fixed number of target views with fixed camera poses. In contrast, EscherNet can generate an unrestricted number of target views with arbitrary camera poses, offering superior flexibility in view synthesis.\n' +
      '\n' +
      '## 3 EscherNet\n' +
      '\n' +
      'Problem Formulation and NotationIn EscherNet, we recast the view synthesis as a conditional generative modelling problem, formulated as:\n' +
      '\n' +
      '\\[\\mathcal{X}^{T}\\sim p(\\mathcal{X}^{T}|\\mathcal{X}^{R},\\mathcal{P}^{R}, \\mathcal{P}^{T}). \\tag{1}\\]\n' +
      '\n' +
      'Here, \\(\\mathcal{X}^{T}=\\{\\mathbf{X}_{1:M}^{T}\\}\\) and \\(\\mathcal{P}^{T}=\\{\\mathbf{P}_{1:M}^{T}\\}\\) represent a set of \\(M\\) target views \\(\\mathbf{X}_{1:M}^{T}\\) with their global camera poses \\(\\mathbf{P}_{1:M}^{T}\\). Similarly, \\(\\mathcal{X}^{R}=\\{\\mathbf{X}_{1:N}^{R}\\}\\) and \\(\\mathcal{P}^{R}=\\{\\mathbf{P}_{1:N}^{R}\\}\\) represent a set of \\(N\\) reference views \\(\\mathbf{X}_{1:N}^{R}\\) with their global camera poses \\(\\mathbf{P}_{1:N}^{R}\\). Both \\(N\\) and \\(M\\) can take on arbitrary values during both model training and inference.\n' +
      '\n' +
      'We propose a neural architecture design, such that the generation of each target view \\(\\mathbf{X}_{i}^{T}\\in\\mathcal{X}^{T}\\) solely depends on its relative camera transformation to the reference views \\((\\mathbf{P}_{j}^{R})^{-1}\\mathbf{P}_{i}^{T},\\forall\\mathbf{P}_{j}^{R}\\in \\mathcal{P}^{R}\\), introduced next.\n' +
      '\n' +
      '### Architecture Design\n' +
      '\n' +
      'We design EscherNet following two key principles: i) It builds upon an existing 2D diffusion model, inheriting its strong web-scale prior through large-scale training, and ii) It encodes camera poses for each view/image, similar to how language models encode token positions for each token. So our model can naturally handle an arbitrary number of views for _any-to-any view synthesis_.\n' +
      '\n' +
      'Multi-View GenerationEscherNet can be seamlessly integrated with any 2D diffusion model with a transformer architecture, with _no additional learnable parameters_. In this work, we design EscherNet by adopting a latent diffusion architecture, specifically StableDiffusion v1.5 [42]. This choice enables straightforward comparisons with numerous 3D diffusion models that also leverage the same backbone (more details in the experiment section).\n' +
      '\n' +
      'To tailor the Stable Diffusion model, originally designed for text-to-image generation, to multi-view generation as\n' +
      '\n' +
      'Figure 2: **3D representations overview.** EscherNet generates a set of \\(M\\) target views \\(\\mathbf{X}_{1:M}^{T}\\) based on their camera poses \\(\\mathbf{P}_{1:M}^{T}\\), leveraging information gained from a set of \\(N\\) reference views \\(\\mathbf{X}_{1:N}^{R}\\) and their camera poses \\(\\mathbf{P}_{1:N}^{R}\\). EscherNet presents a new way of learning implicit 3D representations by only considering the relative camera transformation between the camera poses of \\(\\mathbf{P}^{R}\\) and \\(\\mathbf{P}^{T}\\), making it easier to scale with multi-view posed images, independent of any specific coordinate systems.\n' +
      '\n' +
      'applied in EscherNet, several key modifications are implemented. In the original Stable Diffusion\'s denoiser U-Net, the self-attention block was employed to learn interactions within different patches within the same image. In EscherNet, we re-purpose this self-attention block to facilitate learning interactions within distinct patches across \\(M\\) different target views, thereby ensuring target-to-target consistency. Likewise, the cross-attention block, originally used to integrate textual information into image patches, is repurposed in EscherNet to learn interactions within \\(N\\) reference to \\(M\\) target views, ensuring reference-to-target consistency.\n' +
      '\n' +
      'Conditioning Reference ViewsIn view synthesis, it is crucial that the conditioning signals accurately capture both the high-level semantics and low-level texture details present in the reference views. Previous works in 3D diffusion models [21, 22] have employed the strategy of encoding high-level signals through a frozen CLIP pre-trained ViT [41] and encoding low-level signals by concatenating the reference image into the input of the U-Net of Stable Diffusion. However, this design choice inherently constrains the model to handle only one single view.\n' +
      '\n' +
      'In EscherNet, we choose to incorporate both high-level and low-level signals in the conditioning image encoder, representing reference views as sets of tokens. This design choice allows our model to maintain flexibility in handling a variable number of reference views. Early experiments have confirmed that using a frozen CLIP-ViT alone may fail to capture low-level textures, preventing the model from accurately reproducing the original reference views given the same reference view poses as target poses. While fine-tuning the CLIP-ViT could address this issue, it poses challenges in terms of training efficiency. Instead, we opt to fine-tune a lightweight vision encoder, specifically ConvNeXtv2-Tiny [53], which is a highly efficient CNN architecture. This architecture is employed to compress our reference views to smaller resolution image features. We treat these image features as conditioning tokens, effectively representing each reference view. This configuration has proven to be sufficient in our experiments, delivering superior results in generation quality while simultaneously maintaining high training efficiency.\n' +
      '\n' +
      '### Camera Positional Encoding (CaPE)\n' +
      '\n' +
      'To encode camera poses efficiently and accurately into reference and target view tokens within a transformer architecture, we introduce Camera Positional Encoding (CaPE), drawing inspiration from recent advancements in the language domain. We first briefly examine the distinctions between these two domains.\n' +
      '\n' +
      '- In language, token positions (associated with each word) follow a _linear and discrete_ structure, and their length can be _infinite_. Language models are typically trained with fixed maximum token counts (known as context length), and it remains an ongoing research challenge to construct a positional encoding that enables the model to behave reasonably beyond this fixed context length [13, 35].\n' +
      '\n' +
      '- In 3D vision, token positions (associated with each camera) follow a _cyclic, continuous, and bounded_ structure for rotations and a _linear, continuous, and unbounded_ structure for translations. Importantly, unlike the language domain where the token position always starts from zero, there are no _standardised absolute global camera poses_ in\n' +
      '\n' +
      'Figure 3: **EscherNet architecture details.** EscherNet adopts the Stable Diffusion architectural design with minimal but important modifications. The lightweight vision encoder captures both high-level and low-level signals from \\(N\\) reference views. In U-Net, we apply self-attention within \\(M\\) target views to encourage target-to-target consistency, and cross-attention within \\(M\\) target and \\(N\\) reference views (encoded by the image encoder) to encourage reference-to-target consistency. In each attention block, CaPE is employed for the key and query, allowing the attention map to learn with relative camera poses, independent of specific coordinate systems.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '### Results on Novel View Synthesis\n' +
      '\n' +
      'We evaluate EscherNet in novel view synthesis on the Google Scanned Objects dataset (GSO) [10] and the RTMV dataset [49], comparing with 3D diffusion models for view synthesis, such as Zero-1-to-3 [21] and RealFusion [27] (primarily for generation quality with minimal reference views). Additionally, we also evaluate on NeRF Synthetic Dataset [29], comparing with state-of-the-art scene-specific neural rendering methods, such as InstantNGP [30] and 3D Gaussian Splatting [18] (primarily for rendering accuracy with multiple reference views).\n' +
      '\n' +
      'Notably, many other 3D diffusion models [20, 22, 23, 43, 57] prioritise 3D generation rather than view synthesis. This limitation confines them to predicting target views with _fixed target poses_, making them not directly comparable.\n' +
      '\n' +
      'Compared to 3D Diffusion ModelsIn Tab. 1 and Fig. 5, we show that EscherNet significantly outperforms 3D diffusion baselines, by a large margin, both quantitatively and qualitatively. Particularly, we outperform Zero-1-to-3-XL despite it being trained on \\(\\times 10\\) more training data, and RealFusion despite it requiring expensive score distillation for iterative scene optimisation [38]. It\'s worth highlighting that Zero-1-to-3 by design is inherently limited to generating a single target view and cannot ensure self-consistency across multiple target views, while EscherNet can generate multiple consistent target views jointly and provides more precise camera control.\n' +
      '\n' +
      'Compared to Neural Rendering MethodsIn Tab. 2 and Fig. 4, we show that EscherNet again offers plausible view synthesis in a zero-shot manner, without scene-specific optimisation required by both InstantNGP and 3D Gaussian Splatting. Notably, EscherNet leverages a generalised understanding of objects acquired through large-scale training, allowing it to interpret given views both semantically and spatially, even when conditioned on a limited number of reference views. However, with an increase in the number of reference views, both InstantNGP and 3D Gaussian Splatting exhibit a significant improvement in the rendering quality. To achieve a photo-realistic neural rendering while retaining the advantages of a generative formulation remains an important research challenge.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{Training \\# Ref.} & \\multicolumn{3}{c}{**GSO-30**} & \\multicolumn{3}{c}{**RTMV**} \\\\  & Data & Views & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline RealFusion & - & 1 & 12.76 & 0.758 & 0.382 & - & - & - \\\\ Zero123 & 800K & 1 & 18.51 & 0.856 & 0.127 & 10.16 & 0.505 & 0.418 \\\\ Zero123-XL & 10M & 1 & 18.93 & 0.856 & 0.124 & 10.59 & 0.520 & 0.401 \\\\ \\hline EscherNet & 800k & 1 & 20.24 & 0.884 & 0.095 & 10.56 & 0.518 & 0.410 \\\\ EscherNet & 800k & 2 & 22.91 & 0.908 & 0.064 & 12.66 & 0.585 & 0.301 \\\\ EscherNet & 800k & 3 & 24.09 & 0.918 & 0.052 & 13.59 & 0.611 & 0.258 \\\\ EscherNet & 800k & 5 & 25.09 & 0.927 & 0.043 & 14.52 & 0.633 & 0.222 \\\\ EscherNet & 800k & 10 & 25.90 & 0.935 & 0.036 & 15.55 & 0.657 & 0.185 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Novel view synthesis performance on GSO and RTMV datasets.** EscherNet outperforms Zero-1-to-3-XL with significantly less training data and RealFusion without extra SDS optimisation. Additionally, EscherNet’s performance exhibits further improvement with the inclusion of more reference views.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{\\# Reference Views (Less \\(\\rightarrow\\) More)} & & & \\\\  & 1 & 2 & 3 & 5 & 10 & 20 & 50 & 100 \\\\ \\hline \\multicolumn{8}{l}{**InstantNGP (Scene Specific Training)**} & & & \\\\ PSNR\\(\\uparrow\\) & 10.92 & 12.42 & 14.27 & 18.17 & 22.96 & 24.99 & 26.86 & 27.30 \\\\ SSIM\\(\\uparrow\\) & 0.449 & 0.521 & 0.618 & 0.761 & 0.881 & 0.917 & 0.946 & 0.953 \\\\ LPIPS\\(\\downarrow\\) & 0.627 & 0.499 & 0.391 & 0.228 & 0.091 & 0.058 & 0.034 & 0.031 \\\\ \\hline \\multicolumn{8}{l}{**GaussianSplitating (Scene Specific Training)**} & & & \\\\ PSNR \\(\\uparrow\\) & 9.44 & 10.78 & 12.87 & 17.09 & 23.04 & 25.34 & 26.98 & 27.11 \\\\ SSIM\\(\\uparrow\\) & 0.391 & 0.432 & 0.546 & 0.732 & 0.876 & 0.919 & 0.942 & 0.944 \\\\ LPIPS\\(\\downarrow\\) & 0.610 & 0.541 & 0.441 & 0.243 & 0.085 & 0.054 & 0.041 & 0.041 \\\\ \\hline \\multicolumn{8}{l}{**EscherNet (Zero Shot Inference)**} & & & \\\\ PSNR\\(\\uparrow\\) & 13.36 & 14.95 & 16.19 & 17.16 & 17.74 & 17.91 & 18.05 & 18.15 \\\\ SSIM\\(\\uparrow\\) & 0.659 & 0.700 & 0.729 & 0.748 & 0.761 & 0.765 & 0.769 & 0.771 \\\\ LPIPS\\(\\downarrow\\) & 0.291 & 0.208 & 0.161 & 0.127 & 0.114 & 0.106 & 0.099 & 0.097 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Novel view synthesis performance on NeRF Synthetic dataset.** EscherNet outperforms both InstantNGP and Gaussian Splatting when provided with fewer than five reference views while requiring no scene-specific optimisation. However, as the number of reference views increases, both methods show a more significant improvement in rendering quality.\n' +
      '\n' +
      'Figure 4: **Generated views visualisation on the NeRF Synthetic drum scene.** EscherNet generates plausible view synthesis even when provided with very limited reference views, while neural rendering methods fail to generate any meaningful content. However, when we have more than 10 reference views, scene-specific methods exhibit a substantial improvement in rendering quality. We report the mean PSNR averaged across all test views from the drum scene. Results for other scenes and/or with more reference views are shown in Appendix D.\n' +
      '\n' +
      '### Results on 3D Generation\n' +
      '\n' +
      'In this section, we perform single/few-image 3D generation on the GSO dataset. We compare with SoTA 3D generation baselines: Point-E [31] for direct point cloud generation, Shape-E [17] for direct NeRF generation, DreamGaussian [17] for optimising 3D Gaussian [18] with SDS guidance, One-2-3-45 [20] for decoding an SDF using multiple views predicted from Zero-1-to-3, and SyncDreamer [22] for fitting an SDF using NeuS [51] from 16 consistent fixed generated views. We additionally include NeuS trained on reference views for few-image 3D reconstruction baselines.\n' +
      '\n' +
      'Given any reference views, EscherNet can generate multiple 3D consistent views, allowing for the straightforward adoption with NeuS [51] for 3D reconstruction. We generate 36 fixed views, varying the azimuth from 0\\({}^{\\circ}\\) to 360\\({}^{\\circ}\\) with a rendering every 30\\({}^{\\circ}\\) at a set of elevations (-30\\({}^{\\circ}\\), 0\\({}^{\\circ}\\), 30\\({}^{\\circ}\\)), which serve as inputs for our NeuS reconstruction.\n' +
      '\n' +
      'ResultsIn Tab. 3 and Fig. 6, we show that EscherNet stands out by achieving significantly superior 3D reconstruction quality compared to other image-to-3D generative models. Specifically, EscherNet demonstrates an approximate 25% improvement in Chamfer distance over SyncDreamer, considered as the current best model, when conditioned on a single reference view, and a 60% improvement when conditioned on 10 reference views. This impressive performance is attributed to EscherNet\'s ability to flexibly handle any number of reference and target views, providing comprehensive and accurate constraints for 3D geometry. In contrast, SyncDreamer faces challenges due to sensitivity to elevation angles and constraints imposed by a fixed 30\\({}^{\\circ}\\) elevation angle by design, thus hindering learning a holistic representation of complex objects. This limitation results in degraded reconstruction, particularly evident in the lower regions of the generated geometry.\n' +
      '\n' +
      '### Results on Text-to-3D Generation\n' +
      '\n' +
      'EscherNet\'s flexibility in accommodating any number of reference views enables a straightforward approach to the\n' +
      '\n' +
      'Figure 5: **Novel view synthesis visualisation on GSO and RTMV datasets.** EscherNet outperforms Zero-1-to-3-XL, delivering superior generation quality and finer camera control. Notably, when conditioned with additional views, EscherNet exhibits an enhanced resemblance of the generated views to ground-truth textures, revealing more refined texture details such as in the backpack straps and turtle shell.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\# Ref. Views & Chamfer Dist. \\(\\downarrow\\) & Volume IoU \\(\\uparrow\\) \\\\ \\hline Point-E & 1 & 0.0447 & 0.2503 \\\\ Shape-E & 1 & 0.0448 & 0.3762 \\\\ One2345 & 1 & 0.0632 & 0.4209 \\\\ One2345 XL & 1 & 0.0667 & 0.4016 \\\\ DreamGaussian & 1 & 0.0605 & 0.3757 \\\\ DreamGaussian XL & 1 & 0.0459 & 0.4531 \\\\ SyncDreamer & 1 & 0.0400 & 0.5220 \\\\ \\hline NeuS & 3 & 0.0366 & 0.5352 \\\\ NeuS & 5 & 0.0245 & 0.6742 \\\\ NeuS & 10 & 0.0195 & 0.7264 \\\\ \\hline EscherNet & 1 & 0.0314 & 0.5974 \\\\ EscherNet & 2 & 0.0215 & 0.6868 \\\\ EscherNet & 3 & 0.0190 & 0.7189 \\\\ EscherNet & 5 & 0.0175 & 0.7423 \\\\ EscherNet & 10 & 0.0167 & 0.7478 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **3D reconstruction performance on GSO.** EscherNet outperforms all other image-to-3D baselines in generating more visually appealing with accurate 3D geometry, particularly when conditioned on multiple reference views.\n' +
      '\n' +
      'text-to-3D generation problem by breaking it down into two stages: text-to-image, relying on any off-the-shelf text-to-image generative model, and then image-to-3D, relying on EscherNet. In Fig. 7, we present visual results of dense novel view generation using a text-to-4view model with MVDream [44] and a text-to-image model with SDXL [37]. Remarkably, even when dealing with out-of-distribution and counterfactual content, EscherNet generates consistent 3D novel views with appealing textures.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      'In this paper, we have introduced EscherNet, a multi-view conditioned diffusion model designed for scalable view synthesis. Leveraging Stable Diffusion\'s 2D architecture empowered by the innovative Camera Positional Embedding (CaPE), EscherNet adeptly learns implicit 3D representations from varying number of reference views, achieving consistent 3D novel view synthesis. We provide detailed discussions and additional ablative analysis in Appendix F.\n' +
      '\n' +
      'Limitations and DiscussionsEscherNet\'s flexibility in handling any number of reference views allows for autoregressive generation, similar to autoregressive language models [1, 5]. While this approach significantly reduces inference time, it leads to a degraded generation quality. Additionally, EscherNet\'s current capability operates within a 3 DoF setting constrained by its training dataset, which may not align with real-world scenarios, where views typically span in \\(SE(3)\\) space. Future work will explore scaling EscherNet with 6 DoF training data with real-world scenes, striving for a more general 3D representation.\n' +
      '\n' +
      'Figure 6: **Single view 3D reconstruction visualisation on GSO.** EscherNet’s ability to generate dense and consistent novel views significantly improves the reconstruction of complete and well-constrained 3D geometry. In contrast, One-2-3-45-XL and DreamGaussian-XL, despite leveraging a significantly larger pre-trained model, tend to produce over-smoothed and noisy reconstructions; SyncDreamer, constrained by sparse fixed-view synthesis, struggles to tightly constrain geometry, particularly in areas in sofa and the bottom part of the bell.\n' +
      '\n' +
      'Figure 7: **Text-to-3D visualisation with MVDream (up) and SDXL (bottom).** EscherNet offers compelling and realistic view synthesis for synthetic images generated with user-provided text prompts. Additional results are shown in Appendix E.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      'The research presented here was supported by Dyson Technology Ltd. Xin Kong holds a China Scholarship Council-Imperial Scholarship. We would like to thank Sayak Paul and HuggingFace for contributing the training compute that facilitated early project exploration. We would also like to acknowledge Yifei Ren for his valuable discussions on formulating the 6DoF CaPE.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. _arXiv preprint arXiv:2304.06714_, 2023.\n' +
      '* [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. _arXiv preprint arXiv:2307.05663_, 2023.\n' +
      '* [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerf: Single-view nerf synthesis with language-guided diffusion as general image priors. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In _Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)_. IEEE, 2022.\n' +
      '* [11] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* [12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. _arXiv preprint arXiv:2308.16137_, 2023.\n' +
      '* [14] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [16] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [17] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. _arXiv preprint arXiv:2305.02463_, 2023.\n' +
      '* [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics (TOG)_, 2023.\n' +
      '* [19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [20] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16928_, 2023.\n' +
      '* [21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [22] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncedreamer: Generating multiview-consistent images from a single-view image. _arXiv preprint arXiv:2309.03453_, 2023.\n' +
      '* [23] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. _arXiv preprint arXiv:2310.15008_, 2023.\n' +
      '* [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [25] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [26] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In _Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)_, 2015.\n' +
      '* [27] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [30] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 2022.\n' +
      '* [31] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [32] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [33] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [34] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [35] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarm: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* [36] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [40] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [43] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyei Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model. _arXiv preprint arXiv:2310.15110_, 2023.\n' +
      '* [44] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [45] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* [46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* [47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [48] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Muller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable bitrate neural fields. In _Proceedings of SIGGRAPH_, 2022.\n' +
      '* [49] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Muller,Charles Loop, Nathan Morrical, Koki Nagano, et al. Rtmv: A ray-traced multi-view synthetic dataset for novel view synthesis. _arXiv preprint arXiv:2205.07058_, 2022.\n' +
      '* [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [51] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.\n' +
      '* [52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 2004.\n' +
      '* [53] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [54] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2015.\n' +
      '* [55] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [56] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [57] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. _arXiv preprint arXiv:2310.03020_, 2023.\n' +
      '* [58] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [59] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5505-5514, 2018.\n' +
      '* [60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '\n' +
      '## Appendix A Python Implementation of CaPE\n' +
      '\n' +
      '```\n' +
      'defcompute_4dof_cape(v,P,s):""" :paramv:inputfeaturevectorwithitsdimensionmustbedivisibleby8 :paramP:list=[alpha,beta,gamma,r] :params:asmallscalarforradius :return:rotatedvwithitscorrespondingcameraposeP """ v=v.reshape([-1,8]) psi=np.zeros([8,8]) foriinrange(4): ifi<3: psi[2=s:i2*(i+1),2*i:2*(i+1)]=\\ psi[2=v.array([(np.cos(P[i]),-np.sin(P[i])],[np.sin(P[i]),np.cos(P[i])]]) else: psi[2*i:2*(i+1),2*i:2*(i+1)]=\\ psi[(np.cos(s*np.log(P[i])),-np.sin(s*np.log(P[i]))],[np.sin(s*np.log(P[i])),np.cos(s*np.log(P[i]))]]) returnv.dot(psi).reshape(-1)\n' +
      '```\n' +
      '\n' +
      'Listing 1: Python implementation for 4 DoF CaPE.\n' +
      '\n' +
      '```\n' +
      'defcompute_6dof_cape(v,P,s=0.001,key=True):""" :paramv:inputfeaturevectorwithitsdimensionmustbedivisibleby4 :paramP:4x483matrix :params:asmallscalarfortranslation :return:rotatedvwithitscorrespondingcameraposeP """ v=v.reshape([-1,4]) V=v.reshape([-1,4]) P[i3,3]+=s psi=Pifkeyelsemp.linalg.inv(P).T returnv.dot(psi).reshape(-1)\n' +
      '```\n' +
      '\n' +
      'Listing 2: Python implementation for 6 DoF CaPE.\n' +
      '\n' +
      '## Appendix B Additional Training Details and Experimental Settings\n' +
      '\n' +
      'Optimisation and ImplementationEscherNet is trained using the AdamW optimiser [24] with a learning rate of \\(1\\cdot 10^{-4}\\) and weight decay of \\(0.01\\) for \\([256\\times 256]\\) resolution images. We incorporate cosine annealing, reducing the learning rate to \\(1\\cdot 10^{-5}\\) over a total of 100,000 training steps, while linearly warming up for the initial 1000 steps. To speed up training, we implement automatic mixed precision with a precision of bf16 and employ gradient checkpointing. Our training batches consist of 3 reference views and 3 target views randomly sampled with replacement from 12 views for each object, with a total batch size of 672 (112 batches per GPU). The entire model training process takes 1 week on 6 NVIDIA A100 GPUs.\n' +
      '\n' +
      'MetricsFor 2D metrics used in view synthesis, we employ PSNR, SSIM [52], LPIPS [60]. For 3D metrics used in 3D generation, we employ Chamfer Distance and Volume IoU. To ensure a fair and efficient evaluation process, each baseline method and our approach are executed only once per scene per viewpoint. This practice has proven to provide stable averaged results across multiple scenes and viewpoints.\n' +
      '\n' +
      '### Evaluation Details\n' +
      '\n' +
      'In NeRF Synthetic Dataset [29],we consider and evaluate all 8 scenes provided in the original dataset. To assess performance with varying numbers of reference views, we train all baseline methods and our approach using the same set of views randomly sampled from the training set. The evaluation is conducted on all target views defined in the test sets across all 8 scenes (with 200 views per scene). For InstantNGP [30], we run 10k steps (\\(\\approx\\) 1min) for each scene. For 3D Gaussian Splatting [18], we run 5k steps (\\(\\approx\\) 2min) for each scene.\n' +
      '\n' +
      'In Google Scanned Dataset (GSO) [10],we evaluate the same 30 objects chosen by SyncDreamer [22]. For each object, we render 25 views with randomly generated camera poses and a randomly generated environment lighting condition to construct our test set. For each object, we choose the first 10 images as our reference views and the subsequent 15 images as our target views for evaluation. It\'s crucial to note that all reference and target views are rendered with random camera poses, establishing a more realistic and challenging evaluation setting compared to the evaluation setups employed in other baselines: _e.g._ SyncDreamer uses an evenly distributed environment lighting to render all GSO data, and the reference view for each object is manually selected based on human preference.1 Additionally, the evaluated target view is also manually selected based on human preference chosen among four independent generations.2\n' +
      '\n' +
      'Footnote 1: [https://github.com/liuyuan-pal/SyncDreamer/issues/21](https://github.com/liuyuan-pal/SyncDreamer/issues/21)\n' +
      '\n' +
      'Footnote 2: [https://github.com/liuyuan-pal/SyncDreamer/issues/21#issuecomment-1770345260](https://github.com/liuyuan-pal/SyncDreamer/issues/21#issuecomment-1770345260)\n' +
      '\n' +
      'In evaluating 3D generation, we randomly sample 4096 points evenly distributed from the generated 3D mesh or point cloud across all methods. Each method\'s generated mesh is aligned to the ground-truth mesh using the camera pose of the reference views. Specifically in Point-E [31] and Shape-E [17], we rotate 90/180 degrees along each x/y/z axis to determine the optimal alignment for the final mesh pose. Our evaluation approach again differs from SyncDreamer, which initially projects the 3D mesh into their fixed 16 generated views to obtain depth maps. Then, points are sampled from these depth maps for the final evaluation.3\n' +
      '\n' +
      'Footnote 3: [https://github.com/liuyuan-pal/SyncDreamer/issues/44](https://github.com/liuyuan-pal/SyncDreamer/issues/44)\n' +
      '\n' +
      'In RTMV Dataset [49],we follow the evaluation setting used in Zero-1-to-3 [21], which consists of 10 complex scenes featuring a pile of multiple objects from the GSO dataset. Similar to the construction of our GSO test set, we then randomly select a fixed subset of the first 10 images as our reference views and the subsequent 10 views as our target views for evaluation.\n' +
      '\n' +
      '## Appendix C Additional Results on 6 DoF CaPE\n' +
      '\n' +
      'To validate the effectiveness of the 6 DoF CaPE design, we demonstrate its performance in novel view synthesis on GSO and RTMV datasets in Tab. 3(a) and on the NeRF Synthetic dataset in Tab. 3(c). We also provide 3D reconstruction results on GSO dataset in Tab. 3(b). It is evident that EscherNet with 6 DoF CaPE achieves comparable, and often, slightly improved results when compared to our 4 DoF CaPE design.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 4: EscherNet 6 DoF presents a similar and sometimes improved performance than EscherNet 4 DoF.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      'E Additional Results on Text-to-3D\n' +
      '\n' +
      'We present additional visualisation on text-to-image-to-3D using EscherNet trained with 4 DoF CaPE.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline \\multicolumn{1}{c}{A robot made of vegetables.} \\\\ \\hline A nurse cogi. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline A cute steamunk elephant. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline A bull dog wearing a black pirate hat. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline An astronaut riding a horse. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline Medieval House, grass, medieval, & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline medieval-decor, 3d asset. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Text-to-3D generation with SDXL (top 3) and MVDream (bottom 3).\n' +
      '\n' +
      '## Appendix F Additional Discussions, Limitations and Future Work\n' +
      '\n' +
      'Direct v.s. Autoregressive GenerationEscherNet\'s flexibility in handling arbitrary numbers of reference and target views offers multiple choices for view synthesis. In our experiments, we employ the straightforward direct generation to jointly generate all target views. Additionally, an alternative approach is autoregressive generation, where target views are generated sequentially, similar to text generation with autoregressive language models.\n' +
      '\n' +
      'For generating a large number of target views, autoregressive generation can be significantly faster than direct generation (_e.g._ more than \\(20\\times\\) faster for generating 200 views). This efficiency gain arises from converting a quadratic inference cost into a linear inference cost in each self-attention block. However, it\'s important to note that autoregressive generation may encounter a _content drifting problem_ in our current design, where the generated quality gradually decreases as each newly generated view depends on previously non-perfect generated views. Autoregressive generation boasts many advantages in terms of inference efficiency and is well-suited for specific scenarios like SLAM (Simultaneous Localization and Mapping). As such, enhancing rendering quality in such a setting represents an essential avenue for future research.\n' +
      '\n' +
      'Stochasticity and Consistency in Multi-View GenerationWe also observe that to enhance the target view synthesis quality, especially when conditioning on a limited number of reference views, introducing additional target views can be highly beneficial. These supplementary target views can either be randomly defined or duplicates with the identical target camera poses. Simultaneously generating multiple target views serves to implicitly reduce the inherent stochasticity in the diffusion process, resulting in improved generation quality and consistency. Through empirical investigations, we determine that the optimal configuration ensures a minimum of 15 target views, as highlighted in orange in Fig. 8. Beyond this threshold, any additional views yield marginal performance improvements.\n' +
      '\n' +
      'Training Data Sampling StrategyWe have explored various combinations of \\(N\\in\\{1,2,3,4,5\\}\\) reference views and \\(M\\in\\{1,2,3,4,5\\}\\) target views during EscherNet training. Empirically, a larger number of views demand more GPU memory and slow down training speed, while a smaller number of views may restrict the model\'s ability to learn multi-view correspondences. To balance training efficiency and performance, we set our training views to \\(N=3\\) reference views and \\(M=3\\) target views for each object, a configuration that has proven effective in practice. Additionally, we adopt a random sampling approach with replacement for these 6 views, introducing the possibility of repeated images in the training views. This sampling strategy has demonstrated a slight improvement in performance compared to sampling without replacement.\n' +
      '\n' +
      'Scaling with Multi-view VideoEscherNet\'s flexibility sets it apart from other multi-view diffusion models [22, 23] that require a set of fixed-view rendered images from 3D datasets for training. EscherNet can efficiently construct training samples using just a pair of posed images. While it can benefit from large-scale 3D datasets like [6, 7], EscherNet\'s adaptability extends to a broader range of posed image sources, including those directly derived from videos. Scaling EscherNet to accommodate multiple data sources is an important direction for future research.\n' +
      '\n' +
      'Figure 8: **Novel view synthesis with a different number of reference and target views.** We present the averaged performance of EscherNet on _one_ pre-selected target view across objects in the GSO dataset. We observe a clear improvement in view synthesis quality as the number of both reference and target views increases. In this scenario, the multiple target views are essentially multiple duplicates of the initially chosen single pre-selected view, a strategy we find effective in enhancing view synthesis quality.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>