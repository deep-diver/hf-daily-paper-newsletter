<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# EscherNet: 확장 가능한 뷰 합성을 위한 생성 모델\n' +
      '\n' +
      '신공\\({}^{1}\\) 시쿤 류\\({}^{1}\\) 샤오양 류\\({}^{2}\\) 마르완 태어\\({}^{1}\\)\n' +
      '\n' +
      '샤오주안기({}^{2}\\) Andrew J. Davison\\({}^{1}\\)\n' +
      '\n' +
      '홍콩 런던대학교 다이슨 로봇 연구실\n' +
      '\n' +
      '[x.kong21, shikun.liu17}@imperial.ac.uk.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '뷰 합성을 위한 다중 뷰 조건 확산 모델인 EscherNet을 소개한다. 에셔넷은 전문화된 카메라 위치 인코딩과 결합된 암시적 및 생성적 3D 표현을 학습하여 임의의 수의 참조 뷰와 타겟 뷰 사이의 카메라 변환의 정밀하고 연속적인 상대 제어를 허용한다. 에셔넷은 뷰 합성에서 탁월한 일반성, 유연성 및 확장성을 제공하며, 3개의 참조 뷰에서 3개의 목표 뷰로 고정된 수로 훈련됨에도 불구하고 단일 소비자 등급 GPU에서 100개 이상의 일관된 목표 뷰를 동시에 생성할 수 있다. 결과적으로, EscherNet은 제로 샷의 새로운 뷰 합성을 다룰 뿐만 아니라 단일 및 다중 이미지 3D 재구성을 자연스럽게 통합하여 이러한 다양한 작업을 단일 응집 프레임워크로 결합한다. 우리의 광범위한 실험은 에셔넷이 각 개별 문제에 대해 특별히 맞춤화된 방법과 비교할 때에도 여러 벤치마크에서 최첨단 성능을 달성한다는 것을 보여준다. 이 놀라운 다기능성은 3D 비전을 위한 확장 가능한 신경 아키텍처를 설계하기 위한 새로운 방향을 열어준다. 프로젝트 페이지: [https://kxhit.github.io/EscherNet](https://kxhit.github.io/EscherNet)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '뷰 합성은 컴퓨터 비전과 컴퓨터 그래픽에서 기본적인 작업으로 자리잡고 있다. 한 세트의 기준 시점들에 기초하여 임의의 시점들로부터 장면의 재렌더링을 허용함으로써, 이는 비인간적 시각에서 관찰되는 적응성을 모방한다. 이러한 능력은 사물 조작과 항해와 같은 실제적인 일상 업무에 중요할 뿐만 아니라 인간의 창의성을 기르는 데 중추적인 역할을 하여 깊이감, 원근감, 몰입감을 지닌 사물을 구상하고 제작할 수 있게 한다.\n' +
      '\n' +
      '이 논문에서, 우리는 뷰 합성의 문제를 재검토하고 질문한다:_어떻게 우리는 확장 가능한 뷰 합성을 용이하게 하기 위해 일반적인 3D 표현을 배울 수 있는가?_ 우리는 다음 두 가지 관찰에서 이 질문을 조사하려고 시도한다.\n' +
      '\n' +
      '**i)** 지금까지, 뷰 합성의 최근의 발전은 주로 트레이닝 속도 및/또는 렌더링 효율에 초점을 맞추었다[12, 18, 30, 47]. 특히, 이러한 발전은 모두 장면 최적화를 위한 체적 렌더링에 대한 공통된 의존도를 공유한다. 따라서, 이러한 모든 뷰 합성 방법들은 본질적으로 _scene-specific_이며, 전역 3D 공간 좌표와 결합된다. 대조적으로, 우리는 3D 표현이 장면 색상과 기하학에만 의존하는 패러다임 전환을 옹호하며, 지상-진실 3D 기하학이 필요 없이 암시적 표현을 학습하면서 특정 좌표계로부터의 독립성을 유지한다. 이러한 구분은 장면별 인코딩에 의해 부과되는 제약을 극복하기 위한 확장성을 달성하는 데 중요하다.\n' +
      '\n' +
      '**ii)**뷰 합성은, 본질적으로, 생성 이미지 인페인팅(25, 59)과 유사한, _조건부 생성 모델링 문제_로서 캐스팅되는 것이 더 적합하다. 희소 참조 뷰 세트만 주어진 경우 원하는 모델은 생성 공식 내에서 고유한 확률성을 활용하고 자연 이미지 통계 및 다른 이미지 및 객체에서 학습된 의미론적 사전으로부터 통찰력을 끌어내는 여러 그럴듯한 예측을 제공해야 한다. 이용 가능한 정보가 증가함에 따라, 생성된 장면은 더 제약되고, 점진적으로 지상-진실 표현에 더 가깝게 수렴한다. 특히, 기존의 3D 생성 모델들은 현재 단일 참조 뷰[20, 21, 22, 43, 23]만을 지원한다. 우리는 보다 바람직한 생성 공식이 다양한 수준의 입력 정보를 유연하게 수용해야 한다고 주장한다.\n' +
      '\n' +
      '이러한 통찰력을 바탕으로 뷰 합성을 위한 이미지 대 이미지 조건부 확산 모델인 에셔넷을 소개한다. EscherNet은 참조-대-타겟 뷰와 타겟-대-타겟 뷰 일관성 사이의 복잡한 관계를 포착하기 위해 도트-제품 자기 주의를 사용하는 변압기 아키텍처[50]를 활용한다. 에셔넷 내의 핵심 혁신은 카메라 위치 인코딩(CaPE)의 설계로, 4개의 DoF(객체 중심) 및 6개의 DoF 카메라 포즈 모두를 표현하는 데 전용된다. 이 인코딩은 공간 구조를 토큰에 통합하여 모델이 상대 카메라 변환만으로 쿼리와 키 사이의 자기 주의를 계산할 수 있게 한다. 요약하면, EscherNet은 이러한 현저한 특성들을 나타낸다:\n' +
      '\n' +
      '**일관성**: EscherNet은 카메라 위치 인코딩의 설계 덕분에 본질적으로 뷰 일관성을 통합하여, _reference-to-target 및 타겟-to-target 뷰 일관성_를 모두 장려한다.\n' +
      '**Scalability**: 장면별 최적화에 의해 제약되는 많은 기존의 신경 렌더링 방법과 달리, EscherNet은 값비싼 3D 연산(_e.g._3D 컨볼루션 또는 볼륨 렌더링) 없이, 임의의 특정 좌표계 및 지상-진실 3D 기하학의 필요로부터 자신을 디커플링하여, 일상적인 포즈 2D 이미지 데이터_로 _스케일링을 더 용이하게 한다.\n' +
      '***일반화**: 3개의 타겟 뷰에 대한 고정된 수의 3개의 참조에 대해서만 트레이닝됨에도 불구하고, EscherNet은 임의의 수의 참조 뷰_에 기초하여 임의의 카메라 포즈를 갖는 임의의 수의 타겟 뷰를 생성하는 능력을 나타낸다. 특히, EscherNet은 증가된 참조 뷰 수로 향상된 생성 품질을 나타내어 원래 설계 목표와 완벽하게 일치합니다.\n' +
      '\n' +
      '우리는 새로운 뷰 합성 및 단일/다중 이미지 3D 재구성 벤치마크 모두에 걸쳐 포괄적인 평가를 수행한다. 본 연구 결과는 EscherNet이 생성 품질 측면에서 모든 3D 확산 모델보다 우수할 뿐만 아니라 매우 제한된 뷰에서 그럴듯한 뷰 합성을 생성할 수 있음을 보여준다. 이는 종종 그러한 제약 하에서 의미 있는 콘텐츠를 생성하기 위해 고군분투하는 InstantNGP[30] 및 Gaussian Splatting[18]과 같은 이러한 장면-특정 신경 렌더링 방법들과 대조된다. 이것은 우리의 방법의 간단하면서도 확장 가능한 설계의 효율성을 강조하며, 전체적으로 뷰 합성과 3D 비전을 발전시킬 수 있는 유망한 방법을 제공한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '신경 3D 표현 학습의 초기 작업은 명시적 3D 표현 학습을 위해 복셀[26] 및 포인트 클라우드[39, 40]와 같은 표현을 사용하여 3D 데이터에 직접 최적화하는 데 중점을 두었다. 대안적으로, 다른 작업 라인은 암시적 3D 표현 학습을 위해 3D 공간 좌표를 서명된 거리 함수[34] 또는 점유자[28, 36]에 매핑하기 위해 신경망을 훈련시키는 데 중점을 두었다. 그러나 이러한 모든 방법은 지상-진실 3D 기하학에 크게 의존하여 소규모 합성 3D 데이터에 대한 적용 가능성을 제한한다[2, 54].\n' +
      '\n' +
      '더 넓은 범위의 데이터 소스들을 수용하기 위해, 미분가능한 렌더링 함수들[32, 45]이 다중 뷰 포즈 이미지들을 갖는 신경 암시적 형상 표현들을 최적화하기 위해 도입되었다. 보다 최근에, NeRF[29]는 5D 래디언스 필드들을 인코딩하기 위해 MLP들을 최적화함으로써 이러한 방법들에 비해 렌더링 품질에서 상당한 향상을 위한 길을 열었다. 3D 장면을 공간 좌표와 밀접하게 결합하는 것과 대조적으로, 우리는 임의의 좌표계와 무관하게 다중 뷰 포즈 이미지 간의 상호 작용을 학습하기 위해 신경망을 최적화하여 3D 표현 학습의 대안으로 EscherNet을 소개한다.\n' +
      '\n' +
      '새로운 뷰 합성 NeRF의 성공은 공간 이산화[3, 12, 14], 코드북[48], 해시 테이블[30] 또는 가우시안[18]을 사용한 인코딩의 다양한 변형을 통합함으로써 더 빠른 훈련 및/또는 렌더링 효율성을 해결하는 일련의 추적 방법을 촉발했다.\n' +
      '\n' +
      '다양한 장면과 몇 개의 장면 환경에서 NeRF의 일반화 능력을 향상시키기 위해 PixelNeRF[58]은 여러 장면을 공동으로 최적화하여 이전에 장면을 학습하려고 시도하지만 볼륨 렌더링에 필요한 높은 계산 요구 사항에 의해 제한된다. 다양한 다른 접근법들은 로컬 패치들로부터의 저-레벨 사전들을 통합하고[33], 의미적 일관성을 보장하고[16], 인접한 광선 주파수를 고려하며[56], 및 깊이 신호들을 통합하고[9]와 같은 규칙화 기법들을 도입함으로써 이 문제를 해결하였다. 대조적으로, EscherNet은 이미지 공간을 통해 장면을 직접 인코딩하여 대규모 데이터 세트를 통해 보다 일반화된 장면 사전의 학습을 가능하게 한다.\n' +
      '\n' +
      '3D 확산 모델 2D 생성 확산 모델의 출현은 사실적인 객체와 장면을 생성하는 데 인상적인 능력을 보여주었다[15, 42]. 이러한 발전은 미리 훈련된 2D 확산 모델로부터 점수 증류 샘플링(SDS)에 의해 안내되는 복사 필드를 최적화함으로써 드림퓨전[38] 및 매직3D[19]와 같은 텍스트 대 3D 확산 모델의 초기 설계에 영감을 주었다. 그러나 SDS는 계산 집약적인 반복 최적화가 필요하며, 종종 수렴을 위해 최대 1시간이 필요하다. 또한, 최근 제안된 이미지-대-3D 생성 접근법[8, 27, 55]을 포함한 이러한 방법들은 제한된 3D 이해로 인해 종종 비현실적인 3D 생성 결과를 생성하여 다중-얼굴 야누스 문제와 같은 문제를 야기한다.\n' +
      '\n' +
      '3D 이전 데이터를 보다 효율적으로 통합하기 위해 대안적인 접근 방식은 포인트 클라우드[31] 또는 신경 필드[4, 11, 17]와 같은 표현을 사용하여 3D 데이터 세트에서 직접 3D 생성 모델을 훈련하는 것을 포함한다. 그러나, 이 설계는 3D 컨볼루션 및 체적 렌더링과 같은 3D 연산에 의존하며, 이는 계산적으로 비싸고 스케일링하기 어렵다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 다중 뷰 포즈 데이터에 대해 훈련된 확산 모델이 3D 연산 없이 설계된 유망한 방향으로 부상했다. Zero-1-to-3[21]은 대규모 3D 객체 데이터 세트[6, 7]에서 렌더링된 쌍을 이루는 2D 포즈 이미지에서 뷰 합성을 학습하는 선구적인 작업으로 눈에 띈다. 그러나, 그것의 능력은 단일 참조 뷰 상에 컨디셔닝된 단일 타겟 뷰를 생성하는 것으로 제한된다. 최근 멀티뷰 확산 모델[20, 22, 43, 44, 57]의 발전은 3D 생성에 초점을 맞추었고 고정된 카메라 포즈로 고정된 수의 타겟 뷰만을 생성할 수 있다. 대조적으로, EscherNet은 임의의 카메라 포즈로 제한 없는 수의 타겟 뷰를 생성할 수 있어 뷰 합성에서 우수한 유연성을 제공한다.\n' +
      '\n' +
      '## 3 EscherNet\n' +
      '\n' +
      'EscherNet에서 문제 공식화 및 주석, 우리는 뷰 합성을 조건부 생성 모델링 문제로 재구성하고 다음과 같이 공식화한다.\n' +
      '\n' +
      '\\[\\mathcal{X}^{T}\\sim p(\\mathcal{X}^{T}|\\mathcal{X}^{R},\\mathcal{P}^{R},\\mathcal{P}^{T}). \\tag{1}\\\n' +
      '\n' +
      '여기서, \\(\\mathcal{X}^{T}=\\{\\mathbf{X}_{1:M}^{T}\\}) 및 \\(\\mathcal{P}^{T}=\\{\\mathbf{P}_{1:M}^{T}\\})는 글로벌 카메라 포즈 \\(\\mathbf{P}_{1:M}^{T}\\)을 갖는 \\(M\\) 목표 뷰 \\(\\mathbf{X}_{1:M}^{T}\\)의 집합을 나타낸다. 마찬가지로, \\(\\mathcal{X}^{R}=\\{\\mathbf{X}_{1:N}^{R}\\}) 및 \\(\\mathcal{P}^{R}=\\{\\mathbf{P}_{1:N}^{R}\\}\\)은 글로벌 카메라 포즈 \\(\\mathbf{P}_{1:N}^{R}\\)과 함께 \\(N\\) 참조 뷰 \\(\\mathbf{X}_{1:N}^{R}\\)의 집합을 나타낸다. 모델 학습과 추론 과정에서 \\(N\\)과 \\(M\\)은 모두 임의의 값을 취할 수 있다.\n' +
      '\n' +
      '본 논문에서는 각 목표 뷰\\(\\mathbf{X}_{i}^{T}\\in\\mathcal{X}^{T}\\)의 생성이 기준 뷰\\((\\mathbf{P}_{j}^{R})^{-1}\\mathbf{P}_{i}^{T},\\forall\\mathbf{P}_{j}^{R}\\in\\mathcal{P}^{R}\\in\\mathcal{P}^{R}\\)에 대한 상대적인 카메라 변환에만 의존하도록 하는 신경망 구조를 제안한다.\n' +
      '\n' +
      '### Architecture Design\n' +
      '\n' +
      'EscherNet은 i) 기존의 2D 확산 모델을 기반으로 대규모 학습을 통해 강력한 웹-스케일을 계승하고 ii) 언어 모델이 각 토큰에 대한 토큰 위치를 인코딩하는 방식과 유사하게 각 뷰/이미지에 대한 카메라 포즈를 인코딩한다. 따라서 본 논문에서 제안하는 모델은 임의의 수의 뷰들을 자연스럽게 처리할 수 있다.\n' +
      '\n' +
      'Multi-View GenerationEscherNet은 추가적인 학습 가능한 파라미터가 없는 트랜스포머 구조를 갖는 임의의 2D 확산 모델과 원활하게 통합될 수 있다. 본 연구에서는 잠재 확산 구조, 특히 StableDiffusion v1.5[42]를 채택하여 EscherNet을 설계한다. 이 선택은 동일한 백본(실험 섹션의 더 자세한 내용)을 활용하는 수많은 3D 확산 모델과의 직접적인 비교를 가능하게 한다.\n' +
      '\n' +
      '텍스트-이미지 생성을 위해 원래 설계된 안정적인 확산 모델을 다시점 생성으로 맞춤화하기 위해\n' +
      '\n' +
      '도 2: **3D 표현 개요.** EscherNet은 그들의 카메라 포즈 \\(\\mathbf{P}_{1:M}^{T}\\)에 기초하여 \\(M\\) 타겟 뷰 \\(\\mathbf{X}_{1:M}^{T}\\)의 세트를 생성하고, \\(N\\) 참조 뷰 \\(\\mathbf{X}_{1:N}^{R}\\) 및 그들의 카메라 포즈 \\(\\mathbf{P}_{1:N}^{R}\\)의 세트로부터 얻은 정보를 레버리지한다. EscherNet은 임의의 특정 좌표계와는 무관하게 다시점 포즈 영상에서의 스케일링을 용이하게 하기 위해 \\(\\mathbf{P}^{R}\\)과 \\(\\mathbf{P}^{T}\\)의 카메라 포즈 사이의 상대적인 카메라 변환만을 고려하여 암시적 3D 표현을 학습하는 새로운 방법을 제시한다.\n' +
      '\n' +
      '에셔넷에 적용, 몇 가지 주요 수정이 구현된다. 원래의 Stable Diffusion의 denoiser U-Net에서, 자기 주의 블록은 동일한 이미지 내의 상이한 패치들 내의 상호작용들을 학습하기 위해 사용되었다. EscherNet에서는 이 self-attention block을 다시 사용하여 서로 다른 목표 뷰에 걸쳐 별개의 패치 내에서 학습 상호 작용을 용이하게 하여 목표 대 목표 일관성을 보장한다. 마찬가지로, 원래 텍스트 정보를 이미지 패치에 통합하는 데 사용되는 교차 주의 블록은 에셔넷에서 용도 변경되어 \\(M\\) 목표 뷰에 대한 \\(N\\) 참조 내에서 상호 작용을 학습하여 기준 대 목표 일관성을 보장한다.\n' +
      '\n' +
      '컨디셔닝 참조 뷰sIn 뷰 합성에서, 컨디셔닝 신호들은 참조 뷰들에 존재하는 하이-레벨 시맨틱스 및 로우-레벨 텍스처 상세들 둘 모두를 정확하게 캡처하는 것이 중요하다. 3D 확산 모델[21, 22]의 이전 작업은 동결된 CLIP 사전 훈련된 ViT[41]를 통해 고레벨 신호를 인코딩하고 안정확산의 U-Net의 입력에 참조 이미지를 연결하여 저레벨 신호를 인코딩하는 전략을 사용했다. 그러나 이 설계 선택은 본질적으로 모델이 하나의 단일 뷰만 처리하도록 제한합니다.\n' +
      '\n' +
      '에셔넷에서는 참조 뷰를 토큰 세트로 나타내는 컨디셔닝 이미지 인코더에서 상위 레벨 및 하위 레벨 신호를 모두 통합하기로 선택한다. 이러한 설계 선택을 통해 본 모델은 다양한 참조 뷰 수를 처리할 수 있는 유연성을 유지할 수 있습니다. 초기 실험은 동결된 CLIP-ViT를 단독으로 사용하는 것이 낮은 수준의 텍스처를 캡처하는 데 실패하여 모델이 목표 포즈와 동일한 참조 뷰 포즈가 주어진 원래 참조 뷰를 정확하게 재현하는 것을 방지할 수 있음을 확인했다. CLIP-ViT를 미세 조정하면 이 문제를 해결할 수 있지만 훈련 효율성 측면에서 문제가 있다. 대신, 우리는 고효율 CNN 아키텍처인 경량 비전 인코더, 특히 ConvNeXtv2-Tiny[53]를 미세 조정하기로 선택한다. 이 구조는 참조 뷰를 더 작은 해상도의 이미지 특징으로 압축하기 위해 사용된다. 이러한 이미지 특징을 컨디셔닝 토큰으로 취급하여 각 참조 뷰를 효과적으로 표현합니다. 이 구성은 우리의 실험에서 충분한 것으로 입증되어 높은 훈련 효율을 유지하면서 생성 품질에서 우수한 결과를 제공한다.\n' +
      '\n' +
      '### 카메라 위치 부호화(CaPE)\n' +
      '\n' +
      '트랜스포머 아키텍처 내에서 카메라 포즈를 참조 및 대상 뷰 토큰으로 효율적이고 정확하게 인코딩하기 위해, 언어 도메인의 최근 진보로부터 영감을 끌어내는 카메라 위치 인코딩(CaPE)을 소개한다. 먼저 이 두 영역의 차이를 간략하게 살펴본다.\n' +
      '\n' +
      '- 언어에서, 토큰 위치(각 단어와 연관됨)는 _linear 및 discrete_ 구조를 따르며, 이들의 길이는 _infinite_일 수 있다. 언어 모델은 일반적으로 고정된 최대 토큰 카운트(컨텍스트 길이로 알려져 있음)로 훈련되며, 모델이 이러한 고정된 컨텍스트 길이를 초과하여 합리적으로 거동할 수 있게 하는 위치 인코딩을 구성하는 것은 지속적인 연구 과제로 남아 있다[13, 35].\n' +
      '\n' +
      '- 3D 비전에서, 토큰 위치(각 카메라와 연관됨)는 회전을 위한 _cyclic, continuous, bounded_ 구조와 번역을 위한 _linear, continuous, unbounded_ 구조를 따른다. 중요한 것은, 토큰 위치가 항상 0으로부터 시작하는 언어 도메인과 달리, _표준화된 절대 글로벌 카메라 포즈_in이 없다는 것이다\n' +
      '\n' +
      '그림 3: **EscherNet 아키텍처 세부사항.** EscherNet은 최소이지만 중요한 수정을 가한 Stable Diffusion 아키텍처 설계를 채택한다. 경량 비전 인코더는 \\(N\\) 참조 뷰로부터 하이 레벨 및 로우 레벨 신호를 모두 캡처한다. U-Net에서는 표적 간 일관성을 유도하기 위해 \\(M\\) 표적 뷰 내에서 자기 주의를 적용하고, 참조 간 일관성을 유도하기 위해 \\(M\\) 표적 및 \\(N\\) 참조 뷰 내에서 교차 주의를 적용한다. 각각의 어텐션 블록에서, CaPE가 키 및 쿼리에 채용되어, 어텐션 맵이 특정 좌표계들과 무관하게 상대적인 카메라 포즈들로 학습할 수 있게 한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '### 새로운 시점 합성 결과\n' +
      '\n' +
      '본 논문에서는 구글 스캔 객체 데이터셋(GSO, Google Scanned Objects dataset) [10]과 RTMV 데이터셋 [49]의 새로운 뷰 합성에서 EscherNet을 Zero-1-to-3 [21]과 RealFusion [27]과 같은 뷰 합성을 위한 3D 확산 모델과 비교하여 평가한다. 또한, NeRF Synthetic Dataset[29]에 대해 InstantNGP[30] 및 3D Gaussian Splatting[18]과 같은 최신 장면별 신경 렌더링 방법과 비교하여 평가한다.\n' +
      '\n' +
      '특히, 많은 다른 3D 확산 모델[20, 22, 23, 43, 57]은 뷰 합성보다는 3D 생성을 우선시한다. 이 제한은 그들을 _고정 타겟 포즈_로 타겟 뷰를 예측하는 것으로 제한하여, 이들을 직접적으로 비교할 수 없게 한다.\n' +
      '\n' +
      'Tab의 3D 확산 모델과 비교됩니다. 도 1 및 도 1을 참조하여 설명한다. 도 5에서, 우리는 EscherNet이 3D 확산 기준선을 양적으로나 정성적으로 크게 능가한다는 것을 보여준다. 특히, 0-1-to-3-XL(zero-1-to-3-XL)은 10\\(\\times 10\\)의 학습 데이터로 학습됨에도 불구하고, RealFusion은 반복적인 장면 최적화를 위해 값비싼 점수 증류를 필요로 함에도 불구하고 우수한 성능을 보인다[38]. 디자인에 의한 Zero-1-to-3은 본질적으로 단일 타겟 뷰를 생성하는 것에 제한되고 다수의 타겟 뷰에 걸쳐 자기 일관성을 보장할 수 없는 반면, EscherNet은 다수의 일관된 타겟 뷰를 공동으로 생성할 수 있고 보다 정밀한 카메라 제어를 제공한다는 점을 강조할 가치가 있다.\n' +
      '\n' +
      '탭의 신경망 렌더링 방법과 비교합니다. 도 2 및 도 2를 참조하여 설명한다. 도 4를 참조하면, 우리는 EscherNet이 InstantNGP와 3D Gaussian Splatting 모두에 필요한 장면별 최적화 없이 제로 샷 방식으로 그럴듯한 뷰 합성을 다시 제공한다는 것을 보여준다. 특히, EscherNet은 대규모 훈련을 통해 획득한 객체에 대한 일반화된 이해를 활용하여 제한된 수의 참조 뷰에 조건화된 경우에도 의미론적 및 공간적으로 주어진 뷰를 해석할 수 있다. 그러나, 참조 뷰들의 수의 증가와 함께, InstantNGP 및 3D 가우시안 스플래팅 모두는 렌더링 품질에서 상당한 개선을 나타낸다. 생성 공식의 장점을 유지하면서 광 사실적 신경 렌더링을 달성하는 것은 중요한 연구 과제로 남아 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{Training \\# Ref.} & \\multicolumn{3}{c}{**GSO-30**} & \\multicolumn{3}{c}{**RTMV**} \\\\  & Data & Views & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) & PSNR\\(\\uparrow\\) & SSIM\\(\\uparrow\\) & LPIPS\\(\\downarrow\\) \\\\ \\hline RealFusion & - & 1 & 12.76 & 0.758 & 0.382 & - & - & - \\\\ Zero123 & 800K & 1 & 18.51 & 0.856 & 0.127 & 10.16 & 0.505 & 0.418 \\\\ Zero123-XL & 10M & 1 & 18.93 & 0.856 & 0.124 & 10.59 & 0.520 & 0.401 \\\\ \\hline EscherNet & 800k & 1 & 20.24 & 0.884 & 0.095 & 10.56 & 0.518 & 0.410 \\\\ EscherNet & 800k & 2 & 22.91 & 0.908 & 0.064 & 12.66 & 0.585 & 0.301 \\\\ EscherNet & 800k & 3 & 24.09 & 0.918 & 0.052 & 13.59 & 0.611 & 0.258 \\\\ EscherNet & 800k & 5 & 25.09 & 0.927 & 0.043 & 14.52 & 0.633 & 0.222 \\\\ EscherNet & 800k & 10 & 25.90 & 0.935 & 0.036 & 15.55 & 0.657 & 0.185 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: GSO 및 RTMV 데이터 세트에 대한 **새로운 뷰 합성 성능.** EscherNet은 추가 SDS 최적화 없이 훈련 데이터 및 RealFusion이 훨씬 적은 Zero-1-to-3-XL보다 우수하다. 또한, 에셔넷의 성능은 더 많은 참조 뷰를 포함함에 따라 더 향상된 성능을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{\\# Reference Views (Less \\(\\rightarrow\\) More)} & & & \\\\  & 1 & 2 & 3 & 5 & 10 & 20 & 50 & 100 \\\\ \\hline \\multicolumn{8}{l}{**InstantNGP (Scene Specific Training)**} & & & \\\\ PSNR\\(\\uparrow\\) & 10.92 & 12.42 & 14.27 & 18.17 & 22.96 & 24.99 & 26.86 & 27.30 \\\\ SSIM\\(\\uparrow\\) & 0.449 & 0.521 & 0.618 & 0.761 & 0.881 & 0.917 & 0.946 & 0.953 \\\\ LPIPS\\(\\downarrow\\) & 0.627 & 0.499 & 0.391 & 0.228 & 0.091 & 0.058 & 0.034 & 0.031 \\\\ \\hline \\multicolumn{8}{l}{**GaussianSplitating (Scene Specific Training)**} & & & \\\\ PSNR \\(\\uparrow\\) & 9.44 & 10.78 & 12.87 & 17.09 & 23.04 & 25.34 & 26.98 & 27.11 \\\\ SSIM\\(\\uparrow\\) & 0.391 & 0.432 & 0.546 & 0.732 & 0.876 & 0.919 & 0.942 & 0.944 \\\\ LPIPS\\(\\downarrow\\) & 0.610 & 0.541 & 0.441 & 0.243 & 0.085 & 0.054 & 0.041 & 0.041 \\\\ \\hline \\multicolumn{8}{l}{**EscherNet (Zero Shot Inference)**} & & & \\\\ PSNR\\(\\uparrow\\) & 13.36 & 14.95 & 16.19 & 17.16 & 17.74 & 17.91 & 18.05 & 18.15 \\\\ SSIM\\(\\uparrow\\) & 0.659 & 0.700 & 0.729 & 0.748 & 0.761 & 0.765 & 0.769 & 0.771 \\\\ LPIPS\\(\\downarrow\\) & 0.291 & 0.208 & 0.161 & 0.127 & 0.114 & 0.106 & 0.099 & 0.097 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: NeRF 합성 데이터세트에 대한 **새로운 뷰 합성 성능.** EscherNet은 장면별 최적화를 요구하지 않으면서 5개 미만의 참조 뷰로 제공될 때 인스턴트NGP 및 가우시안 스플래팅 모두를 능가한다. 그러나, 참조 뷰들의 수가 증가함에 따라, 두 방법들 모두 렌더링 품질에서 더 큰 향상을 보인다.\n' +
      '\n' +
      '도 4: **NeRF 합성 드럼 장면에서 생성된 뷰 시각화.** EscherNet은 매우 제한된 참조 뷰와 함께 제공되는 경우에도 그럴듯한 뷰 합성을 생성하는 반면, 신경 렌더링 방법은 의미 있는 콘텐츠를 생성하지 못한다. 그러나, 10개 이상의 참조 뷰들을 갖는 경우, 장면-특정 방법들은 렌더링 품질에서 상당한 향상을 보인다. 우리는 드럼 장면의 모든 테스트 뷰에 걸쳐 평균 PSNR을 보고한다. 다른 장면 및/또는 더 많은 참조 뷰에 대한 결과는 부록 D에 나와 있다.\n' +
      '\n' +
      '### 3D 생성 결과\n' +
      '\n' +
      '이 섹션에서는 GSO 데이터 세트에 대해 단일/소수의 이미지 3D 생성을 수행한다. 직접점군 생성을 위한 점-E[31], 직접 NeRF 생성을 위한 Shape-E[17], SDS 안내를 통한 3D Gaussian[18]을 최적화하기 위한 DreamGaussian[17], Zero-1-to-3으로부터 예측된 다수의 뷰를 사용하여 SDF를 디코딩하기 위한 One-2-3-45[20], 16개의 일관된 고정 생성 뷰로부터 NeuS[51]를 사용하여 SDF를 피팅하기 위한 SyncDreamer[22]와 비교한다. 우리는 또한 소수의 이미지 3D 재구성 기준선에 대해 참조 뷰에 대해 훈련된 NeuS를 포함한다.\n' +
      '\n' +
      '임의의 참조 뷰가 주어지면, EscherNet은 다수의 3D 일관된 뷰를 생성할 수 있어서, 3D 재구성을 위해 NeuS[51]로 간단한 채택을 허용한다. 우리는 36개의 고정 뷰를 생성하는데, 방위각을 0\\({}^{\\circ}\\)에서 360\\({}^{\\circ}\\)으로 변화시키면서 고도(-30\\({}^{\\circ}\\), 0\\({}^{\\circ}\\), 30\\({}^{\\circ}\\))에서 30\\({}^{\\circ}\\)마다 렌더링한다.\n' +
      '\n' +
      '결과 탭입니다. 도 3 및 도 3을 참조하여 설명한다. 도 6을 참조하면, EscherNet은 다른 이미지-대-3D 생성 모델에 비해 상당히 우수한 3D 재구성 품질을 달성함으로써 눈에 띈다. 특히, EscherNet은 단일 참조 뷰에서 조건화되었을 때 현재 최상의 모델로 간주되는 SyncDreamer에 비해 챔퍼 거리가 약 25% 개선되고 10 참조 뷰에서 조건화되었을 때 60% 개선됨을 보여준다. 이 인상적인 성능은 에셔넷이 임의의 수의 참조 뷰 및 타겟 뷰를 유연하게 처리함으로써 3D 기하학에 대한 포괄적이고 정확한 제약을 제공하는 능력에 기인한다. 반면 SyncDreamer는 고도각에 대한 민감성과 설계로 고정된 30\\({}^{\\circ}\\) 고도각에 의해 부과되는 제약으로 인해 복잡한 객체의 전체적 표현을 학습하는 데 어려움을 겪는다. 이러한 제한은 특히 생성된 지오메트리의 하부 영역에서 열화된 재구성을 초래한다.\n' +
      '\n' +
      '문자-3D 생성에 대한### 결과\n' +
      '\n' +
      '임의의 수의 참조 뷰를 수용하는 에셔넷의 유연성은 그에 대한 간단한 접근을 가능하게 한다\n' +
      '\n' +
      '그림 5: GSO 및 RTMV 데이터 세트에 대한 **새로운 뷰 합성 시각화.** EscherNet은 Zero-1-to-3-XL을 능가하여 우수한 생성 품질과 더 미세한 카메라 제어를 제공합니다. 특히, 추가 뷰로 조정될 때 에셔넷은 생성된 뷰와 지상진실 텍스처의 향상된 유사성을 나타내어 백팩 스트랩 및 거북 껍질과 같은 보다 세련된 텍스처 세부 사항을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\# Ref. Views & Chamfer Dist. \\(\\downarrow\\) & Volume IoU \\(\\uparrow\\) \\\\ \\hline Point-E & 1 & 0.0447 & 0.2503 \\\\ Shape-E & 1 & 0.0448 & 0.3762 \\\\ One2345 & 1 & 0.0632 & 0.4209 \\\\ One2345 XL & 1 & 0.0667 & 0.4016 \\\\ DreamGaussian & 1 & 0.0605 & 0.3757 \\\\ DreamGaussian XL & 1 & 0.0459 & 0.4531 \\\\ SyncDreamer & 1 & 0.0400 & 0.5220 \\\\ \\hline NeuS & 3 & 0.0366 & 0.5352 \\\\ NeuS & 5 & 0.0245 & 0.6742 \\\\ NeuS & 10 & 0.0195 & 0.7264 \\\\ \\hline EscherNet & 1 & 0.0314 & 0.5974 \\\\ EscherNet & 2 & 0.0215 & 0.6868 \\\\ EscherNet & 3 & 0.0190 & 0.7189 \\\\ EscherNet & 5 & 0.0175 & 0.7423 \\\\ EscherNet & 10 & 0.0167 & 0.7478 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: GSO.** EscherNet에 대한 **3D 재구성 성능은 특히 여러 참조 뷰에서 조정될 때 정확한 3D 기하학으로 보다 시각적으로 매력적인 이미지를 생성하는 데 있어 다른 모든 이미지 대 3D 기준선보다 우수하다.\n' +
      '\n' +
      '텍스트-투-3D 생성 문제는 텍스트-투-이미지, 기성 텍스트-투-이미지 생성 모델에 의존하는 단계, 그리고 이미지-투-3D, 에셔넷에 의존하는 단계로 나뉜다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 7에서는 MVDream을 사용한 텍스트-투-4뷰 모델[44]과 SDXL을 사용한 텍스트-투-이미지 모델[37]을 사용하여 조밀한 신규 뷰 생성의 시각적 결과를 제시한다. 놀랍게도, 유통 외 및 반사실적 콘텐츠를 다룰 때에도, EscherNet은 매력적인 텍스처로 일관된 3D 신규 뷰를 생성한다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 논문에서는 스케일러블 뷰 합성을 위해 설계된 다중 뷰 조건 확산 모델인 EscherNet을 소개한다. 혁신적인 카메라 위치 임베딩(CaPE)에 의해 강화된 안정적인 확산의 2D 아키텍처를 활용하여, EscherNet은 다양한 참조 뷰로부터 암시적 3D 표현을 능숙하게 학습하여 일관된 3D 신규 뷰 합성을 달성한다. 우리는 부록 F에서 자세한 논의와 추가 절제 분석을 제공한다.\n' +
      '\n' +
      '제한 및 논의 에셔넷의 임의의 수의 참조 뷰를 처리하는 유연성은 자기회귀 언어 모델 [1, 5]와 유사하게 자기회귀 생성을 허용한다. 이 접근 방식은 추론 시간을 크게 줄이는 반면, 발전 품질을 저하시킨다. 또한, EscherNet의 현재 능력은 훈련 데이터 세트에 의해 제한된 3 DoF 설정 내에서 작동하며, 이는 실제 시나리오와 일치하지 않을 수 있으며, 여기서 뷰는 일반적으로 \\(SE(3)\\) 공간에 걸쳐 있다. 향후 연구에서는 실제 장면과 함께 6개의 DoF 훈련 데이터로 스케일링 EscherNet을 탐색하여 보다 일반적인 3D 표현을 위해 노력할 것이다.\n' +
      '\n' +
      '그림 6: **GSO에 대한 단일 뷰 3D 재구성 시각화.** 에셔넷의 조밀하고 일관된 신규 뷰를 생성하는 능력은 완전하고 잘 구속된 3D 기하학의 재구성을 상당히 향상시킨다. 대조적으로, One-2-3-45-XL 및 DreamGaussian-XL은 상당히 더 큰 사전 훈련된 모델을 활용함에도 불구하고 과도하게 평활화되고 시끄러운 재구성을 생성하는 경향이 있으며, 희박한 고정 뷰 합성에 의해 제한되는 SyncDreamer는 특히 소파의 영역과 종 바닥 부분에서 기하학을 엄격하게 제한하려고 애쓴다.\n' +
      '\n' +
      '도 7: **MVDream(up) 및 SDXL(bottom)을 사용한 **Text-to-3D 시각화.** EscherNet은 사용자 제공 텍스트 프롬프트로 생성된 합성 이미지에 대해 설득력 있고 사실적인 뷰 합성을 제공한다. 추가 결과는 부록 E에 나와 있다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '여기에 제시된 연구는 다이슨테크놀로지의 지원을 받았다. 신콩은 중국 장학회-제국 장학금을 보유하고 있다. 초기 프로젝트 탐사를 용이하게 한 훈련 계산에 기여한 사약 폴과 허깅페이스에 감사드립니다. 우리는 또한 6DoF CaPE를 공식화하는 것에 대한 그의 귀중한 논의에 대해 이페이 렌을 인정하고자 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [2] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [3] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [4] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. _arXiv preprint arXiv:2304.06714_, 2023.\n' +
      '* [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [6] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. _arXiv preprint arXiv:2307.05663_, 2023.\n' +
      '* [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [8] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerf: Single-view nerf synthesis with language-guided diffusion as general image priors. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [9] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items. In _Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)_. IEEE, 2022.\n' +
      '* [11] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* [12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [13] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. _arXiv preprint arXiv:2308.16137_, 2023.\n' +
      '* [14] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [16] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [17] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. _arXiv preprint arXiv:2305.02463_, 2023.\n' +
      '* [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics (TOG)_, 2023.\n' +
      '* [19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [20] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16928_, 2023.\n' +
      '* [21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [22] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncedreamer: Generating multiview-consistent images from a single-view image. _arXiv preprint arXiv:2309.03453_, 2023.\n' +
      '* [23] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. _arXiv preprint arXiv:2310.15008_, 2023.\n' +
      '* [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [25] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [26] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In _Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS)_, 2015.\n' +
      '* [27] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [28] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [29] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [30] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 2022.\n' +
      '* [31] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [32] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [33] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [34] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [35] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarm: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* [36] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In _Proceedings of the European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [40] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [43] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyei Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model. _arXiv preprint arXiv:2310.15110_, 2023.\n' +
      '* [44] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [45] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* [46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* [47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [48] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Muller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable bitrate neural fields. In _Proceedings of SIGGRAPH_, 2022.\n' +
      '* [49] Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas Muller,Charles Loop, Nathan Morrical, Koki Nagano, et al. Rtmv: A ray-traced multi-view synthetic dataset for novel view synthesis. _arXiv preprint arXiv:2205.07058_, 2022.\n' +
      '* [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [51] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.\n' +
      '* [52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 2004.\n' +
      '* [53] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [54] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2015.\n' +
      '* [55] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360deg views. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [56] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with free frequency regularization. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [57] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. _arXiv preprint arXiv:2310.03020_, 2023.\n' +
      '* [58] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [59] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5505-5514, 2018.\n' +
      '* [60] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '\n' +
      '## 부록 CaPE의 파이썬 구현\n' +
      '\n' +
      '```\n' +
      'defcompute_4dof_cape(v,P,s):":paramv:inputfeaturevectorwithitsdimension.beta,gamma,r]:params:asmallscalarforradius:return:rotatedvwithitscorrespondingcameraposeP" v=v.reshape([-1,8]) psi=np.zeros([8,8]) foriinrange(4): ifi<3: psi[2=s:i2*(i+1),2*i:2*(i+1)]=\\psi[2=v.array([(np.cos(P[i])],[np.sin(P[i])],[np.cos(s*np.log(P[i])])])retv.dot(psi.reshape(-1])],[np.cos(s*np.log(P[i])],[np.cos(s*np.log(P[i])]]\n' +
      '```\n' +
      '\n' +
      '목록 1: 4 DoF CaPE에 대한 파이썬 구현.\n' +
      '\n' +
      '```\n' +
      'defcompute_6dof_cape(v,P,s=0.001,key=True):":paramv:inputfeaturevectorwithitsdimension:paramP:4x483matrix:params:asmallscalarfortranslation:return:rotatedvwithitscrespondingcameraposeP"v=v.reshape([-1,4]) V=v.reshape([-1,4]) P[i3,3]+=s psi=Pifkeyelsemp.linalg.inv(P.T returnv.dot(psi.reshape(-1))\n' +
      '```\n' +
      '\n' +
      '목록 2: 6 DoF CaPE에 대한 파이썬 구현.\n' +
      '\n' +
      '## 부록 B 추가 교육 세부사항 및 실험 설정\n' +
      '\n' +
      '최적화 및 구현 에셔넷은 \\(256\\times 256]\\) 해상도 영상에 대해 \\(1\\cdot 10^{-4}\\)의 학습률과 \\(0.01\\)의 가중치 감쇠를 갖는 AdamW 최적화기 [24]를 사용하여 학습된다. 본 논문에서는 코사인 어닐링(cosine annealing)을 이용하여 총 100,000 단계의 학습 단계에서 학습률을 \\(1\\cdot 10^{-5}\\)으로 감소시키면서, 초기 1000 단계 동안 선형적으로 예열한다. 학습의 속도를 높이기 위해 bf16의 정밀도로 자동 혼합 정밀도를 구현하고 기울기 체크포인팅을 사용한다. 우리의 훈련 배치는 각 객체에 대해 12개의 뷰에서 교체하여 무작위로 샘플링된 3개의 참조 뷰와 3개의 목표 뷰로 구성되며 총 배치 크기는 672개(GPU당 112개 배치)이다. 전체 모델 훈련 과정은 6개의 NVIDIA A100 GPU에서 1주일이 소요된다.\n' +
      '\n' +
      '뷰 합성에 사용되는 2D 메트릭에 대한 메트릭은 PSNR, SSIM[52], LPIPS[60]을 사용한다. 3D 생성에 사용되는 3D 메트릭의 경우 챔퍼 거리와 볼륨 IoU를 사용한다. 공정하고 효율적인 평가 프로세스를 보장하기 위해 각 기준 방법과 접근 방식은 시점당 장면당 한 번만 실행된다. 이 관행은 여러 장면과 시점에 걸쳐 안정적인 평균 결과를 제공하는 것으로 입증되었다.\n' +
      '\n' +
      '### Evaluation Details\n' +
      '\n' +
      'NeRF 합성 데이터세트[29]에서는 원본 데이터세트에 제공된 8개의 장면 모두를 고려하고 평가한다. 다양한 수의 참조 뷰로 성능을 평가하기 위해, 우리는 훈련 세트에서 무작위로 샘플링된 동일한 뷰 세트를 사용하여 모든 기준 방법과 접근 방식을 훈련한다. 평가는 8개의 장면들(장면당 200개의 뷰들) 모두에 걸쳐 테스트 세트들에서 정의된 모든 타겟 뷰들에 대해 수행된다. 인스턴트NGP[30]의 경우 각 장면에 대해 10k 단계(\\(\\approx\\) 1분)를 실행한다. 3차원 가우시안 스플래팅[18]을 위해 각 장면에 대해 5k 단계(\\(\\approx\\) 2분)를 실행한다.\n' +
      '\n' +
      'GSO(Gogle Scanned Dataset) [10]에서는 SyncDreamer[22]가 선택한 동일한 30개의 객체를 평가한다. 각 객체에 대해 무작위로 생성된 카메라 포즈와 무작위로 생성된 환경 조명 조건으로 25개의 뷰를 렌더링하여 테스트 세트를 구성한다. 각 객체에 대해 첫 번째 10개의 이미지를 참조 뷰로 선택하고 다음 15개의 이미지를 평가 대상 뷰로 선택한다. 모든 기준 뷰 및 목표 뷰는 임의의 카메라 포즈로 렌더링된다는 점에 유의할 필요가 있다: _예._ SyncDreamer는 모든 GSO 데이터를 렌더링하기 위해 균등하게 분포된 환경 조명을 사용하고, 각 객체에 대한 기준 뷰는 인간 선호도에 기초하여 수동으로 선택된다.1 추가로, 평가된 목표 뷰는 또한 4개의 독립 세대 중 선택된 인간 선호도에 기초하여 수동으로 선택된다.2\n' +
      '\n' +
      '각주 1: [https://github.com/liuyuan-pal/SyncDreamer/issues/21](https://github.com/liuyuan-pal/SyncDreamer/issues/21)\n' +
      '\n' +
      '각주 2: [https://github.com/liuyuan-pal/SyncDreamer/issues/21#issuecomment-1770345260](https://github.com/liuyuan-pal/SyncDreamer/issues/21#issuecomment-1770345260)\n' +
      '\n' +
      '3D 생성을 평가할 때 생성된 3D 메쉬 또는 포인트 클라우드에서 모든 방법에 걸쳐 고르게 분포된 4096개의 포인트를 무작위로 샘플링한다. 각각의 메소드의 생성된 메쉬는 참조 뷰들의 카메라 포즈를 사용하여 지상-진실 메쉬에 정렬된다. 특히 Point-E[31]와 Shape-E[17]에서는 각 x/y/z 축을 따라 90/180도 회전하여 최종 메쉬 포즈에 대한 최적의 정렬을 결정한다. 우리의 평가 접근법은 깊이 맵을 얻기 위해 처음에 3D 메쉬를 고정된 16개의 생성된 뷰로 투영하는 SyncDreamer와 다시 다르다. 그런 다음 최종 평가를 위해 이러한 깊이 맵에서 점을 샘플링한다.3\n' +
      '\n' +
      '각주 3: [https://github.com/liuyuan-pal/SyncDreamer/issues/44](https://github.com/liuyuan-pal/SyncDreamer/issues/44)\n' +
      '\n' +
      'RTMV Dataset[49]에서는 GSO 데이터 세트의 여러 객체 더미를 특징으로 하는 10개의 복잡한 장면으로 구성된 Zero-1에서 3까지의 [21]에서 사용되는 평가 설정을 따른다. GSO 테스트 세트의 구성과 유사하게 처음 10개 이미지의 고정 부분 집합을 참조 뷰로, 후속 10개 뷰를 평가 대상 뷰로 무작위로 선택한다.\n' +
      '\n' +
      '## 부록 C 6 DoF CaPE 추가 결과\n' +
      '\n' +
      '6 DoF CaPE 설계의 유효성을 검증하기 위해 탭의 GSO 및 RTMV 데이터 세트에 대한 새로운 뷰 합성에서 성능을 보여준다. 3(a) 및 탭의 NeRF 합성 데이터 세트에서. 3(c). 또한 Tab에서 GSO 데이터 세트에 대한 3D 재구성 결과를 제공한다. 3(b). 6 DoF CaPE가 있는 EscherNet은 4 DoF CaPE 설계와 비교할 때 비슷하고 종종 약간 개선된 결과를 달성한다는 것이 분명하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 4: EscherNet 6 DoF는 EscherNet 4 DoF와 유사하고 때로는 개선된 성능을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '텍스트-3D에 대한 추가 결과\n' +
      '\n' +
      '우리는 4 DoF CaPE로 훈련된 EscherNet을 사용하여 텍스트 대 이미지 대 3D에 대한 추가 시각화를 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline \\multicolumn{1}{c}{A robot made of vegetables.} \\\\ \\hline A nurse cogi. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline A cute steamunk elephant. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline A bull dog wearing a black pirate hat. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline An astronaut riding a horse. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline Medieval House, grass, medieval, & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline medieval-decor, 3d asset. & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: SDXL(top 3) 및 MVDream(bottom 3)을 갖는 Text-to-3D 생성.\n' +
      '\n' +
      '## 부록 F 추가 토론, 한계 및 향후 작업\n' +
      '\n' +
      'Direct v.s. Autoregressive GenerationEscherNet의 임의 수의 참조 뷰 및 대상 뷰를 처리하는 유연성은 뷰 합성을 위한 다중 선택을 제공한다. 실험에서 우리는 모든 목표 뷰를 공동으로 생성하기 위해 간단한 직접 생성을 사용한다. 추가적으로, 대안적인 접근법은 자기회귀 생성이며, 여기서 타겟 뷰들은 자기회귀 언어 모델들을 갖는 텍스트 생성과 유사하게 순차적으로 생성된다.\n' +
      '\n' +
      '많은 수의 타겟 뷰들을 생성하기 위해, 자기회귀 생성은 직접 생성보다 상당히 빠를 수 있다(200 뷰들을 생성하기 위해 \\(20\\times\\)보다 더 빠를 수 있다). 이러한 효율 이득은 각 자기 주의 블록에서 2차 추론 비용을 선형 추론 비용으로 변환함으로써 발생한다. 그러나, 자동 회귀 생성은 현재 설계에서 _content drifting 문제에 직면할 수 있으며, 여기서 각각의 새로 생성된 뷰가 이전에 불완전하게 생성된 뷰에 의존함에 따라 생성되는 품질이 점진적으로 감소한다. 자기회귀 생성은 추론 효율성 측면에서 많은 장점을 자랑하며 SLAM(Simultaneous Localization and Mapping)과 같은 특정 시나리오에 적합하다. 이와 같이, 이러한 설정에서 렌더링 품질을 향상시키는 것은 향후 연구에 필수적인 방법을 나타낸다.\n' +
      '\n' +
      '또한 다중 뷰 생성의 확률성과 일관성은 목표 뷰 합성 품질을 향상시키기 위해, 특히 제한된 수의 참조 뷰에서 컨디셔닝할 때, 추가 목표 뷰를 도입하는 것이 매우 유익할 수 있다는 것을 관찰한다. 이러한 보조 대상 뷰는 무작위로 정의되거나 동일한 대상 카메라 포즈와 중복될 수 있다. 동시에 다수의 타겟 뷰들을 생성하는 것은 확산 프로세스에서 내재된 확률성을 암시적으로 감소시키는 역할을 하여, 향상된 생성 품질 및 일관성을 초래한다. 경험적 조사를 통해 그림 8의 주황색으로 강조 표시된 대로 최적의 구성이 최소 15개의 목표 뷰를 보장한다는 것을 결정하며, 이 임계값을 넘어 추가 뷰는 한계 성능 개선을 산출한다.\n' +
      '\n' +
      '트레이닝 데이터 샘플링 전략은 에셔넷 트레이닝 동안 \\(N\\in\\{1,2,3,4,5\\}\\) 참조 뷰와 \\(M\\in\\{1,2,3,4,5\\}\\) 목표 뷰의 다양한 조합을 탐색했다. 경험적으로, 더 많은 수의 뷰들은 더 많은 GPU 메모리를 요구하고 트레이닝 속도를 느리게 하는 반면, 더 적은 수의 뷰들은 멀티-뷰 대응들을 학습하는 모델의 능력을 제한할 수 있다. 훈련 효율성과 성능의 균형을 맞추기 위해 훈련 뷰를 각 객체에 대해 \\(N=3\\) 참조 뷰와 \\(M=3\\) 목표 뷰로 설정했는데, 이는 실무에서 효과가 입증된 구성이다. 또한, 이 6개의 뷰에 대해 무작위 샘플링 방식을 채택하여 학습 뷰에서 반복 이미지의 가능성을 도입한다. 이 샘플링 전략은 대체 없이 샘플링에 비해 성능이 약간 향상되었음을 보여주었다.\n' +
      '\n' +
      'Multi-view VideoEscherNet의 유연성으로 스케일링하는 것은 훈련을 위해 3D 데이터세트로부터 고정-뷰 렌더링된 이미지들의 세트를 필요로 하는 다른 멀티뷰 확산 모델들[22, 23]과 구별된다. 에셔넷은 한 쌍의 포즈를 취한 이미지를 사용하여 훈련 샘플을 효율적으로 구성할 수 있다. [6, 7]과 같은 대규모 3D 데이터 세트의 이점을 얻을 수 있지만, 에셔넷의 적응성은 비디오에서 직접 파생된 것을 포함하여 더 넓은 범위의 포즈 이미지 소스로 확장된다. 여러 데이터 소스를 수용하기 위한 EscherNet의 스케일링은 향후 연구에 중요한 방향이다.\n' +
      '\n' +
      '그림 8: **참조 뷰 및 타겟 뷰의 수가 다른 새로운 뷰 합성.** GSO 데이터세트의 객체에 걸쳐 _one_ 사전 선택된 타겟 뷰에 대한 EscherNet의 평균 성능을 제시한다. 참조 뷰와 목표 뷰의 수가 증가함에 따라 뷰 합성 품질이 뚜렷하게 개선되는 것을 관찰한다. 이 시나리오에서, 다수의 타겟 뷰는 본질적으로 초기에 선택된 단일 사전-선택된 뷰의 다수의 중복이며, 이는 뷰 합성 품질을 향상시키는 데 효과적임을 발견하는 전략이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>