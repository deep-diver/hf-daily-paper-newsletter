<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_EMPTY:1]\n' +
      '\n' +
      'to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation.\n' +
      '\n' +
      'To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Generating large amount of high-quality 3D contents is key for many practical applications, including video-games, film-making, augmented and virtual reality (AR/VR). The increasing demand for high-quality digital contents has made 3D generation a significant topic of research. Recently, in the 2D domain, denoise diffusion models [14], have demonstrated remarkable results in image synthesis and beyond, leading to the development of production-ready 2D generation tools, such as Stable Diffusion [15], Midjourney, and Dall-E [11]. The success in 2D domain has significantly sparked interest in the development of 3D generation tools. A multitude of researches on 3D generation have been published recently, most notable works include DreamFusion [13], Rodin [12], Get3D [14], Zero123 [15], SynC Dreamer [16], and LRM ([12]), etc.\n' +
      '\n' +
      'However, existing methods mainly focus on the generation of 3D content with fixed spatial extent (such as a small object of finite size). In this paper, we investigate a relatively new yet increasingly important task: _generating expandable (hence infinite) 3D scenes_. This task is particularly valuable for video gaming industry, as it delivers an immersive gaming experience by allowing users to interact freely with the world without being restricted by a predetermined world boundary, as seen in open-world games. Nonetheless, creating an unbounded and freely explorable scene is a non-trivial task. Current practices typically rely on artists\' manual labor, a time-consuming and costly process.\n' +
      '\n' +
      'Generating expandable 3D scenes using diffusion models poses two major challenges: First, 1) the generation of high-fidelity 3D shapes at the scene level is a difficult problem. The variance in 3D scenes is orders of magnitude greater than in single objects. A scene comprises basic objects, and the possibilities for arranging these objects are limitless. This high level of diversity makes it difficult to approximate its distribution using diffusion probabilistic models. Besides, 2) the expansion from an existing scene to a larger one is non-trival. The transition area between the old and new scenes needs to be both semantically and geometrically harmonious, adding another layer of complexity to the task.\n' +
      '\n' +
      'Text2Room [10] is the most closely related work to our task. It employs a pre-trained 2D diffusion model to generate 2D images and lifts them to a 3D scene via the camera viewpoint and the estimated depth images. A scene is expanded by merging generated images from incrementally added camera viewpoints. Therefore, it is able to generate expandable 3D scenes with impressive texture results, though only at the room scale. However, since it critically relies on a monocular depth prediction, a poor depth prediction will lead to distorted geometry with missing details. In addition, the way it expands a scene (i.e., by leveraging a moving perspective camera) makes it difficult to be extended beyond the room scale. This is because a perspective camera is prone to occlusion. For instance, when the camera passes through a wall, the continuity of the image can be disrupted by occlusion, which could also lead to discontinuities in the generated 3D shapes.\n' +
      '\n' +
      'Instead of generating 3D through 2D image lifting, another research direction involves directly learning to produce 3D data, using supervision from either 3D shape ground truths or posed multi-view images. Notable methods include EG3D [23], Rodin [12], and Get3D [14], etc. These approaches represent 3D data with a continuous hybrid neural field architecture, typically consisting of a tri-plane and an MLP decoder. The tri-plane is a tensor used to factorize the dense 3D volume grid. It is built on three axis-aligned 2D planes: the XY, YZ, and XZ planes. The MLP decoder converts the tri-plane feature into a continuous value representing the scene, which could be occupancy, signed distance field (SDF), radiance field [13], etc. Tri-plane is significantly more compact and computationally efficient than a full 3D tensor and conducive to generative architectures developed for 2D image synthesis. This has been a key factor in making high-quality direct 3D data generation possible.\n' +
      '\n' +
      'In this paper, we develop a tri-plane diffusion based approach to generate expandable 3D scenes. Our method is called BlockFusion. It generates 3D scenes in the form of cubic blocks and extends the scene in a straightforward sliding-block way. To generate high-quality 3D shapes, we directly train BlockDiffusion on 3D scene datasets. For network training, we randomly crop complete 3D scenes into incomplete 3D blocks with fixed sizes. We run per-block fitting to convert all training blocks into tri-planes, which we call the raw tri-planes. We found that directly training diffusion on raw tri-planes results in undesirable collapsed shapes. This issue is possibly caused by the high redundancy in the raw tri-planes and the substantial shape variance in the data. Inspired by stable diffusion [13], we apply an auto-encoder to compress the raw tri-planes into a latent tri-plane space to run diffusion. The latent tri-plane space is significantly more compact and computationally efficient than the raw tri-plane while maintaining similar representation power. In contrast to previous work, tri-plane diffusion on such a latent representation allows for the first time to reach high-quality and diverse 3D shape generation at scene level.\n' +
      '\n' +
      'To expand a scene, we add empty blocks to overlap with the current scene and extrapolate existing tri-planes to populate the new blocks. Specifically, the extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the reverse diffusion iterations. The extrapolation is also carried out in the latent tri-plane space. This process produces semantically and geometrically meaningful transitions that seamlessly blend with the existing scene, ensuring a coherent and visually pleasing scene expansion.\n' +
      '\n' +
      'To provide users with more control over the generation process, we introduce a 2D layout conditioning mechanism, which allows users to precisely determine the placement and arrangement of elements by manipulating 2D object bounding boxes. We also demonstrate that the color and texture of the scenes can be created using an off-the-shelf texture generation tool, thereby increasing the visual allure of the scene.\n' +
      '\n' +
      'To summarize, BlockFusion presents 1) a generalizable, high-quality 3D generation model based on latent tri-plane diffusion, 2) a latent tri-plane extrapolation mechanism that allows harmonious scene expansion, and 3) a 2D layout condition mechanism for precise control over scene generation. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '### Diffusion models\n' +
      '\n' +
      'Starting from Gaussian noise samples, Diffusion probabilistic models (Ho et al., 2020; Sohl-Dickstein et al., 2015) generate clear images by learning to progressively remove noise from the original noise sample. Recent advances in diffusion models (Dhariwal and Nichol, 2021; Nichol and Dhariwal, 2021; Ramesh et al., 2022; Saharia et al., 2022) have demonstrated unprecedented capabilities in synthesizing high-quality and diverse images. Nonetheless, training diffusion models directly in high-resolution pixel space can be computationally prohibitive. Latent diffusion models (LDMs) (Rombach et al., 2022) address this issue with a two-stage approach: they first compress the image through an auto-encoder and then apply diffusion models on smaller spatial representations in the latent space. Diffusion models can be trained with guiding information (e.g., text prompt, semantic layout, category label) to facilitate personalization, customization, or task-specific image generation. There are basically two ways of manipulating generated content. The first is realized through training a new model from scratch or finetuning a pretrained diffusion model, adding various conditioning controls, e.g., sketch, depth, segmentation, (Avrahami et al., 2023; Bashkirova et al., 2023; Brooks et al., 2023; Gal et al., 2022; Huang et al., 2023; Li et al., 2023; Mou et al., 2023; Nichol et al., 2021; Ramesh et al., 2022; Rombach et al., 2022, 2022; Ruiz et al., 2023; Voynov et al., 2023; Wang et al., 2022; Zhang et al., 2023). This approach requires extensive dataset building and extra computational consumption. The other line of methods adapts pretrained model and add some controlled generation capability during inference. With only slight modification to the generative process, (Avrahami et al., 2022; Bar-Tal et al., 2023; Hertz et al., 2022; Tumanyan et al., 2023) examine a wide variety of controlling diffusion models in a training/finetuning-free way.\n' +
      '\n' +
      '### 3D shape generation\n' +
      '\n' +
      'The success of 2D generation tools based on diffusion models, notably Stable Diffusion(Rombach et al., 2022), Midjourney, and Dall-E, has significantly sparked interest in the development of 3D generation tools. There are two main streams for this task: the methods that lift 2D (generated) images into 3D models, and the methods that directly run diffusion on 3D data.\n' +
      '\n' +
      '**2D-lifting methods.** DreamFusion (Poole et al., 2022) optimizes a Neural Radiance Field (Mildenhall et al., 2021) using the Score Distillation Sampling (SDS) loss, which distills prior knowledge from 2D image diffusion models into the volume rendering output of the NeRF. Magic3D (Lin et al., 2023) adopts an SDS loss-based second stage to further refine the mesh extracted from DreamFusion. SDS-based approaches demonstrate impressive results. However, they typically require hours of optimization and struggle with maintaining shape consistency, leading to a phenomenon called the Janus-face problem (Poole et al., 2022). Several methods have been developed that focus on the direct generation of consistency-enhanced multi-view 2D images, and these techniques reconstruct 3D shapes from the generated multi-view images. Zero123 (Liu et al., 2023) fine-tunes Stable Diffusion model (Rombach et al., 2022) to generate novel views by conditioning on the input image and camera transformation. One2345 (Liu et al., 2023) converts the multi-view image from Zero123 to 3D using an SDF-based neural surface reconstruction method. One2345++ (Liu et al., 2023) fine-tunes a 2D diffusion model for consistent multi-view image generation, and then elevating these images to 3D with the aid of multi-view conditioned 3D diffusion models. Syncedreamer (Liu et al., 2023) and Consistent (Yang et al., 2023) synchronize multi-view image generation process by explicitly correlating features in 3D space. Wonder3d (Long et al., 2023) improves the generation fidelity by introducing a cross-domain diffusion model that generates multi-view normal maps in addition to the color images. LRM (Hong et al., 2023) treats the single-image-to-3D problem as a reconstruction problem and solves it using Transformer in a deterministic way. However, LRM can lead to blurry and washed-out textures for unseen parts of objects due to mode averaging. To address this issue, Instant3D (Li et al., 2023) inputs multi-view consistent images into LRM to infer geometry and textures for unseen parts. DMV3D (Xu et al., 2023) employs LRM as a multi-view denoiser, which iteratively produces a cleaner tri-plane NeRF from noisy sparsely posed multi-view images.\n' +
      '\n' +
      '**3D diffusion models.** Another line of research involves directly training diffusion models to generate 3D shapes. As the supervision comes directly from 3D shape ground truth or posed multi-view images, the generated results typically exhibit superior geometric quality compared to those from 2D diffusion-based methods. These methods can be categorized based on the type of 3D representations they employ, including: polygon meshes (Gao et al., 2022; Liu et al., 2023), point clouds (Nichol et al., 2022; Zeng et al., 2022), explicit 3D grids holding occupancy or SDF values (Liu et al., 2023; Zheng et al., 2023), or neural fields (Chen et al., 2023; Chou et al., 2023; Erkoc et al., 2023; Jun and Nichol, 2023; Muller et al., 2023; Shue et al., 2023; Wang et al., 2023; Xu et al., 2023). The hybrid neural field, which incorporates a tri-plane followed by a neural decoder, has been widely adopted in 3D diffusion models due to its computational efficiency. Rodin (Wang et al., 2023) first fits tri-plane NeRFs for a human upper body dataset, and then uses a two-stage coarse-to-fine diffusion model to generate the corresponding tri-planes. Similarly, NFD (Shue et al., 2023) trains a tri-plane diffusion model for 3D data parametrized via occupancy values. SSDNerf [31] merge tri-plane fitting and generation into a single-stage pipeline. However, in practice, tri-plane diffusion is still challenging to train due to its high dimensionality and irregularity. Existing methods only demonstrated simple cases with small data varieties, i.e., Rodin for canonicalized human upper-body dataset [24], and NFD and SSDNeRF for single-category objects in ShapeNet [3]. This paper follows this line of research but introduces a major change: we use an auto-encoder to compress the tri-plane into a highly compact latent tri-plane space for diffusion. We demonstrate that this approach significantly improves the stability, generalizability, and output quality of tri-plane diffusion.\n' +
      '\n' +
      '### 3D scene generation\n' +
      '\n' +
      'Generating 3D scenes presents a more substantial challenge than generating single objects. This is because scenes are geometrically more complex than individual objects, and they cannot be contained in a fixed spatial size. Object retrieval-based approaches assume there is a database of objects, and they arrange the retrieved objects to fill an empty scene, as seen in Diffuscene [19] and Sceneformer [23], consequently, the synthesized scene can not contain novel elements that do not exist in the database. Text2Room [12] is the first method that uses 2D diffusion model to build a 3D generation tool. It first generates color and depth frames using 2D diffusion models, and then shift camera positions to generate new frames, which are integrated into a global map. A similar approach for broader outdoor scenarios can be found in SceneScape [14]. To allow precise control over the contents generated in a scene, ControlRoom3D[15] and CTRL-ROOM[22] develop panorama-based room generation models that take 3D room layouts as input conditions. Citygen [3] represents city scenes using the height map proxy, leading to a 2.5D scene generation. Other approaches focus on generating scenes with high-quality visual appearances. Given a room scene mesh, MVDiffusion [19] generates coherent multiview perspective images, which can be lifted to the 3D as the UV texture of the mesh. SceneDreamer [3] leverages in-the-wild 2D images to construct large scenes with photo-realistic volume rendering effects. However, it still relies on high-quality 3D scene meshes as input, consequently, the dimensions of the generated scenes are bounded by the dimensions of the input meshes.\n' +
      '\n' +
      'In this paper, we address the fundamental challenge of generating an unbounded scene by developing an auto-regressive scene expansion algorithm based on tri-plane diffusion.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'BlockFusion generates scenes as blocks and expands scenes using a sliding-window progressive generation approach. Fig. 2 presents the training pipeline. This section is organized as follows:\n' +
      '\n' +
      '* Sec. 3.1 demonstrates how the training blocks are generated.\n' +
      '* Sec. 3.2: we run per-block fitting to convert all training blocks into tri-planes, which we call the raw tri-planes.\n' +
      '* Sec. 3.3: the raw tri-planes are compressed into a latent tri-plane space for efficient 3D representation.\n' +
      '* Sec: 3.4: we train the diffusion model on the latent tri-plane space.\n' +
      '* Sec. 3.5: we leverage the pre-trained latent tri-plane diffusion model to expand a scene.\n' +
      '* Sec. 3.6: a post-processing technique is adapted to reduce seams.\n' +
      '* Sec. 3.7: large scenes are built by running BlockFusion progressively.\n' +
      '\n' +
      '### Crop training scenes into 3D blocks\n' +
      '\n' +
      'We use scene meshes for network training. We convert scene meshes to water-tight meshes and then randomly crop the meshes into cubic blocks. The size of the block is adjusted such that it is large enough to enclose major objects in the scene, e.g. beds in room scenes, or houses in outdoor scenes. Given that the blocks are randomly positioned within the scene, objects may be split by these blocks. In addition, the possible arrangements of objects within a block are limitless. Considerably, the variance in such a randomly cropped shape dataset is much larger than that of a single object-centered dataset. As a result, training diffusion on this type of data presents a greater challenge. We test on three different types of scenes including room, city, and village. Examples of training blocks can be found in Fig. 3. In addition to the shapes, we also create a 2D layout map for each scene. The layout map is the ground plane projection of the objects, grouped by their categories. These layout maps can be used\n' +
      '\n' +
      'Figure 2. **BlockFusion training pipeline.** The training contains three steps: First, **1)** the training 3D blocks are converted to raw tri-planes via per-block shape fitting, c.f. Sec. 3.2. Then, **2)** an auto-encoder compresses the raw tri-planes into a more compact latent tri-plane space, c.f. Sec. 3.3. Lastly, **3)** DDPM is trained to approximate the distributions of latent tri-planes, and during this process, layout control can also be integrated, c.f. Sec. 3.4.\n' +
      '\n' +
      'as input conditions for diffusion, so we also crop them accordingly. Examples can be seen in Fig. 2.\n' +
      '\n' +
      '### Raw tri-plane fitting\n' +
      '\n' +
      '**Hybrid Neural SDF.** We use the signed distance field (SDF) to represent the shape. An SDF is a continuous distance function with values indicating the distance to the surface and signs indicating whether a point is inside or outside the object. The final surface can be extracted via marching cubes. The shape is reconstructed using the hybrid neural field structure, which consists of a tri-plane to hold the geometry feature and a multiple layer perceptron (MLP) with parameter \\(\\theta\\) to decode the signed distance value. The tri-plane is a tensor used to factorize the dense 3D volume grid. It is built on three axis-aligned 2D planes: the XY, YZ, and XZ planes. Formally, it reads \\(x=\\{x(i)\\}\\)\\(\\mathbf{x}(i)\\in\\mathbb{R}^{N^{2}\\times C}\\), \\(i\\in\\{1,2,3\\}\\big{\\}}\\), where \\(N^{2}\\) is the plane resolution and \\(C\\) is the dimension of the feature. Given a query point \\(p\\in\\mathbb{R}^{3}\\), the function \\(\\Phi:\\mathbb{R}^{3}\\mapsto\\mathbb{R}\\) outputs the signed distance value:\n' +
      '\n' +
      '\\[\\Phi(p)=\\text{MLP}_{\\theta}\\bigg{(}\\bigoplus_{i\\in\\{1,2,3\\}}\\text{Interp}_{X( i)}\\big{(}\\text{Proj}_{X(i)}\\big{(}p\\big{)}\\big{)}\\bigg{)} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\text{Proj}(\\cdot)\\) represents orthogonal point-to-plane projection, \\(\\text{Interp}(\\cdot)\\) refers to bi-linear interpolation that queries feature vectors from each plane respectively, and \\(\\oplus\\) denotes element-wise addition. The addition operation is performed along the feature dimension, reducing the three feature vectors into a single final feature.\n' +
      '\n' +
      '**Training points sampling.** Given the mesh of a training block, we sample on-surface points and off-surface points, and then compute the ground truth SDF values. On-surface point set, denoted as \\(\\Omega_{0}\\), is randomly sampled on the surface. Their SDF values are equal to zero and we also compute the surface normal GT for each of these points. Off-surface point set denoted as \\(\\Omega\\) is sampled uniformly at random inside the block. To avoid incorrect distance values resulting from mesh cropping, the ground truth SDF values of the off-surface points are computed with respect to the original water-tight mesh. We empirically found that the point set sizes \\(|\\Omega|=100000\\) and \\(|\\Omega_{0}|=500000\\) achieve solid shape-fitting results while maintaining optimization costs at a manageable level. The XYZ coordinates of all sampled points are normalized to the range [-1, 1].\n' +
      '\n' +
      '**Triplane Fitting.** Our goal is to transform all training blocks into tri-planes, which will then be used for training our generative model. Inspired by pioneering works on shape representation with neural field-based SDFs (Atzmon and Lipman, 2020; Gropp et al., 2020; Park et al., 2019), we jointly optimize tri-plane \\(x\\) and MLP weights \\(\\theta\\) with the following geometry loss:\n' +
      '\n' +
      '\\[\\mathcal{L}_{geo}=\\mathcal{L}_{SDF}+\\mathcal{L}_{Normal}+\\mathcal{L}_{Eikonal} \\tag{2}\\]\n' +
      '\n' +
      'The three terms are:\n' +
      '\n' +
      '\\[\\mathcal{L}_{SDF}=\\lambda_{1}\\sum_{p\\in\\Omega_{0}}||\\Phi(p)||+ \\lambda_{2}\\sum_{p\\in\\Omega}||\\Phi(p)-d_{p}|| \\tag{4}\\] \\[\\mathcal{L}_{Normal}=\\lambda_{3}\\sum_{p\\in\\Omega_{0}}||\\nabla_{ p}\\Phi(p)-\\mathrm{n}_{p}||\\] (5) \\[\\mathcal{L}_{Eikonal}=\\lambda_{4}\\sum_{p\\in\\Omega_{0}}||\\nabla_{ p}\\Phi(p)|-1|| \\tag{3}\\]\n' +
      '\n' +
      'where \\(d_{p}\\) and \\(\\mathrm{n}_{p}\\) are ground truth SDF values and surface normal vector. The gradient \\(\\nabla_{p}\\Phi(p)=[\\frac{\\partial\\Phi(p)}{\\partial X},\\frac{\\partial\\Phi(p)}{ \\partial Y},\\frac{\\partial\\Phi(p)}{\\partial Z}]\\) represents the direction of the steepest change in SDF. It can be computed using finite difference, e.g. the partial derivative for the X-axis component reads\n' +
      '\n' +
      '\\[\\frac{\\partial\\Phi(p)}{\\partial X}=\\frac{\\Phi(p+[\\delta,0,0])-\\Phi(p-[\\delta,0, 0])}{2\\delta} \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\delta\\) is the step size. The Eikonal loss constrains \\(|\\nabla_{p}\\Phi(p)|\\) to be \\(1\\) almost everywhere, thus maintaining the intrinsic physical property of the signed distance function. We adopt the MLP initialization trick as introduced in SAL (Atzmon and Lipman, 2020), which constrains the initial SDF output to roughly approximate a sphere. This spherical geometry initialization technique significantly facilitates global convergence. Empirically, the loss weights are set to \\(\\lambda_{1}=100.0\\), \\(\\lambda_{2}=3.0\\), \\(\\lambda_{3}=1.0\\), and \\(\\lambda_{4}=0.5\\) across all datasets. The MLP is jointly trained with the tri-planes using a training subset consisting of 500 blocks. Upon convergence, the MLP is regarded as a generalizable SDF decoder. Then we freeze MLP and optimize the tri-planes for all blocks in the training data. In this work, the output tri-plane size is set to \\(N^{2}=128^{2},C=32\\). Following (Yan et al., 2024), a tri-plane is optimized in a coarse-to-fine manner, i.e., the resolution is initialized with \\(8^{2}\\) and gradually up-scaled to \\(128^{2}\\). Compared to directly optimizing at the final resolution, this trick significantly improves fitting robustness and reduces running time.\n' +
      '\n' +
      'Now, we can convert a dataset of 3D blocks into a dataset of tri-planes with size \\(3\\times 128^{2}\\times 32\\). These tri-planes can faithfully reconstruct the 3D blocks, c.f. Fig. 10. We call them as the _raw tri-planes_.\n' +
      '\n' +
      '### Compressing to latent tri-plane space\n' +
      '\n' +
      'Although our raw tri-planes can reconstruct high-quality shapes, we found that generating such tri-planes is significantly difficult. Directly training diffusion models on such tri-planes leads to collapsed results, as shown in Fig. 4. We argue that there are mainly two reasons for this: 1) the raw tri-plane is highly redundant, and 2) the shape diversity in our scene block dataset is too large. Although previous works like Rodin (Wang et al., 2023) and NFD (Shue et al., 2023) have proven the feasibility of diffusion on raw tri-planes, they only work on datasets with much smaller varieties, i.e., Rodin for canonicalized human upper bodies, and NFD for single-category\n' +
      '\n' +
      'Figure 3. Examples of randomly cropped 3D blocks.\n' +
      '\n' +
      'objects from ShapeNet (Chang et al., 2015). When we attempted to retrain NFD on our scene blocks, it also failed to produce meaningful shapes, as shown in Fig. 4.\n' +
      '\n' +
      'We need to find a feature representation for 3D shapes that is compact, easy to train diffusion models, memory and computationally efficient, and capable of generalizing to large shape variations. In the 2D scenario, Stable Diffusion (Rombach et al., 2022) compresses raw images into a latent 2D feature space for diffusion. This approach results in a more robust model that generates higher-quality images. Inspired by Stable Diffusion, we train an auto-encoder to compress raw tri-planes into a latent tri-plane space with reduced resolution and feature channels. Precisely, given an raw tri-plane \\(x\\in\\mathbb{R}^{3\\times N^{2}\\times C}\\), the encoder \\(\\mathcal{E}\\) encodes \\(x\\) into a latent representation \\(z=\\mathcal{E}(x)\\), and the decoder \\(\\mathcal{D}\\) reconstructs the raw tri-plane from the latent, giving \\(\\dot{x}=\\mathcal{D}(z)=\\mathcal{D}(\\mathcal{E}(x))\\).\n' +
      '\n' +
      '**Training objective** of the auto-encoder is shown as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{AE}=\\mathcal{L}_{rec}(x,\\mathcal{D}(\\mathcal{E}(x)))+\\mathcal{L} _{KL}(x,\\mathcal{D},\\mathcal{E})+\\mathcal{L}_{geo} \\tag{7}\\]\n' +
      '\n' +
      'where \\(\\mathcal{L}_{rec}\\) is a light \\(L_{1}\\) norm applied between \\(x\\) and its reconstruction \\(\\mathcal{D}(\\mathcal{E}(x))\\). \\(\\mathcal{L}_{KL}\\) is a the Kullback-Leibler-term between \\(q_{\\mathcal{E}}(z|x)=\\mathcal{N}(z;\\mathcal{L}_{\\mathcal{E}},\\mathcal{L}_{geo ^{2}})\\) and a standard normal distribution \\(N(z;0,1)\\) as in a standard vae (Kingma and Welling, 2013). To obtain high-fidelity shape reconstructions we only use a very small weight for \\(\\mathcal{L}_{KL}\\). \\(\\mathcal{L}_{geo}\\) is the geometry loss defined in Eqn. 2. It is assessed based on the same set of points as in Sec. 3.2. Since the purpose is to learn a latent tri-plane that can faithfully represent the shape, we rely on \\(L_{geo}\\) as the dominate loss for training the auto-encoder.\n' +
      '\n' +
      'The latent \\(z\\) mantains a tri-plane structure with \\(z=\\big{\\{}z(i)|z(i)\\in\\mathbb{R}^{n^{2}\\times c},i\\in\\{1,2,3\\}\\big{\\}}\\). We call it as the _latent tri-plane_. This is in contrast to the previous work DiffusionSDF (Chou et al., 2023), which relies on an arbitrary one-dimensional latent vector \\(z\\) to model its distribution autoregressively and thereby ignores much of the inherent 3D structure of \\(z\\). Hence, our compression model preserves details of \\(x\\) better (see Fig. 10). Empirically, the latent resolution is set to \\(n^{2}=32^{2}\\). And we investigate two different latent feature dimensions with \\(c=2\\) and \\(c=16\\).\n' +
      '\n' +
      '### Latent Triplane Diffusion\n' +
      '\n' +
      'With our trained tri-plane auto-encoder, comprising \\(\\mathcal{E}\\) and \\(\\mathcal{D}\\), we now have access to an efficient, low-dimensional latent tri-plane space where high-frequency, imperceptible details are abstracted away. In comparison to the raw tri-plane space, the latent tri-plane space is more suitable for likelihood-based generative models, as they can now concentrate on the essential, semantic aspects of the data and train in a lower-dimensional, computationally much more efficient space.\n' +
      '\n' +
      '**Background on Diffusion Probabilistic Models.** Diffusion Models are probabilistic models designed to learn a data distribution \\(z_{0}\\sim q(z_{0})\\) by gradually denoising a normally distributed variable. This process corresponds to learning the reverse operation of a fixed Markov Chain with a length of \\(T\\). The inference process works by sampling a random noise \\(z_{T}\\) and gradually denoising it until it reaches a meaningful latent \\(z_{0}\\). DDPM (Ho et al., 2020) defines a diffusion process that transform latent \\(z_{0}\\) to white Gaussian noise \\(z_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\) in \\(T\\) time steps. Each step in the forward direction is given by:\n' +
      '\n' +
      '\\[q(z_{1},...,z_{T}|z_{0})=\\prod_{t=1}^{T}q(z_{t}|z_{t-1}) \\tag{8}\\]\n' +
      '\n' +
      '\\[q(z_{t}|z_{t-1})=\\mathcal{N}(z_{t};\\sqrt{1-\\beta_{t}}z_{t-1},\\beta_{t} \\mathbf{I}) \\tag{9}\\]\n' +
      '\n' +
      'The noisy latent \\(z_{t}\\) is obtained by scaling the previous noise sample \\(z_{t-1}\\) with \\(\\sqrt{1-\\beta_{t}}\\) and adding Gaussian noise with variance \\(\\beta_{t}\\) at timestep \\(t\\). During training, DDPM reverses the diffusion process, which is modeled by a neural network \\(\\Psi\\) that predicts the parameters \\(\\mu_{\\Psi}(z_{t},t)\\) and \\(\\Sigma_{\\Psi}(z_{t},t)\\) of a Gaussian distribution.\n' +
      '\n' +
      '\\[p_{\\Psi}(z_{t-1}|z_{t}) =\\mathcal{N}(z_{t-1};\\mu_{\\Psi}(z_{t},t),\\Sigma_{\\Psi}(z_{t},t)) \\tag{10}\\]\n' +
      '\n' +
      'With \\(\\alpha_{t}:=1-\\beta_{t}\\) and \\(\\bar{\\alpha}:=\\prod_{s=0}^{t}a_{s}\\), we can write marginal distribution:\n' +
      '\n' +
      '\\[q(z_{t}|z_{0}) =\\mathcal{N}(z_{t};\\sqrt{\\bar{\\alpha}_{t}}z_{0},(1-\\bar{\\alpha}_ {t})\\mathbf{I}) \\tag{12}\\] \\[z_{t} =\\sqrt{\\bar{\\alpha}_{t}}z_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon \\tag{11}\\]\n' +
      '\n' +
      'where \\(\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\). Using Bayes theorem, one can calculate the posterior \\(q(z_{t-1}|z_{t},z_{0})\\) in terms of \\(\\tilde{\\beta}_{t}\\) and \\(\\tilde{\\mu}_{t}(z_{t},z_{0})\\) which are defined as follows:\n' +
      '\n' +
      '\\[\\tilde{\\beta}_{t}:=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t} \\tag{13}\\]\n' +
      '\n' +
      '\\[\\tilde{\\mu}_{t}(z_{t},z_{0}):=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_{t}}{1-\\bar{\\alpha}_{t}}z_{0}+\\frac{\\sqrt{\\bar{\\alpha}_{t}}(1-\\bar{ \\alpha}_{t-1})}{1-\\bar{\\alpha}_{t}}z_{t} \\tag{14}\\]\n' +
      '\n' +
      '\\[q(z_{t-1}|z_{t},z_{0}) =\\mathcal{N}(z_{t-1};\\tilde{\\mu}_{t}(z_{t},z_{0}),\\tilde{\\beta}_{t} \\mathbf{I}) \\tag{15}\\]\n' +
      '\n' +
      'There are different ways to parameterize \\(\\mu_{\\Psi}(z_{t},t)\\) in the prior. Instead of predicting the added noise as in the original DDPM, in this paper, we predict \\(z_{0}\\) directly with a neural network \\(\\Psi\\). The prediction could be used in Eqn. 14 to produce \\(\\mu_{\\Psi}(z_{t},t)\\). Specifically, with a\n' +
      '\n' +
      'Figure 4. **Qualitative unconditioned block generation results.** NFD (Shue et al., 2023) is also based on tri-plane diffusion. They utilize occupancy value to represent shapes, whereas ours employ SDF. All three methods are trained on room blocks.\n' +
      '\n' +
      'uniformly sampled time step \\(t\\) from \\(\\{1,...,T\\}\\), we sample noise to obtain \\(z_{t}\\) from input latent vector \\(z_{0}\\). A time-conditioned denoising auto-encoder \\(\\Psi\\) learns to reconstruct \\(z_{0}\\) from \\(z_{t}\\). The objective of latent tri-plane diffusion reads\n' +
      '\n' +
      '\\[\\mathcal{L}_{LTD}=\\|\\Psi(z_{t},\\gamma(t))-z_{0}\\|_{2} \\tag{16}\\]\n' +
      '\n' +
      'where \\(\\gamma(\\cdot)\\) is a positional encoding function and \\(\\|\\cdot\\|_{2}\\) is MSE loss. Since the forward process is fixed, \\(z_{t}\\) can be efficiently obtained from \\(\\mathcal{E}\\) during training. During test time, we iteratively denoise \\(z_{T}\\) until we obtain the final output \\(z^{\\prime}\\). \\(z^{\\prime}\\) can be decoded to the raw tri-plane \\(x^{\\prime}\\) with a single pass through \\(\\mathcal{D}\\). Finally, the pretrained MLP decodes \\(x^{\\prime}\\) to a dense SDF volume for marching cube-shape extraction.\n' +
      '\n' +
      '**2D layout as user control.** To control the generation process, we add floor layout control by informing the model with 2D bounding box projections of objects. The floor layout is converted into a feature map \\(l\\in\\mathbb{R}^{n^{2}\\times m}\\), where \\(n^{2}\\) represents the feature resolution (identical to the plane resolution in latent \\(z\\)), and the channel number \\(m\\) corresponds to the total number of object categories. Each channel consists of a binary image indicating whether or not an object class is placed. The loss of layout-conditioned latent tri-plane diffusion reads\n' +
      '\n' +
      '\\[\\mathcal{L}_{c-LTD}=\\|\\Psi(z_{t},\\gamma(t),l)-z_{0}\\|_{2} \\tag{17}\\]\n' +
      '\n' +
      'In practice, \\(l\\) is directly concatenated to three planes of \\(z_{t}\\). In our experiments, we show that this type of conditioning successfully controls the arrangement of scene elements while still preserving variance in the generated shapes.\n' +
      '\n' +
      '**3D aware denoising U-Net.** The neural backbone \\(\\Psi(\\cdot)\\) of our model is realized as a time-conditional U-Net. The advantage of tri-planes is that we can treat them as 2D tensors and therefore apply efficient 2D convolutions. However, naively running convolution on tri-planes does not produce satisfactory results, as the 3D relationships among the plane features are ignored. \\(\\Psi(\\cdot)\\) needs to incorporate operations that can account for the cross-plane feature relationships. To address this, Rodin (Wang et al., 2023) introduces a 3D-aware convolution, which employs max-pooling and concatenation to associate features between planes based on their 3D correlations. However, the simple max-pooling may lose valuable information. In this work, we build \\(\\Psi(\\cdot)\\) by leveraging the more powerful transformer to achieve cross-plane communication. The overall architecture of \\(\\Psi(\\cdot)\\) is shown in Fig. 5. This architecture enables effective 3D-aware feature learning.\n' +
      '\n' +
      '### Latent tri-plane Extrapolation\n' +
      '\n' +
      'Repaint (Lugmayr et al., 2022) demonstrate impressive image inpainting and extrapolation results using a pre-trained diffusion model. Their key idea is to synchronize the denoising process of the unknown pixels using the noised version of the known pixels. Inspired by Repaint, we leverage our pre-trained denoising backbone \\(\\Psi(\\cdot)\\) to extrapolate tri-planes. The extrapolation is carried out in the latent tri-plane space. Formally, given a known block \\(P\\) with latent code \\(z^{P}=\\left\\{z^{P}(i)|i\\in\\{1,2,3\\}\\right\\}\\) as a condition, and an empty block \\(Q\\) that partially overlaps with \\(P\\), the goal is to generate the latent tri-plane \\(z^{Q}=\\left\\{z^{Q}(i)|i\\in\\{1,2,3\\}\\right\\}\\) that can represent the new block. For simplicity, this paper only considers the case where \\(Q\\) is positioned by sliding along only one of the XYZ axes, which is sufficient for scene expansion.\n' +
      '\n' +
      '**Plane-wise extrapolation.** The tri-plane is a factored representation of a dense 3D volume. The three planes are compressed but highly correlated, which makes extrapolation on tri-planes a non-intuitive task. To address this, as shown in Fig. 6, we factor tri-plane extrapolation into the extrapolation of three 2D planes separately, and then utilize our 3D-aware denoising backbone, \\(\\Psi\\), to blend information from the three planes. Specifically, given the \\(i\\)-th axis-align plane with \\(i\\in\\{1,2,3\\}\\), the overlap mask between plane \\(z^{P}(i)\\) and plane \\(z^{Q}(i)\\) is denoted as \\(O_{i}\\). Following Repaint (Lugmayr et al., 2022), extrapolating \\(z^{P}(i)\\) to obtain \\(z^{Q}(i)\\) is realized by synchronizing the denoising process of \\(z^{Q}(i)\\) using the noised version \\(z^{P}(i)\\) inside the overlap mask \\(O_{i}\\). Specifically, at step \\(t-1\\), we obtain the noised \\(z^{P}_{t-1}(i)\\) via\n' +
      '\n' +
      '\\[z^{P}_{t-1}(i)\\sim\\mathcal{N}(\\sqrt{\\alpha_{t}}z^{P}_{0}(i),(1-\\alpha_{t}) \\mathbf{I}) \\tag{18}\\]\n' +
      '\n' +
      'Fig. 5. **3D aware denoising U-Net.** The latent tri-plane is unfolded into three independent planes to run down-sampling convolutions. After the down-sampling layers, the three feature maps are flattened into 1D tokens and concatenated together to forward through a sequence of self-attention (Vaswani et al., 2017) and residual block by \\(K=6\\) times. Finally, the 1D array is reshaped into planes for up-sampling convolution and re-assembled into the tri-plane structure.\n' +
      '\n' +
      'Fig. 6. **Latent triplane extrapolation.** Given the known block \\(P\\) and the unknown block \\(Q\\), the goal is to extrapolate the known latent tri-plane \\(z^{P}\\) to obtain the unknown tri-plane \\(z^{Q}\\) (top row). This tri-plane extrapolation is factored into the extrapolation of three 2D planes separately (bottom row).\n' +
      '\n' +
      'and the denoised \\(z_{t-1}^{Q}(i)\\) from previous step \\(t\\) by\n' +
      '\n' +
      '\\[z_{t-1}^{Q}(i)\\sim\\mathcal{N}(\\mu_{\\psi}(z_{t}^{Q}(i),t),\\Sigma_{\\psi}(z_{t}^{Q} (i),t)) \\tag{19}\\]\n' +
      '\n' +
      'Then, \\(z_{t-1}^{Q}(i)\\) is synchronized by\n' +
      '\n' +
      '\\[z_{t-1}^{Q}(i)\\leftarrow\\text{Cat}\\Big{(}z_{t-1}^{P}(i)\\in O_{i},\\ z_{t-1}^{Q}(i )\\notin O_{i}\\Big{)} \\tag{20}\\]\n' +
      '\n' +
      'where \\(\\text{Cat}(\\cdot)\\) refers to the tensor concatenation. However, as shown in Fig. 6, when \\(i=3\\), the two planes \\(z^{P}(3)\\) and \\(z^{Q}(3)\\) are parallel to each other, and thus they do not have explicit overlap. We can only perform synchronization for planes \\(i\\in\\{1,2\\}\\). Nevertheless, our denoising backbone \\(\\Psi\\) constructed using a sequence of self-attention layers is designed to identify cross-plane dependencies. This architecture allows the synchronized features in planes \\(\\{1,2\\}\\) to be effectively propagated to the 3rd plane via attention layers throughout the denoising steps. We found in experiments that this approach successfully achieves meaningful 3D shape extrapolation. The overall procedure for latent tri-plane extrapolation is outlined in Algorithm 1.\n' +
      '\n' +
      '```\n' +
      '\\(z_{T}^{Q}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\) for\\(t=T,...,1\\)do \\(z_{t-1}^{Q}\\sim\\mathcal{N}(\\sqrt{\\mu}z_{0}^{P},(1-\\hat{a}_{t})\\mathbf{I})\\) \\(z_{t-1}^{Q}\\sim\\mathcal{N}(\\mu_{\\psi}(z_{t}^{Q},t),\\Sigma_{\\psi}(z_{t}^{Q},t))\\) for\\(i\\in\\{1,2\\}\\)do \\(z_{t-1}^{Q}(i)\\leftarrow\\text{Cat}\\Big{(}z_{t-1}^{P}(i)\\in O_{i},z_{t-1}^{Q}(i )\\notin O_{i}\\Big{)}\\) endfor endfor return\\(z_{0}^{P},\\ z_{0}^{Q}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Latent tri-plane extrapolation\n' +
      '\n' +
      '**Resampling.** We found that simply applying synchronization does not always yield semantically and geometrically consistent results. This is because the noise-adding process in overlapping regions does not take into account the newly generated parts of the tri-plane in the non-overlapping region, thereby introducing disharmony. To address this issue, we leverage the resampling strategy as introduced in Repaint (Lugmayr et al., 2022). Specifically, at certain steps of the denoising process, noise is added again to the output using the forward diffusion equation in 9, meaning the inference process is rolled back. There are two hyper-parameters for resampling: 1) the roll-back step \\(J\\), and 2) the number of resampling times \\(R\\). In this paper, we set \\(J=100\\) and conduct an ablation study on \\(R=\\{0,1,2,3,7\\}\\). Experimental results show that increasing the number of resampling times enhances the generation performance.\n' +
      '\n' +
      '### Surface refinement with non-rigid registration\n' +
      '\n' +
      'The synchronized tri-planes \\(z^{P}\\) and \\(z^{Q}\\) generate shapes that are semantically aligned; however, the latent space synchronization does not guarantee point-accurately aligned shapes, resulting in small visible seams. To address this problem, we explicitly align the extracted surface mesh. From the two latent codes \\(z^{P}\\) and \\(z^{Q}\\), we derive dense SDF volumes and run marching cubes to extract the surface meshes \\(S^{P}\\) and \\(S^{Q}\\). We uniformly sample points on mesh triangles that lie inside the overlapping region, obtaining the point sets denoted as \\(\\Omega_{P}^{QI}\\) and \\(\\Omega_{Q}^{QI}\\). Then, we uniformly sample points on triangles of \\(S^{Q}\\) that lie outside of the overlapping region, resulting in a point set denoted as \\(\\Omega_{Q}^{new}\\). We align \\(S^{P}\\) and \\(S^{Q}\\) by optimizing the non-rigid registration cost:\n' +
      '\n' +
      '\\[\\mathcal{L}_{nrr}=\\mathcal{L}_{CD}\\Big{(}\\mathcal{W}\\big{(}\\Omega_{P}^{QI} \\big{)},\\Omega_{Q}^{QI}\\Big{)}+\\mathcal{L}_{CD}\\Big{(}\\mathcal{W}\\big{(} \\Omega_{Q}^{new}\\big{)},\\Omega_{Q}^{new}\\Big{)} \\tag{21}\\]\n' +
      '\n' +
      'where \\(\\mathcal{L}_{CD}(\\cdot)\\) represents the Chamfer Distance between two point clouds, and \\(\\mathcal{W}(\\cdot)\\) is the dense non-rigid warping function that predicts per-point transformations. \\(\\mathcal{W}(\\cdot)\\) is based on NDP (Li and Harada, 2022), which approximates scene deformation using hierarchical coarse-to-fine neural deformation fields. This non-rigid registration cost encourages the extrapolated mesh \\(S^{Q}\\) to approximate the condition mesh \\(S^{P}\\) as closely as possible within the overlapping region, while maintaining its own structure in the non-overlapping region.\n' +
      '\n' +
      '### Building unbounded large scenes with BlockFusion.\n' +
      '\n' +
      'Based on Algorithm 1, one can construct large, unbounded scenes at any scale. The naive strategy for this purpose involves initially creating a block and then expanding the scene by extrapolating block by block in the sliding window fashion. However, this serial operations requires a significant amount of time.\n' +
      '\n' +
      'Given that remote blocks are likely to be independent of each other, large scene generation can be executed in parallel. This process involves initially generating isolated seed blocks simultaneously, from which we extrapolate the remaining empty blocks, also in parallel. Specifically, we first use sliding window to sli ce the world into small blocks, denoted as \\(\\mathcal{B}=\\{B_{1},B_{2},...\\}\\), with overlaps between each pair of neighboring blocks We select a strided subset \\(\\mathcal{B}^{seed}\\) from those blocks. We make sure blocks in \\(\\mathcal{B}^{seed}\\) should not overlap with each other. The complementary set of \\(\\mathcal{B}^{seed}\\) is denoted as \\(\\mathcal{B}^{extra}\\). Blocks in \\(\\mathcal{B}^{seed}\\) are independently generated in parallel. The rest empty blocks in \\(\\mathcal{B}^{extra}\\) are extrapolated from \\(\\mathcal{B}^{seed}\\).\n' +
      '\n' +
      '## 4. Experimental Results\n' +
      '\n' +
      '### Implementation details.\n' +
      '\n' +
      '**Water-tight remeshing.** We use 3D scene meshes for network training. These scene meshes are typically created by 3D artists and are not always guaranteed to be watertight. We transform the raw meshes into watertight ones using Blender\'s _voxel remeshing_ tool. After remeshing, the object has a clearly defined inside and outside, which is essential for training a continuous neural field representation.\n' +
      '\n' +
      '**Datasets.** We test our algorithm on three different types of scenes: room, city, and village. Room scene data is obtained from 3DFront (Fu et al., 2021) and 3D-FUTURE (Fu et al., 2021), which contains 18,968 indoor scenes with 34 classes of indoor objects. We obtain 57K random crops from 3DFront, with each block size set to \\(3.2^{3}\\) cubic meters. We filtered out empty rooms and rooms with less than 5 objects and finally got 9123 rooms. For simplicity, we regroup the objects in 3DFUTURE based on their similarities into 9 classes: "floor",Our method is implemented using Pytorch and trained on Nvidia V100 GPU. For the 3DFront dataset with 57K cropped blocks, raw tri-plane fitting, auto-encoder training, and diffusion training take 4750, 768, and 384 GPU hours, respectively. Running a single tri-plane extrapolation under layout conditions costs 6 minutes. With the large scene generation strategy described in Sec. 3.7, producing the large indoor scene in Fig. 7 takes around 3 hours.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '**Reconstruction metric.** We evaluate the reconstruction quality using the Chamfer Distance (CD) at \\(10^{-3}\\) scale, Surface Normal Error (\\(E_{NRM}\\)) in degrees, and Surface SDF error (\\(E_{SDF}\\)) in centimeters.\n' +
      '\n' +
      '**Unconditioned generation metric.** The evaluation of unconditional 3D shape synthesis presents inherent challenges due to the lack of direct ground truth correspondence. Therefore, we resort to well-established metrics for evaluation, in line with previous works (Chou et al., 2023; Siddiqui et al., 2023; Zeng et al., 2022). These metrics include Minimum Matching Distance (MMD), Coverage (COV), and 1-Nearest-Neighbor Accuracy (1-NNA). For MMD, lower\n' +
      '\n' +
      'Figure 8. Large city scene generation.\n' +
      '\n' +
      'Figure 7. Large room scene generation.\n' +
      '\n' +
      'is better; for COV, higher is better; for 1-NNA, 50% is the optimal. We employ the Chamfer Distance (CD) and EMD (Earth Mover\'s Distance) as the distance measure for computing these metrics. More comprehensive details about these metrics are available in the respective literature.\n' +
      '\n' +
      '**User study metric.** We carried out a user study involving seven participants who were asked to rate the scene generation results based on Perceptual Quality (PQ) and Structure Completeness (SC) of the entire scene, using a scale from 1 to 5. This is done in two modes: textured mode (T-) and geometry-only mode (G-). In the Textured Mode, participants viewed a textured mesh, while in the Geometry-Only Mode, the texture was replaced with a monochrome material to emphasize the geometry. As a result, we derived four metrics from this study: T-PQ, T-SC, G-PQ and G-SC.\n' +
      '\n' +
      '### Comparison with SOTA.\n' +
      '\n' +
      '**Single block generation.** We regard NFD (Shue et al., 2023) as the baseline for single block generation. NFD is also based on tri-plane diffusion. However, they use occupancy values to represent shapes, whereas we employ SDF. We retrain NFD on our indoor scene blocks before evaluation. Tab. 2 and Fig. 4 show the results of unconditioned indoor block generation. Quantitatively, our method significantly outperforms NFD, with a 21.17% and 23.67% increase in coverage (Cov) scores under the CD and EMD metrics, respectively. Qualitatively, NFD is unable to generate meaningful shapes.\n' +
      '\n' +
      '**Indoor scene generation.** We consider Text2Room (Hollen et al., 2023) as the baseline for indoor scene generation. Text2Room takes text prompt as input whereas ours is based on 2D layout map. For a fair comparison, we describe our input room layout using natural language and then concatenate it as part of the text prompt for Text2Room. While our method does not directly generate textured mesh, we leverage an off-the-shelf text-to-texture generation tool, Meshy 1, to produce textures for our mesh. Meshy utilizes the same text prompt as Text2Room. To enhance the texture generation results of Meshy, we combine all blocks into a single entity using Blender\'s voxel remembsing tool. Tab. 2 and Fig. 4 show the results of room generation. Qualitatively, due to the use of monocular depth estimation, the shape of Text2Room appears distorted, while BlockFusion produces significantly better room shapes. In addition, Text2Room cannot precisely react to the text prompt; it generates\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{Textured} & \\multicolumn{2}{c}{Geometry-only} \\\\ \\cline{2-5}  & TPQ\\(\\uparrow\\) & TSC\\(\\uparrow\\) & GPC\\(\\uparrow\\) & GSC\\(\\uparrow\\) \\\\ \\hline Text2Room (Hollen et al., 2023) & 2.14 & 2.29 & 1.00 & 1.28 \\\\ \\hline Ours & **4.14** & **4.14** & **3.71** & **3.86** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Quantitative indoor scene generation results.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{MMD \\(\\downarrow\\)} & \\multicolumn{2}{c}{COV(\\(\\uparrow\\),\\(\\uparrow\\))} & \\multicolumn{2}{c}{1-**NN**(\\(\\uparrow\\),\\(\\downarrow\\))} \\\\ \\cline{2-7}  & CD & EMD & CD & EMD & CD & EMD \\\\ \\hline NFD (Shue et al., 2023) & 0.0445 & 0.2363 & 22.66 & 29.66 & 89.08 & 83.25 \\\\ \\hline Raw tri-plane Diff. (Ours) & 0.0544 & 0.2744 & 23.99 & 27.50 & 89.91 & 88.50 \\\\ \\hline Latent tri-plane Diff. (Ours) & **0.0324** & **0.1884** & **51.83** & **53.33** & **70.66** & **60.08** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Quantitative unconditional generation results for indoor blocks.\n' +
      '\n' +
      'Figure 9. **Qualitative room generation results.** Text2Room (Hollen et al., 2023) generates distorted shapes and cannot accurately respond to the number of objects in the scene. For instance, when given the prompt one bed, it generates multiple beds. In contrast, BlockFusion produces higher-quality shapes and correctly responds to numerical prompts.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '**How does resampling affect the extrapolation?** Fig. 12 shows the shape synchronization results of layout-conditioned tri-plane extrapolation. We tested different resampling times with \\(R=\\{1,2,3,7\\}\\). The chamfer distance drops steadily with more resampling steps and stabilizes after 3 resamplings, where the variance in the Chamer Distance also converges. This suggests that augmenting the number of resampling times can improve the quality of synchronization results. For clarity, \\(R=0\\) means that we do not perform synchronizations, i.e. the two blocks are generated independently while adhering to the shared layout conditions. Note that in this case, the Chamfer Distance is extremely high, indicating that using layout conditioning alone does not ensure consistent geometry between blocks.\n' +
      '\n' +
      '**Is non-rigid registration-based post-processing necessary?** Yes. Latent tri-plane extrapolation generates semantically and geometrically reasonable transitions. However, since the high-frequency, imperceptible details are abstracted away by the auto-encoder, extrapolation in the latent tri-plane space inevitably results in minor seams. As shown in Fig. 13, non-rigid registration-based post-processing can effectively mitigate this issue.\n' +
      '\n' +
      '**Does BlockFusion posses creativity?** BlockFusion does generate novel shapes that do not exist in the training dataset. This primarily arises from its ability to rearrange existing elements in novel ways. For instance, as shown in Fig. 14, BlockFusion manages to generate\n' +
      '\n' +
      'Fig. 11. **Qualitative results of tri-plane extrapolation.** The 3D box shows the block to extrapolate. The overlap ratios are 25% for top three rows and 50% for bottom three rows.\n' +
      '\n' +
      'Fig. 12. **Layout-conditioned tri-plane extrapolation with different resampling times (\\(R\\)).** The Chamfer Distance is calculated based on point sets sampled from the two block meshes within their overlapping region. The shape consistency significantly improves after 1-time synchronization, and employing additional synchronization steps (i.e. resampling) further enhances shape consistency. \\(R=0\\) means no synchronizations.\n' +
      '\n' +
      'a new table shaped like the number "24" and a novel room shaped like a heart. This is made possible by its ability to re-combine basic shapes, such as fractions of tables and walls, under layout guidance. This demonstrates the potential of BlockFusion as a powerful tool for generating diverse and visually appealing scenes.\n' +
      '\n' +
      '### Large Scene Generation.\n' +
      '\n' +
      'We showcase the capability of BlockFusion for large scene generation. The results are displayed in Fig. 1, 7, and 8, for village, city, and room scenes, respectively. The generation process is conditioned on layout maps that are created using an easy-to-use graphical user interface (GUI). It is important to emphasize that the scope of the scenes can be expanded infinitely. We believe that BlockFusion is the first method capable of generating 3D scenes at such a large scale while maintaining a high level of shape quality.\n' +
      '\n' +
      '## 5. Conclusion and Discussion\n' +
      '\n' +
      'Experiments show the proposed BlockFusion is capable of generating diverse, geometrically consistent, and unbounded large 3D scenes with high-quality geometry in both indoor and outdoor scenarios. The generated mesh can be seamlessly integrated with off-the-shelf texture generation tools, yielding textured results with visually pleasing appearance. We believe this approach represents an important step towards fully automated, industry-quality, large-scale 3D content generation.\n' +
      '\n' +
      'The expansive nature of BlockFusion allows it to serve as a map generator for open-world games. We integrate BlockFusion to Unity to develop such an open-world game, where players can roam and explore the world freely without being restricted by a predetermined world boundary. A demo of this can be found in the supplementary video.\n' +
      '\n' +
      '**Limitations.** The current implementation of BlockFusion faces several limitations. Our method may fail to generate very fine geometric details in the scene, such as the legs of a chair. This issue primarily stems from the limited resolution used for the tri-planes. A possible solution is to adopt tri-plane super-resolution. Moreover, the bounding box condition can only control the approximate placement of objects, not their orientations. We believe that precise orientation control could be achieved by training diffusion conditioning on both the bounding box map and an object orientation map. This orientation map can also be easily obtained from user instructions. Lastly, while we have demonstrated textured mesh results on small scenes, the task of generating globally consistent textures for large scene meshes is both a challenging and intriguing future endeavor.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* M. Atzmon and Y. Lipman (2020)Sal: sign agnostic learning of shapes from raw data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2565-2574. Cited by: SS1.\n' +
      '* O. Avraham, T. Hayes, O. Gafni, S. Gupta, Y. Taigman, D. Parikh, D. Lischinski, O. Fried, and X. Yin (2023)Spactnet: spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18370-18380. Cited by: SS1.\n' +
      '* O. Avraham, D. Lischinski, and O. Fried (2022)Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208-18218. Cited by: SS1.\n' +
      '* O. Bar-Tal, L. Yariv, Y. Lipman, and T. Dieek (2023)Multidiffusion: fusing diffusion paths for controlled image generation. Cited by: SS1.\n' +
      '* D. Bashkirova, J. Lezama, K. Sohn, K. Senehzo, and I. Essa (2023)MaskKetch: unpaired structure-guided instance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1879-1889. Cited by: SS1.\n' +
      '* T. Brooks, A. Holynski, and A. Efros (2010)Instruction2pix: learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392-18402. Cited by: SS1.\n' +
      '* E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo, L. J. Guhys, J. Tremblay, S. Khamis, et al. (2022)Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1612-1613. Cited by: SS1.\n' +
      '* A. X. Chang, T. Funkhouser, L. Guhys, P. Hancha, Q. Huang, Z. Li, S. Savares, M. Savus, S. Song, H. Su, et al. (2015)ShapeNet: an information-rich 3d model repository. arXiv preprint arXiv:1512.08012. Cited by: SS1.\n' +
      '* D. D. Chen, T. Siddiqi, H. Lee, S. Tulyakov, and M. Neiferen (2023)Telext: text-driven texture synthesis via diffusion models. arXiv preprint arXiv:2303.11396. Cited by: SS1.\n' +
      '* H. Chen, J. Gu, A. Chen, W. Tian, Z. Tu, L. Liu, and H. Su (2023)Single-stage diffusion net: a unified approach to 3d generation and reconstruction. arXiv preprint arXiv:2304.0714. Cited by: SS1.\n' +
      '* Z. Chen, G. Wang, and Z. Liu (2022)Sceontedreamer: unbounded 3d scene generation from 2d image collections. arXiv preprint arXiv:2302.01330. Cited by: SS1.\n' +
      '* G. Chou, Y. Bahat, and F. Heide (2023)Diffusion-sdf: conditional generative modeling of signed distance functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2262-2272. Cited by: SS1.\n' +
      '* J. Deng, W. Chai, J. Guo, Q. Huang, W. Hu, J. Hwang, and C. Wang (2023)City-cnn: infinite and controllable 3d city layout generation. arXiv preprint arXiv:2312.01508. Cited by: SS1.\n' +
      '* P. Dhariwal and A. Nichol (2021)Diffusion models beat gans on image synthesis. Advances in neural information processing systems 43, pp. 8780-8794. Cited by: SS1.\n' +
      '* Z. Erkoc, F. Ma, Q. Shan, M. Niessner, and A. Dai (2023)Hyperdiffusion: generating implicit neural fields with weight-space diffusion. arXiv preprint arXiv:2303.17015. Cited by: SS1.\n' +
      '\n' +
      'Fig. 14. **Using layout control to create rooms that do not exist in the training set.** The textures are generated using Text2tex (Chen et al., 2023b) using the corresponding text prompt.\n' +
      '\n' +
      'Fig. 13. Left: latent tri-plane extrapolation result, rights: after applying non-rigid registration.\n' +
      '\n' +
      '* Fang et al. (2023) Chuan Fang, Xiaotao Hu, Kunming Luo, and Ping Tan. 2023. Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints. _arXiv preprint arXiv:2310.03080_ (2023).\n' +
      '* Fridman et al. (2023) Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. 2023. SceneScape: Text-Driven Consistent Scene Generation. _arXiv preprint arXiv:2302.01133_ (2023).\n' +
      '* Fu et al. (2021) Hun Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jianming Wang, Cao Li, Qiun Zeng, Chengyue Sun, Rongfei Jia, Binjang Zhao, et al. 2021. 3d-front: 3d furnished rooms with layouts and semantics. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 10933-10942.\n' +
      '* Fu et al. (2021) Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binjang Zhao, Steve Maybank, and Dacheng Tu. 2021. 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision_ (2021), 1-25.\n' +
      '* Gil et al. (2022) Rinon Gil, Yival Alahiof, Yuval Atzmon, Or Patashnik, Amit H Bermona, Gal Chechik, and Daniel Cohen-Orz. 2022. An image is worth over one!. Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_ (2022).\n' +
      '* Gao et al. (2022) Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangvue Yin, Daiqing Li, Or Lianz, Zang Gojic, and Sanja Fidler. 2022. GeStd: A generative model of high quality 3d retrieved shapes learned from images. _Advances in Neural Information Processing Systems_ 35 (2022), 31841-31854.\n' +
      '* Gropp et al. (2020) Amos Gropp, Izor Yariv, Niri Ma, Matan Atzmon, and Yacim Lipman. 2020. Implicit geometric regularization for learning shapes. _arXiv preprint arXiv:2002.10099_ (2020).\n' +
      '* Hertz et al. (2002) Amir Hertz, Rox Mokady, J. Terenbaum, Kif Akerman, Yael Pritch, and Daniel Cohen-Orz. 2022. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01262_ (2022).\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. _Advances in neural information processing systems_ 33 (2020), 6840-6851.\n' +
      '* Holling et al. (2023) Lukas Holling, Gao Cao, Andrew Owens, Justin Johnson, and Matthias Niefner. 2023. Text2room: Extracting textured 3d meshes from 2d text-to-image models. _arXiv preprint arXiv:2303.11988_ (2023).\n' +
      '* Hong et al. (2023) Yicong Hong, Kai Zhang, Jiaxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Tuning Bui, and Hao Tan. 2023. Lmm: Large reconstruction model for single image to 3d. _arXiv preprint arXiv:2311.04020_ (2023).\n' +
      '* Huang et al. (2023) Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. 2023. Compose: Creative and controllable image synthesis with composable conditions. _arXiv preprint arXiv:2302.07978_ (2023).\n' +
      '* Jun and Nichol (2023) Heewoo Jun and Alex Nichol. 2023. Shap-e: Generating conditional 3d implicit functions. _arXiv preprint arXiv:2305.0643_ (2023).\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ (2013).\n' +
      '* Hao et al. (2023) Jiahao Li, Hao Nai, Kang Zhang, Zexiang Xu, Fujjun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. 2023b. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _arXiv preprint arXiv:2311.06214_ (2023).\n' +
      '* Li and Harada (2022) Yang Li and Matyuz Harada. 2022. Non-rigid point cloud registration with neural deformation pyramid. _Advances in Neural Information Processing Systems_ 35 (2022), 27737-27768.\n' +
      '* Li et al. (2022) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2022a. Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 22511-22521.\n' +
      '* Lin et al. (2023) Chen-Hsuan Lin, Jun Gao, Liang Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. MSR1: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 300-309.\n' +
      '* Liu et al. (2023) Minghua Liu, Ruoxi Shi, Linzhao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Cheng Zeng, Jiayuan Gu, and Hao Su. 2023c. One-2-3-45++ Fast single image 3d objects with consistent multi-view generation and 3d diffusion. _arXiv preprint arXiv:2311.07885_ (2023).\n' +
      '* Liu et al. (2023a) Minghua Liu, Chao Xu, Hiaan Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. 2023c. One-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. _arXiv preprint arXiv:2306.16982_ (2023).\n' +
      '* Lin et al. (2023) Ruoxhi Lin, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023a. Zero-1. t-3-o: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 9298-9390.\n' +
      '* Liu et al. (2023) Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxoxong Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2023b. SyncBrear: Generating Multiview-consistent Images from a Single-view Image. _arXiv preprint arXiv:2309.04534_ (2023).\n' +
      '* Yao et al. (2023) Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzenhakim, Lian Paul, and Weiyang Liu. 2023a. MeshDiffusion: Score-based Generative 3D Mesh Modeling. In _International Conference on Learning Representations_. [https://openreview.net/forum?id=0qpX2aPPv60](https://openreview.net/forum?id=0qpX2aPPv60)\n' +
      '* Long et al. (2023) Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. 2023. Wonder3d: Single image to 3d using cross-domain diffusion. _arXiv preprint arXiv:2310.15008_ (2023).\n' +
      '* Lugmayr et al. (2022) Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. Repair: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 11461-11471.\n' +
      '* Mildenthal et al. (2021) Ben Mildenthal, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramanmamoorthi, and Ren Ng. 2021. Next: Representing scenes as neural radiance fields for view synthesis. _Commun. ACM_ 65, 1 (2021), 99-106.\n' +
      '* Mao et al. (2023) Chong Mao, Xuitong Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qiu. 2023. 273a: Long-layer: Learning adaptive to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_ (2023).\n' +
      '* Miller et al. (2023) Norman Miller, Jiasarar Sudigotti, Lorenzo Porzi, Samuel Rota Baba, Peter Kontschieder, and Mattis Nielsen. 2023. Drift: Rendering-guided 3d radiance field diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 4328-4338.\n' +
      '* Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Rames, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_ (2021).\n' +
      '* Nichol et al. (2022) Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_ (2022).\n' +
      '* Nichol and Dhariwal (2021) Alexander Quinm Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_. PMLR, 8162-8171.\n' +
      '* Park et al. (2019) Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsoft: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 165-174.\n' +
      '* Poole et al. (2022) Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_ (2022).\n' +
      '* Ramesh et al. (2022) Atitiya Ramesh, Prafulla Dhariwal, Alexe Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip letters. _arXiv preprint arXiv:2204.06125_ 1, 2 (2022).\n' +
      '* Zanesh et al. (2021) Atitiya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Garg, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In _International Conference on Machine Learning_. PMLR, 8821-8831.\n' +
      '* Roinhond et al. (2022) Robin Roinhond, Andreas Blattmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 10684-10695.\n' +
      '* Roinhond et al. (2022) Robin Roinhond, Andreas Blattmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 10684-10695.\n' +
      '* Ruiz et al. (2023) Natani Ruiz, Yuuzhen Li, Varun Jampeni, Yael Pritch, Michael Rubinstein, and Kif Akerman. 2023. Dreamphoto: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 22500-22510.\n' +
      '* Saharia et al. (2022) Chifwan Saharia, William Chan, Suzuhuha Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar* Voyrov et al. (2023) Andrey Voyrov, Krik Aherman, and Daniel Cohen-Or. 2023. Sketch-guided text-to-image diffusion models. In _ACM SIGGRAPH 2023 Conference Proceedings_. 1-11.\n' +
      '* Wang et al. (2023) Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Rao, Tadas Baltrusaitis, Jingting Shen, Dong Chen, Fang Wen, Qing Chen, et al. 2023. Rolim: A generative model for sculpting 3d digital avars using diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 4563-4573.\n' +
      '* Wang et al. (2022) Tengfei Wang, Ting Zhang, Rio Zhang, Hao Ouyang, Dong Chen, and Fang Wen. 2022. Pretraining all you need for image-to-image translation. _arXiv preprint arXiv:2205.12952_ (2022).\n' +
      '* Wu et al. (2022) Weiliuun Wang, Jianmin Rao, Wengeng Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. 2022. Semantic image synthesis via diffusion models. _arXiv preprint arXiv:2207.00050_ (2022).\n' +
      '* Wang et al. (2021) Xinpeng Wang, Chandan Tsehwath, and Matthias Niefner. 2021. Scenformer: Indoor scene generation with transformers. In _2021 International Conference on 3D Vision (SDV)_. IEEE, 106-115.\n' +
      '* Wood et al. (2021) Froml Wood, Tadas Baltrusaitis, Charlie Hewitt, Sebastian Dzidzio, Thomas J Cashman, and Jamie Shotton. 2021. Fake it till you make it: face analysis in the wild using synthetic data alone. In _Proceedings of the IEEE/CVF international conference on computer vision_. 3681-3691.\n' +
      '* Xu et al. (2023a) Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. 2023a. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. _arXiv preprint arXiv:2311.09217_ (2023).\n' +
      '* Xu et al. (2023b) Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. 2023b. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. _arXiv preprint arXiv:2311.09217_ (2023).\n' +
      '* Yan et al. (2024) Han Yan et al. 2024. Frankenstein: Generating Semantic-Compositional 3D Room in One Trjplane. (2024).\n' +
      '* Yang et al. (2023) Jiayi Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. 2023. Consistent: Enforcing 3D Consistency for Multi-view Im diffusion. _arXiv preprint arXiv:2301.1043_ (2023).\n' +
      '* Zeng et al. (2022) Xiaohui Zeng, Anshu Vahdat, Francis Williams, Zan Gojek, Or Litany, Sanja Fidler, and Karsten Kreis. 2022. LION: Latent point diffusion models for 3D shape generation. _arXiv preprint arXiv:2201.06978_ (2022).\n' +
      '* Limin et al. (2023) Lvmin Zhang, Angy Rao, and Manneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 3836-3847.\n' +
      '* Zheng et al. (2023) Xin-Yang Zheng, Hao Pan, Peng-Shui Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. 2023. Locally attentional sdf diffusion for controllable 3d shape generation. _arXiv preprint arXiv:2308.0461_ (2023).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>