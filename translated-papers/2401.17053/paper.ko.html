<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_EMPTY:1]\n' +
      '\n' +
      '상기 삼면들을 상기 잠재 삼면 공간으로 압축하고, 상기 잠재 삼면 공간에서 상기 잡음 제거 확산 프로세스가 수행되는, 방법. 잠재적 표현들에 적용된 확산은 고품질의 다양한 3D 장면 생성을 가능하게 한다.\n' +
      '\n' +
      '세대 동안 장면을 확장하기 위해, 빈 블록을 현재 장면과 중첩하도록 추가하고 새로운 블록을 채우기 위해 기존의 잠재 트라이플레인들을 외삽하기만 하면 된다. 외삽은 잡음 제거 반복 동안 겹치는 삼중 평면의 특징 샘플로 생성 프로세스를 조정함으로써 수행된다. 잠재적 삼면 외삽은 기존의 장면과 조화롭게 조화를 이루는 의미론적, 기하학적으로 의미 있는 전이를 생성한다. 장면 요소들의 배치 및 배치를 제어하기 위해 2D 레이아웃 컨디셔닝 메커니즘이 사용된다. 실험 결과, 블록퓨전은 실내 및 실외 시나리오 모두에서 전례 없는 고품질 형상을 가진 다양하고 기하학적으로 일관되고 경계가 없는 대형 3D 장면을 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      '많은 양의 고품질 3D 콘텐츠를 생성하는 것은 비디오 게임, 영화 제작, 증강 및 가상 현실(AR/VR)을 포함한 많은 실제 응용 분야에서 핵심이다. 고품질 디지털 콘텐츠에 대한 수요가 증가함에 따라 3D 세대는 중요한 연구 주제가 되었다. 최근 2D 도메인에서 데노이즈 확산 모델[14]은 이미지 합성 및 그 이상의 놀라운 결과를 보여 안정적인 확산[15], 미드저니, Dall-E[11]와 같은 생산 준비가 된 2D 생성 도구의 개발로 이어졌다. 2D 영역의 성공은 3D 생성 도구 개발에 대한 관심을 크게 불러일으켰다. 최근 3D 세대에 대한 많은 연구가 발표되고 있으며, 가장 주목받는 작품으로는 DreamFusion[13], Rodin[12], Get3D[14], Zero123[15], SynC Dreamer[16], LRM[12] 등이 있다.\n' +
      '\n' +
      '그러나, 기존의 방법들은 주로 고정된 공간 범위(유한한 크기의 작은 객체 등)를 갖는 3D 콘텐츠의 생성에 초점을 맞추고 있다. 본 논문에서는 확장 가능한 (무한한) 3D 장면 생성이라는 비교적 새롭고 점점 더 중요한 과제를 조사한다. 이 작업은 오픈 월드 게임에서 볼 수 있듯이 사용자가 미리 정해진 세계 경계에 구애받지 않고 자유롭게 세계와 상호작용할 수 있도록 함으로써 몰입형 게임 경험을 제공하기 때문에 비디오 게임 산업에 특히 가치가 있다. 그럼에도 불구하고, 무한하고 자유롭게 탐험할 수 있는 장면을 만드는 것은 사소한 일이 아니다. 현재 관행은 일반적으로 시간이 많이 걸리고 비용이 많이 드는 작업인 예술가의 육체 노동에 의존한다.\n' +
      '\n' +
      '확산 모델을 사용하여 확장 가능한 3D 장면을 생성하는 것은 두 가지 주요 과제를 제기한다. 첫째, 장면 수준에서 고 충실도의 3D 형상을 생성하는 것은 어려운 문제이다. 3D 장면의 분산은 단일 객체보다 10배 더 크다. 장면은 기본 객체들을 포함하고, 이러한 객체들을 배열하는 가능성은 무한하다. 이러한 높은 수준의 다양성은 확산 확률 모델을 사용하여 분포를 근사화하는 것을 어렵게 만든다. 게다가, 2) 기존의 장면에서 더 큰 장면으로의 확장은 삼중이 아니다. 이전 장면과 새로운 장면 사이의 전환 영역은 의미론적, 기하학적으로 조화되어야 하며, 작업에 또 다른 복잡성의 층을 추가할 필요가 있다.\n' +
      '\n' +
      'Text2Room [10]은 우리의 작업과 가장 밀접한 관련이 있는 작업이다. 이를 위해 미리 학습된 2차원 확산 모델을 이용하여 2차원 영상을 생성하고, 카메라 시점과 추정된 깊이 영상을 통해 3차원 장면으로 끌어올린다. 점진적으로 추가된 카메라 시점으로부터 생성된 영상을 병합하여 장면을 확장한다. 따라서 실내 규모에서만 인상적인 텍스처 결과로 확장 가능한 3D 장면을 생성할 수 있습니다. 그러나, 이는 단안 깊이 예측에 비판적으로 의존하기 때문에, 열악한 깊이 예측은 누락된 세부사항들을 갖는 왜곡된 기하학으로 이어질 것이다. 또한, 장면(즉, 움직이는 투시 카메라를 활용하여)을 확장하는 방식은 실내 스케일을 넘어 확장되기 어렵게 만든다. 원근 카메라가 가려지기 쉽기 때문입니다. 예를 들어, 카메라가 벽을 통과할 때, 이미지의 연속성은 폐색에 의해 파괴될 수 있고, 이는 또한 생성된 3D 형상들에서 불연속으로 이어질 수 있다.\n' +
      '\n' +
      '또 다른 연구 방향은 2D 이미지 리프팅을 통해 3D를 생성하는 대신 3D 모양 그라운드 트루스 또는 포즈된 다중 뷰 이미지로부터의 감독을 사용하여 3D 데이터를 생성하는 것을 직접 학습하는 것이다. 주목할 만한 방법으로는 EG3D[23], 로댕[12], Get3D[14] 등이 있다. 이러한 접근법들은 전형적으로 트라이-플레인 및 MLP 디코더로 구성된 연속적인 하이브리드 신경 필드 아키텍처를 갖는 3D 데이터를 나타낸다. 3-평면은 조밀한 3D 볼륨 그리드를 인수분해하기 위해 사용되는 텐서이다. XY, YZ 및 XZ 평면의 세 축 정렬 2D 평면에 구축됩니다. MLP 디코더는 트라이 플레인 특징을 장면을 나타내는 연속적인 값으로 변환하는데, 이는 점유, 부호 거리 필드(SDF), 복사 필드[13] 등일 수 있다. 삼면 평면은 완전한 3D 텐서보다 훨씬 더 작고 계산적으로 효율적이며 2D 이미지 합성을 위해 개발된 생성 아키텍처에 도움이 된다. 이는 고품질의 직접 3D 데이터 생성을 가능하게 하는 핵심 요소였다.\n' +
      '\n' +
      '본 논문에서는 확장 가능한 3차원 장면 생성을 위한 3-평면 확산 기반 접근 방법을 개발한다. 우리의 방법은 BlockFusion이라고 불린다. 입방 블록 형태의 3D 장면을 생성하고, 간단한 슬라이딩 블록 방식으로 장면을 확장한다. 고품질 3D 도형을 생성하기 위해 3D 장면 데이터 세트에서 블록확산을 직접 훈련한다. 네트워크 교육을 위해 완성된 3D 장면을 고정된 크기를 가진 불완전한 3D 블록으로 무작위로 크롭한다. 우리는 모든 훈련 블록을 트라이 플레인으로 변환하기 위해 블록당 피팅을 실행하는데, 이를 원시 트라이 플레인이라고 한다. 우리는 원시 삼면에 대한 확산을 직접 훈련하면 바람직하지 않은 붕괴된 모양이 발생한다는 것을 발견했다. 이 문제는 원시 삼평면의 높은 중복성과 데이터의 상당한 모양 차이로 인해 발생할 수 있다. 안정적인 확산에 의해 영감을 얻은 [13]은 자동 인코더를 적용하여 원시 삼면을 잠재 삼면 공간으로 압축하여 확산을 실행한다. 잠재 트라이-평면 공간은 유사한 표현력을 유지하면서 원시 트라이-평면보다 훨씬 더 컴팩트하고 계산적으로 효율적이다. 이전 연구와 달리, 이러한 잠재 표현에서의 삼면 확산은 장면 수준에서 고품질 및 다양한 3D 형상 생성에 처음으로 도달한다.\n' +
      '\n' +
      '장면을 확장하기 위해 현재 장면과 중첩되도록 빈 블록을 추가하고 새로운 블록을 채우기 위해 기존 3면을 외삽한다. 구체적으로, 역확산 반복 동안 중첩 트라이플레인으로부터의 특징 샘플들로 생성 프로세스를 컨디셔닝함으로써 외삽이 수행된다. 외삽은 잠재 3면 공간에서도 수행된다. 이 프로세스는 기존의 장면과 매끄럽게 조화를 이루는 의미적, 기하학적으로 의미 있는 전환을 생성하여 일관성 있고 시각적으로 만족스러운 장면 확장을 보장한다.\n' +
      '\n' +
      '생성 과정에 대한 보다 많은 제어를 사용자에게 제공하기 위해 2D 객체 바운딩 박스를 조작하여 요소의 배치 및 배치를 정확하게 결정할 수 있는 2D 레이아웃 컨디셔닝 메커니즘을 소개한다. 또한 장면의 색상과 질감을 기성 텍스쳐 생성 도구를 사용하여 생성할 수 있어 장면의 시각적 매력을 높일 수 있음을 보여준다.\n' +
      '\n' +
      '요약하면, BlockFusion은 1) 잠재 삼면 확산에 기반한 일반화 가능한 고품질 3D 생성 모델, 2) 조화로운 장면 확장을 허용하는 잠재 삼면 외삽 메커니즘, 3) 장면 생성에 대한 정밀한 제어를 위한 2D 레이아웃 조건 메커니즘을 제시한다. 실험 결과, 블록퓨전은 실내 및 실외 시나리오 모두에서 전례 없는 고품질 형상을 가진 다양하고 기하학적으로 일관되고 경계가 없는 대형 3D 장면을 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '##2. 관련업무\n' +
      '\n' +
      '### Diffusion models\n' +
      '\n' +
      '가우시안 잡음 샘플들로부터 시작하여, 확산 확률 모델들(Ho et al., 2020; Sohl-Dickstein et al., 2015)은 원래의 잡음 샘플로부터 잡음을 점진적으로 제거하도록 학습함으로써 선명한 이미지들을 생성한다. 최근 확산 모델(Dhariwal and Nichol, 2021; Nichol and Dhariwal, 2021; Ramesh et al., 2022; Saharia et al., 2022)의 발전은 고품질 및 다양한 이미지를 합성하는 전례 없는 능력을 입증했다. 그럼에도 불구하고, 고해상도 픽셀 공간에서 직접 확산 모델을 트레이닝하는 것은 계산적으로 금지될 수 있다. 잠재 확산 모델들(LDMs)(Rombach et al., 2022)은 이 문제를 2단계 접근법으로 해결한다: 그들은 먼저 오토 인코더를 통해 이미지를 압축한 다음 잠재 공간 내의 더 작은 공간 표현들에 확산 모델들을 적용한다. 확산 모델들은 개인화, 커스터마이징, 또는 태스크-특정 이미지 생성을 용이하게 하기 위해 안내 정보(예를 들어, 텍스트 프롬프트, 시맨틱 레이아웃, 카테고리 라벨)로 트레이닝될 수 있다. 생성된 콘텐츠를 조작하는 방법은 기본적으로 두 가지가 있다. 첫 번째는, 스크래치로부터 새로운 모델을 트레이닝하거나 미리 트레이닝된 확산 모델을 미세조정하여, 다양한 컨디셔닝 컨트롤들, 예를 들어, 스케치, 깊이, 세그먼트화 등을 추가함으로써 실현된다(Avrahami et al., 2023; Bashkirova et al., 2023; Brooks et al., 2023; Gal et al., 2022; Huang et al., 2023; Li et al., 2023; Mou et al., 2023; Nichol et al., 2021; Ramesh et al., 2022, 2022; Rombach et al., 2022, 2022; Ruiz et al., 2023; Voynov et al., 2023; Wang et al., 2022; Zhang et al., 2023). 이 접근 방식은 광범위한 데이터 세트 구축과 추가 계산 소비를 필요로 한다. 다른 방법은 사전 훈련된 모델을 적용하고 추론 중에 일부 제어된 생성 능력을 추가한다. 생성 프로세스에 약간의 수정만으로, (Avrahami et al., 2022; Bar-Tal et al., 2023; Hertz et al., 2022; Tumanyan et al., 2023)은 훈련/피네튜닝-프리 방식으로 매우 다양한 제어 확산 모델을 조사한다.\n' +
      '\n' +
      '##### 3D 형상 생성\n' +
      '\n' +
      '확산 모델을 기반으로 한 2D 생성 도구의 성공, 특히 Stable Diffusion(Rombach et al., 2022), Midjourney, Dall-E가 3D 생성 도구 개발에 큰 관심을 불러일으켰다. 이 작업을 위한 두 가지 주요 스트림은 2D(생성) 이미지를 3D 모델로 들어올리는 방법과 3D 데이터에서 직접 확산을 실행하는 방법이다.\n' +
      '\n' +
      '**2D-리프팅 방법들.** 드림퓨전(Poole et al., 2022)은 2D 이미지 확산 모델들로부터의 사전 지식을 NeRF의 볼륨 렌더링 출력으로 증류하는 스코어 증류 샘플링(Score Distillation Sampling, SDS) 손실을 사용하여 신경 복사 필드(Mildenhall et al., 2021)를 최적화한다. Magic3D(Lin et al., 2023)는 DreamFusion으로부터 추출된 메쉬를 추가로 정제하기 위해 SDS 손실 기반 제2 스테이지를 채택한다. SDS 기반 접근 방식은 인상적인 결과를 보여줍니다. 그러나, 이들은 전형적으로 몇 시간의 최적화가 필요하고 형상 일관성을 유지하는 데 어려움을 겪으며, 야누스-페이스 문제(Poole et al., 2022)라고 불리는 현상을 초래한다. 일관성 강화 다시점 2차원 영상의 직접 생성에 초점을 맞춘 여러 방법들이 개발되었으며, 이러한 기법들은 생성된 다시점 영상으로부터 3차원 형상을 재구성한다. Zero123(Liu et al., 2023) fine-tunes Stable Diffusion model(Rombach et al., 2022)을 이용하여 입력 영상에 대한 컨디셔닝과 카메라 변환을 통해 새로운 뷰를 생성한다. One2345(Liu et al., 2023)는 SDF 기반 신경 표면 재구성 방법을 사용하여 다시점 영상을 Zero123에서 3D로 변환한다. One2345++(Liu et al., 2023)는 일관된 다시점 영상 생성을 위해 2D 확산 모델을 미세 조정한 후, 다시점 조건화 3D 확산 모델의 도움으로 이들 영상을 3D로 상승시킨다. Syncedreamer(Liu et al., 2023)와 Consistent(Yang et al., 2023)은 3D 공간상의 특징을 명시적으로 연관시켜 다시점 영상 생성 과정을 동기화한다. Wonder3d(Long et al., 2023)는 컬러 이미지 이외에 멀티뷰 정규 맵을 생성하는 크로스 도메인 확산 모델을 도입함으로써 생성 충실도를 향상시킨다. LRM(Hong et al., 2023)은 단일 영상-to-3D 문제를 재구성 문제로 취급하고 이를 결정론적 방식으로 Transformer를 이용하여 해결한다. 그러나, LRM은 모드 평균화로 인해 물체의 보이지 않는 부분에 대해 흐릿하고 씻겨진 텍스처를 초래할 수 있다. 이 문제를 해결하기 위해, Instant3D(Li et al., 2023)는 보이지 않는 부분에 대한 기하학 및 텍스처를 추론하기 위해 LRM에 다시점 일관 이미지를 입력한다. DMV3D(Xu et al., 2023)는 LRM을 멀티뷰 디노이저로서 채용하고, 이는 반복적으로 잡음이 희박하게 포즈된 멀티뷰 이미지로부터 더 깨끗한 3-평면 NeRF를 생성한다.\n' +
      '\n' +
      '**3D 확산 모델.** 또 다른 연구 라인은 3D 형상을 생성하기 위해 확산 모델을 직접 트레이닝하는 것을 포함한다. 감독은 3차원 형상 진리 또는 포즈된 다시점 영상으로부터 직접 나오기 때문에, 생성된 결과는 일반적으로 2차원 확산 기반 방법에 비해 우수한 기하학적 품질을 나타낸다. 이러한 방법들은 그들이 채용하는 3D 표현들의 유형에 기초하여 분류될 수 있는데, 다음과 같이, 다각형 메쉬들(Gao et al., 2022; Liu et al., 2023), 포인트 클라우드들(Nichol et al., 2022; Zeng et al., 2022), 점유 또는 SDF 값들을 보유하는 명시적 3D 그리드들(Liu et al., 2023; Zheng et al., 2023), 또는 신경 필드들(Chen et al., 2023; Chou et al., 2023; Erkoc et al., 2023; Jun and Nichol, 2023; Muller et al., 2023; Shue et al., 2023; Wang et al., 2023; Xu et al., 2023). 3-평면과 신경망 디코더를 결합한 하이브리드 신경장은 계산 효율로 인해 3차원 확산 모델에 널리 채택되었다. 로딘(Wang et al., 2023)은 먼저 인체 상체 데이터세트에 대한 트라이-평면 NeRFs를 피팅하고, 이어서 대응하는 트라이-평면들을 생성하기 위해 2단계 거친-대-미세 확산 모델을 사용한다. 유사하게, NFD(Shue et al., 2023)는 점유 값들을 통해 파라미터화된 3D 데이터에 대한 삼면 확산 모델을 트레이닝한다. SSDNerf[31]는 3-평면 피팅 및 생성을 단일-스테이지 파이프라인으로 병합한다. 그러나 실제로 3면 확산은 고차원성과 불규칙성으로 인해 여전히 훈련하기 어렵다. 기존 방법은 ShapeNet[3]의 단일 카테고리 객체에 대한 NFD 및 SSDNeRF와 같이 작은 데이터 품종, 즉 표준화된 인체 상체 데이터 세트에 대한 로댕과 같은 간단한 사례만 보여주었다. 이 논문은 이러한 연구 라인을 따르지만 주요 변화를 소개한다: 우리는 자동 인코더를 사용하여 삼중면을 매우 컴팩트한 잠재 삼중면 공간으로 압축하여 확산을 한다. 우리는 이 접근법이 3면 확산의 안정성, 일반화 가능성 및 출력 품질을 크게 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '##### 3D 장면 생성\n' +
      '\n' +
      '3D 장면들을 생성하는 것은 단일 객체들을 생성하는 것보다 더 실질적인 도전을 제시한다. 장면은 개별 객체보다 기하학적으로 복잡하며, 고정된 공간 크기로 담을 수 없기 때문이다. 객체 검색 기반 접근 방식은 객체의 데이터베이스가 있다고 가정하고, 검색된 객체를 Diffuscene[19] 및 Sceneformer[23]에서 볼 수 있듯이 빈 장면을 채우기 위해 배열하므로, 결과적으로 합성된 장면은 데이터베이스에 존재하지 않는 새로운 요소를 포함할 수 없다. Text2Room[12]는 2D 확산 모델을 사용하여 3D 생성 도구를 구축하는 첫 번째 방법이다. 먼저 2D 확산 모델을 사용하여 색상 및 깊이 프레임을 생성한 다음 카메라 위치를 이동시켜 새로운 프레임을 생성하고 이를 전역 맵에 통합한다. 더 넓은 실외 시나리오에 대한 유사한 접근법이 장면 스케이프[14]에서 발견될 수 있다. ControlRoom3D[15]와 CTRL-ROOM[22]는 한 장면에서 생성된 콘텐츠에 대한 정밀한 제어를 위해 3D 룸 레이아웃을 입력 조건으로 하는 파노라마 기반 룸 생성 모델을 개발한다. 시티겐[3]은 높이 맵 프록시를 사용하여 도시 장면을 나타내며, 2.5D 장면 생성으로 이어진다. 다른 접근법들은 고품질의 시각적 외관을 갖는 장면들을 생성하는 것에 초점을 맞춘다. 룸 장면 메시가 주어지면, MVD 확산[19]은 간섭성 멀티뷰 원근 이미지들을 생성하며, 이는 메시의 UV 텍스처로서 3D로 들어올려질 수 있다. 씬드리머[3]는 와일드 2D 이미지를 활용하여 사진 사실적인 볼륨 렌더링 효과로 큰 장면을 구성합니다. 그러나, 이는 여전히 입력으로서 고품질 3D 장면 메시들에 의존하며, 결과적으로, 생성된 장면들의 치수들은 입력 메시들의 치수들에 의해 경계된다.\n' +
      '\n' +
      '본 논문에서는 3면 확산에 기반한 자기회귀적 장면 확장 알고리즘을 개발함으로써 무한한 장면을 생성하는 근본적인 과제를 다룬다.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'BlockFusion은 슬라이딩 윈도우 점진적 생성 방식을 사용하여 장면을 블록으로 생성하고 장면을 확장한다. 도. 도 2는 트레이닝 파이프라인을 제시한다. 이 섹션은 다음과 같이 구성된다:\n' +
      '\n' +
      '* Sec. 3.1은 트레이닝 블록들이 어떻게 생성되는지를 설명한다.\n' +
      '* Sec. 3.2: 모든 트레이닝 블록을 트라이 플레인으로 변환하기 위해 블록당 피팅을 실행하며, 이를 원시 트라이 플레인이라고 한다.\n' +
      '* Sec. 3.3: 원삼면들은 효율적인 3D 표현을 위해 잠재 삼면 공간으로 압축된다.\n' +
      '* Sec: 3.4: 잠재 삼면 공간에서 확산 모델을 훈련한다.\n' +
      '* Sec. 3.5: 우리는 미리 훈련된 잠재 삼면 확산 모델을 활용하여 장면을 확장한다.\n' +
      '* Sec. 3.6: 솔기를 감소시키기 위해 후처리 기술이 적응된다.\n' +
      '* Sec. 3.7: 블록퓨전을 점진적으로 실행함으로써 큰 장면들이 구축된다.\n' +
      '\n' +
      '3D 블록에서 훈련 장면 만들기\n' +
      '\n' +
      '네트워크 교육을 위해 장면 메쉬를 사용합니다. 우리는 장면 메쉬를 수밀 메쉬로 변환한 다음 무작위로 메쉬를 입방체 블록으로 자른다. 블록의 크기는 장면 내의 주요 객체들, 예를 들어 방 장면 내의 침대들, 또는 실외 장면 내의 집들을 둘러싸기에 충분히 크도록 조정된다. 블록들이 장면 내에 랜덤하게 위치된다는 점을 감안할 때, 객체들은 이들 블록들에 의해 분할될 수 있다. 또한, 블록 내의 객체의 가능한 배열은 무한하다. 이러한 무작위로 크롭된 형상 데이터 세트의 분산은 단일 객체 중심 데이터 세트의 분산보다 훨씬 크다. 결과적으로, 이러한 유형의 데이터에 대한 트레이닝 확산은 더 큰 도전을 제시한다. 우리는 방, 도시, 마을을 포함한 세 가지 유형의 장면에 대해 테스트합니다. 학습 블록의 예는 그림 3에서 찾을 수 있다. 모양 외에도 각 장면에 대한 2D 레이아웃 맵도 생성한다. 배치 맵은 객체의 범주로 그룹화된 지표면 투영입니다. 이러한 레이아웃 맵은 사용될 수 있다\n' +
      '\n' +
      '도 2. **BlockFusion 트레이닝 파이프라인.** 트레이닝은 세 단계를 포함한다: 먼저, **1)** 트레이닝 3D 블록들은 블록당 형상 피팅, c.f. Sec. 3.2를 통해 원시 트라이 플레인들로 변환된다. 그리고, **2)** 오토 인코더는 원시 트라이 플레인들을 보다 컴팩트한 잠재 트라이 플레인 공간, c.f. Sec. 3.3으로 압축한다. 마지막으로, **3)** DDPM은 잠재 트라이 플레인들의 분포들을 근사하도록 트레이닝되며, 이 프로세스 동안 레이아웃 제어도 통합될 수 있다. c.f. Sec. 3.4.\n' +
      '\n' +
      '확산을 위한 입력 조건이므로 그에 따라 크롭도 합니다. 예들은 도 2에서 볼 수 있다.\n' +
      '\n' +
      '### Raw tri-plane fitting\n' +
      '\n' +
      '**Hybrid Neural SDF.** 부호 거리 필드(SDF)를 사용하여 모양을 표현한다. SDF는 표면까지의 거리를 나타내는 값들과 한 점이 물체의 내부 또는 외부에 있는지를 나타내는 부호들을 갖는 연속적인 거리 함수이다. 최종 표면은 행진 큐브를 통해 추출될 수 있다. 하이브리드 신경장 구조를 이용하여 형상을 재구성하였으며, 형상특징을 유지하기 위한 3면과 부호화된 거리값을 해독하기 위한 매개변수\\(\\theta\\)를 갖는 다중층 퍼셉트론(MLP)으로 구성된다. 3-평면은 조밀한 3D 볼륨 그리드를 인수분해하기 위해 사용되는 텐서이다. XY, YZ 및 XZ 평면의 세 축 정렬 2D 평면에 구축됩니다. 형식적으로는 \\(x=\\{x(i)\\}\\)\\(\\mathbf{x(i)\\in\\mathbb{R}^{N^{2}\\times C}\\), \\(i\\in\\{1,2,3\\}\\big{\\}\\)을 읽으며, 여기서 \\(N^{2}\\)은 평면 분해능이고 \\(C\\)은 특징의 차원이다. 질의점\\(p\\in\\mathbb{R}^{3}\\)이 주어지면, 함수\\(\\Phi:\\mathbb{R}^{3}\\mapsto\\mathbb{R}\\)는 서명된 거리 값을 출력한다:\n' +
      '\n' +
      '[\\Phi(p)=\\text{MLP}_{\\theta}\\bigg{(}\\bigoplus_{i\\in\\{1,2,3\\}}\\text{Interp}_{X(i)}\\big{(}\\text{Proj}_{X(i)}\\big{(}p\\big{}\\big{)}\\bigg{}\\tag{1}\\bigg{(}\\figoplus_{i\\in\\2,3\\}}\\text{Interp}_{X(i)}\\big{(}p\\big{}}\\big{)}\\bigg{}\\tag{1}\\bigg{(}\\figoplus_{i\\in\\2,3\\}}\\text{Interp}_{X(i)}\\big{(}p\\big{}}\\big{}}\\big{(}p\\big{)}\\bigg{}\\tag{1}\\big\n' +
      '\n' +
      '여기서 \\(\\text{Proj}(\\cdot)\\)는 직교점-평면 투영을 나타내고, \\(\\text{Interp}(\\cdot)\\)은 각 평면으로부터 특징 벡터를 각각 질의하는 쌍선형 보간을 나타내며, \\(\\oplus\\)은 요소별 덧셈을 나타낸다. 덧셈 연산은 특징 차원을 따라 수행되며, 세 개의 특징 벡터를 하나의 최종 특징으로 감소시킨다.\n' +
      '\n' +
      '**트레이닝 포인트 샘플링.** 트레이닝 블록의 메시가 주어지면, 우리는 표면 상의 포인트들 및 표면 외부의 포인트들을 샘플링하고, 그 다음 지상 진실 SDF 값들을 계산한다. 표면에서 \\(\\Omega_{0}\\)으로 표시된 표면 점 집합은 표면에서 무작위로 샘플링된다. 그들의 SDF 값은 0과 같고 우리는 또한 이러한 점 각각에 대한 표면 정규 GT를 계산한다. 표면외 점집합은 \\(\\Omega\\)으로 표시되며 블록 내부에서 랜덤하게 균일하게 샘플링된다. 메쉬 크롭으로 인한 잘못된 거리 값을 피하기 위해 원래 수밀 메쉬에 대해 표면 외 점의 그라운드 트루스 SDF 값을 계산한다. 실험 결과, 점집합 크기 \\(|\\Omega|=100000\\)와 \\(|\\Omega_{0}|=500000\\)은 최적설계 비용을 관리 가능한 수준으로 유지하면서 견고한 형상적합 결과를 얻을 수 있었다. 샘플링된 모든 지점의 XYZ 좌표는 [-1, 1] 범위로 정규화된다.\n' +
      '\n' +
      '**Triplane Fitting.** 우리의 목표는 모든 훈련 블록을 Tri-plane으로 변형시키는 것이며, 이는 우리의 생성 모델을 훈련하는 데 사용될 것이다. 신경장 기반 SDF(Atzmon and Lipman, 2020; Gropp et al., 2020; Park et al., 2019)를 사용한 형상 표현에 대한 선구적인 작업에서 영감을 받아 다음과 같은 기하학적 손실로 3면 \\(x\\) 및 MLP 가중치 \\(\\theta\\)을 공동으로 최적화한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{geo}=\\mathcal{L}_{SDF}+\\mathcal{L}_{Normal}+\\mathcal{L}_{Eikonal} \\tag{2}\\]\n' +
      '\n' +
      '세 가지 조건은 다음과 같다.\n' +
      '\n' +
      '\\lambda_{L}_{SDF}=\\lambda_{1}\\sum_{p\\in\\Omega_{0}}||\\Phi(p)-d_{p}||\\tag{4}\\\\mathcal{L}_{Normal}=\\lambda_{3}\\sum_{p\\in\\Omega_{0}}||\\nabla_{p}\\Phi(p)-\\mathrm{n}_{p}||\\nabla_{p}\\mega_{0}}||\\nabla_{p}\\Phi(p)-\\mathrm{n}_{p}||\\nabla_{p}\\Phi(p)}\n' +
      '\n' +
      '여기서 \\(d_{p}\\) 및 \\(\\mathrm{n}_{p}\\)은 지상진실 SDF 값 및 표면 법선 벡터이다. 구배\\(\\nabla_{p}\\Phi(p)=[\\frac{\\partial\\Phi(p)}{\\partial X},\\frac{\\partial\\Phi(p)}{ \\partial Y},\\frac{\\partial\\Phi(p)}{\\partial Z}]\\는 SDF에서 가장 가파른 변화의 방향을 나타낸다. 유한 차분, 예를 들어 X축 성분 판독에 대한 편미분을 사용하여 계산할 수 있다.\n' +
      '\n' +
      '\\[\\frac{\\partial\\Phi(p)}{\\partial X}=\\frac{\\Phi(p+[\\delta,0,0])-\\Phi(p-[\\delta,0,0])}{2\\delta}\\tag{6}\\.\n' +
      '\n' +
      '여기서 \\(\\delta\\)는 스텝 크기이다. Eikonal loss는 \\(|\\nabla_{p}\\Phi(p)|\\)를 거의 모든 곳에서 \\(1\\)으로 제한하므로 부호 거리 함수의 고유 물성을 유지한다. 우리는 SAL(Atzmon and Lipman, 2020)에 소개된 MLP 초기화 트릭을 채택하여 초기 SDF 출력이 구에 대략 근사하도록 제한한다. 이러한 구면 기하 초기화 기법은 전역 수렴을 상당히 용이하게 한다. 경험적으로, 손실 가중치는 모든 데이터 세트에서 \\(\\lambda_{1}=100.0\\), \\(\\lambda_{2}=3.0\\), \\(\\lambda_{3}=1.0\\), \\(\\lambda_{4}=0.5\\)으로 설정된다. MLP는 500개의 블록들로 구성된 트레이닝 서브세트를 사용하여 트라이-플레인들과 공동으로 트레이닝된다. 수렴 시, MLP는 일반화 가능한 SDF 디코더로 간주된다. 그런 다음 MLP를 동결하고 훈련 데이터의 모든 블록에 대해 3면을 최적화한다. 본 연구에서는 출력 3면 크기를 \\(N^{2}=128^{2},C=32\\)으로 설정하였다. (Yan et al., 2024)에 이어서, 3-평면은 조대-미세 방식으로 최적화된다. 즉, 분해능은 \\(8^{2}\\)으로 초기화되고 점차적으로 \\(128^{2}\\)으로 상향 스케일링된다. 최종 해상도에서 직접 최적화하는 것과 비교하여, 이 트릭은 피팅 견고성을 상당히 향상시키고 실행 시간을 감소시킨다.\n' +
      '\n' +
      '이제, 우리는 3D 블록들의 데이터 세트를 크기 \\(3\\times 128^{2}\\times 32\\)의 3-평면들의 데이터 세트로 변환할 수 있다. 이러한 삼중 평면은 3D 블록인 c.f. Fig.를 충실히 재구성할 수 있다. 10. 우리는 그것들을 _raw tri-plane_라고 부른다.\n' +
      '\n' +
      '### 잠재 삼면 공간 압축\n' +
      '\n' +
      '우리의 원시 삼면기는 고품질 모양을 재구성할 수 있지만 이러한 삼면기를 생성하는 것이 상당히 어렵다는 것을 발견했다. 이러한 3면에 대한 확산 모델을 직접 학습하면 그림 4와 같이 붕괴된 결과가 나타난다. 1) 원시 3면이 매우 중복되고 2) 장면 블록 데이터 세트의 형상 다양성이 너무 크다는 두 가지 이유가 주로 있다고 주장한다. 로댕(Wang et al., 2023)과 NFD(Shue et al., 2023)와 같은 이전 작업은 원시 삼평면에서의 확산 가능성을 입증했지만, 훨씬 더 작은 품종, 즉 표준화된 인체 상체에 대한 로댕, 단일 범주에 대한 NFD를 가진 데이터 세트에서만 작업한다.\n' +
      '\n' +
      '도 3. 랜덤하게 크롭된 3D 블록들의 예시들.\n' +
      '\n' +
      'Objects from ShapeNet (Chang et al., 2015). 장면 블록에서 NFD를 재훈련하려고 시도했을 때 그림 4와 같이 의미 있는 모양도 생성하지 못했다.\n' +
      '\n' +
      '우리는 3D 모양에 대한 특징 표현을 찾아야 하는데, 이것은 작고, 확산 모델을 훈련하기 쉽고, 메모리와 계산적으로 효율적이며, 큰 모양 변화로 일반화할 수 있다. 2D 시나리오에서, Stable Diffusion(Rombach et al., 2022)은 확산을 위해 원시 이미지들을 잠재 2D 특징 공간으로 압축한다. 이 접근법은 더 높은 품질의 이미지를 생성하는 더 강력한 모델을 초래한다. 안정적인 확산에서 영감을 받아, 우리는 분해능과 특징 채널이 감소된 잠재 삼면 공간으로 원시 삼면을 압축하는 자동 인코더를 훈련한다. 정확하게는 원시 삼면(x\\in\\mathbb{R}^{3}times N^{2}times C}\\)이 주어지면, 인코더\\(\\mathcal{E}\\)은 잠재 표현\\(z=\\mathcal{E}(x)\\)으로 인코딩하고, 디코더\\(\\mathcal{D}\\)은 잠재로부터 원시 삼면(\\(\\dot{x}=\\mathcal{D}(z)=\\mathcal{D}(\\mathcal{E}(x))\\)을 복원한다.\n' +
      '\n' +
      '**오토 인코더의 훈련 목적**은 다음과 같이 표시된다:\n' +
      '\n' +
      '\\mathcal{L}_{AE}=\\mathcal{L}_{rec}(x,\\mathcal{D}(\\mathcal{E}(x)))+\\mathcal{L}_{KL}(x,\\mathcal{D},\\mathcal{E})+\\mathcal{L}_{geo}\\tag{7}\\mathcal{L}_{rec}(x,\\mathcal{E})\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{rec}\\)은 \\(x\\)과 그 재구성 \\(\\mathcal{D}(\\mathcal{E}(x))\\) 사이에 적용되는 가벼운 \\(L_{1}\\) 규범이다. \\(\\mathcal{L}_{rec}\\) (\\mathcal{L}_{KL}\\)은 표준배(Kingma and Welling, 2013)에서와 같이 \\(q_{\\mathcal{E}}(z|x)=\\mathcal{N}(z;\\mathcal{L}_{\\mathcal{E},\\mathcal{L}_geo^{2}})와 표준정규분포 \\(N(z;0,1)\\) 사이의 Kullback-Leibler-term이다. 고충실도 형상 재구성을 얻기 위해 우리는 \\(\\mathcal{L}_{KL}\\)에 대해 매우 작은 가중치만을 사용한다. \\ (\\mathcal{L}_{geo}\\)는 Eqn. 2에 정의된 기하학적 손실로서 Sec. 3.2와 동일한 점 집합에 기초하여 평가되며, 그 목적은 형태를 충실하게 표현할 수 있는 잠재 트라이 평면을 학습하는 것이므로 오토 인코더를 훈련하기 위해 지배적 손실로서 \\(L_{geo}\\)에 의존한다.\n' +
      '\n' +
      '잠재적인 \\(z\\)은 \\(z=\\big{\\{}z(i)|z(i)\\in\\mathbb{R}^{n^{2}\\times c}, i\\in\\{1,2,3\\}\\big{\\}}의 삼면구조를 갖는다. 우리는 그것을 _latent tri-plane_이라고 부른다. 이는 기존의 DiffusionSDF(Chou et al., 2023)와 달리 임의의 1차원 잠재벡터 \\(z\\)에 의존하여 분포를 자동회귀적으로 모델링하고, 이로 인해 \\(z\\)의 고유한 3D 구조의 많은 부분을 무시하는 것과 대조적이다. 따라서 압축 모델은 \\(x\\)의 세부 사항을 더 잘 보존한다(그림 10 참조). 경험적으로 잠재분해능은 \\(n^{2}=32^{2}\\)으로 설정하였다. 그리고 우리는 \\(c=2\\)와 \\(c=16\\)의 두 가지 잠재 특징 차원을 조사한다.\n' +
      '\n' +
      '잠복 삼면 확산\n' +
      '\n' +
      '학습된 3-평면 자동인코더는 \\(\\mathcal{E}\\)과 \\(\\mathcal{D}\\)으로 구성되어 있으며, 이제 고주파, 불감성 디테일이 추상화된 효율적인 저차원 잠재 3-평면 공간에 접근할 수 있다. 미가공 삼면 공간과 비교하여 잠재 삼면 공간은 이제 데이터의 필수적이고 의미적인 측면에 집중하고 더 낮은 차원, 계산적으로 훨씬 더 효율적인 공간에서 훈련할 수 있기 때문에 가능성 기반 생성 모델에 더 적합하다.\n' +
      '\n' +
      '**확산 확률 모델에 대한 배경.**확산 모델은 정규 분포 변수를 점진적으로 제거하여 데이터 분포 \\(z_{0}\\sim q(z_{0})\\)를 학습하도록 설계된 확률 모델이다. 이 과정은 길이가 \\(T\\)인 고정 마르코프 체인의 역연산을 학습하는 것에 해당한다. 추론 과정은 랜덤 잡음(z_{T}\\)을 샘플링하여 의미 있는 잠재 잡음(z_{0}\\)에 도달할 때까지 점진적으로 잡음제거를 한다. DDPM(Ho et al., 2020)은 \\(T\\) 시간 단계에서 잠재 \\(z_{0}\\)을 백색 가우시안 잡음 \\(z_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)으로 변환하는 확산 과정을 정의한다. 순방향의 각 단계는 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[q(z_{1},...,z_{T}|z_{0})=\\prod_{t=1}^{T}q(z_{t}|z_{t-1}) \\tag{8}\\]\n' +
      '\n' +
      '[q(z_{t}|z_{t-1})=\\mathcal{N}(z_{t};\\sqrt{1-\\beta_{t}}z_{t-1},\\beta_{t}\\mathbf{I}}\\tag{9}\\]\n' +
      '\n' +
      '잡음 잠재 \\(z_{t}\\)은 이전 잡음 샘플 \\(z_{t-1}\\)을 \\(\\sqrt{1-\\beta_{t}}\\)으로 스케일링하고 타임스텝 \\(t\\)에서 분산이 \\(\\beta_{t}\\)인 가우시안 잡음을 추가하여 구한다. 학습 과정에서 DDPM은 가우시안 분포의 매개변수 \\(\\mu_{\\Psi}(z_{t},t)\\)와 \\(\\Sigma_{\\Psi}(z_{t},t)\\)를 예측하는 신경망 \\(\\Psi\\)으로 모델링되어 확산 과정을 반전시킨다.\n' +
      '\n' +
      '\\[p_{\\Psi}(z_{t-1}|z_{t}) =\\mathcal{N}(z_{t-1};\\mu_{\\Psi}(z_{t},t),\\Sigma_{\\Psi}(z_{t},t)) \\tag{10}\\t.\n' +
      '\n' +
      '우리는 \\(\\alpha_{t}:=1-\\beta_{t}\\)와 \\(\\bar{\\alpha}:=\\prod_{s=0}^{t}a_{s}\\)으로 한계분포를 쓸 수 있다:\n' +
      '\n' +
      '[q(z_{t}|z_{0})=\\mathcal{N}(z_{t};\\sqrt{\\bar{\\alpha}_{t}z_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}) \\tag{12}\\] \\[z_{t} =\\sqrt{\\bar{\\alpha}_{t}z_{0}+\\sqrt{1-\\bar{\\alpha}_{t}\\epsilon \\tag{11}\\]\n' +
      '\n' +
      '여기서 \\(\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\). 베이즈 정리를 사용하여 다음과 같이 정의되는 \\(\\tilde{\\beta}_{t}\\)와 \\(\\tilde{\\mu}_{t}(z_{t},z_{0})\\)의 관점에서 사후 \\(q(z_{t-1}|z_{t},z_{0})\\)을 계산할 수 있다.\n' +
      '\n' +
      '\\[\\tilde{\\beta}_{t}:=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t} \\tag{13}\\]\n' +
      '\n' +
      '\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}{1-\\bar{\\alpha}_{t}}z_{0}+\\frac{\\sqrt{\\bar{\\alpha}_{t}}(1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}z_{t}\\tag{14}\\frac{\\sqrt{\\bar{\\alpha}_{t}}{1-\\bar{\\alpha}_{t}}\n' +
      '\n' +
      '\\[q(z_{t-1}|z_{t},z_{0})=\\mathcal{N}(z_{t-1};\\tilde{\\mu}_{t}(z_{t},z_{0}),\\tilde{\\beta}_{t}\\mathbf{I}}\\tag{15}\\]\n' +
      '\n' +
      '이전에는 \\(\\mu_{\\Psi}(z_{t},t)\\)를 매개변수화하는 다양한 방법이 있다. 기존의 DDPM과 같이 추가된 잡음을 예측하는 대신, 본 논문에서는 신경망(\\Psi\\)으로 \\(z_{0}\\)을 직접 예측한다. 예측은 Eqn에서 사용될 수 있다. 도 14를 이용하여 \\(\\mu_{\\Psi}(z_{t},t)\\)을 제조하였다. 구체적으로,\n' +
      '\n' +
      '도 4. **Qualitative unconditioned block generation results.** NFD(Shue et al., 2023) 또한 삼면 확산에 기초한다. 그들은 도형을 나타내기 위해 점유 가치를 사용하는 반면, 우리는 SDF를 사용한다. 세 가지 방법 모두 룸 블록에 대해 훈련된다.\n' +
      '\n' +
      '입력 잠재 벡터 \\(z_{0}\\)로부터 \\(t\\)의 시간차를 균일하게 샘플링하고, 입력 잠재 벡터 \\(z_{0}\\)으로부터 \\(z_{t}\\)의 잡음을 샘플링한다. 시간 조건 잡음 제거 자동 인코더\\(\\Psi\\)는 \\(z_{t}\\)으로부터 \\(z_{0}\\)을 복원하도록 학습한다. 잠재적 삼면 확산 판독의 목적\n' +
      '\n' +
      '\\[\\mathcal{L}_{LTD}=\\|\\Psi(z_{t},\\gamma(t))-z_{0}\\|_{2} \\tag{16}\\]\n' +
      '\n' +
      '여기서 \\(\\gamma(\\cdot)\\)는 위치 부호화 함수이고 \\(\\\\\\cdot\\|_{2}\\)는 MSE 손실이다. 전진 과정이 고정되어 있기 때문에 훈련 시 \\(\\mathcal{E}\\)로부터 \\(z_{t}\\)을 효율적으로 얻을 수 있다. 테스트 기간 동안 최종 출력 \\(z^{\\prime}\\)을 얻을 때까지 반복적으로 \\(z_{T}\\)을 잡음 제거한다. \\ (z^{\\prime}\\)는 1회 통과(\\mathcal{D}\\)로 원시 삼면(x^{\\prime}\\)으로 디코딩될 수 있다. 마지막으로, 미리 훈련된 MLP는 행진 큐브 형상 추출을 위해 밀도가 높은 SDF 볼륨으로 \\(x^{\\prime}\\)을 디코딩한다.\n' +
      '\n' +
      '**2D 레이아웃은 사용자 제어.** 생성 프로세스를 제어하기 위해 객체의 2D 바운딩 박스 투영으로 모델에 알려 바닥 레이아웃 제어를 추가한다. 바닥 레이아웃은 특징 맵(l\\in\\mathbb{R}^{n^{2}\\times m}\\)으로 변환되며, 여기서 \\(n^{2}\\)은 특징 해상도(잠재 \\(z\\)의 평면 해상도와 동일)를 나타내며, 채널 번호 \\(m\\)는 전체 객체 카테고리 수에 해당한다. 각 채널은 객체 클래스의 배치 여부를 나타내는 이진 이미지로 구성된다. 레이아웃 조건 잠재 삼면 확산 읽기의 손실\n' +
      '\n' +
      '\\[\\mathcal{L}_{c-LTD}=\\|\\Psi(z_{t},\\gamma(t),l)-z_{0}\\|_{2} \\tag{17}\\]\n' +
      '\n' +
      '실제로 \\(l\\)은 \\(z_{t}\\)의 세 평면에 직접 연결된다. 실험에서, 이러한 유형의 컨디셔닝이 장면 요소들의 배열을 성공적으로 제어하면서 생성된 형상들의 분산을 여전히 보존한다는 것을 보여준다.\n' +
      '\n' +
      '**3D aware denoising U-Net.** 신경 백본 \\(\\Psi(\\cdot)\\)은 시간조건 U-Net으로 구현된다. 삼면기의 장점은 2차원 텐서로 취급할 수 있고, 따라서 효율적인 2차원 컨볼루션을 적용할 수 있다는 것이다. 그러나, 평면 특징들 사이의 3D 관계들이 무시되기 때문에, 트리-플레인들 상에서 순진하게 실행되는 컨볼루션은 만족스러운 결과를 생성하지 못한다. \\ (\\Psi(\\cdot)\\)는 교차 평면 특징 관계를 설명할 수 있는 연산을 통합해야 한다. 이를 해결하기 위해, 로댕(Wang et al., 2023)은 3D-인식 컨벌루션을 도입하는데, 이 컨벌루션은 그들의 3D 상관관계에 기초하여 평면들 사이의 피처들을 연관시키기 위해 최대-풀링 및 연접을 채용한다. 그러나, 단순한 맥스 풀링은 귀중한 정보를 잃을 수 있다. 본 연구에서는 보다 강력한 변압기를 이용하여 면간 통신을 수행하여 \\(\\Psi(\\cdot)\\)를 구축한다. 그림 5에 \\(\\Psi(\\cdot)\\)의 전체 아키텍처를 나타내었다. 이 아키텍처는 효과적인 3D 인식 특징 학습을 가능하게 한다.\n' +
      '\n' +
      '### 잠복 3면 외삽법\n' +
      '\n' +
      'Repaint(Lugmayr et al., 2022)는 사전 학습된 확산 모델을 사용하여 인상적인 이미지 인페인팅 및 외삽 결과를 보여준다. 그들의 핵심 아이디어는 알려진 픽셀의 잡음 버전을 사용하여 알려지지 않은 픽셀의 잡음 제거 프로세스를 동기화하는 것이다. 리페인트에서 영감을 받아 미리 훈련된 노이즈 제거 백본\\(\\Psi(\\cdot)\\)을 사용하여 3면 외삽한다. 외삽은 잠재 3면 공간에서 수행된다. 형식적으로 잠재 코드 \\(z^{P}=\\left\\{z^{P}(i)|i\\in\\{1,2,3\\}\\right\\}\\)를 조건으로 하는 알려진 블록 \\(P\\)과 부분적으로 겹치는 빈 블록 \\(Q\\)이 주어지면 새로운 블록을 나타낼 수 있는 잠재 삼면 \\(z^{Q}=\\left\\{z^{Q}(i)|i\\in\\{1,2,3\\}\\right\\}\\)을 생성하는 것이 목표이다. 단순화를 위해 본 논문에서는 장면 확장에 충분한 XYZ 축 중 한 축만을 따라 슬라이딩하여 \\(Q\\)을 위치시킨 경우만을 고려한다.\n' +
      '\n' +
      '**평면-방향 외삽.** 3-평면은 조밀한 3D 볼륨의 팩터링된 표현이다. 3개의 평면은 압축되지만 상관 관계가 높기 때문에 3개의 평면에 대한 외삽은 직관적이지 않은 작업이다. 이를 해결하기 위해 도 1에 도시된 바와 같다. 도 6을 참조하면, 3차원 평면 외삽을 2차원 평면 3개의 외삽에 각각 인수분해한 후, 3차원 인식 잡음제거 백본 \\(\\Psi\\)을 이용하여 3차원 평면의 정보를 혼합한다. 구체적으로, \\(i\\in\\{1,2,3\\}\\)을 갖는 \\(i\\in\\{1,2,3\\}\\)번째 축-정렬 평면을 고려할 때, 평면 \\(z^{P}(i)\\)과 평면 \\(z^{Q}(i)\\) 사이의 오버랩 마스크는 \\(O_{i}\\)으로 표시된다. Repaint(Lugmayr et al., 2022)에 이어서, 중첩 마스크 \\(O_{i}\\) 내부의 잡음화된 버전 \\(z^{P}(i)\\)을 사용하여 \\(z^{Q}(i)\\)의 잡음제거 프로세스를 동기화함으로써 \\(z^{P}(i)\\)을 얻기 위해 \\(z^{P}(i)\\)을 외삽하는 것이 실현된다. 구체적으로, 단계 \\(t-1\\)에서, 우리는 잡음 \\(z^{P}_{t-1}(i)\\)을 통해 얻는다.\n' +
      '\n' +
      '\\sqrt{\\alpha_{t-1}(i)\\sim\\mathcal{N}(\\sqrt{\\alpha_{t}}z^{P}_{0}(i),(1-\\alpha_{t})\\mathbf{I})\\tag{18}\\mmathcal{N}(\\sqrt{\\alpha_{t}}z^{P}_{0}(i),(1-\\alpha_{t})\\mathbf{I})\\tag{18}\\mmathcal{N}(\\sqrt{\\alpha_{t}}z^{P}_{0}(i),(1-\\alpha_{t})\n' +
      '\n' +
      '그림 5. **3D 인식 잡음 제거 U-Net.** 잠재 3평면은 3개의 독립적인 평면으로 펼쳐져 다운 샘플링 컨볼루션을 실행한다. 다운-샘플링 층들 이후에, 세 개의 특징 맵들은 1D 토큰들로 평탄화되고 함께 연결되어 자기-어텐션(Vaswani et al., 2017) 및 잔차 블록의 시퀀스를 통해 \\(K=6\\) 배만큼 전진한다. 마지막으로, 1D 어레이는 업샘플링 컨벌루션을 위해 평면들로 재형성되고 트라이-평면 구조로 재조립된다.\n' +
      '\n' +
      '그림 6. **잠재 트라이플레인 외삽.** 알려진 블록\\(P\\)과 알려지지 않은 블록\\(Q\\)이 주어지면, 알려진 잠재 트라이플레인\\(z^{P}\\)을 외삽하여 알려지지 않은 트라이플레인\\(z^{Q}\\)(상단 행)을 얻는 것이 목표이다. 이 3차원 평면 외삽은 3개의 2차원 평면(맨 아래 행)의 외삽으로 인수화된다.\n' +
      '\n' +
      '및 이전 단계 \\(t\\)에서 잡음 제거된 \\(z_{t-1}^{Q}(i)\\)\n' +
      '\n' +
      '\\sim\\mathcal{N}(\\mu_{\\psi}(z_{t-1}^{Q}(i)\\mu_{\\psi}(z_{t}^{Q}(i),t),\\Sigma_{\\psi}(z_{t}^{Q}(i),t)) \\tag{19}\\t.\n' +
      '\n' +
      '그 다음, \\(z_{t-1}^{Q}(i)\\)에 의해 동기화된다\n' +
      '\n' +
      '[z_{t-1}^{Q}(i)\\leftarrow\\text{Cat}\\Big{(}z_{t-1}^{P}(i)\\in O_{i}, z_{t-1}^{Q}(i)\\notin O_{i}\\Big{}\\tag{20}\\]\n' +
      '\n' +
      '여기서 \\(\\text{Cat}(\\cdot)\\)는 텐서 연접을 의미한다. 단, 도 1에 도시한 바와 같이 한다. 도 6을 참조하면, \\(i=3\\)일 때, 두 평면 \\(z^{P}(3)\\)과 \\(z^{Q}(3)\\)은 서로 평행하므로 명시적인 중첩을 갖지 않는다. 우리는 평면(i\\in\\{1,2\\}\\)에 대해서만 동기화를 수행할 수 있다. 그럼에도 불구하고, 자기 주의 층의 시퀀스를 사용하여 구성된 잡음 제거 백본\\(\\Psi\\)은 교차 평면 의존성을 식별하도록 설계되었다. 이 구조는 평면에서 동기화된 특징(\\(\\{1,2\\}\\)이 잡음 제거 단계 전반에 걸쳐 주의 계층을 통해 제3 평면으로 효과적으로 전파될 수 있도록 한다. 실험에서 이 방법이 의미 있는 3차원 형상 외삽을 성공적으로 달성한다는 것을 발견했다. 잠재 3면 외삽을 위한 전체 절차는 알고리즘 1에 요약되어 있다.\n' +
      '\n' +
      '```\n' +
      '\\(t=T,...,1)do\\(z_{t-1}^{Q}\\sim\\mathcal{N}(\\sqrt{\\mu}z_{0}^{P},(1-hat{t-1}^{P}(i)\\notin O_{i}})) endfor endfor return\\(z_{0}^{P},(1-hat{t-1}^{Q}(i)\\leftarrow\\text{N}(\\mu_{t-1}^{Q},t)\\mathbf{I}))\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 잠재 3면 외삽\n' +
      '\n' +
      '**리샘플링.** 동기화를 단순히 적용하는 것이 항상 의미적 및 기하학적으로 일관된 결과를 산출하는 것은 아니라는 것을 발견했다. 중첩 영역에서의 노이즈 부가 과정은 비중첩 영역에서 새롭게 생성된 삼면 부분을 고려하지 않아 부조화를 도입하기 때문이다. 이 문제를 해결하기 위해 Repaint(Lugmayr et al., 2022)에 소개된 리샘플링 전략을 활용한다. 구체적으로, 디노이징 프로세스의 특정 단계에서, 9에서 순방향 확산 방정식을 사용하여 출력에 잡음이 다시 추가되며, 이는 추론 프로세스가 롤백된다는 것을 의미한다. 리샘플링을 위한 두 개의 하이퍼파라미터는 1) 롤백 단계\\(J\\)와 2) 리샘플링 횟수\\(R\\)이다. 본 논문에서는 \\(J=100\\)을 설정하고 \\(R=\\{0,1,2,3,7\\}\\)에 대한 절제 연구를 수행한다. 실험 결과는 리샘플링 횟수를 증가시키면 생성 성능이 향상됨을 보여준다.\n' +
      '\n' +
      '비강체 정합을 이용한 표면 미세화\n' +
      '\n' +
      '동기화된 3면 \\(z^{P}\\)과 \\(z^{Q}\\)은 의미적으로 정렬된 형상을 생성하지만, 잠재 공간 동기화는 점-순차적으로 정렬된 형상을 보장하지 못하여 작은 가시 이음매가 생성된다. 이 문제를 해결하기 위해 추출된 표면 메쉬를 명시적으로 정렬한다. 두 잠재 코드(z^{P}\\)와 \\(z^{Q}\\)로부터 밀도 SDF 볼륨을 유도하고, 표면 메쉬(S^{P}\\)와 \\(S^{Q}\\)를 추출하기 위해 행진 큐브를 실행한다. 우리는 중첩영역 내에 있는 메쉬삼각형의 점들을 균일하게 샘플링하여 \\(\\Omega_{P}^{QI}\\)와 \\(\\Omega_{Q}^{QI}\\)로 표시된 점 집합들을 얻는다. 그런 다음, 중첩영역 바깥쪽에 있는 \\(S^{Q}\\) 삼각형의 점들을 균일하게 샘플링하여 \\(\\Omega_{Q}^{new}\\)으로 표시된 점 집합을 생성한다. 비강체 정합 비용을 최적화하여 \\(S^{P}\\)과 \\(S^{Q}\\)을 정렬한다.\n' +
      '\n' +
      '\\mathcal{L}_{nrr}=\\mathcal{L}_{CD}\\Big{(}\\Omega_{P}^{QI}\\big{},\\Omega_{Q}^{QI}\\big{}+\\mathcal{L}_{CD}\\Big{(}\\mathcal{W}\\big{(}\\Omega_{Q}^{new}\\big{},\\Omega_{Q}^{new}\\Big{}}\\tag{21}\\big{}\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{CD}(\\cdot)\\)는 두 점 구름 사이의 챔퍼 거리를 나타내고, \\(\\mathcal{W}(\\cdot)\\)는 점당 변환을 예측하는 조밀한 비강성 워핑 함수이다. \\(\\mathcal{L}_{CD}(\\cdot)\\) (\\mathcal{W}(\\cdot)\\)는 NDP(Li and Harada, 2022)를 기반으로 계층적 조대-미세 신경 변형 필드를 사용하여 장면 변형을 근사화한다. 이러한 비강체 정합 비용은 외삽된 메쉬(S^{Q}\\)가 중첩 영역 내에서 가능한 한 가깝게 조건 메쉬(S^{P}\\)를 근사하도록 유도하면서 중첩되지 않는 영역에서는 자체 구조를 유지한다.\n' +
      '\n' +
      '블록퓨전으로 무한한 대형 장면을 연출합니다.\n' +
      '\n' +
      '알고리즘 1에 기초하여, 어떤 스케일에서든 크고 경계가 없는 장면들을 구성할 수 있다. 이 목적을 위한 순진한 전략은 처음에 블록을 생성한 다음 슬라이딩 윈도우 방식으로 블록을 블록 단위로 외삽하여 장면을 확장하는 것이다. 그러나, 이러한 직렬 동작은 상당한 양의 시간을 필요로 한다.\n' +
      '\n' +
      '원격 블록들이 서로 독립적일 가능성이 높다는 점을 감안할 때, 큰 장면 생성은 병렬로 실행될 수 있다. 이 프로세스는 초기에 추출된 시드 블록을 동시에 생성하는 것을 포함하며, 여기서 나머지 빈 블록도 병렬로 외삽한다. 구체적으로, 우리는 먼저 슬라이딩 윈도우를 사용하여 세계를 작은 블록으로 나누는데, \\(\\mathcal{B}=\\{B_{1},B_{2},...\\}\\)으로 표시된다. 이웃한 블록들의 각 쌍 사이에 겹침이 있는 부분집합 \\(\\mathcal{B}^{seed}\\)을 선택한다. 우리는 \\(\\mathcal{B}^{seed}\\)의 블록들이 서로 겹치지 않도록 한다. [\\(\\mathcal{B}^{seed}\\)의 상보적 집합은 [\\(\\mathcal{B}^{extra}\\)으로 표시된다. \\(\\mathcal{B}^{seed}\\)의 블록은 독립적으로 병렬로 생성된다. (\\mathcal{B}^{extra}\\)의 나머지 빈 블록은 \\(\\mathcal{B}^{seed}\\)으로부터 외삽된다.\n' +
      '\n' +
      '##4. 실험결과\n' +
      '\n' +
      '### Implementation details.\n' +
      '\n' +
      '**수밀 리메싱** 3D 장면 메쉬를 네트워크 훈련에 사용합니다. 이러한 장면 메쉬는 일반적으로 3D 아티스트에 의해 생성되며 항상 수밀성이 보장되는 것은 아니다. 블렌더의 _voxel remeshing_ tool을 이용하여 미가공 메쉬를 수밀 메쉬로 변환하였다. 리메싱 후, 오브젝트는 내부 및 외부에 명확하게 정의된 것을 가지며, 이는 연속적인 신경 필드 표현을 트레이닝하는데 필수적이다.\n' +
      '\n' +
      '**데이터 세트** 우리는 방, 도시, 마을의 세 가지 다른 유형의 장면에 대해 알고리즘을 테스트합니다. 룸 장면 데이터는 3DFront(Fu et al., 2021) 및 3D-FUTURE(Fu et al., 2021)로부터 획득되며, 이는 34개의 클래스들의 실내 객체들을 갖는 18,968개의 실내 장면들을 포함한다. 우리는 3DFront에서 57K의 무작위 작물을 얻으며, 각 블록 크기는 \\(3.2^{3}\\) 입방 미터로 설정된다. 우리는 5개 미만의 객실과 빈 객실을 걸러내고 마침내 9123개의 객실을 얻었다. 단순화를 위해 3DFUTURE에서 객체들의 유사성을 기반으로 "바닥"의 9가지 클래스로 재그룹화하며, 이 방법은 Pytorch를 사용하여 구현되고 Nvidia V100 GPU에서 훈련된다. 57K 크롭 블록이 있는 3DFront 데이터 세트의 경우 원시 트리 플레인 피팅, 자동 인코더 훈련 및 확산 훈련은 각각 4750, 768 및 384 GPU 시간이 소요된다. 레이아웃 조건에서 단일 삼면 외삽을 실행하는 데 6분이 소요된다. Sec. 3.7에 설명된 대형 장면 생성 전략으로 그림 1에서 대형 실내 장면을 생성한다. 7시간은 3시간 정도 걸립니다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '**재구성 메트릭.** \\(10^{-3}\\) 스케일에서 챔퍼 거리(CD), 도에서 표면 정규 오차(\\(E_{NRM}\\)) 및 센티미터에서 표면 SDF 오차(\\(E_{SDF}\\))를 사용하여 재구성 품질을 평가한다.\n' +
      '\n' +
      '**무조건 생성 메트릭.**무조건 3D 형상 합성의 평가는 직접적인 지상 진실 대응의 결여로 인해 내재된 과제를 제시한다. 따라서, 우리는 이전 작업들에 기초하여, 평가를 위한 잘 확립된 메트릭들에 의존한다(Chou et al., 2023; Siddiqui et al., 2023; Zeng et al., 2022). 이러한 메트릭에는 최소 매칭 거리(MMD), 커버리지(COV) 및 1-최근접-이웃 정확도(1-NNA)가 포함된다. MMD의 경우 더 낮음\n' +
      '\n' +
      'Figure 8. Large City scene generation.\n' +
      '\n' +
      '그림 7. 큰 방 장면 생성.\n' +
      '\n' +
      '더 좋고, COV의 경우 더 높고, 1-NNA의 경우 50%가 최적이다. 이러한 메트릭을 계산하기 위한 거리 측정치로 Chamfer Distance(CD)와 EMD(Earth Mover\'s Distance)를 사용한다. 이러한 메트릭에 대한 보다 포괄적인 세부 사항은 각 문헌에서 확인할 수 있다.\n' +
      '\n' +
      '**사용자 연구 메트릭.** 전체 장면의 지각 품질(PQ) 및 구조 완전성(SC)에 기초하여 장면 생성 결과를 평가하도록 요청받은 7명의 참가자를 대상으로 1에서 5까지의 척도를 사용하여 사용자 연구를 수행했으며, 이는 텍스처 모드(T-) 및 기하학 전용 모드(G-)의 두 가지 모드로 수행된다. 텍스처 모드에서 참가자는 텍스처 메쉬를 보는 반면 지오메트리 전용 모드에서는 지오메트리를 강조하기 위해 텍스처를 흑백 재료로 대체했다. 그 결과 T-PQ, T-SC, G-PQ, G-SC의 4가지 메트릭을 도출하였다.\n' +
      '\n' +
      'SOTA와의 비교\n' +
      '\n' +
      '**단일 블록 생성.** 우리는 NFD(Shue et al., 2023)를 단일 블록 생성을 위한 베이스라인으로 간주한다. NFD는 또한 삼면 확산에 기초한다. 그러나 도형을 나타내기 위해 점유 값을 사용하는 반면 SDF를 사용합니다. 우리는 평가 전에 실내 장면 블록에서 NFD를 재훈련한다. 탭 도 2 및 도 2를 참조하여 설명한다. 도 4는 무조건 실내 블록 생성 결과를 나타낸다. 정량적으로, 우리의 방법은 CD 및 EMD 메트릭에서 커버리지(Cov) 점수가 각각 21.17% 및 23.67% 증가하여 NFD보다 크게 우수하다. 질적으로 NFD는 의미 있는 모양을 생성할 수 없다.\n' +
      '\n' +
      '**실내 장면 생성.** Text2Room(Hollen et al., 2023)을 실내 장면 생성을 위한 베이스라인으로 고려한다. Text2Room은 텍스트 프롬프트를 입력으로 하는 반면, 우리는 2D 레이아웃 맵을 기반으로 한다. 공정한 비교를 위해 자연어를 사용하여 입력실 레이아웃을 설명한 다음 Text2Room에 대한 텍스트 프롬프트의 일부로 연결합니다. 우리의 방법은 텍스처 메쉬를 직접 생성하지 않지만, 우리는 메쉬에 대한 텍스처를 생성하기 위해 기성 텍스트 대 텍스처 생성 도구인 메시 1을 활용한다. 메시는 텍스트2룸과 동일한 텍스트 프롬프트를 사용합니다. Meshy의 질감 생성 결과를 향상시키기 위해 Blender의 복셀 기억 도구를 사용하여 모든 블록을 하나의 개체로 결합한다. 탭 도 2 및 도 2를 참조하여 설명한다. 도 4는 룸 생성의 결과를 나타낸다. 질적으로 단안 깊이 추정의 사용으로 인해 Text2Room의 모양이 왜곡된 것처럼 보이는 반면 BlockFusion은 훨씬 더 나은 실내 모양을 생성한다. 또한 텍스트2룸은 텍스트 프롬프트에 정확하게 응답할 수 없습니다. 텍스트를 생성합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{Textured} & \\multicolumn{2}{c}{Geometry-only} \\\\ \\cline{2-5}  & TPQ\\(\\uparrow\\) & TSC\\(\\uparrow\\) & GPC\\(\\uparrow\\) & GSC\\(\\uparrow\\) \\\\ \\hline Text2Room (Hollen et al., 2023) & 2.14 & 2.29 & 1.00 & 1.28 \\\\ \\hline Ours & **4.14** & **4.14** & **3.71** & **3.86** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. 정량적 실내 장면 생성 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{MMD \\(\\downarrow\\)} & \\multicolumn{2}{c}{COV(\\(\\uparrow\\),\\(\\uparrow\\))} & \\multicolumn{2}{c}{1-**NN**(\\(\\uparrow\\),\\(\\downarrow\\))} \\\\ \\cline{2-7}  & CD & EMD & CD & EMD & CD & EMD \\\\ \\hline NFD (Shue et al., 2023) & 0.0445 & 0.2363 & 22.66 & 29.66 & 89.08 & 83.25 \\\\ \\hline Raw tri-plane Diff. (Ours) & 0.0544 & 0.2744 & 23.99 & 27.50 & 89.91 & 88.50 \\\\ \\hline Latent tri-plane Diff. (Ours) & **0.0324** & **0.1884** & **51.83** & **53.33** & **70.66** & **60.08** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. 실내 블록에 대한 정량적 무조건 생성 결과.\n' +
      '\n' +
      '도 9. **Qualitative Room 생성 결과.** Text2Room(Hollen et al., 2023)은 왜곡된 형상을 생성하여 장면 내 객체의 개수에 정확하게 대응할 수 없다. 예를 들어, \'침대 1개\'라는 프롬프트가 주어지면 여러 개의 침대가 생성됩니다. 대조적으로, BlockFusion은 더 높은 품질의 형상을 생성하고 수치 프롬프트에 정확하게 응답한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '**리샘플링이 외삽에 어떤 영향을 미치는가?** 도 12는 레이아웃-조건 3-평면 외삽의 형상 동기화 결과들을 도시한다. 우리는 \\(R=\\{1,2,3,7\\}\\)으로 다른 리샘플링 시간을 테스트했다. 모따기 거리는 더 많은 재샘플링 단계에 따라 꾸준히 떨어지고 3번의 재샘플링 후에 안정화되며, 여기서 채머 거리의 분산도 수렴한다. 이는 리샘플링 횟수를 증가시키면 동기화 결과의 품질을 향상시킬 수 있음을 시사한다. 명확성을 위해 \\(R=0\\)은 동기화를 수행하지 않는다는 것을 의미하며, 즉 두 블록은 공유 레이아웃 조건을 준수하면서 독립적으로 생성된다. 이 경우 챔퍼 거리가 매우 높아 레이아웃 컨디셔닝만으로는 블록 간의 일관된 지오메트리를 보장할 수 없음을 나타냅니다.\n' +
      '\n' +
      '** 비강체 등록 기반 후처리가 필요한가요?** 네. 잠재적 3면 외삽은 의미론적 및 기하학적으로 합리적인 전이를 생성한다. 그러나, 고주파, 불감성 디테일은 자동 인코더에 의해 추상화되기 때문에, 잠재 3면 공간에서의 외삽은 필연적으로 사소한 이음매를 초래한다. 도 1에 도시된 바와 같다. 도 13을 참조하면, 비강성 등록 기반 후처리는 이 문제를 효과적으로 완화할 수 있다.\n' +
      '\n' +
      '**BlockFusion이 창의성을 가지고 있습니까?** BlockFusion은 학습 데이터셋에 존재하지 않는 새로운 모양을 생성합니다. 이것은 주로 새로운 방식으로 기존 요소를 재배열하는 능력에서 발생한다. 예를 들어, 도 1에 도시된 바와 같다. 14, BlockFusion manages to generate\n' +
      '\n' +
      '도. 11. **3-평면 외삽의 정성적인 결과.** 3D 박스는 외삽할 블록을 보여준다. 오버랩 비율은 상위 3개 행의 경우 25%, 하위 3개 행의 경우 50%이다.\n' +
      '\n' +
      '도. 12. 상이한 리샘플링 시간(\\(R\\)).** 챔퍼 거리는 그들의 중첩 영역 내의 두 블록 메시들로부터 샘플링된 포인트 세트들에 기초하여 계산된다. 1회 동기화 후 형상 일관성이 크게 향상되며, 추가 동기화 단계(즉, 리샘플링)를 사용하면 형상 일관성이 더욱 향상된다. \\ (R=0\\)은 동기화가 없음을 의미한다.\n' +
      '\n' +
      '숫자 "24"처럼 생긴 새 테이블과 하트처럼 생긴 새 방 이는 배치 안내에 따라 테이블과 벽의 분수와 같은 기본 모양을 다시 조합할 수 있는 능력으로 가능하다. 이는 블록퓨전이 다양하고 시각적으로 매력적인 장면을 생성하는 강력한 도구로서의 가능성을 보여준다.\n' +
      '\n' +
      '### 대형 장면 세대.\n' +
      '\n' +
      '우리는 대규모 장면 생성을 위한 BlockFusion의 기능을 보여준다. 결과는 그림 1에 나와 있다. 마을, 도시, 방 장면은 각각 1, 7, 8이다. 생성 프로세스는 사용하기 쉬운 그래픽 사용자 인터페이스(GUI)를 사용하여 생성된 레이아웃 맵에 조건화된다. 장면의 범위가 무한히 확장될 수 있음을 강조하는 것이 중요하다. 우리는 블록퓨전이 높은 수준의 형상 품질을 유지하면서 이러한 대규모로 3D 장면을 생성할 수 있는 첫 번째 방법이라고 믿는다.\n' +
      '\n' +
      '##5. 결론 및 논의\n' +
      '\n' +
      '실험 결과 제안된 BlockFusion은 실내 및 실외 시나리오 모두에서 고품질 기하학을 가진 다양하고 기하학적으로 일관되며 경계가 없는 대형 3D 장면을 생성할 수 있음을 보여준다. 생성된 메쉬는 기성 질감 생성 도구와 매끄럽게 통합되어 시각적으로 만족스러운 외관으로 질감 결과를 얻을 수 있다. 우리는 이 접근법이 완전히 자동화된 산업 품질의 대규모 3D 콘텐츠 생성을 위한 중요한 단계를 나타낸다고 믿는다.\n' +
      '\n' +
      '블록퓨전(BlockFusion)의 광범위한 특성으로 인해 오픈 월드 게임을 위한 지도 생성기의 역할을 할 수 있다. 블록퓨전을 유니티로 통합해 플레이어들이 정해진 세계 경계에 구애받지 않고 자유롭게 돌아다니며 세계를 탐험할 수 있는 오픈월드 게임을 개발한다. 이에 대한 데모는 보충 영상에서 확인할 수 있다.\n' +
      '\n' +
      '**Limitations.** BlockFusion의 현재 구현은 몇 가지 한계에 직면해 있다. 우리의 방법은 의자의 다리와 같은 장면에서 매우 미세한 기하학적 세부 사항을 생성하지 못할 수 있다. 이 문제는 주로 3중 비행기에 사용되는 제한된 해결책에서 비롯된다. 가능한 해결책은 3면 초해상도를 채택하는 것이다. 또한 경계 상자 조건은 객체의 방향이 아닌 대략적인 배치만 제어할 수 있습니다. 우리는 바운딩 박스 맵과 객체 배향 맵 모두에서 확산 컨디셔닝을 트레이닝함으로써 정확한 배향 제어가 달성될 수 있다고 믿는다. 이 배향 맵은 또한 사용자 명령으로부터 용이하게 획득될 수 있다. 마지막으로, 작은 장면에서 텍스처 메쉬 결과를 보여주었지만, 큰 장면 메쉬에 대해 전역적으로 일관된 텍스처를 생성하는 작업은 도전적이고 흥미로운 향후 노력이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* M. Atzmon and Y. Lipman(2020)Sal: 원시 데이터에서 모양에 대한 불가지론 학습을 나타낸다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2565-2574. Cited by: SS1.\n' +
      '*O. 아브라함, T 헤이즈, 오 가프니 굽타영 타이그먼, D. 파릭, D. 리스친스키, O. 튀김, X. 음(2023)Spactnet: 제어 가능한 이미지 생성을 위한 공간-텍스트 표현. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18370-18380. Cited by: SS1.\n' +
      '*O. 아브라함, D. 리스친스키, O. 튀김(2022) 자연 이미지의 텍스트 기반 편집을 위한 블렌디드 확산. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18208-18218. Cited by: SS1.\n' +
      '*O. L. 바탈 야리브 립맨과 T Dieek(2023)Multidiffusion: 제어된 이미지 생성을 위한 확산 경로 융합. 인용: SS1.\n' +
      '* D. Bashkirova, J. Lezama, K. 손경 Senehzo, and I. Essa(2023)MaskKetch: unpaired structure-guided instance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1879-1889. Cited by: SS1.\n' +
      '*T. Brooks, A. Holynski, and A. Efros (2010)Instruction2pix: 이미지 편집 지시를 따르는 것을 배우는 것. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18392-18402. Cited by: SS1.\n' +
      '* E. R. Chan, C. Z. Lin, M. A. Chan, K. 나가노, B. 판 데멜로, 오. Gallo, L. J. Guhys, J. Tremblay, S. Khamis, et al.(2022)Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1612-1613. Cited by: SS1.\n' +
      '* A. X. Chang, T. 펑크하우스, L. 구이스, P. 한차, Q 황진 이성 사바레스 사버스 Song, H. Su, et al.(2015)ShapeNet: a information-rich 3d model repository. ArXiv:1512.08012. 인용: SS1.\n' +
      '* D. D. Chen, T. 싯디치, 이현석 Tulyakov와 M. Neiferen(2023)Telext: diffusion model을 통한 text-driven texture synthesis. ArXiv:2303.11396. 인용: SS1.\n' +
      '* H. Chen, J. Gu, A. Chen, W. 톈지 투락 Liu, H. Su (2023) Single-stage diffusion net: 3d generation과 reconstruction에 대한 통일된 접근방법. ArXiv:2304.0714. 인용: SS1.\n' +
      '*Z. 천지왕, 지. Liu(2022)Sceontedreamer: 2d 이미지 컬렉션으로부터 무제한 3d 장면 생성. ArXiv:2302.01330. 인용: SS1.\n' +
      '*G. Chou, Y Bahat, and F. Heide(2023)Diffusion-sdf: conditional generative modeling of signed distance functions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2262-2272. Cited by: SS1.\n' +
      '* J. Deng, W. 채정국 황원 Hu, J. Hwang, and C. Wang(2023)City-cnn: infinite and controllable 3d City layout generation. ArXiv:2312.01508. 인용: SS1.\n' +
      '* P. Dhariwal and A. Nichol (2021)Diffusion model beat gans on image synthesis. 신경 정보 처리 시스템 43, pp. 8780-8794. 인용: SS1.\n' +
      '*Z. Erkoc, F. Ma, Q 샨만 Niessner, and A. Dai(2023)Hyperdiffusion: generate implicit neural fields with weight-space diffusion. ArXiv:2303.17015. 인용: SS1.\n' +
      '\n' +
      '도. 14. **트레이닝 세트에 존재하지 않는 룸들을 생성하기 위해 레이아웃 제어를 사용하는.** 텍스처들은 대응하는 텍스트 프롬프트를 사용하여 Text2tex(Chen et al., 2023b)를 사용하여 생성된다.\n' +
      '\n' +
      '도. 13. 좌측: 잠재 삼면 외삽 결과, 우측: 비강체 등록 적용 후.\n' +
      '\n' +
      '* Fang et al. (2023) Chuan Fang, Xiaotao Hu, Kunming Luo, and Ping Tan. 2023. Ctrl-Room: 레이아웃 제약조건을 갖는 제어가능한 텍스트-투-3D Room Meshes Generation. _ arXiv preprint arXiv:2310.03080_(2023).\n' +
      '* Fridman et al. (2023) Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. 2023. 장면 스케이프: 텍스트 구동 일관성 장면 생성_ arXiv preprint arXiv:2302.01133_(2023).\n' +
      '* Fu et al. (2021) Hun Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jianming Wang, Cao Li, Qiun Zeng, Chengyue Sun, Rongfei Jia, Binjang Zhao, et al. 2021. 3d-front: 3d furnished rooms with layouts and semantics. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 10933-10942.\n' +
      '* Fu et al. (2021) Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binjang Zhao, Steve Maybank, and Dacheng Tu. 2021. 3d-미래: 질감을 갖는 3d 가구 형상_ International Journal of Computer Vision_ (2021), 1-25.\n' +
      '* Gil et al. (2022) Rinon Gil, Yival Alahiof, Yuval Atzmon, or Patashnik, Amit H Bermona, Gal Chechik, and Daniel Cohen-Orz. 2022. 이미지는 1보다 가치가 있다. 텍스트 역산을 이용한 텍스트-이미지 생성을 개인화한다. _ arXiv preprint arXiv:2208.01618_(2022).\n' +
      '* Gao et al. (2022) Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangvue Yin, Daiqing Li, Or Lianz, Zang Gojic, and Sanja Fidler. 2022. GeStd: 이미지로부터 학습된 고품질 3d 검색 형상의 생성 모델. _ 신경 정보 처리 시스템_35(2022), 31841-31854에서의 발전.\n' +
      '* Gropp et al. (2020) Amos Gropp, Izor Yariv, Niri Ma, Matan Atzmon, and Yacim Lipman. 2020. 형상 학습을 위한 암묵적 기하 규칙화_ arXiv preprint arXiv:2002.10099_(2020).\n' +
      '* Hertz et al. (2002) Amir Hertz, Rox Mokady, J. Terenbaum, Kif Akerman, Yael Pritch, and Daniel Cohen-Orz. 2022. 교차 주의 제어를 갖는 프롬프트-대 프롬프트 이미지 편집. _ arXiv preprint arXiv:2208.01262_(2022).\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. 디노이징 확산 확률 모델. _ 신경 정보 처리 시스템들_33(2020), 6840-6851에서의 진보들.\n' +
      '* Holling et al.(2023) Lukas Holling, Gao Cao, Andrew Owens, Justin Johnson, and Matthias Niefner. 2023. Text2room: 2d 텍스트-투-이미지 모델들로부터 텍스처링된 3d 메쉬들을 추출하는 단계; _ arXiv preprint arXiv:2303.11988_(2023).\n' +
      '* Hong et al. (2023) Yicong Hong, Kai Zhang, Jiaxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Tuning Bui, and Hao Tan. 2023. Lmm : 단일 영상에 대한 대형 재구성 모델 to 3d. _ arXiv preprint arXiv:2311.04020_(2023).\n' +
      '* Huang et al. (2023) Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. 2023. 작성: 작성 가능한 조건을 갖는 창의적이고 제어 가능한 이미지 합성. _ arXiv preprint arXiv:2302.07978_(2023).\n' +
      '*Jun and Nichol(2023) Heewoo Jun and Alex Nichol. 2023. Shap-e : 조건부 3d 암묵함수 생성_ arXiv preprint arXiv:2305.0643_(2023).\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes _ arXiv preprint arXiv:1312.6114_(2013).\n' +
      '* Hao et al. (2023) Jiahao Li, Hao Nai, Kang Zhang, Zexiang Xu, Fujjun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. 2023b. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. _ arXiv preprint arXiv:2311.06214_(2023).\n' +
      '* Li and Harada(2022) Yang Li and Matyuz Harada. 2022. 신경 변형 피라미드를 갖는 비강성 점군 등록. _ 신경 정보 처리 시스템_35(2022), 27737-27768에서의 발전.\n' +
      '* Li et al. (2022) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2022a. Gligen: 오픈셋 접지 텍스트 대 이미지 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 22511-22521.\n' +
      '* Lin et al. (2023) Chen-Hsuan Lin, Jun Gao, Liang Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. MSR1: 고해상도 텍스트-to-3d 콘텐츠 생성. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 300-309\n' +
      '* Liu et al. (2023) Minghua Liu, Ruoxi Shi, Linzhao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Cheng Zeng, Jiayuan Gu, and Hao Su. 2023c. One-2-3-45++ 일관된 다시점 생성과 3d 확산을 갖는 고속 단일 영상 3d 객체 _ arXiv preprint arXiv:2311.07885_(2023).\n' +
      '* Liu et al. (2023a) Minghua Liu, Chao Xu, Hiaan Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. 2023c. One-3-45: 임의의 단일 이미지 내지 3d 메쉬를 45초 내에 per-shape 최적화 없이_ arXiv preprint arXiv:2306.16982_(2023).\n' +
      '* Lin et al. (2023) Ruoxhi Lin, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023a. Zero-1. t-3-o: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 9298-9390\n' +
      '* Liu et al. (2023) Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxoxong Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2023b. SyncBrear: 단일 시점 영상으로부터 다시점 정합 영상 생성. _ arXiv preprint arXiv:2309.04534_(2023).\n' +
      '* Yao et al. (2023) Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzenhakim, Lian Paul, and Weiyang Liu. 2023a. MeshDiffusion: Score-based Generative 3D Mesh Modeling. In _International Conference on Learning Representations_. [https://openreview.net/forum?id=0qpX2aPPv60] (https://openreview.net/forum?id=0qpX2aPPv60)\n' +
      '* Long et al. (2023) Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. 2023. Wonder3d: cross-domain diffusion을 이용한 단일 이미지 내지 3d; _ arXiv preprint arXiv:2310.15008_(2023).\n' +
      '* Lugmayr et al. (2022) Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. 리페어: 디노이징 확산 확률 모델을 이용한 인페인팅. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 11461-11471.\n' +
      '* Mildenthal et al. (2021) Ben Mildenthal, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramanmamoorthi, and Ren Ng. 2021. 다음: 장면들을 뷰 합성을 위한 신경 복사 필드로 표현. _ 커뮤니티 ACM_65, 1(2021), 99-106.\n' +
      '* Mao et al. (2023) Chong Mao, Xuitong Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qiu. 2023. 273a: 롱-레이어: 텍스트-대-이미지 확산 모델들에 대한 보다 제어가능한 능력을 파내도록 적응적인 학습 _ arXiv preprint arXiv:2302.08453_(2023).\n' +
      '* Miller et al. (2023) Norman Miller, Jiasarar Sudigotti, Lorenzo Porzi, Samuel Rota Baba, Peter Kontschieder, and Mattis Nielsen. 2023. 드리프트: 렌더링-유도 3d 래디언스 필드 확산. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 4328-4338\n' +
      '* Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Rames, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. 글라이드: 텍스트 유도 확산 모델을 사용한 실사 이미지 생성 및 편집을 향한다. _ arXiv preprint arXiv:2112.10741_(2021).\n' +
      '* Nichol et al. (2022) Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. 포인트-e: 복잡한 프롬프트로부터 3d 포인트 클라우드를 생성하기 위한 시스템. _ arXiv preprint arXiv:2212.08751_(2022).\n' +
      '* Nichol and Dhariwal (2021) Alexander Quinm Nichol and Prafulla Dhariwal. 2021. 개선된 잡음 제거 확산 확률 모델. In _International Conference on Machine Learning_. PMLR, 8162-8171.\n' +
      '* Park et al. (2019) Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsoft: 형상 표현을 위한 연속 부호 거리 함수 학습. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 165-174.\n' +
      '* Poole et al. (2022) Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. 드림퓨전 : 2d 확산을 이용한 Text-to-3d. _ arXiv preprint arXiv:2209.14988_(2022).\n' +
      '* Ramesh et al. (2022) Atitiya Ramesh, Prafulla Dhariwal, Alexe Nichol, Casey Chu, and Mark Chen. 2022. 클립 글자를 갖는 계층적 텍스트-조건부 이미지 생성_ arXiv preprint arXiv:2204.06125_1, 2 (2022).\n' +
      '* Zanesh et al. (2021) Atitiya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Garg, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In _International Conference on Machine Learning_. PMLR, 8821-8831.\n' +
      '* Roinhond et al. (2022) Robin Roinhond, Andreas Blattmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 10684-10695\n' +
      '* Roinhond et al. (2022) Robin Roinhond, Andreas Blattmann, Dominic Lorenz, Patrick Esser, and Bjorn Ommer. 2022. 잠재 확산 모델을 이용한 고해상도 영상 합성. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 10684-10695\n' +
      '* Ruiz et al. (2023) Natani Ruiz, Yuuzhen Li, Varun Jampeni, Yael Pritch, Michael Rubinstein, and Kif Akerman. 2023. 드림포토: 피사체 중심 생성을 위한 텍스트-이미지 확산 모델을 미세 조정한다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 22500-22510.\n' +
      '* Saharia et al. (2022) Chifwan Saharia, William Chan, Suzuhuha Saxena, Lala Li, Jay Hwang, Emily L Denton, Kamyar* Voyrov et al. (2023) Andrey Voyrov, Krik Aherman, and Daniel Cohen-Or. 2023. 스케치-유도 텍스트-이미지 확산 모델. In _ACM SIGGRAPH 2023 Conference Proceedings_. 1-11\n' +
      '* Wang et al. (2023) Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Rao, Tadas Baltrusaitis, Jingting Shen, Dong Chen, Fang Wen, Qing Chen, et al. 2023. Rolim:diffusion을 이용하여 3d 디지털 아바를 조각하는 생성 모델. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 4563-4573\n' +
      '* Wang et al. (2022) Tengfei Wang, Ting Zhang, Rio Zhang, Hao Ouyang, Dong Chen, and Fang Wen. 2022. 이미지 대 이미지 변환을 위해 필요한 모든 것을 사전 훈련. _ arXiv preprint arXiv:2205.12952_(2022).\n' +
      '* Wu et al. (2022) Weiliuun Wang, Jianmin Rao, Wengeng Zhou, Dong Dong Chen, Dong Chen, Lu Yuan, and Houqiang Li. 2022. 확산 모델들을 통한 의미론적 이미지 합성_ arXiv preprint arXiv:2207.00050_(2022).\n' +
      '* Wang et al. (2021) Xinpeng Wang, Chandan Tsehwath, and Matthias Niefner. 2021. Scenformer: 트랜스포머를 구비한 실내 장면 생성. _2021 국제회의 on 3D Vision(SDV)_. IEEE, 106-115.\n' +
      '* Wood et al. (2021) Froml Wood, Tadas Baltrusaitis, Charlie Hewitt, Sebastian Dzidzio, Thomas J Cashman, and Jamie Shotton. 2021년, 될 때까지 위장해 합성 데이터만으로 야생에서 얼굴 분석 In _Proceedings of the IEEE/CVF international conference on computer vision_. 3681-3691.\n' +
      '* Xu et al. (2023a) Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. 2023a. Dmv3d: 3d 대형 재구성 모델을 이용한 다시점 확산 잡음 제거_ arXiv preprint arXiv:2311.09217_(2023).\n' +
      '* Xu et al. (2023b) Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. 2023b. Dmv3d: 3d 대형 재구성 모델을 이용한 다시점 확산 잡음 제거_ arXiv preprint arXiv:2311.09217_(2023).\n' +
      '* Yan et al. (2024) Han Yan et al. 2024. Frankenstein: Generating Semantic-Compositional 3D Room in One Trjplane. (2024).\n' +
      '* Yang et al. (2023) Jiayi Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. 2023. Consistent: Multiview Im 확산을 위한 3D Consistency를 Enforcing arXiv preprint arXiv:2301.1043_(2023).\n' +
      '* Zeng et al. (2022) Xiaohui Zeng, Anshu Vahdat, Francis Williams, Zan Gojek, Or Litany, Sanja Fidler, and Karsten Kreis. 2022. LION: 3차원 형상 생성을 위한 잠재점 확산 모델 _ arXiv preprint arXiv:2201.06978_(2022).\n' +
      '* Limin et al. (2023) Lvmin Zhang, Angy Rao, and Manneesh Agrawala. 2023. 텍스트 대 이미지 확산 모델에 조건부 제어를 추가하는 단계. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 3836-3847\n' +
      '* Zheng et al. (2023) Xin-Yang Zheng, Hao Pan, Peng-Shui Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. 2023. 제어 가능한 3d 형상 생성을 위한 국부 주의 sdf 확산 _ arXiv preprint arXiv:2308.0461_(2023).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>