<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'loss [36] transferring 3D-aware knowledge. More recently, alternative approaches focus on training feed-forward 3D diffusion models for fast generation, facilitated by large-scale 3D object datasets like Objaverse [7]. Once trained, these models can produce signed distance field [5], points [32], radiance fields [18; 52], mesh [27], or multi-view images [25] through a single forward inference within one minute.\n' +
      '\n' +
      'Despite compelling results on simple object generation, these feed-forward methods usually encounter difficulties when applied to more complex data, such as scenes with multiple objects and complex occlusion. Fig. 2 illustrates the drawbacks of existing models when dealing with such combining objects. However, upon generating each object separately, we observed that these models performed well. We perform an in-depth analysis of this "multi-object gap" and conjecture that this gap comes from the bias of their training data, _i.e.,_ Obja-verse. The scarcity of 3D assets containing multiple objects makes it challenging for trained models to manage composites beyond the training data distribution.\n' +
      '\n' +
      'Given the observations made above, is it possible to design a generative system that can produce 3D content containing multiple objects? Typically, skilled human artists create each object separately before integrating them into a whole. This has motivated us to present a compositional generation paradigm termed _ComboVerse_, which generates each object individually and then focuses on automatically combining them to create a composite. A key advantage of our proposed paradigm is its ability to effectively manage complex assets containing multiple objects and occlusion.\n' +
      '\n' +
      'Our approach comprises two stages: single-object reconstruction and multi-object combination. We first decompose and reconstruct each object within an image independently, using an occlusion removal module and an image-to-3D\n' +
      '\n' +
      'Figure 1: _ComboVerse_ can generate high-quality 3D models from a single image that contains multiple objects, _e.g.,_ a squirrel sitting on a paper box. We show textured meshes of created 3D content, showcasing stunning reconstruction quality.\n' +
      '\n' +
      'model. In the second stage, we aim to automatically combine the generated 3D objects into a single model, accounting for various factors such as object scale, placement, and occlusion. However, this process poses a challenge due to depth-size ambiguity in the input image, leading to inaccurate composition.\n' +
      '\n' +
      'To address this issue, we opt for pre-trained diffusion models as spatial guidance for object positioning. Unlike previous SDS-based methods [46, 23, 36, 4] that require optimizing both the shape and texture from scratch, we fix the 3D model of individual objects and focus only on achieving a reasonable spatial layout so that the optimization process would be much faster. However, we have found that the standard SDS is insufficient for accurately placing objects, as it tends to prioritize content over position to match the given text prompt (see Fig. 5). To address this issue, we introduce a spatially-aware SDS loss that places greater emphasis on the spatial relationships between objects. Specifically, we reweight [14] the attention map of the position tokens that indicate the spatial relation for score distillation. By prioritizing the awareness of position, the proposed loss can effectively distill the spatial knowledge from well-trained diffusion models for object placement.\n' +
      '\n' +
      'To evaluate our method, we collect a benchmark consisting of 100 images that comprise a diverse range of complex scenes. We evaluate _ComboVerse_ on this benchmark, and extensive experiments show clear improvements over previous methods in terms of handling multiple objects, occlusion, and camera settings. Our main contributions can be summarized as:\n' +
      '\n' +
      '* We propose _ComboVerse_, an automatic pipeline that extends object-level 3D generative models to generate compositional 3D assets from an image.\n' +
      '* We perform an in-depth analysis of the "multi-object gap" of existing feed-forward models from both model and data perspectives.\n' +
      '* We propose spatially-aware diffusion guidance, enabling pre-trained image diffusion models to provide guidance on spatial layout for object placement.\n' +
      '\n' +
      '## 2 Related works\n' +
      '\n' +
      '**3D Generation with 2D Diffusion Prior.** Many methods opt to pretrained 2D diffusion models [41, 15] as a source of 3D guidance. Early works [36] proposed a score distillation sampling method to leverage the imaginative power of 2D diffusion for text-conditioned 3D content creation. Later works have improved the quality by using two-stage optimization [4, 23, 30], better score distillation [54], and stronger foundation diffusion models [22, 60]. Other works [29, 37, 43, 51, 58, 46] extend the approach to generate 3D models from a single image. Some works [59, 45] replace implicit representation with 3D gaussian splatting. Although the results are promising, creating a 3D model in this way can take several minutes to hours of optimization.\n' +
      '\n' +
      '**Feed-forward 3D Generative Models.** Another line of approaches trained feed-forward models for fast generation, eliminating the need for per-case optimization. 3D-aware generative adversarial networks [1, 2, 11, 33, 34, 57] have gained considerable research interest in early research. Later, many attempts have been made to leverage diffusion models for image-conditioned and text-conditioned 3D generation. Once trained, they can produce signed distance field [5, 6], points [32], radiance fields [16, 18, 52, 61, 12], mesh [27], or multi-view images [24, 25, 26, 28, 47] without optimization. Besides diffusion models, recent works also explore feed-forward 3D reconstruction with transformer architecture [17, 21] or UNet architecture [44]. Despite fast generation, these methods are limited by the training data, restricting their ability to reconstruct complex 3D assets. We aim to build on these object-level generative models and extend them to handle more complex objects or scenes.\n' +
      '\n' +
      '**Compositional 3D Generation.** Previous studies [33] have investigated the use of compositional neural radiance fields in an adversarial learning framework for the purpose of 3D-aware image generation. Additional studies have explored the concept of part-based shape generation, which involves assembling 3D parts into a 3D model. The seminal work [9] retrieves from a mesh database to find parts of interest and composite the cut parts to produce novel objects. Later, the following works involve probabilistic models for part suggestion [19], semantic attribute [3], fabrication [42], and CAD assembly [55]. Some works [48] use neural radiance fields to represent different 3D components and then render these parts into a 3D model. With pretrained diffusion models as guidance, recent work [35] generates compositional 3D scenes with user-annotated 3D bounding boxes and text prompts. Concurrent works generate 3D scenes from text prompts by using large language models (LLMs) to propose 3D layouts as an alternative for human annotations [10, 49, 53], or jointly learning layout during optimization process [8]. These approaches can produce 3D scenes that match the text prompts, but texts can be unclear and imprecise when describing how objects are arranged in space. In contrast, our approach focuses on reconstructing complex 3D assets from a reference image. Unlike text descriptions that can be unclear or ambiguous, images represent the spatial relations among objects more accurately, requiring higher standards of composition quality.\n' +
      '\n' +
      '## 3 ComboVerse\n' +
      '\n' +
      'In this section, we will first analyze the "multi-object gap" of state-of-the-art image-to-3D generation methods trained on Objaverse, followed by a discussion of our compositional generative scheme. We then present the details of the two stages involved in our approach: single-object reconstruction and multi-object combination. The overview architecture is shown in Fig. 3.\n' +
      '\n' +
      '### Analysis of "Multi-Object Gap"\n' +
      '\n' +
      'Most existing feed-forward models are trained on Objaverse [7]. As shown in Fig. 2 and Fig. 12, these methods suffer three typical failure cases for multiple objects generation due to data and model biases.\n' +
      '\n' +
      '**Camera Setting Bias.** When setting up cameras, most image-to-3D methods assume that the object has a normalized size and is centered in the image.\n' +
      '\n' +
      'However, in scenarios with multiple objects, an object could appear in a corner of the scene or be very small in the image, which does not conform to object-centric assumptions. Such a case can result in a significant decline in modeling quality.\n' +
      '\n' +
      '**Dataset Bias.** The Objaverse dataset predominantly features single-object assets, which poses a challenge for models trained on it to generalize to complex composites. Additionally, the near absence of occlusion in Objaverse results in these models struggling to handle occluded objects. As a result, generated objects often blend together due to occlusion ambiguity.\n' +
      '\n' +
      '**Leaking Pattern.** Existing methods tend to exhibit leakage issues when generating multiple objects simultaneously, where the geometry and appearance of one object can affect another. This issue may stem from the model\'s biases, as it is trained to generate a single object where different parts are consistent. However, in scenes with multiple objects, different objects may have different geometry and texture. If they still affect each other, it can lead to bleeding patterns.\n' +
      '\n' +
      '**Motivation.** As shown in Fig. 2, though the current methods have difficulties in generating compositional objects, we have observed that these methods are successful in reconstructing each component object. This observation suggests the possibility of generating each object separately (Sec. 3.2) and subsequently combining them to form the desired compositional object (Sec. 3.3).\n' +
      '\n' +
      'Figure 2: **“Multi-object gap” of models trained on Objaverse.** (a) Camera Setting Bias. The reconstruction quality for small and non-centered objects will significantly downgrade compared to separate reconstruction. (b) Occlusion. The reconstruction results tend to blend when an object is occluded by another. (c) Leaking Pattern. The shape and texture of an object will be influenced by other objects in the input image. For example, in (c), the tiger’s back face adopts the owl’s color, and its back surface becomes convex instead of concave due to the owl’s shape influence.\n' +
      '\n' +
      '### Single-Object Reconstruction\n' +
      '\n' +
      '**Components Decomposition.** Given an input image \\(I\\), we first specify each object\'s 2D bounding box \\(\\{b_{i}\\in\\mathbb{Z}^{4}\\}\\), indicating the coordinates of the upper left and bottom right corners. Given bounding boxes \\(\\{b_{i}\\in\\mathbb{Z}^{4}\\}\\) for different objects, we use SAM [20] to segment each object as follows:\n' +
      '\n' +
      '\\[O_{i},M_{i}=\\text{SAM}(I,b_{i}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(O_{i}\\) and \\(M_{i}\\) are the RGB channels and binary mask of \\(i\\)-th object.\n' +
      '\n' +
      '**Object Inpainting.** To complete \\(O_{i}\\) that is possibly occluded by another object, we utilize Stable Diffusion (SD) [41] for objects inpainting. However, we face the challenge of not having a known mask to identify occluded regions. To address this issue, we design a strategy for completing the occluded parts of objects. First, to avoid generating a white or black border around the objects when inpainting, we replace the background of image \\(O_{i}\\) with random noise, and the noised image \\(I_{i}\\) is generated as follows:\n' +
      '\n' +
      '\\[I_{i}=O_{i}+noise*(\\sim M_{i}), \\tag{2}\\]\n' +
      '\n' +
      'where the \\(\\sim M_{i}\\) is the background region of \\(O_{i}\\). The noised image \\(I_{i}\\) is illustrated in Fig. 4. Second, the background region and bounding box \\(b_{i}\\) are combined to generate an inpainting mask \\(m_{i}\\) for each object, which indicates the inpainting region for each one. Specifically, for each inpainting mask \\(m_{i}\\), the pixels that\n' +
      '\n' +
      'Figure 3: **Overview of our method. Given an input image that contains multiple objects, our method can generate high-quality 3D assets through a two-stage process. In the single-object reconstruction stage, we decompose every single object in the image with object inpainting, and perform single-image reconstruction to create individual 3D models. In the multi-object combination stage, we maintain the geometry and texture of each object while optimizing their scale, rotation, and translation parameters \\(\\{s_{i},r_{i},t_{i}\\}\\). This optimization process is guided by our proposed spatially-aware SDS loss \\(\\mathcal{L}_{\\text{SSDS}}\\), calculated on novel views, emphasizing the spatial token by enhancing its attention map weight. For example, considering the prompt _“A fox lying on a toolbox.”_ given to the 2D diffusion model, we emphasize the spatial token “lying” by multiplying its attention map with a constant \\(c\\) (\\(c>1\\)). Also, we utilize the reference loss \\(\\mathcal{L}_{\\text{Ref}}\\), calculated on a reference view for additional constraints.**lie in the bounding box but outside the foreground object are set to 1, and the others are set to 0. That is:\n' +
      '\n' +
      '\\[m_{i}=(\\sim M_{i})\\cap b_{i}, \\tag{3}\\]\n' +
      '\n' +
      'where the \\(\\sim M_{i}\\) is the background region of \\(i\\)-th object \\(O_{i}\\), and \\(b_{i}\\) indicates its bounding box. The mask \\(m_{i}\\) is illustrated in Fig. 4. Finally, we input \\(I_{i}\\) and \\(m_{i}\\) that contain bounding box information to SD, to complete the object \\(I_{i}\\) inside the bounding box \\(b_{i}\\): \\(\\hat{I}_{i}=SD(I_{i},m_{i})\\), where \\(\\hat{I}_{i}\\) is the completed object, which is illustrated in Fig. 4. For better completion, we input a text prompt _"a complete 3D model"_ to SD when inpainting. After that, each inpainted object \\(\\hat{I}_{i}\\) can be reconstructed by image-to-3D methods to produce single 3D models.\n' +
      '\n' +
      '### Multi-Object Combination\n' +
      '\n' +
      'At this stage, we seek to combine separate 3D models by optimizing their scale, rotation, and translation parameters \\(\\{s_{i},r_{i},t_{i}\\}\\), such that they align with the input image \\(I\\) and semantic spatial relationships. We begin by initializing each object\'s scale, rotation, and translation based on \\(I\\), and then refine them using the proposed spatially-aware diffusion priors and guidance from the reference image. We will first introduce a spatially-aware diffusion distillation scheme, followed by a discussion on its application for automatic object combinations.\n' +
      '\n' +
      '**Spatially-Aware Diffusion Guidance.** DreamFusion [36] presents a method that optimizes 3D representations from textual descriptions, by employing a pre-trained 2D diffusion model. The subject is represented as a differentiable parameterization [31], where a differentiable MLP renderer \\(g\\) renders 2D images \\(x=g(\\theta)\\) from a neural radiance field parameterized as \\(\\theta\\). It leverages a diffusion model \\(\\phi\\) to provide a score function \\(\\hat{\\epsilon}_{\\phi}(x_{t};y,t)\\), which predicts the sampled noise \\(\\epsilon\\) given the noisy image \\(x_{t}\\), text-embedding \\(y\\), and noise level \\(t\\). This score function guides the direction of the gradient for updating the neural parameters \\(\\theta\\), and the gradient is calculated by Score Distillation Sampling (SDS):\n' +
      '\n' +
      '\\[\\bigtriangledown_{\\theta}\\mathcal{L}_{\\text{SDS}}(\\phi,x)=\\mathbb{E}_{t, \\epsilon}\\left[w(t)(\\hat{\\epsilon}_{\\phi}(x_{t};y,t)-\\epsilon)\\frac{\\partial x }{\\partial\\theta}\\right], \\tag{4}\\]\n' +
      '\n' +
      'while \\(w(t)\\) is a weighting function.\n' +
      '\n' +
      'However, we find that SDS is unstable for position adjustment in our case. We use a text prompt _"a squirrel is sitting on a box"_ and an image of a squirrel and a box as a toy example, and aim to test the ability of SDS to adjust the position of the image elements according to the text prompt. As shown in Fig. 5, SDS does not produce the correct placement, as the image content (squirrel and box) already matches the prompt and SDS does not push the adjustment of position. We thus propose spatially-aware SDS to emphasize the position adjustment when calculating the gradient.\n' +
      '\n' +
      'Recall that in SDS, for each text embedding \\(y\\) and time step \\(t\\), we use a UNet to predict the noise \\(\\hat{\\epsilon}_{\\phi}(x_{t};y,t)\\). The features of noisy image \\(\\phi(x_{t})\\) are projected to a query matrix \\(Q=\\mathcal{F}_{Q}(\\phi(x_{t}))\\), and the textual embedding is projected to a key matrix \\(K=\\mathcal{F}_{K}(y)\\) and a value matrix \\(V=\\mathcal{F}_{V}(y)\\), via the learned linear projections \\(\\mathcal{F}_{Q}\\), \\(\\mathcal{F}_{K}\\) and \\(\\mathcal{F}_{V}\\). The attention maps are then calculated by:\n' +
      '\n' +
      '\\[M=\\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d}}\\right), \\tag{5}\\]\n' +
      '\n' +
      'where the \\(M_{j}\\) indicates the attention map of \\(j\\)-th token, and d is the latent projection dimension of the keys and queries.\n' +
      '\n' +
      'To prioritize the refinement of spatial layout, we strengthen the key token that describes the spatial relationship. The key token can be the word describing spatial relationships, such as "front", "on," and "below," or the word describing object interaction, such as "riding" and "holding", which can be extracted by LLMs or indicated by the user. For example, consider the prompt _"a squirrel is sitting on a box"_, we want to strengthen the effect of the word _"sitting on"_, which describes the relationship between the squirrel and paper box. To achieve this spatially-aware optimization, we scale the attention maps of the assigned tokens \\(j^{\\star}\\) with a constant c (\\(c>1\\)), similar to [14], resulting in a stronger focus on the spatial relationship. The rest of the attention maps remain unchanged:\n' +
      '\n' +
      '\\[M:=\\begin{cases}c\\cdot M_{j}&\\quad\\text{if}\\quad j=j^{\\star}\\\\ M_{j}&\\quad\\text{otherwise.}\\end{cases} \\tag{6}\\]\n' +
      '\n' +
      'The spatially-aware SDS loss (SSDS) can be formulated as:\n' +
      '\n' +
      '\\[\\bigtriangledown_{\\theta}\\mathcal{L}_{\\text{SSDS}}(\\phi^{\\star},x)=\\mathbb{E} _{t,\\epsilon}\\left[w(t)(\\hat{\\epsilon}_{\\phi^{\\star}}(x_{t};y,t)-\\epsilon) \\frac{\\partial x}{\\partial\\theta}\\right], \\tag{7}\\]where the \\(\\hat{\\epsilon}_{\\phi^{*}}(x_{t};y,t)\\) is the predicted noise calculated with the strengthened attention maps which focus on the spatial words. For timesteps, we sample \\(t\\) from a range with high noise levels, as these steps have a bigger impact on the spatial layout of a generated image.\n' +
      '\n' +
      '**Combine the objects.** We begin with a coarse initialization of scale, rotation and translation \\(\\{s_{i},r_{i},t_{i}\\}\\) from the bounding box \\(b_{i}\\) and estimated depth \\(d_{i}\\). Specifically, the scale \\(s_{i}\\) is decided by the ratio of the bounding box size and image size:\n' +
      '\n' +
      '\\[s_{i}=max\\left\\{\\frac{W_{b_{i}}}{W_{I}},\\frac{H_{b_{i}}}{H_{I}}\\right\\}, \\tag{8}\\]\n' +
      '\n' +
      'where the \\(W_{b_{i}}\\), \\(H_{b_{i}}\\), \\(W_{I}\\), \\(H_{I}\\) are the width and height of bounding box \\(b_{i}\\) and input image \\(I\\) respectively. As for translation \\(t_{i}\\), we use a monocular depth prediction model [39] to estimate the average depth \\(d_{i}\\) for \\(i\\)-th object, which is set as z-dimension in \\(t_{i}\\), and the x and y dimensions are initialized by the center coordinates of bounding box \\(b_{i}\\) and image size. That is:\n' +
      '\n' +
      '\\[\\begin{array}{l}t_{i}=(X_{b_{i}}-\\frac{W_{I}}{2},Y_{b_{i}}-\\frac{H_{I}}{2},d _{i}),\\\\ d_{i}=\\text{Average}(\\text{Depth}(O_{i})),\\end{array} \\tag{9}\\]\n' +
      '\n' +
      'where \\(X_{b_{i}}\\), \\(Y_{b_{i}}\\) are the center coordinates of bounding box \\(b_{i}\\), \\(d_{i}\\) is the average depth of each pixel that lie in \\(i\\)-th object \\(O_{i}\\). The rotation angles in three dimensions \\(r_{i}\\) are initialized as \\((0,0,0)\\).\n' +
      '\n' +
      'However, due to depth-size ambiguity in a single-view image, the predicted depth can be inaccurate, leading to an unreasonable initialization of depth and size. To alleviate the single-view ambiguity, we refine spatial parameters \\(\\{s_{i},r_{i},t_{i}\\}\\) with the proposed spatially-aware SDS loss (SSDS) as novel-view supervision. To stabilize the optimization, we also constrain the reconstruction error between reference-view rendering \\(\\hat{I}\\) in and input image \\(I\\):\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{Ref}}=\\lambda_{\\text{RGB}}\\left|\\hat{I}_{\\text{RGB}}-I_{ \\text{RGB}}\\right|+\\lambda_{\\text{A}}\\left|\\hat{I}_{\\text{A}}-I_{\\text{A}} \\right|, \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\lambda_{\\text{RGB}}\\) and \\(\\lambda_{\\text{A}}\\) are weights for RGB and alpha channels. The total loss is a weighted summation of \\(\\mathcal{L}_{\\text{Ref}}\\) and \\(\\mathcal{L}_{\\text{SSDS}}\\).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'We set the guidance scale to 7.5 and the number of inference steps to 30 when inpainting the image with Stable Diffusion. We use Pytorch3D [40] as our differentiable rendering engine. We randomly sample timestep \\(t\\) between 800 and 900. We use Adam as our optimizer, and the learning rate of translation on the z dimension is 0.01, while the others are set to 0.001. The loss weight \\(\\lambda_{\\text{Ref}}\\), \\(\\lambda_{\\text{SSDS}}\\), \\(\\lambda_{\\text{RGB}}\\), \\(\\lambda_{\\text{A}}\\), are set to 1, 1, 1,000, 1,000 respectively. We set the multiplier \\(c\\) for attention map to 25. We downsample each separate mesh to 50,000 faces in the multi-object combination stage and render 10 views for each iteration.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**Benchmark.** To evaluate our method, we built a test benchmark containing 100 images covering a variety of complex 3D assets. The benchmark includes 50 images generated with stable diffusion, and 50 images constructed from real images with PhotoShop. Each image has a foreground mask, a set of bounding boxes for objects, and a text caption. We use GPT4 to propose text prompts and spatial tokens, followed by manual filtering. We will make this benchmark publicly available.\n' +
      '\n' +
      '**Comparison Methods.** We compare our method with three state-of-the-art single-image reconstruction methods: 1) SyncDreamer [26], which we implement using the official code. The textured meshes are derived from the NeuS [50] representation. 2) LRM [17], which we implement using the publicly available code [13]. 3) Wonder3D [28], which we implement using the official code and also use as our base model for image-to-3D reconstruction.\n' +
      '\n' +
      '**Qualitative Comparison.** As shown in Fig. 6, our method can accurately reconstruct each object and preserve good spatial relationships among them. Other methods often struggle to generate high-quality geometry and texture for\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & CLIP-Score \\(\\uparrow\\) & GPT-3DScore \\(\\uparrow\\) \\\\ \\hline SyncDreamer [26] & 81.47\\% & 13.54\\% \\\\ OpenLRM [17] & 83.65\\% & 53.12\\% \\\\ Wonder3D [28] & 85.57\\% & 56.25\\% \\\\ Ours & **86.58**\\% & **65.63\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Quantitative comparison.**\n' +
      '\n' +
      'Figure 6: **Visual comparison for image-to-3D generation.** Given an input image, previous methods reconstruct inaccurate geometry and blurry texture, especially in novel views. Our method produces higher-fidelity 3D models with the proposed compositional generation scheme.\n' +
      '\n' +
      'Figure 7: Qualitative results. _ComboVerse_ can generate high-quality 3D models from a single image that contains multiple objects.\n' +
      '\n' +
      'small objects in the input image, and the junction parts of different objects often blend. More qualitative results are shown in Fig. 7.\n' +
      '\n' +
      '**Quantitative Comparison.** We use CLIP-Score [38] to measure semantic similarities between novel-view images and the reference image. We also involve GPT-based evaluation following [56]. We conduct pair-wise comparisons for each method across all samples, and report the probability of success for each method. Table 1 shows that our method outperforms comparison methods in both semantic similarity and GPT evaluation.\n' +
      '\n' +
      '**User Study.** Besides numerical metrics, we also perform a user study to compare our method with others. We collect 990 replies from 22 human users. Participants are shown a reference image and a random pair of 3D models (ours and baselines) at once and are asked to select a more realistic one in terms of both geometry and texture quality. All choices are given in a shuffled order without time limitation. Fig. 8 illustrates that our method outperforms previous approaches in terms of human preference.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Effectiveness of Object Inpainting.** We performed a study to examine designs in object completion. We first investigate the effect of noisy backgrounds on object completion. The inpainted results without noise background had black\n' +
      '\n' +
      'Figure 8: **User study. Our method consistently outperforms competitors in terms of human evaluation.**\n' +
      '\n' +
      'Figure 9: **Analysis for objects inpainting. We produce compelling inpainting results with random background (EQ. 2), bounding-aware mask proposal (EQ. 3), and text prompting.**\n' +
      '\n' +
      'borders and were not complete, as shown in Fig. 9. Then, we simply used the background mask for inpainting instead of a bounding-aware mask, and the inpainted results had extra parts, which caused inconsistency in the 3D reconstruction with the input image. We also removed the text prompt "a complete 3D model" for diffusion models during inpainting, and the results also degraded.\n' +
      '\n' +
      '**Effectiveness of Spatially-Aware Diffusion Guidance.** As shown in Fig. 10, we use an example of "an astronaut is riding a red horse" to analyze different guidance settings in object combination. _Base_ only enforces reconstruction loss in the reference view without additional guidance in novel views, and thus yields incorrect relative depth between the astronaut and the horse. With a standard _SDS loss_ or _depth loss_ from a depth prediction model as spatial guidance, the interaction between the astronaut and the horse improves, but it was still far from accurate. By strengthening the attention to the word "riding" with the proposed _SSDS loss (full)_, the full model achieves the best result. This confirms the improved spatial control capability of the proposed method over the standard SDS. As discussed in Sec. 3.3, we sampled from a high noise range ([800, 900]) for Stable Diffusion when performing SSDS, as these steps have a bigger impact on the spatial layout of a generated image. We also experiment with SSDS with different sample ranges of noise timesteps, _low noise range_ ([100, 200]), and _uniform noise range_ ([20, 980]) and observe a performance drop. We also give quantitative ablation results in _supplementary materials_..\n' +
      '\n' +
      '### Application in Scene Reconstruction\n' +
      '\n' +
      'Besides the generation of 3D assets with two objects, we also validate the generalization ability of the proposed method to multiple objects. As illustrated in Fig. 11, we use the proposed method to reconstruct 3D scenes consisting of multiple (\\(>2\\)) objects. Previous methods that work on the object level have difficulties in generating scenes and produce obvious artifacts in both geometry and texture. Also, for small objects such as the dog in the last example, existing methods tend to ignore it due to the training bias mentioned before. In contrast, our method achieves realistic and high-quality reconstruction.\n' +
      '\n' +
      'Figure 10: **Analysis for objects combination.** Compared with standard SDS and depth constrain, SSDS provides stronger guidance on object positioning.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'In this paper, we present _ComboVerse_, a novel method for creating high-quality compositional 3D assets from a single image. With an in-depth analysis of the "multi-object gap", we build on object-level 3D generative models and extend them to deal with more complex objects. With reconstructed 3D models of various objects, we seek to adjust their sizes, rotation angles, and locations to create a 3D asset that matches the given image. To this end, we proposed spatially-aware score distillation sampling from pretrained diffusion models to guide the placement of objects. Our approach can be a valuable step for complex 3D object reconstruction and pave the way for future 3D scene generation.\n' +
      '\n' +
      '**Limitations.** The proposed method performs well for assets consisting of two or a few (usually \\(<5\\)) objects. However, similar to existing text-based works, our\n' +
      '\n' +
      'Figure 11: **Comparison of scene reconstruction.** We show some challenging cases that contain more than two objects. The first example involves four objects: _a car, a dinosaur, a tree, and a cone_. The second example involves three examples: _two robots and a ball_. The third example involves three examples: _a house, a dog, and a tree_. Our method achieves compelling reconstruction quality with the compositional scheme.\n' +
      '\n' +
      'method still faces challenges in creating very complex scenes with more objects. Another limitation of our approach is the lack of optimization for the geometry and texture in the combination process. Thus, the quality of the final results relies on the performance of the image-to-3D method that we use as backbone. We expect that our methods can be further enhanced with more robust backbone methods in the future.\n' +
      '\n' +
      '## Appendix 0.A Quantitative Ablation Analysis\n' +
      '\n' +
      'To evaluate the effectiveness of the proposed SSDS, we performed an ablation analysis and have shown qualitative results in Fig. 10 in the main paper. Beyond visual comparison, we also provide quantitative ablation analysis in table 2. Two CLIP models, including CLIP B/16 and ResNet50 backbones, are utilized to examine different guidance configurations in object combinations. To disentangle geometry and appearance quality, we evaluate multi-view CLIP similarities for both colored rendering and untextured geometry rendering. _Base_ only imposes reconstruction loss in the reference view, lacking additional guidance in novel views. Applying either a standard _SDS_ loss or _depth_ loss from a depth prediction model as spatial guidance yielded sub-optimal CLIP scores for color and geometry. However, by strengthening the attention to spatial layout through the proposed _SSDS loss (full)_, the full model achieves the best result, confirming its enhanced spatial control over standard SDS. As discussed in Sec. 3.3, high noise intervals ([800, 900]) were selected for Stable Diffusion during SSDS due to their bigger impact on the spatial layout of a generated image. We also experiment with SSDS with different sample ranges of noise timesteps, _low noise range_ ([100, 200]), and _uniform noise range_ ([20, 980]) and observe a performance drop.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{4}{c}{CLIP Score\\(\\uparrow\\)} \\\\ Guidance & \\multicolumn{2}{c}{CLIP B/16} & \\multicolumn{2}{c}{ResNet50} \\\\  & Color & Geometry & Color & Geometry \\\\ \\hline Base (without guidance) & 86.62\\% & 75.24\\% & 80.35\\% & 74.19\\% \\\\ Depth Loss & 84.57\\% & 78.42\\% & 81.69\\% & 75.83\\% \\\\ SDS & 84.16\\% & 78.25\\% & 84.08\\% & 74.66\\% \\\\ SSDS (uniform noise range) & 85.33\\% & 78.49\\% & 85.55\\% & 75.85\\% \\\\ SSDS (low noise range) & 84.86\\% & 79.03\\% & 84.42\\% & 75.44\\% \\\\ SSDS (full) & **89.01**\\% & **79.66**\\% & **86.60**\\% & **78.10**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Quantitative analysis for ablation study.\n' +
      '\n' +
      'More Results of "Multi-Object Gap"\n' +
      '\n' +
      'As discussed in Sec.3.1, the current feed-forward models, mainly trained on Obfjaverse, exhibit limitations in generalizing to multi-object scenarios. Fig. 12 uses TripoSR, a state-of-the-art method in image-to-3D reconstruction, as another case to demonstrate this limitation. Despite its advancements, TripoSR still exhibits three typical failure modes when tasked with generating multiple objects, stemming from inherent data and model biases. The detailed analysis was illustrated in Sec. 3.1.\n' +
      '\n' +
      'Figure 12: “Multi-object gap” of models trained on Obfjaverse. (a) Camera Setting Bias. The reconstruction quality for small and non-centered objects will significantly downgrade compared to separate reconstruction. (b) Occlusion. The reconstruction results tend to blend when an object is occluded by another. (c) Leaking Pattern. The shape and texture of an object will be influenced by other objects in the input image.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: CVPR (2022)\n' +
      '* [2] Chan, E.R., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5799-5809 (2021)\n' +
      '* [3] Chaudhuri, S., Kalogerakis, E., Giguere, S., Funkhouser, T.: Attribit: content creation with semantic attributes. In: Proceedings of the 26th annual ACM symposium on User interface software and technology. pp. 193-202 (2013)\n' +
      '* [4] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3d content creation. [https://arxiv.org/abs/2303.13873](https://arxiv.org/abs/2303.13873) (2023)\n' +
      '* [5] Cheng, Y.C., Lee, H.Y., Tulyakov, S., Schwing, A., Gui, L.: SDFusion: Multimodal 3d shape completion, reconstruction, and generation. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [6] Chou, G., Bahat, Y., Heide, F.: Diffusion-SDF: Conditional generative modeling of signed distance functions. [https://arxiv.org/abs/2211.13757](https://arxiv.org/abs/2211.13757) (2023)\n' +
      '* [7] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. [https://arxiv.org/abs/2212.08051](https://arxiv.org/abs/2212.08051) (2022)\n' +
      '* [8] Epstein, D., Poole, B., Mildenhall, B., Efros, A.A., Holynski, A.: Disentangled 3d scene generation with layout learning. arXiv preprint arXiv:2402.16936 (2024)\n' +
      '* [9] Funkhouser, T., Kazhdan, M., Shilane, P., Min, P., Kiefer, W., Tal, A., Rusinkiewicz, S., Dobkin, D.: Modeling by example. ACM transactions on graphics (TOG) **23**(3), 652-663 (2004)\n' +
      '* [10] Gao, G., Liu, W., Chen, A., Geiger, A., Scholkopf, B.: Graphdreamer: Compositional 3d scene synthesis from scene graphs. arXiv preprint arXiv:2312.00093 (2023)\n' +
      '* [11] Gu, J., Liu, L., Wang, P., Theobalt, C.: Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. In: ICLR (2022)\n' +
      '* [12] Gupta, A., Xiong, W., Nie, Y., Jones, I., Oguz, B.: 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371 (2023)\n' +
      '* [13] He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM](https://github.com/3DTopia/OpenLRM) (2023)\n' +
      '* [14] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)\n' +
      '* [15] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [16] Hong, F., Tang, J., Cao, Z., Shi, M., Wu, T., Chen, Z., Wang, T., Pan, L., Lin, D., Liu, Z.: 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234 (2024)\n' +
      '* [17] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [18] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. [https://arxiv.org/abs/2305.02463](https://arxiv.org/abs/2305.02463) (2023)\n' +
      '* [* [19] Kalogerakis, E., Chaudhuri, S., Koller, D., Koltun, V.: A probabilistic model for component-based shape synthesis. Acm Transactions on Graphics (TOG) **31**(4), 1-11 (2012)\n' +
      '* [20] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023)\n' +
      '* [21] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model (2023)\n' +
      '* [22] Li, W., Chen, R., Chen, X., Tan, P.: SweetDreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. [https://arxiv.org/abs/2310.02596](https://arxiv.org/abs/2310.02596) (2023)\n' +
      '* [23] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3D: High-resolution text-to-3d content creation. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [24] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [25] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. [https://arxiv.org/abs/2303.11328](https://arxiv.org/abs/2303.11328) (2023)\n' +
      '* [26] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: SyncDreamer: Generating multiview-consistent images from a single-view image. [https://arxiv.org/abs/2309.03453](https://arxiv.org/abs/2309.03453) (2023)\n' +
      '* [27] Liu, Z., Feng, Y., Black, M.J., Nowrouzezahrai, D., Paull, L., Liu, W.: Meshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133 (2023)\n' +
      '* [28] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [29] Melas-Kyriazi, L., Rupprecht, C., Laina, I., Vedaldi, A.: RealFusion: 360 reconstruction of any object from a single image. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [30] Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf for shape-guided generation of 3d shapes and textures. [https://arxiv.org/abs/2211.07600](https://arxiv.org/abs/2211.07600) (2022)\n' +
      '* [31] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [32] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-E: A system for generating 3d point clouds from complex prompts. [https://arxiv.org/abs/2212.08751](https://arxiv.org/abs/2212.08751) (2023)\n' +
      '* [33] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11453-11464 (2021)\n' +
      '* [34] Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J.J., Kemelmacher-Shlizerman, I.: Stylesdf: High-resolution 3d-consistent image and geometry generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13503-13513 (2022)\n' +
      '* [35] Po, R., Wetzstein, G.: Compositional 3d scene generation using locally conditioned diffusion. arXiv preprint arXiv:2303.12218 (2023)* [36] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: DreamFusion: Text-to-3d using 2d diffusion. In: International Conference on Learning Representations (ICLR) (2023)\n' +
      '* [37] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., Ghanem, B.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. [https://arxiv.org/abs/2306.17843](https://arxiv.org/abs/2306.17843) (2023)\n' +
      '* [38] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [39] Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. ICCV (2021)\n' +
      '* [40] Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari, G.: Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501 (2020)\n' +
      '* [41] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [42] Schulz, A., Shamir, A., Levin, D.I.W., Sitthi-Amorn, P., Matusik, W.: Design and fabrication by example. ACM Transactions on Graphics (Proceedings SIGGRAPH 2014) **33**(4) (2014)\n' +
      '* [43] Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. [https://arxiv.org/abs/2310.16818](https://arxiv.org/abs/2310.16818) (2023)\n' +
      '* [44] Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054 (2024)\n' +
      '* [45] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation (2023)\n' +
      '* [46] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. In: International Conference on Computer Vision ICCV (2023)\n' +
      '* [47] Tang, S., Chen, J., Wang, D., Tang, C., Zhang, F., Fan, Y., Chandra, V., Furukawa, Y., Ranjan, R.: Mvdiffusion++: A dense high-resolution multi-view diffusion model for single to sparse-view 3d objectreconstruction. arXiv preprint arXiv:2402.12712 (2024)\n' +
      '* [48] Tertikas, K., Despoina, P., Pan, B., Park, J.J., Uy, M.A., Emiris, I., Avrithis, Y., Guibas, L.: Partnerf: Generating part-aware editable 3d shapes without 3d supervision. arXiv preprint arXiv:2303.09554 (2023)\n' +
      '* [49] Vilesov, A., Chari, P., Kadambi, A.: Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907 (2023)\n' +
      '* [50] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Advances in Neural Information Processing Systems **34**, 27171-27183 (2021)\n' +
      '* [51] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [52] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., Guo, B.: Rodin: A generative model for sculpting 3d digital avatars using diffusion. [https://arxiv.org/abs/2212.06135](https://arxiv.org/abs/2212.06135) (2023)\n' +
      '* [* [53] Wang, Z., Li, M., Chen, C.: Luciddreaming: Controllable object-centric 3d generation. arXiv preprint arXiv:2312.00588 (2023)\n' +
      '* [54] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. [https://arxiv.org/abs/2305.16213](https://arxiv.org/abs/2305.16213) (2023)\n' +
      '* [55] Willis, K.D., Jayaraman, P.K., Chu, H., Tian, Y., Li, Y., Grandi, D., Sanghi, A., Tran, L., Lambourne, J.G., Solar-Lezama, A., et al.: Joinable: Learning bottom-up assembly of parametric cad joints. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15849-15860 (2022)\n' +
      '* [56] Wu, T., Yang, G., Li, Z., Zhang, K., Liu, Z., Guibas, L., Lin, D., Wetzstein, G.: Gpt-4v(sion) is a human-aligned evaluator for text-to-3d generation. arXiv preprint arXiv:2401.04092 (2023)\n' +
      '* [57] Xiang, J., Yang, J., Deng, Y., Tong, X.: Gram-hd: 3d-consistent image generation at high resolution with generative radiance manifolds. arXiv preprint arXiv:2206.07255 (2022)\n' +
      '* [58] Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: NeuralLift-360: Lifting an in-the-wild 2d photo to a 3d object with 360 views. [https://arxiv.org/abs/2211.16431](https://arxiv.org/abs/2211.16431) (2023)\n' +
      '* [59] Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: GaussianDreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. [https://arxiv.org/abs/2310.08529](https://arxiv.org/abs/2310.08529) (2023)\n' +
      '* [60] Yichun, S., Peng, W., Jianglong, Y., Long, M., Kejie, L., Xiao, Y.: MVDream: Multi-view diffusion for 3d generation. [https://arxiv.org/abs/2308.16512](https://arxiv.org/abs/2308.16512) (2023)\n' +
      '* [61] Zhao, Z., Liu, W., Chen, X., Zeng, X., Wang, R., Cheng, P., FU, B., Chen, T., YU, G., Gao, S.: Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>