<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '3D 인식 지식을 전달하는 손실[36] 보다 최근에, 대안적인 접근법은 Objaverse [7]과 같은 대규모 3D 객체 데이터 세트에 의해 촉진되는 빠른 생성을 위한 피드-포워드 3D 확산 모델을 훈련하는 데 중점을 둔다. 일단 훈련되면, 이 모델들은 1분 내에 단일 전방 추론을 통해 서명된 거리 필드[5], 포인트[32], 복사 필드[18; 52], 메시[27], 또는 멀티뷰 이미지[25]를 생성할 수 있다.\n' +
      '\n' +
      '단순 객체 생성에 대한 강력한 결과에도 불구하고 이러한 피드포워드 방법은 일반적으로 여러 객체가 있는 장면 및 복잡한 폐색과 같은 더 복잡한 데이터에 적용할 때 어려움을 겪는다. 도. 도 2는 이러한 결합 객체들을 다룰 때 기존 모델들의 단점들을 예시한다. 그러나 각 객체를 개별적으로 생성할 때 이러한 모델이 잘 수행되는 것을 관찰했다. 우리는 이 "다중 객체 간격"에 대한 심층 분석을 수행하고 이 간격이 훈련 데이터의 편향, 즉_ Obja-verse에서 비롯된다는 추측을 수행한다. 다중 객체를 포함하는 3D 자산의 부족은 훈련된 모델이 훈련 데이터 분포를 넘어 합성물을 관리하는 것을 어렵게 만든다.\n' +
      '\n' +
      '위에서 이루어진 관찰들을 고려할 때, 다수의 객체들을 포함하는 3D 콘텐츠를 생성할 수 있는 생성 시스템을 설계하는 것이 가능할까? 전형적으로, 숙련된 인간 예술가들은 각각의 객체를 전체적으로 통합하기 전에 개별적으로 생성한다. 이는 각 객체를 개별적으로 생성한 후 자동으로 결합하여 합성물을 생성하는 _ComboVerse_라는 합성 생성 패러다임을 제시하도록 동기를 부여했다. 제안된 패러다임의 주요 장점은 다중 객체 및 폐색을 포함하는 복잡한 자산을 효과적으로 관리할 수 있다는 것이다.\n' +
      '\n' +
      '우리의 접근법은 단일 객체 재구성 및 다중 객체 조합의 두 단계로 구성된다. 우리는 먼저 폐색 제거 모듈과 이미지 대 3D를 사용하여 이미지 내에서 각 객체를 독립적으로 분해하고 재구성한다.\n' +
      '\n' +
      '도 1: _ComboVerse_는 종이 박스에 앉아 있는 다람쥐와 같은 여러 객체를 포함하는 단일 이미지로부터 고품질 3D 모델을 생성할 수 있다. 제작된 3D 콘텐츠의 텍스처 메쉬를 보여 놀라운 재구성 품질을 보여줍니다.\n' +
      '\n' +
      '모델. 두 번째 단계에서는 생성된 3차원 객체를 객체 크기, 배치, 폐색 등 다양한 요소를 고려하여 하나의 모델로 자동 결합하는 것을 목표로 한다. 그러나, 이러한 프로세스는 입력 이미지에서 깊이-크기 모호성으로 인해 문제를 제기하여 부정확한 구도를 초래한다.\n' +
      '\n' +
      '이 문제를 해결하기 위해, 우리는 객체 포지셔닝을 위한 공간 안내로서 미리 훈련된 확산 모델을 선택한다. 모양과 질감을 처음부터 모두 최적화해야 하는 기존의 SDS 기반 방법[46, 23, 36, 4]과 달리, 우리는 개별 객체의 3D 모델을 고정하고 최적화 프로세스가 훨씬 더 빨라지도록 합리적인 공간 레이아웃을 달성하는 데에만 초점을 맞춘다. 그러나, 우리는 표준 SDS가 주어진 텍스트 프롬프트와 일치하도록 위치보다 콘텐츠를 우선시하는 경향이 있기 때문에 객체를 정확하게 배치하기에 불충분하다는 것을 발견했다(도 5 참조). 이 문제를 해결하기 위해 객체 간의 공간 관계에 더 중점을 두는 공간 인식 SDS 손실을 소개한다. 구체적으로, 우리는 점수 증류에 대한 공간적 관계를 나타내는 위치 토큰의 주의 지도를 재가중치[14]한다. 제안된 손실은 위치 인식을 우선시함으로써 객체 배치를 위한 잘 훈련된 확산 모델로부터 공간 지식을 효과적으로 증류할 수 있다.\n' +
      '\n' +
      '제안하는 방법을 평가하기 위해 다양한 범위의 복잡한 장면을 포함하는 100개의 이미지로 구성된 벤치마크를 수집한다. 이 벤치마크에서 _ComboVerse_를 평가하며, 광범위한 실험은 여러 객체 처리, 오클루전 및 카메라 설정 측면에서 이전 방법에 비해 명확한 개선을 보여준다. 우리의 주요 기여는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 이미지로부터 합성 3D 자산을 생성하기 위해 객체 레벨 3D 생성 모델을 확장하는 자동 파이프라인인 _ComboVerse_를 제안한다.\n' +
      '* 우리는 모델과 데이터 관점 모두에서 기존 피드포워드 모델의 "다중 객체 갭"에 대한 심층 분석을 수행한다.\n' +
      '* 공간 인식 확산 안내를 제안하여, 객체 배치를 위한 공간 레이아웃에 대한 안내를 제공하기 위해 미리 훈련된 이미지 확산 모델이 가능하게 한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**3D Diffusion Prior.** 많은 방법들은 3D 안내의 소스로서 미리 훈련된 2D 확산 모델들[41, 15]을 선택한다. 초기 연구 [36]은 텍스트 조건 3D 콘텐츠 생성을 위해 2D 확산의 상상력을 활용하기 위한 점수 증류 샘플링 방법을 제안했다. 이후의 연구는 2단계 최적화[4, 23, 30], 더 나은 점수 증류[54], 더 강한 기초 확산 모델[22, 60]을 사용하여 품질을 개선했다. 다른 작업[29, 37, 43, 51, 58, 46]은 단일 이미지로부터 3D 모델을 생성하기 위한 접근법을 확장한다. 일부 작업[59, 45]은 암시적 표현을 3D 가우스 스플래팅으로 대체한다. 비록 결과가 유망하지만, 이러한 방식으로 3D 모델을 만드는 것은 몇 분에서 몇 시간의 최적화가 걸릴 수 있다.\n' +
      '\n' +
      '**피드-포워드 3D 생성 모델.** 빠른 생성을 위해 피드-포워드 모델을 훈련시킨 또 다른 접근법 라인은 케이스당 최적화의 필요성을 제거한다. 3D 인식 생성적 적대 네트워크[1, 2, 11, 33, 34, 57]는 초기 연구에 상당한 연구 관심을 얻었다. 이후, 이미지-조건화 및 텍스트-조건화 3D 생성을 위한 확산 모델을 활용하려는 많은 시도가 있었다. 일단 훈련되면, 그들은 최적화 없이 서명된 거리 필드[5, 6], 포인트[32], 복사 필드[16, 18, 52, 61, 12], 메시[27], 또는 멀티뷰 이미지[24, 25, 26, 28, 47]를 생성할 수 있다. 확산 모델 외에도 최근 연구는 트랜스포머 아키텍처[17, 21] 또는 UNet 아키텍처[44]를 사용하여 피드포워드 3D 재구성을 탐구한다. 빠른 생성에도 불구하고 이러한 방법은 훈련 데이터에 의해 제한되어 복잡한 3D 자산을 재구성하는 능력을 제한한다. 우리는 이러한 객체 수준의 생성 모델을 구축하고 더 복잡한 객체 또는 장면을 처리할 수 있도록 확장하는 것을 목표로 한다.\n' +
      '\n' +
      '**구성 3D 생성.** 이전 연구 [33]은 3D 인식 이미지 생성을 목적으로 적대적 학습 프레임워크에서 합성 신경 복사 필드의 사용을 조사했다. 추가 연구는 3D 부품을 3D 모델로 조립하는 부품 기반 형상 생성 개념을 탐구했다. 정석 작업 [9]는 메쉬 데이터베이스에서 검색하여 관심 있는 부분을 찾고 절단된 부분을 합성하여 새로운 객체를 생성한다. 이후, 부분 제안[19], 의미 속성[3], 제작[42], CAD 조립[55]에 대한 확률 모델을 포함한다. 일부 작업 [48]은 신경 복사 필드를 사용하여 서로 다른 3D 구성요소를 표현한 다음 이러한 부분을 3D 모델로 렌더링합니다. 사전 훈련된 확산 모델들을 안내로서, 최근 작업 [35]는 사용자 주석이 달린 3D 바운딩 박스들 및 텍스트 프롬프트들을 갖는 합성 3D 장면들을 생성한다. 동시 작업은 인간 주석에 대한 대안으로 3D 레이아웃을 제안하기 위해 대형 언어 모델(LLM)을 사용하여 텍스트 프롬프트로부터 3D 장면을 생성하거나[10, 49, 53], 또는 최적화 프로세스[8] 동안 공동으로 레이아웃을 학습한다. 이러한 접근법들은 텍스트 프롬프트들과 매칭되는 3D 장면들을 생성할 수 있지만, 텍스트들은 객체들이 공간에 배열되는 방법을 기술할 때 불분명하고 부정확할 수 있다. 대조적으로, 우리의 접근법은 참조 이미지로부터 복잡한 3D 자산들을 재구성하는 것에 초점을 맞춘다. 이미지는 불분명하거나 모호할 수 있는 텍스트 설명과 달리 객체 간의 공간적 관계를 보다 정확하게 나타내므로 구성 품질의 높은 기준을 필요로 한다.\n' +
      '\n' +
      '## 3 ComboVerse\n' +
      '\n' +
      '이 절에서는 먼저 Objaverse에서 훈련된 최첨단 이미지 대 3D 생성 방법의 "다중 객체 간격"을 분석한 후 구성 생성 방식에 대해 논의한다. 그런 다음 단일 객체 재구성 및 다중 객체 조합이라는 접근법과 관련된 두 단계의 세부 사항을 제시한다. 개요 아키텍처는 그림 3에 나와 있다.\n' +
      '\n' +
      '"Multi-Object Gap"의### 분석\n' +
      '\n' +
      '대부분의 기존 피드포워드 모델은 Objaverse[7]에서 학습된다. 도 1에 도시된 바와 같다. 도 2 및 도 2를 참조하여 설명한다. 도 12를 참조하면, 이러한 방법들은 데이터 및 모델 편향으로 인해 다중 객체 생성에 대한 세 가지 전형적인 실패 사례를 겪는다.\n' +
      '\n' +
      '**카메라 설정 바이어스.** 카메라를 설정할 때, 대부분의 이미지-투-3D 방법은 객체가 정규화된 크기를 가지며 이미지 중앙에 있다고 가정한다.\n' +
      '\n' +
      '그러나, 다수의 객체들을 갖는 시나리오들에서, 객체는 장면의 코너에 나타나거나 이미지에서 매우 작을 수 있으며, 이는 객체-중심 가정들에 부합하지 않는다. 이러한 경우 모델링 품질이 크게 저하될 수 있습니다.\n' +
      '\n' +
      '**Dataset Bias.** Objaverse 데이터 세트는 주로 단일 객체 자산을 특징으로 하며, 이는 복잡한 복합체로 일반화하기 위해 그것에 대해 훈련된 모델에 대한 도전을 제기한다. 또한 Objaverse에서 폐색이 거의 없는 경우 이러한 모델은 폐색된 객체를 처리하는데 어려움을 겪는다. 결과적으로, 생성된 객체들은 종종 폐색 모호성으로 인해 함께 블렌딩된다.\n' +
      '\n' +
      '**누설 패턴.** 기존 방법은 여러 객체를 동시에 생성할 때 누출 문제를 나타내는 경향이 있으며, 여기서 한 객체의 형상 및 외관은 다른 객체에 영향을 미칠 수 있다. 이 문제는 다른 부분이 일치하는 단일 객체를 생성하도록 훈련되기 때문에 모델의 편향에서 비롯될 수 있다. 그러나, 다수의 객체들을 갖는 장면들에서, 상이한 객체들은 상이한 지오메트리 및 텍스처를 가질 수 있다. 만약 그들이 여전히 서로에게 영향을 준다면, 그것은 출혈 패턴으로 이어질 수 있다.\n' +
      '\n' +
      '**동기.** 2. 현재 방법들은 구성 객체를 생성하는데 어려움이 있지만, 이러한 방법들이 각 구성 객체를 재구성하는데 성공적이라는 것을 관찰하였다. 이러한 관찰은 각각의 객체를 개별적으로 생성하고(Sec. 3.2), 이어서 이들을 조합하여 원하는 조성 객체(Sec. 3.3)를 형성할 가능성을 시사한다.\n' +
      '\n' +
      '도 2: Objaverse.**(a) Camera Setting Bias에서 훈련된 모델들의 **“Multi-object gap”. 소형 및 비중심 객체에 대한 재구성 품질은 별도의 재구성에 비해 크게 다운그레이드될 것이다. (b) 폐색. 재구성 결과는 물체가 다른 물체에 의해 가려질 때 혼합되는 경향이 있다. (c) 누출 패턴. 객체의 모양과 질감은 입력 이미지 내의 다른 객체들에 의해 영향을 받을 것이다. 예를 들어, (c)에서 호랑이의 등면은 올빼미의 색을 채택하고, 올빼미의 모양 영향으로 등면이 오목하지 않고 볼록하게 된다.\n' +
      '\n' +
      '### Single-Object Reconstruction\n' +
      '\n' +
      '입력 영상이 주어지면 먼저 각 물체의 2차원 경계 박스(\\{b_{i}\\in\\mathbb{Z}^{4}\\}\\)를 지정하며, 이는 왼쪽 상단과 오른쪽 하단 모서리의 좌표를 나타낸다. 서로 다른 객체들에 대해 주어진 바운딩 박스들 \\(\\{b_{i}\\in\\mathbb{Z}^{4}\\}\\)이 주어지면, 우리는 SAM[20]을 사용하여 각 객체들을 다음과 같이 분할한다:\n' +
      '\n' +
      '\\[O_{i},M_{i}=\\text{SAM}(I,b_{i}), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(O_{i}\\) 및 \\(M_{i}\\)은 \\(i\\)번째 객체의 RGB 채널 및 이진 마스크이다.\n' +
      '\n' +
      '**객체 인페인팅.** 다른 객체에 의해 가려질 수 있는 \\(O_{i}\\)을 완성하기 위해, 우리는 객체 인페인팅에 대해 안정적인 확산(SD; Stable Diffusion)[41]을 활용한다. 그러나 가려진 영역을 식별하기 위해 알려져 있는 마스크가 없다는 문제에 직면해 있다. 이 문제를 해결하기 위해 우리는 물체의 가려진 부분을 완성하기 위한 전략을 설계한다. 첫째, 인페인팅 시 객체 주변에 흰색 또는 검은색 테두리를 생성하지 않기 위해 이미지\\(O_{i}\\)의 배경을 랜덤 노이즈로 대체하고 잡음 이미지\\(I_{i}\\)을 다음과 같이 생성한다.\n' +
      '\n' +
      '\\[I_{i}=O_{i}+noise*(\\sim M_{i}), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(\\sim M_{i}\\)은 \\(O_{i}\\)의 배경 영역이다. 노이즈된 이미지 \\(I_{i}\\)는 그림 4에 설명되어 있다. 둘째, 배경 영역과 바운딩 박스 \\(b_{i}\\)을 결합하여 각 객체에 대한 인페인팅 마스크 \\(m_{i}\\)을 생성하며, 이는 각 객체에 대한 인페인팅 영역을 나타낸다. 구체적으로는, 각 인페인팅 마스크 \\(m_{i}\\)에 대해, 화소가,\n' +
      '\n' +
      '그림 3: **우리 방법의 개요. 다중 객체를 포함하는 입력 영상이 주어지면, 본 논문에서 제안하는 방법은 2단계 과정을 통해 고품질의 3차원 자산을 생성할 수 있다. 단일 객체 재구성 단계에서는 객체 인페인팅으로 영상 내 모든 객체를 분해하고, 단일 영상 재구성을 수행하여 개별 3D 모델을 생성한다. 다중 객체 조합 단계에서는 각 객체의 크기, 회전, 병진 파라미터 \\(\\{s_{i},r_{i},t_{i}\\}\\})을 최적화하면서 각 객체의 기하와 질감을 유지한다. 이 최적화 과정은 새로운 뷰에서 계산된 공간 인식 SDS 손실\\(\\mathcal{L}_{\\text{SSDS}}\\)에 의해 유도되며, 어텐션 맵 가중치를 향상시켜 공간 토큰을 강조한다. 예를 들어, 2D 확산 모델에 주어진 prompt _“A fox lying on a toolbox.”_를 고려하여, 우리는 그것의 어텐션 맵에 상수 \\(c\\)(\\(c>1\\))를 곱하여 공간 토큰 "lying"을 강조한다. 또한, 추가적인 제약 조건을 위해 참조 뷰 상에서 계산된 기준 손실\\(\\mathcal{L}_{\\text{Ref}}\\)을 이용한다.**lie는 바운딩 박스 내에 있지만 전경 객체 외부에 있는 것은 1로 설정되고, 다른 것들은 0으로 설정된다. 즉:\n' +
      '\n' +
      '\\[m_{i}=(\\sim M_{i})\\cap b_{i}, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\sim M_{i}\\)은 \\(i\\)번째 물체 \\(O_{i}\\)의 배경 영역이고, \\(b_{i}\\)은 그것의 바운딩 박스를 나타낸다. 마지막으로 경계 박스 정보를 포함하는 \\(m_{i}\\)과 \\(m_{i}\\)을 SD에 입력하여 경계 박스 내부에 있는 \\(b_{i}\\): \\(\\hat{I}_{i}=SD(I_{i},m_{i})\\)를 완성한다. 여기서 \\(\\hat{I}_{i}\\)은 그림 4에 예시된 완성된 객체이다. 더 나은 완료를 위해, 인페인팅 시 텍스트 프롬프트 _"완전 3D 모델"_를 SD에 입력한다. 그 후, 각 인페인팅된 객체 \\(\\hat{I}_{i}\\)을 이미지-투-3D 방법으로 재구성하여 단일 3D 모델을 생성할 수 있다.\n' +
      '\n' +
      '### Multi-Object Combination\n' +
      '\n' +
      '이 단계에서는 입력 영상(I\\)과 의미적 공간 관계에 맞도록 스케일, 회전, 병진 파라미터 \\(\\{s_{i},r_{i},t_{i}\\}\\)을 최적화하여 별도의 3차원 모델을 결합하고자 한다. 각 물체의 스케일, 회전, 병진동을 \\(I\\) 기반으로 초기화하고, 공간 인식 확산 사전과 참조 영상의 안내를 사용하여 정제한다. 먼저 공간 인식 확산 증류 방식을 소개하고 자동 객체 조합에 대한 적용에 대해 논의한다.\n' +
      '\n' +
      '**Spatially-Aware Diffusion Guidance.** DreamFusion[36]은 사전 훈련된 2D 확산 모델을 채택하여 텍스트 기술로부터 3D 표현을 최적화하는 방법을 제시한다. 피사체는 미분 가능한 파라미터화[31]로 표현되며, 미분 가능한 MLP 렌더러\\(g\\)는 \\(\\theta\\)으로 파라미터화된 신경 복사도 필드로부터 2D 이미지\\(x=g(\\theta)\\)를 렌더링한다. 확산 모델(\\phi\\)을 이용하여 잡음 영상(x_{t}\\), 텍스트 임베딩(y\\), 잡음 레벨(t\\)이 주어졌을 때 샘플링된 잡음(\\epsilon\\)을 예측하는 스코어 함수(\\hat{\\epsilon}_{\\phi}(x_{t};y,t)\\)를 제공한다. 이 점수 함수는 신경 파라미터 \\(\\theta\\)를 업데이트하기 위한 기울기의 방향을 안내하고, 기울기는 스코어 증류 샘플링(SDS)에 의해 계산된다:\n' +
      '\n' +
      '\\bigtriangledown_{\\theta}\\mathcal{L}_{\\text{SDS}(\\phi,x)=\\mathbbb{E}_{t, \\epsilon}\\left[w(t)(\\hat{\\epsilon}_{\\phi}(x_{t;y,t)-\\epsilon)\\frac{\\partial x}{\\partial\\theta}\\right], \\tag{4}\\t.\n' +
      '\n' +
      '한편 \\(w(t)\\)는 가중치 함수이다.\n' +
      '\n' +
      '그러나 SDS는 우리의 경우 위치 조정에 불안정하다는 것을 발견했다. 다람쥐가 상자 "_"에 앉아 있는 텍스트 프롬프트와 다람쥐와 상자의 이미지를 장난감 예로 사용하며, 텍스트 프롬프트에 따라 이미지 요소의 위치를 조정하는 SDS의 능력을 테스트하는 것을 목표로 한다. 도 1에 도시된 바와 같다. 도 5를 참조하면, 이미지 콘텐츠(다람쥐 및 상자)가 이미 프롬프트와 일치하고 SDS가 위치 조정을 푸시하지 않기 때문에 SDS는 정확한 배치를 생성하지 않는다. 따라서 기울기를 계산할 때 위치 조정을 강조하기 위해 공간적으로 인식하는 SDS를 제안한다.\n' +
      '\n' +
      'SDS에서 각 텍스트 임베딩 \\(y\\)과 시간 단계 \\(t\\)에 대해 UNet을 사용하여 잡음 \\(\\hat{\\epsilon}_{\\phi}(x_{t};y,t)\\)을 예측한다는 것을 기억하라. 노이지 영상의 특징\\(\\phi(x_{t})\\)은 질의 행렬\\(Q=\\mathcal{F}_{Q}(\\phi(x_{t}))에 투영되고, 텍스트 임베딩은 키 행렬\\(K=\\mathcal{F}_{K}(y)\\)과 값 행렬\\(V=\\mathcal{F}_{V}(y)\\)에 투영되며, 학습된 선형 투영\\(\\mathcal{F}_{Q}\\), \\(\\mathcal{F}_{K}\\) 및 \\(\\mathcal{F}_{V}\\)을 통해 투영된다. 그런 다음 주의 맵은 다음과 같이 계산된다.\n' +
      '\n' +
      '\\[M=\\text{Softmax}\\left(\\frac{QK^{T}}{\\sqrt{d}}\\right), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(M_{j}\\)는 \\(j\\)번째 토큰의 어텐션 맵을 나타내고, d는 키 및 쿼리의 잠재 투영 차원이다.\n' +
      '\n' +
      '공간 레이아웃의 개선을 우선시하기 위해 공간 관계를 설명하는 핵심 토큰을 강화한다. 키 토큰은 "전방", "위" 및 "아래"와 같은 공간적 관계를 기술하는 단어 또는 LLM에 의해 추출되거나 사용자에 의해 지시될 수 있는 "승차" 및 "유지"와 같은 객체 상호작용을 기술하는 단어일 수 있다. 예를 들어, 프롬프트 _"a squirrel이 박스 "_에 앉아 있는 것을 고려하면, 우리는 다람쥐와 종이 박스 사이의 관계를 설명하는 단어 _"sitting on"_의 효과를 강화하고자 한다. 이러한 공간인식 최적화를 달성하기 위해 [14]와 유사한 상수 c(\\(c>1\\))로 할당된 토큰의 어텐션 맵(j^{\\star}\\)을 스케일링하여 공간관계에 대한 집중도를 높였다. 나머지 주의 지도는 변함이 없다:\n' +
      '\n' +
      '\\begin{cases}c\\cdot M_{j}&\\quad\\text{if}\\quad j=j^{\\star}\\\\m_{j}&\\quad\\text{otherwise.}\\end{cases}\\tag{6}\\\n' +
      '\n' +
      '공간 인식 SDS 손실(SSDS)은 다음과 같이 공식화될 수 있다:\n' +
      '\n' +
      '√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√√ 타임스테프의 경우, 이러한 단계가 생성된 이미지의 공간 레이아웃에 더 큰 영향을 미치기 때문에 잡음 수준이 높은 범위에서 \\(t\\)을 샘플링한다.\n' +
      '\n' +
      '오브젝트들을 결합한다. 바운딩 박스로부터 스케일, 회전 및 이동의 대략적인 초기화(\\{s_{i},r_{i},t_{i}\\}\\(b_{i}\\)와 추정된 깊이\\(d_{i}\\)로 시작한다. 구체적으로, 스케일 \\(s_{i}\\)은 바운딩 박스 크기 및 이미지 크기의 비율에 의해 결정된다:\n' +
      '\n' +
      '\\[s_{i}=max\\left\\{\\frac{W_{b_{i}}}{W_{I}},\\frac{H_{b_{i}}}{H_{I}}\\right\\}, \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(W_{b_{i}}\\), \\(H_{b_{i}}\\), \\(W_{I}\\), \\(H_{I}\\)은 각각 바운딩 박스 \\(b_{i}\\) 및 입력 이미지 \\(I\\)의 폭과 높이이다. 번역 \\(t_{i}\\)은 단안 깊이 예측 모델 [39]를 사용하여 \\(t_{i}\\)에서 z 차원으로 설정된 \\(i\\)번째 물체에 대한 평균 깊이 \\(d_{i}\\)을 추정하고, x 및 y 차원은 경계 상자 \\(b_{i}\\)의 중심 좌표와 이미지 크기에 의해 초기화된다. 즉,\n' +
      '\n' +
      '{l}t_{i}=(X_{b_{i}}-\\frac{W_{I}}{2},Y_{b_{i}}-\\frac{H_{I}}{2},d_{i}),\\\\d_{i}=\\text{Average}(\\text{Depth}(O_{i})),\\end{array}\\tag{9}\\t{i}=(X_{b_{i}}-\\frac{W_{I}}{2},Y_{b_{i}}-\\frac{H_{I}}{2},d_{i})\n' +
      '\n' +
      '여기서 \\(X_{b_{i}}\\), \\(Y_{b_{i}}\\)는 경계 상자 \\(b_{i}\\), \\(d_{i}\\)는 \\(i\\)번째 물체 \\(O_{i}\\)에 있는 각 픽셀의 평균 깊이이다. 3차원 회전각 \\(r_{i}\\)은 \\((0,0,0)\\)으로 초기화된다.\n' +
      '\n' +
      '그러나, 단일 시점 영상에서의 깊이-크기 모호성으로 인해, 예측된 깊이는 부정확할 수 있어, 깊이 및 크기의 불합리한 초기화가 초래될 수 있다. 단일 시점 모호성을 완화하기 위해 제안된 공간 인식 SDS 손실(SSDS)을 새로운 시점 감독으로 사용하여 공간 매개변수 \\(\\{s_{i},r_{i},t_{i}\\}\\)을 정제한다. 최적화를 안정화하기 위해 참조 시점 렌더링과 입력 영상 사이의 복원 오차도 제한한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{Ref}=\\lambda_{\\text{RGB}\\left|\\hat{I}_{\\text{RGB}-I_{\\text{RGB}}\\right|+\\lambda_{\\text{A}\\left|\\hat{I}_{\\text{A}}-I_{\\text{A}}\\right|, \\tag{10}\\tag{10}}\n' +
      '\n' +
      '여기서 \\(\\lambda_{\\text{RGB}\\) 및 \\(\\lambda_{\\text{A}\\)는 RGB 및 알파 채널에 대한 가중치이다. 총 손실은 \\(\\mathcal{L}_{\\text{Ref}}\\)과 \\(\\mathcal{L}_{\\text{SSDS}}\\)의 가중합이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '안정확산으로 이미지를 인페인팅할 때 유도척도를 7.5, 추론단계 수를 30으로 설정하였다. 우리는 파이토치3D[40]을 미분 가능한 렌더링 엔진으로 사용한다. 800에서 900 사이의 타임스탬프(t\\)를 무작위로 샘플링하고, Adam을 최적기로 사용하여 z 차원에 대한 번역의 학습률은 0.01이고, 나머지 차원은 0.001로 설정하였으며, 손실 중량(\\lambda_{\\text{Ref}}\\), \\(\\lambda_{\\text{SSDS}}\\), \\(\\lambda_{\\text{RGB}}\\), \\(\\lambda_{\\text{A}}\\)은 각각 1, 1, 1,000, 1,000으로 설정하였다. 어텐션 맵에 대한 곱셈기 \\(c\\)를 25로 설정하고, 다중 객체 조합 단계에서 각 개별 메쉬를 50,000개의 면으로 다운샘플링하고 각 반복에 대해 10개의 뷰를 렌더링한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '**Benchmark.** 우리의 방법을 평가하기 위해, 우리는 다양한 복잡한 3D 자산을 포함하는 100개의 이미지를 포함하는 테스트 벤치마크를 구축했다. 벤치마크는 안정적인 확산으로 생성된 50개의 이미지와 포토샵으로 실제 이미지로 구성된 50개의 이미지를 포함한다. 각 이미지에는 전경 마스크, 객체에 대한 경계 상자 세트 및 텍스트 캡션이 있습니다. GPT4를 사용하여 텍스트 프롬프트와 공간 토큰을 제안한 후 수동 필터링을 수행한다. 우리는 이 벤치마크를 공개적으로 사용할 수 있도록 할 것이다.\n' +
      '\n' +
      '**비교 방법.** 우리는 공식 코드를 사용하여 구현한 SyncDreamer [26]의 세 가지 최신 단일 이미지 재구성 방법과 우리의 방법을 비교한다. 텍스처 메쉬는 NeuS[50] 표현으로부터 도출된다. 2) 공개 코드[13]를 사용하여 구현하는 LRM[17]. 3) 원더3D [28]은 공식 코드를 사용하여 구현하며 이미지-투-3D 복원을 위한 기본 모델로 사용한다.\n' +
      '\n' +
      '**정성적 비교.** 도에 도시된 바와 같이. 도 6을 참조하면, 제안하는 방법은 각 객체들을 정확하게 재구성하고, 이들 사이의 좋은 공간 관계를 보존할 수 있다. 다른 방법은 종종 고품질 지오메트리 및 텍스처를 생성하는 데 어려움을 겪습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & CLIP-Score \\(\\uparrow\\) & GPT-3DScore \\(\\uparrow\\) \\\\ \\hline SyncDreamer [26] & 81.47\\% & 13.54\\% \\\\ OpenLRM [17] & 83.65\\% & 53.12\\% \\\\ Wonder3D [28] & 85.57\\% & 56.25\\% \\\\ Ours & **86.58**\\% & **65.63\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **정량비교.**\n' +
      '\n' +
      '그림 6: **이미지 대 3D 생성에 대한 시각적 비교.** 입력 이미지가 주어지면, 이전의 방법들은 특히 새로운 뷰들에서 부정확한 기하학 및 흐릿한 텍스처를 재구성한다. 제안된 합성 생성 기법을 이용하여 보다 높은 충실도의 3차원 모델을 생성한다.\n' +
      '\n' +
      '도 7 : 정성적 결과 ComboVerse_는 다수의 객체를 포함하는 단일 이미지로부터 고품질의 3D 모델을 생성할 수 있다.\n' +
      '\n' +
      '입력 이미지 내의 작은 객체들과, 상이한 객체들의 접합 부분들은 종종 혼합된다. 보다 정성적 결과는 그림 7에 나와 있다.\n' +
      '\n' +
      '**Quantitative Comparison.** CLIP-Score[38]을 이용하여 신규 시점 영상과 기준 영상 간의 의미적 유사성을 측정한다. 우리는 또한 [56]에 따른 GPT 기반 평가를 포함한다. 우리는 모든 샘플에 걸쳐 각 방법에 대해 쌍별 비교를 수행하고 각 방법에 대한 성공 확률을 보고한다. 표 1은 우리의 방법이 의미 유사성과 GPT 평가 모두에서 비교 방법보다 우수함을 보여준다.\n' +
      '\n' +
      '**사용자 스터디.** 수치 메트릭 외에도 사용자 스터디를 수행하여 다른 방법과 비교합니다. 우리는 22명의 인간 사용자로부터 990개의 답변을 수집합니다. 참가자들은 기준 이미지와 3D 모델(우리 및 기준선)의 무작위 쌍을 한 번에 보여주며 기하학과 질감 품질 측면에서 보다 현실적인 모델을 선택하도록 요청받는다. 모든 선택은 시간 제한 없이 셔플된 순서로 주어진다. 도. 도 8은 우리의 방법이 인간의 선호도 측면에서 이전의 접근법들보다 우수하다는 것을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**오브젝트 인페인팅의 효과**오브젝트 완성도의 디자인을 살펴보기 위한 연구를 수행하였다. 먼저 잡음 배경이 객체 완성에 미치는 영향을 조사한다. 소음이 없는 채색된 결과는 검은색이었다.\n' +
      '\n' +
      '도 8: **사용자 연구. 우리의 방법은 인간 평가 측면에서 경쟁사보다 일관되게 우수합니다.**\n' +
      '\n' +
      '그림 9: 인페인팅 대상에 대한 **분석. 우리는 무작위 배경(EQ. 2)으로 설득력 있는 인페인팅 결과를 생성한다. , 경계 인식 마스크 제안(EQ. 3) 및 문자 프롬프트.**\n' +
      '\n' +
      '그림 9와 같이 경계 및 완전하지 않은 경우 경계 인식 마스크 대신 배경 마스크를 단순히 인페인팅에 사용했으며 인페인팅 결과에는 추가 부분이 있어 입력 이미지와의 3D 재구성에 불일치가 발생했다. 또한 인페인팅 중 확산 모델에 대한 텍스트 프롬프트 "완전한 3D 모델"을 제거했으며 결과도 저하되었다.\n' +
      '\n' +
      '**Spatially-Aware Diffusion Guidance의 효과.** 도 10을 참조하면, "우주비행사가 붉은 말을 타고 있다"의 예를 이용하여 객체 조합의 서로 다른 안내 설정을 분석한다. _ Base_는 새로운 뷰에서 추가 안내 없이 참조 뷰에서 재구성 손실만을 강제하므로, 우주인과 말 사이의 부정확한 상대 깊이를 산출한다. 공간 안내로서 깊이 예측 모델의 표준 _SDS 손실_ 또는 _깊이 손실_를 사용하면 우주인과 말 사이의 상호 작용이 개선되지만 여전히 정확하지 않았다. 제안된 _SSDS 손실(full)_로 단어 "승차"에 대한 주의를 강화함으로써, 전체 모델이 최상의 결과를 달성한다. 이는 표준 SDS에 비해 제안된 방법의 향상된 공간 제어 능력을 확인한다. Sec. 3.3에서 논의한 바와 같이, SSDS를 수행할 때 안정적인 확산을 위해 높은 노이즈 범위([800, 900])에서 샘플링했는데, 이러한 단계는 생성된 이미지의 공간 레이아웃에 더 큰 영향을 미치기 때문이다. 또한 잡음 시간 범위, _low noise range_ ([100, 200]) 및 _uniform noise range_ ([20, 980])의 샘플 범위를 달리한 SSDS를 실험하여 성능 저하를 관찰하였다. 우리는 또한 보충 재료_에서 정량적 절제 결과를 제공한다.\n' +
      '\n' +
      '장면 재구성을 위한### 응용\n' +
      '\n' +
      '또한, 2개의 객체를 갖는 3차원 자산을 생성하는 것 외에도, 제안된 방법의 일반화 능력을 다중 객체에 대해 검증한다. 도 1에 도시된 바와 같다. 도 11을 참조하면, 제안하는 방법을 이용하여 다수의 (\\(>2\\)) 객체들로 구성된 3차원 장면들을 재구성한다. 객체 레벨에서 동작하는 기존의 방법들은 장면 생성과 기하학 및 질감 모두에서 명백한 아티팩트를 생성하는데 어려움이 있다. 또한, 마지막 예에서 개와 같은 작은 물체들에 대해, 기존의 방법들은 앞서 언급된 트레이닝 편향으로 인해 그것을 무시하는 경향이 있다. 이와는 대조적으로, 우리의 방법은 현실적이고 고품질의 재구성을 달성한다.\n' +
      '\n' +
      '도 10: **객체 조합에 대한 분석.** 표준 SDS 및 깊이 제약과 비교하여, SSDS는 객체 포지셔닝에 대한 더 강한 안내를 제공한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 단일 영상으로부터 고품질의 합성 3D 자산을 생성하는 새로운 방법인 _ComboVerse_를 제시한다. "다중 객체 격차"에 대한 심층 분석을 통해 객체 수준 3D 생성 모델을 구축하고 더 복잡한 객체를 다루기 위해 확장한다. 다양한 객체의 재구성된 3D 모델을 사용하여 주어진 이미지와 일치하는 3D 자산을 생성하기 위해 크기, 회전 각도 및 위치를 조정한다. 이를 위해 사물의 배치를 안내하기 위해 사전 훈련된 확산 모델에서 공간 인식 점수 증류 샘플링을 제안했다. 우리의 접근 방식은 복잡한 3D 객체 재구성을 위한 귀중한 단계가 될 수 있으며 향후 3D 장면 생성을 위한 길을 열어줄 수 있다.\n' +
      '\n' +
      '**Limitations.** 제안된 방법은 두 개 또는 몇 개(일반적으로 \\(<5\\))의 객체로 구성된 자산에 대해 좋은 성능을 보인다. 그러나 기존의 텍스트 기반 작업과 유사하게, 우리는\n' +
      '\n' +
      '그림 11: ** 장면 재구성의 비교.** 2개 이상의 객체를 포함하는 몇 가지 도전 사례를 보여준다. 첫 번째 예는 자동차, 공룡, 나무, 그리고 콘_의 네 가지 물체를 포함한다. 두 번째 예는 로봇 2개와 볼_의 세 가지 예를 포함한다. 세 번째 예는 집, 개, 트리_의 세 가지 예를 포함한다. 본 논문에서 제안하는 방법은 구성 기법을 이용하여 강인한 복원 성능을 보인다.\n' +
      '\n' +
      '방법은 여전히 더 많은 객체들과 함께 매우 복잡한 장면들을 생성하는데 있어서 어려움에 직면해 있다. 우리 접근법의 또 다른 한계는 결합 과정에서 기하학과 질감에 대한 최적화가 부족하다는 것이다. 따라서 최종 결과의 품질은 우리가 백본으로 사용하는 이미지 대 3D 방법의 성능에 달려 있다. 향후 보다 강력한 백본 방법으로 우리의 방법을 더욱 향상시킬 수 있을 것으로 기대한다.\n' +
      '\n' +
      '## 부록 0. 정량적 절제 분석\n' +
      '\n' +
      '제안된 SSDS의 효과를 평가하기 위해 절제 분석을 수행했으며 그림 1에서 정성적 결과를 보여주었다. 본문 10. 시각적 비교 외에도 표 2에서 정량적 절제 분석을 제공하며, CLIP B/16 및 ResNet50 백본과 같은 두 개의 CLIP 모델을 사용하여 객체 조합의 다양한 유도 구성을 검사한다. 지오메트리 및 외관 품질을 분해하기 위해 컬러 렌더링 및 텍스처링되지 않은 지오메트리 렌더링 모두에 대해 멀티뷰 CLIP 유사성을 평가한다. _ Base_는 참조 뷰에서 재구성 손실만을 부과하고, 신규 뷰에서 추가 지침이 부족하다. 공간 안내로서 깊이 예측 모델로부터의 표준 _SDS_ 손실 또는 _depth_ 손실을 적용하면 색상 및 지오메트리에 대한 차선책 CLIP 점수가 산출되었다. 그러나 제안된 _SSDS 손실(full)_을 통해 공간 레이아웃에 대한 주의를 강화함으로써 전체 모델이 최상의 결과를 달성하여 표준 SDS에 대한 향상된 공간 제어를 확인한다. Sec. 3.3에서 논의한 바와 같이, 생성된 이미지의 공간 레이아웃에 더 큰 영향을 미치기 때문에 SSDS 동안 안정적인 확산을 위해 높은 노이즈 간격([800, 900])이 선택되었다. 또한 잡음 시간 범위, _low noise range_ ([100, 200]) 및 _uniform noise range_ ([20, 980])의 샘플 범위를 달리한 SSDS를 실험하여 성능 저하를 관찰하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{4}{c}{CLIP Score\\(\\uparrow\\)} \\\\ Guidance & \\multicolumn{2}{c}{CLIP B/16} & \\multicolumn{2}{c}{ResNet50} \\\\  & Color & Geometry & Color & Geometry \\\\ \\hline Base (without guidance) & 86.62\\% & 75.24\\% & 80.35\\% & 74.19\\% \\\\ Depth Loss & 84.57\\% & 78.42\\% & 81.69\\% & 75.83\\% \\\\ SDS & 84.16\\% & 78.25\\% & 84.08\\% & 74.66\\% \\\\ SSDS (uniform noise range) & 85.33\\% & 78.49\\% & 85.55\\% & 75.85\\% \\\\ SSDS (low noise range) & 84.86\\% & 79.03\\% & 84.42\\% & 75.44\\% \\\\ SSDS (full) & **89.01**\\% & **79.66**\\% & **86.60**\\% & **78.10**\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 절제 연구를 위한 정량적 분석.\n' +
      '\n' +
      '"다중 객체 간격"에 대한 추가 결과\n' +
      '\n' +
      'Sec.3.1에서 논의된 바와 같이, 주로 Obfjaverse에 대해 훈련된 현재 피드포워드 모델은 다중 객체 시나리오로 일반화하는 데 한계를 나타낸다. 도. 도 12는 이러한 한계를 입증하기 위한 또 다른 사례로 이미지-투-3D 복원에서 최첨단 방법인 TripoSR을 사용한다. 개선에도 불구하고 TripoSR은 고유한 데이터와 모델 편향으로 인해 여러 개체를 생성하는 작업을 수행할 때 여전히 세 가지 전형적인 고장 모드를 나타낸다. 자세한 분석은 Sec. 3.1에 설명되어 있다.\n' +
      '\n' +
      '그림 12: Obfjaverse에서 훈련된 모델의 "다중 객체 간격"이다. (a) 카메라 설정 바이어스. 소형 및 비중심 객체에 대한 재구성 품질은 별도의 재구성에 비해 크게 다운그레이드될 것이다. (b) 폐색. 재구성 결과는 물체가 다른 물체에 의해 가려질 때 혼합되는 경향이 있다. (c) 누출 패턴. 객체의 모양과 질감은 입력 이미지 내의 다른 객체들에 의해 영향을 받을 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: CVPR (2022)\n' +
      '* [2] Chan, E.R., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5799-5809 (2021)\n' +
      '* [3] Chaudhuri, S., Kalogerakis, E., Giguere, S., Funkhouser, T.: Attribit: content creation with semantic attributes. In: Proceedings of the 26th annual ACM symposium on User interface software and technology. pp. 193-202 (2013)\n' +
      '* [4] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3d content creation. [https://arxiv.org/abs/2303.13873](https://arxiv.org/abs/2303.13873) (2023)\n' +
      '* [5] Cheng, Y.C., Lee, H.Y., Tulyakov, S., Schwing, A., Gui, L.: SDFusion: Multimodal 3d shape completion, reconstruction, and generation. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [6] Chou, G., Bahat, Y., Heide, F.: Diffusion-SDF: Conditional generative modeling of signed distance functions. [https://arxiv.org/abs/2211.13757](https://arxiv.org/abs/2211.13757) (2023)\n' +
      '* [7] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3d objects. [https://arxiv.org/abs/2212.08051](https://arxiv.org/abs/2212.08051) (2022)\n' +
      '* [8] Epstein, D., Poole, B., Mildenhall, B., Efros, A.A., Holynski, A.: Disentangled 3d scene generation with layout learning. arXiv preprint arXiv:2402.16936 (2024)\n' +
      '* [9] Funkhouser, T., Kazhdan, M., Shilane, P., Min, P., Kiefer, W., Tal, A., Rusinkiewicz, S., Dobkin, D.: Modeling by example. ACM transactions on graphics (TOG) **23**(3), 652-663 (2004)\n' +
      '* [10] Gao, G., Liu, W., Chen, A., Geiger, A., Scholkopf, B.: Graphdreamer: Compositional 3d scene synthesis from scene graphs. arXiv preprint arXiv:2312.00093 (2023)\n' +
      '* [11] Gu, J., Liu, L., Wang, P., Theobalt, C.: Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. In: ICLR (2022)\n' +
      '* [12] Gupta, A., Xiong, W., Nie, Y., Jones, I., Oguz, B.: 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371 (2023)\n' +
      '* [13] He, Z., Wang, T.: Openlrm: Open-source large reconstruction models. [https://github.com/3DTopia/OpenLRM](https://github.com/3DTopia/OpenLRM) (2023)\n' +
      '* [14] Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.: Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626 (2022)\n' +
      '* [15] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems **33**, 6840-6851 (2020)\n' +
      '* [16] Hong, F., Tang, J., Cao, Z., Shi, M., Wu, T., Chen, Z., Wang, T., Pan, L., Lin, D., Liu, Z.: 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234 (2024)\n' +
      '* [17] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [18] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. [https://arxiv.org/abs/2305.02463](https://arxiv.org/abs/2305.02463) (2023)\n' +
      '*[*[19] Kalogerakis, E., Chaudhuri, S., Koller, D., Koltun, V. : 컴포넌트 기반 형상 합성을 위한 확률 모델. Acm Transactions on Graphics (TOG) **31**(4), 1-11 (2012)\n' +
      '* [20] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023)\n' +
      '* [21] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model (2023)\n' +
      '* [22] Li, W., Chen, R., Chen, X., Tan, P.: SweetDreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. [https://arxiv.org/abs/2310.02596](https://arxiv.org/abs/2310.02596) (2023)\n' +
      '* [23] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3D: High-resolution text-to-3d content creation. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [24] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [25] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. [https://arxiv.org/abs/2303.11328](https://arxiv.org/abs/2303.11328) (2023)\n' +
      '* [26] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: SyncDreamer: Generating multiview-consistent images from a single-view image. [https://arxiv.org/abs/2309.03453](https://arxiv.org/abs/2309.03453) (2023)\n' +
      '* [27] Liu, Z., Feng, Y., Black, M.J., Nowrouzezahrai, D., Paull, L., Liu, W.: Meshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133 (2023)\n' +
      '* [28] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [29] Melas-Kyriazi, L., Rupprecht, C., Laina, I., Vedaldi, A.: RealFusion: 360 reconstruction of any object from a single image. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n' +
      '* [30] Metzer, G., Richardson, E., Patashnik, O., Giryes, R., Cohen-Or, D.: Latent-nerf for shape-guided generation of 3d shapes and textures. [https://arxiv.org/abs/2211.07600](https://arxiv.org/abs/2211.07600) (2022)\n' +
      '* [31] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [32] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-E: A system for generating 3d point clouds from complex prompts. [https://arxiv.org/abs/2212.08751](https://arxiv.org/abs/2212.08751) (2023)\n' +
      '* [33] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11453-11464 (2021)\n' +
      '* [34] Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J.J., Kemelmacher-Shlizerman, I.: Stylesdf: High-resolution 3d-consistent image and geometry generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13503-13513 (2022)\n' +
      '* [35] Po, R., Wetzstein, G.: Compositional 3d scene generation using locally conditioned diffusion. arXiv preprint arXiv:2303.12218 (2023)* [36] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: DreamFusion: Text-to-3d using 2d diffusion. In: International Conference on Learning Representations (ICLR) (2023)\n' +
      '* [37] Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov, I., Wonka, P., Tulyakov, S., Ghanem, B.: Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. [https://arxiv.org/abs/2306.17843](https://arxiv.org/abs/2306.17843) (2023)\n' +
      '* [38] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [39] Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. ICCV (2021)\n' +
      '* [40] Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y., Johnson, J., Gkioxari, G.: Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501 (2020)\n' +
      '* [41] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [42] Schulz, A., Shamir, A., Levin, D.I.W., Sitthi-Amorn, P., Matusik, W.: Design and fabrication by example. ACM Transactions on Graphics (Proceedings SIGGRAPH 2014) **33**(4) (2014)\n' +
      '* [43] Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. [https://arxiv.org/abs/2310.16818](https://arxiv.org/abs/2310.16818) (2023)\n' +
      '* [44] Tang, J., Chen, Z., Chen, X., Wang, T., Zeng, G., Liu, Z.: Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054 (2024)\n' +
      '* [45] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation (2023)\n' +
      '* [46] Tang, J., Wang, T., Zhang, B., Zhang, T., Yi, R., Ma, L., Chen, D.: Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. In: International Conference on Computer Vision ICCV (2023)\n' +
      '* [47] Tang, S., Chen, J., Wang, D., Tang, C., Zhang, F., Fan, Y., Chandra, V., Furukawa, Y., Ranjan, R.: Mvdiffusion++: A dense high-resolution multi-view diffusion model for single to sparse-view 3d objectreconstruction. arXiv preprint arXiv:2402.12712 (2024)\n' +
      '* [48] Tertikas, K., Despoina, P., Pan, B., Park, J.J., Uy, M.A., Emiris, I., Avrithis, Y., Guibas, L.: Partnerf: Generating part-aware editable 3d shapes without 3d supervision. arXiv preprint arXiv:2303.09554 (2023)\n' +
      '* [49] Vilesov, A., Chari, P., Kadambi, A.: Cg3d: Compositional generation for text-to-3d via gaussian splatting. arXiv preprint arXiv:2311.17907 (2023)\n' +
      '* [50] Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Advances in Neural Information Processing Systems **34**, 27171-27183 (2021)\n' +
      '* [51] Wang, P., Shi, Y.: Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv preprint arXiv:2312.02201 (2023)\n' +
      '* [52] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., Guo, B.: Rodin: A generative model for sculpting 3d digital avatars using diffusion. [https://arxiv.org/abs/2212.06135](https://arxiv.org/abs/2212.06135) (2023)\n' +
      '*[*[53] Wang, Z., Li, M., Chen, C.: Luciddreaming: Controllable object-centric 3d generation. arXiv preprint arXiv:2312.00588 (2023)\n' +
      '* [54] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. [https://arxiv.org/abs/2305.16213](https://arxiv.org/abs/2305.16213) (2023)\n' +
      '* [55] Willis, K.D., Jayaraman, P.K., Chu, H., Tian, Y., Li, Y., Grandi, D., Sanghi, A., Tran, L., Lambourne, J.G., Solar-Lezama, A., et al.: Joinable: Learning bottom-up assembly of parametric cad joints. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15849-15860 (2022)\n' +
      '* [56] Wu, T., Yang, G., Li, Z., Zhang, K., Liu, Z., Guibas, L., Lin, D., Wetzstein, G.: Gpt-4v(sion) is a human-aligned evaluator for text-to-3d generation. arXiv preprint arXiv:2401.04092 (2023)\n' +
      '* [57] Xiang, J., Yang, J., Deng, Y., Tong, X.: Gram-hd: 3d-consistent image generation at high resolution with generative radiance manifolds. arXiv preprint arXiv:2206.07255 (2022)\n' +
      '* [58] Xu, D., Jiang, Y., Wang, P., Fan, Z., Wang, Y., Wang, Z.: NeuralLift-360: Lifting an in-the-wild 2d photo to a 3d object with 360 views. [https://arxiv.org/abs/2211.16431](https://arxiv.org/abs/2211.16431) (2023)\n' +
      '* [59] Yi, T., Fang, J., Wu, G., Xie, L., Zhang, X., Liu, W., Tian, Q., Wang, X.: GaussianDreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. [https://arxiv.org/abs/2310.08529](https://arxiv.org/abs/2310.08529) (2023)\n' +
      '* [60] Yichun, S., Peng, W., Jianglong, Y., Long, M., Kejie, L., Xiao, Y.: MVDream: Multi-view diffusion for 3d generation. [https://arxiv.org/abs/2308.16512](https://arxiv.org/abs/2308.16512) (2023)\n' +
      '* [61] Zhao, Z., Liu, W., Chen, X., Zeng, X., Wang, R., Cheng, P., FU, B., Chen, T., YU, G., Gao, S.: Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In: Thirty-seventh Conference on Neural Information Processing Systems (2023)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>