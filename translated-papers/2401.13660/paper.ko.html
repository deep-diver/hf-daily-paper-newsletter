<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '투텐이 없는 선택 상태 공간 모델# 팜바 바:\n' +
      '\n' +
      '알렉산 옌 알렉산드 M 루샤르 강아바라푸.\n' +
      '\n' +
      'Cornell University\n' +
      '\n' +
      '{jw2544,tg352,jy858,arush}@cornell.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '토큰 없는 언어 모델은 원시 바이트로부터 직접 학습하고, 서브워드 토큰화의 편향을 제거한다. 그러나 바이트에서 작동하는 것은 훨씬 더 긴 서열을 초래하고 이러한 환경에서 표준 자가회귀 전환기 척도를 제대로 초래하지 않는다. Mamba 상태 공간 모델의 토큰이 없는 적응인 MambaByte를 사용하여 바이트 서열에서 자동 훈련했다. 우리의 실험은 다른 바이트 수준 모델과 비교하여 만바 바이트의 계산 효율을 나타낸다. 우리는 또한 몰바 바테가 최첨단 서브워드 트랜스퍼와 경쟁하고 심지어 능가할 수 있는 것을 발견했다. 또한, 길이의 선형 스케일링으로 인해, 삼바 바테는 트랜스포머에 비해 빠른 추론으로부터 혜택을 받는다. 우리의 연구 결과는 토큰이 없는 언어 모델링을 가능하게 하는 데 있어 만바 바테의 생존력을 확립한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '언어 모델을 정의할 때, 기본 토큰화는 일반적으로 단어들(Bengio et al., 2000), 하위 키워드들(Schuster and 낙자마, 2012; Sennrich et al, 2015; Sennrich et al. Sengiima, 2015) 중 어느 하나(Bengio et al., 2000)로 사용된다.\n' +
      '\n' +
      '그림 1: **Bench마크 바이트 수준 모델을 PG19(\\(8,192\\) 연속 바이트에 대해 고정 파라미터 예산**언어 모델링 결과를 표시하고 표준 트랜스포머(Vaswani et al, 2017; Su et al, 2021), 메가 바이트 트랜스포머(유 et al, 2023), 게이팅 대각화 S4(Mehta et al., 2023), 몰바 바이트(Mehta et al., 2023)를 비교한 표준 트랜스포머(Vaswani et al. 트레이닝 단계에 대한 모델 손실(Left)입니다. (맞습니다) FLOP 정상화된 훈련 비용은요. 앰바 바테는 계산 예산의 3분의 1 미만에서 전환자 손실에 도달한다.\n' +
      '\n' +
      '2020년(Gao et al., 2020) 또는 인물(Gao et al., 2020)이다. 이 중 하위어 토큰화는 훈련 효율성과 욕설 외 단어를 처리할 수 있는 능력 사이의 자연스러운 타협을 달성하기 때문에 가장 인기 있는 선택이었다. 그러나 여러 작품(예: Xue et al.(2022))은 티포스에 대한 견고성 부족, 철자 및 자본화 변형 및 형태학적 변화와 같은 하위 단어 토큰기에 대한 문제를 언급했다.\n' +
      '\n' +
      '연구자(Clark et al, 2022, Xue et al, 2022, 유 et al., 2023)는 바이트 서열을 사용하는 대안적 접근, 즉 원시 데이터에서 중간 토큰화 없이 예측에 대한 종단 간 매핑을 사용했다. 서브워드 모델에 비해 바이트 수준 언어 모델은 정통 및 형태학적 변이체를 통해 보다 쉽게 일반화할 수 있다. 물론 바이트와 같은 텍스트 모델링은 결과 서열이 하위 단어 대응물보다 훨씬 더 길다는 것을 의미한다. 이는 효율성 문제를 건축 자체에 상류로 밀어 넣는다.\n' +
      '\n' +
      '효율성 문제는 언어 모델링(브라운 et al., 2020; Touvron et al., 2023)을 지배하는 자기회귀 전환기(Vaswani et al., 2017)에 대해 특히 두드러진다. 2차 주의 비용으로 인해 트랜스포머 척도는 긴(비트) 서열(브라운 et al., 2020; 장 et al., 2022)에 대해 좋지 않다. 연구자들은 _ 압축_ 내부 트랜스포머 표현을 가지고 있으며, 예를 들어, 토큰 그룹이 중간 층 내에 병합되는 길이 인식 모델링 접근법(Dai et al, 2020; Nawrot et al, 2022)을 개발한다. 최근 유 등은 바이트의 고정 크기 패치 형태로 압축을 사용하는 메가 바이트 트랜스포머를 하위어 유사체로 제안하였다. 결과적으로 메가 바테는 더 낮은 계산 비용을 가능하게 한다.\n' +
      '\n' +
      '부츠 1: 우리의 실험(그림 1 참조)은 패칭이 표준 트랜스포머에 비해 모델 성능을 낮출 수 있음을 나타낸다.\n' +
      '\n' +
      '본 연구에서는 효율적이고 간단한 바이트 수준의 언어 모델인 만바 바테를 소개한다. 이 모델은 서열 모델링을 위한 선형 시간 접근인 최근에 도입된 만바 아키텍처(구 및 다오, 2023)의 간단한 적응이다. 삼바는 국가공간모델(SSM)에 의해 개척된 접근법을 구축한다(Gupta et al, 2021; Gupta et al, 2022; 구 et al, 2022; 스미스 et al., 2023). 텍스트와 같은 이산 데이터에 더 효과적인 선택 메커니즘을 도입하여 효율적인 GPU 구현을 제공한다. 우리의 간단한 관찰은 모바(수정 없이)를 사용하면 언어 모델링에서 주요 계산 병목 현상을 완화하여 사용 가능한 계산 예산의 패칭 제거 및 효과적인 사용을 허용한다는 것이다.\n' +
      '\n' +
      '실험은 모바 바이트와 고정 파라미터의 트랜스포머, SSM 및 메가 바이트(매칭) 아키텍처를 비교하고 여러 롱폼 텍스트 데이터셋에 대한 고정된 계산 설정을 비교한다. 그림 1은 우리의 주요 결과를 요약한 것이다. 빈바 바이트 수준의 트랜스포머에 비해, 만바 바테는 더 나은 성능을 더 빠르게 달성하고 훨씬 더 많은 계산 효율이 있다. 또한 기존 최첨단 서브워드 모델과 비교하여 토큰이 없는 언어 모델의 생존 가능성을 고려한다. 이와 관련하여, 우리는 상당히 긴 서열을 취급했음에도 불구하고 다양한 하위 단어 기저부와 경쟁할 만바 바테를 찾는다. 우리의 결과는 기존의 토큰라이저 의존성 모델에 대한 강력한 대안으로 몰바 바테를 설정하고 엔드 투 엔드 학습을 용이하게 하기 위해 사용을 옹호한다.\n' +
      '\n' +
      '선택적 상태 공간 시퀀스 모델 모델##2 백그라운드.\n' +
      '\n' +
      'SSM은 1차 차동 방정식을 통해 시간에 걸쳐 숨겨진 상태의 진화를 모델링한다. 선형 시간-_invariant_ SSM(Gu et al, 2021; Gupta et al, 2022; Gupta et al., 2022; Smith et al., 2023)은 여러 양식에 걸쳐 딥러닝에서 유망한 결과를 보여주었다. 그러나 구와 다오(2023)는 최근 이러한 접근법의 끊임없는 역학이 언어 모델링과 같은 작업에 필요할 수 있는 숨겨진 상태에서 투입 의존적 맥락 _선택_이 부족하다는 주장을 하고 있다. 이를 위해 이들은 주어진 입력 \\(x(t) 자틸린\\mathbb{R}\\), 은닉 상태 \\(h(t) 자틸린\\mathbb{R}^{n}\\)에 대한 시변 연속 상태 역학을 정의하고 \\(y(t)\\in\\mathbb{R}\\)에서 출력(y(t)\\in\\mathb{R}\\)으로 규정하는 몰바를 제안했다.\n' +
      '\n' +
      '(t)}{\\mathrm{d}}(t){\\mathrm{d}(t),\\mathrm{A}(t)+\\mathrm{B}(t)}(t)\n' +
      '\n' +
      '대각선 시간-불변 시스템 매트릭스 \\(\\mathbb{R}^{n\\times n}\\) 및 시간 의존적 입력 및 출력 매트릭스 \\(\\mathrm{B}(t)\\in\\mathbb}{R}^{n\\i) 및\\(\\mathrm{C}^{C}^mathbb{R}^{R}^Mathbb{R}^{n\\)에 의해 매개변수가 된다.\n' +
      '\n' +
      '바이트와 같은 이산 시간 서열을 모델링하기 위해서는 (1)의 연속 시간 역학을 폐기화를 통해 근사화해야 한다. 이것은 각 타임스팟, \\(히버라인{\\Lambda}\\), \\(히버라인{\\text{B}}) 및 \\(히버라인{\\text{C}}\\)에서 새로운 매트릭스로 이산적으로 숨겨진 상태를 초래합니다.\n' +
      '\n' +
      '\\[h[k]=\\overline{\\Lambda}[k]h[k-1]+\\overline{\\text{B}}[k]x[k]]},\\quad y[k]= \\overline{C}[k]h[k]]]]].\n' +
      '\n' +
      '(2)가 순환 신경망의 선형 버전을 닮아 언어 모델 생성 동안 이러한 순환 형태로 적용될 수 있음을 관찰한다. 탈진화는 \\(t_{k}=t_{k}=1}^{k}\\Delta[j]\\)에 대해 \\(x[k]=x\\left(t_{k}\\right)\\ 처리에 해당하는 각 입력 위치에 대한 타임스팟, \\(\\Delta[k]\\)을 필요로 한다. 이산 시간 행렬 \\(\\overline{\\Lambda}\\), \\(\\overline{\\text{B}}\\), \\(\\overline{\\text{C}}\\)는 \\(\\Delta[k]\\)에서 계산될 수 있다. 그림 2는 Mampa 모델이 이산 서열을 어떻게 모델링하는지 보여준다.\n' +
      '\n' +
      'Mampa에서 SSM 항은 입력 선택, 즉 \\(\\text{B}\\), \\(\\text{C}\\), \\(\\Delta\\)는 입력 \\(x[k]\\in\\mathbb{R}^{d}\\)의 함수로 정의된다.\n' +
      '\n' +
      '(W_{\\Delta})=W_{\\Delta}(W_{R}x[k]) = W_{\\text{B}[k],\\tag{3}]\n' +
      '\n' +
      'r\\(W_{\\math{B}{\\b{R}^{n\\i d}\\)는 유사하게 정의되며, \\(W_{\\in\\mathbbb{R}\\in\\mathbb{R}^{d\\i{R}^{d\\tot) 및 \\(W_{\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\mathbb{R}\\in\\bb{R}\\bb{R}\\bb{R}\\bb{R}\\b{R}\\b{R}\\b{R}\\b{R}\\b{R}\\b{R}\\ SSM 매개변수 \\(\\text{A}\\), \\(\\text{B}\\), \\(\\text{C}\\)는 각 입력 차원(d\\)에 대해 동일하지만, 타임스팟 \\(\\Delta\\)은 구별되며, 이는 타임스메프 \\(n\\times d\\)당 \\(n\\tco d\\)의 숨겨진 상태 크기를 초래한다는 점에 주목한다. (불복 및 선택성에 대한 세부 사항에 대한 부록 D 참조)\n' +
      '\n' +
      '암마(Mampa)는 이 SSM 층을 전체 신경망 언어 모델에 장식한다. 구체적으로, 모델은 이전 게이팅 SSM(Mehta et al, 2023)에 의해 영감을 받은 게이팅 레이어의 스택을 사용한다. 그림 3은 SSM 레이어와 게이팅 신경망이 결합된 탬파 아키텍처를 보여준다.\n' +
      '\n' +
      '그림 3: **Mampa 블록**\\ (\\sigma\\)은 스와쉬 활성화 (Ramachandran et al, 2017)를 나타낸다.\n' +
      '\n' +
      '그림 2: ** 탬파 SSM** (a) 이산-시간 입력 \\ (x[k]\\)와 함께 입력-선택적 \\ (\\Delta[k]\\)를 계산한다. (b) 연속 시간 신호 \\(x(t)\\) (c)수학적으로 SSM은 \\(n\\)-차원 히든 상태( 어디에, \\(n=4\\))를 통해 연속 시간 \\(x(t)\\를 변환하고, 모수 \\(\\text{A}\\)와 \\(\\text{B}(t)\\)를 사용하여\\(y(t)\\(t)\\로 매핑한다. (d) 정형적으로는 \\(\\Delta[k]\\) 및 이산 시간 행렬 \\(\\overline{A}[k]\\), \\(\\overline{B}[k]\\), \\(\\overline{C}[k]\\)에 의해 정의된 단계에서 이산 시간 병렬 스캔을 사용하여 \\(y[k]\\)를 계산한다. 추론에서 우리는 직접 재발을 실행합니다.\n' +
      '\n' +
      '선형 재발에 대한 병렬 스캔은 훈련 시간에 전체 서열 \\(x\\)에 접근할 수 있어 선형 재발을 보다 효율적으로 계산할 수 있다. 스미스 등(2023)은 선형 SSM에서 순차적 재발을 효율적으로 계산하기 위해 작업 효율적인 병렬 스캔(Blelloch, 1990)의 사용을 보여주었다. (A_{k},b_{k})\\콜론q(A_{k},b_{k}[k],\\overline{\\mathrm{B}}[k]x[k])를 사용하여 a\\(L\\) tuples의 서열에 재발을 먼저 매핑한 다음, \\(e_{j} e_{k})(A_{k}. 마지막으로, 서열 \\([(차선{\\Lambda}[1],h[1]), (차선{\\Lambda}[2]\\overline{\\Lambda}[1],h[2]\\ldots]\\)를 계산하는 병렬 스캔을 적용한다. 일반적으로 이는 \\(T_{\\불릿}\\log_{\\log_{2}(L)) 시간을 필요로 하며, 여기서 \\(T_{\\불릿}\\)는 매트릭스-매트릭스 곱셈의 비용이다. I\\(\\overline{\\Lambda}\\)를 대각 행렬로 표시하면 선형 재발은 \\(\\mathcal{O}(n\\log_{2}(L)) 시간과 \\(\\mathcal{O}(nL) 공간)에서 병렬로 계산될 수 있다. 대각선 매트릭스를 사용한 병렬 스캔은 또한 작동에 효율적이어서 \\(\\mathcal{O}(nL)\\(FLOP)를 필요로 한다.\n' +
      '\n' +
      '3개의 설정을 합니다.\n' +
      '\n' +
      '우리의 실험은 만바 바테와 다른 바이트 수준의 트랜스퍼러 및 SSM을 비교한다. 모든 모델은 동일한 훈련 레시피(자세한 내용은 부록 C 참조)를 사용합니다. 다양한 장편 텍스트 데이터 세트인 PG19(Rae et al., 2020), 스트레이즈(Trinh and Le, 2018), 북(Gao et al., 2020), 아엑스브(Gao et al., 2020), 코드(Gao et al., 2020)를 활용한다. 데이터베이스 크기 및 평균 문서 길이는 부록 A에 포함된다.\n' +
      '\n' +
      '건축가 전반에 걸친 성능 비교는 주의가 필요하다. 이를 위해 계산 일치 및 파라미터 일치의 두 가지 설정을 고려한다. 이 설정은 기본 메가 바이트 트랜스포머가 입력의\\(8\\) 패턴화된 표현과 함께 작동하는 _global_ 모듈을 사용하므로 \\(8\\)를 사용하여 원시 트랜스포머보다 바이트당 사료 포워드 FLOP를 훨씬 더 적게 사용하는 동시에 훨씬 더 많은 매개변수를 가지고 있기 때문에 필요하다. 표 1은 실험에 사용된 메가 바이트 및 만바 바이트 모델 크기를 보여준다. 다양한 모델 아키텍처에 대한 (포워드 패스) FLOP 연산 및 사용된 연관된 하이퍼파라미터는 부록 B에 자세히 설명되어 있다.\n' +
      '\n' +
      '모든 모바 바이트 모델은 오픈 소스 만바 코드 베이스.2를 사용하여 훈련되었으며 훈련에서는 무작위 위치에서 시작하여 문서(8,192\\) 바이트(문서당 1개)의 연속 서열을 삽입하고 사용했다. 우리는 스케일에서의 훈련 효율화를 위해 BF\\(16\\)를 이용한 혼합 정밀 교육을 가능하게 한다. 최적화기, 학습률 스케줄러 및 기타 훈련 세부 사항은 부록 C에 명시되어 있다.\n' +
      '\n' +
      '뿌리 2: [https://github.com/state-spode/mamba] (https://github.com/state-spode/mamba)\n' +
      '\n' +
      '압류 등(2021)은 추론 중에 성능을 위해 속도를 차단하기 위해 슬라이딩 윈도우를 사용하는 것을 제안했다. 그 후, 우리는 표 3의 최첨단 서브워드 모델과 비교할 때 길이 \\(L_{\\text{ctx}}/2\\)의 바이트 서열에 대해 슬라이딩 윈도우(L_{\\text{ctx}/2\\)를 사용한다.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '표 2는 각 데이터 세트에 걸쳐 바이트당 비트(\\(\\mathrm{BPB}\\))를 보여준다. 이 실험을 위해 메가 바이트-\\(758\\)M+\\(262\\)M 및 만바 바이트 모델은 바이트당 동일한 수의 FLOP를 사용한다(표 1 참조). 우리는 모든 데이터 세트에 걸쳐 매바 바테를 일관되게 능가하는 것을 관찰한다. 또한, 우리는 금전적 제약으로 인해 전체 \\(80\\)B 바이트에 대해 몰바 바테를 훈련시킬 수 없었지만, 몰바 바테는 \\(0.63\\)로 메가 바테를 능가하여 계산 및 훈련 데이터를 덜 훈련시킨다는 점에 주목한다. 또한, 몰바 바이트-\\(353\\)M도 바이트 수준의 트랜스포머와 피르시버AR을 능가한다.\n' +
      '\n' +
      '마바 바테는 몇 번의 훈련 단계에서 훨씬 더 큰 모델보다 더 나은 성능을 발휘하는가? 그림 1은 동일한 수의 매개변수를 가진 모델을 보고 이 관계를 추가로 탐구한다. 그래프는 동일한 파라미터 크기의 메가 바이트 모델에 대해 입력 패칭이 적은 모델이 더 잘 수행되지만 계산되면 유사하게 수행됨을 나타낸다. 사실, 전체 길이의 트랜스포머는 절대적인 의미에서 느린 반면, 계산 정상화될 때 메가 바이트와도 유사하게 수행된다. 대조적으로, 만바 아키텍처로의 전환은 계산 사용량과 모델 성능을 모두 유의하게 향상시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline \\multirow{2}{*}{Experiment} & \\multirow{2}{*}{Models} & FLOPs per \\\\  & & train byte \\\\ \\hline Medium- & MegaByte-\\(758\\)M+\\(262\\)**M** : & \\(1.02:1\\) \\\\ scale & MambaByte-\\(353\\)M & \\\\ \\hline Large- & MegaByte-\\(1.3\\)B+\\(350\\)**M** : & \\(0.54:1\\) \\\\ scale & MambaByte-\\(972\\)M & \\\\ \\hline  & MegaByte-\\(1.3\\)B+\\(218\\)**M** : & \\(0.40:1\\) \\\\  & MambaByte-\\(972\\)M & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** 상대적 훈련 FLOP는 모델 크기*** 모든 메가 바이트 모델은 \\(8\\)의 패치 크기를 사용한다.\n' +
      '\n' +
      '이러한 발견에 따라 표 3은 PG19 데이터 세트에서 이러한 모델의 더 큰 버전을 비교한다. 이 실험을 위해 메바 바이트-\\(972\\)M과 메가 바이트-\\(1.3\\)B+\\(350\\)M 및 기타 바이트 수준 모델을 비교하고 여러 최첨단 서브워드 모델을 비교한다. (\\mathrm{BPB}\\)에서 주위성(\\(\\mathrm{PPL}\\))으로의 전환은 부록 E에 자세히 설명되어 있다. MambaByte-\\(972\\)M이 \\(150\\)B 바이트에 대해 훈련된 경우에도 모든 바이트 수준의 모델을 능가하고 하위 단어 모델로 경쟁 성능을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & \\begin{tabular}{c} Bytes \\\\ trained \\\\ \\end{tabular} & Context & \\begin{tabular}{c} Test \\\\ \\(\\mathrm{BPB}\\) \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} Generation \\\\ time (s) \\\\ \\end{tabular} \\\\ \\hline Transformer-\\(350\\)M & \\(-\\) & \\(1,024\\) & \\(1.064\\) & \\(132\\) \\\\ MegaByte-\\(1.3\\)B+\\(218\\)M (patch: \\(8\\)) & \\(-\\) & \\(8,192\\) & \\(0.991\\) & \\(93\\) \\\\ \\hline MegaByte-\\(1.3\\)B+\\(218\\)M (patch: \\(8\\))6  & \\(-\\) & \\(8,192\\) & \\(-\\) & 265 \\\\ MambaByte-\\(972\\)M & \\(75\\)B\\({}^{*}\\) & \\(8,192\\) & \\(\\mathbf{0.883}\\) & \\(\\mathbf{29}\\) \\\\ w/ sliding window (\\(2\\times\\) bytes) & & & \\(\\mathbf{0.863}\\) & \\(\\mathbf{58}\\) \\\\ MambaByte-\\(1.6\\)B & \\(-\\) & \\(8,192\\) & \\(-\\) & \\(36\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: ** 생성 속도 벤치마킹**¨을 사용하여 \\(8,192\\) 바이트를 생성했으며, \\(-\\)가 표시된 분야는 알려져 있지 않다. (우퍼) PG19의 \\(\\mathrm{BPB}\\)와 트랜스퍼러와 메가 바테의 생성 시간은 유 등(2023)에서 가져온다. (하)메가 바이트와 만바 바테는 같은 하드웨어로 실행됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c} \\hline \\hline \\multirow{2}{*}{Byte-level model} & \\multirow{2}{*}{Context} & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} Bytes \\\\ trained \\\\ \\end{tabular} } & \\multicolumn{4}{c}{Test \\(\\mathrm{BPB}\\downarrow\\)} \\\\ \\cline{3-8}  & & & PG19 & Stories & Books & ArXiv & Code \\\\ \\hline Transformer-\\(320\\)M & \\(1,024\\) & \\(80\\)B & \\(1.057\\) & \\(1.064\\) & \\(1.097\\) & \\(0.816\\) & \\(0.575\\) \\\\ PerceiverAR-\\(248\\)M & \\(8,192\\) & \\(80\\)B & \\(1.104\\) & \\(1.070\\) & \\(1.104\\) & \\(0.791\\) & \\(0.546\\) \\\\ MegaByte-\\(758\\)M+\\(262\\)M (patch: \\(8\\)) & \\(8,192\\) & \\(80\\)B & \\(1.000\\) & \\(0.978\\) & \\(1.007\\) & \\(0.678\\) & \\(0.411\\) \\\\ MambaByte-\\(353\\)M & \\(8,192\\) & \\(30\\)B\\({}^{*}\\) & \\(\\mathbf{0.930}\\) & \\(\\mathbf{0.908}\\) & \\(\\mathbf{0.966}\\) & \\(\\mathbf{0.663}\\) & \\(\\mathbf{0.396}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: ** 중간 규모의 실험** 메가 바이트 및 만바 바테는 바이트당 동일한 FLOP를 사용한다. 트랜스포머, 퍼세르시버AR 및 메가 바테에 대한 \\(\\mathrm{BPB}\\)는 유 등(2023)에서 가져온다.\n' +
      '\n' +
      '텍스트 생성은 트랜스포머 모델의 자기회귀 추론은 전체 맥락을 캐싱해야 하며, 이는 생성 속도에 상당한 영향을 미칠 수 있다. 앰바 바테는 시간이 지남에 따라 진화하는 층당 단일 히든 상태를 유지하여 생성 단계당 일정한 시간을 가능하게 하기 때문에 이 병목 현상을 겪지 않는다. 표 4는 A100 80GB PCIe GPU에서 엠바 바이트-\\(972\\)M 및 모바 바이트-\\(1.6\\)B의 텍스트 생성 속도와 메가 바이트-\\(1.3\\)B+\\(350\\)M을 비교한다. 메가 바테는 패칭으로 발전 비용을 크게 줄이는 반면, 모바 바테는 재발성 세대의 사용으로 인해 매개변수 일치 설정에서 더 빨리 \\(2.6\\\\)로 관찰한다. 어플리케이션스(F)는 생성 과정에 대한 더 많은 정보를 포함한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '긴 바이트 서열을 모델링하기 위한 토큰이 없는 SSM인 몰바 바테를 소개합니다. 앰바 바테는 여러 데이터 세트에 걸쳐 다른 바이트 수준 모델을 능가하고 서브워드 트랜스포머와 경쟁적인 결과를 보여 유망한 토큰화 대안으로 작용한다. SSM은 또한 반복적인 특성으로 인해 상당히 빠른 텍스트 생성을 가능하게 하여 바이트 모델을 실용화한다. 우리의 연구 결과는 향후 대형 모델에서 토큰이 없는 언어 모델링의 가능성을 확립한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 바스완이 등은 (2017) 아샤시 바소와이, 노암 샤제리, 니키 파마르, 작노 우즈코레이트, 리온 존스, 디단 노 고메즈, 루카즈 카이저, 일리아 폴로숙신 등이 있다. . __의 귀추가 주목된다. _의 귀추가 주목된다. 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* 수 등은 (2021) 지아니린 수, 유루, 선펑 판, 보원, 윤펑 류 등이 있다. 로포름러: 로터리 위치 임베딩이 있는 __Roformer: 회전식 변압기: 회전식 위치 임베딩이 있는 강화 변압기. arXiv e- 프린팅_, 페이지 arXiv-2104 2021.\n' +
      '*유 등은 알(2023) 루리유, 다니엘 시미그, 콜린 플러헤티, 아르메드 애가잔, 루크 제트렘요어, 마이크 루이스 등이 있다. 메가 바이트: 멀티플로일 트랜스퍼가 있는 100억 바이트 서킷을 예측하세요. Nural 정보 처리 시스템__30-seventh 콘퍼런스, 2023년 URL[https://openopenreview.net/forum?id=JTm02V9Xpz] (https://openopenreview.net/forum=JTm02V9Xpz) 네이션 정보 처리 시스템_에서.\n' +
      '* Mehta 등은 (2023) 호스 메하타, 안키트 구타, 애쇼크컷코스키, 베남네시스카우 등이 있다. 길드 스테이트 스폿을 통해 모델화하는 긴 풋 언어입니다. Eleventh 국제 학습 발표회_, 2023년 URL[https://openreview.net/forum?id=5MkYIYCbva](https://openreview.net/forum=5MkYIYCbva)에서 학습 설명회.net/forum?\n' +
      '* 벤지오 등은 (2000) 요슈아 벤지오, 레잔 도차르메, 파스칼 빈센트 등이 있다. 신경 안정화 언어 모델 __신경 안정화 언어 모델. __ 신경 안정화 언어 모델. 신경 정보 처리 시스템_, 13, 2000의 기능.\n' +
      '* 슈스터와 나카지마(2012) 마이크 슈스터와 카이스키 나카지마. 일본어와 한국 음성 검색. ICASSP)__2012 IEEE 음향, 음성 및 신호 처리(ICASSP)_ 페이지 5149-5152. IEEE. 2012.\n' +
      '* 세니히 등 (2015)리코 제니히, 바리 하도, 알렉산드라 비르흐. 레어 워드들의 신경 기계 번역은 서브워드 유니트가 있는 레어 워드들의 __신경 기계 변환이다. arXiv 프리프린트 arXiv:1508.07909_ 2015.\n' +
      '* 우 등(2016) 용희 우, 마이크 슈스터, 지펑 첸, 퀀펑 크레인, 모하마드 노우지, 볼프강 마카오, 맥스쿤, 막심 켈룬, 원카오, 진가오, 클루우스 마크로시 등 구글의 신경 기계 번역 시스템 : 휴먼과 기계 번역 사이의 갑을 브라이딩한다. arXiv 프리프린트 arXiv:1609.08144_ 2016.\n' +
      '* 왕 등 (2020) 창한왕, 경윤 조, 지아토 구. 바이트 레벨 암호가 있는 신경 기계 번역입니다. 인공 지능_에 대한 AAAI 회의의 _발표에서 부피 34, 페이지 9154-9160, 2020.\n' +
      '* 가오 등은 (2020a) 빙창가오, 니콜라 이 니콜로프, 유황후, 리처드 HR 하멜러이다. 자기 의사를 가진 특성 수준 변환입니다. 종합교통협회 제58차 연차회의 _검토에서 1591-1604쪽 2020a.\n' +
      '*Xue et al.(2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam 로버츠 및 Colin Raffel. T5: 비교예. _ 토워드는 미리 훈련된 바이트 대 바이트 모델로 토큰이 없는 미래를 제공한다. 컴퓨터 로직_, 10:291-306, 2022년 협회의 거래.\n' +
      '* 클락 등은 (2022) 조나단 H 클락, 단 가렛, 이울리아 터크, 존 위엣잉 등이다. 캐나인: 언어 제시를 위한 효율적인 토벌화-자유 엔코더를 사전 훈련한다. __언어 제시를 위한 효율적인 토벌화-자유 엔코더를 교육한다. 컴퓨터 언어학협회, 2022년 10:73-91의 거래.\n' +
      '* 브라운 등 (2020) 톰 브라운, 벤자민 맨, 니크 리더, 멜라노이 수비아, 자르드 디 카플란, 프라풀라 다하리왈, 아빈드 네나칸탄, 프라빈드 뉴하칸탄, 프라나브 시흐람, 기리시 스스티스, 아미다 아사셀, 언어모달은 Few-Shot Learner이다. 신경 정보 처리 시스템_, 2020년 33:1877-1901의 발전이다.\n' +
      '아데냐 스켈레위, 아말리아누에, 아흐네르와, 아네네르, 아네우우, 아네우우, 아네우우, 아네우우, 아흐네우, 아흐네우, 카네우우, 라흐네르스, 아흐네우, 마흐네르네, 주니니 크라누, 후미, 다케나 루이, 아흐네르와, 아흐네르와, 아흐네르네, 아흐네르네, 아흐네르네, 아흐네르네, 아흐네르네, 아흐네르네, 아흐네르네, 아흐네르네, 아에에로, 아흐네르네우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우우, 아에에, 아흐네르네, 아에에, 아흐네르네, 아에에, 아흐네르네, 아에에, 아흐네르네, 아에에, 아에에, 아에에, 라마 2:오픈 재단과 파인테드 차트 모델 2023.\n' +
      '* 투브론 등 (2016)수산 장, 스테판 롤러, 남안 고이살, 미켈 아르테크스, 모야 첸, 슈오의 첸, 크리스토퍼 듀란, 몬아 디바, 시안 리, 시 빅토리아 린 등은 OPT: 오픈 프레트레이닝된 트랜스폼어 모델. arXiv 프리프린트 arXiv:2205.01068_, 2022.\n' +
      '*다이 등은 (2020) 장다이, 구쿤라이, 예밍양, 퀘크라이 등이 있다. 분무-변조: 효율적인 언어 처리를 위한 필수 적강도를 필터링한다. 신경 정보 처리 시스템_, 2020년 33:4271-4282의 발전이다.\n' +
      '*나와롯 et al.(2022) 피엇 나왈롯, 시지몬 타워크로스키, 미칼 타이롤스키, 루카즈 카이저, 유후이 우, 크리스티안 세제금 및 헨리크 미칼레세키. 계층적 변환기, 모어 효율적인 언어 모델입니다. 컴퓨팅 기법 협회의 _ 찾음: NAACL 2022_ 페이지, 2022년 페이지 1559-1571.\n' +
      '* 구앤다오(2023) 앨버트 구와 트리다오. 삼바: 선-시간 검증 모델화는 선택 국가 공간으로 모델링된다. __ arXiv 프리프린트 arXiv:2312.00752_, 2023.\n' +
      '* 구 등은 알버트 구(2021)와 카라 고엘, 크리스토퍼 레 등이 있다. 구조화된 스테이트 스폿으로 롱 세크를 효율적으로 모델링하는 __ 구조화된 스테이트 스폿으로 롱 세크를 효율적으로 모델링한다. arXiv 프리프린트 arXiv:2111.00396_ 2021.\n' +
      '* 구파 등 (2022) 아나킷 구타, 앨버트 구, 조나단 베란트 등이다. 대각 국가 분야는 구조화된 국가 공간만큼 효과적입니다. __ 구조화된 국가 지역만큼 효과적입니다. 신경 정보 처리 시스템_, 2022년 35:22982-22994의 발전입니다.\n' +
      '* 구 등은 알버트 구(2022)와 카라 고엘, 안킷 구파, 크리스토퍼 레 등이 있다. 다이각형 스테이트 스페이스 모달스의 파라파라미터화 및 인타프린팅에서 __. 신경 정보 처리 시스템_, 2022년 35:35971-35983의 발전입니다.\n' +
      '* 스미스 등은 (2023) 지미 T.H. 스미스, 앤드루 워링턴, 스콧 린더맨 등이 있다. 모델링을 위한 단순화된 스테이트 스페이스 플레이어입니다. Eleventh 국제 학습 발표_, 2023년 URL[https://openreview.net/forum?id=Ai8Cre3AXqks] (https://openopenreview.net/forum?Ai8ape3AXqks)에서 학습 설명_, 2023. URL[https://openreview.net/forum?\n' +
      '* 라마차란 등은 (2017) 프라지트 라마차란란, 바렛 조프 및 Quoc V Le. 활성화 함수 검색. __ 활성화 함수 검색. __ 활성화 함수 검색. arXiv 프리프린트 arXiv:1710.05941_ 2017.\n' +
      '* 버렐로치(1990) 가이 에블로치. 프리픽스 스미스 및 테어 앱입니다. 1990년 노브(CMU-CS-90-190) URL[https://www.cs.cmu/~guyb/papers/Ble93pdf](https://www.cs.cmu/~guyb/papers/Ble93pdf).\n' +
      '* 라 등은 (2020) 잭 W. 래, 안나 Potapenko, Siddhant M. 자야쿠마르, 차로에 힐리에, 티모티 P.릴리케이랩. 롱 오렌지 쿼스 모델링을 위한 압축 트랜스포머입니다. 학습 발표__국제회의에서는 2020년 URL[https://openopenreview.net/forum?id=SylKikSYDH](https://openopenreview.net/forum?id=SylKikSYDH)에서.\n' +
      '* Trinh & Le(2018) Trieu H. Trinh 및 Quoc V. 어. 콘센스 컨슈머링에 대한 심플 방법 __. arXiv 프리프린트 arXiv:1806.02847_ 2018.\n' +
      '* 가오 등은 (2020) 레오 가오, 스텔라 비둘란, 사이드 블랙, 로렌스 그폴딩, 트래비스 호페, 찰스 포스터, 제이슨 프랑, 호라스 하이, 안시 티이트, 노아 나비시마, 섀슨 프레스러, 콘노르 레아히 등이 있다. 파일: 어학모델링을 위한 디버스텍스트의 Pile: An 800GB Dataset. __ arXiv 프리프린트 arXiv:2101.00027_, 2020b.\n' +
      '* 프레스 등은 (2021) 미르 프레스, 노아 스미스, 마이크 루이스 등이 있다. 짧은: 스쇼터 인풋을 사용한 베터 언어 모델링. 제59차 컴퓨터 통계협회 연차 회의 및 제11차 자연 언어 처리에 관한 국제 공동 회의(1: 롱 파이어스)_, 5493-5505쪽, 2021년 8월 온라인 컴퓨터 통계 협회의 개시. 10.18653/v1/2021.27 URL[https://aclanthology.org/2021.acl-오랜.427](https://aclanthology.org/2021.acl-오랜.427).\n' +
      '* 로이 등은 알(2021) 아우코 로이, 모하마드 사파르, 아슈 비스완리, 데이비드 그랑에 등이 있다. 아웃링 트랜스포머에 대한 효율적인 콘텐츠 기반 응답. __ Routing Transformers와의 효율적인 콘텐츠 기반 응답. 컴퓨터 모형_, 9:53-68, 2021. 도이: 10.1162/tacl_a_00353 URL[https://aclanthology.org/2021.tacl-1.4](https://aclanthology.org/2021.tacl-1.4])의 상호 작용.\n' +
      '(2022) 커티스 하세스토르네, 앤드루 재게르네, 세바스티나 코르체나, 세바스티안 보라기, 샤를리 나시, 마테우스즈 말리노프스키, 세더더 디네즈, 오리올 비네스, 마테와 보트비닉, 이안 시몬, 한나 쉬한, 닐 지그니다르, 장-베티스트 알레이크, 조오 카레라, 제네 등은 샤를리보르네, 마테와 보리안 베이지, 마테와 보리안 비네, 마테와 보른, 마테와 보테비단, 한나 시아한, 닐 시미다, 네네, 안드위니아 시네, 한나 시지드, 한, 닐 시미다한, 닐 시지드, 네네, 자네, 요오 카레고라, 조오 카레고라, 조오카이라, 자네, 자네, 자네, 자네, 자네, 자네, 자네, 자네, 자네, 자네, 자네, 조오 카레고라, 자네, 자 일반 목적, 롱-컨텍스트 자기회귀 모델링은 Perceiver AR과 함께 모델링된다. 제39차 기계학습에 관한 국제회의에서의 _프로칭 머신 러닝 연구_의 부피 162, 페이지 8535-8558. PMLR, 17-23 줄 2022. URL[콘솔레프레스/v162/hawthne22a.html] (https://proing/v162/hawthne22a.html].ml.ml.ml.ml.ml.ml.ml.\n' +
      '* 크라이즈헤프스키 등은 (2015)데레슬리 허친스, 이마놀 슈라그, 유후이 우, 에탄 다이어, 베남 네오사베르 등이 있다. 블록-재현 변환기 __블록-재전류 변환기 __블록-재전류 변환기. 신경 정보 처리 시스템_, 2022년 35:33248-33261의 발전입니다.\n' +
      '* 헨드라이브, 김펠(2016) 단 헨드롭, 케빈 김펠. 가우시안 오차 선형 단위 __ 가우시안 오차 선형 단위(겔루스). arXiv 프리프린트 arXiv:1606.08415_ 2016.\n' +
      '* 오르비에토 등은 (2023) 안토니오 오르비토, 사마누엘 L 스미스, 알버트 구, 아나산 페르난도, 카게라 굴체르, 라잔 파스칸루, 소함 데 등이 있다. 장기 검사용 신장 신경네트네트웍스 __ 장기 검사용 리큐베이션 리버스 신경네트웍스. arXiv 프리프린트 arXiv:2303.06349_, 2023.\n' +
      '* 구 등은 알버트 구(2023)와 존슨, 아만 탐살리나, 아트리 루드라, 크리스토퍼 레 등이 있다. HiPPO: 일반화된 오리엔티시스 프로젝트와 함께 국가 우주 모델을 추적하는 방법. 학습 발표_, 2023년 국제 컨퍼런스에서 URL[https://openreview.net/forum?id=klK17QQ3KB](https://openreview.net/forum?klK17QQ3KB).\n' +
      '* 응우옌 등은 Eric Nguyen, 카라 고엘, 알버트 구, 고든 다운스, 프레이 샤, 트리 다오, 스티븐 바쿠스, 크리스토퍼 Re. S4ND: 이미지 및 비데스를 스테이트스페이스가 있는 멀티차원 시그니처로 모델링한다. __. 신경 정보 처리 시스템_, 2022년 35:2846-2861의 발전이다.\n' +
      '* 홀츠만 등은 (2020) 아리 홀츠만, 얀 부이스, 리두, 맥스웰 포브스, 예진 최씨 등이다. 신경학적 텍스트 재현의 놀라운 사례. 학습 발표__국제회의에서 2020년 URL[https://openreview.net/forum?�rygGQyrFvH](https://openopenreview.net/forum?id=rygGQyrFvH).\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:10]\n' +
      '\n' +
      'Dataset specifics\n' +
      '\n' +
      '**Text 데이터셋 통계*** 데이터셋 데이터셋 통계량.** 데이터셋별 총 바이트, 총 문서 및 평균 문서 크기(문서당 비트)를 포함한다.\n' +
      '\n' +
      '다양한 긴 형태의 텍스트 데이터 세트에 대한 결과를 벤치마킹합니다. PG19 데이터세트[14]는 프로젝트 구텐베르크 온라인 라이브러리에서 전체 길이의 영어 서적(1919\\ 이전에 작성됨)의 광범위한 모음이다. PG19 데이터 세트는 장거리 컨텍스트 모델링 [13]에 대해 테스트하기에 이상적입니다. 스토리 데이터세트[15]는 커먼센스 추론 및 언어 모델링에 사용되는 커먼크로스 데이터의 서브세트이다. 책 데이터셋[13]은 영어 책의 또 다른 모음입니다. ArXiv 데이터 세트[13]는 arXiv 온라인 아카이브에서 나옵니다. 마지막으로, 코드 데이터셋[13]은 공개적으로 이용 가능한 오픈 소스 코드(아피시, MIT 또는 BSD 라이선스 아래)의 큰 데이터세트이다. 데이터베이스 통계량은 표 5에 표로 표기되어 있다.\n' +
      '\n' +
      'PG19 데이터 세트의 경우 Rae et al. [20]에 표시된 대로 열차, 검증 및 테스트 데이터를 사용한다. 스토리, 북, ArXiv 및 코드 데이터 세트의 경우 테스트용 \\(40\\)M 연속 바이트와 나머지는 몰바 바테를 훈련시키기 위해 무작위로 샘플한다.\n' +
      '\n' +
      '적용자 B 컴플라이언스입니다.\n' +
      '\n' +
      '앞서 언급한 바와 같이, 우리는 계산 제어 환경에서 만바 바테를 평가하고 벤치마킹한다. 이를 위해 다양한 바이트 수준의 모델 아키텍처가 발생하는 바이트당 FLOP를 추정한다. (글로벌/지방) 잔류 스트림(글로벌/지방)의\\(d\\(d_{g}/d{g}/d{l}\\)) 수, 치수 \\(d_{g}/d{l}\\))를 사용하여 아키텍처를 매개변수화한다. 우리는 또한 입력 컨텍스트에 \\(L_{\\text{ctx}}\\) 바이트를 포함한다. 전방 패스들에 대한 상세한 구성 요소별 계산 계수는 표 6에 포함된다.\n' +
      '\n' +
      '중간 규모의 언어 모델링 실험(표 1, SS5, 유 et al.(2023])의 경우 유 등은 메가 바이트-\\(758\\)M+\\(262\\)M 모델을 사용하는데, 맥락 길이는 \\(8,192\\)이고 패치 크기는 \\(80\\)B 바이트로 훈련되었다. 그림 5에서 볼 수 있듯이 모바 바이트-\\(353\\)M(n=53\\), \\(d=1,024\\), \\(e=2\\)) 및 메가 바이트-\\(758\\)M+\\(262\\)M은 FLOP에서 동일한 총 계산기를 사용하므로, 우리는 몰바 바이트-\\(758\\)M을 사용하여 SS4의 표 2의 메가 바이트-\\(758\\)M+\\(262\\)M에 대해 벤치마킹한다.\n' +
      '\n' +
      'M메가 바이트-\\(1.3\\)B+\\(350\\)M(8,192\\)M(8\\) 및\\(8\\)의 패치 크기를 사용하여 PG19 스케일링 실험의 경우(표 2, SS5 및 아펜딕스 D.3) 관찰된 최신 단어 수준을 벤치마킹했다. 하드웨어 한계로 인해 마바 바이트-\\(972\\)M(n=48\\), \\(d=1,792\\), \\(e=2\\)) 및 사용된 총 계산량에 대한 제어(관련 계산 비용을 보기 위해 그림 5 참조)를 훈련시킨다. 이 작업에 사용된 모든 모델 크기와 관련 하이퍼파라미터는 표 7에 표로 표시된다.\n' +
      '\n' +
      '부록 C 훈련.\n' +
      '\n' +
      '이 연구의 모든 모델은 \\(\\beta=(0.9,0.95)를 사용한 AdamW 최적기를 사용하여 훈련되었다. 우리는 코사인 어닐링에 이어 선형 학습률 평가(첫 번째\\(500\\) 단계를 사용했다. 일관적으로 일관됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Total bytes & Total docs & \\(\\text{Bytes}/\\text{doc}\\) \\\\ \\hline PG19 & \\(11.74\\)G & \\(28,752\\) & \\(4,082,210\\) \\\\ Stories & \\(34.18\\)G & \\(948,247\\) & \\(36,045\\) \\\\ Books & \\(108.38\\)G & \\(196,640\\) & \\(551,179\\) \\\\ ArXiv & \\(60.27\\)G & \\(1,264,405\\) & \\(47,665\\) \\\\ Code & \\(677\\)G & \\(56,626,342\\) & \\(11,958\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **Text 데이터셋 통계** 각 데이터셋에 대한 총 바이트, 총 문서 및 평균 문서 크기(문서당 비트)를 참조한다.\n' +
      '\n' +
      '그림 4: **Gated-S4D 블록*** Mehta et al. [2023]에서 매핑되었으며,\\(\\varphi\\)는 GELU 활성화 [1]을 나타낸다.\n' +
      '\n' +
      '메가 바이트 훈련(유 등 2023)과 함께 모든 실험에 걸쳐 배치 크기의 \\(48\\)를 사용했다. 또한, 우리는 어떤 모델도 드롭아웃을 사용하지 않습니다.\n' +
      '\n' +
      '그림 1의 실험을 위해 \\(0.0002\\), \\(0.0006\\), \\(0.0008\\)의 피크 학습률을 사용하여 하이퍼파라미터 검색을 수행하고 모든 모델에 대해 구배 규범을 \\(1.0\\)에 닫았다. 각 모델에 대해 가장 잘 관측된 성능 곡선은 그림 1에 보고되어 있으며, 우리는 Layer 정규, 회전 위치 인코더(Su et al, 2021) 대신 RMS 정규을 사용하는 개선된 트랜스폼 레시피와 편향 없이 선형 항(유 et al, 2023)을 사용한다.\n' +
      '\n' +
      '표 2에 표시된 중간 규모 실험에서 피크 학습률을 \\(0.0004\\)로 설정하고 구배 규범을 \\(0.1\\)로 닫았다. 우리는 \\(80\\)K 단계와 동일한 총 \\(80\\)K 단계에 대해 만바 바이트-\\(353\\)M을 훈련했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Model & Component & FLOPs per byte \\\\ \\hline Transformer & Multi-head attention & \\(2n(4d^{2}+2L_{\\text{ctx}}d)\\) \\\\\n' +
      '[Vaswani et al., 2017] & Pointwise feed-forward & \\(2n(2ed^{2})\\) \\\\ \\hline \\multirow{4}{*}{MegaByte [Yu et al., 2023]} & Embedding projection & \\(2d_{g}^{2}\\) \\\\  & Global transformer model & \\(2n_{g}(4d_{g}^{2}+2d_{g}L_{\\text{ctx}}/p+2ed_{g}^{2})/p\\) \\\\  & Global-to-local projection & \\(2d_{g}d_{l}\\) \\\\  & Local transformer model & \\(2n_{l}(4d_{l}^{2}+2pd_{l}+2ed_{l}^{2})\\) \\\\ \\hline \\multirow{4}{*}{Gated-S4D (Figure 4)} & Linear projections & \\(2n(3ed^{2}+d^{2})\\) \\\\  & Kernel via Vandermonde \\(v(\\overline{\\Lambda})\\) & \\(n(\\alpha_{\\text{r}}ed(n_{\\text{state}}+L_{\\text{ctx}})\\log_{2}^{2}(n_{\\text{ state}}+L_{\\text{ctx}})/L_{\\text{ctx}})\\) \\\\  & S4D SSM with convolution & \\(n(\\alpha_{\\text{fl}}\\log(L_{\\text{ctx}})ed+ed)\\) \\\\  & Element-wise gating & \\(ned\\) \\\\ \\hline \\multirow{4}{*}{MamboByte (Figure 3)} & Linear projections & \\(2n(3ed^{2})\\) \\\\  & Pre-SSM 1D convolution & \\(2nked\\) \\\\  & \\(\\Delta,\\mathrm{B},\\mathrm{C}\\) from input \\(x\\) & \\(2n(2edr+2edn_{\\text{state}})\\) \\\\  & Discretization, pre-scan: \\(\\overline{\\Lambda}\\), \\(\\overline{\\mathrm{B}}x\\) & \\(n(3edn_{\\text{state}})\\) \\\\  & Recurrence with parallel scan & \\(n(edn_{\\text{state}})\\) \\\\  & Output: \\(y=\\overline{\\mathrm{C}}h+\\overline{\\mathrm{D}}x\\) & \\(2nedn_{\\text{state}}+ned\\) \\\\  & Element-wise gating & \\(ned\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **컴퓨션(포워드 패스) 추정치는 다양한 바이트 수준의 언어 모델에 대한 추정치** 엠블로그, 디블로그, 편향, 비선형, 레이어 규범과 같은 하위 선도 용어들이 생략된다. (\\(\\알파_{*}\\)는 구현 특정 상수 스케일링 용어를 나타낸다.\n' +
      '\n' +
      '그림 5: ** 컴퓨터 가격은 다양한 규모에서 서로 다른 모델 아키텍처에 대한 계산 비용*** 모든 모델은 \\(8,192\\)의 컨텍스트 길이를 사용하고 메가 바이트 아키텍처는 \\(8\\)의 패치 크기를 사용한다.\n' +
      '\n' +
      'PG19에 대한 대규모 실험에서 우리는 중간 규모 실험에서 피크 학습률이 \\(0.0004\\)로 설정되고 구배 규범이 \\(0.1\\)에 충돌하는 것과 유사한 설정을 사용한다. 캄바 바이트-\\(972\\)M은 \\(380\\)K 단계에 대해 훈련되며, 이는 \\(380,000\\) 48\\(8,192\\ 승인 150\\)B 바이트와 동일하다.\n' +
      '\n' +
      '확인 및 선정.\n' +
      '\n' +
      '순환은 모델 정규화(Orvieto et al, 2023; Gu et al, 2023) 및 해상도 불변(Nguyen et al, 2022)과 같은 바람직한 특성을 허용하는 연속 시간 시스템과 깊은 연결을 가지고 있다. 이 절에서는 선택적 SSM의 제로차 홀드 디스펜싱이 재발성 네트워크에서 게이팅 메커니즘의 일반화로 볼 수 있는 방법을 보여준다.\n' +
      '\n' +
      '(t_{k}=.{k})\\(t_{k} <\\-sum_{k} < <{k}>\\)에 대한 \\(t_{k}\\)의 경우, \\(x_{k})의 경우, \\(x_{k}. 관련 이산 SSM의 결과 행렬은:8이다.\n' +
      '\n' +
      '부타 8: 핀바(구 및 다오, 2023), \\(\\mathrm{B}\\)는 \\(\\mathrm{B}\\)보다 더 중요한 \\(\\mathrm{A}\\)의 경험적 관찰에서 단순화된 오일러(0차 홀드에 반대)를 통해 폐기되며, 성능은 \\(\\mathrm{B}\\)에서 단순화로 크게 변하지 않는다.\n' +
      '\n' +
      '\\(\\quad\\\\mathrm{A}})\\[\\mathline{\\mathrm{A}} = \\mathrm{A}^{A}^{A}^{-1}(\\mathrm{A}},\\mathrm{A}},\\mathrm{A},\\mathrm{A})\\,\\mathrm{A}(\\mathrm{A})\\,\\mathrm{A}(\\mathrm{A}/\\mathrm{A}:\\mathrm{A}/\\mathrm{A}.^{A}(\\mathrm{A}/\\mathrm{A})}(\\mathrm{A}(\\mathrm{A})}(\\mathrm{A}(\\mathrm{A})}(\\mathrm{A})}(\\mathrm{A})}(\\mathrm{A}(\\mathrm{A})}(\\mathrm{A}(\\mathrm\n' +
      '\n' +
      '재발성 네트워크에서 선택 역학 및 게이팅(2023)은 선택적 SSM이 \\(\\Delta=\\mathrm{softplus})(z(x))=\\mathrm{softplus}(W_{\\Delta}(W_{R}x))(SS2의 (3)에 표시된 것)을 설정하여 게이팅 재발로 실현될 수 있다는 점에 주목한다. 저자들은 \\(\\mathrm{A}=-1\\), \\(\\mathrm{B}=1\\), \\(n=1\\)를 복용함으로써 관찰한다.\n' +
      '\n' +
      'MS(\\mathrm{A}} =\\mathrm{A}\\mathrm{A}\\mathrm{A}\\mathrm{A}\\,\\mathrm{A}\\,\\mathrm{A}})\\,\\mathrm{A}\\,\\mathrm{B}}=\\mathrm{A}\\,\\mathrm{A}\\,\\mathrm{A}\\,\\mathrm{A}\\mathrm{A}\\mathrm{A}\\mathrm{A}\\mathrm{A}\\mathrm{A}\\mathrm{A}\\)\\,\\mathrm{A}\\,\\mathrm{A}\\,\\mathrm{A}\\,\\mathrm{A}{A}\\,\\mathrm{A}{A}\\,\\mathrm{A}{A}\\[\\mathrm{A}{A}\\.{A}\\,\\{A}{A}\\,\\{A}{A}\\ [=\\sigma(-z(x))\\] \\[=1-\\sigma[=1-\\sigma(z(x)]].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Parameters} & \\multicolumn{5}{c}{Hyperparameters} \\\\ \\cline{3-6}  & & \\(n\\) & \\(d\\) & \\(e\\) & \\(L_{\\mathrm{ex}}\\) & Others \\\\  & & \\((n_{g}/n_{l})\\) & \\((d_{g}/d_{l})\\) & & & \\\\ \\hline \\multirow{2}{*}{Transformer} & \\(320\\)M (Yu et al., 2023) & \\(22\\) & \\(1,024\\) & \\(4\\) & \\(1,024\\) & heads: \\(-\\) \\\\  & \\(350\\)M (Yu et al., 2023) & \\(24\\) & \\(1,024\\) & \\(4\\) & \\(1,024\\) & heads: \\(16\\) \\\\  & \\(361\\)M & \\(28\\) & \\(1,024\\) & \\(4\\) & \\(8,192\\) & heads: \\(16\\) \\\\ \\hline \\multicolumn{6}{l}{PerceiverAR} & \\(248\\)M (Yu et al., 2023) & \\(17\\) & \\(1,024\\) & \\(4\\) & \\(8,192\\) & latents: \\(1,024\\) \\\\ \\hline \\multirow{2}{*}{MegaByte} & \\(193\\)M+\\(177\\)M\\({}^{\\prime}\\) & \\(14/14\\) & \\(1,024/1,024\\) & \\(4\\) & \\(8,192\\) & \\(p=4,8\\); heads: \\(16/16\\) \\\\  & \\(758\\)M+\\(262\\)M (Yu et al., 2023) & \\(14/18\\) & \\(2,048/1,024\\) & \\(4\\) & \\(8,192\\) & \\(p=8\\); heads: \\(16/16\\) \\\\  & \\(1.3\\)B+\\(218\\)M (Yu et al., 2023) & \\(24/15\\) & \\(2,048/1,024\\) & \\(4\\) & \\(8,192\\) & \\(p=8\\); heads: \\(32/-\\) \\\\  & \\(1.3\\)B+\\(350\\)M (Yu et al., 2023) & \\(24/24\\) & \\(2,048/1,024\\) & \\(4\\) & \\(8,192\\) & \\(p=8\\); heads: \\(32/16\\) \\\\ \\hline \\multicolumn{6}{l}{Gated-S4D} & \\(368\\)M & \\(26\\) & \\(1,024\\) & \\(4\\) & \\(8,192\\) & \\(n_{\\mathrm{state}}=64\\) \\\\ \\hline \\multirow{2}{*}{MambaByte} & \\(353\\)M & \\(53\\) & \\(1,024\\) & \\(2\\) & \\(8,192\\) & \\(k=4;n_{\\mathrm{state}}=16;r=64\\) \\\\  & \\(972\\)M & \\(48\\) & \\(1,792\\) & \\(2\\) & \\(8,192\\) & \\(k=4;n_{\\mathrm{state}}=16;r=112\\) \\\\  & \\(1.6\\)B & \\(48\\) & \\(2,304\\) & \\(2\\) & \\(8,192\\) & \\(k=4Using \\(\\overline{\\text{A}}\\) and \\(\\overline{\\text{B}}\\) from above in the discrete recurrence (2), the selective SSM takes the form of a 1D gated recurrence:\n' +
      '\n' +
      '\\[h[k]=\\left(1-\\sigma(z(x))\\right)h[k-1]+\\sigma(z(x))x[k]. \\tag{4}\\]\n' +
      '\n' +
      '(I\\_{\\Delta\\to\\infty}h[k]=x[k]\\)과 \\(\\lim_{\\Delta\\to 0}h[k] 0}h[k]]\\)의 대형 \\(\\Delta\\-h[k]=h[k-1]\\)은 현재 입력에만 초점을 맞추고 상태를 잊어버리는 시스템의 진화를 의미한다. 대조적으로, 작은 \\(\\Delta\\) (\\(\\Delta\\to 0\\))는 무시되는 일시적인 입력을 나타낸다.\n' +
      '\n' +
      'A, B 및 C 매트릭스의***z 및 Dao(2023)의 선택성은 시스템 매트릭스 A가 \\(\\Delta\\), 즉,\\(\\overline{\\text{A}}=\\text{A}\\,\\Delta)\\을 통해서만 모델에 영향을 미친다고 주장한다. 따라서 \\(\\Delta\\)의 선택성은 A의 선택성을 보장하기에 충분하다.\n' +
      '\n' +
      '\\(\\Delta\\)의 선택성은 입력 행렬 B에서 선택성을 가능하게 하는 반면, 구와 다오(2023)는 \\(\\text{B}\\)와 \\(\\text{C}\\)을 선택적으로 만드는 것(\\{C}\\)은 내용 \\(x[k]\\) 및 진화하는 컨텍스트 \\(h[k]\\)에 기초한 보다 미세하게 제어할 수 있다고 가정한다.\n' +
      '\n' +
      '대리점 E.\n' +
      '\n' +
      '하위어 기반 언어 모델(Vaswani et al, 2017, 하와이 등, 2022, Hutchins et al., 2022)은 단어 수준 \\(\\mathrm{PPL}\\)에서 성능을 보고하며, 바이트 수준 언어 모델(Xue et al, 2022, 유 et al, 2023)은 \\(\\mathrm{BPB}\\)에서 그들의 성능을 보고한다. 의미 있는 비교를 용이하게 하기 위해 토큰 수준 모델과 비교할 때 바이트 수준 모델과 \\(\\mathrm{PPL}\\)를 벤치마킹할 때 \\(\\mathrm{BPB}\\)에서 성능을 보고한다. 이 절에서는 단어 수준 \\(\\mathrm{PPL}\\)와 \\(\\mathrm{BPB}\\) 간의 전환을 상세히 설명한다.\n' +
      '\n' +
      '기본 분할과 관계없이 주어진 데이터세트 \\(D\\)에서 정보 \\(I(D)\\의 양은 일정하다. 그냥.\n' +
      '\n' +
      'I(D) =L_{T}\\, 토큰당 표준{B}(L_{B},\\tag{5a},\\tag{5b}\\)} <\\tag{5a}>}.\n' +
      '\n' +
      'i\\(L_{T}\\) 및 \\(L_{B}\\)가 각각 토큰과 바이트에서 데이터 세트의 길이이다. (5)부터 우리는 관찰합니다.\n' +
      '\n' +
      '\\[\\mathrm{BPB}=\\frac{-\\ln(D;\\text{model})/L_{B}}{\\ln(2)}=\\frac{\\ell_{\\text{byte} }}{\\ln(2)},\\]\n' +
      '\n' +
      'HH(\\ell_{\\text{byte}}\\)가 관찰된 바이트 수준의 음의 로그 가능성 손실(\\ln\\)이다. (5)에서 우리는 또한 \\(\\mathrm{BPB}\\)에서 단어 수준 \\(\\mathrm{PPL}\\)로 전환한 다음 사항을 주목한다.\n' +
      '\n' +
      '}\\ a\\arrow\\mathrm{PPL} =\\arrow\\mathrm{PPL}} <\\frac{L_{B}} <\\frac{L_{B}}}{frac{L_{B}}} <\\frac{L_{B}}}} <\\frac{L_{B}}}{frac{L_{B}}}}{frac{L_{B}}}{L_{L_{L_{B}}}}{L_{L_{B}}}{L_{L_{B}}}{L_{L_{B}}}}{L_{L_{B}}}}}{L_{L_{B}}}}}{L_{L_{B}}}}}{L_{L_{B}}}}}}{L_{L_{B}}}}}}{L_{B}}}}}}}}{L_{B}}}}}{L_{L}}}\n' +
      '\n' +
      'PG19 데이터셋의 경우 학습 데이터보다 BPB를 최소화하고 테스트 데이터에 단어 수준 PPL을 보고하기 위해 마모보 바이트-\\(972\\)M을 훈련한다. PG19 데이터 세트에 대한 \\(L_{B}/L_{T}\\)의 회전별 값은 표 8에 표로 표시된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & \\(L_{B}\\) & \\(L_{T}\\) & \\(L_{B}/L_{T}\\) \\\\ \\hline Train & \\(11,677,824,216\\) & \\(1,973,048,393\\) & \\(5.92\\) \\\\ Validation & \\(17,733,002\\) & \\(3,007,061\\) & \\(5.90\\) \\\\ Test & \\(41,289,101\\) & \\(6,965,511\\) & \\(5.93\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: **PG19 데이터셋 통계** Split-wise UTF-\\(8\\)는 PG19 데이터셋에서 이진(L_{B}\\) 및 공간 분리 토큰 수 \\(L_{T}\\)를 암호화한다. (바이트 카운트는 새로운 라인 문자를 포함한다.) 또한 토큰 \\당 연관된 바이트(L_{B}/L_{T}\\)를 나타낸다.\n' +
      '\n' +
      '샘플은PG19.\n' +
      '\n' +
      '이 섹션에는 PG19 데이터 세트에 훈련된 만바 바이트-\\(972\\)M의 몇 가지 샘플 세대가 포함된다. 우리는 \\(p=0.98\\)(Holtzman 등 2020)를 사용한 핵 샘플링을 사용하고 총 \\(8,192\\) 바이트(주어진 컨텍스트 프리픽스를 포함한다)에 대한 연속체를 생성한다. 또한, 우리는 Rae et al.(2020)의 부록 F에 사용된 동일한 테스트 세트 프리픽스를 선택했다. 우리는 모델이 프리픽스의 스타일로 대화를 계속하고 수백 바이트 이상의 캐릭터 이름을 효과적으로 떠올릴 수 있다는 것을 관찰한다.\n' +
      '\n' +
      '마가레트 메이오******* 아기 아기 아기 아기 아기 아기 아기 아기 아기 먹는 마가레트 마이오** 마가렛 마이오***\n' +
      '\n' +
      '텍스트(\\(487\\) 바이트.\n' +
      '\n' +
      '* [noitemsep]\n' +
      '*는 대학 알프레드 하디에서도 고정된 아이디어와 높은 이상을 가진 청년이었고 자랑스러워했다.\n' +
      '* 히스 친구 지미 진크는 생각이 적고 이상도 없었고, 기뻐했고, 첫 대학 임기의 절반이 지나기 전에 지미는 자신의 마음을 꾸미거나 자신의 도덕을 연출하는 등 모든 고민을 없애왔다. 알프레드는 이 모든 것을 훨씬 더 잘 했고, 더 나아가 알프레드 LIKED가 그에게 그 즐거움을 주기 위해 그것을 친구에게 누락시켰다고 주장했다.\n' +
      '* [noitemsep]\n' +
      '*는 알프레드가 정말 친구를 이상적인 남편으로 여겼다는 것이 아니라 지미가 다르게 보였다고 인정했다. 그리고, 그것은 모두 인생에 있었다.\n' +
      '* 앤 앤드는 오랜 세월의 솔직한 동지와 무해한 약속과 버블링 친절함을 통해 알프레드를 떠났다. 그가 지미와 우정을 잊은 지 얼마 되지 않은 그의 마음으로, 지미의 이상이라는 우열을 깨달았기 때문이다.\n' +
      '\n' +
      '윌리엄 왕세자가 피해자로 전락할 것으로 예상했던 것이다. 아마도 그는 수정된 베팅 남자였고, 그 후 자연과 동료들을 희생자로 버려서 조롱하거나 위험한 방법의 팔로워가 되었을 것이다.\n' +
      '\n' +
      '이후 윌리엄 왕세자가 다시 사회에 들어가 아마도 미스 라 로슈와의 인터뷰를 구했고, 그가 로맨스 죽음의 피해자이자 자살이라는 사실을 알게 됐고, 황당한 로란 크로스를 호소받았다. 아무쪼록 알프레드의 영웅적 퇴화의 신화나 시장판에 있는 노랑견의 신화를 본 것이다. 그것은 그를 점차 괴롭혔습니다.\n' +
      '\n' +
      '그는 거의 전환자가 되었고, 미스 로제터 선비를 입찰함에 있어서 절대 잊지 않을 것이라고 말했고, 친구의 가족과 친근하지 않을 것이라고 말한 것은 알프레드였다. 지금까지 미스 라 로스가 정말 좋은 삶이 우수한 충동을 초래한다는 것을 깨달은 것은 아니지만, 후유증은 미스 라 로스의 예측의 진리를 증명했다. 지미 씨는 "김미 왕자가 복장에 다이아몬드 스타를 입고 감탄할 때 짐미의 정문을 타고 지남의 앞문을 보고 지남이 지미에게 찾아오기도 했고, 그 후 짐마가 옆집에 간다고 해서 지금 이 곳을 넘겼고, 미스 라 로스의 정신이 그곳에 있던 이후로 결혼을 해야 한다고 주장하는 경우가 많았다"며 "이 위대한 벨기에에서 먼 집을 위해 가겠다"고 말했다.\n' +
      '\n' +
      '요일이 흘러서 영광스러운 봄 아침이었습니다. 지난 3월 초 뉴욕 집에는 두툼한 가을비가 내려 잎 없는 나무들이 이상하고 이상해 보이고, 바람이 불빛의 물결처럼 울려 퍼졌다. 오후에는 천둥이 돌아다니고 가끔 젖은 거리에 대한 회색이 우거져 있고 도시 광경과 틴트는 분홍색과 붉은색, 호박색, 보라색, 일몰색의 세계로 합쳐지기 시작했다.\n' +
      '\n' +
      '저천장 기둥은 금과 보석 보물을 덮었고, 몽환하게 온화한 반빛의 큰 얼룩진 유리창들은 마법 틴트의 붉은색과 금색 줄무늬로 스스로 배열되었다. 섬세한 색상 방식은 한 번 실망스러웠음에도 불구하고 10개월 후 이 방에서 1시간 동안 확실히 영향을 받았고 적어도 오래 머물게 되었다.\n' +
      '\n' +
      '낮에는 부드러운 남풍이 열린 문을 통해 표류했고, 갑작스러운 동풍이 프랑스 미스 라 로스 침실의 창문을 열었고 천장과 탑승층 사이에 순화풍을 타고 비가 내렸다.\n' +
      '\n' +
      '알프레드 방은 그의 존재감으로 향기롭게 느껴졌습니다.\n' +
      '\n' +
      '>아니... 조금 노래하는 새야.\n' +
      '\n' +
      '즉, 케이지에 사는 것은 친구가 필요합니다.\n' +
      '\n' +
      '냉혈 미끼는 사랑으로 마음을 따뜻하게 할 수 있습니다.\n' +
      '\n' +
      '어렵게 상처를 입혔어요.\n' +
      '\n' +
      '애완한 정원에서처럼 홀 방에 있습니다.\n' +
      '\n' +
      '추우셨네요, 하루 종일. 이미 객실이 과열된 것 같았습니다. 커튼은 이미 반으로 그려졌습니다.\n' +
      '\n' +
      'She shivered.\n' +
      '\n' +
      '\'중겨울, 요일\'은 알프레드를 생각, 안일의 하얀 어깨의 스윕을 보고 얇은 면 개구리를 두드리는 등 \'7일이 있었다\'고 생각했다.\n' +
      '\n' +
      '그리고 7개는 아직 풀리지 않았습니다. 좋아, 좋은 하루, 그리스도! 이 오래된 소트에서 나와, 우리는 날아갈 거예요. 어웨이. 하나님께서 지옥에서 영혼을 쉬시면 이 넓은 생지구가 크롤링되었습니다. 어디 있어, 애?\n' +
      '\n' +
      '아닌이 기다리고 떨리며 왜 날카로운 목소리에 대해 갑자기 손에 체크북을 물어보는지 몰랐습니다.\n' +
      '\n' +
      '알프레드 감독은 "지미 점심에 대한 비용을 지불할 만큼 변화를 줘라"고 말했다.\n' +
      '\n' +
      '홀 테이블 위의 빈 의자와 커튼 아래에는 텅 빈 의자가 놓이기 전에.\n' +
      '\n' +
      '준비된 돈의 더미를 무너뜨립니다. 알프레드는 "그리고 창문 색조가 닫혀 있다"고 덧붙였다.\n' +
      '\n' +
      '"비가 닫히지 않을 것"이라고 웃었다.\n' +
      '\n' +
      '"그러나 그는 신경 쓰지 않을 것"이라며 안에 항의했다.\n' +
      '\n' +
      '알프레드는 공정한 황금 모발에 잠시 강한 인출 손을 놓았다.\n' +
      '\n' +
      '그는 "괜찮습니다"라고 동봉했다. "추간 뒤에 중앙이 없으면 4천 개를 넣고 이렇게 떨어지는 가격에 밑바닥을 닫을 수 있다"고 창문 칠 6인치를 들고 있었다.\n' +
      '\n' +
      '그 사람은 자신을 반창하는 동안 서 있었다.\n' +
      '\n' +
      '알프레드는 "하루를 살리는 것은 운이 좋다"며 "그는 보상 없이 당신을 도와준다"고 말했다.\n' +
      '\n' +
      '>형님, 너무 안 좋았어.\n' +
      '\n' +
      '아닌은 오늘 오후에 위험한 것을 언급했습니다.\n' +
      '\n' +
      '“노래하고 놀 수 있냐”고 물어봤다.\n' +
      '\n' +
      '\'아니, 아니!\' 알프레드를 주장했다. "아이 CAN\'T가 놀고 노래한다. 방은 춥다. 안에 따뜻하다.\n' +
      '\n' +
      '알프레드는 운이 좋았을 때 옷을 갈아입고 있었고, 순간적으로 알프레드는 자신의 부채를 잊었다. 언니는 테이블 위에 올려놓은 법안을 깔았고 알프레드를 갔을 때는 보지 못했으며, 기억하면서 그 마음의 틀에서 본 행위로서 다시 돌려보냈다.\n' +
      '\n' +
      '이제 알프레드는 완전히 춥고 기질적이었으며, 다른 동료에게 이동하려고 했던 의무를 조사했을 때 더 쉽게 떠올렸다. 당일 저녁 식사에서 지미, 추위, 배고픔이 방에서 길을 흔들었고, 수프 스토브에 의해 가까운 의자에 들렀을 때, 청년의 시선은 금의 플래터에 격렬하게 고정되었고, 지미의 뜨거운 요일에 대한 감상을 일깨워줬던 표현 강도로 곧바로 다른 음식에 시작되었고, 안우의 관찰감까지 불러일으켰다.\n' +
      '\n' +
      '지미의 사용자는 데아레스 케이츠 위로 성공하지 못한 행 끝에 클로스 스트리트에서 그를 만났습니다. 자연스럽게 관측된 소년이 아닌 지미씨는 고용주의 움직임의 줄을 지키려다 알프레드에게 생계를 위해 한 일을 말하려 했지만 알프레드 기운은 모두 사라졌고, 지미의 눈을 사로잡은 음주에도 빨리 창문에 대한 불쌍한 심사로 나타났다. 고용주의 지미는 충분하고 전복되었지만, 그의 신격함과 미묘한 소박함은 실패하지 않았다.\n' +
      '\n' +
      '고용의 하나는 이러한 요격적 사실계의 가장 파악하기 어려운 명제였기 때문에, 그다음에 알프레드는 완벽한 드리프트우드를 발견했기 때문에 지미의 위독과 재방식의 방식으로 만나 수용되고 서 있었다. 즉, 자신의 진정한 자질을 보고 지미를 구했고, 평론가들은 지미에게 알프레드의 스니커를 지우고 코트를 벗기 위해 더 자주 연습생들에게 부탁받았고, 즉시 변호사 겸 대학 교수의 지미 딘 조수의 정신적인 그림이 되었다고 한다.\n' +
      '\n' +
      '그가 가진 행위나 태도가 아닌 엄청난 관객을 선출한 안왕의 시선에서 안일을 이끌었던 한 사람의 두려움 없는 단일 에너지가 아닌 지미의 무모한 비하인격이었다. 지미 씨는 여전히 소년을 동료 빈소로 생각했는데, 이제 그의 사부님이 갔었다.\n' +
      '\n' +
      '그것은 짐미의 만족스러운 드리프트우드였습니다.\n' +
      '\n' +
      '그 저녁 안 여사는 지미가 있던 침실에 들어가 지미가 많이 지쳤다는 말을 다시 한 번 했고, 안은 긴 얼굴의 이유로 배정된 만큼 형식적인 체에 시달리고 있다고 단정하는 것은 불가능하지 않았다.\n' +
      '\n' +
      '그러나 낮잠만 관리했고 지미에서 벗어날 때 다소 침묵했기 때문에 안은 피곤하고 착용하는 남자도 피곤해야 한다고 판단했고, 아침부터 내일은 짐미 딘의 친구의 미해결 미스터리와 관련하여 지미에 대해 상상할 수 있는 모든 증거를 손에 놓을 것이라고 몰래 판단했다.\n' +
      '\n' +
      '## Chapter II\n' +
      '\n' +
      '그 오후 지미가 저녁 식사로 교수님의 집으로 옮겨졌다. 반 총장의 할이 더 잘 맞았습니다.\n' +
      '\n' +
      '##, 사마엘 피피스의 일기.\n' +
      '\n' +
      '텍스트(\\(826\\) 바이트.\n' +
      '\n' +
      '6월 1일. W씨 잎을 가져갔어요. 오늘 아침에 휘츠타이드를 유지하기 위해 갔던 바텐과 내 레이디는 W입니다. 펜과 아이, 가우덴은 물에 의해 울리치에게, 배에서 배를 타고 가서 정체를 주문하고 통지한 다음 데프포드에 가서 그곳 주막에서 선장 포롤과 함께 위울리치에 다니다. 데프포드로부터 반쪽 집에서 부르는 레드리프까지 걸었고, 휘츠트미드를 상대로 만든 새로운 케이크가 무한하게 놓여 있는 방으로 들어가게 되었고, 우리는 매우 자취를 감추지 못했다. 물집에 의해 사무실 사업장이 있었습니다. 다른 사람들 중 제 주님께서 L1000에 대한 충동을 받았고, 카네스의 L10,000은 서명된 이 지폐에 반대했습니다. 나라에 편지를 쓰고 제가 잠자리에 들었던 것들을 읽었습니다.\n' +
      '\n' +
      'MambaByte-\\(972\\)M:\n' +
      '\n' +
      '오늘 아침에는 제 선박이 나왔는데, 이 날까지 우리가 길을 잃었다고 생각했지만, 북쪽 바람의 기구는 무사히 도착했습니다. 우리만의 행방을 찾아 찾아갈 때, 우리는 가슴을 크게 잃지 않고 밤까지 일어나기 위해 많은 애도를 가지고 있었다. 정오에 국왕과 왕후와 모든 재판소(배 부종으로 인한 일부 고통에 빠질 수 없다)가 왔다. 남들과 함께 저녁을 만나러 갔는데, 우리 레이디 샌드위치는 우리 레이디 제엠의 특별 초청으로 코트에서 식사하러 왔다. “많은 사람들이 이 담론에 대한 호기심과 놀라운 나이에 놀랄 것”이라며 “이 담론에 대해 거의 놀랐다”고 말하는데, (나는 알) 여자는 나이와 놀라운 나이로 놀랐다.\n' +
      '\n' +
      '저녁 식사에서 우리는 그녀의 사랑하는 딘포드로부터 여자로 다가온 그녀의 전진성에 대해 담화했다고 말했다. 나의 레이디는 너무 부드러워서, 나는 그녀에게 R 선생님을 위해 저녁 식사에 전화를 걸면서 조금 빛을 발할 기회를 주는 데 도움이 되지 않았습니다. 내가 정해지기 전, 그레이브센데까지 머물며 가는 것이 레이디에게 제안되었고, 그 중 그녀가 가웬의 치마에도 매우 기뻤습니다. 하지만 그 후 가장 엄격한 문의를 찾았을 때 Hinchinbroke에 도착했고, 레이디가 코치로 가는 동안 울어들었는데, 그녀는 그녀를 위해 묵었다고 말해야 할 것 같아요. 조금 아프지 않은 난, 나 없이 내 레이디가 정해지는 모습을 보기 위해 큰 혼란에 빠졌다고 고백하고 있는데, 내가 알았을 경우 현명하게 해결하지 않았어야 했다. 마지막으로 W 선생님 사이에 동의했습니다. 바텐과 내 레이디에는 기녀가 보트에서 직접 가야 한다는 얘기와 아내와 함께 가야 할 마음이 컸던 피닝턴 여사와 다른 배에 가라고 부인이 다닌다. 나의 레이디는 제인을 위해 보내는 생각을 가지고 있었습니다. 내 집에서는 영업방식 없이 만나보고 싶었던 마음과 반대가 없고, 제인과 작은 부인이 함께 지내기 위해 제인과 함께 지내려고 하고, 두 배를 더 숙고하고 발사하기 위해 지금 건물에 가서 처분하는 데 문제가 있지만, 그곳에 있는 것은 나를 고통받지 않고 자신의 즐거움으로 맡긴다. 하루가 오후 내내 가장 유쾌함을 증명하여 동의하였고, 준비까지 했는데, 마치 일이 더 나아지지 않았을 것 같은 내 마음도 멀리하고 낙담했다. 이 불량한 와치(찾아가는 내 레이디)를 보내면 나는 일어나 아내와 아들 T를 데려갔다. 타워 정원에 있는 제 위대한 코치님, 마지막 감독님이 저를 빌려줄 테니 코치가 잘 선택되고 좋습니다. 그래서 말이 가장 깔끔하고 백인을 선택할 수 있도록 주문했고, 피닝턴 여사는 그 길을 다잡고 박스에 올려놓았으며, 내가 울지 말아야 할 고운 진주를 잃은 제 큰 담론은 나를 향해 질투하는 그런 고통이다. 람베스 샹펠에 가는 길, 내가 지각하는 나의 주님은 누가 자신의 샤펠에 앉을지 이해하고자 하는 욕구를 주님께서 전 설교에 서게 되었습니다. 이것은 설교의 대부분을 이어갔는데, 우리의 다리 위에 있는 다리를 넘어갈 때 주님께서 그렇게 할 수 있는 결의를 지켰을 것이라고 생각해보았습니다. 그래서 저녁에는 설교 후 카드에 매우 민사적인 방향을 주님의 큰 만족으로 주님에게 주는 것 같았는데 (더 이상 변함이 없는 신사들의 선에 너무 깊이 빠져들) 그 변신이 될 수 있어서 기분이 좋지 않은 것 같았고, 제 주님이 아주 쉽게 떨어졌고, 그가 가지고 있던 모든 것을 잃고, 매우 심하게 그리고 애도하게 서명하고, 그렇게 도망갔습니다. 그 사업은 이 설교에 대해 크게 메리드가 되지 않았고, 미안하고, 우리가 킹스턴으로 줄지어 서운하게 남았고, 일부 엄숙한 서비스가 그곳에 착륙하여 몇 마일을 오르기도 하고, 약간의 집까지 올라갔습니다. 그런데 주님! 예전에 인생에서 전혀 몰랐던 남학생들의 겸손함, 우여곡절, 찢어짐, 욕을 해줬기 때문에 나는 현재 그들의 벨리들을 기대했던 대로 윗부분이나 그 어느 남자도 앉지 않았다. 그래서 처음 희망했던 것을 넘어 기쁨으로 이 사업을 끝냈다. 스타울에게 다시 돌아보세요.\n' +
      '\n' +
      '## 란프 컨노르르에 의한 순춤길의 조사.\n' +
      '\n' +
      '텍스트 (1,059\\) 바이트.\n' +
      '\n' +
      '고개를 휩쓴 바람에는 험준한 잭파인 연합 잭이 갈대하게 접히는 가운데 언덕 위에 높이 올라섰다. 그 갈란 깃발은 캐나다 태평양 철도 회사의 건설선을 따라 법 보전 및 질서라는 특별한 의무가 있는 북서산 경찰의 스트롱 교육감 본부를 표시했으며, 현재는 일부 스코어를 마일리지로 밀었다. 강철과 평행하게 달려든 토트로드에는 남성, 어두운 피부, 약간의 벌레가 달리며 딱딱한 팬트, 스트리밍 얼굴, 소진을 선포하는 열린 입이 나왔다. 왼쪽으로 가던 작은 산책로에서 불빛 깃발을 향해 그 코스를 짚어 돌린 뒤 나무 조각에 올 때까지 바위 언덕을 고군분투했고, 깊은 포치가 둥글게 달리며 고군분투하고, 깔끔한 정원이 묘사된 녹슬한 울타리로 둘러싸여 있다. 주자는 게이트를 지나 작은 자갈 산책을 올라가면서 발걸음을 오르기 시작했다.\n' +
      '\n' +
      '언급하고 무의식적이지도 않은 사람은 이 남자였다. 그의 모든 존재는 캐나다의 젊은 생장들의 행들 사이에서 빠르게 걷다 보니 자신에게 대한 안보감까지 올라갔다. 그는 문의 격차로 충동적으로 휘둘렀고 정박했다. 그런 다음 도시 시계의 알람 벨이 깊고 끔찍한 소리로 날카롭게 반복되었다. 이 벨은 개입 양허선의 주민들의 업무 욕구를 알렸을 뿐만 아니라 서산 후기 엔지니어가 다른 사람의 돌발하고 빠른 경력에 의해 구성된 높은 경계에 대한 추가 측면에서 경고하기도 했다. 권총이 떨어졌고, 주자는 다른 곳을 찾고 있었다. 청탁자는 자신의 얼굴을 손에 묻었다. "하나님의 이름으로, 남자! "어떻게 알까? 그냥 하나 찾았지 않냐"라고 벨을 무릅쓰고 이제 그림자와 거친 수포들 사이에서 다시 쓰러졌다. 소리는 그를 시작했어요. 그는 그를 보기 전에 한순간에 수혈되었다. “그 발끝이 어디 가는지 아는 사람이 한 명밖에 없다”며 여전히 숨을 지킨 뒤 귀를 기울였다. 첫 번째 소식이 나오는 \'티스\', 그리고 내가 볼 수 있어! 노련리그로 알려진 용역의 하이벨과 짧은 날씬한 박자가 그 위에 있는 장교였으며, 또 다른 큰 종들이 조금 더 돌아다녔는데, 한 번 더 슬림한 여녀를 따라 넘어갔는데, 한 번 더 슬림한 여녀를 향해 도망쳤으며, 그는 한 번 더 슬림한 걸음을 기다렸다. 자기가 약간 긁혔고, 혼자 먹었을 뿐이고, 그토록 다쳐버렸었는데, 한숨이 깡짖는 것 같기도 하고, 맨날 털을 꿋꿋한 느낌이 들었고, 몇 초 만에 남자의 피가 더 깊어졌나 봐라. 나야, 파드너, 교육감. 내가 여기서 나온 후 멕시코에 있던 직업을 위해 수씨에서 내려온 거야. 저는 캐나다 태평양 철도와 연결되어 있으며 캐나다 암석으로 손가락이 상처받은 남성을 사냥하고 있습니다. 그의 작은 깃발과 함께 \'작은 깃발을 보내고 있다\'는 말을 떠올리며, 왼쪽 뺨을 일직선으로 누운 거친 피부 표장이 흉터의 위치를 표시하고 갑작스런 정지로 데려왔다. 그의 눈은 그의 머리 위쪽에 있는 글자에 있었다.\n' +
      '\n' +
      '“받으러 내려갔어. 어차피 맨 아래까지 구해야겠다.” 브리티시 컬럼비아 오, 하나님!\n' +
      '\n' +
      '그는 그의 목소리를 높였습니다. 그가 출발한 순간. 몇 분 만에 그는 소녀를 재조명했습니다. 그들은 권유자를 재조명하여 철도회사 회의장 이전에 개방공간으로 복귀하였다. 그들은 권유자가 말했던 미숙식 식사와 함께 탁자를 둥글게 모았으니 말이다. 철도 회사는 브리티시 컬럼비아에서 몬트리올까지 일하고 있었습니다.\n' +
      '\n' +
      '그는 "우리의 싸움에서 우리가 열심히 했다"고 말했다. 오타와 북쪽 노선이 막히자마자 이 땅에서 버텨야 한다는 것을 알았고, 런던에서 북도로로 가는 길 전체가 막혔을 때까지 캐나다로 향했고, 10월쯤에 오타와의 주선을 따라 갈 수 없었다. 첫 번째 리그 국기가 올라갔습니다.\n' +
      '\n' +
      '권고인은 "저쪽에 있는 법인이 뭐야"라고 말했다. "제 기억이 났을 때 유니온리그가 처음 운영하는 클레임 게시물을 구하는 데 도움이 됐다"며 "석탄 사냥을 위해 하루 만에 내려와 반란을 키웠다"고 비난받았고, 체포돼 수씨로 옮겨져 이전에 깔린 특정 사건에서 증거를 줬다"고 말했다.\n' +
      '\n' +
      '"그리고 불만이 정확히 일어난 원인은 무엇이었는가?"라는 주자의 질문을 받았다.\n' +
      '\n' +
      '"글쎄요, 전혀 그렇지 않았는데 사실이었다. 그게 전부"라고 제약 이유를 설명했다.\n' +
      '\n' +
      '이어 "런던과 북서 주자들의 말을 들은 것부터 그 작품이 너무 신나고 위험하지 않은 것 같다"며 "아직 다른 사람의 일이기도 하고 체포됐다"며 "2년 전 어린 낙인, 결혼한 소녀로 총살당했다"고 말했다.\n' +
      '\n' +
      '“자신과 친인척 또는 그와 함께 있던 남성 중 일부에 대한 재판에 가뭄”이라며 철도 교육감의 얼굴에는 희미하고 완만한 표현이 있었다. 그는 피고인에 대한 재판에 출석하지 않았다는 이유로 훨씬 높은 순위를 기록하고 있었다. 그는 주자를 꿈틀렀어요.\n' +
      '\n' +
      '‘어렵다’ 수사용주의 가슴에 입에 담긴 음식물이 밀스톤처럼 작동하고 있었다. 그러다가 무의식적으로 자신에게도 입술이 \'아니오\' 대신 \'그렇다\'고 말했고, 대신 \'보여주다\'며 \'네가 있는 한 그쪽으로 휘두른다, 다른 것 아니냐\'는 그의 얼굴을 보고 무엇을 느끼는지 강한 욕망으로 침묵하는 모습을 지켜보고 있었다. 오지는 않았고 한숨을 내며 의자에 자리를 잡았습니다.\n' +
      '\n' +
      '"저게 짧은 일이었어. 그들은 여기서 어린 새끼를 마흔들었고, 그에게 캐나다 분단을 주었어요. 본사에서 내려보내는 캐나다 선에서 온 역장경으로 자신이 옳고 자신에게 어떤 말을 들어보지 않은 것을 보여주기 위해서였다. 그리고 우리가 줘야 할 증언의 최악이 아니라는 것을 모르신다면, 사다리. 그게 제일 좋지 않았어요.\n' +
      '\n' +
      '그 사실은 당시 청년이 3주 형을 받고 있는 것이다.\n' +
      '\n' +
      '‘한 달 전까지만 해도 됐다’는 전보를 준비하던 업소 같은 주자에 부러졌다. >뭘 해줬어, 우리?\n' +
      '\n' +
      '그 목소리와 청년의 방식으로 한뜻한 것이 있었다. 그러다가 자신의 이야기를 장착하면서 언더오퍼가 사과 톤으로 실을 차지했지만, 상대의 목소리에 의해 멈춤으로써 순간의 심각한 관심으로 되돌아왔다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>