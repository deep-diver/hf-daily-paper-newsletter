<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '동영상 트랜스퍼를 좋아합니다.\n' +
      '\n' +
      '매트 코월\\({}^{1,3}\\): 아킬 더브\\({}^{3}\\): 라이언 암브루스\\({}^{3}\\)\n' +
      '\n' +
      '아드리아노노스 가이돈\\({}^{1,2}\\)  폴리탄이노스 지아르디아 데파니스\\({}^{3,2}\\)  > 파벨 토카노프막프\\({}^{3}\\)))  ，시판티노스 지아르디아 데파니스\\({}^{1,2}\\)).\n' +
      '\n' +
      '다요타 연구소({}^{1}\\) 에티오피아 대학교,\\({}^{2}\\) 삼성전자 AI 센터 토론토, \\({}^{3}\\)\n' +
      '\n' +
      'yorkucvil.github.io/VTCD\n' +
      '\n' +
      '도요타 연구소 인턴십 기간 동안 완료된 작업입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문은 동영상에 대한 변압기 표상의 개념 기반 해석 가능성 문제를 연구한다. 이와 같이 자동으로 발견되는 고도의 시공간적 개념을 바탕으로 영상 변압기의 의사결정 과정을 설명하고자 한다. 개념 기반 해석 가능성에 대한 선행 연구는 이미지 수준 과제에만 집중되어 왔다. 비교적으로 비디오 모델은 추가된 시간적 차원을 처리하고 복잡성을 높이고 시간이 지남에 따라 동적 개념을 식별하는 데 어려움을 제기한다. 이 작업에서 우리는 첫 번째 비디오 전송기 개념 발견(VTCD) 알고리즘을 도입하여 이러한 문제를 체계적으로 다룬다. 이를 위해 비디오 변환기 표현 단위의 비지도 식별 - 개념을 제안하고 모델의 출력에 대한 중요성을 랭킹하기 위한 효율적인 접근법을 제안한다. 결과 개념은 매우 해석 가능하고, 비구조화된 비디오 모델에서 시공간 추론 메커니즘 및 객체 중심 표현을 나타낸다. 다양한 감독 및 자기 지도 표현 세트를 통해 이 분석을 공동으로 수행하면 이러한 메커니즘 중 일부가 비디오 변압기에서 보편적이라는 것을 발견했다. 마지막으로, 우리는 VTCD가 미세 구성 작업에 대한 모델 성능을 향상시키는 데 사용될 수 있음을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '신경망 내의 숨겨진 표현을 이해하는 것은 규제 문제를 해결하기 위해 필수적입니다[11, 31], 배치 중 폐해를 방지하는 것[5, 30], 혁신적인 모델 설계[13]를 도울 수 있다. 이 문제는 컨볼루션 신경망(CNN)[4, 22, 26, 35] 및 최근에는 변압기[51, 64]에 대해 이미지 세계에서 광범위하게 연구되어 왔으며, 이는 다수의 주요 통찰력을 초래한다. 예를 들어, 이미지 분류 모델은 초기 층에서 저수준 위치 및 텍스처 신호를 추출하고 나중에 층 [4, 24, 45]에서 점차 상위 수준, 의미 개념들로 결합시킨다.\n' +
      '\n' +
      '그러나 비디오 변압기는 이미지 수준 ViT와 전반적인 아키텍처를 공유하는 반면 기존 작품에서 얻은 통찰력은 내부 메커니즘을 설명하는 데 거의 도움이 되지 않는다. 예를 들어, 그림 1(톱)에 표시된 폐색 객체 [62]를 추적하기 위한 최근의 접근법을 고려한다. 냄비 내부의 보이지 않는 물체의 궤적을 정확하게 이유하기 위해 질감이나 의미적 신호만으로는 충분하지 않을 것이다. 그렇다면 이 접근법에 의해 사용되는 _spatiotal_ 메커니즘은 무엇인가? 그리고 다양한 작업을 위해 훈련된 비디오 모델에 걸쳐 이러한 메커니즘 _universal_가 있습니까?\n' +
      '\n' +
      '이 작품에서 우리는 이러한 질문에 답하기 위해 제안합니다.\n' +
      '\n' +
      '그림 1: 교합(톱)을 통한 추적을 위한 TCOW 모델[62]의 하이맵 예측과 우리의 VTCD(바닥)에서 발견한 개념이 있다. 우리는 모델이 초기 층에서 위치 정보를 인코딩하고, 중층에서의 용기 및 충돌 이벤트를 식별하고 후층에서의 교반을 통해 추적한다는 것을 알 수 있다. 하나의 비디오만 표시되지만, 발견된 개념은 많은 데이터세트 샘플들(전체 결과에 대한 비디오 참조) 사이에 공유된다.\n' +
      '\n' +
      '비디오 트랜스포머 개념 발견 알고리즘(VTCD) - 딥 비디오 변압기의 표현을 해석하기 위한 첫 번째 개념 발견 방법론이다. 우리는 복잡한 모델의 분산 표상의 의사결정 과정을 고수준 직관적인 용어로 설명할 수 있는 능력 때문에 개념 기반 해석 가능성 [21, 22, 26, 69]에 초점을 맞추고 있다. 우리의 목표는 임의의 주어진 계층에서 표현을 라벨링된 데이터(즉, 개념 발견) 없이 인간 추론 가능한 \'개념\'으로 분해한 다음 모델 출력에 대한 중요도 측면에서 순위를 매기는 것이다.\n' +
      '\n' +
      '분명히, 우리는 먼저 SLIC 군집링 [1]을 통해 주어진 층에서 시공간 튜브로 그룹 _m모델 특징_를 그룹화했으며, 이는 우리의 분석의 기초가 된다(섹션 3.1.1). 다음으로, 이 튜브렛을 비디오에 걸쳐 군집화하여 고수준의 개념[14, 21, 22, 39, 69]를 발견했다(섹션 3.1.2). 폐색된 객체 추적 방법 [62]에 대한 결과 개념은 그림 1(바닥)에 나와 있으며 충돌과 같은 사건을 감지하거나 용기를 추적하는 시공간적 신호를 포함하여 광범위한 신호에 걸쳐 있다.\n' +
      '\n' +
      '비디오 변압기의 의사 결정 메커니즘을 더 잘 이해하기 위해 우리는 모델의 예측에 대한 개념의 중요성을 정량화한다. 중등도 지도(49)에 대한 이전 연구에서 영감을 받아 개념 중요도를 추정하기 위한 새로운 소음 사육 접근법을 제안한다(섹션 3.2). 구배에 의존하는 기존 기술[35] 또는 개념 폐색 [21]과 달리 우리의 접근법은 변압기 아키텍처의 자기 의도 머리에서 중복성을 효과적으로 처리한다.\n' +
      '\n' +
      '다음으로, 우리는 VTCD를 사용하여 훈련 목표와 관계없이 나타나는 비디오 변압기 모델에 보편적인 메커니즘이 있는지 여부를 연구했다. 이를 위해 우리는 4.1절에서 여러 모델 간에 공유되는 _important_ 개념을 자동으로 식별하기 위해 Dravid et al.[16]에 의해 최근 작업을 확장한다. 여러 가지 발견, 자기 지도, 또는 비디오 언어는 실제로 다양한 과제에 대해 훈련된 모델 간에 많은 개념이 공유되고, (i) 초기 계층은 나머지 정보 처리를 전제로 하는 시공간적 기초를 형성하는 경향이 있으며, (iii) 후기 계층은 자기 지도 방식으로 훈련된 모델에서도 객체 중심 비디오 표현을 형성한다.\n' +
      '\n' +
      '마지막으로, VTCD는 미리 학습된 비디오 변압기를 적어도 중요한 단위를 프루닝하여 효율적이고 효과적인 미세 제조 인식 모델로 전환하는 데 사용할 수 있다. 우리는 5.4절에서 액션 분류 모델에서 머리의 1/3을 제거하는 것이 계산을 33% 감소시키면서 정확도의 4.3% 증가를 초래한다는 것을 보여준다.\n' +
      '\n' +
      '2개의 관련 작업.\n' +
      '\n' +
      '우리의 연구는 _비디오 이해_에 대한 _트랜스포머 기반 표현_에 초점을 맞춘 새로운 _개념 기반 해석성_ 알고리즘을 제안한다. 아래에서는 이들 각 분야에서 가장 관련성이 높은 작품을 검토한다.\n' +
      '\n' +
      '**개념 기반 해석 가능성**는 모델이 주어진 태스크에 활용하는 표현인 사후에 이해하는데 사용되는 신경망 해석 가능성 방법의 계열이다. 폐쇄세계 해석 가능성은 개념[4, 35]의 라벨링된 데이터셋을 갖는 전제하에 작동한다. 그러나 동영상의 경우 선험적인 것으로 알려졌더라도 어떤 개념이 존재할 수 있고, 영상을 조밀하게 라벨링하기 어려운 경우도 불분명하다.\n' +
      '\n' +
      '대조적으로, 비지도 개념 발견은 의미론적 개념의 존재에 대한 가정을 만들지 않으며 클러스터링을 사용하여 데이터를 모델의 특징 공간 내에서 해석 가능한 구성 요소로 분할한다. ACE[26] 및 CRAFT[22] 세그먼트는 주어진 레이어에서 클러스터링을 적용하기 전에 초픽셀 및 무작위 작물에 이미지를 입력한다. 그러나 비디오에서 잠재적인 튜브는 이미지 작물이 훨씬 많아 제3.1.1절에서 비디오를 제안으로 분할하는 보다 효율적인 방법을 도입하게 했다.\n' +
      '\n' +
      '개념 기반 해석성의 필요한 구성 요소는 중요도(_i.e_ 충실도)를 측정하고 있다. 발견된 개념들 중 모델에 대한 개념들. 그러나 앞서 언급한 방법[21, 22, 26, 35, 69]은 CNN에 대해 개발되었으며 변압기에 쉽게 적용할 수 없다. 주의 수장에서의 순위 개념의 주요 과제는 자기 의도 계층에서 사소한 섭동에 대한 변압기의 견고성 때문이다. 이러한 한계를 해결하기 위해 3.2절에서 머리와 헤드 내 개념을 모두 덮는 모든 건축 단위의 중요성을 순위를 매기는 새로운 알고리즘을 소개한다.\n' +
      '\n' +
      '최근의 작업[16]은 다양한 이미지 모델(변압기를 포함한다)에 걸쳐 유사한 활성화 맵을 생성하는 뉴런을 식별한다. 그러나 뉴런은 많은 차원[18]에 걸쳐 분포하기 때문에 모델의 표현 정도를 완전히 설명할 수 없다. 대조적으로, 우리의 방법은 모든 차원 특징에 대해 작동하며 비디오 기반 모델에 적용된다.\n' +
      '\n' +
      '변압기***의 해석 가능성은 다양한 컴퓨터 비전 문제에서 이러한 아키텍처의 성공으로 인해 최근 상당한 관심을 받고 있다. 초기 작업[51]은 CNN과 대조된 비전 변압기 표현 (계층별 표현 차이, 수용 필드, 정보의 국소화 등)을 나타낸다. 다른 작업은 모델 [8, 9]의 주의 지도를 기반으로 염기성 열맵을 생성하는 것을 목표로 한다. 이후의 작업은 서로 다른 훈련 프로토콜[47, 64](_e.g_ 자기 지도 학습(SSL)의 영향을 이해하는 데 초점을 맞췄다. 감독) 감독. 강건성[50, 70] 특정 SSL 비전 변압기 DINO[2]의 특징을 자세히 탐색하고 부분 기반 세분화 작업에 놀라운 유용성을 갖는 것으로 나타났다. 그러나 이러한 작업 중 어느 것도 개념 기반 해석 가능성이나 연구 비디오 표현을 다루지 않는다.\n' +
      '\n' +
      '독립적으로 자연어 처리(NLP)에 대한 연구는 자기 의도 계층[17, 63]을 분석했으며, 머리는 종종 다른 언어적 또는 문법 현상을 포착하는 데 특화된 것으로 나타났다. 이는 서로 다른 자기지도자[12, 34]에 대해 서로 다른 주의지도들을 보여주는 작품들에서 질적으로 볼 수 있다. 더욱이, 다른 NLP웍스[44, 63]는 머리를 제거하는 영향을 탐색하고 유사한 성능을 생산하기 위해 소수의 숫자만 유지해야 한다는 것을 발견했다. 우리의 연구 결과는 이러한 작업의 증거에 동의하며 섹션 5.4에서 비디오 변압기의 중요하지 않은 헤드를 프루닝하는 것이 실제로 _im 개선되었다_ 모델의 성능을 개선할 수 있음을 추가로 보여준다.\n' +
      '\n' +
      '** 비디오 모델 해석 가능성**는 액션 인식 [36], 비디오 객체 분할[40, 48, 59, 62], 자기 지도 접근[20, 52, 58, 60, 65]에서 딥러닝 모델의 최근 성공을 고려한 연구 과소 설명 영역이다. 모델이 동적 정보[25, 29, 33] 또는 장면 편향[10, 41, 42]을 사용하는 정도를 측정하기 위해 대리 작업을 사용했다. 하나의 방법은 비디오 모델의 중간 표현 [37, 38]에 포함된 정적 및 동적 정보를 정량화한다. 그러나 이러한 방법은 하나 또는 두 개의 미리 정의된 개념(_i.e_ 정적, 동적 또는 장면 정보)만을 측정할 수 있다. 우리의 접근법은 개념의 하위 집합에 국한되지 않는다. 또 다른 작업은 시간적 규칙화[19]로 활성화 최대화를 통해 3D CNN에서 단일 뉴런(또는 필터)을 활성화하는 비디오를 시각화한다. 이 방법은 뉴런이 인코딩할 수 있는 방법에 제한이 없지만 3D CNN에만 적용되고 분산 표현(_i.e_: 비디오 전체에 일반화하는 특징 공간 방향)에 걸쳐 \'개념\'을 진정으로 포착하지 않는다.\n' +
      '\n' +
      '비디오 변압기 개념 발견 3번 비디오 변압기 개념 발견.\n' +
      '\n' +
      '우리는 비디오 표현을 고수준의 개방형 세계 개념 집합으로 분해하고 모델의 예측에 대한 중요성을 랭킹하는 문제를 연구한다. 우리는 (RGB) 비디오, \\(Extextbf{X}\\in\\mathbb{R}^{N\\tcer 3\\tcer T\\times W}\\), 여기서 \\(N\\), \\(T\\), \\(H\\), \\(W\\)는 각각 데이터세트 크기, 시간, 높이 및 폭을 나타내고 \\(L\\) 레이어 전처리 모델, \\(f\\)을 나타낸다. \\(f_{[1,l]}) - \\(f_{[1,l]})를 사용하여 모델 형태 층(r\\) 내지 \\(l\\)을 나타낸다. 기존 이미지 수준 접근법(c_{Q}\\}\\)은 [22, 26]은 먼저 \\(M\\) 제안 집합으로 매핑되며, 이는 \\(텍스트bf{C} <\\-인터내셔널(M\\) 개념, \\(텍스트bf{T}\\) 개념, \\(텍스트bf{T} <\\bf{T}<\\bf{T} <\\bf{T} <\\)의 집합으로 매핑한다. 그런 다음 이러한 제안은 할당 행렬 \\(W\\in\\mathbb{R}^{M\\cer Q}\\)를 형성하기 위해 모델의 특징 공간에서 \\(Q<<M\\) 개념으로 군집링된다. 마지막으로, 모델의 예측에 대한 각 개념 \\(c_{i}\\)의 중요성은 점수 \\(s_{i}\\in[0,1]\\)에 의해 정량화된다. 이 분석을 \\(f\\)에서 모든 계층에 대해 수행하는 것은 모델에 대한 전체 개념 세트를 생성한다(\\textbf{C}=\\{\\textbf{C}_{1}, a.,\\textbf{C}_{L}\\}\\)와 해당 중요도 점수를 함께 생성한다.\n' +
      '\n' +
      '그러나 기존의 접근 방식은 규모가 잘 되지 않고 2D CNN 아키텍처에 초점을 맞추고 있기 때문에 비디오 변압기에 즉시 적용할 수 없다. 이 작품에서, 우리는 개념 기반 해석성을 비디오 표현으로 확장한다. 이를 위해 먼저 시공간 특징 부피보다 작동하고 시공간 튜렛을 출력하는 계산 가능한 제안 생성 방법(섹션 3.1.1)을 설명한다. 다음으로, 기존 개념 군집화 기법을 비디오 변환기 표현에 적응시킨다(섹션 3.1.2). 마지막으로, 섹션 3.2에서 CRIS - 변압기 헤드를 포함한 모든 아키텍처 유닛에 적용할 수 있는 새로운 개념 중요도 추정 접근법을 제안한다.\n' +
      '\n' +
      '### Concept discovery\n' +
      '\n' +
      '튜브렛 제안서 3.1.1.1 Tubelet 제안서 3.1.1.1 Tubelet 제안서###############\n' +
      '\n' +
      '이전 방법[21, 22, 26]은 RGB 공간에서 슈퍼픽셀 또는 작물을 사용하여 세그먼트를 제안하지만 가능한 세그먼트의 수는 비디오에 대해 기하급수적으로 더 크다. 더욱이, 색상 공간의 제안은 제한적이지 않으며 모델의 인코딩된 정보와 일치하지 않을 수 있으므로 많은 관련되지 않거나 시끄러운 세그먼트로 이어질 수 있다. 이러한 단점을 해결하기 위해 각 레이어 내에 포함된 정보를 기반으로 자연스럽게 비디오를 분할하는 _feature 스페이스_의 제안(그림 2의 표)을 인스턴싱한다.\n' +
      '\n' +
      '보다 구체적으로, 우리는 심플라인러 Iterative Clustering [1, 32](SLIC) 상에서 심플라인더 Iterative Clustering [1, 32](SLIC)을 통해 비디오당 튜브렛을 구성한다.\n' +
      '\n' +
      '그림 2: 비디오 트랜스퍼 컨셉트 디스커버리(VTCD)는 비디오의 데이터셋인 **X**를 입력으로 하여 모델, \\(f_{[1,l]}\\)로 전달한다. 그런 다음 비디오 특징 세트 **Z**는 특징 공간에서 SLIC 클러스터링을 통해 시공간 튜브레 제안 **T**(빨간색으로 이동)에 파싱된다. 마지막으로, 튜브렛은 네트워크 표현-개념, **C**(오른쪽)의 고수준의 단위를 발견하기 위해 비디오에 군집링된다.\n' +
      '\n' +
      '공식적인 특징은\n' +
      '\n' +
      '[\\mathbf{T]=\\text{GAP}(\\mathot\\mathbf{Z})\n' +
      '\n' +
      'M\\bf{B}\\math{B}\\in}^{C\\tcer N\\{C\\time}\\math{C\\tacill H^{\\prime}\\tcer H^{C\\time}\\t 대다수가 SLIC 군집링에서 얻은 시공간적 바이너리 지원 마스크(\\mathb{T}\\b{C\\tic H^{C\\time W ^{C\\time W ^{C\\tic H^{C\\time}\\i H^{C\\t unw H^{C\\tic H^{C\\tic H^{C\\time W ^{C\\time W ^{C\\time W ^{C\\time W ^{C\\time W ^{C\\time W ^{C\\time}\\time W ^{C\\time}\\time W ^{C\\time}\\time W ^{C\\time}\\time W ^{C\\time}\\time}\\time W ^{\n' +
      '\n' +
      'SLIC는 클러스터 지원 규칙성과 적응력 간의 교역을 제어하는 K-Means 알고리즘의 연장선상에서, 군집들이 마스크를 연결하도록 지원하는 것을 제약한다. 이와 함께 이러한 특성은 한 번에 비디오에서 여러 지역에 참석해야 하는 필요성을 줄이기 때문에 인간에게 해석하기 쉬운 비분열 튜브렛을 생성한다. 또한, SLIC의 프루닝 단계는 자동으로 프루즈 스펀지, 단절된 튜브가 있기 때문에 원하는 군집의 수를 제어하는 하이퍼파라미터에 더 강력해진다. 다음으로, 개별 튜브렛을 상위 개념 클러스터로 그룹화하기 위한 접근 방식을 설명한다.\n' +
      '\n' +
      '3.1.2 개념 군집링####### 3.1.2 개념 군집링.\n' +
      '\n' +
      '최근의 작업[21, 22, 69]은 비음성 매트릭스 요인화(NMF) [14]를 사용하여 제안을 개념으로 군집화했다. r\\(\\mathb{C}^math{C}^mathb{R}\\math{R}\\math{R}\\math{R}\\math{R}^{Q\\) 및 \\(\\math{b}\\math{R}\\math{R}^{Q\\i C}\\math{R}\\math{R}\\math{R}\\math{R}\\b}\\in{R}\\mathb}\\b}\\in{R}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\b}\\i{R}\\b}\\b}\\g{R}\\i{R}\\ 불행히도 NMF는 ReLU가 아닌 GeLU 비선형성을 사용하여 부정적인 활성화를 초래하기 때문에 변압기에 적용될 수 없다.\n' +
      '\n' +
      '우리는 컨베텍스 비음성 Matrix Factorization[14](CNMF)를 활용하여 이 문제를 해결한다. 명의에도 불구하고 CNMF는 NMF를 확장하며 음의 입력 값을 허용한다. 이는 \\(\\mathbf{W}\\)의 컬럼이 \\(\\mathbf{T}\\), _i._i. \\(\\mathbf{W}\\)의 컬럼의 볼록한 조합이라는 요인화를 제약함으로써 달성된다. 이 제약은 이 제약으로 작성될 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{W}=\\mathbf{T}\\mathbf{G}, \\tag{2}\\]\n' +
      '\n' +
      '\\(\\mathbf{G}\\in[0,1]^{C\\tcer Q}\\) 및 \\(\\sum_{j}\\mathbf{G}_{i,j}=1\\)의 경우. 모돈의 세트인 \\(\\mathbff{T}\\)를 상응하는 개념으로 군집화하기 위해 최적화되어 세르트, \\(\\mathbf{T}\\) 세트를 해당 개념으로 군집화한다.\n' +
      '\n' +
      '>0}\\mathbf{T}\\mathbf{C}\\mathbf{2}\\\\[\\math{G}^{*}]\n' +
      '\n' +
      '최종 개념 세트가 매트릭스 \\(\\mathbf{C}\\), _i._i._개념 중심형(c_{i}\\)의 행으로 표시되는 경우, \\(i^{th}\\)의 \\(i^{th}\\) 행은 \\(\\mathbf{C}\\)이다(그림 2, 우측).\n' +
      '\n' +
      '### Concept importance\n' +
      '\n' +
      '발견된 개념 세트를 감안할 때, 우리는 이제 모델 성능에 미치는 영향을 정량화하는 것을 목표로 한다. 그림 3에 나타난 한 가지 접근법은 각 개념을 독립적으로 가리고, 성과 하락(21])을 기준으로 중요도를 순위를 매기는 것이다. 형식적으로,\\(c_{l}\\)는 단일 목표 개념이고, \\(\\mathbf{B}_{c_{l}}\\in\\{0,1\\}^{C\\tep M\\{C\\tcer N\\tross T^{\\prime}\\tross H^{ \\f{X}\\)는 \\(\\mathbf{B}_{c_{c_{l}}\\in\\{0,1\\{C\\ii)는 단일 목표 개념이고, H\\{C\\{0,1\\{C\\tcer N\\tcer N\\times N\\tcer H^{C\\tcer H^{C\\tross H^{C\\tross H^{C\\t unw H^{C\\time}\\tic H^{C\\time}\\time}\\tross H^{C\\tross H^{C\\tross H^{C\\tross H^{C\\tross H^{C\\tross H^{\\time}\\time}\\t 그런 다음 레이어 \\(l\\)를 통해 마스킹할 수 있습니다.\n' +
      '\n' +
      '\\[\\hat{y}=f_{[l,L]}(\\mathbf{Z}_{l}\\odot(1-\\mathbf{B}_{c_{l}})). \\tag{4}\\]\n' +
      '\n' +
      '그러나 이 접근법은 CNN[21]에 대해 잘 작동하지만 변압기는 자기 의도 계층[44, 63] 내의 작은 섭동에 강한다. 따라서 단일 개념 마스킹은 성과에 거의 영향을 미치지 않는다(그림 4의 결과별 다운로드). 대신, 우리는 샘플링된 개념 중 _a 높은 백분율_를 병렬로 마스크(모든 층과 머리로 교차)한 다음 수천 개 샘플 이상의 결과를 평균화하는 섹션 5.1에서 경험적으로 검증하여 유효한 개념 순위를 생성한다.\n' +
      '\n' +
      '형식적으로, 우리는 관심 단위에 대한 중요성을 계산하는 강력한 방법인 \\(\\mathbf{C}\\) 개념 \\(\\mathbf{R}\\) 무작위화된 중요도 \\(\\mathbf{S}\\) 샘플링(CRIS)을 제안한다. 이를 위해 먼저 서로 다른 개념 세트를 무작위로 샘플 \\(K\\)하여 각 \\(\\mathbf{C}^{k}\\subset\\mathbf{C}\\)이 된다. 그런 다음 \\(\\mathbf{C}^{k}_{l}\\)를 층(l\\)에서 발견된 \\(\\mathbf{C}^{k}^{k}\\)의 개념 세트로 정의하며, \\(\\mathbf{B}_{\\mathbf{C}_{\\)는 해당 이진 지지 마스크를 나타낸다. 모델의 모든 층에서 모든 개념을 가리고 있습니다.\n' +
      '\n' +
      '}} <\\mathbf{k>} (\\mathbf{k})\\cot f_{{[L-1]} (\\mathbf{f}} <\\mathbf{C>}} <\\mathbf{f} <\\mathbf{f} <\\math{f}_{f}]]}.\n' +
      '\n' +
      'HH(g(\\cdot)\\가 예측 헤드(_e.g_ MLP)이고 \\(\\mathbf{\\tilde{B}}\\)는 역 마스크(_i.e_)를 나타낸다. (1-\\mathbf{B}\\) 마지막으로, 우리는 각 개념의 중요도 \\(c_{i}\\)를 통해 각 개념의 중요성을 계산한다.\n' +
      '\n' +
      '{K}(\\mathb{D})\\mathb{D}(\\tild{y}_{k})\\mathb{D}(\\hathb{d}_{c_{k}]\\mathb{f}.\n' +
      '\n' +
      '아이티(\\tilde{y}\\)가 마스킹 없이 원래 예측이고 \\(\\mathbb{D}\\)는 메트릭 정량 성능(_e.g._정확도)이다.\n' +
      '\n' +
      '4는 VTCD와 함께 변압기를 좋아합니다.\n' +
      '\n' +
      '우리의 알고리즘은 모델의 모든 단위 내에서 개념의 식별을 촉진하고 최종 예측에서 그 중요성을 정량화한다. 그러나 이것은 충분하지 않습니다.\n' +
      '\n' +
      '그림 3: 단일 개념에 대한 개념 마스킹의 시각적 표현. <\\bf{z}_{1}\\) 및 개념, \\(c_{l}\\)을 감안할 때, 우리는 중간 표현 \\(\\mathbf{z}_{1}=f_{[1,l]}(\\mathbf{x}_{1})의 토큰을 조절하여 개념의 이진 지원 마스크, \\(\\mathbf{z}_{1})(\\mathbf{z} <{1,l]})의 토큰을 가리고(\\mathbf{z} <{1>} <f_{1>} <f_{1,l]} <f_{1,l]}(\\mathbf{1,l]})의 토큰을 가리고(\\mathbf{x})의 개념을 감안할 때 <f_{1,l]}(\\mathbf{1,l]}(\\mathbf{x}1,l]}(\\mathbf{mathbf{1,l]})의 개념을 감안할 때 <{[1,l]}(\\\n' +
      '\n' +
      '비디오 변압기에 의해 수행되는 계산을 완전히 나타내도록 한다. 이러한 개념이 모델의 정보 흐름에서 어떻게 사용되는지도 이해하는 것이 중요하다.\n' +
      '\n' +
      '최근의 여러 작품들이 [17, 46]을 보여주듯이 변압기의 잔여 스트림은 정보 흐름의 중추 역할을 한다. 그런 다음 각 자기 의도 블록은 선형 투영으로 잔류 스트림으로부터 정보를 판독하고, 이를 처리하기 위해 자기 의도 동작들을 수행하고, 최종적으로 결과를 다시 잔여 스트림에 기입한다. 즉, 머리마다 개별적으로 자기의도 처리를 수행하고 비전[2, 15, 34] 및 NLP[63]에서 서로 다른 자기의도 헤드가 뚜렷한 정보를 포착하는 여러 연구가 나타났다. 다른 말로, 헤드는 변압기 표현의 기초를 형성한다.\n' +
      '\n' +
      'TCOW의 머리에서 발견된 개념을 VTCD와 더 자세히 분석하면 해당 모델의 정보 처리에서 몇 가지 패턴을 식별할 수 있다. 특히 그림 1은 초기 계층 그룹의 헤드가 시공간 위치에 기초하여 토큰을 입력한다는 것을 보여준다. 그런 다음 이 정보는 물체를 추적하고 중층에서의 이벤트를 식별하는 데 사용되며, 이후 계층은 교합에 대한 이유에 대해 중층 표현을 사용한다. 다음으로, 이러한 메커니즘 중 어느 것이 다양한 목적을 가진 서로 다른 데이터셋에서 훈련된 비디오 변압기에 걸쳐 _universal_인지 여부를 연구한다.\n' +
      '\n' +
      '### Rosetta concepts\n' +
      '\n' +
      '[16]에 의해 영감을 받아 모델 간에 공유되고 동일한 정보를 나타내는 _Rosetta 개념_을 채굴할 것을 제안한다. 로제타 단위를 식별하는 핵심은 강력한 메트릭, \\(R\\)이며, 여기서 더 높은 R-스코어는 더 많은 양의 공유 정보를 갖는 두 유닛에 해당한다. 이전 작업 [16]은 활성 맵을 상관시키는 것을 기반으로 이미지 모델에서 그러한 뉴런을 찾는 데 중점을 두었다. 우리는 대신 개념들의 지원의 평균 교차로(mIoU)를 통해 개념들(분산된 표현들) 간의 유사성을 측정할 것을 제안한다.\n' +
      '\n' +
      'r\\(Billbf{C}.^{j},^{j}},\\) 모델 \\(\\{c_{c_{j}}, a\\{c_{i}}. 그런 다음 모델에서 모든 개념 \\(D\\)-tuox 간의 유사성을 측정하는 것을 목표로 한다. f{B}_{i}^{D}\\}\\}\\) 및 상응하는 이진 지원 마스크, \\(D\\) 개념, \\(D\\) 개념, \\(\\{c_{i}}_{i}^{1},\\{c_{i}.^{1}) 세트를 감안할 때 이러한 개념의 유사성 점수를 정의한다.\n' +
      '\n' +
      'r\\[R_{i}^{i}=\\ff{D}_{i}_{i}\\cap\\cdots\\\\cdots\\\\bf{B}_{i}^{{1}\\cap\\cdots\\\\cdots\\textbf{B}_{i}_{i}^{i}.\n' +
      '\n' +
      '모든 \\(D\\)-tuox 간의 유사성을 인위적으로 계산하면 지수적인 계산 수가 발생하고 심지어 작은 \\(D\\)에도 견딜 수 있다. 이 문제를 완화하기 위해 \\(d<D\\)가 있는 \\(d\\)-tu폴리 중 R-점수가 낮은 (i) 중요하지 않은 개념과 (ii) 두 가지 유형의 개념을 배제한다. 보다 구체적으로, 우리는 각 모델에서 개념의 가장 중요한 \\(\\epsilon\\%\\)만을 고려한다. 그런 다음 \\(d\\in\\{2,...D\\}\\)를 초과하고 참여하는 모든 d-tu에 대해 \\(\\delta\\)보다 적은 R-점수(\\delta\\)를 갖는 개념을 필터링한다. 형식적으로, 필터링된 로제타 d-개념 점수는 필터링된 로제타 d-개념 점수로 정의된다.\n' +
      '\n' +
      '>헤타{d}^{d}^{d}^{d}^{d}^{d}\\ff{d}.\n' +
      '\n' +
      'H\\(\\textbf{R}_{\\epsilon}^{d}\\)가 \\(\\epsilon\\) 중요 필터링 후 \\(d\\) 개념 중 모든 R-점수의 집합이다. 이는 다음 단계 \\(d+1\\)에 대한 후보 풀이 상당히 작아서 알고리즘의 전체 계산 복잡성을 감소시킨다. 마지막으로, 일부 개념이 모델의 하위 집합에 존재할 수 있지만 여전히 연구하기에 흥미롭기 때문에 R-점수 \\(Billbf{R}_{\\epsilon,\\delta}^{2}\\cup\\cup\\cup\\cup\\cup\\textbf{R}_{\\\\epsilon,\\delta}^{D}\\\\delta}^{D}\\)에 해당하는 모든 중요하고 자신감 있는 로제타 d-개념의 조합을 조사한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '우리는 다양한 작업에 대해 훈련된 다양한 모델에 걸쳐 정량적으로 질적으로 우리의 개념 발견 알고리즘의 품질을 평가한다.\n' +
      '\n' +
      '***Dataset** 우리는 TCOW 윤활[62] 및 스메싱-스타일-v2 (SSv2) [27]의 두 가지 데이터 세트를 실험에 사용한다. 전자는 윤활 합성 비디오 생성기[28]를 기반으로 무작위 객체 위치 및 동작이 있는 4,000개의 비디오의 합성, 광현실 데이터세트이다. 이 데이터 세트는 교반을 통한 반지도 비디오 객체 분할(반-VOS)을 위한 것이다. SSv2에는 미세한 액션 인식을 위해 의도된 220,847개의 실제 영상이 포함되어 있다. 각 샘플은 사람-객체 상호 작용(어떤 일을 하는 것)에 대한 인파가 많은 비디오이다. 다른 많은 비디오 분류 벤치마크(6, 56)와 달리 시간적 추론은 SSv2 행동을 구별하기 위한 기본이며, 이는 변압기의 시공간 메커니즘을 분석하는 데 이상적인 선택이다.\n' +
      '\n' +
      '반-VOS, (ii) 비디오MAE[60]의 액션 분류를 위해 SSv2에 대해 훈련된 4개의 모델을 평가하는데, (ii) 비디오MAE[60]는 SSv2(SSL 비디오MAE)에 자체 요약된(iii) 비디오MAE 및 (iv) 내부 비디오 기반 모델[66]은 LAION-400M[54]의 12M 비디오 데이터세트 및 100M 이미지-텍스트 쌍에서 대조적으로 훈련된 비디오 기반 모델에 대해 공동 학습했다. TCOW는 입력으로서 분할 마스크를 필요로 하므로 SSv2에 적용할 때, 우리는 쿼리로 사용하기 위해 초기 프레임에서 가장 두드러진 오브젝트를 핸드 라벨링한다. 우리는 첫 두 모델에 대한 분석을 집중하며 마지막 두 가지를 사용하여 로제타 개념의 보편성을 검증한다.\n' +
      '\n' +
      '** 구현 세부 정보*** 모든 실험에서 무작위로 샘플링된 30개의 비디오에 대해 VTCD를 실행하고 모든 모델 레이어에서 모든 머리에서 개념을 발견했다. 사전 작업[2]은 케스가 자기 의사를 가진 머리에서 가장 의미 있는 클러스터를 생성한다는 것을 보여주므로 여기에서 케이스에 초점을 맞추고 나머지 하이퍼파라미터와 함께 쿼터와 밸리로 결과를 부록에 제시한다. 비디오 변압기 표현을 분석할 수 있는 VTCD 툴박스에 대한 저희 코드가 출시됩니다.\n' +
      '\n' +
      '**VTCD 목표 메트릭***를 대상으로 개념 발굴 및 순위 개념을 확인하기 위해 VTCD는 목표 평가 메트릭을 필요로 한다. TCOW 윤활의 경우 예측된 마스크와 지상 진실 마스크 사이의 교차 결합(IoU)을 사용한다. SSv2의 경우 목표 등급에 대한 분류 정확도를 사용한다.\n' +
      '\n' +
      '양적 개념 평가.\n' +
      '\n' +
      '우리의 방법의 효과를 정량적으로 확인하기 위해 개념 기반 해석 가능성에 대한 표준 평가 프로토콜을 따르고, 발견된 개념 [22, 26, 69]의 _fidelity_를 측정한다. 이를 위해 귀향 곡선[21, 22, 26]을 계산하는데, 여기서 개념(모든 계층에 걸쳐서)이 가장 많은 순서(_양성_교차), 또는 가장 적게는(_음성_교차)에서 모델에서 제거된다. 직관은 충실도가 높고 중요도 점수가 더 정확한 개념들은 가장 중요한 개념들을 제거할 때 성능이 더 빨라질 것이며 역질서에 대한 부의장은 감소한다는 것이다.\n' +
      '\n' +
      '그림 4에서 우리는 10개의 무작위로 샘플링된 클래스 및 평균 결과(바닥)를 표적으로 하는 K 윤활(톱)에 대한 TCOW 및 SSv2에 대한 슈퍼엔드 비디오MAE 방법에 대한 개념 귀속 곡선을 도표팅한다. 또한, 우리는 (i) 개념 제거, (ii) 무작위 순서, 폐색 기반 [21] 개념 중요도 추정 및 (iii) 구배 기반 접근 방식[21, 35]에 대한 결과를 보고한다. 모든 경우에 CRIS는 무작위 순서와 폐색 기준선을 모두 극적으로 능가하는 보다 실행 가능한 중요도 순위를 생성한다. 통합 구배 접근법은 TCOW에 대해 우리와 유사하게 수행되지만 액션 분류 비디오MAE 모델에서 유의하게 더 나쁘다.\n' +
      '\n' +
      '특히, 우리는 가장 중요한 개념의 최대 70%를 제거할 때 VideoMAE의 성능 _increase__increase_가 실제로 제거된다는 것을 관찰한다. SSv2가 미세 제조 액션 분류 데이터세트라는 것을 기억하세요. 우리의 방법은 주어진 계층과 무관한 개념을 제거하므로 모델의 예측의 견고성을 증가시킨다. 5.4절에서 우리는 이 관찰에 대해 자세히 설명하고 VTCD가 어떤 미세 구성 비디오 변압기의 성능과 효율성을 향상시키는 데 사용할 수 있는지 보여준다.\n' +
      '\n' +
      '### Qualitative analysis\n' +
      '\n' +
      'VTCD에 의해 발견된 개념에 할당된 중요성이 모델의 정확도와 잘 일치한다는 것을 발견했다. 우리는 이제 개념 자체를 질적으로 평가하게 된다. 이를 위해 Fig. 5에서 TCOW 및 VideoMAE 모델에 대한 가장 중요한 3개의 상위 개념에 대한 2개의 대표 비디오를 a_ 등급에 캡슐화한 것을 보여준다.\n' +
      '\n' +
      'TCOW의 경우 계층 5에서 가장 중요한 개념이 발생하고 대상 객체를 추적한다. 흥미롭게도 동일한 개념은 외관이 유사한 물체 및 타겟과 2D 위치를 강조한다. 이는 모델이 중층(_i.e__5)에서 가능한 증류기를 먼저 식별함으로써 불균형 문제를 해결한다는 것을 시사한다. 그런 다음 이 정보를 사용하여 최종 레이어에서 타겟을 더 정확하게 추적한다. 사실, 계층 9에서 발생하는 두 번째로 중요한 개념은 비디오 전체에 걸쳐 타겟 객체를 추적한다.\n' +
      '\n' +
      '비디오MAE의 경우 가장 중요한 개념은 물체와 컨테이너가 모두 강조되는 드롭 이벤트까지 떨어뜨리는 대상을 강조한다. 두 번째로 중요한 개념은 취하되는 용기를 명확하게 포착하고, 특히 객체 자체를 포착하지 못하고 링과 같은 형상을 만든다. 이러한 개념은 유사한 클래스(_e.g_ 떨어진다)를 구별하는 데 도움이 되는 모델의 중요한 메커니즘을 식별한다(_into/behind/in-amb_ 무언가를 떨어뜨린다).\n' +
      '\n' +
      '각 모델에 대한 세 번째로 중요한 개념은 초기 계층, 즉 시간적으로 불변하고 공간적인 지원에서 발생하는 유사한 정보를 캡처한다. 이 관찰은 모델 초기에 위치 정보 처리가 발생하고 의미 정보와 토큰 자체 사이의 참조 프레임으로 작용함을 시사하는 연구 [2, 24]를 확증한다. 우리는 이제 로제타 개념이라고 하는 여러 모델에 걸쳐 _shared_인 개념을 발견하기 위해 설정했다.\n' +
      '\n' +
      '### Rosetta concepts\n' +
      '\n' +
      '우리는 4개의 모델에 VTCD를 적용하여 동적 특성으로 인해 선택된 2개의 클래스를 표적으로 하며, _가 평평한 표면_에 걸쳐 무언가를 조절하고 a_ 뒤에 있는 것을 수득한다. 그런 다음 모든 실험에서 4.1절에서 설명한 방법을 사용하고 \\(\\delta=0.15\\) 및 \\(\\epsilon=15\\%\\)를 설정하기 위해 로제타 개념을 채굴한다. 생성된 로제타 4-개념 세트는 평균으로 40개의 투폴을 함유하고 있다.\n' +
      '\n' +
      '그림 4: SSv2(바닥)에서 훈련된 윤활(톱) 및 비디오MAE에서 훈련된 모든 층 TCOW에 대한 개념 귀속 곡선은 그림 4:였다. 우리는 가장 중요한(왼쪽) 또는 가장 중요한(오른쪽)에서 주문한 개념을 제거한다. CRIS는 폐색(Occ) 또는 구배(IG)를 기반으로 하는 방법보다 더 나은 개념 중요도 순위를 생성한다.\n' +
      '\n' +
      '17.1의 점수(식 7)를 비교하여 가능한 모든 4-개념 간의 평균 \\(R\\) 점수는 0.6으로, 선택된 일치의 유의성을 나타낸다. 우리는 후기 층에서 객체 추적 표현을 캡처하는 그림 6의 채굴된 로제타 4 개념 중 하나를 시각화한다. 이 결과는 4가지 모델 모두 사이에 보편적 표현들이 실제로 존재한다는 것을 보여준다. 우리는 프로젝트 웹 페이지에 대한 공유 개념의 많은 예를 비디오 형식으로 보여준다.\n' +
      '\n' +
      '다음으로, 모든 로세타 d개념을 다양한 층에서 \\(d\\in\\{2,3,4\\}\\)로 정성적으로 분석하고, 그림 7의 대표적인 표본을 보여주고 있으며, 첫째, 초기 층에서는 모델들이 시공간적 기초표현을 학습한다는 것을 관측한다(그림 7, 왼쪽). 즉, 비디오의 시공간 볼륨을 이후 계층에서 상위 추론을 용이하게 하는 연결된 영역으로 분해한다. 이는 이미지 변압기[2, 24]의 초기 계층에서 공간 위치를 보여준 선행 작품들과 일치한다.\n' +
      '\n' +
      '중층(그림 7, 중간)에서 우리는 다른 것 중에서 모든 모델이 개별 대상을 지역화하고 추적하도록 학습한다는 것을 알게 된다. 이 결과는 최근 개발된 객체 중심 표현 학습(3, 23, 43, 55])에 새로운 각도를 도입하는데, 이는 비디오 변압기에서 객체 개념이 자연적으로 출현한다는 점을 감안할 때 전문화된 접근법이 어떻게 기여하는지 탐구하도록 초대한다. 또한 합성적으로 훈련된 TCOW를 제외한 모든 모델은 손 추적 개념을 개발하여 상향식 관점[57, 68]에서 액션 인식을 위한 손의 중요성을 확인한다.\n' +
      '\n' +
      '마지막으로 더 깊은 층에서 특정 시공간 이벤트를 포착하기 위해 대상 중심 표현 위에 구축된 개념을 찾는다. 예를 들어, 4개 중 3개는 4개 중 3개이다.\n' +
      '\n' +
      '그림 5: 목표 등급 _수거를 위해 SSv2에서 훈련된 K 윤활(왼쪽) 및 VideoMAE에서 학습된 TCOW 모델에 대해 가장 중요한 개념 < 탑-3>은 무언가를 a_(오른쪽)에 집어넣는다. 각 개념에 대해 두 개의 비디오를 나타내고 질의의 객체는 윤활의 녹색 경계로 표시된다. TCOW의 경우 \\(1^{st}\\)와 \\(2^{nd}\\)(2^{nd}\\)에서 가장 중요한 개념은 대상 및 증류기를 포함한 여러 물체를 추적한다. 비디오MAE의 경우, 상위 개념(톱 우)은 객체 및 낙하 이벤트(_i.e._손, 물체 및 용기)를 캡처하고 \\(2^{nd}\\) 가장 중요한 개념(중우)은 용기만을 캡처한다. 흥미롭게도 모델과 과제 모두에 대해 세 번째로 중요한 개념(바닥)은 시간적으로 불변 튜브릿이다. 5.3절(전체 결과를 위한 비디오)에서 더 많은 논의를 참조한다.\n' +
      '\n' +
      '그림 6: A 샘플 로제타 개념은 다른 작업에 대해 훈련된 4개의 모델에서 발견했다. 흥미롭게도 우리는 모든 모델(전체 결과에 대한 비디오 참조)에서 객체 중심 표현을 찾는다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '보편적인 디스커버리 쿼리입니다.\n' +
      '\n' +
      '이 부록에서 우리는 추가 결과, 시각화 및 구현 세부 사항을 보고한다. 프로젝트 웹 페이지에 대한 추가 비디오 결과와 해당 논의를 포함한다는 점에 유의하며, VTCD의 튜브렛 생성 구성 요소를 준수하고 7.1절에서 기존 개념 발견 접근법과 비교하여 시작하며, 7.2절에서는 7.3절에서 계층 간의 개념 중요도 분포에 대한 통계를 제공했으며, 다음 섹션 7.3에서는 동일한 층에서 다른 개념들이 서로 다른 자기 의도 머리에서 캡처되는 방법을 보여주는 추가 논의와 질적 결과를 제공한다. 마지막으로, 8절에서 더 많은 구현 세부 정보를 제공합니다.\n' +
      '\n' +
      '7가지 결과를 얻을 수 있습니다.\n' +
      '\n' +
      '### Tubelet validation\n' +
      '\n' +
      '픽셀 공간의 제안으로 입력을 분할하는 이전 방법과 달리 VTCD는 모델의 특징 공간에서 SLIC[1] 군집링을 통해 제안을 생성한다는 것을 상기시킨다. VTCD와 CRAFT [22] - 제안 생성을 위해 무작위 크로핑을 사용하는 최근 개념 발견 접근법을 비교하여 이 설계 선택을 표 2에 준수한다.\n' +
      '\n' +
      '특히, 우리는 방법과 CRAFT 모두에 대해 TCOW[62] 및 비디오MAE[60] 모델에 대한 개념 귀속 결과를 보고한다. 모든 경우에 VTCD는 모델의 표현에 더 충실한 개념을 초래한다. 제안이 성능에 미치는 영향을 추가로 분리하기 위해 CRAFT를 개념 중요도 추정 접근법(표에서 \'CRAFT[22] + CRIS\'로 표시된다. 결과는 CRIS가 CRAFT에 사용된 폐색 기반 마스킹보다 우수하다는 주요 논문에서 그림 4로부터 우리의 관찰을 확인시켜준다. 그러나 VTCD는 모든 환경에서 여전히 이 강한 기준선을 능가하여 변압기의 특징 공간에서 튜브렛 제안을 생성하는 것이 실제로 모델의 표현에 더 충실한 개념을 초래한다는 것을 검증한다.\n' +
      '\n' +
      '계층 개념 중요도 분석##\n' +
      '\n' +
      '우리는 이제 본고에서 5.2절에서 분석된 두 개의 표적 모델에 대한 각 모델 계층의 중요성을 정량화한다. 이를 위해 계층당 평균 개념 중요도 순위를 계산하고 이 값을 정규화하여 더 높은 값이 더 중요한 계층을 나타내는 \\([0-1]\\) 점수를 도출하고 그 결과를 그림 8에 나타내었다.\n' +
      '\n' +
      '우리는 즉시 두 모델 간의 유사점과 차이를 본다. 예를 들어, 처음 두 층은 두 모델 모두에 대해 중간 층보다 덜 중요하다. 비디오MAE의 경우 중간(6)과 끝층(12)이 가장 중요하다. 흥미롭게도 TCOW의 경우, 지금까지 가장 중요한 층은 층 3인 반면 최종층은 가장 중요하다. 이는 TCOW가 객체 추적 모델이기 때문에 직관적인 의미가 있기 때문에 초기 중층에서의 시공간적 위치 정보 및 객체 중심적 표현을 가장 많이 활용한다. 대조적으로, VideoMAE는 액션 분류를 위해 훈련되며, 이는 마지막 층에서 미세 구성되고 시공간적인 개념이 필요하다.\n' +
      '\n' +
      '헤드 콘셉트의 적합성이요.\n' +
      '\n' +
      '본고에서 4절에서 살펴본 바와 같이, 우리는 동일한 층이지만 다른 모델 헤드로부터 개념을 질적으로 시각화하여 헤드가 다양한 개념을 인코딩한다는 것을 입증한다. 예를 들어, 그림 9는 TCOW[62] 모델 중 5층 1층, 6층의 머리에서 발견된 개념이 관련 없는 개념(_e.g_위치 및 낙하물)을 인코딩한다는 것을 보여준다. 이것은 독립적인 정보를 포착하는 기존 작업[2, 17, 34, 63, 17]을 확증하므로 VTCD를 이용한 필요한 연구 단위이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{TCOW} & \\multicolumn{2}{c}{VideoMAE} \\\\ Model & Positive \\(\\downarrow\\) & Negative \\(\\uparrow\\) & Positive \\(\\downarrow\\) & Negative \\(\\uparrow\\) \\\\ \\hline CRAFT [22] & 0.174 & 0.274 & 0.240 & 0.300 \\\\ CRAFT [22] + CRIS & 0.166 & 0.284 & 0.157 & 0.607 \\\\ VTCD (Ours) & **0.102** & **0.288** & **0.094** & **0.625** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: TCOW [62] 및 비디오MAE [60] 모두에 대한 CRAFT [22]와 비교하여 관지 제안 접근법의 조정. 우리의 튜브렛은 기준선이 개념 점수 알고리즘(CRAFT [22] + CRIS)을 구비한 경우에도 모델의 표현에 더 충실한 개념을 초래한다.\n' +
      '\n' +
      '그림 8: VOS 모델(TCOW) 및 액션 인식 모델(VideoMAE)에 대한 모든 모델 계층에 대한 평균 개념 중요도이다. 흥미롭게도 비디오MAE는 모델에서 중후기 모두에서 중요한 개념을 인코딩하지만 TCOW는 3층에서 가장 중요한 개념을 인코딩하고 최종 계층에서는 가장 중요한 개념을 인코딩한다.\n' +
      '\n' +
      '8가지 구현 세부 정보를 제공합니다.\n' +
      '\n' +
      'TCOW - 0.01, 비디오MAE - 0.1, SSL-비디오MAE - 0.1, 내부 비디오 - 0.15, CNMF를 사용하여 군집링 개념이 0.15일 때 각 모델에 대해 설정된 조밀도를 제외하고 12개의 세그먼트를 사용하고 다른 모든 하이퍼파라미터를 Scikit-Image[61] 디폴트로 설정하고 각 모델에 대해 설정된 콤팩트-이미지 [1.1, S.1-0.1, VideoMAE - 0.1, SSL-VideoMAE - 0.1, S.1.1.2)를 사용하여 각 모델에 대해 설정된 콤팩트-이미지-이미지-이미지-이미지-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1, S.2-0.1\n' +
      '\n' +
      'CRIS를 사용한 모든 중요도 순위에 대한 개념 중요도는 원래 손실 모델을 사용하여 훈련했다. 내부 비디오는 텍스트 인코더로 클래스명을 부호화한 다음 텍스트와 비디오 특징 사이의 도트 제품을 촬영하여 대상 클래스에 로짓 값을 사용한다. TCOW[62]를 제외한 모든 모델에 대해 4,000회의 마스킹 반복을 사용하여 더 긴 수렴 시간을 경험적으로 관찰하고 8,000개의 마스크를 사용한다.\n' +
      '\n' +
      'VTCD로 프루닝하는 개념 개념 적용(본 논문의 제5.4절)을 대상으로 한 6개 수업은 다음과 같다.\n' +
      '\n' +
      '1.1. 1. 넘칠 때까지 무언가를 무언가를 무언가를 무언가를 무엇으로 만들며 1.1. 넘칠 때까지 무언가를 무언가를 무언가를 무언가를 무언가로 만들어 보세요.\n' +
      '2.2. 2. 무언가에 무언가를 뒤엎는 무언가를 죽일 수 있는 무언가 무엇에 뒤처져 있는 무언가를 던지세요.\n' +
      '3.3. 3.\n' +
      '4.4. 4. 4. 무언가에 무언가를 넣으세요.\n' +
      '5.5. 5. 위에 있는 물건으로 물건을 뜯어 놓으면 떨어져서 그 안에 있는 무언가가 빠지게 된다.\n' +
      '6.6. 무언가에 무언가를 붓고 있고, 6.6.\n' +
      '\n' +
      '그림 9: 같은 층의 서로 다른 헤드는 독립적인 개념을 포착한다. TCOW[62]의 층 5에서 헤드 6(톱 2 열)은 낙하된 물체를 강조하는 반면, 헤드 1(바닥 2 열)은 공간 위치를 포착한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Susstrunk. SLIC superpixels compared to state-of-the-art superpixel methods. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 34(11):2274-2282, 2012.\n' +
      '* [2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep VIT features as dense visual descriptors. In _ECCV Workshops_, 2022.\n' +
      '* [3] Zhipeng Bao, Pavel Tokmakov, Allan Jabri, Yu-Xiong Wang, Adrien Gaidon, and Martial Hebert. Discovering objects that can move. In _CVPR_, 2022.\n' +
      '* [4] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying interpretability of deep visual representations. In _CVPR_, 2017.\n' +
      '* [5] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _ACM FAccT_, 2018.\n' +
      '* [6] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In _CVPR_, 2017.\n' +
      '* [7] Michael Chang, Tomer Ullman, Antonio Torralba, and Joshua Tenenbaum. A compositional object-based approach to learning physical dynamics. In _ICLR_, 2016.\n' +
      '* [8] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In _ICCV_, 2021.\n' +
      '* [9] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In _CVPR_, 2021.\n' +
      '* [10] Jinwoo Choi, Chen Gao, C. E. Joseph Messou, and Jia-Bin Huang. Why can\'t I dance in the mall? Learning to mitigate scene bias in action recognition. In _NeurIPS_, 2019.\n' +
      '* [11] European Commission. Laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts. _European Commission_, 2021.\n' +
      '* [12] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. In _ICLR_, 2020.\n' +
      '* [13] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. _arXiv preprint arXiv:2309.16588_, 2023.\n' +
      '* [14] Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factorizations. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 32(1):45-55, 2008.\n' +
      '* [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* [16] Amil Dravid, Yossi Gandelsman, Alexei A Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in a model zoo. In _ICCV_, 2023.\n' +
      '* [17] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das-Sarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html).\n' +
      '* [18] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schaefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition. _Transformer Circuits Thread_, 2022. [https://transformer-circuits.pub/2022/toy_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html).\n' +
      '* [19] Christoph Feichtenhofer, Axel Pinz, Richard P Wildes, and Andrew Zisserman. Deep insights into convolutional networks for video recognition. _International Journal of Computer Vision_, 128:420-437, 2020.\n' +
      '* [20] Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. _NeurIPS_, 2022.\n' +
      '* [21] Thomas Fel, Victor Boutin, Mazda Moayeri, Remi Cadene, Louis Bethune, Mathieu Chalvidal, Thomas Serre, et al. A holistic approach to unifying automatic concept extraction and concept importance estimation. _NeurIPS_, 2023.\n' +
      '* [22] Thomas Fel, Agustin Picard, Louis Bethune, Thibaut Boissin, David Vigouroux, Julien Colin, Remi Cadene, and Thomas Serre. Craft: Concept recursive activation factorization for explainability. In _CVPR_, 2023.\n' +
      '* [23] Ruohan Gao, Dinesh Jayaraman, and Kristen Grauman. Object-centric representation learning from unlabeled videos. In _ACCV_, 2017.\n' +
      '* [24] Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich, Manli Shu, Micah Goldblum, Andrew Gordon Wilson, and Tom Goldstein. What do vision transformers learn? a visual exploration. _arXiv preprint arXiv:2212.06727_, 2022.\n' +
      '* [25] Amir Ghodrati, Efstratios Gavves, and Cees G. M. Snoek. Video time: Properties, encoders and evaluation. In _BMVC_, 2018.\n' +
      '* [26] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. In _NeurIPS_, 2019.\n' +
      '* [27] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The "something something" video database for learning and evaluating visual common sense. In _ICCV_, 2017.\n' +
      '* [28] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Ganapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: A scalable dataset generator. In _CVPR_, 2022.\n' +
      '* [29] Isma Hadji and Richard P Wildes. A new large scale dynamictexture dataset with application to convnet understanding. In _ECCV_, 2018.\n' +
      '* [30] Sven Ove Hansson, Matts-Ake Belin, and Bjorn Lundgren. Self-driving vehicles-An ethical overview. _Philosophy & Technology_, pages 1-26, 2021.\n' +
      '* [31] The White House. President biden issues executive order on safe, secure, and trustworthy artificial intelligence. _The White House_, 2023.\n' +
      '* [32] Filip Ilic and Axel Pinz. Representing objects in video as space-time volumes by combining top-down and bottom-up processes. In _WACV_, 2020.\n' +
      '* [33] Filip Ilic, Thomas Pock, and Richard P Wildes. Is appearance free action recognition possible? In _ECCV_, 2022.\n' +
      '* [34] Rezaul Karim, He Zhao, Richard P. Wildes, and Mennatullah Siam. MED-VT: Multiscale encoder-decoder video transformer with application to object segmentation. In _CVPR_, 2023.\n' +
      '* [35] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, and Fernanda Viegas. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV). In _ICML_, 2018.\n' +
      '* [36] Yu Kong and Yun Fu. Human action recognition and prediction: A survey. _International Journal of Computer Vision_, 130(5):1366-1401, 2022.\n' +
      '* [37] Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil DB Bruce, Richard P Wildes, and Konstantinos G Derpanis. A deeper dive into what deep spatiotemporal networks encode: Quantifying static vs. dynamic information. In _CVPR_, 2022.\n' +
      '* [38] Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil DB Bruce, Richard P Wildes, and Konstantinos G Derpanis. Quantifying and learning static vs. dynamic information in deep spatiotemporal networks. _arXiv preprint arXiv:2211.01783_, 2022.\n' +
      '* [39] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. _Nature_, 401(6755):788-791, 1999.\n' +
      '* [40] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M Rehg. Video segmentation by tracking many figureground segments. In _ICCV_, 2013.\n' +
      '* [41] Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In _CVPR_, 2019.\n' +
      '* [42] Yingwei Li, Yi Li, and Nuno Vasconcelos. Resound: Towards action recognition without representation bias. In _ECCV_, 2018.\n' +
      '* [43] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. _NeurIPS_, 2020.\n' +
      '* [44] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? _NeurIPS_, 2019.\n' +
      '* [45] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. _Distill_, 2020. [https://distill.pub/2020/circuits/zoom-in](https://distill.pub/2020/circuits/zoom-in).\n' +
      '* [46] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Nousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _Transformer Circuits Thread_, 2022. [https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).\n' +
      '* [47] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn? In _ICLR_, 2023.\n' +
      '* [48] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In _CVPR_, 2016.\n' +
      '* [49] Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: Randomized input sampling for explanation of black-box models. In _BMVC_, 2018.\n' +
      '* [50] Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshminarayanan, Alex Beutel, and Xuezhi Wang. Understanding and improving robustness of vision transformers through patch-based negative augmentation. _NeurIPS_, 2022.\n' +
      '* [51] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? _NeurIPS_, 2021.\n' +
      '* [52] Kanchana Ranasinghe, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Michael S Ryoo. Self-supervised video transformer. In _CVPR_, 2022.\n' +
      '* [53] Peter J Rousseeuw. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. _Journal of Computational and Applied Mathematics_, 20:53-65, 1987.\n' +
      '* [54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [55] Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Scholkopf, Thomas Brox, et al. Bridging the gap to real-world object-centric learning. In _ICLR_, 2023.\n' +
      '* [56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* [57] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric relation network. In _ECCV_, 2018.\n' +
      '* [58] Xinyu Sun, Peihao Chen, Liangwei Chen, Changhao Li, Thomas H Li, Mingkui Tan, and Chuang Gan. Masked motion encoding for self-supervised video representation learning. In _CVPR_, 2023.\n' +
      '* [59] Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the" object" in video object segmentation. In _CVPR_, 2023.\n' +
      '* [60] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learnersfor self-supervised video pre-training. _NeurIPS_, 2022.\n' +
      '* [61] Stefan Van der Walt, Johannes L Schonberger, Juan Nunez-Iglesias, Francois Boulogne, Joshua D Warner, Neil Yager, Emmanuelle Gouillart, and Tony Yu. scikit-image: image processing in python. _PeerJ_, 2:e453, 2014.\n' +
      '* [62] Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, and Carl Vondrick. Tracking through containers and occluders in the wild. In _CVPR_, 2023.\n' +
      '* [63] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In _ACL_, 2019.\n' +
      '* [64] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhinav Shrivastava. Teaching matters: Investigating the role of supervision in vision transformers. In _CVPR_, 2023.\n' +
      '* [65] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. VideoMAE v2: Scaling video masked autoencoders with dual masking. In _CVPR_, 2023.\n' +
      '* [66] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. InterrVideo: General video foundation models via generative and discriminative learning. _arXiv preprint arXiv:2212.03191_, 2022.\n' +
      '* [67] Jiajun Wu, Ilker Yildirim, Joseph J Lim, Bill Freeman, and Josh Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. _NeurIPS_, 2015.\n' +
      '* [68] Chuhan Zhang, Ankush Gupta, and Andrew Zisserman. Helping hands: An object-aware ego-centric video recognition model. In _ICCV_, 2023.\n' +
      '* [69] Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A Ehinger, and Benjamin IP Rubinstein. Invertible concept-based explanations for cnn models with non-negative concept activation vectors. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2021.\n' +
      '* [70] Daquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and Jose M Alvarez. Understanding the robustness in vision transformers. In _ICML_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>