<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '제한된 도메인 데이터로부터 값싼 추론을 갖는 # 전문화된 언어 모델\n' +
      '\n' +
      '데이비드 그랑지에, 안젤로스 카타로풀로스, 피에르 아블린, 아니 한누\n' +
      '\n' +
      'Apple Inc.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델은 다재다능한 도구로 등장했지만, 큰 추론 예산과 큰 도메인 내 훈련 세트가 부족한 작업에 적용하기 어렵다. 이 작업은 이러한 제약 조건을 공식화하고 네 가지 중요한 변수인 사전 훈련 예산(대상 도메인이 알려지기 전의 훈련용), 전문화 예산(대상 도메인이 알려지기 전의 훈련용), 추론 예산 및 도메인 내 훈련 세트 크기를 구별한다. 이러한 설정 전반에 걸쳐 기계 학습 문헌과 다른 접근법을 비교한다. 추론 비용으로 인해 매우 큰 바닐라 변압기 모델을 훈련하는 표준 관행에 대한 더 나은 대안을 찾을 수 있다. 특히, 하이퍼 네트워크와 전문가의 혼합이 대규모 사전 훈련 예산에 대해 더 나은 당혹감을 갖는 반면, 중요 샘플링 데이터 세트에 대해 훈련된 소규모 모델은 대규모 전문화 예산에 대해 매력적이라는 것을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델들을 훈련시키는 것은 다재다능한 모델들을 가능하게 하지만, 그들의 높은 추론 비용은 이들을 고가치 애플리케이션들로 제한한다(Brown et al., 2020; Bommasani et al., 2022). 근사화된 추론(Aminabadi et al., 2022; Sheng et al., 2023; Dettmers & Zettlemoyer, 2023)의 진보에도 불구하고, 대형 모델들은 비용이 많이 들거나 심지어 모바일 하드웨어에 비실용적이다. 엄격한 추론 제약 하에서, 우리는 당면한 도메인에 특화된 작은 모델을 고려할 수 있다. 본 논문은 제한된 도메인 데이터에서도 소규모 전문 모델을 교육하는 것을 연구한다. 낮은 복잡성을 달성하기 위해 일반 훈련 말뭉치, 중요도 샘플링 및 전문가 또는 하이퍼 네트워크의 혼합과 같은 훈련 중보다 추론 시 매개 변수가 적은 비대칭 모델의 세 가지 핵심 요소를 사용한다.\n' +
      '\n' +
      '추론 비용과 도메인 내 훈련 데이터 한계를 사용하여 다양한 훈련 비용을 가진 대체 전략을 연구한다. 또한 교육 비용이 도메인 간에 어떻게 공유될 수 있는지 고려합니다. 우리의 연구를 위해 우리는 4가지 중요한 메트릭을 고려한다:\n' +
      '\n' +
      '**일반적 훈련 비용** : 훈련 단계의 비용\n' +
      '\n' +
      '그림 1: 미리 정의된 계산 예산에 맞는 언어 모델을 훈련하기 위한 실용적인 권장 사항. 일반 학습 세트에서 전문화 데이터를 사용할 수 있기 전에 수행할 수 있습니다. 이 비용은 여러 전문 모델에 걸쳐 공유되며 종종 사전 교육이라고 합니다. 필수적인 것은 아니지만 전문화 데이터가 제한된 경우에는 일반적인 학습 데이터가 필수적이다.\n' +
      '\n' +
      '**전문화 훈련 비용** : 전문화 데이터가 이용가능하면 수행되는 훈련의 비용 이 비용은 다른 특수 모델 간에 공유되지 않습니다.\n' +
      '\n' +
      '** 추론 비용**: 전문화된 모델에 대한 추론을 실행하기 위한 비용. 추론 비용의 일환으로 메모리 및 네트워크 제약 조건을 고려하여 특수 작업당 적은 수의 매개변수를 포함하는 모델에도 관심이 있을 수 있다. 낮은 추론 비용은 더 넓은 모델 배치를 허용한다.\n' +
      '\n' +
      '**전문화 훈련 세트의 크기**: 애플리케이션에 따라 다양하며 사전 훈련 및 전문화 선택에 영향을 미칩니다.\n' +
      '\n' +
      '우리는 추론 비용과 전문화 데이터 크기를 하드 제약으로 간주하고 일반 및 전문화 훈련 비용을 변경하여 발생하는 작동 곡선을 연구한다. 우리는 서로 다른 훈련 접근 방식을 비교하고 어떤 작동 지점이 흥미로운지 강조한다.\n' +
      '\n' +
      '## 2 Methods\n' +
      '\n' +
      '우리는 대규모 일반 사전 훈련 세트를 활용하면서 추론 제약을 만족시키기 위해 다양한 아키텍처를 고려한다.\n' +
      '\n' +
      '우리는 동일한 데이터에 대해 훈련된 더 큰 모델과 비교하여 추론 제약으로부터 당혹감의 감소를 평가한다. 우리의 권장 사항은 그림 1에 요약되어 있다.\n' +
      '\n' +
      '### Large Model\n' +
      '\n' +
      '#### 대형 모델(LLM)\n' +
      '\n' +
      '하나는 일반 데이터에 대해 큰 언어 모델(LLM)을 훈련시키고, 추론 시 이 모델을 -is로 사용한다(Brown et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022). 이 접근법은 높은 사전 훈련 비용을 요구하지만 전문화 데이터를 요구하지 않으며 전문화 비용이 없다. 이 방법에 대한 추론 비용은 높다. 전문화된 데이터를 본 적이 없기 때문에 전문화 분포가 사전 훈련 분포와 거리가 먼 경우 부정확할 수 있습니다.\n' +
      '\n' +
      '일반 사전 훈련 후, 특성화 데이터에 대한 미세 조정은 모델을 적응시킬 수 있다(Howard & Ruder, 2018; Aghajanyan et al., 2021). 이 단계는 보통 당혹감을 개선하지만 전문화 비용을 추가한다. 이 비용은 전문화 데이터의 양이 적을 때 제한되며, 조기 중단은 과적합을 피하기 위해 몇 가지 업데이트로 미세 조정을 제한한다. 미세 조정은 추론 비용을 변경하지 않습니다.\n' +
      '\n' +
      '#### 파라미터 효율적인 튜닝\n' +
      '\n' +
      '하나의 미세 조정은 일단 전문화 데이터가 이용가능하면 파라미터들의 서브세트만을 조정한다(Hu et al., 2021; Lester et al., 2021; Houlsby et al., 2019). 이 전략은 전문화 데이터가 부족하면 과적합을 완화하기 때문에 유리하다. 그러나, 더 적은 수의 파라미터를 튜닝하는 것은 더 많은 미세-튜닝 단계를 필요로 할 수 있고, 따라서 전문화 비용을 증가시킨다. 이 방법은 각 새로운 전문화에 대해 작은 모델 델타만 통신할 필요가 있을 때 실용적이다. 부록의 F절을 참조하십시오.\n' +
      '\n' +
      '### Small Model\n' +
      '\n' +
      '#### 작은 모델(SLM)\n' +
      '\n' +
      '특화 데이터가 사용되기 전에 하나의 작은 언어 모델(SLM)을 훈련시키고 추론 시 이 모델을 그대로 사용한다. 이 방법은 전문화 데이터가 필요하지 않거나 전문화 비용이 발생하지 않습니다. 이 방법에 대한 추론 비용은 낮고 사전 훈련 비용도 낮다. 그러나, 작은 모델은 많은 양의 일반 데이터를 사용할 수 없을 뿐만 아니라 큰 모델이 다운스트림 성능을 악화시킨다. 더 큰 모델과 유사하게 미세 조정은 추가 전문화 비용으로 성능을 향상시킬 수 있다.\n' +
      '\n' +
      '#### No Pretraining(SLM-nopt)\n' +
      '\n' +
      '이 방법은 전문화 데이터에 대한 모델만 훈련합니다. 이는 전문화 예산과 전문화 데이터의 양이 많거나 일반 학습 분포가 전문화 영역에서 매우 멀리 떨어져 있는 경우에 유리하다.\n' +
      '\n' +
      '#### 중요도 샘플링(SLM-is)\n' +
      '\n' +
      '이 방법은 전문화 데이터를 사용할 수 있기 전에 모델을 사전 훈련하지 않습니다. 일단 전문화 세트가 주어지면, SLM-은 전문화 분포에 부합하도록 일반 사전 트레이닝 데이터로부터 맞춤화된 트레이닝 세트를 샘플링한다(Xie et al., 2023). 이 방법은 데이터 선택의 경우(Moore & Lewis, 2010; Grangier & Iter, 2022)로 전문화 데이터가 부족한 경우에 유리하다. 이 방법은 각 전문 영역에 대해 (아마도 큰) 맞춤형 훈련 세트에 대한 사전 훈련이 필요하기 때문에 높은 전문화 비용을 발생시킨다. 사전 훈련 후, 모델은 전문화 데이터에 대해 추가로 미세 조정될 수 있다.\n' +
      '\n' +
      '#### Distillation (SLM-d)\n' +
      '\n' +
      '이 방법(Hinton et al., 2015; Hsieh et al., 2023; Zhu et al., 2023)은 미세 조정된 큰 모델을 교사로 사용하여 작은 학생 모델에서 증류한다. 큰 교사 모델에 비해 추론 비용은 낮지만 정확도 또한 떨어진다. 증류가 없는 작은 모델에 비해 정확도가 더 좋을 수 있습니다. 교사 모델은 더 나은 일반화를 가진 모델에서 풍부한 목표를 제공하여 종종 전문화 세트에 대한 과적합을 감소시킨다. 이 방법은 SLM 미세 조정에 비해 큰 모델과 작은 모델의 일반적인 사전 훈련 비용이 더 높다. 교사 모델을 튜닝한 후 증류하는 동안 교사 출력을 수집해야 하기 때문에 전문화 비용도 더 크다.\n' +
      '\n' +
      '### 전문가의 경질혼합물(SLM-mix)\n' +
      '\n' +
      '이 방법(Eigen et al., 2014; Gross et al., 2017)은 클러스터링(clustering)을 통해 더 작은 서브세트에서 큰 프리트레이닝 세트를 분할하고, 각 파트에서 작은 모델(전문가)을 프리트레이닝한다. 사전 훈련 비용과 전체 매개변수 수는 군집 수에 따라 선형적으로 두 척도로 높다. 경질 혼합물에 대한 추론은 전형적으로 각각의 예를 예제가 속하는 클러스터에 대응하는 전문가에게 포워딩함으로써 수행된다.\n' +
      '\n' +
      '전문화 데이터가 확보되면 각 전문화 작업에 대해 단일 전문가를 선택하여 혼합물을 전문화한다. 한 가지 옵션은 전문화 데이터에서 사전 훈련 클러스터가 가장 빈번한 클러스터인 전문가를 선택하는 것이다. 또는 전문화 예산이 충분하다면 전문화 자료에서 평균적으로 손실이 가장 적은 전문가를 선정할 수 있다. 도메인당 단일 전문가를 선택하는 것은 대상 도메인에 대한 추론을 위해 혼합물 가중치의 작은 부분만 전달하고 로드하기 때문에 유리하지만 전문화 데이터가 여러 클러스터에 걸쳐 확산될 때 해로울 수 있다.\n' +
      '\n' +
      'SLM 믹스는 미세 조정이 가능합니다. 큰 전문화 예산으로 전문가 한 명 한 명을 미세 조정하고 가장 성능이 좋은 것을 선택할 수 있습니다. 예산이 적으면 프리트레이닝에서 가장 우수한 전문가 또는 전문화 데이터에서 프리트레이닝 클러스터가 가장 빈번한 전문가만을 미세 조정할 수 있다. 이 두 번째 옵션은 SLM 미세 조정과 동일한 전문화 비용을 나타낸다.\n' +
      '\n' +
      '### Hyper-Networks (SLM-hn)\n' +
      '\n' +
      '하이퍼-네트워크(Ha et al., 2017)는 하이퍼-서브-네트워크와 인스턴스화된 서브-네트워크의 두 부분으로 분해되는 신경망이다. 하이퍼-서브-네트워크는 인스턴스화된 서브-네트워크에 대한 가중치들을 생성한다. 우리는 전문가의 혼합을 만들기 위해 하이퍼-네트워크에 의존하는데, 하이퍼-서브-네트워크는 서브-네트워크 가중치를 생성하기 위해 입력의 클러스터 멤버쉽을 취한다. 이러한 클러스터별 가중치는 작은 하위 네트워크 또는 전문가를 인스턴스화한다. 전문가들의 단단한 혼합에 비해 SLM-hn은 하이퍼-서브-네트워크를 통해 전문가들 간에 매개변수를 공유하고 혼합물의 용량과 클러스터 수를 독립적으로 선택할 수 있는 유연한 방법을 제공한다. 인스턴스화된 서브 네트워크는 전문화 데이터에 대해 미세 조정될 수 있다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '실험을 위한 데이터셋, 각 방법에 대한 실험 설정 및 평가 메트릭을 제시한다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리의 일반적인 사전 훈련 세트는 c4, commoncrawl(Raffel et al., 2020)에서 파생된 영어 텍스트의 대규모 필터링된 데이터 세트이다. 어휘 크기가 32k인 c4에서 학습된 문장 조각 모델로 데이터를 토큰화한다. Pile(Gao et al., 2021)에서 추출된 9개의 다양한 영역을 전문화하는 것을 고려한다: arxiv(과학 기사), Europarl(의회 절차), freelaw(법률 텍스트), gutenberg(1919년 이전에 출판된 오래된 책), openubtles(연극 자막), openwebtext2(포럼 토론), pubmed-abstracts(의학 기사 초록), stackexchange(대부분 기술 주제에 관한 Q&A), wikipedia(encyclopedia 기사). 우리는 사용 가능한 전문화 학습 데이터의 양을 다양화하고 각 도메인에 대해 크기 1, 8 및 6400만 토큰 세트를 고려한다.\n' +
      '\n' +
      '### Clustering\n' +
      '\n' +
      '경성 혼합 전문가, 하이퍼 네트워크 및 중요도 샘플링은 문서 클러스터링에 의존합니다. 우리는 문장 BERT(Reimers and Gurevych, 2019)를 사용하여 c4 문서를 768차원 벡터로 임베딩하고 kmeans 알고리즘으로 클러스터링한다. 우리는 4개에서 1,024개의 군집에 이르는 다양한 수의 군집을 탐색한다.\n' +
      '\n' +
      '### Language Models\n' +
      '\n' +
      '우리는 변압기 모델(Vaswani et al., 2017)로 실험을 수행한다. 우리는 작은 모델과 큰 모델의 두 가지 크기를 고려합니다. 작은 모델은 126M개의 파라미터를 가지며, 1,024차원의 7개 계층과 4,096차원의 잠재 피드포워드 차원으로 구성되며, 큰 모델은 2,816차원의 7개 계층과 11,264차원의 잠재 피드포워드 차원으로 구성된 770M개의 파라미터를 가지며, 최대 1,024개의 토큰의 컨텍스트로 학습되고 평가되며, 긴 문서를 겹치지 않는 윈도우로 분할한다.\n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      '증류를 위해 우리는 교사로 미세 조정된 LLM과 학생으로 일반 세트에서 사전 훈련된 SLM을 사용한다. 증류 훈련은 특성화 데이터에 대해 작동하고 학생의 예측과 교수 분포 사이의 KL 발산을 최소화하기 위해 학생을 훈련시키며, 이는 데이터 분포와 교사 모델 예측 사이의 혼합물이다(Hinton et al., 2015). 교시 혼합물 중량은 하이퍼파라미터(실험에서 0.95)입니다. 이 방법에서 일반적인 훈련 비용은 교사 모델의 훈련에 의해 지배되고 전문화 비용도 교사 모델의 미세 조정 비용에 의해 지배된다. 이 방법은 일반 데이터세트에서 SLM을 사전 훈련하고 전문화 데이터세트에서 SLM을 가르치는 비용이 추가로 더 작다.\n' +
      '\n' +
      '###전문가들의 혼합\n' +
      '\n' +
      '우리는 프리 트레이닝을 위해 변압기의 하드 혼합물(Gross et al., 2017)을 트레이닝한다. 프리트레이닝 세트는 클러스터로 분할되고 각각의 클러스터에 독립적인 SLM이 트레이닝된다. 전문화를 위해, 우리는 전문화 세트에서 가장 빈번한 클러스터를 결정하기 위해 사전 훈련 중심과 함께 전문화 세트를 클러스터링하는 간단한 전략을 고려한다. 우리는 이 클러스터에서 미리 훈련된 모델만 미세 조정한다. 하드 혼합물은 작은 모델에서만 추론을 미세 조정하고 실행하는 동안 총 매개변수 수가 많은 모델을 훈련할 수 있기 때문에 여기에서 흥미롭다.\n' +
      '\n' +
      '사전 훈련 예산이 낮으면 모든 클러스터에 대한 사전 훈련 모델을 포기하고 대신 전문화 예산을 증가시켜 이 세트가 사용 가능하면 전문화 세트에서 가장 빈번한 클러스터에 해당하는 일반 데이터 세트의 클러스터에서만 모델을 훈련할 수 있다.\n' +
      '\n' +
      '### Hyper-Networks\n' +
      '\n' +
      '하이퍼-네트워크(Ha et al., 2017; Karimi Mahabadi et al., 2021)는 컨디셔닝 입력 변수에 기초하여 2차 네트워크인 _hyper-network_로부터 가중치가 자체적으로 생성되는 신경망의 일반적인 아이디어를 정의한다.\n' +
      '\n' +
      '본 논문에서는 3.2절에서 언급한 클러스터링을 이용하여 각 예제를 클러스터 멤버쉽 변수와 연관시킨다. 이 변수는 하이퍼 네트워크의 입력으로 처음 두 개의 레이어를 제외한 모든 레이어에 대한 트랜스포머 언어 모델의 피드 포워드 행렬(multi-layer perceptron, MLP) 행렬을 생성한다. 변압기의 다른 파라미터는 클러스터에 의존하지 않으며 모든 예에서 동일하다.\n' +
      '\n' +
      '하이퍼 네트워크는 각 계층 \\(l\\)과 각 클러스터 \\(i\\)에 대해 두 개의 MLP 행렬 \\(W^{(1,I,i)}, W^{(2,I,i)}\\)을 인스턴스화한다. 잠재차원(h\\)과 전문가 수(m\\)의 두 가지 하이퍼파라미터에 의존한다. 각 군집 \\(i\\)은 h차원 임베딩 \\(c^{(i)}\\)과 연관된다. 각 층 1은 \\(h\\times m\\)-matrix \\(M^{(l)}\\)과 연관되어 있다. 우리는 행렬을 계산한다.\n' +
      '\n' +
      '\\[W^{(1,I,i)}=c^{(i)}\\M^{(l)}\\cdot T^{(1,I)}\\text{ and}\\W^{(2,I,i)}=c^{(i)}\\M^{(l)}\\cdot T^{(2,I)}\\text{\n' +
      '\n' +
      '벡터\\(c^{(i)}\\M^{(l)}\\)와 3차원 텐서\\(T^{(1,I)},T^{(2,I)}\\)의 가중합으로 형상\\(m\\times d\\text{latent}}\\times d\\text{in}\\)과 \\(m\\times d\\text{in}}\\times d\\text{latent}\\)을 각각 구한다. 이러한 텐서는 대부분의 모델 매개변수, 즉 해당 MLP 행렬의 수보다 \\(m\\)배의 매개변수를 보유한다. 이 전략을 통해 각 클러스터에 대해 인스턴스화된 모델의 크기를 일정하게 유지하면서 전체 모델 용량을 증가시킬 수 있다. 물론, 하이퍼-네트워크 아키텍처 중 하나의 선택을 설명하지만, 많은 대안들이 가능하다(Muqeeth et al., 2023; Abnar et al., 2023). 하이퍼 네트워크는 전문가의 단단한 혼합에 비해 훈련 문제를 독립적인 저메모리 훈련 작업으로 나눌 수 없기 때문에 용량 제한이 더 강하다. 반면에, 각 \\(m\\) 전문가의 가중치는 모든 전문가에 대해 동일한 주의 파라미터와 MLP 텐서 모두 공동으로 훈련되므로 하이퍼 네트워크 모델이 더 효율적인 파라미터가 될 수 있다.\n' +
      '\n' +
      '전문화를 위해 하드 혼합 사례와 유사한 전략을 따르며, 전문화 집합에서 가장 빈번한 클러스터에서 모델을 인스턴스화하고 미세 조정한다. 따라서 미세 조정은 큰 하이퍼 네트워크에서 작동하지 않고 작은 인스턴스화된 모델에서만 작동한다.\n' +
      '\n' +
      '### Importance Sampling\n' +
      '\n' +
      '중요도 샘플링 방법은 섹션 3.2의 k-평균 군집화에 의존하며, 표적 분포(h^{t}\\)에서 군집 주파수의 히스토그램만을 필요로 하는 스트리밍 방법이다. 이것은 사전 훈련 문서(예: \\(N\\simeq 100\\)k)의 큰 버퍼에 의존한다. 버퍼에서 클러스터 히스토그램 \\(h^{b}\\)을 계산하고 각 클러스터 \\(i\\)에서 \\(N_{i}\\) 문서를 취한다. \\ (N_{i}=N\\times h^{t}_{i}\\times\\min_{j}(h^{b}_{j}/h^{t}_{j})\\). 는 \\(N_{i}\\)의 히스토그램이 목표 히스토그램 \\(h^{t}\\)과 일치하도록 시행하면서 각 클러스터에 대해 취할 수 있는 최대 문서 수이다. 그런 다음 선택한 데이터를 학습에 사용합니다.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '우리는 평가를 위해 표준 언어 모델링 메트릭인 당혹감에 의존한다. 데이터 세트당 20k 문서를 사용하여 보류된 데이터에 대한 복잡성을 측정한다. 우리는 언어 모델링에만 초점을 맞추고 다운스트림 작업(예: 질문 응답, 감정 분석, 번역 등)에 대한 모델을 평가하는 것은 논문의 범위를 벗어난다.\n' +
      '\n' +
      '동일한 하드웨어(Nvidia-A100)에서 그래픽 프로세서 연산 시간(GPUh)의 시간으로 훈련 비용(사전 훈련 및 전문화)을 측정한다. 우리는 10에서 650 GPUh 범위의 사전 훈련 비용과 0.3에서 120 GPUh 범위의 전문화 비용을 고려한다.\n' +
      '\n' +
      '##4 실증결과\n' +
      '\n' +
      '각 방법에 대한 자세한 논의에 들어가기 전에 먼저 주요 결과를 보고합니다.\n' +
      '\n' +
      '표 1은 사전 훈련된 모형과 전문화된 모형에 대한 모수의 수를 보고한다. 표 1은 SLM-hn 및 SLM-믹스가 전문화 후 추론을 위해 SLM만큼 작은 반면 사전 훈련된 매개변수의 전체 수는 LLM보다 더 크다는 것을 보여준다. 표 2는 모델의 처리량을 보고한다. 모든 SLM 모델은 동일한 전문화 처리량을 갖는 반면 SLM-hn은 사전 훈련을 위한 SLM, SLM-믹스보다 낮은 처리량을 갖는다. LLM은 모든 경우에 더 비싸다. 표 3은 모든 설정에 대한 사전 훈련 및 전문화를 위한 훈련 예산의 상한을 나타낸다.\n' +
      '\n' +
      '우리는 그림 2의 각 방법에 대해 다양한 사전 훈련 예산을 고려하고 일반적인 사전 훈련 세트(c4)에 대한 복잡성을 보고한다. SLM-hn 및 SLM-믹스를 고려할 때 사전 훈련 매개변수의 수가 LLM보다 크더라도 좋은 복잡성을 즐기지 않는다는 것을 관찰한다. 그러나 단일 클러스터에서 테스트하거나 미세 조정할 때 효율성만큼 복잡성이 SLM보다 우수하다.\n' +
      '\n' +
      'c4의 복잡성은 우리의 주요 목표가 아니며 파일에서 전문화 도메인에 대한 복잡성을 보고한다. 섹션 3.1. 우리는 9개의 세트에 대해 _macro-averaging_에 의해 보류된 복잡성을 보고한다. 우리는 평균 음수를 계산한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r} \\hline \\hline Model & \\multicolumn{2}{c|}{Num. parameters (M)} \\\\  & Generic Pretrain & Inference \\\\ \\hline Small LM (SLM) & 126 & 126 \\\\ Mixt. of experts (SLM-mix) & 2,016 & 126 \\\\ Hyper network (SLM-hn) & 1,422 & 126 \\\\ Large LM (LLM) & 771 & 771 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 사전 훈련 및 추론을 위한 파라미터 수(수백만 개).\n' +
      '\n' +
      '각 집합에 대한 토큰당 로그 가능성, 9개의 수를 평균화하고 지수를 계산합니다. 따라서 모든 도메인은 고정 집합의 크기에 관계없이 동일한 가중치를 받습니다.\n' +
      '\n' +
      '그림 3(a)는 미세 조정 전 결과를 보고한다. 보고된 복잡성은 c4 복잡성보다 훨씬 높으며 전문화가 필요함을 나타낸다. 그림 3(b)는 각 도메인의 1M 토큰 데이터셋에 각 방법에 대해 미리 훈련된 여러 체크포인트를 미세 조정한 후 결과를 보고한다. 각 도메인별 모델은 매크로 평균화 전에 평가됩니다. 1M 토큰은 작은 집합이기 때문에 미세 조정은 작은 학습률과 조기 정지(기본 학습률을 3으로 나누면 하나의 GPU에서 2k 미만의 미세 조정 단계 후에 항상 정지)에 의존한다. 미세 조정은 모든 방법에 매우 유익하며 상당히 개선된 당혹감을 초래한다. 우리는 또한 파일 상의 사전-미세 조정 당혹성이 반드시 사후-미세 조정 당혹성의 좋은 지표는 아니라고 언급한다: 예를 들어 SLM 체크포인트 순서는 두 곡선에서 매우 다르며, SLM-믹스와 SLM-hn 사이의 순서 또한 미세 조정 동안 변경된다.\n' +
      '\n' +
      '우리는 또한 각 도메인에 대해 8 및 6400만 토큰에 대한 미세 조정을 고려하는데, 그림 3(c) 및 (d)를 참조한다. 더 많은 데이터를 통해 우리는 약간 더 오래 훈련할 수 있고 과적합 없이 기본 학습률을 유지할 수 있다. 우리는 각각 8M 및 64M 경우에 대해 최대 4k 단계 및 30k 단계 후에 멈춘다. 우리는 SLM-hn과 SLM-mix(SLM에 비해)가 제공하는 좋은 시작점의 이점이 도메인 트레이닝 세트 크기가 증가함에 따라 침식되는 것을 관찰한다.\n' +
      '\n' +
      '이 수치들은 SLM-의 당혹감을 일정한 선으로 보고한다. 이 방법은 도메인 데이터가 사용 가능한 경우에만 훈련을 시작할 수 있으므로 사전 훈련이 없습니다. 전문화 단계에서 모든 훈련 비용을 부담해야 합니다. SML-is는 사후 전문화 당혹성 측면에서 작은 추론 모델을 가진 가장 좋은 방법이다. 흥미롭게도, 그것은 심지어 특정 도메인 내 데이터가 부족할 때 훨씬 더 큰 모델(즉, 1M 토큰 사례)보다 우수하다.\n' +
      '\n' +
      '### 작은 모형과 큰 모형\n' +
      '\n' +
      '표 4는 기준 변압기 모델에 대한 파일 하위 집합의 복잡성을 비교한다. 사전 훈련과 미세 조정은 모두 전문화 세트에서 좋은 당혹감을 얻기 위해 필요합니다. 사전 훈련(SLM-nopt) 없이, 수용 가능한 성능을 얻기 위해 많은 전문화 데이터(도메인당 64M 토큰)가 필요하다. 우리는 또한 대형 및 소형 모델 모두에 대해 피네튜닝 전후의 복잡성에 큰 격차가 있음을 관찰하며, 1M 도메인 토큰에서도 피네튜닝이 상당한 성능 향상을 가져올 수 있음을 분명히 한다. 마지막으로 예상대로 LLM 결과는 또한 큰 추론 및 사전 훈련 예산에 대해 사전 훈련 세트(c4)에서 큰 모델을 훈련하는 것이 유익하다는 것을 보여준다.\n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      '증류 공정은 사전 훈련된 교사(LLM)와 사전 훈련된 학생(SLM)을 사용한다. 우리는 전문화 세트에서 선생님을 미세 조정하고 미세 조정된 선생님을 사용하여 같은 세트에서 학생을 감독한다. 이 과정에서 일반적인 사전 훈련 비용은 교사와 학생 사전 훈련이라는 두 가지 용어를 합산한다. 이 두 조건 사이의 비용을 어떻게 가장 잘 분산시킬 것인지가 문제이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r} \\hline \\hline Model & \\multicolumn{2}{c|}{Training} & Inference \\\\  & Generic Pre. & Specialization & \\\\ \\hline SLM & 2.2 & 2.2 & 0.61 \\\\ SLM-mix & 2.2 & 2.2 & 0.61 \\\\ SLM-hn & 3.6 & 2.2 & 0.61 \\\\ SLM-is & N/A & 2.2 & 0.61 \\\\ LLM & 7.7 & 7.7 & 2.54 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모델 처리량(1B 트레이닝 토큰당 GPU 시간)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r r r} \\hline \\hline Model & Pretraining & \\multicolumn{3}{c}{Specialization} \\\\  & & 1M & 8M & 64M \\\\ \\hline LLM & \\(\\leq\\) 650 & \\(\\leq\\) 0.12 & \\(\\leq\\) 0.5 & \\(\\leq\\) 3.5 \\\\ SLM & \\(\\leq\\) 530 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ SLM-is & 0 & \\(\\leq\\) 130 & \\(\\leq\\) 130 & \\(\\leq\\) 130 \\\\ SLM-d & \\(\\leq\\) 1,850 & \\(\\leq\\) 0.7 & \\(\\leq\\) 2.8 & \\(\\leq\\) 21 \\\\ SLM-mix & \\(\\leq\\) 650 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ SLM-hn & \\(\\leq\\) 650 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 사전 훈련 및 전문화를 위한 열차비 한도(GPUh)\n' +
      '\n' +
      '그림 2: c4에 대한 일반적인 사전 훈련 복잡성.\n' +
      '\n' +
      '그림 4(왼쪽)는 서로 다른 양의 교사 사전 훈련에 해당하는 각 곡선으로 SLM-d 당혹감을 보고하고 학생 사전 훈련을 x축으로 한다. 그것은 교사 사전 훈련의 276 GPUh 이상의 설정(300k 단계)에서 학생 모델 SLM-d가 동일한 수준의 학생 사전 훈련에서 바닐라 SLM보다 훨씬 더 우수하다는 것을 보여준다. 이 그림은 전문화 설정된 목표에 대해서만 훈련된 SLM에 대한 좋은 교사의 이점을 보여준다.\n' +
      '\n' +
      '그림 4(오른쪽)은 교사 및 학생 사전 훈련 비용을 합산하여 전체 일반 사전 훈련 비용을 보고하기 위해 x축을 변경하는 동일한 데이터를 보여준다. 교사 사전 훈련 비용을 고려할 때 SLM-d는 SLM-hn 및 SLM-믹스와 같은 최상의 방법과 경쟁력이 없다.\n' +
      '\n' +
      '그림 4: 다양한 교사 사전 훈련 예산에 대한 1M 토큰 전문화 세트에 대한 증류 결과(점선) 왼쪽에서는 학생 사전 훈련 비용과 관련하여 당혹감을 나타내고 오른쪽에서는 전체 사전 훈련 비용과 관련하여 당혹감을 나타낸다.\n' +
      '\n' +
      '그림 3: 다른 양의 전문화 데이터로 미세 조정 전후의 파일 하위 집합(평균)에 대한 특정 복잡성.\n' +
      '\n' +
      '###전문가들의 혼합\n' +
      '\n' +
      '전문가들의 단단한 혼합물은 클러스터에서 분할된 일반 데이터 세트에 의존하며, 섹션 3.2를 참조하며, 전문가 수는 클러스터 수에 해당한다. 세부 조정을 위해 대상 도메인 데이터 세트에서 가장 빈번한 클러스터에 해당하는 전문가만 세부 조정한다. 이 절에서는 클러스터의 수를 달리하고 가장 빈번한 클러스터를 선택하는 것이 좋은 전략인지에 대해 논의한다.\n' +
      '\n' +
      '혼합물의 전체 용량과 훈련 비용은 클러스터 수에 비례한다. 우리의 주요 결과(그림 2, 그림 3 등) 16명의 전문가를 사용합니다. 4~256명의 전문가와 결과를 비교한다. 직관적으로 전문가의 수가 너무 많으면 모델을 훈련하는 데 더 많은 비용이 들며 각 클러스터에는 SLM 크기의 모델을 훈련하기에 충분한 데이터가 포함되어 있지 않다. 반대로, 전문가의 수가 너무 적으면 훈련 비용은 낮지만 SLM 크기의 각 전문가는 큰 클러스터에서 훈련을 받고 훈련 세트를 과소 적합시킬 것이다. 또한 큰 군집은 너무 일반적이며 대상 집합의 분포와 거리가 멀 수 있습니다. 그림 5는 1M 토큰 전문화 세트의 경우 서로 다른 혼합물 크기에 대한 일반적인 사전 훈련 시간의 함수로 파일 상의 매크로 평균 당혹함을 보여준다.\n' +
      '\n' +
      '전술한 바와 같이, 전문화는 단일 전문가를 미세 조정한다. 전문화 자료에서 가장 빈번한 군집에 해당하는 전문가를 선정한다. 또는, 각 전문가에 대한 평가를 포함하는 미세 조정 전에 전문화 세트에서 손실이 가장 적은 전문가를 선택하는 것도 고려한다. 세 번째 비용 추가 옵션으로 모든 전문가를 미세 조정하고 가장 좋은 전문가를 사후에 선택합니다. 표 5는 64 전문가 모델에 대해 1M 토큰에 대한 미세 조정 시 이 결과를 보고한다. 서로 다른 전략의 결과는 서로 0.3 PPL 이내이다. 모든 전문가를 미세 조정하는 가장 비용이 많이 드는 옵션이 가장 잘 작동합니다.\n' +
      '\n' +
      'SLM-믹스에 대한 최종 관찰로서 가장 빈번한 클러스터에 해당하는 전문가만을 미세 조정하는 전략은 사전 훈련에서 전문화로 훈련 비용의 전이를 가능하게 한다. 즉, 대상 도메인이 알려질 때까지 기다린 다음 관심 있는 단일 클러스터에서 하나의 모델만 사전 훈련할 수 있다. 이것은 하나의 도메인이 소수의 도메인만을 대상으로 할 때 흥미롭다. 그러나 이 전략은 그림 6과 같이 중요도 샘플링만큼 잘 수행되지 않는다.\n' +
      '\n' +
      '### Hyper-networks\n' +
      '\n' +
      '우리의 하이퍼 네트워크의 전문가 수는 추론 모델의 크기를 일정하게 유지하면서 전체 매개변수 수를 조정할 수 있다. 그림 7은 1M 토큰에 대한 미세 조정 후 파일 하위 집합에 대한 당혹감을 보여준다. 더더욱\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r r r} \\hline \\hline Model & Pretrained & \\multicolumn{4}{c}{Specialized} \\\\  & & 1M & 8M & 64M \\\\ \\hline SLM & 33.0 & 18.2 & 14.8 & 12.0 \\\\ SLM-nopt & N/A & 227.1 & 45.6 & 17.6 \\\\ LLM & 28.1 & 14.4 & 12.5 & 10.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 소형 및 대형 LMs에 대한 파일 상의 복잡도(평균) (일반 사전 훈련의 650 GPUh의 한계에 대해)\n' +
      '\n' +
      '그림 5: 1M 토큰에 대한 미세 조정 후 파일 하위 집합(평균)에 대한 4-256명의 전문가와 혼합 모델의 특정 복잡성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r} Method & Perplexity & Specialization cost \\\\ \\hline Most frequent cluster & 17.32 & 1x \\\\ Best pre-trained & 17.05 & 1x \\\\ Best fine-tuned & 16.98 & 64x \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 최고의 전문가를 선정하는 것. 1M 토큰에 대한 평균 특정 복잡성은 700k 사전 훈련 단계(\\(\\sim\\) 600 GPUh) 후 64명의 전문가와 동일한 혼합물에서 서로 다른 전문가를 미세 조정한다.\n' +
      '\n' +
      '그림 6: 가장 빈번한 도메인 클러스터에서 SLM-믹스만 훈련하는 경우, 1M 토큰에서 미세 조정 후 특정 복잡성.\n' +
      '\n' +
      '전문가는 반복마다 항상 더 나은 성능을 발휘하지만 32명의 전문가가 우리 설정에서 더 계산 시간이 효율적입니다.\n' +
      '\n' +
      '### Importance Sampling\n' +
      '\n' +
      '중요도 샘플링 전략은 c4의 군집 히스토그램이 대상 파일 하위 집합의 군집 히스토그램과 일치하도록 재샘플링한다. 클러스터의 수는 중요한 매개 변수입니다. 소수의 클러스터는 거친 방식으로만 c4 분포를 변경할 것이며, 타겟팅된 세트와의 낮은 충실도 일치를 제공할 것이다. 반대로, 많은 수의 클러스터는 두 가지 단점을 가지고 있다. 첫째, 전문화 집합이 작은 경우 많은 수의 군집에 대해 군집 빈도가 제대로 추정되지 않을 수 있다. 둘째, 많은 수의 군집에서 표적 히스토그램은 소수의 작은 군집에 질량의 큰 부분을 집중시킬 수 있으며, 이는 재샘플링된 c4 데이터 세트가 이러한 군집에서 많은 반복 지점을 포함할 것임을 의미한다. 이는 재샘플링된 c4 데이터세트의 유효 크기가 이러한 반복으로 더 작아짐에 따라 성능을 저하시킬 수 있다.\n' +
      '\n' +
      '주요 결과는 1,024개의 군집으로 중요도 샘플링 결과를 보고한다. 그림 8은 16, 64, 256 및 1,024개의 군집으로 결과를 보고한다.\n' +
      '\n' +
      '중요도 샘플링 방법은 전문화 세트가 주어지기 전에 트레이닝을 시작하지 않고, 각 전문화 태스크에 대해 상이한 재샘플링된 데이터세트 상에서 모델이 처음부터 사전 트레이닝된다. 이는 중요도 샘플링이 미세 조정에 비해 훨씬 더 큰 전문화 비용을 가지며 이러한 불일치는 많은 작업을 처리할 때만 더 중요해진다는 것을 의미한다. 모델의 경우 \\(N\\) 작업에 대한 전문화의 총 비용은 다음과 같습니다.\n' +
      '\n' +
      '\\[C_{\\text{total}}(N)=C_{\\text{generic\\ pretrain}}+C_{\\text{specialization}}\\times N.\\tag{1}\\\n' +
      '\n' +
      '하이퍼 네트워크와 같은 방법의 경우, 대부분의 비용은 \\(C_{\\text{generic\\ pretrain}}\\)이며, 총 비용을 변화시키는 주요 매개변수는 일반적인 사전 훈련 단계의 수이다. 중요도 샘플링 방법은 중요도 샘플링된 사전 훈련 집합에 대한 훈련 시 수행되는 단계 수(C_{\\text{generic\\pretrain}=0\\)와 전체 비용을 변화시키는 주요 매개변수이다.\n' +
      '\n' +
      '우리는 방정식 1에 따라 x축을 스케일링하여 1, 7, 50개의 태스크를 가정적으로 다룰 때 SLM-hn과 SLM-is에 대한 총 비용을 변화시킨다. 그림 9는 태스크의 수가 증가할 때 SLM-is가 덜 흥미로워짐을 보여준다. 작업의 수에 따라 선형적으로 증가하는 SLM-hn의 미세 조정에 소요되는 전문화 비용은 \\(\\sim 1\\)GPU 분으로 무시할 수 있다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '언어 모델링을 위한 도메인 적응은 신경망 언어 모델보다 오래된 역사를 가지고 있다(Rosenfeld, 2000). 이 연구는 대상 도메인에서 멀리 떨어진 많은 양의 데이터에 대해 훈련된 모델이 최종 응용 프로그램에 영향을 미친다는 관찰에서 비롯되었다.\n' +
      '\n' +
      '그림 8: 1M 토큰에 대한 미세 조정 후 클러스터 수가 다른 중요도 샘플링에 대한 특정 복잡성.\n' +
      '\n' +
      '그림 7: 1M 토큰에 대한 미세 조정 후 전문가 수가 다른 하이퍼 네트워크에 대한 특정 당혹감.\n' +
      '\n' +
      '그림 9: 1M 토큰 전문화 데이터 세트에 대한 하이퍼-네트워크 대 중요도 샘플링에 대한 특정 복잡성. 과제 수를 변화시키면 중요도 샘플링 비용이 선형적으로 증가한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'Fei-Fei, L., Finn, C., Gale, T., Grossman, T., Hashimoto, T., Henderson, P., Hewitt, J., Ogut, G., Orr, L., Reich, R., Ren, H., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khattab, O., Koh, P. L., M., Lee, T., Malik, A., Manning, C.\n' +
      '* Brants et al. (2007) Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. Large language models in machine translation. In Eisner, J. (ed.), _Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)_, pp. 858-867, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL[https://aclanthology.org/D07-1090](https://aclanthology.org/D07-1090).\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Grey, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D. 언어 모델들은 소수의 학습자들이다. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, Volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL[https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips\n' +
      '* Caruana(1993) Caruana, R. 멀티태스크 학습: 귀납적 편견의 지식 기반 원천. In _Proceedings of the Tenth International Conference on Machine Learning_, pp. 41-48. Citeseer, 1993.\n' +
      '* Chowdhery et al.(2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, K., Schuh, P., Shi, K., Tsyvashchenko, S., Maynez, J., Ghemawat, S., Dounes, P., Tay, Y., Prabhakaran, V., Reif, R., Du, N., Hutchinson, B., Firat, O., Catasta, M., Dai, J., Meier-Hellstern, K., M. 팜: 2022년 경로가 있는 언어 모델링 스케일링\n' +
      '* Clark et al. (2022) Clark, A., Casas, D. d., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Cai, T., Borgeaud, S., Driessche, G. v. d., Rutherford, E., Hennigan, T., Johnson, M., Millican, K., Cassirer, A., Jones, C., Buchatskaya, E., Budden, D., Sifre, L., Osindero, S., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K., and Simonyan, K. 라우팅된 언어 모델에 대한 통합 스케일링 법칙. In _Proceedings of the 39th International Conference on Machine Learning_. PMLR, 2022년\n' +
      '* Collobert et al. (2011) Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kksa, P. Natural Language Processing(거의) from scratch. _ Journal of machine learning research_, 12(ARTICLE):2493-2537, 2011.\n' +
      '* Dettmers & Zettlemoyer (2023) Dettmers, T. L. Zettlemoyer. 4비트 정밀도의 경우: k비트 추론 스케일링 법칙. Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 7750-7774. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/dettmers23a.html](https://proceedings.mlr.press/v202/dettmers23a.html).\n' +
      '* Du et al. (2022) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, E., Webster, K., Pellat, M., Robinson, K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen, Z., and Cui, C. GLaM: Efficient scaling of language models with mixture-of-experts. Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL[https://proceedings.mlr.press/v162/du22c.html](https://proceedings.mlr.press/v162/du22c.html).\n' +
      '* Du et al. (2020)Eigen, D., Ranzato, M., and Sutskever, I. Learning factored representations in the deep mixture of experts. 영벵지오 및 LeCun, Y. (eds.), _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings_, 2014. URL[http://arxiv.org/abs/1312.4314](http://arxiv.org/abs/1312.4314).\n' +
      '* 18, 2022_, pp. 2893-2902. ACM, 2022. doi: 10.1145/3534678.3539173. URL[https://doi.org/10.1145/3534678.3539173](https://doi.org/10.1145/3534678.3539173).\n' +
      '* Gao et al. (2022) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. pile: An 800gb dataset of various text for language modeling. _ CoRR_, abs/2101.00027, 2021.\n' +
      '* Grangier & Iter (2022) Grangier, D. and Iter, D. The trade-offs of domain adaptation for neural language models. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pp. 3802-3813. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.264. URL[https://doi.org/10.18653/v1/2022.acl-long.264](https://doi.org/10.18653/v1/2022.acl-long.264](https://doi.org/10.18653/v1/2022.acl-long.264)\n' +
      '* Gross et al. (2017) Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of experts for large scale weakly supervised vision. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pp. 5085-5093. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.540.\n' +
      '* Gururangan et al. (2020) Gururangan, S., Marasovic, A., Sawayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don\'t stop pre-training: 언어 모델을 도메인 및 태스크에 적응시킨다. Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL[https://aclanthology.org/2020.acl-main.740](https://aclanthology.org/2020.acl-main.740).\n' +
      '*Ha et al.(2017) Ha, D., Dai, A. M., and Le, Q. V. Hypernetworks. _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.\n' +
      '* Hinton et al. (2015) Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in neural network. _ CoRR_, abs/1503.02531, 2015. URL[http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531).\n' +
      '* December 9, 2022_, 2022. URL[http://papers.nips.cc/paper_files/paper/2022/hash/cle2faff6f58870935f114ebe04a3e5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/cle2faff6f58870935f114ebe04a3e5-Abstract-Conference.html)\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgui, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. NLP를 위한 파라미터 효율적인 전이 학습. 차우두리 및 Salakhutdinov, R. (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 2790-2799. PMLR, 09-15 Jun 2019. URL[https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).\n' +
      '* Howard & Ruder (2018) Howard, J. and Ruder, S. 텍스트 분류를 위한 범용 언어 모델 미세 조정. 구레비치, I.와 미야오, Y. (eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 328-339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL[https://aclanthology.org/P18-1031](https://aclanthology.org/P18-1031).\n' +
      '* Hsieh et al. (2018) Hsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C., and Pfister, T. 차근차근 증류합니다! 학습 데이터가 적고 모델 크기가 작을수록 더 큰 언어 모델을 능가합니다. Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, July 9-14, 2023_, pp. 8003-8017. Association for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.FINDINGS-ACL. 507. URL[https://doi.org/10.18653/v1/2023.findings-acl.507](https://doi.org/10.18653/v1/2023.findings-acl.507).\n' +
      '* Hsieh et al. (2023) Hsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C., and Pfister, T. 차근차근 증류합니다! 학습 데이터가 적고 모델 크기가 작을수록 더 큰 언어 모델을 능가합니다. Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, July 9-14, 2023_, pp. 8003-8017. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.FINDINGS-ACL. 507. URL[https://doi.org/10.18653/v1/2023.findings-acl.507](https://doi.org/10.18653/v1/2023.findings-acl.507).\n' +
      '* Hu et al. (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. _International Conference on Learning Representations_, 2021.\n' +
      '* Kahn & Harris (1951) Kahn, H. and Harris, T. E., Estimation of particle transmission by random sampling. _ 국가 표준국은 1951년 수학 시리즈_, 12:27-30을 적용했다.\n' +
      '* Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models., 2020.\n' +
      '* Karimi Mahabadi et al. (2021) Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Henderson, J. Parameter-efficient multi-task fine-tuning for transformer via shared hypernetworks. 2021년 계산 언어학 협회 연례 회의에서.\n' +
      '* Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. 매개 변수 효율적인 프롬프트 조정을 위한 축척의 검정력입니다. 모엔스 - F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL[https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243)\n' +
      '* Ma et al. (2023) Ma, X., Fang, G., and Wang, X. Llm-pruner: 2023년 대형 언어 모델의 구조적 가지치기에 관한 것이다.\n' +
      '* Moore & Lewis (2010) Moore, R. C. and Lewis, W. 언어 모델 훈련 데이터의 지능적 선택. Hajic, J., Carberry, S., Clark, S., and Nivre, J. (eds.), _Proceedings of the ACL 2010 Conference Short Papers_, pp. 220-224, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL[https://aclanthology.org/P10-2041](https://aclanthology.org/P10-2041).\n' +
      '* Muqeeth et al. (2023) Muqeeth, M., Liu, H., and Raffel, C. Soft merging of experts with adaptive routing, 2023.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limit of transfer learning with unified text-to-text transformer. _ Journal of Machine Learning Research_, 21(140):1-67, 2020.\n' +
      '* Reimers & Gurevych (2019) Reimers, N. 및 Gurevych, I. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.\n' +
      '* Rosenfeld(2000) Rosenfeld, R. 20년 동안의 통계적 언어 모델링: 여기서 우리는 어디로 갑니까? _ 프로크 IEEE_, 88(8):1270-1278, 2000. doi: 10.1109/5.880083. URL[https://doi.org/10.1109/5.880083](https://doi.org/10.1109/5.880083).\n' +
      '* Shazeer et al. (2017) Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outragely large neural networks: The sparsely-gated mixture-of-experts layer. _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL[https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg).\n' +
      '* Sheng et al. (2023) Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Re, C., Stoica, I., and Zhang, C. FlexGen: High-throughput generatingative inference with a single GPU. Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 31094-31116. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html).\n' +
      '* Touvron et al. (2020) Touvron, H., Martin, L., Stone, K., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Karkez, V., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M. - A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, E. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Zu, P., Yan, Z., Zarov, I., Zambadur, M., Fan, A., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.\n' +
      '* Vapnik(1995) Vapnik, V. N. _The nature of statistical learning theory_. Springer-Verlag New York, Inc., 1995. ISBN 0-387-94559-8\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention만 있으면 된다. Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.\n' +
      '* Wang et al. (2018) Wang, W., Watanabe, T., Hughes, M., Nakagawa, T., and Chelba, C. Denoising neural machine translation training with trusted data and online data selection. Bojar, O., Chatterjee, R., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Monz, C., Negri, M., Neveol, A., Neves, M., Post, M., Specia, L., Turchi, M., and Verspoor, K. (eds.), _Proceedings of the Third Conference on Machine Translation: Research Papers_, pp. 133-143, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6314. URL[https://aclanthology.org/W18-6314](https://aclanthology.org/W18-6314).\n' +
      '* Xia et al. (2023) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning, 2023.\n' +
      '* Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: 대형 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화. Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 38087-38099. PMLR, 23-29 Jul 2023. URL[https://proceedings.mlr.press/v202/xiao23c.html](https://proceedings.mlr.press/v202/xiao23c.html).\n' +
      '* Xie et al. (2023) Xie, S. M., Santurkar, S., Ma, T., and Liang, P. Data selection for language models via importance resampling. _ CoRR_, abs/2302.03169, 2023. doi: 10.48550/ARXIV.2302.03169. URL[https://doi.org/10.48550/arXiv.2302.03169](https://doi.org/10.48550/arXiv.2302.03169).\n' +
      '* Zhu et al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. 대형 언어 모델에 대한 모델 압축에 대한 조사, 2023년.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r|r} \\hline \\hline Domain & \\multicolumn{4}{c}{Num. clusters} \\\\  & 16 & 64 & 256 & 1024 \\\\ \\hline arxiv & 0.41 & 1.02 & 1.80 & 2.58 \\\\ euporal & 1.48 & 1.83 & 2.31 & 3.14 \\\\ freelaw & 1.01 & 0.70 & 1.44 & 2.49 \\\\ gutenberg & 1.57 & 2.42 & 3.21 & 3.85 \\\\ opensubtitles & 1.16 & 2.61 & 2.95 & 3.44 \\\\ openwebtext2 & 2.19 & 3.60 & 4.89 & 6.12 \\\\ pubmed abs. & 1.07 & 2.14 & 3.22 & 4.43 \\\\ stackexchange & 0.39 & 0.97 & 1.78 & 3.24 \\\\ wikipedia & 1.73 & 3.20 & 4.54 & 5.64 \\\\ \\hline c4 & 2.73 & 4.07 & 5.46 & 6.85 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 도메인별 클러스터 히스토그램의 엔트로피.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r|r|r r r} \\hline \\hline Model & Pretrain & Num. & Num. & Generic & & \\multicolumn{3}{c}{Specific PPL} \\\\  & cost & steps & GPU & PPL & No ft & 1M & 8M & 64M \\\\ \\hline SLM & 100 & 798k & 8 & 20.51 & 33.74 & 19.31 & 15.61 & 12.37 \\\\ SLM-mix & 100 & 464k & 16 & 17.13 & 34.35 & 19.82 & 15.82 & 12.62 \\\\ SLM-hn & 100 & 195k & 8 & 18.90 & 33.44 & 18.57 & 15.58 & 12.53 \\\\ LLM & 100 & 108k & 8 & 17.00 & 29.22 & 17.11 & 15.49 & 11.55 \\\\ \\hline SLM & 200 & 1597k & 8 & 19.71 & 34.43 & 18.58 & 15.12 & 12.09 \\\\ SLM-mix & 200 & 928k & 16 & 15.92 & 31.94 & 18.48 & 14.98 & 12.15 \\\\ SLM-hn & 200 & 390k & 8 & 17.74 & 32.30 & 17.76 & 14.95 & 12.13 \\\\ LLM & 200 & 217k & 8 & 15.58 & 28.18 & 15.62 & 14.03 & 10.81 \\\\ \\hline SLM & 400 & 3195k & 8 & 19.17 & 36.61 & 18.22 & 14.80 & 12.00 \\\\ SLM-mix & 400 & 1000k & 16 & 15.82 & 31.04 & 17.56 & 14.42 & 11.84 \\\\ SLM-hn & 400 & 780k & 8 & 16.90 & 32.54 & 17.17 & 14.48 & 11.86 \\\\ LLM & 400 & 434k & 8 & 14.54 & 28.98 & 15.03 & 13.05 & 10.28 \\\\ \\hline SLM-mix & 600 & 1000k & 16 & 15.82 & 31.03 & 17.18 & 14.21 & 11.73 \\\\ SLM-hn & 600 & 1170k & 8 & 16.53 & 32.53 & 16.95 & 14.29 & 11.74 \\\\ LLM & 600 & 651k & 8 & 14.09 & 28.62 & 14.50 & 12.64 & 10.07 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 고정 프리트레이닝 비용(GPUh)에서의 보간된 당혹감\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r|r|r} \\hline \\hline Domain & \\multicolumn{4}{c}{Num. clusters} \\\\  & 4 & 16 & 64 & 256 & 1024 \\\\ \\hline arxiv & 0.95 & 0.92 & 0.55 & 0.52 & 0.29 \\\\ euporal & 0.52 & 0.53 & 0.45 & 0.44 & 0.27 \\\\ freelaw & 0.48 & 0.73 & 0.87 & 0.72 & 0.35 \\\\ gutenberg & 0.75 & 0.54 & 0.35 & 0.27 & 0.29 \\\\ opensubtitles & 0.97 & 0.68 & 0.26 & 0.28 & 0.32 \\\\ openwebtext2 & 0.53 & 0.35 & 0.12 & 0.04 & 0.02 \\\\ pubmed abs. & 0.94 & 0.54 & 0.41 & 0.20 & 0.06 \\\\ stackexchange & 0.95 & 0.94 & 0.78 & 0.61 & 0.31 \\\\ wikipedia & 0.71 & 0.58 & 0.21 & 0.07 & 0.03 \\\\ \\hline c4 & 0.32 & 0.12 & 0.04 & 0.02 & 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 도메인당 가장 빈번한 클러스터에 있는 데이터의 분수.\n' +
      '\n' +
      '## 부록 F 파라미터 효율적인 미세조정\n' +
      '\n' +
      '또한 LLM에 대한 미세 조정 방법으로 Low Rank Adaptation (LoRA)(Hu et al., 2021)을 평가한다. LoRA는 전문화가 거의 없을 때 미세 조정 프로세스를 정규화하는 데 도움이 될 수 있다. 또한 각 도메인에 대해 소수의 파라미터만 학습되기 때문에 많은 도메인을 다룰 때 많은 전문 모델을 관리하는 스토리지 및 통신 비용을 줄인다. LoRA는 프리트레이닝 비용을 감소시키지 않으며, 단계당 유사한 비용으로 더 많은 미세튜닝 단계를 요구하므로 미세튜닝 비용까지 증가시킨다. LoRA 실험에서 LLM보다 최대 5\\(\\times\\) 더 많은 단계에 대해 5M 훈련 가능한 매개변수를 생성하고 미세 조정을 하는 순위 64의 낮은 순위 행렬을 사용한다. 우리는 LLM-lora가 1M 토큰 데이터 세트의 경우 LLM보다 25% 더 많은 단계를 필요로 하고 64M 토큰 데이터 세트의 경우 3\\(\\times\\) 더 많은 단계를 필요로 한다는 것을 관찰했다. 그러나 전문화 비용은 사전 훈련 비용에 비해 무시할 수 있기 때문에 이러한 추가 단계는 전체 훈련 비용에 실제로 영향을 미치지 않는다. 그림 13은 결과를 보고한다. LoRA는 LLM(0.5 perplexity 미만의 차분)과 매우 유사하게 수행하고 64M 토큰의 "대형" 도메인 특정 레짐을 제외하고 일부 과적합 완화를 관찰할 수 있다. 마지막으로, LoRA는 여전히 추론을 위한 계산 예산이 작은 경우에 적합하지 않은 큰 모델을 초래한다.\n' +
      '\n' +
      '그림 10: 사전 훈련 비용의 함수로서 미세 조정 비용.\n' +
      '\n' +
      '그림 11: 1M 토큰에 대한 미세 조정 후 개별 하위 집합에 대한 특정 당혹감.\n' +
      '\n' +
      '그림 12: 1M 토큰에 대한 미세 조정 후 개별 하위 집합에 대한 특정 당혹감.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '그림 13: 사전 훈련 비용과 관련하여 파일 하위 집합에 대한 LoRA 미세 조정의 특정 복잡성. 우리는 LoRA 미세조정이 0.5 미만의 복잡도 차이로 전통적인 미세조정과 매우 유사하게 수행됨을 관찰한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>