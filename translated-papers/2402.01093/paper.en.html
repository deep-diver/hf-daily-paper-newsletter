<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Specialized Language Models with Cheap Inference from Limited Domain Data\n' +
      '\n' +
      'David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun\n' +
      '\n' +
      'Apple Inc.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Training large language models enables versatile models, but their high inference cost limits them to high-value applications (Brown et al., 2020; Bommasani et al., 2022). Despite progress in approximated inference (Aminabadi et al., 2022; Sheng et al., 2023; Dettmers & Zettlemoyer, 2023), large models remain costly, or even impractical for mobile hardware. Under tight inference constraints, one might consider a small model specialized to the domain at hand. This paper studies training small specialized models, even with limited domain data. To achieve low perplexity, we use three key elements: generic training corpora, importance sampling, and asymmetric models with fewer parameters at inference than during training, such as mixtures of experts or hyper-networks.\n' +
      '\n' +
      'With inference cost and in-domain training data limits, we study alternative strategies with varying training cost. We also take into account how training cost can be shared across domains. For our study, we consider 4 important metrics:\n' +
      '\n' +
      '**Generic training cost**: the cost of the training phase\n' +
      '\n' +
      'Figure 1: Practical recommendations for training language models that fit a predefined computational budget. that can be performed before the specialization data are available, on a generic training set. This cost is shared across multiple specialized models and is often called pretraining. Although not mandatory, generic training data are essential when specialization data are limited.\n' +
      '\n' +
      '**Specialization training cost**: the cost of the training performed once the specialization data are available. This cost is not shared across different specialized models.\n' +
      '\n' +
      '**Inference cost**: the cost for running inference on a specialized model. As part of the inference cost, one might also be interested in a model which involves a small number of parameters per specialized task, e.g. considering memory and network constraints. Low inference cost allows wider model deployment.\n' +
      '\n' +
      '**Size of the specialization training set**: varies across applications and influences pretraining and specialization choices.\n' +
      '\n' +
      'We take the inference cost and the specialization data size as hard constraints and study the operating curves resulting from varying the generic and specialization training costs. We compare different training approaches and highlight at which operating point they are interesting.\n' +
      '\n' +
      '## 2 Methods\n' +
      '\n' +
      'We consider different architectures to satisfy our inference constraint while leveraging a large generic pretraining set.\n' +
      '\n' +
      'We assess the drop in perplexity from our inference constraint compared to a larger model trained on the same data. Our recommendations are summarized in Figure 1.\n' +
      '\n' +
      '### Large Model\n' +
      '\n' +
      '#### Large Model (LLM)\n' +
      '\n' +
      'One trains a large language model (LLM) on the generic data and uses this model as-is at inference (Brown et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022). This approach requires a high pretraining cost but does not require specialization data and has no specialization cost. Inference cost for this method is high. Having never seen specialized data, it might be inaccurate when the specialization distribution is far from the pretraining distribution.\n' +
      '\n' +
      'After generic pretraining, fine-tuning over specialization data can adapt the model (Howard & Ruder, 2018; Aghajanyan et al., 2021). This step usually improves perplexity but adds a specialization cost. This cost is limited when the amount of specialization data is small: early stopping limits fine-tuning to a few updates to avoid overfitting. Fine-tuning leaves the inference cost unchanged.\n' +
      '\n' +
      '#### Parameter Efficient Tuning\n' +
      '\n' +
      'One fine-tunes only a subset of the parameters once the specialization data are available (Hu et al., 2021; Lester et al., 2021; Houlsby et al., 2019). This strategy is advantageous if the specialization data are scarce because it mitigates overfitting. However, tuning fewer parameters may require more fine-tuning steps and hence increase the specialization cost. This method is practical when one needs to communicate only small model deltas for each new specialization. See Section F in the Appendix.\n' +
      '\n' +
      '### Small Model\n' +
      '\n' +
      '#### Small Model (SLM)\n' +
      '\n' +
      'One trains a single small language model (SLM) before the specialization data are available and uses this model as-is at inference. This method does not require specialization data or incur any specialization cost. The inference cost for this method is low and so is the pretraining cost. However, a small model cannot use a large amount of generic data as well as a large model resulting in worse downstream performance. Similarly to larger models, fine-tuning can improve performance with an additional specialization cost.\n' +
      '\n' +
      '#### No Pretraining (SLM-nopt)\n' +
      '\n' +
      'This method only trains the model on the specialization data. This is advantageous when the specialization budget and amount of specialized data are large or when the generic training distribution is very far from the specialization domain.\n' +
      '\n' +
      '#### Importance Sampling (SLM-is)\n' +
      '\n' +
      'This method does not pretrain the model before the specialization data are available. Once the specialization set is given, SLM-is samples a tailored training set from the generic pretraining data to match the specialization distribution (Xie et al., 2023). This method is a case of data selection (Moore & Lewis, 2010; Grangier & Iter, 2022) and is advantageous when the specialization data are scarce. This method incurs a high specialization cost since pretraining on a (possibly large) tailored training set is necessary for each specialization domain. After pretraining, the model can be further fine-tuned over the specialization data.\n' +
      '\n' +
      '#### Distillation (SLM-d)\n' +
      '\n' +
      'This method (Hinton et al., 2015; Hsieh et al., 2023; Zhu et al., 2023) uses a fine-tuned large model as a teacher and distills it in a small student model. Compared to the large teacher model, inference cost is lower, but accuracy is also reduced. Compared to a small model without distillation, the accuracy can be better. The teacher model provides rich targets from a model with better generalization, often reducing overfitting to the specialization set. Compared to SLM fine-tuning, this method has a higher generic pretraining cost: it requires generic training of a large model and a small model. Its specialization cost is also greater since it requires tuning the teacher model and then collecting the teacher outputs during distillation.\n' +
      '\n' +
      '### Hard Mixture of Experts (SLM-mix)\n' +
      '\n' +
      'This method (Eigen et al., 2014; Gross et al., 2017) divides the large pretraining set in smaller subsets, e.g. via clustering, and pretrains a small model (expert) on each part. Its pretraining cost and its overall number of parameters are high as both scale linearly with the number of clusters. Inference with a hard mixture is typically performed by forwarding each example to the expert corresponding to the cluster the example belongs to.\n' +
      '\n' +
      'Once the specialization data are available, we specialize the mixture by selecting a single expert for each specialization task. One option is to select the expert whose pretraining cluster is the most frequent cluster in the specialization data. Alternatively, if the specialization budget is sufficient, we can select the expert with the smallest loss on average on the specialization data. Selecting a single expert per domain is advantageous as it communicates and loads only a small part of the mixture weights for inference on the target domain, but it might be detrimental when the specialization data are spread across multiple clusters.\n' +
      '\n' +
      'SLM-mix can be fine-tuned. With a large specialization budget, one can fine-tune each expert and select the best-performing one. With a smaller budget, one can instead fine-tune just the best-performing expert at pretraining or the expert whose pretraining cluster is the most frequent cluster in the specialization data. This second option makesthe cost of specialization identical to SLM fine-tuning.\n' +
      '\n' +
      '### Hyper-Networks (SLM-hn)\n' +
      '\n' +
      'Hyper-networks (Ha et al., 2017) are neural networks that decompose into two parts: the hyper-sub-network and the instantiated sub-network. The hyper-sub-network creates weights for the instantiated sub-network. We rely on hyper-networks to create a mixture of experts: the hyper-sub-network takes the cluster membership of an input to produce the sub-network weights. These cluster-specific weights instantiate a small sub-network or expert. Compared to a hard mixture of experts, SLM-hn shares parameters across experts via the hyper-sub-network and provides a flexible way to independently select the capacity of the mixture and the number of clusters. An instantiated sub-network can be fine-tuned on specialization data.\n' +
      '\n' +
      '## 3 Experimental Setup\n' +
      '\n' +
      'We present the datasets for our experiments, the experimental setting for each method and the evaluation metrics.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'Our generic pretraining set is c4, a large filtered dataset of English text derived from commoncrawl (Raffel et al., 2020). We tokenize the data with a sentence piece model trained on c4 with a vocabulary size of 32k. We consider specializing to nine diverse domains, extracted from the Pile (Gao et al., 2021): arxiv (science articles), europarl (parliamentary proceedings), freelaw (legal text), gutenberg (old books pusblished before 1919), opensubtitles (theatrical subtitles), openwebtext2 (forum discussions), pubmed-abstracts (medical article abstracts), stackexchange (Q&A mostly about technical topics), wikipedia (encyclopedia articles). We vary the amount of specialization training data available and consider sets of size 1, 8 and 64 million tokens for each domain.\n' +
      '\n' +
      '### Clustering\n' +
      '\n' +
      'Hard mixture-of-experts, hyper-networks and importance sampling rely on document clustering. We use sentence BERT (Reimers and Gurevych, 2019) to embed the c4 documents into 768-dimensional vectors and cluster them with the kmeans algorithm. We explore different numbers of clusters ranging for 4 to 1,024 clusters.\n' +
      '\n' +
      '### Language Models\n' +
      '\n' +
      'We perform our experiments with transformer models (Vaswani et al., 2017). We consider two model sizes, small and large. The small model has 126M parameters and consists of 7 layers with a dimension of 1,024 and a latent feed-forward dimension of 4,096. Our large model has 770M parameters with 7 layers with a dimension of 2,816 and a latent feed-forward dimension of 11,264. Models are trained and evaluated with a context of at most 1,024 tokens, splitting longer documents into non-overlapping windows.\n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      'For distillation, we use a fine-tuned LLM as the teacher and an SLM pretrained on the generic set as the student. Distillation training operates on the specialization data and trains the student to minimize the KL divergence between its prediction and the teaching distribution, a mixture between the data distribution and the teacher model prediction (Hinton et al., 2015). The teaching mixture weight is a hyperparameter (0.95 in our experiments). In this method, the generic training cost is dominated by the training of the teacher model while the specialization cost is also dominated by the cost of fine-tuning the teacher model. This method has an additional smaller cost of pretraining the SLM on the generic dataset and teaching the SLM on the specialization dataset.\n' +
      '\n' +
      '### Mixture of Experts\n' +
      '\n' +
      'We train hard mixtures (Gross et al., 2017) of transformers for pretraining. The pretraining set is divided into clusters and an independent SLM is trained on each cluster. For specialization, we consider a simple strategy: we cluster the specialization set with the pretraining centroids to determine the most frequent cluster in the specialization set. We fine-tune only the model pretrained on this cluster. Hard mixtures are interesting here since they allow training a model with a large total number of parameters while fine-tuning and running inference only with a small model.\n' +
      '\n' +
      'If the pretraining budget is low, one can forgo pretraining models on all clusters and, instead, increase the specialization budget to train a model only on the cluster of the generic dataset corresponding to the most frequent cluster in the specialization set, once this set is available.\n' +
      '\n' +
      '### Hyper-Networks\n' +
      '\n' +
      'Hyper-network (Ha et al., 2017; Karimi Mahabadi et al., 2021) defines the general idea of a neural network whose weights are themselves generated from a secondary network, the _hyper-network_, based on a conditioning input variable.\n' +
      '\n' +
      'In our case we associate each example with its cluster membership variable, using the clustering mentioned in Section 3.2. This variable is the input of the hyper-network which produces the feed-forward (i.e. multi-layer perceptron, MLP) matrices of a transformer language model for all layers except the first two. The other parameters of the transformer do not depend on the cluster and are the same for all examples.\n' +
      '\n' +
      'Our hyper-network instantiates two MLP matrices \\(W^{(1,I,i)},W^{(2,I,i)}\\) for each layer \\(l\\) and each cluster \\(i\\). It relies on two hyper-parameters: the latent dimension \\(h\\) and the number of experts \\(m\\). Each cluster \\(i\\) is associated with the h-dimensional embedding \\(c^{(i)}\\). Each layer 1 is associated with the \\(h\\times m\\)-matrix \\(M^{(l)}\\). We compute the matrices\n' +
      '\n' +
      '\\[W^{(1,I,i)}=c^{(i)}\\ M^{(l)}\\cdot T^{(1,I)}\\ \\text{and}\\ W^{(2,I,i)}=c^{(i)}\\ M^{(l)} \\cdot T^{(2,I)}\\]\n' +
      '\n' +
      'as the weighted sum between the vector \\(c^{(i)}\\ M^{(l)}\\) and the three dimensional tensors \\(T^{(1,I)},T^{(2,I)}\\) respectively of shape \\(m\\times d_{\\text{latent}}\\times d_{\\text{in}}\\) and \\(m\\times d_{\\text{in}}\\times d_{\\text{latent}}\\). These tensors hold most of the model parameters, i.e. \\(m\\) times as many parameters as the corresponding MLP matrices. This strategy enables increasing \\(m\\) to increase the overall model capacity while keeping the size of the model instantiated for each cluster constant. Of course, we illustrate one choice of hyper-network architecture, but many alternatives are possible (Muqeeth et al., 2023; Abnar et al., 2023). Compared to hard mixtures of experts, the hyper-networks have stronger capacity limitations since the training problem cannot be split into independent, low-memory training tasks. On the other hand, the weights of each of the \\(m\\) experts -- both the attention parameters, which are the same for all experts, and the MLP tensors -- are trained jointly, hence the hyper-network model can be more parameter efficient.\n' +
      '\n' +
      'For specialization, we follow a strategy similar to the hard mixture case: we instantiate the model at the most frequent cluster on the specialization set and fine-tune it. Fine-tuning therefore does not operate on the large hyper-network but only on the small instantiated model.\n' +
      '\n' +
      '### Importance Sampling\n' +
      '\n' +
      'Our importance sampling method relies on the k-means clustering from Section 3.2. It is a streaming method that requires only the histogram of cluster frequencies in the targeted distribution \\(h^{t}\\). It relies on a large buffer of pretraining documents, e.g. \\(N\\simeq 100\\)k. We compute the cluster histogram \\(h^{b}\\) in the buffer and take \\(N_{i}\\) documents in each cluster \\(i\\). \\(N_{i}=N\\times h^{t}_{i}\\times\\min_{j}(h^{b}_{j}/h^{t}_{j})\\). is the maximum number of documents we can take for each cluster while enforcing that the histogram of the \\(N_{i}\\) matches the target histogram \\(h^{t}\\). The selected data are then used for training.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'We rely on perplexity, the standard language modeling metric, for our evaluation. We measure perplexity on held out data using 20k documents per dataset. We focus solely on language modeling and evaluating the models on downstream tasks (e.g. question answering, sentiment analysis, translation, etc) is beyond the scope of the paper.\n' +
      '\n' +
      'We measure training cost (pretraining and specialization) in hours of graphic processor compute time (GPUh) on the same hardware (Nvidia-A100). We consider pretraining costs ranging from 10 to 650 GPUh and specialization cost ranging from 0.3 to 120 GPUh.\n' +
      '\n' +
      '## 4 Empirical Results\n' +
      '\n' +
      'We first report our main results before diving into a detailed discussion for each method.\n' +
      '\n' +
      'Table 1 reports the number of parameters for the pre-trained and specialized models. Table 1 illustrates that SLM-hn and SLM-mix are as small as SLM for inference after specialization while their overall number of pretrained parameters is larger than LLM. Table 2 reports the throughput of the models. All SLM models have the same specialization throughput while SLM-hn has a lower throughput than SLM, SLM-mix for pretraining. LLM is more expensive in all cases. Table 3 presents the upper limit in training budgets for pretraining and specialization over all settings.\n' +
      '\n' +
      'We consider varying pretraining budget and report perplexity on the generic pretraining set (c4) for each method in Figure 2. When we consider SLM-hn and SLM-mix, we observe that even if the number of pretrained parameters is larger than LLM, they do not enjoy as good perplexity. However, their perplexity is better than SLM while they are as efficient when tested or fine-tuned on a single cluster.\n' +
      '\n' +
      'Perplexity on c4 is not our main goal and we report the perplexity on the specialization domains from Pile, see Section 3.1. We report held-out perplexity by _macro-averaging_ on the nine sets. We compute the mean negative\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r} \\hline \\hline Model & \\multicolumn{2}{c|}{Num. parameters (M)} \\\\  & Generic Pretrain & Inference \\\\ \\hline Small LM (SLM) & 126 & 126 \\\\ Mixt. of experts (SLM-mix) & 2,016 & 126 \\\\ Hyper network (SLM-hn) & 1,422 & 126 \\\\ Large LM (LLM) & 771 & 771 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Number of parameters (in millions) for pretraining and inference.\n' +
      '\n' +
      'log likelihood per token for each set, average the nine numbers and compute their exponential. All domains therefore get the same weight, regardless of the size of the held-out set.\n' +
      '\n' +
      'Figure 3 (a) reports the results before fine-tuning. The reported perplexities are much higher than the c4 perplexities, and indicate that specialization is necessary. Figure 3 (b) reports the results after fine-tuning several pretrained checkpoints for each method on the 1M token dataset of each domain. Each domain-specific model is evaluated before macro-averaging. Since 1M tokens is a small set, fine-tuning relies on a small learning rate and early stopping (base learning rate divided by 3, always stopping after less than 2k fine-tuning steps on one GPU). Fine-tuning is highly beneficial for all methods and results in significantly improved perplexity. We also remark that pre-fine-tuning perplexity on the Pile is not necessarily a good indicator of post-fine-tuning perplexity: e.g. the SLM checkpoints ordering is very different on the two curves, the ordering between SLM-mix and SLM-hn also changes during fine-tuning.\n' +
      '\n' +
      'We also consider fine-tuning on 8 and 64 million tokens for each domain, see Figure 3 (c) and (d). More data allows us to train slightly longer and keep the base learning rate without overfitting. We stop at most after 4k steps and 30k steps for the 8M and 64M cases respectively. We observe that the benefit of a good starting point provided by SLM-hn and SLM-mix (compared to SLM) erodes as the domain training set size increases.\n' +
      '\n' +
      'These figures report the perplexity of SLM-is as a constant line. This method has no pretraining as we can only start training once the domain data are available; bearing all the training cost in the specialization phase. SML-is is the best method with a small inference model in terms of post-specialization perplexity. Interestingly, it even outperforms the much larger model when specific in-domain data are scarce (ie the 1M tokens case).\n' +
      '\n' +
      '### Small and Large Models\n' +
      '\n' +
      'Table 4 compares the perplexity on the Pile subsets for the baseline transformer models. Pretraining and fine-tuning are both necessary to achieve good perplexity on our specialization sets. Without pretraining (SLM-nopt), a lot of specialization data (64M tokens per domain) are required in order to get acceptable performance. We also observe that for both large and small models there is a large gap in perplexity before and after finetuning; making it clear that finetuning even on 1M in-domain tokens can result in significant boost in performance. Finally, as expected, the LLM results also illustrate that, for large inference and pretraining budgets, it is beneficial to train large models on the pretraining set (c4).\n' +
      '\n' +
      '### Distillation\n' +
      '\n' +
      'Our distillation process takes a pretrained teacher (LLM) and a pretrained student (SLM). We fine-tune the teacher on the specialization set and we use the fine-tuned teacher to supervise the student on the same set. In this process, the generic pretraining cost sums two terms: teacher and student pretraining. It is a question how to best spread the cost between these two terms.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r} \\hline \\hline Model & \\multicolumn{2}{c|}{Training} & Inference \\\\  & Generic Pre. & Specialization & \\\\ \\hline SLM & 2.2 & 2.2 & 0.61 \\\\ SLM-mix & 2.2 & 2.2 & 0.61 \\\\ SLM-hn & 3.6 & 2.2 & 0.61 \\\\ SLM-is & N/A & 2.2 & 0.61 \\\\ LLM & 7.7 & 7.7 & 2.54 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Model throughput (GPU hours per 1B training tokens).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r r r} \\hline \\hline Model & Pretraining & \\multicolumn{3}{c}{Specialization} \\\\  & & 1M & 8M & 64M \\\\ \\hline LLM & \\(\\leq\\) 650 & \\(\\leq\\) 0.12 & \\(\\leq\\) 0.5 & \\(\\leq\\) 3.5 \\\\ SLM & \\(\\leq\\) 530 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ SLM-is & 0 & \\(\\leq\\) 130 & \\(\\leq\\) 130 & \\(\\leq\\) 130 \\\\ SLM-d & \\(\\leq\\) 1,850 & \\(\\leq\\) 0.7 & \\(\\leq\\) 2.8 & \\(\\leq\\) 21 \\\\ SLM-mix & \\(\\leq\\) 650 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ SLM-hn & \\(\\leq\\) 650 & \\(\\leq\\) 0.02 & \\(\\leq\\)0.07 & \\(\\leq\\) 0.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Train cost limits for pretraining and specialization (GPUh)\n' +
      '\n' +
      'Figure 2: Generic pretraining perplexity on c4.\n' +
      '\n' +
      'Figure 4 (left) reports SLM-d perplexities with each curve corresponding to a different amount of teacher pretraining and has the student pretraining as the x-axis. It shows that for settings over 276 GPUh of teacher pretraining (300k steps), the student model SLM-d is significantly better than vanilla SLM at the same level of student pretraining. This plot demonstrates the benefit of a good teacher over an SLM trained only over the specialization set targets.\n' +
      '\n' +
      'Figure 4 (right) shows the same data changing the x-axis to report the overall generic pretraining cost, summing the teacher and student pretraining cost. When the teacher pretraining cost is accounted for, SLM-d is not competitive with the best methods like SLM-hn and SLM-mix.\n' +
      '\n' +
      'Figure 4: Distillation results (dashed lines) on the 1M token specialization set for various teacher pretraining budgets. On the left we show perplexity with respect to the student pretraining cost only and on the right with respect to the overall pretraining cost.\n' +
      '\n' +
      'Figure 3: Specific perplexity on the Pile subsets (average) before and after fine-tuning with different amounts of specialization data.\n' +
      '\n' +
      '### Mixture of Experts\n' +
      '\n' +
      'Our hard mixture of experts relies on the generic dataset split in clusters, see Section 3.2, and its number of experts corresponds to the number of clusters. For fine-tuning, we fine-tune only the expert corresponding to the most frequent cluster in the targeted domain dataset. In this section, we vary the number of clusters and discuss whether selecting the most frequent cluster is a good strategy.\n' +
      '\n' +
      'The overall capacity of the mixture and its training cost is proportional to the number of clusters. Our main results (Fig. 2, Fig. 3, etc) use 16 experts. We compare results with 4 to 256 experts. Intuitively, if the number of experts is too large, the model would cost more to train and each cluster would not contain enough data to train a model of the size of SLM. Conversely, if the number of experts is too small, the training cost is low but each SLM-sized expert would be trained from a large cluster and would underfit its training set. Also, the large clusters might be too generic and far from the distribution of the targeted set. Figure 5 shows the macro-averaged perplexity on the Pile as a function of the generic pretraining time for the different mixture sizes in the case of the 1M token specialization set.\n' +
      '\n' +
      'As mentioned above, specialization fine-tunes a single expert. We select the expert corresponding to the most frequent cluster in the specialization data. Alternatively, we also consider selecting the expert which has the lowest loss on the specialization set before fine-tuning, which involves evaluating each expert. As a third costlier option, we fine-tune all experts and pick the best one a posteriori. Table 5 reports this result when fine-tuning on 1M tokens for the 64 expert model. The results of the different strategies are within 0.3 PPL of each other. The most costly option of fine-tuning all experts performs best.\n' +
      '\n' +
      'As a final observation on SLM-mix, the strategy of fine-tuning only the expert corresponding to the most frequent cluster enables the transfer of training cost from pretraining to specialization. Namely, one can wait until the targeted domain is known and then pretrain only one model on the single cluster of interest. This is interesting when one targets only a few domains. However, this strategy does not perform as well as importance sampling as shown in Figure 6.\n' +
      '\n' +
      '### Hyper-networks\n' +
      '\n' +
      'The number of experts in our hyper-networks allows tuning the overall number of parameters while keeping the size of the inference model constant. Figure 7 shows perplexity on the Pile subsets after fine-tuning on 1M tokens. More\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r r r} \\hline \\hline Model & Pretrained & \\multicolumn{4}{c}{Specialized} \\\\  & & 1M & 8M & 64M \\\\ \\hline SLM & 33.0 & 18.2 & 14.8 & 12.0 \\\\ SLM-nopt & N/A & 227.1 & 45.6 & 17.6 \\\\ LLM & 28.1 & 14.4 & 12.5 & 10.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Perplexity on the Pile (average) for small and large LMs (for a limit of 650 GPUh of generic pretraining)\n' +
      '\n' +
      'Figure 5: Specific perplexity of mixture models with 4-256 experts on Pile subsets (average) after fine-tuning on 1M tokens.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r} Method & Perplexity & Specialization cost \\\\ \\hline Most frequent cluster & 17.32 & 1x \\\\ Best pre-trained & 17.05 & 1x \\\\ Best fine-tuned & 16.98 & 64x \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Selecting the best expert. Average specific perplexity for 1M tokens, fine-tuning different experts from the same mixture of 64 experts after 700k pretraining steps (\\(\\sim\\) 600 GPUh).\n' +
      '\n' +
      'Figure 6: Specific perplexity after fine-tuning on 1M tokens, when one is only training SLM-mix on the most frequent domain cluster.\n' +
      '\n' +
      'experts always perform better per iteration, however, 32 experts is more compute-time efficient in our setup.\n' +
      '\n' +
      '### Importance Sampling\n' +
      '\n' +
      'Our importance sampling strategy resamples c4 such that its cluster histogram matches the cluster histogram from the targeted Pile subset. The number of clusters is an important parameter. A small number of clusters will change the c4 distribution only in a coarse manner and will provide a low fidelity match with the targeted set. Conversely, a large number of clusters has two drawbacks. Firstly, when the specialization set is small, cluster frequencies might be poorly estimated for a large number of clusters. Secondly, with a large number of clusters, the targeted histogram might concentrate a big fraction of the mass on a few small clusters, meaning that the resampled c4 dataset will contain many repeated points from these clusters. This can degrade performance as the effective size of the resampled c4 dataset will be smaller with these repetitions.\n' +
      '\n' +
      'Our main results report the importance sampling results with 1,024 clusters. Figure 8 reports the results with 16, 64, 256 and 1,024 clusters.\n' +
      '\n' +
      'The importance sampling method does not start training before the specialization set is given and a model is pretrained from scratch on a different resampled dataset for each specialization task. This means that importance sampling has a much larger specialization cost when compared to fine-tuning and this discrepancy only becomes more important when addressing many tasks. For a model, the total cost of specialization over \\(N\\) tasks is\n' +
      '\n' +
      '\\[C_{\\text{total}}(N)=C_{\\text{generic\\ pretrain}}+C_{\\text{specialization}} \\times N. \\tag{1}\\]\n' +
      '\n' +
      'For methods like hyper-networks, most of the cost is \\(C_{\\text{generic\\ pretrain}}\\) and the main parameter to vary the total cost is the number of generic pretraining steps. For the importance sampling method, \\(C_{\\text{generic\\ pretrain}}=0\\) and the main parameter to vary the total cost is the number of steps performed when training on the importance sampled pretraining set, which is part of \\(C_{\\text{specialization}}\\).\n' +
      '\n' +
      'We vary the total cost for SLM-hn and SLM-is when hypothetically addressing 1, 7 and 50 tasks by scaling the x-axis following Equation 1. Figure 9 shows that SLM-is becomes less interesting when the number of tasks increases. The specialization cost of fine-tuning for SLM-hn, which increases linearly with the number of tasks, can be ignored as it takes \\(\\sim 1\\)GPU minute.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'Domain adaptation for language modeling has a long history, predating neural network language models (Rosenfeld, 2000). This research stemmed from the observation that models trained on large amount of data, even far from the targeted domain were impactful on end applications (Brants\n' +
      '\n' +
      'Figure 8: Specific perplexity for importance sampling with different number of clusters after fine-tuning on 1M tokens.\n' +
      '\n' +
      'Figure 7: Specific perplexity for hyper-networks with different number of experts after fine-tuning on 1M tokens.\n' +
      '\n' +
      'Figure 9: Specific perplexity for the hyper-network vs importance sampling for the 1M token specialization datasets. Varying the number of tasks increase the cost of importance sampling linearly.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T., Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Re, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A. W., Tramer, F., Wang, R. E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and risks of foundation models, 2022.\n' +
      '* Brants et al. (2007) Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. Large language models in machine translation. In Eisner, J. (ed.), _Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)_, pp. 858-867, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL [https://aclanthology.org/D07-1090](https://aclanthology.org/D07-1090).\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).\n' +
      '* Caruana (1993) Caruana, R. Multitask learning: A knowledge-based source of inductive bias. In _Proceedings of the Tenth International Conference on Machine Learning_, pp. 41-48. Citeseer, 1993.\n' +
      '* Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsyvashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022.\n' +
      '* Clark et al. (2022) Clark, A., Casas, D. d. l., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., Driessche, G. v. d., Rutherford, E., Hennigan, T., Johnson, M., Millican, K., Cassirer, A., Jones, C., Buchatskaya, E., Budden, D., Sifre, L., Osindero, S., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K., and Simonyan, K. Unified scaling laws for routed language models. In _Proceedings of the 39th International Conference on Machine Learning_. PMLR, 2022.\n' +
      '* Collobert et al. (2011) Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. _Journal of machine learning research_, 12(ARTICLE):2493-2537, 2011.\n' +
      '* Dettmers & Zettlemoyer (2023) Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 7750-7774. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/dettmers23a.html](https://proceedings.mlr.press/v202/dettmers23a.html).\n' +
      '* Du et al. (2022) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen, Z., and Cui, C. GLaM: Efficient scaling of language models with mixture-of-experts. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/du22c.html](https://proceedings.mlr.press/v162/du22c.html).\n' +
      '* Du et al. (2020)Eigen, D., Ranzato, M., and Sutskever, I. Learning factored representations in a deep mixture of experts. In Bengio, Y. and LeCun, Y. (eds.), _2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings_, 2014. URL [http://arxiv.org/abs/1312.4314](http://arxiv.org/abs/1312.4314).\n' +
      '* 18, 2022_, pp. 2893-2902. ACM, 2022. doi: 10.1145/3534678.3539173. URL [https://doi.org/10.1145/3534678.3539173](https://doi.org/10.1145/3534678.3539173).\n' +
      '* Gao et al. (2022) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling. _CoRR_, abs/2101.00027, 2021.\n' +
      '* Grangier & Iter (2022) Grangier, D. and Iter, D. The trade-offs of domain adaptation for neural language models. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pp. 3802-3813. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.264. URL [https://doi.org/10.18653/v1/2022.acl-long.264](https://doi.org/10.18653/v1/2022.acl-long.264).\n' +
      '* Gross et al. (2017) Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of experts for large scale weakly supervised vision. In _2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pp. 5085-5093. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.540.\n' +
      '* Gururangan et al. (2020) Gururangan, S., Marasovic, A., Sawayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N. A. Don\'t stop pre-training: Adapt language models to domains and tasks. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL [https://aclanthology.org/2020.acl-main.740](https://aclanthology.org/2020.acl-main.740).\n' +
      '* Ha et al. (2017) Ha, D., Dai, A. M., and Le, Q. V. Hypernetworks. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017.\n' +
      '* Hinton et al. (2015) Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. _CoRR_, abs/1503.02531, 2015. URL [http://arxiv.org/abs/1503.02531](http://arxiv.org/abs/1503.02531).\n' +
      '* December 9, 2022_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/cle2faff6f58870935f114ebe04a3e5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/cle2faff6f58870935f114ebe04a3e5-Abstract-Conference.html).\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgui, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pp. 2790-2799. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).\n' +
      '* Howard & Ruder (2018) Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In Gurevych, I. and Miyao, Y. (eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 328-339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL [https://aclanthology.org/P18-1031](https://aclanthology.org/P18-1031).\n' +
      '* Hsieh et al. (2018) Hsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C., and Pfister, T. Distilling step-by-step! outperforming larger language modelswith less training data and smaller model sizes. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023_, pp. 8003-8017. Association for Computational Linguistics, 2023a. doi: 10.18653/V1/2023.FINDINGS-ACL. 507. URL [https://doi.org/10.18653/v1/2023.findings-acl.507](https://doi.org/10.18653/v1/2023.findings-acl.507).\n' +
      '* Hsieh et al. (2023) Hsieh, C., Li, C., Yeh, C., Nakhost, H., Fujii, Y., Ratner, A., Krishna, R., Lee, C., and Pfister, T. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023_, pp. 8003-8017. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.FINDINGS-ACL. 507. URL [https://doi.org/10.18653/v1/2023.findings-acl.507](https://doi.org/10.18653/v1/2023.findings-acl.507).\n' +
      '* Hu et al. (2021) Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021.\n' +
      '* Kahn & Harris (1951) Kahn, H. and Harris, T. E. Estimation of particle transmission by random sampling. _National Bureau of Standards applied mathematics series_, 12:27-30, 1951.\n' +
      '* Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models., 2020.\n' +
      '* Karimi Mahabadi et al. (2021) Karimi Mahabadi, R., Ruder, S., Dehghani, M., and Henderson, J. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In _Annual Meeting of the Association for Computational Linguistics_, 2021.\n' +
      '* Lester et al. (2021) Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243).\n' +
      '* Ma et al. (2023) Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models, 2023.\n' +
      '* Moore & Lewis (2010) Moore, R. C. and Lewis, W. Intelligent selection of language model training data. In Hajic, J., Carberry, S., Clark, S., and Nivre, J. (eds.), _Proceedings of the ACL 2010 Conference Short Papers_, pp. 220-224, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL [https://aclanthology.org/P10-2041](https://aclanthology.org/P10-2041).\n' +
      '* Muqeeth et al. (2023) Muqeeth, M., Liu, H., and Raffel, C. Soft merging of experts with adaptive routing, 2023.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.\n' +
      '* Reimers & Gurevych (2019) Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410.\n' +
      '* Rosenfeld (2000) Rosenfeld, R. Two decades of statistical language modeling: where do we go from here? _Proc. IEEE_, 88(8):1270-1278, 2000. doi: 10.1109/5.880083. URL [https://doi.org/10.1109/5.880083](https://doi.org/10.1109/5.880083).\n' +
      '* Shazeer et al. (2017) Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL [https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg).\n' +
      '* Sheng et al. (2023) Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Re, C., Stoica, I., and Zhang, C. FlexGen: High-throughput generative inference of large language models with a single GPU. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 31094-31116. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html).\n' +
      '* Touvron et al. (2020) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.\n' +
      '* Vapnik (1995) Vapnik, V. N. _The nature of statistical learning theory_. Springer-Verlag New York, Inc., 1995. ISBN 0-387-94559-8.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.\n' +
      '* Wang et al. (2018) Wang, W., Watanabe, T., Hughes, M., Nakagawa, T., and Chelba, C. Denoising neural machine translation training with trusted data and online data selection. In Bojar, O., Chatterjee, R., Federmann, C., Fishel, M., Graham, Y., Haddow, B., Huck, M., Yepes, A. J., Koehn, P., Monz, C., Negri, M., Neveol, A., Neves, M., Post, M., Specia, L., Turchi, M., and Verspoor, K. (eds.), _Proceedings of the Third Conference on Machine Translation: Research Papers_, pp. 133-143, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6314. URL [https://aclanthology.org/W18-6314](https://aclanthology.org/W18-6314).\n' +
      '* Xia et al. (2023) Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning, 2023.\n' +
      '* Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and efficient post-training quantization for large language models. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp. 38087-38099. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/xiao23c.html](https://proceedings.mlr.press/v202/xiao23c.html).\n' +
      '* Xie et al. (2023) Xie, S. M., Santurkar, S., Ma, T., and Liang, P. Data selection for language models via importance resampling. _CoRR_, abs/2302.03169, 2023. doi: 10.48550/ARXIV.2302.03169. URL [https://doi.org/10.48550/arXiv.2302.03169](https://doi.org/10.48550/arXiv.2302.03169).\n' +
      '* Zhu et al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on model compression for large language models, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r|r} \\hline \\hline Domain & \\multicolumn{4}{c}{Num. clusters} \\\\  & 16 & 64 & 256 & 1024 \\\\ \\hline arxiv & 0.41 & 1.02 & 1.80 & 2.58 \\\\ euporal & 1.48 & 1.83 & 2.31 & 3.14 \\\\ freelaw & 1.01 & 0.70 & 1.44 & 2.49 \\\\ gutenberg & 1.57 & 2.42 & 3.21 & 3.85 \\\\ opensubtitles & 1.16 & 2.61 & 2.95 & 3.44 \\\\ openwebtext2 & 2.19 & 3.60 & 4.89 & 6.12 \\\\ pubmed abs. & 1.07 & 2.14 & 3.22 & 4.43 \\\\ stackexchange & 0.39 & 0.97 & 1.78 & 3.24 \\\\ wikipedia & 1.73 & 3.20 & 4.54 & 5.64 \\\\ \\hline c4 & 2.73 & 4.07 & 5.46 & 6.85 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: Entropy of the cluster histogram for each domain.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r|r|r r r} \\hline \\hline Model & Pretrain & Num. & Num. & Generic & & \\multicolumn{3}{c}{Specific PPL} \\\\  & cost & steps & GPU & PPL & No ft & 1M & 8M & 64M \\\\ \\hline SLM & 100 & 798k & 8 & 20.51 & 33.74 & 19.31 & 15.61 & 12.37 \\\\ SLM-mix & 100 & 464k & 16 & 17.13 & 34.35 & 19.82 & 15.82 & 12.62 \\\\ SLM-hn & 100 & 195k & 8 & 18.90 & 33.44 & 18.57 & 15.58 & 12.53 \\\\ LLM & 100 & 108k & 8 & 17.00 & 29.22 & 17.11 & 15.49 & 11.55 \\\\ \\hline SLM & 200 & 1597k & 8 & 19.71 & 34.43 & 18.58 & 15.12 & 12.09 \\\\ SLM-mix & 200 & 928k & 16 & 15.92 & 31.94 & 18.48 & 14.98 & 12.15 \\\\ SLM-hn & 200 & 390k & 8 & 17.74 & 32.30 & 17.76 & 14.95 & 12.13 \\\\ LLM & 200 & 217k & 8 & 15.58 & 28.18 & 15.62 & 14.03 & 10.81 \\\\ \\hline SLM & 400 & 3195k & 8 & 19.17 & 36.61 & 18.22 & 14.80 & 12.00 \\\\ SLM-mix & 400 & 1000k & 16 & 15.82 & 31.04 & 17.56 & 14.42 & 11.84 \\\\ SLM-hn & 400 & 780k & 8 & 16.90 & 32.54 & 17.17 & 14.48 & 11.86 \\\\ LLM & 400 & 434k & 8 & 14.54 & 28.98 & 15.03 & 13.05 & 10.28 \\\\ \\hline SLM-mix & 600 & 1000k & 16 & 15.82 & 31.03 & 17.18 & 14.21 & 11.73 \\\\ SLM-hn & 600 & 1170k & 8 & 16.53 & 32.53 & 16.95 & 14.29 & 11.74 \\\\ LLM & 600 & 651k & 8 & 14.09 & 28.62 & 14.50 & 12.64 & 10.07 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Interpolated perplexities at fixed pretraining costs (GPUh)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r|r|r|r|r} \\hline \\hline Domain & \\multicolumn{4}{c}{Num. clusters} \\\\  & 4 & 16 & 64 & 256 & 1024 \\\\ \\hline arxiv & 0.95 & 0.92 & 0.55 & 0.52 & 0.29 \\\\ euporal & 0.52 & 0.53 & 0.45 & 0.44 & 0.27 \\\\ freelaw & 0.48 & 0.73 & 0.87 & 0.72 & 0.35 \\\\ gutenberg & 0.75 & 0.54 & 0.35 & 0.27 & 0.29 \\\\ opensubtitles & 0.97 & 0.68 & 0.26 & 0.28 & 0.32 \\\\ openwebtext2 & 0.53 & 0.35 & 0.12 & 0.04 & 0.02 \\\\ pubmed abs. & 0.94 & 0.54 & 0.41 & 0.20 & 0.06 \\\\ stackexchange & 0.95 & 0.94 & 0.78 & 0.61 & 0.31 \\\\ wikipedia & 0.71 & 0.58 & 0.21 & 0.07 & 0.03 \\\\ \\hline c4 & 0.32 & 0.12 & 0.04 & 0.02 & 0.00 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Fraction of data in the most frequent cluster, per domain.\n' +
      '\n' +
      '## Appendix F Parameter Efficient Fine-tuning\n' +
      '\n' +
      'We also evaluate Low Rank Adaptation (LoRA) (Hu et al., 2021) as a fine-tuning method for the LLM. LoRA can help regularize the fine-tuning process when little specialization is available. It also reduces the storage and communication costs of managing many specialized models when addressing many domains since only few parameters are learned for each domain. LoRA does not reduce the pretraining cost, and even increases the fine-tuning cost as it requires more fine-tuning steps, with a similar cost per step. In our LoRA experiments we use low-rank matrices of rank 64 which results in 5M trainable parameters and fine-tune for up to 5\\(\\times\\) more steps than for the LLM. We observe that LLM-lora required from 25% more steps than the LLM for the 1M token dataset and 3\\(\\times\\) more steps for the 64M token dataset. However, since the specialization cost is negligible in comparison to the pretraining cost these extra steps do not really impact the overall training cost. Figure 13 reports the results. LoRA performs very similarly to the LLM (differences of less than 0.5 perplexity) and with the exception of the "large" domain-specific regime of 64M tokens we can observe some overfitting mitigation. Finally, LoRA still results in a large model which is not suitable for the cases where the computational budget for inference is small.\n' +
      '\n' +
      'Figure 10: Fine tuning cost as a function of the pretraining cost.\n' +
      '\n' +
      'Figure 11: Specific perplexity on individual subsets after fine-tuning on 1M tokens.\n' +
      '\n' +
      'Figure 12: Specific perplexity on individual subsets after fine-tuning on 1M tokens.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'Figure 13: Specific perplexity of LoRA fine-tuning on the Pile subsets with respect to the pretraining cost. We observe that LoRA fine-tuning performs very similarly to traditional fine-tuning with less than 0.5 perplexity differences.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>