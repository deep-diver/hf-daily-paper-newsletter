<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StructLM: Building Generalist Models for for\n' +
      '\n' +
      '구조화된 지식 접지\n' +
      '\n' +
      'Alex Zhuang\\({}^{1}\\), Ge Zhang\\({}^{1,2,7}\\),\n' +
      '\n' +
      '**Tianyu Zheng\\({}^{2}\\), Xinrun Du\\({}^{2}\\), Junjie Wang\\({}^{3,4}\\), Weiming Ren\\({}^{1,7}\\), Stephen W. Huang\\({}^{6}\\), Jie Fu\\({}^{2,4}\\), Xiang Yue\\({}^{5}\\), Wenhu Chen\\({}^{1,2,7}\\)**\n' +
      '\n' +
      '({}^{1}\\)Waterloo 대학교, \\({}^{2}\\)Multimodal Art Projection Research Community,\n' +
      '\n' +
      'Waseda University, \\({}^{3}\\)Waseda University, \\({}^{4}\\)HKUST, \\({}^{5}\\)Ohio State University, \\({}^{6}\\)harmony.ai, \\({}^{7}\\)Vector Institute Institute\n' +
      '\n' +
      '[https://tiger-ai-lab.github.io/StructLM/](https://tiger-ai-lab.github.io/StructLM/)\n' +
      '\n' +
      'Corresponding Author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '표, 그래프 및 데이터베이스와 같은 구조화된 데이터 소스는 유비쿼터스 지식 소스이다. 일반 텍스트에 대한 대규모 언어 모델(LLM)의 입증된 능력에도 불구하고, 구조화된 데이터를 해석하고 활용하는 능력은 여전히 제한적이다. 우리의 조사는 구조화된 데이터를 처리하는 LLM의 능력, 예를 들어 Chat-GPT가 최신 모델(SoTA)보다 평균 35% 지연되는 현저한 결핍을 보여준다. LLM에서 구조적 지식 접지(Structured Knowledge Grounding, SKG) 기능을 강화하기 위해 110만 개의 예를 포함하는 포괄적인 명령어 튜닝 데이터 세트를 개발했다. 이 데이터 세트를 사용하여 7B에서 34B 매개변수 범위의 코드-LLaMA 아키텍처를 기반으로 StructLM이라고 하는 일련의 모델을 훈련한다. 우리의 StructLM 시리즈는 18개의 평가된 데이터 세트 중 14개의 태스크별 모델을 능가하고 7개의 SKG 태스크에 대한 새로운 SoTA 성과를 수립한다. 또한, StructLM은 6개의 새로운 SKG 작업에 걸쳐 예외적인 일반화를 보여준다. 예상과 달리, 우리는 스케일링 모델 크기가 한계 이점을 제공한다는 것을 관찰하며, StructLM-34B는 StructLM-7B에 비해 약간의 개선만 보여준다. 이는 구조화된 지식 접지가 여전히 도전적인 과제이며 새로운 차원으로 나아가기 위해서는 보다 혁신적인 설계가 필요함을 시사한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '전통적으로 사용자는 테이블, 데이터베이스, 지식 그래프 등과 같은 구조화된 데이터와 인터페이스하기 위해 프로그램을 작성해야 한다. 사용자가 SQL, SPARQL 등과 같은 도메인 특정 언어를 마스터해야 합니다. 최근, 연구자들은 질문 응답(Pasupat and Liang, 2015; Zhong et al., 2017; Nan et al., 2022), 요약(Parikh et al., 2020; Nan et al., 2021; Bao et al., 2018) 및 사실 검증(Aly et al., 2021; Chen et al., 2019; Gupta et al., 2020)에서 모두 구조화된 지식 소스에 근거한다. 이러한 노력은 최종 사용자가 방대한 양의 구조화된 데이터에 액세스할 수 있는 장벽을 낮출 수 있다.\n' +
      '\n' +
      '이전 작업(Yu et al., 2020; Liu et al., 2021; Xie et al., 2022; Zhang et al., 2023)은 대부분 다소 제한된 일반화 능력을 갖는 상이한 작업에 대한 작업-특정 모델을 구축하는 것에 집중되어 왔다. 광범위한 작업에 걸쳐 일반 구조 지식 접지(SKG) 시스템을 구축하는 것은 어려운 것으로 판명되었다. 이는 주로 데이터 형식과 사용 사례의 이질성에 기인한다. 18개의 SKG 태스크에 대해 GPT-3.5-Turbo (Jiang et al., 2023)를 평가하여 그 성능을 관찰하였다.\n' +
      '\n' +
      '도 1: StructLM은 인간 질의에 응답하기 위해 구조화된 지식 및 비구조화된 지식을 접지할 수 있다. 이전의 SoTA는 TAPEX(Liu et al., 2021), USKG(Xie et al., 2022), TableLAMA(Zhang et al., 2023), BINDER-Codex(Cheng et al., 2022) 등과 같은 많은 상이한 태스크-특정 모델들에 의해 달성되었다. StructLM(단일 모델)은 18개의 SKG 작업 중 7개에서 이전 SoTA를 능가한다.\n' +
      '\n' +
      '이 값은 SoTA 전문 모델보다 평균 35% 낮습니다. SKG에 대한 LLM의 능력이 사전 훈련 단계에서 크게 간과되고 있음을 보여준다.\n' +
      '\n' +
      '본 논문에서는 인간과 상호작용할 수 있는 다양한 유형의 구조 및 비정형 지식을 기반으로 할 수 있는 LLM을 기반으로 한 일반주의 모델을 구축할 수 있는 가능성을 탐색한다. 구체적으로, 우리는 100만 개 이상의 명령어 추적 예제의 대규모 데이터 세트를 구성하는데, 그 대부분은 SKG 데이터와 추가 일반 명령어 추적 데이터를 함께 사용하여 일반화 가능성을 개선한다. 우리는 코드 기반 모델의 코드-LLaMA 패밀리를 기반으로 7B, 13B 및 34B의 세 가지 척도로 모델을 미세 조정한다. USKG와 비교했을 때, 우리는 7B 모델이 18\\(18\\)의 14\\(14\\)의 성능에서 34B 모델이 18\\(18\\)의 성능을 능가하면서 11\\(11\\)의 작업에서 단일 작업 모델을 능가한다는 것을 발견했다. 그림 1에서 볼 수 있듯이 StructLM은 18개의 평가된 작업 중 7\\(7\\)에서 SoTA를 달성하여 Chat-GPT를 큰 차이로 이겼다.\n' +
      '\n' +
      '구조LM의 성능, 즉 모델이 데이터 세트 혼합물로부터 교차 작업 일반화 이점을 경험하는지 여부를 연구하고, 다중 작업 모델이 정확히 동일한 매개변수 척도의 단일 작업 모델보다 전반적으로 훨씬 더 나은 성능을 보인다는 것을 발견했다. 또한 코드 또는 수학과 같은 특수 사전 훈련 체제가 효과적인 SKG 추론 능력에 기여하는지 여부를 결정하기 위해 다양한 사전 훈련 데이터가 미세화된 성능에 미치는 영향을 연구한다. 우리는 코드 사전 훈련이 가장 효과적이라는 것을 발견했다. 우리는 결과를 확인하고 주장을 뒷받침하기 위해 추가 절제를 수행한다. 우리의 기여는...\n' +
      '\n' +
      '* 우리는 백만 원(110만 원)의 샘플로 큰 SKG 명령어 조정 데이터 세트를 구성한다. 총 18\\(14\\)의 작업에서 개별 작업을 미세 조정한 기존 3B USKG보다 우수한 3가지 모델을 훈련하고 출시합니다. StructLM은 또한 이들 중 \\(7\\)에서 SoTA 결과를 달성한다.\n' +
      '* 우리는 StructLM이 이전 모델에서 보여주지 않았던 보이지 않는 구조 지식 접지 작업에서 강력한 제로 샷 일반화 능력을 보여줄 수 있음을 보여준다.\n' +
      '* 우리는 일반적인 명령어-튜닝 데이터를 스케일링하는 것이 일반화 능력을 향상시키고, 코드-프리트레이닝된 베이스 모델들이 실제로 SKG 태스크들에 대한 모델 성능을 향상시킨다는 것을 발견한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'SKG 과제 해결\n' +
      '\n' +
      '웹 테이블, 지식 그래프, 데이터베이스와 같은 구조화된 지식은 오랫동안 지식 접지의 연구 대상이 되어 왔다. 그러나 SKG 태스크는 이질적인 데이터 형식을 가지고 있어 이러한 표현을 학습하기 위해 특정 훈련 설정을 활용하는 방법을 고안했다. 예를 들어, PTab Liu 등(2022) 및 MultiHiertt Zhao 등(2022)은 문맥을 학습한다.\n' +
      '\n' +
      '도 2: StructLM 개요. 이 그림은 선형화된 데이터베이스 스키마 및 지식 그래프를 포함하여 선형화된 데이터 테이블을 넘어 다양한 형태의 구조화된 데이터를 처리할 수 있는 능력을 강조하는 StructLM의 프롬프트 구조를 보여준다. StructLM은 보유 태스크 그룹과 유사성을 갖지만 극복해야 하는 차이점 또한 보유 태스크에 대해 평가된다.\n' +
      '\n' +
      '특정 훈련 방법 또는 추론 접근법을 통해 의미 정보를 통합하여 표 형식의 데이터를 나타낸다. RASAT Qi et al.(2022)은 관계인식 자기주장을 Transformer seq2seq 구조와 통합하고 SQL 문제를 해결하기 위해 다양한 관계구조를 활용한다. TAPEX Liu et al.(2021)은 수퍼비전을 제공하기 위해 SQL 실행기의 도움으로 표/데이터베이스 데이터에 대한 사전 훈련을 수행한다.\n' +
      '\n' +
      '보다 최근에는 이러한 보조적 과제별 구조에서 벗어나기 위한 방법들이 시작되고 있다. USKG Xie et al.(2022)은 많은 SKG 태스크들을 시퀀스-투-시퀀스 포맷으로 최초로 통합함으로써, 이들이 동일한 데이터 혼합물로 집성될 수 있게 했다. StructGPT Jiang et al.(2023)은 보다 견고하고 정확도로 태스크들을 해결하기 위해 강력한 LLM들 상의 프롬프트 프레임워크들을 사용하는 작업 라인을 나타낸다. 대조적으로, 우리의 작업은 개방형 모델을 조사하고 기본 기능을 평가하려고 한다. TableLlama Zhang et al.(2023)은 우리의 작업과 유사하지만 표 데이터에만 초점을 맞추고 있으며, 이는 SKG의 하위 집합을 고려한다.\n' +
      '\n' +
      'USKG는 더 강력한 언어 모델에서 시퀀스 대 시퀀스 형식 통일의 이점을 보여주었지만, 성능 측면에서 단일 작업 모델이나 접두사 조정과 같은 작업별 훈련 방법에 비해 SKG 데이터의 다중 작업 믹스를 구성하는 데 강한 이점을 보여주지 못했다. 우리는 USKG의 태스크들의 큰 서브세트를 갖는 명령어-조정 SKG 데이터세트와, 강력한 멀티-태스크 성능을 나타내는 결과 모델인 StructLM을 구축함으로써 이러한 격차를 해소하고자 한다.\n' +
      '\n' +
      '명령어 튜닝을 이용한### LLMs\n' +
      '\n' +
      'LLM에서 교육 목표와 사용자 목표 사이의 격차를 해결하기 위한 방법으로 명령어 조정(IT)이 대중화되었다. 이 기술은 명령어 및 출력 쌍을 사용하여 LLM의 추가 훈련을 포함한다. IT는 모델의 제어 가능성과 예측 가능성을 모두 향상시켜 사용자의 기대에 더 가깝게 정렬합니다. 또한, 최근 FLAN Wei et al.(2022), UL2 Tay et al.(2023), LLAMA2 Touvron et al.(2023) 등의 연구에서는 IT가 다양한 데이터 유형에 걸쳐 멀티 태스크 학습을 통해 다운스트림 태스크의 성능을 향상시킬 수 있음을 보여주었다. FLAN-UL2는 USKG의 11개 작업의 하위 집합에서 훈련하는 반면, 더 많은 관련 없는 언어 작업에서도 훈련한다. 우리의 작업에서는 SKG 데이터에 초점을 맞추어 이 과제 유형에 대한 성과 향상을 위한 향후 작업의 참고가 될 수 있는 집중적인 연구를 제공하고자 한다.\n' +
      '\n' +
      '### LLMs에서의 추론 능력\n' +
      '\n' +
      '추론은 프로그래밍이나 수학적 문제 해결 Li 등(2022)과 같이 전통적으로 인간의 사고를 필요로 하는 많은 사고 집약적 작업을 자율적으로 완료할 수 있도록 하는 실제 AI 응용 프로그램 개발에 있어 LLM의 중추적인 기술로 자리 잡고 있다. 최근의 연구들 Li 등(2022, 2023); Roziere 등(2023); Azerbayev 등(2023)은 코드 및 수학적 데이터 세트들에 대해 트레이닝된 LLMs들이 심오한 추론 기술을 나타내고, 심지어 인간 레벨들과 동등한 성능을 달성할 수 있다는 것을 나타낸다. 예를 들어, 더 많은 프로그래밍 데이터에 대해 트레이닝된 기초 모델인 Code-Llama Roziere 등(2023)은 다양한 프로그래밍 및 수학적 벤치마크에 걸쳐 상당히 개선된 추론 능력을 갖는다. 나아가, LLemma Azerbayev et al.(2023)은 과학 논문, 수학 관련 웹 데이터, 및 수학 코드의 혼합물 상에서 코드-라마 모델을 계속해서 사전 훈련시킨다. 그 결과는 MATH 벤치마크인 Hendrycks et al.(2021)에 대한 우수한 추론 능력과 더 이상의 미세 조정 없이 정리를 증명하는 능력을 보여준다. 파인-튜닝 측면에서, WizardMath Luo et al. (2023), WizardCoder Luo et al. (2023)은 고품질 데이터가 주어졌을 때 추론 능력에 대한 명령어 튜닝의 효과를 보여주었다.\n' +
      '\n' +
      '본 연구에서는 구조화된 데이터를 LLM 내에서 다른 종류의 추론에 대한 세 번째 테스트베드로 간주한다. 우리는 수학적 또는 논리적 추론 외에도 구조화된 입력 내에서 패턴을 인식하고 사용하는 능력이 모델이 데이터에서 관계에 대한 강력한 표현을 가지고 있음을 나타낸다. 이러한 표현은 다운스트림에서 추가 추론을 위한 강력한 사전 역할을 할 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Dataset Curation\n' +
      '\n' +
      '일반적으로 광범위한 정형 데이터 작업을 수행할 수 있는 언어 모델(LM)을 훈련시킨다는 목표에 의해, 우리는 연구할 총 \\(25\\) SKG 작업을 선택한다. 우리는 보류된 과제와 보류된 과제에 대한 결과를 보고하는데, 각 보류된 과제는 보류된 과제의 일반화 능력을 대략적으로 평가하는 것을 의미했다. 총 보유 훈련 데이터 세트에는 약 700k의 SKG 예가 포함되어 있다. 우리는 아래에 보유 데이터 세트 그룹을 설명한다.\n' +
      '\n' +
      '**데이터 to Text Generation** 이 데이터 세트는 테이블에서 지식 3배, 공식 언어로 구조화된 데이터의 요약 또는 해석과 함께 처리됩니다. 그들의 포함은 유용한 LM이 매우 다양한 구조화된 정보를 이해하고 자연 언어로 의미를 매핑할 수 있어야 한다는 생각에 동기 부여된다. 이 작업 그룹에 대한 해당 보류 데이터 세트는 위키표 텍스트로 의도된다.\n' +
      '\n' +
      '**표 기반 질문 답변** 이 데이터 세트 그룹은 선택적으로 텍스트 패시지와 결합된 표형 데이터를 구체적으로 다룬다. 질문에 정확하게 답하고 테이블에서 정보를 검색할 수 있는 LM은 조수로서 널리 유용할 수 있다. 이 작업 그룹에 대한 해당 보류 데이터 세트는 SQA입니다.\n' +
      '\n' +
      '**지식 기반 대화** 이 작업 그룹은 지식 접지 대화를 평가합니다. 인간이 자연스럽게 LM과 인터페이스하는 것은 채팅을 통해 이루어지며, 이러한 기능을 가능하게 하는 것은 저장된 구조화된 데이터에 있는 정보에 접근하는 장벽을 낮출 수 있다. 이러한 작업은 제공된 대화를 통해 사용자 의도를 추적하고 모델에 최신 질문에 대한 답변을 제공하도록 요청한다. 이 작업 그룹에 대해 보류된 데이터 세트는 CoSQL입니다.\n' +
      '\n' +
      '\'사실 확인\' 표의 일반적인 사용 사례 중 하나는 사실을 참조하는 것이다. 질문 응답에 더하여, 테이블 내의 데이터가 문을 지원하는지를 신뢰성 있게 결정하는 능력은 테이블의 데이터의 강건한 표현의 존재를 시그널링한다. 이 작업 그룹의 보류된 데이터 집합은 인포탭입니다.\n' +
      '\n' +
      '**SQL 또는 도메인 특정 언어** SQL은 오늘날 구조화된 데이터와 인터페이스하기 위해 가장 일반적으로 사용되는 언어이다. SQL을 작성하는 방법을 이해하려면 테이블의 추상화와 함께 연결되는 방법에 대한 이해도 필요합니다. 다른 도메인-특정 언어들에서, MTOP 태스크는 사양을 파싱하고 API 호출을 생성하는 모델의 능력을 측정하고, 이는 LLM 툴 사용에서의 가능성을 본다(예를 들어, (Qin et al., 2023)). 이 태스크 그룹에 대한 해당 보류 데이터 세트는 BIRD(Li et al., 2023b)로 의도되며, 이는 SQL 생성 능력을 추가로 테스트한다.\n' +
      '\n' +
      '수학적 추론 표형 데이터의 분석은 또한 그들의 콘텐츠에 대해 빠른 수학적 계산을 수행하는 것을 요구할 수 있다. 이러한 데이터 세트에 대한 성능은 모델이 구조화된 지식과 수학적 추론을 얼마나 잘 결합할 수 있는지 알려줍니다. 현재 SKG와 수학적 추론을 결합하는 데이터 세트의 수가 제한되어 있기 때문에 이 범주에는 보유 코퍼스에 TabMWP만 포함된다. 우리는 FinQA를 도전적인 보류 데이터 세트 아날로그로 설정했다. 금융 영역 지식을 필요로 할 뿐만 아니라 표상의 정보와 긴 텍스트 구절을 결합하여 수학적 코드의 생성을 요구한다.\n' +
      '\n' +
      '**일반 명령 데이터** 보유 데이터세트 혼합물 내에 SKG 데이터세트 외에도 모델의 명령어 추종 능력을 유지하기 위해 구조화된 지식 구성요소 없이 일반적인 명령어 튜닝 데이터도 포함했다. 우리는 SlimOrca(Lian et al., 2023)를 사용하는데, 이는 기존의 일반적인 대규모 명령어-추종 데이터세트로부터의 다수의 프롬프트에 대해 세척된 GPT-4 응답으로부터 구성된다. 절제 결과를 기반으로 보류된 데이터 세트에 대한 데이터 오염 징후를 감지하지 못한다. 표 1의 모든 데이터 세트 통계에 대한 자세한 개요를 제공한다.\n' +
      '\n' +
      '### 명령어 파이네튜닝 접근방법\n' +
      '\n' +
      '모델 조정을 지시하기 위해 데이터 세트의 각 예는 시스템 프롬프트, 지시, 입력 및 출력으로 구성된다. 모든 SKG 데이터 예제에 대해 동일한 시스템 프롬프트를 사용합니다. 각 데이터 세트에 대해 훈련 샘플을 구성할 때 무작위화된 10개의 명령어 변형을 작성한다. SKG 데이터의 경우, 입력은 구조화된 지식 입력과 과제 지정에 필요한 질문, 진술 또는 무엇이든 될 수 있는 수반되는 텍스트의 조합으로 구성된다.\n' +
      '\n' +
      '[그림 3] : 구조화된 지식 유형과 과제의 분류. 내부에서는 데이터 세트에서 다양한 범주의 구조화된 입력에 대한 대략적인 분석을 볼 수 있습니다. 외부에서 우리는 이러한 구조화된 지식 유형에 대해 표현되는 작업을 본다. 우리는 또한 SlimOrca(Lian et al., 2023)의 데이터에 따른 일반적인 명령의 상당한 비율을 가지고 있다.\n' +
      '\n' +
      '정확한 프롬프트 형식은 그림 6에 나와 있다.\n' +
      '\n' +
      '### 훈련 및 평가 세부사항\n' +
      '\n' +
      'StructLM에 대한 기본 모델들은 모델들의 CodeLlama-Instruct 계열이다(Roziere et al., 2023). 우리는 A800 gpus에서 \\(3\\) 에폭에 대해 배치 크기가 \\(512\\)인 모든 모델을 세밀하게 조정한다. 이러한 트레이닝 셋업은 크게 WizardLM(Xu et al., 2023), WizardMath(Luo et al., 2023), 및 WizardCoder(Luo et al., 2023) 모델에 사용되는 설정과 같은 커뮤니티 컨벤션에 부합한다.\n' +
      '\n' +
      '우리는 USKG(Xie et al., 2022)의 정형 데이터 선형화 규약을 따른다. 그러나 우리는 아래에 설명된 것과 다른 절단 방식을 사용한다. 훈련하는 동안, 우리는 최대 수열 길이를 \\(2048\\) 유지한다. USKG는 더 긴 맥락으로 추론을 훈련하고 실행하는 것이 유익하다는 점에 주목한다. 따라서 잘라낼 때 프롬프트 입력 및 출력 라벨의 결합된 토큰 길이를 고려한다. 우리는 입력의 구조화된 지식 부분만을 잘라서 예제 길이가 최대 \\(2048\\) 토큰이 되도록 한다. 표 1의 데이터 세트 통계에서 볼 수 있듯이, 데이터 세트에서 예제의 최대 토큰 길이를 \\(2048\\)으로 설정하면 거의 모든 예가 드문 절단으로 컨텍스트 창 내에 들어갈 수 있다. 우리는 이러한 구조화된 입력 절단조차도 불충분한 예(예: 출력이 너무 길다)를 폐기한다. 추론 과정에서 입력 토큰 길이를 \\(2048\\)으로 설정하여 입력 컨텍스트 내에 더 많은 구조화된 정보가 배치되도록 한다. 최대 생성 길이를 1024로 설정했으며, 이는 모든 데이터 세트에서 모든 올바른 응답에 충분한다. 단일 작업 세분화된 모델을 포함한 각 모델에 대해 각 에폭의 끝에 생성된 \\(3\\) 체크포인트 중 가장 성능이 좋은 체크포인트를 선택한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '먼저, SKG 태스크에 대한 언어 모델의 현재 성능을 설명하기 위해 ChatGPT(GPT-3.5-turbo)와 기본 모델 CodeLlama-7B-Instruct를 1-shot 환경에서 평가한다. 유지된 훈련에서와 동일한 선형화된 지식 구조를 사용하는 우리의 프롬프트 스킴은 보이지 않는 구조 지식 형식으로 인해 많은 작업으로 인해 전면적으로 어려움을 겪는 것을 본다. ChatGPT는 텍스트 기반 작업에서 우수하지만 SKG 작업에서는 성과가 부진하다. 상기 갭은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r|r r|r r r r|r r r} \\hline \\hline  & \\multicolumn{2}{c|}{Overall Length} & \\multicolumn{4}{c|}{Train} & \\multicolumn{4}{c}{Test} \\\\ \\hline \\multirow{2}{*}{Dataset} & Input & Output & \\multirow{2}{*}{Count} & \\multicolumn{2}{c}{Input} & \\multicolumn{2}{c}{Output} & \\multirow{2}{*}{\\# trunc.} & \\multirow{2}{*}{Count} & \\multicolumn{2}{c}{Input} & \\multirow{2}{*}{Output} \\\\  & (avg) & & & & & & & & & \\\\ \\hline TabMWP & 207.8 & 4.5 & 23059 & 709 & 33 & 0 & 7686 & 703 & 31 & 0 \\\\ ToTTo & 251.8 & 31.0 & 120761 & 2040 & 155 & 467 & 7700 & 2048 & 119 & 31 \\\\ GrailQA & 281.0 & 44.1 & 44337 & 884 & 134 & 0 & 6463 & 546 & 123 & 0 \\\\ SQL2Text & 122.3 & 18.1 & 5600 & 337 & 61 & 0 & 1034 & 245 & 38 & 0 \\\\ MMQA & 656.2 & 7.7 & 15688 & 2047 & 146 & 234 & 1501 & 2048 & 94 & 11 \\\\ Spider & 266.6 & 36.0 & 7000 & 1369 & 226 & 0 & 1034 & 453 & 146 & 0 \\\\ KVRet & 573.4 & 17.1 & 6288 & 1217 & 161 & 0 & 807 & 1147 & 82 & 0 \\\\ HybridQA & 700.4 & 6.8 & 62682 & 2047 & 91 & 200 & 3466 & 2048 & 79 & 6 \\\\ SParC & 276.3 & 32.6 & 12059 & 1417 & 226 & 0 & 1625 & 467 & 146 & 0 \\\\ CompWebQ & 1350.3 & 11.9 & 27639 & 2047 & 321 & 321 & 2816 & 2048 & 256 & 8 \\\\ TabFact & 660.1 & 4.6 & 92283 & 2045 & 5 & 2 & 12779 & 1687 & 4 & 0 \\\\ WikiTQ & 831.8 & 5.8 & 11321 & 2028 & 273 & 0 & 4344 & 2048 & 148 & 10 \\\\ WikiSQL & 689.2 & 7.1 & 56355 & 2047 & 518 & 16 & 15878 & 2048 & 244 & 1 \\\\ FeTaQA & 653.2 & 38.8 & 7326 & 1853 & 158 & 0 & 2003 & 1548 & 114 & 0 \\\\ FEVEROUS & 799.3 & 3.4 & 40669 & 2047 & 5 & 2052 & 4285 & 2048 & 4 & 195 \\\\ MultiWOZ & 777.2 & 154.5 & 56668 & 1656 & 196 & 0 & 7368 & 1344 & 185 & 0 \\\\ DART & 133.7 & 30.3 & 62659 & 406 & 258 & 0 & 5097 & 261 & 109 & 0 \\\\ Logic2Text & 166.1 & 26.9 & 8566 & 358 & 67 & 0 & 1092 & 347 & 60 & 0 \\\\ MTOP & 961.0 & 34.4 & 15667 & 1002 & 215 & 0 & 4386 & 990 & 113 & 0 \\\\ SlimOrca & 278.9 & 152.4 & 512069 & 2047 & 1808 & 0 & - & - & - \\\\ \\hline BIRD & 439.8 & 63.3 & 9428 & 1992 & 347 & 99 & 1534 & 1214 & 386 & 0 \\\\ CoSQL & 287.4 & 34.9 & 9502 & 1640 & 226 & 0 & 1300 & 535 & 190 & 0 \\\\ SQA & 656.9 & 34.9 & 12275 & 1812 & 1012 & 2 & 3011 & 1725 & 769 & 0 \\\\ Infotabs & 276.9 & 3.7 & 16538 & 1009 & 5 & 0 & 5400 & 1105 & 4 & 0 \\\\ WikiTableText & 149.6 & 27.4 & 10000 & 313 & 97 & 0 & 2000 & 226 & 89 & 0 \\\\ Finqa & 1230.3 & 21.0 & 6251 & 2040 & 72 & 186 & 1147 & 2048 & 61 & 25 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 열차 및 테스트 세트의 각 데이터 세트에 대한 토큰 시퀀스 길이 통계량. 입력 및 출력 통계는 토큰으로 표시됩니다. 우리는 각 데이터 세트에서 잘린 예제의 수를 보고한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'Held-out ResultsOn held out 태스크에서 StructLM은 6개의 태스크 중 5개의 태스크에서 ChatGPT를 능가하는 강력한 일반화 성능을 보인다. FinQA[3] 데이터셋에 대한 제로샷 프롬프트 방법은 모델이 커스텀 코드를 생성해야 하고 금융 도메인에만 집중되며, Infotabs[11]에 대한 입력은 입력 데이터에서 보이지 않는 다른 유형의 테이블 구조이며, 홀딩된 사실 확인 작업의 선택과는 다른 부울 출력 카테고리에 대한 레이블이 있다. 이러한 일반화 능력은 각 보유 태스크의 이름이 훈련 입력에 지정되기 때문에 USKG 모델에는 존재하지 않는다. TableLLaMA와 비교할 때 StructLM도 훨씬 더 잘 일반화된다.\n' +
      '\n' +
      '## 5 Ablation Studies\n' +
      '\n' +
      '**기본 모델 사전 훈련 데이터의 효과** 우리는 수학적 텍스트 [1]에서 추가로 사전 훈련된 비특이적 Llama2-7b 기본 모델과 Llemma를 미세 조정함으로써 기본 모델인 코델라마-7b-지시를 선택한다. 직관적으로 코딩 능력은 프로그래밍 언어와 코드 작성 시나리오의 상징적 특성으로 인해 우리가 연구하고 있는 SKG 태스크 유형에서 성능으로의 전달 가능성이 가장 높다고 추측할 수 있다. 그러나 수학과 같은 추론 능력을 높이기 위한 다른 유형의 사전 훈련은 훨씬 더 큰 전이 가능성을 가질 수 있다.\n' +
      '\n' +
      '표 6의 절제 결과는 그림 4와 같이 작업의 그룹으로 나눌 수 있다. 코드에 미리 훈련된 모델은 실제로 약간 수행한다.\n' +
      '\n' +
      '그림 4: 관련 과제 그룹화에서 다양한 사전 교육 커리큘럼이 SKG 미세화 성과에 미치는 영향.\n' +
      '\n' +
      '그림 5: 일반 명령어 추적 데이터가 보류된 SKG 데이터 세트 성능에 미치는 영향. 성과는 보류 그룹 또는 보류 그룹 내의 모든 작업에 걸쳐 평가 지표에 대한 평균으로 측정됩니다. 보유 공연이 보유 공연 이익에 비해 더 가벼운 하락을 경험한다는 점에 유의해야 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline Purpose & Train & Eval & FT & Result \\\\ \\hline \\hline Schema task transfer & Spider, SParC, Logic2Text & Logic2Text & 89.47 & 89.93 \\\\ \\hline KT task transfer & CompWebQ, WebQSP, GrailQa, Dart & Dart & 60.28 & 60.34 \\\\ \\hline \\multirow{3}{*}{Table task transfer} & FetaQA, HybridQA, WikiTQ, TabMWP, ToTTo, MMQA, WikiSQL, KVRet, Tab Fact, Feverous, Infotabs & TabFact, Feverous & \\multirow{3}{*}{75.46} & \\multirow{3}{*}{80.81} \\\\  & Feverous, Infotabs & & & \\\\ \\hline Summ. data type transfer & ToTTo, Dart & Dart & 60.28 & 61.42 \\\\ \\hline QA data type transfer & CompWebQ, WikiSQL & WikiSQL & 85.49 & 86.36 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 크로스 태스크 및 크로스 데이터 타입 전송 결과. FT는 Eval 열의 데이터 세트에 대한 단일 작업 성능의 평균입니다.\n' +
      '\n' +
      '더 좋고, 이러한 이득은 반드시 문법적으로 규칙적인 입력을 명시적으로 수반하거나 코드의 생성을 요구하는 태스크에 제한되지 않는다. 수학 사전 훈련은 Llama2 기반 모델의 성능을 향상시키는 것으로 보이지만 코드 사전 훈련만큼 향상되지는 않는다. 전반적으로 코드 사전 훈련은 이 SKG 설정에서 수행 모델을 훈련하는 데 유용한 단계인 것으로 보이며, 이는 특정 작업에 대한 개념적 유사성 때문일 수 있다.\n' +
      '\n' +
      '**혼합에서 일반 명령 데이터의 효과** 훈련 데이터 혼합물에 대략 \\(500\\)k 일반 명령 후속 예제를 포함시킴으로써 이 데이터를 포함하는 것의 중요성을 관찰한다. 그림 5에서 볼 수 있듯이 보류된 성능은 추가된 일반적인 예에 의해 상대적으로 영향을 받지 않지만 보류된 성능은 보다 일반적인 데이터로 크게 개선된다. 경험적으로, 우리는 또한 많은 양의 작업별 입력 및 출력 형식을 훈련할 때 모델이 제로 샷 설정에서 새로운 작업에 대한 지침을 따를 수 없게 된다는 것을 관찰한다. 이 일반적인 혼합물에 대한 훈련은 훈련 세트의 작업 형식에 대한 과적합을 줄일 수 있기 때문에 제로 샷 성능에 도움이 된다고 가정한다.\n' +
      '\n' +
      '**교차 작업 및 교차 형식 전달 가능성** 입력 구조 지식 유형 간 및 출력 작업 유형 간 성능의 전달 가능성을 제거합니다. 이를 테스트하기 위해 우리는 여러 작업을 함께 훈련하고 단일 작업 미세 조정 모델과 비교한다. 우리의 결과는 발생하는 성능의 교차 작업 전달 가능성이 있음을 나타낸다. 스키마와 지식 트리플에서, 다른 과제에 대한 훈련은 그 평가 과제에 대해서만 세밀하게 훈련하는 것에 비해 평가 과제의 성능을 약하게 향상시키는 것으로 보인다. 테이블에서, 우리는 이 효과가 훨씬 더 크다는 것을 안다. 이에 대한 한 가지 설명은 훈련 믹스에 포함된 테이블 태스크가 매우 다양하고 보조 데이터의 양이 다른 데이터 유형보다 크다는 것이다.\n' +
      '\n' +
      '반면에, 우리는 동일한 작업(즉, 요약)으로 다른 데이터 유형에 대한 미세 조정도 성능에 대한 이점을 제공한다는 것을 안다. 요약 및 질의응답 실험에서는 표와 지식 그래프 데이터를 학습한다. 우리는 다트를 사용하여 요약을 평가하고 WikiSQL을 사용하여 QA를 평가한다. 우리는 각 경우에 하나의 추가 데이터 세트만 추가했지만 이러한 데이터 세트가 매우 다른 지식 유형을 가지고 있음에도 불구하고 단일 작업 모델에 비해 향상된 결과를 여전히 볼 수 있음을 알 수 있다. 따라서 전반적인 증거는 우리의 훈련 세트 내 작업의 수행이 데이터 유형 전반에 걸쳐 서로 이익을 얻고 있음을 뒷받침한다.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      '우리는 SKG가 미래 언어 모델에 중요한 능력이라고 주장한다. ChatGPT와 LLama2 계열에 대한 실험을 통해 개선의 여지가 있음을 확인했다.\n' +
      '\n' +
      '우리는 SKG 과제에 대한 집중적인 지도 조정을 통해 강력한 모델을 생성할 수 있음을 발견했지만, 7B에서 34B 구조LM 모델 사이의 성능 차이는 극적이지 않다는 것을 관찰했다. 이것은 SKG 데이터의 상태에 대한 우려를 제기한다: 우리가 성능 상한선에 접근할 수 있을까? 우리가 훨씬 더 큰 모델인 UL2-20b를 3개의 작업에서 7B 모델로 능가할 수 있었다는 사실과 결합하여 다양한 규모의 LLM이 SKG 역량에 어려움을 겪고 있는 것으로 보인다.\n' +
      '\n' +
      '실제로 모델의 입력에서 구조화된 지식에 직접 접지하는 것은 추론과 입력 민감도에 대한 도전을 나타낸다. 그러나, 그것은 광범위한 잠재적 이점을 가지고 있습니다. SKG 능력을 의미 있게 향상시키기 위해 향후 연구에서는 보다 구조화된 데이터 포맷에 대한 개방형 기초 모델의 지속적인 사전 훈련을 탐색할 수 있다고 제안한다. 코드 또는 수학 프리트레이닝의 현재 시도들과 유사하게, 테이블 또는 다른 유형의 정규 데이터 포맷팅으로 인터리빙된 텍스트 상의 프리트레이닝 모델들이 기초 모델 능력으로서 SKG를 확립하는 것을 향해 나아가는 것을 도울 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 구조화된 지식접지 작업에 대한 개방형 언어 모델의 현재 능력을 탐구한다. 우리는 LLM이 현재 SKG 작업에 약하다는 것을 보여준다. 이 격차를 해결하기 위해 1.1M 예제의 명령어 조정 데이터 세트 혼합물을 구성하고 대부분의 작업(\\(14\\)\\(18\\))에서 USKG를 능가하는 모델을 릴리스하고 그 중\\(7\\)에서 SOTA를 달성한다. 또한 이러한 과제 유형에 대한 모델의 성능에 영향을 미치는 다양한 요인의 영향을 연구한다. 우리는 우리의 작업이 SKG 도메인에서 달성할 수 있는 것에 대한 업데이트된 이해를 제공하고 향후 개선을 위한 강력한 기준선 역할을 할 수 있기를 바란다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'StructLM에 대한 학습 데이터를 구성하는 데 사용되는 수집 프로세스는 다양한 데이터 유형을 포함하려고 한다. 우리가 본 바와 같이, 이러한 다양성이 혼합물의 각 데이터 세트에 전달 가능한 이점을 제공할 수 있다는 증거가 있다. 그러나 우리가 훈련하고 평가하는 작업은 각각 특정 목적을 위해 큐레이션되고 설계된 학술 데이터 세트이다. 이것들은 실제로 다양하지만 SKG 도메인은 특정 포맷팅 및 프롬프트 컨벤션에 의존하며, 이는 우리 모델이 기차 세트 내의 컨벤션에 대해 불필요한 특이성을 가질 수 있다. SKG가 자체 도메인으로 수행하는 방법에 대한 보다 명확한 그림을 개발하려면 더 이질적인 포맷팅 규칙을 가진 더 큰 규모의 데이터 세트가 필요할 수 있다. 더 강력한 SKG 모델을 훈련시킬 수 있는 추가 기회는 이러한 방식으로 구조화된 데이터 유형의 다양성을 증가시키는 데 있을 수 있다.\n' +
      '\n' +
      '또한, 모델을 최대한으로 평가하려고 노력했지만, 보유 데이터 세트의 많은 작업은 모델 출력의 휴리스틱 매칭 단계를 통해 정확도를 측정한다. Zero 또는 few-shot 설정에서, 어떤 규칙이나 문법에 순응하도록 자기회귀 변압기의 세대를 정확하게 제어하는 것은 상당히 어렵고, 이것은 다른 작업들(즉, PICARD(Scholak et al., 2021))에서의 연구의 주제였다. 우리는 이러한 현실 때문에 제로 또는 소수의 맥락에서의 열악한 결과가 모델이 이미 학습한 유용한 표상의 존재를 배신할 수 있다는 점에 주목한다. 더 이상의 촉진 또는 미세 조정 노력 없이, 이러한 능력을 조명하는 것은 어려울 수 있다. 따라서, 우리의 방법에 대한 개선의 또 다른 기회는 언어 모델 평가의 보다 유연한 제한 방법을 포함할 수 있다.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      '본 논문은 어떠한 윤리적 우려도 제기하지 않는다. 본 연구에서 사용된 데이터와 추가 리소스는 오픈 소스이며 기존 작업에서 널리 활용된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aly et al. (2021) Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. FeverOUS: 비구조화 및 구조화된 정보에 대한 팩트 추출 및 베리피케이션.\n' +
      '* Azerbayev et al. (2023a) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 2023a. Llemma: 수학을 위한 개방형 언어 모델. _ CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023b) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 2023b. Llemma: 수학을 위한 개방형 언어 모델. _ CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023c) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 2023c. Llemma: 수학을 위한 개방형 언어 모델. _ CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023d) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 2023d. Llemma: 수학을 위한 개방형 언어 모델. _ CoRR_, abs/2310.10631.\n' +
      '* Baro et al. (2018) Junwei Baro, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, Tiejun Zhao. 2018. Table-to-text: 자연어로 테이블 영역을 기술하는 것. _ AAAI Conference on Artificial Intelligence_, 32(1).\n' +
      '* 태스크 지향 대화 모델링을 위한 대규모 다중 도메인 마법사-of-Oz 데이터세트. [Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 5016-5026, Brussels, Belgium. 컴퓨터 언어학과의 연관성\n' +
      '* Chen et al. (2019) Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019. Tabfact: table-based fact verification을 위한 대규모 데이터셋. _International Conference on Learning Representations_.\n' +
      '* Chen et al. (2020a) Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2020a. Hybridqa: 표 및 텍스트 데이터에 대한 멀티홉 질문 응답의 데이터세트. _ EMNLP 2020_의 결과.\n' +
      '* Chen et al. (2021) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. Finqa: 금융 데이터에 대한 수치추론의 데이터셋. _ EMNLP 2021_의 진행례.\n' +
      '* Chen et al. (2020b) Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyu Zhou, Yunkai Zhang, Sairam Sundaresan, and William Yang Wang. 2020b. Logic2Text: 논리적 양식으로부터 고충실도 자연어 생성. _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 2096-2111, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Cheng et al. (2022) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022. Binding language models in symbolic language. _The Eleventh International Conference on Learning Representations_.\n' +
      '* Cive et al. (2021) Jordan Cive, Kris Cao, and Marek Rei. 2021. 파라미터 효율적인 텍스트 생성을 위한 컨트롤 프리픽스_ arXiv preprint arXiv:2110.08329_.\n' +
      '* Dai et al. (2021) Yinpei Dai, Hanyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, 및 Xiaodan Zhu. 2021. 미리보기, 참석 및 검토: 멀티 도메인 대화 상태 추적을 위한 스키마 인식 커리큘럼 학습.\n' +
      '* Eric et al.(2017) Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. 2017. 태스크 지향 대화를 위한 키-값 검색 네트워크. <제18회 연례 SIGdial Meeting on Discourse and Dialogue_>에서. 컴퓨터 언어학과의 연관성\n' +
      '* Gu et al. (2023) Yu Gu, Xiang Deng, and Yu Su. 2023. 생성, 구별하지 않음: 언어 모델을 실제 환경에 접지하는 제안.\n' +
      '* Gu et al. (2021) Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. ID를 넘어: 지식 기반에 대한 질문 답변을 위한 일반화의 세 가지 수준. _Proceedings of the Web Conference 2021_, pages 3477-3488. ACM.\n' +
      '\n' +
      '디팍 굽타, 수라비 쿠마리, 아시프 엑발, 푸시팍 바타차리야. 2018. MMQA: 영어와 힌디어를 위한 다중 도메인 다중 언어 질문 응답 프레임워크. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan. 유럽 언어 자원 협회(ELRA).\n' +
      '* Gupta et al. (2020a) Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020a. Infotabs: 반구조화된 데이터로서 테이블에 대한 추론. _ arXiv preprint arXiv:2005.06117_.\n' +
      '* Gupta et al. (2020b) Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020b. INFOTABS: 반구조화된 데이터로서 테이블에 대한 추론. _Proceedings of the 58th Annual Meeting for Computational Linguistics_, pages 2309-2324, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 수학 데이터세트로 수학 문제 해결을 측정하는 단계.\n' +
      '* Iyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. 순차적인 질의 응답을 위한 검색 기반 신경망 구조화 학습. _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1821-1831, Vancouver, Canada. 컴퓨터 언어학과의 연관성\n' +
      '* Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: 구조화된 데이터를 추론하기 위한 대규모 언어 모델을 위한 일반 프레임워크. ArXiv:2305.09645[cs].\n' +
      '* Lee et al.(2023) 이성민, 박은환, 서대룡, 전동현, 강인호, 나승훈. 2023. MAFiD: Moving average equipped fusion-in-decoder for question answering over tabular and textual data. _Findings of the Association for Computational Linguistics: EACL 2023_, pages 2337-2344, Dubrovnik, Croatia. 컴퓨터 언어학과의 연관성\n' +
      '* Li et al. (2021) Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: 포괄적인 다국어 태스크 지향 시맨틱 파싱 벤치마크. _Proceedings of the 16th Conference of the European chapter of the Computational Linguistics Association: Main Volume_, pages 2950-2962, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Li et al. (2023a) Haoyang Li, Jing Zhang, Cuiping Li, 및 Hong Chen. 2023a. ResdSql: 텍스트-to-sql에 대한 스키마 연결 및 스켈레톤 파싱 해제. _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 13067-13075.\n' +
      '* Li et al. (2023b) Jinyang Li, Bingyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Baiilin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023b. llm이 이미 데이터베이스 인터페이스 역할을 할 수 있습니까? 대규모 데이터베이스 기반 텍스트에서 sqlS를 위한 큰 벤치\n' +
      '* Li et al. (2023) Raymond Li, Loughna Ben Allal, Yangtian Zi, Niklas Muennighoff, Terry Yoe Zhuo, Denis Koccetkov, Cristopher Akiki, Jia Li, Jenny Chim, Olivier Dehaene, Mishib Davaadj, Joel Lamy-Poiter, Joao Monteiro, Oleh Shliazhko, Nicolas Gantier, Nicholas Meade, Muhasham Oblokulov, Jano Zillagoni, Jano Zillagoni, Muhasham Oblokulov, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jano Zillagoni, Jan 2023c. 스타코더: 우리가 당신과 함께 출처를 알 수 있기를! _ CoRR_, abs/2305.06161.\n' +
      '* Li et al. (2024) Shujie Li, Liang Li, Ruying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, and Yongbin Li. 2024. 구조화된 데이터를 데이터 대 텍스트 사전 트레이닝을 위한 그래프로 통합. _ ArXiv_, abs/2401.01183.\n' +
      '* Li et al. (2022) Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Kimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\'Autume, Igor Babuschkin, Xinjun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cheredanyo, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. 알파벳 코드를 갖는 경쟁 레벨 코드 생성. _ CoRR_, abs/2203.07814.\n' +
      '* Lian et al. (2023) Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium." 2023. Slimorca: 검증과 함께 gpt-4 증강 플란 추론 트레이스의 오픈 데이터세트.\n' +
      '* Liu et al. (2022) Guang Liu, Jie Yang, and Ledell Wu. 2022. Ptab: 표형 데이터 모델링을 위해 사전 학습된 언어 모델을 이용. _ CoRR_, abs/2209.08060.\n' +
      '* Liu et al. (2021) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, 및 Jian-Guang Lou. 2021. Tapex: neural sql executor의 학습을 통한 테이블 사전 학습.\n' +
      '* Lu et al. (2023) Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. 반구조화된 수학적 추론을 위한 정책 기울기를 통한 동적 프롬프트 학습. In _International Conference on Learning Representations (ICLR)_.\n' +
      '* Luo et al. (2023a) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jiangguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shiffeng Chen, and Dongmeng Zhang. 2023a. Wizardmath: 강화된 evol-instruct를 통해 큰 언어 모델에 대한 수학적 추론력을 강화한다. _ arXiv preprint arXiv:2308.09583_.\n' +
      '* Luo et al. (2023b) Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, and Wei Lin. 2023b. Chatkbqa: 미세 조정된 대규모 언어 모델을 사용하여 지식 기반 질문 응답을 위한 생성 후 검색 프레임워크.\n' +
      '* Luo et al. (2023c) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023c. Wizardcoder:evol-instruct로 코드 대언어 모델을 Empowering하는 단계; _ arXiv preprint arXiv:2306.08568_.\n' +
      '* Nan et al. (2020) Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutthia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022. Fetaqa : 자유형 테이블 질의응답 _ The Association for Computational Linguistics_, 10:35-49.\n' +
      '* Nan et al. (2023) Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xianru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Iwranto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazzenen Fatema Rajani. 2021. DART: 텍스트 생성에 대한 오픈 도메인 구조화 데이터 레코드. _Proceedings of the 2021 of the North American chapter of the Computational Linguistics Association: Human Language Technologies_, pages 432-447, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Parikh et al. (2020) Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: 제어된 테이블 대 텍스트 생성 데이터세트. In _Proceedings of EMNLP_.\n' +
      '* Pasupat and Liang(2015) Panupong Pasupat and Percy Liang. 2015. Semi-structured table에 대한 Compositional semantic parsing. _Proceedings of the 53th Annual Meeting of the Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1470-1480, China, Beijing. 컴퓨터 언어학과의 연관성\n' +
      '* Qi et al. (2022a) Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, Zhouhan Lin. 2022a. RASAT: 텍스트-to-sql을 위한 미리 훈련된 seq2seq 모델에 관계 구조를 통합한다. _EMNLP_, 페이지 3215-3229. Computational Linguistics Association.\n' +
      '* Qi et al. (2022b) Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, Zhouhan Lin. 2022b. Rasat: 텍스트-to-sql을 위한 미리 훈련된 seq2seq 모델에 관계 구조를 통합한다. _ arXiv preprint arXiv:2205.06983_.\n' +
      '* Qian et al. (2023) Cheng Qian, Chi Han, Yi R. 풍, 유지아 진, 지위안 류, 헝지. 2023. 크리에이터: 대규모 언어 모델의 추상적 추론과 구체적 추론을 해체하기 위한 도구 생성.\n' +
      '*진 등 (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. 툴림: 16000+ 실세계 아피스를 마스터하기 위해 대형 언어 모델을 용이하게 하는 것.\n' +
      '*Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Gratfaroti, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Tourvon, Louis Martin, Nicolas Usunier, Thomas Scialom and Gabriel Synnaeve. 2023. 코드 일라마: 코드에 대한 기초 모델을 개방한다. _ CoRR_, abs/2308.12950.\n' +
      '* Scholak et al. (2021) Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: 언어 모델들로부터 제약된 자동-회귀 디코딩을 위해 점진적으로 파싱하는 단계. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. 컴퓨터 언어학과의 연관성\n' +
      '* Shu et al. (2021) Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, and Rui Zhang. 2021. 시맨틱 파싱으로부터 논리-일관성 텍스트 생성. _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 4414-4426, Online. 컴퓨터 언어학과의 연관성\n' +
      '* Sun et al. (2022) Jiashuo Sun, Hang Zhang, Chen Lin, Yeyun Gong, Jian Guo, and Nan Duan. 2022. 아폴로: 긴 형태의 수치추론을 위한 최적화된 훈련 접근법 _ arXiv preprint arXiv:2212.07249_.\n' +
      '* Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. 웹은 복잡한 질문에 답하기 위한 지식 기반이다. [Proceedings of the 2018 of the North American chapter of the Computational Linguistics Association: Human Language Technologies, Volume 1(Long Papers)_]. 컴퓨터 언어학과의 연관성\n' +
      '* Tay et al. (2023a) Yi Tay, Mostafa Dehghani, Vinh Q. 트란, 하비에르 가르시아, 제이슨 웨이, 셰지 왕, 형원 정, 다라 바리, 탈 슈스터, 화익슈 스티븐 정, 데니 저우, 닐 홀스비, 도널드 메츨러 등이다. 2023a. UL2: 언어 학습 패러다임의 통합. _ICLR_에서. OpenReview.net.\n' +
      '* Tay et al. (2023b) Yi Tay, Mostafa Dehghani, Vinh Q. 트라, 사비에르 가르시아, 제이슨 웨이, 슈에지 왕, 형원 정, 시아막 샤케리, 다라 바리, 탈 슈스터, 화익슈 스티븐 정, 데니 저우, 닐 홀스비, 도널드 메츨러 등이다. 2023b. Ul2: 언어 학습 패러다임의 통합.\n' +
      '* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Likel, Lucas Blecher, Cristian Canton-Ferrer, Moya Bhosale, Dan Bikel, Lucas Blecher, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Cynthia Gao, Vedan Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inanc, Vindan Kardas, Viktor Kerkez, Madian Lavril, Yixin Nie, Andrew Michael Smith, Ranjan Subramaniam, Jeremy Reizenstein, Rashi Runga, Kalyan Zarov, Likan Larov, Lucan Larov, Aureyen Rodriguez, S. 2023. 라마 2: 오픈 파운데이션 및 파인 튜닝된 채팅 모델들_ CoRR_, abs/2307.09288.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. 자오, 켈빈 구, 아담스 웨이유, 브라이언 레스터, 난두, 앤드류 M. 다이, 콕 V 르 2022. Finetuned language models is zero-shot learners. _ICLR_에서. OpenReview.net.\n' +
      '*샤오 등(2022) Dongling Xiao, Linzheng Chai, Qian-Wen Zhang, Zhao Yan, Zhoujun Li, and Yunbo Cao. 2022. Cqr-sql: 대화 질문 재구성은 문맥 종속 텍스트-to-sql 파서를 향상시켰다.\n' +
      '* Xie et al. (2022) Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingnegong Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg: 텍스트-텍스트 언어 모델들로 통합 및 다중-태스킹 구조화된 지식 접지를 통합한다.\n' +
      '\n' +
      '_2022년 자연 언어 처리에서의 경험적 방법에 관한 회의의 진행례_.\n' +
      '*Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. 마법사: 복잡한 명령어를 따르도록 큰 언어 모델을 엠파워링하는 단계 _ arXiv preprint arXiv:2304.12244_.\n' +
      '* Xu et al. (2021) Kuan Xu, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. 2021. Sead: schema-aware denoising을 갖는 End-to-end text-to-sql 생성 arXiv preprint arXiv:2105.07911_.\n' +
      '* Ye et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. 대용량 언어 모델들은 다재다능한 분해자들이다: 테이블 기반 추론에 대한 증거 및 질문들을 분해한다. _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 174-184.\n' +
      '* Yu et al. (2020) Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Caiming Xiong, et al. 2020. Grappa: Grammar-augmented pre-training for table semantic parsing. _International Conference on Learning Representations_.\n' +
      '* Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. 스파이더: 복잡하고 교차 도메인 시맨틱 파싱 및 텍스트-대-sql 태스크를 위한 대규모 인간-라벨 데이터세트. [Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_] 컴퓨터 언어학과의 연관성\n' +
      '* Yu et al. (2019) Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrook Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. 2019. Sparc: context에서의 cross-domain semantic parsing. _Proceedings of the 57th Annual Meeting of the Computational Linguistics_. 컴퓨터 언어학과의 연관성\n' +
      '* Zhang et al. (2023) Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. 2023. Tablellama: 테이블용 대형 일반 모델을 엽니다.\n' +
      '* Zhao et al. (2022) Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. Multihiert: 다계층 표 및 텍스트 데이터에 대한 수치 추론. _ACL(1)_에서 페이지 6588-6600. Computational Linguistics Association.\n' +
      '* Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: 강화학습을 이용하여 자연어로부터 구조화된 질의를 생성하는 단계 _ CoRR_, abs/1709.00103.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c c c} \\hline \\hline  & Metric & 0\\% & 10\\% & 20\\% & 50\\% & 57\\% \\\\ \\hline \\multicolumn{6}{c}{Held-In Datasets} \\\\ \\hline TabMWP & Acc & 71.14 & 70.35 & 70.52 & 69.01 & 69.36 \\\\ ToTTo & BLEU & 49.78 & 49.51 & 49.47 & 49.31 & 49.38 \\\\ GrailQA & EM & 81.09 & 80.46 & 80.29 & 80.89 & 80.38 \\\\ SQL2Text & Blec & 95.07 & 94.39 & 94.49 & 94.97 & 93.81 \\\\ MMQA & F1 & 84.26 & 84.31 & 84.11 & 83.40 & 85.15 \\\\ Spider & EM & 72.92 & 71.57 & 73.40 & 72.73 & 72.44 \\\\ KVRet & All Micro & 71.60 & 73.90 & 70.34 & 72.25 & 72.61 \\\\ HybridQA & Acc & 59.23 & 59.09 & 59.09 & 59.03 & 59.17 \\\\ SParC & EM & 63.09 & 62.34 & 63.26 & 64.59 & 61.93 \\\\ CompWebQ & Acc & 80.61 & 79.15 & 78.76 & 78.73 & 78.34 \\\\ TabFact & Acc & 83.41 & 81.09 & 81.42 & 80.92 & 80.77 \\\\ WikiTQ & All Ex & 50.02 & 48.50 & 49.24 & 48.30 & 50.09 \\\\ WikiSQL & All Ex & 87.33 & 86.45 & 86.73 & 86.68 & 88.67 \\\\ FeTaQA & BLEU & 36.58 & 37.26 & 36.55 & 36.72 & 36.03 \\\\ Feverous & Acc & 85.02 & 84.13 & 84.11 & 83.73 & 84.41 \\\\ MultiWOZ & Joint Acc & 54.66 & 54.10 & 53.73 & 53.92 & 54.49 \\\\ Dart & BLEU & 61.38 & 61.89 & 61.08 & 62.24 & 62.24 \\\\ Logic2Text & Blec & 88.83 & 89.47 & 89.19 & 90.57 & 88.92 \\\\ MTOP & EM & 82.44 & 81.71 & 81.19 & 80.92 & 81.21 \\\\ \\hline \\multicolumn{6}{c}{Held-Out Datasets} \\\\ \\hline BIRD & Acc & 21.30 & 22.30 & 22.30 & 23.00 & 22.30 \\\\ CoSQL & EM & 51.24 & 49.95 & 50.84 & 50.74 & 49.75 \\\\ SQA & Acc & 49.02 & 46.03 & 43.11 & 48.39 & 49.72 \\\\ Infotabs & Acc & 38.00 & 56.26 & 57.87 & 62.35 & 62.46 \\\\ WikiTableText & BLEU & 14.78 & 13.51 & 6.66 & 7.27 & 8.27 \\\\ Finqa & Acc & 19.70 & 24.32 & 27.55 & 25.37 & 27.29 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 트레이닝 세트 내의 일반 데이터의 혼합물에 대한 절제 결과. 총 5개의 모델을 훈련하는데, 여기서 백분율은 일반적인 훈련 데이터의 백분율을 나타낸다. 보유된 데이터에서 FinQA 및 InfoTabs 데이터 세트에 대한 일반화 성능이 눈에 띄게 향상되었습니다. 특히, FinQA는 파이썬 실행 가능한 수학 식을 생성해야 하고 InfoTabs는 이전에 보이지 않은(부울) 옵션 3개와 정확하게 일치해야 한다. 위키표 텍스트 성능은 어려움을 겪는 것처럼 보이지만, 하나의 목표 문장으로 BLEU 점수를 기반으로 평가된다. 결과적으로, 우리는 훈련 데이터에서 보이지 않는 새로운 출력 사양에 대한 모델의 0-샷 적응 능력에 더 중점을 둔다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|l l l} \\hline \\hline Tasks & Metric & Code-LM & LLaMA & Math-LM \\\\ \\hline \\multicolumn{5}{c}{Held-In Datasets} \\\\ \\hline TabMWP & Acc & 71.14 & 62.96 & 66.5 \\\\ ToTTo & BLEU & 49.78 & 48.26 & 47.4 \\\\ GrailQA & EM & 81.09 & 75.72 & 77.66 \\\\ SQL2Text & Blec & 95.07 & 94.49 & 94.58 \\\\ MMQA & F1 & 84.26 & 83.96 & 82.13 \\\\ Spider & EM & 72.92 & 65.96 & 71.95 \\\\ KVRet & All Micro & 71.6 & 70.36 & 70.03 \\\\ HybridQA & Acc & 59.23 & 59.26 & 57.04 \\\\ SParC & EM & 63.09 & 56.94 & 60.35 \\\\ CompWebQ & Acc & 80.61 & 77.31 & 76.6 \\\\ TabFact & Acc & 83.41 & 80.46 & 79.47 \\\\ WikiTQ & All Ex & 50.02 & 45.6 & 46.89 \\\\ WikiSQL & All Ex & 87.33 & 83.93 & 85.49 \\\\ FeTaQA & BLEU & 36.58 & 34.37 & 34.1 \\\\ Feverous & Acc & 85.02 & 83.2 & 82.52 \\\\ MultiWOZ & Joint Acc & 54.66 & 55.43 & 53.79 \\\\ Dart & BLEU & 61.38 & 61.52 & 61.24 \\\\ Logic2Text & Blec & 88.83 & 88.0 & 90.38 \\\\ MTOP & EM & 82.44 & 77.18 & 75.56 \\\\ \\hline \\multicolumn{5}{c}{Held-Out Datasets} \\\\ \\hline BIRD & Acc & 21.3 & 15.9 & 18.8 \\\\ CoSQL & EM & 51.24 & 42.8 & 48.76 \\\\ SQA & Acc & 49.02 & 37.03 & 49.05 \\\\ Infotabs & Acc & 38.0 & 4.44 & 32.54 \\\\ WikiTableText & BLEU & 14.78 & 13.0 & 14.82 \\\\ Finqa & Acc & 19.7 & 6.63 & 21.53 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 상이한 베이스 모델들에서 수행된 미세조정을 비교한 평가 결과들. 코드는 Codellama-Instruct-7B를 지칭한다. 수학은 LLemma-7b를 의미한다. LLaMA는 LLaMA-2-7b를 지칭한다.\n' +
      '\n' +
      '[INST] <<SYS>> 당신은 구조화된 정보에 대한 분석과 추론을 전문으로 하는 AI 비서이다. 경우에 따라 일부 구조화된 지식 입력을 통해 작업이 주어집니다. 지정한 경우, 답변은 출력 형식을 엄격하게 준수해야 합니다. <</SYS>>{instruction}{input}[/INST]\n' +
      '\n' +
      '도 6: 모든 SKG 예들에 대한 프롬프트 포맷. 이러한 포맷팅 규약은 LLama2(Touvron et al., 2023)를 따른다. 입력은 임의의 다른 컨텍스트, 질문 또는 진술과 함께 선형화된 구조화된 데이터를 포함한다. 명령어는 작업을 지정합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>