<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# StructLM: Towards Building Generalist Models for\n' +
      '\n' +
      'Structured Knowledge Grounding\n' +
      '\n' +
      'Alex Zhuang\\({}^{1}\\), Ge Zhang\\({}^{1,2,7}\\),\n' +
      '\n' +
      '**Tianyu Zheng\\({}^{2}\\), Xinrun Du\\({}^{2}\\), Junjie Wang\\({}^{3,4}\\), Weiming Ren\\({}^{1,7}\\), Stephen W. Huang\\({}^{6}\\), Jie Fu\\({}^{2,4}\\), Xiang Yue\\({}^{5}\\), Wenhu Chen\\({}^{1,2,7}\\)**\n' +
      '\n' +
      '\\({}^{1}\\)University of Waterloo, \\({}^{2}\\)Multimodal Art Projection Research Community,\n' +
      '\n' +
      '\\({}^{3}\\)Waseda University, \\({}^{4}\\)HKUST, \\({}^{5}\\)Ohio State University, \\({}^{6}\\)harmony.ai, \\({}^{7}\\)Vector Institute\n' +
      '\n' +
      '[https://tiger-ai-lab.github.io/StructLM/](https://tiger-ai-lab.github.io/StructLM/)\n' +
      '\n' +
      'Corresponding Author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Structured data sources, such as tables, graphs, and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of large language models (LLMs) on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in LLMs\' ability to process structured data, e.g., Chat-GPT lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a comprehensive instruction tuning dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalization across 6 novel SKG tasks. Contrary to expectations, we observe that scaling model size offers marginal benefits, with StructLM-34B showing only slight improvements over StructLM-7B. This suggests that structured knowledge grounding is still a challenging task and requires more innovative design to push to a new level.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Traditionally, users need to write programs to interface with structured data like tables, databases, knowledge graphs, etc. It requires the users to master the domain-specific language like SQL, SPARQL, etc. Recently, researchers have explored the possibility of automating the interface with natural language to enable potential use cases in question-answering (Pasupat and Liang, 2015; Zhong et al., 2017; Nan et al., 2022), summarization (Parikh et al., 2020; Nan et al., 2021; Bao et al., 2018), and fact verification (Aly et al., 2021; Chen et al., 2019; Gupta et al., 2020), among others, all grounded to a structured knowledge source. This effort can lower the barrier for end users to access massive amount of structured data.\n' +
      '\n' +
      'The previous work (Yu et al., 2020; Liu et al., 2021; Xie et al., 2022; Zhang et al., 2023) has been mostly focused on building task-specific models for different tasks with rather limited generalization ability. Building a generalist structure knowledge grounding (SKG) system across a wide range of tasks proves to be challenging. This is mainly due to the heterogeneity of data format and use cases. We evaluated GPT-3.5-Turbo (Jiang et al., 2023) on 18 SKG tasks and observed that its performance\n' +
      '\n' +
      'Figure 1: StructLM can ground on structured and unstructured knowledge to respond to human queries. The previous SoTA was attained by many different task-specific models like TAPEX (Liu et al., 2021), USKG (Xie et al., 2022), TableLAMA (Zhang et al., 2023), BINDER-Codex (Cheng et al., 2022), etc. StructLM (a single model) beats the previous SoTAs on seven out of eighteen SKG tasks.\n' +
      '\n' +
      'is on average 35% lower than the SoTA specialized models. It shows that the LLM\'s ability on SKG is heavily overlooked during the pre-training phase.\n' +
      '\n' +
      'In this paper, we explore the possibility of building a generalist model based on LLMs that can ground on diverse types of structure and unstructured knowledge to interface with humans. Specifically, we construct a large data set of over a million instruction-following examples, a majority of which is SKG data, along with additional general instruction-following data, which we find improves generalizability. We fine-tune models at three scales: 7B, 13B, and 34B, based on the Code-LLaMA family of code foundation models. When compared to USKG, we find that our 7B model surpasses these single-task models from on \\(11\\) of \\(18\\) tasks with our 34B model outperforming on \\(14\\) of \\(18\\). As depicted in Figure 1, StructLM achieves SoTA on \\(7\\) out of 18 evaluated tasks, beating Chat-GPT by a huge margin.\n' +
      '\n' +
      'We study the performance of StructLM, namely whether the model experiences cross-task generalization benefits from the dataset mixture, and find that our multi-task model performs significantly better overall than single-task models of the exact same parameter scale. We also study the effect of different pretraining data on our finetuned performance to determine whether special pretraining regimes, such as code or math, contribute to effective SKG reasoning ability. We find that code pretraining is the most effective. We perform additional ablations to confirm our results and support our claims. Our contributions are:\n' +
      '\n' +
      '* We construct a large SKG instruction-tuning dataset with \\(1.1\\) million samples. We train and release our 3 models that outperform the previous 3B USKG fine-tuned on individual tasks on a total of \\(14\\) of \\(18\\) tasks. StructLM also achieves SoTA results on \\(7\\) of them.\n' +
      '* We show that StructLM is able to show strong zero-shot generalization capability on unseen structure knowledge grounding tasks, which was not shown by previous models.\n' +
      '* We find that scaling general instruction-tuning data improves generalization ability, and that code-pretrained base models indeed improve model performance on the SKG tasks.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Solving SKG tasks\n' +
      '\n' +
      'Structured knowledge, such as web tables, knowledge graphs, and databases, have long been the subject of study in knowledge grounding. However, SKG tasks have heterogeneous data formats which have inspired methods that leverage specific training setups to learn those representations. For example, PTab Liu et al. (2022) and MultiHiertt Zhao et al. (2022) learn the contextual\n' +
      '\n' +
      'Figure 2: Overview of StructLM. This figure illustrates the prompting structure of StructLM, highlighting its capability to process various forms of structured data beyond linearized data tables, including linearized database schemas and knowledge graphs. StructLM is also assessed on held-out tasks that bear similarity to groups of held-in tasks, but also differences that must be overcome.\n' +
      '\n' +
      'representation of tabular data by incorporating semantic information through specific training methods or reasoning approaches. RASAT Qi et al. (2022) integrates relation-aware self-attention with the Transformer seq2seq architecture and utilizes various relational structures to address SQL problems. TAPEX Liu et al. (2021) conducts pretraining over tabular/database data with the help of an SQL executor to provide supervision.\n' +
      '\n' +
      'More recently, methods have begun to move away from these auxiliary task-specific structures. USKG Xie et al. (2022) were the first to unify many SKG tasks into a sequence-to-sequence format, allowing them to be aggregated into the same data mixture. StructGPT Jiang et al. (2023) represents a line of work that uses prompting frameworks on powerful LLMs to solve tasks with more robustness and accuracy. In contrast, our work examines open models and tries to assess their fundamental capabilities. TableLlama Zhang et al. (2023) is similar to our work but focuses only on tabular data, which we consider a subset of SKG.\n' +
      '\n' +
      'USKG showed the benefit of the sequence-to-sequence format unification in more powerful language models, however, it was not able to show strong advantages to constructing a multi-task mix of SKG data over single-task models or task-specific training methods such as prefix-tuning in terms of performance. We seek to address this gap with our construction of an instruction-tuning SKG dataset with a large subset of USKG\'s tasks, and the resulting model, StructLM, which does show strong multi-task performance.\n' +
      '\n' +
      '### LLMs with Instruction Tuning\n' +
      '\n' +
      'Instruction-tuning (IT) has been popularized as a method to address the gap between training objectives and user goals in LLMs. This technique involves additional training of LLMs using pairs of instructions and outputs. IT enhances both the controllability and the predictability of the models, aligning them more closely with user expectations. Furthermore, recent studies such as FLAN Wei et al. (2022), UL2 Tay et al. (2023), and LLAMA2 Touvron et al. (2023) have shown that IT can improve the performance of downstream tasks through multi-task learning across diverse data types. While FLAN-UL2 trains on a subset of 11 tasks from USKG, it also trains on many more unrelated language tasks. In our work, by focusing on SKG data, we hope to provide a focused study that can act as a reference for future work to improve performance on this task type.\n' +
      '\n' +
      '### Reasoning Capability in LLMs\n' +
      '\n' +
      'Reasoning stands as a pivotal skill for LLMs in the development of real-world AI applications which would enable the autonomous completion of many thought-intensive tasks viewed traditionally to require human thinking, like programming or mathematical problem-solving Li et al. (2022). Recent studies Li et al. (2022, 2023); Roziere et al. (2023); Azerbayev et al. (2023) indicate that LLMs trained on code and mathematical datasets exhibit profound reasoning skills, and can even achieve performance on par with human levels. For example, Code-Llama Roziere et al. (2023), a foundation model trained on more programming data, has significantly improved reasoning capabilities across a variety of programming and mathematical benchmarks. Furthermore, LLemma Azerbayev et al. (2023) continues to pretrain the Code-Llama model on a mix of scientific papers, math-related web data, and mathematical code. Its results show excellent reasoning capabilities on the MATH benchmark Hendrycks et al. (2021) and the ability to prove theorems without further fine-tuning. On the fine-tuning side, WizardMath Luo et al. (2023), and WizardCoder Luo et al. (2023) have shown the effectiveness of instruction tuning on reasoning capabilities, given high quality data.\n' +
      '\n' +
      'In this work, we view structured data as a third testbed for a different kind of reasoning within LLMs. We posit that in addition to mathematical or logical reasoning, the ability to recognize and make use of patterns within a structured input indicates that a model has robust representations of relationships in data. These representations may serve as a strong prior for further reasoning downstream.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Dataset Curation\n' +
      '\n' +
      'Motivated by the goal of training a language model (LM) generally capable of a wide range of structured data tasks, we select a total of \\(25\\) SKG tasks to study. We report results on \\(18\\) held-in and \\(6\\) held-out tasks, where each held-out task meant to roughly evaluate the generalization capability of a held-in task group. In total, our held-in training dataset contains approximately \\(700\\)k SKG examples. We describe the held-in dataset groups below.\n' +
      '\n' +
      '**Data to Text Generation**. This group of datasetsdeals with the summarization or interpretation of structured data from tables to knowledge triples to formal languages. Their inclusion is motivated by the idea that useful LMs should be able to make sense of a wide variety of structured information and map it to meaning in natural language. The corresponding held-out dataset for this task group is intended to be WikiTableText.\n' +
      '\n' +
      '**Table based Question Answering.** This group of datasets deals specifically with tabular data, optionally combined with text passages. LMs which are able to accurately answer questions and retrieve information from tables can be widely useful as assistants. The corresponding held-out dataset for this task group is SQA.\n' +
      '\n' +
      '**Knowledge-grounded Conversations**. This group of tasks evaluates knowledge grounding conversation. Humans naturally interface with LMs is through chat, and enabling this capability can lower the barrier to accessing the information in stored structured data. These tasks track user intention through provided dialogue and ask the model to provide an answer to the latest question. The held-out dataset for this task group is CoSQL.\n' +
      '\n' +
      '**Fact verification**. One common use case for tables is to reference facts. In addition to question answering, the ability to reliably determine if data in a table supports a statement signals the existence of a robust representation of the table\'s data. The held-out dataset for this task group is InfoTabs.\n' +
      '\n' +
      '**SQL or domain-specific languages** SQL is the language most commonly used to interface with structured data today. Understanding how to write SQL also requires understanding of abstractions of tables and how they are linked together. In other domain-specific languages, the MTOP task measures a model\'s ability to parse a specification and generate an API call, which sees potential in LLM tool use (e.g., (Qin et al., 2023)). The corresponding held-out dataset for this task group is intended to be BIRD (Li et al., 2023b), which further tests SQL generation abilities.\n' +
      '\n' +
      '**Mathematical reasoning**. An analysis of tabular data may also require performing quick mathematical computations over their contents. Performance on these datasets tells us how well models can combine both structured knowledge and mathematical reasoning. As there are currently a limited number of datasets that combine mathematical reasoning with SKG, this category includes just TabMWP in the held-in corpus. We set FinQA as a challenging held-out dataset analogue. Not only does it require financial domain knowledge, but it combines tabular information with long text passages, and requires the generation of mathematical code.\n' +
      '\n' +
      '**General instruction data**. In addition to the SKG datasets within the held-in dataset mixture, we also included general instruction tuning data without any structured knowledge component, to maintain the instruction-following ability of our model. We use SlimOrca (Lian et al., 2023), which is constructed from cleaned GPT-4 responses to a number of prompts from existing general large-scale instruction-following datasets. We detect no signs of data contamination for our held-out datasets based on our ablation results. We give a detailed overview of all dataset statistics in Table 1.\n' +
      '\n' +
      '### Instruction Finetuning Approach\n' +
      '\n' +
      'To instruction tune our model, each example in our dataset consists of a system prompt, instruction, input, and output. For all SKG data examples, we use the same system prompt. For each dataset, we write 10 instruction variations, which are randomized when constructing the training samples. For SKG data, the input is composed of a combination of a structured knowledge input and accompanying text that could be a question, statement, or anything that would be required to specify the task.\n' +
      '\n' +
      'Figure 3: Breakdown of Structured Knowledge Types and Tasks. On the inside, we see a coarse breakdown of the different categories of structured inputs in our dataset. On the outside, we see the tasks that are represented for those structured knowledge types. We also have a considerable proportion of general instruction following data from SlimOrca (Lian et al., 2023).\n' +
      '\n' +
      'The exact prompt format is provided in Figure 6.\n' +
      '\n' +
      '### Training and Evaluation Details\n' +
      '\n' +
      'The base models for StructLM are the CodeLlama-Instruct family of models (Roziere et al., 2023). We finetune all models with a batch size of \\(512\\) for \\(3\\) epochs on A800 gpus. This training setup is largely in line with community conventions, such as the settings used for the WizardLM (Xu et al., 2023), WizardMath (Luo et al., 2023), and WizardCoder (Luo et al., 2023) models.\n' +
      '\n' +
      'We follow the structured data linearization conventions in USKG (Xie et al., 2022). However, we use a different truncation scheme as described below. During training, we maintain a maximum sequence length of \\(2048\\). USKG notes that training and running inference with longer context is beneficial. As such, when truncating, we consider the combined token length of the prompt input and output label. We truncate only the structured knowledge portion of the input so that the example becomes at most \\(2048\\) tokens long. As shown in the dataset statistics in Table 1, setting the max token length of the examples in our dataset to \\(2048\\) allows nearly all examples to fit within the context window with rare truncations. We discard examples for which even this structured input truncation is insufficient (e.g. the output is too long). During inference, we set the input token length to \\(2048\\), to allow even more structured information to be placed within the input context. We set the maximum generation length to 1024, which is sufficient for all correct responses in all datasets. For each model, including our single-task finetuned models, we choose the best performing checkpoint of the \\(3\\) checkpoints produced at the end of each epoch.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'BaselinesFirstly, to illustrate the current performance of language models on SKG tasks, we evaluate ChatGPT (GPT-3.5-turbo) and the base model CodeLlama-7B-Instruct under a 1-shot setting. Our prompting scheme, using the same linearized knowledge structures as in our held-in training, sees them struggle across the board with many tasks due to the unseen structure knowledge format. Although ChatGPT is superior on text-based tasks, its performance is lackluster on SKG tasks. Its gap\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r|r r|r r r r|r r r} \\hline \\hline  & \\multicolumn{2}{c|}{Overall Length} & \\multicolumn{4}{c|}{Train} & \\multicolumn{4}{c}{Test} \\\\ \\hline \\multirow{2}{*}{Dataset} & Input & Output & \\multirow{2}{*}{Count} & \\multicolumn{2}{c}{Input} & \\multicolumn{2}{c}{Output} & \\multirow{2}{*}{\\# trunc.} & \\multirow{2}{*}{Count} & \\multicolumn{2}{c}{Input} & \\multirow{2}{*}{Output} \\\\  & (avg) & & & & & & & & & \\\\ \\hline TabMWP & 207.8 & 4.5 & 23059 & 709 & 33 & 0 & 7686 & 703 & 31 & 0 \\\\ ToTTo & 251.8 & 31.0 & 120761 & 2040 & 155 & 467 & 7700 & 2048 & 119 & 31 \\\\ GrailQA & 281.0 & 44.1 & 44337 & 884 & 134 & 0 & 6463 & 546 & 123 & 0 \\\\ SQL2Text & 122.3 & 18.1 & 5600 & 337 & 61 & 0 & 1034 & 245 & 38 & 0 \\\\ MMQA & 656.2 & 7.7 & 15688 & 2047 & 146 & 234 & 1501 & 2048 & 94 & 11 \\\\ Spider & 266.6 & 36.0 & 7000 & 1369 & 226 & 0 & 1034 & 453 & 146 & 0 \\\\ KVRet & 573.4 & 17.1 & 6288 & 1217 & 161 & 0 & 807 & 1147 & 82 & 0 \\\\ HybridQA & 700.4 & 6.8 & 62682 & 2047 & 91 & 200 & 3466 & 2048 & 79 & 6 \\\\ SParC & 276.3 & 32.6 & 12059 & 1417 & 226 & 0 & 1625 & 467 & 146 & 0 \\\\ CompWebQ & 1350.3 & 11.9 & 27639 & 2047 & 321 & 321 & 2816 & 2048 & 256 & 8 \\\\ TabFact & 660.1 & 4.6 & 92283 & 2045 & 5 & 2 & 12779 & 1687 & 4 & 0 \\\\ WikiTQ & 831.8 & 5.8 & 11321 & 2028 & 273 & 0 & 4344 & 2048 & 148 & 10 \\\\ WikiSQL & 689.2 & 7.1 & 56355 & 2047 & 518 & 16 & 15878 & 2048 & 244 & 1 \\\\ FeTaQA & 653.2 & 38.8 & 7326 & 1853 & 158 & 0 & 2003 & 1548 & 114 & 0 \\\\ FEVEROUS & 799.3 & 3.4 & 40669 & 2047 & 5 & 2052 & 4285 & 2048 & 4 & 195 \\\\ MultiWOZ & 777.2 & 154.5 & 56668 & 1656 & 196 & 0 & 7368 & 1344 & 185 & 0 \\\\ DART & 133.7 & 30.3 & 62659 & 406 & 258 & 0 & 5097 & 261 & 109 & 0 \\\\ Logic2Text & 166.1 & 26.9 & 8566 & 358 & 67 & 0 & 1092 & 347 & 60 & 0 \\\\ MTOP & 961.0 & 34.4 & 15667 & 1002 & 215 & 0 & 4386 & 990 & 113 & 0 \\\\ SlimOrca & 278.9 & 152.4 & 512069 & 2047 & 1808 & 0 & - & - & - \\\\ \\hline BIRD & 439.8 & 63.3 & 9428 & 1992 & 347 & 99 & 1534 & 1214 & 386 & 0 \\\\ CoSQL & 287.4 & 34.9 & 9502 & 1640 & 226 & 0 & 1300 & 535 & 190 & 0 \\\\ SQA & 656.9 & 34.9 & 12275 & 1812 & 1012 & 2 & 3011 & 1725 & 769 & 0 \\\\ Infotabs & 276.9 & 3.7 & 16538 & 1009 & 5 & 0 & 5400 & 1105 & 4 & 0 \\\\ WikiTableText & 149.6 & 27.4 & 10000 & 313 & 97 & 0 & 2000 & 226 & 89 & 0 \\\\ Finqa & 1230.3 & 21.0 & 6251 & 2040 & 72 & 186 & 1147 & 2048 & 61 & 25 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Token sequence length statistics for each dataset in our train and test sets. Input and output statistics are in tokens. We report the number of examples which have been truncated in each dataset.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      'Held-out ResultsOn held out tasks, StructLM shows strong generalization performance, outperforming ChatGPT on 5 of 6 tasks. We note that our zero-shot prompting method for the FinQA [3] dataset requires the model to generate custom code and is focused only on the financial domain, and the input to Infotabs [11] is a different type of table structure unseen in the input data, with different labels for the boolean output categories than the choices from the held-in fact verification tasks. This generalization capability is non-existent in USKG models as the name of each held-in task is specified in their training inputs. When compared to TableLLaMA, StructLM also generalizes much better.\n' +
      '\n' +
      '## 5 Ablation Studies\n' +
      '\n' +
      '**Effect of base model pretraining data**. We ablate our choice of base model, Codellama-7b-Instruct, by finetuning the unspecialized Llama2-7b base model and Llemma, which is further pretrained on mathematical texts [1]. Intuitively, one might guess that coding ability has the most transferability to performance on the types of SKG tasks we are studying due to the symbolic nature of programming languages and code-writing scenarios. However, it is possible that other types of pretraining to boost reasoning ability, such as math, have even greater transferability.\n' +
      '\n' +
      'Our ablation results in Table 6 can be broken down into groupings of tasks, as in Figure 4. Models pretrained on code indeed perform slightly\n' +
      '\n' +
      'Figure 4: Effect of different pretraining curricula on SKG finetuning performance in relevant task groupings.\n' +
      '\n' +
      'Figure 5: Effect of general instruction-following data on held-out SKG dataset performance. Performance is measured as the average over evaluation metrics across all tasks within held-in or held-out groups. Note that the held-in performance experiences a milder dip compared to the held-out performance gains.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c} \\hline \\hline Purpose & Train & Eval & FT & Result \\\\ \\hline \\hline Schema task transfer & Spider, SParC, Logic2Text & Logic2Text & 89.47 & 89.93 \\\\ \\hline KT task transfer & CompWebQ, WebQSP, GrailQa, Dart & Dart & 60.28 & 60.34 \\\\ \\hline \\multirow{3}{*}{Table task transfer} & FetaQA, HybridQA, WikiTQ, TabMWP, ToTTo, MMQA, WikiSQL, KVRet, Tab Fact, Feverous, Infotabs & TabFact, Feverous & \\multirow{3}{*}{75.46} & \\multirow{3}{*}{80.81} \\\\  & Feverous, Infotabs & & & \\\\ \\hline Summ. data type transfer & ToTTo, Dart & Dart & 60.28 & 61.42 \\\\ \\hline QA data type transfer & CompWebQ, WikiSQL & WikiSQL & 85.49 & 86.36 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Cross task and cross datatype transfer results. FT is an average of single-task performance over the datasets in the Eval column.\n' +
      '\n' +
      'better, and these gains are not necessarily limited to tasks which explicitly involve a grammatically regular input, or require the generation of code. Math pretraining does seem to improve the performance of the Llama2 base model, but not by as much as code pretraining. Overall, it seems that code pretraining is a useful step in training a performant model in this SKG setting, which may be due to conceptual similarity on certain tasks.\n' +
      '\n' +
      '**Effect of general instruction data in mixture** We ablate our inclusion of approximately \\(500\\)k general instruction-following examples into the training data mixture to observe the importance of including this data. As we see in Figure 5, the held-in performance is relatively unaffected by the added general examples, but held-out performance improves significantly with more general data. Empirically, we also observe that when training a large volume of task-specific input and output formats, the model becomes less capable of following instructions on new tasks in a zero-shot setting. We hypothesize that training on this general mixture helps zero-shot performance because it can reduce overfitting to the task formats in the training set.\n' +
      '\n' +
      '**Cross-task and cross-format transferability** We ablate the transferability of performance between input structure knowledge types and between output task types. To test this, we train a number of tasks together and compare them to their single-task finetuned models. Our results are indicative that there is cross-task transferability of performance occurring. In schema and knowledge triples, training on different tasks seems to weakly improve the performance of the evaluation task compared to finetuning on only that evaluation task. On tables, we see that this effect is much greater. One explanation for this is that there are a wide variety of table tasks included in the training mix, and the volume of auxiliary data is larger than the other data types.\n' +
      '\n' +
      'On the other hand, we see that finetuning on different datatypes with the same task (i.e. summarization) also yields benefits to performance. On the summarization and question-answering (QA) experiments, we train on both tabular and knowledge graph data. We evaluate summarization with Dart and QA with WikiSQL. We see that even though we have only added one extra dataset in each case, we still see an improved result compared to the single-task model even though these datasets are have very different knowledge types. Thus, overall evidence supports that the performance of tasks within our training set are benefiting from each other, even across data types.\n' +
      '\n' +
      '## 6 Discussion\n' +
      '\n' +
      'We argue that SKG is an important capability for future language models. We have seen through our experiments on ChatGPT and the LLama2 family that there is significant room for improvement.\n' +
      '\n' +
      'We found that we could produce a strong model by focused instruction-tuning on SKG tasks, however, we also observe that the performance difference between 7B to 34B StructLM models was not dramatic. This raises a concern about the state of SKG data: could we be approaching a performance ceiling? Combined with the fact that we were able to outperform UL2-20b, a much larger model, with our 7B model on 3 tasks, it seems that LLMs at various scales are struggling with SKG capabilities.\n' +
      '\n' +
      'Indeed, grounding to structured knowledge directly in a model\'s input represents a challenge in reasoning and input sensitivity. However, it has a wide range of potential benefits. To meaningfully improve SKG capability, we propose that future work may explore continued pretraining of open foundation models on more structured data formats. Similar to current attempts at code or math pretraining, it is possible that pretraining models on text interleaved with tables or other types of regular data formatting will help us move towards establishing SKG as a foundational model capability.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In this paper, we explore the current capabilities of open language models on structured knowledge grounding tasks. We show that LLMs are currently weak at SKG tasks currently. To address this gap, we construct an instruction-tuning dataset mixture of 1.1M examples and release models that outperform USKG on a majority (\\(14\\) of \\(18\\)) tasks and achieve SOTA on \\(7\\) of them. We also study the effects of various factors that influence the performance of our model on these task types. We hope that our work provides an updated understanding of what is achievable in the SKG domain, and can serve as a strong baseline for future improvements.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'The collection process used to construct the training data for StructLM tries to include a wide a variety of data types. As we have seen, there is evidence that this diversity is capable of affording transferable benefits to each dataset in the mixture. However, the tasks that we train and evaluate on are still academic datasets which have each been curated and designed for a specific purpose. While these are indeed diverse, the SKG domain relies on specific formatting and prompting conventions, which may result in our models having unnecessary specificity towards the conventions within our train set. To develop a clearer picture of how SKG performs as its own domain, we may require larger scale datasets with more heterogeneous formatting conventions. Further opportunities for training more robust SKG models may lie in increasing the diversity of structured data types in this way.\n' +
      '\n' +
      'Additionally, while we have tried to evaluate our models to the best of our ability, many of the tasks of our held-out datasets measure accuracy through a heuristic matching step of a model\'s output. In zero or few-shot settings, it is quite challenging to exactly control the generations of an autoregressive transformer to be adherent to a certain rule or grammar, and this has been a subject of study in other works (i.e. PICARD (Scholak et al., 2021)). We note that because of this reality, poor results in zero or few-shot context may betray the existence of useful representations that the model has already learned. Without further prompting or finetuning efforts, it may be difficult to bring these capabilities to light. As such, another opportunity for improvement upon our methods may involve more flexible constrained methods of language model evaluation.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'This paper does not raise any ethical concerns. The data and additional resources employed in this study are open-source and widely utilized in existing works.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Aly et al. (2021) Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact extraction and VERification over unstructured and structured information.\n' +
      '* Azerbayev et al. (2023a) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023a. Llemma: An open language model for mathematics. _CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023b) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023b. Llemma: An open language model for mathematics. _CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023c) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023c. Llemma: An open language model for mathematics. _CoRR_, abs/2310.10631.\n' +
      '* Azerbayev et al. (2023d) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023d. Llemma: An open language model for mathematics. _CoRR_, abs/2310.10631.\n' +
      '* Baro et al. (2018) Junwei Baro, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-to-text: Describing table region with natural language. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1).\n' +
      '* a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 5016-5026, Brussels, Belgium. Association for Computational Linguistics.\n' +
      '* Chen et al. (2019) Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019. Tabfact: A large-scale dataset for table-based fact verification. In _International Conference on Learning Representations_.\n' +
      '* Chen et al. (2020a) Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2020a. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. _Findings of EMNLP 2020_.\n' +
      '* Chen et al. (2021) Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. Finqa: A dataset of numerical reasoning over financial data. _Proceedings of EMNLP 2021_.\n' +
      '* Chen et al. (2020b) Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, and William Yang Wang. 2020b. Logic2Text: High-fidelity natural language generation from logical forms. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 2096-2111, Online. Association for Computational Linguistics.\n' +
      '* Cheng et al. (2022) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, et al. 2022. Binding language models in symbolic languages. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Cive et al. (2021) Jordan Cive, Kris Cao, and Marek Rei. 2021. Control prefixes for parameter-efficient text generation. _arXiv preprint arXiv:2110.08329_.\n' +
      '* Dai et al. (2021) Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, and Xiaodan Zhu. 2021. Preview, attend and review: Schema-aware curriculum learning for multi-domain dialog state tracking.\n' +
      '* Eric et al. (2017) Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. 2017. Key-value retrieval networks for task-oriented dialogue. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_. Association for Computational Linguistics.\n' +
      '* Gu et al. (2023) Yu Gu, Xiang Deng, and Yu Su. 2023. Don\'t generate, discriminate: A proposal for grounding language models to real-world environments.\n' +
      '* Gu et al. (2021) Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. Beyond iid: three levels of generalization for question answering on knowledge bases. In _Proceedings of the Web Conference 2021_, pages 3477-3488. ACM.\n' +
      '\n' +
      'Deepak Gupta, Surabhi Kumari, Asif Ekbal, and Pushpak Bhattacharyya. 2018. MMQA: A multi-domain multi-lingual question-answering framework for English and Hindi. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan. European Language Resources Association (ELRA).\n' +
      '* Gupta et al. (2020a) Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020a. Infotabs: Inference on tables as semi-structured data. _arXiv preprint arXiv:2005.06117_.\n' +
      '* Gupta et al. (2020b) Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020b. INFOTABS: Inference on tables as semi-structured data. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2309-2324, Online. Association for Computational Linguistics.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset.\n' +
      '* Iyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1821-1831, Vancouver, Canada. Association for Computational Linguistics.\n' +
      '* Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. 2023. StructGPT: A General Framework for Large Language Model to Reason over Structured Data. ArXiv:2305.09645 [cs].\n' +
      '* Lee et al. (2023) Sung-Min Lee, Eunhwan Park, Daeryong Seo, Donghyeon Jeon, Inho Kang, and Seung-Hoon Na. 2023. MAFiD: Moving average equipped fusion-in-decoder for question answering over tabular and textual data. In _Findings of the Association for Computational Linguistics: EACL 2023_, pages 2337-2344, Dubrovnik, Croatia. Association for Computational Linguistics.\n' +
      '* Li et al. (2021) Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 2950-2962, Online. Association for Computational Linguistics.\n' +
      '* Li et al. (2023a) Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. 2023a. ResdSql: Decoupling schema linking and skeleton parsing for text-to-sql. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 13067-13075.\n' +
      '* Li et al. (2023b) Jinyang Li, Bingyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Baiilin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023b. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqlS.\n' +
      '* Li et al. (2023) Raymond Li, Loughna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koccetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zhehtonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishib Davaadj, Joel Lamy-Poiter, Joao Monteiro, Oleh Shliazhko, Nicolas Gantier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhasham Oblokulov, Zhiruo Wang, Rota Murthy V, Jason Stillerman, Siva Sankal Patel, Dmitry Aublinanov, Marco Zocca, Maneu Dey, Zhihan Zhang, Nour Moustaaf-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferranidis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Wern, and Harm de Vries. 2023c. Starcoder: may the source we with you! _CoRR_, abs/2305.06161.\n' +
      '* Li et al. (2024) Shujie Li, Liang Li, Ruying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, and Yongbin Li. 2024. Unifying structured data as graph for data-to-text pre-training. _ArXiv_, abs/2401.01183.\n' +
      '* Li et al. (2022) Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d\'Autume, Igor Babuschkin, Xinjun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cheredanyo, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. _CoRR_, abs/2203.07814.\n' +
      '* Lian et al. (2023) Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification.\n' +
      '* Liu et al. (2022) Guang Liu, Jie Yang, and Ledell Wu. 2022. Ptab: Using the pre-trained language model for modeling tabular data. _CoRR_, abs/2209.08060.\n' +
      '* Liu et al. (2021) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2021. Tapex: Table pre-training via learning a neural sql executor.\n' +
      '* Lu et al. (2023) Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In _International Conference on Learning Representations (ICLR)_.\n' +
      '* Luo et al. (2023a) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jiangguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shiffeng Chen, and Dongmeng Zhang. 2023a. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_.\n' +
      '* Luo et al. (2023b) Haoran Luo, Haihong E, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai Zhang, Chenghao Ma, Guanting Dong, Meina Song, and Wei Lin. 2023b. Chatkbqa: A generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models.\n' +
      '* Luo et al. (2023c) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023c. Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* Nan et al. (2020) Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutthia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru,Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022. Fetaqa: Free-form table question answering. _Transactions of the Association for Computational Linguistics_, 10:35-49.\n' +
      '* Nan et al. (2023) Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Iwranto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazzenen Fatema Rajani. 2021. DART: Open-domain structured data record to text generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 432-447, Online. Association for Computational Linguistics.\n' +
      '* Parikh et al. (2020) Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to-text generation dataset. In _Proceedings of EMNLP_.\n' +
      '* Pasupat and Liang (2015) Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In _Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 1470-1480, Beijing, China. Association for Computational Linguistics.\n' +
      '* Qi et al. (2022a) Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. 2022a. RASAT: integrating relational structures into pretrained seq2seq model for text-to-sql. In _EMNLP_, pages 3215-3229. Association for Computational Linguistics.\n' +
      '* Qi et al. (2022b) Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin. 2022b. Rasat: Integrating relational structures into pretrained seq2seq model for text-to-sql. _arXiv preprint arXiv:2205.06983_.\n' +
      '* Qian et al. (2023) Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models.\n' +
      '* Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToollIM: Facilitating large language models to master 16000+ real-world apis.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Gratfaroti, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Tourvon, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code Ilama: Open foundation models for code. _CoRR_, abs/2308.12950.\n' +
      '* Scholak et al. (2021) Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. 2021. Picard: Parsing incrementally for constrained auto-regressive decoding from language models. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics.\n' +
      '* Shu et al. (2021) Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, and Rui Zhang. 2021. Logic-consistency text generation from semantic parses. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 4414-4426, Online. Association for Computational Linguistics.\n' +
      '* Sun et al. (2022) Jiashuo Sun, Hang Zhang, Chen Lin, Yeyun Gong, Jian Guo, and Nan Duan. 2022. Apollo: An optimized training approach for long-form numerical reasoning. _arXiv preprint arXiv:2212.07249_.\n' +
      '* Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. Association for Computational Linguistics.\n' +
      '* Tay et al. (2023a) Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023a. UL2: unifying language learning paradigms. In _ICLR_. OpenReview.net.\n' +
      '* Tay et al. (2023b) Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023b. Ul2: Unifying language learning paradigms.\n' +
      '* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inanc, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kolumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenny Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Runga, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramaniam, Xiaqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. _CoRR_, abs/2307.09288.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned language models are zero-shot learners. In _ICLR_. OpenReview.net.\n' +
      '* Xiao et al. (2022) Dongling Xiao, Linzheng Chai, Qian-Wen Zhang, Zhao Yan, Zhoujun Li, and Yunbo Cao. 2022. Cqr-sql: Conversational question reformulation enhanced context-dependent text-to-sql parsers.\n' +
      '* Xie et al. (2022) Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingnegong Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models.\n' +
      '\n' +
      '_Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_.\n' +
      '* Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_.\n' +
      '* Xu et al. (2021) Kuan Xu, Yongbo Wang, Yongliang Wang, Zujie Wen, and Yang Dong. 2021. Sead: End-to-end text-to-sql generation with schema-aware denoising. _arXiv preprint arXiv:2105.07911_.\n' +
      '* Ye et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 174-184.\n' +
      '* Yu et al. (2020) Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Caiming Xiong, et al. 2020. Grappa: Grammar-augmented pre-training for table semantic parsing. In _International Conference on Learning Representations_.\n' +
      '* Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics.\n' +
      '* Yu et al. (2019) Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrook Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard Socher, and Dragomir Radev. 2019. Sparc: Cross-domain semantic parsing in context. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_. Association for Computational Linguistics.\n' +
      '* Zhang et al. (2023) Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. 2023. Tablellama: Towards open large generalist models for tables.\n' +
      '* Zhao et al. (2022) Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. Multihiert: Numerical reasoning over multi hierarchical tabular and textual data. In _ACL (1)_, pages 6588-6600. Association for Computational Linguistics.\n' +
      '* Zhong et al. (2017) Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. _CoRR_, abs/1709.00103.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c c c} \\hline \\hline  & Metric & 0\\% & 10\\% & 20\\% & 50\\% & 57\\% \\\\ \\hline \\multicolumn{6}{c}{Held-In Datasets} \\\\ \\hline TabMWP & Acc & 71.14 & 70.35 & 70.52 & 69.01 & 69.36 \\\\ ToTTo & BLEU & 49.78 & 49.51 & 49.47 & 49.31 & 49.38 \\\\ GrailQA & EM & 81.09 & 80.46 & 80.29 & 80.89 & 80.38 \\\\ SQL2Text & Blec & 95.07 & 94.39 & 94.49 & 94.97 & 93.81 \\\\ MMQA & F1 & 84.26 & 84.31 & 84.11 & 83.40 & 85.15 \\\\ Spider & EM & 72.92 & 71.57 & 73.40 & 72.73 & 72.44 \\\\ KVRet & All Micro & 71.60 & 73.90 & 70.34 & 72.25 & 72.61 \\\\ HybridQA & Acc & 59.23 & 59.09 & 59.09 & 59.03 & 59.17 \\\\ SParC & EM & 63.09 & 62.34 & 63.26 & 64.59 & 61.93 \\\\ CompWebQ & Acc & 80.61 & 79.15 & 78.76 & 78.73 & 78.34 \\\\ TabFact & Acc & 83.41 & 81.09 & 81.42 & 80.92 & 80.77 \\\\ WikiTQ & All Ex & 50.02 & 48.50 & 49.24 & 48.30 & 50.09 \\\\ WikiSQL & All Ex & 87.33 & 86.45 & 86.73 & 86.68 & 88.67 \\\\ FeTaQA & BLEU & 36.58 & 37.26 & 36.55 & 36.72 & 36.03 \\\\ Feverous & Acc & 85.02 & 84.13 & 84.11 & 83.73 & 84.41 \\\\ MultiWOZ & Joint Acc & 54.66 & 54.10 & 53.73 & 53.92 & 54.49 \\\\ Dart & BLEU & 61.38 & 61.89 & 61.08 & 62.24 & 62.24 \\\\ Logic2Text & Blec & 88.83 & 89.47 & 89.19 & 90.57 & 88.92 \\\\ MTOP & EM & 82.44 & 81.71 & 81.19 & 80.92 & 81.21 \\\\ \\hline \\multicolumn{6}{c}{Held-Out Datasets} \\\\ \\hline BIRD & Acc & 21.30 & 22.30 & 22.30 & 23.00 & 22.30 \\\\ CoSQL & EM & 51.24 & 49.95 & 50.84 & 50.74 & 49.75 \\\\ SQA & Acc & 49.02 & 46.03 & 43.11 & 48.39 & 49.72 \\\\ Infotabs & Acc & 38.00 & 56.26 & 57.87 & 62.35 & 62.46 \\\\ WikiTableText & BLEU & 14.78 & 13.51 & 6.66 & 7.27 & 8.27 \\\\ Finqa & Acc & 19.70 & 24.32 & 27.55 & 25.37 & 27.29 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Ablation results for the mixtures of general data in the training set. In total, we train 5 models, where the percentage represents the percent of the training data that is general. In the held out data, we see noticeable gains in generalization performance for FinQA and InfoTabs datasets. Notably, FinQA requires the generation of a python-executable math expression and InfoTabs requires an exact match to 3 previously unseen (boolean) options. WikiTableText performance seems to suffer, but is evaluated based the BLEU score with only one target sentence. As a result, we place more emphasis on the model’s 0-shot adaptation ability to new output specifications unseen in the training data.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|l l l} \\hline \\hline Tasks & Metric & Code-LM & LLaMA & Math-LM \\\\ \\hline \\multicolumn{5}{c}{Held-In Datasets} \\\\ \\hline TabMWP & Acc & 71.14 & 62.96 & 66.5 \\\\ ToTTo & BLEU & 49.78 & 48.26 & 47.4 \\\\ GrailQA & EM & 81.09 & 75.72 & 77.66 \\\\ SQL2Text & Blec & 95.07 & 94.49 & 94.58 \\\\ MMQA & F1 & 84.26 & 83.96 & 82.13 \\\\ Spider & EM & 72.92 & 65.96 & 71.95 \\\\ KVRet & All Micro & 71.6 & 70.36 & 70.03 \\\\ HybridQA & Acc & 59.23 & 59.26 & 57.04 \\\\ SParC & EM & 63.09 & 56.94 & 60.35 \\\\ CompWebQ & Acc & 80.61 & 77.31 & 76.6 \\\\ TabFact & Acc & 83.41 & 80.46 & 79.47 \\\\ WikiTQ & All Ex & 50.02 & 45.6 & 46.89 \\\\ WikiSQL & All Ex & 87.33 & 83.93 & 85.49 \\\\ FeTaQA & BLEU & 36.58 & 34.37 & 34.1 \\\\ Feverous & Acc & 85.02 & 83.2 & 82.52 \\\\ MultiWOZ & Joint Acc & 54.66 & 55.43 & 53.79 \\\\ Dart & BLEU & 61.38 & 61.52 & 61.24 \\\\ Logic2Text & Blec & 88.83 & 88.0 & 90.38 \\\\ MTOP & EM & 82.44 & 77.18 & 75.56 \\\\ \\hline \\multicolumn{5}{c}{Held-Out Datasets} \\\\ \\hline BIRD & Acc & 21.3 & 15.9 & 18.8 \\\\ CoSQL & EM & 51.24 & 42.8 & 48.76 \\\\ SQA & Acc & 49.02 & 37.03 & 49.05 \\\\ Infotabs & Acc & 38.0 & 4.44 & 32.54 \\\\ WikiTableText & BLEU & 14.78 & 13.0 & 14.82 \\\\ Finqa & Acc & 19.7 & 6.63 & 21.53 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: The evaluation results comparing finetuning done on different base models. Code refers to Codellama-Instruct-7B. Math refers to LLemma-7b. LLaMA refers to LLaMA-2-7b.\n' +
      '\n' +
      '[INST] <<SYS>> You are an AI assistant that specializes in analyzing and reasoning over structured information. You will be given a task, optionally with some structured knowledge input. Your answer must strictly adhere to the output format, if specified. <</SYS>> {instruction} {input} [/INST]\n' +
      '\n' +
      'Figure 6: Prompt format for all SKG examples. This formatting convention follows LLama2 (Touvron et al., 2023). The input contains the linearized structured data, together with any other context, question or statement. The instruction specifies the task.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>