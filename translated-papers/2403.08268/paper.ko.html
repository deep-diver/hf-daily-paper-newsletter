<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Follow-Your-Click: Short 프롬프트를 통한 Open-domain Regional Image Animation\n' +
      '\n' +
      'Yue Ma\n' +
      '\n' +
      '동등한 기부금 HKUST, \\({}^{2}\\)텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력 사용자 클릭 출력 사용자 클릭\n' +
      '\n' +
      'Yingqing He\n' +
      '\n' +
      '동등한 기부금 HKUST, \\({}^{2}\\)텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력\n' +
      '\n' +
      'Hongfa Wang\n' +
      '\n' +
      '교신저자 HKUST, \\({}^{2}\\)텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력\n' +
      '\n' +
      'Andong Wang\n' +
      '\n' +
      '교신저자 HKUST, \\({}^{2}\\)텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력\n' +
      '\n' +
      'Chengfei Cai\n' +
      '\n' +
      '텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력\n' +
      '\n' +
      'Xiu Li\n' +
      '\n' +
      'Zifeng Li\n' +
      '\n' +
      'Heung-Yeung Shum\n' +
      '\n' +
      'Wei Liu\n' +
      '\n' +
      '교신저자 HKUST, \\({}^{2}\\)텐센트, 훈위안, \\({}^{3}\\) 칭화대학교\n' +
      '\n' +
      '[https://follow-your-click.github.io/User](https://follow-your-click.github.io/User) 출력 사용자 클릭 출력\n' +
      '\n' +
      'Qifeng Chen\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이미지 대 비디오 생성의 최근 발전에도 불구하고, 더 나은 제어성 및 로컬 애니메이션은 덜 탐구된다. 대부분의 기존의 이미지-투-비디오 방법들은 국부적으로 인식되지 않고 전체 장면을 이동시키는 경향이 있다. 하지만, 인간 예술가들은 다른 사물이나 지역의 움직임을 통제할 필요가 있을 수 있다. 추가적으로, 현재의 I2V 방법들은 사용자가 타겟 모션에 대해 기술할 뿐만 아니라 프레임 컨텐츠에 대한 중복된 상세한 설명을 제공할 것을 요구한다. 이 두 가지 문제는\n' +
      '\n' +
      '그림 1: **클릭과 짧은 프롬프트를 이용한 지역 이미지 애니메이션. 본 논문에서는 사용자 제공 클릭(_where_ to move)과 짧은 모션 프롬프트(_how_ to move)를 통해 로컬 인식 이미지 애니메이션을 용이하게 하는 새로운 프레임워크를 제시한다. 우리의 프레임워크는 생생한 객체 움직임, 배경 움직임(예: 폭풍) 및 다중 객체 움직임을 제공할 수 있다. _ 동영상을 클릭하여 애니메이션을 재생할 수 있도록 지원하는 Acrobat Reader_로 가장 잘 볼 수 있습니다. _ 모든 결과의 정적 프레임 및 비디오는 보충 자료_로 제공된다.\n' +
      '\n' +
      '현재 I2V 도구의 실용적 활용. 본 논문에서는 간단한 사용자 클릭(_what_ to move 지정)과 짧은 모션 프롬프트(_how_ to move 지정)로 이미지 애니메이션을 구현하기 위한 Follow-Your-Click이라는 실용적인 프레임워크를 제안한다. 기술적으로, 우리는 비디오 생성 품질을 크게 향상시키는 첫 프레임 마스킹 전략과 모델의 짧은 프롬프트 추종 능력을 향상시키기 위해 짧은 모션 프롬프트 데이터 세트가 장착된 모션 증강 모듈을 제안한다. 움직임 속도를 더 제어하기 위해, 우리는 목표 움직임의 속도를 더 정밀하게 제어하기 위한 흐름 기반 움직임 크기 제어를 제안한다. 우리의 프레임워크는 이전 방법보다 간단하면서도 정확한 사용자 제어와 더 나은 생성 성능을 가지고 있다. 상업적 도구와 8가지 메트릭에 대한 연구 방법을 모두 포함하여 7가지 기준선과 비교한 광범위한 실험은 우리의 접근법의 우월성을 시사한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지-투-비디오 생성(I2V)은 이미지를 합리적인 움직임으로 동적 비디오 클립으로 애니메이션화하는 것을 목표로 한다. 그것은 영화 제작 산업, 증강 현실 및 자동 광고에 널리 응용되고 있다. 전통적으로, 이미지 애니메이션 방법은 주로 자연 장면[17, 43, 46, 79], 인간 머리[75], 초상화[27, 73] 및 신체[9, 11, 74, 44]와 같은 도메인-특정 카테고리에 초점을 맞추어서 실제 세계에서의 실용적 응용을 제한한다. 최근, 대규모 이미지 데이터 세트에 대해 훈련된 확산 모델[52, 55, 57]의 상당한 발전은 텍스트 프롬프트를 기반으로 다양하고 사실적인 이미지를 생성할 수 있게 했다. 이러한 성공에 고무된 연구자들은 이미지 대 비디오 생성을 위한 강력한 이미지 생성 이전을 활용하는 것을 목표로 이러한 모델을 I2V의 영역으로 확장하기 시작했다[78, 13, 60, 72].\n' +
      '\n' +
      '그러나, 기존의 I2V 작품[71, 5, 13, 78]은 영상의 어느 부분을 이동시켜야 하는지에 대한 제어가 부족하고, 전체 장면의 움직임과 함께 비디오를 제작한다; 그리고 SVD[13]과 같은 일부 작품들은 보다 생생한 객체 움직임을 무시하고 항상 카메라 움직임과 함께 비디오를 전달하는 경향이 있다. 그들은 인간 예술가들에게 중요한 지역적 이미지 애니메이션을 달성할 수 없다(예를 들어, 사용자는 배경을 정적으로 유지하면서 전경 객체를 애니메이션화하기를 원할 수 있다). 또한, 사용자가 I2V 모델에 제공하는 대표적인 프롬프트는 전체 장면 콘텐츠에 대한 설명이다. 그러나, 공간 콘텐츠는 사용자가 다시 기술하기 위해 필요하지 않은 입력 이미지를 통해 충분히 기술된다. 사실, 보다 직관적인 방법은 모션 전용 프롬프트를 제공하는 것이지만, 현재의 접근법은 짧은 모션 프롬프트에 덜 민감하다. 이전 작업에서 공통 가설은 확산 모델이 프롬프트 기반 프레임워크이며 상세한 프롬프트가 생성된 결과의 품질을 향상시킬 수 있다는 것이다. 그러나 이러한 기능은 현실 세계의 사용자를 위한 실제 적용을 극적으로 제한한다. WebVid[8] 및 HDVILA[81]과 같은 기존의 데이터 세트는 객체의 움직임을 무시하면서 주로 캡션에서 장면 및 이벤트를 설명하는 데 중점을 둔다. 이러한 데이터 세트에 대한 트레이닝은 생성된 모션의 품질을 감소시키고 모션 관련 키워드에 대한 불감증을 초래할 수 있다.\n' +
      '\n' +
      ' 본 논문에서는 이러한 문제를 해결할 수 있는 보다 실용적이고 제어 가능한 I2V 모델을 고안하고자 한다. 이를 위해 사용자 클릭과 짧은 모션 프롬프트를 통해 지역 이미지 애니메이션이 가능한 새로운 I2V 프레임워크인 **Follow-Your-Click**을 제안한다. 좋은 생성 성능을 얻으면서 이 간단한 사용자 상호 작용 메커니즘을 달성하기 위해 먼저 SAM[18]을 통합하여 사용자 클릭을 이진 지역 마스크로 변환하며, 이는 네트워크 조건 중 하나이다. 그리고 시간적인 상관관계를 정확하게 학습하기 위해 효과적인 _first-frame masking_ 전략을 소개하고 성능향상의 큰 여유를 관찰한다. 짧은 프롬프트 추종 능력을 얻기 위해, 우리는 객체의 감정, 행동, 공통 동작을 강조하며 비디오 캡션을 필터링하고 주석을 달기 위해 LLM(Large Language Model)을 활용하여 구축된 _WebVid-Motion_라고 하는 데이터 세트를 구성한다. 그런 다음 데이터 세트에 더 잘 적응하고 모션 관련 단어에 대한 모델의 응답을 향상시키고 짧은 프롬프트 지침을 이해하기 위해 _motion-augmented module_을 설계한다. 또한, 다양한 객체 유형이 다양한 움직임 속도를 나타낼 수 있음을 관찰한다. 이전 작업[78]에서, 초당 프레임 레이트(FPS)는 주로 다수의 객체의 움직임 속도를 간접적으로 조정하기 위한 전역 스케일링 팩터의 역할을 한다. 그러나 움직이는 물체의 속도를 효과적으로 제어할 수 없다. 예를 들어, 조형물을 특징으로 하는 비디오는 높은 FPS를 갖지만 모션 속도는 제로일 수 있다. 움직임 속도의 정확한 학습이 가능하도록 새로운 _flow 기반 움직임 크기 제어_를 제안한다.\n' +
      '\n' +
      '우리의 설계로 8가지 다양한 평가 메트릭에서 놀라운 결과를 얻을 수 있습니다. 또한 다중 클릭을 통해 _multiple_ 객체 및 이동 유형의 제어를 용이하게 할 수 있다. 또한, 보다 세밀한 모션 제어를 달성하기 위해 인간의 골격과 같은 제어 신호와 우리의 접근법을 통합하는 것이 쉽다. 우리의 기여는 다음과 같이 요약할 수 있다:\n' +
      '\n' +
      '* 우리가 아는 한, Follow-Your-Click은 지역 이미지 애니메이션을 위한 간단한 _click_ 및 _short 모션 프롬프트를 지원하는 첫 번째 프레임워크이다.\n' +
      '* 이러한 사용자 친화적이고 제어 가능한 I2V 프레임워크를 달성하기 위해 기술적으로 일반적인 생성 품질을 향상시키기 위한 _first-frame masking_, 짧은 프롬프트 후속을 위한 _short 프롬프트 dataset_가 장착된 _motion-augmented module_ 및 보다 정확한 모션 속도 제어를 위한 _flow 기반 모션 크기_를 제안한다.\n' +
      '* 우리는 우리의 접근 방식을 평가하기 위해 광범위한 실험과 사용자 연구를 수행했으며, 이는 우리의 방법이 최첨단 성능을 달성한다는 것을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '텍스트-투-비디오 생성은 최근 광범위한 연구를 통해 인기 있는 주제이다. 확산 모델이 등장하기 전에, 많은 접근법이 생성된 콘텐츠에 대한 텍스트 제어를 달성하기 위해 변압기 아키텍처[20, 31, 32, 38, 42, 48, 54, 69, 76, 82, 83, 84]를 기반으로 개발되었다. 확산 모델의 비상 사태[62]는 더 높은 품질과 더 다양한 결과를 제공한다. LVDM[36] 및 모델스코프[70]와 같은 초기 작업은 시간 모듈의 통합을 탐구한다. 비디오 확산 모델(VDM)[40]은 픽셀 공간에서 시공간 인수분해 U-Net을 사용하여 저해상도 비디오를 모델링하기 위해 제안된다. 최근의 모델들은 훈련 확산 기반 모델의 안정성으로부터 이익을 얻는다[55]. 이러한 모델은 거대한 데이터 세트에 의해 확장될 수 있으며 텍스트 대 비디오 생성에서 놀랍게도 좋은 결과를 보여준다. 매직 비디오[88]와 gen1[3]은 텍스트에서 이미지까지 모델을 초기화하고 추가 시간 인식 레이어를 통해 연속 콘텐츠를 생성한다. 또한, 공간 및 시간 모듈을 분리하는 VDM의 카테고리가 등장했다[29, 30]. 외모와 움직임을 개별적으로 제어할 수 있는 잠재력을 제공하지만 여전히 비디오 지역 통제의 도전에 직면해 있다.\n' +
      '\n' +
      '이러한 모델은 고품질 비디오를 생성할 수 있지만, 주로 의미적 안내를 위해 텍스트 프롬프트에 의존하며, 이는 모호할 수 있고 사용자의 의도를 정확하게 설명하지 못할 수 있다. 이러한 문제를 해결하기 위해, 제어가능한 비디오 생성을 위해 구조[22, 25, 77], 포즈[49, 86, 87], 및 캐니 에지[86]와 같은 많은 제어 신호들이 적용된다. Dynamicrafter[78], VideoComposer[71], I2VGen-XL[5]의 많은 최근 및 동시 방법들은 비디오 합성을 안내하는 조건으로 RGB 이미지를 탐색한다. 그러나 이들은 특정 도메인에 집중하여 입력 영상의 세부 사항을 보존하면서 시간적으로 일관성 있는 프레임 및 사실적인 움직임을 생성하지 못한다. 게다가, 대부분의 프롬프트들은 이미지 콘텐츠를 기술하기 위해 사용되며, 사용자들은 그들의 의도에 따라 이미지를 애니메이션화할 수 없다. 제안하는 방법은 텍스트 기반 VDM을 기반으로 하며, 배경의 일관성을 유지하면서 이미지 내의 객체를 애니메이션하는 강력한 생성 능력을 활용한다.\n' +
      '\n' +
      '### Image Animation\n' +
      '\n' +
      '이미지 대 비디오 생성은 일관된 비디오를 생성하면서 입력 이미지의 동일성을 유지하는 중요한 요구를 수반한다. 이것은 이미지의 아이덴티티를 보존하는 것과 비디오 생성의 동적 특성 사이의 균형을 맞추는 데 중요한 도전을 제시한다. 물리적 시뮬레이션에 기반한 초기 접근법[21, 68, 61, 33, 53, 73]은 특정 객체의 움직임을 시뮬레이션하는 데 집중하며, 각 객체 범주의 개별 모델링으로 인해 일반화 가능성이 떨어진다. 딥러닝의 성공으로 더 많은 GAN 기반 작업[37, 45, 59]이 수동 분할을 없애고 더 자연스러운 움직임을 합성할 수 있다. MCVD[67] 및 SEINE[16]과 같은 마스크 기반 접근법들은 태스크를 달성하기 위해 단일 이미지로부터 시작하는 미래의 비디오 프레임들을 예측한다. 그들은 생성된 비디오 프레임 전체에 걸쳐 입력 이미지의 동일성의 일관성을 보존하여 정적에서 동적으로의 원활한 전환을 보장하는 데 중요한 역할을 한다. 현재, 확산에 기초한 주류 작업[14, 26, 41, 51, 74]은 비디오 확산 모델을 사용하여 프레임을 생성할 수 있다. Dynamicrafter[78]와 Livephoto[15]는 실제 이미지 애니메이션을 위한 강력한 프레임워크를 제안하고 경쟁적인 성능을 달성한다. I2V-어댑터[28] 및 PIA[87]와 같은 플러그-투-플레이 어댑터는 이미지를 애니메이션화하기 위해 퍼블릭 로라[2] 가중치 및 체크포인트를 적용한다. 그러나 큐레이션된 도메인에만 초점을 맞추고 시간적으로 일관된 실제 프레임을 생성하지 못한다. 또한 일부 상용 대형 모델인 Gen-2[3], Genmo[4], Pika Labs[6]은 2023년 11월 업데이트에서 사실적인 이미지 영역에서 인상적인 결과를 제공한다. 그러나 이러한 작업은 지역 이미지 애니메이션과 정확한 제어를 달성할 수 없다. 동시작품 중 최신판 젠-2는 2024년 1월 지역 애니메이션을 지원하는 모션브러시를 출시했다. 그러나, 그것은 여전히 현실적인 동작을 합성해야 하는 도전에 직면해 있다(도 3 참조). 또한 사용자 클릭과 짧은 프롬프트 상호 작용을 지원할 수 없습니다. 또한, 상용 도구로서 Gen-2는 연구를 위한 기술적 솔루션과 체크포인트를 출시하지 않을 것이다. 대조적으로, 우리의 방법은 간단한 상호 작용, 모션 증강 학습 및 더 나은 생성 품질에서 독특한 이점을 가지고 있다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '**잠재 확산 모델(LDM)**잠재 확산 모델[55](LDM)을 백본 생성 모델로 선택한다. 확산 모델에서 파생된 LDM은 잠재 공간 내에서 확산 및 잡음 제거 절차를 재구성한다. 이 과정은 잠재 코드에 가우스 잡음을 점진적으로 추가하는 마르코프 체인으로 간주될 수 있다. 먼저, 인코더\\(\\mathcal{E}\\)는 픽셀 공간 이미지\\(x\\)를 저해상도 잠재 이미지\\(z=\\mathcal{E}(x)\\)로 압축하고, 이는 디코더\\(\\mathcal{D}\\(z)\\approx x\\)에 의해 잠재 특징에서 이미지\\(\\mathcal{D}(z)\\approx x\\)으로 재구성될 수 있다. 그런 다음, 자기 주의[66]와 교차 주의[66]를 갖는 U-Net[56]\\(\\varepsilon_{\\theta}\\)을 학습하여 이 목적을 통해 추가된 잡음을 추정한다:\n' +
      '\n' +
      '\\[\\min_{\\theta}E_{z_{0},\\varepsilon\\sim N(0,I),t\\sim\\text{Uniform}(1,T)}\\left\\|\\varepsilon-\\varepsilon_{\\theta}\\left(z_{t},t,p\\right)\\right\\|_{2}^{2},\\tag{1}\\text{Uniform}(1,I),t\\sim\\text{Uniform}(1,T)}\\left\\|\\varepsilon-\\varepsilon_{\\theta}\\left(z_{t},t,p\\right)\\right\\|_{2}^{2},\\tag{1}\\text{Uniform}(1,T)}\\left\\varepsilon-\\varepsilon-\\varepsilon_{\\theta}\\left(z_{t},t,p\\right)\\right\\|_{2}^{2},\\tag{1}\\\n' +
      '\n' +
      '여기서 \\(p\\)은 텍스트 프롬프트의 임베딩이고 \\(z_{t}\\)은 타임스텝 \\(t\\)에서 \\(z_{0}\\)의 잡음 샘플이다. 학습 후, 랜덤 가우시안 잡음(z_{T}\\)과 텍스트 임베딩(p\\)으로부터 단계적인 잡음제거를 통해 깨끗한 이미지 잠재력(z_{0}\\)을 생성하고, 그 잠재력을 \\(\\mathcal{D}\\)으로 픽셀 공간으로 디코딩할 수 있다.\n' +
      '\n' +
      '**비디오 잠재 확산 모델(VDM).** 이전 작업 [30, 55]에 이어서, 시간적 모션 모듈을 통합함으로써 잠재 확산 모델을 비디오 버전(VDM)으로 확장한다. 구체적으로, VDM에서 공간 모듈의 가중치는 미리 훈련된 이미지 LDM으로 초기화되고 훈련 중에 동결된다. 이 작업은 모델이 강력한 이미지 LDM에서 생성 이전을 상속하는 데 도움이 될 수 있다. 1-D 시간적 주의를 포함하는 시간적 모션 모듈들은 각각의 공간 모듈 이후에 삽입되며, 이들은 상이한 프레임들에 걸쳐 동일한 공간 위치의 표현들 사이의 시간적 의존성을 캡처하는 것을 담당한다. 영상(\\mathbf{x}\\in\\mathbb{R}^{L\\times C\\times H\\times W}\\)이 주어졌을 때, 영상 길이, 채널 수, 높이 및 너비를 각각 나타내는 \\(L,C,H,W\\)이 주어졌을 때, 먼저 이를 잠재 공간 프레임 단위로 인코딩하여 \\(\\mathbff{z}\\in\\mathbbb{R}^{L\\times c\\times h\\times w}\\)의 잠재 공간 프레임을 얻는다. 그리고, 이 잠재공간에서 순방향 확산과정과 역방향 잡음제거과정이 모두 수행된다. 마지막으로, 생성된 비디오들은 디코더를 통해 획득된다.\n' +
      '\n' +
      '## 4 Follow-Your-Click\n' +
      '\n' +
      '### Problem Formulation\n' +
      '\n' +
      '정지 이미지가 주어졌을 때, 우리의 목표는 사용자가 선택한 영역을 애니메이션화하여, 나머지 이미지를 정지 상태로 유지하면서 사실적인 움직임을 보여주는 짧은 비디오 클립을 만드는 것이다. 형식적으로, 입력 영상\\(\\mathcal{I}\\), 점 프롬프트\\(p\\) 및 원하는 동작\\(t\\)에 대한 짧은 동작 관련 동사 설명이 주어지면, 본 논문에서 제안하는 방법은 타겟 애니메이션 영상\\(\\mathcal{V}\\)을 생성한다. 이 작업을 지역 애니메이션의 생성 품질 향상, 짧은 동작 프롬프트 제어 생성, 동작 크기 제어 가능 생성 등 여러 하위 문제로 분해한다. 타겟 영역은 후속 프레임들에서 생성된 객체의 모션을 제한하기보다는 애니메이트된 객체를 선택하기 위해 활용된다는 점에 유의한다. 즉, 객체는 지정된 영역 내에 머무르도록 구속되지 않고, 필요한 경우 그들 외부로 이동할 수 있다.\n' +
      '\n' +
      '**사용자 상호작용 및 제어.** 사용자가 애니메이션화하기를 원하는 입력 이미지가 주어짐. 직관적인 방법은 먼저 이미지의 어느 부분을 이동해야 하는지 선택한 다음 텍스트 프롬프트를 사용하여 원하는 이동 패턴을 설명하는 것이다. 연구 작업 I2VGen-XL, SVD, 동적 측정기 및 피카 랩 및 젠모와 같은 상용 도구와 같은 현재 접근 방식은 지역 통제 능력이 부족하다. Gen-2[3]의 모션 브러쉬와 animate-anything[19]는 이러한 목표를 달성할 수 있지만 모션 마스크는 사용자에 의해 제공되거나 그려질 필요가 있어 사용자에게 효율적이고 직관적이지 않다. 따라서 사용자 친화적인 제어를 제공하기 위해 이진 마스크 대신 _point prompt_를 사용하도록 설계한다. 더욱이, 현재의 이미지-투-비디오 방법들은 전체 장면 및 프레임 콘텐츠를 기술하기 위한 입력 프롬프트를 필요로 하며, 이는 지루하고 불필요하다. 반대로, 우리는 동사 단어 또는 짧은 구만을 사용하여 짧은 모션 프롬프트로 이 절차를 단순화한다. 를 포함하고,\n' +
      '\n' +
      '그림 2: **프레임워크 개요.** 본 프레임워크의 주요 구성 요소는 첫 번째 프레임 마스킹, 짧은 모션 프롬프트 팔로우를 위한 모션 증강 모듈, 흐름 기반 모션 강도 제어입니다. 추론 동안, 지역 애니메이션은 사용자 클릭들 및 짧은 모션 프롬프트들에 의해 달성될 수 있다.\n' +
      '\n' +
      '이를 달성하기 위해, 우리는 신속한 분할 도구 SAM[18]을 통합하여 포인트를 프롬프트\\(p\\)로 고품질 객체 마스크\\(\\mathcal{M}\\)로 변환한다. 본 논문에서는 4.2절에서 제안하는 마스킹 제어 지역 애니메이션을 소개하고, 4.3절에서 제안하는 모션 증강 모듈을 제안한다.\n' +
      '\n' +
      '지역 이미지 애니메이션\n' +
      '\n' +
      '**광학 흐름 기반 모션 마스크 생성.** WebVid[8] 및 HDVILA[81]와 같은 공공 데이터 세트에 대한 트레이닝은 이동이 큰 지역에 대한 대응하는 바이너리 마스크 안내의 부족으로 인해 지역 이미지 애니메이션을 달성하기 어렵다. 이를 해결하기 위해 광학 흐름 예측 모델을 이용하여 이동 영역을 나타내는 마스크를 자동으로 생성한다. 구체적으로, 학습 비디오 프레임 \\(\\{x_{0},x_{1}...,x_{L-1}\\}\\)을 제공하고, 오픈소싱 광흐름 추정기 \\(\\mathcal{E}_{flow}\\)[64]를 사용하여 연속된 두 프레임 쌍의 광흐름 맵 \\(\\mathcal{F}_{i}\\)을 추출하며, 여기서 \\(i\\)은 비디오의 프레임 인덱스이다. 각 흐름 맵(\\mathcal{F}_{i}\\)에 대해, 평균 크기를 통해 계산된 임계값을 통해 맵을 이진 맵(\\mathcal{M}_{i}\\)으로 임계한다. 마지막으로, 모든 마스크의 합을 구하여 움직임의 영역을 나타내는 최종 마스크\\(\\mathcal{M}_{1},\\mathcal{M}_{2},...,\\mathcal{M}_{L-1}\\)을 구한다. 형식적으로, 모션 영역 안내는 다음과 같이 구현된다.\n' +
      '\n' +
      '\\mathcal{F}_{i}=\\mathcal{E}_{flow}(x_{i},x_{i-1}), \\tag{2}\\] \\[\\mathcal{M}_{i}=\\text{Binarize}(\\mathcal{F}_{i},\\text{Avg}(\\|\\mathcal{F}_{i}\\|)),\\\\mathcal{M}_{final}=\\bigcup_{i=0}^{L-1}(\\mathcal{M}_{i}})}.\n' +
      '\n' +
      '여기서 \\(i=1,2,3,\\ldots,L\\), \\(\\text{Binarize}(\\cdot,\\cdot)\\)는 이진화 연산이고 \\(\\|\\cdot\\|\\)은 각 픽셀에서의 광 흐름의 크기를 나타낸다. 훈련 중, 우리는 지상진실 영상의 움직임 영역을 표현하기 위해 \\(\\mathcal{M}_{final}\\)을 사용한다. 추론하는 동안 사용자가 클릭한 이진 마스크를 프롬프트 이미지 분할 도구 SAM[18]을 통해 이진 마스크에 전달한 다음 이진 마스크를 네트워크에 공급한다. 또한 보조 재료에서 조건부 마스크의 일반화 능력에 대해서도 연구한다.\n' +
      '\n' +
      '**First-frame masking training.** 이동영역 마스크 \\(\\mathcal{M}_{final}\\)을 구한 후 다운샘플링된 버전, 첫 번째 프레임 잠재 \\(z_{0}\\)과 잠재공간에서 채널차원에서의 랜덤잡음을 연결하여 크기 \\([9,L,h,w]\\)으로 입력을 구한 후 네트워크로 공급한다. \\ (z_{0}\\)는 VAE 인코더를 통해 인코딩된 첫 번째 프레임 \\(x_{0}\\)의 잠재성이다. 잠재 프레임의 해상도와 일치하도록 \\(\\mathcal{M}_{final}\\)을 다운샘플링한다. 타겟 생성 프레임의 마스크 \\(\\mathcal{M}_{1},\\mathcal{M}_{2},...,\\mathcal{M}_{L-1}\\)는 0으로 설정되고, 첫 번째 프레임은 안내 역할을 하며 \\(L\\) 프레임으로 반복된다. 9개의 채널은 입력 영상 잠재 4채널, 생성된 프레임 4채널, 이진 마스크 1채널로 구성된다. [58]에서 제안된 \\(\\mathbf{v}\\)-예측 파라미터화는 몇 단계의 추론 단계에서 더 나은 샘플링 안정성을 갖기 때문에 훈련에 채택된다. 그러나, 우리는 이러한 방식으로 직접 훈련하는 것이 시간적 구조 왜곡 문제를 나타낸다는 것을 관찰한다. 최근의 마스킹 전략에서 영감을 얻은 [23; 34; 50], 우리는 훈련에서 조건 정보를 증강하는 것이 모델이 시간적 상관 관계를 더 잘 학습하는 데 도움이 될 수 있다고 가정한다. 따라서 입력 영상의 잠재 임베딩을 \\(z_{0}\\)을 \\(\\mathcal{R}\\)의 비율로 랜덤하게 마스킹하고 마스킹된 영역을 \\(0\\)으로 설정한다. 도 1에 도시된 바와 같다. 2, 마스킹된 첫 번째 프레임 잠재량은 다운샘플링된 \\(\\mathcal{M}_{final}\\) 및 잡음이 있는 비디오 잠재 \\(\\mathbf{z}\\)과 함께 연결되며 최적화를 위해 네트워크에 공급된다. 경험적으로, 잠재된 입력 이미지를 랜덤하게 마스킹하는 것이 생성된 비디오 클립의 품질을 상당히 향상시킬 수 있다는 것을 발견한다. Sec. 5.3에서는 마스크 비율 선택에 대한 자세한 분석을 수행한다.\n' +
      '\n' +
      '시간 이동 제어\n' +
      '\n' +
      '짧은 모션 캡션 구성.우리는 현재 광범위한 데이터 세트에서 캡션이 더 적은 동적 또는 모션 관련 설명과 함께 항상 수많은 장면 설명 용어를 포함한다는 것을 발견한다. 더 나은 짧은 프롬프트를 달성하기 위해 GPT4 [1]을 사용하여 WebVid-10M 데이터 세트를 필터링하고 다시 주석하여 데이터 세트인 WebVid-Motion 데이터 세트를 구성한다. 특히 GPT4의 문맥 내 학습을 위해 50개의 샘플을 구성하였으며, 각 샘플에는 원래 프롬프트, 객체 및 짧은 동작 관련 설명이 포함되어 있다. 이 샘플은 JSON 형식으로 GPT4에 공급된 다음 WebVid-10M에서 다른 짧은 모션 프롬프트를 예측하기 위해 GPT4에 동일한 질문을 한다. 마지막으로, 재구성된 데이터세트는 "머리 조정", "미소", "깜빡임" 및 "달리기"와 같은 캡션 및 그들의 모션 관련 구절을 포함한다. 우리는 짧은 모션 프롬프트 팔로우의 더 나은 능력을 얻기 위해 이 데이터 세트에서 모델을 미세 조정한다.\n' +
      '\n' +
      '**모션-증강 모듈.** 이전 기술들을 통해 트레이닝된 모델을 사용하여 [30], 네트워크가 짧은 모션 프롬프트를 추가로 인지하도록, 모션-증강 모듈을 설계하여 모션-관련 프롬프트들에 대한 모델의 응답들을 개선한다. 구체적으로, 각 모션 모듈 블록에 새로운 크로스 어텐션 레이어를 삽입한다. 짧은 모션 관련 어구들은 트레이닝을 위해 모션 증강 모듈에 공급되고, 추론 동안, 이러한 어구들은 U-Net에서 모션 증강 모듈 및 크로스 어텐션 모듈 모두에 입력된다. 이 모듈 덕분에, 본 모델은 사용자가 제공하는 짧은 모션 관련 프롬프트만으로 추론 동안 원하는 성능을 생성할 수 있어 중복 완전한 문장의 필요성을 없앨 수 있다.\n' +
      '\n' +
      '**광학 흐름 기반 모션 강도 제어.** 모션 강도를 제어하기 위한 종래의 방법은 주로 초당 프레임(FPS) 조정에 의존하며 트레이닝 동안 동적 FPS 메커니즘을 채용한다[88]. 그러나, 우리는 운동 강도와 FPS 사이의 관계가 선형적이지 않다는 것을 관찰한다. 비디오 촬영 스타일의 변화로 인해, FPS와 모션 강도 사이에 상당한 차이가 있을 수 있다. 예를 들어, 낮은-FPS 비디오들(변화들이 높은-FPS 비디오들에서보다 _rapidly_ 더 많이 발생하는 경우)에서도, 느린-모션 비디오들은 최소의 모션을 나타낼 수 있다. 이 접근법은 움직임의 강도를 정확하게 나타내지 못한다. 이를 해결하기 위해, 우리는 움직임 강도를 제어하는 수단으로 광 흐름의 크기를 사용하는 것을 제안한다. 섹 4.2에서 언급한 바와 같이 움직임이 가장 큰 영역에 대한 마스크를 얻으면 해당 영역 내에서 광학 흐름의 평균 크기를 계산한다. 그런 다음 이 크기는 위치 임베딩으로 투영되고 잔차 블록 내의 각 프레임에 추가되어 모든 프레임에 걸쳐 모션 강도의 일관된 적용을 보장한다.\n' +
      '\n' +
      'Experiments\n' +
      '\n' +
      '이 섹션에서는 Sec. 5.1의 세부 구현을 소개하고, Sec. 5.2에서 성능을 종합적으로 평가하기 위해 다양한 기준선을 사용하여 접근 방식을 평가하고, Sec. 5.3에서 주요 구성 요소를 제거하여 효율성을 보여주고, 마지막으로 Sec. 5.4에서 접근 방식을 다른 도구와 통합할 수 있는 가능성을 입증하는 두 가지 응용 프로그램을 제공한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '실험에서 공간 모듈은 SD(Stable Diffusion) V1.5[55]를 기반으로 하고, 모션 모듈은 해당 AnimateDiff[30] 체크포인트 V2를 사용하여 각 비디오 프레임을 잠재 표현으로 인코딩하기 위해 SD 이미지 오토인코더를 동결한다. 우리는 WebVid-10M [8]에서 60k 단계에 대해 모델을 훈련시킨 다음 재구성된 WebVid-Motion 데이터셋에서 30k 단계에 대해 미세 조정한다. 학습용 비디오는 16 프레임, 4보폭의 해상도를 가지며, 3일 동안 8개의 NVIDIA A800 GPU에서 Adam[47]을 사용하여 전체 프레임웍을 최적화하였으며, 학습률을 1배(1배 10^{-4}\\)로 설정하여 성능을 향상시켰다. 트레이닝 과정에서 첫 번째 프레임의 마스크 비율은 0.7이다. 추론에서, 우리는 실험에서 분류기가 없는 안내[39] 척도 7.5와 함께 DDIM 샘플러[62]를 적용한다.\n' +
      '\n' +
      '###기준선 비교\n' +
      '\n' +
      '**정성적 결과** 우리는 우리의 접근법을 Animate anything[19], SVD[10], Dynamicrafter[78] 및 I2VGen-XL[5]를 포함한 가장 최근의 오픈소싱된 최첨단 애니메이션 방법과 정성적으로 비교한다. 우리는 또한 우리의 접근법을 Gen-2[3], Genmo[4], Pika Labs[6]과 같은 상용 도구와 비교한다. 2024년 2월 15일에 액세스한 결과는 빠른 버전 반복으로 인해 현재 제품 버전과 다를 수 있습니다. 동적 결과는 그림 3에서 찾을 수 있다. 벤치마크 이미지, 해당 프롬프트 및 선택된 영역을 감안할 때, 본 접근법에 의해 생성된 비디오가 짧은 모션 관련 프롬프트 "쉐이크 바디"에 더 나은 응답을 나타내는 것을 관찰할 수 있다. 한편, 본 논문에서 제안하는 방법은 지역 애니메이션을 구현함과 동시에 입력 영상 콘텐츠로부터 더 나은 디테일을 보존할 수 있다. 대조적으로, SVD 및 Dynamicrafter는 입력 이미지의 부적절한 의미적 이해로 인해 후속 프레임이 초기 프레임에서 벗어나는 경향이 있기 때문에 일관된 비디오 프레임을 생성하기 위해 고군분투한다. 반면 I2VGen-XL은 부드러운 움직임으로 동영상을 생성하지만 이미지 디테일을 잃는다. 우리는 Genmo가 모션 프롬프트에 민감하지 않고 작은 모션으로 비디오를 생성하는 경향이 있음을 관찰한다. 애니멀-애니메이션은 지역적 애니메이션을 얻을 수 있고, 우리의 접근법에 의해 생성된 것만큼 큰 움직임을 생성할 수 있지만, 심각한 왜곡과 텍스트 정렬로 인해 어려움을 겪는다. 상업용 제품으로 Pika Labs와 Gen-2는 매력적인 고해상도 및 장기 동영상을 제작할 수 있습니다. 그러나 Gen-2는 주어진 프롬프트에 덜 반응하기 때문에 어려움을 겪는다. 피카 랩은 더 큰 다이내믹을 생성하려고 시도할 때 덜 동적인 스틸 비디오를 생성하는 경향이 있으며 흐릿함을 나타낸다. 이러한 결과는 큰 모션이 있는 경우에도 짧은 모션 관련 프롬프트를 사용하여 일관된 결과를 생성하는 데 우리의 접근법이 우수한 성능을 가짐을 입증한다.\n' +
      '\n' +
      '**정량적 결과**광범위한 평가를 위해 30개의 프롬프트, 이미지 및 해당 영역 마스크를 포함하는 정량적 비교를 위한 벤치마크를 구성한다. 저작권이 없는 웹사이트 Pix-abay에서 이미지를 다운로드하고 GPT4를 사용하여 이미지 내용과 가능한 모션에 대한 프롬프트를 생성합니다. 프롬프트 및 이미지는 다양한 콘텐츠(캐릭터, 동물, 풍경) 및 스타일(_e.g._, 사실적, 만화 스타일, 반 고흐 스타일)을 포함한다. 정량적 테스트를 완료하기 위해 4가지 평가 메트릭이 적용된다. (1) \\(I_{1}-\\)**MSE**: [78]을 따라 생성된 첫 번째 프레임과 주어진 이미지 사이의 일관성을 측정한다. (2) **Temporal Consistency(Tem-Consis)**: 생성된 동영상의 시간적 일관성을 평가한다. 우리는 계산합니다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c} \\hline  & \\multicolumn{4}{c}{Automatic Metrics} & \\multicolumn{4}{c}{User Study} \\\\ \\cline{2-10} Method & \\(I_{1}\\)-MSE\\(\\downarrow\\) & Tem-Consis\\(\\uparrow\\) & Text-Align\\(\\uparrow\\) & FVD \\(\\downarrow\\) & Mask-Corr\\(\\downarrow\\) & Motion\\(\\downarrow\\) & Appearance\\(\\downarrow\\) & Overall \\(\\downarrow\\) \\\\ \\hline \\hline Gen-2 [3] & 54.72 & 0.8997 & 0.6337 & 496.17 & 3.12 & 5.11 & 2.52 & 2.91 \\\\ Genno [4] & 91.84 & 0.8316 & 0.6158 & 547.16 & 6.43 & 4.57 & 3.51 & 3.76 \\\\ Pika Labs [6] & **33.27** & **0.9724** & **0.7163** & **337.84** & 3.92 & **2.86** & **2.17** & **2.88** \\\\ \\hline Dynamicafter [78] & 98.19 & 0.8341 & 0.6654 & 486.37 & 5.27 & 6.25 & 4.91 & 5.93 \\\\ I2VGen-XL [5] & 117.86 & 0.6479 & 0.5349 & 592.13 & 7.19 & 7.79 & 6.98 & 7.26 \\\\ SVD [5] & 43.57 & 0.9175 & 0.5007 & 484.26 & 4.91 & 3.74 & 3.94 & 4.81 \\\\ Animate-anything [5] & 53.72 & 0.7983 & 0.6372 & 477.42 & **2.73** & 4.73 & 5.47 & 5.75 \\\\ \\hline\n' +
      '**Ours** & **21.46** & **0.9613** & **0.7981** & **271.74** & **1.38** & **1.91** & **1.87** & **1.78** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **기준선과 접근 방식 간의 정량적 비교**. 우리의 방법은 여러 메트릭에 걸쳐 최고 또는 비교 가능한 성능을 보여준다. 최우수 방법에 대한 메트릭은 빨간색으로 강조되고 두 번째로 우수 방법에 대한 메트릭은 파란색으로 강조됩니다.\n' +
      '\n' +
      '그림 3: 기준선과 접근 방식 간의 **정성적 비교**. 우리는 Gen-2[3], Genmo[4], Pika[6]을 포함한 근접 소스 상용 도구와 Animate-anything[19], SVD[13], Dynamicafter[78], I2VGen-XL[5]를 포함한 연구 작업과 비교한다. 비디오를 클릭하여 _Adobe Acrobat Reader를 통해 애니메이션 클립을 재생하십시오. 정적 프레임들은 부가 재료들에 제공된다._\n' +
      '\n' +
      'CLIP 임베딩 공간에서 연속적으로 생성된 프레임들 간의cosine 유사도는 시간적 일관성을 측정한다. (3) **Text 정렬(Text-Align)**: 생성된 동영상과 입력된 짧은 움직임 프롬프트 간의 의미 정렬 정도를 측정한다. 구체적으로, CLIP 텍스트와 이미지 인코더에 의해 추출된 특징을 사용하여 프롬프트와 생성된 각 프레임 간의 유사성 점수를 계산한다. (4) **FVD**: MSRVTT[80]로부터 1024개의 샘플에 대한 전체 생성 성능을 평가하기 위해 프레쳇 비디오 거리[65]를 보고한다. (5) **User Study**: 우리는 네 가지 다른 측면에 대한 사용자 연구를 수행한다. _ Mask-Corr_는 지역 애니메이션과 가이드 마스크의 대응관계를 평가한다. _ Motion_은 생성된 모션의 품질을 평가한다. _ Appearance_는 주어진 이미지와 함께 생성된 1번째 프레임의 일관성을 측정하고, _Overall_는 생성된 비디오의 주관적 품질을 평가한다. 우리는 32명의 피험자에게 이 네 가지 측면에서 다른 방법의 순위를 매길 것을 요청한다. 테이블에서요 도 1을 참조하면, 본 논문에서 제안하는 방법은 베이스라인들에 대해 최적의 비디오-텍스트 정렬 및 시간적 일관성을 달성함을 알 수 있다. 사용자 연구는 상용 제품에 비해 시간 일관성 및 입력 적합성 측면에서 우수한 성능을 얻으면서도 우수한 동작 품질을 보인다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**입력 이미지 마스크 비율.** 훈련에서 입력 이미지에 대한 첫 번째 프레임 마스킹 전략과 다른 마스크 비율의 영향을 조사하기 위해 마스크 비율을 0에서 0.9까지 변화시키는 정량적 실험을 수행하고 [12, 78], UCF-101 [63] 및 MSRVTT [80]에 대한 모든 방법의 생성 성능을 평가한다. FVD(Frechet Video Distance) [65] 및 PIC(Perceptual Input Conformity) [65]는 입력 이미지와 애니메이션 결과 사이의 지각적 일관성을 추가로 평가하는 것으로 보고된다. PIC는 \\(\\frac{1}{L}\\sum_{i=0}^{L-1}\\left(1-D(\\mathcal{I},x_{i})\\right)\\)에 의해 계산될 수 있으며, 여기서 \\(\\mathcal{I},x_{i},L\\은 입력 영상, 비디오 프레임 및 비디오 프레임이다.\n' +
      '\n' +
      '그림 4: 첫 번째 프레임 마스킹 전략의 마스킹 비율에 대한 **절제 연구. 다른 마스킹 비율은 생성 품질(FVD) 및 지각 입력 적합성(PIC)[78]에 유의한 영향을 미친다.**\n' +
      '\n' +
      '도 5: 상이한 마스킹 비율을 삭마한 **시각적 결과. 마스킹이 없는 훈련은 열악한 움직임, 시간적 일관성 및 비디오 품질을 나타낸다. 프롬프트는 "운전"입니다.**\n' +
      '\n' +
      'length, respectively. \\ (D(\\cdot,\\cdot)\\)는 지각 거리 메트릭 드림심[24]을 나타낸다. 본 논문에서는 16개의 프레임을 사용하여 256(\\times\\)256의 해상도에서 이러한 메트릭을 측정한다. 도 1에 도시된 바와 같다. 도 4를 참조하면, 최적 비율은 놀라울 정도로 높다. 70%의 비율은 두 가지 메트릭에서 최고의 성능을 얻습니다. 극도로 높은 마스크 비율은 입력 영상의 약한 조건으로 인해 생성된 영상의 품질 저하를 초래한다. 또한, 첫 번째 프레임 마스킹이 없는 훈련의 시각적 결과와 그림 4의 최적의 마스킹 비율을 비교한다. 결과로부터, 첫 번째 프레임 마스킹 훈련이 없는 모델은 정확한 시간적 움직임을 학습하지 못하고 잘못된 구조를 제시한다는 것을 관찰할 수 있다. 그런 다음 그림 6에서 마스킹된 입력 이미지와 생성된 비디오 프레임의 재구성 결과를 시각화하여 생성 과정에서 첫 번째 프레임이 합리적으로 재구성될 수 있고 생성된 비디오가 입력 이미지와 좋은 배경 일관성을 유지하는 것을 관찰할 수 있다.\n' +
      '\n' +
      '**Motion-augmented module.** 데이터셋과 Motion-augmented (MA) 모듈의 역할을 조사하기 위해 1) **Ours w/o D+M**, AnimateDiff[35]에서 설계된 기본 모션 모듈을 적용하고 WebVid-10M에서 모델을 미세조정한다. 2) **s w/o D**, 훈련 단계 동안, 제안된 방법을 최적화하기 위해 공개 웹Vid-10M만을 사용한다. MA 모듈의 입력은 WebVid-10M의 원래 프롬프트이다. 3) **Our\'s w/o M**, remove the MA module. 짧은 모션 관련 프롬프트는 공간 모듈에서 교차 주의로 공급된다. 우리는 또한 그림 7에서 정성적 비교를 수행한다. "우리의 w/o D+M"의 성능은 그것의 무능으로 인해 크게 감소한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c} \\hline  & \\multicolumn{4}{c}{Automatic Metrics} & \\multicolumn{4}{c}{User Study} \\\\ Method & \\(I_{1}\\)-MSE\\(\\downarrow\\) & Tem-Consis\\(\\uparrow\\) & Text-Align\\(\\uparrow\\) & FVD \\(\\downarrow\\) & Mask-Corr\\(\\downarrow\\) & Motion\\(\\downarrow\\) & Appearance\\(\\downarrow\\) & Overall \\(\\downarrow\\) \\\\ \\hline \\hline w/o Data \\& MA & 35.72 & 0.8465 & 0.3659 & 698.21 & 2.92 & 3.27 & 3.34 & 3.18 \\\\ w/o MA & **26.46** & **0.9178** & **0.6294** & **391.47** & **1.97** & **2.17** & **2.08** & **2.24** \\\\ w/o Data & 29.18 & 0.8824 & 0.4356 & 562.33 & 2.46 & 2.38 & 2.35 & 2.79 \\\\ \\hline Ours & **21.46** & **0.9613** & **0.7981** & **271.74** & **1.43** & **1.59** & **1.17** & **1.31** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **모션 증강 모듈(MA) 및 구축된 짧은 프롬프트 데이터세트(Data)**의 정량적 절제 결과. 최상의 방법은 빨간색으로 강조 표시되고 두 번째 방법은 파란색으로 강조 표시됩니다.\n' +
      '\n' +
      '그림 6: **Reconstruction and generation results of the masked first frame.** to clearly illustrate the performance of our reconstruction, we presented static frame, while _dynamic video are provided in the supplementary materials_.\n' +
      '\n' +
      '짧은 프롬프트 없이 입력 이미지를 의미적으로 이해하여 생성된 비디오에서 작은 움직임을 유도한다(2열 참조). MA 모듈을 제거하면 제한된 움직임 크기를 나타낸다. 우리는 표에서 설계된 모듈의 정량적 절제 연구를 보고한다. 2 및 Sec. 5.2와 동일한 설정을 적용하여 성능을 종합적으로 평가한다. 웹비드 모션 미세 조정을 제거하면 FVD 및 텍스트 정렬이 크게 감소한다. 이와는 대조적으로, 전체 방법은 자연스러운 움직임과 일관성 있는 프레임을 갖는 지역적 이미지 애니메이션을 효과적으로 달성한다.\n' +
      '\n' +
      '**동작 크기 제어.** 비교 결과를 그림 1에 제시한다. 8은 각각 FPS 기반 및 플로우 기반 모션 크기 제어를 위한 것이다. 우리는 FPS를 이용한 모션 제어가 충분히 정확하지 않다는 것을 관찰한다. 예를 들어, FPS=4와 FPS=8의 차이는 크지 않다(도 7의 제2 행). 대조적으로, 모션 제어를 위한 광학 흐름 크기(OFM)는 모션의 세기를 효과적으로 관리할 수 있다. OFM=4에서 OFM=16까지, **"Sad"**에 대한 동작 강도의 증가를 관찰하는 것이 명백하다. OFM=16에서 소녀는 고개를 숙이고 얼굴을 가리는 것으로 슬픔을 표현하는 것이 흥미롭다.\n' +
      '\n' +
      '도 8: 우리의 광학 흐름 모션 크기 제어(OFM)와 FPS 기반 모션 크기 제어(FPS) 간의 비교. 우리의 제어 방법은 움직임 세기를 효과적이고 거의 선형적으로 제어할 수 있다. _ 애니메이션 클립을 재생하려면 곡예사 판독기로 봅니다._\n' +
      '\n' +
      '도 7: 구성된 짧은 프롬프트 데이터세트(D) 및 모션-증강 모듈(M)을 절제하는 정성적 결과. 모션 프롬프트가 "실행 중"입니다.\n' +
      '\n' +
      '### Application\n' +
      '\n' +
      '**다중 영역 이미지 애니메이션.** 지역 프롬프터의 기술[7]을 사용하여 서로 다른 짧은 움직임 프롬프트에 의해 다중 영역 이미지 애니메이션을 달성할 수 있다. 그림 1의 왼쪽에 표시된 것처럼. 도 10을 참조하면, 우리는 각각 "걷기, 운전"을 사용하여 남성과 자동차를 애니메이션화할 수 있다. 비디오의 배경은 안정적이고, 선택된 객체들만이 애니메이션된다.\n' +
      '\n' +
      '**Regional Image Animation with ControlNet[85].** 또한, 본 프레임워크는 Conditional Regional Image Animation을 위한 ControlNet과 결합될 수 있다. 그림의 오른쪽에 있는 경우입니다. 도 10을 참조하여, 조건부 생성을 위한 포즈 컨디셔닝의 사용을 제시한다. 이는 배경의 안정성을 유지하면서 시간 일관성이 좋은 포즈 정렬 문자를 생성함을 보여준다.\n' +
      '\n' +
      '## 6 Limitation\n' +
      '\n' +
      '우리의 접근법은 클릭과 짧은 모션 프롬프트 제어를 가능하게 하지만 그림과 같이 크고 복잡한 모션을 생성하는 도전에 직면해 있다. 10. 이는 모션의 복잡성 및 데이터세트 바이어스, _예를 들어, 트레이닝 데이터세트는 복잡한 모션을 갖는 제한된 샘플들을 포함하기 때문일 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 제어 가능한 애니메이션과 로컬 애니메이션의 생성 문제를 해결하기 위해 Follow-Your-Click을 제시한다. 우리가 아는 한, 우리는 간단한 _click_와 _short 모션 관련 프롬프트를 통해 지역 이미지 애니메이션이 가능한 최초의 I2V 프레임워크이다. 이를 지원하기 위해 먼저 사용자 친화적인 상호 작용을 위해 신속한 분할 도구 SAM을 프레임워크에 통합한다. 본 논문에서는 짧은 프롬프트 추종 능력을 달성하기 위해 모션 증강 모듈과 이 목표를 달성하기 위해 구성된 짧은 프롬프트 데이터셋을 제안한다. 생성된 시간적 움직임 품질을 향상시키기 위해, 본 논문에서는 생성 성능을 크게 향상시키는 첫 번째 프레임 마스킹 기법을 제안한다. 움직임 속도에 대한 정확한 학습이 가능하도록 광학 흐름 점수를 활용하여 움직임의 크기를 정확하게 제어한다. 우리의 실험 결과는 기존의 기준선에 비해 우리의 접근법의 효율성과 우수성을 강조한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '우리는 그들의 도움이 되는 논평에 대해 Jiaxi Feng, Yabo Zhang, Wenzhe Zhao, Mengyang Liu, Jianbing Wu 그리고 Qi Tian에게 감사한다. 이 프로젝트는 보조금 번호 2022ZD0161501로 중국의 국가 핵심 R&D 프로그램에서 지원되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]C. Arbel, P. B. Blattmann, T. 도크혼 멘델리비치 킬리안, D. 로렌츠, Y. 리바이, 지 영어, V Voleti, A. Letts, et al.(2023) Stable video diffusion: latent video diffusion models to large datasets. ArXiv:2311.15127. 인용: SS1.\n' +
      '*[2]A. 블랫만 밀비치 Dorkenwald, and B. Ommer (2021) Understanding object dynamics for interactive image-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5171-5181. Cited by: SS1.\n' +
      '*[3]A. R. 블랫만 롬바흐 H. 링, T. 도크혼, 김승원 피들러 Kreis와 K. 잠복기 정렬: 잠재 확산 모델과 고해상도 비디오 합성 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22563-22575. Cited by: SS1.\n' +
      '*[4]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[5]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[6]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[7]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[8]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[9]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[10]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[11]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[12]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[13]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[14]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[15]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[16]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[17]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[18]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[19]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[20]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[21]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[22]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[23]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[24]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[25]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[26]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[27]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[28]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[29]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[30]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[31]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[32]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[33]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[34]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[35]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[36]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[37]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[38]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[39]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[40]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[41]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '*[42]W. 채선 곽규왕, Y. Lu(2023) Stablevideo: text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23040-23050. Cited by: SS1.\n' +
      '\n' +
      '* [14] Chen, W., Wu, J., Xie, P., Wu, H., Li, J., Xia, X., Xiao, X., Lin, L.: Control-a-video: Controllable text-to-video generation with diffusion models (2023)\n' +
      '* [15] Chen, X., Liu, Z., Chen, M., Feng, Y., Liu, Y., Shen, Y., Zhao, H.: Livephoto: Real image animation with text-guided motion control. arXiv preprint arXiv:2312.02928 (2023)\n' +
      '* [16] Chen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J., Wang, Y., Lin, D., Qiao, Y., Liu, Z.: Seine: Short-to-long video diffusion model for generative transition and prediction. arXiv preprint arXiv:2310.20700 (2023)\n' +
      '* [17] Cheng, C.C., Chen, H.Y., Chiu, W.C.: Time flies: Animating a still image with time-lapse video as reference. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5641-5650 (2020)\n' +
      '* [18] Cheng, Y., Li, L., Xu, Y., Li, X., Yang, Z., Wang, W., Yang, Y.: Segment and track anything. arXiv preprint arXiv:2305.06558 (2023)\n' +
      '* [19] Dai, Z., Zhang, Z., Yao, Y., Qiu, B., Zhu, S., Qin, L., Wang, W.: Animateanything: Fine-grained open domain image animation with motion guidance. arXiv e-prints pp. arXiv-2311 (2023)\n' +
      '* [20] Ding, M., Zheng, W., Hong, W., Tang, J.: Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems **35**, 16890-16902 (2022)\n' +
      '* [21] Dorkenwald, M., Milbich, T., Blattmann, A., Rombach, R., Derpanis, K.G., Ommer, B.: Stochastic image-to-video synthesis using cinns. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3742-3753 (2021)\n' +
      '* [22] Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure and content-guided video synthesis with diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7346-7356 (2023)\n' +
      '* [23] Feichtenhofer, C., Li, Y., He, K., et al.: Masked autoencoders as spatiotemporal learners. Advances in neural information processing systems **35**, 35946-35958 (2022)\n' +
      '* [24] Fu, S., Tamir, N., Sundaram, S., Chai, L., Zhang, R., Dekel, T., Isola, P.: Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344 (2023)\n' +
      '* [25] Gao, K., Bai, J., Wu, B., Ya, M., Xia, S.T.: Imperceptible and robust backdoor attack in 3d point cloud. IEEE Transactions on Information Forensics and Security **19**, 1267-1282 (2023)\n' +
      '* [26] Gao, K., Bai, Y., Gu, J., Xia, S.T., Torr, P., Li, Z., Liu, W.: Inducing high energy-latency of large vision-language models with verbose images. In: ICLR (2024)\n' +
      '* [27] Geng, J., Shao, T., Zheng, Y., Weng, Y., Zhou, K.: Warp-guided gans for single-photo facial animation. ACM Transactions on Graphics (ToG) **37**(6), 1-12 (2018)\n' +
      '* [28] Guo, X., Zheng, M., Hou, L., Gao, Y., Deng, Y., Ma, C., Hu, W., Zha, Z., Huang, H., Wan, P., et al.: I2v-adapter: A general image-to-video adapter for video diffusion models. arXiv preprint arXiv:2312.16693 (2023)\n' +
      '* [29] Guo, Y., Yang, C., Rao, A., Agrawala, M., Lin, D., Dai, B.: Sparsectrl: Adding sparse controls to text-to-video diffusion models (2023)\n' +
      '* [30] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023)* [31] He, C., Li, K., Zhang, Y., Tang, L., Zhang, Y., Guo, Z., Li, X.: Camouflaged object detection with feature decomposition and edge reconstruction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22046-22055 (2023)\n' +
      '* [32] He, C., Li, K., Zhang, Y., Xu, G., Tang, L., Zhang, Y., Guo, Z., Li, X.: Weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping. arXiv preprint arXiv:2305.11003 (2023)\n' +
      '* [33] He, C., Li, K., Zhang, Y., Zhang, Y., Guo, Z., Li, X., Danelljan, M., Yu, F.: Strategic preys make acute predators: Enhancing camouflaged object detectors by generating camouflaged objects (2024)\n' +
      '* [34] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022)\n' +
      '* [35] He, Y., Xia, M., Chen, H., Cun, X., Gong, Y., Xing, J., Zhang, Y., Wang, X., Weng, C., Shan, Y., et al.: Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940 (2023)\n' +
      '* [36] He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221 (2022)\n' +
      '* [37] Hinz, T., Fisher, M., Wang, O., Wermter, S.: Improved techniques for training single-image gans. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1300-1309 (2021)\n' +
      '* [38] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022)\n' +
      '* [39] Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)\n' +
      '* [40] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video diffusion models. arXiv:2204.03458 (2022)\n' +
      '* [41] Holynski, A., Curless, B.L., Seitz, S.M., Szeliski, R.: Animating pictures with eulerian motion fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5810-5819 (2021)\n' +
      '* [42] Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.: Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 (2022)\n' +
      '* [43] Jhou, W.C., Cheng, W.H.: Animating still landscape photographs through cloud motion creation. IEEE Transactions on Multimedia **18**(1), 4-13 (2015)\n' +
      '* [44] Karras, J., Holynski, A., Wang, T.C., Kemelmacher-Shlizerman, I.: Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint arXiv:2304.06025 (2023)\n' +
      '* [45] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8110-8119 (2020)\n' +
      '* [46] Li, Z., Tucker, R., Snavely, N., Holynski, A.: Generative image dynamics. arXiv preprint arXiv:2309.07906 (2023)\n' +
      '* [47] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n' +
      '* [48] Ma, Y., Cun, X., He, Y., Qi, C., Wang, X., Shan, Y., Li, X., Chen, Q.: Magic-stick: Controllable video editing via control handle transformations. arXiv preprint arXiv:2312.03047 (2023)49] Ma, Y., He, Y., Cun, X., Wang, X., Shan, Y., Li, X., Chen, Q.: Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186 (2023)\n' +
      '* [50] Ma, Y., Yang, T., Shan, Y., Li, X.: Simvtp: Simple video text pre-training with masked autoencoders. arXiv preprint arXiv:2212.03490 (2022)\n' +
      '* [51] Mallya, A., Wang, T.C., Liu, M.Y.: Implicit warping for animation with image sets. Advances in Neural Information Processing Systems **35**, 22438-22450 (2022)\n' +
      '* [52] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n' +
      '* [53] Prashnani, E., Noorkami, M., Vaquero, D., Sen, P.: A phase-based approach for animating images using video examples. In: Computer Graphics Forum. vol. 36, pp. 303-311. Wiley Online Library (2017)\n' +
      '* [54] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: International Conference on Machine Learning. pp. 8821-8831. PMLR (2021)\n' +
      '* [55] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [56] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. pp. 234-241. Springer (2015)\n' +
      '* [57] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [58] Salimans, T., Ho, J.: Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 (2022)\n' +
      '* [59] Shaham, T.R., Dekel, T., Michaeli, T.: Singan: Learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 4570-4580 (2019)\n' +
      '* [60] Shi, X., Huang, Z., Wang, F.Y., Bian, W., Li, D., Zhang, Y., Zhang, M., Cheung, K.C., See, S., Qin, H., et al.: Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. arXiv preprint arXiv:2401.15977 (2024)\n' +
      '* [61] Siarohin, A., Woodford, O.J., Ren, J., Chai, M., Tulyakov, S.: Motion representations for articulated animation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13653-13662 (2021)\n' +
      '* [62] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)\n' +
      '* [63] Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)\n' +
      '* [64] Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16. pp. 402-419. Springer (2020)\n' +
      '* [65] Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Fvd: A new metric for video generation (2019)* [66] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems **30** (2017)\n' +
      '* [67] Voleti, V., Jolicoeur-Martineau, A., Pal, C.: Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in Neural Information Processing Systems **35**, 23371-23385 (2022)\n' +
      '* [68] Wang, F.Y., Chen, W., Song, G., Ye, H.J., Liu, Y., Li, H.: Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264 (2023)\n' +
      '* [69] Wang, F.Y., Huang, Z., Shi, X., Bian, W., Song, G., Liu, Y., Li, H.: Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. arXiv preprint arXiv:2402.00769 (2024)\n' +
      '* [70] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang, S.: Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 (2023)\n' +
      '* [71] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., Zhou, J.: Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018 (2023)\n' +
      '* [72] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., Zhou, J.: Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [73] Wang, Y., Yang, D., Bremond, F., Dantcheva, A.: Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043 (2022)\n' +
      '* [74] Weng, C.Y., Curless, B., Kemelmacher-Shlizerman, I.: Photo wake-up: 3d character animation from a single photo. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5908-5917 (2019)\n' +
      '* [75] Xiao, W., Liu, W., Wang, Y., Ghanem, B., Li, B.: Automatic animation of hair blowing in still portrait photos. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 22963-22975 (2023)\n' +
      '* [76] Xiao, Y., Luo, Z., Liu, Y., Ma, Y., Bian, H., Ji, Y., Yang, Y., Li, X.: Bridging the gap: A unified video comprehension framework for moment retrieval and highlight detection. arXiv preprint arXiv:2311.16464 (2023)\n' +
      '* [77] Xing, J., Xia, M., Liu, Y., Zhang, Y., Zhang, Y., He, Y., Liu, H., Chen, H., Cun, X., Wang, X., et al.: Make-your-video: Customized video generation using textual and structural guidance. arXiv preprint arXiv:2306.00943 (2023)\n' +
      '* [78] Xing, J., Xia, M., Zhang, Y., Chen, H., Wang, X., Wong, T.T., Shan, Y.: Dynami-crafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190 (2023)\n' +
      '* [79] Xiong, W., Luo, W., Ma, L., Liu, W., Luo, J.: Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2364-2373 (2018)\n' +
      '* [80] Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288-5296 (2016)\n' +
      '* [81] Xue, H., Hang, T., Zeng, Y., Sun, Y., Liu, B., Yang, H., Fu, J., Guo, B.: Advancing high-resolution video-language representation with large-scale video transcriptions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5036-5045 (2022)\n' +
      '* [82] Yan, W., Zhang, Y., Abbeel, P., Srinivas, A.: Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157 (2021)* [83] Yu, J., Li, X., Koh, J.Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., Wu, Y.: Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627 (2021)\n' +
      '* [84] Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789 (2022)\n' +
      '* [85] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models (2023)\n' +
      '* [86] Zhang, Y., Wei, Y., Jiang, D., Zhang, X., Zuo, W., Tian, Q.: Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077 (2023)\n' +
      '* [87] Zhang, Y., Xing, Z., Zeng, Y., Fang, Y., Chen, K.: Pia: Your personalized image animator via plug-and-play modules in text-to-image models (2023)\n' +
      '* [88] Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., Feng, J.: Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>