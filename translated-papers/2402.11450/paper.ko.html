<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 인간 피드백으로부터 더 빨리 배우기 위한 학습\n' +
      '\n' +
      '언어 모델 예측 제어\n' +
      '\n' +
      '재키 량\\({}^{*}\\), 페이 샤\\({}^{*}\\), 원하오 유\\({}^{*}\\), 앤디 젱\\({}^{*}\\)\n' +
      '\n' +
      '몬세라트 곤잘레스 아레나스, 마리아 아테리안, 마리아 바우자, 매튜 베니스, 알렉스 베일리, 아딜 도스트모하메드 추위안 켈리 푸\n' +
      '\n' +
      '니메로드 길레디, 마리사 주스티나, 키르타나 고팔라크리시난, 레너드 하젠클레버, 얀 험플릭, 자스민 허수, 니킬 조시, 벤 조베니스, 체이스 큐, 션 키르마니, 상웨이 에드워드 리, 아사프 허비츠 미채리, 조스 무어, 켄 오슬런드\n' +
      '\n' +
      'Dushyant Rao, Allen Ren, Baruch Tabanpour, Quan Vuong, Ayzana Wahid, Ted Xiao, Ying Xu, Vincent Zhuang\n' +
      '\n' +
      'Peng Xu\\({}^{\\dagger}\\), Erik Frey\\({}^{\\dagger}\\), Ken Caluwaerts\\({}^{\\dagger}\\), Tingnan Zhang\\({}^{\\dagger}\\), Brian Ichter\\({}^{\\dagger}\\), Jonathan Tompson\\({}^{\\dagger}\\), Leila Takayama\\({}^{\\dagger}\\), Vincent Vanhoucke\\({}^{\\dagger}\\)\n' +
      '\n' +
      'Izhak Shafran\\({}^{\\dagger}\\), Maja Mataric\\({}^{\\dagger}\\), Dorsa Sadigh\\({}^{\\dagger}\\), Nicolas Heess\\({}^{\\dagger}\\), Kanishka Rao\\({}^{\\dagger}\\), Nik Stewart\\({}^{\\dagger}\\), Jie Tan\\({}^{\\dagger}\\), Carolina Parada\\({}^{\\dagger}\\).\n' +
      '\n' +
      '({}^{\\dagger}\\)advising lead, all other authors in alphabetical order.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 언어 명령으로부터 로봇 코드를 작성하는 것과 같은 광범위한 능력을 나타내는 것으로 나타났는데, 이는 비전문가가 로봇 행동을 지시하거나 피드백을 기반으로 수정하거나 새로운 작업을 수행하도록 구성하는 것을 가능하게 한다. 그러나 이러한 기능(컨텍스트 내 학습에 의해 주도됨)은 단기적인 상호 작용으로 제한되며, 여기서 사용자의 피드백은 LLM의 컨텍스트 크기 내에 맞는 한에만 관련이 있으며 더 긴 상호 작용에서 잊혀질 수 있다. 본 연구에서는 로봇 코드 작성 LLM의 미세 조정을 통해 상황 내 상호 작용을 기억하고, 인간 입력에 얼마나 효율적으로 적응하는지(사용자가 작업을 성공적으로 고려하기 전에 평균 수정 횟수로 측정) 개선한다. 우리의 주요 관찰은 인간-로봇 상호 작용이 부분적으로 관찰 가능한 마르코프 결정 과정(인간 언어 입력이 관찰이고 로봇 코드 출력이 액션인 경우)으로 공식화되면 이전 상호 작용을 완료하도록 LLM을 훈련시키는 것은 전환 역학 모델을 훈련시키는 것으로 볼 수 있으며, 이는 더 짧은 성공 경로를 발견하기 위해 모델 예측 제어(MPC)와 같은 고전적인 로봇 기술과 결합될 수 있다. 이는 5개의 로봇 구현에 걸쳐 78개의 태스크에 대한 도달성을 향상시키기 위해 PALM 2를 미세 조정하는 프레임워크인 LMPC(Language Model Predictive Control)를 생성하는데, 이는 보이지 않는 태스크의 평균 교정의 수를 \\(2.4\\)에서 \\(1.9\\)으로 줄이면서 비전문가 교육 성공률을 \\(26.9\\%\\) 향상시킨다. 실험 결과 LMPC는 강력한 메타 학습자를 생성하여 보이지 않는 로봇 실시예 및 API에 대한 상황 내 새로운 태스크의 성공률을 \\(31.5\\%\\) 향상시켰다. [https://robot-teaching.github.io/](https://robot-teaching.github.io/)에서 비디오, 코드 및 데모를 참조하십시오.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '자연어는 로봇을 가르칠 수 있는 풍부하고 접근 가능한 인터페이스를 제공하며, 최소한의 훈련을 통해 누구나 행동을 지시하고 선호도를 표현하며 피드백을 제공할 수 있는 잠재력을 가지고 있다. 최근 연구에 따르면 인터넷 규모의 데이터에 미리 훈련된 대규모 언어 모델(LLM)은 언어 명령[1, 30]이 주어진 일련의 단계를 계획하는 것부터 로봇 코드 작성[41, 59, 71, 45]까지 로봇 공학에 적용할 수 있는 즉각적인 기능을 보여준다. 언어 입력은 또한 예를 들어, 실시간 모션 제어[71]를 통해 새로운 사족 동작을 구성하기 위해 인간 피드백으로부터 보상 함수 코드를 생성 및 수정하기 위해 다중 턴 설정에서 시퀀싱될 수 있다(도 1의 예).\n' +
      '\n' +
      'LLM 기반 로봇 티칭(도 1에 도시된 바와 같이). 1) 인-컨텍스트 학습 [9] (예를 들어, 코드 및 대화 데이터 상에서) 에 의해 구동될 수 있고, 여기서 이전 상호작용들은 후속 것들에 대한 입력 컨텍스트로서 유지된다. 컨텍스트 내 학습은 모델 가중치에 대한 그래디언트 업데이트 없이 추론 동안 발생하여 언어 명령어(예제 기반 구성 일반화[11, 32])에 대한 빠른 적응을 가능하게 한다. 그러나 이러한 적응은 LLM의 컨텍스트 크기 내에 맞는 한 사용자의 피드백이 여전히 관련이 있는 단기 반응 상호작용으로 제한된다. 결과적으로 인간의 지시가 퇴행 외부에 있는 더 긴 다단계 상호 작용에 걸쳐 축적되면\n' +
      '\n' +
      '도. 1: 코드-쓰기 대형 언어 모델들(LLM)은 비전문가들이 로봇들에게 언어로 새로운 작업들을 가르칠 수 있는 기회들을 제시한다 - 인-컨텍스트 학습(좌측)을 통한 빠른 적응에 의해 인에이블된다. 이 작업에서 기본 LLM을 미세 조정하여 빠른 적응을 더욱 가속화하고 도달 가능성(오른쪽)을 향상시킨다. 78개의 태스크(회색)에서 로봇 실시예를 가르치는 비전문가로부터 인간-로봇 상호작용을 얻은 결과, 우리의 프레임워크(중간({}^{*}\\))는 상위 수행 사용자(보라색)를 식별하고, 그들의 상호작용을 활용하여 모든 사용자(파란색)에 대한 LLM 성능 개선을 유도할 수 있음을 보여준다. 즉, 보이지 않는 태스크에 대한 성공률, 사용자 피드백에 대한 응답성, 사용자 수정 횟수 측면에서 측정된다. 실험들은 이러한 개선들이 새로운 로봇 실시예들 및 API들에 일반화되는 것을 보여준다.\n' +
      '\n' +
      '맥락 지평선, 이전의 지시들은 단순히 잊혀질 수 있다.\n' +
      '\n' +
      '우리는 로봇 작업에 대한 LLM의 _teachability_, 즉 LLM이 컨텍스트 내 상호 작용을 기억할 수 있도록 함으로써 인간 피드백에 얼마나 효율적으로 적응하는지 개선하는 데 관심이 있다. 멀티턴 언어 기반 인간-로봇 상호작용(HRI)에서의 교수 가능성은 로봇이 작업에서 성공하기 전에 인간 입력(예를 들어, 수정)의 평균 수 \\(n\\)로 측정될 수 있다. 예를 들어 \\(n\\!=\\!1\\) 설정 [33, 44] 이후의 표준 제로 샷 명령어를 나타낸다. 선행 연구는 인간 피드백[75] 또는 선호도[68]의 언어적 요약을 생성하여 메모리에 인덱싱하고 나중에 컨텍스트 내에서 검색하여 미래의 상호 작용을 안내함으로써 교시성을 향상시키는 것을 제안한다. 그러나, 그러한 방법들은 종종, 미세 조정을 통한 인-가중치 학습으로부터의 일반화와 대조적으로(보다 "예시적인-기반"인 것으로 관찰됨), 인-컨텍스트 학습 일반화(즉, 인-컨텍스트 예[11, 57])에 의해 제한된다(이는 트레이닝 데이터 [11, 6]에서 카테고리 경계를 지원하는 최소 특징들에 기초하여, 보다 "규칙-기반"인 경향이 있다). 그 후, 이전의 방법들은 훈련 작업들에 과적합하는 것에 탁월하지만, 보이지 않는 작업들에 대한 제한된 일반화(예를 들어, 도메인-레벨 적응)를 제공한다. 이러한 단점을 해결하기 위해 두 가지 형태의 학습을 모두 활용하는 것이 가능합니까?\n' +
      '\n' +
      '본 연구에서는 로봇 코드 작성 LLM의 교수 가능성을 낮에는 _in-context learning_ (fast adaptation)을 통해 향상시키고, 밤에는 모델 _fine-tuning_ (slow adaptation)을 통해 향상시키고, 다음 날에는 빠른 적응을 가속화하도록 조사한다. 1 비전문가들이 로봇에게 언어로 새로운 작업을 가르치는 설정을 고려할 때, 본 연구의 목표는 상황 내 학습에서 수집된 데이터를 가장 잘 활용하여 미래의 교수 가능성을 향상시킬 수 있는 방법(예: fine-tuning을 통해)을 연구하는 것이다. 우리의 주요 관찰은 인간-로봇 상호작용이 부분적으로 관찰 가능한 마르코프 결정 과정(POMDP - 인간 언어 입력은 관찰이고 로봇 코드 출력은 행동)으로 공식화될 때, LLM을 자동으로 완성하도록 훈련하는 것은 성공에 이르는 더 짧은 경로를 발견하기 위해 모델 예측 제어(MPC)와 같은 고전적인 로봇 기술과 결합될 수 있는 전이 역학 모델을 훈련하는 것으로 볼 수 있다. 이는 인간-로봇 상호작용의 상상된 미래 롤아웃을 예측하도록 LLM(Language Model Predictive Control)을 훈련하는 LMPC(Language Model Predictive Control)를 발생시키며, 추론 시간에는 가장 좋은 것을 검색하고 다음 조치(즉, 디코딩 전략으로 지평선 제어 감소)를 취하기 위해 다중 선물(0이 아닌 디코딩 온도를 갖는)을 샘플링한다. (개별 사용자 선호도를 모델링하는 것과 같은) 고전적으로 도전적인 HRI 문제는 예를 들어 사용자 이름("사용자 _might say...")에서 LMPC 롤아웃을 단순히 컨디셔닝함으로써 더 간단해진다.\n' +
      '\n' +
      '각주 1: 주간에 의한 맥락 내 학습과 밤에 의한 미세 조정은 생물학적 시스템의 학습과 기억에서 일주기 리듬의 역할에 의해 느슨하게 영감을 받는다.\n' +
      '\n' +
      '광범위한 실험(맹인 A/B 평가를 통한)을 통해 LMPC로 미세 조정하면 5개의 로봇 실시예(시뮬레이션 및 실제 플랫폼)에 걸쳐 78개의 작업에서 PaLM 2[3]의 교시성이 향상되어 비전문가가 보이지 않는 작업에서 더 높은 성공률을 달성하도록 로봇을 교시할 수 있으며 평균 인간 교정의 수를 \\(26.9\\%\\)에서 \\(2.4\\)에서 \\(1.9\\)으로 줄임을 보여준다. 특히, LMPC는 강력한 메타 학습자를 생성하는데, 교수성 향상은 보이지 않는 실시예로 일반화하여 새로운 로봇 API로 새로운 태스크를 학습하는 성공률을 \\(31.5\\%\\) 향상시킨다. 흥미롭게도, 우리는 (i) 최상위 사용자를 자율적으로 식별하고 (ii) 특정 사용자 이름 "상위 사용자"와 함께 데이터를 그룹화한 다음 (iii) 이 특정 사용자 이름에 대한 추론 시간 LMPC 롤아웃(즉, 모든 사람이 최상위 사용자라고 가정)을 조건화하는 최상위 사용자 조건화된 LMPC의 상당한 이득을 관찰한다. 상위 사용자가 14%의 태스크만을 보았음에도 불구하고, 실험을 통해 본 컨디셔닝 메커니즘이 보이지 않는 태스크를 포함한 모든 태스크에 대해 \\(10.5\\%\\)의 성능 향상을 유도함을 보인다. LMPC는 또한 검색 기준[75]을 능가하며, 사용자 연구는 성능 개선이 사용자 교수 숙련도보다는 모델 능력의 변화의 결과일 가능성이 있음을 확인한다. 우리의 접근 방식은 제한이 없는 것이 아니다 - 우리는 섹션 V에서 향후 작업을 위한 이러한 영역과 영역에 대해 논의한다. 비디오, 코드 및 데이터 세트가 출시됩니다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**언어 및 로보틱스.** 계획 프리미티브에 언어를 매핑하는 것[63, 37, 46, 5, 35], 언어 명령어와 함께 시연으로부터 모방 학습[33, 44, 58, 60, 47], 언어 조건 보상 기능을 학습하는 것[48, 34, 21, 14, 50, 2], 언어를 수정 피드백으로 사용하여 새로운 행동을 적응시키거나 정의하는 것[75, 16, 15]을 포함하는 언어 및 로보틱스를 통합한다. 우리는 이 분야의 사전 작업에 대한 보다 완전한 검토를 위해 독자를 포괄적인 조사에 회부한다[64, 43].\n' +
      '\n' +
      '최근 인터넷 규모의 데이터를 학습한 LLM은 단계별 계획[1, 30, 18, 69, 17, 42, 68, 54], 로봇 코드 작성[41, 59, 73, 70, 4, 4], 상식 추론[62, 39], 인간의 선호도를 포착하는 대리 보상 기능[38, 29, 71]에 이르기까지 심오한 기능을 발휘하는 것으로 나타났다. 이 작업에서는 언어 피드백[56, 74, 54, 75, 31]을 통해 새로운 행동을 적응하고 가르치는 LLM의 힘을 활용하는 데에도 관심이 있지만 이전 작업과 달리 상황 내 학습(예: LLM 촉진)을 통해 온라인 적응을 평가하는 것뿐만 아니라 오프라인 모델 미세 조정을 통해 적응을 개선할 수 있는 방법에 중점을 둔다.\n' +
      '\n' +
      '**In-Context Learning for Robot Adaptation.**In-Context Learning은 감독 메타 학습[9]의 한 형태로서, 동일한 데이터세트로부터의 다수의 예제 및 명령어[51]는 비감독 자기회귀 완성 목표를 갖는 LLM에 입력으로서 공급되는 컨텍스트 버퍼에 순차적으로 패킹된다[66]. 명령어들 및 예들은 태스크들("태스크 프리픽스들", 즉 미리 정의된 토큰 시퀀스들[53, 52]의 개념을 확장)을 특정하며, 여기서 모델은 다음에 오는 것을 예측함으로써 태스크의 추가 인스턴스들을 완료할 것으로 예상된다.\n' +
      '\n' +
      '로봇 공학에서 상황 내 학습은 피드백에 응답[74, 31], 하위 수준 행동을 수정[7, 55, 49, 4, 38], 사용자 선호도를 기억하고 적용하는[68], 도움을 요청하는[54] 등 광범위한 기능을 이끌어내는 데 사용되었다. 우리의 작업과 관련된 대부분은 Zha et al. [75]이며, 이는 인간의 피드백을 요약하여 로봇 교시를 조사하고 이를 메모리에 인덱싱하여 검색을 통한 유사한 미래 상호 작용을 위한 컨텍스트 내 예제로 다시 사용된다(예: 검색 증강 생성(RAG) [40]. 대조적으로, 우리는 인간 언어 입력(다중 라운드 컨텍스트일 수 있음)으로부터 맥락 내 학습을 개선하기 위해 기본 LLM을 직접 미세 조정하는 데 중점을 둔다. 우리는 추가적인 외부 모듈 없이 보이지 않는 작업을 가르치는 검색 기반 방법의 성능을 능가한다는 것을 발견한다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '**LLM 정렬을 사용자 피드백으로 개선.** 우리의 작업은 사용자 의도를 가진 _align_LLM에 대한 활발한 연구 영역을 기반으로 한다[51]. 일반적인 접근법은 전문가 인간 입력 및 출력에 대한 모델을 감독 미세 조정(SFT)한 다음 모델 출력에서 비전문가 라벨링된 순위(선호도)를 사용하여 인간 피드백(RLHF)으로부터 강화 학습을 위한 보상 모델을 훈련하는 것이다[13, 61, 7]. 그러나 이러한 작업은 종종 단일 사용자 입력에서 선호하는 출력(예: 단일 회전 대화)으로의 매핑에 중점을 둔다. 본 연구에서는 인간 피드백에 의한 학습도 조사하지만, 다중회전 상호작용 인간 피드백을 기반으로 로봇 코드를 작성하고 개선하는 LLM의 교시성을 향상시키는 데 중점을 둔다. 우리의 LMPC 접근법은 인간-로봇 상호작용 역학을 모델링하기 위해 SFT를 사용하고, 추론-시간 탐색 및 후퇴 지평선 제어를 사용하여 태스크 성공에 대한 더 짧은 경로(더 적은 수의 수정 라운드)를 발견한다.\n' +
      '\n' +
      '## III 언어 모델 예측 제어\n' +
      '\n' +
      '우리는 사용자가 MuJoCo 시뮬레이션 엔진 [65]를 사용하여 로봇 및 그 주변의 시뮬레이션 시각화 옆에 채팅과 같은 인터페이스를 통해 텍스트 메시지를 통해 로봇과 통신하는 언어 기반 인간-로봇 상호작용의 맥락에서 교시 가능성을 조사한다(도 3 참조, 부록 VI-L에서 더 자세한 내용). 사용자 메시지는 자유 형식이며 사용자의 재량에 따라 제공됩니다. 명령, 환경설정, 피드백 등을 포함할 수 있습니다. 각 메시지에 응답하여, 시스템은 로봇 코드를 출력하며, 로봇 코드는 모의 또는 실제 로봇 상의 실시간 모션 제어기로 직접 전송된다(섹션 III-B). 그런 다음 사용자는 관찰된 로봇 동작에 기초하여 후속 피드백을 제공한다.\n' +
      '\n' +
      '각각의 인간-로봇 대화(즉, 채팅 세션)는 목표-구동된다: 사용자들은 세션당 하나의 태스크를 가르치도록 요청받고, 각 세션의 끝에서 로봇이 태스크를 완료했다고 믿는지 여부에 따라 "성공" 또는 "실패" 라벨이 결정된다. 채팅 세션들은 성공하기 전에 다수의 채팅 턴들(즉, 인간-로봇 입력-출력 쌍들)로 구성될 수 있다. 평균적으로, 성공적인 세션은 2-3회의 채팅 턴 동안 실행되는 반면, 실패 세션은 5-6회의 채팅 턴 동안 실행된다(도 3 참조; 왼쪽 하단에 표시된 막대 플롯). 사용자 메시지는 보다 복잡한 것을 결합하기 위한 수정 또는 세분화된 단계별 하위 작업일 수 있으며 일반적으로 다중 라운드 컨텍스트이다. 데이터 수집 동안 사용자는 개별 로봇 응답을 "좋음" 또는 "나쁨"으로 평가합니다. - 로봇이 가장 최근의 인간 피드백에 올바르게 응답했다면(아직 전체 작업을 완료하는 데 성공하지 못할 수 있지만), 그렇지 않으면 좋지 않습니다. 우리는 좋은 채팅 순번 등급의 비율이 작업 성공과 상관관계가 있음을 발견한다(그림 3, 오른쪽 하단).\n' +
      '\n' +
      '### _Problem Statement_\n' +
      '\n' +
      '우리의 목표는 로봇 코드를 작성하기 위해 인간의 지시와 피드백을 따르는 LLM의 교시성을 향상시키는 것이다. 가르침성은 로봇이 작업에서 성공하기 전에 인간 입력(채팅 턴)의 평균 수 \\(n\\)로 정의된다. 이 메트릭은 로봇이 인간의 입력에 얼마나 효율적으로 적응하는지를 측정하며, \\(n=1\\)은 설정 [33, 44] 이후의 표준 제로 샷 명령과 동일하다. _improve_ teachingability는 원하는 성공률 이전에 채팅 턴 수 \\(n\\)를 줄이는 것으로, 메타-학습 목적, 즉 인간 피드백으로부터 더 빨리 학습하는 학습으로 볼 수 있다[26]. 직관적으로, 모델의 교시성을 향상시키는 것은, (사용자에 따라) 올바른 행동을 생성할 가능성을 최대화하기 위한 수단으로서, 피드백에 대한 그것의 반응성을 장려해야 한다. 가르침성은 또한 모델이 선호도에 얼마나 잘 적응하는지 반영할 수 있다. 예를 들어, "왼쪽으로 약간 이동"하는 사용자 입력은 사용자에 따라 다른 로봇 행동 수정을 산출할 수 있다 - 강한 메타 학습자는 (가르침성과 관련하여) 이 차이를 학습하여 상호작용하는 사람과 조건화된 상호작용의 수를 최소화할 수 있다.\n' +
      '\n' +
      '언어 기반 인간-로봇 상호작용은 부분적으로 관찰 가능한 마르코프 결정 과정으로 공식화되며, 정책은 로봇(동작)에서 동작 제어를 통해 실행되는 코드를 통해 인간 교사(환경의 일부)와 상호작용하고, 인간은 자연 언어 피드백(관찰)을 제공하고 교수 세션의 성공(보상)을 나타낸다. 이 정책의 목표는 로봇이 인간이 의도한 대로 행동하도록 유도하는 코드를 생성하는 것이지만, 이러한 목표 행동은 인간의 피드백으로부터 추론되어야 한다. 수학적으로, \\(s_{t}\\in\\mathcal{S}\\)는 시간에서의 관측되지 않은 인간(사용자) 상태이고, 인간 텍스트 입력은 상태의 관찰 \\(o_{t}\\in\\mathcal{O}\\)이고, 에이전트(LLM)는 정책 \\(\\pi(o_{t}|o_{0},\\ldots,o_{t-1})에 따라 행동 \\(a_{t}\\in\\mathcal{A}\\)으로서 코드를 생성한다. \\(o_{t}|o_{0},\\ldots,o_{t-1})\\)은 로봇에서 실행된다. (\\mathcal{P}(o_{t+1},r_{t}|o_{<t},a_{t}\\leq t,r<t)\\)는 인간 상호작용의 전이 확률로서, 임의의 시간에 사용자가 태스크를 "성공"이라고 결정하면, 에피소드는 희소 보상 \\(r=1\\)으로 종료되고, 채팅 세션은 종료된다. 로봇이 7시간 이상 개선하려고 고군분투하는 경우(채팅 턴, 또는 \\((o_{t},a_{t})\\) 튜플), 에피소드는 "실패"(\\(r=0\\))로 종료된다. 여기서 교시성을 향상시키는 것은 인간이 의도한 목표 행동을 추론하는 \\(\\pi\\)의 능력, 즉 작업 성공에 대한 POMDP에서 더 짧은 경로를 발견하는 것으로 정의될 수 있다(즉, 인간-표지된 성공 \\(r=1\\)).\n' +
      '\n' +
      '진정한 전이 역학은 알려져 있지 않지만(인간 교사에 의존하기 때문에), 인간 텍스트 입력\\(o_{t}\\)과 로봇 코드를 동작으로서 입력\\(a_{t}\\)으로 표현할 수 있는 언어 모델을 사용하여 POMDP를 모델링하는 데 관심이 있으며, 디코더 전용 트랜스포머 아키텍처[66]를 사용하여 자동으로 모델링되었다. 따라서 우리는 전체 상호작용 시퀀스의 확률을 조건부 확률의 곱으로 인수분해하여 \\(\\hat{\\mathcal{P}}(o_{t},a_{t},r_{0}\\ldots,r_{7})\\(\\hat{\\mathcal{P}}(o_{t+1},r_{t},a_{t}|o_{<t},a_{<t},r_{<t})=)을 학습한다.\n' +
      '\n' +
      '도. 3: 우리의 채팅 인터페이스(좌상단)는 비전문가들이 언어를 사용하여 로봇들에게 새로운 행동(시뮬레이션, 우상단)을 가르칠 수 있게 한다. 우리의 LLM은 보상 코드로 반응하여 시뮬레이션 또는 실제 로봇의 실시간 모션 제어를 구동한다. 통계에 따르면 기본 모델 데이터는 기대에 부합한다: 성공적인 교수 세션은 실패(왼쪽 하단)보다 더 적은 채팅 턴을 취하고, 과제 성공률은 더 적은 채팅 턴(\\(r=-0.85\\), 중간 하단) 및 더 높은 평가율(즉, 피드백에 대한 반응성, \\(r=0.92\\), 오른쪽 하단)과 상관관계가 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '본 연구에서는 언어모델 예측제어(LMPC: Language Model Predictive Control)를 이용하여 LLM에 SFT(Supervised Fine-tuning)를 적용한다. LMPC는 인간-로봇 교환을 POMDP(III-A절 참조)로 공식화하고, 인간 텍스트 입력은 관찰(o_{t}\\), 로봇 코드 출력은 동작(a_{t}\\)으로 표현한다. 각 채팅 세션은 성공 또는 실패 보상으로 종료됩니다. 우리는 LMPC의 두 가지 변형(그림 1에 나와 있음)을 구현한다. 4): **(i) LMPC-Rollout.** 현재 채팅 세션(시스템 프롬프트 및 현재 채팅 히스토리)이 주어지면, LLM은 에피소드가 끝날 때 보상 \\(r\\)을 받을 때까지, 채팅 세션의 나머지 부분( \\(o_{t}\\) 및 \\(a_{t}\\)의 시퀀스)을 자동으로 예측하도록 트레이닝된다. 트레이닝을 위해, LLM에 대한 입력은 초기 사용자 명령을 갖는 시스템 프롬프트이다; 둘 다 상이한 로봇 실시예들이 상이한 시스템 프롬프트들(로봇 API들)을 갖기 때문에 포함되고, 이것은 LLM 생성이 추론 시간에 상이한 로봇 API들을 지원할 수 있게 한다. 타겟은 LLM이 현재 부분에 컨디셔닝된 채팅 세션의 임의의 잔여 부분을 예측하는 것이다. 우리는 LMPC-Rollout만을 성공적인 궤적에 대해 훈련한다(성공과 실패 모두에 대한 훈련은 훨씬 더 나쁜 성능을 산출했다. 부록 VI-B 참조). 전체 채팅 세션(관찰 및 액션의 시간 순서 시퀀스, 컨텍스트당 하나의 세션)에서 인과적 주의를 갖는 트랜스포머를 훈련시키는 것은 POMDP의 시퀀스 조건 전환 역학 모델을 훈련시키는 것과 직접 일치한다. 이 역학 모델은 추론 동안 검색에 사용된다.\n' +
      '\n' +
      'LMPC의 핵심 측면은 추론 시간에 미세 조정된 LLM을 모델 예측 제어(MPC)와 함께 전환 모델로 사용하여 성공에 이르는 최적의 경로를 발견한다는 것이다. MPC는 시퀀스 레벨 디코딩 전략[19]으로 생각할 수 있지만, 다음 최상의 동작을 탐색하기 위해 다수의 에피소드 롤아웃을 생성하고, 모든 의사 결정 단계에서 프로세스를 반복하기 때문에 현대 언어 모델에서 사용되는 표준과 다르다. 이를 위해 최대 토큰 길이(4096\\)에 대해 영이 아닌 온도 샘플링(다음 토큰 디코딩)을 사용하여 LLM\\(8\\) 롤아웃에서 샘플링한다. 샘플링된 궤적이 최대 토큰 길이 내에서 종료에 도달하면 LMPC-롤아웃은 성공적인 데이터에 대해서만 훈련되기 때문에 성공적인 것으로 간주한다. 이러한 종료된 샘플에서 그림 4(중심)와 같이 예측된 타임스텝이 가장 적은 궤적(즉, 채팅 턴)을 선택하고 첫 번째 액션을 반환한다. 만약 샘플이 종료되지 않는다면, 우리는 무작위로 궤적을 선택하고 그것의 \\(a_{t+1}\\)을 반환한다. 그런 다음 이 프로세스는 모든 채팅 턴에 대해 새로운 인간 입력이 주어지면 반복된다. 직관적으로 LMPC-롤아웃은 인간-로봇 상호작용을 통해 LLM을 훈련하는 것으로 생각할 수 있으며, 훈련과 추론 모두에서 생각 사슬[67]의 형태로 생각되며, 성공적인 코드를 복제하기보다는 LMPC가 올바른 코드에 도달하는 과정을 학습하고 추론 시간에 검색을 통해 가속화한다. **(ii) LMPC-Skip**는 동일한 훈련 입력을 갖지만, 상이한 타겟을 갖는 LMPC-롤아웃의 변형이다. LMPC-Skip은 중간 궤적의 예측을 건너뛰며 마지막 액션만을 예측하도록 훈련된다(도 4, 우측 참조). 이는 미세 조정된 모델이 1턴 성공을 위해 최적화하는 것과 같이 가능한 한 빨리 최종 올바른 코드를 예측하도록 장려한다. 그러나 LMPC-Skip은 트레이닝되지 않고 사용자와의 중간 상호 작용을 모델링하지 않기 때문에, 교정 피드백에 덜 반응할 수 있다. 추론하는 동안, 지금까지 채팅 세션으로 시스템 프롬프트 상에서 LMPC-Skip의 생성을 조건화하고, 응답을 생성하기 위해 모델을 한 번만 쿼리한다. LMPC-롤아웃과 마찬가지로 LMPC-Skip은 성공적인 채팅 세션에만 교육됩니다.\n' +
      '\n' +
      '**Top-User Conditioning.** fine-tuning으로 LLM 교시성을 더욱 향상시키기 위해, 본 발명자들은 사용자에 대한 트레이닝 및 추론 동안 LLM 세대들을 컨디셔닝하는 것을 제안한다. 교육을 위해, 우리는 고유한 ID 라벨을 사용하여 다음 채팅 세션을 생성한 사용자가 누구인지 포함하도록 입력 프롬프트를 수정한다. 학습 데이터 셋에서 최상위 사용자는 자율적으로 식별되며, "상위 사용자"라는 특별한 ID를 부여받는다. 추론 동안, 우리는 항상 "상위 사용자" 라벨에 LLM 세대들을 컨디셔닝한다. 사용자 성능 점수에 따라 상위 사용자를 상위(25\\%\\) 사용자로 식별한다. 이 점수는 모든 사용자에 대한 태스크의 실패율인 태스크 난이도에 의해 가중된 사용자의 태스크 성공률의 평균이다. 자세한 내용은 부록 VI-C를 참조하십시오.\n' +
      '\n' +
      '최상위 사용자 조건화는 LMPC의 맥락에서 사용자의 상위 백분위수에 가장 가까운 관측치\\(o_{t}\\)(예상 인간 입력) 및 행동\\(a_{t}\\)(예상 코드 출력)의 분포를 생성하기 위해 LLM을 조건화하는 것으로 해석될 수 있다. 직관적으로, 관찰들이 참(사용자) 상태(또는 의도, 티칭 동안)의 부분적인 잡음 표현으로서 보여진다면, 상이한 사용자 숙련도 레벨들은 변화하는 양의 잡음(즉, 더 높은 숙련도는 더 적은 잡음)에 대응할 수 있으며, 이는 최상위 사용자들 상의 컨디셔닝이 LLM으로 하여금 더 적은 잡음으로 롤아웃들을 생성하도록 프롬프트한다. 최상위 사용자 컨디셔닝은 MPC를 통한 추론-시간 검색을 사용하여 (조밀한 보상이 없는 경우)에도 불구하고 결정 트랜스포머[12]를 사용한 성능 컨디셔닝과 유사성을 도출한다. 최상위 사용자 컨디셔닝은 광범위한 사용자-관련 속성들(예를 들어, 선호도들, 사용자-특정 스타일들 등)을 나타내는 분포들을 광범위하게 인덱싱할 수 있으며, 리워드에 대한 성능 컨디셔닝만이 제공할 수 있는 것의 범위를 넘어 확장될 수 있다는 점에 유의한다.\n' +
      '\n' +
      '## IV Experiments\n' +
      '\n' +
      '실험을 통해 제안된 다양한 미세 조정 전략(느린 적응)이 자연 언어 피드백을 통해 상호작용적으로 로봇을 가르치는 인간에 대한 온라인 상황 내 학습(빠른 적응)을 얼마나 향상시켰는지 평가한다. 시뮬레이션에서 \\(5\\)의 로봇 구현과 실제 하드웨어에서 \\(2\\)의 로봇 작업에 대한 평가를 수행한다. 우리는 구체적으로 다음과 같은 질문을 탐구한다.\n' +
      '\n' +
      '* 미세 조정은 특히 테스트 작업에서 가르침성을 얼마나 향상시키나요?\n' +
      '* LMPC-롤아웃과 LMPC-스킵은 어떻게 비교되는가?\n' +
      '* Top-User Conditioning의 장점은 무엇인가?\n' +
      '* 미세 조정은 교차 실시 형태 일반화를 가능하게 하는가?\n' +
      '* 반복적인 미세조정이 교시성을 더욱 향상시킬 수 있는가?\n' +
      '\n' +
      '모든 데이터 수집 및 대부분의 평가는 시뮬레이션에서 수행되었다. 모든 모델은 시뮬레이션으로 얻은 데이터에 대해 훈련되었다. 우리는 실제 로봇에 대해 세분화된 모델을 별도로 평가하지만 실제 로봇을 가르치는 데 따른 데이터에 대한 교육을 실험하지 않았다.\n' +
      '\n' +
      '도. 4: 로봇에게 언어를 사용하여 새로운 작업을 가르치는 사용자 데이터 세트(온라인 상황 내 학습에서 텍스트 입력 및 코드 출력으로 표시됨 - 왼쪽)가 주어지면, LMPC-롤아웃은 현재 채팅 이력(중간)에 조건화된 후속 입력 및 출력을 예측하도록 훈련되고, 추론-시간 검색을 위해 MPC(추적 지평선 제어)를 사용하여 다음 최상의 액션(성공 전에 가장 적은 예상 수정 없이)을 반환한다. LMPC-Skip은 마지막 동작(오른쪽)을 직접 예측하도록 훈련된 대체 변형이다. 두 LMPC 변형 모두 상황 내 학습을 통해 빠른 로봇 적응을 가속화한다.\n' +
      '\n' +
      '###_Data Collection and Evaluation__\n' +
      '\n' +
      '인간 교수 데이터를 수집하고 교수 성과를 평가하기 위해 하루에 350회의 채팅 세션을 수집할 수 있는 35명의 비전문가 사용자와 협력했다. 이 사용자들은 비전문가이다: 그들은 연구원이나 엔지니어가 아니며, 기본적인 LLM이나 로봇 코드에 익숙하지 않다. 우리는 LLM이 작성한 코드에 대해 기술적인 피드백을 주거나 피드백을 주는 대신 채팅 턴마다 로봇의 행동에 대해 자연어 피드백을 줄 것을 사용자에게 지시한다. 데이터 수집 프로토콜 세부 정보는 부록 VI-A에 나와 있다. 사용자가 새로운 채팅 세션과 상호작용할 때, 임의의 로봇 실시예 및 태스크가 샘플링되고, 사용자는 로봇에게 그 태스크를 가르치도록 요청받는다. 데이터 수집은 1) 기본 모델을 사용한 초기 데이터 수집과 2) 미세화된 모델을 사용한 후속 데이터 수집(평가)의 두 단계로 나뉜다. 단계\\(2\\)에서는 사용자가 어떤 모델과 상호작용하는지 무작위로 샘플링하고 사용자는 현재 어떤 모델과 상호작용하는지 알지 못한다. 이를 통해 블라인드 A/B 평가가 편향을 최소화할 수 있다. 주어진 모델에 대해, 수집된 데이터는 다운스트림 미세조정 및 모델을 평가하는 데 모두 사용될 수 있다.\n' +
      '\n' +
      '\\(78\\)의 과제 중 \\(51\\)은 열차과제(\\(65\\%\\))이고 \\(27\\)은 시험과제(\\(35\\%\\))이다. 작업을 트레인과 테스트 분할로 분리하면 모델 일반화 성능을 측정할 수 있지만 훈련에 사용할 수 있는 데이터가 적다는 것을 의미합니다. 이를 해결하고 사용자 티칭 노이즈에 대한 데이터 분포를 견고하게 만들기 위해 일반적으로 모델의 데이터 수집 및 평가가 2일에 걸쳐 집계된다. 유효하지 않은 데이터와 부정확한 데이터를 제거하기 위해 추가 데이터 필터링을 수행했다. 총 299개의 성공적인 채팅 세션이 초기 데이터 수집에서 미세 조정이 가능해졌다. 채팅 세션 전체에서 최대 총 토큰 길이는 \\(3900\\), 중앙값은 \\(1800\\)입니다. 제한된 데이터 양을 고려하여 LLM 응답을 사용자 피드백의 작은 차이에 더 강력하게 만들기 위해 수집된 데이터에 대한 데이터 증강을 수행한다. 이는 PaLM\\(2\\)-L을 이용하여 사용자 지시(LMPC-Rollout 훈련에 대한 중간 피드백뿐만 아니라)의 \\(5\\) 변형을 생성함으로써 수행된다. 우리는 로봇 코드의 변형을 생성하지 않는다. 증강 데이터와 원본 데이터를 결합한 학습 세트에는 약 \\(3\\)M 토큰이 포함되어 있다.\n' +
      '\n' +
      '평가를 위해, 우리는 우리가 평가하는 모든 모델 변형들에 대해 대략 \\(350\\)개의 채팅 세션들을 수집하고, 모든 플랫폼들 및 태스크들에 걸쳐 분할한다. 우리는 시간이 지남에 따라 최소한의 사용자 성능 드리프트를 관찰하므로(부록 VI-I 참조), 모델 성능의 차이는 사용자의 교수 숙련도가 아닌 모델 능력의 변화로 인해 발생할 수 있다.\n' +
      '\n' +
      '###_로봇 실시예 및 과제_\n' +
      '\n' +
      '이 섹션에서는 실험에서 \\(5\\) 로봇 실시예에 대한 간략한 개요를 제공한다. 우리는 단일 팔을 필요로 하는 작업에서, 양수 작업, 그리고 능숙하고 움직이는 작업에 이르기까지 다양한 로봇 능력 세트를 가르치는 것을 탐구하기 위해 이러한 실시예를 선택했다. Fig.를 참조한다. 1은 삽화를 위한 것이다. 우리는 부록 VI-M에 각 실시예의 전체 작업 목록을 포함한다.\n' +
      '\n' +
      '**1. Robot Dog.** 첫 번째 실시예는 Unitree A1 및 MIT Mini-Cheetah 4족 모두와 유사한 치수 및 무게를 갖는 소형 맞춤형 4족 로봇[10]이다. 로봇 개는 각 다리에 3개씩 총 12개의 작동 자유도(DoF)를 가지고 있습니다. 로봇 개를 위한 우리의 작업은 앉기, 하이파이브와 같은 정지 포즈 작업에서 발걸음과 문 열기와 같은 더 역동적인 작업에 이르기까지 다양합니다. 우리는 시뮬레이션과 실제 세계 모두에서 로봇 개 실험을 수행한다.\n' +
      '\n' +
      '**2. 모바일 매니퓰레이터** 이 실시예에서, 우리는 7 DoF 암과 평행 죠 그리퍼를 갖는 모바일 매니퓰레이터[24]를 사용한다. 우리는 물체를 뒤집고 쌓는 것과 같은 단단한 물체를 가진 탁상 조작 작업을 탐구한다. 모바일 매니퓰레이터는 시뮬레이션과 실제 세계에서도 사용할 수 있습니다.\n' +
      '\n' +
      '**3. Aloha.** 2개의 6 DoF 암이 각각 평행한 죠 그리퍼[76]로 부착된 2-수동 실시예이다. 두 팔은 단단한 가정용 물체 세트가 있는 테이블 위에 서로 직접 마주보고 앉는다. 물체 이동과 같이 양팔과 조율이 필요한 작업을 탐색합니다.\n' +
      '\n' +
      '**4. Bi-arm Kuka.** Bi-arm Kuka는 엔드 이펙터가 없는 2개의 DoF Kuka LBR IIWA14 암으로 구성된다. 엔드-이펙터의 생략은 우리가 이 실시예로 전신 조작 작업(예를 들어, 로봇 암의 임의의 부분으로 물체를 조작하는 것)을 탐색할 수 있게 한다. 우리는 작업 공간을 다양한 크기와 색상의 상자로 채우고, 로봇은 개별 또는 객체 세트를 원하는 목표 위치(작업 공간 표면 또는 공기 중일 수 있음)와 주어진 순서로 조작할 필요가 있다.\n' +
      '\n' +
      '**5. Kuka+Hand.** 이 실시예는 맞춤형 세 손가락 손으로 부착된 7 DoF Kuka 암을 포함한다. 토크 제어를 통해 전체 시스템을 제어합니다. 암과 함께 작업 공간에는 강성 객체 세트가 제공됩니다. Kuka+Hand를 사용하여, 우리는 손 안의 여러 물체를 들어올리는 것과 플러그 삽입과 같은 다른 조작 실시예로 수행하기 어려운 능숙한 조작 작업을 탐색한다.\n' +
      '\n' +
      '### _Compared Methods_\n' +
      '\n' +
      '우리는 기본 모델(PaLM 2-S), 두 가지 미세 조정 변형 LMPC-롤아웃 및 LMPC-Skip, 그리고 검색-증강 생성(RAG) [40] 기준선에서 성능을 비교한다. LMPC-롤아웃과 LMPC-Skip을 비교하면 LLM을 미세 조정하여 전체 인간-로봇 채팅 상호 작용을 활용하고 예측하는 것과 최종 로봇-코드 응답을 예측하는 것 사이의 차이를 포착한다. 모델 가중치 또는 미세 조정에 필요한 리소스에 액세스할 수 없는 경우 도메인에서 LLM의 개선이 가능한 경우 RAG 캡처와 비교한다. RAG의 경우, 사전 훈련된 임베딩 모델을 사용하여 훈련 데이터에서 관련 예를 검색한 다음 로봇 동작을 적응시키기 위한 다른 RAG 애플리케이션과 유사하게 LLM 컨텍스트에 삽입한다[75]. 부록 VI-D의 구현 세부 정보를 참조하십시오.\n' +
      '\n' +
      '### _Experiment Results_\n' +
      '\n' +
      '우리는 LLM의 접근가능성(_reachability_)을 \\(<\\!N\\) 사용자 상호작용 또는 "채팅 턴"에 대한 태스크-성공으로 평가한다. 이것은 그림 1의 곡선으로 시각화됩니다. 도 5를 참조하면, 각 포인트는 일정 횟수의 채팅 턴수(x축) 이하로 성공(y축)을 달성한 채팅 세션의 비율을 나타낸다. 가르침성이 더 좋은 모델은 더 높고 왼쪽에 있는 곡선을 가질 것이다.\n' +
      '\n' +
      '도. 도 5는 베이스 PaLM 2-S 모델, 미세 조정된 모델 LMPC-롤아웃 및 LMPC-Skip, 및 RAG를 갖는 베이스 모델에 대해 모든 실시예에 걸쳐 집계된 주요 교시성 결과를 보고한다. 미세 조정을 통해 모델은 기본 모델의 교시 성능을 초과할 수 있다. 열차 작업에서는 LMPC-Skip이 가장 좋은 성능을 발휘합니다. 테스트 작업에서 LMPC-Rollout이 가장 우수한 성능을 보여 기본 모델보다 성공률이 \\(27\\%\\) 향상되었습니다. 두 모델 모두 기본 모델보다 빠른 성공률에 도달합니다. 단 한 번의 채팅 전환 후 기본 모델의 최종 성공률을 일치시키거나 초과합니다. 테스트 작업에서 LMPC-Skip이 LMPC-Rollout보다 높은 1회전 성공률을 달성하는 반면, 2회의 채팅 턴에서 시작하여 순서가 뒤집힌다. 이는 LMPC-롤아웃이 사용자 피드백의 개선에 더 적합함을 시사한다. RAG는 기본 모델을 놓고 경쟁적으로 수행하지만 열차 및 테스트 작업 모두에서 미세 조정 방법 뒤에 추적된다. 부록 VI-B의 표 VIII에 실시예별로 분리된 이러한 결과를 참조한다.\n' +
      '\n' +
      '표 I은 다음을 포함하여 평가된 모든 모델에 걸쳐 추가적인 정량적 비교를 제공한다:\n' +
      '\n' +
      '* _Success Rate_: 모든 태스크 및 실시예에 대한 전체 성공률\n' +
      '* _Num Chat Turns_: 성공적인 채팅 세션을 위한 평균 채팅 턴 수\n' +
      '*_Good Rating Rate_: 첫 번째 채팅 전환 후 긍정적으로 평가된 채팅 전환의 비율(교정 피드백에 대한 반응성을 포착함)\n' +
      '* _성공적인 태스크 레이트_: 적어도 하나의 성공적인 채팅 세션을 갖는 태스크의 비율\n' +
      '* _1 turn Success Rate_: 단 하나의 채팅 턴으로 성공한 채팅 세션의 비율(제1 명령어)\n' +
      '* _2+ turn Success Rate_: \\(>1\\) 채팅 턴으로 성공한 채팅 세션의 비율. 이는 전체 성공률과 1회전 성공률의 차이이다.\n' +
      '\n' +
      '열차 및 테스트 작업 모두에서 LMPC-Skip은 성공적인 채팅 세션에 대해 가장 낮은 Num 채팅 전환과 가장 높은 1턴 성공률을 달성합니다. 이는 LMPC-Skip이 가능한 한 빨리 최종 코드를 예측하도록 훈련되는 방식을 반영한다. 그러나 LMPC-롤아웃은 2+턴 성공률이 가장 높으며, 이는 잘못된 첫 번째 응답을 감안할 때 수정 피드백에 가장 적합함을 시사한다. 이러한 결과는 실제 성능을 극대화하기 위해 초기 사용자 명령에 응답하기 위해 LMPC-Skip을 사용하고 후속 사용자 피드백에 응답하기 위해 LMPC-Rollout을 사용해야 함을 시사한다. RAG의 경우 전체 성공률에 대한 기본 모델에서는 방법이 개선되지만 테스트 작업에서는 기본 모델보다 낮은 성공 작업 속도를 달성한다. 이는 RAG가 검색된 예제와 유사한 작업의 성공률을 높이는 데 능숙할 수 있지만 새로운 작업에 대해 잘 수행하기 위해 어려움을 겪는다는 것을 시사한다.\n' +
      '\n' +
      '**Top-User Conditioning의 효과.** 표 II에서 1) 모든 사용자의 데이터와 2) 최상위 사용자의 데이터에 대해 최상위 사용자 컨디셔닝 없이 훈련할 때 작업 성공의 변화를 보여준다. 이러한 절제는 시간 제약으로 인해 로봇 개 및 모바일 매니퓰레이터 실시예에서만 수행되었다. 기본 모형에서 수집된 초기 자료로부터 35\\(10\\)의 이용자들이 최상위 이용자로 확인되었고, 50\\(50\\)의 열차 업무 중 11\\(11\\)만을 다루었다. 그러나, 이러한 작은 커버리지에도 불구하고, 최상위 사용자 컨디셔닝은 모델 유형(LMPC-롤아웃 및 LMPC-Skip) 및 태스크 유형(훈련 및 테스트)에 걸쳐 최상위 사용자 컨디셔닝이 없는 두 변종 모두를 상당히 능가한다. 이는 최상위 사용자 컨디셔닝을 통해 모델이 최상위 사용자 교육에 의해 유도된 반응 스타일을 새로운 작업으로 전달하는 방법을 배울 수 있음을 시사한다. 또한 다양한 데이터 분포뿐만 아니라 고품질 데이터 분포에서 세대를 모방하도록 LLM을 훈련하는 것의 중요성을 강조한다. 상위 사용자의 교육 스타일에 대한 분석은 부록 VI-C를 참조하십시오.\n' +
      '\n' +
      '**Cross-Embodiment Generalization.** 테스트 작업들에 대한 일반화를 평가하는 것 외에도, 본 발명자들은 또한 실시예들의 서브세트에 대한 트레이닝이 피니튜닝된 모델들이 트레이닝되지 않은 새로운 실시예들에 대한 향상된 성능으로 이어질지 여부를 평가한다. LLM에 대해, 실시예의 차이는 프롬프트를 통해 캡처되며, 프롬프트는 각 실시예에 대해 상이한 로봇 설명 및 API를 포함한다. 로봇견, 모바일 매니퓰레이터의 데이터를 대상으로 LMPC 모델을 학습시키는 실험을 수행하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline Data & Model & Train Tasks & Test Tasks \\\\ \\hline All Users & LMPC-Rollouts & -8.4\\% & -10.5\\% \\\\  & LMPC-Skip & -16.3\\% & -26.1\\% \\\\ \\hline Only Top Users & LMPC-Rollouts & -23.8\\% & -21.7\\% \\\\  & LMPC-Skip & -9.6\\% & -13.6\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Changes in success rate without Top-User Conditioning. We evaluate two variants of LMPC-Rollouts and LMPC-Skip that do not apply top-user conditioning, training on data from all users and training on data from only top users. Success rates degrade significantly for both variants, suggesting that 1) focusing LLM generation on the style of top-users is important and 2) top-user data alone is insufficient, and training on the wider data distribution of all users is still important.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Tasks & Model & Success Rate & Num Chat Turns & Good Rating Rate & Successful Tasks Rate & 1 Turn Success Rate & 2+ Turn Success Rate \\\\ \\hline Train & PaLM 2-S & 34.8\\% & 2.3 & 16.7\\% & 74.0\\% & 13.0\\% & 21.7\\% \\\\  & RAG & 46.4\\% & 2.2 & 21.4\\% & **83.3\\%** & 25.1\\% & 21.2\\% \\\\  & LMPC-Skip & **56.0\\%** & **1.7** & **25.6\\%** & **83.3\\%** & **34.6\\%** & 21.4\\% \\\\  & LMPC-Rollouts & 51.9\\% & 2.2 & 21.8\\% & 74.0\\% & 23.5\\% & **28.4\\%** \\\\ \\hline Test & PaLM 2-S & 39.4\\% & 2.4 & 18.1\\% & 81.5\\% & 17.5\\% & 21.9\\% \\\\  & RAG & 51.9\\% & 2.0 & 20.9\\% & 75.0\\% & 27.9\\% & 24.0\\% \\\\  & LMPC-Skip & 59.4\\% & **1.6** & 24.7\\% & **88.9\\%** & **41.7\\%** & 17.8\\% \\\\  & LMPC-Rollouts & **66.3\\%** & 1.9 & **26.5\\%** & **88.9\\%** & 34.8\\% & **31.5\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Comparing base and finetuned models across all embodiments. _Success_: overall success rate on all tasks. _Num Chat Turns_: mean number of chat turns for successful chat sessions. _Good Rating_: proportion of positively rated chat turns after the turn. _Successful Tasks_: proportion of tasks with at least one successful chat session. _1 turn Success_: the proportion of chat sessions that were successful with just one chat turn. _2+ turn Success_: the proportion of chat sessions that were successful with two chat turns.\n' +
      '\n' +
      '도. 5: LMPC-롤아웃 및 LMPC-Skip을 갖는 우리의 미세 조정 LLM은 기본 모델(PaLM 2-S)의 교시성을 향상시키고, 모든 실시예에 걸쳐 RAG[40] 기준선을 능가한다. LMPC-Skip 오버핏은 작업(왼쪽)을 훈련시키는 반면, LMPC-롤아웃은 다중 턴 세션(채팅 턴이 두 개 이상인)에 대해 보이지 않는 테스트 작업(오른쪽)에서 더 나은(즉, 더 가르칠 수 있고 피드백에 응답하는) 것을 일반화한다.\n' +
      '\n' +
      '그리고 알로하, 바이암 쿠카와 쿠카+핸드 생략. 미세 조정 모델과 기본 모델 간의 성공률 차이를 보고하는 표 IV의 결과를 참조하십시오. 우리는 LMPC-Skip의 경우 \\(18.6\\%\\), LMPC-Rollout의 경우 \\(31.5\\%\\)의 테스트 실시예의 개선을 볼 수 있으며, 이는 미세화된 모델이 테스트 작업뿐만 아니라 테스트 로봇 실시예에도 일반화되어 있음을 시사한다. 이러한 일반화는 실시예들이 서로 매우 다른 API를 가지고 있기 때문에 자명하지 않으며, 테스트 실시예들은 복잡한 손재주 조작 행동을 유도할 수 있는 로봇 보상 코드를 작성해야 한다.\n' +
      '\n' +
      '**실세계 평가.** 실세계의 모바일 매니퓰레이터 및 로봇 개에 대한 작업의 하위 집합에 대한 접근 방식을 평가한다(도 6). 각 작업에 대해 사용자는 실제 로봇에서 직접 4회의 티칭 세션을 수행할 것을 요청합니다. 표 III의 PaLM 2-S 및 LMPC-롤아웃을 비교한 결과를 참조하십시오. LMPC-롤아웃은 모든 작업에서 PaLM 2-S보다 높은 성공률을 달성한다. 성공적인 세션에 대한 Num Chat Turns는 이러한 작업에서 PaLM 2-S 및 LMPC-롤아웃에 대해 거의 동일하지만 LMPC-롤아웃은 훨씬 더 높은 성공률을 달성한다. 부록 VI-G에서 SIM과 실제 실행 간의 자세한 비교를 참조하십시오.\n' +
      '\n' +
      '**다중 미세 조정 반복.** 미세 조정 모델이 기본 모델에 비해 향상된 도달성 성능을 나타낸다는 점을 감안할 때, 미세 조정 모델로 수집된 데이터를 사용한 추가적이고 반복적인 훈련은 잠재적으로 성능을 더욱 향상시킬 수 있다. 우리는 반복 1 모델에 의해 수집된 데이터로 반복 2 LMPC 모델을 훈련시켜 이 가설을 테스트했다. 결과는 표 V에 나와 있다. 현재 우리는 미세조정의 두 번째 반복에서 추가 개선을 관찰하지 않는다. 이는 모델의 두 번째 반복을 훈련하는 데 사용되는 데이터 분포 또는 데이터 양이 첫 번째 반복과 크게 다르지 않으므로 결과 모델 동작은 크게 변하지 않음을 의미한다. 최근 연구는 LLM[72, 25, 22]에 대한 반복적인 자체 개선 미세화를 입증했지만 인간 피드백과 로봇 코드 실행에 기반을 둔 LLM의 반복적인 개선을 가능하게 하고 있지만 아직 연구되지 않았으며 이 주제를 향후 연구로 미루고 있다.\n' +
      '\n' +
      '## V Discussions\n' +
      '\n' +
      '1) 인간-로봇 상호작용을 POMDP로 공식화하고 2) 인간-로봇 상호작용의 역학을 예측하기 위해 미세 조정된 LLM을 사용하여 언어 모델 예측 제어를 수행함으로써 LLM의 교시성을 향상시키는 방법을 소개한다. LMPC는 인간의 피드백으로부터 더 빨리 학습하는 것을 학습할 수 있으며, 테스트 작업 및 테스트 로봇 실시예에 대한 성능 향상을 관찰한다. 유망한 결과에도 불구하고 우리의 작업에는 잠재적인 미래 연구를 지적할 수 있는 몇 가지 한계가 있다. 일부 제한은 자원 가용성에서 비롯됩니다. 우리는 MJPC(예: 128 CPU 코어)와 LLM 미세 조정 모두에 대해 충분한 계산 리소스에 대한 액세스를 가정한다. 보다 효율적인 MPC 및 미세 조정 기술(예: LoRA[28])이 도움이 될 것이다.\n' +
      '\n' +
      '다른 제한 사항은 기초 모델과 관련이 있다. 우리는 베이스 LLM이 학습 프로세스를 부트스트랩하기 위한 몇 가지 긍정적인 채팅 세션을 생성할 수 있다고 가정한다. 또한 언어 모델만 사용한다. 멀티모달 기반 모델을 사용하여 피드백 모달리티(예: 비디오/오디오 입력)를 확장하는 향후 작업은 피드백의 풍부함을 확장할 뿐만 아니라 로봇 행동에 대한 인간 반응을 예측하는 미세화된 모델의 능력을 향상시킬 수 있다. 마지막으로, 우리의 접근법은 현재 여러 컨텍스트 내 학습 및 미세 조정 주기에 걸쳐 학습의 이점을 관찰하지 못한다. 활성 작업 탐색 또는 합성 데이터 생성과 같은 방법을 통해 데이터 배포를 조정하면 추가 성능 이득을 해제할 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Success Rate Diff from Iter 1} \\\\ \\cline{2-3} Model & Train Tasks & Test Tasks \\\\ \\hline LMPC-Skip Iter 2 & +5.1\\% & -4.7\\% \\\\ LMPC-Rollouts Iter 2 & -5.5\\% & -1.9\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Further finetuning on data generated from both the base model and the first finetuned models models does not yield performance improvements.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline  & & \\multicolumn{2}{c}{PaLM 2-S} & \\multicolumn{2}{c}{LMPC-Rollouts} \\\\ \\cline{3-6} Embodiment & Task & Success & Num Chat Turns & Success & Num Chat Turns \\\\ \\hline Robot Dog & “downward dog” & \\(100\\%\\) & \\(1.3\\) & \\(100\\%\\) & \\(2.8\\) \\\\  & “hop” & \\(25\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(2.3\\) \\\\  & “high-five with left hand” & \\(75\\%\\) & \\(2.3\\) & \\(75\\%\\) & \\(3.0\\) \\\\  & “walk forward in a trotting gait” & \\(25\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(2.8\\) \\\\  & “hop while turning counterclockwise” & \\(25\\%\\) & \\(5.0\\) & \\(25\\%\\) & \\(4.0\\) \\\\ \\hline Mobile Manipulator & “knock over coke can” & \\(20\\%\\) & \\(5.0\\) & \\(20\\%\\) & \\(3.0\\) \\\\  & “open top drawer half-way” & \\(100\\%\\) & \\(3.4\\) & \\(100\\%\\) & \\(3.2\\) \\\\  & “push coke can from right to left” & \\(60\\%\\) & \\(2.0\\) & \\(80\\%\\) & \\(2.0\\) \\\\ \\hline \\hline \\multicolumn{6}{c}{Average} & \\(53.8\\%\\) & \\(\\mathbf{2.9}\\) & \\(\\mathbf{75\\%}\\) & \\(\\mathbf{2.9}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: LMPC-Rollouts has higher success than PaLM 2-S on real robots. Test tasks are started\\({}^{*}\\). Robot Dog tasks are performed 4 times, Mobile Manipulator tasks \\(5\\) times.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Tain Embodiments} & Test Embodiments \\\\ \\cline{2-3} Model & Train Tasks & Test Tasks \\\\ \\hline LMPC-Skip & \\(+28.8\\%\\) & \\(+19.0\\%\\) & \\(+18.6\\%\\) \\\\ LMPC-Rollouts & \\(+17.2\\%\\) & \\(+23.8\\%\\) & \\(+31.5\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Finetuned models can generalize to new robot embodiments and APIs not seen during training. Higher improvements in test tasks and embodiments are caused by the finetural split not being explicitly selected for uniform task difficulty and baseline performance; doing so is infeasible as the split needs to be chosen before starting evaluations, when task difficulty and baseline performance were unknown.\n' +
      '\n' +
      '도. 6: 실세계 모바일 매니퓰레이터 및 로봇견에서 평가된 작업.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]M. 안아브론한 브라운영 체보타르, 오 코테스, B. 데이비드, C. 핀, C. 푸, K. 고팔라크리쉬난 Hausman, et al. (2022) Do as i can, not as i say: grounding language in robotic affordances. ArXiv:2204.01691. 인용: SS1.\n' +
      '*[2]A. 아카지안, C. 콜디스, P. 커슬리, T. M. Oudeyer 체투아니, O. Sigaud(2020)는 목표 생성을 통해 자율적으로 습득한 기술로 언어를 접지한다. ArXiv:2006.07185. 인용: SS1.\n' +
      '*[3]R. 안일아목대 피라트 존슨, D. 렙킨, A. 파소스, S. 샤케리, E. 타로파, P. 베일리, Z. Chen, et al. (2023) Palm 2 기술 보고서. ArXiv:2305.10403. 인용: SS1.\n' +
      '*[4]M. 곤잘레스 아레나스 샤오상 싱브이 자인 A. R. 렌 Q. Vuong, J. Varkey, A. Herzog, I. Leal, S. Kirmani, et al. (2023) How to prompt your robot: a prombook for manipulation skills with code as policies. 일반주의 로봇을 향한: 확장 가능한 Still Acquisition@ CoRL2023에 대한 학습 파라다짐, 인용: SS1.\n' +
      '*[5]Y. 아틱과 L. Zettlemoyer (2013) Weakly supervised learning of semantic parsers for mapping instructions to action. Transactions of the Association for Computational Linguistics (TACL)1, pp. 49-62. Cited by: SS1.\n' +
      '*[6]F. Gregory Ashby and J. T. Townsend (1986) Varies of perceptual independence. Psychological review93(2), pp. 1541. Cited by: SS1.\n' +
      '*[7]Y. 배승 가다얀 Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. (2022) Constitution ai: harmlessness from ai feedback. ArXiv:2212.08073. 인용: SS1.\n' +
      '*[8]A. 브라운, 노 브라운, J. 카르바할, Y. 체보타르, J. 다비스, C. 핀, K. 고팔라크리쉬난 Hausman, A. Herzog, J. Hsu, J. B. Ichter, A. Irpan, T. 잭슨 제스월 조시경 줄리안, 야카슈니코프, Y. 경일렬 희성 레빈영 루우 Malla, D. Manjuntan, I. Mordatch, O. 나첨, C. 파라다, J. 펄라, E. 페레스, K. 퍼치, J. 퀴아노, K. 라오만 류지살레자르 P. 산케이 세이드, J. 싱, S. 손탁타케, A. Stone, C. Tan, H. Tran, V. S. 바누크 베가, Q. V. Voong, F. Xi, F. Xiao, P. Xu, S. 서태호 Yu, 및 B. Zitkovich(2023) RI-16 로보틱스 변압기는 규모에서 실제 제어를 위한 것이다. 외부 링크: 2306.10880 인용: SS1.\n' +
      '*[9]T. 브라운, B. 만, N. 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neekalatan, P. Shyam, G. Sastry, A. Askell, et al. (2020) 언어 모델은 소수의 학습자이다. neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '*[10]K. 칼루와츠, A. 아이센, J. 큐, W. 유태 장동리만 이락 이승환 살리세티, V. Zhuang, et al. (2021) BARKour: 4족 로봇으로 동물 수준의 민첩성을 벤치마킹한다. ArXiv:2305.14654. 인용: SS1.\n' +
      '*[11]S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill (2022) 데이터 분배 속성은 트랜스포머에서 새로운 컨텍스트 학습을 유도한다. Advances in Neural Information Processing Systems35, pp. 18878-18891. Cited by: SS1.\n' +
      '*[12]L. 천경 루아주 라제스와란 이아그로버 Laskin, P. Abbeel, A. Srinivas, and I. Mordatch (2021) Decision transformer: Sequence Modeling을 통한 강화학습. neural information processing systems34, pp. 15084-15097. Cited by: SS1.\n' +
      '*[13]P. Christiano, J. Leike, T. B. Brown, M. 마틱 Legg, and D. Amodei (2020) Deep reinforcement learning from human preferences. 외부 링크: 2006.07185 인용: SS1.\n' +
      '*[14]G. 사이다론 서린, F. 스트럽, O. 피에틴(2019) 후각 경험을 가진 자가 교육 언어 에이전트는 다음 지시를 위해 다시 재생한다. 딥마인드 인용: SS1.\n' +
      '*[15]J. D. C.-Reyes, A. Gupta, S. N. 산지브 Altieri, J. DeNero, P. Abbeel, S. Levine(2018)은 메타 학습을 통해 언어로 정책을 안내한다. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '* 공유 자율성을 통한 로봇 조작을 위한 온라인 언어 교정. In Proceedings of the 2023 ACM/IEEE Conference on Human-Robot Interaction (HRI), Cited by: SS1.\n' +
      '*[17]Y. 딩, 엑스 장씨 팩스턴, S. Zhang (2023) Task and motion planning with large language models for object rearmagent. ArXiv:2303.06247. 인용: SS1.\n' +
      '*[18]D. Driessi, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhory, B. Litter, A. Wahid, J. Tompson, Q. 부엉태 투원 황영 체보타르, P. 서머넷, D. 덕워스, S. 레비네, 브이 바누크 하우스만 투생 Greff, A. Zeng, I. Mordatch, and P. Florence(2023) Palm-e: embodied multimodal language model. 외부 링크: 2306.10880 인용: SS1.\n' +
      '*[19]M. 프리택과 Y. Al-Onizan (2017) Beam search strategies for neural machine translation. ArXiv:1702.01806. 인용: SS1.\n' +
      '*[20]E. Galy, J. Paxion, and C. Berthelon (2018) Measuring mental workload with the nasa-tlx: a example with driving. Ergonomics61(4), pp. 517-527. Cited by: SS1.\n' +
      '*[21]P. 고열성 Niekum, and R. J. Mooney(2020) PSLP: 픽셀을 리워드에 매핑하여 자연어를 이용한 강화 학습을 안내한다. 2007.15543. 인용: SS1.\n' +
      '*[22]C. 굴체르, T 데 페인 K. 스리니바산 Konyushkova L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al.(2020) Reinforced self-training(rest) for language modeling. ArXiv:2308.089896. 인용: SS1.\n' +
      '*[23]S. G. Hart, N. 부하 지수(nasa-tlx) 20년 후. The 50th HFES Conference, pp. 904-908. Cited by: SS1.\n' +
      '*[24]A. 허조그 라오경 하우즈먼 Lu P. N. Mehringenau, Y 양진린 곤잘레스 아레나스 샤오, D. 캐플러, D. 호, J. 레팅하우스, Y. 체보타르 이경호 고팔라크리쉬난 줄리안, A. 리, C. 켈리 푸, B. 웨이, S. 라메쉬 홀든 클리버, D. 렌들맨, S. 키르마니, J. 빙엄, J. 와이즈, Y. Xu, Wentling, L. H. Benincke, C. Fong, D. D. J. Lesian, Y. 배비홀슨 N. 퀸란 브라운 Kalakrishnan, J. Ibarz, P. Pastor, S. Levine(2023) 깊은 rl 규모: 사무실 건물에서 쓰레기를 이동식 조작기로 분류한다. 외부 링크: 2306.108651 인용: SS1.\n' +
      '*[25]O. 호노비치, T. 사이알롬, 오 레비랑 T Schick(2022) 부자연스러운 명령: 인간의 노동이 거의 없는 언어 모델을 조율하는 것. ArXiv:2212.096989. 인용: SS1.\n' +
      '*[26]T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: survey. IEEE transactions on pattern analysis and machine intelligence44(9), pp. 5149-5169. Cited by: SS1.\n' +
      '*[27]T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: survey. IEEE transactions on pattern analysis and machine intelligence44(9), pp. 5149-5169. Cited by: SS1.\n' +
      '*[28]T. Hospedales, A. Attoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: survey. IEEE transactions on pattern analysis and machine intelligence44(9), pp. 5149-5169. Cited by: SS1.\n' +
      '*[29]F. Hu and D. Sadigh (2021) 언어는 인간과 협응을 위한 강화 학습을 지시했다. 제40회 기계 학습 국제 회의(ICML: International Conference on Machine Learning)에서, 인용: SS1.\n' +
      '*[30]W. Huang, P. Abbeel, D. Pathak, and I. Mordatch (2022) 언어 모델은 제로샷 플래너로서: 체화된 에이전트에 대한 행동가능한 지식을 추출한다. In International Conference on Machine Learning, pp. 9118-9147. Cited by: SS1.\n' +
      '*[31]W. 황성철 샤오, H. Chan, J. Liang, P. Florence, A. Zong, J. Tompson, I. Mordatch, Y. Chebotar, et al. (2022) Inner monologue: 언어 모델로 계획을 통해 체화된 추론. ArXiv:2207.05608. 인용: SS1.\n' +
      '*[32]D. 휴플스, 브이 당커스, M Mul, and E. Bruni (2020) Compositionality 분해: 신경망은 어떻게 일반화되는가? Journal of Artificial Intelligence Research67, pp. 7757-795. Cited by: SS1.\n' +
      '*[33]E. 장아일판 카사리, D. 캐플러, F. 에버트, C. 린치, S. Levine, and C. Finn (2022) Eco-z: 로봇 모방 학습을 이용한 제로 샷 작업 일반화. 외부 링크: 2202.02566 인용: SS1.\n' +
      '*[34]Y. 계층적 심층 강화 학습을 위한 추상화로서 장, S. S. 구, K. P. 머피, C. 핀(2019) 언어이다. 뉴립스 인용: SS1.\n' +
      '*[35]S. 카람체티, E. C. 윌리엄스, D. 아루무감, M. 이남 고팔란, L. L. S. 웡, S. Tellex (2017) 두 드래그의 타일: 액션 지향 및 목표 지향 지시를 해석하기 위한 하이브리드 접근법. 로보틱스용 언어 접지에 대한 첫 번째 워크샵 @ ACL, 인용: SS1.\n' +
      '*[36]A. 키릴로프, E. 미툰, N. 라비, H. 마오, C. 홀란드, L. 구스타프손 샤오상 화이트헤드 A. C. 버그 Lo, et al. (2022) Segment anything. ArXiv:2304.02643. 인용: SS1.\n' +
      '*[37]T. 칼라성 텔렉스, D. 로이, N. 로이(2010) 자연어 방향을 이해한다. In Human-Robot Interaction, pp. 259-266. Cited by: SS1.\n' +
      '*[38]M. 권성민 Bullard, and D. Sadigh (2023) Reward design with language models. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '*[39]M. 권희후 마이어스 Karamcheti, A. Dragan and D. SadighKarpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ 2020년, 신경망 정보 처리 시스템_, 33:9459-9474의 발전.\n' +
      '* [14] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9493-9500. IEEE, 2023.\n' +
      '* [15] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. L1M-4P: Empowering language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.\n' +
      '* [16] Jelena Luketina, Nantas Nardelli, Gregory Farughar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktaschel. A survey of reinforcement learning informed by natural language, 2019.\n' +
      '* [17] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data, 2021.\n' +
      '* [18] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. _arXiv preprint arXiv:2310.12931_, 2023.\n' +
      '* [19] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In _International Symposium on Experimental Robotics (ISER)_, 2012.\n' +
      '* [20] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. _IEEE Robotics and Automation Letters_, 7(3):7327-7334, 2022.\n' +
      '* [21] Suvir Mirchandani, Siddharth Karamchedi, and Dora Sadigh. Ella: Exploration through learned language abstraction, October 2021.\n' +
      '* [22] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. _arXiv preprint arXiv:2307.04712_, 2023.\n' +
      '* [23] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. _arXiv:1704.08795_, 2017.\n' +
      '* [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhimi Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. URL. [https://d4mucfpksyw.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksyw.cloudfront.net/better-language-models/language-models.pdf).\n' +
      '* [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [27] Allen Z. Ren, Anushi Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Vareley, Zhenjia Xu, Dresa Sadigh, Andy Zeng, and Anithuda Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners, 2023.\n' +
      '* [28] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Cherfeng Xu, Ping Luo, Shengbo Ehen Li, Masayoshi Tomiaka, Wei Zhan, and Mingyu Ding. Language: Large language models as decision makers for autonomous driving, 2023.\n' +
      '* [29] Pratyusha Sharma, Bakakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. Correcting robot plans with natural language feedback. _arXiv preprint arXiv:2204.05186_, 2022.\n' +
      '* [30] Roger N Shepard and Jih-Jie Chang. Stimulus generalization in the learning of classifications. _Journal of Experimental Psychology_, 65(1):94, 1963.\n' +
      '* [31] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _CoRL_, 2021.\n' +
      '* [32] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Proformpot: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11523-11530. IEEE, 2023.\n' +
      '* [33] Simon Stepputtis, Joseph Campbell, Mariano Phieipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. _NeurIPS_, 2020.\n' +
      '* [34] Nisan Sietmon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alex Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.\n' +
      '* [35] Alon Talmor, Ori Yoran, Ronan L Eras, Chandra Bhagavatula, Yow Goldberg, Yejin Choi, and Jonathan Berant. CommonsenseQA 2.0: Exposing the limits of AI through gamification. _arXiv preprint arXiv:2201.05320_, 2022.\n' +
      '* [36] Stefanie Tellek, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In _AAAI_, 2011.\n' +
      '* [37] Stefanie Tellek, Nakul Gopalan, Hradas Kress-Gazit, and Cynthia Matuszek. Robots that use language. _Annual Review of Control, Robotics, and Autonomous Systems_, 3(1):25-55, 2020. doi: 10.1146/annurev-control-101119-071628. URL: [https://doi.org/10.1146/annurev-control-101119-071628](https://doi.org/10.1146/annurev-control-101119-071628).\n' +
      '* [38] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoc: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386100.\n' +
      '* [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Demy Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* [41] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. _arXiv preprint arXiv:2305.05568_, 2023.\n' +
      '* [42] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. _arXiv preprint arXiv:2302.05128_, 2023.\n' +
      '* [43] Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew R. Walter. State: maintaining language models for embodied reasoning, 2023.\n' +
      '* [44] Wenhao Yu, Nimorod Gileadi, Chuyuan Fu, Sean Kirmani, Kunja-Huei Lee, Montse Gonzalez Arenas, Hao-Ten Lewis Chiang, Tom Ercez, Leonard Hasenclever, Jan Humphik, et al. Language to rewards for robotic skill synthesis, _arXiv preprint arXiv:2306.08647_, 2023.\n' +
      '* [45] Weizhe Yuan, Richard Yuanz Pangz, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.\n' +
      '* [46] Eric Zklinkman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A (de-) compositional framework for algorithmic reasoning with language models. _arXiv preprint arXiv:2212.10561_, 2023.\n' +
      '* [47] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveeke Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_, 2022.\n' +
      '* [48] Lihan Zhang, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat G. Arenas, Andy Zeng, Fei Xia, and Dorsa Sadigh. Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. In _2024 IEEE International Conference on Robotics and Automation (ICRA)_, 2024. URL: [https://arxiv.org/abs/2311.10678](https://arxiv.org/abs/2311.10678).\n' +
      '* [49] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. _arXiv preprint arXiv:2304.13705_, 2023.\n' +
      '\n' +
      '## 저작 및 인정\n' +
      '\n' +
      '존 길야드는 그의 전문 애니메이션에 감사하고 자일스 루스코는 아름다운 렌더링에 감사한다. 우리는 스티븐 보헤즈, 유발 타사, 톰 에레즈, 무릴로 마틴스, 루길 페베체비치우트, 데이비드 렌들맨, 코너 쉔크에게 우리가 강력한 모의 환경을 확보하도록 헌신한 것에 감사한다. 트래비스 암스트롱, 노아 브라운, 스펜서 굿리치, 크레이그 힉먼, 틸 이센, 제라드 커클랜드, 제이슨 파월, 스테파노 살리세티, 론 슬로아트, 세르게이 야로셴코, 에디 유, 그레이스 베솜, 제이크 발리에게 로봇 플랫폼 지원과 로봇 실험실 운영에 대해 감사드린다. 이 프로젝트를 지원해주신 마이클 안, 켄드라 번, 알렉산드라 파우스트, 르네 바그너, 위청광, 야오루, 얀송팡, 주오슈에게 특별한 감사를 드립니다.\n' +
      '\n' +
      '로봇 교육 데이터를 수집하기 위해 자원한 모든 사용자에게 감사드립니다. 또한 채팅 인터페이스 개발과 지원에 도움을 주신 구글 딥마인드 시각화 및 휴먼 인터렉션 팀에 감사드립니다. 우리는 또한 이 논문에서 추가적인 지원을 위해 지칠 줄 모르는 노력을 추적할 수 있는 구글 딥마인드 로보틱스 팀 전체에 감사를 표하고 싶다. 여기에는 팀의 모든 성공에 영향을 미치는 행정, 제품, 프로그램 및 전략 팀이 포함됩니다. 우리는 또한 구글 딥마인드와 구글 리서치의 친구들에게 그들의 지도, 영감을 주는 연구, 심지어 직접적인 기여에 대해 감사하고 싶다.\n' +
      '\n' +
      '**Program Leads**\n' +
      '\n' +
      '이 프로젝트는 구글 딥마인드 2023 프로그램 \'어프렌티스봇\'의 일부로, \'누구든지 로봇을 가르칠 수 있고, 누구로부터도 배울 수 있는 로봇\'이라는 미션 선언과 함께 대화형 구현된 AI 달샷이다.\n' +
      '\n' +
      '캐롤리나 파라다, _Director_\n' +
      '\n' +
      '닉 스튜어트, _Technical Program Manager_\n' +
      '\n' +
      '지탄_팀 리드_\n' +
      '\n' +
      '**Technical Leads**\n' +
      '\n' +
      'Andy Zeng, _Research Lead_\n' +
      '\n' +
      'Wenhao Yu, Fei Xia, _Data Collection & Teaching Leads_\n' +
      '\n' +
      '재키 량, _모델 훈련 및 개선 리드_\n' +
      '\n' +
      '재스민 Hsu _Data & Logging Lead_\n' +
      '\n' +
      'Peng Xu _Infrastructure Lead_\n' +
      '\n' +
      'Ben Jvenis, _Operations Lead_\n' +
      '\n' +
      '에릭 프레이 _Simulation Lead_\n' +
      '\n' +
      '**Operations**\n' +
      '\n' +
      '벤 자베니스, 트래비스 암스트롱, _운영의 헤드_\n' +
      '\n' +
      '자스민 허, 재키 량, _데이터 수집 모니터링_\n' +
      '\n' +
      '로봇 Dog_Pilot 연구를 위한 Wenhao Yu\n' +
      '\n' +
      'Fei Xia_Pilot Study for Mobile Manipulator_\n' +
      '\n' +
      'Aloha_Pilot 연구를 위한 Baruch Tabanpour\n' +
      '\n' +
      'Maria Attarian, Jonathan Tompson, _Pilot Study for Bi-arm Kuka_\n' +
      '\n' +
      'Joss Moore, Maria Bauza, _Pilot Study for Kuka+Hand_\n' +
      '\n' +
      'Contributors_: Maria Attarian, Ken Caluwaerts, Jasmine Hsu, Jacky Liang, Assaf Hurwitz Michaely, Jonathan Tompson, Fei Xia, Wenhao Yu, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Data Logging Infrastructure**\n' +
      '\n' +
      'Jasmine Hsu, Ken Caluwaerts, _Datasets and dashboard_\n' +
      '\n' +
      'Peng Xu, Assaf Hurwitz Michaely, Jacky Liang, _aterialization Contributors_: Adil Dostmohamed, Marissa Giustina, Nikhil Joshi, Jacky Liang, Quan Vuong, Tingnan Zhang\n' +
      '\n' +
      '**Model Serving Infrastructure**\n' +
      '\n' +
      '아사프 허비츠 마이클리, 잉 슈, _Core contributors_\n' +
      '\n' +
      'Jasmine Hsu, Ken Caluwaerts, Adil Dostmohamed, _LLM Chat UI Contributors_: Jacky Liang, Allen Ren, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Model Training Infrastructure**\n' +
      '\n' +
      '공헌자_: Assaf Hurwitz Michaely, Jacky Liang, Peng Xu, Andy Zeng, Jasmine Hsu, Edward Lee\n' +
      '\n' +
      '_Contributors_: Quan Vuong, Tingnan Zhang\n' +
      '\n' +
      '**Evaluations & Analysis**\n' +
      '\n' +
      '재키 량, _Technical Lead_\n' +
      '\n' +
      '레일라 타카야마 _인간-로봇 상호작용 Lead_\n' +
      '\n' +
      '_Contributors_: Alex Bewley, Keerthana Gopalakrishnan, Jasmine Hsu, Jacky Liang, Assaf Hurwitz Michaely, Dorsa Sadigh, Fei Xia, Ted Xiao, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Prompt Engineering**\n' +
      '\n' +
      '마리아 바우자, 마리사 주스티나, 광후이, 재키 량, 조스 무어, 두샹 라오, 바뤼흐 타반푸르, 페이샤, 원하오 유, 앤디 젠하오\n' +
      '\n' +
      '**Simulation & MJPC**\n' +
      '\n' +
      '마리아 어테리언, 켄 칼루와트, 에릭 프레이, 추위안 켈리 푸, 님로드 길레디, 레너드 하젠클레버, 얀 험플릭, 니킬 조시, 벤 제니스, 조스 무어, 두시안 라오, 바뤼흐 타반푸르, 페이샤, 테드 샤오, 원하오 유, 칭난 장\n' +
      '\n' +
      '**Robot-Specific Infrastructure**\n' +
      '\n' +
      'Robot Dog_: Ken Caluwaerts, Marissa Giustina, Chase Kew, Ken Oslund, Wenhao Yu Tingnan Zhang.\n' +
      '\n' +
      '_Mobile Manipulator_: Fei Xia, Chuyuan Kelly Fu\n' +
      '\n' +
      'Aloha_: Baruch Tabanpour, Jonathan Tompson, Eric Frey\n' +
      '\n' +
      '_Bi-arm Kuka_: Maria Atarian\n' +
      '\n' +
      '_Kuka+Hand_: Maria Bauza, Joss Moore, Dushyant Rao, Nimrod Gileadi\n' +
      '\n' +
      '**실제 로봇 배치 및 정책 증류**\n' +
      '\n' +
      'Ken Caluwaerts, Chuyuan Kelly Fu, Leonard Hasenclever, Jan Humplik, Chase Kew, Sean Kirmani, Kuang-Huei Lee, Ken Oslund, Allen Ren, Jonathan Tompson, Quan Vuong, Fei Xia, Ted Xiao, Zhuo Xu, Wenhao Yu, Tingnan Zhang.\n' +
      '\n' +
      '**Advising**\n' +
      '\n' +
      '알렉스 비울리, 에릭 프레이, 레너드 하젠클리버, 자스민 슈, 얀 험플리크, 브라이언 이히터, 광휴이 리, 재키 량, 캐롤라이나 파라다, 두샹 라오, 도르사 새디, 닛 스튜어트, 레일라 다카야마, 지탄, 페이샤, 테드 샤오, 펑슈, 원하오 유, 앤디 젱, 칭난 장\n' +
      '\n' +
      '**Additional Contributions**\n' +
      '\n' +
      '_Authorship and Acknowledgments_: Nik Stewart\n' +
      '\n' +
      'Paper Content and Web Posts_: Carolina Parada, Andy Zeng, Wenhao Yu, Jacky Liang, Fei Xia, Tingnan Zhang\n' +
      '\n' +
      '_Steering_: Carolina Parada, Nik Stewart, Izhak Shafran, Vincent Vanhoucke, Maja Mataric, Leila Takayama, Jie Tan, Dorsa Sadigh, Andy Zeng, Wenhao Yu, Jacky Liang, Fei Xia, Tingnan Zhang\n' +
      '\n' +
      '## VI Appendix\n' +
      '\n' +
      '부록은 다음과 같이 정리한다.\n' +
      '\n' +
      '* 데이터 수집(예를 들어, 채팅 UI) 및 평가 프로토콜(예를 들어, 태스크 샘플링)에 대한 세부사항. 섹션 VI-A\n' +
      '* VI-B절에서의 추가 결과 및 평가.\n' +
      '* 최상위 사용자 컨디셔닝에 대한 세부사항(섹션 VI-C): 상위 사용자 교육 데이터가 다른 사용자와 어떻게 다른지에 대한 정량적 및 정성적 분석.\n' +
      '* 검색 기준 상세(섹션 VI-D) 및 데이터 증강(섹션 VI-E).\n' +
      '* 채팅 피드백 임베딩의 정량적 분석(섹션 VI-F).\n' +
      '* 실제 로봇 실험 세부사항(섹션 VI-G)은 모델 훈련 및 배치(섹션 VI-H)를 포함한다.\n' +
      '* 사용자 스터디 및 성능 드리프트 섹션 VI-I.\n' +
      '* 고장모드 해석 섹션 VI-J.\n' +
      '* 기존 코드 작성 벤치마크인 Section VI-K에 대한 모델 성능.\n' +
      '* Robot-specific embodiment details(Section VI-L), tasks(Section VI-M), prompts(Section VI-N).\n' +
      '\n' +
      '###_Data Collection and Evaluation Details_\n' +
      '\n' +
      '데이터 수집 동안, 비전문가 사용자는 브라우저 기반 채팅 UI(도 3에 도시됨)를 통해 자연 언어를 사용하여 로봇과 상호작용한다. 채팅 UI는 MuJoCo[65]를 이용하여 사용자 입력함, 메시지 히스토리, 시뮬레이션된 로봇 및 그 주변의 시각화를 표시한다. 인간은 텍스트 입력을 제공하고 LLM은 실행 가능한 코드로 각각의 후속 사용자 질의에 응답한다. 그런 다음 사용자는 시뮬레이터에서 코드를 실행하여 결과적인 동작을 관찰하거나 실제 로봇에서 실행하도록 버튼을 선택할 수 있다. 사용자는 피드백(멀티턴 컨텍스트일 수 있음)을 계속 제공할 수 있고, 원하는 로봇 행동이 달성될 때까지 채팅 UI 내의 텍스트 입력을 통해 행동을 계속 수정할 수 있다. 각 사용자는 클라우드에서 고성능 기계(128코어)의 공유 풀에서 가져온 원격 데스크톱을 통해 하나의 기계에 원격으로 연결됩니다. 무호코의 MJPC[27]가 상호작용 속도로 로봇 동작을 합성하기 위해서는 높은 코어 카운트를 갖는 기계가 필요하며, 이는 더 나은 저수준의 로봇 동작 및 후속적인 사용자 피드백 데이터로 이어진다.\n' +
      '\n' +
      '각 채팅 세션에 대해, 사용자는 언어를 통해 특정된 하나의 작업을 로봇-개에게 "앉아서 하이파이브를 주라"고 가르친다. 각 채팅 턴(사용자-입력, LLM-출력 쌍)에 대해, 사용자는 개별 로봇 응답을 \'좋음\' 또는 \'나쁨\'으로 평가하는 옵션을 갖는다. 이러한 단일 턴 양호 등급 비율은(훈련 중에 사용되지 않는 동안) 개별 응답에 걸쳐 피드백에 대한 반응성을 평가하는 데 도움이 되며, 이는 작업 성공과 강한 상관관계가 있음을 발견한다(도 3 참조). 마지막으로, 사용자는 대화 중에 로봇이 작업에서 성공한 경우 성공 버튼을 클릭하거나, 인간이 입력한 7라운드 내에서 작업이 성공하지 못한 경우 "실패"로 전체 채팅 세션을 "성공"으로 레이블링할 수 있다. 성공의 변화가 예상되고, 사용자는 (코드의 정확성과는 대조적으로) 로봇의 관찰된 행동에 기초하여 성공을 평가하도록 장려된다. 채팅 세션을 라벨링한 후, 채팅 히스토리 UI가 리프레시되고, 로봇 시뮬레이터가 리셋되고, 새로운 샘플링된 태스크 및 실시예가 사용자에게 제시된다. 사용자는 기술적인 어려움에 대비하여 채팅 세션을 플래그할 수 있습니다.\n' +
      '\n' +
      '우리의 UI 백엔드는 태스크 샘플러를 사용하며, 이는 (i) 5개의 실시예(도면에 예시된 플랫폼)에 걸쳐 78개의 태스크 세트로부터 태스크를 랜덤하게 샘플링하도록 구성된다. 1) 및 (ii) 연결할 LLM 모델을 무작위로 샘플링한다. 사용자는 어떤 모델을 말하는지 모르기 때문에 공정한 블라인드 A/B 평가를 수행할 수 있습니다. 모든 실험 번호는 이 샘플러를 사용하여 수집된 데이터로 계산된다.\n' +
      '\n' +
      '사용자 관점에서, 우리의 데이터 수집 프로토콜은 우리의 평가 프로토콜과 동일하다 - 우리는 UI를 통해 모델과 상호작용하는 사용자로부터 수집된 데이터를 훈련하고, 블라인드 A/B 평가가 있는 UI를 통해 사용자가 모델이 개선되었다고 믿는지(좋은 등급 비율과 세션 성공 라벨에 대한 통계를 통해) 측정한다. 이는 로봇 학습 파이프라인(예: 수집 데이터, 훈련 및 평가의 3단계 파이프라인)의 표준 규범에서 벗어나 실제 인프라/운영 이점(주로 단순성 주변)을 제시한다.\n' +
      '\n' +
      '데이터 수집을 작동화하기 위해 각 실시예에 대해 사용자와 여러 파일럿 세션을 실행하는 것으로 시작했다. 이러한 파일럿 세션은 이 35명의 비전문가 사용자를 채팅 UI, 가르칠 것으로 예상되는 작업의 유형 및 MuJoCo 시뮬레이션 환경에 소개하는 데 중점을 두었다. 파일럿 세션이 종료된 후, 사용자는 태스크 샘플러를 통해 모든 실시예에 대해 하루에 10회의 채팅 세션을 기여해야 하며, 매일 350회의 채팅 세션에 달한다. 또한 사용자는 데이터 수집 및 전체 교육 세션에 대한 경험에 대해 매일 후 피드백을 위한 간단한 설문지를 작성하도록 요청받았다. 하루 350회의 채팅 세션의 일일 목표를 충족하기 위해서는 모든 사용자의 참여를 동등하게 유지하여 데이터에서 예상되는 다양성 수준을 얻는 것이 중요했다. 우리는 로봇 개, 모바일 매니퓰레이터, 알로하, 바이암 쿠카, 쿠카+핸드 등 여러 실시예에 걸쳐 채팅 세션 수의 일관된 분포를 유지했다.\n' +
      '\n' +
      '실험에 참여한 이용자는 23-43세(\\(M\\)=30.5, \\(SD\\)=5.6)로 시스젠더 여성 11명, 시스젠더 남성 17명, 비이진 1명이었다. 그들은 다양한 교육 학위(9개 협회 학위 또는 일부 대학, 6개 예술 학사, 11개 과학 학사, 3개 석사 학위) - 14개 비기술 및 15개 기술 학위를 가지고 있었다. ML 모델에 대한 친숙도를 1(0 친숙도)에서 5(가장 친숙도)의 척도로 질문했을 때 15명의 사용자가 1(0 친숙도)을, 11명의 사용자가 2를, 3명의 사용자가 3을 보고했으며, 어느 사용자도 ML 모델에 더 친숙하다고 보고하지 않았다(4 또는 5).\n' +
      '\n' +
      '도. 7: 오퍼레이터 성공률과 실패 시까지의 Num Chat 전환의 상관관계\n' +
      '\n' +
      '**사용자 지속성 분석.** 우리는 도 7에서 각 사용자에 걸친 실패한 세션에서의 평균 채팅 턴 수에 대한 각 사용자의 성공률을 플롯한다. 실패에 대한 평균 채팅 턴 수가 높을수록, 사용자가 로봇을 가르치는데 더 오래 있었다(즉, 사용자가 일찍 포기하지 않았다). 이 두 양은 약간의 양의 상관관계를 나타내며, 이는 평균적으로 가르침에 더 끈기 있는 사용자가 약간 더 높은 성공률을 달성했음을 시사한다.\n' +
      '\n' +
      '### _Additional Results_\n' +
      '\n' +
      '**실시형태 평가.** 도 8은 우리의 주요 교시성 결과를 나타낸다(도 5). 실시예들에 의해 분리된다. 테스트 작업에서 모델은 기본 모델에서 알로하 및 바이암 쿠카에서 가장 많이 개선된 반면 LMPC-롤아웃은 다른 모델보다 핸드를 사용한 쿠카에서 훨씬 더 많이 개선되었다.\n' +
      '\n' +
      '**채팅 지속 시간 분석.** 채팅 세션의 지속 시간을 측정 및 분석하고 채팅 전환 및 서로 다른 실시예 및 모델에 걸쳐 비교했습니다. 채팅 턴 지속 시간은 모델이 응답하는 데, 사용자가 시뮬레이션에서 로봇 코드를 실행하는 데, 사용자가 결과 로봇 동작을 관찰하는 데, 사용자가 후속 언어 피드백을 입력하는 데 걸린 총 시간을 측정한다. 도. 도 9는 모델들 및 실시예들 모두에 걸친 채팅 턴 지속기간들의 분포를 도시한다. 이러한 분포에는 명백한 차이가 없지만 일부는 다른 분포보다 더 긴 꼬리이며 중앙값 통계에서 이를 볼 수 있다. 표 VI에서, 우리는 채팅 세션에 대한 중간 지속 시간을 보여주고 채팅은 다른 실시예에 걸쳐 전환한다. Kuka+Hand 및 Bi-arm Kuka는 다른 실시예들보다 상당히 높은 지속기간을 갖는다. 이는 이러한 실시예들이 다른 실시예들보다 더 긴 지평을 갖는 태스크들을 가졌을 뿐만 아니라 가르치는 것이 더 어려웠을 가능성이 있음을 반영한다(사용자가 응답하는 데 더 오래 걸렸습니다). 표 VII에서 LMPC-롤아웃 및 LMPC-Skip의 중간 기간을 비교한다. LMPC-롤아웃은 채팅 턴과 채팅 세션 기간이 약간 더 높으며, 이러한 차이는 LMPC-롤아웃에 대한 추론(전체 채팅 세션에 대한 LLM 디코딩)이 LMPC-Skip에 대한 추론보다 약간 더 오래 걸리는 방식을 반영한다. 마지막으로, Fig. 도 10에서, 우리는 작업 성공률과 채팅 기간 사이에 작은 음의 상관 관계를 보여준다 - 사용자가 채팅 턴을 완료하는 데 더 오래 걸릴수록 해당 작업이 성공할 가능성은 더 적다.\n' +
      '\n' +
      '**성공과 실패 모두에 대해 LMPC를 훈련.**원칙적으로, LMPC-롤아웃(다이내믹스 모델로 볼 때)은 성공과 실패 데이터 모두에 대해 훈련될 수 있다(태스크 성공에서 세션이 종료되었는지 여부에 관계없이 모든 채팅 전환은 유효한 전환이기 때문이다). 이 버전에서 LMPC-롤아웃은 또한 예측된 롤아웃이 성공 또는 실패로 이어질지 여부를 (궤적 종료 시) 예측한다. 추론 시간 검색은 예측된 실패로 끝난 샘플링 롤아웃을 무시하도록 적절하게 조정됩니다. 이것이 LMPC-롤아웃의 흥미로운 측면으로 남아 있지만, 우리의 주요 실험은 성공 데이터에 대해서만 LMPC-롤아웃을 훈련한 결과(성공 데이터에 대해서만 훈련할 수 있는 LMPC-Skip과의 공정한 비교)를 보고하며, 훈련 데이터에 실패 세션을 혼합하는 동안 성능 향상을 관찰한다(표 VIII의 결과). 성공 세션과 실패 세션 모두에 대한 훈련의 대안이 실패로 종료되는 더 많은 미사용 예측 롤아웃으로 이어지기 때문에 작업 성공으로 종료된 세션에만 LMPC-롤아웃을 훈련하면 더 효율적인 추론 시간 검색이 생성된다고 가정한다.\n' +
      '\n' +
      'Autonomous Top-Users Selection___Top-Users 및 상세\n' +
      '\n' +
      '훈련 과제(부록 -D.2)에 대해 얼마나 잘 수행하는지를 평가하여 최상위 사용자를 식별하며, 과제 난이도에 가중치를 부여한다. N\\(N\\)의 작업과 K\\(K\\)의 사용자가 존재하도록 하자. (s(n,k)\\)는 \\(k)번째 사용자에 대한 \\(n\\)번째 태스크의 자체 보고 성공률을 나타내고, \\(c(n,\\,k)\\)는 \\(k)번째 사용자가 \\(n\\)번째 태스크를 가르친 횟수를 나타내고, \\(\\bar{c}(n,k)=\\mathbbm{1}(c(n,k)\\geq 1)\\)는 \\(k)번째 사용자가 \\(n\\)번째 태스크를 가르쳤는지 여부를 나타낸다. 실제적인 제약으로 인해 많은 사용자-작업 쌍에 대해 \\(\\bar{c}(n,k)=0\\이다. 우리는 작업 난이도 등급 \\(d(n)\\)을 모든 사용자의 평균 작업 실패율로 정의한다. \\(d(n)=1-\\frac{1}{K_{n}}\\sum_{k=1}^{K}s(n,k)\\bar{c}(n,k)\\), 여기서 \\(K_{n}=\\sum_{k=1}^{K}\\bar{c}(n,k)\\). 그런 다음, 작업 난이도에 가중치를 둔 사용자 평균 성공률로 사용자 수행 점수를 정의한다. \\(h(k)=\\sum_{n=1}^{N_{k}}d(n)s(n,k)\\bar{c}(n,k)\\, 여기서 \\(N_{k}=\\sum_{n=1}^{N}\\bar{c}(n,k)\\). 이 성능 점수에 따라 상위 사용자를 상위 백분위수(75%)에 있는 사용자로 정의합니다. 우리는 나머지 사용자를 "다른 사용자"라고 한다.\n' +
      '\n' +
      '표 IX는 최상위 사용자 및 기타 사용자가 분할한 기본 모델에 대한 사용자 조건 LMPC의 평균 성능 개선을 보여준다. 우리는 LMPC-Rollout(최상위 사용자 조건)이 상위 사용자에게 직접 제공될 때 가장 큰 성능 향상을 관찰하며, 이는 LMPC-Skip에서 덜 분명하며, 이는 향후 상호 작용에 대한 추론-시간 탐색(MPC를 통한)이 상위 사용자의 교시성을 개선(예: 성공 기준을 충족)하는 데 더 잘 수행됨을 시사한다.\n' +
      '\n' +
      '본 논문의 실험(표 II)은 최상위 사용자에 대한 LMPC 조정이 모든 사용자에 대한 성능 개선을 유도할 수 있음을 보여주지만, 최상위 사용자 교육 데이터가 다른 사용자와 다른 점은 무엇입니까? 이 문제를 탐구하기 위해 4개의 축을 정의합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Embodiment & Chat Session Duration (s) & Chat Turn Duration (s) \\\\ \\hline Kuka+Hand & 429 & 97 \\\\ Bi-arm Kuka & 406 & 88 \\\\ Aloha & 200 & 66 \\\\ Mobile Manipulator & 238 & 65 \\\\ Robot Dog & 138 & 41 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Median Chat Session and Chat Turn Durations across Embodiments\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Top Users & Other Users \\\\ \\hline LMPC-Skip & \\(+15.1\\%\\) & \\(+14.2\\%\\) \\\\ LMPC-Rollouts & \\(+26.3\\%\\) & \\(+18.9\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IX: Success rate improvements by user group for test tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Train Tasks & Test Tasks \\\\ \\hline LMPC-Rollouts-with-Failures & \\(-11.5\\%\\) & \\(-14.0\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VIII: Success Rates of Training LMPC-Rollouts on both Success and Failure chat sessions.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '우리는 T-SNE 임베딩 벡터를 계산하고, 각각의 임베딩을 연관된 특징들과 매핑한다: 쿼리가 "Top User"로부터 왔는지 여부, 쿼리에 대한 LLM 응답을 사용자가 "Good" 또는 "Bad"로 평가했는지 여부, 쿼리가 "Success" 또는 "Fail"을 초래한 세션으로부터 왔는지 여부, 그리고 사용된 세션을 구현하는 로봇이다. 먼저, 도 12에 도시된 바와 같이, 사용자 질의는 실제로 특정 실시예와 높은 상관 관계가 있다는 것을 발견한다. 언어 임베딩은 특정 실시예에만 존재하는 작업 또는 로봇 물리학에 특정한 특정 구문 또는 구두 제안과 같은 의미론적 세부 사항을 고려할 것이기 때문에 직관적으로 의미가 있다(예를 들어, "발을 더 높이 들어"는 로봇 도그 실시예에만 관련이 있다). 둘째, 동일한 실시예에 대해서도 "상위 사용자" 의미언어 임베딩이 "다른 사용자들"과 별도로 명확하게 클러스터링되는 경우가 있음을 알 수 있다. 또한, "다른 사용자들"이 "불량 등급"의 클러스터를 부여함으로써 LLM 응답에 대해 더 비관적으로 보이는 것과 같은 다른 흥미로운 클러스터가 발견되며, 이는 "성공" 또는 "실패"를 초래한다.\n' +
      '\n' +
      '### _Real Robotperiments_\n' +
      '\n' +
      '**로봇 개를 위한 증류.** 우리의 로봇 개 증류 정책은 트랜스포머를 사용하여 속도 명령, 고유수용성 관찰 및 과거 동작의 시퀀스를 다음 동작에 매핑하는 Locomotion-Transformer 모델을 기반으로 한다[10]. 우리는 원래 속도 명령 공식을 목적 토큰으로 MJPC 비용 가중치 및 매개변수로 일반화한다. 최종 정책은 \\(d=256\\)의 변압기와 4개의 층으로 구성되어 있으며, 총 320만 개의 매개변수가 있다.\n' +
      '\n' +
      '정책을 훈련하기 위해 정적 포즈와 이동 행동을 모두 포함하는 작업의 분배에 대해 전문가 MJPC 정책에 대해 온라인 모방 학습(DAgger)을 사용했다. 이 작업 분포는 로봇 속도, 몸통 높이 및 피치, 발 위치 및 발 디딤을 포함한 주요 목표 매개변수 값을 균일하게 무작위화하여 구성되었다. 작업 분산과 도메인 랜덤화의 다양성으로 인해 오프라인 모방(BC)이 실패하는 것으로 나타났다. 또한 강도 0.9의 지수 필터를 적용하여 모든 동작을 매끄럽게 합니다.\n' +
      '\n' +
      '**MJPC-as-Planner for real Mobile Manipulator.** 시뮬레이션에서 가르친 기술을 실제 모바일 매니퓰레이터 로봇에 배포하는 주요 실험 결과에 대해, 우리는 Yu 등의 선행 작업으로부터 MJPC-as-Planner 접근 방식을 확장한다[71]. 특히, 실제 장면의 시뮬레이트된 복제본을 획득하기 위해, Yu 등은 오픈-어휘 객체 검출기를 사용하여 장면 내의 객체들을 검출하고 세그먼트화하고 대응하는 포인트 클라우드들에 알려진 메시 모델들을 피팅하였다. 재구성된 시뮬레이션 장면은 궤적 계획을 생성하기 위해 MJPC에서 사용되며, 이는 로봇 상에서 실행된다.\n' +
      '\n' +
      '비록 선행 연구는 실세계에서 좋은 결과를 보였지만, 객체 탐지 모델을 조회하고 장면을 재현하기 위해서는 장면 내의 객체 목록과 해당 메쉬를 알아야 했다. 본 논문에서는 두 전면의 인식 파이프라인을 개선한다 : 1) VLM(Large Visual Language Model)을 사용하여 환경에서 로봇이 보는 모든 물체를 식별하고, 각각의 물체를 세분화한 후 SAM(Segment Anything) 모델[36]을 사용하여 정확한 물체 위치 파악을 수행한다. 2) 캡슐과 상자로 구성된 일반적인 원시 형상을 사용하여 물체를 표현함으로써 세밀한 메쉬를 얻을 필요 없이 넓은 범위의 물체를 표현할 수 있다. 결과적으로, 우리는 알려지지 않은 물체가 있는 보다 다양한 환경에 접근 방식을 적용하고 로봇 조작 기술을 가르칠 수 있다. 예를 들어, 도 1에서 확인할 수 있다. 14.\n' +
      '\n' +
      '**Sim-to-Real Gap.** 표 XI는 실제 세계에서 우리가 평가한 작업 세트에 대한 sim과 실제 성능 간의 비교를 보여준다. 개방형 서랍 작업의 경우, 이 작업은 준정적이어서 물리적 영역 간격이 거의 없기 때문에 실제 세계에서 100%의 성공률을 달성한다. 이와는 대조적으로, 노크 오버 코크는 엔드 이펙터의 속도가 충분히 빠르지 않기 때문에 실제 세계에서 단지 \\(20\\%\\)의 성공률을 달성할 수 있다. 이것은 로봇이 더 낮은 엔드 이펙터 속도로 코크스 캔을 넘어뜨릴 수 있는 물리적 모델링 도메인 갭에 의해 발생한다. 작업 전환 시 홉에 대해 시뮬레이션과 실제 세계 사이의 큰 불일치를 관찰한다. 우리가 로봇에게 깡충깡충 뛰도록 가르칠 수 있지만, 그것은 종종 몇 번 깡충 뛰다가 넘어집니다. 이는 선회 행동을 하면서 매우 민첩한 호핑이 증류 정책의 훈련 분포 밖에 있기 때문이다. 증류 훈련 분포를 교수 데이터에 적응시키는 것은 향후 연구에 유망한 방향이다.\n' +
      '\n' +
      '**모바일 매니퓰레이터를 위한 증류.** MJPC-as-Planner 접근법에는 몇 가지 제한 사항이 있다: 1) MJPC로 계획을 생성하는 것은 계산 요구 사항으로 인해 온보드 컴퓨팅에 적합하지 않으며, 2) 객체를 식별하고 분할하기 위해 여러 모델이 필요하며, 시스템에 추가적인 복잡성을 추가하며, 3) 실행 중 환경의 변화에 반응하지 않는다. 이러한 문제를 해결하기 위해 우리는 모바일 매니퓰레이터의 로봇 개에 사용되는 보상 조건 정책 증류 접근법을 탐구한다. 구체적으로, 두 가지 설정에서 아이디어를 검증한다: 1) 피킹 작업에 대한 최종 객체 높이를 조건으로 리워드를 사용하고, 2) 컨디션 피킹 또는 캔을 두드리는 조건으로 리워드를 사용한다. MJPC를 사용하여 최대 150단계로 각각 30k 및 10k 궤적을 생성한다. 각 단계의 로봇 관찰은 로봇 카메라의 시뮬레이션된 깊이 영상과 보상 파라미터로 구성된다. 생성된 데이터셋을 이용하여 RT-1 모델[8]을 기반으로 정책을 학습하고 실제 모바일 매니퓰레이터 로봇에 정책을 전개하였다. 보상 조건 정책을 확장하면 온보드 컴퓨팅으로 정책을 배포하고 폐루프 제어로 보다 강력한 동작을 달성할 수 있다. 우리는 아직 증류된 모바일 매니퓰레이터 정책에 대한 정량적 평가를 수행하지 않았지만 그림 1에서 예시적인 정책 롤아웃을 보여준다. 15.\n' +
      '\n' +
      '###_언어 모델 학습 상세_\n' +
      '\n' +
      '피네튜닝 모델의 경우 사용 가능한 학습 데이터의 \\(10\\) 에폭에 해당하는 학습 단계의 수를 설정하고 Adam을 적용한다.\n' +
      '\n' +
      '도. 13: 전문가 및 비전문가, 및 양호/불량 채팅 등급(좌측)에 걸친 인간 피드백의 임베딩의 T-SNE 플롯 및 그 피드백이 결국 성공/실패(우측)였던 채팅 세션에 속하는지 여부.\n' +
      '\n' +
      '학습률은 \\(5\\times 10^{-3}\\), 선형 램프 업 및 코사인 감쇠 학습률 스케줄러, 배치 크기 4, 컨텍스트 길이 4096 토큰이다.\n' +
      '\n' +
      'LMPC-Rollout 모델은 나머지 채팅 세션 전체를 예측해야 하기 때문에 추론 시간에 LMPC-Skip보다 훨씬 느리다. 사용자 피드백에 따르면, 느려진 추론 시간은 교수 경험을 저하시켜, 채팅 세션을 덜 매력적으로 만들고, 잠재적으로 데이터 품질을 감소시킨다. 이 문제를 해결하기 위해\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c} \\hline \\hline\n' +
      '**Embodiment** & **Task** & **Ours-Sim** & **Ours-Real** & **N Chat Turns** & **PaLM 2-S Sim** & **PaLM 2-S Real** & **N Chat Turns** \\\\ \\hline Robot Dog & High-Five with left hand & \\(100\\%\\) & \\(100\\%\\) & \\(3.0\\) & \\(100\\%\\) & \\(75\\%\\) & \\(2.3\\) \\\\ Downward Dog & \\(100\\%\\) & \\(100\\%\\) & \\(2.8\\) & \\(100\\%\\) & \\(100\\%\\) & \\(1.3\\) \\\\ Walk forward in a trotting gait & \\(100\\%\\) & \\(100\\%\\) & \\(2.8\\) & \\(25\\%\\) & \\(25\\%\\) & \\(2.0\\) \\\\ Hop & \\(100\\%\\) & \\(75\\%\\) & \\(2.3\\) & \\(50\\%\\) & \\(25\\%\\) & \\(2.0\\) \\\\ Hop while turning counterclockwise & \\(100\\%\\) & \\(25\\%\\) & \\(4.0\\) & \\(100\\%\\) & \\(25\\%\\) & \\(5.0\\) \\\\ \\hline Mobile Manipulator & Open top drawer half-way & \\(100\\%\\) & \\(100\\%\\) & \\(3.2\\) & \\(100\\%\\) & \\(100\\%\\) & \\(3.4\\) \\\\ Push coke can from right to left & \\(100\\%\\) & \\(80\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(60\\%\\) & \\(2.0\\) \\\\ Knock over coke can & \\(100\\%\\) & \\(20\\%\\) & \\(3.0\\) & \\(80\\%\\) & \\(20\\%\\) & \\(5.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XI: Sim vs. Real Results\n' +
      '\n' +
      '도. 14: MJPC-as-Planner 접근법을 사용하는 모바일 매니퓰레이터에 대한 현실 세계 교시 예.\n' +
      '\n' +
      '도. 15: **Example Rollout of reward conditioned distilled policy on mobile manipulator.** Apart of MPJC-as-Planner for real world deployment, 또한, 로봇 개 예제에 따라 모방 학습을 이용하여 행동을 정책으로 증류하는 것을 탐색하였다. 이것은 더 이상 정확한 상태 추정을 필요로 하지 않는다.\n' +
      '\n' +
      '우리는 미세 조정 후 LMPC-Rollout 모델에 대해 8비트 양자화를 수행했으며, 양자화된 LMPC-Rollout 모델에 서비스를 제공한다. 우리는 양자화된 모델에서 눈에 띄게 성능이 떨어지는 것을 관찰하지 못했다. 이러한 모델에 대한 측정된 추론 시간은 표 XII를 참조하십시오. 양자화가 없는 LMPC-롤아웃은 LMPC-Skip보다 훨씬 느리지만 양자화가 있는 경우 추론 시간은 유사합니다.\n' +
      '\n' +
      '###_사용자 성능 드리프트 분석\n' +
      '\n' +
      '장기간에 걸쳐 모델을 평가하는 것은 사용자가 로봇을 가르치는 연습을 더 많이 얻을수록 더 능숙해지고 모델 성능 향상이 실제로 개선된 모델 능력 대신 사용자의 향상된 교수 기술로 인해 발생할 수 있다는 우려를 도입한다. 우리는 이 우려를 완화하기 위해 세 가지 조치를 취했다. 먼저, 각 로봇 실시예에 대한 파일럿 데이터 수집 세션을 수행하여 사용자가 공식 데이터 수집을 수행하기 전에 각 실시예에 대한 기본 친숙도를 획득할 수 있었다. 둘째, 동일한 데이터 수집일 동안 LMPC 변형을 평가했기 때문에 사용자 성능 드리프트로 인해 차이가 발생할 가능성이 낮다. 셋째, 실험 전반기와 후반기 동안 사용자 성능을 베이스 LLM과 명시적으로 비교했다. 첫 번째 기간부터 두 번째 기간까지 각 사용자의 평균 성공률은 모든 작업에서 \\(-0.6\\%\\) 변화했으며 표준 편차는 \\(9.3\\%\\)이고 두 기간은 피어슨 상관계수 \\(0.87\\)을 보여준다.\n' +
      '\n' +
      '사용자들의 수업경험에 대한 잠재적 변화를 측정하기 위한 또 다른 방법은 하루가 끝날 때 교사의 자기보고식 인지부하를 측정하는 것이다. 사용자의 하위 집합이 서로 다르기 때문에, 각 사용자가 첫 날과 마지막 날에 로봇을 가르치는 인지부하를 어떻게 경험했는지에 대한 측면에서 데이터를 분석하였고, NASA-TLX 인지부하 측정값의 하위 집합을 사용하여 인지된 정신적 요구, 노력, 수행, 좌절감 차원을 분석하였다. (p<0.26), 노력(p<0.47), 수행(p<0.22) 또는 좌절(p<0.54). 이들은 모두 0.05의 통계적 유의성에 대한 컷오프 p-값보다 훨씬 높으며, 본페로니 보정을 사용하면 컷오프 값이 0125에서 훨씬 낮다.\n' +
      '\n' +
      '이러한 결과는 시간이 지남에 따라 사용자 성능이나 사용자 경험 변화가 최소화됨을 시사하므로 모델 간의 차이는 사용자 교수 숙련도가 아닌 모델 능력의 변화의 결과일 가능성이 더 높다.\n' +
      '\n' +
      '### _ failure mode analysis_\n' +
      '\n' +
      '비교된 모델에 걸쳐 다음과 같은 고장 모드를 분류했습니다. 고장 모드 1)은 오류가 있는 코드 또는 실행 가능한 코드를 출력하는 것이다. 실행 가능한 코드를 출력하는 경우, 2) 반복된 코드 출력에서 실패했는지, 3) 불완전한 계획에서 실패했는지 또는 4) LLM에서 사용자의 피드백에 응답하지 않는지 추가로 확인했다. 이러한 장애 모드를 식별하기 위해 LLM이 채팅 세션 데이터에서 분류하도록 프롬프트했다.\n' +
      '\n' +
      '표 XIII를 참조하십시오. 여기서 모든 대화 세션에 걸쳐 각 장애 모드를 초래한 대화 세션의 백분율을 표시합니다. 분모는 해당 모델의 전체 대화 세션 수이지 해당 모델의 장애 수는 아닙니다. 특정 채팅 세션이 여러 장애 모드로 나타날 수 있으므로 이러한 장애 모드 세트가 구분되지 않습니다. 예상대로 LMPC 모델은 기준선보다 전반적으로 고장이 적습니다. 가장 빈번한 고장 모드는 사용자 피드백에 응답하지 않는 코드를 출력하는 것이다. 최빈 고장 모드는 불완전한 코드를 출력하는 것이다.\n' +
      '\n' +
      '코드라이팅 벤치마크s_의 미세조정된 모델\n' +
      '\n' +
      '모델 미세 조정에 대한 한 가지 우려는 미세 조정 모델이 원래 기능 중 일부를 잊어버릴 수 있다는 것입니다. 우리의 경우, 우리는 특히 우리의 모델을 미세조정하는 것이 LLM의 일반적인 코드-쓰기 능력을 저하시키는지 여부에 대해 우려한다. 이를 테스트하기 위해 로보CdeGen 벤치마크에서 미세 조정을 1회 및 2회 반복한 후 모델을 평가했다[41]. 표 XIV에서 볼 수 있듯이, 첫 번째 반복과 기준선 모델 사이 또는 첫 번째와 두 번째 반복 사이에는 상대적으로 열화가 없다. 우리는 이것을 훈련 데이터가 코드 사용뿐만 아니라 기본 LLM의 분포에 있으므로 모델을 코드 생성에서 편향시키지 않기 때문이라고 생각한다.\n' +
      '\n' +
      '###_로보트 실시예 상세_\n' +
      '\n' +
      '**로봇견**로봇견은 탑재된 컴퓨터와 배터리 전원을 갖춘 소형 4족 로봇입니다. 그 로봇은 [10]의 디자인을 바탕으로 사내에서 개발되었다. 로봇의 스탠딩 높이는 약 0.4m입니다. 로봇의 4개의 다리 각각에는 최대 관절 토크가 18Nm인 3개의 DoF가 있습니다.\n' +
      '\n' +
      '증류 정책(섹션 VI-G)은 온보드 컴퓨터(Intel NUC11TNBv7)에서 실행되며 1kHz에서 조인트 토크를 출력하는 저레벨 PD 컨트롤러에 50Hz에서 조인트 위치 명령을 제공한다. P-이득은 50 Nm/rad, D-이득은 1.1 Nm s/rad로 설정하였다. Wi-Fi 연결을 통해 ROS2를 사용하여 사용자가 채팅 UI에서 _Run on Robot_ 명령을 클릭하면 데스크탑 컴퓨터에서 모델 출력(보상 기능 코드)을 로봇으로 전송한다.\n' +
      '\n' +
      '로봇 개를 위한 시뮬레이션 환경은 걸쇠가 없는 문과 3단계 주방 서랍이 있습니다. MJPC 구현에서는 로봇 몸통 및 엔드 이펙터에 대한 위치 및 방향 센서와 관절 객체에 대한 관절 센서(예: 도어 힌지 각도)를 배치한다. 그런 다음 LLM이 원하는 절대 및 상대 센서 값을 변조하는 데 사용할 수 있는 API 세트를 설계한다( 프롬프트에서 자세한 내용을 참조). MJPC 시뮬레이션에서 다양한 다리의 움직임을 조정함으로써 로봇 개는 이동에서 포즈에 이르기까지 풍부한 기술을 얻을 수 있다. 나아가, 로봇 개는 그리퍼 형태가 전혀 없지만, 몸통과 팔다리를 이용하여 외부 세계와 상호 작용하여 도어를 열거나 서랍을 닫는 등의 작업을 수행할 수 있다.\n' +
      '\n' +
      '**모바일 매니퓰레이터.** 모바일 매니퓰레이터[24]는 7-DoF 암과 평행 죠 그리퍼로 구성된다. 시뮬레이션된 환경은 서랍이 있는 카운터에 놓인 가정용 물체(애플, 코크스 캔, 큐브) 앞에 시뮬레이션된 로봇을 포함한다.\n' +
      '\n' +
      'MJPC 구현을 위해 로봇에는 그리퍼와 관절 센서, 가정용 물체에는 위치 및 방향 센서, 관절형 물체에는 관절 센서(예: 서랍 힌지)를 배치한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline LMPC-Skip & LMPC-Rollouts & LMPC-Rollouts-No-Quantization \\\\ \\hline \\(1.1\\pm 0.2\\) & \\(1.0\\pm 0.4\\) & \\(7.4\\pm 4.7\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: Model inference times in seconds.\n' +
      '\n' +
      'MJPC 플래너를 통해 로봇은 서랍 반쪽 열기, 물체 B를 넘어뜨리지 않고 물체 A를 특정 위치로 픽업/푸싱하기, 물체를 특정 위치 및 방향까지 들어올리기 등 다양한 제약 조건을 가진 풍부한 작업 세트를 실행할 수 있다. 우리는 실제 로봇에 대한 기술을 추가로 테스트합니다.\n' +
      '\n' +
      '우리는 Wi-Fi를 통한 gRPC 연결을 사용하여 사용자가 채팅 UI에서 _Run on Robot_ 명령을 클릭할 때 모델이 출력하는 보상 기능 코드를 데스크톱 컴퓨터에서 로봇으로 전송한다. **알로하.** 알로하 이중 수동 로봇 [76]은 테이블에 고정된 2개의 6-DoF 암으로 구성되며, 각각 1-DoF 평행 그리퍼가 있다. 가정용품(예: 사과, 탄산음료 캔, 큐브, 그릇)이 테이블 위에 놓여 있습니다.\n' +
      '\n' +
      'MJPC 구현은 각 암의 위치 및 방향을 위한 센서와 작업 공간 내의 모든 객체의 위치 및 방향을 얻기 위한 API를 포함한다. MJPC를 사용하면 알로하에서는 객체를 선택/배치하고, 양팔을 사용하여 객체의 방향을 재지정하고, 한 팔에서 다른 팔로 객체를 넘기는 등 매우 다양한 작업을 수행할 수 있다.\n' +
      '\n' +
      'MJPC가 14DoF 액션 공간에서 성공적으로 계획할 수 있도록 25% 실시간 속도로 시뮬레이션을 실행했다. MJPC는 테이블 위에서 사과를 굴려서 다른 물체에 더 가까이 이동시키거나 한 팔을 지렛대로 사용하여 다른 팔로 그릇을 거꾸로 뒤집는 것과 같은 작업을 해결하기 위한 동적 행동을 찾을 수 있다. 이러한 정책은 시뮬레이션에서만 테스트되며 실제로의 전송을 위해 조정되지 않습니다.\n' +
      '\n' +
      '**Bi-arm Kuka.**Kuka bi-arm 로봇은 지면에 고정된 2개의 7-DoF Kuka LBR IIWA14 암으로 구성된다. 이 작업의 범위에는 엔드 이펙터가 부착되지 않았다. 장면에서 방향 매핑에 자연 언어와 관련하여 데이터 수집을 용이하게 하기 위해 "왼쪽" 및 "오른쪽" 단어와 함께 화살표 표시기가 추가되었다. 이 실시예를 위해, 우리는 두 개의 별개의 장면들을 전개했다:\n' +
      '\n' +
      '1. **단일 대형 큐브**: 이 장면은 팔의 작업 공간의 중앙에 단일 대형 큐브를 포함한다.\n' +
      '2. **입자 조작**: 이 장면은 팔의 작업 공간 내의 임의의 위치에서 초기화된 적색, 녹색, 청색, 황색, 보라색-의 다양한 색상의 5개의 작은 큐브(입자)를 포함한다. 보다 구체적으로, 초기화 시 이들의 x 및 y 좌표는 각각 (\\(-0.5\\),\\(0.5\\)) 사이의 균일한 분포로부터 랜덤하게 샘플링된다.\n' +
      '\n' +
      '마지막으로 두 장면 모두 4개의 개별 골포인트를 포함하고 있는데, 공중에 2개(파란색 골과 빨간색 골)와 지면에 2개(녹색 골과 보라색 골)가 고정되어 있으며 움직이는 물체의 목표 위치로 사용할 수 있다.\n' +
      '\n' +
      '본 실시예에 대한 MJPC 구현은 각각의 암, 각각의 목표 및 각각의 이동 가능한 객체의 위치 및 배향을 위한 센서들을 포함한다. 또한 모든 객체의 포즈를 획득하고 설정할 수 있는 API를 포함한다. 이 플랫폼의 경우, MJPC는 큰 큐브를 목표 위치를 향해 이동하거나 상대 위치에서, 큐브를 픽업, 큐브를 스윕하는 것과 같은 단일 작업을 수행하기 위해 활용되며, 하나의 블록을 다른 블록을 향해 이동하는 것과 같은 순차적인 작업은 세 번째 블록 또는 목표 위치를 향한 후속 이동을 뒤따른다.\n' +
      '\n' +
      '본 실시예의 현재 제한은 시뮬레이션에서만 평가된다는 것이다. 암 컨트롤뿐만 아니라 설정도 아직 현실적이지 않으며 심2리얼 전송을 방해할 수 있습니다. 또한 이 작업에서 제시된 모든 결과에 대해 양팔 쿠카는 훈련이 아닌 테스트에만 사용되는 보이지 않는 실시예로 간주되었다. 이것은 채팅 UI 상호작용의 수를 증가시킬 수 있는 생성된 코드(예를 들어, 다른 실시예로부터의 API와 같은 마이너 코드 실수들이 경우에 따라 생성될 수 있음)의 관점에서 일부 도메인 이동을 야기한다. **Kuka+Hand.** 이 실시예는 세 손가락(각각 4-DoF)을 가진 맞춤형 손으로 부착된 7-DoF Kuka LBR IIWA14 암을 포함한다. 팔은 빨간색과 녹색 블록, 플러그 베이스에 삽입할 수 있는 커넥터의 네 가지 물체가 들어 있는 바스켓 내에 고정됩니다.\n' +
      '\n' +
      '본 실시예에 대한 시뮬레이션 및 MJPC 구현은 모든 손가락 및 팔 관절의 위치 및 손 및 장면의 모든 객체의 포즈/배향(pose/orientation)을 제공하는 센서를 포함한다.\n' +
      '\n' +
      'MJPC를 사용하면, Kuka+Hand는 물체를 재배열하기 위해 손가락을 능숙하게 사용하는 것과 같은 많은 흥미로운 행동을 실행할 수 있다. 물체와의 접촉을 유지하는 데 부과되는 제약이 없기 때문에, 우리는 손가락이 때때로 동적 조작(예를 들어, 작업 공간의 한 부분에서 다른 부분으로 물체를 플릭하거나, 물체를 아래로 놓기 위해 파지하기 전에 공기 중 방향을 재조정하기 위해 저글링)을 이용할 수 있다는 것을 관찰한다. 물론 주의사항은 이러한 행동이 시뮬레이션에서 최적화되고 예측 제어 롤아웃 동안 객체 포즈 정보를 필요로 한다는 것이다(심2real 증류를 통해 실제 세계로 전달하는데 어려움을 겪을 수 있음).\n' +
      '\n' +
      '제한점에서는 모든 \\(7\\!+\\!4\\!\\times\\!3\\!=\\!19\\)의 능숙한 조작을 위한 예측제어 탐색공간이다. 자유도는 크고 샘플링 기반 제어를 수행하기 어려울 수 있다. 따라서 시뮬레이터는 조작 솔루션을 발견하기 위해 실시간 속도(128개의 코어를 가진 계산 결합 MJPC를 허용)의 15%에서만 실행된다. 채팅 턴 지속 시간(표 VI에 표시됨)은 쿠카+핸드 플랫폼이 사용자가 가르치는 데 가장 긴 시간이 소요됨을 시사합니다. 각 채팅 턴은 평균 1.5분이 소요되고, 채팅 세션은 약 7분이 소요되며, 대부분의 시간은 로봇이 작업을 수행하는 방법을 온라인으로 "구축"하는 데 소요됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{Pass@1} \\\\  & Iteration 1 & Iteration 2 \\\\ \\hline PalLM 2-S & \\multicolumn{3}{c}{51\\%} \\\\ LMPC-Kollouts & 51\\% & 51\\% \\\\ LMPC-Skip & 49\\% & 49\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIV: Performance on RoboCodeGen on finetuned models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{Failure Modes} \\\\ \\cline{2-5} Model & Invalid Code & Repeated Code & Non-responsive Code & Incomplete Code & All Failures \\\\ \\hline PaLM 2-S & 17.4\\% & 10.9\\% & 16.8\\% & 7.6\\% & 35.3\\% \\\\ RAG & 6.4\\% & **6.7\\%** & 19.8\\% & 6.4\\% & 38.5\\% \\\\ LMPC-Skip & 9.5\\% & 7.6\\% & 11.9\\% & **3.8\\%** & **23.0\\%** \\\\ LMPC-Rollout & **7.8\\%** & 7.0\\% & **11.3\\%** & 4.0\\% & 24.7\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Failure Mode as percentage of all chat sessions.\n' +
      '\n' +
      '### _Tasks_\n' +
      '\n' +
      '**로봇 도그.** 로봇 도그 실시예에 대한 19개의 작업을 설계하며, 그 중 12개는 LLM 훈련에 사용된다:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Robot Dog Train Tasks** \\\\ \\hline Sit. \\\\ High-five with the front right paw. \\\\ Downward dog. \\\\ Walk to the left. \\\\ Walk forward. \\\\ Hop. \\\\ Turn around clockwise. \\\\ Walk backward. \\\\ Walk backward while turning to face right. \\\\ Walk forward while turning left. \\\\ Close the middle drawer. \\\\ Open the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '그리고 7은 LLM 성능을 테스트하기 위해 보류된다:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Robot Dog Test Tasks** \\\\ \\hline High-five with the front left paw. \\\\ Walk to the right. \\\\ Turn around counterclockwise. \\\\ Hop while turning clockwise. \\\\ Close the bottom drawer. \\\\ Close the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Robot Dog Test Tasks** \\\\ \\hline High-five with the front left paw. \\\\ Walk to the right. \\\\ Turn around counterclockwise. \\\\ Hop while turning clockwise. \\\\ Close the bottom drawer. \\\\ Close the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**로봇 매니퓰레이터.** 우리는 사용자에게 모바일 매니퓰레이터 14개의 작업을 가르치도록 작업하며, 그 중 11개는 훈련에 사용된다:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Mobile Manipulator Train Tasks** \\\\ \\hline Grasp the apple. \\\\ Knock over coke can. \\\\ Lift the apple high. \\\\ Place the apple next to the cube. \\\\ Push the apple toward the cube. \\\\ Move the cube further away from the robot. \\\\ Move the cube a little bit to the left. \\\\ Open the top drawer. \\\\ Place the cube behind the apple. \\\\ Flip the cube upside down. \\\\ Place the apple on the cube. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Aloha.** 로봇이 사용자들로부터 총 16개의 작업을 수행하도록 지시받았으며, LLM 훈련에 사용된 것은 다음과 같다:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Aloha Train Tasks** \\\\ \\hline Grasp the apple and lift it up. \\\\ Grasp the coke can and lift it up. \\\\ Pick up the cube and lift it above the apple. \\\\ Pick up the box and lift it above the coke can. \\\\ Flip the apple upside down. \\\\ Flip the drink upside down. \\\\ Flip the apple upside down and move the apple to the center \\\\ of the table. \\\\ Move the box and the apple close to each other. \\\\ Push the box and the bowl close to each other. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Bi-arm Kuka.** 로봇은 이용 가능한 두 장면에 걸쳐 16개의 상이한 작업(테스트만)을 수행하도록 사용자에 의해 지시되었다:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Bi-arm Kuka Single Large Cube Scene Test Tasks** \\\\ \\hline Pick up the cube and lift it up to the blue goal. \\\\ Pick up the cube and lift it up by 20cm. \\\\ Move the cube to the green goal on the floor. \\\\ Move the cube 20cm to the right without rotating it. \\\\ Pick up the cube and lift it up to the red goal. \\\\ Move the cube 20cm to the left of the purple goal on the floor \\\\ and rotate it 90 degrees. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Bi-arm Kuka Particle Manipulation Scene Test Tasks** \\\\ \\hline Move the blue cube to the green goal on the floor. \\\\ Move the green cube 20cm to the right. \\\\ Move the red cube to the green goal, then to the purple goal. \\\\ Sweep the red cube and the blue cube towards the green goal. \\\\ Sweep all the cubes to the purple goal. \\\\ Bring the red cube 20cm to the left of the green goal. \\\\ Move the blue cube 20cm in front of the green cube. \\\\ Sweep the yellow cube to the blue cube, then to the red cube. \\\\ Move the purple cube to the yellow cube, then to the green \\\\ cube, then to the blue cube. \\\\ Move the purple cube 10cm to the right of the yellow cube, \\\\ then 20cm behind the blue cube. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Kuka+Hand.** 로봇이 사용자들로부터 18개의 상이한 작업들(테스트만)을 수행하도록 지시받았다:\n' +
      '\n' +
      '### _Prompts_\n' +
      '\n' +
      '각 실시예에 대한 프롬프트가 도에 도시되어 있다. 도 16을 참조하면, 도 17을 참조하면, 도 18을 참조하면, 도 19, 및 도 6을 참조하면 다음과 같다. 20개씩. LLM들은 각각의 로봇 실시예에 대해 미리 마련된 입력 프롬프트들로 세션 데이터를 완료하도록 트레이닝된다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:24]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:26]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>