<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Learning to Learn Faster from Human Feedback\n' +
      '\n' +
      'with Language Model Predictive Control\n' +
      '\n' +
      'Jacky Liang\\({}^{*}\\), Fei Xia\\({}^{*}\\), Wenhao Yu\\({}^{*}\\), Andy Zeng\\({}^{*}\\)\n' +
      '\n' +
      'Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed, Chuyuan Kelly Fu\n' +
      '\n' +
      'Nimrod Gileadi, Marissa Giustina, Keerthana Gopalakrishnan, Leonard Hasenclever, Jan Humplik, Jasmine Hsu, Nikhil Joshi, Ben Jvenis, Chase Kew, Sean Kirmani, Tsang-Wei Edward Lee, Kuang-Huei Lee, Assaf Hurwitz Michaely, Joss Moore, Ken Oslund\n' +
      '\n' +
      'Dushyant Rao, Allen Ren, Baruch Tabanpour, Quan Vuong, Ayzana Wahid, Ted Xiao, Ying Xu, Vincent Zhuang\n' +
      '\n' +
      'Peng Xu\\({}^{\\dagger}\\), Erik Frey\\({}^{\\dagger}\\), Ken Caluwaerts\\({}^{\\dagger}\\),Tingnan Zhang\\({}^{\\dagger}\\), Brian Ichter\\({}^{\\dagger}\\), Jonathan Tompson\\({}^{\\dagger}\\), Leila Takayama\\({}^{\\dagger}\\), Vincent Vanhoucke\\({}^{\\dagger}\\)\n' +
      '\n' +
      'Izhak Shafran\\({}^{\\dagger}\\), Maja Mataric\\({}^{\\dagger}\\), Dorsa Sadigh\\({}^{\\dagger}\\), Nicolas Heess\\({}^{\\dagger}\\), Kanishka Rao\\({}^{\\dagger}\\), Nik Stewart\\({}^{\\dagger}\\), Jie Tan\\({}^{\\dagger}\\), Carolina Parada\\({}^{\\dagger}\\)\n' +
      '\n' +
      '\\({}^{*}\\)corresponding authors in alphabetical order, \\({}^{\\dagger}\\)advising leads, all other authors in alphabetical order\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models (LLMs) have been shown to exhibit a wide range of capabilities, such as writing robot code from language commands - enabling non-experts to direct robot behaviors, modify them based on feedback, or compose them to perform new tasks. However, these capabilities (driven by in-context learning) are limited to short-term interactions, where users\' feedback remains relevant for only as long as it fits within the context size of the LLM, and can be forgotten over longer interactions. In this work, we investigate fine-tuning the robot code-writing LLMs, to remember their in-context interactions and improve their _reachability_ to, how efficiently they adapt to human inputs (measured by average number of corrections before the user considers the task successful). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions can be viewed as training a transition dynamics model - that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths of success. This gives rise to Language Model Predictive Control (LMPC), a framework that fine-tunes PALM 2 to improve its reachability on 78 tasks across 5 robot embodiments - improving non-expert teaching success rates of unseen tasks by \\(26.9\\%\\) while reducing the average number of human corrections from \\(2.4\\) to \\(1.9\\). Experiments show that LMPC also produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs by \\(31.5\\%\\). See videos, code, and demos at: [https://robot-teaching.github.io/](https://robot-teaching.github.io/).\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'Natural language provides a rich and accessible interface for teaching robots - with the potential to enable anyone with minimal training to direct behaviors, express preferences, and provide feedback. Recent works show that large language models (LLMs), pretrained on Internet-scale data, exhibit out-of-the-box capabilities that can be applied to robotics - from planning a sequence of steps given language commands [1, 30], to writing robot code [41, 59, 71, 45]. Language inputs can also be sequenced in a multi-turn setting for example, to generate and modify reward function code from human feedback to compose new quadruped behaviors via real-time motion control [71] (example in Fig. 1).\n' +
      '\n' +
      'LLM-based robot teaching (as shown in Fig. 1) can be driven by in-context learning [9] (e.g., on code and dialogue data), where previous interactions are kept as input context for subsequent ones. In-context learning occurs during inference without gradient updates to model weights, enabling fast adaptation to language instructions (via exemplar-based compositional generalization [11, 32]). However, this adaptation is limited to short-term reactive interactions where the users\' feedback remains relevant for only as long as it fits within the context size of the LLM. As a result, if human instructions accumulate over longer multi-step interactions that fall outside the receding\n' +
      '\n' +
      'Fig. 1: Code-writing large language models (LLMs) present opportunities for non-experts to teach robots new tasks with language - enabled by fast adaptation via in-context learning (left). In this work, we fine-tune the underlying LLMs to further accelerate fast adaptation and improve their reachability (right). Results with human-robot interactions from non-experts teaching \\(\\gamma\\) robot embodiments on 78 tasks (gray) show that our framework (middle\\({}^{*}\\)) can identify top performing users (purple), and leverage their interactions (only 14% of task coverage) to drive LLM performance improvements for all users (blue) – measured in terms teaching success rates on unseen tasks, responsiveness to user feedback, and number of user corrections. Experiments show that these improvements generalize to new robot embodiments and APIs.\n' +
      '\n' +
      'context horizon, previous instructions can simply be forgotten.\n' +
      '\n' +
      'We are interested in improving LLMs\' _teachability_ for robot tasks, i.e., how efficiently they adapt to human feedback, by enabling LLMs to remember their in-context interactions. Teachability in multi-turn language-based human-robot interaction (HRI) can be measured as the average number of human inputs (e.g., corrections) \\(n\\) before the robot succeeds at the task. For instance, \\(n\\!=\\!1\\) refers to the standard zero-shot instruction following setting [33, 44]. Prior works propose to improve teachability by generating linguistic summaries of human feedback [75] or preferences [68] that can be indexed into memory and later retrieved in-context to guide future interactions. However, such methods are often constrained by in-context learning generalization (observed to be more "exemplar-based" i.e., on the basis of similarity to in-context examples [11, 57]), as opposed to generalization from in-weights learning via fine-tuning (which tends to be more "rule-based" i.e., on the basis of minimal features that support category boundaries in the training data [11, 6]). Subsequently, prior methods excel at overfitting to training tasks, but offer limited generalization (e.g., domain-level adaptation) to unseen tasks. Is it possible to leverage both forms of learning to address these shortcomings?\n' +
      '\n' +
      'In this work, we investigate improving the teachability of robot code-writing LLMs via _in-context learning_ (fast adaptation) by day, and model _fine-tuning_ (slow adaptation) by night, to accelerate fast adaptation the next day.1 Given a setting where non-experts teach robots new tasks with language, our goal is to study which methods of improvement (e.g., via fine-tuning) can best leverage data collected from in-context learning to improve future teachability (as measured on unseen tasks). Our key observation is that when human-robot interactions are formulated as a partially observable Markov decision process (POMDP - in which human language inputs are observations, and robot code outputs are actions), then training an LLM to autoregressively complete previous interactions can be viewed as training a transition dynamics model - that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success. This gives rise to Language Model Predictive Control (LMPC), where we train the LLM to predict imagined future rollouts of human-robot interactions - and at inference time, sample multiple futures (with non-zero decoding temperature) to search for the best one and take the next action (i.e., receding horizon control as a decoding strategy). Classically challenging HRI problems (such as modeling individual user preferences) become more straightforward e.g., by simply conditioning LMPC rollouts on usernames ("user _might say..."), with the intuition that different users cover different areas of the POMDP.\n' +
      '\n' +
      'Footnote 1: In-context learning by day & fine-tuning by night analogy is loosely inspired by the role of circadian rhythms in the learning and memory of biological systems.\n' +
      '\n' +
      'Extensive experiments (via blind A/B evaluations) show that fine-tuning with LMPC improves the teachability of PaLM 2 [3] on 78 tasks across 5 robot embodiments (on simulated and real platforms) - enabling non-experts to teach robots to achieve higher success rates on unseen tasks by \\(26.9\\%\\), and reduces average number of human corrections from \\(2.4\\) to \\(1.9\\). In particular, LMPC produces strong meta-learners - teachability improvements generalize to unseen embodiments, improving the success rate of in-context learning new tasks with new robot APIs by \\(31.5\\%\\). Interestingly, we observe substantial gains from top-user conditioned LMPC, which (i) autonomously identifies top users (by performance on training tasks), (ii) groups their data together with a special username "top-user," then (iii) conditions inference-time LMPC rollouts on this special username (i.e., assume everyone is a top-user). Despite top users having seen only 14% of tasks, experiments show this conditioning mechanism drives performance improvements for all users on all tasks, including unseen ones by \\(10.5\\%\\). LMPC also outperforms retrieval baselines [75], and user studies affirm that performance improvements are likely the result of changes in model capability, rather than user teaching proficiency. Our approach is not without limitations - we discuss these and areas for future work in Section V. Videos, code, and datasets will be released.\n' +
      '\n' +
      '## II Related Work\n' +
      '\n' +
      '**Language and Robotics.** A large body of work integrates language and robotics, including mapping language to planning primitives [63, 37, 46, 5, 35], imitation learning from demonstrations along with language instructions [33, 44, 58, 60, 47], learning language-conditioned reward functions [48, 34, 21, 14, 50, 2], and using language as corrective feedback to adapt or define new behaviors [75, 16, 15]. We refer the reader to comprehensive surveys for a more complete review of prior work in this area [64, 43].\n' +
      '\n' +
      'Recently, LLMs trained on Internet-scale data have been shown to exhibit profound capabilities ranging from step-by-step planning [1, 30, 18, 69, 17, 42, 68, 54], writing robot code [41, 59, 73, 70, 4, 4], commonsense reasoning [62, 39], and acting as a proxy reward function capturing human preferences [38, 29, 71]. In this work, we are also interested in leveraging the power of LLMs for adapting and teaching new behaviors via language feedback [56, 74, 54, 75, 31] - but in contrast to prior work, we focus on not only evaluating online adaptation via in-context learning (e.g., prompting LLMs), but also on how we can improve that adaptation via offline model fine-tuning.\n' +
      '\n' +
      '**In-Context Learning for Robot Adaptation.** In-context learning is a form of supervised meta-training [9], where multiple examples and instructions [51] from the same dataset are packed sequentially into a context buffer that is fed as input to an LLM with an unsupervised autoregressive completion objective [66]. The instructions and examples specify tasks (extending the concept of "task prefixes", i.e., predefined token sequences [53, 52]), where the model is expected to complete further instances of the task by predicting what comes next.\n' +
      '\n' +
      'In robotics, in-context learning (via prompting) has been used to elicit a wide-range of capabilities - responding to feedback [74, 31], modifying low-level behaviors [7, 55, 49, 4, 38], remembering and applying user preferences [68], and asking for help [54]. Most related to our work is Zha et al. [75], which investigates robot teaching by summarizing human feedback, and indexing it in memory to be used again as in-context examples for similar future interactions via retrieval (e.g., retrieval augmented generation (RAG) [40]). In contrast, we focus on directly fine-tuning the underlying LLM to improve in-context learning from human language inputs (which can be multi-round contextual). We find finetuning exceeds the performance of retrieval-based methods for teaching unseen tasks - without additional external modules.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '**Improving LLM Alignment to User Feedback.** Our work builds on an active area of research to _align_ LLMs with user intent [51]. A common approach is to supervised fine-tune (SFT) the model on expert human inputs and outputs, then use non-expert labeled rankings (preferences) from model outputs to train reward models for reinforcement learning from human feedback (RLHF) [13, 61, 7]. However, these works often focus on mapping from single user inputs to preferred outputs (e.g., single-turn dialogue). In this work, we also investigate learning from human feedback, but we focus on improving the teachability of LLMs that write and improve robot code based on multi-turn, interactive human feedback. Our LMPC approach uses SFT to model human-robot interaction dynamics, and uses inference-time search and receding horizon control to discover shorter paths (with fewer rounds of corrections) to task success.\n' +
      '\n' +
      '## III Language Model Predictive Control\n' +
      '\n' +
      'We investigate teachability in the context of language-based human-robot interactions, where users communicate with robots via text messages through a chat-like interface next to a simulated visualization of the robot and its surroundings using the MuJoCo simulation engine [65] (see Fig. 3, more details in Appendix VI-L). User messages are free-form and up to users\' discretion; they may include instructions, preferences, feedback, etc. In response to each message, the system outputs robot code, which is directly sent to a real-time motion controller on a simulated or real robot (Section III-B). Users then provide subsequent feedback based on the observed robot behavior.\n' +
      '\n' +
      'Each human-robot conversation (i.e., chat session) is goal-driven: users are asked to teach one task per session and at the end of each session label "success" or "failure" conditioned on whether they believe the robot to have completed the task. Chat sessions can consist of multiple chat turns (i.e., human-robot input-output pairs) before success. On average, successful sessions run for 2-3 chat turns, while failure sessions run for 5-6 chat turns (see Fig. 3; bar plot shown in bottom left). User messages can be corrections or broken-up step-by-step sub-tasks to piece together more complex ones, and they are usually multi-round contextual. During data collection, users rate individual robot responses as "good" or "bad\' -- good if the robot responded correctly to the most recent human feedback (although it may not be successful at completing the entire task yet), and bad otherwise. We find that the ratio of good chat turn ratings correlates with task success (Fig. 3, bottom right).\n' +
      '\n' +
      '### _Problem Statement_\n' +
      '\n' +
      'Our goal is to improve the teachability of LLMs that follow human instructions and feedback to write robot code. Teachability is defined as the average number of human inputs (chat turns) \\(n\\) before the robot succeeds at the task. This metric measures how efficiently the robot adapts to human inputs, and \\(n=1\\) is equivalent to a standard zero-shot instruction following setting [33, 44]. To _improve_ teachability is to reduce the number of chat turns \\(n\\) before a desired success rate, and can be viewed as a meta-learning objective - i.e., learning to learn faster from human feedback [26]. Intuitively, improving teachability of a model should encourage its responsiveness to feedback, as a means to maximize the likelihood of generating the correct behavior (according to the user). Teachability can also reflect how well a model adapts to preferences. For instance, user input "move a bit to the left" might yield different robot behavior modifications depending on the user - a strong meta-learner (with respect to teachability) is one that can learn this difference to minimize the number of interactions \\(n\\), conditioned on with whom it interacts.\n' +
      '\n' +
      'We formulate language-based human-robot interaction as a partially observable Markov decision process, in which the policy interacts with the human teacher (part of the environment) through code that is executed via motion control on the robot (the action), and the human gives natural language feedback (the observation) and indicates the success of the teaching session (the reward). The policy\'s goal is to produce code that leads the robot to behave as intended by the human, however, this target behavior has to be inferred from the human feedback. Mathematically, \\(s_{t}\\in\\mathcal{S}\\) is the unobserved human (user) state at time \\(t\\), human text inputs are observations \\(o_{t}\\in\\mathcal{O}\\) of the state, and the agent (LLM) generates code as actions \\(a_{t}\\in\\mathcal{A}\\) according to a policy \\(\\pi(o_{t}|o_{0},\\ldots,o_{t-1})\\) that is then executed on the robot. \\(\\mathcal{P}(o_{t+1},r_{t}|o_{<t},a_{t}\\leq t,r<t)\\) is the transition probability of human interactions, and if at any time the user determines the task is a "success", the episode terminates with a sparse reward \\(r=1\\), and the chat session terminates. If the robot struggles to improve for more than 7 timesteps (chat turns, or \\((o_{t},a_{t})\\) tuples), then the episode terminates as a "failure" (\\(r=0\\)). Improving teachability here can be defined as improving the ability of \\(\\pi\\) to infer the target behavior intended by the human, i.e. to discover shorter paths in the POMDP to task success (i.e., human-labeled success \\(r=1\\)).\n' +
      '\n' +
      'While the true transition dynamics are unknown (as they depend on the human teacher), we are interested in modeling the POMDP with language models, with which the human text inputs \\(o_{t}\\) and robot code as actions \\(a_{t}\\) can both be represented as a sequence of symbols \\((x_{0},\\ldots,x_{n})\\), modeled autoregressively with a decoder-only Transformer architecture [66]. Thus we learn \\(\\hat{\\mathcal{P}}(o_{t},a_{t},r_{0}\\ldots,r_{7})\\) by factorizing the probability of the full interaction sequence into the product of conditional probabilities \\(\\hat{\\mathcal{P}}(o_{t+1},r_{t},a_{t}|o_{<t},a_{<t},r_{<t})=\\)\n' +
      '\n' +
      'Fig. 3: Our chat interface (top left) allows non-experts to use language to teach robots new behaviors (visualized in simulation, top right). Our LLM responds with reward code, to drive real-time motion control of a simulated or real robot. Statistics show that base model data meets expectations: successful teaching sessions take fewer chat turns than failures (bottom left), and task success rates correlate with fewer chat turns (\\(r=-0.85\\), bottom middle) and higher good rating rates (i.e., responsiveness to feedback, \\(r=0.92\\), bottom right).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      'In this work, we apply supervised fine-tuning (SFT) on our LLM, using Language Model Predictive Control (LMPC). LMPC formulates the human-robot exchange as a POMDP (see Section III-A), with human text inputs as observations \\(o_{t}\\), and robot code outputs as actions \\(a_{t}\\). Each chat session terminates with a success or failure reward \\(r\\). We implement two variants of LMPC (depicted in Fig. 4): **(i) LMPC-Rollouts.** Given the current chat session (system prompt and current chat history), the LLM is trained to autoregressively predict the rest of the chat session (sequence of \\(o_{t}\\) and \\(a_{t}\\)\'s, until receiving a reward \\(r\\) at the end of the episode). For training, the input to the LLM is the system prompt with the initial user instruction; both are included because different robot embodiments have different system prompts (robot APIs), and this allows the LLM generation to support different robot APIs at inference time. The target is for the LLM to predict any remaining portions of a chat session, conditioned on the current portion. We only train LMPC-Rollouts on successful trajectories (training on both successes and failures yielded much worse performance. See Appendix VI-B). Training Transformers with causal attention on entire chat sessions (time-ordered sequences of observations and actions, one session per context) directly amounts to training a sequence-conditioned transition dynamics model of the POMDP. This dynamics model is used for search during inference.\n' +
      '\n' +
      'A key aspect of LMPC is that at inference time, the fine-tuned LLM is used as a transition model together with model predictive control (MPC) to discover optimal paths to success. MPC can be thought of as a sequence-level decoding strategy [19], but differs from standard ones used in modern language models as it generates multiple episodic rollouts to search for the next best action, and repeats the process at every decision-making step. To do so, we sample from the LLM \\(8\\) rollouts with non-zero temperature sampling (next-token decoding) for a max token length of \\(4096\\). If a sampled trajectory reaches termination within the max token length, we treat it as successful, since LMPC-Rollouts is only trained on successful data. From these terminated samples, we choose the trajectory with the fewest predicted timesteps (i.e., chat turns) and return its first action, as shown in Fig 4 (center). If no sample terminates, then we randomly pick a trajectory and return its \\(a_{t+1}\\). This process is then repeated given new human input for every chat turn. Intuitively, LMPC-Rollouts can be thought of as training the LLM via human-robot interaction as a form of chain-of-thought [67] during both training and inference -- rather than cloning successful code, LMPC learns the process of getting to the correct code, and accelerating it via search at inference time. **(ii) LMPC-Skip** is a variant of LMPC-Rollouts with the same training inputs, but different targets. LMPC-Skip is trained to predict only the last action, skipping predictions of the interim trajectory (see Fig 4, right). This encourages the finetuned model to predict the final correct code as soon as possible e.g., optimizing for 1-turn success. However, because LMPC-Skip is not trained on nor does it model intermediate interactions with the user, it may be less responsive to corrective feedback. During inference, we condition LMPC-Skip\'s generation on the system prompt with the chat session so far, and only query the model once to generate a response. Like LMPC-Rollouts, LMPC-Skip is only trained on successful chat sessions.\n' +
      '\n' +
      '**Top-User Conditioning.** To further improve LLM teachability with fine-tuning, we propose conditioning LLM generations, during both training and inference, on the user. For training, we modify the input prompt to include which user generated the following chat session using a unique ID label. Top-users are autonomously identified from the training dataset and are given a special ID "top-user." During inference, we always condition LLM generations on the "top-user" label. We identify top-users as the top \\(25\\%\\) of users by their user performance score. This score is the average of a user\'s task success rate weighted by task difficulty, which is the task\'s failure rate across all users. See Appendix VI-C for more details.\n' +
      '\n' +
      'Top-user conditioning, in the context of LMPC, can be interpreted as conditioning the LLM to generate the distribution of observations \\(o_{t}\\) (expected human inputs) and actions \\(a_{t}\\) (expected code outputs) closest to the top \\(75\\)th percentile of users. Intuitively, if observations are viewed as a partial noisy representation of the true (user) state (or intent, during teaching), then different user proficiency levels can correspond to a varying amount of noise (i.e., higher proficiency is less noise), to which conditioning on top-users prompts the LLM to generate rollouts with less noise. Top-user conditioning draws similarity to performance conditioning with Decision Transformers [12], albeit (in the absence of dense rewards) using inference-time search via MPC. Note that top-user conditioning can broadly index distributions that represent a wide range of user-related attributes (e.g., preferences, user-specific styles, etc.), expanding beyond the scope of what performance conditioning on rewards alone can provide.\n' +
      '\n' +
      '## IV Experiments\n' +
      '\n' +
      'Our experiments evaluate how much the various proposed finetuning strategies (slow adaptation) improve online in-context learning (fast adaptation) for humans interactively teaching robots via natural language feedback. Evaluations are performed on \\(78\\) robot tasks, across \\(5\\) robot embodiments in simulation and \\(2\\) on real hardware. We specifically explore the following questions:\n' +
      '\n' +
      '* How much does fine-tuning improve teachability, especially on test tasks?\n' +
      '* How do LMPC-Rollouts and LMPC-Skip compare?\n' +
      '* What are the benefits of Top-User Conditioning?\n' +
      '* Does finetuning enable cross-embodiment generalization?\n' +
      '* Can iterative finetuning further improve teachability?\n' +
      '\n' +
      'All data collection and most evaluations were performed in simulations. All models were trained on data obtained with simulation. We separately evaluate finetuned models on real robots, but we have not experimented with training on data from teaching real robots.\n' +
      '\n' +
      'Fig. 4: Given a dataset of users teaching robots new tasks with language (represented as text inputs and code outputs from online in-context learning – left), LMPC-Rollouts is trained to predict subsequent inputs and outputs conditioned on the current chat history (middle), and uses MPC (receding horizon control) for inference-time search to return the next best action (with fewest expected corrections before success). LMPC-Skip is an alternate variant that is trained to directly predict the last action (right). Both LMPC variants accelerate fast robot adaptation via in-context learning.\n' +
      '\n' +
      '### _Data Collection and Evaluation_\n' +
      '\n' +
      'To collect human teaching data and evaluate teaching performance, we worked with \\(35\\) non-expert users, who were able to collect \\(350\\) chat sessions per day. These users are non-experts: they are not researchers or engineers, and they are not familiar with the underlying LLMs or robot code. We instruct users to give natural language feedback on the behavior of the robot for each chat turn, instead of giving technical feedback or giving feedback on the code written by the LLM. Data collection protocol details are in Appendix VI-A. When a user interacts with a new chat session, a random robot embodiment and task is sampled, and the user is asked to teach the robot that task. Data collection is separated into two phases: 1) initial data collection with the base model and 2) subsequent data collection (evaluations) with finetuned models. In phase \\(2\\), we randomly sample which model the user interacts with, and the user does not know which model they are currently engaging with. This allows for blind A/B evaluations to minimize biases. For a given model, the data collected can be used for both downstream finetuning and evaluating the model.\n' +
      '\n' +
      'Out of the \\(78\\) tasks, \\(51\\) are train tasks (\\(65\\%\\)), while \\(27\\) are test (\\(35\\%\\)). While separating tasks into train and test splits allows us to measure model generalization performance, it also means there are less data available for training. To address this and also to make the data distribution robust to user teaching noise, data collection and evaluation of models are typically aggregated across 2 days. Additional data filtering were performed to remove invalid and incorrect data (\\(<4\\%\\)). In total, \\(299\\) successful chat sessions from the initial data collection were made available for fine-tuning. Across chat sessions, the max total token length is \\(3900\\), with \\(1800\\) as the median. Given the limited amount of data, and to make LLM responses more robust to small differences in user feedback, we perform data augmentation on the collected data. This is done by generating \\(5\\) variations of user instructions (as well as intermediate feedbacks for training LMPC-Rollouts) using PaLM \\(2\\)-L. We do not generate variations of the robot code. Combining the augmented and the original data, the training set contains about \\(3\\)M tokens.\n' +
      '\n' +
      'For evaluations, we collect approximately \\(350\\) chat sessions for all model variants we evaluate, split across all platforms and tasks. We observe minimal user performance drift over time (see Appendix VI-I), so differences in model performance are likely due to changes in model capabilities, and not in users\' teaching proficiency.\n' +
      '\n' +
      '### _Robot Embodiments and Tasks_\n' +
      '\n' +
      'In this section we give a brief overview of the \\(5\\) robot embodiments in our experiments. We chose these embodiments to explore teaching a diverse set of robot capabilities, from tasks that require a single arm, to bi-manual tasks, and to dexterous and locomotion tasks. See Fig. 1 for illustrations. We include the full list of tasks each embodiment in the Appendix VI-M.\n' +
      '\n' +
      '**1. Robot Dog.** The first embodiment is a small custom quadruped robot [10] with comparable dimension and weight to both Unitree A1 and MIT Mini-Cheetah quadrupeds. Robot Dog has a total of 12 actuated degrees-of-freedom (DoF), 3 on each leg. Our tasks for the Robot Dog range from stationary posing tasks, like sitting and high-five, to more dynamic tasks, like trotting and door opening. We perform Robot Dog experiments in both simulation and the real world.\n' +
      '\n' +
      '**2. Mobile Manipulator** In this embodiment, we use a mobile manipulator [24] with a 7 DoF arm and parallel jaw grippers. We explore tabletop manipulation tasks with rigid objects, such as flipping and stacking objects. The Mobile Manipulator is also available both in simulation and the real world.\n' +
      '\n' +
      '**3. Aloha.** This is a bi-manual embodiment with two 6 DoF arms, each attached with a parallel jaw gripper [76]. The two arms sit directly opposite of each other on a table that has a set of rigid household objects. We explore tasks that require coordination with both arms, such as object transfers.\n' +
      '\n' +
      '**4. Bi-arm Kuka.** Bi-arm Kuka consists of two 7 DoF Kuka LBR IIWA14 arms without end-effectors. The omission of end-effectors allows us to explore whole-body manipulation tasks (e.g. manipulating objects with any part of the robot arm) with this embodiment. We populate the workspace with boxes of different sizes and colors, and the robot needs to manipulate individual or sets of objects to desired goal locations (which may be on the workspace surface or in the air) and in a given order.\n' +
      '\n' +
      '**5. Kuka+Hand.** This embodiment comprises a 7 DoF Kuka arm attached with a custom three-fingered hand. The full system is controlled via torque control. Along with the arm, a set of rigid objects is provided in the workspace. With Kuka+Hand, we explore dexterous manipulation tasks that are difficult to perform with the other manipulation embodiments, such as lifting multiple objects in-hand and plug insertion.\n' +
      '\n' +
      '### _Compared Methods_\n' +
      '\n' +
      'We compare performances across the base model (PaLM 2-S), the two finetuned variants LMPC-Rollouts and LMPC-Skip, and a Retrieval-Augmented Generation (RAG) [40] baseline. Comparing LMPC-Rollouts and LMPC-Skip captures the difference between finetuning the LLM to leverage and predict the entire human-robot chat interaction, versus skipping to predicting the final robot-code response. Comparing to RAG captures if the LLM\'s improvement in our domain is possible if we do not have access to model weights or the resources needed for finetuning. For RAG, we use a pretrained embedding model to retrieve relevant examples from the training data then inserting them into the LLM context, similar to other RAG applications for adapting robot behavior [75]. See implementation details in the Appendix VI-D.\n' +
      '\n' +
      '### _Experiment Results_\n' +
      '\n' +
      'We evaluate the LLM\'s _reachability_ as task-success for \\(<\\!N\\) user interactions or "chat turns". This is visualized in a curve in Fig. 5, where each point indicates the proportion of chat sessions that achieved success (y-axis) with equal to or less than a certain number of chat turns (x-axis). Models that have better teachability would have a curve that is higher and to the left.\n' +
      '\n' +
      'Fig. 5 reports the main teachability results aggregated over all embodiments for the base PaLM 2-S model, the finetuned models LMPC-Rollouts and LMPC-Skip, and the base model with RAG. Through finetuning, models are able to exceed teachability performance of the base model. On train tasks, LMPC-Skip performs the best. On test tasks, LMPC-Rollouts perform the best, improving success rate over the base model by \\(27\\%\\). Both modelsalso reach high success rates faster than the base model - matching or exceeding the final success rate of the base model after just one chat turn. While LMPC-Skip achieves the higher 1-turn success rate than LMPC-Rollouts on test tasks, the order flips starting at 2 chat turns. This suggests that LMPC-Rollouts is more amenable to improvements from user feedback. RAG performs competitively over the base model, but it trails behind the finetuned methods in both train and test tasks. See these results separated by embodiment in Table VIII in Appendix VI-B.\n' +
      '\n' +
      'Table I provides additional quantitative comparisons across all models evaluated, including:\n' +
      '\n' +
      '* _Success Rate_: overall success rate on all tasks and embodiments\n' +
      '* _Num Chat Turns_: mean number of chat turns for successful chat sessions\n' +
      '* _Good Rating Rate_: proportion of positively rated chat turns after the first chat turn (captures responsiveness to corrective feedback)\n' +
      '* _Successful Tasks Rate_: the proportion of tasks with at least one successful chat session\n' +
      '* _1 turn Success Rate_: proportion of chat sessions that were successful with just one chat turn (1st instruction)\n' +
      '* _2+ turn Success Rate_: proportion of chat sessions that were successful with \\(>1\\) chat turns. This is the difference between the overall success rate and 1 turn Success Rate\n' +
      '\n' +
      'For both train and test tasks, LMPC-Skip achieves the lowest Num Chat Turns for successful chat sessions, as well as the highest 1-turn Success Rate. These reflect how LMPC-Skip is trained to predict the final code as fast as possible. However, LMPC-Rollouts has the highest 2+ turn Success Rate, suggesting it is most amenable to corrective feedback given an incorrect first response. To maximize performance in practice, these results suggest that one should use LMPC-Skip for responding to the initial user instruction, then LMPC-Rollouts for responding to subsequent user feedback. For RAG, while the method improves upon the base model on overall success rate, it achieves lower Successful Task Rate than the base model on test tasks. This suggests that while RAG may be proficient at increasing the success rate of tasks similar to the retrieved examples, it struggles to perform well on novel tasks.\n' +
      '\n' +
      '**Effects of Top-User Conditioning.** In Table II, we show the change in task success when training without top-user conditioning on 1) data from all users and 2) data from only top users. These ablations were only performed on the Robot Dog and Mobile Manipulator embodiments due to time constraints. From the initial data collected on the base model, \\(10\\) out of \\(35\\) users were identified as top-users, and they only covered \\(11\\) out of \\(50\\) train tasks. However, despite this small coverage, top-user conditioning significantly outperforms both variants of no top-user conditioning, across model types (LMPC-Rollouts and LMPC-Skip) and task types (train and test). This suggests that with top-user conditioning, models can learn to transfer the style of responses induced by top-users teaching to novel tasks. It also highlights the importance of training the LLM to mimic generations from a high-quality data distribution as well as across a diverse data distribution. See Appendix VI-C for analysis on the teaching styles of top-users.\n' +
      '\n' +
      '**Cross-Embodiment Generalization.** Beyond evaluating generalization towards test tasks, we also evaluate whether training on a subset of embodiments would lead to improved performance on new embodiments that the finetuned models were not trained on. To the LLM, the difference in embodiment is captured through the prompt, which contains different robot descriptions and APIs for each embodiment. We performed an experiment where we train the LMPC models on data from Robot Dog, Mobile Manipulator,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline Data & Model & Train Tasks & Test Tasks \\\\ \\hline All Users & LMPC-Rollouts & -8.4\\% & -10.5\\% \\\\  & LMPC-Skip & -16.3\\% & -26.1\\% \\\\ \\hline Only Top Users & LMPC-Rollouts & -23.8\\% & -21.7\\% \\\\  & LMPC-Skip & -9.6\\% & -13.6\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE II: Changes in success rate without Top-User Conditioning. We evaluate two variants of LMPC-Rollouts and LMPC-Skip that do not apply top-user conditioning, training on data from all users and training on data from only top users. Success rates degrade significantly for both variants, suggesting that 1) focusing LLM generation on the style of top-users is important and 2) top-user data alone is insufficient, and training on the wider data distribution of all users is still important.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c} \\hline \\hline Tasks & Model & Success Rate & Num Chat Turns & Good Rating Rate & Successful Tasks Rate & 1 Turn Success Rate & 2+ Turn Success Rate \\\\ \\hline Train & PaLM 2-S & 34.8\\% & 2.3 & 16.7\\% & 74.0\\% & 13.0\\% & 21.7\\% \\\\  & RAG & 46.4\\% & 2.2 & 21.4\\% & **83.3\\%** & 25.1\\% & 21.2\\% \\\\  & LMPC-Skip & **56.0\\%** & **1.7** & **25.6\\%** & **83.3\\%** & **34.6\\%** & 21.4\\% \\\\  & LMPC-Rollouts & 51.9\\% & 2.2 & 21.8\\% & 74.0\\% & 23.5\\% & **28.4\\%** \\\\ \\hline Test & PaLM 2-S & 39.4\\% & 2.4 & 18.1\\% & 81.5\\% & 17.5\\% & 21.9\\% \\\\  & RAG & 51.9\\% & 2.0 & 20.9\\% & 75.0\\% & 27.9\\% & 24.0\\% \\\\  & LMPC-Skip & 59.4\\% & **1.6** & 24.7\\% & **88.9\\%** & **41.7\\%** & 17.8\\% \\\\  & LMPC-Rollouts & **66.3\\%** & 1.9 & **26.5\\%** & **88.9\\%** & 34.8\\% & **31.5\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE I: Comparing base and finetuned models across all embodiments. _Success_: overall success rate on all tasks. _Num Chat Turns_: mean number of chat turns for successful chat sessions. _Good Rating_: proportion of positively rated chat turns after the turn. _Successful Tasks_: proportion of tasks with at least one successful chat session. _1 turn Success_: the proportion of chat sessions that were successful with just one chat turn. _2+ turn Success_: the proportion of chat sessions that were successful with two chat turns.\n' +
      '\n' +
      'Fig. 5: Our fine-tuned LLMs with LMPC-Rollouts and LMPC-Skip improve the teachability of the base model (PaLM 2-S), and outperforms a RAG [40] baseline across all embodiments. LMPC-Skip overfits to train tasks (left), while LMPC-Rollouts generalizes better (i.e., more teachable and responsive to feedback) on unseen test tasks (right) for multi-turn sessions (with more than one chat turn).\n' +
      '\n' +
      'and Aloha, omitting Bi-arm Kuka and Kuka+Hand. See results in Table IV, where we report success rate differences between the finetuned models and the base model. We see improvements in test embodiments of \\(18.6\\%\\) for LMPC-Skip and \\(31.5\\%\\) for LMPC-Rollouts, suggesting that finetuned models generalize not only to test tasks, but test robot embodiments as well. This generalization is non-trivial as the embodiments have very different APIs from each other, and the test embodiments require writing robot reward code that can induce complex dexterous manipulation behaviors.\n' +
      '\n' +
      '**Real-world Evaluations.** We evaluate our approach on a subset of tasks for the Mobile Manipulator and the Robot Dog in the real world (Fig. 6). For each task, we ask users to perform four teaching sessions on the real-robot directly. See results that compare PaLM 2-S and LMPC-Rollouts in Table III. LMPC-Rollouts achieves higher success rate than PaLM 2-S across all tasks. While Num Chat Turns for successful sessions is about the same for PaLM 2-S and LMPC-Rollouts on these tasks, LMPC-Rollouts achieves much higher success rates. See more detailed comparisons between sim and real executions in the Appendix VI-G.\n' +
      '\n' +
      '**Multiple Fine-tune Iterations.** Given that the finetuned models exhibit improved reachability performance over the base model, additional, iterative training with data collected with the finetuned models could potentially further improve performance. We tested this hypothesis by training Iteration 2 LMPC models with data collected by the Iteration 1 models. Results are shown in Table V. Currently, we do not observe further improvements from the second iteration of finetuning. This implies that the data distribution or data amount used to train the second iteration of models do not differ significantly from that of the first iteration, so the resultant model behaviors remain largely unchanged. While recent works have demonstrated iterative self-improving finetuning for LLMs [72, 25, 22], enabling LLM iterative improvement with human feedback and grounded on robot code executions remain promising but under-explored, and we defer this topic to future research.\n' +
      '\n' +
      '## V Discussions\n' +
      '\n' +
      'We introduce a method that improves the teachability of LLMs by 1) formulating human-robot interaction as a POMDP and 2) performing Language Model Predictive Control with LLMs finetuned to predict the dynamics of human-robot interactions. LMPC can learn to learn faster from human feedback, and we observe performance gains on test tasks and test robot embodiments. Despite the promising results, there are several limitations to our work that can point to potential future research. Some limitations stem from resource availability. We assume access to sufficient computational resources both for MJPC (e.g. 128 CPU cores) and for LLM finetuning. More efficient MPC and finetuning techniques (e.g. LoRA [28]) would help.\n' +
      '\n' +
      'Other limitations relate to the foundation model. We assume that the base LLM can generate some positive chat sessions for bootstrapping the learning process. We also only use language models; future work on expanding feedback modality (e.g. to video/audio inputs) by using multimodal foundation models can expand the richness of feedback as well as improve finetuned models\' ability to predict human reactions to robot behavior. Lastly, our approach currently observes no benefit from learning across multiple in-context learning and fine-tuning cycles. Adapting the data distribution, through methods like active task exploration or synthetic data generation, may unlock additional performance gains.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Success Rate Diff from Iter 1} \\\\ \\cline{2-3} Model & Train Tasks & Test Tasks \\\\ \\hline LMPC-Skip Iter 2 & +5.1\\% & -4.7\\% \\\\ LMPC-Rollouts Iter 2 & -5.5\\% & -1.9\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE V: Further finetuning on data generated from both the base model and the first finetuned models models does not yield performance improvements.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline  & & \\multicolumn{2}{c}{PaLM 2-S} & \\multicolumn{2}{c}{LMPC-Rollouts} \\\\ \\cline{3-6} Embodiment & Task & Success & Num Chat Turns & Success & Num Chat Turns \\\\ \\hline Robot Dog & “downward dog” & \\(100\\%\\) & \\(1.3\\) & \\(100\\%\\) & \\(2.8\\) \\\\  & “hop” & \\(25\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(2.3\\) \\\\  & “high-five with left hand” & \\(75\\%\\) & \\(2.3\\) & \\(75\\%\\) & \\(3.0\\) \\\\  & “walk forward in a trotting gait” & \\(25\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(2.8\\) \\\\  & “hop while turning counterclockwise” & \\(25\\%\\) & \\(5.0\\) & \\(25\\%\\) & \\(4.0\\) \\\\ \\hline Mobile Manipulator & “knock over coke can” & \\(20\\%\\) & \\(5.0\\) & \\(20\\%\\) & \\(3.0\\) \\\\  & “open top drawer half-way” & \\(100\\%\\) & \\(3.4\\) & \\(100\\%\\) & \\(3.2\\) \\\\  & “push coke can from right to left” & \\(60\\%\\) & \\(2.0\\) & \\(80\\%\\) & \\(2.0\\) \\\\ \\hline \\hline \\multicolumn{6}{c}{Average} & \\(53.8\\%\\) & \\(\\mathbf{2.9}\\) & \\(\\mathbf{75\\%}\\) & \\(\\mathbf{2.9}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE III: LMPC-Rollouts has higher success than PaLM 2-S on real robots. Test tasks are started\\({}^{*}\\). Robot Dog tasks are performed 4 times, Mobile Manipulator tasks \\(5\\) times.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Tain Embodiments} & Test Embodiments \\\\ \\cline{2-3} Model & Train Tasks & Test Tasks \\\\ \\hline LMPC-Skip & \\(+28.8\\%\\) & \\(+19.0\\%\\) & \\(+18.6\\%\\) \\\\ LMPC-Rollouts & \\(+17.2\\%\\) & \\(+23.8\\%\\) & \\(+31.5\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IV: Finetuned models can generalize to new robot embodiments and APIs not seen during training. Higher improvements in test tasks and embodiments are caused by the finetural split not being explicitly selected for uniform task difficulty and baseline performance; doing so is infeasible as the split needs to be chosen before starting evaluations, when task difficulty and baseline performance were unknown.\n' +
      '\n' +
      'Fig. 6: Tasks evaluated in the real-world Mobile Manipulator and Robot Dog.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]M. Ahn, A. Bronhan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. (2022) Do as i can, not as i say: grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Cited by: SS1.\n' +
      '* [2]A. Akakzian, C. Coldis, P. Cousley, T. Oudeyer, M. Chetouani, and O. Sigaud (2020) Grounding language to autonomously-acquired skills via goal generation. arXiv:2006.07185. Cited by: SS1.\n' +
      '* [3]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepkin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. (2023) Palm 2 technical report. arXiv preprint arXiv:2305.10403. Cited by: SS1.\n' +
      '* [4]M. Gonzalez Arenas, T. Xiao, S. Singh, V. Jain, A. R. Ren, Q. Vuong, J. Varkey, A. Herzog, I. Leal, S. Kirmani, et al. (2023) How to prompt your robot: a prombook for manipulation skills with code as policies. In Towards Generalist Robots: Learning Paradigms for Scalable Still Acquisition@ CoRL2023, Cited by: SS1.\n' +
      '* [5]Y. Artig and L. Zettlemoyer (2013) Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL)1, pp. 49-62. Cited by: SS1.\n' +
      '* [6]F. Gregory Ashby and J. T. Townsend (1986) Varieties of perceptual independence. Psychological review93 (2), pp. 1541. Cited by: SS1.\n' +
      '* [7]Y. Bai, S. Kadayan, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. (2022) Constitutional ai: harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Cited by: SS1.\n' +
      '* [8]A. Brown, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, J. B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi, K. Julian, D. Yakashnikov, Y. Kung, I. Leal, K. Hee, S. Levine, Y. Lu, U. Malla, D. Manjuntan, I. Mordatch, O. Nachum, C. Parada, J. Perla, E. Perez, K. Pertsch, J. Quianno, K. Rao, M. Ryoo, G. Salezar, P. Sankei, K. Sayed, J. Singh, S. Sontaktake, A. Stone, C. Tan, H. Tran, V. Vanhoucke, S. Vega, Q. V. Voong, F. Xi, F. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich (2023) RI-16 robotics transformer for real-world control of at scale. External Links: 2306.10880 Cited by: SS1.\n' +
      '* [9]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neekalatan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.\n' +
      '* [10]K. Caluwaerts, A. Iscen, J. Kew, W. Yu, T. Zhang, D. Freeman, K. Lee, L. Lee, S. Saliceti, V. Zhuang, et al. (2021) BARKour: benchmarking animal-level agility with quadruped robots. arXiv preprint arXiv:2305.14654. Cited by: SS1.\n' +
      '* [11]S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill (2022) Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems35, pp. 18878-18891. Cited by: SS1.\n' +
      '* [12]L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch (2021) Decision transformer: reinforcement learning via sequence modeling. Advances in neural information processing systems34, pp. 15084-15097. Cited by: SS1.\n' +
      '* [13]P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei (2020) Deep reinforcement learning from human preferences. External Links: 2006.07185 Cited by: SS1.\n' +
      '* [14]G. Cideron, M. Seurin, F. Strub, and O. Pietquin (2019) Self-educated language agent with hindsight experience replay for instruction following. DeepMind. Cited by: SS1.\n' +
      '* [15]J. D. C.-Reyes, A. Gupta, S. Sanjeev, N. Altieri, J. DeNero, P. Abbeel, and S. Levine (2018) Guiding policies with language via meta-learning. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* online language corrections for robotic manipulation via shared autonomy. In Proceedings of the 2023 ACM/IEEE Conference on Human-Robot Interaction (HRI), Cited by: SS1.\n' +
      '* [17]Y. Ding, X. Zhang, C. Paxton, and S. Zhang (2023) Task and motion planning with large language models for object rearmagent. arXiv preprint arXiv:2303.06247. Cited by: SS1.\n' +
      '* [18]D. Driessi, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhory, B. Litter, A. Wahid, J. Tompson, Q. Vuong, T. Tu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence (2023) Palm-e: an embodied multimodal language model. External Links: 2306.10880 Cited by: SS1.\n' +
      '* [19]M. Freitag and Y. Al-Onizan (2017) Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806. Cited by: SS1.\n' +
      '* [20]E. Galy, J. Paxion, and C. Berthelon (2018) Measuring mental workload with the nasa-tlx needs to examine each dimension rather than relying on the global score: an example with driving. Ergonomics61 (4), pp. 517-527. Cited by: SS1.\n' +
      '* [21]P. Goyal, S. Niekum, and R. J. Mooney (2020) PSLP: guiding reinforcement learning using natural language by mapping pixels to rewards. arXiv:2007.15543. Cited by: SS1.\n' +
      '* [22]C. Gulcehre, T. De Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. (2020) Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.089896. Cited by: SS1.\n' +
      '* [23]S. G. Hart, N. load index (nasa-tlx) 20 years later. Proceedings of the 50th HFES Conference, pp. 904-908. Cited by: SS1.\n' +
      '* [24]A. Herzog, K. Rao, K. Hausman, Y. Lu, P. N. Mehringenau, Y. Yan, J. Lin, M. Gonzalez Arenas, T. Xiao, D. Kappler, D. Ho, J. Rettinghouse, Y. Chebotar, K. Lee, K. Gopalakrishnan, R. Julian, A. Li, C. Kelly Fu, B. Wei, S. Ramesh, K. Holden, K. Kleiver, D. Rendleman, S. Kirmani, J. Bingham, J. Weisz, Y. Xu, Wentling, L. H. Benincke, C. Fong, D. D. J. Lesian, Y. Bai, B. Holson, M. Quinlan, N. Brown, M. Kalakrishnan, J. Ibarz, P. Pastor, and S. Levine (2023) Deep rl at scale: sorting waste in office buildings with a fleet of mobile manipulators. External Links: 2306.108651 Cited by: SS1.\n' +
      '* [25]O. Honovich, T. Scialom, O. Levy, and T. Schick (2022) Unnatural instructions: tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.096989. Cited by: SS1.\n' +
      '* [26]T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: a survey. IEEE transactions on pattern analysis and machine intelligence44 (9), pp. 5149-5169. Cited by: SS1.\n' +
      '* [27]T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: a survey. IEEE transactions on pattern analysis and machine intelligence44 (9), pp. 5149-5169. Cited by: SS1.\n' +
      '* [28]T. Hospedales, A. Attoniou, P. Micaelli, and A. Storkey (2021) Meta-learning in neural networks: a survey. IEEE transactions on pattern analysis and machine intelligence44 (9), pp. 5149-5169. Cited by: SS1.\n' +
      '* [29]F. Hu and D. Sadigh (2021) Language instructed reinforcement learning for human-a coordination. In 40th International Conference on Machine Learning (ICML), Cited by: SS1.\n' +
      '* [30]W. Huang, P. Abbeel, D. Pathak, and I. Mordatch (2022) Language models as zero-shot planners: extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. Cited by: SS1.\n' +
      '* [31]W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zong, J. Tompson, I. Mordatch, Y. Chebotar, et al. (2022) Inner monologue: embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608. Cited by: SS1.\n' +
      '* [32]D. Huples, V. Dankers, M. Mul, and E. Bruni (2020) Compositionality decomposed: how do neural networks generalise?. Journal of Artificial Intelligence Research67, pp. 7757-795. Cited by: SS1.\n' +
      '* [33]E. Jang, A. Irpan, M. Khasari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn (2022) Eco-z: zero-shot task generalization with robotic imitation learning. External Links: 2202.02566 Cited by: SS1.\n' +
      '* [34]Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn (2019) Language as an abstraction for hierarchical deep reinforcement learning. NeurIPS. Cited by: SS1.\n' +
      '* [35]S. Karamcheti, E. C. Williams, D. Arumugam, M. Rhee, N. Gopalan, L. L. S. Wong, and S. Tellex (2017) A tile of two drags: a hybrid approach for interpreting action-oriented and goal-oriented instructions. In First Workshop on Language Grounding for Robotics @ ACL, Cited by: SS1.\n' +
      '* [36]A. Kirillov, E. Mittun, N. Ravi, H. Mao, C. Holland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Lo, et al. (2022) Segment anything. arXiv preprint arXiv:2304.02643. Cited by: SS1.\n' +
      '* [37]T. Kollar, S. Tellex, D. Roy, and N. Roy (2010) Toward understanding natural language directions. In Human-Robot Interaction, pp. 259-266. Cited by: SS1.\n' +
      '* [38]M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh (2023) Reward design with language models. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* [39]M. Kwon, H. Hu, V. Myers, S. Karamcheti, A. Dragan, and D. SadighKarpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474, 2020.\n' +
      '* [14] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9493-9500. IEEE, 2023.\n' +
      '* [15] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. L1M-4P: Empowering language models with optimal planning proficiency. _arXiv preprint arXiv:2304.11477_, 2023.\n' +
      '* [16] Jelena Luketina, Nantas Nardelli, Gregory Farughar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktaschel. A survey of reinforcement learning informed by natural language, 2019.\n' +
      '* [17] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data, 2021.\n' +
      '* [18] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. _arXiv preprint arXiv:2310.12931_, 2023.\n' +
      '* [19] C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox. Learning to parse natural language commands to a robot control system. In _International Symposium on Experimental Robotics (ISER)_, 2012.\n' +
      '* [20] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. _IEEE Robotics and Automation Letters_, 7(3):7327-7334, 2022.\n' +
      '* [21] Suvir Mirchandani, Siddharth Karamchedi, and Dora Sadigh. Ella: Exploration through learned language abstraction, October 2021.\n' +
      '* [22] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. _arXiv preprint arXiv:2307.04712_, 2023.\n' +
      '* [23] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. _arXiv:1704.08795_, 2017.\n' +
      '* [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhimi Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018. URL. [https://d4mucfpksyw.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksyw.cloudfront.net/better-language-models/language-models.pdf).\n' +
      '* [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* [27] Allen Z. Ren, Anushi Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Vareley, Zhenjia Xu, Dresa Sadigh, Andy Zeng, and Anithuda Majumdar. Robots that ask for help: Uncertainty alignment for large language model planners, 2023.\n' +
      '* [28] Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Cherfeng Xu, Ping Luo, Shengbo Ehen Li, Masayoshi Tomiaka, Wei Zhan, and Mingyu Ding. Language: Large language models as decision makers for autonomous driving, 2023.\n' +
      '* [29] Pratyusha Sharma, Bakakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, and Dieter Fox. Correcting robot plans with natural language feedback. _arXiv preprint arXiv:2204.05186_, 2022.\n' +
      '* [30] Roger N Shepard and Jih-Jie Chang. Stimulus generalization in the learning of classifications. _Journal of Experimental Psychology_, 65(1):94, 1963.\n' +
      '* [31] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In _CoRL_, 2021.\n' +
      '* [32] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Proformpot: Generating situated robot task plans using large language models. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11523-11530. IEEE, 2023.\n' +
      '* [33] Simon Stepputtis, Joseph Campbell, Mariano Phieipp, Stefan Lee, Chitta Baral, and Heni Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. _NeurIPS_, 2020.\n' +
      '* [34] Nisan Sietmon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alex Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.\n' +
      '* [35] Alon Talmor, Ori Yoran, Ronan L Eras, Chandra Bhagavatula, Yow Goldberg, Yejin Choi, and Jonathan Berant. CommonsenseQA 2.0: Exposing the limits of AI through gamification. _arXiv preprint arXiv:2201.05320_, 2022.\n' +
      '* [36] Stefanie Tellek, Thomas Kollar, Steven Dickerson, Matthew Walter, Ashis Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In _AAAI_, 2011.\n' +
      '* [37] Stefanie Tellek, Nakul Gopalan, Hradas Kress-Gazit, and Cynthia Matuszek. Robots that use language. _Annual Review of Control, Robotics, and Autonomous Systems_, 3(1):25-55, 2020. doi: 10.1146/annurev-control-101119-071628. URL: [https://doi.org/10.1146/annurev-control-101119-071628](https://doi.org/10.1146/annurev-control-101119-071628).\n' +
      '* [38] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoc: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386100.\n' +
      '* [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Demy Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* [41] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large language models. _arXiv preprint arXiv:2305.05568_, 2023.\n' +
      '* [42] Yaqi Xie, Chen Yu, Tongyao Zhu, Jinbin Bai, Ze Gong, and Harold Soh. Translating natural language to planning goals with large-language models. _arXiv preprint arXiv:2302.05128_, 2023.\n' +
      '* [43] Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew R. Walter. State: maintaining language models for embodied reasoning, 2023.\n' +
      '* [44] Wenhao Yu, Nimorod Gileadi, Chuyuan Fu, Sean Kirmani, Kunja-Huei Lee, Montse Gonzalez Arenas, Hao-Ten Lewis Chiang, Tom Ercez, Leonard Hasenclever, Jan Humphik, et al. Language to rewards for robotic skill synthesis, _arXiv preprint arXiv:2306.08647_, 2023.\n' +
      '* [45] Weizhe Yuan, Richard Yuanz Pangz, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.\n' +
      '* [46] Eric Zklinkman, Qian Huang, Gabriel Poesia, Noah D Goodman, and Nick Haber. Parsel: A (de-) compositional framework for algorithmic reasoning with language models. _arXiv preprint arXiv:2212.10561_, 2023.\n' +
      '* [47] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveeke Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_, 2022.\n' +
      '* [48] Lihan Zhang, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat G. Arenas, Andy Zeng, Fei Xia, and Dorsa Sadigh. Distilling and retrieving generalizable knowledge for robot manipulation via language corrections. In _2024 IEEE International Conference on Robotics and Automation (ICRA)_, 2024. URL: [https://arxiv.org/abs/2311.10678](https://arxiv.org/abs/2311.10678).\n' +
      '* [49] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. _arXiv preprint arXiv:2304.13705_, 2023.\n' +
      '\n' +
      '## Authorship and Acknowledgments\n' +
      '\n' +
      'We thank John Guilyard for his expert animations, and Giles Ruscoe for beautiful renderings. We thank Steven Bohez, Yuval Tassa, Tom Erez, Murilo Martins, Rugile Pevceviciute, David Rendleman, and Connor Schenck for their dedication to ensuring we had strong simulated environments. We thank Travis Armstrong, Noah Brown, Spencer Goodrich, Craig Hickman, Atil Iscen, Jerad Kirkland, Jason Powell, Stefano Saliceti, Ron Sloat, Sergey Yaroshenko, Eddie Yu, Grace Vesom, and Jake Varley for additional robot platform support and robot lab operations. Special thanks to Michael Ahn, Kendra Byrne, Aleksandra Faust, Rene Wagner, Yuheng Kuang, Yao Lu, Yansong Pang, and Zhuo Xu for supporting this project.\n' +
      '\n' +
      'We thank all the users who volunteered to collect the robot teaching data. We also thank the Google DeepMind Visualization and Human Interaction teams for their help with the development and support of the chat interface. We also want to thank the entire Google DeepMind Robotics team whose tireless efforts can be traced to additional support on this paper. This includes Administrative, Product, Programs, and Strategy teams whose contributions impact all of the team\'s successes. We also want to thank our friends in Google DeepMind and Google Research for their guidance, inspirational research, and even direct contributions.\n' +
      '\n' +
      '**Program Leads**\n' +
      '\n' +
      'This project is part of the Google DeepMind 2023 program "ApprenticeBots," an interactive embodied AI moonshot with the mission statement: "anyone can teach a robot, and a robot that can learn from anyone."\n' +
      '\n' +
      'Carolina Parada, _Director_\n' +
      '\n' +
      'Nik Stewart, _Technical Program Manager_\n' +
      '\n' +
      'Jie Tan, _Team Lead_\n' +
      '\n' +
      '**Technical Leads**\n' +
      '\n' +
      'Andy Zeng, _Research Lead_\n' +
      '\n' +
      'Wenhao Yu, Fei Xia, _Data Collection & Teaching Leads_\n' +
      '\n' +
      'Jacky Liang, _Model Training & Improvement Lead_\n' +
      '\n' +
      'Jasmine Hsu, _Data & Logging Lead_\n' +
      '\n' +
      'Peng Xu, _Infrastructure Lead_\n' +
      '\n' +
      'Ben Jvenis, _Operations Lead_\n' +
      '\n' +
      'Erik Frey, _Simulation Lead_\n' +
      '\n' +
      '**Operations**\n' +
      '\n' +
      'Ben Jvenis, Travis Armstrong, _Head of operations_\n' +
      '\n' +
      'Jasmine Hsu, Jacky Liang, _Data collection monitoring_\n' +
      '\n' +
      'Wenhao Yu, _Pilot studies for Robot Dog_\n' +
      '\n' +
      'Fei Xia, _Pilot studies for Mobile Manipulator_\n' +
      '\n' +
      'Baruch Tabanpour, _Pilot studies for Aloha_\n' +
      '\n' +
      'Maria Attarian, Jonathan Tompson, _Pilot studies for Bi-arm Kuka_\n' +
      '\n' +
      'Joss Moore, Maria Bauza, _Pilot studies for Kuka+Hand_\n' +
      '\n' +
      '_Contributors_: Maria Attarian, Ken Caluwaerts, Jasmine Hsu, Jacky Liang, Assaf Hurwitz Michaely, Jonathan Tompson, Fei Xia, Wenhao Yu, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Data Logging Infrastructure**\n' +
      '\n' +
      'Jasmine Hsu, Ken Caluwaerts, _Datasets and dashboards_\n' +
      '\n' +
      'Peng Xu, Assaf Hurwitz Michaely, Jacky Liang, _Materialization Contributors_: Adil Dostmohamed, Marissa Giustina, Nikhil Joshi, Jacky Liang, Quan Vuong, Tingnan Zhang\n' +
      '\n' +
      '**Model Serving Infrastructure**\n' +
      '\n' +
      'Assaf Hurwitz Michaely, Ying Xu, _Core contributors_\n' +
      '\n' +
      'Jasmine Hsu, Ken Caluwaerts, Adil Dostmohamed, _LLM Chat UI Contributors_: Jacky Liang, Allen Ren, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Model Training Infrastructure**\n' +
      '\n' +
      '_Core contributors_: Assaf Hurwitz Michaely, Jacky Liang, Peng Xu, Andy Zeng, Jasmine Hsu, Edward Lee\n' +
      '\n' +
      '_Contributors_: Quan Vuong, Tingnan Zhang\n' +
      '\n' +
      '**Evaluations & Analysis**\n' +
      '\n' +
      'Jacky Liang, _Technical Lead_\n' +
      '\n' +
      'Leila Takayama, _Human-Robot Interaction Lead_\n' +
      '\n' +
      '_Contributors_: Alex Bewley, Keerthana Gopalakrishnan, Jasmine Hsu, Jacky Liang, Assaf Hurwitz Michaely, Dorsa Sadigh, Fei Xia, Ted Xiao, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Prompt Engineering**\n' +
      '\n' +
      'Maria Bauza, Marissa Giustina, Kuang-Huei Lee, Jacky Liang, Joss Moore, Dushyant Rao, Baruch Tabanpour, Fei Xia, Wenhao Yu, Andy Zeng\n' +
      '\n' +
      '**Simulation & MJPC**\n' +
      '\n' +
      'Maria Attarian, Ken Caluwaerts, Erik Frey, Chuyuan Kelly Fu, Nimrod Gileadi, Leonard Hasenclever, Jan Humplik, Nikhil Joshi, Ben Jyenis, Joss Moore, Dushyant Rao, Baruch Tabanpour, Fei Xia, Ted Xiao, Wenhao Yu, Tingnan Zhang\n' +
      '\n' +
      '**Robot-Specific Infrastructure**\n' +
      '\n' +
      '_Robot Dog_: Ken Caluwaerts, Marissa Giustina, Chase Kew, Ken Oslund, Wenhao Yu Tingnan Zhang,\n' +
      '\n' +
      '_Mobile Manipulator_: Fei Xia, Chuyuan Kelly Fu\n' +
      '\n' +
      '_Aloha_: Baruch Tabanpour, Jonathan Tompson, Erik Frey\n' +
      '\n' +
      '_Bi-arm Kuka_: Maria Atarian\n' +
      '\n' +
      '_Kuka+Hand_: Maria Bauza, Joss Moore, Dushyant Rao, Nimrod Gileadi\n' +
      '\n' +
      '**Real Robot Deployment & Policy Distillation**\n' +
      '\n' +
      'Ken Caluwaerts, Chuyuan Kelly Fu, Leonard Hasenclever, Jan Humplik, Chase Kew, Sean Kirmani, Kuang-Huei Lee, Ken Oslund, Allen Ren, Jonathan Tompson, Quan Vuong, Fei Xia, Ted Xiao, Zhuo Xu, Wenhao Yu, Tingnan Zhang\n' +
      '\n' +
      '**Advising**\n' +
      '\n' +
      'Alex Bewley, Erik Frey, Leonard Hasenclever, Jasmine Hsu, Jan Humplik, Brian Ichter, Kuang-Huei Lee, Jacky Liang, Carolina Parada, Dushyant Rao, Dorsa Sadigh, Nik Stewart, Leila Takayama, Jie Tan, Fei Xia, Ted Xiao, Peng Xu, Wenhao Yu, Andy Zeng, Tingnan Zhang\n' +
      '\n' +
      '**Additional Contributions**\n' +
      '\n' +
      '_Authorship and Acknowledgments_: Nik Stewart\n' +
      '\n' +
      '_Paper Content and Web Posts_: Carolina Parada, Andy Zeng, Wenhao Yu, Jacky Liang, Fei Xia, Tingnan Zhang\n' +
      '\n' +
      '_Steering_: Carolina Parada, Nik Stewart, Izhak Shafran, Vincent Vanhoucke, Maja Mataric, Leila Takayama, Jie Tan, Dorsa Sadigh, Andy Zeng, Wenhao Yu, Jacky Liang, Fei Xia, Tingnan Zhang\n' +
      '\n' +
      '## VI Appendix\n' +
      '\n' +
      'We organize the appendix as follows:\n' +
      '\n' +
      '* Details on data collection (e.g., chat UI), and evaluation protocol (e.g., task sampling). Section VI-A\n' +
      '* Additional results and evaluations in Section VI-B.\n' +
      '* Details on top-users conditioning (Section VI-C): how they are autonomously selected, quantitative and qualitative analysis on how top-user teaching data differs from other users.\n' +
      '* Retrieval baseline details (Section VI-D) and data augmentation (Section VI-E).\n' +
      '* Quantitative analysis of chat feedback embeddings (Section VI-F).\n' +
      '* Real robot experiments details (Section VI-G) including model training and deployment (Section VI-H).\n' +
      '* User studies and performance drift Section VI-I.\n' +
      '* Failure mode analysis Section VI-J.\n' +
      '* Model performance on existing code-writing benchmarks Section VI-K.\n' +
      '* Robot-specific embodiment details (Section VI-L), tasks (Section VI-M), and prompts (Section VI-N).\n' +
      '\n' +
      '### _Data Collection and Evaluation Details_\n' +
      '\n' +
      'During data collection, non-expert users interact with the robot using natural language through a browser-based chat UI (shown in Fig. 3). The chat UI displays a user input box, the message history, and a visualization of the simulated robot and its surroundings using MuJoCo [65]. The human provides textual input and the LLM replies to each subsequent user query with executable code. The user can then select a button to either run the code in the simulator to observe the resulting motion, or run it on a real robot. The user can continue to provide feedback (which can be multi-turn contextual) and continue modifying the behavior through text inputs in the chat UI until the desired robot behavior is achieved. Each user is remotely connected (via Remote Desktop) to one machine, drawn from a shared pool of high performance machines (128 cores) in the cloud. Machines with high core counts are necessary for Mujoco\'s MJPC [27] to synthesize robot motion at an interactive rate - leading to better low-level robot behaviors and subsequently user feedback data.\n' +
      '\n' +
      'For each chat session, the user teaches the robot one task specified via language e.g., teach the robot-dog to "sit down and give a high-five." For each chat turn (user-input, LLM-output pair), the user has the option to rate the individual robot response as \'good\' or \'bad\'. These single turn good rating rates (while not used during training) help us evaluate responsiveness to feedback across individual responses, and we find they are strongly correlated to task success (see Fig. 3). Finally, users can label the entire chat session as "success" by clicking a success button if the robot succeeded at the task during the conversation, or "failure" if the task does not succeed within 7 rounds of human input. Variation in success is expected, and users are encouraged to rate success based on the observed behaviors of the robot (as opposed to the accuracy of the code). After labeling a chat session, the chat history UI refreshes, the robot simulator is reset, and a new sampled task and embodiment is presented to the user. Users are able to flag chat sessions in case of technical difficulties.\n' +
      '\n' +
      'Our UI backend uses a Task Sampler, which is configured to (i) randomly sample tasks from the set of 78 tasks across 5 embodiments (platforms illustrated in Fig. 1), and (ii) randomly sample an LLM model to connect to. Users do not know which model they speak to, which allows us to perform fair blind A/B evaluations. All experiment numbers are computed with data collected using this sampler.\n' +
      '\n' +
      'From the perspective of users, our data collection protocol is equivalent to our evaluation protocol - we train on data collected from users interacting with the model(s) through the UI, and we measure whether users believe the model(s) to have improved (via statistics on good rating rates and session success labels) through the UI with blind A/B evaluations. This deviates from the standard norm in robot learning pipelines (e.g., 3-stage pipeline of collect data, train, and evaluate), and presents practical infrastructure/operations advantages (predominantly around simplicity).\n' +
      '\n' +
      'To operationalize data collection, we started off with running multiple pilot sessions with the users for each embodiment. These pilot sessions were focused on introducing these 35 non-expert users to the the chat UI, the type of tasks they are expected to teach, and the MuJoCo simulation environment. After the pilot sessions conclude, the users were tasked to contribute 10 chat sessions per day on all embodiments through the task sampler, amounting to 350 chat sessions every day. The users were also asked to fill out a brief questionnaire for a feedback after each day about their experience on the data collection and the overall teaching session. To meet the daily target of 350 chat sessions per day, it was important to maintain participation from all of the users equally to obtain the expected level of diversity in the data. We maintained consistent distribution of number of chat sessions across multiple embodiments i.e. Robot Dog, Mobile Manipulator, Aloha, Bi-arm Kuka, Kuka+Hand.\n' +
      '\n' +
      'The users who participated in these experiments were 23-43 years of age (\\(M\\)=30.5, \\(SD\\)=5.6), including 11 who identified as cisgender women, 17 who identified as cisgender men, and 1 as non-binary. They had a range of educational degrees (9 Associates degrees or some college, 6 Bachelor of Arts, 11 Bachelor of Sciences, and 3 Masters degrees) - 14 non-technical and 15 technical. When asked about their familiarity with the ML models on a scale of 1 (zero familiarity) to 5 (most familiar), 15 users reported 1 (zero familiarity), 11 users reported 2, and 3 users reported 3; none of the users reported to have more familiarity with ML models (4 or 5).\n' +
      '\n' +
      'Fig. 7: Correlation of Operator Success Rate and Num Chat Turns until Failures\n' +
      '\n' +
      '**User Persistence Analysis.** We plot the success rate of each user against the mean number of chat turns in failed sessions across each user in Fig. 7. The higher the mean number of chat turns for failure, the more persistant the user was in teaching the robot (i.e. the user did not give up early). These two quantities exhibit a slight positive correlation, suggesting that on average, users who were more persistant at teaching achieved slightly higher success rates.\n' +
      '\n' +
      '### _Additional Results_\n' +
      '\n' +
      '**Per Embodiment Evaluation.** Fig. 8 shows our main teachability result (Fig. 5) separated by embodiments. On test tasks, models improved upon the base model the most in Aloha and Bi-arm Kuka, while LMPC-Rollouts improved much higher on Kuka with Hand than the other model.\n' +
      '\n' +
      '**Chat Duration Analysis.** We measured and analyzed the duration of chat sessions and chat turns and compared them across different embodiments and models. Chat turn duration measures the total time it took for the model to respond, for the user to run the robot code in simulation, for the user to observe the resulting robot behavior, and for the user to input the subsequent language feedback. Fig. 9 shows the distribution of chat turn durations across both models and embodiments. While there are no obvious differences in these distributions, some are more long-tailed than others, and we see this in the median statistics. In Table VI, we show the median durations for chat sessions and chat turns across different embodiments. Kuka+Hand and Bi-arm Kuka have significantly higher durations than other embodiments. This reflects that these embodiments were likely more difficult to teach (it took longer for users to respond) as well as taking longer to simulate (they had tasks that had longer horizons than the other embodiments). In Table VII, we compare the median durations for LMPC-Rollouts and LMPC-Skip. LMPC-Rollouts has slightly higher chat turn and chat session durations, and this difference reflects how inference (decoding the LLM for entire chat sessions) for LMPC-Rollouts takes slightly longer than inference for LMPC-Skip. Lastly, in Fig. 10, we show a small negative correlation between task success rate and chat duration -- the longer it takes for users to complete a chat turn, the less likely it is for that task to be successful.\n' +
      '\n' +
      '**Training LMPC on Both Success and Failures.** In principle, LMPC-Rollouts (when viewed as a dynamics model) can be trained on both success and failure data (since all chat turns are valid transitions, regardless of whether the session ended in task success). In this version, LMPC-rollouts also predicts (on trajectory termination) whether the predicted rollout would lead to a success or failure. Inference-time search would then be adjusted accordingly to disregard sampled rollouts that ended in predicted failure. While this remains an interesting aspect of LMPC-Rollouts, our main experiments report results from training LMPC-Rollouts on success data only (as a fair comparison with LMPC-Skip, which can only be trained on success data), with which we do observe performance improvements over mixing failure sessions into the training data (results in Table VIII). We hypothesize that training LMPC-Rollouts only on sessions that ended in task success yields more efficient inference-time search, since the alternative of training on both success and failure sessions leads to more unused predicted rollouts that terminate with failure.\n' +
      '\n' +
      '### _Top-Users and Details on Autonomous Top-Users Selection_\n' +
      '\n' +
      'We identify top-users by evaluating how well they perform on training tasks (Appendix -D.2), weighted by task difficulty. Let there be \\(N\\) tasks and \\(K\\) users. Let \\(s(n,k)\\) denote the self-reported success rate of the \\(n\\)th task for the \\(k\\)th user, \\(c(n,\\,k)\\) denote the number of times the \\(k\\)th user taught the \\(n\\)th task, and \\(\\bar{c}(n,k)=\\mathbbm{1}(c(n,k)\\geq 1)\\) to indicate whether or not the \\(k\\)th user has taught the \\(n\\)th task. Due to practical constraints, \\(\\bar{c}(n,k)=0\\) for many user-task pairs. We define the task difficulty rating \\(d(n)\\) as the average task failure rate across all users: \\(d(n)=1-\\frac{1}{K_{n}}\\sum_{k=1}^{K}s(n,k)\\bar{c}(n,k)\\), where \\(K_{n}=\\sum_{k=1}^{K}\\bar{c}(n,k)\\). Then, we define a user performance score as a user\'s average success rate weighted by the task difficulty rating: \\(h(k)=\\sum_{n=1}^{N_{k}}d(n)s(n,k)\\bar{c}(n,k)\\), where \\(N_{k}=\\sum_{n=1}^{N}\\bar{c}(n,k)\\). We define top-users as those who are in the top \\(75\\)th percentile by this performance score. We refer to the remaining users as "other users"\n' +
      '\n' +
      'Table IX shows the average performance improvements of user-conditioned LMPC over the base model split by top users and other users. We observe largest performance improvements when LMPC-Rollouts (conditioned on top-users) is served to top users directly, and this is less evident with LMPC-Skip, suggesting that inference-time search (via MPC) over future interactions performs better at catering to improving the teachability of top users (e.g., satisfying their criterion for success).\n' +
      '\n' +
      'Our experiments in the main paper (Table II) demonstrate that conditioning LMPC on top-users can drive performance improvements for all users - but what makes top-user teaching data different from other users? To explore this question, we define 4 axes\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Embodiment & Chat Session Duration (s) & Chat Turn Duration (s) \\\\ \\hline Kuka+Hand & 429 & 97 \\\\ Bi-arm Kuka & 406 & 88 \\\\ Aloha & 200 & 66 \\\\ Mobile Manipulator & 238 & 65 \\\\ Robot Dog & 138 & 41 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VI: Median Chat Session and Chat Turn Durations across Embodiments\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Top Users & Other Users \\\\ \\hline LMPC-Skip & \\(+15.1\\%\\) & \\(+14.2\\%\\) \\\\ LMPC-Rollouts & \\(+26.3\\%\\) & \\(+18.9\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE IX: Success rate improvements by user group for test tasks.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & Train Tasks & Test Tasks \\\\ \\hline LMPC-Rollouts-with-Failures & \\(-11.5\\%\\) & \\(-14.0\\%\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE VIII: Success Rates of Training LMPC-Rollouts on both Success and Failure chat sessions.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      'we compute a T-SNE embedding vectors, mapping each embedding with associated features for: whether the query was from a "Top User" or not, whether the user rated the LLM response to the query as "Good" or "Bad", whether the query was from a session which resulted in a "Success" or "Fail", and which robot embodiment the session used. First, we find that user queries are indeed highly correlated with specific embodiments, as shown in Figure 12. This intuitively makes sense since language embeddings will consider semantic details like specific syntax or verbal suggestions that are specific to tasks or robot physics that are only present on a specific embodiment (for example, "raise your paw higher" is only relevant for the Robot Dog embodiment). Second, we find that there are clear cases where for even the same embodiment, "Top User" semantic language embeddings are clearly clustered separately from "Other Users", as shown in Figure 13. Additionally, we also find other interesting clusters, such as where "Other Users" seem to be more pessimistic about LLM responses by giving clusters of "Bad Ratings", which result in either "Success" or "Fail".\n' +
      '\n' +
      '### _Real Robot Experiments_\n' +
      '\n' +
      '**Distillation for Robot Dog.** Our robot dog distilled policy is based on the Locomotion-Transformer model, which uses a Transformer to map sequences of velocity commands, proprioceptive observations, and past actions to next actions [10]. We generalize the original velocity command formulation to MJPC cost weights and parameters as the objective tokens. Our final policy consists of a transformer with \\(d=256\\) and four layers, totaling roughly 3.2 million parameters.\n' +
      '\n' +
      'To train the policy, we used online imitation learning (DAgger) against an expert MJPC policy over a distribution of tasks encompassing both static posing and locomotion behaviors. This task distribution was constructed by uniformly randomizing key target parameter values, including robot velocity, torso height and pitch, foot positions, and foot stepping. Due to the diversity of the task distribution and domain randomization, we found offline imitation (BC) to be unsuccessful. We also smooth all actions by applying an exponential filter with strength 0.9.\n' +
      '\n' +
      '**MJPC-as-Planner for real Mobile Manipulator.** For the main experiment results in deploying the taught skills in simulation to the real mobile manipulator robot, we extend the MJPC-as-Planner approach from prior work by Yu et al [71]. In particular, to obtain a simulated replica of the real scene, Yu et al. used an open-vocabulary object detector to detect and segment objects in the scene and fit known mesh models to the corresponding point clouds. The reconstructed simulation scene is used in MJPC to generate a trajectory plan, which is then executed on the robot.\n' +
      '\n' +
      'Though the prior work showed good results in real-world, it required knowing the list of objects in the scene and their corresponding meshes in order to query the object detection model and recreate the scene. In this work, we improve the perception pipeline on both fronts: 1) we use a large visual language model (VLM) to identify all the objects seen by the robot in the environment, each of which is then segmented using the Segment Anything (SAM) model [36] to achieve precise object localization, 2) we opt to use generic primitive shapes consisting of capsules and boxes to represent the objects, which enables us to represent a wide range of objects without having to obtain detailed meshes. As a result, we can apply our approach to more diverse environments with unknown objects and be able to teach the robot manipulation skills on them. An example can be seen in Fig. 14.\n' +
      '\n' +
      '**Sim-to-Real Gap.** Table XI shows a comparison between sim and real performances on the set of tasks we evaluated in the real world. For open drawer task, we achieve \\(100\\%\\) success rate in real world, likely because this task is quasi-static thus there is very little physical domain gap. By contrast, knock over coke can only achieved \\(20\\%\\) success rate in the real world, due to the velocity of the end effector not being fast enough. This is caused by physical modelling domain gap, which allows the robot to knock over the coke can with a lower end-effector velocity. For the hop while turning task we observe a large discrepancy between simulation and real world. While we are able to teach the robot to hop, it often trips and falls after a few hops. This is due to that a highly agile hopping while turning behavior is outside the training distribution of the distilled policy. Adapting the distillation training distribution to the teaching data is a promising direction for future research.\n' +
      '\n' +
      '**Distillation for Mobile Manipulator.** There are a few limitations with the MJPC-as-Planner approach: 1) generating the plan with MJPC is not feasible for onboard computing due to computation requirements, 2) it needs multiple models to identify and segment the objects, adding additional complexities to the system, 3) it does not respond to changes in the environment during execution. To make a step towards addressing these issues, we explore the reward-conditioned policy distillation approach used for the Robot Dog on the Mobile Manipulator. Specifically, we validate the idea in two settings: 1) use the reward to condition final object height for the picking task, 2) use reward to condition picking up or knocking over a can. We use MJPC to generate 30k and 10k trajectories respectively with maximally 150 steps. The robot observation in each step consists of the simulated depth image from robot camera and the reward parameters. We train the policies based on the RT-1 model [8] using the generated dataset and deployed the policies on a real mobile manipulator robot. Distilling a reward conditioned policy allows us to deploy the policy with onboard computing and achieve more robust behavior with closed-loop control. Although we have yet to perform quantitative evaluations of the distilled Mobile Manipulator policy, we demonstrate example policy rollouts in Fig. 15.\n' +
      '\n' +
      '### _Language Model Training Details_\n' +
      '\n' +
      'For finetuning models, we set the number of training steps to cover \\(10\\) epochs of the available training data, apply Adam with a\n' +
      '\n' +
      'Fig. 13: T-SNE plot of embeddings of human feedback across experts and non-experts, and across good/bad chat ratings (left) and whether or not that feedback belongs to a chat session that was eventually a success/failure (right).\n' +
      '\n' +
      'learning rate of \\(5\\times 10^{-3}\\), a linear ramp up and cosine decay learning rate scheduler, a batch size of 4, and a context length of 4096 tokens.\n' +
      '\n' +
      'Because the LMPC-Rollouts model needs to predict the entire remaining chat session, it is much slower than LMPC-Skip at inference time. According to user feedback, the slowed inference time degrades the teaching experience, making the chat session less engaging, potentially reducing data quality. To address this issue,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c} \\hline \\hline\n' +
      '**Embodiment** & **Task** & **Ours-Sim** & **Ours-Real** & **N Chat Turns** & **PaLM 2-S Sim** & **PaLM 2-S Real** & **N Chat Turns** \\\\ \\hline Robot Dog & High-Five with left hand & \\(100\\%\\) & \\(100\\%\\) & \\(3.0\\) & \\(100\\%\\) & \\(75\\%\\) & \\(2.3\\) \\\\ Downward Dog & \\(100\\%\\) & \\(100\\%\\) & \\(2.8\\) & \\(100\\%\\) & \\(100\\%\\) & \\(1.3\\) \\\\ Walk forward in a trotting gait & \\(100\\%\\) & \\(100\\%\\) & \\(2.8\\) & \\(25\\%\\) & \\(25\\%\\) & \\(2.0\\) \\\\ Hop & \\(100\\%\\) & \\(75\\%\\) & \\(2.3\\) & \\(50\\%\\) & \\(25\\%\\) & \\(2.0\\) \\\\ Hop while turning counterclockwise & \\(100\\%\\) & \\(25\\%\\) & \\(4.0\\) & \\(100\\%\\) & \\(25\\%\\) & \\(5.0\\) \\\\ \\hline Mobile Manipulator & Open top drawer half-way & \\(100\\%\\) & \\(100\\%\\) & \\(3.2\\) & \\(100\\%\\) & \\(100\\%\\) & \\(3.4\\) \\\\ Push coke can from right to left & \\(100\\%\\) & \\(80\\%\\) & \\(2.0\\) & \\(100\\%\\) & \\(60\\%\\) & \\(2.0\\) \\\\ Knock over coke can & \\(100\\%\\) & \\(20\\%\\) & \\(3.0\\) & \\(80\\%\\) & \\(20\\%\\) & \\(5.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XI: Sim vs. Real Results\n' +
      '\n' +
      'Fig. 14: Real world teaching example on mobile manipulator using the MJPC-as-Planner approach.\n' +
      '\n' +
      'Fig. 15: **Example Rollouts of reward conditioned distilled policy on mobile manipulator.** Apart from using MPJC-as-Planner for real world deployment, we also explored distilling the behavior into a policy using imitation learning following the robot dog example. This no longer requires accurate state estimation.\n' +
      '\n' +
      'we performed 8-bit quantization on the LMPC-Rollout models after finetuning, and we serve the quantized LMPC-Rollout models. We did not observe noticeably performance drops with the quantized model. See Table XII for measured inference times for these models -- LMPC-Rollouts without quantization is much slower than LMPC-Skip, while with quantization the inference times are similar.\n' +
      '\n' +
      '### _User Performance Drift Analysis_\n' +
      '\n' +
      'Evaluating models over an extended period of time introduces the concern that as users obtain more practice teaching the robot, they become more proficient, and model performance improvements may actually be caused by users\' improved teaching skills, instead of improved model capability. We took three measures to mitigate this concern. First, we conducted pilot data collection sessions for each robot embodiment, so users could acquire a base level of familiarity with each embodiment before conducting official data collection. Second, we evaluated LMPC variants during the same data collection days, so their differences are unlikely to be caused by user performance drift. Third, we explicitly compared user performance with the base LLM during the first half and the second half of our experiments. From the first to the second period, mean success rate for each user changed by \\(-0.6\\%\\) across all tasks, with a standard deviation of \\(9.3\\%\\), and the two periods show a Pearson correlation coefficient of \\(0.87\\).\n' +
      '\n' +
      'Another way to gauge potential changes in the users\' teaching experience is by measuring the self-reported cognitive load of the teachers at the end of each day of data. Because different subsets of users taught robots on different days, we analyzed our data in terms of how each user experienced the cognitive load of teaching the robots on their first day vs. on their last day. We used a subset of the NASA-TLX measure of cognitive load [23] and analyzed the perceived mental demand, effort, performance, and frustration dimensions [20]. There were 13 users who completed our end-of-day questionnaires so we ran pair-wise t-tests (2-sided) on their data (N=13). We found no statistically significant differences in teachers\' first vs. last days of teaching robots in terms of mental demand (\\(p\\)=0.26), effort (\\(p\\)=0.47), performance (\\(p\\)=0.22) or frustration (\\(p\\)=0.54). These are all well above the cut-off p-value for statistical significance of.05; with Bonferroni corrections, the cut-off value would be even lower at.0125.\n' +
      '\n' +
      'These results suggest minimal user performance or user experience change over time, so differences among models are more likely the result of changes in model capability, not user teaching proficiency.\n' +
      '\n' +
      '### _Failure mode analysis_\n' +
      '\n' +
      'We categorized the following failure modes across our compared models. Failure mode 1) is outputting code with errors or executable code. In instances where it was outputting executable code, we further checked if 2) the failure was from repeated code outputs, 3) from incomplete plans, or 4) from the LLM not responding to the user\'s feedback. In order to identify these failure modes, we prompted an LLM to classify them from chat session data.\n' +
      '\n' +
      'See Table XIII, where it shows the percentage of chat sessions that resulted in each failure mode across all chat sessions (the denominator is the total number of chat sessions for that model, not the number of failures for that model). Please note that a particular chat session may appear in multiple failure modes, so these failure mode sets are not disjoint. As expected, LMPC models have overall fewer failures than baselines. The most frequent failure mode is outputting code that is not responsive to user feedback. The least frequent failure mode is outputting incomplete code.\n' +
      '\n' +
      '### _Fine-tuned Models on Code-writing Benchmarks_\n' +
      '\n' +
      'One concern with model finetuning is that the finetuned model may forget some of its original capabilities. In our case, we are specifically concerned about whether or not finetuning our model degrades general code-writing capabilities of the LLM. To test this, we evaluated our models after one and two iterations of finetuning on the RoboCdeGen benchmark [41]. As seen in Table XIV, there is relatively no degradation between the first iteration and the baseline model or between first and second iteration. We attribute this to our training data being in the distribution of the base LLM as well as using code, therefore not biasing the model away from code generations.\n' +
      '\n' +
      '### _Robot Embodiment Details_\n' +
      '\n' +
      '**Robot Dog.** The robot dog is a small quadruped robot with an onboard computer and battery power. The robot was developed in-house based on the design from [10]. The robot has a standing height of approximately 0.4 m. Each of the robot\'s 4 legs has 3 DoFs with a peak joint torque of 18 Nm.\n' +
      '\n' +
      'The distilled policy (Section VI-G) runs on the onboard computer (Intel NUC11TNBv7) and provides joint position commands at 50 Hz to a low-level PD controller that outputs joint torques at 1 kHz. We set the P-gain to 50 Nm/rad and D-gain to 1.1 Nm s/rad. We use ROS2 over a Wi-Fi connection to transmit the model output (the reward function code) from a desktop computer to the robot when a user clicks the _Run on Robot_ command in the chat UI.\n' +
      '\n' +
      'The simulated environment for the robot dog contains a door without latch and a three-level kitchen drawer. In the MJPC implementation, we place position and orientation sensors for the robot torso and end-effectors, as well as joint sensors for the articulated objects (e.g. door hinge angle). We then design a set of APIs that the LLM can use to modulate the desired absolute and relative sensor values (see more details in prompts). By coordinating the movements of different legs in MJPC simulation, the robot dog can achieve a rich set of skills from locomotion to posing. Furthermore, although the robot dog does not have any form of gripper, it can interact with the external world using its torso and limbs to perform tasks such as open the door or close the drawer.\n' +
      '\n' +
      '**Mobile Manipulator.** The mobile manipulator [24] consists of a 7-DoF arm and a parallel jaw gripper. The simulated environment contains the simulated robot in front of household objects (apple, coke can, and cube) placed on a counter with drawers.\n' +
      '\n' +
      'For the MJPC implementation, we place gripper and joint sensors on the robot, position and orientation sensors on household objects, and joint sensors on articulated objects (e.g. drawer hinges).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline LMPC-Skip & LMPC-Rollouts & LMPC-Rollouts-No-Quantization \\\\ \\hline \\(1.1\\pm 0.2\\) & \\(1.0\\pm 0.4\\) & \\(7.4\\pm 4.7\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XII: Model inference times in seconds.\n' +
      '\n' +
      'With the MJPC planner, the robot is able to execute a rich set of tasks with various constraints, including opening the drawer half way, picking/pushing object A to a certain location without knocking over object B, lifting objects up to a certain position and orientation, etc. We further test the skills on the real robot.\n' +
      '\n' +
      'We use a gRPC connection over Wi-Fi to transmit the reward function code output by the model from a desktop computer to the robot when a user clicks the _Run on Robot_ command in the chat UI. **Aloha.** The Aloha bi-manual robot [76] consists of two 6-DoF arms fixed to a table, each with a 1-DoF parallel gripper. Household objects (e.g. apple, soda can, cube, and bowl) are placed on the table.\n' +
      '\n' +
      'The MJPC implementation includes sensors for the position and orientation of each arm, and an API to get the position and orientation of all objects in the workspace. With MJPC, Aloha is able to execute a wide variety of tasks, such as picking/placing objects, using both arms to re-orient objects, and handing over objects from one arm to the other.\n' +
      '\n' +
      'To allow MJPC to successfully plan in the 14-DoF action space, we run the simulation at 25% real-time speed. MJPC is able to find dynamic behaviors to solve tasks, such as rolling an apple on the table to move it closer to another object, or using one arm as leverage to flip a bowl upside down with the other arm. These policies are only tested in simulation and are not tuned for transfer to real.\n' +
      '\n' +
      '**Bi-arm Kuka.** The Kuka bi-arm robot is comprised by two 7-DoF Kuka LBR IIWA14 arms fixed to the ground. For the scope of this work, no end-effector was attached to them. Arrow indicators along with the words "left" and "right" were added to facilitate data collection with regards to natural language to orientation mappings in the scene. For this embodiment, we have developed two distinct scenes:\n' +
      '\n' +
      '1. **Single large cube**: This scene contains a single large cube in the center of the arms\' workspace.\n' +
      '2. **Particle manipulation**: This scene contains 5 small cubes (particles) of various colors - red, green, blue, yellow, purple - initialized in random positions within the workspace of the arms. More specifically, their x and y coordinates at initialization are each sampled randomly from a uniform distribution between (\\(-0.5\\),\\(0.5\\)).\n' +
      '\n' +
      'Finally both scenes contain four separate goal points, two in the air (blue goal and red goal) and two on the ground (green goal and purple goal) that are stationary and can be used as target positions for moving objects.\n' +
      '\n' +
      'The MJPC implementation for this embodiment includes sensors for the position and orientation of each arm, each goal and each movable object. It also includes an API that allows obtaining and setting the poses of all objects. For this platform, MJPC is leveraged to perform singular tasks that involve moving the large cube towards goal positions or in relative locations, pick up cubes, sweep cubes, as well as sequential tasks such as moving one block towards another followed a subsequent move towards a third block or goal position.\n' +
      '\n' +
      'A current limitation of this embodiment is that it is only evaluated in simulation. The setup as well as the arm control are not yet realistic and would hinder any sim2real transfer. In addition, for all results presented in this work, bi-arm Kuka was considered an unseen embodiment used only for testing and not training. This causes some domain shift in terms of produced code (e.g. minor code mistakes such as APIs from other embodiments can be generated on occasion) which can increase the number of chat UI interactions. **Kuka+Hand.** This embodiment comprises a 7-DoF Kuka LBR IIWA14 arm attached with a custom hand with three fingers (4-DoF each). The arm is fixed within a basket containing four objects: red and green blocks, and a connector that can be inserted into a plug base.\n' +
      '\n' +
      'The simulation and MJPC implementation for this embodiment includes sensors providing the positions of all finger and arm joints and pose/orientation of the hand and all objects in the scene.\n' +
      '\n' +
      'With MJPC, the Kuka+Hand can execute a number of interesting behaviors, such as dexterously using the fingers to rearrange objects. Since there are no constraints imposed on maintaining contact with the objects, we observe that the fingers can sometimes leverage dynamic manipulations e.g., flicking objects from one part of the workspace to another, or juggling to re-orient object mid-air before grasping them to place them down. The caveat of course, is that these behaviors are optimized in simulation and require object pose information during predictive control rollouts (which may struggle to transfer to the real world via sim2real distillation).\n' +
      '\n' +
      'In terms of limitations, the predictive control search space for dexterous manipulation with all \\(7\\!+\\!4\\!\\times\\!3\\!=\\!19\\) degrees of freedom is large and can be challenging to do sampling-based control over. Thus the simulator runs at only 15% of real-time speeds (to allow for compute-bound MJPC with 128 cores) to discover manipulation solutions. Chat turn durations (shown in Table VI) suggest that the Kuka+Hand platform takes the longest time for users to teach - each chat turn takes on average 1.5 minutes, while chat sessions around 7 minutes, much of the time is spent watching the robot "figure out" online how to do the task.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{Pass@1} \\\\  & Iteration 1 & Iteration 2 \\\\ \\hline PalLM 2-S & \\multicolumn{3}{c}{51\\%} \\\\ LMPC-Kollouts & 51\\% & 51\\% \\\\ LMPC-Skip & 49\\% & 49\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIV: Performance on RoboCodeGen on finetuned models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & \\multicolumn{5}{c}{Failure Modes} \\\\ \\cline{2-5} Model & Invalid Code & Repeated Code & Non-responsive Code & Incomplete Code & All Failures \\\\ \\hline PaLM 2-S & 17.4\\% & 10.9\\% & 16.8\\% & 7.6\\% & 35.3\\% \\\\ RAG & 6.4\\% & **6.7\\%** & 19.8\\% & 6.4\\% & 38.5\\% \\\\ LMPC-Skip & 9.5\\% & 7.6\\% & 11.9\\% & **3.8\\%** & **23.0\\%** \\\\ LMPC-Rollout & **7.8\\%** & 7.0\\% & **11.3\\%** & 4.0\\% & 24.7\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table} TABLE XIII: Failure Mode as percentage of all chat sessions.\n' +
      '\n' +
      '### _Tasks_\n' +
      '\n' +
      '**Robot Dog.** We design 19 tasks for the robot dog embodiment, among which 12 are used in training the LLM:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Robot Dog Train Tasks** \\\\ \\hline Sit. \\\\ High-five with the front right paw. \\\\ Downward dog. \\\\ Walk to the left. \\\\ Walk forward. \\\\ Hop. \\\\ Turn around clockwise. \\\\ Walk backward. \\\\ Walk backward while turning to face right. \\\\ Walk forward while turning left. \\\\ Close the middle drawer. \\\\ Open the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      'and 7 are held out for testing the LLM performance:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Robot Dog Test Tasks** \\\\ \\hline High-five with the front left paw. \\\\ Walk to the right. \\\\ Turn around counterclockwise. \\\\ Hop while turning clockwise. \\\\ Close the bottom drawer. \\\\ Close the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Robot Dog Test Tasks** \\\\ \\hline High-five with the front left paw. \\\\ Walk to the right. \\\\ Turn around counterclockwise. \\\\ Hop while turning clockwise. \\\\ Close the bottom drawer. \\\\ Close the door by pushing it. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Robot Manipulator.** We task the users to teach the mobile manipulator 14 tasks, among which 11 are used for training:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Mobile Manipulator Train Tasks** \\\\ \\hline Grasp the apple. \\\\ Knock over coke can. \\\\ Lift the apple high. \\\\ Place the apple next to the cube. \\\\ Push the apple toward the cube. \\\\ Move the cube further away from the robot. \\\\ Move the cube a little bit to the left. \\\\ Open the top drawer. \\\\ Place the cube behind the apple. \\\\ Flip the cube upside down. \\\\ Place the apple on the cube. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Aloha.** The robot was instructed by users to perform 16 tasks in total, with the following 10 used in training the LLM:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Aloha Train Tasks** \\\\ \\hline Grasp the apple and lift it up. \\\\ Grasp the coke can and lift it up. \\\\ Pick up the cube and lift it above the apple. \\\\ Pick up the box and lift it above the coke can. \\\\ Flip the apple upside down. \\\\ Flip the drink upside down. \\\\ Flip the apple upside down and move the apple to the center \\\\ of the table. \\\\ Move the box and the apple close to each other. \\\\ Push the box and the bowl close to each other. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Bi-arm Kuka.** The robot was instructed by users to perform 16 different tasks (test only) across the two available scenes:\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Bi-arm Kuka Single Large Cube Scene Test Tasks** \\\\ \\hline Pick up the cube and lift it up to the blue goal. \\\\ Pick up the cube and lift it up by 20cm. \\\\ Move the cube to the green goal on the floor. \\\\ Move the cube 20cm to the right without rotating it. \\\\ Pick up the cube and lift it up to the red goal. \\\\ Move the cube 20cm to the left of the purple goal on the floor \\\\ and rotate it 90 degrees. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Bi-arm Kuka Particle Manipulation Scene Test Tasks** \\\\ \\hline Move the blue cube to the green goal on the floor. \\\\ Move the green cube 20cm to the right. \\\\ Move the red cube to the green goal, then to the purple goal. \\\\ Sweep the red cube and the blue cube towards the green goal. \\\\ Sweep all the cubes to the purple goal. \\\\ Bring the red cube 20cm to the left of the green goal. \\\\ Move the blue cube 20cm in front of the green cube. \\\\ Sweep the yellow cube to the blue cube, then to the red cube. \\\\ Move the purple cube to the yellow cube, then to the green \\\\ cube, then to the blue cube. \\\\ Move the purple cube 10cm to the right of the yellow cube, \\\\ then 20cm behind the blue cube. \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '**Kuka+Hand.** The robot was instructed by users to perform 18 different tasks (test only):\n' +
      '\n' +
      '### _Prompts_\n' +
      '\n' +
      'Prompts for each of the embodiments are shown in Fig. 16, Fig. 17, Fig. 18, Fig. 19, and Fig. 20 respectively. The LLMs are trained to complete session data with the input prompts prepended for each robot embodiment.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:24]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:26]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>