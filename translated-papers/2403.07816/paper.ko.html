<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '유행을 동기화하고, 모든 커뮤니케이션으로 인해 전문가 수에 따라 커뮤니케이션 비용이 증가합니다.\n' +
      '\n' +
      '이 논문에서 우리는 브랜치-트레인-머지와 뮤지션-오브-전문가들의 장점을 결합하면서 단점을 완화하면서 두 세계의 장점을 모두 추구하는 것을 목표로 한다. 우리는 Branch-Train-Merge 방법과 같이 여러 전문가 LLM을 별도로 훈련함으로써 이를 달성하지만, 이후 MoE 아키텍처를 사용하여 해당 전문가를 단일 모델로 결합한다. 보다 구체적으로, 모든 전문가 LLM으로부터의 피드포워드 서브레이어는 각 계층에서 단일 MoE 모듈로 통합되고, 라우터 네트워크는 모든 토큰에서 어떤 피드포워드 전문가를 사용할지를 선택한다. 우리는 단순히 가중치를 평균하여 자기 주의 계층을 포함한 전문가 LLM의 다른 모듈을 병합한다. 그런 다음 결과 모델은 계속 교육을 통해 결합된 모든 데이터에 대해 MoE-피네튜닝되어 라우터가 전문가 피드포워드(FF) 모듈을 혼합하는 방법을 배울 수 있다. 도 1은 본 방법의 개요를 도시한 것으로, 이를 _Branch-Train-MiX_(BTX)라고 한다.\n' +
      '\n' +
      'MoE에 비해 BTX의 주요 장점은 전문가 교육이 당혹스러울 정도로 병렬적이고 비동기적이어서 통신 비용을 줄이고 훈련 처리량을 증가시킨다는 것이다. 브랜치-트레인-머지에 비해 최종 BTX 모델은 다른 표준 LLM과 마찬가지로 미세 조정되거나 사용될 수 있는 통합된 신경망이다. 최종 BTX 모델은 훨씬 더 많은 수의 매개변수를 가지고 있음에도 불구하고 드물게 활성화되기 때문에 시드 모델에 비해 추론 FLOP를 크게 증가시키지 않을 것이다.\n' +
      '\n' +
      '우리는 Llama-2 7B(Touvron et al., 2023)를 시드 모델로 사용하여 실험을 수행하고 수학, 코드 및 위키피디아 도메인에 해당하는 데이터의 서로 다른 하위 집합에 대해 전문가 LLM을 훈련한다. 오리지널 라마-2 7B 가중치가 네 번째 전문가로 추가된 상태에서 사전 훈련 과정에 비해 상대적으로 짧은 기간 동안 결합된 MoE 모델을 미세 조정한다. 결과적으로 BTX 모델은 다양한 도메인에서 과제에 대한 시드 모델보다 상당한 개선을 가져오며, 특히 수학 및 코드 관련 과제에 대한 전문 모델과 격차를 해소하는 동시에 전문 모델이 치명적인 망각으로 고통받는 원래 능력에 대한 성능을 유지한다. BTX는 MoE 미세 조정을 통해 학습된 라우팅의 이점을 입증하는 모든 작업에서 BTM보다 우수하다. 희소 업사이클링과 같은 순전히 MoE 훈련과 비교하여, BTX는 다른 도메인에서 더 높은 훈련 처리량과 더 균형 잡힌 성능으로 더 계산 효율적이다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '비동기 병렬 학습 연산 효율을 위한 훈련 작업자 간의 의사소통 감소는 딥러닝 시스템 훈련을 위한 주요 연구 주제이다. Zhang et al.(2015)은 상이한 작업자들 상의 모델 인스턴스들이 서로 발산할 수 있게 하는 방법을 도입하여, 동기화의 끊임없는 필요성을 제거하였다. 대신에, 작업자들은 때때로 탄성 평균을 사용하여 마스터 웨이트에 느슨하게 동기화된다. Douillard et al.(2023)에 의한 보다 최근의 작업은 그들의 체중 변화를 평균화하고 네스테로프 모멘텀을 적용함으로써 발산된 작업자들의 덜 빈번한 동기화가 실제에서 잘 작동함을 보여주었다.\n' +
      '\n' +
      '그림 1: ** Branch-Train-Mix(BTQ) 방법은 세 가지 단계를 가지고 있다: 1) 미리 훈련된 종자 LLM에서 여러 사본을 만들어 분기하는 단계; 2) 데이터의 다른 하위 집합에서 해당 사본을 별도로 훈련하여 전문가 LLM을 얻는 단계; 3) 전문가 LLM을 혼합 전문가 피드포워드(FF) 계층을 사용하여 단일 LLM으로 결합하고 전체 통합 모델을 미세 조정함으로써 전문가 LLM을 혼합한다.**\n' +
      '\n' +
      'LLMs를 훈련시키기 위해. Branch-Train-Merge 방법(Li et al., 2022; Gururangan et al., 2023)은 다수의 트레이닝 프로세스를 완전히 독립적으로 실행함으로써 극단으로 병렬 트레이닝을 취한다. 각 학습 과정은 특정 도메인 데이터를 이용하므로 해당 모델은 해당 도메인의 전문가가 된다. 마지막으로, 이러한 전문가 모델의 출력 분포를 평균하여 다음 토큰 예측을 수행한다. 어떤 전문가가 평균을 낼지는 입력을 도메인 중 하나 이상으로 분류하여 결정한다. Wortsman et al.(2022)은 별도로 훈련된 모델들의 단순 평균 파라미터들이 성능을 향상시키는 것을 보여주었지만, 모델들은 단지 그들의 하이퍼 파라미터들에서만 달랐다.\n' +
      '\n' +
      'Mixture-of-ExperstMoE는 간단한 Top-K 라우팅 방식을 사용하여 Shazeer et al.(2017)에서 딥 네트워크를 스케일링하기 위해 사용된다. 라우팅 결정들은 이산적이고 따라서 경사 하강에 의해 트레이닝될 수 없기 때문에, 트랜스포머 아키텍처에 대한 다양한 트레이닝 방법들이 탐색되었다(Fedus et al., 2022; Lewis et al., 2021). 놀랍게도, Roller et al. (2021)은 라우팅을 입력 토큰에 기초한 랜덤 매핑을 통해 수행한다면, 어떠한 학습도 없는 고정된 라우팅 방식도 잘 작동한다는 것을 보여주었다. 최근 LLM에 대한 대규모 실험에서, Jiang et al.(2024)은 MoE 접근법이 훨씬 더 적은 수의 활성 매개변수를 사용하여 조밀한 LLM 대응물의 성능과 일치할 수 있음을 입증했다. Dai et al.(2024)의 연구에서는 보다 세분화된 전문가의 장점은 물론 항상 활동적인 상태를 유지하는 공유 전문가를 보유하고 있다는 것을 보여주었다. 우리의 작업과 더 유사하게, Gururangan et al. (2021)은 피드포워드 계층의 전문가를 도메인-조건 고정 라우팅을 사용하여 특정 도메인에 전문화하지만, 우리의 접근법의 비동기적 트레이닝이 부족하다.\n' +
      '\n' +
      '본 연구의 연속학습 방법은 종자모델 훈련에 사용된 초기 데이터와 다른 분포를 가진 데이터셋에 대해 도메인 전문가가 훈련되기 때문에 연속학습(Awasthi and Sarawagi, 2019)과 관련이 있으며, 이는 분기 후 지속적인 훈련으로 구현된다. 구체적으로, 우리의 접근법은 상이한 도메인에 대해 상이한 파라미터를 갖기 때문에 파라미터 분리 방법(Lange et al., 2019)과 관련된다. Aljundi et al.(2016)은 또한 각 도메인 상에서 트레이닝하기 위한 모델의 새로운 사본을 생성한다. Rusu et al.(2016)은 새로운 도메인으로 새로운 모델을 추가하되, 이전 모델들과 연결하여 기존에 학습된 특징들을 사용할 수 있도록 한다. Roziere et al.(2023)은 코드의 특정 도메인에 대한 시드 LLM의 지속적인 트레이닝이 강력한 도메인 전문가 모델을 생성할 수 있으며, 이는 처음부터 시작하는 것보다 훨씬 빠르게 수렴함을 보여주었다. 수학 전문가 양성을 위해서는 일반적인 LLM이 아닌 코드 전문가로부터 시작하는 것이 더 유익한 것으로 나타났다(Shao et al., 2024; Azerbayev et al., 2023).\n' +
      '\n' +
      '## 3 Branch-Train-Mix\n' +
      '\n' +
      '다양한 주제를 다루는 대형 말뭉치에서 사전 훈련된 기존의 LLM(\\mathcal{M}\\)을 고려할 때, 우리는 전문 분야의 성능 향상을 목표로 한다. 이것은 수학, 코드 등과 같은 특정 지식 영역과 관련된 각 학습 데이터 세트 \\(\\mathcal{D}\\coloneq\\{D_{1},\\dots,D_{N}\\}\\)을 사용하여 계속 사전 훈련함으로써 달성된다. 제안된 방법은 Branch, Train, MiX의 세 단계로 구성된다.\n' +
      '\n' +
      '### 지점 & 훈련 : 당황스러울 정도로 병렬적인 전문가 훈련\n' +
      '\n' +
      '종자 모델(\\(\\mathcal{M}\\)로부터 초기화를 수행한 후, 각 모델(\\(\\mathcal{M}_{1},\\dots,\\mathcal{M}_{N}\\)을 훈련하고, 각 모델(\\mathcal{M}_{i}\\)을 사전 훈련 시와 동일한 방법으로 해당 데이터세트(\\(D_{i}\\)에 대해 일반적인 언어 모델링 목표를 사용하여 훈련한다. 각 전문가 모델\\(\\mathcal{M}_{i}\\)은 다른 모델들과 완전히 분리되어 훈련될 수 있기 때문에, 전체 훈련 과정은 당혹스러울 정도로 평행이 된다. 이러한 훈련 패러다임은 대규모 분산 훈련에서 몇 가지 장점이 있다. 이것은 계산의 크기를 확장할 때 전체 학습 처리량의 선형 스케일링을 허용하는 반면, 조인트 학습은 종종 배치 크기 증가로 인해 불확실한 성능에 직면한다. 모든 통신 비용이 저렴합니다. 또한 단일 훈련 실패는 전체 훈련을 중단하는 대신 \\(N\\) 훈련 과정 중 하나에만 영향을 미치기 때문에 더 탄력적이다.\n' +
      '\n' +
      '모든 전문가 교육이 끝나면, 우리는 각각 특정 분포를 전문으로 하는 \\(N\\)개의 다른 LLM을 갖게 될 것이다. 이 시점에서 Branch-Train-Merge 방법(Li et al., 2022; Gururangan et al., 2023)은 추론 시간에 입력이 어느 도메인에 속하는지를 판단하여 어떤 전문가를 사용할지를 선택하는 등 이러한 도메인 전문가를 그대로 사용한다. 일반적으로 여러 전문가가 선택되며 최종 출력 분포는 다음 토큰을 생성하기 위해 단순히 평균화된다. 대조적으로, 우리의 BTX 접근법은 다음 섹션에서 설명할 바와 같이 이러한 도메인 전문가를 추가로 미세 조정된 단일 LLM으로 다시 통합한다.\n' +
      '\n' +
      '### MiX: 혼합 전문가가 되기 위한 별도의 전문가 결합\n' +
      '\n' +
      '우리는 도메인 전문가 모델\\(\\mathcal{M}_{i}\\)을 결합하기 위해 Mixture-of-Experts 접근법을 사용한다. 그러나, 기존의 \\(\\mathcal{M}_{i}\\)에서 최종 출력을 혼합하는 절차를 사용하지 않고, 트랜스포머의 각 층 내에서 MoE를 수행함으로써 보다 세밀한 혼합을 수행한다. 특히, 우리는 도메인 전문가의 다른 피드포워드 하위층을 단일 MoE 하위층으로 결합한다. 만약 \\(\\mathsf{FF}^{l}_{i}(x)\\)이 \\(i\\)번째 도메인 전문가 \\(\\mathcal{M}_{i}\\)의 \\(l\\)번째 레이어에서 피드포워드 서브 레이어라면, \\(l\\) 레이어에서 입력 표현 \\(x\\)을 위한 결합된 MoE 레이어는 계산될 것이다:\n' +
      '\n' +
      '\\[\\mathsf{FF}^{l}_{\\text{MoE}}(x)=\\sum_{i=1}^{N}g_{i}(W_{l}x)\\mathsf{FF}^{l}_{i} (x).\\]\n' +
      '\n' +
      '여기서 \\(W_{l}\\)은 선형 변환이고 \\(g\\)은 라우팅 함수로서 일반적으로 희소 출력을 가지므로 일부 전문가만 스위칭한다. 해당 라우터 출력이 0일 경우 계산(\\mathsf{FF}^{l}_{i}(x)\\)을 생략할 수 있기 때문에 실제 계산(\\mathsf{FF}^{l}_{text{MoE}(x)\\)은 모든 도메인 전문가를 계산하는 것보다 훨씬 더 효율적일 것이다. 그러나, 라우팅 결정들은 토큰에서 토큰으로 변경될 수 있고, 따라서 하나의 입력 시퀀스는 임의의 주어진 토큰에서 소수의 것만이 액세스되는 경우에도 필요한 경우 모든 도메인 전문가 FF 계층을 채용할 수 있다. 실험에서는 달리 명시되지 않는 한, \\(g(W_{l}x)=\\text{SoftMax}(\\text{TopK}(W_{l}x))\\인 Top-k(k=2) 라우팅을 사용한다.\n' +
      '\n' +
      '자기 주의 서브레이어의 경우 단순히 가중치를 평균하여 서로 다른 도메인 전문가를 결합한다. 이 배경의 동기는 자기 주의 층이 피드포워드 층보다 덜 전문화된 영역이라는 가정이다. 우리는 나머지 매개변수(내장 등)에 대해서도 동일한 평균을 낸다.\n' +
      '\n' +
      '우리가 소개하는 유일한 새로운 매개변수는 라우터의 변환 매개변수\\(W_{l}\\)이며, 이는 네트워크의 나머지 부분에 비해 크기가 무시할 수 있다. 그럼에도 불구하고, 이러한 새로운 파라미터들은 미세 조정되어야 하므로, 라우터는 어떤 도메인\\(\\mathsf{FF}_{i}\\)을 사용할지를 선택하는데 있어서 최적의 결정을 내릴 수 있다. 또한, 자기 주의 가중치는 평균화(averageaging)에 의해 구성되고, 최적이 아닐 가능성이 있기 때문에 미세 조정이 도움이 된다. 전반적으로, 전체 시스템은 당혹스러울 정도로 병렬적인 훈련 프레임워크에서 함께 일하는 데 전혀 최적화되지 않았지만, 우리의 가설은 적은 양의 결합된 미세 조정도 큰 개선을 가져올 수 있다는 것이다.\n' +
      '\n' +
      '### Variations\n' +
      '\n' +
      '우리는 또한 우리의 방법의 몇 가지 변형을 실험했다.\n' +
      '\n' +
      '부하 분산 MoE의 일반적인 문제는 라우터에 의해 전혀 활성화되지 않는 죽은 전문가의 출현이다. Top-k와 같은 일반적인 라우팅 방법은 죽은 전문가가 Top-k 선택에 있지 않고, 따라서 훈련 신호를 수신하지 않기 때문에 그러한 상황에서 벗어날 가능성이 낮다. 부하 분산은 전문가가 동등하게 활용되도록 장려하는 추가 손실 항을 추가하여 간단한 솔루션을 제공합니다. 우리는 (Fedus et al., 2022)와 유사한 손실항을 사용한다:\n' +
      '\n' +
      '\\alpha N\\sum_{i}p_{i}\\quad\\text{where }u_{i}=\\frac{1}{|\\mathcal{B}|}\\sum_{x\\in\\mathcal{B}}g_{i}(W_{l}x)\\text{ and }p_{i}=\\frac{1}{|\\mathcal{B}|}\\sum_{x\\in\\mathcal{B}\\text{SoftMax}_{i}(W_{l}x)\\text{\n' +
      '\n' +
      '여기서 \\(\\mathcal{B}\\)는 현재 데이터 배치이고 \\(\\alpha\\)는 하이퍼파라미터이다. 이 손실은 각 계층에서 계산되고 NLL 손실에 추가된다.\n' +
      '\n' +
      'Top-k 라우팅 외에 라우팅 방법 또한 다른 라우팅 방법에 대해 실험한다.\n' +
      '\n' +
      '* Switch: Fedus 등이 제안한 Top-1 라우팅 방식이다(2022).\n' +
      '* 소프트 라우팅: 라우팅 함수 \\(g\\)로 소프트맥스를 사용하므로 훈련과 추론 시 모두 전문가가 활성화된다. 최고의 성능을 제공할 가능성이 높지만 계산량 증가를 희생해야 합니다.\n' +
      '* Sample Top-1: \\(g\\)을 위해 gumbel softmax(Jang et al., 2016)를 사용한다. 훈련 시간에 검벨 소프트맥스로부터 소프트 샘플을 생성하지만 가장 큰 값을 제외하고 모든 값을 제로로 만든다. 그런 다음 다른 전문가 계산을 생략하면서 이 가장 큰 값에 해당하는 한 전문가만 계산한다.\n' +
      '\n' +
      '추론 시간에 우리는 단순하게 하드 샘플링을 한다. 우리는 훈련과 추론 사이의 불일치를 점진적으로 줄이기 위해 훈련 종료 시 온도를 급격한 분포로 가열냉각한다.\n' +
      '\n' +
      '전문가 분할 MoE 레이어의 모듈 수는 각 모듈이 하나의 도메인에 해당하기 때문에 우리가 훈련하는 도메인의 수와 일치한다. 그러나 각 도메인 FF 하위 계층을 여러 청크로 분할함으로써 간단한 방법으로 모듈 수를 늘릴 수 있다. 주어진 \\(N\\) 도메인과 \\(d_{\\text{FF}}\\의 FF 활성화 크기가 주어지면, 우리는 각 FF 층을 \\(d_{\\text{FF}}/C\\의 차원으로 \\(C\\) 덩어리로 분할한다. 결과적으로, 최종 MoE 층은 \\(MC\\) 모듈들을 가질 것이다.\n' +
      '\n' +
      '혼합 전문가 도메인 전문가로부터 MoE 전문가를 일대일 방식으로 직접 초기화하는 대신, 각 MoE 전문가에 모든 도메인을 포함시키려고 노력한다. 이 배경의 동기는 표준 방식으로 훈련된 MoE 전문가들이 도메인 전문화를 보여주지 않고, 오히려 상이한 도메인들에 걸쳐 균일하게 활성화된다는 관찰이다(Jiang et al., 2024). 대조적으로, 우리의 도메인 전문가들은 그들의 훈련 데이터를 통해 특정 도메인에 특화되어 있다. 이 도메인 전문화를 깨기 위해 각 도메인 전문가의 FF 레이어를 \\(N\\) 청크로 분할한 다음 모든 도메인에서 \\(n\\)번째 청크를 병합하여 \\(n\\)번째 MoE 전문가를 구축한다. 이러한 방식으로, 각각의 MoE 전문가는 모든 도메인으로부터 동일한 양의 파라미터를 포함한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리는 Llama-2 사전 훈련에 사용된 설정을 기반으로 실험을 한다(Touvron et al., 2023). 특히, 우리는 Llama-2 7B 모델을 종자 모델로 사용한다.\n' +
      '\n' +
      '######4.1.1 BTX 훈련\n' +
      '\n' +
      '우리는 7B 파라미터를 가진 사전 훈련된 Llama-2(Touvron et al., 2023)를 시드 모델로 사용한다. 시드 모델 Llama-2 7B의 3개의 사본을 만든 후, 우리는 3개의 도메인 전문가를 도출하기 위해 다음 도메인 데이터세트에 대해 계속 트레이닝한다:\n' +
      '\n' +
      '***Math:** Llemma(Azerbayev et al., 2023) 모델 훈련에서 사용된 동일한 데이터 소스 및 혼합물. Llemma와 비교할 수 있도록 동일한 양의 데이터, 즉 총 201B 토큰으로 48k 단계를 훈련한다.\n' +
      '***Code:** CodeLlama 프리트레이닝에서 사용되는 동일한 데이터 소스 및 코드 데이터의 혼합물(Roziere et al., 2023). 코드 전문가 LLM은 수학 전문가와 비교할 수 있도록 총 210B 토큰으로 50k 단계에 대해 훈련된다.\n' +
      '2022년 6월부터 8월 사이에 추출된 위키피디아 문서 ***위키피디아:** 데이터는 하이퍼링크, 주석 및 기타 포맷팅 상용구를 제거하기 위해 전처리되었다. 더 작은 데이터셋이기 때문에 총 42B 토큰을 학습합니다.\n' +
      '\n' +
      '이 세 가지 도메인 전문가만 진행할 수 있지만 원본 종자 LLM도 "일반주의자" 전문가로 포함시켜 일반 지식이 최종 모델로 이전되도록 한다. 따라서 이 네 가지 전문가 모델을 섹션3.2에 설명된 단일 MoE 모델로 혼합한 다음, 이 MoE 모델을 네 가지 전문가(일반 전문가에 대한 원래 Llama-2 7B 사전 훈련 데이터를 포함)를 훈련하는 데 사용되는 모든 데이터 소스에 대해 미세 조정하고 다른 80B 토큰에 대해 훈련한다. 각 도메인의 데이터세트와 도메인에 걸친 세부 샘플링 비율은 부록A에 설명되어 있다. 기본 Top-2 라우팅이 적용된 BTX의 경우 달리 명시되지 않는 한 \\(\\alpha=0.01\\)과 부하균형을 사용한다. 샘플 Top-1 라우팅은 장 등(2016)의 온도 어닐링 스케줄 \\(\\tau\\)=max\\((0.5, -rt)\\)과 \\(r=1e-4\\)을 사용하며, 여기서 \\(t\\)은 훈련 단계의 수이다. 첫 번째 레이어만 소프트 라우팅을 사용했습니다. 샘플 Top-1 트레이닝이 Top-2보다 더 효율적이기 때문에, 동일한 계산 예산으로 160B 토큰을 트레이닝할 수 있다.\n' +
      '\n' +
      '#### 4.1.2 Baselines\n' +
      '\n' +
      '다음 기준선과 비교한다: **Llama-2:** 시드 모델로 사용하는 원래 Llama-2 7B 및 Llama-2 13B와 비교한다.\n' +
      '* **Dense:** 다른 도메인 데이터 세트에서 별도의 LLM을 훈련하는 대신 조밀한 기준선은 모든 데이터로 시드 LLM을 계속 훈련한다. 우리는 BTX와 정확히 동일한 훈련 데이터를 사용하며, 전문가 훈련 단계에서 사용되는 새로운 도메인 특정 데이터에 대한 첫 번째 훈련, MoE 미세 조정 단계에서 라마-2 사전 훈련 데이터를 포함하는 동일한 데이터 혼합을 사용한다. 우리는 이것을 비교 _data-matching_(DM)이라고 부른다.\n' +
      '* **Sparse 업사이클링:** 이 베이스라인(Komatsuzaki et al., 2022)은 전문가로서 피드포워드 모듈의 4개의 동일한 사본을 만들어 시드 모델로부터 MoE 모델을 초기화한다. 우리는 랜덤하게 초기화된 \\(W_{i}\\) 파라미터를 갖는 Top-2 라우터를 사용한다. BTX에서 사용되는 것과 동일한 데이터와 조밀한 베이스라인으로 베이스라인과 일치하는 데이터를 훈련하는 것 외에도, 우리는 훈련 전반에 걸쳐 MoE 미세조정 데이터 혼합물을 사용하여 동일한 양의 GPU-일, 즉 계산-매칭(CM)으로 희소 업사이클링 베이스라인을 훈련한다. 이는 당혹스러울 정도로 병렬적인 전문가 교육을 포함하지 않는 BTX의 특수한 사례에 해당한다.\n' +
      '***Branch-Train-Merge(BTM):** 이 베이스라인(Li et al., 2022)은 BTX(원본 시드 모델을 포함)와 동일한 전문가 LLM을 사용하지만 MoE 모델을 구축하지 않고 직접 사용한다. 주어진 컨텍스트(입력)에 대해, 컨텍스트와 전문가의 훈련 데이터 사이의 유사성을 기반으로 Top-k 전문가 LLM을 선정한다. Gururangan 등(2023)에서 사용된 효율적인 추론 방법에 따라, 컨텍스트와 전문가의 훈련 데이터는 모두 tf-idf를 통해 임베딩된다. 각 전문가의 평균 tf-idf 임베딩에 대한 코사인 유사도를 기반으로 Top-k 전문가를 선정한다.\n' +
      '**CodeLlama 7B:** 코드 데이터에 대한 동일한 시드 모델 Llama-2 7B의 지속적인 트레이닝에 의해 코드를 전문으로 하는 언어 모델(Roziere et al., 2023). 또한 긴 컨텍스트 및 주입과 같은 다른 기능도 있습니다.\n' +
      '***Llema 7B:** 수학 데이터에 대한 CodeLlama 7B의 지속적인 훈련에 의해 수학을 전문으로 하는 언어 모델(Azerbayev et al., 2023).\n' +
      '\n' +
      '기준선, 전문가 모델 및 MoE 모델의 훈련에는 동일한 최적화 하이퍼파라미터를 사용한다. 가중치 감쇠가 0.1인 AdamW 최적화기를 사용하였으며, 100단계의 워밍업으로 \\(1e-4\\)의 피크까지 학습율을 어닐링하고 코사인 스케줄로 10%의 피크까지 감쇠시켰다. 시퀀스 길이가 4096인 4M 토큰의 배치 크기를 사용합니다.\n' +
      '\n' +
      '#### 4.1.3 Evaluation\n' +
      '\n' +
      '평가를 위해 서로 다른 기술을 테스트하는 여러 벤치마크에서 제로 및 소수 샷 성능을 사용합니다.\n' +
      '\n' +
      '*Math: 수학 추론을 위한 GSM8K(8shot)(Cobbe et al., 2021) 및 MATH(4shot)(Hendrycks et al., 2021)에 대한 평균 성능을 보고한다.\n' +
      '* Code: Code 생성을 위한 HumanEval(0 shot)(Chen et al., 2021)과 MBPP(3 shot)(Austin et al., 2021)의 평균 성능을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**Math**} & \\multicolumn{2}{c}{**Code**} & \\multicolumn{2}{c}{**General knowledge**} \\\\ \\cline{2-9}  & **GSM8K** & **MATH** & **Human** & **MBPP** & **Natural** & **Trivia** & **MMLU** \\\\  & & & **Eval** & & **Questions** & **QA** & \\\\ \\hline Llama-2 7B & 14.7 & 2.5 & 12.8 & 20.8 & 16.4 & **58.5** & 46.1 \\\\ Math expert & **39.5** & **18.8** & 25.0 & 33.6 & 14.4 & 37.1 & **52.0** \\\\ Code expert & 12.0 & 4.0 & **31.7** & **40.2** & 11.5 & 29.9 & 39.6 \\\\ Wikipedia expert & 11.7 & 3.1 & 11.0 & 15.2 & **21.8** & 57.2 & 43.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 시드 모델 Llama-2 7B와 비교하여 대표 태스크에 대한 개별 도메인 전문가 LLM 성능. 예상대로 코드와 수학 전문가가 해당 도메인 작업을 탁월하게 수행합니다. 위키피디아 전문가는 자연 질문에서 더 잘 수행하지만, 수학 전문가는 MMLU에서 가장 좋은 점수를 받는다. 이는 MMLU가 많은 수학 과목을 포함하고 있고 수학 훈련이 이 과제에 도움이 되는 것으로 나타나 있기 때문일 수 있다(Shao et al., 2024).\n' +
      '\n' +
      '* World Knowledge: Natural Questions (5 shot)(Kwiatkowski et al., 2019) and TriviaQA (5 shot)(Joshi et al., 2017)의 평균 성능을 보고한다.\n' +
      '* 추론: ARC-Easy and ARC-Challenge (Clark et al., 2018), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020) 및 WinoGrande (Sakaguchi et al., 2021)의 평균 0-shot 성능을 보고한다.\n' +
      '* General: 다중 도메인을 포괄하는 MMLU(5 shot)(Hendrycks et al., 2021)에 대한 성능을 보고한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '######4.2.1 전체 성능\n' +
      '\n' +
      '도메인 전문가는 각각의 작업에 탁월합니다. 먼저 전문가 LLM이 특정 도메인에 어떻게 전문화하는지 분석합니다. 결과는 표 1에 요약되어 있으며, 예상대로 개별 전문가 LLM은 각 도메인에서 가장 우수한 성능을 달성하며, 여기서 수학 및 코드 도메인은 특히 큰 개선을 보인다. 게다가, 몇 가지 흥미로운 관찰이 있다. 우리는 수학 전문가 교육이 코드 성능도 향상시켰음을 알 수 있으며, 이는 이러한 영역의 밀접한 관계를 나타낸다. 그러나 이러한 단일 도메인 연속 훈련은 다른 도메인의 일부 작업에서 상당한 성능 저하와 함께 치명적인 망각으로 고통받는다. 예를 들어, 수학 및 코드 전문가는 시드 모델보다 트리비아QA에서 훨씬 더 나쁩니다.\n' +
      '\n' +
      'BTX는 전문가가 전문화하는 모든 작업을 개선합니다.표 2와 그림 2(오른쪽)는 여러 도메인에 걸쳐 집계된 성능을 보여줍니다. 보다 상세한 작업당 결과는 부록의 표 8에 보고되어 있다. 시드 모델 Llama-2 7B와 비교하여, BTX 모델(샘플 Top-1 및 Top-2 모두 상이한 수의 활성 파라미터에 대응함)은 상식 추론과 같은 다른 태스크에 회귀하지 않고 수학, 코딩 및 세계 지식과 같은 모든 전문가 도메인에서 개선된다. Top-2 전문가와 함께 BTX(우리의 기본값)는 또한 수학 및 코딩 도메인에서 전문 모델 Llama 7B 및 CodeLama 7B의 최상의 성능에 접근하는 반면, 세계 지식 및 상식 추론과 같은 전문성이 아닌 도메인에서는 이러한 모델보다 획기적으로 개선한다. 밀집 및 희소 업사이클링과 같은 지속적인 사전 훈련을 위한 대안적인 데이터 매칭(DM) 방법에 비해 BTX는 수학 및 코딩 도메인에서 작은 갭으로 평균적으로 더 나은 성능을 달성한다. BTX는 평균적으로 큰 마진만큼 BTM을 능가하며, 이는 토큰-레벨 라우팅을 학습하기 위한 MoE 미세조정이 유익함을 나타낸다. 전반적으로, BTX는 다중 작업 학습으로부터의 작업 간섭에 강건한 지속적인 사전 훈련을 위한 더 계산적인 효율적인 방법임을 입증한다. BTX는 또한 Llama-2 13B가 훨씬 더 많은 트레이닝 컴퓨트를 사용하고 약간 더 활동적인 파라미터를 가지고 있음에도 불구하고, 추론을 제외한 모든 태스크에서 Llama-2 13B를 능가한다.\n' +
      '\n' +
      '우리는 컴퓨트 매칭(CM) 시나리오에서 BTX와 희박한 업사이클링 기준선을 추가로 비교한다. 둘 다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & **Math** & **Code** & **Knowledge** & **Reasoning** & **MMLU** & **Average** \\\\ \\hline \\hline _Specialized LLMs_ & & & & & & \\\\ CodeLlama 7B & 8.1 & 36.3 & 22.2 & 56.6 & 38.6 & 37.9 \\\\ Llemma 7B & 28.0 & 33.5 & 17.2 & 38.8 & 33.5 & 32.1 \\\\ \\hline _Generalist LLMs_ & & & & & & \\\\ Llama-2 7B & 8.6 & 16.8 & 37.4 & 63.3 & 46.1 & 40.7 \\\\ Llama-2 13B & 16.3 & 24.5 & 40.0 & **66.1** & 52.8 & 45.4 \\\\ Dense (DM) & 18.3 & 25.8 & 39.6 & 63.3 & 49.8 & 44.5 \\\\ Sparse upcycling (DM), Top-2 & **28.1** & 34.7 & 34.0 & 62.3 & 51.1 & 46.3 \\\\ BTM, Top-1 & 21.3 & 36.4 & 26.5 & 61.0 & 44.3 & 43.1 \\\\ BTM, Top-2 & 21.5 & **36.6** & 26.9 & 61.2 & 44.3 & 43.4 \\\\ BTX, Sample Top-1 & 26.4 & 31.5 & 40.1 & 63.7 & **53.2** & 47.3 \\\\ BTX, Top-2 & 27.4 & 34.0 & **41.0** & 63.5 & 52.5 & **47.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 일반주의 및 전문 사전 훈련 모델 모두를 포함하는 다양한 기준선에 대해 비교된 BTX의 집계 성능은 인기 있는 벤치마크에 걸쳐 집계된 다양한 능력에 대해 테스트되었다. 밀집, 희박 업사이클링, BTM 및 BTX는 BTM이 미세조정 단계를 갖지 않는 것을 제외하고 정확히 동일한 양과 데이터의 혼합물에 대해 트레이닝된다.\n' +
      '\n' +
      'MoE 단계에서 동일한 데이터 혼합물에서 훈련하지만 MoE 훈련에 사용된 계산의 백분율 측면에서 다르다. 희소 사이클링이 BTX 뒤에 근접하게 수행되는 반면, 전문가의 병렬 트레이닝은 표 3과 같이 BTX의 트레이닝 처리량을 증가시킨다. 그 결과, BTX는 동일한 트레이닝 계산 예산을 주어진 순수한 MoE보다 \\(2\\times\\) 이상의 데이터로 트레이닝할 수 있고, 모든 도메인에서 약간 더 높은 평균 성능을 달성한다.\n' +
      '\n' +
      '######4.2.2 더 나은 계산 성능 절충\n' +
      '\n' +
      '우리는 그림 2(왼쪽)의 계산 효율 측면에서 BTX를 기준선과 비교한다. X축은 GPU 일에서 측정된 시드 모델부터 시작하는 총 학습 계산량을 나타내며, 이는 도메인 전문가 훈련과 MoE 모델의 미세 조정을 포함한다. Y축은 표 2에 보고된 전체 성능을 측정한다.\n' +
      '\n' +
      'MoE 훈련 단계가 사전 훈련에서 총 훈련 예산의 일부를 사용함에도 불구하고(예를 들어, Llama-2 사전 훈련은 2T 토큰을 사용함), BTX는 조밀한 모델의 다중 작업 학습 및 Branch-Train-Merge와 같은 대안적인 연속 사전 훈련 접근법에 비해 일반적인 능력에 대한 가파른 개선을 가져온다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c c c c c} \\hline \\hline  & MoE & Training & Total compute & \\#tokens & Math & Code & Knowledge & Reasoning & MMLU & Average \\\\  & compute & time (days) & (GPU-days) & (B) & & & & & \\\\ \\hline BTX & 23\\% & 7.8 & 926.1 & 533 & 27.4 & 34.0 & 41.0 & 63.5 & 52.5 & 47.9 \\\\ Sparse upcycling (CM) & 100\\% & 7.9 & 1007.1 & 252 & 28.2 & 30.7 & 41.3 & 62.9 & 52.1 & 47.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 계산의 100%가 MoE 훈련에 소비된다는 첫 번째 열에 나타난 바와 같이 전문가 훈련 단계가 없는 BTX의 특수한 경우인 BTX와 Sparse upcycling with compute-matching(CM)의 비교. 또한 총 훈련 시간, 계산 및 훈련 토큰 수를 보고한다. 평균뿐만 아니라 개별 도메인에 대한 두 성능을 비교하면 BTX가 더 높은 처리량 외에도 더 균형 잡힌 성능을 가지고 있음을 알 수 있다.\n' +
      '\n' +
      '도 2: **Left:** 다양한 베이스라인들과 비교하여 BTX의 평균 성능 대 훈련 예산, 서클 사이즈로 표시된 추론 시간에서의 상이한 활성 파라미터들. Llama-2 13B를 제외한 모든 모델은 섹션 4.1.1에 기술된 데이터 세트를 사용하여 Llama-2 7B로부터 트레이닝된다. X-축은 GPU days1에서 측정된 시드 모델로부터 시작하는 총 트레이닝 컴퓨트를 나타내고, Y-축은 모든 태스크에 대한 평균 점수(표 2에서 계산된 바와 같이)이다. BTX 모델은 라마-2 13B뿐만 아니라 동일한 종자 모델에서 시작된 기준선을 능가한다. **Right:** 점수가 가장 높은 영역으로 나누어지는 서로 다른 도메인에 대한 정규화된 성능 우리는 시드 모델 Llama-2 7B와 비교하여 코드(특성화된 모델과 일치하는) 및 수학 과제에서 BTX에 대한 큰 개선을 보고 있으며, 심지어 Llama-2 13B 모델보다 우수하다.\n' +
      '\n' +
      '희소 업사이클링보다 효율적이다.BTX의 특별한 경우, 전문가 교육을 받지 않은 희소 업사이클링은 동일하거나 더 큰 계산 예산을 감안할 때 조밀 및 BTM보다 우수하지만 BTX는 그렇지 않다. BTX의 계산 효율 향상은 MoE 미세 조정 전에 당혹스러울 정도로 병렬적인 전문가의 훈련에서 비롯된다.\n' +
      '\n' +
      '활성 매개변수 수(2(왼쪽)에서 원 크기로 표시됨) 측면에서 MoE 모델은 Llama-2 13B 모델과 유사하다. BTX는 Llama-2 13B에 비해 추가 훈련 계산의 절반 미만을 사용하지만, 전문가 도메인(수학, 코드 및 지식)에서 향상된 성능을 입증하고 더 나은 전체 성능을 달성한다. 이는 BTX의 훈련이 사전 훈련 전반에 걸쳐 동일한 훈련 프로토콜을 사용하는 것보다 사전 훈련의 후기 단계에 더 효과적임을 나타낸다.\n' +
      '\n' +
      '### 요약 및 분석\n' +
      '\n' +
      'BTX 트레이닝의 블립 4.3.1\n' +
      '\n' +
      '먼저, 피니튜닝의 양에 따라 활성 파라미터의 양을 변화시키면서 서로 다른 라우팅 방법을 비교한다. 공정한 비교를 위해, 부하 분산은 그들 중 어느 것에도 사용되지 않는다. 결과는 표 4와 같다. 스위치 라우팅의 경우 용량 팩터를 1.5(하드 리미트 후 라우팅된 토큰이 삭제됨)로 설정했다. 우리는 스위치 라우터가 평균 성능에서 동등하다는 것을 발견했다. 소프트 라우팅은 가장 좋은 성능을 보이지만 희소성이 부족하고 활성 매개 변수의 수가 가장 많기 때문에 예상됩니다. 전반적으로 Top-2 라우팅은 성능과 효율성 사이에서 좋은 균형을 제공합니다.\n' +
      '\n' +
      '또한, BTX의 추가적인 설계 선택사항을 삭제하고, 그 결과를 표 5에 정리하였다. 로드 밸런싱이 없는 MoE 트레이닝은 코딩 태스크(HumanEval)에서 더 나쁜 성능을 보이지만, 수학(GSM8k) 정확도가 더 높다는 것을 발견했다. 다음 섹션의 라우팅 분석은 이러한 절충점에 대한 더 많은 통찰력을 제공할 것입니다. 다음으로 각 전문가로부터 초기화된 피드포워드 모듈을 동결하고 나머지 MoE 모델만 교육하면 모든 작업에 걸쳐 성능에 거의 영향을 미치지 않는다. 이는 개별 전문가들이 브랜치-트레인 단계에서 이미 충분한 도메인 지식을 얻었고, 믹스(MoE finetuning) 단계는 주로 믹스(MoE finetuning) 단계를 훈련했음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Routing method**} & \\multicolumn{2}{c}{**Active parameters (B)**} & \\multicolumn{1}{c}{**MoE Finetune**} & \\multicolumn{1}{c}{**Average**} \\\\ \\cline{2-5}  & **Training** & **Inference** & **tokens (B)** & **score** \\\\ \\hline \\hline Switch Top-1 & 6.7 & 6.7 & 10 & 24.7 \\\\ Sample Top-1 & 6.7 & 6.7 & 10 & 33.0 \\\\ Top-2 & 11.1 & 11.1 & 10 & 34.6 \\\\ Soft routing & 19.7 & 19.7 & 10 & 35.8 \\\\ \\hline Sample Top-1 & 6.7 & 6.7 & 40 & 35.3 \\\\ Top-2 & 11.1 & 11.1 & 40 & 35.9 \\\\ Soft routing & 19.7 & 19.7 & 40 & 37.3 \\\\ \\hline Sample Top-1 & 6.7 & 6.7 & 160 & 36.9 \\\\ Top-2 & 11.1 & 11.1 & 80 & 37.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: BTX 트레이닝 동안 상이한 라우팅 방법들에 대한 어블레이션들. 평균 점수는 GSM8K, HumanEval, Natural Questions, ARC Challenge 및 MMLU를 포함한 대표적인 태스크에 대한 성능을 기반으로 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & **GSM8K** & **Human** & **Natural** & **ARC** & **MMLU** & **Average** \\\\  & & **Eval** & **Questions** & **Challenge** & & **Score** \\\\ \\hline \\hline BTX & 29.8 & 27.4 & 23.0 & 43.4 & 50.0 & 34.7 \\\\ \\hline no load-balancing (LB) & 34.6 & 19.5 & 23.2 & 44.4 & 51.6 & 34.6 \\\\ no LB \\& freeze experts & 34.8 & 18.3 & 24.1 & 44.9 & 51.4 & 34.7 \\\\ blending experts & 13.9 & 17.1 & 9.9 & 34.1 & 36.2 & 22.2 \\\\ split experts, top-2 of 8 & 22.0 & 20.1 & 16.8 & 39.1 & 41.8 & 28.0 \\\\ split experts, top-4 of 8 & 29.6 & 26.8 & 22.9 & 44.0 & 49.4 & 34.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 서로 다른 BTX 훈련 전략에 대한 설명. 모든 변형은 동일한 전문가로부터 초기화되고 MoE 피네튜닝 동안 총 10B 토큰에 대해 훈련된다.\n' +
      '\n' +
      '자기 주의 및 라우터 변환에서 평균 가중치와 같은 다른 매개변수는 \\(W_{i}\\)이다.\n' +
      '\n' +
      '또한 섹션3.3에 설명된 혼합 및 분할 기술을 테스트합니다. 전문가가 혼합될 때 모든 작업에 대한 성능이 저하되어 도메인 FF 레이어가 이러한 방식으로 혼합될 수 없음을 시사한다. MoE 계층에서 8개의 모듈을 얻기 위해 각 도메인 FF를 \\(C=2\\) 청크로 분할하는 것은 또한 Top-4 라우팅이 활성 파라미터 수를 맞추기 위해 사용되더라도 성능을 향상시키지 않는다.\n' +
      '\n' +
      '4.3.2 라우팅 분석\n' +
      '\n' +
      'BTX의 성능을 심층적으로 이해하기 위해 다운스트림 태스크에 대한 모델 평가를 실행하고 전문가 간의 라우팅 결정을 검토한다. 결과는 그림 3에 요약되어 있으며 부록C에서 다양한 BTX 설정에 대한 자세한 절제 결과도 보고한다. 로드 밸런싱을 적용한 Top-2 라우팅은 다른 라우팅 방식에 비해 전문가 간 부하 분산을 보다 균일하게 보장한다. 토큰 확률 분포를 분석하면, 부하 분산을 통해 모든 전문가에 걸쳐 낮은 확률 점수로의 이동을 관찰하며, 특히 모델의 최종 계층에 더 가깝기 때문에 공정한 라우팅에 기여한다. 흥미롭게도 부하 균형이 없는 모든 모델은 수학 전문가에 크게 의존하며 다른 전문가, 특히 코드 전문가의 전반적인 기여도가 낮다. 죽은 코드 전문가가 훈련에 도입된 로드 밸런싱을 통해 "다시 살아난다"고 합니다. 사실, 그것은 가시화될 뿐만 아니라 수학 및 코드 영역의 지배적인 전문가가 된다.\n' +
      '\n' +
      '로드 밸런싱을 갖는 Top-2에 대한 라우팅 결정의 예는 표 6에서 찾을 수 있다. 수학 도메인 태스크 전반에 걸쳐, 토큰은 종종 코드 및 Llama-2 7B 전문가에게 라우팅된다. 보다 상세한 토큰 분포(부록C, 도 6)를 살펴보면, GSM8K 태스크는 Code와 Llama-2 전문가를 선호하고, MATH 태스크는 In-domain Math 전문가에 더 의존함을 알 수 있다. GSM8K 데이터셋이 상식적 지식과 기본적인 산술 연산을 필요로 하는 초등학교 수학 문제로 구성되어 있기 때문에 이러한 현상이 발생한다고 가정한다. 코드 및 세계 지식 작업은 대부분 도메인 코드 및 위키피디아 전문가에게 각각 전달된다. 앞서 섹션4.3.1에서 관찰된 바와 같이 로드 밸런싱이 도입되면 코딩 작업에는 개선이 있지만 수학 작업에는 저하가 있는데, 이는 도메인 전문가 라우팅의 이러한 변화로 설명할 수 있다. 대조적으로 추론 작업은 유사한 행동을 나타내며 동등하게 의존한다.\n' +
      '\n' +
      '도 3: 다양한 계층에서의 토큰들의 BTX 라우팅 결정들을 상이한 다운스트림 태스크들에 대한 상이한 전문가들(위키, 수학, 코드, LLMaMa-2 7B)에 라우팅한다. 작업은 코드(Human Eval, MBPP), 수학(GSM8K, MATH), 세계지식(Natural Questions, TriviaQA), 추론(ARC-Easy, ARC-Challenge, SIQA, PIQA, WinoGrande) 등 도메인별로 집계된다. 부하 분산(top)을 적용한 Top-2 라우팅이 부하 분산(bottom)을 적용하지 않은 Top-2에 비해 전문가 간 부하 분산을 보다 균일하게 보장함을 관찰한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'BTX는 BTM과 비교하여 결합된 전문가를 세분화하는 접근법을 제공하며, 이는 명령어 세분화 또는 RLHF 절차에서 직접 적용될 수 있다. 그러나 우리는 본 논문에서 사전 훈련 단계에 중점을 두었기 때문에 향후 작업을 위해 남겨둔다.\n' +
      '\n' +
      'MoE의 전문가들이 특정 영역을 전문으로 하는 것이 더 나은지 아닌지에 대한 질문은 추가 조사가 필요한 흥미로운 질문이다. 우리의 접근법은 전문가들을 특정 도메인에 명시적으로 묶었지만, 그러한 전문화는 MoE 훈련 동안 자연스럽게 나타나지 않는 것으로 보인다(Jiang et al., 2024). 우리는 일부 전문가가 해당 도메인 작업에서 더 많이 사용되는 것을 관찰했으며, 이는 MoE 미세 조정 후에도 도메인 전문화가 부분적으로 남아 있음을 보여준다.\n' +
      '\n' +
      '우리는 BTX를 전문가 훈련에 할당된 100% 계산과 MoE 피네튜닝에 할당된 0%의 BTM과 전문가 훈련에 할당된 0% 계산과 MoE 피네튜닝에 할당된 100%의 희소 업사이클링이라는 두 가지 특별한 변형과 비교했다. 향후 작업은 전문가 훈련과 MoE 훈련 사이의 계산 할당 비율의 철저한 스윕을 수행할 수 있다. 또한 균일한 샘플링 이외의 MoE 미세 조정을 위해 다른 데이터 혼합물로 실험을 수행하지 않았다.\n' +
      '\n' +
      '## 7 Acknowledgements\n' +
      '\n' +
      '우리는 마가렛 리, 쿠샬 티루말라, 루크 제틀모이어, 아르티도로 파뇽, 수친 구루랑간, 마이크 루이스와 에밀리 디난, 그리고 훈련 시행에 도움을 준 앤드류 코헨과 아룬 바부에게 감사를 표한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aljundi et al. (2016) Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. 전문가 게이트: 전문가 네트워크를 이용한 평생 학습. _ 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7120-7129, 2016. [https://api.semanticscholar.org/CorpusID:914027](https://api.semanticscholar.org/CorpusID:914027)\n' +
      '* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. 르와 찰스 서튼 대용량 언어 모델을 사용한 프로그램 합성 ArXiv_, abs/2108.07732, 2021. [https://api.semanticscholar.org/CorpusID:237142385](https://api.semanticscholar.org/CorpusID:237142385)\n' +
      '* Awasthi and Sarawagi (2019) Abhijeet Awasthi and Sunita Sarawagi. 신경망을 이용한 지속적인 학습: 리뷰. _Proceedings of the ACM India Joint International Conference on Data Science and Management of Data_, pages 362-365, 2019.\n' +
      '* Azerbayev et al. (2023) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 션 웰렉 Llemma: 수학을 위한 개방형 언어 모델. _ ArXiv_, abs/2310.10631, 2023. [https://api.semanticscholar.org/CorpusID:264172303](https://api.semanticscholar.org/CorpusID:264172303)\n' +
      '* Bisk et al. (2020) 요나탄 비스크, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439, 2020.\n' +
      '* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. 지글러, 제프 우, 클레멘스 윈터, 크리스토퍼 헤세, 마크 첸, 에릭 시글러, 마테우스 리트윈, 스콧 그레이, 벤자민 체스, 잭 클락, 크리스토퍼 버너, 샘 맥캔들시, 알렉 래드포드, 일리아 서츠키버, 다리오 아모데이. 언어 모델은 소수의 학습자들입니다. _ ArXiv_, abs/2005.14165, 2020. [https://api.semanticscholar.org/CorpusID:218971783](https://api.semanticscholar.org/CorpusID:218971783)\n' +
      '* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. 커밍스, 마티아스 플라퍼트, 포티오스 찬티스, 엘리자베스 반스, 아리엘 허버트 보스, 윌리엄 H. 거스, 알렉스 니콜, 이고르 바부슈킨, 수키르 발라지, 샨타누 자인, 앤드류 카, 얀 라이케, 조슈아 아치암, 베단트 미스라, 에반 모리카와, 알렉 래드포드, 매튜 M. 나이트, 마일즈 브런디지, 미라 무라티, 케이티 메이어, 피터 웰린더, 밥 맥그루, 다리오 아모디, 샘 맥캔디스, 일리아 서츠키버, 워치치 자렘바 등이다. 코드에서 훈련된 대규모 언어 모델 평가. _ ArXiv_, abs/2107.03374, 2021. [https://api.semanticscholar.org/CorpusID:235755472](https://api.semanticscholar.org/CorpusID:235755472)\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 답하는 걸 해결했다고 생각해? AI2 추론 문제인 ARC를 시도해 보세요. _ arXiv preprint arXiv:1803.05457_, 2018.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 수학 단어 문제를 해결하기 위한 검증자 훈련 arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Dai et al. (2024) Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang. 딥시크모에: 혼합 전문가 언어 모델의 궁극적인 전문가 전문화를 지향합니다. _ ArXiv_, abs/2401.06066, 2024. [https://api.semanticscholar.org/CorpusID:266933338](https://api.semanticscholar.org/CorpusID:266933338)\n' +
      '* Douillard et al. (2023) Arthur Douillard, Qixuang Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc\'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. Diloco: 언어 모델의 분산 저통신 훈련. _ ArXiv_, abs/2311.08105, 2023. [https://api.semanticscholar.org/CorpusID:265158012](https://api.semanticscholar.org/CorpusID:265158012)\n' +
      '* Fedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. 변압기: 간단하고 효율적인 희소성으로 1조 파라미터 모델로 스케일링 The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.\n' +
      '*팀(2023) 쌍둥이자리팀. 쌍둥이자리: 매우 유능한 멀티모달 모델 가족 ArXiv:2312.11805_, 2023. Team et al. (2021) Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others.\n' +
      '\n' +
      '수신 구루랑간, 마이클 루이스, 아리 홀츠만, 노아 A. 스미스, 루크 제틀모이어. Demix 도면층: 모듈식 언어 모델링을 위한 도메인을 분리합니다. _North American Chapter of the Association for Computational Linguistics_, 2021. [https://api.semanticscholar.org/CorpusID:236976189](https://api.semanticscholar.org/CorpusID:236976189)\n' +
      '* Gururangan et al. [2023] Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Scaling expert language models with unsupervised domain discovery. _arXiv preprint arXiv:2303.14177_, 2023.\n' +
      '* Hendrycks et al. [2021a] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해도를 측정하는 중입니다. _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021a. [https://openreview.net/forum?id=47KBjm13GmQ] (https://openreview.net/forum?id=47KBjm13GmQ).\n' +
      '* Hendrycks et al. [2021b] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. 상기 수학 데이터셋으로 수학 문제 풀이를 측정하는 단계; _ ArXiv_, abs/2103.03874, 2021b. [https://api.semanticscholar.org/CorpusID:232134851] (https://api.semanticscholar.org/CorpusID:232134851).\n' +
      '* Jacobs et al. [1991] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3:79-87, 1991. [https://api.semanticscholar.org/CorpusID:572361](https://api.semanticscholar.org/CorpusID:572361).\n' +
      '* Jang et al. [2016] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.\n' +
      '* Jiang et al. [2022] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts. _ArXiv_, abs/2401.04088, 2024. [https://api.semanticscholar.org/CorpusID:266844877](https://api.semanticscholar.org/CorpusID:266844877).\n' +
      '* Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _ArXiv_, abs/1705.03551, 2017. [https://api.semanticscholar.org/CorpusID:26501419](https://api.semanticscholar.org/CorpusID:26501419).\n' +
      '* Komatsuzaki et al. [2022] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. _ArXiv_, abs/2212.05055, 2022. [https://api.semanticscholar.org/CorpusID:254535822](https://api.semanticscholar.org/CorpusID:254535822).\n' +
      '* Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. _Transactions of the Association of Computational Linguistics_, 2019.\n' +
      '* Lange et al. [2019] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44:3366-3385, 2019. [https://api.semanticscholar.org/CorpusID:218889912](https://api.semanticscholar.org/CorpusID:218889912).\n' +
      '* Lewis et al. [2021] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In _International Conference on Machine Learning_, 2021. [https://api.semanticscholar.org/CorpusID:232428341](https://api.semanticscholar.org/CorpusID:232428341).\n' +
      '* Li et al. [2022a] Margaret Li, 수친 Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. Branch-train-merge: 전문가 언어 모델의 당황스러울 정도로 병렬적인 훈련. _ ArXiv_, abs/2208.03306, 2022a. [https://api.semanticscholar.org/CorpusID:251371375] (https://api.semanticscholar.org/CorpusID:251371375).\n' +
      '*1097, 2022b. [https://api.semanticscholar.org/CorpusID:246527904] (https://api.semanticscholar.org/CorpusID:246527904).\n' +
      '* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. _ArXiv_, abs/2203.02155, 2022. [https://api.semanticscholar.org/CorpusID:246426909](https://api.semanticscholar.org/CorpusID:246426909).\n' +
      '* Ouyang et al. [2022]Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. 큰 희소 모델의 해시 도면층 _Neural Information Processing Systems_, 2021. [https://api.semanticscholar.org/CorpusID:235367626](https://api.semanticscholar.org/CorpusID:235367626)\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom and Gabriel Synnaeve. 코드 라마: 코드에 대한 기초 모델을 엽니다. _ ArXiv_, abs/2308.12950, 2023. [https://api.semanticscholar.org/CorpusID:261100919](https://api.semanticscholar.org/CorpusID:261100919)\n' +
      '* Rusu et al. (2016) Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 프로그레시브 신경망 ArXiv_, abs/1606.04671, 2016. [https://api.semanticscholar.org/CorpusID:15350923](https://api.semanticscholar.org/CorpusID:15350923)\n' +
      '* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 적대적 winograd schema challenge at scale. _ ACM_, 64(9):99-106, 2021의 통신.\n' +
      '* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: 사회적 상호작용에 대한 상식적 추론. _ ArXiv preprint arXiv:1904.09728_, 2019.\n' +
      '* Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, R. X. Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. 딥세스kmath: 열린 언어 모델에서 수학적 추론의 한계를 푸는 것. _ ArXiv_, abs/2402.03300, 2024. [https://api.semanticscholar.org/CorpusID:267412607](https://api.semanticscholar.org/CorpusID:267412607)\n' +
      '* Shazeer et al.(2017) Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. 르, 제프리 E. 힌튼 그리고 제프 딘 엄청나게 큰 신경망: 희박하게 게이팅된 혼합 전문가 계층. _ ArXiv_, abs/1701.06538, 2017. [https://api.semanticscholar.org/CorpusID:12462234](https://api.semanticscholar.org/CorpusID:12462234)\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Bynthia Kardas, Vedan Hartshorn, Saghar Hosseini, Rui Hungbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.\n' +
      '* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt. 모델 스프: 다수의 미세 조정 모델의 가중치를 평균하는 것은 추론 시간을 증가시키지 않으면서 정확도를 향상시킨다. _ ArXiv_, abs/2203.05482, 2022. [https://api.semanticscholar.org/CorpusID:247362886](https://api.semanticscholar.org/CorpusID:247362886)\n' +
      '* Xue et al. (2024) Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. 오픈모: 개방형 혼합 전문가 언어 모델에 대한 초기 노력. _ arXiv preprint arXiv:2402.01739_, 2024.\n' +
      '* Zhang et al. (2015) Sixin Zhang, Anna E Choromanska, and Yann LeCun. 탄력적인 평균 sgd를 갖는 딥 러닝. C. Cortes, N. 로렌스, 디.이 Sugiyama와 R. Garnett, Editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. [https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf]).\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. 디아브, 시안 리, 시 빅토리아 린, 토도르 미하일로프, 마일 오트, 샘 슬레이퍼, 커트 슈스터, 대니얼 시미그, 푸닛 싱 쿠라, 안잘리 스리드하르, 톈루 왕, 루크 제틀모이어 등이다. Opt: Open pre-trained transformer language models. _ ArXiv_, abs/2205.01068, 2022. [https://api.semanticscholar.org/CorpusID:248496292](https://api.semanticscholar.org/CorpusID:248496292)\n' +
      '* Zhao et al. (2024) Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond English: 언어능력 전이에 관한 실증적 연구. _ arXiv preprint arXiv:2401.01055_, 2024.\n' +
      '\n' +
      '## 부록 데이터 혼합물\n' +
      '\n' +
      '각 도메인 전문가 훈련에 사용된 정확한 데이터 혼합 비율은 표 7과 같다. MoE 모델을 미세 조정하기 위해 수학 전문가, 코드 전문가, 위키피디아 전문가 및 원래 Llama-2 7B를 훈련하는 데 사용된 데이터 세트를 확률 30.16%, 40.31%, 10.30% 및 19.23%로 샘플링했다.\n' +
      '\n' +
      '## 부록 B 평가\n' +
      '\n' +
      'Touvron et al. (2023) 및 Roziere et al. (2023)에서 사용되는 것과 동일한 평가 메트릭을 사용한다 : 코드 태스크(HumanEval and MBPP)에 대해서는 pass@1, 수학 태스크(GSM8k and MATH)에 대해서는 Pass@1, 지식 태스크(Natural Questions and TriviaQA)에 대해서는 정확한 매칭을 보고하고, MMLU와 ARC에 대해서는 정확도를 보고한다. 우리는 모든 세대를 위해 탐욕스러운 디코딩을 사용한다. 모든 과제에 대한 자세한 결과는 <표 8>에 보고되어 있다.\n' +
      '\n' +
      '## 부록 C 라우팅 분석\n' +
      '\n' +
      '작업 도메인별로 집계된 다른 라우터 설계 및 다운스트림 작업에 대한 라우팅 결정의 계층별 비교는 그림 4에 나와 있다. 라우팅 분포는 처음 몇 개의 계층에서 약간 다르지만 계층마다 빠르게 구별할 수 없게 된다. 한 가지 예외는 스위치 라우팅에서 수학 전문가가 마지막 모델 계층의 작업에 걸쳐 우세하게 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Domain & Dataset & Sampling ratio (\\%) \\\\ \\hline \\multirow{4}{*}{Math} & AlgebraicStack & 13.57 \\\\  & OpenWebMath & 54.27 \\\\  & Arxiv & 27.14 \\\\  & Github & 2.99 \\\\  & Commoncrawl & 5.01 \\\\ \\hline \\multirow{2}{*}{Code} & Code & 82.18 \\\\  & Natural language related to code & 9.90 \\\\  & Natural language & 6.93 \\\\ \\hline \\multirow{2}{*}{Wikipedia} & Wikipedia & 90.91 \\\\  & Commoncrawl & 9.09 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 도메인 전문가에 대한 데이터 소스 및 가중치.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c} \\hline \\hline  & GSM8K & MATH & Human & MBPP & Natural & Trivia & ARC-e & ARC-c & Wino & SIQA & PIQA & MMLU \\\\  & & Eval & & & Questions & QA & & & & & \\\\ \\hline \\multicolumn{11}{l}{_Specialized LLMs_} \\\\ CodeLlama 7B & 13.0 & 3.3 & 31.1 & 41.4 & 11.5 & 32.8 & 67.4 & 34.0 & 62.7 & 46.1 & 72.9 & 38.6 \\\\ Llamma 7B & 39.3 & 16.7 & 25.6 & 41.4 & 9.4 & 24.9 & 28.7 & 26.8 & 50.1 & 37.3 & 51.0 & 33.5 \\\\ \\hline \\multicolumn{11}{l}{_General ILMs_} \\\\ Llama-2 7B & 14.7 & 2.5 & 12.8 & 20.8 & 16.4 & 58.5 & 76.4 & 43.8 & 69.2 & 48.3 & 78.8 & 46.1 \\\\ Llama-2 13B & 28.7 & 3.9 & 18.3 & 30.6 & 16.1 & 63.8 & 77.3 & 49.4 & 73.0 & 50.1 & 80.8 & 52.8 \\\\ Dense (DM) & 26.7 & 9.9 & 20.7 & 30.8 & 24.0 & 55.3 & 76.7 & 44.5 & 68.9 & 48.3 & 78.2 & 49.8 \\\\ Sparse upcveling (DM), Top-2 & 37.3 & 18.9 & 29.3 & 40.2 & 18.8 & 49.2 & 76.3 & 43.4 & 66.4 & 47.3 & 77.9 & 51.1 \\\\ Sparse upcveling (CM), Top-2 & 40.1 & 16.2 & 26.2 & 35.2 & 24.5 & 58.2 & 75.6 & 44.7 & 69.1 & 47.1 & 78.0 & 52.1 \\\\ BTM, Top-1 & 27.4 & 15.2 & 30.8 & 41.9 & 15.0 & 38.0 & 72.8 & 38.1 & 68.4 & 47.8 & 77.9 & 44.3 \\\\ BTM, Top-2 & 27.7 & 15.3 & 30.6 & 42.6 & 15.3 & 38.5 & 73.1 & 38.5 & 68.3 & 48.0 & 78.1 & 44.3 \\\\ BTX, sample Top-1 & 36.9 & 15.8 & 25.6 & 37.4 & 23.7 & 56.4 & 76.7 & 45.0 & 70.6 & 48.0 & 78.2 & 53.2 \\\\ BTX, Top-2 & 37.1 & 17.8 & 28.7 & 39.4 & 24.8 & 57.1 & 76.9 & 45.6 & 67.9 & 48.7 & 78.7 & 52.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: BTX 및 기준선의 개별 과제 수행.\n' +
      '\n' +
      '본 논문에서는 로드 밸런싱을 이용한 Top-2 라우팅에서 코드 전문가가 코드 도메인에서 지배적인 힘임을 관찰한다. 부하 분산이 추가되지 않고 수학 전문가가 도메인에 걸쳐 우세한 다른 모델과의 차이를 주목하십시오. 그림 5에서 코드 영역을 자세히 살펴보고 로드 밸런싱이 있는 모델과 없는 모델에 대한 라우팅 확률 분포를 비교한다. 그림의 하단 세 그래프에서 코드 전문가로의 라우팅 확률이 0으로 이동한 죽은 전문가의 현상을 관찰할 수 있는 반면 로드 밸런싱이 추가되면 전문가 간의 확률 분포가 더 유사해 보이며 코드 전문가에 대한 기대가 약간 더 높다.\n' +
      '\n' +
      '전문가가 다른 영역을 전문으로 하는 경우 이해하기 위해 작업당 분포를 자세히 살펴봅니다. Math와 Reasoning 도메인에서 토큰의 라우팅 결정은 그림 6과 같다. GSM8K 태스크는 Code와 Llama-2 전문가를 선호하는 반면, Math 태스크는 In-domain 전문가에 더 의존함을 알 수 있다. GSM8K 데이터셋은 상식 지식과 기본적인 산술 연산을 필요로 하는 초등학교 수학 단어 문제로 구성된 반면 수학 과제는 대학 수준의 수학 지식을 필요로 하고 수학 전문가의 훈련 데이터와 더 정렬되기 때문에 이러한 현상이 발생한다고 가정한다. 추론 영역에서 모든 작업은 유사한 행동을 보이며 수학 및 일반주의 LLM의 전문성에 동등하게 의존한다.\n' +
      '\n' +
      '## 부록 A도 4: 다양한 계층에서의 토큰들의 BTX 라우팅 결정들은 상이한 다운스트림 태스크들에 대해 상이한 전문가들(위키, 수학, 코드, LLaMa-2 7B)에 대한 것이다. 작업은 코드(Human Eval, MBPP), 수학(GSM8K, MATH), 세계지식(Natural Questions, TriviaQA), 추론(ARC-Easy, ARC-Challenge, SIQA, PIQA, WinoGrande) 등 도메인별로 집계된다. 부하 분산을 통한 Top-2 라우팅은 모든 계층에서 다른 라우팅 방법에 비해 전문가 간의 부하 분산이 더 균일하다는 것을 알 수 있다.\n' +
      '\n' +
      '그림 5: 인간 평가 작업을 위해 서로 다른 계층에서 전문가당 확률을 라우팅합니다. 우리는 top-2 라우팅과 (왼쪽) 및 로드 밸런싱이 없는 (오른쪽) 라우팅을 비교한다.\n' +
      '\n' +
      '그림 6: 수학 및 추론 도메인에서 토큰의 라우팅 결정. GSM8K 태스크는 코드와 라마-2 전문가를 선호하는 반면 MATH 태스크는 도메인 내 전문가에 더 의존한다는 것을 관찰한다. 추론 영역에서는 Math와 LLaMa-2 7B 전문가 간에 부하가 분산된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>