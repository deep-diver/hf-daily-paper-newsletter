<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#GLoRe: Global and Local Refinements를 통한 LLM 추론 개선 시기, 장소 및 방법\n' +
      '\n' +
      'Alex Havvilla\\({}^{1,2,\\ast}\\), Sharath Raparthy\\({}^{1}\\), Christoforos Nalpmantis\\({}^{1}\\), Jane Dwivedi-Yu\\({}^{1}\\), Maksym Zhuravinskyi\\({}^{3}\\), Eric Hambro\\({}^{1}\\), Roberta Raileanu\\({}^{1}\\)\n' +
      '\n' +
      'Meta, \\({}^{2}\\)Georgia Institute of Meta, \\({}^{3}\\)StabilityAI\n' +
      '\n' +
      '메타 인턴십 기간 동안 수행한 업무\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최첨단 언어 모델은 수학, 과학 또는 코딩 작업에 대해 인상적인 추론 정제 기능을 나타낼 수 있습니다. 그러나 최근 연구는 최상의 모델조차도 외부 피드백에 액세스하지 않고_언제 및 어디로 정제해야 하는지 식별하는 데 어려움을 겪고 있음을 보여준다. 정련 시기를 나타내는 최종 답변의 정확성을 예측하도록 훈련된 결과 기반 보상 모델(**ORMs**)은 하나의 편리한 솔루션을 제공합니다. 그러나, 어디를 정제할지를 나타내기 위해 사용될 때, 우리는 ORM들이 중간 추론 단계들을 평가하는 데 사용될 때 _overly-pessimistic_인 경향이 있다는 것을 발견하여, 유효한 솔루션들의 과도한 정제화를 초래한다. 정련할 위치를 나타내는 중간 단계의 정확성을 예측하도록 훈련된 프로세스 기반 보상 모델(**PRMs**)은 거절 샘플링 또는 강화 학습(RL) 미세 조정을 통해 LLM 추론 능력을 향상시키는 데 사용되었다. 그러나 그들은 훈련하는 데 비용이 많이 들고, 광범위한 인간 주석이 필요하다. 본 논문에서는 최적의 정책 또는 \\(V^{\\ast}\\)의 예상 미래 보상을 근사화하기 위해 합성 데이터에 대해서만 훈련된 Stepwise ORM(**SORMs**)을 제안한다. 보다 구체적으로, SORM은 현재 정책을 여러 번 샘플링할 때(ORM의 경우와 같이 단 한 번이 아닌) 최종 답변의 정확성을 예측하도록 훈련된다. 우리의 실험은 SORM이 ORM에 비해 잘못된 추론 단계를 더 정확하게 감지할 수 있으므로 개선 작업을 수행할 때 다운스트림 정확도를 향상시킬 수 있음을 보여준다. 그런 다음 질문과 초안 솔루션만 입력으로 하고 수정된 솔루션을 예측하는 _global_ 정제 모델과 첫 번째 추론 오류 위치를 나타내는 비평을 입력으로 하는 _local_ 정제 모델을 훈련한다. SORM을 학습하기 위해 사용된 데이터를 재사용하여 두 모델에 대한 학습 데이터를 합성적으로 생성한다. ORM을 재순위로 사용하여 글로벌 및 로컬 정제를 결합하는 것이 3개의 샘플 기준선 중 최고 수준뿐만 아니라 개별적으로 한 개보다 훨씬 우수하다는 것을 발견했다. 이 전략을 사용하면 GSM8K에서 LLaMA-2 13B 모델(이미 RL로 미세 조정됨)의 정확도를 탐욕스럽게 샘플링할 때 53%에서 65%로 향상시킬 수 있다.\n' +
      '\n' +
      '**날짜 :** 2월 20일 2024\n' +
      '\n' +
      '**대응:** Alex Havvilla at ahavrilla3@gatech.edu\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최첨단 대형 언어 모델(**LLMs**)은 사전 훈련 후 광범위한 다운스트림 기능을 나타낸다. 이것은 수학, 과학, 또는 코딩 문제들에 대한 그들의 추론을 정제하는 능력을 포함한다(OpenAI, 2023; Touvron et al., 2023; Chowdhery et al., 2022). 그러나, 면밀한 검사 하에서, 이러한 정제 능력은 상당히 부서지기 쉬우며, 종종 솔루션이 정제를 필요로 하는 때를 식별조차 할 수 없다(Huang et al., 2023). LLM이 어려운 추론 작업에 대한 성공적인 개선을 생성할 때, 이는 종종 외부 형태의 피드백, 예를 들어 인간 또는 코드로부터의 피드백, 더 강한 모델 또는 다른 도구의 통합으로 인한 것이다(Zhou et al., 2023; Gou et al., 2023). 본 연구에서는 훈련문제의 지상진실답변 이외의 외부 피드백 없이 추론과제에 대한 LLM의 자기 정제 능력을 면밀히 검토하고 개선한다. 특히, 이것은 우리가 인간이나 더 강한 모델의 데이터나 피드백을 사용하지 않는다는 것을 의미합니다. 이를 위해 우리는 정제 문제를 세 부분으로 휴리스틱하게 분해하는 것으로 시작한다: 먼저 _when_를 정제하기로 결정하고, _where_를 정제하기로 결정하며, 마지막으로 _how_를 정제한다.\n' +
      '\n' +
      '결과 기반 리워드 모델(**ORMs**)(Cobbe et al., 2021)은 솔루션 리워딩을 위한 질문이 주어졌을 때 최종 답변 정확도의 추정치로 처음 소개된 단계 1을 해결하기 위한 자연스러운 선택이다. 정련할 곳을 결정하기 위해 ORM의 중간 단계로의 일반화를 주의 깊게 검토한다. 우리는 기본 데이터 생성 정책 \\(\\pi\\)의 정확성이 ORM의 중간 솔루션 단계의 정확성 학습 능력에 직접적인 영향을 미친다는 것을 발견했다. 이는 ORM이 중간 단계 \\(S_{i}\\)에서 문제의 가용성을 과소 추정하는 경우가 많다. 결과는 오류가 있는 단계를 분류하는 데 사용할 때 높은 위음수 비율이다. 프로세스 기반 보상 모델(**PRMs**)은 대신 각 단계의 정확성을 직접 추정하도록 훈련된다. 그러나 이는 모델 생성 솔루션 단계를 유효하거나 유효하지 않은 것으로 광범위하게 인간 레이블을 지정해야 한다. 중간 단계 피드백을 제공하는 능력을 향상시키기 위해 각 단계에서 오류가 있음을 나타내는 레이블을 명시적으로 예측하는 단계적 ORM(**SORM**)을 소개한다. SORM 학습 데이터는 해를 구하는 단계(S\\(S_{i}\\)에서 학생 정책(\\pi\\)을 여러 번 샘플링하고, 최종 정답에 도달하면 유효하다고 레이블링하여 생성한다. RL 관점에서 볼 때, 이는 기각 샘플링으로 최적 정책 \\(\\pi^{*}\\)의 근사화를 통해 추론 과제의 최적 값 함수 \\(V^{*}\\)을 학습하는 것으로 해석될 수 있다. 결과적인 SORM은 더 나은 중간 단계 수준의 피드백을 제공하여, 우리가 정제하기 위해 _when_ 및 _where_ 둘 다에 대한 정보를 정제 모델에 제공할 수 있게 한다. 그런 다음 정제 모델은 정제할 _how_만 결정해야 한다.\n' +
      '\n' +
      '우리는 초기 초안 해 \\(D\\)을 넘어 피드백 없이 전체 추론 궤적을 정제할 수 있는 _global_ 정제 모델을 초기에 학습한다. 트레이닝 데이터는 Welleck et al.(2022)에서와 같이 올바른 해와 잘못된 해의 쌍을 이루어 합성적으로 생성된다. 글로벌 정제 모델의 평가는 정제 시기를 정확하게 식별할 수 없음을 확인시켜 ORM의 필요성을 보여준다. SORM 학습 데이터를 재사용하여, 첫 번째 잘못된 추론 단계를 식별하기 위해 SORM에 의해 주어진 피드백을 사용하는 _local_ 정제 모델을 학습한다. 그런 다음 잘못된 솔루션 초안의 테스트 세트에 대해 글로벌 개선과 로컬 개선의 성능을 비교하여 유사한 개선 정확도를 찾지만 대부분 서로 다른 문제 세트에 대해 비교한다. 이러한 의미에서 글로벌 및 로컬 정제 모델은 보완적이며 로컬 정제는 종종 글로벌 정제가 해결할 수 없는 문제를 해결할 수 있으며 그 반대의 경우도 마찬가지이다. 최상의 결과를 얻기 위해 ORM을 사용하여 글로벌 및 로컬 개선을 결합하여 초기 초안과 둘 다의 재순위로 작용하여 가장 유망한 것을 선택한다. 이 전략을 사용하여 탐욕스럽게 샘플링할 때 이미 강력한 RL 미세 조정 Llama-2 13B 모드의 정확도를 53%에서 65%로 향상시킬 수 있다.\n' +
      '\n' +
      '요약하면, 우리는 다음과 같은 기여를 한다:\n' +
      '\n' +
      '* 리워드 모델(RMs)을 활용하여 정제 문제를 세 부분으로 분해한다. 즉, 보상 모델(RMs)을 이용하여 해를 언제, 어디서, 어떻게 정제할지를 결정한다.\n' +
      '* 최종 답변의 정확성을 판단할 수 있는 능력에도 불구하고 중간 단계의 정확성을 판단함에 있어 ORM의 한계를 강조한다.\n' +
      '* 합성 데이터에 대해서만 학습되고 ORM보다 중간 단계를 더 정확하게 평가할 수 있는 SORM(Step-wise ORM)을 도입한다.\n' +
      '* ORM을 사용하여 _when_를 정제하고, SORM을 사용하여 _where_를 정제하며, 글로벌 및 로컬 정제 모두를 사용하여 _how_를 정제하는 LLM 추론을 정제하는 새로운 방법을 제안한다. 우리는 두 가지 유형의 미세화가 상호 보완적이라는 것을 발견하는데, 각각은 다른 하나는 해결할 수 없는 큰 종류의 문제를 해결할 수 있다.\n' +
      '* 우리의 접근법을 사용하여 13B LLaMA-2 모델에 대해 GSM8K에서 최대 12%의 성능 개선을 입증한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '추론:** 추론 과제\\(\\tau\\)를 (자연어) 질문/답변 쌍\\((Q,A)\\sim\\tau\\)의 분포로 정의한다. 답은 단일 최종 답, 일반적으로 평가의 용이성을 위한 수학 문제의 경우 수치 값일 수 있거나, 수치 최종 답의 정당성을 입증하는 CoT 스타일 솔루션 추적을 포함할 수 있다. 우리는 종종 원자 단계 \\(A=(S_{1},...,S_{L})\\)로 구성된 답 \\(A\\)을 더 쓰고, 최종 답은 단계 \\(L\\)에 주어진다. 새로운 "단계"의 시작이라는 개념은 문제 의존적이지만 우리의 경우 항상 뉴라인 토큰에 해당한다.\n' +
      '\n' +
      '**Reward Modeling:** 강화 학습(RL) 환경이 주어지면, 보상 모델은 상태 \\(s\\)(Christiano et al., 2017)에서 액션 \\(a\\)으로부터 오는 보상을 근사하도록 훈련될 수 있다. 언어 설정에서, 보상 모델들은 LLM(Ouyang et al., 2022)에 의해 생성된 응답에 주어지는 보상을 근사하도록 트레이닝된다. 보상은 일반적으로 RLHF(Christiano et al., 2017; Ziegler et al., 2019)의 경우와 같이 한 세대가 끝날 때 희박하고 주어지며, 여기서 RL 및 거절 샘플링에 대해 대조적 선호 모델이 학습된다.\n' +
      '\n' +
      '이와 유사하게 GSM8K 솔루션의 순위를 재순위화하기 위해 사용되는 최종 답변 검증기로 처음 제안된 _Outcome-based Reward Model_(**ORM**)가 있다(Cobbe et al., 2021). 형식적으로 우리는 ORM 추정치 \\(p(\\texttt{is\\_correct}(A)|Q,A)\\)에서 \\(Q\\)은 질문이고 \\(A\\)은 모델 생성 답이라고 한다. ORM에 대한 학습 데이터는 추론 과제(\\tau\\)의 질문에 대해 기본 학생 모델\\(\\pi\\)을 여러 번 샘플링하여 생성된다. 그런 다음 ORM은 \\(p(\\texttt{is\\_correct}(A)|Q,P_{i})\\)을 예측하도록 훈련되며, 여기서 \\(P_{i}\\)은 중간 단계 \\((S_{1},...,S_{i})\\)의 접두사이고 \\(A\\)은 \\(\\pi\\)에서 샘플링된 \\(P_{i}\\)의 임의의 가상 연속이다. 즉, 중간 단계에서 ORM은 정확한 최종 답으로 이어질 확률을 추정하는 것으로 해석할 수 있다. 우리는 때때로 데이터 생성 학생 모델\\(\\pi\\)에 대한 ORM의 의존성을 강조하기 위해 \\(ORM_{\\pi}\\)을 쓸 수 있다. 보다 최근에는 용액 \\(A=(S_{1},...,S_{L})\\)(Lightman et al., 2023; Uesato et al., 2022)에서 각 단계의 정확성을 직접 감독하기 위해 _Process-based Reward Models_(**PRMs**)가 제안되었다. 형식적으로, PRM은 \\(p(\\texttt{is\\_correct}(S_{i})|P_{i},Q)\\)를 예측하며, 여기서 \\(S_{i}\\)은 \\(P_{i}\\)의 마지막 단계이다.\n' +
      '\n' +
      '**정제:** 우리는 \\(A_{D}\\)와 \\(A_{D}\\)에 컨디셔닝을 통해 생성된 새로운 솔루션 \\(A_{R}\\)으로 초안 솔루션 \\(A_{D}\\)과 질문 \\(Q\\)의 정제를 정의한다. 입력으로 간주하는 전역 정제 모델(Q,A_{D}\\)과 입력으로 간주하는 전역 정제 모델(p(A_{R}|Q,A_{D})\\)과 입력으로 간주하는 국부 정제 모델(p(A_{R}|Q,A_{D},E)\\)을 예측한다.\n' +
      '\n' +
      '**노트:** 나머지 논문에 대해 우리는 다운스트림 작업에 대해 미리 훈련된 LLM 미세 조정을 _base model_로 언급한다. 우리는 지도 데이터에 기반하거나 RL을 사용하여 질문(Q\\)이 주어졌을 때 답을 생성하는 학생 모델을 생성하기 위해 기본 모델을 미세 조정한다. 때때로 우리는 학습 가능한 매개변수\\(\\theta\\)에 암묵적으로 학생 모델을 정책\\(\\pi\\)으로 쓸 수도 있다. \\\\ 열차분할(\\mathcal{D}_{\\text{TASK}}\\)과 시험분할(\\mathcal{D}_{\\text{TASK}}^{\\text{test}\\)이 내포된 TASK\\(\\tau\\) 데이터세트를 나타내기 위해 (\\mathcal{D}_{\\text{TASK}}}\\)가 사용될 것이다. 우리는 질문을 나타낼 때 \\(Q\\)을 사용하고 해의 흔적을 나타낼 때 \\(A_{1},...,A_{k}\\)을 사용할 것이다. 때때로 우리는 용액 흔적 \\(A\\)을 중간 단계 \\(S_{i}\\)으로 분해하는 \\(A=(S_{1},...,S_{L})\\)을 쓸 것이다. \\ (P_{i}=(S_{1},...,S_{i})\\)는 \\(S_{i}\\)까지의 단계의 접두사를 나타내는 데 사용될 것이다. 추가적으로 \\(A_{GR}\\)과 \\(A_{LR}\\)을 사용하여 \\(A_{D}\\)의 글로벌 및 로컬 정련을 나타낼 수도 있다. \\(A_{GR}\\)과 \\(A_{LR}\\)의 글로벌 및 로컬 정련을 나타낼 수도 있다. (V^{\\pi}\\)는 정책의 가치함수를 나타낸다. \\\\ (V^{*}\\)는 배경 태스크에 대한 의존성을 암시적으로 갖는 최적 값 함수를 나타낸다.\n' +
      '\n' +
      '##3 관련 작품\n' +
      '\n' +
      '**LLM Reasoning:** State-of-the-art(SOTA) 대형 언어 모델들(**LLMs**)(Touvron et al., 2023; Bai et al., 2022; Chowdhery et al., 2022)은 광범위한 수학, 과학 및 코드 벤치마크들에 의해 연구된 바와 같이 하드 추론 태스크들에 대해 점점 더 인상적인 능력들을 입증한다(Cobbe et al., 2021; Hendrycks et al., 2021; Sawada et al., 2023; Liang et al., 2022; Srivastava et al., 2022; Rein et al., 2023; Mialon et al., 2023; Chollet, 2019; Hendrycks et al., 2021; Austin et al., 2021; Patel et al., 2021; Gao et al., 2021). cain of thought_(**CoT**)(Wei et al., 2022) 및 관련 기술들(Chen et al., 2022; Yao et al., 2023; Besta et al., 2023)은 이러한 유형의 태스크들에 대한 LLM 성능을 상당히 부스팅하는 지배적인 방법들로서 등장하였다. CoT 방법을 사용하면 LLM이 문제를 올바르게 해결하는 데 필요한 중간 계산을 포함하는 "생각 사슬"을 먼저 생성함으로써 최종 답변을 연기할 수 있다.\n' +
      '\n' +
      '**LLM 정제:** 추론 능력과 밀접하게 관련된 것은 모델의 이전 답변을 정제하는 능력이다. 이 작업은 큰 언어 모델이 수학 추론 과제에 대한 CoT 솔루션을 스스로 정제하는 능력을 연구한다. 여러 작품들(Yao et al., 2022; Madaan et al., 2023; Zhou et al., 2023)은 프롬프트 및/또는 툴 사용을 통해 다양한 작업들에 대해 SOTA LLM 자기 정제 및 자기 비평 능력들을 입증한다. 그러나 최근 연구(Huang et al., 2023)는 가장 강력한 모델에도 불구하고 이러한 기술은 모델 자체가 정제 중단 시기를 결정해야 하는 하드하고 개방형 추론 작업에서 어려움을 겪는다.\n' +
      '\n' +
      '다른 논문들은 손으로 조작된 데이터 증강(Paul et al., 2023)을 사용하거나 인간 데이터를 수집(Wang et al., 2023; Chen, 2023; Lee et al., 2023; Saunders et al., 2022; Schick et al., 2022)하는 반면, 다른 논문들은 더 큰 모델들에 대한 비평들을 생성하기 위해 강화 학습으로부터의 기법들을 사용한다(Akyurek et al., 2023; Yao et al., 2023). 우리와 관련된 대부분은 (Welleck et al., 2022)로, _low-value_롤아웃과 _high-value_롤아웃을 페어링하여 암시적 강화 학습과 같은 방식으로 글로벌 정제 모델을 훈련한다.\n' +
      '\n' +
      '프로세스 기반 보상 모델링(**PRMs**)(Uesato et al., 2022; Lightman et al., 2023)은 최종 답변의 정확도에 대한 단계의 영향을 명시적으로 모델링하지 않고 특정 단계의 "정확도"에 대해 보다 조밀하고 단계적으로 보상을 제공한다. ORM과 PRM은 모두 많은 수의 후보 솔루션에 대한 재순위로 가장 자주 사용되며, PRM은 일반적으로 ORM을 능가한다(Lightman et al., 2023). 그러나 PRM은 훈련에 비용이 많이 들기 때문에 각 단계의 광범위한 인간 주석이 필요하다. Uesato et al. (2022)는 GSM8K에 대한 70B ORM 대 PRM의 성능을 직접 비교하여, RL에 대한 보상으로서 사용될 때 그리고 재순위화에 대해 둘 다 유사하게 수행하는 것을 발견한다. 그들은 ORM이 PRM과 유사한 방식으로 중간 단계로 다소 일반화되는 것으로 보이지만 여러 모델 또는 작업에 대해 이 관찰을 정량적으로 제거하지는 않는다. Li et al.(2022)는 몬테카를로 트리 탐색을 위해 사용되는 PRM과 유사한 합성 단계적 검증자 s를 훈련시키려고 시도한다. 동시 작업(Wang et al., 2023)은 합성 프로세스 기반 보상 모델을 우리의 SORM과 유사한 방식으로 트레이닝하는 것을 제안한다. 그런 다음 RL 미세 조정 및 거부 샘플링을 위해 RM 다운스트림을 사용한다.\n' +
      '\n' +
      '위의 작업과 달리 단계 수준에서 ORM/SORM 검증 능력을 주의 깊게 비교한다. 그런 다음 우리는 개선을 위해 ORM/SORM을 활용할 것을 제안한다. SORM 및 정제 모델을 모두 훈련할 수 있는 완전 합성 단계적 레이블을 생성함으로써 이를 달성한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '우리는 정제 문제를 세 단계로 분해하는 것으로 시작한다. 첫째, 초안(D\\)이 정확할 때 그리고 정제 작업이 필요할 때 학습한다. 둘째, 첫 번째 부정확한 단계를 식별하여 개선을 시작하기 위해 _where_를 학습한다. 셋째, 초기 초안을 수정하기 위해 _how_를 학습한다. 우리는 초안이 정확할 확률을 예측하도록 훈련된 ORM을 사용하여 자연스럽게 1단계를 다룰 수 있다. 이것은 일부 어려움을 완화하는데, 이제 정제기만이 어디에서 언제 정제할지를 식별하도록 요구한다. 또한, 로컬 리파인닝을 수행할 때, (S)ORM을 사용하여 첫 번째 에러의 위치를 로컬화하는 것을 제안한다. 이것은 이제 로컬 정제기가 오류를 수정하는 방법만 결정하고 거기서 계속해야 하기 때문에 작업을 더욱 단순화한다.\n' +
      '\n' +
      'Reward Models을 이용한 오차의 국지화: \\(P_{i}=(S_{1},...,S_{i})\\)이 모든 단계의 접두어인 \\(S_{i}\\)에서 중간 예측 \\(ORM_{\\pi}(Q,P_{i})\\)을 사용하여 계단 수준에서 오차를 식별함으로써 ORM을 활용할 수 있다. ORM을 상기하는 것은 접두사 \\(P_{i}\\)이 있는 해의 가능성을 예측하도록 훈련되어 정확한 최종 답이 된다. 중요한 것은, 이 훈련 데이터로부터 추론된 가능성은 데이터 생성 정책 \\(\\pi\\)에 크게 의존한다는 것이다. 이러한 이유로 우리는 필요 없을 때 생략하여 첨자\\(ORM_{\\pi}\\)를 포함하기도 한다.\n' +
      '\n' +
      '중간 단계 \\(S_{i}\\)에서 ORM의 예측의 거동을 가장 잘 이해하기 위해 우리는 그것을 \\(\\pi\\)의 _값 함수로 해석할 수 있다. 정책 \\(\\pi\\)의 값함수 \\(V^{\\pi}\\)를 \\(V^{\\pi}(S)=\\mathbb{E}_{\\tau\\sim\\pi(S)}R(\\tau)\\) 즉, 상태 \\(S\\)에서 정책 \\(\\pi\\)의 평균수익률로 계산한다. 추론 문제의 맥락에서 우리가 고려하는 상태는 질문 \\(Q\\)과 중간 단계 \\(S_{j}\\)이 있는 형태 \\(S=(Q,S_{1},...,S_{i})\\)이다. 기본적으로 우리의 설정에는 정확한 최종 답변을 위해 터미널 상태에서 주어지는 \\(+1\\)의 희박한 보상만 있다.\n' +
      '\n' +
      '우리는 \\(ORM_{\\pi}(Q,P_{i})\\approx p(\\texttt{is\\_correct(A)}|Q,P_{i},\\pi)\\)를 쓸 수 있다. 여기서 \\(P_{i}=(S_{1},...,S_{i})\\)는 모든 이전 단계의 접두사이고, is_correct(A)는 접두사를 가진 \\(\\pi\\)에서 샘플링된 풀 솔루션 \\(A\\)이 정확한 최종 답을 갖는다. 그리고 나서 \\(\\mathbb{E}_{A\\sim\\pi(Q,P_{i})}R(A)=\\mathbb{E}_{A\\sim\\pi(Q,P_{i})}\\texttt{1}_{\\texttt{in\\_correct(A)}}=p(\\texttt{is\\_correct(A)}|Q,P_{i},\\pi)\\texttt를 쓸 수 있다. 따라서 정책 \\(\\pi\\)의 가치 함수에 대한 근사치는 중간 단계 \\(S\\)에서 결과 기반 보상 모델과 정확히 동일한 것을 예측한다. 따라서 우리는 학습 데이터를 생성하기 위해 사용되는 학생 모델**\\(\\pi\\)에 대한 값 함수에 근사하는 것으로 **ORM을 다룰 수 있다.\n' +
      '\n' +
      '이상적으로 우리는 ORM을 사용하여 첫 번째 단계 \\(S_{i}\\)을 찾아서 \\(ORM(Q,P_{i})\\leq 0.5\\) 즉, \\(P_{i}\\)이 오답으로 이어질 수 있도록 하여 실수가 발생한 곳을 식별하고자 할 수 있다. 그러나 ORM은 \\(\\pi\\)에 대한 값 함수로 작용하기 때문에 단순히 데이터를 생성하는 학생 \\(\\pi\\)이 실패할 것으로 예상하기 때문에 오류 단계를 **환시하는 경향이 있다. 예를 들어, \\(\\pi\\)가 나눗셈과 관련된 문제에 거의 항상 실패한다면, ORM은 학생이 첫 번째 단계를 밟기 전에도 나눗셈 문제에 낮은 성공 확률을 할당할 것이다. 이러한 경우에 우리는 ORM이 지나치게 비관적이라고 말한다. 이것은 오류 위치를 식별하기 위해 ORM을 사용할 때 이상적이지 않다.\n' +
      '\n' +
      '** Step-Wise ORM(SORM):** 각 단계에서 실수를 식별하기 위해 사용될 수 있는 또 다른 자연 후보는 PRM(Process Based Reward Model)이다(Lightman et al., 2023). PRM은 최종 답에 미치는 영향과 무관하게 단계 \\(S_{i}\\), \\(p(S_{i}\\\\texttt{correct}|Q,S_{1},S_{2},...,S_{i})\\)의 정확성 확률을 추정한다. 그러나 이는 비용이 많이 들며 인간의 주석이 달린 샘플을 수집해야 한다. 대신 추론 과제의 최적값 함수_\\(V^{*}\\)을 근사화하는 방법을 제안한다. \\ (V^{*}\\)는 그림 1의 값 함수에 해당한다: 3단계 정제 훈련 파이프라인에 대한 다이어그램. 먼저, 기본 모델을 세밀하게 조정하여 강력한 학생 정책을 생성합니다. 그런 다음 학습 데이터에 대한 샘플링 \\(\\pi\\)을 통해 ORM/SORM 학습 데이터를 생성한다. 마지막으로, 잘못된 롤아웃과 전역적 및 국부적으로 올바른 롤아웃을 함께 페어링하여 정제 데이터를 생성한다. 주, \\((Q,A,l)\\)은 이진 정확성 레이블 \\(l\\)을 갖는 질문, 답변 쌍을 나타낸다. SORM 훈련 샘플 \\((Q,P_{i},l_{i},T)\\)에는 \\((S_{1},...,S_{i})\\)의 프리픽스, 프리픽스에 대한 이진 정확성 레이블 \\(l_{i}\\) 및 \\(P_{i}\\)의 정확성을 검증하는 검증 롤아웃 집합 \\(T_{1},...,T_{K}\\)이 포함된다. 글로벌 보정 페어링은 잘못된 ORM 롤아웃과 올바른 ORM 롤아웃을 페어링하여 글로벌 정제 트레이닝 데이터를 생성하는 데 사용된다. 유사하게, 로컬 정정 페어링 쌍은 (정정하지 않은)\\(P_{i+1}\\)의 부정확한 검증들\\(T_{-}\\)과 \\(P_{i}\\)의 정확한 검증들\\(T_{+}\\)을 짝짓는다. 그런 다음 초기 초안 \\(A_{D}=T_{-}\\) 및 정제 \\(A_{R}=T_{+}\\)에서 \\(i+1\\) 단계의 오류를 나타내는 레이블 \\(E=i+1\\)을 생성한다.\n' +
      '\n' +
      '임의의 논리적으로 유효한 중간 상태 \\(S_{j}\\)로부터 추론 태스크를 성공적으로 해결할 수 있는 최적 정책_. 이러한 최적값 함수는 실수 없는 솔루션 프리픽스에 대해 \\(V^{*}(Q,S_{1},...,S_{i})=1\\)을 가지며, 프리픽스에 오류가 이미 포함되어 있으면 오답이 발생할 수 있다. 우리는 모델을 \\(V^{*}\\) 단계적 ORM 또는 **SORMs**에 직접 근사하도록 훈련한다고 부른다.\n' +
      '\n' +
      'Uesato et al. (2022)에서 논의된 바와 같이, ORM은 중간 솔루션 정확성에 대한 일부 지식을 보유하여 PRM을 근사화할 수 있다. 그러나 실제로 이 속성은 기본 모델의 크기와 과제의 난이도에 따라 달라지며, ORM은 더 큰 학생들의 데이터에 대해 훈련되고 더 쉬운 과제는 PRM에 더 나은 근사치를 제공한다. ORM을 데이터 생성 학생의 값 함수 \\(V^{\\pi}\\)로 해석할 때 이는 일리가 있다. 더 크고 유능한 학생은 최적의 정책(\\pi^{*}\\)을 더 잘 근사할 것이고, 결과적으로 ORM을 \\(V^{*}\\)으로 더 잘 근사할 것이다.\n' +
      '\n' +
      '### Training pipeline\n' +
      '\n' +
      '상기시켜, 우리는 미세 조정을 위한 인간 또는 더 나은 모델의 데이터에 대한 액세스가 없다고 가정한다. 따라서 전역 및 지역 개선을 위한 모든 학습 데이터를 합성적으로 생성해야 한다. 또한 ORM과 SORM 모두에 대한 데이터를 생성해야 한다. 제안된 훈련 파이프라인을 세 단계로 나눈다. 각 단계를 설명하는 다이어그램은 그림 1을 참조하십시오.\n' +
      '\n' +
      '###### 단계 1: 학생 모델을 미세 조정하는 단계\n' +
      '\n' +
      'ORM/SORM 훈련 데이터와 초기 정제 초안 \\(A_{D}\\)을 생성할 수 있는 베이스 체크포인트를 생성하기 위해 전문가 반복법(**EI**)(Silver et al., 2017)을 사용하여 모델을 미세 조정한다. 이것은 질문당 학생 모델 \\(K=96\\)번을 샘플링하고 오답으로 롤아웃을 필터링하여 수행된다. 그런 다음 나머지 샘플에 대해 중복제거를 수행하여 새로운 미세조정 데이터세트 \\(\\mathcal{R}_{1}\\)를 구성한다. 그런 다음 미리 훈련된 모델을 다시 미세 조정하기 위해 사용하는 \\(\\mathcal{D}_{1}\\)의 SFT 데이터를 생성하는 모든 가용 데이터와 결합한다. 이 과정은 각 후속 미세조정의 maj@1 점수가 수렴할 때까지 반복된다. 주목할 점은, \\(i\\) 단계에서 사용된 미세 조정 데이터 세트는 \\(\\mathcal{D}_{i}=R_{i}\\cup\\mathcal{D}_{i-1}\\): \\(ith\\) 단계에서 생성된 롤아웃과 이전에 생성된 훈련 데이터(\\(\\mathcal{D}_{0}=\\emptyset\\) 또는 \\(SFT\\))의 결합이다. GSM8K의 경우, 먼저 주어진 감독 미세 조정(**SFT**) 데이터에 대해 미리 훈련된 각 모델을 미세 조정한다. CoT SFT 데이터가 없는 SVAMP의 경우 1-샷이 사전 훈련된 모델에 초기 EI 데이터 세트를 구성하는 데 사용되는 솔루션을 생성하도록 프롬프트했다. 우리는 결과 모델을 학생 모델 또는 학생 정책이라고 부른다. 이 교육 과정과 결과 모델에 대한 자세한 내용은 부록의 B절을 참조하십시오.\n' +
      '\n' +
      '######## 단계 2: ORM/SORM 훈련\n' +
      '\n' +
      '우리는 프롬프트당 RL 미세 조정 학생 정책 \\(\\pi\\)\\(K\\) 회수를 샘플링하여 ORM 학습 데이터를 생성한다. 평소와 같이, 우리는 최종 답이 맞다면 각각의 중간 단계 \\(S_{i}\\)를 정답으로 표시하고 그렇지 않으면 오답으로 표시한다. SORM의 학습 데이터를 생성하기 위해 모델 생성 해에서 각 단계 \\(S_{i}\\)에서 최적 정책 \\(\\pi^{*}\\)의 근사치를 샘플링하고 최종 해의 정확성을 확인한다. 본 연구의 목적은 학생정책(\\pi^{*}\\)의 거부표본추출을 통해 \\(\\pi^{*}\\)을 근사화하는 것이다. 구체적으로, 모델 생성 롤아웃에서 단계 \\(S_{i}\\)에 대한 학습 레이블을 생성하기 위해 접두사 \\(P_{i}=(S_{1},...,S_{i})\\에서 시작하는 \\(K\\) 롤아웃에 대한 학생 정책 \\(\\pi\\)을 샘플링한다. 이것은 \\(l_{1},...,l_{K}\\)으로 표시된 정확한 최종 답변으로 검증 트레이스 \\(T_{1},...,T_{K}\\)을 생성한다. 그리고 나서 \\(\\max_{j}l_{j}=1\\)인 경우 \\(S_{i}\\)을 양성으로 표시하였다. 즉, \\(S_{i}\\)에서 출발하여 정확한 최종 답을 찾을 수 있다. 실제로 우리는 한 단계당 최대 300개의 토큰을 생성하는 \\(K=8\\) 롤아웃을 샘플링한다. 그렇지 않으면 우리는 \\(S_{i}\\)을 음수로 표시한다. 그런 다음 SORM을 ORM과 정확히 동일한 방식으로 훈련하여 솔루션의 각 단계 후 적절한 레이블을 예측한다. 이 프로세스에 의해 할당된 레이블을 지상 진리 인간 레이블과 비교하려면 섹션 G를 참조하십시오.\n' +
      '\n' +
      '후처리시######SORM 데이터\n' +
      '\n' +
      '거부 샘플링을 통한 최적 정책의 근사치를 개선하기 위해 여러 후처리 단계를 적용한다. **1)** 단계 \\(S_{i}\\)에 양의 레이블이 있는 경우 \\(l_{i}\\)에 대해 \\(l_{j}=1\\)을 설정한다. 즉, 양의 단계 이전의 모든 단계는 양의 레이블로 표시된다. 이것은 학생들이 \\(S_{i}\\) 단계의 샘플로 해를 찾을 수 있지만 이전 단계 \\(S_{j}\\), \\(j<i\\)이 없는 특히 어려운 문제를 설명한다. **2)** 우리는 검증 롤아웃에 _일관성 제약 조건을 강제하고, 솔루션의 단계 \\(S_{i}\\)에서 계산된 각각의 중간 결과 \\(R_{i}\\)이 나중에 사용될 것을 요구한다. 이는 검증 중인 이전 단계를 완전히 사용하기 위해 검증을 요구함으로써 오탐을 방지하는 데 도움이 된다. 실제로 우리는 \\(P_{i}\\) 다음에 접미사에서 각 \\(R_{i}\\)을 문자열로 확인하여 이를 구현한다. **3* * 훈련 데이터세트의 각 접두사 길이에서 양의 레이블 수와 음의 레이블 수를 균형 있게 조정한다. 그렇지 않으면 솔루션의 시작을 향한 포지티브 라벨과 끝을 향한 네거티브 라벨의 불균형이 있기 때문에 이것은 중요하다. 이러한 불균형은 SORM이 쉽게 이용할 수 있으므로 처음 몇 단계에서 거의 항상 긍정적인 레이블을 예측하는 모델이 끝날 때까지 부정적인 레이블을 예측한다.\n' +
      '\n' +
      '추가 기준선으로 ORM 훈련 데이터 세트에서 질문당 긍정 및 부정 수의 균형을 단순히 맞추는 **균형 ORM**을 고려한다. 이것은 앞서 설명한 ORM의 지나치게 비관적인 행동을 완화하기 위한 시도로 수행된다.\n' +
      '\n' +
      '우리의 SORM 근사치는 동시 작업에서 얻은 관찰에 의해 동기화되어 학생\\(\\pi\\)이 사전 훈련 데이터의 분포에서 대부분의 문제를 충분히 해결하기 위해 너무 많은 탐색, 즉 샘플링에 참여할 필요가 없음을 보여준다. 이는 최적 정책에 대한 적절한 근사치를 제공할 수 있는 거부 샘플링을 제안한다. 또한, 추론 환경의 결정론적 동역학은 최적의 정책\\(\\pi^{*}\\)에서 한 번만 샘플링하여 접두사\\(P_{i}\\)에서 \\(V^{*}\\)을 계산할 수 있도록 한다. 이는 샘플링 요구 사항을 더욱 감소시키면서, 또한 거절 샘플링이 접두사\\(P_{i}\\)에서 문제를 해결할 수 있다면 \\(\\pi^{*}\\)도 \\(P_{i}\\)에서 문제를 해결할 수 있다는 결론을 내릴 수 있다. 물론 거부 샘플링은 \\(\\pi^{*}\\)보다 약할 것이며, 결과적으로 SORM은 \\(V^{*}\\)의 과소 근사치가 된다.\n' +
      '\n' +
      '**단계 3 : 훈련 개선 모델**\n' +
      '\n' +
      '국소 정제 모델을 학습하기 위해서는 \\(Q,A_{D},A_{R},E)\\)의 형태(Q,A_{D},A_{R},E)의 데이터 셋이 필요한데, 여기서 \\(A_{D}\\)는 초기 초안이고, \\(E\\)는 첫 번째 오차의 위치를 \\(A_{D}\\)에 레이블링하여 정확한 최종 답을 갖는 정제이다. 프래티스에서 \\(E\\)은 초안에서 잘못된 단계 \\(S_{i}\\)을 프리픽싱하는 "[BAD]" 토큰으로서 로컬 미세화에 전달된다. 그런 다음 테스트 시간에 초안의 오류를 국지화하기 위해 \\(p(E|Q,A_{D})\\)를 예측하는 모델이 필요하다. 편리하게도, 우리는 \\(A_{D}\\)에서 각 단계의 정확성을 예측하기 위해 명시적으로 SORM을 훈련한다. 따라서, 모든 단계에서 SORM을 추론하고, 예측된 정확도가 임계값\\(T\\) 이하인 첫 번째 단계의 인덱스를 반환한다. 또한, **우리는 SORM 데이터세트**를 사용하여 오류 주석이 있는 정제 훈련 데이터세트를 구성할 수 있다. 잘못된 모델 롤아웃이 주어지면 \\(A=(S_{1},S_{2},...,S_{L})\\)\n' +
      '\n' +
      '그림 2: 수학 단어 문제에 대한 로컬 및 글로벌 개선 사례. **왼쪽:** 지역 교양은 분수로 나누는 데 어려움을 겪는 학생에게는 형편없습니다. 부분 분할로 이어지는 모든 이전 단계가 유효하지만, 로컬 정제 모델은 어려운 작업을 다시 시도하거나 완전히 잘못된 작업을 선택할 수 밖에 없다. 대조적으로, 글로벌 정제 모델은 완전히 새로운 접근법으로 문제를 해결하려고 시도할 수 있다. **Right:** 이 초안에서는 모델이 최종 답변에 매우 가깝고 마지막에 간단한 실수만 합니다. 현지 정련은 이 단순한 실수를 바로잡을 수 있다. 대조적으로, 글로벌 개선은 처음부터 시작해야 한다\n' +
      '\n' +
      '추적에서 첫 번째 제로 레이블로 \\(l_{i}=0\\)을 식별하여 첫 번째 오류를 포함하는 단계 \\(S_{i}\\)를 찾습니다. 그런 다음 이전 (정정) 단계 \\(S_{i-1}\\)에서 정확한 검증 흔적 \\(T\\)과 \\(A\\)을 짝짓는다. 이것은 \\(A,T)\\)의 첫 번째 오차를 \\(E=i\\)으로 레이블링하는 훈련 쌍을 생성한다. 예를 들어, 도 2를 참조한다.\n' +
      '\n' +
      'ORM 학습 데이터 세트를 사용하여 유사하게 전역 정제를 위한 데이터 세트를 구성한다. 이는 동일한 질문(Q\\)에 대해 잘못된 롤아웃(A\\text{incorrect}}\\)과 올바른 롤아웃(A\\text{correct}}\\)을 짝지어 수행된다. 이는 학습 튜플((Q,A_{text{incorrect}},A_{text{correct}})\\)을 구성한다. 로컬 정제와 유사한 형식을 유지하기 위해 잘못된 롤아웃이 시작될 때 \\([BAD]\\) 토큰을 넣는다. 우리는 두 정제 데이터 세트를 결합하여 글로벌 및 로컬 정제 모두 가능한 모델을 훈련한다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '우리는 과제\\(\\tau\\)에서 시험문제\\(Q\\)에 대한 학생 모델을 탐욕스럽게 샘플링하여 ORM/SORM과 정제 모델 모두에 대한 시험 세트를 구성한다. 각 벤치마크에 대해 \\(Q,A_{D})\\) 형태의 프롬프트가 있는 테스트 세트를 제공하며 여기서 \\(Q\\)은 문제이고 \\(A_{D}\\)은 초기 초안이다. 두 벤치마크 모두에 대해 우리는 이것을 \\((Q,D)\\) 테스트 세트로 지칭한다. 중간 단계 레이블을 생성하기 위해 SORM 학습 데이터를 생성하는 데 사용된 프로세스와 동일한 프로세스를 사용한다. 이 테스트 세트의 ORM 및 SORM을 이러한 지상 진리 라벨과 비교하여 평가한다.\n' +
      '\n' +
      '전역 정제 성능을 평가하기 위해 각 \\((Q,A_{D})\\) 샘플에 대한 정제기를 탐욕스럽게 추론하고, 그 결과 정제된 \\(A_{\\text{GR}}\\)과 지상 진리를 비교한다. 로컬 정제 모델을 평가하기 위해 먼저 ORM 또는 SORM을 사용하여 첫 번째 오차의 위치와 각 \\((Q,A_{D})) 쌍에 주석을 달았다. 이것은 우리가 탐욕스럽게 지역 정제기를 샘플링하기 위해 사용하는 \\((Q,A_{D},E)\\) 삼중항을 형성한다.\n' +
      '\n' +
      '최상의 결과를 얻기 위해 전역적 정제\\(A_{\\text{GR}}\\)와 국부적 정제\\(A_{\\text{LR}}\\)을 초안(A_{D}\\)에 대해 샘플링하고 ORM 리랭커를 사용하여 최적의 해를 선택하는 방법을 제안한다. 이 전략은 글로벌 및 로컬 개선이 각각 학생이 처음에 실패한 문제의 보완적이고 부분적으로 겹치지 않는 하위 집합을 해결한다는 관찰에서 비롯된다. 따라서 두 가지 개선 사항을 초안과 결합하면 우리가 해결할 수 있는 문제 세트가 크게 확장된다. 또한 ORM을 사용하여 개선 순위를 재조정하면 초안 생성 학생의 3개 기준선 중 3개 기준선과 더 깔끔하게 비교할 수 있다. 평가 파이프라인의 다이어그램은 그림 3을 참조하십시오.\n' +
      '\n' +
      '우리는 또한 부록에서 더 탐구적인 작업을 강조한다. 본론에서 우리는 솔루션 트레이스에서 추론 오류를 찾는 것에 의존하는 _process-based_local refinement만을 고려한다. 이 접근법의 한 가지 단점은 정련을 하는 학생 모델의 능력에 대한 불가지론이다. 또는 모델이 성공할 가능성이 가장 높은 솔루션에서 단계를 식별하는 피드백에 의존하는 _값 기반_ 정제화를 고려한다. 부록 섹션 J에서 프로세스 기반 정제와 비교된다. 또한, 부록 섹션 C에서는 전문가 반복을 사용한 정제 트레이닝을 다양한 보상 스킴을 갖는 다른 RL 알고리즘들과 비교한다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      'GSM8K(Cobbe et al., 2021) 및 SVAMP(Patel et al., 2021) 수학 단어 문제 벤치마크에 대한 정제 파이프라인을 평가한다. 우리는 라마-2 7B 및 13B를 미세 조정하여 ORM, SORM 및 정제 모델을 포함한 모든 다운스트림 모델을 생성한다. 각 모델 크기에 대한 평가는 서로 다른 크기의 모델의 데이터나 피드백을 활용하지 않고 자체적이다. 그리디 샘플링을 통한maj@1 모델 점수는 모델 성능을 평가하는 데 사용된다. 각 단계의 훈련에 대한 하이퍼파머들은 부록의 A절에서 공급된다.\n' +
      '\n' +
      '### ORM 및 SORM 평가\n' +
      '\n' +
      '**SORM은 중간 답변을 평가할 때 ORM보다 우수하다:** GSM8K 상에서 SORM은 ORM의 중간 단계 정확도에 대해 73%에서 81%까지 최대 8% 개선된다(표 1 참조). 이는 ORM이 중간 단계 정확성을 추정하는 합리적인 작업을 수행하지만 특히 GSM8K와 같은 하드 작업에 대한 더 작은 모델의 경우 여전히 개선될 수 있음을 확인시켜준다. 우리는 라벨 정확도의 이러한 차이가 또한 개선 최종 정확도의 차이로 해석되는 것을 볼 것이며, 여기서 ORM/SORM이 실수의 위치를 확실하게 식별하는 것이 중요하다. 이에 비해 균형 잡힌 ORM은 ORM과 비교할 수 있는 중간 정확도를 가지고 있어 성능이 낮다. 이는 균형 잡힌 ORM이 모든 질문에 대략 50%의 성공 확률을 할당하기 때문에 ORM의 과도한 비관주의를 해결하기 위해 품질적으로 나타났음에도 불구하고 그렇다. 또한 0.5를 분류 임계값으로 사용할 때 SORM이 오양성과 음수의 균형 잡힌 수를 갖도록 찾는 오류 유형을 조사한다.\n' +
      '\n' +
      '**ORM은 더 쉬운 작업에서 더 나은 근사 \\(V^{*}\\)을 갖는다:** SVAMP 상에서 ORM은 GSM8K 상에서보다 더 나은 스텝 정확도를 갖는다(표 1 참조), 특히 13B 모델. 결과적으로 SORM은 더 적은 개선을 제공한다. GSM8K의 대부분의 질문은 상대적으로 더 어려워 해결하기 위해 최소 4단계가 필요하다. 대조적으로, SVAMP의 대부분의 질문은 최대 세 가지 핵심 단계를 필요로 한다. 이 적은 수의 단계는 ORM이 일반화하는 것을 더 쉽게 할 수 있다. 또한 SVAMP로 훈련된 EI 모델은 GSM8K의 동일한 크기 모델보다 평균 15% 더 높은 정확도에 도달한다. 이는 기초 학생 모델을 SVAMP에서 \\(\\pi^{*}\\)에 더 가까운 근사치로 만들고 ORM을 \\(V^{*}\\)에 더 가까운 근사치로 만든다.\n' +
      '\n' +
      'SVAMP에서 7B 모델과 13B 모델 사이의 정확도 차이로 인해 강력한 데이터 생성 학생\\(\\pi\\)의 중요성이 더욱 강조된다. 7B 학생 EI 모델은 58%의 정확도를 얻는 반면 13B 모델은 70%의 정확도를 얻는다. 상응하게, 13B ORM 모델은 7B 모델보다 중간 단계에서 훨씬 더 잘 수행한다. 그러나 대조적으로 GSM8K의 13B ORM은 7B보다 중간 단계에서 약간 더 나쁘게 수행한다. 이것은 아마도 7B 학생에 비해 5%만 향상된 GSM8K에서 13B EI 학생의 성능에 의해 부분적으로 설명될 것이다.\n' +
      '\n' +
      '**ORM은 최종 답변을 평가하는 데 있어 SORM보다 우수하다:**SORM이 일반적으로 중간 단계를 예측하는 데 더 우수함에도 불구하고 ORM에 비해 최종 답변 정확성을 예측하는 데 약간 더 나쁘다. 이것은 GSM8K의 13B SORM이 5% 지연되는 두 벤치마크 모두에 해당된다(표 2 참조). 하지만\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{GSM8K} & \\multicolumn{2}{c}{SVAMP} \\\\ \\cline{2-5}  & 7B & 13B & 7B & 13B \\\\ \\hline ORM & 0.74 & 0.73 & 0.77 & 0.85 \\\\ Balanced ORM & 0.73 & 0.74 & 0.77 & 0.83 \\\\ SORM & **0.79** & **0.81** & **0.78** & **0.87** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 테스트 세트 라벨에 대한 7B/13B ORM 및 SORM의 단계 수준 정확도. 참고: 테스트 세트는 샘플의 45%-55%를 나타내는 양성 라벨과 잘 균형을 이룬다. SORM은 더 단단한 GSM8K 벤치마크에서 ORM보다 더 나은 스텝 레벨 정확도를 갖지만 SVAMP에서는 비교 가능한 스텝 레벨 정확도를 갖는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & \\multicolumn{2}{c}{GSM8K} & \\multicolumn{2}{c}{SVAMP} \\\\ \\cline{2-5}  & 7B & 13B & 7B & 13B \\\\ \\hline ORM & **0.82** & **0.85** & **0.75** & **0.82** \\\\ Balanced ORM & 0.8 & 0.82 & 0.73 & 0.79 \\\\ SORM & 0.79 & 0.8 & 0.74 & 0.79 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 테스트 세트 라벨에 대한 7B/13B ORM 및 SORM의 최종 답변 정확도. 참고: 테스트 세트는 샘플의 45%-55%를 나타내는 양성 라벨과 잘 균형을 이룬다. ORM은 더 단단한 GSM8K 벤치마크에서 ORM보다 더 나은 스텝 레벨 정확도를 갖지만 SVAMP에서는 비교 가능한 스텝 레벨 정확도를 갖는다.\n' +
      '\n' +
      '그림 3: **글로벌 및 로컬 정제 모델에 대한 평가 파이프라인**. 먼저 학생 모델로부터 초안\\(A_{D}\\)을 샘플링하고 전역 및 지역 정제를 샘플링한다. 그런 다음 ORM을 사용하여 이 세 가지 후보 솔루션 중 최종 응답으로 선택할 응답을 결정한다.\n' +
      '\n' +
      '이 차이의 일부는 ORM이 악용할 수 있는 통계적 편향으로 인해 과대 비관주의의 비용으로 최종 답변 정확도를 향상시킬 수 있다. 예를 들어, 문제가 나눗셈을 포함하는 경우 ORM은 학생이 실패할 가능성이 높다는 것을 알고 즉시 낮은 성공 확률을 예측한다. 대조적으로 SORM은 각 중간 단계의 정확성을 주의 깊게 검토하려고 시도하면서 더 낙관적일 수밖에 없다.\n' +
      '\n' +
      '불행히도 최종 답안 예측 변수로서 SORM의 부정확성은 또한 최종 답안 재순위로 약간 더 악화시킨다. 이러한 이유로 우리는 후보 초안 및 개선 사항을 재순위화할 때마다 ORM을 사용한다. GSM8K에 대한 재순위 정확도의 보다 상세한 비교는 그림 4에서 수행된다. 이 비교는 GSM8K에 대한 감독 미세 조정만을 사용하여 훈련된 학생 모델에서 파생된 ORM 및 SORM을 사용하여 수행된다. Rerank 정확도는 학생 \\(K\\)번 샘플링하고 랭커로 각 롤아웃을 채점하여 계산된다. 그런 다음 가장 높은 점수를 받은 롤아웃이 최종 답변으로 선택된다.\n' +
      '\n' +
      '그림 4는 또한 추가 후 액세스 없이 데이터에 대해 훈련된 SORM 모델에 대한 순위 정확도를 다시 표시한다. 최상의 성능을 보이는 SORM은 일관된 검증 롤아웃과 단계별 균형 레이블만을 사용하여 좋은 후처리 선택으로 정당화한다.\n' +
      '\n' +
      '글로벌 및 로컬 리파이닝 평가###\n' +
      '\n' +
      'SORM의 성능을 보다 잘 이해할 수 있게 하기 위하여, 본 논문에서는 GSM(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\(S_{i})\\\n' +
      '\n' +
      '**글로벌 및 로컬 정제 모델 모두 정제 시기를 아는 데 어려움을 겪는다:** 벤치마크에서 글로벌 및 로컬 정제 모두 전체 모델 정확도에 거의 향상을 보이지 않는다. GSM8K 7B 글로벌 개선은 전체 정확도를 심지어 감소시키며, 다른 모델은 최대 1% 개선된다. 로컬 개선은 제1 실수의 위치(따라서 존재)를 나타내는 "[BAD]" 토큰의 존재로 인해 전체 정확도를 더 향상시킨다. 이것은 잘못된 초안을 정제할 시기를 선택할 때 ORM의 중요성을 강조합니다. 우리는 또한 더 큰 모델이 더 나은 개선을 생산한다는 점에 주목합니다.\n' +
      '\n' +
      '**글로벌 및 로컬 정제는 부정확한 초안의 유사한 백분율을 수정한다:** 정제가 필요할 때 우리의 정제가 얼마나 잘 수행되는지 이해하기 위해 그림 5의 테스트 세트에서 부정확한 초안에만 정제를 적용할 때 결과도 보고한다. 이 경우 글로벌 및 로컬 정제는 훨씬 더 잘 수행되어 GSM8K에서 평균 10%, SVAMP에서 8%의 전체 정확도를 향상시킨다. 이것은 정제사들이 정제하는 법을 배웠다는 것을 보여주는데, 그들은 단지 종종 언제 정제하는지 모른다.\n' +
      '\n' +
      '처음에는 글로벌 리파이닝이 로컬 리파이닝과 유사한 비율의 초안을 수정할 수 있다는 것이 다소 놀라운 일이다. 현지 정제는 \\(E\\)에서 추가 정보를 제공받으며, 아마도 글로벌 정제에 비해 성능이 엄격하게 향상될 것이다. 실제로 제공된 \\(E\\)는 불완전한 ORM/SORM에 의해 예측되어야 하므로 잡음이 많다. 우리는 레이블 정확도의 차이조차도 ORM과 SORM이 자명하지 않은 차이를 초래한다는 것을 안다.\n' +
      '\n' +
      '도 4: ORM, 균형 ORM, 및 SORM의 플롯은 동일한 SFT 학생(maj@1 = 0.36)으로 정확도를 재순위화한다. 참고: SORM 자체는 섹션 4에 설명된 바와 같이 균형 잡힌 단계 레이블 또는 일관된 검증자를 추가 전처리 단계로 사용하지 않는다. 두 단계를 모두 추가할 때 재순위화 성능이 ORM의 성능과 거의 일치하도록 크게 개선된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '벤치마크 Bo3 기준선과 비교할 때 GSM8K에서 여전히 약 8%의 상당한 개선을 볼 수 있다. SVAMP에서 재순위 Bo3는 훨씬 더 경쟁력 있는 기준선이며, 그 자체로 초안 정확도에 비해 큰 개선을 제공한다. 오라클 리랭커를 사용할 때 훨씬 더 큰 개선을 볼 수 있으며 13B 정제기는 GSM8K의 Bo3보다 11% 개선된다.\n' +
      '\n' +
      '##6 결론 및 향후 과제\n' +
      '\n' +
      '이 논문에서 우리는 LLM 추론을 정제하기 위해 _when_를 식별하는 것과 LLM 추론을 정제하기 위해 _where_를 식별하는 것 모두를 위한 보상 모델의 사용에 대해 연구한다. ORM 모델은 보다 쉬운 추론 작업에 대한 중간 단계의 정확도를 평가하는 것으로 어느 정도 일반화되지만, 학습 데이터 생성 정책 \\(\\pi\\)이 \\(\\pi^{*}\\)에서 더 멀리 떨어진 더 어려운 작업에 대해 어려움을 겪는다는 것을 발견했다. 그런 다음, SORM 모델을 학습하기 위해 사용되는 중간 단계 \\(S_{i}\\)에 대한 학습 레이블을 생성할 수 있도록 거부 샘플링과 후처리 과정을 통해 최적의 정책 \\(\\pi^{*}\\)을 근사화하는 것을 제안한다. 우리는 SORM이 ORM보다 중간 테스트 단계에서 더 잘 일반화되어 있지만 최종 답의 정확성을 희생해야 한다는 것을 발견했다. 그런 다음 ORM/SORM 학습 데이터를 재사용하여 글로벌/로컬 정제 모델을 학습한다. 우리는 각 유형의 정제 전략이 대체로 독특한 문제 세트를 해결하는 데 도움이 되어 최상의 성능을 위해 ORM 재순위를 통해 둘 다 결합할 수 있음을 발견했다.\n' +
      '\n' +
      '향후 연구는 다음과 같이 분류할 수 있다. 첫째, 지역 오류 비판의 신뢰성과 장황성을 개선하기 위해 _how_에 대한 정보를 더 제공하거나, 둘째, 지역 정유사들이 올바른 해결책을 생성하기 위해 사용하는 정보의 유형을 증가시키는 것이다. ORM과 SORM에 대한 우리의 연구는 단계 수준 추론을 검증할 때 개선의 여지가 크다는 것을 보여준다. 검증자 모델이 사고의 사슬을 생성하도록 허용하는 것은 약간의 이점을 제공하는 것으로 보인다(Dhuliawala et al., 2023). 도구들로 CoT를 추가로 증강 검증하는 것(Zhou et al., 2023)은 GPT-4가 MATH를 효과적으로 해결할 수 있게 한다(Hendrycks et al., 2021). 그러나 GPT-4가 문제를 해결하기 위해 도구에 얼마나 의존하는지는 불분명하며 실제로 도구를 사용하여 _why_a 단계에 대한 자체 이해도를 높이는 것은 잘못된 것이다.\n' +
      '\n' +
      '또 다른 유망한 방향은 알고리즘 증류(Laskin et al., 2022)로부터의 아이디어와 정신적으로 유사한 _in-context exploration_의 형태로서 반복적 정제를 다룬다. 여기서, 목적은 정제하기 위해 _how_를 파악하는 데 필요한 컨텍스트 내 모델 롤아웃의 수를 최소화하는 것이다. 이것은 또한 우리가 미래의 성공에 중요하다고 믿는 방향인 SOTA LLM의 탐사 능력을 증가시키는 것을 목표로 하는 작업과 밀접한 관련이 있다. 올바른 반복 로컬 자체 정제 전략은 모델이 이전에 나이브 iid 반복 샘플링으로 접근할 수 없었던 복잡한 행동에 접근할 수 있기를 희망할 수 있다.\n' +
      '\n' +
      '그림 6: 학생(Bo3) 기준선에서 수집한 3개의 샘플 중 탐욕스럽고 최상의 샘플과 비교하여 모든 초안에 대한 순위 조정 정확성. GSM8K에서 ORM을 사용한 재정렬 개선은 완벽한 재정렬기로 Bo3 기준선에서 최대 9% 및 최대 13% 개선된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Akyurek et al. [2023] Afra Feyza Akyurek, Ekin Akyurek, Aman Madaan, A. Kalyan, Peter Clark, D. Wijaya, and Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. In _Annual Meeting of the Association for Computational Linguistics_, 2023. URL [https://api.semanticscholar.org/CorpusID:258685337](https://api.semanticscholar.org/CorpusID:258685337).\n' +
      '* Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. _ArXiv_, abs/2108.07732, 2021. URL [https://api.semanticscholar.org/CorpusID:237142385](https://api.semanticscholar.org/CorpusID:237142385).\n' +
      '* Bai et al. [2022] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, John Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, E Perez, Jamie Kerr, Jared Mueller, Jeff Ladish, J Landau, Kamal Ndousse, Kamille Lukoitte, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem\'i Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, T. J. Henighan, Tristan Hume, Sam Bowman, Zac Hatfield-Dodds, Benjamin Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom B. Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. _ArXiv_, abs/2212.08073, 2022. URL [https://api.semanticscholar.org/CorpusID:254823489](https://api.semanticscholar.org/CorpusID:254823489).\n' +
      '* Besta et al. [2023] Maciej Besta, Nils Blach, Ale Kubek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. _ArXiv_, abs/2308.09687, 2023. URL [https://api.semanticscholar.org/CorpusID:261030303](https://api.semanticscholar.org/CorpusID:261030303).\n' +
      '* Chen[2023] Angelica Chen. 자연어 피드백으로 학습하여 코드 생성을 개선한다. _ ArXiv_, abs/2303.16749, 2023. URL[https://api.semanticscholar.org/CorpusID:257804798](https://api.semanticscholar.org/CorpusID:257804798)\n' +
      '* Chen et al. [2022] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. _ArXiv_, abs/2211.12588, 2022.\n' +
      '* Chollet [2019] Francois Chollet. 지능의 척도로요 ArXiv_, abs/1911.01547, 2019. URL[https://api.semanticscholar.org/CorpusID:207870692](https://api.semanticscholar.org/CorpusID:207870692).\n' +
      '* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa De, Henryk Michalewski, Kavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24:240:1-240:113, 2022. URL [https://api.semanticscholar.org/CorpusID:247951931](https://api.semanticscholar.org/CorpusID:247951931).\n' +
      '* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _ArXiv_, abs/2110.14168, 2021. URL [https://api.semanticscholar.org/CorpusID:239998651](https://api.semanticscholar.org/CorpusID:239998651).\n' +
      '* Dhuliawala et al. [2023] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. _ArXiv_, abs/2309.11495, 2023. URL [https://api.semanticscholar.org/CorpusID:262062565](https://api.semanticscholar.org/CorpusID:262062565).\n' +
      '* Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).\n' +
      '* Gou et al. [2019] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Largelanguage models can self-correct with tool-interactive critiquing. _ArXiv_, abs/2305.11738, 2023. URL [https://api.semanticscholar.org/CorpusID:258823123](https://api.semanticscholar.org/CorpusID:258823123).\n' +
      '* Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021a 수학 데이터 세트를 사용하여 수학적 문제 해결을 측정한다.\n' +
      '* Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. 상기 수학 데이터셋으로 수학 문제 풀이를 측정하는 단계; _ ArXiv_, abs/2103.03874, 2021b. URL[https://api.semanticscholar.org/CorpusID:232134851](https://api.semanticscholar.org/CorpusID:232134851)\n' +
      '* Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 대형 언어 모델은 아직 스스로 추론을 수정할 수 없다. _ ArXiv_, abs/2310.01798, 2023. URL[https://api.semanticscholar.org/CorpusID:263609132](https://api.semanticscholar.org/CorpusID:263609132)\n' +
      '* Laskin et al. (2022) Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. 알고리즘 증류를 이용한 상황 내 강화 학습. _ ArXiv_, abs/2210.14215, 2022. URL[https://api.semanticscholar.org/CorpusID:253107613](https://api.semanticscholar.org/CorpusID:253107613)\n' +
      '* Lee et al.(2023) Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz. Platypus: llms의 빠르고, 싸고, 강력한 정제. _ ArXiv_, abs/2308.07317, 2023. URL[https://api.semanticscholar.org/CorpusID:260886870](https://api.semanticscholar.org/CorpusID:260886870)\n' +
      '* Li et al. (2022) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, B. Chen, Jian-Guang Lou, and Weizhu Chen. 단계 인식 검증기로 언어 모델을 더 나은 추론자로 만듭니다. _Annual Meeting of the Association for Computational Linguistics_, 2022. URL[https://api.semanticscholar.org/CorpusID:259370847](https://api.semanticscholar.org/CorpusID:259370847).\n' +
      '* Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Re, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yukeksofonl, Nathan Kim, Neel Guha, Niladri S. Orr. 채터지, 오 Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, Yuta Koreeda. 언어 모델에 대한 전체론적 평가 ArXiv_, abs/2211.09110, 2022. URL[https://api.semanticscholar.org/CorpusID:263423935](https://api.semanticscholar.org/CorpusID:263423935)\n' +
      '* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 차근차근 확인해 봅시다. _ ArXiv_, abs/2305.20050, 2023. URL[https://api.semanticscholar.org/CorpusID:258987659](https://api.semanticscholar.org/CorpusID:258987659).\n' +
      '* Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 자기 정제: 자기 피드백으로 반복 정제, 2023.\n' +
      '* Mialon et al. (2023) Gregoire Mialon, Clementine Fourrier, Craig Swift, Thomas Wolf, Yann Andre LeCun, and Thomas Scialom. 가이아: 일반 비서들의 벤치마크입니다. 2023. URL[https://api.semanticscholar.org/CorpusID:265351664](https://api.semanticscholar.org/CorpusID:265351664)\n' +
      '* Mishra et al. (2022) Swaroop Mishra, Pan Lu, and A. Kalyan. 수학적 추론에 대한 통일된 벤치마크 2022. URL[https://api.semanticscholar.org/CorpusID:257405677](https://api.semanticscholar.org/CorpusID:257405677)\n' +
      '* OpenAI(2023) OpenAI. Gpt-4 기술 보고서입니다 ArXiv_, abs/2303.08774, 2023. URL[https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815).\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 인간의 피드백으로 지시를 따르도록 언어 모델을 훈련시키는 것 ArXiv_, abs/2203.02155, 2022. URL[https://api.semanticscholar.org/CorpusID:246426909](https://api.semanticscholar.org/CorpusID:246426909).\n' +
      '* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. nlp 모델은 정말 간단한 수학 단어 문제를 해결할 수 있습니까? 2021년입니다.\n' +
      '* Paul et al. (2023) Debjit Paul, Mete Ismayilkada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 정제기: 중간 표현에 대한 피드백을 추론한다. _ ArXiv_, abs/2304.01904, 2023. URL[https://api.semanticscholar.org/CorpusID:257921623](https://api.semanticscholar.org/CorpusID:257921623)\n' +
      '* Peyrard et al. (2021)David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. 보우먼 Gpqa: 대학원 수준의 구글 인증 q&a 벤치마크. _ ArXiv_, abs/2311.12022, 2023. URL[https://api.semanticscholar.org/CorpusID:265295009](https://api.semanticscholar.org/CorpusID:265295009).\n' +
      '* Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Ouyang Long, Jonathan Ward, and Jan Leike. 인간 평가자를 돕기 위한 자기 평가 모델. _ ArXiv_, abs/2206.05802, 2022. URL[https://api.semanticscholar.org/CorpusID:249626555](https://api.semanticscholar.org/CorpusID:249626555).\n' +
      '* Sawada et al. (2023) Tomohiro Sawada, Daniel Paleka, Alex Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, and Aran Komatsuzaki. Arb: 대형 언어 모델에 대한 고급 추론 벤치마크__ ArXiv_, abs/2307.13692, 2023. URL[https://api.semanticscholar.org/CorpusID:260155126](https://api.semanticscholar.org/CorpusID:260155126)\n' +
      '* Schick et al. (2022) Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: 협력 언어 모델. _ ArXiv_, abs/2208.11663, 2022. URL[https://api.semanticscholar.org/CorpusID:251765117](https://api.semanticscholar.org/CorpusID:251765117)을 포함할 수 있다.\n' +
      '* Silver et al. (2017) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, L. 시프레, 다산 쿠마란, 소어 그레이펠, 티모시 P. 릴리크랩, 카렌 시모얀, 데미스 하사비스. 일반적인 강화학습 알고리즘으로 셀프플레이에 의한 체스와 소기의 숙달. _ ArXiv_, abs/1712.01815, 2017. URL[https://api.semanticscholar.org/CorpusID:33081038](https://api.semanticscholar.org/CorpusID:33081038)\n' +
      '* Srivastava et al. (2017) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. 브라운, 아담 산토로, 아디티야 굽타, 아드리아 가리가-알론소, 아그네츠카 클루스카, 아토르 루코위츠, 악샤트 아가왈, 알레티아 파워, 알렉스 레이, 알렉스 워스타트, 알렉산더 W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Allicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Annasaheb Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Guottari, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Catherine Rathkopf, Chenlin Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, Chitara Mung, D. 페르시크, 대니 에르난데스, 단키 첸, 다프네 이폴리토, 다비드 길보아, 다비드 에르겐스, 데바요티 다탈리, 데바요티 다탈리, 데르겔리, 데르겔리, 데르겔리, 데르겔리, 탐, 데르겔리, 디에르흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐흐 Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua Tenenbaum, Joshua S. 룰, 조이스 두하, 카밀 카넬레르츠, 카렌 리브스쿠, 칼 크라우스, 카르틱 고팔라크리슈난, 카테리나 이그나티바, 카쟈 마커트, 코리 월리스 매튜슨, 크리스틴 치아풀포, 크세니아 슈카루타, 쿠마르 슈리드하르, 카일 맥도넬, 카일 리처드슨, 라리아 레이놀즈, 레오 가오, 리헴 두건, 루크 메츠, 루크 노블, 루드비히 슈센, 루이히 보스마, 마알 파루키, 마르코 사프, 마르코 테르 호베 라빗, 마티아스 하겐, 마티아스 슈베르트, 메디나 바이테미로바, 니클라스 무엔히토프, 니티시 시리시 케스카르, 니니티타 아이르, 니클라스 데커스, 니클라스 무엔히토프, 니티시 시리시 케스카르, 노아 피델, 올리버 장, 오마르 아흐니츠, 오마르 엘바키아, 미히히로 야스나가, 미히르 케일, 마이크 케인, 미케일 베벌라바시, 페이위안 랴오, 퍼시 리앙, 피터 창, 피터 에커슬리, 푸 몬히히아 황, 피밀코프스키, 피유시 S. 파틸, 푸야 페체슈쿠르, 프리티 올리, 차오주 메이, 칭루, 칭랑첸, 라빈 반자데, 레이첼 에타 루돌프, 래퍼 가브리엘, 라헬 하바커, 라몬 리스코, 라파엘 밀리에르, 리듬 가르크, 리처드 반스, 리프 A. 사우루스, 리쿠 아라카와, 로베 레이맥커스, 로버트 프랭크, 로한 시칸드, 로만 노박, 로난 레브라스, 로난 리우, 로완 제이콥스, 루이 장, 루슬란 살라쿠트디노프, 라이언 치, 라이언 리, 라이언 티한, 라이언 양, 사히브 싱, 사이프 M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. 새뮤얼 S. 보먼 Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi S. 함단, 샤론 저우, 샤샨크 스리바스타바, 셰리 시, 시카르 싱, 시마 아사디, 시샹 샤샨 구, 슈바 파친그, 슈함 토신왈, 샤얌 우파다이, 시아몰리마 데브나스, 시나다 샤케리, 사이먼 소르메이어, 시몬 멜지, 시바 레디, 스네하 프리실라 마키니, 이수환, 스펜서 토렌, 스리하르샤 하트와, 스타니슬라스 데하엔, 스테판 디비치, 스테파노 에르몬, 스텔라 바이더만, 스테판 프라사드, 스티븐 티 피안타도시, 스튜어트 M. 시베르, 서머 미셔글리, 스베틀라나 키리첸코, 스와루프 미쉬라, 탈 린젠, 탈 슈스터, 타오 리, 타오 유, 타리크 알리, 테오 린 우, 테오 데스보르드 왕, 티베리우스 니키닐리, 티모 쉬크, 티모페이 코르네프, 토바이어스 게르스텐베르그, 트렌턴 장, 트리샤르 니예르, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프, 타일러 슈르네프 모방 게임을 넘어: 언어 모델의 능력을 정량화하고 추론합니다. 2022. URL[https://api.semanticscholar.org/CorpusID:263625818](https://api.semanticscholar.org/CorpusID:263625818)\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjal Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Cynthia Kardas, Vedan Helkeon Lu, Saghar Hosseini, Rui Hou, Madian Lavrog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.\n' +
      '* Uesato et al. (2022) Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, L. 왕, 안토니아 크레스웰, 제프리 어빙, 이리나 히긴스 과정 및 결과 기반 피드백으로 수학 단어 문제를 해결합니다. _ ArXiv_, abs/2211.14275, 2022. URL[https://api.semanticscholar.org/CorpusID:254017497](https://api.semanticscholar.org/CorpusID:254017497).\n' +
      '* Wang et al. (2022) Peiyi Wang, Lei Li, Zhihong Shao, R. X. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, 및 Zhifang Sui. Mathem-shepherd: 인간의 주석 없이 llms를 단계별로 검증하고 강화한다. _ ArXiv_, abs/2312.08935, 2023a. URL[https://api.semanticscholar.org/CorpusID:266209760](https://api.semanticscholar.org/CorpusID:266209760)\n' +
      '* Wang et al. (2022) Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O\'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 셰퍼드: 언어 모델 생성을 위한 평론가, 2023b.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 사고 유발의 사슬은 큰 언어 모델에서 추론을 이끌어낸다. _ ArXiv_, abs/2201.11903, 2022.\n' +
      '* Welleck et al. (2022) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 자기 수정을 학습하여 시퀀스를 생성하는 단계; _ ArXiv_, abs/2211.00053, 2022. URL[https://api.semanticscholar.org/CorpusID:253244506](https://api.semanticscholar.org/CorpusID:253244506).\n' +
      '* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 반응: 추론과 언어 모델에서의 연기의 동기화. _ ArXiv_, abs/2210.03629, 2022. URL[https://api.semanticscholar.org/CorpusID:252762395](https://api.semanticscholar.org/CorpusID:252762395)\n' +
      '* Yao et al. (2022)Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. 그리피스, 원 카오, 카틱 나라심한 생각의 나무: 큰 언어 모델을 사용하여 문제를 해결합니다. _ ArXiv_, abs/2305.10601, 2023a. URL[https://api.semanticscholar.org/CorpusID:258762525](https://api.semanticscholar.org/CorpusID:258762525)\n' +
      '* Yao et al. (2023) Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phi Thi Mui, Haiquan Wang, Caiming Xiong, and Silvio Savarese. 리트로포머: 정책 기울기 최적화를 가진 후향적 대형 언어 에이전트 _ ArXiv_, abs/2308.02151, 2023b. URL[https://api.semanticscholar.org/CorpusID:260611249](https://api.semanticscholar.org/CorpusID:260611249)\n' +
      '* Zhou et al. (2023) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. 코드 기반 자기 검증이 적용된 gpt-4 코드 해석기를 사용하여 도전적인 수학 단어 문제를 해결한다. _ ArXiv_, abs/2308.07921, 2023. URL[https://api.semanticscholar.org/CorpusID:260900008](https://api.semanticscholar.org/CorpusID:260900008).\n' +
      '* Ziegler et al. (2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 인간의 기호에 따라 언어 모델을 미세 조정합니다. _ ArXiv preprint arXiv:1909.08593_, 2019.\n' +
      '\n' +
      '## 부록 하이퍼파라미터\n' +
      '\n' +
      '각 훈련 작업에 사용되는 훈련 하이퍼파라미터 목록은 표 4를 참조하십시오.\n' +
      '\n' +
      '## 추론을 위한 부록 BRL\n' +
      '\n' +
      '가장 좋은 학생부터 시작하기 위해 우리는 전문가 반복을 사용하여 기본 모델을 미세 조정한다. EI 미세 조정 모델에 대한 maj@1(그리디), maj@96, Rerank@96 및 pass@96 점수는 표 5를 참조하십시오.\n' +
      '\n' +
      '## (글로벌) 미세화를 위한 부록 CRL\n' +
      '\n' +
      '**Setup:** GSM8K 벤치마크에서 개선을 위한 PPO 대 EI의 유용성을 비교한다. EI 모델을 훈련하기 위해 우리는 3.2\\(K=96\\)절에서 훈련된 SFT({}^{2}\\) 모델을 기차 집합에서 프롬프트당 샘플링한다. 그런 다음 고정된 질문에 대한 모든 틀린 해와 정답을 짝짓기하여 학습 튜플을 형성한다. 그런 다음 Llama-2 7B로부터 단일 에폭에 대한 표준 교차 엔트로피 손실로 \\(p(A_text{correct}}|Q,A_text{wrong}})\\)을 예측하기 위해 미세 조정한다. 우리는 5e-5가 5e-7로 부패하는 초기 학습률을 사용한다.\n' +
      '\n' +
      '우리는 위의 섹션 2에서 사용된 SFT\\({}^{2}\\) 체크포인트로부터 PPO 모델을 초기화하고 GSM8K의 SFT 체크포인트로부터 미세 조정될 때와 동일한 PPO 파라미터를 사용한다. 참조용으로 부록에 포함된 단일 예는 개선을 위해 모델을 프롬프트하는 데 사용된다. 학습하는 동안 학생 모델은 질문\\(Q\\)과 초안\\(D\\)이 주어지며, 여기서 초안은 SFT 모델에 의해 생성되고, 개선\\(R\\)을 생성하는 작업이 수행된다. R=\\mathbf{1_{is\\_correct(R)}}-\\mathbf{1_{is\\_correct(D)}}}\\을 롤아웃 종료 시 희소 보상으로 제공한다. 또한 ORM에서 얻은 점수를 혼합하여 \\(R=max(\\mathbf{1_{is\\_correct(R)}}-\\mathbf{1_{is\\_correct(D)}},ORM(R)-ORM(D))\\의 최종 보상을 제공하는 실험을 하였다.\n' +
      '\n' +
      '글로벌 정제 결과\n' +
      '\n' +
      'GSM8K 테스트의 질문과 SFT({}^{2}\\)에 의해 생성된 초안을 사용하여 테스트 세트에 대한 모든 개선 모델을 평가한다. 결과는 표 6에 보고되어 있으며 테스트 시간에서의 샘플링은 탐욕스럽게 수행되므로 maj@1 정확도만 보고된다. 우리는 추가로 기준선으로 라마-2 7B를 촉발한 1발을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r}  & Expert Iteration & (S)ORM & Refiners \\\\ \\hline Epochs & 4 & 1 & 1 \\\\ max lr & 2e-5 & 2e-6 & 2e-5 \\\\ min lr & 2e-7 & 2e-7 & 2e-7 \\\\ Batch size & 128 & 256 & 128 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 모든 훈련 작업에 대한 하이퍼파라미터. 코사인 감쇠 lr 스케줄은 모든 경우에 사용된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r}  & maj@1 & maj@96 & Rerank@96 & pass@96 \\\\ \\hline GSM8K & & & & \\\\ \\hline SFT 7B & 0.41 & 0.47 & 0.54 & 0.72 \\\\ SFT 13B & 0.48 & 0.55 & 0.68 & 0.84 \\\\ EI\\({}_{2}\\) 7B & 0.485 & 0.55 & 0.64 & 0.8 \\\\ El\\({}_{2}\\) 13B & 0.53 & 0.59 & 0.71 & 0.88 \\\\ \\hline SVAMP & & & & \\\\ \\hline EI\\({}_{5}\\) 7B & 0.58 & 0.6 & 0.62 & 0.70 \\\\ EI\\({}_{5}\\) 13B & 0.69 & 0.75 & 0.78 & 0.93 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: GSM8K 및 SVAMP에 대한 EI 미세 조정 모델에 대한 성능 메트릭. 첨자 \\(n\\)은 maj@1 점수의 수렴까지 전문가 반복의 라운드 수를 나타낸다. 재랭킹되는 모델의 샘플로 훈련된 ORM을 통해 재랭킹된다.\n' +
      '\n' +
      '모든 모델은 우리의 최고의 정제 모델을 언제 정제해야 하는지 학습하는 데 어려움을 겪는다. 그러나 EI는 SFT 기준선보다 3%만 개선되며 PPO는 전혀 개선되지 않는다. 두 모델 모두 언제 정제할지를 정확하게 결정하는 데 어려움을 겪기 때문이다. 종종 EI 모델은 올바른 초안을 잘못 정제하도록 선택한다. 프롬프트 사전 훈련된 모델은 GSM8K에 대해 훈련되지 않았으므로 언제 정제해야 하고 올바른 대체 정제물을 생산하는 방법에 어려움을 겪으면서 훨씬 더 악화된다.\n' +
      '\n' +
      'PPO 모델은 단순히 최종 답변 초안을 반환하는 것으로 붕괴되며, 적어도 올바른 초안을 잘못 정제하는 것에 대한 부정적인 보상을 회피한다. 프롬프트된 기준선은 또한 이 복사 동작을 나타내며, 이는 사소한 정확도의 대부분을 차지한다. ORM을 추가 보상으로 사용하고, 정확한 초안을 정제하기 위한 페널티를 제거하며, 학생이 초안\\(D\\)과 정제\\(R\\)을 모두 생성하도록 하여 이러한 퇴화 행동을 방지하기 위한 대체 RL 설정을 실험한다. 그러나 모든 경우에 모델의 복사 편향은 탐사를 계속 제한하여 초기 초안에 대한 개선의 붕괴를 초래한다.\n' +
      '\n' +
      '**토론:** 위의 결과는 몇 가지 고장 모드를 강조합니다. 첫째, 모델은**를 정제할 시기를 결정하는 데 어려움을 겪으며, 종종 전혀 정제하지 않는 것을 기본으로 한다. 둘째, 모델이 올바르게 정제할 위치를 선택할 때 여전히 정제할 위치를 아는 데 어려움을 겪는다. 마지막으로, 언제 어디서 정제해야 하는지 알더라도 모델은 여전히 정제할 **방법**을 결정해야 한다.\n' +
      '\n' +
      '이를 개선하기 위해 각 고장 모드를 해결하기 위해 고유 모델을 사용하여 문제를 분해하는 방법을 제안한다. 다행히도, 언제 정제할지를 결정하는 것은 최종 답변이 정확할 때를 예측하도록 명시적으로 훈련된 ORM에 의해 자연스럽게 처리될 수 있다. 또한 로컬 정제할 때 SORM을 사용하여 정제할 위치를 식별할 수 있다. 이것은 이제 우리가 다듬는 방법을 결정하기 위해 훈련하는 다듬는 모델만 필요로 하므로 작업이 상당히 쉬워진다.\n' +
      '\n' +
      '## 부록 D 미스. 재순위를 위한 목적\n' +
      '\n' +
      'Lightman et al. (2023)에서 PRM은 각 단계 \\(S_{i}\\)에 대한 \\(P(\\mathsf{Good}|S_{i})\\)를 추정하고 제품을 취함으로써 재순위화에 사용된다. 이에 영감을 받아 최종 답변 재순위화를 수행할 때 SORM 중간 단계 추정에 대해 여러 가지 다른 가중치를 실험했다. 형태\\(S=(S_{1},...,S_{L})\\)의 해법에 대해 이들 휴리스틱이 포함된다:\n' +
      '\n' +
      '1. 최종: \\(ORM(S_{L})\\)\n' +
      '2. 평균: \\(\\frac{1}{L}\\sum_{i=1}^{L}ORM(S_{i})\\)\n' +
      '3. 가중평균: \\(\\sum_{i=1}^{L}\\frac{1}{L-i-1}ORM(S_{i})\\)\n' +
      '4. Min: \\(min_{i\\in[L]ORM(S_{i})\\)\n' +
      '5. 제품: \\(\\prod_{i=1}^{L}ORM(S_{i})\\)\n' +
      '6. 두 번째 평균: \\(\\frac{ORM(S_{L-1})-ORM(S_{L})}{2}\\)\n' +
      '\n' +
      '결과는 그림 7에 표시되어 있으며, 전반적으로 최종 ORM 추정치만 사용하면 두 번째 평균이 가까운 초에 들어오는 최상의 재순위 정확도를 제공한다. 가중 평균은 최소 ORM 추정치를 취하더라도 다른 모든 전략보다 현저히 낮다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r}  & Accuracy \\\\ \\hline SFT & 0.36 \\\\ Prompted & 0.15 \\\\ PPO & 0.36 \\\\ ORM PPO & 0.36 \\\\ EI & 0.39 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 프롬프트, PPO 및 EI 모델에 대한 전역 정제 정확도. 주, maj@1 정확도는 정확한 SFT 생성 초안과 부정확한 SFT 생성 초안을 모두 포함하는 전체 테스트 세트에 대해 보고된다.\n' +
      '\n' +
      '## 부록 EORM 및 SORM 엑스트라 모델 일반화\n' +
      '\n' +
      'ORM과 SORM은 모두 데이터를 생성하는 학생\\(\\pi\\)에 과도하게 적합하다는 징후를 보인다. GSM8K 열차에서 평가했을 때 7B ORM 모델은 올바른 솔루션의 42%를 잘못된 솔루션으로 잘못 분류했다. 이를 보다 면밀히 검토하기 위해 EI(전문가 반복 학습)와 SFT(지도 미세 조정)의 두 가지 기본 학생 모델을 취하고, 두 모델을 사용하여 각각 \\(ORM_{\\text{EI}}\\) 및 \\(ORM_{\\text{SFT}}\\)에 대한 학습 데이터를 생성한다. 그런 다음 각 모델에 의해 생성된 테스트 세트에 대해 두 ORM을 모두 평가한다. 결과는 표 7에 보고되어 있으며, 반대 학생 모델에 의해 생성된 테스트 데이터 세트에서 두 ORM 모두 낮은 성능을 나타낸다.\n' +
      '\n' +
      '## 부록 F 대조 vs. 클래식 RMs\n' +
      '\n' +
      'ORM과 SORM은 모두 각 중간 단계 \\(S_{i}\\)에서 좋은 레이블의 확률을 예측하기 위해 분류기로 훈련된다. 그러나 RLHF에서는 솔루션보다 선호도 비교만 있다. 따라서 RLHF 보상 모델은 종종 대비 손실\\(-log(\\sigma(RM(y_text{good}})-RM(y_text{bad}))을 통해 학습된다. 추론 설정에서 대조적 보상 모델의 사용을 탐색하여 재순위화 성능을 ORM과 비교한다. 훈련 데이터는 온도 T = 1.0에서 훈련 프롬프트당 K = 96 롤아웃을 갖는 7B SFT 학생 모델로부터 샘플링된다. ORM 단계 레이블은 보통의 방식으로 할당되며, \\(l_{L}=1\\)과 그렇지 않으면 \\(l_{i}=0\\)인 경우 \\(S_{i}\\)으로 설정된다. 선호 쌍을 구성하기 위해 동일한 프롬프트에 대해 최대 동일한 수의 양의 해와 음의 해를 선택하고 해를 반복하지 않고 쌍(S_{\\text{good}},S_{\\text{bad})을 형성한다. 최종 토큰의 대조 손실만 역전파됩니다. 그런 다음 분류기 ORM과 대조적 ORM 모두에 의해 할당된 점수를 사용하여 테스트 세트의 솔루션을 재정렬한다. 분류기 ORM은 0.56 재순위 정확도를 얻는 반면 대조 ORM은 0.47을 얻음으로써 분류기가 엄격하게 더 나은 재순위임을 시사한다.\n' +
      '\n' +
      '## SORM 데이터 생성 방법의 부록 G 정확도\n' +
      '\n' +
      'SORM 데이터 생성 프로세스는 위양성과 위음성을 모두 겪을 것이다. 잘못된 긍정은 학생 모델이 문제를 잘못 해결하지만 올바른 최종 답을 얻을 때 발생할 수 있다. 접두사(P_{i}\\)가 논리적으로 타당함에도 불구하고, 기각표집된 학생이 접두사 \\(P_{i}\\)로부터 문제를 해결할 수 없을 때 위음성이 발생한다. SORM 단계 검증 프로세스의 정확성을 검증하기 위해 GSM8K에서 몇 가지 모델 생성 솔루션에 핸드라벨을 지정하고 지면이 얼마나 좋은지 계산합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r}  & EI Test Accuracy & SFT Test Accuracy \\\\ \\hline \\(ORM_{\\text{EI}}\\) & 0.64 & 0.51 \\\\ \\(ORM_{\\text{SFT}}\\) & 0.58 & 0.56 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 평가\\(ORM_{\\text{EI}}\\) 및 \\(ORM_{\\text{SFT}}\\) 교차 일반화.\n' +
      '\n' +
      '그림 7: 최종 재순위 점수를 결정하기 위한 휴리스틱의 비교.\n' +
      '\n' +
      '진실 레이블은 생성된 레이블과 정렬됩니다. (n=64\\)과 총 257단계에 걸쳐 SORM 데이터 레이블이 94%의 지상진실과 일치함을 알 수 있다.\n' +
      '\n' +
      '## 부록 H PRM 데이터의 다른 소스 혼합\n' +
      '\n' +
      '우리는 Lightman et al.(2023)의 MATH 데이터셋에 대한 PRM 데이터에 혼합을 추가로 실험한다. 우리는 Llama-2 7B를 훈련하여 각 단계에 대해 음, 중립 및 좋은 라벨을 예측하며 "나쁜" 단계는 올바르지 않고 "중립" 단계는 전진 또는 후진 진행하지 않으며 "좋은" 단계는 문제를 해결하는 데 정확하고 유용한다. 결과 PRM은 MATH PRM 테스트 세트에서 0.91의 정확도를 얻는다. 그러나 PRM은 EI 생성 테스트 세트에서 0.58의 정확도만을 얻으면서 최종 정답 정확성 예측 변수로서 제대로 전달되지 않는 것으로 나타났다.\n' +
      '\n' +
      '## 부록 I SORM을 이용한 자가 지도 학습\n' +
      '\n' +
      'SORM 데이터세트 생성 프로세스는 상당히 시끄러울 가능성이 있다. 저품질 샘플은 다운스트림 SORM의 성능에 직접적인 영향을 미치므로 데이터 세트 품질을 개선하는 것이 중요하다. 잡음 이상치를 제거하기 위해 SORM 자체 모니터링을 통해 SORM 데이터 세트의 버전을 필터링했다. 각 훈련 쌍 \\((Q,P_{i},l_{i})\\)에 대해, \\(Q\\)은 질문이고, \\(P_{i}=(S_{1},...,S_{i})\\)은 \\(i\\) 단계가 있는 솔루션 프리픽스이며, \\(l_{i}\\)은 정확성 레이블이며, \\(SORM((Q,P_{i}))\\을 적용한다. 이는 자체 지도 레이블 \\(l^{\\prime}_{i}=\\mathbf{1}_{SORM((Q,P_{i}))>0.5}\\을 생성한다. 그런 다음 모든 훈련 샘플을 \\(l^{\\prime}_{i}\\neq l_{i}\\)으로 필터링한다.\n' +
      '\n' +
      '우리는 1 에폭에 대해 훈련된 SORM 체크포인트와 2 에폭에 대해 훈련된 다른 SORM 체크포인트로 SORM 데이터 세트를 필터링한다. 첫 번째 모델인 \\(\\text{SORM}_{1}\\)은 SORM 테스트 세트에서는 75%의 정확도를 보였으나 SORM 열차 세트에서는 91%의 정확도를 보였다. \\text{SORM}_{1}\\) (\\text{SORM}_{2}\\)는 78%의 시험결과를 얻었으나, 95%의 시험결과를 얻었다. 이는 열차 집합에 지나치게 적합할 수 있으므로, 필터링된 두 데이터 집합에 대해 새로운 SORM 모델을 학습시킨다. (\\text{SORM}^{\\prime}_{1}\\), \\(\\text{SORM}_{1}\\)로 필터링된 SORM 데이터에 대해 학습한 결과 79%의 정확도를 얻었다. (\\text{SORM}^{\\prime}_{2}\\), \\(\\text{SORM}_{2}\\)으로 필터링된 SORM 데이터로 훈련된 SORM 데이터는 동일해진다.\n' +
      '\n' +
      '## 부록 J값 정제\n' +
      '\n' +
      '이 작업의 본문에 사용된 지역 개선 전략은 논리적 오류가 있는 단계를 찾으려는 비판을 사용한다. 이는 정제 모델의 능력에 대해 피드백 불가지론을 부여하는 _process-based refinement_의 한 유형으로 해석될 수 있다. 보다 정교한 형태의 피드백은 모델의 기본 기능을 고려하여 이 특정 학생이 성공할 가능성을 극대화할 수 있다.\n' +
      '\n' +
      '학생에게 특정한 피드백을 제공하는 대안적인 정제 전략 중 하나는 _value-based 정제_이다. 값 기반 정제 모델은 모델에 대해 가장 높은 값을 갖는 솔루션에서 단계에서 "[BAD]" 토큰의 형태로 피드백을 수신한다. 단계 \\(S_{i}\\)의 값을 회상하면 모형이 \\(S_{i}\\)으로부터 정답을 얻을 확률이다. 이는 가장 높은 값 단계 \\(S_{i}\\) 이후의 단계 \\(S_{i+1}\\)이 반드시 오류를 포함하지 않을 수 있으므로 공정 기반 피드백과 동일한 것은 아니다. 대신에, \\(S_{i+1}\\)은 예를 들어, 정확하게 나누는 데 어려움을 겪는 모델과 나눗셈을 사용하는 것과 같이 문제를 해결하기 위해 시도할 수 있는 어려운 방법이다.\n' +
      '\n' +
      '값 기반 정제 모델을 학습한다. 또한 ORM은 \\(P_{i}=(S_{1},...,S_{i})\\이 있는 접두사에 대해 \\(ORM(P_{i})\\approx V^{\\pi}(S_{i})\\)이 되도록 데이터 생성 정책 \\(\\pi\\)의 값 함수를 직접 추정한다. 추론 과제에서 학생 모델\\(\\pi\\)이 주어지면, 각 프롬프트를 \\(\\tau_{\\text{train}}\\)\\(K=96\\)번 샘플링하여 ORM 훈련 세트\\(\\mathcal{D}_{ORM}\\)을 생성한다. 우리는 ORM을 분류기로 훈련하고, 중간 단계 레이블 \\(l_{i}=l_{L}\\)을 설정하며, 여기서 \\(l_{L}=\\texttt{is\\_correct}(\\texttt{S})\\이다.\n' +
      '\n' +
      '이와 같이 생성된 SORM 데이터세트\\(\\mathcal{D}_{\\text{vrefine}\\)를 정책\\(\\pi\\)과 거부 샘플링을 이용하여 재사용하여 가치기반 정제 데이터세트\\(\\mathcal{D}_{\\text{SORM}\\)을 구성한다. 표본 \\(S=(S_{1},...,S_{L})\\)에 대해 가장 정확한 검증 롤아웃 \\(v^{j}_{i}\\)을 갖는 단계를 선택함으로써 가장 높은 값 \\(S_{i}\\)을 식별한다. 그런 다음 첫 번째 단계가 \\(S_{i+1}\\)와 다른 검증 롤아웃 중 하나를 개선 개선으로 선택한다. 이것은 값-정제 트레이닝 쌍 \\((Q,S,R,C)\\)을 형성하며, 여기서 \\(C\\)은 \\(S\\)에서 단계 \\(S_{i+1}\\) 전에 삽입된 "[BAD]" 토큰이다. 그런 다음 표준 교차 엔트로피 손실로 \\(p(R|Q,S,C)\\)을 최소화하여 값 기반 로컬 정제 모델을 학습한다.\n' +
      '\n' +
      '실제로 GSM8K에서 LLama-2 7B EI를 \\(\\pi\\)로 사용하여 모든 다운스트림 모델 및 데이터 세트를 생성한다.\n' +
      '\n' +
      '**결과:**을 평가하기 위해 우리는 최고의 SFT 체크포인트에서 GSM8K로 훈련된 7B EI 모델을 사용한다. GSM8K 테스트 세트에서 탐욕스럽게 샘플 솔루션 초안을 작성하여 값 기반 로컬 정제 모델에 대한 \\((Q,D)\\) 테스트 세트를 형성한다. 그런 다음 ORM을 사용하여 각 초안의 가장 높은 값 단계 \\(S_{i}\\)에 레이블을 지정하고 "[BAD]" 토큰을 \\(S_{i+1}\\)의 접두사로 배치한다.\n' +
      '\n' +
      '우리는 공정 기반 SORM 개선의 성능을 직접 비교하여 잘못된 초안에 대해서만 평가한다. 값 기반 개선은 잘못된 초안의 14%를 수정하는 반면 SORM 기준선은 21%를 수정한다. 놀랍게도, 글로벌 정제조차도 가치 기반 정제보다 6% 더 우수하다. 이것은 중간 ORM 추정치가 테스트 데이터에서 상당히 시끄럽다는 것을 암시하기 위해 다시 이것을 취한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>