<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# OpenMoE: Open Mixture-of-Experts 언어 모델에 대한 초기 노력\n' +
      '\n' +
      '**Fuzhao Xue\\({}^{1}\\)** **Zian Zheng\\({}^{1}\\)** **Yao Fu\\({}^{2}\\)** **Jinjie Ni\\({}^{1}\\)** **Zangwei Zheng\\({}^{1}\\)**\n' +
      '\n' +
      '**왕춘슈주\\({}^{3}\\)***양유\\({}^{1}\\)***\n' +
      '\n' +
      '싱가포르 국립대학교\n' +
      '\n' +
      '에든버러대학교\n' +
      '\n' +
      '\\({}^{3}\\)ETH Zurich\n' +
      '\n' +
      'Email: f.xue@u.nus.edu\n' +
      '\n' +
      '싱가포르 국립대학교\n' +
      '\n' +
      '에든버러대학교\n' +
      '\n' +
      '\\({}^{3}\\)ETH Zurich\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '오픈 소스 커뮤니티가 Mixture-of-Experts(MoE) 기반 대규모 언어 모델(LLM)에 대한 더 나은 이해를 돕기 위해 650M에서 34B 매개변수에 이르기까지 완전히 오픈 소스되고 재현 가능한 디코더 전용 MoE LLM의 시리즈인 OpenMoE를 훈련하고 출시하며 최대 1T 토큰에 대해 훈련한다. 우리의 조사는 MoE 기반 LLM이 고밀도 LLM보다 더 유리한 비용 효율성 트레이드오프를 제공할 수 있음을 확인시켜 향후 LLM 개발에 대한 잠재적 효과를 강조한다.\n' +
      '\n' +
      '본 연구의 한 가지 더 중요한 기여는 오픈모E 모델 내의 라우팅 메커니즘에 대한 심층 분석이며, 컨텍스트 독립 특성화, 초기 라우팅 학습 및 끝으로의 드롭의 세 가지 중요한 발견으로 이어진다. 우리는 MoE 모델의 라우팅 결정이 주로 컨텍스트 관련성을 최소화하면서 토큰 ID에 기반한다는 것을 발견했다. 토큰 대 전문가 할당은 사전 훈련 단계에서 초기에 결정되며 크게 변경되지 않는다. 이러한 불완전한 라우팅은 특히 다중 턴 대화와 같은 순차적인 작업에서 성능 저하를 초래할 수 있으며, 여기서 시퀀스에서 나중에 나타나는 토큰은 삭제될 가능성이 더 높다. 마지막으로 위에서 언급한 관찰과 분석을 기반으로 설계를 다시 생각해 본다. 향후 MoE LLM 개발을 촉진하기 위해 우리가 발견한 문제를 완화하고 기성 MoE LLM 디자인을 더욱 개선하기 위한 잠재적인 전략을 제안한다.3\n' +
      '\n' +
      '[FOOTNOIntroduction\n' +
      '\n' +
      'LLM(Large Language Model)은 다양한 NLP 과제[27, 35]에서 괄목할 만한 성과를 보였으며, ChatGPT, Bard, Copilot 등의 챗봇 애플리케이션을 통해 우리 일상의 일부가 되기도 하였다. 그러나 LLM은 훈련과 추론 모두에서 계산 비용이 많이 든다. LLM이 점점 더 널리 보급됨에 따라 계산 리소스를 비례적으로 증가시키지 않고 성능을 향상시키는 것은 중요한 과제이다. 이러한 도전에 응답하여, Fedus _et al_. [15] 및 Riquelme _et al_. [36] Mixture-of-Experts(MoE)는 추가적인 계산 오버헤드가 거의 없이 변압기의 훈련 가능한 파라미터를 확장하기 위해 제안했다. GLaM[14] 및 ST-MoE[59]와 같은 MoE 기반 언어 모델의 최근 발전은 다양한 작업에서 우수한 성능을 입증했다. 그러나 OpenMoE가 출시되기 전에는 조 단위의 다양한 데이터 세트로 훈련된 오픈소스 MoE 언어 모델이 거의 없었다.\n' +
      '\n' +
      '본 연구에서는 기존의 LLM 훈련 프레임워크 내에서 디코더 전용 MoE 모델을 훈련하기 위한 첫 번째 시도 솔루션을 제공하는 세 가지 주요 목표를 제시한다. (2) MoE 라우팅 메커니즘에 대한 심층 분석을 수행하여 연구 커뮤니티에 MoE 기반 LLM의 행동과 잠재적 한계에 대한 더 깊은 통찰력을 제공한다. (3) 향후 MoE LLM 개발을 위한 발판을 마련하기 위하여. 이러한 초기 노력을 통해 오픈 소스 MoE 커뮤니티의 성장을 자극하고 가속화하는 것을 목표로 한다.\n' +
      '\n' +
      '**OpenMoE 릴리즈.** 먼저, 오픈 소스 MoE 기반 LLM의 시리즈인 OpenMoE를 릴리즈한다: (1) OpenMoE-Base/16E: 디버깅 목적을 위한 0.65B 파라미터를 갖는 소형 모델. 16E는 MoE 계층당 16명의 전문가를 의미한다; (2) OpenMoE-8B/32E: 이 변형은 총 8B 파라미터를 특징으로 하며, 트랜스포머 블록에서 토큰당 2B 파라미터를 활성화하고 1조 토큰 이상에서 사전 훈련된다; (3) OpenMoE-8B/32E-Chat, OpenMoE-8B/32E의 채팅 버전, WildChat [2] 데이터세트의 100K 서브세트로 미세 조정된다; (4) OpenMoE-34B/32E: 더 큰 스케일 모델, 트랜스포머 블록에서 토큰당 6B 파라미터를 활성화하고 200B 토큰으로 훈련되어 접근 방식의 확장성에 대한 증거 역할을 한다. 자세한 구성은 부록 B에서 찾을 수 있으며 우리의 OpenMoE-8B/32E 모델은 OpenLLaMA-3B[18] 및 TinyLLaMA-1.1B[54]와 유사한 성능을 달성했으며 두 개의 조밀한 개방형 LLM은 더 높은 훈련 비용을 사용했다. 특히, MT-Bench[56]에서 OpenMoE-8B/32E-Chat은 단일 턴 대화에서 두 개의 조밀한 LLM을 크게 능가했다. 또한, 각각 이전보다 200B 더 많은 토큰으로 훈련된 OpenMoE-8B/32E의 5개의 중간 체크포인트를 공개하여 향후 연구를 지원하고 장려한다. 2절과 3절에서는 OpenMoE의 설계, 교육 내용 및 평가 결과에 대해 논의한다.\n' +
      '\n' +
      '고급 훈련 전략 탐색.** 우리의 연구 노력의 일환으로, 우리는 LLM 훈련에서 더 진보된 기술을 탐구하는데 전념한다: (1) 사내 또는 텍스트가 지배하는 오픈 소스 데이터에 대한 훈련 모델의 일반적인 관행과 달리, 사전 훈련 초기 단계에서 최대 52.25%를 구성하는 상당한 비율의 코드로 OpenMoE를 훈련한다; (2) 기존의 다음 토큰 예측 훈련 목표를 넘어 이전 작업에서 입증된 효과와 코딩 데이터와의 양호한 정렬에 의해 동기화된 UL2 훈련 목표[45]를 조사한다. 우리는 우리 모델의 성능이 수용 가능하지만 우리의 기대를 크게 초과하지 않는다는 것을 인정하며, 이는 일부 차선책 설계 선택에 기인할 수 있다. 그럼에도 불구하고, 우리는 이 탐색적 작업이 특히 이러한 미개척 기술의 잠재력과 효과를 평가하는 데 있어 오픈 소스 커뮤니티에 상당한 가치를 제공한다고 믿는다.\n' +
      '\n' +
      '**MoE Routing In-depth.** MoE가 효과적인 반면, MoE가 왜 좋은 성능을 보이는지에 대한 연구는 부족한 실정이다. 높은 수준에서 MoE는 조밀한 매개변수보다 더 많은 훈련 가능한 매개변수를 도입한다. 파라미터 수를 스케일링할 때 FLOP를 고정적으로 유지하기 위해 MoE는 각 토큰을 소수 전문가에게 희소하고 적응적으로 할당하는 라우팅 계층을 적용한다. 희박한 전문가 선택의 이러한 과정은 MoE의 기능에 매우 중요하다. 불행히도, 라우팅 결정[26, 31, 36, 40, 59]을 간략하게 시각화한 기존 문헌에도 불구하고, 특히 다양한 도메인의 데이터 세트를 혼합하여 훈련된 ChatGPT 후 LLM의 경우 라우터가 어떻게 작동하고 라우팅 결정이 MoE 모델에서 결과에 미치는 영향에 대한 명확한 이해가 아직 없다. 본 연구에서는 도메인, 언어, 태스크, 토큰 등 다양한 분류법을 기반으로 이 문제를 연구한다. 주요 연구 결과는 다음과 같다. (1) **Context-independent Specialization**: MoE는 유사한 토큰 수준 의미론에 기초하여 토큰을 단순히 클러스터링하는 경향이 있는데, 이는 문맥에 관계없이 특정 토큰이 특정 전문가에게 라우팅될 가능성이 더 높다는 것을 의미한다; (2) **Early Routing Learning**: Token ID 라우팅 전문화는 사전 훈련 초기에 확립되고 대부분 고정된 상태로 유지되어, 훈련 내내 동일한 전문가에 의해 일관되게 토큰이 처리되며; (3) **Drop-towards-the-End**: 각 전문가가 고정된 최대 용량을 가지므로, 시퀀스에서 나중에 나타나는 토큰은 이미 용량에 있는 경우 드롭될 위험이 더 크다는 것을 의미한다. 이 문제는 명령어 조정 데이터 세트에서 더 심각하다. 이러한 데이터 세트는 종종 사전-트레이닝 데이터와 비교하여 도메인 갭을 나타내며, 이는 초기 사전-트레이닝 동안 확립되고 공고화된 균형 토큰 할당 전략이 명령어-튜닝 시나리오에서 효과적으로 샘플링되지 않을 수 있음을 의미한다. 이는 명령어 데이터가 LLM을 실제 애플리케이션에 배포하는 데 중요한 역할을 하기 때문에 우려된다. 4절에서는 위의 현상들에 대해 자세히 설명한다.\n' +
      '\n' +
      '**우리의 실수를 재고하고 잠재적인 해결책을 제안합니다.** 돌이켜보면, 우리의 프로젝트는 섹션 5에 자세히 설명된 바와 같이 몇 가지 실수를 겪었고 차선책 결정(예: 공격적인 데이터 혼합물)을 했습니다. 초기 오픈 소스 노력으로서 이러한 경험과 통찰력을 공유하는 것이 매우 중요하며, 아마도 성공적인 전략에만 초점을 맞추는 것보다 훨씬 더 중요할 수 있다고 믿습니다. 훈련 중 경험적 발견과 후속 시각화 분석(섹션 4)을 기반으로 잠재적인 솔루션 세트를 개발했다. 이러한 통찰력이 향후 커뮤니티가 더 나은 모델을 개발하는 데 도움이 되기를 진심으로 바랍니다.\n' +
      '\n' +
      '이 논문의 구조는 모든 단계를 포함하는 OpenMoE 프로젝트의 라이프사이클을 반영한다. 여기에는 초기 설계(섹션 2), 교육 및 평가(섹션 3, 심층 분석(섹션 4), OpenMoE 프로젝트의 재고(섹션 5)가 포함된다.\n' +
      '\n' +
      '##2 디자인 OpenMoE\n' +
      '\n' +
      '먼저 사전 학습 데이터, 모델 아키텍처, 학습 목표 및 감독 미세 조정 데이터에 대한 OpenMoE 모델의 초기 설계를 소개한다.\n' +
      '\n' +
      '### 사전 훈련 데이터세트: 평소보다 더 많은 코드\n' +
      '\n' +
      '현대의 LLM은 일반적으로 다양한 도메인들, 즉_데이터 혼합물 [7; 9; 20; 34; 47]로부터의 데이터세트들의 조합에 의해 트레이닝된다. 코딩을 위해 맞춤화된 LLM들(예를 들어,_ StarCoder[28], CodeLLaMA[38])을 제외하고, 대부분의 기존 모델들의 사전 트레이닝 데이터는 텍스트 데이터에 의해 지배된다. 예를 들어, GitHub 데이터세트의 샘플링 비율은 LLaMA[47]의 경우 4.5%에 불과하다. 그러나 우리는 두 가지 이유로 코드 데이터가 매우 중요하다고 주장한다. 먼저, 코드 데이터는 사고 사슬로 복잡한 추론 능력을 향상시킬 가능성이 있다[16]. 더 중요한 것은 때때로 흐릿하고 오해하기 쉬운 자연 언어와 달리 코드는 항상 정확하다. 이를 통해 코드는 기계가 서로 다른(체화된) AI 에이전트 간의 오해 없이 정보를 간결하게 전달하기 위한 보다 효율적인 언어가 될 수 있으며, 그 결과 코드는 실제 응용 프로그램에서 LLM 통신을 지배할 수 있는 큰 잠재력을 가지고 있다. 따라서 보다 코드 중심의 사전 학습 데이터 혼합을 설계한다. 표 1과 같이 RedPajama[11]에서 데이터의 50%, The Stack[24]의 복제 버전에서 데이터의 50%를 추출하였다. 실험 결과는 버전 I 데이터 혼합물이 코드 비율에서 약간 공격적일 수 있음을 보여준다. 사전 교육 후 단계에서 이러한 문제를 해결합니다. 자세한 내용은 다음 섹션 3.2를 참조하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & **Version I** & **Version II** & **Version III** \\\\\n' +
      '**Model** & OpenMoE-Base, OpenMoE-8B/32E & OpenMoE-34B/32E\\\\\n' +
      '**Period** & before 780B tokens \\(\\rightarrow\\) after 780B tokens & from start to end \\\\ \\hline\n' +
      '**Dataset** & & & \\\\ RedPajama & 50.0\\% & 83.5\\% & 67.5\\% \\\\ C4 & 7.50\\% & 15.0\\% & 15.0\\% \\\\ Wikipedia & 2.25\\% & 6.50\\% & 4.50\\% \\\\ Stackexchange & 1.00\\% & 2.50\\% & 1.00\\% \\\\ ArXiv & 1.25\\% & 4.50\\% & 4.50\\% \\\\ Books & 2.25\\% & 6.50\\% & 4.50\\% \\\\ GitHub & 2.25\\% & 5.00\\% & 5.00\\% \\\\ Commoncrawl & 33.5\\% & 43.5\\% & 33.0\\% \\\\ Wikipedia-en & 0.00\\% & 6.50\\% & 2.50\\% \\\\ The Stack Dedup & 50.0\\% & 10.0\\% & 30.0\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: OpenMoE 사전 훈련 데이터 혼합물의 세 가지 버전.\n' +
      '\n' +
      '### 모델 구조 : 디코더 전용 ST-MoE\n' +
      '\n' +
      '**Tokenizer.** 우리는 두 가지 이유로 256K 보캡 크기를 갖는 umT5[10] 토큰타이저를 적용했다: (1) 큰 다중 언어 보캡을 갖는 umT5 토큰타이저는 작은 보캡을 사용하는 토큰타이저보다 낮은 리소스 언어를 더 잘 지원한다(_e.g.,_32K 보캡을 갖는 LLaMA 토큰타이저); (2) BERT[23] 및 T5[35] 토큰타이저와 같은 일부 오래된 토큰타이저와 비교하여, umT5 토큰타이저는 바이트 폴백 기능을 가지고 있어 아웃-오브-보캡 토큰을 더 잘 지원한다.\n' +
      '\n' +
      '**Token-choice Routing.** 우리는 일반적으로 ST-MoE[59]를 따라 모델 아키텍처 및 라우팅 설계를 수행하여 훈련 안정성을 보장하며, 이는 더 큰 모델을 훈련할 때 매우 중요하다. 주어진 \\(E\\) 훈련 가능한 전문가와 입력 표현 \\(x\\in\\mathbb{R}^{D}\\)이 주어지면 MoE 모델의 출력은 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\[\\mathrm{MoE}(x)=\\sum_{i=1}^{E}g(x)_{i}e_{i}(x), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(e_{i}(\\cdot)\\)는 \\(i^{\\mathrm{th}}\\) 전문가의 비선형 변환 \\(\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{D}\\)이고, \\(g(\\cdot)_{i}\\)는 훈련가능 라우터 출력의 \\(i^{\\mathrm{th}\\) 요소이며, 비선형 매핑 \\(\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{E}\\)이다. 일반적으로 \\(e(\\cdot)\\)와 \\(g(\\cdot)\\)은 모두 신경망에 의해 매개변수화된다. 각 전문가는 저희를 포함한 대부분의 MoE 기반 트랜스포머 모델에서 완전한 트랜스포머 모델 대신 FFN 레이어입니다.\n' +
      '\n' +
      '**Top-2 Selection.** 위의 공식에 따르면 \\(g(\\cdot)\\)이 희소 벡터일 때 훈련 중 역 전파에 의해 전문가의 일부만이 활성화되고 업데이트될 것이다. 게이팅 레이어를 Top-K 선택으로 설정했습니다.\n' +
      '\n' +
      '\\[\\mathrm{g}(x)=\\mathrm{TopK}(\\mathrm{softmax}(f(x))), \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(f(\\cdot)\\)는 라우팅 선형 변환 \\(\\mathbb{R}^{D}\\rightarrow\\mathbb{R}^{E}\\)이다. \\(K\\ll E\\)일 때, \\(\\mathrm{g}(x)\\)의 대부분의 원소는 0이므로 희소한 조건 계산이 이루어진다. Zoph _et al._[59] 다음에 \\(K=2\\)을 설정하였다.\n' +
      '\n' +
      '**잔여 MoE.** 각각의 바닐라 트랜스포머 블록은 다음과 같이 기재될 수 있다:\n' +
      '\n' +
      '\\[x^{\\prime} =\\mathrm{LayerNorm}_{i}^{\\mathrm{att}(x), \\tag{3}\\]\\[x =\\mathrm{MHA}(x^{\\prime})+x,\\]\\[x^{\\prime\\prime} =\\mathrm{LayerNorm}_{i}^{\\mathrm{ffn}(x),\\]\\[x =\\mathrm{FFN}(x^{\\prime\\prime})+x,\\]\n' +
      '\n' +
      'OpenMoE에서, 각 MoE 기반 트랜스포머 블록에 대해, 우리는 하나의 고정된 FFN 층이 모든 토큰에 대해 항상 활성화되도록 보장하기 위해 하나의 잔여 MoE 층을 사용한다. 즉,\n' +
      '\n' +
      '\\[x^{\\prime} =\\mathrm{LayerNorm}_{i}^{\\mathrm{att}(x), \\tag{4}\\]\\[x =\\mathrm{MHA}(x^{\\prime})+x,\\]\\[x^{\\prime\\prime} =\\mathrm{LayerNorm}_{i}^{\\mathrm{ffn}(x),\\]\\[x =\\mathrm{MoE}(x^{\\prime\\prime})+\\mathrm{FFN}(x^{\\prime\\prime})+x,\\\n' +
      '\n' +
      '주 우리는 MoE를 모든 트랜스포머 블록에 배치하는 대신 인터리브 방식으로 MoE 기반 트랜스포머 블록을 사용한다. 설정에서 우리는 OpenMoE-Base/16E 및 OpenMoE 34B/32E에서 4개 레이어마다 MoE를 사용하고 OpenMoE-8B/32E에서는 6개 레이어마다 MoE를 사용한다. 이 설정은 ViT-MoE[36]의 발견에서 영감을 얻었으며, 즉 모든 계층에서 MoE를 사용하는_i.,_는 라우팅 동안 더 많은 계산 오버헤드를 도입하고, 인터리빙된 MoE 사용보다 더 나쁜 비용 효율적인 트레이드 오프를 유도한다.\n' +
      '\n' +
      '**Load Balance Loss and Router Z-loss.** ST-MoE[59]는 Shazeer _et al._[40]를 따르며, MoE 부하 균형 손실을 사용하여 MoE 모델이 더 나은 병렬성을 달성할 수 있도록 서로 다른 전문가에게 할당된 균형된 토큰 수를 보장한다. 각 라우팅 작업에 대해 \\(E\\) 전문가 및 \\(B=NL\\) 토큰이 있는 \\(N\\) 배치가 주어지면 훈련 중 총 모델 손실에 다음과 같은 보조 손실이 추가된다:\n' +
      '\n' +
      '\\[L_{b}=E\\cdot\\sum_{i=1}^{E}m_{i}\\cdot P_{i}, \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(m\\)은 벡터이고, \\(P_{i}\\)은 \\(\\mathrm{softmax}(f(x))\\)이다. \\(P_{i}\\)은 \\(\\mathrm{softmax}(f(x))\\)이다. (i\\)는 전문가 ID를 나타낸다. \\(i^{\\mathrm{th}}\\) 요소는 전문가 \\(i\\)에게 파견된 토큰의 분수이다:\n' +
      '\n' +
      '\\(\\mathrm{h}(x_{j})_{i}=\\frac{1}{B}\\sum_{j=1}^{B}\\mathrm{h}(x_{j})_{i}, \\tag{6}\\] 여기서 \\(\\mathrm{h}(\\cdot)\\)은 식 2에서 \\(\\mathrm{TopK}\\)에 의해 선택된 인덱스 벡터이다. \\(\\mathrm{h}(x_{j})_{i}\\)는 \\(i^{\\mathrm{th}}\\)의 \\(\\mathrm{h}(x_{j})\\의 요소이다. Eq의 \\(g(x)_{i}\\)와 다른 것이 눈에 띈다. 2, \\(m_{i}\\) 및 \\(\\mathrm{h}(x_{j})_{i}\\)는 미분할 수 없다. 그러나, MoE를 종단간 방식으로 최적화하기 위해서는 미분 가능한 손실 함수가 필요하므로, 우리는 Eq에서 라우팅 점수 \\(\\mathrm{softmax}(f(x))\\을 사용한다. 2(식 5의_i.e.,_\\(P_{i}\\)) 라우팅 결정을 차별화하고 학습할 수 있습니다.\n' +
      '\n' +
      '부하 균형 손실 외에도 Zoph _et al._[59]는 보다 안정적인 MoE 훈련을 위해 라우터 z-loss를 제안했다:\n' +
      '\n' +
      '[L_{z}(x)=\\frac{1}{B}\\sum_{i=1}^{B}\\left(\\log\\sum_{j=1}^{E}e^{x_{j}^{(i}} \\right}^{2} \\tag{7}\\]\n' +
      '\n' +
      '이 라우터 z 손실은 게이팅 네트워크에 입력되는 큰 로짓에 불이익을 줄 수 있으며 MoE 레이어의 라운드오프 오류를 줄일 수 있도록 숫자의 절대 크기가 작도록 장려할 수 있다. 자세한 설명은 ST-MoE paper[59]를 참고하시기 바랍니다.\n' +
      '\n' +
      '종합하면, 우리의 최종 훈련 손실은 다음과 같이 적을 수 있습니다.\n' +
      '\n' +
      '\\[L=L_{CE}+L_{b}+L_{z} \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(L_{CE}\\)는 언어 모델 사전 훈련에서 교차 엔트로피 손실이다.\n' +
      '\n' +
      '### 훈련 목표: UL2와 CasualLM\n' +
      '\n' +
      '바닐라 캐주얼 언어 모델링(CasualLM)을 직접 채택하는 대신, 스팬 부패(SpanCorrupt)와 프리픽스 언어 모델링(PrefixLM)을 결합한 보다 다양한 언어 모델 사전 훈련 목표인 UL2[44]를 탐색한다[35]. UL2의 SpanCorrupt는 다양한 스팬 길이와 부패율을 혼합하기 때문에 바닐라 SpanCorrupt보다 더 다양하다는 점은 주목할 만하다. 우리는 OpenMoE에서 UL2를 탐구하는 두 가지 이유가 있다. 먼저, UL2는 PaLM-2 [1]에서 유망한 결과를 보였다. 더 중요한 것은, 공격적인 토큰 마스킹은 코파일럿과 같은 현실 세계의 코드 완성 태스크와 매우 유사하다. 바이에른 _et al._[5]는 또한 유사한 중간채움(FiM) 목표가 바닐라 훈련 목표보다 코드를 더 잘 모델링할 수 있다는 것을 발견했다. 사전 훈련 데이터 혼합물에서 더 많은 코드를 사용했기 때문에 FiM을 포함하는 UL2를 적용하는 것이 직관적으로 더 합리적인 선택이다.\n' +
      '\n' +
      '우리의 상세한 UL2 훈련 목표 구성은 표 2와 같다. 훈련 중에 출력 토큰이 적기 때문에 학습이 느려질 수 있기 때문에 20% 낮은 마스크 비율(\\(r\\)=0.15)만을 사용한다. 또한 PrefixLM 훈련에 의해 향상된 영점 및 맥락 내 학습 능력이 중요하다고 생각하기 때문에 기본 UL2 설정보다 PrefixLM을 더 많이 사용한다. 우리는 섹션 3.2에서 논의될 OpenMoE에서 UL2로 훈련할 때 몇 가지 어려움에 직면했다.\n' +
      '\n' +
      '### Supervised Fine-tuning\n' +
      '\n' +
      '정렬이 이 OpenMoE 프로젝트의 초점이 아니지만 여전히 SFT(supervised fine-tuning)를 오픈 소스 와일드챗 데이터 세트 [2]의 하위 집합으로 수행하여 명령어 추적 능력을 향상시키고 SFT 전후에 MoE 모델의 동작을 연구한다. OpenMoE 개발 후기 단계에서 계산 자원이 부족하기 때문에 와일드챗의 GPT-4에서 명령어-응답 쌍만 선택한다. 서브세트는 58K 대화들을 포함하고, 각각의 대화들은 평균적으로 1.8 턴들을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline\n' +
      '**Training Objective** & **Percentage** \\\\ \\hline\n' +
      '**PrefixLM, \\(r\\)=0.5** & 50\\% \\\\\n' +
      '**SpanCorrupt** & \\\\ \\(\\mu\\)=3, \\(r\\)=0.15 & 10\\% \\\\ \\(\\mu\\)=8, \\(r\\)=0.15 & 10\\% \\\\ \\(\\mu\\)=3, \\(r\\)=0.5 & 10\\% \\\\ \\(\\mu\\)=8, \\(r\\)=0.5 & 10\\% \\\\ \\(\\mu\\)=64, \\(r\\)=0.5 & 10\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: UL2의 디노이아 혼합 구성, \\(\\mu\\)은 평균 스팬 길이, \\(r\\)은 마스크 비율이다.\n' +
      '\n' +
      '### Other Designs\n' +
      '\n' +
      '최근 LLM에 이어, 고밀도 및 MoE 트랜스포머 블록 모두에서 위치 임베딩을 위한 RoPE[43]과 FFN을 위한 활성화 기능을 위한 SwiGLU[39]를 채택했다. OpenMoE 모델에 대한 보다 상세한 모델 구성 및 훈련 하이퍼파라미터는 부록 B에서 찾을 수 있다. 데이터 병렬성, 텐서 병렬성[41, 50], 전문가 병렬성[25]을 규모의 훈련 모델에 적용하였다. 구글 클라우드 TPU에서 가용성에 따라 64~512 v3 칩으로 OpenMoE 모델을 교육합니다.\n' +
      '\n' +
      '## 3 훈련 오픈모\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '설계 결정에 대한 초기 평가로서 OpenMoE-Base/16E 모델을 사용하여 절제 연구를 수행했다. 이러한 결과가 초기 통찰력을 제공하지만 주로 대규모 삭제를 방해하는 계산 자원 제약으로 인해 대규모 모델에 대한 일반화 가능성을 확신할 수 없다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '우리의 연구 결과는 MoE 접근법, UL2 훈련 목표 및 코드 데이터에 대한 강조 증가 등 여러 요소가 모두 제로 샷 트리비아QA 작업에서 기본 버전의 성능에 긍정적으로 기여한다는 것을 나타낸다. LLaMA tokenizer[47]를 사용한 모델이 umT5 tokenizer를 사용한 모델보다 성능이 우수하였다. 이 결과는 더 큰 어휘 크기가 성능을 약간 손상시킬 수 있지만 허용 가능한 것으로 간주된다. 기초 모델이 다양한 글로벌 청중에게 접근 가능하고 유익해야 하기 때문에 저자원 언어를 지원하는 것이 중요하다고 믿습니다. 이 신성성 검사 후, 우리는 OpenMoE를 OpenMoE-8B/32E까지 확장한다.\n' +
      '\n' +
      '또한 다른 도메인의 데이터 학습 진행 상황을 비교하기 위해 절제 연구를 수행한다. 그림 1과 같이 모델이 코드 데이터에 대해 더 높은 정확도와 더 낮은 손실을 달성하는 것이 더 쉽다는 것을 관찰할 수 있다. Github에서는 모델이 작지만 여전히 80% 이상의 토큰 예측 정확도를 달성할 수 있습니다. 우리는 이것이 코드 데이터의 롱테일 토큰 분포 때문이라고 추론한다. 예를 들어, 코드의 토큰 수는 "\\(\\mathfrak{n}\\)"과 "\\(\\mathfrak{t}\\)"으로 비교적 예측하기 쉽다.\n' +
      '\n' +
      '그림 1: 다른 사전 훈련 데이터 세트에 대한 검증 손실 및 정확도의 비교. 우리는 모델이 코드 데이터에 대한 더 높은 정확도와 더 낮은 손실을 달성하는 것이 더 쉽다는 것을 관찰할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c} \\hline \\hline\n' +
      '**Method** & **EM** & **F1** \\\\ \\hline OpenMoE & 1.4 & 4.5 \\\\ w/o MoE & 0.1 & 0.3 \\\\ w/o UL2 (PrefixLM only) & 0.0 & 0.0 \\\\ w/o Code data & 0.7 & 1.1 \\\\ w/ LLaMA tokenizer & 2.2 & 5.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 제로-샷 트리비아QA 상의 OpenMoE-Base/16E를 사용한 절제 연구[22].\n' +
      '\n' +
      '### Training Progress\n' +
      '\n' +
      '**UL2 포화** 훈련 중에 UL2가 모델이 훈련 초기 단계에서 더 빨리 학습하는 데 도움이 될 수 있지만 OpenMoE-8B/32E의 후기 훈련 단계에서 포화되는 것이 더 쉽다는 것을 발견했다. 그림 2와 같이 확대하면 OpenMoE-8B/32E가 35K에서 39K 단계로 매우 느리게 개선됨을 알 수 있다. UL2가 더 다양하지만 여전히 CasualLM에 비해 SpanCorrupt가 상대적으로 쉽기 때문일 수 있다고 제안한다. 따라서, 우리는 390K 단계(780B) 토큰 후에 CasualLM으로 후퇴한다. 또한 코드 데이터가 UL2와 더 잘 정렬되고 초기 코드 데이터 혼합물이 상대적으로 공격적이기 때문에 코드 데이터 샘플링 비율을 15%로 줄였다. 두 번째 버전 데이터 혼합물은 표 1에 보고되어 있다.\n' +
      '\n' +
      '명백하게, 그림 2에서, 780B 토큰 이후에, OpenMoE-8B/32E에 대해 390K 단계 후에 토큰 예측 정확도에 상당한 감소가 있다. 이는 더 어려운 CasualLM 객관적이고 덜 쉬운 코드 데이터에 기인한다. 비록 우리가 OpenMoE-8B/32E 훈련의 후반 단계에서 포화 문제에 직면했지만, 우리는 그러한 쉬운 커리큘럼이 LLM 훈련에 도움이 될 수 있다고 생각한다. 따라서 우리는 여전히 OpenMoE-34B/32E에서 25K 단계(50B 토큰)에 UL2를 적용했다. 우리는 OpenMoE-34B/32E에서 비교적 적당한 코드 헤비 데이터 혼합물을 사용했다. 표 1과 같이 전체 코드 데이터의 35%를 활용한다. 계산 자원의 한계로 인해 확장성을 검증하기 위해 200B 토큰만으로 OpenMoE-34B/32E를 학습한다. 가능하면 더 많은 토큰이 포함된 대규모 오픈모(OpenMoE) 교육을 미래 작업으로 남겨둡니다.\n' +
      '\n' +
      '벤치마크에 대한 평가\n' +
      '\n' +
      '1 원시모형 평가\n' +
      '\n' +
      '우선, 우리는 벤치마크를 해킹하지 않았으며 사전 교육은 위에서 언급한 오픈 소스 데이터 세트에 대한 것임을 강조한다. 우리의 모델은 훈련 예산 측면에서 상대적으로 작기 때문에 주로 설정되었지만 하드 벤치마크인 _i.triviaQA[22], HumanEval[8], WMT16-En-Ro[6], BigBench-Lite(24개 작업)[4] 및 13개 작업이 있는 lm 평가-harness 컬렉션[17]의 하위 집합에 대해 원시 모델을 평가한다. 5-shot MMLU[19]와 같은 인기 있지만 상대적으로 도전적인 벤치마크에 대해, 우리의 OpenMoE-8B/32E는 약 26.2%의 정확도를 달성하며, 이는 모델이 네 가지 옵션에서 거의 무작위로 추측된다는 것을 의미한다. 우리는 주로 훈련 비용이 더 많은 오픈 소스 모델인 _i.e.,_TinyLLaMA-1.1B[54] 및 OpenLLaMA-3B[18]와 비교한다. 또한 BigBench-Lite에서 GPT-3[7], Big-G[4] 및 Big-G-Sparse[4]와 비교하였다. Big-G와 Big-G-Sparse는 BigBench-Lite에서 평가된 구글 사내 트랜스포머 모델의 두 집합이며, Big-G-Sparse 모델은 MoE 기반 트랜스포머이다.\n' +
      '\n' +
      '우리는 먼저 Commonsense QA (TriviaQA), Coding (HumanEval) 및 Low-Resource Machine Translation (WMT16 En-Ro)에 대한 결과를 보고한다. 이 세 가지 벤치마크는 (1) OpenMoE가 효율적인 매개변수 스케일링 이점을 고려할 때 더 많은 상식을 암기할 수 있는지 확인하는 것이며, (2) 코딩 관련 사용자 프롬프트, 에이전트로서의 LLM 및 체화된 AI를 해결하는 데 널리 사용되는 사용 사례 때문에 코딩이 중요하며, (3) 기초 모델의 이점을 모두에게 공유하고자 하기 때문에 저자원 기계 번역이 중요하다.\n' +
      '\n' +
      '그림 2: OpenMoE 모델의 토큰 예측 정확도. OpenMoE-8B/32E는 390K 단계 이전에 UL2를 사용하고 790K 단계 이후에 CasualLM으로 다시 떨어진다. OpenMoE-34B/32E는 50B 토큰까지 UL2를 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '도 3에서, 상대 비용은 활성화된 파라미터(Act. Params)를 곱하는 것에 기초하여 계산된다. 인 트랜스포머 블록 및 트레이닝 토큰 수. 컬러 도트들의 크기는 활성화된 파라미터들의 수를 나타내고, 섀도의 크기는 MoE 모델들에 대한 총 파라미터들의 수를 나타낸다. 우리는 OpenMoE가 훈련 비용과 추론 비용 측면에서 빅벤치-라이트에서 더 나은 비용 효율성 트레이드 오프를 달성했음을 관찰할 수 있다.\n' +
      '\n' +
      '또한 LM-평가-Harness 컬렉션의 13개 태스크에 대해 OpenMoE를 평가한다. 표 7에 나타난 바와 같이 OpenMoE와 TinyLLaMA 모두 OpenLLaMA보다 더 나쁜 성능을 보였다. 그러나, OpenMOE에 의해 달성된 점수는 허용가능하다. 코드 데이터에 대한 초기 높은 샘플링 비율이 섹션 5에서 논의할 문제 중 하나인 텍스트가 지배하는 벤치마크에 대한 결과에 해를 끼칠 수 있다고 제안한다.\n' +
      '\n' +
      '###### 3.3.2 채팅 모델 평가\n' +
      '\n' +
      '우리는 모델을 종합적으로 검토할 수 있는 확립된 챗봇 벤치마크인 MTBench에서 모델을 추가로 평가한다. 우리는 그림 3(a)와 표 8에서 단일 턴과 다중 턴 결과를 모두 보고한다. 우리는 OpenMoE가 단일 턴 결과, 특히 코딩 작업에서 큰 마진만큼 기준선을 능가하는 것을 관찰할 수 있다. 그러나 두 번째 턴에서 OpenMoE의 성능이 더 떨어지며, 이로 인해 다중 턴이 더 나빠지는 결과는 그림 3(b)와 같다. 우리는 이것이 아마도 긴 시퀀스의 토큰 드롭에 의해 야기된다는 것을 발견했다. 자세한 분석은 다음 4절을 참조하십시오.\n' +
      '\n' +
      '##4 OpenMoE 분석\n' +
      '\n' +
      '우리는 일반적으로 MoE가 고정된 계산 예산으로 매개변수를 확장하는 효과적인 방법이라고 생각한다. 그러나, 우리는 MoE의 전문가들이 무엇을 전문으로 하는지 거의 알지 못한다. 본 절에서는 라우팅 행동을 연구하기 위해 OpenMoE에 대한 심층 분석을 여러 측면에서 수행한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & **Act. Params** & **Total Tokens** & **Multi-lingual Tokens** & **WMT16 En-Ro** \\\\ \\hline TinyLLaMA-1.1B & 0.9B & 3.0T & 75B & 2.6 \\\\ OpenLLaMA-3B & 2.9B & 1.0T & 24B & 1.9 \\\\ OpenMoE-8B/32E & 2.1B & 1.1T & 38B & 3.1 \\\\ OpenMoE-34B/32E & 6.4B & 0.2T & 9B & 3.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: WMT16 En-Ro(BLEU score)에 대한 결과. 또한 RedPajama 데이터세트의 Wikipedia의 다중 언어 버전인 사전 학습 데이터세트의 명시적 다중 언어 토큰의 수를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Dataset** & **TinyLLaMA-1.1B** & **OpenLLaMA-3B** & **OpenMoE-8B/32E** \\\\ \\hline ANLI-R1 & 34.2 & 33.0 & 32.7 \\\\ ANLI-R2 & 32.4 & 36.0 & 33.2 \\\\ ANLI-R3 & 35.1 & 38.0 & 33.9 \\\\ HellaSwag & 59.2 & 52.0 & 45.5 \\\\ WinoGrande & 59.1 & 63.0 & 60.3 \\\\ PIQA & 73.3 & 77.0 & 74.2 \\\\ ARC-Easy & 55.2 & 68.0 & 64.1 \\\\ ARC-Challenge & 30.1 & 34.0 & 30.3 \\\\ Boolq & 57.8 & 66.0 & 61.2 \\\\ TruthfulQA & 37.6 & 35.0 & 36.0 \\\\ OpenbookQA & 21.8 & 26.0 & 24.6 \\\\ RTE & 51.9 & 55.0 & 53.4 \\\\ WiC & 50.1 & 50.0 & 49.8 \\\\ \\hline Average & 45.9 & **48.7** & 46.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: lm-평가-harness에 대한 OpenMoE-8B/32E를 평가한다. OpenLLaMA의 결과는 두 개의 유효 숫자만 제공하는 홈페이지에서 나온 것이다.\n' +
      '\n' +
      '전문가 전문분야는 무엇입니까?\n' +
      '\n' +
      '**MoE는 도메인 레벨을 전문으로 합니까?** 먼저 레드파자마 데이터 세트의 다른 하위 집합에서 토큰의 라우팅 결정을 시각화합니다. 모든 시각화 결과는 기본적으로 세 번째 MoE 레이어의 결과인데, 이는 레이어 간에 유의미한 차이를 관찰하지 않았기 때문이다. 우리는 서로 다른 부분 집합(_i.e._, 도메인)의 토큰이 도표 상에 분포된 것을 관찰할 수 있다. 즉, \\(E_{21}\\)은 코드 토큰을 약간 선호하고, \\(E_{10}\\)은 책과 비슷하지만, MoE의 대부분의 전문가는 도메인에 따라 전문화되지 않는다.\n' +
      '\n' +
      '**MoE가 언어 수준을 전문으로 합니까?** MoE가 다른 코딩 언어와 자연 언어를 전문으로 하는지 확인하기 위해 더 미세한 곡물 데이터로 진행합니다. 그림 6에서 우리는 4가지 다른 코딩 언어인 _i.e._, Assembly, Blitzmax, Java 및 Python을 비교한다. 도메인 레벨과 유사하게,\n' +
      '\n' +
      '도 4: MTBench 상에서 OpenMoE를 평가한다.\n' +
      '\n' +
      '도 5: RedPajama 데이터셋에 대한 라우팅 결정의 시각화. \\ (E_{i}\\)는 \\(t_{\\mathrm{th}}\\) 전문가에게 라우팅된 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Model & MT-Bench 1st Turn & MT-Bench 2nd Turn & MT-Bench Avg \\\\ \\hline GPT-J-6B (0.4T) & 2.51 & 2.35 & 2.43 \\\\ TinyLAMa-1.1B (3T) & 4.08 & 2.54 & 3.31 \\\\ OpenLLaMA-3B (1T) & 4.36 & **3.62** & **3.99** \\\\ OpenMoE-8B/32E (1.1T) & **4.69** & 3.26 & **3.98** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: MT-벤치에 대한 평균 점수.\n' +
      '\n' +
      '어셈블리와 블리츠맥스, 즉 Java 및 파이썬에 비해 두 가지 낮은 리소스 언어에도 불구하고 여전히 상당한 전문가 전문화를 나타내지 않았다.\n' +
      '\n' +
      '우리는 다양한 자연 언어에 대한 전문가 전문화를 추가로 연구합니다. 다중 언어 병렬 말뭉치, 즉 TED-Parallel-Corpus 4를 플랫폼으로 채택했다. 그림 7에서 우리는 서로 다른 전문가들 사이에서 비교적 명확한 전문화가 있음을 발견했다. 예를 들어, zh-cn(중국어, 단순화)과 zh-tw(중국어, 전통) 모두 \\(E_{5}\\)와 \\(E_{16}\\); ja(일본어)와 ko(한국어) 모두 \\(E_{14}\\)을 선호하는 것으로 나타났다.\n' +
      '\n' +
      '각주 4: [https://github.com/ajinkyakulkarnil14/TED-Multilingual-Parallel-Corpus](https://github.com/ajinkyakulkarnil14/TED-Multilingual-Parallel-Corpus)\n' +
      '\n' +
      '**MoE는 작업 수준을 전문화합니까?** 위의 결과를 바탕으로 더 미세한 데이터에는 더 명확한 전문가 전문화 관찰이 있습니다. 그런 다음 그림 8의 MT-Bench 대화 데이터에 대한 라우팅 결정을 시각화한다. 특히 수학 데이터에 대해 위와 유사한 전문화를 볼 수 있다. 우리는 수학 과제가 다른 과제보다 특별한 토큰을 더 많이 포함하고 있기 때문이라고 제안한다.\n' +
      '\n' +
      '**MoE는 Position ID에 특화되어 있습니까?** MoE의 라우터는 토큰 표현에 기초하여 결정을 내립니다. 토큰 표현들은 토큰 임베딩들 및 포지션 임베딩들로부터 나온다. 이에 따라\n' +
      '\n' +
      '도 6: TheStack 데이터세트에 대한 라우팅 결정의 시각화. \\ (E_{i}\\)는 \\(i_{\\mathrm{th}}\\) 전문가에게 라우팅된 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '도 7: 12개 언어를 포함하는 TED-Parallel-Corpus에 대한 라우팅 결정의 시각화: _i.e.,_ ar(Arabic), de(German), es(Spanish), fr(French), he(Hebrew), it(Italian), ja(Japanese), ko(Korean), nl(Dutch), ru(Russian), zh-cn(Chinese Simplified), zh-tw(Chinese, Traditional), \\(E_{i}\\)는 \\(i_{\\mathrm{th}}\\) 전문가에게 라우팅되는 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '그림 8(a)와 그림 8(b)에서 서로 다른 위치에 대한 라우팅 결정을 시각화합니다. 우리는 (1) 실제로 다른 위치 ID에 몇 가지 전문화가 있다; (2) 연속적인 위치는 그림 8(b)의 \\(E_{10}\\) 및 \\(E_{19}\\)과 같은 유사한 전문가를 선호한다.\n' +
      '\n' +
      '**MoE는 토큰 ID를 전문으로 합니까?** umT5 토큰화기를 사용하고 있기 때문에 일반적으로 다른 언어의 토큰은 다른 토큰 ID를 가지고 있습니다. 따라서, 우리는 MoE의 라우터가 주로 토큰 ID를 기반으로 결정을 내리는지 여부를 추가로 연구한다. 우리는 그림 10에서 몇 가지 대표적인 토큰의 라우팅 결정을 시각화합니다. 이 모든 토큰은 소수의 전문가에게만 매우 강력한 전문화를 보여줍니다. 이는 동일한 Token ID를 가진 토큰이 서로 다른 문장에서 매우 다양한 맥락을 가지고 있기 때문에 매우 흥미로운 발견이다. 예를 들어, 토큰 "ed"는 많은 상이한 워드들, _e.g._, "preferred", 및 "led"의 접미사일 수 있다. 토큰 "an"은 또한 "an apple" 또는 "another"의 일부일 수 있다. 그러나 이러한 모든 토큰은 소수의 고정 전문가에 대해서만 매우 강력한 전문화를 가지고 있습니다. 즉 MoE는 단순히 높은 수준의 의미론 대신 토큰 ID를 기반으로 라우팅한다. 우리는 이 관찰을 다음 섹션에서 **상황 독립적 전문화**로 명명한다. 다른 토큰 ID에 대해서도 컨텍스트 독립적 전문화가 존재하는지 확인하기 위해 부록 E에 라우팅 결정 표준 편차를 표시한다.\n' +
      '\n' +
      '그림 8: MT-Bench에 대한 라우팅 결정의 시각화. 시각화 데이터 소스로 OpenMoE MT-Bench를 평가할 때 대화 이력을 채택한다. \\ (E_{i}\\)는 \\(i_{\\mathrm{th}}\\) 전문가에게 라우팅된 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '도 9: 상이한 위치 ID에서의 라우팅 결정의 가시화. \\ (E_{i}\\)는 \\(i_{\\mathrm{th}}\\) 전문가에게 라우팅된 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '### 토큰 전문화 연구\n' +
      '\n' +
      '**전문가들이 유사한 토큰들을 클러스터링하고 있는가?** 위에서 논의한 바와 같이, 동일한 토큰 ID를 갖는 토큰들은 문맥이 어떤 것이든, _즉,_Context-independent Specialization에 관계없이 항상 동일한 전문가에게 라우팅된다. 따라서 본 논문에서는 저수준 시멘틱이 유사한 토큰에 해당하는 토큰 ID를 선호하는지 여부를 조사한다. 표 9의 전문가별로 상위 10개의 선호 토큰을 나열한다. 유사한 토큰이 전문가에 군집되어 있음을 관찰할 수 있다. 예를 들면, "할 수 있다" "will"과 "would"는 모두 전문가 31에 속한다. "have". "has" 및 "had"는 모두 전문가 30에 포함된다. 이러한 시각화는 또한 위의 많은 관찰을 설명할 수 있다. 예를 들어, 위의 대부분의 그림에서 우리는 대부분의 코딩과 수학 데이터가 전문가 21을 선호한다는 것을 찾을 수 있다. 여기서 실제 이유를 드러낸다. 전문가 21은 수학과 코드에서 더 자주 등장하는 "=", "and", "n"에 대한 선호도가 강하다.\n' +
      '\n' +
      '** 모델은 언제 전문화를 학습하였는가?** 위에서 관찰한 문맥 독립적 전문화에 따르면, 모델은 높은 수준의 의미론에 기반하여 경로를 탐색하는 방법을 학습하고 있지 않다. 따라서 우리는 또 다른 질문을 제기합니다. 모델은 언제 토큰에 대한 라우팅 결정을 배우고 수정했습니까? 우리는 그림 10(a)와 그림 10(b)에서 서로 다른 OpenMoE 중간 체크포인트의 라우팅 결정을 비교한다. 우리는 전문가 선호도가 서로 다른 체크포인트에 대해 거의 완전히 겹치는 것을 볼 수 있으며, 이는 모델이 훈련 초기 단계에서 라우팅을 수정하기 시작했음을 의미한다. 학습 데이터 혼합물(52.25% 코드에서 20% 코드)과 학습 목표(UL2에서 CasualLM)를 변경하더라도 라우팅 결정은 여전히 고정되어 있다. 그 이유는 일반적으로 토큰이 특정 전문가에게 할당될 때, 토큰이 보이지 않는 다른 전문가에게 전송되면 손실이 많이 증가하며, 이는 모델을 밀어 원래 전문가에게 토큰을 다시 할당하기 때문이다. 따라서, 라우팅은 준비 단계 등에서 학습되었을 것이며, 다음 훈련 단계 전반에 걸쳐 유지되었을 것이다.\n' +
      '\n' +
      '라우팅 중### 토큰 드롭\n' +
      '\n' +
      'MoE 모델에서는 일반적으로 모든 전문가에게 균형 잡힌 워크로드를 보장하기 위해 미리 정의된 최대 용량 \\(C\\)을 설정하며, 이는 각 전문가가 \\(C\\) 토큰 이상을 처리할 수 없음을 의미한다.\n' +
      '\n' +
      '도 10: 상이한 토큰 ID에서의 라우팅 결정의 가시화. \\ (E_{i}\\)는 \\(i_{\\mathrm{th}}\\) 전문가에게 라우팅된 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c} \\hline \\hline Expert ID & Top Tokens \\\\ \\hline\n' +
      '0&\\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\\n' +
      '1&\\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\)\\\\\\\\\n' +
      '21&\\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\)\n' +
      '30&1, \\(\\texttt{ed}\\), \\(\\texttt{d}\\), \\(\\texttt{have}\\), \\(\\texttt{ing}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{has}\\), \\(\\texttt{s}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{n}\\), \\(\\texttt{has}\\), \\(\\texttt{s}\\), \\(\\texttt{n}\\), \\(\\texttt{had}\\), \\(\\texttt{had}\\), \\(\\texttt{n}\\), \\(\\texttt{had}\\), \\(\\texttt{n}\\), \\(\\texttt{had}\\), \\(\\texttt{n}\\), \\(\\texttt{had}\\), \\(\n' +
      '31 & \\(\\texttt{to}\\), \\(\\texttt{can}\\), \\(\\texttt{s}\\), \\(\\texttt{of}\\), \\(\\texttt{ing}\\), \\(\\texttt{will}\\), \\(\\texttt{not}\\), \\(\\texttt{e}\\), \\(\\texttt{ed}\\), \\(\\texttt{would}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 각 전문가가 선택한 최상위 토큰.\n' +
      '\n' +
      '이는 전문가 병렬성을 갖는 MoE 모델을 훈련시키고 배치할 때, 즉, 서로 다른 전문가를 서로 다른 GPU에 분배할 때 처리량을 보장할 수 있다. 그러나, 이것은 또한 문제를 도입할 것이고, 이전의 토큰이 전문가를 채웠다면 이후의 토큰은 삭제될 것이다. 디코더 전용 MoE 아키텍처에서, 자동-회귀 특성 때문에, 시퀀스의 나중의 토큰들은 더 많이 드롭될 수 있다. 예를 들어, 한 전문가가 "n" 토큰을 선호하고, 시퀀스가 많은 "n"으로 시작하고 또한 생성된 다음 출력에서 많은 "n"을 갖는다면, 전문가는 "n" 토큰으로 빠르게 채워지고 이 전문가에게 할당되어야 하는 다른 모든 토큰이 나중에 나타나면 삭제될 것이다. 이를 검증하기 위해 서로 다른 위치 ID에서 드롭된 토큰의 비율을 시각화한다. 도 11의 (a)에 도시된 바와 같이, 일반적인 사전 트레이닝 데이터세트들, 예를 들어,_RedPajama 및 TheStack은 1500 이후의 포지셔닝 ID에 대해서도, 토큰들의 작은 비율만이 드롭된 균형 토큰 할당을 달성하였다. 그러나, 다중 언어 및 명령어-추종 데이터세트들에 대해서는, 토큰들의 큰 비율이 드롭된다. 그 이유는 위에서 논의한 바와 같이 라우팅 결정은 훈련 초기에 고정되어 더 이상 변경되지 않기 때문에 사전 훈련 데이터 세트를 기반으로 부하 균형도 달성되기 때문이다. 명령 후속 데이터는 MoE 라우터의 OOD(Out-of-Domain) 데이터의 한 유형으로 볼 수 있으며, 이는 불균형 토큰 할당을 유도하여 나중에 나타나는 많은 토큰이 삭제되도록 한다.\n' +
      '\n' +
      '** 명령어 후속 데이터로 감독 미세 조정이 이러한 드롭-투워드-더-엔드 이슈를 완화할 수 있는가?** 드롭-투워드-더-엔드 이슈는 주로 OOD 데이터에 의해 야기되기 때문에, 명령어 데이터세트로 MoE를 튜닝함으로써 명령어 후속 데이터를 도메인 내 데이터로 변환하는 것이 가능한지 생각하고 연구하는 것은 자연스럽다. 따라서 우리는 전후의 모델을 비교한다.\n' +
      '\n' +
      '도 11: 상이한 중간 체크포인트들의 토큰 ID들의 라우팅 결정의 시각화. \\ (E_{i}\\)는 \\(i_{\\mathrm{th}}\\)로 라우팅되는 토큰의 비율을 나타낸다.\n' +
      '\n' +
      '도 12: 상이한 포지션 ID들에서 드롭된 토큰들의 비율을 비교하는 것.\n' +
      '\n' +
      '도 12b의 감독 미세 조정. 우리는 모델들이 Drop-towards-the-End 이슈에서 큰 차이가 없다는 것을 알 수 있다. 이것은 위의 통찰력, 즉 LLM 사전 훈련의 매우 초기 단계에서 학습되고 고정된 라우팅 행동과 잘 일치한다.\n' +
      '\n' +
      '## 5 Reinking OpenMoE\n' +
      '\n' +
      '이 프로젝트를 수행하는 것은 작가들에게 긴 여정입니다. 우리는 실제로 디자인과 개발 중에 몇 가지 실수를 했지만 분석에서 몇 가지 새로운 통찰력을 얻었습니다. 따라서 우리는 미래의 실무자들을 돕기 위해 이 논문에서 예약 없이 발견한 모든 것을 적습니다. 그런 다음 이 섹션에서는 우리 작업의 가장 중요한 테이크아웃인 향후 더 나은 모델을 훈련하는 방법에 대해 논의한다.\n' +
      '\n' +
      '**우리가 얼마나 많은 코드를 사용할까?** 솔직히 말해서, 우리는 매우 정확한 답을 가지고 있지 않다. 절제 연구를 수행하는 것은 LLM을 규모에서 사전 훈련하는 비용 때문에 매우 비싸다. 결론은 또한 모델 크기와 데이터 품질에 크게 좌우될 수 있다. 그러나 우리가 관찰한 바에 따르면 50% 이상의 코드가 너무 공격적으로 보여 텍스트 작업에 대한 능력에 해를 끼칠 수 있지만, 코드 작성의 중요성을 고려할 때 OpenMoE-34B/32E에서 사용한 것과 같이 약 30%의 코드를 사용하는 것이 좋다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|r r|r r|r} \\hline \\hline\n' +
      '**Dataset** & **Subset** & \\multicolumn{2}{c|}{**LLaMA Tokenizer**} & \\multicolumn{2}{c|}{**umT5 Tokenizer**} & \\multicolumn{1}{c}{**umTS/LLaMA**} \\\\  & & **\\#Tokens** & **Vocab Used** & **\\#Tokens** & **Vocab Used** & \\\\ \\hline RedPajama & arxiv & 125,339 & 8,327 & 131,059 & 8,762 & 1.046 \\\\  & book & 137,972 & 11,603 & 131,072 & 15,202 & 0.950 \\\\  & c4 & 28,592 & 5,439 & 26,428 & 5,554 & 0.924 \\\\  & cc & 78,450 & 8,738 & 73,403 & 9,927 & 0.936 \\\\  & github & 54,707 & 4,769 & 59,732 & 4,539 & 1.092 \\\\  & stackexchange & 40,659 & 4,714 & 43,195 & 4,317 & 1.062 \\\\  & wikipedia & 37,406 & 7,179 & 30,555 & 8,748 & 0.817 \\\\ \\hline TheStack & assembly & 49,143 & 3,066 & 50,738 & 3,130 & 1.032 \\\\  & blitzmax & 78,259 & 4,200 & 80,658 & 4,209 & 1.031 \\\\  & java & 64,236 & 4,229 & 69,902 & 3,905 & 1.088 \\\\  & python & 66,243 & 5,095 & 70,795 & 4,799 & 1.069 \\\\ \\hline MTBench & writing & 6,062 & 1,700 & 5,786 & 1,535 & 0.954 \\\\  & roleplay & 4,309 & 1,291 & 4,076 & 1,172 & 0.946 \\\\  & reasoning & 2,369 & 478 & 2,309 & 429 & 0.975 \\\\  & math & 5,163 & 290 & 5,154 & 282 & 0.998 \\\\  & coding & 4,955 & 651 & 5,256 & 631 & 1.061 \\\\  & extraction & 7,058 & 1,376 & 6,817 & 1,234 & 0.966 \\\\  & stem & 4,783 & 1,151 & 4,527 & 1,039 & 0.946 \\\\  & humanities & 6,398 & 1,451 & 5,946 & 1,320 & 0.929 \\\\ \\hline Multi-lingual & ar & 256,952 & 187 & 88,406 & 8,037 & 0.344 \\\\ TED & de & 103,270 & 4,880 & 80,593 & 8,470 & 0.780 \\\\  & es & 101,212 & 4,745 & 78,713 & 8,519 & 0.778 \\\\  & fr & 115,057 & 5,156 & 95,978 & 8,164 & 0.834 \\\\  & he & 242,446 & 239 & 86,891 & 4,074 & 0.358 \\\\  & it & 109,591 & 4,593 & 84,201 & 8,833 & 0.768 \\\\  & ja & 144,825 & 931 & 63,491 & 6,860 & 0.438 \\\\  & ko & 257,107 & 596 & 106,770 & 2,736 & 0.415 \\\\  & nl & 102,703 & 4,234 & 75,084 & 7,540 & 0.731 \\\\  & ru & 107,144 & 2,502 & 74,445 & 9,658 & 0.695 \\\\  & zh-cn & 149,581 & 1,058 & 88,107 & 3,611 & 0.589 \\\\  & zh-tw & 173,415 & 1,107 & 93,693 & 3,619 & 0.540 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 상이한 데이터세트로부터 추출된 서브세트에 umT5 토큰타이저와 LLaMA 토큰타이저를 비교한다. 사용된 보캡은 전체 서브세트를 토큰화할 때 활성화되는 토큰 ID의 수를 나타낸다. umT5/LLaMA는 동일한 서브세트를 토큰화할 때, umT5와 LLaMA가 생성하는 토큰 수의 비율을 의미한다.\n' +
      '\n' +
      '**토키나이저 선택** 우리의 큰 토키나이저 어휘는 트랜스포머 블록 이후의 마지막 출력 계층에서 계산 오버헤드를 도입한다. 트랜스포머 모델을 확장한 후에는 이러한 오버헤드가 상대적으로 작아지지만, 토큰타이저 선택을 더 똑똑하게 하는 것은 여전히 가치가 있다. 우리는 섹션 4에서 사용한 데이터 세트를 사용하여 토큰화기에 대한 정량적 분석을 수행합니다. 표 10에서 볼 수 있듯이 umT5 토큰화기는 다중 언어 데이터 세트, 특히 저자원 언어에서 LLaMA 토큰화기보다 훨씬 더 우수하다. 또한 명령어 후속 데이터에서 LLaMA보다 약간 더 좋다. 그러나 코드 데이터에 더 많은 토큰을 저장할 수 있다는 예상과 잘 일치하지 않았다. 또한, 두 토키나이저의 토큰 사용량은 매우 긴 테일로 분포되어 있으며, 이는 토키나이저를 개선하고 후속 알고리즘을 개선할 수 있는 큰 여지가 있음을 나타낸다. 우리가 알고 있듯이, 긴 꼬리 데이터로부터 배우는 것은 어렵다[55]. 사전 학습 데이터 혼합물에는 다국어 데이터가 거의 없기 때문에 이러한 저자원 토큰의 로짓 예측 계산 비용이 낭비된다. 우리의 차선책 선택에 기초하여, 우리는 또한 사람들이 체계적으로 토큰타이저를 평가하는 데 도움이 될 견고한 토큰타이저 벤치마크가 필요하다. 그리고 모델을 학습하기 전에 최고의 토큰화기를 선택할 수 있습니다.\n' +
      '\n' +
      '보다 효율적인 MoE Architecture** 관찰에 따르면, MoE 라우팅은 거의 문맥 독립적(_i.e.,_Context-independent Specialization)으로, (1) 웜업 단계 후에 훈련 가능한 라우터를 제거할 수 있다; (2) 어텐션 레이어의 출력을 사용하지 않고 직접 입력을 기반으로 병렬 트랜스포머 레이어 [9; 48] 컴퓨팅 FFN 레이어를 채택한다; (3) 어텐션 레이어 계산과 MoE 레이어 전체 통신과 중첩된다. (1) 및 (3)은 하드웨어 이용률을 향상시킬 것이고 (2)는 [9]를 스케일링 업할 때 성능 저하 없이 (3)을 인에이블할 수 있다.\n' +
      '\n' +
      '**로드 밸런스를 제어하고 Drop-towards-the-End.** 멀티턴 MT-Bench에 대한 우리의 결과에 따르면 Drop-towards-the-End 문제를 완화하는 것은 매우 중요하다. 이를 위해 MoE가 명령어 후속 데이터에 대한 로드 밸런스를 달성하도록 하는 것이 핵심이다. 다시, MoE는 사전 훈련 초기에 라우팅 동작을 학습하고 고정시키기 때문에, 워밍업 동안 명령어-튜닝 데이터를 사전 훈련 코퍼스에 혼합하는 간단한 솔루션이 있다. 이 데이터 혼합은 지침을 따르는 방법을 배우기 위해 모델을 정렬하는 것이 아닙니다. 대신, 우리는 이 모델이 LLM의 최종 사용 사례에 대한 길을 열어주는 명령어 조정 데이터에 대해 균형 잡힌 토큰 라우팅을 달성하기를 바란다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 작업에서 우리는 오픈 소스 커뮤니티를 위해 MoE를 훈련하는 방법을 탐구한다. 우리는 ChatGPT 이후 단계에서 MoE 기반 LLM의 효과를 검증한 긍정적인 결과를 얻었다. 우리는 모든 세부 사항을 공개했으며 공개 소스 코드와 데이터로 모델을 완전히 재현할 수 있다. 더 중요한 것은 MoE 기반 LLM에 대한 심층 분석을 수행했으며 중요한 "상황 독립적 전문화" "초기 라우팅 학습" 및 "드롭 투워드 더 엔드"를 발견했다. 또한 우리가 저지른 실수를 재고하고 미래의 개발자를 위한 가능한 솔루션을 제안합니다. 우리는 이 작업이 오픈 소스 커뮤니티가 MoE 모델에 대해 더 잘 이해하는 데 도움이 되기를 진심으로 바랍니다. 최선을 다해라!\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] R. Anil _et al._, "Palm 2 technical report," _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [2] Anonymous, "(inthe)wildchat: 570k chatGPT interaction logs in the wild," in _The Twelfth International Conference on Learning Representations_, 2024. [Online]. Available: [https://openreview.net/forum?id=B18u7ZR1bM](https://openreview.net/forum?id=B18u7ZR1bM).\n' +
      '* [3] M. Artetxe _et al._, "Efficient large scale language modeling with mixtures of experts," _arXiv preprint arXiv:2112.10684_, 2021.\n' +
      '* [4] B.-b. authors, "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," _Transactions on Machine Learning Research_, 2023, issn: 2835-8856. [Online]. Available: [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj).\n' +
      '* [5] M. Bavarian _et al._, "Efficient training of language models to fill in the middle," _arXiv preprint arXiv:2207.14255_, 2022.\n' +
      '* [6] O. r. Bojar _et al._, "Findings of the 2016 conference on machine translation," in _Proceedings of the First Conference on Machine Translation_, Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 131-198. [Online]. Available: [http://www.aclweb.org/anthology/W/W16/W16-2301](http://www.aclweb.org/anthology/W/W16/W16-2301).\n' +
      '\n' +
      '* [7] T. B. Brown _et al._, "Language models are few-shot learners," _arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* [8] M. Chen _et al._, "Evaluating large language models trained on code," 2021. arXiv: 2107.03374 [cs.LG].\n' +
      '* [9] A. Chowdhery _et al._, "Palm: Scaling language modeling with pathways," _arXiv preprint arXiv:2204.02311_, 2022.\n' +
      '* [10] H. W. Chung _et al._, "Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining," _arXiv preprint arXiv:2304.09151_, 2023.\n' +
      '* [11] T. Computer, _Redpajama: An open source recipe to reproduce lanna training dataset_, 2023. [Online]. Available: [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* [12] D. Dai _et al._, "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models," _arXiv preprint arXiv:2401.06066_, 2024.\n' +
      '* [13] A. Dosovitskiy _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [14] N. Du _et al._, "Glam: Efficient scaling of language models with mixture-of-experts," in _International Conference on Machine Learning_, PMLR, 2022, pp. 5547-5569.\n' +
      '* [15] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," _J. Mach. Learn. Res_, vol. 23, pp. 1-40, 2021.\n' +
      '* [16] H. Fu Yao; Peng and T. Khot, "How does gpt obtain its ability? tracing emergent abilities of language models to their sources," _Yao Fu\'s Notion_, Dec. 2022. [Online]. Available: [https://yaofu.notion.site/How-does-gpt-Obtain-its-ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9a5a36faiddc1](https://yaofu.notion.site/How-does-gpt-Obtain-its-ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9a5a36faiddc1).\n' +
      '* [17] L. Gao _et al._, _A framework for few-shot language model evaluation_, version v0.4.0, Dec. 2023. doi: 10.5281/zenodo.10256836. [Online]. Available: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).\n' +
      '* [18] X. Geng and H. Liu, _Openllama: An open reproduction of lama_, May 2023. [Online]. Available: [https://github.com/openlm-research/open_llama](https://github.com/openlm-research/open_llama).\n' +
      '* [19] D. Hendrycks _et al._, "Measuring massive multitask language understanding," _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* [20] J. Hoffmann _et al._, "Training compute-optimal large language models," _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [21] A. Q. Jiang _et al._, "Mixtral of experts," _arXiv preprint arXiv:2401.04088_, 2024.\n' +
      '* [22] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension," in _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, R. Barzilay and M.-Y. Kan, Eds., Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601-1611. doi: 10.18653/v1/P17-1147. [Online]. Available: [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).\n' +
      '* [23] J. D. M.-W. C. Kenton and L. K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in _Proceedings of naacL-HLT_, vol. 1, 2019, p. 2.\n' +
      '* [24] D. Koetkov _et al._, "The stack: 3 tb of permissively licensed source code," _Preprint_, 2022.\n' +
      '* [25] D. Lepikhin _et al._, "Gshard: Scaling giant models with conditional computation and automatic sharding," _arXiv preprint arXiv:2006.16668_, 2020.\n' +
      '* [26] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, "Base layers: Simplifying training of large, sparse models," in _International Conference on Machine Learning_, PMLR, 2021, pp. 6265-6274.\n' +
      '* [27] J. Li, Z. Zhang, and H. Zhao, "Self-prompting large language models for open-domain qa," _arXiv preprint arXiv:2212.08635_, 2022.\n' +
      '* [28] R. Li _et al._, "Starcoder: May the source be with you!" _arXiv preprint arXiv:2305.06161_, 2023.\n' +
      '* [29] Y. Liu _et al._, "Roberta: A robustly optimized bert pretraining approach," _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* [30] Y. Lou, F. Xue, Z. Zheng, and Y. You, "Cross-token modeling with conditional computation," _arXiv preprint arXiv:2109.02008_, 2021.\n' +
      '\n' +
      '* [31] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, "Multimodal contrastive learning with limoe: The language-image mixture of experts," _Advances in Neural Information Processing Systems_, vol. 35, pp. 9564-9576, 2022.\n' +
      '* [32] E. Nijkamp _et al._, "Xgen-7b technical report," _arXiv preprint arXiv:2309.03450_, 2023.\n' +
      '* [33] J. Puigcerver, C. Riquelme, B. Mustafa, and N. Houlsby, "From sparse to soft mixtures of experts," _arXiv preprint arXiv:2308.00951_, 2023.\n' +
      '* [34] J. W. Rae _et al._, "Scaling language models: Methods, analysis & insights from training gopher," _arXiv preprint arXiv:2112.11446_, 2021.\n' +
      '* [35] C. Raffel _et al._, "Exploring the limits of transfer learning with a unified text-to-text transformer," _Journal of Machine Learning Research_, vol. 21, no. 140, pp. 1-67, 2020. [Online]. Available: [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).\n' +
      '* [36] C. Riquelme _et al._, "Scaling vision with sparse mixture of experts," _Advances in Neural Information Processing Systems_, vol. 34, pp. 8583-8595, 2021.\n' +
      '* [37] S. Roller, S. Sukhbaatar, J. Weston, _et al._, "Hash layers for large sparse models," _Advances in Neural Information Processing Systems_, vol. 34, pp. 17 555-17 566, 2021.\n' +
      '* [38] B. Roziere _et al._, "Code llama: Open foundation models for code," _arXiv preprint arXiv:2308.12950_, 2023.\n' +
      '* [39] N. Shazeer, "Glu variants improve transformer," _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [40] N. Shazeer _et al._, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," _arXiv preprint arXiv:1701.06538_, 2017.\n' +
      '* [41] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* [42] L. Soldaini _et al._, "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research," _arXiv preprint_, 2023.\n' +
      '* [43] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," _Neurocomputing_, vol. 568, p. 127 063, 2024.\n' +
      '* [44] Y. Tay _et al._, "Ul2: Unifying language learning paradigms," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [45] Y. Tay _et al._, "Unifying language learning paradigms," _arXiv preprint arXiv:2205.05131_, 2022.\n' +
      '* [46] L.-M. Team, _Llama-moe: Building mixture-of-experts from llama with continual pre-training_, Dec. 2023. [Online]. Available: [https://github.com/pjlab-sys4nlp/lllama-moe](https://github.com/pjlab-sys4nlp/lllama-moe).\n' +
      '* [47] H. Touvron _et al._, "Llama: Open and efficient foundation language models," _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [48] B. Wang and A. Komatsuzaki, _GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model_, [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax), May 2021.\n' +
      '* [49] G. Wenzek _et al._, "CCNet: Extracting high quality monolingual datasets from web crawl data," English, in _Proceedings of the Twelfth Language Resources and Evaluation Conference_, N. Calzolari _et al._, Eds., Marseille, France: European Language Resources Association, May 2020, pp. 4003-4012, isbn: 979-10-95546-34-4. [Online]. Available: [https://aclanthology.org/2020.lrec-1.494](https://aclanthology.org/2020.lrec-1.494).\n' +
      '* [50] Y. Xu _et al._, "Gspmd: General and scalable parallelization for ml computation graphs," _arXiv preprint arXiv:2105.04663_, 2021.\n' +
      '* [51] F. Xue, X. He, X. Ren, Y. Lou, and Y. You, "One student knows all experts know: From sparse to dense," _arXiv preprint arXiv:2201.10890_, 2022.\n' +
      '* [52] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 36, 2022, pp. 8779-8787.\n' +
      '* [53] P. Yu _et al._, "Efficient language modeling with sparse all-mlp," _arXiv preprint arXiv:2203.06850_, 2022.\n' +
      '* [54] P. Zhang, G. Zeng, T. Wang, and W. Lu, _Tinyllama: An open-source small language model_, 2024. arXiv: 2401.02385 [cs.CL].\n' +
      '* [55] Y. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, "Deep long-tailed learning: A survey," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [56] L. Zheng _et al._, "Judging llm-as-a-judge with mt-bench and chatbot arena," _arXiv preprint arXiv:2306.05685_, 2023.\n' +
      '\n' +
      '* [57] Y. Zhou _et al._, "Brainformers: Trading simplicity for efficiency," in _International Conference on Machine Learning_, PMLR, 2023, pp. 42 531-42 542.\n' +
      '* [58] Y. Zhou _et al._, "Mixture-of-experts with expert choice routing," _Advances in Neural Information Processing Systems_, vol. 35, pp. 7103-7114, 2022.\n' +
      '* [59] B. Zoph _et al._, "St-moe: Designing stable and transferable sparse expert models," _URL https://arxiv. org/abs/2202.08906_, 2022.\n' +
      '\n' +
      '## 부록 자주 묻는 질문\n' +
      '\n' +
      '잠재적으로 자주 묻는 질문과 점 대 점 5의 답변을 다음과 같이 나열한다.\n' +
      '\n' +
      '### 워밍업 단계에서 체크포인트의 토큰 전문화를 보여주는 것은 어떨까요?\n' +
      '\n' +
      '우리는 라우팅이 그렇게 일찍 학습되고 수정될 것이라고 예상하지 못했습니다. 교육 중에는 제한된 스토리지 할당량으로 인해 200B 토큰마다 체크포인트만 보관합니다.\n' +
      '\n' +
      'Mixtral과 DeepSeek-MoE와 같은 고급 개방형 MoE 모델과 비교해보지 않을래?\n' +
      '\n' +
      '먼저 저희 모델은 미스트랄보다 4개월 일찍 발표되고 출시되었으며 딥시크-MoE보다 훨씬 더 많이 출시되었습니다. 둘째, 사내 학습 데이터를 사용하는 모델과 달리 우리의 모델은 완전히 투명합니다. 또한 모든 사람이 비교 가능한 OpenMoE 모델을 처음부터 훈련할 수 있도록 모든 세부 사항과 코드를 공개합니다.\n' +
      '\n' +
      '모이 업사이클링은 어때?\n' +
      '\n' +
      'MoE는 큰 배치 크기에 의해 유도된 더 나은 병렬성 때문에 추론 대신 훈련에 더 효율적이다. 조밀한 LLM 위에 MoE를 구축하는 것은 MoE 모델을 얻는 똑똑하고 빠른 방법이지만 장기적인 관점에서 더 효율적인 방법은 아니다. 대신 MoE를 조밀한 모델[51]로 증류하는 것이 성능 저하가 거의 없는 경우 도움이 될 수 있다.\n' +
      '\n' +
      '아담W 최적화기와 코사인 학습률 스케쥴을 사용하는게 어때?\n' +
      '\n' +
      'STMoE[59]에 따라 Adafactor Optimizer와 Inverse Square Root Learning Rate Schedule을 적용하였다. 애덤W 옵티마이저를 시도했지만 불안정한 문제(_즉, NAN 손실)가 자주 발생하여 상당한 양의 하이퍼-파라미터 스윕을 도입할 수 있음을 발견했다. 우리가 가지고 있는 제한된 계산 자원들을 고려하여, 우리는 ST-MoE[59]로부터 잘 연구된 학습률 스케줄을 단순히 따르기로 결정한다.\n' +
      '\n' +
      '더 나은 데이터 세트와 더 큰 데이터 세트를 사용하는 것은 어떨까요?\n' +
      '\n' +
      '5월 2023년 이 프로젝트를 시작할 때 사용 가능한 오픈 소스 사전 훈련 데이터 세트는 거의 없었다. 그러나 오픈소스 사전학습 데이터셋의 규모와 품질은 점점 더 좋아지고 있다. 예를 들어, 솔다이니 _et al._[42]는 세심한 청소와 함께 3T 토큰을 출시했습니다. 컴퓨터[11]도 총 30T 토큰이 포함된 거대한 데이터셋을 공개했다. 향후 더 나은 데이터에 대한 교육이 일반적으로 LLM 성능을 크게 향상시킬 것이라고 믿습니다.\n' +
      '\n' +
      'Hyper-parameters\n' +
      '\n' +
      'OpenMoE-8B/32E의 경우 64가 아닌 128로 헤드 치수를 설정했는데, 이는 2B 활성화된 트랜스포머 매개변수를 사용하는 모델에 비해 너무 클 수 있다. 64를 사용하면 우리보다 더 나은 비용 효율성 절충을 유도할 수 있다고 제안한다. 위의 표의 매개변수 수는 트랜스포머 블록의 대부분의 매개변수가 주의 층과 FFN 층이기 때문에 단순화를 위해 이 두 개의 훈련 가능한 매개변수만 설명한다.\n' +
      '\n' +
      'AdamW로 훈련된 기존 LLM과 달리 메모리 효율이 높은 최적화기인 Adafactor를 사용했다. 동일한 학습 단계에서 AdamW보다 성능이 약간 떨어지지만, 메모리 효율성은 모델 병렬성을 줄이고 데이터 병렬성을 더 많이 사용할 수 있게 한다. 이 경우 Adafactor를 사용하면 AdamW를 사용하여 동일한 데이터에서 동일한 모델을 훈련하는 것보다 교육을 더 저렴하게 할 수 있다. 그러나, 우리는 이 갭의 마진이 하드웨어 및 모델 크기에 크게 의존하기 때문에 불분명하다는 것을 강조한다. 인프라, 즉 TPUv3의 경우 제한된 온 칩 메모리(코어당 16GB)로 인해 이 격차가 상대적으로 더 커야 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Layout** & \\(H\\) & \\(H_{\\text{FFN}}\\) & \\(N_{\\text{Head}}\\) & \\(H_{\\text{Head}}\\) & \\(L\\) & **\\#Param** & **\\#ActParam w/E** & **\\#ActParam** \\\\ \\hline OpenMoE-8B/32E & Every 4 & 768 & 3072 & 12 & 64 & 12 & 650M & 339M & 142M \\\\ OpenMoE-8B/32E & Every 6 & 2048 & 8192 & 24 & 128 & 24 & 8.78 & 2.68 & 2.18 \\\\ OpenMoE-34B/32E & Every 4 & 3072 & 12288 & 24 & 128 & 32 & 34B & 6.88 & 6.08 \\\\ \\hline TinLAM & - & 2048 & 5632 & 32 & 64 & 22 & 1.08 & 1.08 & 0.98 \\\\ OpenLAM-3B & - & 3200 & 8640 & 32 & 64 & 26 & 3.08 & 3.08 & 2.98 \\\\ LiAM-7B & - & 4096 & 11008 & 32 & 128 & 32 & 6.68 & 6.48 & 6.48 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 모델 구성. \\ (H\\)은 숨겨진 크기이다. “Layout”은 MoE 층을 사용하는 방법을 의미한다. 예를 들어, "Every 4"는 4개의 변압기 블록마다 하나의 MoE 층을 사용한다는 것을 의미한다. \\ (H_{\\text{FFN}}\\)는 FFN 중간 크기이다. \\ (N_{\\text{Head}}\\) 및 \\(H_{\\text{Head}}\\)는 주의 헤드 수 및 주의 헤드 치수이다. \\ (L\\)는 층수이다. # Param은 총 파라미터이다. # ActParam은 트랜스포머 블록에서 각 토큰을 처리하는 데 사용한 매개 변수의 수입니다. ActParam w/E는 #ActParam의 합과 토큰 임베딩 레이어의 파라미터 수이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline  & **Base/16E** & **8B/32E** & **34B/32E** \\\\ \\hline Optimizer & \\multicolumn{3}{c}{Adafactor} \\\\ Batch Size & 128 & 2048 & 2048 \\\\ Training Steps & 500K & 500K & 100K \\\\ Peak Learning Rate & \\multicolumn{3}{c}{0.01} \\\\ Learning Rate Schedule & \\multicolumn{3}{c}{Inverse Square Root Decay} \\\\ Warmup Steps & \\multicolumn{3}{c}{10K} \\\\ Sequence Length & \\multicolumn{3}{c}{2048} \\\\ Load Balance Loss Weight & \\multicolumn{3}{c}{0.01} \\\\ Z-Loss Weight & \\multicolumn{3}{c}{0.001} \\\\ Router Z-Loss Weight & \\multicolumn{3}{c}{0.0001} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: OpenMoE 트레이닝 하이퍼-파라미터.\n' +
      '\n' +
      'Related Work\n' +
      '\n' +
      '### Before OpenMoE\n' +
      '\n' +
      'MoE는 새로운 것이 아니다. 대표적인 초기 노력 중 하나는 Shazeer _et al._[40] MoE 레이어를 순환 언어 모델에 임베딩하는 것이다. 트랜스포머 아키텍처의 확장성으로 인해 GShard[25]는 MoE를 트랜스포머 레이어에 통합하고 전문가 병렬성을 사용하여 규모에서 MoE 기반 트랜스포머를 훈련한다. 스위치 트랜스포머[15]는 인코더-디코더 구조를 사용하고 C4[35] 데이터 세트로 훈련된 가장 초기 오픈 소스 MoE 기반 LM이다. 대규모 사전 훈련에서 스위치 트랜스포머의 성공으로 인해 MoE는 더 많은 관심을 받았고, 더 진보된 라우팅 알고리즘이 발명되었다. 예를 들어, BASE Layers[26]은 토큰 대 전문가 할당을 선형 할당 문제로 공식화하여 각 전문가가 동일한 수의 토큰을 받는 최적의 할당을 허용한다. 롤러 _et al._[37]은 단순히 피드포워드 레이어를 현재 토큰에 따라 다른 가중치 세트에 대한 해시로 수정하고 학습 기반 라우팅과 비교하여 유망한 결과를 달성한다. 위의 상이한 토큰 기반 라우팅, Zhou _et al._[58]은 전문가들이 그들이 선호하는 토큰들, 즉 _i.e._ Expert-Choice Routing을 선택하도록 할 것을 제안한다. 전문가 선택 라우팅은 더 균형 잡힌 토큰 할당과 더 나은 비용 효율성 절충을 달성합니다.\n' +
      '\n' +
      '라우팅 알고리즘 외에도 MoE를 효율적으로 스케일링하는 데 중점을 둔 작업도 있다. Artetxe _et al._[3]은 RoBERTa[29]와 CC100[49](총 112B 토큰)에서 사용되는 데이터셋을 중심으로 MoE 모델을 학습시켰다. GaLM[14]은 1.6T 토큰이 있는 사내 고품질 데이터 세트를 가진 디코더 전용 MoE 모델을 추가로 확장한다. 브레인포머[57]는 MoE 속성들을 발견하기 위한 진화적 탐색을 제안하는데, 예를 들어, 계층들 및 계층 용량들을 인터리브하는 가장 좋은 방법, 계층들을 퓨즈할 때, 및 MoE 모듈들로 계층들을 전문화하고 상이한 스케일들에서 그 효과를 나타낸다.\n' +
      '\n' +
      '언어 모델링 외에도, Vision Transformer(ViT) [13]도 MoE 아키텍처에 의해 향상될 수 있다. ViT-MoE[36]은 ViT 모델에서 MoE의 확장성을 검증한다. 와이드넷[52]은 MoE 기반 트랜스포머 블록을 개별 계층 정규화와 공유하여 더 나은 파라미터 효율을 달성한다. SoftMoE[33]은 소프트 토큰 선택을 적용하여 라우팅 알고리즘을 더욱 개선하여 효율성을 유지할 뿐만 아니라 라우팅 경사도를 안정화시킨다. 또한, MoE를 비-트랜스포머 아키텍처, 예를 들어, 컴퓨터 비전을 위한_Sparse-MLP[30] 및 언어 모델링[53]을 위한 s-MoE에 포함시키기 위한 일부 노력들이 있다.\n' +
      '\n' +
      '### After OpenMoE\n' +
      '\n' +
      '우리는 이 보고서를 작성하는 것보다 훨씬 빨리 모델과 구현을 출시했습니다. 표 13에 나타난 바와 같이, 우리의 출시 후, _예를 들어,_ Mixtral[21] 및 Deepseek-MoE[12]와 같은 일부 부분 오픈 소스 모델이 출시된다. 우리가 알고 있듯이 이러한 모델은 최종 결과 측면에서 훨씬 더 우수하다. 그러나 이러한 모델은 사내 데이터로 훈련되기 때문에 어떻게 일이 발생했는지 알 수 없습니다. 비록 우리의 결과가 그렇게 놀랍지는 않지만, 완전히 개방된 자연과 심층적인 분석은 모두 커뮤니티에 의미가 있다고 믿습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Model Name** & **Dataset Size** & **Reproducible** & **Release Date** \\\\ \\hline Switch Transformer [15] & 156B & Yes & Feb 2021 \\\\ Meta-MoE [3] & 112B & Yes & Dec 2021 \\\\ \\hline OpenMoE (Ours) & 1.1T & Yes & Aug 2023 \\\\ \\hline Mixtral of Experts [21] & Unknown & No & Dec 2023 \\\\ LLaMA-MoE [46] & 200B & Yes & Dec 2023 \\\\ DeepSeek-MoE [12] & 2T & No & Jan 2024 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: 오픈 소스 MoE LLMs 타임라인. 우리는 오픈 소스 MoE LLM을 정렬하기 위한 열쇠로 모델 출시 날짜를 사용합니다. 데이터셋 사이즈는 사전 트레이닝 데이터세트 내의 토큰들의 수, 즉, 하나의 에폭에 대한 토큰들의 수이다. LLaMA-MoE는 기성품 LLaMA 패밀리 모델에 대해 사전 훈련을 계속한다. 우리는 계속 훈련 데이터 세트만 계정합니다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n' +
      '## 부록 E 라우팅 결정 표준 편차\n' +
      '\n' +
      '도 13 및 도 14에서, 우리는 토큰 ID들이 위치 ID들보다 라우팅 결정들에 대해 더 큰 표준 편차를 갖는다는 것을 분명히 알 수 있다. 또한, 대부분의 토큰 ID들은 상대적으로 큰 표준 편차를 가지며, 이는 대부분의 토큰 ID들이 Context-independent Routing을 갖는다는 것을 의미한다.\n' +
      '\n' +
      '도 14: 상이한 토큰 ID들에서의 라우팅 결정 표준 편차. 매우 낮은 자원 토큰은 항상 라우팅 결정 표준 편차가 크기 때문에 우리는 128개 이상의 토큰을 가진 토큰 ID만 가져간다. 토큰 ID도 전혀 분산되지 않았습니다.\n' +
      '\n' +
      '도 13: 상이한 위치 ID에서의 라우팅 결정 표준 편차.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>