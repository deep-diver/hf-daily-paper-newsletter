<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#판다-70M: 다중 교차모달리티 교사가 포함된 캡션 70M 비디오\n' +
      '\n' +
      'Tsai-Shien Chen\\({}^{1,2,*}\\) Aliaksandr Siarohin\\({}^{1}\\) Willi Menapace\\({}^{1,3,*}\\) Ekaterina Deyneka\\({}^{1}\\)\n' +
      '\n' +
      '황위차오({}^{1}\\) 병은전({}^{1}\\) 유위방({}^{1}\\) 신영이({}^{1}\\) 지안렌({}^{1}\\)\n' +
      '\n' +
      'Ming-Hsuan Yang\\({}^{2}\\) Sergey Tulyakov\\({}^{1}\\)\n' +
      '\n' +
      '({}^{1}\\)Snap Inc. \\({}^{1}\\) ({}^{2}\\) California 대학교, Merced \\({}^{3}\\) Trento 대학교\n' +
      '\n' +
      '[https://snap-research.github.io/Panda-70M](https://snap-research.github.io/Panda-70M)\n' +
      '\n' +
      '이 작업은 스냅에서 인턴을 하는 동안 수행되었습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '데이터 및 주석의 품질은 다운스트림 모델의 품질을 상한으로 합니다. 큰 텍스트 코퍼스와 이미지-텍스트 쌍이 존재하지만, 고품질 비디오-텍스트 데이터는 수집하기가 훨씬 더 어렵다. 우선 수동 라벨링은 주석자가 전체 동영상을 시청해야 하기 때문에 시간이 더 많이 소요된다. 둘째, 비디오는 시간적 차원을 가지며, 여러 장면들이 함께 쌓이고, 여러 동작을 보여준다. 따라서 고품질 캡션을 가진 비디오 데이터 세트를 구축하기 위해 텍스트 비디오 설명, 자막 및 개별 비디오 프레임과 같은 멀티모달 입력을 활용하는 자동 접근법을 제안한다. 구체적으로, 공개 가능한 HD-VILA-100M 데이터 세트에서 3.8M 고해상도 비디오를 큐레이션한다. 그런 다음 의미적으로 일관된 비디오 클립으로 분할하고 여러 교차 모달리티 교사 모델을 적용하여 각 비디오에 대한 캡션을 얻는다. 다음으로, 각 비디오의 베스트 캡션이 수동으로 선택된 작은 서브셋에서 검색 모델을 세밀하게 조정한 다음 전체 데이터 세트에서 모델을 사용하여 베스트 캡션을 주석으로 선택한다. 이렇게 하면 고품질 텍스트 캡션과 페어링된 70M 비디오를 얻을 수 있습니다. 우리는 데이터 세트를 판다-70M으로 더빙한다. 본 논문에서는 비디오 캡션, 비디오 및 텍스트 검색, 텍스트 기반 비디오 생성의 세 가지 다운스트림 태스크에 대해 제안된 데이터 세트의 값을 보여준다. 제안된 데이터 점수에 대해 훈련된 모델은 모든 작업에 걸쳐 대부분의 메트릭에서 실질적으로 더 우수하다.\n' +
      '\n' +
      '**그림 1**: **판다-70M을 기존의 대규모 비디오 언어 데이터셋과 비교. 우리는 여러 교차 모달리티 비전 언어 모델에 의해 주석이 달린 캡션을 가진 대규모 비디오 데이터 세트인 판다-70M을 소개한다. 기존 데이터셋의 텍스트 주석[80]에 비해 판다-70M의 캡션은 비디오(녹색으로 강조됨)에서 주요 객체와 동작을 보다 정확하게 설명한다. 게다가, 판다-70M의 비디오들은 의미적으로 일관성이 있고, 고해상도이며, 워터마크로부터 자유롭다. 부록 E.**에서 더 많은 샘플을 찾을 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 멀티모달 학습을 위해서는 컴퓨팅과 데이터의 크기가 필수 불가결한 시대로 진입한다. 대부분의 돌파구는 대규모 컴퓨팅 인프라, 대규모 모델 및 대규모 데이터에 의해 달성된다. 이러한 적분 성분으로 인해 우리는 강력한 텍스트 대 이미지[4, 57, 59, 61, 83]와 이미지 대 텍스트 모델[2, 36, 43, 53]을 가지고 있다. 모델 크기나 계산량을 확장하는 것은 어렵고 비용이 많이 들지만 유한한 엔지니어링 시간이 필요하다. 데이터를 확장하는 것은 인간이 각 샘플을 분석하는 데 시간이 걸리기 때문에 상대적으로 더 어렵다.\n' +
      '\n' +
      '특히, 이미지-텍스트 쌍[10, 12, 62]에 비해, 비디오-텍스트 쌍은 얻기가 더욱 어렵다. 첫째, 주석이 레이블링되기 전에 전체 비디오를 시청해야 하기 때문에 주석이 달린 비디오는 시간이 더 많이 소요된다. 둘째, 비디오들은 종종 함께 스티칭된 다수의 장면들을 포함하고 시간적으로 변하는 콘텐츠로 구성된다. 마지막으로, 자막, 비디오 기술, 보이스 오버와 같은 메타 정보는 종종 너무 광범위하거나 시간적으로 올바르게 정렬되지 않거나 비디오를 정확하게 기술할 수 없다. 예를 들어, HD-VILA-100M [80] 및 HowTo100M [52]와 같은 몇 개의 100M 스케일 데이터 세트는 자동 음성 인식(ASR)에 의해 주석이 달린다. 그러나, 도 1에 도시된 바와 같이, 자막은 보통 비디오에 제시된 주요 콘텐츠 및 액션을 포함하지 못한다. 이는 멀티모달 트레이닝을 위한 그러한 데이터 세트의 값을 제한한다. 우리는 표 1의 커뮤니티에서 사용할 수 있는 데이터 세트를 요약한다. 일부는 저해상도이고 일부는 ASR에 의해 주석이 달리고 일부는 제한된 도메인의 데이터를 포함하고 일부는 소규모이며 일부는 짧은 캡션을 제공한다.\n' +
      '\n' +
      '본 연구에서는 자막 주석이 있는 70M 비디오 클립을 포함하는 대규모 데이터 세트를 제시한다. 여기에는 캡션당 평균 13.2개의 단어를 가진 풍부한 캡션이 있는 오픈 도메인의 고해상도 비디오가 포함됩니다. 70M 비디오에 수동으로 주석을 달면 비용이 엄청나게 많이 들지만 자동 주석을 선택합니다. 우리의 주요 통찰력은 비디오가 일반적으로 자동 캡션을 지원할 수 있는 여러 양식의 정보와 함께 제공된다는 것입니다. 여기에는 비디오의 제목, 설명, 자막, 개별 정적 프레임 및 비디오 자체가 포함된다. 부분적으로만 사용할 경우 이 데이터의 값을 완전히 최대화할 수 없습니다. 이에 비해, 다양한 교차 모달리티 캡션 모델의 입력으로 멀티모달 데이터의 서로 다른 조합을 활용할 것을 제안한다. 이 개념을 입증하기 위해 인간 평가를 기반으로 수치 분석을 수행한다(자세한 내용은 부록 B.3에 나와 있다). 다중 교차 모달리티 모델을 사용하여 일부 비디오 샘플에 대한 캡션을 수행하고 그 결과를 사람에게 보여줌으로써 평가한다면, 우리는 비디오의 \\(31\\%\\) 이상에 대해 좋은 캡션을 생성할 수 있는 단일 모델이 없다는 것을 알 수 있다. 그러나, 서로 다른 모델로부터 모든 캡션을 공동으로 수집하면, 적어도 하나의 좋은 캡션으로 비디오의 \\(84.7\\%\\)에 주석이 달릴 수 있음을 알 수 있다.\n' +
      '\n' +
      '이러한 마음가짐으로 데이터셋을 구축하기 위해 HD-VILA-100M [80]에서 수집한 3.8M 고해상도 긴 비디오를 사용하여 시작하여 다음 세 단계를 통해 처리한다. 먼저, 긴 비디오를 의미적으로 일관된 클립으로 절단하면서, 의미 일관성과 비디오 클립의 지속 시간 사이의 균형을 맞추기 위해 의미 인식 비디오 분할 알고리즘을 설계한다. 둘째, 영상 자막 모델[37]과 영상/영상 시각 질의 응답(VQA) 모델[38, 88, 94]을 포함한 다양한 교차 모달리티 교사 모델을 사용하여 동영상 설명 및 자막과 같은 추가 텍스트 입력을 통해 클립에 대한 여러 후보 캡션을 예측한다. 마지막으로, 인간 주석자가 오라클 역할을 하여 각 비디오에 대한 최상의 캡션을 선택하는 100K 비디오 서브세트를 수집한다. 우리는 이 데이터 세트를 사용하여 세밀한 비디오-텍스트 검색 모델 [39]을 세밀하게 조정한 다음 전체 데이터 세트에 적용하여 가장 정확한 캡션을 주석으로 선택한다. 여러 교사 모델을 실행하는 것은 계산 비용이 많이 들고 시간이 많이 소요된다. 향후 스케일에서 효율적인 비디오 캡셔닝을 추구하기 위해 교사로부터 지식을 증류하기 위한 학생 모델을 훈련한다. 학생 모델은 시각적 입력과 텍스트 입력을 동시에 취할 수 있는 2-분기 구조를 채택하여 멀티모달 정보로부터 캡션을 얻을 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Dataset & Year & Text & Domain & \\#Videos & Avg/Total video len & Avg text len & Resolution \\\\ \\hline HowTo100M [52] & 2019 & ASR & Open & 136M & 3.6s & 134.5Khr & 4.0 words & 240p \\\\ ACAV [32] & 2021 & ASR & Open & 100M & 10.0s & 277.7Khr & - & - \\\\ YT-Temporal-180M [87] & 2021 & ASR & Open & 180M & - & - & - & - \\\\ HD-VILA-100M [80] & 2022 & ASR & Open & 103M & 13.4s & 371.5Khr & 32.5 words & 720p \\\\ \\hline MSVD [13] & 2011 & Manual caption & Open & 1970 & 9.7s & 5.3h & 8.7 words & - \\\\ LSMDC [58] & 2015 & Manual caption & Movie & 118K & 4.8s & 158h & 7.0 words & 1080p \\\\ MSR-VTT [79] & 2016 & Manual caption & Open & 10K & 15.0s & 40h & 9.3 words & 240p \\\\ DiDeMo [3] & 2017 & Manual caption & Flickr & 27K & 6.9s & 87h & 8.0 words & - \\\\ ActivityNet [11] & 2017 & Manual caption & Action & 100K & 36.0s & 849h & 13.5 words & - \\\\ YouCook2 [93] & 2018 & Manual caption & Cooking & 14K & 19.6s & 176h & 8.8 words & - \\\\ VATEX [73] & 2019 & Manual caption & Open & 41K & \\(\\sim\\)10s & \\(\\sim\\)115h & 15.2 words & - \\\\ \\hline\n' +
      '**Panda-70M (Ours)** & 2024 & Automatic caption & Open & 70.8M & 8.5s & 166.8Khr & 13.2 words & 720p \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **판다-70M 및 기타 비디오 언어 데이터 세트의 비교.** 데이터 세트를 두 그룹으로 나누었다.\n' +
      '\n' +
      '광범위한 실험은 제안된 판다-70M1을 사용한 사전 훈련이 비디오 캡션, 비디오 및 텍스트 검색, 텍스트 대 비디오 생성을 포함한 여러 다운스트림 작업에 도움이 될 수 있음을 보여준다. 또한 지식 증류 방식으로 학생 모델을 훈련하면 표 3과 같이 어떤 교사 모델보다 \\(7.7\\%\\) 이상의 선호도 비율만큼 우수한 학생 모델을 학습할 수 있으며, 비디오 기술 및 자막과 같은 추가 텍스트 입력에 의해 성능이 더욱 향상될 수 있음을 보여준다.\n' +
      '\n' +
      '각주 1: 우리는 우리의 데이터 세트 판다라고 부르며, 여러 부분 미술 교사로부터 배우는 판다 포와 비유를 그린다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**Vision-Language Datasets.** 수백만 또는 심지어 수십억 개의 이미지-텍스트 쌍을 갖는 트레이닝 [10, 31, 55, 62, 86]은 강력한 이미지 기초 모델 [2, 6, 21, 25, 27, 82]을 학습하는데 효과적인 것으로 나타났다. 이 작업을 통해 우리의 목표는 풍부한 캡션을 포함하는 대규모 비디오 언어 데이터 세트를 구축하는 것이다. 표 1의 관련 데이터 세트를 비교하였는데, 여러 선행 비디오 언어 데이터 세트[3, 11, 13, 58, 79, 93]에는 액션 인식, 비디오 이해, VQA 및 검색과 같은 다양한 작업을 다루는 데이터가 포함되어 있다. 그러나 수동으로 데이터를 주석하는 것은 비용이 많이 들고 이러한 데이터 세트의 규모를 제한한다(일반적으로 120K 미만의 샘플을 포함한다). 데이터 부족을 완화하기 위해 [52, 80, 87]의 작업은 ASR에 의해 생성된 자막으로 데이터에 자동으로 주석을 달 것을 제안한다. 이 방법은 샘플 100M에 도달하는 데이터 세트 스케일을 크게 증가시키지만, 불행히도 자막은 그림 1과 같이 주요 비디오 콘텐츠를 정확하게 설명하지 못한다. 이에 비해 본 연구에서는 고품질 비디오 캡션 쌍의 데이터 세트를 70M 스케일로 확장할 수 있는 멀티모달 데이터 입력을 갖는 자동 캡션 파이프라인을 제안한다.\n' +
      '\n' +
      '**비전-언어 모델**은 시각적 데이터(이미지 또는 비디오)와 언어적 신호(단어 또는 문장) 사이의 상관관계를 학습하고, 텍스트-구동 이미지 또는 비디오 생성[4, 8, 29, 57, 59, 61, 65, 83, 92], 캡션[2, 36, 37, 43, 63, 81], VQA[14, 38, 53, 88, 94] 및 검색[17, 39, 49]을 포함하는 여러 다운스트림 애플리케이션에 적용될 수 있다. 우리는 판다-70M의 주석을 위해 몇 가지 비전 언어 모델을 활용한다. BLIP-2 [37]은 이미지 캡셔닝을 용이하게 할 수 있는 효율적인 비전 언어 사전 훈련을 소개한다. 우리는 BLIP-2를 교사 중 하나로 사용하고 캡션을 위해 무작위로 샘플링된 비디오 프레임을 입력한다. MiniGPT-4[94]는 LLM(Large Language Model)과 비주얼 인코더를 정렬하기 위해 프로젝션 레이어를 학습하는 이미지 VQA 모델이다. 비디오 프레임 외에도 비디오 설명 및 자막과 같은 추가 텍스트 정보가 있는 프롬프트를 입력하고 모델에 모든 멀티모달 입력을 요약하도록 요청한다. 비디오 촬영장비의 경우 Video-LLaMA[88]와 VideoChat[38]은 모두 비디오 VQA 모델이며 LLM 호환 비주얼 임베딩을 추출하는 방법을 학습한다. 우리는 두 모델을 모두 사용하고 빠른 입력으로 비디오를 캡션하도록 요청합니다. 또한, 마스킹되지 않은 교사[39]는 비디오 이해를 용이하게 할 수 있는 비디오 기초 모델이다. 세밀한 검색을 구현하기 위해 세밀하게 조정하고 더 정확한 캡션을 주석으로 선택하는 데 사용한다.\n' +
      '\n' +
      '**멀티모달 모델을 통한 비디오 주석.** 앞에서 언급한 비전 언어 모델에 대한 개발로 일부 동시 작업 [7, 72, 75]도 이러한 모델을 비디오 캡셔닝에 활용합니다. 비디오 팩토리[72]는 캡션 비디오 클립에 BLIP-2[37]을 사용한다. 그러나 부록 B.3에 보고된 바와 같이 단일 BLIP-2 모델의 성능은 최적이 아니다. 우리의 캡션 파이프라인인 InternVid[75] 및 Stable Video Diffusion[7]과 더 유사하게 요약하기 위해 LLM이 뒤따르는 다중 캡션 모델도 사용한다. 실제로, 우리는 LLM이 시각 언어 모델의 노이즈 출력에서 오류를 전파한다는 것을 발견했다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '팬더-70M을 구축하기 위해, 우리는 HD-VILA-100M에서 수집된 3.8M 고해상도 긴 비디오를 활용한다[80]. 그런 다음 섹션 3.1에 설명된 대로 70.8M 의미적으로 일관성 있는 클립으로 분할한다. 섹션 3.2는 후보 캡션 주석 세트를 생성하기 위해 다중 교차 모달리티 교사 모델이 어떻게 사용되는지 보여준다. 다음으로 세밀한 검색 모델을 세분화하여 섹션 3.3에 자세히 설명된 대로 가장 정확한 캡션을 선택한다. 마지막으로 섹션 3.4에서는 섹션 3.4에 대한 접근 방식을 설명한다.\n' +
      '\n' +
      '도 2: **비디오 캡션 파이프라인. 긴 비디오가 주어지면, 우리는 먼저 그것을 의미적으로 일관성 있는 몇 개의 클립으로 나눈다. 그 후, 비디오 클립에 대한 다중 캡션을 생성하기 위해 서로 다른 멀티모달 입력을 갖는 다수의 교사 모델을 활용한다. 마지막으로 비디오 클립을 주석으로 가장 잘 설명하는 캡션을 선택하기 위해 세밀한 검색 모델을 세밀하게 조정한다.\n' +
      '\n' +
      '판다-70M을 사용하여 학생 캡션 모델을 트레이닝하는 단계를 포함하는 방법. 우리의 접근법에 대한 높은 수준의 견해는 그림 2에 나와 있다.\n' +
      '\n' +
      '### 의미 인식 비디오 분할\n' +
      '\n' +
      '비디오 캡션 데이터 세트에서 원하는 비디오 샘플은 다소 모순되는 두 가지 특성을 가져야 한다. 한편으로, 비디오는 의미적으로 일관되어야 하고, 따라서 비디오 샘플들은 액션 인식과 같은 다운스트림 작업들에 더 잘 이익을 줄 수 있고, 캡션은 또한 모호성 없이 그것의 의미론적 내용을 더 정확하게 표현할 수 있다. 한편, 비디오는 비디오 생성과 같이 작업에 유익한 의미 있는 모션 콘텐츠를 포함하기에 너무 짧거나 단편적일 수 없다.\n' +
      '\n' +
      '두 가지 목표를 달성하기 위해, 우리는 긴 비디오를 의미적으로 일관성 있는 클립으로 절단하기 위해 2단계 의미 인식 분할 알고리즘을 설계한다. 첫 번째 단계에서는 새로운 장면이 시작될 때 시맨틱스가 자주 바뀌기 때문에 샷 경계 검출[1]을 기반으로 비디오를 분할한다. 두 번째 단계에서는 인접한 클립이 첫 번째 단계에서 잘못 분리되면 스티치하여 비디오가 너무 짧아지지 않도록 합니다. 이를 위해 ImageBind[25]를 사용하여 비디오 프레임의 임베딩을 추출하고, 두 클립에서 프레임 임베딩을 유사한 경우 인접한 클립을 병합한다. 또한 1) 컷-씬이 없는 긴 비디오, 2) 일반적으로 컷-씬으로 검출되지 않는 페이드-인 및 페이드-아웃 효과와 같은 복잡한 전환을 사용하는 비디오, 3) 데이터 세트의 다양성을 증가시키기 위해 중복 클립을 제거하는 추가 절차를 구현한다. 분할 알고리즘에 대한 자세한 내용은 부록 A에 있다. 특히, 데이터 세트는 일관된 의미론을 갖는 세밀한 비디오-텍스트 쌍에 초점을 맞추지만, 사용자는 동일한 긴 비디오로부터 분할되기 때문에 연속적인 클립과 캡션을 연결함으로써 다수의 컷-장면을 갖는 긴 비디오를 여전히 획득할 수 있다.\n' +
      '\n' +
      '비디오 클립의 의미적 일관성을 정량적으로 검증하기 위해, 비디오 클립 내에서 가장 중요한 지각 변화를 강조하는 Max Running LPIPS를 소개한다. 형식적으로, \\(n\\) 초의 비디오 클립이 주어지면, 우리는 매 초마다 비디오 프레임들을 서브샘플링하고 키 프레임들을 \\(\\{f_{1},...,f_{n}\\}\\)으로 표시한다. Max Running LPIPS는 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[\\max(\\\\text{LPIPS}(f_{i},f_{i+1})\\mid i\\in[1,n-1]\\}). \\tag{1}\\\n' +
      '\n' +
      '여기서 LPIPS\\((\\cdot,\\cdot)\\)는 두 이미지의 지각적 유사성[89]이다. 표 2에서와 같이, 우리의 분할은 바닐라 샷 경계 검출[1]보다 긴 비디오 길이를 유지하면서 자막 문장의 정렬을 기반으로 하는 분할보다 더 나은 의미 일관성을 달성한다[52, 80].\n' +
      '\n' +
      '교차모달리티 교사를 활용한 캡셔닝\n' +
      '\n' +
      'HD-VILA-100M [80]의 비디오는 캡션에 유익한 풍부한 멀티모달 정보를 포함한다. 구체적으로, 비디오 자체 외에도 유용한 텍스트(_e.g_., 비디오 제목, 설명, 자막) 및 이미지(_e.g_., 개별 비디오 프레임)도 존재한다. 이러한 통찰력에 힘입어 다양한 양식의 입력과 함께 여러 캡션 모델을 사용할 것을 제안한다.\n' +
      '\n' +
      '31개의 캡션 모델을 포함한 큰 풀부터 시작합니다. 모델 풀의 도입은 부록 B.1에 있으며, 70M 비디오 클립에서 모든 모델의 추론을 실행하는 것은 계산 비용이 많이 들기 때문에 사용자 연구를 기반으로 8개의 우수한 모델의 짧은 목록을 구성한다. 목록은 그림 3의 y축에 나와 있으며, 이 과정에 대한 자세한 내용은 부록 B.3에 나와 있다. 간단히 말하면 사전 훈련 가중치와 입력 정보가 다른 5개의 기본 모델로 구성된다. 5가지 기본 모델로는 Video-LLaMA[88](video VQA), VideoChat[38](video VQA), VideoChat Text[38](video content를 텍스트화하는 자연어 모델), BLIP-2[37](image captioning), MiniGPT-4[94](image VQA) 등이 있다. 교차 모달리티 교사 모델에 의한 비디오 캡셔닝을 구현하기 위해 각 모달리티에 맞춘 별개의 캡션 프로세스를 공식화한다. 예를 들어, VQA 모델의 경우 시각적 데이터 외에도 추가 텍스트 정보가 포함된 프롬프트를 입력하고 모델에 모든 멀티모달 입력을 하나의 문장으로 요약하도록 요청한다. 각 교사 모델의 자막화 과정에 대한 자세한 내용은 부록 B.2에 기재되어 있다.\n' +
      '\n' +
      '우리는 서로 다른 촬영장비 데이터를 사용하는 교사 모델이 서로 다른 종류의 비디오에서 잘 수행된다고 가정한다. 예를 들어, 비디오 모델은 시간 정보를 처리하기 위한 추가 모듈로 인해 복잡한 역학을 가진 비디오에 대해 더 잘 수행할 수 있다. 반면에 이미지 모델은 희귀하고 드문 객체로 비디오를 정확하게 캡션할 수 있으며, 이는 대규모 데이터 세트를 사용하여 훈련되었기 때문이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & Max running LPIPS\\(\\downarrow\\) & Avg Video Len \\\\ \\hline Sub. Align [52, 80] & 0.408 & 11.8s \\\\ PySceneDetect [1] & 0.247 & 4.1s \\\\ \\hline Our Splitting & 0.256 & 7.9s \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **분할 알고리즘의 비교.** 1K 긴 비디오를 세 가지 알고리즘으로 분할하고 제안된 Max Running LPIPS에 의해 출력 클립의 의미 일관성을 테스트한다. 우리의 분할은 의미론적 일관성과 클립 길이 사이의 균형을 위해 더 나은 균형을 이룬다.\n' +
      '\n' +
      '그림 3: **교사 모델의 선택적 비율의 분포.** 1,805개의 테스트 비디오에 8명의 교사의 선택적 비율의 분포를 표시한다. 결과는 사전 훈련된(빨간색) 또는 미세 조정된(녹색) 마스크 없는 교사[39] 및 인간 주석자(파란색)의 선택에 기초한다.\n' +
      '\n' +
      '이미지-텍스트 쌍[62]. 마지막으로 시각적으로 이해하기 어려운 동영상의 경우 VQA 모델은 텍스트 단서를 추가로 사용할 수 있으므로 레버리지를 갖는다.\n' +
      '\n' +
      '이 가설은 수치 평가에 의해 뒷받침될 수 있다. 구체적으로, 8명의 후보자 중 가장 좋은 캡션을 선택하도록 참가자에게 요청하는 사용자 연구를 수행한다. 우리는 그림 3(파란색 막대)에 각 교사 모델의 선택 비율을 표시한다. 결과는 최상의 캡션이 다른 교사 모델에 의해 생성됨을 보여준다. 또한, 개별 교사 모델(_i.e_., BLIP-2 with opt6.7b[90])의 가장 높은 선택률은 \\(17.85\\%\\)에 불과하다. 이 사실은 매우 다양한 비디오에서 단일 모델의 제한된 캡션 기능을 표현한다.\n' +
      '\n' +
      '### 세밀한 비디오-텍스트 검색\n' +
      '\n' +
      '비디오에 대한 여러 후보 캡션이 주어지면 비디오 콘텐츠와 가장 잘 일치하는 캡션을 찾습니다. 직관적인 아이디어는 이러한 캡션을 선택하기 위해 사용 가능한 일반적인 비디오-텍스트 검색 모델[25, 39]을 사용하는 것이다. 불행히도, 우리는 그들이 일반적으로 최적의 결과를 선택하지 못한다는 것을 발견했습니다. 한 가지 이유는 일반 모델이 대조 학습 목표[15, 82]를 사용하여 훈련되고, 하나의 샘플을 완전히 관련이 없는 다른 샘플2와 구별하는 것을 학습하기 때문이다. 대조적으로, 우리의 경우, 모든 후보 캡션은 비디오 샘플과 매우 관련이 있고, 모델이 최적의 성능을 위해 각각의 캡션 내에서 미묘한 구별을 식별하도록 요구한다.\n' +
      '\n' +
      '각주 2: 대조 학습을 위한 음성 샘플[15]은 일반적으로 배치 내 데이터에서 무작위로 샘플링되며 앵커에 대한 연관성을 나타내지 않는다.\n' +
      '\n' +
      '검색 모델을 "세밀한" 검색 시나리오에 맞게 조정하기 위해 100K 비디오의 하위 집합을 수집하며, 인간 주석자는 비디오의 주요 내용에 대한 가장 정확하고 상세한 정보를 포함하는 캡션을 선택한다. 그런 다음 이 데이터 세트에서 마스크 없는 교사 [39] (UMT)를 미세 조정합니다. 우리는 대조적 손실을 위해 하드 네거티브 마이닝[16, 35]을 구현하는데, 여기서 주석자가 선택하지 않은 7개의 캡션은 하드 네거티브 샘플을 구성하고 더 큰 훈련 가중치를 할당한다. 우리는 각각 부록 C.1과 C.2에서 UMT의 데이터 세트 수집 및 미세 조정에 대한 세부 사항을 설명한다.\n' +
      '\n' +
      '검증집합에서 미세조정이 있는 UMT와 없는 UMT의 검색 성능을 정량적으로 평가한다. 실험 결과, 세밀하게 조정된 UMT는 사전 훈련된 UMT에 비해 R@1의 정확도(35.90\\%\\)를 크게 향상시킬 수 있음을 알 수 있었다. 특히, 두 명의 다른 사람에게 주석을 재수행하도록 요청하고 그 결과를 원래의 주석과 비교함으로써 인간 일치도 평가를 수행하였다. 인간의 평균 일치도 점수는 \\(44.9\\%\\) R@1에 불과하여 두 개 이상의 캡션이 동등하게 좋은 경우 태스크가 주관적이라는 것을 보여준다. 또는, 세 사람 중 어느 한 사람이 선택한 캡션을 좋은 캡션(_i.e_, 비디오가 여러 개의 좋은 캡션을 가질 수 있음)으로 간주하면 UMT는 \\(78.9\\%\\) R@1을 달성한다. 또한, 그림 3에서, 미세 조정된 UMT(녹색 막대)가 사람이 선택한 캡션(파란색 막대)과 유사하게 분포된 캡션을 선택할 수 있음을 보여준다. 전체 데이터 세트에서 미세 조정된 UMT를 실행하여 부록 C.3에서 정교화된 주석으로 최상의 캡션을 선택한다.\n' +
      '\n' +
      '### 멀티모달 학생 캡션 모델\n' +
      '\n' +
      '전술한 캡션 파이프라인은 유망한 캡션을 생성할 수 있지만, 많은 계산 요구는 데이터세트를 훨씬 더 큰 스케일로 확장하는 능력을 방해한다. 실제로, 하나의 비디오 클립에 주석을 달기 위해서는 \\(8+1\\)의 다른 모델을 실행해야 한다. 이 문제를 해결하기 위해 팬더-70M에 대한 학생 캡션 모델을 학습하여 여러 교사 모델로부터 지식을 증류한다.\n' +
      '\n' +
      '그림 4와 같이 학생 모델은 시각적 및 텍스트 분기를 포함하여 멀티모달 입력을 활용한다. 비전 브랜치는 Video-LLaMA[88]와 동일한 아키텍처를 사용하여 LLM 호환 비디오 표현을 추출한다. 텍스트 분기의 경우, 간단한 설계는 LLM에 텍스트 임베딩을 직접 입력하는 것이다. 그러나, 이것은 두 가지 문제로 이어질 것이다: 첫째, 비디오 기술 및 자막을 갖는 텍스트 프롬프트가 너무 길어 LLM의 결정을 지배하고 무거운 계산을 부담시킬 수 있다; 둘째, 기술 및 자막의 정보는 종종 시끄럽고 비디오의 콘텐츠와 정렬하기 위해 필요하지 않다. 이를 해결하기 위해 텍스트 Q-former를 추가하여 고정된 길이의 텍스트 표현을 추출하고 비디오와 텍스트 표현을 더 잘 연결한다. Q-former는 BLIP-2의 Query Transformer와 동일한 아키텍처를 갖는다[37]. 트레이닝 동안 텍스트 분기로부터 비전 분기로의 그래디언트 전파를 차단하고 비디오 입력만을 기반으로 비주얼 인코더를 트레이닝한다. 학생 모델의 아키텍처 및 훈련에 대한 자세한 내용은 부록 D에 나와 있습니다.\n' +
      '\n' +
      '도 4: **학생 캡션 모델의 아키텍처.**\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '부록 E의 판다-70M 샘플을 시각화한다. 판다-70M의 효과를 정량적으로 평가하기 위해 섹션 4.1의 비디오 캡션, 섹션 4.2의 비디오 및 텍스트 검색, 섹션 4.3의 비디오 생성의 세 가지 다운스트림 응용 프로그램에 대한 사전 훈련 성능을 테스트한다. 다운스트림 모델의 훈련 세부 사항은 명시적으로 명시되지 않는 한 공식 코드베이스를 준수한다.\n' +
      '\n' +
      '### Video Captioning\n' +
      '\n' +
      '**실험 설정.** 비디오 캡셔닝의 성능을 평가하기 위해, 비전 분기만을 베이스 모델로 하는 Video-LLaMA[88]를 사용한다. 우리는 2.5M 비디오-텍스트 쌍과 595K 이미지-텍스트 쌍에 대해 공동으로 훈련하는 공식 가중치[43]와 처음부터 팬더-2M에 대해 훈련하는 가중치 두 가지 사전 훈련 가중치를 비교한다. 판다-2M은 판다-70M의 무작위로 샘플링된 하위 집합이며 공식 가중치와 동일한 양의 훈련 샘플을 공유한다. 또한 더 나은 캡션 성능을 위해 완전한 판다-70M에서 비디오 및 텍스트 분기로 학생 모델을 교육합니다. 모든 모델에 대해 동일한 백본을 사용하여 빅쿠나-7B[18]를 대언어 모델로 사용하고 ViT[22] 및 Q-전[37]을 비디오 인코더로 사용하고 MiniGPT-4[94]의 선형 투영 레이어를 사용한다. 판다-2M 사전 훈련의 경우, 우리는 공정한 비교를 위해 다른 텍스트 정보를 사용하지 않고 비디오와 캡션 데이터만 사용한다. 학생 모델의 경우 동영상 외에 학습 시 메타데이터와 자막도 랜덤하게 모델에 입력한다.\n' +
      '\n' +
      '**다운스트림 데이터세트 및 평가 메트릭.** 우리는 MSR-VTT[79] 및 MSVD[13]의 두 벤치마크에서 제로샷 비디오 캡셔닝을 테스트한다. MSR-VTT에는 각 비디오에 대해 수동으로 주석이 달린 20개의 캡션이 있는 10K 비디오가 포함되어 있으며 2,990개의 테스트 분할에 대한 결과를 보고한다. MSVD는 총 80K 기술의 1,970개의 비디오로 구성되어 있으며, 670개의 테스트 비디오에 대한 번호를 보고한다. 다운스트림 데이터 세트의 훈련 또는 검증 비디오는 사용하지 않습니다. 출력 캡션의 품질을 정량적으로 평가하기 위해 공통 프로토콜[41, 48, 78]을 따르고 BLEU-4[54], ROGUE-L[40], METEOR[5], CIDEr[69]를 보고한다. 모든 메트릭은 요코발캡[42] 패키지를 사용하여 계산됩니다. 또한 BERTScore [91]을 계산하여 각 토큰에 대한 상황적 유사도와 예측된 자막을 평가한다. 결과는 표 3에 보고되었으며, 공정한 비교를 위해 다운스트림 데이터 세트에 대한 추론 동안 학생 모델에 추가 텍스트 정보를 입력하지 않는다. 그림 5에서는 또한 판다-70M의 테스트 세트에서 비디오 샘플과 정성적 비교를 위한 예측된 캡션을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Model & Preference Ratio\\(\\uparrow\\) \\\\ \\hline Video-LLaMA [88] (pretrain) & 9.4 \\\\ Video-LLaMA [88] (finetune) & 7.0 \\\\ VideoChat [38] & 7.7 \\\\ VideoChat Text [38] & 3.3 \\\\ BLIP-2 [37] (opt2.7b) & 10.7 \\\\ BLIP-2 [37] (opt6.7b) & 9.0 \\\\ BLIP-2 [37] (Hant5xI) & 9.9 \\\\ MiniGPT-4 [94] & 3.1 \\\\ \\hline Student (video input) (Ours) & 18.4 \\\\ Student (video+text inputs) (Ours) & 21.4 \\\\ \\hline All Teachers (Ours) & **23.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **교사(들) 및 학생 캡션 모델(%).** 단일 교사, 모든 교사 및 두 학생 모델(텍스트가 있거나 없는)을 비교하기 위해 사용자 연구를 수행한다.\n' +
      '\n' +
      '도 5: **비디오 캡셔닝의 정성적 비교.** 판다-70M의 테스트 세트에서 샘플을 시각화하고 그 주석(최상위)을 보여준다. 또한 공식 가중치를 갖는 Video-LLaMA[88]와 비디오 전용 또는 비디오 및 텍스트 입력을 갖는 학생 모델을 포함한 세 가지 모델에서 예측된 캡션을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Pretraining Data} & \\multicolumn{6}{c}{MSR-VTT} & \\multicolumn{6}{c}{MSVD} \\\\ \\cline{3-11}  & & B4\\(\\uparrow\\) & R\\(\\uparrow\\) & M\\(\\uparrow\\) & C\\(\\uparrow\\) & BERT\\(\\uparrow\\) & B4\\(\\uparrow\\) & R\\(\\uparrow\\) & M\\(\\uparrow\\) & C\\(\\uparrow\\) & BERT\\(\\uparrow\\) \\\\ \\hline Video-LLaMA [88] & 2.5M vid + 595K img & 5.8 & 30.0 & 15.9 & 14.3 & 84.5 & 12.7 & 43.0 & 23.6 & 38.5 & 87.3 \\\\ Video-LLaMA [88] & Panda-2M (Ours) & 23.5 & 48.6 & 26.7 & 29.1 & 87.2 & 31.2 & 59.9 & 34.7 & 47.0 & 89.8 \\\\ \\hline\n' +
      '**Student (Ours)** & **Panda-70M (Ours)** & **25.4** & **50.1** & **27.7** & **31.5** & **87.9** & **32.8** & **61.2** & **35.3** & **49.2** & **90.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Zero-shot video captioning (%).** Video-LLaMA [88]과 공식 가중치(2.5M video 및 595K image에 사전 훈련) 및 우리의 판다-2M 사전 훈련 가중치를 비교한다. 우리는 또한 완전한 판다-70M 데이터 세트에 대해 훈련된 학생 모델(비전 분기만 있음)을 테스트한다. BLEU-4 (B-4) [54], ROUG-L (R) [40], METEOR (M) [5], CIDEr (C) [69] 및 BERTScore (BERT) [91]을 두 벤치마크 MSR-VTT [79] 및 MSVD [13]에서 보고한다.\n' +
      '\n' +
      '표 3에서와 같이, 판다-2M 사전 훈련 가중치를 갖는 Video-LLaMA는 공식 가중치에 비해 상당히 우수한 성능을 달성한다. 수치적으로, 사전 훈련 가중치는 B-4 측면에서 MSR-VTT와 MSVD에서 각각 \\(17.7\\%\\)과 \\(18.5\\%\\)의 향상을 얻을 수 있으며, 그림 5에서 원본 비디오-LLaMA의 캡션에는 날짜와 위치와 같은 관련 없는 일반적인 정보가 포함되어 있음을 알 수 있다. 이에 비해, 우리의 예측은 비디오 콘텐츠와 더 잘 일치합니다.\n' +
      '\n' +
      '** 학생이 교사보다 더 잘 수행할 수 있습니까?** 3.4절에서 우리는 지식 증류 방식으로 학생 모델을 학습한다. 학생 모델의 성능을 평가하기 위해 각 비디오에 대해 10개의 후보에서 가장 좋은 캡션을 선택하도록 참가자에게 요청하는 사용자 연구를 수행한다. 8개의 교사 모델과 2개의 학생 모델(텍스트 입력 유무)로부터 10개의 캡션을 예측한다. 우리는 개인의 주관적인 편견을 줄이기 위해 5명의 참가자로부터 결과를 수집한다. 각 참가자는 테스트 세트에서 무작위로 샘플링된 동일한 200개의 비디오를 보고 학생 모델과 UMT를 훈련하는 동안 볼 수 없었다. 표 4에서 각 모델의 선호도 비율과 finetuned UMT(_i.e_., 모든 교사)의 R@1 정확도를 보고한다. 우리는 학생 모델이 모든 개별 교사 모델보다 우수하고 모든 교사 모델과 유사한 성능을 달성함을 관찰할 수 있다.\n' +
      '\n' +
      '**멀티모달 입력이 비디오 캡셔닝을 활용할 수 있습니까?** 우리 학생 모델은 비디오 및 텍스트 입력을 모두 지원합니다. 표 4에서 동영상과 텍스트 입력이 모두 있는 학생 모델이 동영상 입력이 있는 모델보다 \\(3.0\\%\\) 선호 비율만 더 우수하다는 것을 보여준다. 질적으로 그림 5에서 텍스트 입력이 있거나 없는 예측을 보여준다. 순수 비디오 입력이 있는 예측은 "선인장"과 같은 비디오의 부분 콘텐츠를 포함할 수 있지만 비디오와 텍스트 입력이 있는 모델은 비디오 제목, 설명 및 자막에서 "석핵종" 및 "다른 종"과 같은 키워드를 더 포괄적으로 포함할 수 있다.\n' +
      '\n' +
      '### 비디오 및 텍스트 검색\n' +
      '\n' +
      '**실험 설정.** 우리는 비디오 및 텍스트 검색에 대한 성능을 평가하기 위해 마스킹되지 않은 교사[39]를 기본 모델로 사용한다. 표준 프로토콜 [17, 24, 34, 39, 78]은 CC3M [64]의 3M 이미지와 2.5M 비디오를 사전 훈련 데이터 세트로 공동으로 사용한다. 따라서 공정한 비교를 위해 표준 사전 훈련 데이터 세트와 동일한 수의 훈련 샘플을 공유하는 판다-5M 하위 집합을 무작위로 샘플링한다. 두 데이터 세트에 대해 ViT-L/16 [21]과 BERTlarge [20]으로 구성된 동일한 백본을 사용한다. 표준 데이터 세트 사전 훈련에 공식 가중치를 사용하고 팬더-5M에 대해 처음부터 모델을 훈련한다.\n' +
      '\n' +
      '**다운스트림 데이터 세트 및 평가 메트릭.** 우리는 MSR-VTT[79], DiDeMo[3] 및 MSVD[13]의 세 가지 벤치마크에서 제로 샷 및 피네튠 검색을 모두 테스트한다. MSR-VTT의 경우, 1K 테스팅 분할에 대해 평가하기 위해 공통 프로토콜 [34, 85]를 따르며, 이는 섹션 4.1의 캡션을 위한 테스팅 비디오와 동일하지 않다. DiDeMo [3]의 경우, 총 40K 밀집 캡션을 갖는 10K 플리커 비디오를 포함한다. 이전 표준 [24, 33, 44]에서와 같이, 우리는 한 비디오의 모든 문장 설명을 하나의 쿼리로 연결하여 단락 간 검색을 평가한다. 우리는 1K 테스트 세트에 대한 결과를 보고한다. MSVD[13]의 경우 670개의 테스트 비디오에 대한 결과를 보고한다. 표 5에서 텍스트 대 비디오 및 비디오 대 텍스트 검색 모두에 표준 메트릭을 사용하고 R@1, R@5 및 R@10 정확도를 보고한다.\n' +
      '\n' +
      '우리는 팬더-5M을 사용한 사전 훈련이 제로 샷 및 세절 검색 설정 모두에서 공식 무게보다 우수하다는 것을 관찰할 수 있다. 특히, MSR-VTT[79], DiDeMo[3], MSVD[13]에서 제로샷 텍스트-비디오 검색의 R@1 측면에서 사전학습은 \\(7.0\\%\\), \\(0.6\\%\\), \\(4.9\\%\\)의 리프트를 각각 얻었다. 또한, 팬더-5M으로 UMT[39]를 사전 훈련하는 것이 기존의 최첨단 방법[49, 50, 74]보다 훨씬 많은 시각 텍스트 데이터 쌍(_i.e_., \\(>\\)100M)으로 사전 훈련하는 것보다 우수하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Pretraining Data} & \\multicolumn{4}{c}{MSR-VTT} & \\multicolumn{4}{c}{DiDeMo} & \\multicolumn{4}{c}{MSVD} \\\\ \\cline{3-13}  & & R@1\\(\\uparrow\\) & R@5\\(\\uparrow\\) & R@10\\(\\uparrow\\) & R@1\\(\\uparrow\\) & R@5\\(\\uparrow\\) & R@10\\(\\uparrow\\) & R@1\\(\\uparrow\\) & R@5\\(\\uparrow\\) & R@10\\(\\uparrow\\) \\\\ \\hline \\hline \\multicolumn{13}{l}{_Zero-shot T2V / V2T Retrieval_} \\\\ AlignPrompt [34] & 2.5M vid + 3M img & 24.1 / - & 44.7 / - & 55.4 / - & 23.8 / - & 47.3 / - & 57.9 / - & - / - & - / - & - / - & - / - & - / - \\\\ BridgeFormer [24] & 2.5M vid + 3M img & 26.0 / - & 46.4 / - & 56.4 / - & 25.6 / - & 50.6 / - & 61.6 / - & 43.6 / - & 74.9 / - & 84.9 / - \\\\ \\hline UMT [39] & 2.5M vid + 3M img & 30.2 / 33.3 & 51.3 / 58.1 & 61.6 / 66.7 & 33.6 / 32.1 & 58.1 / 57.3 & 65.5 / **66.7** & 66.3 / **44.4** & 85.5 / **73.3** & 89.3 / **82.4** \\\\ UMT [39] & **Panda-5M (Ours)** & **37.2 / 36.3** & **58.1 / 61.0** & **69.5 / 69.7** & **34.2 / 33.4** & **58.4 / 57.9** & **66.5** / 65.8 & **71.2 / 37.2** & **88.4** / 65.1** & **92.7** / 75.6 \\\\ \\hline \\hline \\multicolumn{13}{l}{_Finetune T2V / V2T Retrieval_} \\\\ CLIP4Clip [49] & 400M img & 44.5 / 40.6 & 71.4 / 69.5 & 81.6 / 79.5 & 43.4 / 42.5 & 70.2 / 70.6 & 80.6 / 80.2 & 46.2 / 62.0 & 76.1 / 87.3 & 84.6 / 92.6 \\\\ X-CLIP [50] & 400M img & 49.3 / 48.9 & 75.8 / 76.8 & 84.8 / 84.5 & 50.4 / **66.8** & 80.6 / **90.4** & - / - & 47.8 / 47.8 & 79.3 / 76.8 & - / - \\\\ InternVideo [74] & 146M vid + 100M img & 55.2 / 57.9 & / - & - / - & - & 57.9 / 59.1 & / - & - & - / - & **58.4** / 76.3 & - / - & - / - \\\\ \\hline UMT [39] & 2.5M vid + 3M img & 53.3 / 51.4 & 76.6 / 76.3 & 83.9 / 82.8 & 59.7 / 59.5 & 84.9 / 84.5 & 90.8 / **90.7** & 53.7 / 77.2 & 80.5 / 91.6 & 86.8 / 94.8 \\\\ UMT [39] & **Panda-5M (Ours)** & **58.4 / 58.5** & **80.9 / 81.0** & **86.9 / 87.0** & **60.6 / 58.9** & **86.0 / 84.6** & **92.4 / 90.4 & 57.5 / **81.3** & **83.6** / **93.7** & **89.5** / **96.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **Video and text retrieval (%).** Unmasked Teacher [39]와 공식 체크포인트(2.5M video and 3M image에 사전 훈련) 및 우리의 판다-5M 사전 훈련을 비교한다. 본 논문에서는 T2V(zero-shot and finetune text-to-video) 검색과 V2T(video-to-text) 검색에 대한 성능을 평가한다. 우리는 MSR-VTT[79], DiDeMo[3] 및 MSVD[13]의 세 가지 벤치마크에서 R@1, R@5 및 R@10 정확도를 보고한다.\n' +
      '\n' +
      '### Text-to-Video Generation\n' +
      '\n' +
      '**실험 설정.** 텍스트-비디오 생성의 효과를 평가하기 위해 AnimateDiff[26]을 기본 모델로 사용하고 2.5M 텍스트-비디오 쌍에 대해 훈련된 공식 릴리스 가중치와 판다-70M의 2.5M 하위 집합인 판다-2M에 대해 훈련된 가중치의 두 가지 가중치를 비교한다. 우리는 공식 코드베이스를 따르고 T2I 생성기로 안정적인 확산 v1.5 [59] (SD)를 사용한다. 훈련하는 동안 T2I 모듈을 고정하고 모션 모델링 모듈만 훈련한다. 각 학습 비디오에 대해 4의 보폭으로 16개의 프레임을 샘플링한 다음 크기 및 중심 크롭을 \\(256\\times 256\\)px 해상도로 조정한다.\n' +
      '\n' +
      '**다운스트림 데이터 세트 및 평가 메트릭.** 모델을 평가하기 위해 UCF101[66] 및 MSR-VTT[79]에 대한 제로 샷 평가를 위한 평가 프로토콜[8, 23, 65, 72]을 따른다. 구체적으로, 16-프레임 비디오를 \\(256\\times 256\\)px 해상도로 생성한다. UCF101[66]의 경우, 각 클래스[23]에 대한 텍스트 프롬프트를 생성하고 원본 데이터 세트[8, 72]와 동일한 클래스 분포를 공유하는 10,000개의 비디오를 생성한다. 우리는 I3D 임베딩에서 FVD(Frechet Video Distance)[68]을 계산한다[84]. MSR-VTT[79]의 경우, 59,800개의 테스트 프롬프트[23, 65] 각각에 대해 비디오 샘플을 생성하고 CLIPsim(CLIPsim)[76]을 계산한다. 표 6의 수치를 보고하고 그림 6의 생성된 비디오 샘플도 보여준다. 결과를 시각화하기 위해 공식 코드베이스를 따르고 SD T2I를 개인화된 드림부스 가중치[60], TUSUN3로 대체한다. 테스트 프롬프트와 공식 가중치를 갖는 AnimateDiff의 비디오 샘플(그림 6의 상단 행)은 AnimateDiff의 프로젝트 페이지에서 직접 가져온다는 점에 유의한다.\n' +
      '\n' +
      '각주 3: [https://civitai.com/models/33194/pallass-catmanual-lora](https://civitai.com/models/33194/pallass-catmanual-lora)\n' +
      '\n' +
      '판다-2M 사전 훈련은 공식 가중치에 비해 두 측정 지표 모두에서 일관되게 우수한 성능을 보여준다. 강조된 바와 같이, 우리의 사전 훈련은 UCF101에서 \\(77.4\\) 더 낮은 FVD를 산출하고 FVD 측면에서 10M 규모 내에서 데이터 세트에서 사전 훈련된 최신 모델보다 우수하다. 질적으로, 우리의 사전 훈련 가중치는 더 의미 있는 움직임과 사실적인 외관으로 비디오를 생성할 수 있고 워터마크를 포함하지 않는다.\n' +
      '\n' +
      '##5 결론 및 한계\n' +
      '\n' +
      '본 논문에서는 자막 주석이 있는 대규모 비디오 데이터세트인 판다-70M을 소개한다. 데이터세트는 고해상도 및 의미적으로 일관된 비디오 샘플을 포함한다. 캡션 70M 비디오를 위해 비디오 설명, 자막 및 개별 정적 비디오 프레임과 같은 멀티모달 정보를 활용할 수 있는 자동 파이프라인을 제안한다. 우리는 판다-70M을 이용한 사전 훈련이 비디오 캡션, 비디오 및 텍스트 검색, 텍스트 대 비디오 생성의 세 가지 다운스트림 작업을 용이하게 할 수 있음을 보여준다.\n' +
      '\n' +
      '인상적인 결과를 보여주었음에도 불구하고 제안된 데이터 세트는 여전히 몇 가지 제한에 묶여 있다. 먼저, 대부분의 샘플이 음성 집약적인 비디오인 HD-VILA-100M[80]에서 비디오를 수집한다. 따라서 데이터 세트의 주요 범주는 뉴스, 텔레비전 쇼, 다큐멘터리 영화, 자기 중심적 비디오, 교육 및 내러티브 비디오입니다. 어노테이션 파이프라인에는 비디오 자막이 필요하지 않기 때문에 이 작업의 중요한 확장으로 더 많은 언보컬 비디오 컬렉션을 나열한다.\n' +
      '\n' +
      '둘째, 비디오 샘플이 의미적으로 일치하여 캡션이 모호성 없이 정확한 의미 내용을 표현할 수 있는 세밀한 데이터세트에 초점을 맞춘다. 그럼에도 불구하고, 그것은 단일 비디오 내의 콘텐츠 다양성을 제한하고 또한 평균 비디오 지속 시간을 감소시킬 것이며, 이는 긴 비디오 생성[9] 및 밀집 비디오 캡션[71, 81]과 같은 다운스트림 작업에 황급할 수 있다. 긴 비디오 및 고밀도 캡션을 사용하여 데이터 세트를 구축하는 미래의 노력은 이러한 다운스트림 응용 프로그램에 도움이 될 수 있다.\n' +
      '\n' +
      '**위험 완화** 데이터셋을 공개하기 전에 내부 자동 파이프라인을 사용하여 유해하거나 폭력적인 언어와 약물 또는 혐오 발언이 포함된 텍스트를 가진 비디오 샘플을 필터링했다. 또한 NLTK 프레임워크를 사용하여 모든 사람의 이름을 "사람"으로 대체합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{(\\#) P-T Videos} & UCF101 & MSR-VTT \\\\ \\cline{3-4}  & & FVD\\(\\downarrow\\) & CLIPSim\\(\\uparrow\\) \\\\ \\hline CogVideo [30] & 5M & 701.6 & - \\\\ MagicVideo [92] & 10M & 699.0 & - \\\\ LVDM [28] & 18K & 641.8 & 0.2751 \\\\ ModelScope [70] & 10M & 639.9 & **0.3000** \\\\ VideoLDM [8] & 10M & 550.6 & - \\\\ \\hline AnimateDiff [26] & 2.5M & 499.3 & 0.2869 \\\\ AnimateDiff [26] & **Panda2M (Ours)** & **421.9** & 0.2880 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **제로-샷 텍스트-투-비디오 생성.** AnimateDiff[26]의 제로-샷 텍스트-투-비디오 생성을 공식 가중치(2.5M 비디오에 사전 훈련)와 우리의 판다-2M 사전 훈련과 비교한다. 우리는 UCF101[66]에 대한 FVD[68] 및 MSR-VTT[79]에 대한 CLIP 유사성(CLIPSim)[76]을 보고한다. 10M 미만의 비디오로 훈련된 모델과만 비교합니다.\n' +
      '\n' +
      '도 6: **텍스트-비디오 생성의 정성적 결과.** AnimateDiff[26]에 의해 생성된 비디오를 공식 가중치(상부) 및 우리의 판다-2M 사전 훈련(하부)으로 시각화한다. 테스트 프롬프트와 원본 AnimateDiff(상단)의 비디오 샘플은 AnimateDiff의 프로젝트 웹사이트에서 직접 가져온 것입니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]P. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. 하손기 Lenc, A. Mensch, K 밀리컨, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning. 뉴립스 인용: SS1.\n' +
      '*[2]L. A. 헨드릭스, O. 왕엔쉬트먼 J시빅 T Darrell, and B. Russell(2017) 자연어로 비디오에서 순간을 현지화합니다. ICCV에서 인용: SS1.\n' +
      '*[3]Y. 발라지 아니, X 황아바닷 J.송 크레이스 아티탈라 아일라 Laine, B. Catanzaro, et al.(2022) ediff-i: text-to-image diffusion models with ensemble of expert denoisers. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[4]S. Banerjee and A. Lavie (2005) Meteor: 인간의 판단과 향상된 상관 관계를 갖는 mt 평가를 위한 자동 메트릭. ACL 워크샵에서 인용된 SS1입니다.\n' +
      '*[5]H. 바오락 동성호 Piao, and F. Wei(2022) BEIT: BERT pre-training of image transformer. ICLR에서 인용: SS1.\n' +
      '*[6]A. 블랫만 도크혼 컬랄, 디 멘델리비치, 엠 킬리안, D. 로렌츠, Y. 리바이, 지 영어, V Voleti, A. Letts, et al.(2023) Stable video diffusion: latent video diffusion models to large datasets. ArXiv:2311.15127. 인용: SS1.\n' +
      '*[7]A. R. 블랫만 롬바흐 H. 링, T. 도크혼, 김승원 피들러와 K Kreis(2023) 잠복기를 정렬: 잠재 확산 모델과 고해상도 비디오 합성 CVPR에서 인용됨: SS1.\n' +
      '*[8]T. 브룩스, J. 헬스텐 아티탈라 왕태 아일라, J. 레티넨, M. 류, A. 에프로스, T. 카라스(2022) 동적 장면들의 긴 비디오들을 생성하는 단계. 뉴립스 인용: SS1.\n' +
      '*[9]M. 변병현 이원 백승훈 Kim(2022) Copyo-700m: 이미지-텍스트 쌍 데이터세트. 참고: [https://github.com/kakaobrain/copyo-dataset](https://github.com/kakaobrain/copyo-dataset) 인용: SS1.\n' +
      '*[10]F. 카바 하일브론 Escorcia, B. Ghanem, and J. Carlos Niebles (2015) Activitynet: 인간 활동 이해를 위한 대규모 비디오 벤치마크. CVPR에서 인용됨: SS1.\n' +
      '*[11]S. 창피노, P. 샤르마, N. Ding and R. Soricut(2021) Conceptual 12M: 롱테일 비주얼 컨셉을 인식하기 위해 웹 스케일 이미지-텍스트 사전 트레이닝을 푸시한다. CVPR에서 인용됨: SS1.\n' +
      '*[12]D. Chen과 W. B. Dolan (2011)은 패러프레이즈 평가를 위해 고도로 병렬적인 데이터를 수집한다. ACL에서 인용됨: SS1.\n' +
      '*[13]J. 천동주 셴진 이종욱 류필장 크리시나모오르티 찬드라 시온과 M Elhoseiny(2020) Minigpt-v2: vision-language multi-task 학습을 위한 통일된 인터페이스로서 큰 언어 모델. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[14]T. 천성호 콘블리쓰 Norouzi, and G. Hinton (2020) 시각적 표상의 대조적 학습을 위한 간단한 프레임워크이다. ICML에서 인용됨: SS1.\n' +
      '*[15]T. 천원 흥흥창 치엔과 M. 양(2022) 대조적 학습을 위한 점진적 오검출. ICLR에서 인용: SS1.\n' +
      '*[16]F. 쳉, X 왕진래 D. 크랜달 Bansal, and G. Bertasius (2023) Vindlu: 효과적인 비디오 및 언어 사전 훈련을 위한 레시피. CVPR에서 인용됨: SS1.\n' +
      '*[17]W. 장종 이종욱 임영식 성진 우현장 정승 장영 장재은곤잘레스 Stoica, and E. P. Y. Xing (2023) Vicuna: 90%* chatgrp 품질을 가진 오픈 소스 챗봇 인상 gpt-4. 인용: SS1.\n' +
      '*[18]H. W. Chung, L. 허승 롱프레, B. 조프, Y 테이원 페더스 리진 왕민 데하니, S. Brahma, et al. (2022) Scaling instruction-finetuned language models. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[19]J. 데블린 장경 이경호 Toutanova (2018) Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[20]A. L. 도소비츠키 Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니 민더러, G. 헤이골드, S. Gelly, J. Uszkoreit, N. Houlsby (2021) 이미지는 16x16 단어의 가치가 있다: 스케일에서 이미지 인식을 위한 트랜스포머. ICLR에서 인용: SS1.\n' +
      '*[21]Y. 팽원 왕병수 선락 우익 왕태 황철 왕, Y. Cao(2023) Eva: 스케일에서 마스킹된 시각적 표현 학습의 한계를 탐구한다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19358-19369. Cited by: SS1.\n' +
      '*[22]S. 지승 나규류 풍, A. Tao, B. Catanzaro, D. Jacobs, J. Huang, M. 류영 Balaji(2023)는 비디오 확산 모델에 대한 잡음 이전의 상관 관계를 보존한다. ICCV에서 인용: SS1.\n' +
      '*[23]Y. 지영 Ge, X 류동열 Shan, A. Qie, and P. Luo(2022) Bridging video-text retrieval with multiple choice questions. CVPR에서 인용됨: SS1.\n' +
      '*[24]R. 거다르, A. 엘-누비, Z. 류민 싱기 바수데프 알왈라, A. 줄린, 그리고 I. 미스라(2023) 이미지 바인드: 그들을 모두 묶을 하나의 임베딩 공간. CVPR에서 인용됨: SS1.\n' +
      '*[25]Y. 곽철양 왕영 Qiao, D. Lin, and B. Dai(2023) Animatediff: 특정 튜닝 없이 개인화된 텍스트 대 이미지 확산 모델을 애니메이션화합니다. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[26]K. 그, X 천성호 시영 리 P. 달러, R. Girshick (2022) 마스킹 오토인코더는 확장 가능한 비전 학습자이다. CVPR에서 인용됨: SS1.\n' +
      '*[27]Y. 그, 토 양영 장영 샨과 Q Chen(2023) 고충실도 긴 비디오 생성을 위한 잠재 비디오 확산 모델. ArXiv 프리프린트. 인용: SS1.\n' +
      '*[28]J. 호원 찬찬사하리아 J.황 가오, A. 그리센코, D. P. 킹마, B. 풀, M. Norouzi, D. J. Fleet, et al.(2022) Imagen video: diffusion model을 갖는 고선명 video generation. ArXiv 프리프린트. 인용: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. _arXiv preprint_, 2022.\n' +
      '* [31] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* [32] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: Automatic curation of large-scale datasets for audio-visual video representation learning. In _ICCV_, 2021.\n' +
      '* [33] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In _CVPR_, 2021.\n' +
      '* [34] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In _CVPR_, 2022.\n' +
      '* [35] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _NeurIPS_, 2021.\n' +
      '* [36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstraping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.\n' +
      '* [37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint_, 2023.\n' +
      '* [38] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. _arXiv preprint_, 2023.\n' +
      '* [39] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. _ICCV_, 2023.\n' +
      '* [40] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In _ACL_, 2004.\n' +
      '* [41] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning. In _CVPR_, 2022.\n' +
      '* [42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.\n' +
      '* [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint_, 2023.\n' +
      '* [44] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. _arXiv preprint_, 2019.\n' +
      '* [45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.\n' +
      '* [46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint_, 2017.\n' +
      '* [47] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In _ICLR_, 2017.\n' +
      '* [48] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univi: A unified video and language pre-training model for multimodal understanding and generation. _arXiv preprint_, 2020.\n' +
      '* [49] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. _Neurocomputing_, 2022.\n' +
      '* [50] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. In _ACM MM_, 2022.\n' +
      '* [51] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. _arXiv preprint_, 2023.\n' +
      '* [52] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In _ICCV_, 2019.\n' +
      '* [53] OpenAI. Gpt-4 technical report. _arXiv preprint_, 2023.\n' +
      '* [54] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _ACL_, 2002.\n' +
      '* [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 2020.\n' +
      '* [57] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, 2021.\n' +
      '* [58] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In _CVPR_, 2015.\n' +
      '* [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _CVPR_, 2023.\n' +
      '* [61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS_, 2022.\n' +
      '\n' +
      '* [62] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _NeurIPS_, 2022.\n' +
      '* [63] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In _CVPR_, 2022.\n' +
      '* [64] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, 2018.\n' +
      '* [65] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In _ICLR_, 2023.\n' +
      '* [66] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* [67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint_, 2023.\n' +
      '* [68] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint_, 2018.\n' +
      '* [69] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In _CVPR_, 2015.\n' +
      '* [70] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. _arXiv preprint_, 2023.\n' +
      '* [71] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning with parallel decoding. In _ICCV_, 2021.\n' +
      '* [72] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. _arXiv preprint_, 2023.\n' +
      '* [73] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In _ICCV_, 2019.\n' +
      '* [74] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. _arXiv preprint_, 2022.\n' +
      '* [75] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023.\n' +
      '* [76] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint_, 2021.\n' +
      '* [77] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. _arXiv preprint_, 2022.\n' +
      '* [78] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: A modularized multi-modal foundation model across text, image and video. _arXiv preprint_, 2023.\n' +
      '* [79] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In _CVPR_, 2016.\n' +
      '* [80] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In _CVPR_, 2022.\n' +
      '* [81] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense video captioning. In _CVPR_, 2023.\n' +
      '* [82] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint_, 2022.\n' +
      '* [83] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _TMLR_, 2022.\n' +
      '* [84] Sihyun Yu, Jihoon Tack, Sangwowoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. _arXiv preprint arXiv:2202.10571_, 2022.\n' +
      '* [85] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In _ECCV_, 2018.\n' +
      '* [86] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint_, 2021.\n' +
      '* [87] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlo: Multimodal neural script knowledge models. _NeurIPS_, 2021.\n' +
      '* [88] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint_, 2023.\n' +
      '* [89] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.\n' +
      '* [90] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint_, 2022.\n' +
      '* [91] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. _arXiv preprint_, 2019.\n' +
      '* [92] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint_, 2022.\n' +
      '* [93] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In _AAAI_, 2018.\n' +
      '* [94] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint_, 2023.\n' +
      '\n' +
      '**판다-70M : 다중 교차모달리티 교사가 있는 캡션 70M 비디오**\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '## 부록 시맨틱을 고려한 비디오 분할 알고리즘\n' +
      '\n' +
      '섹션 3.1에서는 긴 비디오를 여러 개의 의미적으로 일관된 클립으로 절단하는 비디오 분할 알고리즘을 제안한다. 알고리즘은 분할 및 스티칭의 두 단계를 포함하며, 자세한 내용은 부록 A.1 및 A.2에 설명되어 있다.\n' +
      '\n' +
      '### Stage1: Shot Boundary Detection 기반 분할\n' +
      '\n' +
      '우리는 먼저 긴 비디오를 PySceneDetect[1]로 나눈다. 구체적으로는, \\(\\mathrm{Cutscene\\_threshold}\\)이 25인 \\(\\mathrm{ContentDetector}\\)과 \\(\\mathrm{min\\scene\\_len}\\)이 15프레임인 \\(\\mathrm{cutscene\\_threshold}\\)을 갖는 \\(\\mathrm{ContentDetector}\\)을 사용하고, 다음으로 PySceneDetect와 2에 의해 확실하게 검출될 수 없는 fade-in과 fade-out 효과와 같은 복잡한 천이들을 갖는 1)개의 긴 비디오들을 처리하기 위한 2단계 후처리 알고리즘을 설계한다. 절단 장면을 포함하지 않지만 동일한 클립 내에서 의미 변화가 있는 편집되지 않은 화면입니다.\n' +
      '\n' +
      '두 가지 경우를 모두 처리하기 위해 컷 장면 없이 클립에 대해 5초마다 인공 장면 컷을 생성하는 것을 제안한다. 즉, 비디오 클립이 5초보다 길면 처음 5초를 새로운 클립으로 잘라내고 나머지 부분에 동일한 절차를 재귀적으로 적용한다. 의미적으로 일관된 비디오 클립에만 관심이 있기 때문에, 우리는 시작 또는 끝 근처의 프레임의 이미지 바인드[25] 특징을 추출한다. 만약 이 두 프레임의 특징이 극적으로 다르다면 우리는 그 클립을 제거한다. 구체적으로, \\(n\\) 프레임 비디오 클립이 주어졌을 때, 우리는 \\(C_{A}\\)과 \\(C_{B}\\)의 숫자 \\(0.1\\times n\\)과 \\(0.9\\times n\\) 프레임에 대한 \\(C_{A}\\)과 \\(C_{B}\\)의 특징을 추출한다. 우리는 \\(\\|f(C_{A})-f(C_{B})\\|\\leq 1.0\\을 만족하는 경우에만 비디오 클립을 보관한다. 따라서, 우리는 클립 내에서 전이 효과 또는 의미론적 변화가 큰 비디오 클립을 배제할 수 있다.\n' +
      '\n' +
      '### Stage2: 의미 유사도 기반 스티칭\n' +
      '\n' +
      '첫 번째 단계는 동일한 의미적 내용을 가진 짧은 연속 클립을 많이 소개한다. 이를 위해, 우리는 동일한 의미 콘텐츠를 갖는 클립들을 병합하는 추가적인 절차를 제안한다. 형식적으로 인접한 두 개의 클립(C^{1}\\)과 \\(C^{2}\\)을 차례로 주어, \\(\\|f(C^{1}_{B})-f(C^{2}_{A})\\|\\leq 0.6\\이면 클립으로 연결한다.\n' +
      '\n' +
      '마지막으로, 다음 단계로 비디오 클립의 품질과 다양성을 안정화시키기 위한 후처리를 수행한다:\n' +
      '\n' +
      '* 먼저, 우리는 \\(2\\)초보다 짧은 클립 또는 약간의 움직임만을 포함하는 클립(_i.e._, \\(\\|f(C_{A})-f(C_{B})\\|\\leq 0.15\\)을 제외한다. 60\\(60\\)초보다 긴 비디오의 경우, 우리는 처음 \\(60\\)초만 유지한다.\n' +
      '* 다음으로, 각 클립을 스테이지1(섹션 A.1)에서 추출된 ImageBind 특징들의 평균으로 나타내고, 비디오 샘플들의 다양성을 증가시키기 위해 선행 클립들과 의미적으로 다른(_i.e._, 유클리드 거리 \\(>0.3\\)) 비디오 클립들만을 유지한다.\n' +
      '* 마지막으로 동영상의 첫 번째와 마지막 \\(10\\%\\)을 트리밍한다. 이는 동영상의 시작과 끝이 일반적으로 불안정한 카메라의 움직임이나 전환 효과를 포함한다는 것을 알 수 있다.\n' +
      '\n' +
      '제안된 분할 알고리즘을 사용하여, 평균 클립 지속시간이 8.477초인 긴 비디오(3,790,459\\)를 70,817,169\\의 클립으로 분할하였다. 우리는 그림 7에 비디오 길이의 분포를 표시한다.\n' +
      '\n' +
      '## 교사용 캡션 모델의 부록 B 세부사항: 풀, 추론 및 선택\n' +
      '\n' +
      '3.2절에서는 캡셔닝을 위해 다중 교차 모달리티 교사 모델을 사용할 것을 제안한다. 특히 31개의 캡션 모델을 포함한 큰 풀부터 시작합니다. 우리는 모델 풀의 구성과 부록 B.1과 B.2에서 각각 비디오 캡션을 위해 구현하는 방법에 대해 자세히 설명한다. 70M 비디오에 대한 모델의 추론을 실행하는 것은 계산 비용이 많이 들기 때문에, 우리는 인간 평가를 기반으로 8개의 모델만을 대표로 선택한다. 우리는 부록 B.3에서 이 과정에 대해 더 자세히 설명할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Base Model} & \\multirow{2}{*}{Type} & \\multirow{2}{*}{Weights} & \\multicolumn{4}{c}{Input Information} & \\multirow{2}{*}{\\# of Models} \\\\ \\cline{5-6}  & & & V & V-S & V-M & V-S-M \\\\ \\hline Video-LLaMA [88] & Video VQA & pretrain / finetune & ✓ & ✓ & ✓ & ✓ & 8 \\\\ VideoChat [38] & Video VQA & 7B & ✓ & ✓ & ✓ & ✓ & 4 \\\\ VideoChat Text [38] & NLP-based Video VQA & - & ✓ & ✓ & ✓ & ✓ & 4 \\\\ Video-ChatGPT [51] & Video VQA & - & ✓ & ✓ & ✓ & ✓ & 4 \\\\ BLIP-2 [37] & Image Captioning & opt2.7b / opt6.7b / fant5xl & ✓ & ✗ & ✗ & ✗ & 3 \\\\ MiniGPT-4 [94] & Image VQA & 7B / 13B & ✓ & ✓ & ✓ & ✓ & 8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **31 교사 모델의 개요.**31 교사 모델은 다양한 가중치 및 입력 정보를 갖는 6개의 기본 모델로 구성된다. 입력 데이터는 비전(V), 자막(S), 메타데이터(M)를 포함한다. 비전 데이터는 기본 모델의 유형에 따라 비디오 또는 정적 비디오 프레임입니다. 메타데이터는 비디오의 제목과 설명을 포함한다. 예를 들어, MiniGPT-4를 위한 V-S-M은 비디오 프레임, 자막 및 메타데이터의 입력으로 MiniGPT-4를 의미한다.\n' +
      '\n' +
      '도 7: **Panda-70M의 비디오 듀레이션의 분포.**\n' +
      '\n' +
      '### 31개의 캡션 모델 풀 도입\n' +
      '\n' +
      '교차 모달리티 교사 모델을 활용하는 주요 이유는 비디오 캡셔닝에 도움이 되는 멀티모달 데이터를 활용하기 위함이다. 따라서 본 논문에서는 영상/영상 시각 질의 응답(VQA)과 영상 자막 모델을 포함한 기본 모델을 고려한다. 구체적으로 Video-LLaMA[88], VideoChat[38], VideoChat Text[38], Video-ChatGPT[51], BLIP-2[37], MiniGPT-4[94]를 기본 모델로 사용한다. 이 모델을 기반으로 다양한 가중치 및 입력 정보를 사용하여 총 31개의 캡션 모델을 수집한다. 표 7에 모든 캡션 모델의 요약을 나열한다.\n' +
      '\n' +
      '비디오 캡션을 위한 교차모달리티 교사 모델의### 추론\n' +
      '\n' +
      '우리는 다음과 같이 각 기본 모델의 추론 세부 사항을 나열한다:\n' +
      '\n' +
      '***Video-LLaMA**[88]은 비디오 VQA 모델이다. 비전 브랜치만 사용하고 오디오는 사용하지 않습니다. 이 모델은 VQA를 구현하기 위해 LLM으로 Vicuna-7B[18]을 사용한다. 우리는 2.5M 비디오-텍스트 쌍과 LLaVA-CC3M [43]에 대해 훈련된 사전 훈련 가중치와 [38, 43, 94]의 명령-동조 데이터에 대해 추가로 훈련된 피네튜닝 가중치를 포함한 두 가지 공식 가중치를 사용한다.\n' +
      '***VideoChat**[38]** 및 Video-ChatGPT**[51]은 비디오 VQA 모델이다. 우리는 LLM으로 Vicuna-7B를 사용하고 나머지 구성에는 공식 코드베이스를 따른다.\n' +
      '***VideoChat Text**[38]은 자연어 처리(NLP) 기반의 비디오 VQA 모델이다. 모델은 비디오 콘텐츠를 3개의 모델[45, 56, 77]에 의해 각각 비디오 태그, 밀집 캡션 및 일반 캡션으로 텍스트화할 것이다. 이와 같이, 사용자들은 챗봇과 대화를 하고, 추출된 텍스트 콘텐츠에 기초하여 영상에 대해 토론할 수 있다. 원래의 코드베이스는 챗봇으로 ChatGPT-4[53]을 사용하지만 대중에게 자유롭게 공개되지 않는다. 따라서 대규모 캡션을 위해 LLaMA[67]로 교체한다.\n' +
      '****BLIP-2**[37]은 언어-이미지 사전 훈련 모델이다. 이미지 캡션 용도로만 사용하고 텍스트 입력은 하지 않습니다. 우리는 OPT[90](opt2.7b 및 opt6.7b) 및 FlanT5[19](flanT5xl)를 포함하여 다른 LLM을 사용하여 사전 훈련을 사용한다.\n' +
      '***MiniGPT-4**[94]는 이미지 VQA 모델이다. 우리는 LLM으로 Vicuna-7B 및 Vicuna-13B를 사용하여 각각 두 가지 변형을 사용합니다.\n' +
      '\n' +
      '비디오 캡셔닝을 위한 교차 모달리티 교사 모델을 구현하기 위해, 우리는 다양한 모달리티 모델에 대한 알고리즘을 특별히 설계한다. 영상모델은 n\\(n\\) 프레임 비디오 클립을 입력으로 하여 숫자\\(0.3\\times N\\)과 숫자\\(0.7\\times N\\) 사이의 비디오 프레임을 랜덤하게 샘플링한다. VQA 모델의 경우 시각적 데이터 외에도 비디오 제목, 설명 및 자막과 같은 추가 텍스트 정보를 포함할 수 있는 텍스트 프롬프트를 입력하여 비디오 캡션을 지원합니다. 구체적으로, 우리는 캡션을 위해 메타데이터 또는 자막 중 하나 또는 둘 모두의 정보를 포함하고자 하는 경우 그림 8의 프롬프트 템플릿을 사용한다. 이에 반해, 우리는 더미 프롬프트를 사용한다: "_Please faithfully summarized the video(또는 image) in one sentence._" 캡션을 위한 비전 데이터만 입력하면 됩니다.\n' +
      '\n' +
      '인간평가를 기반으로 한 8개의 자막모델 선정\n' +
      '\n' +
      '70M 비디오에서 31개의 캡션 모델을 실행하려면 상당한 연산 자원이 필요하다. 따라서 본 논문에서는 인간 평가 및 모델 선택 알고리즘을 포함한 2단계 알고리즘을 사용하여 모델의 성능이 좋은 부분 집합을 찾는 것을 제안한다.\n' +
      '\n' +
      '**인간 평가.** 먼저, 각 모델의 출력 캡션을 인간에게 보여줌으로써 사용자 연구를 진행한다. 구체적으로, 1K 비디오 클립을 무작위로 샘플링하고 각 비디오에 대해 31개의 캡션 모델의 추론을 수행한다. 다음으로, 인간 주석자들은 "_every good caption_"를 선택하도록 요청되며, 여기서 좋은 캡션은 다음과 같이 정의된다: "_the caption is any wrong information can contain any wrong information and need to cover the main action OR all the main objects in the video._" 캡션이 좋은 캡션이 아닌 경우 주석자는 "올 배드" 옵션을 선택하도록 요청됩니다. 우리는 캡션 순서에 대한 주석자의 편향을 최소화하기 위해 31개의 캡션을 무작위로 셔플한다. 인간이 31개의 캡션 문장을 동시에 읽는 데 집중하기 어렵다는 점을 고려하여 캡션을 세 그룹으로 나눈다. 주석자는 최대 11개의 캡션으로 동일한 비디오를 세 번 볼 수 있습니다. 본 사용자 연구의 인터페이스를 그림 9에 나타내고 그 결과를 그림 10에 나타내었다.\n' +
      '\n' +
      '**모델 선택의 알고리즘.** 두 번째 단계에서는 8개의 캡션 모델 리스트를 대표로 모아 대규모 캡션 연산을 줄인다. 직관적으로 상위 8개 성능을 나타내는 모델을 선택할 수 있습니다. 그럼에도 불구하고, 모델 선택 알고리즘은 전체 캡션 모델을 캡처할 수는 없지만, 전체 캡션 모델을 캡처할 수는 없다.\n' +
      '\n' +
      '그림 8: VQA 모델의 ** 프롬프트 템플릿.**less, 이러한 동작은 캡션 알고리즘의 철학과 일치하지 않는다. 구체적으로, 본 알고리즘은 다양한 유형의 비디오에 대한 좋은 캡션을 커버하기 위해 다중 교차 모달리티 모델을 활용하고 각 비디오에 대한 주석으로서 하나의 최상의 캡션을 검색한다(섹션 3.3에 설명된 바와 같이). 따라서, 우리는 대부분의 비디오 샘플에 대해 좋은 캡션을 공동으로 커버할 수 있는 모델 세트를 사용할 것을 제안한다. 알고리즘은 가장 성능이 좋은 모델(_i.e_, BLIP-2 with opt6.7b)을 선택하는 것으로 시작한다. 다음으로, 이전에 선택된 모델이 좋은 캡션을 생성할 수 없는 비디오만을 고려하고 그 비디오에서 가장 성능이 좋은 모델을 탐욕스럽게 찾는다. 우리는 8개의 캡션 모델 목록을 만들 때까지 이러한 마음가짐 아래 모델을 재귀적으로 수집한다. 선택된 8개의 모델은 그림 10에서 강조 표시된다.\n' +
      '\n' +
      '또한 그림 10을 통해 단일 자막모델이 동영상의 최대 \\(\\mathbf{30.8\\%}\\)에 대한 좋은 자막을 예측할 수 있음을 알 수 있었다. 이에 비해, 31개의 캡션 모두는 (15.3\\%\\의 "All Bad" 비율을 기준으로) 비디오의 \\(\\mathbf{84.7\\%}\\)에 대해 적어도 하나의 좋은 캡션을 공동으로 예측할 수 있다. 이 사실은 비디오에 대한 캡션을 공동으로 예측하기 위해 여러 교차 모달리티 교사 모델을 사용하는 우리의 동기를 뒷받침한다. 마지막으로, 통계에 따르면, 8개의 선택된 교사 자막 모델을 사용하여 31개의 모든 모델과 유사한 성능을 보이는 비디오의 좋은 자막(\\(\\mathbf{76.8\\%}\\)을 공동으로 예측할 수 있으며, 계산 요구량을 크게 줄일 수 있다.\n' +
      '\n' +
      '도 10: 좋은 캡션을 예측하기 위한 개별 캡션 모델의 **Ratio.** 각 막대는 개별 모델을 나타내며 그 입력 정보에 의해 채색된다. 우리는 회색과 함께 8개의 선택된 교사 모델을 강조한다. 우리는 또한 "올 배드"의 비율을 최우측에 보고한다.\n' +
      '\n' +
      '도 9: 사용자 스터디 인터페이스의 **스크린샷.**\n' +
      '\n' +
      '## 세밀한 비디오-텍스트 검색의 부록 C 세부사항: 데이터세트, 훈련 및 추론\n' +
      '\n' +
      '섹션 3.3에서는 사용 가능한 일반 검색 모델 [25, 39]이 교사 모델에 의해 예측된 8명의 후보에서 최상의 캡션을 선택할 수 없음을 언급한다. 주요한 이유는 모든 후보 캡션이 비디오 샘플과 매우 관련이 있으며 최적의 성능을 위해 모델이 각 캡션 내에서 미묘한 차이를 식별할 것을 요구하기 때문이다. 이를 위해 먼저 부록 C.1에 자세히 설명된 대로 최상의 캡션을 수동으로 선택하여 비디오 샘플의 하위 집합에 주석을 달고, 다음으로 부록 C.2와 C.3에 있는 모든 비디오 샘플에 대해 모델의 추론을 실행한다.\n' +
      '\n' +
      '데이터세트 모음\n' +
      '\n' +
      '데이터 세트에서 100K 비디오 샘플을 무작위로 샘플링하고 인간 주석자에게 각 비디오에 대해 "최고의 캡션_"을 선택하도록 요청한다. 태스크의 시작 시에, 주석자는 다음과 같이 태스크 설명을 읽을 것이다:\n' +
      '\n' +
      '"_You are presented with a short video clip and the set of textual summaries that describe this clip. The textual summary that is the most faithful and descriptive of the video clip. You\'re talking on your friends and you need to describe the video to him._"\n' +
      '\n' +
      '이 작업은 인간이 "_every good caption_"를 선택하도록 요청되는 부록 B.3의 사용자 연구와 다르다는 점에 유의한다. 그러나 캡션을 무작위로 섞고 모든 캡션에 잘못된 정보가 포함된 경우 "올 배드" 옵션을 제공합니다. 학습 및 검증을 위해 "All Bad" 옵션을 선택한 12,064\\의 비디오를 필터링하고 데이터셋을 86,131\\과 1,805\\의 비디오로 분할한다. 우리는 그림 3(파란색 막대)의 검증 세트에 각 교사 모델의 선택적 비율을 표시한다.\n' +
      '\n' +
      '###검색 모델의 정밀화\n' +
      '\n' +
      '우리는 훈련 세트에서 텍스트 검색 모델로 마스킹되지 않은 교사 [39]를 세밀하게 조정한다. ViT-L/16 [21]과 BERTlarge [20]으로 구성된 더 큰 모델 구성을 사용하고, 25M 이미지-텍스트와 비디오-텍스트 쌍에 미리 훈련된 가중치로 모델을 초기화한다. 우리는 원래 코드베이스를 따르고 미세 조정을 위해 비디오-텍스트 대조(VTC) 및 비디오-텍스트 매칭(VTM) 손실 함수만을 사용한다. VTC를 위해 선택된 캡션(_i.e._, 포지티브 샘플)과 나머지 7개의 캡션(_i.e._, 하드 네거티브 샘플)을 구별하는 데 초점을 맞춘 모델을 안내하는 하드 네거티브 마이닝[16, 35]을 구현한다. 구체적으로, 긍정 부정과 부정 부정에 대한 학습 가중치를 \\(1\\)으로 설정하고, 다른 부정 부정에 대한 학습 가중치를 \\(0.01\\)으로 설정한다. 트레이닝 비디오의 경우 12개의 비디오 프레임을 무작위로 샘플링하고 \\(\\mathrm{RandomResizedCrop}\\) 변환을 스케일 \\([0.5,1.0]\\)으로 적용하여 \\(224\\times 224\\mathrm{px}\\)의 해상도로 비디오를 얻는다. 학습률 \\(2e^{-5}\\), \\(\\beta=[0.9,0.999]\\), 중량감소 \\(0.02\\)의 AdamW [46] 최적화기를 사용하였다. 배치 크기를 32로 설정하고 10개의 에폭에 대한 교육을 지속합니다. 모델은 8개의 Nvidia A100 GPU(80GB)에서 조정된다.\n' +
      '\n' +
      '판다-70M 검색 모델의 추론\n' +
      '\n' +
      '미세 조정된 UMT를 사용하여 70M 비디오 모두에 대한 주석으로 최상의 캡션을 자동으로 검색합니다. 우리는 그림 11의 finetuned UMT의 선택 분포와 그림 12의 캡션 길이를 설명한다. 또한 주석이 달린 캡션 내에서 풍부한 내용을 강조하기 위해 그림 13에서 무작위로 샘플링된 100K 캡션 주석의 워드 클라우드를 플로팅한다.\n' +
      '\n' +
      '그림 11: 판다-70M.**에서 캡션의 소스 교사 모델의 **분포 검색 결과 외에도 UMT는 비디오-텍스트 쌍에 대한 매칭 스코어도 예측한다. 실제로, 우리는 점수가 비디오-텍스트 쌍 내의 콘텐츠의 정렬과 높은 상관 관계를 발견한다. 일반적으로 \\(0.43\\)보다 높은 점수는 비디오와 캡션 사이의 강한 연관성을 나타낸다. 수치적으로 판다-70M의 표본 중 \\(89.6\\%\\)은 \\(0.43\\)보다 높은 일치도를 보였다.\n' +
      '\n' +
      '## 부록 D 학생 캡션 모델: 건축 및 훈련\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '그림 4는 학생 캡션 모델의 아키텍처를 보여준다. 모델은 추가 자막 및 메타데이터 입력을 위한 비전 브랜치 및 텍스트 브랜치를 포함한다.\n' +
      '\n' +
      '비전 지점은 Video-LLaMA[88]와 동일한 디자인을 공유한다. 구체적으로, 8 프레임 비디오의 해상도가 \\(224\\times 224\\mathrm{px}\\)일 때, 시각적 인코더는 먼저 각 비디오 프레임을 \\(32\\times 768\\)의 차원으로 다중 프레임 레벨 특징으로 개별적으로 인코딩한다. 비주얼 인코더는 EVA-CLIP[22]의 ViT-G/14 및 Q-former[37]를 포함하는 동결 사전 훈련된 비주얼 인코더로 구성된다. 이어서, 시간 융합 모듈은 다수의 프레임 레벨 특징들을 하나의 (32\\times 768\\) 비디오 표현으로 통합한다. 모듈은 비디오 프레임들에 시간적 정보를 주입하기 위한 위치 임베딩 레이어 및 프레임-레벨 특징들을 융합하기 위한 비디오 Q-포머를 포함한다. 마지막으로, 이 모델은 비디오 표현을 선형 레이어에 의해 \\(32\\times 4096\\)의 특징으로 투영한다.\n' +
      '\n' +
      '텍스트 브랜치에 대해 임의의 길이를 갖는 프롬프트가 주어지면, 모델은 먼저 프롬프트를 토큰화하고 미리 훈련된 임베딩 레이어에 의해 각 토큰을 \\(4096\\) 길이의 특징 벡터에 임베딩한다[18]. 더 긴 프롬프트에 대해 토큰 임베딩의 수가 클 수 있고 프롬프트의 정보가 비디오 콘텐츠와 잘 정렬되지 않을 수 있음을 고려하여 텍스트 Q-전자를 설계하여 고정 및 더 짧은 길이의 텍스트 임베딩을 추출함과 동시에 피쳐를 더 잘 브리지한다.\n' +
      '\n' +
      '도 12: **판다-70M의 캡션 길이 분포.**\n' +
      '\n' +
      '도 13: **Panda-70M에서 100K 캡션 샘플의 워드 클라우드**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:20]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>