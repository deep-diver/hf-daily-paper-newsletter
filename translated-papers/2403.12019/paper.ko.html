<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LN3Diff: 확장 가능한 잠재 신경망 확산\n' +
      '\n' +
      '빠른 3D 생성을 위해\n' +
      '\n' +
      'Yushi Lan\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      'Fangzhou Hong\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      'Shuai Yang\n' +
      '\n' +
      '북경대학교 컴퓨터공학연구소\n' +
      '\n' +
      'Shangchen Zhou\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      'Xuyi Meng\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      'Bo Dai\n' +
      '\n' +
      '3 상하이 AI 연구소 3\n' +
      '\n' +
      'Xingang Pan\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      '첸 체인지 로이\n' +
      '\n' +
      '싱가포르 국립공과대학 1S-Lab\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '뉴럴 렌더링 분야는 생성 모델과 미분 가능한 렌더링 기술의 발전으로 상당한 진전을 목격했다. 2D 확산은 성공을 거두었지만 통합된 3D 확산 파이프라인이 불안정하게 남아 있다. 이 논문은 이러한 격차를 해결하고 빠르고 고품질이며 일반적인 조건부 3D 생성을 가능하게 하기 위해 **LN3Dff**라는 새로운 프레임워크를 소개한다. 제안하는 방법은 3D 인식 구조와 가변 자동 인코더(Variational Autoencoder, VAE)를 이용하여 입력 영상을 구조화, 컴팩트화, 3D 잠재 공간으로 인코딩한다. 잠재성은 트랜스포머 기반 디코더에 의해 고용량 3D 신경장으로 디코딩된다. 이 3D 인식 잠재 공간에 대한 확산 모델을 학습함으로써, 본 방법은 ShapeNet에서 3D 생성을 위한 최첨단 성능을 달성하고, 다양한 데이터 세트에 걸쳐 단안 3D 재구성 및 조건부 3D 생성에서 우수한 성능을 보여준다. 또한 추론 속도 측면에서 기존의 3D 확산 방법을 능가하여 인스턴스당 최적화가 필요하지 않다. 제안된 LN3Dff는 3D 생성 모델링에서 상당한 발전을 보이며 3D 비전 및 그래픽 작업에서 다양한 응용 분야에 대한 가능성을 가지고 있다. 비디오 데모는 프로젝트 웹 페이지에서 [https://nirvanalan.github.io/projects/ln3diff](https://nirvanalan.github.io/projects/ln3diff)를 찾을 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      '품질 및 일반 조건부 3D 생성. 본 논문에서는 가변 오토인코더[41](VAE)를 학습하여 입력 영상을 저차원 3차원 잠재 공간으로 압축하는 방법을 제안한다. 이는 픽셀 공간 확산[12, 28, 31, 79]에 비해 표현력이 높고 유연하다. 이 공간으로부터, 3D 인식 트랜스포머 기반 디코더는 잠재된 것을 점차적으로 고용량 3D 신경장으로 디코딩한다. 이 자동 인코딩 단계는 다중 뷰 데이터 세트에 대한 새로운 뷰 감독[8, 11] 및 단안 데이터 세트에 대한 적대적 감독[36]을 통합하는 미분 가능한 렌더링[84]으로 분할 학습된다. 고용량 모델 설계 덕분에, 우리의 방법은 훈련 동안 인스턴스당 두 개의 뷰만 필요로 하는 더 효율적인 데이터이다. 학습 후 학습한 3D 잠재 공간을 조건부 3D 확산 학습에 활용하여 양질의 3D 생성을 위한 학습 모델의 효과적인 활용을 보장한다. 추가적으로, 미리 트레이닝된 인코더는 들어오는 데이터에 대해 데이터 인코딩을 상각할 수 있고, 따라서 동작들을 간소화하고 향후 연구를 위해 효율적인 3D 확산 학습을 용이하게 하는 한편, 3D 표현의 진보와 호환성을 유지한다.\n' +
      '\n' +
      '구체적으로, 본 논문에서 제안하는 방법은 구조화된 잠재 공간을 유지하면서 빠르고 고품질의 3D 복원을 위한 새로운 3D 인식 구조를 제시한다. 컨볼루션 토큰화기를 사용하여 입력 이미지를 _KL_-정규화된 3D 잠재 공간으로 인코딩하여 우수한 지각적 압축 능력을 활용한다[17]. 또한, 잠재 공간의 3D 토큰에 걸쳐 유연한 3D 인식 주의를 가능하게 하기 위해 트랜스포머[13, 61]를 활용한다. 이 접근법은 3D 공간에서의 효율적인 정보 흐름을 향상시켜, 코히어런트 기하학적 재구성을 촉진한다. 마지막으로, 3D 잠재성을 상향 샘플링하고 이미지 공간 감독에 미분 가능한 렌더링을 적용하여 본 방법을 자체 감독 3D 학습자로 만든다[78].\n' +
      '\n' +
      '본질적으로, 본 연구는 일반적인 고품질 3D 생성 모델을 구축하기 위한 3D 표현-진단 파이프라인을 제안하고, 일련의 다운스트림 3D 비전 및 그래픽 작업을 해결할 수 있는 기회를 제공한다. 주요 기여도는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 우리는 고품질 3D 데이터 인코딩을 상각 방식으로 달성하여 3D 확산 모델에서 상당한 발전을 나타내는 새로운 3D 인식 재구성 모델을 제안한다.\n' +
      '* 컴팩트한 잠재 공간을 학습하여, 우리의 모델은 ShapeNet 벤치마크 [8]에서 최신 3D 생성 성능을 보여주며, GAN 기반 및 3D 확산 기반 접근법 모두를 능가한다.\n' +
      '* ShapeNet, FFHQ, Objaverse 데이터셋에서 단안 3D 재구성 및 조건부 3D 생성에서 우수한 성능을 보여주며, 기존의 잠재 없는 3D 확산 방법에 비해 가장 빠른 추론 속도인 _e.g._, \\(3\\times\\)를 보인다[1].\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**3D 인식 GANs.** 생성적 적대 네트워크[20]는 사실적 이미지[3, 37, 38]를 생성하는 유망한 결과를 보여주었고 연구자들이 3D 인식 생성[25, 54, 59]에 노력을 기울이도록 영감을 주었다. 최근 신경망 렌더링의 성공[50, 52, 60]에 힘입어 연구자들은 3D 유도성 편향을 생성 작업[5, 74]에 도입하고 하이브리드 설계[6, 22, 29, 56, 58]를 통해 인상적인 3D 인식 합성을 보여 일련의 다운스트림 응용[80, 81, 98, 44]에 적용 가능하다. 그러나 GAN 기반 방법은 모드 붕괴[86]를 겪고 더 큰 규모와 다양성을 가진 데이터 세트를 모델링하지 못한다[12]. 또한 GAN의 3차원 복원과 편집은 정교하게 설계된 역산 알고리즘을 필요로 한다[45].\n' +
      '\n' +
      '**3D 인식 확산 모델.** 2D 확산 모델[28, 79]의 전례 없는 성공은 연구자들이 이 기술을 여러 전략으로 3D의 맥락에서 적용하도록 영감을 주었다. DreamFusion[92, 64, 32]은 2D 확산 모델을 증류함으로써 3D 생성에 영감을 주었지만, 값비싼 장면당 최적화, 모드 붕괴 및 야누스 문제를 겪는다. 일부 접근법들은 2D 방식으로 3D 사전 및 렌더링 프로세스를 학습하는 것을 제안한다[7, 47, 85]. 이러한 방법들은 광 사실적이지만, 본질적으로 뷰 일관성이 부족하고 합성된 장면의 기본 3D를 추출할 수 없다. 보다 표준적인 3D 확산 파이프라인은 2단계 훈련 패러다임을 따른다: 1단계로 오토 디코더를 멀티뷰 이미지로 사전 훈련한 후 [14, 34, 46, 53, 76, 91], 3D 잠재 코드 그룹은 2단계로 확산 훈련을 위한 훈련 말뭉치 역할을 한다. 그러나, 자동 디코딩 단계는 작은 공유 디코더를 트레이닝해야 하며, 이는 제한된 확장성을 갖는 부정한 잠재 공간으로 이어진다. 또한, 잠재 잠재량은 보통, \\(256\\times 256\\times 96\\)[91]로, 효율적인 확산 학습을 방해한다[31].\n' +
      '\n' +
      '선행 연구인 RenderDiffusion[1, 35]과 동시 DMV3D[95, 30]은 렌더링 과정을 확산 샘플링에 통합하여 잠재 없는 3D 확산을 제안한다. 간단하지만, 이 설계는 각 잡음 제거 단계에서 시간이 많이 걸리는 볼륨 렌더링을 포함하며, 이는 샘플링 속도를 크게 늦춘다. SSDNeRF[9]는 오토 디코더 파이프라인으로 3D 재구성 및 확산을 공동으로 학습할 것을 제안한다. 그러나 정교한 훈련 일정이 필요하며 단일 카테고리 무조건 세대에 대한 성능만 보여줍니다. 비교적, 제안된 LN3Dff는 렌더링 연산이 필요하지 않은 압축 잠재 공간에서 3D 확산을 훈련한다. 우리는 Sec에서 검증한다. 4 제안한 방법은 3D 생성과 단안 3D 재구성 모두에서 3배 빠른 속도로 더 나은 성능을 달성한다. 또한, 일반적이고 다양한 3D 데이터셋에 대해 조건부 3D 생성을 입증하는 반면, RenderDiffusion과 SSDNeRF는 단순 클래스에 대해 무조건적 생성에 초점을 맞춘다.\n' +
      '\n' +
      '**일반화된 3D 재구성 및 뷰 합성.** NeRF의 장면당 최적화를 우회하기 위해, 연구자들은 이미지 기반 렌더링을 통해 사전 모델 [96, 71, 90]을 학습할 것을 제안한다. 그러나 이러한 방법은 뷰 합성을 위해 설계되었으며 명시적인 3D 표현이 부족하다. LoLNeRF[68]은 오토 디코딩을 통해 선행을 학습하지만, 단순한 카테고리별 설정으로 제한된다. 또한, 이러한 방법은 뷰 합성을 위해 설계되었으며 새로운 3D 객체를 생성하지 못한다. VQ3D[72]는 일반화된 재구성 파이프라인을 3D 생성 모델에 적응시킨다. 그러나, 이들은 2D 아키텍처를 채택하고 내재된 3D 구조의 상당 부분을 무시하는 1D 잠재 공간에 대한 자기회귀 모델링과 함께 작업한다. NeRF-VAE[42]는 VAE 후방으로 3D 가능성을 직접 모델링하지만 VAE의 부족한 용량에 의해 단순한 3D 장면으로 제한된다.\n' +
      '\n' +
      '##3 확장 가능한 잠재 신경망 확산\n' +
      '\n' +
      '이 섹션에서는 전용 가변 오토인코더를 사용하여 압축된 잠재 공간에 대한 효율적인 확산을 학습하는 잠재 3D 확산 모델을 소개한다. 구체적으로, 2차원 영상(x)을 잠재 코드(\\mathbf{z}\\)에 매핑하는 인코더(\\mathcal{E}_{\\mathbf{\\phi}\\), 디노이저(\\mathbf{\\epsilon}_{\\mathbf{\\theta}(\\mathbf{z}_{t},t)\\)를 3차원 삼면(\\widetilde{\\mathcal}X}\\)에 매핑하는 인코더(\\mathcal{E}_{\\mathbf{z}_{t}\\) 및 디코더(\\mathcal{D}_{U}\\)를 각각 학습한다.\n' +
      '\n' +
      '이러한 설계는 (1) 3D 데이터 압축과 확산 단계를 명시적으로 분리함으로써, 표현-특정 3D 확산 설계[101, 34, 53, 76, 1]를 피하고 임의의 신경 렌더링 기술에 적용될 수 있는 3D 표현/렌더링-진단 확산을 달성한다. (2) 고차원 3차원 공간을 남김으로써, 계산적으로 효율적인 학습을 위해 잘 연구된 U-Net 아키텍처[70]를 재사용하고, 더 빠른 속도로 더 나은 샘플링 성능을 달성한다. (3) 1단계에서 훈련된 3D 압축 모델은 효율적이고 범용적인 3D 토큰화기의 역할을 하며, 잠재 공간은 다운스트림 애플리케이션에서 쉽게 재사용되거나 새로운 데이터 세트로 확장될 수 있다[97].\n' +
      '\n' +
      '다음 부분에서는 먼저 Sec. 3.1에서 상세한 프레임워크 설계로 압축 단계에 대해 논의하고, 이를 바탕으로 Sec. 3.2에서 3D 확산 발생 단계를 소개하고 Sec. 3.3에서 조건 주입을 제시하며, 방법 개요는 그림 2에 나와 있다.\n' +
      '\n' +
      '그림 2: LN3Diff.**의 **Pipeline. 3D 잠재 공간 학습 단계에서 컨볼루션 인코더\\(\\mathcal{E}_{\\mathbf{\\phi}\\)는 KL-정규화된 잠재 공간에 단안 입력 영상을 인코딩한다. 인코딩된 3D 잠재성은 3D 인식 DiT 변환기에 의해 추가로 디코딩되며, 여기서 우리는 자기-평면 주의와 교차-평면 주의를 수행한다. 트랜스포머-디코드 잠재성은 합성곱 업샘플러 \\(\\mathcal{D}_{U}\\)에 의해 렌더링 수퍼비전용 고-res tri-plane으로 업샘플링된다. 다음 단계에서는 컴팩트한 잠재 공간에 대한 조건부 확산 학습을 수행한다.\n' +
      '\n' +
      '### 지각적 3차원 잠재압축\n' +
      '\n' +
      'Sec에서 분석한 바와 같이. 1, 확산 훈련을 위해 신경 필드를 직접 활용하는 것은 모델 확장성 및 성능을 저해한다. 이전 작업[17, 69]에서 영감을 받아 입력 이미지를 컴팩트한 3D 잠재 공간으로 압축할 것을 제안한다. 이 패러다임은 이미지 도메인 [17, 69]에서 잘 채택되었지만, 특정 3D 작업 [4, 45, 51, 72]에서 유사한 시도를 통해 처음으로 고품질 압축 모델이 실현 가능하며, 잠재 공간은 효율적인 확산 학습을 위한 컴팩트한 프록시 역할을 한다.\n' +
      '\n' +
      '영상(x\\in\\mathbb{R}^{H\\times W\\times 3}\\)이 주어지면, LN3Diff는 Convolutional encoder\\(\\mathcal{E}_{\\mathbf{\\phi}\\)를 채택하여 잠재 표현(\\(\\mathbf{z}\\sim\\mathcal{E}_{\\mathbf{\\phi}(x)\\)으로 인코딩한다. 카메라 조건을 주입하기 위해 입력 [77]의 일부로 Plucker 좌표 \\(\\mathbf{r}=(\\mathbf{d},\\mathbf{p}\\times\\mathbf{d})\\in\\mathbbb{R}^{6}\\)을 포함하고, 여기서 \\(\\mathbf{d}\\)은 정규화된 광선 방향, \\(\\mathbf{p}\\)은 카메라 원점, \\(\\times\\)은 교차 곱을 나타낸다.\n' +
      '\n' +
      '기존 연구 [17, 72]와 달리 1차원 잠재에서 동작하고 내부 구조를 무시하는 3차원 잠재 \\(\\mathbf{z}\\in\\mathbb{R}^{h\\times w\\times d\\times c}\\)을 출력함으로써 3차원 인식 작업을 용이하게 할 수 있으며, 여기서 \\(h=H/f,w=W/f\\)은 다운샘플 팩터를 갖는 공간 분해능이며, \\(d\\)은 3차원 차원을 나타낸다. 여기서 우리는 \\(f=8\\)과 \\(d=3\\)을 설정하여 \\(\\mathbf{z}\\in\\mathbb{R}^{h\\times w\\times 3\\times c}\\)을 삼중 잠복기로 설정하였는데, 이는 삼중 평면 [6, 62]와 유사하지만 컴팩트한 3차원 잠재 공간이다. 우리는 확산 훈련을 용이하게 하기 위해 잘 구조화된 잠재 공간을 장려하기 위해 _KL-reg_[41]을 추가로 부과한다[69, 89].\n' +
      '\n' +
      '**디코더 트랜스포머.** 디코더는 고품질의 3D 복원을 위해 컴팩트한 3D 코드 \\(\\mathbf{z}\\)를 디코딩하는 것을 목표로 한다. 기존의 이미지-투-3D 방법들[4, 6, 21, 45]은 컨볼루션(convolution)을 빌딩 블록으로 채택하는데, 이는 3D 인식 연산이 부족하고 3D 공간에서의 정보 흐름을 방해한다. 여기서는 유연성과 효율성으로 인해 ViT[13, 61]를 디코더 백본으로 채택한다. 로댕[91]에서 영감을 얻은 우리는 3D 유도성 바이어스를 장려하고 관련되지 않은 3D 특징의 혼합을 피하기 위해 원시 ViT 디코더로 다음과 같이 리노베이션했다. 입력 \\(\\mathbf{z}\\in\\mathbb{R}^{l\\times 3\\times c}\\)에서 \\(l=h\\times w\\)이 수열길이일 때, 우리는 세 개의 잠재평면을 각각 데이터 포인트로 취급하고 자기주장을 수행한다. 이 작업은 효율적이고 로컬 피쳐 집계를 장려합니다. (2) _Cross-plane Attention Block_. 3D 귀납적 편향을 더욱 촉진하기 위해, 우리는 잠재 공간의 모든 토큰들이 서로 참석할 수 있도록 긴 수열\\(l\\times 3\\times c\\~3l\\times c\\)로 \\(\\mathbf{z}\\)을 전개한다. 이러한 방식으로, 우리는 보다 일관성 있는 3D 재구성 및 생성을 위해 글로벌 정보 흐름을 장려한다. 로댕과 비교할 때, 우리의 디자인은 완전히 주의 기반이며 값비싼 축 풀링 집계 없이 병렬 컴퓨팅을 자연스럽게 지원한다.\n' +
      '\n' +
      '경험적으로 DiT[61] 블록을 사용하여 잠재 \\(\\mathbf{z}\\)을 조건으로 주입하면 ViT[13, 57] 블록에 비해 더 나은 성능을 얻을 수 있으며, 이는 잠재 \\(\\mathbf{z}_{0}\\)을 정규 입력으로 사용한다. 구체적으로, 적응형 계층 규범(adaLN) 계층[61]은 전술한 어텐션 연산을 위한 학습 가능한 위치 인코딩과 입력 잠재 \\(\\mathbf{z}\\)을 융합한다. 또한, 앞서 언급한 두 가지 유형의 주의 계층을 인터리브하여 전체 파라미터가 미리 정의된 DiT 길이와 일치하는지 확인하여 효율적인 훈련 및 추론을 보장한다. 모든 연산들이 토큰 공간에서 정의됨에 따라, 디코더는 3D 사전들을 홍보하면서 로댕[91]과 비교하여 효율적인 계산을 달성한다.\n' +
      '\n' +
      'Decoder Upsampler.** 모든 주의 동작 후에 출력으로 마지막 트랜스포머 계층 \\(\\widetilde{\\mathbf{z}\\)에서 토큰을 얻는다. 컨텍스트가 풍부한 토큰은 공간 도메인[24]으로 다시 재구성되고, 컨볼루션 디코더에 의해 형상\\(\\hat{H}\\times\\hat{W}\\times 3C\\)을 갖는 최종 3-평면 표현으로 업샘플링된다. 여기서는 효율적인 업샘플링을 위해 간단한 합성곱 디코더를 채택하였으며, 여기서 \\(\\widetilde{\\mathbf{z}\\)의 세 가지 공간적 잠재성을 병렬로 처리한다.\n' +
      '\n' +
      '**지각적으로 풍부하고 온전한 3D 잠재 공간을 학습.** 적대적 학습[20]은 작고 지각적으로 풍부한 잠재 공간을 학습하는 데 널리 적용되었다[17, 69]. 3D 도메인에서, 적대적 손실은 또한 신규-뷰 재구성 감독들이 적용 불가능한 경우 [5, 72, 39], _예를 들어 FFHQ [36]과 같은 단안 데이터세트일 때 정확한 3D 기하학을 장려할 수 있다. 이전 연구[39, 72]에서 영감을 얻은 우리는 압축 단계에서 이 문제를 우회하기 위해 적대적 손실을 활용합니다. 구체적으로, 지각적으로 합리적인 입력 뷰 재구성을 위해 입력 뷰 판별기를, 렌더링된 이미지를 입력 뷰와 신규 뷰 사이에서 구별하기 위해 보조 신규 뷰 판별기를 부과한다. 우리는 소설 뷰 판별기에 소설 뷰 렌더링과 실제 이미지를 대신 구별하도록 요청하면 재구성 모델이 _후방 붕괴_[48]를 겪으며, 이는 입력 관련 없지만 높은 충실도 결과를 출력하여 소설 뷰 판별기를 속일 수 있음을 관찰한다. 이 현상은 [39]에 의해서도 관측되었다.\n' +
      '\n' +
      '학습.** 디코더 \\(\\hat{D}_{\\mathbf{\\psi}\\) 후, 우리는 잠재성으로부터 \\(\\hat{x}=\\mathtt{R}(\\widetilde{\\mathcal{X})=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi}(\\mathbf{\\psi}}(\\mathbf{\\psi}))=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi}(\\mathbf{\\psi}(\\mathbf{\\psi})}(\\mathbf{\\psi})}(\\mathdf{\\psi})}(\\mathbf{\\psi})(\\hat{x}=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi})}(\\mathbf{\\psi}(\\mathbf{\\psi})))을 갖고, 여기서 \\(\\hat{x}=\\mathtt{R}(\\mathcal{D}_ 여기서는 실험을 위해 3면[6, 62]으로 \\(\\widetilde{\\mathcal{X}\\)을 선택하고 볼륨 렌더링[52]으로 \\(\\mathtt{R}\\)을 선택한다. 우리의 압축 모델은 3D 표현/렌더링 불가지론이며 새로운 신경 렌더링 기술[40]은 디코더 아키텍처를 교대함으로써 쉽게 통합될 수 있다[82].\n' +
      '\n' +
      '상기 최종 훈련 목표는 다음과 같다.\n' +
      '\n' +
      '\\mathcal{L}(\\mathbf{\\phi},\\mathbf{\\psi})=\\mathcal{L}_{\\text{render}+\\lambda_{\\text{geo}\\mathcal{L}_{\\text{geo}+\\lambda_{\\text{kl}\\mathcal{L}_{\\text{KL}+\\lambda_{\\text{GAN}\\mathcal{L}_{\\text{GAN}\\text{1}\\tag{1}\\lambda_{\\text}\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{\\text{render}\\)은 \\(L_{2}\\)과 지각 손실[100]을 혼합한 것이고, \\(\\mathcal{L}_{\\text{reg}\\)은 매끄러운 기하학[93]을 장려하고, \\(\\mathcal{L}_{\\text{KL}\\)은 구조화된 잠재 공간[69]을 장려하기 위한 _KL-reg_ 손실이며, \\(\\mathcal{L}_{\\text{GAN}\\)은 지각 품질을 향상시키고 단안 데이터 세트에 대한 올바른 기하학을 강화한다.\n' +
      '\n' +
      'S ShapeNet[8]과 같은 카테고리별 데이터 세트의 경우 이미 충분한 성능을 제공하는 _one_ 신규 뷰만 감독한다. 다양한 모양 변화를 가진 카테고리 없는 데이터 세트, 예를 들어 Objavverse [11]의 경우 _3_ 신규 뷰를 감독한다. 제안하는 방법은 기존의 최첨단 3D 확산 방법[9, 53]에 비해 데이터 효율이 우수하며, 수렴하기 위해서는 50개의 뷰가 필요하다. 구현 세부 사항은 보충 자료에 포함되어 있습니다.\n' +
      '\n' +
      '### 잠복확산 및 잡음제거\n' +
      '\n' +
      '**잠재 확산 모델.** LDM [69, 89]는 지각 잠재 공간 내에서 사전 분포\\(p_{\\mathbf{\\theta}(\\mathbf{z}_{0})\\)를 획득하도록 설계되었으며, 학습 데이터는 학습된 \\(\\mathcal{E}_{\\mathbf{\\phi}\\)로부터 온라인으로 얻은 잠재 데이터이다. 여기서는 DDPM 변량 목표의 연속 도출인 점수 기반 잠재 확산 모형[89]을 사용한다[28]. 구체적으로, 데노이저\\(\\mathbf{\\epsilon}_{\\theta}\\)는 점수 함수 점수[79]를 연속 시간 시퀀스\\(t\\)로 \\(\\nabla_{\\mathbff{z}_{t}}\\log p(\\mathbf{z}_{t}):=-\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t)/\\sigma_{t}\\으로 매개변수화한다. 잡음 입력(\\mathbf{z}_{t}\\), \\(\\mathbf{\\epsilon}_{\\theta}\\)의 잡음 제거된 변형을 예측하도록 훈련함으로써, 역 SDE[28]를 풀어서 표준 정규 사전(\\mathcal{N}(\\mathbf{0},\\mathbf{I})으로부터 점차적으로 잡음 제거를 학습한다.\n' +
      '\n' +
      'LSGM[89]에 이어서, 우리는 시간에서 학습된 선행을 \\(t\\) 기하학적 혼합 \\(\\mathbf{\\epsilon}_{\\theta}(\\mathbf{\\alpha})\\sigma_{t}(1-\\mathbf{\\alpha})\\mathbff{z}_{t+\\mathbf{\\alpha}\\odot\\mathbf{\\epsilon}_{\\theta}^{\\prime}(\\mathbf{z}_{t},t)\\), 여기서 \\(\\mathbf{\\epsilon}_{\\theta}^{\\prime}(\\mathbf{z}_{t},t)\\)는 데노자 출력이고 \\(\\alpha\\in[0,1]\\(\\alpha\\in[0,1]\\)은 학습 가능한 스칼라 계수이다. 직관적으로, 이 공식은 역 SDE가 더 빨리 해결될 수 있는 표준 정규 분포에 더 가깝게 데노이저 입력을 가져올 수 있다. 유사하게, 안정 확산[69, 63]은 또한 단위 분산을 유지하기 위해 잠재된 입력을 요인별로 스케일링하며, 이는 10억-레벨 데이터세트[73]에서 미리 계산된다. 상기 트레이닝 목표는 다음과 같이 판독된다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{diff}=\\mathbb{E}_{\\mathbf{\\varepsilon}_{\\mathbf{\\phi}(x),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1),t}\\Big{[}\\frac{w_{t}{2}\\|\\mathbf{\\epsilon}-\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t}\\|_{2}^{2}\\Big{}\\tag{2}\\frac{w_{t}}{2}\\|\\mathbf{w_{t}-\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t}\\|_{2}^{2}\\Big{}\\tag{2}\\frac{w_{t}\\frac{w_{t}}{2}\\|\\mathbf{\\epsilon}\n' +
      '\n' +
      '여기서 \\(t\\sim\\mathcal{U}[0,1]\\) 및 \\(w_{t}\\)는 경험적 시간 종속 가중 함수이다.\n' +
      '\n' +
      '디노이저\\(\\mathbf{\\epsilon}_{\\theta}\\)는 시간 종속 U-Net[69]에 의해 구현된다. 훈련 중에 우리는 고정된 \\(\\mathcal{E}_{\\mathbf{\\phi}\\)에서 온라인으로 \\(\\mathbf{z}_{0}\\)을 얻고, 3-latent \\(h\\times w\\times c\\to h\\times(3w)\\times c\\)을 롤아웃하고, \\(\\mathbf{z}_{t}\\)을 얻기 위해 시간에 따른 잡음을 추가한다. 여기서는 확산 학습을 위한 \\(\\epsilon\\) 매개변수화에 대해 보다 안정적인 거동을 얻을 수 있는 \\(속도\\)[49] 매개변수화를 갖는 중요도 샘플링 스케줄 [89]를 선택한다. 학습 후, 잡음 제거된 샘플들은 신경망 렌더링이 적용될 수 있는 \\(\\mathcal{D}_{\\mathbf{\\psi}}\\)을 통한 단일 순방향 통과를 통해 3D 신경 필드(_i.e._, 여기서 tri-plane)로 디코딩될 수 있다.\n' +
      '\n' +
      '### Conditioning Mechanisms\n' +
      '\n' +
      '범주별 무조건적 3D 확산 모델[1]에 초점을 맞춘 기존 접근법과 비교하여, 우리는 이미지/텍스트 조건 3D 생성을 지원하기 위해 잠재 3D 확산 모델에 CLIP 임베딩[67]을 주입하는 것을 제안한다. 입력조건\\(\\mathbf{y}\\)이 주어졌을 때, 확산모델은 조건분포\\(p(\\mathbf{z}|\\mathbf{y})\\)을 \\(\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{y})\\으로 수식화한다. 입력 \\(\\mathbf{y}\\)는 Objaverse와 같은 데이터셋에 대한 텍스트 캡션이거나 ShapeNet과 FFHQ와 같은 일반 데이터셋에 대한 이미지일 수 있다.\n' +
      '\n' +
      '**Text Conditioning.** 텍스트 캡션이 있는 데이터셋의 경우, Stable Diffusion[69]을 따라 CLIP 텍스트 인코더 CLIP\\({}_{T}\\)를 직접 활용하여 텍스트 캡션을 조건으로 인코딩한다. 모든 출력 토큰(77\\times 768\\)은 크로스 어텐션 블록(cross attention block)을 갖는 U-Net denoiser에 주입된다.\n' +
      '\n' +
      '**이미지 Conditioning.** 이미지만을 가진 데이터셋에 대해서는 CLIP 이미지 인코더 CLIP({}_{I}\\)를 이용하여 잠재코드 \\(\\mathbf{z}_{0}\\)에 해당하는 입력 이미지 \\(x\\)을 인코딩하고 출력 임베딩을 조건으로 채택한다. 이미지 및 텍스트 조건을 모두 지원하기 위해 이미지 잠재 코드를 미리 계산된 팩터로 재스케일링하여 잠재 텍스트의 척도와 일치시킨다.\n' +
      '\n' +
      '**Classifier-free Guidance.**_classifier-free guidance_[27]을 채택하여 잠재 조건적 및 무조건적 생성을 모두 지원한다. 확산 모델 훈련 동안, 우리는 조건 없는 설정과 조건부 설정을 공동으로 훈련하기 위해 \\(15\\%\\) 확률로 해당 컨디셔닝 래턴트 임베딩을 랜덤하게 제로 아웃한다. 샘플링 동안 조건부 및 무조건 점수 추정치의 선형 조합을 수행한다:\n' +
      '\n' +
      '\\[\\hat{\\mathbf{\\epsilon}_{\\theta}(\\mathbf{\\t},\\mathbf{\\tau}_{\\theta}(y))=s\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},\\mathbf{\\tau}_{\\theta}(y))+(1-s\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t})}\\tag{3}\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t})\n' +
      '\n' +
      '여기서 \\(s\\)은 샘플링 다양성과 품질의 균형을 맞추기 위해 혼합 강도를 제어하기 위한 안내 척도이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**Datasets.** 대부분의 이전 작업에 이어서 ShapeNet[8]을 사용하여 3D 생성 성능을 벤치마킹한다. 우리는 SRN[78]에서 열차를 분할한 차, 의자 및 평면 범주를 사용하여 각각 \\(2151\\), \\(2612\\) 및 \\(3033\\) 인스턴스를 제공한다. 또한 다양한 고품질 3D 데이터 세트에 대한 성능을 평가하기 위해 도전적인 범주와 복잡한 기하학을 가진 가장 큰 3D 데이터 세트인 Objaverse [11]에 대한 실험도 포함한다. 실험을 위해 G-버퍼 Objaverse[66]에서 제공하는 렌더링을 사용하고, 실험을 위해 \\(35K\\)의 3D 인스턴스와 \\(1400K\\)의 멀티뷰 렌더링을 갖는 고품질 서브세트를 선택한다.\n' +
      '\n' +
      '*Training Details.** 입력으로서 \\(H=W=256\\), 하향 샘플 인자 \\(f=8\\), 3면 크기 \\(\\hat{H}=\\hat{W}=128\\) 및 목표 렌더링 크기 \\(128\\times 128\\)을 갖는 단안 이미지를 채택한다. 효율성을 위해 무작위로 잘라낸 패치(64\\times 64\\)에 대한 감독을 부과한다. 적대적 손실의 경우 판별자 훈련을 위해 비포화 GAN 손실[23]과 함께 시력 보조 GAN[43]에서 DINO[57]을 사용한다. 조건부 확산 훈련은 ShapeNet과 FFHQ를 위한 CLIP 이미지 임베딩과 Objaverse를 위한 공식 텍스트 캡션으로부터 CLIP 텍스트 임베딩을 사용한다. 오토인코딩 모델과 확산 모델은 모두 8개의 A100 GPU를 사용하여 약 7일이 걸리는 \\(500K\\) 반복에 대해 학습된다.\n' +
      '\n' +
      '**Metrics.** 이전 작업 [9, 19, 53, 76]에 이어, 2D 및 3D 메트릭을 모두 채택하여 생성 성능을 벤치마킹한다: FID@50K(Frechet Inception Distance) [26] 및 KID@50K(Kernel Inception Distance) [2]는 2D 렌더링을 평가하고, COV(Coverage Score) 및 MMD(Minimum Matching Distance)는 벤치마킹 3D 지오메트리를 평가한다. 우리는 모든 기준선에서 공정한 비교를 위해 \\(128\\times 128\\)의 모든 메트릭을 계산한다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '이 섹션에서는 본 방법을 최신 GAN 기반 방법 EG3D[6], GET3D[19] 및 최근 확산 기반 방법 DiffRF[53], RenderDiffusion[1] 및 SSDNeRF[9]와 비교한다. LN3Diff는 ShapeNet 실험을 위한 평균 \\(v=2\\)이므로 SSDNeRF를 위한 정규 50-view 훈련된 SSDNeRF\\({}_{\\text{v=50}}\\) 버전과 공정한 비교를 위한 재생된 SSDNeRF\\({}_{\\text{v=3}\\)을 모두 포함한다. 우리는 SSDNeRF가 v=2로 수렴하지 못하는 것을 발견한다. 우리는 유도 척도를 식에 설정한다. (3) ~ \\(s=0\\), 모든 조건부 생성 샘플링에서 \\(s=6.5\\)이다.\n' +
      '\n' +
      '** ShapeNet 상의 무조건적 생성.** 기존의 3D 생성 방법에 대한 우리의 방법을 평가하기 위해 탭에서 ShapeNet 상의 무조건적 단일 카테고리 3D 생성에 대한 정량적 및 정성적 결과를 포함한다. 도 1 및 도 3. 우리는 250 DDIM 단계와 \\(psi=0.7\\)의 GAN 기반 기준선을 사용하여 모든 기준선 3D 확산 방법을 평가한다. FID/KID 평가를 위해 기준선을 재훈련하고 모든 데이터 세트에 걸쳐 고정된 상구 타원체 카메라 궤적[78]을 사용하여 메트릭을 계산한다. COV/MMD 평가를 위해 추출된 메쉬와 Ground truth 메쉬 표면을 중심으로 4096개의 지점을 무작위로 샘플링하고 Chamfer Distance를 적용하여 평가하였다.\n' +
      '\n' +
      '탭에 표시된 대로입니다. 1, LN3Diff는 렌더링 품질과 3D 커버리지 측면에서 모든 GAN 기반 베이스라인에 대해 정량적으로 더 나은 성능을 달성한다. 도. 도 3은 GAN 기반 방법이 크게 고통받는다는 것을 추가로 입증함\n' +
      '\n' +
      '그림 3: **ShapeNet Unconditional Generation.** 각 방법에 대해 4개의 샘플을 선보입니다. 최상의 전망을 보려면 확대하십시오.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '이 방법은 ShapeNet 의자에서 SSDNeRF({}_{\\text{v}=3}\\)에 대한 비교 성능을 보여주며, 나머지 데이터 세트에서는 더욱 우수한 성능을 보여준다.\n' +
      '\n' +
      '**조건부 3D 생성.**조건부 3D 생성은 게임 및 영화 산업 모두에서 3D 모델링 프로세스를 간소화할 수 있는 잠재력을 가지고 있다. 그림 1에서 볼 수 있듯이. 4, 텍스트 또는 이미지가 입력 프롬프트 역할을 하는 ShapeNet 데이터셋에 대한 조건부 생성 성능을 제시한다. 시각적으로 검사한 결과, 제안된 방법은 조건부 생성에서 우수한 성능을 보여주며, 생성된 출력과 입력 조건을 밀접하게 정렬한다. 이미지 조건 생성의 경우, 본 방법은 다양성을 유지하면서 의미적으로 유사한 샘플을 생성한다.\n' +
      '\n' +
      '우리는 그림 1에서 Objavorse의 텍스트 조건 생성을 추가로 보여준다. 5와 Tab. 2. LN3Diff의 잠재 공간을 통해 학습된 확산 모델은 일반 3D 데이터셋에 비해 고품질의 3D 생성을 가능하게 한다. 이거.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & VIT-B/32 & VIT-L/14 \\\\ \\hline Point-E [55] & 26.35 & 21.40 \\\\ Shape-E [34] & 27.84 & 25.84 \\\\ Ours & **29.12** & **27.80** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Text-to-3D에 대한 정량적 메트릭.** 제안된 방법은 두 개의 다른 백본에 대한 CLIP 점수에서 Point-E 및 Shape-E보다 우수하다.\n' +
      '\n' +
      '그림 5: **Objavverse Conditional Generation Given Text Prompt.** 각 프롬프트에 대해 2개의 샘플을 보여준다. 최상의 전망을 보려면 확대하십시오.\n' +
      '\n' +
      '기존 3D 확산 기준선에 의해 전례가 없는 가용성이며, 고도로 제어 가능한 3D 세대[99]로 한 걸음 나아간다. 형상-E 및 점-E에 대한 정성적 비교는 보충에서 찾을 수 있다.\n' +
      '\n' +
      '또한, FFHQ에 대한 3D 생성을 지원하는 유일한 3D 확산 방법인 RenderDiffusion에 대한 방법의 비교를 포함한다. 하부에 도시된 바와 같다. 도 6을 참조하면, 본 논문에서 제안하는 방법은 뷰 일관성 3D 생성을 넘어, ${\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim}{\\sim 정량적으로, 우리의 방법은 RenderDiffusion에 의한 59.3에 비해 36.6의 FID 점수를 달성한다.\n' +
      '\n' +
      '**단안 3D 재구성.** 그림 1에 표시된 샘플 너머 1, 우리는 또한 그림의 상반부에 FFHQ 데이터 세트에 대한 단안 재구성 결과를 포함한다. 6 그리고 RenderDiffusion과 비교한다. 본 논문에서 제안하는 방법은 높은 충실도를 보여주며, 자기폐색 영상에서도 의미적 세부사항을 보존한다. 그러나, RenderDiffusion에 의해 생성된 신규 뷰는 흐릿하게 보이고, 안경의 다리와 같은 입력 뷰에서 보이지 않는 의미 컴포넌트들을 놓친다.\n' +
      '\n' +
      '### 절제 연구 및 분석\n' +
      '\n' +
      '탭에서 셋째, 오바버스의 하위 집합에 대한 자동 인코딩 아키텍처의 각 구성 요소를 \\(7K\\) 인스턴스로 벤치마킹하고 \\(100K\\) 반복에서 PSNR을 기록한다.\n' +
      '\n' +
      '도 6: **FFHQ 단안 재구성(상반부) 및 3D 생성(하반부) 단안 재구성을 위해 홀드-아웃 테스트 세트로 방법을 테스트하고 입력 뷰와 신규 뷰를 시각화한다. 기준선과 비교하여 우리의 방법은 재구성 및 생성 모두에서 일관되게 더 나은 성능을 보여준다.**\n' +
      '\n' +
      '각 구성 요소는 무시할 수 있는 매개변수 증가로 일관된 개선을 도입한다.\n' +
      '\n' +
      '또한 탭에 있습니다. 4, 샘플링 속도와 잠재 공간 크기 비교를 제시한다. 컴팩트한 잠재 공간에서 수행함으로써, 우리의 방법은 최상의 생성 성능을 유지하면서 가장 빠른 샘플링을 달성한다. 렌더 확산은 잠재되지 않은 설계를 따르지만, 중간 3D 신경장은 \\(256^{2}\\times 96\\)의 형태를 가지며 효율적인 확산 훈련을 방해한다.\n' +
      '\n' +
      '##5 결론 및 논의\n' +
      '\n' +
      '본 논문에서는 컴팩트한 3차원 인식 잠재공간에서 확산모델을 학습하여 3차원 생성모델의 새로운 패러다임을 제시한다. 전용 변량 오토인코더는 단안 입력 영상을 저차원 구조의 잠재 공간으로 인코딩하며, 여기서 조건부 확산 학습이 효율적으로 수행될 수 있다. 우리는 ShapeNet을 통해 최첨단 성능을 달성하고, 일반적인 카테고리 없는 Objaverse 3D 데이터 세트에 대해 우리의 방법을 시연한다. 우리의 작업은 3D 비전 및 그래픽 작업에서 일련의 다운스트림 응용 프로그램을 용이하게 할 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '**Limitations and Future Work.** 우리의 방법은 해결되지 않은 몇 가지 제한 사항과 함께 제공됩니다. 재구성 측면에서, 우리는 단안 입력이 도전적인 3D 장면을 재구성하기에 충분하지 않다는 것을 관찰하며, 이는 더 나은 성능을 위해 멀티 뷰 인코더가 필요함을 시사한다. 더욱이, 볼륨 렌더링은 메모리를 소모한다. 디코더를 3DGS [40]과 같은 보다 효율적인 3D 표현으로 확장하는 것은 조사할 가치가 있다. 확산 학습의 경우, DiT[61]와 같은 보다 진보된 아키텍처를 활용하는 것은 보다 유연한 3D 인식 동작들을 가능하게 할 수 있다. 더욱이, 재구성-확산 조인트 트레이닝을 가능하게 하는 것[89, 9]은 수렴을 더욱 가속화할 수 있다. 또한, MVImageNet[97]과 같은 더 많은 실제 데이터와 더 많은 제어 조건[99]을 추가하는 것도 탐색할 가치가 있다. 전반적으로, 우리는 우리의 방법이 일반적인 3D 확산 모델을 향한 단계이며 이러한 방향으로 향후 연구를 고무할 수 있다고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & V100-sec & Latent Size \\\\ \\hline Get3D/EG3D & \\textless{}0.5 & 256 \\\\ \\hline SSDNeRF & 8.1 & \\(128^{2}\\times 18\\) \\\\ RenderDiffusion & 15.8 & - \\\\ DiffRF & 18.7 & \\(32^{3}\\times 4\\) \\\\ LN3D\\({}_{\\text{IF}\\text{uncond}}\\) & **5.7** & \\(32^{2}\\times 12\\) \\\\ LN3D\\({}_{\\text{IF}\\text{cfg}}\\) & **7.5** & \\(32^{2}\\times 12\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **확산 샘플링 속도 및 잠재 크기**. 잠재 크기와 함께 1 V100에서 평가된 인스턴스당 샘플링 시간을 제공한다. 제안하는 방법은 우수한 생성 성능을 유지하면서 보다 빠른 샘플링 속도를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Design & PSNR@100K \\\\ \\hline\n' +
      '2D Conv Baseline & 17.46 \\\\ \\hline + ViT Block & 18.92 \\\\ ViT Block \\(\\rightarrow\\) DiT Block & 20.61 \\\\ + Plucker Embedding & 21.29 \\\\ + Cross-Plane Attention & 21.70 \\\\ + Self-Plane Attention & **21.95** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Reconstruction Arch Design의 Ablation.** 우리는 자동 인코딩 아키텍처의 설계를 ablate한다. 각 컴포넌트는 재구성 성능에서 일관된 이득에 기여하여 모델링 용량의 향상을 나타낸다.\n' +
      '\n' +
      '**잠재적 부정적 사회적 영향** LN3Diff의 개체 관계 구성 능력은 실제 인간 수치에 악의적으로 적용될 수 있다. 추가적인 잠재적 영향은 Supplmentary File에서 자세히 논의됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anciukevicius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra, N.J., Guerrero, P.: RenderDiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In: CVPR (2023) 3, 4, 5, 8, 9, 11\n' +
      '* [2] Binkowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying MMD GANs. In: ICLR (2018) 9\n' +
      '* [3] Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high fidelity natural image synthesis. In: ICLR (2019) 3\n' +
      '* [4] Cai, S., Obukhov, A., Dai, D., Van Gool, L.: Pix2NeRF: Unsupervised Conditional p-GAN for Single Image to Neural Radiance Fields Translation. In: CVPR (2022) 6, 26\n' +
      '* [5] Chan, E., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: Pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis. In: CVPR (2021) 4, 7\n' +
      '* [6] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient geometry-aware 3D generative adversarial networks. In: CVPR (2022) 2, 4, 6, 7, 9, 11\n' +
      '* [7] Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., Mello, S.D., Karras, T., Wetzstein, G.: GeNVS: Generative novel view synthesis with 3D-aware diffusion models. In: arXiv (2023) 4\n' +
      '* [8] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model Repository. arXiv preprint arXiv:1512.03012 (2015) 3, 7, 9, 26\n' +
      '* [9] Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffusion nerf: A unified approach to 3D generation and reconstruction. In: ICCV (2023) 2, 4, 7, 9, 11, 14\n' +
      '* [10] Contributors, S.: SpConv: Spatially sparse convolution library. [https://github.com/traveller59/spconv](https://github.com/traveller59/spconv) (2022) 2\n' +
      '* [11] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3D objects. arXiv preprint arXiv:2212.08051 (2022) 3, 7, 9\n' +
      '* [12] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS (2021) 2, 3, 4, 22, 24\n' +
      '* [13] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 3, 6\n' +
      '* [14] Dupont, E., Kim, H., Eslami, S.M.A., Rezende, D.J., Rosenbaum, D.: From data to functa: Your data point is a function and you can treat it like one. In: ICML (2022) 4\n' +
      '* [15] Dupont, E., Martin, M.B., Colburn, A., Sankar, A., Susskind, J., Shan, Q.: Equivariant neural rendering. In: International Conference on Machine Learning. pp. 2761-2770. PMLR (2020) 26\n' +
      '*1210(2018) 26\n' +
      '* [17] Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthesis. In: CVPR (2021)\n' +
      '* [18] Fridovich-Keil and Yu, Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenoxels: Radiance fields without neural networks. In: CVPR (2022)\n' +
      '* [19] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3D: A generative model of high quality 3D textured shapes learned from images. In: NeurIPS (2022)\n' +
      '* [20] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)\n' +
      '* [21] Gu, J., Gao, Q., Zhai, S., Chen, B., Liu, L., Susskind, J.: Learning controllable 3D diffusion models from single-view images. arXiv preprint arXiv:2304.06700 (2023)\n' +
      '* [22] Gu, J., Liu, L., Wang, P., Theobalt, C.: StyleNeRF: A style-based 3D-aware generator for high-resolution image synthesis. In: ICLR (2021)\n' +
      '* [23] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein GANs. In: NeurIPS (2017)\n' +
      '* [24] He, K., Chen, X., Xie, S., Li, Y., Doll\'ar, P., Girshick, R.B.: Masked autoencoders are scalable vision learners. In: CVPR (2022)\n' +
      '* [25] Henzler, P., Mitra, N.J., Ritschel, T.: Escaping plato\'s cave: 3D shape from adversarial rendering. In: ICCV (2019)\n' +
      '* [26] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS (2017)\n' +
      '* [27] Ho, J.: Classifier-free diffusion guidance. In: NeurIPS (2021)\n' +
      '* [28] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS (2020)\n' +
      '* [29] Hong, F., Chen, Z., Lan, Y., Pan, L., Liu, Z.: EVA3D: Compositional 3D human generation from 2d image collections. In: ICLR (2022)\n' +
      '* [30] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. In: ICLR (2023)\n' +
      '* [31] Hoogeboom, E., Heek, J., Salimans, T.: simple diffusion: End-to-end diffusion for high resolution images. In: ICML (2023)\n' +
      '* [32] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: CVPR (2022)\n' +
      '* [33] Jang, W., Agapito, L.: Codenerf: Disentangled neural radiance fields for object categories. In: ICCV. pp. 12949-12958 (2021)\n' +
      '* [34] Jun, H., Nichol, A.: Shap-E: Generating conditional 3D implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [35] Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.: Holodiffusion: Training a 3D diffusion model using 2D images. In: CVPR (2023)\n' +
      '* [36] Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for improved quality, stability, and variation. In: ICLR (2018)\n' +
      '* [37] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: CVPR (2019)\n' +
      '* [38] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of StyleGAN. In: CVPR (2020)\n' +
      '* [39] Kato, H., Harada, T.: Learning view priors for single-view 3D reconstruction. In: CVPR (2019)\n' +
      '*[*[40] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4), 1-14(2023) 7, 14\n' +
      '* [41] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv (2013) 3, 6\n' +
      '* [42] Kosiorek, A.R., Strathmann, H., Zoran, D., Moreno, P., Schneider, R., Mokr\'a, S., Rezende, D.J.: NeRF-VAE: A geometry aware 3D scene generative model. ICML (2021) 5\n' +
      '* [43] Kumari, N., Zhang, R., Shechtman, E., Zhu, J.Y.: Ensembling off-the-shelf models for gan training. In: CVPR (2022) 9\n' +
      '* [44] Lan, Y., Loy, C.C., Dai, B.: DDF: Correspondence distillation from nerf-based gan. IJCV (2022) 2, 4, 28\n' +
      '* [45] Lan, Y., Meng, X., Yang, S., Loy, C.C., Dai, B.: E3dge: Self-supervised geometry-aware encoder for style-based 3D gan inversion. In: CVPR (2023) 2, 4, 6\n' +
      '* [46] Lan, Y., Tan, F., Qiu, D., Xu, Q., Genova, K., Huang, Z., Fanello, S., Pandey, R., Funkhouser, T., Loy, C.C., Zhang, Y.: Gaussian3Diff: 3D gaussian diffusion for 3D full head synthesis and editing. arXiv (2023) 2, 4\n' +
      '* [47] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3D object (2023) 2, 4\n' +
      '* [48] Lucas, J., Tucker, G., Grosse, R.B., Norouzi, M.: Understanding posterior collapse in generative latent variable models. In: ICLR (2019) 7\n' +
      '* [49] Meng, C., Gao, R., Kingma, D.P., Ermon, S., Ho, J., Salimans, T.: On distillation of guided diffusion models. In: CVPR. pp. 14297-14306 (2022) 8\n' +
      '* [50] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy Networks: Learning 3D reconstruction in function space. In: CVPR (2019) 4\n' +
      '* [51] Mi, L., Kundu, A., Ross, D., Dellaert, F., Snavely, N., Fathi, A.: im2nerf: Image to neural radiance field in the wild. In: arXiv (2022) 6\n' +
      '* [52] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020) 2, 4\n' +
      '* [53] Muller, N., Siddiqui, Y., Porzi, L., Bulo, S.R., Kontschieder, P., Niessner, M.: DiffRF: Rendering-guided 3D radiance field diffusion. In: CVPR (2023) 2, 4, 5, 7, 9, 11\n' +
      '* [54] Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.: HoloGAN: Unsupervised Learning of 3D Representations From Natural Images. In: ICCV (2019) 4\n' +
      '* [55] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts (2022) 12, 26\n' +
      '* [56] Niemeyer, M., Geiger, A.: GIRAFFE: Representing scenes as compositional generative neural feature fields. In: CVPR (2021) 4\n' +
      '* [57] Oquab, M., Darceet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.Y., Xu, H., Sharma, V., Li, S.W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: DINOv2: Learning robust visual features without supervision (2023) 6, 9\n' +
      '* [58] Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J.J., Kemelmacher-Shlizerman, I.: StyleSDF: High-resolution 3D-consistent image and geometry generation. In: CVPR (2021) 4\n' +
      '* [59] Pan, X., Dai, B., Liu, Z., Loy, C.C., Luo, P.: Do 2D GANs know 3D shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs. In: ICLR (2021) 4* [60] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning continuous signed distance functions for shape representation. In: CVPR. pp. 165-174 (2019)\n' +
      '* [61] Peebles, W., Xie, S.: Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748 (2022)\n' +
      '* [62] Peng, S., Niemeyer, M., Mescheder, L., Pollefeys, M., Geiger, A.: Convolutional occupancy networks. In: ECCV (2020)\n' +
      '* [63] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: SDXL: Improving latent diffusion models for high-resolution image synthesis. In: arXiv (2023)\n' +
      '* [64] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: DreamFusion: Text-to-3D using 2D diffusion. ICLR (2022)\n' +
      '* [65] Qi, C., Su, H., Mo, K., Guibas, L.: PointNet: Deep learning on point sets for 3D classification and segmentation. arXiv (2016)\n' +
      '* [66] Qiu, L., Chen, G., Gu, X., zuo, Q., Xu, M., Wu, Y., Yuan, W., Dong, Z., Bo, L., Han, X.: Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d. arXiv preprint arXiv:2311.16918 (2023)\n' +
      '* [67] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n' +
      '* [68] Rebain, D., Matthews, M., Yi, K.M., Lagun, D., Tagliasacchi, A.: LOLNeRF: Learn from one look. In: CVPR (2022)\n' +
      '* [69] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022)\n' +
      '* [70] Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In: MICCAI (2015)\n' +
      '* [71] Sajjadi, M.S.M., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora, S., Lucic, M., Duckworth, D., Dosovitskiy, A., Uszkoreit, J., Funkhouser, T., Tagliasacchi, A.: Scene Representation Transformer: Geometry-free novel view synthesis through set-latent scene representations. CVPR (2022)\n' +
      '* [72] Sargent, K., Koh, J.Y., Zhang, H., Chang, H., Herrmann, C., Srinivasan, P.P., Wu, J., Sun, D.: VQ3D: Learning a 3D-aware generative model on imagenet. ICCV (2023)\n' +
      '* [73] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAION-5B: An open large-scale dataset for training next generation image-text models. In: arXiv (2022)\n' +
      '* [74] Schwarz, K., Liao, Y., Niemeyer, M., Geiger, A.: GRAF: Generative radiance fields for 3D-aware image synthesis. In: NeurIPS (2020)\n' +
      '* [75] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. In: arXiv (2023)\n' +
      '* [76] Shue, J., Chan, E., Po, R., Ankner, Z., Wu, J., Wetzstein, G.: 3d neural field generation using triplane diffusion. In: CVPR (2022)\n' +
      '* [77] Sitzmann, V., Rezchikov, S., Freeman, W.T., Tenenbaum, J.B., Durand, F.: Light field networks: Neural scene representations with single-evaluation rendering. In: NeurIPS (2021)\n' +
      '* [78] Sitzmann, V., Zollhofer, M., Wetzstein, G.: Scene Representation Networks: Continuous 3D-structure-aware neural scene representations. In: NeurIPS (2019)\n' +
      '*[*[79] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based generative modeling through stochastic differential equations. In : ICLR(2021)\n' +
      '* [80] Sun, J., Wang, X., Shi, Y., Wang, L., Wang, J., Liu, Y.: Ide-3d: Interactive disentangled editing for high-resolution 3D-aware portrait synthesis. ACM Transactions on Graphics (TOG) **41**(6), 1-10 (2022)\n' +
      '* [81] Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., Wang, J.: FENeRF: Face editing in neural radiance fields. In: arXiv (2021)\n' +
      '* [82] Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3D reconstruction. In: arXiv (2023)\n' +
      '* [83] Tatarchenko, M., Dosovitskiy, A., Brox, T.: Multi-view 3d models from single images with a convolutional network (2016)\n' +
      '* [84] Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Xu, Z., Simon, T., Niessner, M., Tretschk, E., Liu, L., Mildenhall, B., Srinivasan, P., Pandey, R., Orts-Escolano, S., Fanello, S., Guo, M.G., Wetzstein, G., y Zhu, J., Theobalt, C., Agrawala, M., Goldman, D.B., Zollhofer, M.: Advances in neural rendering. Computer Graphics Forum **41** (2021)\n' +
      '* [85] Tewari, A., Yin, T., Cazenavette, G., Rezchikov, S., Tenenbaum, J.B., Durand, F., Freeman, W.T., Sitzmann, V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In: NeurIPS (2023)\n' +
      '* [86] Thanh-Tung, H., Tran, T.: Catastrophic forgetting and mode collapse in gans. IJCNN pp. 1-10 (2020)\n' +
      '* [87] Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.: KPConv: Flexible and deformable convolution for point clouds. In: ICCV (2019)\n' +
      '* [88] Trevithick, A., Yang, B.: GRF: Learning a general radiance field for 3D scene representation and rendering. In: ICCV (2021)\n' +
      '* [89] Vahdat, A., Kreis, K., Kautz, J.: Score-based generative modeling in latent space. In: NeurIPS (2021)\n' +
      '* [90] Wang, Q., Wang, Z., Genova, K., Srinivasan, P.P., Zhou, H., Barron, J.T., Martin-Brualla, R., Snavely, N., Funkhouser, T.A.: IBRNet: Learning Multi-View Image-Based Rendering. In: CVPR (2021)\n' +
      '* [91] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., et al.: RODIN: A generative model for sculpting 3D digital avatars using diffusion. In: CVPR (2023)\n' +
      '* [92] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In: NeurIPS (2023)\n' +
      '* [93] Weng, C.Y., Srinivasan, P.P., Curless, B., Kemelmacher-Shlizerman, I.: PersonNeRF: Personalized reconstruction from photo collections. In: CVPR. pp. 524-533 (June 2023)\n' +
      '* [94] Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J., Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. Computer Graphics Forum **41** (2021)\n' +
      '* [95] Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wetzstein, G., Xu, Z., Zhang, K.: DMV3D: Denoising multi-view diffusion using 3D large reconstruction model. In: ICLR (2023)\n' +
      '* [96] Yu, A., Ye, V., Tancik, M., Kanazawa, A.: PixelNeRF: Neural radiance fields from one or few images. In: CVPR (2021)\n' +
      '* [97] Yu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Liang, T., Chen, G., Cui, S., Han, X.: MVImgNet: A large-scale dataset of multi-view images. In: CVPR (2023)* [98] Zhang, J., Lan, Y., Yang, S., Hong, F., Wang, Q., Yeo, C.K., Liu, Z., Loy, C.C.: Deformtoon3d: Deformable 3D toonification from neural radiance fields. In: ICCV (2023) 4\n' +
      '* [99] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: ICCV (2023) 2, 13, 14, 27\n' +
      '* [100] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018) 7\n' +
      '* [101] Zhou, L., Du, Y., Wu, J.: 3D shape generation and completion through point-voxel diffusion. In: ICCV (2021)In this supplementary material, we provide additional details regarding the implementations and additional results. We also discuss the limitations of our model.\n' +
      '\n' +
      '**Broader Social Impact.** 본 논문에서는 단일 모델을 사용하여 고품질의 질감과 기하학을 생성하도록 설계된 새로운 잠재 3D 확산 모델을 소개한다. 결과적으로, 우리의 접근법은 딥페이크 또는 기만적인 3D 자산을 생성하는 데 적용될 가능성이 있으며, 위조된 이미지 또는 비디오의 생성을 용이하게 한다. 이것은 개인들이 잘못된 정보를 퍼뜨리거나 평판을 더럽히는 것을 목표로 악의적인 의도를 가지고 그러한 기술을 이용할 수 있기 때문에 우려를 불러일으킨다.\n' +
      '\n' +
      '## 부록 0.구현 상세사항\n' +
      '\n' +
      '### Training details\n' +
      '\n' +
      '**Diffusion.** 우리는 주로 ADM[12]으로부터의 확산 훈련 파이프라인 구현, LDM[69]으로부터의 공간 트랜스포머 주의 구현과 함께 LSGM[89]으로부터의 연속적인 잡음 스케줄을 채택한다. 탭 5에 하이퍼파라미터를 나열합니다.\n' +
      '\n' +
      '*VAE Architecture.** 컨벌루셔널 인코더\\(\\mathcal{E}_{\\mathbf{\\phi}\\)의 경우, 효율성을 위해 채널 64와 1개의 잔여 블록을 갖는 LDM[69] 인코더의 더 가벼운 버전을 채택한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Diffusion Model Details & \\\\ \\hline Learning Rate & \\(2e-5\\) \\\\ Batch Size & 96 \\\\ Optimizer & AdamW \\\\ Iterations & 500K \\\\ \\hline U-Net base channels & 320 \\\\ U-Net channel multiplier & 1, 1, 2, 2, 4, 4 \\\\ U-Net res block & 2 \\\\ U-Net attention resolutions & 4,2,1 \\\\ U-Net Use Spatial Transformer & True \\\\ U-Net Learn Sigma & False \\\\ U-Net Spatial Context Dim & 768 \\\\ U-Net attention head channels & 64 \\\\ U-Net pred type & \\(v\\) \\\\ U-Net norm layer type & GroupNorm \\\\ \\hline Noise Schedules & Linear \\\\ CFG Dropout prob & 15\\% \\\\ CLIP Latent Scaling Factor & 18.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 확산 모델 \\(\\mathbf{\\epsilon}_{\\theta}\\)의 하이퍼파라미터 및 아키텍처.\n' +
      '\n' +
      '컨볼루션 업샘플러 \\(\\mathcal{D}_{U}\\)의 경우, 채널의 절반은 32로 추가되며, 다른 모든 하이퍼-파라미터들은 디폴트 설정들에 유지된다. 변압기 디코더\\(\\mathcal{D}_{T}\\)에 대해서는 DiT-B/2 구조를 사용하였다. 전체 저장된 VAE 모델은 약 450MiB 스토리지를 사용합니다. 우리는 전체 계산 한계로 인해 더 빠른 훈련 속도로 더 작은 모델을 교환하고 더 무거운 모델은 확실히 더 나은 성능에 힘을 실어줄 것이다[61, 95]. 우리는 ShapeNet 및 FFHQ 데이터 세트에 대한 플러커 카메라 조건을 무시하며, 여기서 원시 RGB 입력은 이미 충분한 성능을 제공한다.\n' +
      '\n' +
      '### 데이터 및 기준선 비교\n' +
      '\n' +
      '**트레이닝 데이터.** ShapeNet의 경우 GET3D[19]에 이어 블렌더를 사용하여 전경 마스크를 사용하여 모든 ShapeNet 데이터셋에 대해 50개 시점의 다시점 영상을 렌더링한다. 이 카메라들은 반지름이 1.2인 공의 상부 구에서 표본을 추출합니다. Objaverse의 경우 실험을 위해 G-버퍼 Objaverse[66]에서 전처리된 렌더링에서 고품질 하위 집합을 사용한다.\n' +
      '\n' +
      '**평가.** 2D 메트릭은 50k개의 생성된 이미지와 모든 사용 가능한 실제 이미지 사이에서 계산된다. 또한 기하학적 품질 비교를 위해 5000개 물체의 표면에서 4096점을 샘플링하고 Chamfer Distance(CD)를 사용하여 Coverage Score(COV)와 Minimum Matching Distance(MMD)를 다음과 같이 적용한다.\n' +
      '\n' +
      '{split}&CD(X,Y)=\\sum_{x\\in X}{min}||x-y||_{2}+\\sum_{y\\in Y}\\underset{x\\in X}{min}||x-y||_{2}^{2},\\\\&COV(S_{g},S_{r})=\\frac{|\\text{arg min}_{Y\\in S_{r}}{min}CD(X,Y},S_{r}}\\underset{X\\in S_{g}}{m}{|S_{r}}{n\n' +
      '\n' +
      '여기서 \\(X\\in S_{g}\\) 및 \\(Y\\in S_{r}\\)은 생성된 형상 및 기준 형상을 나타낸다.\n' +
      '\n' +
      'COV와 MMD를 계산하기 위해 5k개의 생성된 객체\\(S_{g}\\)와 모든 훈련 모양\\(S_{r}\\)을 사용한다는 점에 유의한다. 공정성을 위해 원본을 중심으로 그 정도를 [-1,1]로 회상하여 모든 포인트 클라우드를 정규화한다. Coverage Score는 생성된 샘플의 다양성을 평가하는 것을 목표로 하며, MMD는 생성된 샘플의 품질을 측정하는 데 사용된다. 2D 메트릭은 128 \\(\\times\\)128의 해상도로 평가되며, GT 데이터는 intern 구조를 포함하고 있기 때문에 모든 방법과 Ground truth의 결과를 위해 물체의 외부 표면에서 점만을 샘플링한다.\n' +
      '\n' +
      'FID/KID 평가의 경우, 서로 다른 방법이 고유한 평가 설정을 가지고 있기 때문에, 크기 20의 고정된 상구 타원체 카메라 포즈 궤적을 사용하여 각 기준선의 샘플을 재렌더링하여 이 프로세스를 표준화하고, 각 방법에 대해 \\(2.5K\\) 샘플링된 3D 인스턴스를 사용하여 FID@50K/KID@50K를 재계산하여 모든 방법에 걸쳐 공정한 비교를 보장한다.\n' +
      '\n' +
      '**Baselines.** 공식 출시된 코드베이스를 사용하여 ShapeNet 렌더링에서 EG3D, GET3D 및 SSDNeRF를 재생산합니다. RenderDiffusion의 경우 ShapeNet 실험을 위해 저자가 공유한 코드와 사전 학습된 모델을 사용한다. FFHQ 데이터세트와 관련하여 저자의 해당 추론 구성과 체크포인트를 사용할 수 없기 때문에 논문에 보고된 대로 무조건 생성 및 단안 재구성 결과를 통합한다. DiffRF의 경우, 공개 코드가 없는 경우, 우리는 그들의 방법을 Plenoxel[18]과 ADM[12]로 재현한다.\n' +
      '\n' +
      '## 부록 0.B 추가 결과\n' +
      '\n' +
      '## 보다 질적인 3D 생성 결과\n' +
      '\n' +
      '그림 1의 ShapeNet에서 방법으로 생성된 더 많은 미경화 샘플을 포함한다. 도 7 및 그림 8의 FFHQ에서 Objaverse의 경우 정성적 내용을 포함한다.\n' +
      '\n' +
      '도 7: ** LN3Diff(Uncurated)에 의한 무조건 3D 생성. 우리는 ShapeNet의 세 가지 범주에서 LN3Diff에 의해 생성된 미경화 샘플을 보여준다. 각 샘플에 대해 두 개의 뷰를 시각화합니다. 확대해 보는게 좋을거야\n' +
      '\n' +
      '도 8: ** LN3Diff에 의한 무조건적 3D 생성(Uncurated).** FFHQ 상의 LN3Diff에 의해 생성된 미경화 샘플을 보여준다. 추출된 깊이와 함께 각 샘플에 대해 두 개의 뷰를 시각화한다. 확대해 봐\n' +
      '\n' +
      '그림 9: **Text-to-3D**의 질적 비교 ShapeNet에서 LN3Diff에 의해 생성된 미경화 샘플을 세 가지 범주로 보여준다. 각 샘플에 대해 두 개의 뷰를 시각화합니다. 확대해 봐\n' +
      '\n' +
      '최첨단 일반 3D 생성 모델(Fig.의 Shape-E[34] 및 Point-E[55])에 대한 평가. 9, 탭의 정량적 벤치마크와 함께. 2. 본고. 우리는 텍스트-3D 정렬을 평가하기 위해 DreamField[32]에서 CLIP 정밀도 점수를 사용한다. 알 수 있는 바와 같이, LN3Diff는 Shape-E 및 Point-E에 대해 CLIP 점수가 높을수록 더 많은 기하학 및 외관 세부 사항을 보여준다.\n' +
      '\n' +
      '### More Monocular 3D 재구성 결과\n' +
      '\n' +
      '우리는 단계-1 단안 3D 재구성 VAE의 일반화 능력을 추가로 벤치마킹한다. S ShapeNet의 경우, 정량적 평가를 Tab. 6에 포함시켰으며, 본 방법은 단안 3차원 재구성 기준선과 유사한 성능을 보였다. 엄밀히 말하면, 우리의 스테이지-1 VAE는 Pix2NeRF[4]와 유사한 설정을 공유하며, 그 인코더는 또한 생성 모델링을 위한 잠재 공간을 갖는다. PixelNeRF[96]와 같은 다른 재구성-특정 방법들은 이러한 요건들을 갖지 않으며, 재구성 성능을 더욱 높이기 위해 픽셀-정렬된 특징들 및 롱-스킵 연결들과 같은 일부 설계들을 활용할 수 있다. 우리는 주로 참조용으로 그들의 성능을 포함하고 향후 작업을 위해 최첨단 3D 재구성 모델과 유사한 성능을 가진 단계-1 VAE 모델을 훈련시킨다.\n' +
      '\n' +
      '또한 그림 1의 Objaverse 분할에 대한 LN3Diff의 단계-1 단안 VAE 재구성 성능을 시각화한다. 11. 보듯이, 다만\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) \\\\ \\hline GRF [88] & 21.25 & 0.86 \\\\ TCO [83] & 21.27 & 0.88 \\\\ dGQN [16] & 21.59 & 0.87 \\\\ ENR [15] & 22.83 & - \\\\ SRN* [78] & 22.89 & 0.89 \\\\ CodeNeRF* [33] & 22.39 & 0.87 \\\\ PixelNeRF [96] & **23.72** & **0.91** \\\\ \\hline Pix2NeRF [4] conditional & 18.14 & 0.84 \\\\ Ours & 20.91 & 0.89 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: ShapeNet-SRN [78, 8] 의자에 대한 정량적 결과는 \\(128\\times 128\\)에 대해 평가한다. 전설: * - 테스트 시간 최적화가 필요합니다. 우리의 단계-1 VAE는 Pix2NeRF[96]와만 동일한 설정을 공유하며, 이는 또한 생성 학습을 위한 명시적인 잠재 공간을 가지고 있다. 참고용으로 다른 기준선이 포함되어 있습니다.\n' +
      '\n' +
      '하나의 뷰가 입력으로 제공되며, 우리의 단안 VAE 재구성은 상세한 깊이 맵을 사용하여 고품질 및 뷰 일치 3D 재구성을 산출할 수 있다. 정량적으로, 전체 Objaverse 데이터셋에 대한 새로운 뷰 재구성 성능은 평균 26.14의 PSNR을 달성하며, 이는 우리의 잠재 공간이 효율적인 3D 확산 훈련을 위한 컴팩트한 프록시로 취급될 수 있음을 보여준다.\n' +
      '\n' +
      '## 부록 0.C 제한 및 실패 사례\n' +
      '\n' +
      '주요 제출에 제한 사항에 대한 간략한 논의를 포함시켰습니다. 여기서는 LN3Diff의 한계와 향후 개선 방향에 대한 보다 심층적인 분석을 위해 시각 장애 사례와 함께 더 자세한 내용을 포함한다.\n' +
      '\n' +
      '### VAE Limitations\n' +
      '\n' +
      '인코더 입력으로 단안 이미지를 사용하면 고품질 3D 복원을 달성할 수 있음을 입증했다. 그러나 다양한 색상과 기하학적 세부 사항을 가진 일부 도전적인 경우에 단안 인코더는 흐릿한 아티팩트를 초래한다는 것을 발견했다. 그림 1에 표시된 대로. 도 11에 도시된 바와 같이, 단안 입력을 갖는 우리의 방법은 보이지 않는 시점들에 걸쳐 부동 아티팩트들을 산출할 수 있다. 우리는 이러한 인공물이 주로 단안 입력의 모호성과 훈련 중 회귀 손실(L2/LPIPS)의 사용에 기인한다고 가정한다. 이러한 관찰은 더 나은 성능을 위해 멀티뷰 인코더로의 전환이 필요하다는 것을 입증한다.\n' +
      '\n' +
      '게다가, VAE는 입력으로서 플러커 카메라 조건을 요구하기 때문에, 미리 훈련된 VAE 방법은 포징되지 않은 데이터 세트에 직접 적용될 수 없다. 그러나 현재 방법은 여전히 Objaverse와 같은 기존 고품질 포즈 3D 데이터 세트에서 예상보다 낮은 성능을 발휘한다는 점을 고려할 때 현재로서는 연구 문제가 아니라고 생각한다.\n' +
      '\n' +
      '##3D 확산 한계\n' +
      '\n' +
      '오바버스에서 작동하는 초기 3D 확산 모델 중 하나로서, 우리의 방법은 여전히 향후 조사가 필요한 몇 가지 한계를 겪고 있다. **(1)** Objaverse 상의 image-to-3D의 지원. 현재 우리는 77개의 토큰을 조건부 입력으로 사용하는 \\(\\text{CLIP}_{\\text{text}\\) 인코더를 활용한다. 그러나, T2I 모델[69]을 갖는 2D AIGC와 달리, 쉽게 얻을 수 있는 2D 이미지를 제공함으로써 3D 콘텐츠 생성을 크게 단순화할 수 있다. 직관적인 구현은 ShapeNet 3D 확산 설정을 사용하여 최종 정규화된 CLIP 텍스트 임베딩을 확산 조건으로 제공한다. 다만, 도 1의 하반부에 도시된 바와 같다. 주요 제출에서 CLIP 인코더는 낮은 수준의 시각적 세부 사항보다 높은 수준의 의미론을 추출하는 데 더 우수하다. 따라서, 단안 3D 재구성 및 제어가 가능하도록 ControlNet[99]과 같은 보다 정확한 이미지 조절 3D 확산 설계를 통합하는 것은 향후 탐색할 가치가 있다. **(2)** 구성. 현재 본 논문에서 제안하는 방법은 객체 중심 데이터셋에 간단한 자막으로 학습되어 현재 모델은 구성된 3D 생성을 지원하지 않는다. 예를 들어, 프롬프트 "팔가슴이 있는 노란색 플라스틱 의자 2개"는 그림 1에서 볼 수 있듯이 여전히 하나의 의자를 생성한다. 10. **(3)** UV 맵. 학습 기반 방법을 게임 및 영화 산업에 더 잘 통합하기 위해서는 고품질 UV 텍스처 맵이 필요하다. 잠재적인 해결책은 학습된 기하학과 질감 공간을 풀고 조밀한 대응 관계를 통해 UV 공간과의 연결을 구축하는 것이다[44].\n' +
      '\n' +
      '도 11: ** Objayerse에 대한 LN3Diff 단계-1 VAE에 의한 단안 3D 재구성(Uncurated).** Objayerse에 대한 LN3Diff에 의해 단안 재구성된 미경화 샘플을 보여준다. 좌측으로부터 우측으로 입력 영상, 4개의 재구성된 새로운 뷰를 해당 깊이 맵으로 시각화한다. 유물은 빨간색으로 표시되어 있습니다. 확대해 봐\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>