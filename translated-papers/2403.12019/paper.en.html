<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LN3Diff: Scalable Latent Neural Fields Diffusion\n' +
      '\n' +
      'for Speedy 3D Generation\n' +
      '\n' +
      'Yushi Lan\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      'Fangzhou Hong\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      'Shuai Yang\n' +
      '\n' +
      '2Wangxuan Institute of Computer Technology, Peking University 2\n' +
      '\n' +
      'Shangchen Zhou\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      'Xuyi Meng\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      'Bo Dai\n' +
      '\n' +
      '3Shanghai AI Laboratory 3\n' +
      '\n' +
      'Xingang Pan\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      'Chen Change Loy\n' +
      '\n' +
      '1S-Lab, Nanyang Technological University, Singapore 1\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called **LN3Dff** to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Dff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks. Video demos can be found on our project webpage: [https://nirvanalan.github.io/projects/ln3diff](https://nirvanalan.github.io/projects/ln3diff).\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      'quality and generic conditional 3D generation. In essence, our method involves training a variational autoencoder [41] (VAE) to compress input images into a lower-dimensional 3D-aware latent space, which is more expressive and flexible compared with pixel-space diffusion [12, 28, 31, 79]. From this space, a 3D-aware transformer-based decoder gradually decodes the latent into a high-capacity 3D neural field. This autoencoding stage is trained amortized with differentiable rendering [84], incorporating novel view supervision for multi-view datasets [8, 11] and adversarial supervision for monocular dataset [36]. Thanks to the high-capacity model design, our method is more data efficient, requiring only two views per instance during training. After training, we leverage the learned 3D latent space for conditional 3D diffusion learning, ensuring effective utilization of the trained model for high-quality 3D generation. Additionally, the pre-trained encoder can amortize the data encoding over incoming data, thus streamlining operations and facilitating efficient 3D diffusion learning for future research, while remaining compatible with advances in 3D representations.\n' +
      '\n' +
      'Specifically, our approach presents a novel 3D-aware architecture tailored for fast and high-quality 3D reconstruction, while maintaining a structured latent space. We employ a convolutional tokenizer to encode the input image into a _KL_-regularized 3D latent space, leveraging its superior perceptual compression ability [17]. Additionally, we utilize transformers [13, 61] to enable flexible 3D-aware attention across 3D tokens in the latent space. This approach enhances efficient information flow in the 3D space, thereby promoting coherent geometry reconstruction. Finally, we up-sample the 3D latent and apply differentiable rendering for image-space supervision, thereby making our method a self-supervised 3D learner [78].\n' +
      '\n' +
      'In essence, our work proposes a 3D-representation-agnostic pipeline for building generic high-quality 3D generative models, and provides opportunities to resolve a series of downstream 3D vision and graphics tasks. The main contributions can be summarized as follows:\n' +
      '\n' +
      '* We propose a novel 3D-aware reconstruction model that achieves high-quality 3D data encoding in an amortized manner, marking a significant advancement in 3D diffusion models.\n' +
      '* Learning the compact latent space, our model demonstrates state-of-the-art 3D generation performance on the ShapeNet benchmark [8], surpassing both GAN-based and 3D diffusion-based approaches.\n' +
      '* We showcase superior performance in monocular 3D reconstruction and conditional 3D generation on ShapeNet, FFHQ, and Objaverse datasets, with the fastest inference speed, _e.g._, \\(3\\times\\) faster against existing latent-free 3D diffusion methods [1].\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**3D-aware GANs.** Generative Adversarial Network [20] has shown promising results in generating photorealistic images [3, 37, 38] and inspired researchers to put efforts on 3D aware generation [25, 54, 59]. Motivated by the recent success of neural rendering [50, 52, 60], researchers introduce 3D inductive bias into the generation task [5, 74] and show impressive 3D-awareness synthesis through a hybrid design [6, 22, 29, 56, 58], making it applicable to a series of downstream applications [80, 81, 98, 44]. However, GAN-based methods suffer from mode collapse [86] and fail to model datasets with larger scale and diversity [12]. Besides, 3D reconstruction and editing of GANs require elaborately designed inversion algorithm [45].\n' +
      '\n' +
      '**3D-aware Diffusion Models.** The unprecedented success of the 2D diffusion model [28, 79] has inspired researchers to apply this technique in the context of 3D with several strategies. DreamFusion [92, 64, 32] inspired 3D generation by distilling 2D diffusion model, but suffers from expensive per-scene optimization, mode collapse, and Janus problem. Some approaches propose to learn the 3D prior and rendering process in a 2D manner [7, 47, 85]. Though photo-realistic, these methods intrinsically lack view consistency and cannot extract the underlying 3D of the synthesized scene. A more canonical 3D diffusion pipeline follows a two-stage training paradigm: after pre-training an auto-decoder as the first stage with multi-view images [14, 34, 46, 53, 76, 91], a group of 3D latent codes will serve as the training corpus for diffusion training in the second stage. However, the auto-decoding stage requires training a tiny shared decoder, which leads to an unclean latent space with limited scalability. Besides, the latent is usually heavy, _e.g._, \\(256\\times 256\\times 96\\)[91], which hinders efficient diffusion learning [31].\n' +
      '\n' +
      'Prior works RenderDiffusion [1, 35] and concurrent DMV3D [95, 30] propose a latent-free 3D diffusion by integrating rendering process in the diffusion sampling. Though straightforward, this design involves time-consuming volume rendering in each denoising step, which significantly slows down the sampling speed. SSDNeRF [9] proposes to jointly learn 3D reconstruction and diffusion with an auto-decoder pipeline. However, it requires a sophisticated training schedule and only demonstrates the performance over a single-category unconditional generation. Comparatively, the proposed LN3Dff trains 3D diffusion on the compressed latent space where no rendering operations are required. We validate in Sec. 4 that our method achieves better performance on both 3D generation and monocular 3D reconstruction with 3 times faster speed. Besides, we demonstrate conditional 3D generation over generic, diverse 3D datasets, while RenderDiffusion and SSDNeRF focus on unconditional generation over simple classes.\n' +
      '\n' +
      '**Generalized 3D Reconstruction and View Synthesis.** To circumvent the per-scene optimization of NeRF, researchers propose to learn a prior model [96, 71, 90] through image-based rendering. However, these methods are designed for view synthesis and lack explicit 3D representations. LoLNeRF [68] learns a prior through auto decoding, but is limited to the simple category-specific settings. Moreover, these methods are designed for view synthesis and fail to generate new 3D objects. VQ3D [72] adapts the generalized reconstruction pipeline to 3D generative models. However, they adopt a 2D architecture and work with autoregressive modeling over 1d latent space, which ignores much of the inherent 3D structure. NeRF-VAE [42] directly models 3D likelihood with VAE posterior, but is limited to simple 3D scenes by the deficient capacity of VAE.\n' +
      '\n' +
      '## 3 Scalable Latent Neural Fields Diffusion\n' +
      '\n' +
      'This section introduces our latent 3D diffusion model, which learns efficient diffusion prior over the compressed latent space by a dedicated variational autoencoder. Specifically, the goal of training is to learn the encoder \\(\\mathcal{E}_{\\mathbf{\\phi}}\\) to map a 2D image \\(x\\) to a latent code \\(\\mathbf{z}\\), denoiser \\(\\mathbf{\\epsilon}_{\\mathbf{\\theta}}(\\mathbf{z}_{t},t)\\) to denoise the noisy latent code \\(\\mathbf{z}_{t}\\) given diffusion time step \\(t\\), and decoder \\(\\mathcal{D}_{\\mathbf{\\psi}}\\) (including a Transformer \\(\\mathcal{D}_{T}\\) and an Upsampler \\(\\mathcal{D}_{U}\\)) to map \\(\\mathbf{z}_{0}\\) to the 3D tri-plane \\(\\widetilde{\\mathcal{X}}\\) of \\(x\\), respectively.\n' +
      '\n' +
      'Such design offers several advantages: (1) By explicitly separating the 3D data compression and diffusion stage, we avoid representation-specific 3D diffusion design [101, 34, 53, 76, 1] and achieve 3D representation/rendering-agnostic diffusion which can be applied to any neural rendering techniques. (2) By leaving the high-dimensional 3D space, we reuse the well-studied U-Net architecture [70] for computationally efficient learning and achieve better sampling performance with faster speed. (3) The trained 3D compression model in the first stage serves as an efficient and general-purpose 3D tokenizer, whose latent space can be easily re-used over downstream applications or extended to new datasets [97].\n' +
      '\n' +
      'In the following subsections, we first discuss the compressive stage with a detailed framework design in Sec. 3.1. Based on that, we introduce the 3D diffusion generation stage in Sec. 3.2 and present the condition injection in Sec. 3.3. The method overview is shown in Fig. 2.\n' +
      '\n' +
      'Figure 2: **Pipeline of LN3Diff.** In the 3D latent space learning stage, a convolutional encoder \\(\\mathcal{E}_{\\mathbf{\\phi}}\\) encodes a monocular input image into the KL-regularized latent space. The encoded 3D latent is further decoded by a 3D-aware DiT transformer \\(\\mathcal{D}_{T}\\), in which we perform self-plane attention and cross-plane attention. The transformer-decode latent is up-sampled by a convolutional upsampler \\(\\mathcal{D}_{U}\\) towards a high-res tri-plane for rendering supervisions. In the next stage, we perform conditional diffusion learning over the compact latent space.\n' +
      '\n' +
      '### Perceptual 3D Latent Compression\n' +
      '\n' +
      'As analyzed in Sec. 1, directly leveraging neural fields for diffusion training hinders model scalability and performance. Inspired by previous work [17, 69], we propose to compress the input image(s) into a compact 3D latent space. Though this paradigm is well-adopted in the image domain [17, 69] with similar trials in specific 3D tasks [4, 45, 51, 72], we, for the first time, demonstrate that a high-quality compression model is feasible, whose latent space serves as a compact proxy for efficient diffusion learning.\n' +
      '\n' +
      '**Encoder.** Given an image \\(x\\in\\mathbb{R}^{H\\times W\\times 3}\\), LN3Diff adopts a convolutional encoder \\(\\mathcal{E}_{\\mathbf{\\phi}}\\) to encode \\(x\\) into a latent representation \\(\\mathbf{z}\\sim\\mathcal{E}_{\\mathbf{\\phi}}(x)\\). To inject camera condition, we include Plucker coordinates \\(\\mathbf{r}=(\\mathbf{d},\\mathbf{p}\\times\\mathbf{d})\\in\\mathbb{R}^{6}\\) as part of the input [77], where \\(\\mathbf{d}\\) is the normalized ray direction, \\(\\mathbf{p}\\) is the camera origin and \\(\\times\\) denotes the cross product.\n' +
      '\n' +
      'Unlike existing works [17, 72] that operate on 1D order latent and ignore the internal structure, we choose to output 3D latent \\(\\mathbf{z}\\in\\mathbb{R}^{h\\times w\\times d\\times c}\\) to facilitate 3D-aware operations, where \\(h=H/f,w=W/f\\) are the spatial resolution with down-sample factor \\(f\\), and \\(d\\) denotes the 3D dimension. Here we set \\(f=8\\) and \\(d=3\\) to make \\(\\mathbf{z}\\in\\mathbb{R}^{h\\times w\\times 3\\times c}\\) a tri-latent, which is similar to tri-plane [6, 62] but in the compact 3D latent space. We further impose _KL-reg_[41] to encourage a well-structured latent space to facilitate diffusion training [69, 89].\n' +
      '\n' +
      '**Decoder Transformer.** The decoder aims to decode the compact 3D codes \\(\\mathbf{z}\\) for high-quality 3D reconstruction. Existing image-to-3D methods [4, 6, 21, 45] adopt convolution as the building block, which lacks 3D-aware operations and impede information flow in the 3D space. Here, we adopt ViT [13, 61] as the decoder backbone due to its flexibility and effectiveness. Inspired by Rodin [91], we made the following renovation to the raw ViT decoder to encourage 3D inductive bias and avoid the mixing of uncorrelated 3D features: (1) _Self-plane Attention Block_. Given the input \\(\\mathbf{z}\\in\\mathbb{R}^{l\\times 3\\times c}\\) where \\(l=h\\times w\\) is the sequence length, we treat each of the three latent planes as a data point and conduct self-attention within itself. This operation is efficient and encourages local feature aggregation. (2) _Cross-plane Attention Block_. To further encourage 3D inductive bias, we roll out \\(\\mathbf{z}\\) as a long sequence \\(l\\times 3\\times c\\to 3l\\times c\\) to conduct attention across planes, so that all tokens in the latent space could attend to each other. In this way, we encourage global information flow for more coherent 3D reconstruction and generation. Compared to Rodin, our design is fully attention-based and naturally supports parallel computing without the expensive axis pooling aggregation.\n' +
      '\n' +
      'Empirically, we observe that utilizing DiT [61] block and injecting the latent \\(\\mathbf{z}\\) as conditions yields better performance compared to the ViT [13, 57] block, which takes the latent \\(\\mathbf{z}_{0}\\) as the regular input. Specifically, the adaptive layer norm (adaLN) layer [61] fuses the input latent \\(\\mathbf{z}\\) with the learnable positional encoding for aforementioned attention operations. Moreover, we interleave the aforementioned two types of attention layers to make sure the overall parameters count consistent with the pre-defined DiT length, ensuring efficient training and inference. As all operations are defined in the token space, the decoder achieves efficient computation compared with Rodin [91] while promoting 3D priors.\n' +
      '\n' +
      '**Decoder Upsampler.** After all the attention operations, we obtain the tokens from the last transformer layer \\(\\widetilde{\\mathbf{z}}\\) as the output. The context-rich tokens are reshaped back into spatial domain [24] and up-sampled by a convolutional decoder to the final tri-plane representation with shape \\(\\hat{H}\\times\\hat{W}\\times 3C\\). Here, we simply adopt a lighter version of the convolutional decoder for efficient upsampling, where the three spatial latent of \\(\\widetilde{\\mathbf{z}}\\) are processed in parallel.\n' +
      '\n' +
      '**Learning a Perceptually Rich and Intact 3D Latent Space.** Adversarial learning [20] has been widely applied in learning a compact and perceptually rich latent space [17, 69]. In the 3D domain, the adversarial loss can also encourage correct 3D geometry when novel-view reconstruction supervisions are inapplicable [5, 72, 39], _e.g._, the monocular dataset such as FFHQ [36]. Inspired by previous research [39, 72], we leverage adversarial loss to bypass this issue in our compression stage. Specifically, we impose an input-view discriminator to perceptually-reasonable input view reconstruction, and an auxiliary novel-view discriminator to distinguish the rendered images between the input views and novel views. We observe that if asking the novel-view discriminator to differentiate novel-view renderings and real images instead, the reconstruction model will suffer from _posterior collapse_[48], which outputs input-irrelevant but high-fidelity results to fool the novel-view discriminator. This phenomenon has also been observed by [39].\n' +
      '\n' +
      '**Training.** After the decoder \\(\\mathcal{D}_{\\mathbf{\\psi}}\\) decodes a high-resolution neural fields \\(\\hat{\\mathbf{z}_{0}}\\) from the latent, we have \\(\\hat{x}=\\mathtt{R}(\\widetilde{\\mathcal{X}})=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi }}(\\mathbf{z}))=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi}}(\\mathcal{E}_{\\mathbf{\\phi}}(x)))\\), where \\(\\mathtt{R}\\) stands for differentiable rendering [84] and we take \\(\\mathcal{D}_{\\mathbf{\\psi}}(\\mathbf{z})=\\mathtt{R}(\\mathcal{D}_{\\mathbf{\\psi}}( \\mathbf{z}))\\) for brevity. Here, we choose \\(\\widetilde{\\mathcal{X}}\\) as tri-plane [6, 62] and \\(\\mathtt{R}\\) as volume rendering [52] for experiments. Note that our compression model is 3D representations/rendering agnostic and new neural rendering techniques [40] can be easily integrated by alternating the decoder architecture [82].\n' +
      '\n' +
      'The final training objective reads as\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathbf{\\phi},\\mathbf{\\psi})=\\mathcal{L}_{\\text{render}}+\\lambda_{\\text{ geo}}\\mathcal{L}_{\\text{geo}}+\\lambda_{\\text{kl}}\\mathcal{L}_{\\text{KL}}+ \\lambda_{\\text{GAN}}\\mathcal{L}_{\\text{GAN}} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathcal{L}_{\\text{render}}\\) is a mixture of the \\(L_{2}\\) and perceptual loss [100], \\(\\mathcal{L}_{\\text{reg}}\\) encourages smooth geometry [93], \\(\\mathcal{L}_{\\text{KL}}\\) is the _KL-reg_ loss to encourage a structured latent space [69], and \\(\\mathcal{L}_{\\text{GAN}}\\) improves perceptual quality and enforces correct geometry for monocular datasets.\n' +
      '\n' +
      'For category-specific datasets such as ShapeNet [8], we only supervise _one_ novel view, which already yields good enough performance. For category-free datasets with diverse shape variations, _e.g._, Objavverse [11], we supervise _three_ novel views. Our method is more data-efficient compared with the existing state-of-the-art 3D diffusion method [9, 53], which requires 50 views to converge. The implementation details are included in the supplementary material.\n' +
      '\n' +
      '### Latent Diffusion and Denoising\n' +
      '\n' +
      '**Latent Diffusion Models.** LDM [69, 89] is designed to acquire a prior distribution \\(p_{\\mathbf{\\theta}}(\\mathbf{z}_{0})\\) within the perceptual latent space, whose training data is thelatent obtained online from the trained \\(\\mathcal{E}_{\\mathbf{\\phi}}\\). Here, we use the score-based latent diffusion model [89], which is the continuous derivation of DDPM variational objective [28]. Specifically, the denoiser \\(\\mathbf{\\epsilon}_{\\theta}\\) parameterizes the score function score [79] as \\(\\nabla_{\\mathbf{z}_{t}}\\log p(\\mathbf{z}_{t}):=-\\mathbf{\\epsilon}_{\\theta}(\\mathbf{ z}_{t},t)/\\sigma_{t}\\), with continuous time sequence \\(t\\). By training to predict a denoised variant of the noisy input \\(\\mathbf{z}_{t}\\), \\(\\mathbf{\\epsilon}_{\\theta}\\) gradually learns to denoise from a standard Normal prior \\(\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\) by solving a reverse SDE [28].\n' +
      '\n' +
      'Following LSGM [89], we formulate the learned prior at time \\(t\\) geometric mixing \\(\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t):=\\sigma_{t}(1-\\mathbf{\\alpha})\\odot \\mathbf{z}_{t}+\\mathbf{\\alpha}\\odot\\mathbf{\\epsilon}_{\\theta}^{\\prime}(\\mathbf{z}_{t},t)\\), where \\(\\mathbf{\\epsilon}_{\\theta}^{\\prime}(\\mathbf{z}_{t},t)\\) is the denoiser output and \\(\\alpha\\in[0,1]\\) is a learnable scalar coefficient. Intuitively, this formulation can bring the denoiser input closer to a standard Normal distribution, over where the reverse SDE can be solved faster. Similarly, Stable Diffusion [69, 63] also scales the input latent by a factor to maintain a unit variance, which is pre-calculated on the billion-level dataset [73]. The training objective reads as\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{diff}}=\\mathbb{E}_{\\mathbf{\\varepsilon}_{\\mathbf{\\phi}}(x),\\mathbf{ \\epsilon}\\sim\\mathcal{N}(0,1),t}\\Big{[}\\frac{w_{t}}{2}\\|\\mathbf{\\epsilon}-\\mathbf{ \\epsilon}_{\\theta}(\\mathbf{z}_{t},t)\\|_{2}^{2}\\Big{]}\\,, \\tag{2}\\]\n' +
      '\n' +
      'where \\(t\\sim\\mathcal{U}[0,1]\\) and \\(w_{t}\\) is an empirical time-dependent weighting function.\n' +
      '\n' +
      'The denoiser \\(\\mathbf{\\epsilon}_{\\theta}\\) is realized by a time-dependent U-Net [69]. During training, we obtain \\(\\mathbf{z}_{0}\\) online from the fixed \\(\\mathcal{E}_{\\mathbf{\\phi}}\\), roll-out the tri-latent \\(h\\times w\\times 3\\times c\\to h\\times(3w)\\times c\\), and add time-dependent noise to get \\(\\mathbf{z}_{t}\\). Here, we choose the importance sampling schedule [89] with \\(velocity\\)[49] parameterization, which yields more stable behavior against \\(\\epsilon\\) parameterization for diffusion learning. After training, the denoised samples can be decoded to 3D neural field (_i.e._, tri-plane here) with a single forward pass through \\(\\mathcal{D}_{\\mathbf{\\psi}}\\), on which neural rendering can be applied.\n' +
      '\n' +
      '### Conditioning Mechanisms\n' +
      '\n' +
      'Compared with the existing approach that focuses on category-specific unconditional 3D diffusion model [1], we propose to inject CLIP embeddings [67] into the latent 3D diffusion model to support image/text-conditioned 3D generation. Given input condition \\(\\mathbf{y}\\), the diffusion model formulates the conditional distribution \\(p(\\mathbf{z}|\\mathbf{y})\\) with \\(\\mathbf{\\epsilon}_{\\theta}(\\mathbf{z}_{t},t,\\mathbf{y})\\). The inputs \\(\\mathbf{y}\\) can be text captions for datasets like Objaverse, or images for general datasets like ShapeNet and FFHQ.\n' +
      '\n' +
      '**Text Conditioning.** For datasets with text caption, we follow Stable Diffusion [69] to directly leverage the CLIP text encoder CLIP\\({}_{T}\\) to encode the text caption as the conditions. All the output tokens \\(77\\times 768\\) are used and injected to U-Net denoiser with cross attention block.\n' +
      '\n' +
      '**Image Conditioning.** For datasets with images only, we encode the input image \\(x\\) corresponding to the latent code \\(\\mathbf{z}_{0}\\) using CLIP image encoder CLIP\\({}_{I}\\) and adopt the output embedding as the condition. To support both image and text conditions, we re-scale the image latent code with a pre-calculated factor to match the scale of the text latent.\n' +
      '\n' +
      '**Classifier-free Guidance.** We adopt _classifier-free guidance_[27] for latent conditioning to support both conditional and unconditional generation. During diffusion model training, we randomly zero out the corresponding conditioninglatent embeddings with \\(15\\%\\) probability to jointly train the unconditional and conditional settings. During sampling, we perform a linear combination of the conditional and unconditional score estimates:\n' +
      '\n' +
      '\\[\\hat{\\mathbf{\\epsilon}}_{\\theta}(\\mathbf{z}_{t},\\mathbf{\\tau}_{\\theta}(y))=s\\mathbf{\\epsilon }_{\\theta}(\\mathbf{z}_{t},\\mathbf{\\tau}_{\\theta}(y))+(1-s)\\mathbf{\\epsilon}_{\\theta}( \\mathbf{z}_{t}) \\tag{3}\\]\n' +
      '\n' +
      'where \\(s\\) is the guidance scale to control the mixing strength to balance sampling diversity and quality.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**Datasets.** Following most previous work, we use ShapeNet [8] to benchmark the 3D generation performance. We use the Car, Chair and Plane category with the train split from SRN [78], giving \\(2151\\), \\(2612\\), and \\(3033\\) instances correspondingly. Moreover, to evaluate the performance over diverse high-quality 3D datasets, we also include the experiments over Objaverse [11], which is the largest 3D dataset with challenging categories and complicated geometry. We use the renderings provided by G-buffer Objaverse [66] and choose a high-quality subset with around \\(35K\\) 3D instances with \\(1400K\\) multi-view renderings for experiments.\n' +
      '\n' +
      '**Training Details.** We adopt a monocular image with \\(H=W=256\\) as the input, down-sample factor \\(f=8\\), tri-plane size \\(\\hat{H}=\\hat{W}=128\\) with \\(C=32\\) and target rendering size \\(128\\times 128\\). For efficiency, we impose supervision over \\(64\\times 64\\) randomly cropped patches. For adversarial loss, we use DINO [57] in vision-aided GAN [43] with non-saturating GAN loss [23] for discriminator training. For conditional diffusion training, we use the CLIP image embedding for ShapeNet and FFHQ, and CLIP text embedding from the official text caption for Objaverse. Both the autoencoding model and diffusion model are trained for \\(500K\\) iterations, which take around 7 days with 8 A100 GPUs in total.\n' +
      '\n' +
      '**Metrics.** Following prior work [9, 19, 53, 76], we adopt both 2D and 3D metrics to benchmark the generation performance: Frechet Inception Distance (FID@50K) [26] and Kernel Inception Distance (KID@50K) [2] to evaluate 2D renderings, as well as Coverage Score (COV) and Minimum Matching Distance (MMD) to benchmark 3D geometry. We calculate all metrics under \\(128\\times 128\\) for fair comparisons across all baselines.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      'In this section, we compare our method with both state-of-the-art GAN-based methods: EG3D [6], GET3D [19] as well as recent diffusion-based methods: DiffRF [53], RenderDiffusion [1] and SSDNeRF [9]. Since LN3Diff only leverages \\(v=2\\) for ShapeNet experiments, for SSDNeRF, we include both the official 50-views trained SSDNeRF\\({}_{\\text{v=50}}\\) version as well as the reproduced SSDNeRF\\({}_{\\text{v=3}}\\) for fair comparison. We find SSDNeRF fails to converge with v=2. We set the guidance scale in Eq. (3) to \\(s=0\\) for unconditional generation, and \\(s=6.5\\) for all conditional generation sampling.\n' +
      '\n' +
      '**Unconditional Generation on ShapeNet.** To evaluate our methods against existing 3D generation methods, we include the quantitative and qualitative results for unconditional single-category 3D generation on ShapeNet in Tab. 1 and Fig. 3. We evaluate all baseline 3D diffusion methods with 250 DDIM steps and GAN-based baselines with \\(psi=0.7\\). For FID/KID evaluation, we re-train the baselines and calculate the metrics using a fixed upper-sphere ellipsoid camera trajectory [78] across all datasets. For COV/MMD evaluation, we randomly sample 4096 points around the extracted sampled mesh and ground truth mesh surface and adopt Chamfer Distance for evaluation.\n' +
      '\n' +
      'As shown in Tab. 1, LN3Diff achieves quantitatively better performance against all GAN-based baselines in matters of both rendering quality and 3D coverage. Fig. 3 further demonstrates that GAN-based methods suffer greatly\n' +
      '\n' +
      'Figure 3: **ShapeNet Unconditional Generation.** We showcase four samples for each method. Zoom in for the best view.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'method with \\(v=2\\) shows comparative performance against SSDNeRF\\({}_{\\text{v}=3}\\) on the ShapeNet Chair and even better performance on the remaining datasets.\n' +
      '\n' +
      '**Conditional 3D Generation.** Conditional 3D generation has the potential to streamline the 3D modeling process in both the gaming and film industries. As visualized in Fig. 4, we present our conditional generation performance on the ShapeNet dataset, where either text or image serves as the input prompt. Visually inspected, our method demonstrates promising performance in conditional generation, closely aligning the generated outputs with the input conditions. For the image-conditioned generation, our method yields semantically similar samples while maintaining diversity.\n' +
      '\n' +
      'We additionally demonstrate the text-conditioned generation of Objavorse in Fig. 5 and Tab. 2. As can be seen, the diffusion model trained over LN3Diff\'s latent space empowers high-quality 3D generation over generic 3D datasets. This\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & VIT-B/32 & VIT-L/14 \\\\ \\hline Point-E [55] & 26.35 & 21.40 \\\\ Shape-E [34] & 27.84 & 25.84 \\\\ Ours & **29.12** & **27.80** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Quantitative Metrics on Text-to-3D.** The proposed method outperforms Point-E and Shape-E on CLIP scores over two different backbones.\n' +
      '\n' +
      'Figure 5: **Objavverse Conditional Generation Given Text Prompt.** For each prompt, we showcase two samples. Zoom in for the best view.\n' +
      '\n' +
      'ability is unprecedented by existing 3D diffusion baselines and makes a step towards highly-controllable 3D generation [99]. Qualitative comparison against Shape-E and Point-E can be found in the supplementary.\n' +
      '\n' +
      'Moreover, we include the comparison of our method against RenderDiffusion, the only 3D diffusion method that supports 3D generation over FFHQ. As shown in the lower part in Fig. 6, beyond view-consistent 3D generation, our method further supports conditional 3D generation at \\(128\\times 128\\) resolution, while RenderDiffusion is limited to \\(64\\times 64\\) resolution due to the expensive volume rendering integrated in its diffusion training. Quantitatively, our method achieves an FID score of 36.6, compared to 59.3 by RenderDiffusion.\n' +
      '\n' +
      '**Monocular 3D Reconstruction.** Beyond the samples shown in Fig. 1, we also include monocular reconstruction results over FFHQ datasets in the upper half of Fig. 6 and compare against RenderDiffusion. As can be seen, our method demonstrates high fidelity and preserves semantic details even in self-occluded images. However, the novel view generated by RenderDiffusion appears blurry and misses semantic components that are not visible in the input view, such as the leg of the eyeglass.\n' +
      '\n' +
      '### Ablation Study and Analysis\n' +
      '\n' +
      'In Tab. 3, we benchmark each component of our auto-encoding architecture over a subset of Objaverse with \\(7K\\) instances and record the PSNR at \\(100K\\) iterations.\n' +
      '\n' +
      'Figure 6: **FFHQ Monocular Reconstruction (upper half) and 3D Generation (lower half). For monocular reconstruction, we test our method with hold-out test set and visualize the input-view and novel-view. Compared to baseline, our method shows consistently better performance on both reconstruction and generation.**\n' +
      '\n' +
      'Each component introduces consistent improvements with negligible parameter increases.\n' +
      '\n' +
      'Furthermore, In Tab. 4, we showcase the sampling speed and latent space size comparison. By performing on the compact latent space, our method achieves the fastest sampling while keeping the best generation performance. Note that though RenderDiffusion follows a latent-free design, its intermediate 3D neural field has a shape of \\(256^{2}\\times 96\\) and hinders efficient diffusion training.\n' +
      '\n' +
      '## 5 Conclusion and Discussions\n' +
      '\n' +
      'In this work, we present a new paradigm of 3D generative model by learning the diffusion model over a compact 3D-aware latent space. A dedicated variational autoencoder encodes monocular input image into a low-dim structured latent space, where conditional diffusion learning can be efficiently performed. We achieve state-of-the-art performance over ShapeNet, and demonstrate our method over generic category-free Objaverse 3D datasets. Our work holds the potential to facilitate a series of downstream applications in 3D vision and graphics tasks.\n' +
      '\n' +
      '**Limitations and Future Work.** Our method comes with some limitations unresolved. In terms of reconstruction, we observe that monocular input is not enough for reconstructing challenging 3D scenes, suggesting that a multi-view encoder is required for better performance. Moreover, volume rendering is memory-consuming. Extending our decoder to more efficient 3D representations such as 3DGS [40] is worth investigating. As for diffusion learning, leveraging a more advanced architecture such as DiT [61] may enable more flexible 3D-aware operations. Moreover, enabling reconstruction-diffusion joint training [89, 9] may further accelerate the convergence. Besides, adding more real-world data such as MVImageNet [97] and more control conditions [99] is also worth exploring. Overall, we believe our method is a step towards a general 3D diffusion model and can inspire future research in this direction.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & V100-sec & Latent Size \\\\ \\hline Get3D/EG3D & \\textless{}0.5 & 256 \\\\ \\hline SSDNeRF & 8.1 & \\(128^{2}\\times 18\\) \\\\ RenderDiffusion & 15.8 & - \\\\ DiffRF & 18.7 & \\(32^{3}\\times 4\\) \\\\ LN3D\\({}_{\\text{IF}\\text{uncond}}\\) & **5.7** & \\(32^{2}\\times 12\\) \\\\ LN3D\\({}_{\\text{IF}\\text{cfg}}\\) & **7.5** & \\(32^{2}\\times 12\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Diffusion Sampling Speed and Latent Size**. We provide the sampling time per instance evaluated on 1 V100, along with the latent size. Our method achieves faster sampling speed while maintaining superior generation performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Design & PSNR@100K \\\\ \\hline\n' +
      '2D Conv Baseline & 17.46 \\\\ \\hline + ViT Block & 18.92 \\\\ ViT Block \\(\\rightarrow\\) DiT Block & 20.61 \\\\ + Plucker Embedding & 21.29 \\\\ + Cross-Plane Attention & 21.70 \\\\ + Self-Plane Attention & **21.95** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Ablation of Reconstruction Arch Design.** We ablate the design of our auto-encoding architecture. Each component contributes to a consistent gain in the reconstruction performance, indicating an improvement in the modeling capacity.\n' +
      '\n' +
      '**Potential Negative Societal Impacts**. The entity relational composition capabilities of LN3Diff could be applied maliciously on real human figures. Additional potential impacts are discussed in the Supplmentary File in depth.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Anciukevicius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra, N.J., Guerrero, P.: RenderDiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In: CVPR (2023) 3, 4, 5, 8, 9, 11\n' +
      '* [2] Binkowski, M., Sutherland, D.J., Arbel, M., Gretton, A.: Demystifying MMD GANs. In: ICLR (2018) 9\n' +
      '* [3] Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high fidelity natural image synthesis. In: ICLR (2019) 3\n' +
      '* [4] Cai, S., Obukhov, A., Dai, D., Van Gool, L.: Pix2NeRF: Unsupervised Conditional p-GAN for Single Image to Neural Radiance Fields Translation. In: CVPR (2022) 6, 26\n' +
      '* [5] Chan, E., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: Pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis. In: CVPR (2021) 4, 7\n' +
      '* [6] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient geometry-aware 3D generative adversarial networks. In: CVPR (2022) 2, 4, 6, 7, 9, 11\n' +
      '* [7] Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala, M., Mello, S.D., Karras, T., Wetzstein, G.: GeNVS: Generative novel view synthesis with 3D-aware diffusion models. In: arXiv (2023) 4\n' +
      '* [8] Chang, A.X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: ShapeNet: An Information-Rich 3D Model Repository. arXiv preprint arXiv:1512.03012 (2015) 3, 7, 9, 26\n' +
      '* [9] Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffusion nerf: A unified approach to 3D generation and reconstruction. In: ICCV (2023) 2, 4, 7, 9, 11, 14\n' +
      '* [10] Contributors, S.: SpConv: Spatially sparse convolution library. [https://github.com/traveller59/spconv](https://github.com/traveller59/spconv) (2022) 2\n' +
      '* [11] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of annotated 3D objects. arXiv preprint arXiv:2212.08051 (2022) 3, 7, 9\n' +
      '* [12] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS (2021) 2, 3, 4, 22, 24\n' +
      '* [13] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021) 3, 6\n' +
      '* [14] Dupont, E., Kim, H., Eslami, S.M.A., Rezende, D.J., Rosenbaum, D.: From data to functa: Your data point is a function and you can treat it like one. In: ICML (2022) 4\n' +
      '* [15] Dupont, E., Martin, M.B., Colburn, A., Sankar, A., Susskind, J., Shan, Q.: Equivariant neural rendering. In: International Conference on Machine Learning. pp. 2761-2770. PMLR (2020) 26\n' +
      '* 1210 (2018) 26\n' +
      '* [17] Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthesis. In: CVPR (2021)\n' +
      '* [18] Fridovich-Keil and Yu, Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenoxels: Radiance fields without neural networks. In: CVPR (2022)\n' +
      '* [19] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3D: A generative model of high quality 3D textured shapes learned from images. In: NeurIPS (2022)\n' +
      '* [20] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)\n' +
      '* [21] Gu, J., Gao, Q., Zhai, S., Chen, B., Liu, L., Susskind, J.: Learning controllable 3D diffusion models from single-view images. arXiv preprint arXiv:2304.06700 (2023)\n' +
      '* [22] Gu, J., Liu, L., Wang, P., Theobalt, C.: StyleNeRF: A style-based 3D-aware generator for high-resolution image synthesis. In: ICLR (2021)\n' +
      '* [23] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved training of wasserstein GANs. In: NeurIPS (2017)\n' +
      '* [24] He, K., Chen, X., Xie, S., Li, Y., Doll\'ar, P., Girshick, R.B.: Masked autoencoders are scalable vision learners. In: CVPR (2022)\n' +
      '* [25] Henzler, P., Mitra, N.J., Ritschel, T.: Escaping plato\'s cave: 3D shape from adversarial rendering. In: ICCV (2019)\n' +
      '* [26] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS (2017)\n' +
      '* [27] Ho, J.: Classifier-free diffusion guidance. In: NeurIPS (2021)\n' +
      '* [28] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS (2020)\n' +
      '* [29] Hong, F., Chen, Z., Lan, Y., Pan, L., Liu, Z.: EVA3D: Compositional 3D human generation from 2d image collections. In: ICLR (2022)\n' +
      '* [30] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. In: ICLR (2023)\n' +
      '* [31] Hoogeboom, E., Heek, J., Salimans, T.: simple diffusion: End-to-end diffusion for high resolution images. In: ICML (2023)\n' +
      '* [32] Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: CVPR (2022)\n' +
      '* [33] Jang, W., Agapito, L.: Codenerf: Disentangled neural radiance fields for object categories. In: ICCV. pp. 12949-12958 (2021)\n' +
      '* [34] Jun, H., Nichol, A.: Shap-E: Generating conditional 3D implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [35] Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.: Holodiffusion: Training a 3D diffusion model using 2D images. In: CVPR (2023)\n' +
      '* [36] Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for improved quality, stability, and variation. In: ICLR (2018)\n' +
      '* [37] Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: CVPR (2019)\n' +
      '* [38] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of StyleGAN. In: CVPR (2020)\n' +
      '* [39] Kato, H., Harada, T.: Learning view priors for single-view 3D reconstruction. In: CVPR (2019)\n' +
      '* [* [40] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4), 1-14 (2023) 7, 14\n' +
      '* [41] Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv (2013) 3, 6\n' +
      '* [42] Kosiorek, A.R., Strathmann, H., Zoran, D., Moreno, P., Schneider, R., Mokr\'a, S., Rezende, D.J.: NeRF-VAE: A geometry aware 3D scene generative model. ICML (2021) 5\n' +
      '* [43] Kumari, N., Zhang, R., Shechtman, E., Zhu, J.Y.: Ensembling off-the-shelf models for gan training. In: CVPR (2022) 9\n' +
      '* [44] Lan, Y., Loy, C.C., Dai, B.: DDF: Correspondence distillation from nerf-based gan. IJCV (2022) 2, 4, 28\n' +
      '* [45] Lan, Y., Meng, X., Yang, S., Loy, C.C., Dai, B.: E3dge: Self-supervised geometry-aware encoder for style-based 3D gan inversion. In: CVPR (2023) 2, 4, 6\n' +
      '* [46] Lan, Y., Tan, F., Qiu, D., Xu, Q., Genova, K., Huang, Z., Fanello, S., Pandey, R., Funkhouser, T., Loy, C.C., Zhang, Y.: Gaussian3Diff: 3D gaussian diffusion for 3D full head synthesis and editing. arXiv (2023) 2, 4\n' +
      '* [47] Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3D object (2023) 2, 4\n' +
      '* [48] Lucas, J., Tucker, G., Grosse, R.B., Norouzi, M.: Understanding posterior collapse in generative latent variable models. In: ICLR (2019) 7\n' +
      '* [49] Meng, C., Gao, R., Kingma, D.P., Ermon, S., Ho, J., Salimans, T.: On distillation of guided diffusion models. In: CVPR. pp. 14297-14306 (2022) 8\n' +
      '* [50] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy Networks: Learning 3D reconstruction in function space. In: CVPR (2019) 4\n' +
      '* [51] Mi, L., Kundu, A., Ross, D., Dellaert, F., Snavely, N., Fathi, A.: im2nerf: Image to neural radiance field in the wild. In: arXiv (2022) 6\n' +
      '* [52] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020) 2, 4\n' +
      '* [53] Muller, N., Siddiqui, Y., Porzi, L., Bulo, S.R., Kontschieder, P., Niessner, M.: DiffRF: Rendering-guided 3D radiance field diffusion. In: CVPR (2023) 2, 4, 5, 7, 9, 11\n' +
      '* [54] Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.: HoloGAN: Unsupervised Learning of 3D Representations From Natural Images. In: ICCV (2019) 4\n' +
      '* [55] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts (2022) 12, 26\n' +
      '* [56] Niemeyer, M., Geiger, A.: GIRAFFE: Representing scenes as compositional generative neural feature fields. In: CVPR (2021) 4\n' +
      '* [57] Oquab, M., Darceet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.Y., Xu, H., Sharma, V., Li, S.W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: DINOv2: Learning robust visual features without supervision (2023) 6, 9\n' +
      '* [58] Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J.J., Kemelmacher-Shlizerman, I.: StyleSDF: High-resolution 3D-consistent image and geometry generation. In: CVPR (2021) 4\n' +
      '* [59] Pan, X., Dai, B., Liu, Z., Loy, C.C., Luo, P.: Do 2D GANs know 3D shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs. In: ICLR (2021) 4* [60] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning continuous signed distance functions for shape representation. In: CVPR. pp. 165-174 (2019)\n' +
      '* [61] Peebles, W., Xie, S.: Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748 (2022)\n' +
      '* [62] Peng, S., Niemeyer, M., Mescheder, L., Pollefeys, M., Geiger, A.: Convolutional occupancy networks. In: ECCV (2020)\n' +
      '* [63] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: SDXL: Improving latent diffusion models for high-resolution image synthesis. In: arXiv (2023)\n' +
      '* [64] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: DreamFusion: Text-to-3D using 2D diffusion. ICLR (2022)\n' +
      '* [65] Qi, C., Su, H., Mo, K., Guibas, L.: PointNet: Deep learning on point sets for 3D classification and segmentation. arXiv (2016)\n' +
      '* [66] Qiu, L., Chen, G., Gu, X., zuo, Q., Xu, M., Wu, Y., Yuan, W., Dong, Z., Bo, L., Han, X.: Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d. arXiv preprint arXiv:2311.16918 (2023)\n' +
      '* [67] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n' +
      '* [68] Rebain, D., Matthews, M., Yi, K.M., Lagun, D., Tagliasacchi, A.: LOLNeRF: Learn from one look. In: CVPR (2022)\n' +
      '* [69] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022)\n' +
      '* [70] Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional networks for biomedical image segmentation. In: MICCAI (2015)\n' +
      '* [71] Sajjadi, M.S.M., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora, S., Lucic, M., Duckworth, D., Dosovitskiy, A., Uszkoreit, J., Funkhouser, T., Tagliasacchi, A.: Scene Representation Transformer: Geometry-free novel view synthesis through set-latent scene representations. CVPR (2022)\n' +
      '* [72] Sargent, K., Koh, J.Y., Zhang, H., Chang, H., Herrmann, C., Srinivasan, P.P., Wu, J., Sun, D.: VQ3D: Learning a 3D-aware generative model on imagenet. ICCV (2023)\n' +
      '* [73] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAION-5B: An open large-scale dataset for training next generation image-text models. In: arXiv (2022)\n' +
      '* [74] Schwarz, K., Liao, Y., Niemeyer, M., Geiger, A.: GRAF: Generative radiance fields for 3D-aware image synthesis. In: NeurIPS (2020)\n' +
      '* [75] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: a single image to consistent multi-view diffusion base model. In: arXiv (2023)\n' +
      '* [76] Shue, J., Chan, E., Po, R., Ankner, Z., Wu, J., Wetzstein, G.: 3d neural field generation using triplane diffusion. In: CVPR (2022)\n' +
      '* [77] Sitzmann, V., Rezchikov, S., Freeman, W.T., Tenenbaum, J.B., Durand, F.: Light field networks: Neural scene representations with single-evaluation rendering. In: NeurIPS (2021)\n' +
      '* [78] Sitzmann, V., Zollhofer, M., Wetzstein, G.: Scene Representation Networks: Continuous 3D-structure-aware neural scene representations. In: NeurIPS (2019)\n' +
      '* [* [79] Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-based generative modeling through stochastic differential equations. In: ICLR (2021)\n' +
      '* [80] Sun, J., Wang, X., Shi, Y., Wang, L., Wang, J., Liu, Y.: Ide-3d: Interactive disentangled editing for high-resolution 3D-aware portrait synthesis. ACM Transactions on Graphics (TOG) **41**(6), 1-10 (2022)\n' +
      '* [81] Sun, J., Wang, X., Zhang, Y., Li, X., Zhang, Q., Liu, Y., Wang, J.: FENeRF: Face editing in neural radiance fields. In: arXiv (2021)\n' +
      '* [82] Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3D reconstruction. In: arXiv (2023)\n' +
      '* [83] Tatarchenko, M., Dosovitskiy, A., Brox, T.: Multi-view 3d models from single images with a convolutional network (2016)\n' +
      '* [84] Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Xu, Z., Simon, T., Niessner, M., Tretschk, E., Liu, L., Mildenhall, B., Srinivasan, P., Pandey, R., Orts-Escolano, S., Fanello, S., Guo, M.G., Wetzstein, G., y Zhu, J., Theobalt, C., Agrawala, M., Goldman, D.B., Zollhofer, M.: Advances in neural rendering. Computer Graphics Forum **41** (2021)\n' +
      '* [85] Tewari, A., Yin, T., Cazenavette, G., Rezchikov, S., Tenenbaum, J.B., Durand, F., Freeman, W.T., Sitzmann, V.: Diffusion with forward models: Solving stochastic inverse problems without direct supervision. In: NeurIPS (2023)\n' +
      '* [86] Thanh-Tung, H., Tran, T.: Catastrophic forgetting and mode collapse in gans. IJCNN pp. 1-10 (2020)\n' +
      '* [87] Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.: KPConv: Flexible and deformable convolution for point clouds. In: ICCV (2019)\n' +
      '* [88] Trevithick, A., Yang, B.: GRF: Learning a general radiance field for 3D scene representation and rendering. In: ICCV (2021)\n' +
      '* [89] Vahdat, A., Kreis, K., Kautz, J.: Score-based generative modeling in latent space. In: NeurIPS (2021)\n' +
      '* [90] Wang, Q., Wang, Z., Genova, K., Srinivasan, P.P., Zhou, H., Barron, J.T., Martin-Brualla, R., Snavely, N., Funkhouser, T.A.: IBRNet: Learning Multi-View Image-Based Rendering. In: CVPR (2021)\n' +
      '* [91] Wang, T., Zhang, B., Zhang, T., Gu, S., Bao, J., Baltrusaitis, T., Shen, J., Chen, D., Wen, F., Chen, Q., et al.: RODIN: A generative model for sculpting 3D digital avatars using diffusion. In: CVPR (2023)\n' +
      '* [92] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In: NeurIPS (2023)\n' +
      '* [93] Weng, C.Y., Srinivasan, P.P., Curless, B., Kemelmacher-Shlizerman, I.: PersonNeRF: Personalized reconstruction from photo collections. In: CVPR. pp. 524-533 (June 2023)\n' +
      '* [94] Xie, Y., Takikawa, T., Saito, S., Litany, O., Yan, S., Khan, N., Tombari, F., Tompkin, J., Sitzmann, V., Sridhar, S.: Neural fields in visual computing and beyond. Computer Graphics Forum **41** (2021)\n' +
      '* [95] Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wetzstein, G., Xu, Z., Zhang, K.: DMV3D: Denoising multi-view diffusion using 3D large reconstruction model. In: ICLR (2023)\n' +
      '* [96] Yu, A., Ye, V., Tancik, M., Kanazawa, A.: PixelNeRF: Neural radiance fields from one or few images. In: CVPR (2021)\n' +
      '* [97] Yu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Liang, T., Chen, G., Cui, S., Han, X.: MVImgNet: A large-scale dataset of multi-view images. In: CVPR (2023)* [98] Zhang, J., Lan, Y., Yang, S., Hong, F., Wang, Q., Yeo, C.K., Liu, Z., Loy, C.C.: Deformtoon3d: Deformable 3D toonification from neural radiance fields. In: ICCV (2023) 4\n' +
      '* [99] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: ICCV (2023) 2, 13, 14, 27\n' +
      '* [100] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018) 7\n' +
      '* [101] Zhou, L., Du, Y., Wu, J.: 3D shape generation and completion through point-voxel diffusion. In: ICCV (2021)In this supplementary material, we provide additional details regarding the implementations and additional results. We also discuss the limitations of our model.\n' +
      '\n' +
      '**Broader Social Impact.** In this paper, we introduce a new latent 3D diffusion model designed to produce high-quality textures and geometry using a single model. As a result, our approach has the potential to be applied to generating DeepFakes or deceptive 3D assets, facilitating the creation of falsified images or videos. This raises concerns as individuals could exploit such technology with malicious intent, aiming to spread misinformation or tarnish reputations.\n' +
      '\n' +
      '## Appendix 0.A Implementation details\n' +
      '\n' +
      '### Training details\n' +
      '\n' +
      '**Diffusion.** We mainly adopt the diffusion training pipeline implementation from ADM [12], continuous noise schedule from LSGM [89] with the spatial transformer attention implementation from LDM [69]. We list the hyperparameters in Tab. 5.\n' +
      '\n' +
      '**VAE Architecture.** For the convolutional encoder \\(\\mathcal{E}_{\\mathbf{\\phi}}\\), we adopt a lighter version of LDM [69] encoder with channel 64 and 1 residual blocks for efficiency.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Diffusion Model Details & \\\\ \\hline Learning Rate & \\(2e-5\\) \\\\ Batch Size & 96 \\\\ Optimizer & AdamW \\\\ Iterations & 500K \\\\ \\hline U-Net base channels & 320 \\\\ U-Net channel multiplier & 1, 1, 2, 2, 4, 4 \\\\ U-Net res block & 2 \\\\ U-Net attention resolutions & 4,2,1 \\\\ U-Net Use Spatial Transformer & True \\\\ U-Net Learn Sigma & False \\\\ U-Net Spatial Context Dim & 768 \\\\ U-Net attention head channels & 64 \\\\ U-Net pred type & \\(v\\) \\\\ U-Net norm layer type & GroupNorm \\\\ \\hline Noise Schedules & Linear \\\\ CFG Dropout prob & 15\\% \\\\ CLIP Latent Scaling Factor & 18.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Hyperparameters and architecture of diffusion model \\(\\mathbf{\\epsilon}_{\\theta}\\).\n' +
      '\n' +
      'For convolutional upsampler \\(\\mathcal{D}_{U}\\), we further half the channel to 32. All other hyper-parameters remain at their default settings. Regarding the transformer decoder \\(\\mathcal{D}_{T}\\), we employ the DiT-B/2 architecture. The overall saved VAE model takes around 450MiB storage. Note that we trade off a smaller model with faster training speed due to the overall compute limit, and a heavier model would certainly empower better performance [61, 95]. We ignore the plucker camera condition for the ShapeNet and FFHQ dataset, over which we find raw RGB input already yields good enough performance.\n' +
      '\n' +
      '### Data and Baseline Comparison\n' +
      '\n' +
      '**Training data.** For ShapeNet, following GET3D [19], we use the blender to render the multi-view images from 50 viewpoints for all ShapeNet datasets with foreground mask. Those camera points sample from the upper sphere of a ball with a 1.2 radius. For Objaverse, we use a high-quality subset from the pre-processed rendering from G-buffer Objaverse [66] for experiments.\n' +
      '\n' +
      '**Evaluation.** The 2D metrics are calculated between 50k generated images and all available real images. Furthermore, for comparison of the geometrical quality, we sample 4096 points from the surface of 5000 objects and apply the Coverage Score (COV) and Minimum Matching Distance (MMD) using Chamfer Distance (CD) as follows:\n' +
      '\n' +
      '\\[\\begin{split}& CD(X,Y)=\\sum_{x\\in X}\\underset{y\\in Y}{min}||x-y|| _{2}^{2}+\\sum_{y\\in Y}\\underset{x\\in X}{min}||x-y||_{2}^{2},\\\\ & COV(S_{g},S_{r})=\\frac{|\\{\\text{arg min}_{Y\\in S_{r}}CD(X,Y)|X \\in S_{g}\\}|}{|S_{r}|}\\ \\,\\\\ & MMD(S_{g},S_{r})=\\frac{1}{|S_{r}|}\\sum_{Y\\in S_{r}}\\underset{X \\in S_{g}}{min}CD(X,Y)\\end{split} \\tag{4}\\]\n' +
      '\n' +
      'where \\(X\\in S_{g}\\) and \\(Y\\in S_{r}\\) represent the generated shape and reference shape.\n' +
      '\n' +
      'Note that we use 5k generated objects \\(S_{g}\\) and all training shapes \\(S_{r}\\) to calculate COV and MMD. For fairness, we normalize all point clouds by centering in the original and recalling the extent to [-1,1]. Coverage Score aims to evaluate the diversity of the generated samples, and MMD is used for measuring the quality of the generated samples. 2D metrics are evaluated at a resolution of 128 \\(\\times\\) 128. Since the GT data contains intern structures, we only sample the points from the outer surface of the object for results of all methods and ground truth.\n' +
      '\n' +
      'For FID/KID evaluation, since different methods have their unique evaluation settings, we standardize this process by re-rendering each baseline\'s samples using a fixed upper-sphere ellipsoid camera pose trajectory of size 20. With \\(2.5K\\) sampled 3D instances for each method, we recalculate FID@50K/KID@50K, ensuring a fair comparison across all methods.\n' +
      '\n' +
      '**Details about Baselines.** We reproduce EG3D, GET3D, and SSDNeRF on our ShapeNet rendering using their officially released codebases. In the case of RenderDiffusion, we use the code and pre-trained model shared by the author for ShapeNet experiments. Regarding FFHQ dataset, due to the unavailability of the corresponding inference configuration and checkpoint from the authors, we incorporate their unconditional generation and monocular reconstruction results as reported in their paper. For DiffRF, given the absence of the public code, we reproduce their method with Plenoxel [18] and ADM [12].\n' +
      '\n' +
      '## Appendix 0.B More Results\n' +
      '\n' +
      '### More Qualitative 3D Generation Results\n' +
      '\n' +
      'We include more uncurated samples generated by our method on ShapeNet in Fig. 7, and on FFHQ in Fig. 8. For Objaverse, we include its qualitative\n' +
      '\n' +
      'Figure 7: **Unconditional 3D Generation by LN3Diff (Uncurated). We showcase uncurated samples generated by LN3Diff on ShapeNet three categories. We visualize two views for each sample. Better zoom in.**\n' +
      '\n' +
      'Figure 8: **Unconditional 3D Generation by LN3Diff (Uncurated).** We showcase uncurated samples generated by LN3Diff on FFHQ. We visualize two views for each sample along with the extracted depth. Better zoom in.\n' +
      '\n' +
      'Figure 9: **Qualitative Comparison of Text-to-3D** We showcase uncurated samples generated by LN3Diff on ShapeNet three categories. We visualize two views for each sample. Better zoom in.\n' +
      '\n' +
      'evaluation against state-of-the-art generic 3D generative models (Shape-E [34] and Point-E [55]in Fig. 9, along with the quantitative benchmark in Tab. 2 in the main paper. We use CLIP-precision score in DreamField [32] to evaluate the text-3D alignment. As can be seen, LN3Diff shows more geometry and appearance details with higher CLIP scores against Shape-E and Point-E.\n' +
      '\n' +
      '### More Monocular 3D Reconstruction Results\n' +
      '\n' +
      'We further benchmark the generalization ability of our stage-1 monocular 3D reconstruction VAE. For ShapeNet, we include the quantitative evaluation in Tab. 6. Our method achieves a comparable performance with monocular 3D reconstruction baselines. Note that strictly saying, our stage-1 VAE shares a similar setting with Pix2NeRF [4], whose encoder also has a latent space for generative modeling. Other reconstruction-specific methods like PixelNeRF [96] do not have these requirements and can leverage some designs like pixel-aligned features and long-skip connections to further boost the reconstruction performance. We include their performance mainly for reference and leave training the stage-1 VAE model with performance comparable with those state-of-the-art 3D reconstruction models for future work.\n' +
      '\n' +
      'Besides, we visualize LN3Diff\'s stage-1 monocular VAE reconstruction performance over our Objaverse split in Fig. 11. As can be seen, though only\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Method & PSNR \\(\\uparrow\\) & SSIM \\(\\uparrow\\) \\\\ \\hline GRF [88] & 21.25 & 0.86 \\\\ TCO [83] & 21.27 & 0.88 \\\\ dGQN [16] & 21.59 & 0.87 \\\\ ENR [15] & 22.83 & - \\\\ SRN* [78] & 22.89 & 0.89 \\\\ CodeNeRF* [33] & 22.39 & 0.87 \\\\ PixelNeRF [96] & **23.72** & **0.91** \\\\ \\hline Pix2NeRF [4] conditional & 18.14 & 0.84 \\\\ Ours & 20.91 & 0.89 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Quantitative results on ShapeNet-SRN [78, 8] chairs evaluate on \\(128\\times 128\\). Legend: *  requires test time optimization. Note that our stage-1 VAE shares the same setting only with Pix2NeRF [96], which also has an explicit latent space for generative learning. Other baselines are included for reference.\n' +
      '\n' +
      'one view is provided as the input, our monocular VAE reconstruction can yield high-quality and view-consistent 3D reconstruction with a detailed depth map. Quantitatively, the novel-view reconstruction performance over our whole Objaverse dataset achieves an average PSNR of 26.14. This demonstrates that our latent space can be treated as a compact proxy for efficient 3D diffusion training.\n' +
      '\n' +
      '## Appendix 0.C Limitation and Failure Cases\n' +
      '\n' +
      'We have included a brief discussion of limitations in the main submission. Here we include more details along with the visual failure cases for a more in-depth analysis of LN3Diff\'s limitations and future improvement directions.\n' +
      '\n' +
      '### VAE Limitations\n' +
      '\n' +
      'We have demonstrated that using a monocular image as encoder input can achieve high-quality 3D reconstruction. However, we noticed that for some challenging cases with diverse color and geometry details, the monocular encoder leads to blurry artifacts. As labeled in Fig. 11, our method with monocular input may yield floating artifacts over unseen viewpoints. We hypothesize that these artifacts are largely due to the ambiguity of monocular input and the use of regression loss (L2/LPIPS) during training. These observations demonstrate that switching to a multi-view encoder is necessary for better performance.\n' +
      '\n' +
      'Besides, since our VAE requires plucker camera condition as input, the pre-trained VAE method cannot be directly applied to the unposed dataset. However, we believe this is not a research issue at the current time, considering the current methods still perform lower than expected on existing high-quality posed 3D datasets like Objaverse.\n' +
      '\n' +
      '### 3D Diffusion Limitations\n' +
      '\n' +
      'As one of the earliest 3D diffusion models that works on Objaverse, our method still suffers from several limitations that require investigation in the future. **(1)** The support of image-to-3D on Objaverse. Currently, we leverage \\(\\text{CLIP}_{\\text{text}}\\) encoder with the 77 tokens as the conditional input. However, unlike 2D AIGC with T2I models [69], 3D content creation can be greatly simplified by providing easy-to-get 2D images. An intuitive implementation is by using our ShapeNet 3D diffusion setting, which provides the final normalized CLIP text embeddings as the diffusion condition. However, as shown in the lower half of Fig. 4 in the main submission, the CLIP encoder is better at extracting high-level semantics rather than low-level visual details. Therefore, incorporating more accurate image-conditioned 3D diffusion design like ControlNet [99] to enable monocular 3D reconstruction and control is worth exploring in the future. **(2)** Compositionality. Currently, our method is trained on object-centric dataset with simple captions, so the current model does not support composed 3D generation. For example,the prompt "Two yellow plastic chair with armchests" will still yield one chair, as visualized in Fig. 10. **(3)** UV map. To better integrate the learning-based method into the gaming and movie industry, a high-quality UV texture map is required. A potential solution is to disentangle the learned geometry and texture space and build the connection with UV space through dense correspondences [44].\n' +
      '\n' +
      'Figure 11: **Monocular 3D Reconstruction by LN3Diff stage-1 VAE on Objayerse (Uncurated).** We showcase uncurated samples monocular-reconstructed by LN3Diff on Objayerse. From left to right, we visualize the input image, four reconstructed novel views with the corresponding depth maps. Artifacts are labeled in Red. Better zoom in.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>