<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'environments. We present two pilot challenges: "Guessing 0.8 of the Average" and "Survival Auction Game", as show in Figure 2 - both games serve as microcosms of the complex decision-making processes prevalent in dynamic environments, while also being well-defined for providing a test bed for evaluating the LLMs\' dynamic reasoning capabilities.\n' +
      '\n' +
      'Through extensive simulation and evaluation, we observe that conventional reasoning methods, which have been successful in static scenarios, struggle to maintain their performance in the dynamic settings presented by our pilot challenges. Understanding others\' perspectives, thinking patterns and anticipating their actions are crucial for optimal decision-making in dynamic settings. This concept is referred to k-level thinking1(Nagel, 1995; Stahl II Wilson, 1994) - a recursive reasoning process accounting for the level of strategic depth. The existing methods do not employ k-level thinking and fail to anticipate the actions of other participants, leading to suboptimal decision-making.\n' +
      '\n' +
      'Footnote 1: According to cognitive hierarchy theory, k-level thinking involves considering what rivals are likely to do, what they think you will do, and what they believe you think they will do, and so on.\n' +
      '\n' +
      'To overcome these limitations, we introduce a novel reasoning method termed "K-Level Reasoning", which approaches k-level thinking by recursively adopting the perspective of rivals into the decision-making process. The recursive reasoning process allows for a deeper understanding of the strategic landscape and enables LLMs to make more informed decisions that take into account the likely responses of their rivals, achieving much better results in both pilot challenges than traditional reasoning methods.\n' +
      '\n' +
      'We summarize the contributions of this work as follows:\n' +
      '\n' +
      '* We study the dynamic reasoning capabilities of Large Language Models from a game theory perspective and introduce two pilot tasks. Both tasks mirror the complexities of real-world dynamic decision-making and are also well-defined for evaluating LLMs\' dynamic reasoning abilities.\n' +
      '* the "K-Level Reasoning" method. It integrates cognitive hierarchy theory into reasoning process, empowering LLMs to recursively predict and respond to the thoughts and actions of rivals in competitive and dynamic scenarios.\n' +
      '\n' +
      '## 2 Problem Definition\n' +
      '\n' +
      '### Dynamic Reasoning with LLMs\n' +
      '\n' +
      'Dynamic reasoning problems, characterized by multiple individual interactions and the ever-changing environment, present a more complex scenario. As shown in Figure.1, two key aspects define these challenges:\n' +
      '\n' +
      '* **Dynamic Environment**: The environment evolves dynamically, influenced by the decisions and actions of\n' +
      '\n' +
      'Figure 2: Illustration of two dynamic reasoning problems in this paper. **Left:** Guessing 0.8 of the Average. It involves ten-round games, where each player selects a number between 1 to 100. The winner of each round is the player whose chosen number is closest to 80% of the average number picked by all players. This challenge mirrors the dynamics of market predictions, where players must anticipate collective market behavior. **Right:** Survival Auction Game. Players bid in daily auctions to secure scarce living resources, balancing their health and finances to survive a 10-day period, simulating scenarios of resource scarcity and financial decision-making.\n' +
      '\n' +
      ' the individuals.\n' +
      '* **Adaptive Strategies**: Individuals continuously adjust and optimize their strategies in response to environmental changes.\n' +
      '\n' +
      'While similar tasks like strategic reasoning (Bakhtin et al., 2022) also involve multiple participants in a competitive or cooperative contexts, dynamic reasoning emphasizes more on adapting to changes and interplay in the environment.\n' +
      '\n' +
      'We set the initial environment as \\(E^{0}\\). The environment evolves to \\(E^{t+1}\\) in response to the collective decisions \\(d^{t}_{i}\\) of all individuals at step \\(t\\). This transformation reflects the cumulative impact of individuals\' actions. Consequently, individuals aiming to make optimal decisions, denoted as \\(d^{t+1}_{i}\\), must consider the latest environmental state, \\(E^{t+1}\\), while also taking into account the historical and potential future behaviors of others (\\(\\mathbf{D}\\)).\n' +
      '\n' +
      '\\[\\begin{split} d^{t}_{i}&=f(E^{0},\\mathbf{D}^{0} \\Rightarrow E^{1},\\ldots,\\mathbf{D}^{t-1}\\Rightarrow E^{t})\\\\ \\mathbf{D}^{t}&=\\{d^{t}_{i},i\\in N\\}\\\\ E^{t+1}&=\\mathrm{Transfer}(E^{t},\\mathbf{D}^{t})\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'This dynamic interplay underscores the need for LLMs to not only process current data but also to anticipate and adapt to evolving strategies and environmental changes.\n' +
      '\n' +
      '### Pilot Tasks\n' +
      '\n' +
      'To formally study and quantitatively research the dynamic reasoning capabilities of LLMs, we explore two game settings inspired by game theory problems: the Guessing 0.8 of the Average (G0.8A) and the Survival Auction Game (SAG). Both tasks are well-defined and controllable, ensuring a clear test of LLMs\' capabilities. The specific game settings and prompts can be found in Appendix A.\n' +
      '\n' +
      '#### 2.2.1 Guessing 0.8 of the Average (G0.8A)\n' +
      '\n' +
      'The Guessing 0.8 of the Average (Figure.2 Left) is a classic game introduced by Alain Ledoux (Ledoux, 1981). It involves ten-round games, where each player selects a number between 1 to 100. The objective is to select a number that is closest to 80% of the group\'s average choice. The key idea is to guess how others will estimate the average and thus decide the number to submit.\n' +
      '\n' +
      'This idea is also illustrated in the Keynesian Beauty Contest, a contest where participants select the six most attractive faces out of 100, and those whose choices align with the final results win. As original stated in Keynes\'s 1936 work, "The General Theory of Employment, Interest, and Money" (Keynes, 1936), "it is not a case of choosing those [faces] that, to the best of one\'s judgment, are really the prettiest, nor even those that average opinion genuinely thinks the prettiest. We have reached the third degree where we devote our intelligence\'s to anticipating what average opinion expects the average opinion to be."\n' +
      '\n' +
      'This game mirrors the challenge of anticipating the collective behavior of the financial markets. Investors must predict not only the value of an asset but also how they believe others will value it in the future.\n' +
      '\n' +
      '#### 2.2.2 Survival Auction Game (SAG)\n' +
      '\n' +
      'The Survival Auction Game (Figure.2 Right), as derived from the Water Allocation Challenge proposed in (Mao et al., 2023), is set in a fictional town grappling with a severe drought. The goal for each resident is to survive a 10-day drought period, which is achieved by bidding water resources and maintaining health points above zero. If a player successfully bid the water resources, they will gain health points; otherwise, they will lose health points equal to the number of consecutive days \\(n\\), during which they have not obtained water resources.\n' +
      '\n' +
      'The integration of the auction system with the health points mechanism creates a dynamic environment where players must balance health and finances. It challenges their strategic resource management skills and their capacity to anticipate and respond to the competitive actions of others.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      'In order to effectively measure the capabilities of LLMs and LLM agents in dynamic reasoning scenarios, we establish a comprehensive evaluation system consisting of four metrics. As these tasks are well-defined, the metrics are straightforward.\n' +
      '\n' +
      '**Win Rate** is calculated based on the number of wins over game going, providing a measure of the overall ability.\n' +
      '\n' +
      '\\[\\mathrm{WinRate}=\\frac{\\mathrm{Num\\ of\\ Wins}}{\\mathrm{Total\\ Round\\ per\\ Test\\times Num\\ of\\ Test}} \\tag{2}\\]\n' +
      '\n' +
      'In games such as the G0.8A, where elimination is not a factor, the Win Rate is a crucial measure.\n' +
      '\n' +
      '**Average Survival Round** calculates the average round in which the player remains in the game. It\'s an effective way to assess performance in elimination-based game, like SAG.\n' +
      '\n' +
      '\\[\\mathrm{AvgSurvivalRound}=\\frac{\\sum\\mathrm{Survival\\ Round\\ in\\ Each\\ Test}}{ \\mathrm{Num\\ of\\ Test}} \\tag{3}\\]\n' +
      '\n' +
      '**Adaptation Index** evaluates the player\'s ability to adapt and improve the performance over time. It is determined by comparing the deviation from the optimal strategy in the first half of the rounds to that in the second half.\n' +
      '\n' +
      '\\[\\mathrm{Adaptation\\ Index}=\\frac{\\mathrm{Target\\ Deviation\\ @\\ (second\\ half)}}{ \\mathrm{Target\\ Deviation\\ @\\ (first\\ half)}} \\tag{4}\\]For the G0.8A, we calculate the deviation between the number chosen by player and the target number. For the SAG, considering that the optimal winning bid is just slightly higher than the opponent\'s highest bid, we calculate the absolute difference between the agent\'s bid and the opponent\'s highest bid.\n' +
      '\n' +
      '**Prediction Accuracy** evaluates the accuracy of player\'s predictions regarding rivals\' future moves. In the G0.8A, it involves calculating the absolute difference between the player\'s predicted average and the actual average in each round:\n' +
      '\n' +
      '\\[\\mathrm{Pred\\ Acc}=\\frac{\\sum|\\mathrm{Avg}_{Pred}-\\mathrm{Avg}_{ Actual}|}{\\mathrm{Num\\ of\\ Test}} \\tag{5}\\]\n' +
      '\n' +
      'In the SAG, the focus shifts to measuring the absolute error between the player\'s prediction of the highest bid by opponents and the actual highest bid made by them.\n' +
      '\n' +
      '\\[\\mathrm{Pred\\ Acc}=\\frac{\\sum|\\mathrm{Max\\ Bid}_{Pred}-\\mathrm{Max\\ Bid}_{ Actual}|}{\\mathrm{Num\\ of\\ Test}} \\tag{6}\\]\n' +
      '\n' +
      '## 3 K-Level Reasoning with LLMs\n' +
      '\n' +
      'Cognitive hierarchy theory (Stahl, 1993) provides a fundamental framework for understanding decision-making in dynamic, interactive, and competitive scenarios. Players categorize themselves and others into different levels based on their rationality and strategic depth.\n' +
      '\n' +
      'As shown in Figure.3 (Left), players in the first-level thinking directly react to the environment without considering others\' actions. Players at higher thinking levels, such as second and third level, base their decisions on the assumption that other players are operating at lower levels of strategic thinking. This **recursive** reasoning process where each player attempts to outthink others by understanding and anticipating their strategic reasoning level allows for a layered understanding of strategic interactions.\n' +
      '\n' +
      'We draw on this idea to improve the reasoning capabilities of LLMs in dynamic challenges, and propose a novel recursive dynamic reasoning approach, termed the "K-Level Reasoning". As illustrated in Figure 3 (Right), this method involves recursively simulating the opponent\'s behavior using available historical information, followed by predicting the optimal behavior based on the opponents\' behavior modeling.\n' +
      '\n' +
      'The K-Level Reasoning process is formulated as:\n' +
      '\n' +
      'Initial condition:\n' +
      '\n' +
      '\\[d^{t,1}=f(E^{t}) \\tag{7}\\]\n' +
      '\n' +
      'Recursive formula:\n' +
      '\n' +
      '\\[d^{t,k}=f(E^{t},\\hat{d}_{1}^{t,k-1},...,\\hat{d}_{n}^{t,k-1}) \\tag{8}\\]\n' +
      '\n' +
      'where \\(d^{t,k}\\) indicates the decision made at step \\(t\\) in k-th level thinking. \\(f\\) represents a LLM that generates the decision based on inputs. \\(\\hat{d}_{1}^{t,k-1},...,\\hat{d}_{n}^{t,k-1}\\) represent the anticipated decisions of other players (1 to \\(n\\)) at \\(k-1\\) level of reasoning. The \\(\\hat{d}_{1}^{t,k-1}\\) can be recursively derived by applying this process through modeling of the \\(k-2\\) th level.\n' +
      '\n' +
      'Applying K-Level Reasoning to LLMs could significantly enhance their ability to understand the dynamic and strategic scenarios. By simulating opponents\' behavior and predicting optimal responses, LLMs could effectively adapt to the strategies of other participants. However, deeper modeling doesn\'t always lead to better results. Optimal decisions occur when one\'s understanding is just one level above their\n' +
      '\n' +
      'Figure 3: **Left**: K-level thinking is a recursive reasoning process. In first-level thinking, individuals react directly to the environment, akin to static reasoning. In second-level thinking, individuals take into account the first-level thinking of others, and so on and so forth. **Right**: In K-Level Reasoning, LLM adopts the perspective of rivals to recursively employ k-level thinking based on available historical information. It significantly improves the prediction accuracy of rivals’ subsequent moves and informs more strategic decision-making.\n' +
      '\n' +
      'competitor\'s. Overthinking can also cause significant bias- and errors. We\'ll explore how recursion depth affect decisions in experiments.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Base Techniques\n' +
      '\n' +
      'We adapt a variety of approaches, originally from traditional reasoning benchmarks, to explore the reasoning methods within dynamic challenges. These base techniques include:\n' +
      '\n' +
      '**Standard Prompting (Direct)**: This is the conventional prompting method in which the LLM generates the final answer (Action) in response to the given game setting prompt.\n' +
      '\n' +
      '**Chain-of-Thought (CoT)**(Wei et al., 2022): This approach employs the zero-shot native Chain-of-Thought reasoning method (Kojima et al., 2022).\n' +
      '\n' +
      '**Persona Prompting (Persona)**(Deshpande et al., 2023): This technique modifies the standard prompting process by incorporating "Game Expert" persona information to augment the reasoning capabilities of LLM.\n' +
      '\n' +
      '**Reflexion (Reflect)**(Shinn et al., 2023): This method refers the concept of language agents with verbal reinforcement learning. It has been specifically adapted for dynamic tasks, with modifications that are explained in detail in the C.1.2.\n' +
      '\n' +
      '**Self-Refine (Refine)**(Madaan et al., 2023): This is a multi-round iterative reasoning approach where an additional LLM offers comments and adjustments prior to reaching a final decision. The distinctions between Self-Refine and Reflect are elaborated upon in the Appendix D.\n' +
      '\n' +
      '**Prediction Chain of Thought (PCoT)**: This is a strong baseline we proposed. Diverging from CoT, PCoT uniquely requires that the LLM explicitly predicts the opponents\' actions before making decisions. Although it also emphasizes the prediction of opponents\' actions, PCoT is more about direct prediction based on context, while K-Level Reasoning involves a recursive, layered approach to anticipate opponents\' actions.\n' +
      '\n' +
      'For details on the implementations and specific examples of these techniques, please refer to the Appendix C.\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      'To evaluate the dynamic reasoning capabilities of a specific approach, we set a controllable environment and distinguish between two roles: the player (primary focus) and the opponents. In each game, there is **1** player and **4** opponents. The "player" will be equipped with a specific method, and all opponents use the same reasoning approach. This well-defined setting constructs a clearer comparison of dynamic reasoning capabilities between methods. For each setting,\n' +
      '\n' +
      'Figure 4: Illustration of different methods in the Guessing 0.8 of the Average game. **Left**: In the Chain of Thought, the LLM outputs reasoning logic in a step-by-step format. However, the LLM demonstrates poor understanding of situations and prediction of opponents’ actions. **Middle**: In the Prediction Chain-of-Thought, with an explicit requirement to make predictions about rivals’ next moves, the LLM clearly divides the reasoning into prediction and reasoning phases. However, the predictions are still somewhat biased. **Right**: In the K-Level Reasoning, the LLM recursively predict rivals’ next moves with public historical information. The implementation is based on Equation 7&8. Thank to a deeper strategic depth than rivals, the prediction and decision are more accurate.\n' +
      '\n' +
      'experiments are repeated 10 times to ensure significant results. Meanwhile, we compared reasoning methods with several programmatic strategic patterns as references. The experimental results can be found in the Appendix B.\n' +
      '\n' +
      'We implemented all methods using the GPT-4 (Achiam et al., 2023) (gpt4-32k), with the temperature set at \\(0.7\\) and the top-p set at \\(0.9\\). Unless specified otherwise, the level of thinking in K-Level Reasoning is set to K=2.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '#### 4.3.1 Guessing 0.8 of the Average\n' +
      '\n' +
      '**Win Rate** Analysis of player Win Rate (Table 1) demonstrates the K-Level Reasoning (K-R) method\'s superiority in dynamic decision-making environments. Its strategic effectiveness lies in its ability to anticipate opponent moves, outperforming other prompting methods.\n' +
      '\n' +
      '**Adaptation Index** The Adaptation Index (Table 2), calculated as the ratio of the average absolute deviation in player choices between the first and second halves of the game, reveal insightful trends. Despite a lack of direct correlation with Win Rate, the PCoT and K-R approaches show significant learning progress, suggesting an adaptive move towards optimal strategies. Whereas, the CoT and Refine methods display limited learning progression, indicating that inflexible thought processes might hinder adaptability.\n' +
      '\n' +
      '**Prediction Accuracy** Initially, K-R displays higher prediction accuracy than PCoT (Figure.5), starting with more accurate and less random predictions. Furthermore, the predictions converged very quickly and were quite accurate in the second half of the game. This trend underscores the LLM\'s increasing proficiency in understanding strategic depths with more gameplay context. K-R\'s superior long-term prediction accuracy suggests its alignment with LLM\'s intrinsic context-learning capabilities.\n' +
      '\n' +
      '#### 4.3.2 Survival Auction Game\n' +
      '\n' +
      '**Average Survival Round** Different reasoning methods\' performance in the Survival Auction Game (Table 3) reveals that the K-Level Reasoning method enhances survival rates compared to other approaches. Unlike in the performance in the G0.8A, PCoT does not show a distinct advantage here.\n' +
      '\n' +
      '**Adaptation Index** K-Level Reasoning exhibits the highest Adaptation Index (Table 4), indicating minimal extra overhead and superior adaptation skills in the game\'s latter half. This metric also reveals the method\'s remarkable adaptability, especially since poor performance in the game\'s first half can lead to early elimination. The inability of some methods to reach the second half of the game under certain conditions further underscores K-R\'s adaptiveness.\n' +
      '\n' +
      '**Prediction Accuracy** The prediction accuracy for the highest opponent bid in the Survival Auction Game (Figure.6) highlights the increased complexity of this challenge. While both PCoT and K-R faced difficulties in accurately predicting bids due to the need to consider multiple variables like opponent\'s balance, health condition and bidding style, K-R consistently showed a smaller prediction deviation, underscoring its effectiveness in complex scenarios.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      'Figure 4 demonstrates how different reasoning approaches function in the "Guessing 0.8 of the Average" game. The\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.43 & 0.67 & 0.62 & 0.53 & 0.43 & 0.61 & **0.82** \\\\ CoT & 0.07 & 0.32 & 0.35 & 0.14 & 0.22 & 0.45 & **0.63** \\\\ Persona & 0.05 & 0.37 & 0.29 & 0.05 & 0.37 & **0.11** & **0.46** \\\\ Reflect & 0.42 & 0.68 & 0.63 & 0.39 & 0.64 & 0.74 & **0.78** \\\\ Refine & 0.10 & 0.34 & 0.32 & 0.31 & 0.23 & 0.22 & **0.46** \\\\ PCoT & 0.03 & 0.44 & 0.52 & 0.21 & 0.51 & 0.54 & **0.85** \\\\ K-R(**ours**) & 0.04 & 0.15 & 0.14 & 0.04 & 0.17 & 0.14 & **0.52** \\\\ \\hline Average & 0.16 & 0.42 & 0.41 & 0.24 & 0.37 & 0.40 & **0.65** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Win Rate of the player against different opponents in Guessing 0.8 of the Average game. Due to the simultaneous wins of multiple players in a particular game, the Win Rate of each method against itself exceeds 0.2. We set the Win Rate of each method against itself (as indicated on the diagonal) as comparison baseline. A Win Rate significantly higher than this suggests a stronger dynamic reasoning ability.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.60 & 1.47 & 0.64 & 0.71 & 1.27 & 0.76 & **0.44** \\\\ CoT & 0.44 & 0.67 & 0.40 & **0.31** & 0.58 & 0.40 & 0.45 \\\\ Persona & 0.43 & 0.47 & 0.34 & 0.37 & 0.62 & 0.32 & **0.25** \\\\ Reflect & 0.62 & 0.60 & 0.65 & 0.60 & 0.87 & 0.53 & **0.26** \\\\ Refine & 0.41 & 0.57 & **0.28** & 0.36 & 0.55 & 0.42 & 0.30 \\\\ PCoT & 0.50 & 0.60 & 0.54 & 0.31 & 0.57 & 0.32 & **0.23** \\\\ K-R(**ours**) & 0.61 & 0.60 & 0.49 & 0.48 & 0.74 & 0.47 & **0.25** \\\\ \\hline Average & 0.51 & 0.71 & 0.48 & 0.45 & 0.74 & 0.46 & **0.31** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Adaptation Index of different methods in the Guessing 0.8 of the Average. Lower values in the table indicate stronger adaptability. For ease of comparison, the optimal values for each row are **bolded**, and the second-best values are underlined.\n' +
      '\n' +
      'Figure 5: The absolute error in predicting the opponent during the G0.8A between PCOT and K-Level Reasoning.\n' +
      '\n' +
      'COT and PCOT methods exhibit limitations in understanding and predicting opponents\' moves, thus leading to biased or less accurate decisions. K-Level Reasoning, on the other hand, recursively utilizes historical data to make more nuanced predictions about opponents\' behaviors and thinking levels, resulting in greater accuracy and strategic depth.\n' +
      '\n' +
      '## 5 Rethinking K-Level Reasoning\n' +
      '\n' +
      '### Better Reasoning Methodology VS Stronger Foundation Model\n' +
      '\n' +
      'There is a consensus that LLM trained with more data and possessing larger parameter sizes demonstrate stronger reasoning capabilities. We explore whether K-Level Reasoning can significantly enhance the dynamic reasoning abilities of relatively weaker LLMs. Therefore, we conduct experiments to compare the performance between the K-Level Reasoning with GPT-3.5 (K-R[GPT-3.5]) and the other reasoning methods based on GPT-4. All experiments are repeated 10 times.\n' +
      '\n' +
      'From the results of Table 5, we can see that the superior performance of K-R[GPT-3.5] over the standard prompting method of GPT4 (Direct[GPT4]). Furthermore, when battling with opponents equipped with reasoning methods on GPT4, K-R[GPT-3.5] demonstrates remarkable capabilities. K-R, with its excellent formalization on dynamic challenges and restoration of rival\'s perspective, greatly enhances the LLM\'s ability in dynamic reasoning. The results imply that integrating advanced reasoning methodologies can be a crucial factor in enhancing the dynamic reasoning of LLMs.\n' +
      '\n' +
      '### The Deeper Thinking Level, the Better Strategic Performance?\n' +
      '\n' +
      'The K-Level Reasoning approach is distinguished by its recursive modeling of rivals\' thinking processes. We are curious about how thinking levels affect reasoning outcomes. Therefore, we compare the performance of K-R[K=2] and K-R[K=3] in two games. K-R[K=3] assumes that all opponents are at the second level of the cognitive hierarchy. In other words, when simulating opponent predictions, the opponent uses the K-R[K=2] reasoning approach. The results, detailed in Table 6, reveal an intriguing picture of the impact of increased depth in thinking levels.\n' +
      '\n' +
      'When against the Direct method, which can be treated as the first-level thinking, K-R[K=3] showed a decreased win rate in G0.8A and keep the performance in SAG. This suggests a potential downside to excessive depth in reasoning, possibly indicating overthinking. However, against K-R[K=2] opponents, K-R[K=3] exhibited a significant improvement in both games. This improvement demonstrates the advantages of being one step ahead in the reasoning process.\n' +
      '\n' +
      'This juxtaposition leads to a compelling insight: the critical factor in K-Level Reasoning is not simply the depth of\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.86 & 1.09 & 2.07 & 2.36 & 1.82 & **0.39** & 1.24 \\\\ CoT & 1.32 & 1.56 & 1.29 & - & 2.27 & **0.67** & 1.27 \\\\ Persona & 1.19 & 1.62 & 1.30 & 2.31 & 1.08 & 1.79 & **1.04** \\\\ Reflect & 1.00 & 2.02 & 1.21 & **0.66** & 1.06 & 1.21 & 1.18 \\\\ Refine & 1.43 & 1.69 & 1.95 & 2.03 & 1.74 & 1.73 & **1.21** \\\\ PCOT & 1.60 & 1.37 & 1.05 & 0.90 & 1.69 & **0.83** & 1.19 \\\\ K-R(**ours**) & - & **0.64** & 1.27 & - & 0.70 & - & 2.27 \\\\ \\hline Average & - & 1.43 & 1.45 & - & 1.48 & - & **1.34** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Adaptation Index of different reasoning methods in the Survival Auction Game. The missing parts in the table are indicated by ”-” to signify that no record of this method survive to the second half rounds. These missing data were not included in the calculation of the Average value.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 5.90 & 7.00 & 7.50 & 4.70 & 8.70 & 6.60 & **9.40** \\\\ CoT & 5.70 & 6.50 & 5.30 & 4.00 & 8.10 & 5.30 & **10.00** \\\\ Persona & 5.70 & 7.70 & 7.40 & 5.20 & 6.30 & 7.20 & **9.30** \\\\ Reflect & 9.40 & 9.40 & 9.90 & 5.20 & 8.60 & 8.20 & **10.00** \\\\ Refine & 6.30 & 6.40 & 8.10 & **4.30** & **8.20** & 5.30 & 7.90 \\\\ PCOT & 8.50 & 9.60 & 9.90 & 6.30 & 8.50 & 6.20 & 7.90 \\\\ K-R & 4.10 & 5.50 & 5.00 & 4.04 & 5.70 & **4.40** & **6.80** \\\\ \\hline Average & 6.51 & 7.44 & 7.59 & 4.82 & 7.73 & 6.17 & **9.01** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Average Survival Round of the player against different opponents in Survival Auction Game. We set the Average Survival Round of each method against itself (as indicated on the diagonal as comparison baseline. A survival round significantly higher than this suggests a stronger dynamic reasoning ability.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c c c} \\hline \\hline \\multicolumn{3}{c|}{_Games 0.8 of the Average_} & \\multicolumn{3}{c}{_Survival Auction Game_} \\\\ \\hline Opponent & Direct & K-R & Direct & K-R & Direct & K-R & Direct & K-R & Direct & K-R \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} Direct \\\\ (GPT-3.5) \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-3.5] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } \\\\ \\hline Direct & 0.18 & 0.18 & 0.43 & 0.82 & 5.00 & 0.90 & 5.90 & 0.90 \\\\ CoT & 0.14 & 0.37 & **0.00** & 0.63 & 5.00 & 8.10 & 5.70 & **10.00** \\\\ Persona & 0.10 & 0.23 & **0.05** & 0.46 & 5.00 & 7.50 & 5.70 & 9.30 \\\\ Reflect & 0.24 & 0.38 & 0.42 & **0.78** & 5.00 & 8.50 & **9.40** & **10.00** \\\\ Refine & 0.14 & 0.13 & **0.10** & 0.46 & 5.10 & 6.70 & 6.30 & 7.90 \\\\ PCOT & 0.19 & 0.46 & **0.03** & **0.35** & **4.10** & 6.80 & 8.50 & **9.70** \\\\ \\hline Average & 0.16 & 0.29 & 0.18 & 0.67 & **1.49** & 7.83 & 6thought, but the relative depth compared to the opponent. Having a one level deeper thinking-level offers a strategic advantage, as seen in the superior performance of K-R[K=2] over Direct and K-R[K=3] over K-R[K=2]. However, advancing two levels beyond the opponents, as in the case of K-R[K=3] versus Direct, may result in diminishing returns due to the complexity of over-anticipation.\n' +
      '\n' +
      'In dynamic challenges, identifying the exact thinking levels of opponents is challenging. Adapting to thinking-levels dynamically and employing K-Level Reasoning for deeper analysis presents a valuable direction for future research.\n' +
      '\n' +
      '## 6 Related Work\n' +
      '\n' +
      '### Reasoning with LLMs\n' +
      '\n' +
      'Large Language Models (LLMs) excel in diverse complex reasoning tasks, such as mathematical (Miao et al., 2021; Patel et al., 2021), common sense (Talmor et al., 2022; Bhakthavatsalam et al., 2021), and symbolic reasoning (Srivastava et al., 2022; Suzgun et al., 2022). A notable reasoning approach involves breaking down complex questions into a series of intermediate steps, a technique known as the Chain-of-Thought (CoT) method (Wei et al., 2022; Kojima et al., 2022). Subsequently, some works have emerged to extend CoT, with innovations like Tree of Thought (ToT) (Yao et al., 2023), Graph of Thought (GoT) (Besta et al., 2023) and Skeleton-of-thought (Ning et al., 2023). Besides, approaches like Self-Refine (Madaan et al., 2023) and Reflexion (Shinn et al., 2023) enhance CoT\'s consistency by having LLMs review and refine their responses. Moreover, recent research has revealed that integrating persona information into LLMs significantly improves their reasoning processes (Deshpande et al., 2023). A series of studies (Fu et al., 2023; Wang et al., 2023) have been conducted to incorporate more persona information, aiming to enhance the rationality and knowledge ability of the LLM reasoning process. However, our comprehensive testing revealed that all these methods are inadequate for dynamic problems.\n' +
      '\n' +
      'Recently, research on large language models (LLMs) in strategic reasoning has been conducted(Gandhi et al., 2023; Bakhtin et al., 2022), these approaches involve prompting LLMs to recognize the intricacies of strategic tasks, like our proposed Prediction Chain-of-Thought baseline. However, our experimental results indicate that this approach fails to establish a clear cognitive hierarchy necessary for recursive and deeper strategic thinking. We propose the first recursive dynamic reasoning method with LLMs - K-Level Reasoning, and demonstrate its superiority through exhaustive empirical evidence.\n' +
      '\n' +
      '### Dynamic Problems in Machine Learning\n' +
      '\n' +
      'Dynamic problems arise when multiple participants are involved in multi-round interactions. One key factor is the simultaneous interactions of multiple participants with the environment. In machine learning area, it is also referred as multi-agent system (MAS) (Dorri et al., 2018). Unlike single-agent systems, MAS encounters a broader range of issues and challenges, as noted by Wong et al. (2021), including computational complexity (Ding and Dong, 2020), nonstationarity (Papoudakis et al., 2019), partial observability (Mahajan et al., 2019; Foerster et al., 2016), and challenges in credit assignment (Sunehag et al., 2017). Particularly, in the context of inference using LLMs, the nonstationarity of the environment poses a distinct challenge. LLMs struggle with the nonstationarity of environments, as their traditional training focuses on next-word prediction.(Dai and Le, 2015; Devlin et al., 2018; Radford et al., 2018).\n' +
      '\n' +
      'Addressing nonstationarity requires deep environmental understanding. In this regard, opponent modeling (Albrecht and Stone, 2018; Schrittwieser et al., 2020; Li et al., 2020) proves to be an effective approach for understanding future environmental changes. Existing opponent predictions based on language models have been applied to strategic problems(Bakhtin et al., 2022), while the need for extensive data for training limits the flexibility of the agents. We propose "K-Level-Reasoning", which is a recursive modeling on opponents with LLMs. This method fully exploits the in-context learning capabilities of LLMs.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'This paper represents a significant stride in understanding and enhancing the capabilities of LLMs in dynamic reasoning. The Guessing 0.8 of the Average and Survival Auction Game serve not only as effective tools for assessment but also as bridges connecting AI research with practical, real-world economic and dynamic scenarios. By introducing the game settings, we have provided a novel and robust testbed for systematically evaluating LLMs in scenarios that mirror the complexities of real-world decision-making. We analyze the limitations of the existing reasoning methods in addressing dynamic challenges and propose a new reasoning approach "K-Level-Reasoning". Through experiments, we validate the rationality of decision-making and adaptability to dynamic environments offered by this method. This paper highlights the vast potential of LLMs in navigating and deciphering the ever-changing tapestry of dynamic strategic decision-making.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c|c c|c} \\hline \\hline  & \\multicolumn{3}{c|}{_Guessing 0.8 of the Average_} & \\multicolumn{3}{c}{_Survival Auction Game_} \\\\ \\hline Opponent & Direct & K-R[K=2] & K-R[K=3] & Direct & K-R[K=2] & K-R[K=3] \\\\ \\hline Direct & 0.43 & **0.82** & 0.77 (0.005) & 5.90 & **0.940** & **0.940** (0.000) \\\\ K-R[K-2] & 0.04 & 0.52 & **0.60** (4.08) & **4.10** & 6.80 & **8.30** (e.i.50) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Comparison between K-Level Reasoning[K=2] and K-Level Reasoning[K=3] in the two games. For G0.8A we report Wong Rate, and for SAG we report Average Survival Round.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aher et al. (2023) Aher, G. V., Arriaga, R. I., and Kalai, A. T. Using large language models to simulate multiple humans and replicate human subject studies. In _International Conference on Machine Learning_, pp. 337-371. PMLR, 2023.\n' +
      '* Albrecht & Stone (2018) Albrecht, S. V. and Stone, P. Autonomous agents modelling other agents: A comprehensive survey and open problems. _Artificial Intelligence_, 258:66-95, 2018.\n' +
      '* Bakhtin et al. (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A., et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. _Science (New York, NY)_, pp. eade9097-eade9097, 2022.\n' +
      '* Besta et al. (2023) Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. _arXiv preprint arXiv:2308.09687_, 2023.\n' +
      '* Bhakhtavatsalam et al. (2021) Bhakhtavatsalam, S., Khashabi, D., Khot, T., Mishra, B. D., Richardson, K., Sabharwal, A., Schoenick, C., Tafjord, O., and Clark, P. Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. _arXiv preprint arXiv:2102.03315_, 2021.\n' +
      '* Chen et al. (2023) Chen, J., Yuan, S., Ye, R., Majumder, B. P., and Richardson, K. Put your money where your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. _arXiv preprint arXiv:2310.05746_, 2023.\n' +
      '* Dai & Le (2015) Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. _Advances in neural information processing systems_, 28, 2015.\n' +
      '* Deshpande et al. (2023) Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. Toxicity in chatgpt: Analyzing persona-assigned language models. _arXiv preprint arXiv:2304.05335_, 2023.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Diao et al. (2023) Diao, S., Wang, P., Lin, Y., and Zhang, T. Active prompting with chain-of-thought for large language models. _ArXiv_, abs/2302.12246, 2023. doi: 10.48550/arXiv.2302.12246.\n' +
      '* Ding & Dong (2020) Ding, Z. and Dong, H. Challenges of reinforcement learning. _Deep Reinforcement Learning: Fundamentals, Research and Applications_, pp. 249-272, 2020.\n' +
      '* Dorri et al. (2018) Dorri, A., Kanhere, S. S., and Jurdak, R. Multi-agent systems: A survey. _Ieee Access_, 6:28573-28593, 2018.\n' +
      '* Foerster et al. (2016) Foerster, J., Assael, I. A., De Freitas, N., and Whiteson, S. Learning to communicate with deep multi-agent reinforcement learning. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* Fu et al. (2023) Fu, Y., Peng, H., Khot, T., and Lapata, M. Improving language model negotiation with self-play and in-context learning from ai feedback. _arXiv preprint arXiv:2305.10142_, 2023.\n' +
      '* Gandhi et al. (2023) Gandhi, K., Sadigh, D., and Goodman, N. D. Strategic reasoning with language models. _arXiv preprint arXiv:2305.19165_, 2023.\n' +
      '* Han et al. (2023) Han, X., Wu, Z., and Xiao, C. " guinea pig trials" utilizing gpt: A novel smart agent-based modeling approach for studying firm competition and collusion. _arXiv preprint arXiv:2308.10974_, 2023.\n' +
      '* Keynes (1936) Keynes, J. M. The general theory of employment. _The quarterly journal of economics_, 51(2):209-223, 1936.\n' +
      '* Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. _Advances in neural information processing systems_, 35:22199-22213, 2022.\n' +
      '* Lan et al. (2023) Lan, Y., Hu, Z., Wang, L., Wang, Y., Ye, D., Zhao, P., Lim, E.-P., Xiong, H., and Wang, H. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. _arXiv preprint arXiv:2310.14985_, 2023.\n' +
      '* Ledoux (1981) Ledoux, A. Concours resultats complets. _Les victimes se sont pu a jouer le_, 14:10-11, 1981.\n' +
      '* Li et al. (2020) Li, J., Yang, F., Tomizuka, M., and Choi, C. Evolvegraph: Multi-agent trajectory prediction with dynamic relational reasoning. _Advances in neural information processing systems_, 33:19783-19794, 2020.\n' +
      '* Madaan et al. (2023) Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. _arXiv preprint arXiv:2303.17651_, 2023.\n' +
      '* Mahajan et al. (2019) Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S. Maven: Multi-agent variational exploration. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* Mao et al. (2023) Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge, T., and Wei, F. Alympics: Language agents meet game theory. _arXiv preprint arXiv:2311.03220_, 2023.\n' +
      '\n' +
      '* Miao et al. (2021) Miao, S.-Y., Liang, C.-C., and Su, K.-Y. A diverse corpus for evaluating and developing english math word problem solvers. _arXiv preprint arXiv:2106.15772_, 2021.\n' +
      '* Nagel (1995) Nagel, R. Unraveling in guessing games: An experimental study. _The American economic review_, 85(5):1313-1326, 1995.\n' +
      '* Ning et al. (2023) Ning, X., Lin, Z., Zhou, Z., Yang, H., and Wang, Y. Skeleton-of-thought: Large language models can do parallel decoding. _arXiv preprint arXiv:2307.15337_, 2023.\n' +
      '* Papoudakis et al. (2019) Papoudakis, G., Christianoos, F., Rahman, A., and Albrecht, S. V. Dealing with non-stationarity in multi-agent deep reinforcement learning. _arXiv preprint arXiv:1906.04737_, 2019.\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.\n' +
      '* Patel et al. (2021) Patel, A., Bhattacharya, S., and Goyal, N. Are nlp models really able to solve simple math word problems? _arXiv preprint arXiv:2103.07191_, 2021.\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.\n' +
      '* Schrittwieser et al. (2020) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.\n' +
      '* Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '* Srivastava et al. (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.\n' +
      '* Stahl (1993) Stahl, D. O. Evolution of smartn players. _Games and Economic Behavior_, 5(4):604-617, 1993.\n' +
      '* Stahl and Wilson (1995) Stahl, D. O. and Wilson, P. W. On players\' models of other players: Theory and experimental evidence. _Games and Economic Behavior_, 10(1):218-254, 1995.\n' +
      '* Stahl et al. (1994) Stahl II, D. O. and Wilson, P. W. Experimental evidence on players\' models of other players. _Journal of economic behavior & organization_, 25(3):309-327, 1994.\n' +
      '* Sunehag et al. (2017) Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multi-agent learning. _arXiv preprint arXiv:1706.05296_, 2017.\n' +
      '* Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Talmor et al. (2022) Talmor, A., Yoran, O., Bras, R. L., Bhagavatula, C., Goldberg, Y., Choi, Y., and Berant, J. Commonsenseqa 2.0: Exposing the limits of ai through gamification. _arXiv preprint arXiv:2201.05320_, 2022.\n' +
      '* Wang et al. (2023) Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., and Ji, H. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. _arXiv preprint arXiv:2307.05300_, 2023.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.\n' +
      '* Wong et al. (2021) Wong, A., Back, T., Kononova, A. V., and Plaat, A. Deep multiagent reinforcement learning: Challenges and directions. _arXiv preprint arXiv:2106.15691_, 2021.\n' +
      '* Xu et al. (2023) Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu, Y. Exploring large language models for communication games: An empirical study on werewolf. _arXiv preprint arXiv:2309.04658_, 2023.\n' +
      '* Yao et al. (2022) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. _ArXiv_, abs/2210.03629, 2022. doi: 10.48550/arXiv.2210.03629.\n' +
      '* Yao et al. (2023) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. _arXiv preprint arXiv:2305.10601_, 2023.\n' +
      '\n' +
      '## Appendix A Game Setting\n' +
      '\n' +
      '### Guessing 0.8 of the Average\n' +
      '\n' +
      '**Initial Setup**: For each round, each player select a number between 1 and 100. The objective is to select a number that is closest to 80% of the group\'s average choice. Formally, each player \\(i\\) chooses a number \\(n_{i}\\), aiming for \\(n_{i}\\approx 0.8\\times\\overline{n}\\), where \\(\\overline{n}\\) is the average of all chosen numbers.\n' +
      '\n' +
      '**Scoring and Continuation**: A player scores a point if his/her chosen number is closest to 80% of the average number chosen by the group. If all players select the same number, no points are awarded for this round. Mathematically, the score for player \\(i\\) in in round \\(t\\) is given by \\(s_{i}^{t}\\), which is 1 if \\(|n_{i}-0.8\\times\\overline{n}|\\) is the minimum among all players, and 0 otherwise.\n' +
      '\n' +
      '### Survival Auction Game\n' +
      '\n' +
      '**Initial Setup**: Players start with 8 health points, out of a maximum of 10. Every day, each player possesses a fixed income of $100. The daily water supply can only satisfy one resident\'s requirement.\n' +
      '\n' +
      '**Scoring and Continuation**: Everyday, players engage in a daily auction to secure the necessary water resources, and the highest bidder wins. In case of a tie, the resources are not allocated to any player. If a player successfully bid the water resources, they will gain 2 health points; otherwise, they will lose health points equal to the number of consecutive days, denoted as \\(n\\), during which they have not obtained water resources. Once a player\'s health points fall to 0 or below, they will be eliminated. The health point of player \\(i\\) on day \\(t\\), denoted as \\(h_{i}^{t}\\), is crucial in determining their survival and bidding strategy.\n' +
      '\n' +
      'Figure 7: Prompts used in Guessing 0.8 of the Average game.\n' +
      '\n' +
      '## Appendix B Performance of Large Language Models Competing with Programmatic Strategies\n' +
      '\n' +
      'In addition to using the **LLM-LLM Combat** comparison setting in Section 4, we have also designed a set of **LLM-Programmatic Strategy Combat** comparisons. In Player-Programmatic Strategy Combat setting, the "player" will be\n' +
      '\n' +
      'Figure 8: Prompts used in Survival Auction Game.\n' +
      '\n' +
      'equipped with a specific reasoning method, while opponents will play according to programmatic strategic patterns, which will not be adjusted with the game going. This mode is to check different methods\' adaptation to different predefined but fixed patterns. For the Player-Programmatic Player Combat Combat in G0.8A, the programmatic strategies include:\n' +
      '\n' +
      '1) **0-Level (Fix)**: The characteristic of a 0-Level player is that their choice of strategy is uniform (Stahl & Wilson, 1995). We limit the choice space of the 0-Level Computer player to only 40.\n' +
      '\n' +
      '2) **0-Level (Var)**: Modified from the 0-Level (Fix) strategy, the selection is sampled from a Gaussian distribution with a mean of 40 and a variance of 5.\n' +
      '\n' +
      '3) **MonoTrend (Fix)**: The numbers chosen by the computer players follow an arithmetic sequence with a decreasing common difference, and the common differences for all four computer players are the same.\n' +
      '\n' +
      '4) **MonoTrend (Var)**: The numbers chosen by the computer players follow an arithmetic sequence with a decreasing common difference, and the common differences for the four computer players are randomly generated from 1 to 5.\n' +
      '\n' +
      '5) **LastBids (Fix)**: The computer player chooses the target number from the previous round (selects 40 in the first round).\n' +
      '\n' +
      '6) **LastBids (Var)**: Modified from the LastBids strategy, the selection is sampled from a Gaussian distribution with a mean equal to the target number of the previous round and a variance of 5.\n' +
      '\n' +
      'Overall, the dynamic changes of these three settings are LastBids \\(>\\) MonoTrend \\(>\\) 0-Level. We use these three programs to test the reasoning capability to adapt to and counter different patterns.\n' +
      '\n' +
      'Table 7 reveals a significant trend in the performance of players against other approaches. The effectiveness of reasoning approaches decreases in the following order: 0-Level, MonoTrend, and LastBids. This pattern highlights a reduction in the efficacy of the LLM in more dynamic environments. We found that only K-Level Reasoning shows an advantage in LastBids (Fix), indicating that compared to K-Level Reasoning, previous reasoning methods on static problems lack observation and judgment of the opponent. Conversely, this also demonstrates that K-Level Reasoning can implicitly infer the behavior pattern of the opponent based on their historical actions.\n' +
      '\n' +
      'Intriguingly, reasoning methods significantly influence the performance in dynamic settings. Methods like CoT and Self-Refine, traditionally favored in static reasoning, also demonstrate substantial improvements over the Standard Prompt approach. This finding underscores the necessity for more elaborate reasoning processes in dynamic decision-making, akin to static problem-solving scenarios.\n' +
      '\n' +
      '## Appendix C Reasoning Method Implementation Details\n' +
      '\n' +
      '### G0.8A Game\n' +
      '\n' +
      '#### c.1.1 Direct\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, AlexI Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '1 choose 45.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l||c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline \\multicolumn{7}{c}{_Player VS Programmatic Strategies_} \\\\\n' +
      '0-Level (Fix) & 0.65 & 0.87 & 0.87 & 0.81 & 0.99 & 0.80 & 0.97 \\\\\n' +
      '0-Level (Var) & 0.44 & 0.67 & 0.69 & 0.61 & 0.54 & 0.76 & 0.77 \\\\ MonoTrend (Fix) & 0.05 & 0.06 & 0.15 & 0.00 & 0.29 & 0.15 & 0.48 \\\\ MonoTrend (Var) & 0.34 & 0.44 & 0.57 & 0.33 & 0.49 & 0.46 & 0.74 \\\\ LastBids (Fix) & 0.01 & 0.12 & 0.16 & 0.01 & 0.27 & 0.06 & 0.75 \\\\ LastBids (Var) & 0.06 & 0.15 & 0.18 & 0.19 & 0.18 & 0.14 & 0.18 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Win Rate of the player against different programmatic strategies in Guessing 0.8 of the Average game.\n' +
      '\n' +
      '#### c.1.2. Reflect\n' +
      '\n' +
      'Because the Reflect method requires correct labels, which is unrealistic in dynamic problems, we modified this method to have the LLM attempt to summarize the experience of victory or failure at the end of each round, and save the generated feedback content to assist the decision-making process in the next round. The content in Appendix D shows the implementation differences between Reflexion and Self-Refine.\n' +
      '\n' +
      '#### c.1.3. CoT\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, Alex! Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let\'s think step by step, and finally answer a number you think you can win.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Step 1: Consider the range of possible numbers. Since the integers can be between 1 and 100, the average of all chosen numbers will likely fall within this range as well.\n' +
      '\n' +
      'Step 2: Determine the target number. Since the winning number is closest to 0.8 * the average, we should focus on finding a number that will be close to this value.\n' +
      '\n' +
      'Step 3: Make an educated guess. Since it\'s difficult to predict the exact numbers other players will choose, we can make an educated guess based on the range of possible numbers. Assuming players choose numbers evenly across the range, the average should be around the midpoint, which is 50.\n' +
      '\n' +
      'Step 4: Calculate the target number based on our guess. 0.8 * 50 = 40.\n' +
      '\n' +
      'Step 5: Choose a number close to the target number. To increase the chances of winning, we should choose a number close to 40.\n' +
      '\n' +
      'My final answer: 1 will choose 40.\n' +
      '\n' +
      '#### c.1.4. PCot\n' +
      '\n' +
      'PCot diverges from the standard Chain of Thought (CoT) by **explicitly** requiring the LLM to **predict opponents\' actions** before making a decision. This method responds to the immediate problem by anticipating future scenarios, which is crucial in dynamic reasoning.\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, AlexI Now is the ROUND 2, and your HP is at 9. Please choose an integer between 1 and 100 for this round.\n' +
      '\n' +
      'First of all, predict the next round of choices based on the choices of other players in the previous round. The players\' choices in the previous rounds are as follows:\n' +
      '\n' +
      'Round 1:Alex chose 50, Bob chose 47, Gindy chose 50, David chose 45, Eric chose 50. Your output should be of the following format:\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      'The choice of each player in the next round here.\n' +
      '\n' +
      'Based on the prediction of other players, the average number in the next round here, and the target number in the next round (0.8 * the average of all chosen numbers) here.\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      'the number will you choose to win the next round game here.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      'Based on the choices of other players in the previous round, I predict the following choices for the next round:\n' +
      '\n' +
      'Player 1 (Bob): 47\n' +
      '\n' +
      'Player 2 (Gindy): 50\n' +
      '\n' +
      'Player 3 (David): 45\n' +
      '\n' +
      'Player 4 (Fric): 50\n' +
      '\n' +
      'Based on these predictions, the average number in the next round would be (47 + 50 + 45 + 50)/4 = 48, and the target number (0.8 * the average of all chosen numbers) would be 0.8 * 48 = 38.4.\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      'For this round, I will choose 39 to get closer to the target number of 38.4.\n' +
      '\n' +
      '**Introduction Prompt:**\n' +
      '\n' +
      'You are Alex and involved in a survive challenge. **You are a game expert, good at predicting other people\'s behavior and deducing calculations, and using the most favorable strategy to win the game.** SURIVIVE CHALLENGE!! You are one of five players in the game.\n' +
      '\n' +
      'Everyone is required to choose an integer between 1 and 100 in each round. The player whose chosen number is closest to (0.8 * the average of all chosen numbers) wins the round without any HP deduction. All other players will have 1 HP deducted. But if all players choose the same number, their health points are deducted together.\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, AlexI Now is the ROUND 1, and your HP is at 10. Please choose an integer between 1 and 100 for this round.Don\'t forget your expert status, use your expertise to win this round!\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Based on my expertise in predicting other people\'s behavior and deducing calculations, I will choose the number 40 for this round. I\'m assuming that the average of all chosen numbers will be around 50, and 0.8 times that is 40.\n' +
      '\n' +
      '#### Refine\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, AlexI Now is the ROUND 1, and your HP is at 10. Guess which number will win in the next round. Let\'s think step by step, and finally answer a number you think you can win.\n' +
      '\n' +
      '**First Output:**\n' +
      '\n' +
      'I\'ll start by assuming that the other players might choose numbers randomly or evenly distributed between 1 and 100. In this case, the average of the chosen numbers could be around 50.\n' +
      '\n' +
      'Next, I\'ll calculate 0.8 times the average, which would be 0.8 * 50 = 40.\n' +
      '\n' +
      'Considering this, I will choose 40 as my number for this round.\n' +
      '\n' +
      '**Feedback Prompt:**\n' +
      '\n' +
      'Carefully study the user\'s strategy in this round of the game. As a game expert, can you give a suggestion to optimize the user\'s strategy so that he can improve his winning rate in this round?\n' +
      '\n' +
      '**Refine Prompt:**\n' +
      '\n' +
      'I have a game expert\'s advice on your strategy in this round. You can adjust your strategy just now according to his suggestion. Here are his suggestions:\n' +
      '\n' +
      '"Certainly! To optimize the user\'s strategy, we can apply a more advanced approach, taking into consideration the other players\' potential strategies. This is known as "iterated elimination of dominated strategies."\n' +
      '\n' +
      'Let\'s assume that all players will think similarly to Alex and also aim to choose a number closest to 0.8 * the average. We can then narrow down the range of numbers that they would likely choose.\n' +
      '\n' +
      'Since the range is between 1 and 100, and the average would be around 50, we can eliminate numbers below 0.8 * 50 = 40, as these numbers would not be an optimal choice for the other players.\n' +
      '\n' +
      'Now, we\'ll focus on the range between 40 and 100. The average within this range is (40 + 100) / 2 = 70. Then, we can calculate 0.8 * 70 = 56.\n' +
      '\n' +
      'Given this information, Alex should choose 56 to improve his winning rate in this round. This strategy assumes that the other players are also thinking strategically and eliminating dominated options."\n' +
      '\n' +
      '**Second Output:**\n' +
      '\n' +
      'Thank you for the expert\'s advice. Based on the iterated elimination of dominated strategies, I will adjust my choice and select 56 for this round to improve my chances of winning.\n' +
      '\n' +
      '#### K-Level Reasoning\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Ok, AlexI Now is the ROUND 1, and your HP is at 10. Another game expert\'s prediction for the next round of other players is as follows: Bob might choose 50, Cindy might choose 57, David might choose 45, Eric might choose 55. According to the game expert\'s prediction, what number will you give in the next round is most likely to be close to the target number?\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'First, let\'s find the average of the predicted numbers chosen by the other players: (50 + 57 + 45 + 55) / 4 = 207 / 4 = 51.75\n' +
      '\n' +
      'Now, let\'s calculate 0.8 times the average: 0.8 * 51.75 = 41.4\n' +
      '\n' +
      'To have the best chance of winning the round, you should choose a number dose to 41.4. Since you can only choose integers, you should choose either 41 or 42 in the next round.\n' +
      '\n' +
      '### Survival Auction Game,\n' +
      '\n' +
      'C.2.1. Direct\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Hello, Alerel Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n' +
      '\n' +
      'name: Alex balance: 100 HP:8 no_drink:1\n' +
      '\n' +
      'Please carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE11 Now, if you want to participate in today\'s water resource auction, please provide your bid.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'I will bid $25 for today\'s water resource auction.\n' +
      '\n' +
      '#### c.2.2 CoT\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Hello, Alerel Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      'Please carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE11 Now, if you want to participate in today\'s water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to be most likely to survive. Let\'s think step by step, and finally provide your bid.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'I need to carefully consider my bidding strategy for today. With a balance of $100, I have some room to bid, but I also need to think about the other residents and their bids. Since we all have the same daily water requirement and budget, I need to outbid at least one resident to secure my water resources for today.\n' +
      '\n' +
      'Considering that the total daily water supply is 10 units, which is enough to meet only one resident\'s water requirements, the competition will be tough. However, bidding too high might leave me with less money for the upcoming days.\n' +
      '\n' +
      'Taking all these factors into account, I will place a bid of $60 for today\'s water resource auction. This should give me a decent chance of outbidding at least one resident while still maintaining some funds for the next few days.\n' +
      '\n' +
      '#### c.2.3. Pcot\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Hello, Alcal Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      'Please carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVEE11 Now, if you want to participate in today\'s water resource auction, please provide your bid. First of all, predict the next round of bidding of opponents based on the choices of other players in the previous round. Since this is the first round, there is no historical information about the last round. You can predict according to your understanding. Your output should be of the following format:\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      'The choice of each player in the next round here.\n' +
      '\n' +
      'Based on the prediction of other players, think carefully about your next round of bidding strategy to be most likely to survive. Let\'s think step by step, and finally provide your bid. Answer:\n' +
      '\n' +
      'the bidding will you choose in the next round game here.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      'Player 1: 530\n' +
      '\n' +
      'Player 2: 540\n' +
      '\n' +
      'Player 3: 550\n' +
      '\n' +
      'Player 4: 560\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      'Since this is the first round and we don\'t have any historical information about the other players, I will choose a bid of $35 for the next round. This bid is higher than the lowest predicted bid but not too high, which should give me a chance to secure water resources while preserving my budget.\n' +
      '\n' +
      '#### c.2.4. Persona\n' +
      '\n' +
      '**Introduction Prompt:**\n' +
      '\n' +
      'You are Alex and a resident living in W-Town. W Town is experiencing a rare drought. Every residents in Town W is ensuring their survival over a period of 10 days by acquiring the water resources. **You are a game expert, good at predicting other people\'s behavior and deducing calculations, and using the most favorable strategy to win the game. \\(\\bullet\\)\\(\\bullet\\) (The following is the same as the introduction Prompt.)\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Hello, Alcal Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      'Please carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVEE11 Now, if you want to participate in today\'s water resource auction, please provide your bid. Don\'t forget your expert status, use your expertise to win this round!\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Given that everyone starts with the same budget and water requirements, it\'s important to strategize and predict the bidding behavior of the other residents. As it is the first day of the challenge, I will assume that other residents may bid aggressively to secure their water resources.\n' +
      '\n' +
      'To increase my chances of winning, I will place a bid slightly higher than the average expected bid. Since the daily salary is $100, I predict most residents might bid around $60 to $80.\n' +
      '\n' +
      'Considering this, I will place a bid of $85 for Day 1 of the Water Allocation Challenge.\n' +
      '\n' +
      '#### 5.2.5 Refine\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      'Hello, Alexel Today is the Day 1 of the Water Allocation Challenge, with a quantity of 10 units. Your status:\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8NO_DRINK:1\n' +
      '\n' +
      'Please carefully analyze your situation to decide on this round of bidding. Remember, the most important thing is to SURVIVE11 Now, if you want to participate in today\'s water resource auction, please provide your bid. Think carefully about your next round of bidding strategy to be most likely to survive. Let\'s think step by step, and finally provide your bid.\n' +
      '\n' +
      '**First Output:**\n' +
      '\n' +
      'Considering the current situation, my status is:\n' +
      '\n' +
      '- Balance: $100\n' +
      '\n' +
      '- Health Points: 8\n' +
      '\n' +
      '- No Drink Days: 1\n' +
      '\n' +
      'As it\'s the first day of the challenge, 1 want to ensure that 1 have enough water to meet my needs and avoid any health point deductions. However, I also need to be strategic about my bidding to ensure 1 have enough money for future bids.\n' +
      '\n' +
      'Since the total daily water supply is 10 units, and 1 need 10 units of water per day, 1 will need to secure all the available water resources for today. As it\'s the first day, other residents might also be bidding aggressively to secure their water resources.\n' +
      '\n' +
      'Therefore, 1 will bid $60 for the water resources today, which is a little over half of my daily salary. This should give me a competitive edge while also leaving me with enough money for future bids.\n' +
      '\n' +
      'My bid for today\'s water resource auction: $60.\n' +
      '\n' +
      '**Feedback Prompt:**\n' +
      '\n' +
      'Carefully study the user\'s strategy in this round of the game. As a game expert, can you give a suggestion to optimize the user\'s strategy so that he can improve his winning rate in this round?\n' +
      '\n' +
      '**Refine Prompt:**\n' +
      '\n' +
      '1 have a game expert\'s advice on your strategy in this round. You can adjust your strategy just now according to his suggestion. Here are his suggestions: While the user\'s strategy of bidding $60 is a good starting point, there is room for optimization to improve the winning rate in this round. Since everyone has the same daily water requirement and budget, it\'s essential to consider the bidding behavior of the other residents.\n' +
      '\n' +
      'A suggested optimization would be to slightly increase the bid to $65. This increment gives an added advantage over competitors who might also be bidding around half of their daily salary. Additionally, bidding an odd number reduces the chances of a tie, as other residents may place round number bids.\n' +
      '\n' +
      'With this optimized strategy, the user still maintains a good balance of aggressive bidding and budget management, increasing their chances of securing the water resources for today.\n' +
      '\n' +
      'Optimized bid for today\'s water resource auction: $65.\n' +
      '\n' +
      'Finally provide your bid. Answer: the bidding will you choose.\n' +
      '\n' +
      '**Second Output:**\n' +
      '\n' +
      'After considering the game expert\'s advice, 1 have decided to adjust my bid for today\'s water resource auction. 1 will increase my bid to $65, as this gives me a better chance of winning the auction and reduces the risk of a tie.\n' +
      '\n' +
      'My final bid for today\'s water resource auction: $65.\n' +
      '\n' +
      '#### c.2.6 K-Level Reasoning\n' +
      '\n' +
      'In our experiment, we implemented two LLM self-refinement reasoning methods: Reflect (Madaan et al., 2023) and Refine (Shinn et al., 2023), and noticed that Refine performed significantly better than Reflect in the experimental results. To further explore the differences in performance between these two methods, we analyzed their respective working principles and applicable scenarios.\n' +
      '\n' +
      'Figure 9: Illustration of Reflect and Refine methods in the Guessing 0.8 of the Average game.\n' +
      '\n' +
      'The Reflect method involves making decisions first, then summarizing the experience based on feedback from the environment. This method may be effective in scenarios where the environment does not change much or where the decision-making cycle is long, as it allows for quick decision-making. However, its drawback is that, in dynamic environments, the experience from the previous round may not be suitable for the next round. In fact, in the Survival Auction Game (SAG), a rapidly changing environment, the survival rate of the Reflect method is even lower compared to making direct decisions. This is likely because this method does not sufficiently take into account the dynamic nature of the environment.\n' +
      '\n' +
      'In contrast, the Refine method involves multiple analyses before making a decision, including an initial analysis and improvements to that initial analysis. Importantly, both of these analyses are conducted in the context of the current decision-making environment. This makes the Refine method more adaptable to dynamic environments, as it can consider real-time changes in the current environment, thus making more accurate decisions.\n' +
      '\n' +
      'In summary, the reason why the Refine method performed better in our experiment is mainly that it adapts better to rapidly changing dynamic environments.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>