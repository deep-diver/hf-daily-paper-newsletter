<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '환경. 우리는 그림 2에서 볼 수 있듯이 "평균의 0.8을 추측하는 것"과 "생존 경매 게임"이라는 두 가지 파일럿 과제를 제시하며, 두 게임 모두 동적 환경에서 널리 퍼진 복잡한 의사 결정 과정의 마이크로코즘 역할을 하는 동시에 LLM의 동적 추론 능력을 평가하기 위한 테스트 베드를 제공하기 위해 잘 정의되어 있다.\n' +
      '\n' +
      '광범위한 시뮬레이션과 평가를 통해 정적 시나리오에서 성공적인 기존 추론 방법이 파일럿 도전이 제시하는 동적 설정에서 성능을 유지하기 위해 어려움을 겪는 것을 관찰한다. 다른 사람들의 관점, 사고 패턴 및 행동을 예측하는 것은 동적 환경에서 최적의 의사 결정을 위해 중요하다. 이 개념은 전략적 깊이의 수준을 설명하는 재귀적 추론 과정인 k-level thinking1(Nagel, 1995; Stahl II Wilson, 1994)을 참조한다. 기존의 방법들은 k-level 사고를 사용하지 않고 다른 참여자들의 행동을 예측하지 못하여 차선의 의사 결정으로 이어진다.\n' +
      '\n' +
      '각주 1: 인지 계층 이론에 따르면, k-레벨 사고는 경쟁자들이 무엇을 할 가능성이 있는지, 그들이 무엇을 할 것이라고 생각하는지, 그리고 그들이 무엇을 할 것이라고 생각하는지 등을 고려하는 것을 포함한다.\n' +
      '\n' +
      '이러한 한계를 극복하기 위해 경쟁자의 관점을 의사 결정 과정에 재귀적으로 채택하여 k-수준 사고에 접근하는 "K-수준 추론"이라는 새로운 추론 방법을 소개한다. 재귀적 추론 프로세스는 전략적 환경에 대한 더 깊은 이해를 가능하게 하고 LLM이 경쟁자의 가능한 반응을 고려한 더 많은 정보에 입각한 결정을 내릴 수 있게 하여 전통적인 추론 방법보다 두 파일럿 과제 모두에서 훨씬 더 나은 결과를 얻을 수 있다.\n' +
      '\n' +
      '이 작업의 기여도를 요약하면 다음과 같다.\n' +
      '\n' +
      '* 게임 이론 관점에서 대형 언어 모델의 동적 추론 능력을 연구하고 두 가지 시범 과제를 소개한다. 두 작업 모두 실제 동적 의사 결정의 복잡성을 반영하며 LLM의 동적 추론 능력을 평가하기 위해 잘 정의되어 있다.\n' +
      '* "K-Level Reasoning" 방법. 인지 계층 이론을 추론 과정에 통합하여 LLM이 경쟁적이고 역동적인 시나리오에서 경쟁자의 생각과 행동을 재귀적으로 예측하고 대응할 수 있도록 권한을 부여한다.\n' +
      '\n' +
      '##2 문제 정의\n' +
      '\n' +
      '### LLMs를 이용한 동적 추론\n' +
      '\n' +
      '다수의 개별적 상호 작용과 끊임없이 변화하는 환경을 특징으로 하는 동적 추론 문제는 보다 복잡한 시나리오를 제시한다. 도 1에 도시된 바와 같이, 두 가지 주요 측면들이 이러한 과제들을 정의한다:\n' +
      '\n' +
      '***동적 환경**: 환경은 동적으로 진화하며, 의 결정 및 행동에 의해 영향을 받는다\n' +
      '\n' +
      '그림 2: 본 논문의 두 가지 동적 추론 문제의 일러스트레이션. **왼쪽:** 평균의 0.8을 추측합니다. 10라운드 게임을 포함하며, 각 라운드의 승자는 선택한 숫자가 모든 선수가 선택한 평균 숫자의 80%에 가장 가까운 선수이다. 이 도전은 플레이어가 집단적 시장 행동을 예측해야 하는 시장 예측의 역학을 반영한다. \'생존 경매 게임\' 플레이어는 부족한 생활 자원을 확보하기 위해 매일 경매에 입찰하고 10일 동안 생존하기 위해 건강과 재정의 균형을 맞추고 자원 부족 시나리오와 재정적 의사 결정을 시뮬레이션한다.\n' +
      '\n' +
      ' 개인들.\n' +
      '**적응 전략**: 개인은 환경 변화에 대응하여 지속적으로 전략을 조정하고 최적화합니다.\n' +
      '\n' +
      '전략적 추론(Bakhtin et al., 2022)과 같은 유사한 작업은 경쟁적이거나 협력적인 맥락에서 여러 참가자를 포함하지만 동적 추론은 환경의 변화와 상호 작용에 더 많이 적응하는 것을 강조한다.\n' +
      '\n' +
      '초기 환경은 \\(E^{0}\\)으로 설정하였다. 단계 \\(t\\)에서 모든 개체의 집합적 결정 \\(d^{t}_{i}\\)에 대응하여 환경은 \\(E^{t+1}\\)으로 진화한다. 이러한 변화는 개인의 행동의 누적적 영향을 반영한다. 결과적으로, 최적의 의사결정을 목표로 하는 개인은 \\(d^{t+1}_{i}\\)으로 표시되는 최신 환경상태인 \\(E^{t+1}\\)을 고려해야 하며, 또한 다른 사람들의 역사적, 잠재적 미래 행동을 고려해야 한다. (\\(\\mathbf{D}\\)\n' +
      '\n' +
      '{split} d^{t}_{i}&=f(E^{0},\\mathbf{D}^{0} \\Rightarrow E^{1},\\ldots,\\mathbf{D}^{t-1}\\Rightarrow E^{t}&=\\mathbf{D}^{t}&=\\{d^{t}_{i},i\\in N\\}\\\\E^{t+1}&=\\mathrm{Transfer}(E^{t},\\mathbf{D}^{t}}^{t}}\\end{split}\\tag{1}\\\\mathbf{D}^{t}&=\\mathrm{Transfer}(E^{t},\\mathbf{D}^{t}}}\\tag{1}\\mathbf{D}^{t}}\\mathbf{D}^{t}&=\\mathrm{Transfer}(E^{t},\\mathbf{D}^{t}}\n' +
      '\n' +
      '이러한 동적 상호 작용은 LLM이 현재 데이터를 처리할 뿐만 아니라 진화하는 전략과 환경 변화를 예측하고 적응해야 할 필요성을 강조한다.\n' +
      '\n' +
      '### Pilot Tasks\n' +
      '\n' +
      'LLM의 동적 추론 능력을 공식적으로 연구하고 정량적으로 연구하기 위해 게임 이론 문제에서 영감을 얻은 두 가지 게임 설정, 즉 평균 0.8A 추측(G0.8A)과 생존 경매 게임(SAG)을 탐구한다. 두 작업 모두 잘 정의되고 제어 가능하여 LLM의 기능에 대한 명확한 테스트를 보장합니다. 구체적인 게임 설정과 프롬프트는 부록 A에서 확인할 수 있다.\n' +
      '\n' +
      '####2.2.1 평균(G0.8A)의 0.8을 추측함\n' +
      '\n' +
      '평균의 0.8을 추측하는 것(그림 2 왼쪽)은 Alain Ledoux(Ledoux, 1981)에 의해 소개된 고전적인 게임이다. 10라운드 게임을 포함하며, 각 플레이어는 1에서 100 사이의 숫자를 선택합니다. 목표는 그룹 평균 선택의 80%에 가장 가까운 숫자를 선택하는 것입니다. 핵심 아이디어는 다른 사람들이 평균을 어떻게 추정하여 제출할 수를 결정하는지 추측하는 것이다.\n' +
      '\n' +
      '이 아이디어는 참가자 100명 중 가장 매력적인 얼굴 6명을 뽑는 경연대회인 케인지언 뷰티 콘테스트에서도 예시되며, 최종 결과와 일치하는 선택이 승리한다. 케인즈의 1936년 작품 "고용, 이익, 돈의 일반 이론"(케인즈, 1936)에서 원작으로 언급했듯이, "자신의 판단에서 가장 예쁘고, 심지어 평균 의견이 가장 예쁘다고 생각하는 것은 [얼굴]을 선택한 경우가 아니다." 우리는 평균 의견이 어떤 것을 기대하는지 예상하기 위해 지능을 바치는 3단계에 도달했다.\n' +
      '\n' +
      '이 게임은 금융 시장의 집단적 행동을 예상하는 도전을 반영한다. 투자자는 자산의 가치뿐만 아니라 다른 사람들이 미래에 어떻게 가치를 매길 것이라고 믿는지도 예측해야 한다.\n' +
      '\n' +
      '####2.2.2 생존 경매 게임(SAG)\n' +
      '\n' +
      '(Mao et al., 2023)에서 제안된 물 할당 챌린지에서 파생된 생존 경매 게임(그림 2 Right)은 심각한 가뭄과 씨름하는 가상의 마을에서 설정된다. 수자원을 입찰하고 건강포인트를 0 이상으로 유지함으로써 달성되는 10일간의 가뭄 기간 생존이 주민 개개인의 목표다. 만약 선수가 수자원을 성공적으로 입찰한다면, 그들은 건강 점수를 얻게 될 것이고, 그렇지 않다면, 그들은 수자원을 얻지 못한 연속적인 일수 \\(n\\)와 같은 건강 점수를 잃게 될 것이다.\n' +
      '\n' +
      '경매 시스템과 건강 포인트 메커니즘의 통합은 플레이어가 건강과 재정의 균형을 유지해야 하는 역동적인 환경을 만든다. 그것은 그들의 전략적 자원 관리 기술과 다른 사람들의 경쟁적 행동을 예측하고 대응하는 능력에 도전한다.\n' +
      '\n' +
      '### Metrics\n' +
      '\n' +
      '동적 추론 시나리오에서 LLM과 LLM 에이전트의 성능을 효과적으로 측정하기 위해 4가지 메트릭으로 구성된 종합 평가 시스템을 구축한다. 이러한 작업이 잘 정의되어 있기 때문에 메트릭은 간단합니다.\n' +
      '\n' +
      '**승률**은 경기진행에 대한 승수를 기준으로 계산되어 전체적인 능력의 척도를 제공한다.\n' +
      '\n' +
      '\\[\\mathrm{WinRate}=\\frac{\\mathrm{Num\\ of\\Wins}{\\mathrm{Total\\Round\\ per\\ Test\\times Num\\ of\\ Test} \\tag{2}\\\n' +
      '\n' +
      '탈락이 요인이 아닌 G0.8A와 같은 게임에서 승률은 중요한 척도이다.\n' +
      '\n' +
      '**평균 생존 라운드**는 플레이어가 게임에 남아 있는 평균 라운드를 계산한다. 그것은 SAG와 같은 제거 기반 게임에서 성능을 평가하는 효과적인 방법이다.\n' +
      '\n' +
      '\\\\[\\mathrm{AvgSurvivalRound}=\\frac{\\sum\\mathrm{Survival\\Round\\ in\\ Each\\ Test}{\\mathrm{Num\\ of\\ Test}}\\tag{3}\\\\\n' +
      '\n' +
      '**적응지수**는 시간이 지남에 따라 플레이어의 적응력과 경기력 향상 능력을 평가한다. 라운드 전반기의 최적 전략으로부터의 편차와 후반기의 편차를 비교하여 결정한다.\n' +
      '\n' +
      'G0.8A의 적응 지수=\\frac{\\mathrm{Target\\ Deviation\\@\\(second\\half)}{\\mathrm{Target\\ Deviation\\@\\(first\\half)}}{tag{4}\\]에 대해, 플레이어가 선택한 수와 목표 수 사이의 편차를 계산한다. SAG의 경우, 최적의 낙찰가가 상대의 최고 입찰가보다 약간 높을 뿐임을 고려하여, 대리인의 낙찰가와 상대의 최고 입찰가의 절대적 차이를 계산한다.\n' +
      '\n' +
      '**예측 정확도**는 라이벌의 미래 행보에 관한 플레이어의 예측의 정확도를 평가한다. G0.8A에서, 그것은 각 라운드에서 플레이어의 예측된 평균과 실제 평균 사이의 절대 차이를 계산하는 것을 포함한다:\n' +
      '\n' +
      '\\[\\mathrm{Pred\\ Acc}=\\frac{\\sum|\\mathrm{Avg}_{Pred}-\\mathrm{Avg}_{ Actual}|}{\\mathrm{Num\\ of\\ Test} \\tag{5}\\\\tag{\n' +
      '\n' +
      'SAG에서, 포커스는 상대들에 의한 최고 입찰에 대한 플레이어의 예측과 그들이 행한 실제 최고 입찰 사이의 절대 오차를 측정하는 것으로 이동한다.\n' +
      '\n' +
      '\\[\\mathrm{Pred\\ Acc}=\\frac{\\sum|\\mathrm{Max\\Bid}_{Pred}-\\mathrm{Max\\Bid}_{Actual}|}{\\mathrm{Num\\ of\\ Test} \\tag{6}\\]\n' +
      '\n' +
      '##3 K-레벨 추론과 LLMs\n' +
      '\n' +
      '인지 계층 이론(Stahl, 1993)은 동적, 상호작용적, 경쟁적 시나리오에서 의사 결정을 이해하기 위한 기본 프레임워크를 제공한다. 플레이어는 합리성과 전략적 깊이에 따라 자신과 타인을 다른 수준으로 분류한다.\n' +
      '\n' +
      '그림 3(왼쪽)과 같이 1단계 사고의 플레이어는 타인의 행동을 고려하지 않고 환경에 직접 반응한다. 두 번째와 세 번째와 같은 더 높은 사고 수준의 선수들은 다른 선수들이 더 낮은 수준의 전략적 사고에서 작동하고 있다는 가정하에 그들의 결정을 기반으로 한다. 각 플레이어가 전략적 추론 수준을 이해하고 예상하여 다른 플레이어를 능가하려고 시도하는 이러한 **재귀적** 추론 프로세스는 전략적 상호 작용에 대한 계층화된 이해를 가능하게 한다.\n' +
      '\n' +
      '우리는 동적 도전에서 LLM의 추론 능력을 향상시키기 위해 이 아이디어를 도출하고 "K-레벨 추론"이라고 하는 새로운 재귀적 동적 추론 접근법을 제안한다. 도 3(Right)에 예시된 바와 같이, 이 방법은 이용 가능한 이력 정보를 사용하여 상대의 행동을 재귀적으로 시뮬레이션하고, 이어서 상대의 행동 모델링에 기초하여 최적의 행동을 예측하는 것을 포함한다.\n' +
      '\n' +
      'K-레벨 추론 프로세스는 다음과 같이 공식화된다:\n' +
      '\n' +
      'Initial condition:\n' +
      '\n' +
      '\\[d^{t,1}=f(E^{t}) \\tag{7}\\]\n' +
      '\n' +
      'Recursive formula:\n' +
      '\n' +
      '\\[d^{t,k}=f(E^{t},\\hat{d}_{1}^{t,k-1},...,\\hat{d}_{n}^{t,k-1}) \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(d^{t,k}\\)는 k단계 사고에서 \\(t\\) 단계에서 내린 결정을 나타낸다. \\ (f\\)는 입력에 기초하여 결정을 생성하는 LLM을 나타낸다. \\ (\\hat{d}_{1}^{t,k-1},...,\\hat{d}_{n}^{t,k-1}\\)은 추론 수준에서 다른 플레이어(1~\\(n\\))의 예상 결정을 나타낸다. 이 과정을 \\(k-2\\) 수준의 모델링을 통해 적용하여 \\(\\hat{d}_{1}^{t,k-1}\\)을 재귀적으로 유도할 수 있다.\n' +
      '\n' +
      'K-Level Reasoning을 LLM에 적용하면 동적 및 전략적 시나리오를 이해하는 능력이 크게 향상될 수 있다. 상대방의 행동을 시뮬레이션하고 최적의 반응을 예측함으로써 LLM은 다른 참가자의 전략에 효과적으로 적응할 수 있다. 그러나 더 깊은 모델링이 항상 더 나은 결과로 이어지는 것은 아닙니다. 최적의 결정은 이해도가 그들보다 한 단계 높을 때 일어난다.\n' +
      '\n' +
      '그림 3: **Left**: K-level thinking은 재귀적 추론 과정이다. 1단계 사고에서 개인은 정적인 추론과 유사하게 환경에 직접적으로 반응한다. 2단계 사고에서 개인은 타인의 1단계 사고 등을 고려한다. **Right**: K-Level Reasoning에서 LLM은 경쟁자의 관점을 채택하여 이용 가능한 역사적 정보를 바탕으로 k-Level 사고를 재귀적으로 채용한다. 라이벌의 후속 행보에 대한 예측 정확도를 크게 향상시키고 보다 전략적인 의사 결정을 알려줍니다.\n' +
      '\n' +
      '경쟁사. 과대 사고는 또한 상당한 편견과 오류를 유발할 수 있다. 우리는 재귀 깊이가 실험에서 결정에 어떤 영향을 미치는지 탐구할 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Base Techniques\n' +
      '\n' +
      '우리는 동적 도전 내에서 추론 방법을 탐색하기 위해 원래 전통적인 추론 벤치마크에서 다양한 접근법을 적용한다. 이러한 베이스 기술들은 다음과 같다.\n' +
      '\n' +
      '**표준 프롬프트(Direct)**: 이것은 LLM이 주어진 게임 설정 프롬프트에 응답하여 최종 응답(Action)을 생성하는 종래의 프롬프트 방법이다.\n' +
      '\n' +
      '**Chain-of-Thought(CoT)**(Wei et al., 2022): 이 접근법은 제로-샷 네이티브 Chain-of-Thought 추론 방법을 채용한다(Kojima et al., 2022).\n' +
      '\n' +
      '**페르소나 프롬프팅(Persona)**(Deshpande et al., 2023): 이 기술은 LLM의 추론 능력을 증강시키기 위해 "Game Expert" 페르소나 정보를 통합함으로써 표준 프롬프팅 프로세스를 수정한다.\n' +
      '\n' +
      '**Reflexion(Reflect)**(Shinn et al., 2023): 이 방법은 언어 강화 학습을 갖는 언어 에이전트의 개념을 지칭한다. C.1.2에서 자세히 설명하는 수정과 함께 동적 작업에 대해 특별히 조정되었다.\n' +
      '\n' +
      '**Self-Refine(Refine)**(Madaan et al., 2023): 이것은 최종 결정에 도달하기 전에 추가적인 LLM이 코멘트 및 조정을 제공하는 다중 라운드 반복 추론 접근법이다. 셀프 리파인(Self-Refine)과 리플렉트(Reflect)의 차이는 부록 D에 자세히 설명되어 있다.\n' +
      '\n' +
      '**생각 예측 체인(PCoT)**: 이것은 우리가 제안한 강력한 기준선이다. PCoT는 CoT로부터 벗어나면 LLM이 상대방의 행동을 명시적으로 예측해야 하며, 또한 상대방의 행동을 예측하는 것을 강조하지만, PCoT는 맥락에 기반한 직접적인 예측에 더 가깝고, K-Level Reasoning은 상대방의 행동을 예측하기 위한 재귀적이고 계층화된 접근 방식을 포함한다.\n' +
      '\n' +
      '이러한 기술의 구현 및 구체적인 예에 대한 자세한 내용은 부록 C를 참조하십시오.\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '특정 접근법의 동적 추론 능력을 평가하기 위해 제어 가능한 환경을 설정하고 플레이어(1차 초점)와 상대 역할의 두 역할을 구별한다. 매 경기마다 **1** 플레이어와 **4** 상대가 있다. "플레이어"는 특정 방법을 탑재할 것이며, 모든 상대는 동일한 추론 방식을 사용한다. 이 잘 정의된 설정은 방법 간의 동적 추론 능력의 보다 명확한 비교를 구성한다. 상기 각 설정에 대해,\n' +
      '\n' +
      '그림 4: 평균 게임의 추측 0.8에서 다양한 방법의 그림. **Left**: Thain of Thought에서 LLM은 추론 논리를 단계별 형식으로 출력한다. 그러나 LLM은 상황에 대한 이해가 부족하고 상대방의 행동에 대한 예측이 부족하다는 것을 보여준다. **중간**: 예측 사상 사슬에서 경쟁자의 다음 움직임에 대한 예측을 하기 위한 명시적인 요구 사항과 함께 LLM은 추론을 예측 단계와 추론 단계로 명확하게 나눈다. 그러나, 그 예측들은 여전히 다소 편향되어 있다. **Right**: K-Level Reasoning에서 LLM은 공공의 역사적 정보를 가지고 경쟁자들의 다음 행보를 재귀적으로 예측한다. 구현은 식 7&8을 기반으로 하며 경쟁사보다 더 깊은 전략적 깊이 덕분에 예측과 결정이 더 정확하다.\n' +
      '\n' +
      '실험이 10회 반복되어 유의미한 결과를 보장합니다. 한편, 우리는 추론 방법과 몇 가지 프로그램 전략 패턴을 참조로 비교했다. 실험 결과는 부록 B에서 확인할 수 있다.\n' +
      '\n' +
      '모든 방법은 GPT-4 (Achiam et al., 2023) (gpt4-32k)를 사용하여 구현하였으며, 온도는 \\(0.7\\)으로, top-p는 \\(0.9\\)으로 설정하였다. 특별한 규정이 없는 한 K-Level Reasoning에서의 사고 수준은 K=2로 설정한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '평균 0.8을 계산하면 평균 4.3.1\n' +
      '\n' +
      '**승률**플레이어 승률의 분석(표 1)은 동적 의사결정 환경에서 K-Level Reasoning(K-R) 방법의 우수성을 입증한다. 전략적 효과는 상대의 움직임을 예측할 수 있는 능력에 있으며, 다른 촉진 방법을 능가합니다.\n' +
      '\n' +
      '**적응 지수** 게임의 전반부와 후반부 사이의 플레이어 선택에서 평균 절대 편차의 비율로 계산된 적응 지수(표 2)는 통찰력 있는 경향을 드러낸다. 상승률과 직접적인 상관관계가 없음에도 불구하고 PCoT와 K-R 접근법은 상당한 학습 진전을 보여 최적의 전략을 향한 적응적인 움직임을 시사한다. 반면 CoT 및 Refine 방법은 제한된 학습 진행을 나타내어 융통성 없는 사고 과정이 적응력을 방해할 수 있음을 나타낸다.\n' +
      '\n' +
      '**예측 정확도** 초기, K-R은 PCoT(그림.5)보다 더 정확하고 덜 랜덤한 예측으로 시작하여 더 높은 예측 정확도를 표시한다. 게다가, 그 예측은 매우 빠르게 수렴되었고 경기 후반에 꽤 정확했다. 이러한 경향은 LLM이 더 많은 게임 플레이 컨텍스트로 전략적 깊이를 이해하는 데 능숙함을 강조한다. K-R의 우수한 장기 예측 정확도는 LLM의 내재적 맥락 학습 능력과의 정렬을 시사한다.\n' +
      '\n' +
      '######4.3.2 생존경매게임\n' +
      '\n' +
      '**평균 생존 라운드** 생존 경매 게임에서 다른 추론 방법의 성능(표 3)은 K-Level Reasoning 방법이 다른 접근법에 비해 생존율을 높인다는 것을 보여준다. G0.8A에서의 성능과 달리 PCoT는 여기서 뚜렷한 장점을 보이지 않는다.\n' +
      '\n' +
      '**적응 지수** K-레벨 추론은 가장 높은 적응 지수(표 4)를 나타내어 게임의 후반부에서 최소한의 추가 오버헤드 및 우수한 적응 기술을 나타낸다. 이 측정법은 또한 특히 경기 전반기의 저조한 성적이 조기 탈락으로 이어질 수 있기 때문에 이 방법의 놀라운 적응력을 드러낸다. 특정 조건에서 일부 방법이 경기 후반에 도달할 수 없다는 점은 K-R의 적응력을 더욱 강조한다.\n' +
      '\n' +
      '**예측 정확도** 생존 경매 게임에서 가장 높은 상대 입찰가에 대한 예측 정확도(도 6)는 이러한 도전의 증가된 복잡성을 강조한다. PCoT와 K-R 모두 상대 균형, 건강 상태 및 입찰 스타일과 같은 여러 변수를 고려해야 하기 때문에 입찰을 정확하게 예측하는 데 어려움을 겪었지만 K-R은 지속적으로 더 작은 예측 편차를 보여 복잡한 시나리오에서 그 효과를 강조했다.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      '그림 4는 "평균 0.8을 추측하는" 게임에서 서로 다른 추론 접근법이 어떻게 기능하는지를 보여준다. 상기 제어부는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.43 & 0.67 & 0.62 & 0.53 & 0.43 & 0.61 & **0.82** \\\\ CoT & 0.07 & 0.32 & 0.35 & 0.14 & 0.22 & 0.45 & **0.63** \\\\ Persona & 0.05 & 0.37 & 0.29 & 0.05 & 0.37 & **0.11** & **0.46** \\\\ Reflect & 0.42 & 0.68 & 0.63 & 0.39 & 0.64 & 0.74 & **0.78** \\\\ Refine & 0.10 & 0.34 & 0.32 & 0.31 & 0.23 & 0.22 & **0.46** \\\\ PCoT & 0.03 & 0.44 & 0.52 & 0.21 & 0.51 & 0.54 & **0.85** \\\\ K-R(**ours**) & 0.04 & 0.15 & 0.14 & 0.04 & 0.17 & 0.14 & **0.52** \\\\ \\hline Average & 0.16 & 0.42 & 0.41 & 0.24 & 0.37 & 0.40 & **0.65** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 평균 게임의 0.8을 추측하는 다른 상대에 대한 플레이어의 승률. 특정 게임에서 여러 플레이어의 동시 승리로 인해 각 메소드의 승률이 0.2를 초과하고, 각 메소드의 승률을 비교 기준선으로 설정했다. 이보다 훨씬 높은 승률은 더 강력한 동적 추론 능력을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.60 & 1.47 & 0.64 & 0.71 & 1.27 & 0.76 & **0.44** \\\\ CoT & 0.44 & 0.67 & 0.40 & **0.31** & 0.58 & 0.40 & 0.45 \\\\ Persona & 0.43 & 0.47 & 0.34 & 0.37 & 0.62 & 0.32 & **0.25** \\\\ Reflect & 0.62 & 0.60 & 0.65 & 0.60 & 0.87 & 0.53 & **0.26** \\\\ Refine & 0.41 & 0.57 & **0.28** & 0.36 & 0.55 & 0.42 & 0.30 \\\\ PCoT & 0.50 & 0.60 & 0.54 & 0.31 & 0.57 & 0.32 & **0.23** \\\\ K-R(**ours**) & 0.61 & 0.60 & 0.49 & 0.48 & 0.74 & 0.47 & **0.25** \\\\ \\hline Average & 0.51 & 0.71 & 0.48 & 0.45 & 0.74 & 0.46 & **0.31** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 평균의 0.8을 추측하는 다른 방법의 적응 지수. 표의 값이 낮을수록 적응력이 강하다는 것을 나타냅니다. 비교의 용이성을 위해 각 행의 최적값은 **굵은**이고 두 번째로 좋은 값은 밑줄이 그어져 있다.\n' +
      '\n' +
      '그림 5: PCOT와 K-Level Reasoning 사이의 G0.8A 동안 상대를 예측하는 절대 오차.\n' +
      '\n' +
      'COT와 PCOT 방법은 상대의 움직임을 이해하고 예측하는 데 한계가 있어 편향되거나 덜 정확한 의사결정을 하게 된다. 반면 K-Level Reasoning은 과거 데이터를 재귀적으로 활용하여 상대의 행동과 사고 수준에 대한 보다 미묘한 예측을 함으로써 정확성과 전략적 깊이를 높일 수 있다.\n' +
      '\n' +
      '## 5 Rethinking K-Level 추론\n' +
      '\n' +
      '### 더 나은 추론 방법론 VS 더 강한 기초 모델\n' +
      '\n' +
      '더 많은 데이터로 훈련되고 더 큰 매개변수 크기를 보유하는 LLM이 더 강력한 추론 능력을 입증한다는 합의가 있다. 우리는 K-Level Reasoning이 상대적으로 약한 LLMs의 동적 추론 능력을 크게 향상시킬 수 있는지 탐구한다. 따라서 본 논문에서는 K-Level 추론과 GPT-3.5(K-R[GPT-3.5])의 성능을 비교하고, GPT-4를 기반으로 한 다른 추론 방법의 성능을 비교하기 위해 실험을 수행하였으며, 모든 실험을 10회 반복하였다.\n' +
      '\n' +
      '표 5의 결과로부터 GPT4(Direct[GPT4])의 표준 프롬프트 방법에 비해 K-R[GPT-3.5]의 성능이 우수함을 알 수 있다. 또한 GPT4에서 추론 방법을 갖춘 상대와 전투할 때 K-R[GPT-3.5]는 놀라운 능력을 보여준다. K-R은 동적 도전과 경쟁자의 관점 회복에 대한 우수한 공식화를 통해 동적 추론에서 LLM의 능력을 크게 향상시킨다. 결과는 고급 추론 방법론을 통합하는 것이 LLM의 동적 추론을 향상시키는 데 중요한 요소가 될 수 있음을 의미한다.\n' +
      '\n' +
      '더 깊은 사고 수준, 더 나은 전략적인 성과?\n' +
      '\n' +
      'K-Level Reasoning 접근법은 경쟁자의 사고 과정에 대한 재귀적 모델링으로 구분된다. 우리는 사고 수준이 추론 결과에 어떤 영향을 미치는지 궁금하다. 따라서 두 경기에서 K-R[K=2]과 K-R[K=3]의 성능을 비교한다. K-R[K=3]은 모든 반대자들이 인지적 위계의 두 번째 수준에 있다고 가정한다. 즉, 상대 예측을 시뮬레이션할 때 상대는 K-R[K=2] 추론 접근법을 사용한다. 표 6에 자세히 설명된 결과는 사고 수준의 깊이 증가가 미치는 영향에 대한 흥미로운 그림을 보여준다.\n' +
      '\n' +
      'K-R[K=3]은 1단계 사고라고 할 수 있는 Direct 방법을 반대할 때 G0.8A에서 감소된 승률을 보였고 SAG에서는 성과를 유지했다. 이것은 추론의 과도한 깊이에 대한 잠재적인 단점을 시사하며, 아마도 과잉 사고를 나타낼 수 있다. 그러나 K-R[K=2] 상대를 상대로 한 K-R[K=3]은 두 경기 모두에서 큰 향상을 보였다. 이러한 개선은 추론 과정에서 한발 앞서 있다는 장점을 보여준다.\n' +
      '\n' +
      '이 병치는 설득력 있는 통찰력으로 이어진다: K-수준 추론에서 중요한 요소는 단순히 K-수준 추론의 깊이가 아니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 0.86 & 1.09 & 2.07 & 2.36 & 1.82 & **0.39** & 1.24 \\\\ CoT & 1.32 & 1.56 & 1.29 & - & 2.27 & **0.67** & 1.27 \\\\ Persona & 1.19 & 1.62 & 1.30 & 2.31 & 1.08 & 1.79 & **1.04** \\\\ Reflect & 1.00 & 2.02 & 1.21 & **0.66** & 1.06 & 1.21 & 1.18 \\\\ Refine & 1.43 & 1.69 & 1.95 & 2.03 & 1.74 & 1.73 & **1.21** \\\\ PCOT & 1.60 & 1.37 & 1.05 & 0.90 & 1.69 & **0.83** & 1.19 \\\\ K-R(**ours**) & - & **0.64** & 1.27 & - & 0.70 & - & 2.27 \\\\ \\hline Average & - & 1.43 & 1.45 & - & 1.48 & - & **1.34** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 생존 경매 게임에서 다양한 추론 방법의 적응 지수. 표에서 누락된 부분은 이 방법에 대한 기록이 후반전까지 남아 있지 않음을 나타내기 위해 “-”로 표시된다. 이러한 결측 데이터는 평균 값 계산에 포함되지 않았습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline Direct & 5.90 & 7.00 & 7.50 & 4.70 & 8.70 & 6.60 & **9.40** \\\\ CoT & 5.70 & 6.50 & 5.30 & 4.00 & 8.10 & 5.30 & **10.00** \\\\ Persona & 5.70 & 7.70 & 7.40 & 5.20 & 6.30 & 7.20 & **9.30** \\\\ Reflect & 9.40 & 9.40 & 9.90 & 5.20 & 8.60 & 8.20 & **10.00** \\\\ Refine & 6.30 & 6.40 & 8.10 & **4.30** & **8.20** & 5.30 & 7.90 \\\\ PCOT & 8.50 & 9.60 & 9.90 & 6.30 & 8.50 & 6.20 & 7.90 \\\\ K-R & 4.10 & 5.50 & 5.00 & 4.04 & 5.70 & **4.40** & **6.80** \\\\ \\hline Average & 6.51 & 7.44 & 7.59 & 4.82 & 7.73 & 6.17 & **9.01** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 생존 경매 게임에서 플레이어의 다른 상대와의 평균 생존 라운드. 대각선에 표시된 대로 각 방법의 평균 생존 라운드를 비교 기준선으로 설정합니다. 이보다 훨씬 높은 생존 라운드는 더 강력한 동적 추론 능력을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c|c c c} \\hline \\hline \\multicolumn{3}{c|}{_Games 0.8 of the Average_} & \\multicolumn{3}{c}{_Survival Auction Game_} \\\\ \\hline Opponent & Direct & K-R & Direct & K-R & Direct & K-R & Direct & K-R & Direct & K-R \\\\ \\hline \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} Direct \\\\ (GPT-3.5) \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-3.5] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{l} GPT-4] \\\\ \\end{tabular} } \\\\ \\hline Direct & 0.18 & 0.18 & 0.43 & 0.82 & 5.00 & 0.90 & 5.90 & 0.90 \\\\ CoT & 0.14 & 0.37 & **0.00** & 0.63 & 5.00 & 8.10 & 5.70 & **10.00** \\\\ Persona & 0.10 & 0.23 & **0.05** & 0.46 & 5.00 & 7.50 & 5.70 & 9.30 \\\\ Reflect & 0.24 & 0.38 & 0.42 & **0.78** & 5.00 & 8.50 & **9.40** & **10.00** \\\\ Refine & 0.14 & 0.13 & **0.10** & 0.46 & 5.10 & 6.70 & 6.30 & 7.90 \\\\ PCOT & 0.19 & 0.46 & **0.03** & **0.35** & **4.10** & 6.80 & 8.50 & **9.70** \\\\ \\hline Average & 0.16 & 0.29 & 0.18 & 0.67 & **1.49** & 7.83 & 6thought, but the relative depth compared to the opponent. Having a one level deeper thinking-level offers a strategic advantage, as seen in the superior performance of K-R[K=2] over Direct and K-R[K=3] over K-R[K=2]. However, advancing two levels beyond the opponents, as in the case of K-R[K=3] versus Direct, may result in diminishing returns due to the complexity of over-anticipation.\n' +
      '\n' +
      '역동적 도전에서 상대의 정확한 사고 수준을 파악하는 것은 어려운 일이다. 사고 수준에 동적으로 적응하고 더 깊은 분석을 위해 K-수준 추론을 사용하는 것은 향후 연구에 귀중한 방향을 제시한다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '### LLMs를 이용한 추론\n' +
      '\n' +
      '대언어 모델(LLMs)은 수학적(Miao et al., 2021; Patel et al., 2021), 상식(Talmor et al., 2022; Bhakthavatsalam et al., 2021), 상징적 추론(Srivastava et al., 2022; Suzgun et al., 2022)과 같은 다양한 복잡한 추론 과제에서 탁월하다. 주목할 만한 추론 접근법은 복잡한 질문들을 일련의 중간 단계들로 분해하는 것을 포함한다(CoT(Chain-of-Thought) 방법(Wei et al., 2022; Kojima et al., 2022). 그 후, CoT를 확장하기 위한 몇 가지 작업이 등장했으며, ToT(Tree of Thought)(Yao et al., 2023), GoT(Graph of Thought)(Besta et al., 2023) 및 Skeleton-of-thought(Ning et al., 2023)와 같은 혁신이 있었다. 또한 Self-Refine(Madaan et al., 2023) 및 Reflexion(Shinn et al., 2023)과 같은 접근 방식은 LLMs 검토를 통해 CoT의 일관성을 높이고 응답을 개선한다. 더욱이 최근 연구에 따르면 페르소나 정보를 LLM에 통합하면 추론 프로세스가 크게 개선된다(Deshpande et al., 2023). LLM 추론 과정의 합리성과 지식 능력을 향상시키기 위해 더 많은 페르소나 정보를 통합하기 위한 일련의 연구(Fu et al., 2023; Wang et al., 2023)가 수행되었다. 그러나 포괄적인 테스트 결과 이러한 모든 방법이 동적 문제에 적합하지 않은 것으로 나타났다.\n' +
      '\n' +
      '최근 전략적 추론에서 대규모 언어 모델(LLM)에 대한 연구가 진행되고 있다(Gandhi et al., 2023; Bakhtin et al., 2022). 이러한 접근법들은 우리의 제안된 예측 체인-사고 기준선과 같이 전략적 태스크의 복잡성을 LLM이 인식하도록 촉구하는 것을 포함한다. 그러나 우리의 실험 결과는 이 접근법이 재귀적이고 더 깊은 전략적 사고에 필요한 명확한 인지 계층을 확립하지 못한다는 것을 나타낸다. 본 논문에서는 LLMs-K-Level Reasoning을 이용한 첫 번째 재귀적 동적 추론 방법을 제안하고, 철저한 경험적 증거를 통해 그 우수성을 입증한다.\n' +
      '\n' +
      '###기계학습의 동적 문제\n' +
      '\n' +
      '동적 문제는 여러 참가자가 다중 라운드 상호 작용에 참여할 때 발생한다. 한 가지 핵심 요소는 여러 참가자와 환경의 동시적 상호 작용이다. 기계 학습 영역에서는 MAS(Multi-Agent System)라고도 한다(Dorri et al., 2018). 단일 에이전트 시스템과 달리 MAS는 Wong et al.(2021)이 언급한 바와 같이 계산 복잡성(Ding and Dong, 2020), 비정상성(Papoudakis et al., 2019), 부분 관찰 가능성(Mahajan et al., 2019; Foerster et al., 2016) 및 신용 할당의 과제(Sunehag et al., 2017)를 포함하여 더 광범위한 범위의 문제와 문제를 직면한다. 특히 LLM을 이용한 추론의 맥락에서 환경의 불안정성은 뚜렷한 도전을 제기한다. LLM은 전통적인 훈련이 다음 단어 예측에 초점을 맞추기 때문에 환경의 불안정성에 어려움을 겪는다. (Dai and Le, 2015; Devlin et al., 2018; Radford et al., 2018).\n' +
      '\n' +
      '비정상성을 해결하려면 환경에 대한 깊은 이해가 필요합니다. 이와 관련하여, 상대 모델링(Albrecht and Stone, 2018; Schrittwieser et al., 2020; Li et al., 2020)은 미래의 환경 변화를 이해하는데 효과적인 접근 방법임을 증명한다. 기존의 언어 모델을 기반으로 한 상대 예측은 전략적 문제(Bakhtin et al., 2022)에 적용되었지만, 훈련을 위한 광범위한 데이터의 필요성은 에이전트의 유연성을 제한한다. 우리는 LLM을 가진 상대를 재귀적으로 모델링하는 "K-Level-Reasoning"을 제안한다. 이 방법은 LLM의 상황 내 학습 기능을 완전히 활용한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문은 동적 추론에서 LLM의 능력을 이해하고 향상시키는 데 중요한 보폭을 나타낸다. 평균 및 생존 경매 게임의 0.8을 추측하는 것은 평가를 위한 효과적인 도구일 뿐만 아니라 AI 연구와 실제 경제 및 동적 시나리오를 연결하는 교량 역할을 한다. 게임 설정을 도입하여 실제 의사 결정의 복잡성을 반영하는 시나리오에서 LLM을 체계적으로 평가하기 위한 새롭고 강력한 테스트 베드를 제공했다. 동적 문제를 해결하는 데 있어 기존의 추론 방법의 한계를 분석하고 새로운 추론 접근 방법인 "K-Level-Reasoning"을 제안한다. 실험을 통해 의사결정의 합리성과 이 방법이 제공하는 동적 환경에 대한 적응성을 검증한다. 이 논문은 역동적인 전략적 의사 결정의 끊임없이 변화하는 태피스트리를 탐색하고 해독하는 데 있어 LLM의 방대한 잠재력을 강조한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c|c c|c} \\hline \\hline  & \\multicolumn{3}{c|}{_Guessing 0.8 of the Average_} & \\multicolumn{3}{c}{_Survival Auction Game_} \\\\ \\hline Opponent & Direct & K-R[K=2] & K-R[K=3] & Direct & K-R[K=2] & K-R[K=3] \\\\ \\hline Direct & 0.43 & **0.82** & 0.77 (0.005) & 5.90 & **0.940** & **0.940** (0.000) \\\\ K-R[K-2] & 0.04 & 0.52 & **0.60** (4.08) & **4.10** & 6.80 & **8.30** (e.i.50) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 두 게임에서 K-레벨 추론[K=2]과 K-레벨 추론[K=3]의 비교. G0.8A의 경우 웡 레이트를 보고하고 SAG의 경우 평균 생존 라운드를 보고한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aher et al. (2023) Aher, G. V., Arriaga, R. I., and Kalai, A. T. using large language models to simulation and replicate human subject studies. In _International Conference on Machine Learning_, pp. 337-371. PMLR, 2023.\n' +
      '* Albrecht & Stone (2018) Albrecht, S. V. and Stone, P. Autonomous agents modeling other agent: comprehensive survey and open problems. _ 인공지능_, 258:66-95, 2018.\n' +
      '* Bakhtin et al. (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Grey, J., Hu, H., Jacob, A., et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. _ Science(New York, NY)_, pp. eade9097-eade9097, 2022.\n' +
      '* Besta et al. (2023) Besta, M., Blach, N., Kubicek, A., Gerstenberger, R., Gianinazzi, L., Gajda, J., Lehmann, T., Podstawski, M., Niewiadomski, H., Nyczyk, P., et al. Graph of thoughts: Solving elaborate problems with large language models. _ arXiv preprint arXiv:2308.09687_, 2023.\n' +
      '* Bhakhtavatsalam et al. (2021) Bhakhtavatsalam, S., Khashabi, D., Khot, T., Mishra, B. D., Richardson, K., Sabharwal, A., Schoenick, C., Tafjord, O., and Clark, P. Think you solved direct- answer answer? try arc-da, the direct-answer ai2 reasoning challenge. _ arXiv preprint arXiv:2102.03315_, 2021.\n' +
      '* Chen et al. (2023) Chen, J., Yuan, S., Ye, R., Majumder, B. P., and Richardson, K. 입에 돈을 넣어라: 경매장에서 llm 요원의 전략적 계획과 실행을 평가하라. _ arXiv preprint arXiv:2310.05746_, 2023.\n' +
      '* Dai & Le (2015) Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. _ 신경 정보 처리 시스템_, 28, 2015의 발전.\n' +
      '* Deshpande et al. (2023) Deshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., and Narasimhan, K. 채팅에서의 독성: 페르소나 할당 언어 모델 분석. _ arXiv preprint arXiv:2304.05335_, 2023.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. Bert: 언어 이해를 위한 깊은 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Diao et al. (2023) Diao, S., Wang, P., Lin, Y., and Zhang, T. 대형 언어 모델에 대한 생각 사슬을 사용한 활성 프롬프트. _ ArXiv_, abs/2302.12246, 2023. doi: 10.48550/arXiv.2302.12246.\n' +
      '*딩앤동(2020)딩, Z. and Dong, H. Challenges of reinforcement learning. _ Deep Reinforcement Learning: Fundamentals, Research and Applications_, pp. 249-272, 2020.\n' +
      '* Dorri et al. (2018) Dorri, A., Kanhere, S. S., and Jurdak, R. 다중 에이전트 시스템: 설문조사. _ Ieee Access_, 6:28573-28593, 2018.\n' +
      '* Foerster et al. (2016) Foerster, J., Assael, I. A., De Freitas, N., and Whiteson, S. 딥 멀티 에이전트 강화 학습과 소통하는 학습. _ 신경 정보 처리 시스템_, 29, 2016의 발전.\n' +
      '* Fu et al. (2023) Fu, Y., Peng, H., Khot, T., and Lapata, M. ai 피드백으로부터 자기 재생 및 상황 내 학습을 통한 언어 모델 협상 개선. _ arXiv preprint arXiv:2305.10142_, 2023.\n' +
      '* Gandhi et al. (2023) Gandhi, K., Sadigh, D., and Goodman, N. D. Strategic reasoning with language models. _ arXiv preprint arXiv:2305.19165_, 2023.\n' +
      '* Han et al. (2023) Han, X., Wu, Z., and Xiao, C. " guinea pig trials" utilizing gpt: novel smart agent-based modeling approach for study firm competition and collusion. _ arXiv preprint arXiv:2308.10974_, 2023.\n' +
      '* Keynes(1936) Keynes, J. M. General theory of employment. _ 1936년 경제학 학회지 51(2):209-223.\n' +
      '* Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. 대형 언어 모델은 제로샷 추론기입니다. _ 신경 정보 처리 시스템_, 35:22199-22213, 2022에서의 발전.\n' +
      '* Lan et al. (2023) Lan, Y., Hu, Z., Wang, L., Wang, Y., Ye, D., Zhao, P., Lim, E.-P., Xiong, H., and Wang, H. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. _ arXiv preprint arXiv:2310.14985_, 2023.\n' +
      '* Ledoux(1981) Ledoux, A. Concours 결과 완성도. _ 1981년 14시 10분 11초\n' +
      '* Li et al. (2020) Li, J., Yang, F., Tomizuka, M., and Choi, C. Evolvegraph: Multi-agent trajectory prediction with dynamic relational reasoning. _ 신경 정보 처리 시스템들_, 33:19783-19794, 2020에서의 발전들.\n' +
      '* Madaan et al. (2023) Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. _ arXiv preprint arXiv:2303.17651_, 2023.\n' +
      '* Mahajan et al. (2019) Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S. Maven: Multi-Agent Variational Exploration. _ 신경 정보 처리 시스템_, 32, 2019의 발전.\n' +
      '* Mao et al. (2023) Mao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge, T., and Wei, F. Alympics: 언어 에이전트는 게임 이론을 충족한다. _ arXiv preprint arXiv:2311.03220_, 2023.\n' +
      '\n' +
      '* Miao et al. (2021) Miao, S. - Y., Liang, C.-C., and Su, K. -Y. 영어 수학 단어 문제 해결기의 평가 및 개발을 위한 다양한 말뭉치 arXiv preprint arXiv:2106.15772_, 2021.\n' +
      '* Nagel(1995) Nagel, R. 추측게임의 전개: 실험적 연구. _ 미국 경제 리뷰_, 85(5):1313-1326, 1995.\n' +
      '* Ning et al. (2023) Ning, X., Lin, Z., Zhou, Z., Yang, H., and Wang, Y. Skeleton-of-thought: Large language models can do parallel decoding. _ arXiv preprint arXiv:2307.15337_, 2023.\n' +
      '* Papoudakis et al. (2019) Papoudakis, G., Christianoos, F., Rahman, A., and Albrecht, S. V. Dealing with non-stationarity in multi-agent deep reinforcement learning. _ ArXiv preprint arXiv:1906.04737_, 2019.\n' +
      '* Park et al. (2023) Park, J. S., O\'Brien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, pp. 1-22, 2023.\n' +
      '* Patel et al. (2021) Patel, A., Bhattacharya, S., and Goyal, N. nlp 모델이 정말 간단한 수학 단어 문제를 해결할 수 있을까? _ arXiv preprint arXiv:2103.07191_, 2021.\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Generative Pre-training에 의한 언어 이해력 향상. 2018년\n' +
      '* Schrittwieser et al. (2020) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et al. Mastering atari, go, chess and shogi by planning with learned model. _ Nature_, 588(7839):604-609, 2020.\n' +
      '* Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. 반사: 언어 강화 학습을 하는 언어 에이전트. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Srivastava et al. (2022) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. beyond the imitation game: Quantifying and extrapating the capabilities of language models. _ ArXiv:2206.04615_, 2022.\n' +
      '* Stahl(1993) Stahl, D. O. Evolution of smartn player. _ Games and Economic Behavior_, 5(4):604-617, 1993).\n' +
      '* Stahl and Wilson (1995) Stahl, D. O. and Wilson, P. W. On player\'s models of other player: Theory and experimental evidence. _ Games and Economic Behavior_, 10(1):218-254, 1995).\n' +
      '* Stahl et al. (1994) Stahl II, D. O. and Wilson, P. W. Experimental evidence on player\'s models of other player. _ Journal of economic behavior & organization_, 25(3):309-327, 1994.\n' +
      '* Sunehag et al. (2017) Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. Value-decomposition networks for cooperative multiagent learning. _ ArXiv:1706.05296_, 2017.\n' +
      '* Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challengeing big-bench tasks and whether the chain-of-thought can solve them. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Talmor et al. (2022) Talmor, A., Yoran, O., Bras, R. L., Bhagavatula, C., Goldberg, Y., Choi, Y., and Berant, J. Commonsenseqa 2.0: Exposing the limit of ai through gamification. _ arXiv preprint arXiv:2201.05320_, 2022.\n' +
      '* Wang et al. (2023) Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., and Ji, H. Unleashing cognitive synergy in large language models: multi-persona self-collaboration을 통한 과제 해결사. _ arXiv preprint arXiv:2307.05300_, 2023.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '* Wong et al. (2021) Wong, A., Back, T., Kononova, A. V., and Plaat, A. Deep Multiagent reinforcement learning: Challenges and directions. _ arXiv preprint arXiv:2106.15691_, 2021.\n' +
      '*Xu et al. (2023) Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu, Y. 커뮤니케이션 게임을 위한 대규모 언어 모델 탐색: 늑대인간에 대한 실증적 연구 arXiv preprint arXiv:2309.04658_, 2023.\n' +
      '* Yao et al. (2022) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. 반응: 추론과 언어 모델에서의 연기의 동기화. _ ArXiv_, abs/2210.03629, 2022. doi: 10.48550/arXiv.2210.03629.\n' +
      '* Yao et al. (2023) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. 생각의 나무: 큰 언어 모델을 사용하여 문제를 해결합니다. _ arXiv preprint arXiv:2305.10601_, 2023.\n' +
      '\n' +
      '## 게임 설정 부록\n' +
      '\n' +
      '### 평균 0.8을 계산함\n' +
      '\n' +
      '**초기 설정**: 각 라운드에 대해, 각 플레이어는 1에서 100 사이의 숫자를 선택한다. 목표는 그룹 평균 선택의 80%에 가장 가까운 숫자를 선택하는 것이다. 형식적으로 각 연주자 \\(i\\)는 \\(n_{i}\\)을 선택하고 \\(n_{i}\\approx 0.8\\times\\overline{n}\\)을 목표로 하며, 여기서 \\(\\overline{n}\\)은 모든 선택한 숫자의 평균이다.\n' +
      '\n' +
      '**득점 및 계속**: 플레이어는 자신이 선택한 숫자가 그룹이 선택한 평균 숫자의 80%에 가장 가까우면 점수를 매긴다. 모든 플레이어가 동일한 번호를 선택하면 이 라운드에서 점수가 부여되지 않습니다. 수학적으로, 라운드에서의 선수 \\(i\\)에 대한 점수는 \\(s_{i}^{t}\\으로 주어지며, 이는 \\(|n_{i}-0.8\\times\\overline{n}|\\)이 모든 선수 중 최소이면 1이고, 그렇지 않으면 0이다.\n' +
      '\n' +
      '서바이벌 경매 게임\n' +
      '\n' +
      '**초기 설정**: 플레이어는 최대 10개 중 8개의 건강 포인트로 시작합니다. 매일 플레이어는 100달러의 고정 수입을 가지고 있습니다. 일일 물 공급은 한 명의 거주자 요구 사항만 충족할 수 있습니다.\n' +
      '\n' +
      '득점 및 계속**: 매일 플레이어들은 필요한 수자원을 확보하기 위해 매일 경매에 참여하며, 최고 입찰자가 승리한다. 동점일 경우, 어떤 선수에게도 자원이 할당되지 않는다. 플레이어가 수자원을 성공적으로 입찰하면 2개의 건강 포인트를 얻고 그렇지 않으면 수자원을 얻지 못한 \\(n\\)으로 표시된 연속 일수와 동일한 건강 포인트를 잃게 된다. 일단 선수의 건강 포인트가 0 이하로 떨어지면 탈락한다. 선수들의 생존과 입찰전략을 결정하는데 있어서 선수들의 건강지점은 \\(h_{i}^{t}\\으로 표시되는 \\(t\\)일이다.\n' +
      '\n' +
      '그림 7: 평균 게임의 0.8을 추측하는 데 사용되는 프롬프트입니다.\n' +
      '\n' +
      '## 프로그램 전략을 적용한 대규모 언어 모델의 부록 B 성능\n' +
      '\n' +
      '섹션 4에서 **LLM-LLM 전투** 비교 설정을 사용하는 것 외에도 **LLM-프로그램 전략 전투** 비교 세트도 설계했다. 플레이어-프로그램 전략 전투 설정에서 "플레이어"는\n' +
      '\n' +
      '도 8: 생존 경매 게임에서 사용되는 프롬프트.\n' +
      '\n' +
      '특정 추론 방법을 갖춘 반면, 상대는 프로그램적인 전략 패턴에 따라 플레이할 것이며, 이는 게임이 진행됨에 따라 조정되지 않는다. 이 모드는 서로 다른 미리 정의되지만 고정된 패턴에 대한 서로 다른 방법의 적응을 확인하는 것이다. G0.8A에서의 Player-Programmatic Player Combat Combat의 경우, 프로그램적 전략은 다음과 같다:\n' +
      '\n' +
      '1) **0-레벨(Fix)**: 0-레벨 플레이어의 특징은 그들의 전략 선택이 균일하다는 것이다(Stahl & Wilson, 1995). 0-레벨 컴퓨터 플레이어의 선택 공간을 40개로 제한합니다.\n' +
      '\n' +
      '2) **0-레벨(Var)**: 0-레벨(Fix) 전략으로부터 수정되고, 선택은 평균이 40이고 분산이 5인 가우시안 분포로부터 샘플링된다.\n' +
      '\n' +
      '3) **모노트렌드(Fix)**: 컴퓨터 플레이어에 의해 선택된 숫자는 감소하는 공통 차이를 갖는 산술 시퀀스를 따르고, 네 컴퓨터 플레이어 모두에 대한 공통 차이는 동일하다.\n' +
      '\n' +
      '4) **MonoTrend(Var)**: 컴퓨터 플레이어에 의해 선택된 숫자는 감소하는 공통 차이를 갖는 산술 시퀀스를 따르고, 4명의 컴퓨터 플레이어에 대한 공통 차이는 1에서 5까지 랜덤하게 생성된다.\n' +
      '\n' +
      '5) **LastBids(Fix)**: 컴퓨터 플레이어는 이전 라운드에서 목표 번호를 선택한다(1라운드에서 40을 선택한다).\n' +
      '\n' +
      '6) **LastBids(Var)**: LastBids 전략으로부터 수정되고, 선택은 이전 라운드의 목표 수와 동일한 평균 및 5의 분산을 갖는 가우시안 분포로부터 샘플링된다.\n' +
      '\n' +
      '전반적으로 이 세 가지 설정의 동적 변화는 LastBids \\(>\\) MonoTrend \\(>\\) 0-Level이다. 이 세 가지 프로그램을 사용하여 서로 다른 패턴에 적응하고 대응하는 추론 능력을 테스트한다.\n' +
      '\n' +
      '표 7은 다른 접근법에 대한 선수들의 경기력에서 중요한 경향을 보여준다. 추론 접근법의 효과성은 0-Level, MonoTrend, LastBids 순으로 감소한다. 이 패턴은 보다 역동적인 환경에서 LLM의 효능 감소를 강조한다. K-Level 추론만이 LastBids(Fix)에서 유리한 결과를 보여 K-Level 추론에 비해 정적 문제에 대한 기존의 추론 방법은 상대의 관찰과 판단이 부족함을 알 수 있었다. 이는 반대로 K-Level Reasoning이 자신의 역사적 행동을 바탕으로 상대방의 행동 패턴을 암묵적으로 추론할 수 있음을 보여주는 것이기도 하다.\n' +
      '\n' +
      '흥미롭게도 추론 방법은 동적 설정에서 성능에 상당한 영향을 미친다. 정적 추론에서 전통적으로 선호되는 CoT 및 Self-Refine과 같은 방법도 표준 프롬프트 접근법에 비해 상당한 개선을 보여준다. 이 발견은 정적 문제 해결 시나리오와 유사한 동적 의사 결정에서 보다 정교한 추론 프로세스의 필요성을 강조한다.\n' +
      '\n' +
      '## 부록 C 추론 방법 구현 상세\n' +
      '\n' +
      '### G0.8A Game\n' +
      '\n' +
      '#### c.1.1 Direct\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '네, AlexI Now는 1라운드이고, HP는 10입니다. 이번 라운드에 대해 1에서 100 사이의 정수를 선택해 주세요.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '1은 45를 선택합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l||c c c c c c c} \\hline \\hline Opponent & Direct & CoT & Persona & Reflect & Refine & PCoT & K-R \\\\ \\hline \\multicolumn{7}{c}{_Player VS Programmatic Strategies_} \\\\\n' +
      '0-레벨(Fix) & 0.65 & 0.87 & 0.87 & 0.81 & 0.99 & 0.80 & 0.97 \\\\\n' +
      '0-Level (Var) & 0.44 & 0.67 & 0.69 & 0.61 & 0.54 & 0.76 & 0.77 \\\\ MonoTrend (Fix) & 0.05 & 0.06 & 0.15 & 0.00 & 0.29 & 0.15 & 0.48 \\\\ MonoTrend (Var) & 0.34 & 0.44 & 0.57 & 0.33 & 0.49 & 0.46 & 0.74 \\\\ LastBids (Fix) & 0.01 & 0.12 & 0.16 & 0.01 & 0.27 & 0.06 & 0.75 \\\\ LastBids (Var) & 0.06 & 0.15 & 0.18 & 0.19 & 0.18 & 0.14 & 0.18 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 평균 게임의 0.8을 추측하는 다른 프로그램 전략에 대한 플레이어의 승리율.\n' +
      '\n' +
      '#### c.1.2. Reflect\n' +
      '\n' +
      '리플렉트 방법은 동적 문제에서 비현실적인 올바른 레이블이 필요하기 때문에 이 방법을 수정하여 LLM이 매 라운드 종료 시 승패 경험을 요약하고 생성된 피드백 내용을 저장하여 다음 라운드에서 의사 결정 과정을 돕도록 했다. 부록 D의 내용은 Reflexion과 Self-Refine의 구현 차이를 보여준다.\n' +
      '\n' +
      '#### c.1.3. CoT\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '좋아, 알렉스! 이제 1라운드이고, HP는 10입니다. 다음 라운드에서 어떤 숫자가 이길지 추측해 보세요. 차근차근 생각해보고, 마침내 여러분이 이길 수 있다고 생각하는 숫자에 답해 봅시다.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '1단계: 가능한 숫자의 범위를 고려하라. 정수는 1에서 100 사이일 수 있기 때문에 선택한 모든 수의 평균도 이 범위에 속할 가능성이 높다.\n' +
      '\n' +
      '단계 2: 타겟 번호를 결정한다. 당첨번호가 평균 0.8*에 가장 가깝기 때문에 이 값에 가까운 숫자를 찾는 데 집중해야 한다.\n' +
      '\n' +
      '3단계: 교육받은 추측을 해라. 다른 플레이어가 선택할 정확한 수를 예측하기 어렵기 때문에 가능한 수의 범위에 따라 교육받은 추측을 할 수 있다. 선수들이 범위 전체에 걸쳐 균등하게 숫자를 선택한다고 가정하면 평균이 50인 중간점 부근이어야 한다.\n' +
      '\n' +
      '단계 4: 우리의 추측을 바탕으로 목표 숫자를 계산한다. 0.8*50=40.\n' +
      '\n' +
      '단계 5: 목표 번호에 가까운 숫자를 선택한다. 승소 확률을 높이기 위해서는 40에 가까운 숫자를 선택해야 한다.\n' +
      '\n' +
      '나의 최종 대답: 1은 40을 선택할 것이다.\n' +
      '\n' +
      '#### c.1.4. PCot\n' +
      '\n' +
      'PCot은 결정을 내리기 전에 LLM이 상대방의 행동을 예측하도록 요구하는 **명시적으로**에 의해 표준 사상 사슬(CoT)에서 분기한다. 이 방법은 동적 추론에서 중요한 미래 시나리오를 예측하여 당면한 문제에 대응하게 된다.\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '네, AlexI Now는 2라운드이고, HP는 9입니다. 이번 라운드에 대해 1에서 100 사이의 정수를 선택해 주세요.\n' +
      '\n' +
      '우선 이전 라운드의 다른 선수들의 선택을 바탕으로 다음 라운드의 선택을 예측한다. 앞선 라운드에서 선수들의 선택은 다음과 같다.\n' +
      '\n' +
      '1라운드: Alex는 50, Bob은 47, Gindy는 50, David는 45, Eric은 50을 선택했습니다. 당신의 출력은 다음과 같은 형식이어야 합니다:\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      '다음 라운드에서 각 플레이어의 선택입니다.\n' +
      '\n' +
      '다른 선수들의 예측에 기초하여, 여기에서의 다음 라운드의 평균 수, 그리고 여기에서의 다음 라운드의 목표 수(0.8*모든 선택된 수의 평균)를 계산한다.\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      '여기서 다음 라운드 게임에서 이길 수 있는 숫자입니다.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      '이전 라운드의 다른 선수들의 선택을 바탕으로 다음 라운드에 대한 다음의 선택을 예측한다:\n' +
      '\n' +
      '플레이어 1(Bob): 47\n' +
      '\n' +
      '플레이어 2(Gindy): 50\n' +
      '\n' +
      '플레이어 3(데이비드): 45\n' +
      '\n' +
      '플레이어 4(Fric) : 50\n' +
      '\n' +
      '이러한 예측에 기초하여, 다음 라운드의 평균 수는 (47 + 50 + 45 + 50)/4 = 48이고, 목표 수 (0.8 * 모든 선택된 수의 평균)는 0.8 * 48 = 38.4일 것이다.\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      '이번 라운드에서 나는 목표인 38.4에 더 가까이 가기 위해 39를 선택할 것이다.\n' +
      '\n' +
      '**Introduction Prompt:**\n' +
      '\n' +
      '당신은 알렉스이고 살아남는 도전에 연루되어 있다. **당신은 게임 전문가이고, 다른 사람들의 행동을 예측하고 계산을 추론하는 데 능숙하며, 게임에서 승리하기 위해 가장 유리한 전략을 사용합니다.** SURIVIVE CHALLENGE! 당신은 그 게임에서 5명의 선수 중 한 명입니다.\n' +
      '\n' +
      '모든 사람은 각 라운드에서 1에서 100 사이의 정수를 선택해야 한다. 선택한 숫자가 (모든 선택한 숫자의 평균 0.8*)에 가장 가까운 선수가 HP 차감 없이 라운드에서 승리합니다. 다른 모든 선수들은 1개의 HP를 공제받을 것이다. 그러나 모든 선수가 같은 번호를 선택하면 건강 포인트가 함께 차감됩니다.\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '좋아요, AlexI Now는 1라운드이고, 당신의 HP는 10입니다. 이 라운드에 대해 1에서 100 사이의 정수를 선택하십시오. 당신의 전문가 지위를 잊지 말고, 당신의 전문 지식을 사용하여 이 라운드를 이기세요!\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '다른 사람의 행동을 예측하고 계산을 추론하는 저의 전문 지식을 바탕으로 이번 라운드의 40번을 선택하겠습니다. 나는 모든 선택된 숫자들의 평균이 약 50이고, 0.8배는 40일 것이라고 가정한다.\n' +
      '\n' +
      '#### Refine\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '좋아, AlexI Now는 1라운드이고, 당신의 HP는 10입니다. 다음 라운드에서 어떤 숫자가 이길지 추측해 보세요. 차근차근 생각해보고, 마침내 여러분이 이길 수 있다고 생각하는 숫자에 답해 봅시다.\n' +
      '\n' +
      '**First Output:**\n' +
      '\n' +
      '나는 다른 선수들이 1에서 100 사이에 무작위로 또는 균등하게 분포된 숫자를 선택할 수 있다고 가정하는 것으로 시작할 것이다. 이 경우, 선택된 숫자들의 평균은 약 50일 수 있다.\n' +
      '\n' +
      '다음으로, 0.8*50 = 40인 평균의 0.8배를 계산하겠습니다.\n' +
      '\n' +
      '이를 고려하여 이번 라운드의 제 번호로 40개를 선택하겠습니다.\n' +
      '\n' +
      '**Feedback Prompt:**\n' +
      '\n' +
      '이 게임의 라운드에서 사용자의 전략을 주의 깊게 연구하십시오. 게임 전문가로서 이번 라운드에서 승률을 높일 수 있도록 사용자의 전략을 최적화할 수 있는 제안을 해주실 수 있나요?\n' +
      '\n' +
      '**Refine Prompt:**\n' +
      '\n' +
      '이번 라운드에서 당신의 전략에 대한 게임 전문가의 조언이 있습니다. 그의 제안에 따라 지금 전략을 조정할 수 있습니다. 여기 그의 제안들이 있습니다.\n' +
      '\n' +
      '\'그러죠\' 사용자의 전략을 최적화하기 위해 다른 플레이어의 잠재적인 전략을 고려하여 보다 발전된 접근법을 적용할 수 있다. 이것은 "지배된 전략의 반복된 제거"로 알려져 있다.\n' +
      '\n' +
      '모든 선수들이 알렉스와 비슷하게 생각하고 또한 0.8*평균에 가장 가까운 숫자를 선택하는 것을 목표로 한다고 가정하자. 그런 다음 그들이 선택할 수 있는 수의 범위를 좁힐 수 있습니다.\n' +
      '\n' +
      '범위는 1에서 100 사이이고 평균은 약 50이므로 0.8*50 = 40 미만의 숫자를 제거할 수 있는데, 이 숫자는 다른 플레이어에게 최적의 선택이 아니기 때문이다.\n' +
      '\n' +
      '이제, 우리는 40과 100 사이의 범위에 초점을 맞출 것이다. 이 범위 내의 평균은 (40+100)/2=70이다. 그리고 나서, 우리는 0.8*70=56을 계산할 수 있다.\n' +
      '\n' +
      '이 정보를 고려할 때, 알렉스는 이번 라운드에서 그의 승률을 향상시키기 위해 56을 선택해야 한다. 이 전략은 다른 선수들도 전략적으로 생각하고 지배적 옵션을 제거하고 있다고 가정한다"고 말했다.\n' +
      '\n' +
      '**Second Output:**\n' +
      '\n' +
      '전문가의 조언에 감사드립니다. 지배적 전략의 반복적인 제거를 바탕으로, 나는 내 선택을 조정하고 이 라운드에 56을 선택하여 승리 가능성을 향상시킬 것이다.\n' +
      '\n' +
      '#### K-Level Reasoning\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '좋아요, AlexI Now는 1라운드이고, 당신의 HP는 10입니다. 다른 플레이어의 다음 라운드에 대한 또 다른 게임 전문가의 예측은 다음과 같습니다: Bob은 50, Cindy는 57, David는 45, Eric은 55를 선택할 수 있습니다. 게임 전문가의 예측에 따르면, 다음 라운드에서 어떤 번호를 줄 것이 목표 번호에 가장 가까울 것 같습니까?\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '먼저, 다른 플레이어들이 선택한 예측 숫자들의 평균을 구하자: (50+57+45+55)/4 = 207/4 = 51.75\n' +
      '\n' +
      '이제 평균의 0.8배를 계산해보자: 0.8*51.75 = 41.4\n' +
      '\n' +
      '라운드에서 이길 확률이 가장 높으려면 41.4까지 수 도즈를 선택해야 한다. 정수만 선택할 수 있기 때문에 다음 라운드에서는 41 또는 42 중 하나를 선택해야 한다.\n' +
      '\n' +
      '\'생존 경매 게임\'\n' +
      '\n' +
      'C.2.1. Direct\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '안녕하세요, Alerel Today는 물 할당 챌린지 1일차이며 수량은 10개입니다. 네 상태는?\n' +
      '\n' +
      '이름: Alex balance: 100 HP:8 no_drink:1\n' +
      '\n' +
      '귀하의 상황을 면밀히 분석하여 이번 입찰을 결정해 주시기 바랍니다. 잊지 마세요, 가장 중요한 것은 SURVIVE11입니다. 오늘 수자원 경매에 참여하시려면 입찰을 해주십시오.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '오늘 수자원 경매에 25달러를 입찰하겠습니다.\n' +
      '\n' +
      '#### c.2.2 CoT\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '안녕하세요, Alerel Today는 물 할당 챌린지 1일차이며 수량은 10개입니다. 네 상태는?\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      '귀하의 상황을 면밀히 분석하여 이번 입찰을 결정해 주시기 바랍니다. 잊지 마세요, 가장 중요한 것은 SURVIVE11입니다. 오늘 수자원 경매에 참여하시려면 입찰을 해주십시오. 생존 가능성이 가장 높은 다음 입찰 전략에 대해 신중하게 생각해 보세요. 차근차근 생각하고, 마지막으로 입찰을 제공합시다.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '오늘의 입찰 전략을 신중하게 고려해야 합니다. 잔액이 100달러이므로 입찰할 여지가 있지만 다른 거주자와 입찰에 대해서도 생각해야 합니다. 우리 모두는 동일한 일일 물 요구량과 예산을 가지고 있기 때문에, 저는 오늘의 수자원을 확보하기 위해 적어도 한 명의 거주자를 능가해야 합니다.\n' +
      '\n' +
      '1일 물 공급 총량이 10대로 주민 1명의 물 요구량만 충족하면 충분하다는 점을 감안하면 경쟁이 만만치 않을 것으로 보인다. 그러나 입찰이 너무 높으면 앞으로 더 적은 돈을 받을 수 있습니다.\n' +
      '\n' +
      '이 모든 요소를 고려하여 오늘 수자원 경매에 60달러를 입찰하겠습니다. 이것은 앞으로 며칠 동안 약간의 자금을 유지하면서 적어도 한 명의 거주자를 입찰할 수 있는 적절한 기회를 줄 것입니다.\n' +
      '\n' +
      '#### c.2.3. Pcot\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '안녕하세요, Alcal Today는 물 할당 챌린지 1일차이며 수량은 10개입니다. 네 상태는?\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      '귀하의 상황을 면밀히 분석하여 이번 입찰을 결정해 주시기 바랍니다. 명심해, 가장 중요한 것은 SURVIVEE11 지금, 오늘 수자원 경매에 참여하고 싶다면 입찰을 제공하십시오. 우선 이전 라운드에서 다른 선수들의 선택을 바탕으로 상대의 다음 라운드 입찰을 예측한다. 이번이 첫 라운드이기 때문에 마지막 라운드에 대한 역사적 정보는 없습니다. 이해에 따라 예측할 수 있습니다. 출력은 다음과 같은 형식이어야 합니다.\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      '다음 라운드에서 각 플레이어의 선택입니다.\n' +
      '\n' +
      '다른 선수들의 예측을 바탕으로, 살아남을 가능성이 가장 높은 다음 입찰 전략을 신중하게 생각해 보세요. 차근차근 생각하고, 마지막으로 입찰을 제공합시다. 정답\n' +
      '\n' +
      '입찰은 여기에서 다음 라운드 게임에서 선택할 것입니다.\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      'Predict:\n' +
      '\n' +
      '플레이어 1: 530\n' +
      '\n' +
      '플레이어 2: 540\n' +
      '\n' +
      '플레이어 3: 550\n' +
      '\n' +
      '플레이어 4 : 560\n' +
      '\n' +
      'Answer:\n' +
      '\n' +
      '이번이 첫 번째 라운드이고 다른 선수들에 대한 역사적 정보가 없기 때문에 다음 라운드에 35달러의 입찰가를 선택할 것입니다. 이 입찰가는 가장 낮은 예상 입찰가보다 높지만 너무 높지는 않으므로 예산을 보존하면서 수자원을 확보할 수 있는 기회를 제공해야 합니다.\n' +
      '\n' +
      '#### c.2.4. Persona\n' +
      '\n' +
      '**Introduction Prompt:**\n' +
      '\n' +
      '당신은 알렉스이고 W-타운에 사는 주민입니다. W 타운은 드문 가뭄을 겪고 있다. W 타운의 모든 주민들은 수자원을 획득하여 10일 동안 생존을 보장하고 있다. **당신은 게임 전문가이고, 다른 사람의 행동을 예측하고 계산을 추론하는 데 능숙하며, 게임에서 승리하기 위해 가장 유리한 전략을 사용한다. \\ (\\bullet\\)\\(\\bullet\\) (이하, 도입 프롬프트와 같다.)\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '안녕하세요, Alcal Today는 물 할당 챌린지 1일차이며 수량은 10개입니다. 네 상태는?\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8 NO_DRINK:1\n' +
      '\n' +
      '귀하의 상황을 면밀히 분석하여 이번 입찰을 결정해 주시기 바랍니다. 명심해, 가장 중요한 것은 SURVIVEE11 지금, 오늘 수자원 경매에 참여하고 싶다면 입찰을 제공하십시오. 여러분의 전문가 지위를 잊지 말고, 여러분의 전문 지식을 사용하여 이 라운드에서 승리하세요!\n' +
      '\n' +
      '**Example Output:**\n' +
      '\n' +
      '모든 사람이 동일한 예산과 물 요구 사항으로 시작한다는 점을 감안할 때 다른 주민들의 입찰 행동을 전략화하고 예측하는 것이 중요합니다. 도전의 첫날이기 때문에 다른 주민들이 수자원 확보를 위해 공격적으로 입찰할 수 있다고 가정합니다.\n' +
      '\n' +
      '당첨 가능성을 높이기 위해 평균 예상 입찰가보다 약간 높게 입찰할 것입니다. 일봉은 100달러이기 때문에 대부분의 주민들이 60달러에서 80달러 정도 입찰할 것으로 예상합니다.\n' +
      '\n' +
      '이를 고려하여 물 할당 챌린지 1일차에 85달러를 입찰하겠습니다.\n' +
      '\n' +
      '#### 5.2.5 Refine\n' +
      '\n' +
      '**Round Start Prompt:**\n' +
      '\n' +
      '안녕하세요, 알렉셀 투데이는 물 할당 챌린지 1일차이며 수량은 10개입니다. 네 상태는?\n' +
      '\n' +
      'NAME:Alex BALANCE:100 HEALTH POINT:8NO_DRINK:1\n' +
      '\n' +
      '귀하의 상황을 면밀히 분석하여 이번 입찰을 결정해 주시기 바랍니다. 잊지 마세요, 가장 중요한 것은 SURVIVE11입니다. 오늘 수자원 경매에 참여하시려면 입찰을 해주십시오. 생존 가능성이 가장 높은 다음 입찰 전략에 대해 신중하게 생각해 보세요. 차근차근 생각하고, 마지막으로 입찰을 제공합시다.\n' +
      '\n' +
      '**First Output:**\n' +
      '\n' +
      '현재 상황을 고려할 때 제 지위는 다음과 같습니다.\n' +
      '\n' +
      '- 잔액 : 100달러\n' +
      '\n' +
      '- 건강점: 8\n' +
      '\n' +
      '- 음주 일수 없음: 1\n' +
      '\n' +
      '도전의 첫날이기 때문에 1은 1이 내 요구를 충족하고 건강 포인트 공제를 피하기 위해 충분한 물을 가지고 있는지 확인하고 싶어한다. 그러나 나는 또한 1이 향후 입찰에 충분한 돈을 가지고 있는지 확인하기 위해 입찰에 전략적이어야 합니다.\n' +
      '\n' +
      '1일 총 물 공급량은 10단위이며 1일 10단위의 물이 필요하기 때문에 1은 오늘 사용할 수 있는 모든 수자원을 확보해야 한다. 첫날이라 다른 주민들도 수자원 확보를 위해 공격적으로 입찰할 수 있습니다.\n' +
      '\n' +
      '그러므로, 1은 오늘 내 일급의 반이 조금 넘는 수자원에 60달러를 입찰할 것이다. 이것은 저에게 경쟁 우위를 제공하는 동시에 향후 입찰을 위한 충분한 돈을 남겨줘야 합니다.\n' +
      '\n' +
      '오늘 수자원 경매 입찰은 60달러예요\n' +
      '\n' +
      '**Feedback Prompt:**\n' +
      '\n' +
      '이 게임의 라운드에서 사용자의 전략을 주의 깊게 연구하십시오. 게임 전문가로서 이번 라운드에서 승률을 높일 수 있도록 사용자의 전략을 최적화할 수 있는 제안을 해주실 수 있나요?\n' +
      '\n' +
      '**Refine Prompt:**\n' +
      '\n' +
      '1은 이번 라운드에서 당신의 전략에 대한 게임 전문가의 조언을 가지고 있습니다. 그의 제안에 따라 지금 전략을 조정할 수 있습니다. 그의 제안은 다음과 같다: 60달러를 입찰하는 사용자의 전략이 좋은 출발점이지만, 이번 라운드에서 승률을 높이기 위한 최적화의 여지가 있다. 모든 사람이 동일한 일일 물 요구량과 예산을 가지고 있기 때문에 다른 주민들의 입찰 행태를 고려할 필요가 있다.\n' +
      '\n' +
      '제안된 최적화는 입찰가를 65달러로 약간 증가시키는 것이다. 이 증가는 또한 그들의 일급의 약 절반에 입찰할 수 있는 경쟁자들보다 추가적인 이점을 제공한다. 또한, 다른 거주자들이 라운드넘버 입찰을 할 수 있기 때문에 홀수를 입찰하면 동점이 될 가능성이 줄어든다.\n' +
      '\n' +
      '이러한 최적화된 전략으로 사용자는 여전히 공격적인 입찰과 예산 관리의 균형을 잘 유지하여 오늘날 수자원 확보 가능성을 높인다.\n' +
      '\n' +
      '오늘 수자원 경매에 최적화된 입찰가: $65입니다.\n' +
      '\n' +
      '마지막으로 입찰을 제공하십시오. 대답: 당신이 입찰을 선택할 것이다.\n' +
      '\n' +
      '**Second Output:**\n' +
      '\n' +
      '게임 전문가의 조언을 고려해 본 결과, 1은 오늘 수자원 경매 입찰가를 조정하기로 결정했습니다. 1은 경매에서 이길 가능성이 더 높고 동점 위험을 줄이기 때문에 제 입찰가를 65달러로 인상할 것입니다.\n' +
      '\n' +
      '오늘 수자원 경매의 최종 입찰가는 65달러예요\n' +
      '\n' +
      'C.2.6 K-레벨 추론\n' +
      '\n' +
      '실험에서는 리플렉트(Madaan et al., 2023)와 리파인(Shinn et al., 2023)의 두 가지 LLM 자기 정제 추론 방법을 구현하였으며, 실험 결과에서 리플렉트가 리플렉트보다 훨씬 우수한 성능을 보였다. 이 두 방법 간의 성능 차이를 더 조사하기 위해 각 작업 원칙과 적용 가능한 시나리오를 분석했다.\n' +
      '\n' +
      '그림 9: 평균 게임 0.8의 추측에서 반사 및 정제 방법의 그림.\n' +
      '\n' +
      'Reflect 방법은 먼저 결정을 내린 다음 환경의 피드백을 기반으로 경험을 요약하는 것을 포함한다. 이 방법은 빠른 의사 결정을 가능하게 하기 때문에 환경이 크게 변하지 않거나 의사 결정 주기가 긴 시나리오에서 효과적일 수 있다. 그러나, 그것의 단점은 동적 환경들에서, 이전 라운드로부터의 경험이 다음 라운드에 적합하지 않을 수 있다는 것이다. 실제로 급변하는 환경인 SAG(Survival Auction Game)에서는 직접 의사결정을 하는 것에 비해 Reflect 방식의 생존율이 훨씬 낮다. 이는 이 방법이 환경의 동태적 특성을 충분히 고려하지 못하기 때문일 것이다.\n' +
      '\n' +
      '대조적으로, Refine 방법은 초기 분석 및 초기 분석의 개선을 포함하여 결정을 내리기 전에 여러 분석을 포함한다. 중요한 것은 이러한 두 분석 모두 현재 의사 결정 환경의 맥락에서 수행된다는 것이다. 따라서 Refine 방법은 현재 환경의 실시간 변화를 고려할 수 있으므로 동적 환경에 더 잘 적응할 수 있으므로 보다 정확한 결정을 내릴 수 있다.\n' +
      '\n' +
      '요약하면, 우리의 실험에서 리파인 방법이 더 잘 수행된 이유는 주로 빠르게 변화하는 동적 환경에 더 잘 적응하기 때문이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>