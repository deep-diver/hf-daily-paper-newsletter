<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Transfer Learning for Text Diffusion Models\n' +
      '\n' +
      'Kehang Han\\({}^{1}\\), Kathleen Kenealy\\({}^{1}\\), Aditya Barua\\({}^{2}\\), Noah Fiedel\\({}^{1}\\), Noah Constant\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Google DeepMind \\({}^{2}\\)Google\n' +
      '\n' +
      '{kehanghan, kkenealy, adityabarua, nfiedel, nconstant}@google.com\n' +
      '\n' +
      'Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this report, we explore the potential for _text diffusion_ to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call "AR2Diff". We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff--adapting AR models to use diffusion decoding. These results are promising given that text diffusion is relatively underexplored and can be significantly faster than AR decoding for long text generation.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In recent years, large language models (LLMs) have grown in scale, capability, and popularity Brown et al. (2020); Chowdhery et al. (2022), and are increasingly used to generate long-form text such as summaries, blocks of code, or in-depth explanations OpenAI (2023); Anil et al. (2023). To our knowledge, all popular LLMs are _autoregressive_ (AR)--generating one token at a time in textual order, each conditioned on the sequence generated so far. While AR generation is well understood and has been highly optimized, its strict left-to-right factorization may be overly constraining. Generating token-by-token is inherently inefficient, particularly on long but predictable spans of text (e.g., copying a serial number from the context one digit at a time). Additionally, this strict order may not provide the ideal scaffold for planning a composition. Human writers typically outline, draft, revise, and proofread their work, and it seems plausible that machines could benefit from a similar iterative approach.1\n' +
      '\n' +
      'Footnote 1: “Chain of thought” prompting Wei et al. (2022) provides a mechanism for models to reason about or draft the desired output before producing it. However, the final output is still generated autoregressively.\n' +
      '\n' +
      'As an alternative, many _non-AR_ decoding methods have been proposed (see section SS2), which generate multiple sequence positions in parallel, or make progressive edits to a "rough" initial generation. Several of these have shown promising results on specific tasks. For example, SUNDAE\'s _text diffusion_ approach Savinov et al. (2022) achieves similar quality to an AR baseline on machine translation while decoding over 2\\(\\times\\) faster.\n' +
      '\n' +
      'However, despite positive findings, non-AR techniques have failed to gain traction, and remain unused in the space of large language models. We suspect this may be due to the inertia behind classic AR methods, and the high cost and risk of tuning and training large models from scratch using non-standard training losses and decoding methods.\n' +
      '\n' +
      'With an eye to lowering this cost of entry and easing the transition to more efficient text generation at scale, in this paper we investigate the potential for adapting existing pretrained AR model checkpoints to perform non-AR generation. We use a simplified version of SUNDAE text diffusion as our canonical non-AR implementation; thus we refer to this lightweight adaptation process as **AR2Diff (AR to Diffusion)**.\n' +
      '\n' +
      'More specifically, we are interested in testing the ability of text diffusion methods to compete at scale in the popular transfer learning setting, where a model is pretrained on unsupervised data andapplied to diverse downstream tasks. We conduct a series of experiments comparing text diffusion to AR baselines across different model architectures, tasks, and transfer learning settings.\n' +
      '\n' +
      'Our main contributions are: (1) showing that language models pretrained and fine-tuned using text diffusion can be competitive with autoregressive models on several downstream tasks, (2) showing that pretrained AR models can be transformed into diffusion models via a lightweight adaptation.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Previous work has explored a wide range of non-autoregressive methods for text generation Gu et al. (2018); Lee et al. (2018); Stern et al. (2019); Ghazvininejad et al. (2019). In the last few years, diffusion models Sohl-Dickstein et al. (2015) have emerged as the primary technique for _image_ generation Rombach et al. (2021); Ramesh et al. (2022); Saharia et al. (2022). Many recent efforts have applied diffusion methods to _text_ generation Savinov et al. (2022); Li et al. (2022); Reid et al. (2023); Chen et al. (2023); Strudel et al. (2022); Dieleman et al. (2022); Zheng et al. (2023); Lin et al. (2023); Gong et al. (2023); Yuan et al. (2023); Wu et al. (2023), but none has yet gained adoption in the space of large language models.\n' +
      '\n' +
      'While promising, text diffusion techniques have largely not been tested at scale or in multitask transfer learning settings, though see Lin et al. (2023) and Ye et al. (2023) for recent work in this direction. Furthermore, it remains unclear if these methods demand training new diffusion models from scratch, or if AR models can be efficiently adapted into diffusion models. We explore these questions empirically in section SS4.\n' +
      '\n' +
      'One line of previous work shows that non-AR methods benefit from "AR distillation Kim and Rush (2016); Gu et al. (2018); Saharia et al. (2020); Gu and Kong (2021)--training a non-AR model from scratch on silver data generated via the predictions of an existing AR model. AR distillation is similar to our AR2Diff adaptation in that both leverage a preexisting AR model. However they differ in that our method initializes the diffusion model directly from an AR checkpoint, and trains on gold data. Given the significant recent investment in training large AR models, we believe that lightweight adaptation of existing checkpoints is a promising direction compared to training non-standard models from scratch.\n' +
      '\n' +
      'Recently, Lin et al. (2023) show good results pretraining a text diffusion encoder-decoder model and fine-tuning it on downstream tasks. Like our work, this validates the effectiveness of pretraining text diffusion models at scale.\n' +
      '\n' +
      'More recently, building on "reparameterized discrete diffusion models" Zheng et al. (2023), Ye et al. (2023) show the possibility of converting large AR models (up to 10B parameters) into text diffusion models during task-specific fine-tuning--their "diffusive adaptation". This work shares our goal of demonstrating that text diffusion can be practical at scale. Our work differs in (i) building on SUNDAE as opposed to RDM, (ii) including diffusion models pretrained from scratch as baselines, (iii) comparing different architectures and objectives for diffusion pretraining, and (iv) testing adaptation during pretraining (our AR2Diff\\({}_{N}\\) with \\(N>0\\)), as opposed to only during fine-tuning (our AR2Diff\\({}_{0}\\)).\n' +
      '\n' +
      '## 3 Evaluation Tasks\n' +
      '\n' +
      'We experiment with three downstream tasks. First, we use **WMT14 French-English translation**Bojar et al. (2014), as machine translation is widely used to evaluate generative models, particularly in work on non-AR models.\n' +
      '\n' +
      'Second, we evaluate on the popular **SQuAD question answering task**Rajpurkar et al. (2016). As an extractive QA task, this does not require open generation, and most targets are fairly short, often just a few words long. While text diffusion models are unlikely to deliver speed gains on tasks with short outputs (see Section SS4.7), we feel it is still important to test for quality on text _understanding_ tasks. This can help establish whether pretrained diffusion models can be an effective general foundation for language understanding, and ensures that our findings are interpretable within the literature on transfer learning in NLP.\n' +
      '\n' +
      'Finally, we evaluate on **Mostly Basic Python Problems (MBPP)**Austin et al. (2021), a recent benchmark requiring models to generate full solutions to simple Python programming tasks. This task is fairly open-ended, as there are many working solutions to a given task, depending on choices of algorithm, coding style, variable names, and so on. Compared to open-ended natural language generation, this benchmark has clear and meaningful automatic evaluation metrics, as we can run the generated code and assess whether it passes relevant test cases. When tokenized using the PaLM Chowdhery et al. (2022) vocabulary we adopt in our experiments, median target length is \\(59\\) tokens, and 90th percentile is \\(150\\) tokens.\n' +
      '\n' +
      'Footnote 1: [https://www.tensorflow.org/](https://www.tensorflow.org/)\n' +
      '\n' +
      'Footnote 2: We sample from \\(l_{1}\\) using temperature \\(0.0\\) (argmax), as opposed to SUNDAE’s temperature \\(1.0\\), as we found this performed best in early ablations on WMT14, with temperature in \\(\\{0.0,0.1,1.0\\}\\).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Diffusion implementation\n' +
      '\n' +
      'Our diffusion implementation follows SUNDAE Savinov et al. (2022). More specifically, we use standard Transformer Vaswani et al. (2017) architectures (either encoder-decoder or decoder-only) as implemented in the T5X Roberts et al. (2022) library. As SUNDAE performs discrete diffusion in surface token space, the decoder inputs and outputs are tokens, in line with standard AR models. These implementation choices allow us to reuse existing frameworks for autoregressive LLM training with relatively minor changes. As a result, we can easily experiment with using pretrained AR model checkpoints and adapting these to perform text diffusion.\n' +
      '\n' +
      'For training, we use the SUNDAE \\(L^{(1:2)}\\) loss, which incorporates one step of "unrolled denoising", encouraging the model to be able to refine its single-step predictions further towards the target. More concretely, for target sequence \\(x\\), we randomly corrupt a random proportion of tokens (sampling from a uniform distribution) to produce \\(x^{c}\\), which is passed as input to the denoising model to produce logits \\(l_{1}\\). The "logits loss" \\(L^{(1)}\\) is the cross-entropy between \\(l_{1}\\) and \\(x\\). "Unrolled logits" are computed by sampling2 from \\(l_{1}\\) and passing these tokens back as inputs to the denoising model, producing \\(l_{2}\\). The "unrolled logits loss" \\(L^{(2)}\\) is the cross-entropy between \\(l_{2}\\) and \\(x\\). For the overall loss, we use \\(L^{(1)}+L^{(2)}\\).\n' +
      '\n' +
      'Footnote 2: We sample from \\(l_{1}\\) using temperature \\(0.0\\) (argmax), as opposed to SUNDAE’s temperature \\(1.0\\), as we found this performed best in early ablations on WMT14, with temperature in \\(\\{0.0,0.1,1.0\\}\\).\n' +
      '\n' +
      'For inference, we follow SUNDAE in using low-temperature sampling (\\(\\tau=0.2\\)), decoding \\(N\\) samples in parallel (we use \\(N=8\\) by default), and reranking them based on "model score": the cross-entropy between the decoder input and output logits on the final step of diffusion. We use \\(10\\) diffusion decoding steps by default; thus on tasks with targets longer than \\(10\\) tokens, our diffusion models use fewer decoding steps than an AR model.3 These choices are ablated in section SS4.6.\n' +
      '\n' +
      'Footnote 3: As AR models can cache and reuse activations from earlier sequence positions for subsequent decoding steps (thanks to the causal attention mask), they use significantly fewer FLOPs per step, when other factors are held constant. We do not present a full picture of the speed vs. quality tradeoffs of text diffusion models here. Previous work has shown that text diffusion can be competitive on speed and quality, even comparing against AR inference with caching enabled Savinov et al. (2022). We assume here that diffusion in \\(10\\) steps is fast enough to have practical value, and focus on quality.\n' +
      '\n' +
      'For simplicity, we forgo SUNDAE\'s target length prediction module, opting instead to let the model learn to predict sequence length end-to-end through the presence of padding tokens observed during training. As a result, our text diffusion models have no additional parameters beyond those within the Transformer (encoder-)decoder.\n' +
      '\n' +
      '### Selecting objective and architecture\n' +
      '\n' +
      'Previous work on text diffusion has focused on the single-task setting, either training and evaluating on unconditional text generation, or training from scratch on an end task, such as machine translation.4 In contrast, we aim to evaluate text diffusion in the _transfer learning_ setting--pretraining a large model, and adapting it to a range of downstream tasks. As a first step, and to cut down the space of further experiments, we first seek to identify a model architecture and pretraining objective well-suited to text diffusion.\n' +
      '\n' +
      'Footnote 4: Ye et al. (2023) adapt pretrained AR models for diffusion across multiple tasks, but do not explore pretraining a general-purpose diffusion model that can be adapted to specific tasks.\n' +
      '\n' +
      'The T5 study on transfer learning for AR text-to-text models Raffel et al. (2020) recommends using an encoder-decoder architecture and a "span corruption" objective--masking multi-token spans in the input, and reconstructing these in the target. By comparison, many subsequent LLMs have converged on a decoder-only architecture with a standard LM objective Brown et al. (2020); Chowdhery et al. (2022). To establish which setting works best for diffusion, we test all four combinations of architecture (**encoder-decoder** vs. **decoder-only**) and objective (**span corruption** vs. **prefix LM**), as shown in Figure 1.5\n' +
      '\n' +
      'Footnote 5: We choose the “prefix LM” objective rather than the standard causal LM objective, as it is compatible with the encoder-decoder architecture, and has been shown to outperform causal LM in apples-to-apples comparisons Tay et al. (2023).\n' +
      '\n' +
      'We train each model on the same pretraining mixture, consisting of 80% multilingual web crawl data from mC4 Xue et al. (2021) and 20% Python code from "The Stack" Kocetkov et al. (2022). All models use the T5 Base size transformer architecture and pretrain for \\(1\\) million steps on batches of size \\(128\\) and sequence length \\(1024\\). We then fine-tune each model separately on WMT14 En-Fr, SQuAD, and MBPP (producing 12 fine-tuned models total) and evaluate across all tasks. We use a fine-tuning batch size of \\(128\\) and a constant learning rate of \\(0.001\\) across all tasks. We fine-tune \\(500\\)K steps for WMT14 En-Fr and \\(250\\)K steps for SQuAD, with checkpoints taken every \\(1{,}000\\) steps. For MBPP due to smaller dataset size, we fine-tune for \\(5{,}000\\) steps with checkpoints taken every \\(50\\) steps. In all cases, we terminate fine-tuning if clear evidence of over-fitting is observed. We reuse the \\(256\\)K token SentencePiece vocabulary from PaLM (Chowdhery et al., 2022). Our decoder-only models have roughly \\(280\\)M parameters (including embedding parameters), while our encoder-decoder models have roughly \\(590\\)M parameters.\n' +
      '\n' +
      'The results in Table 1 show that our decoder-only models perform the best across all three tasks, despite their lower parameter count. This advantage is especially clear on code synthesis (MBPP), where the encoder-decoder models fail to solve any problem in the test set, even on the permissive "Pass@80" metric that samples the model \\(80\\) times and is scored as correct if _any_ of these candidates passes. In line with Tay et al. (2023), we suspect that pretraining the model to generate longer contiguous spans is a better-matched objective for downstream tasks like MBPP requiring long coherent generation.\n' +
      '\n' +
      'Our findings on pretraining objective are less conclusive, with Prefix LM performing the best on WMT and MBPP, while Span Corruption does best on SQuAD. With this in mind, we select "decoder-only + prefix LM" for our subsequent experiments, as this setup is increasingly standard for LLM training, and does relatively well (best or second-best)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline  & **Pretraining** & **WMT14 En-Fr** & **SQuAD** & **MBPP** \\\\\n' +
      '**Architecture** & **Objective** & **(BLEU)** & **(F1)** & **(Pass@80 \\%)** \\\\ \\hline Encoder-Decoder & Prefix LM & 27.6 & 75.8 & 0.0 \\\\ Decoder-only & Prefix LM & **29.8** & 77.4 & **12.2** \\\\ Encoder-Decoder & Span Corruption & 28.7 & 78.2 & 0.0 \\\\ Decoder-only & Span Corruption & 29.1 & **80.6** & 11.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Diffusion model performance on three tasks across model architecture and pretraining objective. The Decoder-only architecture outperforms Encoder-Decoder across all three tasks, despite using fewer parameters.\n' +
      '\n' +
      'Figure 1: Pretraining objectives and model architectures. The <X> and <Y> symbols are unique sentinel tokens denoting masked spans. Note, the “masking noise” applied to produce the span corruption input/target is independent from the “diffusion noise” which randomly corrupts a subset of target tokens. Loss is only computed over target tokens. In the decoder-only setting, input tokens are frozen when computing the unrolled logits input (\\(l_{2}\\)).\n' +
      '\n' +
      'across all our tasks.\n' +
      '\n' +
      '### Transfer learning baselines\n' +
      '\n' +
      'We now turn to testing various transfer learning strategies across model scales. As our core baselines, we pretrain both AR and diffusion models at Base (\\(280\\)M), Large (\\(270\\)M), and XL (\\(1.7\\)B) sizes. These all use a decoder-only architecture and prefix LM objective, and train on the same pretraining mixture from the previous section (\\(80\\)% multilingual web pages and \\(20\\)% Python code). As before, we pretrain for \\(1\\)M steps, with batch size \\(128\\) and sequence length \\(1024\\). Note, our diffusion models use bidirectional attention to allow modifying all sequence positions in parallel, but are otherwise architecturally identical to their AR counterparts.\n' +
      '\n' +
      'For the AR baselines, at inference time, we use greedy decoding for SQuAD, following T5, and use temperature sampling for MBPP, following Austin et al. (2021). For WMT, we use greedy decoding as opposed to the more commonly used beam search for a fairer comparison, as we did not investigate the use of beam search for diffusion models; see Reid et al. (2023) for work in this direction.\n' +
      '\n' +
      'We then fine-tune each of these models separately for each of our three tasks. Results are shown in Table 2, and discussed in section SS4.5.\n' +
      '\n' +
      '### AR2Diff: Adapting from AR to diffusion\n' +
      '\n' +
      'Beyond pure AR and pure diffusion training, we explore "AR2Diff" methods for adapting a pretrained AR model into a diffusion model later in training. First, we experiment with simply fine-tuning an AR checkpoint directly using our diffusion training procedure--enabling bidirectional attention, and using the SUNDAE diffusion training loss. We refer to this method as AR2Diff0, and use our baseline AR model checkpoint as the starting point for fine-tuning.\n' +
      '\n' +
      'We also experiment with pretraining the model for additional steps as a diffusion model _before_ fine-tuning, as illustrated in Figure 2. We start with our pretrained AR checkpoint, continue pretraining for an additional \\(N\\) steps using diffusion training, and then fine-tune (still with diffusion) on each evaluation task separately. We refer to this method as AR2Diff\\({}_{N}\\).\n' +
      '\n' +
      '### Core results\n' +
      '\n' +
      'Results comparing AR2Diff to our autoregressive and diffusion baselines across model sizes are shown in Table 2.\n' +
      '\n' +
      'On WMT14 En-Fr, the AR baseline performs the best across model sizes.6 Our observed gap between diffusion and AR is larger than that of Savinov et al. (2022), where SUNDAE text diffusion comes with \\(1\\) BLEU point of an AR baseline. The difference may be due to our (i) using a transfer learning setting where we pretrain before fine-tuning, (ii) not using SUNDAE\'s length prediction module, (iii) sampling fewer candidates at inference time (\\(8\\) vs. \\(16\\)).\n' +
      '\n' +
      'Footnote 6: We note our Base AR baseline underperforms (\\(32.27\\) vs. \\(37.5\\)) a similar baseline from Raffel et al. (2020), a Base size decoder-only model trained with the same prefix LM objective. This could stem from differences in pretraining data, model architecture, fine-tuning procedure, and/or inference settings (e.g., our use of greedy decoding).\n' +
      '\n' +
      'Interestingly, while at Base size AR2Diff provides no advantage on WMT, at Large and XL sizes we see AR2Diff delivers a significant gain over the pure diffusion baseline, and this gain increases with the length of adaptation. This suggests that AR2Diff may be valuable not just as a resource-saving method (leveraging AR checkpoints to avoid pretraining diffusion models from scratch), but also as a means of achieving stronger diffusion models through mixed-objective training.\n' +
      '\n' +
      'On SQuAD question answering, our diffusion baseline outperforms the AR baseline at Base and Large sizes (Base: \\(68.1\\to 77.4\\), Large:\n' +
      '\n' +
      'Figure 2: Illustration of our AR2Diff method. 1) Pretrain an AR decoder with causal attention. 2) Continue pretraining as a diffusion model with bidirectional attention. 3) Fine-tune as a diffusion model on the end task.\n' +
      '\n' +
      '\\(78.4\\!\\rightarrow\\!80.6\\)), but underperforms at XL size (\\(84.1\\!\\rightarrow\\!82.8\\)).7 While adapting to diffusion only during fine-tuning (AR2Diff\\({}_{0}\\)) is ineffective, adapting for \\(N\\) steps before fine-tuning (AR2Diff\\({}_{N}\\)) outperforms the AR baseline at most sizes, and improves monotonically with \\(N\\).\n' +
      '\n' +
      'Footnote 7: As on WMT, these scores are below the results reported by Raffel et al. (2020) using a similar baseline (\\(85.4\\)). See footnote 6.\n' +
      '\n' +
      'On MBPP code synthesis, diffusion outperforms the AR baseline for two out of three model sizes, including the largest XL size (\\(15.5\\!\\rightarrow\\!18.8\\)). As on other tasks, AR2Diff tends to improve with longer adaptation before fine-tuning.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      'Our results so far have performed diffusion inference by running \\(10\\) steps ("num_steps") of denoising over \\(8\\) randomly sampled decoder inputs per example ("num_samples"). Note, only the output with the highest model score is used for evaluation. Table 3 shows the results of varying num_steps\\(\\in\\{5,\\,10,\\,20\\}\\) and num_samples\\(\\in\\{4,\\,8,\\,16\\}\\). On the MBPP code synthesis task, we find that increasing step and samples boosts performance, in line with Savinov et al. (2022). Increasing denoising steps is particularly helpful (\\(5.5\\!\\rightarrow\\!16.7\\)), but at the cost of slower inference. On SQuAD the effect of these parameters is more marginal. More generally, we suspect that additional steps and samples may be helpful on long-form text generation tasks like MBPP that are relatively underspecified (e.g., admitting many correct answers in different styles). By comparison, SQuAD targets are typically short, and are constrained to be spans from the input.\n' +
      '\n' +
      '### Inference speed analysis\n' +
      '\n' +
      'Diffusion language models have the potential to reduce inference serving costs of long text generation, compared with AR models. Here we show some preliminary results on the inference speed quantitatively. We decode sequences of equal length with AR and diffusion models, and measure corresponding wall-clock times. For diffusion models, we use \\(10\\) diffusion steps as our base case, matching our primary evaluation setup for the WMT, SQuAD and MBPP tasks.\n' +
      '\n' +
      'We observe an increasing advantage of using diffusion for inference speedup when the generation\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline  & \\multicolumn{2}{c}{**WMT14 En-Fr**} & \\multicolumn{1}{c}{**SQuAD**} & \\multicolumn{1}{c}{**MBPP**} \\\\\n' +
      '**Method** & **Size** & **(BLEU)** & **(F1)** & **(Pass@80 \\%)** \\\\ \\hline Autoregressive & Base & **33.27** & 68.11 & 5.5 \\\\ Diffusion & Base & 29.83 & **77.41** & **12.2** \\\\ AR2Diff\\({}_{0}\\) & Base & 29.62 & 64.77 & 1.1 \\\\ AR2Diff\\({}_{10,000}\\) & Base & 29.41 & 68.12 & 4.4 \\\\ AR2Diff\\({}_{100,000}\\) & Base & 29.92 & 71.87 & 7.7 \\\\ \\hline Autoregressive & Large & **34.92** & 78.43 & **15.5** \\\\ Diffusion & Large & 29.36 & 80.56 & 12.2 \\\\ AR2Diff\\({}_{0}\\) & Large & 31.14 & 77.82 & 3.3 \\\\ AR2Diff\\({}_{10,000}\\) & Large & 31.97 & 79.62 & 8.8 \\\\ AR2Diff\\({}_{100,000}\\) & Large & 32.20 & **80.71** & 10.0 \\\\ \\hline Autoregressive & XL & **35.48** & **84.08** & 15.5 \\\\ Diffusion & XL & 29.30 & 82.78 & **18.8** \\\\ AR2Diff\\({}_{0}\\) & XL & 32.36 & 80.95 & 6.6 \\\\ AR2Diff\\({}_{10,000}\\) & XL & 32.39 & 80.71 & 11.1 \\\\ AR2Diff\\({}_{100,000}\\) & XL & 32.55 & 83.54 & 15.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Performance of various models across three tasks and three sizes, comparing: (i) an AR baseline, (ii) a diffusion baseline, and (iii) AR2Diff models that adapt the pretrained AR baseline via diffusion training for \\(N\\) steps before fine-tuning using diffusion, with \\(N\\in\\{0,\\,10\\text{K},\\,100\\text{K}\\}\\).\n' +
      '\n' +
      'Figure 3: By varying the decoding sequence length, we measure inference time of autoregressive decoding vs. diffusion decoding\n' +
      '\n' +
      'is long. Figure 3 shows as the decoding sequence length increases from \\(500\\) tokens (e.g., MBPP task) to \\(4\\),\\(000\\) tokens, the speedup gained by diffusion (using \\(10\\) steps) increases from \\(10\\times\\) to \\(30\\times\\).\n' +
      '\n' +
      'Note that a single AR decoding step (\\(14\\) ms per token generated) is still much faster than a single diffusion step (\\(179\\) ms per denoising step) in our implementation. This is likely due to the diffusion model\'s lacking the key-value caching widely used to optimize AR inference. Whether caching or other efficiency optimizations can further extend the speed gains of diffusion is an interesting question for future research.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We are grateful to Jiaxin Shi for helpful comments on an earlier draft.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2015) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Cergy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheria, Matthew Jagielski, Wenhao Jia, Kathleen Keneady, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aviko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.\n' +
      '* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models.\n' +
      '* Bojar et al. (2014) Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales Tambyna. 2014. Findings of the 2014 workshop on statistical machine translation. In _Proceedings of the Ninth Workshop on Statistical Machine Translation_, pages 12-58, Baltimore, Maryland, USA. Association for Computational Linguistics.\n' +
      '* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\n' +
      '* Chen et al. (2023) Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. 2023.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & & & \\begin{tabular}{c} **SQuAD** \\\\ **(F1)** \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} **MBPP** \\\\ **(Pass@80 \\%)** \\\\ \\end{tabular} \\\\ \\hline Autoregressive & - & - & 68.11 & 5.5 \\\\ \\hline Diffusion & 5 & 8 & 77.41 & 5.5 \\\\ Diffusion & 10 & 8 & 77.41 & 12.2 \\\\ Diffusion & 20 & 8 & **77.72** & **16.7** \\\\ \\hline Diffusion & 10 & 4 & **77.51** & 11.1 \\\\ Diffusion & 10 & 8 & 77.41 & 12.2 \\\\ Diffusion & 10 & 16 & 77.13 & **13.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablations on diffusion inference hyperparameters num_steps and num_samples. Increasing steps and samples leads to clear gains on MBPP, which requires long-form code synthesis, while the effects on SQuAD extractive QA are marginal.\n' +
      '\n' +
      'Analog bits: Generating discrete data using diffusion models with self-conditioning. In The Eleventh International Conference on Learning Representations, Cited by: SS1.\n' +
      '* A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsyvashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Orbakharan, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Lu, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omenick, A. M. Dai, T. Sankaranarayana Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel (2022)Palm: scaling language modeling with pathways. Cited by: SS1.\n' +
      '* S. Dieleman, L. Sartran, A. Roshannai, N. Savinov, Y. Ganin, P. H. Richemond, A. Doucet, R. Strudel, C. Dyer, C. Durkan, C. Hawthorne, R. Leblond, W. Grathwohl, and J. Adler (2022)Continuous diffusion for categorical data. Cited by: SS1.\n' +
      '* M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer (2019)Mask-predict: parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, pp. 6112-6121. Cited by: SS1.\n' +
      '* S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong (2023)Diffuseq: sequence to sequence text generation with diffusion models. Cited by: SS1.\n' +
      '* J. Gu, J. Bradbury, C. Xiong, V. O.K. Li, and R. Socher (2018)Non-autoregressive neural machine translation. In International Conference on Learning Representations, Cited by: SS1.\n' +
      '* J. Gu and X. Kong (2021)Fully non-autoregressive neural machine translation: tricks of the trade. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, pp. 120-133. Cited by: SS1.\n' +
      '* Y. Kim and A. M. Rush (2016)Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas, pp. 1317-1327. Cited by: SS1.\n' +
      '* D. K. Koetkov, R. Li, L. Ben Allal, J. Li, C. Mou, C. Mouz Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries (2022)The stack: 3 tb of permissively licensed source code. Cited by: SS1.\n' +
      '* J. Lee, E. Mansimov, and K. Cho (2018)Deterministic non-autoregressive neural sequence modeling by iterative refinement. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1173-1182. Cited by: SS1.\n' +
      '* X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. Hashimoto (2022)Diffusion-LM improves controllable text generation. In Advances in Neural Information Processing Systems, Cited by: SS1.\n' +
      '* Z. Lin, Y. Gong, Y. Shen, T. Wu, Z. Fan, C. Lin, N. Duan, and W. Chen (2023)Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise. Cited by: SS1.\n' +
      '* O. L. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020)Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research21 (140), pp. 1-67. Cited by: SS1.\n' +
      '* P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang (2016)SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas, pp. 2383-2392. Cited by: SS1.\n' +
      '* A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen (2022)Hierarchical text-conditional image generation with clip latents. Cited by: SS1.\n' +
      '* M. Reid, V. Josua Hellendoorn, and G. Neubig (2023)DiffusER: diffusion via edit-based reconstruction. In The Eleventh International Conference on Learning Representations, Cited by: SS1.\n' +
      '* A. Roberts, H. W. Chung, A. Levskaya, G. Mishra, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu, M. van Zee, J. Austin, S. Goodman, L. Baldini Soares, H. Hu, S. Tsyvashchenko, A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Kenealy, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omenick, B. Saeta, R. Sepassi, A. Spiridonov,Joshua Newlan, and Andrea Gesmundo. 2022. Scaling up models and data with t5x and seqio. _arXiv preprint arXiv:2203.17189_.\n' +
      '* Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2021. High-resolution image synthesis with latent diffusion models. _CoRR_, abs/2112.10752.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic text-to-image diffusion models with deep language understanding. In _Advances in Neural Information Processing Systems_.\n' +
      '* Saharia et al. (2020) Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi. 2020. Non-autoregressive machine translation with latent alignments. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1098-1108, Online. Association for Computational Linguistics.\n' +
      '* Savinov et al. (2022) Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. 2022. Step-unrolled denoising autoencoders for text generation. In _International Conference on Learning Representations_.\n' +
      '* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265, Lille, France. PMLR.\n' +
      '* Stern et al. (2019) Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit. 2019. Insertion transformer: Flexible sequence generation via insertion operations. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 5976-5985. PMLR.\n' +
      '* Strudel et al. (2022) Robin Strudel, Corentin Tallec, Florent Altche, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, and Remi Leblond. 2022. Self-conditioned embedding diffusion for text generation.\n' +
      '* Tay et al. (2023) Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc.\n' +
      '* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In _Advances in Neural Information Processing Systems_.\n' +
      '* Wu et al. (2023) Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, and Weizhu Chen. 2023. Ar-diffusion: Auto-regressive diffusion model for text generation.\n' +
      '* Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498, Online. Association for Computational Linguistics.\n' +
      '* Ye et al. (2023) Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. 2023. Diffusion language models can perform many tasks with scaling and instruction-finetuning.\n' +
      '* Yuan et al. (2023) Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. 2023. Seqdiffuseq: Text diffusion with encoder-decoder transformers.\n' +
      '* Zheng et al. (2023) Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. 2023. A reparameterized discrete diffusion model for text generation.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>