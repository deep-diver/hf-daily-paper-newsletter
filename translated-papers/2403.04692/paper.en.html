<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PixArt-\\(\\Sigma\\): Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation\n' +
      '\n' +
      'Junsong Chen\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Chongjian Ge\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Enze Xie\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Yue Wu\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Lewei Yao\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Xiaozhe Ren\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Zhongdao Wang\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Ping Luo\n' +
      '\n' +
      '3Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Huchuan Lu\n' +
      '\n' +
      '2Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' Zhenguo Li\n' +
      '\n' +
      '1Huawei Noah\'s Ark Lab 2\n' +
      '\n' +
      'Dalian University of Technology 3 HKU 4 HKUST\n' +
      '\n' +
      'Project Page: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'In this paper, we introduce PixArt-\\(\\Sigma\\), a Diffusion Transformer model (DiT) capable of directly generating images at 4K resolution. PixArt-\\(\\Sigma\\) represents a significant advancement over its predecessor, PixArt-\\(\\alpha\\), offering images of markedly higher fidelity and improved alignment with text prompts. A key feature of PixArt-\\(\\Sigma\\) is its training efficiency. Leveraging the foundational pre-training of PixArt-\\(\\alpha\\), it evolves from the \'weaker\' baseline to a\'stronger\' model via incorporating higher quality data, a process we term "weak-to-strong training". The advancements in PixArt-\\(\\Sigma\\) are twofold: (1) High-Quality Training Data: PixArt-\\(\\Sigma\\) incorporates superior-quality image data, paired with more precise and detailed image captions. (2) Efficient Token Compression: we propose a novel attention module within the DiT framework that compresses both keys and values, significantly improving efficiency and facilitating ultra-high-resolution image generation. Thanks to these improvements, PixArt-\\(\\Sigma\\) achieves superior image quality and user prompt adherence capabilities with significantly smaller model size (0.6B parameters) than existing text-to-image diffusion models, such as SDXL (2.6B parameters) and SD Cascade (5.1B parameters). Moreover, PixArt-\\(\\Sigma\\)\'s capability to generate 4K images supports the creation of high-resolution posters and wallpapers, efficiently bolstering the production of high-quality visual content in industries such as film and gaming.\n' +
      '\n' +
      'Keywords:T2I Synthesis, Diffusion Transformer, Efficient Model +\n' +
      'Footnote †: Equal contribution. Work done during the students’ internships at Huawei Noah’s Ark Lab.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The recent emergence of high-quality Text-to-Image (T2I) models has profoundly impacted the AI Generated Content (AIGC) community. This includes both proprietary models such as DALL-E 3 [30], Midjourney [28], as well as open-source models like Stable Diffusion [35] and PixArt-\\(\\alpha\\)[4]. Nonetheless, developing a top-tier T2I model involves considerable resources; for instance, training SD1.5 from scratch necessitates about 6000 A100 GPU days [35], posing a substantial barrier to individual researchers with limited resources and impeding innovation\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      'within the AIGC community. Over time, the AIGC community will gain access to continuously updated, higher-quality datasets and more advanced algorithms. A pivotal question is: _how can we efficiently integrate these new elements into an existing model, achieving a more powerful version within the constraints of limited resources?_\n' +
      '\n' +
      'To explore this issue, our research focuses on enhancing PixArt-\\(\\alpha\\), an efficient T2I training method. PixArt-\\(\\alpha\\) represents an early venture within the DiT framework, a model structure with significant potential, as evidenced by works such as Sora [32] and Stable Diffusion 3 [41]. To maximize this potential, we build upon the pre-trained foundation of PixArt-\\(\\alpha\\), integrating advanced elements to facilitate its continuous improvement, resulting in a more powerful model, PixArt-\\(\\Sigma\\). We refer to this process of evolving from a relatively weaker baseline to a stronger model through efficient training as "weak-to-strong training". Specifically, to achieve "weak-to-strong training", we introduce the following enhancements:\n' +
      '\n' +
      '**Higher-Quality Training Data**: We collect a high-quality dataset superior to that used in PixArt-\\(\\alpha\\), focusing on two key aspects: (i) **High-quality images**: The dataset comprises 33M high-resolution images sourced from the Internet, all exceeding 1K resolution, including 2.3M images with resolutions around 4K. These images are predominantly characterized by their high aesthetic and encompass a wide range of artistic styles. (ii) **Dense and accurate captions**: To provide more precise and detailed captions for the aforementioned images, we replace the LLaVA [21] used in PixArt-\\(\\alpha\\) with a more powerful image captioner, Share-Captioner [5]. Furthermore, to improve the model\'s alignment capacity between the textual and visual concepts, we extend the token length of the text encoder (_i.e._, Flan-T5 [9]) to approximately 300 words. We observe these improvements effectively eliminate the model\'s tendency for hallucination, leading to higher-quality text-image alignment.\n' +
      '\n' +
      '**Efficient Token Compression**: To enhance PixArt-\\(\\alpha\\), we expand its generation resolution from 1K to 4K. Generating images at ultra-high resolutions (_e.g._, 2K/4K) introduces a significant increase in the number of tokens, leading to a substantial rise in computational demand. To address this challenge, we introduced a self-attention module with key and value token compression tailored to the DiT framework. Specifically, we utilize group convolutions with a stride of 2 for local aggregation of keys and values. Additionally, we employ a specialized weight initialization scheme, allowing for a smooth adaptation from a pre-trained model without KV compression. This design effectively reduces training and inference time by \\(\\sim\\)34% for high-resolution image generation.\n' +
      '\n' +
      '**Weak-to-Strong Training Strategy**: we propose several fine-tuning techniques to rapidly adapt from a weak model to a strong model efficiently. That includes (1) replacing with a more powerful Variational Autoencoder (VAE) [35], (2) scaling from low to high resolution, and (3) evolving from a model without Key-Value (KV) compression to one with KV compression. These outcomes confirm the validity and effectiveness of the "weak-to-strong training" approach.\n' +
      '\n' +
      'Through the proposed improvements, PixArt-\\(\\Sigma\\) achieves high-quality 4K resolution image generation at a minimal training cost and model parameters. Specifically, fine-tuning from a pre-trained model, we additionally utilize only **9%** of the GPU days required by PixArt-\\(\\alpha\\) to achieve a strong 1K high-resolution image generation model, which is impressive considering we replace with new training data and a more powerful VAE. Moreover, we use only **0.6B** parameters while SDXL [35] and SD Cascade [34] use 2.6B and 5.1B parameters respectively. Images generated by PixArt-\\(\\Sigma\\) possess an aesthetic quality comparable to current top-tier T2I products, such as DALL-E 3 [30] and MJV6 [28] (as illustrated in Fig. 4). Additionally, PixArt-\\(\\Sigma\\) also demonstrates exceptional capability for fine-grained alignment with textual prompts (as shown in Fig. 2 and 3).\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Diffusion Transformers.** The Transformer architecture has achieved remarkable success across various domains, such as language modeling [36, 37], computer vision [22, 42, 50, 52], and other areas [3, 13]. In the realm of diffusion models, DiT [33] and UViT [2] pioneer the use of Transformer architecture. Subsequent works, including DiffiT [14], SiT [27], and FiT [25], have improved upon DiT\'s architecture, while [11, 51] enhance training efficiency through masked modeling techniques. For Text-to-Image (T2I) synthesis, PixArt-\\(\\alpha\\)[4] explore efficient T2I training schemes, achieving the first Transformer-based T2I model capable of\n' +
      '\n' +
      'Figure 2: 4K image generation with complex dense instructions. PixArt-\\(\\Sigma\\) can directly generate 4K resolution images without post-processing, and accurately respond to the given prompt.\n' +
      '\n' +
      'generating 1024px high-quality images. The recent advent of the powerful video generation model Sora [32] has further underscored the potential of Diffusion Transformers. In this work, for the first time, we explore using the Transformer architecture to generate 4K ultra-high-resolution images directly, tackling the computational complexity challenges posed by involving long-sequence tokens.\n' +
      '\n' +
      '**High Resolution Image Generation** greatly enhances visual quality and is important in various industries such as film and gaming. However, increasing\n' +
      '\n' +
      'Figure 3: **Comparison of PixArt-\\(\\Sigma\\) with open-source models, e.g., PixArt-\\(\\alpha\\) and SDXL**: Compared with PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\) improves the realism of portraits and the capability of semantic analysis. Compared with SDXL, our method has a better ability to follow user instructions. The keywords are highlighted as blue.\n' +
      '\n' +
      'image resolution introduces challenges due to the substantial increase in computational demands. Numerous methods have been explored in this direction. For instance, Imagen [39], GigaGAN [17] and Stable Diffusion [38] introduce an additional super-resolution network, while Stable Cascade [34] employs multiple diffusion networks to increase resolution progressively. These combined-model solutions, however, can introduce cumulative errors. On the other hand, works like SDXL [35], DALL-E 2 [29], Playground [18] and PixArt-\\(\\alpha\\)[4] have attempted to generate high-resolution images using diffusion models directly. Nevertheless, these efforts are capped at generating images with resolutions up to 1024px\n' +
      '\n' +
      'Figure 4: **Compare PixArt-\\(\\Sigma\\) and four other T2I products**: Firefly 2, Imagen 2, Dalle 3, and Midjourney 6. Images generated by PixArt-\\(\\Sigma\\) are very competitive with these commercial products.\n' +
      '\n' +
      'due to computational complexity. In this paper, we push this boundary to 4K resolution, significantly enhancing the visual quality of the generated content.\n' +
      '\n' +
      '**Efficient Transformer architecture.** The self-attention mechanism in Transformer suffers from quadratic computational complexity with respect to the number of tokens, which hinders the scaling up of token quantity. Many works have sought improvements in this area: (1) Sparse Attention [6, 7, 44, 45, 47], which reduces the overall computational load by selectively processing a subset of tokens. For instance, PVT v2 [45] employs a convolutional kernel to condense the space of the key and value, thus lowering the complexity involved in computing the attention. (2) Local Attention [12, 22, 46, 53] focuses attention within nearby regions; notably, Swin Transformer [22] utilizes window-based attention to limit computations to within a specified window size. (3) Low-rank/Linear Attention [8, 24, 43]. The Linformer [43] reduces the computational complexity of the self-attention mechanism through low-rank approximations. In this paper, inspired by PVT v2 [45], we employ a self-attention mechanism based on key/value compression to mitigate the high complexity of processing 4K images.\n' +
      '\n' +
      '## 3 Framework\n' +
      '\n' +
      '### Data Analysis\n' +
      '\n' +
      '**Higher Aesthetic and higher Resolution.** To enhance the aesthetic quality of our dataset, we expand our internal data from 14M to 33M. For clarity, we name the two datasets Internal-\\(\\alpha\\) and Internal-\\(\\Sigma\\), respectively. Note that this expansion still falls short compared to the vast images utilized by currently available open-source models like SD v1.5, which uses 2B data. We demonstrate that effective training strategies with limited data amount can still obtain a strong T2I model.\n' +
      '\n' +
      'The images within Internal-\\(\\Sigma\\) are above 1K resolution. To facilitate 4K resolution generation, we additionally collect a dataset of 8M real photographic images at 4K resolution. To ensure aesthetic quality, we employ an aesthetic scoring model (AES) [1] to filter these 4K images. This process yields a highly refined dataset of 2M ultra-high-resolution and high-quality images.\n' +
      '\n' +
      'Figure 5: **Comparative illustration of hallucinations: Contrasting differences in hallucination occurrences between LLaVA and Share-Captioner, with red indicating hallucinations and green denoting correctness.**\n' +
      '\n' +
      'Interestingly, we have observed that as the resolution of the images increases, there is an improvement in model\'s fidelity (Frechet Inception Distance (FID) [16]) and semantic alignment (CLIP Score), which underscores the importance of the capabilities of generating high-resolution images.\n' +
      '\n' +
      '**Better Text-Image Alignment.**\n' +
      '\n' +
      'Recent works such as PixArt-\\(\\alpha\\)[4] and DALL-E 3 [30] emphasize the significance of text-image description alignment. Strengthening this alignment is crucial for boosting model capabilities. To refine our collected "raw" descriptions further, we focus on improving both the length and accuracy of our captions. Notably, our captions (Internal-\\(\\Sigma\\)) show several advantages over the one used in PixArt-\\(\\alpha\\) (Internal-\\(\\alpha\\)) in the following aspects:\n' +
      '\n' +
      '1. Enhanced caption accuracy: As depicted in Fig. 5, LLaVa used in PixArt-\\(\\alpha\\) has a certain hallucination problem. We leverage a more powerful Visual-language model, i.e., Share-Captioner [5], to generate detailed and correct captions, augmenting the collected raw prompts.\n' +
      '\n' +
      '2. Increased caption length: As shown in Tab. 1 and Fig. 6, the average caption length increased significantly to 180 words, highly enhancing the descriptive power of the captions. Additionally, we extend the token processing length of the text encoder from 120 tokens (as in Internal-\\(\\alpha\\)) to 300 tokens. Our model is trained on a mix of long (Share-Captioner) and short (raw) captions with a ratio of 60% and 40%, respectively. This approach enhances the diversity of textual descriptions and mitigates potential biases that might arise from solely relying on generative captions.\n' +
      '\n' +
      'Tab. 1 demonstrates a summary for both Internal-\\(\\alpha\\) and -\\(\\Sigma\\), where we assess the diversity of the datasets through various metrics, including the noun variety, total noun count, average caption length, and average nouns per image.\n' +
      '\n' +
      '**High-Quality Evaluation Dataset.** Most SoTA T2I models chose MSCOCO [20] as the evaluation set to assess the FID and CLIP Scores. However, we observe evaluations conducted on the MSCOCO dataset may not adequately reflect a model\'s capabilities in aesthetics and text-image alignment. Therefore, we propose a curated set comprising 30,000 high-quality, aesthetically pleasing text-image pairs to facilitate the assessment. The selected samples of the dataset are presented in the appendix. This dataset is designed to provide a more comprehensive evaluation of a model\'s performance, particularly in capturing the intricacies of aesthetic appeal and the fidelity of alignment between textual descriptions and\n' +
      '\n' +
      'Figure 6: Histogram Visualization of the Caption Length. We randomly select 1M captions from the raw captions, Internal-\\(\\alpha\\), and Internal-\\(\\Sigma\\) to draw the corresponding histogram. _ACL_ denotes the average caption length.\n' +
      '\n' +
      'visual content. Unless otherwise specified, the evaluation experiments in the paper are conducted in the collected High-Quality Evaluation Dataset.\n' +
      '\n' +
      '### Efficient DiT Design\n' +
      '\n' +
      'An efficient DiT network is essential since the computational demand significantly increases when generating images at ultra-high resolutions. The attention mechanism plays a pivotal role in the efficacy of Diffusion Transformers, yet its quadratic computational demands significantly limit model scalability, particularly at higher resolutions _e.g._, 2K and 4K. Inspired by PVT v2 [45], we incorporate KV compression within the original PixArt-\\(\\alpha\\)\'s framework to address the computational challenges. This design adds a mere 0.018% to the total parameters yet achieves efficient reduction in computational costs via token compression, while still preserving both spatial and semantic information.\n' +
      '\n' +
      '**Key-Value (KV) Token Compression.** Our motivation stems from the intriguing observation that applying key-value (KV) token compression directly to the pre-trained PixArt-\\(\\alpha\\) can still generate reasonable images. This suggests a redundancy in the features. Considering the high similarity within adjacent \\(R\\times R\\) patches, we assume that feature semantics within a window are redundant and can be compressed reasonably. We propose KV token compression, which is denoted as \\(f_{c}(\\cdot)\\), to compress token features within a \\(R\\times R\\) window through a compression operator, as depicted in Fig. 7.\n' +
      '\n' +
      'Furthermore, to mitigate the potential information loss caused by KV compression in self-attention computation, we opt to retain all the tokens of queries (Q). This strategy allows us to utilize KV compression effectively while mitigating the risk of losing crucial information. By employing KV compression, we enhance the efficiency of attention computations and reduce the computation complexity from \\(O(N^{2})\\) to \\(O\\left(\\frac{N^{2}}{R^{2}}\\right)\\), thereby making the computational cost of directly generating high-resolution images manageable.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Volume** & **Caption** & **VN/DN** & **Total** & **Noun** & **ACL** & **Average** \\\\ \\hline Internal-\\(\\alpha\\) & 14M & Raw & 187K/931K & 175M & 25 & 11.7/Img \\\\ Internal-\\(\\alpha\\) & 14M & LLaVA & 28K/215K & 536M & 98 & 29.3/Img \\\\ Internal-\\(\\alpha\\) & 14M & Share-Captioner & 51K/420K & 815M & 184 & 54.4/Img \\\\ \\hline Internal-\\(\\Sigma\\) & 33M & Raw & 294K/1512K & 485M & 35 & 14.4/Img \\\\ Internal-\\(\\Sigma\\) & 33M & Share-Captioner & 77K/714K & 1804M & 180 & 53.6/Img \\\\\n' +
      '4K-\\(\\Sigma\\) & 2.3M & Share-Captioner & 24K/96K & 115M & 163 & 49.5/Img \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Statistics of noun concepts for different datasets.****VN**: valid distinct nouns (appearing more than 10 times); **DN**: total distinct nouns; **Average**: average noun count per image; **ACL**: Average Caption length.\n' +
      '\n' +
      'Figure 7: **Design of KV Token Compression.** We merge KV tokens in spatial space to reduce the computation complexity.\n' +
      '\n' +
      '\\[\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{Q\\cdot f_{c}(K)^{T}}{\\sqrt{d_{k}}} \\right)f_{c}(V) \\tag{1}\\]\n' +
      '\n' +
      'We compress deep layers using the convolution operator "Conv2\\(\\times\\)2" with specific initialization. Detailed experiments on other design variants are discussed in Sec. 5. Specifically, we design a specialized convolution kernel initialization "Conv Avg Init" that utilizes group convolution and initializes the weights \\(w=\\frac{1}{R^{2}}\\), equivalent to an average operator. This initialization strategy can initially produce coarse results, accelerating the fine-tuning process while only introducing 0.018% additional parameters.\n' +
      '\n' +
      '### Weak-to-Strong Training Strategy\n' +
      '\n' +
      'We propose several efficient training strategies to enhance the transition from a "weak" model to a "strong" model. These strategies encompass VAE rapid adaptation, high-resolution fine-tuning, and KV Token compression.\n' +
      '\n' +
      '**Adapting model to new VAEs.** As VAEs continue to develop, training T2I models from scratch is resource-intensive. We replace PixArt-\\(\\alpha\\)\'s VAE with SDXL\'s VAE and continue fine-tuning the diffusion model. We observe a rapid convergence phenomenon that fine-tuning quickly converges at 2K training steps as shown in Fig 8 (a). Fine-tuning is more efficient when dealing with VAE model transferring and negates the necessity of training from scratch.\n' +
      '\n' +
      '**Adapting to Higher-Resolution.** When we fine-tune from a low-resolution (LR) model to a high-resolution (HR) model, we observe a performance degra\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Resolution & Iterations & FID \\(\\downarrow\\) & CLIP \\(\\uparrow\\) \\\\ \\hline\n' +
      '256 & 20K & 16.56 & 0.270 \\\\\n' +
      '256 \\(\\rightarrow\\) 512 & 1K & 9.75 & 0.272 \\\\\n' +
      '256 \\(\\rightarrow\\) 512 & 100K & 8.91 & 0.276 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: We fine-tune a high-resolution model from a low-resolution model and observe that even fine-tuning for a relatively short duration, such as 1K steps, can still yield high-quality results.\n' +
      '\n' +
      'Figure 8: This illustration demonstrates how our training strategy designs expedite the model’s convergence during the transition to VAEs, adjustment to higher resolutions, and the KV compression process, facilitating rapid learning from weak to strong.\n' +
      '\n' +
      'dation as shown in Fig. 8 (b), which we attribute to discrepancies in positional embeddings (PE) between different resolutions. To mitigate this issue, we utilize the "PE Interpolation" trick [4, 48]: initializing the HR model\'s PE by interpolating the LR model\'s PE, significantly enhancing the HR model\'s initial status and expediting the fine-tuning process. We can obtain visually pleasing images even within only 100 training iterations. Besides, we quantitatively evaluate the model\'s performance change as illustrated in Tab. 2. The fine-tuning quickly converges at 1K steps, and further training slightly improves the performance. This illustrates that using the "PE Interpolation" trick enables rapid convergence of higher resolution generation, obviating the need for training from scratch for generating at higher resolutions.\n' +
      '\n' +
      '**Adapting model to KV compression.** We can use KV compression directly when fine-tuning from LR pre-trained models without KV compression. As shown in Fig. 8 (c), with our "Conv Avg Init." strategy, PixArt-\\(\\Sigma\\) starts from a better initial state, making converging easier and faster. Notably, PixArt-\\(\\Sigma\\) performs satisfied visual results even within 100 training steps. Finally, through the KV compression operators and compression layers design in Sec 3.2, we can reduce \\(\\sim\\)34% of the training and inference time.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**Training Details.** We follow Imagen [39] and PixArt-\\(\\alpha\\)[4] to employ the T5 [9]\'s encoder (_i.e._, Flan-T5-XXL) as the text encoder for conditional feature extraction, and use PixArt-\\(\\alpha\\)[4] as our base diffusion model. Unlike most works that extract fixed 77 text tokens, we adjust the length of text tokens from PixArt-\\(\\alpha\\)\'s 120 to 300, as the caption curated in Internal-\\(\\Sigma\\) is much denser to provide highly fine-grained details. To capture the latent features of input images, we employ a pre-trained and frozen VAE from SDXL [35]. Other implementation details are the same as PixArt-\\(\\alpha\\). Models are finetuned on the PixArt-\\(\\alpha\\)\'s 256px pre-trained checkpoint with the position embedding interpolation trick [4]. Our final models, including 1K resolution, are trained on 32 V100 GPUs. We additionally use 16 A100 GPUs to train the 2K and 4K image generation models. For further information, please refer to the appendix.\n' +
      '\n' +
      'Note that we use CAME optimizer [26] with a weight decay of 0 and a constant learning rate of 2e-5, instead of the regular AdamW [23] optimizer. This helps us reduce the dimension of the optimizer\'s state, leading to lower GPU memory without performance degradation.\n' +
      '\n' +
      '**Evaluation Metrics.** To better illustrate aesthetics and semantic ability, we collect 30K high-quality text-image pairs (as mentioned in Sec. 3.1) to benchmark the most powerful T2I models. We mainly evaluate PixArt-\\(\\Sigma\\) via human and AI preference study since FID [38] metrics may not adequately reflect the generation quality. However, we still provide the FID results on the collected dataset in the appendix.\n' +
      '\n' +
      '### Performance Comparisons\n' +
      '\n' +
      'Image Quality Assessment.We qualitatively evaluated our methodology against both closed-source text-to-image (T2I) products and open-source models. As illustrated in Fig. 1, our model can produce high-quality, photo-realistic images with intricate details over diverse aspect ratios and styles. This capability underscores the superior performance of our approach in generating visually compelling content from textual descriptions. As shown in Fig. 3, we compare PixArt-\\(\\Sigma\\) with open-source models SDXL [35] and PixArt-\\(\\alpha\\)[4], our method enhances the realism of portraits and boosts the capacity for semantic analysis. In contrast to SDXL, our approach demonstrates superior proficiency in adhering to user instructions.\n' +
      '\n' +
      'Not only superior to open-source models, but our method is also very competitive with current T2I closed-source products, as depicted in Fig. 4. PixArt-\\(\\Sigma\\) produces photo-realistic results and adheres closely to user instructions, which is on par with contemporary commercial products.\n' +
      '\n' +
      'High-resolution Generation.Our method is capable of directly generating 4K resolution images without the need for any post-processing. Additionally, it excels in accurately following complex, detailed, and long text provided by users, as demonstrated in Fig. 2. Thus, users do not need prompt engineering to achieve satisfactory results.\n' +
      '\n' +
      'Our approach enables direct 4K image generation. In parallel, studies [10, 15] have introduced tuning-free post-processing techniques aimed at generating HR images from LR models or employing super-resolution models [49] to produce HR images. However, their corresponding results often exhibit artifacts for two primary reasons: (1) Accumulative error may arise due to the cascade pipeline. (2) These methods do not capture the true distribution of 4K images nor learn the alignment between text and 4K images. We argue that our method might be a more promising way to generate high-resolution images. Our method yields superior results, and more visual comparison is included in the supplement.\n' +
      '\n' +
      'Human/AI (GPT4V) Preference Study.We evaluate the well-trained model in both the human and AI preference study using a subset of 300 captions randomly collected from the High-Quality Evaluation Dataset mentioned in Sec. 3.1. We collect images generated by overall six open-source models, including PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\), SD1.5 [38], Stable Turbo [40], Stable XL [35], Stable Cascade [34] and Playground-V2.0 [19]. We develop a website for the human preference study to display the prompts and their corresponding images.\n' +
      '\n' +
      'Figure 9: Human(blue)/AI(orange and green) preference evaluation against currrent open T2I models. PixArt-\\(\\Sigma\\) compares favorably against current state-of-the-art T2I models in both image quality and prompt-following.\n' +
      '\n' +
      'This website was distributed to trained evaluators, who were asked to assess the images, ranking them according to quality and how well they matched the text prompts. The results, illustrated by the blue bar in Fig. 9, indicate a marked preference for PixArt-\\(\\Sigma\\) over the other six T2I generators. PixArt-\\(\\Sigma\\) generates superior high-quality images that closely follow user prompts, using a much smaller size (0.6B parameters) compared to existing T2I diffusion models like SDXL (2.6B parameters) and SD Cascade (5.1B parameters).\n' +
      '\n' +
      'Additionally, in our AI preference study, we employ the advanced multimodal model, GPT-4 Vision [31], as the evaluator. For each trial, we supply GPT-4 Vision with two images: one from PixArt-\\(\\Sigma\\) and another from a competing T2I model. We craft distinct prompts guiding GPT-4 Vision to vote based on image quality and image-and-text alignment. The results, represented by orange and green bars in Fig. 9, demonstrate consistent outcomes in both human and AI preference studies. Specifically, PixArt-\\(\\Sigma\\) surpasses the baseline, PixArt-\\(\\alpha\\), in effectiveness. Compared to contemporary advanced models such as Stable Cascaded, PixArt-\\(\\Sigma\\) exhibits competitive or superior performance in terms of image quality and instruction-following abilities.\n' +
      '\n' +
      '## 5 Ablation Studies\n' +
      '\n' +
      'We conduct ablation studies on generation performance on various KV compression designs. Unless specified, the experiments are conducted on 512px generation. The detailed settings of each ablation experiment are included in the appendix.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Layers & FID \\(\\downarrow\\) CLIP-Score \\(\\uparrow\\) \\\\ \\hline N/A & 8.244 & 0.276 \\\\ Shallow (1-14) & 9.278 & 0.275 \\\\ Middle (7-20) & 9.063 & 0.276 \\\\ Deep (14-27) & 8.532 & 0.275 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: KV-Token Compression Settings in Image Generation. This study employs FID, CMMD, and CLIP-Score metrics to assess the impact of various token compression components, such as compression ratio, positions, operators, and varying resolutions. Speed calculation in Tab. 2(c) is Second/Iteration/384 Batch-size.\n' +
      '\n' +
      '### Experimental settings\n' +
      '\n' +
      'We use the test set described in Sec. 3.1 for evaluation. We employ FID to compute the distributional difference between the collected and generated data for comparative metrics. Furthermore, we utilize CLIP-Score to assess the alignment between prompts and the generated images.\n' +
      '\n' +
      '### Compression Designs\n' +
      '\n' +
      '**Compression positions.** We implemented KV compression at different depths within the Transformer structure: in the shallow layers (1\\(\\sim\\)14), the intermediate layers (7\\(\\sim\\)20), and the deep layers (14\\(\\sim\\)27). As indicated in Tab. 3(a), employing KV compression on deep layers notably achieves superior performance. We speculate this is because shallow layers typically encode detailed texture content, while deep layers abstract high-level semantic content. Because compression tends to affect image quality rather than semantic information, compressing deep layers can achieve the least loss of information, making it a practical choice for accelerating training but not compromising generation quality.\n' +
      '\n' +
      '**Compression operators.** We explored the impact of different compression operators. We employed three techniques, random discarding, average pooling, and parametric convolution, to compress 2\\(\\times\\)2 tokens into a single token. As illustrated in Table 3(b), the "Conv 2\\(\\times\\)2" method outperforms the others, underscoring the advantage of using a learnable kernel to more effectively reduce redundant features than simple discarding methods.\n' +
      '\n' +
      '**Compression ratios on different resolutions.** We investigated the influence of varying compression ratios on different resolutions. As shown in Tab. 3(c), remarkably, we find that token compression does not affect the alignment between textual and generated images (CLIP Score) but influences the image quality (FID) across resolutions. Although there is a slight degradation in image quality with increasing compression ratios, our strategy brings a training speedup of 18% to 35%. This suggests that our proposed KV compression is both effective and efficient for achieving high-resolution T2I generation.\n' +
      '\n' +
      '**Speed comparisons on different resolutions.** We further comprehensively validate the speed acceleration in both training and inference in Tab. 3(d). Our method can speed up training and inference by approximately 35% in the 4K generation. Notably, we observe that the training acceleration increases as the resolution rises. For example, the training gradually accelerates from 18% to 35% as the resolution increases from 1K to 4K. This indicates the effectiveness of our method with increasing resolution, demonstrating its potential applicability to even higher-resolution image generation tasks.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'In this paper, we introduce PixArt-\\(\\Sigma\\), a Text-to-Image (T2I) diffusion model capable of directly generating high-quality images at 4K resolution. Building uponthe pre-trained foundation of PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\) achieves efficient training through a novel "weak-to-strong training" methodology. This approach is characterized by the incorporation of higher-quality data and the integration of efficient token compression. PixArt-\\(\\Sigma\\) excels at producing high-fidelity images while adhering closely to textual prompts, surpassing the high standards set by its predecessor, PixArt-\\(\\alpha\\). We believe that the innovations presented in PixArt-\\(\\Sigma\\) will not only contribute to advancements in the AIGC community but also pave the way for entities to access more efficient, and high-quality generative models.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      'Figure 10: Compare PixArt-\\(\\Sigma\\) and four other T2I products: Firefly 2, Imagen 2, Dalle 3, and Midjourney 6. Images generated by PixArt-\\(\\Sigma\\) are very competitive with these commercial products.\n' +
      '\n' +
      'Figure 11: **Compare PixArr-\\(\\Sigma\\) and four other T2I products**: Firefly 2, Imagen 2, Dalle 3, and Midjourney 6. Images generated by PixArt-\\(\\Sigma\\) are very competitive with these commercial products.\n' +
      '\n' +
      'Figure 12: **Illustrations of High-quality images generated by PixArt-\\(\\Sigma\\).** PixArt-\\(\\Sigma\\) is able to generating high-quality images with fine-grained details, and diverse images with different aspect ratios.\n' +
      '\n' +
      'Figure 13: **High-resolution (4K) images generated by PixArt-\\(\\Sigma\\).** PixArt-\\(\\Sigma\\) can directly generate high-quality 4K HD (3840\\(\\times\\)2560) images while preserving fine-grained details.\n' +
      '\n' +
      'Figure 14: High-resolution (4K) images generated by PixArt-\\(\\Sigma\\). PixArt-\\(\\Sigma\\) can directly generate high-quality 4K HD (3840\\(\\times\\)2560) images while preserving fine-grained details.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]Aesthetic predictor (2023), [https://github.com/christophschuhmann/improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor)[2] Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., Zhu, J.: All are worth words: A vit backbone for diffusion models. In: CVPR (2023) [3] Chandra, A., Tunnermann, L., Lofstedt, T., Gratz, R.: Transformer-based deep learning for predicting protein properties in the life sciences. eLife Sciences Publications, Ltd (2023) [4] Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., Li, Z.: PixArt-\\(\\alpha\\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. In: ICLR (2024) [5] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: ShareGPT4V: Improving large multi-modal models with better captions. In: arXiv (2023) [6] Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z.: Chasing sparsity in vision transformers: An end-to-end exploration. In: NeurIPS (2021) [7] Chen, X., Liu, Z., Tang, H., Yi, L., Zhao, H., Han, S.: SparseViT: Revisiting activation sparsity for efficient high-resolution vision transformer. In: CVPR (2023) [8] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.: Rethinking attention with performers. In: ICLR (2021) [9] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. In: arXiv (2022) [10] Du, R., Chang, D., Hospedales, T., Song, Y.Z., Ma, Z.: Demofusion: Democratising high-resolution image generation with no SSSS. In: CVPR (2024) [11] Gao, S., Zhou, P., Cheng, M.M., Yan, S.: Masked diffusion transformer is a strong image synthesizer. In: ICCV (2023) [12] Ge, C., Ding, X., Tong, Z., Yuan, L., Wang, J., Song, Y., Luo, P.: Advancing vision transformers with group-mix attention. In: arXiv (2023) [13] von Glehn, I., Spencer, J.S., Pfau, D.: A self-attention ansatz for ab-initio quantum chemistry. In: ICLR (2023) [14] Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.: Diffit: Diffusion vision transformers for image generation. In: arXiv (2023) [15] He, Y., Yang, S., Chen, H., Cun, X., Xia, M., Zhang, Y., Wang, X., He, R., Chen, Q., Shan, Y.: ScaleCrafter: Tuning-free higher-resolution visual generation with diffusion models. In: ICLR (2024) [16] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local nash equilibrium. In: NeurIPS (2017) [17] Kang, M., Zhu, J.Y., Zhang, R., Park, J., Shechtman, E., Paris, S., Park, T.: Scaling up gans for text-to-image synthesis. In: CVPR (2023) [18] Li, D., Kamko, A., Akhgari, E., Sabet, A., Xu, L., Doshi, S.: Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. In: arXiv (2024) [19] Li, D., Kamko, A., Sabet, A., Akhgari, E., Xu, L., Doshi, S.: Playground v2, [https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic12](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic12)* [20] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 8\n' +
      '* [21] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: arXiv (2023) 3\n' +
      '* [22] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 4, 7\n' +
      '* [23] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: arXiv (2017) 11\n' +
      '* [24] Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., Zhang, L.: Soft: Softmax-free transformer with linear complexity. In: NeurIPS (2021) 7\n' +
      '* [25] Lu, Z., Wang, Z., Huang, D., Wu, C., Liu, X., Ouyang, W., Bai, L.: Fit: Flexible vision transformer for diffusion model. In: arXiv (2024) 4\n' +
      '* [26] Luo, Y., Ren, X., Zheng, Z., Jiang, Z., Jiang, X., You, Y.: CAME: Confidence-guided adaptive memory efficient optimization. In: ACL (2023) 11\n' +
      '* [27] Ma, N., Goldstein, M., Albergo, M.S., Boffi, N.M., Vanden-Eijnden, E., Xie, S.: SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In: arXiv (2024) 4\n' +
      '* [28] Midjourney: Midjourney (2023), [https://www.midjourney.com](https://www.midjourney.com) 1, 4\n' +
      '* [29] OpenAI: Dalle-2 (2023), [https://openai.com/dall-e-2](https://openai.com/dall-e-2) 6\n' +
      '* [30] OpenAI: Dalle-3 (2023), [https://openai.com/dall-e-3](https://openai.com/dall-e-3) 1, 4, 8\n' +
      '* [31] OpenAI: Gpt-4v(ision) system card. In: OpenAI (2023) 13\n' +
      '* [32] OpenAI: Sora (2024), [https://openai.com/sora](https://openai.com/sora) 3, 5\n' +
      '* [33] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV (2023) 4\n' +
      '* [34] Pernias, P., Rampas, D., Richter, M.L., Pal, C., Aubreville, M.: Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. In: ICLR (2023) 4, 6, 12\n' +
      '* [35] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: SDXL: Improving latent diffusion models for high-resolution image synthesis. In: arXiv (2023) 1, 3, 4, 6, 11, 12\n' +
      '* [36] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training. OpenAI blog (2018) 4\n' +
      '* [37] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog (2019) 4\n' +
      '* [38] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 6, 11, 12\n' +
      '* [39] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. In: NeurIPS (2022) 6, 11\n' +
      '* [40] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. In: arXiv (2023) 12\n' +
      '* [41] Stability.AI: Stable diffusion 3 (2024), [https://stability.ai/news/stable-diffusion-3](https://stability.ai/news/stable-diffusion-3) 3\n' +
      '* [42] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: ICML (2021) 4\n' +
      '* [43] Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity. In: arXiv (2020)* [44] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)\n' +
      '* [45] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)\n' +
      '* [46] Xia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: CVPR (2022)\n' +
      '* [47] Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: SegFormer: Simple and efficient design for semantic segmentation with transformers. In: NeurIPS (2021)\n' +
      '* [48] Xie, E., Yao, L., Shi, H., Liu, Z., Zhou, D., Liu, Z., Li, J., Li, Z.: DiffFit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. In: ICCV (2023)\n' +
      '* [49] Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-image generation via large mixture of diffusion paths. In: NeurIPS (2023)\n' +
      '* [50] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token ViT: Training vision transformers from scratch on imagenet. In: ICCV (2021)\n' +
      '* [51] Zheng, H., Nie, W., Vahdat, A., Anandkumar, A.: Fast training of diffusion models with masked transformers. In: arXiv (2023)\n' +
      '* [52] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In: CVPR (2021)\n' +
      '* [53] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: Deformable transformers for end-to-end object detection. ICLR (2021)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>