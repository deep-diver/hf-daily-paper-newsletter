<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# PixArt-\\(\\Sigma\\): 4K 텍스트-이미지 생성을 위한 확산변압기의 약-강 훈련\n' +
      '\n' +
      'Junsong Chen\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 정건게\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 엔제시\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 유우\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 류웨이야오\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 허샤오세\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 중다오 왕\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 핑루오\n' +
      '\n' +
      '화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 후천루\n' +
      '\n' +
      '화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      ' 리진구\n' +
      '\n' +
      '1화웨이 노아 방주 연구소 2\n' +
      '\n' +
      '다롄공과대학교 3HKU 4HKUST\n' +
      '\n' +
      '프로젝트 페이지: [https://pixart-alpha.github.io/PixArt-sigma-project/1](https://pixart-alpha.github.io/PixArt-sigma-project/1)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 4K 해상도의 영상을 직접 생성할 수 있는 Diffusion Transformer Model(DiT)인 PixArt-\\(\\Sigma\\)을 소개한다. PixArt-\\(\\Sigma\\)는 기존의 PixArt-\\(\\alpha\\)보다 훨씬 더 높은 충실도와 텍스트 프롬프트와의 정렬이 향상된 이미지를 제공한다. PixArt-\\(\\Sigma\\)의 주요 특징은 훈련 효율성이다. PixArt-\\(\\alpha\\)의 기초적 사전학습을 활용하여 보다 높은 품질의 데이터를 접목하여 \'Weaker\' 베이스라인에서 \'tronger\' 모델로 진화하며, "weak-to-strong training"이라는 프로세스를 제안한다. PixArt-\\(\\Sigma\\)의 발전은 (1) 고품질 트레이닝 데이터: PixArt-\\(\\Sigma\\)이 우수한 품질의 이미지 데이터를 통합하고, 보다 정확하고 상세한 이미지 캡션을 결합한다. (2) 효율적인 토큰 압축: DiT 프레임워크 내에서 키와 값을 압축하고, 효율성을 크게 향상시키며 초고해상도 이미지 생성을 용이하게 한다. 이러한 개선으로 인해 PixArt-\\(\\Sigma\\)은 SDXL(2.6B 파라미터) 및 SD Cascade(5.1B 파라미터)와 같은 기존의 텍스트-이미지 확산 모델보다 훨씬 작은 모델 크기(0.6B 파라미터)로 우수한 이미지 품질 및 사용자 신속 순응 능력을 달성하고, PixArt-\\(\\Sigma\\)의 4K 이미지 생성 능력은 고해상도 포스터 및 배경화면 생성을 지원하며, 또한 PixArt-\\(\\Sigma\\)의 4K 이미지 생성을 효과적으로 지원한다.\n' +
      '\n' +
      '키워드:T2I 합성, 확산 변압기, 효율적인 모델 +\n' +
      '각주 † : 균등 기여. 화웨이 노아의 방주 연구소에서 학생들이 인턴십을 하는 동안 일을 했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 고품질 텍스트-이미지(T2I) 모델의 출현은 AI 생성 콘텐츠(AIGC) 커뮤니티에 큰 영향을 미쳤다. 여기에는 DALL-E 3[30], Midjourney[28]과 같은 독점 모델과 Stable Diffusion[35] 및 PixArt-\\(\\alpha\\)[4]와 같은 오픈 소스 모델이 모두 포함된다. 그럼에도 불구하고 최상위 T2I 모델 개발에는 상당한 리소스가 포함된다. 예를 들어 SD1.5를 처음부터 훈련하려면 약 6000 A100 GPU 일[35]이 필요하며, 이는 제한된 리소스로 개별 연구자에게 상당한 장벽을 제기하고 혁신을 방해한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      'AIGC 커뮤니티 내에서. 시간이 지남에 따라 AIGC 커뮤니티는 지속적으로 업데이트되는 고품질 데이터 세트와 고급 알고리즘에 액세스할 수 있습니다. 중추적 질문은 다음과 같습니다. _어떻게 이러한 새로운 요소를 기존 모델에 효율적으로 통합할 수 있으며 제한된 리소스의 제약 내에서 보다 강력한 버전을 달성할 수 있습니까?_\n' +
      '\n' +
      '이 문제를 해결하기 위해 본 연구에서는 효율적인 T2I 훈련 방법인 PixArt-\\(\\alpha\\)를 개선하는데 초점을 맞추고 있다. PixArt-\\(\\alpha\\)는 Sora[32]와 Stable Diffusion 3[41]과 같은 작품에서 알 수 있듯이, 상당한 잠재력을 가진 모델 구조인 DiT 프레임워크 내의 초기 벤처를 나타낸다. 이러한 가능성을 극대화하기 위해 PixArt-\\(\\alpha\\)의 사전 훈련된 기초를 기반으로 고급 요소를 통합하여 지속적인 개선을 촉진하여 보다 강력한 모델인 PixArt-\\(\\Sigma\\)을 생성했다. 우리는 효율적인 훈련을 통해 상대적으로 약한 기준선에서 더 강한 모델로 진화하는 이 과정을 "약한 훈련에서 강한 훈련"이라고 한다. 구체적으로 "약대강 훈련"을 달성하기 위해 다음과 같은 개선 사항을 소개한다.\n' +
      '\n' +
      '고품질 훈련 데이터**: 우리는 PixArt-\\(\\alpha\\)에서 사용되는 것보다 우수한 고품질 데이터세트를 수집하며, 두 가지 주요 측면에 초점을 맞춘다: (i) ** 고품질 이미지**: 데이터세트는 인터넷에서 조달한 33M 고해상도 이미지로 구성되며, 모두 4K 전후의 해상도를 가진 2.3M 이미지를 포함하여 1K 해상도를 초과한다. 이러한 이미지는 주로 높은 미학을 특징으로 하며 광범위한 예술적 스타일을 포함한다. (ii) **Dense and accurate captionions**: 앞서 언급한 이미지에 대한 보다 정확하고 상세한 캡션을 제공하기 위해, PixArt-\\(\\alpha\\)에 사용된 LLaVA[21]을 보다 강력한 이미지 캡셔너인 Share-Captioner[5]로 대체한다. 또한, 텍스트와 시각적 개념 사이의 모델의 정렬 용량을 향상시키기 위해 텍스트 인코더(_i.e._, Flan-T5[9])의 토큰 길이를 약 300 단어로 확장한다. 이러한 개선은 모델의 환각 경향을 효과적으로 제거하여 고품질 텍스트 이미지 정렬로 이어진다.\n' +
      '\n' +
      '**효율적인 토큰 압축**: PixArt-\\(\\alpha\\)을 향상시키기 위해 1K에서 4K로 세대 해상도를 확장한다. 초고해상도(_e.g._, 2K/4K)에서 이미지를 생성하면 토큰 수가 크게 증가하여 계산 수요가 크게 증가한다. 이 문제를 해결하기 위해, 우리는 DiT 프레임워크에 맞춘 키 및 값 토큰 압축을 갖는 자기 주의 모듈을 도입했다. 특히 키와 값의 로컬 집계를 위해 2의 보폭을 갖는 그룹 컨볼루션을 활용한다. 또한, KV 압축 없이 미리 훈련된 모델에서 부드러운 적응을 가능하게 하는 특수 가중치 초기화 기법을 사용한다. 이 설계는 고해상도 이미지 생성을 위해 학습 및 추론 시간을 \\(\\sim\\)34% 효과적으로 줄인다.\n' +
      '\n' +
      '**약한-강한 훈련 전략**: 약한 모델에서 강한 모델로 효율적으로 빠르게 적응하기 위해 몇 가지 미세 조정 기법을 제안한다. 그것은 (1) 보다 강력한 Variational Autoencoder (VAE) [35]로 교체하는 것, (2) 낮은 해상도에서 높은 해상도로 스케일링하는 것, (3) Key-Value (KV) 압축이 없는 모델에서 KV 압축이 있는 모델로 진화하는 것을 포함한다. 이러한 결과는 "약한 훈련에서 강한 훈련" 접근법의 타당성과 유효성을 확인한다.\n' +
      '\n' +
      '제안된 개선점을 통해 PixArt-\\(\\Sigma\\)은 최소한의 훈련 비용과 모델 파라미터로 고품질의 4K 해상도 영상 생성을 달성한다. 구체적으로, 사전 학습된 모델로부터 미세 조정을 통해 PixArt-\\(\\alpha\\)에서 요구하는 GPU 일수의 **9%**만을 추가로 활용하여 강력한 1K 고해상도 영상 생성 모델을 구현하였으며, 이는 새로운 학습 데이터와 보다 강력한 VAE로 대체한 것을 고려할 때 인상적이다. 또한, 우리는 **0.6B** 매개 변수만 사용하는 반면 SDXL[35] 및 SD 캐스케이드[34]는 각각 2.6B 및 5.1B 매개 변수를 사용한다. PixArt-\\(\\Sigma\\)에 의해 생성된 이미지는 DALL-E 3[30] 및 MJV6[28]과 같은 현재의 최상위 T2I 제품에 필적하는 심미적 품질을 갖는다(도 4에 도시된 바와 같다). 또한 PixArt-\\(\\시그마\\)도 텍스트 프롬프트와 함께 미세 정렬에 대한 탁월한 능력을 보여준다(그림 2 및 3에 나와 있음).\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**확산 트랜스포머.** 트랜스포머 아키텍처는 언어 모델링[36, 37], 컴퓨터 비전[22, 42, 50, 52] 및 기타 영역[3, 13]과 같은 다양한 도메인에서 현저한 성공을 거두었다. 확산 모델의 영역에서 DiT[33]와 UViT[2]는 트랜스포머 아키텍처의 사용을 개척한다. DiffiT[14], SiT[27], FiT[25]를 포함한 후속 작업은 DiT의 아키텍처에 따라 개선되었으며, [11, 51]은 마스킹 모델링 기술을 통해 훈련 효율성을 향상시켰다. T2I(Text-to-Image) 합성을 위해 PixArt-\\(\\alpha\\)[4]는 효율적인 T2I 훈련 기법을 탐색하여 최초의 트랜스포머 기반 T2I 모델을 달성한다.\n' +
      '\n' +
      '그림 2: 복잡한 밀집 명령어를 사용한 4K 이미지 생성. PixArt-\\(\\Sigma\\)는 후처리 없이 4K 해상도 영상을 직접 생성할 수 있으며, 주어진 프롬프트에 정확하게 응답할 수 있다.\n' +
      '\n' +
      '1024px 고품질 이미지를 생성합니다. 최근 강력한 비디오 생성 모델 소라[32]의 출현은 확산 트랜스포머의 잠재력을 더욱 강조했다. 본 연구에서는 최초로 트랜스포머 구조를 이용하여 4K 초고해상도 이미지를 직접 생성함으로써, 긴 시퀀스의 토큰이 수반되어 발생하는 계산 복잡도 문제를 해결한다.\n' +
      '\n' +
      '**고해상도 이미지 생성**은 시각적 품질을 크게 향상시키며 영화, 게임 등 다양한 산업에서 중요합니다. 그러나, 증가하는\n' +
      '\n' +
      '그림 3: PixArt-\\(\\Sigma\\)과 오픈소스 모델, 예를 들어 PixArt-\\(\\alpha\\) 및 SDXL**의 **비교: PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\)과 비교하여 초상화의 사실성과 의미분석의 능력을 향상시킨다. SDXL과 비교했을 때, 우리의 방법은 사용자 지시를 따르는 더 나은 능력을 가지고 있다. 키워드는 파란색으로 강조 표시됩니다.\n' +
      '\n' +
      '이미지 해상도는 계산 요구의 실질적인 증가로 인한 도전을 도입한다. 이 방향으로 수많은 방법이 탐구되었다. 예를 들어, Imagen[39], GigaGAN[17] 및 Stable Diffusion[38]은 추가적인 초해상도 네트워크를 도입하는 반면, Stable Cascade[34]는 해상도를 점진적으로 증가시키기 위해 다수의 확산 네트워크를 채용한다. 그러나 이러한 결합 모델 솔루션은 누적 오류를 유발할 수 있다. 한편, SDXL[35], DALL-E2[29], Playground[18], PixArt-\\(\\alpha\\)[4]와 같은 작업은 확산 모델을 직접 사용하여 고해상도 이미지를 생성하려고 시도했다. 그럼에도 불구하고 이러한 노력은 최대 1024px 해상도로 이미지를 생성하는 데 제한한다.\n' +
      '\n' +
      '그림 4: **PixArt-\\(\\Sigma\\) 및 기타 4가지 T2I 제품**: 반딧불이 2, Imagen 2, Dalle 3, Midjourney 6. PixArt-\\(\\Sigma\\)에 의해 생성된 이미지는 이러한 상용 제품과 매우 경쟁력이 있다.\n' +
      '\n' +
      '계산 복잡성으로 인해. 본 논문에서는 이 경계를 4K 해상도로 밀어내어 생성된 콘텐츠의 시각적 품질을 크게 향상시킨다.\n' +
      '\n' +
      '**효율적인 트랜스포머 아키텍처.** 트랜스포머에서의 셀프-어텐션 메커니즘은 토큰의 수에 대한 2차 계산 복잡성을 겪으며, 이는 토큰 양의 스케일 업을 방해한다. (1) Sparse Attention[6, 7, 44, 45, 47]은 토큰의 서브세트를 선택적으로 처리함으로써 전체 계산 부하를 감소시킨다. 예를 들어, PVT v2[45]는 컨볼루션 커널을 채용하여 키 및 값의 공간을 응축시키고, 따라서 주의를 계산하는 것과 관련된 복잡도를 낮춘다. (2) Local Attention[12, 22, 46, 53]은 주변 지역 내에서 주의를 집중시킨다; 특히, Swin Transformer[22]는 윈도우 기반 주의를 활용하여 계산을 지정된 윈도우 크기 이내로 제한한다. (3) Low-rank/Linear Attention[8, 24, 43]. 린포머[43]은 낮은 순위 근사치를 통해 자기 주의 메커니즘의 계산 복잡도를 감소시킨다. 본 논문에서는 PVT v2[45]에서 착안한 4K 영상 처리의 복잡도를 줄이기 위해 키/값 압축에 기반한 자기 주의 메커니즘을 사용한다.\n' +
      '\n' +
      '## 3 Framework\n' +
      '\n' +
      '### Data Analysis\n' +
      '\n' +
      '**더 높은 미적 및 더 높은 해상도.** 데이터 세트의 미적 품질을 향상시키기 위해 내부 데이터를 14M에서 33M으로 확장합니다. 명확성을 위해 두 데이터 세트를 각각 Internal-\\(\\alpha\\) 및 Internal-\\(\\Sigma\\)으로 명명한다. 이 확장은 2B 데이터를 사용하는 SD v1.5와 같이 현재 사용 가능한 오픈 소스 모델이 사용하는 방대한 이미지와 비교할 때 여전히 부족하다. 우리는 제한된 데이터 양을 가진 효과적인 훈련 전략이 여전히 강력한 T2I 모델을 얻을 수 있음을 보여준다.\n' +
      '\n' +
      '내부 영상(\\Sigma\\)은 1K 이상의 해상도를 가진다. 4K 해상도 생성을 용이하게 하기 위해 4K 해상도에서 8M 실제 사진 이미지의 데이터 세트를 추가로 수집한다. 미학적 품질을 보장하기 위해, 우리는 이러한 4K 이미지들을 필터링하기 위해 미적 스코어링 모델(AES) [1]을 사용한다. 이 프로세스는 2M 초고해상도 및 고품질 이미지의 고도로 정제된 데이터 세트를 생성한다.\n' +
      '\n' +
      '그림 5: **환각 비교 예시: LLaVA와 Share-Captioner 사이의 환각 발생 차이를 대조하며 빨간색은 환각을 나타내고 녹색은 정확성을 나타낸다.**\n' +
      '\n' +
      '흥미롭게도 우리는 이미지의 해상도가 증가함에 따라 모델의 충실도(FID(Frechet Inception Distance)[16])와 시맨틱 정렬(CLIP Score)이 향상되어 고해상도 이미지를 생성하는 능력의 중요성을 강조한다는 것을 관찰했다.\n' +
      '\n' +
      '**Better Text-Image Alignment.**\n' +
      '\n' +
      'PixArt-\\(\\alpha\\)[4]와 DALL-E 3[30]과 같은 최근의 작품들은 텍스트-이미지 기술 정렬의 중요성을 강조한다. 이 정렬을 강화하는 것은 모델 기능을 강화하는 데 중요합니다. 수집된 "원시" 설명을 더 다듬기 위해 캡션의 길이와 정확성을 개선하는 데 중점을 둔다. 특히, 우리의 캡션(Internal-\\(\\Sigma\\))은 다음 측면에서 PixArt-\\(\\alpha\\)(Internal-\\(\\alpha\\))에 사용된 캡션보다 몇 가지 장점을 보여준다.\n' +
      '\n' +
      '1. 향상된 캡션 정확도: 도 1에 도시된 바와 같다. 5, PixArt-\\(\\alpha\\)에 사용된 LLaVa는 어떤 환각 문제를 가지고 있다. 우리는 보다 강력한 시각적 언어 모델, 즉 Share-Captioner[5]를 활용하여 상세하고 올바른 캡션을 생성하여 수집된 원시 프롬프트를 증가시킨다.\n' +
      '\n' +
      '2. 캡션 길이 증가 : 탭에 도시된 바와 같다. 도 1 및 도 1을 참조하여 설명한다. 도 6을 참조하면, 평균 자막 길이는 180 단어로 크게 증가하여 자막의 설명력이 매우 향상되었다. 또한, 텍스트 인코더의 토큰 처리 길이를 Internal-\\(\\alpha\\)에서와 같이 120개의 토큰에서 300개의 토큰으로 확장한다. 모델은 각각 60%와 40%의 비율로 긴(Share-Captioner) 캡션과 짧은(raw) 캡션을 혼합하여 학습한다. 이 접근법은 텍스트 설명의 다양성을 향상시키고 생성 캡션에만 의존하여 발생할 수 있는 잠재적인 편향을 완화한다.\n' +
      '\n' +
      '탭 1은 내부-\\(\\alpha\\)과 -\\(\\Sigma\\)에 대한 요약을 보여주며, 여기서 우리는 명사 다양성, 총 명사 수, 평균 자막 길이, 이미지당 평균 명사 등 다양한 메트릭을 통해 데이터 세트의 다양성을 평가한다.\n' +
      '\n' +
      '**High-Quality Evaluation Dataset.** 대부분의 SoTA T2I 모델은 FID 및 CLIP 점수를 평가하기 위한 평가 세트로 MSCOCO[20]를 선택했다. 그러나 MSCOCO 데이터셋에 대한 평가는 미학과 텍스트 이미지 정렬에서 모델의 능력을 적절하게 반영하지 못할 수 있음을 관찰한다. 따라서 평가를 용이하게 하기 위해 30,000개의 고품질, 심미적으로 만족스러운 텍스트 이미지 쌍으로 구성된 큐레이션 세트를 제안한다. 데이터 세트의 선택된 샘플은 부록에 나와 있다. 이 데이터 세트는 특히 미적 매력의 복잡성과 텍스트 설명 간의 정렬의 충실도를 포착하는 데 있어 모델의 성능에 대한 보다 포괄적인 평가를 제공하도록 설계되었다.\n' +
      '\n' +
      '도 6: 캡션 길이의 히스토그램 시각화. 원자막, Internal-\\(\\alpha\\), Internal-\\(\\Sigma\\)에서 1M 자막을 무작위로 선택하여 해당 히스토그램을 그린다. _ ACL_는 평균 캡션 길이를 나타낸다.\n' +
      '\n' +
      '시각 콘텐츠. 달리 명시되지 않는 한, 논문의 평가 실험은 수집된 고품질 평가 데이터 세트에서 수행된다.\n' +
      '\n' +
      '##### 효율적인 DiT 설계\n' +
      '\n' +
      '초고해상도로 영상을 생성할 때 연산량이 크게 증가하기 때문에 효율적인 DiT 네트워크는 필수적이다. 주의 메커니즘은 디퓨전 트랜스포머의 효과에 중추적인 역할을 하지만 2차 계산 요구는 특히 더 높은 해상도인 _g._, 2K 및 4K에서 모델 확장성을 크게 제한한다. PVT v2[45]에서 영감을 얻은 우리는 계산 문제를 해결하기 위해 원래 PixArt-\\(\\alpha\\) 프레임워크에 KV 압축을 통합한다. 이 설계는 전체 매개변수에 대해 0.018%에 불과하지만 토큰 압축을 통해 계산 비용을 효율적으로 절감하면서도 공간 정보와 의미 정보를 모두 보존한다.\n' +
      '\n' +
      '**키-값(KV) 토큰 압축.** 우리의 동기는 키-값(KV) 토큰 압축을 미리 훈련된 PixArt-\\(\\alpha\\)에 직접 적용하는 것이 여전히 합리적인 이미지를 생성할 수 있다는 흥미로운 관찰에서 비롯된다. 이것은 그 기능들의 중복성을 암시한다. 인접한 \\(R\\times R\\) 패치 내의 높은 유사성을 고려하여, 윈도우 내의 특징 의미들은 중복적이고 합리적으로 압축될 수 있다고 가정한다. 우리는 그림 7과 같이 압축 연산자를 통해 \\(R\\times R\\) 창 내에서 토큰 특징을 압축하기 위해 \\(f_{c}(\\cdot)\\으로 표시된 KV 토큰 압축을 제안한다.\n' +
      '\n' +
      '또한, 자기 주의 계산에서 KV 압축으로 인한 잠재적인 정보 손실을 줄이기 위해 쿼리의 모든 토큰(Q)을 유지한다. 이 전략을 통해 중요한 정보를 잃을 위험을 완화하면서 KV 압축을 효과적으로 활용할 수 있다. KV 압축을 이용하여 주의력 계산의 효율성을 높이고 계산 복잡도를 \\(O(N^{2})\\)에서 \\(O\\left(\\frac{N^{2}}{R^{2}}\\right)\\)으로 줄임으로써 고해상도 영상을 직접 생성하는 계산 비용을 관리할 수 있게 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Volume** & **Caption** & **VN/DN** & **Total** & **Noun** & **ACL** & **Average** \\\\ \\hline Internal-\\(\\alpha\\) & 14M & Raw & 187K/931K & 175M & 25 & 11.7/Img \\\\ Internal-\\(\\alpha\\) & 14M & LLaVA & 28K/215K & 536M & 98 & 29.3/Img \\\\ Internal-\\(\\alpha\\) & 14M & Share-Captioner & 51K/420K & 815M & 184 & 54.4/Img \\\\ \\hline Internal-\\(\\Sigma\\) & 33M & Raw & 294K/1512K & 485M & 35 & 14.4/Img \\\\ Internal-\\(\\Sigma\\) & 33M & Share-Captioner & 77K/714K & 1804M & 180 & 53.6/Img \\\\\n' +
      '4K-\\(\\Sigma\\) & 2.3M & Share-Captioner & 24K/96K & 115M & 163 & 49.5/Img \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 데이터 세트에 대한 명사 개념의 **통계.****VN**: 유효한 별개의 명사(10회 이상 출현); **DN**: 총 별개의 명사; **Average**: 이미지당 평균 명사 수; **ACL**: 평균 캡션 길이.\n' +
      '\n' +
      '도 7: **KV 토큰 압축의 설계.** 계산 복잡도를 줄이기 위해 공간 공간에서 KV 토큰을 병합한다.\n' +
      '\n' +
      '\\[\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{Q\\cdot f_{c}(K)^{T}}{\\sqrt{d_{k}}} \\right)f_{c}(V)\\tag{1}\\w]\n' +
      '\n' +
      '본 논문에서는 Convolution 연산자 "Conv2\\(\\times\\)2"를 이용하여 특정 초기화를 통해 심층 레이어를 압축한다. 다른 설계 변형들에 대한 상세한 실험은 Sec. 5에서 논의된다. 구체적으로, 그룹 컨벌루션을 활용하고 평균 연산자와 동등한 가중치 \\(w=\\frac{1}{R^{2}}\\)를 초기화하는 특화된 컨벌루션 커널 초기화 "Conv Avg Init"를 설계한다. 이 초기화 전략은 초기에 거친 결과를 생성하여 미세 조정 프로세스를 가속화하는 동시에 0.018%의 추가 매개변수만 도입할 수 있다.\n' +
      '\n' +
      '#약한-강한 훈련 전략\n' +
      '\n' +
      '우리는 "약한" 모델에서 "강한" 모델로 전환을 향상시키기 위한 몇 가지 효율적인 훈련 전략을 제안한다. 이러한 전략은 VAE의 신속한 적응, 고해상도 미세 조정 및 KV 토큰 압축을 포함한다.\n' +
      '\n' +
      '**새로운 VAE에 모델을 적응.** VAE가 계속 발전함에 따라 T2I 모델을 처음부터 교육하는 것은 자원 집약적이다. 우리는 PixArt-\\(\\alpha\\)의 VAE를 SDXL의 VAE로 대체하고 확산 모델을 미세 조정한다. 그림 8(a)와 같이 2K 훈련 단계에서 미세 조정이 빠르게 수렴하는 급격한 수렴 현상을 관찰한다. VAE 모델을 이전할 때 미세 조정이 더 효율적이며 처음부터 교육의 필요성을 부정한다.\n' +
      '\n' +
      '**High-Resolution에 적응.** 저해상도(LR) 모델에서 고해상도(HR) 모델로 미세 조정하면 성능 디그라를 관찰한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Resolution & Iterations & FID \\(\\downarrow\\) & CLIP \\(\\uparrow\\) \\\\ \\hline\n' +
      '256&20K&16.56&0.270\\\\\n' +
      '256 \\(\\rightarrow\\)512 & 1K & 9.75 & 0.272 \\\\\\\n' +
      '256 \\(\\rightarrow\\) 512 & 100K & 8.91 & 0.276 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 저해상도 모델에서 고해상도 모델을 미세 조정하고 1K 단계와 같이 비교적 짧은 기간 동안 미세 조정하더라도 여전히 고품질 결과를 얻을 수 있음을 관찰한다.\n' +
      '\n' +
      '그림 8: 이 그림은 우리의 훈련 전략 설계가 VAE로의 전환, 더 높은 해상도로의 조정 및 KV 압축 프로세스 동안 모델의 수렴을 가속화하여 약한 것에서 강한 것으로의 신속한 학습을 촉진하는 방법을 보여준다.\n' +
      '\n' +
      '도 1에 도시된 바와 같은 디네이션. 도 8의 (b)를 참조하면, 서로 다른 해상도 사이의 위치 임베딩(PE)의 불일치에 기인한다. 이 문제를 완화하기 위해 LR 모델의 PE를 보간하여 HR 모델의 PE를 초기화하고 HR 모델의 초기 상태를 크게 향상시키며 미세 조정 프로세스를 신속하게 하는 "PE 보간" 트릭[4, 48]을 사용한다. 우리는 오직 100번의 훈련 반복 내에서도 시각적으로 즐거운 이미지를 얻을 수 있다. 또한, Tab. 2에 나타낸 바와 같이 모델의 성능 변화를 정량적으로 평가하였으며, 1K 단계에서 미세 조정이 빠르게 수렴하고, 추가적인 훈련이 성능을 약간 향상시켰다. 이것은 "PE 보간" 트릭을 사용하는 것이 더 높은 해상도 생성의 신속한 수렴을 가능하게 하여, 더 높은 해상도에서의 생성을 위해 처음부터 트레이닝의 필요성을 배제한다는 것을 예시한다.\n' +
      '\n' +
      '**KV 압축에 모델을 적응.**KV 압축 없이 LR 사전 훈련된 모델에서 미세 조정할 때 KV 압축을 직접 사용할 수 있다. 도 1에 도시된 바와 같다. 도 8의 (c)를 참조하면, "Conv Avg Init." 전략으로, PixArt-\\(\\Sigma\\)는 더 나은 초기 상태에서 시작하여 수렴을 더 쉽고 빠르게 만든다. 특히, PixArt-\\(\\Sigma\\)은 100개의 훈련 단계에서도 만족스러운 시각적 결과를 보여준다. 마지막으로 Sec 3.2의 KV 압축 연산자와 압축 계층 설계를 통해 학습 및 추론 시간의 34%를 줄일 수 있다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '본 논문에서는 T5[9]의 인코더(_i.e._, Flan-T5-XXL)를 조건부 특징 추출을 위한 텍스트 인코더로 사용하고, PixArt-\\(\\alpha\\)[4]를 베이스 확산 모델로 사용한다. 고정된 77개의 텍스트 토큰을 추출하는 대부분의 작업과 달리, 내부-\\(\\Sigma\\)에서 큐레이션된 캡션이 훨씬 조밀하기 때문에 PixArt-\\(\\alpha\\)의 120에서 300까지의 텍스트 토큰의 길이를 조정한다. 입력 이미지의 잠재 특징을 캡처하기 위해 SDXL[35]에서 미리 훈련되고 동결된 VAE를 사용한다. 다른 구현 세부 사항은 PixArt-\\(\\alpha\\)와 동일하다. 모델은 위치 임베딩 보간 트릭[4]을 사용하여 PixArt-\\(\\alpha\\)의 256px 사전 훈련된 체크포인트에서 미세 조정된다. 1K 해상도를 포함한 최종 모델은 32개의 V100 GPU에서 훈련됩니다. 추가로 16개의 A100 GPU를 사용하여 2K 및 4K 이미지 생성 모델을 훈련한다. 자세한 내용은 부록을 참조하시기 바랍니다.\n' +
      '\n' +
      '우리는 일반적인 AdamW[23] 최적화기 대신에 가중치 감쇠가 0이고 일정한 학습 속도가 2e-5인 CAME 최적화기[26]를 사용한다는 점에 유의한다. 이를 통해 최적화기의 상태 차원을 줄일 수 있어 성능 저하 없이 GPU 메모리를 낮출 수 있다.\n' +
      '\n' +
      '**평가 메트릭.** 미학과 의미 능력을 더 잘 설명하기 위해 가장 강력한 T2I 모델을 벤치마킹하기 위해 30K 고품질 텍스트 이미지 쌍(Sec. 3.1에서 언급됨)을 수집한다. PixArt-\\(\\Sigma\\)은 FID[38]의 메트릭이 생성 품질을 적절하게 반영하지 못할 수 있기 때문에 인간 및 AI 선호도 연구를 통해 주로 평가한다. 그러나 우리는 여전히 부록에서 수집된 데이터 세트에 대한 FID 결과를 제공한다.\n' +
      '\n' +
      '### Performance Comparisons\n' +
      '\n' +
      '이미지 품질 평가.우리는 T2I(closed-source text-to-image) 제품과 오픈소스 모델 모두에 대한 방법론을 정성적으로 평가했다. 도 1에 도시된 바와 같다. 1, 본 모델은 다양한 종횡비와 스타일에 걸쳐 복잡한 디테일을 가진 고품질의 사진 사실적 이미지를 생성할 수 있다. 이 기능은 텍스트 설명으로부터 시각적으로 설득력 있는 콘텐츠를 생성하는 데 있어 접근법의 우수한 성능을 강조합니다. 도 1에 도시된 바와 같다. 셋째, PixArt-\\(\\Sigma\\)을 오픈소스 모델인 SDXL[35]과 PixArt-\\(\\alpha\\)[4]와 비교하여, 본 논문에서 제안하는 방법은 초상화의 사실성을 높이고 의미론적 분석 능력을 향상시킨다. SDXL과 달리 우리의 접근법은 사용자 지침을 준수하는 데 탁월한 숙련도를 보여준다.\n' +
      '\n' +
      '오픈 소스 모델보다 우수할 뿐만 아니라 우리의 방법은 그림 4와 같이 현재 T2I 폐쇄 소스 제품과 매우 경쟁력이 있다. PixArt-\\(\\Sigma\\)는 사진 사실적인 결과를 생성하고 현대 상용 제품과 동등한 사용자 지침을 준수한다.\n' +
      '\n' +
      '고해상도 생성 방법은 후처리 없이 4K 해상도 영상을 직접 생성할 수 있다. 또한 그림 2에서 설명한 대로 사용자가 제공하는 복잡하고 상세하며 긴 텍스트를 정확하게 따르는 데 탁월합니다. 따라서 사용자는 만족스러운 결과를 얻기 위해 신속한 엔지니어링이 필요하지 않습니다.\n' +
      '\n' +
      '우리의 접근 방식은 직접 4K 이미지 생성을 가능하게 한다. 동시에, 연구 [10, 15]는 LR 모델로부터 HR 이미지를 생성하거나 HR 이미지를 생성하기 위해 초해상도 모델[49]을 사용하는 것을 목표로 하는 튜닝 없는 후처리 기술을 도입했다. 그러나, 그들의 대응하는 결과는 종종 두 가지 주요 이유들, 즉 (1) 캐스케이드 파이프라인으로 인해 누적 에러가 발생할 수 있다. (2) 이러한 방법들은 4K 이미지들의 진정한 분포를 캡처하거나 텍스트와 4K 이미지들 사이의 정렬을 학습하지 않는다. 우리는 우리의 방법이 고해상도 이미지를 생성하는 더 유망한 방법일 수 있다고 주장한다. 우리의 방법은 우수한 결과를 얻으며, 더 많은 시각적 비교가 보충제에 포함된다.\n' +
      '\n' +
      '인간/AI(GPT4V) 선호도 연구.3.1 Sec.에 언급된 고품질 평가 데이터세트로부터 무작위로 수집된 300개의 캡션의 하위 집합을 사용하여 인간 및 AI 선호도 연구에서 잘 훈련된 모델을 평가한다. PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\), SD1.5[38], Stable Turbo[40], Stable XL[35], Stable Cascade[34], Playground-V2.0[19] 등 총 6개의 오픈 소스 모델에 의해 생성된 이미지를 수집한다. 본 연구에서는 인간의 기호 연구를 위한 웹 사이트를 개발하여 프롬프트와 해당 이미지를 표시한다.\n' +
      '\n' +
      '그림 9: currrent open T2I 모델에 대한 인간(파란색)/AI(주황색 및 녹색) 선호도 평가. PixArt-\\(\\Sigma\\)는 기존의 최신 T2I 모델들과 화질 및 프롬프트 팔로우 모두에서 잘 비교된다.\n' +
      '\n' +
      '이 웹사이트는 훈련된 평가자에게 배포되었으며, 평가자는 이미지를 평가하여 품질에 따라 순위를 매기고 텍스트 프롬프트와 얼마나 잘 일치하는지 확인했다. 결과는 그림 1의 파란색 막대로 표시된다. 도 9는 다른 6개의 T2I 발전기에 비해 PixArt-\\(\\Sigma\\)에 대한 현저한 선호도를 나타낸다. PixArt-\\(\\Sigma\\)는 SDXL(2.6B 파라미터) 및 SD 캐스케이드(5.1B 파라미터)와 같은 기존의 T2I 확산 모델에 비해 훨씬 작은 크기(0.6B 파라미터)를 사용하여 사용자 프롬프트를 밀접하게 따르는 우수한 고품질 이미지를 생성한다.\n' +
      '\n' +
      '또한, 인공지능 선호도 연구에서는 고급 멀티모달 모델인 GPT-4 Vision [31]을 평가자로 사용한다. 각 시험마다 GPT-4 Vision에 PixArt-\\(\\Sigma\\)의 영상과 경쟁 T2I 모델의 영상을 제공한다. 우리는 이미지 품질과 이미지 및 텍스트 정렬을 기반으로 GPT-4 비전이 투표하도록 안내하는 뚜렷한 프롬프트를 만듭니다. 그림 1의 주황색 및 녹색 막대로 표시된 결과이다. 9는 인간 및 AI 선호 연구 모두에서 일관된 결과를 보여준다. 구체적으로, PixArt-\\(\\Sigma\\)이 기준선인 PixArt-\\(\\alpha\\)을 능가하는 효과를 보인다. PixArt-\\(\\Sigma\\)는 Stable Cascaded와 같은 현대의 고급 모델들에 비해 화질 및 명령어 추종 능력 면에서 경쟁적이거나 우수한 성능을 보인다.\n' +
      '\n' +
      '## 5 Ablation Studies\n' +
      '\n' +
      '다양한 KV 압축 설계에 대한 생성 성능에 대한 절제 연구를 수행한다. 명시되지 않는 한, 실험은 512px 생성에 대해 수행된다. 각 절제 실험의 세부 설정은 부록에 포함되어 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Layers & FID \\(\\downarrow\\) CLIP-Score \\(\\uparrow\\) \\\\ \\hline N/A & 8.244 & 0.276 \\\\ Shallow (1-14) & 9.278 & 0.275 \\\\ Middle (7-20) & 9.063 & 0.276 \\\\ Deep (14-27) & 8.532 & 0.275 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 이미지 생성에서의 KV-토큰 압축 설정. 본 연구는 압축률, 위치, 연산자 및 다양한 해상도와 같은 다양한 토큰 압축 구성요소의 영향을 평가하기 위해 FID, CMMD 및 CLIP-Score 메트릭을 사용한다. 탭에서 속도 계산 2(c)는 Second/Iteration/384 Batch-size이다.\n' +
      '\n' +
      '### Experimental settings\n' +
      '\n' +
      '우리는 평가를 위해 Sec. 3.1에 설명된 테스트 세트를 사용한다. 비교 메트릭을 위해 수집된 데이터와 생성된 데이터 간의 분포 차이를 계산하기 위해 FID를 사용한다. 또한 CLIP-Score를 사용하여 프롬프트와 생성된 이미지 간의 정렬을 평가한다.\n' +
      '\n' +
      '### Compression Designs\n' +
      '\n' +
      '압축 위치.** 변압기 구조 내에서 얕은 층(1\\(\\sim\\)14), 중간 층(7\\(\\sim\\)20) 및 깊은 층(14\\(\\sim\\)27)의 서로 다른 깊이에서 KV 압축을 구현했다. 탭에 표시된 대로입니다. 도 3의 (a)에 도시된 바와 같이, 깊은 층에 KV 압축을 채용하는 것은 현저하게 우수한 성능을 달성한다. 우리는 얕은 레이어가 일반적으로 상세한 텍스처 콘텐츠를 인코딩하는 반면 깊은 레이어는 높은 수준의 의미 콘텐츠를 추상화하기 때문이라고 추측한다. 압축은 의미적 정보보다는 이미지 품질에 영향을 미치는 경향이 있기 때문에, 딥 레이어 압축은 정보의 손실을 가장 적게 달성할 수 있어, 트레이닝을 가속화하지만 생성 품질을 손상시키지 않는 실용적인 선택이 될 수 있다.\n' +
      '\n' +
      '**압축 연산자.** 다양한 압축 연산자의 영향을 조사했다. 2\\(\\times\\)2개의 토큰을 하나의 토큰으로 압축하기 위해 랜덤 폐기, 평균 풀링, 파라메트릭 컨벌루션의 세 가지 기법을 사용하였다. 표 3(b)에 예시된 바와 같이, "Conv 2\\(\\times\\)2" 방법은 다른 방법들보다 우수하므로, 간단한 폐기 방법보다 중복 특징을 더 효과적으로 감소시키기 위해 학습 가능한 커널을 사용하는 이점을 강조한다.\n' +
      '\n' +
      '**다양한 해상도에 대한 압축 비율.** 다양한 해상도에 대한 다양한 압축 비율의 영향을 조사했다. 탭에 표시된 대로입니다. 셋째, 토큰 압축은 텍스트 이미지와 생성된 이미지 사이의 정렬에 영향을 미치지 않지만 해상도에 따라 이미지 품질(FID)에 영향을 미친다는 것을 알 수 있다. 압축률이 증가함에 따라 화질에 약간의 저하가 있지만, 우리의 전략은 18%에서 35%의 학습 속도를 제공한다. 이것은 제안된 KV 압축이 고해상도 T2I 생성을 달성하는 데 효과적이고 효율적임을 시사한다.\n' +
      '\n' +
      '**다양한 해상도에 대한 속도 비교.** 탭의 훈련 및 추론 모두에서 속도 가속도를 추가로 종합적으로 검증합니다. 3(d). 제안하는 방법은 4K세대에서 약 35%의 학습과 추론을 빠르게 할 수 있다. 특히, 우리는 해상도가 증가함에 따라 훈련 가속도가 증가하는 것을 관찰한다. 예를 들어, 훈련은 해상도가 1K에서 4K로 증가함에 따라 18%에서 35%로 점진적으로 가속된다. 이것은 고해상도 이미지 생성 작업에 대한 잠재적 적용 가능성을 입증하는 해상도 증가에 대한 우리의 방법의 효율성을 나타낸다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 4K 해상도에서 고품질의 영상을 직접 생성할 수 있는 T2I(Text-to-Image) 확산 모델인 PixArt-\\(\\Sigma\\)을 소개한다. PixArt-\\(\\alpha\\), PixArt-\\(\\Sigma\\)의 사전 훈련된 기반을 바탕으로 새로운 "약자-강자 훈련" 방법론을 통해 효율적인 훈련을 달성한다. 이 접근법은 고품질 데이터의 통합과 효율적인 토큰 압축의 통합을 특징으로 한다. PixArt-\\(\\Sigma\\)는 기존의 PixArt-\\(\\alpha\\)을 능가하면서 텍스트적 프롬프트에 밀착하면서 고충실도 이미지를 생산하는 데 탁월하다. 우리는 PixArt-\\(\\Sigma\\)에 제시된 혁신이 AIGC 커뮤니티의 발전에 기여할 뿐만 아니라 기업이 보다 효율적이고 고품질의 생성 모델에 접근할 수 있는 길을 열어줄 것이라고 믿는다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '그림 10: PixArt-\\(\\Sigma\\)와 다른 4가지 T2I 제품인 반딧불 2, Imagen 2, Dalle 3, Midjourney 6을 비교. PixArt-\\(\\Sigma\\)에 의해 생성된 이미지는 이러한 상용 제품과 매우 경쟁력이 있다.\n' +
      '\n' +
      '그림 11: **PixArr-\\(\\Sigma\\) 및 기타 4개의 T2I 제품**: 반딧불이 2, Imagen 2, Dalle 3, Midjourney 6. PixArt-\\(\\Sigma\\)에 의해 생성된 이미지는 이러한 상용 제품과 매우 경쟁력이 있다.\n' +
      '\n' +
      '그림 12: PixArt-\\(\\Sigma\\.** PixArt-\\(\\Sigma\\)에 의해 생성된 고품질 이미지의 **그림들.** PixArt-\\(\\Sigma\\)은 세밀한 디테일과 다양한 종횡비를 갖는 고품질 이미지를 생성할 수 있다.\n' +
      '\n' +
      '도 13: PixArt-\\(\\Sigma\\.** PixArt-\\(\\Sigma\\)에 의해 생성된 **고해상도(4K) 이미지들.** PixArt-\\(\\Sigma\\)는 미세한 디테일을 보존하면서 고품질의 4K HD(3840\\(\\times\\)2560) 이미지들을 직접 생성할 수 있다.\n' +
      '\n' +
      '도 14: PixArt-\\(\\Sigma\\)에 의해 생성된 고해상도(4K) 이미지. PixArt-\\(\\Sigma\\)는 미세한 디테일을 유지하면서 고품질의 4K HD(3840\\(\\times\\)2560) 영상을 직접 생성할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]Aesthetic predictor (2023), [https://github.com/christophschuhmann/improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor)[2] Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., Zhu, J.: All is worth words: A vit backbone for diffusion model. In: CVPR(2023) [3] Chandra, A., Tunnermann, L., Lofstedt, T., Gratz, R.: Transformer-based deep learning for predicting protein properties in the life sciences. eLife Sciences Publications, Ltd(2023) [4] Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., Li, Z.: PixArt-\\(\\alpha\\): photorealistic text-to-image 합성을 위한 확산 변압기의 빠른 훈련. In: ICLR(2024) [5] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: ShareGPT4V: 더 나은 캡션을 갖는 대형 멀티모달 모델 개선. In: arXiv(2023) [6] Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z.: Chasing sparsity in vision transformer: a end-to-end exploration. In: NeurIPS(2021) [7] Chen, X., Liu, Z., Tang, H., Yi, L., Zhao, H., Han, S.: SparseViT: Revisiting activation sparsity for efficient high-resolution vision transformer. In: CVPR(2023) [8] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al.: Rethinking attention with performers. In: ICLR(2021) [9] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. In: arXiv(2022) [10] Du, R., Chang, D., Hospedales, T., Song, Y.Z., Ma, Z.: Demofusion: Democraticatising high-resolution image generation without SSSS. In: CVPR(2024) [11] Gao, S., Zhou, P., Cheng, M.M., Yan, S.: Masked diffusion transformer는 강한 영상 합성기이다. In: ICCV(2023) [12] Ge, C., Ding, X., Tong, Z., Yuan, L., Wang, J., Song, Y., Luo, P.: group-mix attention을 갖는 비전 트랜스포머 전진. In: arXiv(2023) [13] von Glehn, I., Spencer, J.S., Pfau, D.: A self-attention ansatz for ab-initio quantum chemistry. In: ICLR(2023) [14] Hatamizadeh, A., Song, J., Liu, G., Kautz, J., Vahdat, A.: Diffit: Diffusion vision transformer for image generation. In:ArXiv(2023) [15] He, Y., Yang, S., Chen, H., Cun, X., Xia, M., Zhang, Y., Wang, X., He, R., Chen, Q., Shan, Y.: ScaleCrafter: Tuning-free high-resolution visual generation with diffusion models. In: ICLR(2024) [16] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: 2개의 시간-스케일 업데이트 규칙에 의해 트레이닝된 GAN들은 로컬 내쉬 평형에 수렴한다. In: NeurIPS(2017) [17] Kang, M., Zhu, J. Y., Zhang, R., Park, J., Shechtman, E., Paris, S., Park, T.: text-to-image synthesis를 위한 gans scaling up. In: CVPR(2023) [18] Li, D., Kamko, A., Akhgari, E., Sabet, A., Xu, L., Doshi, S.: Playground v2.5: Text-to-image generation에서 심미적 품질을 향상시키는 세 가지 통찰력. In: arXiv(2024) [19] Li, D., Kamko, A., Sabet, A., Akhgari, E., Xu, L., Doshi, S.: Playground v2, [https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic12](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic12)*[20] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV(2014) 8\n' +
      '* [21] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: arXiv (2023) 3\n' +
      '* [22] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 4, 7\n' +
      '* [23] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: arXiv (2017) 11\n' +
      '* [24] Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., Zhang, L.: Soft: Softmax-free transformer with linear complexity. In: NeurIPS (2021) 7\n' +
      '* [25] Lu, Z., Wang, Z., Huang, D., Wu, C., Liu, X., Ouyang, W., Bai, L.: Fit: Flexible vision transformer for diffusion model. In: arXiv (2024) 4\n' +
      '* [26] Luo, Y., Ren, X., Zheng, Z., Jiang, Z., Jiang, X., You, Y.: CAME: Confidence-guided adaptive memory efficient optimization. In: ACL (2023) 11\n' +
      '* [27] Ma, N., Goldstein, M., Albergo, M.S., Boffi, N.M., Vanden-Eijnden, E., Xie, S.: SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In: arXiv (2024) 4\n' +
      '* [28] Midjourney: Midjourney (2023), [https://www.midjourney.com](https://www.midjourney.com) 1, 4\n' +
      '* [29] OpenAI: Dalle-2 (2023), [https://openai.com/dall-e-2](https://openai.com/dall-e-2) 6\n' +
      '* [30] OpenAI: Dalle-3 (2023), [https://openai.com/dall-e-3](https://openai.com/dall-e-3) 1, 4, 8\n' +
      '* [31] OpenAI: Gpt-4v(ision) system card. In: OpenAI (2023) 13\n' +
      '* [32] OpenAI: Sora (2024), [https://openai.com/sora](https://openai.com/sora) 3, 5\n' +
      '* [33] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: ICCV (2023) 4\n' +
      '* [34] Pernias, P., Rampas, D., Richter, M.L., Pal, C., Aubreville, M.: Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. In: ICLR (2023) 4, 6, 12\n' +
      '* [35] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., Rombach, R.: SDXL: Improving latent diffusion models for high-resolution image synthesis. In: arXiv (2023) 1, 3, 4, 6, 11, 12\n' +
      '* [36] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training. OpenAI blog (2018) 4\n' +
      '* [37] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog (2019) 4\n' +
      '* [38] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: CVPR (2022) 6, 11, 12\n' +
      '* [39] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. In: NeurIPS (2022) 6, 11\n' +
      '* [40] Sauer, A., Lorenz, D., Blattmann, A., Rombach, R.: Adversarial diffusion distillation. In: arXiv (2023) 12\n' +
      '* [41] Stability.AI: Stable diffusion 3 (2024), [https://stability.ai/news/stable-diffusion-3](https://stability.ai/news/stable-diffusion-3) 3\n' +
      '* [42] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers & distillation through attention. In: ICML (2021) 4\n' +
      '* [43] Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity. In: arXiv (2020)* [44] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)\n' +
      '* [45] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)\n' +
      '* [46] Xia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: CVPR (2022)\n' +
      '* [47] Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: SegFormer: Simple and efficient design for semantic segmentation with transformers. In: NeurIPS (2021)\n' +
      '* [48] Xie, E., Yao, L., Shi, H., Liu, Z., Zhou, D., Liu, Z., Li, J., Li, Z.: DiffFit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. In: ICCV (2023)\n' +
      '* [49] Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-image generation via large mixture of diffusion paths. In: NeurIPS (2023)\n' +
      '* [50] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token ViT: Training vision transformers from scratch on imagenet. In: ICCV (2021)\n' +
      '* [51] Zheng, H., Nie, W., Vahdat, A., Anandkumar, A.: Fast training of diffusion models with masked transformers. In: arXiv (2023)\n' +
      '* [52] Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T., Torr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In: CVPR (2021)\n' +
      '* [53] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: Deformable transformers for end-to-end object detection. ICLR (2021)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>