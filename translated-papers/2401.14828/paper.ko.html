<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '액세스 3D 편집자 뿐만 아니라 이미지-프로그래프트.\n' +
      '\n' +
      '진유 자낭\\({}^{1,2}\\) {-페미 카오\\({}^{2}\\)  {}^{1}\\  {.\n' +
      '\n' +
      '아이템랩({}^{1}\\,{}^{2}\\) 10%의 AI랩.\n' +
      '\n' +
      '디앙@outlook.com, 디오야니@gmail.com, caoyanpei@gmail.com, caoyani@gmail.com,zhuangjy6@mail2.sysn.\n' +
      '\n' +
      'Sysu.edu.cn, linliang@ieee.org, yingsshan@tencent.com올리고안빈@mail.\n' +
      '\n' +
      '응답자. 이 논문은 검토 중입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_텍스트 구동 3D 장면 편집은 편의성과 사용자 친화성으로 인해 상당한 관심을 받았다. 그러나 기존의 방법은 여전히 텍스트 설명의 고유한 한계로 인해 지정된 외관 및 편집 결과의 위치에 대한 정확한 제어가 부족하다. 이를 위해 편집 영역을 특정하기 위해 텍스트와 이미지 프롬프트와 3D바운딩 박스를 모두 수용하는 3D 장면 편집 프레임워크인 TIP-Editor를 제안한다. 이미지 프롬프트로 사용자는 텍스트 설명에 보완하여 타겟 콘텐츠의 상세 외관/식 등을 편리하게 특정할 수 있어 외모에 대한 정확한 제어를 가능하게 한다. 구체적으로, TIP-Editor는 바운딩 박스에 의해 특정된 올바른 객체 배치를 장려하기 위해 국소화 손실이 제안된 기존 장면과 참조 이미지의 표현을 더 잘 학습하기 위해 단계적 2D 개인화 전략을 사용한다. 또한 TIP-Editor는 배경의 변화를 유지하면서 지역 편집을 용이하게 하기 위해 명시적이고 유연한 3D 가우시안 스플링을 3D 표현으로 활용한다. 광범위한 실험은 TIP-Editor가 지정된 바운딩 박스 영역에서 텍스트 및 이미지 프롬프트에 따라 정확한 편집을 수행하고, 편집 품질의 기저부를 일관되게 능가하고, 프롬프트, 질적으로 및 정량적으로 정렬한다는 것을 입증했다. 웹페이지를 참고하세요.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '유례 없는 광학적 렌더링 품질로 인해 영상의학 분야 관련 표현(예: NeRF[31]과 3D 가우시안 스플라팅[20]을 사용하는 방법은 3D 재구성 분야[5, 60, 25]와 질감 편집[41, 56], 형상 변형[57, 58], 장면 분해[50] 및 양식화[53] 등 다양한 하류 3D 편집 작업에서 점점 더 인기가 높았다.\n' +
      '\n' +
      '높은 수준의 명령어(예: 텍스트 프롬프트)만 필요로 하는 생성 편집은 _extensive_ 사용자 상호 작용이 필요한 이전 그림과 유사한 편집 접근법[56, 58]에 대한 보완에서 새로운 접근법으로 나타난다. 이러한 방법 중 텍스트 중심 방법[63, 15]은 편의성으로 큰 주목을 받았고, 대규모 텍스트 대 이미지(T2I) 모델의 성공으로 괄목할 만한 발전을 이루었다.\n' +
      '\n' +
      '그러나 텍스트만을 조건으로 사용하는 방법은 텍스트 설명의 고유한 한계로 인해 지정된 위치에서 지정된 모습으로 편집 결과를 정확하게 생성하기 위해 고군분투한다. 예를 들어, 기존 텍스트 구동 방법은 일반적으로 덜 만족스러운 결과를 생성(그림 3)한다. 특수한 심장 모양의 선글라스로 장난감을 입고 싶거나 남성 조커 메이크업이 영화 _더 다크케이야_에 출연하게 된다. 더욱이, 정확한 편집 위치를 텍스트 안내로 특정하기 어렵다(그림 4). 이러한 과제는 주로 생성된 물체의 다양한 출현과 생성된 장면의 다양한 공간 레이아웃에서 비롯된다.\n' +
      '\n' +
      '위의 과제를 극복하기 위해 텍스트 프롬프트와 이미지 프롬프트 모두를 사용하여 사용자가 직관적으로, 편리하고 정확하게 GS 기반 라디에이터 필드를 편집할 수 있는 TIP-에디터를 제시한다. 우리의 프레임워크는 두 가지 중요한 디자인을 통해 그러한 능력을 달성합니다. (1) 첫 번째는 정밀한 외관 제어(기준 이미지)와 위치 제어(3D 바운딩 박스)를 가능하게 하는 새로운 단계적 2D 개인화 전략이다. 구체적으로, 사용자 정의 편집 영역 내부에서 편집이 발생하는 것을 보장하기 위해 로컬화 손실을 포함하는 장면 개인화 단계와 LoRA[18]에 기초하여 참조 이미지에 전용되는 별도의 신규 콘텐츠 개인화 단계를 포함한다. (2) 둘째는 효율적이며 더 중요하게는 지역 편집에 매우 적합하기 때문에 명시적이고 유연한 3D 가우시안 스플라팅[20]을 3D 표현으로 채택하고 있다.\n' +
      '\n' +
      '우리는 사물, 인간 얼굴, 야외 장면을 포함한 다양한 현실 세계 장면에 걸쳐 TIP-에디터에 대한 포괄적인 평가를 수행한다. 우리의 편집 결과(그림 1 및 그림 7) 참조 이미지에 명시된 고유한 특성을 성공적으로 캡처한다. 이는 편집 과정의 제어 가능성을 크게 향상시켜 상당한 실질적인 가치를 제시한다. 질적 비교와 양적 비교 모두에서 TIP-Editor는 기존의 방법과 비교할 때 편집 품질, 시각 충실도 및 사용자 만족에서 지속적으로 우수한 성능을 보여준다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* We는 텍스트 프롬프트뿐만 아니라 참조 이미지에 의해 안내되는 다양한 편집 동작(예: 객체 삽입, 객체 교체, 재텍스트화, 스타일화)을 수행할 수 있는 다재다능한 3D 장면 편집 프레임워크인 TIP-Editor를 제시한다.\n' +
      '* 우리는 장면 개인화 단계에서 국산화 손실을 특징으로 하는 새로운 단계적 2D 개인화 전략과 LoRA 기반의 참조 이미지에 전용된 별도의 신규 콘텐츠 개인화 단계를 제시하여 정확한 위치와 외관 제어를 가능하게 한다.\n' +
      '* 우리는 렌더링 효율로 인해 장면을 나타내기 위해 3D 가우시안 스플라팅을 채택하며, 더 중요하게는 명시적인 포인트 데이터 구조를 채택하며, 이는 정확한 로컬 편집에 매우 적합하다.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '편집된 이미지 생성 및 편집 및 편집\n' +
      '\n' +
      '대규모 페어드 이미지-텍스트 데이터셋에 대해 훈련된 텍스트 대 이미지(T2I) 확산 모델[39, 42, 45]은 복잡한 텍스트 프롬프트와 일치하는 다양하고 고품질의 이미지를 생성할 수 있기 때문에 상당한 관심을 끌었다. 스크래치로부터 직접 이미지를 생성하는 대신, 또 다른 인기 있고 밀접한 관련 작업은 텍스트 프롬프트[1, 3, 10, 17, 19, 30]에 따라 주어진 이미지를 편집하는 것이다.\n' +
      '\n' +
      '다른 인기 있는 작업은 객체/개념 개인화로서, 주어진 이미지 수집에서 정의된 지정된 객체/개념에 대한 이미지를 생성하는 것을 목표로 한다. 텍스트 임베딩 공간에서 특수 텍스트 토큰(들)을 최적화하여 지정된 개념을 나타낸다. 드림보스[44]는 계층별 사전 보존 손실이 정규직화됨에 따라 전체 확산 모델을 미세 조정한다. 일반적으로 드림보드는 더 많은 양의 업데이트된 모델 파라미터(즉, 전체 UNet 모델)를 포함하기 때문에 고품질 이미지를 생성한다. 다만, 앞서 언급한 모든 방법들은 다수의 개인화된 객체들을 동시에 포함하는 이미지들을 생성하는 것을 지원하지 않는다.\n' +
      '\n' +
      '맞춤형 디퓨전[24]은 위의 과제를 확장하여 하나의 영상에서 다수의 개인화된 _개념_을 동시에 생성한다. 각 _개념_에 별도의 특수 텍스트 토큰이 할당되지만 UNet는 모든 _개념s_에 의해 업데이트되어 만족스러운 개인화 결과가 떨어진다. 또한, 2개의 _개념들_(그림 10) 사이의 상호 작용을 특정하는 국소화 메커니즘이 부족하다. 이에 비해 기존의 장면과 새로운 내용을 별도로 학습하여 질 높고 충실한 개인화 결과를 달성하고 순차적 편집 시나리오에 일반화하기 위한 단계적 2D 개인화 전략을 제안한다.\n' +
      '\n' +
      '현장 기반 3D 라이드입니다.\n' +
      '\n' +
      'T2I 확산 모델의 성공은 크게 3D 객체/scene 생성의 발전을 발전시켰다. 한 정액 기여, 드림퓨전[35]은 3D 데이터에 의존하지 않고 미리 훈련된 2D T2I 모델에서 _optimance_ 방사선 분야에서 지식을 증류하는 점수 증류 샘플링(SDS)을 소개한다. 이후의 대부분의 작품은 이러한 최적화 기반 파이프라인을 채택하고 추가 정제 단계(예: 매직3D[27] 및 드림보트3D[38])를 도입하거나 더 적합한 SDS 변이체(예: VSD[55])를 제안하거나 더 강력한 3D 표현 [7, 9, 59]를 사용하여 추가 진행을 수행한다.\n' +
      '\n' +
      '또한, 연구 기관[11, 29, 36, 51]은 최적화 프레임워크 내에서 참조 이미지를 통합하기 위해 노력한다. 이러한 통합은 재건축 손실 적용, 예측 깊이 맵의 고용, 미세 조정 과정의 실행 등 다양한 기술에 의해 촉진된다. 그럼에도 불구하고 이러한 방법은 처음부터 단일 객체를 생성하기 위해 제한되며 기존의 3D 장면을 편집할 수 없다.\n' +
      '\n' +
      '현장 기반 3D 편집을 선택하세요.\n' +
      '\n' +
      '이전 작품[52, 53]은 주로 주어진 3D 장면의 글로벌 스타일 변환에 초점을 맞추고 있으며, 이는 텍스트 프롬프트 또는 참조 이미지를 입력으로 하고 일반적으로 최적화 중에 CLIP 기반 유사성 측정[37]을 레버리지한다. 여러 연구에서 2D 이미지 조작 기술(예: 인서팅) [2, 23, 28]을 활용하여 기존 영상의 업데이트 분야를 업데이트하기 위한 새로운 훈련 이미지를 얻음으로써 일반 장면에 대한 로컬 편집이 가능하다. 일부 작업은 메시 변형을 기저 방사선 분야에 전파하기 위해 3D 모델링 기술(예: 메쉬 변형) [57, 58, 61]을 채택한다. 그러나 이러한 방법은 광범위한 사용자 상호 작용이 필요하다.\n' +
      '\n' +
      '최근 텍스트 중심의 라디에이터 필드 편집 방법은 편집 유연성과 접근성에 점점 더 많은 관심을 받고 있다. 예를 들어, 아키텍처NeRF2NeRF[15]는 이미지 기반 확산 모델(구성픽스2Pix[3])을 사용하여 사용자의 지시에 의해 렌더링된 이미지를 수정한 후 수정된 이미지로 3D 방사 필드를 업데이트한다. 드림에디터[63] 및 Vox-E[48]는 각각 명시적인 3D 표현(즉, 메쉬 및 복셀)을 채택하여 더 나은 로컬 편집을 가능하게 하며, 여기서 편집 영역은 2D 교차 의도 맵에 의해 자동으로 결정된다. 가우시안 에디터[8, 12]는 GS를 장면 표현으로 채택하고 효율적이고 정밀한 장면 편집을 용이하게 하기 위해 3D 시맨틱 분할[6, 21]을 통합한다. 그러나 이러한 텍스트 중심 접근법은 편집 결과의 지정된 외관 및 위치에 대한 정확한 제어가 부족하다.\n' +
      '\n' +
      '동시 작업인 고객NeRF[16]은 우리의 과제 설정과 가장 관련이 있다. 그러나 고객NeRF는 편집 대상으로서 암묵적 NeRF 장면에 존재하는 분할 도구[22]에 의해 검출될 수 있는 객체가 필요하기 때문에 객체 대체 작업만을 지원한다. 대조적으로, 우리는 명시적인 GS를 3D 표현으로 채택하여 더 많은 편집 작업(예를 들어, 객체 삽입 및 스타일화)을 수행하는 방법을 용이하게 한다.\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      '## 3D 가우시안 스플링.\n' +
      '\n' +
      '3D 가우시안 스플라팅(GS) [20]은 높은 렌더링 품질과 효율성으로 인해 빠르게 엄청난 관심을 끌고 있다. GS는 점 유사 이방성 가우스 \\(g_{i}\\) 세트를 사용하여 현장(\\mathcal{G}=\\{g_{1},g_{2},g_{N}\\}\\)을 나타낸다. 각 \\(g_{i}\\)는 중심 위치 \\(\\mu\\in\\mathbb{R}^{3}\\), 불투명도 \\(\\alpha\\in\\mathbb{R}^{1}\\), 3D 공분산 행렬 \\(\\Sigma\\), 컬러 \\(c\\)를 포함하여 일련의 최적화된 속성을 포함한다. 상이할 수 있는 비장 렌더링 프로세스를 요약하면 다음과 같다.\n' +
      '\n' +
      '1}} (1-\\prod_{i})\\\\prod_{i}} (x)\\pha_{i}.\n' +
      '\n' +
      '\\(g_{i}\\)는 상행 순서에 따라 광학 중심까지의 거리에 따라 \\(g_{i}\\)의 동양인을 지수하고, \\(c_{i}\\), \\(알파_{i}\\), \\(x_{i}\\)는 각각 \\(i\\)-가우시안(i\\)의 중심점까지의 색상, 밀도, 거리를 나타낸다.\n' +
      '\n' +
      '###은 SDS Loss와 함께 래딩 퓨즈를 최적화합니다.\n' +
      '\n' +
      'Score 증류 샘플링(SDS) [35]은 3D 생성을 위한 Text-to-Image(T2I) 확산 모델에서 사제들을 증류하여 방사계를 최적화한다. 사전 훈련된 확산 모델 \\(\\파이\\)은 비지도 이미지 \\(\\hat{I}_{t}\\) 및 이의 텍스트 조건 \\(y\\)을 감안할 때 추가된 소음을 예측하는 데 사용된다.\n' +
      '\n' +
      '{\\hat{I}}\\at{\\g{t}},\\tag{t}},\\tag{2}}(\\tag{t) 인큐베이터{f}(\\math{i},\\tag{2}) 채널(\\math{i})은\\math{E}(\\i){\\i}(\\i){\\i}}(\\i){\\i}.{\\i},\\tag{t},\\tag{t},\\tag{at{f},\\tag{t},\\apapapapapa{f},\\f},\\\\et{d\\i:\\f},\\o{f},\\apapapa{f},\\i){f}(\\i){f}(\\i){f}(\\i){f}(\\i){f}(\\i){f}(\\i){f}(\\i){f}.{f},\\i<\\i<\\i<\\i<\\i<\\i<\\i<\\i<\\\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '목표 장면의 제기된 이미지(즉, 이미지 및 COLMAP[46])에 의해 추정된 관련 카메라 파라미터를 감안할 때, 우리의 목표는 사용자 지정 3D 바운딩 박스 내에서 하이브리드 텍스트 이미지 프롬프트 후 보다 정확한 편집을 가능하게 하는 것이다. GS는 명시적이고 유연한 3D 표현 방법이기 때문에 3D 장면을 나타내기 위해 3D 가우시안 스플라팅(GS) [20]을 선택하는데, 이는 다음과 같은 편집 작업, 특히 지역 편집에 유리하다.\n' +
      '\n' +
      '그림과 같이. 2, TIP-에디터는 기존 장면의 단계적 2D 개인화, 새로운 콘텐츠(Sec 4.1), 2)는 점수 증류 샘플링(SDS) [35](Sec. 4.2), 3D 장면의 픽셀 레벨 정제(Sec. 4.3)를 사용하여 거친 3D 편집 단계를 포함하는 3개의 주요 단계를 포함한다.\n' +
      '\n' +
      '2D 개인화.\n' +
      '\n' +
      '일반적으로 사전 훈련된 T2I 모델(즉, 스테이블 디퓨전(SD) [42])의 단계적 개인화는 드림보트[44]를 기반으로 하지만 두 가지 상당한 수정을 가지고 있다. 이러한 변화는 기존의 장면과 참조 영상에서 새로운 내용을 모두 개인화하는 데 필수적이다. 먼저 기존 장면의 2D 개인화에서는 제공된 3D 바운딩 박스(예: 이마에 선글라스, 그림 4)에 의해 지정된 기존 콘텐츠와 소설 콘텐츠의 상호 작용을 강제하기 위한 주의 기반 현지화 손실을 제안한다. 참조 이미지는 이 단계에 관여하지 않는다. 둘째, 소설 콘텐츠의 2D 개인화에서는 참조 영상에서 지정된 아이템의 고유한 특성을 더 잘 캡처하기 위해 LoRA 레이어를 소개합니다.\n' +
      '\n' +
      '#### 4.1.1 2D 기존 장면의 개인화이다.\n' +
      '\n' +
      '우리는 먼저 SD를 주어진 장면에 개인화하여 이후 다양한 형태의 장면을 편집할 수 있도록 한다. 구체적으로, 이미지 캡션 모델인 BLIP-2 [26]을 사용하여 초기 텍스트 프롬프트(예: "장난감")를 얻는다. 장면의 특수성을 높이기 위해 장면을 설명하는 명사 앞에 특별한 토큰 \\(V_{1}\\)을 추가하여 [63]에서와 같이 장면별 텍스트 프롬프트(예: "a \\(V_{1}\\)"를 생성한다. T2I 모델의 UNet \\(\\epsilon_{\\파이}\\)는 재구성 손실과 사전 보존 손실 [44]로 미세 조정된다. 재건축 훈련의 입력은 장면별 텍스트와 랜덤 뷰로부터 3D 장면의 렌더링된 이미지를 포함한다. 사전 보존 훈련의 입력은 초기 텍스트 및 실행된 보존 훈련을 포함한다.\n' +
      '\n' +
      '그림 2: ** 방법 개요. TIP-에디터는 주어진 하이브리드 텍스트 이미지 프롬프트와 일치하도록 3D 가우시안 스플라팅(GS)으로 표시되는 3D 장면을 최적화한다. 편집 과정은 장면 개인화 단계에서 국산화 손실을 특징으로 하는 단계적 2D 개인화 전략과 LoRA(Sec 4.1; 2)를 기반으로 하는 기준 이미지에 전용되는 별도의 신규 콘텐츠 개인화 단계를 포함하는 것으로, SDS(Sec. 4.2)를 이용한 거친 편집 단계, 2)는 렌더링된 이미지와 변성된 이미지(Sec. 4.3) 모두에서 주의 깊게 생성된 의사-GT 이미지를 이용하여 픽셀 레벨 텍스쳐 정제 단계를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3), 및 변성 이미지(Sec.4.3)를 사용하여 픽셀 레벨 정제 단계를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)와 변성 이미지(Sec.4.3)에서 조심스럽게 생성된 의사-GT 이미지를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)와 변성 이미지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4.3)를 사용하여 픽셀 레벨 정제 스테이지(Sec.4\n' +
      '\n' +
      '초기 텍스트를 입력으로 하여 SD에 의해 생성된 도메인 이미지(그림 2에 표시하여 클러터를 감소시킨다)를 입력한다. 위의 손실은 다음과 같이 계산된다.\n' +
      '\n' +
      '}.^math{2} 〈^math{*]}}<^math{_\\d{*]}.\n' +
      '\n' +
      'I\\(y\\)는 텍스트 프롬프트, \\(t\\) 타임스팟, \\(z_{t}\\)은 입력 장면 이미지에서 추출된 \\(t\\)-번째 타임스메프에서 안내하는 잠재 코드, \\(p\\) 카메라 포즈를 나타낸다. Superscript \\(*\\)는 사전 보존 교육에 사용된 해당 변수를 나타낸다. 우리는 SD로부터 생성된 이미지의 더 나은 관점 제어를 갖도록 네트워크 내의 조건 임베딩에 추가 카메라 포즈 \\(p\\)를 추가하여 후속 SDS 기반 3D 장면 최적화를 촉진한다는 점에 유의한다. 사전 보존 교육을 위해 무작위로 생성된 이미지는 의미 있는 "스코어 포즈"가 없기 때문에 렌더링에 사용되지 않는 고정된 카메라 포즈 \\(p^{*}=I_{4}\\)를 할당한다.\n' +
      '\n' +
      '대상 객체의 정확한 위치를 장려하기 위해 주의 기반 현지화 손실(그림 2)을 소개한다. 2D 장면 개인화를 통해 SD가 필요한 장면-객체 상호작용을 포함하는 이미지를 생성하도록 장려한다. 이 단계는 표적이 거의 볼 수 없는 위치(예를 들어 이마에 선글라스, 그림 4)에서 특정되는 경우 특히 중요하다. SD에 의해 생성된 대상 객체의 실제 위치는 [17]에 따른 객체 키워드(예: "성글라스")의 교차 의도 맵 \\(A_{t}\\)에서 추출된다. 타겟 객체(즉, GT 편집 영역)의 원하는 위치는 제공된 3D 바운딩 박스를 이미지 평면에 투영하여 얻는다. 실제 위치와 원하는 위치 사이의 손실은 그대로 정의된다.\n' +
      '\n' +
      '}}{t}}^{2}]]\n' +
      '\n' +
      '여기서 \\(\\lambda\\)는 GT 편집 마스크 영역(\\mathcal{S}\\)인 \\(\\mathcal{S}\\)의 주입(\\mathcal{B}\\))과 \\(\\tilde{\\mathcal{S}\\)의 두 가지 용어를 균형을 맞추는 가중치이다. 직관적으로, 이러한 손실은 편집 영역 내부의 높은 확률을 장려하고 편집 영역 외부의 대상 객체의 존재를 처벌한다. 우리의 절제 연구에서 입증된 바와 같이(그림 4) 이 손실은 지정된 지역 내에서 정확한 편집을 보장하는 데 중요하다.\n' +
      '\n' +
      '#### 4.1.2 2D 신규 콘텐츠 개인화입니다.\n' +
      '\n' +
      '기준 이미지에 포함된 고유한 특성을 더 잘 포착하기 위해 LoRA[18](UNet 고정)를 이용한 전용 개인화 단계를 소개한다. 이 단계는 여러 개념을 학습(개인화)할 때 부정적인 영향(예를 들어 [24]를 잊는 개념)을 줄여 장면과 소설 내용을 모두 더 잘 표현하게 하는 데 필수적이다. 구체적으로, 이전에 개인화되고 고정된 T2I 모델 \\(\\epsilon_{\\phi^{*}}\\)에 삽입된 추가 LoRA 층을 훈련시킨다. 마지막 단계와 유사하게 BLIP-2 모델을 사용하여 초기 텍스트 프롬프트를 얻고 특수 토큰 \\(V_{2}\\)을 그에 삽입하여 참조 객체(예:\\(V_{2}\\)의 객체 특이적 텍스트 프롬프트(y^{r}\\)를 생성한다(예:\\(V_{2}\\) 선글라스로 생성한다. 새로운 LoRA 계층은 다음과 같은 손실 함수로 훈련된다.\n' +
      '\n' +
      '}.{z{r}.\\mathb{r}\\epsilon_{{r}}t,^{r}.\n' +
      '\n' +
      '훈련 후 장면의 내용과 기준 이미지는 각각 UNet과 추가된 LoRA층에 저장되어 상호 간섭이 크게 감소한다.\n' +
      '\n' +
      'SDS 로스를 통해\n' +
      '\n' +
      '선택된 가우스 \\(\\mathcal{G}^{\\mathcal{B}}\\in\\mathcal{B}\\)를 최적화하고(즉, 바운딩 박스 \\(\\mathcal{B}\\)) 내부는 개인화된 T2I 확산 모델 \\(\\epsilon_{\\phi_{\\2}}\\)에서 SDS 손실로 최적화한다. 구체적으로, 샘플링된 카메라 포즈 \\(p\\) 및 텍스트 프롬프트 \\(y^{G}\\)를 사용하여 무작위로 렌더링된 이미지 \\(\\hat{I}\\)를 T2I 모델 \\(\\epsilon_{\\phi_{\\_{2}}\\)에 입력하고, 다음과 같이 글로벌 장면 SDS Loss를 계산한다.\n' +
      '\n' +
      '}\\i_{f}}\\mathcal{G}}\\math{g}}\\math{{}(t)\\math{{}.\n' +
      '\n' +
      'HPA(y^{G}\\)가 특수 토큰 \\(V_{1},V_{2}\\)을 포함한 텍스트 프롬프트로서 GS 렌더링 알고리즘인\\(f(\\cdot)를 설명한다.\n' +
      '\n' +
      '주목할 점은 최적화된 가우스 \\(\\mathcal{G}^{\\mathcal{B}}\\)의 선택 및 업데이트 기준이 다양한 유형의 편집 작업에 대해 약간 다르다는 점이다. 객체 삽입을 위해 바운딩 박스 내부의 모든 가우스인들을 복제하고 이러한 새로운 가우스 동양인의 모든 속성을 독점적으로 최적화한다. 객체 교체 및 재텍스트화를 위해 바운딩 박스 내부의 모든 가우스 사람들이 업데이트됩니다. 스타일링을 위해 현장에서 모든 가우스에게 최적화가 적용된다. 모든 속성을 업데이트하는 대신 재텍스트화를 위한 색상(즉, 구형 고조파 계수)만 업데이트하면 된다.\n' +
      '\n' +
      'GS 기반 장면의 전경 및 배경은 바운딩 박스 \\(\\mathcal{G}^{\\mathcal{B}}\\)를 고려할 때 쉽게 분리할 수 있기 때문에 아티팩트를 줄이기 위해 대상 중심 편집(예: 객체 삽입/교체)에 대한 또 다른 로컬 SDS 손실을 소개합니다.\n' +
      '\n' +
      '}(\\mathcal{I}\\math{}}\\math{{})\\math{{}.\n' +
      '\n' +
      '여기서 \\(y^{L}\\)는 특수 토큰 \\(V_{2}\\)을 포함하는 텍스트 프롬프트이며 우리의 원하는 새로운 객체 \\(\\hat{I}\\)만을 포함하는 렌더링된 이미지만을 설명한다.\n' +
      '\n' +
      '우리는\\(\\mathcal{L}_{SDS}^{G}\\)와\\(\\mathcal{L}_{SDS}^{L}^{L}\\)를 사용하여\\(\\gamma\\)를 최적화한다.\n' +
      '\n' +
      '}}^{G}}^{G}^{L}\\tag{8}\\\\\\mathcal{L.\n' +
      '\n' +
      '픽셀 수준 이미지 감금\n' +
      '\n' +
      '이 단계에서 SDS 손실로 직접 최적화된 3D 결과에는 일반적으로 인공물(예: 안경 프레임에 녹색 노이즈, 그림 11의 모발에 바늘과 같은 노이즈)이 포함되어 있기 때문에 편집 결과의 품질을 효과적으로 향상시키기 위해 픽셀 수준 재구성 손실을 소개합니다.\n' +
      '\n' +
      '이 단계의 핵심은 유사GT 이미지 \\(I_{gt}\\)를 생성하여 거친 GS로부터 렌더링된 이미지 \\(I_{c}\\)를 감독하는 것이다. 먼저 SDEdit[30]을 따르고 \\(I_{c}\\)에 소음을 추가하여 \\(I_{c}\\)를 얻은 다음 개인화된 T2I 모델 \\(e_{\\phi_{2}}\\)을 데노징 네트워크로 활용하고 \\(I_{c}^{d}\\)를 얻는다. 데오징 과정은 \\(I_{c}\\)의 유물을 효과적으로 감소시키지만(보충에서 그림 D.1 참조) 배경 이미지도 변경한다. 둘째, 편집 가능한 가우스 \\(\\mathcal{G}^{\\mathcal{B}}\\)만을 렌더링하여 편집된 객체/부품의 이진 인스턴스 마스크 \\(M^{inst}\\)를 획득하고 불투명성 마스크를 보유한다. 그런 다음 고정된 가우스만 있는 배경 이미지 \\(I_{bg}\\)를 만듭니다. 마지막으로 의사-GT 이미지 \\(I_{gt}\\)를 구한다.\n' +
      '\n' +
      'I_{gt}\\{ot I_{c}^{d}+(1-M^{inst})\\odot I_{bg}\n' +
      '\n' +
      '이 과정은 T2I 모델(\\epsilon_{\\phi_{2}}\\)에 의해 전경 편집 영역이 향상되는 동안 배경 이미지가 깨끗하고 원본 장면과 동일함을 보장한다. 이 의사-GT 이미지를 픽셀 레벨 감독으로 사용하여 결과적인 텍스처를 효과적으로 향상시키고 플로터를 감소시킨다(그림 11). 데이터된 이미지 \\(I_{c}\\)와 생성된 유사GT 이미지 \\(I_{gt}\\) 사이에 MSE 손실이 적용된다. I_{gt}\\(I_{gt}\\)의 완전한 준비를 나타내는 흐름도(그림 B.1)가 보충에 포함된다.\n' +
      '\n' +
      '더 나은 커버리지를 유지하기 위해 렌더링 카메라는 미리 정의된 범위 내에서 \\(30^{\\ 회로}\\)의 간격을 갖는 모든 고도 및 방위각을 커버한다. 추상된 이미지의 더 나은 시야를 유지하기 위해 SDEdit에서 작은 소음 레벨(t_{0}=0.05\\), 즉 "중간 시간"을 설정했다. 이러한 작은 소음 레벨을 사용하여 미세한 질감 디테일을 효과적으로 향상시키고, 작은 아티팩트를 제거하고, 큰 모양과 외모 변화를 도입하지 않아 목표 편집 영역에 대한 보다 나은 시야 일관성을 유지한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '** 구현 디테일*** 우리는 공식 코드를 사용하여 기본 하이퍼-파라미터와 함께 원래 장면 GS를 훈련시킨다. 단계적 2D 개인화 단계에서 장면 개인화 단계는 1k 반복으로 구성되며, 새로운 콘텐츠 개인화에는 500개의 반복이 포함되어 있으며 \\(\\mathcal{L}_{loc}\\)에 \\(\\lambda=0.1\\)를 설정한다. 거친 편집 단계에서는 [63]의 조회 표본 추출 전략을 채택한다. 렌더링된 이미지의 크기는 512\\(CSI)512로 편집 작업의 복잡성이 다르기 때문에 이 단계는 1K\\(\\ capillary\\)5K 반복을 최적화하여 약 5\\(\\ason\\)25분을 섭취해야 한다. 활성화 단계는 생성된 \\(I_{gt}\\)의 감독으로 3K 반복을 취하여 3분 이내에 마무리한다. 보다 많은 구현 세부 사항을 보충에서 찾을 수 있다.\n' +
      '\n' +
      '**Dataset***의 방법을 종합적으로 평가하기 위해 이전 작품 [8, 15, 63]에 따라 복잡도가 다른 6개의 대표 장면을 선택한다. 이러한 장면들은 단순한 배경, 인간의 얼굴, 복잡한 실외 장면의 객체들을 포함한다. 우리는 원래 GS를 훈련시키기 위해 COLMAP[47]에서 추출한 장면 이미지와 추정된 카메라 포즈를 사용한다. 각 편집에 대해, 텍스트로 구성된 하이브리드 프롬프트와 인터넷으로부터 획득된 참조 이미지로 구성된 하이브리드 프롬프트를 사용하여 편집을 안내한다. 또한 편집 영역을 정의하기 위해 수동으로 3D 바운딩 상자를 설정했습니다.\n' +
      '\n' +
      '** 기본*** 전용 이미지 기반 편집 바젤의 결핍과 비교하여 구성-NeRF2NeRF("I-N2N") [15] 및 드림에디터[63]를 포함한 2가지 최첨단 텍스트 기반 방사계 편집 방법과 비교한다. I-N2N은 특수 텍스트 지침에 따라 렌더링된 멀티 뷰 이미지를 업데이트하기 위해 구성-픽셀2 픽셀[3]을 사용한다. 드림에디터는 메쉬 기반 표현을 채택하고 있으며 지역 편집을 지원하기 위한 주의 기반 현지화 동작을 포함한다. 공정한 비교를 위해 자동 위치를 보다 정확한 수동 선택으로 교체합니다. 더 많은 구현 세부 사항에 대한 보충 사항을 참조하십시오.\n' +
      '\n' +
      '*** 평가 기준*** 정량 평가를 위해 [15, 63]에 따른 CLIP 텍스트 이미지 지향성 유사성을 채택하여 주어진 텍스트 프롬프트와 편집 결과의 정렬을 평가한다. 이미지 이미지 정렬(편집된 장면과 참조 이미지 사이에)을 평가하기 위해 [16]을 따라 참조 이미지와 편집된 3D 장면의 렌더링된 멀티 뷰 이미지 사이의 평균 DINO 유사성[33]을 계산한다. 이러한 계산에 대한 자세한 정보는 보충적으로 사용할 수 있다.\n' +
      '\n' +
      '또한 사용자 연구를 수행하고 참가자에게 두 가지 측면(모든 "품질", "위탁"과 참조 이미지에 대한 다른 방법의 결과를 평가하기 위해(총 50명) 참가자에게 요청한다. 사용자 연구는 각각 10개의 질문을 포함하며, 2개의 기저부의 편집된 결과를 포함하고 우리는 랜덤 순서로 영상(보충 참조)으로 회전하는 영상으로 렌더링된다. 10개 문항은 다양한 시나리오에서 방법을 더 잘 비교하기 위해 다양한 장면 및 편집 유형을 다루었다.\n' +
      '\n' +
      '##, TIP-Editor의 일정 결과.\n' +
      '\n' +
      '그림 1 및 그림 1에 나와 있다. 7, TIP-Editor의 정성적 결과를 제시한다. 영상 시위는 보충에 포함되어 있습니다. 다양한 3D 장면에 대한 실험은 TIP-에디터는 재텍스트화, 객체 삽입, 객체 대체 및 스타일링을 포함한 다양한 편집 작업을 효과적으로 실행하여 고품질 결과를 달성하고 제공된 텍스트 프롬프트 및 참조 이미지에 따라 엄격하게 실행함을 보여준다.\n' +
      '\n' +
      'TIP-Editor와 이전 방법 간의 가장 구별 가능한 차이점 중 참조 이미지에 의해 특정된 고유한 특성***1은 TIP-Editor도 이미지 프롬프트를 지원하여 보다 정확한 제어를 제공하고 실제 애플리케이션에서 보다 사용자 친화적으로 만드는 것이다. 그림의 결과는 그림 1과 같다. 1&7은 업데이트된 3D 장면과 기준 이미지(예: 선글라스의 _styles_; _화이트_ giraffe; _가상 유령_마; _virtual Sh-up; j 포커 메이크업업은 영화 _The Dark Knight_)에서 높게 나타났다. 또한 그림 하단에 묘사된 바와 같이. 1, 우리의 방법은 또한 기준 이미지의 _Modigliani_ 스타일로 전체 장면을 전달하는 것과 같은 글로벌 장면 편집을 수행할 수 있다.\n' +
      '\n' +
      '** 서열 편집*** TIP-Editor는 GS의 로컬 업데이트와 단계적 2D 개인화 전략 덕분에 초기 장면을 여러 번 순차적으로 편집할 수 있어 기존 장면과 신규 콘텐츠의 간섭을 효과적으로 줄일 수 있다. 그림 8의 결과는 순차 편집 능력을 보여준다. 여러 번의 편집 후 관찰 가능한 품질 저하가 없으며 서로 다른 편집 동작 간에 간섭이 없다.\n' +
      '\n' +
      '생성된 이미지를 참조로 사용하여*** 참조 이미지가 없으면 T2I 모델에서 여러 후보를 생성하고 사용자가 만족스러운 이미지를 선택하도록 할 수 있다. 이러한 상호 작용은 사용자에게 더 많은 제어를 제공하고 최종 결과를 더 예측 가능하게 한다. 그림. 9는 몇 가지 예를 보여준다.\n' +
      '\n' +
      '##는 국가와 비교합니다.\n' +
      '\n' +
      '*** 정성적 비교*** 그림 3은 우리의 방법과 기저부의 시각적 비교를 보여준다. 두 기저부 모두 이미지 프롬프트를 입력으로 지원하지 않기 때문에, 객체 카테고리에 속하는 제어되지 않은(아마도 가장 일반적인) 항목을 생성한다. 대조적으로, 우리의 결과는 참조 이미지(즉, _heart-자형_ 선글라스; _화이트_기린; _화이트_기린; 영화 _The Dark Knight_)의 조커에 명시된 고유한 특성을 일관되게 유지한다.\n' +
      '\n' +
      '더욱이, 구성-N2N은 때때로 키워드(1로)를 오해하거나 (2로) 간과할 수 있거나 제한된 실험(3열)에서 지정된 외관을 생성할 수 없으며, 이는 아마도 구성-Pix2Pix에서 제한된 지원 지침 때문일 것이다. 드림에디터는 사용자가 지정된 선글라스 아이템(1번까지)을 추가하고자 하는 경우에도 어려움을 겪고 있다. 또한, 드림에디터는 더 적은 형태를 채택하여 기존의 물체에 명백한 형태 변화(2로)를 하기 어렵다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & CLIP\\({}_{dir}\\) & DINO\\({}_{sim}\\) & Vote\\({}_{quality}\\) & Vote\\({}_{alignment}\\) \\\\ \\hline Instruct-N2N & 8.3 & 36.4 & 21.6\\% & 8.8\\% \\\\ DreamEditor & 11.4 & 36.8 & 7.6\\% & 10.0\\% \\\\ Ours & **15.5** & **39.5** & **70.8\\%** & **81.2\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 정량적 비교는 표 1이다. CLIP\\({}_{dir}\\)는 CLIP 텍스트 이미지 지향성 유사성이다. DINO\\({}_{ason}\\)는 DINO 유사성이다.\n' +
      '\n' +
      '그림 3: 서로 다른 방법 간의 시각적 비교는 그림 3이다. 우리의 방법은 분명히 더 높은 품질의 결과를 생성하고 _백신_는 기준 이미지 입력(1열에서 오른쪽 모서리)을 따른다. 강사-N2N은 때때로 (1로) 오해하거나 (2로) 키워드를 간과한다. 드림에디터는 명백한 형상 변화(2열)를 만드는 데 어려움을 겪고 있다. 둘 다 이미지 프롬프트가 상세한 외모/스타일을 특정하여 덜 통제된 결과를 생성하는 것을 지원하지 않는다.\n' +
      '\n' +
      '유연한 메쉬 기반 표현(즉, NeuMesh)\n' +
      '\n' +
      '** 정량적 비교** Tab. 1은 CLIP 텍스트 이미지 지향성 유사성(CLIP\\({}_{dir}\\)) 및 DINO 유사성(DINO\\({}_{sim}\\)의 결과를 보여준다. 결과는 두 메트릭 모두에서 우리의 방법의 우월성을 분명히 보여주며, 이는 우리의 방법으로 생성된 외관이 텍스트 프롬프트와 이미지 프롬프트 둘 다와 더 잘 정렬됨을 시사한다. 사용자 연구에 따라 유사한 결론이 도출되었다. 우리의 결과는 _퀄리티_ 평가(70.8\\%\\) 표와 _alignment_ 평가(81.2\\%\\) 표 모두에서 상당한 차이가 있는 기저부를 능가한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '단계적 2D 개인화에 대한**Ablation 연구는 단계적 2d 개인화에서 \\(\\mathcal{L}_{loc}\\) 및 LoRA 플레이어를 사용하는 이점을 입증하기 위해 그림 4에서 절제 실험을 수행한다. i\\(\\mathcal{L}_{loc}\\)가 없으면, 미세 조정 T2I 모델은 원래 T2I 모델의 학습 데이터에 존재하는 편향으로 인해 지정된 영역(즉, 이마에 선글라스를 배치하지 못한다. 기준 영상에서 고유한 특징을 개인화하기 위해 전용 LoRA 레이어를 도입하면 더 충실한 출력이 생성되어 참조 영상에서 세부 정보를 캡처하는 데 제안된 단계적 2D 개인화 전략의 효과를 보여준다.\n' +
      '\n' +
      '다른 3D 표현에 대한**Ablation 연구는*** 그림에서 서로 다른 3D 표현을 테스트한다. 5는 다른 모든 설정을 그대로 유지하면서. GS를 사용하면 배경이 변하지 않는 상태에서 최상의 편집 결과를 얻을 수 있다. 인판-NGP[32]의 경우 공유 MLP 디코더 및 다중 해상도 그리드의 채택으로 인해 다른 위치의 함량이 독립적이지 않기 때문에 배경의 원치 않는 변화를 관찰했다.\n' +
      '\n' +
      '화소 수준의 정제 단계의*** 효과 그림 11과 같이 정제 단계를 도입하는 것은 인공물을 효과적으로 줄이고 질감을 향상시켜 품질이 상당히 향상된다.\n' +
      '\n' +
      '거친 편집에서 다른 \\(\\gamma\\)의 감염******** 그림 6과 같이 전 세계적으로 SDS 손실과 국소 SDS 손실이 모두 필요하며 솔루션이 최고의 결과를 얻을 수 있다. 구체적으로, 글로벌 SDS 손실 \\(\\mathcal{L}_{SDS}^{G}\\)만을 사용하면 편집 가능한 영역에서 명백한 유물이 발생한다. 로컬 SDS 손실 \\(\\mathcal{L}_{SDS}^{L}\\)만을 사용하여 편집 중에 컨텍스트 정보가 누락되어 배경과 신규 콘텐츠 간의 부정확한 배치와 부자연스러운 색상 불일치를 초래한다.\n' +
      '\n' +
      '6개의 할당 및 임금은 6개의 할당 및 임무\n' +
      '\n' +
      '이 논문에서 제안된 TIP-Editor는 새로운 텍스트 구동 3D 편집과 텍스트 설명의 보완으로서 추가 이미지 프롬프트를 동일시하고 배경을 변하지 않고 텍스트 및 이미지 프롬프트와 정확하게 정렬된 고품질 편집 결과를 생성한다. TIP-Editor는 유의하게 향상된 제어성을 제공하며 객체 삽입, 객체 교체, 재텍스트화, 스타일링을 포함한 다양한 애플리케이션을 가능하게 한다.\n' +
      '\n' +
      'TIP-Editor의 한 가지 한계는 거친 경계 박스 입력이다. 편리하지만, 바운딩 박스가 원치 않는 요소를 포함할 수 있는 복잡한 장면에서 투쟁한다. 장면의 3D 인스턴스 분할을 자동으로 얻는 것이 매우 유익할 것이다. 또 다른 한계는 GS로 대표되는 장면에서 부드럽고 정확한 메쉬를 추출하기 어렵기 때문에 기하학 추출과 관련이 있다.\n' +
      '\n' +
      '그림 4: 단계적 2D 개인화에서 제안된 구성요소에 대한 변형 연구는 다음과 같다. 우리는 생성된 개인화된 T2I 모델(톱 로우)의 이미지와 업데이트된 3D 장면(바닥 행)의 렌더링된 이미지를 비교한다. 지역화 손실 \\를 제거(\\mathcal{L}_{loc}\\)하는 것은 지정된 장소에 새로운 객체를 배치하지 못한다. 기준 이미지의 개인화를 위해 전용된 별도의 LoRA 레이어를 제거하면 덜 유사한 결과(하트 모양 대 일반 원형 형태)가 생성된다.\n' +
      '\n' +
      '그림 5: 이 작업에 대한 GS의 이점을 보여주기 위한 다양한 3D 표현에 대한 구조 연구는 다음과 같다. 인판-NGP를 사용하면 NeuMesh를 사용하면서 변화된 배경이 될 수 있으며 충분한 형상 변형을 생성할 수 없다. 대조적으로, _explicit_ 및 _flexible_ GS를 사용하면 배경이 변하지 않는 상태에서 최상의 전경 편집 결과를 얻을 수 있다.\n' +
      '\n' +
      '그림 6: 글로벌 및 로컬 SDS(Eq. 8)의 영향에 대한 아플레이션 연구는 전 세계 및 로컬 SDS(Eq. 8)의 영향에 관한 연구이다. 거친 무대에서. 상위 행은 편집 가능한 가우스 \\(\\mathcal{G}^{B}\\)의 렌더링을 보여준다. 글로벌 SDS \\(\\mathcal{L}_{SDS}^{G}\\)만을 사용하여 낮은 품질의 전경 객체/부품을 생산하는 반면, 로컬 SDS \\(\\mathcal{L}_{SDS}_{SDS}^{L}\\)만을 사용하여 기존 장면(예: 색상, 배치)과 합성할 때 부자연스러운 전경을 생성한다.\n' +
      '\n' +
      '그림 8: 필수 편집 결과. 우리는 톱좌표 모서리에 있는 숫자로 표시된 모든 편집 단계 후에 3D 장면의 두 개의 렌더링된 이미지를 보여준다. (V_{*}\\), \\(V_{***}\\), \\(V_{***}\\), \\(V_{**+}\\)는 편집의 서로 다른 시퀀스에서 장면의 특별한 토큰을 나타낸다.\n' +
      '\n' +
      '그림 7: 제안된 TIP-Editor의 모어 편집 결과. 텍스트 프롬프트의 이미지는 최적화 없이 고정된 관련 _rare 토큰_을 나타낸다.\n' +
      '\n' +
      '그림 11: 거친 편집 결과 및 정제 결과를 비교했다. 화살표로 표시된 영역은 편집 결과의 품질을 향상시키는 정제 단계의 효능을 보여준다.\n' +
      '\n' +
      '그림 10: 상이한 2D 개인화 방법의 비교이다. 개인화(톱) 후 T2I 모델의 생성 이미지와 최종 업데이트된 3D 장면(바닥)이 제시된다. __ 텍스트 프롬_ <V_{1}\\>은 이마에 선글라스(V_{2}\\)를 착용한 장난감이다.\n' +
      '\n' +
      '그림 9: 생성된 이미지를 참조로 사용하는 결과이다. 우리는 먼저 텍스트 프롬프트를 사용하여 확산 모델에 의해 여러 후보 이미지를 생성한 다음 편집을 위한 참조 이미지로 하나를 선택한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _CVPR 2022_, pages 18208-18218, 2022.\n' +
      '* [2] Chong Bao, Yinda Zhang, and Banghang et al. Yang. SINE: Semantic-driven image-based nerf editing with prior-guided editing field. In _CVPR 2023_, pages 20919-20929, 2023.\n' +
      '* [3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. InstructPix2Pix: Learning to follow image editing instructions. _arXiv preprint arXiv:2211.09800_, 2022.\n' +
      '* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [5] Raoul Cao Anh-Quan, de Charette. SceneRF: Self-supervised monocular 3d scene reconstruction with radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9387-9398, 2023.\n' +
      '* [6] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3D with NeRFs. In _NeurIPS_, 2023.\n' +
      '* [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3D content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [8] Yiwen Chen, Zilong Chen, and Chi. eta Zhang. GaussianEditor: Swift and controllable 3D editing with gaussian splatting. _arXiv preprint arXiv:2311.14521_, 2023.\n' +
      '* [9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. _arXiv preprint arXiv:2309.16585_, 2023.\n' +
      '* [10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. DiffEdit: Diffusion-based semantic image editing with mask guidance. _arXiv preprint arXiv:2210.11427_, 2022.\n' +
      '* [11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. NeRDi: Single-view NeRF synthesis with language-guided diffusion as general image priors. _arXiv preprint arXiv:2212.03267_, 2022.\n' +
      '* [12] Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. GaussianEditor: Editing 3D gaussians delicately with text instructions. _arXiv preprint arXiv:2311.16037_, 2023.\n' +
      '* [13] Rinon Gal, Yuval Alaulf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [14] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-guided domain adaptation of image generators. _ACM Transactions on Graphics (TOG)_, 41(4):1-13, 2022.\n' +
      '* [15] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3d scenes with instructions. _arXiv preprint arXiv:2303.12789_, 2023.\n' +
      '* [16] Runze He, Shaofei Huang, Xuecheng Nie, and et al. Hui, Tianrui. Customize your NeRF: Adaptive source driven 3d scene editing via local-global iterative training. _arXiv preprint arXiv:2312.01663_, 2023.\n' +
      '* [17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. _arXiv preprint arXiv:2210.09276_, 2022.\n' +
      '* [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, and et al. Mao, Hanzi. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.\n' +
      '* [23] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. _arXiv preprint arXiv:2205.15585_, 2022.\n' +
      '* [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [25] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. Mine: Towards continuous depth mpi with nerf for novel view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12578-12588, 2021.\n' +
      '* [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. _arXiv preprint arXiv:2211.10440_, 2022.\n' +
      '* [28] Hao-Kang Liu, I Shen, Bing-Yu Chen, et al. NeRF-In: Freeform NeRF inpainting with RGB-D priors. _arXiv preprint arXiv:2206.04901_, 2022.\n' +
      '* [29] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. RealFusion: 360{\\(\\backslash\\)deg} reconstruction of any object from a single image. _arXiv preprint arXiv:2302.10663_, 2023.\n' +
      '* [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [32] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, and et al. Vo, Huy. DINov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* [34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. DreamFusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [36] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [37] Alec Radford, Jong Wook Kim, Chris Hallacy, and et al. Learning transferable visual models from natural language supervision. In _ICML 2021_, pages 8748-8763, 2021.\n' +
      '* [38] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dream-Booth3D: Subject-driven text-to-3D generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [40] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 12179-12188, 2021.\n' +
      '* [41] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3D shapes. _arXiv preprint arXiv:2302.01721_, 2023.\n' +
      '* [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.\n' +
      '* [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.\n' +
      '* [44] Chitwan Saharia, William Chan, and Saurabh et al. Saxena. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS 2022_, 35:36479-36494, 2022.\n' +
      '* [45] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [46] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4104-4113, 2016.\n' +
      '* [47] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-E: Text-guided voxel editing of 3d objects. _arXiv preprint arXiv:2303.12048_, 2023.\n' +
      '* [48] Snois Sixtyboo. gaussian-splitting.\n' +
      '* [49] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerf via rank-residual decomposition. _Advances in Neural Information Processing Systems_, 35:14798-14809, 2022.\n' +
      '* [50] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D creation from a single image with diffusion prior. _arXiv preprint arXiv:2303.14184_, 2023.\n' +
      '* [51] Can Wang, Menglei Chai, Mingming He, and et al. CLIP-NeRF: Text-and-image driven manipulation of neural radiance fields. In _CVPR 2022_, pages 3835-3844, 2022.\n' +
      '* [52] Can Wang, Ruixiang Jiang, Menglei Chai, and et al. He, Mingming. NeRF-Art: Text-driven neural radiance fields stylization. _IEEE Transactions on Visualization and Computer Graphics_, 2023.\n' +
      '* [53] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.\n' +
      '* [54] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [55] Fanbo Xiang, Zexiang Xu, Milos Hasan, and et al. Neutex: Neural texture mapping for volumetric neural rendering. In _CVPR 2021_, pages 7119-7128, 2021.\n' +
      '* [56] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII_, pages 159-175. Springer, 2022.\n' +
      '* [57] Bangbang Yang, Chong Bao, and Junyi et al. Zeng. NeuMesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In _ECCV 2022_, pages 597-614. Springer, 2022.\n' +
      '**[59] 타이란 이, 지민 포, 구안준 우, 링시 시, 샤오펑 장, 위유 류, 키 톈, 쉬강 왕. 가우시안-드림러: 텍스트에서 3d 가우시안까지의 패스트 세대는 포인트 클라우드 사제들과 튀었다. arXiv 프리프린트 arXiv:2310.08529_, 2023.\n' +
      '* [60] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4578-4587, 2021.\n' +
      '* [61] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, and et al. NeRF-editing: geometry editing of neural radiance fields. In _CVPR 2022_, pages 18353-18364, 2022.\n' +
      '* [62] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. _ACM Transactions on Graphics (ToG)_, 40(6):1-18, 2021.\n' +
      '* [63] Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, and Guanbin Li. DreamEditor: Text-driven 3d scene editing with neural fields. In _SIGGRAPH Asia 2023 Conference Papers_, pages 1-10, 2023.\n' +
      '\n' +
      '우리의 보충 자료에서 우리는 더 많은 결과(A), 우리의 방법(B)과 기저부(C)의 구현 세부 사항 및 평가 세부 사항(D)을 제공한다.\n' +
      '\n' +
      '자.\n' +
      '\n' +
      '*** 디에노징 결과*** 그림 A.1은 거친 3DGS로부터의 렌더링된 이미지 \\(I_{c}\\)와 데노징 과정 후 해당 향상된 이미지의 텍스처 세부 사항을 보여준다.\n' +
      '\n' +
      '*** 그림 2는 Eq 8에서 서로 다른 \\(\\gamma\\)로 최적화된 거친 편집 결과를 보여주었으며(\\gamma\\) 값은 증가(즉, 글로벌 SDS 손실의 중요성을 증가), 편집 가능한 영역(i.g. 선글라스)에서 점점 더 분명한 유물이 나타난다. 반면, 너무 작은 \\(\\gamma\\) 값을 사용하면 선글라스에 인공물이 적게 나타난다. 그러나 최종 복합 이미지는 최적화 중 상황 정보의 부족으로 인해 부자연(예: 원본 장면과 새로운 객체의 색상 갭, 새로운 객체의 배치)이 된다. 일반적으로 Eq에 \\(\\gamma=0.5\\)를 설정합니다. 8은 좋은 균형을 잡고 편집 가능한 가우스 \\(\\mathcal{G}^{\\mathcal{B}}\\)의 소음을 효과적으로 감소시켜 선글라스를 올바르게 배치하고 화합 색상을 생산한다.\n' +
      '\n' +
      '신청자 B 적용\n' +
      '\n' +
      '공간에 의해 본지에 포함될 수 없는 보다 많은 구현 세부 정보를 제공합니다. 모든 실험은 단일 NVIDIA 테슬라 A100 GPU에서 수행된다.\n' +
      '\n' +
      '초기 GS 장면의 최적화####.\n' +
      '\n' +
      '우리의 실험에서 우리는 3D 장면을 나타내기 위해 3D 가우시안 스플라팅(GS) [20]을 사용한다. 표적 장면의 다양한 뷰에서 촬영한 이미지를 감안할 때, 우리는 먼저 COLMAP[46]를 사용하여 해당 카메라 파라미터를 추정한다. 그런 다음 공식 3DGS 저장소[49]에서 기본 하이퍼 파라미터로 초기 GS 장면을 훈련시킨다.\n' +
      '\n' +
      '단계적 2D 개인화 전략##\n' +
      '\n' +
      '우리는 허깅 페이스에 제공된 공개적으로 이용 가능한 Stable Diffusion V2.1 [43] 모델을 사용한다.\n' +
      '\n' +
      '*** 장면 개인화 단계.** 카메라를 가진 샘플 장면 이미지는 미리 정의된 범위(원 장면 이미지의 시각적 범위에 따라 측정됨) 내에서 \\(30^{\\ 회로}\\)의 간격을 갖는 모든 고도 및 방위각을 덮는 포즈이다. 우리는 초기 장면 텍스트 프롬프트를 얻기 위해 BLIP-2 [26] 모델을 사용하고, 명사 앞에 특수 토큰 \\(V_{1}\\)을 삽입하여 이 특정 장면을 기술하여 미세 조정 중에 장면별 텍스트 프롬프트 \\(y\\)를 생성한다. 사전 보존 손실을 위해 동일한 초기 장면 텍스트(y^{*}\\)를 입력으로 사용하여 해상도 \\(512\\점 512\\)에서 무작위로 200개의 이미지를 생성한다. 우리는 미세 조정에 대해 \\(lr=5\\cdot 10^{-6}\\), \\(베타1=0.9\\), \\(베타2=0.999\\), \\(가중치=10^{-2}\\) 및 \\(epsilon=10^{-8}\\)를 사용하여 AdamW 최적화를 사용한다. 장면의 개인화 단계는 배치 크기가 1인 1K 반복(\\(\\;\\심 10\\) min)으로 구성되며, \\(\\mathcal{L}_{loc}\\)에서\\(\\lambda=0.1\\)를 설정한다(Eq. 4).\n' +
      '\n' +
      '새로운 내용물 분할 모델[40]을 사용하여 512\\(1r=10^{-4}\\), \\(베타1=0.9\\), \\(베타2=0.999\\), \\(가중치=10^{-2}\\) 및 \\(epsilon=10^{-8}\\)를 사용하여 참조 영상에서 전경 객체를 512\\(lr=10^{-4}\\)로 재구성하고 미세 조정한다. 장면의 개인화 단계는 배치 크기가 1인 500회 반복(\\(\\ason 5\\) 분)으로 구성된다.\n' +
      '\n' +
      '거친 편집 단계의 자료###.\n' +
      '\n' +
      '우리는 피토르치[34]에서 거친 편집 단계를 구현합니다. [63]에서와 같이 견해 샘플링 전략을 채택하며, 렌더링된 이미지의 해상도는 512\\(\\t\\)512이며, 최적화기는 \\(beta1=0.9\\), \\(베타2=0.999\\), \\(가중치=0\\), \\(epsilon=10^{-15}\\)로 사용한다. 우리는 \\(10^{-3}\\)에서 \\(2\\cdot 10^{-5}\\)까지의 중심 위치 최적화(\\mu\\)에 대한 선형 붕괴 전략을 채택한다. 다른 속성에 대해서는 3D 공분산 행렬 \\(\\ Sigma\\)에 대한 공식 GS 설정(i. \\(\\alpha\\), \\(c\\)에 대한 \\(10^{-2}\\), \\(5\\cdot 10^{-2}\\)과 동일한 고정 학습률을 사용한다. \\(\\mathcal{L}_{SDS}}_{SDS}}_{SDS}) 및\\(\\mathcal{L}.{SDS}_{SDS}\\)의 배치 크기로 CFG 중량을 10으로 설정하고 단순 2단계 가열냉각(\\;\\ 5\\) 시간 단계를 크게 다르기 때문에 최적화 반복은 1K(\\,\\ 5\\)에서 10까지 다양하므로(\\;\\ 5\\) 시간 단계를 1K(\\-단계 가열냉각)에서 1K(\\)에서 1단계 가열냉각(\\)에서 1단계 가열냉각(\\)에서 1K(\\)에서 1단계 가열냉각(\\)에서 1단계(\\(\\)에서 1단계 가열냉각(\\)에서 5K(\\(\\)에서 1단계 가열냉각)에서 1K(\\(\\)에서 5K(\\(\\)에서 1단계 가열냉각)에서 5K(\\(\\(\\)에서 1단계 가열냉각)에서 5K(\\(\\(\\)에서 10)로 변화한다.\n' +
      '\n' +
      '그림 A.1: 거친 3DGS로부터의 렌더링된 이미지 \\(I_{c}\\)의 텍스처 세부 정보는 데노징 프로세스에 의해 더욱 향상된다.\n' +
      '\n' +
      '정액 단계의 규격###.\n' +
      '\n' +
      '거친 편집 단계의 결과를 초기화로 사용하여 렌더링된 이미지와 조심스럽게 생성된 의사 GT 이미지 \\(I_{gt}\\)에 적용되는 픽셀 레벨 이미지 재구성 손실로 GS를 계속 최적화한다. 그림 B. 1은 \\(I_{gt}\\)의 생성을 보여준다. 최적화기와 훈련 하이퍼 파라미터는 훈련 반복(3K)을 적게 제외하고 초기 GS 장면(Sec B.1)을 훈련하는 것과 동일하다.\n' +
      '\n' +
      '그림 A.2: 서로 다른 \\(\\gamma\\)로 최적화된 희박 편집 결과를 나타내었다. 우리는 편집된 가우시안 \\(\\mathcal{G}^{\\mathcal{B}}\\)와 현장(\\mathcal{G}\\)의 모든 가우시안(Bottom 행)의 렌더링 이미지를 보여준다. 구체적으로, 글로벌 SDS 손실 \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\)만을 사용하면 편집 가능한 영역에서 명백한 인공물이 발생한다. 다른 \\(\\gamma\\)로 최적화된 거친 편집 결과입니다. 우리는 편집된 가우시안 \\(\\mathcal{G}^{\\mathcal{B}}\\)와 현장(\\mathcal{G}\\)의 모든 가우시안(Bottom 행)의 렌더링 이미지를 보여준다. \\(\\gamma\\) 값이 증가함에 따라 글로벌 SDS 손실 \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\)는 분명히 편집 가능한 영역에서 더 많은 유물을 생성한다. 반대로 \\(\\gamma\\)의 감소는 유물이 적지만 최적화 과정에서 상황 정보가 누락되어 부정확한 객체 배치의 위험과 배경과 새로운 내용 사이의 부자연스러운 색상 불일치를 증가시킨다. 일반적으로 \\(\\gamma=0.5\\)을 설정하는 것은 \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\)와 \\(\\mathcal{L}^{L}_{SDS}\\)를 효과적으로 균형을 이루어 선글라스를 올바르게 배치하면서 노이즈 가우신을 줄인다.\n' +
      '\n' +
      'C 바젤린스 신청서, C 바젤린스 적용\n' +
      '\n' +
      '** 나노-NeRF2NeRF** 나노-NeRF2NeRF("I-N2N")는 감독을 위해 GPT3[4]로 얻은 특수 텍스트 지침에 따라 렌더링된 멀티뷰 이미지를 업데이트하기 위해 구조-픽셀2픽셀[3]을 사용한다. 공식 코드를 사용하여 구현합니다.\n' +
      '\n' +
      'BLIP-2 [26]를 사용하여 기준 이미지에 대한 텍스트 설명을 얻은 다음 I-N2N에서와 같이 적합한 명령어로 변환한다. 다른 모든 설정은 3D 표현 방법(Nerfacto[62]), 구조-픽셀2 픽셀의 버전 및 훈련 하이퍼-파라미터를 포함하여 이들의 공식 코드 내의 디폴트 구성들과 동일하다.\n' +
      '\n' +
      '***DreamEditor** 드림Editor** 드림Editor는 NeuS[54]에서 증류된 명시적 NeuMesh[58] 표현을 채택하고 있으며 지역 편집을 지원하기 위한 주의 기반 국소화 작업을 포함한다. 공식 코드를 사용하여 구현하고 기본 훈련 하이퍼파라미터를 따르죠. 공정 비교를 위해 자동 위치를 보다 정확한 수동으로 선택된 편집 영역으로 교체합니다. 더욱이 드림에디터는 목표 장면을 개인화하기 위해 드림보드를 고용하고 있다. 우리는 하나의 단계에서 서로 다른 두 개의 특수 토큰을 사용하여 편집 장면과 참조 이미지를 동시에 개인화하기 위해 드림보드를 확장한다. 우리는 장면을 편집하기 위해 우리의 방법과 동일한 텍스트 프롬프트를 사용한다.\n' +
      '\n' +
      '평가 기준.\n' +
      '\n' +
      '우리는 3개의 장면과 다른 편집 유형을 포괄하는 10개의 결과에 대한 앞서 언급한 기저부와 방법을 비교한다. 각 결과에 대해 사용자 연구에 사용되는 비디오 데모스를 얻기 위해 동일한 카메라 궤적을 사용하여 모든 방법에 대해 편집된 장면을 렌더링한다. 이 보충에는 모든 영상이 포함되어 있습니다.\n' +
      '\n' +
      '계량 계산을 위해 위의 비디오에서 5프레임마다 1개의 이미지를 추출하고 CLIP 기반 유사성 메트릭(즉, CLIP 텍스트 이미지 지향성 유사성) 및 DINO 유사성을 계산한다. 우리는 Tab에서 그들의 평균 값을 보고한다. 저희 본지의 1.\n' +
      '\n' +
      'CLIP 텍스트 이미지 지향성 유사성.\n' +
      '\n' +
      'CLIP 텍스트-이미지 지향성 유사성 [14]는 두 이미지의 변화(즉, 편집 전)와 두 개의 텍스트 프롬프트(i. 초기 텍스트 설명 \\(t_{o}\\)와 편집 텍스트 프롬프트 \\(t_{e}\\) 사이의 정렬 정도를 평가한다. 수학은 다음과 같이 계산된다.\n' +
      '\n' +
      'E_{T}(t_{e})\\\\[\\Delta I&(i_{e})-E_{I}(i_{e})-E_{I}(i_{o})\\\\ CLIP_{dir}(i_{o})\\\\ CLIP_\\D\\cdot\\Delta I\\cdot\\Delta I\\dot\\delta T})=1)\\\\\\ CLIP_{I}.\n' +
      '\n' +
      'r\\(E_{I}\\)와 \\(E_{T}\\)는 CLIP의 이미지와 텍스트 인코더, \\(i_{o}\\) 및 \\(i_{e}\\)는 원본 및 편집된 장면 이미지, \\(t_{o}\\) 및 \\(t_{e}\\)는 원본 및 편집된 장면 이미지를 설명하는 텍스트이다. 텍스트 설명은 기준 이미지에 대한 BLIP-2(즉, \\(i_{o}\\), \\(i_{o}\\), \\(t_{r}\\)로 얻는다. 그리고 \\(t_{o}\\) 및 \\(t_{r}\\)에 따라 수동으로 \\(t_{e}\\)를 생성한다. 이러한 텍스트 설명은 최적화에 사용되는 것과 반드시 동일하지 않으며 특별한 토큰을 포함하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '### DINO similarity\n' +
      '\n' +
      '[16]에 이어, 우리는 DINO[33] 유사도를 사용하여 편집된 장면과 다른 견해의 참조 이미지 간의 유사성을 측정한다.\n' +
      '\n' +
      '\\[DINO_{sim}=\\frac{E_{D}(i_{e})\\cdot E_{D}(i_{r})}}{|E_{D}(i_{e})||E_{D}(i_{r})}}(i_{r})\\] (D.2)\\] (D.\n' +
      '\n' +
      '여기서 \\(E_{D}\\)는 DINOv2[33], \\(i_{r}\\)의 이미지 인코더이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>