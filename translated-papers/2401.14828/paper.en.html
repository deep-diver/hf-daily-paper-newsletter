<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts\n' +
      '\n' +
      'Jingyu Zhuang\\({}^{1,2}\\)  Di Kang\\({}^{2}\\)  Yan-Pei Cao\\({}^{2}\\)  Guanbin Li\\({}^{1}\\) Liang Lin\\({}^{1}\\)  Ying Shan\\({}^{2}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Sun Yat-sen University  \\({}^{2}\\)Tencent AI Lab\n' +
      '\n' +
      'zhuangjy6@mail2.sysu.edu.cn, di.kang@outlook.com, caoyanpei@gmail.com,\n' +
      '\n' +
      'liguanbin@mail.sysu.edu.cn, linliang@ieee.org, yingsshan@tencent.com\n' +
      '\n' +
      'Corresponding author. This paper is under review.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '_Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIP-Editor, that accepts both text and image prompts and a 3Dbounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIP-Editor utilizes explicit and flexible 3D Gaussian splatting as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively. Refer to our webpage.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Due to the unprecedented photorealistic rendering quality, methods that use radiance field-related representations (e.g. NeRF [31] and 3D Gaussian Splatting [20]) have been more and more popular in 3D reconstruction field [5, 60, 25] and various downstream 3D editing tasks, such as texture editing [41, 56], shape deformation [57, 58], scene decomposition [50], and stylization [53].\n' +
      '\n' +
      'Generative editing, which only requires high-level instructions (e.g. text prompts), emerges as a new approach in complement to previous painting-like and sculpting-like editing approaches [56, 58] that require _extensive_ user interactions. Among these methods, text-driven methods [63, 15] have gained significant attention due to their convenience and have achieved remarkable progress due to the success of large-scale text-to-image (T2I) models.\n' +
      '\n' +
      'However, methods using only text as the condition struggle to precisely generate editing results with the specified appearance at the specified location due to the inherent limitations of the text description. For example, existing text-driven methods usually produce less satisfactory results (Fig. 3) if we want to dress the toy in a special heart-shaped sunglasses or give the male the Joker makeup appeared in the movie _The Dark Knight_. Moreover, it is hard to specify the accurate editing location by text guidance (Fig. 4). These challenges primarily stem from the diverse appearances of the generated objects and the diverse spatial layout of the generated scenes.\n' +
      '\n' +
      'To overcome the challenges above, we present TIP-Editor, which allows the users to intuitively, conveniently, and accurately edit the exiting GS-based radiance fields using both text prompts and image prompts. Our framework achieves such capabilities through two crucial designs. (1) The first one is a novel stepwise 2D personalization strategy that enables precise appearance control (via a reference image) and location control (via a 3D bounding box). Specifically, it contains a scene personalization step, which includes a localization loss to ensure the editing occurs inside the user-defined editing region, and a separate novel content personalization step dedicated to the reference image based on LoRA [18]. (2) The second one is adopting explicit and flexible 3D Gaussian splatting [20] as the 3D representation since it is efficient and, more importantly, highly suitable for local editing.\n' +
      '\n' +
      'We conduct comprehensive evaluations of TIP-Editor across various real-world scenes, including objects, human faces, and outdoor scenes. Our editing results (Fig. 1 and Fig. 7) successfully capture the unique characteristics specified in the reference images. This significantly enhances the controllability of the editing process, presenting considerable practical value. In both qualitative and quantitative comparisons, TIP-Editor consistently demonstrates superior performance in editing quality, visual fidelity, and user satisfaction when compared to existing methods.\n' +
      '\n' +
      'Our contributions can be summarized as follows:\n' +
      '\n' +
      '* We present TIP-Editor, a versatile 3D scene editing framework that allows the users to perform various editing operations (e.g. object insertion, object replacement, re-texturing, and stylization) guided by not only the text prompt but also by a reference image.\n' +
      '* We present a novel stepwise 2D personalization strategy, which features a localization loss in the scene personalization step and a separate novel content personalization step dedicated to the reference image based on LoRA, to enable accurate location and appearance control.\n' +
      '* We adopt 3D Gaussian splatting to represent scenes due to its rendering efficiency and, more importantly, its explicit point data structure, which is very suitable for precise local editing.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      '### Text-guided image generation and editing\n' +
      '\n' +
      'Text-to-image (T2I) diffusion models [39, 42, 45], trained on large-scale paired image-text datasets, have gained significant attention since they can generate diverse and high-quality images that match the complicated text prompt. Instead of directly generating images from scratch, another popular and closely related task is to edit the given image according to the text prompt [1, 3, 10, 17, 19, 30].\n' +
      '\n' +
      'Another popular task is object/concept personalization, which aims at generating images for a specified object/concept defined in the given image collection. Textual Inversion (TI) [13] optimizes special text token(s) in the text embedding space to represent the specified concept. DreamBooth [44] fine-tunes the entire diffusion model with a class-specific prior preservation loss as regularization. In general, DreamBooth generates higher-quality images since it involves a larger amount of updated model parameters (i.e. the whole UNet model). However, all the aforementioned methods do not support generating images containing multiple personalized objects simultaneously.\n' +
      '\n' +
      'Custom Diffusion [24] extends the above task to generate multiple personalized _concepts_ in one image simultaneously. Although separate special text tokens are assigned to each _concept_, the UNet is updated by all _concepts_, resulting in less satisfactory personalization results. Furthermore, it lacks a localization mechanism to specify the interaction between two _concepts_ (Fig. 10). In contrast, we propose a stepwise 2D personalization strategy to learn the existing scene and the new content separately, achieving high-quality and faithful personalization results and being generalizable to sequential editing scenarios.\n' +
      '\n' +
      '### Radiance field-based 3D generation\n' +
      '\n' +
      'The success of T2I diffusion models has largely advanced the development of 3D object/scene generation. One seminal contribution, DreamFusion [35], introduces score distillation sampling (SDS), which distills knowledge from a pre-trained 2D T2I model to _optimize_ a radiance field without the reliance on any 3D data. Most of the subsequent works adopt such an optimization-based pipeline and make further progresses by introducing an extra refinement stage (e.g., Magic3D [27] and DreamBooth3D [38]), or proposing more suitable SDS variants (e.g., VSD [55]), or using more powerful 3D representations [7, 9, 59].\n' +
      '\n' +
      'Furthermore, a body of research [11, 29, 36, 51] endeavors to integrate reference images within the optimization framework. This integration is facilitated by various techniques, including the application of reconstruction loss, employment of predicted depth maps, and the execution of a fine-tuning process. Nevertheless, these methods are constrained to generate a single object from scratch and cannot edit existing 3D scenes.\n' +
      '\n' +
      '### Radiance field-based 3D editing\n' +
      '\n' +
      'Earlier works [52, 53] mainly focus on global style transformation of a given 3D scene, which takes text prompts or reference images as input and usually leverage a CLIP-based similarity measure [37] during optimization. Several studies enable local editing on generic scenes by utilizing 2D image manipulation techniques (e.g. inpainting) [2, 23, 28] to obtain new training images to update the existing radiance field. Some works adopt 3D modeling techniques (e.g. mesh deformation) [57, 58, 61] to propagate the mesh deformation to the underlying radiance field. However, these methods require extensive user interactions.\n' +
      '\n' +
      'Recently, text-driven radiance field editing methods have gained more and more attention for their editing flexibility and accessibility. For example, InstructNeRF2NeRF [15] employs an image-based diffusion model (InstructPix2Pix [3]) to modify the rendered image by the users\' instructions, and subsequently update the 3D radiance field with the modified image. DreamEditor [63] and Vox-E [48] enable better local editing by adopting explicit 3D representations (i.e. mesh and voxel, respectively), where the editing region is automatically determined by the 2D cross-attention maps. GaussianEditor [8, 12] adopts GS as the scene representation and incorporates 3D semantic segmentation [6, 21] to facilitate efficient and precise scene editing. However, these text-driven approaches lack precise control over the specified appearance and position of the editing results.\n' +
      '\n' +
      'A concurrent work, CustomNeRF [16], is most related to our task setting. But CustomNeRF only supports the object replacement task, since it requires an object that can be detected by the segmentation tool [22] existing in the implicit NeRF scene, as the editing target. In contrast, we adopt explicit GS as the 3D representation which facilitates our method to perform more editing tasks (e.g., object insertion and stylization).\n' +
      '\n' +
      '## 3 Background\n' +
      '\n' +
      '### 3D Gaussian Splitting.\n' +
      '\n' +
      '3D Gaussian Splatting (GS) [20] quickly draws tremendous attention due to its high rendering quality and efficiency. GS utilizes a set of point-like anisotropic Gaussians \\(g_{i}\\) to represent the scene: \\(\\mathcal{G}=\\{g_{1},g_{2},...,g_{N}\\}\\). Each \\(g_{i}\\) contains a series of optimizable attributes, including center position \\(\\mu\\in\\mathbb{R}^{3}\\), opacity \\(\\alpha\\in\\mathbb{R}^{1}\\), 3D covariance matrix \\(\\Sigma\\), and color \\(c\\). The differentiable splatting rendering process is outlined as follows:\n' +
      '\n' +
      '\\[\\begin{split} C=\\sum_{i\\in\\mathcal{N}}c_{i}\\sigma_{i}\\prod_{i-1} ^{j=1}(1-\\sigma_{j}),\\\\ \\sigma_{i}=\\alpha_{i}G(x)=\\alpha_{i}e^{-\\frac{1}{2}(x)^{T}\\Sigma ^{-1}(x)}\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'where \\(j\\) indexes the Gaussians in front of \\(g_{i}\\) according to their distances to the optical center in ascending order, \\(\\mathcal{N}\\) is the number of Gaussians that have contributed to the ray, and \\(c_{i}\\), \\(\\alpha_{i}\\), and \\(x_{i}\\) represent the color, density, and distance to the center point of the \\(i\\)-th Gaussian, respectively.\n' +
      '\n' +
      '### Optimizing Radiance Fields with SDS Loss.\n' +
      '\n' +
      'Score distillation sampling (SDS) [35] optimizes a radiance field by distilling the priors from a Text-to-Image (T2I) diffusion model for 3D generation. The pre-trained diffusion model \\(\\phi\\) is used to predict the added noise given a noised image \\(\\hat{I}_{t}\\) and its text condition \\(y\\).\n' +
      '\n' +
      '\\[\\nabla_{\\theta}\\mathcal{L}_{SDS}(\\phi,\\hat{I}=f(\\theta))=\\mathbb{E}_{\\epsilon, t}\\bigg{[}w(t)(\\epsilon_{\\phi}(\\hat{I}_{t};y,t)-\\epsilon)\\frac{\\partial\\hat{I}}{ \\partial\\theta}\\bigg{]}, \\tag{2}\\]where \\(\\theta\\) denotes the parameters of the radiance field, \\(f(\\cdot)\\) is the differentiable image formation process, and \\(w(t)\\) is a predefined weighting function derived from noise level \\(t\\).\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      'Given posed images (i.e., images and their associated camera parameters estimated by COLMAP [46]) of the target scene, our goal is to enable more accurate editing following a hybrid text-image prompt within a user-specified 3D bounding box. We choose 3D Gaussian splatting (GS) [20] to represent the 3D scene since GS is an explicit and highly flexible 3D representation method, which is beneficial for the following editing operations, especially local editing.\n' +
      '\n' +
      'As shown in Fig. 2, TIP-Editor contains three major steps, including 1) a stepwise 2D personalization of the existing scene and the novel content (Sec. 4.1), 2) a coarse 3D editing stage using score distillation sampling (SDS) [35] (Sec. 4.2), and 3) a pixel-level refinement of the 3D scene (Sec. 4.3).\n' +
      '\n' +
      '### Stepwise 2D Personalization\n' +
      '\n' +
      'In general, our stepwise personalization of the pre-trained T2I model (i.e., Stable Diffusion (SD) [42]) is based on DreamBooth [44], but with two significant modifications. These changes are essential to personalize both the existing scene and the novel content in the reference image. First, in the 2D personalization of the existing scene, we propose an attention-based localization loss to enforce the interaction between the existing and the novel content specified by the provided 3D bounding box (e.g., sunglasses on the forehead, see Fig. 4). Note that the reference image is not involved in this step. Second, in the 2D personalization of the novel content, we introduce LoRA layers to better capture the unique characteristics of the specified item in the reference image.\n' +
      '\n' +
      '#### 4.1.1 2D personalization of the existing scene.\n' +
      '\n' +
      'We first personalize the SD to the given scene to facilitate various types of editing of the scene afterward. Specifically, the initial text prompt (e.g. "a toy") is obtained using an image captioning model, BLIP-2 [26]. To enhance the specificity of the scene, we add a special token \\(V_{1}\\) in front of the noun describing the scene, resulting in a scene-specific text prompt (e.g., "a \\(V_{1}\\) toy") as in [63]. The UNet \\(\\epsilon_{\\phi}\\) of the T2I model is fine-tuned with the reconstruction loss and the prior preservation loss [44]. The input of the reconstruction training includes the scene-specific text and a rendered image of the 3D scene from a random view. The input of the prior preservation training includes the initial text and a ran\n' +
      '\n' +
      'Figure 2: **Method overview. TIP-Editor optimizes a 3D scene that is represented as 3D Gaussian splatting (GS) to conform with a given hybrid text-image prompt. The editing process includes three stages: 1) a stepwise 2D personalization strategy, which features a localization loss in the scene personalization step and a separate novel content personalization step dedicated to the reference image based on LoRA (Sec. 4.1); 2) a coarse editing stage using SDS (Sec. 4.2); and 3) a pixel-level texture refinement stage, utilizing carefully generated pseudo-GT image from both the rendered image and the denoised image (Sec. 4.3).**\n' +
      '\n' +
      'dom image generated by SD using the initial text as input (omitted in Fig. 2 to reduce clutter). The above losses are computed as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}_{scene}=&\\mathbb{E}_{z,y, \\epsilon,t}||\\epsilon_{\\phi_{1}}(z_{t},t,p,y)-\\epsilon||_{2}^{2}+\\\\ &\\mathbb{E}_{z^{*},y^{*},\\epsilon,t^{*}}||\\epsilon_{\\phi_{1}}(z_ {t}^{*},t^{*},p^{*},y^{*})-\\epsilon||_{2}^{2}\\end{split} \\tag{3}\\]\n' +
      '\n' +
      'where \\(y\\) denotes the text prompt, \\(t\\) the timestep, \\(z_{t}\\) the noised latent code at \\(t\\)-th timestep extracted from the input scene image, and \\(p\\) the camera pose. Superscript \\(*\\) denotes the corresponding variables used in prior preservation training. Note that we add an additional camera pose \\(p\\) to the condition embeddings in the network to have a better viewpoint control of the generated images from the SD, facilitating the subsequent SDS-based 3D scene optimization. Since randomly generated images for prior preservation training do not have a meaningful "scene pose", we assign a fixed camera pose \\(p^{*}=I_{4}\\) that will never be used for rendering.\n' +
      '\n' +
      'To encourage accurate localization of the target object, we introduce an attention-based localization loss (Fig. 2) during the 2D scene personalization to encourage the SD to generate images containing the required scene-object interaction. This step is particularly important if the target object is specified at a rarely seen location (e.g., sunglasses on the forehead, see Fig. 4). The actual location of the target object generated by SD is extracted from the cross-attention map \\(A_{t}\\) of the object keyword (e.g., "sunglasses") following [17]. The wanted location of the target object (i.e., GT editing region) is obtained by projecting the provided 3D bounding box to the image plane. The loss between the actual and the wanted location is defined as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{loc}=(1-\\underset{s\\in\\mathcal{S}}{max}(A_{t}^{s}))+\\lambda\\sum_ {s\\in\\tilde{\\mathcal{S}}}||A_{t}^{s}||_{2}^{2} \\tag{4}\\]\n' +
      '\n' +
      'where, \\(\\lambda\\) is a weight to balance two terms, \\(\\mathcal{S}\\) the GT editing mask region (projection of the 3D bounding box \\(\\mathcal{B}\\)) and \\(\\tilde{\\mathcal{S}}\\) the otherwise. Intuitively, this loss encourages a high probability inside the editing area and penalizes the presence of the target object outside the editing area. As demonstrated in our ablation study (Fig. 4), this loss is crucial for ensuring precise editing within the specified region.\n' +
      '\n' +
      '#### 4.1.2 2D personalization of the novel content.\n' +
      '\n' +
      'We introduce a dedicated personalization step using LoRA [18] (with the UNet fixed) to better capture the unique characteristics contained in the reference image. This step is essential to reduce the negative influence (e.g. concept forgetting [24]) when learning (personalizing) multiple concepts, resulting in a better representation of both the scene and the novel content. Specifically, we train the additional LoRA layers inserted to the previously personalized and fixed T2I model \\(\\epsilon_{\\phi^{*}}\\). Similar to the last step, we obtain the initial text prompt using BLIP-2 model and insert a special token \\(V_{2}\\) into it, yielding an object-specific text prompt \\(y^{r}\\) of the reference object (e.g. "\\(V_{2}\\) sunglasses"). The new LoRA layers are trained with the following loss function:\n' +
      '\n' +
      '\\[\\mathcal{L}_{ref}=\\mathbb{E}_{z^{r},y^{r},\\epsilon,t}||\\epsilon_{\\phi_{2}}(z_ {t}^{r},t,p^{*},y^{r})-\\epsilon||_{2}^{2} \\tag{5}\\]\n' +
      '\n' +
      'After training, the content of the scene and the reference image are stored in UNet and added LoRA layers, respectively, resulting in largely reduced mutual interference.\n' +
      '\n' +
      '### Coarse Editing via SDS Loss\n' +
      '\n' +
      'We optimize the selected Gaussians \\(\\mathcal{G}^{\\mathcal{B}}\\in\\mathcal{B}\\) (i.e., those inside the bounding box \\(\\mathcal{B}\\)) with SDS loss from the personalized T2I diffusion model \\(\\epsilon_{\\phi_{2}}\\). Specifically, we input randomly rendered images \\(\\hat{I}\\) using sampled camera poses \\(p\\) and the text prompt \\(y^{G}\\) into the T2I model \\(\\epsilon_{\\phi_{2}}\\), and calculate the global scene SDS Loss as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\nabla_{\\mathcal{G}}\\mathcal{L}_{SDS}^{G}(\\phi_{2},f (\\mathcal{G}))=\\\\ \\mathbb{E}_{\\epsilon,t}\\bigg{[}w(t)(\\epsilon_{\\phi_{2}}(z_{t};t, p,y^{G})-\\epsilon)\\frac{\\partial z}{\\partial\\hat{I}}\\frac{\\partial\\hat{I}}{ \\partial\\mathcal{G}}\\bigg{]}\\end{split} \\tag{6}\\]\n' +
      '\n' +
      'where \\(y^{G}\\) is the text prompt including special tokens \\(V_{1},V_{2}\\) and describes our wanted result, \\(f(\\cdot)\\) the GS rendering algorithm.\n' +
      '\n' +
      'It is noteworthy that the selection and update criteria of the Gaussians \\(\\mathcal{G}^{\\mathcal{B}}\\) to be optimized are slightly different for different types of editing tasks. For object insertion, we duplicate all the Gaussians inside the bounding box and exclusively optimize all the attributes of these new Gaussians. For object replacement and re-texturing, all the Gaussians inside the bounding box will be updated. For stylization, optimization is applied to all the Gaussians in the scene. Note that we only update the colors (i.e., the spherical harmonic coefficients) for re-texturing instead of updating all the attributes.\n' +
      '\n' +
      'Since the foreground and background of a GS-based scene are readily separable given the bounding box \\(\\mathcal{G}^{\\mathcal{B}}\\), we introduce another local SDS loss for object-centric editing (e.g., object insertion/replacement) to reduce artifacts as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\nabla_{\\mathcal{G}^{\\mathcal{B}}}\\mathcal{L}_{SDS}^{L}(\\phi_{2},f(\\mathcal{G}^{\\mathcal{B}}))=\\\\ \\mathbb{E}_{\\epsilon,t}\\bigg{[}w(t)(\\epsilon_{\\phi_{2}}(z_{t};t, p,y^{L})-\\epsilon)\\frac{\\partial z}{\\partial\\hat{I}}\\frac{\\partial\\hat{I}}{ \\partial\\mathcal{G}^{\\mathcal{B}}}\\bigg{]}\\end{split} \\tag{7}\\]\n' +
      '\n' +
      'where \\(y^{L}\\) is the text prompt including the special tokens \\(V_{2}\\) and only describes our wanted new object, \\(\\hat{I}\\) the rendered images containing only the foreground object.\n' +
      '\n' +
      'We employ \\(\\mathcal{L}_{SDS}^{G}\\) and \\(\\mathcal{L}_{SDS}^{L}\\) with \\(\\gamma\\) to optimize \\(\\mathcal{G}^{\\mathcal{B}}\\):\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\mathcal{SDS}}=\\gamma\\mathcal{L}_{SDS}^{G}+(1-\\gamma)\\mathcal{L }_{SDS}^{L} \\tag{8}\\]\n' +
      '\n' +
      '### Pixel-Level Image Refinement\n' +
      '\n' +
      'In this stage, we introduce a pixel-level reconstruction loss to effectively enhance the quality of the editing results, since the 3D results directly optimized with SDS loss usually contain artifacts (e.g. green noise on the glasses\' frame, needle-like noise on the hair in Fig. 11).\n' +
      '\n' +
      'The core of this stage is to create a pseudo-GT image \\(I_{gt}\\) to supervise the rendered image \\(I_{c}\\) from the coarse GS. Firstly, we follows SDEdit [30] to add noise on \\(I_{c}\\) to obtain \\(I_{c}\\) and then utilized the personalized T2I model \\(e_{\\phi_{2}}\\) as a denoising network and obtain \\(I_{c}^{d}\\). The denoising process effectively reduces the artifacts in \\(I_{c}\\) (see Fig. D.1 in the supplementary), but also alters the background image. Secondly, we obtain the binary instance mask \\(M^{inst}\\) of the edited object/part by rendering only the editable Gaussians \\(\\mathcal{G}^{\\mathcal{B}}\\) and thresholding its opacity mask. Then, we render a background image \\(I_{bg}\\) with only the fixed Gaussians. Finally, the pseudo-GT image \\(I_{gt}\\) is obtained as:\n' +
      '\n' +
      '\\[I_{gt}=M^{inst}\\odot I_{c}^{d}+(1-M^{inst})\\odot I_{bg} \\tag{9}\\]\n' +
      '\n' +
      'This process ensures that the background image is clean and the same as the original scene while the foreground editable region is enhanced by the T2I model \\(\\epsilon_{\\phi_{2}}\\). Using this pseudo-GT image as pixel-level supervision effectively enhances the resultant texture and reduces floaters (Fig. 11). MSE loss is applied between the rendered image \\(I_{c}\\) and the created pseudo-GT image \\(I_{gt}\\). A flowchart (Fig. B.1) depicting the complete preparation of \\(I_{gt}\\) is included in the supplementary.\n' +
      '\n' +
      'To maintain better coverage, the rendering camera poses cover all elevation and azimuth angles with an interval of \\(30^{\\circ}\\) within a predefined range. To maintain better view-consistency of the denoised images, we set a small noise level (\\(t_{0}=0.05\\), i.e, "intermediate time" in SDEdit). Using such a small noise level effectively enhances fine texture details, removes small artifacts, and does not introduce significant shape and appearance change, maintaining better view consistency for the target editing region.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Implementation Details.** We use the official code to train the original scene GS, with the default hyper-parameters. In the stepwise 2D personalization stage, the scene personalization step consists of 1k iterations, while the novel content personalization contains 500. We set \\(\\lambda=0.1\\) in \\(\\mathcal{L}_{loc}\\). In the coarse editing stage, we adopt the sampling strategy of views from [63]. The size of the rendered images is 512\\(\\times\\)512. Owing to the different complexity of the editing task, this stage requires optimizing for 1K\\(\\sim\\)5K iterations, consuming approximately 5\\(\\sim\\)25 minutes. The refinement stage takes 3K iterations with the supervision of the generated \\(I_{gt}\\), concluding in less than 3 minutes. More implementation details can be found in the supplementary.\n' +
      '\n' +
      '**Dataset.** To comprehensively evaluate our method, We select six representative scenes with different levels of complexity following previous works [8, 15, 63]. These scenes include objects in simple backgrounds, human faces, and complex outdoor scenes. We use scene images and the estimated camera poses extracted from COLMAP [47] to train the original GS. For each editing, a hybrid prompt, consisting of text and a reference image obtained from the Internet, is employed to guide the editing. Additionally, we manually set a 3D bounding box to define the editing region.\n' +
      '\n' +
      '**Baselines.** Due to the lack of dedicated image-based editing baselines, we compare with two state-of-the-art text-based radiance field editing methods, including Instruct-NeRF2NeRF ("I-N2N") [15] and DreamEditor [63]. I-N2N utilizes Instruct-pix2pix [3] to update the rendered multi-view images according to special text instructions. DreamEditor adopts a mesh-based representation and includes an attention-based localization operation to support local editing. For a fair comparison, we replace its automatic localization with a more accurate manual selection. See our supplementary for more implementation details.\n' +
      '\n' +
      '**Evaluation Criteria.** For quantitative evaluation, we adopt CLIP Text-Image directional similarity following [15, 63] to assess the alignment of the editing outcomes with the given text prompt. To evaluate image-image alignment (between the edited scene and the reference image), we follow [16] to calculate the average DINO similarity [33] between the reference image and the rendered multi-view images of the edited 3D scene. Detailed information about these calculations is available in the supplementary.\n' +
      '\n' +
      'Additionally, we conduct a user study and ask the participants (50 in total) to evaluate the results of different methods from two aspects (overall "Quality", and "Alignment" to the reference image). The user study includes 10 questions, each containing the edited results of the two baselines and ours rendered into rotating videos in random order (see our supplementary). The 10 questions have covered various scenes and editing types to better compare the methods under different scenarios.\n' +
      '\n' +
      '### Visual Results of TIP-Editor\n' +
      '\n' +
      'In Fig.1 and Fig. 7, we present qualitative results of TIP-Editor. Video demonstrations are included in the supplementary. Experiments on diverse 3D scenes demonstrate that TIP-Editor effectively executes various editing tasks, including re-texturing, object insertion, object replacement, and stylization, achieving both high-quality results and strictly following the provided text prompt and reference image.\n' +
      '\n' +
      '**Keeping unique characteristics specified by the reference image.** One of the most distinguishable differences between TIP-Editor and previous methods is that TIP-Editor also supports an image prompt, which offers more accurate control and makes it more user-friendly in real applications. Results in Fig. 1&7 demonstrate high consistency between the updated 3D scene and the reference image (e.g. the _styles_ of the sunglasses; the _white_ giraffe; the _virtual ghost_ horse; the joker make-up appeared in movie _The Dark Knight_). Moreover, as depicted in the bottom of Fig. 1, our method can also perform global scene editing, such as transferring the entire scene in the _Modigliani_ style of the reference image.\n' +
      '\n' +
      '**Sequential editing.** TIP-Editor can sequentially edit the initial scene multiple times thanks to the local update of the GS and the stepwise 2D personalization strategy, which effectively reduces the interference between the existing scene and the novel content. Results in Fig.8 demonstrate the sequential editing capability. There is no observable quality degradation after multiple times of editing and no interference between different editing operations.\n' +
      '\n' +
      '**Using generated image as the reference.** In the absence of the reference image, we can generate multiple candidates from a T2I model and let the user choose a satisfactory one. This interaction offers the user more control and makes the final result more predictable. Fig. 9 shows some examples.\n' +
      '\n' +
      '### Comparisons with State-of-the-Art Methods\n' +
      '\n' +
      '**Qualitative comparisons.** Fig.3 shows visual comparisons between our method and the baselines. Since both baselines do not support image prompts as input, they generate an uncontrolled (probably the most common) item belonging to the object category. In contrast, our results consistently maintain the unique characteristics specified in the reference images (i.e., the _heart-shaped_ sunglasses; the _white_ giraffe; the joker from the movie _The Dark Knight_).\n' +
      '\n' +
      'Moreover, Instruct-N2N sometimes misunderstands (row 1) or overlooks (row 2) the keywords, or cannot generate a specified appearance in limited experiments (row 3), probably due to limited supported instructions in Instruct-Pix2Pix. DreamEditor also faces difficulty if the user wants to add a specified sunglasses item (row 1). Additionally, it is difficult for DreamEditor to make obvious shape changes (row 2) to the existing object due to its adoption of a less\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & CLIP\\({}_{dir}\\) & DINO\\({}_{sim}\\) & Vote\\({}_{quality}\\) & Vote\\({}_{alignment}\\) \\\\ \\hline Instruct-N2N & 8.3 & 36.4 & 21.6\\% & 8.8\\% \\\\ DreamEditor & 11.4 & 36.8 & 7.6\\% & 10.0\\% \\\\ Ours & **15.5** & **39.5** & **70.8\\%** & **81.2\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative comparisons. CLIP\\({}_{dir}\\) is the CLIP Text-Image directional similarity. DINO\\({}_{sim}\\) is the DINO similarity.\n' +
      '\n' +
      'Figure 3: Visual comparisons between different methods. Our method produces obviously higher-quality results and _accurately_ follows the reference image input (bottom-right corner in column 1). Instruct-N2N sometimes misunderstands (row 1) or overlooks (row 2) the keywords. DreamEditor faces difficulty in making obvious shape changes (row 2). Both of them do not support image prompts to specify detailed appearance/style, producing less controlled results.\n' +
      '\n' +
      'flexible mesh-based representation (i.e., NeuMesh).\n' +
      '\n' +
      '**Quantitative comparisons.** Tab. 1 shows the results of the CLIP Text-Image directional similarity (CLIP\\({}_{dir}\\)) and DINO similarity (DINO\\({}_{sim}\\)). The results clearly demonstrate the superiority of our method in both metrics, suggesting that the appearance generated by our method aligns better with both the text prompt and the image prompt. A similar conclusion has been drawn according to the user study. Our results surpass the baselines with a substantial margin on both the _quality_ evaluation (\\(70.8\\%\\) votes) and the _alignment_ evaluation (\\(81.2\\%\\) votes).\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**Ablation studies on the stepwise 2D personalization.** We conduct ablative experiments in Fig.4 to demonstrate the benefit of using \\(\\mathcal{L}_{loc}\\) and LoRA Layers in the stepwise 2d personalization. Without \\(\\mathcal{L}_{loc}\\), the fine-tuned T2I model fails to place the sunglasses in the specified region (i.e. on the forehead) due to the bias present in the training data of the original T2I model. Introducing dedicated LoRA layers to personalize the unique features in the reference image results in more faithful output, demonstrating the effectiveness of the proposed stepwise 2D personalization strategy in capturing details in the reference image.\n' +
      '\n' +
      '**Ablation study on different 3D representations.** We test different 3D representations in Fig. 5 while keeping all the other settings the same. Using GS obtains the best editing result while keeping the background unchanged. For Instant-NGP [32], we observe undesired changes in the background since its content in different locations is not independent due to its adoption of a shared MLP decoder and multi-resolution grid.\n' +
      '\n' +
      '**Effectiveness of the pixel-level refinement step.** As in Fig.11, introducing the refinement stage effectively reduces artifacts and enhances the texture, resulting in substantially improved quality.\n' +
      '\n' +
      '**Influence of different \\(\\gamma\\) in coarse editing.** As in Fig.6, both the global and local SDS loss are necessary and our solution achieves the best result. Specifically, only using global SDS loss \\(\\mathcal{L}_{SDS}^{G}\\) results in obvious artifacts in the editable region. Only using local SDS loss \\(\\mathcal{L}_{SDS}^{L}\\) results in inaccurate placement of the object and unnatural color discrepancy between the background and the novel content since the context information is missing during editing.\n' +
      '\n' +
      '## 6 Conclusion and Limitations\n' +
      '\n' +
      'In this paper, our proposed TIP-Editor equips the emerging text-driven 3D editing with an additional image prompt as a complement to the textual description and produces high-quality editing results accurately aligned with the text and image prompts while keeping the background unchanged. TIP-Editor offers significantly enhanced controllability and enables versatile applications, including object insertion, object replacement, re-texturing, and stylization.\n' +
      '\n' +
      'One limitation of TIP-Editor is the coarse bounding box input. Although convenient, it struggles in complex scenes where bounding boxes may include unwanted elements. It would be very beneficial to automatically obtain 3D instance segmentation of the scene. Another limitation is related to geometry extraction since it is hard to extract a smooth and accurate mesh from GS-represented scenes.\n' +
      '\n' +
      'Figure 4: Ablation study on the components proposed in stepwise 2D personalization. We compare the generated images of the personalized T2I model (top row) and the rendered images of the updated 3D scene (bottom row). Removing the localization loss \\(\\mathcal{L}_{loc}\\) fails to place the new object in the specified place. Removing the separate LoRA layers dedicated for the personalization of the reference image produces less similar results (heart-shaped vs. regular round shape).\n' +
      '\n' +
      'Figure 5: Ablation study on different 3D representations to show the advantage of GS for this task. Using Instant-NGP results in a changed background while using NeuMesh cannot produce large enough shape deformation. In contrast, using _explicit_ and _flexible_ GS obtains the best foreground editing result while keeping the background unchanged.\n' +
      '\n' +
      'Figure 6: Ablation study on the influence of global and local SDS (Eq. 8) in the coarse stage. The top row shows the rendering of the editable Gaussians \\(\\mathcal{G}^{B}\\). Only using global SDS \\(\\mathcal{L}_{SDS}^{G}\\) produces low-quality foreground object/part, while only using local SDS \\(\\mathcal{L}_{SDS}^{L}\\) produces unnatural foreground when composited with the existing scene (e.g., color, placement).\n' +
      '\n' +
      'Figure 8: Sequential editing results. We show two rendered images of the 3D scene after every editing step, indicated by the number in the top-left corner. \\(V_{*}\\), \\(V_{**}\\), and \\(V_{**+}\\) represent the special tokens of the scene in different sequences of editing.\n' +
      '\n' +
      'Figure 7: More editing results of the proposed TIP-Editor. Images in the text prompts denote their associated _rare tokens_, which are fixed without optimization.\n' +
      '\n' +
      'Figure 11: Comparison of the coarse editing results and the refinement results. The region indicated by the arrow demonstrates the efficacy of the refinement step in enhancing the quality of the editing results.\n' +
      '\n' +
      'Figure 10: Comparison of different 2D personalization methods. Generated images of the T2I models after personalization (top) and the final updated 3D scene (bottom) are presented. _Text prompt_: “A \\(V_{1}\\) toy wearing \\(V_{2}\\) sunglasses on the forehead”\n' +
      '\n' +
      'Figure 9: Results of using a generated image as the reference. We first generate several candidate images by the diffusion model using text prompts, then we choose one as the reference image for editing.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _CVPR 2022_, pages 18208-18218, 2022.\n' +
      '* [2] Chong Bao, Yinda Zhang, and Banghang et al. Yang. SINE: Semantic-driven image-based nerf editing with prior-guided editing field. In _CVPR 2023_, pages 20919-20929, 2023.\n' +
      '* [3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. InstructPix2Pix: Learning to follow image editing instructions. _arXiv preprint arXiv:2211.09800_, 2022.\n' +
      '* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [5] Raoul Cao Anh-Quan, de Charette. SceneRF: Self-supervised monocular 3d scene reconstruction with radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9387-9398, 2023.\n' +
      '* [6] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment anything in 3D with NeRFs. In _NeurIPS_, 2023.\n' +
      '* [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling geometry and appearance for high-quality text-to-3D content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [8] Yiwen Chen, Zilong Chen, and Chi. eta Zhang. GaussianEditor: Swift and controllable 3D editing with gaussian splatting. _arXiv preprint arXiv:2311.14521_, 2023.\n' +
      '* [9] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. _arXiv preprint arXiv:2309.16585_, 2023.\n' +
      '* [10] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. DiffEdit: Diffusion-based semantic image editing with mask guidance. _arXiv preprint arXiv:2210.11427_, 2022.\n' +
      '* [11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. NeRDi: Single-view NeRF synthesis with language-guided diffusion as general image priors. _arXiv preprint arXiv:2212.03267_, 2022.\n' +
      '* [12] Jiemin Fang, Junjie Wang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. GaussianEditor: Editing 3D gaussians delicately with text instructions. _arXiv preprint arXiv:2311.16037_, 2023.\n' +
      '* [13] Rinon Gal, Yuval Alaulf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [14] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-guided domain adaptation of image generators. _ACM Transactions on Graphics (TOG)_, 41(4):1-13, 2022.\n' +
      '* [15] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-NeRF2NeRF: Editing 3d scenes with instructions. _arXiv preprint arXiv:2303.12789_, 2023.\n' +
      '* [16] Runze He, Shaofei Huang, Xuecheng Nie, and et al. Hui, Tianrui. Customize your NeRF: Adaptive source driven 3d scene editing via local-global iterative training. _arXiv preprint arXiv:2312.01663_, 2023.\n' +
      '* [17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. _arXiv preprint arXiv:2210.09276_, 2022.\n' +
      '* [20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), 2023.\n' +
      '* [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, and et al. Mao, Hanzi. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. _arXiv:2304.02643_, 2023.\n' +
      '* [23] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. _arXiv preprint arXiv:2205.15585_, 2022.\n' +
      '* [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [25] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. Mine: Towards continuous depth mpi with nerf for novel view synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12578-12588, 2021.\n' +
      '* [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstraping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-3D content creation. _arXiv preprint arXiv:2211.10440_, 2022.\n' +
      '* [28] Hao-Kang Liu, I Shen, Bing-Yu Chen, et al. NeRF-In: Freeform NeRF inpainting with RGB-D priors. _arXiv preprint arXiv:2206.04901_, 2022.\n' +
      '* [29] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. RealFusion: 360{\\(\\backslash\\)deg} reconstruction of any object from a single image. _arXiv preprint arXiv:2302.10663_, 2023.\n' +
      '* [30] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [32] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, and et al. Vo, Huy. DINov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* [34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. DreamFusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [36] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3D object generation using both 2D and 3D diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [37] Alec Radford, Jong Wook Kim, Chris Hallacy, and et al. Learning transferable visual models from natural language supervision. In _ICML 2021_, pages 8748-8763, 2021.\n' +
      '* [38] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dream-Booth3D: Subject-driven text-to-3D generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [40] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 12179-12188, 2021.\n' +
      '* [41] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3D shapes. _arXiv preprint arXiv:2302.01721_, 2023.\n' +
      '* [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.\n' +
      '* [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.\n' +
      '* [44] Chitwan Saharia, William Chan, and Saurabh et al. Saxena. Photorealistic text-to-image diffusion models with deep language understanding. _NeurIPS 2022_, 35:36479-36494, 2022.\n' +
      '* [45] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [46] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4104-4113, 2016.\n' +
      '* [47] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-E: Text-guided voxel editing of 3d objects. _arXiv preprint arXiv:2303.12048_, 2023.\n' +
      '* [48] Snois Sixtyboo. gaussian-splitting.\n' +
      '* [49] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerf via rank-residual decomposition. _Advances in Neural Information Processing Systems_, 35:14798-14809, 2022.\n' +
      '* [50] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3D: High-fidelity 3D creation from a single image with diffusion prior. _arXiv preprint arXiv:2303.14184_, 2023.\n' +
      '* [51] Can Wang, Menglei Chai, Mingming He, and et al. CLIP-NeRF: Text-and-image driven manipulation of neural radiance fields. In _CVPR 2022_, pages 3835-3844, 2022.\n' +
      '* [52] Can Wang, Ruixiang Jiang, Menglei Chai, and et al. He, Mingming. NeRF-Art: Text-driven neural radiance fields stylization. _IEEE Transactions on Visualization and Computer Graphics_, 2023.\n' +
      '* [53] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _arXiv preprint arXiv:2106.10689_, 2021.\n' +
      '* [54] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [55] Fanbo Xiang, Zexiang Xu, Milos Hasan, and et al. Neutex: Neural texture mapping for volumetric neural rendering. In _CVPR 2021_, pages 7119-7128, 2021.\n' +
      '* [56] Tianhan Xu and Tatsuya Harada. Deforming radiance fields with cages. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII_, pages 159-175. Springer, 2022.\n' +
      '* [57] Bangbang Yang, Chong Bao, and Junyi et al. Zeng. NeuMesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing. In _ECCV 2022_, pages 597-614. Springer, 2022.\n' +
      '** [59] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-dreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. _arXiv preprint arXiv:2310.08529_, 2023.\n' +
      '* [60] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4578-4587, 2021.\n' +
      '* [61] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, and et al. NeRF-editing: geometry editing of neural radiance fields. In _CVPR 2022_, pages 18353-18364, 2022.\n' +
      '* [62] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. _ACM Transactions on Graphics (ToG)_, 40(6):1-18, 2021.\n' +
      '* [63] Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, and Guanbin Li. DreamEditor: Text-driven 3d scene editing with neural fields. In _SIGGRAPH Asia 2023 Conference Papers_, pages 1-10, 2023.\n' +
      '\n' +
      'In our supplementary material, we provide more results (A), implementation details of our method (B) and the baselines (C), and evaluation details (D)\n' +
      '\n' +
      '## Appendix A More Experimental Results\n' +
      '\n' +
      '**Denoising results.** Fig. A.1 demonstrates the texture details of the rendered image \\(I_{c}\\) from the coarse 3DGS and its corresponding enhanced image after the denoising process.\n' +
      '\n' +
      '**Influence of the weighting parameter in the coarse stage.** Fig. A.2 demonstrates the coarse editing results optimized with different \\(\\gamma\\) in Eq. 8. As the \\(\\gamma\\) value increases (i.e. increasing the importance of the global SDS loss), more and more obvious artifacts appear in the editable region (i.g. the sunglasses). On the other hand, using too small \\(\\gamma\\) values results in fewer artifacts in the sunglasses. However, the final composited image becomes unnatural (e.g. the color gap between the original scene and the new object, the placement of the new object) due to lack of context information during optimization. Generally, setting \\(\\gamma=0.5\\) in Eq. 8 strikes a good balances, effectively reducing the noise in the editable Gaussians \\(\\mathcal{G}^{\\mathcal{B}}\\), correctly placing the sunglasses, and producing harmony colors.\n' +
      '\n' +
      '## Appendix B Implementation Details\n' +
      '\n' +
      'We provide more implementation details that cannot be included in the main paper due to space. All our experiments are conducted on a single NVIDIA Tesla A100 GPU.\n' +
      '\n' +
      '### Optimization of the initial GS scene\n' +
      '\n' +
      'In our experiments, we employ 3D Gaussian Splatting (GS) [20] to represent the 3D scenes. Given images shot from different views of the target scene, we first use COLMAP [46] to estimate their corresponding camera parameters. Then, we train the initial GS scene with the default hyper-parameters in the official 3DGS repository [49].\n' +
      '\n' +
      '### Details of the stepwise 2D personalization strategy\n' +
      '\n' +
      'We use the publicly available Stable Diffusion V2.1 [43] model provided on Hugging Face.\n' +
      '\n' +
      '**The scene personalization step.** We sample scene images with the camera poses covering all elevation and azimuth angles with an interval of \\(30^{\\circ}\\) within a predefined range (varied according to the visual scope of the original scene images). We use BLIP-2 [26] model to obtain the initial scene text prompt, and insert a special token \\(V_{1}\\) in front of the noun to describe this specific scene, resulting in a scene-specific text prompt \\(y\\) during fine-tuning. For the prior preservation loss, we randomly generate 200 images at resolution \\(512\\times 512\\) using the same initial scene text \\(y^{*}\\) as input. We use the AdamW optimizer with \\(lr=5\\cdot 10^{-6}\\), \\(beta1=0.9\\), \\(beta2=0.999\\), \\(weightdecay=10^{-2}\\), and \\(epsilon=10^{-8}\\) for fine-tuning. The scene personalization step consists of 1K iterations (\\(\\sim 10\\) min.) with a batch size of 1. We set \\(\\lambda=0.1\\) in \\(\\mathcal{L}_{loc}\\) (Eq. 4).\n' +
      '\n' +
      '**The novel content personalization step.** We extract the foreground object in the reference image using an off-the-shelf segmentation model [40] and resize it into 512\\(\\times\\)512. We use the AdamW optimizer with \\(lr=10^{-4}\\), \\(beta1=0.9\\), \\(beta2=0.999\\), \\(weightdecay=10^{-2}\\), and \\(epsilon=10^{-8}\\) for fine-tuning. The scene personalization step consists of 500 iterations (\\(\\sim 5\\) min.) with a batch size of 1.\n' +
      '\n' +
      '### Details of the coarse editing stage\n' +
      '\n' +
      'We implement the coarse editing stage in Pytorch [34]. We adopt a sampling strategy of views as in [63], the resolution of the rendered images is 512\\(\\times\\)512. We use the Adam optimizer with \\(beta1=0.9\\), \\(beta2=0.999\\), \\(weightdecay=0\\), and \\(epsilon=10^{-15}\\) during optimization. We adopt a linear decay strategy for the learning rate optimizing center position \\(\\mu\\) from \\(10^{-3}\\) to \\(2\\cdot 10^{-5}\\). As for the other attributes, we use a fixed learning rate the same as the official GS settings (i.e. \\(10^{-1}\\) for opacity \\(\\alpha\\), \\(10^{-2}\\) for color \\(c\\), and \\(5\\cdot 10^{-2}\\) for the 3D covariance matrix \\(\\Sigma\\)). Since the complexity of different scenes varies significantly, the optimization iterations are varied from 1K (\\(\\sim 5\\) min.) to 5K (\\(\\sim 5\\) min.) with batch size of 2. For calculating \\(\\mathcal{L}^{G}_{SDS}\\) and \\(\\mathcal{L}^{L}_{SDS}\\), we set the CFG weight to 10, and utilize simple two-stage annealing of time step \\(t\\): we sample time steps \\(t\\sim\\mathcal{U}(0.02,0.75)\\) and anneal into \\(t\\sim\\mathcal{U}(0.02,0.25)\\).\n' +
      '\n' +
      'Figure A.1: The texture details of the rendered image \\(I_{c}\\) from the coarse 3DGS are further enhanced by the denoising process.\n' +
      '\n' +
      '### Details of the refinement stage\n' +
      '\n' +
      'Using results from the coarse editing stage as initialization, we continue optimizing the GS with pixel-level image reconstruction loss, which is applied to the rendered image and the carefully generated pseudo GT image \\(I_{gt}\\). Fig. B.1 illustrates the generation of \\(I_{gt}\\). The optimizer and training hyper-parameters are the same as those in training the initial GS scene (Sec. B.1) except for fewer training iterations (3K).\n' +
      '\n' +
      'Figure A.2: Coarse editing results optimized with different \\(\\gamma\\). We demonstrate the rendering images of the edited Gaussian \\(\\mathcal{G}^{\\mathcal{B}}\\) (top row) and all Gaussians in the scene \\(\\mathcal{G}\\) (bottom row). Specifically, only using global SDS loss \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\) results in obvious artifacts in the editable region. Coarse editing results optimized with different \\(\\gamma\\). We demonstrate the rendering images of the edited Gaussian \\(\\mathcal{G}^{\\mathcal{B}}\\) (top row) and all Gaussians in the scene \\(\\mathcal{G}\\) (bottom row). As the \\(\\gamma\\) value increases, global SDS loss \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\) obviously results in more artifacts in the editable region. On the contrary, the decreasing of \\(\\gamma\\) results in fewer artifacts, but increases the risk of inaccurate object placement and an unnatural color discrepancy between the background and the novel content since the context information is missing during optimization. Generally, setting \\(\\gamma=0.5\\) effectively balances \\(\\mathcal{L}^{\\mathcal{G}}_{SDS}\\) and \\(\\mathcal{L}^{L}_{SDS}\\), reducing the noise Gaussian while correctly placing the sunglasses.\n' +
      '\n' +
      '## Appendix C Baselines\n' +
      '\n' +
      '**Instruct-NeRF2NeRF.** Instruct-NeRF2NeRF ("I-N2N") utilizes Instruct-pix2pix [3] to update the rendered multi-view images according to special text instructions obtained with GPT3 [4] for supervision. We implement it using its official code.\n' +
      '\n' +
      'We use BLIP-2 [26] to obtain the textual description of our reference image and then convert it into a suitable instruction as in I-N2N. All the other settings are the same as the default configurations in their official code, including the 3D representation method (Nerfacto [62]), the version of Instruct-pix2pix, and the training hyper-parameters.\n' +
      '\n' +
      '**DreamEditor.** DreamEditor adopts the explicit NeuMesh [58] representation distilled from a NeuS [54] and includes an attention-based localization operation to support local editing. We implement it using its official code and follow its default training hyperparameters. For fair comparisons, we replace its automatic localization with a more accurate manually selected editing area. Moreover, DreamEditor employs Dreambooth to personalize the target scene. We extend its Dreambooth process to simultaneously personalize both the editing scene and the reference image using two different special tokens in one step. We use the same text prompts as our method to edit the scenes.\n' +
      '\n' +
      '## Appendix D Evaluation Criteria\n' +
      '\n' +
      'We compare our method with the aforementioned baselines on 10 results, covering 3 scenes and different editing types. For each result, we render the edited scene for all methods using the same camera trajectory to obtain video demos, which are used for the user study. All the videos are included in this supplementary.\n' +
      '\n' +
      'For metric calculation, we extract 1 image every 5 frames from the above videos and calculate the CLIP-based similarity metric (i.e., CLIP Text-Image directional similarity) and DINO similarity. We report their mean values in Tab. 1 in our main paper.\n' +
      '\n' +
      '### CLIP Text-Image directional similarity\n' +
      '\n' +
      'CLIP Text-Image directional similarity [14] evaluates the degree of alignment between the change of two images (i.e. before-after editing) and two text prompts (i.e. initial text description \\(t_{o}\\) and the editing text prompt \\(t_{e}\\)). Mathematically, it is computed as follows:\n' +
      '\n' +
      '\\[\\begin{split}\\Delta T&=E_{T}(t_{e})-E_{T}(t_{o})\\\\ \\Delta I&=E_{I}(i_{e})-E_{I}(i_{o})\\\\ CLIP_{dir}&=1-\\frac{\\Delta I\\cdot\\Delta T}{| \\Delta I||\\Delta T|}\\end{split}\\] (D.1)\n' +
      '\n' +
      'where \\(E_{I}\\) and \\(E_{T}\\) are CLIP\'s image and text encoder, \\(i_{o}\\) and \\(i_{e}\\) are the original and edited scene image, \\(t_{o}\\) and \\(t_{e}\\) are the texts describing the original and edited scene image. The textual descriptions are obtained with BLIP-2 (i.e. \\(t_{o}\\) for \\(i_{o}\\), \\(t_{r}\\) for the reference image). And we manually generate \\(t_{e}\\) according to \\(t_{o}\\) and \\(t_{r}\\). Note that these text descriptions are not necessarily the same as those used for optimization and do not contain special tokens.\n' +
      '\n' +
      '### DINO similarity\n' +
      '\n' +
      'Following [16], we use DINO [33] similarity measure the similarity between the edited scene and the reference image from different views.\n' +
      '\n' +
      '\\[DINO_{sim}=\\frac{E_{D}(i_{e})\\cdot E_{D}(i_{r})}{|E_{D}(i_{e})||E_{D}(i_{r})|}\\] (D.2)\n' +
      '\n' +
      'where \\(E_{D}\\) is the image encoder of DINOv2 [33], and \\(i_{r}\\) the Reference image.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>