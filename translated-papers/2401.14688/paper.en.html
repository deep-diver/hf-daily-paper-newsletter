<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      'Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support\n' +
      '\n' +
      'Xiaojun Wu\\({}^{\\blacktriangledown}\\)\n' +
      '\n' +
      '**Dixiang Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ruyi Gan\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Junyu Lu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ziwei Wu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Renliang Sun\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Jiaxing Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Pingjian Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Yan Song\\({}^{\\blacktriangle}\\)**\n' +
      '\n' +
      '\\({}^{\\blacktriangledown}\\)International Digital Economy Academy \\({}^{\\blacklozenge}\\)South China University of Technology\n' +
      '\n' +
      '\\({}^{\\blacklozenge}\\)University of Science and Technology of China\n' +
      '\n' +
      '{wuxiaojun, zhangdixiang, ganruyi, lujunyu, wuzwei, sunrenliang}@idea.edu.cn zhangjiaxing@idea.edu.cn pjzhang@scut.edu.cn clksong@gmail.com\n' +
      '\n' +
      'Equal Contribution.Project Leader.Corresponding Author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent advancements in text-to-image models have significantly enhanced image generation capabilities, yet a notable gap of open-source models persists in bilingual or Chinese language support. To address this need, we present Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model which is developed by extending the capabilities of CLIP and Stable-Diffusion-XL through a process of bilingual continuous pre-training. This approach includes the efficient expansion of vocabulary by integrating the most frequently used Chinese characters into CLIP\'s tokenizer and embedding layers, coupled with an absolute position encoding expansion. Additionally, we enrich text prompts by large vision-language model, leading to better images captions and possess higher visual quality. These enhancements are subsequently applied to downstream text-to-image models. Our empirical results indicate that the developed CLIP model excels in bilingual image-text retrieval. Furthermore, the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass previous models. This research leads to the development and open-sourcing of the Taiyi-Diffusion-XL model, representing a notable advancement in the field of image generation, particularly for Chinese language applications. The model and demonstration are made publicly available at [https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/](https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/), fostering further research and collaboration in this domain.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recent developments in diffusion models, such as those presented in Stable Diffusion(SD) (Rombach et al., 2022; Podell et al., 2023),DALL-E (Ramesh et al., 2022; Betker et al., 2023), Imagen (Sahara et al., 2022), and Deepfloyd-IF (Shonenkov et al., 2023), have showcased their potential in generating high-quality images from text descriptions. However, it is important to note that the majority of current open-source text-to-image models predominantly support English, with very few offering bilingual support for both Chinese and English. This advancement diverges from the conventional methodology of employing translation software to convert Chinese text into English for subsequent image generation with English-centric models. In particular, works such as Taiyi-Diffusion (Zhang et al., 2022), Pai-Diffusion (Wang et al., 2023) and Alt-Diffusion(Ye et al., 2023) have made significant strides in adapting text-to-image models for Chinese scenarios, demonstrating the feasibility and importance of native language support in such models. Such models adeptly handle language-specific expressions, ensuring the preservation of original meaning and emotional nuances that might otherwise be lost in translation process. These models often obtain Chinese understanding capabilities by replacing multi-language text encoders (Radford et al., 2021; Devlin et al., 2019) and retaining unet(Ronneberger et al., 2015) while this methodology will discard the original English understanding capabilities.\n' +
      '\n' +
      'Building on these advancements, our work, Taiyi-Diffusion-XL(Taiyi-XL), specifically focuses on augmenting these models for Chinese text-to-image generation while preserving the original English ability, addressing the unique linguistic and cultural aspects of the bilingual language. In summary, while translation tools offer a certain level of convenience for cross-language applications, native language support in models, especially for languages like Chinese, provides distinct advantages in terms of comprehension, accuracy, and efficiency. Our contributions are aimed at enhancing these capabilities, thereby offering more effective and inclusive tools for the research community. Our research contributes to this evolving field in three significant ways:\n' +
      '\n' +
      '* _Efficient Algorithms for Bilingual Expansion_: We develop algorithms for expanding vocabulary and position encoding in text-to-image models tailored for bilingual contexts. This advancement facilitates more accurate and culturally tuned image generation.\n' +
      '* _Enrichment of Text Prompts by Large Vision-Language Models_: We employ large vision-language models to enrich text prompts. This approach marks a substantial enhancement in the model\'s ability to interpret and visualize complex textual descriptions.\n' +
      '* _Creation of Bilingual Models_: Utilizing the capabilities of multimodal foundation model, we develop and open-source the text-to-image model, Taiyi-XL, which significantly advances the research and application of bilingual text-to-image models.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      'Our methodology for text-to-image generation, especially with diffusion models, encompasses two primary phases, focusing on dataset preparation and model training.\n' +
      '\n' +
      'Figure 1: An illustration of Taiyi-XL showcasing text-to-image generation results under various styles and prompts.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '\\(T\\) with pure noise, the model iteratively denoises the input, converging to \\(x_{0}\\), the clean image, as described by:\n' +
      '\n' +
      '\\[x_{t-1}=x_{t}-\\epsilon_{\\theta}(x_{t},t,\\tau_{\\theta}(y)),\\quad\\lim_{t\\to 0}x_{t}=x_{0} \\tag{3}\\]\n' +
      '\n' +
      '## 3 Experiment And Analysis\n' +
      '\n' +
      '**Training Settings.** We base our Taiyi-XL model on the pre-trained Stable Diffusion XL (SD-XL) (Podell et al., 2023) checkpoint, providing a strong foundation for image generation. To enhance efficiency and manage GPU memory use, we adopt the BFLOAT16 format. Our training approach involves a learning rate of 1e-5, starting with a warmup phase for stable learning, followed by a cosine decay schedule to fine-tune and refine the model. These strategies are essential for balancing training speed with model performance.\n' +
      '\n' +
      '**Evaluation Protocols.** Our evaluation framework encompasses both machine and human evaluation to provide a comprehensive understanding of the model\'s performance. Machine evaluation metrics include CLIP performance evaluation with image-to-text retrieval and text-to-image retrieval; CLIP Similarity (CLIP Sim), which measures the semantic alignment between the generated images and text descriptions; Inception Score (IS), assessing the quality and diversity of the images; and Frechet Inception Distance (FID), evaluating the distance between the distributions of generated and real images. In the context of human evaluation of text-to-image generation, it is acknowledged that such assessments inherently possess a degree of subjectivity. Consequently, this study primarily employs a case analysis approach to discern and articulate the distinct characteristics of image generation outcomes produced by different models. Rather than providing direct quantitative results that delineate superiority or inferiority among the models, the focus is on a qualitative examination that highlights the unique attributes and performance nuances of each model in image generation tasks.\n' +
      '\n' +
      '**Baselines.** For our comparative analysis, we include several established models as baselines: SD-XL (Podell et al., 2023), Midjourney1, DALL-E \\(3\\)2(Betker et al., 2023), along with other open-sourced models such as our previous work Taiyi-v0.1(Wang et al., 2022), Alt-Diffusion(Ye et al., 2023) and Pai-Diffusion(Wang et al., 2023). DALL-E 3, recognized for its innovative text-to-image capabilities, sets a high standard in generating quality images from text descriptions. SD-XL, a\n' +
      '\n' +
      'Figure 2: Overview of the Taiyi-Diffusion-XL(Taiyi-XL) training process, encompassing data pre-processing, image-text contrastive learning and multi-resolution denoising training process.\n' +
      '\n' +
      'variant of the Stable Diffusion model, excels in complex image synthesis tasks. By comparing Taiyi-XL with these models, we aim to showcase the advancements and efficacy of our approach, particularly in bilingual image generation and fidelity to textual prompts.\n' +
      '\n' +
      '### Machine Evaluation\n' +
      '\n' +
      'CLIP Model Evaluation.Our CLIP model\'s performance is exemplary on both English and Chinese datasets, as evidenced by the zero-shot image-text retrieval results. The original CLIP model (Radford et al., 2021), while establishing a foundational understanding, exhibits modest retrieval rates on the Flickr (Young et al., 2014) and MSCOCO datasets (Lin et al., 2014). This outcome highlights the inherent challenges associated with cross-lingual transfer learning. In contrast, Alt-CLIP (Chen et al., 2022) and our enhanced CLIP model demonstrate significant improvements, with our model achieving the highest recall rates across most evaluation metrics. Particularly noteworthy is our model\'s performance in the Text \\(\\rightarrow\\) Image retrieval task on the Flickr-CN (Young et al., 2014) and MSCOCO-CN datasets (Li et al., 2019), where it attains recall rates of 88.1% and 69.7% at R@1, respectively. These results indicate a robust alignment between textual prompts and visual content, underscoring the effectiveness of our tailored modifications in enhancing CLIP\'s cross-lingual performance. The results, presented in Table 1, demonstrate the potential of specialized models in handling diverse linguistic contexts within multimodal AI applications. The superior performance of our CLIP model, particularly in bilingual contexts, significantly bolsters the capabilities of the Taiyi-XL model. This enhancement allows for a more nuanced understanding of user-input prompts, leading to the generation of images that more accurately reflect the given prompts. The results affirm the importance of developing robust bilingual comprehension capabilities in models for advanced multimodal applications.\n' +
      '\n' +
      'Diffusion Model Evaluation.Based on the data presented in Table 2, a comprehensive analysis of the performance of various models in bilingual image generation tasks reveals significant insights. The evaluation metrics used for this analysis include CLIP Similarity (CLIP Sim), Inception Score (IS), and Frechet Inception Distance (FID), which collectively offer a robust assessment of model performance in terms of image quality, diversity, and alignment with textual descriptions. In the English dataset (COCO), our Taiyi-XL model demonstrates superior performance across all metrics, notably achieving the highest CLIP Sim score, the highest IS, and the most favorable FID. These results indicate that Taiyi-XL not only generates images that are closely aligned with the given text prompts but also ensures high image quality and diversity. The model outperforms other contenders such as Alt-Diffusion, SD-v1.5, and SD-XL, highlighting its effectiveness in handling English language prompts in image generation tasks. Similarly, in the Chinese dataset (COCO-CN), Taiyi-XL again stands out, achieving the best results with a CLIP Sim score, IS and FID. Compared to other models like Taiyi-v0.1, Alt-Diffusion, and Pai-Diffusion, Taiyi-XL exhibits a remarkable ability to generate high-quality images that are well-aligned with Chinese textual descriptions. This performance underscores the model\'s robust bilingual capabilities, making it particularly suitable for applications requiring high-fidelity image generation from diverse linguistic inputs.\n' +
      '\n' +
      'Overall, the results from both datasets affirm the efficacy of the Taiyi-XL model in bilingual image generation tasks. Its ability to consistently produce high-quality, diverse images that accurately reflect the content of both English and Chinese text prompts positions it as a leading solution in\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{6}{c}{Fickr30K} & \\multicolumn{6}{c}{MSCOCO} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ Model & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\\\ \\hline CLIP (Radford et al., 2021) & 85.1 & 97.3 & 99.2 & 65.0 & 87.1 & 92.2 & 56.4 & 79.5 & 86.5 & 36.5 & 61.1 & 71.1 \\\\ AltCLIP (Chen et al., 2022) & 86.0 & 98.0 & 99.1 & 72.5 & 91.6 & 95.4 & 58.6 & 80.6 & 87.8 & 42.9 & 68.0 & 77.4 \\\\ Our-CLIP & **88.4** & **95.8** & **99.9** & **75.7** & **93.8** & **96.9** & **61.2** & **84.8** & **90.3** & **49.2** & **70.3** & **79.6** \\\\ \\hline  & \\multicolumn{6}{c}{Fickr30K-CN} & \\multicolumn{6}{c}{MSCOCO-CN} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ CLIP (Radford et al., 2021) & 2.3 & 8.1 & 12.6 & 0 & 2.4 & 4.0 & 0.6 & 4.1 & 7.1 & 1.8 & 6.7 & 11.9 \\\\ AltCLIP (Chen et al., 2022) & 69.8 & 89.9 & 94.7 & 84.8 & 97.4 & 98.8 & 63.9 & 87.2 & 93.9 & 62.8 & 88.8 & 95.5 \\\\ Our-CLIP & **73.2** & **90.3** & **96.5** & **88.1** & **98.2** & **99.1** & **66.0** & **91.1** & **96.6** & **69.7** & **91.3** & **96.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Zero-shot image-text retrieval results on Flickr30K, MSCOCO, Flickr30K-CN, and MSCOCO-CN datasets. The best results are marked in **bold**.\n' +
      '\n' +
      'the field of multimodal AI applications. The superior performance of Taiyi-XL in these bilingual contexts highlights the potential of specialized models in navigating the complexities of different linguistic environments within image generation tasks.\n' +
      '\n' +
      '### Human Preference Evaluation\n' +
      '\n' +
      'In our comprehensive analysis, as depicted in Figures 3 and 4 showcasing the performance of various models in Chinese and English text-to-image generation, several key observations and conclusions have emerged. The XL versions of the models such as SD-XL and Taiyi-XL exhibit a significant improvement over the 1.5 versions such as SD-v1.5 and Alt-Diffusion, indicating advancements in the scale of model parameters, underlying algorithms and training methodologies. DALL-E 3, while occasionally producing overly vivid colors, stands out for its exceptional prompt-following capability, setting a high benchmark in generating images that closely align with the given textual descriptions. Our model, characterized by a photographic style, closely parallels the performance of Midjourney, particularly in its aesthetic appeal. However, a notable distinction lies in our model\'s enhanced support for bilingual (Chinese and English) text-to-image generation, a feature that is especially valuable in diverse linguistic contexts. This capability underscores the importance of language versatility in the realm of generative models.\n' +
      '\n' +
      'The final conclusion drawn from this analysis is that while our model may not yet match the performance of commercial models, it significantly surpasses current bilingual open-source models. We attribute the gap with commercial models primarily to differences in the quantity, quality, and diversity of the image-text data used for training. Our model has been trained exclusively on copyright-compliant image-text data, highlighting the ongoing challenge of copyright issues in text-to-image and AI-generated content (AIGC) models. This aspect remains a critical factor in the development and refinement of generative models, underscoring the need for access to diverse and high-quality datasets while navigating the complexities of copyright constraints.\n' +
      '\n' +
      'We also evaluated the impact of employing Latent Consistency Models (LCM) (Song et al., 2023; Luo et al., 2023;b) to accelerate the image generation process. A notable observation 5 from these tests is the correlation between the reduction in inference steps and the consequent decline in image quality. Specifically, when the generation is constrained to a single step, the resulting images predominantly exhibit only basic outlines and lack finer details. However, extending the generation process to 8 steps ensures a considerably higher quality of the generated images. This finding suggests that while LCM can effectively speed up the generation process, a balance must be struck between the number of steps and the desired image quality. Maintaining a minimum number of steps, such as eight in our tests, appears to be crucial for preserving a satisfactory level of detail and overall image fidelity.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & CLIP Sim(\\(\\uparrow\\)) & FID(\\(\\downarrow\\)) & IS(\\(\\uparrow\\)) \\\\ \\hline \\multicolumn{4}{c}{English Dataset (COCO)} \\\\ \\hline Alt-Diffusion(Ye et al., 2023) & 0.220 & 27.600 & 31.577 \\\\ SD-v1.5(Rombach et al., 2022) & 0.225 & 25.342 & 32.876 \\\\ SD-XL(Podell et al., 2023) & 0.231 & 23.887 & 33.793 \\\\ Taiyi-XL & **0.254** & **22.543** & **35.465** \\\\ \\hline \\multicolumn{4}{c}{Chinese Dataset (COCO-CN)} \\\\ \\hline Taiyi-v0.1(Wang et al., 2022) & 0.197 & 69.226 & 21.060 \\\\ Alt-Diffusion(Ye et al., 2023) & 0.220 & 68.488 & 22.126 \\\\ Pai-Diffusion(Wang et al., 2023) & 0.196 & 72.572 & 19.145 \\\\ Taiyi-XL & **0.225** & **67.675** & **22.965** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Comparison of different models based on CLIP Sim, IS, and FID across English (COCO) and Chinese (COCO-CN) datasets. The best results are marked in **bold**.\n' +
      '\n' +
      '## 4 Related Work\n' +
      '\n' +
      '### Advancements in Image Generation and Diffusion Models\n' +
      '\n' +
      'Recent years have seen substantial advancements in the field of text-to-image generation. This work diverges from traditional approaches such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Arjovsky et al., 2017), Variational Autoencoders (VAEs) (Kingma and Welling, 2013), Flow-based models (Rezende and Mohamed, 2015), and autoregressive models (Ramesh et al., 2021; Ding et al., 2021; 2022), focusing instead on the more advanced diffusion model. The evolution and refinement of diffusion theory and techniques (Vincent, 2011; Ho et al., 2020; Song et al., 2020; Cao et al., 2022) have positioned the diffusion model as a leading technology in image generation. Noteworthy developments in this area include Dall-E 2 (Ramesh et al., 2022), which utilizes a hierarchical network (Dall-E 2), and a hierarchical network (Dall-E 3). The proposed approach is to use the diffusion model to generate the diffusion model.\n' +
      '\n' +
      'Figure 4: Comparison of Different Models in English Text-to-Image Generation Performance.\n' +
      '\n' +
      'Figure 3: Comparison of Different Models in Chinese Text-to-Image Generation Performance.\n' +
      '\n' +
      'archical approach for generating images based on textual descriptions with CLIP latents. Similarly, Imagen (Saharia et al., 2022) and Deepfloyd-IF (Shonenkov et al., 2023) demonstrate the capability of diffusion models to produce photorealistic images from text, emphasizing deep language understanding. The latent diffusion model (Rombach et al., 2022), encompassing works such as stable-diffusion-v1-5, stable-diffusion-2-1, and stable-diffusion-xl (Podell et al., 2023), represents the forefront of this technology. These models primarily leverage the CLIP text model for textual feature extraction, integrating these features into the latent diffusion process to reduce computational overhead and memory requirements.\n' +
      '\n' +
      '### Text-to-Image Models in Bilingual Context\n' +
      '\n' +
      'In response to the requirements of text-to-image generation in bilingual scenarios, especially in Chinese language, researchers have made significant contributions. initially, the CLIP text encoder is substituted with a Chinese-specific encoder, followed by pre-training for text-image matching on Chinese datasets. Key works in this domain include Taiyi-CLIP (Zhang et al., 2022), Chinese-CLIP (Yang et al., 2022), and Alt-CLIP (Chen et al., 2022). Subsequently, the text encoder in stable diffusion is replaced, and further training on Chinese text-image datasets is conducted to enhance text-to-image generation capabilities. This leads to the development of Chinese versions of diffusion image generation models, such as Taiyi-diffusion (Zhang et al., 2022), Alt-diffusion (Ye et al., 2023) and Pai-diffusion(Wang et al., 2023). However, it is noteworthy that replacing the CLIP text encoder can result in the loss of English language capabilities in the model, and the training process can be resource-intensive.\n' +
      '\n' +
      '### The Role of Text-Image Datasets\n' +
      '\n' +
      'Datasets are pivotal in both text-image matching and text-to-image generation. Traditional image caption datasets like COCO (Lin et al., 2014) and Flickr (Young et al., 2014) in English, and COCO-CN (Li et al., 2019) and Flickr-CN (Li et al., 2016) in Chinese, provide a foundational training base but are limited in size, generally below one million entries. Consequently, web-crawled datasets such as Laion(Schuhmann et al., 2021) (primarily in English) and Wukong(Gu et al., 2022) (primarily in Chinese) have emerged as more critical data sources for training diffusion text-to-image models, boasting sizes of up to 100 million or even 5 billion.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'Our research demonstrates the profound impact of integrating bilingual support into text-to-image models, significantly advancing multimodal research in Chinese contexts. The development of Taiyi-CLIP and Taiyi-XL models, with their expanded vocabulary and position encoding, marks a notable advancement in image-text retrieval and image generation. These models lay the foundation\n' +
      '\n' +
      'Figure 5: Taiyi-XL generation examples with Latent Consistency Model\n' +
      '\n' +
      'for future innovations in bilingual multimodal studies. Additionally, the use of large vision-language models to enrich text prompts has led to more accurate and detailed image generation, aligning closely with user intent. This approach underscores the importance of accurate and complex language understanding in text-to-image generation. As we continue to make our findings and models open-sourced, we invite collaboration and further exploration, contributing to a more inclusive and linguistically diverse future in artificial intelligence research.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks. In _International conference on machine learning_, pp. 214-223. PMLR, 2017.\n' +
      '* Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. _openai cdn.openai.com/papers/dall-e-3.pdf_, 2023.\n' +
      '* Cao et al. (2022) Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li. A survey on generative diffusion model. _arXiv preprint arXiv:2209.02646_, 2022.\n' +
      '* Chen et al. (2023) Yihao Chen, Xianbiao Qi, Jianan Wang, and Lei Zhang. Disco-clip: A distributed contrastive loss for memory efficient clip training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22648-22657, 2023.\n' +
      '* Chen et al. (2022) Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, and Ledell Wu. Alt-clip: Altering the language encoder in clip for extended language capabilities. _arXiv preprint arXiv:2211.06679_, 2022.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4171-4186, 2019.\n' +
      '* Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. _Advances in Neural Information Processing Systems_, 34:19822-19835, 2021.\n' +
      '* Ding et al. (2022) Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. _Advances in Neural Information Processing Systems_, 35:16890-16902, 2022.\n' +
      '* Gan et al. (2023) Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, et al. Ziya2: Data-centric learning is all l lms need. _arXiv preprint arXiv:2311.03301_, 2023.\n' +
      '* Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.\n' +
      '* Gu et al. (2022) Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark. _Advances in Neural Information Processing Systems_, 35:26418-26431, 2022.\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* Li et al. (2016) Xirong Li, Weiyu Lan, Jianfeng Dong, and Hailong Liu. Adding chinese captions to images. In _Proceedings of the 2016 ACM on international conference on multimedia retrieval_, pp. 271-275, 2016.\n' +
      '\n' +
      '* Li et al. (2019) Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-lingual image tagging, captioning, and retrieval. _IEEE Transactions on Multimedia_, 21(9):2347-2360, 2019.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755. Springer, 2014.\n' +
      '* Lu et al. (2023a) Junyu Lu, Ruyi Gan, Dixiang Zhang, Xiaojun Wu, Ziwei Wu, Renliang Sun, Jiaxing Zhang, Pingjian Zhang, and Yan Song. Lyrics: Boosting fine-grained language-vision alignment and comprehension via semantic-aware visual objects. _arXiv preprint arXiv:2312.05278_, 2023a.\n' +
      '* Lu et al. (2023b) Junyu Lu, Dixiang Zhang, Xiaojun Wu, Xinyu Gao, Ruyi Gan, Jiaxing Zhang, Yan Song, and Pingjian Zhang. Ziya-vi: Bilingual large vision-language model via multi-task instruction tuning. _arXiv preprint arXiv:2310.08166_, 2023b.\n' +
      '* Luo et al. (2023a) Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023a.\n' +
      '* Luo et al. (2023b) Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module. _arXiv preprint arXiv:2311.05556_, 2023b.\n' +
      '* Podell et al. (2023) Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pp. 8821-8831. PMLR, 2021.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* Rezende and Mohamed (2015) Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pp. 1530-1538. PMLR, 2015.\n' +
      '* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241. Springer, 2015.\n' +
      '* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Shonenkov et al. (2023) Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva, Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova. If: Title of the repository, 2023.\n' +
      '* Shonenkov et al. (2021)* Song et al. (2020) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\n' +
      '* Vincent (2011) Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.\n' +
      '* Wang et al. (2023) Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, and Jun Huang. Paidiffusion: Constructing and serving a family of open chinese diffusion models for text-to-image synthesis on the cloud. _arXiv preprint arXiv:2309.05534_, 2023.\n' +
      '* Wang et al. (2022) Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo, Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen, Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, Chongpei Chen, Ruyi Gan, and Jiaxing Zhang. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. _CoRR_, abs/2209.02970, 2022.\n' +
      '* Yang et al. (2022) An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. _arXiv preprint arXiv:2211.01335_, 2022.\n' +
      '* Ye et al. (2023) Fulong Ye, Guangyi Liu, Xinya Wu, and Ledell Yu Wu. Altdiffusion: A multilingual text-to-image diffusion model. _ArXiv_, abs/2308.09991, 2023. URL [https://api.semanticscholar.org/CorpusID:261048720](https://api.semanticscholar.org/CorpusID:261048720).\n' +
      '* Young et al. (2014) Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.\n' +
      '* Zhang et al. (2022) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, et al. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. _arXiv preprint arXiv:2209.02970_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>