<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '대발전 비전-언어 모델 지원-XL\n' +
      '\n' +
      'Xiaojun Wu\\({}^{\\blacktriangledown}\\)\n' +
      '\n' +
      '**Dixiang Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ruyi Gan\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Junyu Lu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Ziwei Wu\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Renliang Sun\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Jiaxing Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Pingjian Zhang\\({}^{\\blacktriangledown}\\)**\n' +
      '\n' +
      '**Yan Song\\({}^{\\blacktriangle}\\)**\n' +
      '\n' +
      '국제 디지털 경제 아카데미({}^{\\blacklozDeepown}\\)\n' +
      '\n' +
      '대중국 과학 기술({}^{\\blacklozenge}\\)\n' +
      '\n' +
      '장디앙, 장디앙, 가누이, 루준유, 우즈웨이, 선렌지앙,@idea.cn zhangjiaxing@scut}@ideu.cn zhangjiaxing@scut.\n' +
      '\n' +
      '동일한 기여도 프로젝트 지도자 응답 권한이 있습니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 텍스트 대 이미지 모델의 발전은 이미지 생성 능력이 크게 향상되었지만 이중 언어 또는 중국어 지원에서 개방된 소스 모델의 현저한 격차는 지속된다. 이러한 필요성을 해결하기 위해 이중언어 연속 사전 훈련 과정을 통해 CLIP와 Stable-Diffusion-XL의 능력을 확장하여 개발된 새로운 중국어 및 영어 이중언어 텍스트 대 이미지 모델 타이이-Diffusion-XL을 제시한다. 이 접근법은 가장 자주 사용되는 한자를 CLIP 토큰화기와 임베딩 층에 통합하여 어휘의 효율적인 확장을 포함하며, 절대 위치 인코딩 확장과 결합한다. 또한 대형 비전 언어 모델에 의해 텍스트 프롬프트를 풍부하게 하여 더 나은 이미지를 사로잡고 더 높은 시각적 품질을 가지고 있습니다. 이러한 향상은 이후 하류 텍스트 대 이미지 모델에 적용된다. 우리의 경험적 결과는 개발된 CLIP 모델이 이중언어 이미지-텍스트 검색에서 탁월함을 나타낸다. 또한 타이이-디확산-XL의 이중언어 이미지 생성 능력은 이전 모델을 능가한다. 본 연구는 타이이-디확산-XL 모형의 개발 및 개방으로 이어지며, 특히 중국어 응용 분야에 대한 이미지 생성 분야의 주목할 만한 발전을 나타낸다. 모델 및 시연은 [https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/](https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/)에서 공개 사용 가능하며, 이 도메인의 추가 연구와 협력을 촉진한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '스테이블 디퓨전(SD)(Rombach et al., 2022; Podell et al., 2023), Podell et al.,DALL-E(Ramesh et al., 2022; Betker et al., 2023), 임젠(Sahara et al., 2022), 딥플로이드-IF(Shonenkov et al., 2023)에서 제시된 것과 같은 확산 모델의 최근 발전은 텍스트 설명으로부터 고품질 이미지를 생성하는 잠재력을 보여주었다. 그러나 현재 오픈소스 텍스트 대 이미지 모델의 대부분은 주로 영어를 지원하며 중국과 영어 모두에 이중언어 지원을 제공하는 것은 거의 없다는 점에 유의하는 것이 중요하다. 이러한 발전은 번역 소프트웨어를 사용하여 중국 텍스트를 영어 중심 모델로 후속 이미지 생성을 위해 영어로 변환한다는 기존의 방법론에서 파생된다. 특히 타이이-디퓨전(Zhang et al, 2022), 파이-디퓨전(Wang et al., 2023), 알트-디퓨전(예 et al., 2023)과 같은 작품은 중국 시나리오에 대한 텍스트 대 이미지 모델을 적응시키는 데 상당한 지평을 만들어 이러한 모델에서 언어 지원의 타당성과 중요성을 보여주었다. 이러한 모델은 언어 특이적 표현을 잘 처리함으로써 번역 과정에서 그렇지 않으면 손실될 수 있는 원래의 의미와 정서적 뉘앙스의 보존을 보장한다. 이 모델은 종종 다중 언어 텍스트 인코더(라드포드 등 2021, Devlin et al., 2019)를 교체하고 언넷(론네버거 등 2015)을 유지함으로써 중국어의 이해 능력을 얻는 반면 이 방법론은 원 영어 이해 능력을 폐기할 것이다.\n' +
      '\n' +
      '이러한 진보에 대한 구축, 우리의 작품 타이이-디확산-XL(타이이-XL)은 특히 이 언어 언어의 독특한 언어 및 문화적 측면을 다루는 독창적인 영어 능력을 보존하면서 중국 텍스트 대 이미지 생성을 위한 이러한 모델을 증가시키는 데 중점을 둔다. 요약하면, 번역 도구는 교차 언어 응용에 대한 일정 수준의 편의를 제공하는 반면, 모델, 특히 중국어와 같은 언어에 대한 네이티브 언어 지원은 이해력, 정확성 및 효율성의 관점에서 뚜렷한 이점을 제공한다. 우리의 기여는 이러한 능력을 향상시키기 위한 것이며, 따라서 연구 커뮤니티에 더 효과적이고 포괄적인 도구를 제공한다. 우리의 연구는 이 진화하는 분야에 세 가지 중요한 방식으로 기여한다.\n' +
      '\n' +
      'B언어적 확장_에 대한* _효율적인 알고리즘: 이중언어 맥락에 맞춘 텍스트 대 이미지 모델에서 어휘 및 위치 인코딩을 확장하기 위한 알고리즘을 개발한다. 이러한 발전은 보다 정확하고 문화적 튜닝된 이미지 생성을 용이하게 한다.\n' +
      '대형 비전-언어 모델_에 의한 텍스트 프롭트의* _ 농축: 텍스트 프롬프트를 풍부하게 하기 위해 대형 비전-언어 모델을 사용한다. 이 접근법은 복잡한 텍스트 설명을 해석하고 시각화하는 모델의 능력에 상당한 향상을 나타낸다.\n' +
      '다중 기반 모델의 능력을 활용하여 이중언어 텍스트 대 이미지 모델의 연구와 적용을 크게 발전시키는 텍스트 대 이미지 모델 타이이-XL을 개발하고 개방한다.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '특히 확산 모델을 가진 텍스트 대 이미지 생성을 위한 우리의 방법론은 데이터세트 준비 및 모델 교육을 중심으로 두 가지 1차 단계를 포함한다.\n' +
      '\n' +
      '그림 1: 타이이-XL의 일러스트레이션은 다양한 스타일과 프롬프트에서 텍스트 대 이미지 생성 결과를 보여준다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '순수한 잡음이 있는\\(T\\)는 모델이 반복적으로 입력을 변성시켜 전술된 것처럼 깨끗한 이미지인 \\(x_{0}\\)로 수렴한다.\n' +
      '\n' +
      '<x_{t-1>} (x_{t},t,\\t_{\\ta}) (t,\\tau_{\\ta}.\n' +
      '\n' +
      '3개의 실험.\n' +
      '\n' +
      '*** 훈련 설정** 우리는 사전 훈련된 Stable Diffusion XL(SD-XL)(Podell et al., 2023) 체크포인트에서 타이이-XL 모델을 베이스로 하여 이미지 생성의 강력한 기반을 제공한다. 효율성을 높이고 GPU 메모리 사용을 관리하기 위해 BFLOAT16 형식을 채택합니다. 우리의 훈련 접근법은 안정 학습을 위한 평가 단계로 시작하여 코사인 붕괴 일정이 미세 조정되고 모델을 정제하는 1e-5의 학습 속도를 포함한다. 이러한 전략은 훈련 속도와 모델 성능을 균형을 맞추는 데 필수적이다.\n' +
      '\n' +
      '*** 평가 개신도** 우리 평가 프레임워크는 모델의 성능에 대한 포괄적인 이해를 제공하기 위해 기계 및 인간 평가를 모두 포함한다. 기계 평가 메트릭에는 이미지 대 텍스트 검색 및 텍스트 대 이미지 검색을 사용한 CLIP 성능 평가; 생성된 이미지와 텍스트 설명 사이의 의미 정렬을 측정하는 CLIP 유사성(CLIP 심), 이미지들의 품질과 다양성을 평가하는 인셉션 스코어(IS), 및 프로체트 인셉션 거리(FID)가 포함되며, 생성된 이미지와 실제 이미지의 분포 사이의 거리를 평가한다. 텍스트 대 이미지 생성의 인간 평가의 맥락에서 이러한 평가는 본질적으로 주관성의 정도를 가지고 있음을 인정한다. 결과적으로, 본 연구는 주로 다른 모델에 의해 생성된 이미지 생성 결과의 뚜렷한 특성을 식별하고 표현하기 위해 사례 분석 접근법을 사용한다. 모델들 간의 우열 또는 열위를 묘사하는 직접적인 양적 결과를 제공하기보다는 이미지 생성 작업에서 각 모델의 고유한 속성과 성능 뉘앙스를 부각시키는 질적 검사에 초점을 맞추고 있다.\n' +
      '\n' +
      '비교 분석을 위해 SD-XL(Podell et al., 2023), 미드자르니1, DALL-E\\(3\\)2(Betker et al., 2023), 이전 작업 타이이-v0.1(Wang et al., 2022), 알트-디퓨전(예 et al., 2023), Pai-Diffusion(Wang et al.,Pai-Diffusion) 및 Pai-Diffusion(Wang et al., 2023)과 같은 다른 개방형 모델과 함께 여러 확립된 모델을 기저 모델로 포함한다. 혁신적인 텍스트 대 이미지 능력을 인정받은 DALL-E 3은 텍스트 설명에서 품질 이미지를 생성하는 데 높은 기준을 설정한다. SD-XL, 즉SD-XL, 이는SD-XL, 즉SD-XL이다.\n' +
      '\n' +
      '그림 2: 타이이-디확산-XL(Taiyi-XL) 훈련 과정의 오버뷰는 데이터 전처리, 이미지-텍스트 대비 학습 및 다중 해상도 데노징 학습 과정을 포함한다.\n' +
      '\n' +
      '케이블 디퓨전 모델의 변인은 복잡한 이미지 합성 작업에서 탁월하다. 타이이-XL을 이러한 모델과 비교하여 특히 이중 언어 이미지 생성 및 텍스트 프롬프트에 대한 충실도에서 접근법의 발전 및 효능을 보여주는 것을 목표로 한다.\n' +
      '\n' +
      '### Machine Evaluation\n' +
      '\n' +
      'CLIP 모델 평가는 제로샷 이미지-텍스트 검색 결과에 의해 입증된 바와 같이, 우리의 CLIP 모델의 성능은 영어 및 중국어 데이터 세트 모두에서 모범적이다. 원래의 CLIP 모델(라드포드 등 2021)은 발견된 이해를 확립하면서 Flickr(젊은 et al, 2014), MSCOCO 데이터셋(Lin et al, 2014)에 대한 적당한 회수율을 나타낸다. 이 결과는 교차 수정 전이 학습과 관련된 고유한 과제를 강조한다. 대조적으로, Alt-CLIP(Chen et al., 2022) 및 강화된 CLIP 모델은 대부분의 평가 메트릭에서 가장 높은 회수율을 달성하면서 상당한 개선을 보여준다. 특히 주목할 점은 R@1에서 각각 88.1%와 69.7%의 리콜률을 차지하는 Flickr-CN(젊은 et al., 2014), MSCOCO-CN 데이터셋(Li et al., 2019)에 대한 Text \\(\\rightarrow\\) 이미지 검색 과제에 대한 우리의 모델의 성능이다. 이러한 결과는 텍스트 프롬프트와 시각적 콘텐츠 사이의 강력한 정렬을 나타내며 CLIP의 교차 수정 성능을 향상시키는 데 맞춤형 변화의 효과를 강조한다. 표 1에 제시된 결과는 다중 모드 AI 애플리케이션 내에서 다양한 언어적 컨텍스트를 처리하는 데 특화된 모델의 잠재력을 보여준다. 특히 이중언어 맥락에서 CLIP 모델의 우수한 성능은 타이이-XL 모델의 능력을 크게 약화시킨다. 이러한 향상은 사용자 입력 프롬프트에 대한 보다 미묘한 이해를 허용하여 주어진 프롬프트를 보다 정확하게 반영하는 이미지 생성을 유도한다. 결과는 고급 복합 애플리케이션을 위한 모델에서 강력한 이중언어 이해 능력을 개발하는 것의 중요성을 긍정한다.\n' +
      '\n' +
      '<표 2>에 제시된 자료를 바탕으로 이중언어 영상 생성 과제에서 다양한 모델의 성능을 종합적으로 분석한 결과 유의미한 통찰력이 나타난다. 이 분석에 사용된 평가 메트릭에는 CLIP 유사성(CLIP 심), 인셉션 스코어(IS), 프로체트 인셉션 거리(FID)가 포함되며, 이는 이미지 품질, 다양성 및 텍스트 설명과의 정렬 측면에서 모델 성능에 대한 강력한 평가를 일괄적으로 제공한다. 영어 데이터셋(COCO)에서 우리의 타이이-XL 모델은 모든 메트릭에 걸쳐 우수한 성능을 보여주며 특히 가장 높은 CLIP 심 점수인 가장 높은 IS와 가장 유리한 FID를 달성했다. 이러한 결과는 타이이-XL이 주어진 텍스트 프롬프트와 밀접하게 정렬된 이미지를 생성할 뿐만 아니라 높은 화질과 다양성을 보장한다는 것을 나타낸다. 이 모델은 Alt-Diffusion, SD-v1.5 및 SD-XL과 같은 다른 경쟁자를 능가하여 이미지 생성 작업에서 영어 처리 효과를 강조한다. 마찬가지로 중국 데이터셋(COCO-CN)에서는 타이이-XL이 다시 돋보이며 CLIP 심 스코어 IS와 FID로 최고의 성과를 거뒀다. 타이이-v0.1, Alt-Diffusion 및 Pai-Diffusion과 같은 다른 모델에 비해 타이이-XL은 중국 텍스트 설명과 잘 정렬된 고품질 이미지를 생성하는 놀라운 능력을 나타낸다. 이 성능은 모델의 강력한 이중언어 능력을 강조하여 다양한 언어적 입력들로부터 고 충실도 이미지 생성이 필요한 응용 분야에 특히 적합하다.\n' +
      '\n' +
      '전반적으로 두 데이터셋의 결과는 이중언어 이미지 생성 작업에서 타이이-XL 모델의 효능을 확인한다. 영어와 중국 텍스트의 내용을 정확하게 반영하는 고품질의 다양한 이미지를 지속적으로 생산하는 능력이 이를 선도적인 해결책으로 배치할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{6}{c}{Fickr30K} & \\multicolumn{6}{c}{MSCOCO} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ Model & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\\\ \\hline CLIP (Radford et al., 2021) & 85.1 & 97.3 & 99.2 & 65.0 & 87.1 & 92.2 & 56.4 & 79.5 & 86.5 & 36.5 & 61.1 & 71.1 \\\\ AltCLIP (Chen et al., 2022) & 86.0 & 98.0 & 99.1 & 72.5 & 91.6 & 95.4 & 58.6 & 80.6 & 87.8 & 42.9 & 68.0 & 77.4 \\\\ Our-CLIP & **88.4** & **95.8** & **99.9** & **75.7** & **93.8** & **96.9** & **61.2** & **84.8** & **90.3** & **49.2** & **70.3** & **79.6** \\\\ \\hline  & \\multicolumn{6}{c}{Fickr30K-CN} & \\multicolumn{6}{c}{MSCOCO-CN} \\\\  & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} & \\multicolumn{3}{c}{Image \\(\\rightarrow\\) Text} & \\multicolumn{3}{c}{Text \\(\\rightarrow\\) Image} \\\\ CLIP (Radford et al., 2021) & 2.3 & 8.1 & 12.6 & 0 & 2.4 & 4.0 & 0.6 & 4.1 & 7.1 & 1.8 & 6.7 & 11.9 \\\\ AltCLIP (Chen et al., 2022) & 69.8 & 89.9 & 94.7 & 84.8 & 97.4 & 98.8 & 63.9 & 87.2 & 93.9 & 62.8 & 88.8 & 95.5 \\\\ Our-CLIP & **73.2** & **90.3** & **96.5** & **88.1** & **98.2** & **99.1** & **66.0** & **91.1** & **96.6** & **69.7** & **91.3** & **96.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1:제로 샷 이미지-텍스트 검색 결과는 Flickr30K, MSCOCO, Flickr30K-CN 및 MSCOCO-CN 데이터셋에 대한 것이다. 가장 좋은 결과는 **bold***로 표시됩니다.\n' +
      '\n' +
      '복합 AI 응용 분야입니다. 이러한 이중언어 맥락에서 타이이-XL의 우수한 성능은 이미지 생성 작업 내에서 상이한 언어 환경의 복잡성을 탐색하는 데 특화된 모델의 잠재력을 강조한다.\n' +
      '\n' +
      '인간이.\n' +
      '\n' +
      '우리의 종합 분석에서 그림 3과 그림 4에 묘사된 바와 같이 중국어와 영어 텍스트 대 이미지 생성에서 다양한 모델의 성능을 보여주듯이 몇 가지 주요 관찰과 결론이 등장했다. SD-XL 및 타이이-XL과 같은 모델의 XL 버전은 SD-v1.5 및 Alt-D확산과 같은 1.5 버전에 걸쳐 상당한 개선을 나타내며, 이는 모델 파라미터의 규모, 기본 알고리즘 및 훈련 방법론의 발전을 나타낸다. DALL-E 3은 간혹 지나치게 생생한 색상을 생성하는 반면, 예외적인 신속한 제거 능력이 돋보이며 주어진 텍스트 설명과 밀접하게 일치하는 이미지를 생성하는 데 높은 벤치마크를 설정합니다. 광학적 스타일로 특징지어지는 우리의 모델은 특히 미학적 호소에서 미도호의 성능과 밀접하게 유사하다. 그러나 주목할만한 차이점은 특히 다양한 언어적 맥락에서 가치가 있는 특징인 이중언어(중국어와 영어) 텍스트 대 이미지 생성에 대한 우리 모델의 강화된 지원에 있다. 이 능력은 생성 모델의 영역에서 언어 다용성의 중요성을 강조한다.\n' +
      '\n' +
      '이 분석에서 도출한 최종 결론은 우리의 모델이 아직 상업적 모델의 성능과 일치하지 않을 수 있지만 현재 이중언어 개방형 소스 모델을 크게 능가한다는 것이다. 우리는 상용 모델과의 격차를 주로 학습에 사용된 이미지 텍스트 데이터의 수량, 품질 및 다양성의 차이에 속한다. 우리의 모델은 저작권 준수 이미지-텍스트 데이터에서만 교육되어 텍스트-이미지 및 AI 생성 콘텐츠(AIGC) 모델에서 저작권 문제의 지속적인 도전을 강조한다. 이러한 측면은 생성 모델의 개발 및 개선에서 중요한 요소로 남아 있으며, 저작권 제약의 복잡성을 탐색하면서 다양하고 고품질 데이터 세트에 대한 접근 필요성을 강조한다.\n' +
      '\n' +
      '또한 이미지 생성 과정을 가속화하기 위해 라트 컨시스트리티 모델(LCM)(송 et al., 2023; 루에 등, 2023;b)을 사용하는 영향을 평가했다. 이러한 테스트에서 주목할 만한 관찰 5는 추론 단계의 감소와 그에 따른 화질의 감소 사이의 상관관계이다. 구체적으로, 생성이 단일 단계로 제약될 때, 결과 이미지는 주로 기본 윤곽만 나타내고 더 미세한 세부 사항이 부족하다. 그러나 생성 프로세스를 8단계로 확장하면 생성된 이미지의 품질이 상당히 높을 수 있다. 이 발견은 LCM이 생성 과정을 효과적으로 가속화할 수 있지만 단계 수와 원하는 화질 사이에 균형을 맞추어야 함을 시사한다. 우리의 테스트에서 8개의 단계와 같은 최소 수의 단계를 유지하는 것은 만족스러운 수준의 세부 및 전반적인 이미지 충실도를 보존하는 데 중요한 것으로 판단된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & CLIP Sim(\\(\\uparrow\\)) & FID(\\(\\downarrow\\)) & IS(\\(\\uparrow\\)) \\\\ \\hline \\multicolumn{4}{c}{English Dataset (COCO)} \\\\ \\hline Alt-Diffusion(Ye et al., 2023) & 0.220 & 27.600 & 31.577 \\\\ SD-v1.5(Rombach et al., 2022) & 0.225 & 25.342 & 32.876 \\\\ SD-XL(Podell et al., 2023) & 0.231 & 23.887 & 33.793 \\\\ Taiyi-XL & **0.254** & **22.543** & **35.465** \\\\ \\hline \\multicolumn{4}{c}{Chinese Dataset (COCO-CN)} \\\\ \\hline Taiyi-v0.1(Wang et al., 2022) & 0.197 & 69.226 & 21.060 \\\\ Alt-Diffusion(Ye et al., 2023) & 0.220 & 68.488 & 22.126 \\\\ Pai-Diffusion(Wang et al., 2023) & 0.196 & 72.572 & 19.145 \\\\ Taiyi-XL & **0.225** & **67.675** & **22.965** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2는 영어(COCO)와 중국(COCO-CN) 데이터 세트를 가로질러 CLIP 심, IS, FID를 기반으로 한 다양한 모델의 비교이다. 가장 좋은 결과는 **bold***로 표시됩니다.\n' +
      '\n' +
      '4번 관련 작업.\n' +
      '\n' +
      '임세대 및 세확산 모델 제품 개발\n' +
      '\n' +
      '최근 몇 년 동안 텍스트 대 이미지 생성 분야에서 상당한 발전이 나타났다. 이 작업은 더 발전된 확산 모델에 대신 초점을 맞춘 유전적 적대적 네트웍스(GAN)(2014; 아르조프스키 등), 비리티브 오토인코더(VAE), 플로우 기반 모델(레젠드 및 모하메드, 2015), 자기회귀 모델(레즈 et al., 2021; Ding et al., 2022)과 같은 전통적인 접근 방식에서 분기된다. 확산 이론과 기법의 진화 및 개선(빈센트, 2011; 호 등, 2020; 송 등, 2020; 카오 등 2022)은 이미지 생성의 선도 기술로 확산 모델을 위치시켰다. 이 영역의 주목할 만한 발전에는 계층적 네트워크(Dall-E 2)를 활용하는 Dall-E 2(Ramesh et al., 2022), 계층적 네트워크(Dall-E 3) 등이 있다. 제안된 접근법은 확산 모델을 사용하여 확산 모델을 생성하는 것이다.\n' +
      '\n' +
      '그림 4: 영어 교과서-이미지 세대 성과에서의 차별 모형 비교이다.\n' +
      '\n' +
      '그림 3: 중국 교과서-이미지 세대 성과에서의 차별 모형 비교이다.\n' +
      '\n' +
      'CLIP 래치와의 텍스트 설명을 기반으로 이미지를 생성하기 위한 어휘적 접근 방법이 있다. 유사하게, 임아젠(사아리아 et al., 2022)과 딥플로이드-IF(쇼네노코프 et al., 2023)는 확산 모델이 텍스트에서 광학적 이미지를 생성하는 능력을 발휘하여 깊은 언어 이해를 강조한다. 안정확산-v1-5, 안정확산-2-1, 안정확산-xl(Podell et al., 2023)과 같은 작품을 포함하는 잠재확산 모델(Rombach et al., 2022)은 이 기술의 최전선을 나타낸다. 이들 모델은 주로 텍스트 특징 추출을 위한 CLIP 텍스트 모델을 레버리지하여 이러한 특징을 잠재 확산 과정에 통합하여 계산 오버헤드 및 메모리 요구 사항을 감소시킨다.\n' +
      '\n' +
      '아이언얼 콘텍스트에서 이미지 모델.\n' +
      '\n' +
      '이중언어 시나리오, 특히 중국어로 텍스트 대 이미지 생성의 요구 사항에 대응하여 연구자들은 상당한 기여를 하였다. 즉, CLIP 텍스트 인코더는 중국 고유의 인코더로 대체되고, 이어서 중국 데이터셋에서 텍스트 이미지 매칭을 위한 사전 학습이 진행된다. 이 영역의 주요 작품으로는 타이이-CLIP(Zhang et al, 2022), 중국-CLIP(양 et al., 2022), Alt-CLIP(Chen et al., 2022) 등이 있다. 이어서 안정적인 확산의 텍스트 인코더를 대체하고, 텍스트 대 이미지 생성 능력을 향상시키기 위해 중국 텍스트 이미지 데이터셋에 대한 추가 훈련을 실시한다. 이는 타이이확산(Zhang et al, 2022), 알트확산(예 et al, 2023), 피-확산(왕 et al., 2023) 등 중국 버전의 확산 영상 생성 모델이 발달하게 된다. 그러나 CLIP 텍스트 인코더를 대체하면 모델에서 영어 역량의 손실이 발생할 수 있고, 훈련 과정은 자원 집약적일 수 있다는 점은 주목할 만하다.\n' +
      '\n' +
      '문자 이미지 다타자세트입니다.\n' +
      '\n' +
      '데이터베이스는 텍스트 이미지 매칭과 텍스트 대 이미지 생성 모두에서 중추적이다. 영어로 COCO(Lin et al., 2014), Flickr(젊은 et al., 2014), 중국어로 구성된 COCO-CN(Li et al., 2019), Flickr-CN(Li et al., 2016)과 같은 전통적인 이미지 캡션 데이터 세트는 발견된 훈련 기반을 제공하지만 일반적으로 100만 엔트리 미만에는 제한적이다. 결과적으로 라온(슈만 등), 2021년(주로 영어)과 우롱(구 등 2022)과 같은 웹스케일링 데이터셋이 확산 텍스트 대 이미지 모델을 훈련하는 데 더 중요한 데이터 소스로 등장해 최대 1억 개 또는 50억 개까지 크기를 자랑한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리의 연구는 이중언어 지원을 텍스트 대 이미지 모델에 통합하여 중국 맥락에서 복합 연구를 크게 발전시키는 심각한 영향을 보여준다. 타이이-CLIP 및 타이이-XL 모델의 개발은 확장된 어휘와 위치를 인코딩하여 이미지-텍스트 검색 및 이미지 생성에서 주목할 만한 발전을 나타낸다. 이 모델은 기반을 갖추었습니다.\n' +
      '\n' +
      '그림 5:타이이-XL 생성 사례\n' +
      '\n' +
      '이국어 복합 연구에서의 미래 혁신을 위한 것이다. 또한, 텍스트 프롬프트를 풍부하게 하기 위해 큰 비전-언어 모델을 사용하면 보다 정확하고 상세한 이미지 생성이 발생하여 사용자 의도와 밀접하게 일치한다. 이 접근법은 텍스트 대 이미지 생성에서 정확하고 복잡한 언어 이해의 중요성을 강조한다. 우리가 연구 결과와 모델을 계속 개방화하기 때문에 협업과 추가 탐색을 초대하여 인공 지능 연구에서 보다 포괄적이고 언어적으로 다양한 미래에 기여한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 아르조프스키 등은 (2017) 마르틴 아조프스키, 삿미스 치날라, 레온 보투 등이 있다. 웨저스타인 생성 적대 네트워크. 머신러닝_, pp. 214-223에 관한 _국제회의에서 2017. PMLR.\n' +
      '* 베커 등은 (2023) 제임스 베커, 가브리엘 고, 리징, 팀 브룩스, 지안펑 왕, 리안펑 왕, 린지 리, 롱 오우양, 준탕 주황, 조이스 리, 유페이 구오, 위삼 마샤라, 프레풀라 다하리왈, 케이시 추, 윈신 자오, 아디사 라메쉬 등이 있다. 더 나은 캡션으로 이미지 생성을 개선하는 __, 더 나은 캡션으로 이미지 생성을 개선합니다. 오픈라이 cdn. 폴라이.com/이자/달-e-3.pdf_, 2023.\n' +
      '*카오 등은 한춘오, 정탄, 장양가오, 광용선, 청안훈, 스탠쩌이 등이 있다. 생성 확산 모델에 대한 조사 __ 생성 확산 모델에 대한 조사. __ arXiv 프리프린트 arXiv:2209.02646_ 2022년입니다.\n' +
      '* 첸 등은 (2023) 이하오 첸, 시안비아오 큐, 지안왕, 레이 장 등이 있다. 디코클립: A에는 메모리 효율적인 클립 교육을 위한 대조적인 손실이 분포되어 있다. 컴퓨터 비전 및 패턴 인식_, pp. 22648-22657, 2023에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '* 첸 등은 중히 첸, 광류, 보웬 장, 풀롱 예, 청홍양, 레델우 등이 있다. Alt-clip: 확장된 언어 능력을 위해 클립에서 언어 인코더를 렌더링하는 __알트-클립: 확장 언어 능력을 위해 클립에서 렌더링한다. arXiv 프리프린트 arXiv:2211.06679_, 2022.\n' +
      '* 데블린 등은 (2019) 자코브 데블린, 명위 창, 켄톤 이, 크리스티나 투타노바 등이 있다. 저: 언어 이해를 위한 깊은 양방향 변압기의 사전 훈련. 2019 북미 컴퓨터 통계 협회의 _검토에서 인간 언어 기술, 권 1(장기 및 짧은 논문)_, pp. 4171-4186.\n' +
      '* 다이닝 등은 (2021) 명딩, 주요이 양, 원이 홍, 원디 정, 창주, 다진, 준양 린, 주양우, 주샤오, 홍시아 양 등 변압기를 통한 마스터 텍스트 대 이미지 생성. 2021년 신경 정보 처리 시스템_, 34:19822-19835의 발전입니다.\n' +
      '* Ding et al.(2022) 명딩, 월디 정, 원이 홍, 지 당. Cogview2: Faster 및 계층적 변압기를 통한 더 나은 텍스트 대 이미지 생성. __Cogview2: Faster 및 더 나은 텍스트 대 이미지 생성. 신경 정보 처리 시스템_, 2022년 35:16890-16902의 발전입니다.\n' +
      '* 간 등은 (2023) 루이 간, 지웨이 우, 리리앙 선, 준유 루, 샤오준 우, 딕시앙 장, 쿤하오 판, 핑 양, 키양, 지잉 장, 데이터 중심 학습은 모두 lms 필요이다. arXiv 프리프린트 arXiv:2311.03301_, 2023.\n' +
      '* 굿펠로우 등 (2014) 이안 굿펠로우, 장푸겟-아아디, 메흐디 미자, 빙쉬, 다비드 워드-팔리, 셔질 오제르, 아론포빌, 요슈화 벤지오 등이 있다. Al_ 생성적 역관망 __ 생성적 역관망. _2. 신경 정보 처리 시스템_, 2014년 27.\n' +
      '* 구 등은 (2022) 자오시구, 샤오준 멍, 구송 루, 루 허, 니후 미니허, 샤오단 리앙, 레이웨이 유오, 루누휘 황, 웨이 장, 신장, 등 1억 개의 대규모 중국 교차 훈련 전 벤치마킹 : 1억 개의 대규모 중국 교차 훈련 벤치마크. 신경 정보 처리 시스템_, 2022년 35:26418-26431의 발전입니다.\n' +
      '*호 등은 (2020) 조나단호, 아약 자인, 피에테르 압벨 등이 있다. 덴노징 확산 확률적 모델 __데노징 확산 확률적 모델. _<덴도징 확산 확률적 모델. 신경 정보 처리 시스템_, 2020:6840-6851의 발전.\n' +
      '* 킹마, 웰링(2013) 디에릭 P 킹마, 맥스 웰링. 오토 인코딩 변량 만 __ 오토 인코딩 변량 만. __ 오토 인코딩 변량 만이다. arXiv 프리프린트 arXiv:1312.6114_ 2013.\n' +
      '* 리 등은 (2016) 시롱리, 위유란, 지안펑동, 하일롱류 등이 있다. 중국의 캡션을 이미지에 추가하세요. 2016년 2016년 멀티미디어 검색_, pp. 271-275 국제 회의에 관한 2016년 ACM의 _검토에서.\n' +
      '\n' +
      '*리 등은 (2019) 시롱 리, 차록시 주, 샤오수 왕, 위유 란, 정시온그 주아, 강양, 지핑 주 등이 있다. 교차 수정 이미지 태깅, 캡션 및 검색을 위한 __Coco-cn. __Coco-cn. IEEE 거래는 2019년 멀티미디어_, 21(9):2347-2360에 관한 것이다.\n' +
      '* 린 등은 (2014) 타성이린, 마이클 마이어, 세르게 벨롱기, 제임스 하이이스, 피에트로 페로나, 데바 라만, 프리오르 딜, 코렌스 지트닉 등이 있다. 마이크로소프트 코모: 맥락상 공통 객체. "컴퓨터 비전-ECCV 2014: 제13차 유럽 회의, 스위스 취리히, 2014년 9월 6-12일, 제작, 파트 V 13_, pp 740-755. 스프링거 2014.\n' +
      '* 루 등은 (2023a) 준유 루, 루이 간, 딕시앙 장, 샤오준 우, 지웨이 우, 리리앙 선, 지잉 장, 핑지안 장, 연송 등이다. 액세서리: 시맨틱 인식 시각적 객체를 통한 미세 편성된 언어-비전 정렬 및 이해력. _미시어 인식 시각적 객체를 통한 이해력. arXiv 프리프린트 arXiv:2312.05278_, 2023a.\n' +
      '* 루 등 (2023b) 준유 루, 딕시앙 장, 샤오준 우, 신유 가오, 루이 간, 지잉 장, 옌 송, 핑지안 장 등이 있다. 다중 태스크 명령어 튜닝을 통한 __Ziya-vi: 이중언어 대형 비전-언어 모델. arXiv 프리프린트 arXiv:2310.08166_, 2023b.\n' +
      '* 루오 등은 (2023a) 시미안 루오, 이킨 탄, 롱보 황, 지안 리, 항 자오 등이 있다. 가벼운 일관성 모델: 2023a 단계 추론이 거의 없는 고해상도 이미지를 합성합니다.\n' +
      '* 루노 등은 (2023b) 시미안 루오, 이킨 탄, 수리자 파틸, 다니엘 구, 패트릭 폰 플라텐, 아폴리나리오 고소, 롱보 황, 지안 리, 항 자오 등이다. Lcm 인스턴스: 보편적인 안정확산 가속도 모듈. __A 보편적 안정확산 가속도 모듈. arXiv 프리프린트 arXiv:2311.05556_, 2023b.\n' +
      '* 포델 등은 (2023) 더스틴 포델, 지온 영어, 카일 레이시, 안드레아스 블라트만, 팀 도코렌, 조 뮬러, 조펜나, 로빈 람바흐 등을 들 수 있다. 고해상도 이미지 합성을 위한 잠재 확산 모델 개선: __고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 프리프린트 arXiv:2307.01952_, 2023.\n' +
      '* Radford et al.(2021) 알레크 라드포드, 종욱 김, 크리스 홀리스, 아디아 레즈, 가브리엘 고, 샌히니 아가왈, 기리시 사스트리, 아미다 아셀, 파멜라 미슈킨, 잭 클라크 등 자연 언어 감독으로부터 시각적 모델을 전수할 수 있다. 머신러닝_, pp. 8748-8763에 관한 _국제회의에서 2021년 PMLR.\n' +
      '* 라메쉬 등은 아디다 라메시, 미하일 파블로프, 가브리엘 고, 스콧 그레이, 첼시 보스, 알레셀퍼드, 마크 첸, 아이라이아 세이츠케버 등이다. 제로샷 텍스트 대 이미지 생성. "기계 학습_, pp. 8821-8831"에서 2021년 PMLR.\n' +
      '* 라즈(2022) 아디야 레즈, 프라풀라 다라리왈, 알렉스 니콜, 케이시 추, 마크 첸 등이 있다. 클립 래치들을 가진 __ 계층적 텍스트-조건 이미지 생성  _ 클립 래치들을 갖는 계층적 텍스트-조건 이미지 생성이다. arXiv 프리프린트 arXiv:2204.06125_, 1(2):3, 2022.\n' +
      '* 레젠데와 모하메드(2015) 다닐로 레젠데와 샤키르 모하메드이다. 유동을 정규화하는 가변 추론. 머신러닝_, pp. 1530-1538에 관한 _국제회의에서 2015. PMLR.\n' +
      '* 람바흐 등(2022) 로빈 라이바흐, 안드레아스 블라트만, 도미니크 로렌츠, 패트릭 에저, 비콘 오머 등이 있다. 잠재 확산 모델을 사용한 고해상도 이미지 합성입니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, pp 10684-10695, 2022년 6월 IEEE/CVF 회의의 _검토에서.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, 필리필 피셔 및 토머스 Brox. U-net: 생물의학적 이미지 분할을 위한 콘볼루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 합의, 부분 III 18_ pp. 234-241. 스프링거.\n' +
      '* 사하라리아 등은 윌리엄 샤아리아, 윌리엄 찬, 소라바 시세나, 라라 리, 자 휘앙, 에밀릴 리, 카마르 게르미포, 라파엘 게티조 로프, 버쿠 카가골 아얀, 팀 살림산 등 언어 이해가 깊은 포토어리스틱 텍스트 대 이미지 확산 모델. 신경정보처리시스템_, 2022년 35:36479-36494의 효과.\n' +
      '* 슈하만 등은 (2021) 크리스토프 슈하만, 리처드 베르누, 로메인 베아룸트, 로베르트 카카마키크, 클레이튼 갈리스, 아루시 카타, 테오 코바스, 제니아 지트세프, 아란 코마쓰자키 등이다. 라온-400m: 클립 필터링된 4억 이미지-텍스트 쌍의 오픈 데이터셋. __ 클립 필터링된 4억 이미지-텍스트 쌍. arXiv 프리프린트 arXiv:2111.02114_ 2021.\n' +
      '* 시넨코프(2023) 알렉스 샤네노프, 미샤 키스탄티노프, 다리아 바샬다바바, 크리스토프 슈하만, 케니아 이바노바, 나디아 클레오코바 등이 있다. 만약 저장소 Title, 2023.\n' +
      '* 슈네노코프(2021)* 송 등 (2020) 지밍 송, 첸린 멍, 스테파노 에르몬 등이 있다. 덴노징 확산 암묵 모델 __데노징 확산 암묵 모델. _다노징 확산 암묵 모델. arXiv 프리프린트 arXiv:2010.02502_, 2020.\n' +
      '* 송 등은 (2023) 양송, 프라풀라 다하리왈, 마크 첸, 아이라이아 세이츠케버 등이 있다. 일관성 모델. 2023.\n' +
      '* 빈센트 (2011) 파스칼 빈센트. 점수 매칭과 데노징 오토인코더 사이의 연결. __ 점수 매칭과 데노징 오토인코더 사이의 연결. 신경 계산_, 23(7):1661-1674, 2011.\n' +
      '* 왕 등은 왕(2023) 선규, 중지 단안, 빙옌 류, 잔니 주, 크엔 첸, 코이자아, 준황 등이 있다. 인디퓨전: 클라우드에서 텍스트 대 이미지 합성을 위한 개방형 중국 확산 모델 계열을 구성 및 서빙한다. __ arXiv 프리프린트 arXiv:2309.05534_, 2023.\n' +
      '임진왕, 유시장, 신규장, 샤오장, 샤오장, 샤오장, 주창동, 지안정장, 차이양, 용펑황, 시야유리, 양한우, 준유루, 신유위우, 위펑셴, 티핑한, 건하판, 로이왕, 하오왕, 중수준, 총피훈, 루이간, 조잉장(2022)은장, 장, 샤오장, 지청장, 장, 장, 장, 장, 장, 장, 장, 장, 정주우, 우, 차우, 차우, 차우, 차우, 차우, 차우, 차우, 차우, 로이위우, 로이위우, 중세우, 중위위위위위, 중위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위위 펑셴방 1.0: 중국 인지지능의 토대를 이루는 __펑셴방 1.0: CoRR_, abs/2209.02970, 2022.\n' +
      '* 양 등은 안양(2022) 안양, 준추판, 준양린, 루이남, 요창장, 진렌주, 창주 등이 있다. 중국 클립 __중국 클립: 중국어로 사전 실습하는 정책 비전 언어. _중국식 전조. arXiv 프리프린트 arXiv:2211.01335_, 2022.\n' +
      '* 예 등은 (2023) 풀롱 예, 광이 류, 신야 우, 레델 유우. 다중언어 텍스트-이미지 확산 모델 _tdiffusion: 다중언어 텍스트-이미지 확산 모델. ArXiv_, abs/2308.09991, 2023. URL[https://apisemanticscholar.org/CorpusID:261048720](https://apisemanticscholar.org/CorpusID:261048720).\n' +
      '*영 등은 (2014) 피터영, 앨리스라이, 미라 호도시, 줄리아 호켄마이어 등이 있다. 이미지 설명에서 시각적 데모션까지 __ 의미론적 추론을 위한 새로운 유사도 메트릭: 이벤트 설명보다 의미론적 추론을 위한 새로운 유사도 메트릭. 컴퓨터 로직_, 2014년 2:67-78에 대한 협회의 거래.\n' +
      '*장 등은 (2022) 자득장, 루이간, 준지왕, 유시강장, 린장, 핑양, 신유가오, 지웨이 우, 샤오쿤동, 준칭하 등 중국인지지성의 근간을 이루고 있다. arXiv 프리프린트 arXiv:2209.02970_ 2022년입니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>