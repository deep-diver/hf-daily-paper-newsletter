<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '_InteractiveVideo_: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions\n' +
      '\n' +
      'Yiyuan Zhang\\({}^{1}\\) Yuhao Kang\\({}^{2}\\) Zhixin Zhang\\({}^{2}\\)\n' +
      '\n' +
      'Xiaohan Ding\\({}^{3}\\) Sanyuan Zhao\\({}^{2}\\) Xiangyu Yue\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\) Multimedia Lab, The Chinese University of Hong Kong\n' +
      '\n' +
      '\\({}^{2}\\) Beijing Institute of Technology \\({}^{3}\\)Tencent AI Lab\n' +
      '\n' +
      'yiyuanzhang.ai@gmail.com, kangyuhao@bit.edu.cn, xyyue@ie.cuhk.edu.hk\n' +
      '\n' +
      '[https://invictus717.github.io/InteractiveVideo](https://invictus717.github.io/InteractiveVideo)\n' +
      '\n' +
      'Equal Contribution\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce "InteractiveVideo", a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users\' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With InteractiveVideo, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at [https://github.com/invictus717/InteractiveVideo](https://github.com/invictus717/InteractiveVideo).\n' +
      '\n' +
      'Figure 1: **Interactive Video Generation** We propose a user-centric framework that effectively synergizes usersâ€™ multimodal instructions. Users can easily edit key components in the video generation process, leading to high-quality video and increased user satisfaction.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Video generation has attracted significant attention due to its promising future in the AI-Generated Content field [3, 18, 26, 40, 43] and its potential to enhance the efficiency of movie creation and serve as a new infrastructure technology for the film industry [7, 14, 29, 51]. The advancement of diffusion models [13, 30, 31, 47] has infused the field of video generation with new potential [3, 4, 26]. The success of Gen-1 [8], MagicVideo [51], and Align your latents [1] has significantly inspired further exploration of high-quality visual content generation for videos.\n' +
      '\n' +
      'As the capabilities of video generation models improve, user expectations for the generated videos are concurrently elevated, leading to an increased demand for videos that accurately meet their specific requirements. Existing video generation models typically utilize a reference image, known as the _image condition_, and a textual description, referred to as the _text condition_, as inputs. Enhancing video generation foundation models by making them larger, more advanced, and more sophisticated could potentially fulfill\n' +
      '\n' +
      'Figure 2: Comparison between Gen-2 and _InteractiveVideo_. For each case, the first row is the generation results of Gen-2, and the second row is our results. (More comparison results with Pika Labs, I2VGen-XL [48], and Gen-2 can be found in Appendix Figures 7, 8, and 9.)user requirements more effectively by enhancing the understanding of image and text conditions, thereby producing videos of superior quality. However, our objective is to empower existing video generation models to more accurately fulfill user requirements from a different angle - by equipping models with the capability to interpret complex, multi-dimensional human instructions. This approach is driven by the observation that the current conditioning mechanisms (image and text) fall short of capturing the full spectrum of user intentions. **1**) The text condition may not be informative enough. Even though existing video generation models support long and detailed text prompts, it is difficult to precisely depict complex video motions and dynamics using only text. As a result, it becomes challenging for generative models to fully interpret the intended video content. **2**) The conditional image does not contain temporal information. The absence of optical flow and temporal consistency can easily lead to the introduction of unsatisfactory artifacts in the video generation process. Moreover, **3**) there is a significant demand from users for the customization of videos, which entails the intuitive manipulation of video contents, semantics, and motions. In response to these challenges, we propose a novel approach that improves the ability of existing video generation models to better understand human intentions and generate videos guided by more detailed and multifaceted human instructions.\n' +
      '\n' +
      'Recently, remarkable progress in large language models has drawn widespread attention across the community [2]. One key to the success of large language models is learning from human feedback through reinforcement learning [5, 20, 23, 32] which significantly improves the performance of language models and leads to superior generation results. Pioneers in the visual content generation field have also introduced human feedback to generate high-quality images [41]. Nonetheless, the intricacy, diversity, and level of control required for video generation far surpass those needed for single-image generation, making it a highly significant yet relatively underexplored challenge.\n' +
      '\n' +
      'To address these challenges, we propose _InteractiveVideo_, a user-centric video generation framework that empowers users to actively participate in the generation process through multimodal instructions, enabling control over video content, semantics, and motions. Users can customize a video through various manipulations such as painting and dragging, as illustrated in Figure 1. More specifically, we propose a Synergistic Multimodal Instruction mechanism that empowers generative models to interpret and act upon users\' editing and revision instructions across various facets, such as video content, regional semantics, object motion, subjects, and the overall dynamics of the video. In our framework, we capture user interactions in the form of image, text, motion, and trajectory prompts, and we incorporate these user instructions as independent conditions of probabilistic models. As a result, _InteractiveVideo_ is a training-free framework that can be easily and flexibly applied to different fundamental generative models. It is worth noting that our proposed framework seamlessly integrates with existing generative models and practical techniques, such as Stable Diffusion [25], DreamBooth [27], and LoRA [17], thus expanding the video generation capabilities with our interactive framework.\n' +
      '\n' +
      'In this framework, user interactions are involved through four distinct types of instructions which can be employed independently or collaboratively to effectively guide the video generation process. The four types of instructions are:\n' +
      '\n' +
      '* Image Instruction: the image condition or prompt for image-to-video generation.\n' +
      '* Content Instruction: a textual description of the visual elements and the painting edits of the user to control the video content.\n' +
      '* Motion Instruction: a textual description specifying the desired movements and dynamics of elements within the video.\n' +
      '* Trajectory Instruction: user-defined motion trajectories for specific video elements, expressed through interactions such as dragging.\n' +
      '\n' +
      'By incorporating these detailed and multidimensional human instructions, we can generate videos that better align with the unique preferences and requirements of users.\n' +
      '\n' +
      'We compare our _InteractiveVideo_ with the advanced video generation solutions, Gen-21, I2VGen-XL [48], and Pika Labs. Comparison results in Figure 2, 7, 8, 9 show the superiority of _InteractiveVideo_ with higher quality, better flexibility, and richer controllability. Our _InteractiveVideo_ paves the way for a novel paradigm in visual content generation, integrating user interactions to enable highly customized video generation. This empowers users to effortlessly obtain high-quality videos they desire through intuitive manipulation and effective interactions.\n' +
      '\n' +
      'Footnote 1: [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2)\n' +
      '\n' +
      'In summary, our contributions are as follows:\n' +
      '\n' +
      '* **Framework Deisgn**: we propose a novel interactive framework that empowers users to precisely control video generation by intuitive manipulations.\n' +
      '* **Generation Algorithm**: we propose a Synergistic Multimodal Instructions mechanism, which integrates user prompts as probabilistic conditions and enables interaction without the need for additional training.\n' +
      '* **High-quality Video Generation**: our generation results demonstrate superiority over state-of-the-art video generation methods, including Gen-2, I2VGen-XL [48], and Pika Labs.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Video Generation\n' +
      '\n' +
      'Initial attempts in video generation primarily leveraged Generative Adversarial Networks (GANs) [9, 16, 21, 24, 29, 33, 37, 44] and Variational Autoencoders (VAEs) [21, 22, 42]. These methods, however, faced considerable challenges in effectively modeling the intricate spatio-temporal dynamics necessary for text-driven video generation, leaving the problem largely unsolved. Subsequent innovations shifted towards diffusion models [13, 30, 31, 47] to enhance diversity and fidelity in video outputs [1, 3, 7, 14, 15, 18, 26, 40, 43, 45, 46, 51] and to scale up pre-training data and model architecture [6, 14, 16, 28, 36, 49]. Recent efforts have introduced spatio-temporal conditions [4, 7, 34, 43], for instance, through VideoComposer [34], Gen-1 [7] and DragNUWA [43]. These methods aim to provide a more controlled generation process but still encounter constraints in achieving flexible and user-satisfied video synthesis.\n' +
      '\n' +
      '### Models Guided by Human Feedback\n' +
      '\n' +
      'The idea of learning from human feedback, initially investigated in reinforcement learning and agent alignment contexts [5, 20], was subsequently applied to large language models [23, 32]. This approach has significantly improved the generation of textual outputs that are helpful, honest, and harmless. In the field of visual content generation, particularly in video generation and editing, a similar goal is pursued. [19, 38, 41] demonstrates the great potential of human guidance for the visual content generation field.\n' +
      '\n' +
      'These works collectively highlight the growing trend of incorporating human feedback in various forms of generative models, extending its utility from text-based to visual content generation. However, learning from human feedback for video generation remains under-explored owing to its complicated elements of motion, subjects, and spatial-temporal dynamics. We aim to fill this gap, providing a training-free and user-friendly solution for elevating existing video generative models with effective human guidance and generating more user-satisfying and higher-quality videos.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      'As shown in Figure 3, _InteractiveVideo_ realizes controllable video generation with two generative pipelines based on latent diffusion models - 1) the text-to-image (T2I) pipeline \\(\\mathcal{P}_{img}\\) and **2)** the image-to-video (I2V) pipeline \\(\\mathcal{P}_{video}\\). The framework outputs a video containing \\(N_{F}\\) frames \\(\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots,\\mathbf{v}_{N_{F}}\\}\\). We denote the Image Instruction by \\(\\mathbf{x}\\in\\mathbb{R}^{C\\times H\\times W}\\), the Content Instruction by \\(y\\), the Motion Instruction by \\(y^{\\prime}\\), and the Trajectory Instruction by \\(r\\). More specifically, the Trajectory Instruction is represented by start and end points and region masks, which indicate the desired moving trajectories of specific objects. The whole pipeline can be formulated as\n' +
      '\n' +
      '\\[\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots,\\mathbf{v}_{N_{F}}\\}=\\mathcal{P}_{video}(\\mathcal{ P}_{img}(\\mathbf{x},y),y^{\\prime},r). \\tag{1}\\]\n' +
      '\n' +
      'In practice, we may implement \\(\\mathcal{P}_{img}\\) with any off-the-shelf T2I model as long as it takes a text condition and an image condition as inputs. We use \\(\\tilde{\\mathbf{x}}\\) to denote its generated image, _i.e_., the intermediate image, which is the input to the I2V model.\n' +
      '\n' +
      'We then use \\(\\tilde{\\mathbf{x}}\\) as the image condition of the I2V pipeline and the Motion Instruction \\(y^{\\prime}\\) as the text condition. We may use any off-the-shelf I2V diffusion models which require image and text conditions. Let \\(\\mathcal{E}\\) be the image encoder of the I2V model, \\(\\mathbf{z}_{0}=\\mathcal{E}(\\tilde{\\mathbf{x}})\\) be the corresponding latent code, \\(\\epsilon_{t}\\) be the predicted noise at step \\(t\\), the classic (_i.e_., interaction-free) video denoising process can be denoted with\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\cdot \\epsilon_{t}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\bar{\\alpha}_{t}\\) is a parameter related to the variance schedule [12].\n' +
      '\n' +
      '### Synergistic Multimodal Instructions\n' +
      '\n' +
      'We control the video diffusion process with users\' multimodal instructions via _altering the predicted noise according to the users\' operations_. Conceptually, with \\(R\\) denoting the function that changes \\(\\epsilon_{t}\\) according to users\' operations, our interaction-controlled video diffusion process can be represented by\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\cdot R (\\epsilon_{t}). \\tag{3}\\]\n' +
      '\n' +
      'The proposed concrete implementation for the function \\(R(\\cdot)\\) is realized by treating user interactions as denoising residuals. Since the intermediate image \\(\\tilde{\\mathbf{x}}\\) is utilized as the condition image of \\(\\mathcal{P}_{video}\\), it is seen as the "interface" between user and video generation model. Consequently, our framework empowers users to interact with the target video by introducing their interactions as new generation conditions of the video denoising process.\n' +
      '\n' +
      'Specifically, we transform users\' operations into denoising residuals to eventually control the video diffusion process. Formally, in the video denoising process, assume the original intermediate image is \\(\\tilde{\\mathbf{x}}\\) and the corresponding latent code is \\(\\mathbf{z}_{0}=\\mathcal{E}(\\tilde{\\mathbf{x}})\\). Once the user has operated on theimage (_e.g_., painted some lines or set some trajectories2), the intermediate image changes accordingly, and we denote the resultant intermediate image as \\(\\tilde{\\mathbf{x}}^{\\prime}\\) and the corresponding latent code becomes \\(\\mathbf{z}_{0}^{\\prime}=\\mathcal{E}(\\tilde{\\mathbf{x}}^{\\prime})\\). We use \\(\\mathbf{z}_{0}^{\\prime}\\) to predict the noise in the video diffusion process. Formally, let \\(t\\) be the time step, \\(\\epsilon_{t}\\) be the noise predicted with \\(\\mathbf{z}_{t-1}\\) and \\(\\epsilon_{t}^{\\prime}\\) be the noise predicted with \\(\\mathbf{z}_{t-1}^{\\prime}\\), the noise we use is given by\n' +
      '\n' +
      'Footnote 2: Painting and trajectory drawing affect the intermediate image in different ways. The former makes a difference on the intermediate image through the T2I pipeline as it changes the very beginning Image Instruction. The latter moves the handle points within the specified region to the target points and changes the optical flow of the intermediate image.\n' +
      '\n' +
      '\\[\\hat{\\epsilon}_{t}=\\lambda\\cdot\\epsilon_{t}+(1-\\lambda)\\cdot\\epsilon_{t}^{ \\prime}\\,, \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\lambda\\) is a hyper-parameter to balance the learned noise residual and human instructions. Then we use \\(\\hat{\\epsilon}_{t}\\), instead of the original \\(\\epsilon_{t}\\), in the denoising process to generate the eventual video.\n' +
      '\n' +
      'Note that after the user operations on the image, the human-crafted discrepancy may affect the temporal coherence of the resulting video. This is because the user operations may have deviated the intermediate image from the distribution on which the I2V model was trained (_e.g_., the user has drawn a twisted yellow curve to create a sun in the sky, which is unusual in the training data of the I2V model). To solve this problem, upon the completion of the video diffusion process, we post-process the resultant video following AnimateDiff [10]. Every single frame is aligned with the intermediate image via a Group Normalization [39] layer, a SiLU [11] activation, and a 2D convolutional layer adopted from AnimateDiff or PIA [50], as such structures are found to generalize well to our common distribution produced by users\' typical operations. Specifically, the eventual \\(i\\)-th video frame \\(\\mathbf{v}_{i}^{\\prime}\\) can be computed as:\n' +
      '\n' +
      '\\[\\mathbf{v}_{i}^{\\prime}=\\texttt{Conv2D}(\\texttt{SiLU}(\\texttt{GroupNorm}(\\mathbf{v}_{i }-\\tilde{\\mathbf{x}})))\\,. \\tag{5}\\]\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'In this section, we present features of the _Interactive-Video_ framework including personalization (SS 4.1), fine-grained video editing (SS 4.2), and precise motion control (SS 4.3). Besides, we also conduct quantitative analysis (SS 4.4) on generation quality and user study on the satisfaction rates with our framework. Then, we demonstrate the generation efficiency (SS 4.5) of our framework.\n' +
      '\n' +
      '### Personalizing a Video\n' +
      '\n' +
      'Existing methods [7, 10, 34] have made significant progress in the animation of static images into videos. However, these methods are limited to animating objects or scenes already present in the original static images, and encounter difficulties when it comes to generating a video with objects or scenes absent from the referenced images. In other words, existing methods have limited ability to control video content, especially when users want to add or animate previously unseen objects or scenes.\n' +
      '\n' +
      'With _InteractiveVideo_, we enable video content manipulation by incorporating abundant elements. In Figure 4, we demonstrate that our framework supports the users to customize the video content freely. For example, we use a brush to paint sketches of birds, waves, and polar lights in Figure 4 (a), (b), and (c), respectively. The added objects are seamlessly integrated and animated throughout the entire\n' +
      '\n' +
      'Figure 3: **Framework Illustration**. In _InteractiveVideo_, users can utilize multimodal instructions to interact with generative models on video content, motion, and trajectory.\n' +
      '\n' +
      'video. Seen from the following frames, _InteractiveVideo_ enables users to create a video of satisfactory temporal consistency even though the referenced image does not directly contain the objects. Meanwhile, such cases also demonstrate the versatility and adaptability of our framework in creating diverse and engaging video content, highlighting its potential for a wide range of applications in content creation and editing.\n' +
      '\n' +
      '### Fine-grained Video Editing\n' +
      '\n' +
      'Another significant limitation of current generation methods is the challenge of performing precise regional editing. During the generation process, models have difficulty interpreting natural language references such as "left", "right", "up", and "down". This makes it hard to accurately edit regional semantics, which is crucial for user experience.\n' +
      '\n' +
      'Fortunately, _InteractiveVideo_ overcomes this limitation by enabling intuitive manipulation in the intermediate image. As illustrated in Figure 5 (a), it is difficult for users to edit the color of a specific tree or control the color of a particular cluster of falling leaves using existing methods. In contrast, our framework allows users to perform fine-grained semantic editing on any region. For example, after the editing process, the trees in Figure 5 (a), clouds in Figure 5 (b), and the logo in Figure 5 (c) can be easily modified. The generated videos are of high quality, featuring realistic motion, appropriate light reflection, and visually appealing textures.\n' +
      '\n' +
      '### Precise Motion Control\n' +
      '\n' +
      'Motion control, particularly precise motion control, poses a significant challenge in the field of video generation due to the complexity of modeling spatial-temporal patterns. The primary difficulty lies in maintaining the temporal consistency of generated videos, especially when handling substantial motion. This issue mainly stems from the limited temporal receptive field of 1D temporal attention, which struggles to accommodate the full range of motion-related changes over time. As a result, ensuring smooth and consistent representation of motion in generated videos remains a considerable obstacle in this field. Differently, _InteractiveVideo_ excels in precise motion control, which we will discuss from three aspects as follows:\n' +
      '\n' +
      '**1)** Large Motion. As shown in the first two rows of Figure 6 first two rows, we present the large motion control by turning around characters in both realistic and cartoon styles. The details of turning around the female character are impressive, with the motion of her hair appearing highly realistic.\n' +
      '\n' +
      'Figure 4: **Video Content Manipulation** with _InteractiveVideo_. In (a), (b), and (c), we present the content manipulation by adding birds, waves, and polar lights. Then, these added objects are driven in the whole video. We use these results to show the flexibility of our framework for video content creation.\n' +
      '\n' +
      '**2)** Precise Motion. As seen in the third row of Figure 6, the adorable corgi holding the "INTERACTIVE VIDEO" brand displays several different charming gestures, including wagging its tail, smiling with an open mouth, turning its head, and shaking its ears.\n' +
      '\n' +
      '**3)** Multi-Object Motion. The last two rows in Figure Figure 6 showcase the ability of _InteractiveVideo_ to control multi-object motion. Our framework precisely controls the movements of both the cute girl and the lovable dog. When adjusting the dog\'s head, its tail also wags, and the girl naturally lowers her hand. While controlling these two objects, the girl smiles sweetly, and the dog turns its head to the other side.\n' +
      '\n' +
      '### Quantitative Analysis\n' +
      '\n' +
      '**AnimateBench.** Since _InteractiveVideo_ is a general framework for open-domain video generation, we use **AnimateBench** for comparison. We assessed the text-based video generation capability using 105 unique cases with varying content, styles, and concepts. These cases were created using seven distinct text-to-image models, with five images per model for thorough comparison. Additionally, we crafted three motion-related prompts for each image to evaluate motion controllability across different methods, focusing on potential single-shot image motions.\n' +
      '\n' +
      '**Evaluation Metrics.** We evaluate generation quality by considering image and text alignment, using CLIP scores to measure cosine similarity between embeddings. Image alignment compares input images and video frame embeddings, while text alignment examines text and frame embedding similarities.\n' +
      '\n' +
      '**User Study.** To substantiate the enhancement of our method in terms of visual quality and user experience, we carried out a user study comparing our approach with other video models. This study utilized 40 prompts from AnimateBench, which feature a variety of scenes, styles, and objects. Compared to existing video generation methods, our _InteractiveVideo_ notably outperforms in terms of human preference scores and delivers state-of-the-art performance in user satisfaction rates. These quantitative results, coupled with the user study, effectively demonstrate the significance and superiority of the interactive generation paradigm and\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{CLIP Score} & \\multicolumn{2}{c}{User Study} & \\multicolumn{2}{c}{Satisfaction Rate} \\\\  & Image & Text & Image & Text & (\\%) \\\\ \\hline VideoComposer[35] & 225.3 & 62.85 & 0.180 & 0.110 & 43.5 \\\\ AnimateDiff[10] & 218.0 & 63.13 & 0.295 & 0.220 & 51.6 \\\\ PIA [50] & 225.9 & 63.68 & 0.525 & 0.670 & 52.5 \\\\ \\hline _InteractiveVideo_\\({}_{\\text{rec}}\\) & **234.6** & **65.31** & **0.745** & **0.813** & **72.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Quantitative comparison on AnimateBench.\n' +
      '\n' +
      'Figure 5: **Fine-grained Video Editing** with _InteractiveVideo_. In (a), (b), and (c), we perform fine-grained **regional** semantic editing on changing colors and appearances of specific objects, These results show the outstanding controllability of our framework for video generation.\n' +
      '\n' +
      'user-centric designs.\n' +
      '\n' +
      '### Generation Efficiency\n' +
      '\n' +
      '_InteractiveVideo_ takes only 16GB CUDA memory in the inference process, and it runs on a single RTX 4090. Besides, in Table 2, we also report the latency of _InteractiveVideo_. It is worth noting that _InteractiveVideo_ can generate a video within about 12 seconds though it requires two independent diffusion models for better controllability.\n' +
      '\n' +
      '## 5 Responsible AI and Ethic Claim\n' +
      '\n' +
      'In developing _InteractiveVideo_, our research rigorously adheres to the principles of Responsible AI and ethical guidelines. This innovative framework for video generation is designed with a strong commitment to ethical AI practices, ensuring that user interactions with the system - through text, images, and direct manipulation - are processed with the utmost integrity and transparency. The implementation of our Synergistic Multimodal Instruction mechanism is a testament to our dedication to these principles. It not only facilitates a seamless integration of diverse user inputs but also ensures that the AI operates within ethical boundaries, avoiding biases and respecting user intent. By empowering users to interactively manipulate the video generation process, _InteractiveVideo_ promotes not just creativity but also responsibility in AI use. This approach aligns with our commitment to uphold ethical standards in AI, ensuring that _InteractiveVideo_ serves as a model for responsible innovation in the realm of AI-driven content creation\n' +
      '\n' +
      '## 6 Conclusion and Discussion\n' +
      '\n' +
      'In summation, we introduce _InteractiveVideo_, a novel paradigm shift in the domain of video generation that champions a user-centric approach over the conventional methodologies reliant on pre-defined images or textual prompts. This framework is distinguished by its capacity to facilitate dynamic, real-time interactions between the user and the generative model, enabled by a suite of intuitive interfaces including, but not limited to, text and image prompts, manual painting, and drag-and-drop capabilities. Central to our framework is the innovative Synergistic Multimodal Instruction mechanism, a testament to our commitment to integrating multifaceted user interaction into the generative process cohesively and efficiently. This mechanism augments the interactive experience and significantly refines the granularity with which users can influence the generation outcomes. The resultant capability for users to meticulously customize key video elements to their precise preferences, coupled with the consequent elevation in the visual quality of the generated content, underscores the transformative potential of _InteractiveVideo_ in the landscape of video generation technologies.\n' +
      '\n' +
      '**Discussion on the Computational Efficiency**. Notwithstanding the promising advancements heralded by _InteractiveVideo_, the adoption of a user-centric generative approach is not devoid of challenges. Paramount among these is the imperative to ensure the framework\'s accessibility and intuitive usability across a broad spectrum of users, alongside maintaining the generative models\' efficacy and computational efficiency amidst diverse and dynamic input scenarios. Future research endeavors might fruitfully focus on\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline Process & Image Instruction & Context Instruction & Motion Instruction & Trajectory Instruction \\\\ \\hline Time & 19-34ms & 31.47ms & 12.22s & 77.35 ms \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Latency Analysis of _InteractiveVideo_.\n' +
      '\n' +
      'Figure 6: **Precise Motion Control** of _InteractiveVideo_. Our framework shows strong controllability in large motion control, precise gesture control, and multi-object motion control.\n' +
      '\n' +
      'the refinement of these models to enhance scalability and the development of adaptive algorithms capable of more accurately interpreting and actualizing user intentions.\n' +
      '\n' +
      '**Future Works**. We may delve into several promising directions. Enhancing the AI\'s understanding of complex user inputs, such as emotional intent or abstract concepts, could lead to more nuanced and contextually relevant video generation. Additionally, exploring the integration of real-time feedback loops where the model suggests creative options based on user input history could further personalize the user experience. Investigating the application of this framework in virtual and augmented reality environments opens up new dimensions for immersive content creation. Furthermore, extending the framework\'s capabilities to include collaborative generation where multiple users can interact and contribute to a single generative process may revolutionize co-creation in digital media.\n' +
      '\n' +
      '**Further Applications**. The potential applications of _InteractiveVideo_ extend well into the realms of education, where bespoke video content could significantly enrich the pedagogical experience and entertainment, particularly in the creation of interactive narratives. As we continue to iterate upon and enhance this framework, the scope for its application appears limitless, heralding a future in which video generation transcends mere content creation to become a conduit for deep, interactive engagement between creators and their digital canvases.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:10]\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      'Figure 8: **Comparison with existing methods on landscapes**. We compare _InteractiveVideo_ (4th row) with Pika Labs (1st row), I2VGen-XL (2nd row), and Gen-\\(2\\) (3rd row).\n' +
      '\n' +
      'Figure 9: **Comparison with existing methods on dynamic scenes**. We compare _InteractiveVideo_ (4th row) with Pika Labs (1st row), I2VGen-XL (2nd row), and Gen-2 (3rd row).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22563-22575, 2023.\n' +
      '* [2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. _arXiv preprint arXiv:2307.03109_, 2023.\n' +
      '* [3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023.\n' +
      '* [4] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023.\n' +
      '* [5] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [6] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan. Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. _arXiv preprint arXiv:2311.15599_, 2023.\n' +
      '* [7] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Gransskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. _arXiv preprint arXiv:2302.03011_, 2023.\n' +
      '* [8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Gransskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7346-7356, 2023.\n' +
      '* [9] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell. Tell me what happened: Unifying text-guided video completion via multimodal masked video generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10681-10692, 2023.\n' +
      '* [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.\n' +
      '* [16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. _arXiv preprint arXiv:2205.15868_, 2022.\n' +
      '* [17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [18] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [19] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. _arXiv preprint arXiv:2302.12192_, 2023.\n' +
      '* [20] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. _arXiv preprint arXiv:1811.07871_, 2018.\n' +
      '* [21] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In _Proceedings of the AAAI conference on artificial intelligence_, 2018.\n' +
      '* [22] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1096-1104, 2017.\n' +
      '* [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [24] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1789-1798, 2017.\n' +
      '* [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [26] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10219-10228, 2023.\n' +
      '* [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [28] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [29] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3626-3636, 2022.\n' +
      '* [30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Computer Vision_, pages 2256-2265. PMLR, 2015.\n' +
      '* [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.\n' +
      '* [33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [34] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [35] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [37] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In _European Conference on Computer Vision_, pages 720-736. Springer, 2022.\n' +
      '* [38] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2096-2105, 2023.\n' +
      '* [39] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.\n' +
      '* [40] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicfare: Animating open-domain images with video diffusion priors. 2023.\n' +
      '* [41] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagenward: Learning and evaluating human preferences for text-to-image generation. _arXiv preprint arXiv:2304.05977_, 2023.\n' +
      '* [42] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videoopt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [43] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. _arXiv preprint arXiv:2308.08089_, 2023.\n' +
      '* [44] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. _arXiv preprint arXiv:2202.10571_, 2022.\n' +
      '* [45] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18456-18466, 2023.\n' +
      '* [46] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023.\n' +
      '* [47] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. _arXiv preprint arXiv:2206.05564_, 2022.\n' +
      '* [48] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [49] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Metatransformer: A unified framework for multimodal learning. _arXiv preprint arXiv:2307.10802_, 2023.\n' +
      '* [50] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. _arXiv preprint arXiv:2312.13964_, 2023.\n' +
      '* [51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>