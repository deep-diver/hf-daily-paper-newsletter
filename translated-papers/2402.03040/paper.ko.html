<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '비디오__InteractiveVideo_: Synergistic Multimodal 명령어를 이용한 사용자 중심의 제어 가능한 비디오 생성\n' +
      '\n' +
      '이위안 장({}^{1}\\) 유하오 강({}^{2}\\) Zhixin 장({}^{2}\\)\n' +
      '\n' +
      '샤오한 딩\\({}^{3}\\) 산양자오\\({}^{2}\\) 상규유\\({}^{1}\\)\n' +
      '\n' +
      '중국 홍콩대학 멀티미디어 연구실\n' +
      '\n' +
      '베이징공업기술원({}^{3}\\({}^{2}\\) 텐센트 AI Lab\n' +
      '\n' +
      'hang.ai@gmail.com, kangyuhao@bit.edu.cn, xyyue@ie.cuhk.edu.hk\n' +
      '\n' +
      '[https://invictus717.github.io/InteractiveVideo](https://invictus717.github.io/InteractiveVideo)\n' +
      '\n' +
      'Equal Contribution\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 비디오 생성을 위한 사용자 중심 프레임워크인 "인터랙티브 비디오"를 소개한다. 사용자 제공 이미지 또는 텍스트를 기반으로 작동하는 전통적인 생성 접근법과 달리 본 프레임워크는 동적 상호 작용을 위해 설계되어 사용자가 텍스트 및 이미지 프롬프트, 페인팅, 드래그 앤 드롭 등과 같은 전체 생성 프로세스 동안 다양한 직관적 메커니즘을 통해 생성 모델을 지시할 수 있다. 본 논문에서는 사용자의 멀티모달 명령어를 생성 모델에 원활하게 통합함으로써 사용자 입력과 생성 프로세스 간의 협력적이고 반응적인 상호작용을 가능하게 하는 시너지적 멀티모달 명령어 메커니즘을 제안한다. 이 접근법은 정확하고 효과적인 사용자 명령을 통해 생성 결과의 반복적이고 세밀한 개선을 가능하게 한다. 인터랙티브 비디오로 사용자는 비디오의 주요 측면을 꼼꼼하게 조정할 수 있는 유연성을 부여받는다. 기준 이미지를 페인트칠하고, 의미론을 편집하고, 요구 사항이 완전히 충족될 때까지 비디오 모션을 조정할 수 있습니다. 코드, 모델 및 데모는 [https://github.com/invictus717/InteractiveVideo](https://github.com/invictus717/InteractiveVideo)에서 사용할 수 있다.\n' +
      '\n' +
      '그림 1: **인터랙티브 비디오 생성** 사용자의 멀티모달 명령어를 효과적으로 시너지 효과를 내는 사용자 중심 프레임워크를 제안한다. 사용자는 동영상 생성 과정에서 핵심 구성 요소를 쉽게 편집할 수 있어 고화질의 동영상으로 이어지고 사용자 만족도를 높일 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '비디오 세대는 AI 생성 콘텐츠 분야[3, 18, 26, 40, 43]의 유망한 미래와 영화 제작의 효율성을 높이고 영화 산업의 새로운 인프라 기술로서의 역할을 할 수 있는 잠재력으로 인해 상당한 관심을 끌었다[7, 14, 29, 51]. 확산 모델[13, 30, 31, 47]의 발전은 새로운 잠재력을 가진 비디오 생성 분야를 주입했다[3, 4, 26]. Gen-1[8], MagicVideo[51], Align your Latents[1]의 성공은 비디오를 위한 고품질 시각적 콘텐츠 생성에 대한 추가 탐구에 상당한 영감을 주었다.\n' +
      '\n' +
      '비디오 생성 모델들의 능력들이 향상됨에 따라, 생성된 비디오들에 대한 사용자 기대들이 동시에 상승되어, 그들의 특정 요건들을 정확하게 충족시키는 비디오들에 대한 수요 증가로 이어진다. 기존의 비디오 생성 모델들은 통상적으로 _image condition_로 알려진 참조 이미지와 _text condition_로 지칭되는 텍스트 디스크립션을 입력으로 활용한다. 비디오 생성 기반 모델을 더 크고 고급화하며 정교하게 만들어 잠재적으로 충족할 수 있음\n' +
      '\n' +
      '그림 2: Gen-2와 _InteractiveVideo_의 비교. 각 경우에 대해 첫 번째 행은 Gen-2의 생성 결과이고 두 번째 행은 우리의 결과이다. (Pika Labs, I2VGen-XL[48], 및 Gen-2와의 더 많은 비교 결과들은 부록 도 7, 도 8, 및 도 9.) 사용자 요구 사항들을 이미지 및 텍스트 조건들의 이해를 향상시킴으로써 보다 효과적으로 발견하여 우수한 품질의 비디오들을 생산한다. 그러나, 우리의 목적은 기존의 비디오 생성 모델들이 상이한 각도에서 사용자 요구 사항들을 보다 정확하게 이행하도록 권한을 부여하는 것 - 복잡하고 다차원적인 인간 지시들을 해석하는 능력을 갖춤 - 이 접근법은 현재의 컨디셔닝 메커니즘들 (이미지 및 텍스트) 의 관찰에 의해 구동된다. 사용자 의도에 대한 전체 스펙트럼을 포착하지 못했습니다. **1**) 텍스트 조건이 충분히 유익하지 않을 수 있습니다. 기존의 비디오 생성 모델은 길고 상세한 텍스트 프롬프트를 지원하지만 텍스트만을 사용하여 복잡한 비디오 모션과 동역학을 정확하게 묘사하는 것은 어렵다. 결과적으로 생성 모델이 의도된 비디오 콘텐츠를 완전히 해석하는 것은 어려워진다. **2**) 조건부 이미지는 시간적 정보를 포함하지 않는다. 광학 흐름의 부재와 시간적 일관성은 비디오 생성 프로세스에서 불만족스러운 아티팩트의 도입으로 쉽게 이어질 수 있다. 더욱이, **3**) 비디오의 커스터마이즈에 대한 사용자들의 상당한 요구가 있으며, 이는 비디오 콘텐츠, 의미론 및 모션의 직관적인 조작을 수반한다. 이러한 문제에 대응하여, 우리는 인간의 의도를 더 잘 이해하고 보다 상세하고 다면적인 인간 지시에 의해 안내되는 비디오를 생성하는 기존의 비디오 생성 모델의 능력을 향상시키는 새로운 접근법을 제안한다.\n' +
      '\n' +
      '최근 대규모 언어 모델의 괄목할 만한 진전이 커뮤니티 전반에 걸쳐 널리 주목받고 있다[2]. 대형 언어 모델의 성공의 한 가지 핵심은 강화 학습[5, 20, 23, 32]을 통해 인간의 피드백으로부터 학습하는 것으로 언어 모델의 성능을 크게 향상시키고 우수한 생성 결과를 이끌어낸다. 시각 콘텐츠 생성 분야의 개척자들도 인간의 피드백을 도입하여 고품질의 이미지를 생성하였다[41]. 그럼에도 불구하고 비디오 생성에 필요한 복잡성, 다양성 및 제어 수준은 단일 이미지 생성에 필요한 수준을 훨씬 능가하여 매우 중요하지만 상대적으로 미개척된 도전이다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해, 우리는 사용자가 멀티모달 명령어를 통해 생성 프로세스에 능동적으로 참여할 수 있도록 하여 비디오 콘텐츠, 의미론 및 모션에 대한 제어를 가능하게 하는 사용자 중심의 비디오 생성 프레임워크인 _InteractiveVideo_를 제안한다. 그림 1과 같이 그림 및 드래그와 같은 다양한 조작을 통해 사용자가 비디오를 커스터마이징할 수 있다. 보다 구체적으로, 우리는 생성 모델이 비디오 콘텐츠, 지역적 의미론, 객체 모션, 주제 및 비디오의 전반적인 역학 등 다양한 요소에 걸쳐 사용자의 편집 및 수정 지시를 해석하고 행동하도록 권한을 부여하는 시너지적 멀티모달 명령어 메커니즘을 제안한다. 프레임워크에서는 이미지, 텍스트, 모션 및 궤적 프롬프트의 형태로 사용자 상호 작용을 캡처하고 이러한 사용자 명령을 확률 모델의 독립적인 조건으로 통합한다. 결과적으로 _InteractiveVideo_는 다양한 기본 생성 모델에 쉽고 유연하게 적용할 수 있는 훈련 없는 프레임워크이다. 제안된 프레임워크는 기존의 생성 모델 및 실제 기법인 Stable Diffusion[25], DreamBooth[27], LoRA[17]와 원활하게 통합되어 대화형 프레임워크로 비디오 생성 기능을 확장한다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '이 프레임워크에서 사용자 상호작용은 비디오 생성 프로세스를 효과적으로 안내하기 위해 독립적으로 또는 협력적으로 사용될 수 있는 4가지 유형의 명령어를 통해 수반된다. 네 가지 유형의 지침은 다음과 같습니다.\n' +
      '\n' +
      '* 이미지 명령어: 이미지 대 비디오 생성을 위한 이미지 조건 또는 프롬프트.\n' +
      '* 컨텐츠 명령어: 비디오 컨텐츠를 제어하기 위해 시각적 요소들에 대한 텍스트 기술 및 사용자의 페인팅 편집들.\n' +
      '* 모션 명령어: 비디오 내의 요소들의 원하는 움직임들 및 역학들을 특정하는 텍스트 기술.\n' +
      '* Trajectory Instruction: 드래그와 같은 상호작용을 통해 표현되는, 특정 비디오 요소에 대한 사용자 정의 모션 궤적.\n' +
      '\n' +
      '이러한 상세하고 다차원적인 인간 지침을 통합함으로써 사용자의 고유한 선호도와 요구 사항에 더 잘 일치하는 비디오를 생성할 수 있다.\n' +
      '\n' +
      '우리는 우리의 _InteractiveVideo_를 고급 비디오 생성 솔루션인 Gen-21, I2VGen-XL[48] 및 Pika Labs와 비교한다. 그림 2, 7, 8, 9의 비교 결과는 더 높은 품질, 더 나은 유연성 및 더 풍부한 제어성을 갖는 _InteractiveVideo_의 우수성을 보여준다. 우리의 _InteractiveVideo_는 사용자 상호 작용을 통합하여 고도로 맞춤화된 비디오 생성을 가능하게 하는 시각적 콘텐츠 생성의 새로운 패러다임을 위한 길을 열어준다. 이를 통해 사용자는 직관적인 조작과 효과적인 상호 작용을 통해 원하는 고품질 비디오를 쉽게 얻을 수 있습니다.\n' +
      '\n' +
      '각주 1: [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2)\n' +
      '\n' +
      '요약하면, 우리의 기여는 다음과 같다:\n' +
      '\n' +
      '**Framework Deisgn**: 사용자가 직관적인 조작으로 비디오 생성을 정확하게 제어할 수 있는 새로운 대화형 프레임워크를 제안한다.\n' +
      '***생성 알고리즘**: 우리는 사용자 프롬프트를 확률적 조건으로 통합하고 추가 훈련 없이 상호 작용을 가능하게 하는 시너지 멀티모달 명령어 메커니즘을 제안한다.\n' +
      '* ** 고품질 비디오 생성**: 우리의 생성 결과는 Gen-2, I2VGen-XL[48] 및 Pika Labs를 포함한 최첨단 비디오 생성 방법보다 우월함을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### Video Generation\n' +
      '\n' +
      '비디오 생성의 초기 시도는 주로 GAN(Generative Adversarial Networks) [9, 16, 21, 24, 29, 33, 37, 44] 및 VAE(Variational Autoencoder) [21, 22, 42]를 활용했다. 그러나 이러한 방법은 텍스트 기반 비디오 생성에 필요한 복잡한 시공간 역학을 효과적으로 모델링하는 데 상당한 어려움에 직면하여 문제를 크게 해결하지 못했다. 후속 혁신은 비디오 출력[1, 3, 7, 14, 15, 18, 26, 40, 43, 45, 46, 51]에서 다양성 및 충실도를 향상시키고 사전 훈련 데이터 및 모델 아키텍처[6, 14, 16, 28, 36, 49]를 확장하기 위해 확산 모델[13, 30, 31, 47]로 이동했다. 최근의 노력은 예를 들어 VideoComposer[34], Gen-1[7] 및 DragNUWA[43]를 통해 시공간 조건[4, 7, 34, 43]을 도입하였다. 이러한 방법들은 보다 제어된 생성 프로세스를 제공하는 것을 목표로 하지만, 유연하고 사용자 만족 비디오 합성을 달성하는 데 있어서 여전히 제약에 직면한다.\n' +
      '\n' +
      '인간 피드백에 의한### 모델\n' +
      '\n' +
      '초기 강화 학습 및 에이전트 정렬 컨텍스트[5, 20]에서 조사된 인간 피드백으로부터의 학습에 대한 아이디어는 후속적으로 대형 언어 모델[23, 32]에 적용되었다. 이 접근법은 도움이 되고 정직하며 무해한 텍스트 출력의 생성을 크게 개선했다. 시각 콘텐츠 생성 분야, 특히 비디오 생성 및 편집 분야에서 유사한 목표를 추구한다. [19, 38, 41] 시각 콘텐츠 생성 분야에 대한 인간 지도의 큰 잠재력을 입증한다.\n' +
      '\n' +
      '이러한 작업은 인간의 피드백을 다양한 형태의 생성 모델에 통합하는 증가하는 추세를 집합적으로 강조하여 텍스트 기반에서 시각적 콘텐츠 생성으로 그 유용성을 확장한다. 그러나 비디오 생성을 위한 인간의 피드백에 의한 학습은 복잡한 움직임 요소, 주제 및 시공간 역학으로 인해 아직 탐구되지 않았다. 우리는 이 격차를 메우고 효과적인 인간 지침으로 기존 비디오 생성 모델을 높이고 보다 사용자 만족스럽고 고품질 비디오를 생성하기 위한 훈련 없이 사용자 친화적인 솔루션을 제공하는 것을 목표로 한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '도 3에 도시된 바와 같이, _InteractiveVideo_는 잠재 확산 모델을 기반으로 두 개의 생성 파이프라인으로 제어 가능한 비디오 생성을 실현한다 - 1) 텍스트-대-이미지(T2I) 파이프라인 \\(\\mathcal{P}_{img}\\) 및 **2)** 이미지-대-비디오(I2V) 파이프라인 \\(\\mathcal{P}_{video}\\) 프레임워크는 \\(N_{F}\\) 프레임 \\(\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots,\\mathbf{v}_{N_{F}}\\})을 포함하는 비디오를 출력한다. 우리는 이미지 명령어를 \\(\\mathbf{x}\\in\\mathbb{R}^{C\\times H\\times W\\), 콘텐츠 명령어를 \\(y\\), 모션 명령어를 \\(y^{\\prime}\\), 궤적 명령어를 \\(r\\)으로 나타낸다. 보다 구체적으로, Trajectory Instruction은 특정 객체의 원하는 이동 궤적을 나타내는 시작점과 끝점 및 영역 마스크에 의해 표현된다. 전체 파이프라인은 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\[\\{\\mathbf{v}_{1},\\mathbf{v}_{2},\\cdots,\\mathbf{v}_{N_{F}}\\}=\\mathcal{P}_{video}(\\mathcal{P}_{img}(\\mathbf{x},y),y^{\\prime},r). \\tag{1}\\}\n' +
      '\n' +
      '실제로, 텍스트 조건과 이미지 조건을 입력으로 하는 한 선반 외 T2I 모델로 \\(\\mathcal{P}_{img}\\)을 구현할 수 있다. I2V 모델의 입력인 중간 영상인 _i.e_를 표현하기 위해 \\(\\tilde{\\mathbf{x}\\)을 사용한다.\n' +
      '\n' +
      '그런 다음 I2V 파이프라인의 이미지 조건으로 \\(\\tilde{\\mathbf{x}\\)을 사용하고 텍스트 조건으로 Motion Instruction \\(y^{\\prime}\\)을 사용한다. 이미지 및 텍스트 조건이 필요한 기성품 I2V 확산 모델을 사용할 수 있다. I2V 모델의 영상 부호화기는 \\(\\mathcal{E}\\)이고, \\(\\mathbf{z}_{0}=\\mathcal{E}(\\tilde{\\mathbf{x})\\)은 해당 잠재 코드이고, \\(\\epsilon_{t}\\)은 단계 \\(t\\)에서 예측된 잡음이며, 고전적인 (_i.e_, 상호작용 없는) 비디오 잡음 제거 프로세스는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}\\cdot\\epsilon_{t}, \\tag{2}\\t}\n' +
      '\n' +
      '여기서 \\(\\bar{\\alpha}_{t}\\)는 분산 스케줄 [12]에 관련된 파라미터이다.\n' +
      '\n' +
      '###### 상승적 멀티모달 명령\n' +
      '\n' +
      '사용자의 동작에 따라 예측된 잡음을 수정함으로써 사용자의 멀티모달 명령으로 비디오 확산 과정을 제어한다. 개념적으로 사용자의 조작에 따라 \\(\\epsilon_{t}\\)이 변하는 함수를 나타내는 \\(R\\)으로 상호작용 제어 비디오 확산 과정을 표현할 수 있다.\n' +
      '\n' +
      '\\sqrt{\\bar{\\alpha}_{t}\\mathbff{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}\\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}\\cdot R(\\epsilon_{t}). \\tag{3}\\\n' +
      '\n' +
      '제안된 함수 \\(R(\\cdot)\\)에 대한 구체적인 구현은 사용자 상호작용을 잡음제거 잔차로 처리하여 실현된다. 중간 이미지\\(\\tilde{\\mathbf{x}\\)를 \\(\\mathcal{P}_{video}\\)의 조건 이미지로 활용하기 때문에 사용자와 영상 생성 모델 사이의 "인터페이스"로 볼 수 있다. 결과적으로, 본 프레임워크는 사용자들이 비디오 잡음 제거 프로세스의 새로운 생성 조건으로서 그들의 상호작용을 도입함으로써 타겟 비디오와 상호작용할 수 있게 한다.\n' +
      '\n' +
      '특히, 사용자의 동작을 노이즈 제거 잔차로 변환하여 비디오 확산 프로세스를 제어한다. 형식적으로 비디오 노이즈 제거 과정에서 원본 중간 이미지는 \\(\\tilde{\\mathbf{x}\\)이고, 해당 잠재 코드는 \\(\\mathbf{z}_{0}=\\mathcal{E}(\\tilde{\\mathbf{x})\\)이라고 가정한다. 사용자가 영상을 조작(예를 들어, 일부 선을 그거나 궤적 2를 설정)하면 중간 영상은 그에 따라 변하게 되고, 생성된 중간 영상은 \\(\\tilde{\\mathbf{x}^{\\prime}\\)으로 표시되며, 이에 대응하는 잠재 코드는 \\(\\tilde{\\mathbf{z}_{0}^{\\prime}=\\mathcal{E}(\\tilde{\\mathbf{x}^{\\prime})이 된다. 비디오 확산 과정에서 잡음을 예측하기 위해 \\(\\mathbf{z}_{0}^{\\prime}\\)을 사용한다. 형식적으로 시간 단계가 \\(t\\)이고, \\(\\epsilon_{t}\\)이 \\(\\mathbf{z}_{t-1}\\)으로 예측된 소음이고, \\(\\epsilon_{t}^{\\prime}\\)이 \\(\\mathbf{z}_{t-1}^{\\prime}\\)으로 예측된 소음이고, 우리가 사용하는 소음이 \\(\\epsilon_{t}^{\\prime}\\)으로 주어진 소음이다.\n' +
      '\n' +
      '각주 2: 그림 그리기와 궤적 그리기는 서로 다른 방식으로 중간 이미지에 영향을 미칩니다. 전자는 초기 이미지 명령어를 변경함에 따라 T2I 파이프라인을 통해 중간 이미지에 차이를 만든다. 후자는 특정된 영역 내의 핸들 포인트들을 타겟 포인트들로 이동하고 중간 이미지의 광학 흐름을 변경한다.\n' +
      '\n' +
      '\\[\\hat{\\epsilon}_{t}=\\lambda\\cdot\\epsilon_{t}+(1-\\lambda\\cdot\\epsilon_{t}^{ \\prime}\\, \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda\\)는 학습된 잡음 잔차 및 인간 지시의 균형을 맞추기 위한 하이퍼-파라미터이다. 그리고 잡음제거 과정에서 원본(\\epsilon_{t}\\) 대신 \\(\\hat{\\epsilon}_{t}\\)을 사용하여 최종 영상을 생성한다.\n' +
      '\n' +
      '이미지 상의 사용자 동작들 후에, 인간-조작된 불일치는 결과적인 비디오의 시간적 일관성에 영향을 미칠 수 있다는 점에 유의한다. 이는 사용자 조작들이 중간 이미지를 I2V 모델이 트레이닝된 분포로부터 벗어났을 수 있기 때문이다(_e.g_., 사용자는 I2V 모델의 트레이닝 데이터에서 이례적인 하늘에 태양을 생성하기 위해 트위스트 옐로우 곡선을 그렸다). 이를 해결하기 위해 영상 확산 과정이 완료되면 AnimateDiff[10]에 따라 결과 영상을 후처리한다. 각각의 프레임은 그룹 정규화[39] 계층, SiLU[11] 활성화 및 AnimateDiff 또는 PIA[50]로부터 채택된 2D 컨볼루션 계층을 통해 중간 이미지와 정렬되며, 이러한 구조는 사용자의 일반적인 동작에 의해 생성되는 우리의 공통 분포에 잘 일반화되는 것으로 밝혀졌다. 구체적으로, 최종적인 \\(i\\)번째 비디오 프레임 \\(\\mathbf{v}_{i}^{\\prime}\\)은 다음과 같이 계산될 수 있다:\n' +
      '\n' +
      '\\[\\mathbf{v}_{i}^{\\prime}=\\texttt{Conv2D}(\\texttt{SiLU}(\\texttt{GroupNorm}(\\mathbf{v}_{i}-\\tilde{\\mathbf{x}))\\, \\tag{5}\\)\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 개인화(SS 4.1), 세립 비디오 편집(SS 4.2), 정밀 모션 컨트롤(SS 4.3)을 포함하는 _Interactive-Video_ framework의 특징을 제시한다. 또한, 발전품질에 대한 정량적 분석(SS 4.4)과 사용자 만족도 분석을 프레임워크로 수행하였다. 그런 다음 프레임워크의 생성 효율(SS 4.5)을 입증한다.\n' +
      '\n' +
      '### 비디오 개인화\n' +
      '\n' +
      '기존의 방법[7, 10, 34]은 정적 이미지를 비디오로 애니메이션하는 데 상당한 진전을 이루었다. 그러나, 이러한 방법들은 원래의 정적 이미지들에 이미 존재하는 객체들 또는 장면들을 애니메이팅하는 것에 제한되며, 참조된 이미지들에 존재하지 않는 객체들 또는 장면들을 갖는 비디오를 생성하는 것에 관한 경우 어려움들을 직면한다. 즉, 기존의 방법들은 특히 사용자가 이전에 보지 못한 객체들 또는 장면들을 추가하거나 애니메이트하기를 원할 때 비디오 콘텐츠를 제어하는 능력이 제한적이다.\n' +
      '\n' +
      '_InteractiveVideo_를 사용하여 풍부한 요소를 통합하여 비디오 콘텐츠 조작을 가능하게 한다. 그림 4에서, 우리는 우리의 프레임워크가 사용자가 비디오 콘텐츠를 자유롭게 사용자 정의할 수 있도록 지원한다는 것을 보여준다. 예를 들어 그림 4의 (a), (b), (c)에서 각각 새, 파도, 극광의 스케치를 그릴 때 붓을 사용한다. 추가된 객체는 전체 전체에 걸쳐 매끄럽게 통합되고 애니메이션화됩니다.\n' +
      '\n' +
      '도 3: **프레임워크 일러스트레이션**. _InteractiveVideo_에서, 사용자들은 비디오 콘텐츠, 모션 및 궤적에 대한 생성 모델들과 상호작용하기 위해 멀티모달 명령어들을 활용할 수 있다.\n' +
      '\n' +
      '비디오 다음 프레임에서 볼 수 있는 _InteractiveVideo_는 참조된 이미지가 객체를 직접 포함하지 않더라도 사용자가 만족스러운 시간적 일관성의 비디오를 생성할 수 있게 한다. 한편, 이러한 사례들은 다양하고 매력적인 비디오 콘텐츠를 만드는 데 있어 프레임워크의 범용성과 적응성을 보여주며, 콘텐츠 생성 및 편집에서 광범위한 응용 분야에 대한 잠재력을 강조한다.\n' +
      '\n' +
      '세밀한 비디오 편집\n' +
      '\n' +
      '현재 생성 방법의 또 다른 중요한 한계는 정밀한 지역 편집을 수행해야 하는 과제이다. 생성 과정에서 모델은 "좌", "우", "상", "하"와 같은 자연어 참조를 해석하는데 어려움이 있다. 이것은 사용자 경험에 중요한 지역적 의미론을 정확하게 편집하는 것을 어렵게 만든다.\n' +
      '\n' +
      '다행히도 _InteractiveVideo_는 중간 영상에서 직관적인 조작을 가능하게 함으로써 이러한 한계를 극복한다. 도 5의 (a)에 예시된 바와 같이, 사용자들은 기존의 방법들을 이용하여 특정 나무의 색상을 편집하거나 특정 낙엽 클러스터의 색상을 제어하기 어렵다. 이와는 대조적으로, 본 프레임워크는 사용자가 임의의 영역에 대해 세밀한 의미 편집을 수행할 수 있게 한다. 예를 들어, 편집 과정을 거친 후, 도 5(a)의 수목, 도 5(b)의 구름, 도 5(c)의 로고를 용이하게 수정할 수 있다. 생성된 비디오는 사실적인 모션, 적절한 빛 반사 및 시각적으로 매력적인 텍스처를 특징으로 하는 고품질입니다.\n' +
      '\n' +
      '### 정밀 동작 제어\n' +
      '\n' +
      '움직임 제어, 특히 정밀한 움직임 제어는 공간-시간 패턴을 모델링하는 복잡성으로 인해 비디오 생성 분야에서 중요한 과제를 제기한다. 주요 어려움은 특히 실질적인 동작을 처리할 때 생성된 비디오의 시간적 일관성을 유지하는 데 있다. 이 문제는 주로 1D 시간적 주의의 제한된 시간적 수용 분야에서 비롯되며, 이는 시간에 따른 움직임 관련 변화의 전체 범위를 수용하기 위해 고군분투한다. 결과적으로, 생성된 비디오에서 모션의 매끄럽고 일관된 표현을 보장하는 것은 이 분야에서 상당한 장애물로 남아 있다. 다르게는, _InteractiveVideo_는 정밀한 모션 제어에서 탁월하며, 이는 다음과 같이 세 가지 측면에서 논의될 것이다:\n' +
      '\n' +
      '**1)** Large Motion. 그림 6의 첫 번째 두 행에서 볼 수 있듯이, 우리는 사실적 스타일과 만화 스타일 모두에서 캐릭터를 돌려서 큰 모션 컨트롤을 제시한다. 여성 캐릭터를 돌리는 디테일이 인상적이며, 머리카락의 모션이 매우 사실적으로 보입니다.\n' +
      '\n' +
      '도 4: **Video Content Manipulation** with _InteractiveVideo_. (a), (b), (c)에서는 새, 파도, 극광을 추가하여 내용 조작을 제시한다. 그런 다음, 추가된 객체들은 전체 비디오에서 구동된다. 이러한 결과는 비디오 콘텐츠 생성을 위한 프레임워크의 유연성을 보여주기 위해 사용된다.\n' +
      '\n' +
      '**2)** 정밀 모션. <그림 6>의 세 번째 줄에서 볼 수 있듯이, "INTERACTIVE VIDEO" 브랜드를 들고 있는 사랑스러운 코기는 꼬리 흔들기, 입을 벌린 채 미소짓기, 고개를 돌리기, 귀를 흔드는 등 여러 가지 매력적인 몸짓을 보여준다.\n' +
      '\n' +
      '**3)** 멀티 오브젝트 모션. 그림 6의 마지막 두 행은 다중 객체 움직임을 제어하는 _InteractiveVideo_의 능력을 보여준다. 우리의 프레임워크는 귀여운 소녀와 사랑스러운 강아지의 움직임을 정확하게 제어합니다. 개의 머리를 조절할 때 꼬리도 흔들리고, 소녀는 자연스럽게 손을 내린다. 이 두 물건을 통제하면서 소녀는 달달하게 웃고 개는 고개를 반대편으로 돌린다.\n' +
      '\n' +
      '### Quantitative Analysis\n' +
      '\n' +
      '**AnimateBench.** _InteractiveVideo_는 오픈 도메인 비디오 생성을 위한 일반적인 프레임워크이므로 비교를 위해 **AnimateBench**를 사용한다. 다양한 콘텐츠, 스타일 및 개념을 가진 105개의 고유한 사례를 사용하여 텍스트 기반 비디오 생성 능력을 평가했다. 이 사례는 철저한 비교를 위해 모델당 5개의 이미지가 있는 7개의 별개의 텍스트 대 이미지 모델을 사용하여 생성되었다. 또한, 잠재적인 단일 샷 이미지 모션에 초점을 맞추어 다양한 방법에 걸쳐 모션 제어 가능성을 평가하기 위해 각 이미지에 대한 세 가지 모션 관련 프롬프트를 제작했다.\n' +
      '\n' +
      '**평가 메트릭.** 이미지 및 텍스트 정렬을 고려하여 생성 품질을 평가하며, CLIP 점수를 사용하여 임베딩 간의 코사인 유사성을 측정한다. 이미지 정렬은 입력 이미지와 비디오 프레임 임베딩을 비교하는 반면 텍스트 정렬은 텍스트와 프레임 임베딩 유사성을 조사한다.\n' +
      '\n' +
      '**User Study.** 시각적 품질 및 사용자 경험 측면에서 본 방법의 향상을 입증하기 위해, 본 방법을 다른 비디오 모델과 비교하는 사용자 연구를 수행했다. 본 연구는 다양한 장면, 스타일, 사물을 특징으로 하는 AnimateBench의 40개의 프롬프트를 활용하였다. 기존의 비디오 생성 방법들과 비교했을 때, 우리의 _InteractiveVideo_는 인간의 선호도 점수 면에서 두드러지게 능가하고 사용자 만족도 면에서 최첨단 성능을 제공한다. 이러한 정량적 결과는 사용자 연구와 결합되어 대화형 생성 패러다임의 중요성과 우수성을 효과적으로 입증한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{CLIP Score} & \\multicolumn{2}{c}{User Study} & \\multicolumn{2}{c}{Satisfaction Rate} \\\\  & Image & Text & Image & Text & (\\%) \\\\ \\hline VideoComposer[35] & 225.3 & 62.85 & 0.180 & 0.110 & 43.5 \\\\ AnimateDiff[10] & 218.0 & 63.13 & 0.295 & 0.220 & 51.6 \\\\ PIA [50] & 225.9 & 63.68 & 0.525 & 0.670 & 52.5 \\\\ \\hline _InteractiveVideo_\\({}_{\\text{rec}}\\) & **234.6** & **65.31** & **0.745** & **0.813** & **72.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: AnimateBench에 대한 정량적 비교.\n' +
      '\n' +
      '도 5: **Fine-grained Video Editing** with _InteractiveVideo_. 본 논문에서는 (a), (b), (c)에서 특정 객체의 색상과 출현에 대한 세밀한 **영역** 의미 편집을 수행한다. 이러한 결과는 비디오 생성을 위한 프레임워크의 뛰어난 제어 가능성을 보여준다.\n' +
      '\n' +
      'user-centric designs.\n' +
      '\n' +
      '### Generation Efficiency\n' +
      '\n' +
      '추론 과정에서 16GB CUDA 메모리만 사용하며, 단일 RTX 4090에서 실행되며, 표 2에서도 _InteractiveVideo_의 지연시간을 보고한다. _InteractiveVideo_는 더 나은 제어성을 위해 두 개의 독립적인 확산 모델을 필요로 하지만 약 12초 이내에 비디오를 생성할 수 있다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '##5 책임 있는 AI 및 윤리 주장\n' +
      '\n' +
      'InteractiveVideo_를 개발할 때, 우리의 연구는 책임 있는 AI와 윤리 지침의 원칙을 엄격하게 준수한다. 비디오 생성을 위한 이 혁신적인 프레임워크는 윤리적 AI 관행에 대한 강력한 약속으로 설계되어 텍스트, 이미지 및 직접 조작을 통한 시스템과의 사용자 상호 작용이 최대한의 무결성과 투명성으로 처리되도록 한다. 우리의 시너지적 멀티모달 명령어 메커니즘의 구현은 이러한 원칙에 대한 우리의 헌신에 대한 증거이다. 다양한 사용자 입력의 원활한 통합을 촉진할 뿐만 아니라 AI가 윤리적 경계 내에서 작동하도록 하여 편견을 피하고 사용자 의도를 존중한다. 사용자가 비디오 생성 프로세스를 상호작용적으로 조작할 수 있도록 권한을 부여함으로써 _InteractiveVideo_는 창의성뿐만 아니라 AI 사용에 대한 책임도 촉진한다. 이 접근 방식은 AI에서 윤리적 표준을 준수하려는 우리의 약속과 일치하여 _인터랙티브 비디오_가 AI 주도 콘텐츠 생성 영역에서 책임 있는 혁신의 모델이 되도록 한다.\n' +
      '\n' +
      '##6 결론 및 논의\n' +
      '\n' +
      '요약하면, 우리는 사전 정의된 이미지 또는 텍스트 프롬프트에 의존하는 기존의 방법론보다 사용자 중심 접근법을 채택하는 비디오 생성 도메인의 새로운 패러다임 변화인 _InteractiveVideo_를 소개한다. 이 프레임워크는 텍스트 및 이미지 프롬프트, 수동 페인팅 및 드래그 앤 드롭 기능을 포함하지만 이에 제한되지 않는 직관적인 인터페이스 세트에 의해 인에이블되는 사용자와 생성 모델 사이의 동적, 실시간 상호작용을 용이하게 하는 능력에 의해 구별된다. 우리의 프레임워크의 핵심은 혁신적인 시너지적 멀티모달 명령어 메커니즘이며, 이는 다면적 사용자 상호 작용을 생성 프로세스에 응집적이고 효율적으로 통합하기 위한 약속의 증거이다. 이 메커니즘은 대화형 경험을 증가시키고 사용자가 생성 결과에 영향을 미칠 수 있는 세분성을 상당히 개선한다. 생성된 콘텐츠의 시각적 품질에서 결과적인 상승과 결합된 사용자가 주요 비디오 요소를 정확한 선호도에 맞게 세심하게 사용자 정의할 수 있는 결과적 능력은 비디오 생성 기술의 풍경에서 _InteractiveVideo_의 변환 잠재력을 강조한다.\n' +
      '\n' +
      '**계산 효율성에 대한 논의** _InteractiveVideo_에 의해 예고된 유망한 발전에도 불구하고, 사용자 중심 생성 접근법의 채택에는 도전이 없는 것은 아니다. 이 중 가장 중요한 것은 다양하고 역동적인 입력 시나리오에서 생성 모델의 효율성과 계산 효율성을 유지하는 것과 함께 광범위한 사용자 스펙트럼에 걸쳐 프레임워크의 접근성과 직관적인 사용성을 보장하는 것이다. 미래의 연구 노력은 유익하게 초점을 맞출 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline Process & Image Instruction & Context Instruction & Motion Instruction & Trajectory Instruction \\\\ \\hline Time & 19-34ms & 31.47ms & 12.22s & 77.35 ms \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: _InteractiveVideo_의 Latency Analysis.\n' +
      '\n' +
      '도 6: _InteractiveVideo_의 **Precise Motion Control**. 제안된 프레임워크는 대형 모션 제어, 정밀한 제스처 제어, 다중 객체 모션 제어에서 강력한 제어 가능성을 보여준다.\n' +
      '\n' +
      '확장성을 강화하기 위한 이러한 모델의 개선 및 사용자 의도를 보다 정확하게 해석하고 실현할 수 있는 적응형 알고리즘의 개발.\n' +
      '\n' +
      '**Future Works*** 우리는 몇 가지 유망한 방향을 파헤칠 수 있다. 감정적 의도 또는 추상적 개념과 같은 복잡한 사용자 입력에 대한 AI의 이해도를 향상시키면 더 미묘하고 맥락적으로 관련된 비디오 생성이 발생할 수 있다. 또한, 실시간 피드백 루프의 통합을 탐색하여 모델이 사용자 입력 이력에 기반한 창의적인 옵션을 제안하면 사용자 경험을 더욱 개인화할 수 있다. 가상 및 증강 현실 환경에서 이러한 프레임워크의 적용을 조사하는 것은 몰입형 콘텐츠 생성을 위한 새로운 차원을 열어준다. 또한, 여러 사용자가 상호 작용하고 단일 생성 프로세스에 기여할 수 있는 협업 생성을 포함하도록 프레임워크의 기능을 확장하면 디지털 미디어에서 공동 생성에 혁명을 일으킬 수 있다.\n' +
      '\n' +
      '추가 응용 프로그램입니다. InteractiveVideo_의 잠재적인 응용 프로그램은 교육 영역으로 잘 확장되며, 여기서 맞춤형 비디오 콘텐츠는 특히 대화형 내러티브의 생성에서 교육학적 경험과 엔터테인먼트를 상당히 풍부하게 할 수 있다. 이 프레임워크를 계속 반복하고 개선함에 따라 그 적용 범위는 무한한 것으로 보이며, 비디오 생성이 단순한 콘텐츠 생성을 초월하여 크리에이터와 디지털 캔버스 간의 깊고 상호 작용하는 참여를 위한 도관이 되는 미래를 예고한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:10]\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '그림 8: **풍경에 대한 기존 방법과의 비교**. 우리는 _InteractiveVideo_ (4번째 행)과 Pika Labs (1번째 행), I2VGen-XL (2번째 행), Gen-\\(2\\)(3번째 행)을 비교한다.\n' +
      '\n' +
      '그림 9: **동적 장면에 대한 기존 방법과의 비교**. 우리는 _InteractiveVideo_ (4번째 행)과 Pika Labs (1번째 행), I2VGen-XL (2번째 행), Gen-2 (3번째 행)을 비교한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22563-22575, 2023.\n' +
      '* [2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. _arXiv preprint arXiv:2307.03109_, 2023.\n' +
      '* [3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023.\n' +
      '* [4] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023.\n' +
      '* [5] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [6] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan. Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. _arXiv preprint arXiv:2311.15599_, 2023.\n' +
      '* [7] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Gransskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. _arXiv preprint arXiv:2302.03011_, 2023.\n' +
      '* [8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Gransskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7346-7356, 2023.\n' +
      '* [9] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, and Sean Bell. Tell me what happened: Unifying text-guided video completion via multimodal masked video generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10681-10692, 2023.\n' +
      '* [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [11] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.\n' +
      '* [16] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. _arXiv preprint arXiv:2205.15868_, 2022.\n' +
      '* [17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [18] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [19] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. _arXiv preprint arXiv:2302.12192_, 2023.\n' +
      '* [20] Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. _arXiv preprint arXiv:1811.07871_, 2018.\n' +
      '* [21] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In _Proceedings of the AAAI conference on artificial intelligence_, 2018.\n' +
      '* [22] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1096-1104, 2017.\n' +
      '* [23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [24] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from captions. In _Proceedings of the 25th ACM international conference on Multimedia_, pages 1789-1798, 2017.\n' +
      '* [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.\n' +
      '* [26] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10219-10228, 2023.\n' +
      '* [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [28] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [29] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3626-3636, 2022.\n' +
      '* [30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Computer Vision_, pages 2256-2265. PMLR, 2015.\n' +
      '* [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [32] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.\n' +
      '* [33] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [34] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [35] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. _arXiv preprint arXiv:2306.02018_, 2023.\n' +
      '* [36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* [37] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In _European Conference on Computer Vision_, pages 720-736. Springer, 2022.\n' +
      '* [38] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2096-2105, 2023.\n' +
      '* [39] Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV)_, pages 3-19, 2018.\n' +
      '* [40] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicfare: Animating open-domain images with video diffusion priors. 2023.\n' +
      '* [41] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagenward: Learning and evaluating human preferences for text-to-image generation. _arXiv preprint arXiv:2304.05977_, 2023.\n' +
      '* [42] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videoopt: Video generation using vq-vae and transformers. _arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* [43] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. _arXiv preprint arXiv:2308.08089_, 2023.\n' +
      '* [44] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. _arXiv preprint arXiv:2202.10571_, 2022.\n' +
      '* [45] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18456-18466, 2023.\n' +
      '* [46] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation, 2023.\n' +
      '* [47] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. _arXiv preprint arXiv:2206.05564_, 2022.\n' +
      '* [48] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. _arXiv preprint arXiv:2311.04145_, 2023.\n' +
      '* [49] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Metatransformer: A unified framework for multimodal learning. _arXiv preprint arXiv:2307.10802_, 2023.\n' +
      '* [50] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. _arXiv preprint arXiv:2312.13964_, 2023.\n' +
      '* [51] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>