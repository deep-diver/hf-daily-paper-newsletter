<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Orca-Math: 초등학교 수학에서 SLM의 잠재력 잠금 해제\n' +
      '\n' +
      '아린담 미트라1, 하메드 칸푸르, 코비 로셋, 아메드 아와달라\n' +
      '\n' +
      '각주 1: arindam.mitra@microsoft.com에 대한 대응\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 SLM이 200K 합성 수학 문제에 대해서만 훈련된 GSM8K에서 \\(\\sim 87\\%\\) pass@1에 도달할 수 있음을 보여준다. 수학적 단어 문제 해결은 오랫동안 작은 언어 모델(SLM)에 대한 복잡한 과제로 인식되어 왔다. 최근 연구에 따르면 GSM8K 벤치마크에서 80% 이상의 정확도를 달성하는 데 필요한 가장 작은 모델 크기는 340억 매개 변수이다. 더 작은 모델로 이러한 수준의 성능에 도달하기 위해 연구자는 종종 SLM을 훈련하여 파이썬 코드를 생성하거나 계산 오류를 방지하는 데 도움이 되는 도구를 사용한다. 또한 앙상블을 사용하여 최대 100개의 모델 런의 출력을 결합하여 보다 정확한 결과에 도달한다. 결과 선택은 합의, 다수결 또는 SLM과 함께 사용되는 별도의 검증자 모델을 사용하여 수행된다. 인셀링은 정확도의 상당한 증가를 제공하지만 모델에 대한 다수의 호출과 함께 상당한 비용 증가(예를 들어, Phi-GSM은 성능을 68.2에서 81.5로 향상시키기 위해 top-48을 사용하고, [38]은 LLAMA-2의 성능을 38.6%에서 71.9%로 향상시키기 위해 top-100을 사용한다.\n' +
      '\n' +
      '본 논문에서는 Mistral-7B 기반의 70억 파라미터 SLM인 Orca-Math를 제안한다. Orca-Math는 다수의 모델 호출이나 검증기, 코드 실행 또는 기타 외부 도구의 사용 없이도 GSM8k에서 86.81%를 달성한다. 본 논문에서 제안하는 방법은 (1) 에이전트가 협업하여 데이터를 생성하는 멀티 에이전트 셋업을 이용하여 생성된 200K 수학 문제의 고품질 합성 데이터셋, (2) SLM이 문제 해결을 연습하고, 그 해들에 대한 피드백을 받고, SLM 해와 피드백을 통합한 선호도 쌍으로부터 학습할 수 있는 반복 학습 기법이다. Supervised Fine-Tuning만으로 훈련하였을 때, Orca-Math는 GSM8k pass@1 metric에서 81.50%를 달성하였다. 반복적 선호도 학습을 통해, Orca-Math는 86.81% pass@1을 달성한다. Orca-Math는 LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5와 같은 상당히 큰 모델의 성능을 능가하며, 훨씬 작은 데이터(수십만 대 수백만 개의 문제)를 사용하면서도 다른 작은 모델보다 훨씬 우수한 성능을 보인다.\n' +
      '\n' +
      'Problem Setup\n' +
      '\n' +
      'GPT-4 [1]과 같은 프론티어 언어 모델은 이전에 더 작은 모델에서 볼 수 없었던 능력, 특히 추론하는 놀라운 능력(예: 언어 이해와 수학적 이해 모두를 필요로 하는 수학적 추론)을 입증했다. 이러한 기능은 모델 크기, 데이터 세트 크기 및 궁극적으로 학습에 필요한 계산량의 매우 큰 규모에 크게 기인한다.\n' +
      '\n' +
      '최근 몇 가지 연구는 작은 언어 모델(SLM)의 추론 능력을 향상시키는 데 초점을 맞추고 있다. 그럼에도 불구하고 추론 능력을 달성하기 위해 어느 정도의 척도가 필요한지는 여전히 열린 연구 질문이다.\n' +
      '\n' +
      'SLM의 추론 능력을 향상시키는 유망한 방향 중 하나는 GPT-4와 같은 프론티어 언어 모델을 사용하여 SLM을 훈련하는 데 사용할 수 있는 맞춤형 고품질 합성 데이터를 만드는 것이다. 훈련 데이터의 고품질과 더 풍부한 학습 신호(예: 설명)를 이끌어내는 능력은 훨씬 더 큰 규모로 이전에 나타났던 기술을 습득하는 데 SLM 능력을 크게 향상시키는 것으로 나타났다.\n' +
      '\n' +
      '이 패러다임은 큰 모델(교사)이 SLM(학생)이 배울 수 있는 시연을 만들고 있는 교사-학생 접근법에 부합한다. 이 연구에서 우리는 인기 있는 GSM8K 벤치마크를 사용하여 초등학교 수학 세계 문제에 대한 수학적 추론에 중점을 두고 이 방향을 추가로 탐구한다.\n' +
      '\n' +
      '최근 Phi-GSM[21], OVM[38] 등과 같은 SLM을 사용한 GSM8K에 대한 여러 다른 연구에서 긍정적인 결과가 입증되었다. 그러나, 이들 중 다수는 앙상블을 채용하는데, 여기서 최대 100개의 모델 런의 출력이 결합되어 보다 정확한 결과에 도달한다. 결과 선택은 산출물을 점수화/검증하고 최상의 답변을 선택하기 위해 별도의 검증자 모델을 사용하거나 합의, 다수결을 사용하여 수행된다. 인셀링은 상당한 정확도 향상을 제공한다(예를 들어, Phi-GSM은 성능을 68.2에서 81.5로 부스팅하기 위해 top-48을 사용하고, [22]는 LLAMA-2의 성능을 38.6%에서 71.9%로 부스팅하기 위해 top-100을 사용한다). 그러나, 모델에 대한 다수의 호출과 함께 비용이 크게 증가하며, 100개의 상이한 솔루션을 생성하고 검증하는 것은 모델에 대한 200개의 상이한 호출을 필요로 한다. 또한, 그들 중 일부는 매우 더 많은 양의 데이터(예를 들어, Phi-GSM의 경우 12M)를 사용하거나 계산 오류를 피하기 위해 도구 또는 코드를 사용한다.\n' +
      '\n' +
      '이 작업에서 우리는 다음과 같이 고품질 합성 훈련 데이터를 사용하여 교사-학생 패러다임을 반복 학습 설정으로 확장한다.\n' +
      '\n' +
      '* GPT-4-터보 솔루션과 쌍을 이루는 200K 수학 문제의 합성 데이터셋인 Orca-Math-dataset을 생성한다. 데이터 세트는 기존 문제를 패러프레이징할 뿐만 아니라 다양성과 어려움 모두에서 문제 세트를 확장하는 것을 목표로 하는 에이전트 기반 설정, 즉 에이전트-인스트럭션을 사용하여 생성되었다.\n' +
      '* 우리는 반복 학습 절차를 소개한다: (1) 시연에 대한 SLM을 훈련하기 위해 감독된 미세 조정을 위해 데이터 세트를 사용하고, (2) SLM이 다중 솔루션을 생성하는 것을 연습하도록 하고 (3) 선생님을 사용하여 학생들에게 피드백을 제공하는 것이다. 피드백은 학생이 생성한 풀이를 평가하거나 교사 풀이를 제공하는 형태로 온다.\n' +
      '\n' +
      '감독된 미세 조정만으로 pass@1 메트릭에서 GSM8k에서 81.50%를 달성한다. 반복 학습 루프는 pass@1을 86.81%로 더욱 향상시킨다. 여러 모델 호출이나 검증기 사용, 코드 실행 또는 기타 외부 도구가 필요하지 않습니다. LLAMA-2-70B(56.8%), WizardMath-70B(81.6%), Gemini Pro(86.5%) 및 GPT-3.5(77.4%)와 같이 훨씬 더 큰 모델을 초과했다. 가장 주목할 만한 것은 200K 예제(다른 데이터 세트보다 작은 크기의 순서)만으로 이 수준에 도달할 수 있다는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline Model & Base model & Model size & Answer format & Eval method & GSM8K (\\%) \\\\ \\hline  & & 7B & & 14.6 \\\\ Llama-2 [34] & - & 13B & nlp & pass@1 & 28.7 \\\\  & & 34B & & & 42.2 \\\\  & & 70B & & & 56.8 \\\\ \\hline  & & 7B & & & 66.5 \\\\ MetaMath [39] & Llama-2 & 13B & nlp & pass@1 & 72.3 \\\\  & & 70B & & & **82.3** \\\\ \\hline  & & 7B & & & 54.9 \\\\ WizardMath [23] & Llama-2 & 13B & nlp & pass@1 & 63.9 \\\\  & & 70B & & & **81.6** \\\\ \\hline  & Code-Llama & 7B & & & 59.4 \\\\ MAmmoTH [42] & Code-Llama & 12B & code & & 59.4 \\\\  & Code-Llama & 34B & & pass@1 & 64.7 \\\\  & Llama-2 & 70B & nlp & & 72.7 \\\\ \\hline  & & 7B & nlp & maj1@8 & 52.2 \\\\ Mistral [14] & - & 8\\(\\times\\)7B & & nlp & 58.4 \\\\ \\hline  & Llama-2 & 7B+7B & & nlp & verify100@1 & 73.7 \\\\  & & 7B+7B & & & **84.7** \\\\ \\hline  & & 7B & & & 36.4 \\\\  & & 34B & nlp & pass@1 & 51.5 \\\\ \\hline  & & 7B & & & 72.6 \\\\ ToRA-Code [12] & Llama-2 & 13B & code & COT@1 & 75.8 \\\\  & & 34B & & & **80.7** \\\\  & & 70B & & & **84.3** \\\\ \\hline  & & 7B & & & 55.72 \\\\ Orca 2 [27] & Llama-2 & 13B & nlp & pass@1 & 65.73 \\\\ \\hline  & & & & & **86.5** \\\\ Gemini Ultra [11] & - & - & nlp & maj1@32 & 94.4 \\\\ \\hline  & & & & & 77.4 \\\\ GPT-3.5-0613 [29] & - & - & code & pass@1 & **97.0** \\\\ \\hline  & & 1.3B & code & pass@1 & 44.6 \\\\ \\hline  & Phi-1.5-tiny & 125M & & & & 63.1 \\\\ Phi-GSM [21] & Phi-1.5-small & 350M & & & 65.9 \\\\  & Phi-1.5 & 1.3B & code & pass@1 & 68.2 \\\\  & Phi-2 & 2.7B & & & 74.3 \\\\ \\hline  & & Phi-1.5-tiny & 125M+125M & & & 68.9 \\\\ Phi-GSM+V [21] & Phi-1.5-small & 350M+350M & code & verify48@1 & 71.3 \\\\  & & Phi-1.5 & 1.3B+1.3B & & & **81.5** \\\\ \\hline  & & & & & \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: GSM8K에 대한 결과. 그 탁자는 [21]에서 용도 변경되었다. **굵은** 레이블은 정확성 **80% 초과**를 나타냅니다. "8\\(\\times\\)7B"라는 용어는 8명의 전문가의 혼합을 나타내며, 각 전문가는 7B 매개변수를 갖는다. 7B+7B\'는 7B 생성 모델과 7B 검증 모델의 결합을 의미한다. 검증자 모델의 추가는 \'+V\'로 표시된다.\n' +
      '\n' +
      '데이터셋 구축: 에이전트-인스트럭션\n' +
      '\n' +
      '이 단계의 목표는 쉬운 문제와 어려운 문제를 모두 포함하는 다양한 초등학교 수학 단어 문제를 만드는 것이다. 이 목표를 향해 우리는 다양한 에이전트를 만듭니다.\n' +
      '\n' +
      'Seed SetWe는 기존의 오픈 소스 데이터 세트, 즉 NumGLUE[26], AddSub[13], ALGES[17], ASDiv[24], DRAW[35], GSM8k[7], MATHQA[2], MultiArith[32], SingeOP[33], SingleEQ[16], SVAMP[30]에서 샘플 수학 단어 문제를 수집하는 것으로 시작한다. 우리는 총 36,217\\의 문제를 수집한다. 데이터 세트를 수집하기 위해 라일라[25] 벤치마크를 활용한다. 구체적으로, 우리는 기차에서 문제를 수집하고 종자 세트를 구성하기 위해 라일라의 검증 분할을 수집한다. 관심 있는 독자는 라일라[25]를 참조하세요.\n' +
      '\n' +
      '에이전트 - 질문 Me Anything 시드 세트의 각 문제에서 여러 단어 문제를 생성하여 시드 세트를 확장합니다. 우리는 문제 생성을 위해 후속 프롬프트를 활용한다.\n' +
      '\n' +
      '**당신의 목표는 주어진 단어 문제와 그 대답으로부터 여러 단어 문제를 만드는 것이다. 먼저 단어 문제의 문제를 진술로 변환합니다. 그런 다음 변환된 문제의 각 숫자에 대해 새로운 단어 문제를 만듭니다. 여기 몇 가지 예가 있다:**\n' +
      '\n' +
      '예시 1:** Q: 나탈리아는 4월에 48명의 친구들에게 클립을 팔았고, 5월에는 클립의 반을 팔았다. 나탈리아는 4월과 5월에 총 몇 개의 클립을 팔았나요?\n' +
      '\n' +
      'Answer: 72\n' +
      '\n' +
      '질문을 진술로 바꾸기: 나탈리아는 4월에 48명의 친구들에게 클립을 팔았고, 5월에는 절반의 클립을 팔았다. 나탈리아는 4월과 5월에 모두 72개의 클립을 팔았다.\n' +
      '\n' +
      '**All questions:**\n' +
      '\n' +
      '\\(<\\)target\\(>\\) 48\n' +
      '\n' +
      '나탈리아는 4월에 몇몇 친구들에게 클립을 팔았고, 5월에는 클립의 반을 팔았다. 나탈리아는 4월과 5월에 모두 72개의 클립을 팔았다. 그녀는 4월에 몇 개의 클립을 팔았나요?\n' +
      '\n' +
      '\\(<\\)target\\(>\\) half\n' +
      '\n' +
      '나탈리아는 4월에 48명의 친구들에게 클립을 팔았고, 5월에는 클립을 팔았다. 나탈리아는 4월과 5월에 모두 72개의 클립을 팔았다. 4월에 판매된 숫자 클립과 5월에 판매된 숫자 클립의 비율은 얼마입니까?\n' +
      '\n' +
      '\\(<\\)target\\(>\\) 72\n' +
      '\n' +
      '나탈리아는 4월에 48명의 친구들에게 클립을 팔았고, 5월에는 클립의 반을 팔았다. 나탈리아는 4월과 5월에 총 몇 개의 클립을 팔았나요?\n' +
      '\n' +
      '** 예 2:** Q: 웡은 베이비시터 비용으로 시간당 12달러를 번다. 어제, 그녀는 단지 50분 동안 아이를 돌보았다. 그녀는 얼마를 벌었습니까?\n' +
      '\n' +
      'Answer: 10\n' +
      '\n' +
      '문장으로 질문을 대체하는 것: 웡은 베이비시터로 시간당 12달러를 번다. 어제, 그녀는 단지 50분 동안 아이를 돌보았다. 10달러 벌었어\n' +
      '\n' +
      '**All questions:**\n' +
      '\n' +
      '\\(<\\)target\\(>\\) 12\n' +
      '\n' +
      '웡은 베이비시터로서 시간당 일정액을 번다. 어제, 그녀는 50분 동안 보모를 하고 10달러를 벌었다. 그녀는 시간당 얼마를 벌었는가?\n' +
      '\n' +
      '\\(<\\)target\\(>\\) 50\n' +
      '\n' +
      '웡은 베이비시터로 시간당 12달러를 번다. 어제 애 봐주고 10달러 벌었어 애 봐주는 데 얼마나 썼어? <타겟> 10\n' +
      '\n' +
      '<질문> 웡은 베이비시터로 시간당 12달러를 번다. 어제, 그녀는 단지 50분 동안 아이를 돌보았다. 그녀는 얼마를 벌었습니까?\n' +
      '\n' +
      '예시 3:** Q: 베티는 100달러짜리 새 지갑을 위해 돈을 모으고 있다. 베티는 필요한 돈의 절반만 가지고 있다. 그녀의 부모는 그녀를 위해 15개를 주기로 결심했고, 그녀의 조부모는 그녀의 부모보다 두 배나 더 많이 주었다. 베티가 지갑을 사려면 돈이 얼마나 더 필요해?\n' +
      '\n' +
      'Answer: 5\n' +
      '\n' +
      '문장으로 질문을 대체하는 것: 베티는 100달러짜리 새 지갑을 위해 돈을 저축하고 있다. 베티는 필요한 돈의 절반만 가지고 있다. 그녀의 부모는 그 목적을 위해 그녀에게 15달러를 주기로 결정했고, 그녀의 조부모는 그녀의 부모보다 2배 더 많이 주었다. 그녀는 지갑을 사기 위해 5개가 더 필요하다.\n' +
      '\n' +
      '**All questions:**\n' +
      '\n' +
      '<target> 100\n' +
      '\n' +
      '<질문> 베티는 새 지갑을 위해 돈을 모으고 있다. 베티는 필요한 돈의 절반만 가지고 있어 그녀의 부모는 그녀를 위해 15개를 주기로 결심했고, 그녀의 조부모는 그녀의 부모보다 두 배나 더 많이 주었다. 그녀는 지갑을 사기 위해 5개가 더 필요하다. 지갑의 가격은 얼마입니까?\n' +
      '\n' +
      '<target> half\n' +
      '\n' +
      '<질문> 베티는 100달러짜리 새 지갑을 위해 돈을 저축하고 있다. 그녀는 약간의 돈을 저축했고, 그녀의 부모님은 그녀에게 15달러를 주기로 결정했고, 그녀의 조부모님은 그녀의 부모님보다 두 배나 더 주셨다. 이제, 베티는 지갑을 사기 위해 5개가 더 필요하다. 베티가 처음에 저축한 돈의 지갑 가격에 대한 비율은 얼마인가?\n' +
      '\n' +
      '<target> 15\n' +
      '\n' +
      '<질문> 베티는 100달러짜리 새 지갑을 위해 돈을 모으고 있다. 베티는 필요한 돈의 반을 가지고 있고, 그녀의 부모님은 그녀에게 약간의 돈을 주기로 결정했고, 그녀의 조부모님은 그녀의 부모님보다 두 배 더 많이 주었다. 이제, 베티는 지갑을 사기 위해 5개가 더 필요하다. 그녀의 부모님은 그녀에게 얼마의 돈을 주셨나요?\n' +
      '\n' +
      '<target> twice\n' +
      '\n' +
      '<질문> 베티는 100달러짜리 새 지갑을 위해 돈을 모으고 있다. 베티는 필요한 돈의 절반만 가지고 있다. 그녀의 부모는 그 목적을 위해 그녀에게 15달러를 주기로 결정했고, 그녀의 조부모 또한 돈을 댔다 이제, 베티는 지갑을 사기 위해 5개가 더 필요하다. 부모님이 주신 금액 대비 조부모님이 주신 금액의 비율은 어떻게 되나요?\n' +
      '\n' +
      '<target> 5\n' +
      '\n' +
      '<질문> 베티는 100달러짜리 새 지갑을 위해 돈을 모으고 있다. 베티는 필요한 돈의 절반만 가지고 있다. 그녀의 부모는 그녀를 위해 15개를 주기로 결심했고, 그녀의 조부모는 그녀의 부모보다 두 배나 더 많이 주었다. 베티가 지갑을 사려면 돈이 얼마나 더 필요해?\n' +
      '\n' +
      '**Now solve this:**\n' +
      '\n' +
      '** 예제 4:** Q: 선생님께서 200점짜리 시험을 치르고 계십니다. 총 30개의 5점, 10점 문항이 있다. 그 시험에는 5점짜리 문항이 몇 개나 있나요? 정답: 20\n' +
      '\n' +
      '주목할 점은, "Ask Me Anything" 에이전트는 실시예 4에서의 시드에 기초하여 문제들을 발생하고 있다. 실시예 1 내지 3은 수-샷 시연으로서 제공된다. 이 에이전트는 총 120,445\\의 새로운 문제를 발생시킨다. 생성된 모든 문제는 시드 단어 문제와 유사한 내러티브를 보인다는 점에 유의하는 것이 중요하다. 이러한 단어 문제에 대한 해결책은 GPT4-Trubo를 사용하여 생성된다.\n' +
      '\n' +
      '에이전트 - 제안자 & 편집자는 도전적인 문제를 개발하여 시드 세트를 더욱 확장합니다. 이를 위해 _Suggester_와 _Editor_의 두 가지 새로운 에이전트를 소개한다. 이 설정은 미리 정의된 목표를 향해 맞춰진 데이터 세트를 생성하기 위해 협력적으로 작업하는 에이전트 팀을 설정하는 것을 포함한다. 이 경우 기존 문제를 수정하여 난이도를 높이는 것이 목적이다.\n' +
      '\n' +
      '_Suggester_는 특정 문제를 조사하고, 실제 문제를 생성하지 않고 복잡도를 높이기 위한 몇 가지 방법을 제안한다. 이어서, _Editor_는 원래의 단어 문제 및 Suggester의 추천을 취하여 업데이트되고, 더 도전적인 문제를 생성한다. 이러한 반복 프로세스는 여러 라운드에 걸쳐 발생할 수 있으며, 각 라운드는 이전에 생성된 문제의 복잡성을 더욱 증가시킨다. 우리는 AutoGen[37] 프레임워크를 활용하여 멀티 에이전트 워크플로우를 구현한다. 여기 반복 프로세스의 일례가 있다:\n' +
      '\n' +
      '씨앗 문제: 매시간 조앤은 쇼핑몰 안에 있는 분수에서 동전을 수집해야 합니다. 첫 시간 동안, 그녀는 15개의 동전을 모았다. 그 다음 두 시간 동안, 그녀는 분수에서 35개의 동전을 모았다. 4시간 동안, 그녀는 분수에서 50개의 동전을 모았지만, 탄산음료를 살 수 있도록 15개의 동전을 동료에게 주었습니다. 그녀는 4시간 후에 몇 개의 동전을 가지고 있었나요?\n' +
      '\n' +
      '** 제안자:** 시간 수를 늘리고, 각 시간마다 변수를 더 추가하고, 추가 트랜잭션을 통합합니다.\n' +
      '\n' +
      '매시간, 조앤은 A분수와 B분수라는 쇼핑몰 내부의 두 개의 분리된 분수에서 동전을 수집해야 합니다. 첫 시간 동안, 그녀는 A분수에서 20개, B분수에서 30개 동전을 수집했습니다. 다음 시간에는 이전 시간 동안 A분수에서 수집한 것보다 25개 더 많은 동전을 수집했고, 이전 시간 동안 B분수에서 수집한 것보다 10개 더 적은 동전을 수집했습니다. 세 번째 시간에는 두 번째 시간에 모은 것과 같은 수의 동전을 A분수에서, 이번에는 B분수에서 45개의 동전을 추가로 수집했다. 네 번째 시간에는 총 90개의 동전을 수집했지만, 그녀는 동료에게 탄산음료를 사기 위해 20개의 동전을, 다른 동료에게 40개의 동전을 주고 식사를 샀다. 5시간 후에 조앤은 15달러를 기부하기로 했어요\n' +
      '\n' +
      '** 제안자:** 분수를 더 많이 통합하고, 기간을 늘리고, 수집되고 사용되거나 기증된 동전에 대해 더 복잡한 계산을 추가한다.\n' +
      '\n' +
      '조앤은 8시간 동안 매시간 A분수, B분수, C분수, D분수 등 4개의 분수에서 동전을 수집해야 합니다. 첫 시간에는 A분수에서 20개의 동전, B분수에서 30개의 동전, C분수에서 40개의 동전, D분수에서 50개의 동전을 수집한다. 그 다음 시간에는 다음과 같은 변동이 있다.\n' +
      '\n' +
      '*분수 A의 경우, 그녀는 이전 시간보다 시간당 10개의 동전을 더 많이 모은다.\n' +
      '* B분수의 경우, 그녀의 컬렉션은 이전 시간에 비해 시간당 5개의 동전만큼 줄어든다.\n' +
      '* C분수의 경우, 그녀는 A분수에서 같은 시간에 수집된 동전 수에 비해 시간당 동전 수를 두 배로 모은다.\n' +
      '* 분수 D의 경우, 수집되는 동전의 수는 매시간 50개의 동전으로 일정하게 유지된다.\n' +
      '\n' +
      '세 번째 시간이 끝날 때 그녀는 20달러를 기부한다.\n' +
      '\n' +
      '7시간 동안, 그녀는 동료들과 동전을 공유하고 다음과 같이 나누어야 합니다: 1번 동전은 15개, 2번 동전은 20개, 70개를 보관합니다.\n' +
      '\n' +
      '8시간 후, 조앤은 모은 동전을 세기로 결심합니다. 8시간 말에 그녀가 몇 개의 동전을 가지고 있었는가? 우리는 GPT4-터보가 생성한 답이 \\(1800\\)자를 초과하는 문제들과 필터 문제들 당 두 번의 반복을 허용한다. 이 과정이 끝나면 우리는 \\(37,157\\) 문제를 수집한다.\n' +
      '\n' +
      '또한, 우리는 DMath로부터 야기된 \\(6,216\\) 문제들을 포함한다[15]. 이러한 문제들은 GPT4-Turbo에 의해 계산된 풀이가 정확한 금 표준 답과 일치하는 DMath 훈련 집합에 존재하는 \\(7,943\\) 문제의 부분 집합을 나타낸다.\n' +
      '\n' +
      '## 3 Training\n' +
      '\n' +
      '### 감독 미세조정 실험(반복 #1)\n' +
      '\n' +
      '우리는 Orca-Math-200K 데이터 세트에서 Mistral-7B를 세분화한다. 우리는 포장을 사용하지 않았다. 데이터는 다음의 명령어 포맷으로 제시된다:\n' +
      '\n' +
      'USER:\\n{question}\\n\\nASSISTANT:\\n{answer} Loss는 답변 토큰에서만 계산된다. 우리는 \\(1\\times 10^{-6}\\)의 일정한 학습률을 사용한다. 장치당 배치 크기는 \\(3\\)으로 설정됩니다. 교육은 8개의 A100 노드에 대해 하나의 에폭에 대해 수행되며 각 노드는 8개의 GPU를 포함한다.\n' +
      '\n' +
      '### 긍정신호와 부정신호에서 반복학습\n' +
      '\n' +
      'Dataset Construction Iteration #2는 각 문제에 대한 추가적인 양의 해와 음의 해를 생성하기 위해 반복 #1로부터 SFT-tuned 모델로부터 4개의 응답을 샘플링하며, 구체적으로 \\(\\mathit{top\\_p}=0.95\\)과 \\(\\mathit{temperature}=0.7\\)을 이용한다. 이 과정에서 각 문제들은 하나의 GPT4-Turbo 생성 해와 4개의 학생 생성 해로 구성된 데이터 셋이 생성된다. 이후 GPT4-Based-Exact-Match(자세한 내용은 섹션 4 참조)에 정의된 프롬프트를 사용하여 교사(GPT4-Turbo) 답변과 학생의 답변 간의 정렬을 평가합니다. 학생이 생성한 답이 교사의 답과 일치하지 않는 모든 해의 경우, 우리는 그것들을 _negative_로 표기하고, 그렇지 않으면 해를 _positive_로 표기한다. 그런 다음 선호도 데이터 세트를 다음과 같이 구성한다.\n' +
      '\n' +
      '* 각 질문에 대해 \\(q_{i}\\)은 \\(q_{i}^{+}\\), \\(q_{i}\\)에 대한 모든 _positive_해의 집합을 구성한다. 우리는 교사 해결책을 긍정적인 것으로 취급하므로, 이 구성에 의한 집합은 적어도 하나의 요소를 포함한다.\n' +
      '* 각 질문에 대해 \\(q_{i}\\)에 대한 모든 _negative_ 해의 집합인 \\(q_{i}^{-}\\)도 구성한다. 이 집합은 모든 \\(4\\) 응답이 교사의 해와 일치하면 비어 있을 수 있다. 사실, 이것은 약 \\(80k\\) 질문의 경우입니다. 이러한 상황에서, 우리는 \\(q_{j}^{-}\\)과 \\(|q_{j}^{-}|>0\\)에 대한 \\(q_{j}^{-}\\)의 반응을 무작위로 샘플링한다. 이 특수한 상황 \\(|q_{i}^{+}|=4\\)에 유의하라.\n' +
      '* Let, \\(Q_{i}=\\{(q_{i},a_{i}^{+},a_{i}^{-})|(a_{i}^{+},a_{i}^{-})\\in q_{i}^{+}\\times q_{i}^{-}}\\)은 \\(q_{i}\\) 주변의 선호도 데이터세트이다. 최종 선호도 데이터 세트는 훈련 데이터 세트에서 모든 \\(q_{i}\\)에 대한 \\(Q_{i}\\)의 조합을 취하여 생성된다.\n' +
      '\n' +
      'Dataset Construction Iteration #3M2는 Iteration #2에 대해 구축된 데이터셋에 \\(\\mathit{KTO}\\)[10]으로 훈련된 모델을 의미한다. Iteration #3에 대한 데이터셋 구축을 위해 동일한 절차를 반복하지만, 반복 #1에서 SFT-tuned 모델 대신 M2를 사용하여 네 가지 응답을 생성한다.\n' +
      '\n' +
      '긍정적 피드백과 부정적 피드백 모두에서 학습하기 위해 [31]에서 설명한 직접 선호 최적화(DPO)와 [10]에서 소개한 Kahneman-Tversky 최적화(KTO)의 두 가지 알고리즘의 성능을 평가했다. DPO는 선호도에 맞게 언어 모델을 효율적으로 미세 조정하기 위한 간단하고 인기 있는 접근법이다. 또한 출력 품질을 평가하기 위해 이진 "예" 또는 "아니오" 응답만 요구함으로써 자신을 구별하는 KTO의 기능을 조사했다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '우리는 정확한 일치를 미터법으로 사용한다. 모델 생성 답변이 주어지면 GPT4에게 최종 단답을 추출하고 금 단답과 일치시키라고 촉구한다. 우리는 이 메트릭을 _GPT4-based-Exact-Match_라고 부를 것이다. 다음 그림은 프롬프트 템플릿을 보여줍니다.\n' +
      '\n' +
      '**SYSTEM**\n' +
      '\n' +
      '전문 수학 교사로서, 여러분의 역할은 단어 문제에 대한 학생의 답을 평가하는 것입니다. 문제에는 문제 설정자가 제공하는 올바른 해결 방법이 수반된다. 단어 문제를 해결하기 위한 다양한 방법이 있을 수 있으므로 학생의 단계가 항상 문제 설정자의 해결책에 있는 단계와 일치하지 않을 수 있음을 기억하는 것이 중요하다. 그러나 일반적으로 숫자인 최종 답은 고유해야 하며 문제 설정자의 답과 일치해야 한다. 당신의 임무는 학생의 해결책을 분석하여 어떤 실수를 식별하고 오류를 수정하기 위해 답을 수정할 수 있는지 여부를 결정하는 것이다. 학생의 답변이 수정 불가능한 경우, 학생들의 이해도를 높이는 데 도움이 되는 연습 문제를 만드는 것을 고려하십시오.\n' +
      '\n' +
      '다음 형식을 사용합니다.\n' +
      '\n' +
      '오류 분석: 한 문장에서 문제 설정자의 해에서 최종 답을 추출하여 학생의 답과 비교한다. 일치해요?\n' +
      '\n' +
      '최종 평가: 정답/틀림\n' +
      '\n' +
      '**USER**\n' +
      '\n' +
      '질문: 빌리는 사람들이 세금을 내는 것을 돕기 위해 그의 시간을 자원하고 있다. 그는 하루에 3시간 동안 시간당 2명을 도울 수 있다. 만약 그가 3월 1일에서 4월 19일 사이의 날의 20%를 쉬고, 다른 날에는 사람들을 돕는다. 그가 얼마나 많은 사람들을 돕나요? (3월에 31일이 있다는 것을 기억하세요.)\n' +
      '\n' +
      '더 나은 답은?\n' +
      '\n' +
      '먼저 3월 1일부터 4월 19일 사이의 일수를 구한다 : 31일 + 19일 = 50일\n' +
      '\n' +
      '그런 다음 빌리가 이륙한 시간의 백분율을 100%에서 빼서 그가 일하는 시간의 백분율을 구하라: 100% - 20% = 80%\n' +
      '\n' +
      '그런 다음 그 수에 80%를 곱하여 빌리 작업 일수를 찾습니다: 50일 * 80% = 40일\n' +
      '\n' +
      '그럼 빌리가 하루에 얼마나 많은 사람들을 도울 수 있는지 찾아봐. 2명/시간 * 3시간/일 = 6명/일\n' +
      '\n' +
      '그런 다음 그가 하루에 돕는 사람의 수를 그가 돕는 총 사람의 수를 찾기 위해 일하는 일수에 곱하라: 6명/일 * 40일 = 240명\n' +
      '\n' +
      'Student answer:\n' +
      '\n' +
      '먼저 3월 1일부터 4월 19일 사이의 총 일수를 계산해 봅시다.\n' +
      '\n' +
      '3월은 31일, 4월은 30일입니다. 그래서 3월 1일부터 4월 19일까지의 총 일수는 다음과 같습니다.\n' +
      '\n' +
      '31(3월)+19(4월)=50일\n' +
      '\n' +
      '이제 빌리가 며칠을 쉬는지 계산해 봅시다. 그는 50일의 20%를 쉰다.\n' +
      '\n' +
      '50일의 20% = 0.20*50 = 10일\n' +
      '\n' +
      '그래서 빌리는 10일을 쉬어요. 이것은 그에게 맡겨진다: 50일 - 10일 = 40일 빌리는 하루에 3시간 동안 사람들을 돕고, 그는 시간당 2명을 도울 수 있다. 언젠가는 도와줄 수 있을 거야\n' +
      '\n' +
      '3시간/일 * 2명/시간 = 6명/일\n' +
      '\n' +
      '이제, 그가 40일 동안 얼마나 많은 사람들을 돕는지 계산해보자:\n' +
      '\n' +
      '40일 * 6명/일 = 240명\n' +
      '\n' +
      '따라서 빌리는 240명을 돕습니다.\n' +
      '\n' +
      '**ASSISTANT**\n' +
      '\n' +
      '오류 분석: 240명을 돕는 학생의 최종 답변이 문제와 일치함\n' +
      '\n' +
      'setter\'s solution.\n' +
      '\n' +
      '최종 평결: 정답\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '표 2는 1319개의 단어 문제를 포함하는 GSM8k 테스트 세트에 대한 여러 훈련 절차의 성능을 캡처한다. 우리는 최대 세 번의 반복을 위해 미스트랄-7B를 미세 조정한다. 첫 번째 반복에서는 M1을 얻기 위해 감독된 미세 조정을 사용하고 두 번째 반복에서는 SFT, DPO[31] 및 KTO를 비교한다. KTO 훈련된 모델은 이 그룹에서 더 나은 성능을 보인다. 이를 M2라고 하고 M2를 사용하여 반복 #3에 대한 데이터 세트를 생성하고, 세 번째 반복에 대해 M2 서버가 시작점으로 있는 DPO와 KTO를 비교한다. 또한 이를 Orca-Math-200K 데이터 세트에서 SFT 훈련의 세 시대와 비교한다. 모든 SFT 훈련에 대해 우리는 \\(1\\times 10^{-6}\\)의 일정한 학습률을 사용한다. 장치당 배치크기는 3, 장치당 배치크기는 1로 설정하였으며, DPO와 KTO 훈련작업은 베타 0.3, 장치당 배치크기는 3, 기울기-축적-단계는 11, 장치당 개수 1로 설정하였다. 반복시간 #2에서 DPO와 KTO 훈련은 반복시간 #3의 일정한 학습률(1\\times 10^{-6}\\)과 반복시간 #3의 일정한 학습률(1\\times 10^{-7}\\)을 사용하였다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      'Model Generated Positives 우리는 교사 생성 솔루션만을 포함하도록 \\(q_{i}^{+}\\)을 제한하여 _positives_를 생성하는 충격 모델을 연구한다. 다시 말해서, 반복 #2에 대한 데이터세트 생성에서 생성된 모델인 \\(a_{i}^{+}\\)을 제거한다. 표 3은 이 데이터세트에 대해 DPO와 KTO로 M1을 학습한 결과를 보여준다. 반복 #2를 위해 하이퍼파라미터를 재사용한다. 학습 알고리즘과는 무관하게, 상당한 성능 저하를 보인다.\n' +
      '\n' +
      '합성 네거티브 선호 데이터세트 생성은 M1 또는 M2에서 생성된 4개의 응답이 모두 _positive_인 상황에서 합성 _negative_ 생성을 포함한다. 우리 연구\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline Training Procedure & Pass@1 Accuracy on GSM8K Test set \\\\ \\hline SFT (M1) & 79.91 \\\\ SFT (M1) \\(\\rightarrow\\) SFT & 81.50 \\\\ SFT (M1) \\(\\rightarrow\\) DPO & 84.23 \\\\ SFT (M1) \\(\\rightarrow\\) KTO (M2) & 85.06 \\\\ SFT (M1) \\(\\rightarrow\\) SFT \\(\\rightarrow\\) SFT & 80.44 \\\\ SFT \\(\\rightarrow\\) KTO (M2) \\(\\rightarrow\\) DPO & 84.91 \\\\ SFT \\(\\rightarrow\\) KTO (M2) \\(\\rightarrow\\) KTO (Orca-Math) & **86.87** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 표는 GSM8k 테스트 세트에 대한 여러 반복 학습 실험 및 기준선의 성능을 캡처한다. SFT는 Orca-Math-200K 데이터 세트에 대한 교육의 한 시대를 의미한다. SFT\\(\\rightarrow\\) SFT는 Orca-Math-200K에 대한 두 시대의 훈련이다 SFT\\(\\rightarrow\\) DPO(KTO)는 M1을 시작으로 DPO(KTO)와 반복 #2에 대한 데이터세트에서의 훈련의 한 시기를 의미한다. SFT\\(\\rightarrow\\) SFT\\(\\rightarrow\\) SFT는 Orca-Math-200K에서의 훈련의 세 시기를 의미한다. SFT\\(\\rightarrow\\) KTO\\(\\rightarrow\\) DPO(KTO)는 M2를 시작으로 DPO(KTO)와 함께 반복 #3에 대한 데이터 셋에 대한 훈련의 한 시기를 의미하며, 평가를 위해 탐욕 디코딩을 사용한다.\n' +
      '\n' +
      '모든 표본 응답이 양수인 질문 \\(q_{i}\\)을 무시함으로써 이러한 합성 음수의 영향(표 4). 이것은 반복 #2의 질문 수를 약 \\(80k\\) 감소시키고 반복 #3의 질문 수를 약 \\(104k\\) 감소시킨다.\n' +
      '\n' +
      'Math Benchmark beyond GSM8k\n' +
      '\n' +
      '표 5는 여러 다른 단어 문제 데이터셋에 대한 Orca-Math의 성능을 나타낸다. 평가의 용이성을 위해 각 문제에 대한 답이 단일 수인 데이터 세트를 선택했다. 벤치마크들의 테스트 세트들은 라일라로부터 얻어진다. GPT4 기반 정확 매칭 메트릭을 사용하며, 그리디 디코딩을 이용하여 모델 응답을 생성한다.\n' +
      '\n' +
      '### Contamination Check\n' +
      '\n' +
      '우리는 훈련 중 또는 합성 문제 생성을 위한 종자로 GSM8K 또는 기타 데이터 세트의 테스트 분할을 사용하지 않는다. 그럼에도 불구하고 잠재적인 텍스트 오염을 감지하기 위해 다음과 같은 접근법을 취한다.\n' +
      '\n' +
      '1. 모든 문자를 소문자로 변환하고, 구두점을 제거하고, 텍스트를 개별 단어로 토큰화하고, 데이터의 균일성을 보장하기 위해 공통 영어 불용어를 제거하는 텍스트 전처리로 시작한다.\n' +
      '2. 우리는 TF-IDF(Term Frequency-Inverse Document Frequency) 방법을 이용하여 텍스트 코퍼스를 벡터화하고, 테스트와 트레이닝 세트 사이의 코사인 유사도를 결정하며, 이 중에서 각 테스트 쿼리에 대해 가장 유사한 상위 k개(k=10)의 질문을 선택한다.\n' +
      '3. 마지막으로, n-gram 중첩이 가장 높은 테스트 질문의 수를 미리 설정된 임계값 0.5 이상으로 카운트하여 해당 훈련 세트 일치로 텍스트 오염 정도를 평가한다. 우리는 자카드 유사성을 사용하여 텍스트 쌍 간의 n-그램의 중첩을 계산한다. 엄격한 오염 검사를 수행하기 위해 n=1로 설정했으며, 자카드 유사도를 사용하여 측정할 때 n-그램 겹침은 n의 비증가 함수라는 점에 유의하는 것이 중요하다.\n' +
      '4. 알고리즘을 실행한 결과 상당한 n-그램 중첩을 나타내는 테스트 질문의 수가 **8**이므로 정의된 임계값에 따라 테스트 세트 내에서 무시할 수 있는 텍스트 오염을 나타낸다. 제한할 때\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline M1 \\(\\rightarrow\\) DPO & 60.73 (\\(-\\)23.5) \\\\ M1 \\(\\rightarrow\\) KTO & 85.22 (\\(+\\)0.17) \\\\ M1 \\(\\rightarrow\\) KTO \\(\\rightarrow\\) KTO & 85.44 (\\(-\\)1.43) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 표는 샘플링된 모든 반응이 긍정적인 문제를 포함하는 것이 유익하다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|} \\hline\n' +
      '**Test Set** & **Orca-Math-Sft (M1)** & **Orca-Math** \\\\ \\hline Addsub & 88.99 & 91.74 \\\\ \\hline ASDiv & 91.10 & 91.10 \\\\ \\hline MultiArith & 98.28 & 98.28 \\\\ \\hline SingleOp & 98.74 & 99.37 \\\\ \\hline SingleIeq & 97.25 & 99.08 \\\\ \\hline Swamp structured & 87.63 & 91.30 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 시드 문제만을 포함하도록 Iteration #1(M1) 및 Orca-Math on AddSub, ASDiv, MultiArith, SingleOp, SingIEq, Svamp 구조화된 트레인 세트에서 SFT 트레이닝된 모델의 성능, 상당한 n-그램 중첩을 나타내는 테스트 질문의 카운트는 **7**이다. \\(n\\geq 2\\)에 대해, 상당한 n-그램 중첩을 나타내는 시험 질문의 카운트는 **0**임을 유의한다.\n' +
      '\n' +
      '##6 관련 작품\n' +
      '\n' +
      '생성 인공지능(AI) 모델을 통한 합성 데이터의 생성은 빠르게 진화해 왔다. 수많은 데이터 세트[27, 20, 28, 23, 9, 8, 45, 6, 36]가 전문 도메인과 일반 도메인 모두에 대해 제안되었으며 수학 관련 데이터 세트[40, 43, 44, 18]는 우리의 작업과 밀접한 관련이 있다.\n' +
      '\n' +
      '풍부한 신호에서 배우는 것도 최근에 큰 관심을 받았습니다. 여러 연구[31, 10, 22, 3, 5, 41]에서 선호도 학습의 유용성을 입증하였다. 본 연구에서는 초등학교 수준의 수학 영역에서 에이전트 기반 합성 데이터 생성과 반복 선호 학습에 대한 자세한 분석을 제시한다. 구체적으로, 우리는 DPO에 대한 KTO의 견고성과 모델 트레이닝을 개선하기 위해 모델 생성 포지티브를 사용하는 효과를 입증한다. 우리는 이것이 도전적인 영역에서 작은 언어 모델의 반복적인 학습과 자기 개선을 위한 예비 단계라고 믿는다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '본 연구는 스몰 언어 모델의 수학적 추론 능력이 실질적으로 향상될 수 있다는 강력한 증거를 제공한다. 반복 학습 기술을 사용하고 긍정적인 신호와 부정적인 신호를 모두 활용함으로써 GSM8k 벤치마크에서 이전에 인식된 80% 장벽을 성공적으로 극복했다. 200K 데이터로 훈련된 당사의 7B 모델은 86.81%의 인상적인 정확도를 달성했습니다. 또한 데이터 세트 생성에 에이전트를 통합하면 더 다양하고 흥미로운 데이터 세트를 생성할 수 있는 귀중한 접근법이 입증되었다. 이러한 결과는 SLM 성능의 상당한 개선 가능성을 강조할 뿐만 아니라 강력한 SLM 생성을 가속화하는 데 혁신적인 학습 전략 및 데이터 세트 생성 방법의 중요성을 강조한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [2] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _arXiv preprint arXiv:1905.13319_, 2019.\n' +
      '* [3] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences. _arXiv preprint arXiv:2310.12036_, 2023.\n' +
      '* [4] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. _arXiv preprint arXiv: 2310.10631_, 2023.\n' +
      '* [5] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. _arXiv preprint arXiv:2401.01335_, 2024.\n' +
      '* [6] Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehension. _arXiv preprint arXiv:2309.09530_, 2023.\n' +
      '* [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _arXiv preprint arXiv: Arxiv-2110.14168_, 2021.\n' +
      '\n' +
      '* [8] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\n' +
      '* [9] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.\n' +
      '* [10] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. _arXiv preprint arXiv:2402.01306_, 2024.\n' +
      '* [11] Google Gemini Team. Gemini: A family of highly capable multimodal models.\n' +
      '* [12] Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving. _arXiv preprint arXiv: 2309.17452_, 2023.\n' +
      '* [13] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 523-533, 2014.\n' +
      '* [14] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. _arXiv preprint arXiv: 2310.06825_, 2023.\n' +
      '* [15] Jiwoo Kim, Youngbin Kim, Ilwoong Baek, JinYeong Bak, and Jongwuk Lee. It ain\'t over: A multi-aspect diverse math word problem dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 14984-15011, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.927. URL [https://aclanthology.org/2023.emnlp-main.927](https://aclanthology.org/2023.emnlp-main.927).\n' +
      '* [16] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. _Transactions of the Association for Computational Linguistics_, 3:585-597, 2015.\n' +
      '* [17] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 271-281, 2014.\n' +
      '* [18] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large scale language model society, 2023.\n' +
      '* [19] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: **phi-1.5** technical report. _arXiv preprint arXiv:2309.05463_, 2023.\n' +
      '* [20] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". Openorca: An open dataset of gpt augmented flan reasoning traces. https://[https://huggingface.co/Open-Orca/OpenDrca](https://huggingface.co/Open-Orca/OpenDrca), 2023.\n' +
      '* [21] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving\\(>80\\%\\) on gsm8k with small language models. _arXiv preprint arXiv:2312.09241_, 2023.\n' +
      '* [22] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. _arXiv preprint arXiv:2309.06657_, 2023.\n' +
      '* [23] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv: 2308.09583_, 2023.\n' +
      '* [24] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. _arXiv preprint arXiv:2106.15772_, 2021.\n' +
      '\n' +
      '* [25] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. _arXiv preprint arXiv:2210.17517_, 2022.\n' +
      '* [26] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavedeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. _arXiv preprint arXiv:2204.05660_, 2022.\n' +
      '* [27] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdabiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. Orca 2: Teaching small language models how to reason, 2023.\n' +
      '* [28] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '* [29] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [30] Arkil Patel, Satwik Bhattacharya, and Navin Goyal. Are nlp models really able to solve simple math word problems? _arXiv preprint arXiv:2103.07191_, 2021.\n' +
      '* [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* [32] Subhro Roy and Dan Roth. Solving general arithmetic word problems. _arXiv preprint arXiv:1608.01413_, 2016.\n' +
      '* [33] Subhro Roy, Tim Vieira, and Dan Roth. Reasoning about quantities in natural language. _Transactions of the Association for Computational Linguistics_, 3:1-13, 2015.\n' +
      '* [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv: 2307.09288_, 2023.\n' +
      '* [35] Shyam Upadhyay and Ming-Wei Chang. Draw: A challenging and diverse algebra word problem set. Technical report, Citeseer, 2015.\n' +
      '* [36] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.\n' +
      '* [37] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* [38] Fei Yu, Anningzhe Gao, and Benyou Wang. Outcome-supervised verifiers for planning in mathematical reasoning. _arXiv preprint arXiv: 2311.09724_, 2023.\n' +
      '* [39] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv: 2309.12284_, 2023.\n' +
      '* [40] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.\n' +
      '\n' +
      '* [41] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.\n' +
      '* [42] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. _arXiv preprint arXiv: 2309.05653_, 2023.\n' +
      '* [43] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. _arXiv preprint arXiv:2309.05653_, 2023.\n' +
      '* [44] Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Templatemath: Syntactic data generation for mathematical problems, 2024.\n' +
      '* [45] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. _arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>