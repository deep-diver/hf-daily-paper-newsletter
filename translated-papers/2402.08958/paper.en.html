<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers\n' +
      '\n' +
      'Junhan Kim\n' +
      '\n' +
      'Kyungphil Park\n' +
      '\n' +
      'Chungman Lee\n' +
      '\n' +
      'Ho-young Kim\n' +
      '\n' +
      'Joonyoung Kim\n' +
      '\n' +
      'Yongkweon Jeon\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called _aespa_ is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive experiments on various language models and complexity analysis, we demonstrate that _aespa_ is accurate and efficient in quantizing Transformer models.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Model size has been gradually growing, resulting in deep generative models such as diffusion (Rombach et al., 2022) and large-scale language models (LLMs) (Touvron et al., 2023; Zhang et al., 2022) becoming more mainstream; the trend of AI is transitioning from discriminative models to generative models with numerous parameters in trillions.\n' +
      '\n' +
      'With the explosive growth (or enlargement) of model complexity (parameters), the performance of AI models has been advancing and is now approaching or even exceeding human intelligence levels. However, this growth in scale has resulted in a corresponding increase in computational costs on both mobile devices and the cloud, which necessitates the efficient processing and compression of AI models. Interestingly, one tries to expand the complexity of AI models to scale up performance, whereas the other aims to compress models to reduce cost.\n' +
      '\n' +
      'Quantization is a promising solution and indispensable procedure facilitating the efficient deployment of AI models on devices that mainly support fixed-point arithmetic. By reducing the precision of weight, the memory bandwidth requirements can be relieved, and the embarrassing parallelism of the quantized models can be SIMDified using highly efficient vector processing units such as NPU. To minimize inevitable performance degradation caused by quantization, we can choose one of two approaches: quantization-aware training (QAT) (Esser et al., 2019; Jung et al., 2019) and post-training quantization (PTQ) (Nagel et al., 2020; Li et al., 2021). Considering the model complexity and required resources such as training costs and available datasets, QAT is not practical for compressing models with billions of parameters. Consequently, recent quantization works on hyper-scale Transformer (Vaswani et al., 2017) models have focused more on PTQ.\n' +
      '\n' +
      'While existing PTQ schemes have successfully quantized relatively small models, such as ResNet (Nagel et al., 2020; Hubara et al., 2021; Li et al., 2021; Frantar and Alistarh, 2022; Jeon et al., 2022), they also have difficulty handling large-scale models because of their time and space complexity. An efficient backpropagation-free PTQ algorithm has been proposed as a cost-effective alternative (Frantar et al., 2023). However, its performance is somewhat limited because it does not consider inter-layer dependency and is reliant on the nearest rounding. There is an accuracy-efficiency trade-off; thus, we aim to bridge the gap toward next-level quantization of hyper-scale Transformer models.\n' +
      '\n' +
      'In this paper, we propose a novel post-training quantization algorithm, called _aespa_,1 that pursues both accuracy and efficiency. The key idea of _aespa_ is to perform quantization layer-wise for efficiency while considering cross-layer dependency by refining quantization loss functions to preserve the attention score.\n' +
      '\n' +
      'Footnote 1: _aespa_: attention-centric efficient and scalable post-training quantization algorithmOur contributions are summarized as follows:\n' +
      '\n' +
      '* We propose a new quantization strategy that balances accuracy and efficiency. Our scheme reconstructs the attention output to consider the cross-layer dependency while quantizing models layer-wise to pursue efficiency.\n' +
      '* To accelerate the quantization process, we propose refined quantization objectives for the attention module. Through a complexity analysis, we demonstrate that about 10 times faster quantization than existing block-wise approaches can be achieved by exploiting the proposed objectives.\n' +
      '* From extensive experiments on various language models, we demonstrate that our approach outperforms conventional quantization schemes by a significant margin, particularly for low-bit precision (INT2).\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'Recent studies on PTQ have mostly attempted to minimize the loss degradation (\\(\\Delta\\mathcal{L}\\)) incurred by quantization rather than the quantization error itself (\\(\\Delta\\mathbf{W}\\)). Consider a pre-trained neural network parameterized by weights \\(\\mathbf{W}\\). When we denote the task loss of the network as \\(\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W})\\), the problem of quantizing weights \\(\\mathbf{W}\\) to minimize the loss perturbation is formulated as\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}}\\ \\ \\mathbb{E}\\left[\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W}+ \\Delta\\mathbf{W})-\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W})\\right], \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{X}\\), \\(\\mathbf{Y}\\), and \\(\\Delta\\mathbf{W}\\) are the input, output, and weight perturbations caused by quantization, respectively.\n' +
      '\n' +
      'Using the second-order Taylor series, the loss perturbation \\(\\Delta\\mathcal{L}\\) in (1) can be approximated as\n' +
      '\n' +
      '\\[\\Delta\\mathcal{L}\\approx\\mathbb{E}[\\Delta\\mathbf{w}^{T}\\cdot\\mathbf{g}^{(\\mathbf{w})} +\\frac{1}{2}\\Delta\\mathbf{w}^{T}\\cdot\\mathbf{H}^{(\\mathbf{w})}\\cdot\\Delta\\mathbf{w}], \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbf{w}\\) is the flattened weights, and \\(\\mathbf{g}^{(\\mathbf{w})}\\) and \\(\\mathbf{H}^{(\\mathbf{w})}\\) are the gradient and Hessian related to \\(\\mathbf{w}\\), respectively. Assuming that the network is well-converged, the first term can be omitted, and thus (1) is simplified to (LeCun et al., 1989)\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{w}}\\ \\ \\mathbb{E}\\left[\\frac{1}{2}\\Delta\\mathbf{w}^{T}\\cdot \\mathbf{H}^{(\\mathbf{w})}\\cdot\\Delta\\mathbf{w}\\right]. \\tag{3}\\]\n' +
      '\n' +
      'Because computing and storing the exact Hessian matrix \\(\\mathbf{H}^{(\\mathbf{w})}\\) is infeasible, (3) has often been relaxed to the following layer-wise (4a) or block-wise reconstruction problem (4b) by assuming independence between different layers or blocks (e.g., a residual and Transformer block) (Nagel et al., 2020; Li et al., 2021):\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}^{(\\ell)}}\\ \\mathbb{E}\\left[\\left\\|\\mathcal{Q}(\\mathbf{W}^{( \\ell)})\\mathbf{X}-\\mathbf{W}^{(\\ell)}\\mathbf{X}\\right\\|_{F}^{2}\\right], \\tag{4a}\\] \\[\\min_{\\Delta\\mathbf{W}^{(\\ell)}}\\ \\mathbb{E}\\left[\\left\\|f^{(\\ell)} \\left(\\mathcal{Q}(\\mathbf{W}^{(\\ell)}),\\mathbf{X}\\right)-f^{(\\ell)}\\left(\\mathbf{W}^{( \\ell)},\\mathbf{X}\\right)\\right\\|_{F}^{2}\\right], \\tag{4b}\\]\n' +
      '\n' +
      'where \\(\\mathbf{W}^{(\\ell)}\\) is the weights of the \\(\\ell\\)-th layer or the \\(\\ell\\)-th block \\(f^{(\\ell)}\\) and \\(\\mathcal{Q}\\) denotes the quantization function parameterized by the scale \\(s\\), zero-point \\(z\\), and bit-width \\(n\\). For a uniform quantization, \\(\\mathcal{Q}\\) can be defined as\n' +
      '\n' +
      '\\[\\mathcal{Q}(x)=s\\left(\\text{clamp}\\left(\\left\\lfloor\\frac{x}{s}\\right\\rfloor+z,0,2^{n}-1\\right)-z\\right), \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\left\\lfloor\\cdot\\right\\rfloor\\) represents the round-off.\n' +
      '\n' +
      'Various quantization methods have been proposed to solve (4a) and (4b). These methods can be classified into two categories: 1) learning a rounding policy to assign each weight to a "proper" quantization grid rather than an adjacent grid such as AdaRound (Nagel et al., 2020), AdaQuant (Hubara et al., 2021), and Brecq (Li et al., 2021) and 2) learning quantization parameters (scale \\(s\\) and zero-point \\(z\\)) satisfying (4a) or (4b) such as AWQ (Lin et al., 2023) and OmniQuant (Shao et al., 2023). Hybrid approaches that jointly learn the rounding policy and parameters have also been proposed in Genie (Jeon et al., 2023). In general, quantization schemes targeting block-wise reconstruction (_e.g.,_Brecq, OmniQuant, and Genie) outperform methods aimed at layer-wise reconstruction (_e.g.,_ AdaRound and AWQ) and exhibit reasonable performance even for a low bit-width (_e.g.,_INT2) (Li et al., 2021).\n' +
      '\n' +
      'Although these schemes have successfully quantized DNNs, their application to large-scale Transformers with billions of parameters is challenging. Because of the repetitive attention operations required to compute the reconstruction error in every iteration of the quantization process (see (4b) and Section 3.5), more than 10 hours are required to quantize relatively small language models, such as OPT-1.3b (see Table 5 in Section 4). To mitigate this computational overhead, OPTQ (or GPTQ), which does not rely on gradient-based optimization, has been proposed for large-scale Transformer models (Frantar et al., 2023). OPTQ dramatically accelerates the quantization process and enables the quantization of large-scale Transformers using only a single NVIDIA A100 GPU (Frantar et al., 2023). However, its performance is somewhat limited because they are based on layer-wise reconstruction, and thus cannot consider inter-layer dependency (_e.g.,_ dependency between query, key, and value) (Jeon et al., 2023).\n' +
      '\n' +
      'Thus, we propose a novel PTQ scheme that balances accuracy and efficiency. In summary, our scheme targets block-wise reconstruction to consider the inter-layer dependency but quantizes models layer-wise for efficiency. We refine the block-wise objective function in (4b) into a computationally efficient form such that the computational overhead caused by repeated attention operations can be reduced significantly.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      'Before presenting the proposed _aespa_, we consider the layer-wise reconstruction to gain insight into our approach. Let \\(\\Delta\\mathbf{W}^{(\\ell)}=\\mathcal{Q}(\\mathbf{W}^{(\\ell)})-\\mathbf{W}^{(\\ell)}\\) be the weight quantization error, then the layer-wise reconstruction error in (4a) can be expressed as (we omit \\(\\ell\\) for simplicity)\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\left\\|\\mathcal{Q}(\\mathbf{W})\\mathbf{X}\\!-\\!\\mathbf{W}\\mathbf{X} \\right\\|_{F}^{2}\\right]\\!=\\!\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}\\mathbf{X}\\right\\|_ {F}^{2}\\right]\\\\ =\\!\\mathbb{E}\\left[\\operatorname{tr}\\!\\left(\\Delta\\mathbf{W}\\mathbf{X} \\mathbf{X}^{T}\\Delta\\mathbf{W}^{T}\\right)\\right]\\\\ =\\!\\operatorname{tr}\\!\\left(\\Delta\\mathbf{W}\\!\\cdot\\!\\mathbb{E}\\left[ \\mathbf{X}\\mathbf{X}^{T}\\right]\\!\\cdot\\!\\Delta\\mathbf{W}^{T}\\right)\\!. \\tag{6}\\]\n' +
      '\n' +
      'Consequently, the layer-wise quantization problem can be recast as follows:\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}}\\ \\operatorname{tr}\\left(\\Delta\\mathbf{W}\\cdot\\mathbb{E}\\left[ \\mathbf{X}\\mathbf{X}^{T}\\right]\\cdot\\Delta\\mathbf{W}^{T}\\right). \\tag{7}\\]\n' +
      '\n' +
      'The new form of the quantization objective in (7) implies that if \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) is pre-computed and stored before quantization, we can measure the reconstruction error over the entire calibration dataset with a single matrix multiplication and element-wise multiplication.2 This is in contrast to the original formulation in (4a) which requires the computation of \\(\\mathcal{Q}(\\mathbf{W})\\mathbf{X}\\) or \\(\\Delta\\mathbf{W}\\mathbf{X}\\) for every input sequence \\(\\mathbf{X}\\).\n' +
      '\n' +
      'Footnote 2: We note that the computation of \\(\\operatorname{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}^{T})\\) can be implemented as \\(\\operatorname{torch}\\!\\cdot\\!\\operatorname{sum}((\\mathbf{A}\\mathbf{B})\\odot \\mathbf{C})\\), where \\(\\odot\\) denotes the element-wise product operation. They are mathematically equivalent.\n' +
      '\n' +
      'A natural question that arises from this finding is _"Can we also measure the block reconstruction error efficiently based on such a pre-computation?"_. In the following subsections, we describe our main strategy for simplifying block-wise quantization and then introduce a refined quantization objective for the attention module, where the objective can be computed efficiently with certain pre-computed values (such as \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) in (7)).\n' +
      '\n' +
      '### Quantization Strategy of _aespa_\n' +
      '\n' +
      'When quantizing the attention module using conventional block-wise reconstruction methods (Figure 1(a)), the query, key, and value projection matrices (\\(\\mathbf{W}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V}\\)) have been jointly optimized such that\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}_{Q},\\Delta\\mathbf{W}_{K},\\Delta\\mathbf{W}_{V}}\\ \\mathbb{E}\\left[\\left\\| \\operatorname{SA}(\\widehat{\\mathbf{Q}},\\widehat{\\mathbf{K}},\\widehat{\\mathbf{V}})\\!- \\!\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\right\\|_{F}^{2}\\right], \\tag{8}\\]\n' +
      '\n' +
      'where the output of attention module \\(\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\) is defined as\n' +
      '\n' +
      '\\[\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\operatorname{softmax}\\left(\\frac{ \\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right)\\mathbf{V}=\\mathbf{A}\\mathbf{V}.\\]\n' +
      '\n' +
      'In such a case, we need to compute \\(\\operatorname{SA}(\\widehat{\\mathbf{Q}},\\widehat{\\mathbf{K}},\\widehat{\\mathbf{V}})\\) for every batch in each iteration, which is computationally heavy and time-consuming (see Section 3.5 for details on complexity).\n' +
      '\n' +
      'To overcome this computational overhead, we quantize each projection matrix _separately_ in a divide-and-conquer manner. For example, when we quantize the query projection matrix \\(\\mathbf{W}_{Q}\\), we fix \\(\\mathbf{W}_{K}\\) and \\(\\mathbf{W}_{V}\\) to full-precision (Figure 1(b)), which facilitates the factoring out of common terms affected by \\(\\mathbf{W}_{K}\\) and \\(\\mathbf{W}_{V}\\) (see Section 3.3 for further details). We emphasize that this strategy differs from conventional layer-wise quantization schemes (e.g., AdaRound, OPTQ, and AWQ) in that we aim to minimize the reconstruction error for the attention module, not the reconstruction error for each layer.\n' +
      '\n' +
      'Figure 1: Overview of the proposed _aespa_. Each weight is quantized separately to reconstruct the attention output.\n' +
      '\n' +
      'We conduct experiments to demonstrate the importance of targeting block-wise reconstruction and the validity of the proposed separate quantization strategy. In our experiments, we learn a weight-rounding policy using conventional AdaRound, but we set the loss function for each projection matrix as the attention reconstruction error in (8) (not the layer reconstruction error; see Figure 2(c)). Table 1 summarizes the performances of the OPT-125M model quantized using AdaRound, Brecq, and our approach. As evident, our approach uniformly outperforms AdaRound for all bit-widths, although both methods quantize models layer-wise. This is because we can consider cross-layer dependency (i.e., the relationship between the query, key, and value) by targeting attention output reconstruction, which is different from AdaRound wherein layers are considered independent. Furthermore, once we target block-wise reconstruction, separate layer-wise quantization does not result in severe performance degradation compared to the joint quantization method (Brecq). Our approach causes only a marginal performance degradation for 2-bit quantization and exhibits comparable performance for 3-bit and 4-bit quantization (see Table 1).\n' +
      '\n' +
      'One might ask whether our strategy incurs more computational cost than that required by joint quantization methods because we update only one layer at a time with one attention operation (see Figure 2(c)). This is in contrast to conventional methods, where all layers inside the attention module are updated simultaneously (Figure 2(b)). To reduce this additional cost, we refine the quantization objective in (8) into a computationally efficient form, as described in the following subsections.\n' +
      '\n' +
      '### Quantization Objective for _aespa_\n' +
      '\n' +
      'Under the proposed separate quantization strategy, the quantization objective in (8) differs depending on which projection matrix is quantized. We present the simplified quantization objective for each projection matrix.\n' +
      '\n' +
      '**Value projection matrix** When quantizing the value projection matrix \\(\\mathbf{W}_{V}\\), the query and key projection matrices are fixed as full-precision matrices. In this case, the reconstruction error \\(\\Delta\\mathrm{SA}_{V}\\) is expressed as\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{V}=\\mathbb{E}\\left[\\left\\|\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{ \\widehat{V}})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\right\\|_{F}^{2}\\right].\\]\n' +
      '\n' +
      'By factoring out the common term influenced by \\(\\mathbf{Q}\\) and \\(\\mathbf{K}\\), we simplify \\(\\Delta\\mathrm{SA}_{V}\\) as follows:\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{V} =\\mathbb{E}\\left[\\left\\|\\mathbf{A}\\mathbf{\\widehat{V}}-\\mathbf{A}\\mathbf{V}\\right\\| _{F}^{2}\\right]=\\mathbb{E}\\left[\\left\\|\\mathbf{A}\\Delta\\mathbf{V}\\right\\|_{F}^{2}\\right]\\] \\[=\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}_{V}\\mathbf{X}\\mathbf{A}^{T}\\right\\| _{F}^{2}\\right].\\]\n' +
      '\n' +
      'Thus, the problem of quantizing \\(\\mathbf{W}_{V}\\) to minimize the attention reconstruction error can be formulated as\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}_{V}}\\ \\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}_{V}\\mathbf{X}\\mathbf{A}^{T} \\right\\|_{F}^{2}\\right]. \\tag{9}\\]\n' +
      '\n' +
      '**Query projection matrix** When we fix key and value projection matrices with full-precision, the reconstruction error \\(\\Delta\\mathrm{SA}_{Q}\\) caused by \\(\\Delta\\mathbf{W}_{Q}\\) is expressed as\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{Q} =\\mathbb{E}\\left[\\left\\|\\mathrm{SA}(\\mathbf{\\widehat{Q}},\\mathbf{K},\\mathbf{ V})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\right\\|_{F}^{2}\\right] \\tag{10}\\] \\[=\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{A}\\mathbf{V}\\right\\|_{F}^{2}\\right], \\tag{11}\\]\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\[\\Delta\\mathbf{A}=\\mathrm{softmax}\\left(\\frac{\\mathbf{\\widehat{Q}}\\mathbf{K}^{T}}{\\sqrt{d}} \\right)-\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right). \\tag{12}\\]\n' +
      '\n' +
      'To avoid repetitive softmax operations in every quantization iteration, we utilize the following first-order Taylor series approximation for \\(\\Delta\\mathbf{A}\\):\n' +
      '\n' +
      '\\[\\Delta\\mathbf{A}\\approx\\frac{\\Delta\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\cdot\\mathbf{J}_{ \\mathrm{softmax}}^{T}, \\tag{13}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & Quantization & Reconstruction & W2A16 & \\multicolumn{2}{c}{W3A16} & \\multicolumn{2}{c}{W4A16} \\\\ \\cline{3-10}  & Granularity & Target & WikiText-2 & C4 & WikiText-2 & C4 & WikiText-2 & C4 \\\\ \\hline AdaRound & **Layer-wise** & **Layer Output** & 160.7 & 95.63 & 35.44 & 31.86 & 29.51 & 27.78 \\\\ Brecq & **Block-wise** & **Attention Output** & 58.80 & 45.68 & 32.69 & 29.70 & 29.07 & 27.42 \\\\ \\hline\n' +
      '**Proposed** & **Layer-wise** & **Attention Output** & **62.69** & **48.20** & **31.93** & **29.73** & **28.73** & **27.41** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Performance comparison of different quantization strategies. Perplexity (\\(\\downarrow\\)) of the quantized OPT-125M model is reported.\n' +
      '\n' +
      'Figure 2: Quantization strategies (simplified)\n' +
      '\n' +
      'where \\(\\mathbf{J}_{\\mathrm{softmax}}\\) is the Jacobian matrix of the softmax function. By combining (11) and (13), we obtain\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{Q} \\approx\\frac{1}{d}\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{Q}\\mathbf{K}^{T} \\mathbf{J}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\right\\|_{F}^{2}\\right]\\] \\[=\\frac{1}{d}\\mathbb{E}\\left[\\left\\|\\mathbf{V}^{T}\\mathbf{J}_{\\mathrm{ softmax}}\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right]. \\tag{14}\\]\n' +
      '\n' +
      'Although we can circumvent conducting attention operations using the modified form in (14), it requires a large amount of memory to store the Jacobian matrix \\(\\mathbf{J}_{\\mathrm{softmax}}\\); more than 100 GB of memory is required to store \\(\\mathbf{J}_{\\mathrm{softmax}}\\) even for small language models such as OPT-125M.3 As a cost-effective alternative, we build an upper bound of (14) and then employ it as a surrogate of \\(\\Delta\\mathrm{SA}_{Q}\\) when quantizing \\(\\mathbf{W}_{Q}\\). Specifically, we note that\n' +
      '\n' +
      'Footnote 3: Note that the shape of \\(\\mathbf{J}_{\\mathrm{softmax}}\\) is \\([L,L,L]\\) (\\(L\\) is the input sequence length) for each attention head since \\(\\mathbf{J}_{\\mathrm{softmax}}(\\mathbf{a}_{\\ell})=\\mathrm{diag}(\\mathbf{a}_{\\ell})- \\mathbf{a}_{\\ell}^{T}\\mathbf{a}_{\\ell}\\in\\mathbb{R}^{L\\times L}\\) for each row \\(\\mathbf{a}_{\\ell}\\) of \\(\\mathbf{A}\\).\n' +
      '\n' +
      '\\[\\left\\|\\mathbf{V}^{T}\\mathbf{J}_{\\mathrm{softmax}}\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X} \\right\\|_{F}^{2}\\] \\[\\leq\\left\\|\\mathbf{V}^{T}\\mathbf{J}_{\\mathrm{softmax}}\\right\\|_{F}^{2} \\cdot\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\]\n' +
      '\n' +
      'and the term \\(\\left\\|\\mathbf{V}^{T}\\mathbf{J}_{\\mathrm{softmax}}\\right\\|_{F}^{2}\\) remains unchanged in the quantization process. Thus, we minimize \\(\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\) with the hope that \\(\\Delta\\mathrm{SA}_{Q}\\) also becomes small. In other words, our quantization objective for \\(\\mathbf{W}_{Q}\\) is formulated as\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}_{Q}}\\ \\mathbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X} \\right\\|_{F}^{2}\\right]. \\tag{15}\\]\n' +
      '\n' +
      '**Key projection matrix** By taking similar steps to develop the quantization objective in (15) for the query projection matrix, we can refine the quantization objective for the key projection matrix \\(\\mathbf{W}_{K}\\) as follows:\n' +
      '\n' +
      '\\[\\min_{\\Delta\\mathbf{W}_{K}}\\ \\mathbb{E}\\left[\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X} \\right\\|_{F}^{2}\\right]. \\tag{16}\\]\n' +
      '\n' +
      'For the sake of completeness, we include the detailed derivation in Appendix A.\n' +
      '\n' +
      '### Algorithm Description\n' +
      '\n' +
      'The proposed _aespa_ algorithm consists of two main steps. Specifically, _aespa_ first determines the quantization parameters (i.e., scale and zero-point), and then optimizes a quantization grid (or integer weight \\(\\mathbf{W}_{int}\\)) for each weight parameter.\n' +
      '\n' +
      '```\n' +
      '1:def Quantization(\\(\\mathbf{W}\\),\\(\\mathbf{X}\\))\n' +
      '2:Approximate the Hessian \\(\\mathbf{H}\\)\\(\\triangleright\\) See Table 2\n' +
      '3:Estimate \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}],\\mathbb{E}[\\mathbf{Q}^{T}\\mathbf{Q}]\\) for \\(\\mathbf{W}_{Q},\\mathbf{W}_{K}\\)\\(\\triangleright\\) Table 2\n' +
      '4:Set the step size \\(\\mathbf{S}\\) s.t. \\(\\min_{\\alpha}\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}\\Delta\\mathbf{W}^{T}\\right)\\)\n' +
      '5:repeat\n' +
      '6:Compute the Loss \\(\\mathcal{L}\\)\\(\\triangleright\\) Table 2\n' +
      '7:Optimize \\(\\mathbf{S}\\) or \\(\\mathbf{W}_{int}\\) w.r.t \\(\\mathcal{L}\\) by certain algorithm\n' +
      '8:until converged\n' +
      '9:return \\(\\mathbf{S}\\) and \\(\\mathbf{W}_{int}\\)\\(\\triangleright\\) step size and integer weight\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1** Quantization\n' +
      '\n' +
      'Note that we assumed only layer-wise quantization and used only the definition of the attention operation when developing the refined objective functions in (9), (15), and (16). Therefore, our objective functions can be integrated into any layer-wise quantization scheme without effort. For example, we can compute quantization parameters by combining existing parameter initialization algorithms (e.g., AWQ (Lin et al., 2023) and Z-Fold(Jeon et al., 2023)) with our objective functions. In addition, quantization grids can be optimized using conventional weight-rounding methods (e.g., AdaRound (Nagel et al., 2020) and AdaQuant (Hubara et al., 2021)) together with the proposed refined objectives (see Appendix C for further details). In the proposed _aespa_, we utilize Z-Fold to compute the quantization parameters and employ AdaRound to learn a weight-rounding policy.4 In Algorithm 1, we summarize the proposed _aespa_.\n' +
      '\n' +
      'Footnote 4: We use the layer-wise objective function in (7) for the weights other than the query, key, and value projection matrices (i.e., out-projection layer and weights inside the feed-forward network).\n' +
      '\n' +
      'To accelerate the weight-rounding learning process, we further modify the objective functions such that the value can be computed efficiently via pre-computation, as in (6).\n' +
      '\n' +
      '**Modified objective for (9)** Note that\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}_{V}\\mathbf{X}\\mathbf{A}^{T}\\right\\|_{F} ^{2}\\right]\\] \\[\\qquad\\quad=\\mathbb{E}\\left[\\mathrm{tr}\\left(\\Delta\\mathbf{W}_{V}(\\mathbf{X }\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T})\\Delta\\mathbf{W}_{V}^{T}\\right)\\right]\\] \\[\\qquad\\quad=\\mathrm{tr}\\left(\\Delta\\mathbf{W}_{V}\\mathbb{E}\\left[\\mathbf{X }\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{V}^{T}\\right). \\tag{17}\\]\n' +
      '\n' +
      'The modified objective in (17) implies that we can efficiently compute the loss for rounding policy learning by computing \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\) before quantization and reusing it in the learning process.5 Therefore, the term \\(\\left\\|\\Delta\\mathbf{W}_{V}\\mathbf{X}\\mathbf{A}^{T}\\right\\|_{F}^{2}\\) need not be computed for every input sequence \\(\\mathbf{X}\\), and each iteration of the gradient-based optimization can be performed efficiently (with one simple matrix multiplication and a single element-wise multiplication; see Footnote 2).\n' +
      '\n' +
      'Footnote 5: We do not need to update \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\) as it is not affected by \\(\\Delta\\mathbf{W}_{V}\\) and thus fixed in the learning process.\n' +
      '\n' +
      'Another intriguing feature of this modification is that it facilitates a more reliable update of \\(\\Delta\\mathbf{W}_{V}\\) than the objective\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Layer & \\(\\mathbf{H}\\) & \\(\\mathcal{L}\\) \\\\ \\hline \\(\\mathbf{W}_{Q}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right]\\cdot\\Delta\\mathbf{W} \\mathbf{H}_{Q}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\(\\mathbf{W}_{K}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\mathbb{E}\\left[\\mathbf{Q}^{T}\\mathbf{Q}\\right]\\cdot\\Delta\\mathbf{W} \\mathbf{H}_{K}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\(\\mathbf{W}_{V}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}_{V}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ Others & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Row-wise Hessian \\(\\mathbf{H}\\) and quant. loss \\(\\mathcal{L}\\) for each layer function in (9). Specifically, because \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\) is precomputed using all the calibration data, the loss computed using (17) considers the entire calibration dataset (i.e., the batch size is the total number of data). Consequently, a better estimate of the true gradient can be obtained without any memory issues, which could result in more consistent updates of \\(\\Delta\\mathbf{W}_{V}\\) and faster convergence (Smith et al., 2018).\n' +
      '\n' +
      'Furthermore, the modified objective in (17) implies that the Hessian matrix \\(\\mathbf{H}_{V}\\) corresponding to each row of \\(\\mathbf{W}_{V}\\) is\n' +
      '\n' +
      '\\[\\mathbf{H}_{V}=\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]. \\tag{18}\\]\n' +
      '\n' +
      'We note that our Hessian matrix in (18) differs from\n' +
      '\n' +
      '\\[\\mathbf{H}=\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}], \\tag{19}\\]\n' +
      '\n' +
      'which is commonly used as an approximated Hessian matrix in conventional methods (Frantar and Alistarh, 2022; Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023)). The key reason for the different Hessian is that we target attention-wise reconstruction, not layer-wise reconstruction, to consider the dependency between \\(\\mathbf{W}_{Q}\\), \\(\\mathbf{W}_{K}\\), and \\(\\mathbf{W}_{V}\\) whereas the previous methods assumed independence. To observe the effect of considering the cross-layer dependency, we compare the performance of conventional Z-Fold under different Hessian matrices (see Table 3). As evident, the quantization performance is significantly enhanced by exploiting the Hessian in (18), which demonstrates the importance of considering cross-layer dependency.\n' +
      '\n' +
      '**Modified objectives for (15) and (16)** If we denote the vectorized representation of \\(\\Delta\\mathbf{W}_{Q}\\) as \\(\\Delta\\mathbf{w}_{Q}\\), then the objective function in (15) can be expressed as (see Appendix B for the proof)\n' +
      '\n' +
      '\\[\\mathbb{E}\\Big{[}\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\|_{F}^{2}\\Big{]}= \\Delta\\mathbf{w}_{Q}^{T}\\cdot\\mathbb{E}\\big{[}\\mathbf{X}\\mathbf{X}^{T}\\otimes\\mathbf{K}^{T} \\mathbf{K}\\big{]}\\cdot\\Delta\\mathbf{w}_{Q}. \\tag{20}\\]\n' +
      '\n' +
      'where \\(\\otimes\\) is the Kronecker product operation. To reduce the memory cost required to store the term \\(\\mathbb{E}\\big{[}\\mathbf{X}\\mathbf{X}^{T}\\otimes\\mathbf{K}^{T}\\mathbf{K}\\big{]}\\), we approximate it as (Botev et al., 2017)\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\otimes\\mathbf{K}^{T}\\mathbf{K}\\right]\\approx\\mathbb{E }\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\otimes\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right]. \\tag{21}\\]\n' +
      '\n' +
      'By combining (20) and (21), we obtain\n' +
      '\n' +
      '\\[\\mathbb{E}\\Big{[}\\big{\\|}\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\big{\\|}_{F}^ {2}\\Big{]}\\\\ =\\mathbf{w}_{Q}^{T}\\cdot\\left(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right] \\otimes\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right]\\right)\\cdot\\Delta\\mathbf{w}_{Q}\\\\ \\stackrel{{(a)}}{{=}}\\operatorname{tr}\\!\\left( \\mathbb{E}\\big{[}\\mathbf{K}^{T}\\mathbf{K}\\big{]}\\Delta\\mathbf{W}_{Q}\\mathbb{E}\\left[\\mathbf{X} \\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{Q}^{T}\\right). \\tag{22}\\]\n' +
      '\n' +
      'The proof of (a) is provided in Appendix B. By taking similar steps, the objective function for the key projection matrix can be recast as\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\big{\\|}\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\big{\\|}_{F}^ {2}\\right]\\\\ =\\operatorname{tr}\\!\\left(\\mathbb{E}\\big{[}\\mathbf{Q}^{T}\\mathbf{Q}\\big{]} \\Delta\\mathbf{W}_{K}\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{K}^{T} \\right). \\tag{23}\\]\n' +
      '\n' +
      'The modified objective functions in (22) and (23) imply that the loss over the total calibration dataset can be efficiently calculated by computing \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\), \\(\\mathbb{E}[\\mathbf{Q}^{T}\\mathbf{Q}]\\), and \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) in advance.\n' +
      '\n' +
      '### Complexity Analysis for _aespa_\n' +
      '\n' +
      'We discuss the computational complexity of _aespa_. Specifically, we analyze the number of floating-point operations (flops) required to perform each iteration based on the proposed loss functions in (17), (22), and (23) (line 6 in Algorithm 1) and compare it with conventional block-wise quantization methods.\n' +
      '\n' +
      'The complexity of _aespa_ for each projection matrix is summarized as follows.\n' +
      '\n' +
      '* **Value**: Using the pre-computed matrix \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\), the loss in (17) can be computed with one matrix multiplication and one element-wise multiplication/addition (see Footnote 2). The associated cost is \\(2d_{h}d^{2}+d_{h}d-1\\) flops, where \\(d\\) is the hidden size and \\(d_{h}\\) is the input dimension for each attention head.\n' +
      '* **Query/key**: Once \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\), \\(\\mathbb{E}[\\mathbf{Q}^{T}\\mathbf{Q}]\\), and \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) are computed in advance, the loss functions in (22) and (23) can be computed by performing two matrix multiplications and one element-wise multiplication/addition step. This requires \\(2d_{h}d^{2}+2d_{h}^{2}d-1\\) flops for each projection matrix.\n' +
      '\n' +
      'To summarize, _aespa_ requires total\n' +
      '\n' +
      '\\[\\mathcal{C}_{\\textit{aespa}}=6d_{h}d^{2}+4d_{h}^{2}d+d_{h}d-3=\\mathcal{O}(d_{h} d^{2}) \\tag{24}\\]\n' +
      '\n' +
      'flops to perform one quantization iteration for each attention head. We emphasize that the loss considering the entire dataset can be computed with \\(C_{\\textit{aespa}}\\) flops, regardless of the total amount of calibration data.\n' +
      '\n' +
      'We now compare the complexities of _aespa_ and conventional block-wise quantization methods. It can be easily\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Approximated Hessian} & \\multicolumn{2}{c}{W2A16} & \\multicolumn{2}{c}{W3A16} & \\multicolumn{2}{c}{W4A16} & \\multicolumn{2}{c}{W4A16} \\\\ \\cline{2-10}  & WikiText-2 & PTB & C4 & WikiText-2 & PTB & C4 & WikiText-2 & PTB & C4 \\\\ \\hline \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) (conventional) & 179.5 & 273.7 & 129.7 & 38.74 & 56.24 & 33.21 & 30.84 & 44.25 & 28.90 \\\\ \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\) **(ours)** & **152.9** & **202.4** & **114.4** & **35.26** & **48.54** & **31.20** & **29.28** & **41.47** & **27.52** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Performance of Z-Fold with different Hessian matrices. Perplexity (\\(\\downarrow\\)) of OPT-125M after the quantization is reported.\n' +
      '\n' +
      'verified that existing methods require (see Appendix D)\n' +
      '\n' +
      '\\[\\mathcal{C}_{exist} =B(6d_{h}dL+4d_{h}L^{2}+2L^{2}-L-1) \\tag{25}\\] \\[=\\mathcal{O}(Bd_{h}L\\cdot\\max\\{d,L\\})\\]\n' +
      '\n' +
      'flops for handling \\(B\\) input sequences of length \\(L\\). Table 4 summarizes the computational costs for different sizes of the OPT models. For the conventional methods, we report the cost of using four batch sequences in each iteration (\\(B=4\\)). We observe that the computational cost of _aespa_ is considerably lower than that of conventional methods. In particular, for small-scale models, _aespa_ performs ten times fewer number of flops. One can notice that the gap between \\(\\mathcal{C}_{\\textit{aespa}}\\) and \\(\\mathcal{C}_{exist}\\) decreases as the model size increases. This is because the hidden size \\(d\\) exceeds the sequence length \\(L\\) (fixed for all models) as the model size increases. Nevertheless, _aespa_ still incurs a lower computational cost, and the gap would increase if conventional methods use larger batch sizes.\n' +
      '\n' +
      '## 4 Experimental Results\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'We quantize publicly available LLMs including OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022), and LLMaA (Touvron et al., 2023)) using the proposed _aespa_. In our experiments, we combine the proposed loss functions in Table 2 with Z-Fold and AdaRound to determine the quantization parameters and optimize the quantization grids; however, we emphasize that _aespa_ is compatible with any quantization schemes. When computing quantization parameters, we set the stopping criterion for Z-Fold as that considered by (Jeon et al., 2023b).6 When optimizing the quantization grids, we set the number of iterations, learning rate, and the weight of the rounding loss in AdaRound to 2,000, 0.015, and 1.5, respectively.\n' +
      '\n' +
      'Footnote 6: That is, we stop the alternating updates of Z-Fold when the loss perturbation \\(\\Delta\\mathcal{L}\\) increases after the update or the total number of updates reaches 30.\n' +
      '\n' +
      'As a calibration dataset, we use 128 random 2048 token segments from the C4 dataset (Raffel et al., 2020) as in (Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023), which implies that we do not utilize any task-specific data for quantization. In our experiments (except for Table 7), we quantize only weights and retain activations in full-precision as in (Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023) because activations are not a significant bottleneck for LLMs (Frantar et al., 2023) and the inference of LLMs can be accelerated sufficiently by reducing memory movement through weight quantization (Kim et al., 2023).\n' +
      '\n' +
      'We evaluate the performance of the quantized LLMs using three benchmark datasets (WikiText-2 (Merity et al., 2016), C4 (Raffel et al., 2020), and PTB (Marcus et al., 1993)) and several zero-shot tasks. All experiments have been conducted with a single NVIDIA A100 GPU (80 GB).\n' +
      '\n' +
      '### Comparison with Prior Arts\n' +
      '\n' +
      '**Comparison with block-wise PTQ schemes** First, we compare the proposed _aespa_ with Brecq, a representative block-wise quantization scheme. The estimated quantization time includes the time required for pre-computation (lines 2-4 in Algorithm 1). When implementing Brecq, we employ the hyper-parameter settings provided in the original Brecq paper (Li et al., 2021). As evident from Table 5, _aespa_ completes quantization considerably faster than Brecq, which is consistent with our complexity analysis in Table 4. For example, while Brecq requires more than 10 hours to quantize the model with 1B parameters, _aespa_ completes quantization in 2 hours, which demonstrates that the quantization process can indeed be accelerated by exploiting the proposed loss functions in Table 2.\n' +
      '\n' +
      'One may wonder why the performance of the models quantized using Brecq worsens as the model size increases. We conjecture that this is attributable to the choice of hyper-parameters (e.g., learning rate and weight of rounding loss). In fact, the hyper-parameters presented in (Li et al., 2021) have been optimized for ImageNet, but not for LLMs. It is expected that we can obtain better performance for Brecq via deliberate hyper-parameter tuning; however, this would not be feasible for real-world deployment because it requires considerable time (only two hyper-parameter tunings are possible per day with one GPU).\n' +
      '\n' +
      'Next, we compare _aespa_ with OmniQuant, a recently proposed PTQ scheme that learns quantization parameters together with foldable parameters (Shao et al., 2023). For OmniQuant, we run the official code7 and report the obtained results because different calibration datasets were used by (Shao et al., 2023). The results presented in Table 6\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & 125M & 350M & 1.3B & 2.7B & 6.7B & 13B \\\\ \\hline \\(\\mathcal{C}_{exist}\\) & 6.7 & 7.5 & 11 & 15 & 34 & 41 \\\\ \\(\\mathcal{C}_{\\textit{aespa}}\\) & 0.24 & 0.42 & 1.6 & 3.2 & 13 & 20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Cost of _aespa_ and conventional methods (GFLOPS)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Quantization} & \\multicolumn{2}{c}{W2A16} \\\\ \\cline{3-5}  & & Time (hr) & Wiki2 & PTB & C4 \\\\ \\hline \\multirow{2}{*}{OPT-125M} & Brecq & 1.81 & 58.80 & 82.96 & 45.68 \\\\  & _aespa_ & 0.11 & 70.32 & 109.1 & 55.96 \\\\ \\hline \\multirow{2}{*}{OPT-1.3B} & Brecq & 10.32 & 63.99 & 190.3 & 44.88 \\\\  & _aespa_ & 1.44 & 25.31 & 40.74 & 25.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Performance of _aespa_ and Brecq (perplexity \\(\\downarrow\\))\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Botev et al. (2017) Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In _International Conference on Machine Learning_, pp. 557-565. PMLR, 2017.\n' +
      '* Chee et al. (2023) Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. QuIP: 2-bit quantization of large language models with guarantees. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '* Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. _arXiv:1803.05457v1_, 2018.\n' +
      '* Esser et al. (2019) Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size quantization. In _International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression: A framework for accurate post-training quantization and pruning. _Advances in Neural Information Processing Systems_, 35:4475-4488, 2022.\n' +
      '* Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained Transformers. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.\n' +
      '* Hubara et al. (2021) Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Accurate post training quantization with small calibration sets. In _International Conference on Machine Learning_, pp. 4466-4475. PMLR, 2021.\n' +
      '* Jeon et al. (2022) Jeon, Y., Lee, C., Cho, E., and Ro, Y. Mr. BiQ: Post-training non-uniform quantization based on minimizing the reconstruction error. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12329-12338, 2022.\n' +
      '* Jeon et al. (2023a) Jeon, Y., Lee, C., and Kim, H.-y. Genie: Show me the data for quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12064-12073, 2023a.\n' +
      '* Jeon et al. (2023b) Jeon, Y., Lee, C., Park, K., and Kim, H.-y. A frustratingly easy post-training quantization scheme for LLMs. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 14446-14461, 2023b.\n' +
      '* Jung et al. (2019) Jung, S., Son, C., Lee, S., Son, J., Han, J.-J., Kwak, Y., Hwang, S. J., and Choi, C. Learning to quantize deep networks by optimizing quantization intervals with task loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 4350-4359, 2019.\n' +
      '* Kim et al. (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. SqueezeLLM: Dense-and-sparse quantization. _arXiv:2306.07629_, 2023.\n' +
      '* LeCun et al. (1989) LeCun, Y., Denker, J. S., Solla, S. A., Howard, R. E., and Jackel, L. D. Optimal brain damage. In _Advances in Neural Information Processing Systems (NIPS)_, volume 2, pp. 598-605, 1989.\n' +
      '* Li et al. (2021) Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S. BRECQ: Pushing the limit of post-training quantization by block reconstruction. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. AWQ: Activation-aware weight quantization for LLM compression and acceleration. _arXiv:2306.00978_, 2023.\n' +
      '* Marcus et al. (1993) Marcus, M., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of english: The penn treebank. 1993.\n' +
      '* Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. _arXiv:1609.07843_, 2016.\n' +
      '* Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. Up or down? Adaptive rounding for post-training quantization. In _International Conference on Machine Learning (ICML)_, pp. 7197-7206, 2020.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Scao et al. (2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. BLOOM: A 176B-parameter open-access multilingual language model. _arXiv:2211.05100_, 2022.\n' +
      '* Shao et al. (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. OmniQuant: Omnidirectionally calibrated quantization for large language models. _arXiv:2308.13137_, 2023.\n' +
      '\n' +
      'Smith, S. L., Kindermans, P.-J., Ying, C., and Le, Q. V. Don\'t decay the learning rate, increase the batch size. In _International Conference on Learning Representations_, 2018.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. _arXiv:2302.13971_, 2023.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4791-4800, 2019.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. OPT: Open pre-trained transformer language models. _arXiv:2205.01068_, 2022.\n' +
      '\n' +
      '## Appendix A Refined Quantization Objective (16) for Key Projection Matrix\n' +
      '\n' +
      'When quantizing the key projection matrix \\(\\mathbf{W}_{K}\\), we fix the query and value projection matrices in full-precision, and thus the reconstruction error \\(\\Delta\\mathrm{SA}_{K}\\) can be expressed as\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{K}=\\mathbb{E}\\left[\\left\\|\\mathrm{SA}(\\mathbf{Q}, \\widehat{\\mathbf{K}},\\mathbf{V})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\right\\|_{F}^{2} \\right]=\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{A}\\mathbf{V}\\right\\|_{F}^{2}\\right],\\]\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\[\\Delta\\mathbf{A}=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\widehat{\\mathbf{K}}^{ T}}{\\sqrt{d}}\\right)-\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}} \\right).\\]\n' +
      '\n' +
      'Then, by approximating \\(\\Delta\\mathbf{A}\\) with the first-order Taylor series expansion, we have\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{K}\\approx\\frac{1}{d}\\mathbb{E}\\left[\\left\\| \\mathbf{Q}\\Delta\\mathbf{K}^{T}\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\right\\|_{F }^{2}\\right]=\\frac{1}{d}\\mathbb{E}\\left[\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X} \\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\right\\|_{F}^{2}\\right]. \\tag{26}\\]\n' +
      '\n' +
      'To reduce the huge cost required to store \\(\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}\\) having \\(L^{3}\\) elements, we establish an upper bound of (26) and then use it as a surrogate of \\(\\Delta\\mathrm{SA}_{K}\\). Specifically, we separate the term \\(\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\) into two components as follows:\n' +
      '\n' +
      '\\[\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V }\\right\\|_{F}^{2}\\leq\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\right\\|_{F}^{2}\\cdot \\left\\|\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\right\\|_{F}^{2}.\\]\n' +
      '\n' +
      'Noting that the term \\(\\left\\|\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}^{T}\\mathbf{V}\\right\\|_{F}^{2}\\) is fixed during the quantization of \\(\\mathbf{W}_{K}\\), we minimize \\(\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\right\\|_{F}^{2}\\) to enforce \\(\\Delta\\mathrm{SA}_{K}\\) to be small, which leads to the proposed objective in (16).\n' +
      '\n' +
      '## Appendix B Proof of (20) and (22)\n' +
      '\n' +
      'Note that \\(\\mathbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right]= \\mathbb{E}\\left[\\left\\|\\mathrm{vec}\\left(\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right) \\right\\|_{2}^{2}\\right]\\), where \\(\\mathrm{vec}(\\cdot)\\) denotes the vectorization operation. Then, by exploiting the following properties of Kronecker product\n' +
      '\n' +
      '\\[\\mathrm{vec}\\left(\\mathbf{ABC}\\right) =\\left(\\mathbf{C}^{T}\\otimes\\mathbf{A}\\right)\\mathrm{vec}(\\mathbf{B}),\\] \\[\\left(\\mathbf{A}\\otimes\\mathbf{B}\\right)^{T} =\\mathbf{A}^{T}\\otimes\\mathbf{B}^{T},\\] \\[\\left(\\mathbf{A}\\otimes\\mathbf{B}\\right)\\left(\\mathbf{C}\\otimes\\mathbf{D}\\right) =\\mathbf{A}\\mathbf{C}\\otimes\\mathbf{B}\\mathbf{D},\\]\n' +
      '\n' +
      'we have\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right] =\\mathbb{E}\\left[\\left\\|\\left(\\mathbf{X}^{T}\\otimes\\mathbf{K}\\right) \\Delta\\mathbf{w}_{Q}\\right\\|_{2}^{2}\\right]\\] \\[=\\mathbb{E}\\left[\\Delta\\mathbf{w}_{Q}^{T}\\left(\\mathbf{X}^{T}\\otimes\\mathbf{ K}\\right)^{T}\\left(\\mathbf{X}^{T}\\otimes\\mathbf{K}\\right)\\Delta\\mathbf{w}_{Q}\\right]\\] \\[=\\mathbb{E}\\left[\\Delta\\mathbf{w}_{Q}^{T}\\left(\\mathbf{X}\\otimes\\mathbf{K}^{T }\\right)\\left(\\mathbf{X}^{T}\\otimes\\mathbf{K}\\right)\\Delta\\mathbf{w}_{Q}\\right] \\tag{27}\\] \\[=\\mathbb{E}\\left[\\Delta\\mathbf{w}_{Q}^{T}\\left(\\mathbf{X}\\mathbf{X}^{T} \\otimes\\mathbf{K}^{T}\\mathbf{K}\\right)\\Delta\\mathbf{w}_{Q}\\right]\\] \\[=\\Delta\\mathbf{w}_{Q}^{T}\\cdot\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\otimes \\mathbf{K}^{T}\\mathbf{K}\\right]\\cdot\\Delta\\mathbf{w}_{Q},\\]\n' +
      '\n' +
      'which is the desired result in (20).\n' +
      '\n' +
      'We now prove (22). Using the approximation on the expectation of the Kronecker product in (21), we have\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right] \\approx\\Delta\\mathbf{w}_{Q}^{T}\\cdot\\left(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right] \\otimes\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right]\\right)\\cdot\\Delta\\mathbf{w}_{Q}.\\]\n' +
      '\n' +
      'Note that since \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) and \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\) are symmetric, there exist \\(\\mathbf{G}_{X}\\) and \\(\\mathbf{G}_{K}\\) such that\n' +
      '\n' +
      '\\[\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}] =\\mathbf{G}_{X}\\mathbf{G}_{X}^{T},\\ \\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]=\\mathbf{G}_{K}^{T}\\mathbf{G}_{K}.\\]Then, by following the steps used to derive (27) in the reverse order, we have\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right] =\\Delta\\mathbf{w}_{Q}^{T}\\left(\\mathbf{G}_{X}\\mathbf{G}_{X}^{T}\\otimes\\mathbf{G}_{ K}^{T}\\mathbf{G}_{K}\\right)\\Delta\\mathbf{w}_{Q}\\] \\[=\\left\\|\\mathbf{G}_{K}\\Delta\\mathbf{W}_{Q}\\mathbf{G}_{X}\\right\\|_{F}^{2}\\] \\[=\\operatorname{tr}\\left(\\mathbf{G}_{K}\\Delta\\mathbf{W}_{Q}\\mathbf{G}_{X}\\mathbf{G }_{X}^{T}\\Delta\\mathbf{W}_{Q}^{T}\\mathbf{G}_{K}^{T}\\right)\\] \\[=\\operatorname{tr}\\left(\\mathbf{G}_{K}^{T}\\mathbf{G}_{K}\\cdot\\Delta\\mathbf{W} _{Q}\\cdot\\mathbf{G}_{X}\\mathbf{G}_{X}^{T}\\cdot\\Delta\\mathbf{W}_{Q}^{T}\\right)\\] \\[=\\operatorname{tr}\\left(\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right] \\Delta\\mathbf{W}_{Q}\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{Q}^{T} \\right),\\]\n' +
      '\n' +
      'which completes the proof of (22).\n' +
      '\n' +
      '## Appendix C Integration of Proposed Loss Functions into Existing PTQ Schemes\n' +
      '\n' +
      'We recall that we only utilized the definition of the attention operation when developing the proposed loss functions for the attention output reconstruction. Therefore, our loss functions can be integrated into any PTQ schemes based on layer-wise reconstruction and used to enhance their performance. In this section, we describe how to combine our loss functions with existing quantization schemes by taking AdaRound as an example.\n' +
      '\n' +
      'In short, AdaRound learns a weight-rounding mechanism by solving the following optimization problem (Nagel et al., 2020):\n' +
      '\n' +
      '\\[\\operatorname*{arg\\,min}_{\\mathbf{B}}\\ \\left\\|\\mathbf{W}\\mathbf{X}-\\widetilde{\\mathbf{W}} \\mathbf{X}\\right\\|_{F}^{2}+\\lambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{i,j})-1\\right| ^{\\beta}\\right), \\tag{28}\\]\n' +
      '\n' +
      'where \\(\\mathbf{B}\\) is the continuous variable to be learned, \\(h\\) is the rectified sigmoid function, and \\(\\widetilde{\\mathbf{W}}\\) is the soft-quantized weights defined as\n' +
      '\n' +
      '\\[\\widetilde{\\mathbf{W}}=s\\cdot\\text{clamp}\\left(\\left|\\frac{\\mathbf{W}}{s}\\right|+h( \\mathbf{B}),n,p\\right).\\]\n' +
      '\n' +
      'One can see that the loss function of AdaRound consists of two components, layer-wise reconstruction error and weight-rounding loss.\n' +
      '\n' +
      'To consider the cross-layer dependency between \\(\\mathbf{W}_{Q}\\), \\(\\mathbf{W}_{K}\\), and \\(\\mathbf{W}_{V}\\) in the learning process, we integrate the proposed loss functions developed for the attention output reconstruction into (28). In other words, we replace the layer-wise reconstruction error in (28) with our loss functions in (17), (22), and (23). For example, when learning the rounding policy for the query projection matrix \\(\\mathbf{W}_{Q}\\), the objective of the proposed _aespa_ is expressed as\n' +
      '\n' +
      '\\[\\operatorname*{arg\\,min}_{\\mathbf{B}_{Q}}\\ \\operatorname{tr}\\left( \\mathbb{E}\\big{[}\\mathbf{K}^{T}\\mathbf{K}\\big{]}\\Delta\\mathbf{W}_{Q}\\mathbb{E}\\left[\\mathbf{X }\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{Q}^{T}\\right)+\\lambda\\sum_{i,j}\\left(1-\\left| 2h(\\mathbf{B}_{Q,i,j})-1\\right|^{\\beta}\\right), \\tag{29}\\]\n' +
      '\n' +
      'where \\(\\Delta\\mathbf{W}_{Q}=\\mathbf{W}_{Q}-\\widetilde{\\mathbf{W}}_{Q}\\).\n' +
      '\n' +
      '## Appendix D Complexity Analysis for Conventional Block-wise Quantization Schemes\n' +
      '\n' +
      'Recall from (8) that conventional block-wise quantization schemes require to compute \\(\\operatorname{SA}(\\widehat{\\mathbf{Q}},\\widehat{\\mathbf{K}},\\widehat{\\mathbf{V}})\\) in each iteration. This means that for each input sequence, one needs to perform\n' +
      '\n' +
      '* forward pass for \\(\\widehat{\\mathbf{Q}}\\), \\(\\widehat{\\mathbf{K}}\\), and \\(\\widehat{\\mathbf{V}}\\): \\(3d_{h}L(2d-1)\\) flops\n' +
      '* matrix multiplications for computing \\(\\widehat{\\mathbf{Q}}\\widehat{\\mathbf{K}}^{T}\\) and \\(\\widehat{\\mathbf{A}}\\widehat{\\mathbf{V}}\\): \\(4d_{h}L^{2}-d_{h}L-L^{2}\\) flops\n' +
      '* softmax operation with additional scaling (i.e., \\(\\operatorname{softmax}(\\frac{\\partial\\widetilde{\\mathbf{K}}^{T}}{\\sqrt{d_{h}}})\\)): \\(3L^{2}+d_{h}L-L\\) flops\n' +
      '* final computation of reconstruction error: \\(3d_{h}L-1\\) flops\n' +
      '\n' +
      'If \\(B\\) input sequences are used in each quantization iteration, then total \\(B(6d_{h}dL+4d_{h}L^{2}+2L^{2}-L-1)\\) flops are required in conventional methods.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>