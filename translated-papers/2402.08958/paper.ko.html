<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 하이퍼스케일 변압기의 차세대 후학습 양자화에 관한 연구\n' +
      '\n' +
      'Junhan Kim\n' +
      '\n' +
      'Kyungphil Park\n' +
      '\n' +
      'Chungman Lee\n' +
      '\n' +
      'Ho-young Kim\n' +
      '\n' +
      'Joonyoung Kim\n' +
      '\n' +
      'Yongkweon Jeon\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '생성 AI 모델의 복잡성이 증가함에 따라, 모바일 장치 및 TV와 같은 에지 장치에 하이퍼 스케일 모델을 배치하기 위한 유망한 솔루션으로 훈련 후 양자화(PTQ)가 등장했다. 그러나 기존의 PTQ 기법은 많은 시간과 자원을 소모하며, 이는 빈번한 모델 업데이트와 여러 개의 하이퍼 파라미터 튜닝이 필요한 실제 상황에서 병목 현상이 될 수 있다. 비용 효율적인 대안으로 One-shot PTQ 방식이 제안되었다. 그러나 트랜스포머의 매우 중요한 특징인 어텐션 모듈 내의 레이어 간 의존성을 고려하지 못하여 성능이 다소 제한적이다. 따라서 본 논문에서는 정확도와 효율의 균형을 유지하는 새로운 PTQ 알고리즘을 제안한다. 제안된 알고리즘인 _aespa_의 핵심 아이디어는 어텐션 스코어를 보존하기 위해 교차 계층 의존성을 고려하면서 효율성을 위해 양자화 계층별을 수행하는 것이다. 다양한 언어 모델과 복잡도 분석에 대한 광범위한 실험을 통해, _aespa_가 Transformer 모델을 양자화하는 데 정확하고 효율적임을 입증한다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '모델 크기가 점차 커지면서 확산(Rombach et al., 2022)과 대규모 언어 모델(LLM)(Touvron et al., 2023; Zhang et al., 2022)과 같은 심층 생성 모델이 더 주류를 이루고 있으며, AI의 추세는 판별 모델에서 수조 단위의 수많은 매개 변수를 가진 생성 모델로 전환되고 있다.\n' +
      '\n' +
      '모델 복잡성(파라미터)의 폭발적인 성장(또는 확대)으로 AI 모델의 성능은 발전하고 있으며 현재 인간의 지능 수준에 접근하거나 심지어 초과하고 있다. 그러나 이러한 규모의 증가는 모바일 장치와 클라우드 모두에서 그에 상응하는 계산 비용의 증가를 초래하여 AI 모델의 효율적인 처리 및 압축을 필요로 한다. 흥미롭게도, 하나는 성능을 확장하기 위해 AI 모델의 복잡성을 확장하려는 반면, 다른 하나는 비용을 줄이기 위해 모델을 압축하는 것을 목표로 한다.\n' +
      '\n' +
      '양자화는 고정 소수점 연산을 주로 지원하는 장치에 AI 모델을 효율적으로 배치하는 데 도움이 되는 유망한 솔루션이자 필수 불가결한 절차이다. 가중치의 정밀도를 줄임으로써 메모리 대역폭 요구 사항을 완화할 수 있으며 NPU와 같은 고효율 벡터 처리 장치를 사용하여 양자화된 모델의 당황스러운 병렬성을 SIMDification할 수 있다. 양자화에 의한 필연적인 성능 저하를 최소화하기 위해, 양자화 인식 훈련(Quantization-aware Training; QAT)(Esser et al., 2019; Jung et al., 2019)과 훈련 후 양자화(Post-training quantization; PTQ)(Nagel et al., 2020; Li et al., 2021)의 두 가지 방법 중 하나를 선택할 수 있다. 모델 복잡성과 훈련 비용 및 가용 데이터 세트와 같은 필요한 리소스를 고려할 때 QAT는 수십억 개의 매개변수로 모델을 압축하는 데 실용적이지 않다. 결과적으로, 최근 하이퍼-스케일 트랜스포머(Vaswani et al., 2017) 모델에 대한 양자화 작업은 PTQ에 더 초점을 맞추고 있다.\n' +
      '\n' +
      '기존의 PTQ 기법들은 ResNet(Nagel et al., 2020; Hubara et al., 2021; Li et al., 2021; Frantar and Alistarh, 2022; Jeon et al., 2022)과 같은 비교적 작은 모델들을 성공적으로 양자화한 반면, 이들은 또한 그들의 시공간 복잡성 때문에 대규모 모델들을 다루는데 어려움을 겪고 있다. 효율적인 역전파-프리 PTQ 알고리즘이 비용-효과적인 대안으로 제안되었다 (Frantar et al., 2023). 그러나 레이어 간 의존성을 고려하지 않고 가장 가까운 라운딩에 의존하기 때문에 성능이 다소 제한적이다. 정확도 효율성 절충이 있으므로 하이퍼 스케일 트랜스포머 모델의 다음 단계 양자화를 위한 격차를 줄이는 것을 목표로 한다.\n' +
      '\n' +
      '본 논문에서는 정확도와 효율성을 동시에 추구하는 새로운 후처리 양자화 알고리즘인 _aespa_,1을 제안한다. _aespa_의 핵심 아이디어는 어텐션 스코어를 보존하기 위해 양자화 손실 함수를 정제함으로써 크로스 레이어 의존성을 고려하면서 효율성을 위해 양자화 레이어-와이즈를 수행하는 것이다.\n' +
      '\n' +
      '각주 1: _aespa_: 주의 중심 효율적이고 확장 가능한 사후 훈련 양자화 알고리즘 우리의 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 정확도와 효율성의 균형을 이루는 새로운 양자화 전략을 제안한다. 제안하는 기법은 계층별 모델을 양자화하여 효율성을 추구하면서 계층 간 종속성을 고려하기 위해 주의력 출력을 재구성한다.\n' +
      '* 양자화 과정을 가속화하기 위해, 어텐션 모듈에 대한 정제된 양자화 목표를 제안한다. 복잡도 분석을 통해 제안된 목적들을 이용하여 기존의 블록 단위 접근 방식보다 약 10배 빠른 양자화를 달성할 수 있음을 보인다.\n' +
      '* 다양한 언어 모델들에 대한 광범위한 실험으로부터, 우리는 우리의 접근법이 특히 낮은 비트 정밀도(INT2)에 대해 상당한 마진만큼 종래의 양자화 방식들을 능가한다는 것을 입증한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '최근 PTQ에 대한 연구는 양자화 오차 자체(\\(\\Delta\\mathbf{W}\\)보다는 양자화에 의해 발생하는 손실(\\(\\Delta\\mathcal{L}\\))을 최소화하려는 시도가 대부분이었다. 가중치\\(\\mathbf{W}\\)에 의해 파라미터화된 사전 훈련된 신경망을 고려한다. 우리가 네트워크의 작업 손실을 \\(\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W})\\)로 나타낼 때, 손실 섭동을 최소화하기 위한 가중치 \\(\\mathbf{W}\\)의 양자화 문제는 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W}}\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W})-\\mathcal{L}(\\mathbf{X},\\mathbf{Y},\\mathbf{W}}\\right], \\tag{1}\\tag{1}\\tag{Y},\\mathbf{W}+\\Delta\\mathbf{W})\n' +
      '\n' +
      '여기서 \\(\\mathbf{X}\\), \\(\\mathbf{Y}\\) 및 \\(\\Delta\\mathbf{W}\\)은 각각 양자화에 의한 입력, 출력 및 가중치 섭동이다.\n' +
      '\n' +
      '2차 테일러 급수를 사용하여 (1)의 손실 섭동 \\(\\Delta\\mathcal{L}\\)을 근사할 수 있다.\n' +
      '\n' +
      '\\Delta\\mathbb{E}[\\Delta\\mathbf{w}^{T}\\cdot\\mathbf{g}^{(\\mathbf{w}} +\\frac{1}{2}\\Delta\\mathbf{w}\\Delta\\mathbf{w}\\cdot\\mathbf{H}^{(\\mathbf{w}}\\cdot\\Delta\\mathbf{w}], \\tag{2}\\frac{1}{2}\\Delta\\mathbf{w}\\cdot\\mathbf{H}^{(\\mathbf{w}}\\cdot\\Delta\\mathbf{w}}\n' +
      '\n' +
      '여기서 \\(\\mathbf{w}\\)는 평탄화된 가중치이고, \\(\\mathbf{g}^{(\\mathbf{w})}\\) 및 \\(\\mathbf{H}^{(\\mathbf{w})}\\)는 각각 \\(\\mathbf{w}\\)에 관련된 구배 및 헤시안이다. 네트워크가 잘 수렴되어 있다고 가정하면 첫 번째 항을 생략할 수 있으므로 (1)을 (LeCun 등, 1989)로 간략화한다.\n' +
      '\n' +
      '\\\\frac{1}{2}\\Delta\\mathbf{w}\\\\mathbb{E}\\left[\\frac{1}{2}\\Delta\\mathbf{w}\\cdot\\mathbf{H}^{(\\mathbf{w}}\\cdot\\Delta\\mathbf{w}\\right]. \\tag{3}\\\n' +
      '\n' +
      '정확한 Hessian 행렬 \\(\\mathbf{H}^{(\\mathbf{w})}\\)을 계산하고 저장하는 것은 불가능하기 때문에, (3)은 종종 상이한 층들 또는 블록들(예를 들어, 잔차 및 트랜스포머 블록) 사이의 독립성을 가정함으로써 다음의 층별(4a) 또는 블록별 재구성 문제(4b)로 완화되었다(Nagel 등, 2020; Li 등, 2021):\n' +
      '\n' +
      '\\mathbf{W}-\\mathbf{W}^{(\\ell)}\\mathbf{W}^{(\\min_{\\Delta\\mathbf{W}^{(\\ell)}\\mathbf{X}\\right\\|_{F}^{2}\\right], \\tag{4a}\\mathbbb{E}\\left(\\mathbf{W}^{(\\ell)}\\mathbf{W}^{(\\ell)}\\left(\\mathbf{X}\\right\\|_{F}^{2}\\right), \\tag{4b}\\tag{4a}\\mathbbb{E}\\left(\\mathbf{W}^{(\\ell)}\\mathbf{W}^{(\\ell)\\mathbff{W}^{(\\ell)\\right\\|_{F}^{2}\\right), \\tag{4b}\\tag{4a}\\mathbbb{E}\\min_{\\Delta\\mathbb\n' +
      '\n' +
      '여기서 \\(\\mathbf{W}^{(\\ell)}\\)는 \\(\\ell\\)번째 레이어 또는 \\(\\ell\\)번째 블록 \\(f^{(\\ell)}\\)의 가중치이고 \\(\\mathcal{Q}\\)은 스케일 \\(s\\), 영점 \\(z\\) 및 비트-폭 \\(n\\)에 의해 파라미터화된 양자화 함수를 나타낸다. 균일한 양자화에 대해, \\(\\mathcal{Q}\\)는 다음과 같이 정의될 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{Q}(x)=s\\left(\\text{clamp}\\left(\\left\\lfloor\\frac{x}{s}\\right\\rfloor+z,0,2^{n}-1\\right)-z\\right), \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\left\\lfloor\\cdot\\right\\rfloor\\)는 라운드오프를 나타낸다.\n' +
      '\n' +
      '(4a) 및 (4b)를 해결하기 위해 다양한 양자화 방법이 제안되었다. 이러한 방법들은 1) AdaRound(Nagel et al., 2020), AdaQuant(Hubara et al., 2021), Brecq(Li et al., 2021)와 같은 인접 격자가 아닌 "proper" 양자화 격자에 각각의 가중치를 할당하기 위한 라운딩 정책을 학습하는 것과 2) AWQ(Lin et al., 2023) 및 OmniQuant(Shao et al., 2023)와 같은 (4a) 또는 (4b)를 만족하는 양자화 파라미터(scale \\(s\\) 및 영점 \\(z\\))를 학습하는 것 두 가지로 분류될 수 있다. 반올림 정책과 파라미터를 공동으로 학습하는 하이브리드 접근법이 지니(Jeon et al., 2023)에서도 제안되었다. 일반적으로 블록-와이즈 재구성(_e.g.,_Brecq, OmniQuant, and Genie)을 목표로 하는 양자화 방식은 레이어-와이즈 재구성(_e.g.,_AdaRound and AWQ)을 목표로 하는 방법을 능가하고 낮은 비트-폭(_e.g.,_INT2)에도 합리적인 성능을 나타낸다(Li et al., 2021).\n' +
      '\n' +
      '이러한 기법들이 성공적으로 DNN을 양자화했지만, 수십억 개의 파라미터를 갖는 대규모 트랜스포머에 적용하는 것은 어렵다. 양자화 프로세스의 매 반복에서 재구성 에러를 계산하기 위해 요구되는 반복적인 주의 동작들 때문에((4b) 및 섹션 3.5 참조), OPT-1.3b와 같은 비교적 작은 언어 모델들을 양자화하기 위해 10시간 이상이 요구된다(섹션 4의 표 5 참조). 이러한 계산 오버헤드를 완화하기 위해, 그래디언트 기반 최적화에 의존하지 않는 OPTQ(또는 GPTQ)가 대규모 트랜스포머 모델들에 대해 제안되었다 (Frantar et al., 2023). OPTQ는 양자화 프로세스를 극적으로 가속화하고 단일 NVIDIA A100 GPU만을 사용하여 대규모 트랜스포머의 양자화를 가능하게 한다(Frantar et al., 2023). 그러나, 그 성능은 계층별 재구성에 기초하기 때문에 다소 제한적이며, 따라서 계층간 의존성(_예를 들어, 질의, 키, 및 값 간의 의존성)을 고려할 수 없다(전 등, 2023).\n' +
      '\n' +
      '따라서 본 논문에서는 정확도와 효율의 균형을 유지하는 새로운 PTQ 기법을 제안한다. 요약하면, 제안하는 기법은 계층 간 의존성을 고려하기 위해 블록 단위의 재구성을 목표로 하지만 효율성을 위해 계층 단위의 모델을 양자화한다. 우리는 (4b)의 블록별 목적 함수를 계산적으로 효율적인 형태로 정제하여 반복된 어텐션 연산으로 인한 계산 오버헤드를 크게 줄일 수 있다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      '제안된 _aespa_를 제시하기 전에, 우리는 접근법에 대한 통찰력을 얻기 위해 계층별 재구성을 고려한다. \\(\\Delta\\mathbf{W}^{(\\ell)}=\\mathcal{Q}(\\mathbf{W}^{(\\ell)})-\\mathbf{W}^{(\\ell)}\\)을 가중치 양자화 오차라 하고, (4a)의 계층별 복원 오차는 (단순화를 위해 \\(\\ell\\을 생략)로 나타낼 수 있다.\n' +
      '\n' +
      '\\mathbbb{E}\\left[\\left\\|\\mathcal{Q}(\\mathbf{W}\\mathbf{W}\\mathbf{X}\\right\\|_{F}^{2}\\right]\\mathbf{E}\\left[\\mathbf{W}\\mathbf{W}\\mathbf{X}\\right\\|_{F}^{2}\\right]\\!=\\!\\\\ mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}\\mathbf{X}\\right\\|_{F}^{2}\\right\\\\\\=\\!\\ mathbb{E}\\left[\\operatorname{tr}\\!\\left(\\Delta\\mathbf{W}\\mathbf{X}\\mathbf{X}^{T}\\Delta\\mathbf{W}^{T}\\right]\\\\\\\\=\\!\\ operator name{tr}\\!\\ 좌(\\Delta\\mathbf{W}\\!\\cdot\\!\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\!\\cdot\\!\\Delta\\mathbf{W}^{T}\\right] \\!. \\tag{6}\\.\n' +
      '\n' +
      '결과적으로, 계층별 양자화 문제는 다음과 같이 재방송될 수 있다:\n' +
      '\n' +
      '\\Delta\\mathbff{W}\\cdot\\mathbbb{E}\\operatorname{tr}\\left(\\Delta\\mathbf{W}\\cdot\\mathbbb{E}\\left[\\mathbf{X}\\mathbf{X}\\mathbf{X}^{T}\\right]\\cdot\\Delta\\mathbf{W}^{T}\\right}\\tag{7}\\\\math\n' +
      '\n' +
      '(7)의 새로운 형태의 양자화 목표는 \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\)이 양자화 전에 미리 계산되어 저장된다면, 단일 행렬 곱셈과 요소별 곱셈으로 전체 교정 데이터세트에 대한 복원 오차를 측정할 수 있다. 2. 이것은 모든 입력 시퀀스 \\(\\mathbf{X}\\(\\mathcal{Q}(\\mathbf{W})\\mathbf{X}\\) 또는 \\(\\Delta\\mathbff{W}\\mathbf{X}\\)의 계산을 필요로 하는 (4a)의 원래의 공식과는 대조적이다.\n' +
      '\n' +
      '각주 2: \\(\\operatorname{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}^{T})의 계산이 \\(\\operatorname{torch}\\!\\\\mathbf{C}^{T})으로 구현될 수 있음을 주목한다. cdot\\!\\ 연산자명 {sum}((\\mathbf{A}\\mathbf{B})\\odot \\mathbf{C})\\), 여기서 \\(\\odot\\)은 요소별 곱 연산을 나타낸다. 그들은 수학적으로 동등하다.\n' +
      '\n' +
      '이 발견으로부터 발생하는 자연스러운 질문은 _"우리는 또한 그러한 사전 계산?"_에 기초하여 블록 재구성 에러를 효율적으로 측정할 수 있는가?"이다. 다음 부분에서는 블록 단위 양자화를 단순화하기 위한 주요 전략을 설명하고 어텐션 모듈을 위한 정제된 양자화 목표를 소개한다. 여기서 목표는 (7)에서 미리 계산된 값(예: \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\)으로 효율적으로 계산될 수 있다.\n' +
      '\n' +
      '### _aespa_의 양자화 전략\n' +
      '\n' +
      '종래의 블록-와이즈 복원 방법들(도 1(a))을 사용하여 어텐션 모듈을 양자화할 때, 질의, 키 및 값 투영 행렬들(\\(\\mathbf{W}_{Q},\\mathbf{W}_{K},\\mathbf{W}_{V}\\))이 공동으로 최적화되도록 최적화되었다.\n' +
      '\n' +
      '\\!\\operatorname{SA}(\\widehat{\\mathbf{Q},\\Delta\\mathbf{W}_{Q},\\Delta\\mathbf{W}_{V}}\\\\mathbbb{E}\\left[\\left\\|\\operatorname{SA}(\\widehat{\\mathbf{Q},\\widehat{\\mathbf{K},\\widehat{\\mathbf{V}}},\\widehat\\mathbf{V}}\\!\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right\\|_{F}^{2}\\right],\\tag{8}\\tag{8}\\tag{8}}\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\\aat\n' +
      '\n' +
      '여기서 주의 모듈 \\(\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})의 출력은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\operatorname{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\operatorname{softmax}\\left(\\frac{ \\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d}}\\right)\\mathbf{V}=\\mathbf{A}\\mathbf{V}.\\]\n' +
      '\n' +
      '이러한 경우, 각 반복에서 매 배치마다 계산량이 많고 시간이 많이 소요되는 연산자명(\\widehat{\\mathbf{Q},\\widehat{\\mathbf{K},\\widehat{\\mathbf{V})을 계산해야 한다(복잡도에 대한 자세한 내용은 섹션 3.5 참조).\n' +
      '\n' +
      '이러한 계산 오버헤드를 극복하기 위해 각 프로젝션 행렬을 분할 및 정복 방식으로 양자화한다. 예를 들어, 질의 투영 행렬 \\(\\mathbf{W}_{Q}\\)을 양자화할 때, \\(\\mathbf{W}_{K}\\) 및 \\(\\mathbf{W}_{V}\\)을 완전 정밀도로 고정시킨다(그림 1(b)), 이는 \\(\\mathbf{W}_{K}\\) 및 \\(\\mathbf{W}_{V}\\)에 의해 영향을 받는 공통 항의 인수분해를 용이하게 한다(자세한 내용은 섹션 3.3 참조). 우리는 이 전략이 각 계층에 대한 재구성 오류가 아니라 주의 모듈에 대한 재구성 오류를 최소화하는 것을 목표로 한다는 점에서 기존의 계층별 양자화 방식(예: AdaRound, OPTQ, AWQ)과 다르다는 점을 강조한다.\n' +
      '\n' +
      '도 1: 제안된 _aespa_의 개요. 각 가중치는 주의 출력을 재구성하기 위해 별도로 양자화된다.\n' +
      '\n' +
      '실험을 통해 타겟팅 블록 단위의 재구성의 중요성과 제안된 분리 양자화 전략의 타당성을 입증한다. 실험에서는 기존의 AdaRound를 사용하여 가중치 반올림 정책을 학습하지만, (8)에서 각 투영 행렬에 대한 손실 함수를 주의력 복원 오차로 설정(계층 복원 오차는 제외; 그림 2(c) 참조)한다. 표 1은 AdaRound, Brecq 및 우리의 접근법을 사용하여 양자화된 OPT-125M 모델의 성능을 요약한다. 분명한 바와 같이, 두 방법 모두 모델을 계층별로 양자화하지만, 우리의 접근법은 모든 비트 폭에 대해 AdaRound보다 균일하게 우수하다. 이는 계층들이 독립적인 것으로 간주되는 AdaRound와는 다른 어텐션 출력 재구성을 목표로 하여 교차 계층 의존성(즉, 질의, 키 및 값 사이의 관계)을 고려할 수 있기 때문이다. 또한, 블록 단위의 재구성을 목표로 하면, 별도의 계층 단위의 양자화는 결합 양자화 방법(Brecq)에 비해 심각한 성능 저하를 초래하지 않는다. 우리의 접근법은 2-비트 양자화에 대한 한계 성능 저하만을 야기하고 3-비트 및 4-비트 양자화에 대해 유사한 성능을 나타낸다(표 1 참조).\n' +
      '\n' +
      '하나의 어텐션 연산으로 한 번에 하나의 레이어만 업데이트하기 때문에 우리의 전략이 조인트 양자화 방법에서 필요로 하는 것보다 더 많은 계산 비용이 발생하는지 물어볼 수 있다(도 2의 (c) 참조). 이는 주의 모듈 내부의 모든 층들이 동시에 업데이트되는 종래의 방법들과 대조적이다(도 2(b)). 이 추가 비용을 줄이기 위해 다음 하위 섹션에서 설명한 대로 (8)의 양자화 목표를 계산적으로 효율적인 형태로 정제한다.\n' +
      '\n' +
      '###_aespa_를 위한 양자화 목표\n' +
      '\n' +
      '제안된 분리 양자화 전략 하에서, (8)의 양자화 목적은 어떤 투영 행렬을 양자화하느냐에 따라 다르다. 우리는 각 투영 행렬에 대한 단순화된 양자화 목표를 제시한다.\n' +
      '\n' +
      '**값 투영 행렬**값 투영 행렬 \\(\\mathbf{W}_{V}\\)을 양자화할 때, 질의 및 키 투영 행렬은 완전 정밀 행렬로서 고정된다. 이때, 재구성 오차\\(\\Delta\\mathrm{SA}_{V}\\)는 다음과 같이 표현된다.\n' +
      '\n' +
      '\\[\\Delta\\mathrm{SA}_{V}=\\mathbb{E}\\left[\\left\\|\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{ \\widehat{V}})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\\right\\|_{F}^{2}\\right].\\]\n' +
      '\n' +
      '\\(\\mathbf{Q}\\)와 \\(\\mathbf{K}\\)의 영향을 받는 공통항을 인수분해하여 \\(\\Delta\\mathrm{SA}_{V}\\)을 다음과 같이 단순화한다.\n' +
      '\n' +
      '수학bf{A}\\mathbf{A}\\mathbf{A}\\mathbf{E}\\left[\\Delta\\mathbf{A}\\right\\|_{F}^{2}\\right]=\\mathbbb{E}\\left[\\left\\|\\mathbf{A}\\Delta\\mathbf{V}\\mathbf{W}\\Delta\\mathbf{A}\\mathbf{A}\\right\\right\\|_{F}^{2}\\right]\\\\mathbbb{E}\\left[\\left\\Delta\\mathbf{W}\\mathbf{A}\\mathbf{A}\\right\\right\\right\\\\mathbf{A}\\mathbf{A}\\right\\right\\\\mathbf{A}\\mathbf{A}\\right\\right\\\\mathbf{W}\\left[\\left\\Delta\\mathbf{A}\\mathbf{A}\\right\\right\\\\mathbf{A}\\right\\\n' +
      '\n' +
      '따라서, 주의력 복원 에러를 최소화하기 위해 \\(\\mathbf{W}_{V}\\)을 양자화하는 문제는 다음과 같이 공식화될 수 있다.\n' +
      '\n' +
      '\\\\min_{\\Delta\\mathbf{W}_{V}\\\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{W}_{V}\\mathbf{X}\\mathbf{A}^{T}\\right\\|_{F}^{2}\\right]. \\tag{9}\\\\tag{\n' +
      '\n' +
      '**쿼리 프로젝션 행렬** 키와 값 프로젝션 행렬을 완전 정밀도로 고정할 때 \\(\\Delta\\mathrm{SA}_{Q}\\)에 의한 복원 오차 \\(\\Delta\\mathrm{SA}_{Q}\\)은 다음과 같이 표현된다.\n' +
      '\n' +
      '\\mathbb{E}\\left[\\Delta\\mathrm{SA}_{Q} =\\mathbb{E}\\left[\\Delta\\mathbf{V}}\\mathm{SA}(\\mathbf{\\widehat{Q}},\\mathbf{K},\\mathbf{V})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right\\|_{F}^{2}\\right] \\tag{10}\\[=\\mathbb{E}\\left[\\left\\|\\Delta\\mathbf{V}\\right\\|_{F}^{2}\\right], \\tag{11}\\mathbf{A}\\mathbb{E}\\left[\\left\\Delta\\mathbf{A}\\mathbf{V}\\right\\mathbf{A}\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\frac{\\mathbf\\mathbf{A}=\\mathrm{softmax}\\left(\\frac{\\mathbf{\\widehat{Q}\\mathbf{K}^{T}{\\sqrt{d}}\\right)\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}{\\sqrt{d}\\right)\\tag{12}\\t.\n' +
      '\n' +
      '매 양자화 반복에서 반복적인 소프트맥스 연산을 피하기 위해, 우리는 \\(\\Delta\\mathbf{A}\\)에 대해 다음의 1차 테일러 급수 근사를 이용한다:\n' +
      '\n' +
      '\\Delta\\mathbf{A}\\approx\\frac{\\Delta\\mathbf{Q}\\mathbf{K}^{T}{\\sqrt{d}\\cdot\\mathbf{J}_{\\mathrm{softmax}}^{T}, \\tag{13}\\\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & Quantization & Reconstruction & W2A16 & \\multicolumn{2}{c}{W3A16} & \\multicolumn{2}{c}{W4A16} \\\\ \\cline{3-10}  & Granularity & Target & WikiText-2 & C4 & WikiText-2 & C4 & WikiText-2 & C4 \\\\ \\hline AdaRound & **Layer-wise** & **Layer Output** & 160.7 & 95.63 & 35.44 & 31.86 & 29.51 & 27.78 \\\\ Brecq & **Block-wise** & **Attention Output** & 58.80 & 45.68 & 32.69 & 29.70 & 29.07 & 27.42 \\\\ \\hline\n' +
      '**Proposed** & **Layer-wise** & **Attention Output** & **62.69** & **48.20** & **31.93** & **29.73** & **28.73** & **27.41** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 양자화 전략의 성능 비교. 양자화된 OPT-125M 모델의 복잡도(\\(\\downarrow\\))가 보고된다.\n' +
      '\n' +
      '도 2: 양자화 전략(단순화)\n' +
      '\n' +
      '여기서 \\(\\mathbf{J}_{\\mathrm{softmax}}\\)는 소프트맥스 함수의 자코비안 행렬이다. (11)과 (13)을 조합함으로써, 우리는\n' +
      '\n' +
      'ff{K}^{T}\\mathbbb{E}\\left[\\Delta\\mathrm{SA}_{Q}\\approx\\frac{1}{d}\\mathbf{J}\\mathrm{softmax}^{T}\\mathbf{V}\\right\\|_{F}^{2}\\right]\\\\frac{E}\\left[\\left\\|\\mathbf{V}\\mathbf{J}\\mathbf{W}\\Delta\\mathbf{W}\\mathbf{Q}\\mathbf{W}\\Delta\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathb\n' +
      '\n' +
      '(14)에서 수정된 형태를 사용하여 주의 연산을 수행할 수 있지만, Jacobian 행렬 \\(\\mathbf{J}_{\\mathrm{softmax}\\)을 저장하기 위해서는 많은 양의 메모리가 필요하며, OPT-125M.3과 같은 작은 언어 모델에서도 \\(\\mathbf{J}_{\\mathrm{softmax}\\)을 저장하기 위해서는 100 GB 이상의 메모리가 필요하다. 비용 효율적인 대안으로 (14)의 상한을 구축한 후 양자화할 때 \\(\\Delta\\mathrm{SA}_{Q}\\)의 대용치로 사용한다. 구체적으로, 우리는 주목한다\n' +
      '\n' +
      '각주 3: \\(\\mathbf{J}_{\\mathrm{softmax}\\)의 모양은 \\([L,L,L]\\)(\\(L\\)은 각 주의 머리에 대한 입력 시퀀스 길이이다. \\(\\mathbf{J}_{\\mathrm{softmax}(\\mathbf{a}_{\\ell})=\\mathrm{diag}(\\mathbf{a}_{\\ell})-\\mathbff{a}_{\\ell}T}\\mathbbb{R}^{L\\times L}\\(\\mathbf{A}\\(\\mathbf{a}_{\\ell}\\)의 각 행에 대한 \\(\\mathbf{a}_{\\ell}\\)\n' +
      '\n' +
      'bf{W}_{Q}\\mathbf{J}_{\\mathrm{softmax}\\mathbf{X}\\right\\|_{F}^{2}\\\\leq\\left\\|\\mathbf{V}\\mathbf{J}_{\\mathrm{softmax}\\right\\|\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\mathbf{K}\\Delta\\mathbf{W}\\cdot\\left\\|\\mathbf{W}\\Delta\\mathbf{W}\\cdot\\mathbf{F}^{2}\\mathbf{W}\\Delta\\mathbf{W}\\\\mathbf{W}\\Delta\\mathbf{F}^{2}\\mathbf{W}\\Delta\\mathbf{W}\\\\mathbf{W}\\\\mathbf{W}\\\\mathbf{W}\\\\mathbf{W}\\\\mathbf{\n' +
      '\n' +
      '양자화 과정에서 \\(\\left\\|\\mathbf{V}^{T}\\mathbf{J}_{\\mathrm{softmax}\\right\\|_{F}^{2}\\)라는 용어는 변하지 않는다. 따라서 우리는 \\(\\left\\|\\mathbff{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\)을 최소화하며 \\(\\Delta\\mathrm{SA}_{Q}\\)도 작아지기를 희망한다. 즉, \\(\\mathbf{W}_{Q}\\)에 대한 양자화 대물렌즈는 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\\\min_{\\Delta\\mathbf{W}_{Q}}\\\\mathbbb{E}\\left[\\left\\|\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right]. \\tag{15}\\\n' +
      '\n' +
      '**키 프로젝션 행렬** 질의 프로젝션 행렬에 대한 (15)에서 양자화 목적어를 개발하기 위해 유사한 단계를 취함으로써, 다음과 같이 키 프로젝션 행렬 \\(\\mathbf{W}_{K}\\)에 대한 양자화 목적어를 정제할 수 있다:\n' +
      '\n' +
      '\\\\min_{\\Delta\\mathbf{W}_{K}}\\\\mathbb{E}\\left[\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\right\\|_{F}^{2}\\right]. \\tag{16}\\\n' +
      '\n' +
      '완전성을 위해 우리는 부록 A에 상세한 도출을 포함한다.\n' +
      '\n' +
      '### Algorithm Description\n' +
      '\n' +
      '제안된 _aespa_ 알고리즘은 크게 두 단계로 구성된다. 구체적으로, _aespa_는 먼저 양자화 파라미터(즉, 스케일 및 영점)를 결정한 후, 각 가중치 파라미터에 대한 양자화 그리드(또는 정수 가중치\\(\\mathbf{W}_{int}\\))를 최적화한다.\n' +
      '\n' +
      '```\n' +
      '1:def Quantization(\\(\\mathbf{W}\\),\\(\\mathbf{X}\\))\n' +
      '2: 헤시안\\(\\mathbf{H}\\)\\(\\triangleright\\)의 근사치 표 2 참조\n' +
      '3: \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}],\\mathbb{E}[\\mathbf{Q}\\mathbf{Q}\\mathbf{Q}]\\(\\mathbf{W}_{Q},\\mathbf{W}_{K}\\)\\(\\triangleright\\) 표 2\n' +
      '4: step size \\(\\mathbf{S}\\) s.t. \\(\\min_{\\alpha}\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}\\Delta\\mathbf{W}^{T}\\right)를 설정)\n' +
      '5:repeat\n' +
      '6: Loss \\(\\mathcal{L}\\)\\(\\triangleright\\) 표 2\n' +
      '7: 특정 알고리즘에 의한 \\(\\mathbf{S}\\) 또는 \\(\\mathbf{W}_{int}\\) w.r.t\\(\\mathcal{L}\\)의 최적화\n' +
      '8:until converged\n' +
      '9:return \\(\\mathbf{S}\\) and \\(\\mathbf{W}_{int}\\)\\(\\triangleright\\) step size and integer weight\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 양자화\n' +
      '\n' +
      '우리는 (9), (15), (16)에서 정제된 목적 함수를 개발할 때 계층별 양자화만을 가정하고 주의 연산의 정의만을 사용했다는 점에 유의한다. 따라서, 우리의 목적 함수는 노력 없이 임의의 계층별 양자화 방식에 통합될 수 있다. 예를 들어, 기존의 파라미터 초기화 알고리즘(예: AWQ(Lin et al., 2023) 및 Z-Fold(Jeon et al., 2023))을 목적 함수에 결합하여 양자화 파라미터를 계산할 수 있다. 또한, 양자화 그리드는 제안된 정제된 목적들과 함께 종래의 가중치-라운드 방법들(예를 들어, AdaRound(Nagel et al., 2020) 및 AdaQuant(Hubara et al., 2021))을 사용하여 최적화될 수 있다(추가 세부사항들에 대해서는 부록 C 참조). 제안된 _aespa_에서는 Z-Fold를 이용하여 양자화 매개 변수를 계산하고 AdaRound를 이용하여 가중치 라운드 정책을 학습한다.4 알고리즘 1에서는 제안된 _aespa_를 요약한다.\n' +
      '\n' +
      '각주 4: 질의, 키 및 값 투영 행렬 이외의 가중치(즉, 아웃-투영 계층 및 피드-포워드 네트워크 내부의 가중치)에 대해 (7)의 계층별 목적 함수를 사용한다.\n' +
      '\n' +
      '가중치 반올림 학습 과정을 가속화하기 위해 (6)과 같이 사전 계산을 통해 값이 효율적으로 계산될 수 있도록 목적 함수를 추가로 수정한다.\n' +
      '\n' +
      '**(9)에 대한 수정된 목적**\n' +
      '\n' +
      'bf{W}_{V}\\mathbf{A}^{T}\\left[\\mathrm{tr}\\right\\|_{F}\\right] \\\\qquad\\w[\\mathbf{A}\\mathbf{W}_{V}\\mathbf{A}\\mathbf{W}_{V}\\mathbf{A}\\mathbf{W}_{V}\\mathbf{A}\\mathbf{W}_{V}\\mathbf{A}\\mathbf{W}_{V}\\mathbf{W}\\mathbf{T}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{V}\\mathbf{W}\\mathbf{W}\\mathbf{V}\\mathbf{W}\\mathbf{W}\\mathbf{V}\\mathbf{W}\\mathbf{W}\\mathbf{V}\\mathbf{W}\\mathbf{\n' +
      '\n' +
      '(17)의 수정된 목표는 양자화 전에 \\(\\mathbb{E}[\\mathbff{A}\\mathbf{A}\\mathbf{A}\\mathbf{A}\\mathbf{A}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{A}\\mathbf{X}\\mathbf{\n' +
      '\n' +
      '각주 5: \\(\\Delta\\mathbf{W}_{V}\\)의 영향을 받지 않기 때문에 \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\)을 갱신할 필요가 없으며, 따라서 학습 과정에서 고정된다.\n' +
      '\n' +
      '이 변형의 또 다른 흥미로운 특징은 목표보다 \\(\\Delta\\mathbf{W}_{V}\\)의 보다 신뢰성 있는 업데이트를 용이하게 한다는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Layer & \\(\\mathbf{H}\\) & \\(\\mathcal{L}\\) \\\\ \\hline \\(\\mathbf{W}_{Q}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\mathbb{E}\\left[\\mathbf{K}^{T}\\mathbf{K}\\right]\\cdot\\Delta\\mathbf{W} \\mathbf{H}_{Q}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\(\\mathbf{W}_{K}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\mathbb{E}\\left[\\mathbf{Q}^{T}\\mathbf{Q}\\right]\\cdot\\Delta\\mathbf{W} \\mathbf{H}_{K}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\(\\mathbf{W}_{V}\\) & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}_{V}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ Others & \\(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\) & \\(\\mathrm{tr}\\left(\\Delta\\mathbf{W}\\mathbf{H}\\Delta\\mathbf{W}^{T}\\right)\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Row-wise Hessian \\(\\mathbf{H}\\) 및 양자. (9)의 각 층함수에 대한 손실\\(\\mathcal{L}\\) 구체적으로, 모든 교정 데이터를 이용하여 \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\)를 미리 계산하므로, (17)을 이용하여 계산된 손실은 전체 교정 데이터 집합(즉, 배치 크기는 총 데이터 수)을 고려한다. 결과적으로, 메모리 문제 없이 더 나은 참 기울기를 추정할 수 있으며, 이는 \\(\\Delta\\mathbf{W}_{V}\\)의 더 일관된 업데이트와 더 빠른 수렴을 초래할 수 있다(Smith et al., 2018).\n' +
      '\n' +
      '또한, (17)의 수정된 목표는 \\(\\mathbf{W}_{V}\\)의 각 행에 해당하는 헤시안 행렬 \\(\\mathbf{H}_{V}\\)이 있음을 의미한다.\n' +
      '\n' +
      '\\[\\mathbf{H}_{V}=\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]. \\tag{18}\\]\n' +
      '\n' +
      '우리는 (18)의 헤시안 행렬과 다르다는 점에 주목한다.\n' +
      '\n' +
      '\\[\\mathbf{H}=\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}], \\tag{19}\\]\n' +
      '\n' +
      '이는 종래의 방법들(Frantar and Alistarh, 2022; Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023)에서 근사화된 Hessian 행렬로서 일반적으로 사용된다. 헤시안(Hessian)이 다른 이유는 계층별 재구성이 아닌 주의별 재구성을 대상으로 하여 \\(\\mathbf{W}_{Q}\\), \\(\\mathbf{W}_{K}\\) 및 \\(\\mathbf{W}_{V}\\)의 의존성을 고려하였기 때문이다. 교차층 의존성을 고려하는 효과를 관찰하기 위해 서로 다른 헤시안 매트릭스에서 기존의 Z-폴드의 성능을 비교한다(표 3 참조). 명백한 바와 같이, (18)에서 헤시안(Hessian)을 이용함으로써 양자화 성능이 상당히 향상되며, 이는 교차-계층 의존성을 고려하는 것의 중요성을 입증한다.\n' +
      '\n' +
      '(15)와 (16)**에 대한 수정된 목적 \\(\\Delta\\mathbf{W}_{Q}\\)의 벡터화된 표현을 \\(\\Delta\\mathbf{w}_{Q}\\)으로 나타내면, (15)의 목적함수는 (증명을 위해 부록 B를 참조)와 같이 나타낼 수 있다.\n' +
      '\n' +
      'bf{W}_{Q}\\mathbf{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\|_{F}^{2}\\Big{}=\\Delta\\mathbf{w}_{Q}\\cdot\\mathbbb{E}\\big{[}\\mathbf{X}\\mathbf{X}\\otimes\\mathbf{K}^{T}\\cdot\\Delta\\mathbf{K}\\big{}\\cdot\\Delta\\mathbf{w}_{Q}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\\cdot\\mathbf{W}\n' +
      '\n' +
      '여기서 \\(\\otimes\\)은 크로네커 제품 작동이다. \\(\\mathbb{E}\\big{[}\\mathbf{X}\\mathbf{X}^{T}\\otimes\\mathbf{K}^{T}\\mathbf{K}\\big{]}\\(Botev et al., 2017)으로 근사한다.\n' +
      '\n' +
      'bf{X}\\mathbf{X}\\times\\mathbf{K}^{T}\\mathbf{K}\\approx\\mathbbb{E}\\left[\\mathbf{X}\\mathbf{X}\\mathbf{X}\\times\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{K}\\mathbf{K}\\times\\times\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{K}\\mathbf{K}\\times\\times\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{K}\\times\\times\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{K}\\times\\times\\times\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{K}\\times\\times\\times\\times\\times\\times\n' +
      '\n' +
      '(20)과 (21)을 조합함으로써, 우리는\n' +
      '\n' +
      'bb{E}\\Big{[}\\big{\\|}\\mathbf{K}\\Delta\\mathbf{X}\\big{\\|}_{F}^{2}\\Big{}\\mathbf{w}\\cdot\\left(\\mathbbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right}\\cdot\\delta\\stackrel{{(a)}\\delta\\mathbbf{W}\\big{\\|}\\mathbbf{W}\\cdot\\left(\\mathbbb{E}\\left[\\mathbbf{X}\\mathbbb{W}\\mathbc}\\tackrel{{(a)}\\delta\\mathbc}\\tackrel{{ left(\\mathbb{E}\\big{[}\\mathbf{K}^{T}\\mathbf{K}\\big{}\\Delta\\mathbf{W}_{Q}\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{Q}\\right)\\tag{22}\\mathbf{W}_{Q}\\right)\n' +
      '\n' +
      '(a)의 증명은 부록 B에 제공된다. 유사한 단계들을 취함으로써, 키 투영 매트릭스에 대한 목적 함수는 다음과 같이 재방송될 수 있다.\n' +
      '\n' +
      'bb{E}\\left[\\big{\\|}\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\big{\\|}_{F}^{2}\\right]\\\\\\operatorname{tr}\\!\\ 좌(\\mathbb{E}\\big{[}\\mathbf{Q}^{T}\\mathbf{Q}\\big{}\\Delta\\mathbf{W}_{K}\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}^{T}\\right]\\Delta\\mathbf{W}_{K}^{T}\\right}\\tag{23}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{K}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{K}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\\mathbb{W}\n' +
      '\n' +
      '(22)와 (23)의 수정된 목적함수는 \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\), \\(\\mathbb{E}[\\mathbf{Q}^{T}\\mathbf{Q}]\\), \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\)을 미리 계산함으로써 전체 교정 데이터 세트에 대한 손실을 효율적으로 계산할 수 있음을 의미한다.\n' +
      '\n' +
      '###_aespa__에 대한 복잡도 분석\n' +
      '\n' +
      '우리는 _aespa_의 계산 복잡도에 대해 논의한다. 구체적으로, (17), (22), (23)(알고리즘 1의 라인 6)에서 제안된 손실 함수를 기반으로 각 반복을 수행하는 데 필요한 부동 소수점 연산(플롭) 수를 분석하고 기존의 블록 단위 양자화 방법과 비교한다.\n' +
      '\n' +
      '각 프로젝션 행렬에 대한 _aespa_의 복잡도를 정리하면 다음과 같다.\n' +
      '\n' +
      '**값**: 미리 계산된 행렬 \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{A}\\mathbf{X}^{T}]\\)을 사용하여, (17)의 손실은 하나의 행렬 곱셈과 하나의 요소별 곱셈/첨가로 계산될 수 있다(각주 2 참조). 관련 비용은 \\(2d_{h}d^{2}+d_{h}d-1\\) flops이며, 여기서 \\(d\\)은 숨은 크기이고 \\(d_{h}\\)은 각 주의 머리에 대한 입력 차원이다.\n' +
      '***Query/key**: Once \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\), \\(\\mathbb{E}[\\mathbf{Q}\\mathbf{Q}]\\), \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\)을 미리 계산하고, (22)와 (23)에서의 손실함수는 두 행렬 곱셈과 하나의 요소별 곱셈/추가 단계를 수행하여 계산할 수 있다. 이를 위해서는 각 투영행렬에 \\(2d_{h}d^{2}+2d_{h}^{2}d-1\\)플롭이 필요하다.\n' +
      '\n' +
      '요약하자면, _aespa_는 총합이 필요하다\n' +
      '\n' +
      '\\[\\mathcal{C}_{\\textit{aespa}}=6d_{h}d^{2}+4d_{h}^{2}d+d_{h}d-3=\\mathcal{O}(d_{h}d^{2}} \\tag{24}\\]\n' +
      '\n' +
      '플롭들을 이용하여 각 어텐션 헤드에 대해 하나의 양자화 반복을 수행한다. 전체 데이터 셋을 고려한 손실은 전체 교정 데이터의 양에 관계없이 \\(C_{\\textit{aespa}}\\) 플롭으로 계산될 수 있음을 강조한다.\n' +
      '\n' +
      '우리는 이제 _aespa_와 기존의 블록 단위 양자화 방법의 복잡도를 비교한다. 쉽게 할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Approximated Hessian} & \\multicolumn{2}{c}{W2A16} & \\multicolumn{2}{c}{W3A16} & \\multicolumn{2}{c}{W4A16} & \\multicolumn{2}{c}{W4A16} \\\\ \\cline{2-10}  & WikiText-2 & PTB & C4 & WikiText-2 & PTB & C4 & WikiText-2 & PTB & C4 \\\\ \\hline \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) (conventional) & 179.5 & 273.7 & 129.7 & 38.74 & 56.24 & 33.21 & 30.84 & 44.25 & 28.90 \\\\ \\(\\mathbb{E}[\\mathbf{X}\\mathbf{A}^{T}\\mathbf{A}\\mathbf{X}^{T}]\\) **(ours)** & **152.9** & **202.4** & **114.4** & **35.26** & **48.54** & **31.20** & **29.28** & **41.47** & **27.52** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 상이한 헤시안 매트릭스를 갖는 Z-폴드의 성능. 양자화 후 OPT-125M의 복잡도(\\(\\downarrow\\))가 보고되었다.\n' +
      '\n' +
      '기존 방법이 필요하다고 확인(부록 D 참조)\n' +
      '\n' +
      '\\[\\mathcal{C}_{exist} = B(6d_{h}dL+4d_{h}L^{2}+2L^{2}-L-1) \\tag{25}\\] \\[=\\mathcal{O}(Bd_{h}L\\cdot\\max\\{d,L\\})\\]\n' +
      '\n' +
      '길이\\(L\\)의 입력 시퀀스를 처리하기 위한 플롭. 표 4는 OPT 모델의 다양한 크기에 대한 계산 비용을 요약한 것이다. 기존 방법의 경우 각 반복에서 4개의 배치 시퀀스를 사용하는 비용을 보고한다(\\(B=4\\)). 우리는 _aespa_의 계산 비용이 기존의 방법에 비해 상당히 낮다는 것을 관찰한다. 특히 소규모 모델의 경우 _aespa_는 10배 적은 수의 플롭을 수행한다. 모델 크기가 증가함에 따라 \\(\\mathcal{C}_{\\textit{aespa}\\)와 \\(\\mathcal{C}_{exist}\\) 사이의 간격이 감소하는 것을 알 수 있다. 이는 은닉 크기\\(d\\)가 모델 크기가 증가함에 따라 시퀀스 길이\\(L\\)(모든 모델에 대해 고정)을 초과하기 때문이다. 그럼에도 불구하고, _aespa_는 여전히 더 낮은 계산 비용을 초래하며, 기존의 방법이 더 큰 배치 크기를 사용하는 경우 갭이 증가할 것이다.\n' +
      '\n' +
      '## 4 실험 결과\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '제안된 _aespa_를 이용하여 OPT(Zhang et al., 2022), BLOOM(Scao et al., 2022) 및 LLMaA(Touvron et al., 2023)를 포함하는 공개적으로 이용 가능한 LLM을 양자화한다. 실험에서는 표 2의 손실 함수를 Z-Fold 및 AdaRound와 결합하여 양자화 매개 변수를 결정하고 양자화 그리드를 최적화하지만, _aespa_가 임의의 양자화 방식과 호환됨을 강조한다. 양자화 파라미터를 계산할 때 Z-Fold에 대한 정지 기준을 고려된 것으로 설정하였다 (전 등, 2023b).6 양자화 그리드를 최적화할 때 AdaRound에서 반복 횟수, 학습률, 반올림 손실의 가중치를 각각 2,000, 0.015, 1.5로 설정하였다.\n' +
      '\n' +
      '각주 6: 즉, 갱신 후 손실 섭동 \\(\\Delta\\mathcal{L}\\)이 증가하거나 전체 갱신 횟수가 30에 도달하면 Z-Fold의 교대 갱신을 중단한다.\n' +
      '\n' +
      '교정 데이터세트로서, C4 데이터세트(Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023)에서와 같이 128개의 랜덤 2048개의 토큰 세그먼트를 사용한다(Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023). 우리의 실험들(표 7을 제외하고)에서, 우리는 활성화가 LLMs에 대한 중요한 병목 현상이 아니기 때문에 (Frantar et al., 2023; Jeon et al., 2023b; Chee et al., 2023)에서와 같이 가중치만을 양자화하고 전체 정밀도에서 활성화를 유지한다(Frantar et al., 2023). LLMs의 추론은 가중치 양자화를 통해 메모리 이동을 감소시킴으로써 충분히 가속화될 수 있다(Kim et al., 2023).\n' +
      '\n' +
      '3개의 벤치마크 데이터 세트(WikiText-2(Merity et al., 2016), C4(Raffel et al., 2020), PTB(Marcus et al., 1993))와 여러 개의 제로샷 태스크를 사용하여 양자화된 LLM의 성능을 평가한다. 모든 실험은 단일 NVIDIA A100 GPU(80GB)로 수행되었다.\n' +
      '\n' +
      '### 종래기술과의 비교\n' +
      '\n' +
      '**블록별 PTQ 방식과의 비교** 우선, 제안된 _aespa_와 대표적인 블록별 양자화 방식인 Brecq를 비교한다. 추정된 양자화 시간은 사전 계산에 필요한 시간(알고리즘 1의 라인 2-4)을 포함한다. Brecq를 구현할 때, 우리는 원래의 Brecq 용지에 제공된 하이퍼-파라미터 설정을 채용한다(Li et al., 2021). 표 5로부터 명백한 바와 같이, _aespa_는 표 4의 복잡도 분석과 일치하는 Brecq보다 상당히 빠르게 양자화를 완료한다. 예를 들어, Brecq는 1B 파라미터로 모델을 양자화하는데 10시간 이상이 필요한 반면, _aespa_는 2시간 내에 양자화를 완료하며, 이는 표 2에서 제안된 손실 함수를 이용함으로써 양자화 과정이 실제로 가속화될 수 있음을 입증한다.\n' +
      '\n' +
      'Brecq를 사용하여 양자화된 모델의 성능이 모델 크기가 증가함에 따라 악화되는 이유를 궁금해할 수 있다. 우리는 이것이 하이퍼 파라미터(예: 학습 속도 및 라운딩 손실의 가중치)의 선택에 기인한다고 추측한다. 실제로, (Li 등, 2021)에 제시된 하이퍼-파라미터들은 ImageNet에 최적화되었지만, LLM들에 대해서는 최적화되지 않았다. 의도적인 하이퍼-파라미터 튜닝을 통해 Brecq에 대해 더 나은 성능을 얻을 수 있을 것으로 예상되지만, 이는 상당한 시간이 필요하기 때문에 실제 배치에서는 실현 가능하지 않을 것이다(GPU 하나로 하루에 두 개의 하이퍼-파라미터 튜닝만 가능하다).\n' +
      '\n' +
      '다음으로, 최근 제안된 PTQ 기법인 OmniQuant와 _aespa_를 비교하여 양자화 파라미터를 폴더블 파라미터와 함께 학습한다(Shao et al., 2023). OmniQuant의 경우, 우리는 공식 코드7을 실행하고 다른 교정 데이터 세트가 사용되었기 때문에 얻은 결과를 보고한다(Shao et al., 2023). 표 6에 제시된 결과\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & 125M & 350M & 1.3B & 2.7B & 6.7B & 13B \\\\ \\hline \\(\\mathcal{C}_{exist}\\) & 6.7 & 7.5 & 11 & 15 & 34 & 41 \\\\ \\(\\mathcal{C}_{\\textit{aespa}}\\) & 0.24 & 0.42 & 1.6 & 3.2 & 13 & 20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4:_aespa_ 및 종래의 방법(GFLOPS)의 비용\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{Quantization} & \\multicolumn{2}{c}{W2A16} \\\\ \\cline{3-5}  & & Time (hr) & Wiki2 & PTB & C4 \\\\ \\hline \\multirow{2}{*}{OPT-125M} & Brecq & 1.81 & 58.80 & 82.96 & 45.68 \\\\  & _aespa_ & 0.11 & 70.32 & 109.1 & 55.96 \\\\ \\hline \\multirow{2}{*}{OPT-1.3B} & Brecq & 10.32 & 63.99 & 190.3 & 44.88 \\\\  & _aespa_ & 1.44 & 25.31 & 40.74 & 25.04 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: _aespa_ 및 Brecq(perplexity \\(\\downarrow\\))의 성능\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Botev et al. (2017) Botev, A., Ritter, H., and Barber, D. Practical Gauss-Newton optimisation for deep learning. In _International Conference on Machine Learning_, pp. 557-565. PMLR, 2017.\n' +
      '* Chee et al. (2023) Chee, J., Cai, Y., Kuleshov, V., and De Sa, C. QuIP: 2-bit quantization of large language models with guarantee. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 질문에 답하는 걸 해결했다고 생각해? AI2 추론 문제인 ARC를 시도해 보세요. _ arXiv:1803.05457v1_, 2018.\n' +
      '* Esser et al. (2019) Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size quantization. In _International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression: 정확한 post-training quantization and pruning을 위한 framework. _ 신경 정보 처리 시스템_, 35:4475-4488, 2022에서의 발전.\n' +
      '* Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained Transformers. _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. _International Conference on Learning Representations_, 2020.\n' +
      '* Hubara et al. (2021) Hubara, I., Nahshan, Y., Hanani, Y., Banner, R., and Soudry, D. Accurate post training quantization with small calibration sets. In _International Conference on Machine Learning_, pp. 4466-4475. PMLR, 2021.\n' +
      '* Jeon et al.(2022) Jeon, Y., Lee, C., Cho, E., and Ro, Y. BiQ 씨: 재구성 에러를 최소화하는 것에 기초한 비균일 양자화 후 트레이닝. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12329-12338, 2022.\n' +
      '* Jeon et al. (2023a) Jeon, Y., Lee, C., and Kim, H.-y. 양자화를 위한 데이터를 보여줘 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12064-12073, 2023a.\n' +
      '* Jeon et al. (2023b) Jeon, Y., Lee, C., Park, K., and Kim, H.-y. LLM에 대한 좌절스러울 정도로 쉬운 훈련 후 양자화 기법. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 14446-14461, 2023b.\n' +
      '* Jung et al. (2019) Jung, S., Son, C., Lee, S., Son, J., Han, J.-J., Kwak, Y., Hwang, S. J., and Choi, C. Learning to Quantization intervals with task loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 4350-4359, 2019.\n' +
      '* Kim et al. (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. SqueezeLLM: Dense-and-sparse quantization. _ arXiv:2306.07629_, 2023.\n' +
      '* LeCun et al. (1989) LeCun, Y., Denker, J. S., Solla, S. A., Howard, R. E., and Jackel, L. D. Optimal brain damage. In _Advances in Neural Information Processing Systems (NIPS)_, volume 2, pp. 598-605, 1989.\n' +
      '*Li 등(2021) Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang, W., and Gu, S. BRECQ: 블록 재구성에 의한 트레이닝 후 양자화의 한계를 푸시하는 단계. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. AWQ: LLM 압축 및 가속을 위한 활성화 인식 가중치 양자화_ arXiv:2306.00978_, 2023.\n' +
      '* Marcus et al. (1993) Marcus, M., Santorini, B., and Marcinkiewicz, M. A. Building a large annotated corpus of English: The penn treebank. 1993년\n' +
      '* Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. 포인터 센티넬 혼합 모델 ArXiv:1609.07843_, 2016.\n' +
      '* Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and Blankevoort, T. 위로요, 아래로요? 훈련 후 양자화를 위한 적응적 반올림 In _International Conference on Machine Learning(ICML)_, pp. 7197-7206, 2020.\n' +
      '* Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limit of transfer learning with unified text-to-text transformer. _ Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Scao et al. (2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. BLOOM: A 176B-parameter open-access multilingual language model. _ 2211.05100_, 2022.\n' +
      '* Shao et al. (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. OmniQuant: Omnidirectionally calibrated quantization for large language models. _ arXiv:2308.13137_, 2023.\n' +
      '\n' +
      'Smith, S. L., Kindermans, P.-J., Ying, C., and Le, Q. V. Don\'t decay the learning rate, increase a batch size. _International Conference on Learning Representations_, 2018.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. _ arXiv:2302.13971_, 2023.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라 스웨그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4791-4800, 2019.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. OPT: Open pre-trained transformer language models. _ arXiv:2205.01068_, 2022.\n' +
      '\n' +
      '## 키 프로젝션 행렬을 위한 정제 양자화 목표 (16)\n' +
      '\n' +
      '키 투영 행렬 \\(\\mathbf{W}_{K}\\)을 양자화할 때 질의와 값 투영 행렬을 완전 정밀도로 고정하므로 복원 오차 \\(\\Delta\\mathrm{SA}_{K}\\)는 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '[\\Delta\\mathrm{SA}_{K}=\\mathbb{E}\\left[\\left\\|\\mathrm{SA}(\\mathbf{Q}, \\widehat{\\mathbf{K},\\mathbf{V})-\\mathrm{SA}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}\\right\\|_{F}^{2} \\right]=\\mathbbb{E}\\left[\\left\\|\\Delta\\mathbf{V}\\right\\|_{F}^{2}\\right],\\mathbf{V}\\mathbf{A}\\mathbf{V}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\right\\mathbf{A}\\mathbf{A}\\right\\mathbf{A}\\right\\math\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\frac{\\delta\\mathbf{A}=\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\widehat{\\mathbf{K}}^{T}{\\sqrt{d}\\right)-\\mathrm{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}{\\sqrt{d}\\right)}\\right).\\frac{\\mathbf{Q}\\mathbf{K}^{T}{\\sqrt{d}\\right).\\\n' +
      '\n' +
      '그런 다음 1차 테일러 급수 확장으로 \\(\\Delta\\mathbf{A}\\)을 근사함으로써 우리는\n' +
      '\n' +
      'bb{E}\\left[\\Delta\\mathrm{SA}_{K}\\approx\\frac{1}{d}\\mathbf{K}\\mathbf{T}\\mathrm{J}_{\\mathrm{softmax}^{T}\\mathbf{V}\\right\\|_{F}\\right\\mathbbb{E}\\left[\\left\\|\\mathbf{Q}\\Delta\\mathbf{W}\\Delta\\mathbf{X}\\mathbf{V}\\right\\mathm{J}\\mathbf{V}\\right\\mathm{{F}^{2}\\right]\\tag{26}\\mathbf{V}\\right\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\mathbf{W}\\math\n' +
      '\n' +
      '\\(L^{3}\\) 원소를 갖는 \\(\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}\\)의 저장에 소요되는 막대한 비용을 줄이기 위해 (26)의 상한을 설정한 후 \\(\\Delta\\mathrm{SA}_{K}\\)의 대용치로 사용한다. 구체적으로 \\(\\mathbf{Q}\\Delta\\mathbf{W}_{K}\\mathbf{X}\\mathbf{X}\\mathbf{J}_{\\mathrm{softmax}^{T}\\mathbf{V}\\)라는 용어를 다음과 같이 두 개의 성분으로 분리한다.\n' +
      '\n' +
      'mmathbf{W}_{K}\\mathbf{W}_{K}\\mathbf{V}\\right\\|_{F}^{2}\\leq\\left\\|\\mathbf{W}_{K}\\mathbf{X}\\right\\|_{F}^{2}\\cdot\\mathbf{V}\\right\\|_{F}^{2}\\mathbf{V}\\right\\|_{F}^{2}\\mathbf{V}\\right\\|\\mathbf}\\mathbf{V}\\right\\\\mathm}^{T}\\mathbf{V}\\right\\|_{F}^{2}\\mathbf{W}\\Delta\\mathbf{W}\\Delta\\mathbf{W}\\\\mathbf{W}\\right\\\\mathm}\\mathm}\\mathbf{V}\\right\\|\\mathm}^{T}\\mathbf{V}\\right\\|_{F}^{2}\n' +
      '\n' +
      '\\(\\left\\|\\mathbf{\\mathrm{J}}_{\\mathrm{softmax}}T}\\mathbff{V}\\right\\|_{F}^{2}\\(\\mathbf{W}_{K}\\)의 양자화 과정에서 \\(\\left\\|\\mathbff{Q}\\Delta\\mathbff{W}\\Delta\\mathbff{W}\\Delta\\mathbf{X}\\right\\|_{F}^{2}\\)의 양자화 과정에서 \\(\\left\\|\\mathrmbff{V}\\right\\\\mathbf{V}\\right\\\\mathbf{W}\\Delta\\mathbf{W}\\Delta\\mathbf{W}\\mathbf{X}\\right\\\\mathbf{F}^{2}\\\\(\\delta\\mathrm{SA}_{K}\\)의 양자화 과정에서 \\(\\left\\|\\mathrm{J}}_softmax}T}\\mathbf{V}\\\n' +
      '\n' +
      '## (20) 및 (22)의 부록 B 증명\n' +
      '\n' +
      '주목할 점은 \\(\\mathbb{E}\\left[\\left\\|mathbff{K}\\Delta\\mathbf{W}_{Q}\\mathbf{X}\\right\\|_{F}^{2}\\right]= \\mathbb{E}\\left[\\left\\|\\mathrm{vec}\\left(\\mathbff{K}\\Delta\\mathbff{W}_{Q}\\mathbff{X}\\right)\\right\\right\\|_{2}\\right]\\), 여기서 \\(\\mathrm{vec}(\\cdot\\)는 벡터화 연산을 나타낸다. 그런 다음 크로네커 제품의 다음 특성을 활용하여\n' +
      '\n' +
      '\\mathbf{C}^{T}\\otimes\\mathbf{A}\\right)\\mathrm{vec}(\\mathbf{B}\\times\\mathbf{B}\\otimes\\mathbf{B}\\times\\mathbf{D}\\mathbf{D},\\]\n' +
      '\n' +
      'we have\n' +
      '\n' +
      'bf{w}_{Q}\\mathbf{k}^{T}\\left(\\mathbf{w}_{Q}\\mathbf{k}\\mathbf{w}_{Q}\\mathbf{w}_{T}\\mathbf{w}_{Q}\\mathbf{t}\\mathbf{w}_{Q}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathbf{w}\\mathb\n' +
      '\n' +
      '(20)에서 원하는 결과이다.\n' +
      '\n' +
      '이제 우리는 증명한다(22). (21)에서 Kronecker 곱의 기대치에 대한 근사치를 사용하여 우리는\n' +
      '\n' +
      'bf{W}_{Q}\\mathbf{K}\\Delta\\mathbf{X}\\right\\|_{F}^{2}\\right]\\approx\\Delta\\mathbf{w}_{Q}\\cdot\\left(\\mathbb{E}\\left[\\mathbf{X}\\mathbf{X}\\mathbbb{E}\\left[\\mathbf{K}\\mathbf{W}\\mathbf{K}\\right]\\cdot\\Delta\\mathbf{w}_{Q}\\mathbbf{w}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathbbf{W}\\mathb\n' +
      '\n' +
      '주목할 점은 \\(\\mathbb{E}[\\mathbf{X}\\mathbf{X}^{T}]\\) 및 \\(\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}]\\)이 대칭이므로 \\(\\mathbf{G}_{X}\\) 및 \\(\\mathbf{G}_{K}\\)이 존재하므로\n' +
      '\n' +
      'b{E}[\\mathbf{X}\\mathbf{X}^{T}] =\\mathbf{G}_{X}\\mathbf{G}_{X}^{T},\\\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}] =\\mathbf{G}_{K}\\mathbf{G}_{G}_{K}.\\mathbb{E}[\\mathbf{K}^{T}\\mathbf{K}}}\\mathbf{G}_{G}_{K}.\\mathbf{G}_{G}_{T}}.\\mathbb{E}[\\mathbf{K}\\mathbf{E}[\\mathbf{K}}\\mathbf{G}_{T}\\mathbf{G}_{K}}}\\mathbf{G}_{K}}}\\mathbf{G}_{T}}\\mathbf{G} 그런 다음 (27)을 역순으로 도출하는 데 사용된 단계를 따라 우리는\n' +
      '\n' +
      'bf{G}_{Q}\\mathbf{G}_{Q}\\mathbf{G}_{Q}\\mathbf{G}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{G}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{W}_{Q}\\mathbf{\n' +
      '\n' +
      '(22)의 증명을 완료한다.\n' +
      '\n' +
      '## 제안된 손실함수를 기존의 PTQ 방식에 통합한 부록 C\n' +
      '\n' +
      '우리는 주의력 출력 재구성을 위해 제안된 손실 함수를 개발할 때 주의력 연산의 정의만을 활용했다는 것을 기억한다. 따라서, 손실 함수는 계층별 재구성을 기반으로 하는 모든 PTQ 기법에 통합되어 성능을 향상시키는데 사용될 수 있다. 본 절에서는 AdaRound를 예로 들어 손실 함수를 기존의 양자화 방식과 결합하는 방법을 설명한다.\n' +
      '\n' +
      '요컨대, AdaRound는 다음과 같은 최적화 문제를 해결하여 가중치-라운드 메커니즘을 학습한다(Nagel et al., 2020):\n' +
      '\n' +
      '\\mathbff{X}-\\widetilde\\[\\operatorname*{arg\\,min}_{\\mathbf{B}}\\\\left\\|\\mathbf{W}\\mathbff{X}\\right\\|_{F}^{2}+\\lambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{i,j})-1\\right|^{\\beta}\\right), \\tag{28}\\tag{28}\\mambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{i,j})-1\\right|^{\\beta}\\right), \\tag{28}\\mambda\\sum_2}+\\lambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{i,j})-1\\right|\\beta}\\right)\n' +
      '\n' +
      '여기서 \\(\\mathbf{B}\\)는 학습될 연속 변수이고, \\(h\\)는 정류된 시그모이드 함수이며, \\(\\widetilde{\\mathbf{W}}\\)는 다음과 같이 정의되는 소프트 양자화 가중치이다.\n' +
      '\n' +
      '\\[\\widetilde{\\mathbf{W}}=s\\cdot\\text{clamp}\\left(\\left|\\frac{\\mathbf{W}}{s}\\right|+h( \\mathbf{B}),n,p\\right).\\]\n' +
      '\n' +
      'AdaRound의 손실 함수는 계층별 재구성 오차와 가중치 반올림 손실의 두 가지 성분으로 구성되어 있음을 알 수 있다.\n' +
      '\n' +
      '학습 과정에서 \\(\\mathbf{W}_{Q}\\), \\(\\mathbf{W}_{K}\\) 및 \\(\\mathbf{W}_{V}\\) 사이의 교차 계층 의존성을 고려하기 위해, 주의 출력 재구성을 위해 개발된 제안된 손실 함수를 (28)에 통합한다. 즉, (28)의 계층별 재구성 오류를 (17), (22), (23)의 손실 함수로 대체한다. 예를 들어, 질의 투영 행렬 \\(\\mathbf{W}_{Q}\\)에 대한 라운딩 정책을 학습할 때, 제안된 _aespa_의 목적은 다음과 같이 표현된다.\n' +
      '\n' +
      '\\mathbbf{K}^{T}\\mathbbf{W}_{Q}\\mathbbb{W}_{Q}\\mathbbb{W}_{Q}\\mathbbb{W}_{Q}\\mathbbb{W}_{Q}\\mathbbb{W}_{Q}\\right)+\\lambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{Q,i,j})-1\\right|^{\\beta}\\right), \\tag{29}\\tag{29}\\mathbbb{E}\\left[\\mathbf{X}\\mathbff{X}\\mathbf{W}_{Q}\\right)+\\lambda\\sum_{i,j}\\left(1-\\left|2h(\\mathbf{B}_{Q,i,j})-1\\right|^{\\beta}\\right), \\tag{29}\\mathbbb{W}_{\n' +
      '\n' +
      'where \\(\\Delta\\mathbf{W}_{Q}=\\mathbf{W}_{Q}-\\widetilde{\\mathbf{W}}_{Q}\\).\n' +
      '\n' +
      '## 부록 D 복잡도 분석을 이용한 기존의 블록단위 양자화 기법\n' +
      '\n' +
      '(8)을 참조하면, 기존의 블록 단위 양자화 기법들은 각 반복에서 \\(operatorname{SA}(\\widehat{\\mathbf{Q},\\widehat{\\mathbf{K},\\widehat{\\mathbf{V})를 계산해야 한다. 이것은 각각의 입력 시퀀스에 대해, 하나가 수행해야 한다는 것을 의미한다.\n' +
      '\n' +
      '* Forward Pass for \\(\\widehat{\\mathbf{Q}}\\), \\(\\widehat{\\mathbf{K}}\\), \\(\\widehat{\\mathbf{V}}\\): \\(3d_{h}L(2d-1\\) flops\n' +
      '* 계산을 위한 행렬 곱셈 \\(\\widehat{\\mathbf{Q}\\widehat{\\mathbf{K}}^{T}\\) 및 \\(\\widehat{\\mathbf{A}\\widehat{\\mathbf{V}\\) : \\(4d_{h}L^{2}-d_{h}L-L^{2}\\)플롭\n' +
      '*추가 스케일링( \\(\\operatorname{softmax}(\\frac{\\partial\\widetilde{\\mathbff{K}}^{T}{\\sqrt{d_{h}}))을 갖는 소프트맥스 동작: \\(3L^{2}+d_{h}L-L\\ 플롭스\n' +
      '* 복원 오차의 최종 계산: \\(3d_{h}L-1\\) 플롭\n' +
      '\n' +
      '각 양자화 반복에 \\(B\\) 입력 시퀀스를 사용한다면, 기존의 방법에서는 총 \\(B(6d_{h}dL+4d_{h}L^{2}+2L^{2}-L-1)\\)플롭이 필요하다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:14]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>