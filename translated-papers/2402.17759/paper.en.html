<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      'With the thriving of language models (LMs; HZD\\({}^{+}\\)21, BHA\\({}^{+}\\)21), there is an increasing focus on improving the **learning**[12, 13] of LMs, which aims at accelerating the learning speed and achieving a certain model performance with as few training steps as possible [14]. This focus helps humans explore the limits of LMs given the rapid growth of their computational demand [15], and promotes democratization [16] of large language models (LLMs; BMR\\({}^{+}\\)20, Ope22, Ope23, CND\\({}^{+}\\)23, ADF\\({}^{+}\\)23), which is valuable for both research communities and industry sectors [17, 18, 19].\n' +
      '\n' +
      'In this paper, we present a theory for optimal learning of LMs. Unlike prior works exploring practical acceleration methods at the model-level [20, 18], optimizer-level [20, 17], or data-level [21, 22, 23, 24], our work demonstrates the principles of optimizing the LM learning speed, including the optimization objective, the property of optimal learning dynamics, and the essential improvement of the learning acceleration.\n' +
      '\n' +
      'Specifically, for the optimization objective, we propose to minimize the area under the loss curve (AUC; [14]), which has a clear physical significance: the _description length_ when we view the next-token-prediction LM training process as lossless compression of the training data [1, 13]. As shown in Figure 1, a learning process with the smallest loss AUC corresponds to the highest compression ratio. Simultaneously, the loss in this process also converges to a small value at the highest rate, given sufficiently large total training steps. Therefore, we consider **optimizing LM learning equivalent to maximizing the corresponding compression ratio of the learning process**, and adopt the latter as the optimization objective in our theory. Similar objectives are also employed to interpret the remarkable generalization performance of recent LLMs [21, 23].\n' +
      '\n' +
      'We then derive a theorem, named _Learning Law_, that characterizes the property of dynamics in the LM learning process that achieves the optimum of our objective. Here, a learning process is induced by a _learning policy_ that determines which data points the LM learns as the training progresses. In this way, we solve the optimal learning policy in the sense that the corresponding compression ratio is maximized, and obtain our Learning Law (see Theorem 3.1 for a formal expression):\n' +
      '\n' +
      'As shown in Figure 3, the contribution of an example is defined as the dot-product of its gradient and the gradient of a desired loss2, which measures its influence on the LM in the desired learning\n' +
      '\n' +
      'Figure 3: **A**: 3-D illustration of Learning Law (Theorem 3.1). In the optimal learning process, all training examples should have the same contribution to LM learning, where the contribution is defined as the dot-product of the gradient on individual samples (\\(\\nabla l_{m}\\), \\(\\nabla l_{n}\\), and \\(\\nabla l_{k}\\)) and the gradient of a desired loss (\\(\\nabla L\\)). See Section 3.2 for rigorous notation definitions. **B**: Experimental evidence of Learning Law. When LM learning approaches the optimum, the similarity of example contributions tends to \\(+\\infty\\), which means all examples have the same contribution to the LM.\n' +
      '\n' +
      'direction. Learning Law also suggests a _matching of local and global learning speed_ in the optimal learning process, which interprets the optimal learning policy as a dynamic data re-weighting strategy that encourages the LM to learn highly contributive examples and simultaneously avoid over-fitting them. Similar mechanisms are also found critical to the best teaching methods for humans in psychological research [14, 15].\n' +
      '\n' +
      'We examine our theory by experiments on linear classification tasks based on Perceptron3[13] and real-world language modeling tasks based on Transformer [20]. We first design a gradient-based method to search for the optimal learning policy under our objective. Then, we verify that the dynamics of the learning process induced by the found near-optimal policy aligns well with our Learning Law. Finally, as shown in Table 1, we provide empirical evidence showing that the near-optimal learning policy essentially improves the coefficients in the training step scaling law of LMs [11], which leads to 5.50\\(\\times\\) and 2.41\\(\\times\\) speedup to Perceptron and Transformer learning, respectively. This emphasizes the promise and significance of exploring more scalable methods to optimize the learning policy in practice and accelerate the training of LLMs.\n' +
      '\n' +
      'Footnote 3: In Appendix A.3, we provide a lossless data compression view of the Perceptron training, indicating that our theory also applies.\n' +
      '\n' +
      '## 2 Problem Formulation\n' +
      '\n' +
      'We consider LM training on a large-scale dataset with \\(N\\) examples \\(\\{x_{n}^{\\text{tm}}\\}_{n=1}^{N}\\) for a sufficiently large total training time steps \\(T\\). Let \\(\\mathbf{\\gamma}_{n,t}\\) denote the weight of the \\(n^{\\text{th}}\\) training example at the time step \\(t\\), a **learning policy** is represented by a time-variant distribution over \\(N\\) training examples \\(\\mathbf{\\gamma}_{t}=[\\gamma_{1,t},\\gamma_{2,t},\\cdots,\\gamma_{n,t}]^{\\top}\\), satisfying \\(\\sum_{n=1}^{N}\\gamma_{n,t}=1\\) and \\(\\gamma_{n,t}\\geq 0\\) for \\(1\\leq n\\leq N,0\\leq t\\leq T-1\\). The conventionally trained LM learns with a policy \\(\\gamma_{n,t}^{c}=\\frac{1}{N}\\) (**conventional learning**). Recent works [13, 1] have shown that theories derived based on Gradient Decent (GD) offer insights into other gradient-based algorithms [10]. Therefore, for simplicity, we assume the LM is trained with GD for \\(t=0,1,\\cdots,T-1\\):\n' +
      '\n' +
      '\\[\\begin{split} L_{t}^{\\text{tn}}(\\mathbf{\\theta}_{t})& =\\sum_{n=1}^{N}\\gamma_{n,t}l(x_{n}^{\\text{tn}},\\mathbf{\\theta}_{t}), \\\\ \\mathbf{\\theta}_{t+1}&=\\mathbf{\\theta}_{t}-\\eta\\nabla L_{t} ^{\\text{tn}}(\\mathbf{\\theta}_{t}),\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\mathbf{\\theta}_{t}\\in\\mathbb{R}^{D}\\) is the model parameters flattened into a \\(D\\)-dimensional vector at the time step \\(t\\), \\(\\eta\\) is the learning rate, and \\(l(\\cdot,\\cdot)\\) is the loss function of the learning problem. For LMs, \\(l(\\cdot,\\cdot)\\) is typically the Maximum Likelihood Estimation (MLE) loss: \\(l(x,\\mathbf{\\theta}_{t})=-\\log p_{\\mathbf{\\theta}_{t}}(x)\\), where \\(x\\) is a text sequence. Following [17] and [15], we focus on the learning speed reflected by the reduction rate of a desired loss \\(L^{\\text{dsr}}\\) computed on \\(K\\) examples \\(\\{x_{k}^{\\text{dsr}}\\}_{k=1}^{K}\\) that _do not necessarily_ follow the same distribution as the training examples:\n' +
      '\n' +
      '\\[L^{\\text{dsr}}(\\mathbf{\\theta}_{t})=\\frac{1}{K}\\sum_{k=1}^{K}l(x_{k}^{\\text{dsr}}, \\mathbf{\\theta}_{t}). \\tag{2}\\]\n' +
      '\n' +
      'This formulation applies to a broad of practical scenarios including classical machine learning using a validation set to prevent over-fitting [20], large-scale pre-training relying on a carefully curated held-out corpus to evaluate generalization performance [11], and domain adaptation where a natural difference exists between training and target distribution [17]. As such, we search for the learning policy \\(\\mathbf{\\gamma}_{t}\\) that maximizes the reduction rate of \\(L^{\\text{dsr}}(\\mathbf{\\theta}_{t})\\) to optimize LM learning.\n' +
      '\n' +
      'However, direct analysis of this optimization problem is difficult due to the discreteness of GD. Therefore, we focus on the _continuous limit_ of GD by considering the corresponding gradient flow of Equation 1 for \\(t\\in[0,T]\\), which is more amenable to theoretical analysis [13]:\n' +
      '\n' +
      '\\[\\frac{\\mathrm{d}}{\\mathrm{d}t}\\mathbf{\\theta}(t)=-\\nabla L^{\\text{tm}}(\\mathbf{\\theta} (t),t)=-\\nabla\\sum_{n=1}^{N}\\gamma_{n}(t)l(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t)), \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\gamma_{n}(t)\\) is a smooth interpolation function of \\(\\gamma_{n,t}\\). According to the results in numerical analysis, GD defined in Equation 1 is the _Euler method_ to approximately solve the initial value problem of the gradient flow in Equation 3, and \\(\\mathbf{\\theta}(t)\\approx\\mathbf{\\theta}_{t}\\) when \\(\\eta\\) is sufficiently small [1]. In Section 4, we show that the results derived from this limit align well with the experiments in discrete settings.\n' +
      '\n' +
      'Theory for Optimal Learning of LMs\n' +
      '\n' +
      'In this section, we present our theory in the continuous limit of GD. We first propose an objective for "maximizing the reduction rate of \\(L^{\\text{dst}}\\) by optimizing the learning policy". Then, we derive our main theorem, named _Learning Law_, which introduces a necessary condition for the dynamics of the learning process induced by the policy that achieves the optimum of the objective.\n' +
      '\n' +
      '### Objective: Maximizing Compression Ratio\n' +
      '\n' +
      'We characterize the reduction rate of \\(L^{\\text{dst}}\\) with the area under the curve of \\(L^{\\text{dst}}(\\mathbf{\\theta}(t))\\) (AUC of \\(L^{\\text{dst}}\\)) and minimize this area to achieve high learning speed:\n' +
      '\n' +
      '\\[\\begin{split}\\min_{\\mathbf{\\gamma}(t)}&\\int_{0}^{T}L^{ \\text{dst}}(\\mathbf{\\theta}_{\\mathbf{\\gamma}}(t))\\mathrm{d}t,\\\\ \\text{s.t.}&\\sum_{n=1}^{N}\\gamma_{n}(t)=1,\\\\ &\\gamma_{n}(t)\\geq 0,n=1,2,\\cdots,N,\\end{split} \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\mathbf{\\gamma}(t)=\\left[\\gamma_{1}(t),\\gamma_{2}(t),\\cdots,\\gamma_{n}(t)\\right]^ {\\top}\\) and \\(\\mathbf{\\theta}_{\\mathbf{\\gamma}}(t)\\) is an alias of \\(\\mathbf{\\theta}(t)\\) satisfying Equation 3 to emphasize its dependency on \\(\\mathbf{\\gamma}(t)\\). As shown in Figure 1, for sufficiently large \\(T\\), a learning process with minimal loss AUC owns the highest loss reduction rate. Interestingly, the AUC of \\(L^{\\text{dst}}\\) has a physical significance from the "LM-training-as-lossless-compression" view [14]: **the resulting description length of compressing data drawn from the desired data distribution**. Therefore, Equation 4 is equivalent to maximizing the corresponding compression ratio. Note that unlike [13] that studies encoding data using a well-trained LM, we view the entire LM training as a compression process. We provide more discussion of these two perspectives in Section 5. Besides, there are still slight differences between our statement and that in prior works viewing the training process as lossless compression [13, 12, 14]: we consider _the desired loss AUC of GD training for multiple epochs_, while the previous statement is about _the training loss AUC with single-epoch SGD training_. More discussion about this difference can be found in Appendix A.2.\n' +
      '\n' +
      '### Learning Law\n' +
      '\n' +
      'Equation 4 defines an Optimal Control problem that can be solved by _Maximum Principle_[15]. However, we find the solution hard to interpret and verify in practical LM learning. Therefore, in this work, we derive a looser necessary condition for the optimum of Equation 4.\n' +
      '\n' +
      '**Theorem 3.1** (Learning Law).: _When an LM is trained with an optimal learning policy, which yields a learning process corresponding to a maximum compression ratio on the desired data distribution, the following condition holds for \\(0<t\\leq T\\) and any \\(m\\), \\(n\\) such that \\(\\gamma_{m}(t)>0\\), \\(\\gamma_{n}(t)>0\\):_\n' +
      '\n' +
      '\\[\\nabla L\\cdot\\nabla l_{m}=\\nabla L\\cdot\\nabla l_{n}=\\mathrm{Const}, \\tag{5}\\]\n' +
      '\n' +
      '_where \\(\\nabla L=\\nabla L^{\\text{dst}}(\\mathbf{\\theta}(t))=\\nabla\\frac{1}{K}\\sum_{k=1}^{ K}l(x_{k}^{\\text{dst}},\\mathbf{\\theta}(t))\\), \\(\\nabla l_{m}=\\nabla l(x_{m}^{\\text{trn}},\\mathbf{\\theta}(t))\\), \\(\\nabla l_{n}=\\nabla l(x_{n}^{\\text{trn}},\\mathbf{\\theta}(t))\\), and \\(\\mathbf{\\cdot}\\) is dot-product. \\(\\mathrm{Const}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dst}}(\\mathbf{\\theta}(t))\\) is the desired loss change rate over time and **is independent of \\(\\mathbf{n}\\) and \\(\\mathbf{m}\\)**._\n' +
      '\n' +
      'To prove Theorem 3.1, we apply the Euler-Lagrange (EL) equation [16] and Karush-Kuhn-Tucker (KKT) conditions [13] to Equation 4, which results in the condition: \\(\\nabla L^{\\text{dst}}(\\mathbf{\\theta}(t))\\cdot\\nabla l(x_{n}^{\\text{tm}},\\mathbf{ \\theta}(t))=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dst}}(\\mathbf{\\theta}(t))\\). A full proof is shown in Appendix B.\n' +
      '\n' +
      '\\(\\nabla L\\cdot\\nabla l_{n}\\) in Equation 5 represents the **contribution** of the training example \\(x_{n}^{\\text{tm}}\\) to \\(L^{\\text{dst}}(\\mathbf{\\theta}(t))\\), which is maximized when the gradient on \\(x_{n}^{\\text{tm}}\\) shares the same direction with the gradient of \\(L^{\\text{dst}}(\\mathbf{\\theta}(t))\\). We denote \\(\\mathbf{CT}_{n}(\\mathbf{t})=\\mathbf{\\nabla L}\\cdot\\mathbf{\\nabla l}_{n}=\\mathbf{\\nabla L}^{ \\text{dst}}(\\mathbf{\\theta}(t))\\cdot\\mathbf{\\nabla l}(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))\\) for convenience in the rest of the paper. Note that when the model is converged (\\(\\nabla L^{\\text{tm}}(\\mathbf{\\theta}(t),t)\\approx\\mathbf{0}\\)), \\(\\mathrm{CT}_{n}(t)\\) can be viewed as an approximation of the Influence Function [11] by setting the Hessian matrix of \\(L^{\\text{tm}}(\\mathbf{\\theta},t)\\) at \\(\\mathbf{\\theta}=\\mathbf{\\theta}(t)\\) to an identity matrix [11]. In essence, Equation 5 means \\(\\mathrm{CT}_{n}(t)\\) equals a value independent of \\(n\\). Since the zero-weight examples (\\(\\gamma_{n}(t)=0\\)) are typically noisy (verified in Section 4.5), Theorem 3.1 suggests that **all non-noisy examples should be identically contributive to the LM in the optimal learning process**. In the following, we provide more discussion of this theorem.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      'Theorem 3.1 suggests a matching of the local and global learning.Another interpretation of \\(\\mathrm{CT}_{n}(t)\\) is the "local learning speed": how fast the LM learns the knowledge in \\(x_{n}^{\\text{tm}}\\) that is helpful to reduce \\(L^{\\text{dsf}}\\). This is because the dot-product operation in \\(\\mathrm{CT}_{n}(t)\\) can be viewed as the projection of the individual loss descending velocity \\(\\nabla l(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))\\) on the desired direction. Correspondingly, \\(\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dsf}}(\\mathbf{\\theta}(t))\\) represents the LM\'s "global learning speed": how fast the LM gets better by learning all individual \\(x_{n}^{\\text{tm}}\\). As a result, \\(\\mathrm{CT}_{n}(t)=\\mathrm{Const}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dsf }}(\\mathbf{\\theta}(t))\\) in Theorem 3.1 indicates that the local learning speed should match the global learning speed in the optimal learning process.\n' +
      '\n' +
      'The optimal learning policy establishes a dynamic data re-weighting strategy.Generally, as the learning of LM progresses, \\(\\mathrm{CT}_{n}(t)\\) drops because the gradient norm on each example \\(||\\nabla l(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))||\\) decreases as the LM fits \\(x_{n}^{\\text{tm}}\\). In addition, the direction of \\(\\nabla l(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))\\) diverges from \\(\\nabla L^{\\text{dsf}}(\\mathbf{\\theta}(t))\\) due to the possible discrepancy between the distribution of \\(x_{n}^{\\text{tm}}\\) and \\(x_{k}^{\\text{dsf}}\\), which also contributes to the decrease of \\(\\mathrm{CT}_{n}(t)\\). Therefore, Theorem 3.1 guarantees that _highly contributive example \\(x_{n}^{\\text{tm}}\\) with high \\(\\mathrm{CT}_{n}(t)\\) obtains large weights for training_, in order to reduce \\(\\mathrm{CT}_{n}(t)\\) to meet the value of other examples. On the other hand, Theorem 3.1 also ensures that _the weights of \\(x_{n}^{\\text{tm}}\\) are lowered before the LM over-fits it_ because \\(\\mathrm{CT}_{n}(t)\\) should not be too small to match the global learning speed. Altogether, this forms a dynamic training data re-weighting strategy, which is intuitively essential for the optimal learning policy that maximizes the learning speed of an LM.\n' +
      '\n' +
      'Theorem 3.1 is a necessary condition for the optimal learning dynamics.This is because the E-L equation and KKT conditions are necessary conditions for the global optimum when the optimization problem is non-convex. Therefore, a learning process satisfying Theorem 3.1 is not guaranteed optimal. For example, by setting \\(\\gamma_{1}(t)=1\\) and \\(\\gamma_{2}(t)=\\gamma_{3}(t)=\\cdots=\\gamma_{N}(t)=0\\), Equation 5 is satisfied, regardless of the values of \\(\\mathrm{CT}_{n}(t)\\). This learning policy corresponds to using SGD with mini-batch size = 1, which is unlikely to be the optimal [14]. Therefore, searching for the optimal policy according to Theorem 3.1 may need regularization terms in practice, which we leave for future work to explore.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We conduct experiments in the discrete setting of Equation 1, where the conclusions derived from the continuous limits in Section 3 are still applicable when \\(\\eta\\) is sufficiently small [1]. We first design a method to find the optimal learning policy \\(\\mathbf{\\gamma}_{t}\\in\\mathbb{R}^{N}\\) for \\(0\\leq t\\leq T-1\\), by explicitly minimizing the AUC of \\(L^{\\text{dsf}}(\\mathbf{\\theta}_{t})\\) in the discrete setting, which maximizes the corresponding compression ratio of data drawn from the desired distribution. Then we examine our Learning Law (Theorem 3.1) on the learning process induced by the found policies. Finally, we empirically verify that maximizing the compression ratio essentially improves the scaling law coefficients [14], indicating the practical significance and promise of our theory.\n' +
      '\n' +
      '### Finding the Optimal Learning Policy\n' +
      '\n' +
      'To find the optimal \\(\\mathbf{\\gamma}_{t}\\), we directly solve the discrete version of the optimization problem defined in Equation 4 with a Proximal Gradient Method [1]:\n' +
      '\n' +
      '\\[\\begin{split} J(\\mathbf{\\gamma})&=\\sum_{t=1}^{T}L^{ \\text{dsf}}(\\mathbf{\\theta}_{t}),\\\\ \\mathbf{\\gamma}_{t}&\\leftarrow\\mathrm{Proj}\\left[\\mathbf{ \\gamma}_{t}-\\epsilon\\nabla_{\\mathbf{\\gamma}_{t}}J(\\mathbf{\\gamma})\\right],\\;0\\leq t \\leq T-1,\\end{split} \\tag{6}\\]\n' +
      '\n' +
      'where \\(J(\\mathbf{\\gamma})\\) is a discrete approximation of the integral in Equation 4, \\(\\epsilon\\) is the learning rate and \\(\\mathrm{Proj}[.]\\) projects a point in \\(\\mathbb{R}^{N}\\) to the \\(N\\)-simplex, ensuring that \\(\\mathbf{\\gamma}_{t}\\) is a probability distribution over \\(N\\) training examples. The optimization process can be implemented efficiently using dynamic programming and Jacobian-Vector-Product in PyTorch [13], which is described in detail in Appendix C.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      'We conduct experiments on a linear classification task based on Perceptron [14] and a language modeling task based on Transformer [13]. See Appendix D for hyper-parameter configurations.\n' +
      '\n' +
      'Perceptron Linear Classification.We adopt a teacher-student setting [1] where each example \\(x_{n}=(\\mathbf{z}_{n},y_{n})\\) is a pair of \\(D\\)-dimensional vector \\(\\mathbf{z}_{n}\\in\\mathbb{R}^{D}\\) drawn i.i.d. from Gaussian distribution, and a scalar \\(y_{n}=\\operatorname{sign}(\\mathbf{T}\\cdot\\mathbf{z}_{n})\\) given the ground truth weight \\(\\mathbf{T}\\in\\mathbb{R}^{D}\\). We introduce a shift between the training and the desired data distribution to reflect their differences. The data are learned by an one-layer Perception parameterized by \\(\\boldsymbol{\\theta}\\in\\mathbb{R}^{D}\\): \\(o_{n}=\\sigma(\\boldsymbol{\\theta}\\cdot\\mathbf{z}_{n})=\\frac{1}{1+\\exp(- \\boldsymbol{\\theta}\\cdot\\mathbf{z}_{n})}\\), which is trained with Maximum Likelihood Estimation (MLE) loss \\(l(x_{n},\\boldsymbol{\\theta})=-\\log{o_{n}^{y_{n}}(1-o_{n})^{1-y_{n}}}\\). In Appendix A.3, we show that Perceptron can be viewed as a one-step LM, which means our theory still applies.\n' +
      '\n' +
      'Transformer Language Modeling.Considering the computation cost of the optimal policy searching, we adopt a two-layer Transformer with about 1.7M parameters and train it on TinyStories [1], a high-quality pre-training corpus. We add perturbations to the training examples (see Appendix D for details), which mimics the relatively low quality of the pre-training corpus in practice. Since our theoretical derivation is generally applicable, we believe that our theory also applies to larger LMs.\n' +
      '\n' +
      'To migrate the risk of over-fitting the \\(K\\) examples used to compute \\(L^{\\text{sfr}}(\\boldsymbol{\\theta}_{t})\\) in Section 4.1, we additionally construct a held-out test set with \\(K\\) examples from the desired data distribution in both Perceptron linear classification and Transformer language modeling experiments. In the following,\n' +
      '\n' +
      'Figure 4: Learning policy optimization results in Perceptron linear classification **(a)** and Transformer language modeling tasks **(b).** We plot the learning policy optimization loss \\(J(\\gamma)\\) (solid lines), defined in Equation 6, which represents the area under the curve (AUC) of the desired Perceptron or Transformer loss. We also show the corresponding compression ratio of the training process (dashed lines) in an "LM-as-Lossless-Compression" view. The optimization starts from conventional learning and smoothly converges to near-optimal learning with low loss AUC and high comprehension rate.\n' +
      '\n' +
      'Figure 5: Curves of the desired loss \\(L^{\\text{sfr}}(\\boldsymbol{\\theta}_{t})\\) when the model is trained using the conventional and the near-optimal learning policy. The near-optimal learning process achieves \\(5.50\\times\\) speedup in Perceptron linear classification **(a)** and \\(2.41\\times\\) speedup in Transformer language modeling **(b)**.\n' +
      '\n' +
      'we **compute and report the evaluation metrics by treating the test examples, unseen during the policy optimization, as \\(\\mathbf{x}_{k}^{\\text{sfx}}\\) in Equation 2**.\n' +
      '\n' +
      '### Learning Policy Optimization Results\n' +
      '\n' +
      'A near-optimal learning policy can be found with the method in Section 4.1.In Figure 4, we show the optimization process of finding the optimal learning policy. We plot the learning policy optimization loss \\(J(\\mathbf{\\gamma})\\), which is also the AUC of \\(L^{\\text{dst}}(\\mathbf{\\theta}_{t})\\) in the learning process induced by \\(\\mathbf{\\gamma}_{t}\\), and the corresponding compression ratio \\(\\mathrm{CR}=\\frac{T\\log|V|}{\\sum_{t=1}^{T}L^{\\text{dst}}(\\mathbf{\\theta}_{t})}\\), where \\(V\\) is the size of the label / vocabulary space for Perceptron / transformer (see Appendix A.1 for more explanation). The curve of \\(J(\\mathbf{\\gamma})\\) is smooth and almost converges at the end, indicating that a near-optimal learning policy is found.\n' +
      '\n' +
      'The near-optimal learning policy yields a high acceleration ratio of the learning speed.In Figure 5, we plot the curve of \\(L^{\\text{dst}}(\\mathbf{\\theta}_{t})\\) when the Perceptron and Transformer are trained under the conventional and near-optimal learning policies. The near-optimal policies significantly improve the loss AUC, bringing about acceleration \\(5.50\\times\\) and \\(2.41\\times\\) at the end of the Perceptron and Transformer training, respectively. Note that all reported metrics are computed on the test set unseen during the policy optimization, suggesting that the near-optimal policy does not over-fit the specific examples used to compute \\(L^{\\text{dst}}(\\mathbf{\\theta}_{t})\\) but helps the model learn faster on the desired distribution.\n' +
      '\n' +
      '### Direct Verification of Learning Law (Theorem 3.1)\n' +
      '\n' +
      'We examine the similarity between \\(\\mathrm{CT}_{n,t}\\) which is the discrete version of the individual sample contribution \\(\\mathrm{CT}_{n}(t)\\) in a certain learning policy and satisfies \\(\\mathrm{CT}_{n,t}=\\mathrm{CT}_{n}(t)\\) for \\(t=1,2,\\cdots,T\\). The similarity (\\(\\mathrm{SIM}\\)) is measured by the _Signal-Noise-Ratio_ of \\(\\mathrm{CT}_{n,t}\\):\n' +
      '\n' +
      '\\[\\mathrm{SIM}_{t}=\\frac{\\overline{\\mathrm{CT}}_{t}}{s_{\\mathrm{CT},t}}, \\tag{7}\\]\n' +
      '\n' +
      'where \\(\\overline{\\mathrm{CT}}_{t}=\\sum_{n=1}^{N}\\gamma_{n,t}\\mathrm{CT}_{n,t}\\) is the weighted mean and \\(s_{\\mathrm{CT},t}=\\sqrt{\\frac{\\sum_{n=1}^{N}1\\left[\\gamma_{n,t}\\neq 0 \\right]\\left(\\mathrm{CT}_{n,t}-\\overline{\\mathrm{CT}}_{t}\\right)^{2}}{\\sum_{n =1}^{N}1\\left[\\gamma_{n,t}\\neq 0\\right]-1}}\\) is the standard deviation of \\(\\mathrm{CT}_{n,t}\\) for training examples with non-zero weight. The higher \\(\\mathrm{SIM}_{t}\\) means that the training examples have more similar \\(\\mathrm{CT}_{n,t}\\). Note that \\(\\mathrm{SIM}_{t}\\) is dimensionless, which avoids the impact of the absolute value scale change of \\(\\mathrm{CT}_{n,t}\\) during learning. We also consider \\(\\overline{\\mathrm{SIM}}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{SIM}_{t}\\), which summarizes the similarities of \\(\\mathrm{CT}_{n,t}\\) throughout the learning process.\n' +
      '\n' +
      'Figure 6: Empirical evidence of our Learning Law (Theorem 3.1) in Perceptron linear classification **(a)** and Transformer language modeling **(b)** tasks. We measure the degree of similarity in contribution among different samples by \\(\\mathrm{SIM}_{t}\\), the _Signal-Noise-Ratio_ of the contribution \\(\\mathrm{CT}_{n,t}\\) of training examples, calculated as the mean divided by the standard deviation of \\(\\mathrm{CT}_{n,t}\\) across examples (Equation 7). Higher \\(\\mathrm{SIM}_{t}\\) means better contribution similarity. We plot \\(\\mathrm{SIM}_{t}\\) with respect to the desired loss \\(L^{\\text{dst}}(\\mathbf{\\theta}_{t})\\) under different learning processes. Each line is a certain learning process, whose color means the corresponding compression ratio (\\(\\mathrm{CR}\\)). Runs with higher \\(\\mathrm{CR}\\) generally get higher \\(\\mathrm{SIM}_{t}\\) throughout learning, indicating that the example contributions are more similar to each other in a learning process closer to the optimum, which is in line with our Learning Law (Theorem 3.1).\n' +
      '\n' +
      'Higher compression ratio correlates with higher sample contribution similarities.In Figure 6, we examine the value of \\(\\mathrm{SIM}_{t}\\) in the learning process induced by each policy found along the optimization process of \\(\\mathbf{\\gamma}_{t}\\). Since the found policies bring about faster convergence, we plot \\(\\mathrm{SIM}_{t}\\) with respect to \\(L^{\\text{\\tiny{flr}}}(\\mathbf{\\theta}_{t})\\), rather than \\(t\\). In this way, \\(\\mathrm{SIM}_{t}\\) are compared at the same "stage" of the model learning, migrating the impact of different convergence speeds. Figure 6 demonstrates that the learning process with a higher compression ratio (\\(\\mathrm{CR}\\)) generally keeps higher \\(\\mathrm{SIM}_{t}\\) in model learning, indicating that the contributions \\(\\mathrm{CT}_{n,t}\\) of individual samples are more similar to each other throughout the learning process, which aligns with our Learning Law (Theorem 3.1).\n' +
      '\n' +
      'Sample contributions tend to be equal when the learning process approaches the optimum.In Figure 7, we plot \\(\\overline{\\mathrm{SIM}}\\) with respect to \\(\\mathrm{CR}\\) for each learning process. We observe an evident tendency that \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\) when \\(\\mathrm{CR}\\) approaches a certain value. Accordingly, we use the function \\(\\overline{\\mathrm{SIM}}=\\log\\left(\\frac{a}{b-\\mathrm{CR}}\\right)^{c}\\) to fit the tendency of the experimental observations. Figure 7 indicates that when the learning process continuously improves until the optimum (\\(\\mathrm{CR}\\to b\\)), the standard deviation of \\(\\mathrm{CT}_{n,t}\\) should be zero to allow \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\). This verifies Learning Law (Theorem 3.1) that the contributions of non-zero-weight training samples (\\(\\mathrm{CT}_{n,t}\\)) are identical in optimal learning.\n' +
      '\n' +
      '### Properties of Zero-Weight Examples\n' +
      '\n' +
      'The experiments in Section 4.4 mostly focus on the non-zero-weight examples. In this section, we provide more empirical evidence for Learning Law (Theorem 3.1) by examining the properties of the examples with \\(\\gamma_{n,t}=0\\). We derive three properties of the optimal learning dynamics from Theorem 3.1 and then verify them through experiments. **The first property** guarantees that examples with non-positive contributions receive \\(\\gamma_{n,t}=0\\), indicating that the "noisy" examples at each time step are excluded by the optimal learning policy:\n' +
      '\n' +
      '**Property 4.1**.: _The training example \\(x_{n}^{\\text{\\tiny{flr}}}\\) whose \\(\\mathrm{CT}_{n,t}\\leq 0\\) gets \\(\\gamma_{n,t}=0\\) before the model converges._\n' +
      '\n' +
      'Proof.: Before convergence, \\(\\frac{\\mathrm{d}L^{\\text{\\tiny{flr}}}(\\mathbf{\\theta}(t))}{\\mathrm{d}t}<0\\) holds, indicating \\(\\mathrm{CT}_{n,t}>0\\) for \\(x_{n}^{\\text{\\tiny{flr}}}\\) that satisfies \\(\\gamma_{n,t}>0\\), according to Theorem 3.1. Therefore, \\(\\mathrm{CT}_{n,t}\\leq 0\\Rightarrow\\gamma_{n,t}=0\\).\n' +
      '\n' +
      'Figure 7: Empirical evidence of the Learning Law (Theorem 3.1) in Perceptron linear classification **(a)** and Transformer language modeling **(b)** tasks. Following Figure 6, we consider \\(\\overline{\\mathrm{SIM}}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{SIM}_{t}\\), which summarizes the similarity of the training example contributions in a learning process. We plot the relationship between \\(\\overline{\\mathrm{SIM}}\\) and \\(\\mathrm{CR}\\), and observe an evident tendency that \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\) when \\(\\mathrm{CR}\\) approaches a certain value, which can be fit by \\(\\overline{\\mathrm{SIM}}=\\log\\left(\\frac{a}{b-\\mathrm{CR}}\\right)^{c}\\). When the learning process approaches the optimum (\\(\\mathrm{CR}\\to b\\)), the standard deviations of training example contributions should be zero to allow \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\). This verifies Learning Law (Theorem 3.1) that all training examples have the same contribution to the model in optimal learning.\n' +
      '\n' +
      '_Empirical Evidence._ We calculate the fraction of zero-weight examples (\\(\\gamma_{n,t}=0\\)) among all examples with non-positive contributions at \\(t\\) (\\(\\mathrm{CT}_{n,t}\\leq 0\\)): \\(\\frac{\\sum_{n,t}1[\\gamma_{n,t}=0]1[\\mathrm{CT}_{n,t}\\leq 0]}{\\sum_{n,t}1[ \\mathrm{CT}_{n,t}\\leq 0]}\\) and plot this fraction with respect to the \\(\\mathrm{CR}\\) value of the corresponding learning process in Figure 8. We can see that when the learning process approaches the optimum, the fraction tends to 100%, indicating that the non-contributive examples are discarded.\n' +
      '\n' +
      '**The second property** is derived only for Perceptron linear classification, which indicates that the optimal learning policy will ignore those perfectly learned training examples:\n' +
      '\n' +
      '**Property 4.2**.: _For Perceptrons, the perfectly learned \\(x_{n}^{\\mathrm{trn}}\\), whose margin \\((2y_{n}^{\\mathrm{trn}}-1)\\mathbf{\\theta}_{t}\\cdot\\mathbf{z}_{n}^{\\mathrm{trn}} \\rightarrow+\\infty\\) at the time step \\(t\\), gets \\(\\gamma_{n,t}=0\\) in the optimal learning policy when the model is yet converged._\n' +
      '\n' +
      'Proof.: When \\((2y_{n}^{\\text{trn}}-1)\\mathbf{\\theta}_{t}\\cdot\\mathbf{z}_{n}^{\\text{trn}}\\to+\\infty\\), we have \\(o_{n}^{\\text{trn}}-y_{n}^{\\text{trn}}\\to 0\\), which means \\(\\nabla l(x_{n}^{\\text{trn}},\\mathbf{\\theta}_{t})=(o_{n}^{\\text{trn}}-y_{n}^{\\text {trn}})\\mathbf{z}_{n}^{\\text{trn}}\\to\\mathbf{0}\\) and \\(\\mathrm{CT}_{n,t}\\to 0\\). Assuming \\(\\gamma_{n,t}\\neq 0\\), according to Theorem 3.1, we have \\(\\mathrm{CT}_{n,t}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dsr}}(\\mathbf{\\theta}(t))\\) in the optimal learning process, which means that \\(\\big{|}\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dsr}}(\\mathbf{\\theta}(t))\\big{|}\\) should be arbitrarily small. This does not hold when the model is not converged. Therefore, we have \\(\\gamma_{n,t}=0\\). \n' +
      '\n' +
      '_Empirical Evidence._ In Figure 9, we plot the cumulative probability distribution function of \\(\\frac{\\gamma_{n,t}}{\\max_{n}\\{\\gamma_{n,t}\\}}\\) for the well-learned Perceptron training examples \\(x_{n}^{\\text{trn}}\\) with near-zero per-instance training loss: \\(l(x_{n}^{\\text{trn}},\\mathbf{\\theta})<1\\times 10^{-6}\\). Figure 9 shows that for the near-optimal policy, more than 90% well-learned examples have relatively low \\(\\gamma_{n,t}\\) (\\(\\mathbf{<}\\) 0.2 \\(\\max_{n}\\{\\gamma_{n,t}\\}\\)). This trend becomes more evident as the learning policy approaches the optimum (\\(\\mathrm{CR}\\) increases), which verifies Property 4.2.\n' +
      '\n' +
      'Figure 8: Empirical evidence of Property 4.1: non-contributive and noisy examples are excluded in optimal learning. The y-axis is the fraction of zero-weight examples among those with \\(\\mathrm{CT}_{n,t}\\leq 0\\) at the same time step. Each point represents a learning policy, which tends to assign the example weight \\(\\gamma_{n,t}=0\\) to 100% of noisy and non-contributive data when it approaches the optimum.\n' +
      '\n' +
      'Figure 9: Empirical evidence of Property 4.2: perfectly learned examples are ignored in optimal learning. We plot the cumulative distribution function (CDF) of the example weights \\(\\gamma_{n,t}\\) that satisfies \\(l(x_{n}^{\\text{trn}},\\mathbf{\\theta}_{t})<1\\times 10^{-6}\\). Each line corresponds to a learning process. A large fraction of low-loss examples (perfectly learned) in the near-optimal learning obtain small \\(\\gamma_{n,t}\\) values (ignored), and this tendency becomes more evident when the learning approaches its optimum (\\(\\mathrm{CR}\\) increases).\n' +
      '\n' +
      '**The third property** suggests that the optimal learning policy will discard the "redundant" training examples. Although this property is derived from Perceptron linear classification, we empirically find that it also applies to Transformer language modeling. We call a set \\(\\{x_{n}\\}_{n=1}^{N}\\) has "redundant" examples when the example inputs in the set are linearly correlated, i.e., there exist \\(K\\) scalars \\(\\{\\alpha_{n}\\}_{n=1}^{N}\\), not all zero, such that \\(\\sum_{n=1}^{N}\\alpha_{n}\\mathbf{z}_{n}=\\mathbf{0}\\).\n' +
      '\n' +
      '**Property 4.3**.: _For Perceptrons, if the training set \\(\\{x_{n}^{\\mathrm{trn}}\\}_{n=1}^{N}\\) has redundant examples, with probability 1, at least one example \\(x_{i}^{\\mathrm{trn}}\\) gets \\(\\gamma_{i,t}=0\\) at the time step \\(t\\) when the model is yet converged in the optimal learning process._\n' +
      '\n' +
      'Proof.: Given that \\(\\{x_{n}^{\\text{{\\sf{tm}}}}\\}_{n=1}^{N}\\) has redundant examples, there exist scalars \\(\\{\\alpha_{n}\\}_{n=1}^{N}\\), not all zero, such that \\(\\sum_{n=1}^{N}\\alpha_{n}\\mathbf{z}_{n}^{\\text{{\\sf{tm}}}}=\\mathbf{0}\\), which means \\(\\sum_{n=1}^{N}\\frac{\\alpha_{n}}{\\sigma_{n}^{\\text{{\\sf{tm}}}}-y_{n}^{\\text{{ \\sf{tm}}}}}\\mathrm{CT}_{n,t}=0\\). Assuming \\(\\forall 1\\leq n\\leq N\\), \\(\\gamma_{n,t}\\neq 0\\), according to Theorem 3.1, we have \\(\\mathrm{CT}_{n,t}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{{\\sf{ds}}}}( \\boldsymbol{\\theta}(t))\\), suggesting \\(\\left(\\sum_{n=1}^{N}\\frac{\\alpha_{n}}{\\sigma_{n}^{\\text{{\\sf{tm}}}}-y_{m}^{ \\text{{\\sf{tm}}}}}\\right)\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{{\\sf{ds}}}}( \\boldsymbol{\\theta}(t))=0\\). For i.i.d. inputs \\(\\{\\mathbf{z}_{n}^{\\text{{\\sf{tm}}}}\\}_{n=1}^{N}\\), with probability 1, \\(\\sum_{n=1}^{N}\\frac{\\alpha_{n}}{\\sigma_{n}^{\\text{{\\sf{tm}}}}-y_{m}^{\\text{{ \\sf{tm}}}}}\\neq 0\\), which means \\(\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{{\\sf{ds}}}}(\\boldsymbol{\\theta}(t))=0\\). This does not hold when the model is yet converged. Therefore, we have the property that \\(\\exists 1\\leq n_{0}\\leq N,\\text{such that }\\gamma_{n_{0},t}=0\\). \n' +
      '\n' +
      '_Empirical Evidence_. In Figure 10, we visualize the dynamics of the \\(\\gamma_{n,t}\\) value satisfying \\(\\mathrm{CT}_{n,t}>0\\) throughout the learning process of Perceptron and Transformer. For Perceptron, the model dimension (128) is lower than the number of training examples (4096), which means the training dataset is redundant. Figure 10 shows that, _given the absence of the non-contributive examples_, a large fraction of \\(\\boldsymbol{\\gamma}_{t}\\) still receives relatively small values before the model converges, which is caused by the redundancy of the training set. In Figure 10, we observe a similar phenomenon for Transformer, although the dimension of \\(\\boldsymbol{\\theta}_{t}\\) is larger than the number of training instances. We suspect the reason is that the intrinsic dimension of Transformer is usually much smaller than the dimension of \\(\\boldsymbol{\\theta}_{t}\\)[1], which leads to the redundancy of the training set.\n' +
      '\n' +
      '### Essence of Learning Acceleration\n' +
      '\n' +
      'We investigate the essential improvement brought by the near-optimal learning policy in the perspective of the scaling laws of LMs [13], which reveals a power law between the increase of training steps and the reduction of the test loss (\\(L^{\\text{{\\sf{ds}}}}(\\boldsymbol{\\theta}_{t})\\)) after a warming-up stage \\(t_{0}\\):\n' +
      '\n' +
      '\\[L^{\\text{{\\sf{ds}}}}(\\boldsymbol{\\theta}_{t})=\\left(\\frac{B}{t}\\right)^{\\beta} \\!,\\;t>t_{0}, \\tag{8}\\]\n' +
      '\n' +
      'where \\((B,\\beta)\\) are scaling law coefficients. In the following, we study the scaling properties of the learning processes induced by the conventional and near-optimal learning policies.\n' +
      '\n' +
      'Figure 10: Empirical evidence of Property 4.3: redundant training examples are discarded in optimal learning. We randomly sample 2048 training examples satisfying \\(\\mathrm{CT}_{n,t}>0\\) (contributive and unlearned examples) throughout the near-optimal learning process and show the dynamics of the example weight \\(\\gamma_{n,t}\\) (represented by the color in (a) and (b)). Since Perceptron converges quickly, we only plot its \\(\\gamma_{n,t}\\) dynamics for \\(t\\leq 50\\). The near-optimal policies assign \\(\\gamma_{n,t}=0\\) to redundant examples in addition to the perfectly learned and non-contributive data points.\n' +
      '\n' +
      'The near-optimal learning policy improves the scaling law coefficients of LMs.In Figure 11, we fit the Transformer\'s loss curves induced by the conventional and near-optimal learning policies with Equation 8 by setting \\(t_{0}=400\\)4. We observe that the near-optimal learning process still follows the scaling law, with \\(B\\) and \\(\\beta\\) improved by 96.6% and 21.2% respectively. Additionally, Table 2 shows that the improvement holds for the near-optimal policies found in the setting of larger \\(T\\) and \\(N\\). We let \\(N\\) grow with \\(T\\) to ensure the sufficiency of training data [12]. The improvement of scaling law coefficients, especially \\(\\beta\\), provides significant potential in boosting the speed of LLM learning by taking advantage of power law growth. For two learning policies \\(\\mathbf{\\gamma}^{(1)}\\) and \\(\\mathbf{\\gamma}^{(2)}\\) which induce two loss curves \\(L^{\\text{dsr}}_{\\mathbf{\\gamma}^{(1)}}(\\mathbf{\\theta}_{t})\\) and \\(L^{\\text{dsr}}_{\\mathbf{\\gamma}^{(2)}}(\\mathbf{\\theta}_{t})\\) with two sets of scaling law coefficients \\((B_{1},\\beta_{1})\\) and \\((B_{2},\\beta_{2})\\), the acceleration ratio of \\(\\mathbf{\\gamma}^{(2)}\\) over \\(\\mathbf{\\gamma}^{(1)}\\) is:\n' +
      '\n' +
      'Footnote 4: In practice, we convert Equation 8 to \\(\\ln L^{\\text{dsr}}(\\mathbf{\\theta}_{t})=-\\beta\\ln t+\\beta\\ln B\\) and perform linear regression.\n' +
      '\n' +
      '\\[\\mathrm{AR}=\\frac{T}{\\arg\\min_{t}\\left\\{L^{\\text{dsr}}_{\\mathbf{\\gamma}^{(2)}}(\\bm {\\theta}_{t})\\leq L^{\\text{dsr}}_{\\mathbf{\\gamma}^{(1)}}(\\mathbf{\\theta}_{T})\\right\\}}= \\frac{B_{1}^{\\frac{\\beta_{1}}{\\beta_{2}}}}{B_{2}}T^{1-\\frac{\\beta_{1}}{\\beta_ {2}}}. \\tag{9}\\]\n' +
      '\n' +
      'For an LM pre-trained for 10M steps, we will obtain more than \\(9\\times\\) acceleration at the end of the training if the scaling property of the LM is improved as in Figure 11 and Table 2. Based on the recent experience in training LLMs [13, 14, 15], models are far from fully converged under the current training budget, which means small models (like 7B) have the potential to reach the performance of large models (like 65B), given enough training steps. However, according to Chinchilla\'s law [12], extending the training steps requires more computation than enlarging the model to achieve a certain performance. Therefore, by optimizing the learning policy to improve\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline \\(T\\) & \\(N\\) & \\(|\\frac{\\Delta B}{B}|\\) (\\%) & \\(|\\frac{\\Delta\\beta}{\\beta}|\\) (\\%) & \\(\\mathrm{AR}\\) \\\\ \\hline\n' +
      '1K & \\(2^{12}\\) & 88.5 & 10.0 & 2.16 \\\\\n' +
      '2K & \\(2^{13}\\) & 94.9 & 18.0 & 2.31 \\\\\n' +
      '4K & \\(2^{14}\\) & 93.7 & 18.7 & 2.41 \\\\\n' +
      '8K & \\(2^{15}\\) & 94.8 & 19.0 & 2.48 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: The improvements of the scaling law coefficients brought by the near-optimal learning policy for different total training steps (\\(T\\)) and data sizes (\\(N\\)) in Transformer language modeling. The vocabulary size increases with the growth of \\(N\\) (see Appendix D for details). \\(\\mathrm{AR}\\) stands for the acceleration ratio as defined in Equation 9. The improvements hold for larger \\(T\\) and \\(N\\).\n' +
      '\n' +
      'Figure 11: Illustration of the scaling law [12]: \\(L^{\\text{dsr}}(\\mathbf{\\theta}_{t})=(B/t)^{\\beta}\\) for conventional and near-optimal LM learning in Transformer language modeling. We fit the loss curves by the scaling law to obtain the correlation coefficient \\(r^{2}\\) and show the loss curve (solid lines) together with the fit curve (dashed lines) in a log-log plot. The scaling law fits well for both conventional and near-optimal LM learning. The near-optimal LM learning essentially improves the coefficients \\((B,\\beta)\\) in the scaling law by 96.6% and 21.2%, which shows great potential for speedup in training LLMs.\n' +
      '\n' +
      'learning speed, the cost of training well-performed small models can be largely reduced, which is beneficial both for open-source endeavors in the LM research community and for the efficiency of industrial products. This indicates the promise and significance of designing practical learning policy optimization approaches, and our theory can be a valuable guide.\n' +
      '\n' +
      '## 5 Related Work\n' +
      '\n' +
      'Improving the Learning Speed of Language Model.There is a broad range of works that propose approaches to accelerate LM learning speed such as modifying model architectures [20, 21] or optimizers [20, 24, 25]. There are also works studying the pre-training programming to speed up LM convergence, such as data de-duplication [26, 27], domain mixture [28], intrinsic task discovery [13], and online data selection or reordering [3, 1], which can be viewed as special cases of optimizing learning policy. Unlike these works, we investigate the principles of optimizing LM learning in this paper.\n' +
      '\n' +
      'Language Modeling and Lossless Compression.The recent success of LLMs calls for new interpretations beyond classical statistic learning theory for the fact that larger model sizes constantly cause better downstream generalization [24, 25]. One of the interpretations is to view the next-token-prediction training process of an LM as lossless data compression [1, 13, 14]. In this perspective, larger LMs have higher compression ratios, corresponding to better modeling of data generation regularities. It is worth noting that some recent works [24, 25] explore using well-trained LMs as compressors and thus the model sizes should be counted into the compressed data. Unlike these works, viewing LM training as compression does not require including the model parameters in the compressed data (see Appendix A.1 for a constructive proof) and thus is more compatible with the model size scaling law of LMs [25].\n' +
      '\n' +
      '## 6 Discussion and Conclusion\n' +
      '\n' +
      'Summary.In this work, we establish a theory for the optimal learning of LMs. We propose an objective that maximizes the compression ratio in an LM-training-as-losses-compression view. Then we derive a theorem, named _Learning Law_, suggesting that all examples should be equally contributive to the LM in the optimal learning process, which is then validated by experiments in linear classification and real-world language modeling tasks. Finally, we empirically show that the optimal learning process essentially improves the scaling law coefficients of LMs, which sheds light on future works that design practical learning acceleration approaches.\n' +
      '\n' +
      'Limitations.One limitation of our work is that the experiments are conducted on relatively small scales. This is because our method to find the near-optimal learning policy corresponds to training a neural network with \\(L\\times T\\) layers, where \\(L\\) is the layers of the LM and \\(T\\) is the LM\'s total training steps (see Appendix C for details). This leads to a high computational overhead when \\(L\\) and \\(T\\) scale up. However, since the theoretical derivation is generally applicable, we believe that our theory can be applied to LLMs. Another limitation is that our derivation assumes the LM is trained with full-batch GD, rather than some more commonly used techniques like mini-batch Adam [1]. Since these methods are essentially gradient-based, our theory can still offer insights to future LM learning acceleration studies based on these techniques [24, 25].\n' +
      '\n' +
      'Future Work.We believe that an important direction of future work is designing practical methods to find the optimal learning policies based on our theory for the large-scale training of LMs. Indeed, there are non-negligible challenges in this direction. Since the learning law provides a necessary condition for the learning policy\'s optimality, more regularization conditions may be required to prevent sub-optimal solutions. In addition, the approach to finding the optimal learning policy should be efficient enough without contributing much to the overall computation cost. Nevertheless, our work demonstrates the promise and potential of this direction. According to recent works on LLMs training [24, 25], the losses are still far from convergence, which means that small models have the potential to reach the similar performance as large models, but are hindered by the computation overhead brought by the large total training steps. The optimal learning policy potentially brings about a large acceleration of training with the help of the power-law growth in Equation 9, which makes it possible to explore the limits of LMs given (inevitably) constrained computation and train a well-performed small LM that replaces current LLMs in practice.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [ABL\\({}^{+}\\)22] Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Towards tracing knowledge in language models back to the training data. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Findings of EMNLP_, 2022.\n' +
      '* [ADF\\({}^{+}\\)23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [AGZ21] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In _Proceedings of ACL_, 2021.\n' +
      '* [APRW23] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. In _NeurIPS 2023 Workshop on R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models_, 2023.\n' +
      '* [ATS\\({}^{+}\\)23] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. SemDeDup: Data-efficient learning at web-scale through semantic deduplication. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023.\n' +
      '* [BC11] HH Bauschke and PL Combettes. _Convex Analysis and Monotone Operator Theory in Hilbert Spaces_. Springer, 2011.\n' +
      '* [Bel19] Fabrice Bellard. NNCP: Lossless data compression with neural networks, 2019.\n' +
      '* [Ber16] Dimitri Bertsekas. _Nonlinear programming_, volume 4. Athena scientific, 2016.\n' +
      '* [BHA\\({}^{+}\\)21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselt, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [BMR\\({}^{+}\\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are few-shot learners. In _Proceedings of NeurIPS_, 2020.\n' +
      '* [CM03] Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In _Proceedings of NeurIPS_, 2003.\n' +
      '* [CMS\\({}^{+}\\)23] Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, et al. h2oGPT: Democratizing large language models. _arXiv preprint arXiv:2306.08161_, 2023.\n' +
      '* [CND\\({}^{+}\\)23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, et al. PaLM: Scaling language modeling with pathways. _JMLR_, 2023.\n' +
      '* [CRB\\({}^{+}\\)23] Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce Zhang, Frederic Sala, and Christopher Re. Skill-it! a data-driven skills framework for understanding and training language models. In _Proceedings of NeurIPS_, 2023.\n' +
      '* [DRD\\({}^{+}\\)24] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. In _Proceedings of ICLR_, 2024.\n' +
      '* [EC21] Omer Elkabetz and Nadav Cohen. Continuous vs. discrete optimization of deep neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Proceedings of NeurIPS_, 2021.\n' +
      '* [EL23] Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still speak coherent english? _arXiv preprint arXiv:2305.07759_, 2023.\n' +
      '* [Eng01] Andreas Engel. _Statistical mechanics of learning_. Cambridge University Press, 2001.\n' +
      '\n' +
      '* [GAH23] David Grangier, Pierre Ablin, and Awni Hannun. Bilevel optimization to learn training distributions for language modeling under domain shift. In _NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models_, 2023.\n' +
      '* [GHLH22] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for few-shor learning. In _Proceedings of ACL_, 2022.\n' +
      '* [GS\\({}^{+}\\)00] Izrail Moiseevitch Gelfand, Richard A Silverman, et al. _Calculus of variations_. Courier Corporation, 2000.\n' +
      '* [HBD\\({}^{+}\\)20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In _Proceedings of ICLR_, 2020.\n' +
      '* [HBM\\({}^{+}\\)22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [HZD\\({}^{+}\\)21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past, present and future. _AI Open_, 2021.\n' +
      '* [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of CVPR_, 2016.\n' +
      '* [JSM\\({}^{+}\\)23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* [KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of ICLR_, 2015.\n' +
      '* [KEC22] Ilia Kulikov, Maksim Eremeev, and Kyunghyun Cho. Characterizing and addressing the issue of over-smoothing in neural autoregressive sequence modeling. In _Proceedings of AACL_, 2022.\n' +
      '* [KL17] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _Proceedings of ICML_, 2017.\n' +
      '* [KMH\\({}^{+}\\)20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [KPA12] Celeste Kidd, Steven T Piantadosi, and Richard N Aslin. The goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex. _PloS one_, 7(5):e36399, 2012.\n' +
      '* [LLH\\({}^{+}\\)24] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training. In _Proceedings of ICLR_, 2024.\n' +
      '* [LXLM23] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models. In _Proceedings of ICML_, 2023.\n' +
      '* [MBR\\({}^{+}\\)22] Soren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holtgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In _Proceedings of ICML_, 2022.\n' +
      '* [MCKX22] Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. Trace: A fast transformer-based general-purpose lossless compressor. In _Proceedings of WWW_, New York, NY, USA, 2022.\n' +
      '* [Met09] Janet Metcalfe. Metacognitive judgments and control of study. _Current directions in psychological science_, 18(3):159-163, 2009.\n' +
      '\n' +
      '* [MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. _arXiv preprint arXiv:1812.06162_, 2018.\n' +
      '* [MP43] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. _The bulletin of mathematical biophysics_, 1943.\n' +
      '* [NKB\\({}^{+}\\)19] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: Where bigger models and more data hurt. In _Proceedings of ICLR_, 2019.\n' +
      '* [Ope22] OpenAI. OpenAI: Introducing chatgpt, 2022.\n' +
      '* [Ope23] OpenAI. GPT-4 technical report, 2023.\n' +
      '* [PGM\\({}^{+}\\)19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In _Proceedings of NeurIPS_, 2019.\n' +
      '* [PLKS20] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. In _NeurIPS_, 2020.\n' +
      '* [Pon18] Lev Semenovich Pontryagin. _Mathematical theory of optimal processes_. Routledge, 2018.\n' +
      '* [PR12] Warren B Powell and Ilya O Ryzhov. _Optimal learning_, volume 841. John Wiley & Sons, 2012.\n' +
      '* [Rae23] Jack Rae. Compression for agi, 2023.\n' +
      '* [RM87] David E. Rumelhart and James L. McClelland. Learning internal representations by error propagation. In _Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations_, 1987.\n' +
      '* [RRRH20] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training trillion parameter models. In _Proceedings of SC20_, 2020.\n' +
      '* [SDBD20] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. In _Proceedings of ICLR_, 2020.\n' +
      '* [TLI\\({}^{+}\\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [TSAM23] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In _Proceedings of NeurIPS_, 2023.\n' +
      '* [Vap99] Vladimir Vapnik. _The nature of statistical learning theory_. Springer science & business media, 1999.\n' +
      '* [VNK\\({}^{+}\\)23] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, and Srinivas Shakkottai. LLMZip: Lossless text compression using large language models. _arXiv preprint arXiv:2306.04050_, 2023.\n' +
      '* [VSP\\({}^{+}\\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proceedings of NeurIPS_, 2017.\n' +
      '\n' +
      '* [WKR\\({}^{+}\\)19] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. In _Proceedings of ICLR_, 2019.\n' +
      '* [WSSC19] Robert C Wilson, Amitai Shenhav, Mark Straccia, and Jonathan D Cohen. The eighty five percent rule for optimal learning. _Nature communications_, 10(1):4646, 2019.\n' +
      '* [WTB\\({}^{+}\\)22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, et al. Emergent abilities of large language models. _TMLR_, 2022.\n' +
      '* [WWL\\({}^{+}\\)23] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. Efficient large language models: A survey. _arXiv preprint arXiv:2312.03863_, 2023.\n' +
      '* [XPD\\({}^{+}\\)23] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. DoReMi: Optimizing data mixtures speeds up language model pretraining. In _Proceedings of NeurIPS_, 2023.\n' +
      '* [XSML23] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. In _Proceedings of NeurIPS_, 2023.\n' +
      '* [XYH\\({}^{+}\\)20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In _Proceedings of ICML_, 2020.\n' +
      '* [YLR\\({}^{+}\\)20] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In _Proceedings of ICLR_, 2020.\n' +
      '* [ZH20] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language models with progressive layer dropping. _Proceedings of NeurIPS_, 2020.\n' +
      '* [ZHSJ20] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In _Proceedings of ICLR_, 2020.\n' +
      '\n' +
      'Discussion of LM Training as Lossless Compression\n' +
      '\n' +
      '### Original View: Compressing the Training Data.\n' +
      '\n' +
      'The idea of using an LM to compress data originates from the literature in the lossless text compression field [1, 13], and is recently adopted to interpret the essence of the next-token-prediction-based pre-training of LMs [14]. We restate its core spirit by the following Theorem and the constructive proof from [14]:\n' +
      '\n' +
      '**Theorem A.1**.: _Consider an LM trained on a text corpus \\(\\mathcal{D}\\) with \\(M\\) tokens using mini-batch next-token-prediction for one epoch. Let \\(B\\) be the number of tokens in a batch and \\(L_{t}\\) be the batch-averaged training loss at the time step \\(t\\). Assume that \\(M\\) is divisible by \\(B\\). The training process can be viewed as lossless compression of the training data. The description length of the compressed data \\(\\mathcal{C}\\) is_\n' +
      '\n' +
      '\\[d(\\mathcal{C})=\\sum_{t=1}^{M/B}B\\cdot L_{t}+d(\\mathrm{LM}), \\tag{10}\\]\n' +
      '\n' +
      '_where \\(d(\\mathrm{LM})\\) is the length of the necessary code represented by a 0-1 string to run the LM training._\n' +
      '\n' +
      'Proof.: The basic idea of the proof is to construct a lossless encoding and decoding process for \\(\\mathcal{D}\\) with the LM. Let \\(p_{\\mathbf{\\theta}_{t}}(\\cdot|w_{<m})\\) be the output distribution of the LM parameterized by \\(\\mathbf{\\theta}_{t}\\) at the time step \\(t\\), conditioning on the token prefix \\(w_{<m}=[w_{m-1},w_{m-2},\\cdots,w_{1}]\\). For simplicity, we assume that the LM is trained using mini-batch Stochastic Gradient Decent (SGD) with a learning rate \\(\\eta\\), where each batch is linearized to a continuous list of tokens. The batch-averaged training loss is \\(L_{t}=-\\frac{1}{B}\\sum_{m=1}^{B}\\log p_{\\mathbf{\\theta}_{t}}(w_{m}|w_{<m})\\)5. The encoding the decoding process are described in Algorithm 1 and 2. Basically, the main body of the algorithms other than the blue-colored parts implements the LM training. For encoding, the description length of a token \\(w_{m}\\) is \\(-\\log p_{\\mathbf{\\theta}_{t}}(w_{m}|w_{<m})\\) according to Arithmetic Coding 5, and thus the compressed length of a batch \\(\\mathcal{W}=\\{w_{m}\\}_{m=1}^{B}\\) is \\(\\sum_{m=1}^{B}\\left[-\\log p_{\\mathbf{\\theta}_{t}}(w_{m}|w_{<m})\\right]=B\\cdot L_{t}\\). \\(d(\\mathcal{C})\\) equals the sum of per-batch description lengths throughout the training plus the length of the code for LM training. Therefore, we get \\(d(\\mathcal{C})=B\\cdot\\sum_{t=1}^{M/B}L_{t}+d(\\mathrm{LM})\\). For decoding, since the code for LM training is the same as that in encoding, we have \\(\\mathbf{\\theta}_{1}^{\\prime}=\\mathbf{\\theta}_{1}\\), and thus \\(w_{m}^{\\prime}=w_{m}\\) for any steps in Algorithm 2, which can be easily proved by mathematical induction. As a result, \\(\\mathcal{D}\\) can be completely reconstructed from \\(\\mathcal{C}\\), indicating the encoding (compression) is lossless. \n' +
      '\n' +
      'Footnote 5: \\(\\log(\\cdot)\\) stands for \\(\\log_{2}(\\cdot)\\) in the following sections.\n' +
      '\n' +
      '_Remark 1_.: The description length of the compressed data \\(d(\\mathcal{C})\\) is approximately proportional to the area under the training loss curve (\\(\\mathrm{AUC}\\)) when \\(M\\gg 1\\) because the size of LM training codes is much smaller than that of the compressed corpus and thus \\(d(\\mathcal{C})\\approx\\sum_{t=1}^{M/B}B\\cdot L_{t}=B\\cdot\\mathrm{AUC}\\).\n' +
      '\n' +
      '_Remark 2_.: Let \\(V\\) be the vocabulary size of the LM and assume \\(M\\gg 1\\). The corresponding compression ratio of the learning process in Theorem A.1 is \\(\\mathrm{CR}=\\frac{M\\log V}{d(\\mathcal{C})}\\approx\\frac{M\\log V}{\\sum_{t=1}^{M /B}B\\cdot L_{t}}\\propto\\frac{1}{\\mathrm{AUC}}\\). As the LM fits the data, we generally have \\(L_{t}<\\log V\\) because \\(\\log V\\) is the loss for a randomly initialized LM. This means the compression is valid, resulting in a compression ratio \\(\\mathrm{CR}>1\\).\n' +
      '\n' +
      'Altogether, Theorem A.1 bridges a connection between data compression and LM training. Generally, a higher compression ratio indicates that the compression algorithm models the underlying data knowledge better and corresponds to a better performed LM, as stated in the following remark:\n' +
      '\n' +
      '_Remark 3_.: The LM\'s ability to model the knowledge in data is characterized by the corresponding lossless compression ratio of its learning process, which is inversely proportional to the loss AUC.\n' +
      '\n' +
      'Note that the model parameters are not included in the calculation of \\(d(\\mathcal{C})\\), and enlarging the model sizes typically reduces the loss AUC, which explains the remarkable performance of LLMs. In addition, \\(d(\\mathcal{C})\\) relates to the whole LM training process, not just the final loss. This is in line with the fact that larger LMs tend to perform better than smaller models, even if their final losses are the same [11]. This observation supports the perspective that LM training can be conceptualized as a process of lossless data compression.\n' +
      '\n' +
      '### Our View: Compressing Data from the Desired Distribution.\n' +
      '\n' +
      'Although we also focus on the loss AUC throughout the paper, our setting differs from that in Appendix A.1: (1) we assume the LM is trained with full-batch Gradient Descent (GD) for multiple epochs while Theorem A.1 lies in the scenario where the LM is trained with SGD for only one epoch; (2) we consider \\(L^{\\text{sfx}}\\) computed on data other than the training examples, while \\(p_{\\mathbf{\\theta}_{t}}\\) in Equation 10 is computed on the training data. However, although not entirely rigorous, we argue that Remark 3 still holds despite the differences in (1) and (2). The reason is that: **regarding (1)**, mini-batch SGD is an approximation of GD, which means they share the similar training dynamics when the batch size of SGD is large enough; **regarding (2)**, just like the training loss AUC, the AUC of \\(L^{\\text{sfx}}\\) can be viewed as the description length of _compressing examples from the desired data distribution_ during the learning process. In this way, Remark 3 indicates that minimizing the AUC of \\(L^{\\text{sfx}}\\) corresponds to optimizing the data compression on the desired distribution, which improves the LM\'s ability to model the desired data knowledge. This is more of practical concern because in most scenarios, the model performance is measured on a dataset other than the training set, such as the validation set in classical machine learning [20], the high-quality held-out corpus in large-scaling pre-training [13], or the target set in domain adaption [21].\n' +
      '\n' +
      '```\n' +
      '0: Compressed data \\(\\mathcal{C}\\): list of 0-1 string\n' +
      '0: Training corpus \\(\\mathcal{D}\\)  Get the LM training code from the first string in \\(\\mathcal{C}\\)  Pop the first string from \\(\\mathcal{C}\\)  Initialize \\(\\mathcal{D}\\) to an empty list  Initialize the LM\'s parameters to \\(\\mathbf{\\theta}_{1}\\) for\\(t\\gets 1\\) to \\(M/B\\)do  Get a batch of tokens \\(\\mathcal{W}=\\{w_{m}\\}_{m=1}^{B}\\)  from the training corpus \\(\\mathcal{D}\\) for\\(m\\gets 1\\) to \\(B\\), \\(w_{m}\\in\\mathcal{W}\\)do  Encode \\(w_{m}\\) to a 0-1 string \\(s\\) with Arithmetic Coding  thetic Coding based on \\(p_{\\theta_{t}}(\\cdot|w_{<m})\\)  Append the 0-1 string \\(s\\) to \\(\\mathcal{C}\\) endfor \\(L_{t}\\leftarrow-\\frac{1}{B}\\sum_{m=1}^{B}\\log p_{\\mathbf{\\theta}_{t}}(w_{m}|w_{<m})\\) \\(\\mathbf{\\theta}_{t+1}\\leftarrow\\mathbf{\\theta}_{t}-\\eta\\nabla L_{t}\\) endfor\n' +
      '```\n' +
      '\n' +
      '**Algorithm 2** Decoding\n' +
      '\n' +
      '### Perceptron Training as Lossless Compression\n' +
      '\n' +
      'Viewing model training as lossless compression stems from the next-token-prediction learning paradigm of LMs. We show that this perspective also fits in the one-epoch Maximum Likelihood Estimation (MLE) training of Perceptrons on the linear classification task, where the _label of each example is compressed given the input vectors_. Specifically, the proof in Appendix A.1 still applies if we treat linear classification as a one-step language modeling with vocabulary size \\(V=2\\). Following the notation in Section 4.2, for a Perceptron parameterized by \\(\\mathbf{\\theta}_{t}\\) at the time step \\(t\\), its probability of outputting \\(y\\) conditioning on \\(\\mathbf{z}\\) is \\(p_{\\mathbf{\\theta}_{t}}(y|\\mathbf{z})=o^{y}(1-o)^{1-y}\\), where \\(o=\\sigma(\\mathbf{\\theta}_{t}\\cdot\\mathbf{z})\\). For a batch \\(\\mathcal{B}_{t}=\\{(z_{n},y_{n})\\}_{n=1}^{B}\\), the batch-averaged loss is \\(L_{t}=-\\frac{1}{B}\\sum_{n=1}^{B}\\log p_{\\mathbf{\\theta}_{t}}(y_{n}|\\mathbf{z}_{n})\\). With Algorithm 1 and 2 applied for encoding and decoding, the description length of the compressed \\(\\mathcal{B}_{t}\\) is \\(\\sum_{n=1}^{B}\\left[-\\log p_{\\mathbf{\\theta}_{t}}(y_{n}|\\mathbf{z}_{n})\\right]=B \\cdot L_{t}\\), which means Theorem A.1 still holds and the discussion in Appendix A.2 also applies. For a dataset with \\(N\\) examples in total, the compression ratio \\(\\operatorname{CR}\\approx\\frac{N\\log V}{\\sum_{n=1}^{N/B}B\\cdot L_{t}}=\\frac{N} {\\sum_{n=1}^{N/B}B\\cdot L_{t}}\\). For a randomly initialized \\(\\mathbf{\\theta}_{1}\\), \\(L_{1}\\approx 1\\), and as the model trains, \\(L_{t}\\to 0\\), indicating a valid data compressing process of compression ratio \\(\\operatorname{CR}>1\\).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '**Interpreting \\(\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}}\\).**\\(\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}}\\) measures how the change of \\(\\Gamma_{n}(t)\\) influence the change of \\(L^{\\text{dsf}}(t)\\) at the time step \\(t\\) when other free variables are fixed. Specifically, if \\(\\Gamma_{n}(t)\\) changes by a small value \\(\\Gamma_{n}(t)\\to\\Gamma_{n}(t)+\\Delta\\Gamma_{n}(t)\\), then \\(L^{\\text{dsf}}(t)\\) correspondingly changes by a small value \\(L^{\\text{dsf}}(t)\\to L^{\\text{dsf}}(t)+\\Delta L^{\\text{dsf}}(t)\\), and \\(\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}}=\\frac{\\Delta L^{\\text{ dsf}}(t)}{\\Delta\\Gamma_{n}(t)}\\). Then, we consider \\(\\frac{\\mathrm{d}L^{\\text{dsf}}(t)}{\\mathrm{d}t}\\) with Equation 3:\n' +
      '\n' +
      '\\[\\frac{\\mathrm{d}L^{\\text{dsf}}(t)}{\\mathrm{d}t} =\\nabla L^{\\text{dsf}}(\\mathbf{\\theta}(t))\\cdot\\frac{\\mathrm{d}\\mathbf{ \\theta}(t)}{\\mathrm{d}t} \\tag{16}\\] \\[=-\\sum_{n=1}^{N}\\gamma_{n}(t)\\nabla L^{\\text{dsf}}(\\mathbf{\\theta}( t))\\cdot\\nabla l(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))\\] \\[=-\\sum_{n=1}^{N}\\frac{\\mathrm{d}\\Gamma_{n}(t)}{\\mathrm{d}t} \\nabla L\\cdot\\nabla l_{n}.\\]\n' +
      '\n' +
      'As a result, for a small \\(\\Delta t\\), we have:\n' +
      '\n' +
      '\\[L^{\\text{dsf}}(t+\\Delta t)-L^{\\text{dsf}}(t)=-\\sum_{n=1}^{N}\\left[\\Gamma_{n}( t+\\Delta t)-\\Gamma_{n}(t)\\right]\\nabla L\\cdot\\nabla l_{n}. \\tag{17}\\]\n' +
      '\n' +
      'Now we consider the change of \\(L^{\\text{dsf}}(t)\\) and \\(\\Gamma_{n}\\) at \\(t+\\Delta t\\). Since \\(\\nabla L\\cdot\\nabla l_{n}\\) is computed at the time step \\(t\\), it is not affected by the variants. Therefore, we get:\n' +
      '\n' +
      '\\[\\Delta L^{\\text{dsf}}(t+\\Delta t)=-\\Delta\\Gamma_{n}(t+\\Delta t)\\nabla L\\cdot \\nabla l_{n}, \\tag{18}\\]\n' +
      '\n' +
      'When \\(\\Delta t\\to 0\\), \\(\\frac{\\Delta L^{\\text{dsf}}(t+\\Delta t)}{\\Delta\\Gamma_{n}(t+\\Delta t)}\\to \\frac{\\Delta L^{\\text{dsf}}(t)}{\\Delta\\Gamma_{n}(t)}=\\frac{\\partial L^{\\text {dsf}}}{\\partial\\Gamma_{n}}\\), which means:\n' +
      '\n' +
      '\\[\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}}=-\\nabla L\\cdot\\nabla l _{n}. \\tag{19}\\]\n' +
      '\n' +
      'By substituting Equation 19 into Equation 15, we obtain that for the \\(m^{\\text{th}}\\) and \\(n^{\\text{th}}\\) training examples satisfying \\(\\gamma_{m}(t)>0\\) and \\(\\gamma_{n}(t)>0\\) the following equation holds:\n' +
      '\n' +
      '\\[\\nabla L\\cdot\\nabla l_{m}=\\nabla L\\cdot\\nabla l_{n}=-\\dot{\\lambda}(t)=\\mathrm{ Const}, \\tag{20}\\]\n' +
      '\n' +
      'where \\(\\mathrm{Const}\\) stands for "a constant independent of \\(m\\) and \\(n\\)". Equation 20 is essentially equivalent to Equation 5.\n' +
      '\n' +
      '\\[\\begin{split}\\text{Proving \\text{Const}}=-\\frac{\\mathrm{d}L^{\\text{dsf}}(t)}{ \\mathrm{d}t}.\\end{split}\\]\n' +
      '\n' +
      'By substituting \\(\\nabla L\\cdot\\nabla l_{n}\\) with \\(\\mathrm{Const}\\) in Equation 16, we get:\n' +
      '\n' +
      '\\[\\begin{split}\\frac{\\mathrm{d}L^{\\text{dsf}}(t)}{\\mathrm{d}t}& =-\\sum_{n=1}^{N}\\frac{\\mathrm{d}\\Gamma_{n}(t)}{\\mathrm{d}t} \\cdot\\mathrm{Const},\\\\ &=-\\mathrm{Const}\\sum_{n=1}^{N}\\gamma_{n}(t),\\\\ &=-\\mathrm{Const}.\\end{split} \\tag{21}\\]\n' +
      '\n' +
      'As such, by combining Equation 20 with Equation 21, we complete the proof of Theorem 3.1.\n' +
      '\n' +
      '## Appendix C Details of Learning Policy Optimization\n' +
      '\n' +
      'In Section 4.1, we search for the optimal learning policy by Proximal Gradient Decent [1]. Specifically, we view the whole learning process in \\(0\\leq t\\leq T\\) as a neural network with \\(T\\) layers parameterized by \\(\\mathbf{\\gamma}=[\\mathbf{\\gamma}_{0},\\cdots,\\mathbf{\\gamma}_{t-1}]\\in\\mathbb{R}^{N\\times T}\\). As illustrated in Figure 12, each layer of the network consists of the gradient update function and a residual connection [11], where the "hidden states" are \\(\\mathbf{\\theta}_{t}\\). Then, we adopt Backward Propagation (BP; [14]) to compute \\(\\nabla_{\\mathbf{\\gamma}_{t}}J(\\mathbf{\\gamma})\\) in Equation 6. The backward operation at each layer is:\n' +
      '\n' +
      '\\[\\begin{split}\\frac{\\partial J}{\\partial\\mathbf{\\gamma}_{t}}& =-\\eta\\sum_{t^{\\prime}=t+1}^{T}\\nabla L^{\\text{dsf}}(\\mathbf{\\theta}_{t^ {\\prime}})\\frac{\\partial\\mathbf{\\theta}_{t^{\\prime}}}{\\partial\\mathbf{\\theta}_{t+1}} \\mathbf{G}^{\\text{trn}}(\\mathbf{\\theta}_{t})\\\\ \\frac{\\partial\\mathbf{\\theta}_{t^{\\prime}}}{\\partial\\mathbf{\\theta}_{t+1}}& =\\frac{\\partial\\mathbf{\\theta}_{t^{\\prime}}}{\\partial\\mathbf{\\theta}_{t+2}} \\left[\\mathbf{I}-\\eta\\mathbf{H}^{\\text{tm}}(\\mathbf{\\theta}_{t+1})\\right],\\end{split} \\tag{22}\\]where \\(\\mathbf{G}^{\\text{trn}}(\\mathbf{\\theta}_{t})=[\\nabla l(x_{1}^{\\text{trn}},\\mathbf{\\theta}_{t }),\\cdots,\\nabla l(x_{N}^{\\text{trn}},\\mathbf{\\theta}_{t})]\\), \\(\\mathbf{I}\\) is the identity matrix, and \\(\\mathbf{H}^{\\text{trn}}(\\mathbf{\\theta}_{t+1})\\) is the Hessain matrix of \\(L^{\\text{lm}}(\\mathbf{\\theta})\\) at \\(\\mathbf{\\theta}=\\mathbf{\\theta}_{t+1}\\). We implement the BP operations with dynamic programming and Jacobian-Vector-Product7 in PyTorch [19] for efficiency. To reduce the single-device GPU memory use, we also implement an activation partition algorithm inspired by ZeRO-2 [17], where the "hidden states" \\(\\mathbf{\\theta}_{t}\\) in one model are stored in different GPU devices.\n' +
      '\n' +
      'Footnote 7: [https://pytorch.org/docs/stable/func.api.html](https://pytorch.org/docs/stable/func.api.html)\n' +
      '\n' +
      '## Appendix D Hyper-Parameter Configurations\n' +
      '\n' +
      'Perceptron Linear Classification.Following the teacher-setting described in Section 4.2, we use \\(D=128\\) and \\(\\mathbf{T}\\) is randomly drawn from a Gaussian distribution \\(\\mathbf{T}\\sim\\mathcal{N}(\\mathbf{0},\\sqrt{D}\\mathbf{I})\\). We generate \\(N=4096\\) training inputs \\(\\mathbf{z}^{\\text{tm}}\\) from \\(\\mathcal{N}(\\mathbf{0},3\\mathbf{I})\\), and \\(M=512\\) target inputs \\(\\mathbf{z}^{\\text{dsr}}\\) from \\(\\mathcal{N}(0.51,\\mathbf{I})\\), where \\(\\mathbf{1}=[1,1,\\cdots,1]^{\\top}\\in\\mathbb{R}^{D}\\). For each learning policy, we initialize \\(\\gamma_{n,0}=\\frac{1}{N}\\) and train the Perceptron with \\(\\eta=0.1\\) for \\(T=2000\\) time steps, which is sufficient for the model to converge. For learning policy optimization, we initialize the learning policy to the constant policy \\(\\gamma_{n,t}^{c}=\\frac{1}{N}\\), setting \\(\\epsilon=5\\times 10^{-6}\\) and train the network for 500 epochs.\n' +
      '\n' +
      'Transformer Language Modeling.We conduct experiments based on a two-layer Transformer with 128 hidden dimensions and 8 attention heads. For all experiments except that in Table 2, we randomly sample \\(N\\) = 16,384 examples as \\(x_{n}^{\\text{tm}}\\) and \\(K\\) = 512 examples as \\(x_{k}^{\\text{dsr}}\\) with the max sequence length 64 from the TinyStories [18] corpus8. We use the BPE tokenizer of Mistral [19] and construct a vocabulary with 5K tokens by mapping the infrequent tokens to [UNK]. The model contains about 1.7M parameters. To reflect the difference between the training and desired distribution, we add perturbations to 50% training sentences by iteratively applying one of the following operations 20 times: (1) replacing one token with a random token in the vocabulary [18]; (2) deleting the last token [17]; (3) repeating one token in a sentence [20]. This corresponds to the fact that the large-scale pre-training corpus tends to be more noisy than the desired set (the carefully curated held-out corpus or high-quality downstream data) to evaluate the model generalization performance in practice. We set \\(\\eta=0.1\\), \\(T=4,000\\), \\(\\gamma_{n,0}=\\frac{1}{N}\\) for each learning policy. We start from the constant policy and optimize the learning policy for 15 epochs using \\(\\eta=0.1,0.2,0.4\\). \\(\\eta=0.4\\) yields the lowest loss at the end of the training. Therefore, we only plot the optimization process for \\(\\eta=0.4\\) in Figure 4(b) and 5(b). For experiments in Table 2, we vary the total training steps and the corresponding training data sizes and simultaneously, change the vocabulary sizes to adapt to different data sizes. We use vocabulary sizes 4K, 4.5K, 5K, and 6K for \\(N=2^{12},2^{13},2^{14}\\), and \\(2^{15}\\), respectively.\n' +
      '\n' +
      'Footnote 8: [https://huggingface.co/datasets/roneneldan/TinyStories/tree/main](https://huggingface.co/datasets/roneneldan/TinyStories/tree/main)\n' +
      '\n' +
      'Figure 12: The architecture of the equivalent neural network to find the optimal learning policy, Each layer consists of the gradient update and a residual connection.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>