<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '언어 모델의 번창(LMs; HZD\\({}^{+}\\)21, BHA\\({}^{+}\\)21)으로 인해 학습 속도를 가속화하고 가능한 한 적은 훈련 단계로 특정 모델 성능을 달성하는 것을 목표로 하는 LMs의 **학습**[12, 13] 개선에 대한 관심이 증가하고 있다[14]. 이 초점은 인간이 계산 요구의 급속한 성장을 감안할 때 LM의 한계를 탐구하는 데 도움이 되며, 대규모 언어 모델(LLM; BMR\\({}^{+}\\)20, Ope22, Ope23, CND\\({}^{+}\\)23, ADF\\({}^{+}\\)23)의 민주화를 촉진한다.\n' +
      '\n' +
      '본 논문에서는 LMs의 최적 학습을 위한 이론을 제시한다. 모델 수준 [20, 18], 최적화 수준 [20, 17], 또는 데이터 수준 [21, 22, 23, 24]에서 실용적인 가속 방법을 탐구하는 이전 작업과 달리, 본 연구는 최적화 목표, 최적 학습 동역학의 속성 및 학습 가속도의 본질적인 개선을 포함하여 LM 학습 속도를 최적화하는 원리를 보여준다.\n' +
      '\n' +
      '구체적으로, 최적화 목적을 위해, 우리는 다음-토큰-예측 LM 트레이닝 프로세스를 트레이닝 데이터의 무손실 압축으로 볼 때 분명한 물리적 유의성을 갖는 손실 곡선 아래의 영역(AUC; [14])을 최소화하는 것을 제안한다[1, 13]. 그림 1과 같이 손실 AUC가 가장 작은 학습 과정이 가장 높은 압축비에 해당한다. 동시에, 이 프로세스에서의 손실은 또한 충분히 큰 총 트레이닝 단계들이 주어졌을 때, 최고 레이트에서 작은 값으로 수렴한다. 따라서 우리는 학습 과정**의 해당 압축 비율을 최대화하는 것과 동등한 **최적화 LM 학습을 고려하고, 후자를 우리의 이론에서 최적화 목표로 채택한다. 또한 최근 LLM[21, 23]의 현저한 일반화 성능을 해석하기 위해 유사한 목표가 사용된다.\n' +
      '\n' +
      '그런 다음 우리는 목표의 최적을 달성하는 LM 학습 과정에서 동역학의 특성을 특징짓는 _Learning Law_라는 정리를 도출한다. 여기서, 학습이 진행됨에 따라 LM이 어떤 데이터를 포인팅하는지 결정하는 _learning policy_에 의해 학습 과정이 유도된다. 이와 같이 해당 압축률이 최대화된다는 의미에서 최적의 학습 정책을 해결하고, 우리의 학습 법칙을 얻는다(형식적인 표현은 정리 3.1 참조):\n' +
      '\n' +
      '그림 3에서 볼 수 있듯이 예제의 기여도는 원하는 학습에서 LM에 대한 영향을 측정하는 원하는 손실2의 기울기와 기울기의 닷 곱으로 정의된다.\n' +
      '\n' +
      '도 3: **A**: 학습법칙(정리 3.1)의 3-D 예시도. 최적 학습 과정에서 모든 학습 예제는 LM 학습에 동일한 기여를 해야 하며, 여기서 기여도는 개별 샘플에 대한 기울기의 점 곱(\\(\\nabla l_{m}\\), \\(\\nabla l_{n}\\), \\(\\nabla l_{k}\\))과 원하는 손실의 기울기(\\(\\nabla L\\))로 정의된다. 엄격한 표기 정의는 섹션 3.2를 참조하십시오. 학습 법칙의 실험적 증거입니다 LM 학습이 최적에 접근할 때, 예제 기여도의 유사성은 \\(+\\infty\\) 경향이 있으며, 이는 모든 예제가 LM에 동일한 기여도를 갖는다는 것을 의미한다.\n' +
      '\n' +
      '방향. 학습 법칙은 또한 최적 학습 과정에서 로컬 및 글로벌 학습 속도의_매칭을 제안하며, 이는 최적 학습 정책을 LM이 기여도가 높은 예를 학습하도록 장려하고 동시에 과적합을 방지하는 동적 데이터 재가중 전략으로 해석한다. 유사한 메커니즘이 심리학 연구에서 인간에 대한 최상의 교수법에도 중요한 것으로 밝혀졌다[14, 15].\n' +
      '\n' +
      '퍼셉트론3[13] 기반의 선형 분류 과제와 트랜스포머[20] 기반의 실제 언어 모델링 과제에 대한 실험을 통해 우리의 이론을 살펴본다. 먼저, 목적하에서 최적의 학습 정책을 탐색하기 위한 기울기 기반 방법을 설계한다. 그런 다음, 우리는 발견된 거의 최적인 정책에 의해 유도된 학습 과정의 역학이 우리의 학습 법칙과 잘 일치하는지 확인한다. 마지막으로, 표 1과 같이, LMs[11]의 학습 단계 스케일링 법칙에서 거의 최적인 학습 정책이 계수를 본질적으로 개선한다는 경험적 증거를 제공하며, 이는 각각 퍼셉트론 및 트랜스포머 학습에 대해 5.50\\(\\times\\) 및 2.41\\(\\times\\)의 속도 향상으로 이어진다. 이는 실제 학습 정책을 최적화하고 LLM의 교육을 가속화하기 위해 보다 확장 가능한 방법을 탐색하는 약속과 중요성을 강조한다.\n' +
      '\n' +
      '각주 3: 부록 A.3에서 우리는 퍼셉트론 훈련의 무손실 데이터 압축 뷰를 제공하며, 이는 우리의 이론이 또한 적용됨을 나타낸다.\n' +
      '\n' +
      '##2 문제 정식화\n' +
      '\n' +
      '우리는 충분히 큰 총 학습 시간 단계(T\\)에 대해 \\(N\\)의 예제\\(\\{x_{n}^{\\text{tm}}\\}_{n=1}^{N}\\)을 가진 대규모 데이터 세트에서 LM 학습을 고려한다. \\(\\mathbf{\\gamma}_{n,t}\\)는 시간 단계 \\(t\\)에서 \\(n^{\\text{th}}\\) 훈련 예제의 가중치를 나타내며, **학습 정책**은 \\(n\\) 훈련 예제 \\(\\mathbf{\\gamma}_{t}=[\\gamma_{1,t},\\gamma_{2,t},\\cdots,\\gamma_{n,t}]^{\\top}\\)에 대해 \\(\\sum_{n=1}^{N}\\gamma_{n,t}=1\\) 및 \\(\\gamma_{n,t}\\geq 0\\)에 대한 시변 분포로 표현된다. 전통적으로 훈련된 LM은 정책\\(\\gamma_{n,t}^{c}=\\frac{1}{N}\\)으로 학습한다(**conventional learning**). 최근 연구들[13, 1]은 Gradient Decent(GD)에 기초하여 도출된 이론들이 다른 Gradient 기반 알고리즘들에 대한 통찰력을 제공한다는 것을 보여주었다[10]. 따라서 단순화를 위해 LM이 \\(t=0,1,\\cdots,T-1\\)에 대해 GD로 훈련된다고 가정한다:\n' +
      '\n' +
      '{split} L_{t}^{\\text{tn}(\\mathbf{\\theta}_{t}& =\\sum_{n=1}^gamma_{n,t}l(x_{n}^{\\text{tn},\\mathbf{\\theta}_{t}), \\\\mathbf{\\theta}_{t+1}&=\\mathbf{\\theta}_{t}-\\eta\\nabla L_{t}{text{tn}(\\mathbf{\\theta}_{t}),\\end{split}\\tag{1}\\mathbf{\\theta}_{t}}(\\mathbf{\\theta}_{t}})\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\theta}_{t}\\in\\mathbbb{R}^{D}\\)는 시간 단계 \\(t\\)에서 \\(D\\)차원 벡터로 평탄화된 모델 파라미터이며, \\(\\eta\\)은 학습률이고 \\(l(\\cdot,\\cdot)\\)은 학습문제의 손실함수이다. LMs의 경우, \\(l(\\cdot,\\cdot)\\)은 일반적으로 최대 우도 추정(MLE) 손실: \\(l(x,\\mathbf{\\theta}_{t})=-\\log p_{\\mathbf{\\theta}_{t}(x)\\)이며, 여기서 \\(x\\)은 텍스트 시퀀스이다. [17]과 [15]에 이어서, 우리는 반드시 학습 예제와 같은 분포를 따르지 않는 \\(K\\) 예제들 \\(\\{x_{k}^{\\text{dsr}}\\}_{k=1}^{K}\\)에서 계산된 원하는 손실 \\(L^{\\text{dsr}}\\)의 감소율에 의해 반영된 학습 속도에 초점을 맞춘다:\n' +
      '\n' +
      '\\[L^{\\text{dsr}(\\mathbf{\\theta}_{t})=\\frac{1}{K}\\sum_{k=1}^{K}l(x_{k}^{\\text{dsr}, \\mathbf{\\theta}_{t}). \\tag{2}\\\n' +
      '\n' +
      '이 공식은 과적합을 방지하기 위해 검증 세트를 사용하는 고전적 기계 학습[20], 일반화 성능을 평가하기 위해 신중하게 선별된 고정 말뭉치에 의존하는 대규모 사전 훈련[11], 훈련과 목표 분포 사이에 자연스러운 차이가 존재하는 도메인 적응[17]을 포함한 광범위한 실제 시나리오에 적용된다. 이와 같이 LM 학습을 최적화하기 위해 \\(L^{\\text{dsr}(\\mathbf{\\theta}_{t})\\의 감소율을 최대화하는 학습정책 \\(\\mathbf{\\gamma}_{t}\\)을 탐색한다.\n' +
      '\n' +
      '그러나, 이러한 최적화 문제에 대한 직접적인 분석은 GD의 불명확성으로 인해 어렵다. 따라서, 우리는 이론적인 분석에 더 적합한 \\(t\\in[0,T]\\)에 대해 수학식 1의 대응하는 기울기 흐름을 고려하여 GD의 _continuous limit_에 초점을 맞춘다:\n' +
      '\n' +
      '\\\\nabla L^{\\theta}(t)=-\\nabla\\sum_{n=1}^{N}\\gamma_{n}(t)l(x_{n}^{\\text{tm},\\mathbf{\\theta}(t)), \\tag{3}\\t.\n' +
      '\n' +
      '여기서 \\(\\gamma_{n}(t)\\)는 \\(\\gamma_{n,t}\\)의 부드러운 보간 함수이다. 수치해석 결과에 따르면, 식 1에 정의된 GD는 식 3의 기울기 흐름의 초기값 문제를 근사적으로 해결하기 위한 _Euler method_이고, \\(\\eta\\)이 충분히 작을 때 \\(\\mathbf{\\theta}(t)\\approx\\mathbf{\\theta}_{t}\\)이다. 섹션 4에서 이 한계에서 파생된 결과가 이산 설정의 실험과 잘 일치함을 보여준다.\n' +
      '\n' +
      'LMs의 최적 학습을 위한 이론\n' +
      '\n' +
      '이 절에서는 GD의 연속 한계에서 우리의 이론을 제시한다. 먼저 학습정책을 최적화하여 \\(L^{\\text{dst}}\\)의 감소율을 최대화하기 위한 목표를 제안한다. 그리고 목표의 최적을 달성하는 정책에 의해 유도되는 학습 과정의 역학에 필요한 조건을 소개하는 주요 정리인 _Learning Law_를 도출한다.\n' +
      '\n' +
      '### 목표: 압축률 최대화\n' +
      '\n' +
      '본 논문에서는 \\(L^{\\text{dst}}(\\mathbf{\\theta}(t))\\) (AUC of \\(L^{\\text{dst}}\\))의 곡선 아래 면적을 갖는 \\(L^{\\text{dst}}\\)의 감소율을 특성화하고, 이 면적을 최소화하여 높은 학습 속도를 달성한다:\n' +
      '\n' +
      '{split}\\min_{\\mathbf{\\gamma}(t)}&\\int_{0}^{T}L^{text{dst}(\\mathbf{\\theta}_{\\mathbf{\\gamma}(t))\\mathrm{d}t,\\\\text{s.t.&\\sum_{n=1}^{N}\\gamma_{n}(t)=1,\\\\&\\gamma_{n}(t)\\geq 0,n=1,2,\\cdots,N,\\end{split}\\tag{4}\\mathrm{d}t,\\text{s.t.&\\sum_{n=1}^{N}\\gamma_{n}(t)=1,\\\\&\\gamma_{n}(t)\\geq 0,n=1,2,\\cdots,N,\\end{split}\\tag{4}\\mathrm{d}t,\\text{s.t.&\\sum_{n=1}^\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\gamma}(t)=\\left[\\gamma_{1}(t),\\gamma_{2}(t),\\cdots,\\gamma_{n}(t)\\right]^{\\top}\\) 및 \\(\\mathbf{\\theta}_{\\mathbf{\\gamma}(t)\\)는 \\(\\mathbf{\\gamma}(t)\\)에 대한 의존성을 강조하기 위해 식 3을 만족하는 \\(\\mathbf{\\theta}(t)\\)의 별칭이다. 그림 1과 같이 충분히 큰 \\(T\\)에 대해서는 손실 AUC가 최소인 학습 과정이 가장 높은 손실 감소율을 가지고 있다. 흥미롭게도, \\(L^{\\text{dst}}\\)의 AUC는 "LM-training-as-lossless-compression" 뷰 [14]에서 물리적 의미가 있다: **원하는 데이터 분포**에서 가져온 압축 데이터의 결과 설명 길이. 따라서, 수학식 4는 해당 압축비를 최대화하는 것과 같다. 잘 훈련된 LM을 사용하여 데이터를 인코딩하는 연구를 하는 [13]과 달리, 우리는 전체 LM 훈련을 압축 과정으로 간주한다. 이 두 가지 관점에 대해 섹션 5에서 더 많은 논의를 제공한다. 또한, 우리의 진술과 훈련 과정을 무손실 압축으로 보는 이전 작업에서는 여전히 약간의 차이가 있다[13, 12, 14]: 다중 에포크에 대한 GD 훈련의 원하는 손실 AUC를 고려하는 반면, 이전 진술은 단일 에포크 SGD 훈련에 대한 훈련 손실 AUC에 관한 것이다. 이러한 차이에 대한 더 많은 논의는 부록 A.2에서 찾을 수 있다.\n' +
      '\n' +
      '### Learning Law\n' +
      '\n' +
      '수학식 4는 _Maximum Principle_[15]에 의해 해결될 수 있는 최적 제어 문제를 정의한다. 그러나 실제 LM 학습에서는 해석과 검증이 어려운 해법을 발견한다. 따라서 본 연구에서는 식 4의 최적을 위한 더 느슨한 필요조건을 도출한다.\n' +
      '\n' +
      '**정리 3.1** (학습법칙):_LM이 최적의 학습정책으로 학습될 때, 원하는 데이터 분포에서 최대 압축비에 해당하는 학습과정을 얻을 수 있도록 \\(0<t\\leq T\\) 및 임의의 \\(m\\), \\(n\\)에 대해 다음과 같은 조건이 성립한다. \\(\\gamma_{m}(t)>0\\), \\(\\gamma_{n}(t)>0\\):_\\(t)>0\\).\n' +
      '\n' +
      '\\[\\nabla L\\cdot\\nabla l_{m}=\\nabla L\\cdot\\nabla l_{n}=\\mathrm{Const}, \\tag{5}\\}\n' +
      '\n' +
      '\\(\\nabla L=\\nabla L^{\\text{dst}(\\mathbf{\\theta}(t))=\\nabla\\frac{1}\\sum_{k}1}^{K}l(x_{k}^{\\text{dst},\\mathbf{\\theta}(t))\\,\\(\\nabla l_{m}=\\nabla l(x_{m}^{\\text{trn},\\mathbf{\\theta}(t))\\,\\(\\nabla l_{n}=\\nabla l(x_{n}^{\\text{trn},\\mathbf{\\theta}(t))\\,\\(\\mathbf{\\theta}(t))\\,\\(\\nabla l_{n}=\\nabla l(x_{n}^{\\text{trn},\\mathbf{\\theta}(t))\\,\\(\\mathbf{\\theta}(t))\\,\\(\\nabla l_{n} (\\mathrm{Const}=-\\frac{\\mathrm{d}{\\mathrm{d}t}L^{\\text{dst}(\\mathbf{\\theta}(t))\\)는 시간에 따른 원하는 손실 변화율이고 **는 \\(\\mathbf{n}\\) 및 \\(\\mathbf{m}\\)**._\n' +
      '\n' +
      '정리 3.1을 증명하기 위해 Euler-Lagrange (EL) 식 [16]과 Karush-Kuhn-Tucker (KKT) 조건 [13]을 식 4에 적용하여 다음과 같은 조건을 얻었다. \\(\\nabla L^{\\text{dst}(\\mathbf{\\theta}(t))\\cdot\\nabla l(x_{n}^{\\text{tm},\\mathbf{\\theta}(t))=-\\frac{\\mathrm{d}t}L^{\\text{dst}(\\mathbf{\\theta}(t)) 전체 증명은 부록 B에 나와 있다.\n' +
      '\n' +
      '수학식 5의 (\\nabla L\\cdot\\nabla l_{n}\\)은 학습예 \\(x_{n}^{\\text{tm}\\)에서 \\(L^{\\text{dst}(\\mathbf{\\theta}(t))\\)까지의 기여도를 나타내며, 이는 \\(x_{n}^{\\text{tm}\\)에서의 기울기가 \\(L^{\\text{dst}(\\mathbf{\\theta}(t))\\의 기울기와 같은 방향을 공유할 때 최대가 된다. 나머지 논문의 편의상 \\(\\mathbf{CT}_{n}(\\mathbf{t})=\\mathbf{\\nabla L}\\cdot\\mathbf{\\nabla l}_{n}=\\mathbf{\\nabla L}^{\\text{dst}(\\mathbf{\\theta}(t))\\cdot\\mathbf{\\nabla l}(x_{n}^{\\text{tm},\\mathbf{\\theta}(t))을 나타낸다. 모델이 수렴될 때 (\\(\\nabla L^{\\text{tm}}(\\mathbf{\\theta}(t),t)\\approx\\mathbf{0}\\)), \\(\\mathrm{CT}_{n}(t)\\)은 \\(\\mathbf{\\theta}(t)\\(\\mathbf{\\theta}(t)\\)에서 \\(L^{\\text{tm}(\\mathbf{\\theta}(t))의 헤시안 행렬을 아이덴티티 행렬[11]로 설정함으로써 영향 함수[11]의 근사치로 볼 수 있다는 점에 유의한다. 본질적으로 수학식 5는 \\(\\mathrm{CT}_{n}(t)\\)와 무관한 값을 의미한다. 제로-가중치 예제들(\\(\\gamma_{n}(t)=0\\))은 전형적으로 노이즈(섹션 4.5에서 검증됨)이기 때문에, 정리 3.1은 최적의 학습 프로세스**에서 **모든 비-노이즈 예제가 LM에 동일하게 기여해야 함을 시사한다. 다음에서 우리는 이 정리에 대한 더 많은 논의를 제공한다.\n' +
      '\n' +
      '### Discussion\n' +
      '\n' +
      '정리 3.1은 지역학습과 전역학습의 정합을 제안하며, 또 다른 해석으로는 "지역학습속도"로 LM이 얼마나 빨리 지식을 학습하는지(x_{n}^{\\text{tm}\\)\\(L^{\\text{dsf}\\)를 줄이는 데 도움이 된다. 이는 \\(\\mathrm{CT}_{n}(t)\\)에서의 도트 곱 연산이 원하는 방향에 대한 개별 손실 하강 속도 \\(\\nabla l(x_{n}^{\\text{tm},\\mathbf{\\theta}(t))\\의 투영으로 볼 수 있기 때문이다. 이에 대응하여, \\(\\frac{\\mathrm{d}{\\mathrm{d}t}L^{\\text{dsf}(\\mathbf{\\theta}(t))\\)는 모든 개인 \\(x_{n}^{\\text{tm}\\)을 학습함으로써 LM이 얼마나 빠르게 개선되는지를 나타내는 LM의 "글로벌 학습 속도"이다. 그 결과, 정리 3.1에서 \\(\\mathrm{CT}_{n}(t)=\\mathrm{Const}=-\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{dsf}(\\mathbf{\\theta}(t))\\)는 지역 학습 속도가 최적의 학습 과정에서 전역 학습 속도와 일치해야 함을 나타낸다.\n' +
      '\n' +
      '최적 학습 정책은 동적 데이터 재가중화 전략을 수립하는데, 일반적으로 LM 학습이 진행됨에 따라 각 예제의 기울기 규범(||\\nabla l(x_{n}^{\\text{tm},\\mathbf{\\theta}(t))||\\)이 LM 적합에 따라 감소하기 때문에 \\(\\mathrm{CT}_{n}(t)\\)이 감소한다. 또한, \\(x_{n}^{\\text{tm}},\\mathbf{\\theta}(t))\\)의 방향은 \\(\\nabla L^{\\text{dsf}(\\mathbf{\\theta}(t))\\에서 발산하는데, 이는 \\(x_{n}^{\\text{tm}\\)과 \\(x_{k}^{\\text{dsf}\\)의 분포 사이의 불일치로 인해 \\(\\mathrm{CT}_{n}(t)\\)의 감소에 기여한다. 따라서, 정리 3.1은 높은 \\(\\mathrm{CT}_{n}(t)\\)을 갖는 _highly contributionutive 예제\\(x_{n}^{\\text{tm}\\)이 다른 예제의 값에 부합하도록 \\(\\mathrm{CT}_{n}(t)\\)을 줄이기 위해 훈련에 큰 가중치를 얻는 것을 보장한다. 한편, 정리 3.1은 전역 학습 속도에 맞추기 위해 \\(\\mathrm{CT}_{n}(t)\\)이 너무 작지 않아야 하기 때문에 LM 오버핏 이전에 \\(x_{n}^{\\text{tm}}\\)의 가중치가 낮아지는 것을 보장한다. 이는 총체적으로 LM의 학습 속도를 최대화하는 최적의 학습 정책에 직관적으로 필수적인 동적 학습 데이터 재가중 전략을 형성한다.\n' +
      '\n' +
      '정리 3.1은 최적 학습 동역학을 위한 필수 조건이다. E-L 방정식과 KKT 조건은 최적화 문제가 비볼록일 때 전역 최적을 위한 필수 조건이기 때문이다. 따라서 정리 3.1을 만족하는 학습 과정이 최적으로 보장되지 않는다. 예를 들어, \\(\\gamma_{1}(t)=1\\) 및 \\(\\gamma_{2}(t)=\\gamma_{3}(t)=\\cdots=\\gamma_{N}(t)=0\\)으로 설정함으로써, \\(\\mathrm{CT}_{n}(t)=0\\)의 값에 관계없이 수학식 5를 만족한다. 이 학습 정책은 미니 배치 크기 = 1인 SGD를 사용하는 것에 해당하며, 이는 최적일 것 같지 않다[14]. 따라서 정리 3.1에 따라 최적의 정책을 찾는 것은 실제적으로 정규화 용어가 필요할 수 있으며, 이는 향후 탐색할 작업을 위해 남겨둔다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 식 1의 이산 설정에서 실험을 수행하는데, 여기서 섹션 3의 연속 한계로부터 도출된 결론은 \\(\\eta\\)이 충분히 작은 [1]일 때 여전히 적용 가능하다. 먼저 이산 설정에서 원하는 분포로부터 추출한 데이터의 압축률을 최대화하는 \\(L^{\\text{dsf}(\\mathbf{\\theta}_{t})\\(0\\leq t\\leq T-1\\)의 AUC를 명시적으로 최소화함으로써 \\(\\mathbf{\\gamma}_{t}\\in\\mathbbb{R}^{N}\\)에 대한 최적의 학습 정책을 찾는 방법을 설계한다. 그런 다음 발견된 정책에 의해 유도된 학습 과정에 대한 학습 법칙(정리 3.1)을 살펴본다. 마지막으로, 압축률을 최대화하는 것이 스케일링 법칙 계수[14]를 본질적으로 향상시킨다는 것을 실증적으로 검증함으로써 우리 이론의 실질적인 의의와 가능성을 나타낸다.\n' +
      '\n' +
      '### 최적 학습 정책 찾기\n' +
      '\n' +
      '최적 \\(\\mathbf{\\gamma}_{t}\\)을 찾기 위해, 우리는 식 4에 정의된 최적화 문제의 이산 버전을 프록시멀 구배 방법[1]으로 직접 해결한다:\n' +
      '\n' +
      '\\mathbf{\\gamma}J(\\mathbf{\\gamma})&=\\sum_{t=1}^{T}L^{dsf}(\\mathbf{\\theta}_{t}),\\\\mathbf{\\gamma}_{t}&\\leftarrow\\mathrm{Proj}\\left[\\mathbf{\\gamma}_{t}-\\epsilon\\nabla_{\\mathbf{\\gamma}_{t}J(\\mathbf{\\gamma}\\right],\\;0\\leq t\\leq T-1,\\end{split}\\tag{6}\\text{t}\\mathbf\\gamma}\\text{t}\\mathbf\\gamma}\\text{t}\\text{df}\\text{df}\\text{df}\\text{df}\\text{df}\\mathbf\\gamma}\\text{t}\\text{df}\\text{df}\\text{df}\n' +
      '\n' +
      '여기서, \\(J(\\mathbf{\\gamma})\\)는 수학식 4에서 적분의 이산 근사치이고, \\(\\epsilon\\)은 학습률이고 \\(\\mathrm{Proj}[.]\\)는 \\(\\mathbb{R}^{N}\\)의 점을 \\(N\\)-simplex에 투영하여 \\(\\mathbf{\\gamma}_{t}\\)이 \\(N\\) 훈련 예제에 대한 확률 분포임을 보장한다. 최적화 과정은 부록 C에 자세히 설명된 PyTorch [13]에서 동적 프로그래밍과 Jacobian-Vector-Product을 사용하여 효율적으로 구현될 수 있다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '퍼셉트론(Perceptron, 14) 기반의 선형 분류 태스크와 트랜스포머(Transformer, 13) 기반의 언어 모델링 태스크에 대한 실험을 수행한다. 하이퍼 매개변수 구성은 부록 D를 참조하십시오.\n' +
      '\n' +
      '퍼셉트론 선형 분류.우리는 교사-학생 설정 [1]을 채택한다. 여기서 각 예 \\(x_{n}=(\\mathbf{z}_{n},y_{n})\\)은 가우스 분포로부터 i.i.d로 그려진 \\(D\\)-차원 벡터 \\(\\mathbf{z}_{n}\\in\\mathbbb{R}^{D}\\)와 스칼라 \\(y_{n}=\\operatorname{sign}(\\mathbf{T}\\cdot\\mathbbb{z}_{n})\\)의 지면진리 가중치 \\(\\mathbf{T}\\in\\mathbbb{R}^{D}\\)이 주어졌을 때 \\(D\\)차원 벡터 \\(\\mathbf{z}_{n}\\in\\mathbbbb{R}^{D}\\)의 쌍이다. 우리는 그들의 차이를 반영하기 위해 훈련과 원하는 데이터 분포 사이의 변화를 소개한다. 데이터는 \\(\\boldsymbol{\\theta}\\in\\mathbb{R}^{D}\\): \\(o_{n}=\\sigma(\\boldsymbol{\\theta}\\cdot\\mathbff{z}_{n})=\\frac{1}{1+\\exp(- \\boldsymbol{\\theta}\\cdot\\mathbff{z}_{n}}\\)에 의해 매개되는 한 층 지각 매개변수에 의해 학습되며, 최대 우도 추정(MLE) 손실 \\(l(x_{n},\\boldsymbol{\\theta})=-\\log{o_{n}^{y_{n}(1-o_{n})}}으로 학습된다. 부록 A.3에서 퍼셉트론을 1단계 LM으로 볼 수 있음을 보여주는데, 이는 우리의 이론이 여전히 적용된다는 것을 의미한다.\n' +
      '\n' +
      '트랜스포머 언어 모델링.최적 정책 탐색의 계산 비용을 고려하여 약 1.7M 파라미터를 갖는 2계층 트랜스포머를 채택하여 고품질 사전 훈련 코퍼스인 TinyStories[1]에서 훈련한다. 우리는 훈련 예제에 섭동을 추가한다(자세한 내용은 부록 D 참조). 이는 실습에서 사전 훈련 말뭉치의 상대적으로 낮은 품질을 모방한다. 우리의 이론적 도출은 일반적으로 적용 가능하기 때문에, 우리는 우리의 이론이 더 큰 LMs에도 적용된다고 믿는다.\n' +
      '\n' +
      '4.1절에서 \\(L^{\\text{sfr}}(\\boldsymbol{\\theta}_{t})\\)을 계산하는 데 사용되는 \\(K\\) 예제를 과적합할 위험을 이동하기 위해 퍼셉트론 선형 분류 및 트랜스포머 언어 모델링 실험 모두에서 원하는 데이터 분포로부터 \\(K\\) 예제를 갖는 유지 테스트 세트를 추가로 구성한다. 하기에서,\n' +
      '\n' +
      '도 4: 학습 정책 최적화 결과 퍼셉트론 선형 분류 **(a)** 및 트랜스포머 언어 모델링 작업 **(b).** 원하는 퍼셉트론 또는 트랜스포머 손실의 곡선 아래 면적(AUC)을 나타내는 수학식 6에 정의된 학습 정책 최적화 손실 \\(J(\\gamma)\\)(실선)을 플로팅한다. 또한 "LM-as-Lossless-Compression" 뷰에서 훈련 과정(점선)의 해당 압축률을 보여준다. 최적화는 기존 학습에서 시작하여 손실 AUC가 낮고 이해율이 높은 거의 최적인 학습으로 원활하게 수렴한다.\n' +
      '\n' +
      '그림 5: 기존과 거의 최적인 학습 정책을 사용하여 모델을 학습시켰을 때 원하는 손실(L^{\\text{sfr}}(\\boldsymbol{\\theta}_{t}))의 곡선. 근최적 학습 과정은 Perceptron 선형 분류 **(a)**에서는 \\(5.50\\times\\)의 속도 향상을, Transformer 언어 모델링 **(b)**에서는 \\(2.41\\times\\)의 속도 향상을 달성한다.\n' +
      '\n' +
      '우리는 방정식 2**의 \\(\\mathbf{x}_{k}^{\\text{sfx}}\\)으로 정책 최적화 과정에서 보이지 않는 테스트 예를 처리하여 평가 메트릭을 계산하고 보고한다.\n' +
      '\n' +
      '#### : 학습정책 최적화 결과\n' +
      '\n' +
      '4.1절의 방법으로 거의 최적에 가까운 학습 정책을 찾을 수 있다. 그림 4에서는 최적 학습 정책을 찾는 최적화 과정을 보여준다. 또한, 학습과정에서 \\(\\mathbf{\\theta}_{t}\\)에 의해 유도되는 학습과정에서의 \\(L^{\\text{dst}(\\mathbf{\\theta}_{t})의 AUC인 \\(J(\\mathbf{\\gamma})\\)와 그에 따른 압축률 \\(\\mathrm{CR}=\\frac{T\\log|V|}{\\sum_{t=1}^{T}L^{\\text{dst}(\\mathbf{\\theta}_{t})}\\)의 학습정책 최적화 손실(J(\\mathbf{\\gamma})\\)을 구하였으며, 여기서 \\(V\\)은 퍼셉트론/트랜스포머의 레이블/어휘 공간의 크기이다(부록 A.1 참조). (J(\\mathbf{\\gamma})\\)의 곡선은 매끄럽고 마지막에 거의 수렴하여 거의 최적인 학습 정책이 발견됨을 나타낸다.\n' +
      '\n' +
      '근최적 학습 정책은 학습 속도의 높은 가속 비율을 산출한다. 도 5에서는 기존의 학습 정책과 근최적 학습 정책에 따라 Perceptron과 Transformer를 학습시켰을 때 \\(L^{\\text{dst}}(\\mathbf{\\theta}_{t})\\의 곡선을 그린다. 최적화에 가까운 정책은 Perceptron 훈련과 Transformer 훈련이 끝날 때 각각 가속(5.50\\times\\)과 가속(2.41\\times\\)을 가져와 손실 AUC를 크게 개선한다. 보고된 모든 메트릭은 정책 최적화 동안 보이지 않는 테스트 세트에서 계산되며, 이는 거의 최적인 정책이 계산하는 데 사용되는 특정 예제(\\(L^{\\text{dst}}(\\mathbf{\\theta}_{t}))에 과도하게 적합되지 않지만 모델이 원하는 분포에서 더 빨리 학습하는 데 도움이 됨을 시사한다.\n' +
      '\n' +
      '### 학습법칙의 직접 검증(정리 3.1)\n' +
      '\n' +
      '본 논문에서는 특정 학습 정책에서 개별 표본 기여도\\(\\mathrm{CT}_{n}(t)\\)의 이산 버전인 \\(\\mathrm{CT}_{n,t}\\)과 \\(\\mathrm{CT}_{n,t}=\\mathrm{CT}_{n}(t)\\(t=1,2,\\cdots,T\\)에 대한 \\(\\mathrm{CT}_{n,t}=\\mathrm{CT}_{n}(t)\\)의 유사성을 살펴본다. 유사도(\\(\\mathrm{SIM}\\))는 \\(\\mathrm{CT}_{n,t}\\)의 _Signal-Noise-Ratio_에 의해 측정된다:\n' +
      '\n' +
      '\\[\\mathrm{SIM}_{t}=\\frac{\\overline{\\mathrm{CT}}_{t}}{s_{\\mathrm{CT},t}}, \\tag{7}\\]\n' +
      '\n' +
      '\\(\\overline{\\mathrm{CT}}_{t}=\\sum_{n}\\gamma_{n,t}\\mathrm{CT}_{n,t}\\mathrm{CT}_{n,t}\\mathrm{CT}_{n,t}\\mathrm{CT}=\\sum_{n,t}\\mathrm{CT}_{n,t}\\mathrm{CT}=\\sum_{n,t}\\mathrm{CT}=1}^{N}1\\left[\\gamma_{n,t}\\neq0\\right]\\left(\\mathrm{CT}_{n,t}\\neq0\\right]\\mathrm{CT}=1}^{N}1\\left[\\gamma_{n,t}\\neq0\\right]\\mathrm{CT}=1}^{N}1\\left[\\gamma_{n,t}\\neq0\\right]\\xqrt{\\frac{\\sum_{n=1}^{N}1 높은 \\(\\mathrm{SIM}_{t}\\)은 훈련 예제들이 더 유사한 \\(\\mathrm{CT}_{n,t}\\)을 가짐을 의미한다. \\(\\mathrm{SIM}_{t}\\)은 무차원이므로 학습 중 \\(\\mathrm{CT}_{n,t}\\)의 절대값 스케일 변화의 영향을 피한다. 또한 학습 과정 전반에 걸쳐 \\(\\overline{\\mathrm{SIM}}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{SIM}_{t}\\mathrm{CT}_{n,t}\\의 유사성을 요약한 \\(\\overline{\\mathrm{SIM}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{SIM}_{t}\\)을 고려한다.\n' +
      '\n' +
      '그림 6: 퍼셉트론 선형 분류 **(a)** 및 트랜스포머 언어 모델링 **(b)** 태스크에서 우리의 학습 법칙(정리 3.1)의 경험적 증거. 학습예제의 기여도(\\(\\mathrm{CT}_{n,t}\\)의 평균과 예제(식 7)에 걸친 \\(\\mathrm{CT}_{n,t}\\)의 표준편차를 나눈 값으로 계산한 \\(\\mathrm{SIM}_{t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\), \\(\\mathrm{CT}_{n,t}\\)에 의해 서로 다른 샘플 간의 기여도의 유사도를 측정한다. 더 높은 \\(\\mathrm{SIM}_{t}\\)은 더 나은 기여 유사성을 의미한다. 다양한 학습 과정에서 원하는 손실(L^{\\text{dst}(\\mathbf{\\theta}_{t})에 대해 \\(\\mathrm{SIM}_{t}\\)을 표시한다. 각 라인은 일정한 학습 과정이며, 그 색상은 해당 압축 비율(\\(\\mathrm{CR}\\))을 의미한다. 학습 전반에 걸쳐 더 높은 \\(\\mathrm{CR}\\)을 갖는 실행은 일반적으로 더 높은 \\(\\mathrm{SIM}_{t}\\)을 가지며, 이는 우리의 학습 법칙(정리 3.1)과 일치하는 최적에 가까운 학습 과정에서 예제 기여도가 서로 더 유사함을 나타낸다.\n' +
      '\n' +
      '높은 압축비는 높은 표본 기여도 유사성과 상관관계가 있다. 그림 6에서 우리는 \\(\\mathbf{\\gamma}_{t}\\)의 최적화 과정을 따라 발견된 각 정책에 의해 유도된 학습 과정에서 \\(\\mathrm{SIM}_{t}\\)의 값을 조사한다. 연구된 정책들은 보다 빠른 수렴을 가져오기 때문에, 우리는 \\(t\\)보다는 \\(L^{\\text{\\tiny{flr}}(\\mathbf{\\theta}_{t})\\에 대해 \\(\\mathrm{SIM}_{t}\\)을 표시한다. 이러한 방법으로 모델 학습의 동일한 "단계"에서 \\(\\mathrm{SIM}_{t}\\)을 비교하여 서로 다른 수렴 속도의 영향을 마이그레이션한다. 그림 6은 모델 학습에서 압축률(\\(\\mathrm{CR}\\))이 높은 학습 과정이 일반적으로 더 높은 \\(\\mathrm{SIM}_{t}\\)을 유지한다는 것을 보여주며, 이는 개별 샘플의 기여도 \\(\\mathrm{CT}_{n,t}\\)가 학습 과정 전반에 걸쳐 서로 더 유사함을 나타내며, 이는 우리의 학습 법칙(정리 3.1)과 일치한다.\n' +
      '\n' +
      '표본 기여도는 학습 과정이 최적에 접근할 때 동일한 경향이 있다. 그림 7에서는 각 학습 과정에 대해 \\(\\overline{\\mathrm{SIM}}\\)을 \\(\\mathrm{CR}\\)에 대해 플롯한다. 우리는 \\(\\mathrm{CR}\\)이 어떤 값에 접근할 때 \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\)의 분명한 경향을 관찰한다. 따라서 본 연구에서는 실험 관측의 경향에 맞는 함수\\(\\overline{\\mathrm{SIM}}=\\log\\left(\\frac{a}{b-\\mathrm{CR}}\\right)^{c}\\)를 사용한다. 그림 7은 최적(\\(\\mathrm{CR}\\to b\\)), \\(\\mathrm{CT}_{n,t}\\)을 허용하기 위해 \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\)의 표준편차가 0이어야 함을 나타낸다. 이를 통해 학습법칙(정리 3.1)은 0이 아닌 훈련 샘플(\\(\\mathrm{CT}_{n,t}\\))의 기여도가 최적 학습에서 동일함을 검증한다.\n' +
      '\n' +
      '제로-웨이트 실시예의 특성###\n' +
      '\n' +
      '섹션 4.4의 실험은 대부분 0이 아닌 예제에 초점을 맞춘다. 이 절에서는 \\(\\gamma_{n,t}=0\\)의 예제의 성질을 살펴봄으로써 학습법칙(정리 3.1)에 대한 보다 실증적 증거를 제시한다. 정리 3.1로부터 최적 학습 역학의 세 가지 성질을 도출한 후 실험을 통해 검증한다. **첫 번째 속성**은 비-양성 기여도를 갖는 예들이 \\(\\gamma_{n,t}=0\\)을 수신하도록 보장하며, 이는 각 시간 단계에서의 "시끄러운" 예들이 최적의 학습 정책에 의해 배제됨을 나타낸다:\n' +
      '\n' +
      '특성 4.1**.:_모델이 수렴하기 전에 \\(\\mathrm{CT}_{n,t}\\leq 0\\)이 \\(\\gamma_{n,t}=0\\)이 되는 학습예\\(x_{n}^{\\text{\\tiny{flr}}\\)\n' +
      '\n' +
      'Proof.: 수렴하기 전에 \\(\\frac{\\mathrm{d}L^{\\text{\\tiny{flr}}(\\mathbf{\\theta}(t))}{\\mathrm{d}t<0\\)이 성립하여 \\(\\mathrm{CT}_{n,t}>0\\)에 대한 \\(\\mathrm{CT}_{n,t}>0\\)을 나타내며, 정리 3.1에 따르면 \\(\\gamma_{n,t}>0\\)을 만족하는 \\(\\mathrm{CT}_{n,t}\\leq 0\\Rightarrow\\gamma_{n,t}=0\\)을 나타낸다.\n' +
      '\n' +
      '도 7: 퍼셉트론 선형 분류 **(a)** 및 트랜스포머 언어 모델링 **(b)** 태스크에서 학습 법칙(정리 3.1)의 경험적 증거. 그림 6에 이어 학습 과정에서 학습 예제 기여도의 유사성을 요약한 \\(\\overline{\\mathrm{SIM}}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathrm{SIM}_{t}\\)을 고려한다. 우리는 \\(\\overline{\\mathrm{SIM}}\\)과 \\(\\mathrm{CR}\\)의 관계를 도식화하고 \\(\\overline{\\mathrm{SIM}}\\to+\\infty\\)이 어떤 값에 접근할 때 \\(\\overline{\\mathrm{SIM}}=\\log\\left(\\frac{a}{b-\\mathrm{CR}}\\right)^{c}\\)에 의해 적합할 수 있는 뚜렷한 경향을 관찰한다. 학습 과정이 최적(\\(\\mathrm{CR}\\to b\\))에 근접할 때, 학습 예제 기여도의 표준 편차는 0이어야 \\(\\overline{\\mathrm{SIM}\\to+\\infty\\)이 가능하다. 이는 학습법칙(정리 3.1)에서 모든 학습예제가 최적의 학습에서 모델에 동일한 기여를 한다는 것을 검증한다.\n' +
      '\n' +
      '실증적 증거.__ 우리는 \\(t\\)(\\(\\mathrm{CT}_{n,t}\\leq 0\\): \\(\\frac{\\sum_{n,t}1[\\gamma_{n,t}=0]1[\\mathrm{CT}_{n,t}\\leq 0]}{\\sum_{n,t}\\leq 0]에서 모든 예제들 중 제로 가중치 예제들(\\(\\gamma_{n,t}=0\\))의 분율을 계산하고, 이 분율을 그림 8에서 해당 학습 과정의 \\(\\mathrm{CR}\\) 값에 대해 플롯하면, 이 분율이 100%의 경향이 있음을 알 수 있다.\n' +
      '\n' +
      '**두 번째 속성**은 퍼셉트론 선형 분류에 대해서만 유도되며, 이는 최적의 학습 정책이 완벽하게 학습된 훈련 예들을 무시할 것임을 나타낸다:\n' +
      '\n' +
      '속성 4.2**: 퍼셉트론의 경우, 모델이 아직 수렴되지 않았을 때 최적의 학습정책에서 \\(\\gamma_{n,t}=0\\)을 얻을 수 있다.\n' +
      '\n' +
      'Proof.: \\((2y_{n}^{\\text{trn}-1)\\mathbf{z}_{n}^{\\text{trn}}\\to+\\infty\\일 때, \\(o_{n}^{\\text{trn}}-y_{n}^{\\text{trn}}\\to0\\), \\(\\nabla l(x_{n}^{\\text{trn},\\mathbf{\\theta}_{t})=(o_{n}^{\\text{trn}-y_{n}^{\\text{trn}}\\to\\mathbf{0}\\) 및 \\(\\mathrm{CT}n,t}\\to0\\)을 의미한다. \\(\\gamma_{n,t}\\neq 0\\)을 가정하면, 정리 3.1에 따라 최적 학습 과정에서 \\(\\mathrm{CT}_{n,t}=-\\frac{\\mathrm{d}t}L^{\\text{dsr}(\\mathbf{\\theta}(t))\\)이 존재하므로 \\(\\big{|}\\frac{\\mathrm{d}t}L^{\\text{dsr}(\\mathbf{\\theta}(t))\\big{|\\)는 임의로 작아야 한다. 모형이 수렴되지 않을 때는 이 값이 적용되지 않습니다. 따라서 우리는 \\(\\gamma_{n,t}=0\\)을 갖는다.\n' +
      '\n' +
      '실증적 증거.__ 그림 9에서, 우리는 잘 학습된 퍼셉트론 훈련 예제 \\(x_{n}^{\\text{trn}\\)에 대한 \\(\\frac{\\gamma_{n,t}{\\max_{n}\\{\\gamma_{n,t}\\}\\)의 누적 확률 분포 함수를 인스턴스당 훈련 손실이 거의 0에 가까운 \\(l(x_{n}^{\\text{trn},\\mathbf{\\theta})<1\\times 10^{-6}\\)으로 플롯한다. 그림 9는 거의 최적의 정책에 대해 90% 이상의 잘 학습된 예가 상대적으로 낮은 \\(\\gamma_{n,t}\\) (\\(\\mathbf{<}\\) 0.2 \\(\\max_{n}\\{\\gamma_{n,t}\\}\\)임을 보여준다. 이러한 경향은 학습정책이 최적(\\(\\mathrm{CR}\\)이 증가함에 따라 더욱 뚜렷해지며, 이는 Property 4.2를 검증한다.\n' +
      '\n' +
      '그림 8: 속성 4.1의 경험적 증거: 비-기여적 및 잡음적 예는 최적 학습에서 제외된다. y축은 같은 시간 단계에서 \\(\\mathrm{CT}_{n,t}\\leq 0\\)을 갖는 예 중에서 0 가중치 예제의 분수이다. 각 점은 학습 정책을 나타내며, 최적에 접근할 때 예제 가중치 \\(\\gamma_{n,t}=0\\)을 잡음 및 비-기여 데이터의 100%에 할당하는 경향이 있다.\n' +
      '\n' +
      '그림 9: Property 4.2의 경험적 증거: 완벽하게 학습된 예는 최적의 학습에서 무시된다. 우리는 \\(x_{n}^{\\text{trn},\\mathbf{\\theta}_{t})<1\\times 10^{-6}\\을 만족하는 예제 가중치 \\(\\gamma_{n,t}\\)의 누적분포함수(CDF)를 나타낸다. 각 행은 학습 과정에 해당한다. 거의 최적인 학습에서 저손실 예제(완전 학습)의 많은 부분은 작은 \\(\\gamma_{n,t}\\) 값(무시됨)을 얻으며, 이러한 경향은 학습이 최적(\\(\\mathrm{CR}\\)에 근접할 때 더욱 뚜렷해진다.\n' +
      '\n' +
      '**세 번째 속성**은 최적의 학습 정책이 "중복" 훈련 사례를 폐기할 것임을 시사한다. 이 성질은 Perceptron 선형 분류로부터 유도되었지만, 트랜스포머 언어 모델링에도 적용된다는 것을 경험적으로 발견하였다. 우리는 집합의 예제 입력이 선형적으로 상관될 때 집합(\\{x_{n}\\{n}_{n=1}^{N}\\)을 "중복" 예제라고 부른다. 즉, 집합의 예제 입력이 모두 0이 아닌 \\(K\\) 스칼라 \\(\\{\\alpha_{n}\\{n}_{n=1}^{N}\\(\\sum_{n=1}^{n}\\alpha_{n}\\mathbff{z}_{n}=\\mathbf{0}\\)이 존재한다.\n' +
      '\n' +
      '특성 4.3**: _For Perceptrons, if the training set \\(\\{x_{n}^{\\mathrm{trn}}\\}_{n=1}^{N}\\)이 중복된 예제를 갖는 경우, 확률 1, 적어도 하나의 예제 \\(x_{i}^{\\mathrm{trn}\\)은 모델이 최적의 학습 과정에서 아직 수렴될 때 시간 단계 \\(t\\)에서 \\(\\gamma_{i,t}=0\\)을 얻는다.__\n' +
      '\n' +
      '증명: \\(\\{x_{n}^{\\text{sf{tm}}}}_{n=1}^{N}})이 중복된 예제들을 갖는다면, \\(\\{\\alpha_{n}\\{n}}_{n=1}^{N}}), 모든 0이 아닌, \\(\\sum_{n=1}^{n}\\alpha_{n}\\mathbff{z}_{n}^{text{sf{tm}}}=\\mathbf{0}}, \\(\\sum_{n}\\frac{\\alpha_{n}}{\\sigma_{n}^{\\text{sf{tm}}}}-y_{n}^{text{n,t}}}}\\mathrm{CT}_{n,t}=0\\(\\sum_{n}\\alpha_{n}\\alpha_{n}\\mathbfff{z}^{text{sf{tm}}}}}=\\mathbf{ 정리 3.1에 따라 \\(\\forall 1\\leq n\\leq N\\), \\(\\gamma_{n,t}\\neq 0\\)을 가정하면, \\(\\mathrm{CT}_{n,t}=-\\frac{\\mathrm{d}t}L^{\\text{{\\sf{ds}}(\\boldsymbol{\\theta}(t))\\)이 있으며, 이는 \\(\\sum_{n=1}\\frac{\\alpha_{n}{\\sigma_{n}}{\\text{sf{tm}}}-y_m}L^{\\text{sf{tm}}(\\boldsymbol{\\theta}(t))\\\\(\\sum_n=1}\\frac{\\alpha_{n}{\\sigma_n}}}-y_m}L^{\\text{sf{tm}}(\\boldsym{\\theta}(t))\\frac{\\mathrm{d}t}L^{\\text{sf{ds}} i.i.d. 입력 \\(\\{\\mathbf{z}_{n}}^{\\text{sf{tm}}}}_{n=1}^{N}\\frac{\\alpha_{n}{\\sigma_{n}^{\\text{sf{tm}}}-y_m}^{\\text{sf{tm}}}\\neq 0\\(\\frac{\\mathrm{d}}{\\mathrm{d}t}L^{\\text{sf{ds}}(\\boldsymbol{\\theta(t))=0\\)을 의미한다. 모형이 아직 수렴된 경우에는 이 값이 적용되지 않습니다. 따라서, 우리는 \\(1\\leq n_{0}\\leq N,\\text{such }\\gamma_{n_{0},t}=0\\)의 성질을 갖는다.\n' +
      '\n' +
      '_Empirical Evidence_ 그림 10에서는 Perceptron과 Transformer의 학습 과정에서 \\(\\mathrm{CT}_{n,t}>0\\)을 만족하는 \\(\\gamma_{n,t}\\) 값의 동역학을 시각화하였다. 퍼셉트론의 경우, 모델 차원(128)은 트레이닝 예제의 수(4096)보다 낮으며, 이는 트레이닝 데이터세트가 중복된다는 것을 의미한다. 그림 10은 non-contributive 예제의 부재를 고려할 때, 모델이 수렴하기 전에 \\(\\boldsymbol{\\gamma}_{t}\\)의 큰 부분이 여전히 상대적으로 작은 값을 수신함을 보여주는데, 이는 훈련 세트의 중복성에 기인한다. 그림 10에서 우리는 훈련 인스턴스 수보다 \\(\\boldsymbol{\\theta}_{t}\\)의 차원이 더 크지만 트랜스포머에 대해서도 유사한 현상을 관찰한다. 그 이유는 트랜스포머의 고유 차원이 훈련 집합의 중복성으로 이어지는 \\(\\boldsymbol{\\theta}_{t}\\)[1]의 차원에 비해 훨씬 작기 때문인 것으로 추측된다.\n' +
      '\n' +
      '###학습 가속도의 본질\n' +
      '\n' +
      '학습 단계의 증가와 테스트 손실의 감소 사이의 멱 법칙(\\(L^{\\text{{\\sf{ds}}}(\\boldsymbol{\\theta}_{t}))을 드러내는 LMs[13]의 스케일링 법칙의 관점에서 거의 최적인 학습 정책이 가져온 본질적인 개선 사항을 워밍업 단계 \\(t_{0}\\) 후에 조사한다.\n' +
      '\n' +
      '\\[L^{\\text{{\\sf{ds}}}(\\boldsymbol{\\theta}_{t})=\\left(\\frac{B}{t}\\right)^{\\beta}\\!,\\;t>t_{0}, \\tag{8}}}\n' +
      '\n' +
      '여기서 \\((B,\\beta)\\)는 스케일링 법칙 계수이다. 다음에서는 기존 및 거의 최적인 학습 정책에 의해 유도된 학습 프로세스의 스케일링 속성을 연구한다.\n' +
      '\n' +
      '그림 10: Property 4.3의 경험적 증거: 중복 훈련 예는 최적의 학습에서 폐기된다. 근-최적 학습 과정 전반에 걸쳐 \\(\\mathrm{CT}_{n,t}>0\\)(동반 예제와 미학습 예제)을 만족하는 2048개의 훈련 예제를 무작위로 표본화하고 예제 가중치 \\(\\gamma_{n,t}\\)((a)와 (b)의 색으로 나타냄)의 역학을 보여준다. 퍼셉트론은 빠르게 수렴하기 때문에 \\(t\\leq 50\\)에 대한 \\(\\gamma_{n,t}\\) 역학만 표시한다. 최적화에 가까운 정책들은 완벽하게 학습된 데이터 포인트와 비귀납적 데이터 포인트 외에 중복 예제에 \\(\\gamma_{n,t}=0\\)을 할당한다.\n' +
      '\n' +
      '근-최적 학습 정책은 LMs의 스케일링 법칙 계수를 개선한다. 그림 11에서, \\(t_{0}=400\\)4를 설정하여 기존 및 근-최적 학습 정책에 의해 유도된 트랜스포머의 손실 곡선을 수학식 8과 함께 피팅한다. 근-최적 학습 프로세스는 여전히 스케일링 법칙을 따르며, \\(B\\) 및 \\(\\beta\\)은 각각 96.6% 및 21.2% 개선되는 것을 관찰한다. 또한, 표 2는 더 큰 \\(T\\) 및 \\(N\\)의 설정에서 발견된 거의 최적인 정책에 대한 개선이 유지됨을 보여준다. 우리는 학습 데이터의 충분성을 보장하기 위해 \\(N\\)을 \\(T\\)과 함께 성장하도록 하였다[12]. 스케일링 법칙 계수, 특히 \\(\\beta\\)의 개선은 멱법칙 성장을 활용하여 LLM 학습의 속도를 높이는 데 상당한 잠재력을 제공한다. \\(\\mathbf{\\gamma}^{(1)})와\\(\\mathbf{\\gamma}^{(2)}\\)에 대한 두 개의 손실 곡선(L^{\\text{dsr}_{\\mathbf{\\gamma}^{(1)}(\\mathbf{\\theta}_{t)})과\\(L^{dsr}_{\\mathbf{\\gamma}^{(2)}}(\\mathbf{\\gamma}^{(1)}))에 대한 두 개의 스케일링 법칙 계수(\\((B_{1},\\beta_{1})와\\((B_{2},\\beta_{2)})에 대한 \\(\\mathbf{\\gamma}^{(2)})의 가속비는 다음과 같다.\n' +
      '\n' +
      '각주 4: 실제로 식 8을 \\(\\ln L^{\\text{dsr}}(\\mathbf{\\theta}_{t})=-\\beta\\ln t+\\beta\\ln B\\로 변환하고 선형 회귀를 수행한다.\n' +
      '\n' +
      '\\frac{T}{\\arg\\min_{t}_{\\mathbf{\\gamma}^{(2}}(\\bm{\\theta}_{t})\\leq L^{\\text{dsr}_{\\mathbf{\\gamma}^{(1}}(\\mathbf{\\theta}_{T}}\\right\\}}=\\frac{B_{1}^{\\frac{\\beta_{1}}{\\beta_{2}}}{B_{2}}T^{1-\\frac{\\beta_{1}}{\\beta_{2}}}{tag{9}\\t}}{{beta_{2}}}{{beta_{2}}}{{beta_{2}}}{{beta_{2}}{{beta_{2}}}{{beta_{2}}{{beta_{2}}{{beta_{2}}{{beta_{2}}{{beta_{2}}\n' +
      '\n' +
      '10M 단계에 대해 사전 훈련된 LM에 대해, 우리는 그림 11 및 표 2와 같이 LM의 스케일링 특성이 개선되면 훈련 종료 시 \\(9\\times\\) 이상의 가속도를 얻을 것이다. LLMs [13, 14, 15] 훈련의 최근 경험에 따르면, 모델은 현재 훈련 예산 하에서 완전히 수렴되지 않으며, 이는 충분한 훈련 단계가 주어지면 작은 모델(7B)이 큰 모델(65B)의 성능에 도달할 가능성이 있음을 의미한다. 그러나, Chinchilla의 법칙[12]에 따르면, 훈련 단계를 확장하는 것은 특정 성능을 달성하기 위해 모델을 확대하는 것보다 더 많은 계산을 필요로 한다. 따라서, 학습 정책을 최적화하여 개선\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c} \\hline \\hline \\(T\\) & \\(N\\) & \\(|\\frac{\\Delta B}{B}|\\) (\\%) & \\(|\\frac{\\Delta\\beta}{\\beta}|\\) (\\%) & \\(\\mathrm{AR}\\) \\\\ \\hline\n' +
      '1K&(2^{12}\\) & 88.5&10.0&2.16\\\\\\\n' +
      '2K & \\(2^{13}\\) & 94.9 & 18.0 & 2.31 \\\\\\\n' +
      '4K&\\(2^{14}\\) & 93.7&18.7&2.41\\\\\\\n' +
      '8K & \\(2^{15}\\) & 94.8 & 19.0 & 2.48 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 트랜스포머 언어 모델링에서 다양한 총 훈련 단계(\\(T\\)) 및 데이터 크기(\\(N\\))에 대한 거의 최적화된 학습 정책이 가져온 스케일링 법칙 계수의 개선. 어휘 크기는 \\(N\\)의 성장에 따라 증가한다.(자세한 내용은 부록 D 참조). \\(N\\) (\\mathrm{AR}\\)은 수학식 9에 정의된 바와 같은 가속 비율을 나타낸다. 개선은 더 큰 \\(T\\) 및 \\(N\\)에 대해 유지된다.\n' +
      '\n' +
      '그림 11: 트랜스포머 언어 모델링에서 기존 및 거의 최적인 LM 학습을 위한 스케일링 법칙[12]의 그림: \\(L^{\\text{dsr}}(\\mathbf{\\theta}_{t})=(B/t)^{\\beta}\\. 우리는 손실 곡선을 스케일링 법칙에 의해 피팅하여 상관 계수 \\(r^{2}\\)을 구하고 손실 곡선(실선)과 적합 곡선(점선)을 로그 로그 플롯에 표시한다. 스케일링 법칙은 기존의 LM 학습과 거의 최적이 아닌 LM 학습 모두에 잘 어울린다. 근최적 LM 학습은 스케일링 법칙의 계수 \\((B,\\beta)\\)를 96.6%와 21.2% 향상시킴으로써 LLM의 학습 속도 향상에 큰 잠재력을 보여준다.\n' +
      '\n' +
      '학습 속도, 잘 수행된 소규모 모델을 훈련하는 비용을 크게 줄일 수 있으며, 이는 LM 연구 커뮤니티의 오픈 소스 노력과 산업 제품의 효율성 모두에 도움이 된다. 이는 실제적인 학습정책 최적화 접근법 설계의 가능성과 의의를 나타내며, 우리의 이론은 귀중한 지침이 될 수 있다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '언어 모델의 학습 속도 개선 모델 아키텍처 [20, 21] 또는 최적화기 [20, 24, 25]와 같은 LM 학습 속도를 가속화하기 위한 접근법을 제안하는 광범위한 작업이 있다. 학습 정책 최적화 특례로 볼 수 있는 데이터 중복 제거[26, 27], 도메인 혼합[28], 내재적 과제 발견[13], 온라인 데이터 선택 또는 재정렬[3, 1] 등 LM 융합 속도를 높이기 위한 사전 학습 프로그래밍을 연구하는 작업도 있다. 이러한 작업과 달리 본 논문에서는 LM 학습의 최적화 원리를 조사한다.\n' +
      '\n' +
      '언어 모델링 및 무손실 압축.최근 LLM의 성공은 더 큰 모델 크기가 지속적으로 더 나은 다운스트림 일반화를 유발한다는 사실에 대해 고전적 통계 학습 이론을 넘어서는 새로운 해석을 요구한다[24, 25]. 해석들 중 하나는 LM의 다음-토큰-예측 트레이닝 프로세스를 무손실 데이터 압축으로 보는 것이다[1, 13, 14]. 이러한 관점에서, 더 큰 LM은 더 높은 압축 비율을 가지며, 이는 데이터 생성 규칙성의 더 나은 모델링에 해당한다. 일부 최근 작업[24, 25]은 잘 훈련된 LM을 압축기로 사용하여 탐색하므로 모델 크기를 압축 데이터로 계산해야 한다는 점에 주목할 가치가 있다. 이러한 작업과 달리 LM 트레이닝을 압축으로 보는 것은 압축된 데이터에 모델 파라미터를 포함시킬 필요가 없으므로(보강 증명을 위해 부록 A.1 참조) LMs의 모델 크기 스케일링 법칙[25]과 더 호환된다.\n' +
      '\n' +
      '##6 토론 및 결론\n' +
      '\n' +
      '요약.이 작업에서는 LMs의 최적 학습을 위한 이론을 수립한다. 본 논문에서는 LM-training-as-losses-compression 관점에서 압축률을 최대화하는 목표를 제안한다. 그리고 최적의 학습 과정에서 모든 예제들이 LM에 균등하게 기여해야 함을 제안하는 _Learning Law_라는 정리를 유도하고, 선형 분류 및 실제 언어 모델링 작업에 대한 실험을 통해 검증한다. 마지막으로, 최적의 학습 과정이 LMs의 스케일링 법칙 계수를 본질적으로 개선한다는 것을 경험적으로 보여주며, 이는 실용적인 학습 가속 접근법을 설계하는 향후 작업에 대해 조명한다.\n' +
      '\n' +
      '한계.우리 작업의 한 가지 한계는 실험이 비교적 작은 규모로 수행된다는 것이다. 근최적 학습 정책을 찾는 방법은 \\(L\\times T\\) 계층으로 신경망을 학습시키는 방법에 해당하기 때문에 \\(L\\)은 LM 계층이고 \\(T\\)은 LM 전체 학습 단계(자세한 내용은 부록 C 참조)이다. 이것은 \\(L\\) 및 \\(T\\) 스케일 업 시 높은 계산 오버헤드로 이어진다. 그러나 이론적 도출이 일반적으로 적용 가능하기 때문에 LLMs에 우리의 이론이 적용될 수 있다고 믿는다. 또 다른 한계는 LM이 미니 배치 Adam[1]과 같이 더 일반적으로 사용되는 기술보다는 전체 배치 GD로 훈련된다고 가정한다는 것이다. 이러한 방법들은 본질적으로 기울기 기반이기 때문에, 우리의 이론은 여전히 이러한 기법들을 기반으로 한 미래의 LM 학습 가속 연구에 통찰력을 제공할 수 있다[24, 25].\n' +
      '\n' +
      '미래 작업.우리는 향후 작업의 중요한 방향이 LMs의 대규모 교육을 위해 우리의 이론을 기반으로 최적의 학습 정책을 찾기 위한 실용적인 방법을 설계하는 것이라고 믿는다. 정말로, 이 방향에는 무시할 수 없는 도전들이 있다. 학습 법칙은 학습 정책의 최적성에 필요한 조건을 제공하기 때문에, 차선의 해를 방지하기 위해 보다 많은 정규화 조건이 요구될 수 있다. 또한, 최적의 학습 정책을 찾는 접근법은 전체 계산 비용에 크게 기여하지 않으면서 충분히 효율적이어야 한다. 그럼에도 불구하고 우리의 작업은 이 방향의 약속과 잠재력을 보여준다. 최근 LLMs 훈련에 대한 연구[24, 25]에 따르면, 손실은 여전히 수렴과는 거리가 멀며, 이는 작은 모델이 큰 모델과 유사한 성능에 도달할 가능성이 있지만 큰 총 훈련 단계에 의해 야기되는 계산 오버헤드에 의해 방해를 받는다는 것을 의미한다. 최적의 학습 정책은 식 9의 멱법칙 성장의 도움으로 잠재적으로 훈련의 큰 가속을 가져오며, 이는 주어진 (불가피하게) 제약된 계산의 한계를 탐색하고 실제에서 현재의 LLM을 대체하는 잘 수행된 작은 LM을 훈련할 수 있게 한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[ABL\\({}^{+}\\)22] Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. 학습 데이터로 회귀하는 언어 모델의 지식을 추적합니다. 요브 골드버그, 조르니사 코자레바, Yue Zhang에서 편집자, _Findings of EMNLP_, 2022.\n' +
      '*[ADF\\({}^{+}\\)23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen et al. Palm 2 기술 보고서. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [AGZ21] 아르멘 아하잔얀, 소날굽타, 루크 제틀모이어. 내재적 차원성은 언어 모델 미세 조정의 효과를 설명한다. In _Proceedings of ACL_, 2021.\n' +
      '* [APRW23] Alon Albalak, Liangming Pan, Colin Raffel, William Yang Wang. 언어 모델 사전 교육을 위한 효율적인 온라인 데이터 혼합. R0-FoMo에 대한 _NeurIPS 2023 워크샵에서: 대규모 기초 모델_, 2023에서 Few-shot 및 Zero-shot 학습의 견고성.\n' +
      '*[ATS\\({}^{+}\\)23] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. SemDeDup: 의미 중복 제거를 통한 웹 스케일에서의 데이터 효율적인 학습. In _ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models_, 2023.\n' +
      '*[BC11] HH Bauschke and PL Combettes. _ 힐버트 공간에서의 볼록 해석과 모노톤 연산자 이론 스프링거, 2011년\n' +
      '* [벨19] Fabrice Bellard. NNCP: Lossless data compression with neural networks, 2019.\n' +
      '*[Ber16] Dimitri Bertsekas. _ 비선형 프로그래밍_, 볼륨 4. 아테나 과학, 2016.\n' +
      '*[BHA\\({}^{+}\\)21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselt, Emma Brunskill, et al. arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '*[BMR\\({}^{+}\\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. 언어 모델은 소수의 학습자이다. In _Proceedings of NeurIPS_, 2020.\n' +
      '*[CM03] Corinna Cortes and Mehryar Mohri. AUC 최적화 vs. 오류율 최소화. In _Proceedings of NeurIPS_, 2003.\n' +
      '*[CMS\\({}^{+}\\)23] Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, et al. h2oGPT: Democraticatizing large language models. _ arXiv preprint arXiv:2306.08161_, 2023.\n' +
      '*[CND\\({}^{+}\\)23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts 등 PaLM: 경로를 갖는 스케일링 언어 모델링. _ JMLR_, 2023.\n' +
      '*[CRB\\({}^{+}\\)23] Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Re. 스킬잇! 언어 모델을 이해하고 훈련하기 위한 데이터 기반 기술 프레임워크 In _Proceedings of NeurIPS_, 2023.\n' +
      '*[DRD\\({}^{+}\\)24] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 언어 모델링은 압축이다. In _Proceedings of ICLR_, 2024.\n' +
      '*[EC21] Omer Elkabetz and Nadav Cohen. 연속 대 연속 심층 신경망의 이산 최적화. A. 베이겔지머, Y. Dauphin, P. Liang and J. Wortman Vaughan, editorators, _Proceedings of NeurIPS_, 2021.\n' +
      '[EL23] 로난 엘단과 위안지 리. TinyStories: 언어 모델이 얼마나 작고 여전히 일관성 있는 영어를 말할 수 있을까? _ arXiv preprint arXiv:2305.07759_, 2023.\n' +
      '*[Eng01] Andreas Engel. _ 학습의 통계적 역학. 케임브리지 대학 출판사 2001년\n' +
      '\n' +
      '[GAH23] David Grangier, Pierre Ablin, and Awni Hannun. 도메인 시프트 하에서 언어 모델링을 위한 학습 분포를 학습하는 이중 레벨 최적화. _NeurIPS 2023 배포 변경에 대한 워크샵: 기초 모델을 가진 새로운 프론티어, 2023.\n' +
      '* [GHLH22] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: 소수의 학습을 위한 사전 훈련된 프롬프트 튜닝. In _Proceedings of ACL_, 2022.\n' +
      '*[GS\\({}^{+}\\)00] Izrail Moiseevitch Gelfand, Richard A Silverman, et al. _Calculus of variations_. 2000년 택배 회사\n' +
      '*[HBD\\({}^{+}\\)20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 신경 텍스트 퇴화의 기이한 사례. In _Proceedings of ICLR_, 2020.\n' +
      '*[HBM\\({}^{+}\\)22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '*[HZD\\({}^{+}\\)21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained model: 과거, 현재 및 미래. _ AI Open_, 2021.\n' +
      '* [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 및 Jian Sun. 이미지 인식을 위한 딥 잔차 학습. In _Proceedings of CVPR_, 2016.\n' +
      '*[JSM\\({}^{+}\\)23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier 등 Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '[KB15] Diederik P. Kingma and Jimmy Ba. 아담: 확률적 최적화를 위한 방법. In _Proceedings of ICLR_, 2015.\n' +
      '* [KEC22] Ilia Kulikov, Maksim Eremeev, and Kyung Hyun Cho. 신경 자기 회귀 시퀀스 모델링에서 과평활화 문제를 특성화하고 해결한다. In _Proceedings of AACL_, 2022.\n' +
      '* [KL17] Pang Wei Koh and Percy Liang. 영향 함수를 통해 블랙박스 예측을 이해합니다. In _Proceedings of ICML_, 2017.\n' +
      '*[KMH\\({}^{+}\\)20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei. 신경 언어 모델의 법칙을 확장합니다. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '[KPA12] 셀레스트 키드, 스티븐 티 피안타도시, 리처드 N 아슬린. 골디락스 효과: 인간 유아는 너무 단순하지도 너무 복잡하지도 않은 시각적 시퀀스에 주의를 기울인다. _ PloS one_, 7(5):e36399, 2012.\n' +
      '*[LLH\\({}^{+}\\)24] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma. 소피아: 언어 모델 사전 훈련을 위한 확장 가능한 확률적 2차 최적화기. In _Proceedings of ICLR_, 2024.\n' +
      '* [LXLM23] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. 동일한 사전 훈련 손실, 더 나은 다운스트림: 언어 모델에 대한 암시적 편향 문제. In _Proceedings of ICML_, 2023.\n' +
      '*[MBR\\({}^{+}\\)22] Soren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holtgen, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on the point are learnable, worth learning, yet learnednt. In _Proceedings of ICML_, 2022.\n' +
      '* [MCKX22] Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. 추적: 고속 트랜스포머 기반 범용 무손실 압축기입니다. In _Proceedings of WWW_, New York, NY, USA, 2022.\n' +
      '*[Met09] Janet Metcalfe. 초인지적 판단과 연구의 통제 현재 심리과학의 방향_, 18(3):159-163, 2009.\n' +
      '\n' +
      '[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. 대배치 훈련의 경험적 모형. _ arXiv preprint arXiv:1812.06162_, 2018.\n' +
      '[MP43] Warren S McCulloch와 Walter Pitts. 신경 활동에 내재된 개념들의 논리적인 미적분학. _ 1943년 수학 생물 물리학 발표회\n' +
      '*[NKB\\({}^{+}\\)19] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep Double Descent: 더 큰 모델과 더 많은 데이터가 손상되는 곳. In _Proceedings of ICLR_, 2019.\n' +
      '*[Ope22] OpenAI. 오픈AI: 2022년 채팅팅을 소개합니다.\n' +
      '*[Ope23] OpenAI. GPT-4 기술 보고서, 2023\n' +
      '*[PGM\\({}^{+}\\)19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Kimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In _Proceedings of NeurIPS_, 2019.\n' +
      '* [PLKS20] 가리마 프루스티, 프레데릭 류, 사티엔 케일, 무쿤 순다라얀. 경사 하강을 추적하여 훈련 데이터의 영향을 추정합니다. 2020년 _NeurIPS_에서.\n' +
      '*[Pon18] Lev Semenovich Pontryagin. _ 최적 과정의 수학 이론_. 2018년 Routledge\n' +
      '*[PR12] Warren B Powell and Ilya O Ryzhov. _ Optimal learning_, volume 841. John Wiley & Sons, 2012.\n' +
      '*[Rae23] 잭 레이. 2023년, agi에 대한 압축\n' +
      '[RM87] 데이비드 E. 루멜하트와 제임스 L. 맥클랜드 오류 전파에 의한 내부 표현 학습 _Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations_, 1987.\n' +
      '* [RRRH20] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: 조 단위 매개변수 모델을 훈련하기 위한 메모리 최적화. In _Proceedings of SC20_, 2020.\n' +
      '[SDBD20] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. 확률적 경사 하강의 암시적 규칙화의 기원에 관한 것이다. In _Proceedings of ICLR_, 2020.\n' +
      '*[TLI\\({}^{+}\\)23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 개방적이고 효율적인 기초 언어 모델 arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '*[TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [TSAM23] 쿠살 티루말라, 다니엘 시미그, 아르멘 아하잔얀, 아리 S 모르코스. D4: 문서 중복 제거 및 다양화를 통한 llm 사전 훈련 개선. In _Proceedings of NeurIPS_, 2023.\n' +
      '*[Vap99] Vladimir Vapnik. _ 통계적 학습 이론의 성질. 1999년 스프링거 과학 및 비즈니스 미디어\n' +
      '*[VNK\\({}^{+}\\)23] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai. LLMZip: 대용량 언어 모델을 이용한 무손실 텍스트 압축 arXiv preprint arXiv:2306.04050_, 2023.\n' +
      '*[VSP\\({}^{+}\\)17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목만 해주시면 됩니다. In _Proceedings of NeurIPS_, 2017.\n' +
      '\n' +
      '*[WKR\\({}^{+}\\)19] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Cho경현, Jason Weston. 비우도 훈련으로 신경 텍스트를 생성합니다. In _Proceedings of ICLR_, 2019.\n' +
      '[WSSC19] Robert C Wilson, Amitai Shenhav, Mark Straccia, and Jonathan D Cohen. 최적의 학습을 위한 85퍼센트의 규칙 Nature communications_, 10(1):4646, 2019.\n' +
      '*[WTB\\({}^{+}\\)22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, et al. TMLR_, 2022.\n' +
      '*[WWL\\({}^{+}\\)23] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. Efficient large language models: A survey. _ arXiv preprint arXiv:2312.03863_, 2023.\n' +
      '*[XPD\\({}^{+}\\)23] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. DoReMi: 데이터 혼합물을 최적화하면 언어 모델 사전 훈련이 빨라집니다. In _Proceedings of NeurIPS_, 2023.\n' +
      '* [XSML23] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, Percy Liang. 중요도 재샘플링을 통한 언어 모델의 데이터 선택 In _Proceedings of NeurIPS_, 2023.\n' +
      '*[XYH\\({}^{+}\\)20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tieyan Liu. 트랜스포머 아키텍처에서 온 레이어 정규화. In _Proceedings of ICML_, 2020.\n' +
      '*[YLR\\({}^{+}\\)20] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, Cho-Jui Hsieh. 딥러닝을 위한 대규모 배치 최적화: 76분 안에 버트를 교육합니다. In _Proceedings of ICLR_, 2020.\n' +
      '* [ZH20] Minjia Zhang and Yuxiong He. 프로그레시브 레이어 드롭을 갖는 변압기 기반 언어 모델의 트레이닝 가속화. _ Proceedings of NeurIPS_, 2020.\n' +
      '* [ZHSJ20] 징자오 장, 톈싱 헤, 수브릿 스라, 알리 자드바비. 왜 그래디언트 클리핑이 훈련을 가속화하는가: 적응성에 대한 이론적 정당화. In _Proceedings of ICLR_, 2020.\n' +
      '\n' +
      '무손실 압축을 이용한 LM 훈련에 관한 고찰\n' +
      '\n' +
      '### 원본 보기: 훈련 데이터를 압축합니다.\n' +
      '\n' +
      '데이터를 압축하기 위해 LM을 사용하는 아이디어는 무손실 텍스트 압축 분야[1, 13]의 문헌에서 유래했으며 최근 LMs[14]의 다음 토큰 예측 기반 사전 훈련의 본질을 해석하기 위해 채택되었다. 우리는 다음의 정리와 [14]의 건설적 증명을 통해 그것의 핵심 정신을 재조명한다.\n' +
      '\n' +
      '**정리 A.1**.: __하나의 에폭에 대한 미니배치 다음-토큰-예측을 사용하여 \\(M\\) 토큰으로 텍스트 코퍼스 \\(\\mathcal{D}\\)에서 훈련된 LM을 고려한다. 시간 단계 \\(t\\)에서 \\(B\\)은 회분식에서의 토큰 수이고 \\(L_{t}\\)은 회분식 평균 학습 손실이다. \\(M\\)은 \\(B\\)으로 나눌 수 있다고 가정하자. 학습 과정은 학습 데이터의 무손실 압축으로 볼 수 있다. 압축 데이터의 설명 길이\\(\\mathcal{C}\\)는_\n' +
      '\n' +
      '\\[d(\\mathcal{C})=\\sum_{t=1}^{M/B}B\\cdot L_{t}+d(\\mathrm{LM}), \\tag{10}\\]\n' +
      '\n' +
      '\\(d(\\mathrm{LM})\\)는 LM 트레이닝을 실행하기 위해 0-1 문자열로 표현되는 필요한 코드의 길이이다.__\n' +
      '\n' +
      '증명: 증명의 기본 아이디어는 LM으로 \\(\\mathcal{D}\\)에 대한 무손실 인코딩 및 디코딩 프로세스를 구성하는 것이다. 시간 단계 \\(t\\), 토큰 프리픽스 \\(w_{<m}=[w_{m-1},w_{m-2},\\cdots,w_{1}]에서 \\(\\mathbf{\\theta}_{t}}(\\cdot|w_{<m})\\)에 의해 파라미터화된 LM의 출력 분포를 \\(\\mathbf{\\theta}_{t}}(\\cdot|w_{<m})으로 하자. 단순화를 위해, LM은 학습률\\(\\eta\\)을 갖는 SGD(Mini-batch Stochastic Gradient Decent)를 사용하여 훈련된다고 가정하며, 여기서 각 배치는 연속적인 토큰 리스트로 선형화된다. 배치 평균 학습 손실은 \\(L_{t}=-\\frac{1}{B}\\sum_{m=1}^{B}\\log p_{\\mathbf{\\theta}_{t}(w_{m}|w_{<m})\\)5이다. 복호화 과정은 알고리즘 1과 알고리즘 2에 설명되어 있으며, 기본적으로는 블루 컬러가 아닌 다른 알고리즘들의 본체가 LM 학습을 구현한다. 인코딩을 위해, 토큰 \\(w_{m}\\)의 설명 길이는 산술 코딩 5에 따라 \\(-\\log p_{\\mathbf{\\theta}_{t}(w_{m}|w_{<m})\\)이고, 따라서 배치 \\(\\mathcal{W}=\\{w_{m}\\}_{m}\\}_{m=1}^{B}\\(\\sum_{m=1}^{B}\\left[-\\log p_{\\mathbf{\\theta}_{t}(w_{m}|w_{<m})\\right]=B\\cdot L_{t}\\). (d(\\mathcal{C})\\)는 훈련 전반에 걸친 배치당 설명 길이와 LM 훈련에 대한 코드의 길이의 합과 같다. 따라서 우리는 \\(d(\\mathcal{C})=B\\cdot\\sum_{t=1}^{M/B}L_{t}+d(\\mathrm{LM})\\을 얻는다. 복호화를 위해, LM 트레이닝을 위한 코드는 부호화와 동일하기 때문에, 우리는 알고리즘 2의 임의의 단계에 대해 \\(\\mathbf{\\theta}_{1}^{\\prime}=\\mathbf{\\theta}_{1}\\), 따라서 수학적 귀납법으로 쉽게 증명될 수 있는 \\(w_{m}^{\\prime}=w_{m}\\)을 갖는다. 결과적으로, \\(\\mathcal{D}\\)은 \\(\\mathcal{C}\\)으로부터 완전히 재구성될 수 있으며, 이는 인코딩(압축)이 무손실임을 나타낸다.\n' +
      '\n' +
      '각주 5: \\(\\log(\\cdot)\\)는 다음 섹션에서 \\(\\log_{2}(\\cdot)\\)을 나타낸다.\n' +
      '\n' +
      '1_Remark: \\(d(\\mathcal{C})\\) 압축 데이터의 설명 길이는 \\(\\mathrm{AUC}\\)\\(M\\gg 1\\)일 때 학습 손실 곡선 아래의 면적(\\(\\mathrm{AUC}\\))에 대략 비례한다. 왜냐하면 LM 학습 코드의 크기가 압축 코퍼스보다 훨씬 작기 때문이다. 따라서 \\(d(\\mathcal{C})\\approx\\sum_{t=1}^{M/B}B\\cdot L_{t}=B\\cdot\\mathrm{AUC}\\(M\\gg 1\\)일 때.\n' +
      '\n' +
      '_Remark 2_.: \\(V\\)을 LM 의 어휘 크기로 하고 \\(M\\gg 1\\)이라고 가정하자. 정리 A.1에서 학습 과정의 대응하는 압축비는 \\(\\mathrm{CR}=\\frac{M\\log V}{d(\\mathcal{C})}\\approx\\frac{M\\log V}{\\sum_{t=1}^{M/B}B\\cdot L_{t}\\propto\\frac{1}{\\mathrm{AUC}\\\\(\\mathrm{CR}=\\frac{M\\log V}{d(\\mathcal{C})}\\approx\\frac{M\\log V}{\\sum_{t=1}^{M/B}B\\cdot L_{t}\\propto\\frac{1}{\\mathrm{AUC}\\이다. LM이 데이터에 적합함에 따라, 우리는 일반적으로 \\(L_{t}<\\log V\\)을 갖는다. 왜냐하면 \\(\\log V\\)은 랜덤하게 초기화된 LM에 대한 손실이기 때문이다. 이것은 압축이 유효하다는 것을 의미하며, 결과적으로 압축 비율 \\(\\mathrm{CR}>> 1\\).\n' +
      '\n' +
      '종합하면 정리 A.1은 데이터 압축과 LM 훈련 간의 연결을 연결합니다. 일반적으로, 더 높은 압축 비율은 다음의 언급에서 언급된 바와 같이, 압축 알고리즘이 기본 데이터 지식을 더 잘 모델링하고 더 잘 수행되는 LM에 대응한다는 것을 나타낸다:\n' +
      '\n' +
      '_Remark 3_.: 데이터에서 지식을 모델링하는 LM의 능력은 손실 AUC에 반비례하는 그것의 학습 프로세스의 대응하는 무손실 압축 비율을 특징으로 한다.\n' +
      '\n' +
      '모델 매개변수는 \\(d(\\mathcal{C})\\) 계산에 포함되지 않으며, 모델 크기를 확대하면 일반적으로 손실 AUC가 감소하므로 LLM의 놀라운 성능을 설명한다. 또한, \\(d(\\mathcal{C})\\)는 최종 손실뿐만 아니라 전체 LM 훈련 과정과 관련이 있다. 이는 최종 손실이 동일하더라도 더 큰 LMs가 더 작은 모델보다 더 나은 성능을 보이는 경향이 있다는 사실과 일치한다[11]. 이러한 관찰은 LM 훈련이 무손실 데이터 압축의 과정으로 개념화될 수 있다는 관점을 뒷받침한다.\n' +
      '\n' +
      '### 우리의 관점: 원하는 배포의 데이터를 압축합니다.\n' +
      '\n' +
      '또한 본 논문에서 손실 AUC에 초점을 맞추었지만, 부록 A.1의 설정과 다르다: (1) 다중 에폭에 대해 전체 배치 기울기 하강(GD)으로 LM을 훈련한다고 가정하는 반면, 정리 A.1은 단일 에폭에 대해 SGD로 LM을 훈련하는 시나리오에 있다; (2) 훈련 예 이외의 데이터에 대해 계산되는 \\(L^{\\text{sfx}}\\)을 고려하는 반면, 수학식 10의 \\(p_{\\mathbf{\\theta}_{t}}\\)은 훈련 데이터에 대해 계산된다. 그러나 완전히 엄밀하지는 않지만, 우리는 (1)과 (2)의 차이에도 불구하고 Remark 3이 여전히 성립한다고 주장한다. 그 이유는, (1)**와 관련하여, mini-batch SGD는 GD의 근사치로서, SGD의 배치 크기가 충분히 클 때 유사한 훈련 동역학을 공유한다는 것을 의미한다; (2)**와 관련하여, 훈련 손실 AUC와 마찬가지로, \\(L^{\\text{sfx}}\\)의 AUC는 학습 프로세스 동안 원하는 데이터 분포_로부터 예시들을 압축하는 것의 설명 길이로 볼 수 있다. 이러한 방법으로, Remark 3은 \\(L^{\\text{sfx}}\\)의 AUC를 최소화하는 것이 원하는 분포에 대한 데이터 압축을 최적화하는 것에 해당함을 나타내며, 이는 원하는 데이터 지식을 모델링하는 LM의 능력을 향상시킨다. 이는 대부분의 시나리오에서 모델 성능이 고전적 기계 학습(20)의 검증 세트, 대규모 사전 학습(13)의 고품질 보유 말뭉치 또는 도메인 적응(21)의 목표 세트와 같은 훈련 세트 이외의 데이터세트에서 측정되기 때문에 더 실용적인 관심사이다.\n' +
      '\n' +
      '```\n' +
      '0 : 압축 데이터\\(\\mathcal{C}\\): 0-1 문자열 리스트\n' +
      '0: 학습 코퍼스 \\(\\mathcal{D}\\)에서 첫 번째 문자열인 \\(\\mathcal{D}\\)에서 빈 목록으로 LM 학습 코드를 얻다\\(t\\gets 1\\)에서\\(M/B\\)do\\(m\\gets 1\\)에서\\(\\mathcal{W}=\\{w}\\{m}\\{m}\\{m}\\{m}\\{m}\\{m}\\{m}\\\\(p\\theta_{t}(\\cdot|w_<m})에 기초한 산술 부호화 \\(\\mathcal{D}\\)에서 0-1 문자열 \\(\\mathcal{C}\\)에서 0-1 문자열 \\(\\mathbf{\\theta}_{t}\\{m}\\{m}\\{m}\\{m}\\\\(\\mathbf{\\theta}\\{m}\\{m}\\{m}\\{m}\\\\(\\mathcal}\\)에서 0-1 문자열 \\(\\mathcal}C}\\(\\mathbf\\\n' +
      '```\n' +
      '\n' +
      '**알고리즘 2** 디코딩\n' +
      '\n' +
      '무손실 압축을 위한 퍼셉트론 훈련\n' +
      '\n' +
      '모델 학습을 무손실 압축으로 보는 것은 LMs의 다음 토큰 예측 학습 패러다임에서 비롯된다. 우리는 이 관점이 선형 분류 작업에서 퍼셉트론의 MLE(One-epoch Maximum Likelihood Estimation) 훈련에도 적합함을 보여주며, 여기서 각 예제의 _label은 입력 벡터_로 압축된다. 특히, 부록 A.1의 증명은 선형 분류를 어휘 크기\\(V=2\\)를 갖는 1단계 언어 모델링으로 취급하더라도 여전히 적용된다. 섹션 4.2의 표기법에 이어서, 시간 단계 \\(t\\)에서 \\(\\mathbf{\\theta}_{t}\\)에 의해 파라미터화된 퍼셉트론에 대해, \\(\\mathbf{z}\\) 상의 \\(y\\) 컨디셔닝을 출력할 확률은 \\(p_{\\mathbf{\\theta}_{t}(y|\\mathbf{z})=o^{y}(1-o)^{1-y}\\), 여기서 \\(o=\\sigma(\\mathbf{\\theta}_{t}\\cdot\\mathbf{z})\\(p_{\\mathbf{\\theta}_{t}}(y|\\mathbf{z})=o^{y}(1-o)^{1-y}\\)이다. 배치 \\(\\mathcal{B}_{t}=\\{(z_{n},y_{n})\\}_{n=1}^{B}\\)의 경우, 배치 평균 손실은 \\(L_{t}=-\\frac{1}{B}\\sum_{n=1}^{B}\\log p_{\\mathbf{\\theta}_{t}(y_{n}|\\mathbf{z}_{n})\\이다. 인코딩 및 디코딩을 위해 알고리즘 1과 알고리즘 2를 적용하였을 때, 압축 \\(\\mathcal{B}_{t}\\)의 설명 길이는 \\(\\sum_{n=1}^{B}\\left[-\\log p_{\\mathbf{\\theta}_{t}(y_{n}|\\mathbf{z}_{n})\\right]=B\\cdot L_{t}\\)로, 이는 정리 A.1이 여전히 유지되고 부록 A.2에서의 논의도 적용된다. 전체적으로 \\(N\\) 예제를 가진 데이터 세트의 압축 비율 \\(\\operatorname{CR}\\approx\\frac{N\\log V}{\\sum_{n=1}^{N/B}B\\cdot L_{t}=\\frac{N}{\\sum_{n=1}^{N/B}B\\cdot L_{t}\\). 무작위로 초기화된 \\(\\mathbf{\\theta}_{1}\\), \\(L_{1}\\approx 1\\) 및 모델로서 \\(L_{t}\\to 0\\)에 대해, 압축비 \\(\\operatorname{CR}>1\\)의 유효한 데이터 압축 과정을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n' +
      '***(\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}\\).***(\\frac{\\partial L^{\\text{dsf}}(t)}{\\partial\\Gamma_{n}}(t)\\)을 해석하는 것은 다른 자유변수들이 고정된 시간 단계 \\(t\\)에서 \\(\\gamma_{n}(t)\\)의 변화가 \\(L^{\\text{dsf}}(t)\\)의 변화에 어떻게 영향을 미치는지 측정한다. 구체적으로, \\(\\Gamma_{n}(t)\\to\\Gamma_{n}(t)+\\Delta\\Gamma_{n}(t)\\)에 의해 \\(\\Gamma_{n}(t)\\)이 변화하면, \\(L^{\\text{dsf}(t)\\to L^{\\text{dsf}(t)\\to L^{\\text{dsf}(t)+\\Delta L^{\\text{dsf}(t)\\) 및 \\(\\frac{\\partial L^{\\text{dsf}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)\\{\\delta\\Gamma_{n}(t)\\\\(L^{\\text{dsf}(t)\\)에 의해 \\(\\Gamma_{n}(t)\\{\\delta\\Gamma_{n}(t)\\{\\delta\\Gamma_{n}(t)\\\\(L^ 그런 다음 식 3을 사용하여 \\(\\frac{\\mathrm{d}L^{\\text{dsf}}(t)}{\\mathrm{d}t}\\)을 고려한다.\n' +
      '\n' +
      '\\nabla L^{\\text{dsf}(t){\\mathrm{d}L^{\\text{dsf}(t))\\cdot\\frac{\\mathrm{d}t}{\\mathrm{d}t}{\\mathrm{d}t}\\nabla L^{\\text{d}(t)\\nabla L^{\\text{tm}(\\mathb}t}\\nabla l(x_{n}{\\text{tm},\\mathbf{\\theta}(t))\\nabla\n' +
      '\n' +
      '그 결과, 작은 \\(\\Delta t\\)에 대해, 우리는 다음과 같다.\n' +
      '\n' +
      '\\[L^{\\text{dsf}}(t+\\Delta t)-L^{\\text{dsf}(t)=-\\sum_{n=1}^{N}\\left[\\Gamma_{n}(t+\\Delta t)-\\Gamma_{n}(t)\\right]\\nabla L\\cdot\\nabla l_{n}. \\tag{17}\\t}\n' +
      '\n' +
      '이제 우리는 \\(t+\\Delta t\\)에서 \\(L^{\\text{dsf}}(t)\\)와 \\(\\Gamma_{n}\\)의 변화를 고려한다. 시간 단계 \\(t\\)에서 \\(\\nabla L\\cdot\\nabla l_{n}\\)이 계산되므로 변형의 영향을 받지 않는다. 그러므로, 우리는:\n' +
      '\n' +
      '\\[\\Delta L^{\\text{dsf}}(t+\\Delta t)=-\\Delta\\Gamma_{n}(t+\\Delta t)\\nabla L\\cdot\\nabla l_{n}, \\tag{18}\\nabla\n' +
      '\n' +
      '\\(\\Delta t\\to 0\\)일 때,\\(\\frac{\\Delta L^{\\text{dsf}}(t+\\Delta t)}{\\Delta\\Gamma_{n}(t+\\Delta t)}{\\Delta\\Gamma_{n}(t)}{\\Delta\\Gamma_{n}(t)}{\\Delta\\Gamma_{n}(t)}{\\Delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}(t)}{\\delta\\Gamma_{n}{\\partial L^{\\text{dsf}}}{\\partial L^{\\text{dsf}}}{\\partial Gamma_{n\n' +
      '\n' +
      '\\[\\frac{\\partial L^{\\text{dsf}}(t}{\\partial\\Gamma_{n}}=-\\nabla L\\cdot\\nabla l _{n}. \\tag{19}\\}\n' +
      '\n' +
      '수학식 19를 수학식 15에 대입하여, \\(\\gamma_{m}(t)>0\\) 및 \\(\\gamma_{n}(t)>0\\)을 만족하는 \\(m^{\\text{th}}\\) 및 \\(n^{\\text{th}}\\) 훈련 예제에 대해 다음의 수학식이 성립함을 알 수 있다:\n' +
      '\n' +
      '\\[\\nabla L\\cdot\\nabla l_{m}=\\nabla L\\cdot\\nabla l_{n}=-\\dot{\\lambda}(t)=\\mathrm{Const}, \\tag{20}\\.\n' +
      '\n' +
      '여기서 \\(\\mathrm{Const}\\)은 " \\(m\\)과 \\(n\\)에 독립적인 상수"를 의미한다. 수학식 20은 수학식 5와 본질적으로 동일하다.\n' +
      '\n' +
      '\\[\\begin{split}\\text{Proving \\text{Const}=-\\frac{\\mathrm{d}L^{\\text{dsf}}(t}{ \\mathrm{d}t}.\\end{split}\\\n' +
      '\n' +
      '수학식 16에서 \\(\\nabla L\\cdot\\nabla l_{n}\\)을 \\(\\mathrm{Const}\\)으로 대체함으로써, 우리는 다음과 같이 얻는다.\n' +
      '\n' +
      '{split}L^{\\text{dsf}(t}{\\mathrm{d}L^{\\text{dsf}(t}{\\mathrm{d}\\gamma_{n}(t}{\\mathrm{d}\\gamma_{n}(t}{\\mathrm{d}\\gamma_{n}(t),\\\\&=-\\mathrm{Const}\\sum_{n}\\gamma_{n}(t),\\\\&=-\\mathrm{Const}\\end{split}\\tag{21}\\t}\n' +
      '\n' +
      '이와 같이 식 20과 식 21을 결합하여 정리 3.1의 증명을 완성한다.\n' +
      '\n' +
      '## 부록 C 학습정책 최적화 세부사항\n' +
      '\n' +
      '4.1절에서는 [1]에 의한 최적 학습 정책을 탐색한다. 구체적으로, 우리는 \\(0\\leq t\\leq T\\)의 전체 학습 과정을 \\(\\mathbf{\\gamma}=[\\mathbf{\\gamma}_{0},\\cdots,\\mathbf{\\gamma}_{t-1}]\\in\\mathbb{R}^{N\\times T}\\으로 매개변수화된 \\(T\\) 레이어를 갖는 신경망으로 본다. 도 12에 예시된 바와 같이, 네트워크의 각 계층은 그래디언트 업데이트 함수 및 잔여 연결[11]로 구성되며, 여기서 "은닉 상태들"은 \\(\\mathbf{\\theta}_{t}\\)이다. 그런 다음, 수학식 6에서 \\(\\nabla_{\\mathbf{\\gamma}_{t}J(\\mathbf{\\gamma})\\)를 계산하기 위해 Backward Propagation(BP; [14])을 채택한다. 각 계층에서의 역방향 연산은 다음과 같다.\n' +
      '\n' +
      '\\mathbf{H}^{\\text{lm}(\\mathbf{\\theta}_t^{t}}(\\mathbf{\\theta}_t^{t+1}})\\frac{\\text{lm}(\\mathbf{\\theta}_t^{t+1}}(\\mathbf{\\theta}_t}}(\\mathbf{\\theta}_t^{t}})}은 \\nabla l(x_{lm}(\\mathbf{\\theta}_t+1})의 Hessain 행렬이고,\\end{bf{H}^{\\text{lm}(\\mathbf{\\theta}_t+1})은 \\nabla l(x_{lm}(\\mathbf{\\theta}_t+1})의 Hessain 행렬이다. 효율성을 위해 PyTorch[19]에서 동적 프로그래밍과 Jacobian-Vector-Product7로 BP 연산을 구현한다. 또한 단일 장치 GPU 메모리 사용을 줄이기 위해 ZeRO-2[17]에서 영감을 얻은 활성화 파티션 알고리즘을 구현하였으며, 여기서 한 모델의 "은닉 상태"\\(\\mathbf{\\theta}_{t}\\)는 서로 다른 GPU 장치에 저장된다.\n' +
      '\n' +
      '각주 7: [https://pytorch.org/docs/stable/func.api.html](https://pytorch.org/docs/stable/func.api.html)\n' +
      '\n' +
      '## 부록 D 하이퍼-파라미터 구성\n' +
      '\n' +
      'Perceptron Linear Classification.4.2절에 기술된 교사 설정 후, 우리는 \\(D=128\\)을 사용하고 \\(\\mathbf{T}\\sim\\mathcal{N}(\\mathbf{0},\\sqrt{D}\\mathbf{I})\\의 가우시안 분포에서 \\(\\mathbf{T}\\sim\\mathcal{N}(\\mathbf{0},\\sqrt{D}\\mathbf{I})를 무작위로 추출한다. 우리는 \\(N=4096\\)의 학습입력 \\(\\mathbf{z}^{\\text{tm}\\)을 \\(\\mathbf{0},3\\mathbf{I})\\으로부터 생성하고 \\(M=512\\)의 목표입력 \\(\\mathbf{z}^{\\text{dsr}\\)을 \\(\\mathcal{N}(0.51,\\mathbf{I})\\으로부터 생성하며, 여기서 \\(\\mathbf{1}=[1,1,\\cdots,1]^{\\top}}R}^{D}\\)의 학습입력 \\(\\mathbf{z}^{\\text{tm}\\)을 생성한다. 각 학습 정책에 대해 \\(\\gamma_{n,0}=\\frac{1}{N}\\)을 초기화하고, \\(T=2000\\) 시간 단계에 대해 \\(\\eta=0.1\\)으로 퍼셉트론을 학습하여 모델이 수렴하기에 충분하다. 학습 정책 최적화를 위해 학습 정책을 상수 정책 \\(\\gamma_{n,t}^{c}=\\frac{1}{N}\\), 설정 \\(\\epsilon=5\\times 10^{-6}\\)으로 초기화하고 500 에폭 동안 네트워크를 훈련한다.\n' +
      '\n' +
      '트랜스포머 언어 모델링.128개의 숨겨진 차원과 8개의 주의 헤드를 가진 2층 트랜스포머를 기반으로 실험을 수행한다. 표 2를 제외한 모든 실험에서, TinyStories [18] corpus8에서 max sequence length 64로 \\(x_{n}^{\\text{tm}\\)으로 \\(N\\) = 16,384개의 예제와 \\(x_{k}^{\\text{dsr}}\\)으로 \\(K\\) = 512개의 예제를 무작위로 샘플링하고, Mistral [19]의 BPE tokenizer를 사용하여 빈발 토큰들을 [UNK]에 매핑하여 5K 토큰으로 어휘를 구성한다. 모델에는 약 1.7M 매개변수가 포함되어 있습니다. 훈련과 원하는 분포의 차이를 반영하기 위해, 우리는 (1) 어휘 [18]에서 하나의 토큰을 랜덤 토큰으로 교체하는 단계; (2) 마지막 토큰 [17]을 삭제하는 단계; (3) 문장 [20]에서 하나의 토큰을 반복하는 단계; (3) 다음 동작들 중 하나를 20회 반복 적용하여 50% 훈련 문장들에 섭동을 추가한다. 이는 실제 모델 일반화 성능을 평가하기 위해 대규모 사전 훈련 말뭉치가 원하는 집합(엄밀히 선별된 보유 말뭉치 또는 고품질 다운스트림 데이터)보다 더 시끄러운 경향이 있다는 사실에 해당한다. 학습정책별로 \\(\\eta=0.1\\), \\(T=4,000\\), \\(\\gamma_{n,0}=\\frac{1}{N}\\)을 설정하였다. 우리는 상수 정책에서 시작하여 \\(\\eta=0.1,0.2,0.4\\)을 사용하여 15개 에폭에 대한 학습 정책을 최적화한다. \\ (\\eta=0.4\\) 훈련 종료 시 가장 낮은 손실률을 보였다. 따라서 그림 4(b)와 그림 5(b)에서 \\(\\eta=0.4\\)에 대한 최적화 과정만을 도식화한다. 표 2의 실험을 위해 총 학습 단계와 해당 학습 데이터 크기를 변경하고 동시에 다른 데이터 크기에 적응하도록 어휘 크기를 변경한다. 어휘 크기 4K, 4.5K, 5K, 6K를 각각 \\(N=2^{12},2^{13},2^{14}\\) 및 \\(2^{15}\\)에 사용한다.\n' +
      '\n' +
      '각주 8: [https://huggingface.co/datasets/roneneldan/TinyStories/tree/main](https://huggingface.co/datasets/roneneldan/TinyStories/tree/main)\n' +
      '\n' +
      '그림 12: 최적 학습 정책을 찾기 위한 등가 신경망의 아키텍처, 각 레이어는 그래디언트 업데이트와 잔여 연결로 구성된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>