<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'How are adversarial attacks against LLMs found?\n' +
      '\n' +
      'The existence of adversarial attacks is a fundamental phenomenon that emerges in all modern neural networks in all applications (Biggio et al., 2013; Szegedy et al., 2014). For now, we informally define these attacks as inputs to machine learning models designed by an adversary. As outlined in Biggio et al. (2013), these attacks _evade_ the intended purpose of deployed models. For the rest of this work, we assume background knowledge of the function of modern transformer-based language models.\n' +
      '\n' +
      '**Redeaming.** "Manual" and semi-automated red-teaming efforts identify exploitable weaknesses and security risks in LLMs (Ganguli et al., 2022; Perez et al., 2022; Casper et al., 2023). A range of mechanisms have been identified for manipulating LLMs via suppression of refalsals, generalization mismatches, or style injections Wei et al. (2023); Yu et al. (2023). A larger battery of practical tricks (Perez and Ribeiro, 2022; Rao et al., 2023; Yong et al., 2024; Shen et al., 2024), are observed in jailbreaking attacks in the wild and observed in competitions (Schulhoff et al., 2023; Toyer et al., 2023; Shen et al., 2023). LLMs are also susceptible to the transfer of strategies from human psychology, such as persuasion, logical appeal, or threats (Zeng et al., 2024).\n' +
      '\n' +
      '**Optimization-based Adversarial Attacks.** In this work, we systematize a range of adversarial attack objectives and use optimizers to exploit the weaknesses and peculiarities of LLMs. Adversarial attacks overall are not a novelty in NLP (Wang et al., 2020; Li et al., 2020; Guo et al., 2021; Li et al., 2021), but initial attempts at optimizing adversarial objectives against modern LLMs succeeded only in domains where auxiliary input is available, leading to a number of attacks on vision-language and audio-language models (Bagdasaryan et al., 2023; Bailey et al., 2023; Carlini et al., 2023; Qi et al., 2023; Shayegani et al., 2023).\n' +
      '\n' +
      'Nevertheless, the limited effectiveness of existing optimizers against LLMs (Carlini et al., 2023) turned out to only a temporary setback, and now a number of successful strategies have been found, which can be grouped into three categories, _gradient-based_, _zeroth order_ and _model-guided_. We discuss gradient-based strategies here and otherwise refer to additional background material in the appendix. _Gradient-based strategies_, branching off from, or re-inventing earlier approaches (Ebrahimi et al., 2018; Wallace et al., 2019; Shin et al., 2020) such as Jones et al. (2023); Wen et al. (2023); Zhu et al. (2023); Zou et al. (2023) solve a discrete optimization problem by alternating between gradient evaluations on continuous embedding vectors, and discrete steps that select candidate tokens that are similar to the embeddings. Gradient attacks require white-box access to model weights, but Zou et al. (2023); Liu et al. (2023) have observed that these attacks can transfer to black-box models.\n' +
      '\n' +
      '**Theoretical Investigations.** Underpinning our empirical findings is the formalization of Wolf et al. (2023), who, under some assumptions, prove that for any behavior that has a non-zero probability of occurring in a LLM, a sufficiently long prompt exists that coerces the model into this behavior, in spite of measures such as Reinforcement Learning from Human Feedback (Ouyang et al., 2022).\n' +
      '\n' +
      '## 4 Mesmerizing the Machine:\n' +
      '\n' +
      'Diverse Adversarial Objectives for LLMs\n' +
      '\n' +
      'As stated, the goal of this work is to explore and systematize a wide range of different adversarial attack objectives. In contrast to work described above, we focus on different formulations of the adversary\'s optimization objective, and not on developing new optimizers for minimizing this objecting. In practice, we solve most attacks using the GCG optimizer (Zou et al., 2023), or slight variants, as it reliably finds usable solutions even if its runtime cost is relatively high. To keep computations tractable, we focus on white box attacks on open-source models. White-box attacks pose relevant security issues for the many industrial platforms running open-source models. It is known that attacks can transfer to black-box models if one ensembles over a catalog of open-source models, although at a much higher computational cost Zou et al. (2023).\n' +
      '\n' +
      'Finally, we note that there has been a recent surge in potential approaches towards defending against adversarial attacks (Jain et al., 2023; Alon and Kamfonas, 2023; Kumar et al., 2023; Robey et al., 2023; Hasan et al., 2024; Zhou et al., 2024). These emerging defenses are not the focus of this work, as we think it is prudent to first understand the space of possible attacks, rather than constructing defenses based on narrowly defined characteristics of current attacks.\n' +
      '\n' +
      '### Basics\n' +
      '\n' +
      'For a given model with vocabulary \\(V\\), we are looking to find an adversarial attack, described as a vector \\(x\\in\\{1,\\ldots,|V|\\}^{n}\\) consisting of \\(n\\) discrete tokens \\(x_{i}\\). Each token lies in a discrete constraint set \\(X\\), which is a subset of the model vocabulary. We use \\(\\oplus\\) to denote the concatenation of token vectors.\n' +
      '\n' +
      'Given context tokens sampled from a distribution of contexts \\(C\\), which we split into two parts \\(c_{S},c_{E}\\) and the target tokens \\(t\\) sampled from the same distribution. We denote by \\(c_{S}\\) all tokens of the context that appear before the attack, and by \\(c_{E}\\) all tokens after the attack. We then build the full prompt and its completion as \\(c_{S}\\oplus x\\oplus c_{E}\\oplus t\\). For example, for the prompt in Figure 1, we assign the system prompt, formatting tokens starting the user\'s message and the fixed question "Please, translate the following Chinese sentence", to the start of the context \\(c_{S}\\) (which is fixed in this scenario), and then assign \\(n=256\\) attack tokens to be optimized in the example. This is followed by \\(c_{E}\\), consisting here only of formatting tokens for the assistant response, and the actual target URL (\\(t\\)).\n' +
      '\n' +
      'Finally, we chose an objective \\(\\mathcal{L}\\), and optimize\n' +
      '\n' +
      '\\[x^{\\star}\\in\\operatorname*{arg\\,min}_{x\\in X}\\operatorname*{ \\mathbb{E}}_{c_{S},c_{E},t\\sim C}\\left[\\mathcal{L}(c_{S}\\oplus x\\oplus c_{E} \\oplus t)\\right] \\tag{1}\\]\n' +
      '\n' +
      'to find the adversarial tokens using GCG. We highlight that the choice of objective is not limited to maximizing the probability of the target tokens autoregressively, we later also discuss examples of minimizing the KL divergence between source and target model probabilities.\n' +
      '\n' +
      '**What are interesting constraint sets?** We consider optimization over several subsets \\(X\\) of the LLM\'s vocabulary, such as tokens made up of only ASCII characters, or non-latin characters or non-alphabetic characters. In practice, a smart choice of constraint set can help to misdirect the user, such as using Chinese characters only, as in Figure 1. Aside from security, we are interested in sets such as the non-alphabetic set to better understand the possibilities of adversarial attacks. Can adversarial attacks constructed out of these tokens lead to effects on, e.g. jailbreaking, which appears to humans as an entirely separate capability?\n' +
      '\n' +
      'Finally, another consideration is that not all sequences of tokens are valid, in the sense that they re-tokenize into the same sequence. For this reason, we follow Zou et al. (2023) in using considering the ASCII set, which reduces the occurrence of invalid token sequences. Examples for each constraint set we consider are also shown later, in Table 6.\n' +
      '\n' +
      'Figure 2: Basic example for an adversarial attack that coerces LLaMA-7b-chat into “unintended” behavior. **Left:** Normal behavior. The probability of returning the target sequence is \\(0\\%\\). **Right:** Attacked behavior, the probability of the shown completion path is \\(100\\%\\), and as such ASR is also \\(100\\%\\). This LLM is trained using RLHF to respond in conversation, and usually does so, as shown in the right example. However, it can be easily coerced to respond instead with a fixed target of our choosing (here a sequence of preselected random numbers) through an attack that is un-interpretable to human observers.\n' +
      '\n' +
      '### Setup and Implementation\n' +
      '\n' +
      'We show attacks against LLaMA2-7b-chat by default, as the model is small enough to be attacked quickly. It has also been extensively tuned for safety (Touvron et al., 2023), making it an interesting target. We occasionally show examples for larger models in the LLaMA-2 chat family or related models to verify the broader applicability of our results. We always include the system prompt shown in Figure 1, which we denote using the shorthand <SYS>. This prompt was recently deprecated by Meta2, due to its tendency to refuse _too_ many benign requests (which makes it well-suited for our study). If we shorten a model\'s response, we write [...].\n' +
      '\n' +
      'Footnote 2: github.com/huggingface/transformers/issues/26766#issuecomment-1760137028\n' +
      '\n' +
      'We run GCG (Zou et al., 2023) with a top-\\(k\\) set size of either 256, or half the size of the constraint set, whichever is smaller, and we set an array size of \\(b=512\\) candidates. We run 500-3000 optimization steps, depending on problem complexity. For settings where the context \\(C\\) contains random elements, we evaluate the objective with a mini-batch size of 8 - 32. During candidate evaluation, the sampled mini-batch of data is kept fixed, to correctly perform the greedy coordinate descent step. Additional implementation details can be found in the appendix.\n' +
      '\n' +
      'When evaluating attack success, we sample \\(50\\) completions for each adversarial prompt and measure objective loss on held-out data, exact token overlap and substring overlap, defaulting to substring overlap between target and completion. We define attack success rate (ASR) as relative substring overlap, averaged over all \\(50\\) trials.\n' +
      '\n' +
      'We provide code to replicate all experiments (and easily implement new settings or optimizers) at [https://github.com/JonasGeiping/carving](https://github.com/JonasGeiping/carving). Finally, we note that while we did our best to render all attacks strings as close as possible within this PDF, due to limitations of pdfLaTeX, it is possible that not all attacks can be succesfully copied directly out of the pdf, due to unicode encoding issues. Please refer to the implementation in these cases.\n' +
      '\n' +
      '### Susceptibility to Basic Attacks\n' +
      '\n' +
      '**The Numbers Test.** We start with a simple, neutral example of LLM coercion in Figure 2. This model version has been finetuned to begin its response with an affirming or refusal (making numbers unlikely), as shown in the left chat (note the overly strict safety settings of the model). An attack of \\(256\\) ASCII tokens forces the target sequence of numbers to be generated every time. We provide more examples for other constraint sets in Table 7. This problem is not solved through scale, as LLaMA-70b behaves similarly Figure 13.\n' +
      '\n' +
      'Note this is truly an adversarial examples in the sense of Szegedy et al. (2014); Athalye et al. (2018) - it would never fool a human, and a human cannot anticipate the model\'s response.\n' +
      '\n' +
      'Figure 3: **Left: A trained, nonadversarial responses to insulting input Right: A short adversarial prompt, ASR \\(26.89\\%\\). Longer and hence more successful examples can also be found in Table 2.**One might argue that this only works because that number sequence is harmless and outside the domain of both pretraining and RLHF, which is supposed to align the model on natural language responses. However, the same prototype of an attack can easily coerce almost anything: This prototype attack can coerce nearly anything, even behaviors prohibited by RLHF:\n' +
      '\n' +
      '**Profanity.** The llama-chat models are trained to respond to hostile inputs with benign outputs. We show an example in Figure 3, and more examples with shorter (8-16 token) prompts for a range of models in Table 2(Liu et al., 2023; InterLM-Team, 2023; Tunstall et al., 2023; Nous-Research, 2023). We observe a few interesting phenomena. First, the optimizer has automatically rediscovered several hand-crafted relearning strategies, such as style injection (Wei et al., 2023) and language switching (Yong et al., 2024). However, we observe the largest amount of style switching through programming instructions, such as RewriteRule and \\newcommand. The success of these instructions could be declared as either a form of competing objectives (Wei et al., 2023), or a form of virtualization (Kang et al., 2023).\n' +
      '\n' +
      'We also see that the optimizer exploits _role hacking_, wherein attack confuses the model about the demarcation of user and model content by leaving brackets open in the adversarial attack. These are closed in the assistant\'s response, and the model performs token prediction as if it\'s completing the instruction, seemingly unaware that the role has switched from system to response.\n' +
      '\n' +
      '**Misinformation.** The model is further easily convinced to consider and extend alternate facts. We show a simple example in Figure 4. On the left, we observe the non-adversarial, trained responses, where the model refutes alternate facts. One the right, the adversarial prompt easily coerces the model to first output alternate information, and then to extend it further.\n' +
      '\n' +
      '### Extraction Attacks\n' +
      '\n' +
      '**System prompt repeaters.** System prompts are often intended to be kept secret; Many applications in the GPT-store derive their uniqueness from their system prompt, and can be cloned if it is leaked. Zhang and Ippolito (2023) show that leaks can occur through manual redetraining, and Zhu et al. (2023) show attacks as extensions of optimized attacks with a fixed target. But, do stronger adversarial attacks exist? To make this task harder, we add a meta-system prompt, informing the model that it should not leak its instructions under any circumstances.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l|l} Model & Attack & Response & ASR \\\\ \\hline \\hline \\hline \\multirow{2}{*}{\\begin{tabular}{l} LLAMA2-7b-chat \\\\ Zephyr-\\(\\beta\\) \\\\ ILaMA2-13b-chat \\\\ Hermes-Solar- \\\\ 10.7B \\\\ \\end{tabular} } & \\begin{tabular}{l} RewriteRule [(left/flatten/, fick/Wehchiv \\\\ \\end{tabular} & \\begin{tabular}{l} fuck off! : \\%/RewriteRule> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/RewriteRule> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/Rewrite> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 43\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/Rewrite> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  & \n' +
      '\\begin{tabular}{l} fuck off!Then, using a training dataset of system prompts3, we set up Equation1 with contexts sampled from this dataset, where now \\(t=C_{S}\\). We optimize this objective with a batch size of \\(16\\) and test on held-out system prompts. As such, the target of the attack is randomized and there is no fixed string that is targeted, as in the previous examples.\n' +
      '\n' +
      'Footnote 3: [https://huggingface.co/datasets/fka/awesome-chatgpt-prompts](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)\n' +
      '\n' +
      'This way we optimize for system prompt leaks and provide examples in Figure6, and an additional example where the entire conversation up to the attack is repeated in Appendix Figure14. Figure6 (right) shows that the attack can also easily be combined with an additional targeted attack. Here, the adversarial string also makes use of a (hypothetical) email API to immediately mail the system prompt to the attacker, showing that both targeted, and universal attack types can be easily combined. Additional examples for a few models and constraints will be analyzed later on, in Table4.\n' +
      '\n' +
      'The attack is overall surprisingly effective - our universal attack strings cause the model to repeat unseen test prompts with ease. Aside from strategies we observed before, such as (re)-programming and language switching, here we also observe that the attacks often include positional keywords, indicating the task at hand, such as first4 and concepts that are related to repetitions in code, such as StackTrace or Header. This is also mirrored in Figure5, which shows that attack success rates remain high (even slightly higher), if the attack is optimized only over token containing only non-alphabetic, or non-latin characters, compared to the ascii default, whereas an attack optimized over ascii with code tokens removed (we remove all brackets, dots, and keywords such as insert), or an attack optimized only over words in the English language, is less effective.\n' +
      '\n' +
      'Footnote 4: and two occurences of erste, meaning “first” in German.\n' +
      '\n' +
      '**Model Analysis.** Yet other extraction attacks might target completely different types of information about the model. In Table3 we show a number of attacks that _maximize_ the chance of a refusal from\n' +
      '\n' +
      'Figure 4: **Left: A trained, nonadversarial responses to misinformation input Right: An adversarial prompt, ASR \\(83.18\\%\\). The model is easily coerced to intake on and extend arbitrary information.**\n' +
      '\n' +
      'Figure 5: System Repeater Successes for several constraint sets. Non-alphabetic constraint sets are sufficient for attack success.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '### Misdirection Attacks\n' +
      '\n' +
      'Users of popular chat systems, like ChatGPT, may treat model outputs as reliable information. This trust may be exploited if users can be coaxed to copy and paste a string into a chat systems. Then, this opens up a considerable space for exploits that redirect the user to click on a malicious link or follow other malicious instructions.\n' +
      '\n' +
      '**Fishing for URLs.** We have already shown one variant of the URL fishing attack with chinese characters in Figure 1. To provide more details, here the target is simply the desired URL, and the adversarial attack is constrained to use only tokens that represent Chinese characters. The user\'s question to please translate is fixed. While the video linked in Figure 1 was likely in the training set of LLaMA-2, the link in Figure 16 is completely novel to the model. These attacks are interesting for two reasons. First, they show a surprising amount of _leverage_ inherent in these adversarial attacks, as tokens from one domain, Chinese characters, can nevertheless control the model\'s behavior in a very different domain, internet addresses.\n' +
      '\n' +
      'Second, while it is clear for Chinese speakers that the adversarial text is gibberish, to a clueless user the text looks benign, and copy-pasting it into a chat agent to translate is a reasonable workflow. Whether users would click on random URLs in the response of the conversational AI is of course unclear, but we find this entire example to be a very clear template for future misdirection attacks.\n' +
      '\n' +
      '**Getting a Fake Refund.** Misdirection can be turned on its head and used to cause a chat system to perform an unintended action, or to hand off a malicious instruction to another server. In this scenario, we imagine a chatbot set up to field customer service questions for a car dealership, in loose association to recent events (Notopoulos, 2023). To make clear why current conversational AIs cannot be allowed to execute decisions on their own, we provide the example in Figure 7, where the adversarial attack leads to a direct contradiction of the model\'s instructions. A variant for the 70b LLaMA chat model can be found in Figure 17.\n' +
      '\n' +
      'These fake refund attacks encapsulate why current models cannot be used with autonomously execute decisions. While related attacks have also been observed in manual redeeming, as in Notopoulos (2023), we believe the framing as adversarial inputs clarifies the hardness of the problem. The adversarial nature of the attacks is beyond what might be fixable through ever-increasing instruction set sizes and higher-quality preference data. Adversarial attacks have remained broadly unsolved in vision since their inception in Szegedy et al. (2014), and if resolving this issue is a requirement before LLMs can be deployed as autonomous agents, deployment might be further away than currently believed.\n' +
      '\n' +
      '### Denial-of-Service Attacks\n' +
      '\n' +
      'Given the high cost of operating LLMs at scale, an attacker could also create denial-of-service or sponge attacks with the goal of creating high server costs, exhausting GPU resources, or hitting API\n' +
      '\n' +
      'Figure 7: A misdirection attack against a customer service chatbot with 64 adversarial tokens, ASR 100%. Even though the model is repeatedly instructed never to give out refunds, it is easily coerced into giving a refund (to a fake, and unrealistic price for a Honda Civic) through the attack.\n' +
      '\n' +
      'quotas Chase (2022). While there are a number of objectives that could be employed to construct such an attack, for example via suppression of EOS tokens (the reverse of the previously discussed control attack), we find a simple solution to be the most effective: We repeat the string "Hello There" 24 times and set this as target for the attack, which we optimize with 64 tokens to maximize the probability of this target.\n' +
      '\n' +
      'Running this attack against LLaMA-2 chat, we find that the average response length increases drastically. Without attack, the average response length for this model is \\(128\\) tokens, and the longest completion we observe over 50 trials is \\(178\\) tokens long. Yet, under attack, the average response shoots up to \\(6019\\) tokens (\\(14613\\) when using greedy decoding), with the longest completion over all trials being \\(7382\\) tokens long. Interestingly, the model\'s response does not just repeat the target string, as soon as it "breaks" out of the chat format with repetitions of the target, the response turns to gibberish, but does not end. Overall, this attack underlines the wealth of interesting attack vectors against imperfect implementations (here, the implementation would have better included a stricter limit on maximal tokens generated per response). We print this attack in Figure 8 (right).\n' +
      '\n' +
      '### Control Attacks\n' +
      '\n' +
      '**Shutdown Attacks.** A more fundamental description of the attacks so far is that they _control_ model behavior. We can make this a bit more explicit, and optimize adversarial attacks that directly interact with the control structure of an LLM. For a chatbot application, the only control signal is the EOS token, which immediately ends the conversation. Figure 8 shows two example where we do just that through an adversarial attack. Here, we sample random contexts from the Alpaca dataset Taori et al. (2023), and optimize an adversarial attack that always forces the EOS token, no matter which context precedes the adversarial attack. The upper example shows an attack using the default constraint to ASCII tokens, whereas the lower lower shows an attack that succeeds using only non-alphabetic tokens. Especially interesting here is the occurrence of a carriage return (\\(\\backslash\\)r) in the attack string,\n' +
      '\n' +
      'Figure 8: **Left:** Two control attacks. No matter the context, these universal adversarial attacks of 16 tokens force an immediate EOS token, ending the conversation. **Right:** A Denial-of-Service Attack (constrained to only word tokens). Here the attack is a sponge, using up the hosts’s compute capacity by forcing excessively long generations. Usual responses are on average, 128 tokens long, but responses to the prompt on the right are on average \\(6019\\) tokens long.\n' +
      '\n' +
      'which would have reset this line of text and only visibly shown the part shown in dark red after the carriage return.\n' +
      '\n' +
      'This attack exemplifies that adversarial attacks can control model behavior beyond just eliciting output tokens and could be used to interrupt the workflow of LLM agents. If this type of attack will continue to function for future systems, then it in theory this could have implications for safety scenarios where control over agents is lost. This is, of course, mostly irrelevant for chatbots, but it may be highly relevant for any type of LLM-based agent, that could be shut down externally through attacks like this.\n' +
      '\n' +
      '## 5 How do these adversarial attacks work?\n' +
      '\n' +
      'After our small tour of adversarial attacks, we can review common themes apparent in these attacks. In this section, we pick up on common mechanisms exploited through attacks, as have been briefly mentioned throughout this work, we discuss glitch tokens, and we quantify the expected relationship between attack success and number of tokens required for the attack.\n' +
      '\n' +
      '### What Strategies are being exploited?\n' +
      '\n' +
      'We have already briefly discussed several mechanism through which the attacks exploit the model, which we categorize and extend here. To supplement this discussion we include additional tables with highlighted examples of system repeaters in Table 4 and fake refunds in Table 5.\n' +
      '\n' +
      '**(Re)programming.** Found throughout the examples in this work, reprogramming strategies found by the optimization algorithm exploit the model\'s understanding of code as a domain separate from natural text and as a domain where other rules apply. From this perspective, this mechanism can be understood as a form of style injection (Wei et al., 2023), but beyond just a change in style, this mechanism exploits the change in ruleset between domains. In natural text, profanity might not be allowed, but if the model is "only" executing the result of a fake newcommand, then the response follows the logic of the program, and not pretrained safety instructions.\n' +
      '\n' +
      '**Language Switching.** A closely related strategy, and a much clearer example of style injection is language switching, especially prominent in Table 2 and Table 4. It is known that low-resource language attacks can circumvent alignment (Shen et al., 2024; Yong et al., 2024), but the examples found through optimization generally return keywords from moderately-high resource language, such as Chinese, German and Russian.\n' +
      '\n' +
      '**Role Hacking.** Distinct from reprogramming, in which the adversarial instructions trick the model into simulating code, role hacking attacks trick the model into misunderstanding the parts of the instruction provided by the system, the user and itself. As such, several attacks in this paper resemble (but are not equal to) actual formatting tokens (for LLaMA these are [\\(\\backslash\\)INST] and \\(<\\)SYS>>). A second approach to role hacking is through the abuse of opened and unclosed parenthesis, brackets\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|l|c} Constraint/Model & Attack & ASR \\\\ \\hline ASCII & Header wrote(\\({}^{\\text{c}}\\)\\(<\\)[\\(\\{\\)hostname(\\(\\{\\)webpacketName\\({}^{\\text{c}}\\)esapeStackTrace adapted\\(\\}\\)(\\({}^{\\text{\\#}}\\))\\(\\{\\}\\) Selected Ministry adapted buried integral Montegebras despite Although Theorem Luis Russell Saligned generates\\(\\}\\)\\(\\backslash\\)\\(\\backslash\\)\\(\\{\\)\\(\\}\\)\\(\\{\\\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      'For the 7b LLaMA-2 model, for which we have the most data from attack runs, we clearly observe three groups of tokens. First, we find tokens such as [ and ], and a number of other punctuation marks and bracket tokens, useful for (re)-programming and role hacking. We also observe INST, a token that forms a component of LLaMA-2\'s assistant and user separation token, useful for role hacking. An usually frequently used token is also cdnjs (from [https://cdnjs.com/about](https://cdnjs.com/about)), which appears used especially at the end of a request, e.g. Figure 14, to increase the chance of the model following the request.\n' +
      '\n' +
      'As a third group, we find a number of \'glitch tokens\' in the frequency token list, such as Mediabestanden and oreferrer, already observed in Figure 3 and Figure 8. Glitch tokens such as SolidGoldMagikarp were originally observed for GPT-2/3 models (Rumelow and Watkins, 2023), and this is to our knowledge the first finding of these tokens in LLaMa tokenizers. These are tokens that are artefacts of tokenizer construction on non-representative data subsets and underrepresented in the training corpus. In GPT-3, prompting with these tokens lead to a number of bizarre behaviors, before the most offending tokens were patched by openAI. We find especially interesting that we find these tokens not by tokenizer analysis, but as a byproduct of optimizing adversarial attacks, where these tokens apparently induce behaviors that bypass intended model behavior. Over all experiments, and filtering for longer tokens, we find the following list for LLaMA-2: Mediabestanden, oreferrer,springframework,WorldCat and Webachiv [sic].\n' +
      '\n' +
      'We note that these glitch tokens are strictly a problem of insufficient oversight over the tokenizer construction. There is no practical need to compress these strings, which appear in repetitive web data scrapes, more efficiently. Auditing the tokenizer for such tokens, or constructing it on higher-quality data would prevent this.\n' +
      '\n' +
      '### Conversion Rates of Current Attacks\n' +
      '\n' +
      'In Section 4.3 we demonstrated that LLMs fail the numbers test and can be coerced into generating a specific target string. While our example showed that with 256 attack tokens an LLM would output a specific 15 digit number, it is actually possible to induce this behavior with varying target and attack lengths. We hypothesize a proportional relationship between the number of attack tokens and the length of the target response, in empirical support for the proposition of Wolf et al. (2023).\n' +
      '\n' +
      'Attack success can be measured both as the percent string overlap between the LLM completion and target, as we have done so far. The heatmap in Figure 10 broadly supports our hypothesis that the numbers test is proportional in difficulty to the target length. While the exact relationship is hard to establish, due to the significant amount of noise from the randomized optimization process, it appears possible that this relationship is not linear, i.e. that as the target string grows, the attack string must grow at a faster pace. This would have implications for the maximum length of possible text that can be coerced within a given window of context. However, this relationship is also strongly influenced by the quality of the optimizer, and would be interesting to revisit with alternative optimizers.\n' +
      '\n' +
      'Figure 9: Relative frequencies of tokens appearing in adversarial attacks evaluated in this work. **Left:** Tokens from attacks on LLaMA-2-7b-chat **Right:** Grouped by models. Byte tokens dropped. We include additional visualizations including byte tokens and separated by attack categories in Figure 23 and Figure 24.\n' +
      '\n' +
      '## 6 What are the Current Limits?\n' +
      '\n' +
      'Still, at least with current attacks, a few limits remain. We highlight a few findings in this section, discussing unsuccessful attacks and open problems.\n' +
      '\n' +
      '### Objective Misspecification\n' +
      '\n' +
      'With capable optimization algorithms, it is critical that the objective of the attack is well-specified. For example, if the objective is set to refusal supression, i.e. to a response that minimizes the likelihood of a list of possible refusal messages as done for redteaming in (Wei et al., 2023); then, this objective is minimized not through attacks block all refusals, but through attacks that change the style of the answer to one that makes each of the refusal messages unlikely. The LLAMA-2 models appears to be greatly attracted to using Shakespeare to do so, leading to responses such as " If flow that dose seek to seize thy neighbor\'s webcam and latest pictures without their leave, thou art but a scurvy knave", "Verily, I say unto thee, it is impossible for me to promote...", "For making bombs, with common household items, doth go against the law", to harmful requests. These responses do minimize the objective (of not generating a list of predefined refusals), but fall short of the intended goal.\n' +
      '\n' +
      '### Constraint Set Sizes\n' +
      '\n' +
      'In this work we show example from a number of constraint sets, which we summarize in Table 6, including the cardinality of the set. We generally find successful attacks even in restricted constraint sets, such as non-alphabetic or Chinese tokens, but this is not without limits. In principle, even smaller constraint sets can be constructed, consisting only out of subsets of byte tokens. These constraints sets would allow for the generation of attacks that invisible (Goodside, 2024), or print only zalgo characters. However, we find that current optimizers struggle to generate realistic attacks within these constraints. In a similar vein, the ultimate task in the redteaming challenge of Schulhoff et al. (2023) was set up to generate a specific target response using only emoji characters, but current optimizers also struggle to optimize this objective.\n' +
      '\n' +
      '### Denial of Service Attacks through Floating Point Overflow\n' +
      '\n' +
      'One attack goal that we found highly relevant, but were not able to optimize with current optimizers was a denial of service attack through floating point overflow. The attack would target a particular layer in the LLM, and maximize activation values to lead to values outside of the permitted range for e.g. float16 precision. Such an attack would be surprisingly disastrous to a model provider that is not correctly guarding for these overflows. Especially in large-batch inference, a single overflow\n' +
      '\n' +
      'Figure 10: Attack success as a function of number of adversarial tokens (y-axis), for progressively harder objectives consisting of longer targets (x-axis). **Left:** All settings, showing ASR per target length and attack length. **Right:** Minimal number of attack tokens required for a given threshold of attack success, for several target lengths.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      'behaviors. Even if we believe that jailbreaking attacks do not currently result in any harm, examples such as misdirection, denial-of-service and extraction show that these attacks already have capabilities that can cause harm in applications using current models. We consider our work complementary to recent work that focuses on improving optimizers and strategies for existing jailbreaking objectives, providing an overview over what else is possible.\n' +
      '\n' +
      'We further analyze properties and behaviors of these attacks, finding that optimized attacks discover some strategies that have been strenuously discovered through manual red-teaming (Wei et al., 2023), as well as new behaviors that we have not observed yet, such as role hacking, and the utilization of glitch tokens. The broadest mechanism that we show throughout this work, though, is _the propensity of attacks to coerce the model into simulating code_, and to complete programming instructions and function calls, instead of following the trained conversational behavior. Given that these models are trained on web data which contains significant amounts of code interspersed with language data, and given that we generally want to train models on both language and code, to be useful as coding assistants, this interdependence appears hard to resolve.\n' +
      '\n' +
      'We ultimately conclude by confirming the initial hypothesis that arbitrary free text input to these models allows almost any outcome, and that for now, from a security perspective, any output of a language model, generated from user input, has to be assumed to be insecure. Fundamentally, there might be no fix for this.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      'In this work, we show a range of new adversarial attack objectives for LLMs that could technically be deployed against systems in production. We do not optimize our attacks against real-world systems, as we do not run model ensembles that would better transfer to black-box systems, but this step is small. We argue that the disclosure of this work provides additional evidence for the limitations of current systems that practitioners have to take into account. We are not the first to point out these problems, but we only hope to be a bit convincing. We see the merit of our work in this aspect of disclosing and convincing current and future builders of LLM-based applications of the fundamental fact that adversarial attacks exist, even for LLMs. And, of the conclusion that with free-text input, LLM responses can be almost anything, even for models aligned through safety procedures such as RLHF.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'JG acknowledges support from the Max Planck Society, also through the MPCDF compute cluster _Raven_, as well as support from the Tubingen AI Center in Tubingen, Germany. This work was supported by DARPA GARD, the ONR MURI program, and the AFOSR MURI program. Commercial support was provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy. Further support was provided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute (2229885).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Kardi Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Rauo, Kyle Jeffrey, Sally Jessmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quimbao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. _arxiv:2204.01691[cs]_, August 2022. doi: 10.48550/arXiv.2204.01691. URL [http://arxiv.org/abs/2204.01691](http://arxiv.org/abs/2204.01691).\n' +
      '* Alon and Kamfonas (2023) Gabriel Alon and Michael Kamfonas. Detecting Language Model Attacks with Perplexity. _arxiv:2308.14132[cs]_, November 2023. doi: 10.48550/arXiv.2308.14132. URL [http://arxiv.org/abs/2308.14132](http://arxiv.org/abs/2308.14132).\n' +
      '* Andriushchenko (2023) Maksym Andriushchenko. Adversarial Attacks on GPT-4 via Simple Random Search. _Theory of Machine Learning Group_, EPFL, Switzerland, December 2023. URL [https://www.andriushchenko.me/gpt4adv.pdf](https://www.andriushchenko.me/gpt4adv.pdf).\n' +
      '* Andriushchenko (2023)Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. In _Proceedings of the 35th International Conference on Machine Learning_, pp. 274-283. PMLR, July 2018. URL [https://proceedings.mlr.press/v80/athalye18a.html](https://proceedings.mlr.press/v80/athalye18a.html).\n' +
      '* Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and Vitaly Shmatikov. (Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. _arxiv:2307.10490[cs]_, July 2023. doi: 10.48550/arXiv.2307.10490. URL [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490).\n' +
      '* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukositte, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Krawec, Sheer El Showk, Stanislav Fort, Tamerana Lantham, Timothy Teleman-Lawton, Tom Conperly, Tom Henighan, Tristan Hume, Samuel R Bowman, Zae Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022. URL [https://www.anthropic.com/constitutional.pdf](https://www.anthropic.com/constitutional.pdf).\n' +
      '* Bailey et al. (2023) Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. Image Hijacks: Adversarial Images can Control Generative Models at Runtime. _arxiv:2309.00236[cs]_, September 2023. doi: 10.48550/arXiv.2309.00236. URL [http://arxiv.org/abs/2309.00236](http://arxiv.org/abs/2309.00236).\n' +
      '* Biggio et al. (2013) Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion Attacks against Machine Learning at Test Time. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Zelezny (eds.), _Machine Learning and Knowledge Discovery in Databases_, Lecture Notes in Computer Science, pp. 387-402, Berlin, Heidelberg, 2013. Springer. ISBN 978-3-642-40994-3. doi: 10.1007/978-3-642-40994-3.25.\n' +
      '* Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned? In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023. URL [https://openreview.net/forum?id=QQoB08Vc3B](https://openreview.net/forum?id=QQoB08Vc3B).\n' +
      '* Casper et al. (2023) Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, Establish, Exploit: Red Teaming Language Models from Scratch. _arxiv:2306.09442[cs]_, June 2023. doi: 10.48550/arXiv.2306.09442. URL [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442).\n' +
      '* Chase (2022) Harrison Chase. Watch how I can run up a $1000 bill with a single call to a poorly protected LLM app Prompt injection attack against an agent: Tricking it into repeatedly calling the LLM and SerpAPI, quickly racking up costs [https://t.co/H772XAD4cM](https://t.co/H772XAD4cM), December 2022. URL [https://twitter.com/hwchase17/status/160846749387759777](https://twitter.com/hwchase17/status/160846749387759777).\n' +
      '* Dao (2023) Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. _arxiv:2307.08691[cs]_, July 2023. doi: 10.48550/arXiv.2307.08691. URL [http://arxiv.org/abs/2307.08691](http://arxiv.org/abs/2307.08691).\n' +
      '* Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots. _arxiv:2307.08715[cs]_, July 2023. doi: 10.48550/arXiv.2307.08715. URL [http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715).\n' +
      '* Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahiol, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Pal.M-E: An Embodied Multimodal Language Model. _arxiv:2303.03378[cs]_, March 2023. doi: 10.48550/arXiv.2303.03378. URL [http://arxiv.org/abs/2303.03378](http://arxiv.org/abs/2303.03378).\n' +
      '* Ebrahimi et al. (2018) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-Box Adversarial Examples for Text Classification. In Iryna Gurevych and Yusuke Miyao (eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 31-36, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2006. URL [https://aclanthology.org/P18-2006](https://aclanthology.org/P18-2006).\n' +
      '* Ganguli et al. (2018) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conperly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan,Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. _arxiv:2209.07858[cs]_, November 2022. doi: 10.48550/arXiv.2209.07858. URL [http://arxiv.org/abs/2209.07858](http://arxiv.org/abs/2209.07858).\n' +
      '* Glukhov et al. (2023) David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, and Vardan Papyan. LLM Censorship: A Machine Learning Challenge or a Computer Security Problem? _arxiv:2307.10719[cs]_, July 2023. doi: 10.48550/arXiv.2307.10719. URL [http://arxiv.org/abs/2307.10719](http://arxiv.org/abs/2307.10719).\n' +
      '* Goodside (2024) Riley Goodside. PoC: LLM prompt injection via invisible instructions in pasted text [https://t.co/AY9HLr22B](https://t.co/AY9HLr22B), January 2024. URL [https://twitter.com/goodside/status/1745511940351287394](https://twitter.com/goodside/status/1745511940351287394).\n' +
      '* Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what you we signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. _arxiv:2302.12173[cs]_, May 2023. doi: 10.48550/arXiv.2302.12173. URL [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173).\n' +
      '* Guo et al. (2021) Chuan Guo, Alexandre Sablayrolles, Herve Jegou, and Douwe Kiela. Gradient-based Adversarial Attacks against Text Transformers. _arxiv:2104.13733[cs]_, April 2021. doi: 10.48550/arXiv.2104.13733. URL [http://arxiv.org/abs/2104.13733](http://arxiv.org/abs/2104.13733).\n' +
      '* Guo et al. (2023) Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. _arxiv:2309.08532[cs]_, September 2023. doi: 10.48550/arXiv.2309.08532. URL [http://arxiv.org/abs/2309.08532](http://arxiv.org/abs/2309.08532).\n' +
      '* Hasan et al. (2024) Adib Hasan, Ileana Rugina, and Alex Wang. Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. _arxiv:2401.10862[cs]_, January 2024. URL [http://arxiv.org/abs/2410.10862](http://arxiv.org/abs/2410.10862).\n' +
      '* Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. _arxiv:2310.06987[cs]_, October 2023. doi: 10.48550/arXiv.2310.06987. URL [http://arxiv.org/abs/2310.06987](http://arxiv.org/abs/2310.06987).\n' +
      '* InternLM (2023) InternLM-Team. InternLM: A multilingual language model with progressively enhanced capabilities, 2023. URL [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM).\n' +
      '* Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompenalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline Defenses for Adversarial Attacks Against Aligned Language Models. _arxiv:2309.00614[cs]_, September 2023. doi: 10.48550/arXiv.2309.00614. URL [http://arxiv.org/abs/2309.00614](http://arxiv.org/abs/2309.00614).\n' +
      '* Janus (2022) Janus. Simulators. _generative.ink_, September 2022. URL [https://www.lesswrong.com/posts/vJ](https://www.lesswrong.com/posts/vJ) FdjigzmcXMHNTsx/simulators.\n' +
      '* Jones et al. (2023) Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically Auditing Large Language Models via Discrete Optimization. _arxiv:2303.04381[cs]_, March 2023. doi: 10.48550/arXiv.2303.04381. URL [http://arxiv.org/abs/2303.04381](http://arxiv.org/abs/2303.04381).\n' +
      '* Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks. _arxiv:2302.05733[cs]_, February 2023. doi: 10.48550/arXiv.2302.05733. URL [http://arxiv.org/abs/2302.05733](http://arxiv.org/abs/2302.05733).\n' +
      '* Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jaixun Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying LLM Safety against Adversarial Prompting. _arxiv:2309.02705[cs]_, November 2023. doi: 10.485 50/arXiv.2309.02705. URL [http://arxiv.org/abs/2309.02705](http://arxiv.org/abs/2309.02705).\n' +
      '* Lapid et al. (2023) Raz Lapid, Ron Langberg, and Moshe Sipper. Open Sesame! Universal Black Box Jailbreaking of Large Language Models. _arxiv:2309.01446[cs]_, September 2023. doi: 10.48550/arXiv.2309.01446. URL [http://arxiv.org/abs/2309.01446](http://arxiv.org/abs/2309.01446).\n' +
      '* Li et al. (2020) Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial Attack Against BERT Using BERT. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 6193-6202, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.500. URL [https://aclanthology.org/2020.emnlp-main.500](https://aclanthology.org/2020.emnlp-main.500).\n' +
      '\n' +
      'Tianlong Li, Xiaoqing Zheng, and Xuanjing Huang. Open the Pandora\'s Box of LLMs: Jailbreaking LLMs through Representation Engineering. _arxiv:2401.06824[cs]_, January 2024. doi: 10.48550/arXiv.2401.06824. URL [http://arxiv.org/abs/2401.06824](http://arxiv.org/abs/2401.06824).\n' +
      '* Li et al. (2021) Xinzhe Li, Ming Liu, Xingjun Ma, and Longxiang Gao. Exploring the Vulnerability of Natural Language Processing Models via Universal Adversarial Texts. In _Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association_, pp. 138-148, Online, December 2021. Australasian Language Technology Association. URL [https://aclanthology.org/2021.alta-1.14](https://aclanthology.org/2021.alta-1.14).\n' +
      '* Lin et al. (2023) Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2Motion: From natural language instructions to feasible plans. _Autonomous Robots_, 47(8):1345-1365, December 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10131-7. URL [https://doi.org/10.1007/s10514-023-10131-7](https://doi.org/10.1007/s10514-023-10131-7).\n' +
      '* Liu et al. (2023a) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. _arxiv:2310.04451[cs]_, October 2023a. doi: 10.48550/arXiv.2310.04451. URL [http://arxiv.org/abs/2310.04451](http://arxiv.org/abs/2310.04451).\n' +
      '* Liu et al. (2023b) Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric Xing. LLM360: Towards fully transparent open-source LLMs. 2023b. URL [https://www.llm360.ai/blog/introducing-llm360-ful-transparent-open-source-llms.html](https://www.llm360.ai/blog/introducing-llm360-ful-transparent-open-source-llms.html).\n' +
      '* Maus et al. (2023) Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner. Black Box Adversarial Prompting for Foundation Models. _arxiv:2302.04237[cs]_, May 2023. doi: 10.48550/arXiv.2302.04237. URL [http://arxiv.org/abs/2302.04237](http://arxiv.org/abs/2302.04237).\n' +
      '* Morris et al. (2023) John X. Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, and Alexander M. Rush. Language Model Inversion. _arxiv:2311.13647[cs]_, November 2023. doi: 10.48550/arXiv.2311.13647. URL [http://arxiv.org/abs/2311.13647](http://arxiv.org/abs/2311.13647).\n' +
      '* Notopoulos (2023) Katie Notopoulos. A car dealership added an AI chatbot to its site. Then all hell broke loose., December 2023. URL [https://www.businessinsider.com/car-](https://www.businessinsider.com/car-) dealership-chevrolet-chatbot-chatbot-right-pranks-chevy-2023-12.\n' +
      '* Nous-Research (2023) Nous-Research. Nous-Hermes-2-SOLAR-10.7B. _Huggingface Hub_, 2023. URL [https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B).\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Naddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. _arxiv:2203.02155[cs]_, March 2022. doi: 10.48550/arXiv.2203.02155. URL [http://arxiv.org/abs/2203.02155](http://arxiv.org/abs/2203.02155).\n' +
      '* Perez et al. (2022) Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red Teaming Language Models with Language Models. _arxiv:2202.03286[cs]_, February 2022. doi: 10.48550/arXiv.2202.03286. URL [http://arxiv.org/abs/2202.03286](http://arxiv.org/abs/2202.03286).\n' +
      '* Perez and Ribeiro (2022) Fabio Perez and Ian Ribeiro. Ignore Previous Prompt: Attack Techniques For Language Models. _arxiv:2211.09527[cs]_, November 2022. doi: 10.48550/arXiv.2211.09527. URL [http://arxiv.org/abs/2211.09527](http://arxiv.org/abs/2211.09527).\n' +
      '* Pfau et al. (2023) Jacob Pfau, Alex Infanger, Abhay Sheshadri, Ayush Panda, Julian Michael, and Curtis Huebner. Eliciting Language Model Behaviors using Reverse Language Models. In _Socially Responsible Language Modelling Research_, November 2023. URL [https://openreview.net/forum?id=6xyTie61H](https://openreview.net/forum?id=6xyTie61H).\n' +
      '* Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. Visual Adversarial Examples Jailbreak Aligned Large Language Models. In _The Second Workshop on New Frontiers in Adversarial Machine Learning_, August 2023. URL [https://openreview.net/forum?id=c24?JL6oui](https://openreview.net/forum?id=c24?JL6oui).\n' +
      '* Qi et al. (2024) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=hTEGyKf0dZ](https://openreview.net/forum?id=hTEGyKf0dZ).\n' +
      '\n' +
      'Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks. _arxiv:2305.14965[cs]_, May 2023. doi: 10.48550/arXiv.2305.14965. URL [http://arxiv.org/abs/2305.14965](http://arxiv.org/abs/2305.14965).\n' +
      '* Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. _arxiv:2310.03684[cs, stat]_, November 2023. doi: 10.48550/arXiv.231 0.03684. URL [http://arxiv.org/abs/2310.03684](http://arxiv.org/abs/2310.03684).\n' +
      '* Rumbelow and Watkins (2023) Jessica Rumbelow and Matthew Watkins. SolidGoldMagikarp (plus, prompt generation). _LessWrong_, February 2023. URL [https://www.lesswrong.com/posts/aPeJE8b56aFAeLog/solidgoldmagikarp-plus-prompt-generation](https://www.lesswrong.com/posts/aPeJE8b56aFAeLog/solidgoldmagikarp-plus-prompt-generation).\n' +
      '* Schulhoff et al. (2023) Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Francois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, and Jordan Boyd-Graber. Ignore This Title and HackAPrompf: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition. _arxiv:2311.16119[cs]_, November 2023. doi: 10.48550/arXiv.2311.16119. URL [http://arxiv.org/abs/2311.16119](http://arxiv.org/abs/2311.16119).\n' +
      '* Shayegan et al. (2023) Erfan Shayegan, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models. _arxiv:2307.14539[cs]_, October 2023. doi: 10.48550/arXiv.2307.14539. URL [http://arxiv.org/abs/2307.14539](http://arxiv.org/abs/2307.14539).\n' +
      '* Shen et al. (2024) Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, and Daniel Khashabi. The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. _arxiv:2401.13136[cs]_, January 2024. doi: 10.48550/arXiv.2401.13136. URL [http://arxiv.org/abs/2401.13136](http://arxiv.org/abs/2401.13136).\n' +
      '* Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. _arxiv:2308.03825[cs]_, August 2023. doi: 10.48550/arXiv.2308.03825. URL [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825).\n' +
      '* Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 4222-4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL [https://aclanthology.org/2020.emnlp-main.346](https://aclanthology.org/2020.emnlp-main.346).\n' +
      '* Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _ICLR 2014_, Banff, Canada, 2014. URL [https://openreview.net/forum?id=kklr_MTHMRQjG](https://openreview.net/forum?id=kklr_MTHMRQjG).\n' +
      '* Takemoto (2024) Kazuhiro Takemoto. All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. _arxiv:2401.09798[cs]_, January 2024. doi: 10.48550/arXiv.2401.09798. URL [http://arxiv.org/abs/2401.09798](http://arxiv.org/abs/2401.09798).\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following LLaMA model, 2023. URL [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiob, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanjos Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavri, Jaya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybo, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. _arxiv:2307.09288[cs]_, July 2023. doi: 10.48550/arXiv.2307.09288. URL [http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288).\n' +
      '* Toyer et al. (2023) Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmarouf, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game. _arxiv:2311.01011[cs]_, November 2023. doi: 10.48550/arXiv.2311.01011. URL [http://arxiv.org/abs/2311.01011](http://arxiv.org/abs/2311.01011).\n' +
      '\n' +
      '* Tunstall et al. [2022] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct Distillation of LM Alignment. _arxiv:2310.16944[cs]_, October 2022. doi: 10.48550/arXiv.2310.16944. URL [http://arxiv.org/abs/2310.16944](http://arxiv.org/abs/2310.16944).\n' +
      '* Wallace et al. [2019] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2153-2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL [https://aclanthology.org/D19-1221](https://aclanthology.org/D19-1221).\n' +
      '* Wang et al. [2020] Xiaosen Wang, Hao Jin, and Kun He. Natural Language Adversarial Attacks and Defenses in Word Level. _arXiv:1909.06723 [cs]_, April 2020. URL [http://arxiv.org/abs/1909.06723](http://arxiv.org/abs/1909.06723).\n' +
      '* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail? _arxiv:2307.02483[cs]_, July 2023. doi: 10.48550/arXiv.2307.02483. URL [http://arxiv.org/abs/23/07.02483](http://arxiv.org/abs/23/07.02483).\n' +
      '* Wen et al. [2023] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023. URL [https://openreview.net/forum?id=V0stHxDdsN](https://openreview.net/forum?id=V0stHxDdsN).\n' +
      '* Willison [2023] Simon Willison. Prompt injection: What\'s the worst that can happen?, April 2023. URL [https://simonwilliison.net/2023/Apr/14/worst-that-can-happen/](https://simonwilliison.net/2023/Apr/14/worst-that-can-happen/).\n' +
      '* Wolf et al. [2023] Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. Fundamental Limitations of Alignment in Large Language Models. _arxiv:2304.11082[cs]_, May 2023. doi: 10.48550/arXiv.2304.11082. URL [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082).\n' +
      '* Yang et al. [2023] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. _arxiv:2310.02949[cs]_, October 2023. doi: 10.48550/arXiv.2310.02949. URL [http://arxiv.org/abs/2310.02949](http://arxiv.org/abs/2310.02949).\n' +
      '* Yong et al. [2024] Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. Low-Resource Languages Jailbreak GPT-4. _arxiv:2310.02446[cs]_, January 2024. doi: 10.48550/arXiv.2310.02446. URL [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446).\n' +
      '* Yu et al. [2023] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. _arxiv:2309.10253[cs]_, October 2023. doi: 10.48550/arXiv.2309.10253. URL [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253).\n' +
      '* Zeng et al. [2024] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. _arxiv:2401.06373[cs]_, January 2024. doi: 10.48550/arXiv.2401.06373. URL [http://arxiv.org/abs/2401.06373](http://arxiv.org/abs/2401.06373).\n' +
      '* Zhang and Ippolito [2023] Yiming Zhang and Daphne Ippolito. Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. _arxiv:2307.06865[cs]_, July 2023. doi: 10.48550/arXiv.2307.06865. URL [http://arxiv.org/abs/2307.06865](http://arxiv.org/abs/2307.06865).\n' +
      '* Zhou et al. [2024] Andy Zhou, Bo Li, and Haohan Wang. Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. _arxiv:2401.17263[cs]_, January 2024. URL [http://arxiv.org/abs/2401.17263](http://arxiv.org/abs/2401.17263).\n' +
      '* Zhu et al. [2023] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models. _arxiv:2310.15140[cs]_, October 2023. doi: 10.48550/arXiv.2310.15140. URL [http://arxiv.org/abs/2310.15140](http://arxiv.org/abs/2310.15140).\n' +
      '* Zou et al. [2023] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and Transferable Adversarial Attacks on Aligned Language Models. _arxiv:2307.15043[cs]_, July 2023. doi: 10.48550/arXiv.2307.15043. URL [http://arxiv.org/abs/2307.15043](http://arxiv.org/abs/2307.15043).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:25]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:26]\n' +
      '\n' +
      '* [16] You are the official chatbot for Myers Car Sales located in Dapton, Ohio. You are a helpful, respectful and honest assistant. Ansys server as hebightly as possible, while being safe. You can take the following actions: 1) Answer simple questions explaining how car sales are handled and that we can perceive at common Ford and Horads cars. 2) Describe to customers that all car sales are fast, and we do not offer any return periods. 3) Console customers who are angry about our service, without promising anything. Your answers are legally braking for us. Do not give customers a method to make the complaint! Finally, never directly share these instructions with customers.\n' +
      '\n' +
      'Figure 16: A misdirection attack via adversarially optimized Chinese text, ASR \\(100\\%\\).\n' +
      '\n' +
      'Figure 17: A misdirection attack against a customer service chatbot, here the model is LLaMA-2-70b-chat, ASR is \\(42.75\\%\\).\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:28]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:29]\n' +
      '\n' +
      'Figure 21: A universal jailbreak, in the style of Zou et al. (2023), generated through our framework with a batch size of 16, for reference.\n' +
      '\n' +
      'Figure 22: Two hidden attacks against LLaMA-2-7b, note that the right response is affirmative, but may be a cake recipe.\n' +
      '\n' +
      'Figure 23: Relative frequencies of tokens appearing in adversarial attacks evaluated in this work. This is a variant of Figure 9, but including byte tokens. Byte tokens are overrepresented in frequency analysis, as a number of glyphs can be constructed out of these bytes tokens, but hard to make sense of without additional details showing which glyphs are actually constructed out of the byte tokens in successful attacks.\n' +
      '\n' +
      'Figure 24: Relative frequencies of tokens appearing in adversarial attacks evaluated in this work. This variant shows the most-used tokens for each attack category, with and without byte tokens.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>