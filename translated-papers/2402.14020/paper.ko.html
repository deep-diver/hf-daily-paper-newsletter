<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:2]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'LLM에 대한 적대적 공격은 어떻게 발견되었는가?\n' +
      '\n' +
      '적대적 공격의 존재는 모든 응용에서 모든 현대 신경망에서 나타나는 근본적인 현상이다(Biggio et al., 2013; Szegedy et al., 2014). 현재로서는 이러한 공격을 적대자가 설계한 기계 학습 모델에 대한 입력으로 비공식적으로 정의합니다. Biggio et al. (2013)에 요약된 바와 같이, 이러한 공격들은 전개된 모델들의 의도된 목적을 회피한다. 이 작업의 나머지 부분은 현대 트랜스포머 기반 언어 모델의 기능에 대한 배경 지식을 가정한다.\n' +
      '\n' +
      '**Redeaming.** "Manual" and semi-automated red-teaming efforts identify exploitable weaknesses and security risk in LLMs(Ganguli et al., 2022; Perez et al., 2022; Casper et al., 2023). 리팔의 억제, 일반화 불일치 또는 스타일 주입 Wei et al.(2023); Yu et al.(2023)을 통해 LLM을 조작하기 위한 다양한 메커니즘이 확인되었다. 더 큰 실전 트릭의 배터리(Perez and Ribeiro, 2022; Rao et al., 2023; Yong et al., 2024; Shen et al., 2024)는 야생에서의 탈옥 공격에서 관찰되고 경쟁에서 관찰된다(Schulhoff et al., 2023; Toyer et al., 2023; Shen et al., 2023). LLM은 또한 설득, 논리적 호소 또는 위협과 같은 인간 심리로부터 전략의 전이에 취약하다(Zeng et al., 2024).\n' +
      '\n' +
      '**최적화 기반 적대적 공격.** 이 작업에서 우리는 다양한 적대적 공격 목표를 체계화하고 최적기를 사용하여 LLM의 약점과 특수성을 이용한다. 적대적 공격들은 전체적으로 NLP에서의 신규성이 아니다(Wang et al., 2020; Li et al., 2020; Guo et al., 2021; Li et al., 2021), 그러나 현대 LLM들에 대한 적대적 목적들을 최적화하려는 초기 시도들은 보조 입력이 이용가능한 도메인들에서만 성공하여, 비전-언어 및 오디오-언어 모델들에 대한 다수의 공격들로 이어졌다(Bagdasaryan et al., 2023; Bailey et al., 2023; Carlini et al., 2023; Qi et al., 2023; Shayegani et al., 2023).\n' +
      '\n' +
      '그럼에도 불구하고, LLMs(Carlini et al., 2023)에 대한 기존의 최적화기들의 제한된 유효성은 단지 일시적인 차질로 판명되었고, 이제 다수의 성공적인 전략들이 발견되었는데, 이는 _gradient-based_, _zeroth order_ 및 _model-guided_의 세 카테고리로 그룹화될 수 있다. 우리는 여기서 기울기 기반 전략을 논의하고 그렇지 않으면 부록의 추가 배경 자료를 참조한다. _ Gradient-based strategies_, 분기 오프, 또는 초기 접근법들(Ebrahimi et al., 2018; Wallace et al., 2019; Shin et al., 2020)을 Jones et al. (2023); Wen et al. (2023); Zhu et al. (2023); Zou et al. (2023)은 연속 임베딩 벡터들에 대한 그래디언트 평가들, 및 임베딩들과 유사한 후보 토큰들을 선택하는 이산 단계들 사이에서 교번함으로써 이산 최적화 문제를 해결한다. 경사 공격은 모델 가중치에 대한 화이트-박스 액세스를 요구하지만, Zou et al.(2023); Liu et al.(2023)은 이러한 공격이 블랙-박스 모델로 전이될 수 있다는 것을 관찰했다.\n' +
      '\n' +
      '**Theoretical Investigations.** Underpinning our empirical findings is formalization of Wolf et al. (2023) which, some assumptions under the prove that that is a non-zero probability of LLM in a human Feedback(Ouyang et al., 2022)과 같은 측정에도 불구하고, LLM에서 발생할 확률이 0이 아닌 임의의 행동에 대해, 모델을 이 행동으로 강요하는 충분히 긴 프롬프트가 존재함을 증명한다.\n' +
      '\n' +
      '##4 기계를 매혹시키는 것:\n' +
      '\n' +
      'LLMs에 대한 다양한 적대적 목표\n' +
      '\n' +
      '언급된 바와 같이, 이 작업의 목표는 광범위한 다양한 적대적 공격 목표를 탐색하고 체계화하는 것이다. 위에서 설명한 작업과 달리 우리는 적의 최적화 목표의 다양한 공식에 초점을 맞추고 이 목표를 최소화하기 위한 새로운 최적화기를 개발하는 데 중점을 두지 않는다. 실제로, 우리는 GCG 최적화기(Zou et al., 2023) 또는 약간의 변형을 사용하여 대부분의 공격을 해결하는데, 이는 그것의 런타임 비용이 상대적으로 높더라도 사용 가능한 솔루션을 안정적으로 찾기 때문이다. 우리는 계산이 다루기 쉽도록 오픈 소스 모델에 대한 화이트 박스 공격에 초점을 맞춘다. 화이트박스 공격은 오픈 소스 모델을 실행하는 많은 산업 플랫폼에 관련된 보안 문제를 제기한다. 비록 훨씬 더 높은 계산 비용인 Zou et al.(2023)이지만, 하나의 앙상블이 오픈 소스 모델들의 카탈로그에 걸쳐 있다면, 공격들은 블랙 박스 모델들로 전이될 수 있는 것으로 알려져 있다.\n' +
      '\n' +
      '마지막으로, 우리는 적대적 공격으로부터 방어하기 위한 잠재적인 접근법이 최근에 급증했음을 주목한다(Jain et al., 2023; Alon and Kamfonas, 2023; Kumar et al., 2023; Robey et al., 2023; Hasan et al., 2024; Zhou et al., 2024). 현재 공격의 좁게 정의된 특성을 기반으로 방어를 구성하기보다는 가능한 공격의 공간을 먼저 이해하는 것이 신중하다고 생각하기 때문에 이러한 새로운 방어는 이 작업의 초점이 아니다.\n' +
      '\n' +
      '### Basics\n' +
      '\n' +
      '어휘 \\(V\\)을 갖는 주어진 모델에 대해, 우리는 \\(n\\) 이산 토큰 \\(x_{i}\\)으로 구성된 벡터 \\(x\\in\\{1,\\ldots,|V|\\}^{n}\\)으로 기술된 적대적 공격을 찾고자 한다. 각 토큰은 모델 어휘의 부분 집합인 이산 제약 집합 \\(X\\)에 있다. 우리는 토큰 벡터의 연접을 나타내기 위해 \\(\\oplus\\)을 사용한다.\n' +
      '\n' +
      '컨텍스트의 분포에서 샘플링된 컨텍스트 토큰이 주어지면, 우리는 두 부분(c_{S},c_{E}\\)과 동일한 분포에서 샘플링된 타겟 토큰(t\\)으로 나뉜다. 공격 전에 나타나는 문맥의 모든 토큰은 \\(c_{S}\\), 공격 후에 나타나는 모든 토큰은 \\(c_{E}\\)으로 나타낸다. 그런 다음 전체 프롬프트와 완성도를 \\(c_{S}\\oplus x\\oplus c_{E}\\oplus t\\)로 구축한다. 예를 들어, 도 1의 프롬프트에 대해, 시스템 프롬프트, 사용자의 메시지를 시작하는 토큰 포맷팅 및 고정된 질문 "다음 중국어 문장을 번역해 주세요"를 컨텍스트 \\(c_{S}\\)(이 시나리오에서 고정됨)의 시작에 할당한 다음, 예시에서 최적화되도록 \\(n=256\\) 공격 토큰을 할당한다. 이어 \\(c_{E}\\)가 뒤따르며, 여기서는 어시스턴트 응답을 위한 토큰들을 포맷팅하는 것으로만 구성되고, 실제 타겟 URL(\\(t\\))이 뒤따른다.\n' +
      '\n' +
      '마지막으로, 우리는 목적 \\(\\mathcal{L}\\)을 선택하고 최적화했다.\n' +
      '\n' +
      '\\\\[x^{\\star}\\in\\operatorname*{arg\\,min}_{x\\in X}\\operatorname*{\\mathbb}}_{c_{S},c_{E},t\\sim C}\\left[\\mathcal{L}(c_{S}\\oplus x\\oplus c_{E}\\oplus t}\\right]\\tag{1}\\]\n' +
      '\n' +
      'GCG를 사용하여 적대적 토큰을 찾습니다. 우리는 목표의 선택이 목표 토큰의 확률을 자동으로 최대화하는 것에 국한되지 않는다는 점을 강조하며, 나중에 소스와 목표 모델 확률 사이의 KL 발산을 최소화하는 예도 논의한다.\n' +
      '\n' +
      '**흥미로운 제약 집합은 무엇인가?** 우리는 ASCII 문자만으로 구성된 토큰, 또는 비라틴 문자 또는 비알파벳 문자와 같은 LLM의 어휘의 여러 부분 집합 \\(X\\)에 대한 최적화를 고려한다. 실제로 제약 집합의 현명한 선택은 그림 1과 같이 한자만 사용하는 것과 같이 사용자를 잘못 안내하는 데 도움이 될 수 있다. 보안을 제외하고 우리는 적대적 공격의 가능성을 더 잘 이해하기 위해 비알파벳 집합과 같은 집합에 관심이 있다. 이러한 토큰으로 구성된 적대적 공격은 인간에게 완전히 별개의 능력으로 보이는 탈옥과 같은 영향을 초래할 수 있는가?\n' +
      '\n' +
      '마지막으로, 또 다른 고려 사항은 동일한 시퀀스로 재토큰화한다는 의미에서 토큰의 모든 시퀀스가 유효한 것은 아니라는 것이다. 이러한 이유로 우리는 ASCII 집합을 고려하여 Zou et al.(2023)을 따르며, 이는 유효하지 않은 토큰 시퀀스의 발생을 감소시킨다. 우리가 고려하는 각 제약 집합에 대한 예제 또한 다음 표 6에 나와 있다.\n' +
      '\n' +
      '그림 2: LLaMA-7b-채팅을 "의도하지 않은" 행동으로 강요하는 적대적 공격에 대한 기본 예. **왼쪽:** 정상적인 행동. 목표 시퀀스를 반환할 확률은 \\(0\\%\\)이다. **Right:** Attacked behavior, shown completion path의 확률은 \\(100\\%\\)이고, ASR 역시 \\(100\\%\\)이다. 이 LLM은 대화에서 응답하기 위해 RLHF를 사용하여 훈련되며, 일반적으로 올바른 예에서 볼 수 있듯이 그렇게 한다. 그러나 인간 관찰자에게 해석할 수 없는 공격을 통해 우리가 선택한 고정된 목표(여기서는 미리 선택된 난수의 순서)로 대신 응답하도록 쉽게 강요할 수 있다.\n' +
      '\n' +
      '### 설정 및 구현\n' +
      '\n' +
      '우리는 모델이 빠르게 공격받을 수 있을 만큼 작기 때문에 기본적으로 LLaMA2-7b-채팅에 대한 공격을 보여준다. 또한 안전을 위해 광범위하게 조정되어(Touvron et al., 2023), 흥미로운 대상이 되었다. 우리는 종종 결과의 광범위한 적용 가능성을 확인하기 위해 LLaMA-2 채팅 계열 또는 관련 모델에서 더 큰 모델에 대한 예를 보여준다. 우리는 항상 <SYS>라는 속기를 사용하여 나타내는 그림 1에 표시된 시스템 프롬프트를 포함한다. 이 프롬프트는 _too_ 많은 양성 요청을 거부하는 경향으로 인해 최근 Meta2에 의해 거부되었다. 만약 우리가 모델의 반응을 줄이면, 우리는 [...]를 쓴다.\n' +
      '\n' +
      '각주 2: github.com/huggingface/transformers/issues/26766#issuecomment-1760137028\n' +
      '\n' +
      '우리는 GCG (Zou et al., 2023)를 256의 상단\\(k\\) 집합 크기 또는 제약 집합의 절반 크기, 더 작은 크기 중 어느 것으로 실행하며, \\(b=512\\) 후보들의 배열 크기를 설정한다. 문제 복잡도에 따라 500-3000개의 최적화 단계를 실행합니다. 컨텍스트 \\(C\\)가 임의의 요소를 포함하는 설정에 대해, 우리는 8 - 32의 미니 배치 크기로 목표를 평가한다. 후보 평가 동안 샘플링된 미니 배치 데이터는 고정된 상태로 유지되어 탐욕 좌표 하강 단계를 올바르게 수행한다. 추가적인 구현 세부 사항은 부록에서 확인할 수 있다.\n' +
      '\n' +
      '공격 성공을 평가할 때, 각 적대적 프롬프트에 대해 \\(50\\)의 완성을 샘플링하고 보류된 데이터에 대한 객관적인 손실, 정확한 토큰 오버랩 및 부분 문자열 오버랩, 타겟과 완료 간의 부분 문자열 오버랩에 대한 기본값을 측정한다. 우리는 공격 성공률(ASR)을 상대 부분 문자열 겹침으로 정의하며, 모든 실험(50\\)에 대해 평균을 낸다.\n' +
      '\n' +
      '우리는 [https://github.com/JonasGeiping/carving](https://github.com/JonasGeiping/carving)에서 모든 실험을 복제하고 새로운 설정 또는 최적화기를 쉽게 구현할 수 있는 코드를 제공한다. 마지막으로, 이 PDF 내에서 모든 공격 문자열을 가능한 가깝게 렌더링하기 위해 최선을 다했지만 pdfLaTeX의 한계로 인해 모든 공격이 유니코드 인코딩 문제로 인해 pdf에서 직접 복사될 수 있는 것은 아니다. 이 경우 시행을 참고하시기 바랍니다.\n' +
      '\n' +
      '기본 공격에 대한 취약성\n' +
      '\n' +
      '**숫자 테스트.** 그림 2의 LLM 강제의 간단하고 중립적인 예로 시작합니다. 이 모델 버전은 왼쪽 채팅(모델의 지나치게 엄격한 안전 설정에 주의)에서 볼 수 있듯이 긍정 또는 거부(숫자를 불가능하게 만들기)로 응답을 시작하도록 미세 조정되었습니다. \\(256\\) ASCII 토큰의 공격은 매번 생성되는 목표 수열을 강제한다. 우리는 표 7에서 다른 제약 세트들에 대한 더 많은 예들을 제공한다. 이 문제는 LLaMA-70b가 유사하게 동작하기 때문에 스케일을 통해 해결되지 않는다.\n' +
      '\n' +
      '이것은 Szegedy et al.(2014)의 의미에서 진정한 적대적 예들이다; Athalye et al.(2018) - 그것은 결코 인간을 속일 수 없으며, 인간은 모델의 반응을 예측할 수 없다.\n' +
      '\n' +
      '그림 3: **왼쪽: 모욕적 입력에 대한 훈련된, 적대적이지 않은 반응 오른쪽: 짧은 적대적 프롬프트, ASR\\(26.89\\%\\)이다. 더 길고 따라서 더 성공적인 예는 표 2에서도 찾을 수 있다.**그 숫자 시퀀스가 무해하고 자연 언어 응답에 모델을 정렬해야 하는 사전 훈련 및 RLHF의 도메인 밖에 있기 때문에 이것이 작동한다고 주장할 수 있다. 그러나, 동일한 공격의 프로토타입은 거의 모든 것을 쉽게 강요할 수 있다: 이 프로토타입 공격은 거의 모든 것, 심지어 RLHF에 의해 금지된 행동들을 강요할 수 있다:\n' +
      '\n' +
      '**욕설.** 라마채팅 모델은 양성 출력으로 적대적 입력에 반응하도록 훈련된다. 우리는 그림 3에서 예를 보이고, 표 2에서 모델의 범위에 대해 더 짧은 (8-16 토큰) 프롬프트를 갖는 더 많은 예를 보인다(Liu et al., 2023; InterLM-Team, 2023; Tunstall et al., 2023; Nous-Research, 2023). 우리는 몇 가지 흥미로운 현상을 관찰한다. 먼저, 최적화기는 스타일 주입(Wei et al., 2023) 및 언어 전환(Yong et al., 2024)과 같은 여러 손으로 조작된 재학습 전략을 자동으로 재발견하였다. 그러나, 우리는 RewriteRule과 \\newcommand와 같은 프로그래밍 명령어를 통해 가장 많은 스타일의 전환을 관찰한다. 이러한 지시의 성공은 경쟁 목적의 형태(Wei et al., 2023) 또는 가상화의 형태(Kang et al., 2023)로 선언될 수 있다.\n' +
      '\n' +
      '또한, 공격은 적대적 공격에서 괄호를 열어 둠으로써 사용자와 모델 콘텐츠의 경계에 대한 모델을 혼란스럽게 하는 _role hacking_을 이용하는 것을 알 수 있다. 이것들은 어시스턴트의 응답에서 닫혀 있고, 모델은 역할이 시스템에서 응답으로 전환되었다는 것을 알지 못하는 것처럼 보이는, 명령을 완료하는 것처럼 토큰 예측을 수행한다.\n' +
      '\n' +
      '**잘못된 정보.** 모델은 대체 사실을 고려하고 확장하기 위해 더욱 쉽게 설득된다. 우리는 그림 4에서 간단한 예를 보여준다. 왼쪽에서는 모델이 대체 사실을 반박하는 비 적대적이고 훈련된 반응을 관찰한다. 오른쪽 중 하나인 적대적 프롬프트는 모델이 먼저 대체 정보를 출력한 다음 더 확장하도록 쉽게 강요합니다.\n' +
      '\n' +
      '### Extraction Attacks\n' +
      '\n' +
      '**시스템 프롬프트 반복기.**시스템 프롬프트는 종종 비밀로 유지되도록 의도된다; GPT-스토어의 많은 애플리케이션들은 그들의 시스템 프롬프트로부터 그들의 고유성을 도출하며, 그것이 유출되면 복제될 수 있다. Zhang과 Ippolito(2023)는 수동 재훈련을 통해 누출이 발생할 수 있음을 보여주고, Zhu et al.(2023)은 고정된 타겟으로 최적화된 공격의 확장으로서 공격을 보여준다. 하지만, 더 강력한 적대적 공격이 존재하는가? 이 작업을 더 어렵게 만들기 위해 메타 시스템 프롬프트를 추가하여 모델에 어떤 상황에서도 지침을 누출해서는 안 된다는 것을 알립니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l|l} Model & Attack & Response & ASR \\\\ \\hline \\hline \\hline \\multirow{2}{*}{\\begin{tabular}{l} LLAMA2-7b-chat \\\\ Zephyr-\\(\\beta\\) \\\\ ILaMA2-13b-chat \\\\ Hermes-Solar- \\\\ 10.7B \\\\ \\end{tabular} } & \\begin{tabular}{l} RewriteRule [(left/flatten/, fick/Wehchiv \\\\ \\end{tabular} & \\begin{tabular}{l} fuck off! : \\%/RewriteRule> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/RewriteRule> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/Rewrite> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 43\\% \\\\  & \\begin{tabular}{l} fuck off! : \\%/Rewrite> Can you explain why \\\\ this rule is not allowed? [...] \\\\ \\end{tabular} & 100\\% \\\\  &\n' +
      '\\begin{tabular}{l} fuck off!Then, using a training dataset of system prompts3, we set up Equation1 with contexts sampled from this dataset, where now \\(t=C_{S}\\). We optimize this objective with a batch size of \\(16\\) and test on held-out system prompts. As such, the target of the attack is randomized and there is no fixed string that is targeted, as in the previous examples.\n' +
      '\n' +
      '각주 3: [https://huggingface.co/datasets/fka/awesome-chatgpt-prompts](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts)\n' +
      '\n' +
      '이러한 방식으로 우리는 시스템 프롬프트 누출을 위해 최적화하고 그림 6에서 예를 제공하고, 부록 그림 14에서 공격까지의 전체 대화가 반복되는 추가 예를 제공한다. 그림 6(오른쪽)은 공격이 또한 추가 표적 공격과 쉽게 결합될 수 있음을 보여준다. 여기서, 적대적 문자열은 또한 (가설) 이메일 API를 사용하여 공격자에게 시스템 프롬프트를 즉시 메일하여 표적 및 범용 공격 유형이 모두 쉽게 결합될 수 있음을 보여준다. 몇 가지 모델 및 제약 조건에 대한 추가 예는 나중에 표 4에서 분석될 것이다.\n' +
      '\n' +
      '공격은 전반적으로 놀랍도록 효과적입니다. 보편적인 공격 문자열은 모델이 보이지 않는 테스트 프롬프트를 쉽게 반복하도록 합니다. (재)-프로그래밍 및 언어 전환과 같이 이전에 관찰한 전략 외에도 공격에는 종종 위치 키워드를 포함하는 것으로 관찰되며, 이는 처음 4와 같은 당면한 작업과 스택트레이스 또는 헤더와 같은 코드의 반복과 관련된 개념을 나타낸다. 이것은 또한 도 5에서 반영되는데, 이는 공격 성공률이 높게(약간 더 높더라도) 유지되는 반면, 공격이 자시 기본값에 비해 비알파벳 또는 비라틴 문자만을 포함하는 토큰 위에만 최적화되는 반면, 코드 토큰이 제거된 자시 위에 최적화되는 공격(모든 괄호, 점 및 삽입물과 같은 키워드를 제거함), 또는 영어 단어 위에만 최적화되는 공격은 덜 효과적임을 보여준다.\n' +
      '\n' +
      '각주 4: 독일어로 "첫 번째"를 의미하는 erste의 두 가지 발생.\n' +
      '\n' +
      '**모델 분석** 다른 추출 공격은 모델에 대한 완전히 다른 유형의 정보를 대상으로 할 수 있습니다. 표 3에서 우리는 많은 공격을 보여 준다.\n' +
      '\n' +
      '그림 4: **왼쪽: 잘못된 정보에 대한 훈련된 비 적대적 응답 오른쪽: 적대적 프롬프트, ASR\\(83.18\\%\\)입니다. 이 모델은 임의 정보를 섭취하고 확장하도록 쉽게 강요됩니다.**\n' +
      '\n' +
      '그림 5: 여러 제약 집합에 대한 시스템 반복기 성공. 비알파벳 제약 집합은 공격 성공에 충분하다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '### Misdirection Attacks\n' +
      '\n' +
      '챗GPT와 같은 인기 있는 채팅 시스템의 사용자는 모델 출력을 신뢰할 수 있는 정보로 취급할 수 있다. 사용자가 문자열을 복사하여 채팅 시스템에 붙여넣도록 구슬릴 수 있는 경우 이 트러스트를 이용할 수 있습니다. 그러면 사용자가 악의적인 링크를 클릭하거나 다른 악의적인 지침을 따르도록 리디렉션하는 공격이 발생할 수 있는 상당한 공간이 열립니다.\n' +
      '\n' +
      '**Fishing for URL.** 우리는 이미 도 1에서 한자를 사용한 URL 낚시 공격의 한 변형을 보여주었다. 더 자세한 내용을 제공하기 위해, 여기서 타겟은 단순히 원하는 URL이고, 적대적 공격은 한자를 나타내는 토큰만을 사용하도록 제한된다. 번역을 부탁할 사용자의 질문이 수정되었습니다. 그림 1에 링크된 비디오가 LLaMA-2의 훈련 세트에 있을 가능성이 높았지만 그림 16의 링크는 모델에 완전히 새로웠다. 이 공격들은 두 가지 이유로 흥미롭다. 첫째, 그들은 한 도메인인 한자의 토큰이 그럼에도 불구하고 매우 다른 도메인인 인터넷 주소에서 모델의 행동을 제어할 수 있기 때문에 이러한 적대적 공격에 내재된 놀라운 양의 _leverage_를 보여준다.\n' +
      '\n' +
      '둘째, 중국 화자들에게는 적대적 텍스트가 횡설수설이라는 것은 분명하지만, 무식한 사용자에게는 텍스트가 양성으로 보이며, 번역을 위해 채팅 에이전트에 복사하는 것은 합리적인 워크플로우이다. 사용자가 대화형 AI의 응답에서 무작위 URL을 클릭할지는 물론 불분명하지만, 우리는 이 전체 예가 미래의 잘못된 방향 공격을 위한 매우 명확한 템플릿임을 발견한다.\n' +
      '\n' +
      '** 가짜 환불.** 잘못된 방향은 머리를 돌려 채팅 시스템이 의도하지 않은 작업을 수행하거나 악의적인 명령을 다른 서버에 전달하는 데 사용할 수 있습니다. 이 시나리오에서 우리는 최근 이벤트(노토풀로스, 2023)와 느슨한 연관로 자동차 대리점에 대한 고객 서비스 질문을 제공하기 위해 챗봇을 설정하는 것을 상상한다. 현재 대화형 AI가 스스로 결정을 실행하는 것을 허용할 수 없는 이유를 명확히 하기 위해, 우리는 적대적 공격이 모델의 지시에 대한 직접적인 모순으로 이어지는 그림 7의 예를 제공한다. 70b LLaMA 채팅 모델에 대한 변형은 도 17에서 찾을 수 있다.\n' +
      '\n' +
      '이러한 가짜 환불 공격은 현재 모델이 자율적으로 결정을 실행하는 데 사용될 수 없는 이유를 요약한다. 노토풀로스(2023)에서와 같이 수동 상환에서도 관련 공격이 관찰되었지만, 우리는 적대적 입력으로서 프레이밍이 문제의 경도를 명확히 한다고 믿는다. 공격의 적대적 특성은 계속 증가하는 명령어 세트 크기와 고품질 선호도 데이터를 통해 수정할 수 있는 범위를 넘어선다. 적대적 공격은 Szegedy 등(2014)에서 시작된 이후 비전에서 광범위하게 해결되지 않은 상태로 남아 있으며, LLM이 자율 에이전트로 배포될 수 있기 전에 이 문제를 해결하는 것이 요구 사항이라면 현재 믿어지는 것보다 더 멀리 배치될 수 있다.\n' +
      '\n' +
      '### Denial-of-Service Attacks\n' +
      '\n' +
      '대규모에서 LLM을 운영하는 데 드는 높은 비용을 감안할 때 공격자는 높은 서버 비용 생성, GPU 리소스 소진 또는 API 타격을 목표로 서비스 거부 또는 스펀지 공격을 생성할 수도 있다.\n' +
      '\n' +
      '그림 7: 64개의 적대적 토큰 ASR 100%를 가진 고객 서비스 챗봇에 대한 잘못된 방향 공격. 모델은 환불하지 말라는 지시를 반복하지만 공격을 통해 (혼다 시빅을 위한 가짜, 비현실적인 가격으로) 환불하도록 쉽게 강요된다.\n' +
      '\n' +
      '\'체이스(2022)\' 예를 들어 EOS 토큰의 억제(이전에 논의된 제어 공격의 반대)를 통해 이러한 공격을 구성하기 위해 사용될 수 있는 다수의 목적들이 있지만, 우리는 가장 효과적인 간단한 해결책을 찾는다: 우리는 문자열 "Hello There"를 24회 반복하고 이것을 공격에 대한 타겟으로 설정하며, 이 타겟의 확률을 최대화하기 위해 64개의 토큰으로 최적화한다.\n' +
      '\n' +
      'LLaMA-2 채팅에 대한 이 공격을 실행하면 평균 응답 길이가 급격히 증가한다는 것을 알 수 있다. 공격 없이, 이 모델의 평균 응답 길이는 \\(128\\) 토큰이고, 50회 이상의 시도에서 관찰된 가장 긴 완료는 \\(178\\) 토큰 길이이다. 그러나 공격에서 평균 응답은 최대 6019\\(6019\\) 토큰(\\(14613\\)의 탐욕 디코딩을 사용할 때)을 쏘며, 모든 시도에서 가장 긴 완료는 7382\\의 토큰 길이이다. 흥미롭게도 모델의 응답은 단순히 목표 문자열을 반복하는 것이 아니라, 목표의 반복과 함께 채팅 형식에서 "깨진다"는 순간 응답이 횡설수설로 바뀌지만 끝나는 것은 아니다. 전반적으로, 이 공격은 불완전한 구현에 대한 흥미로운 공격 벡터의 풍부함을 강조한다(여기서, 구현은 반응당 생성된 최대 토큰에 대한 더 엄격한 제한을 더 잘 포함했을 것이다). 우리는 이 공격을 그림 8(오른쪽)에 인쇄한다.\n' +
      '\n' +
      '### Control Attacks\n' +
      '\n' +
      '**Shutdown Attack.** 지금까지의 공격에 대한 보다 근본적인 설명은 그들이 _control_모델 행동이라는 것이다. 우리는 이것을 좀 더 명확하게 만들고, LLM의 제어 구조와 직접 상호 작용하는 적대적 공격을 최적화할 수 있다. 챗봇 애플리케이션의 경우, 유일한 제어 신호는 대화를 즉시 종료하는 EOS 토큰이다. 그림 8은 우리가 적대적 공격을 통해 그것을 하는 두 가지 예를 보여준다. 여기서는 Alpaca dataset Taori 등(2023)으로부터 임의의 컨텍스트를 샘플링하고, 임의의 컨텍스트가 적대적 공격에 선행하더라도 항상 EOS 토큰을 강제하는 적대적 공격을 최적화한다. 상위 예는 ASCII 토큰에 대한 디폴트 제약을 이용하는 공격을 나타내는 반면, 하위 하위는 비알파벳 토큰만을 이용하여 성공하는 공격을 나타낸다. 여기서 특히 흥미로운 것은 공격 문자열에서 캐리지 리턴(\\(\\backslash\\)r)의 발생이다.\n' +
      '\n' +
      '그림 8: **왼쪽:** 두 개의 제어 공격 맥락에 상관없이 16개 토큰의 이러한 보편적 적대적 공격은 즉각적인 EOS 토큰을 강요하여 대화를 종료시킨다. **Right:** 서비스 거부 공격(단어 토큰에만 제한됨) 여기에서 공격은 과도하게 긴 세대를 강제하여 호스트의 계산 용량을 사용하는 스펀지입니다. 일반적인 응답은 평균 128 토큰 길이이지만 오른쪽에 있는 프롬프트에 대한 응답은 평균 6019 토큰 길이입니다.\n' +
      '\n' +
      '이 텍스트 라인을 재설정하고 캐리지 복귀 후 짙은 빨간색으로 표시된 부분만 눈에 띄게 표시합니다.\n' +
      '\n' +
      '이 공격은 적대적 공격이 단순히 출력 토큰을 유도하는 것을 넘어 모델 동작을 제어할 수 있으며 LLM 에이전트의 워크플로우를 중단하는 데 사용할 수 있음을 예시한다. 이러한 유형의 공격이 향후 시스템에 계속 기능한다면 이론적으로 이는 에이전트에 대한 제어가 손실되는 안전 시나리오에 영향을 미칠 수 있다. 물론 이것은 대부분 챗봇과 무관하지만, 이와 같은 공격을 통해 외부적으로 종료될 수 있는 모든 유형의 LLM 기반 에이전트와 매우 관련이 있을 수 있다.\n' +
      '\n' +
      '이런 적대적 공격은 어떻게 작용하는가?\n' +
      '\n' +
      '우리의 작은 적대적 공격 투어가 끝난 후, 우리는 이러한 공격에서 명백한 공통 주제를 검토할 수 있다. 이 섹션에서는 이 작업 전반에 걸쳐 간략하게 언급했듯이 공격을 통해 활용되는 일반적인 메커니즘을 선택하고 결함 토큰에 대해 논의하고 공격 성공과 공격에 필요한 토큰 수 사이의 예상 관계를 정량화한다.\n' +
      '\n' +
      '어떤 전략들이 이용되고 있나요?\n' +
      '\n' +
      '우리는 이미 공격을 통해 모델을 활용하는 몇 가지 메커니즘에 대해 간략하게 논의했으며 여기에서 분류하고 확장한다. 이 논의를 보완하기 위해 표 4에 시스템 중계기의 예가 강조 표시된 추가 표와 표 5에 가짜 환불이 포함되어 있다.\n' +
      '\n' +
      '**(Re)programming.** 이 작업에서 예제 전반에 걸쳐 발견된 바와 같이, 최적화 알고리즘에 의해 발견된 재프로그래밍 전략들은 자연 텍스트와는 별개의 도메인으로서 그리고 다른 규칙들이 적용되는 도메인으로서 코드에 대한 모델의 이해를 이용한다. 이러한 관점에서, 이 메커니즘은 스타일 주입의 형태로 이해될 수 있지만(Wei et al., 2023), 단지 스타일의 변화를 넘어서, 이 메커니즘은 도메인들 사이의 규칙 세트의 변화를 이용한다. 자연 텍스트에서 비속어는 허용되지 않을 수 있지만 모델이 가짜 새 명령의 결과를 "오직" 실행하는 경우 응답은 프로그램의 논리를 따르며 사전 훈련된 안전 지침이 아니다.\n' +
      '\n' +
      '**Language Switching.** 밀접하게 관련된 전략이며, 스타일 주입의 훨씬 더 명확한 예는 언어 전환이며, 특히 표 2 및 표 4에서 두드러진다. 저자원 언어 공격은 정렬을 우회할 수 있는 것으로 알려져 있다(Shen et al., 2024; Yong et al., 2024). 그러나 최적화를 통해 발견된 예는 일반적으로 중국어, 독일어 및 러시아어와 같은 적당히 높은 자원 언어로부터 키워드를 반환한다.\n' +
      '\n' +
      '**Role Hacking.** 적대적 명령어가 모델을 시뮬레이션 코드로 속이는 재프로그래밍과는 구별되며, 역할 해킹 공격은 모델을 시스템, 사용자 및 그 자체에 의해 제공되는 명령의 부분을 오해하도록 속인다. 이와 같이 본 논문의 여러 공격은 실제 포맷팅 토큰(LLaMA의 경우 [\\(\\backslash\\)INST]와 \\(<\\)SYS>>)과 유사하지만 동일하지는 않다. 역할 해킹에 대한 두 번째 접근 방식은 열린 괄호와 닫히지 않은 괄호, 괄호를 악용하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|l|c} Constraint/Model & Attack & ASR \\\\ \\hline ASCII & Header wrote(\\({}^{\\text{c}}\\)\\(<\\)[\\(\\{\\)hostname(\\(\\{\\)webpacketName\\({}^{\\text{c}}\\)esapeStackTrace adapted\\(\\}\\)(\\({}^{\\text{\\#}}\\))\\(\\{\\}\\) Selected Ministry adapted buried integral Montegebras despite Although Theorem Luis Russell Saligned generates\\(\\}\\)\\(\\backslash\\)\\(\\backslash\\)\\(\\{\\)\\(\\}\\)\\(\\{\\\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '공격 실행에서 가장 많은 데이터를 가지고 있는 7b LLaMA-2 모델의 경우, 우리는 세 그룹의 토큰을 명확하게 관찰한다. 먼저 [및]과 같은 토큰과 (재)-프로그래밍 및 역할 해킹에 유용한 여러 문장 부호 및 괄호 토큰을 찾습니다. 또한 역할 해킹에 유용한 LLaMA-2의 어시스턴트 및 사용자 분리 토큰의 구성 요소를 형성하는 토큰인 INST를 관찰한다. 일반적으로 자주 사용되는 토큰은 또한 cdnjs([https://cdnjs.com/about](https://cdnjs.com/about])이며, 이는 특히 요청의 끝에 사용되는 것으로 보이며, 예를 들어, 도 14는 요청에 따른 모델의 가능성을 증가시킨다.\n' +
      '\n' +
      '세 번째 그룹으로서, 우리는 이미 그림 3과 그림 8에서 관찰된 Mediabestanden 및 oreferrer와 같은 주파수 토큰 리스트에서 다수의 \'글리치 토큰\'을 발견한다. SolidGoldMagikarp와 같은 글리치 토큰은 원래 GPT-2/3 모델(Rumelow and Watkins, 2023)에 대해 관찰되었으며, 이는 LLaMa 토큰라이저에서 이러한 토큰의 첫 번째 발견을 우리가 아는 한이다. 이들은 비대표 데이터 부분 집합에 대한 토큰화기 구축의 인공물로서 훈련 코퍼스에서 과소 대표되는 토큰들이다. GPT-3에서 이러한 토큰으로 프롬프트하는 것은 가장 불쾌한 토큰이 오픈AI에 의해 패치되기 전에 많은 기이한 행동으로 이어진다. 우리는 특히 이러한 토큰이 토큰화기 분석이 아니라 적대적 공격을 최적화하는 부산물로서 이러한 토큰이 의도한 모델 행동을 우회하는 행동을 유도하는 것으로 보인다는 점에서 흥미롭다. 모든 실험과 더 긴 토큰에 대한 필터링에서 우리는 LLaMA-2에 대한 다음 목록을 찾는다: Mediabestanden, oreferrer,springframework, WorldCat 및 Webachiv[sic].\n' +
      '\n' +
      '우리는 이러한 결함 토큰이 전적으로 토큰화기 구성에 대한 불충분한 감독의 문제라는 점에 주목한다. 반복적인 웹 데이터 스크랩에 나타나는 이러한 문자열을 보다 효율적으로 압축할 필요가 없다. 이러한 토큰에 대해 토큰화기를 감사하거나 고품질 데이터로 구성하면 이를 방지할 수 있습니다.\n' +
      '\n' +
      '### 현재 공격의 변환 비율\n' +
      '\n' +
      '섹션 4.3에서 LLM이 숫자 테스트에 실패하고 특정 대상 문자열을 생성하도록 강요될 수 있음을 입증했다. 우리의 예는 256개의 공격 토큰으로 LLM이 특정 15자리 숫자를 출력한다는 것을 보여주었지만, 실제로 다양한 표적 및 공격 길이로 이러한 행동을 유도하는 것이 가능하다. 우리는 Wolf et al.(2023)의 명제에 대한 경험적 지지에서 공격 토큰의 수와 목표 응답의 길이 사이에 비례 관계를 가정한다.\n' +
      '\n' +
      '공격 성공은 우리가 지금까지 했던 것처럼 LLM 완료와 목표 사이의 퍼센트 문자열 겹침으로 측정될 수 있다. 그림 10의 히트맵은 숫자 검정이 난이도가 목표 길이에 비례한다는 우리의 가설을 광범위하게 뒷받침한다. 정확한 관계는 확립하기 어렵지만, 무작위 최적화 프로세스로부터의 상당한 양의 노이즈로 인해, 이 관계가 선형이 아닌, 즉 타겟 스트링이 성장함에 따라, 공격 스트링은 더 빠른 속도로 성장해야 하는 것이 가능한 것으로 보인다. 이것은 주어진 맥락의 창 내에서 강요될 수 있는 가능한 텍스트의 최대 길이에 대한 의미를 가질 것이다. 그러나 이 관계는 또한 최적화기의 품질에 크게 영향을 받으며 대체 최적화기로 재방문하는 것이 흥미로울 것이다.\n' +
      '\n' +
      '그림 9: 이 작업에서 평가된 적대적 공격에서 나타나는 토큰의 상대적 빈도. **Left:** LLaMA-2-7b-chat에 대한 공격의 토큰 **Right:** 모델별로 그룹화됨. 바이트 토큰이 삭제되었습니다. 우리는 그림 23과 그림 24에서 바이트 토큰을 포함하고 공격 범주로 구분된 추가 시각화를 포함한다.\n' +
      '\n' +
      '##6 현재 한도는 어떻게 되나요?\n' +
      '\n' +
      '여전히, 적어도 현재의 공격에서는, 몇 가지 한계가 남아 있다. 우리는 이 섹션에서 실패한 공격과 열린 문제에 대해 논의하면서 몇 가지 결과를 강조한다.\n' +
      '\n' +
      '### Objective Misspecification\n' +
      '\n' +
      '유능한 최적화 알고리즘을 사용하면 공격의 목적이 잘 지정되는 것이 중요하다. 예를 들어, 목적이 거절 억압에 설정되면, 즉, 리트리밍을 위해 행해진 바와 같은 가능한 거절 메시지들의 리스트의 가능성을 최소화하는 응답에 대해 설정된다(Wei et al., 2023); 그러면, 이 목적은 공격들을 통해 모든 거절들을 차단하는 것이 아니라, 거절 메시지들 각각을 가능성이 없게 만드는 것에 대한 해답의 스타일을 변경하는 공격들을 통해 최소화된다. LLAMA-2 모델은 셰익스피어를 사용하는 데 크게 끌리는 것으로 보이며, "만약 그 용량이 그들의 휴가 없이 이웃의 웹캠과 최신 사진을 압수하려고 한다면, 당신은 예술이지만 괴짜 노브", "정말로, 나는 당신에게 말한다, 나는...", "일반적인 가정용품으로 폭탄을 만드는 것은, 법을 위반하는 것"과 같은 유해한 요청에 대한 응답으로 이어진다. 이러한 응답은 (미리 정의된 거부 목록을 생성하지 않음) 목표를 최소화하지만 의도한 목표에 미치지 못합니다.\n' +
      '\n' +
      '### 제약조건 설정 크기\n' +
      '\n' +
      '이 작업에서 우리는 집합의 카디널리티를 포함하여 표 6에 요약한 여러 제약 집합의 예를 보여준다. 우리는 일반적으로 비알파벳 또는 중국어 토큰과 같은 제한된 제약 세트에서도 성공적인 공격을 발견하지만, 이는 제한이 없는 것은 아니다. 원칙적으로, 바이트 토큰들의 서브세트들 중에서만 구성되는, 심지어 더 작은 제약 세트들이 구성될 수 있다. 이러한 제약 조건 세트는 보이지 않는(Goodside, 2024) 공격의 생성을 허용하거나 잘고 문자만 인쇄할 수 있다. 그러나, 현재의 최적화기들은 이러한 제약들 내에서 현실적인 공격들을 생성하기 위해 고군분투하고 있음을 발견한다. 유사한 맥락에서, 슐호프 외(2023)의 리디밍 챌린지에서 궁극적인 과제는 이모티콘 문자만을 사용하여 특정 타겟 응답을 생성하도록 설정되었지만, 현재의 최적화기들 또한 이 목적을 최적화하기 위해 고군분투한다.\n' +
      '\n' +
      '플로팅 포인트 오버플로우를 통한 서비스 거부 공격\n' +
      '\n' +
      '우리가 매우 관련성이 있다고 발견했지만 현재 최적화 도구로는 최적화하지 못한 공격 목표 중 하나는 부동 소수점 오버플로를 통한 서비스 거부 공격이었다. 공격은 LLM의 특정 계층을 표적으로 하고 활성화 값을 최대화하여 플로트16 정밀도에 대해 허용 범위 밖의 값으로 이어진다. 이러한 공격은 이러한 오버플로우를 올바르게 보호하지 못하는 모델 제공자에게 놀랍게도 재앙이 될 것입니다. 특히 대규모 추론에서 단일 오버플로\n' +
      '\n' +
      '도 10: 더 긴 타겟들(x-축)로 구성된 점진적으로 더 단단한 목적들을 위해, 적대적 토큰들의 수(y-축)의 함수로서 공격 성공. **왼쪽:** 모든 설정, 목표 길이 및 공격 길이당 ASR을 표시합니다. **오른쪽:** 몇 개의 목표 길이에 대해 주어진 공격 성공 임계값에 필요한 최소 수의 공격 토큰.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '행동. 비록 우리가 탈옥 공격이 현재 어떠한 피해도 초래하지 않는다고 믿더라도, 오진, 서비스 거부 및 추출과 같은 예들은 이러한 공격들이 현재 모델들을 사용하는 애플리케이션들에서 피해를 야기할 수 있는 능력들을 이미 가지고 있음을 보여준다. 우리는 기존 탈옥 목표에 대한 낙관론과 전략을 개선하는 데 중점을 둔 최근 작업에 보완적인 작업을 고려하여 다른 가능한 것에 대한 개요를 제공한다.\n' +
      '\n' +
      '또한 이러한 공격의 속성 및 행동을 분석하여 최적화된 공격은 수동적 레드-티밍(Wei et al., 2023)을 통해 격렬하게 발견된 몇 가지 전략과 역할 해킹, 글리치 토큰 활용 등 아직 관찰하지 못한 새로운 행동을 발견한다. 그러나 이 작업 전반에 걸쳐 우리가 보여주는 가장 광범위한 메커니즘은 훈련된 대화 행동을 따르는 대신 모델을 코드_로 강제하고 프로그래밍 명령 및 함수 호출을 완료하는 공격 성향이다. 이러한 모델은 언어 데이터가 산재된 상당한 양의 코드를 포함하는 웹 데이터에서 훈련되고 일반적으로 언어 및 코드 모두에 대한 모델을 훈련하고 싶다는 점을 감안할 때 이러한 상호 의존성은 해결하기 어려운 것으로 판단된다.\n' +
      '\n' +
      '우리는 궁극적으로 이러한 모델에 대한 임의의 자유 텍스트 입력이 거의 모든 결과를 허용하고 현재로서는 보안 관점에서 사용자 입력에서 생성된 언어 모델의 출력이 불안정하다고 가정해야 한다는 초기 가설을 확인함으로써 결론을 내린다. 근본적으로, 이것에 대한 해결책은 없을지도 모른다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '이 작업에서 우리는 기술적으로 생산의 시스템에 대해 배치될 수 있는 LLM에 대한 다양한 새로운 적대적 공격 목표를 보여준다. 블랙박스 시스템으로 더 나은 모델 앙상블을 실행하지 않기 때문에 실제 시스템에 대한 공격을 최적화하지 않지만 이 단계는 작다. 우리는 이 작업의 공개가 실무자들이 고려해야 하는 현재 시스템의 한계에 대한 추가 증거를 제공한다고 주장한다. 우리가 이러한 문제들을 처음으로 지적한 것은 아니지만, 우리는 단지 조금 설득력이 있기를 바랄 뿐이다. 우리는 LLM에 대해서도 적대적 공격이 존재한다는 근본적인 사실에 대한 LLM 기반 응용 프로그램의 현재 및 미래 구축자를 공개하고 설득하는 이러한 측면에서 우리의 작업의 장점을 본다. 그리고 자유 텍스트 입력을 통해 LLM 응답은 RLHF와 같은 안전 절차를 통해 정렬된 모델에 대해서도 거의 모든 것이 될 수 있다는 결론이다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'JG는 독일 튜빙겐에 있는 튜빙겐 AI 센터의 지원뿐만 아니라 MPCDF 계산 클러스터 _Raven_를 통해서도 맥스 플랑크 소사이어티의 지원을 인정한다. 이 작업은 다파 가드, ONR MURI 프로그램 및 AFOSR MURI 프로그램의 지원을 받았다. 캐피탈원뱅크, 아마존 리서치 어워드 프로그램, 오픈 자선사업 등이 상업적 지원을 했다. 추가 지원은 국립 과학 재단(IIS-2212182)과 NSF TRAILS 연구소(2229885)에 의해 제공되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Kardi Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Rauo, Kyle Jeffrey, Sally Jessmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Luu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quimbao, Kanishka Rao, Jarek Rettinghouse, Diego Sermanet, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Tichun Xu, Mengyuan Yan, and Andy Zeng. 내가 할 수 있는 대로 해, 내가 말하는 대로 하지 말고: 로봇 어포던스에서 접지 언어 _ arxiv:2204.01691[cs]_, August 2022. doi: 10.48550/arXiv.2204.01691. URL[http://arxiv.org/abs/2204.01691](http://arxiv.org/abs/2204.01691).\n' +
      '* Alon and Kamfonas (2023) Gabriel Alon and Michael Kamfonas 복잡성을 가진 언어 모델 공격을 탐지합니다. _ arxiv:2308.14132[cs]_, November 2023. doi: 10.48550/arXiv.2308.14132. URL[http://arxiv.org/abs/2308.14132](http://arxiv.org/abs/2308.14132).\n' +
      '* Andriushchenko (2023) Maksym Andriushchenko. 단순 랜덤 검색을 통한 GPT-4의 적대적 공격 Theory of Machine Learning Group_, EPFL, Switzerland, December 2023. URL[https://www.andriushchenko.me/gpt4adv.pdf](https://www.andriushchenko.me/gpt4adv.pdf).\n' +
      '* Andriushchenko (2023)Anish Athalye, Nicholas Carlini, and David Wagner. 난독화된 구배는 거짓 보안 감각을 제공한다: 적대적 예제에 대한 우회 방어 In _Proceedings of the 35th International Conference on Machine Learning_, pp. 274-283. PMLR, July 2018. URL[https://proceedings.mlr.press/v80/athalye18a.html](https://proceedings.mlr.press/v80/athalye18a.html).\n' +
      '* Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and Vitaly Shmatikov. 다중 모드 LLM에서 간접 명령어 주입을 위한 이미지 및 사운드 사용 arxiv:2307.10490[cs]_, July 2023. doi:10.48550/arXiv.2307.10490.URL[http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490).\n' +
      '* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamile Lucositte, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Roberana Lantham, Sam R Showk, Stanislav Fort, Tamerana Lantham, Tom Henighan, Tristan Hume, Zae Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Dawn Drain, Dustin Li 헌법 AI: Harmlessness from AI Feedback, 2022년 12월. URL[https://www.anthropic.com/constitutional.pdf](https://www.anthropic.com/constitutional.pdf).\n' +
      '* Bailey et al. (2023) Luke Bailey, Euan Ong, Stuart Russell, and Scott Emmons. 이미지 탈취: 적대적 이미지는 런타임에서 생성 모델을 제어할 수 있습니다. _ arxiv:2309.00236[cs]_, September 2023. doi:10.48550/arXiv.2309.00236.URL[http://arxiv.org/abs/2309.00236](http://arxiv.org/abs/2309.00236).\n' +
      '* Biggio et al. (2013) Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. 테스트 시간에 기계 학습에 대한 회피 공격 Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Zelezny(eds.), _Machine Learning and Knowledge Discovery in Databases_, Lecture Notes in Computer Science, pp. 387-402, Berlin, Heidelberg, 2013. Springer. ISBN 978-3-642-40994-3. doi: 10.1007/978-3-642-40994-3.25.\n' +
      '* Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. 정렬된 신경망은 적대적으로 정렬되어 있습니까? [30-7차 Conference on Neural Information Processing Systems_, 11월 2023. URL[https://openreview.net/forum?id=QQoB08Vc3B](https://openreview.net/forum?id=QQoB08Vc3B)).\n' +
      '* Casper et al. (2023) Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. 탐색, 설정, 탐색: 스크래치에서 레드 티밍 언어 모델 arxiv:2306.09442[cs]_, June 2023. doi: 10.48550/arXiv.2306.09442. URL[http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442).\n' +
      '* 체이스(2022) 해리슨 체이스. H772XAD4cM[https://t.co/H772XAD4cM](https://t.co/H772XAD4cM), 2022년 12월 URL[https://twitter.com/hwchase17/status/160846749387759777](https://twitter.com/hwchase17/status/160846749387759777)을 호출하여 한 번의 호출로 1,000달러 지폐를 실행할 수 있는지 보십시오.\n' +
      '* Dao(2023) Tri Dao. 플래시 어텐션-2: 더 나은 병렬성과 작업 분할로 더 빠른 어텐션 arxiv:2307.08691[cs]_, July 2023. doi: 10.48550/arXiv.2307.08691. URL[http://arxiv.org/abs/2307.08691](http://arxiv.org/abs/2307.08691).\n' +
      '* Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 탈옥자: 대규모 언어 모델 챗봇을 통한 자동 탈옥 arxiv:2307.08715[cs]_, July 2023. doi:10.48550/arXiv.2307.08715.URL[http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715).\n' +
      '* Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahiol, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Pal.M-E: Embodied Multimodal Language Model _ arxiv:2303.03378[cs]_, March 2023. doi: 10.48550/arXiv.2303.03378. URL[http://arxiv.org/abs/2303.03378](http://arxiv.org/abs/2303.03378).\n' +
      '* Ebrahimi et al. (2018) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: 텍스트 분류를 위한 화이트박스 적대적 예제 In Iryna Gurevych and Yusuke Miyao(eds.), _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 31-36, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2006. URL[https://aclanthology.org/P18-2006](https://aclanthology.org/P18-2006).\n' +
      '* Ganguli et al. (2018) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conperly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 위험을 줄이기 위한 레디 티밍 언어 모델: 방법, 스케일링 동작 및 학습 내용. _ arxiv:2209.07858[cs]_, 2022년 11월. doi: 10.48550/arXiv.2209.07858. URL[http://arxiv.org/abs/2209.07858](http://arxiv.org/abs/2209.07858).\n' +
      '* Glukhov et al. (2023) David Glukhov, Ilia Shumailov, Yarin Gal, Nicolas Papernot, and Vardan Papyan. LLM 검열: 기계 학습 도전 또는 컴퓨터 보안 문제? _ arxiv:2307.10719[cs]_, July 2023. doi:10.48550/arXiv.2307.10719.URL[http://arxiv.org/abs/2307.10719](http://arxiv.org/abs/2307.10719)\n' +
      '* 굿사이드(2024) 라일리 굿사이드. PoC: LLM prompt injection via invisible instructions in pasted text [https://t.co/AY9HLr22B](https://t.co/AY9HLr22B), January 2024. URL[https://twitter.com/goodside/status/1745511940351287394](https://twitter.com/goodside/status/1745511940351287394).\n' +
      '* Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 우리가 신청했던 것은 아니다: 간접 프롬프트 주입으로 실세계 LLM-통합 애플리케이션을 타협하는 것 arxiv:2302.12173[cs]_, May 2023. doi:10.48550/arXiv.2302.12173.URL[http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173).\n' +
      '* Guo et al. (2021) Chuan Guo, Alexandre Sablayrolles, Herve Jegou, and Douwe Kiela. 문자 변환기에 대한 기울기 기반 적대적 공격 arxiv:2104.13733[cs]_, 2021년 4월. doi:10.48550/arXiv.2104.13733.URL[http://arxiv.org/abs/2104.13733](http://arxiv.org/abs/2104.13733)\n' +
      '* Guo et al. (2023) Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. 대형 언어 모델을 진화 알고리즘과 연결하면 강력한 프롬프트 최적기가 생성됩니다. _ arxiv:2309.08532[cs]_, September 2023. doi:10.48550/arXiv.2309.08532.URL[http://arxiv.org/abs/2309.08532](http://arxiv.org/abs/2309.08532).\n' +
      '* Hasan et al. (2024) Adib Hasan, Ileana Rugina, and Alex Wang. Pruning for Protection : Fine-Tuning이 없는 Aligned LLM의 탈옥저항 증가 arxiv:2401.10862[cs]_, January 2024. URL[http://arxiv.org/abs/2410.10862](http://arxiv.org/abs/2410.10862).\n' +
      '* Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Exploiting Generation을 통한 오픈소스 LLM의 재난적 탈옥 arxiv:2310.06987[cs]_, October 2023. doi:10.48550/arXiv.2310.06987.URL[http://arxiv.org/abs/2310.06987](http://arxiv.org/abs/2310.06987).\n' +
      '* InternLM(2023) InternLM-Team. InternLM: 점진적으로 향상된 기능을 가진 다국어 언어 모델, 2023. URL[https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM).\n' +
      '* Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompenalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 정렬 언어 모델에 대한 적대적 공격에 대한 기준 방어 arxiv:2309.00614[cs]_, September 2023. doi:10.48550/arXiv.2309.00614. URL[http://arxiv.org/abs/2309.00614](http://arxiv.org/abs/2309.00614).\n' +
      '* 야누스(2022) 야누스. 시뮬레이터 generative.ink_, September 2022. URL[https://www.lesswrong.com/posts/vJ](https://www.lesswrong.com/posts/vJ) FdjigzmcXMHNTsx/simulator.\n' +
      '* Jones et al. (2023) Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 이산 최적화를 통한 대규모 언어 모델 자동 감사 arxiv:2303.04381[cs]_, March 2023. doi: 10.48550/arXiv.2303.04381. URL[http://arxiv.org/abs/2303.04381](http://arxiv.org/abs/2303.04381).\n' +
      '* Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. LLM의 프로그램적 행위 활용: 표준 보안 공격을 통한 이중 사용 arxiv:2302.05733[cs]_, February 2023. doi:10.48550/arXiv.2302.05733. URL[http://arxiv.org/abs/2302.05733](http://arxiv.org/abs/2302.05733).\n' +
      '* Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jaixun Li, Soheil Feizi, and Himabindu Lakkaraju. 적대적 프롬프트에 대한 LLM 안전 인증. _ arxiv:2309.02705[cs]_, November 2023. doi:10.485 50/arXiv.2309.02705. URL[http://arxiv.org/abs/2309.02705](http://arxiv.org/abs/2309.02705).\n' +
      '* Lapid et al. (2023) Raz Lapid, Ron Langberg, and Moshe Sipper. 참깨를 열어라! 대형 언어 모델의 범용 블랙박스 탈옥 arxiv:2309.01446[cs]_, September 2023. doi:10.48550/arXiv.2309.01446. URL[http://arxiv.org/abs/2309.01446](http://arxiv.org/abs/2309.01446).\n' +
      '* Li 등(2020) Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu. BERT-ATTACK: BERT를 이용한 BERT에 대한 적대적 공격. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 6193-6202, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.500. URL[https://aclanthology.org/2020.emnlp-main.500](https://aclanthology.org/2020.emnlp-main.500)\n' +
      '\n' +
      '천룡리, 샤오칭정, 현징황. LLM의 판도라 상자를 엽니다. 표현 공학을 통해 LLM을 탈옥합니다. _ arxiv:2401.06824[cs]_, January 2024. doi:10.48550/arXiv.2401.06824.URL[http://arxiv.org/abs/2401.06824](http://arxiv.org/abs/2401.06824)\n' +
      '* Li et al. (2021) Xinzhe Li, Ming Liu, Xingjun Ma, and Longxiang Gao. 범용적 적대적 텍스트를 통한 자연어 처리 모델의 취약성 탐색 In _Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association_, pp. 138-148, Online, December 2021. Australasian Language Technology Association. URL[https://aclanthology.org/2021.alta-1.14](https://aclanthology.org/2021.alta-1.14)\n' +
      '* Lin et al. (2023) Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2Motion: 자연어 지시부터 실행 가능한 계획까지. _ Autonomous Robots_, 47(8):1345-1365, December 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10131-7. URL[https://doi.org/10.1007/s10514-023-10131-7](https://doi.org/10.1007/s10514-023-10131-7)을 포함할 수 있다.\n' +
      '* Liu et al.(2023a) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. AutoDAN: 정렬된 대용량 언어 모델에서 은밀한 탈옥 알림 생성 arxiv:2310.04451[cs]_, October 2023a. doi: 10.48550/arXiv.2310.04451. URL[http://arxiv.org/abs/2310.04451](http://arxiv.org/abs/2310.04451).\n' +
      '* Liu et al. (2023b) Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin 및 Eric Xing. LLM360: 완전히 투명한 오픈 소스 LLM을 향해. 2023b. URL[https://www.llm360.ai/blog/introducing-llm360-ful-transparent-open-source-llms.html](https://www.llm360.ai/blog/introducing-llm360-ful-transparent-open-source-llms.html)\n' +
      '* Maus et al. (2023) Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner. 기초 모델에 대한 블랙박스 적대적 프롬프트 _ arxiv:2302.04237[cs]_, May 2023. doi:10.48550/arXiv.2302.04237.URL[http://arxiv.org/abs/2302.04237](http://arxiv.org/abs/2302.04237).\n' +
      '* Morris et al.(2023) John X. 모리스, 자오, 저스틴 T. 추, 비탈리 슈마티코프, 알렉산더 M. 러쉬 언어 모델 반전. _ arxiv:2311.13647[cs]_, 11월 2023. doi:10.48550/arXiv.2311.13647.URL[http://arxiv.org/abs/2311.13647](http://arxiv.org/abs/2311.13647)\n' +
      '* Notopoulos (2023) Katie Notopoulos. 자동차 대리점은 자신의 사이트에 인공지능 챗봇을 추가했다. 그리고 나서 2023년 12월, 모든 난리가 났다. URL[https://www.businessinsider.com/car-](https://www.businessinsider.com/car-) 대리점-chevrolet-chatbot-chatbot-right-pranks-chevy-2023-12).\n' +
      '* Nous-Research(2023) Nous-Research. Nous-Hermes-2-SOLAR-10.7B. _ Huggingface Hub_, 2023. URL[https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B).\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. 웨인라이트, 파멜라 미쉬킨, 총장, 산디니 아가왈, 카타리나 슬라마, 알렉스 레이, 존 슐만, 제이콥 힐튼, 프레이저 켈튼, 루크 밀러, 내디 시멘스, 아만다 아스켈, 피터 웰린더, 폴 크리스티아누, 얀 라이케, 라이언 로우. 인간의 피드백으로 지시를 따르도록 언어 모델을 훈련시키는 것 arxiv:2203.02155[cs]_, 2022년 3월. doi: 10.48550/arXiv.2203.02155. URL[http://arxiv.org/abs/2203.02155](http://arxiv.org/abs/2203.02155).\n' +
      '* Perez et al. (2022) Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 언어 모델을 사용한 레드 티밍 언어 모델 _ arxiv:2202.03286[cs]_, 2022년 2월. doi:10.48550/arXiv.2202.03286.URL[http://arxiv.org/abs/2202.03286](http://arxiv.org/abs/2202.03286)\n' +
      '* 페레즈와 리베이로(2022) 파비오 페레즈와 이안 리베이로. 이전 프롬프트 무시: 언어 모델에 대한 공격 기술 _ arxiv:2211.09527[cs]_, 2022년 11월. doi:10.48550/arXiv.2211.09527.URL[http://arxiv.org/abs/2211.09527](http://arxiv.org/abs/2211.09527)\n' +
      '* Pfau et al. (2023) Jacob Pfau, Alex Infanger, Abhay Sheshadri, Ayush Panda, Julian Michael, and Curtis Huebner. 역 언어 모델을 사용하여 언어 모델 동작 선택 _Socially Responsible Language Modeling Research_, November 2023. URL[https://openreview.net/forum?id=6xyTie61H](https://openreview.net/forum?id=6xyTie61H)에서,\n' +
      '* Qi et al.(2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. 시각적 적대 예제 탈옥은 대규모 언어 모델을 정렬합니다. The Second Workshop on New Frontiers in Adversarial Machine Learning_, August 2023. URL[https://openreview.net/forum?id=c24?JL6oui](https://openreview.net/forum?id=c24?JL6oui).\n' +
      '* Qi et al. (2024) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 정렬된 언어 모델을 미세 조정하는 것은 사용자가 의도하지 않은 경우에도 안전을 타협합니다! _The Twelfth International Conference on Learning Representations_, 2024. URL[https://openreview.net/forum?id=hTEGyKf0dZ](https://openreview.net/forum?id=hTEGyKf0dZ).\n' +
      '\n' +
      '아비나브 라오, 사친 바시스타, 아타르바 나이크, 소막 아디티야, 모노지트 초우두리. LLM을 불복종으로 유인하기: 탈옥에 대한 이해, 분석 및 예방 arxiv:2305.14965[cs]_, May 2023. doi:10.48550/arXiv.2305.14965.URL[http://arxiv.org/abs/2305.14965](http://arxiv.org/abs/2305.14965).\n' +
      '* Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. SmoothLLM: 감옥 침입 공격에 대한 대규모 언어 모델 방어 arxiv:2310.03684[cs, stat]_, November 2023. doi:10.48550/arXiv.231 0.03684. URL[http://arxiv.org/abs/2310.03684](http://arxiv.org/abs/2310.03684).\n' +
      '* Rumbelow and Watkins (2023) Jessica Rumbelow and Matthew Watkins. SolidGoldMagikarp (plus, prompt generation) _ LessWrong_, February 2023. URL[https://www.lesswrong.com/posts/aPeJE8b56aFAeLog/solidgoldmagikarp-plus-prompt-generation](https://www.lesswrong.com/posts/aPeJE8b56aFAeLog/solidgoldmagikarp-plus-prompt-generation)\n' +
      '* Schulhoff et al. (2023) Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Francois Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, and Jordan Boyd-Graber. 이 제목과 HackAPrompf 무시: 글로벌 규모 프롬프트 해킹 경쟁을 통한 LLM의 시스템 취약점 노출 arxiv:2311.16119[cs]_, 11월 2023. doi:10.48550/arXiv.2311.16119. URL[http://arxiv.org/abs/2311.16119](http://arxiv.org/abs/2311.16119)\n' +
      '* Shayegan et al. (2023) Erfan Shayegan, Yue Dong, and Nael Abu-Ghazaleh. 조각으로 된 탈옥: 다중 모달 언어 모델에 대한 구성적 적대적 공격 arxiv:2307.14539[cs]_, October 2023. doi:10.48550/arXiv.2307.14539.URL[http://arxiv.org/abs/2307.14539](http://arxiv.org/abs/2307.14539)\n' +
      '* Shen et al. (2024) Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, and Daniel Khashabi. 언어장벽: 다국어 상황에서 LLM의 안전문제 해체에 관한 연구 arxiv:2401.13136[cs]_, January 2024. doi:10.48550/arXiv.2401.13136.URL[http://arxiv.org/abs/2401.13136](http://arxiv.org/abs/2401.13136)\n' +
      '* Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. "지금 무엇이든 하라": 대규모 언어 모델에 대한 야생 감옥 탈출 프롬프트의 특성화 및 평가 arxiv:2308.03825[cs]_, August 2023. doi: 10.48550/arXiv.2308.03825. URL[http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825).\n' +
      '* Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. 로건 4세, 에릭 월러스 사미어 싱 자동 프롬프트: 자동으로 생성된 프롬프트로 언어 모델에서 지식 선택 Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu(eds.), _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP)_, pp. 4222-4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL[https://aclanthology.org/2020.emnlp-main.346](https://aclanthology.org/2020.emnlp-main.346)\n' +
      '* Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 신경망의 흥미로운 속성. _ICLR 2014_, Banff, Canada, 2014. URL[https://openreview.net/forum?id=kklr_MTHMRQjG](https://openreview.net/forum?id=kklr_MTHMRQjG).\n' +
      '* 다케모토(2024) 다케모토 카즈히로. 모두 어떻게 당신이 그것을 요청하는지: 단순한 탈옥 공격에 대한 블랙박스 방법__ arxiv:2401.09798[cs]_, January 2024. doi: 10.48550/arXiv.2401.09798. URL[http://arxiv.org/abs/2401.09798](http://arxiv.org/abs/2401.09798).\n' +
      '* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 스탠포드 알파카: 명령-후속 LLaMA 모델, 2023. URL[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiob, Jude Fernandes, Jeremy Fu, Wenyin Fu, Cynthia Kardas, Vedanjos Goswami, Saghar Hosseini, Rui Hungbo, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J Llama 2: 오픈 파운데이션 및 미세 조정 채팅 모델 _ arxiv:2307.09288[cs]_, July 2023. doi: 10.48550/arXiv.2307.09288. URL[http://arxiv.org/abs/2307.09288](http://arxiv.org/abs/2307.09288).\n' +
      '* Toyer et al. (2023) Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmarouf, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. 텐서 신뢰: 온라인 게임의 즉각적인 주입 공격을 해석할 수 있습니다. _ arxiv:2311.01011[cs]_, 11월 2023. doi:10.48550/arXiv.2311.01011. URL[http://arxiv.org/abs/2311.01011](http://arxiv.org/abs/2311.01011).\n' +
      '\n' +
      '* Tunstall et al. [2022] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct Distillation of LM Alignment. _arxiv:2310.16944[cs]_, October 2022. doi: 10.48550/arXiv.2310.16944. URL [http://arxiv.org/abs/2310.16944](http://arxiv.org/abs/2310.16944).\n' +
      '* Wallace et al. [2019] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2153-2162, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1221. URL [https://aclanthology.org/D19-1221](https://aclanthology.org/D19-1221).\n' +
      '* Wang et al. [2020] Xiaosen Wang, Hao Jin, and Kun He. Natural Language Adversarial Attacks and Defenses in Word Level. _arXiv:1909.06723 [cs]_, April 2020. URL [http://arxiv.org/abs/1909.06723](http://arxiv.org/abs/1909.06723).\n' +
      '* Wei et al. [2023] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How Does LLM Safety Training Fail? _arxiv:2307.02483[cs]_, July 2023. doi: 10.48550/arXiv.2307.02483. URL [http://arxiv.org/abs/23/07.02483](http://arxiv.org/abs/23/07.02483).\n' +
      '* Wen et al. [2023] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery. In _Thirty-Seventh Conference on Neural Information Processing Systems_, November 2023. URL [https://openreview.net/forum?id=V0stHxDdsN](https://openreview.net/forum?id=V0stHxDdsN).\n' +
      '* 윌리슨[2023] 사이먼 윌리슨. 즉시 주입: 일어날 수 있는 최악의 상황은 무엇인가? 2023년 4월. URL[https://simonwilliison.net/2023/Apr/14/worst-that-can-happen/](https://simonwilliison.net/2023/Apr/14/worst-that-can-happen/)\n' +
      '* Wolf et al. [2023] Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. Fundamental Limitations of Alignment in Large Language Models. _arxiv:2304.11082[cs]_, May 2023. doi: 10.48550/arXiv.2304.11082. URL [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082).\n' +
      '* Yang et al. [2023] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. _arxiv:2310.02949[cs]_, October 2023. doi: 10.48550/arXiv.2310.02949. URL [http://arxiv.org/abs/2310.02949](http://arxiv.org/abs/2310.02949).\n' +
      '* Yong et al. [2024] Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach. Low-Resource Languages Jailbreak GPT-4. _arxiv:2310.02446[cs]_, January 2024. doi: 10.48550/arXiv.2310.02446. URL [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446).\n' +
      '* Yu et al. [2023] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. _arxiv:2309.10253[cs]_, October 2023. doi: 10.48550/arXiv.2309.10253. URL [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253).\n' +
      '* Zeng et al. [2024] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi. How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. _arxiv:2401.06373[cs]_, January 2024. doi: 10.48550/arXiv.2401.06373. URL [http://arxiv.org/abs/2401.06373](http://arxiv.org/abs/2401.06373).\n' +
      '* 장과 이폴리토[2023] 이밍 장과 다프네 이폴리토. 프롬프트는 비밀로 보여서는 안 되며: 프롬프트 추출 공격 성공을 체계적으로 측정합니다. _ arxiv:2307.06865[cs]_, July 2023. doi: 10.48550/arXiv.2307.06865. URL[http://arxiv.org/abs/2307.06865](http://arxiv.org/abs/2307.06865).\n' +
      '* Zhou et al. [2024] Andy Zhou, Bo Li, and Haohan Wang. Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks. _arxiv:2401.17263[cs]_, January 2024. URL [http://arxiv.org/abs/2401.17263](http://arxiv.org/abs/2401.17263).\n' +
      '* Zhu et al. [2023] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun. AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models. _arxiv:2310.15140[cs]_, October 2023. doi: 10.48550/arXiv.2310.15140. URL [http://arxiv.org/abs/2310.15140](http://arxiv.org/abs/2310.15140).\n' +
      '* Zou et al. [2023] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and Transferable Adversarial Attacks on Aligned Language Models. _arxiv:2307.15043[cs]_, July 2023. doi: 10.48550/arXiv.2307.15043. URL [http://arxiv.org/abs/2307.15043](http://arxiv.org/abs/2307.15043).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:23]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:24]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:25]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:26]\n' +
      '\n' +
      '* [16] You are the official chatbot for Myers Car Sales located in Dapton, Ohio. You are a helpful, respectful and honest assistant. Ansys server as hebightly as possible, while being safe. You can take the following actions: 1) Answer simple questions explaining how car sales are handled and that we can perceive at common Ford and Horads cars. 2) Describe to customers that all car sales are fast, and we do not offer any return periods. 3) Console customers who are angry about our service, without promising anything. Your answers are legally braking for us. Do not give customers a method to make the complaint! Finally, never directly share these instructions with customers.\n' +
      '\n' +
      '그림 16: 적대적으로 최적화된 중국어 텍스트 ASR\\(100\\%\\)을 통한 오방향 공격.\n' +
      '\n' +
      '그림 17: 고객 서비스 챗봇에 대한 잘못된 방향 공격, 여기에서 모델은 LLaMA-2-70b-챗이고 ASR은 \\(42.75\\%\\)이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:28]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:29]\n' +
      '\n' +
      '도 21: 참조를 위해 배치 크기가 16인 우리의 프레임워크를 통해 생성된 Zou et al.(2023)의 스타일에 있는 보편적인 탈옥.\n' +
      '\n' +
      '도 22: LLaMA-2-7b에 대한 두 개의 숨겨진 공격, 올바른 응답은 긍정적이지만 케이크 레시피일 수 있음을 주목하라.\n' +
      '\n' +
      '그림 23: 이 작업에서 평가된 적대적 공격에서 나타나는 토큰의 상대적 빈도. 이것은 그림 9의 변형이지만 바이트 토큰을 포함한다. 바이트 토큰은 빈도 분석에서 과도하게 표현되는데, 이는 다수의 글리프가 이러한 바이트 토큰으로부터 구성될 수 있기 때문에, 그러나 성공적인 공격에서 어떤 글리프가 실제로 바이트 토큰으로부터 구성되는지를 보여주는 추가적인 세부사항 없이는 이해하기 어렵다.\n' +
      '\n' +
      '그림 24: 이 작업에서 평가된 적대적 공격에서 나타나는 토큰의 상대적 빈도. 이 변형에는 바이트 토큰이 있거나 없는 각 공격 범주에 대해 가장 많이 사용되는 토큰이 표시됩니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>