<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CoLLaVO: Crayon Large Language and Vision mOdel\n' +
      '\n' +
      ' Byung-Kwan Lee\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' leebk@kaist.ac.kr\n' +
      '\n' +
      '&Beomchan Park\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' bpark@810@kaist.ac.kr\n' +
      '\n' +
      '&Chae Won Kim\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' chaewonkim@kaist.ac.kr\n' +
      '\n' +
      '&Yong Man Ro\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' ymro@kaist.ac.kr\n' +
      '\n' +
      'Corresponding author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The remarkable success of Large Language Models (LLMs) and instruction tuning drives the evolution of Vision Language Models (VLMs) towards a versatile general-purpose model. Yet, it remains unexplored whether current VLMs genuinely possess quality object-level image understanding capabilities determined from \'what objects are in the image?\' or \'which object corresponds to a specified bounding box?\'. Our findings reveal that the image understanding capabilities of current VLMs are strongly correlated with their zero-shot performance on Vision Language (VL) tasks. This suggests that prioritizing basic image understanding is crucial for VLMs to excel at VL tasks. To enhance object-level image understanding, we propose **C**rayon **L**arge **L**anguage **a**nd **V**ision **mO**del () **CoLLaVO**), which incorporates instruction tuning with _crayon prompt_ as a new visual prompt tuning scheme based on panoptic color maps. Furthermore, we present a learning strategy of _Dual QLoRA_ to preserve object-level image understanding without forgetting it during visual instruction tuning, thereby achieving a significant leap in zero-shot numerous VL benchmarks.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Spurred by the enduring ambition for artificial general intelligence (AGI) and the success of language models such as BERT [4], GPT-3 [13], and LLaMA [14], there has been a surge in demand for a general-purpose model in a task-unified format via natural language instruction, leading to the emergence of instruction tuning [15, 16]. Building on the success of Large Language Models (LLMs) and instruction tuning, InstructBLIP [15], LLaVA1.5 [17], and Qwen-VL [18] have either directly designed or utilized visual instruction tuning datasets for a wide range of Vision Language (VL) tasks using natural language instructions. Consequently, they have become paradigm-shifting in Vision Language Models (VLMs), showcasing remarkable zero-shot performance in VL tasks.\n' +
      '\n' +
      'However, it is yet uncharted whether the current leading VLMs truly possess a comprehensive understanding of fine-grained object information, and how this understanding influences their zero-shot performance in VL tasks related to each object. Hence, we delve into the analysis of object-level image understanding and zero-shot performance in VL tasks across different objects. To illustrate the behavior of object-level image understanding, we employ four strong baselines: BLIP2 [16], InstructBLIP [16], LLaVA1.5 [17], and Qwen-VL [18]. We pose two types of simple questions to gauge their object-level understanding such as: (1) _\'Is there any [object name] in this image?\' (Class2Binary: C2B)_, and (2) _\'Which object is in the specified bounding box [\\(x_{\\text{min}}\\), \\(y_{\\text{min}}\\), \\(x_{\\text{max}}\\)\n' +
      '\n' +
      'Figure 1: Zero-shot performance of CoLLaVO-7B on challenging VL datasets compared with closed-source VLMs [13, 14, 15]. Note: The scores of MME are rescaled by \\(1/20\\) to match the scales with the accuracies of others.\n' +
      '\n' +
      '_ymax1)?\' (Box2Class: B2C)_. We then evaluate the accuracy of their responses for 80 object categories (See Section 4 for more details) while assessing their zero-shot performance on VL tasks across the same set of categories.\n' +
      '\n' +
      'Following this assessment, Figure 2 illustrates that four strong baselines typically exhibit poor performance on object-level image understanding for several object categories with C2B and B2C accuracies lower than average. This phenomenon arises from various factors, such as biases in co-occurring objects or object size. In Figure 3, we observe a strong correlation between the level of object-level image understanding exhibited by VLMs and their subsequent zero-shot performance. This trend appears consistent across all four baseline VLMs. Consequently, enhancing the object-level image understanding capabilities of VLMs is expected to significantly improve their zero-shot performance in VL tasks.\n' +
      '\n' +
      'To improve object-level image understanding, we introduce a new visual prompt called _crayon prompt_ to assist VLMs in focusing more efficiently on objects. The crayon prompt starts from a panoptic segmentation model (Cheng et al., 2022) that generates a panoptic color map for any given image. This map contains semantic information for objects and their numbering. Leveraging this information, we replace both aspects with learnable queries representing semantic and numbering embeddings, correctly termed as the _crayon prompt_.\n' +
      '\n' +
      'This simple yet effective idea is inspired by the practice of drawing red circles on images (Shtedritski et al., 2023), aiming to direct attention to a specific area. They note that red circles potentially invoke the object-level image understanding of VLMs. However, they may distort image contents, posing a risk to VL tasks, and cannot consider foreground and background objects simultaneously. Instead, the crayon prompt encompasses all foreground and background objects simultaneously, thanks to the panoptic color map. Unlike drawing a visual prompt directly on an image, we integrate the crayon prompt into image embedding\n' +
      '\n' +
      'Figure 2: Asking four baselines (BLIP2, InstructBLIP, Qwen-VL, and LLaVA1.5) two types of questions, Class2Binary (C2B) and Box2Class (B2C), and measuring their accuracies on each object category.\n' +
      '\n' +
      'features at every attention module layer in the backbone Multi-modal Language Model (MLM) of \\(\\circledW\\) CoLAVo, thereby keeping the raw visual context of the image intact. The crayon prompt imparts semantic information about objects and their numbering, akin to how positional embedding (Vaswani et al., 2017) assigns sequential information to token embedding features.\n' +
      '\n' +
      'By employing the crayon prompt, we create simple crayon instructions to enhance object-level image understanding. Additionally, we utilize the visual instruction tuning datasets (Liu et al., 2023, 2023) for zero-shot VL tasks. However, naively conducting visual instruction tuning may result in the forgetting of object-level image understanding acquired from crayon instructions. Hence, we propose a learning strategy called _Dual QLoRA_ involving two QLoRA (Dettmers et al., 2023) modules. One module is trained for crayon instructions while the other module for visual instruction tuning datasets is frozen, and vice versa. This approach enables efficient fusion of crayon instructions and visual instruction tuning datasets while preserving the capabilities of both object-level image understanding and complex question answering. Pursuing parameter-efficient training, we employ quantized LoRA (QLoRA) instead of normal LoRA (Hu et al., 2021).\n' +
      '\n' +
      'Following the aforementioned methods, we propose a new large language and vision model called **Crayon** Large **L**anguage **and** **V**ision **m**O**del (\\(\\circledW\\)**CoLAVo**), where the crayon prompt and a VLM collaborate to enhance object-level image understanding, which subsequently affects zero-shot VL performance. Our contribution can be summarized as follows:\n' +
      '\n' +
      '* To the best of our knowledge, we first reveal the intriguing property of current VLMs, wherein object-level image understanding is strongly correlated with zero-shot VL performance.\n' +
      '* We propose the _crayon prompt_ and _Dual QLoRA_, which enhance object-level image understanding and effectively maintain it alongside complex VL performance, respectively.\n' +
      '* By applying all these ingredients, we present an efficient model, \\(\\circledW\\) CoLAVo-7B, which significantly achieves state-of-the-art zero-shot VL performance compared to closed-source VLMs and open-source VLMs.\n' +
      '\n' +
      '## 2 Research Backgrounds\n' +
      '\n' +
      'Visual Prompting.Researchers have prioritized enhancing natural language prompts in constructing instruction tuning datasets for LLMs (Wei et al., 2022; Chung et al., 2022; Touvron et al., 2023). On the other hand, dealing with VLMs offers new opportunities to manipulate both visual and textual aspects of prompts. Earlier studies on visual prompting have focused on techniques such as learnable token embedding concatenated with visual embedding (Jia et al., 2022; Sandler et al., 2022), or learned perturbation patterns directly applied to an input image (Bahng et al., 2022; Chen et al., 2023; Oh et al., 2023). While these methods aim to find the optimal visual prompt, the learned visual prompts lack human interpretability, hindering the understanding of their effectiveness.\n' +
      '\n' +
      'To address this, current VLMs use human-interpretable visual prompts such as marks (Shredritski et al., 2023; Yang et al., 2023; Cai et al., 2023) or semantic masks (Yang et al., 2023). Shredritski et al. (2023) draw red circles on images and then demonstrate that CLIP (Radford et al., 2021), by itself, can recognize the simple visual prompts on images, showing improved zero-shot performance for tasks such as referring expressions\n' +
      '\n' +
      'Figure 3: Plotting the regressed relationships between (a) C2B and B2C for each object category, (b) the average of C2B & B2C and zero-shot GQA (Hudson and Manning, 2019) performance for each object category, (c) the average of C2B & B2C and zero-shot TextVQA (Singh et al., 2019) performance for each object category to visualize their correlations.\n' +
      '\n' +
      'comprehension and key point localization. By using SEEM Zou et al. (2023) or SAM Kirillov et al. (2023), Yang et al. (2023) employs special marks including alphanumerics and masks to help VLMs understand fine-grained spatial information. Yang et al. (2023) uses semantic masks created by an object detection model and SAM, along with visual prompts like contour masks, colorful masks, grayscale reverse masks, and blur reverse masks, to enhance local attention in CLIP.\n' +
      '\n' +
      'In brief, previous studies have focused on guiding VLMs towards specific areas using marks and semantic masks. Similar to Yang et al. (2023), we propose _crayon prompt_ encompassing all foreground and background objects at once. However, compared with a direct visual prompt on the image Liu et al. (2023); Shtedritski et al. (2023); Yang et al. (2023); Cai et al. (2023); Yang et al. (2023), the crayon prompt is injected into image embedding features at every Transformer Vaswani et al. (2017) layer in a backbone MLM to keep the image intact and not disrupt its raw visual context. The crayon prompt provides semantic information about objects in the image and their numbering, similar to how positional embedding Vaswani et al. (2017) provides sequential information about the relative orders of token embedding features.\n' +
      '\n' +
      'LLMs, VLMs, and Instruction Tuning.Flan Wei et al. (2022) pioneered the development of instruction tuning by consolidating 62 language datasets, covering a diverse range of tasks. It demonstrates significant improvements in zero-shot performance. In efforts to expand the scope of tasks and the capacity of language models, Chung et al. (2022) introduced Flan-PaLM and Flan-T5, leveraging PaLM Chowdhery et al. (2023) and T5 Raffel et al. (2020). Continuing along the trajectory of instruction-tuned LLMs, LLaVA Liu et al. (2023) utilizes a language-only GPT-4 to produce visual dialogues, intricate deductions, and detailed image descriptions for the LLaVA-Instruct-665K dataset. Simultaneously, various VLMs Dai et al. (2023); Ye et al. (2023); Li et al. (2023); Zhu et al. (2023); Chen et al. (2023); Bai et al. (2023) have developed unique instruction tuning datasets to enhance grounding capability and mitigate hallucinations.\n' +
      '\n' +
      'Amidst the current surge of VLMs, we approach them from a fresh angle, notwithstanding the strides made in instruction tuning. Consequently, our focus shifts towards probing whether VLMs effectively grasp object-level image understanding. Should they fall short, we then question whether this inadequacy correlates with their VL performance. In essence, Figure 2-3 emphasize the importance of foundational image understanding and its potential impact on VL performance, in other words, a facet often overlooked in previous studies. Thus, we advocate for a fusion of object-level image understanding and visual instruction tuning.\n' +
      '\n' +
      '## 3 CoLLaVO\n' +
      '\n' +
      'Model Architecture and Prompt Protocol.The structure of \\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO, as illustrated in Figure 4, comprises a vision encoder, crayon prompt, a backbone MLM, and MLP connectors between the vision and language components. CLIP Radford et al. (2021) is considered as the vision encoder, benefiting from its adeptness in image understanding. The MLM utilized in \\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO is from InternLM-7B Team (2023), which is a multilingual foundation model instruction tuned by 1.6T multilingual datasets with RLHF Christiano et al. (2017); Stiennon et al. (2020); Ouyang et al. (2022). More\n' +
      '\n' +
      'Figure 4: Overview of two-step training for \\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO. Note that ‘Vision’ represents vision encoder, and that the fire symbols represent the modules to learn.\n' +
      '\n' +
      'over, two fully-connected MLPs with GELU activation function (Hendrycks and Gimpel, 2016) serve as the bridge connector. Regarding ) CoLLaVO input, adherence to a prompt protocol is maintained, where \'<image>\' signifies a special token for image embedding features, \'<stop>\' denotes a stop token for text generation, \'User: {}\' represents a question template, and \'Assistant: {}\' indicates an answer template (See below Figure 5 for an example).\n' +
      '\n' +
      'Crayon Prompt Tuning (CPT).To ensure a comprehensive object-level grasp on the entire image, CoLLaVO should recognize all distinct objects within it, including both foreground (_e.g.,_ person, bus, bottle, hairdryer, and handbag) and background (_e.g.,_ sky, road, river, sea, and snow) objects. To achieve this, we employ a panoptic segmentation model (Cheng et al., 2022), which generates a panoptic color map as illustrated in Figure 4(a). This map enables the discrimination of 133 different object categories (See Appendix A) of foreground and background objects from MSCOCO 2017 (Lin et al., 2014), serving as a visual cue for ) CoLLaVO to focus on all objects within the image.\n' +
      '\n' +
      'Notably, the panoptic map contains two crucial pieces of information for each object: semantic information and numbering information. For instance, if an image depicts two people and a bus on the road, as illustrated in Figure 4(b), the panoptic map assigns each object a category label and a numbering index, as shown in Figure 5. The two people receive different numbering indices \'1\' and \'2\' but share the same object category \'person\'. Other objects, being singular, are all labeled with the numbering index \'1\'. It is worth noting that the unknown category is assigned the numbering index \'0\'. To streamline the next process, we prepare 133+1(**unk**) learnable semantic queries, including the aforementioned 133 categories and an **unk**nown category. In addition, we prepare 20+1(\'0\' for **unk**) learnable numbering queries under the assumption that no more than 20 instances of the same object category appear within one image.\n' +
      '\n' +
      'Leveraging 134 semantic queries and 21 numbering queries, we then replace both the semantic and numbering color maps with these queries, akin to generating vector quantized features through a codebook mechanism (Van Den Oord et al., 2017; Esser et al., 2021). This process results in the generation of semantic and numbering embeddings in Figure 5, which are subsequently combined in the backbone MLM. This combined representation is referred to as _crayon prompt_. The crayon prompt meets the MLP connector, and then its output is added with the image features at every attention module layer in the MLM as shown in Figure 6(a). We then utilize crayon instructions, as shown in the lower half of Figure 5, and perform _crayon prompt tuning_ (CPT) to align the crayon prompt to the backbone MLM and enhance object-level image understanding. Here, the magenta colored-text is auto-regressively learned, as demonstrated in the crayon instruction example below Figure 5.\n' +
      '\n' +
      'Figure 5: Describing how crayon prompt is generated from the panoptic color map with learnable semantic queries and numbering queries. In addition, crayon instruction examples are given, which are used to conduct CPT and CIT. Note that, ‘{}’ denotes the place where we adaptively input information.\n' +
      '\n' +
      '**Crayon prompt-based Instruction Tuning (CIT).** CPT focuses solely on learning semantic and numbering queries in the crayon prompt and its MLP connector with the MS-COCO 2017 dataset (Lin et al., 2014), aligning them with the backbone MLM to enhance object-level image understanding of \\(\\Join\\)CoLLaVO. On the other hand, _crayon prompt-based instruction tuning_ (CIT) utilizes the visual instruction tuning datasets (Liu et al., 2023, 20; Chen et al., 2023) as well as crayon instructions to handle complex question answering for VL tasks. It involves training the semantic and numbering queries and the MLP connector again, this time along with the backbone MLM of \\(\\Join\\)CoLLaVO.\n' +
      '\n' +
      'When training the MLM with CIT, we introduce a learning strategy called _Dual QLoRA_, which manages object-level image understanding and complex VL performance, respectively, to effectively maintain both aspects. Figure 6 provides an overview of Dual QLoRA, where _Image-CIT_ denotes using crayon instructions to bootstrap object-level image understanding and training only the first QLoRA module, while _VL-CIT_ indicates using complex question-answer pairs from visual instruction tuning datasets to achieve zero-shot VL performance and training only the second QLoRA module. During CIT, we present an image in the form of crayon prompt to \\(\\Join\\)CoLLaVO, and randomly determine whether to proceed with Image-CIT or VL-CIT. The overarching objective of Dual QLoRA is to efficiently preserve both capabilities of object-level image understanding and complex VL performance. Note that the key distinction between CPT and Image-CIT lies in whether the backbone MLM of \\(\\Join\\)CoLLaVO is trained or not. Further details on minor aspects will be addressed in the following section.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**Implementation Details of \\(\\Join\\)CoLLaVO.** To ensure successful reproducibility, we outline the following five crucial technical details of \\(\\Join\\)CoLLaVO: (a) QLoRA, (b) crayon prompt, (c) instruction detail of Image-CIT and VL-CIT, (d) training hyper-parameters, and (e) text-generation.\n' +
      '\n' +
      '**(a)**: we employ Quantized Low-Rank Adaptation (QLoRA) (Hu et al., 2021; Dettmers et al., 2023) since \\(\\Join\\)CoLLaVO pursues efficient training with minimal parameter tuning. Double quantization and normalized float 4-bit (nf4) are used with LoRA of \\(r=64\\) and \\(\\alpha=64\\). **(b)**: In contrast to CPT with only crayon instructions and images from MS-COCO 2017, CIT is conducted with visual instruction tuning datasets (Liu et al., 2023, 20; Chen et al., 2023) as well. Hence, many images contain unrecognizable objects, such as text, code, posters, or mathematical symbols. Consequently, a panoptic color map with the unknown category and \'0\' numbering will be generated, and the semantic queries of the **unk** category and numbering queries of \'0\' will operate to create the crayon prompt in these cases. **(c)**: Once the color map is given with discernible objects, text descriptions, including object names, their numbering indices, and their bounding box coordinates, are added to the question template. Conversely, if an image contains no objects, the question template includes the phrase "None of detailed object information\n' +
      '\n' +
      'Figure 6: Illuminating (a) how the crayon prompt is injected into image embedding features and learning strategies of (b), (c) Dual QLoRA for the object-level image understanding capability (Image-CIT) and VL task capability (VL-CIT) to efficiently coexist without catastrophic forgetting (Luo et al., 2023).\n' +
      '\n' +
      'for image." **(d)**: Regarding training, we train CoLLaVO with a batch size of 32 in one epoch using the AdamW (Loshchilov and Hutter, 2019) optimizer, scheduled by cosine annealing (Loshchilov and Hutter, 2016) from a learning rate of 1e-4 to 1e-6 for CPT and from 1e-5 to 1e-6 for CIT, respectively. **(e)**: To find best performances, **CoLLaVO** uses greedy or beam search (\\(n=3\\)) for text generation without any other hyper-parameter.\n' +
      '\n' +
      'Object-level Image Understanding.Before delving into validating CoLLaVO in VL tasks, it is crucial to ensure its proficiency in object-level image understanding. We assessed the accuracy of 80 object categories classified as \'thing\' (See Appendix A) in the MS-COCO 2017 across two directions: Class2Binary (C2B) and Box2Class(B2C), using four strong baselines: BLIP2, InstructBLIP, Qwen-VL, and LLaVA1.5. As illustrated in Figure 7(a)-(b), **CoLLaVO** nearly outperforms the baselines in three cases: Top-20, Bot-20, and Avg for both C2B and B2C. Furthermore, It has the smallest performance gap between the Top-20 accuracy and the Bottom-20 accuracy for both C2B and B2C. Such observation indicates that CoLLaVO has a solid object-level image understanding across numerous object classes.\n' +
      '\n' +
      'Zero-shot VL Evaluation.Following improved object-level image understanding, CoLLaVO is evaluated to measure zero-shot performance of VL tasks on 17 renowned datasets (See Appendix B). As shown in Figure 1, 7(c), and Table 1, **CoLLaVO** surpasses several closed-source VLMs like GPT-4V, Gemini-Pro, Qwen-VL-Pro, as well as numerous open-source VLMs (See Appendix C for all VLMs used in evaluation). Particularly, noteworthy is its superiority over other models in the following benchmarks: MME, MM-Bench, MM-Bench-Chinese, and Q-Bench, which primarily evaluate visual perception and cognition abilities, where CoLLaVO demonstrates its significant advancements.\n' +
      '\n' +
      'The effectiveness of Crayon Prompt and CIT.We ablate the following factors in CoLLaVO: semantic embedding in crayon prompt, numbering embedding in crayon prompt, Dual QLoRA, Image-CIT, and VL-CIT. As illustrated in Table 2, it is evident that the semantic and numbering embedding in the crayon prompt significantly boost the zero-shot performance of CoLLaVO on MME dataset. It is noteworthy that the semantic embedding alone can improve the zero-shot performance by a large margin, especially in MME-P with \'E&P\' scores, implying that injecting object-level semantics helps the model perceive the existence of objects better for solid object-level image understanding. Moreover, the numbering embedding considerably boosts the \'Count\' score, demonstrating its effectiveness in differentiating objects of the same category by further refining the performance.\n' +
      '\n' +
      'Table 3 demonstrates that Dual QLoRA, Image-CIT, and VL-CIT contribute to improving zero-shot performance, respectively. VL-CIT alone exhibits better performance of \\(1599.2\\) in MME-P and \\(414.1\\) in MME-C over other open-source VLMs, with the assistance of the crayon prompt. Additionally, Image-CIT also enhances performance, albeit to a limited extend without QLoRA, by integrating crayon instructions into CIT as well as CPT. Finally, Dual QLoRA produces the most significant improvement, demonstrating its efficacy in fully leveraging both aspects of Image-CIT and VL-CIT.\n' +
      '\n' +
      '## 5 Discussion and Conclusion\n' +
      '\n' +
      'We have showcased the effectiveness of **CoLLaVO** alongside _crayon prompts_ and _Dual QLoRA_ serving as a key in enhancing the object-level image understanding. Notably, Figure 8(a) illustrates the impressive ability of **CoLLaVO**\n' +
      '\n' +
      'Figure 7: In (a) and (b), the mean accuracy over the Top-20 object categories is denoted by the blue triangles, that over the Bottom-20 is denoted by the green triangles, and the red squares represent the average of all categories to visualize object-level image understanding of VLMs. In (c), zero-shot performance of VLMs on several VL benchmarks (MME-P (1/20 of score), GQA, TextVQA, SQA-IMG, SEED-IMG (accuracy)) are shown.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      'Crayon prompts, relying on a panoptic color map, an external source beyond VLMs, may be constrained by the performance of the segmentation model and its encompassing number of object classes. Despite this, we have achieved commendable scores across all zero-shot tasks. It is expected for \\(\\mathbf{\\mathsf{\\hat{w}}}\\)**CoLLaVO** to further improve once it incorporates a plethora of visual prompts obtained from diverse sources like robust object classification or image captioning models (Lee et al., 2020, 2022, 2023; Kim et al., 2023c), object-centric causally human-interpretable information (Kim et al., 2021, 2023b), open object detection (Zhang et al., 2023a), visual grounding (Liu et al., 2023d; Ren et al., 2024), interactive or unsupervised segmentation (Kirillov et al., 2023; Kim et al., 2023a), and optical characteristic recognition model (Bautista and Atienza, 2022).\n' +
      '\n' +
      '## 7 Ethics Statement\n' +
      '\n' +
      'We affirm that all research presented in this paper adheres to the principles of ethical conduct and integrity. The experiments conducted and the results reported are based on rigorous scientific methods and strive to contribute positively to the field of vision language models. All datasets used in this study: MS-COCO 2017 (Lin et al., 2014) and visual instruction datasets (Liu et al., 2023c,b; Chen et al., 2023d) were obtained and analyzed in compliance with relevant regulations and guidelines for research ethics and data privacy. In addition, any potential limitations have been transparently discussed, so we are committed to upholding the highest standards of integrity, accountability, and respect for communities affected by our research.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bahng et al. (2022) Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022. Exploring visual prompts for adapting large-scale models. _arXiv preprint arXiv:2203.17274_.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_.\n' +
      '* Bautista and Atienza (2022) Darwin Bautista and Rowel Atienza. 2022. Scene text recognition with permuted autoregressive sequence models. In _European Conference on Computer Vision_, pages 178-196. Springer.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Cai et al. (2023) Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. 2023. Making large multimodal models understand arbitrary visual prompts. _arXiv preprint arXiv:2312.00784_.\n' +
      '* Chen et al. (2023a) Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. 2023a. Understanding and improving visual prompting: A label-mapping perspective. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19133-19143.\n' +
      '* Chen et al. (2023b) Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023b. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_.\n' +
      '* Chen et al. (2023c) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023c. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint arXiv:2306.15195_.\n' +
      '* Chen et al. (2023d) Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023d. Sharegpt4v: Improving large multimodal models with better captions. _arXiv preprint arXiv:2311.12793_.\n' +
      '* Cheng et al. (2022) Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. Masked-attention mask transformer for universal image segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1290-1299.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Marcic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_.\n' +
      '* Contributors (2023) XTuner Contributors. 2023. Xtuner: A toolkit for efficiently fine-tuning llm. [https://github.com/InternLM/xtuner](https://github.com/InternLM/xtuner).\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_.\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.\n' +
      '* Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12873-12883.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_.\n' +
      '* Gong et al. (2023) Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. _arXiv preprint arXiv:2305.04790_.\n' +
      '* Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_.\n' +
      '* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_.\n' +
      '* Hu et al. (2021)Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709.\n' +
      '* Iyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1821-1831.\n' +
      '* Jia et al. (2022) Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022. Visual prompt tuning. In _European Conference on Computer Vision_, pages 709-727. Springer.\n' +
      '* Kembhavi et al. (2016) Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. A diagram is worth a dozen images. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer.\n' +
      '* Kim et al. (2021) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2021. Distilling robust and non-robust features in adversarial examples by information bottleneck. _Advances in Neural Information Processing Systems_, 34:17148-17159.\n' +
      '* Kim et al. (2023a) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2023a. Causal unsupervised semantic segmentation. _arXiv preprint arXiv:2310.07379_.\n' +
      '* Kim et al. (2023b) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2023b. Demystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12302-12312.\n' +
      '* Kim et al. (2023c) Yeonju Kim, Junho Kim, Byung-Kwan Lee, Sebin Shin, and Yong Man Ro. 2023c. Mitigating dataset bias in image captioning through clip confounder-free captioning network. In _2023 IEEE International Conference on Image Processing (ICIP)_, pages 1720-1724. IEEE.\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 2023. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4015-4026.\n' +
      '* Laurencon et al. (2023) Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. 2023. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. _arXiv preprint arXiv:2306.16527_.\n' +
      '* Lee et al. (2022) Byung-Kwan Lee, Junho Kim, and Yong Man Ro. 2022. Masking adversarial damage: Finding adversarial saliency for robust and sparse network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15126-15136.\n' +
      '* Lee et al. (2023) Byung-Kwan Lee, Junho Kim, and Yong Man Ro. 2023. Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4499-4509.\n' +
      '* Lee et al. (2020) Byung-Kwan Lee, Youngjoon Yu, and Yong Man Ro. 2020. Towards adversarial robustness of bayesian neural network through hierarchical variational inference.\n' +
      '* Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: A multi-modal model with in-context instruction tuning. _arXiv preprint arXiv:2305.03726_.\n' +
      '* Li et al. (2023b) Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023b. Seed-bench: Benchmarking multimodal llms with generative comprehension. _arXiv preprint arXiv:2307.16125_.\n' +
      '* Li et al. (2023c) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. Bip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_.\n' +
      '* Li et al. (2023d) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Evaluating object hallucination in large vision-language models. _arXiv preprint arXiv:2305.10355_.\n' +
      '* Li et al. (2023e) Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. 2023e. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pages 740-755. Springer.\n' +
      '* Liu et al. (2023a) Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. _arXiv preprint arXiv:2310.14566_.\n' +
      '* Liu et al. (2023b) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_.\n' +
      '* Liu et al. (2023c)* Liu et al. (2023c) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023c. Visual instruction tuning. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Liu et al. (2023d) Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023d. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_.\n' +
      '* Liu et al. (2023e) Weihuang Liu, Xi Shen, Chi-Man Pun, and Xiaodong Cun. 2023e. Explicit visual prompting for low-level structure segmentations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19434-19445.\n' +
      '* Liu et al. (2023f) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023f. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_.\n' +
      '* Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm restarts. _arXiv preprint arXiv:1608.03983_.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In _International Conference on Learning Representations_.\n' +
      '* Lu et al. (2023) Pan Lu, Hrtik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. _arXiv preprint arXiv:2310.02255_.\n' +
      '* Luo et al. (2023) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. _arXiv preprint arXiv:2308.08747_.\n' +
      '* Oh et al. (2023) Changdae Oh, Hyeji Hwang, Hee-young Lee, YongTaek Lim, Geunyoung Jung, Jiyoung Jung, Hosik Choi, and Kyungwoo Song. 2023. Blackvip: Black-box visual prompting for robust transfer learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24224-24235.\n' +
      '* OpenAI (2023a) OpenAI. 2023a. Gpt-4v(sision) system card. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card), Last accessed on 2024-02-13.\n' +
      '* OpenAI (2023b) OpenAI. 2023b. Gpt-4v(sision) technical work and authors. [https://openai.com/contributions/gpt-4v](https://openai.com/contributions/gpt-4v), Last accessed on 2024-02-13.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Ren et al. (2024) Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. _arXiv preprint arXiv:2401.14159_.\n' +
      '* Sandler et al. (2022) Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Andrew Jackson. 2022. Fine-tuning image transformers using learnable memory. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12155-12164.\n' +
      '* Shtedritski et al. (2023) Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. 2023. What does clip know about a red circle? visual prompt engineering for vlms. _arXiv preprint arXiv:2304.06712_.\n' +
      '* Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8317-8326.\n' +
      '* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_.\n' +
      '* Team (2023) InternLM Team. 2023. Internlm: A multilingual language model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM-techreport](https://github.com/InternLM/InternLM-techreport).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2020)Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Van Den Oord et al. (2017) Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. _Advances in neural information processing systems_, 30.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_, 30.\n' +
      '* Wang et al. (2023) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. _arXiv preprint arXiv:2311.03079_.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_.\n' +
      '* Wu et al. (2023) Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. 2023. Q-bench: A benchmark for general-purpose foundation models on low-level vision. _arXiv preprint arXiv:2309.14181_.\n' +
      '* Yang et al. (2023a) Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023a. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. _arXiv preprint arXiv:2310.11441_.\n' +
      '* Yang et al. (2023b) Lingfeng Yang, Yuez Wang, Xiang Li, Xinlong Wang, and Jian Yang. 2023b. Fine-grained visual prompting. In _Thirty-seventh Conference on Neural Information Processing Systems_.\n' +
      '* Ye et al. (2023a) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_.\n' +
      '* Ye et al. (2023b) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. _arXiv preprint arXiv:2308.02490_.\n' +
      '* Zhang et al. (2023a) Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. 2023a. A simple framework for open-vocabulary segmentation and detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1020-1031.\n' +
      '* Zhang et al. (2023b) Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Duyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. 2023b. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. _arXiv preprint arXiv:2309.15112_.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_.\n' +
      '* Zou et al. (2023) Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. 2023. Segment everything everywhere all at once. _arXiv preprint arXiv:2304.06718_.\n' +
      '\n' +
      'COCO Classes for Panoptic Color Map\n' +
      '\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline \\multicolumn{4}{c}{COCO Panoptic Classes} \\\\ \\hline person & bicycle & car & motorcycle \\\\ airplane & bus & train & truck \\\\ boat & traffic light & fire hydrant & stop sign \\\\ parking meter & bench & bird & cat \\\\ dog & horse & sheep & cow \\\\ elephant & bear & zebra & giraffe \\\\ backpack & umbrella & handbag & tie \\\\ suitcase & frisbee & skis & snowboard \\\\ sports ball & kite & baseball bat & baseball glove \\\\ skateboard & surfboard & tennis racket & bottle \\\\ wine glass & cup & fork & knife \\\\ spoon & bowl & banana & apple \\\\ sandwich & orange & broccoli & carrot \\\\ hot dog & pizza & donut & cake \\\\ chair & couch & potted plant & bed \\\\ dining table & toilet & tv & laptop \\\\ mouse & remote & keyboard & cell phone \\\\ microwave & oven & toaster & sink \\\\ refrigerator & book & clock & vase \\\\ scissors & teddy bear & hair drier & toothbrush \\\\ banner\\({}^{*}\\) & blanket\\({}^{*}\\) & bridge\\({}^{*}\\) & cardboard\\({}^{*}\\) \\\\ counter\\({}^{*}\\) & curtain\\({}^{*}\\) & door\\({}^{*}\\) & floor-wood\\({}^{*}\\) \\\\ flower\\({}^{*}\\) & fruit\\({}^{*}\\) & gravel\\({}^{*}\\) & house\\({}^{*}\\) \\\\ light\\({}^{*}\\) & mirror\\({}^{*}\\) & net\\({}^{*}\\) & pillow\\({}^{*}\\) \\\\ platform\\({}^{*}\\) & playingfield\\({}^{*}\\) & railroad\\({}^{*}\\) & river\\({}^{*}\\) \\\\ road\\({}^{*}\\) & roof\\({}^{*}\\) & sand\\({}^{*}\\) & sea\\({}^{*}\\) \\\\ shelf\\({}^{*}\\) & snow\\({}^{*}\\) & stairs\\({}^{*}\\) & tent\\({}^{*}\\) \\\\ towel\\({}^{*}\\) & wall-brick\\({}^{*}\\) & wall-stone\\({}^{*}\\) & wall-tile\\({}^{*}\\) \\\\ wall-wood\\({}^{*}\\) & water\\({}^{*}\\) & window-blind\\({}^{*}\\) & window\\({}^{*}\\) \\\\ tree\\({}^{*}\\) & fence\\({}^{*}\\) & ceiling\\({}^{*}\\) & sky\\({}^{*}\\) \\\\ cabinet\\({}^{*}\\) & table\\({}^{*}\\) & floor\\({}^{*}\\) & pavement\\({}^{*}\\) \\\\ mountain\\({}^{*}\\) & grass\\({}^{*}\\) & dirt\\({}^{*}\\) & paper\\({}^{*}\\) \\\\ food\\({}^{*}\\) & building\\({}^{*}\\) & rock\\({}^{*}\\) & wall\\({}^{*}\\) \\\\ rug\\({}^{*}\\) & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '* Object class that is not classified as \'thing\' (countable) but\'stuff\' (uncountable)\n' +
      '\n' +
      '## Appendix B Zero-shot Vision Language Datasets used in Evaluation\n' +
      '\n' +
      '* **GQA**(Hudson and Manning, 2019) is a visual question answering dataset comprising real-world images annotated with scene graphs. It tackles the issue of semantic compositionality by utilizing semantic representations of scenes and questions. It encompasses 22 million questions covering a wide array of images, each associated with structured representations of image objects, attributes, and relations.\n' +
      '* **SQA-IMG**(Iyyer et al., 2017), a subset of the ScienceQA (SQA) dataset that includes image context, comprises 10,332 multiple-choice questions sourced from elementary and high school science education materials, covering diverse sub-fields. A majority of the questions in the SQA dataset are accompanied by supplementary lectures (83.9%) and detailed explanations (90.5%), enriching understanding with broader knowledge and specific reasoning for correct answers.\n' +
      '* **TextVQA**(Singh et al., 2019) is a large-scale complex benchmark to analyze and understand text embedded within images in order to respond to associated questions. This involves integrating textual information present within images and reasoning over it to provide answers. The dataset comprises 28,408 images sourced from OpenImages, accompanied by 45,336 questions and 453,360 corresponding ground truth answers.\n' +
      '* **POPE**(Li et al., 2023d) serves as a polling-based binary classification query dataset, tailored to assess object hallucination challenges within VLMs. It comprises three distinct subsets, _i.e.,_ random, popular, and adversarial, each crafted using varied sampling techniques, resulting in a total of 8,910 entries.\n' +
      '* **MME**(Fu et al., 2023) is introduced as a novel comprehensive benchmark aimed at assessing the performance of VLMs by measuring both perception and cognition abilities across 14 subtasks. To mitigate potential data leakage issues associated with public datasets, all annotations for instruction-answer pairs are manually designed.\n' +
      '* **MMBench, MMBench-Chinese**(Liu et al., 2023f) establish a comprehensive evaluation framework spanning multiple modalities. These frameworks encompass around 3000 multiple-choice questions addressing 20 distinct capability dimensions in both English and Chinese languages. An innovative approach is introduced through the integration of ChatGPT into the evaluation process.\n' +
      '* **MM-Vet**(Yu et al., 2023) is a multi-modal assessment benchmark that assesses a broad range of capabilities essential for handling real-world scenarios, such as solving mathematical problems or interpreting visual humor. The dataset consists of 187 images collected from diverse online platforms and presents 205 questions, each requiring the application of one or more capabilities for an answer. These questions vary in type and necessitate open-ended responses of varying lengths.\n' +
      '* **Q-Bench**(Wu et al., 2023) evaluates VLMs across three dimensions relevant to low-level vision: perception, description, and assessment. To assess perception, the framework utilizes 2,990 diverse images, each accompanied by a human-generated question focusing on its low-level attributes. For evaluating VLMs\' description regarding low-level information, human-labeled textual descriptions for 499 images are utilized, alongside a comparison pipeline involving GPT. Additionally, the framework assesses VLMs\' visual quality assessment abilities, aiming to align with human opinion scores.\n' +
      '* **MathVista**(Lu et al., 2023) assesses VLMs\' mathematical reasoning ability within visual contexts, with 6,141 examples sourced from 28 existing multimodal datasets on mathematics. MathVista provides a comprehensive evaluation platform, requiring meticulous visual comprehension and compositional reasoning, posing challenges even to state-of-the-art foundational models.\n' +
      '* **AI2D**(Kembhavi et al., 2016), or AI2 Diagrams, is a dataset comprising over 5,000 grade school science diagrams. It includes comprehensive annotations of constituents and relationships, along with rich syntactic parses and over 15,000 corresponding multiple-choice questions.\n' +
      '* **SEED-IMG**(Li et al., 2023b) comprises a subset of SEED-Bench, focusing on the image modality. The original SEED-Bench includes 19,000 multiple-choice questions with precise human annotations, covering 12 evaluation dimensions, including comprehension of both image and video modalities.\n' +
      '* **HallusionBench**(Liu et al., 2023a) introduces a comprehensive benchmark tailored for evaluating image-context reasoning abilities. It prioritizes nuanced comprehension and interpretation of visual information. The benchmark consists of 346 images accompanied by 1129 expert-crafted questions, enabling a quantitative analysis of model response tendencies, logical consistency, and diverse failure modes.\n' +
      '\n' +
      '## Appendix C Vision Language Models used in Evaluation\n' +
      '\n' +
      '* **BLIP2**[11] introduces Q-Former that serves as an intermediary between frozen unimodal models, extracting pertinent visual features from a frozen image encoder and providing them to a frozen large language model to generate text.\n' +
      '* **InstructBLIP**[23] presents a vision-language instruction tuning framework designed to address the challenges of generalizing to diverse tasks, through a systematic study involving 26 datasets transformed into instruction tuning format across 11 task categories.\n' +
      '* **Shikra**[1] proposes a unified model designed for referential dialogue tasks, which encompass various vision-language tasks such as VQA, image captioning, and location-related tasks like referring expression comprehension and PointQA.\n' +
      '* **IDEFICS**[12] introduces a curated web-scale dataset comprising 141 million multimodal English web documents, each containing associated images and text, totaling 353M images and 115B tokens. They aim to provide full multimodal documents preserving the natural context of images within web pages.\n' +
      '* **Qwen-VL, Qwen-VL-Chat**[23] introduces Qwen-VL series, a collection of highly performant and versatile vision-language models based on Qwen language model. They support multiple languages and handling of multi-image inputs, and fine-grained visual understanding capabilities.\n' +
      '* **MiniGPT-4**[24] presents a vision-language model that combines Vicuna with freezed pre-trained vision components of Q-Former from BLIP2, aiming to replicate the exceptional capabilites demonstrated by GPT-4.\n' +
      '* **MiniGPT-v2**[1] is designed to effectively handle multiple vision-language tasks by employing a task-oriented instruction training scheme, through three training stage and utilization of higher-resolution images.\n' +
      '* **Otter**[11] addresses the gap between DeepMind Flamingo by employing OpenFlamingo and multi-modal in-context instruction tuning (MIMIC-IT) dataset.\n' +
      '* **LLaVA**[12] first introduces the concept of visual instruction tuning, extending language only instruction tuning to vision language instruction tuning to develop a general-purpose visual assistant.\n' +
      '* **LLaVA-XTuner**[13] is a tool to fine-tune LLaVA to achieve general-purpose model.\n' +
      '* **mPLUG-Owl**[25] introduces a modularized training paradigm for large multi-modal language models capable of supporting multiple modalities simultaneously. Inspired by modularization concepts, their method integrates pre-trained language models, visual knowledge modules, and visual abstractor modules to achieve effective alignment between images and text.\n' +
      '* **mPLUG-Owl2**[25] features a modularized network design to handle both modality collaboration and interference. They introduce shared functional modules to promote collaboration and a modality-adaptive module to manage different modalities effectively.\n' +
      '* **ShareGPT4V**[1] argues that current Large multi-modal models face sub-optimal modality alignment due to the lack of high-quality image-text pairs. To address this issue, they collected high-quality captions on a larger scale in two phases. This effort led to the creation of the ShareGPT4V dataset, comprising 100K GPT4-Vision generated captions and 1.2M captions crafted by their caption model.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>