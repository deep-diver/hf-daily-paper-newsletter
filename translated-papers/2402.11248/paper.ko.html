<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#CoLLaVO: Crayon Large Language and Vision mOdel\n' +
      '\n' +
      ' 이병관\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' leebk@kaist.ac.kr\n' +
      '\n' +
      '&Beomchan Park\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' bpark@810@kaist.ac.kr\n' +
      '\n' +
      '김채원\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' chaewonkim@kaist.ac.kr\n' +
      '\n' +
      '용만로\n' +
      '\n' +
      ' KAIST\n' +
      '\n' +
      ' ymro@kaist.ac.kr\n' +
      '\n' +
      'Corresponding author.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)과 명령어 튜닝의 놀라운 성공은 비전 언어 모델(VLM)을 다목적 범용 모델로 진화시킨다. 그러나 현재 VLM이 \'이미지 내에 어떤 객체가 있는가?\' 또는 \'어떤 객체가 지정된 경계 상자에 해당하는가?\'에서 결정된 품질 객체 수준의 이미지 이해 능력을 실제로 보유하고 있는지 여부는 아직 조사되지 않았다. 본 연구 결과는 현재 VLM의 이미지 이해 능력이 비젼 언어(VL) 작업에서 제로 샷 성능과 강한 상관 관계가 있음을 보여준다. 이는 VLM이 VL 작업에서 탁월하기 위해서는 기본 이미지 이해의 우선 순위가 중요함을 시사한다. 객체 수준의 이미지 이해도를 높이기 위해, 우리는 판옵틱 컬러 맵에 기반한 새로운 시각적 프롬프트 튜닝 방식으로 _crayon prompt_를 사용하여 명령어 튜닝을 통합하는 **C**rayon **L**arge **L**anguage **a**nd **V**ision **mO**del() **CoLLaVO**를 제안한다. 또한, 비주얼 명령어 튜닝 시 객체 수준의 이미지 이해를 잊지 않고 보존하기 위한 _Dual QLoRA_의 학습 전략을 제시하여 제로샷 수많은 VL 벤치마크에서 상당한 비약을 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인공 일반 지능(AGI)에 대한 지속적인 야망과 BERT[4], GPT-3[13], LLaMA[14]와 같은 언어 모델의 성공으로 인해 촉발된 자연 언어 수업을 통한 과제 통합 형식의 범용 모델에 대한 수요가 급증하여 명령어 튜닝[15, 16]이 등장했다. LLM(Large Language Models)과 명령어 튜닝의 성공을 기반으로 InstructBLIP[15], LLaVA1.5[17], Qwen-VL[18]은 자연어 명령어를 사용하여 광범위한 비젼 언어(VL) 작업에 대한 시각적 명령어 튜닝 데이터 세트를 직접 설계하거나 활용했다. 그 결과, 비젼 언어 모델(VLM)의 패러다임이 바뀌면서 VL 태스크에서 괄목할 만한 제로샷 성능을 보여주었다.\n' +
      '\n' +
      '그러나 현재 주요 VLM이 실제로 세립 객체 정보에 대한 포괄적인 이해를 가지고 있는지 여부와 이러한 이해가 각 객체와 관련된 VL 태스크에서 제로 샷 성능에 어떻게 영향을 미치는지 아직 미지수이다. 따라서 본 논문에서는 VL 태스크의 객체 수준 이미지 이해도 및 제로샷 성능 분석을 서로 다른 객체에 대해 분석한다. 객체 수준 이미지 이해의 동작을 설명하기 위해 BLIP2[16], InstructBLIP[16], LLaVA1.5[17], Qwen-VL[18]의 네 가지 강력한 기준선을 사용한다. 본 논문에서는 (1) 이 이미지에 [객체명]이 있는가? (Class2Binary: C2B)_와 (2) 지정된 경계 상자에 어떤 객체가 있는가[\\(x_{\\text{min}}\\), \\(y_{\\text{min}}\\), \\(x_{\\text{max}}\\)와 같은 두 가지 유형의 간단한 질문을 제시한다.\n' +
      '\n' +
      '그림 1: 폐쇄 소스 VLM [13, 14, 15]과 비교하여 도전 VL 데이터 세트에 대한 CoLLaVO-7B의 제로 샷 성능. 주: MME의 점수는 다른 사람들의 정확도와 척도를 일치시키기 위해 \\(1/20\\)만큼 재조정된다.\n' +
      '\n' +
      '_ymax1)?\'(Box2Class: B2C)_. 그런 다음 동일한 카테고리 세트에 걸쳐 VL 태스크에 대한 제로 샷 성능을 평가하면서 80개 객체 카테고리(자세한 내용은 섹션 4 참조)에 대한 응답의 정확도를 평가한다.\n' +
      '\n' +
      '이 평가 후, 그림 2는 C2B 및 B2C 정확도가 평균보다 낮은 여러 객체 범주에 대해 4개의 강력한 기준선이 일반적으로 객체 수준 이미지 이해에서 좋지 않은 성능을 나타냄을 보여준다. 이러한 현상은 동시 발생 객체 또는 객체 크기의 편향과 같은 다양한 요인에서 발생한다. 그림 3에서 VLM이 나타내는 객체 수준 이미지 이해 수준과 후속 제로 샷 성능 사이의 강한 상관 관계를 관찰한다. 이 경향은 4개의 기준 VLM 모두에서 일관되게 나타난다. 결과적으로, VLM의 객체 수준 이미지 이해 능력을 향상시키면 VL 작업에서 제로 샷 성능이 크게 향상될 것으로 예상된다.\n' +
      '\n' +
      '객체 수준의 이미지 이해도를 향상시키기 위해, 우리는 VLM이 객체에 더 효율적으로 초점을 맞추는 것을 돕기 위해 _crayon prompt_라는 새로운 시각적 프롬프트를 소개한다. 크레용 프롬프트는 임의의 주어진 이미지에 대한 판옵틱 컬러 맵을 생성하는 판옵틱 분할 모델(Cheng et al., 2022)로부터 시작한다. 이 지도에는 객체와 번호 매기기에 대한 의미 정보가 포함되어 있습니다. 이 정보를 활용하여 우리는 두 측면을 모두 _crayon prompt_라고 올바르게 명명된 의미적 임베딩과 넘버링 임베딩을 나타내는 학습 가능한 쿼리로 대체한다.\n' +
      '\n' +
      '이 간단하면서도 효과적인 아이디어는 이미지에 붉은 원을 그리는 관행(Shtedritski et al., 2023)에서 영감을 받아 특정 영역에 주의를 기울이는 것을 목표로 한다. 그들은 빨간색 원이 VLM의 객체 수준 이미지 이해를 잠재적으로 불러온다는 점에 주목한다. 그러나, 이들은 영상 콘텐츠를 왜곡하여 VL 태스크에 위험을 초래할 수 있으며, 전경 및 배경 객체를 동시에 고려할 수 없다. 대신, 크레용 프롬프트는 판옵틱 컬러 맵 덕분에 모든 전경 및 배경 객체를 동시에 포함한다. 이미지에 직접 시각적 프롬프트를 그리는 것과 달리 크레용 프롬프트를 이미지 임베딩에 통합한다.\n' +
      '\n' +
      '그림 2: 4가지 기준(BLIP2, InstructBLIP, Qwen-VL, LLaVA1.5)의 두 가지 유형의 질문인 Class2Binary(C2B)와 Box2Class(B2C)를 묻고 각 객체 범주에서 정확도를 측정한다.\n' +
      '\n' +
      '(\\circledW\\) CoLAVo의 백본 다중-모달 언어 모델(MLM) 내의 모든 주의 모듈 계층에서 특징지어지며, 이로써 이미지의 원시 시각적 컨텍스트를 그대로 유지한다. 크레용 프롬프트는 위치 임베딩(Vaswani et al., 2017)이 토큰 임베딩 특징들에 순차 정보를 할당하는 방법과 유사하게 객체들 및 이들의 넘버링에 관한 의미 정보를 부여한다.\n' +
      '\n' +
      '크레용 프롬프트를 사용하여 객체 수준의 이미지 이해도를 높이기 위해 간단한 크레용 명령어를 생성합니다. 추가적으로, 우리는 제로-샷 VL 태스크들을 위해 시각적 명령 튜닝 데이터세트들(Liu et al., 2023, 2023)을 활용한다. 그러나, 순진하게 시각적 명령어 튜닝을 수행하는 것은 크레용 명령어로부터 획득된 객체-레벨 이미지 이해의 망각을 초래할 수 있다. 따라서 본 논문에서는 두 개의 QLoRA(Dettmers et al., 2023) 모듈을 포함하는 _Dual QLoRA_라는 학습 전략을 제안한다. 한 모듈은 크레용 명령에 대해 훈련되는 반면, 다른 모듈은 시각적 명령 튜닝 데이터 세트를 동결하고, 그 반대의 경우도 마찬가지이다. 이 접근법은 객체 수준 이미지 이해 및 복잡한 질문 응답의 기능을 보존하면서 크레용 명령어와 시각적 명령 튜닝 데이터세트의 효율적인 융합을 가능하게 한다. 파라미터-효율적인 트레이닝을 추구하기 위해, 우리는 정규 LoRA 대신에 양자화된 LoRA(QLoRA)를 채용한다(Hu et al., 2021).\n' +
      '\n' +
      '앞서 언급한 방법에 따라, 우리는 새로운 대형 언어 및 비전 모델인 **Crayon** Large **L**anguage ** and****V**ision **m**O**del (\\(\\circledW\\)**CoLAVo**)을 제안한다. 여기서 크레용 프롬프트와 VLM은 객체 수준의 이미지 이해를 향상시키기 위해 협력하며, 이는 결과적으로 제로 샷 VL 성능에 영향을 미친다. 우리의 기여도는 다음과 같이 요약할 수 있다.\n' +
      '\n' +
      '* 우리가 아는 한, 우리는 먼저 객체 수준 이미지 이해가 제로 샷 VL 성능과 강한 상관 관계가 있는 현재 VLM의 흥미로운 특성을 드러낸다.\n' +
      '* 객체 수준의 이미지 이해도를 높이고 복잡한 VL 성능과 함께 효과적으로 유지하는 _crayon prompt_와 _Dual QLoRA_를 각각 제안한다.\n' +
      '* 이 모든 성분을 적용하여 폐쇄 소스 VLM 및 오픈 소스 VLM에 비해 최첨단 제로 샷 VL 성능을 크게 달성하는 효율적인 모델 \\(\\circledW\\) CoLAVo-7B를 제시한다.\n' +
      '\n' +
      '##2 연구배경\n' +
      '\n' +
      'Visual Prompting. researchers는 LLMs(Wei et al., 2022; Chung et al., 2022; Touvron et al., 2023)에 대한 명령어 튜닝 데이터 세트를 구성함에 있어서 자연 언어 프롬프트를 향상시키는 것을 우선시하였다. 반면에 VLM을 다루는 것은 프롬프트의 시각적 측면과 텍스트적 측면을 모두 조작할 수 있는 새로운 기회를 제공한다. 시각적 프롬프트에 대한 이전의 연구들은 시각적 임베딩과 연결된 학습 가능한 토큰 임베딩(Jia et al., 2022; Sandler et al., 2022), 또는 입력 이미지에 직접 적용되는 학습된 섭동 패턴(Bahng et al., 2022; Chen et al., 2023; Oh et al., 2023)과 같은 기법들에 초점을 맞추었다. 이러한 방법은 최적의 시각적 프롬프트를 찾는 것을 목표로 하는 반면, 학습된 시각적 프롬프트는 인간의 해석성이 부족하여 그 효과에 대한 이해를 방해한다.\n' +
      '\n' +
      '이를 해결하기 위해, 현재의 VLM은 마크(Shredritski et al., 2023; Yang et al., 2023; Cai et al., 2023) 또는 시맨틱 마스크(Yang et al., 2023)와 같은 인간 해석가능한 시각적 프롬프트를 사용한다. Shredritski et al.(2023)은 이미지에 붉은 원을 그린 후 CLIP(Radford et al., 2021)이 스스로 이미지에 대한 간단한 시각적 프롬프트를 인식할 수 있음을 입증하여 표현 참조와 같은 작업에 대해 향상된 제로샷 성능을 보여준다.\n' +
      '\n' +
      '도 3: 각각의 오브젝트 카테고리에 대한 (a) C2B와 B2C 사이의 회귀된 관계를 플롯팅하는 것, (b) 각각의 오브젝트 카테고리에 대한 C2B&B2C와 제로-샷 GQA(Hudson and Manning, 2019) 성능의 평균, (c) 각각의 오브젝트 카테고리에 대한 C2B&B2C와 제로-샷 TextVQA(Singh et al., 2019) 성능의 평균으로 이들의 상관관계를 시각화하는 것.\n' +
      '\n' +
      '이해와 핵심 지역화. SEEM Zou et al. (2023) 또는 SAM Kirillov et al. (2023)을 사용함으로써, Yang et al. (2023)은 VLM이 세립 공간 정보를 이해하는 것을 돕기 위해 영숫자 및 마스크를 포함하는 특수 마크를 채용한다. Yang et al. (2023)은 객체 검출 모델 및 SAM에 의해 생성된 시맨틱 마스크들을 윤곽 마스크들, 화려한 마스크들, 그레이스케일 역방향 마스크들, 및 블러 역방향 마스크들과 같은 시각적 프롬프트들과 함께 사용하여 CLIP에서 로컬 주의를 강화한다.\n' +
      '\n' +
      '간략히 설명하자면 이전 연구는 마크 및 시맨틱 마스크를 사용하여 VLM을 특정 영역으로 안내하는 데 중점을 두었다. Yang et al.(2023)과 유사하게, 우리는 모든 전경 및 배경 객체를 한번에 포괄하는 _crayon prompt_를 제안한다. 그러나, 이미지 상의 직접적인 시각적 프롬프트 Liu et al. (2023); Shtedritski et al. (2023); Yang et al. (2023); Cai et al. (2023); Yang et al. (2023)과 비교하여, 크레용 프롬프트는 이미지를 손상되지 않고 그것의 원시 시각적 컨텍스트를 방해하지 않기 위해 백본 MLM 내의 모든 트랜스포머 Vaswani et al. (2017) 층에서 이미지 임베딩 특징부에 주입된다. 크레용 프롬프트는 위치 임베딩 Vaswani et al.(2017)이 토큰 임베딩 특징들의 상대적인 순서들에 관한 순차적인 정보를 제공하는 방법과 유사하게, 이미지 내의 객체들 및 이들의 넘버링에 관한 의미론적 정보를 제공한다.\n' +
      '\n' +
      'LLM, VLM 및 Instruction Tuning.Flan Wei et al.(2022)은 다양한 범위의 작업을 포함하는 62개의 언어 데이터 세트를 통합하여 명령어 튜닝의 개발을 개척했다. 그것은 제로 샷 성능에서 상당한 개선을 보여준다. 과제 범위와 언어 모델의 역량 확대를 위한 노력의 일환으로 Chung 등(2022)은 Flan-PaLM과 Flan-T5를 도입하여 PaLM Chowdhery 등(2023)과 T5 Raffel 등(2020)을 활용하였다. 명령어-조정 LLM의 궤적을 따라 계속하여, LLaVA Liu 등(2023)은 언어 전용 GPT-4를 활용하여 LLaVA-Instruct-665K 데이터세트에 대한 시각적 대화, 복잡한 추론 및 상세한 이미지 설명을 생성한다. 동시에, 다양한 VLMs Dai et al. (2023); Ye et al. (2023); Li et al. (2023); Zhu et al. (2023); Chen et al. (2023); Bai et al. (2023)은 접지 능력을 향상시키고 환각을 완화하기 위해 고유한 명령어 튜닝 데이터 세트를 개발하였다.\n' +
      '\n' +
      '현재 VLM이 급증하는 가운데, 우리는 지침 조정에서 이루어진 보폭에도 불구하고 새로운 각도에서 접근한다. 결과적으로 VLM이 객체 수준 이미지 이해도를 효과적으로 파악하는지 여부를 조사하는 데 초점을 맞춘다. 그들이 부족하면 이러한 부적절함이 VL 성능과 상관관계가 있는지 여부에 의문을 제기한다. 본질적으로 그림 2-3은 기반 이미지 이해의 중요성과 VL 성능에 미치는 잠재적 영향, 즉 이전 연구에서 종종 간과되는 측면을 강조한다. 따라서, 우리는 객체 수준의 이미지 이해와 시각적 명령어 튜닝의 융합을 옹호한다.\n' +
      '\n' +
      '## 3 CoLLaVO\n' +
      '\n' +
      'Model Architecture and Prompt Protocol.\\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO 구조는 그림 4와 같이 비전 인코더, 크레용 프롬프트, 백본 MLM, 비전과 언어 구성요소 사이의 MLP 커넥터로 구성된다. 비전 인코더로서 CLIP Radford 등(2021)이 고려되어, 이미지 이해의 능숙성으로부터 이익을 얻는다. [\\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO에 사용된 MLM은 InternLM-7B Team(2023)의 것으로, RLHF Christiano et al.(2017); Stiennon et al.(2020); Ouyang et al.(2022)과 함께 1.6T 다국어 데이터세트로 튜닝된 다국어 기반 모델 명령어이다. 더더욱\n' +
      '\n' +
      '그림 4: \\(\\leavevmode\\hbox{\\circled{\\char 37}}\\) CoLLaVO에 대한 2단계 훈련 개요. \'비전\'은 비전 인코더를 나타내고, 화재 기호는 학습할 모듈을 나타낸다.\n' +
      '\n' +
      '오버, GELU 활성화 기능을 가진 두 개의 완전히 연결된 MLP(헨드리크 및 김펠, 2016)가 브리지 커넥터 역할을 한다. CoLLaVO 입력과 관련하여, 프롬프트 프로토콜에 대한 준수가 유지되며, 여기서 \'<image>\'는 이미지 임베딩 특징들에 대한 특수 토큰을 나타내고, \'<stop>\'은 텍스트 생성을 위한 정지 토큰을 나타내고, \'User:{}\'는 질문 템플릿을 나타내고, \'Assistant:{}\'는 답변 템플릿을 나타낸다(예를 들어, 하기 도 5 참조).\n' +
      '\n' +
      'Crayon Prompt Tuning(CPT) 전체 이미지에 대한 포괄적인 객체 수준의 파악을 보장하기 위해 CoLLaVO는 전경(_예: 사람, 버스, 병, 헤어드라이어 및 핸드백) 및 배경(_예: 하늘, 도로, 강, 바다 및 눈) 객체를 모두 포함하여 그 안에 있는 모든 별개의 객체를 인식해야 한다. 이를 위해 그림 4(a)와 같이 팬옵틱 컬러 맵을 생성하는 팬옵틱 분할 모델(Cheng et al., 2022)을 사용한다. 이 맵은 MSCOCO 2017(Lin et al., 2014)으로부터 전경 및 배경 객체의 133개의 상이한 객체 카테고리(부록 A 참조)의 구별을 가능하게 하며, CoLLaVO가 이미지 내의 모든 객체에 초점을 맞추는 시각적 큐 역할을 한다.\n' +
      '\n' +
      '특히, 팬옵틱 맵은 각 객체에 대한 두 가지 중요한 정보, 즉 의미 정보와 넘버링 정보를 포함한다. 예를 들어, 이미지가 도로 위의 두 사람과 버스를 묘사한다면, 그림 4(b)에 예시된 바와 같이, 판옵틱 맵은 그림 5에 도시된 바와 같이, 각각의 객체에 카테고리 라벨 및 넘버링 인덱스를 할당한다. 두 사람은 상이한 넘버링 인덱스 \'1\' 및 \'2\'를 수신하지만 동일한 객체 카테고리 \'사람\'을 공유한다. 단수인 다른 객체들은 모두 넘버링 인덱스 \'1\'로 라벨링된다. 알 수 없는 범주에는 넘버링 인덱스 \'0\'이 할당되어 있다는 점에 주목할 필요가 있다. 다음 프로세스를 간소화하기 위해 앞서 언급한 133개의 카테고리 및 **unk** 알려지지 않은 카테고리를 포함하여 133+1(**unk**) 학습 가능한 의미 질의를 준비한다. 또한, 동일한 객체 카테고리의 20개 이하의 인스턴스들이 하나의 이미지 내에 출현하지 않는다는 가정 하에, **unk**에 대해 20+1(\'0\')의 학습 가능한 넘버링 쿼리들을 준비한다.\n' +
      '\n' +
      '134개의 시맨틱 쿼리와 21개의 넘버링 쿼리를 활용하여 시맨틱 컬러 맵과 넘버링 컬러 맵을 모두 이러한 쿼리로 대체하여 코드북 메커니즘을 통해 벡터 양자화된 특징을 생성한다(Van Den Oord et al., 2017; Esser et al., 2021). 이 프로세스는 후속적으로 백본 MLM에서 결합되는 그림 5의 의미적 및 넘버링 임베딩의 생성을 초래한다. 이러한 조합된 표현을 _crayon prompt_라고 한다. 크레용 프롬프트는 MLP 커넥터를 만나고, 그 출력은 그림 6(a)와 같이 MLM의 모든 주의 모듈 레이어에서 이미지 특징과 함께 추가된다. 그런 다음 그림 5의 하단과 같이 크레용 명령어를 활용하고, 크레용 프롬프트를 백본 MLM에 정렬하고 객체 수준 이미지 이해도를 높이기 위해 _crayon prompt tuning_(CPT)를 수행한다. 여기서, 마젠타 컬러-텍스트는, 도 5 이하의 크레용 지시 예에서 설명한 바와 같이, 자동-회귀적으로 학습된다.\n' +
      '\n' +
      '그림 5: 학습 가능한 의미적 질의 및 넘버링 질의로 범옵틱 컬러 맵으로부터 크레용 프롬프트가 어떻게 생성되는지를 기술하는 것. 또한, CPT 및 CIT를 수행하기 위해 사용되는 크레용 지시 예들이 주어진다. 참고로, \'{}\'은 우리가 적응적으로 정보를 입력하는 장소를 의미한다.\n' +
      '\n' +
      '**Crayon prompt-based Instruction Tuning (CIT).**CPT는 MS-COCO 2017 데이터세트(Lin et al., 2014)와 함께 크레용 프롬프트 및 그 MLP 커넥터에서 시멘틱 및 넘버링 쿼리를 학습하는 것에만 초점을 맞추고, 이들을 백본 MLM과 정렬하여 \\(\\Join\\)CoLLaVO 의 객체-레벨 이미지 이해를 향상시킨다. 한편, _crayon prompt-based instruction tuning_(CIT)는 VL 태스크에 대한 복잡한 질문 응답을 처리하기 위한 크레용 명령어뿐만 아니라 시각적 명령어 튜닝 데이터세트(Liu et al., 2023, 20; Chen et al., 2023)를 활용한다. 이것은 \\(\\Join\\)CoLLaVO의 백본 MLM과 함께 의미 및 번호 지정 질의와 MLP 커넥터를 다시 훈련하는 것을 포함한다.\n' +
      '\n' +
      'MLM을 CIT로 훈련할 때, 우리는 두 측면을 효과적으로 유지하기 위해 객체 수준의 이미지 이해와 복잡한 VL 성능을 각각 관리하는 _Dual QLoRA_라는 학습 전략을 소개한다. 도 6은 Dual QLoRA의 개요를 제공하며, 여기서 _Image-CIT_는 객체 레벨 이미지 이해 및 첫 번째 QLoRA 모듈만을 부트스트랩하고 트레이닝하기 위해 크레용 명령어를 사용하는 것을 나타내는 반면, _VL-CIT_는 제로 샷 VL 성능을 달성하기 위해 시각적 명령어 튜닝 데이터세트로부터 복잡한 질문-응답 쌍을 사용하고 두 번째 QLoRA 모듈만을 트레이닝하는 것을 나타낸다. CIT 동안 우리는 \\(\\Join\\)CoLLaVO에 크레용 프롬프트 형태로 이미지를 제시하고 이미지-CIT 또는 VL-CIT를 진행할지 여부를 무작위로 결정한다. Dual QLoRA의 가장 큰 목적은 객체 수준의 이미지 이해 능력과 복잡한 VL 성능을 효율적으로 보존하는 것이다. CPT와 Image-CIT의 주요 차이점은 \\(\\Join\\)CoLLaVO 백본 MLM이 훈련되었는지 여부에 있다. 사소한 측면에 대한 추가 세부 사항은 다음 섹션에서 다루어질 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**성공적인 재현성을 보장하기 위해 \\(\\Join\\)CoLLaVO.**의 구현 세부사항을 다음과 같이 요약한다. \\(\\Join\\)CoLLaVO: (a) QLoRA, (b) 크레용 프롬프트, (c) Image-CIT 및 VL-CIT의 지시 세부사항, (d) 훈련 하이퍼 파라미터, (e) 텍스트 생성.\n' +
      '\n' +
      '**(a)**: Quantized Low-Rank Adaptation (QLoRA) (Hu et al., 2021; Dettmers et al., 2023)을 채용한다. \\(\\Join\\)CoLLaVO는 최소 파라미터 튜닝으로 효율적인 트레이닝을 추구하기 때문이다. 이중 양자화와 정규화된 float 4-bit (nf4)를 LoRA (r=64\\)와 \\(\\alpha=64\\)로 사용하였다. **(b)**: MS-COCO 2017로부터의 크레용 명령어 및 이미지만을 갖는 CPT와 대조적으로, CIT는 시각적 명령어 튜닝 데이터세트(Liu et al., 2023, 20; Chen et al., 2023)로 또한 수행된다. 따라서 많은 이미지에는 텍스트, 코드, 포스터 또는 수학적 기호와 같이 인식할 수 없는 객체가 포함되어 있다. 결과적으로, 알려지지 않은 카테고리와 \'0\' 넘버링을 갖는 범광학 컬러 맵이 생성될 것이고, **unk** 카테고리의 시맨틱 쿼리들 및 \'0\' 넘버링 쿼리들은 이러한 경우들에서 크레용 프롬프트를 생성하기 위해 동작할 것이다. **(c)**: 컬러 맵이 식별 가능한 객체들과 함께 주어지면, 객체 이름들, 그들의 넘버링 인덱스들, 및 그들의 바운딩 박스 좌표들을 포함하는 텍스트 설명들이 질문 템플릿에 추가된다. 반대로, 이미지가 객체가 없는 경우, 질문 템플릿은 "세부 객체 정보 없음"이라는 문구를 포함한다.\n' +
      '\n' +
      '도 6: (a) 크레용 프롬프트가 (b), (c)의 이미지 임베딩 특징 및 학습 전략에 어떻게 주입되는지를 조명하는 단계(Luo et al., 2023) 객체-레벨 이미지 이해 능력(Image-CIT) 및 VL 태스크 능력(VL-CIT)을 위한 Dual QLoRA는 재난적 망각 없이 효율적으로 공존한다.\n' +
      '\n' +
      '**(d)**: 훈련과 관련하여 CPT의 경우 1e-4에서 1e-6, CIT의 경우 1e-5에서 1e-6의 학습률로부터 코사인 어닐링(Loshchilov and Hutter, 2016)에 의해 스케줄링된 AdamW(Loshchilov and Hutter, 2019) 최적화기를 사용하여 한 에포크로 32의 배치 크기를 갖는 CoLLaVO를 훈련한다. **(e)**: 최상의 성능을 찾기 위해, **CoLLaVO**는 다른 하이퍼-파라미터 없이 텍스트 생성을 위해 그리디 또는 빔 탐색(\\(n=3\\))을 사용한다.\n' +
      '\n' +
      '객체 수준 이미지 이해.VL 작업에서 CoLLaVO를 검증하기 전에 객체 수준 이미지 이해의 숙련도를 보장하는 것이 중요하다. 본 연구에서는 MS-COCO 2017에서 \'thing\'(부록 A 참조)으로 분류된 80개의 객체 범주를 Class2Binary(C2B)과 Box2Class(B2C)의 두 가지 방향에 대해 BLIP2, InstructBLIP, Qwen-VL, LLaVA1.5의 네 가지 강력한 기준선을 사용하여 정확도를 평가했으며, 그림 7(a)-(b)에서 볼 수 있듯이 C2B와 B2C 모두에서 Top-20, Bot-20, Avg의 세 가지 경우에서 **CoLLaVO**가 기준선보다 거의 우수한 성능을 보였다. 또한, C2B와 B2C 모두에서 Top-20 정확도와 Bottom-20 정확도 사이의 성능 격차가 가장 작다. 이러한 관찰은 CoLLaVO가 수많은 객체 클래스에 걸쳐 솔리드 객체 수준 이미지 이해도를 가지고 있음을 나타낸다.\n' +
      '\n' +
      'Zero-shot VL Evaluation.개선된 객체-레벨 이미지 이해에 따라, CoLLaVO는 17개의 유명한 데이터 세트들에 대한 VL 태스크들의 제로-shot 성능을 측정하기 위해 평가된다(부록 B 참조). 그림 1, 7(c) 및 표 1에서 볼 수 있듯이 **CoLLaVO**는 GPT-4V, 제미니-Pro, Qwen-VL-Pro와 같은 여러 폐쇄 소스 VLM과 수많은 오픈 소스 VLM을 능가한다(평가에 사용된 모든 VLM에 대해 부록 C 참조). 특히 주목할 만한 것은 CoLLaVO가 크게 발전한 시지각과 인지능력을 주로 평가하는 MME, MM-Bench, MM-Bench-Chinese, Q-Bench 등의 벤치마크에서 다른 모델들에 비해 우월하다는 점이다.\n' +
      '\n' +
      'Crayon Prompt와 CIT의 효과 CoLLaVO에서 시멘틱 임베딩, 크레용 프롬프트의 넘버링 임베딩, Dual QLoRA, Image-CIT, VL-CIT 등의 요소를 제거하였다. 표 2에 예시된 바와 같이, 크레용 프롬프트 내의 시맨틱 및 넘버링 임베딩은 MME 데이터세트 상에서 CoLLaVO 의 제로-샷 성능을 상당히 증가시킨다는 것이 명백하다. 특히 \'E&P\' 점수를 가진 MME-P에서 의미 임베딩만으로도 제로샷 성능을 크게 향상시킬 수 있다는 점은 객체 수준 의미학을 주입하는 것이 모델이 객체 수준 이미지 이해를 위해 객체의 존재를 더 잘 인식하는 데 도움이 된다는 것을 암시한다. 또한, 넘버링 임베딩은 \'카운트\' 점수를 상당히 높여 성능을 더욱 개선함으로써 동일한 카테고리의 객체를 차별화하는 데 그 효과를 입증한다.\n' +
      '\n' +
      '표 3은 Dual QLoRA, Image-CIT 및 VL-CIT가 각각 제로샷 성능 향상에 기여함을 보여준다. VL-CIT 단독은 다른 오픈 소스 VLM에 비해 MME-P에서 \\(1599.2\\) 및 MME-C에서 \\(414.1\\)의 더 나은 성능을 나타내며, 크레용 프롬프트의 도움을 받는다. 또한 이미지-CIT는 크레용 명령어를 CPT뿐만 아니라 CIT에 통합함으로써 QLoRA 없이 제한된 확장에도 불구하고 성능을 향상시킨다. 마지막으로 Dual QLoRA는 Image-CIT와 VL-CIT의 두 측면을 모두 활용하는 데 가장 큰 효과를 발휘한다.\n' +
      '\n' +
      '##5 토론 및 결론\n' +
      '\n' +
      '우리는 객체 수준의 이미지 이해도를 높이는 열쇠 역할을 하는 _crayon prompts_ 및 _Dual QLoRA_와 함께 **CoLLaVO**의 효과를 보여주었다. 특히, 도 8(a)는 **CoLLaVO**의 인상적인 능력을 예시한다\n' +
      '\n' +
      '도 7: (a) 및 (b)에서, Top-20 객체 카테고리들에 대한 평균 정확도는 청색 삼각형들로 표시되고, Bottom-20에 대한 평균 정확도는 녹색 삼각형들로 표시되고, 적색 사각형들은 VLM들의 객체-레벨 이미지 이해를 시각화하기 위해 모든 카테고리들의 평균을 나타낸다. (c)에서는 여러 VL 벤치마크에서 VLM의 제로샷 성능(MME-P(점수의 1/20), GQA, TextVQA, SQA-IMG, SEED-IMG(정확도))이 표시된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      '## 6 Limitations\n' +
      '\n' +
      'VLM을 넘어서는 외부 소스인 판옵틱 컬러 맵에 의존하는 크레용 프롬프트는 세그먼트화 모델의 성능 및 객체 클래스의 수를 포함하는 것에 의해 제한될 수 있다. 그럼에도 불구하고, 우리는 모든 제로 샷 작업에서 칭찬할 만한 점수를 얻었습니다. (\\mathbf{\\hat{w}}\\)**CoLLaVO**는 강력한 객체 분류 또는 이미지 캡션 모델(Lee et al., 2020, 2022, 2023; Kim et al., 2023c), 객체 중심 인과적 인간 해석 가능 정보(Kim et al., 2021, 2023b), 개방 객체 검출(Zhang et al., 2023a), 시각적 접지(Liu et al., 2023d; Ren et al., 2024), 상호작용 또는 비감독 분할(Kirillov et al., 2023; Kim et al., 2023a), 광학 특성 인식 모델(Bautista and Atienza, 2022)을 포함할 수 있다.\n' +
      '\n' +
      '##7 윤리성명\n' +
      '\n' +
      '우리는 이 논문에서 제시된 모든 연구가 윤리적 행위와 청렴의 원칙을 준수한다는 것을 긍정한다. 수행된 실험과 보고된 결과는 엄격한 과학적 방법을 기반으로 하며 비전 언어 모델 분야에 긍정적으로 기여하기 위해 노력한다. 이 연구에 사용된 모든 데이터 세트: MS-COCO 2017(Lin et al., 2014) 및 시각적 지시 데이터 세트(Liu et al., 2023c,b; Chen et al., 2023d)를 획득하고 연구 윤리 및 데이터 프라이버시를 위한 관련 규정 및 지침을 준수하여 분석하였다. 또한 잠재적인 한계가 투명하게 논의되었으므로 연구의 영향을 받는 커뮤니티에 대한 무결성, 책임 및 존중의 최고 표준을 준수하기 위해 최선을 다하고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Bahng et al.(2022) Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022. 대규모 모델 적응을 위한 시각적 프롬프트 탐색. _ arXiv preprint arXiv:2203.17274_.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: 다재다능한 능력을 가진 프론티어 대형 비전 언어 모델 _ arXiv preprint arXiv:2308.12966_.\n' +
      '* Bautista와 Atienza (2022) Darwin Bautista와 Rowel Atienza. 2022. permuted autoregressive sequence model을 이용한 장면 텍스트 인식. \'유럽 컴퓨터 비전 회의\' 178-196페이지 스프링어입니다\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Cai et al. (2023) Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. 2023. 대형 멀티모달 모델을 제작하여 임의의 시각적 프롬프트를 이해한다. _ arXiv preprint arXiv:2312.00784_.\n' +
      '* Chen et al. (2023a) Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. 2023a. 시각적 프롬프트를 이해하고 개선: 레이블 매핑 원근법입니다. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19133-19143.\n' +
      '* Chen et al. (2023b) Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023b. Minigpt-v2: 비젼-언어 멀티태스크 학습을 위한 통합 인터페이스로서의 큰 언어 모델. _ arXiv preprint arXiv:2310.09478_.\n' +
      '* Chen et al. (2023c) Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. 2023c. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _ arXiv preprint arXiv:2306.15195_.\n' +
      '* Chen et al. (2023d) Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023d. Sharegpt4v: 더 나은 캡션을 갖는 대형 멀티모달 모델의 개선. _ arXiv preprint arXiv:2311.12793_.\n' +
      '* Cheng et al. (2022) Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. 범용 영상 분할을 위한 마스크-어텐션 마스크 트랜스포머. IEEE/CVF Conference on computer vision and pattern recognition_의 _Proceedings, pages 1290-1299.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Marcic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _ arXiv preprint arXiv:2210.11416_.\n' +
      '* Contributors(2023) XTuner Contributors. 2023. Xtuner: 효율적으로 미세조정 llm. [https://github.com/InternLM/xtuner]을 위한 툴킷 (https://github.com/InternLM/xtuner).\n' +
      '* Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: 명령어 튜닝을 갖는 범용 비전-언어 모델들을 향한다. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Dettmers et al.(2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Quantized llms의 효율적인 finetuning. _ arXiv preprint arXiv:2305.14314_.\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. _ arXiv preprint arXiv:1810.04805_.\n' +
      '* Esser et al. (2021) Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. 고해상도 영상 합성을 위한 트래밍 트랜스포머. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 _Proceedings에서, 페이지 12873-12883.\n' +
      '* Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: 종합 평가 벤치마크 for multimodal large language models. _ arXiv preprint arXiv:2306.13394_.\n' +
      '* Gong et al. (2023) Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, 및 Kai Chen. 2023. 멀티모달-gpt: 인간과의 대화를 위한 비전과 언어 모델. _ arXiv preprint arXiv:2305.04790_.\n' +
      '* 헨드릭스와 김펠(2016) 댄 헨드릭스와 케빈 깐펠. 2016. 가우시안 오차 선형 단위(gelus) _ arXiv preprint arXiv:1606.08415_.\n' +
      '* Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. 로라: 대형 언어 모델의 저순위 적응. _ arXiv preprint arXiv:2106.09685_.\n' +
      '* Hu et al. (2021)Drew A Hudson and Christopher D Manning. 2019. Gqa: real-world visual reasoning and compositional question answering을 위한 새로운 데이터셋. IEEE/CVF Conference on computer vision and pattern recognition_의 _Proceedings, pages 6700-6709.\n' +
      '* Iyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. 순차적인 질의 응답을 위한 검색 기반 신경망 구조화 학습. _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1821-1831.\n' +
      '* Jia et al. (2022) Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. 2022. 시각적 프롬프트 튜닝. 유럽 컴퓨터 비전 회의 709-727 페이지 스프링어\n' +
      '* Kembhavi et al. (2016) Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjun Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016년. 다이어그램은 12개의 이미지 가치가 있습니다. _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 235-251. Springer.\n' +
      '* Kim et al.(2021) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2021. 정보 병목 현상에 의한 적대적 예에서 강건하고 비강인성 특징을 증류하는 단계 _ 신경 정보 처리 시스템_, 34:17148-17159에서의 발전.\n' +
      '* Kim et al. (2023a) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2023a. 인과 비감독 시맨틱 세분화 arXiv preprint arXiv:2310.07379_.\n' +
      '* Kim et al. (2023b) Junho Kim, Byung-Kwan Lee, and Yong Man Ro. 2023b. 적대적 예제에 대한 인과적 특징과 적대적 도구 변수 회귀에 의한 강력한 네트워크에 대한 인과적 접종을 해석한다. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 12302-12312.\n' +
      '*김 등(2023c) 연주 김, 준호 김, 병관 이, 세빈 신, 용만로. 2023c. 클립 교란기 없는 캡션 네트워크를 통해 이미지 캡션에서 데이터 세트 편향을 완화합니다. _2023 IEEE ICIP(International Conference on Image Processing)_에서, 페이지 1720-1724. IEEE.\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 2023, 아무거나 구분해 _Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV)_, pages 4015-4026.\n' +
      '* Laurencon et al. (2023) Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. 2023. Obelisc: open web-scale filtered dataset of interleaved image-text documents. _ arXiv preprint arXiv:2306.16527_.\n' +
      '* Lee et al.(2022) Lee병관, Kim Junho, and Yong Man Ro. 2022. 마스킹 적대적 손상: 강건하고 희박한 네트워크에 대한 적대적 현저성 발견. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 15126-15136.\n' +
      '* Lee et al.(2023) Lee병관, Kim Junho, and Yong Man Ro. 2023. 적대적 이중 기계 학습에 의한 인과적 파라미터 추정을 통해 적대적 취약성을 완화한다. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 4499-4509.\n' +
      '* Lee et al.(2020) 이병관, 유영준, 그리고 용만로. 2020. 계층적 변분추론을 통한 베이지안 신경망의 적대적 강건성을 향한다.\n' +
      '* Li 등(2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, 및 Ziwei Liu. 2023a. Otter: in-context 명령어 튜닝을 갖는 멀티모달 모델. _ arXiv preprint arXiv:2305.03726_.\n' +
      '* Li et al. (2023b) Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. 2023b. 종자 벤치: 생성적 이해도를 가진 다중 모드 llms를 벤치마킹 arXiv preprint arXiv:2307.16125_.\n' +
      '* Li et al. (2023c) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023c. Bip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 훈련 arXiv preprint arXiv:2301.12597_.\n' +
      '*Li 등(2023d) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d. 대용량 시각 언어 모델에서 객체 환각을 평가하는 방법. _ arXiv preprint arXiv:2305.10355_.\n' +
      '* Li et al. (2023e) Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, 및 Xiang Bai. 2023e. Monkey: 이미지 해상도 및 텍스트 라벨은 대형 멀티모달 모델의 중요한 사항이다. _ arXiv preprint arXiv:2311.06607_.\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: context in common objects. _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, Proceedings, Part V 13_, pages 740-755. Springer.\n' +
      '* Liu et al. (2023a) Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a. 어떻게 생각하는지 봤어? 아니면 네가 본 걸 생각하는 거야? gpt-4v(ision), llava-1.5 및 기타 다중 모델 모델에 대해 도전하는 이미지 컨텍스트 추론 벤치마크. _ arXiv preprint arXiv:2310.14566_.\n' +
      '* Liu et al. (2023b) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b. 시각적 지시 조정을 통해 개선된 기준선입니다. _ arXiv preprint arXiv:2310.03744_.\n' +
      '*Liu et al. (2023c)*Liu et al. (2023c) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023c. 시각적 지시 조율 IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Liu et al. (2023d) Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023d. 접지 디노: 오픈셋 객체 검출을 위한 접지된 사전 훈련으로 디노를 결혼시키는 단계; _ arXiv preprint arXiv:2303.05499_.\n' +
      '* Liu et al. (2023e) Weihuang Liu, Xi Shen, Chi-Man Pun, and Xiaodong Cun. 2023e. 저수준 구조 세그먼트에 대한 명시적인 시각적 프롬프트 IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 19434-19445.\n' +
      '*Liu et al. (2023f) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023f. 멀티모달 모델이 만능 선수인가요? arXiv preprint arXiv:2307.06281_.\n' +
      '* Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. 2016. Sgdr: 따뜻한 재시작과 함께 확률적 경사 하강. _ arXiv preprint arXiv:1608.03983_.\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. _International Conference on Learning Representations_.\n' +
      '* Lu et al. (2023) Pan Lu, Hrtik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: 시각적 문맥에서 기초 모델의 수학적 추론 평가 _ arXiv preprint arXiv:2310.02255_.\n' +
      '* Luo et al. (2023) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. Continual Fine-tuning 동안 대규모 언어모델에서의 catastrophic forgetting에 대한 실증적 연구 _ arXiv preprint arXiv:2308.08747_.\n' +
      '* 오 등(2023) 창대 오, 혜지 황, 희영 이, 용택 임, 근영 정, 지영 정, 호식 최, 경우 송. 2023. Blackvip: 강력한 전이 학습을 위한 Black-box 시각적 프롬프트. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24224-24235.\n' +
      '* OpenAI(2023a) OpenAI. 2023a. Gpt-4v(sision) 시스템 카드. [https://openai.com/research/gpt-4v-system-card] (https://openai.com/research/gpt-4v-system-card), Last access on 2024-02-13.\n' +
      '* OpenAI(2023b) OpenAI. 2023b. Gpt-4v(sision) 기술 작업 및 저자 [https://openai.com/contributions/gpt-4v] (https://openai.com/contributions/gpt-4v), 2024-02-13에서 마지막으로 액세스했다.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022, training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744의 발전.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. 자연어 감독으로부터 전이 가능한 시각적 모델을 학습하는 단계. 제38회 머신러닝 국제회의_Proceedings of the 38th International Conference on Machine Learning_, vol 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 통일된 텍스트-텍스트 변환기를 이용한 전이학습의 한계점 탐색. _ The Journal of Machine Learning Research_, 21(1):5485-5551.\n' +
      '* Ren et al. (2024) Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. _ arXiv preprint arXiv:2401.14159_.\n' +
      '* Sandler et al. (2022) Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, and Andrew Jackson. 2022. 학습 가능한 메모리를 이용하여 이미지 트랜스포머를 미세 조정한다. IEEE/CVF Conference on Computer Vision and Pattern Recognition_의 _Proceedings, pages 12155-12164.\n' +
      '* Shtedritski et al. (2023) Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. 2023년, 클립이 빨간 원에 대해 뭘 알아? vlms에 대한 시각적 프롬프트 엔지니어링. _ arXiv preprint arXiv:2304.06712_.\n' +
      '* Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. 읽을 수 있는 vqa 모델을 향해. 컴퓨터 비전 및 패턴 인식에 관한 IEEE/CVF 회의의 _Proceedings에서, 페이지 8317-8326.\n' +
      '* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. 휴먼 피드백으로 요약하는 학습 신경 정보 처리 시스템_, 33:3008-3021의 발전.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: High capable multiimodal model의 가족. _ arXiv preprint arXiv:2312.11805_.\n' +
      '*팀(2023) InternLM팀. 2023. Internlm: 점진적으로 향상된 능력을 가진 다국어 언어 모델. [https://github.com/InternLM/InternLM-techreport] (https://github.com/InternLM/InternLM-techreport).\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. 개방적이고 효율적인 기초 언어 모델 arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ arXiv preprint arXiv:2307.09288_.\n' +
      '* Van Den Oord et al. (2017) Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의력만 있으면 됩니다 _ 신경 정보 처리 시스템_, 30의 발전.\n' +
      '* Wang et al. (2023) Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. _ arXiv preprint arXiv:2311.03079_.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai와 Quoc V Le 2022. Finetuned language models is zero-shot learners. _International Conference on Learning Representations_.\n' +
      '* Wu et al. (2023) Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. 2023. Q-bench: Low level vision에 대한 범용 기초 모델의 벤치마크. _ arXiv preprint arXiv:2309.14181_.\n' +
      '* Yang et al. (2023a) Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, 및 Jianfeng Gao. 2023a. 표시 세트 프롬프트는 gpt-4v에서 특별한 시각적 접지를 방출합니다. _ arXiv preprint arXiv:2310.11441_.\n' +
      '* Yang et al. (2023b) Lingfeng Yang, Yuez Wang, Xiang Li, Xinlong Wang, and Jian Yang. 2023b. 세밀한 시각적 프롬프트입니다. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의\n' +
      '* Ye et al. (2023a) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality. _ arXiv preprint arXiv:2304.14178_.\n' +
      '* Ye et al. (2023b) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, 및 Jingren Zhou. 2023b. mplug-owl2: 모달리티 협업으로 멀티모달 대형 언어 모델을 혁명화. _ arXiv preprint arXiv:2311.04257_.\n' +
      '* Yu et al. (2023) Weihao Yu, Zhen규안 Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: 통합 능력에 대한 대형 멀티모달 모델 평가 _ arXiv preprint arXiv:2308.02490_.\n' +
      '* Zhang et al. (2023a) Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. 2023a. 개방형 어휘 분할 및 검출을 위한 간단한 프레임워크. IEEE/CVF International Conference on Computer Vision_의 _Proceedings, pages 1020-1031.\n' +
      '* Zhang et al. (2023b) Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Duyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. 2023b. Interlm-xcomposer: 고급 텍스트 이미지 이해 및 구성을 위한 비전 언어 대형 모델. _ arXiv preprint arXiv:2309.15112_.\n' +
      '* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: 고급 대형 언어 모델로 비젼-언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_.\n' +
      '* Zou et al. (2023) Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. 2023. 모든 곳을 한꺼번에 분할한다. _ arXiv preprint arXiv:2304.06718_.\n' +
      '\n' +
      'Panoptic Color Map을 위한 COCO 클래스\n' +
      '\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline \\multicolumn{4}{c}{COCO Panoptic Classes} \\\\ \\hline person & bicycle & car & motorcycle \\\\ airplane & bus & train & truck \\\\ boat & traffic light & fire hydrant & stop sign \\\\ parking meter & bench & bird & cat \\\\ dog & horse & sheep & cow \\\\ elephant & bear & zebra & giraffe \\\\ backpack & umbrella & handbag & tie \\\\ suitcase & frisbee & skis & snowboard \\\\ sports ball & kite & baseball bat & baseball glove \\\\ skateboard & surfboard & tennis racket & bottle \\\\ wine glass & cup & fork & knife \\\\ spoon & bowl & banana & apple \\\\ sandwich & orange & broccoli & carrot \\\\ hot dog & pizza & donut & cake \\\\ chair & couch & potted plant & bed \\\\ dining table & toilet & tv & laptop \\\\ mouse & remote & keyboard & cell phone \\\\ microwave & oven & toaster & sink \\\\ refrigerator & book & clock & vase \\\\ scissors & teddy bear & hair drier & toothbrush \\\\ banner\\({}^{*}\\) & blanket\\({}^{*}\\) & bridge\\({}^{*}\\) & cardboard\\({}^{*}\\) \\\\ counter\\({}^{*}\\) & curtain\\({}^{*}\\) & door\\({}^{*}\\) & floor-wood\\({}^{*}\\) \\\\ flower\\({}^{*}\\) & fruit\\({}^{*}\\) & gravel\\({}^{*}\\) & house\\({}^{*}\\) \\\\ light\\({}^{*}\\) & mirror\\({}^{*}\\) & net\\({}^{*}\\) & pillow\\({}^{*}\\) \\\\ platform\\({}^{*}\\) & playingfield\\({}^{*}\\) & railroad\\({}^{*}\\) & river\\({}^{*}\\) \\\\ road\\({}^{*}\\) & roof\\({}^{*}\\) & sand\\({}^{*}\\) & sea\\({}^{*}\\) \\\\ shelf\\({}^{*}\\) & snow\\({}^{*}\\) & stairs\\({}^{*}\\) & tent\\({}^{*}\\) \\\\ towel\\({}^{*}\\) & wall-brick\\({}^{*}\\) & wall-stone\\({}^{*}\\) & wall-tile\\({}^{*}\\) \\\\ wall-wood\\({}^{*}\\) & water\\({}^{*}\\) & window-blind\\({}^{*}\\) & window\\({}^{*}\\) \\\\ tree\\({}^{*}\\) & fence\\({}^{*}\\) & ceiling\\({}^{*}\\) & sky\\({}^{*}\\) \\\\ cabinet\\({}^{*}\\) & table\\({}^{*}\\) & floor\\({}^{*}\\) & pavement\\({}^{*}\\) \\\\ mountain\\({}^{*}\\) & grass\\({}^{*}\\) & dirt\\({}^{*}\\) & paper\\({}^{*}\\) \\\\ food\\({}^{*}\\) & building\\({}^{*}\\) & rock\\({}^{*}\\) & wall\\({}^{*}\\) \\\\ rug\\({}^{*}\\) & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\n' +
      '* \'thing\'(countable)으로 분류되지 않고 \'tuff\'(uncountable)로 분류되는 객체 클래스\n' +
      '\n' +
      '## 부록 B 제로샷 비젼 언어 데이터셋 평가\n' +
      '\n' +
      '***GQA**(Hudson and Manning, 2019)는 장면 그래프로 주석이 달린 실세계 이미지를 포함하는 시각적 질문 응답 데이터세트이다. 장면과 질문의 의미 표상을 활용하여 의미 구성성의 문제를 해결한다. 그것은 이미지 객체, 속성 및 관계의 구조화된 표현과 각각 관련된 광범위한 이미지를 포함하는 2,200만 개의 질문을 포함한다.\n' +
      '이미지 컨텍스트를 포함하는 사이언스QA(SQA) 데이터세트의 하위 집합인 ***SQA-IMG**(Iyyer et al., 2017)는 다양한 하위 분야를 포함하는 초·고등학교 과학교육 자료에서 조달한 10,332개의 객관식 질문으로 구성된다. SQA 데이터 세트의 대부분의 질문에는 보충 강의(83.9%)와 상세한 설명(90.5%)이 수반되어 더 광범위한 지식과 정답에 대한 특정 추론으로 이해도를 풍부하게 한다.\n' +
      '**TextVQA**(Singh et al., 2019)는 관련 질문에 대응하기 위해 이미지 내에 내재된 텍스트를 분석하고 이해하기 위한 대규모 복합 벤치마크이다. 여기에는 이미지 내에 존재하는 텍스트 정보를 통합하고 그에 대한 추론을 통해 답변을 제공하는 것이 포함된다. 데이터 세트는 45,336개의 질문과 453,360개의 대응하는 지상 진실 답변을 수반하는 오픈이미지에서 조달된 28,408개의 이미지로 구성된다.\n' +
      '* **POPE**(Li et al., 2023d)는 VLM들 내의 객체 환각 챌린지를 평가하도록 맞춤화된 폴링 기반 이진 분류 쿼리 데이터세트의 역할을 한다. 그것은 각각 다양한 샘플링 기술을 사용하여 조작된 _즉, 무작위, 인기 및 적대적인 3개의 별개의 하위 집합으로 구성되어 총 8,910개의 항목을 생성한다.\n' +
      'Fu et al., 2023은 14개의 하위 작업에서 인지 능력과 인지 능력을 모두 측정하여 VLM의 성능을 평가하는 것을 목표로 하는 새로운 포괄적인 벤치마크로 도입되었다. 공공 데이터 세트와 관련된 잠재적인 데이터 누출 문제를 완화하기 위해 명령-응답 쌍에 대한 모든 주석을 수동으로 설계한다.\n' +
      '**MMBench, MMBench-Chinese**(Liu et al., 2023f)는 다중 양식에 걸쳐 포괄적인 평가 프레임워크를 설정한다. 이러한 프레임워크는 영어 및 중국어 모두에서 20개의 별개의 능력 차원을 다루는 약 3000개의 객관식 질문을 포함한다. 평가 과정에 ChatGPT를 통합하여 혁신적인 접근 방식을 도입한다.\n' +
      '**MM-Vet**(Yu et al., 2023)는 수학적 문제 해결 또는 시각적 유머 해석과 같은 실제 시나리오 처리에 필수적인 광범위한 능력을 평가하는 멀티모달 평가 벤치마크이다. 데이터세트는 다양한 온라인 플랫폼에서 수집된 187개의 이미지로 구성되며 205개의 질문을 제시하며, 각각은 답변을 위해 하나 이상의 기능을 적용해야 한다. 이러한 질문은 유형이 다양하며 다양한 길이의 개방형 응답이 필요하다.\n' +
      '* **Q-Bench**(Wu et al., 2023)는 낮은 수준의 시각과 관련된 세 가지 차원, 즉 인식, 설명 및 평가에 걸쳐 VLM을 평가한다. 인식을 평가하기 위해 프레임워크는 2,990개의 다양한 이미지를 활용하며, 각각은 낮은 수준의 속성에 초점을 맞춘 인간 생성 질문을 동반한다. 저수준 정보에 대한 VLM의 설명을 평가하기 위해, GPT와 관련된 비교 파이프라인과 함께 499개의 이미지에 대한 인간 라벨 텍스트 설명이 사용되며, 프레임워크는 VLM의 시각적 품질 평가 능력을 평가하여 인간의 의견 점수와 일치하도록 한다.\n' +
      '* **MathVista**(Lu et al., 2023)는 시각적 맥락 내에서 VLM의 수학적 추론 능력을 평가하며, 수학에 대한 28개의 기존 멀티모달 데이터 세트에서 6,141개의 예를 제공한다. MathVista는 종합적인 평가 플랫폼을 제공하여 세심한 시각적 이해와 구성 추론이 필요하며 최첨단 기반 모델에도 도전을 제기한다.\n' +
      '***AI2D**(Kembhavi et al., 2016), 또는 AI2 Diagrams는 5,000개 이상의 초등학교 과학 다이어그램을 포함하는 데이터 세트이다. 여기에는 풍부한 구문 구문 분석 및 15,000개 이상의 해당 객관식 질문과 함께 구성 요소와 관계에 대한 포괄적인 주석이 포함된다.\n' +
      '***SEED-IMG**(Li et al., 2023b)는 이미지 모달리티에 초점을 맞춘 SEED-Bench의 서브세트를 포함한다. 원래의 SEED-벤치는 이미지 및 비디오 양식 모두에 대한 이해를 포함하여 12개의 평가 차원을 포함하는 정확한 인간 주석이 있는 19,000개의 객관식 질문을 포함한다.\n' +
      '* **HallusionBench**(Liu et al., 2023a)는 이미지-컨텍스트 추론 능력을 평가하기 위해 맞춤화된 포괄적인 벤치마크를 소개한다. 시각 정보의 미묘한 이해와 해석을 우선시합니다. 벤치마크는 1129개의 전문가 작성 질문이 포함된 346개의 이미지로 구성되어 모델 반응 경향, 논리적 일관성 및 다양한 실패 모드에 대한 정량적 분석이 가능하다.\n' +
      '\n' +
      '## 부록 C 비젼 언어 모델 평가\n' +
      '\n' +
      '***BLIP2**[11]은 냉동 유니모달 모델 간의 중간자 역할을 하는 Q-Pear를 도입하여 냉동 이미지 인코더에서 적절한 시각적 특징을 추출하여 냉동 대형 언어 모델에 제공하여 텍스트를 생성한다.\n' +
      '**InstructBLIP**[23]은 11개의 작업 범주에 걸쳐 명령어 조정 형식으로 변환된 26개의 데이터 세트를 포함하는 체계적인 연구를 통해 다양한 작업에 일반화하는 문제를 해결하기 위해 설계된 비전 언어 명령어 조정 프레임워크를 제시한다.\n' +
      '***Shikra**[1]은 VQA, 이미지 캡션, 표현 이해 참조 및 PointQA와 같은 위치 관련 작업과 같은 다양한 비전 언어 작업을 포괄하는 참조 대화 작업을 위해 설계된 통합 모델을 제안한다.\n' +
      '***IDEFICS**[12]는 각각 관련 이미지 및 텍스트를 포함하는 14억 1,100만 개의 멀티모달 영어 웹 문서로 구성된 큐레이티드 웹-스케일 데이터세트, 총 353M 이미지 및 115B 토큰을 소개한다. 그들은 웹 페이지 내에서 이미지의 자연스러운 맥락을 보존하는 완전한 멀티모달 문서를 제공하는 것을 목표로 한다.\n' +
      '***Qwen-VL, Qwen-VL-Chat**[23]은 Qwen 언어 모델을 기반으로 하는 고성능이고 다재다능한 비전 언어 모델의 모음인 Qwen-VL 시리즈를 소개한다. 다중 언어 및 다중 이미지 입력 처리, 세밀한 시각적 이해 기능을 지원합니다.\n' +
      '***MiniGPT-4**[24]는 비쿠나와 BLIP2의 Q-Former의 동결된 사전 훈련된 비전 구성요소를 결합한 비전 언어 모델을 제시하며 GPT-4에 의해 입증된 예외적 능력을 복제하는 것을 목표로 한다.\n' +
      '***MiniGPT-v2**[1]은 3단계의 학습 단계와 고해상도 영상의 활용을 통해, 태스크 지향적 명령어 학습 기법을 적용하여 다수의 비전 언어 태스크를 효과적으로 처리할 수 있도록 설계되었다.\n' +
      '***Otter**[11]은 OpenFlamingo와 MIMIC-IT(multi-modal in-context instruction tuning) 데이터셋을 활용하여 DeepMind Flamingo 간의 격차를 해결한다.\n' +
      '**LLaVA**[12]는 먼저 시각적 명령어 튜닝의 개념을 도입하여, 범용 시각적 어시스턴트를 개발하기 위해 언어만 명령어 튜닝을 비전 언어 명령어 튜닝으로 확장한다.\n' +
      '**LLaVA-XTuner**[13]은 범용 모델을 달성하기 위해 LLaVA를 미세 조정하는 도구이다.\n' +
      '**mPLUG-Owl**[25]는 다중 모달리티를 동시에 지원할 수 있는 대형 다중 모달 언어 모델에 대한 모듈화된 훈련 패러다임을 소개한다. 모듈화 개념에서 영감을 얻은 이 방법은 미리 훈련된 언어 모델, 시각적 지식 모듈 및 시각적 추상기 모듈을 통합하여 이미지와 텍스트 간의 효과적인 정렬을 달성한다.\n' +
      '* **mPLUG-Owl2**[25]는 모듈화된 네트워크 설계로 모달리티 협업과 간섭을 모두 처리할 수 있습니다. 그들은 협업을 촉진하기 위한 공유 기능 모듈과 서로 다른 모달리티를 효과적으로 관리하기 위한 모달리티 적응 모듈을 소개한다.\n' +
      '***ShareGPT4V**[1]은 현재 대형 멀티모달 모델은 고품질 이미지-텍스트 쌍의 부족으로 인해 차선책의 모달리티 정렬에 직면해 있다고 주장한다. 이 문제를 해결하기 위해 두 단계에서 더 큰 규모로 고품질 캡션을 수집했다. 이러한 노력은 100K GPT4-Vision 생성 캡션 및 그들의 캡션 모델에 의해 조작된 1.2M 캡션을 포함하는 ShareGPT4V 데이터세트의 생성으로 이어졌다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>