<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Training-Free Consistent Text-to-Image Generation\n' +
      '\n' +
      'Yoad Tewel\n' +
      '\n' +
      'Omri Kadupki\n' +
      '\n' +
      'Rinnon Gal\n' +
      '\n' +
      'Yonli Kaasten\n' +
      '\n' +
      'Lior Woller\n' +
      '\n' +
      'Gal Cheechik\n' +
      '\n' +
      'NVIDIAia Yuvala Atzmon\n' +
      '\n' +
      'NVIDIAia\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      'Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray _the same_ subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portray multiple subjects. Here, we present _ConsiStory_, a _training-free_ approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare _ConsiStory_ to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, _ConsiStory_ can naturally extend to multi-subject scenarios, and even enable training-free _personalization_ for common objects.\n' +
      '\n' +
      'Code will be available at our project page.\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Large-scale text-to-image (T2I) diffusion models empower users to create imaginative scenes from text, but their stochastic nature poses challenges when trying to portray _visually consistent_ subjects across an array of prompts. Such consistency is crucial for many applications: from illustrating books and stories, through designing virtual assets, to creating graphic novels and synthetic data.\n' +
      '\n' +
      'In the field of consistent image generation, current approaches [33, 4, 11, 26] predominantly rely on _personalization_, a process where the text-to-image model learns a new word to represent a _specific_ subject in a given image set. However, these personalization-based methods suffer from several drawbacks: They require per-subject training; they struggle to portray multiple consistent subjects simultaneously in one image; and they can suffer from trade-offs between subject consistency and prompt-alignment. Alternatives, like training image-conditioned diffusion models (_e.g._ using an encoder [54, 14, 57]), require significant computational resources, andtheir extension to multi-object scenes remains unclear. A common thread in all these approaches is that they attempt to enforce consistency _a posteriori_. That is, they operate to make generated images consistent with a specific, given target. Such approaches have two drawbacks. They are bound to constrain the model\'s "creativity" to the given target image, and they tend to drive the model away from its training distribution.\n' +
      '\n' +
      'We show here that the limitations of a posteriori methods can be avoided, and propose a way to achieve consistency in a _zero-shot_ manner - without conditioning on existing images. The key idea is to promote cross-frame consistency _a priori_ during generation. To achieve this, we leverage the internal feature representations of the diffusion model to align the generated images with each other, without any need to further align them with an external source. In doing so, we can enable on-the-fly consistent generation (Fig. 1), without requiring lengthy training or backpropagation, making generation roughly x20 faster than the current state-of-the-art.\n' +
      '\n' +
      'Our approach operates in three steps. First, we localize the subject across a set of noisy generated images. We then encourage subject consistency by allowing each generated image to attend to subject patches in other frames via an extension of the self-attention mechanism. This leads to more consistent subjects across the batch but causes the layout diversity to greatly diminish, as observed in other contexts that use similar extension (Kang et al., 2019). Our second step is, therefore, to maintain diversity in two ways: by incorporating features from a vanilla, non-consistent sampling step, and by introducing a new inference-time dropout on the shared keys and values. Finally, we aim to enhance consistency in finer details. To achieve this, we align the self-attention _output_ features between corresponding subject pixels across the entire set.\n' +
      '\n' +
      'Our full method, which we term _ConsiStory_, combines these components to enable training-free consistent generation. We compare _ConsiStory_ to prior approaches and demonstrate that by aligning features during the generative process, we not only substantially speed up the process, but also maintain better prompt-alignment. Importantly, our method is trivial to extend to multi-subject scenes, avoiding pitfalls introduced by personalization-based approaches.\n' +
      '\n' +
      'Finally, we show that _ConsiStory_ is compatible with existing editing tools like ControlNet (Wang et al., 2019), we introduce methods for re-using the consistent identities, and even apply our ideas to training-free personalization for common object classes, being _the first to show_ training-free personalization, with no encoder use.\n' +
      '\n' +
      'In summary, this paper makes the following contributions: First, we present a training-free method for achieving subject consistency across varying prompts. Second, we develop new techniques to combat layout collapse in extend-attention applications. Additionally, we share a new benchmark dataset for consistency evaluation.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      'Consistent T2I and storybook generationConsistent character generation has been studied largely in the context of storybook generation (Kang et al., 2019; Wang et al., 2020). Early works utilized extensive fine-tuning and personalization (Kang et al., 2019; Zhang et al., 2020) to promote consistency. Jeong et al. (2020) replace character faces by combining textual inversion (Kang et al., 2019) and image editing techniques. Gong et al. (2020) iteratively generate multi-character images using models personalized with LoRA (Gong et al., 2020), and require pre-training a text-to-layout model. Others (Kang et al., 2019; Wang et al., 2020; Wang et al., 2020) fine-tune a T2I model on storyboard datasets, enabling conditioning on image frames. This resembles encoder-based personalization methods (Kang et al., 2019; Wang et al., 2020; Wang et al., 2020; Wang et al., 2020) like IP-Adapter (Wang et al., 2020) and ELITE (Wang et al., 2020). Richardson et al. (Richardson et al., 2020) optimize a token embedding to generate consistent, novel concepts. In a concurrent work, (Beng et al., 2020) train a personalized LoRA model by iteratively generating images, clustering them to find similar outputs, learning a personalized model on a highly-similar cluster, and repeating the process.\n' +
      '\n' +
      '_ConsiStory_ does not tune or personalize the pre-trained T2I model. Instead, it leverages cross-frame feature sharing to promote subject consistency during generation.\n' +
      '\n' +
      'Attention-based consistencyIn the realm of videos, a common practice is to increase temporal consistency by sharing self-attention keys and values (Wang et al., 2020) across frames. This can be done for generation (Wang et al., 2020; Wang et al., 2020; Wang et al., 2020) or for video editing (Kang et al., 2019; Wang et al., 2020). Others use attention keys and values from a source image in order to inject a consistent identity across video frames (Wang et al., 2020; Wang et al., 2020; Wang et al., 2020; Wang et al., 2020).\n' +
      '\n' +
      'When considering images, early works in text-based editing (Kang et al., 2019; Wang et al., 2020; Wang et al., 2020) proposed to maintain the structure of an image by extracting its attention masks or features, and injecting them into follow-up generations. Others optimized image latents or model conditions such that attention maps created during generation will align with some external mask (Wang et al., 2020; Wang et al., 2020).\n' +
      '\n' +
      'More recent work explored extended-attention mechanisms to maintain consistent appearances when modifying image layouts (Wang et al., 2020; Wang et al., 2020), or for training-free appearance- (Beng et al., 2020) and style-transfer (Kang et al., 2019) tasks.\n' +
      '\n' +
      'Our method draws on these attention-sharing ideas but applies them to the task of consistent T2I generation. We do not draw features from existing images or align entire frames, but develop tools to enable subject-level consistency across novel images.\n' +
      '\n' +
      'Appearance transfer using dense correspondence mapshas been widely studied. Liao et al. (Liao et al., 2019) transfer appearance between images with similar structures using VGG-based maps. Others (Wang et al., 2020; Wang et al., 2020) trained generative models to leverage these mappings for image-to-image translation. Recently, diffusion models have been found to establish strong zero-shot correspondence between images (Kang et al., 2019; Wang et al., 2020; Wang et al., 2020), enabling applications like instance swapping, image editing, and robust registration.\n' +
      '\n' +
      'Here, we leverage the diffusion-based DIFT maps (Dosov et al., 2016) to share features across multiple images and encourage the generation of subjects with consistent appearance. This aligns features throughout the denoising process rather than doing appearance transfer as a post-hoc step.\n' +
      '\n' +
      '## 3. Preliminaries: Self-Attention in T2I Models\n' +
      '\n' +
      'Our method manipulates self-attention in T2I diffusion models. We start by outlining its mechanism and introducing key notations.\n' +
      '\n' +
      'A self-attention layer receives a series of tokens, each of which contains features describing a single image _patch_. Each such token undergoes linear projections through three self-attention matrices: \\(W_{K}\\), \\(W_{V}\\) and \\(W_{Q}\\). The results of these projections are known as "Keys", "Values" and "Queries", respectively.\n' +
      '\n' +
      ' More concretely, consider the \\(i^{th}\\) image entry in the generated batch. Let \\(x_{i}\\in\\mathbb{R}^{P\\times d}\\) be a sequence of \\(P\\) input token vectors with feature dimension \\(d\\). We define \\(K_{i}=x_{i}\\cdot\\mathbf{W}_{K}\\), \\(V_{i}=x_{i}\\cdot\\mathbf{W}_{V}\\), \\(Q_{i}=x_{i}\\cdot\\mathbf{W}_{Q}\\). The self-attention map is then given by:\n' +
      '\n' +
      '\\[A_{i}=\\textit{softmax}\\left(Q_{i}K_{i}^{\\top}/\\sqrt{d_{k}}\\right)\\in\\mathbb{R }^{P\\times P}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(d_{k}\\) is the feature dimension of \\(\\mathbf{W}_{K}\\), \\(\\mathbf{W}_{Q}\\) projections. Intuitively, this map provides a relevancy score between every pair of patches in the image. It is then used to weight how much the "Value" features of a given target patch should influence a source patch \\(h_{i}=A_{i}\\cdot V_{i}\\), where \\(h\\) denotes an intermediary, hidden feature set.\n' +
      '\n' +
      'These are projected using a fourth, "output-projection" matrix, \\(\\mathbf{W}_{O}\\), yielding \\(x_{i}^{out}=\\mathbf{W}_{O}\\cdot h_{i}\\), which is then summed with the input features \\(x_{i}\\) to create the input for the next layer.\n' +
      '\n' +
      'Our method intervenes in this self-attention mechanism by allowing images in a generated batch to attend to each other, and be influenced by each other\'s \\(x^{out}\\) activations.\n' +
      '\n' +
      '## 4. Method\n' +
      '\n' +
      'Our goal is to generate a set of images portraying consistent subjects across an array of prompts. We propose to do so by better aligning the internal activation of the T2I model during image denoising. Importantly, we aim to enforce consistency exclusively through an inference-based mechanism, without additional training.\n' +
      '\n' +
      'Our approach is comprised of three main components. First, we introduce a **subject-driven self-attention** mechanism (SDSA), aimed at sharing subject-specific information across relevant model activations in the generated image batch. Second, we observe that the above component comes at the cost of reducing the variation in the generated layouts. Therefore, we propose strategies for mitigating this form of mode collapse through an attention-dropout mechanism, and by blending query features obtained from a vanilla, non-consistent, sampling step. Third, we incorporate a **feature injection** mechanism to further refine the results. There, we map features from one generated image to another based on a cross-image dense-correspondence map derived from the diffusion features. Below, we outline each of these components in detail.\n' +
      '\n' +
      '### Subject-driven self-attention\n' +
      '\n' +
      'Consider a simple idea for promoting consistency: expanding the self-attention, so that queries from one image can also attend to keys and values from other images in the batch. This enables repeated objects to naturally attend to each other, thus sharing visual features across images. This idea is often used in video generation and editing works (Song et al., 2018), leading to increased consistency across frames. However, generated videos differ from our scenario. First, they are created with a single prompt that is shared across frames. Second, they typically require little variation in backgrounds or layout from one frame to the next. In contrast, we want each frame to follow a unique prompt, and we want to maintain diversity in backgrounds and layouts. Naively employing these video-based mechanisms leads to uniform backgrounds and drastically reduced alignment with each image\'s prompt - in line with expectations for a single video scene, but in direct conflict with our objectives.\n' +
      '\n' +
      'One way to tackle these limitations is by reducing the amount of information being shared at background patches. As we are only concerned about sharing subject appearance, we mask the expanded self-attention, so that queries from one image can only match keys and values from the same image, or from regions containing the subject in other images. This way, features for repeated subject elements can be shared, while background features remain separate.\n' +
      '\n' +
      'To this end, we employ a similar approach to prior art (Beng et al., 2018; Chen et al., 2018) and identify noisy latent patches that are likely to contain the subject using cross-attention features. Specifically, we average and threshold the cross-attention maps related to the subject token across\n' +
      '\n' +
      'Figure 2. **Architecture outline (left): Given a set of prompts, at every generation step we localize the subject in each generated image \\(I_{i}\\). We utilize the cross-attention maps up to the current generation step, to create subject masks \\(M_{i}\\). Then, we replace the standard self-attention layers in the U-net decoder with Subject Driven Self-Attention layers that share information between subject instances. We also add Feature Injection for additional refinement. Subject Driven Self-Attention (right): We extend the self-attention layer so the Query from generated image \\(I_{i}\\) will also have access to the Keys from all other images in the batch (\\(I_{j}\\), where \\(j\\neq i\\)), restricted by their subject masks \\(M_{j}\\). To enrich diversity we: (1) Weaken the SDSA via dropout and (2) Blend Query features with vanilla Query features from a non-consistent sampling step, yielding \\(Q_{1}^{*}\\).**\n' +
      '\n' +
      'diffusion steps and layers to create subject-specific masks (details in Appendix B). With these masks, we propose Subject-Driven Self-Attention (SDSA) where attention is masked so each image can only attend to its own patches or the subject patches within the batch (See Fig. 2).\n' +
      '\n' +
      '\\[K^{+} =[K_{1}\\oplus K_{2}\\oplus\\ldots\\oplus K_{N}]\\in\\mathbb{R}^{N\\cdot P \\times d_{k}}\\] \\[V^{+} =[V_{1}\\oplus V_{2}\\oplus\\ldots\\oplus V_{N}]\\in\\mathbb{R}^{N\\cdot P \\times d_{o}}\\] \\[M^{+}_{i} =[M_{1}\\ldots M_{i-1}\\oplus\\mathds{1}\\oplus M_{i+1}\\ldots M_{N}]\\] \\[A^{+}_{i} =softmax\\left(Q_{i}K^{+\\top}/\\sqrt{d_{k}}+\\log M^{+}_{i}\\right) \\in\\mathbb{R}^{P\\times N\\cdot P}\\] \\[h_{i} =A^{+}_{i}\\cdot V^{+}\\in\\mathbb{R}^{P\\times d_{o}}. \\tag{2}\\]\n' +
      '\n' +
      'Here, \\(M_{i}\\) is the subject mask for the \\(i^{th}\\) entry in the batch, and \\(\\oplus\\) indicates matrix concatenation. We use standard attention masking, which null-out softmax\'s logits by assigning their scores to \\(-\\infty\\) according to the mask. Note that the Query tensors remain unaltered, and that the concatenated mask \\(M^{+}_{i}\\) is set to be an array of 1\'s for patch indices that belong to the \\(i^{th}\\) image itself.\n' +
      '\n' +
      '### Enriching layout diversity\n' +
      '\n' +
      'The use of SDSA restores prompt alignment and avoids background collapse. However, we observe that it can still lead to excessive similarity between image layouts. For example, subjects will typically be generated in similar locations and poses.\n' +
      '\n' +
      'To improve the diversity of our results, we propose two strategies: first, incorporating features from a vanilla, non-consistent sampling step; and second, further _weakening_ the subject-driven shared attention through a dropout mechanism.\n' +
      '\n' +
      '**Using Vanilla Query Features.** Recent work (Garf et al., 2018), demonstrated that one can use diffusion models to combine the appearance of one image with the structure of another. They do so by injecting self-attention Keys and Values from the appearance image, and Queries from the structure image. Inspired by this, we aim to enhance pose variation by aligning more closely with a structure predicted by a more diverse vanilla forward pass (_i.e._ without our modifications). We focus on the early steps of the diffusion process, which have been shown to primarily control layout (Garf et al., 2018; Wang et al., 2018), and apply the following query-blending mechanism: Let \\(z_{t}\\) be the noisy latents at step \\(t\\). We first apply a vanilla denoising step to \\(z_{t}\\), without SDSA, and cache the self-attention queries generated by the diffusion network: \\(Q^{\\textit{main}}_{t}\\). Then, we denoise the same latents \\(z_{t}\\) again, this time using SDSA. During this second pass, for all SDSA layers, we linearly interpolate the generated queries towards the vanilla queries, resulting in:\n' +
      '\n' +
      '\\[Q^{*}_{t}=(1-v_{t})Q^{\\textit{SDSA}}_{t}+v_{t}Q^{\\textit{ganilla}}_{t}, \\tag{3}\\]\n' +
      '\n' +
      'where \\(v_{t}\\) is a linearly decaying blending parameter (see Appendix).\n' +
      '\n' +
      '**Self-Attention Dropout** Our second strategy to enhance layout variation involves weakening SDSA using a dropout mechanism. Specifically, at each denoising step, we randomly nullify a subset of patches from \\(M_{i}\\) by setting them to 0. This weakens the attention sharing between different images and subsequently promotes richer layout variations. Notably, by adjusting the dropout probability, we can regulate the strength of consistency, and strike a balance between visual consistency and layout variations.\n' +
      '\n' +
      'Through these two mechanisms, we aim to tackle two aspects of the layout-collapse problem: Query-feature blending allows to retain aspects of diversity from the non-consistent sampling, while attention-dropout encourages the model to rely less on the shared keys and values, avoiding over-consistency. By mixing them, we achieve increased diversity without significant harm to consistency.\n' +
      '\n' +
      '### Feature injection\n' +
      '\n' +
      'The shared attention mechanism notably improves subject consistency but may struggle with fine visual features, which may hurt the subject\'s identity. Hence, we propose to further improve consistency through a novel cross-image **Feature Injection** mechanism.\n' +
      '\n' +
      'Here, we aim to improve the similarity of features from corresponding regions (_e.g._ the left eye) across different images in the batch. Specifically, we find that substantial texture information is contained in the self-attention output features, \\(\\mathbf{x}^{out}\\), and aligning these features between matching areas can enhance consistency.\n' +
      '\n' +
      'To align these features, we first build a patch correspondence map between every pair of images \\(I_{t}\\) and \\(I_{s}\\) in the batch, using DIFT (Zhou et al., 2017) features \\(D_{t}\\) and \\(D_{s}\\) (See Appendix). We denote the correspondence map by \\(C_{t\\to s}\\). Intuitively, when applied on patch index \\(p\\) from \\(I_{t}\\), \\(C_{t\\to s}[p]\\) yields the most similar patch in \\(I_{s}\\), as illustrated in Fig. 3.\n' +
      '\n' +
      'Then, to promote feature similarity, we can blend corresponding features based on this mapping. We extend this idea to a _many-to-one_ scenario, where each image \\(I_{t}\\) is blended with the other images in the batch. For each patch index \\(p\\) in image \\(I_{t}\\), we compare its corresponding patches in all other images and select the one with the highest cosine similarity in the DIFT feature space. Formally:\n' +
      '\n' +
      '\\[\\operatorname{src}(p)=\\operatorname*{arg\\,max}_{s\\neq t}similarity(D_{t}[p],D_{s} [C_{t\\to s}[p]]), \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\operatorname{src}(p)\\) is the "best" source patch for the target patch \\(p\\), and _similarity_ is the cosine similarity score.\n' +
      '\n' +
      'Finally, we blend the self-attention output layer features of the target image \\(\\mathbf{x}^{out}_{t}\\), and its corresponding source patches, \\(\\mathbf{x}^{out}_{s}\\).\n' +
      '\n' +
      'Figure 3. **Feature Injection: To further refine the subject’s identity across images, we introduce a mechanism for blending features within the batch. We extract a patch correspondence map between each pair of images (Middle), and then inject features between images based on that map (Right).**\n' +
      '\n' +
      '\\[\\hat{x_{t}}^{out}=(1-\\alpha)\\cdot x_{t}^{out}+\\alpha\\cdot\\text{src}(x_{t}^{out}), \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\alpha\\) is a blending parameter, and \\(\\text{src}(x_{t}^{out})\\in\\mathbb{R}^{P\\times d}\\) is the tensor obtained by pooling the corresponding features for each patch \\(p\\) in \\(x_{t}^{out}\\) from the associated patch \\(src(p)\\).\n' +
      '\n' +
      'In practice, to enforce consistency between appearances of the same subject, without affecting the background, we exclusively apply the feature injection according to the subject masks \\(M_{i}\\). Additionally, we apply a threshold to inject features only between patches with high enough similarity in the DIFT space (see Appendix). This approach ensures that features contributing to the appearance of the subject are collectively drawn from all source images, promoting a more comprehensive and representative synthesis.\n' +
      '\n' +
      '### Anchor images and reusable subjects\n' +
      '\n' +
      'As an additional optimization, we can reduce the computational complexity of our approach by designating a subset of generated images as **"anchor images"**. Rather than sharing keys and values across all generated images during SDSA steps, we allow the images to only observe keys and values derived from the anchors. Similarly, for feature injection, we only consider the anchors as valid feature sources. Note that the anchors can still observe each other during generation, but they too do not observe features from non-anchor images. We find that for most cases, two anchors are sufficient.\n' +
      '\n' +
      'This offers several benefits: First, it allows for faster inference and reduced VRAM requirements, because it restricts the size of extended attention. Second, it can improve generation quality in large batches, where we notice it can reduce visual artifacts. Most importantly, we can now reuse the same subjects in novel scenes by creating a new batch where the same prompts and seeds are used to re-create the anchor images, but the non-anchor prompts have changed. Through this mechanism, our approach can yield **reusable subjects**, and unlimited consistent image generation.\n' +
      '\n' +
      '### Multi-subject consistent generation\n' +
      '\n' +
      'Personalization-based approaches struggle in maintaining consistency over _multiple_ subjects within a single image [17, 30, 42, 50]. However, with _ConsiStory_, multi-subject consistent generation is possible in a simple, straightforward manner, by simply taking a _union_ of the subject masks. When the subjects are semantically different, information leakage between them is not a concern. This is due to the exponential form of the attention softmax, which acts as a gate that suppresses information leakage between unrelated subjects. Similarly, thresholding the correspondence map during feature injection yields a gating effect that safeguards against information leakage. Additional details such as hyperparameter choices are in the supplementary.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'We begin our evaluation by comparing _ConsiStory_ with a range of prior and concurrent baselines. We open with a qualitative comparison, showing that our method can achieve improved subject-consistency and higher prompt-alignment when compared to the state-of-the-art. Next, we evaluate our method quantitatively, including a large-scale user study which demonstrates that users typically favor our results. Moving on, we conduct an ablation study to highlight the contribution and effect of each component in our method. Finally, we conclude with a set of extended applications, showing that our method is compatible with existing tools such as ControlNet [59], and it can even be used to enable training-free personalization for common object classes.\n' +
      '\n' +
      '### Evaluation baselines\n' +
      '\n' +
      'We compare our method to three classes of baselines: **(1)** The baseline SDXL model, without adaptations. **(2)** Optimization-based personalization approaches that teach the model about a new subject by fine-tuning parts of the model: Textual Inversion (TI) [13] fine-tunes the text encoder\'s word embeddings. DreamBooth-LoRA (DB-LORA) [46] tunes the diffusion U-Net using Low Rank Adaptation [22]. **(3)** Encoder-based approaches that take a single image as input and output a conditioning code to the diffusion model: IP-Adapter [57], ELITE [54] and E4T [14]. For the personalization and encoder baselines, we first generate a single image of a target subject using a prompt describing the subject, then use it to personalize the diffusion model. All methods except ELITE are based on a pre-trained SDXL model.\n' +
      '\n' +
      'For _ConsiStory_, we use two anchor images and 0.5 dropout. For the automated metric, we also use lower dropout values.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      'In Fig. 4 and Fig. A.1, we show qualitative comparisons. Our method can achieve a high degree of subject consistency, while better adhering to the text prompts. Tuning-based personalization approaches tend to either overfit the single training image, producing no variations, or underfit and fail to maintain consistency. IP-Adapter similarly struggles to match complex prompts, particularly when styles are involved. Our method can successfully achieve both subject consistency and text-alignment. Additionally, in Fig. 5 we show that with different initial noise inputs, our method can generate varied sets of consistent images. Additional results are shown in the supplementary, including the qualitative comparisons with ELITE [54] and E4T [14].\n' +
      '\n' +
      '**Multi-Subject Generation** In Fig. 1 (bottom) we demonstrate that _ConsiStory_ can create scenes with multiple consistent subjects. In Fig. 6 we further compare our method to LORA-DB. Notably, LORA-DB tends to neglect the consistency of one or even both subjects. This pitfall is common when combining personalization approaches, as they learn each subject in isolation. In contrast, our method simply builds on the diffusion model\'s inherent compositional ability. In Figures A.2, A.3 we provide additional comparisons.\n' +
      '\n' +
      'Fig. 15 provides more results with single and multiple subjects.\n' +
      '\n' +
      '(1) a subject description, _e.g._, "_A red dragon_" (2) a setting description, _e.g._, "_blowing bubbles_" or "_in a castle_", and (3) a style descriptor, _e.g._, "_Origami style_". For subject descriptions, we utilized both detailed examples (_"A red dragon_"), and non-detailed ones (_"A dragon"_). For setting descriptions, we asked ChatGPT to provide descriptions that naturally fit the subject. Each of the 100 sets contains prompts sharing the same subject description and style but with varying setting descriptions. Further details can be found in Appendix D.\n' +
      '\n' +
      'We follow the concurrent work of Avrahami et al. [4] and evaluate the methods on two axes - prompt-alignment, and subject consistency. For prompt-alignment, we use CLIP to measure the similarity between each generated image and its conditioning prompt, and report the average CLIP-score [21] over all 500 generated images.\n' +
      '\n' +
      'Fig. 4: **Qualitative Results** We evaluated our method against IP-Adapter, Tl, and DB-LORA. Some methods failed to maintain consistency (TI), or follow the prompt (IP-Adapter). Other methods alternated between keeping consistency or following text, but not both (DB-LoRA). Our method successfully followed the prompt while maintaining consistency. Additional results are shown at Figure A.1For consistency evaluation, we use DreamSim [12], which has been shown to better correlate with human judgment of inter-image similarity. We calculate the pair-wise similarity between each pair of images in each of the 100 sets. To focus on subject consistency, we follow Dreamsim\'s background removal protocol. We report the average score over these sets. In our results, the error bars represent the Standard Error of the Mean (S.E.M).\n' +
      '\n' +
      'The results are provided in Fig. 7. As can be seen, our method is situated on the Pareto-front. Our standard setup matches SDXL in text-alignment scores, demonstrating that promoting consistency a-priori can better maintain the model\'s knowledge.\n' +
      '\n' +
      'However, qualitatively, we observed that the consistency metric tends to bias its scores towards configurations with slight layout changes, rather than assessing consistency in the subject\'s identity. Therefore, given the limitations of the automated metric, we conducted a large-scale user study. We concentrated on the most effective techniques, omitting both vanilla SDXL and ELITE. We used the standard two-alternative forced-choice format. Users faced two types of questions: (1) Subject-consistency, where they were shown two sets of 5 images each. They were asked to choose the set that better shows the same subject, ignoring background, pose, and image quality. (2) Text-alignment, where they selected the image that best matched a textual description from two images. We gathered 500 responses per baseline for each question type, totaling \\(3,000\\) responses. The results are shown in Fig. 8. Despite being a training-free approach, _ConsiStory_ outperforms the baselines, both regarding textual alignment and subject consistency.\n' +
      '\n' +
      '**Runtime comparison** We conducted a runtime analysis of the main methods, focusing on their time-to-consistent-subject (TTCS) using an Hi100 GPU. Our method, _ConsiStory_, achieved the fastest TTCS result, at 32 seconds for generating two anchors and an image based on a new prompt. This is x25 faster than the SoTA approach by Avrahami et al. [4] which is estimated at 13 minutes on an Hi100 GPU. Moreover, our method is x8-14 faster than LORA-DB (4.5 min.) and TI (7.5 min.). Comparing our approach to encoder-based methods like IP-Adapter is more convoluted. These techniques require _weeks_ of pre-training, but once trained, they can generate an anchor and another image based on a new prompt in just 8 seconds.\n' +
      '\n' +
      'Fig. 5: **Seed Variation.** Given different starting noise, _ConsiStory_ generates different consistent set of images.\n' +
      '\n' +
      'Fig. 8: **User Study** results indicate a notable preference among participants for our generated images both in regards to Subject Consistency (_Visual_) and Textual Similarity (_Textual_).\n' +
      '\n' +
      'Fig. 6: **Multiple Subjects:**_ConsiStory_ generates multiple consistent subjects, while other methods often neglect at least one subject.\n' +
      '\n' +
      'Fig. 7: **Subject Consistency VS Textual Similarity:**_ConsiStory_ (green) achieves the optimal balance between Subject Consistency and Textual Similarity. Encoder-based methods such as ELITE and IP-Adapter often overfit to visual appearance, while optimization-based methods such as LoRA-DB and TI do not exhibit high subject consistency as in our method. \\(d\\) denotes different self-attention dropout values. Error bars are S.E.M.\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      'We now move to evaluate the impact of different components of our own method through an ablation study of the following components: **(1)** SDSA steps **(2)** Feature Injection (FI) **(3)** Attention dropout and query-feature blending. **(4)** Without using a subject mask. Qualitative comparisons are provided in Figures 9, 10. Quantitative results are provided in Appendix A.4.\n' +
      '\n' +
      'Removing SDSA leads to poor consistency, both regarding shape and texture. Removing FI yields subjects with similar shapes but less accurate identities. Finally, removing the vanilla query blending and attention dropout leads to significant reductions in layout diversity. We quantify this loss of diversity in Appendix C.\n' +
      '\n' +
      '### Extended Applications\n' +
      '\n' +
      '**Spatial Controls.** We first demonstrate that _ConsiStory_ is compatible with existing guided generation tools like ControlNet (Xu et al., 2019). As these are compatible with standard personalization methods (Beng et al., 2019; Chen et al., 2020; Zhang et al., 2020), we want to ensure that our alternative approach maintains this compatibility. In Fig. 11 we show that pose-based controls with ControlNet successfully guide our consistent image generation method.\n' +
      '\n' +
      '**Training-free Personalization.** We are _the first to show_ training-free _personalization_ (Fig. 12), where _ConsiStory_ enables personalization without any tuning or encoder use. Specifically, we show how to personalize common subject classes. Given two images of a subject, we invert them using Edit Friendly DDPM-Inversion (Zhu et al., 2020). These latents and noise maps are used as anchors for _ConsiStory_, allowing the rest of the batch to draw on their visual appearance. This application requires minor modifications of _ConsiStory_, which we detail in Appendix E.\n' +
      '\n' +
      'In Fig. A.6 we show that this approach struggles with complex objects, and is incompatible with style-changing prompts. Yet, we believe it can serve as a quick and cheaper alternative to current personalization methods, and leave it for future work.\n' +
      '\n' +
      'Fig. 11. **ControlNet Integration.** Our method can be integrated with ControlNet to generate a consistent character with pose control.\n' +
      '\n' +
      'Fig. 12. **Training-Free Personalization.** We utilize edit-friendly inversion to invert 2 real images per subject. These inverted images used as anchors in our method for training-free personalization.\n' +
      '\n' +
      'Fig. 10. **Subject Masking:** Without the subject mask, there is noticeable background leakage across images.\n' +
      '\n' +
      'Fig. 9. **Component Ablation** We ablated several components: Subject-Driven Self Attention (SDSA), Feature Injection (FI), and our variation enriching strategies: self-attention dropout and Query-feature blending (Variation). All ablated cases fail to maintain consistency as our method.\n' +
      '\n' +
      '## 6. Conclusions and Limitations\n' +
      '\n' +
      'We introduced _ConsiStory_, a training-free approach for creating visually consistent subjects using a pre-trained text-to-image diffusion model. When compared to the state of the art, our method is not only \\(\\times 20\\) faster, but can better preserve the output\'s alignment with the given prompts. Moreover, our method can be easily extended to tackle more challenging cases such as multi-subject scenarios, and even enable training-free personalization for common objects.\n' +
      '\n' +
      'Our approach has several limitations, shown in Figure 13. First, it relies on the localization of objects through cross-attention maps. This process may occasionally fail, particularly when dealing with unusual styles. However, from our observations so far, such failures appear relatively infrequent (fewer than 5%), and they can be resolved by simply changing the seed. Another limitation is found in the entanglement between appearance and style. Our method struggles to separate the two, and hence we are limited to consistent generations where images share the same style. Finally, we observe that the underlying SDXL model may exhibit biases towards certain groups. We demonstrate that these biases can be significantly reduced by specifying modifiers like gender or ethnicity, as illustrated in Figure 14.\n' +
      '\n' +
      'We hope that our results will assist in a consistent generation of creative endeavors and that they will inspire others to continue exploring training-free alternatives to personalization-based tasks.\n' +
      '\n' +
      '###### Acknowledgements.\n' +
      '\n' +
      ' We thank Assaf Shocher, Eli Meirom, Chen Tessler, Dvir Samuel and Yael Vinker for useful discussions and for providing feedback on an earlier version of this manuscript.\n' +
      '\n' +
      'Figure 14. **Model Bias.** The underlying SDXL model may exhibit biases towards certain ethnic groups, and our approach inherits them. Our method can generate consistent subjects belonging to diverse groups when these are highlighted in the prompt.\n' +
      '\n' +
      'Figure 13. **Limitations:** Our method often struggles with different styles in the same set of image (Top), and is dependent on the quality of the model cross-attention to localize the subject correctly (Bottom).\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:10]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1)\n' +
      '* Alahf et al. (2023) Yuval Alahf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. 2023. Cross-Image Attention for Zero-Shot Appearance Transfer. arXiv:2311.03335 [cs.CV]\n' +
      '* Arzar et al. (2023) Mooah Arzar, Riton Gil, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermarno. 2023. Domain-agnostic tuning-encoder for fast personalization of text-to-image models. In _SIGGRAPH Asia 2023 Conference Papers_. 1-10.\n' +
      '* Arvashami et al. (2023) Omri Arvashami, Krik Aherman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-A-Scene: Extracting Multiple Concepts from a Single Image. In _SIGGRAPH Asia 2023 Conference Papers_, _Sydney_, Nowst, Australia) _(SA \'23)_. Association for Computing Machinery, New York, NY, USA, Article 96, 12 pages. [https://doi.org/10.1145/3610548.361854](https://doi.org/10.1145/3610548.361854)\n' +
      '* Arvashami et al. (2023) Omri Arvashami, Amir Hertz, Yael Vinker, Mooah Arzar, Shilon Fuchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. The Chosen One: Consistent Characters in Text-to-Image Diffusion Models. _arXiv preprint arXiv:2311.10093_ (2023).\n' +
      '* Balaji et al. (2022) Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laing, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. 2022. eprint: Text-to-Image Diffusion Models with Ensemble of Expern Devices. _arXiv preprint arXiv:2211.01324_ (2022).\n' +
      '* Benain et al. (2020) Sagei Benain, Ron Mokady, Amit Bermano, Daniel Cohen-Or, and Iior Wolf. 2020. Structural-analog from a Single Image Pair. _Computer Graphics Forum_ n (2020). [https://doi.org/10.1111/cgf.14186](https://doi.org/10.1111/cgf.14186)\n' +
      '* Car et al. (2020) Mingsden Car, Xintao Wang, Zhongta Qi, Ying Shan, Xiaubin Qu, and Yingjiang Zheng. 2020. MassCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_. 22560-22570.\n' +
      '* Ceylan et al. (2023) Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. 2023. PixEvideo: Video editing using image diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 23206-23217.\n' +
      '* Chang et al. (2023) Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Solyczynani. 2023. MagicDance: Bastile Human Dance Video Generation with Motions & Facial Expressions Transfer. _arXiv preprint arXiv:2311.21052_ (2023).\n' +
      '* Cheter et al. (2023) Hils Cheter, Yuval Alahf, Yael Vinker, Iior Wolf, and Daniel Cohen-Or. 2023. Attend-and-ex-active Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics (TOG)_ 42, 4 (2023), 1-10.\n' +
      '* Feng et al. (2023) Zhangy Feng, Yuchen Ren, Xinmao Fu, Xiaecheng Feng, Duyu Tang, Shuming Shi, and Bing Qin. 2023. Improved Visual Story Generation with Adaptive Context Modeling. _arXiv preprint arXiv:2305.16811_ (2023).\n' +
      '* Fu et al. (2023) Stepanie Fu, Natenah Yadik Tanri, Shobita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023. Dreamsim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. In _Thirty-seventh Conference on Neural Information Processing Systems_. [https://openreview.net/forum?id=DENSWithL%7](https://openreview.net/forum?id=DENSWithL%7)\n' +
      '* Gal et al. (2022) Rinon Gal, Yuval Alahf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An Image is Worth One!: From-Word: Personalizing Text-to-Image Generation using Textual Inversion. [https://doi.org/10.48550/ARXIV.2208.01618](https://doi.org/10.48550/ARXIV.2208.01618)\n' +
      '* Gal et al. (2023) Rinon Gal, Mooah Arzar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2023. Encoder-based domain tuning for fast personalization of text-to-image models. _ACM Transactions on Graphics (TOG)_ 42, 4 (2023), 1-13.\n' +
      '* Geyer et al. (2023) Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tail Dekel. 2023. TokenFlow: Consistent Diffusion Features for Consistent Video Editing. _arXiv preprint arxiv:2307.10737_ (2023).\n' +
      '* Gong et al. (2023) Ivan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Haoxin Chen, Longyguue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujui Yang. 2023. TaleCrafer: Interactive Story Visualization with Multiple Characters. _arXiv preprint arXiv:2305.18427_ (2023).\n' +
      '* Gu et al. (2023) Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyong Xiao, Rui Zhao, Shuming Chang, Weizia Wu, et al. 2023. Mix-of-Show: Decentralized Non-Rank Adaptation for Multi-Concept Customization of Diffusion Models. _arXiv preprint arXiv:2305.18292_ (2023).\n' +
      '* Heeidl et al. (2023) Eric Heeidl, Gopal Sharma, Shirotahiu, Hossan Isack, Abhishek Kar, Andrea Taglassachi, and Kwong Mo. Yio. 2023. Unsupervised Semantic Correspondence Using Stable Diffusion. (2023). arXiv:2305.15581 [cs.CV]\n' +
      '* Hertz et al. (2022) Amir Hertz, Ron Mokady, Jay Tenenbaum, Kifr Aherman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. (2022).\n' +
      '* Hertz et al. (2023) Amir Hertz, Andrey Vorony, Shilon Fuchter, and Daniel Cohen-Or. 2023. Style Assigned Image Generation via Shared Attention. (2023).\n' +
      '* Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 5714-5728. [https://doi.org/10.18635/v1/2021.emnlp-main.595](https://doi.org/10.18635/v1/2021.emnlp-main.595)\n' +
      '* Heuard et al. (2021) Edward J. Hu, Velong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanhi Li, Shean Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. _ArXiv preprint arXiv:2106.09685_ (2021).\n' +
      '* Hu et al. (2023) Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Lefeng Bo. 2023. Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Charactersamination. _arXiv preprint arXiv:2311.17117_ (2023).\n' +
      '* Huang and Belongie (2017) Xun Huang and Serge Belongie. 2017. Arbitrary style transfer in real-time with adaptive instance normalization. In _Proceedings of the IEEE international conference on computer vision_. 1501-1510.\n' +
      '* Huberman-Spiegelglas et al. (2023) Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. 2023. An Edit Friendly DDPM Noise: Bervervational and Manipulations. _arXiv preprint arXiv:2304.0140_ (2023).\n' +
      '* Igogogo et al. (2023) Jiro Igogo, Gilvun Kwon, and Jong Chul Ye. 2023. Zero-shot generation of coherent storybook from plain text story using diffusion models. _arXiv preprint arXiv:2302.03900_ (2023).\n' +
      '* Jia et al. (2023) Xuhui Jia, Fang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. 2023. Taming encoder for zero fine-tuning image augmentation with text-to-image diffusion models. _arXiv preprint arXiv:2304.02426_ (2023).\n' +
      '* Khachatryan et al. (2023) Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhungyang Wang, Shant Navasaryandyn, and Humphrey Shi. 2023. Text2Video-Zero: Text-to-Image Diffusion Models and Zero-Shot Video Generators. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_. 15954-15964.\n' +
      '* Khani et al. (2023) Ainsghar Khani, Saeid Agosar Taghanski, Aditya Sanghi, Ali Mahdavi Amiri, and Ghassan Hamarneh. 2023. Slime: Segment like me. _arXiv preprint arXiv:2309.03179_ (2023).\n' +
      '* Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion. _arXiv_ (2022).\n' +
      '* Li et al. (2023) Dongruq Li, Junnan Li, and Steven CH Hoi. 2023. flip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_ (2023).\n' +
      '* Liao et al. (2017) Jing Liao, Yuan Yao, Luan, Gang Hua, and Bing Bing Kang. 2017. Visual Attribute Transfer Through Deep Image Analogy. _ACM Trans. Graph._ 36, 4, Article 120 (July 2017), 15 pages. [https://doi.org/10.1145/3702959.3706383](https://doi.org/10.1145/3702959.3706383)\n' +
      '* Liu et al. (2023) Chang Liu, Haoming Wu, Yujie Zhong, Xiaoyuan Zhang, and Weizl Xie. 2023. Intelligent Crimir* [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation. (2022).\n' +
      '* [46] Simo Ryu. 2023. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning. [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora).\n' +
      '* [47] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. 2023. Instantbooth: Personalized text-to-image generation without test-time finetuning. _arXiv preprint arXiv:2304.03411_ (2023).\n' +
      '* [48] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. 2023. FreeU: Free Lunch in Diffusion U-Net. _arXiv preprint arXiv:2309.11497_ (2023).\n' +
      '* [49] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. 2023. Emergent Correspondence from Image Diffusion. In _Thirty-seventh Conference on Neural Information Processing Systems_. [https://openreview.net/forum?id=ypOiXjdfnU](https://openreview.net/forum?id=ypOiXjdfnU)\n' +
      '* [50] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. 2023. Key-locked rank one editing for text-to-image personalization. In _ACM SIGGRAPH 2023 Conference Proceedings_. 1-11.\n' +
      '* [51] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. 2023. MotionEditor: Editing Video Motion via Content-Aware Diffusion. _arXiv preprint arXiv:2311.18830_ (2023).\n' +
      '* [52] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2022. Splicing ViT Features for Semantic Appearance Transfer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 10748-10757.\n' +
      '* [53] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 1921-1930.\n' +
      '* [54] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. 2023. ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_. 15943-15953.\n' +
      '* [55] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 7623-7633.\n' +
      '* [56] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. 2023. MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model.\n' +
      '* [57] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. lp-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_ (2023).\n' +
      '* [58] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. 2023. A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence. (2023).\n' +
      '* [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 3836-3847.\n' +
      '\n' +
      '## Appendix: Training-Free Consistent Text-to-Image Generation\n' +
      '\n' +
      '### Additional Results\n' +
      '\n' +
      'We provide additional Qualitative and Quantitative results of our method. In Fig. A.1, Fig. A.2 and Fig. A.3 we show additional single and multi subject qualitative comparisons to existing baselines. In Fig. A.4 and Fig. A.5 we provide quantitative ablation results with respect to Textual Similarity, Subject Consistency, and Layout Diversity.\n' +
      '\n' +
      '### Additional Implementation Details\n' +
      '\n' +
      '**Subject-Driven Self-Attention** is applied at all timesteps, in the decoder layers of the U-net.\n' +
      '\n' +
      '**Feature injection and DIFT features:** Feature injection is applied at timesteps \\(t\\in\\left[680,900\\right]\\), with \\(\\alpha=0.8\\). We only inject patches with similarity scores above a threshold that is set automatically by the Otsu method.\n' +
      '\n' +
      'For feature injection, we denoise the image from \\(t=1000\\) to \\(t=261\\), for computing the DIFT features at \\(t=261\\) as in (Zhou et al., 2017). We then denoise again from \\(t=1000\\) to \\(t=0\\), guiding the feature injection with the precomputed DIFT features.\n' +
      '\n' +
      '**Pose Variation** We apply the injection of Vanilla Query Features over the first 5 denoising steps, with \\(v_{t}\\) values that linearly decay with \\(t\\) from 0.9 to 0.8. Self-Attention Dropout is applied with \\(p=0.5\\).\n' +
      '\n' +
      '**Diffusion Process** Images were sampled with 50 DDIM steps and a guidance scale of 5. Similarly to (Beng et al., 2017; Chen et al., 2018), we used Free-U (Zhou et al., 2017) to enhance the generation quality.\n' +
      '\n' +
      '**Extracting per subject mask** To extract subject masks from noisy latent patches, we collect all cross-attention maps that relate to each subject\'s token, across all previous diffusion steps, and all cross-attention layers of resolution \\(32\\times 32\\). We then average them, and threshold them using the "Otsu\'s method" (Zhou et al., 2017). The subject mask at the generation step \\(r\\) is, given by:\n' +
      '\n' +
      '\\[m_{i}=\\mathbb{E}_{t=0}^{L}\\mathbb{B}_{t=T}^{r}\\left[A_{i}\\right]\\] \\[M_{i}=otsu\\left(m_{i}\\right)\\in\\left\\{0,1\\right\\}^{P}, \\tag{7}\\]\n' +
      '\n' +
      'where \\(L\\) is the number of network layers, \\(P\\) is the number of patches and \\(\\mathbb{E}\\) denotes averaging.\n' +
      '\n' +
      '**Constructing DIFT pairwise correspondence maps** For a source image \\(I_{s}\\) and a target image \\(I_{t}\\), we denote their DIFT features by \\(D_{s}\\), \\(D_{t}\\in\\mathbb{R}^{P\\times D_{DIFT}}\\) respectively, where \\(d_{DIFT}\\) is the feature dimension. The cross-image patch similarity scores are given by the cosine similarity between these features:\n' +
      '\n' +
      '\\[\\text{Sim}(I_{s},I_{t})_{p_{s}p_{t}}=\\frac{D_{s}[p_{s}]\\cdot D_{t}[p_{t}]}{ \\left\\lVert D_{s}[p_{s}]\\right\\rVert\\left\\lVert D_{t}[p_{t}]\\right\\rVert}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(p_{s}\\), \\(p_{t}\\) are the indices of specific patches in the source and target image respectively and \\(D_{s}[p_{s}]\\), \\(D_{t}[p_{t}]\\) are the DIFT matrix rows (_i.e._ feature-vectors) matching these patches. Given these patch-similarity scores, we can calculate a patch-wise dense-correspondence map \\(C_{t\\to s}\\):\n' +
      '\n' +
      '\\[\\forall p_{t}\\in I_{t}:\\;\\;C_{t\\to s}[p_{t}]=\\operatorname*{arg\\,max}_{p_{s} \\in I_{s}}\\text{Sim}(I_{s},I_{t})_{p_{s},p_{t}}. \\tag{9}\\]\n' +
      '\n' +
      'Here, for each patch \\(p_{t}\\) in the target image \\(I_{t}\\), \\(C_{t\\to s}[p_{t}]\\) gives the index of the most similar patch in the source image, \\(I_{s}\\).\n' +
      '\n' +
      '**Control Net** To enhance the impact of the pose from ControlNet, we raise the Self-Attention Dropout value to 0.7.\n' +
      '\n' +
      '## Appendix C Diversity Evaluations\n' +
      '\n' +
      'In our quest to enhance layout diversity, we have proposed two strategies: Vanilla Query Injection and Self-Attention Dropout. To quantitatively assess the contributions of these strategies, we construct an automatic evaluation metric for layout diversity.\n' +
      '\n' +
      'Specifically, we utilize the DIFT features and dense correspondence maps between image pairs. We measure layout diversity by calculating the average displacement between corresponding points across each image pair. A low displacement score indicates that the subject\'s layout is largely aligned in both images. If this persists across the entire image set, we can conclude that the images have limited diversity. Notably, we opt for a geometric-diversity metric rather than an image-based one since the latter may conflate layout diversity with undesired changes in the subject\'s appearance. In Fig. A.5, we show the results of our diversity metric when ablating components from our method. Scores are normalized to the displacement value of images sampled from a vanilla non-consistent SDXL model. Our full method attains high diversity scores, surpassing solutions that omit one or more of the layout enrichment components.\n' +
      '\n' +
      '### Prompts Dataset Details\n' +
      '\n' +
      'To evaluate our method on a large scale, we constructed a prompts dataset. We asked ChatGPT to construct sets of \\(5\\) prompts each, where every prompt in a set contains the same recurring subject. In addition, we instructed it to generate the following metadata for each prompt set:\n' +
      '\n' +
      '* **Superclass**: We asked the model to group each set into one of the following superclasses: humans, animals, fantasy, inanimate.\n' +
      '* **Subject Token**: A single token representing the subject (e.g., \'cat\', \'boy\').\n' +
      '* **Subject Description**: The description of the recurring subject (e.g., \'A 16-year-old girl with blonde hair").\n' +
      '* **Description Level**: This refers to whether the subject description is generic or detailed. For example, a generic description might simply be "A dog", while a detailed description could elaborate as "A dog with brown and white fur, fluffy hair, and pointy ears".\n' +
      '* **Style**: The style of the requested image; for example, "A 3D animation of."\n' +
      '\n' +
      'We generated 100 prompt sets, comprising a total of 500 images, spanning various superclasses, description levels, and styles. We aimed to cover a wide spectrum of visual themes and subjects, ensuring a broad and inclusive representation across the different categories.\n' +
      '\n' +
      'Our prompt dataset is provided as a YAML file in the supplemental materials.\n' +
      '\n' +
      'Figure A.1: **Additional Qualitative Comparisons** We evaluated our method against IP-Adapter, TI, ELITE, E4T and DB-LORA. Some methods failed to maintain consistency (TI) or follow the prompt (IP-Adapter). Other methods alternated between keeping consistency or following text, but not both (DB-LoRA). Our method successfully followed the prompt while maintaining consistency.\n' +
      '\n' +
      '## Appendix E Training-Free Personalization Details\n' +
      '\n' +
      'We implemented edit-friendly inversion (Zhu et al., 2017) for SDXL and set the guidance scale to 2.0. We inverted two real-images, as shown at Fig. 12, and used them as anchors in our method. However, in our method, the anchors are generated with attention sharing between the two images.\n' +
      '\n' +
      'Figure A.3. **Comparison to The Chosen One, Multi-Subject** We evaluated our method against a concurrent work by (Beng et al., 2017). Notably, while _ConsiStory_ is training-free, in contrast to the concurrent work that requires an iterative optimization process, we demonstrate that our method outperforms it in preserving consistency for multiple subjects.\n' +
      '\n' +
      'Figure A.2. **Additional Qualitative Multi-Subject Comparisons** We evaluated our method against DB-LORA for generation of multiple consistent subjects. Lora DB tends to neglect the consistency of at least one of the subjects, while our method succeeds in both.\n' +
      '\n' +
      'Figure A.4. **Quantitative Component Ablation** We conducted a quantitative evaluation of _ConsiStory_ by ablating various components: Self-Attention Dropout (Dropout), Query Injection, Feature Injection (FI), and Subject-Driven Self-Attention (SDSA). Notably, omitting either SDSA or FI resulted in reduced subject consistency. Eliminating the variation enhancement mechanisms (Dropout and Query Injection) decreased textual similarity.\n' +
      '\n' +
      'them, which is not suitable for the case of inverted anchors. Therefore, we modify our anchoring process such that anchors will only share attention maps with generated images, rather than with other anchors. We observed that naively using the inverted image attention features with our method gave poor personalization results, and we hypothesize it is related to a distribution shift between the features of the inverted and generated images. Hence, we added Adain [24] to align the self-attention keys of inverted features with the self-attention keys of the generated images.\n' +
      '\n' +
      'Since edit-friendly inversion is based on DDPM, we modified our generation process to also use DDPM scheduling with 100 generation steps. We used the default guidance scale of 5.0 for the generated images. We note that this training-free personalization only works for simple objects, and provide failure cases at Fig. A.6.\n' +
      '\n' +
      '## Appendix F User Study Details\n' +
      '\n' +
      'We evaluate the models through two Amazon Mechanical Turk user studies using a two-alternative forced choice protocol. In the first study, named **"Visual consistency"**, raters saw two image sets, each produced by a different approach. They chose the set where the subject\'s identity remained most consistent. In the second study named **"Textual alignment"**, users received a text description and two images from different approaches. They selected the image that better matched the description.\n' +
      '\n' +
      '### Visual Consistency\n' +
      '\n' +
      'For the first study, in each trial, raters were presented with two sets of images, each set depicting a subject in various situations. They were instructed to select the set where the subject\'s identity remained consistent across all images. This decision was to be based solely on the subject\'s features and identity, disregarding elements such as background, clothing, or pose. The focus was on the consistency of the subject\'s identity, including aspects like eye color, texture, facial features, and other subtle details. Example images and solutions were provided for guidance. Figure A.7 illustrates the experimental framework used in the trials. Figure A.8 displays the example images provided to help guide raters through the instructions.\n' +
      '\n' +
      'The study included 100 trials of images from 100 unique subjects. Each trial was repeated 5 times with 5 different raters. Each image set consisted of 5 images generated from 5 distinct prompt settings. For the _Consitory_ images, we selected the variant with less stringent identity preservation parameters. Raters were paid 0.15 per trial. To maintain the quality of the study, we only selected raters with an Amazon Mechanical Turk "Masters" qualification, demonstrating a high degree of approval rate over a wide range of tasks. Furthermore, we also conducted a qualification test on the prescreened pool of raters, consisting of 5 curated trials that were simple.\n' +
      '\n' +
      '### Textual Alignment\n' +
      '\n' +
      'For the second study, in each trial, raters were presented with a textual description and two images. They were instructed to determine which image better matched the textual description. They were advised to focus on the details of the description. For example, if the text states "A bear is drinking from a cup", raters should choose the image where the bear is actually drinking from the cup. For guidance, example images with solutions were provided. Figure A.9 illustrates the experimental framework that was used in trials. The text descriptions only included the general subject class (_e.g._ dog") rather than fully detailed descriptions (_e.g._ A cute brown and white fluffy puppy dog with blue eyes"). This helped raters focus on the subject\'s setting. Figure A.10 displays the examples provided to help guide raters through the instructions.\n' +
      '\n' +
      'The study included 500 trials of images from 100 unique subjects, each subject had 5 distinct prompt settings. For the _ConsitSory_ images, we selected the variant with less stringent identity preservation parameters. We paid $0.05 per trial. To maintain the quality of the study, we only selected raters with an Amazon Mechanical Turk "Masters" qualification. Furthermore, we also conducted a qualification test on the prescreened pool of raters, consisting of 5 curated trials that were simple.\n' +
      '\n' +
      'Figure A.6. **Training-Free Personalization Failure** Our training-free personalization method may fail on non common subjects.\n' +
      '\n' +
      'Figure A.5. **Layout Diversity Ablation** Our method attains the highest Diversity Score, when compared to variations omitting the Dropout and/or the QQuery Injection components.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>