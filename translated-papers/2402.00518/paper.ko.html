<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# EE-Tuning: 경제적이면서도 확장 가능한 조기종료형 대용량 언어모델 튜닝 솔루션\n' +
      '\n' +
      '서천판1, 옌시천1, 얄양리, 볼린딩, 징렌저우\n' +
      '\n' +
      '{panxuchen.pxc, chenyanxi.cyx, yaliang.li, bolin.ding, jingren.zhou}@alibaba-inc.com\n' +
      '\n' +
      'Alibaba Group\n' +
      '\n' +
      '각주 1: 공동 최초 작가들.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 학습/동조_early-exit_큰 언어 모델(LLM)에 대한 경량적이고 경제적인 해결책인 EE-Tuning을 소개한다. 전체-파라미터 사전-트레이닝의 일반적인 접근법과 대조적으로, EE-튜닝은 파라미터-효율적인 방식으로 튜닝되는 추가적인 초기-출구 층들을 갖는 임의의 사전-트레이닝된(및 아마도 미세-튜닝된) 표준 LLM을 증대시키며, 이는 상당히 적은 계산 자원들 및 트레이닝 데이터를 요구한다. EE-Tuning의 구현은 3D 병렬성과의 완전한 호환성으로 인해 확장성뿐만 아니라 광범위한 성능 최적화를 통해 뛰어난 훈련 효율성을 달성한다. 체계적인 실험 결과는 EE-Tuning의 유효성을 검증하여 제한된 훈련 예산으로 효과적인 조기 출구 LLM 추론이 달성될 수 있음을 확인한다. 초기 출구 LLM을 커뮤니티에 접근할 수 있도록 하기 위해 [https://github.com/pan-x-c/EE-LLM](https://github.com/pan-x-c/EE-LLM)에서 EE-Tuning 구현의 소스 코드를 공개한다.\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '*1 소개\n' +
      '*2 방법론\n' +
      '	* 2.1 Stage 1 : 초기 출사층 초기화\n' +
      '	* 2.2 Stage 2 : 조기 출사층 튜닝\n' +
      '	* 2.3 부가 기능\n' +
      '* 3 실험\n' +
      '	* 3.1 EE-Tuning의 효율\n' +
      '	* 3.2 early-exit layer의 구조\n' +
      '	* 3.3 초기 출사층의 초기화\n' +
      '	* 3.4 다양한 크기의 모델에 대한 EE-Tuning\n' +
      '	* 3.5 추가 실험\n' +
      '*4 한계 및 향후 작업\n' +
      '* 5 결론\n' +
      '* 추가 실험\n' +
      '* A.1 EE-Tuning을 위한 훈련 데이터\n' +
      '* A.2 섹션 3.4에 대한 보충 결과\n' +
      '* A.3 Dynamic token-wise loss weighting\n' +
      '* EE-Tuning 후 A.4 Continued Pre-training (CPT)\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '트랜스포머 기반 대형 언어 모델(LLM)은 다양한 언어 작업[51, 4, 32, 48, 49, 7]에서 놀라운 성능을 달성했다. 한편, 이러한 모델은 점점 더 큰 크기로 인해 추론 단계에서 높은 비용과 대기 시간을 발생시킨다. _ 조기 종료_는 LLM 및 기타 심층 신경망의 추론을 가속화하기 위한 간단하면서도 효과적인 기술임이 입증되었다. 이 접근법에서, 초기-출구 층들은 원래의 심층 신경망에 부착되며, 이는 중간 은닉 상태들을 초기-출구 출력으로 변환할 수 있다. 추론 동안, 모델은 각각의 입력 샘플에 대한 출력을 생성하기 위해 하나의 조기 출구를 적응적으로 선택할 수 있고, 네트워크의 나머지 층들의 순방향 계산을 건너뛸 수 있다. 조기 퇴장은 자연 언어 처리[13, 18, 57, 41, 29, 11, 52, 27, 40, 53, 54, 19], 컴퓨터 비전[33, 47, 22, 21] 및 많은 다른 영역[38, 26, 14, 9]에서 성공을 발견했다.\n' +
      '\n' +
      '이 작업은 생성 LLM 및 자기회귀 자연 언어 생성을 위해 토큰별 조기 퇴장을 고려한다[40, 8, 2, 50, 12, 6]. 이 분야의 대부분의 이전 작업은 초기 출구 _inference_ 메커니즘을 설계하는 데 중점을 두었지만, 대신 먼저 초기 출구 LLM을 _train_하는 방법에 중점을 둔다. 조기 퇴장에 대한 대부분의 선행 연구에서 채택된 표준적이고 간단한 방법은 조기 및 최종 퇴장으로부터의 훈련 손실의 가중 합을 최소화함으로써 모든 모델 매개변수(네트워크 백본 및 조기 퇴장 레이어를 포함)를 처음부터 공동으로 훈련시키는 것이다. 최근 연구에서는 이러한 접근 방식을 대규모 3D 병렬과 호환하여 초기 출구 LLM을 최신 LLM 프레임워크[43, 31, 6]로 훈련할 수 있는 모든 표준 LLM만큼 큰 크기로 확장했다. 이 접근법의 명백한 문제는 지나치게 높은 비용과 복잡성이다. 실제로 수십억 개의 매개변수를 가진 LLM을 훈련하는 데 필요한 방대한 양의 계산 리소스는 단순히 커뮤니티의 대부분의 구성체에 접근할 수 없다. 종종 실제로 개인 또는 오픈 소스일 수 있는 기존 사전 훈련(및 아마도 미세 조정) 표준 LLM의 가중치에 액세스할 수 있으며 처음부터가 아니라 이러한 정보를 활용하여 조기 퇴장 LLM을 훈련하는 것이 가능한지 궁금하다.\n' +
      '\n' +
      '이 모든 동기들은 우리가 기존의 생성 LLM을 초기-출구 one_로 변환하도록 동기를 부여한다.\n' +
      '\n' +
      '*는 최소의 연산 자원을 필요로 하고;\n' +
      '*는 만족스러운 추론 가속도로 이어지고; 그리고\n' +
      '*는 원래의 LLM의 전체 능력을 보존한다.\n' +
      '\n' +
      '주요 공헌점.이 연구는 사전 훈련된(및 아마도 미세 조정된) LLM을 조기 출구로 변환하는 원칙적이고 가벼운 접근법인 EE-Tuning을 도입하며, 이는 위의 모든 요구 사항을 충족한다. EE-Tuning의 핵심은 직관적이고 실용적인 2단계 절차이다:\n' +
      '\n' +
      '1. 미리 훈련된 표준 LLM을 입력으로서 취하고, 파라미터가 적절하게 초기화되는 초기-출구 계층으로 그 아키텍처를 증강시키고;\n' +
      '2. 원래 표준 LLM의 모듈들이 동결된 상태에서, 파라미터-효율적인 방식으로 특정 트레이닝 손실들의 역전파를 통해 초기-출구 층들을 조정한다.\n' +
      '\n' +
      '본 방법의 시각화는 그림 1을 참조하며, 섹션 2에서 자세한 내용을 설명한다. 본 구현은 최근 제안된 EE-LLM 프레임워크 [6]을 기반으로 하며, 낮은 계산 복잡도와 3D 병렬성과의 완전한 호환성 덕분에 _accessible_ 및 _scalable_인 조기 출구 LLM을 훈련하는 대체 솔루션으로 후자를 보완한다. 즉, 하나의 GPU 또는 수천 개의 GPU를 가진 클러스터에 액세스할 수 있는 모든 LLM 개발자는 EE-Tuning이 조기 출구를 연구하고 적용하는 데 유용하고 실용적인 도구를 찾을 것이다. 우리의 구현은 또한 다양한 구성 및 기타 유리한 기능에 대한 지원을 포함하며, 이는 더더욱 사용을 편리하게 한다.1\n' +
      '\n' +
      '각주 1: 앞으로 EE-Tuning은 문맥에 따라 제안된 2단계 방법 또는 구현 방법을 참조하도록 한다.\n' +
      '\n' +
      'EE-튜닝의 효능은 초기 출구 LLM에 대한 전례 없는 규모인 최대 700억(70B) 매개변수가 있는 모델에 대해 광범위하고 체계적인 실험을 통해 검증된다. 보다 구체적으로, 사전 훈련된 LLM은 빠르고 안정적인 수렴으로 튜닝 프로세스를 통해 조기 퇴장 능력을 빠르게 획득할 수 있으며, 이는 사전 훈련 단계에서 사용되는 GPU 시간 및 훈련 데이터의 1/1000 미만이 소요되고 하나 또는 몇 개의 GPU만 필요로 한다. 한편, 변환된 모델은 유사한 또는 심지어 더 나은 벤치마크 점수를 유지하면서 조기 퇴장을 통해 다양한 다운스트림 작업에서 1.2\\(\\times\\)에서 1.6\\(\\times\\)의 속도 향상을 달성하거나 약간의 출력 품질 저하가 허용될 경우 더 높은 속도 향상을 달성할 수 있다. 다양한 디자인 선택의 효과를 철저히 조사하고 EE-Tuning의 성능을 극대화하기 위한 실용적인 지침을 제공한다. EE-Tuning의 소스 코드는 [https://github.com/pan-x-c/EE-LLM](https://github.com/pan-x-c/EE-LLM)에서 사용할 수 있다.\n' +
      '\n' +
      '관련된 연구들. EE-Tuning의 아이디어, 즉 파라미터-효율적인 방식으로 튜닝되는 초기-출구 층들을 갖는 미리 트레이닝된 신경망을 증강시키는 것은 완전히 새로운 것은 아니다. 이 전략은 분류 작업에 맞춘 모델 아키텍처, 예를 들어 인코더 전용 BERT 모델[52, 29, 19] 또는 다른 모델[33, 22, 3, 9]에 대한 일부 이전 작업에서 채택되었다. 그러나 이러한 작업의 결과와 결론이 우리 작업의 초점인 _autoregressive sequence generation_에 맞춘 _decoder-only_ Transformers의 경우로 안전하게 이전될 수 있다는 보장은 없다. 또 다른 최근 작업 [50]은 사전 훈련된 표준 LLM으로 조기 퇴장 LLM의 모델 매개변수를 초기화하고 전체 매개변수 훈련을 수행하도록 제안했다. 설정 및 훈련 방법론에 가장 가까운 것은 최대 355M 크기의 생성 LLM을 조사하고 무작위로 초기화된 선형 출구 헤드만 고려한 최근 작업[12]이다. 또한, 조기 출구를 통한 추론을 가속화하기보다는 전체 모델 추론의 최종 출력을 향상시키기 위해 다중 출구를 사용하도록 제안하였다. 이와는 대조적으로, 제안된 EE-Tuning 방법의 구현은 광범위한 구성에 대한 지원으로 통일되고 체계적이며, (2) 3D 병렬성과의 완전한 호환성 덕분에 확장 가능하며, (3) 자동 회귀 추론 동안 뛰어난 가속도를 달성하는 초기 출구 LLM을 반환하기 위한 광범위한 실험을 통해 입증되었다.\n' +
      '\n' +
      '## 2 Methodology\n' +
      '\n' +
      '이 섹션에서는 그림 1에서 시각화된 2단계 절차인 EE-Tuning을 통해 잘 훈련된 조기 출구 LLM을 얻는 방법론과 구현에 대해 자세히 설명한다.\n' +
      '\n' +
      '예비.현대의 LLM은 대부분 트랜스포머 아키텍처를 기반으로 한다[51]. 우리는 디코더 전용 생성 사전 훈련(GPT) 트랜스포머 아키텍처[35, 36]에 초점을 맞추지만, 이 작업에서 제시된 많은 기술은 더 넓은 설정으로 일반화될 수 있다. 그림 1에서 시각화된 바와 같이 GPT Transformer는 입력 처리를 위한 초기 계층, 백본으로 Transformer 계층의 스택, 최종 은닉 상태를 어휘에 로짓으로 변환하는 출력 계층으로 구성되어 새로운 토큰 생성에 사용될 수 있다. 각 트랜스포머 레이어는 어텐션 모듈과 다중 레이어 퍼셉트론(MLP)으로 구성되며, 레이어 정규화[1]와 잔류 연결[15]이 여러 곳에서 적용된다. 최종 출력 계층은 선택적 계층 정규화 모듈을 포함하고, 이어서 큰 출력 임베딩 행렬을 포함한다.\n' +
      '\n' +
      'GPT 트랜스포머는 레이블이 지정되지 않은 말뭉치에서 언어 모델링 손실을 최적화함으로써 감독되지 않은 방식으로 훈련될 수 있다. 보다 구체적으로, 주어진 모델 파라미터 \\(\\mathbf{\\theta})와 토큰들의 시퀀스 \\(\\mathbf{x}=(x_{1},x_{2},\\dots,x_{T})\\), 자기회귀적 언어 모델링 손실, 즉 다음-토큰 예측의 음의 로그-우도는 \\(\\mathcal{L}(\\mathbf{x};\\mathbf{\\theta})\\coloneqq-\\log\\mathbb{P}(\\mathbf{x};\\mathbf{\\theta})=- \\sum_{t\\in[T]}\\log\\mathbbb{P}(x_{t}|x_{1},\\dots,x_{t-1};\\mathbf{\\theta})로 정의된다.\n' +
      '\n' +
      '그림 1: 사전 훈련된 표준 LLM을 잘 훈련된 조기 출구 LLM으로 변환하는 제안된 2단계 절차인 EE-Tuning의 개요.\n' +
      '\n' +
      '###단계 1 : 초기출사층 초기화\n' +
      '\n' +
      'EE-Tuning의 첫 번째 단계에서는 미리 훈련된 표준 LLM 체크포인트를 로드하고 미리 지정된 위치에 초기 출구 레이어를 추가하여 모델 아키텍처를 확장한다. 보다 구체적으로, 우리는 먼저 초기 출구 레이어의 아키텍처를 지정하고 다음에서 설명하는 바와 같이 매개변수를 초기화한다.\n' +
      '\n' +
      '초기출구의 구조.이론에서 초기출구 레이어는 \\(\\mathbb{R}^{h}\\)에서 \\(\\mathbb{R}^{V}\\)까지의 일반적인 매핑일 수 있으며, 여기서 \\(h\\)은 숨겨진 크기이고 \\(V\\)은 어휘 크기이다. 우리의 구현은 이전 작업 [6]에서 제안된 조기 출구 아키텍처를 지원하며, 이는 아래에서 요약한다.\n' +
      '\n' +
      '* 임베딩: 이것은 단일 선형 계층, 즉 숨겨진 상태를 어휘 상의 로짓으로 변환하는 크기 \\(h\\times V\\)의 출력 임베딩 매트릭스를 갖는 미니멀리즘 초기-출구 아키텍처이다.\n' +
      '*Norm: Embedding 아키텍처 앞에 훈련의 안정화 및 모델의 출력 품질을 향상시킬 수 있는 LayerNorm[1] 또는 RMSNorm[55] 모듈이 추가된다. 최종 출구 레이어에 하나 있는 경우 각 초기 출구에 정규화 모듈을 포함하는 초기 출구 아키텍처를 원래 LLM의 최종 출구 레이어와 일치하도록 하는 것이 좋다.\n' +
      '* MLP: 트랜스포머 백본의 MLP와 동일한 구조를 갖는 MLP가 노름 아키텍처 앞에 추가된다.\n' +
      '* 레이어: 백본에 있는 것과 동일한 구조의 완전한 트랜스포머 레이어가 노름 아키텍처 앞에 추가됩니다.\n' +
      '\n' +
      '이러한 아키텍처의 시각화는 그림 2를 참조하십시오. 일반적으로, 더 많은 수의 훈련 가능한 파라미터들은 더 높은 표현성 및 적응성을 가져오지만, 또한 조기-출구 층들의 튜닝 프로세스 동안 더 큰 추론 지연 및 잠재적으로 더 높은 과적합 위험을 초래한다. 또한 부산물로서 네트워크의 입력과 특정 조기 출구의 출력 사이의 각각의 서브모델은 표준 GPT 트랜스포머로 간주될 수 있다는 점에 유의한다.\n' +
      '\n' +
      '_Remark 1_.: 이 작업을 통해, 우리는 달리 명시되지 않는 한 모든 출구의 출력 임베딩 행렬이 풀렸다고 가정한다. 초기 출구 LLM에 대한 일부 최근 작업은 출력 임베딩 행렬[39, 50]을 묶기로 선택했으며 EE-튜닝은 일부 수정으로 이 경우에 적용할 수 있다.\n' +
      '\n' +
      '초기 출구의 초기화.모델 파라미터를 초기화하기 위한 하나의 자연스러운 옵션은 표준 LLM을 처음부터 사전 트레이닝하기 전에 수행되는 것과 동일한 방식으로 _random_ 초기화이다. EE-Tuning의 수렴을 가속화하기 위해, 우리는 원래 미리 훈련된 LLM의 특정 모듈로부터 _copying_ 모델 파라미터를 사용하여 초기 출구 레이어를 초기화하는 새로운 접근법을 제안한다. 보다 구체적으로,\n' +
      '\n' +
      '*Embedding and Norm에 대해, 출력 임베딩 행렬 및 정규화 모듈의 파라미터들은 원래의 LLM의 최종-출구 계층 내의 대응하는 모듈들로부터 복사될 수 있고;\n' +
      '\n' +
      '그림 2: 다양한 초기 출구 아키텍처의 시각화입니다. 각 주의 또는 MLP 모듈은 사전 정규화와 함께 잔차 구조를 따른다.\n' +
      '\n' +
      '* MLP에 대해, 일부 트랜스포머 층에 연결된 조기 출구를 위한 MLP 모듈은 동일한 트랜스포머 층 내의 원래의 MLP의 복제로서 초기화될 수 있고;\n' +
      '*For Layer, exit 내의 Transformer layer는 원래의 LLM의 마지막 Transformer layer의 중복으로 초기화될 수 있다.\n' +
      '\n' +
      '제안된 방법은 기존의 LLM에서 널리 채택된 잔차 구조[15]에서 주로 영감을 얻는데, 이는 각 트랜스포머 레이어의 출력이 입력에 작은 잔차 성분을 추가하여 생성됨을 의미한다.2 복사에 의한 초기화 후, 입력에서 초기 출구 출력으로의 순방향 통과는 특정 트랜스포머 레이어를 건너뛰는 것을 제외하고는 원래의 LLM과 동일하다. 스킵된 계층들의 잔여 구조를 고려할 때, 초기-출구 출력은 여전히 다소 의미가 있다고 가정하는 것이 합리적이며, 이는 제안된 초기화 방법을 정당화한다. 수렴을 가속화하는 것 외에도 복사하여 초기화하는 또 다른 잠재적인 이점은 조기 출구 계층이 미리 훈련된 표준 LLM의 모듈에 학습되고 내장된 지식과 능력을 상속할 수 있다는 것이다.\n' +
      '\n' +
      '각주 2: 더 엄격하게는, 이것은 정상화 이전의 트랜스포머(우리가 이 작업에서 고려하는 경우)에 대한 것이지, 정상화 이후의 것이 아니다. 그럼에도 불구하고 복사에 의한 초기화 이면의 근거는 후자의 경우 대부분 정확하다.\n' +
      '\n' +
      '### Stage 2 : 조기출구 레이어 튜닝\n' +
      '\n' +
      'EE-Tuning의 두 번째 단계에서 우리는 초기 출구 레이어를 여러 출구로부터의 훈련 손실의 표준 역전파를 통해 튜닝하는 반면 원래 표준 LLM의 모듈은 동결된다. 기본적으로 다른 옵션이 가능하지만 레이블이 지정되지 않은 말뭉치의 오픈 소스 데이터 세트에서 자동 회귀 언어 모델링 손실을 사용한다. 도 1에서 시각화된 바와 같이, 이러한 튜닝 프로세스는 다수의 얕은 네트워크들을 독립적으로 그리고 병렬로 학습하는 것과 동등하며, 이들 각각은 백본 상의 특정 트랜스포머 층에서의 숨겨진 상태들을 대응하는 초기-출구 출력들에 매핑하는 초기-출구 층임에 유의한다. 훈련 가능한 매개변수의 수가 상대적으로 적기 때문에 사전 훈련된 심층 신경망에서 매개변수의 작은 비율을 미세 조정하는 표준 관행과 유사하게 빠른 수렴과 좋은 일반화를 추구하기 위해 작은 배치 크기를 선택한다.\n' +
      '\n' +
      '계산 효율.EE-Tuning의 구현은 최대 계산 효율을 위해 잘 최적화되었다. 보다 구체적으로, (1) 트랜스포머 백본의 부분 순방향 패스 _up to_ 마지막 얼리 출구에 연결된 히든 상태들, 및 (2) 얼리 출구들 사이의 임의의 종속성 없이, 각 얼리 출구 레이어에 대한 순방향 컴퓨테이션, 역방향 컴퓨테이션, 및 파라미터 업데이트를 포함하여, 필요한 최소한의 컴퓨테이션만이 실행된다. 메모리 사용량은 (1) 부분 체크포인트 로딩을 구현하여, 마지막 얼리 아웃에 연결된 히든 상태 앞에 있는 원래 LLM의 모듈만 GPU 메모리에 로딩할 필요가 있고, (2) 얼리 아웃 레이어에 대해서만 최적화 상태를 유지한다. 구현 결과, EE-Tuning은 기존 LLM의 부분 순방향 패스에 비해 적은 계산 오버헤드가 발생한다.\n' +
      '\n' +
      '_Remark 2_.: 입력 임베딩 행렬로부터 풀린 다수의 출구들로부터의 출력 임베딩 행렬들을 묶기로 선택하면, EE-Tuning은 이전과 같이 초기-출구 레이어들에 대해서만 역방향 계산을 요구한다. 그러나, 출력 임베딩 행렬들이 입력 임베딩[34]과 더 묶이면, 기울기 계산을 위해 전체 네트워크를 통한 완전한 후방 통과가 필요하며, 이는 훨씬 더 높은 계산 비용을 발생시킨다.\n' +
      '\n' +
      '그림 3: 4개의 파이프라인 단계와 블록의 숫자로 인덱싱된 8개의 마이크로배치가 있는 설정에서 EE-튜닝에 사용되는 맞춤형 파이프라인 스케줄의 한 번의 훈련 반복.\n' +
      '\n' +
      '3D 병렬에 대한 지원.이전 작업[43, 31, 6]에 구축된 EE-Tuning의 구현은 데이터, 텐서, 시퀀스 및 파이프라인 병렬3과 같은 대규모의 3D 병렬을 자연스럽게 지원한다. 간결성을 위해, 관심 있는 독자들은 여기에 생략된 3D 병렬에 대한 세부 사항은 메가트론-LM 시리즈[43, 31, 25, 45]와 같은 이전 작업들을 참조한다. EE-Tuning의 경우, 그림 3과 같이 순방향 통신만을 사용하여 맞춤형 파이프라인 스케줄을 설계하고 구현한다. 전체 파라미터 트레이닝의 경우, 각 파이프라인 스테이지는 순방향 계산의 모든 중간 활성화를 저장하고, 역방향 계산이 시작되기 전에 다음 스테이지까지 역방향 전송된 그래디언트를 기다려야 함을 상기한다. 이와는 대조적으로, EE-Tuning은 훨씬 쉽다: 특정 스테이지 내의 각각의 초기-출구 레이어는 그 스테이지 내의 트랜스포머 백본의 순방향 패스가 완료되는 즉시, 자신의 손실을 계산하고 독립적으로 역방향 계산을 실행할 수 있다. 결과적으로, 트랜스포머 백본 상의 중간 활성화, 또는 역전파를 위한 역방향 통신을 저장할 필요가 없다.\n' +
      '\n' +
      '각주 3: 파이프라인 병렬성을 갖는, 심층 신경망은 깊이 차원을 따라 다수의 파이프라인 스테이지들로 분할되며, 이들은 상이한 계산 디바이스들에 할당된다. 또한, 각각의 데이터 배치가 다수의 마이크로배치로 분할되어, 상이한 스테이지에 대한 이들의 순방향 및 역방향 계산이 일부 스케줄을 통해 파이프라인화 및 병렬화될 수 있다.\n' +
      '\n' +
      '### Additional features\n' +
      '\n' +
      'EE-Tuning의 구현에서 몇 가지 추가 기능이 다음에서 소개된다.\n' +
      '\n' +
      '플러그 앤 플레이 초기 출구.구현을 통해 먼저 여러 초기 출구를 튜닝한 다음 유연한 플러그 앤 플레이 방식으로 추론을 위해 실제로 활성화할 하나를 결정할 수 있다. 조정된 초기 출구들의 동일한 세트의 다양한 서브세트들이 상이한 사용 케이스들에 대해 선택될 수 있다. 초기 출구의 최상의 수와 위치는 종종 사전에 알려지지 않고 특정 사용 사례에 따라 달라질 수 있기 때문에 배치의 이러한 유연성은 실제로 유익할 수 있다.\n' +
      '\n' +
      '동적 토큰 손실 가중치.지금까지 우리는 각 조기 출구가 훈련 목표를 정의할 때 훈련 데이터의 모든 토큰에 동일한 가중치를 할당한다고 가정했다. 그러나 이는 각 출구가 모든 토큰이 아닌 자신감이 높은 "쉬운" 토큰에 대해서만 예측을 할 필요가 있는 조기 출구 추론의 실제 프로세스와 편차를 야기한다. 일부 최근 작업[46, 37, 10, 33, 3]은 동적 토큰별 손실 가중치에 의해 이러한 불일치를 줄이고 긍정적인 결과를 관찰하도록 제안했으며, 이는 우리가 이 접근법의 한 버전을 구현하도록 동기를 부여한다. 보다 구체적으로, 각각의 데이터 배치의 순방향 패스 동안, 각각의 출구는 배치 내의 토큰들에 대한 그의 신뢰도 값들을 기록하며, 이는 역방향 계산 전에 트레이닝 손실을 정의하기 위한 토큰들의 가중치들로서 사용될 것이다. 이렇게 각 초기 출구는 능력을 넘어서는 "하드" 토큰에 대한 예측을 강요받지 않고 자신감이 높은 토큰을 처리하는 방법을 배운다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '이 섹션에서는 광범위한 실험을 통해 EE-튜닝의 효능을 검증한다. 먼저 섹션 3.1에서 문헌에서 전례 없는 초기 출구 모델 규모인 최대 70B 크기의 모델에 대한 EE-튜닝의 효율성을 보여준다. 섹션 3.2는 조기 출구 아키텍처가 훈련 손실과 다운스트림 성능에 미치는 영향을 조사하고 섹션 3.3은 조기 출구의 매개변수를 초기화하는 두 방법을 비교한다. 우리는 섹션 3.4에서 7B에서 70B까지의 다양한 크기의 모델에 대한 EE-튜닝의 유효성을 추가로 확인한다. 추가 경험적 결과는 부록 A에 보류된다.\n' +
      '\n' +
      '모든 실험은 각 노드가 8개의 Nvidia A800-80G GPU를 호스트하는 다중 노드 GPU 클러스터로 수행되었다. 우리는 경험적 결과를 제시하기 전에 실험 전반에 걸쳐 채택되는 일반적인 설정 아래에 설명한다.\n' +
      '\n' +
      '모델.표준 LLM의 경우, 사전 정규화를 갖는 디코더 전용 GPT 아키텍처를 따르는 크기 7B, 13B 및 70B의 오픈 Llama 2-Chat 모델 [49]를 사용한다. 각 모델은 수조 개의 토큰의 학습 데이터로 사전 훈련을 거쳤고, 인간의 피드백으로부터 지도된 미세 조정 및 강화 학습을 통해 인간의 선호도와 정렬을 거쳤다. 우리의 초기 출구 LLM은 다양한 아키텍처의 초기 출구 레이어로 이러한 Llama 2-Chat 모델을 보완하여 구성된다. 편의를 위해 섹션 2.1에 도입된 모델 매개변수를 초기화하는 두 가지 방법을 각각 무작위 및 사본으로 표시한다.\n' +
      '\n' +
      '튜닝.2절에서 제안된 튜닝 프로세스를 위해 배치 크기를 16의 작은 값으로, 시퀀스 길이를 2048로, 트레이닝 반복 횟수를 \\(4\\times 10^{4}\\)으로 설정했다. 따라서 조기출구 LLM의 튜닝을 위한 학습 데이터의 총량은 \\(16\\times 2048\\times 4\\times 10^{4}\\approx 1.3\\)억 토큰으로, 표준 또는 조기출구 LLM의 전체 파라미터 학습에 필요한 토큰의 수에 비해 거의 무시할 수 있다. 우리의 훈련 데이터는 달리 명시되지 않는 한 Data-Juicer[5]에서 제공하는 정제된 사전 훈련 데이터에서 무작위로 샘플링된다. 섹션 2에서 설명한 바와 같이 표준 자기회귀 언어 모델링 손실은 실험에서 훈련 목표로 사용된다. 초매개변수 \\(\\beta_{1}=0.9,\\beta_{2}=0.95,\\epsilon=10^{-5}\\)을 갖는 Adam optimizer [23]을 사용하여 선형 감쇠, 워밍업 분율 0.01, 최대 학습률 \\(10^{-4}\\) 및 최소 학습률 \\(10^{-5}\\)을 갖는 학습률 스케줄과 함께 사용한다.\n' +
      '\n' +
      '추론.Early-Exit 추론을 위해 greedy decoding과 confidence-based exit condition을 사용한다. 즉, 모델은 어떤 early exit에서 next-token prediction의 최대 확률이 미리 지정된 임계치 이상이 되면 가장 가능성이 높은 토큰을 출력한다. 우리는 KV 캐슁과 호환되는 선행 작업 [6]의 파이프라인 기반 추론 메커니즘을 활용한다. 추론 효율은 벽-시계 레이턴시로 측정된다. 신뢰 임계값을 1로 설정하면 초기 출구가 비활성화되므로 초기 출구 모델은 원래 라마 2-챗 모델과 동일합니다. 이 경우의 전체 모델 추론 레이턴시는 더 작은 임계값을 갖는 추론에 의해 상대 속도 향상을 계산하기 위한 베이스라인으로서 사용된다. 우리는 CNN/DailyMail[42], XSUM[30], NarrativeQA[24] 및 MMLU[16]의 네 가지 작업에 대해 HELM[28]을 사용하여 다운스트림 평가를 수행한다. 각 태스크에 대해 2048의 최대 컨텍스트 길이를 사용하고 생성된 토큰의 수를 지정하고 5-샷 성능을 보고한다. 작업 요약은 표 1을 참조하십시오.\n' +
      '\n' +
      '### EE-Tuning의 효율\n' +
      '\n' +
      '**관찰**: Nvidia A800-80G GPU와 모델 분할 없이 EE-튜닝할 수 있음\n' +
      '\n' +
      '(1) 20G의 피크 GPU 메모리로, 20 GPU 시간 내에 Llama 2-Chat 13B 모델을 변환하는 단계;\n' +
      '\n' +
      '(2) 최대 GPU 메모리 78G로 120 GPU 시간 내에 Llama 2-Chat 70B 모델을 변환한다.\n' +
      '\n' +
      '우리의 첫 번째 실험은 EE-튜닝의 효율성을 보여준다. 이 설정에서 Llama 2-Chat 모델의 1/4 깊이에 추가된 하나의 MLP 조기 출구를 조정하는 것을 고려한다. 우리는 텐서나 파이프라인 병렬성 없이 4의 데이터 병렬도를 위해 4개의 GPU를 사용한다. 결과는 13B 모델을 튜닝하는 데 약 5시간이 걸리고 최대 메모리 사용량이 20G라는 것이다. 이러한 작은 메모리 풋프린트는 24G 메모리를 가진 Nvidia RTX 4090도 이 작업을 처리할 수 있음을 의미한다. 또한, 70B 모델을 튜닝하는 데 약 30시간이 걸리고 78G의 피크 메모리 사용이 소요되며, 이는 단일 A800-80G GPU에 적합할 수 있다. 구현을 통해 Llama 2-Chat 모델 체크포인트의 첫 번째 1/4 부분만 메모리에 로드할 필요가 있고, 초기-출구 계층에 대한 최적화 상태만 유지하면 되며, 필요한 최소 연산량만 수행되기 때문에 이러한 놀라운 효율성을 얻을 수 있다.\n' +
      '\n' +
      '반복되는 것을 피하기 위해 섹션 3.4로 보류된 다양한 출구에서의 훈련 손실 곡선은 우리의 조정 프로세스의 수렴을 확인한다. 부록의 표 2는 튜닝된 모델에 의해 생성된 몇 가지 예제 텍스트를 보여주며, EE-Tuning이 가속 추론을 통해 표준 LLM을 조기 출구 텍스트로 성공적으로 변환했음을 확인시켜준다. 나머지 실험에서는 보다 체계적인 방법으로 조기 퇴장의 성능을 살펴본다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Task & Type & Metric & Num. tokens \\\\ \\hline CNN/DailyMail & Summarization & ROUGE-L & 128 \\\\ XSUM & Summarization & ROUGE-L & 64 \\\\ NarrativeQA & Reading comprehension & F1 & 100 \\\\ MMLU & Language understanding & Exact Match & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다운스트림 평가를 위한 작업.\n' +
      '\n' +
      '### 초기-출구 층의 구조\n' +
      '\n' +
      '**관찰**: 조기출구 아키텍처에 관하여,\n' +
      '\n' +
      '(1) 더 훈련가능한 파라미터를 갖거나 더 깊은 위치에 위치된 조기-출구 계층이 더 낮은 훈련 손실을 달성하고;\n' +
      '\n' +
      '(2) 만족스러운 추론 속도 향상은 비교가능하거나 심지어 부스팅된 점수와 함께 달성될 수 있고, MLP는 속도와 품질 사이의 최상의 전체 균형을 타격한다.\n' +
      '\n' +
      '본 실험은 2.1절에 소개된 초기-출구 아키텍처를 비교한다. 그 내부의 레이어 정규화를 위해 Llama 2[49]를 따르고 RMSNorm[55]을 사용한다. 우리는 트랜스포머 백본의 1/8, 2/8,..., 8/8 깊이에서 균일하게 이격된 40층 13B Llama 2-Chat 모델에 8개의 초기 출구를 추가하고 복사 방법으로 초기화한다. 원래 최종 출구 층에 나란히 배치된 마지막 초기 출구 층은 실제 사용보다는 실험 목적으로만 추가된다.\n' +
      '\n' +
      '훈련 손실.그림 4는 EE-Tuning 종료 시 각 조기 퇴장의 훈련 손실을 보여준다. 정규화 모듈을 포함하지 않는 초기-출구 레이어 튜닝은 수렴할 때 발산되거나 높은 훈련 손실이 발생하는 반면, 노름 아키텍처의 튜닝은 문제를 효과적으로 피할 수 있음을 관찰했다. 따라서 우리는 더 이상 남은 실험에서 임베딩을 고려하지 않는다. 또 다른 관찰은 튜닝이 끝날 때 조기 퇴장 훈련 손실의 관점에서, 하나는 \\(\\texttt{Norm}>\\texttt{MLP}>\\texttt{Layer}\\texttt{Layer})이라는 것이다. 이것은 조기-출구 층들에서 더 훈련가능한 파라미터들을 갖는 것이 더 낮은 훈련 손실로 이어진다는 것을 확인시켜준다. 마지막으로, 각 아키텍처에 대해, 더 깊은 층에서의 초기 출구의 트레이닝 손실은 이전 층에서의 트레이닝 손실보다 일관되게 더 낮다. 신경망의 더 깊은 층에서의 숨겨진 상태는 평균적으로 입력에 대해 더 풍부하고 더 유용한 정보를 포함할 것으로 예상되기 때문에 이는 놀라운 일이 아니다.\n' +
      '\n' +
      '추론 품질과 속도 향상.그림 5는 앞서 언급한 구성들로 EE-Tuning에 의해 학습된 조기 출구 모델들의 다운스트림 성능을 나타낸다. 각 모델에 대해 1/4, 2/4 및 3/4 깊이에서 세 개의 초기 출구만 활성화하며, 이는 구현의 플러그 앤 플레이 기능 덕분에 쉽게 수행할 수 있다. 우리가 고려하는 작업에서 MLP 모델은 측면에서 최고의 전체 성능을 달성한다.\n' +
      '\n' +
      '그림 4: 다양한 조기 출구 아키텍처에 대한 EE-Tuning 종료 시 모든 조기 출구의 훈련 손실.\n' +
      '\n' +
      '그림 5: 다양한 초기 출구 아키텍처를 가진 13B 모델의 다운스트림 성능. 우상단 코너에 더 가까운 포인트는 더 나은 성능(즉, 더 높은 스피드업 및 점수)을 나타낸다. 각 곡선의 마커는 이 실험에 사용하는 신뢰 임계값의 이산 값에 해당한다. 임계값이 감소함에 따라 속도는 왼쪽에서 오른쪽으로 증가하며, \\{1.0,0.9,0.8,0.6,0.4,0.2\\}\\의 값을 취한다.\n' +
      '\n' +
      '점수 및 속도 향상, 노름 모델이 그 뒤를 잇습니다. 특히, Layer 모델은 일부 작업에서 가장 낮은 훈련 손실과 가장 높은 점수를 달성했음에도 불구하고 상대적으로 더 약한 추론 속도를 나타낸다. 이는 섹션 2.1에서 설명한 바와 같이 초기 출구 층 자체의 큰 추론 지연에 기인할 가능성이 크다.\n' +
      '\n' +
      '주의 깊은 독자들은 표준 풀 모델 추론에 비해 더 높은 평가 점수와 사소한 속도 향상이 일부 작업에서 _동시에 달성될 수 있다는 것을 알아차릴 수 있다. 이것은 놀라운 것처럼 보일 수 있지만 이전 작업에서도 유사한 결과가 관찰되었으며 한 가지 이유는 조기 퇴사가 _overthinking_[22]를 완화하는 데 도움이 될 수 있으며, 즉 조기 퇴사가 특정 입력에 대해 실수를 하는 동안 올바른 예측을 할 수 있기 때문이다. 우리의 경우 또 다른 가능한 이유는 원래 라마 2-챗 모델이 인간의 선호도와 정렬을 위해 광범위하게 미세 조정되었지만 추가 초기 출구 레이어는 사전 훈련 데이터에 대한 언어 모델링 목적으로만 조정되어 정렬 tax_가 적어 특정 벤치마크에서 전체 모델 출력에 비해 약간의 이점이 있기 때문이다. 마지막으로, EE-Tuning 중 데이터 누출의 위험과 관련하여, 우리는 우리의 훈련 데이터가 LLM 사전 훈련을 위해 일반적으로 사용되는 오픈 소스 데이터 세트에서 무작위로 샘플링된 1.3B 토큰의 작은 하위 집합임을 상기한다[5]. 따라서 EE-Tuning 동안 초기 출구 레이어는 평가 작업의 샘플이 거의 없다고 가정하는 것이 합리적이다.\n' +
      '\n' +
      '### 초기-출구 레이어의 초기화\n' +
      '\n' +
      '**관찰**: 복사는 랜덤에 비해 더 빠른 수렴과 더 낮은 손실을 초래하는 반면, 최종 추론 품질과 속도 향상은 두 경우 모두 유사하다.\n' +
      '\n' +
      '이 실험은 섹션 2.1에 도입된 두 가지 방법, 즉 복사 및 랜덤을 비교하여 초기 출구 레이어의 모델 매개변수를 초기화한다. 섹션 3.2와 동일한 설정을 따르고 MLP 초기 출구 레이어가 정규 랜덤 변수로 초기화되는 경우를 추가한다.\n' +
      '\n' +
      '그림 6은 초기화의 두 가지 방법에 대한 각 초기 출구에서의 훈련 손실 곡선을 보여준다. 우리는 복사에 의한 초기화가 튜닝 초기 단계(특히 더 깊은 층에서의 출구에 대해) 동안 훨씬 더 낮은 손실과 더 빠른 수렴으로 이어지는 것을 관찰하지만, 무작위 경우의 트레이닝 손실은 결국 한계 갭까지 따라갈 것이다. 단순화를 위해 실험 전반에 걸쳐 튜닝 반복 횟수를 수정하지만, 이러한 결과는 복사본을 사용하면 손실이 포화되는 즉시 튜닝 프로세스를 더 일찍 종료함으로써 트레이닝 효율성을 더욱 향상시킬 수 있음을 시사한다. 그림 7은 복사 또는 랜덤으로 초기화된 모델 간에 다운스트림 성능에 큰 차이가 없음을 추가로 보여준다.\n' +
      '\n' +
      '그림 6: 복사 또는 랜덤으로 초기화된 두 출구의 손실 곡선입니다. 8개의 출구에 대한 손실 곡선이 있는 전체 버전은 그림 17을 참조하십시오.\n' +
      '\n' +
      '그림 7: 복사 또는 랜덤으로 초기화된 초기 출구 레이어를 가진 모델의 다운스트림 성능.\n' +
      '\n' +
      '다양한 크기의 모델에 대한#### EE-Tuning\n' +
      '\n' +
      '**관측:** EE-Tuning은 서로 다른 크기의 모델에 대해 출력 품질을 희생시키지 않으면서 부드럽게 수렴하여 \\(1.2\\times\\)에서 \\(1.6\\times\\)의 추론 속도 향상에 도달하고, 큰 모델은 일반적으로 (MMLU를 제외하고) 더 나은 속도 향상을 달성한다.\n' +
      '\n' +
      '이 실험은 다양한 크기의 LLM에 대한 EE-튜닝의 효능을 검증한다. 이전 실험이 13B 모델에 초점을 맞추고 있지만 여기서는 크기 7B, 13B 또는 70B의 라마 2-챗 모델을 고려한다. 각 모델에 대해 트랜스포머 백본에 고르게 배치된 MLP 아키텍처를 사용하여 8개의 초기 출구를 추가하고 복사 방법으로 초기화한다.\n' +
      '\n' +
      '그림 8은 모델별 모든 출구의 학습 손실 곡선을 보여 다양한 크기의 모델에 대한 EE-Tuning의 수렴을 확인하였다. 놀랍지 않게, 더 큰 모델은 더 낮은 트레이닝 손실을 달성하고, 더 깊은 층에서 빠져나가는 것은 각 모델 내에서 더 낮은 손실을 달성한다. 그림 9는 \\(1/4\\), \\(2/4\\) 및 \\(3/4\\) 깊이에서 세 개의 초기 출구가 활성화된 각 모델이 우리가 고려하는 다운스트림 작업에서 전체 모델 추론보다 비슷하거나 때로는 더 높은 점수로 조기 출구 속도 향상을 달성한다는 것을 추가로 확인한다.\n' +
      '\n' +
      '도 8: 7B 내지 70B 범위의 크기의 모델에 대한 EE-Tuning의 트레이닝 손실 곡선.\n' +
      '\n' +
      '### Additional experiments\n' +
      '\n' +
      '우리는 EE-Tuning의 다른 측면들을 탐구했다. (1) 튜닝 프로세스를 위한 상이한 트레이닝 데이터 소스들; (2) 추론을 위해 활성화하기 위해 튜닝된 초기 출구들의 최상의 서브세트를 선택하는 것; (3) 초기 출구들에 의해 유도된 서브모델들의 다운스트림 성능; (4) 섹션 2.3에서 제안된 정적 또는 토큰-방향 동적 손실 가중치들을 사용한 튜닝들 사이의 차이; 및 (5) EE-Tuning이 완료된 후 풀-모델 파라미터 업데이트로 계속된 사전 트레이닝의 잠재적인 이점들. 그 결과는 부록 A에서 찾을 수 있다.\n' +
      '\n' +
      '##4 한계 및 향후 작업\n' +
      '\n' +
      'EE-Tuning vs. 조인트 트레이닝.EE-Tuning에서, 초기-출구 레이어들은 튜닝되고, 원래의 표준 LLM의 모듈들은 동결된다. 결과적으로, 튜닝 프로세스는 매우 효율적이며, 풀-모델 출력에 영향을 미치지 않는다. 명백한 단점은 트랜스포머 백본이 원래 초기 출구 출력이 아닌 전체 모델 출력을 생성하는 데 유용한 중간 은닉 상태를 생성하도록 훈련되었기 때문에 초기 출구의 표현성과 적응성이 제한적이라는 것이다. 즉, 조기 출구의 능력은 제한된 수의 훈련 가능한 매개변수에 의해 제한될 수밖에 없다. 충분한 계산 자원이 이용 가능할 때, 튜닝된 조기-출구 모델을 더욱 개선하기 위한 자연스러운 전략은 LoRA와 같은 완전-파라미터 연속 사전-트레이닝(CPT) 또는 파라미터-효율적인 미세-튜닝을 통해 네트워크 백본 및 조기 출구 모두의 공동 학습이다[20]. 실제로, 부록 A.4의 예비 실험 결과는 CPT 동안 조기 출구 손실이 계속 부드럽게 감소함을 확인한다. EE-Tuning + CPT의 훈련 효율성과 다운스트림 성능이 처음부터 사전 훈련과 어떻게 비교되는지 보고 두 경우 모두 학습 역학이 어떻게 다른지 이해하는 것이 흥미로울 것이다.\n' +
      '\n' +
      '다른 훈련 목표. EE-Tuning 방법의 한 가지 잠재적인 개선 사항은 사전 훈련 데이터에 대한 자기 회귀 언어 모델링 손실을 넘어 보다 일반적인 훈련 목표를 사용하는 것이다. 예를 들어, 원래 표준 LLM이 사전 훈련되고 미세 조정되었다는 점을 감안할 때 지식 증류[17]에서 교사 역할을 할 수 있으며 자체 출력 로짓을 소프트 라벨로 사용하여 조기 출구 레이어의 훈련을 감독할 수 있다. 때때로 자가 증류[29, 56]라고 하는 이 접근법은 초기 출구 모델을 처음부터 교육하기 위한 일부 이전 작업에서 채택되었지만 EE-튜닝 설정에 특히 적절하다는 것을 발견했다. 시끄럽고 잠재적으로 유해하거나 바람직하지 않은 콘텐츠를 포함하는 외부 사전 훈련 데이터에 의존하지 않고 원래 LLM에 의해 생성된 텍스트를 EE-튜닝을 위한 훈련 데이터로 사용하는 것을 고려할 수도 있다. 이러한 수정으로 조정된 초기 출구는 원래 LLM의 지식과 능력을 더 잘 계승할 수 있다.\n' +
      '\n' +
      '실험을 위한 제한된 구성.실험을 감당할 수 있는 한 광범위하게 만들려고 노력했지만, 여전히 여러 가지 요인이 고정되어 있습니다. 예를 들어, 우리는 초기 출구 LLM을 초기화하기 위해 Llama 2-Chat 모델만 시도했으며 모든 다운스트림 평가에 하나의 추론 메커니즘(탐욕 디코딩 및 신뢰 기반 출구 조건)을 사용했다. 또한, 7B 모델보다 70B 모델을 튜닝하기 위해 더 큰 배치 크기와 총 토큰 수를 사용하는 것이 더 합리적일 수 있지만, 트레이닝을 위한 하이퍼파라미터는 다양한 크기의 모델에 대해 동일하다. 결과적으로, 우리의 실험에서 광범위한 설정으로 관찰과 결론을 추론하는 데 주의해야 한다.\n' +
      '\n' +
      '그림 9: 다양한 크기의 초기 출구 모델의 다운스트림 성능. 각 모델에 대해 1/4, 2/4 및 3/4 깊이의 조기 출구가 활성화됩니다. 신뢰역치가 감소함에 따라 좌에서 우로 속도가 증가하여 \\(\\{1.0,0.9,0.8,0.6,0.4,0.2\\}\\)의 값을 취한다.\n' +
      '\n' +
      '조기 종료 추론 동안의 출력 품질 및 속도는 트레이닝 구성 또는 추론/디코딩 메커니즘의 더 나은 선택에 의해 더 향상될 수 있다.\n' +
      '\n' +
      '정렬을 위한 미세 조정이 부족하다.우리의 초기 출구 모델은 인간 선호도와 정렬을 위해 미세 조정된 라마 2-챗 모델로 초기화되었다[49]. 그러나 초기 출구 레이어는 사전 훈련 데이터와 언어 모델링 손실만으로 조정되었다. 조정된 초기 출구 모델은 초기 출구 레이어에서 모델 매개변수의 수가 상대적으로 적기 때문에 원래 Llama 2-Chat 모델의 정렬 특성을 대부분 보존한다고 추측한다. 즉, 초기 출구 모델의 유용성과 안전성과 같은 관련 메트릭을 경험적으로 평가하지 않았습니다. 이러한 모델을 배포하기 전에 주의를 기울여야 하며 더 나은 정렬을 위한 추가 미세 조정 단계가 도움이 될 수 있다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '이 연구는 기존의 LLM을 매개변수 효율적인 방식으로 조기 출구 LLM으로 변환하는 경량적이고 경제적인 접근법인 EE-Tuning에 대한 통합적이고 체계적인 연구를 제공했다. EE-Tuning의 구현은 최대 계산 효율에 잘 최적화되어 있으며, 대용량 3D 병렬성과의 호환성 덕분에 확장성이 뛰어나다. 광범위한 실험 결과는 전체 매개변수 훈련에 비해 적은 훈련 비용으로 EE-Tuning이 추론 단계에서 출력 품질의 저하 없이 뛰어난 속도를 달성하는 조기 출구 LLM을 성공적으로 반환한다는 것을 검증했다. 이 작업이 조기 퇴장 LLM을 지역사회에 더 쉽게 접근할 수 있게 해주기를 바랍니다.\n' +
      '\n' +
      '## 부록 추가 실험\n' +
      '\n' +
      '이 섹션에는 EE-Tuning에 대한 추가 실험 및 경험적 결과가 포함된다.\n' +
      '\n' +
      '### EE 튜닝을 위한 훈련 데이터\n' +
      '\n' +
      '이 실험은 EE-Tuning에 사용되는 학습 데이터의 영향을 탐구한다. 이전 실험과 유사하게 트랜스포머 백본에 균일한 간격으로 8개의 MLP 초기 출구가 있는 13B 모델을 고려하고 복사 방법으로 초기화한다. 유일한 차이점은 이전 실험에서 사용된 _pre-training_ 데이터 대신, 여기서 튜닝 프로세스를 위해 Alpaca-CoT 데이터세트[44]의 정제된 서브세트인 Data-Juicer[5]에서 제공하는 _instruction fine-tuning_(IFT) 데이터4를 사용한다는 것이다. 일반적으로, 사전 훈련 데이터는 더 다양하지만, IFT 데이터는 더 깨끗하고 구조화되어 질의 응답 형식으로 구성된다.\n' +
      '\n' +
      '각주 4: [https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer](https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer]\n' +
      '\n' +
      '그림 10은 튜닝 프로세스의 시작과 끝 모두에서 다른 초기 출구에서의 트레이닝 손실을 보여준다. 사전 훈련 데이터와 비교하여 IFT 데이터를 사용하면 초기에 더 높은 손실이 발생하지만 튜닝이 완료된 후에는 더 낮은 손실이 발생한다. 이것은 초기 출구가 튜닝 동안 IFT 데이터의 포맷 및 스타일에 빠르게 적응하기 때문에 가능하며, 이후, IFT 데이터가 더 깨끗하고, 더 구조화되고, 덜 다양하기 때문에 더 낮은 손실이 달성될 수 있다.\n' +
      '\n' +
      '그림 11은 다양한 초기 출구 아키텍처를 가진 모델의 다운스트림 성능을 보여준다. 튜닝이 사전 학습 데이터가 아닌 IFT로 이루어진다는 점을 제외하면 그림 5와 유사하다. 우리는 MLP 아키텍처에 해당하는 두 그림의 곡선을 추가로 취하고 그림 12에서 나란히 비교하며, 이는 사전 훈련 데이터로 튜닝된 모델이 IFT 데이터로 튜닝된 모델보다 실제로 더 우수하다는 것을 보여준다. 부록의 표 3은 두 모델에 의해 생성된 일부 예제 텍스트를 보여준다. IFT 데이터로 튜닝된 모델은 컨텍스트 내 예제의 형식을 엄격하게 따르는 대신 출력에서 바람직하지 않은 형식을 사용하는 경우가 있으며, 이는 IFT 데이터에 어느 정도 과적합하여 발생할 수 있다. 이러한 결과는 EE-Tuning을 위해 사전 학습 데이터를 먼저 사용하는 것이 더 합리적일 수 있으며, 이는 추가된 조기 출구가 일반적인 언어 능력을 습득할 수 있도록 하며, 이후 IFT 데이터를 추가 미세 조정에 사용할 수 있음을 시사한다.\n' +
      '\n' +
      '그림 11: IFT 데이터로 조정된 다양한 초기 출구 아키텍처를 가진 13B 모델의 다운스트림 성능. 각 곡선에 대해 신뢰 역치는 \\({1.0,0.9,0.8,0.6,0.4,0.2}\\)의 값을 취하여 왼쪽에서 오른쪽으로 감소한다.\n' +
      '\n' +
      '그림 12: 사전 훈련 또는 IFT 데이터로 튜닝된 조기 출구 LLM의 다운스트림 성능 간의 나란히 비교. 여기서의 결과는 MLP 아키텍처에 해당하는 그림 5와 11의 곡선의 중복이다.\n' +
      '\n' +
      '도 10: 사전-트레이닝 또는 IFT 데이터 중 하나로 튜닝 프로세스의 시작(왼쪽) 또는 종료(오른쪽)에서 MLP 조기 출구의 트레이닝 손실.\n' +
      '\n' +
      '### 섹션 3.4에 대한 추가 결과\n' +
      '\n' +
      '아래는 다양한 크기의 모델에 대한 EE-튜닝의 유효성을 검증하는 섹션 3.4의 이전 실험에 대한 추가 경험적 결과이다.\n' +
      '\n' +
      '여러 개의 튜닝된 조기 출구가 있는 경우 활성화할 조기 출구를 선택하면 최상의 추론 성능을 위해 어떤 하위 집합이 활성화되어야 하는지 의문이 들 수 있습니다. 예비 탐사를 위해 우리는 단일 조기 탈출구만 활성화하는 것으로 제한한다. 각 옵션에 대한 다운스트림 태스크에서의 스코어 및 스피드 업은 그림 13에서 찾을 수 있다. 여기서의 핵심 트레이드 오프는, 더 깊은 층에서의 초기 출구들이 일반적으로 더 높은 능력을 갖지만, 또한 더 큰 추론 레이턴시를 갖는다는 것이다. 우리의 경험적 결과에 따르면 스위트 스팟은 사례마다 다른 것으로 보이며, 따라서 배치 전에 검증 세트에서 표준 하이퍼파라미터 최적화를 통해 활성화된 초기 출구의 하위 집합을 선택하는 것이 좋다.\n' +
      '\n' +
      '부산물: 조기 출구에 의해 유도된 서브모델.섹션 2.1로부터, 네트워크의 시작으로부터 조기 출구의 출력까지의 순방향 패스에 의해 커버되는 모듈들을 포함하는, 각각의 조기 출구에 의해 유도된 서브모델은 원래의 전체 모델보다 더 적은 트랜스포머 층들을 갖는 표준 트랜스포머 모델로 간주될 수 있음을 상기한다. 이러한 하위 모델은 표준 LLM 추론을 위해 배치될 수 있으며, 이는 또한 초기 출구 추론의 단순화된 메커니즘, 즉 시퀀스의 모든 토큰을 생성하기 위해 미리 지정된 동일한 조기 출구를 사용하는 것으로 간주될 수 있다. 실제로 그림 14의 경험적 결과는 이러한 하위 모델, 특히 심층층의 초기 출구에 해당하는 모델이 하류 작업에서 합리적으로 잘 수행됨을 확인시켜준다.\n' +
      '\n' +
      '### 동적 토큰별 손실 가중치 부여\n' +
      '\n' +
      '본 절에서는 EE-Tuning에서 동적 토큰 단위의 손실 가중치를 이용한 예비 탐색을 제공한다. 보다 구체적으로, 현재 학습 반복에서 모델 파라미터를 \\(\\mathbf{\\theta}\\)으로 표기하고, 데이터 배치에서 토큰 \\(\\mathbf{x}=[x_{1},x_{2},\\dots,x_{T}]\\)의 시퀀스를 고려하고, \\(\\mathbf{c}=[c_{1},c_{2},\\dots,c_{T}]\\in[0,1]^{T}\\)을 신뢰 값(즉, 다음 토큰 예측의 최대 확률)으로 한다.\n' +
      '\n' +
      '도 14: 초기 출구들에 의해 유도된 서브 모델들을 갖는 표준 추론의 다운스트림 성능.\n' +
      '\n' +
      '도 13: 다운스트림 작업에서 점수(상단) 및 속도 향상(하단)을 기록하고, 하나의 단일 조기 출구를 활성화시키는 상이한 선택을 갖는다. 신뢰 임계값은 0.8로 고정됩니다.\n' +
      '\n' +
      '계산 그래프에서 분리되어 상수로 간주되는 정방향 통과 동안 특정 초기 출구에서 계산됩니다. 그런 다음, 초기 출구에서의 이 시퀀스에 대한 트레이닝 손실은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathbf{x};\\mathbf{\\theta})\\coloneqq-\\sum_{t\\in[T]}c_{t}\\cdot\\log\\mathbb{P}( x_{t}|x_{1},\\dots,x_{t-1};\\mathbf{\\theta}).\\]\n' +
      '\n' +
      '즉, 각 토큰에 대한 음의 로그 우도는 해당 신뢰 값에 의해 가중치가 부여된다. 이상적으로, 이 방법은 각각의 초기 출구 계층이 그 능력 내에 있는 토큰들을 예측하는 것을 학습하도록 장려한다.\n' +
      '\n' +
      '이 방법의 유효성을 검증하기 위해 훈련 데이터의 절반에 대해 조정된 섹션 3.2의 13B MLP 모델의 중간 체크포인트를 기반으로 실험을 수행한다. 우리는 동적 토큰별 손실 가중이 활성화된 나머지 데이터의 절반에 대해 조정 프로세스를 계속하는 반면 다른 하이퍼파라미터는 변경되지 않은 상태로 유지한다. 그림 15는 위의 방법으로 얻은 모델과 전체에 걸쳐 일정한 손실 가중치를 사용하여 튜닝된 원래 모델의 다운스트림 성능을 비교한다. 불행하게도, 우리는 그들 사이에 뚜렷한 차이를 보지 못한다. 이것은 (i) 동적 가중치를 갖는 트레이닝의 짧은 기간; (ii) 트레이닝의 후반부에서의 작은 학습 속도; 또는 (iii) 트레이닝가능한 파라미터들의 작은 수에 의해 야기될 수 있다. 이러한 근본적인 이유와 동적 가중치의 이득을 얻을 수 있는 올바른 방법은 향후 작업에서 더 조사될 필요가 있다.\n' +
      '\n' +
      '### EE-Tuning 후 CPT(Continued Pre-training)\n' +
      '\n' +
      '전체 파라미터 CPT가 EE-Tuning을 통해 얻은 조기 출구 LLM을 더 개선할 수 있는지 이해하기 위해, 우리는 EE-LLM[6]에서 제공하는 사전 훈련을 위한 스크립트를 사용하여 7B 조기 출구 모델에 대한 CPT를 수행한다. 2개의 초기 출구 및 최종 출구에 대한 훈련 손실 곡선은 그림 16에서 찾을 수 있다. 흥미롭게도 초기 출구 손실은 계속 부드럽게 감소하는 반면 최종 출구 손실은 처음 몇 번의 반복 동안 빠르게 감소하다가 일정하게 유지되는 것을 관찰한다. 이러한 결과는 CPT가 전체 모델 출력에 부정적인 영향을 미치지 않으면서 초기 출구를 더욱 개선할 수 있음을 시사한다.\n' +
      '\n' +
      '그림 16: EE-Tuning이 완료된 후 CPT 동안 훈련 손실의 수렴.\n' +
      '\n' +
      '도 15: 일정 또는 동적 손실 가중치로 튜닝된 모델들의 다운스트림 성능.\n' +
      '\n' +
      '그림 17: 복사 또는 랜덤으로 초기화된 각 출구에서의 학습 손실 곡선입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{284.5pt}} \\hline \\hline Prompt & Artificial General Intelligence is \\\\ \\hline\n' +
      '13B & the holy grail of AI research. It is the ability of an AI system to perform any intellectual task that a human can, and to do so at a level of performance that is indistinguishable from a human. AGI would be a system that can learn, reason, and self-improve at a pace that is exponentially faster than human intelligence. \\\\  & There are many challenges to achieving AGI, including: \\\\  & 1. Lack of \\\\ \\hline\n' +
      '13B & the holy grail of AI research. It is the ability to create a machine that can perform any intellectual task that a human can, and potentially even surpass human-level performance. However, achieving AGI is a daunting task, and many experts believe that it is still decades away. In this article, we will explore the current state of AGI research, the challenges that researchers face, and the potential implications of achieving AGI. \\\\ \\hline\n' +
      '13B & the holy grail of AI research. It is the ability to create a machine that can perform any intellectual task that a human can, and potentially even surpass human-level performance. However, achieving AGI is a daunting task, and many experts believe that it is still decades away. \\\\  & One of the main challenges in achieving AGI is the problem of ”explainability.” As AI systems become more complex and sophisticated, it \\\\ \\hline\n' +
      '70B & a hypothetical AI that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, much like human intelligence. While we have made significant progress in developing AI systems that can perform specific tasks, such as image recognition, natural language processing, and autonomous driving, we are still far from achieving true AGI. \\\\  & One of the main challenges in developing AGI is the lack of understanding of human intelligence. While we have made significant progress in \\\\ \\hline\n' +
      '70B & a hypothetical AI that possesses the ability to understand, learn, and apply knowledge across a wide range of domains and tasks. It is an AI that can perform any intellectual task that a human can. \\\\  & Artificial Intelligence is a broader term that refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision making, and language translation. \\\\  & Artificial Intelligence can be applied \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 13B 또는 70B 조기-출구에 의해 생성된 예제 텍스트들(및 대응하는 추론 레이턴시들) LLM은 트랜스포머 백본의 1/4 깊이에서 하나의 MLP 조기 출구를 갖는다. 전체 모델 추론에 의해 생성된 텍스트와의 차이가 강조된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. _ArXiv_, abs/1607.06450, 2016.\n' +
      '* [2] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. _ArXiv_, abs/2310.05424, 2023.\n' +
      '* [3] Arian Bakhtiarinia, Qi Zhang, and Alexandros Iosifidis. Improving the accuracy of early exits in multi-exit architectures via curriculum learning. In _IJCNN_, 2021.\n' +
      '* [4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.\n' +
      '* [5] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. Data-juicer: A one-stop data processing system for large language models. _ArXiv_, abs/2309.02033, 2023.\n' +
      '* [6] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism. _ArXiv_, abs/2312.04916, 2023.\n' +
      '* [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodokumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24:240:1-240:113, 2023.\n' +
      '* [8] Luciano Del Corro, Allison Del Giorno, Sahaj Agarwal, Ting Yu, Ahmed Hassan Awadallah, and Subhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient llm inference. _ArXiv_, abs/2307.02628, 2023.\n' +
      '* [9] Yinwei Dai, Rui Pan, Anand Iyer, Kai Li, and Ravi Netravali. Apparate: Rethinking early exits to tame latency-throughput tensions in ml serving. _ArXiv_, abs/2312.05385, 2023.\n' +
      '* [10] Rahul Duggal, Scott Freitas, Sunny Dhammani, Duen Horng Chau, and Jimeng Sun. Elf: An early-exiting framework for long-tailed classification. _ArXiv_, abs/2006.11979, 2020.\n' +
      '* [11] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In _ICLR_, 2020.\n' +
      '* [12] Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and Eyal Shnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In _ACL_, 2023.\n' +
      '* [13] Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_, abs/1603.08983, 2016.\n' +
      '* [14] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. _IEEE Trans. Pattern Anal. Mach. Intell._, 44(11):7436-7456, 2022.\n' +
      '\n' +
      '* [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.\n' +
      '* [16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _ICLR_, 2020.\n' +
      '* [17] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. _ArXiv_, abs/1503.02531, 2015.\n' +
      '* [18] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic BERT with adaptive width and depth. In _NeurIPS_, 2020.\n' +
      '* [19] Boren Hu, Yun Zhu, Jiacheng Li, and Siliang Tang. Smartbert: A promotion of dynamic early exiting mechanism for accelerating bert inference. In _IJCAI_, 2023.\n' +
      '* [20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In _ICLR_, 2022.\n' +
      '* [21] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. Multi-scale dense networks for resource efficient image classification. In _ICLR_, 2018.\n' +
      '* [22] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep networks: Understanding and mitigating network overthinking. In _ICML_, volume 97, pages 3301-3310, 2019.\n' +
      '* [23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2014.\n' +
      '* [24] Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. _Trans. Assoc. Comput. Linguistics_, 6:317-328, 2018.\n' +
      '* [25] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence C. McAfee, Michael Andersson, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. _ArXiv_, abs/2205.05198, 2022.\n' +
      '* [26] Stefanos Laskaridis, Alexandros Kouris, and Nicholas D. Lane. Adaptive inference through early-exit networks: Design, challenges and directions. In _EMDL@MobiSys_, pages 1-6. ACM, 2021.\n' +
      '* [27] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. Accelerating bert inference for sequence labeling via early-exit. In _ACL_, 2021.\n' +
      '* 146, 2023.\n' +
      '* [29] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. Fastbert: a self-distilling BERT with adaptive inference time. In _ACL_, pages 6035-6044, 2020.\n' +
      '* [30] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\'t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In _EMNLP_, pages 1797-1807, 2018.\n' +
      '* [31] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on GPU clusters using megatron-lm. In _SC_, page 58, 2021.\n' +
      '\n' +
      '* [32] OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023.\n' +
      '* [33] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy. Conditional deep learning for energy-efficient and enhanced pattern recognition. In _2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)_, pages 475-480, 2015.\n' +
      '* [34] Ofir Press and Lior Wolf. Using the output embedding to improve language models. In _EACL_, pages 157-163, 2017.\n' +
      '* [35] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\n' +
      '* [36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019.\n' +
      '* [37] Florence Regol, Joud Chataoui, and Mark Coates. Jointly-learned exit and inference for a dynamic neural network : Jei-dnn. _ArXiv_, abs/2310.09163, 2023.\n' +
      '* 966, 2020.\n' +
      '* [39] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In _NeurIPS_, 2022.\n' +
      '* [40] Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Consistent accelerated inference via confident adaptive transformers. In _EMNLP_, pages 4962-4979, 2021.\n' +
      '* [41] Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A. Smith. The right tool for the job: Matching model and instance complexities. In _ACL_, pages 6640-6651, 2020.\n' +
      '* [42] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In _ACL_, pages 1073-1083, 2017.\n' +
      '* [43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _ArXiv_, abs/1909.08053, 2019.\n' +
      '* [44] Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical study of instruction-tuning large language models in chinese. _ArXiv_, abs/2310.07328, 2023.\n' +
      '* [45] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _ArXiv_, abs/2201.11990, 2022.\n' +
      '* [46] Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Y. Li, and Dongkuan Xu. Deediff: Dynamic uncertainty-aware early exiting for accelerating diffusion model generation. _ArXiv_, abs/2309.17074, 2023.\n' +
      '* [47] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early exiting from deep neural networks. In _ICPR_, pages 2464-2469, 2016.\n' +
      '* [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _ArXiv_, abs/2302.13971, 2023.\n' +
      '\n' +
      '* [49] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _ArXiv_, abs/2307.09288, 2023.\n' +
      '* [50] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by enabling intermediate layer decoding via instruction tuning with lite. _ArXiv_, abs/2310.18581, 2023.\n' +
      '* [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, pages 5998-6008, 2017.\n' +
      '* [52] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating BERT inference. In _ACL_, pages 2246-2251, 2020.\n' +
      '* [53] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. Berrit: Early exiting for BERT with better fine-tuning and extension to regression. In _EACL_, pages 91-104, 2021.\n' +
      '* [54] Canwen Xu and Julian McAuley. A survey on dynamic neural networks for natural language processing. In _EACL_, pages 2370-2381, 2023.\n' +
      '* [55] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In _NeurIPS_, pages 12360-12371, 2019.\n' +
      '* [56] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. _IEEE Trans. Pattern Anal. Mach. Intell._, 44(8):4388-4403, 2022.\n' +
      '* [57] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu Wei. BERT loses patience: Fast and robust inference with early exit. In _NeurIPS_, 2020.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>