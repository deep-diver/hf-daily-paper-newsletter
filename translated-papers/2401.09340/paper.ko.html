<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# SceneVerse:\n' +
      '\n' +
      '계획된 모델을 위한 3D 비전-언어 학습.\n' +
      '\n' +
      ' 유야야 자송 니수오네그 자니아\\({}^{**}\\)\n' +
      '\n' +
      '리위, 황우.\n' +
      '\n' +
      '베이징종합인공지능연구소(BIGAI).\n' +
      '\n' +
      '[https://scene-verse.github.io](https://scene-verse.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3D 물리적 환경과 언어를 정렬하는 데 중점을 둔_3D 비전 라운딩은 체화된 에이전트의 개발에 초석으로 서 있다. 2D 영역의 최근 진보와 비교하여 3D 장면의 접지 언어는 다양한 객체 구성, 풍부한 속성 및 복잡한 관계로 인한 3D 장면의 고유한 복잡성, (i) 쌍을 이루는 3D 비전-언어 데이터의 부족, (ii) 접지된 3D 데이터로부터 지식을 증류시키기 위한 통일된 학습 프레임워크의 부재 등 몇 가지 중요한 도전에 직면해 있다. 이 연구에서 우리는 실내 환경에서 3D 비전-언어 학습을 체계적으로 업스케일링할 수 있는 가능성을 조사하여 3D 비전-언어에서 이 세 가지 주요 과제를 해결하는 것을 목표로 한다. 우리는 약 \\(68\\)K 3D 실내 장면을 포함하고 인간 주석 및 확장 가능한 장면-사진 기반 생성 접근법에서 파생된 \\(2.5\\)M 비전-언어 쌍을 포함하는 첫 **100만 규모의** 3D 비전-언어 데이터세트 SceneVerse를 소개한다. 우리는 이 스케일링이 3D 비전 언어 학습을 위해 통합 사전 훈련 프레임워크인 스카이크(GPS)를 준비한다는 것을 보여준다. 광범위한 실험을 통해 기존의 모든 3D 시각적 접지 벤치마크에 대한 최첨단 성능을 달성하여 GPS의 효과를 보여주고 있다. 스카네버스와 GPS의 방대한 잠재력은 도전적인 3D 비전-언어 작업에서 제로샷 전달 실험을 통해 공개된다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간의 인지 발달의 근간은 물리적 세계[46, 73, 97] 내의 언어의 접지 속에 있다. 최근 대규모 언어 모델(LLM) [11, 75]의 진행은 종종 "정리 모델"[10]으로 지칭되었으며, 100억 규모의 비전 언어 데이터 세트[71, 96]를 활용하여 비전과 언어 [3, 51, 66] 간의 정렬을 현저하게 촉진했다. 그럼에도 불구하고, 이러한 발전은 주로 2D 도메인에 초점을 맞추고 있기 때문에 3D 물리적 환경에 대한 근거 이해는 이전 단계 [1, 5, 16]에 남아 있다. 인간 인지 [7, 8]를 형성하는 데 근거 있는 3D 경험의 중추적인 역할과 이 영역의 연구 개발이 지연되었음을 인식하면 특히 3D 장면의 맥락에서 비전-언어 학습 도전에 대한 탐구를 강화할 필요성이 절실하다.\n' +
      '\n' +
      '2D 비전-언어(2D-VL) 성과로부터 통찰력을 보고, 성공의 주요 요인은 쌍을 이루는 시력-언어 데이터[15, 45, 71]의 주목할 만한 스케일 업이었다. 그러나 이러한 원칙을 2D에서 3D로 직접 적용하는 것은 도전으로 인해 취약하다. 주로 3D 데이터 수집은 스캐닝 장치에 크게 의존하여 2D 이미지를 수집하는 것보다 본질적으로 훨씬 복잡하고 비용이 많이 든다. 3D 장면 데이터[9, 23, 58, 87]의 부피를 증가시키기 위한 꾸준한 노력에도 불구하고 대부분의 데이터 세트는 수천 장면으로 제한되었으며, 이는 실질적으로 기존 2D 데이터 세트의 규모보다 뒤쳐진다. 이 격차는 다양한 속성, 다양한 배열 및 복잡한 객체 간 관계를 갖는 다양한 객체 인스턴스를 특징으로 하는 3D 장면들의 고유한 복잡성에 의해 더욱 확대되었다. 3D 장면의 이러한 독특한 측면은 물체에 대한 정확한 설명과 이들의 관계를 더 어렵게 만들 뿐만 아니라 철저한 장면 묘사에 필요한 언어 설명의 수를 상당히 증가시킨다. 결과적으로, 이는 지상 장면 이해에 중요한 고품질 쌍을 이루는 장면-언어 데이터의 충분한 공급을 얻는 데 중요한 도전을 제시한다.\n' +
      '\n' +
      '이러한 도전에 맞서기 위해 지상 장면 이해를 위한 3D 비전 언어(3D-VL) 학습을 진전시키는 것을 목표로 하는 최초의 **100만 규모의** 데이터세트인 SceneVerse 구축에 대한 현재의 노력을 공고히 할 것을 제안한다. 현장 수준에서 기존 데이터세트[9, 23, 40, 67, 78]에서 3D 장면 데이터를 단일화하고 합성 장면[27, 95]로 컬렉션을 보충한다. 이 편집은 현재까지 수집된 가장 광범위한 3D 장면 데이터를 나타내며, 접지용 \\(68,406\\) 장면에 해당한다. 또한 3D 장면 그래프[4, 79]와 LLM을 활용한 자동화 생성 파이프라인을 제안하여 포괄적인 고품질 장면 언어 쌍을 만듭니다. I\\(190,836\\) 인간 주석이 달린 쌍과 총 \\(2.5\\)M 장면-언어 쌍을 포함한 이 정제된 컬렉션은 3D 장면 내의 객체 수준 및 장면 수준 설명에 대한 상세하고 포괄적인 묘사를 제공한다.\n' +
      '\n' +
      '우리는 대규모 사전 학습을 통해 스켄버스에서 데이터 스케일 업이 제공하는 잠재력을 철저히 조사한다. 구체적으로 장면 수준 및 객체 수준 정렬 목표로 설계되고 보조 손실 및 설계가 없는 스켄(GPS)을 위한 새롭고 통일된 사전 훈련 프레임워크를 제시한다. 다단계 대비 정렬을 통해 기존의 모든 3D 시각적 접지 벤치마크에 걸쳐 유의한 성능 개선을 관찰하여 간단하고 효과적인 사전 훈련 과정을 통해 새로운 최첨단 결과를 달성했다. 또한, 우리는 스켄버스와 GPS가 제공하는 방대한 가능성을 제로 샷 전송 설정에서 3D-VL 작업에서 공개하고 있다. 마지막으로, 우리는 미래의 방향을 지적하기 위해 광범위한 절제 실험을 통해 스켄버스에서 데이터-스케일링 효과에 대한 보다 포괄적인 이해도를 제공한다.\n' +
      '\n' +
      '우리의 주요 기여금은 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* 우리는 그라운드 장면 이해를 위해 처음 100만 규모의 3D-VL 데이터세트인 SceneVerse를 소개한다. SceneVerse는 인간 주석 및 자동화된 생성 방법의 조합을 통해 조달된 \\(68\\)K 3D 장면을 \\(2.5\\)M 장면-언어 쌍과 포함한다. 이는 사전 데이터 세트와 비교하여 데이터 다양성과 규모 측면에서 상당한 개선을 나타낸다.\n' +
      '* 우리는 기존 모든 3D-VL 접지 벤치마크에서 최첨단 결과를 달성하는 다단계 장면-텍스트 정렬으로 훈련된 효율적인 변압기 기반 모델인 GPS를 제안하며, SceneVerse에서 다중 수준의 장면-언어 쌍에 대한 사전 훈련에서 혜택을 받는다.\n' +
      '* 우리는 데이터 스케일업 및 모델 설계와 함께 미리 훈련된 모델이 2D-VL 모델에서 볼 수 있는 성공과 일치하여 그라운드 장면 이해에서 새로운 제로 샷 일반화 능력을 나타낸다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '정렬된 3D 언어 데이터를 달성하기 위한 데이터베이스는 본질적으로 어려운 작업이다. 3D 객체 모델링에서 샤프넷[14]과 같은 선구적 작품은 온라인 리플렉스에서 3D 자산을 조달하여 고품질 3D 객체 데이터셋[22, 60, 81]의 후속 증식으로 이어진다. 특히, 최근 개발에는 3D-언어 정렬에 대한 객체 수준 캡션[83]의 통합과 함께 오바마자버스 [25, 26]을 사용한 인터넷 규모의 데이터 수집이 포함된다. 이러한 데이터 세트에 대해 훈련된 모델은 분류 [52], 생성 [53], 자막 작업[55]에서 분명하고 오브젝트에 대한 향상된 이해도를 보여준다.\n' +
      '\n' +
      '대조적으로, 장면 획득 및 주석의 광범위한 요구 사항으로 인해 지상 3D 장면 이해를 위한 데이터 세트를 개발하는 것은 훨씬 더 어렵다. 기존 작품들은 RGB-D를 큐레이트하고 실내 장면 데이터셋[9, 13, 23, 58, 67, 78]을 처음에 3D 객체 검출 및 분할 [30, 42, 59, 72, 77]과 같은 고전적 접지 작업을 벤치마킹하는 데 사용하였다. 이러한 의미적으로 표지된 장면들은 이후 객체 참조 [1, 16, 93], 자막 [17, 19, 20, 88], 시력-언어-결찰[38, 56, 63, 80] 및 추론 [5, 37, 57]과 같은 미세화된 장면 접지 작업에 적용된다. 최근 작업은 계층적 구조와 간결하게 장면들을 묘사하는 3D 장면 그래프(3DSG)(4, 69, 79)의 표현을 이용한다. 이 표현은 LLM과의 호환성으로 인해 [2, 68] 및 캡션[33]을 계획하는 데 특히 유리하다. 그럼에도 불구하고 타브에서 볼 수 있듯이. 1, 이러한 데이터 세트는 장면 및 언어 규모 모두에서 상당히 제한되며, 접지된 장면 이해도를 향상시키기 위해 미세화된 장면 정렬 데이터를 스케일링할 필요성을 강조한다.\n' +
      '\n' +
      '**비전 언어 학습**휴양년은 변압기 기반 사전 학습 모델[11, 28, 62] 및 대규모 이미지 언어 데이터 세트[15, 71]에 의해 구동되는 2D 비전 언어 학습 [3, 24, 49, 51, 66, 70, 76]에서 엄청난 진전을 목격했다. 언어 및 2D-VL 영역을 가로지르는 중심 주제는 단순화된 대조 사전 훈련 파이프라인[66]을 통해 개방형 동물 이해 [32, 44, 47, 50]에서 정렬 및 확장 능력에 의해 입증된 바와 같이 데이터 스케일링[43]의 효과이다.\n' +
      '\n' +
      '그러나 지상 장면 이해에서 모델의 주요 과제는 2D-VL에서 얻은 통찰력의 적용을 제한하는 쌍을 이루는 3D 장면-언어 데이터의 제한된 가용성이었다. 3D 장면 접지 [6, 18, 35, 40, 41, 54, 82, 86, 94]의 현재 모델은 모델 및 손실 설계 또는 고급 최적화 전략[98] 모두에서 과제별 지식에 크게 의존한다. 이러한 격차를 해소하기 위해 3D-VL[34, 36, 64, 74, 83, 91, 92]에 대한 사전 훈련된 2D-VL 모델을 사용하는 데 중점을 두고 있다. 그럼에도 불구하고, 이러한 모델은 주로 2D-VL 모델(_e.g_., 객체 속성, 어포던스, _etc_)에서 이용 가능한 정보에 대해 도출되며, 3D 데이터를 통해서만 달성 가능한 객체 공간 관계처럼 중요한 정보를 캡처하는 데 짧다. 이것은 특히 3D 특정 정보에 관한 언어와 3D 장면 간의 다중 수준의 정렬의 필요성을 강조한다. 기존 3D 사전 훈련 방법의 초기 단계[29, 84, 98]를 고려할 때, 우리는 SceneVerse와 GPS가 3D-VL 연구에서 새로운 대안을 구축할 가능성이 있다고 믿는다.\n' +
      '\n' +
      '## 3 SceneVerse\n' +
      '\n' +
      'SceneVerse는 지상 장면 이해를 위해 고안된 최초의 100만 규모의 데이터세트이다. 우리의 3D 장면들은 실제 및 합성 환경의 다양한 기존 데이터 세트들로부터 큐레이션된다. 3D 장면 그래프와 LLM의 힘을 살려, 대상 수준 및 장면 수준 설명 모두에 대해 종합적이고 고품질 언어를 생성하기 위한 자동화 파이프라인을 소개합니다. 우리는 현재까지 가장 광범위한 인간 미공개 객체 참조를 통합하여 이 분야에서 새로운 훈련 소스 및 벤치마크를 제공한다.\n' +
      '\n' +
      '### Scene Curation\n' +
      '\n' +
      '사용 가능한 3D 장면 데이터의 부족을 해결하기 위해 기존 다양한 데이터세트로부터 3D 장면 데이터를 일원화하여 SceneVerse를 구성한다. 스카넷[23], ARKitScenes[9], HM3D[67], 3RScan[78] 및 멀티Scan[58]을 포함한 실제 장면 데이터 세트를 사용하여 구조화된 3D[95] 및 ProcTHOR[27]의 합성 환경과 함께 사용한다. 이러한 합성 데이터 세트를 포함하는 것은 주로 3D-VL 정렬을 위한 확장 가능한 데이터 소스로서의 잠재력에 의해 동기화된다. 다양한 출처에 걸쳐 응집력을 보장하기 위해 객실 분할, 포인트 서브샘플링, 축 정렬, 정규화 및 의미론적 라벨 정렬과 같은 전처리 단계를 수행한다. 각 스캔은 포인트 클라우드 \\(\\mathrm{P}\\in\\mathbb{R}^{N\\te 8}\\)로 표시되며, 각 지점은 3D 좌표, RGB 색상, 인스턴스 id 및 의미 라벨로 정의된다. 전체적으로 스켄버스에서 \\(68,406\\) 3D 장면을 큐레이트합니다.\n' +
      '\n' +
      '3D 스켄 브러시 건설.\n' +
      '\n' +
      '노드 \\(\\mathcal{V},\\mathcal{V},\\mathcal{V})는 tuples \\(\\mathcal{V}(\\mathcal{V},\\mathcal{E})\\ 세트로 정의되며, 여기서 노드 \\(\\mathcal{V}_{1}\\mathcal{V}_{1}\\mathcal{V},\\mathcal{V},\\mathcal{V},\\mathcal{V}. 각각의 노드(v\\)는 중심형 \\(\\mathbf{p}_{i}\\in\\mathbb{R}^{R}^{3}\\)에 의해 매개변수가 되는 하나의 별개의 3D 객체 인스턴스를 나타내며, \\(\\mathbf{b}^{R}^{3}\\) 및 바운딩 박스 크기(B\\mathbf{b}_{i}_{i}_{i}. 위치 \\(\\mathcal{E}\\)는 노드들 간의 공간적 관계를 나타낸다.\n' +
      '\n' +
      '장면 그래프 \\(\\mathcal{G}\\)를 구성하기 위해 먼저 포인트 구름에서 인스턴스 주석으로 노드를 인스턴스하고 해당 의미 라벨로 객체 클래스를 할당한다. 사전 작업[1, 79]에 이어 다음과 같은 공간 관계를 고려한다.\n' +
      '\n' +
      '**Vertical 근접**는 접촉 내 관계(_e.g_), 지지체, 내부, 내장) 및 비접촉자 관계(_e.g_ 이하)를 모두 포함한다.\n' +
      '\n' +
      '**수평 근접** 호라이온 관계에서는 앞, 뒤에, _etc_와 같은 근접 관계를 설명한다. 왼쪽과 같은 관계는 맥락적으로 참조 뷰에 의존하며, 여기서 오른쪽은 맥락적으로 기준 뷰에 의존하게 된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Dataset} & \\multicolumn{2}{c}{3D Data} & \\multicolumn{3}{c}{Language} \\\\ \\cline{2-5}  & Scene & Object & Anno. & Syn. & Total \\\\ \\hline ScanRefer[16] & \\multirow{2}{*}{\\(|\\)} & \\multirow{2}{*}{\\(|\\)} & 52K & - & 52K \\\\ ReferIt3D[1] & & & 42K & 200K & 242K \\\\ ScanQA[5] & 1.5K & 33K & 27K & - & 27K \\\\ SQA3D[57] & & & - & 33K & 33K \\\\ Multi3DRefer[93] & \\multirow{2}{*}{\\(|\\)} & \\multirow{2}{*}{\\(|\\)} & 52K & 10K & 62K \\\\ Cap3D[55] & & & 66K & 58K & 666K & 724K \\\\ ScanScribe[98] & 3K & 56K & 94K & 184K & 278K \\\\ \\hline \\hline SceneVerse & 68K & 1.5M & 190K & 2.3M & 2.5M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** SceneVerse와 기존 3DVL Datasets의 비교는 표 1: **이다. SceneVerse는 사전 작업의 데이터 척도를 크기만큼 확장한다. 아니, 인간 주석. 신: 템플릿 또는 LLM 생성 설명**만터 객체는 뷰 방향을 설정하기 위해 사용된다. 두 객체 사이의 거리 또한 객체가 우주에서 멀리 있는지 또는 근처에 있는지 여부를 설명하기 위해 계산된다.\n' +
      '\n' +
      '다객체 관계는 다중 객체, _e.g_의 공간적 배열, 정렬 및 사이를 모델링한다.\n' +
      '\n' +
      '노드 계층화는 지지 관계에 의해 결정된다. 우리는 잘못된 것을 바로잡기 위해 자동 검증 절차를 거친 공간 관계를 계산하기 위해 모든 객체 노드를 횡단한다. 현장 그래프 구축 및 관계 결정에 대한 보다 자세한 설명은 부록 A.2를 참조하시기 바랍니다.\n' +
      '\n' +
      'LLMs.\n' +
      '\n' +
      '스카네버스의 장면-언어 쌍은 3D 장면의 다양한 측면을 포착하는 것을 목표로 하며, 이는 객체 자막에서 상세한 객체 속성 설명, 객체 참조 내의 객체 간의 공간적 관계, 장면 자막에서 글로벌 장면 서술 등을 포함한다. 3D 장면 그래프를 기반으로 템플릿과 LLM을 모두 사용하여 이 세 가지 과립에 대한 설명을 자동으로 생성한다.\n' +
      '\n' +
      '객체 소비 객체 캡션은 물체의 시각적 및 물리적 특성에 대한 자세한 설명을 제공하는 것을 목표로 하며, 고유한 특징으로 객체 수준의 접지력을 용이하게 한다. 멀티뷰 이미지를 감안할 때, 우리는 오브젝트 \\(v\\in\\mathcal{V}\\)의 포인트 클라우드를 사용하여 렌더링을 통해 이미지 내의 발생을 식별한다. 그런 다음 이미지를 렌더링된 바운딩 박스로 크롭하고 BLIP2 [48]를 통해 처리하여 초기 객체 캡처를 생성한다. 캡션을 정제하기 위해 CLIP[66] 유사도 점수가 가장 높고 폐색이 최소인 상위 10개의 문장을 선택한다. 선택된 문장들은 객체 캡션에 대한 일관된 요약들을 얻기 위해 LLM에 공급된다. 이 과정에서 우리는 잠재적 오류를 식별하고 보정하기 위해 언어 모델을 명시적으로 지시한다. 세부 객체 캡션 파이프라인은 부록 A.3에 나와 있다.\n' +
      '\n' +
      '객체 리커랄객체 관계 캡션은 현장에서 자신의 공간적 관계를 표현함으로써 물체를 의미한다. 공간 관계 트리플츠((v_{i},v_{j},e_{ij})\\)는 먼저 구축된 3차원 장면 그래프에서 추출된다. 우리는 각 관계 유형에 대한 설명을 생성하기 위해 다양한 템플릿을 설계하여 엔티티들을 (대상-객체, 공간-상관, 앵커-객체들)의 형태로 할당한다. 이는 "의자가 팔경 옆에 있다", "소파를 맞아 신발 오른쪽에 먼 여행지가 있다", "냉장고는 캐비닛과 소파 사이에 있다."와 같은 예를 들 수 있다. 템플릿 기반 설명에 복잡성을 추가하기 위해 장면 그래프에서 랜덤하게 선택된 3개의 인접 객체들에 대한 관계를 기술함으로써 대상 객체에 대한 참조가 생성된 "스타 판독" 템플릿을 설계한다. 설계된 템플릿은 수동적이고 활성적인 텐스뿐만 아니라 반전 조항에도 걸쳐서 생성된 텍스트의 풍부성에 기여한다. 설명의 자연스러움을 높이기 위해 문장 재사용을 위해 LLM을 사용한다. 그림. 2는 재설정 전후의 설명에 대한 통계를 제시한다.\n' +
      '\n' +
      '그림 2: **SceneVerse 수집 및 통계. 3D 장면(a)을 감안할 때, 당사의 자동화 파이프라인(c)은 장면 캡션, 객체 캡션 및 객체 추천을 포함한 세 가지 유형의 설명을 생성한다. (b) 다른 언어원 및 데이터 구성의 비교****(b) 다른 언어원 및 데이터 구성의 비교.\n' +
      '\n' +
      '장면의 캡션(Scene Captioning)은 글로벌 정보를 강조하여, 그 속성과 기능성과 함께 현장의 핵심 객체들을 묘사한다. 우리는 구축된 3D 장면 그래프와 신속한 LLM을 활용하여 이러한 캡처를 생성한다. 장면의 포획의 다양성을 향상시키기 위해 에지 및 노드의 하위 집합을 장면 맥락으로 무작위로 샘플링하는 서브그래프 샘플링 전략을 사용한다. 객체 수는 또한 LLM 프롬프트로 제공되며, 이러한 주석을 데이터셋에서 사용할 수 있는 경우 객실 유형 및 객체 속성과 함께 제공된다.\n' +
      '\n' +
      '후만인에 의한 설명.\n' +
      '\n' +
      '자동으로 생성된 장면-텍스트 쌍 외에도 SceneVerse는 현재까지 인간-노출된 상황-풍부한 객체 참조의 가장 포괄적인 세트를 포함하며, 지상 장면 이해 능력을 평가하기 위한 귀중한 벤치마크 역할을 한다. 인간 주석에는 ARKitScenes[9], HM3D[67] 및 멀티Scan[58]에 \\(96,863\\) 설명이 포함되어 있다. 주석 과정에서 한 사람의 주석을 배치하여 3D 장면 내의 단일 3D 객체를 명확하게 지칭하기 위해 적어도 20개의 단어를 작성하도록 하였다. 그런 다음 각 추천 텍스트는 3D 장면과 주석이 달린 추천 텍스트를 기반으로 참조된 객체를 정확하게 찾을 수 있도록 의무화된 두 개의 추가 검토자에 의한 독립적인 검증을 거치게 된다. 두 리뷰어에 의해 검증을 통과하지 않는 모든 객체 참조는 재투표를 위해 깃발된다.\n' +
      '\n' +
      '소매 및 통계.\n' +
      '\n' +
      '총 SceneVerse는 총 \\(68,406\\) 룸 레벨 3D 스캔으로 구성되며 소스 구성은 그림 1에 나와 있다. 2(b). 데이터세트에는 이전 작업 [1, 79]에 따른 \\(21\\) 유형의 관계를 포함하는 \\(1.5\\)M 객체 인스턴스가 포함되어 있다. 언어 설명을 위해 우리는 라마[75]와 GPT-3.5 [61]에 의해 재연된 LLM에 의해 \\(1\\)M 템플릿 기반 텍스트와 \\(1\\)M 문장을 생성한다. 모든 재피싱 및 요약 프롬프트는 전체 관계 세트와 함께 부록 A.3에 자세히 설명되어 있으며 자동화된 언어 생성 파이프라인의 효능을 확인하기 위해 12K 생성 객체 수준 설명을 인간 검증을 위해 무작위로 선택하여 \\(96.93\\%\\) 합격률을 달성하는 품질 검사(QC)를 수행한다. 이것은 우리의 제안된 장면 사진 기반 생성 접근법이 고품질 언어 설명을 생성하는 능력을 보여주며 미래의 확장성에 강력한 기초를 제공한다.\n' +
      '\n' +
      '4 G가 스톱을 위한 Pre-트레이닝을 조직했다.\n' +
      '\n' +
      '본 절에서는 3D 장면과 텍스트를 정렬하기 위한 다단계 대비손실로 학습된 효율적인 변압기 기반 모델인 GPS를 소개한다. 그림과 같이. 3, 우리는 다른 수준에서 수집된 언어 설명을 에코하여 GPS의 대조적 목표를 위해 대상 수준, 추천 대상 수준 및 장면 수준에서 장면-언어 쌍을 형성한다. 다음 섹션에서 각 수준의 설계를 설명합니다.\n' +
      '\n' +
      '### Object-level Grounding\n' +
      '\n' +
      '3D 장면 클라우드(\\mathcal{S}\\)를 감안할 때, 3D 장면 클라우드 \\(N\\-shelf 3D 객체 분할 모델을 사용하여 \\(N\\) 객체 \\(\\mathcal{S}=\\{\\mathbf{o}_{1},\\mathbf{o}_{1},\\mathbf{o}_{n})의 백으로 분해한다. 우리는 물체(\\{\\mathbf{f}_{i}^{O}\\}\\)를 객체 포인트 클라우드 인코더와 텍스트 특징(\\{\\\\{\\mathbf{f}_{i}^{O}\\}\\)을 객체 배열 \\(\\{\\mathbf{f}_{i}^{i}_{i}^{\\}}\\}\\)을 냉동 언어 모델에 공급하여\\(\\{\\:\\{\\bf{f}_{i}^{i}d-captions \\)을 가지고 있다. [83] 이후, 우리는 객체 특징 및 텍스트 특징에 대한 교차 모달 정렬을 통해 수행한다.\n' +
      '\n' +
      'r.\\\\frac{1}}(D^{\\ill{j}}:d^{\\ot{r}})\\log{\\sum_{r}}(p,q)\\\\ce{r}}}<\\log{\\ill{I}}}(D^{\\ue{obj}})}\\log{\\leit)}(D^{\\fusion{\\ill{r}}} <\\\\fusion{\\ill{B}})\\fusion{\\\\\\\\fusion{erg)\\leit}}}(p,q)\\\\fusion{\\ill{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\fusion{\\ill{Ex)\\leit}}}}}}}<\\fusion{\\]\\\\fusion{\\\\\\\\\\max{I}}}}}})\\\\fusion{\\\\\\\\\\\\\\\\\\\\\\\\ap)\\leit}}}}}}}<\\\\f\n' +
      '\n' +
      'r\\(D^{\\text{obj}}(p,q)=(\\mathbf{f}_{p}^{f}^mathbf{f}_{q}^{T}_{q}^{T}/\\tau)\\)는 대상체와 텍스트 특징 사이의 점 생성물을 나타내고 \\(p,q)\\는 훈련 배치에서 모든 오브젝트-텍스트 쌍에서 정렬된 오브젝트-텍스트 쌍에서 정렬된 오브젝트-텍스트 쌍(r\\) 쌍을 나타낸다. CLIP [66]과 유사하게 모델 학습을 용이하게 하기 위해 학습 가능한 온도 매개변수 \\(\\tau\\)를 사용한다.\n' +
      '\n' +
      '### Scene-level Grounding\n' +
      '\n' +
      '정렬된 객체 특징들을 사용하여, 우리는 추출된 객체 특징들에 객체 공간 위치들을 통합함으로써 장면을 인코딩한다. 구체적으로, 우리는 [18, 98]에 이어 공간 위치 특징 \\(\\{\\mathbf{f{f}_{i}^{O}\\\\)로 추출된 객체 특징을 인코딩하기 위해 공간 변압기 모델을 사용한다.\n' +
      '\n' +
      '\\[\\mathbf{f}^{S}=\\mathrm{SpatialAttn}(\\{\\mathbf{f}_{i}^{O}\\},\\{\\mathbf{l}_{i}\\})\\]\n' +
      '\n' +
      '\\(\\{\\mathbf{f}_{i}^{S}\\}\\)는 공간 위치 기능으로 인코딩한 후 객체 \\(\\mathbf{o}_{i}\\)의 특징을 나타낸다. 현장 수준의 정렬을 수행하기 위해\n' +
      '\n' +
      '제안된 GPS 모델*****의 대조 정렬은 모델 학습을 위한 마스크 언어 모델링 객관적인 \\(\\mathcal{L}_{\\text{L}_{\\text{obj}}\\) 뿐만 아니라 \\(\\mathcal{L}_{\\text{L}_{\\text{MLM}}\\) 세 가지 수준(\\mathcal{L}_{\\text{L}_{\\ill{L}_{\\)에서 3가지 수준, \\(\\mathcal{L}_{\\{L}_{\\)에서 대조 정렬, \\(\\mathcal{L}_{\\ill{L}_{\\)과 \\(\\mathcal{L}_{\\ill{L}_{\\)에서 대조 정렬), \\(\\mathcal{L}_{\\)에서 대조 정렬, \\(\\mathcal{L}_{\\ill{L}_{\\) 및 \\(\\mathcal{L}_{\\ill{I}_{\\:{\\/{\\)에서 대조 정렬, \\\n' +
      '\n' +
      '우리는 이러한 장면 수준 객체 특징(\\{\\\\\\mathbf{f}_{i}^{S}\\}\\)에서 작동하며 장면 캡션 \\(\\mathbf{T}^{\\text{scene}}\\)과 정렬한다. 구체적으로, 우리는 객체 특징을 투영층으로 공급하여 모든 객체 특징들에 대한 맥스풀링을 사용하여 장면 특징 \\(\\mathbf{g}^{S}\\)을 얻는다. 객체 레벨 접지과 유사하게 튜닝 가능한 언어 모델을 통해 장면 자막을 통과시켜 텍스트 특징 \\(\\mathbf{g}^{T}\\)를 획득하고 장면 수준 대비 정렬을 수행한다.\n' +
      '\n' +
      '카페인{scene}}=D^frac{scene}}(p,q))}}\\log{\\sum_{TP}}}}\\log{\\sum_{TP}}}(p,q)}\\log\\frac{{r}} \\log\\frac{{r}}(D^{\\obacterium{r}})}<\\log\\\\fusion{{r}}:\\\\fusion{fart{TP}}} <\\\\fusion{erg{TP}}}}{d\\fusion{TP}}}}}}{\\\\\\\\\\\\\\{s\\\\\\{TP}}{d\\fusion{finc{TP}}}{d\\fusion{d\\fusion{fusion{TP}}}{d\\fusion{d\\ce{TP}}}}}}{d\\fusion{fusion{TP}}}:\\fusion{d\\obacterium{d\\obacterium{TP}}}}:\\fusion{d\\obacterium{v}}})}}(p,q)}}\n' +
      '\n' +
      '장면의 특징(p,q)=(D^bf{g}_{p}^math{g}}^mathbf}}{p}}{q}/\\tau)은 장면의 특징 \\(\\mathbf{g}_{p}_{p}^{S}}/\\tau)과 훈련 배치에서 정렬된 각 한 쌍의 현장-텍스트 쌍에 대한 장면 특징 \\(\\mathbf{g}<\\mathbf{g}/\\mathbf{g}}.{p}.{p}/\\mathbf{g}}/\\mathbf{g}_{p}_{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}_{p}{p}}{p}{p}}{p}{p}{p}{p}{p}{p}{p}{p}{p}{p}\n' +
      '\n' +
      '### Referral-object-level Grounding\n' +
      '\n' +
      '표정에서 드러난 관계를 모델링하기 위해, 우리는 장면에서 물체 추천에 대한 자기 의도 기반 추론 변압기를 사용한다. 이 변압기는 장면-객체(\\{\\mathbf{f}_{i}^{S}\\}\\)와 객체 추천 \\(\\mathbf{T}^{\\{T}^{\\{ref}}\\)를 특징으로 하며 텍스트 설명과 객체 관계 간의 관계를 배우는 자기 의사를 수행한다. 우리는 대상당 추천 기능을 추출하기 위해 장면 수준 라운딩과 동일한 튜닝 가능한 언어 인코더를 사용한다. 우리는 현장-객체 특징과 함께 이 텍스트 특징을 자기-의도 변압기에 통과시켜 정렬된 객체 특징 \\(\\mathbf{h}_{i}^{S}\\) 및 문장 수준 추천 특징 \\(\\mathbf{h}^{T}\\)를 얻는다. 그런 다음 다음 추천 대상 수준 대비 정렬을 수행합니다.\n' +
      '\n' +
      '}}{{f}^mathbf{h}^{f}^{f}\\tau\\}\n' +
      '\n' +
      'HH(\\bar{\\mathbf{h}}^{S}\\)는 언급된 물체의 특징을 나타내는 경우, \\(p\\)는 동일한 장면 내의 모든 객체들에 걸쳐 반복된다. 특히, 물체- 및 장면-레벨 정렬에서 수행된 맥간 대조와 대조적으로, 우리는 양성 쌍의 선택이 동일한 장면에 있는 것으로 강요하여 미세-곡물 접지에 대한 점수 내 대비를 제공한다. 이는 2D-VL 모델[90]에서 영역-워드 정렬에 일반적으로 사용되는 대조와 이미지 내 및 이미지 간 성공을 모방한다.\n' +
      '\n' +
      '3D 장면과 언어 사이의 다중 수준 정렬을 배우기 위해 먼저 포인트 클라우드 인코더를 객체 레벨 접지 오브젝트로 트레이닝하여 장면 내의 물건을 접지하는 데 좋은 특징 초기화를 얻었다. 장면의 접지 단계 동안, 우리는 입력 대상 판독 텍스트에 대해 마스크 언어 모델링 손실 \\(\\mathcal{L}_{\\text{MLM}}\\)와 함께 인터 및 스코어링 내 목표를 트레이닝하여 언어 인코더 및 자기 의도 변환기 내의 파라미터를 조정한다. 무엇보다 GPS의 학습은 다음과 같은 목적을 최적화하는 것으로 요약될 수 있을 것이다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{\\text{obj}}+\\mathcal{L}_{\\text{scene}}+\\mathcal{L}_{ \\text{ref}}+\\mathcal{L}_{\\text{MLM}}.\\]\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '본 절에서는 다음과 같은 질문을 다루는 평가 결과를 제시한다.\n' +
      '\n' +
      '*는 3D 비주얼 접지용 스켄버스의 데이터 스케일링이 얼마나 효과적인가? 일반 사전 훈련 기반 3D-VL 모델을 위한 스케일 업 작업인가요?\n' +
      '*는 GPS 사전 훈련 파이프라인이 얼마나 잘 되어 있나요? 3D-VL 작업에서 2D-VL 모델의 유사한 특성을 나타내나요?\n' +
      '** 미래의 연구를 위해 스켄버스와 GPS가 제공하는 잠재력은 무엇입니까? 무슨 일이 없어졌나요?\n' +
      '\n' +
      '다음 섹션에서는 이러한 주요 주제에 대한 모델 성능에 대해 자세히 설명한다. 페이지 제한으로 인해, 우리는 구현 세부 사항 및 더 많은 실험 분석을 위해 독자를 부록 B 및 C로 지시한다.\n' +
      '\n' +
      '3D 정품 교환\n' +
      '\n' +
      '세트팅은 3D 시각적 접지, ScanRefer [16], Nr3D 및 Sr3D [1]에 대해 일반적으로 사용되는 3개의 데이터 세트에 대한 모델을 평가한다. Nr3D 및 Sr3D의 경우 Achioptas _et al_[1]]를 따른다. 지상-진실 객체 마스크를 사용하여 모델의 접지 정확도를 보고한다. 스카넌머의 경우 주 _et al_[98]]를 따라간다. Mask3D[72]를 사용하여 객체 제안을 생성합니다. 결과는 물체 바운딩 상자가 IoU \\(>0.5\\)와 지반 진리와 중첩되는 예측의 정확성을 평가하기 위해 Acc\\(@0.5\\)로 보고된다. 비교를 위해 미리 훈련된 GPS 및 데이터셋별 미세 조정 GPS의 결과를 제공함으로써 기존 기저부와 비교한다. 부록 C에서 더 자세한 내용을 확인하십시오.\n' +
      '\n' +
      'Tab에서 볼 수 있는 결과와 분석. 스카네버스에 대해 훈련된 2, GPS는 기존의 모든 3D-VL 접지 벤치마크에서 최첨단 결과를 달성합니다. 초기에는 GPS가 Ours(_scratch_)로 표시된 벤치마크 데이터셋의 훈련 세트에 직접 훈련될 때, 보다 복잡한 구조나 손실 디자인을 사용하는 기존 모델에 비해 과소 측정된다. 이 결과는 대비 정렬 패러다임의 데이터 집약적 특성을 강조한다. 그러나 SceneVerse에서 광범위한 훈련 데이터를 제시할 때, 추가 미세 조정 없이 우리 모델의 결과 _i.e_., Ours(_pre-train_)가 크게 개선되고 이미 ScanRefer와 같은 벤치마크에 대한 최첨단 결과를 달성한다. 더욱이, 데이터세트 특이적 미세 조정 모델인 _i.e_., Ours(_fine-tuned_), _한정-tuned_)은 다른 보조 아키텍처 또는 손실 목적 없이 미세 조정 중에 공동으로 최적화된 미리 학습된 모델 위에 단순 투영 MLP_만 추가된 기존의 기저부를 강력하게 능가한다. 이러한 결과는 3D-VL 작업에 대해 SceneVerse와 GPS 모두의 강력한 잠재력을 강조한다.\n' +
      '\n' +
      '### Zero-Shot Transfer\n' +
      '\n' +
      '스카네베르테 데이터와 GPS 모델의 효과를 더 잘 평가하기 위해 4개의 벤치마크, 스카이머, Sr3D, Nr3D 및 SceneVerse-val에서 모델의 능력을 테스트하기 위해 제로 샷 전달 실험을 추가로 수행한다. 멀티스캔에서 \\(271\\) 장면의 주석이 달린 객체 참조기(8.5K\\)를 사용하여 SceneVerse-val를 생성하고, 홀드 아웃 테스트 세트를 생성하기 위한 4:1 열차/테스트 분할 후 장면을 무작위로 분할한다. 우리는 주로 (i) _제로 샷_: 표적 데이터세트로부터 모든 장면을 제거하고, 운반되지 않은 장면에서 테스트하여 훈련된 모델, (ii) _제로 샷 텍스트_: 표적 데이터셋의 훈련 세트로부터의 3D 장면을 포함하는 데이터에 대해 훈련되었지만, 비선 장면-텍스트 분포로 독점적으로 테스트하였다. 구체적으로, _제로 샷 텍스트_ 설정에 대해, 우리는 SceneVerse에서 생성된 텍스트를 _제로 샷_ 모델에 대한 미세 조정 소스로 사용한다. 우리는 주로 최근 사전 훈련 기반 모델 3D-VisTA와 모델을 비교한다. 부록 C에서 실험 설정 및 구현에 대한 자세한 내용을 참조한다.\n' +
      '\n' +
      '** 결과 및 분석** 우리는 Tab에서 0샷 전달 실험의 결과를 제시한다. 3개, 타브. 4는 다음과 같은 주요 관측치를 가지고 있다.\n' +
      '\n' +
      '* 우리 GPS 모델은 3D-VisTA 모델에 비해 비선 장면에 대한 우수한 일반화를 보여준다. 제로 샷 전달 시나리오에서 우리의 모델은 확립된 벤치마크와 SceneVerse-val에 걸쳐 지속적으로 3D-VisTA를 능가한다. 이는 개방형 동물 재배치 및 전이 능력을 위한 2D-VL 모델에서 볼 수 있는 발전과 일치하여 전통적인 분류 목표에 대한 대조 정렬의 효과를 나타낸다.\n' +
      '* SceneVerse 데이터 세트는 특히 상대적으로 제한된 훈련 데이터 _i._i._ SceneVerse-val을 제공할 때 제로 샷 전달을 통해 3D-VL 접지 능력을 실질적으로 향상시킨다. 타브에서 입증된 바와 같이. 4, 스켄버스에 대해 0샷 방식으로 훈련된 모델을 처음부터 훈련된 모델과 비교할 때 상당히 향상된 성능이 있다. 이는 SceneVerse가 일반적인 3D 장면 접지 지식을 효과적으로 포착할 수 있음을 나타낸다. 결과적으로, _이는 3D-VL 태스크_에 대한 이동 전 학습 데이터세트로서의 잠재력을 강조한다.\n' +
      '* 현장 텍스트 쌍의 광범위한 수집 및 확장 가능한 생성의 영향은 _제로 샷 텍스트_ 설정의 결과에 의해 추가로 입증된다. 특히, Tab에서 볼 수 있듯이. 3, 자동으로 생성된 장면-텍스트 쌍은 장면 분포를 이해하기 위한 충분한 지식을 제공합니다. 이는 _제로 샷_ 성능에 대한 실질적인 개선에 크게 기여한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Overall & Easy & Hard & V-Dep. & V-Indep. \\\\ \\hline\n' +
      '3D-VisTA(_scratch__) & 40.7&37.3 & 37.3&37.3 \\\\-VisTA(_scratch_) & 40.7 & 53.3 \\\\-mm-VisTA(_scratch_) & 40.7 & 21.3 \\\\.3 & 44.3 \\\\-VisTA(_scratch_) & 40.7 및 53.3 \\\\-VisTA(_scratch_4.6 & 44.3 \\\\.3－S.3 \\\\-VisTA.1 & 95.3 ip.3 \\\\-VisTA.7 & 53.3 \\\\-VisTA.7 & 53.3 \\\\-m.3 ＋.3 －4.7 & 53.7 & 53.3 \\\\-VisTA(_4.7 & 53.3 ip.3 ＆.3        \n' +
      '53.7& 52.2 \\\\\\.7 & 52.2 \\\\.3D-VisTA(_제로 샷_) & 52.9 & 52.9 & 52.9 & 52.4 & 52.4 & 52.2 \\\\.3D-VisTA(_제로 샷_) & 52.9 & 52.9 & 52.9 & 52.9 & 52.9 & 52.9 & 52.9 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.9 & 52.9 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.4 & 52.9 & 52.9 및 5.4 & 52.4 & 52.9 및 5.9 및 5.9 및 5.9 및 5.9 및 5.9 및 5.4 및 53.4 및 53.4 및 53.4 및 53.4 및 53.4 및 53.4 및 53.4 및 53.4 및 53.3D-VisTA(0-shot__5.4 및 53.4 및 53.2\n' +
      '3D-VisTA (_zero-shot text_) & 58.1 & 70.0 & 39.6 & 52.5 & 64.1 \\\\ \\hline Ours (_scratch_) & 38.5 & 50.2 & 20.8 & 33.7 & 43.9 \\\\ Ours (_zero-shot_) & 59.2 & 69.4 & 44.0 & 53.1 & 66.3 \\\\ Ours (_zero-shot text_) & 60.6 & 70.9 & 45.1 & 54.8 & 67.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 4> < **Zero-shot 전달: SceneVerse-val. GT 객체 제안을 사용하여 Nr3D/Sr3D 설정 후 모델을 평가한다.****\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{Nr3D} & \\multicolumn{4}{c}{Sr3D} & \\multicolumn{4}{c}{ScanRefer Acc@0.5} \\\\ \\cline{2-13}  & Overall & Easy & Hard & V-Dep. & V-Indep. & Overall & Easy & Hard & V-Dep. & V-Indep. & Overall & Unique & Multiple \\\\ \\hline\n' +
      '앤앤드씨앤앤앤드씨앤앤앤드씨앤앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤앤씨씨앤앤씨씨앤앤앤씨씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤씨앤앤앤씨씨앤앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤씨씨앤앤\n' +
      '53.7&29.8 \\\\-SPS[54] & 51.5 & 58.5 & 51.5 & 51.5 & 99.5 & 63.2 & 61.2 & 63.4 & 29.8 \\\\-SPS[54] & 51.5 & 58.5 & 5.5 & 63.2 & 63.8.2 & 63.8.2 & 63.5 & 58.5 & 51.5 & 65.8.2 & 68.2 & 63.8.2 & 63.8.2 & 63.8.2 & 63.2 & 5.5 & 5.5 & 5.5 및 5.8.2 & 63.8.4 & 63.8.4 & 5.5 & 5.5 & 5.5 & 5.4 & 5.5 & 5.5 & 5.8.5 & 5.5 & 5.4 & 5.4 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.5 & 5.\n' +
      '비앤드앤드앤드앤드앤앤앤앤드(36.3)앤앤디앤앤앤드(64.3)앤앤앤드(64.3&&(36.7 & & 68.6 &(68.6 & 68.6 &(63.6 & 68.6 &(54.6 & 68.0 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 &(68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 68.6 & 6\n' +
      '3D-VisTA(_scratch_) [98.5 & 63.6 & 53.9 & 65.6 & 53.9 & 65.9 & 65.8 & 63.6 & 53.9 & 53.9 & 65.8 & 63.7 & 63.9 & 65.8 & 63.9 & 70.9 & 53.9 & 70.9 & 65.9 & 70.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.9 & 65.8 및 70.8.4 & 65.9 & 70.9 & 65.9 & 70.9 & 65.8.9 & 65.8.9 & 65.9 & 70.9 & 70.9 & 65.9 & 70.9 & 65.8.9 & 70.9 & 70.9 & 65.8.9 & 70.9 & 65.8.7.9 & 70.9 & 65.8이다.\n' +
      '3D-VisTA [98] & 64.2 & 72.1 & 56.7 & 61.5 & 65.1 & 76.4 & 78.8 & 71.3 & 58.9 & 77.3 & 45.8 & 75.1 & 39.1 \\\\ \\hline Ours (_scratch_) & 58.7 & 67.0 & 50.9 & 55.8 & 59.8 & 68.4 & 70.5 & 63.4 & 53.1 & 69.0 & 40.4 & 71.3 & 34.7 \\\\ Ours (_pre-train_) & 55.2 & 62.8 & 48.0 & 45.5 & 58.8 & 74.1 & 76.4 & 68.5 & 54.1 & 75.0 & 47.1 & 77.4 & 41.6 \\\\ Ours (_fine-tuned_) & **64.9** & **72.5** & **57.8** & 56.9 & **67.9** & **77.5** & **80.1** & **71.6** & **62.8** & **78.2** & **48.1** & **77.9** & **42.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **3D 비주얼 접지 결과는 Nr3D, Sr3D 및 ScanRefer에 대한 결과이다. 추가 미세 조정 헤드가 없는 SceneVerse에서 훈련된 모델에는 "_direct_"와 우리의 모델의 데이터 특이적 미세 조정 버전에 대해 "_fine-tune_"를 사용한다. 우리는 과감하게 최고의 결과를 강조합니다.***.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Method & Nr3D & Sq3D & ScanRefer@0.25 & ScanRefer@0.5 \\\\ \\hline\n' +
      '57.5&69.5 \\\\-3D-VisTA(_scratch_) & 57.5 & 69.5 & 45.5 \\\\.3D-VisTA(_scratch_) & 57.5 & 45.5 \\\\.3D-VisTA(_scratch_) & 45.5 \\\\.\n' +
      '3D-VisTA(_crcrw-shot__crcrw-shot_) & 35.2 & 33.2 & 29.6 \\\\ 33.2 & 29.6 \\\\ 33.2 & 33.2 & 29.6 \\\\.3D-VisTA(_crcrw-shot_) & 35.2 & 31.2 & 29.6 \\\\.3D-VisTA(_crcrw-shot_), 35.2 & 33.2 & 29.6 \\\\ 33.2 & 29.6 \\\\ 33.2 & 29.2 & 29.6 \\\\ 33.2 & 29.2 & 35.2 & 29.2 및 3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA(_crcr2 & 29.2 & 29.2 & 29.2 & 35.2 & 29.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA.3D-VisTA(_cr\n' +
      '3D-VisTA (_crrow-shot_) & 43.1 & 36.1 & 41.1 & 36.4 \\\\ \\hline Ours (_scratch_) & 58.7 & 68.4 & 44.5 & 40.4 \\\\ Ours (_crrow-shot_) & 32.4 & 33.3 & 35.2 & 31.1 \\\\ Ours (_crrow-shot text_) & 41.9 & 38.1 & 40.7 & 35.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **Zero-shot 전달 결과는 확립된 벤치마크에 대한 표 3: **Zero-shot 전달 결과****.\n' +
      '\n' +
      '선택학 및 논의.\n' +
      '\n' +
      '이 섹션에서는 주로 스켄버스에서 수집된 데이터에 초점을 맞춘 절제 연구를 제시한다. 우리의 목표는 데이터 스케일링의 효과를 보다 명확하게 설명하는 것이다. 모델 아키텍처에 대한 절제 연구에 관한 논의를 위해 독자를 부록 D라고 하며, 이 섹션에서는 다음과 같은 점을 구체적으로 논의한다.\n' +
      '\n' +
      '**는 데이터-스케일링이 얼마나 중요한가?** 우리는 GPS를 사전 훈련하는 동안 사용된 데이터의 양에 대해 절제 연구를 수행한다. 우리는 ScanRefer 및 SceneVerse-val에서 모델 성능에 대한 데이터-스케일링의 효과를 보여주기 위해 SceneVerse의\\(\\frac{1}{8}\\), \\(\\frac{1}{4}\\), \\(\\frac{1}{2}\\)로 훈련된 모델을 고려한다. 그림과 같이. 4, 우리는 두 설정 모두에 대한 데이터 스케일 증가에 대한 일관된 성능 향상을 관찰한다. 우리는 이러한 스케일링 효과가 3D-VL 접지뿐만 아니라 의미 세분화 [72, 85]와 같은 다른 3D 과제에 유익하다는 것을 보여주기 위해 부록 D에서 추가 실험을 제공한다.\n' +
      '\n' +
      '***는 인간 미충족 데이터와 비교하여 생성된 데이터를 어떻게 평가합니까?*** 우리는 다양한 장면-텍스트 소스를 사용하여 훈련된 모델의 성능을 평가하고, 특히 추가 미세 조정 없이 스카우머 데이터세트에서의 성능에 초점을 맞춘다. 타브에서 보는 바와 같이. 5, 템플릿 기반 생성된 텍스트와 대형 언어 모델(LLM)-반사 텍스트로 훈련된 모델은 ScanRefer에서만 훈련된 모델보다 상당한 개선을 보여준다. 더 중요한 것은, 우리의 모델의 이러한 변이체가 이전 기저부에 비해 이미 최첨단 결과를 달성한다는 것이다. 이것은 우리의 텍스트 세대 파이프라인의 효과를 나타낸다. 마지막으로, 우리는 인간 미표시 데이터를 추가하는 것이 여전히 모델 성능에 유익하다는 것을 관찰한다. 그러나 개선된 것은 생성된 데이터에 대해 훈련된 모델에 비해 상대적으로 한계이다.\n' +
      '\n' +
      '***은 이 스케일 업 과정에서 합성 장면의 역할은 무엇입니까?***와 3D-VL 작업에 대한 대규모 및 다양한 장면 데이터를 제공하는 합성 데이터와 함께 모델의 도메인 전달(Sim2진짜) 능력을 평가한다. 구체적으로, 우리는 SceneVerse, _i.e_, 스구조화된 3D 및 ProcTHOR의 두 합성 하위 집합에 독점적으로 훈련된 모델에 대해 SceneVerse의 모든 실제 장면에서 훈련된 모델을 비교한다. 타브에서 보는 바와 같이. 6, 합성 하위 집합에 대해 훈련된 모델은 실제 또는 기타 합성 장면으로 옮겨졌을 때 고통받으면서 해당 테스트 세트에서 놀라운 성능을 보여준다. 대조적으로, 실제 장면-텍스트 쌍에 대해 훈련된 모델은 합성 장면으로 일반화할 때 덜 심각한 성능을 나타낸다. 이 결과는 3D-VL 접지에서 실제 장면과 합성 장면 사이의 도메인 격차를 긍정하고 장면 자연성이 보장될 수 없을 때 장면 양의 간단한 스케일 업이 부족하다는 것을 보여준다. 우리의 품질 저하 언어 생성의 확장성과 우리의 실험에 나타난 스케일링 효과를 고려할 때, 추가 스케일링-업 3D-VL을 위한 속도 결정 단계는 자연 3D 장면 분포를 포착하는 다양하고 고품질의 실감 있는 장면들의 집합으로 나온다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 작업에서 우리는 지상 장면 이해의 맥락에서 3D-VL을 스케일링한다. 인간 주석 및 제안된 장면-텍스트 생성 접근법에서 제공하는 다양한 장면 및 다중 수준 장면 설명을 포함하는 100만 규모의 3D-VL 데이터세트 SceneVerse를 소개합니다. 스카네베이스를 활용하여 수집된 데이터에 대해 다중 수준의 장면-언어 대조 정렬으로 훈련된 모델인 스켄에 대한 그라운드 프레-트레이닝을 제안한다. 광범위한 실험을 통해 GPS가 기존의 모든 3D-VL 접지 작업에 대한 최첨단 결과를 달성한다는 것을 보여준다. 우리는 이전 기저부에 비해 스켄버스에 대해 훈련된 GPS의 개선된 일반화 성능을 보여주기 위해 제로 샷 전달 실험을 추가로 수행한다. 스카네버스에서 우리의 노력과 성공적인 스케일업 시도가 3D-VL에서 새로운 연구 패러다임의 길을 열어줄 수 있기를 바랍니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Real & Synthetic & SceneVerse-val & S3D & ProcTHOR \\\\ \\hline All & ✗ & 64.8 & 37.1 & 43.4 \\\\ ✗ & S3D & 7.0 & 85.1 & 16.1 \\\\ ✗ & ProcTHOR & 4.2 & 16.3 & 91.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **Cross 도메인 전달 결과***는 추가 미세 조정 없이 실제 및 합성 데이터 세트에서 학습된 모델의 결과를 보여준다. S3D\'는 구조화된 3D를 의미한다.\n' +
      '\n' +
      '그림 4: ** 모델 성능 _vs._ 데이터 척도.** 모달스는 데이터 스케일링 업으로 스카이머 및 SceneVerse-val에 대한 프리-트레인 및 제로-샷 전달 설정 모두에서 일관되게 향상된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline Template & LLM & Anno. & Acc@0.25 & Acc@0.5 \\\\ \\hline ✗ & ✗ & ✗ & 43.5 & 38.4 \\\\ ✓ & ✗ & ✗ & 50.9 & 46.1 \\\\ ✓ & ✓ & ✗ & 51.1 & 46.3 \\\\ ✓ & ✓ & ✓ & 52.0 & 47.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '교육***에 사용되는 장면-텍스트 쌍 유형에 대한 표 5: ** 증폭이다. 추가적인 지느러미가 없는 스칸 리커에 대한 모델 결과를 보고한다.\n' +
      '\n' +
      '## 7 Acknowledgement\n' +
      '\n' +
      '저자는 결과 시각화 프레임워크를 설계하는 BIGAI의 야오위 장, 데이터 생성 및 정제에 대한 제안을 위해 BIGAI의 장용황과 시오그쿤 링후, BIGAI의 동료들에게 도움이 되는 토론과 제안에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [2] Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, and Florian Shkurti. Taskography: Evaluating robot task planning over large 3d scene graphs. In _Proceedings of Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [4] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: A structure for unified semantics, 3d space, and camera. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2019.\n' +
      '* [5] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanga: 3d question answering for spatial scene understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [6] Eslam Bakr, Yasmeen Alsaedy, and Mohamed Elhoseiny. Look around and refer: 2d synthetic semantics knowledge distillation for 3d visual grounding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [7] Lawrence W Barsalou. Perceptual symbol systems. _Behavioral and brain sciences_, 22(4):577-660, 1999.\n' +
      '* [8] Lawrence W Barsalou. Grounded cognition. _Annu. Rev. Psychol._, 59:617-645, 2008.\n' +
      '* [9] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. In _Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track)_, 2021.\n' +
      '* [10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [12] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [13] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. _Proceedings of International Conference on 3D Vision (3DV)_, 2017.\n' +
      '* [14] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [16] Dave Zhenyu Chen, Angel X Chang, and Matthias Niessner. Scanrefer: 3d object localization in rgb-d scans using natural language. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [17] Dave Zhenyu Chen, Qirui Wu, Matthias Niessner, and Angel X Chang. D3net: a speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [18] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [19] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2capdetr. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [20] Zhenyu Chen, Ali Gholami, Matthias Niessner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [21] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Niessner, and Angel X Chang. Unit3d: A unified transformer for 3d dense captioning and visual grounding. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [22] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [23] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedingsof Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017.\n' +
      '* [24] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* [25] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [26] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [27] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)_, 2018.\n' +
      '* [29] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [30] Zhipeng Ding, Xu Han, and Marc Niethammer. Votenet: A deep learning label fusion method for multi-atlas segmentation. In _Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)_, 2019.\n' +
      '* [31] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [32] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [33] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. _arXiv preprint arXiv:2309.16650_, 2023.\n' +
      '* [34] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3d scene understanding from 2d vision-language models. In _Proceedings of Conference on Robot Learning (CoRL)_, 2022.\n' +
      '* [35] Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transfer3d: Entity-and-relation aware transformer for fine-grained 3d visual grounding. In _Proceedings of ACM International Conference on Multimedia (MM)_, 2021.\n' +
      '* [36] Deepit Hegde, Jaya Maria Jose Valanarasu, and Vishal Patel. Clip goes 3d: Leveraging prompt tuning for language grounded 3d recognition. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [37] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [38] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln bert: A recurrent vision-and-language bert for navigation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [39] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 2021.\n' +
      '* [40] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [41] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katrina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2022.\n' +
      '* [42] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [43] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* [44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [45] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In _International Journal of Computer Vision (IJCV)_, 2017.\n' +
      '* [46] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. _Behavioral and brain sciences_, 40:e253, 2017.\n' +
      '* [47] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '\n' +
      '* [48] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.\n' +
      '* [49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _Proceedings of International Conference on Machine Learning (ICML)_, 2022.\n' +
      '* [50] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [52] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. _arXiv preprint arXiv:2305.10764_, 2023.\n' +
      '* [53] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [54] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage 3d visual grounding via referred point progressive selection. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [55] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [56] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2019.\n' +
      '* [57] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In _Proceedings of International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [58] Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel Chang, and Manolis Savva. Multiscale: Scalable rgbd scanning for 3d environments with articulated objects. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [59] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [60] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [61] OpenAI. Introducing chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt), 2022.\n' +
      '* [62] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [63] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [64] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliascchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [67] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. In _Proceedings of Advances in Neural Information Processing Systems Datasets and Benchmarks (NeurIPS Datasets and Benchmarks Track)_, 2021.\n' +
      '* [68] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning. In _Proceedings of Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* [69] Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca Carlone. Kimera: From slam to spatial perception with 3d dynamic scene graphs. _International Journal of Robotics Research (IJRR)_, 2021.\n' +
      '* [70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photoorealistic text-to-image diffusion models with deep language understanding. _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [71] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [72] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In _Proceedings of International Conference on Robotics and Automation (ICRA)_, 2023.\n' +
      '* [73] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies. _Artificial life_, 11(1-2):13-29, 2005.\n' +
      '* [74] Ayca Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Open-mask3d: Open-vocabulary 3d instance segmentation. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2023.\n' +
      '* [75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [76] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [77] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. Softgroup for 3d instance segmentation on point clouds. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [78] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niessner. Rio: 3d object instance re-localization in changing indoor environments. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2019.\n' +
      '* [79] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* [80] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [81] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omnibject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [82] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and Jian Zhang. Eda: Explicit text-decoupling and dense alignment for 3d visual grounding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [83] Le Xue, Mingfei Gao, Chen Xing, Roberto Martin-Martin, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [84] Jihan Yang, Runyu Ding, Zhe Wang, and Xiaojuan Qi. Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. _arXiv preprint arXiv:2304.00962_, 2023.\n' +
      '* [85] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d: A pretrained transformer backbone for 3d indoor scene understanding. _arXiv preprint arXiv:2304.06906_, 2023.\n' +
      '* [86] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [87] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Niessner, and Angela Dai. Scannet++: A high-fidelity dataset of 3d indoor scenes. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [88] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [89] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [90] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Lianian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. In _Proceedings of Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [91] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Point-clip: Point cloud understanding by clip. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [92] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. In _Proceedings of Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [93] Yiming Zhang, ZeMing Gong, and Angel X Chang. Multi3drefer: Grounding text description to multiple 3d objects. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [94] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [95] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In _Proceedings of European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [96] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. _arXiv preprint arXiv:2304.06939_, 2023.\n' +
      '* [97] Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi,Ying Nian Wu, et al. Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense. _Engineering_, 6(3):310-345, 2020.\n' +
      '* Zhu et al. [2023] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In _Proceedings of International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '\n' +
      '## SceneVerse:\n' +
      '\n' +
      '계획된 모델을 위한 3D 비전-언어 학습.\n' +
      '\n' +
      'Supplementary Material\n' +
      '\n' +
      '부록 A에서는 3D 장면 전처리, 장면 그래프 구축, 자동 언어 생성 등 SceneVerse에 대한 자세한 내용을 소개한다. 애플리케이션스 B는 보다 많은 모델 및 구현 세부 사항을 제시한다. 부록 C는 본 논문의 실험에 대한 설정 및 구현에 대한 보다 심층적인 요약과 SceneVerse의 이점을 입증하기 위한 시맨틱 분할에 대한 남용 연구 및 추가 실험을 포함한다.\n' +
      '\n' +
      '부록 A SceneVerse.\n' +
      '\n' +
      '### 3D Scenes\n' +
      '\n' +
      '사용 가능한 3D 장면 데이터의 부족을 해결하기 위해 기존 다양한 데이터세트로부터 3D 장면 데이터를 일원화하여 SceneVerse를 구성한다. 큐레이션은 스턴넷[23], ARKitScenes[9], HM3D[67], 3RScan[78] 및 멀티Scan[58]과 같은 실제 장면의 데이터셋을 사용하여 구조화된 3D[95] 및 ProcTHOR[27]의 합성 환경과 연동된다. 이러한 합성 데이터 세트의 통합은 주로 3D-VL 정렬을 위한 확장 가능한 데이터 소스로서의 잠재력에 의해 주도된다. 훈련 과정을 용이하게 하기 위해 다음과 같은 전처리 단계를 진행한다.\n' +
      '\n' +
      '**룸 세그먼트** HM3D 및 ProcTHOR의 3D 장면은 건물 수준에서 방출되어 여러 방을 포함하고 때로는 50m 이상에 걸쳐 있다. 기존 벤치마크 [1, 16]과 정렬하기 위해 연관된 메타데이터를 활용하여 객실 수준에서 3D 포인트 클라우드를 분할하여 장면 그래프 구축 및 언어 설명 생성에서의 후속 동작을 용이하게 한다. 또한, 매우 큰 객실과 현장에서 4개 미만의 물체를 제외하기 위한 필터링 프로세스를 구현합니다.\n' +
      '\n' +
      '*** 포인트 클라우드 정규화** 다양한 데이터 소스 전체에 걸쳐 다양한 캡처 디바이스에서 발생하는 데이터 격차를 완화하기 위해 각 포인트 클라우드를 최대 \\(240,000\\) 포인트로 하위 샘플링한다. 그런 다음 각 포인트 클라우드는 바닥의 중심점을 중심으로 변환한 다음 회전하여 Chen _et al_[18]에 의한 접근에 따른 축과 객실 레이아웃을 정렬한다.\n' +
      '\n' +
      '***의미적 라벨 정렬***는 다양한 데이터셋에 걸쳐 의미론적 라벨 세트의 분산을 제시하며, 기존 모델 프레임워크 [98]에서 면문 객체 분류 [65]를 용이하게 하기 위해 ScanNet[23]의 \\(607\\) 의미 라벨에 모든 객체 클래스 라벨을 매핑하기 위한 포괄적인 노력을 기울이고 있다. 우리는 LLM과 수동 검증을 통해 각 데이터세트에서의 매핑을 구성한다. GPS의 객체 레벨 라운딩은 CLIP [36]와 유사하게 오픈셋 객체 라벨 또는 캡션을 직접 처리할 수 있다는 점에 유의한다.\n' +
      '\n' +
      '전처리 후, 각 스캔은 포인트 클라우드 \\(\\mathrm{P}\\in\\mathbb{R}^{N\\te 8}\\)로 표시되며, 각 지점은 3D 좌표, RGB 색상, 인스턴스 id 및 의미 라벨로 정의된다. 전체적으로 스켄버스에서 \\(68,406\\) 3D 장면을 큐레이트합니다.\n' +
      '\n' +
      '3D 스켄 브러시 건설.\n' +
      '\n' +
      'Sec. 3.2에서 포인트 클라우드의 3D 장면 그래프를 구성하기 위한 자동화 파이프라인을 소개합니다. 여기서는 보다 많은 구현 세부 사항과 관계 정의를 제공합니다.\n' +
      '\n' +
      '#### a.2.1 Relationships\n' +
      '\n' +
      '우리의 3D 장면 그래프는 Tab에서 볼 수 있듯이 \\(21\\) 유형의 관계를 캡처한다. A.1. 그림 A.1에서 볼 수 있듯이 이러한 관계가 3D 공간에서 어떻게 정의되는지 예시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline Category & Relation \\\\ \\hline \\(|\\) & supported by \\\\ In-contact vertical & embedded into \\\\  & placed in \\\\ \\(|\\) & inside \\\\ \\hline \\multirow{4}{*}{Non-contact vertical} & hanging on \\\\  & affixed on \\\\  & mounted on \\\\ Non-contact vertical & above \\\\  & higher than \\\\  & below \\\\  & lower than \\\\ \\hline \\multirow{4}{*}{Horizontal} & near(far) to the left of \\\\  & near(far) to the right of \\\\  & is behind \\\\ \\cline{1-1}  & is in front of \\\\ \\cline{1-1}  & close to \\\\ \\cline{1-1}  & adjacent to \\\\ \\cline{1-1}  & besides \\\\ \\cline{1-1}  & next to \\\\ \\hline Multi-object & between \\\\ \\cline{1-1}  & aligned \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: ** SceneVerse*** 3D 장면 그래프는 4가지 범주 범위의 21가지 유형의 관계를 캡처한다.\n' +
      '\n' +
      '2.2.2의 스카엔그래프건설###############.\n' +
      '\n' +
      '점 클라우드 표현에서 내재적 소음과 불완전성으로 인해 포인트 구름에서 자동으로 정확하고 포괄적인 관계를 추출하는 것은 비개인적인 과제이다. 아래에서는 Alg 1에 설명된 대로 3D 장면 그래프 구축 과정을 자세히 설명합니다.\n' +
      '\n' +
      '우리는 먼저 포인트 클라우드(\\mathrm{p_{i}}\\in\\mathbb{R}^{3}\\)의 인스턴스 주석과 각 노드를 객체 중심형 \\(\\mathrm{bb{R}^{3}\\)으로 인스턴스 노드를 인스턴스화하고 축 정렬된 바운딩 박스 \\(\\mathrm{b_{{R}^{3}^{i}}\\)의 크기와 크기(\\mathrm{b_{{R}^{3}^{R}^{3}^{R}^{R}^{3}^{R}^{R}^{R}^{3}^{R}^{R}^{3}^{R}^{i}. 다음으로, 우리는 그들의 공간적 관계를 결정하기 위해 모든 노드를 횡단한다(라인 4-22). 특히, 사물 노드가 장면에서 다른 객체들과의 접촉 내 수직 관계가 부족한 경우, 우리는 _"항가능"_와 같은 객체를 지정하고 그들의 비접촉 수직 관계(라인 9-13)를 계산한다. 이러한 물체의 예로는 그림, 커튼, _etc_가 있다. 마지막으로, 우리는 복수의 객체들(라인 23): i) 사이의 관계를 설정하는데, 목표 객체가 좌우로 표시된 두 개의 에지들과 연결되면, 대상 객체는 이웃하는 두 개의 노드들과 함께 관계 트리플트를 형성한다. 이(ii) X축 또는 Y축 방향의 객체 그룹의 중심점 좌표의 오프셋이 지정된 오프셋 임계값(\\delta\\)보다 작은 경우, 이 객체 그룹은 정렬 관계를 형성한다. 오프셋 임계치(\\delta\\)는 장면의 크기를 기준으로 조정될 것이다. 추가적으로, 우리는 장면 그래프를 검증하기 위해 자동 검증 절차를 사용하여 우리가 구축한 장면 그래프의 품질을 더욱 향상시킨다(라인 24). 검증 동작 중 하나는 상식에 기초하여 사물 간의 매핑과 관계 설명을 수동으로 유지하는 것을 포함한다. 예를 들어, 사람들은 보통 "움직이는 것"이 아니라 TV와 벽의 관계를 설명하기 위해 "마운트 온"을 사용한다. 따라서 (TV, 매달려, 벽)부터 (TV, 장착, 벽)까지 자동 정제해 드립니다.\n' +
      '\n' +
      '구성된 3D 장면 그래프(\\mathcal{V},\\mathcal{V},\\mathcal{V}})\\에서 노드 \\(\\mathcal{V}_{1}\\311cup\\mathcal{V}_{2}\\빅cup\\mathcal{V}_{K}\\)는 특정 계층 수준에서 노드 세트를 나타내는 \\(\\mathcal{V})를 포함한다. 위계는 지지 관계에 의해 결정되며, 예를 들어 바닥이 지원하는 오브젝트는 \\(\\mathcal{V}_{0}\\)를 구성하는 반면, 표가 지원하는 오브젝트는 \\(\\mathcal{V}_{1}\\), _etc_를 형성한다. 한 노드 \\(v\\in\\mathcal{V}_{k}\\)에서 유래한 가장자리는 인근 계층(\\mathcal{V}_{k}\\cup\\mathcal{V}_{k}\\cup\\mathcal{V}_{k+1}\\cup\\mathcal{V}_{k+1}\\)에서만 종료될 수 있다. 즉, 장면 그래프 내의 에지들은 동일한 계층적 레벨 내의 노드들을 독점적으로 연결하거나, 또는 한 레벨 이상의 노드들을 연결한다.\n' +
      '\n' +
      '```\n' +
      '입력 :\\(M\\) 객체 포인트 구름(P_{1}, P_{2},\\ldots, P_{m}\\) 출력 :\\(3\\)D 장면 그래프(\\mathcal{G},\\mathcal{V})\n' +
      '1:1, \\(1\\)에서 \\(i\\)까지의\\(i\\)에서 \\(M\\)도(M\\)까지 1:1:\\(i\\)\n' +
      '2: Create 노드 \\(v_{i}\\mathcal{V}\\) 및 오브젝트 포인트 클라우드 \\(P_{i}\\)의 경계 상자 크기 \\(b_{i}\\)를 사용한 Create 노드 \\(v_{i}\\)\n' +
      '3:endfor\n' +
      '4: \\(1\\)에서 \\(i\\)에서 \\(M\\)도까지의\\(i\\)에 대한 4: \\(1\\)에서 \\(M\\)도(M\\(M\\): \\(i\\)에서 \\(M\\)의\\(i\\)에 대한 4: \\(i\\)\n' +
      '5: \\(i+1\\)에서 \\(i+1\\)까지의\\(j\\)부터 \\(M\\)도)까지 5:5:\\(j\\)\n' +
      '6:\\(\\mathrm{RelsType}_{v}\\leftarrow\\mathrm{VerticalInContact}(v_{i},v_{j})\\)\n' +
      '7:\\((v_{i}_{v}})에서\\(\\mathcal{G}},e_{i,j\\)까지의 접촉 내 수직 관계 트리플츠.\n' +
      '8:endfor\n' +
      '9:\\(v_{i}\\)와 수평으로 관련된 노 오브젝트(v_{i}\\)\n' +
      'H\\(1\\)에서 \\(M\\) 및 \\(i\\neq k\\)까지의\\(k\\) 및 \\(i\\(i\\neq k\\)도 10:10:\\(k\\)\n' +
      '11:\\(\\mathrm{RelsType}_{v}\\leftarrow\\mathrm{VerticalNonContact}(v_{i},v_{k})\\)\n' +
      '12: 부비접촉 수직 관계 트리플츠(v_{i},v_{k},e_{v}\\)~\\(\\mathcal{G}\\)\n' +
      '13:endfor\n' +
      '14:endif\n' +
      '15:endfor\n' +
      '16:for\\(v_{i}\\in\\mathcal{V}\\)do\n' +
      'v_{i_{N}}\\}}(v_{i}}\\)는 동일한 접촉 내 수직 모 노드 \\(v_{i_{i}{1}},v_{i_{i_{i_{2}})를 갖는 \\(N\\) 서로 다른 노드이다.\n' +
      '18: \\(1\\)에서 \\(N\\)도까지의\\(j\\)에 대한 18: \\(j\\)\n' +
      '19:\\(\\mathrm{RelsType}_{h}\\leftarrow\\mathrm{Horizontal}(v_{i},v_{i_{j}})\\)\n' +
      '20:\\((v_{i},v_{i_{j}\\) 내지\\(\\mathcal{G}},e_{i,j})의 수평 관계 a\\(v_{i})\n' +
      '21:endfor\n' +
      '22:endfor\n' +
      '23: 업데이트 \\(\\mathcal{G}\\lelearrow\\mathrm{멀티코드}(\\mathcal{G})\n' +
      '자동 검증 절차가 있는 자동 확인 절차 24:업데이트 \\(\\mathcal{G}\\)\n' +
      '```\n' +
      '\n' +
      '원번호 1*\n' +
      '\n' +
      '어깨기는 소매.\n' +
      '\n' +
      'Sec. 3.3에서 템플릿과 LLM을 모두 채택하여 SceneVerse에서 장면 언어 쌍을 자동으로 생성한다. 이 섹션에는 더 많은 기술적 세부 사항 및 예가 제공된다.\n' +
      '\n' +
      'Pipelineing Pipelineing Pipeline#########.3.1을 대상으로 선별 선택 피프라인##############.\n' +
      '\n' +
      '객체 캡션은 물체의 시각적 및 물리적 특성에 대한 자세한 설명을 제공하는 것을 목표로 하며, 고유한 특징으로 객체 수준의 접지력을 촉진한다. 세부 객체 캡션 파이프라인은 Alg 2에서 요약되며, 다중 뷰 이미지 \\(\\{I_{1},I_{2},\\ldots,I_{n}\\}\\)를 통해 대상체의 포인트 클라우드(P_{o}\\)를 사용하여 이미지 스루 렌더링에서 가시점 \\(P_{o,v}^{vis}\\)를 얻는다. 폐쇄 점수 \\(s^{occ}_{o,v}\\)는 가시점 수와 객체점 클라우드 사이의 비율로 계산된다. 그런 다음 이미지를 렌더링된 바운딩 박스로 크롭하고 BLIP2 [48]를 통해 처리하여 초기 객체 캡션 \\(C_{o,v}\\)을 생성한다. 각 초기 자막에 대해 \\(s^{clip}_{o,v}\\)로 표시된 텍스트와 크롭된 이미지 사이의 CLIP [66] 유사성 점수를 계산한다. 정제된 객체 캡션을 얻기 위해 CLIP 점수가 가장 높고 폐색이 최소인 상위 \\(10\\) 초기 캡션을 선택한다. 선택된 문장들은 객체 캡션에 대한 일관된 요약들을 얻기 위해 LLM에 공급된다. 이 과정에서 우리는 잠재적 오류를 식별하고 보정하기 위해 언어 모델을 명시적으로 지시한다.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      '이 아파트에는 5개의 캐비닛, 1개의 침대, 3개의 쓰레기통, 1개의 마이크로파, 1개의 TV가 있습니다. 캐비닛은 쓰레기통 앞에 위치하고 침대는 캐비닛 앞에 있습니다. 쓰레기 캔도 캐비닛 뒤에 있고 침대의 왼쪽에 있습니다. TV는 캐비닛 안에 있습니다. 침대는 캐비닛 뒤쪽에 있고 쓰레기통 오른쪽에 있습니다. 이 아파트는 보관 옵션이 잘 갖춰져 있고 편안한 수면 공간이 있는 것 같습니다.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      '이 방에는 건축층과 벽이 있습니다. 벽은 바닥에 부착되어 큰 문이 있는 방을 만듭니다. 벽에 블라인드 걸이가 있어 창문에 가깝습니다. 객실에는 넓은 창문, 벽면에 연결되는 히터와 천장 오버헤드가 있습니다. 방은 소파, 테이블, 의자가 장착되어 있습니다. 소파에는 쿠션과 콩백, 근처에 식물과 램프가 있습니다. 객실에는 TV, 화이트보드, 바닥에는 클로터도 있습니다. 객실의 전체적인 스타일은 편안하고 모던합니다.\n' +
      '\n' +
      '#### Scene Caption\n' +
      '\n' +
      '이 방에는 침대 2개, 창 2개, 램프 3개, 이불 3개, TV 6개, 베개 2개, 커튼 4개, 선반이 있습니다. TV는 선반보다 높은 위치에 있는 반면 소파는 침대 오른쪽에 있습니다. 베개 중 하나는 침대 내부에 있고 침대는 소파 왼쪽에 있습니다. 또한 램프는 전원 배출구보다 높게 위치하여 램프보다 낮다. 방은 여유와 오락을 위해 다양한 물건들로 편안한 생활공간으로 보인다.\n' +
      '\n' +
      'A.3.2.2 자동언어세대####\n' +
      '\n' +
      '템플릿 기반 템플릿을 생성하여 각 유형의 관계에 대한 설명을 생성한다. 우리는 관련된 물체의 수와 공간적 관계를 기반으로 템플릿을 세 가지 유형으로 분류했다.\n' +
      '\n' +
      '**** 쌍별**: 쌍별 템플릿은 장면에서 타겟 객체와 앵커 객체 사이의 위치 관계를 설명하는 데 사용된다. 우리는 강화 기반 설명을 풍부하게 하고 활동적이고 수동적인 긴장, 반전 조항에 걸쳐 다양한 템플릿을 설계한다. 대표적인 예가 아래에 나와 있습니다.\n' +
      '\n' +
      '타겟-객체(is) 공간-상관 앵커-객체이다.\n' +
      '\n' +
      '그림 A.2: ** 객체 캡션***의 예. ***, **bold**에서 목표 객체를 컬러링한다.\n' +
      '\n' +
      '그림 A.3: **의 장면 캡션 예.*** 장면의 캡션 예.\n' +
      '\n' +
      '* (is) 공간 상관 앵커 객체라는 타겟 객체이다.\n' +
      '*는 (is) 공간 상관 앵커 오브젝트가 있는 타겟 오브젝트가 있다.\n' +
      '* 공간 상관 앵커 오브젝트는 대상 객체이다.\n' +
      '* 공간 상관 앵커 객체, 타겟 오브젝트가 배치된다.\n' +
      '* ** 멀티-객체**: 이는 타겟 객체가 장면에서 다수의 앵커 객체들과 관계를 형성하거나 정렬할 때 사용된다. 주형은 **Pair-wise*** 템플릿과 동일한 건설 규칙을 따른다.\n' +
      '\n' +
      '그림 A.4: ** 오브젝트의 예.* 노트. 그린 바운딩 박스가 타겟 객체를 나타내고 노란색 바운딩 박스가 앵커 오브젝트(들)를 나타낸다는 것을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:18]\n' +
      '\n' +
      '공간 변압기의 특징을 집계하고 장면 자막의 [CLS] 특징과 정렬하기 위한 층입니다. 추천-객체-레벨 접지들을 위해, 우리는 획득한 객체 특징들뿐만 아니라 추천 언어 특징을 4층 자기-의도 변압기에 더 통과시키고 Sec 4.3에 기술된 접지 목적을 사용하여 참조 표현의 참조 오브젝트의 특징 및 [CLS] 특징에 매칭한다.\n' +
      '\n' +
      '** 훈련*** 객체 수준의 사전 학습을 위해 1500epochs 및 평가 기간이 없는 경우 학습률이 \\(1\\t 10^{-2}\\)인 AdamW 최적기를 사용한다. 훈련 중 512의 배치 크기를 사용하고 최소 학습률 \\(1\\t 10^{-3}\\)로 학습률 스케줄링을 위한 코사인 어닐링 방식을 레버리지한다. 언어 인코더(1\\ 10^{-5}\\)의 학습률, 공간 변압기에 대한 \\(1\\t10^{-4}\\)의 학습률, 자기 선택 변압기에 대한 \\(1\\t10^{-4}\\)의 학습률, 그리고 나머지 모든 학습 가능 파라미터(_e._e._e._e._CS.A.A.A.A.5\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4)의 학습률, 자기 선택 변압기에 대한 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)에 대해 \\(5\\ 10^{-4}\\)에 대해 \\(1\\ 10^{-4}\\)의 학습률, 자기 선택 변압기의 학습률(1\\ 10^{-4}\\)의 학습률(1\\ 10^{-4}\\)의 학습률, 그리고 나머지 모든 학습 가능 매개변수(5 모든 실험에 대해 500개의 평가 기간을 가진 150개의 epoch에 대한 모델과 \\(0.1\\)의 최소 학습률 비율로 학습률을 위한 코사인 어닐링 방식을 훈련시킨다. 모든 사전 훈련 실험은 약 2일 동안 스켄버스에 대한 가장 긴 사전 훈련과 함께 8개의 NVIDIA-A100 GPU에서 실행된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{42.7pt} p{284.5pt}} \\hline \\hline Description type & Prompt \\\\ \\hline Object caption & Summarize caption below. The summary should be a description of the **target-object**. Focus on the **target-object**’s attribute, like color, shape and material, _etc._ Identify and correct the potential errors. \\\\  & caption: _A bed in a hotel room. A white comforter on a bed. A bed with a striped comforter._. \\\\  & **target-object**: _Bed_ \\\\ \\hline Object referral & Rewrite the following caption using one random sentence structure. You should give me only one rewritten sentence without explanation. _The bed is between desk and nightstand._ \\\\ \\cline{2-3}  & Rewrite the following caption. You should give me only one rewritten sentence about **target-object** without explanation. Make sure **target-object** is the subject of the sentence, not anchor-object(s). If the sentence is in full inversion, keep the inversion. \\\\  & caption: _The armchair is next to the sofa._ \\\\  & **target-object**: _Armchair_ \\\\  & anchor-object(s): _Sofa_ \\\\ \\hline \\multicolumn{3}{p{42.7pt}}{Rewrite the following caption using one random sentence structure. You need to focus on the location and relations of the **target-object** that appears in the sentence. If multiple **target-object** appear in the sentence, you need to focus on the first **target-object** that appears. You can also add the **target-object**’s function and comfort level based on the sentence, e.g., how the objects can be used by humans and human activities in the scene. You should give me only one rewritten sentence without explanation. _Far from the bowl and peppershaker, the vase is to the left, it is also on the top of countertop._ \\\\  & **target-object**: _Vase_ \\\\ \\hline Scene captioning & Your task is to provide a summary for a scene from a given scene graph. The scene contains some objects, which compose a scene graph in json format. \\\\  & There are 3 types of descriptions in scene graph: “scene type” denotes the type of the scene. “objects count” then listed the objects in the scene and their quantity, it should be noted that the actual objects in the room may be more than listed. “objects relations” describe the spatial relations with objects. \\\\  & Also describe the scene concerning commonsense, e.g., how the objects can be used by human and human activity in the scene. The description should conform to the given scene graph. The spatial relations between objects can only be inferred from the “objects relations” in scene graph. Don’t describe each object in the scene, pick some objects of the scene for summary. Don’t describe each relations in the scene, pick some relations of the scene for summary. You can also summarize the room’s function, style, and comfort level based on the arrangement and count of objects within the room. The summary should be about the object types, object attributes, relative positions between objects. Your summary must not exceed 80 words. You must write using one random sentence structure. \\\\  & scene graph: { ’scene_type’: ’Bedroom’, ’object_count’: {’nightstand’:2,...}, ’relation’: {’nightstand’, ’on’, ’floor’}, {’backback’, ’in} \\\\  & front of’, bed\\(\\}\\),...) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2> <표 2> <표**C 실험 조회수>와 <표 2>의 <표 2>의 <표 2>의 <표** 스켄버테>에 사용된 < ** 절상>과 같다.\n' +
      '\n' +
      '이 절에서는 실험 설정, 모델 구현 및 추가 결과에 대한 세부 사항을 제공합니다.\n' +
      '\n' +
      '3D 정품 교환\n' +
      '\n' +
      '모든 데이터 세트를 설정하면 제공되는 훈련 세트만 있는 모든 모델을 평가합니다. 이전 작품 [98]에 이어 Tab 2에서 모든 데이터 세트의 검증 세트에 대한 모델 성능을 보고하는데, 최적화가 없는 객체 제안을 생성하기 위해 오프 시트 마스크3D 분할 모델을 사용했다.\n' +
      '\n' +
      'Sec. 5.1에서 간략하게 언급한 바와 같이, 우리는 주로 3D 시각적 접지 실험에서 3개의 모델 설정, 즉 _scratch_, _pre-train_ 및 _fine-tuned_를 고려했다. i_pre-train_ 설정의 경우 부록 B.2에서 언급된 동일한 설정을 따르며, _scratch_ 및 _fine-tuned_ 설정에서 다른 데이터세트 특이적 미세 조정 모델과 상당히 비교하기 위해 추천 접지 자기 의도 변환기의 객체 특징에 대해 추가 2층 MLP를 추가한다. 훈련 중 추가된 투영층에 대해 \\(1\\t 10^{-4}\\)의 학습률로 100epoch에 대한 모든 모델 가중치와 함께 이 접지 헤드를 미세 조정하고 부록 B.2에 설명된 구현과 동일한 다른 모든 설정을 설정했다.\n' +
      '\n' +
      '### Zero-shot Transfer\n' +
      '\n' +
      '0샷 실험에서 우리는 먼저 ScanNet 및 멀티Scan의 장면으로부터 SceneVerse에서 장면-텍스트 쌍을 집계하여 홀드 아웃 테스트 세트를 구성한다. 구체적으로 ScanRefer, Nr3D 및 Sr3D의 검증 세트를 사용한다. 스카네 베르테-값의 장면-텍스트 쌍의 경우, 멀티스캔 데이터세트에서 인간-미지 객체 참조의 무작위 샘플링 \\(\\frac{1}{5}\\)에 의해 설정된 테스트를 구성한다. 이는 멀티스캔 데이터세트 내의 8.5k 인간 표지 객체 참조에서 무작위로 추출된 약 1.7K 객체 참조를 갖는 테스트 세트를 초래한다. i_제로 샷_ 설정에서 ScanNet 및 멀티Scan을 제외한 SceneVerse의 데이터세트로부터의 모든 장면-텍스트 쌍을 사용한다. 여기에는 ARKitScenes, 3RScan 및 HM3D에서 인간 표지화 및 생성된 텍스트가 모두 포함된다. 이 설정은 모델의 일반화 능력을 비선지 장면과 비선지 텍스트로 물체를 접지하는 데 테스트하는 역할을 한다. i_제로 샷 텍스트_ 설정에서, 우리는 ScanNet 및 멀티Scan에서 생성된 장면-텍스트 쌍을 _제로 샷_ 설정에 사용된 데이터에 추가함으로써, 주로 엔즈 오브젝트 참조를 포함하는 홀드 아웃 테스트를 만든다.\n' +
      '\n' +
      '제로 샷 실험에서 구현은 주로 3개의 모델 설정 _scratch_, _제로 샷_ 및 _제로 샷 텍스트_를 고려했다. i_제로 샷_ 설정을 위해 제로 샷 전송 설정에서 사용할 수 있는 추가적인 훈련 데이터가 없기 때문에 추가 접지 헤드가 없는 부록 B.2에 따른 모델을 사전 개질한다. i_scratch_ 및 _제로 샷 텍스트_ 설정에서 부록 C.1에 설명된 모델 구현을 따르고 자기 의도 변압기로부터의 객체 특징에 대해 추가 2층 MLP를 추가한다. 부록 C.1에 설명된 동일한 미세 조정 설정을 따른다.\n' +
      '\n' +
      '다.\n' +
      '\n' +
      '이 섹션에서는 추가 실험 결과를 제공합니다. 구체적으로, 우리는 수집된 SceneVerse를 전통적인 3D 의미 세분화 작업에 대한 사전 훈련 데이터 소스로서 레버리지한다. 다음으로 모델 설계에 대한 남용 분석을 제공합니다.\n' +
      '\n' +
      '### Semantic Segmentation\n' +
      '\n' +
      '스카네버스의 스케일링 효과가 3D 이해 과제에 보편적으로 유익한지 테스트하기 위해 3D 시맨틱 분할을 시그니처 과제로 사용하여 SceneVerse의 효과를 설명한다. 특히, Swin3D 모델[85]을 도입한 최근 연구는 3D 의미 세분화[85]에 대한 사전 훈련의 중요성을 확인하였다. 동일한 설정에 따라 사전 훈련 데이터를 SceneVerse에 대입하여 제안된 Swin3D 모델이 더 개선될 수 있는지 테스트한다. 구체적으로, 우리는 20개의 의미 범주로 ScanNet 의미 세분화 작업에 대한 모델의 성능을 테스트하고 평균 IoU와 평균 Acc를 ScanNet의 검증 세트에 보고한다. 스킨3D 사전 훈련의 원래 구현은 추가 입력으로서 표면 정규화가 필요하기 때문에 모델을 재분류하고 점 좌표와 색상만 있는 모든 모델을 사전 번역한다.\n' +
      '\n' +
      'Tab. A.3에서 볼 수 있듯이, 우리는 스카네버테 데이터셋에서 Swin3D-S 모델을 트레이닝함으로써 상당한 모델 성능 개선(\\(\\ Res\\)6%)을 관찰한다. 구조화된 3D에 대한 사전 훈련 세트를 비교하면, 우리는 또한 일관된 모델 성능 개선을 관찰하여 스켄버스에서 스케일링 효과의 이점을 보여준다. 또한 스켄버스에 대한 사전 학습 후 ScanNet에 대한 모델을 미세 조정했습니다. 이 과정은 의미 세분화에 대한 모델 성능의 개선을 더욱 가져온다. 우리는 이러한 결과가 SceneVerse 및 SceneVerse에서 데이터 스케일링의 효과를 검증하는 강력한 증거의 역할을 한다고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Methods & Init. & SceneVerse Pre. & mIoU & mAcc \\\\ \\hline Swin3D\\({}_{n}\\)-S\\(\\dagger\\) & ✗ & ✗ & 75.2 & - \\\\ Swin3D\\({}_{n}\\)-S\\(\\dagger\\) & S3D & ✗ & 75.6 & - \\\\ \\hline Swin3D-S & ✗ & ✗ & 63.2 & 72.8 \\\\ Swin3D-S & S3D & ✗ & 64.1 & 75.1 \\\\ Swin3D-S (_pre-train_) & ✗ & ✓ & 67.7 & 78.0 \\\\ Swin3D-S (_pre-train_) & S3D & ✓ & 69.5 & 80.1 \\\\ Swin3D-S (_fine-tuned_) & S3D & ✓ & **70.6** & **80.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 A.3: **Santic 세분화 결과는 <스카넷 검증 집합>과 같다. (\\dagger\\)**는 표면 정규화를 추가 입력으로 하는 모델을 나타낸다. S3D는 양 _et al_[85]가 제공하는 구조화된 3D에서 미리 훈련된 원래 Swin3D 모델 가중치로 초기화된 모델을 나타낸다.\n' +
      '\n' +
      '3D 시각적 접지 외에 모든 3D 과제에 대한 잠재적인 이점을 제공합니다.\n' +
      '\n' +
      '1.1 모델 어플레이션 촉매######### 2.1.1 모델 압플레이션 촉매 2.1.1 모델 2##########\n' +
      '\n' +
      '이 절에서는 다단계 대비 정렬 설계에 대한 남용 분석을 제공한다. 우리는 주로 모델의 목표 제거를 학대라고 생각한다. 우리는 추천-객체-레벨 정렬 목적을 기본 설정으로 선택하고 (i) 객체-레벨 정렬 목적, (ii) 마스킹된 언어 모델링 목적, (iii) 장면-레벨 정렬 목적 등 제거를 고려한다. 객체-레벨 정렬 목적을 제거하기 위해, 우리는 객체 포인트 클라우드 인코더의 1단계 사전 학습을 제거하고 추천-객체-레벨 정렬 내에서 이 모듈을 공동으로 학습한다. Tab A.4에서 볼 수 있듯이, 우리는 추가 미세 조정 없이 SceneVerse-val에 대한 다양한 모델 설정을 테스트한다. 먼저, 우리는 장면의 정렬 목표가 \\(\\ Res\\)5% 성능 강하와 함께 SceneVerse-val의 추천 객체 접지에도 중요하다는 것을 보여준다. 객체 수준 정렬(\\(\\ason\\) 2% 드롭) 및 마스킹된 언어 모델링 목표(\\(\\ Res\\) 1.5% 드롭) 없이 학습된 모델에 대해서도 유사한 관찰이 이루어질 수 있다. 이러한 결과는 전체 모델 디자인의 효과를 긍정한다고 생각합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Obj-lvl & MLM & Scene-lvl & Overall & Easy & Hard \\\\ \\hline ✗ & ✗ & ✗ & 64.8 & 75.4 & 48.7 \\\\ ✓ & ✗ & ✗ & 65.2 & 77.1 & 47.4 \\\\ ✓ & ✓ & ✗ & 62.4 & 73.4 & 45.8 \\\\ ✓ & ✓ & ✓ & **66.9** & **77.8** & **50.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 A.4: ** 모델 절제가 SceneVerse-val에 대한 모델 절제****모델 절제에 대해 표.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>