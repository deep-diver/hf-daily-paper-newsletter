<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '확산모델의 시변역산을 이용한 #음악스타일 전이\n' +
      '\n' +
      'Sifei Li\\({}^{1,2}\\), Yuxin Zhang\\({}^{1,2}\\), Fan Tang\\({}^{3}\\), Chongyang Ma\\({}^{4}\\), Weiming Dong\\({}^{1,2}\\)1, Changsheng Xu\\({}^{1,2}\\)\n' +
      '\n' +
      '({}^{1}\\)MAIS, 자동화 연구소, 중국과학원\n' +
      '\n' +
      '중국과학원 인공지능학부\n' +
      '\n' +
      '중국과학원 전산연구소\n' +
      '\n' +
      '\\({}^{4}\\)Kuaishou Technology\n' +
      '\n' +
      '{lisifei2022, Weiming. Dong, changsheng.xu}@ia.ac.cn, tangfan@ict.ac.cn, chongyangm@gmail.com\n' +
      '\n' +
      '각주 1 : 교신저자\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델의 개발과 함께 텍스트 유도 이미지 스타일 전달은 고품질 제어 가능한 합성 결과를 보여주었다. 그러나, 다양한 음악 스타일 전달을 위한 텍스트의 활용은 주로 매칭된 오디오-텍스트 데이터세트의 제한된 가용성으로 인해 중요한 문제를 제기한다. 추상적이고 복잡한 예술 형태인 음악은 동일한 장르 내에서도 변주와 복잡성을 나타내므로 정확한 텍스트 서술이 어렵게 된다. 본 논문은 최소한의 데이터를 이용하여 음악적 속성을 효과적으로 포착하는 음악 스타일 전이 접근 방법을 제시한다. 다양한 수준의 멜-스펙트로그램 특징을 정밀하게 포착할 수 있는 새로운 시변 텍스트 반전 모듈을 소개한다. 추론하는 동안, 우리는 안정적인 결과를 얻기 위해 편향 감소 스타일화 기법을 제안한다. 실험 결과는 본 논문에서 제안한 방법이 특정 악기의 스타일을 전달할 수 있을 뿐만 아니라, 자연음을 결합하여 선율을 구성할 수 있음을 보여준다. 샘플 및 소스 코드는 [https://lsthuhuif.github.io/MusicTI/](https://lsthuhuif.github.io/MusicTI/)에서 사용할 수 있다.\n' +
      '\n' +
      '## Introduction\n' +
      '\n' +
      '그림이 천 마디의 가치가 있다면, 모든 멜로디는 영원할 것이다. 음악은 인간 사회에서 필수적인 예술 형태이며, 음악 양식의 변화는 청취자들에게 완전히 새로운 경험과 인식을 제공할 수 있다. 오랫동안 음악 창작은 진입 장벽이 높았다. 그러나, 음악 양식은 일반 개인들이 개인화된 음악 경험을 성취할 수 있는 가능성을 열어주었다. 음악 스타일 전달은 주어진 오디오 클립의 스타일을 자신의 멜로디를 변경하지 않고 다른 것으로 전달하는 과정을 의미한다. 사운드는 우리 삶에 편재하기 때문에 작곡1에서 자연음을 활용하는 음악 창작자에게 영감을 받아 다양한 유형의 사운드 사례를 포괄할 수 있도록 음악 양식 전이가 확장될 수 있다.\n' +
      '\n' +
      '각주 1: 자연음이 음악 제작에 어떻게 관여할 수 있는지는 [https://youtu.be/xixiesRtgKU?list=RDxixiiesRtgKU?list=RDxixiiesRtgKU](https://youtu.be/xixiesRtgKU?list=RDxixiiesRtgKU); [https://theworld.org/stories/2021-03-14/nature-always-singing-now-you-make-own-music-natures-](https://theworld.org/stories/2021-03-14/nature-always-singing-now-you-music-natures-](https://youtu.be/xixiesRtgKU?list=RDxixiiesRtgKU); [https://theworld.org/stories/2021-03-14/nature-always-singing-now-you-music-natures-\n' +
      '\n' +
      '딥러닝 기반 음악 스타일 전이는 최근 몇 년 동안 뜨거운 연구 주제였다. 일부 작품[14, 15, 16]은 특정 음색을 가진 음악을 특정 악기 또는 몇 개의 악기로 스타일화할 수 있는 반면, 다른 작품[17, 18, 19, 20]은 다대다 음악 스타일 전이를 달성했지만 훈련 데이터에 제시된 유한한 스타일의 집합으로 변환을 제한한다. 원샷 음형의 전이를 탐구하려는 노력[15, 14]이 있지만 여전히 자연음을 다루는 데 어려움을 겪고 있다. 대형 언어 모델의 발전으로 일부 작품[16, 15, 17, 18]은 텍스트 유도 음악 생성을 탐색하고 인상적인 결과를 생성하는 놀라운 능력을 보여준다. 특히 MusicLM[19]은 텍스트적 표상과 선율적 표상 모두에서 컨디셔닝을 통해 음형의 전이를 구현한다. 그러나, 기존의 방법들은 장르들(예를 들어, "록", "재즈"), 악기들(예를 들어, "피아노", "기타", "바이올린"), 또는 연주 형태들(예를 들어, "코러스", "현악 사중주")에 대한 거친 설명들에 기초하여 단지 공통 스타일 전이를 달성할 수 있다. 그들은 코넷이나 에르후와 같은 틈새 기구를 다룰 능력이 부족하다. 더욱이, 이러한 방법들은 자연음에 대한 설명 또는 합성된 오디오 효과들과 관련된 복잡한 시나리오들을 다루기에 불충분하다.\n' +
      '\n' +
      '위의 모든 문제를 완화하고 사전 훈련된 대규모 모델의 생성 능력을 활용하기 위해 새로운 예제 유도 음악 스타일화 방법을 제안한다. 우리의 접근 방식은 악기, 자연음 및 합성 효과음을 포괄하는 임의의 예를 기반으로 음형의 전이를 달성하는 것을 목표로 한다. 오디오 클립이 주어지면, 그 스타일을 콘텐츠로 사용되는 임의의 입력 음악으로 전달할 수 있다. 도 1에 예시된 바와 같이, 본 방법은 콘텐츠 멜-스펙트로그램의 구조를 보존하면서, 스타일 멜-스펙트로그램의 텍스처를 콘텐츠 멜-스펙트로그램의 로컬 영역들로 전달할 수 있다.\n' +
      '\n' +
      '이러한 목적을 달성하기 위해, 우리는 입력 오디오의 효과적인 스타일 표현을 얻고자 한다. 대상 이미지의 재구성을 통해 특정 개념을 표현하기 위해 의사 단어를 활용하는 Textual Inversion[19]에서 영감을 받아 스타일 오디오를 유사한 방식으로 표현하는 의사 단어를 학습하는 것을 목표로 한다. 그러나 스타일화 과정에서 스타일 오디오의 내용을 도입하는 것을 피할 수 있기를 기대합니다. 우리는 확산 모델의 다른 타임스테프가 다른 수준의 특징에 초점을 맞춘다고 가정한다. 따라서 본 논문에서는 타임스텝이 증가함에 따라 텍스트 임베딩의 강조가 텍스쳐에서 스타일 멜-스펙트로그램의 구조로 이동하는 시변 텍스트 반전 모듈을 제안한다. 또한, 일부 잡음이 있는 멜로그램을 콘텐츠 안내로 사용한다. 그 결과, DDIM의 실행에서 의사 단어를 안내로서 사용하는 경우[23], 부분적인 잡음 제거 처리가 된다. 이 체계는 선율이나 리듬과 연관된 구조 관련 타임스테프들이 양식화 과정에 참여하는 것을 자연스럽게 배제한다. 한편 내용 멜스펙트로그램의 선율이나 리듬을 보존하고 있다. 콘텐츠 보존에 대한 확산 모델의 편향을 줄이기 위해 랜덤 노이즈 대신 예측된 노이즈를 사용하여 멜 스펙트로그램에 노이즈를 추가하여 보다 안정적인 스타일링 결과를 도출한다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약할 수 있다:\n' +
      '\n' +
      '* 시변 텍스트 역산을 이용한 새로운 예제 기반 음악 전달 방법을 제안한다.\n' +
      '* 우리의 접근법은 음악 스타일 전이를 위한 비음악 오디오의 사용을 가능하게 하고 매우 창의적인 결과를 달성한다.\n' +
      '* 실험 결과는 우리의 방법이 정성적 평가 및 정량적 평가 모두에서 기존의 접근법보다 우수함을 입증한다.\n' +
      '\n' +
      '## Related Work\n' +
      '\n' +
      '**음악 스타일 전달.** 딥러닝 기반 음악 스타일 전달은 음악 생성의 대표적인 메커니즘으로 널리 연구되어 왔다. Dai, Zhang, Xia[2018]는 음악 양식 전이의 개념을 탐색하고 그 전개 양상을 분석한다. 많은 작품들이 다양한 딥러닝 프레임워크[1, 19, 20, 18, 17, 16, 19, 21, 22]를 사용하여 음악 양식에 대한 추가 연구를 수행했다. TimbreTron[16]은 이미지 스타일 전송 기술을 사용하여 여러 스타일에 걸친 음색 전송을 달성합니다. Grinstein et al. [2018]은 오디오 스타일들의 CNN-추출된 통계적 특징들에 기초하여 임의의 오디오들 사이의 음색 전달을 탐색한다. Groove2Groove[14]는 심볼릭 음악에 대한 원샷 스타일 전달을 달성하기 위해 인코더-디코더 구조를 채택한다. Cifka et al. [2021]은 훈련 데이터에 제한되지 않고 원샷 음악 스타일 전송을 위해 벡터 양자화 가변 오토인코더(VQ-VAE)를 채용하여, 실세계 데이터에서도 좋은 성능을 산출한다. 음악-STAR[19]는 멀티 트랙 곡들 간의 스타일 전이를 탐색하지만, 특정 악기에 국한된다. Bonnici, Benning, Saitis[2022]는 변량 오토인코더를 사용한다.\n' +
      '\n' +
      '그림 1: 우리의 방법을 이용한 음악 스타일 이전 결과. 우리의 접근법은 다양한 멜-스펙트로그램(예를 들어, 악기, 자연 사운드, 합성 사운드)의 스타일을 5초 클립만큼 작은 참조 데이터를 사용하여 콘텐츠 멜-스펙트로그램으로 정확하게 전달할 수 있다. 스타일 멜 스펙트로그램에서 블랙박스는 두드러진 질감을 가진 영역을 강조합니다. 파란색 상자에서 스타일 전이 결과가 내용 멜-스펙트로그램과 유사한 구조를 유지하면서 스타일 멜-스펙트로그램과 유사한 질감을 나타내는 것을 관찰할 수 있다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'timestep이 증가하면, 텍스트 임베딩은 텍스처보다는 구조에 점점 더 초점을 맞춘다. 따라서 확산 모델의 더 작은 시간 단계에서 텍스트 임베딩을 스타일의 표현으로 다룰 수 있다.\n' +
      '\n' +
      '구체적으로 시간 가변 인코더(time-varying encoder, TVE) 모듈에 타임스텝(timeestep)\\(t\\)을 공급한다. 타임스테프는 먼저 \\(t_{e}\\)로 임베딩된다. 그 위에 여러 개의 선형 레이어를 수행한 후, 출력을 초기 임베딩 \\(v_{os}\\)에 \\(v^{0}\\)으로 추가한 다음, 여러 주의 모듈을 거쳐 최종 임베딩 \\(v_{i*}\\)을 유도한다. 다중 어텐션 모듈은 \\(v^{0}\\)으로 시작하여, 각각의 어텐션 레이어는 다음과 같이 구현된다:\n' +
      '\n' +
      '[Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d}})\\cdot V.\\tag{1}\\]\n' +
      '\n' +
      'Self attention layer에 대해 \\(Q^{s},K^{s},V^{s}\\)는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[M^{s}=W_{M^{s}}\\cdot v^{0}, \\tag{2}\\]\n' +
      '\n' +
      '그림 3: 우리의 시변 텍스트 반전 모듈은 텍스트 임베딩의 시단계 차원을 확장한다. 스타일 멜-스펙트로그램을 재구성할 때 텍스트 임베딩은 시간 단계 차원에서 차별성을 나타낸다. 시간 단계가 증가함에 따라 텍스트 임베딩의 초점은 텍스처에서 구조로 이동한다.\n' +
      '\n' +
      '그림 2: 우리 방법의 개요. 본 논문에서는 Riffusion(Forsgren and Martiros 2022)을 백본망으로 채택하고, 우측과 같이 주로 시변 인코더(time-varying encoder, TVE)로 구성된 시변 텍스트 반전 모듈을 제안한다. Timestep \\(t_{e}\\)에 대해 여러 선형 레이어를 수행한 후 초기 임베딩 \\(v_{os}\\)에 출력을 추가하면 TVE는 여러 주의 모듈을 통해 최종 임베딩 \\(v_{i*}\\)을 제공한다. (M_{s}\\), \\(hat{M}_{s}\\), \\(M_{c}\\), \\(M_{cn}\\), \\(\\hat{z}_{t_{p}\\), \\(\\hat{M}_{cn}\\), \\(\\hat{M}_{cn}\\), \\(\\hat{M}_{cs}\\), \\(\\hat{M}_{cn}\\), \\(\\hat{M}_{cs}\\)은 각각 스타일 멜-스펙트로그램, 재구성 스타일 멜-스펙트로그램, 콘텐츠 멜-스펙트로그램, 잡음 콘텐츠 멜-스펙트로그램, 예측 잡음, 예측 잡음 콘텐츠 멜-스펙트로그램 및 스타일화된 멜-스펙트로그램을 나타낸다.\n' +
      '\n' +
      '여기서 \\(M^{s}\\)은 \\(\\{Q^{s},K^{s},V^{s}\\}\\)에서 나올 수 있다. 크로스 어텐션 레이어에 대해, \\(Q^{c},K^{c},V^{c}\\)는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[Q^{c}=W_{Q^{c}}\\cdot v^{1},M^{c}=W_{M^{c}}\\cdot v^{0}, \\tag{3}\\]\n' +
      '\n' +
      '\\[v^{1}=Attention(Q^{s},K^{s},V^{s}), \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(M^{c}\\)는 \\(\\{K^{c},V^{c}\\}\\)로부터 나올 수 있다.\n' +
      '\n' +
      '최종 임베딩 \\(v_{i*}\\)은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[v_{i*}=Attention(Q^{c},K^{c},V^{c}). \\tag{5}\\]\n' +
      '\n' +
      '문자 변환을 수행하여 \\(v_{i}\\)을 LDM을 안내하는 조건으로 변환한다. 제안된 개선된 텍스트 인코더 \\(e\\)는 CLIP [1] 텍스트 인코더와 TVE를 통합하여 구성된다. LDM의 손실을 기반으로, 우리의 최적화 목표는 다음과 같이 정의된다:\n' +
      '\n' +
      'v_{i*}=\\operatorname*{arg\\,min}_{v}\\mathbb{E}_{z,y,\\epsilon,t}[\\|\\epsilon-\\epsilon_{\\theta}(z_{t},t,e_{\\theta}(y,t))\\|_{2}^{2}], \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(z\\sim E(x),\\epsilon\\sim\\mathcal{N}(0,1)\\), \\(\\epsilon_{\\theta}\\) 및 \\(e_{\\theta}\\)의 CLIP 텍스트 인코더는 대규모 사전 훈련 모델의 성능을 유지하기 위해 훈련 중에 동결된다.\n' +
      '\n' +
      '### Bias-Reduced Stylization\n' +
      '\n' +
      '확산 모델의 경우 잡음이 있는 이미지에서 실제 이미지로 노이즈 제거 과정에서 타임스텝이 감소함에 따라 초기 1차 구조가 확립되고 세부 사항이 점진적으로 개선되는 것을 관찰한다. 우리는 콘텐츠 안내를 달성하기 위해 양식화 동안 강도 메커니즘을 사용한다.\n' +
      '\n' +
      '바이어스-감소된 양식화는 부분 확산 프로세스, 결정된 확산 프로세스 및 잡음 제거 프로세스를 포함한다(도 2 참조). 부분 확산 과정은 시간-단계가 \\(t_{p}\\)에 도달할 때까지 내용 멜-스펙트로그램 \\(M_{c}\\)에 노이즈를 추가하는 것을 의미하며, 여기서 \\(t_{p}=T\\cdot 강도\\) 및 \\(M_{c}\\)이 잡음 멜-스펙트로그램 \\(M_{cn}\\)으로 변환된다. 결정된 확산 과정은 \\(M_{cn}\\)에 대해 단일 단계 디노이징을 수행하며, 여기서 예측 잡음 \\(\\hat{z}_{t_{p}}\\)은 확산 과정을 수행할 때 랜덤 잡음을 대체하기 위해 사용되어 새로운 잡음 함량 멜-스펙트로그램 \\(\\hat{M}_{cn}\\)을 생성한다. 이 과정은 모델 바이어스의 영향을 상쇄하기 위해 노이즈 이미지에 바이어스를 도입하는 것으로 볼 수 있다. 디노이징 과정은 간단한 프롬프트 "\\(*\\)"로 DDIM[23]에 의해 \\(\\hat{M}_{cn}\\)을 \\(\\hat{M}_{cs}\\)으로 점진적으로 변환된다. 확산 프로세스 및 잡음 제거 프로세스 둘 다는 VAE 인코더의 잠재 공간에서 수행된다는 것에 유의한다. 잡음 제거된 출력은 VAE 디코더에 의한 Mel-spectrogram으로의 디코딩을 필요로 하며, 이는 후속적으로 그리핀-Lim 알고리즘을 사용하여 오디오로 재구성될 수 있다.\n' +
      '\n' +
      '## Experiement\n' +
      '\n' +
      '콘텐츠 보존과 스타일 적합 모두에서 좋은 성능을 보이는 방법의 효과를 입증하기 위해 정성적 평가, 정량적 평가 및 절제 연구를 수행했다.\n' +
      '\n' +
      '### Dataset.\n' +
      '\n' +
      '현재 우리의 요구 사항을 충족하는 음악 스타일 이전을 위해 특별히 맞춤화된 공개적으로 사용할 수 있는 데이터 세트가 부족하다. 우리는 모든 콘텐츠가 무료인 웹사이트([https://pixabay.com](https://pixabay.com))에서 소규모 데이터 세트를 수집했다. 수집된 데이터는 5초 클립으로 분할되어 총 253개의 5초 클립이 생성되었으며, 74개의 스타일 클립과 179개의 콘텐츠 클립이 생성되었다. 스타일 서브세트는 악기, 자연 사운드 및 합성 사운드 효과를 포함하여 18개의 서로 다른 스타일 오디오로 구성된다. 콘텐츠 서브세트는 전자 음악 및 악기 클립으로 구성되어, 주로 단순 단조 오디오를 사용하는 다른 음악 스타일 전달 접근법과 구별된다. 실험에서 우리는 모든 스타일 오디오 클립을 사용하지 않았다. 대신 자연음과 합성음 효과별로 하나의 샘플만을 선별하였습니다. 악기 음표의 가변성을 고려하여 악기별로 3~5개의 클립을 사용하였다.\n' +
      '\n' +
      '우리는 우리의 방법을 세 가지 관련 최첨단 접근법과 비교했다:\n' +
      '\n' +
      '* R+TI: Riffusion(R)[11]과 Textual Inversion(TI)[1]을 기준선으로 결합하였다. R은 원래 안정적인 확산 모델 v1.5로 텍스트와 쌍을 이루는 멜-스펙트로그램의 이미지에 대해 미세 조정된다. 또한 오디오와 멜-스펙트로그램 간의 변환을 위한 변환 라이브러리를 통합한다. TI는 최적화 기반 접근법을 사용하여 제한된 수의 이미지 내에서 개념에 대한 의사 단어를 학습하는 고전적인 방법이다.\n' +
      '* SS VQ-VAE[12]: 원샷 음악 스타일 전이의 최신 이용 가능한 구현.\n' +
      '* MUSICGEN[13] : 멜로디 컨디셔닝으로 텍스트 유도 음악 양식화를 달성하는 최근 출시된 텍스트 유도 음악 생성 방법.\n' +
      '\n' +
      '### Implementation details.\n' +
      '\n' +
      '실험에서는 TVE 모듈을 제외한 LDM과 텍스트 인코더의 파라미터를 수정한다. LDM의 기본 하이퍼파라미터를 사용하여 기본 학습률을 0.001로 설정하였으며, 각 스타일에 대한 학습 과정은 배치 크기가 1인 NVIDIA GeForce RTX3090을 사용하여 약 30분, TI에 필요한 60분 이상 소요된다. 추론하는 동안, 우리의 접근법은 \\(strength\\)과 \\(scale\\)의 두 개의 하이퍼파라미터를 사용한다. 이러한 매개변수는 각각 콘텐츠의 강도를 제어하고 스타일의 강도를 조절합니다. 강도 범위는 0.6~0.7, 규모 범위는 3.0~5.0일 때 가장 좋은 결과를 얻었다.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      '우리의 방법과 다른 접근법 사이의 비교를 보여주는 스타일화된 오디오 샘플은 보충 자료 내에 제공된 정적 웹 페이지에서 액세스할 수 있다. 그림 4와 같이 R+TI[11, 12, 13], SS VQ-VAE[12], MUSICGEN[13]의 세 가지 접근법과 방법을 비교했다. 멜 스펙트로그램의 구조는 내용으로 볼 수 있고, 세부적인 질감은 스타일로 볼 수 있다.\n' +
      '\n' +
      'R+TI의 경우 부분 잡음이 있는 콘텐츠 멜-스펙트로그램을 콘텐츠 안내로 취급하고 학습한 의사 단어를 DDIM을 사용하여 스타일 전달을 위한 텍스트 안내로 사용했다. R+TI는 전체 구조를 잘 보존하지만 국부적인 수준에서 리듬에 가끔 결함을 도입하고 우리의 방법에 비해 약한 질감 전달을 나타냄을 관찰할 수 있다. SS VQ-VAE는 16kHz의 샘플링 속도로 오디오를 처리하므로 스타일링 후 고주파 정보가 손실된다. 멜-스펙트로그램에 심각한 아티팩트를 도입하여 오디오 품질 측면에서 성능이 좋지 않습니다. MUSICGEN과 관련하여 우리는 스타일 이전을 위한 지침으로 스타일 오디오에 대한 텍스트 설명을 사용했다. 결과는 생성 품질이 불안정한 콘텐츠 보존과 제한된 편집성을 특징으로 하는 높은 확률성을 나타냄을 나타낸다. 본 논문에서 제안하는 방법은 다른 방법들에서 관찰되는 아티팩트를 도입하지 않고도, 스타일 멜-스펙트로그램의 고품질 질감 전달을 달성하면서 콘텐츠 멜-스펙트로그램의 구조를 정확하게 보존할 수 있다.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      '음악의 이전 작품[16, 17]에 이어, (a) 콘텐츠 보존과 (b) 스타일 적합의 두 가지 기준에 따라 방법을 평가한다. MUSICGEN[18]과 InST[16]에서 영감을 받아 생성된 멜-스펙트로그램과 콘텐츠 멜-스펙트로그램 간의 CLAP 코사인 유사도를 계산하여 콘텐츠 보존성을 평가한다. 또한 생성된 멜-스펙트로그램과 스타일에 대한 해당 텍스트 설명 간의 CLAP 코사인 유사성을 계산하여 스타일 적합성을 평가한다. 텍스트 서술과 스타일 멜-스펙트로그램 사이의 CLAP 코사인 유사도를 기준으로 계산하였으며, 평균값은 0.4890, 최소값은 0.3424로, 텍스트 서술이 어려운 스타일 오디오는 객관적 메트릭 계산에서 제외하였다. 이것은 우리의 스타일 멜-스펙트로그램과 평가 텍스트 사이의 상관 관계를 보장한다. 표 1과 같이 282개의 콘텐츠 스타일 쌍을 무작위로 선택하고 성능을 평가하여 본 방법 및 다른 접근 방식을 평가했으며, 두 메트릭 모두에서 가장 우수한 성능을 달성하여 콘텐츠 보존 측면에서 기준선을 크게 초과했다. SS VQ-VAE는 우리와 유사한 스타일 핏을 달성하지만, 더 큰 콘텐츠 손실을 겪는다. 뮤직젠이 눈에 띄게\n' +
      '\n' +
      '그림 4: 최첨단 방법[14, 16, 15, 16]과의 질적 비교. (a) Style mel-spectrograms, 왼쪽에 있는 텍스트들은 소리 범주들이다. (b) Mel-spectrogram. (c)-(d) 다양한 방법의 양식화된 결과. 스타일 멜 스펙트로그램에서 블랙박스는 두드러진 질감을 가진 영역을 강조합니다. 파란색 상자에서 우리의 결과만이 내용 멜-스펙트로그램과 유사한 구조를 보존하면서 스타일 멜-스펙트로그램과 유사한 질감을 나타내는 것을 관찰할 수 있다.\n' +
      '\n' +
      '두 측정 기준 모두에서 우리의 방법보다 더 나쁘다.\n' +
      '\n' +
      '사용자 연구.우리의 방법의 성능을 주관적인 평가를 수행하기 위해 세 가지 평가 메트릭에 대해 네 가지 방법을 평가하는 사용자 연구를 설계했다. 우리는 무작위로 15개의 결과 세트(텍스트로 설명하기 어려운 스타일 오디오에 대해 MU-SICGEN [12]와의 비교를 제외)를 선택했다. 테스트 전에 참가자의 음악 직업 수준을 평가하기 위한 질문을 설정하고 음악 양식에 대한 평가 기준을 설명하는 지침을 제공했다. 테스트 동안 각 참가자는 각 질문 세트에 대해 스타일 오디오, 콘텐츠 오디오 및 무작위로 정렬된 4개의 생성 결과를 제시했다. 참가자들은 다음 메트릭을 1(가장 낮음)에서 5(가장 높음)의 척도로 평가하도록 요청받았다:\n' +
      '\n' +
      '* 컨텐츠 보존: 멜로디, 리듬, 유사 속성의 관점에서 생성된 오디오와 컨텐츠 음악 간의 일관성.\n' +
      '* Style Fit: 생성된 오디오와 스타일 오디오 간의 음색, 사운드 단위 및 유사한 속성 측면에서 일치성.\n' +
      '* 전체적인 품질: 생성된 음악의 내용과 스타일 간의 융합의 일관성 등 스타일 전이의 전반적인 성능과 관련된 품질.\n' +
      '\n' +
      '우리의 실험에는 80명의 참가자가 참여했으며 그 중 72명이 유효한 것으로 간주(음악에 대한 지식이 없는 참가자를 제외)되어 총 12960개의 평점이 나왔다. 최대값과 최소값을 제외한 후 참가자의 음악 직업 수준(해당 가중치를 갖는 4단계: 1~4)을 기준으로 가중 평균을 계산했다. 표 1에 제시된 결과는 우리의 방법이 콘텐츠 보존, 스타일 적합성 및 전반적인 품질 측면에서 다른 접근 방식보다 훨씬 우수하다는 것을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '시변 임베딩(Time-varying embedding, TVE) 추론 시 특정 시간 단계에서 의사 단어의 텍스트 임베딩을 고정하고 그림 3과 같이 멜-스펙트로그램 생성을 위한 텍스트 안내로 사용한다. 타임스텝이 증가함에 따라 텍스트 임베딩은 멜-스펙트로그램의 질감에서 구조로 점차 초점이 옮겨간다. 이는 확산 모델이 먼저 노이즈 제거 동안 이미지의 거친 구조를 구성한 다음 세부 사항을 최적화한다는 우리의 예상과 일치한다. 재구성된 결과는 서로 다른 타임스텝에 걸친 특징의 융합으로 인해 고품질 재구성을 반영한다. TVE 모듈의 효과를 더 입증하기 위해 표 2와 같이 사용하지 않은 방법을 평가한다. TVE를 제거한 후 콘텐츠 보존의 차이는 크지 않지만 스타일 적합도가 눈에 띄게 감소하여 TVE가 더 나은 스타일 학습에 기여함을 나타낸다.\n' +
      '\n' +
      '편향 감소 스타일화.편향 감소 스타일화 기법을 제거하는 것이 콘텐츠 보존 및 스타일 매칭에 미치는 영향을 평가한다. 두 지표 모두 감소하는 것을 관찰할 수 있어 콘텐츠 보존과 스타일 전이의 용이성 측면에서 도움이 된다는 것을 알 수 있다.\n' +
      '\n' +
      '### 토론과 한계\n' +
      '\n' +
      '이 방법은 악기, 자연음, 합성음 효과 등 다양한 음원을 사용하여 음형을 전달할 수 있다. 그럼에도 불구하고 특정 상황에서 특정 제한이 발생할 수 있음을 인식하는 것이 중요하다. 예를 들어, 콘텐츠 음악이 다수의 컴포넌트를 포괄하는 경우, 우리의 방법은 각각의 개별 컴포넌트에 대해 스타일 전달을 정확하게 수행하는 데 어려움을 겪을 수 있고, 잠재적으로 부분적인 콘텐츠 손실을 초래할 수 있다. 또한, 스타일 오디오가 비나 바람 소리와 같은 백색 소음을 포함할 때, 그 요소들 내에 내재된 음악성을 포착하여 콘텐츠 참조에 효과적으로 전달하는 것이 어려워진다.\n' +
      '\n' +
      '## Conclusion\n' +
      '\n' +
      '본 논문에서는 스타일 멜-스펙트로그램을 효과적으로 임베딩할 수 있는 확산 모델과 시변 텍스트 역산에 기반한 새로운 음악 스타일화 방법을 제안한다. 실험은 악기, 자연 사운드 및 합성 사운드 효과를 포함한 다양한 유형의 오디오에 대한 방법의 일반성을 보여준다. 우리의 접근 방식은 적은 양의 데이터로 스타일 전송을 달성하여 매우 창의적인 음악을 생성합니다. 비음악 스타일 오디오에 적용하여도 높은 수준의 음악성을 가진 결과를 얻을 수 있다. 우리는 더 강력한 생성 기능을 가진 사전 훈련된 모델을 활용하는 것이 우리 방법의 성능을 더욱 향상시킬 것이라고 믿는다. 향후, 우리는 보다 해석 가능하고 속성이 얽힌 음악 양식 전이를 조사하는 것을 목표로 한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 작업은 중국 국립 자연 과학 재단의 지원을 받지 않았다. 61832016 및 62102162.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\hline  & \\multicolumn{2}{c|}{Objective} & \\multicolumn{2}{c}{Subjective} \\\\ Method & CP & SF & CP & SF & OVL \\\\ \\hline R+TI & 0.3481 & 0.2722 & 2.81 & 3.20 & 2.75 \\\\ SS VQ-VAE & 0.2351 & 0.2809 & 3.36 & 2.34 & 2.60 \\\\ MUSICGEN & 0.2808 & 0.2370 & 2.81 & 2.70 & 2.83 \\\\\n' +
      '**Ours** & **0.4645** & **0.2816** & **3.91** & **3.70** & **3.66** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다른 방법[12, 13, 14]과의 질적 비교. CP, SF, OVL은 각각 콘텐츠 보존, 스타일 핏 및 전체 품질을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline  & Content Preservation & Style Fit \\\\ \\hline w/o TVE & 0.4506 & 0.2418 \\\\ w/o BRS & 0.4415 & 0.2602 \\\\\n' +
      '**Ours** & **0.4645** & **0.2816** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 우리 방법의 절제 연구. TVE와 BR은 각각 시변 임베딩과 편향 축소 스타일화이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Agostinelli, T. I. Denk, Z. 보로소, J.엔젤 버제티, A. 케이용, Q. 황 A. 잰슨 A. 로버츠 M. Tagliasacchi, et al. (2023)MusicLM: 텍스트로부터 음악을 생성한다. ArXiv:2301.11325. 인용: SS1.\n' +
      '* M. 알리누리와 V. Tzerpos(2022)Music-STAR: 오디오 기반 재악기를 위한 스타일 번역 시스템. International Society for Music Information Retrieval Conference (ISMIR)에서 pp. 419-426. Cited by: SS1.\n' +
      '* A. Bitton, P. Esling, and A. Chemla-Romeu-Santos (2018)Modulated Variational Auto-encoder for many-to-many musical timbre transfer. ArXiv:1810.00222. 인용: SS1.\n' +
      '* R. S. Bonnici, M. Benning, and C. Saitis (2022)Timbre transfer with variational auto encoding and cycle-consistent adversarial networks. IJCNN(International Joint Conference on Neural Networks)에서 pp. 1-8. Cited by: SS1.\n' +
      '*Z. 보로소 마리니에, D. 빈센트, E. 하리토노프, O. 피에틴 샤리피, D. 로블렉, O. 테불, D. 그랑지에, M. Tagliasacchi, et al. (2023)Audio: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, Language Processing. 인용: SS1.\n' +
      '* G. Brunner, A. Konrad, Y. 왕, R. Wattenhofer (2018)MIDI-VAE: 모델링 역학 및 스타일 전이에 적용되는 음악의 악기화. International Society for Music Information Retrieval Conference (ISMIR)에서 pp. 747-754. Cited by: SS1.\n' +
      '*Y. 장원 첸과 M. 후(2021) 준감독 다대다 음악 음색 전환. International Conference on Multimedia Retrieval (ICMR)에서 pp. 442-446. Cited by: SS1.\n' +
      '* J. Choi and K. Lee(2023)Pop2Piano: 팝 오디오 기반 피아노 커버 세대. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS1.\n' +
      '*O. Cifka, A. Ozerov, U. Simsekli, and G. Richard (2021) Self-supervised vq-vae for one-shot music style transfer. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 96-100. Cited by: SS1.\n' +
      '*O. 시프카, U. 우제로프 Simsekli, and G. Richard(2020)Groove2Groove: 합성 데이터의 감독과 함께 원샷 음악 스타일 이전. IEEE/ACM Transactions on Audio, Speech, and Language Processing28, pp. 2638-2650. Cited by: SS1.\n' +
      '* J.코펫, F.크룩, I.갓, T. 레메즈, D. 칸트, G. 신내브, Y. Adi, and A. Defossez (2023)Simple and controllable music generation. ArXiv:2306.05284. 인용: SS1.\n' +
      '* S. 다이자 Zhang, and G. G. Xia(2018)Music style transfer: a position paper. ArXiv:1803.06841. 인용: SS1.\n' +
      '* 실시간 음악 생성을 위한 안정적인 확산. 참고: [https://rifrfusion.com/aboutAccessed](https://rifrfusion.com/aboutAccessed): 2022-12-31 Cited by: SS1.\n' +
      '*R. 갈영 알라루프 아츠몬, 오 Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023) 이미지는 텍스트 역산을 사용하여 텍스트 대 이미지 생성을 개인화하는 하나의 단어 가치가 있다. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '*R. 갈민 알로프 아츠몬, 오 Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023) 이미지는 텍스트 역산을 사용하여 텍스트 대 이미지 생성을 개인화하는 하나의 단어 가치가 있다. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '*R. 갈민 아라프 Atzmon, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023)Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics42(4), pp. 150:1-150:13. Cited by: SS1.\n' +
      '* D. Ghosal, N. Majumder, A. Mehrish, S. Poria (2023) Text-to-audio 생성은 명령어 튜닝 LLM과 잠재 확산 모델을 이용하였다. ArXiv:2304.13731. 인용: SS1.\n' +
      '* E. Grinstein, N. Q. Duong, A. Ozerov, and P. Perez(2018)Audio style transfer. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 586-590. Cited by: SS1.\n' +
      '* M. 곽창록 류민 쳉과 S. Hu(2023)Visual attention network. Computational Visual Media9 (4), pp. 733-752. Cited by: SS1.\n' +
      '*Q. 황아잔센 Ganti, J. Y. Li, and D. P. Ellis(2022)MuLan: joint embedding of music audio and natural language. International Society for Music Information Retrieval Conference (ISMIR)에서 pp. 559-566. Cited by: SS1.\n' +
      '*Q. 황동석 왕태일 덴크 천진 Zhang, J. Yu, C. Frank, et al.(2023)Noise2Music: text-conditioned music generation with diffusion models. ArXiv:2302.03917. 인용: SS1.\n' +
      '*R. 황재황 렌락 류민 이종욱 예준우 음, Z Zhao(2023)Make-an-audio: 신속한 확산 모델을 가진 텍스트 대 오디오 생성. ICML(International Conference on Machine Learning)에서 인용된 SS1.\n' +
      '* S. 황규 이찬일 배승호 Oore, and R. B. Grosse(2019)TimbreTron: a wavelet(CycleGAN(CQT(Audio))) Pipeline for musical timbre transfer. ICLR(International Conference on Learning Representations)에서 인용된 SS1.\n' +
      '*Z. 황태 우영 장경찬, 지 Liu(2023)ReVersion: 이미지로부터의 확산 기반 관계 반전. ArXiv:2303.13495. 인용: SS1.\n' +
      '* D. K. Jain, A. Kumar, L. 채승 싱할과 브이 Kumar(2020)ATT: attention-based timbre transfer. 2020년 IJCNN(International Joint Conference on Neural Networks)에서 pp. 1-6. Cited by: SS1.\n' +
      '* S. 이준호 후필성 칸 허영 Wang, and J. Yang(2023)StyleDiffusion: text-based editing을 위한 prompt-embedding inversion. ArXiv:2303.15649. 인용: SS1.\n' +
      '* H. Liu, Z. 천영 위안, 엑스 메익 류덕만딕 Wang, and M. D. Plumbley (2023)Audioldm: text-to-audio generation with latent diffusion models. ArXiv:2301.12503. 인용: SS1.\n' +
      '* C. Lu, M. 서창, 이창, 그리고 L. 수(2019) 원하는 대로 연주: 음색 강화 멀티모달 음악 스타일 이전. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 1061-1068. Cited by: SS1.\n' +
      '* W. T. Lu, L. 수와 A. E. E.\n' +
      '\n' +
      'Mor, N.; 울프, L Polyak, A.; and Taigman, Y. 2019. 유니버설 음악 번역 네트워크. In _International Conference on Learning Representations (ICLR)_.\n' +
      '*[Ouyang et al.2022] Ouyang, L.; 우, J.; 장, X.; 알메이다, D.; 웨인라이트, C.; 미쉬킨, P.; 장, C.; 아가왈, S.; 슬라마, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 35, 27730-27744.\n' +
      '* [Radford et al.2021] Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 8748-8763. PMLR.\n' +
      '* [Rombach et al.2022] Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 10684-10695.\n' +
      '*[Schneider2023] Schneider, F. 2023. Archisound: diffusion을 갖는 Audio generation. _ arXiv preprint arXiv:2301.13267_.\n' +
      '* [Schneider, Jin, and Scholkopf2023] Schneider, F.; Jin, Z.; and Scholkopf, B. 2023. Mousai: Text-to-Music Generation with Long-Context Latent Diffusion. _ arXiv preprint arXiv:2301.11757_.\n' +
      '* [Song, Meng, and Ermon2020] Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. _ arXiv preprint arXiv:2010.02502_.\n' +
      '*[Tewel et al.2023] Tewel, Y.; Gal, R.; Chechik, G.; and Atzmon, Y. 2023. 텍스트-이미지 개인화를 위한 키 잠금 순위 1 편집. In _ACM SIGGRAPH 2023 Conference Proceedings_, 12:1-12:11. New York, NY, USA: Association for Computing Machinery.\n' +
      '*[Voynov et al.2023] Voynov, A.; Chu, Q.; Cohen-Or, D; and Aberman, K. 2023. \\(P+\\): 텍스트-이미지 생성에서의 확장된 텍스트 컨디셔닝 _ arXiv preprint arXiv:2303.09522_.\n' +
      '*[Wu and Yang2023] Wu, S. - L; 및 양용 -H. 2023. 뮤즈모포즈 : 한 대의 트랜스포머 VAE를 이용한 풀송 및 세립 피아노 음악 스타일 변환 IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31:1953-1967.\n' +
      '* [Wu 외.2023a] Wu, Y.; 천경 장태 희영 버그 커크패트릭, T. and Dubnov, S. 2023a. 특징 융합과 키워드-캡션 증강을 이용한 대규모 대조 언어-오디오 사전 훈련. In _IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 1-5. IEEE.\n' +
      '* [Wu et al.2023b] Wu, Y.; 허영 유, X; 왕영 그리고 Dannenberg, R. B. 2023b. 트랜스플레이어: 유연한 팀브 컨트롤이 있는 팀브 스타일 트랜스퍼입니다. In _IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 1-5. IEEE.\n' +
      '*[Zhang et al.2023a] Zhang, Y.; 동원 탕, F.; 황, N.; 황환환 마찬환 이태 - Y; Deussen, O.; 및 Xu, C. 2023a. Propect: 확산 모델의 속성 인식 개인화를 위한 프롬프트 스펙트럼__ ACM Transactions on Graphics_, 42(6):244:1-244:14.\n' +
      '*[Zhang et al.2023b] Zhang, Y.; 황남 Tang, F.; Huang, H.; Ma, C.; Dong, W.; 및 Xu, C. 2023b. 확산 모델을 사용한 반전 기반 스타일 변환 In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 10146-10156.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>