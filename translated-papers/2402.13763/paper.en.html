<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Music Style Transfer with Time-Varying Inversion of Diffusion Models\n' +
      '\n' +
      'Sifei Li\\({}^{1,2}\\), Yuxin Zhang\\({}^{1,2}\\), Fan Tang\\({}^{3}\\), Chongyang Ma\\({}^{4}\\), Weiming Dong\\({}^{1,2}\\)1, Changsheng Xu\\({}^{1,2}\\)\n' +
      '\n' +
      '\\({}^{1}\\)MAIS, Institute of Automation, Chinese Academy of Sciences\n' +
      '\n' +
      '\\({}^{2}\\)School of Artificial Intelligence, University of Chinese Academy of Sciences\n' +
      '\n' +
      '\\({}^{3}\\)Institute of Computing Technology, Chinese Academy of Sciences\n' +
      '\n' +
      '\\({}^{4}\\)Kuaishou Technology\n' +
      '\n' +
      '{lisifei2022, weiming.dong, changsheng.xu}@ia.ac.cn, tangfan@ict.ac.cn, chongyangm@gmail.com\n' +
      '\n' +
      'Footnote 1: Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'With the development of diffusion models, text-guided image style transfer has demonstrated high-quality controllable synthesis results. However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music style transfer approach that effectively captures musical attributes using minimal data. We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we propose a bias-reduced stylization technique to obtain stable results. Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies. Samples and source code are available at [https://lsthuhuif.github.io/MusicTI/](https://lsthuhuif.github.io/MusicTI/).\n' +
      '\n' +
      '## Introduction\n' +
      '\n' +
      'If a picture is worth a thousand words, then every melody is timeless. Music is an essential art form in human society, and a change in music style can offer listeners a completely new experience and perception. For a long time, music creation has had high barriers to entry. However, music style transfer has opened up possibilities for ordinary individuals to achieve personalized music experiences. Music style transfer refers to the process of transferring the style of a given audio clip to another without altering its melody. Sound is omnipresent in our lives, so inspired by music creators who utilize natural sounds in their compositions1, music style transfer can be extended to encompass various types of sound examples.\n' +
      '\n' +
      'Footnote 1: How natural sounds can be involved in music production is well explained by [https://youtu.be/xixiesRtgKU?list=RDxixiiesRtgKU](https://youtu.be/xixiesRtgKU?list=RDxixiiesRtgKU); [https://theworld.org/stories/2021-03-14/nature-always-singing-now-you-can-make-your-own-music-natures-](https://theworld.org/stories/2021-03-14/nature-always-singing-now-you-can-make-your-own-music-natures-) sounds.\n' +
      '\n' +
      'Deep learning-based music style transfer has been a hot research topic in recent years. Some works [14, 15, 16] can stylize music with a specific timbre to a specific or a few instruments, while others [17, 18, 19, 20] have achieved many-to-many music style transfer but restrict the transformation to a finite set of styles presented in the training data. There are efforts [15, 14] to explore one-shot music style transfer, but they still have difficulties in handling natural sounds. With the development of large language models, some works [16, 15, 17, 18] explore text-guided music generation and demonstrate remarkable capacity for generating impressive results. Specially, MusicLM [19] implement music style transfer by conditioning on both textual and melodic representations. However, existing methods can only achieve common style transfer based on coarse descriptions of genres (e.g., "rock", "jazz"), instruments (e.g., "piano", "guitar", "violin"), or performance forms (e.g., "chorus", "string quartet"). They lack the ability to handle niche instruments such as cornet or erhu. Furthermore, these methods are insufficient to address complex scenarios involving the description of natural sounds or synthesized audio effects.\n' +
      '\n' +
      'To alleviate all the above problems and leverage the generative capabilities of pretrained large-scale models, we propose a novel example-guided music stylization method. Our approach aims to achieve music style transfer based on arbitrary examples, encompassing instruments, natural sounds, and synthesized sound effects. Given an audio clip, we can transfer its style to arbitrary input music which is used as content. As illustrated in Figure 1, our method can transfer the texture of the style mel-spectrograms to the local regions of the content mel-spectrograms, while preserving the structure of the content mel-spectrograms.\n' +
      '\n' +
      'To achieve this goal, we seek to obtain an effective style representation of the input audio. Inspired by Textual Inversion [19], which utilizes a pseudo-word to represent a specific concept through the reconstruction of target images, we aim to learn a pseudo-word that represents the style audio in a similar manner. However, we expect to avoid introducing the content of the style audio during the stylization process. We suppose that differenttimesteps of the diffusion model focus on different levels of features. Therefore, we propose a time-varying textual inversion module, where the emphasis of text embedding shifts from texture to structure of the style mel-spectrogram as the timestep increases. Furthermore, we use a partially noisy mel-spectrogram of the content music as the content guidance. As a result, when using the pseudo-word as guidance in the execution of DDIM [23], it becomes a partial denoising process. This scheme naturally excludes structure-related timesteps, which are associated with melody or rhythm, from participating in the stylization process. Meanwhile, it preserves the melody or rhythm of the content mel-spectrogram. To reduce bias of diffusion models on content preservation, we add noise to the mel-spectrogram using the predicted noise instead of random noise, resulting in a more stable stylization result.\n' +
      '\n' +
      'Our contributions can be summarized as follows:\n' +
      '\n' +
      '* We propose a novel example-based method for music style transfer with time-varying textual inversion.\n' +
      '* Our approach enables the use of non-musical audio for music style transfer and achieves highly creative results.\n' +
      '* Experimental results demonstrate that our method outperforms existing approaches in both qualitative and quantitative evaluations.\n' +
      '\n' +
      '## Related Work\n' +
      '\n' +
      '**Music style transfer.** Deep learning-based music style transfer has been widely studied as a typical mechanism of music generation. Dai, Zhang, and Xia [2018] explores the concept of music style transfer and analyzes its development. Many works have conducted further research on music style transfer using various deep learning frameworks [1, 19, 20, 18, 17, 16, 19, 21, 22]. TimbreTron [16] employs image style transfer techniques to achieve timbre transfer across multiple styles. Grinstein et al. [2018] explore timbre transfer between arbitrary audios based on CNN-extracted statistical features of audio styles. Groove2Groove [14] adopts an encoder-decoder structure to achieve one-shot style transfer for symbolic music. Cifka et al. [2021] employs vector-quantized variational autoencoder (VQ-VAE) for one-shot music style transfer without being restricted to the training data, yielding good performance even on real-world data. Music-STAR [19] explores style transfer between multi-track pieces, but it is limited to specific instruments. Bonnici, Benning, and Saitis [2022] utilize variational autoencoders\n' +
      '\n' +
      'Figure 1: Music style transfer results using our method. Our approach can accurately transfer the style of various mel-spectrograms (e.g., instruments, natural sounds, synthetic sound) to content mel-spectrograms using minimal reference data, even as little as a five-second clip. In the style mel-spectrograms, the black box highlights the regions with prominent texture. It can be observed in the blue boxes that the style transfer results preserve a similar structure to the content mel-spectrograms while exhibiting similar texture to the style mel-spectrograms.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'timestep increases, the text embedding gradually focuses more on structure rather than texture. Therefore, we can treat the text embeddings at smaller time steps of the diffusion model as representations of style.\n' +
      '\n' +
      'Specifically, we supply timestep \\(t\\) to the time-varying encoder (TVE) module. The timestep is firstly embedded as \\(t_{e}\\). After performing several linear layers on it, the output is added to the initial embedding \\(v_{os}\\) as \\(v^{0}\\), and then undergoes multiple attention modules to derive the final embedding \\(v_{i*}\\). The multiple attention modules start with \\(v^{0}\\), then each attention layer is implemented as follows:\n' +
      '\n' +
      '\\[Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d}})\\cdot V. \\tag{1}\\]\n' +
      '\n' +
      'For self attention layer, \\(Q^{s},K^{s},V^{s}\\) are defined as:\n' +
      '\n' +
      '\\[M^{s}=W_{M^{s}}\\cdot v^{0}, \\tag{2}\\]\n' +
      '\n' +
      'Figure 3: Our time-varying textual inversion module extends the time-step dimension of text embeddings. When reconstructing style mel-spectrograms, the text embeddings exhibit differentiation in the time-step dimension. As the time steps increase, the focus of the text embeddings shifts from texture to structure.\n' +
      '\n' +
      'Figure 2: An overview of our method. We adopt Riffusion (Forsgren and Martiros 2022) as the backbone network and propose a time-varying textual inversion module, which mainly consists of a time-varying encoder (TVE) as shown on the right. Performing several linear layers on the timestep \\(t_{e}\\), and then adding the output to the initial embedding \\(v_{os}\\), TVE gives the final embedding \\(v_{i*}\\) through multiple attention modules. \\(M_{s}\\), \\(\\hat{M}_{s}\\), \\(M_{c}\\), \\(M_{cn}\\), \\(\\hat{z}_{t_{p}}\\), \\(\\hat{M}_{cn}\\), \\(\\hat{M}_{cs}\\) respectively represent style mel-spectrogram, reconstructed style mel-spectrogram, content mel-spectrogram, noisy content mel-spectrogram, predicted noise, predicted noisy content mel-spectrogam and stylized mel-spectrogram.\n' +
      '\n' +
      'where \\(M^{s}\\) can be from \\(\\{Q^{s},K^{s},V^{s}\\}\\). As for cross attention layer, \\(Q^{c},K^{c},V^{c}\\) are defined as:\n' +
      '\n' +
      '\\[Q^{c}=W_{Q^{c}}\\cdot v^{1},M^{c}=W_{M^{c}}\\cdot v^{0}, \\tag{3}\\]\n' +
      '\n' +
      '\\[v^{1}=Attention(Q^{s},K^{s},V^{s}), \\tag{4}\\]\n' +
      '\n' +
      'where \\(M^{c}\\) can be from \\(\\{K^{c},V^{c}\\}\\).\n' +
      '\n' +
      'The final embedding \\(v_{i*}\\) are defined as:\n' +
      '\n' +
      '\\[v_{i*}=Attention(Q^{c},K^{c},V^{c}). \\tag{5}\\]\n' +
      '\n' +
      'By performing text transformer, \\(v_{i}\\) is transformed into conditions for guiding LDMs. Our improved text encoder \\(e\\) is constructed by integrating the CLIP [1] text encoder with TVE. Based on the loss of LDMs, our optimization objective is defined as follows:\n' +
      '\n' +
      '\\[v_{i*}=\\operatorname*{arg\\,min}_{v}\\mathbb{E}_{z,y,\\epsilon,t}[\\|\\epsilon- \\epsilon_{\\theta}(z_{t},t,e_{\\theta}(y,t))\\|_{2}^{2}], \\tag{6}\\]\n' +
      '\n' +
      'where \\(z\\sim E(x),\\epsilon\\sim\\mathcal{N}(0,1)\\), \\(\\epsilon_{\\theta}\\) and CLIP text encoder of \\(e_{\\theta}\\) are frozen during training to maintain the performance of large pretrained models.\n' +
      '\n' +
      '### Bias-Reduced Stylization\n' +
      '\n' +
      'We observe that for diffusion models, as the timestep decreases during the denoising process from a noisy image to a real image, the primary structure is initially established, followed by the gradual refinement of details. We employ the strength mechanism during the stylization to achieve content guidance.\n' +
      '\n' +
      'Our bias-reduced stylization involves a partial diffusion process, a determined diffusion process, and a denoising process (see Figure 2). The partial diffusion process means adding noise to the content mel-spectrogram \\(M_{c}\\) until the time-step reaches \\(t_{p}\\), where \\(t_{p}=T\\cdot strength\\), and \\(M_{c}\\) is transformed into a noisy mel-spectrogram \\(M_{cn}\\). The determined diffusion process performs a single step denoising on \\(M_{cn}\\), where the predicted noise \\(\\hat{z}_{t_{p}}\\) is used to replace the random noise when performing the diffusion process, resulting in a new noisy content mel-spectrogram \\(\\hat{M}_{cn}\\). This process can be viewed as introducing a bias into the noisy image to counterbalance the impact of model bias. The denoising process progressively transforms \\(\\hat{M}_{cn}\\) into \\(\\hat{M}_{cs}\\) by DDIM [23] with a simple prompt "\\(*\\)". Note that both the diffusion process and denoising process are performed in the latent space of the VAE encoder. The denoised output requires decoding by the VAE decoder into a Mel-spectrogram, which can subsequently be reconstructed into audio using the Griffin-Lim algorithm.\n' +
      '\n' +
      '## Experiement\n' +
      '\n' +
      'We conducted qualitative evaluation, quantitative evaluation and ablation study to demonstrate the effectiveness of our method, which performs well in both content preservation and style fit.\n' +
      '\n' +
      '### Dataset.\n' +
      '\n' +
      'Currently, there is a lack of publicly available datasets specifically tailored for music style transfer that meet our requirements. We collected a small-scale dataset from a website ([https://pixabay.com](https://pixabay.com)) where all the content is free for use. The collected data was segmented into five-second clips, resulting in a total of 253 5-second clips, with 74 style clips and 179 content clips. The style subset consists of 18 different style audios, including instruments, natural sounds, and synthesized sound effects. The content subset consists of electronic music and instrument clips, distinguishing it from other music style transfer approaches that primarily employ simple monophonic audio. In our experiments, we did not utilize all of the style audio clips. Instead, we selected only one sample for each natural sound and synthetic sound effect. Considering the variability of musical instrument notes, we used 3-5 clips for each instrument.\n' +
      '\n' +
      'We compared our method with three related state-of-the-art approaches:\n' +
      '\n' +
      '* R+TI: We combined Riffusion (R) [11] with Textual Inversion (TI) [1] as our baseline. R is the original stable diffusion model v1.5, which is just fine-tuned on images of mel-spectrograms paired with text. Additionally, it incorporates a conversion library for transformation between audio and mel-spectrograms. TI is a classical method that learns a pseudo-word for a concept within a limited number of images using an optimization-based approach.\n' +
      '* SS VQ-VAE [12]: A latest available implementation of one-shot music style transfer.\n' +
      '* MUSICGEN [13]: A recently released text-guided music generation method that achieves text-guided music stylization with melody conditioning.\n' +
      '\n' +
      '### Implementation details.\n' +
      '\n' +
      'In our experiments, we fix the parameters of LDMs and text encoder except for the TVE module. We use the default hyperparameters of LDMs and set a base learning rate of 0.001. The training process on each style takes approximately 30 minutes using an NVIDIA GeForce RTX3090 with a batch size of 1, less than the more than 60 minutes required for TI. During inference, our approach employs two hyperparameters: \\(strength\\) and \\(scale\\). These parameters respectively govern the intensity of the content and regulate the intensity of the style. We achieved the best results when strength ranged from 0.6 to 0.7 and the scale ranged from 3.0 to 5.0.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      'The stylized audio samples, showcasing the comparison between our method and other approaches, can be accessed on the static webpage provided within the supplementary materials. As shown in the Figure 4, we compared our method with three approaches: R+TI [11, 12, 13], SS VQ-VAE [12], and MUSICGEN [13]. The structure of the mel-spectrogram can be seen as the content, while the detailed texture is considered as the style.\n' +
      '\n' +
      'For R+TI, we treated partial noisy content mel-spectrogram as content guidance and used the learned pseudo-word as text guidance for style transfer using DDIM. It can be observed that although R+TI preserves the overall structure well, it introduces occasional flaws in the rhythm at the local level and exhibits weaker texture transfer compared to our method. SS VQ-VAE processes audios with a sampling rate of 16kHz, resulting in the loss of high-frequency information after stylization. It introduces severe artifacts in the mel-spectrogram, resulting in poor performance in terms of audio quality. Regarding MUSICGEN, we used the textual descriptions of the style audios as guidance for style transfer. The results indicate that its generation quality exhibits a high degree of stochasticity, characterized by unstable content preservation and limited editability. Our method can accurately preserve the structure of content mel-spectrograms while achieving high-quality texture transfer of style mel-spectrograms, without introducing the artifacts observed in other methods.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      'Following the previous works on music style transfer [16, 17], we evaluate our method based on two criteria: (a) content preservation and (b) style fit. Taking inspiration from MUSICGEN [18] and InST [16], we compute the CLAP cosine similarity between the generated mel-spectrograms and the content mel-spectrograms to evaluate content preservation. Additionally, we calculate the CLAP cosine similarity between the generated mel-spectrograms and the corresponding textual description of the style to evaluate style fit. We computed the CLAP cosine similarity between the textual descriptions and the style mel-spectrograms as a reference, with an average value of 0.4890 and a minimum value of 0.3424. Thus, we excluded style audios that were difficult to describe in text from the calculation of objective metrics. This ensures the correlation between our style mel-spectrograms and the evaluation text. We evaluated our method and other approaches by randomly selecting 282 content-style pairs and assessing their performance, as shown in Table 1. Our method achieves the best performance in both metrics, significantly surpassing our baseline in terms of content preservation. While SS VQ-VAE achieves a similar style fit to ours, it suffers from greater content loss. MUSICGEN performs noticeably\n' +
      '\n' +
      'Figure 4: Qualitative comparison with state-of-the-arts methods [14, 16, 15, 16]. (a) Style mel-spectrograms, the texts on the left are the sound categories. (b) Mel-spectrograms. (c)-(d) The stylized results of various methods. In the style mel-spectrograms, the black box highlights the regions with prominent texture. It can be observed in the blue boxes that only our results preserve a similar structure to the content mel-spectrograms while exhibiting a similar texture to the style mel-spectrograms.\n' +
      '\n' +
      'worse than our method in both metrics.\n' +
      '\n' +
      'User study.To conduct a subjective evaluation of our method\'s performance, we designed a user study to rate the four methods on three evaluation metrics. We randomly selected 15 sets of results (excluding comparisons with MU-SICGEN [12] for style audios that are difficult to describe with text). Before the test, we set up questions to assess the participants\' music profession level and provided guidelines outlining the evaluation criteria for music style transfer. During the test, each participant was presented with a style audio, a content audio, and four randomly ordered generation results for each set of questions. Participants were asked to rate the following metrics on a scale of 1 (lowest) to 5 (highest):\n' +
      '\n' +
      '* Content Preservation: consistency between the generated audio and the content music in terms of melody, rhythm, and similar attributes.\n' +
      '* Style Fit: consistency between the generated audio and the style audio in terms of timbre, sound units, and similar attributes.\n' +
      '* Overall Quality: the quality related to the overall performance of style transfer, such as the coherence of the fusion between the content and style of generated music.\n' +
      '\n' +
      'Our experiment involved 80 participants, out of which 72 were deemed valid (excluding participants with no knowledge of music), resulting in a total of 12960 ratings. After excluding the maximum and minimum values, We calculated the weighted average based on participants\' music profession level (four levels with corresponding weights: 1 to 4). The results, as presented in Table 1, demonstrate that our method outperforms other approaches significantly in terms of content preservation, style fit, and overall quality.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'Time-varying embedding (TVE).We fix the text embedding of the pseudo-word at a specific time step during inference and use it as the text guidance for mel-spectrogram generation, as shown in Figure 3. As the timestep increases, the text embeddings gradually shift their focus from the texture of the mel-spectrogram to the structure. This aligns with our expectation that the diffusion model first constructs the rough structure of the image during denoising and then optimizes the details. The reconstructed results reflect the high-quality reconstruction due to the fusion of features across different timesteps. To further demonstrate the effectiveness of the TVE module, we evaluate our method without it, as shown in Table 2. Although the difference in content preservation is not significant after removing TVE, there is a noticeable decrease in style fit, indicating that TVE contributes to better style learning.\n' +
      '\n' +
      'Bias-reduced stylization.We evaluate the impact of removing the bias-reduced stylization technique on content preservation and style matching. It can be observed that there is a decrease in both metrics, indicating that it is helpful in terms of preserving content and facilitating style transfer.\n' +
      '\n' +
      '### Discussions and Limitations\n' +
      '\n' +
      'Our method enables music style transfer using diverse audio sources, including instruments, natural sounds, and synthesized sound effects. Nevertheless, it is crucial to recognize that certain limitations may arise in specific contexts. For instance, when the content music encompasses multiple components, our method may encounter challenges in accurately performing style transfer on each individual component, potentially leading to partial content loss. Furthermore, when the style audio incorporates white noise like rain or wind sounds, it becomes challenging to capture the inherent musicality within those elements and transfer it effectively to the content reference.\n' +
      '\n' +
      '## Conclusion\n' +
      '\n' +
      'In this paper, we propose a novel approach for music stylization based on diffusion models and time-varying textual inversion, which effectively embeds style mel-spectrograms. Our experiments demonstrate the generality of our method for various types of audio, including musical instruments, natural sounds, and synthesized sound effects. Our approach achieves style transfer with a small amount of data, generating highly creative music. Even when applied to non-musical style audio, our method produces results with a high level of musicality. We believe that leveraging pre-trained models with stronger generative capabilities would further enhance the performance of our method. In the future, we aim to investigate more interpretable and attribute-disentangled music style transfer.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'This work was supported by the National Natural Science Foundation of China under nos. 61832016 and 62102162.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\hline  & \\multicolumn{2}{c|}{Objective} & \\multicolumn{2}{c}{Subjective} \\\\ Method & CP & SF & CP & SF & OVL \\\\ \\hline R+TI & 0.3481 & 0.2722 & 2.81 & 3.20 & 2.75 \\\\ SS VQ-VAE & 0.2351 & 0.2809 & 3.36 & 2.34 & 2.60 \\\\ MUSICGEN & 0.2808 & 0.2370 & 2.81 & 2.70 & 2.83 \\\\\n' +
      '**Ours** & **0.4645** & **0.2816** & **3.91** & **3.70** & **3.66** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Qualitative comparison with other methods [12, 13, 14]. CP, SF, OVL stands for Content Preservation, Style Fit, and Overall Quality, respectively.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline  & Content Preservation & Style Fit \\\\ \\hline w/o TVE & 0.4506 & 0.2418 \\\\ w/o BRS & 0.4415 & 0.2602 \\\\\n' +
      '**Ours** & **0.4645** & **0.2816** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Ablation study of our method. TVE and BR are Time-Varying Embedding and Bias-Reduced Stylization respectively.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Agostinelli, T. I. Denk, Z. Borosos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, et al. (2023)MusicLM: generating music from text. arXiv preprint arXiv:2301.11325. Cited by: SS1.\n' +
      '* M. Alinoori and V. Tzerpos (2022)Music-STAR: a style translation system for audio-based re-instrumentation. In International Society for Music Information Retrieval Conference (ISMIR), pp. 419-426. Cited by: SS1.\n' +
      '* A. Bitton, P. Esling, and A. Chemla-Romeu-Santos (2018)Modulated variational auto-encoders for many-to-many musical timbre transfer. arXiv preprint arXiv:1810.00222. Cited by: SS1.\n' +
      '* R. S. Bonnici, M. Benning, and C. Saitis (2022)Timbre transfer with variational auto encoding and cycle-consistent adversarial networks. In International Joint Conference on Neural Networks (IJCNN), pp. 1-8. Cited by: SS1.\n' +
      '* Z. Borosos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, et al. (2023)Audio: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing. Cited by: SS1.\n' +
      '* G. Brunner, A. Konrad, Y. Wang, and R. Wattenhofer (2018)MIDI-VAE: modeling dynamics and instrumentation of music with applications to style transfer. In International Society for Music Information Retrieval Conference (ISMIR), pp. 747-754. Cited by: SS1.\n' +
      '* Y. Chang, W. Chen, and M. Hu (2021)Semi-supervised many-to-many music timbre transfer. In International Conference on Multimedia Retrieval (ICMR), pp. 442-446. Cited by: SS1.\n' +
      '* J. Choi and K. Lee (2023)Pop2Piano: pop audio-based piano cover generation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS1.\n' +
      '* O. Cifka, A. Ozerov, U. Simsekli, and G. Richard (2021)Self-supervised vq-vae for one-shot music style transfer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 96-100. Cited by: SS1.\n' +
      '* O. Cifka, U. Ozerov, U. Simsekli, and G. Richard (2020)Groove2Groove: one-shot music style transfer with supervision from synthetic data. IEEE/ACM Transactions on Audio, Speech, and Language Processing28, pp. 2638-2650. Cited by: SS1.\n' +
      '* J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Defossez (2023)Simple and controllable music generation. arXiv preprint arXiv:2306.05284. Cited by: SS1.\n' +
      '* S. Dai, Z. Zhang, and G. G. Xia (2018)Music style transfer: a position paper. arXiv preprint arXiv:1803.06841. Cited by: SS1.\n' +
      '* stable diffusion for real-time music generation. Note: [https://rifrfusion.com/aboutAccessed](https://rifrfusion.com/aboutAccessed): 2022-12-31 Cited by: SS1.\n' +
      '* R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023)An image is worth one word: personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* R. Gal, M. Alauf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023)An image is worth one word: personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* R. Gal, M. Araf, Y. Atzmon, A. H. Bermano, G. Chechik, and D. Cohen-Or (2023)Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics42 (4), pp. 150:1-150:13. Cited by: SS1.\n' +
      '* D. Ghosal, N. Majumder, A. Mehrish, and S. Poria (2023)Text-to-audio generation using instruction tuned LLM and latent diffusion model. arXiv preprint arXiv:2304.13731. Cited by: SS1.\n' +
      '* E. Grinstein, N. Q. Duong, A. Ozerov, and P. Perez (2018)Audio style transfer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 586-590. Cited by: SS1.\n' +
      '* M. Guo, C. Lu, Z. Liu, M. Cheng, and S. Hu (2023)Visual attention network. Computational Visual Media9 (4), pp. 733-752. Cited by: SS1.\n' +
      '* Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. Ellis (2022)MuLan: a joint embedding of music audio and natural language. In International Society for Music Information Retrieval Conference (ISMIR), pp. 559-566. Cited by: SS1.\n' +
      '* Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, X. Chen, Z. Zhang, J. Yu, C. Frank, et al. (2023)Noise2Music: text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917. Cited by: SS1.\n' +
      '* R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao (2023)Make-an-audio: text-to-audio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning (ICML), Cited by: SS1.\n' +
      '* S. Huang, Q. Li, C. Anil, X. Bao, S. Oore, and R. B. Grosse (2019)TimbreTron: a wavelet(CycleGAN(CQT(Audio))) Pipeline for musical timbre transfer. In International Conference on Learning Representations (ICLR), Cited by: SS1.\n' +
      '* Z. Huang, T. Wu, Y. Jiang, K. C. Chan, and Z. Liu (2023)ReVersion: diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495. Cited by: SS1.\n' +
      '* D. K. Jain, A. Kumar, L. Cai, S. Singhal, and V. Kumar (2020)ATT: attention-based timbre transfer. In 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1-6. Cited by: SS1.\n' +
      '* S. Li, J. van de Weijer, T. Hu, F. S. Khan, Q. Hou, Y. Wang, and J. Yang (2023)StyleDiffusion: prompt-embedding inversion for text-based editing. arXiv preprint arXiv:2303.15649. Cited by: SS1.\n' +
      '* H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley (2023)Audioldm: text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503. Cited by: SS1.\n' +
      '* C. Lu, M. Xue, C. Chang, C. Lee, and L. Su (2019)Play as you like: timbre-enhanced multi-modal music style transfer. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 1061-1068. Cited by: SS1.\n' +
      '* W. T. Lu, L. Su, and A. E. E.\n' +
      '\n' +
      'Mor, N.; Wolf, L.; Polyak, A.; and Taigman, Y. 2019. A universal music translation network. In _International Conference on Learning Representations (ICLR)_.\n' +
      '* [Ouyang et al.2022] Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 35, 27730-27744.\n' +
      '* [Radford et al.2021] Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 8748-8763. PMLR.\n' +
      '* [Rombach et al.2022] Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 10684-10695.\n' +
      '* [Schneider2023] Schneider, F. 2023. Archisound: Audio generation with diffusion. _arXiv preprint arXiv:2301.13267_.\n' +
      '* [Schneider, Jin, and Scholkopf2023] Schneider, F.; Jin, Z.; and Scholkopf, B. 2023. Mousai: Text-to-Music Generation with Long-Context Latent Diffusion. _arXiv preprint arXiv:2301.11757_.\n' +
      '* [Song, Meng, and Ermon2020] Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_.\n' +
      '* [Tewel et al.2023] Tewel, Y.; Gal, R.; Chechik, G.; and Atzmon, Y. 2023. Key-Locked Rank One Editing for Text-to-Image Personalization. In _ACM SIGGRAPH 2023 Conference Proceedings_, 12:1-12:11. New York, NY, USA: Association for Computing Machinery.\n' +
      '* [Voynov et al.2023] Voynov, A.; Chu, Q.; Cohen-Or, D.; and Aberman, K. 2023. \\(P+\\): Extended Textual Conditioning in Text-to-Image Generation. _arXiv preprint arXiv:2303.09522_.\n' +
      '* [Wu and Yang2023] Wu, S.-L.; and Yang, Y.-H. 2023. MuseMorphose: Full-Song and Fine-Grained Piano Music Style Transfer With One Transformer VAE. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31: 1953-1967.\n' +
      '* [Wu et al.2023a] Wu, Y.; Chen, K.; Zhang, T.; Hui, Y.; Berg-Kirkpatrick, T.; and Dubnov, S. 2023a. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In _IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 1-5. IEEE.\n' +
      '* [Wu et al.2023b] Wu, Y.; He, Y.; Liu, X.; Wang, Y.; and Dannenberg, R. B. 2023b. Transplayer: Timbre Style Transfer with Flexible Timbre Control. In _IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 1-5. IEEE.\n' +
      '* [Zhang et al.2023a] Zhang, Y.; Dong, W.; Tang, F.; Huang, N.; Huang, H.; Ma, C.; Lee, T.-Y.; Deussen, O.; and Xu, C. 2023a. ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models. _ACM Transactions on Graphics_, 42(6): 244:1-244:14.\n' +
      '* [Zhang et al.2023b] Zhang, Y.; Huang, N.; Tang, F.; Huang, H.; Ma, C.; Dong, W.; and Xu, C. 2023b. Inversion-Based Style Transfer with Diffusion Models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 10146-10156.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>