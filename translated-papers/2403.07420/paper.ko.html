<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 드래그 Any: Entity Representation을 이용한 Anything에 대한 Motion Control\n' +
      '\n' +
      'Wejia Wu\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 장리\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 구우차오\n' +
      '\n' +
      '싱가포르 국립대학교 3Show Lab\n' +
      '\n' +
      'Rui Zhao\n' +
      '\n' +
      '싱가포르 국립대학교 3Show Lab\n' +
      '\n' +
      'Yefei He\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 다비드 준하장\n' +
      '\n' +
      '싱가포르 국립대학교 3Show Lab\n' +
      '\n' +
      ' 마이크정수\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 양리\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 가오\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      ' 장\n' +
      '\n' +
      '1Kuaishou Technology\n' +
      '\n' +
      '2Zhejiang University\n' +
      '\n' +
      '2\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '우리는 제어 가능한 비디오 생성에서 모든 객체에 대한 모션 제어를 달성하기 위해 엔티티 표현을 사용하는 DragAnything를 소개한다. 기존의 모션 제어 방법과 비교하여 DragAnything는 몇 가지 장점을 제공한다. 첫째, 궤적 기반은 다른 유도 신호(예를 들어,_마스크, 깊이 맵)를 획득할 때 상호작용에 대해 더 사용자 친화적이다. 사용자는 상호 작용 중에 선(궤적)만 그리면 됩니다. 둘째, 개체 표현은 임의의 개체를 표현할 수 있는 오픈 도메인 임베딩 역할을 하여 배경을 포함한 다양한 개체들에 대한 움직임 제어를 가능하게 한다. 마지막으로, 우리의 엔티티 표현은 다수의 객체들에 대해 동시적이고 뚜렷한 모션 제어를 허용한다. 광범위한 실험을 통해 제안된 DragAnything가 FVD, FID, User Study에 대한 최신 성능을 달성하였으며, 특히 객체 움직임 제어 측면에서 기존 방법(예를 들어, DragNUWA)을 인간 투표에서 26% 능가하는 성능을 보였다.\n' +
      '\n' +
      '프로젝트 웹 사이트는 "드래그 모든 것"입니다.\n' +
      '\n' +
      '키워드: 움직임 제어 제어 가능한 비디오 생성 확산 모델\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 이마겐 비디오[22], 젠-2[13], 피카랩[1], SVD[3], SORA[38] 등의 주목할 만한 작품들이 커뮤니티로부터 상당한 관심을 받고 있는 등 영상세대에 상당한 발전이 있었다. 그러나 제어 가능한 비디오 생성의 추구는 그 중추적인 중요성에도 불구하고 상대적으로 더 느린 진행에 직면했다. 제어 가능한 정적 이미지 생성[51, 33, 32]과는 달리, 제어 가능한 비디오 생성은 공간 콘텐츠 조작뿐만 아니라 정밀한 시간적 모션 제어를 요구하여 보다 복잡한 도전을 제기한다.\n' +
      '\n' +
      '최근, 궤적 기반 모션 제어[19, 2, 42, 49]는 제어 가능한 비디오 생성을 위한 사용자 친화적이고 효율적인 솔루션임이 입증되었다. 마스크들 또는 깊이 맵들과 같은 다른 안내 신호들과 비교하여, 궤적을 그리는 것은 간단하고 유연한 접근법을 제공한다. 초기 궤적 기반 [19, 2, 4, 5] 작업은 제어 가능한 비디오 생성에서 객체의 움직임을 제어하기 위해 광학 흐름 또는 순환 신경망을 활용했다. 대표적인 작업 중 하나로 Drag-NUWA[49]는 희소 스트로크를 조밀한 흐름 공간으로 인코딩하여 객체의 움직임을 제어하기 위한 유도 신호로 사용한다. 유사하게, MotionCtrl[42]은 각 객체의 궤적 좌표를 벡터 맵으로 직접 인코딩하며, 이 벡터 맵을 객체의 움직임을 제어하기 위한 조건으로 사용한다. 이러한 작업은 제어 가능한 비디오 생성에 상당한 기여를 했다. 그러나, 중요한 질문이 간과되었다: _타겟 상의 단일 포인트가 진정으로 타겟을 나타낼 수 있는가?_\n' +
      '\n' +
      '확실히, 단일 픽셀 포인트는 도 2의 (a) 내지 (b)에 도시된 바와 같이, 전체 객체를 나타낼 수 없다. 따라서, 단일 픽셀 포인트를 드래그하는 것은 그것이 대응하는 객체를 정밀하게 제어하지 않을 수 있다. 도 1에 도시된 바와 같이, 별이 빛나는 하늘의 별 위의 픽셀의 궤적을 고려할 때, 모델은 별의 움직임 또는 별이 빛나는 하늘 전체의 움직임을 제어하는 것을 구별하지 않을 수 있다; 이는 단지 연관된 픽셀 영역을 드래그하는 것이다. 실제로, 이 문제를 해결하려면 두 가지 개념을 명확히 해야 한다: 1) **What entity.** 드래그될 특정 영역 또는 엔티티를 식별해야 한다. 2) ** 드래그하는 방법** 선택된 영역만을 드래그하는 것을 달성하는 방법, 즉 드래그가 필요한 전경으로부터 배경을 분리하는 것을 의미한다. 첫 번째 도전의 경우, 대화형 세분화[26, 40]가 효율적인 해결책이다. 예를 들어, 초기 프레임에서 SAM[26]을 사용하면 우리가 제어하고자 하는 영역을 편리하게 선택할 수 있다. 이에 비해 두 번째 기술적 문제는 더 큰 도전을 제기한다. 이를 해결하기 위해, 본 논문에서는 비디오 내의 임의의 엔티티에 대한 정밀한 모션 제어를 달성하기 위한 새로운 엔티티 표현을 제안한다.\n' +
      '\n' +
      '일부 작업 [11, 16, 37]은 이미 해당 객체를 나타내기 위해 잠재 특징을 사용하는 효과를 입증했다. 애니도어[11]은 객체 커스터마이징을 처리하기 위해 디노 v2[31]로부터의 특징들을 이용하는 반면, 비디오 스왑[16] 및 DIFT[37]은 확산 모델[33]로부터의 특징들을 채용하여 비디오 편집 태스크들을 처리한다. 이러한 작업에서 영감을 받아 각 개체를 나타내기 위해 확산 모델의 잠재 특징을 활용하는 DragAnything를 제시한다. 도 2에 도시된 바와 같이\n' +
      '\n' +
      '도 1: **이전 작업과의 비교.**(a) 이전 작업(Motionctrl[42], DragNUWA[49])은 픽셀 포인트 또는 픽셀 영역을 드래그함으로써 모션 제어를 달성하였다. (b) 드래그Anything는 대응하는 엔티티 표현을 조작함으로써 보다 정밀한 엔티티-레벨 모션 제어를 가능하게 한다.\n' +
      '\n' +
      ' (d) 개체 마스크의 좌표 지수에 기초하여, 첫 번째 프레임의 확산 특징으로부터 대응하는 의미 특징을 추출할 수 있다. 그런 다음 이러한 특징을 사용하여 개체를 표현하여 해당 잠재 특징의 공간 위치를 조작하여 개체 수준의 움직임 제어를 달성한다.\n' +
      '\n' +
      '우리 연구에서 DragAnything는 SVD[3]를 기본 모델로 사용한다. DragAnything 훈련은 모션 궤적 포인트들 및 첫 번째 프레임의 엔티티 마스크와 함께 비디오 데이터를 필요로 한다. 필요한 데이터와 주석을 얻기 위해 우리는 DragAnything를 훈련하기 위해 비디오 분할 벤치마크[30]를 활용한다. 첫 번째 프레임에서 각 개체의 마스크를 사용하여 해당 개체의 중심 좌표를 추출한 다음 CoTrack[25]를 사용하여 해당 점의 움직임 궤적을 개체 움직임 궤적으로 예측한다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같이 요약된다:\n' +
      '\n' +
      '* 픽셀 레벨 모션과 엔티티 레벨 모션 사이의 차이를 드러내는 궤적 기반 제어 가능한 생성을 위한 새로운 통찰력.\n' +
      '* 드래그 픽셀 패러다임과 달리, 우리는 엔티티 표현으로 진정한 엔티티-레벨 모션 제어를 달성할 수 있는 DragAnything를 제시한다.\n' +
      '* DragAnything는 FVD, FID, User Study에 대한 SOTA 성능을 달성하여 모션 제어를 위한 인간 투표에서 기존 방식을 26% 능가한다. 드래그 Any는 도 6 및 도 9에 도시된 바와 같이 배경(_예를 들어,_ sky)을 포함하는 컨텍스트에서 임의의 것에 대한 인터랙티브 모션 제어를 지원한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '###영상 및 동영상 생성\n' +
      '\n' +
      '최근 영상 생성[33, 32, 44, 15, 46, 21, 20]이 상당한 주목을 받고 있다. 안정 AI의 안정 확산[33], 오픈 AI의 DALL-E2[32], 구글의 Imagen[35], SenseTime의 RAPHAEL[48], 및\n' +
      '\n' +
      '도 2: **Different Representation Modeling.**(a) Point representation: coordinate point \\((x,y)\\)를 이용하여 엔티티를 표현한다. (b) 궤적 맵: 개체의 궤적을 나타내기 위해 궤적 벡터 맵을 사용하는 단계. (c) 2D 가우스: 엔티티를 나타내기 위해 2D 가우스 맵을 사용하는 단계를 포함하는, 방법. (c) 박스 표현: 엔티티를 표현하기 위해 바운딩 박스를 사용하는 것. (d) 개체 표현: 개체를 특성화하기 위해 개체의 잠재 확산 특징을 추출하는 단계.\n' +
      '\n' +
      '메타의 Emu[12]는 이미지 생성 작업의 영역에서 상당한 진전과 기여 및 영향을 주었다. 제어 가능한 이미지 생성은 또한 제어넷[51]으로 예시된 상당한 발전과 진전을 보았다. 캐니 에지, 허프 라인, 사용자 낙서, 인간 키 포인트, 세그먼테이션 맵, 정밀한 이미지 생성과 같은 안내 정보를 활용함으로써 달성될 수 있다.\n' +
      '\n' +
      '대조적으로, 비디오 생성 분야의 진행[47, 43, 41, 8, 55, 50]은 여전히 비교적 초기 단계이다. 비디오 확산 모델[24]은 비디오 시퀀스를 예측하고 생성하기 위해 3D U-Net 확산 모델 아키텍처를 사용하여 처음 도입되었다. Imagen Video[22]는 고화질 비디오 생성을 위한 캐스케이드 확산 비디오 모델을 제안하였고, 텍스트-이미지 설정을 비디오 생성으로 이전하려고 시도했다. Show-1 [50]은 화소 공간에서 시간 확산 모델을 직접 구현하고, 고해상도 합성을 위해 인페인팅과 초해상도를 활용한다. 비디오 LDM[6]은 LDM 패러다임을 고해상도 비디오 생성에 처음으로 적용한 것으로 잠재 공간 확산 모델에 시간적 차원을 도입한다. I2vgen-xl[52]은 이 두 요인을 분리하여 모델 성능을 개선하고 정적 이미지를 필수 지침으로 통합하여 데이터 정렬을 보장하는 캐스케이드 네트워크를 도입한다. 학술적 연구 외에도 업계에서는 Gen-2[13], PikaLab[1], SORA[38] 등 수많은 주목할 만한 작품을 생산하기도 하였다. 그러나 일반적인 영상 생성 노력에 비해 제어 가능한 영상 생성의 발전은 여전히 개선의 여지가 있다. 우리는 작업에서 궤적 기반 비디오 생성 분야를 발전시키는 것을 목표로 합니다.\n' +
      '\n' +
      '### 제어 가능한 비디오 생성\n' +
      '\n' +
      'AnimateDiff[18], Control-A-Video[10], Emu Video[14], Motiondirector[54]와 같이 제어 가능한 비디오 생성에 중점을 둔 일부 노력[53, 29, 9, 17, 28]이 있었다. Control-A-Video[10]은 에지 또는 깊이 맵들과 같은 제어 신호들의 시퀀스 상에 컨디셔닝된 비디오들을 2개의 모션-적응 잡음 초기화 전략들로 생성하려고 시도한다. Follow Your Pose[29]는 포즈 제어 가능한 캐릭터 비디오를 획득하기 위해 이미지 포즈 쌍과 포즈 프리 비디오를 활용할 수 있는 2단계 트레이닝 스킴을 제안한다. ControlVideo[53]는 구조적 일관성을 갖는 제어가능한 텍스트-대-비디오 생성이 가능하도록 훈련 없는 프레임워크를 설계한다. 이들 작업은 모두 밀집된 안내 신호(마스크, 인간 포즈, 깊이 등)에 의해 안내되는 비디오 생성 작업에 초점을 맞춘다. 그러나 실제 응용 프로그램에서 조밀한 안내 신호를 얻는 것은 어렵고 사용자 친화적이지 않다. 이에 비해 항력에 대한 궤적 기반 접근법을 사용하는 것이 더 실현 가능한 것으로 판단된다.\n' +
      '\n' +
      '초기 궤적 기반 작업[19, 2, 4, 5]은 종종 움직임 제어를 달성하기 위해 광학 흐름 또는 순환 신경망을 활용했다. TrailBlazer[28]은 피사체의 움직임을 안내하기 위해 바운딩 박스를 사용함으로써 비디오 합성에서의 제어성을 향상시키는 것에 중점을 둔다. DragNUWA[49]는 촘촘한 흐름 공간으로 희소 스트로크를 인코딩하고, 이후 이를 유도 신호로 사용하여 객체의 움직임을 제어한다. 마찬가지로 MotionCtrl[42]은 각 객체의 궤적 좌표를 벡터 맵으로 직접 인코딩하여 객체의 움직임을 제어하는 조건으로 사용한다. 이러한 작업은 궤적 맵(점)과 상자 표현이라는 두 가지 패러다임으로 분류할 수 있다. 박스 표현(_예를 들어,_ TrailBlazer[28])은 인스턴스 레벨 객체만을 취급하고 별빛 하늘과 같은 배경을 수용할 수 없다. 기존의 Trajectory Map Representation (_e.g.,_DragNUWA, MotionCtrl) 방법들은 엔티티들의 의미론적 측면들을 고려하지 않기 때문에 상당히 조잡하다. 즉, 하나의 점이 엔티티를 적절하게 나타낼 수 없다. 본 논문에서는 제안된 개체 표현을 이용하여 실제 개체 수준의 움직임 제어를 수행할 수 있는 DragAnything를 소개한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### 과제형성 및 동기부여\n' +
      '\n' +
      '**Task Formulation.** 궤적 기반 비디오 생성 작업은 모델이 주어진 움직임 궤적에 기초하여 비디오를 합성할 것을 요구한다. 점궤적 \\((x_{1},y_{1}), (x_{2},y_{2}),\\ldots,(x_{L},y_{L})\\)이 주어지면, \\(L\\)은 비디오 길이를 나타내며, 조건부 디노이징 오토인코더 \\(\\epsilon_{\\theta}(z,c)\\)는 움직임 궤적에 해당하는 비디오를 생성하기 위해 사용된다. 본 논문의 안내 신호\\(c\\)는 궤적점, 비디오의 첫 번째 프레임, 첫 번째 프레임의 엔티티 마스크의 세 가지 정보를 포함한다.\n' +
      '\n' +
      '**동기.** 최근에 DragNUWA[49] 및 MotionCtrl[42]와 같은 일부 궤적 기반 작업은 비디오 생성에서 객체의 움직임을 제어하기 위해 궤적점을 사용하여 탐색하였다. 이들 접근법은 통상적으로 제공된 궤적 좌표 또는 이들의 도함수를 사용하여 대응하는 픽셀 또는 픽셀 영역을 직접 조작한다. 그러나, 그들은 중요한 이슈를 간과한다: 도 1 및 도 2에 도시된 바와 같이, _제공된 궤적 포인트들은 우리가 제어하고자 하는 엔티티를 완전히 나타내지 않을 수 있다_ 따라서, 이러한 점들을 드래그하는 것이 반드시 객체의 움직임을 정확하게 제어하는 것은 아닐 수 있다.\n' +
      '\n' +
      '즉, 단순히 픽셀 또는 픽셀 영역을 드래그하는 것이 객체 움직임을 효과적으로 제어할 수 없다는 가설을 검증하기 위해 장난감 실험을 설계하여 확인하였다. 그림 3과 같이 합성 비디오의 모든 픽셀을 추적하고 궤적 변화를 관찰하기 위해 고전적인 점 추적기 _즉,_Co-Tracker[25]를 사용했다. 픽셀 모션의 변화로부터, 우리는 두 개의 새로운 통찰력을 얻는다:\n' +
      '\n' +
      '1: 오브젝트 상의 궤적 포인트들은 엔티티를 나타낼 수 없다.__Insight 1: 오브젝트 상의 궤적 포인트들은 엔티티를 나타낼 수 없다. (도 3의 (a)) DragUNWA의 픽셀 움직임 궤적으로부터, 클라우드의 픽셀 포인트를 드래그하는 것이 클라우드를 이동시키지 않고, 대신에 카메라가 위로 이동하는 결과를 초래한다는 것이 명백하다. 이는 모델이 클라우드를 통제하려는 우리의 의도를 인식할 수 없음을 나타내며, 단일 지점이 클라우드를 나타낼 수 없음을 의미한다. 따라서 우리는 우리가 조작하고자 하는 영역(선택된 영역)을 정밀하게 제어할 수 있는 보다 직접적이고 효과적인 표현이 존재하는지 고민했다.\n' +
      '\n' +
      '_Insight 2: 궤적 포인트 표현 패러다임(도 2의 (a)-(c))에 대해, 드래그 포인트에 더 가까운 픽셀들은 더 큰 영향을 받아 더 큰 모션(도 3의 (b))을 초래한다. 이에 비해, 우리는 DragNUWA에 의해 합성된 비디오들에서, 드래그 포인트에 더 가까운 픽셀들이 더 큰 움직임을 나타내는 것을 관찰한다. 그러나, 우리가 기대하는 것은 개별 픽셀 모션이 아니라 제공된 궤적에 따라 객체가 전체적으로 움직이는 것이다.\n' +
      '\n' +
      '위의 두 가지 새로운 통찰과 관찰을 바탕으로 우리가 제어하고자 하는 대상의 잠재 특징을 그 표현으로 추출하는 새로운 개체 표현을 제시한다. 그림 3에서 볼 수 있듯이 해당 움직임 궤적의 시각화는 우리의 방법이 보다 정밀한 개체 수준의 움직임 제어를 달성할 수 있음을 보여준다. 예를 들어, 도 3의 (b)는 우리의 방법이 갈매기와 어류의 움직임을 정밀하게 제어할 수 있는 반면, DragNUWA는 대응하는 픽셀 영역의 움직임만을 드래그하여 외관의 비정상적인 변형을 초래한다는 것을 보여준다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      'SVD[3]에 이어, 우리의 기본 아키텍처는 공간 및 시간 효율성을 위한 잡음 제거 프로세스를 학습하기 위한 잡음 제거 확산 모델(3D U-Net[34])과 비디오를 잠재 공간으로 인코딩하고 잡음 제거된 잠재 특징을 다시 비디오로 재구성하기 위한 인코더 및 디코더의 세 가지 구성 요소로 구성된다. Controlnet[51]에서 영감을 받아 3D Unet을 사용하여 유도 신호를 부호화하고, 이를 SVD의 잡음제거 3D Unet의 디코더 블록에 적용하여 그림 4와 같이 기존 연구와 달리 개체 표현 추출 메커니즘을 설계하고 2D 가우시안 표현과 결합하여 최종 유효 표현을 구성하였다. 그런 다음 표현으로 개체 수준의 제어 가능한 생성을 달성할 수 있다.\n' +
      '\n' +
      '도 3: Entity Representation의 동기를 위한 **Toy 실험.** 기존 방법(DragNUWA[49] 및 MotionCtrl[42])은 객체를 정확하게 제어할 수 없는 픽셀을 직접 드래그하는 반면, 본 방법은 정확한 제어를 달성하기 위해 엔티티 표현을 사용한다.\n' +
      '\n' +
      '### 개체 의미 표현 추출\n' +
      '\n' +
      '제안된 방법의 조건부 신호는 가우시안 표현(SS3.3)과 대응 개체 표현(SS3.3)을 필요로 한다. 이 섹션에서는 첫 번째 프레임 이미지에서 이러한 표현을 추출하는 방법에 대해 설명한다.\n' +
      '\n' +
      '#### 3.3.1 개체 표현 추출\n' +
      '\n' +
      '첫 번째 프레임 이미지\\(\\mathbf{I}\\in\\mathbb{R}^{H}\\times W\\times 3}\\)에 해당하는 개체 마스크\\(\\mathbf{M}\\)을 주어, 먼저 확산 반전(확산 포워드 프로세스)을 통해 이미지의 잠재 잡음\\(\\mathbf{x}\\)을 얻는데, 이는 훈련할 수 없고 이미지에 가우스 잡음을 점진적으로 추가하는 고정된 마르코프 체인을 기반으로 한다. 그런 다음, 디노이징 U-Net(\\epsilon_{\\theta}\\)을 사용하여 대응하는 잠재 확산 특징 \\(\\mathcal{F}\\in\\mathbb{R}^{H\\times W\\times C}\\)을 다음과 같이 추출한다:\n' +
      '\n' +
      '\\[\\mathcal{F}=\\epsilon_{\\theta}(\\mathbf{x}_{t},t), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(t\\)은 \\(t\\)번째 시간 단계를 나타낸다. 이전 연구들[37, 16, 45]은 이미 표현 추출을 위한 단일 순방향 패스의 효과를 입증했으며, 한 단계에서만 특징을 추출하는 것은 더 빠른 추론 속도와 더 나은 성능이라는 두 가지 장점을 가지고 있다. 확산 특징 \\(\\mathcal{F}\\)을 사용하여, 엔티티 마스크로부터 대응하는 좌표를 인덱싱함으로써 대응하는 엔티티 임베딩을 얻을 수 있다. 편의상, 평균 풀링은 해당 엔티티 임베딩을 처리하여 최종 임베딩 \\(\\{e_{1},e_{2},...,e_{k}\\}\\)을 얻는데 사용되며, 여기서 \\(k\\)은 엔티티의 수를 나타내며 각각은 \\(C\\)의 채널 크기를 갖는다.\n' +
      '\n' +
      '이러한 개체 임베딩을 해당 궤적 점들과 연관시키기 위해 그림 5와 같이 0 행렬 \\(\\mathbf{E}\\in\\mathbb{R}^{H\\times W\\times C}\\)을 직접 초기화한 후 궤적 시퀀스 점들을 기반으로 개체 임베딩을 삽입한다.\n' +
      '\n' +
      '그림 4: **Drag anything Framework. 이 아키텍처는 1) Entity Semantic Representation Extraction의 두 부분으로 구성된다. 확산 모델로부터의 잠재 특징들은 대응하는 엔티티 표현들로서 역할을 하기 위해 엔티티 마스크 인덱스들에 기초하여 추출된다. 2) DragAnything를 위한 메인 프레임워크. 개체들의 움직임을 제어하기 위해 해당 개체 표현과 2D 가우시안 표현을 이용한다.** 훈련 과정인 첫 번째 프레임의 개체 마스크를 이용하여 각 궤적 시퀀스 포인트에 대한 시작점으로 개체의 중심 좌표 \\(\\{(x^{1},y^{1}), (x^{2},y^{2}),...,(x^{k},y^{k})\\})를 추출한다. 이러한 중심 좌표 지수로, 엔티티 임베딩들을 대응하는 제로 행렬 \\(\\mathbf{E}\\)에 삽입함으로써 최종 엔티티 표현 \\(\\mathbf{\\hat{E}\\)을 얻을 수 있다(Deatils see Section 3.4).\n' +
      '\n' +
      '첫 번째 프레임에서 엔티티의 중심 좌표 \\(x^{1},y^{1}), (x^{2},y^{2}),...,(x^{k},y^{k})\\}\\)을 가지고, Co-Tracker[25]를 사용하여 이러한 포인트들을 추적하고 대응하는 움직임 궤적 \\(\\{\\{(x^{1}_{i},y^{1}_{i})\\}_{i=1}^{L},\\{(x^{2}_i},y^{2}_{i})\\}_{i=1}^{L},...,\\{(x^{k}_{i})\\}_{i=1}^{L},...,\\{(x^{k}},y^{k}_{i})\\}_{i=1}^{L})을 얻는다. 여기서 \\(L\\)은 비디오의 길이이다. 그런 다음 각 프레임에 대해 대응하는 개체 표현 \\(\\{\\mathbf{\\hat{E}_{i}}\\}_{i=1}^{L}\\)을 얻을 수 있다.\n' +
      '\n' +
      '#### 2.2.2 2차원 가우시안 표현 추출\n' +
      '\n' +
      '실체의 중심에 더 가까운 픽셀들은 전형적으로 더 중요하다. 제안된 개체 표현은 에지 픽셀의 가중치를 줄이면서 중심 영역에 더 집중되도록 하는 것을 목표로 한다. 2D 가우시안 표현은 도 2의 (c)에 예시된 바와 같이, 중심에 더 가까운 픽셀들이 더 큰 중량을 운반하면서, 이러한 측면을 효과적으로 향상시킬 수 있다. 점궤적 \\(\\{\\{1}_{i},y^{1}_{i})\\}_{i=1}^{L},\\{(x^{2}_{i},y^{2}_{i})\\}_{i=1}^{L},...,\\{(x^{k}_{i},y^{k}_{i})\\}_{i=1}^{L})을 엔코더로 처리한 후, 그림 5와 같이 중심영역 성능에 대한 향상된 초점을 얻기 위해 개체 표현과 병합한다.\n' +
      '\n' +
      '#### 2.2.3 엔코더 for Entity Representation and 2D Gaussian Map.\n' +
      '\n' +
      '그림 4와 같이, \\(\\mathcal{E}\\)로 표기된 인코더는 엔티티 표현 및 2D 가우시안 맵을 잠재 특징 공간으로 인코딩하기 위해 사용된다. 이 인코더에서는 4개의 컨볼루션 블록을 사용하여 해당 입력 유도 신호를 처리했으며, 여기서 각 블록은 2개의 컨볼루션 레이어와 1개의 SiLU 활성화 함수로 구성된다. 각 블록은 입력 특징 해상도를 2의 인자만큼 다운샘플링하여 최종 출력 해상도를 1/8로 만든다. 엔티티 및 가우시안 표현을 처리하기 위한 인코더 구조는 동일하며, 유일한 차이는 첫 번째 블록에서의 채널 수이며, 이는 두 표현에 대한 채널이 다를 때 변한다. 인코더를 통과한 후, Entity Representation과 2D Gaussian Map Representation의 잠재 특징을 동영상의 해당 잠재 잡음과 함께 추가하여 ControlNet[51]을 따른다:\n' +
      '\n' +
      '}_{i=1}^{L}=\\mathcal{E}(\\{\\mathbf{\\hat{E}_{i}}}}_{i=1}^{L})+\\mathcal{E}(\\{\\mathbf{G_{i}}}_{i=1}^{L})+\\{\\mathbf{Z_{i=1}^{L}, \\tag{2}\\}\n' +
      '\n' +
      '여기서 \\(\\mathbf{Z_{\\textit{i}}}\\)는 _i_번째 프레임의 잠재 잡음을 나타낸다. 그리고 잡음제거 3D Unet의 인코더에 특징\\(\\{\\mathbf{R_{\\textit{i}}}\\}_{i=1}^{L}\\)을 입력하여 서로 다른 해상도의 4가지 특징을 얻을 수 있으며, 이는 잠재조건 신호이다. 4개의 특징은 기초 모델의 잡음제거 3D Unet의 특징에 추가된다.\n' +
      '\n' +
      '### 훈련과 추론\n' +
      '\n' +
      '#### 3.4.1 Ground Truth Label Generation.\n' +
      '\n' +
      '학습 과정에서 그림 5와 같이 Entity Representation과 2D Gaussian의 해당 Trajectories를 생성해야 한다. 먼저, 각 Entity에 대해 해당 마스크를 이용하여 중심좌표 \\((x,y)\\)와 반지름 \\(r\\)을 구하여 해당 incircle circle을 계산한다. 그리고 Co-Tracker[25]를 이용하여 중심\\(\\{(x_{i},y_{i})\\}_{i=1}^{L}\\)의 해당 궤적을 구하여 해당 개체의 대표적인 움직임 궤적이 된다. 이러한 궤적점과 반경으로 각 프레임에서 해당하는 가우시안 분포값[7]을 계산할 수 있다. 엔티티 표현을 위해, 우리는 \\(r\\)의 반지름을 갖는 \\((x,y)\\) 좌표를 중심으로 하는 원에 대응하는 엔티티 임베딩을 삽입한다. 마지막으로, 모델을 학습하기 위해 Entity Representation과 2D Gaussian의 해당 궤적을 구한다.\n' +
      '\n' +
      '####3.4.2 손실함수.\n' +
      '\n' +
      '비디오 생성 작업에서 평균 제곱 오차(MSE)는 모델을 최적화하기 위해 일반적으로 사용된다. 대응하는 엔티티 표현\\(\\mathbf{\\hat{E}\\) 및 2D 가우시안 표현\\(\\mathbf{G}\\)이 주어지면, 목표는 다음과 같이 단순화될 수 있다:\n' +
      '\n' +
      '\\mathcal{L}_{\\theta}=\\sum_{i=1}^{L}\\mathbff{M}\\left|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{\\hat{E}}_{i},\\mathcal{E}_{\\theta}(\\mathbf{\\hat{E}}_{i}),\\mathcal{E}_{\\theta}(\\mathbf{G}_{i}\\mathbf{G}_{i}\\right\\right|\\right|_{2}^{2}\\,,\\tag{3}\\mathcal{E}_{\\theta}(\\mathbf{\\hat{E}}_{i}),\\mathcal{E}_{\\theta}(\\mathbf{G}_{i}\\right)\\right|\\right|_{2}^{2}\\,,\\tag{3}\\mathcal{E}_{\\theta}(\\mathbf{\\hat{E}}_{i}\n' +
      '\n' +
      '여기서 \\(\\mathcal{E}_{\\theta}\\)는 엔티티 및 2d 가우시안 표현을 위한 인코더를 나타낸다. \\ (\\mathbf{M}\\)는 각 프레임에서 이미지의 엔티티에 대한 마스크이다. 모델의 최적화 목적은 대상 객체의 움직임을 제어하는 것이다. 다른 객체나 배경에 대해서는 생성 품질에 영향을 주고 싶지 않습니다. 따라서, 본 논문에서는 마스크\\(\\mathbf{M}\\)를 사용하여 MSE 손실을 제한하여 최적화하고자 하는 영역만을 역전파시킨다.\n' +
      '\n' +
      '사용자-궤적 상호작용의 추론####3.4.3\n' +
      '\n' +
      '무엇이든 드래그하는 것은 사용자 친화적입니다. 추론 동안, 사용자는 SAM[26]으로 제어하고자 하는 영역을 선택하기 위해 클릭하기만 하면 되고, 그 후 합리적인 궤적을 형성하기 위해 영역 내의 임의의 픽셀을 드래그하면 된다. 그런 다음 드래그 아무거나 원하는 모션에 해당하는 비디오를 생성할 수 있습니다.\n' +
      '\n' +
      '그림 5: **지상 진실 생성 절차의 설명.** 훈련 프로세스 동안, 우리는 엔티티-레벨 주석들을 갖는 비디오 분할 데이터세트들로부터 지상 진실 라벨들을 생성한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experiment Settings\n' +
      '\n' +
      '**구현 상세.** 우리의 DragAnything는 SVD(Stable Video Diffusion) [3] 아키텍처와 가중치를 기반으로 하며, 이는 \\(320\\times 576\\)의 해상도로 25개의 프레임을 생성하도록 훈련되었다. 모든 실험은 테슬라 A100 GPU가 있는 PyTorch에서 수행된다. AdamW [27]은 학습률이 1e-5인 총 100k\\(100k\\) 훈련 단계의 최적화 도구이다.\n' +
      '\n' +
      '**평가 메트릭.** 우리의 접근법을 종합적으로 평가하기 위해 인간 평가 및 자동 스크립트 메트릭 관점 모두에서 평가를 수행했다. MotionControl[42]에 이어, 두 가지 유형의 자동 스크립트 메트릭을 사용했다: 1) 비디오 품질의 평가_: FID(Frechet Inception Distance) [36] 및 FVD(Frechet Video Distance) [39]를 사용하여 시각적 품질과 시간적 일관성을 평가했다. 2) _객체 움직임 제어 성능 평가_: 객체 움직임 제어를 평가하기 위해 예측된 객체 궤적과 지상진실 객체 궤적 사이의 유클리드 거리(ObjMC)를 사용하였다. 또한, 사용자 연구를 위해 비디오 미학을 고려하여 Google Image에서 30개의 이미지를 수집하여 해당 포인트 궤적 및 해당 마스크와 함께 주석을 달았다. 3명의 전문 평가자가 비디오 품질과 모션 매칭의 두 가지 측면에서 합성된 비디오에 투표해야 한다. 그림 6과 그림 9의 비디오는 이 30가지 사례에서 샘플링된다.\n' +
      '\n' +
      '**데이터셋.** 궤적 유도 비디오 생성 태스크에 대한 평가는 테스트 세트 내의 각 비디오의 움직임 궤적을 입력으로 요구한다. 이러한 주석이 달린 데이터를 얻기 위해 VIPSeg [30] 검증 세트를 테스트 세트로 채택했다. 비디오의 첫 번째 프레임에서 각 객체의 인스턴스 마스크를 이용하여 중심 좌표를 추출하고, Co-Tracker[25]를 사용하여 이 점을 추적하고 해당 움직임 궤적을 지표 진리로 획득하였다. FVD는 동일한 해상도와 길이를 갖는 비디오가 필요하므로, VIPSeg val 데이터셋을 평가하기 위해 \\(256\\times 256\\)의 해상도와 14프레임의 길이로 재조정하였다. 이에 따라 우리는 또한 VIPSeg [30] 훈련 세트를 훈련 데이터로 활용하고 Co-Tracker를 주석으로 사용하여 해당 운동 궤적을 획득했다.\n' +
      '\n' +
      '### 최신 방법과의 비교\n' +
      '\n' +
      '생성된 비디오는 4가지 측면에서 비교된다: 1) FID를 이용한 비디오 품질 평가[36]. 2) FVD와의 시간 일관성 평가[39] 3) ObjMC를 이용한 객체 움직임 평가. 4) Human Voting을 이용한 사용자 연구.\n' +
      '\n' +
      '**VIPSeg val에 대한 비디오 품질의 평가** 표 1은 VIPSeg val 세트에 대한 FID와 비디오 품질의 비교를 제시한다. 우리는 다른 조건들이 동일하도록 제어(베이스 아키텍처)하고, 우리의 방법과 DragNUWA 사이의 성능을 비교한다. 우리의 DragAnything의 FID는 33.5에 도달하여 현재 SOTA 모델 DragNUWA보다 6.3(33.5 \\(vs.\\) 39.8로 크게 능가했다. 그림 6과 그림 9는 또한 DragAnything에서 합성된 비디오가 예외적으로 높은 비디오 품질을 나타냄을 보여준다.\n' +
      '\n' +
      '**VIPSeg val.** FVD [39]에 대한 시간 코히어런스의 평가는 생성된 비디오 내의 특징 분포들을 지상 진실 비디오 내의 특징 분포들과 비교함으로써 생성된 비디오들의 시간 코히어런스를 평가할 수 있다. 표 1과 같이 FVD의 비교를 제시한다. DragNUWA(519.3 FVD)의 성능과 비교하여, 우리의 DragAnything는 24.5의 현저한 개선으로 _i.e._ 494.8의 우수한 시간적 일관성을 달성했다.\n' +
      '\n' +
      '**VIPSeg val.** Following MotionC-trl[42], ObjMC는 예측된 진리 궤적과 지상 진리 궤적 사이의 유클리드 거리를 계산하여 움직임 제어 성능을 평가하는 데 사용된다. 표 1은 VIPSeg val 세트에 대한 ObjMC의 비교를 나타낸다. DragNUWA와 비교하여, 우리의 DragAnything는 18.9의 개선으로 새로운 최첨단 성능 305.7을 달성했다. 그림 7은 두 방법 간의 시각화 비교를 제공한다.\n' +
      '\n' +
      '**동작 제어 및 비디오 품질에 대한 사용자 연구.** 도 8은 동작 제어 및 비디오 품질에 대한 사용자 연구에 대한 비교를 제시한다. 모델은 모션 제어 및 비디오 품질에 대한 인간 투표에서 DragAnything를 각각 26% 및 12% 능가합니다. 또한 그림 7에서 시각적 비교와 그림 6에서 더 많은 시각화를 제공한다. 본 알고리즘은 움직임 제어에 대한 보다 정확한 이해와 구현을 가지고 있다.\n' +
      '\n' +
      '그림 6: **DragAnything에 대한 시각화** 제안된 DragAnything는 개체 수준에서 객체의 움직임을 정확하게 제어할 수 있어 고품질의 동영상을 제작할 수 있다. 20번째 프레임의 픽셀 모션에 대한 시각화는 Co-Track[25]에 의해 얻어진다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '개체 표현과 2D 가우시안 표현은 모두 우리 작업의 핵심 구성 요소이다. 우리는 다른 조건을 일정하게 유지하고 해당 조건부 임베딩 특징만 수정한다. 표 2는 두 가지 표현에 대한 절제 연구를 제시한다.\n' +
      '\n' +
      '** Entity Representation의 효과\\(\\mathbf{\\hat{E}}\\.)** Entity Representation\\(\\mathbf{\\hat{E}}\\)의 영향을 조사하기 위해, 이 표현이 최종 임베딩에 포함되는지 여부를 결정함으로써 성능의 변화를 관찰한다(수학식 2). 영상 생성 시 조건 정보\\(\\mathbf{\\hat{E}}\\)가 객체 움직임에 주로 영향을 미치기 때문에 ObjMC를 비교해야 하는 반면, FVD와 FID 메트릭은 시간적 일관성과 전반적인 영상 품질에 중점을 둔다. Entity Representation \\(\\mathbf{\\hat{E}\\)을 사용하여 모델의 ObjMC는 318.4에 이르는 상당한 개선(92.3)을 달성했다.\n' +
      '\n' +
      '**2D Gaussian Representation의 효과.** Entity Representation과 유사하게, 2D Gaussian Representation이 최종 임베딩에 포함되는지 여부를 판단하여 ObjMC 성능의 변화를 관찰한다. 2차원 가우시안 표현은 71.4의 개선으로 339.3에 도달하였다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c} Method & Base Arch. & ObjMC\\(\\downarrow\\) & FVD\\(\\downarrow\\) & FID\\(\\downarrow\\) & Venue/Date \\\\ \\hline DragNUWA [49] & SVD [3] & 324.6 & 519.3 & 39.8 & arXiv, Aug. 2023 \\\\ DragAnything (Ours) & SVD [3] & **305.7** & **494.8** & **33.5** & - \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **VIPSeg val \\(256\\times 256\\)[30].** 다른 관련 저작물들(_e.g.,_Motionctrl[42])이 SVD[3]에 기초한 소스 코드를 릴리즈하지 않았기 때문에 DragNUWA와의 비교만을 수행하였다.\n' +
      '\n' +
      '도 7: **Visualization Comparison with DragNUWA.** DragNUWA는 외관의 왜곡(첫 번째 행), 통제 불능 하늘과 선박(세 번째 행), 부정확한 카메라 모션(다섯 번째 행)으로 이어지는 반면, DragAnything는 정밀한 모션 제어를 가능하게 한다.\n' +
      '\n' +
      '성능은 Entity와 2D Gaussian Representations를 모두 사용하여 305.7을 달성했을 때 가장 높았으며, 이러한 현상은 두 Representation이 상호 보강 효과가 있음을 시사한다.\n' +
      '\n' +
      '**손실 마스크 M.** 표 3은 손실 마스크 **M**에 대한 절제를 나타낸다. 손실 마스크 **M**를 사용하지 않는 경우, 전체 이미지의 각 픽셀에 대해 MSE 손실을 직접 최적화한다. 손실 마스크는 ObjMC의 약 5.4로 특정 이득을 가져올 수 있다.\n' +
      '\n' +
      '다양한 동작제어에 대한### 토론\n' +
      '\n' +
      '당사의 DragAnything는 매우 유연하고 사용자 친화적이며 비디오에 등장하는 모든 엔티티에 대한 다양한 모션 제어를 지원합니다. 본 절에서는 해당 동작 제어에 대해 4가지 유형으로 분류하여 논의하고자 한다.\n' +
      '\n' +
      '**전경을 위한 움직임 제어** 도 9의 (a)에 도시된 바와 같이, 전경 움직임 제어는 가장 기본적이고 일반적으로 사용되는 동작이다. 태양과 말 모두 전경에 속한다. 우리는 SAM[26]으로 제어되어야 하는 해당 영역을 선택하고, 그 영역 내의 임의의 지점을 드래그하여 객체에 대한 모션 제어를 달성한다. 드래그 무어는 태양과 말의 움직임을 정밀하게 제어할 수 있음을 관찰할 수 있다.\n' +
      '\n' +
      '**움직임 제어 For Background.** 전경에 비해 배경은 일반적으로 더 제어하기 어려운데, 이는 구름, 별 모양의 하늘과 같은 배경 요소의 모양이 예측할 수 없고 특성화하기 어렵기 때문이다. 도 9의 (b)는 두 가지 시나리오에서 비디오 생성을 위한 배경 모션 제어를 보여준다. 드래그먼트는 클라우드 상의 한 점을 드래그함으로써 전체 클라우드 레이어의 이동을 오른쪽 또는 더 멀리 제어할 수 있다.\n' +
      '\n' +
      '**전경과 배경에 대한 동시 모션 제어**끌기 무엇이든 전경 및 배경 모두를 동시에 제어할 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} Entity Rep. & Gaussian Rep. & ObjMC\\(\\downarrow\\) & FVD\\(\\downarrow\\) & FID\\(\\downarrow\\) \\\\ \\hline  & & 410.7 & 496.3 & 34.2 \\\\ ✓ & & 318.4 & 494.5 & 34.1 \\\\ ✓ & ✓ & 339.3 & 495.3 & 34.0 \\\\ ✓ & ✓ & **305.7** & 494.8 & 33.5 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **Entity에 대한 절제 및 2D Gaussian Representation.** 양자의 조합이 가장 큰 이점을 산출한다.\n' +
      '\n' +
      '도 8: **동작 제어 및 비디오 품질을 위한 사용자 연구**. 드래그먼트는 모션 제어 및 비디오 품질 측면에서 우수한 성능을 달성했습니다.\n' +
      '\n' +
      '도 9의 (c)에 도시되어 있다. 예를 들어, 세 개의 픽셀을 드래그함으로써 구름 층이 오른쪽으로 이동하고 해가 위로 떠오르고 말이 오른쪽으로 이동하는 동작 제어를 동시에 달성할 수 있다.\n' +
      '\n' +
      '**카메라 모션 컨트롤.** 비디오 내의 엔티티들에 대한 모션 컨트롤 외에도 DragAnything는 또한 도 9(d)에 도시된 바와 같이 줌인 및 줌아웃과 같은 카메라 모션에 대한 일부 기본 컨트롤을 지원한다. 사용자는 단순히 전체 이미지를 선택한 다음, 대응하는 줌인 또는 줌아웃을 달성하기 위해 4개의 포인트를 드래그하기만 하면 된다. 추가적으로, 사용자는 임의의 지점을 드래그함으로써 전체 카메라의 상하좌우 이동을 제어할 수도 있다.\n' +
      '\n' +
      '도 9: **DragAnything로부터의 다양한 Motion Control.**DragAnything는 전경, 배경, 카메라의 제어 등 다양한 모션 제어를 달성할 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 비디오 생성 작업에서 현재 궤적 기반 모션 제어 접근 방식을 재평가하고, 두 가지 새로운 통찰을 소개한다: 1) 객체 상의 궤적 포인트는 엔티티를 적절하게 표현할 수 없다. 2) 궤적점 표현 패러다임에 대해, 드래그 포인트에 더 가까운 픽셀들은 더 강한 영향력을 발휘하여 더 큰 동작을 초래한다. 이 두 가지 기술적 과제를 해결하기 위해 우리는 각 개체를 나타내기 위해 확산 모델의 잠재 특징을 활용하는 DragAnything를 제시한다. 제안된 엔티티 표현은 임의의 객체를 표현할 수 있는 오픈 도메인 임베딩으로서, 배경을 포함한 다양한 엔티티들에 대한 모션의 제어를 가능하게 한다. 광범위한 실험을 통해 우리의 DragAnything가 사용자 스터디를 위한 SOTA 성능을 달성하여 기존 최신 기술(DragNUWA)을 인간 투표에서 26% 능가함을 알 수 있다.\n' +
      '\n' +
      '## 6 Appendix\n' +
      '\n' +
      '잠재적 부정적 영향에 대한 토론\n' +
      '\n' +
      '한 가지 잠재적인 부정적인 영향은 모델이 사회적 편향을 포함할 수 있는 기존 데이터 세트에서 학습하기 때문에 훈련 데이터에 존재하는 편향을 강화할 가능성이다. 또한, 생성된 콘텐츠가 오용될 위험이 있으며, 이는 오도하거나 부적절한 시각적 자료의 생성으로 이어진다. 또한, 특히 개인의 명시적 동의 없이 개인을 포함하는 비디오를 생성할 때 개인 정보 보호 문제가 발생할 수 있다. 다른 비디오 생성 기술과 마찬가지로 이러한 잠재적인 부정적인 영향을 완화하고 윤리적 사용을 보장하기 위한 경계 및 책임 있는 구현이 필요하다.\n' +
      '\n' +
      '### 제한과 잘못된 사례 분석\n' +
      '\n' +
      '우리의 DragAnything는 유망한 성능을 보여주었지만, 여전히 개선할 수 있는 몇 가지 측면이 있는데, 이는 현재의 다른 궤적 기반 비디오 생성 모델에 공통적이다: 1) 현재의 궤적 기반 모션 제어는 2D 차원에 제한되고, 누군가가 돌아서거나 더 정밀한 신체 회전을 제어하는 것과 같은 3D 장면에서의 모션을 처리할 수 없다. 2) 현재 모델들은 기초 모델의 성능에 의해 제약되고,\n' +
      '\n' +
      '그림 10: **DragAnything에 대한 나쁜 사례.** DragAnything는 특히 더 큰 동작을 제어할 때 여전히 몇 가지 나쁜 사례가 있다.\n' +
      '\n' +
      '안정적인 비디오 확산[3], 그리고 그림 10과 같이 매우 큰 움직임으로 장면을 생성할 수 없다. 비디오 프레임의 첫 번째 열에서 공룡의 다리는 현실 세계의 제약을 준수하지 않는다는 것은 명백하다. 5개의 다리와 몇 가지 이상한 동작이 있는 몇 개의 프레임이 있습니다. 두 번째 줄에서 독수리의 날개가 흐려지는 것과 비슷한 상황이 발생한다. 이는 기초 모델의 생성 능력을 초과하는 과도한 모션으로 인해 비디오 품질이 붕괴될 수 있다. 이 두 가지 문제를 해결할 수 있는 몇 가지 잠재적인 해결책이 있습니다. 첫 번째 도전을 위해, 실현 가능한 접근법은 깊이 정보를 2D 궤적에 통합하고, 3D 궤적 정보로 확장함으로써, 3D 공간에서의 객체 모션의 제어를 가능하게 하는 것이다. 두 번째 도전의 경우, 더 크고 더 강력한 모션 생성 능력을 지원하기 위해 더 강력한 기초 모델의 개발이 필요하다. 예를 들어, 오픈AI인 SORA의 최신 텍스트 대 비디오 기반을 활용하는 것은 의심할 여지 없이 생성된 비디오의 품질을 크게 향상시킬 수 있는 잠재력을 가지고 있다. 또한 그림 11과 같이 참고자료에 더 정교한 영상 사례를 제공하였는데, 보다 많은 시각화를 위해 영상들이 보충자료에 있는 영상들과 더 유사함을 알 수 있었다.\n' +
      '\n' +
      '그림 11: **DragAnything에 대한 더 많은 시각화**\n' +
      '\n' +
      'GIF 형식으로 같은 디렉토리의 DragAnything.html를 참조하십시오. 클릭하기만 하면 열 수 있습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1][https://www.pika.art/](https://www.pika.art/)[2] Ardino, P., De Nadai, M., Lepri, B., Ricci, E., Lathuiliere, S.: Click to move: Controlling video generation with sparse motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14749-14758 (2021)\n' +
      '* [3] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023)\n' +
      '* [4] Blattmann, A., Milbich, T., Dorkenwald, M., Ommer, B.: ipoke: Poking a still image for controlled stochastic video synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14707-14717 (2021)\n' +
      '* [5] Blattmann, A., Milbich, T., Dorkenwald, M., Ommer, B.: Understanding object dynamics for interactive image-to-video synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5171-5181 (2021)\n' +
      '* [6] Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S., Kreis, K.: Align your latents: High-resolution video synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22563-22575 (2023)\n' +
      '* [7] Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation using part affinity fields. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7291-7299 (2017)\n' +
      '* [8] Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., et al.: Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512 (2023)\n' +
      '* [9] Chen, T.S., Lin, C.H., Tseng, H.Y., Lin, T.Y., Yang, M.H.: Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404 (2023)\n' +
      '* [10] Chen, W., Wu, J., Xie, P., Wu, H., Li, J., Xia, X., Xiao, X., Lin, L.: Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840 (2023)\n' +
      '* [11] Chen, X., Huang, L., Liu, Y., Shen, Y., Zhao, D., Zhao, H.: Anydoor: Zero-shot object-level image customization. arXiv preprint arXiv:2307.09481 (2023)\n' +
      '* [12] Dai, X., Hou, J., Ma, C.Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., et al.: Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807 (2023)\n' +
      '* [13] Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure and content-guided video synthesis with diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7346-7356 (2023)\n' +
      '* [14] Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S.S., Shah, A., Yin, X., Parikh, D., Misra, I.: Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709 (2023)* [15] Gu, Y., Wang, X., Wu, J.Z., Shi, Y., Chen, Y., Fan, Z., Xiao, W., Zhao, R., Chang, S., Wu, W., et al.: Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [16] Gu, Y., Zhou, Y., Wu, B., Yu, L., Liu, J.W., Zhao, R., Wu, J.Z., Zhang, D.J., Shou, M.Z., Tang, K.: Videoswap: Customized video subject swapping with interactive semantic point correspondence. arXiv preprint arXiv:2312.02087 (2023)\n' +
      '* [17] Guo, Y., Yang, C., Rao, A., Agrawala, M., Lin, D., Dai, B.: Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv preprint arXiv:2311.16933 (2023)\n' +
      '* [18] Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725 (2023)\n' +
      '* [19] Hao, Z., Huang, X., Belongie, S.: Controllable video generation with sparse trajectories. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7854-7863 (2018)\n' +
      '* [20] He, Y., Liu, J., Wu, W., Zhou, H., Zhuang, B.: Efficientdm: Efficient quantization-aware fine-tuning of low-bit diffusion models. arXiv preprint arXiv:2310.03270 (2023)\n' +
      '* [21] He, Y., Liu, L., Liu, J., Wu, W., Zhou, H., Zhuang, B.: Ptqd: Accurate post-training quantization for diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [22] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022)\n' +
      '* [23] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems **33**, 6840-6851 (2020)\n' +
      '* [24] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video diffusion models. arXiv:2204.03458 (2022)\n' +
      '* [25] Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.: Cotracker: It is better to track together. arXiv:2307.07635 (2023)\n' +
      '* [26] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint arXiv:2304.02643 (2023)\n' +
      '* [27] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n' +
      '* [28] Ma, W.D.K., Lewis, J., Kleijn, W.B.: Trailblazer: Trajectory control for diffusion-based video generation. arXiv preprint arXiv:2401.00896 (2023)\n' +
      '* [29] Ma, Y., He, Y., Cun, X., Wang, X., Shan, Y., Li, X., Chen, Q.: Follow your pose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186 (2023)\n' +
      '* [30] Miao, J., Wang, X., Wu, Y., Li, W., Zhang, X., Wei, Y., Yang, Y.: Large-scale video panoptic segmentation in the wild: A benchmark. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21033-21043 (2022)\n' +
      '* [31] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)\n' +
      '* [32] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 **1**(2), 3 (2022)* [33] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [34] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: MICCAI (2015)\n' +
      '* [35] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems **35**, 36479-36494 (2022)\n' +
      '* [36] Seitzer, M.: pytorch-fid: FID Score for PyTorch. [https://github.com/mseitzer/pytorch-fid](https://github.com/mseitzer/pytorch-fid) (2020)\n' +
      '* [37] Tang, L., Jia, M., Wang, Q., Phoo, C.P., Hariharan, B.: Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [38] Tim, B., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Troy, L., Luhman, E., Ng, C.W.Y., Wang, R., Ramesh, A.: Video generation models as world simulators (2024)\n' +
      '* [39] Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717 (2018)\n' +
      '* [40] Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., Huang, T.: Seggpt: Segmenting everything in context. arXiv preprint arXiv:2304.03284 (2023)\n' +
      '* [41] Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al.: Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103 (2023)\n' +
      '* [42] Wang, Z., Yuan, Z., Wang, X., Chen, T., Xia, M., Luo, P., Shan, Y.: Motionctrl: A unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641 (2023)\n' +
      '* [43] Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7623-7633 (2023)\n' +
      '* [44] Wu, W., Li, Z., He, Y., Shou, M.Z., Shen, C., Cheng, L., Li, Y., Gao, T., Zhang, D., Wang, Z.: Paragraph-to-image generation with information-enriched diffusion model. arXiv preprint arXiv:2311.14284 (2023)\n' +
      '* [45] Wu, W., Zhao, Y., Chen, H., Gu, Y., Zhao, R., He, Y., Zhou, H., Shou, M.Z., Shen, C.: Datasetdm: Synthesizing data with perception annotations using diffusion models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [46] Wu, W., Zhao, Y., Shou, M.Z., Zhou, H., Shen, C.: Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1206-1217 (2023)\n' +
      '* [47] Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z., Jiang, Y.G.: A survey on video diffusion models. arXiv preprint arXiv:2310.10647 (2023)\n' +
      '* [48] Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., Luo, P.: Raphael: Text-to-image generation via large mixture of diffusion paths. arXiv preprint arXiv:2305.18295 (2023)\n' +
      '* [49] Yin, S., Wu, C., Liang, J., Shi, J., Li, H., Ming, G., Duan, N.: Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089 (2023)* [50] Zhang, D.J., Wu, J.Z., Liu, J.W., Zhao, R., Ran, L., Gu, Y., Gao, D., Shou, M.Z.: Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818 (2023)\n' +
      '* [51] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3836-3847 (2023)\n' +
      '* [52] Zhang, S., Wang, J., Zhang, Y., Zhao, K., Yuan, H., Qin, Z., Wang, X., Zhao, D., Zhou, J.: I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145 (2023)\n' +
      '* [53] Zhang, Y., Wei, Y., Jiang, D., Zhang, X., Zuo, W., Tian, Q.: Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077 (2023)\n' +
      '* [54] Zhao, R., Gu, Y., Wu, J.Z., Zhang, D.J., Liu, J., Wu, W., Keppo, J., Shou, M.Z.: Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465 (2023)\n' +
      '* [55] Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., Feng, J.: Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018 (2022)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>