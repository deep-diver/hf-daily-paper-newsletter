<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Deep neural networks (DNNs) have transformed society with significant accuracy improvements for tasks including speech recognition [87], image classification [22, 37, 40, 79, 82, 83], machine translation [21], autonomous agents [41], language processing [14, 68] and text generation [9]. This tremendous, transformative effect has been enabled by a virtuous synergy of (1) better hardware systems, (2) larger datasets, and (3) improved ML structures and algorithms that further benefit from more efficient hardware and larger datasets. This is especially true for Transformers, which have become popular for a wide range of tasks [8] and have shown considerable strides in artificial general intelligence [73]. Transformers have had exponential growth in datasets and model size: parameters have increased from 340 million in BERT [14] to 540 billion in PALM [12]. Accordingly, their memory and computational demands have also increased, making them increasingly reliant on distributed techniques: multiple accelerators (e.g., GPUs) pooling their collective memory capacities and compute capabilities to collaboratively execute a DNN. However, the resulting communication between devices has become a significant proportion of their execution time and has limited the scaling efficiency with increasing device count [36, 49, 64].\n' +
      '\n' +
      'Transformers frequently use two key distributed techniques in conjunction: data parallelism (DP) and model parallelism (MP). DP parallelizes training by partitioning the dataset and replicating the model across devices, requiring communication and aggregation (_all-reduce_) of gradients. Conversely, MP partitions large models that cannot fit in a single device\'s memory. Tensor-parallelism (TP), a type of MP, requires an all-reduce of layer outputs between devices as well. Among these distributed techniques, TP\'s communication typically lies on the critical path of model execution, as shown in Figure 1(a) and can be a significant proportion of runtime (\\(\\sim\\)45% [64]), resulting in a sub-linear increase in throughput as the number of devices increases.\n' +
      '\n' +
      'While some prior works have sped up communication by up to \\(2\\times\\) with _in-network computation_, they are topology-dependent (requiring switches) and further, cannot eliminate serialized communication from the critical path [36]. Distributed techniques with abundant coarse-grained independent compute (e.g., DP) often overlap (and hide) communication with independent computations to improve efficiency. Although serialized communication scenarios also offer such potential, they require a fine-grained overlap of computation and communication, which presents its own challenges. Enabling their fine-grained overlap in current systems either requires expensive fine-grained synchronization [25] or changes to matrix multiplication (GEMMs) kernels which can be disruptive to GPU software infrastructure [86] (Section 3.1). Furthermore, overlapped compute and communication contend for both compute units and memory bandwidth, reducing overlap\'s efficacy [25, 86] (Section 3.2). Prior approaches that reduce contention only address coarse-grained overlap of compute and communication in cases like DP and lack support for fine-grained overlap in serialized collectives [71]. Moreover, they rely on dedicated accelerators. _Therefore, no existing technique achieves a transparent overlap of serialized communication with computation while minimizing resource contention._\n' +
      '\n' +
      'To overcome these, we propose T3 (Figure 1(b)). T3 _transparently_ fuses producer operations with the subsequent communication by _configuring the producer\'s output address space_ to initiate communication directly on the producer\'s store, requiring minimal application changes. It uses a lightweight and programmable hardware _tracker_ to track the producer/communication progress and _triggers_ communication using pre-programmed DMA commands, requiring no additional GPU compute resources for communication. Furthermore, to reduce contention for memory bandwidth between the producer and communication, T3 leverages recently proposed compute-enhanced memories [34, 38] to atomically update memory on stores, thus reducing memory traffic due to communication-related reductions. Finally, T3 employs a simple yet effective arbitration policy between the producer and communication memory streams to minimize any remaining contention. Overall, T3 transparently overlaps serialized communication with minimal resource contention. This improves compute and network utilization, and in turn, can enable better throughput scaling with increasing device count. We make the following key contributions:\n' +
      '\n' +
      '* We propose T3 which enables fine-grained overlap of serialized communication with its producer computation whilst lowering application impact and managing compute and memory interference.\n' +
      '* To manage application impact, T3 configures the producer\'s output address space mapping to initiate communication on stores, requiring minor modifications to the producer kernels.\n' +
      '* To manage compute resources contention, T3 uses a lightweight programmable tracker that tracks producer progress and triggers communication using existing DMA engines requiring no additional compute resources.\n' +
      '* Finally, to tackle memory bandwidth contention between computation and communication, T3 harnesses emerging near-memory compute technology to reduce data movement due to communication. Further, T3 also devices a simple yet effective memory controller arbitration policy to better interleave computation and communication memory traffic.\n' +
      '* Similar to prior work [32], we extend Accel-Sim [33] to accurately model multi-GPU systems (6% error). Our results show that T3 speeds up sliced Transformersub-layers from models like Mega-GPT-2 (Mega et al., 2017) and T-NLG (Meng et al., 2018) by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%). Furthermore, T3\'s benefits persist as models scale: geomean 29% for sublayers in \\(\\sim\\)500-billion parameter models, PALM and MT-NLG. Overall, T3 speeds up model training by up to 12% and inference (prompt phase) by up to 15%.\n' +
      '\n' +
      '## 2. Background & Motivation\n' +
      '\n' +
      '### Transformers & Need for Distributed Computing\n' +
      '\n' +
      'Transformers (Zhou et al., 2017) have become the general-purpose architecture for a wide range of tasks/domains (e.g., for text, image) (Beng et al., 2016). Models use the Transformer _encoder_ or _decoder_ as their basic building block, each with an attention sub-layer and a fully connected (FC) sub-layer (as shown in Figure 2(a)) which manifest as matrix multiplication operations (GEMMs). Each layer also contains a few residual connections and layer normalizations which manifest as element-wise operations, and are often fused (Han et al., 2016; Chen et al., 2017; Chen et al., 2018; Chen et al., 2019) with the GEMMs. As shown in Figure 2(b), these GEMMs entail multiplication of layers\' weight matrices by an input matrix (with each vector representing an input token). During training, the input matrices contain multiple tokens from one or more (if batched) input sequence(s). During inference, there are two execution phases: a _prompt_ phase to process all tokens in the input sequence(s) and a _token generation_ phase to iteratively process and generate one token at a time for each input sequence (Zhou et al., 2017). The prompt phase operations are similar to those in training, while the generation phase has GEMMs with small input matrices or matrix-vector operations (GEMVs) if there is no batching.\n' +
      '\n' +
      'Most Transformer models\' memory capacity requirements exceed a single device. Thus, they employ distributed techniques and use multiple accelerators (e.g., GPUs) collaboratively. Furthermore, the aggregate computational capacity of multiple devices also accelerates training by enabling the processing of large input datasets in parallel. Thus, since Transformers and their datasets (usually large corpora of unlabeled text) have increased by several orders of magnitude in size, distributed techniques are often mandatory and increasingly require many devices. This scaling will only increase for future models.\n' +
      '\n' +
      '### Distributed Techniques & Associated Collectives\n' +
      '\n' +
      'Transformers employ many distributed techniques, each with associated _communication_ between devices. Data parallelism (DP) trains model replicas on multiple devices, each on a disjoint set of data, and requires a reduction of gradients every iteration. Tensor parallelism (TP) (Mega et al., 2017) and pipeline parallelism (e.g., GPipe) (Krizhevsky et al., 2014) slice the model across multiple devices. While the former slices each layer requiring activation reduction, the latter partitions the model layer-wise requiring peer-to-peer transfer of activations. ZeRO-based optimizations (Zhou et al., 2017) also slice model weights or offload them to slower but larger (e.g., CPU) memories, and require them to be gathered before layer executions. Finally expert parallelism (Zhou et al., 2019) partitions mixture-of-expert (MoE) models (Han et al., 2016; Chen et al., 2017) such that each device hosts a single expert and requires exchange of input data based on input-to-expert mapping. These communication patterns are handled by _collectives_ such as _reduce-scatter_, _all-reduce_, _all-gather_, _all-to-all_. While most of this communication can be hidden by independent compute operations (Zhou et al., 2017; Chen et al., 2018; Chen et al., 2019), albeit with some resource contention (Zhou et al., 2019; Chen et al., 2019), the all-reduce in TP is not (detailed in Section 2.4). Thus, we focus on all-reduce in TP and discuss other techniques/collectives in Sections 7.1 and 7.2.\n' +
      '\n' +
      '### All-Reduce & Ring Implementations\n' +
      '\n' +
      'The all-reduce (AR) collective reduces (element-wise sums) arrays from each of the devices. Although there are multiple implementations of AR, one of the most bandwidth-efficient, and thus most commonly used, implementations is _ring-AR_. Ring-AR consists of a ring reduce-scatter (ring-RS) followed by a ring all-gather (ring-AG). As shown in Figure 3, ring-RS is done in multiple steps. The arrays are chunked on each device, and during each step, all devices send their copy of a _unique_ chunk to their neighbor in the ring. The devices then\n' +
      '\n' +
      'Figure 2. (a) Transformer (b) Fully-connected (FC) layer (c) Tensor-sliced FC layer with all-Reduce on the critical path.\n' +
      '\n' +
      'reduce their local copy of the chunk with the received copy and forward it to their neighbor in the next step. With \\(N\\) devices and the array chunked \\(N\\) ways, this process requires \\(N-1\\) steps until each device has a completely reduced copy of one chunk. Ring-AG is similar but does not have reductions; it also requires \\(N-1\\) steps until each device has all the reduced chunks. In the remainder of the paper, we use AR, RS, and AG to refer to their ring implementations and discuss other implementations in Section 7.1.\n' +
      '\n' +
      'All-Reduce is on the Critical Path & can be Large Transformers require tensor parallelism (TP) (Tran et al., 2017) to increase the aggregate memory capacity available to them. However, it requires ARs on the critical path (between layers). Figures 2(b) and 2(c) show the FC sub-layer\'s original operations versus the operations when sliced across two devices (TP=2 in Figure 2(c)). Each device (dotted box) only has a slice of the weights. Since the GEMM corresponding to the second sliced weight only generates a partial output, it requires an AR before the next layer executes (highlighted by "Sliced GEMM\\(\\rightarrow\\)AR"). These GEMM and AR operations execute as separate kernels and are serialized.\n' +
      '\n' +
      'These serialized ARs can become a bottleneck. Figure 4 shows the execution time breakdown of Transformers between "Sliced GEMM\\(\\rightarrow\\)AR" and other operations for multiple current and futuristic Transformers (setup detailed in Section 5.1.2, 5.2). For large models (e.g., Mega-GPT-2, T-NLG) we consider 8- and 16-device TP. For very large models (e.g., PALM, MT-NLG) we consider 32-way slicing, and for futuristic ones with one and ten trillion parameters, we consider 64-way sharding. The increasing TP slicing is necessary because these models\' larger sizes cannot fit in 16 GPUs (Zhu et al., 2017) and the increased slicing is also enabled by nodes with larger device counts (Zhu et al., 2017; Wang et al., 2018). Like prior work (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), we find that communication is a considerable fraction of the overall runtime: Megatron-GPT-2 (Mega-GPT-2) and T-NLG spend up to 34% and 43% of their training and inference (prompt phase) time on communication. These trends also hold for the very large and futuristic Transformers: communication can be up to 46% and 44% of their runtime, respectively. Additionally, since compute FLOPS scales much more than network bandwidth (Han et al., 2017), these proportions will only increase in the future. For example, if the GEMMs become 2\\(\\times\\) faster, communication increases to 75% of model execution time - making scaling to multiple devices extremely inefficient and potentially leaving GPUs idle while communication happens. Thus, addressing serialized AR is critical to Transformer scaling.\n' +
      '\n' +
      '### Enabling Compute-Communication Overlap\n' +
      '\n' +
      'Overlapping collective kernels with independent compute kernels has been key to scaling DNNs in other distributed approaches (e.g., DP, GPipe (Mega-GPT-2, 2018)). While TP does not have independent kernels to overlap AR with, we observe that it can benefit from a _fine-grained overlap_ with the producer GEMM itself. Transformer GEMMs have large outputs, which are tiled/blocked and require many GPU workgroups (WGs) to complete. Consequently, a GEMM cannot always execute all its WGs concurrently on the limited number of GPU compute units (CUs). Thus, a GEMM executes and generates output in multiple _stages_, where each stage is a set of WGs that the CUs can accommodate. This holds even for sliced GEMMs that require AR. As shown in Figure 5, GEMMs in TP are sliced in the \\(K\\) (or dot-product) dimension which decreases compute per WG, but the output size, WG count, and WG stages remain the same. We utilize this observation to enable fine-grained overlap: communication of one stage\'s output data can be overlapped with compute of the next stage. However, achieving practical and efficient fine-grained overlap is challenging as we describe in Section 3.\n' +
      '\n' +
      '## 3. Challenges With Fine-grained Compute-Communication Overlap\n' +
      '\n' +
      'This section details key challenges with the fine-grained overlap of compute and communication.\n' +
      '\n' +
      '### Complex & Expensive to Implement in Software\n' +
      '\n' +
      'The producer and collective operations execute as separate kernels on GPUs; the producer (GEMM) generates the data, after which the collective orchestrates their bulk communication and reduction. Extending the software for their fine-grained interleaving can be complex and expensive. It would involve breaking the producer and collective into smaller kernels or using dynamic parallelism, both of which can\n' +
      '\n' +
      'Figure 4. Transformer time spent on reduce-scatter (RS) and all-gather (AG) collectives as well as GEMMs which require collectives.\n' +
      '\n' +
      'Figure 3. Ring implementation of reduce-scatter collective.\n' +
      '\n' +
      'increase launch overheads and synchronization costs. Alternatively, it can be achieved by writing fused GEMM and collective kernels, but this can incur significant programming effort [(16; 18; 81; 85)]. First, BLAS libraries have hundreds of GEMM kernels optimized for different input sizes and GPU architecture, generated via an expensive tuning process [(3)]. Second, collectives are also of different types, and each has implementations optimized for different topologies. Creating fused kernels for every combination of GEMM and collective implementations can thus be extremely complex and expensive. Hence, it is imperative to achieve a fine-grained overlap of compute and communication without altering GEMM implementations.\n' +
      '\n' +
      '### Resource Contention Between Producer & Collective\n' +
      '\n' +
      'Overlapped GEMM and AR contend for GPU resources, specifically compute units (CUs) and memory bandwidth, which slow down overall execution.\n' +
      '\n' +
      '#### 3.2.1. Compute Sharing\n' +
      '\n' +
      'Concurrently executing GEMM and AR kernels must share CUs and their components including L1 cache, LDS, and vector registers. This contention may affect their performance relative to their isolated execution. Figure 6 evaluates the impact of concurrently executing GEMM and AR using our setup in Section 5.1.1 and Table 1. Specifically, Figure 6 shows the (normalized) GEMM and AR time for Mega-GPT-2 and T-NLG (with TP=8) sub-layers (Attn. and FC-2) when run in isolation with varying CU count splits (e.g., the 72-8 bars show GEMM\'s isolated execution time with 72 CUs and AR\'s with eight CUs). For each case, it also shows _potential-overlap-speedup_, the speedup overlapping AR and GEMM can obtain versus sequentially executing GEMM and AR when each has all 80 CUs. We calculate the overlapped time as max(GEMM time, AR time). The ideal case assumes no sharing impact: the GEMM has all the 80 CUs and the AR is fast but free (evaluated by running it with all 80 CUs in isolation). As a result, the ideal case has the maximum potential overlap speedup of 1.67\\(\\times\\) geomean. However, AR slows down considerably (geomean \\(\\sim\\)41% slowdown) when allocated only eight CUs (72-8 case) compared to when it had all CUs. This significantly decreases the potential-overlap-speedup to 1.18\\(\\times\\) geomean. While AR performance improves with 16 CUs (only \\(\\sim\\)7% slowdown in 64-16 case), GEMMs slow down (geomean \\(\\sim\\)21% slowdown) since they now only have 64 CUs. Overall, while better than the 72-8 case, potential speedups fall short (1.49\\(\\times\\) geomean) compared to the ideal case. Moreover, this assumes no contention due to memory bandwidth sharing (discussed next) and thus underestimates slowdowns. Overall, sharing of CUs reduces overlapping efficacy and it is crucial to preserve the compute resources dedicated to GEMMs.\n' +
      '\n' +
      '#### 3.2.2. Memory Bandwidth Sharing\n' +
      '\n' +
      'GEMM and AR kernels also compete for memory bandwidth when run concurrently. As shown in Figure 3, at each step AR kernels a) read an array chunk from memory to send it to one neighbor GPU and also b) write to memory the chunk it received from another neighbor. Reduce-scatter (RS) additionally requires a memory read of the local chunk for reduction. Moreover, the memory traffic due to AR communication can be bursty. This additional, bursty memory traffic due to AR can slow down critical memory accesses by the producer GEMM, with the impact higher for GEMMs for which inputs do not fit in GPU\'s last level cache (LLC) as we will show in our evaluation in Section 6.1.2 and Figure 17. Thus, to enhance overlap efficiency, it is essential to limit memory traffic due to communication and/or limit their contention with GEMM.\n' +
      '\n' +
      'Prior work also studied contention between communication and computation [(71)], albeit in DP setups with coarse-grained GEMM and AR overlap. They show that AR slows down by up to 2.4\\(\\times\\) when run concurrently with GEMMs, and the slowdown is even higher when run concurrently with memory-intensive embedding lookups in recommendation models. For TP, they observe a 1.4\\(\\times\\) slowdown when executed concurrently with GEMMs.\n' +
      '\n' +
      'Figure 5. GEMM (left) when sliced in the dot-product dimension (right) still generates the same number of data blocks.\n' +
      '\n' +
      'Figure 6. Evaluating how the benefits of overlapping GEMM and RS, across model layers, are impacted by compute unit (CU) sharing. The X-axis shows how CUs are split between GEMM and AR, using the GPU setup from Table 1, in the format _A-B_. \\(A\\) represents the number of CUs the GEMM uses, while \\(B\\) represents the number of CUs AR uses. Ideal assumes no sharing, the GEMM has all CUs, and AR is free.\n' +
      '\n' +
      '## 4. T3: Transparent Tracking & Triggering\n' +
      '\n' +
      'To overcome the aforementioned challenges of complex software and resource contention with fine-grained overlap of compute and communication, we propose T3.\n' +
      '\n' +
      '### T3 Overview\n' +
      '\n' +
      'Modern GPUs first execute the producer GEMMs and store their outputs in their local memory. Afterwards they initiate the collective operation (Section 3). T3 instead initiates the collective immediately as GEMMs generate data to enable fine-grained overlap. It uses a _track & trigger_ mechanism to monitor GEMM\'s/collective\'s progress and to orchestrate communication, requiring no additional CUs (Section 4.2). It leverages _near-memory compute_ for reductions to reduce memory traffic due to communication (Section 4.3). Finally, it does these _transparently_, with minor kernel modifications (Section 4.4).\n' +
      '\n' +
      'Figure 7 illustrates a four-device reduce-scatter (RS) overlapped with its producer GEMM. This GEMM executes in multiple _stages_ of WGs dictated by its input and kernel implementation (Section 2.5), while RS executes in multiple _steps_ dictated by the number of devices involved (Section 2.3). For simplicity of illustration, we show the number of GEMM stages to be one more than the number of required ring steps. In each step, a GEMM stage\'s execution and reduction of its output happen in parallel to the communication of the previous stage output. In the first step, the output is communicated to remote devices directly by the GEMM (_remote_update_). The later, _steady state_, steps require a DMA (_dma_update_). For \\(N\\) devices, this steady state step is performed \\(N-2\\) times, on different chunks. Focusing on GPU-0 in the steady state, step-\\(2\\), as shown in Figures 7, the GPU executes/generates output for GEMM stage-3 while also receiving (via DMA) a copy of stage\'s output (blue) from its neighbor, GPU-1. This occurs in parallel to GPU-0\'s DMA of the reduced copy of GEMM stage-2 data (yellow) to GPU-3, thus overlapping communication. T3 leverages _near-memory computing_ (NMC) to atomically update memory locations on these local and DMA updates, resulting in a partially reduced copy of the stage-3 chunk without requiring additional reads or GPU CUs (Section 4.3). Once they complete, GPU-0 initiates a _dma_update_ of the chunk to its neighbor\'s (GPU-3) memory as shown in step-3. This automatic tracking of updates and DMA triggering is done using a lightweight and programmable hardware Tracker, further reducing dependency on GPU CUs (Section 4.2). These remote / DMA updates are done transparently by configuring the GEMM\'s output address mapping, with minor application and kernel modifications (Section 4.4).\n' +
      '\n' +
      'We also make minor runtime and hardware changes to improve T3\'s performance. To enable the perfect overlap of GEMM and RS in Figure 7, we stagger the scheduling of GEMM workgroups (WGs) across GPUs (Section 4.4). Moreover, we also augment the memory system with a simple yet effective memory controller arbitration (MCA) policy to manage memory contention between compute and communication (Section 4.5).\n' +
      '\n' +
      'Figure 8 shows a GPU with T3\'s enhancements (in orange) executing the steady state step described above. The GPU executes the GEMM to generate local updates for a stage (C1). Concurrently the GPU receives DMA updates for the same stage (D1a) and sends DMA updates for the previous stage (D1b). At the memory controller, the modified _MCA_ arbitrates between the local and DMA traffic to prevent contention. Following this, the updates are sent to _NMC-enhanced DRAM_ (D2a) while the _Tracker_ is updated with their progress (D2b). Once the Tracker observes the required local and DMA updates to a memory region, it triggers their DMA transfer to the neighbor GPU (D3).\n' +
      '\n' +
      'We use the 4-GPU GEMM-RS overlap as a running example to describe T3. RS is more challenging to overlap due to reductions and extra memory traffic. Further, the ring\n' +
      '\n' +
      'Figure 7. Overview of fused GEMM and ring reduce-scatter with T3 on a four-GPU node.\n' +
      '\n' +
      'configuration is more complex than others. Thus, we detail T3 using ring-RS and discuss additional collectives in Section 7.1.\n' +
      '\n' +
      '### T3 Tracking & Triggering\n' +
      '\n' +
      'T3\'s _programmable track & trigger_ mechanism is key to transparently enabling fine-grained overlap of producer and collective without using compute resources. As shown in Figure 9, T3 automatically transfers copies of data between devices when ready (e.g., in Figure 7, T3 triggers DMA update of stage-2 data from GPU-0 to GPU-3 once both GPU-0\'s local and GPU-1\'s remote updates are complete). This is enabled by a lightweight _Tracker_ at the memory controller, that tracks local and remote/DMA accesses to memory regions and triggers a DMA transfer once the required accesses are complete. Since the condition when a DMA is triggered (e.g., number of remote and local updates) and DMA transfer details (e.g., addresses, operation type) vary per collective type and implementation, they are programmed ahead of time using address space configuration (detailed in Section 4.4 and Figure 12).\n' +
      '\n' +
      '#### 4.2.1. Tracker\n' +
      '\n' +
      'The Tracker tracks both local and remote memory updates of a GEMM stage and triggers its DMA. As shown in Figure 9(a) and (b), it does so at wavefront (WF, i.e., a group of threads that execute in lockstep) granularity 1 - i.e., the Tracker tracks the memory region a WF updates. This assumes tiled GEMM implementations and that each WF/WG generates a complete tile of data, as is the case in all evaluated GEMMs (Steiner, 2012; Steiner, 2012). However, T3 can also handle other implementation (Section 7.7). An update increments the counter at its corresponding WF\'s (_wf_id_) Tracker entry 2. This is done by all local, remote, and DMA updates that arrive at the GPU\'s memory controller (e.g., GPU-0 does not track GEMM stage-1 as its WFs neither write locally nor are its remote updates received). The incremented counter value is checked for a maximum threshold, which is set to the product of WF output size (_wf_tile_size_) and the total updates expected per element 3. The _wf_tile_size_ is determined by the GPU driver using the output size and WF count (\\((M*N)/\\#WF\\)). The total updates expected per element for ring-RS is two but changes with collective type/implementation and is thus configurable (detailed in Section 4.4). Once the threshold is reached, the final write triggers the DMA 4 in Figure 9(c) and detailed in Section 4.2.2). The Tracker is checked once the accesses are enqueued in the memory controller queue (MCQ) and thus are not in the critical path.\n' +
      '\n' +
      'WF-based tracking is beneficial as a producer\'s (or GEMM\'s) stage may not update contiguous memory regions. As shown in Figure 9(a) this can happen due to column-major allocation of arrays in BLAS libraries (Bahman et al., 2012; Steiner, 2012) and row-major scheduling. This makes address-based tracking expensive (requires storing several addresses or complex table indexing functions) which WF-based tracking avoids. The Tracker has a total of 256 entries, indexed using the workgroup (WG) ID\'s LSBs, _wg_lsb_ (8 bits). Each entry is set associative and is tagged using _wg_msb_, _wf_id_. _wg_msb_ is \\(log\\_2(maxWGSperstage/256)\\) bits and _wf_id_ is three bits for a maximum of eight WFs per WG. We set the maximum entries based on the maximum WGs possible in a producer stage. Each entry has a starting virtual address (smallest address per WF), and an accesses counter, making the Tracker size 19KB. The tracking additionally requires the source _wg_id_ and _wf_id_ as metadata in memory accesses and forwarding of their virtual addresses to the memory controller (to trigger the DMA in Section 4.2.2).\n' +
      '\n' +
      '#### 4.2.2. Triggering DMA\n' +
      '\n' +
      'Once the required accesses to a WF\'s memory region are issued, T3 DMAs the data to the remote GPU (4 in Figure 9(c)). As shown in Figure 9(c), the DMA commands are pre-programmed by the GPU driver and are configurable (detailed in Section 4.4 and Figure 12) as the DMA regions/operations can differ based on the collective type and implementation. The granularity of the DMA block/table entry is set to be equal to or larger than the Tracker granularity (_wf_tile_). The memory access which completes the required accesses at the Tracker entry (Section 4.2.1) marks the corresponding DMA entry ready and also populates it with the _wg_id_ and _wf_id_ which are required by the destination GPU\'s Tracker. If DMA blocks are a multiple of _wf_tile_, an additional counter per DMA entry can track their completion. Using the pre-programmed starting source/destination virtual address, _wf_tile_size_, and the output dimensions (M, N), the DMA engine dynamically generates the remaining virtual addresses to initiate the DMA.\n' +
      '\n' +
      '### Near-Memory Reductions\n' +
      '\n' +
      'To perform reductions on producer and DMA updates without occupying GPU compute resources, T3 leverages compute-enhanced memories. We assume an HBM-based DRAM architecture with near-memory op-and-store support as has been\n' +
      '\n' +
      'Figure 8. GPU with highlighted T3 enhancements (in orange) executing a steady-state fused GEMM-RS step.\n' +
      '\n' +
      'proposed by recent works (Wang et al., 2019; Wang et al., 2020). We envision such compute support to be implemented via ALUs near DRAM banks as has recently been proposed by memory vendors (Wang et al., 2019; Wang et al., 2020). However, T3 can also leverage other reduction substrates (Section 7.4).\n' +
      '\n' +
      'T3 leverages this near-memory computing (NMC) capability to enable GEMM stores and DMA transfers to directly update and reduce copies of data, when required by collectives. For DMA transfers, the operation type (store vs. updates) is directly specified in the command (address space configuration in Figure 12 and Section 4.4). For GEMMs, we utilize two flags. First, we use an "uncached" flag during memory allocation to ensure that the output is not cached in any GPU\'s caches (such allocations are supported in existing GPUs). Thus, writes are directly sent to DRAM which acts as the point of aggregation for all (local, remote, DMA) updates. The queuing of updates in the memory controller queue guarantees their atomicity; at any given time, only a single instruction can be issued and executed by near-bank ALUs. Second, we use an "update" flag in the GEMM API call to enable stores of the GEMM to update the DRAM. The "update" flag is sent (via kernel packets (Beng et al., 2019)) to the CUs to tag the kernel\'s stores with one-bit "update" info (similar to prior work (Wang et al., 2019; Wang et al., 2020; Wang et al., 2020)). These are processed by the memory controller to generate the op-and-store commands.\n' +
      '\n' +
      'In addition to freeing up CUs for GEMMs, NMC helps reduce memory traffic due to communication. Figure 10 shows memory accesses in a steady-state RS step in baseline and with T3. In baseline RS, CUs read two copies of data (local copy, and received copy from the previous neighbor) and write the reduced data to the next neighbor\'s memory. T3 only requires one read of the data to DMA update the neighbor GPU memory using NMC. Overall, T3 with NMC reduces the dependence on GPU CUs and further reduces (or eliminates, _direct-RS_ in Section 7.1) data movement required for communication.\n' +
      '\n' +
      '### Configuring Producer\'s Output Address Space\n' +
      '\n' +
      'Modifying producer kernels, especially for many GEMMs of different shapes and sizes, to fuse and overlap collectives, can be impractical (Section 3.1). T3 avoids this by configuring the producer\'s output address space mapping which is used to program the Tracker and DMA commands. Figures 11 and 12 show this configuration for GPU-0 from the fused GEMM-RS example in Figure 7.\n' +
      '\n' +
      'Since there are four devices, GEMM\'s output array is chunked four ways. In GPU-0, the GEMM writes its stage-1 output directly to GPU-3\'s memory (step-1 in Figure 7), while its stage-2 and stage-3 output is first written to local memory and later DMA\'d to GPU-3 (stage-4 is only written locally once and is not DMA\'d). Thus, GPU-0 requires memory mappings of these chunks with that of GPU-3 as shown in Figure 11. This configuration differs per collective type and topology-optimized implementation (see Section 7.1) and, similar to modern collective implementations, can be pre-defined in collective libraries (Beng et al., 2019; Wang et al., 2020). Figure 12 shows an example of this using pseudo-code.\n' +
      '\n' +
      'The configuration in Figure 12 defines this mapping for the GEMM output using two different API calls: _remote_map_ and _dma_map_. _remote_map_ is used for fine-grained remote writes/updates (for stage-1), which uses existing GPU support for peer-to-peer load/store by threads (Wang et al., 2020). Conversely, _dma_map_ is used for coarse-grained DMA writes/updates (for stage-2,3) which leverages existing support for memory copies by DMA engines in GPUs (DirectGMA and others (Wang et al., 2019; Wang et al., 2020; Wang et al., 2020)). A _dma_map_ call also defines the DMA functionality (store vs. update), and its triggering condition (number of stores/updates per element). It can also be extended to specify granularity (_wf_tiles_ per DMA block in Figure 9(c)). These calls are used to pre-program the Tracker and DMA commands to enable automatic communication of data when ready (Section 4.2).\n' +
      '\n' +
      'Fusion in ring-based collectives also benefits from producers (on different devices) generating data chunks in a staggered manner. In Figure 7, GPUs stagger the generated data by one stage; in step-1, GPU-0 executes stage-1, while GPU-1 executes stage-2, and so forth. This is enabled by staggering WG scheduling across devices. Alternatively, it can also be enabled by fetching appropriate implementation from BLAS libraries with staggered output tile-to-WG\n' +
      '\n' +
      'Figure 9. T3 Track & Trigger.\n' +
      '\n' +
      'mapping amongst producer kernels. Overall, configuring the output address space mitigates the need to change GEMM implementations to enable fusion with collectives.\n' +
      '\n' +
      '### Communication-aware MC Arbitration (MCA):\n' +
      '\n' +
      'Finally, careful scheduling of memory accesses by the producer kernel and those resulting from communication is crucial to efficiently overlap them. In Section 6.1 we show that a memory controller (MC) arbitration policy which a) round-robins between issuing memory accesses from the compute and communication streams and b) falls back to the other stream if the current stream is empty, results in producer kernel slowdowns. Communication-related memory accesses appear in bursts and can occupy DRAM queues, stalling the compute kernel\'s critical memory reads/writes. Simply prioritizing producer kernel accesses as they appear is also insufficient as prior communication-related memory accesses may already occupy DRAM queues. Finally, giving the local compute stream dedicated access results in wasted cycles and memory bandwidth underutilization. Thus, an efficient overlap of compute and communication requires a dynamic arbitration policy that addresses both contention and under-utilization.\n' +
      '\n' +
      'We implement a simple yet dynamic arbitration policy to overcome this. The MC always prioritizes compute stream accesses, but if empty, falls back to communication stream. Additionally, it monitors the DRAM queue occupancy and only issues communication-related accesses if occupancy is below a threshold. This ensures sufficient room in the queues for future compute stream accesses and prevents their stalls. The occupancy threshold depends on the memory-intensiveness of compute kernels (e.g., smaller if memory-intensive, and vice-versa). This is determined dynamically: MC detects the memory intensiveness of a kernel by monitoring occupancy during its isolated execution (the first stage in Figure 7). Finally, the MC tracks cycles elapsed since the last issue from the communication stream and prioritizes it if it exceeds a limit to ensure it is not starved. Additionally, the communication stream is drained at the producer kernel boundary.\n' +
      '\n' +
      '## 5. Methodology\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '#### 5.1.1. Multi-GPU Simulation\n' +
      '\n' +
      'Although a number of popular GPU simulators are publicly available (7; 20; 39; 74), we chose to evaluate T3 using Accel-Sim (33) because it provides high fidelity for modern GPUs (31). Like prior work (32), we extended Accel-Sim to simulate a multi-GPU system. We observe that in a multi-GPU DNN setup all GPU\'s executions are homogeneous (Figures 2 and 10). Thus, we evaluate both our multi-GPU baseline and T3 by modeling all the activities pertaining to a single GPU. This includes modeling the Tracker which is accessed/updated in parallel with the store/DMA operations and uncached NMC updates. Although we do not model the DMA engine in the simulator, we do model its inter-GPU communication (communication resulting from RS both in the baseline and T3\'s fused GEMM-RS) by executing the compute operations (e.g., GEMM (30; 61)) in Accel-Sim and using Accel-Sim\'s front-end tracing functionality to inject the additional inter-GPU communication traffic. The Tracker\'s DMA triggering overheads are negligible since the DMA commands are pre-queued during the setup process (Figure 12) as is often done, especially for ML operations which are repetitive (24). Table 1 details the GPU configuration we use to evaluate T3, which is the latest GPU architecture Accel-Sim completely supports. Commercial GPUs with such a configuration support a 150 GB/s interconnection ring bandwidth (53). Since recent GPUs frequently scale compute faster than other resources, we also evaluate another configuration with increased CU count while the other parameters stay the same in Section 7.5.\n' +
      '\n' +
      'Figure 13 describes our multi-GPU simulation of RS. In each RS step, a GPU performs a reduction of a sub-array and\n' +
      '\n' +
      'Figure 11. Remote address mapping for T3 GEMM-RS over four GPUs.\n' +
      '\n' +
      'Figure 10. HBM reads & writes in steady-state reduce-scatter step.\n' +
      '\n' +
      'sends it to the neighbor GPU while also receiving a reduced sub-array (corresponding to a different chunk) from another neighbor GPU (Figures 3 and 10(a)). The simulator executes the reduction of the array as-is. Simulating the incoming network traffic requires: (a) determining packet addresses, (b) generating packets at the appropriate rate, and (c) modeling the interconnect costs. Packet addresses are determined using the store trace of WGs from the reduction kernel. Next, since GPU executions are homogeneous, remote traffic is generated at the same rate as the GPU generates the reduction output (which is filtered out to be sent to remote GPU). This also implicitly includes slowdowns due to compute/communication interference at the remote GPU. Finally, we add the interconnect costs to these packets as they arrive, assuming a simple link bandwidth and latency model of the interconnect. To validate this setup, we compare simulated RS times on four GPUs with hardware measurements from a four-GPU node with AMD Instinct(tm) MI210 GPUs (Beng et al., 2015) with same ring network bandwidth as simulated (Table 1). Figure 14 shows that simulation closely follows hardware trends for a range of sizes (6-192 MB): 6% geomean error versus the ideal dotted line.\n' +
      '\n' +
      '**Near-Memory Computing**: We modify the simulator\'s HBM to model NMC updates. Further, memory vendor proposals indicate that NMC operations can be issued without a significant increase in DRAM timings; back-to-back NMC operations can be issued to the same bank group with the same column-to-column access (CCDL) delay (Kumar et al., 2017). To model the additional cost of NMC op-and-store operations (Section 4.3), we modify the simulator\'s HBM to use a 2\\(\\times\\) higher CCDDL delay (termed) following those operations (see Table 1).\n' +
      '\n' +
      '#### 5.1.2. End-to-End Transformer Iteration\n' +
      '\n' +
      'To evaluate end-to-end iterations with T3, we scale the GEMMs and RS times in the baseline Transformer breakdown (shown in Figure 4) by their simulated speedups (described in Section 5.1.1). We leverage a combination of hardware data and analytical modeling as done by prior works (Zhu et al., 2019; Wang et al., 2019) to get the end-to-end breakdowns of models in their distributed setups. We use a single-GPU mixed-precision (Zhu et al., 2019) execution of MLPerf v1.1 (Wang et al., 2019) BERT on an AMD Instinct(tm) MI210 accelerator (GPU) (Beng et al., 2015) and scale its operation times based on changing hyperparameters and setup (e.g., sliced GEMM). This is beneficial as it helps us evaluate larger futuristic models (Transformer models are similar differing only in layers size/counts (Zhu et al., 2019; Wang et al., 2019)) and takes into account several GPU optimizations for Transformers (Zhu et al., 2019; Wang et al., 2019) already in MLPerf implementations. Our projections further match those measured by prior works. For example, AR\'s percentage runtime contribution projected for Mega-GPT-2 with TP-16 matches prior works\' measurements on a similar system configuration (Wang et al., 2019).\n' +
      '\n' +
      '### Applications, Deployment & GEMMs\n' +
      '\n' +
      '**Models and their deployment**: Since Transformers are fast-evolving, we evaluate T3\'s impact on a range of Transformer models and TP degrees (Table 2). For Megaton-GPT-2 (Mega-GPT-2) (Wang et al., 2019) and T-NLG (Wang et al., 2019) we use 16K and 8K input tokens (= input-length \\({}^{*}\\) batch-size) and TP degrees of eight and 16, given their modern intra-node setups (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). For larger Transformers like PALM (Beng et al., 2015), GPT-3 (Beng et al., 2015), and MT-NLG (Wang et al., 2019)) we use a higher slicing degree of 32 given their increasingly large memory capacity requirements (Wang et al., 2019) and availability of nodes with larger device counts that can enable this slicing (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). We evaluate mixed-precision training which entails half-precision (FP16) forward and backpropagation and single-precision (FP32) weight updates. Similarly, we evaluate FP16 inference.\n' +
      '\n' +
      '**GEMMs**: GEMMs from the aforementioned applications are simulated using implementations from state-of-the-art BLAS libraries (Wang et al., 2019; Wang et al., 2019). Most GEMMs (including all GEMMs we evaluate) use a tiled GEMM implementation where each WG generates a complete tile of data (other implementations discussed in Section 7.7). Further, we evaluate GEMMs with both non-transposed (e.g., backward GEMMs) and transposed (e.g., forward GEMMs) input tensors, as observed in MLPerf\'s BERT (Wang et al., 2019; Wang et al., 2019).\n' +
      '\n' +
      '### Configurations\n' +
      '\n' +
      'To evaluate T3\'s efficacy we use the following configurations:\n' +
      '\n' +
      '* _Sequential_: is the baseline configuration. Like modern systems, sequential executes sliced GEMMs and the following AR kernels sequentially.\n' +
      '* _T3_: is our proposal which fuses and overlaps GEMM with RS (as described in Section 4), followed by sequential all-gather (AG).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|} \\hline \\multicolumn{2}{|c|}{**System**} \\\\ \\hline \\#GPUs & 8, 16 \\\\ \\hline Inter-GPU & Ring, 150 GB/s b-directional \\\\ Interconnect & 500 ns link latency \\\\ \\hline \\multicolumn{2}{|c|}{**Per-GPU Config**} \\\\ \\hline \\#CUs & 80, 14 GHz \\\\ \\hline Per-CU Config & 2K threads, 128KB unified LDS + L1 cache (with no write-allocate), 256KB RF \\\\ \\hline L2 & 16MB, 64 banks, 1.4 GHz \\\\ \\hline HBM2 & 1 TR/s, 1 GHz, CCDWL=4, Bank Grp-4, rest (Beng et al., 2015) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Simulation setup.\n' +
      '\n' +
      'Figure 13. Simulating multi-GPU reduce-scatter.\n' +
      '\n' +
      '* _T3-MCA_: uses fused GEMM-RS as in T3, but also includes the memory controller arbitration (MCA) discussed in Section 4.5.\n' +
      '* _Ideal-GEMM-RS-Overlap_: represents ideal GEMM and RS overlap in software. Thus, its performance is the maximum of the GEMM\'s and the RS\'s isolated kernel execution times, followed by the AG time. Moreover, it assumes no dependency constraints or resource contention between GEMM and RS.\n' +
      '* _Ideal-RS+NMC_: uses RS with near-memory computing, which can provide additional speedup beyond a perfect overlap. Thus, its performance is max(GEMM, RS+NMC) over Ideal-GEMM-RS-Overlap.\n' +
      '\n' +
      '## 6. Results\n' +
      '\n' +
      '### Execution Time Distribution & Speedups\n' +
      '\n' +
      'Figures 15 and 16 show results for all sliced sub-layers in Transformers which require an AR: output projection (OP) and fully-connected-2 (FC-2) in forward pass (fwd) and fully-connected-1 (FC-1) and input projection (IP) in backprop (bwd). We show these for Mega-GPT-2 and T-NLG, as well as two TP setups (TP of 8 and 16). Figure 15 shows each case\'s runtime distribution between the GEMM, RS, and AG. Figure 16 shows their speedup over _sequential_ using _T3_, _T3-MCA_, as well as their speedups assuming an ideal overlap of GEMM with RS (_Ideal-GEMM-RS-Overlap_) and additional speedups resulting from a faster RS with NMC (_Ideal RS+NMC_).\n' +
      '\n' +
      '#### 6.1.1. Ideal Speedups\n' +
      '\n' +
      'Figure 16 shows the ideal possible speedups and breaks them into two parts: first from overlapping the GEMM and RS kernels (_Ideal-GEMM-RS-Overlap_) and second from improved RS performance due to NMC (_Ideal RS+NMC_).\n' +
      '\n' +
      'In Figure 16 Ideal-GEMM-RS-Overlap (without resource and data-dependency constraints) shows considerable benefits from overlapping the producer GEMM and following RS: 50% max speedup and 35% geomean versus Sequential. Speedups vary both within and across models and depend on the isolated execution times of GEMM and RS (Figure 15). The situations where the GEMM and RS runtimes are similar (similar proportions in Figure 15) have the maximum potential since the GEMM hides all of RS\'s cost. For example, FC-1 in T-NLG with TP=16 obtains 50% speedup. Alternatively, the cases in which the GEMM and RS times are skewed show the least benefit since most of the GEMM or RS cost is exposed. For example, Ideal-GEMM-RS-Overlap speedup is only 15% for OP in Mega-GPT with TP=16. However, the latter is uncommon and is a consequence of slicing a very small layer (OP is the smallest among all). It does not hold for other sub-layers within the same model, or larger models as shown in the figures (also see Section 6.4). For a given hardware setup, these execution time ratios, and thus Ideal-GEMM-RS-Overlap speedups are dictated by layer parameters (Shen et al., 2017).\n' +
      '\n' +
      'In Figure 16 Ideal-RS+NMC shows that additional speedup is possible beyond what perfect overlap provides. Besides freeing all the CUs for GEMMs, performing RS reductions near memory also lowers RS\'s memory traffic (described in Section 4.3). This speeds up RS by 7% and 3% with TP=8 and TP=16, respectively. NMC only reduces RS\'s final step time as interconnect costs dominate all prior steps and thus its runtime benefit decreases as TP, and thus total steps, increases. As shown in Figure 16, this faster RS can reduce overlapped time and provide additional speedups of up to 4%. Intuitively, the impact of a faster RS is only evident in layers in which RS is longer running than GEMM and is otherwise hidden when overlapped.\n' +
      '\n' +
      '#### 6.1.2. T3 Speedups\n' +
      '\n' +
      'T3 transparently overlaps GEMMs with their corresponding consumer RS in a fine-grained manner. Moreover, T3\'s lightweight track-&-trigger mechanism and use of near-memory compute frees all CUs for GEMMs and reduces DRAM traffic (Figure 18 and Section 6.2), respectively. Thus, T3 achieves speedups of up to 39% (20% geomean, yellow bars, Figure 16).\n' +
      '\n' +
      'Individual speedups vary considerably and are largely impacted by the extent of contention between DRAM traffic from the GEMM and the concurrent, RS (details in Section 6.2). For OP layers, T3 achieves close to the Ideal-GEMM-RS-Overlap speedups, and even exceeds them in certain cases. This happens because the OP GEMMs are small and fit largely in the LLC, having very small DRAM read traffic in Sequential (shown in Figure 18). Thus, the additional DRAM traffic from the overlapped RS in T3 has little impact on the GEMMs\' progress/execution. Instead, T3 further improves\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|} \\hline\n' +
      '**Model Name** & **Hyperparams** & **Inputs** & **TP degree** \\\\ \\hline Mega-GPT-2 & H=3072, L=74 & SL=1K, B=16 & \\(8,16\\) \\\\ \\hline T-NLG & H=4256, L=78 & SL=1K, B=8 & \\(8,16\\) \\\\ \\hline GPT-3 & H=12K, L=96 & SL=1K, B=2 & \\(32\\) \\\\ \\hline PALM & H=18K, L=118 & SL=1K, B=2 & \\(32\\) \\\\ \\hline MT-NLG & H=20K, L=105 & SL=1K, B=2 & \\(32\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Studied models, their hyperparameters & setup.\n' +
      '\n' +
      'Figure 14. Validation of multi-GPU reduce-scatter simulation.\n' +
      '\n' +
      'RS runtimes in these cases via NMC and enables part of the additional Ideal-RS+NMC speedups. Finally, although the track & trigger mechanism operates at a small WF granularity, generally data from multiple WFs/WGs of a GEMM stage are ready to be sent concurrently, resulting in high network bandwidth utilization. Furthermore, even when this is not true, T3 can tolerate this because compute/GEMM execution and communication are overlapped, hiding the latency.\n' +
      '\n' +
      'In many other cases, and especially the much larger FC layers, the benefits are far from those with Ideal-GEMM-RS-Overlap (>15% slower). Figure 17 shows the DRAM traffic (Y-axis) and the GEMM slowdown (X-axis) with fine-grained overlapping, compared to the GEMM\'s isolated execution. An isolated GEMM as shown in Figure 17(a) executes in multiple stages (Section 2.5), each with a read phase (blue) followed by a bursty write phase, which limit read traffic. Overlapping RS induces additional DRAM traffic, as shown in Figure 17(b). Besides additional traffic, in T3, GEMM, and RS writes directly update memory using NMC (Section 4.3). These additional bursts of reads (RS_reads for a stage are issued as soon as both the local and neighbors\' copies have updated the memory) and updates (RS_updates for the next stage from the previous neighbor) can further stall local GEMM reads as shown, causing GEMM to slow down considerably.\n' +
      '\n' +
      '#### 6.1.3. T3-MCA Speedups\n' +
      '\n' +
      'T3-MCA (Section 4.5) limits GEMM reads stalls due to bursty RS traffic (Section 6.1.2, Figure 17) using a simple arbitration logic. It prevents RS traffic from completely occupying DRAM queues by limiting communication-related accesses when a DRAM queue occupancy reaches a threshold (5, 10, 30, or no limit) determined by the memory intensity of the GEMM kernel. T3-MCA provides considerable benefits over sequential execution; maximum of 47% and geomean of 30% (29% maximum and 13% geomean over T3). Furthermore, the geomean speedup with T3-MCA is only 5% smaller than Ideal-GEMM-RS-Overlap. There are individual cases where T3-MCA is far from ideal (e.g., FC-1 in T-NLG with TP=16). These represent cases where L2 bypassing (for near-memory update) of GEMM writes hurts the GEMM\'s performance. Consequently, the overall overlapped runtime also increases.\n' +
      '\n' +
      '### Data Movement Reductions\n' +
      '\n' +
      'Besides improved performance, T3 and T3-MCA also reduce data movement to and from DRAM by a maximum of 36% and an average of 22% for the sub-layers. Figure 18 shows the total memory accesses and their detailed breakdown (amongst GEMM, RS and AG reads/writes) for a single GPU across all cases. While the AG reads/write remain constant between baseline (sequential) and T3-MCA, there is a combination of reasons which impact the rest: (a) fusion of GEMM and RS eliminates local writes from GEMM\'s first stage and reads from RS\'s first step, (b) near-memory reductions eliminate reading of partial copies every RS step, as well as the reads and writes in the final step\'s reduction, and (c) LLC bypassing of GEMM\'s output writes improves input read caching for cache-sensitive GEMMs, reducing GEMM\'s local reads. These impacts also vary depending on the TP degree: the one-time reductions (in the first and last RS step) have a much higher impact with smaller TP degrees due to fewer overall RS steps. Conversely, GEMM read caching impact is higher with a larger TP degree; larger TP/slicing leads to smaller, more LLC-amenable GEMMs. Overall, RS\'s reads reduce by 2.4\\(\\times\\) geomean (2.5\\(\\times\\) for TP=8, 2.2\\(\\times\\) for TP=16), both GEMM\'s and RS\'s writes reduce by 10% geomean (14% for TP=8, 7% for TP=16), and finally GEMM\'s reads decrease by 1.56\\(\\times\\) geomean (1.2\\(\\times\\) for TP=8, 2\\(\\times\\) for TP=16).\n' +
      '\n' +
      '### End-to-end Model Speedups\n' +
      '\n' +
      'As shown in Figure 19, T3 and T3-MCA speed up model training by a maximum of 9% and 12%, and geomean of 7% and 10%, respectively. Benefits are higher at larger TPs due to the overall higher proportion of the sliced sub-layers requiring AR (Section 4). Similarly, prompt processing and/or large input token processing during inference is also sped up by a maximum of 12% and 15%, and geomean of 9% and 12% with T3 and T3-MCA, respectively. Inference speedups are better due to the overall higher proportion of sliced sub-layers resulting from no backprop compute. Finally, the MLPerfv1.1 implementation we evaluate does not include a key fusion optimization (Garon et al., 2018), which makes the non-sliced attention operations a significant 40-45% of execution time. Thus, we\n' +
      '\n' +
      'Figure 16. Transformer sub-layer speedups with T3\n' +
      '\n' +
      'Figure 15. Transformer sub-layer runtime distribution.\n' +
      '\n' +
      'expect T3\'s and T3-MCA\'s benefits to be much higher for newer MLPerf implementations.\n' +
      '\n' +
      '### Impact on Larger Transformers\n' +
      '\n' +
      'We also evaluate larger Transformers with higher TP degrees as shown in Figure 19. Similar to the smaller models, layer-level speedups are high; max 35% and geomean of 29% for GPT-3 (175B parameters), PALM (530B parameters), and MT-NLG (540B parameters). These lead to up to 12% and 14% end-to-end speedup in their training and prompt phase of inference, respectively. Thus, T3-MCA also effectively speeds up larger models.\n' +
      '\n' +
      '## 7. Discussion\n' +
      '\n' +
      '### Other Collectives Implementation & Types\n' +
      '\n' +
      'T3 supports other collectives and implementations via the configuration of GEMM\'s output address space (Section 4.4).\n' +
      '\n' +
      '**Other implementations**: Collectives can have multiple implementations optimized for different topologies. We focus on ring since it is commonly used in intra-node setups where tensor slicing is employed (Zhou et al., 2017). T3 can also support the _direct_ RS implementation in a fully-connected topology. At every GEMM stage, the output from each device is scattered across the remaining devices using dedicated links and reduced at the destination. This is accomplished by changing the configuration in Figure 12 to slice each GEMM stage output and remote_map each slice to a remote device. In this case T3 eliminates memory accesses by the collective as it is completely orchestrated using GEMM stores. Similarly, it can also support other, inter-node implementations via appropriate programming of the track & trigger mechanism.\n' +
      '\n' +
      '**Other types**: Similarly, T3 also supports other collectives. A ring/direct all-gather (AG) reuses ring-RS\'s configuration and setup, except the GEMMs and DMA transfers do not update memory locations. Similar to AG, T3 can also support an all-to-all collective where devices exchange sub-arrays, except here the remote/dma_mapped GEMM output is not written to local memory.\n' +
      '\n' +
      '### Other Distributed Techniques\n' +
      '\n' +
      'Although we focus on communication in tensor-parallel (TP) setups, T3 is also applicable in other distributed setups where a producer\'s output is communicated via a collective.\n' +
      '\n' +
      '**Expert Parallelism**: Similar to TP, expert parallelism in Mixture-of-experts (MoEs) (Zhou et al., 2017; Zhang et al., 2017) require serialized all-to-all communication which can be fused with T3 as discussed in Section 7.1.\n' +
      '\n' +
      '**Data & Pipeline Parallelism**: T3 also applies to data-parallel and pipeline-parallel setups which require RS, and peer-to-peer transfers, respectively. While T3\'s overlapping benefits may not provide additional benefits in such cases (these communications can be overlapped with other independent kernels), T3\'s NMC and MCA techniques can help reduce memory bandwidth contention in these cases as well.\n' +
      '\n' +
      '**TP with All-gather**: T3 can be extended for distributed setups where the collective\'s output requires overlapping with a long-running consumer operation. This is required if the producer is short-running (e.g., TP which all-gather\'s activations). Overlapping collective-consumer pairs is similar in principle to overlapping producer-collective and requires similar tracking/triggering mechanisms. The Tracker would track "all-gathered-input\\(\\rightarrow\\)GEMM-WG" instead of "GEMM-WG\\(\\rightarrow\\)all-reduced-output". Moreover, instead of triggering a DMA, it would trigger a WG scheduling event (such as in Lustig & Martonosi (2018)). This can be challenging since the "all-gathered-input\\(\\rightarrow\\)GEMM-WG" mapping can be kernel implementation dependent. However, additional programming hints could overcome this.\n' +
      '\n' +
      '### Generative Inference\n' +
      '\n' +
      'While we focus on the communication-heavy training and prompt phase of inference, T3 is also applicable in the generation phase of inference. Due to smaller input token counts\n' +
      '\n' +
      'Figure 17. Overall DRAM traffic in (a) baseline GEMM, (b) T3, for T-NLG FC-2 with TP=8 and SLB=4K.\n' +
      '\n' +
      '(Section 2.1), these phases are bound by memory accesses of model weights and can benefit from the aggregate memory bandwidth of multiple devices that TP provides (Beng et al., 2017). The resulting all-reduce of activations, while smaller than those in training and thus potentially latency-bound (due to small token counts), can still be overlapped and hidden with GEMM executions using T3.\n' +
      '\n' +
      '### Other Reduction Substrates\n' +
      '\n' +
      'While T3 leverages NMC for atomic updates required in reduction-based collectives (e.g., RS, AR), it is not a requirement. Such updates could also be handled via system-wide atomics on uncached data without significant loss in performance. Similarly, T3 can also leverage switches for reductions as shown by prior works (Rosen et al., 2019).\n' +
      '\n' +
      '### Future Hardware & Lower Precision\n' +
      '\n' +
      'Since compute FLOPS have scaled much more than network link bandwidths across hardware generations (Han et al., 2016; Wang et al., 2017; Wang et al., 2018; Wang et al., 2019), communication will likely be a larger proportion of the end-to-end execution both in more recent systems than the one we evaluate and in the future. Similarly, lowering precision (Wang et al., 2017; Wang et al., 2018) decreases compute time much more (quadratically) than communication (linearly). Thus, the benefits of hiding communication with techniques like T3 will also apply to other GPU configurations and datatypes besides 16b.\n' +
      '\n' +
      'To evaluate T3\'s hiding capability in future systems, we study a system configuration where compute FLOPS scale more than network link bandwidth (2\\(\\times\\)), which we term GPU-2X-CU. While the scaling GPU FLOPs across generations largely result from more powerful CUs (larger/faster tensor processing), we simulate it by scaling the number of CUs and keeping the underlying network the same. This enables us to use the latest/validated GPU model and GEMM traces that Accel-Sim supports (Zhu et al., 2019). Figure 20 shows that for larger layers (FC-2) where compute time dominates, compute becomes faster with 2\\(\\times\\) CUs which lowers the compute:communication ratio across the models. This shortens the critical path and leads to larger benefits with overlapping compute and communication with T3. Conversely, for smaller layers (OP), where compute and communication are more balanced, faster compute exposes communication on critical path, lowering T3\'s benefits. Note that, for such scenarios, communication optimizations will be necessary (Han et al., 2016; Wang et al., 2018). Nevertheless, the larger layers have a more prominent impact on overall execution and for these, T3\'s benefits only improve.\n' +
      '\n' +
      '### NMC for Following Operations\n' +
      '\n' +
      'Collectives, specifically all-reduce in Transformers, are usually followed by other memory-intensive operations on all devices (e.g., parameter updates in DP (Zhu et al., 2019) or residual/dropout layers in TP). These operations redundantly operate on the entire all-reduced array on each device. With T3, these following memory-intensive operations can be executed using NMC (Zhu et al., 2019) on (reduced) sub-arrays of data before they are all-gathered/broadcasted to the remaining devices, thus reducing redundancy, and further accelerating distributed Transformer models.\n' +
      '\n' +
      '### Other GEMM Implementations\n' +
      '\n' +
      'T3 focuses on the most common tiled GEMM implementation with a WG/WF responsible to generate an entire tile/sub-tile of data. However, T3 can support other implementations, such as split-K (Wang et al., 2018). A split-K implementation slices work in the accumulation or \\(K\\) dimension, such that multiple WGs are responsible for a single tile, each generating a partial tile that is reduced after. Split-K increases parallelism when the output size (\\(MxN\\)) is small but the \\(K\\) dimension is large. However, tensor-sliced GEMMs, which require AR, have large output sizes and small \\(K\\) dimensions. Naively, T3 with a split-K implementation (with more than one update to an element) will cause multiple local and remote updates per memory location. To prevent this, T3 can use the kernel packets\' tile-size metadata to deduce split-k degree (=(#WGs " tile-size)/(M*N)), i.e., the number of updates per element. The virtual addresses in the tracker (Section 4.2.1) can be used to determine WFs/WGs/tracker entries that update the same tile, allowing the tracker to trigger remote DMA only after all updates to the tile are complete.\n' +
      '\n' +
      'Figure 19. End-to-end model speedups.\n' +
      '\n' +
      'Figure 20. T3 on future hardware with 2\\(\\times\\) compute.\n' +
      '\n' +
      '### Multi-node Setups\n' +
      '\n' +
      'Tensor-parallelism, with serialized communication is usually employed within a node, which generally has high-speed homogeneous links. However, T3 can also be applied to serialized communication in inter-node setups with slower and often heterogeneous links. Consequently, communication costs can be much larger than GEMM executions, potentially limiting the benefits from fine-grained overlap: once the computation is completely overlapped, the remaining communication costs will be exposed (Srivastava et al., 2017). Nevertheless, T3 can still provide benefits from hiding the GEMM execution cost as much as possible.\n' +
      '\n' +
      '## 8. Related Work\n' +
      '\n' +
      'Table 3 compares T3-MCA with prior works across several key metrics. Some prior work has designed _in-switch collectives_ to speed up communication by up to 2\\(\\times\\)(Srivastava et al., 2017). However, this cannot eliminate serialized communication from the critical path. Furthermore, they are topology-dependent, requiring switches. Enabling fine-grained overlap of compute and communication is essential to effectively hide the cost of communication. Existing attempts to do this, like _CocoNet(Cai et al., 2019)_ and _Google Decomposition_(Srivastava et al., 2017), have limitations. _Google Decomposition_ requires changes to matrix multiplication (GEMMs) kernels which can be disruptive to GPU software infrastructure (Section 3.1).\n' +
      '\n' +
      'Furthermore, both approaches can suffer from hardware resource contention between compute and communication (Section 3.2). Works that reduce contention only address coarse-grained overlap of compute and communication in cases like DP, lacking support for fine-grained overlap in serialized collectives (Srivastava et al., 2017). Moreover, they rely on dedicated accelerators. Other recent work fuses communication within the computation kernel to enable fine-grained overlap, such that a GPU kernel performs both computation and dependent communication at the WG level (Srivastava et al., 2017). However, this requires explicit changes to the compute kernels and is not readily applicable for collectives involving simple arithmetic operation such as reduce-scatter - which will still be limited by inter-GPU synchronization. Finally, other work like Syndicate increases coarse-grained overlap opportunities and efficiency in distributed training. However, Syndicate cannot hide serialized communication (Srivastava et al., 2017). _T3-MCA overcomes these shortcomings and achieves a transparent overlap of serialized communication with compute, while minimizing resource contention._\n' +
      '\n' +
      '## 9. Conclusion\n' +
      '\n' +
      'Transformer models increasingly rely on distributed techniques, requiring communication between multiple devices. This communication can limit scaling efficiency, especially for techniques like Tensor Parallelism (TP) which serialize communication with model execution. While a fine-grained overlap of the serialized communication with its producer computation can help hide the cost, realizing it with GPUs is challenging due to software complexities and resource contention between compute and communication. To overcome this, we propose T3, which transparently and efficiently fuses and overlaps serialized inter-device communication with the producer\'s compute. It orchestrates communication on the producer\'s stores by configuring the producer\'s output address space mapping and using a programmable track and trigger mechanism in hardware. This reduces application impact and also eliminates contention for GPU compute resources. T3 additionally uses near-memory computing and a memory-controller arbitration policy to reduce memory-bandwidth contention. Overall, T3 improves performance by 30% geomean (max 47%) and reduces data movement by 22% geomean (max 36%) over state-of-the-art approaches. Moreover, T3\'s benefits hold as models and hardware scale.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We would like to thank our shepherd, Saeed Maleki, and the anonymous reviewers for their feedback that helped improve this paper. We would also like to thank Mahmoud Khairy Abdallah for his help with the Accel-Sim simulator. This work is supported in part at the University of Wisconsin-Madison by the Vilas Life Cycle Professorship program and a Fall Research Competition grant, as well as the National Science Foundation under grant ENS-1925485. AMD, AMD Ryzen, AMD Radeon, and combinations thereof are trademarks of Advanced Micro Devices, Inc. Other product names used in this publication are for identification purposes only and may be trademarks of their respective companies.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* (1) AMD. 2018. AMD\'s ROCm Communication Collectives Library. "[https://github.com/ROCmSoftwarePlatform/rccl/wiki](https://github.com/ROCmSoftwarePlatform/rccl/wiki)".\n' +
      '* (2)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline\n' +
      '**Approach /** & **GPU** & **Transparent** & **Overlap** & **Reduce** & **No Additional** & **Topology-** \\\\\n' +
      '**Features** & **Support** & **Communication** & **Communication** & **Contention** & **Accelerator** & **independent** \\\\ \\hline\n' +
      '**In-switch (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**ACE (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**CoCoNet (Cai et al., 2019)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**Google Decomposition (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**T3-MCA** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3. Comparing T3-MCA to prior work.\n' +
      '\n' +
      '* [2] AMD. 2019. AMD\'s BLAS Library. "[https://github.com/ROCmSoftw](https://github.com/ROCmSoftw) arePlatform/rocBLAS".\n' +
      '* [3] AMD. 2020. AMD\'s tool for creating a benchmark-driven backend library for GEMMs. "[https://github.com/ROCmSoftwarePlatform/Te](https://github.com/ROCmSoftwarePlatform/Te) nsile/.\n' +
      '* [4] AMD. 2021. AMD HSA Code Object Format. "[https://rocmdocs.amd.c.om/en/latest/ROCm_Compiler_SDK/ROCm-Codeobj-format.html](https://rocmdocs.amd.c.om/en/latest/ROCm_Compiler_SDK/ROCm-Codeobj-format.html)".\n' +
      '* [5] AMD. 2022. AMD INSTINCT" MI210 ACCELERATOR. [https://www.amd.com/en/products/server-accelerators/amd-instinct-mi210](https://www.amd.com/en/products/server-accelerators/amd-instinct-mi210).\n' +
      '* [6] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)_. IEEE, IEEE Press, Piscataway, NJ, USA, 1-15.\n' +
      '* [7] Yuhui Bao, Yifan Sun, Zlatan Feric, Michael Tian Shen, Micah Weston, Jose L. Abellan, Trinayan Baruah, John Kim, Ajay Joshi, and David Kaeil. 2023. Navisim: A Highly Accurate GPU Simulator for AMD RDNA GPUs. In _Proceedings of the International Conference on Parallel Architectures and Compilation Techniques_ (Chicago, Illinois) (_PACT \'22). Association for Computing Machinery, New York, NY, USA, 333-345. [https://doi.org/10.1145/3559009.3569666](https://doi.org/10.1145/3559009.3569666)\n' +
      '* [8] Nathan Beniach and Ian Hogarth. 2022. State of AI Report 2022. [https://www.stateof.ai/](https://www.stateof.ai/).\n' +
      '* [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henghian, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In _Advances in Neural Information Processing Systems (NeurIPS, Vol. 33)_. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.). Curran Associates Inc., Red Hook, NY, USA, 1877-1901.\n' +
      '* [10] Zixian Cai, Zhengyang Liu, Saeed Maleki, Madanlus Musuvathi, Todd Mytkowicz, Jacob Nelson, and Olli Saarakivi. 2021. Synthesizing Optimal Collective Algorithms. In _Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOP)_. Association for Computing Machinery, New York, NY, USA, 62-75. [https://doi.org/10.1145/3437801.3441620](https://doi.org/10.1145/3437801.3441620)\n' +
      '* [11] Niladrish Chatterjee, Mike O\'Connor, Donghyuk Lee, Daniel R Johnson, Stephen W Keckler, Minsoo Rhu, and William J Dally. 2017. Architecting an Energy-Efficient DRAM System for GPUs. In _23rd IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE, Computer Society, Washington, DC, USA, 73-84.\n' +
      '* [12] Akankanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsyvashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselam Levskaya, Sanjay Ghemawat, Sunjong Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thammalamayan Sankaranavana Pillai, Marie Pellat, Aitor Lewkowycz, Eric Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. _arXiv preprint arXiv:2204.02311_ (2022), 87 pages.\n' +
      '* [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. FlashAttention: Fast and Memory-efficient Exact Attention with IO-Awareness. _Advances in Neural Information Processing Systems_ 35 (2022), 16344-16359.\n' +
      '* [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Morristown, NJ, USA, 4171-4186. [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)\n' +
      '* [15] Shraf Eassa and Sukru Bure Eryilmaz. 2022. The Full Stack Optimization Powering NVIDIA MLPerf Training v2.0 Performance. [https://developer.nvidia.com/blog/boosting-mlperf-training-performance-with-full-stack-optimization/](https://developer.nvidia.com/blog/boosting-mlperf-training-performance-with-full-stack-optimization/).\n' +
      '* [16] Izzat El Hajj, Juan Gomez-Luna, Cheng Li, Li-Wen Chang, Dejan Milojicic, and Wen-mei Hwu. 2016. KLAP: Kernel Launch Aggregation and Promotion for Optimizing Dynamic Parallelism. In _49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)_. IEEE, IEEE Press, Piscataway, NJ, USA, 1-12. [https://doi.org/10.1109/MICRO.2016.7783716](https://doi.org/10.1109/MICRO.2016.7783716)\n' +
      '* [17] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. _The Journal of Machine Learning Research_ 23, 1, Article 120 (jan 2022), 39 pages.\n' +
      '* [18] Jan Fousek, Jiri Filipovic, and Matus Madzin. 2011. Automatic Fusions of CUDA-GPU Kernels for Parallel Map. _SIGARCH Comput. Archit. News_ 39, 4 (Dec. 2011), 98-99. [https://doi.org/10.1145/2082156.2082183](https://doi.org/10.1145/2082156.2082183)\n' +
      '* [19] Amir Gholami. 2021. AI and Memory Wall.\n' +
      '* [20] Anthony Gutierrez, Bradford M. Beckmann, Alexandru Duttu, Joseph Gross, Michael LeBeane, John Kalamatianos, Onur Kayiran, Matthew Poremba, Brandon Potter, Sooraj Puthoor, Matthew D. Sinclair, Michael Wyse, Jieming Yin, Xianwei Zhang, Akshay Jain, and Timothy Rogers. 2018. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level. In _24th IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, Los Alamitos, CA, USA, 608-619. [https://doi.org/10.1109/HPCA.2018.00058](https://doi.org/10.1109/HPCA.2018.00058)\n' +
      '* [21] Hany Hassan Awadalla, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, Will Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuanghi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018. Achieving Human Parity on Automatic Chinese to English News Translation. _arXiv preprint arXiv:1803.05567_ (March 2018), 25 pages. arXiv:1803.05567 [cs.CL]\n' +
      '* [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. _CoRR_ abs/1512.03385 (2015), 12 pages. arXiv:1512.03385 [http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385)\n' +
      '* [23] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhififeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS, Vol. 32)_. Curran Associates Inc., Red Hook, NY, USA, Article 10, 10 pages.\n' +
      '* [24] Changho Hwang, KyoungSoo Park, Ran Shu, Xinyuan Qu, Peng Cheng, and Yongqiang Xiong. 2023. ARK: GPU-driven Code Execution for Distributed Deep Learning. In _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 87-101. [https://www.usenix.org/conference/nsd123/presentatio/n/hwang](https://www.usenix.org/conference/nsd123/presentatio/n/hwang)* Jangda et al. (2022) Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, and Olli Saarikiv. 2022. Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads. In _Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)_. Association for Computing Machinery, New York, NY, USA, 402-416. [https://doi.org/10.1145/3503222.3507778](https://doi.org/10.1145/3503222.3507778)\n' +
      '* Jeaugey (2022) Sylvain Jeaugey. 2022. How is tree reduction implemented? [https://github.com/NVIDIA/ncc/issues/545#issuconnment-1006361565](https://github.com/NVIDIA/ncc/issues/545#issuconnment-1006361565).\n' +
      '* Jog et al. (2014) Adwait Jog, Evgeny Bolotin, Zvika Guz, Mike Parker, Stephen W Keckler, Mahmut T Kandemir, and Chitta R Das. 2014. Application-aware Memory System for Fair and Efficient Execution of Concurrent GPGPU Applications. In _Proceedings of Workshop on General Purpose Processing using GPUs (GPGPU)_. Association for Computing Machinery, New York, NY, USA, 1-8. [https://doi.org/10.1145/2588768.2576780](https://doi.org/10.1145/2588768.2576780)\n' +
      '* Jog et al. (2016) Adwait Jog, Onur Kayin, Ashtshota Pathataki, Mahmut T Kandemir, Onur Mutlu, Ravishankar T kely, and Chitta R Das. 2016. Exploiting Core Criticality for Enhanced GPU Performance. In _Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science_. Association for Computing Machinery, New York, NY, USA, 351-363. [https://doi.org/10.1145/2896377.2901468](https://doi.org/10.1145/2896377.2901468)\n' +
      '* Jouppi et al. (2021) Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Nishant Patil, Sushma Prasad, Clifford Young, Zongwei Zhou, and David Patterson. 2021. Ten Lessons from Three Generations Shaped Google\'s TPUv4i. In _Proceedings of the 48th Annual International Symposium on Computer Architecture_ (Virtual Event, Spain) _(ISCA)_. IEEE Press, Piscataway, NJ, USA, 1-14. [https://doi.org/10.1109/ISCA52012.2021.200010](https://doi.org/10.1109/ISCA52012.2021.200010)\n' +
      '* Kerr et al. (2017) Andrew Kerr, Duane Merrill, Julien Demouth, and John Tran. 2017. cuTLASS: Fast linear algebra in CUDA C++.\n' +
      '* Khairy et al. (2018) Mahmoud Khairy, Akshay Jain, Tor M. Aamodt, and Timothy G. Rogers. 2018. Exploring Modern GPU Memory System Design Challenges through Accurate Modeling. _CoRR_ abs/1810.07269 (2018), 10 pages. arXiv:1810.07269 [http://arxiv.org/abs/1810.07269](http://arxiv.org/abs/1810.07269)\n' +
      '* Khairy et al. (2020) Mahmoud Khairy, Vadim Nikiforov, David Nellans, and Timothy G. Rogers. 2020. Locality-Centric Data and Threadblock Management for Massive GPUs. In _33rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)_. IEEE Computer Society, Los Alamitros, CA, USA, 1022-1036. [https://doi.org/10.1109/MICRO50266.2020.00086](https://doi.org/10.1109/MICRO50266.2020.00086)\n' +
      '* Khairy et al. (2020) Mahmoud Khairy, Zhesheng Shen, Tor M. Aamodt, and Timothy G. Rogers. 2020. Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling. In _ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_. IEEE Press, Piscataway, NJ, USA, 473-486. [https://doi.org/10.1109/ISCA54697.2020.00047](https://doi.org/10.1109/ISCA54697.2020.00047)\n' +
      '* Kim et al. (2021) Heesu Kim, Hannim Park, Taehyun Kim, Kwanheum cho, Eojin Lee, Soojung Ryu, Hyuk-Jae Lee, Kiyoung Choi, and Jinho Lee. 2021. Grad-PIM: A Practical Processing-in-DRAM Architecture for Gradient Descent. In _27th IEEE International Symposium on High-Performance Computer Architecture (HPCA)_. IEEE Computer Society, Washington, DC, USA, 14 pages.\n' +
      '* Kim et al. (2021) Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. 2021. Scalable and Efficient MoE Training for Multitask Multilingual Models. [https://doi.org/10.48550/MXRIV.210.10465](https://doi.org/10.48550/MXRIV.210.10465)\n' +
      '* Klenk et al. (2020) Benjamin Klenk, Nan Jiang, Greg Thorson, and Larry Dennison. 2020. An In-Network Architecture for Accelerating Shared-Memory Multiprocessor Colletives. In _ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Computer Society, Washington, DC, USA, 996-1009.\n' +
      '* Volume 1_ (Lake Tahoe, Nevada) _(NIPS\'12)_. Curran Associates Inc., USA, 1097-1105. [http://dl.acm.org/citation.cfm?id=2.999134.2999257](http://dl.acm.org/citation.cfm?id=2.999134.2999257)\n' +
      '* Lee et al. (2021) Sukhan Lee, Shin-haeng Kang, Jaehoon Lee, Heyousin Kim, Eojin Lee, Seungwoo Seo, Hosang Yoon, Seungwon Lee, Kyounghwan Lim, Hyunung Shin, Jinhyun Kim, O Seongil, Anand Iyer, David Wang, Kyomin Sohn, and Nam Sung Kim. 2021. Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology: Industrial Product. In _ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE Press, Piscataway, NJ, USA, 43-56. [https://doi.org/10.1109/ISCA52012.2021.200013](https://doi.org/10.1109/ISCA52012.2021.200013)\n' +
      '* Lew et al. (2019) Jonathan Lew, Devi A Shah, Suchita Patil, Shaylin Cattell, Mengchi Zhang, Amruth Sandhupatla, Christopher Ng, Negar Goli, Matthew D Sinclair, Timothy G Rogers, and Tor Aamodt. 2019. Analyzing Machine Learning Workloads Using a Detailed GPU Simulator. In _IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)_. IEEE, IEEE Computer Society, Washington, DC, USA, 151-152.\n' +
      '* Lin et al. (2014) Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Network In Network. In _2nd International Conference on Learning Representations (ICLR)_, Yoshua Bengio and Yann LeCun (Eds.). OpenReview.net, 10 pages. [http://arxiv.org/abs/1312.4400](http://arxiv.org/abs/1312.4400)\n' +
      '* Lin et al. (2018) Shih-Chieh Lin, Yunqi Zhang, Chang-Hong Hsu, Matt Skach, M E. Hague, Lingia Tang, and Jason Mars. 2018. The Architectural Implications of Autonomous Driving: Constraints and Acceleration. In _Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems_ (Williamsburg, VA, USA) _(ASPLOS)_. ACM, New York, NY, USA, 751-766. [https://doi.org/10.1145/3173162.3173191](https://doi.org/10.1145/3173162.3173191)\n' +
      '* Lustig and Martonosi (2013) Daniel Lustig and Margaret Martonosi. 2013. Reducing GPU Offload Latency via Fine-Grained CPU-GPU Synchronization. In _Proceedings of the 19th International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, USA, 354-365. [https://doi.org/10.1109/HPCA.2013.6522332](https://doi.org/10.1109/HPCA.2013.6522332)\n' +
      '* Mahajan et al. (2023) Kshiteej Mahajan, Ching-Hsiang Chu, Srinivas Sridharan, and Aditya Akella. 2023. Better Together: Jointly Optimizing ML Collective Scheduling and Execution Planning using SYNDICATE. In _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 809-824. [https://www.usenix.org/conference/nsd32/presentation/mahajan](https://www.usenix.org/conference/nsd32/presentation/mahajan)\n' +
      '* Mattson et al. (2019) Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Mickiveitis, David A. Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim M. Hazelwood, Andrew Hock, Xinyuan Huang, Bill Jia, Daniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Guokai Ma, Depeak Narayanan, Tayo Oguntebi, Genrady Pekhimenko, Lillian Pentecost, Vijay Janga Redi, Taylor Robie, Tom St. John, Carole-jean Wu, Lingjie Xu, Cliff Young, and Matei Zaharia. 2019. MLPerf Training Benchmark. _CoRR_ abs/1910.01500 (2019), 14 pages. arXiv:1910.01500 [http://arxiv.org/abs/1910.01500](http://arxiv.org/abs/1910.01500)\n' +
      '* Mickevicius et al. (2018) Paulius Mickevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Olekski Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed Precision Training. arXiv:1710.03740 [cs.AI] [http://arxiv.org/abs/1710.03740](http://arxiv.org/abs/1710.03740)\n' +
      '* Mickevicius et al. (2022) Paulius Mickevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Hav, Alexander Heinecke, Patrick Judd, John Kamala, Naveen Mellempudi, Stuart Oberman, Mohammad Shneybi, Michael Su, and Hao Wu. 2022. FPR Formats for Deep Learning. _CoRR_ abs/2209.05433 (2022), 9 pages. [https://doi.org/10.48550/ARRV.2209.05433](https://doi.org/10.48550/ARRV.2209.05433) arXiv:2209.05433\n' +
      '* Microsoft (2020) Microsoft. 2020. Turing-NLG: A 17-billion-parameter language model by Microsoft. _Microsoft Research Blog_ 1, 8 (2020), 8 pages. [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)* [48] MLPerf. 2018. MLPerf Benchmark Suite. [https://mlperf.org/](https://mlperf.org/).\n' +
      '* [49] Diksha Moolchandani, Joyjit Kundu, Frederik Ruelens, Peter Vrancx, Timon Evenblij, and Manu Perumkunnil. 2023. AMFeD: An Analytical Model for Performance in Distributed Training of Transformers. In _IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)_. IEEE Computer Society, Los Alamitos, CA, USA, 306-315. [https://doi.org/10.1109/ISPASS57527.2023.00037](https://doi.org/10.1109/ISPASS57527.2023.00037)\n' +
      '* [50] Harini Muthukrishnan, Daniel Lustig, David Nellans, and Thomas Wenisch. 2021. GPS: A Global Publish-Subscribe Model for Multi-GPU Memory Management. In _MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture_ (Virtual Event, Greece) _(MICRO \'21)_. Association for Computing Machinery, New York, NY, USA, 46-58. [https://doi.org/10.1145/3466752.3480088](https://doi.org/10.1145/3466752.3480088)\n' +
      '* [51] Harini Muthukrishnan, David Nellans, Daniel Lustig, Jeffrey A Fessler, and Thomas F Wenisch. 2021. Efficient Multi-GPU Shared Memory via Automatic Optimization of Fine-grained Transfers. In _ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Computer Society, Washington, DC, USA, 139-152.\n' +
      '* [52] Lifeng Nai, Ramyad Hadidi, Jaewoong Sim, Hyojong Kim, Pranith Kumar, and Hyesoon Kim. 2017. GraphPIM: Enabling Instruction-level PIM Offloading in Graph Computing Frameworks. In _IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, Los Alamitos, CA, USA, 457-468. [https://doi.org/10.1109/HPCA.2017.54](https://doi.org/10.1109/HPCA.2017.54)\n' +
      '* [53] NVIDIA. 2017. NVIDIA DGX-1 With Tesla V100 System Architecture. [https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf](https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf).\n' +
      '* [54] NVIDIA. 2018. NVIDIA TESLA V100 GPU ACCELERATOR. [https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-dataset-letter-fnl-web.pdf](https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-dataset-letter-fnl-web.pdf).\n' +
      '* [55] NVIDIA. 2020. NVIDIA NCCL.\n' +
      '* [56] NVIDIA. 2021. NVIDIA A100 TESNR CORE GPU. [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-dataset-the-v104-wiki-1758950-r4-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-dataset-the-v104-wiki-1758950-r4-web.pdf).\n' +
      '* [57] NVIDIA. 2022. GPUDirect. [https://developer.nvidia.com/gpudirect/](https://developer.nvidia.com/gpudirect/).\n' +
      '* [58] NVIDIA. 2023. Efficient GEMM in CUDA. [https://github.com/NVIDIA/Acutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions](https://github.com/NVIDIA/Acutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions).\n' +
      '* [59] NVIDIA. 2023. NVIDIA Announces DGX GH200 AI Supercomputer. [https://nvidia.com/news/nvidia-grace-hopper-superchi-ps-designed-for-accelerated-generative-ai-enter-full-production](https://nvidia.com/news/nvidia-grace-hopper-superchi-ps-designed-for-accelerated-generative-ai-enter-full-production).\n' +
      '* [60] NVIDIA. 2023. NVIDIA H100 TENSOR CORE GPU. [https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-database](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-database).\n' +
      '* [61] NVIDIA Corp. 2016. NVIDIA cuBLAS. [https://developer.nvidia.com/cublas](https://developer.nvidia.com/cublas). Accessed August 6, 2016.\n' +
      '* [62] Prateynis Patel, Eisha Choukes, Chaojie Zhang, Inigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. 2023. Splitwise: Efficient Generative LLM Inference Using Phase Splitting. _arXiv preprint arXiv:2311.18677_ (2023), 12 pages. arXiv:2311.18677 [cs.AR]\n' +
      '* [63] Suchita Pati, Shaizean Aga, Nuwan Jayasena, and Matthew D. Sinclair. 2022. Demystifying BERT: System Design Implications. In _2022 IEEE International Symposium on Workload Characterization (IISWC)_. IEEE Computer Society, Los Alamitos, CA, USA, 296-309. [https://doi.org/10.1109/IISWC55918.2022.00033](https://doi.org/10.1109/IISWC55918.2022.00033)\n' +
      '* [64] Suchita Pati, Shaizean Aga, Nuwan Jayasena, and Matthew D. Sinclair. 2023. Tale of Two Cs: Computation vs. Communication Scaling for Future Transformers on Future Hardware. In _IEEE International Symposium on Workload Characterization (IISWC)_. IEEE Computer Society, Los Alamitos, CA, USA, 140-153. [https://doi.org/10.1109/IISWC59245.2023.00026](https://doi.org/10.1109/IISWC59245.2023.00026)\n' +
      '* [65] Ashutosh Pattaik, Xulong Tang, Onur Kayiran, Adwait Jog, Asit Mishra, Mahmut T Kandemir, Anand Sivasubramaniam, and Chita R Das. 2019. Opportunistic Computing in GPU Architectures. In _Proceedings of the 46th International Symposium on Computer Architecture (ISCA)_. Association for Computing Machinery, New York, NY, USA, 210-223. [https://doi.org/10.1145/3307650.332212](https://doi.org/10.1145/3307650.332212)\n' +
      '* [66] J Thomas Pawlowski. 2011. Hybrid Memory Cube (HMC). In _2011 IEEE Hot Chips 23 Symposium (HotChips)_. IEEE, IEEE, Piscataway, NJ, USA, 1-24.\n' +
      '* [67] Kishore Punniyamurthy, Bradford M Beckmann, and Khaled Hamidouche. 2023. GPU-initiated Fine-grained Overlap of Collective Communication with Computation. _arXiv preprint arXiv:2305.06942_ (2023), 13 pages. arXiv:2305.06942 [cs.DC]\n' +
      '* [68] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. _OpenAI blog_ 1, 8 (2019), 9.\n' +
      '* [69] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-MOE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. In _International Conference on Machine Learning (ICML)_. PMLR, PMLR, 18332-18346.\n' +
      '* [70] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ECo-Informity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, St. Louis, Missouri) _(SC \'21)_. Association for Computing Machinery, New York, NY, USA, Article 59, 14 pages. [https://doi.org/10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)\n' +
      '* [71] Saeed Rashidi, Matthew Denton, Srinivas Sridharan, Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna. 2021. Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms. In _2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Press, Piscataway, NJ, USA, 540-553. [https://doi.org/10.1109/ISCA52012.2021.00049](https://doi.org/10.1109/ISCA52012.2021.00049)\n' +
      '* [72] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmulling, C. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idoumi, T. B. Jabin, J. Jiao, T. S. John, F. Kanwar, D. Lee, J. Liao, A. Lokhmborov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou. 2020. MLPerf Inference Benchmark. In _ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_. IEEE Press, Washington, DC, USA, 446-459. [https://doi.org/10.1109/ISCA45697.2020.00045](https://doi.org/10.1109/ISCA45697.2020.00045)\n' +
      '* [73] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahwar Bordon, and Nando de Freitas. 2022. A Generalist Agent. _Transactions on Machine Learning Research_ 2022 (2022), 42 pages. [https://openreview.net/forum?id=1ik0k4kj/jv](https://openreview.net/forum?id=1ik0k4kj/jv)\n' +
      '* [74] Kyle Roarty and Matthew D. Sinclair. 2020. Modeling Modern GPU Applications in gem5. In _3rd gem5 Users\' Workshop._ 2 pages.\n' +
      '* [75] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. 2020. Pushing the Limits of Narrow Precision Inference at Cloud Scale with Microsoft Floating Point. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_ (Vancouver, BC, Canada) _(NeurIPS\'20)_. Curran Associates Inc., Red Hook, NY, USA, Article 861, 11 pages.\n' +
      '* [76] Aarush Selvan and Pankaj Kanwar. 2022. Google showcases Cloud TPU v4 Pods for large model training. [https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-wt-pods-for-large-model-training](https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-wt-pods-for-large-model-training).\n' +
      '\n' +
      '* Shah et al. (2023) Aashaka Shah, Vijay Chidambaram, Meghan Cowan, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Jacob Nelson, Olli Saarikivi, and Rachee Singh. 2023. TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches. In _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 593-612. [https://www.usenix.org/conference/ns/d23/presentation/shah](https://www.usenix.org/conference/ns/d23/presentation/shah)\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRRabs/1909.08053 (2019), 9 pages. arXiv:1909.08053 [cs.CL]. [http://arxiv.org/abs/1909.08053](http://arxiv.org/abs/1909.08053)\n' +
      '* Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In _3rd International Conference on Learning Representations (ICLR)_, Yoshua Bengio and Yann LeCun (Eds.). OpenReview.net, 14 pages. [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)\n' +
      '* Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530b, a large-scale Generative Language Model. _arXiv preprint arXiv:2201.11990_ (2022), 44 pages. arXiv:2201.11990 [cs.CL]\n' +
      '* Springer et al. (2017) Matthias Springer, Peter Wauligmann, and Hidehiko Masuhara. 2017. Modular Array-Based GPU Computing in a Dynamically-Typed Language. In _Proceedings of the 4th ACM SIGPLAN International Workshop on Libraries, Languages, and Compilers for Array Programming_ (Barcelona, Spain) _(ARRAY 2017)_. Association for Computing Machinery, New York, NY, USA, 48-55. [https://doi.org/10.1145/3091966.3091](https://doi.org/10.1145/3091966.3091)\n' +
      '* Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going Deeper with Convolutions. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE Press, Piscataway, NJ, USA, 1-9.\n' +
      '* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE Press, Piscataway, NJ, USA, 2818-2826. [https://doi.org/10.1109/CVPR.2016.308](https://doi.org/10.1109/CVPR.2016.308)\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_ (Long Beach, California, USA) _(NeurIPS)_. Current Associates, Inc., Red Hook, NY, USA, 6000-6010. [https://proceedings.neurips.cc/paper/2017/hash/35/ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/35/ee243547dee91fbd053c1c4a845aa-Abstract.html)\n' +
      '* Wang et al. (2010) Guibin Wang, YiSong Lin, and Wei Yi. 2010. Kernel Fusion: An Effective Method for Better Power Efficiency on Multithreaded GPU. In _Proceedings of the 2010 IEEE/ACM Int\'1 Conference on Green Computing and Communications & Int\'1 Conference on Cyber, Physical and Social Computing (GREECOM-CFSCOM \'10)_. IEEE Computer Society, USA, 344-350. [https://doi.org/10.1109/GreenCom-CPSCom.2010.102](https://doi.org/10.1109/GreenCom-CPSCom.2010.102)\n' +
      '* Wang et al. (2022) Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivas Murthy, Marcello Maggioni, Qiao Zhang, Sameer Kumar, Tongfiei Guo, Yuanzhong Xu, and Zongwei Zhou. 2022. Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models. In _Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1 (ASPLOS)_. Association for Computing Machinery, New York, NY, USA, 93-106. [https://doi.org/10.1145/356795.3567959](https://doi.org/10.1145/356795.3567959)\n' +
      '* Xiong et al. (2017) Wayne Xiong,, Xuedong Huang, Frank Seide,, and Andreas Stolcke. 2017. Toward Human Parity in Conversational Speech Recognition. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_ 25 (Sept 2017), 2410-2423.\n' +
      '\n' +
      'Received 10 August 2023; revised 3 January 2024; accepted 8 January 2024\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>