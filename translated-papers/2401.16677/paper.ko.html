<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      '딥 뉴럴 네트워크(DNN)는 음성 인식[87], 이미지 분류[22, 37, 40, 79, 82, 83], 기계 번역[21], 자율 에이전트[41], 언어 처리[14, 68] 및 텍스트 생성[9]을 포함하는 작업에 대해 상당한 정확도 개선으로 사회를 변화시켰다. 이 엄청난 변형 효과는 (1) 더 나은 하드웨어 시스템, (2) 더 큰 데이터 세트, (3) 더 효율적인 하드웨어 및 더 큰 데이터 세트로부터 더 많은 이익을 얻는 개선된 ML 구조 및 알고리즘의 선순환 시너지에 의해 가능하게 되었다. 광범위한 작업[8]으로 인기를 끌고 인공지능(73)에서 상당한 진보를 보인 트랜스포머에게는 특히 그렇다. 트랜스포머는 데이터 세트와 모델 크기가 기하급수적으로 증가했으며 매개변수는 BERT[14]의 3억 4천만에서 PALM[12]의 5400억으로 증가했다. 따라서, 이들의 메모리 및 계산 요구는 또한 증가되어, 분산 기술에 점점 더 의존하게 되었다: 다수의 가속기들(예를 들어, GPU들)은 그들의 집단 메모리 용량들을 풀링하고 DNN을 협력적으로 실행하기 위한 컴퓨팅 능력들. 그러나, 디바이스들 간의 결과적인 통신은 그들의 실행 시간의 상당한 비율이 되었고, 디바이스 카운트(36, 49, 64)가 증가함에 따라 스케일링 효율을 제한하였다.\n' +
      '\n' +
      '트랜스포머는 데이터 병렬성(DP)과 모델 병렬성(MP)의 두 가지 주요 분산 기법을 함께 사용하는 경우가 많다. DP는 데이터세트를 분할하고 장치에 걸쳐 모델을 복제하여 학습을 병렬화하며, 그라디언트의 통신 및 집계(_all-reduce_)를 필요로 한다. 반대로 MP는 단일 장치의 메모리에 들어갈 수 없는 큰 모델을 분할한다. MP의 일종인 TTP(Tensor-parallelism)는 장치 간의 레이어 출력도 모두 줄여야 한다. 이러한 분산 기법 중 TP의 통신은 일반적으로 그림 1(a)와 같이 모델 실행의 임계 경로에 있으며 런타임(\\(\\sim\\)45%[64])의 상당한 비율이 될 수 있으며, 결과적으로 장치의 수가 증가함에 따라 처리량이 하위 선형적으로 증가한다.\n' +
      '\n' +
      '기존 연구들은 네트워크 내 연산량을 최대 \\(2\\times\\)까지 증가시켰지만, 토폴로지 의존적인 스위치(요구 스위치)이므로 임계 경로에서 직렬화된 통신을 제거할 수 없다[36]. 풍부한 코어스-그레인 독립 연산(예를 들어, DP)을 갖는 분산 기법들은 종종 효율성을 향상시키기 위해 독립 연산들과 통신을 중첩(및 숨김)한다. 직렬화된 통신 시나리오도 그러한 잠재력을 제공하지만, 그들은 자체 과제를 제시하는 계산과 통신의 미세한 중첩을 필요로 한다. 현재 시스템에서 세밀한 중첩을 가능하게 하려면 값비싼 세밀한 동기화[25] 또는 GPU 소프트웨어 인프라[86](섹션 3.1)에 방해가 될 수 있는 행렬 곱셈(GEMM) 커널의 변경이 필요하다. 또한, 중첩 계산 및 통신은 계산 단위 및 메모리 대역폭 모두에 대해 경합하여 중첩의 유효성을 줄인다[25, 86]. (섹션 3.2). 경쟁을 줄이는 이전의 접근 방식은 DP와 같은 경우 계산과 통신의 조잡한 중첩을 해결하고 직렬화된 집합체에서 세밀한 중첩에 대한 지원이 부족하다[71]. 게다가, 그들은 전용 가속기에 의존한다. _ 따라서, 어떤 기존 기술도 자원 경쟁을 최소화하면서 연산과 직렬 통신의 투명한 중첩을 달성하지 못한다._\n' +
      '\n' +
      '이를 극복하기 위해 T3(도 1(b))를 제안한다. T3 _transparently_는 생산자의 스토어에서 직접 통신을 개시하기 위해 생산자의 출력 주소 공간_을 _configuring함으로써 생산자 동작을 후속 통신과 융합하여 최소한의 애플리케이션 변경을 필요로 한다. 가볍고 프로그래밍이 가능한 하드웨어 _tracker_를 사용하여 생산자/통신 진행 상황을 추적하고 미리 프로그래밍된 DMA 명령을 사용하여 _triggers_ 통신을 수행하므로 통신을 위한 추가 GPU 계산 리소스가 필요하지 않다. 또한, 생산자와 통신 사이의 메모리 대역폭에 대한 경쟁을 줄이기 위해 T3는 최근 제안된 계산 강화 메모리[34, 38]를 활용하여 스토어에서 메모리를 원자적으로 업데이트하여 통신 관련 감소로 인한 메모리 트래픽을 줄인다. 마지막으로, T3는 잔여 경쟁을 최소화하기 위해 생산자와 통신 메모리 스트림 사이에 간단하면서도 효과적인 중재 정책을 사용한다. 전반적으로 T3는 최소한의 자원 경쟁으로 직렬 통신을 투명하게 중첩한다. 이것은 컴퓨팅 및 네트워크 이용률을 향상시키고, 차례로, 디바이스 카운트를 증가시키면서 더 나은 스루풋 스케일링을 가능하게 할 수 있다. 우리는 다음과 같은 주요 공헌을 한다.\n' +
      '\n' +
      '* 우리는 애플리케이션 영향을 낮추고 컴퓨팅 및 메모리 간섭을 관리하면서 생산자 계산과 직렬 통신의 세밀한 중첩을 가능하게 하는 T3를 제안한다.\n' +
      '* 애플리케이션 영향을 관리하기 위해 T3는 생산자의 출력 주소 공간 매핑을 구성하여 매장에서 통신을 시작하여 생산자 커널에 약간의 수정을 요구한다.\n' +
      '* 컴퓨팅 자원 경쟁을 관리하기 위해 T3는 생산자의 진행을 추적하고 추가적인 컴퓨팅 자원이 필요하지 않은 기존의 DMA 엔진을 사용하여 통신을 트리거하는 경량 프로그래머블 트래커를 사용한다.\n' +
      '* 마지막으로, 계산과 통신 사이의 메모리 대역폭 경쟁을 해결하기 위해, T3는 통신으로 인한 데이터 이동을 줄이기 위해 출현하는 근접 메모리 컴퓨팅 기술을 이용한다. 또한, T3는 또한 계산 및 통신 메모리 트래픽을 더 잘 인터리빙하기 위해 간단하면서도 효과적인 메모리 컨트롤러 중재 정책을 장치합니다.\n' +
      '* 선행 작업 [32]와 유사하게 Accel-Sim [33]을 확장하여 다중 GPU 시스템(6% 오류)을 정확하게 모델링한다. 실험 결과 T3는 Mega-GPT-2(Mega et al., 2017) 및 T-NLG(Meng et al., 2018)와 같은 모델에서 슬라이스된 트랜스포머섭 층을 30% 지오메아(max 47%) 증가시키고 데이터 이동을 22% 지오메아(max 36%) 감소시키는 것으로 나타났다. 또한 T3의 편익은 5,000억 \\(\\sim\\)5,000억 모수 모형인 PALM과 MT-NLG에서 하위층에 대해 29%의 지오메인이 모델 척도로 지속된다. 전반적으로 T3는 모델 학습을 최대 12%, 추론(프롬프트 단계)을 최대 15%까지 가속화합니다.\n' +
      '\n' +
      '##2. 배경 및 동기\n' +
      '\n' +
      '분산 컴퓨팅을 위한### 트랜스포머 및 니즈\n' +
      '\n' +
      '트랜스포머(Zhou et al., 2017)는 광범위한 태스크/도메인(예를 들어, 텍스트, 이미지)을 위한 범용 아키텍처가 되었다(Beng et al., 2016). 모델들은 트랜스포머 _encoder_ 또는 _decoder_를 기본 빌딩 블록으로 사용하며, 각각은 어텐션 서브-레이어와 (도 2의 (a)에 도시된 바와 같이) 행렬 곱셈 연산(GEMM)으로서 나타나는 완전 연결(FC) 서브-레이어를 갖는다. 각 레이어는 또한 요소별 연산으로 나타나는 몇 개의 잔여 연결 및 레이어 정규화를 포함하고, GEMM과 종종 융합된다(Han et al., 2016; Chen et al., 2017; Chen et al., 2018; Chen et al., 2019). 도 2(b)에 도시된 바와 같이, 이들 GEMM은 (입력 토큰을 나타내는 각각의 벡터와 함께) 입력 행렬에 의한 층들의 가중치 행렬들의 곱셈을 수반한다. 트레이닝 동안, 입력 매트릭스들은 하나 이상의(배칭된 경우) 입력 시퀀스(들)로부터의 다수의 토큰들을 포함한다. 추론 동안, 입력 시퀀스(들) 내의 모든 토큰을 처리하기 위한 _prompt_ 페이즈와 각 입력 시퀀스에 대해 한 번에 하나의 토큰을 반복 처리하여 생성하기 위한 _token generation_ 페이즈의 두 가지 실행 페이즈가 존재한다(Zhou et al., 2017). 신속한 단계 연산은 훈련 중인 연산과 유사하지만, 생성 단계는 배치가 없는 경우 작은 입력 행렬 또는 행렬 벡터 연산(GEMV)을 갖는 GEMM을 갖는다.\n' +
      '\n' +
      '대부분의 트랜스포머 모델의 메모리 용량 요구 사항은 단일 장치를 초과합니다. 따라서, 이들은 분산 기술을 채용하고 다수의 가속기(예를 들어, GPU)를 협력적으로 사용한다. 또한, 다수의 디바이스들의 집계 계산 능력은 또한 대규모 입력 데이터세트들의 프로세싱을 병렬로 가능하게 함으로써 트레이닝을 가속화한다. 따라서 트랜스포머 및 그 데이터 세트(일반적으로 라벨링되지 않은 텍스트의 큰 말뭉치)가 크기가 몇 배 증가했기 때문에 분산 기술은 종종 필수적이며 점점 더 많은 장치를 필요로 한다. 이 축척은 향후 모델에 대해서만 증가합니다.\n' +
      '\n' +
      '### 분산 기술 및 연관 수집품\n' +
      '\n' +
      '트랜스포머는 많은 분산 기술을 사용하며, 각각은 장치 간의 _communication_과 관련되어 있다. 데이터 병렬(DP)은 여러 장치에서 모델 복제본을 서로 다른 데이터 집합으로 훈련하고 반복할 때마다 기울기의 감소를 요구합니다. 텐서 병렬성(TP)(Mega et al., 2017) 및 파이프라인 병렬성(예를 들어, GPipe)(Krizhevsky et al., 2014)은 다수의 디바이스들에 걸쳐 모델을 슬라이스한다. 전자는 활성화 감소를 필요로 하는 각 레이어를 슬라이스하는 반면, 후자는 활성화의 피어 투 피어 전송을 필요로 하는 모델 레이어를 분할한다. ZeRO-기반 최적화들(Zhou et al., 2017)은 또한 모델 가중치들을 슬라이스하거나 더 느리지만 더 큰(예를 들어, CPU) 메모리들에 오프로드하고, 레이어 실행 전에 수집될 것을 요구한다. 마지막으로 전문가 병렬성(Zhou et al., 2019)은 각 디바이스가 단일 전문가를 호스팅하고 입력-대-전문가 매핑에 기초하여 입력 데이터의 교환을 요구하도록 혼합-전문가(MoE) 모델들을 분할한다(Han et al., 2016; Chen et al., 2017). 이러한 통신 패턴은 _reduce-scatter_, _all-reduce_, _all-gather_, _all-to-all_와 같은 _collectives_에 의해 처리된다. 이러한 통신의 대부분은 독립적인 컴퓨트 연산(Zhou et al., 2017; Chen et al., 2018; Chen et al., 2019)에 의해 숨겨질 수 있지만, 비록 일부 자원 경쟁(Zhou et al., 2019; Chen et al., 2019)이 있지만, TP에서의 올-리듀스는 그렇지 않다(섹션 2.4에서 상세). 따라서 우리는 TP의 올-리듀스에 초점을 맞추고 섹션 7.1 및 섹션 7.2의 다른 기술/컬렉티브에 대해 논의한다.\n' +
      '\n' +
      '### All-Reduce & Ring 구현\n' +
      '\n' +
      'AR(All-Reduce) 집합은 각 장치에서 (요소별 합) 배열을 줄입니다. 가장 대역폭 효율이 높고, 따라서 가장 일반적으로 사용되는 AR의 여러 구현들이 있지만, 구현들은 _ring-AR_이다. Ring-AR은 ring reduction-scatter (ring-RS)와 ring all-gather (ring-AG)로 구성된다. 그림 3과 같이 ring-RS는 여러 단계로 이루어진다. 어레이들은 각각의 디바이스 상에서 청크화되고, 각각의 단계 동안, 모든 디바이스들은 링 내의 그들의 이웃에게 _unique_ 청크의 그들의 사본을 전송한다. 상기 디바이스들은,\n' +
      '\n' +
      '도 2. (a) 변압기 (b) 완전 연결 (FC) 층 (c) 임계 경로 상에서 올-리듀스를 갖는 텐서-슬라이싱된 FC 층.\n' +
      '\n' +
      '수신된 사본으로 청크의 로컬 사본을 줄이고 다음 단계에서 이웃에게 전달합니다. \\(N\\) 장치 및 어레이 청크된\\(N\\) 방식으로, 이 프로세스는 각 장치가 하나의 청크의 완전히 감소된 사본을 가질 때까지\\(N-1\\) 단계를 필요로 한다. 링-AG는 유사하지만 감소가 없다; 또한 각 장치가 감소된 청크를 모두 가질 때까지 \\(N-1\\) 단계가 필요하다. 나머지 논문에서 AR, RS 및 AG를 사용하여 링 구현을 참조하고 섹션 7.1에서 다른 구현에 대해 논의한다.\n' +
      '\n' +
      'All-Reduce is on the Critical Path & can be Large Transformers requires tensor parallelism (TP)(Tran et al., 2017) to increase the aggregate memory capacity available. 그러나 임계 경로(계층 간)에서 AR이 필요합니다. 도 2(b) 및 도 2(c)는 두 개의 디바이스(도 2(c)의 TP=2)에 걸쳐 슬라이스될 때의 동작들에 대한 FC 서브-레이어의 원래 동작들을 도시한다. 각 장치(점선 상자)에는 가중치의 슬라이스만 있습니다. 두 번째 슬라이스 가중치에 해당하는 GEMM은 부분 출력만을 생성하기 때문에 다음 레이어가 실행되기 전에 AR이 필요하다(Sliced GEMM\\(\\rightarrow\\)AR로 강조). 이러한 GEMM 및 AR 연산은 별도의 커널로 실행되며 직렬화된다.\n' +
      '\n' +
      '이러한 직렬화된 AR은 병목 현상이 될 수 있다. 그림 4는 다중 전류 및 미래형 트랜스포머에 대한 "Sliced GEMM\\(\\rightarrow\\)AR"과 다른 작동 사이의 트랜스포머의 실행 시간 분해를 보여준다(섹션 5.1.2, 5.2에 상세한 설정). 대형 모델(예: Mega-GPT-2, T-NLG)의 경우 8개 및 16개 장치 TP를 고려한다. 매우 큰 모델(예: PALM, MT-NLG)의 경우 32방향 슬라이싱을 고려하고 1조 및 10조 매개변수가 있는 미래형 모델의 경우 64방향 샤딩을 고려한다. 증가하는 TP 슬라이싱은 이들 모델의 더 큰 크기가 16개의 GPU에 들어갈 수 없기 때문에 필요하다(Zhu et al., 2017) 그리고 증가된 슬라이싱은 또한 더 큰 디바이스 카운트를 갖는 노드들에 의해 인에이블된다(Zhu et al., 2017; Wang et al., 2018). 이전 작업(Wang et al., 2018; Wang et al., 2018; Wang et al., 2018)과 마찬가지로, 우리는 커뮤니케이션이 전체 런타임의 상당한 부분이라는 것을 발견했다: 메가트론-GPT-2(Mega-GPT-2) 및 T-NLG는 커뮤니케이션에 대한 훈련 및 추론(프롬프트 단계) 시간의 최대 34% 및 43%를 소비한다. 이러한 추세는 또한 매우 큰 트랜스포머와 미래형 트랜스포머에 적용됩니다: 통신은 런타임의 최대 46%와 44%가 될 수 있습니다. 또한, FLOPS를 계산하는 것이 네트워크 대역폭보다 훨씬 더 크기 때문에(Han et al., 2017), 이러한 비율은 향후에만 증가할 것이다. 예를 들어, GEMM이 2\\(\\times\\) 더 빨라지면, 통신은 모델 실행 시간의 75%까지 증가하며, 이는 다수의 디바이스로의 스케일링을 매우 비효율적으로 만들고 통신이 일어나는 동안 잠재적으로 GPU를 유휴 상태로 만든다. 따라서, 직렬화된 AR을 어드레싱하는 것은 트랜스포머 스케일링에 중요하다.\n' +
      '\n' +
      '### 컴퓨터 통신 중첩 활성화\n' +
      '\n' +
      '독립적인 컴퓨트 커널들을 갖는 중첩 집합 커널들은 다른 분산 접근법들(예를 들어, DP, GPipe(Mega-GPT-2, 2018))에서 DNN들을 스케일링하는 데 핵심적이었다. TP는 AR과 겹치는 독립적인 커널이 없지만 생산자 GEMM 자체와의 _fine-grained overlap_의 이점을 얻을 수 있음을 관찰한다. 트랜스포머 GEMM은 타일/차단된 큰 출력을 가지며 완료하려면 많은 GPU 작업 그룹(WG)이 필요하다. 결과적으로, GEMM은 제한된 수의 GPU 계산 유닛(CU)에서 모든 WG를 동시에 실행할 수 없다. 따라서, GEMM은 다수의 _stages_에서 출력을 실행하고 생성하는데, 여기서 각 스테이지는 CU들이 수용할 수 있는 WG들의 집합이다. 이것은 AR이 필요한 슬라이스 GEMM에도 적용됩니다. 그림 5와 같이 TP의 GEMM은 WG당 계산량을 감소시키는 \\(K\\)(또는 dot-product) 차원에서 슬라이스되지만 출력 크기, WG 카운트 및 WG 단계는 동일하게 유지된다. 이 관측치를 이용하여 한 단계의 출력 데이터의 통신을 다음 단계의 연산과 중첩시킬 수 있는 세밀한 중첩을 가능하게 한다. 그러나 섹션 3에서 설명한 대로 실용적이고 효율적인 세립 중첩을 달성하는 것은 어렵다.\n' +
      '\n' +
      '##3. Fine-grained Compute-Communication Overlap과의 과제\n' +
      '\n' +
      '이 섹션에서는 계산과 통신의 세밀한 중첩에 대한 주요 문제를 자세히 설명한다.\n' +
      '\n' +
      '###complex and cost to implementation to Software\n' +
      '\n' +
      '생산자 및 집합 작업은 GPU에서 별도의 커널로 실행되며, 생산자(GEMM)는 데이터를 생성한 후 집합이 대량 통신 및 감소를 조정한다. 세밀한 인터리빙을 위해 소프트웨어를 확장하는 것은 복잡하고 비용이 많이 들 수 있다. 생산자와 집합체를 더 작은 커널로 분해하거나 동적 병렬성을 사용하는 것을 포함할 수 있다.\n' +
      '\n' +
      '그림 4. 수집품을 필요로 하는 GEMM뿐만 아니라 RS(Reduced-Scatter) 및 AG(All-Gather) 수집품에 소모되는 변압기 시간.\n' +
      '\n' +
      '그림 3. 환원산란 집합체의 링 구현.\n' +
      '\n' +
      '실행 오버헤드 및 동기화 비용을 증가시킵니다. 대안적으로, 융합된 GEMM 및 집합 커널을 기입함으로써 달성될 수 있지만, 이는 상당한 프로그래밍 노력[(16; 18; 81; 85)]을 야기할 수 있다. 첫째, BLAS 라이브러리는 값비싼 튜닝 프로세스를 통해 생성된 다양한 입력 크기 및 GPU 아키텍처에 최적화된 수백 개의 GEMM 커널을 갖는다[(3)]. 둘째, 수집품도 서로 다른 유형이며, 각각은 서로 다른 토폴로지에 최적화된 구현을 가지고 있습니다. 따라서 GEMM 및 집합 구현의 모든 조합에 대해 융합된 커널을 생성하는 것은 매우 복잡하고 비용이 많이 들 수 있다. 따라서 GEMM 구현을 변경하지 않고 계산과 통신의 세밀한 중첩을 달성하는 것이 필수적이다.\n' +
      '\n' +
      '생산자와 집단 간의 자원 경쟁\n' +
      '\n' +
      '중첩된 GEMM과 AR은 GPU 자원, 특히 유닛(CU)과 메모리 대역폭을 놓고 경쟁하며, 이는 전체 실행을 늦춘다.\n' +
      '\n' +
      '###### 3.2.1. 계산 공유\n' +
      '\n' +
      'GEMM과 AR 커널을 동시에 실행하려면 CU와 L1 캐시, LDS 및 벡터 레지스터를 포함한 구성 요소를 공유해야 한다. 이러한 경쟁은 고립된 실행과 관련하여 그들의 성능에 영향을 미칠 수 있다. 그림 6은 섹션 5.1.1 및 표 1에서 우리의 설정을 사용하여 GEMM 및 AR을 동시에 실행하는 것의 영향을 평가한다. 구체적으로 그림 6은 Mega-GPT-2 및 T-NLG(TP=8) 하위 계층(Attn)에 대한 (정규화된) GEMM 및 AR 시간을 보여준다. 및 FC-2)를 다양한 CU 카운트 분할로 분리하여 실행할 때(예를 들어, 72-8 막대는 72개의 CU를 갖는 GEMM의 격리된 실행 시간 및 8개의 CU를 갖는 AR을 나타낸다. 각각의 경우에 대해, _potential-overlap-speedup_을 또한 나타내고, 속도업 중첩 AR 및 GEMM은 각각이 80개의 CU를 모두 가질 때 GEMM 및 AR을 순차적으로 실행하는 것에 비해 얻을 수 있다. 중첩된 시간을 최대(GEMM 시간, AR 시간)로 계산한다. 이상적인 경우는 공유 효과가 없다고 가정한다: GEMM은 모든 80개의 CU를 가지고 있고 AR은 빠르지만 자유롭다(80개의 CU를 모두 격리하여 실행함으로써 평가됨). 그 결과, 이상적인 경우는 최대 1.67\\(\\times\\) 지오메인의 중첩 속도를 갖는다. 그러나 AR은 모든 CU를 가질 때와 비교하여 8개의 CU(72-8건)만 할당했을 때 상당히 느려진다(거먼\\(\\sim\\)41% 느려짐). 이는 전위 중첩 속도를 1.18\\(\\times\\) 지오메인으로 크게 감소시킨다. AR 성능은 16개의 CU(64-16 경우 \\(\\sim\\)7% 둔화)로 개선되지만 GEMM은 64개의 CU만 있기 때문에 느려진다(geomean \\(\\sim\\)21% 둔화). 전체적으로 72-8례보다 우수한 성능을 보이는 반면, 이상적인 경우에 비해 잠재적인 속도 향상은 1.49\\(\\times\\) 지오메인에 미치지 못한다. 더욱이, 이것은 메모리 대역폭 공유(다음에 논의됨)로 인한 경쟁을 가정하지 않으며, 따라서 둔화를 과소평가한다. 전반적으로 CU의 공유는 중복되는 효율성을 감소시키며 GEMM 전용 컴퓨팅 리소스를 보존하는 것이 중요하다.\n' +
      '\n' +
      '###### 3.2.2. 메모리 대역폭 공유\n' +
      '\n' +
      'GEMM 및 AR 커널은 동시에 실행될 때 메모리 대역폭을 놓고 경쟁하기도 합니다. 도 3에 도시된 바와 같이, 각각의 단계에서 AR 커널은 a) 어레이 청크를 메모리로부터 판독하여 하나의 이웃 GPU에 전송하고 또한 b) 다른 이웃으로부터 수신한 청크를 메모리에 기록한다. Reduce-scatter(RS)는 감소를 위해 로컬 청크의 메모리 판독을 추가로 필요로 한다. 더욱이, AR 통신으로 인한 메모리 트래픽은 버스트할 수 있다. 이러한 AR로 인한 추가적인 버스트 메모리 트래픽은 생산자 GEMM에 의한 임계 메모리 액세스를 늦출 수 있으며, 섹션 6.1.2 및 그림 17의 평가에서 보여주듯이 GPU의 마지막 레벨 캐시(LLC)에 입력이 맞지 않는 GEMM에 대해 더 높은 영향을 미친다. 따라서 오버랩 효율을 향상시키기 위해서는 통신으로 인한 메모리 트래픽을 제한하거나 GEMM과의 경쟁을 제한하는 것이 필수적이다.\n' +
      '\n' +
      '이전 작업은 거친 GEMM과 AR 중첩이 있는 DP 설정에서도 통신과 계산 사이의 경쟁[(71)]을 연구했다. 그들은 AR이 GEMM과 동시에 실행될 때 최대 2.4\\(\\times\\) 느려지고, 추천 모델에서 메모리 집약적 임베딩 검색과 동시에 실행될 때 느려지는 것이 훨씬 더 높다는 것을 보여준다. TP의 경우 GEMM과 동시에 실행될 때 1.4\\(\\times\\)의 둔화를 관찰한다.\n' +
      '\n' +
      '도 5. 도트-제품 치수(오른쪽)에서 슬라이스될 때 GEMM(왼쪽)은 여전히 동일한 수의 데이터 블록을 생성한다.\n' +
      '\n' +
      '그림 6. 모델 레이어에 걸쳐 중첩 GEMM과 RS의 이점이 컴퓨팅 유닛(CU) 공유에 의해 어떻게 영향을 받는지를 평가한다. X축은 표 1의 GPU 설정을 사용하여 CU가 GEMM과 AR 사이에서 어떻게 분할되는지 형식 _A-B_.\\ (A\\)는 GEMM이 사용하는 CU의 수를 나타내고, \\(B\\)는 CU AR이 사용하는 수를 나타낸다. 이상적으로는 공유가 없으며 GEMM은 모든 CU를 가지고 있으며 AR은 무료라고 가정한다.\n' +
      '\n' +
      '##4. T3: 투명 추적 및 트리거링\n' +
      '\n' +
      '복잡한 소프트웨어와 컴퓨팅과 통신의 세밀한 중첩으로 인한 자원 경쟁의 앞서 언급한 문제를 극복하기 위해 T3를 제안한다.\n' +
      '\n' +
      '### T3 Overview\n' +
      '\n' +
      '현대의 GPU는 먼저 생산자 GEMM을 실행하고 출력을 로컬 메모리에 저장합니다. 그 후에 그들은 집단적 운영을 시작한다(섹션 3). T3는 GEMM이 미세 중첩을 가능하게 하는 데이터를 생성함에 따라 집합체를 즉시 시작한다. GEMM/집단의 진행 상황을 모니터링하고 통신을 조정하기 위해 _track & trigger_ 메커니즘을 사용하여 추가 CU가 필요하지 않다(섹션 4.2). 그것은 통신으로 인한 메모리 트래픽을 줄이기 위해 감소를 위해 _near-memory compute_를 활용한다(섹션 4.3). 마지막으로, 마이너 커널 수정과 함께 이러한 _transparently_를 수행한다(섹션 4.4).\n' +
      '\n' +
      '그림 7은 생산자 GEMM과 겹치는 4개 장치 환원산란(RS)을 보여준다. 이 GEMM은 자신의 입력 및 커널 구현에 의해 지시된 WG의 다중 _stages_에서 실행되는 반면(섹션 2.5), RS는 관련된 장치의 수에 의해 지시된 다중 _steps_에서 실행된다(섹션 2.3). 그림의 단순화를 위해 GEMM 단계의 수가 필요한 링 단계의 수보다 한 개 더 많다는 것을 보여준다. 각 단계에서 GEMM 단의 실행과 출력 감소는 이전 단 출력의 통신과 병행하여 발생한다. 제1 단계에서, 출력은 GEMM(_remote_update_)에 의해 직접 원격 디바이스들에 통신된다. 이후의, _steady state_, 단계들은 DMA(_dma_update_)를 필요로 한다. \\(N\\) 장치의 경우, 이 정상 상태 단계는 다른 청크에서 \\(N-2\\)회 수행된다. 정상 상태의 GPU-0에 초점을 맞추면, 단계-\\(2\\), 도 7에 도시된 바와 같이, GPU는 GEMM 스테이지-3에 대한 출력을 실행/생성하면서, 또한 그 이웃인 GPU-1로부터 스테이지의 출력(파란색)의 복사본을 (DMA를 통해) 수신한다. 이는 GEMM 스테이지-2 데이터(노란색)의 GPU-3로의 축소된 복사본의 GPU-0의 DMA에 병렬로 발생하여, 중첩 통신을 수행한다. T3는 이러한 로컬 및 DMA 업데이트 상의 메모리 위치를 원자적으로 업데이트하기 위해 NMC(near-memory computing_)를 평균하여, 추가적인 판독 또는 GPU CU를 필요로 하지 않고 스테이지-3 청크의 부분적으로 감소된 사본을 생성한다(섹션 4.3). 그들이 완료되면, GPU-0은 단계-3에 도시된 바와 같이 청크의 _dma_update_를 이웃의 (GPU-3) 메모리로 개시한다. 업데이트 및 DMA 트리거링의 이러한 자동 추적은 경량 및 프로그램가능한 하드웨어 트래커를 사용하여 수행되며, GPU CU들에 대한 의존성을 더욱 감소시킨다(섹션 4.2). 이러한 원격/DMA 업데이트는 GEMM의 출력 주소 매핑을 마이너 애플리케이션 및 커널 수정으로 구성함으로써 투명하게 수행된다(섹션 4.4).\n' +
      '\n' +
      '또한 T3의 성능을 향상시키기 위해 작은 런타임과 하드웨어를 변경합니다. 그림 7에서 GEMM과 RS의 완벽한 중첩을 가능하게 하기 위해 GPU(섹션 4.4)에 걸쳐 GEMM 작업 그룹(WG)의 스케줄링을 스태거한다. 또한, 계산과 통신 사이의 메모리 경쟁을 관리하기 위해 간단하면서도 효과적인 메모리 컨트롤러 중재(MCA) 정책으로 메모리 시스템을 보완한다(섹션 4.5).\n' +
      '\n' +
      '도 8은 위에서 설명된 정상 상태 단계를 실행하는 T3의 향상(주황색)을 갖는 GPU를 도시한다. GPU는 GEMM을 실행하여 스테이지(C1)에 대한 로컬 업데이트를 생성한다. 동시에 GPU는 동일한 스테이지(D1a)에 대한 DMA 업데이트들을 수신하고 이전 스테이지(D1b)에 대한 DMA 업데이트들을 전송한다. 메모리 컨트롤러에서 수정된 _MCA_는 경쟁을 방지하기 위해 로컬 트래픽과 DMA 트래픽을 중재한다. 이에 이어서, 업데이트들은 _NMC-강화 DRAM_(D2a)로 전송되는 한편 _Tracker_는 그들의 진행(D2b)으로 업데이트된다. 트래커가 필요한 로컬 및 DMA 업데이트를 메모리 영역에 관찰하면 이웃 GPU(D3)로의 DMA 전송을 트리거한다.\n' +
      '\n' +
      'T3를 설명하기 위해 4-GPU GEMM-RS 중첩을 실행 예로 사용하는데, RS는 감소 및 추가 메모리 트래픽으로 인해 중첩이 더 어렵다. 또한, 상기 링은,\n' +
      '\n' +
      '그림 7. 융합된 GEMM 및 링의 개요는 4-GPU 노드에서 T3와 함께 산란을 감소시킨다.\n' +
      '\n' +
      '구성은 다른 구성보다 복잡합니다. 따라서 링-RS를 사용하여 T3에 대해 자세히 설명하고 섹션 7.1에서 추가 수집품에 대해 논의한다.\n' +
      '\n' +
      '### T3 추적 및 트리거링\n' +
      '\n' +
      'T3의 _programmable track & trigger_ 메커니즘은 컴퓨팅 리소스를 사용하지 않고 생산자와 집합체의 세밀한 중첩을 투명하게 가능하게 하는 핵심이다. 도 9에 도시된 바와 같이, T3은 준비될 때 디바이스 간에 데이터의 복사본을 자동으로 전송한다(예를 들어, 도 7에서, T3은 일단 GPU-0의 로컬 및 GPU-1의 원격 업데이트가 모두 완료되면 스테이지-2 데이터의 DMA 업데이트를 GPU-0에서 GPU-3으로 트리거한다). 이것은 메모리 컨트롤러에서 경량 _Tracker_에 의해 인에이블되며, 이는 메모리 영역에 대한 로컬 및 원격/DMA 액세스를 추적하고 필요한 액세스가 완료되면 DMA 전송을 트리거한다. DMA가 트리거될 때의 조건(예를 들어, 원격 및 로컬 업데이트의 수) 및 DMA 전송 세부사항(예를 들어, 어드레스, 동작 유형)이 집합 유형 및 구현마다 다양하기 때문에, 이들은 어드레스 공간 구성(섹션 4.4 및 도 12에서 상세함)을 사용하여 시간 전에 프로그래밍된다.\n' +
      '\n' +
      '#### 4.2.1. Tracker\n' +
      '\n' +
      '트래커는 GEMM 스테이지의 로컬 및 원격 메모리 업데이트를 모두 추적하고 DMA를 트리거한다. 도 9의 (a) 및 (b)에 도시된 바와 같이, 파면(WF, 즉, 락스텝으로 실행하는 스레드들의 그룹) 입도 1 - 즉, 트래커는 WF 업데이트들을 메모리 영역을 추적한다. 이는 타일링된 GEMM 구현들을 가정하고, 각각의 WF/WG가 모든 평가된 GEMM들(Steiner, 2012; Steiner, 2012)에서의 경우와 같이 데이터의 완전한 타일을 생성한다고 가정한다. 그러나 T3는 다른 구현(섹션 7.7)도 처리할 수 있다. 업데이트는 그 대응하는 WF의 (_wf_id_) 트래커 엔트리 2에서 카운터를 증가시킨다. 이것은 GPU의 메모리 컨트롤러에 도달하는 모든 로컬, 원격 및 DMA 업데이트에 의해 수행된다(예를 들어, GPU-0은 그 WF가 로컬로 기록하지 않고 또한 그 원격 업데이트가 수신되지 않기 때문에 GEMM 스테이지-1을 추적하지 않는다). 증가된 카운터 값은 최대 임계값에 대해 체크되며, 이는 WF 출력 크기(_wf_tile_size_)와 요소 3당 예상되는 총 업데이트의 곱으로 설정된다. _wf_tile_size_는 출력 크기 및 WF 카운트(\\((M*N)/\\#WF\\))를 사용하여 GPU 드라이버에 의해 결정된다. 링-RS에 대해 요소당 예상되는 총 업데이트는 두 가지이지만 집합 유형/구현에 따라 변경되므로 구성할 수 있다(섹션 4.4에서 상세). 일단 임계값에 도달하면, 최종 기입은 그림 9(c)의 DMA 4를 트리거하고 섹션 4.2.2에 상세화된다. 트래커는 일단 액세스들이 메모리 제어기 큐(MCQ)에서 엔큐잉되고, 따라서 임계 경로에 있지 않으면 체크된다.\n' +
      '\n' +
      'WF 기반 추적은 생산자의 (또는 GEMM의) 단계가 연속적인 메모리 영역을 업데이트하지 않을 수 있기 때문에 유익하다. 도 9(a)에 도시된 바와 같이, 이는 BLAS 라이브러리들(Bahman et al., 2012; Steiner, 2012)에서의 어레이의 열-주요 할당 및 행-주요 스케줄링으로 인해 발생할 수 있다. 이것은 WF 기반 추적에서 회피하는 주소 기반 추적을 값비싼(여러 주소를 저장하거나 복잡한 테이블 인덱싱 기능을 필요로 함) 것으로 만든다. 트래커는 총 256개의 엔트리를 가지며, 워크그룹(WG) ID의 LSBs, _wg_lsb_(8비트)를 사용하여 인덱싱된다. 각 엔트리는 연상적으로 설정되며 _wg_msb_, _wf_id_를 이용하여 태깅된다. _ wg_msb_는 \\(log\\_2(maxWGSperstage/256)\\) 비트이고 _wf_id_는 WG당 최대 8개의 WF에 대해 3비트이다. 생산자 단계에서 가능한 최대 WG를 기반으로 최대 항목을 설정했다. 각 항목에는 시작 가상 주소(WF당 가장 작은 주소)가 있으며 카운터에 액세스하여 트래커 크기가 19KB입니다. 추적은 추가적으로 소스 _wg_id_ 및 _wf_id_를 메모리 액세스에서의 메타데이터로서 요구하고 그들의 가상 어드레스를 메모리 컨트롤러로 포워딩한다(섹션 4.2.2에서 DMA를 트리거하기 위해).\n' +
      '\n' +
      '######4.2.2. 트리거 DMA\n' +
      '\n' +
      '일단 WF의 메모리 영역에 대한 요구된 액세스가 발행되면, T3 DMA는 데이터를 원격 GPU에 전송한다(도 9의 (c)). 도 9(c)에 도시된 바와 같이, DMA 명령들은 GPU 드라이버에 의해 사전 프로그래밍되고, DMA 영역들/동작들이 집합 타입 및 구현에 따라 상이할 수 있기 때문에 구성가능(섹션 4.4 및 도 12에서 상세됨)하다. DMA 블록/테이블 엔트리의 granularity는 Tracker granularity(_wf_tile_)와 같거나 크게 설정된다. 트래커 엔트리(섹션 4.2.1)에서 요구되는 액세스들을 완료하는 메모리 액세스는 대응하는 DMA 엔트리를 준비 상태로 표시하고 또한 목적지 GPU의 트래커에서 요구되는 _wg_id_ 및 _wf_id_로 채운다. DMA 블록들이 _wf_tile_의 배수인 경우, DMA 엔트리당 추가 카운터는 이들의 완료를 추적할 수 있다. 미리 프로그래밍된 시작 소스/목적지 가상 어드레스, _wf_tile_size_ 및 출력 치수들(M, N)을 사용하여, DMA 엔진은 DMA를 개시하기 위해 나머지 가상 어드레스들을 동적으로 생성한다.\n' +
      '\n' +
      '### Near-Memory Reductions\n' +
      '\n' +
      'GPU 컴퓨팅 자원을 점유하지 않고 생산자 및 DMA 업데이트에 대한 감소를 수행하기 위해 T3는 컴퓨팅 강화 메모리를 활용한다. 우리는 HBM 기반 DRAM 아키텍처가 거의 메모리 기반의 op-and-store 지원을 가지고 있다고 가정한다.\n' +
      '\n' +
      '그림 8. 정상 상태 융합 GEMM-RS 단계를 실행하는 강조된 T3 향상(주황색)을 갖는 GPU.\n' +
      '\n' +
      '(Wang et al., 2019; Wang et al., 2020). 우리는 최근 메모리 벤더들에 의해 제안된 바와 같이 DRAM 뱅크 근처의 ALU들을 통해 구현되도록 그러한 컴퓨트 지원을 구상한다(Wang et al., 2019; Wang et al., 2020). 그러나, T3는 또한 다른 환원 기판을 레버리지할 수 있다(섹션 7.4).\n' +
      '\n' +
      'T3는 이 NMC(near-memory computing) 기능을 활용하여 GEMM 저장 및 DMA 전송이 수집가가 필요로 할 때 데이터의 사본을 직접 업데이트하고 줄일 수 있도록 한다. DMA 전송의 경우 작업 유형(스토어 대 저장소)입니다. 업데이트)는 명령(도 12 및 섹션 4.4의 주소 공간 구성)에 직접 지정됩니다. GEMM의 경우 두 개의 플래그를 사용합니다. 먼저 메모리 할당 시 "비캐시" 플래그를 사용하여 출력이 GPU의 캐시(기존 GPU에서 이러한 할당이 지원됨)에 캐시되지 않도록 합니다. 따라서, 기입은 모든(로컬, 원격, DMA) 업데이트에 대한 집계의 포인트로서 작용하는 DRAM에 직접 전송된다. 메모리 컨트롤러 큐에서 업데이트들의 큐잉은 그들의 원자성을 보장한다; 임의의 주어진 시간에, 단 하나의 명령만이 근거리 뱅크 ALU들에 의해 발행되고 실행될 수 있다. 둘째, GEMM API 호출에 "업데이트" 플래그를 사용하여 DRAM을 업데이트하기 위해 GEMM의 저장이 가능하다. "업데이트" 플래그는 (커널 패킷들(Beng et al., 2019)을 통해) CU들로 전송되어 커널의 저장소들을 하나의 비트 "업데이트" 정보(이전 작업(Wang et al., 2019; Wang et al., 2020; Wang et al., 2020)로 태깅한다. 이들은 연산 및 저장 명령을 생성하기 위해 메모리 컨트롤러에 의해 처리된다.\n' +
      '\n' +
      'NMC는 GEMM을 위한 CU를 자유롭게 하는 것 외에도 통신으로 인한 메모리 트래픽을 줄이는 데 도움이 된다. 도 10은 베이스라인 및 T3과 함께 정상 상태 RS 단계에서 메모리 액세스를 도시한다. 베이스라인 RS에서 CU는 두 개의 데이터 복사본(로컬 복사본, 및 이전 이웃으로부터 수신된 복사본)을 판독하고 감소된 데이터를 다음 이웃의 메모리에 기록한다. T3는 NMC를 사용하여 이웃 GPU 메모리를 DMA 업데이트하기 위해 데이터의 한 번만 읽으면 된다. 전반적으로, NMC를 갖는 T3는 GPU CU에 대한 의존성을 감소시키고, 추가로 통신에 필요한 데이터 이동을 감소(또는 섹션 7.1에서 _direct-RS_를 제거)한다.\n' +
      '\n' +
      '###제작자의 출력주소공간 구성\n' +
      '\n' +
      '특히 다양한 모양과 크기의 많은 GEMM에 대해 수집품을 융합하고 중첩하기 위해 생산자 커널을 수정하는 것은 비실용적일 수 있다(섹션 3.1). T3는 트래커 및 DMA 명령을 프로그래밍하는 데 사용되는 생산자의 출력 주소 공간 매핑을 구성하여 이를 방지한다. 도 11 및 도 12는 도 7의 융합된 GEMM-RS 예로부터 GPU-0에 대한 이러한 구성을 도시한다.\n' +
      '\n' +
      '4개의 장치가 있기 때문에 GEMM의 출력 배열은 4가지 방법으로 청크된다. GPU-0에서, GEMM은 자신의 스테이지-1 출력을 GPU-3의 메모리에 직접 기입하는 반면(도 7의 단계-1), 자신의 스테이지-2 및 스테이지-3 출력은 먼저 로컬 메모리에 기입되고 나중에 DMA가 GPU-3에 기입된다(단계-4는 단지 로컬적으로 한번 기입되고 DMA가 아니다). 따라서, GPU-0은 도 11에 도시된 바와 같이 GPU-3의 청크와 이들 청크의 메모리 매핑을 필요로 한다. 이러한 구성은 집합 유형 및 토폴로지 최적화 구현마다 상이하며(섹션 7.1 참조), 현대의 집합 구현과 유사하게 집합 라이브러리에서 미리 정의될 수 있다(Beng et al., 2019; Wang et al., 2020). 도 12는 의사-코드를 이용한 이것의 예를 도시한다.\n' +
      '\n' +
      '도 12의 구성은 두 개의 상이한 API 호출들: _remote_map_ 및 _dma_map_를 사용하여 GEMM 출력에 대한 이러한 매핑을 정의한다. _remote_map_ 및 _dma_map__ remote_map_는 스레드들에 의해 피어-투-피어 로드/스토어에 대한 기존의 GPU 지원을 사용하는 미세-그레인 원격 쓰기/업데이트(stage-1에 대해)를 위해 사용된다(Wang et al., 2020). 반대로, _dma_map_는 GPU들(DirectGMA 및 다른 것들(Wang et al., 2019; Wang et al., 2020; Wang et al., 2020))에서 DMA 엔진들에 의한 메모리 카피들에 대한 기존 지원을 활용하는 거친-그레인드 DMA 기입/업데이트들(stage-2,3에 대해)에 사용된다. 또한, _dma_map_ 호출은 DMA 기능(스토어 vs. 업데이트) 및 그 트리거링 조건(요소당 저장소 수/업데이트 수)을 포함할 수 있다. 또한, 입도(도 9(c)의 DMA 블록당_wf_tiles_)를 지정하도록 확장될 수 있다. 이러한 호출은 준비될 때 데이터의 자동 통신을 가능하게 하기 위해 Tracker 및 DMA 명령을 사전 프로그래밍하는데 사용된다(섹션 4.2).\n' +
      '\n' +
      '링 기반 집합체에서의 융합은 또한 (다른 장치에서) 데이터 덩어리를 엇갈리게 생성하는 생산자로부터 이익을 얻는다. 그림 7에서 GPU는 생성된 데이터를 한 단계씩 스태거합니다. 단계-1에서 GPU-0은 단계-1을 실행하는 반면 GPU-1은 단계-2 등을 실행합니다. 이것은 장치 간에 WG 스케줄링을 엇갈리게 하여 활성화됩니다. 대안적으로, 엇갈린 출력 타일-to-WG로 BLAS 라이브러리로부터 적절한 구현을 인출함으로써 또한 인에이블될 수 있다.\n' +
      '\n' +
      '그림 9. T3 Track & Trigger.\n' +
      '\n' +
      '생산자 커널 간의 매핑. 전반적으로, 출력 주소 공간을 구성하는 것은 집단과의 융합을 가능하게 하기 위해 GEMM 구현들을 변경할 필요성을 완화시킨다.\n' +
      '\n' +
      '### 통신 인식 MC 중재(MCA):\n' +
      '\n' +
      '마지막으로, 생산자 커널에 의한 메모리 액세스와 통신으로 인한 액세스에 대한 세심한 스케줄링은 메모리 액세스를 효율적으로 중첩하기 위해 중요하다. 섹션 6.1에서 우리는 a) 계산 스트림과 통신 스트림에서 메모리 액세스 발행 사이의 라운드 로빈과 b) 현재 스트림이 비어 있으면 다른 스트림으로 다시 떨어지는 메모리 컨트롤러(MC) 중재 정책이 생산자 커널 둔화를 초래한다는 것을 보여준다. 통신 관련 메모리 액세스는 버스트에서 나타나고 DRAM 큐를 점유할 수 있어 컴퓨팅 커널의 중요한 메모리 읽기/쓰기를 지연시킨다. 기존의 통신 관련 메모리 액세스가 이미 DRAM 큐를 점유할 수 있기 때문에 프로듀서 커널 액세스가 나타날 때 단순히 우선순위를 정하는 것 또한 불충분하다. 마지막으로, 로컬 컴퓨팅 스트림 전용 액세스를 제공하는 것은 낭비된 사이클 및 메모리 대역폭의 과소용을 초래한다. 따라서 계산과 통신의 효율적인 중첩은 경쟁과 과소 활용을 모두 다루는 동적 중재 정책을 필요로 한다.\n' +
      '\n' +
      '이를 극복하기 위해 간단하면서도 역동적인 중재 정책을 시행한다. MC는 항상 컴퓨트 스트림 액세스를 우선시하지만, 비어 있으면 통신 스트림으로 다시 떨어진다. 또한, DRAM 큐 점유율을 모니터링하고 점유율이 임계값 미만인 경우에만 통신 관련 액세스를 발행한다. 이는 향후 컴퓨팅 스트림 액세스를 위한 대기열의 충분한 공간을 보장하고 그들의 스톨을 방지한다. 점유 임계치는 컴퓨팅 커널의 메모리-강도에 의존한다(예를 들어, 메모리-집약적인 경우 더 작고, 반대로). 이것은 동적으로 결정된다: MC는 고립된 실행 동안 점유를 모니터링함으로써 커널의 메모리 강도를 검출한다(도 7의 첫 번째 단계). 마지막으로, MC는 통신 스트림으로부터 마지막 이슈로부터 경과된 사이클들을 추적하고 그것이 굶주리지 않도록 하기 위해 한계를 초과할 경우 우선순위를 정한다. 추가적으로, 통신 스트림은 생산자 커널 경계에서 배수된다.\n' +
      '\n' +
      '## 5. Methodology\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      'Multi-GPU Simulation\n' +
      '\n' +
      '많은 인기 GPU 시뮬레이터가 공개적으로 사용 가능하지만(7; 20; 39; 74), 현대의 GPU(31)에 대해 높은 충실도를 제공하기 때문에 Accel-Sim(33)을 사용하여 T3를 평가하기로 결정했다. 이전 작업(32)과 마찬가지로 Accel-Sim을 확장하여 다중 GPU 시스템을 시뮬레이션했다. 우리는 다중 GPU DNN 설정에서 모든 GPU의 실행이 균질하다는 것을 관찰한다(도 2 및 도 10). 따라서 단일 GPU와 관련된 모든 활동을 모델링하여 다중 GPU 기준선과 T3를 모두 평가한다. 여기에는 스토어/DMA 작업 및 캐시되지 않은 NMC 업데이트와 병행하여 액세스/업데이트되는 트래커를 모델링하는 것이 포함된다. 시뮬레이터에서 DMA 엔진을 모델링하지는 않았지만, Accel-Sim에서 계산 연산(예: GEMM(30; 61))을 실행하고 Accel-Sim의 프론트 엔드 추적 기능을 사용하여 추가적인 GPU 간 통신 트래픽을 주입함으로써 GPU 간 통신(기준선과 T3의 융합된 GEMM-RS 모두에서 RS로 인한 통신)을 모델링한다. 트래커의 DMA 트리거링 오버헤드는 종종 수행되는 바와 같이 셋업 프로세스(도 12) 동안 DMA 커맨드가 미리 큐잉되기 때문에 무시할 수 있다. 표 1은 최신 GPU 아키텍처 Accel-Sim이 완전히 지원하는 T3를 평가하기 위해 사용하는 GPU 구성을 자세히 설명합니다. 이러한 구성을 갖는 상용 GPU들은 150 GB/s 상호연결 링 대역폭(53)을 지원한다. 최근 GPU는 종종 다른 리소스보다 더 빠르게 계산되기 때문에 다른 매개변수는 섹션 7.5에서 동일하게 유지되는 동안 CU 카운트가 증가된 다른 구성을 평가한다.\n' +
      '\n' +
      '그림 13은 RS의 다중 GPU 시뮬레이션을 설명한다. 각 RS 단계에서 GPU는 서브 어레이의 감소를 수행하고,\n' +
      '\n' +
      '그림 11. 4개의 GPU에 걸쳐 T3 GEMM-RS에 대한 원격 주소 매핑.\n' +
      '\n' +
      '도 10. HBM은 정상 상태 감소-산란 단계에서 판독 및 기록한다.\n' +
      '\n' +
      '(다른 청크에 대응하는) 감소된 서브-어레이를 다른 이웃 GPU로부터 또한 수신하면서 그것을 이웃 GPU에 전송한다(도 3 및 도 10(a)). 시뮬레이터는 어레이의 감소를 그대로 실행한다. 인커밍 네트워크 트래픽을 시뮬레이션하는 것은 (a) 패킷 어드레스를 결정하는 것, (b) 적절한 레이트로 패킷을 생성하는 것, 및 (c) 상호접속 비용을 모델링하는 것을 필요로 한다. 패킷 어드레스들은 감소 커널로부터의 WG들의 스토어 트레이스를 사용하여 결정된다. 다음으로, GPU 실행은 균질하기 때문에, GPU가 감소 출력을 생성하는 것과 동일한 속도로 원격 트래픽이 생성된다(원격 GPU로 전송되도록 필터링 아웃됨). 이것은 또한 원격 GPU에서의 컴퓨팅/통신 간섭으로 인한 둔화를 암시적으로 포함한다. 마지막으로, 우리는 상호접속의 간단한 링크 대역폭 및 지연 모델을 가정하여, 이들 패킷이 도착함에 따라 상호접속 비용을 추가한다. 이 설정을 검증하기 위해, 시뮬레이션된 RS 시간을 시뮬레이션된 것과 동일한 링 네트워크 대역폭을 갖는 AMD 본능(tm) MI210 GPU(Beng et al., 2015)를 갖는 4개의 GPU 노드로부터의 하드웨어 측정과 비교한다(표 1). 도 14는 시뮬레이션이 크기 범위(6-192 MB): 이상적인 점선에 대한 6% 지오메인의 오차에 대해 하드웨어 경향을 밀접하게 따르는 것을 보여준다.\n' +
      '\n' +
      '**Near-Memory Computing**: 시뮬레이터의 HBM을 수정하여 NMC 업데이트를 모델링합니다. 또한, 메모리 벤더 제안들은 DRAM 타이밍들의 상당한 증가 없이 NMC 동작들이 발행될 수 있음을 나타낸다; 백-투-백 NMC 동작들은 동일한 컬럼-투-컬럼 액세스(CCDL) 지연을 갖는 동일한 뱅크 그룹에 발행될 수 있다(Kumar et al., 2017). NMC op-and-store 작업의 추가 비용을 모델링하기 위해(섹션 4.3), 시뮬레이터의 HBM을 수정하여 2\\(\\times\\) 더 높은 CCDDL 지연(테밍)을 사용한다(표 1 참조).\n' +
      '\n' +
      '1.2. End-to-End Transformer Iteration\n' +
      '\n' +
      'T3를 사용하여 종단 간 반복을 평가하기 위해 기본 변압기 고장(그림 4에 표시됨)에서 GEMM 및 RS 시간을 시뮬레이션 속도 향상(섹션 5.1.1에 설명됨)으로 스케일링한다. 본 논문에서는 기존 연구(Zhu et al., 2019; Wang et al., 2019)에 의해 수행된 하드웨어 데이터와 분석 모델링의 조합을 활용하여 분산 설정에서 모델의 종단간 분해를 구한다. AMD Instinct(tm) MI210 가속기(GPU)에서 MLPerf v1.1(Wang et al., 2019) BERT의 단일 GPU 혼합 정밀도(Zhu et al., 2019) 실행을 사용하고, 하이퍼 파라미터 및 셋업(예를 들어, 슬라이스 GEMM)을 변경하는 것에 기초하여 그 동작 시간을 스케일링한다. 이는 더 큰 미래 모델(트랜스포머 모델은 레이어 크기/카운트에서만 유사한 차이가 있음(Zhu et al., 2019; Wang et al., 2019))을 평가하는 것을 돕고, 이미 MLPerf 구현에서 트랜스포머에 대한 여러 GPU 최적화(Zhu et al., 2019; Wang et al., 2019)를 고려하기 때문에 유익하다. 우리의 예측은 이전 작업으로 측정된 것과 더 일치한다. 예를 들어, TP-16과 함께 Mega-GPT-2에 대해 투영된 AR의 퍼센트 런타임 기여도는 유사한 시스템 구성(Wang et al., 2019) 상에서 이전 작업들의 측정치들과 매칭된다.\n' +
      '\n' +
      '### 응용, 배포 및 GEMM\n' +
      '\n' +
      '**모델 및 그 전개**: 트랜스포머는 빠르게 진화하고 있기 때문에, 우리는 트랜스포머 모델의 범위 및 TP 도에 대한 T3의 영향을 평가한다(표 2). Megaton-GPT-2(Mega-GPT-2)(Wang et al., 2019)와 T-NLG(Wang et al., 2019)의 경우, 16K 및 8K 입력 토큰(=입력 길이 \\({}^{*}\\) 배치 크기)과 8 및 16의 TP 정도를 사용한다(Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). PALM(Beng et al., 2015), GPT-3(Beng et al., 2015), 및 MT-NLG(Wang et al., 2019)와 같은 더 큰 트랜스포머의 경우, 우리는 점점 더 큰 메모리 용량 요구 사항(Wang et al., 2019) 및 이러한 슬라이싱을 가능하게 할 수 있는 더 큰 장치 카운트를 갖는 노드의 가용성을 고려할 때 32의 더 높은 슬라이싱 정도를 사용한다(Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). 우리는 반 정밀도(FP16) 전방 및 역전파 및 단일 정밀도(FP32) 가중치 업데이트를 수반하는 혼합 정밀도 훈련을 평가한다. 마찬가지로 FP16 추론을 평가한다.\n' +
      '\n' +
      '**GEMMs**: 전술한 애플리케이션들로부터의 GEMMs는 최신 BLAS 라이브러리들로부터의 구현들을 사용하여 시뮬레이션된다(Wang et al., 2019; Wang et al., 2019). 대부분의 GEMM(우리가 평가하는 모든 GEMM을 포함)은 각각의 WG가 데이터의 완전한 타일(섹션 7.7에서 논의된 다른 구현)을 생성하는 타일형 GEMM 구현을 사용한다. 또한, MLPerf의 BERT(Wang et al., 2019; Wang et al., 2019)에서 관찰된 바와 같이, 비-트랜스포즈(예를 들어, 후방 GEMM) 및 전치(예를 들어, 전방 GEMM) 입력 텐서를 모두 갖는 GEMM을 평가한다.\n' +
      '\n' +
      '### Configurations\n' +
      '\n' +
      'T3의 효능을 평가하기 위해 다음 구성을 사용한다:\n' +
      '\n' +
      '* _Sequential_: 베이스라인 구성이다. 현대 시스템과 마찬가지로 순차적으로 슬라이스된 GEMM과 다음 AR 커널을 순차적으로 실행한다.\n' +
      '* _T3_: GEMM을 RS(섹션 4에서 설명한 대로)와 융합하고 중첩한 후 순차 올-게더(AG)하는 우리의 제안이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|} \\hline \\multicolumn{2}{|c|}{**System**} \\\\ \\hline \\#GPUs & 8, 16 \\\\ \\hline Inter-GPU & Ring, 150 GB/s b-directional \\\\ Interconnect & 500 ns link latency \\\\ \\hline \\multicolumn{2}{|c|}{**Per-GPU Config**} \\\\ \\hline \\#CUs & 80, 14 GHz \\\\ \\hline Per-CU Config & 2K threads, 128KB unified LDS + L1 cache (with no write-allocate), 256KB RF \\\\ \\hline L2 & 16MB, 64 banks, 1.4 GHz \\\\ \\hline HBM2 & 1 TR/s, 1 GHz, CCDWL=4, Bank Grp-4, rest (Beng et al., 2015) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1. 시뮬레이션 설정.\n' +
      '\n' +
      '도 13. 다중-GPU 감소-산란 시뮬레이팅.\n' +
      '\n' +
      '* _T3-MCA_: T3에서와 같이 융합된 GEMM-RS를 사용하지만, 섹션 4.5에서 논의된 메모리 컨트롤러 중재(MCA)도 포함한다.\n' +
      '* _Ideal-GEMM-RS-Overlap_: 소프트웨어에서 이상적인 GEMM 및 RS 중첩을 나타낸다. 따라서, 그 성능은 GEMM과 RS의 고립된 커널 실행 시간 중 최대이며, 그 다음이 AG 시간이다. 또한, GEMM과 RS 사이의 의존성 제약이나 자원 경쟁을 가정하지 않는다.\n' +
      '* _Ideal-RS+NMC_: 가까운 메모리 컴퓨팅과 함께 RS를 사용하며, 이는 완벽한 오버랩을 넘어 추가적인 속도 향상을 제공할 수 있다. 따라서, 그 성능은 이상적인 GEMM-RS-Overlap에 비해 max(GEMM, RS+NMC)이다.\n' +
      '\n' +
      '## 6. Results\n' +
      '\n' +
      '### 실행 시간 분배 및 속도 향상\n' +
      '\n' +
      '도 15 및 도 16은 AR: 순방향 패스(fwd)에서 출력 프로젝션(OP) 및 완전-연결-2(FC-2) 및 백프로프(bwd)에서 완전-연결-1(FC-1) 및 입력 프로젝션(IP)을 필요로 하는 트랜스포머 내의 모든 슬라이스된 서브-레이어에 대한 결과를 도시한다. 이를 Mega-GPT-2 및 T-NLG와 두 개의 TP 설정(TP 8 및 16)에 대해 보여준다. 그림 15는 GEMM, RS 및 AG 간의 각 사례의 런타임 분포를 보여준다. 도 16은 _T3_, _T3-MCA_를 사용한 _sequential_에 대한 그들의 속도 향상뿐만 아니라 GEMM과 RS(_Ideal-GEMM-RS-Overlap_)의 이상적인 중첩을 가정한 속도 향상 및 NMC(_Ideal RS+NMC_)를 사용한 더 빠른 RS로 인한 추가 속도 향상을 도시한다.\n' +
      '\n' +
      '이상적인 스피드업\n' +
      '\n' +
      '도 16은 이상적인 가능한 속도 향상을 보여주고 두 부분으로 나눈다: 첫째는 GEMM 및 RS 커널(_Ideal-GEMM-RS-Overlap_)과 둘째는 NMC(_Ideal RS+NMC_)로 인한 개선된 RS 성능이다.\n' +
      '\n' +
      '도 16에서 이상-GEMM-RS-Overlap(자원 및 데이터 의존 제약 없이)은 생산자 GEMM을 중첩하고 RS를 따르는 상당한 이점을 보여준다: 50% 최대 속도 향상 및 35% 지오미안 대 시퀀스. 속도 향상은 모델 내 및 모델마다 다르며 GEMM 및 RS의 격리된 실행 시간에 따라 달라진다(도 15). GEMM과 RS 런타임이 유사한 상황(그림 15에서 유사한 비율)은 GEMM이 RS의 비용을 모두 숨기기 때문에 최대 잠재력을 가지고 있다. 예를 들어, TP=16인 T-NLG의 FC-1은 50%의 속도 향상을 얻는다. 또는 GEMM 또는 RS 비용이 대부분 노출되기 때문에 GEMM 및 RS 시간이 왜곡되는 경우가 가장 적은 이점을 나타낸다. 예를 들어, TP=16인 Mega-GPT에서 이상적인 GEMM-RS-오버랩 속도 향상은 OP의 경우 15%에 불과하지만, 후자는 드물고 매우 작은 층을 슬라이스한 결과이다(OP는 전체 중 가장 작다). 동일한 모델 내의 다른 하위 계층, 또는 도면에 도시된 바와 같이 더 큰 모델에 대해서는 성립하지 않는다(섹션 6.4도 참조). 주어진 하드웨어 설정에 대해, 이러한 실행 시간 비율, 따라서 이상적인-GEMM-RS-오버랩 속도 향상은 계층 파라미터에 의해 지시된다(Shen et al., 2017).\n' +
      '\n' +
      '그림 16에서 이상적인-RS+NMC는 완벽한 중첩이 제공하는 것 이상으로 추가적인 속도 향상이 가능함을 보여준다. GEMM에 대한 모든 CU를 자유롭게 하는 것 외에도, 메모리 근처에서 RS 감소를 수행하는 것은 또한 RS의 메모리 트래픽을 낮춘다(섹션 4.3에서 설명됨). 이는 TP=8과 TP=16에서 RS가 각각 7%와 3% 빨라진다. NMC는 상호 연결 비용이 모든 이전 단계를 지배하고 따라서 TP가 증가함에 따라 런타임 이점이 감소하고 따라서 총 단계가 증가함에 따라 RS의 최종 단계 시간만 감소시킨다. 도 16에 도시된 바와 같이, 이러한 더 빠른 RS는 중첩 시간을 감소시킬 수 있고 최대 4%의 추가 속도 향상을 제공할 수 있다. 직관적으로, 더 빠른 RS의 영향은 RS가 GEMM보다 더 오래 실행되는 계층에서만 분명하고 중첩될 때 그렇지 않으면 숨겨진다.\n' +
      '\n' +
      'T3 스피드업\n' +
      '\n' +
      'T3는 GEMM과 해당 소비자 RS를 세립 방식으로 투명하게 겹친다. 더욱이, T3의 경량 트랙-&-트리거 메커니즘 및 근거리 메모리 계산의 사용은 GEMM에 대한 모든 CU를 자유롭게 하고 DRAM 트래픽을 감소시킨다(도 18 및 섹션 6.2). 따라서, T3는 최대 39%(20% 지오메안, 옐로우 바, 도 16)의 속도 향상을 달성한다.\n' +
      '\n' +
      '개별 속도 향상은 상당히 다양하며 GEMM으로부터의 DRAM 트래픽과 동시, RS(섹션 6.2의 세부사항) 간의 경쟁 정도에 의해 크게 영향을 받는다. OP 층들의 경우, T3는 이상적인-GEMM-RS-오버랩 속도 향상들에 가깝게 달성하고, 심지어 특정 경우들을 초과한다. 이것은 OP GEMM이 작고 LLC에 크게 적합하기 때문에 발생하며, 시퀀스(도 18에 도시됨)에서 매우 작은 DRAM 판독 트래픽을 갖는다. 따라서, T3에서 중첩된 RS로부터의 추가적인 DRAM 트래픽은 GEMM들의 진행/실행에 거의 영향을 미치지 않는다. 대신, T3는 더욱 개선된다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|} \\hline\n' +
      '**Model Name** & **Hyperparams** & **Inputs** & **TP degree** \\\\ \\hline Mega-GPT-2 & H=3072, L=74 & SL=1K, B=16 & \\(8,16\\) \\\\ \\hline T-NLG & H=4256, L=78 & SL=1K, B=8 & \\(8,16\\) \\\\ \\hline GPT-3 & H=12K, L=96 & SL=1K, B=2 & \\(32\\) \\\\ \\hline PALM & H=18K, L=118 & SL=1K, B=2 & \\(32\\) \\\\ \\hline MT-NLG & H=20K, L=105 & SL=1K, B=2 & \\(32\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2. 스터디된 모델, 하이퍼 파라미터 및 설정.\n' +
      '\n' +
      '도 14. 다중-GPU 감소-산란 시뮬레이션의 검증.\n' +
      '\n' +
      '이러한 경우 NMC를 통해 RS 런타임하고 추가 이상-RS+NMC 속도 향상을 가능하게 한다. 마지막으로, 트랙&트리거 메커니즘은 작은 WF 입도에서 동작하지만, 일반적으로 GEMM 단계의 다수의 WF/WG로부터의 데이터는 동시에 전송될 준비가 되어 높은 네트워크 대역폭 이용률을 초래한다. 더욱이, 이것이 사실이 아닌 경우에도, T3는 컴퓨팅/GEMM 실행과 통신이 중첩되어 대기 시간을 숨기기 때문에 이를 용인할 수 있다.\n' +
      '\n' +
      '다른 많은 경우, 특히 훨씬 더 큰 FC 계층에서, 이점은 이상적인-GEMM-RS-오버랩(>15% 더 느림)을 갖는 것과 거리가 멀다. 그림 17은 GEMM의 고립 실행과 비교하여 세립 중첩이 있는 DRAM 트래픽(Y축)과 GEMM 둔화(X축)를 보여준다. 도 17(a)에 도시된 바와 같은 고립된 GEMM은 다수의 스테이지(섹션 2.5)에서 실행되며, 각각은 판독 단계(파란색) 다음에 버스트 기록 단계를 가지며, 이는 판독 트래픽을 제한한다. 중첩 RS는 그림 17(b)와 같이 추가적인 DRAM 트래픽을 유도한다. 추가 트래픽 외에도 T3, GEMM 및 RS에서는 NMC를 사용하여 업데이트 메모리를 직접 기록한다(섹션 4.3). 이러한 추가적인 읽기 버스트(스테이지에 대한 RS_reads for a stage)는 로컬 및 이웃의 복사본 모두 메모리를 업데이트한 즉시 발행되고 업데이트(이전 이웃으로부터 다음 스테이지에 대한 RS_updates)는 도시된 바와 같이 로컬 GEMM 읽기를 더 지연시킬 수 있어, GEMM이 상당히 느려지게 한다.\n' +
      '\n' +
      'T3-MCA 스피드업\n' +
      '\n' +
      'T3-MCA(섹션 4.5)는 간단한 중재 로직을 사용하여 버스트 RS 트래픽(섹션 6.1.2, 도 17)으로 인해 GEMM 읽기 스톨을 제한한다. DRAM 큐 점유율이 GEMM 커널의 메모리 세기에 의해 결정되는 임계치(5, 10, 30, 또는 제한 없음)에 도달할 때 통신 관련 액세스를 제한함으로써 RS 트래픽이 DRAM 큐를 완전히 점유하는 것을 방지한다. T3-MCA는 순차실행에 비해 최대 47%, 지오메인은 30%(최대 29%, T3에 비해 13%)의 상당한 이점을 제공한다. 또한, T3-MCA를 사용한 지오메인의 속도 향상은 이상적인 GEMM-RS-오버랩보다 5%만 작다. T3-MCA가 이상적이지 않은 개별 사례(예: TP=16인 T-NLG에서 FC-1)가 있다. 이는 GEMM 쓰기의 L2 바이패스(가까운 메모리 업데이트용)가 GEMM의 성능에 부정적인 영향을 미치는 경우를 나타낸다. 결과적으로 전체 중첩 런타임도 증가한다.\n' +
      '\n' +
      '### 데이터 이동 감소\n' +
      '\n' +
      '개선된 성능 외에도, T3 및 T3-MCA는 또한 서브-레이어에 대해 DRAM으로의 및 DRAM으로부터의 데이터 이동을 최대 36% 및 평균 22% 감소시킨다. 도 18은 모든 경우에 걸쳐 단일 GPU에 대한 총 메모리 액세스 및 이들의 상세한 브레이크다운(제1 GEMM, RS 및 AG 판독/기록 중)을 도시한다. AG 읽기/쓰기가 베이스라인(순차적)과 T3-MCA 사이에 일정하게 유지되는 반면, 나머지에 영향을 미치는 이유들의 조합이 있다: (a) GEMM과 RS의 융합은 GEMM의 첫 번째 단계에서 로컬 쓰기를 제거하고 RS의 첫 번째 단계에서 읽기를 제거하고, (b) 근접 메모리 감소는 RS 단계마다 부분 복사본의 읽기를 제거하고, 최종 단계의 감소에서 읽기와 쓰기를 제거하고, (c) GEMM의 출력 쓰기를 LLC 바이패스하는 것은 캐시에 민감한 GEMM에 대한 입력 읽기 캐싱을 개선하여 GEMM의 로컬 읽기를 감소시킨다. 이러한 영향은 TP 정도에 따라 달라지며, 일회성 감소(첫 번째 및 마지막 RS 단계에서)는 전체 RS 단계가 적기 때문에 TP 정도가 작을수록 훨씬 더 큰 영향을 미친다. 반대로, GEMM 읽기 캐싱 영향은 TP 정도가 클수록 더 높으며, TP/슬라이싱이 클수록 LLC-아메니블 GEMM이 더 작아진다. 전체적으로 RS의 읽기는 TP=8의 경우 2.5\\(\\times\\), TP=16의 경우 2.2\\(\\times\\), GEMM과 RS의 쓰기 모두 10%의 지메인이 감소하며(TP=8의 경우 14%, TP=16의 경우 7%), 마지막으로 GEMM의 읽기는 TP=8의 경우 1.56\\(\\times\\), TP=16의 경우 2\\(\\times\\)의 지메인이 감소한다.\n' +
      '\n' +
      '### 엔드 투 엔드 모델 속도 향상\n' +
      '\n' +
      '그림 19와 같이 T3와 T3-MCA는 모델 훈련을 최대 9%와 12%, 지오메인은 7%와 10% 각각 속도를 높인다. AR이 필요한 슬라이스된 하위 층의 비율이 전반적으로 높기 때문에 더 큰 TP에서 이점이 더 높다(섹션 4). 마찬가지로 추론 중 프롬프트 처리 및/또는 큰 입력 토큰 처리도 T3 및 T3-MCA를 사용하여 각각 최대 12% 및 15%, 지오메인이 9% 및 12%만큼 빨라진다. 추론 속도 향상은 백프로프 계산이 없기 때문에 슬라이스된 하위 층의 비율이 전반적으로 더 높기 때문에 더 좋다. 마지막으로, 우리가 평가하는 MLPerfv1.1 구현은 핵심 융합 최적화(Garon et al., 2018)를 포함하지 않으며, 이는 비-슬라이스 어텐션 연산을 실행 시간의 상당한 40-45%로 만든다. 그래서, 우리는\n' +
      '\n' +
      '도 16. T3를 구비한 변압기 서브층 속도 향상\n' +
      '\n' +
      '도 15. 변압기 서브-레이어 런타임 분포.\n' +
      '\n' +
      '더 새로운 MLPerf 구현에 대해 T3 및 T3-MCA의 이점이 훨씬 더 높을 것으로 예상된다.\n' +
      '\n' +
      '대형 변압기에 미치는 영향\n' +
      '\n' +
      '또한 그림 19와 같이 TP도가 높은 대형 트랜스포머를 평가한다. 소형 모델과 유사하게 GPT-3(175B 매개변수), PALM(530B 매개변수) 및 MT-NLG(540B 매개변수)에 대해 최대 35% 및 29%의 계층 수준 속도 향상이 높다. 이들은 각각 훈련에서 최대 12%와 14%의 종단간 속도 향상과 즉각적인 추론 단계로 이어진다. 따라서 T3-MCA도 효과적으로 더 큰 모델의 속도를 높입니다.\n' +
      '\n' +
      '## 7. Discussion\n' +
      '\n' +
      '### 기타 수집품 구현 및 유형\n' +
      '\n' +
      'T3는 GEMM의 출력 주소 공간의 구성을 통해 다른 집합체 및 구현을 지원한다(섹션 4.4).\n' +
      '\n' +
      '**다른 구현**: 수집품들은 상이한 토폴로지들에 대해 최적화된 다수의 구현들을 가질 수 있다. 우리는 링이 텐서 슬라이싱이 사용되는 노드 내 설정에서 일반적으로 사용되기 때문에 링에 초점을 맞춘다(Zhou et al., 2017). T3는 또한 완전히 연결된 토폴로지에서 _direct_RS 구현을 지원할 수 있다. 모든 GEMM 단계에서 각 장치의 출력은 전용 링크를 사용하여 나머지 장치에 흩어져 목적지에서 감소한다. 이는 도 12에서의 구성을 변경하여 각각의 GEMM 스테이지 출력을 슬라이스하고, 각각의 슬라이스를 원격 디바이스로 리모트_맵함으로써 달성된다. 이 경우 T3는 GEMM 저장소를 사용하여 완전히 조정됨에 따라 집합체에 의한 메모리 액세스를 제거한다. 마찬가지로, 트랙&트리거 메커니즘의 적절한 프로그래밍을 통해 다른 노드간 구현도 지원할 수 있다.\n' +
      '\n' +
      '**다른 유형**: 마찬가지로 T3도 다른 컬렉션을 지원합니다. 링/다이렉트 올-게더(AG)는 링-RS의 구성 및 설정을 재사용하지만, GEMM 및 DMA 전송은 메모리 위치를 업데이트하지 않는다. AG와 유사하게, T3는 또한 원격/dma_mapped GEMM 출력이 로컬 메모리에 기록되지 않는 것을 제외하고, 디바이스들이 서브-어레이들을 교환하는 전체-대-전체 집합체를 지원할 수 있다.\n' +
      '\n' +
      '##### 기타 분산 기법\n' +
      '\n' +
      '텐서 병렬(TP) 설정에서 통신에 중점을 두지만, T3는 생산자의 출력이 집합적으로 전달되는 다른 분산 설정에서도 적용할 수 있다.\n' +
      '\n' +
      '**전문가 병렬성**: TP와 유사하게, Mixture-of-experts(MoEs)에서의 전문가 병렬성(Zhou et al., 2017; Zhang et al., 2017)은 섹션 7.1에서 논의된 바와 같이 T3와 융합될 수 있는 직렬화된 전체-전체 통신을 요구한다.\n' +
      '\n' +
      '**Data & Pipeline Parallelism**: T3는 RS 및 피어 투 피어 전송을 각각 필요로 하는 데이터-병렬 및 파이프라인-병렬 셋업에도 적용된다. T3의 중복 혜택들은 그러한 경우들에서 추가적인 혜택을 제공하지 않을 수 있지만(이러한 통신들은 다른 독립적인 커널들과 중첩될 수 있음), T3의 NMC 및 MCA 기법들은 이러한 경우들에서 또한 메모리 대역폭 경쟁을 감소시키는 것을 도울 수 있다.\n' +
      '\n' +
      'All-gather**를 갖는**TP: T3는 집합의 출력이 장기간 지속되는 소비자 운영과 중복을 요구하는 분산 설정에 대해 확장될 수 있다. 생산자가 단기(예: 모든 활동을 수집하는 TP)인 경우 필요합니다. 겹치는 집단-소비자 쌍은 원칙적으로 겹치는 생산자-집단과 유사하며 유사한 추적/트리거 메커니즘을 필요로 한다. 추적기는 "GEMM-WG\\(\\rightarrow\\)all-reduced-output" 대신 "all-gathered-input\\(\\rightarrow\\)GEMM-WG"를 추적한다. 더욱이, DMA를 트리거하는 대신에, WG 스케줄링 이벤트를 트리거할 것이다(예컨대, Lustig & Martonosi (2018)). 이는 "전체 깃털 입력\\(\\rightarrow\\)GEMM-WG" 매핑이 커널 구현에 의존적일 수 있기 때문에 어려울 수 있다. 그러나 추가 프로그래밍 힌트는 이를 극복할 수 있다.\n' +
      '\n' +
      '### Generative Inference\n' +
      '\n' +
      '통신이 많은 훈련과 추론의 신속한 단계에 초점을 맞추지만, T3는 추론의 생성 단계에도 적용할 수 있다. 더 작은 입력 토큰 카운트들로 인해\n' +
      '\n' +
      '도 17. TP=8 및 SLB=4K인 T-NLG FC-2에 대한 (a) 베이스라인 GEMM, (b) T3에서의 전체 DRAM 트래픽.\n' +
      '\n' +
      '(섹션 2.1), 이러한 페이즈들은 모델 가중치들의 메모리 액세스들에 의해 바인딩되고 TP가 제공하는 다수의 디바이스들의 집합 메모리 대역폭으로부터 이익을 얻을 수 있다(Beng et al., 2017). 결과적으로 활성화의 모든 감소는 훈련 중인 것보다 작지만 잠재적으로 잠복기(토큰 수가 적기 때문에)는 여전히 T3를 사용하는 GEMM 실행으로 중복되고 숨겨질 수 있다.\n' +
      '\n' +
      '### 기타 환원 기판\n' +
      '\n' +
      'T3는 환원 기반 집합체(예: RS, AR)에서 필요한 원자 업데이트에 대해 NMC를 활용하지만 요구 사항은 아니다. 이러한 업데이트는 성능에 큰 손실 없이 캐시되지 않은 데이터에 대한 시스템 전체 원자학을 통해 처리될 수도 있다. 유사하게, T3는 또한 이전 작업(Rosen et al., 2019)에 의해 보여지는 바와 같이 감축을 위해 스위치를 레버리지할 수 있다.\n' +
      '\n' +
      '### 미래 하드웨어 및 하위 정밀도\n' +
      '\n' +
      '계산 FLOPS는 하드웨어 세대에 걸쳐 네트워크 링크 대역폭보다 훨씬 더 확장되었기 때문에(Han et al., 2016; Wang et al., 2017; Wang et al., 2018; Wang et al., 2019), 통신은 우리가 평가하고 향후에 평가하는 것보다 더 최근의 시스템 모두에서 종단간 실행의 더 큰 비율이 될 것이다. 유사하게, 정밀도를 낮추는 것(Wang et al., 2017; Wang et al., 2018)은 통신(선형적으로)보다 계산 시간을 훨씬 더 많이(4차적으로) 감소시킨다. 따라서 T3와 같은 기술로 통신을 숨기는 이점은 16b 외에 다른 GPU 구성 및 데이터 유형에도 적용될 것이다.\n' +
      '\n' +
      '본 논문에서는 GPU-2X-CU를 기반으로 하는 네트워크 링크 대역폭(2\\(\\times\\))보다 더 큰 FLOPS 스케일을 계산하는 시스템 구성에 대해 연구한다. GPU FLOP를 세대에 걸쳐 스케일링하는 것은 더 강력한 CU(더 큰/더 빠른 텐서 처리)에서 비롯되는 반면, CU의 수를 스케일링하고 기본 네트워크를 동일하게 유지함으로써 시뮬레이션할 수 있다. 이는 Accel-Sim이 지원하는 최신/검증된 GPU 모델과 GEMM 트레이스(Zhu et al., 2019)를 사용할 수 있게 한다. 도 20은 컴퓨팅 시간이 지배적인 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(OP)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어(FC-2)의 경우, 컴퓨팅 시간과 통신의 균형이 더 큰 레이어( 이러한 시나리오에 대해서는 통신 최적화가 필요할 것이다(Han et al., 2016; Wang et al., 2018). 그럼에도 불구하고 더 큰 계층은 전체 실행에 더 두드러진 영향을 미치며 이에 대해 T3의 이점은 개선만 된다.\n' +
      '\n' +
      '팔로우 동작을 위한### NMC\n' +
      '\n' +
      '수집물, 특히 트랜스포머에서 올-환원(all-reduce)은 일반적으로 모든 장치에 대한 다른 메모리 집약적 동작(예를 들어, DP(Zhu et al., 2019)에서의 파라미터 업데이트 또는 TP에서의 잔류/드롭아웃 층)이 뒤따른다. 이러한 동작들은 각 디바이스 상의 전체-감소된 어레이 상에서 중복적으로 동작한다. T3를 사용하면, 이들 다음의 메모리 집약적인 동작들은 나머지 디바이스들에 모두-풍화/방송되기 전에 데이터의 (감소된) 서브-어레이들 상에서 NMC(Zhu 등, 2019)를 사용하여 실행될 수 있고, 따라서 리던던시를 감소시키고, 분산 트랜스포머 모델들을 더욱 가속화할 수 있다.\n' +
      '\n' +
      '### 기타 GEMM 구현\n' +
      '\n' +
      'T3는 전체 타일/하위 타일 데이터를 생성하는 WG/WF를 사용하는 가장 일반적인 타일형 GEMM 구현에 중점을 둔다. 그러나, T3는 split-K와 같은 다른 구현들을 지원할 수 있다(Wang et al., 2018). 분할-K 구현 슬라이스는 누적 또는 \\(K\\) 차원에서 작동하여 다중 WG가 단일 타일에 대해 책임이 있으며, 각각은 이후에 감소된 부분 타일을 생성한다. Split-K는 출력 크기(\\(MxN\\))가 작지만 \\(K\\) 차원이 클 때 병렬성이 증가한다. 그러나 AR을 필요로 하는 텐서 슬라이스 GEMM은 큰 출력 크기와 작은 \\(K\\) 치수를 갖는다. 순차적으로, 분할-K 구현(요소에 대한 하나 이상의 업데이트 포함)을 갖는 T3는 메모리 위치당 다수의 로컬 및 원격 업데이트를 야기할 것이다. 이를 방지하기 위해 T3는 커널 패킷들의 타일 크기 메타데이터를 이용하여 split-k degree(=(#WGs " tile-size)/(M*N)) 즉, 요소당 업데이트 횟수를 추론할 수 있다. 트래커(섹션 4.2.1) 내의 가상 어드레스들은 동일한 타일을 업데이트하는 WFs/WGs/트랙커 엔트리들을 결정하는 데 사용될 수 있으며, 이는 트랙커가 타일에 대한 모든 업데이트가 완료된 후에만 원격 DMA를 트리거할 수 있게 한다.\n' +
      '\n' +
      '그림 19. 엔드 투 엔드 모델 속도 향상\n' +
      '\n' +
      'Figure 20. T3 on future hardware with 2\\(\\times\\) compute.\n' +
      '\n' +
      '### Multi-node Setups\n' +
      '\n' +
      '직렬화된 통신을 갖는 텐서-병렬주의는 일반적으로 고속의 동종 링크를 갖는 노드 내에서 사용된다. 그러나 T3는 더 느리고 종종 이질적인 링크를 갖는 노드간 설정에서 직렬화된 통신에도 적용될 수 있다. 결과적으로 통신 비용은 GEMM 실행보다 훨씬 더 클 수 있어 세밀한 중첩으로 인한 이점을 잠재적으로 제한할 수 있다: 계산이 완전히 중첩되면 나머지 통신 비용이 노출될 것이다(Srivastava et al., 2017). 그럼에도 불구하고 T3는 여전히 GEMM 실행 비용을 최대한 숨길 수 있는 이점을 제공할 수 있다.\n' +
      '\n' +
      '##8. 관련업무\n' +
      '\n' +
      '표 3은 T3-MCA를 여러 주요 메트릭에 걸친 이전 작업과 비교한다. 일부 선행 연구에서는 최대 2\\(\\times\\)(Srivastava et al., 2017)까지의 통신 속도를 높이기 위해 _in-switch collectives_를 설계하였다. 그러나, 이것은 임계 경로로부터 직렬화된 통신을 제거할 수 없다. 또한 토폴로지에 따라 달라지므로 스위치가 필요합니다. 컴퓨팅과 통신의 세밀한 중첩을 가능하게 하는 것은 통신 비용을 효과적으로 숨기기 위해 필수적이다. 이를 하고자 하는 기존의 시도들은 _CocoNet(Cai et al., 2019)_ 및 _Google Decomposition_(Srivastava et al., 2017)과 같이 한계가 있다. _ Google Decomposition_는 GPU 소프트웨어 인프라스트럭처에 파괴적일 수 있는 행렬 곱셈(GEMM) 커널에 대한 변경을 요구한다(섹션 3.1).\n' +
      '\n' +
      '또한, 두 접근법 모두 컴퓨팅과 통신 사이의 하드웨어 자원 경쟁을 겪을 수 있다(섹션 3.2). 경쟁을 줄이는 작업은 DP와 같은 경우 컴퓨트와 통신의 거친-그레인 오버랩만을 다루며, 직렬화된 집합체에서 세밀한-그레인 오버랩에 대한 지원이 부족하다(Srivastava et al., 2017). 또한 전용 가속기에 의존합니다. 다른 최근의 작업은 GPU 커널이 WG 레벨에서 계산 및 종속 통신을 모두 수행하도록 세밀한 중첩을 가능하게 하기 위해 계산 커널 내의 통신을 융합한다(Srivastava et al., 2017). 그러나, 이것은 계산 커널에 대한 명시적인 변경을 요구하며, 여전히 인터-GPU 동기화에 의해 제한될 리듀스-산란과 같은 간단한 산술 연산을 수반하는 콜렉티브들에 대해 쉽게 적용가능하지 않다. 마지막으로, 신디케이트와 같은 다른 작업은 분산 훈련에서 거친 입도의 중첩 기회와 효율성을 증가시킨다. 그러나, Syndicate는 직렬화된 통신을 숨길 수 없다(Srivastava et al., 2017). _ T3-MCA는 이러한 단점을 극복하고 컴퓨팅과의 직렬 통신의 투명한 중첩을 달성하면서 자원 경쟁을 최소화한다._\n' +
      '\n' +
      '## 9. Conclusion\n' +
      '\n' +
      '트랜스포머 모델은 분산 기술에 점점 더 의존하여 여러 장치 간의 통신을 필요로 한다. 이 통신은 특히 모델 실행과 통신을 직렬화하는 텐서 병렬(TP)과 같은 기술의 경우 스케일링 효율을 제한할 수 있다. 직렬화된 통신과 생산자 계산의 세밀한 중첩은 비용을 숨기는 데 도움이 될 수 있지만, GPU로 구현하는 것은 소프트웨어 복잡성과 컴퓨팅과 통신 간의 자원 경쟁으로 인해 어렵다. 이를 극복하기 위해, 우리는 T3를 제안하며, T3는 직렬화된 장치 간 통신을 생산자의 계산과 투명하고 효율적으로 융합하고 중첩한다. 생산자의 출력 주소 공간 매핑을 구성하고 하드웨어에서 프로그래밍 가능한 트랙 및 트리거 메커니즘을 사용하여 생산자의 저장소에서 통신을 조정한다. 이는 애플리케이션 영향을 감소시키고 또한 GPU 컴퓨팅 리소스에 대한 경쟁을 제거한다. T3는 추가적으로 메모리-대역폭 경쟁을 감소시키기 위해 근거리 메모리 컴퓨팅 및 메모리-컨트롤러 중재 정책을 사용한다. 전반적으로 T3는 최첨단 접근법에 비해 30% 지오메인(최대 47%)의 성능을 개선하고 22% 지오메인(최대 36%)의 데이터 이동을 줄인다. 또한 T3의 이점은 모델과 하드웨어 규모로 유지됩니다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 논문을 개선하는 데 도움이 된 셰퍼드, 새이드 말레키와 익명의 평론가들에게 감사의 인사를 전합니다. 우리는 또한 아셀심 시뮬레이터에 대한 그의 도움에 대해 마흐무드 케어리 압달라에게 감사하고 싶다. 이 작업은 빌라스 라이프 사이클 교수직 프로그램과 낙상 연구 경쟁 보조금, 그리고 ENS-1925485 보조금 아래 국립 과학 재단에 의해 위스콘신-매디슨 대학에서 부분적으로 지원되며, AMD, AMD 라이젠, AMD 라데온 및 이들의 조합은 첨단 마이크로 디바이스의 상표이다. 이 간행물에 사용된 다른 제품 이름은 식별 목적으로만 사용되며 해당 회사의 상표일 수 있습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*(1) AMD. 2018. AMD의 ROCm Communication Collectives Library. "[https://github.com/ROCmSoftwarePlatform/rccl/wiki](https://github.com/ROCmSoftwarePlatform/rccl/wiki)"\n' +
      '* (2)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|c|c|c|c|c|} \\hline\n' +
      '**접근/** & **GPU** & **투명** & **Overlap** & **Reduce** & **추가** & *No Topology**\n' +
      '**Features** & **Support** & **Communication** & **Communication** & **Contention** & **Accelerator** & **independent** \\\\ \\hline\n' +
      '**In-switch (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**ACE (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**CoCoNet (Cai et al., 2019)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**Google Decomposition (Srivastava et al., 2017)** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline\n' +
      '**T3-MCA** & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3. T3-MCA를 이전 작업과 비교한다.\n' +
      '\n' +
      '* [2] AMD. 2019. AMD\'s BLAS Library. "[https://github.com/ROCmSoftw](https://github.com/ROCmSoftw) arePlatform/rocBLAS".\n' +
      '* [3] AMD. 2020. AMD\'s tool for creating a benchmark-driven backend library for GEMMs. "[https://github.com/ROCmSoftwarePlatform/Te](https://github.com/ROCmSoftwarePlatform/Te) nsile/.\n' +
      '* [4] AMD. 2021. AMD HSA Code Object Format. "[https://rocmdocs.amd.c.om/en/latest/ROCm_Compiler_SDK/ROCm-Codeobj-format.html](https://rocmdocs.amd.c.om/en/latest/ROCm_Compiler_SDK/ROCm-Codeobj-format.html)".\n' +
      '* [5] AMD. 2022. AMD INSTINCT" MI210 ACCELERATOR. [https://www.amd.com/en/products/server-accelerators/amd-instinct-mi210](https://www.amd.com/en/products/server-accelerators/amd-instinct-mi210).\n' +
      '* [6] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)_. IEEE, IEEE Press, Piscataway, NJ, USA, 1-15.\n' +
      '* [7] Yuhui Bao, Yifan Sun, Zlatan Feric, Michael Tian Shen, Micah Weston, Jose L. Abellan, Trinayan Baruah, John Kim, Ajay Joshi, and David Kaeil. 2023. Navisim: A Highly Accurate GPU Simulator for AMD RDNA GPUs. In _Proceedings of the International Conference on Parallel Architectures and Compilation Techniques_ (Chicago, Illinois) (_PACT \'22). Association for Computing Machinery, New York, NY, USA, 333-345. [https://doi.org/10.1145/3559009.3569666](https://doi.org/10.1145/3559009.3569666)\n' +
      '* [8] Nathan Beniach and Ian Hogarth. 2022. State of AI Report 2022. [https://www.stateof.ai/](https://www.stateof.ai/).\n' +
      '* [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henghian, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In _Advances in Neural Information Processing Systems (NeurIPS, Vol. 33)_. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.). Curran Associates Inc., Red Hook, NY, USA, 1877-1901.\n' +
      '* [10] Zixian Cai, Zhengyang Liu, Saeed Maleki, Madanlus Musuvathi, Todd Mytkowicz, Jacob Nelson, and Olli Saarakivi. 2021. Synthesizing Optimal Collective Algorithms. In _Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOP)_. Association for Computing Machinery, New York, NY, USA, 62-75. [https://doi.org/10.1145/3437801.3441620](https://doi.org/10.1145/3437801.3441620)\n' +
      '* [11] Niladrish Chatterjee, Mike O\'Connor, Donghyuk Lee, Daniel R Johnson, Stephen W Keckler, Minsoo Rhu, and William J Dally. 2017. Architecting an Energy-Efficient DRAM System for GPUs. In _23rd IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE, Computer Society, Washington, DC, USA, 73-84.\n' +
      '* [12] Akankanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsyvashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselam Levskaya, Sanjay Ghemawat, Sunjong Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thammalamayan Sankaranavana Pillai, Marie Pellat, Aitor Lewkowycz, Eric Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. _arXiv preprint arXiv:2204.02311_ (2022), 87 pages.\n' +
      '* [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. FlashAttention: Fast and Memory-efficient Exact Attention with IO-Awareness. _Advances in Neural Information Processing Systems_ 35 (2022), 16344-16359.\n' +
      '* [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Morristown, NJ, USA, 4171-4186. [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)\n' +
      '* [15] Shraf Eassa and Sukru Bure Eryilmaz. 2022. The Full Stack Optimization Powering NVIDIA MLPerf Training v2.0 Performance. [https://developer.nvidia.com/blog/boosting-mlperf-training-performance-with-full-stack-optimization/](https://developer.nvidia.com/blog/boosting-mlperf-training-performance-with-full-stack-optimization/).\n' +
      '* [16] Izzat El Hajj, Juan Gomez-Luna, Cheng Li, Li-Wen Chang, Dejan Milojicic, and Wen-mei Hwu. 2016. KLAP: Kernel Launch Aggregation and Promotion for Optimizing Dynamic Parallelism. In _49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)_. IEEE, IEEE Press, Piscataway, NJ, USA, 1-12. [https://doi.org/10.1109/MICRO.2016.7783716](https://doi.org/10.1109/MICRO.2016.7783716)\n' +
      '* [17] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. _The Journal of Machine Learning Research_ 23, 1, Article 120 (jan 2022), 39 pages.\n' +
      '* [18] Jan Fousek, Jiri Filipovic, and Matus Madzin. 2011. Automatic Fusions of CUDA-GPU Kernels for Parallel Map. _SIGARCH Comput. Archit. News_ 39, 4 (Dec. 2011), 98-99. [https://doi.org/10.1145/2082156.2082183](https://doi.org/10.1145/2082156.2082183)\n' +
      '* [19] Amir Gholami. 2021. AI and Memory Wall.\n' +
      '* [20] Anthony Gutierrez, Bradford M. Beckmann, Alexandru Duttu, Joseph Gross, Michael LeBeane, John Kalamatianos, Onur Kayiran, Matthew Poremba, Brandon Potter, Sooraj Puthoor, Matthew D. Sinclair, Michael Wyse, Jieming Yin, Xianwei Zhang, Akshay Jain, and Timothy Rogers. 2018. Lost in Abstraction: Pitfalls of Analyzing GPUs at the Intermediate Language Level. In _24th IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, Los Alamitos, CA, USA, 608-619. [https://doi.org/10.1109/HPCA.2018.00058](https://doi.org/10.1109/HPCA.2018.00058)\n' +
      '* [21] Hany Hassan Awadalla, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, Will Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuanghi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018. Achieving Human Parity on Automatic Chinese to English News Translation. _arXiv preprint arXiv:1803.05567_ (March 2018), 25 pages. arXiv:1803.05567 [cs.CL]\n' +
      '* [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. _CoRR_ abs/1512.03385 (2015), 12 pages. arXiv:1512.03385 [http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385)\n' +
      '* [23] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhififeng Chen. 2019. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS, Vol. 32)_. Curran Associates Inc., Red Hook, NY, USA, Article 10, 10 pages.\n' +
      '* [24] Changho Hwang, KyoungSoo Park, Ran Shu, Xinyuan Qu, Peng Cheng, and Yongqiang Xiong. 2023. ARK: GPU-driven Code Execution for Distributed Deep Learning. In _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 87-101. [https://www.usenix.org/conference/nsd123/presentatio/n/hwang](https://www.usenix.org/conference/nsd123/presentatio/n/hwang)* Jangda et al. (2022) Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, and Olli Saarikiv. 2022. Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads. In _Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)_. Association for Computing Machinery, New York, NY, USA, 402-416. [https://doi.org/10.1145/3503222.3507778](https://doi.org/10.1145/3503222.3507778)\n' +
      '* Jeaugey(2022) Sylvain Jeaugey. 2022. 나무 축소가 어떻게 구현되는가? [https://github.com/NVIDIA/ncc/issues/545#issuconnment-1006361565] (https://github.com/NVIDIA/ncc/issues/545#issuconnment-1006361565).\n' +
      '* Jog et al. (2014) Adwait Jog, Evgeny Bolotin, Zvika Guz, Mike Parker, Stephen W Keckler, Mahmut T Kandemir, and Chitta R Das. 2014. 동시 GPGPU 어플리케이션의 공정하고 효율적인 실행을 위한 어플리케이션 인식 메모리 시스템. [Proceedings on General Purpose Processing on GPU(GPGPU)_]에서. Association for Computing Machinery, New York, NY, USA, 1-8. [https://doi.org/10.1145/2588768.2576780](https://doi.org/10.1145/2588768.2576780)\n' +
      '* Jog et al. (2016) Adwait Jog, Onur Kayin, Ashtshota Pathataki, Mahmut T Kandemir, Onur Mutlu, Ravishankar T kely, and Chitta R Das. 2016. 향상된 GPU 성능을 위해 핵심 중요도를 활용한다. In _Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science_. Association for Computing Machinery, New York, NY, USA, 351-363. [https://doi.org/10.1145/2896377.2901468](https://doi.org/10.1145/2896377.2901468)\n' +
      '* Jouppi et al. (2021) Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James Laudon, Sheng Li, Peter Ma, Xiaoyu Ma, Nishant Patil, Sushma Prasad, Clifford Young, Zongwei Zhou, and David Patterson. 2021. 구글의 TPUv4i를 형상화한 3세대의 10가지 교훈. [Proceedings of the 48th Annual International Symposium on Computer Architecture_(Virtual Event, Spain) _(ISCA)_]. IEEE Press, Piscataway, NJ, USA, 1-14. [https://doi.org/10.1109/ISCA52012.2021.200010](https://doi.org/10.1109/ISCA52012.2021.200010)\n' +
      '* Kerr et al. (2017) Andrew Kerr, Duane Merrill, Julien Demouth, and John Tran. 2017. cuTLASS: CUDA C++에서 빠른 선형 대수.\n' +
      '* Khairy et al. (2018) Mahmoud Khairy, Akshay Jain, Tor M. 아모트, 티모시 G. 로저스 2018. 정확한 모델링을 통한 현대 GPU 메모리 시스템 설계 과제 탐색 CoRR_ abs/1810.07269 (2018), 10 페이지. arXiv:1810.07269[http://arxiv.org/abs/1810.07269](http://arxiv.org/abs/1810.07269)\n' +
      '* Khairy et al. (2020) Mahmoud Khairy, Vadim Nikiforov, David Nellans, and Timothy G. Rogers. 2020. Localality-Centric Data and Threadblock Management for Massive GPU. _33rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)_. IEEE Computer Society, Los Alamitros, CA, USA, 1022-1036. [https://doi.org/10.1109/MICRO50266.2020.00086](https://doi.org/10.1109/MICRO50266.2020.00086)\n' +
      '* Khairy et al. (2020) Mahmoud Khairy, Zhesheng Shen, Tor M. 아모트, 티모시 G. 로저스 2020. Accel-Sim: 검증된 GPU 모델링을 위한 확장 가능한 시뮬레이션 프레임워크. IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_에서. IEEE Press, Piscataway, NJ, USA, 473-486. [https://doi.org/10.1109/ISCA54697.2020.00047](https://doi.org/10.1109/ISCA54697.2020.00047)\n' +
      '* Kim et al. (2021) Heesu Kim, Hannim Park, Taehyun Kim, Kwanheum cho, Eojin Lee, Sujung Ryu, Hyuk-Jae Lee, Kiyoung Choi, and Jinho Lee. 2021. Grad-PIM: Gradient Descent를 위한 실용적인 Processing-in-DRAM 아키텍처. In _27th IEEE International Symposium on High-Performance Computer Architecture (HPCA)_. IEEE Computer Society, Washington, DC, USA, 14페이지.\n' +
      '* Kim et al. (2021) Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. 2021. 멀티태스크 다국어 모델을 위한 확장 가능하고 효율적인 MoE Training. [https://doi.org/10.48550/MXRIV.210.10465] (https://doi.org/10.48550/MXRIV.210.10465)\n' +
      '* Klenk et al. (2020) Benjamin Klenk, Nan Jiang, Greg Thorson, and Larry Dennison. 2020. Accelerating Shared-Memory Multiprocessor Colletives를 위한 네트워크 내 구조. IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Computer Society, Washington, DC, USA, 996-1009.\n' +
      '* Volume 1_(Lake Tahoe, Nevada)_(NIPS\'12)_. Curran Associates Inc., USA, 1097-1105. [http://dl.acm.org/citation.cfm?id=2.999134.2999257](http://dl.acm.org/citation.cfm?id=2.999134.2999257)\n' +
      '* Lee et al. (2021) Sukhan Lee, Shin-haeng Kang, Jaehoon Lee, Heyousin Kim, Eojin Lee, Seungwoo Seo, Hosang Yoon, Seungwon Lee, Kyungghwan Lim, Hyunung Shin, Jinhyun Kim, O Seongil, Anand Iyer, David Wang, Kyomin Sohn, and Nam Sung Kim. 2021. 상용 D램 기술 기반의 PIM을 위한 하드웨어 아키텍처 및 소프트웨어 스택: 산업용 제품. IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE Press, Piscataway, NJ, USA, 43-56. [https://doi.org/10.1109/ISCA52012.2021.200013](https://doi.org/10.1109/ISCA52012.2021.200013)\n' +
      '* Lew et al. (2019) Jonathan Lew, Devi A Shah, Suchita Patil, Shaylin Cattell, Mengchi Zhang, Amruth Sandhupatla, Christopher Ng, Negar Goli, Matthew D Sinclair, Timothy G Rogers, and Tor Aamodt. 2019. 정밀 GPU 시뮬레이터를 이용한 기계 학습 작업량 분석. IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)_. IEEE, IEEE Computer Society, Washington, DC, USA, 151-152.\n' +
      '* Lin et al. (2014) Min Lin, Qiang Chen, and Shuicheng Yan. 2014. Network In Network. _2nd International Conference on Learning Representations (ICLR)_, Yoshua Bengio and Yann LeCun (Eds.) OpenReview.net, 10 pages. [http://arxiv.org/abs/1312.4400] (http://arxiv.org/abs/1312.4400)\n' +
      '* Lin et al. (2018) Shih-Chieh Lin, Yunqi Zhang, Chang-Hong Hsu, Matt Skach, M E. Hague, Lingia Tang, and Jason Mars. 2018. 자율주행의 건축적 함의: 제약과 가속. 프로그래밍 언어 및 운영 체제를 위한 건축 지원에 관한 제20차 국제 회의의 _Proceedings_(Williamsburg, VA, USA) _(ASPLOS)_. ACM, New York, NY, USA, 751-766. [https://doi.org/10.1145/3173162.3173191](https://doi.org/10.1145/3173162.3173191)\n' +
      '* Lustig and Martonosi (2013) Daniel Lustig and Margaret Martonosi. 2013. Fine-Grained CPU-GPU 동기화를 통한 GPU Offload Latency 감소 In _Proceedings of the 19th International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, USA, 354-365. [https://doi.org/10.1109/HPCA.2013.6522332](https://doi.org/10.1109/HPCA.2013.6522332)\n' +
      '* 마하잔 등(2023) Kshiteej Mahajan, Ching-Hsiang Chu, Srinivas Sridharan, and Aditya Akella. 2023. Better Together: SYNDICATE를 이용한 ML 집합 스케줄링 및 실행 계획 공동 최적화. _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 809-824. [https://www.usenix.org/conference/nsd32/presentation/mahajan](https://www.usenix.org/conference/nsd32/presentation/mahajan]\n' +
      '* Mattson et al. (2019) Peter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Mickiveitis, David A. Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debojyoti Dutta, Udit Gupta, Kim M. 하젤우드, 앤드류 호크, 신위안 황, 빌 지아, 다니엘 강, 데이비드 칸터, 나빈 쿠마르, 제프리 랴오, 구오카이 마, 데피크 나라야난, 타요 오건테비, 제라디 페키멘코, 릴리안 오텐코스트, 비제이 장가 레디, 테일러 로비, 톰 세인트 존, 캐롤 장 우, 링지 쉬, 클리프 영, 마테이 자하리아. 2019. MLPerf Training Benchmark. _ CoRR_ abs/1910.01500 (2019), 14 페이지. arXiv:1910.01500[http://arxiv.org/abs/1910.01500](http://arxiv.org/abs/1910.01500)\n' +
      '* Mickevicius et al. (2018) Paulius Mickevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Olekski Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. 혼합 정밀 훈련. arXiv:1710.03740[cs.AI][http://arxiv.org/abs/1710.03740](http://arxiv.org/abs/1710.03740)\n' +
      '* Mickevicius et al. (2022) Paulius Mickevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Hav, Alexander Heinecke, Patrick Judd, John Kamala, Naveen Mellempudi, Stuart Oberman, Mohammad Shneybi, Michael Su, and Hao Wu. 2022. 딥러닝을 위한 FPR 포맷들_ CoRR_ abs/2209.05433 (2022), 9 페이지. [https://doi.org/10.48550/ARRV.2209.05433] (https://doi.org/10.48550/ARRV.2209.05433) arXiv:2209.05433\n' +
      '* 마이크로소프트(2020) 마이크로소프트. 2020. Turing-NLG: A 170억 매개 언어 모델 by Microsoft. _ Microsoft Research Blog_ 1, 8(2020), 8 pages. [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/] (https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)*[48] MLPerf. 2018. MLPerf Benchmark Suite. [https://mlperf.org/] (https://mlperf.org/).\n' +
      '* [49] Diksha Moolchandani, Joyjit Kundu, Frederik Ruelens, Peter Vrancx, Timon Evenblij, and Manu Perumkunnil. 2023. AMFeD: An Analytical Model for Performance in Distributed Training of Transformers. In _IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)_. IEEE Computer Society, Los Alamitos, CA, USA, 306-315. [https://doi.org/10.1109/ISPASS57527.2023.00037](https://doi.org/10.1109/ISPASS57527.2023.00037)\n' +
      '* [50] Harini Muthukrishnan, Daniel Lustig, David Nellans, and Thomas Wenisch. 2021. GPS: A Global Publish-Subscribe Model for Multi-GPU Memory Management. In _MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture_ (Virtual Event, Greece) _(MICRO \'21)_. Association for Computing Machinery, New York, NY, USA, 46-58. [https://doi.org/10.1145/3466752.3480088](https://doi.org/10.1145/3466752.3480088)\n' +
      '* [51] Harini Muthukrishnan, David Nellans, Daniel Lustig, Jeffrey A Fessler, and Thomas F Wenisch. 2021. Efficient Multi-GPU Shared Memory via Automatic Optimization of Fine-grained Transfers. In _ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Computer Society, Washington, DC, USA, 139-152.\n' +
      '* [52] Lifeng Nai, Ramyad Hadidi, Jaewoong Sim, Hyojong Kim, Pranith Kumar, and Hyesoon Kim. 2017. GraphPIM: Enabling Instruction-level PIM Offloading in Graph Computing Frameworks. In _IEEE International Symposium on High Performance Computer Architecture (HPCA)_. IEEE Computer Society, Los Alamitos, CA, USA, 457-468. [https://doi.org/10.1109/HPCA.2017.54](https://doi.org/10.1109/HPCA.2017.54)\n' +
      '* [53] NVIDIA. 2017. NVIDIA DGX-1 With Tesla V100 System Architecture. [https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf](https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf).\n' +
      '* [54] NVIDIA. 2018. NVIDIA TESLA V100 GPU ACCELERATOR. [https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-dataset-letter-fnl-web.pdf](https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-dataset-letter-fnl-web.pdf).\n' +
      '* [55] NVIDIA. 2020. NVIDIA NCCL.\n' +
      '* [56] NVIDIA. 2021. NVIDIA A100 TESNR CORE GPU. [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-dataset-the-v104-wiki-1758950-r4-web.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-dataset-the-v104-wiki-1758950-r4-web.pdf).\n' +
      '* [57] NVIDIA. 2022. GPUDirect. [https://developer.nvidia.com/gpudirect/](https://developer.nvidia.com/gpudirect/).\n' +
      '* [58] NVIDIA. 2023. Efficient GEMM in CUDA. [https://github.com/NVIDIA/Acutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions](https://github.com/NVIDIA/Acutlass/blob/main/media/docs/efficient_gemm.md#parallelized-reductions).\n' +
      '* [59] NVIDIA. 2023. NVIDIA Announces DGX GH200 AI Supercomputer. [https://nvidia.com/news/nvidia-grace-hopper-superchi-ps-designed-for-accelerated-generative-ai-enter-full-production](https://nvidia.com/news/nvidia-grace-hopper-superchi-ps-designed-for-accelerated-generative-ai-enter-full-production).\n' +
      '* [60] NVIDIA. 2023. NVIDIA H100 TENSOR CORE GPU. [https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-database](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-database).\n' +
      '* [61] NVIDIA Corp. 2016. NVIDIA cuBLAS. [https://developer.nvidia.com/cublas](https://developer.nvidia.com/cublas). Accessed August 6, 2016.\n' +
      '* [62] Prateynis Patel, Eisha Choukes, Chaojie Zhang, Inigo Goiri, Aashaka Shah, Saeed Maleki, and Ricardo Bianchini. 2023. Splitwise: Efficient Generative LLM Inference Using Phase Splitting. _arXiv preprint arXiv:2311.18677_ (2023), 12 pages. arXiv:2311.18677 [cs.AR]\n' +
      '* [63] Suchita Pati, Shaizean Aga, Nuwan Jayasena, and Matthew D. Sinclair. 2022. Demystifying BERT: System Design Implications. In _2022 IEEE International Symposium on Workload Characterization (IISWC)_. IEEE Computer Society, Los Alamitos, CA, USA, 296-309. [https://doi.org/10.1109/IISWC55918.2022.00033](https://doi.org/10.1109/IISWC55918.2022.00033)\n' +
      '* [64] Suchita Pati, Shaizean Aga, Nuwan Jayasena, and Matthew D. Sinclair. 2023. Tale of Two Cs: Computation vs. Communication Scaling for Future Transformers on Future Hardware. In _IEEE International Symposium on Workload Characterization (IISWC)_. IEEE Computer Society, Los Alamitos, CA, USA, 140-153. [https://doi.org/10.1109/IISWC59245.2023.00026](https://doi.org/10.1109/IISWC59245.2023.00026)\n' +
      '* [65] Ashutosh Pattaik, Xulong Tang, Onur Kayiran, Adwait Jog, Asit Mishra, Mahmut T Kandemir, Anand Sivasubramaniam, and Chita R Das. 2019. Opportunistic Computing in GPU Architectures. In _Proceedings of the 46th International Symposium on Computer Architecture (ISCA)_. Association for Computing Machinery, New York, NY, USA, 210-223. [https://doi.org/10.1145/3307650.332212](https://doi.org/10.1145/3307650.332212)\n' +
      '* [66] J Thomas Pawlowski. 2011. Hybrid Memory Cube (HMC). In _2011 IEEE Hot Chips 23 Symposium (HotChips)_. IEEE, IEEE, Piscataway, NJ, USA, 1-24.\n' +
      '* [67] Kishore Punniyamurthy, Bradford M Beckmann, and Khaled Hamidouche. 2023. GPU-initiated Fine-grained Overlap of Collective Communication with Computation. _arXiv preprint arXiv:2305.06942_ (2023), 13 pages. arXiv:2305.06942 [cs.DC]\n' +
      '* [68] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. _OpenAI blog_ 1, 8 (2019), 9.\n' +
      '* [69] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. DeepSpeed-MOE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale. In _International Conference on Machine Learning (ICML)_. PMLR, PMLR, 18332-18346.\n' +
      '* [70] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ECo-Informity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, St. Louis, Missouri) _(SC \'21)_. Association for Computing Machinery, New York, NY, USA, Article 59, 14 pages. [https://doi.org/10.1145/3458817.3476205](https://doi.org/10.1145/3458817.3476205)\n' +
      '* [71] Saeed Rashidi, Matthew Denton, Srinivas Sridharan, Sudarshan Srinivasan, Amoghavarsha Suresh, Jade Nie, and Tushar Krishna. 2021. Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms. In _2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)_. IEEE, IEEE Press, Piscataway, NJ, USA, 540-553. [https://doi.org/10.1109/ISCA52012.2021.00049](https://doi.org/10.1109/ISCA52012.2021.00049)\n' +
      '* [72] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmulling, C. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idoumi, T. B. Jabin, J. Jiao, T. S. John, F. Kanwar, D. Lee, J. Liao, A. Lokhmborov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, and Y. Zhou. 2020. MLPerf Inference Benchmark. In _ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)_. IEEE Press, Washington, DC, USA, 446-459. [https://doi.org/10.1109/ISCA45697.2020.00045](https://doi.org/10.1109/ISCA45697.2020.00045)\n' +
      '* [73] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahwar Bordon, and Nando de Freitas. 2022. A Generalist Agent. _Transactions on Machine Learning Research_ 2022 (2022), 42 pages. [https://openreview.net/forum?id=1ik0k4kj/jv](https://openreview.net/forum?id=1ik0k4kj/jv)\n' +
      '* [74] Kyle Roarty and Matthew D. Sinclair. 2020. Modeling Modern GPU Applications in gem5. In _3rd gem5 Users\' Workshop._ 2 pages.\n' +
      '* [75] Bita Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, Alessandro Forin, Haishan Zhu, Taesik Na, Prerak Patel, Shuai Che, Lok Chand Koppaka, Xia Song, Subhojit Som, Kaustav Das, Saurabh Tiwary, Steve Reinhardt, Sitaram Lanka, Eric Chung, and Doug Burger. 2020. Pushing the Limits of Narrow Precision Inference at Cloud Scale with Microsoft Floating Point. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_ (Vancouver, BC, Canada) _(NeurIPS\'20)_. Curran Associates Inc., Red Hook, NY, USA, Article 861, 11 pages.\n' +
      '* [76] Aarush Selvan and Pankaj Kanwar. 2022. Google showcases Cloud TPU v4 Pods for large model training. [https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-wt-pods-for-large-model-training](https://cloud.google.com/blog/topics/tpus/google-showcases-cloud-tpu-wt-pods-for-large-model-training).\n' +
      '\n' +
      '*샤 등(2023) Aashaka Shah, Vijay Chidambaram, Meghan Cowan, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Jacob Nelson, Olli Saarikivi, and Rachee Singh. 2023. TACCL: 통신 스케치를 이용한 집합 알고리즘 합성 안내 _20th USENIX Symposium on Networked Systems Design and Implementation (NSDI)_. USENIX Association, Boston, MA, 593-612. [https://www.usenix.org/conference/ns/d23/presentation/shah](https://www.usenix.org/conference/ns/d23/presentation/shah]\n' +
      '* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Model Parallelism을 이용한 Multi-Billion Parameter 언어 모델 학습. CoRRabs/1909.08053 (2019), 9페이지. arXiv:1909.08053[cs.CL]. [http://arxiv.org/abs/1909.08053] (http://arxiv.org/abs/1909.08053)\n' +
      '* Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. 대규모 이미지 인식을 위한 매우 깊은 컨볼루션 네트워크. _3rd International Conference on Learning Representations (ICLR)_, Yoshua Bengio and Yann LeCun (Eds.) OpenReview.net, 14 pages. [http://arxiv.org/abs/1409.1556] (http://arxiv.org/abs/1409.1556)\n' +
      '* Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. DeepSpeed and Megatron to Train Megatron-Turing NLG 530b, large-scale Generative Language Model. _ arXiv preprint arXiv:2201.11990_(2022), 44 페이지. arXiv:2201.11990 [cs.CL]\n' +
      '* Springer et al. (2017) Matthias Springer, Peter Wauligmann, and Hidehiko Masuhara. 2017. Modular Array-Based GPU Computing in a Dynamically-Typed Language. [Proceedings of the 4th ACM SIGPLAN International Workshop on Library, Languages and Compilers for Array Programming_(바르셀로나, Spain) _(ARRAY 2017)_. Association for Computing Machinery, New York, NY, USA, 48-55. [https://doi.org/10.1145/3091966.3091](https://doi.org/10.1145/3091966.3091)\n' +
      '* Szegedy et al. (2015) Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015년, \'컨볼루션\'으로 더 깊어지다 In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE Press, Piscataway, NJ, USA, 1-9.\n' +
      '* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Inception Architecture for Computer Vision. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE Press, Piscataway, NJ, USA, 2818-2826. [https://doi.org/10.1109/CVPR.2016.308](https://doi.org/10.1109/CVPR.2016.308)\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 2017. 주의력만 있으면 됩니다. In _Proceedings of the 31th International Conference on Neural Information Processing Systems_(Long Beach, California, USA) _(NeurIPS)_. Current Associates, Inc., Red Hook, NY, USA, 6000-6010. [https://proceedings.neurips.cc/paper/2017/hash/35/ee243547dee91fbd053c1c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/35/ee243547dee91fbd053c1c4a845aa-Abstract.html]\n' +
      '* Wang et al. (2010) Guibin Wang, YiSong Lin, and Wei Yi. 2010. 커널 퓨전: 멀티스레드 GPU에서 더 나은 전력 효율을 위한 효과적인 방법. In _Proceedings of the 2010 IEEE/ACM Int\'1 Conference on Green Computing and Communications & Int\'1 Conference on Cyber, Physical and Social Computing (GREECOM-CFSCOM\'10)_. IEEE Computer Society, USA, 344-350. [https://doi.org/10.1109/GreenCom-CPSCom.2010.102](https://doi.org/10.1109/GreenCom-CPSCom.2010.102)\n' +
      '* Wang et al. (2022) Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivas Murthy, Marcello Maggioni, Qiao Zhang, Sameer Kumar, Tongfiei Guo, Yuanzhong Xu, 및 Zongwei Zhou. 2022. 대규모 딥러닝 모델에서 분해를 통한 종속 연산과의 중첩 통신. 제28회 ACM 국제 프로그래밍 언어 및 운영 체제를 위한 건축 지원에 관한 회의 [Proceedings of the 28th ACM International Conference for Programming Languages and Operating Systems, Volume 1(ASPLOS)]에서. Association for Computing Machinery, New York, NY, USA, 93-106. [https://doi.org/10.1145/356795.3567959](https://doi.org/10.1145/356795.3567959)\n' +
      '*Xiong et al.(2017) Wayne Xiong, Xuedong Huang, Frank Seide, and Andreas Stolcke. 2017. Toward Human Parity in Conversational Speech Recognition IEEE/ACM Transactions on Audio, Speech, and Language Processing_25(Sept 2017), 2410-2423.\n' +
      '\n' +
      '2023년 8월 10일 접수; 2024년 1월 3일 접수; 2024년 1월 8일 접수\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>