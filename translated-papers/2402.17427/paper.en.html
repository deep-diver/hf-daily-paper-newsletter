<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction\n' +
      '\n' +
      'Jiaqi Lin\\({}^{1*}\\)   Zhihao Li\\({}^{2*}\\)   Xiao Tang\\({}^{2}\\)   Jianzhuang Liu\\({}^{3}\\)   Shiyong Liu\\({}^{2}\\)   Jiayue Liu\\({}^{1}\\)\n' +
      '\n' +
      'Yangdi Lu\\({}^{2}\\)   Xiaofei Wu\\({}^{2}\\)   Songcen Xu\\({}^{2}\\)   Youliang Yan\\({}^{2}\\)   Wenming Yang\\({}^{1\\dagger}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Tsinghua University  \\({}^{2}\\)Huawei Noah\'s Ark Lab  \\({}^{3}\\)Chinese Academy of Sciences\n' +
      '\n' +
      '\\({}^{*}\\) Equal contribution  \\({}^{\\dagger}\\) Corresponding author\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Existing NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splitting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. Project page: [https://vastgaussian.github.io](https://vastgaussian.github.io).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large scene reconstruction is essential for many applications, including autonomous driving [22, 33, 54], aerial surveying [6, 13], and virtual reality, which require photo-realistic visual quality and real-time rendering. Some approaches [41, 44, 52, 53, 61] are introduced to extend neural radiance fields (NeRF) [31] to large-scale scenes, but they still lack details or render slowly. Recently, 3D Gaussian Splatting (3DGS) [21] emerges as a promising approach with impressive performance in visual quality and rendering speed, enabling photo-realistic and real-time rendering at 1080p resolution. It is also applied to dynamic scene reconstruction [28, 51, 55, 56] and 3D content generation [12, 42, 59]. However, these methods focus on small-scale and object-centric scenes. When applied to large-scale environments, there are several scalability issues. _First_, the number of 3D Gaussians is limited by a given video memory, while the rich details of a large scene require numerous 3D Gaussians. Naively applying 3DGS to a large-scale scene would result in either low-quality reconstruction or out-of-memory errors. For intuitive explanation, a \\(32\\) GB GPU can be used to optimize about \\(11\\) million 3D Gaussians, while the small _Garden_ scene in the Mip-NeRF 360dataset [3] with an area of less than \\(100m^{2}\\) already requires approximately \\(5.8\\) million 3D Gaussians for a high-fidelity reconstruction. _Second_, it requires sufficient iterations to optimize an entire large scene as a whole, which could be time-consuming, and unstable without good regularizations. _Third_, the illumination is usually uneven in a large scene, and there are noticeable appearance variations in the captured images, as shown in Fig. 2(a). 3DGS tends to produce large 3D Gaussians with low opacities to compensate for these disparities across different views. For example, bright blobs tend to come up close to the cameras with images of high exposure, and dark blobs are associated with images of low exposure. These blobs turn to be unpleasant floaters in the air when observed from novel views, as shown in Fig. 2(b, d).\n' +
      '\n' +
      'To address these issues, we propose Vast 3D Gaussians (VastGaussian) for large scene reconstruction based on 3D Gaussian Splatting. We reconstruct a large scene in a divide-and-conquer manner: Partition a large scene into multiple cells, optimize each cell independently, and finally merge them into a full scene. It is easier to optimize these cells due to their finer spatial scale and smaller data size. A natural and naive partitioning strategy is to distribute training data geographically based on their positions. This may cause boundary artifacts between two adjacent cells due to few common cameras, and can produce floaters in the air without sufficient supervision. Thus, we propose visibility-based data selection to incorporate more training cameras and point clouds progressively, which ensures seamless merging and eliminates floaters in the air. Our approach allows better flexibility and scalability than 3DGS. Each of these cells contains a smaller number of 3D Gaussians, which reduces the memory requirement and optimization time, especially when optimized in parallel with multiple GPUs. The total number of 3D Gaussians contained in the merged scene can greatly exceed that of the scene trained as a whole, improving the reconstruction quality. Besides, we can expand the scene by incorporating new cells or fine-tune a specific region without retraining the entire large scene.\n' +
      '\n' +
      'To reduce the floaters caused by appearance variations, Generative Latent Optimization (GLO) [5] with appearance embeddings [29] is proposed for NeRF-based methods [41, 61]. This approach samples points through ray-marching, and the point features are fed into an MLP along with appearance embeddings to obtain the final colors. The rendering process is the same as the optimization, which still requires appearance embeddings as input. It is not suitable for 3DGS as its rendering is performed by frame-wise rasterization without MLPs. Therefore, we propose a novel decoupled appearance modeling that is applied only in the optimization. We attach an appearance embedding to the rendered image pixel-by-pixel, and feed them into a CNN to obtain a transformation map for applying appearance adjustment on the rendered image. We penalize the structure dissimilarities between the rendered image and its ground truth to learn constant information, while the photometric loss is calculated on the adjusted image to fit the appearance variations in the training image. Only the consistent rendering is what we need, so this appearance modeling module can be discarded after optimization, thus not slowing down the real-time rendering speed.\n' +
      '\n' +
      'Experiments on several large scene benchmarks confirm the superiority of our method over NeRF-based methods. Our contributions are summarized as follows:\n' +
      '\n' +
      '* We present VastGaussian, the first method for high-fidelity reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting.\n' +
      '* We propose a progressive data partitioning strategy that assigns training views and point clouds to different cells, enabling parallel optimization and seamless merging.\n' +
      '* We introduce decoupled appearance modeling into the optimization process, which suppresses floaters due to appearance variations. This module can be discarded after optimization to obtain the real-time rendering speed.\n' +
      '\n' +
      'Figure 2: (a) Appearance may vary in adjacent training views. (b) Dark or bright blobs may be created near cameras with training images of different brightnesses. (c) 3D Gaussian Splatting uses these blobs to fit the appearance variations, making the renderings similar to the training images in (a). (d) These blobs appear as floaters in novel views. (e) Our decoupled appearance modeling enables the model to learn constant colors, so the rendered images are more consistent in appearance across different views. (f) Our approach greatly reduces floaters in novel views.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Large Scene Reconstruction\n' +
      '\n' +
      'There is significant progress in image-based large scene reconstruction over the past decades. Some works [1, 16, 23, 34, 38, 39, 62] follow a structure-from-motion (SfM) pipeline to estimate camera poses and a sparse point cloud. The following works [17, 19] produce a dense point cloud or triangle mesh from the SfM output based on multi-view stereo (MVS). As NeRF [31] becomes a popular 3D representation for photo-realistic novel-view synthesis in recent years [35], many variants are proposed to improve quality [2, 45, 47, 48, 49, 57, 24, 44, 45], increase speed [8, 9, 11, 14, 20, 32, 36, 37, 40, 43, 46, 58, 60], extend to dynamic scenes [7, 15, 18, 25, 27, 50], and so on. Some methods [41, 44, 52, 53, 61] scale it to large scenes. Block-NeRF [41] divides a city into multiple blocks and distributes training views according to their positions. Mega-NeRF [44] uses grid-based division and assigns each pixel in an image to different grids through which its ray passes. Unlike these heuristics partitioning strategies, Switch-NeRF [61] introduces a mixture-of-NeRF-experts framework to learn scene decomposition. Grid-NeRF [53] does not perform scene decomposition, but rather uses an integration of NeRF-based and grid-based methods. While the rendering quality of these methods is significantly improved over traditional ones, they still lack details and render slowly. Recently, 3D Gaussian Splatting [21] introduces an expressive explicit 3D representation with high-quality and real-time rendering at 1080p resolution. However, it is non-trivial to scale it up to large scenes. Our VastGaussian is the first one to do so with novel designs for scene partitioning, optimizing, and merging.\n' +
      '\n' +
      '### Varying Appearance Modeling\n' +
      '\n' +
      'Appearance variation is a common problem in image-based reconstruction under changing lighting or different camera setting such as auto-exposure, auto-white-balance and tone-mapping. NRW [30] trains an appearance encoder in a data-driven manner with a contrastive loss, which takes a deferred-shading deep buffer as input and produces an appearance embedding (AE). NeRF-W [29] attaches AEs to point-based features in ray-marching, and feeds them into an MLP to obtain the final colors, which becomes a standard practice in many NeRF-based methods [41, 44, 61]. Ha-NeRF [10] makes AE a global representation across different views, and learns it with a view-consistent loss. In our VastGaussian, we concatenate AEs with rendered images, feed them into a CNN to obtain transformation maps, and use the transformation maps to adjust the rendered images to fit the appearance variations.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      'In this paper, we propose VastGaussian for large scene reconstruction and rendering based on 3D Gaussian Splatting (3DGS) [21]. 3DGS represents the geometry and appearance via a set of 3D Gaussians \\(\\mathbf{G}\\). Each 3D Gaussian is characterized by its position, anisotropic covariance, opacity, and spherical harmonic coefficients for view-dependent colors. During the rendering process, each 3D Gaussian is projected to the image space as a 2D Gaussian. The projected 2D Gaussians are assigned to different tiles, sorted and alpha-blended into a rendered image in a point-based volume rendering manner [63].\n' +
      '\n' +
      'The dataset used to optimize a scene contains a sparse point cloud \\(\\mathbf{P}\\) and training views \\(\\mathbf{V}=\\{(\\mathcal{C}_{i},\\mathcal{I}_{i})\\}\\), where \\(\\mathcal{C}_{i}\\) is the \\(i\\)-th camera, and \\(\\mathcal{I}_{i}\\) is the corresponding image. \\(\\mathbf{P}\\) and \\(\\{\\mathcal{C}_{i}\\}\\) are estimated by Structure-from-Motion (SfM) from \\(\\{\\mathcal{I}_{i}\\}\\). \\(\\mathbf{P}\\) is used to initialize 3D Gaussians, and \\(\\mathbf{V}\\) is used for differentiable rendering and gradient-based optimization of 3D Gaussians. For camera \\(\\mathcal{C}_{i}\\), the rendered image \\(\\mathcal{I}_{i}^{r}=\\mathcal{R}(\\mathbf{G},\\mathcal{C}_{i})\\) is obtained by a differentiable rasterizer \\(\\mathcal{R}\\). The properties of 3D Gaussians are optimized with respect to the loss function between \\(\\mathcal{I}_{i}^{r}\\) and \\(\\mathcal{I}_{i}\\) as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}=(1-\\lambda)\\mathcal{L}_{1}(\\mathcal{I}_{i}^{r},\\mathcal{I}_{i})+ \\lambda\\mathcal{L}_{\\text{D-SSIM}}(\\mathcal{I}_{i}^{r},\\mathcal{I}_{i}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\lambda\\) is a hyper-parameter, and \\(\\mathcal{L}_{\\text{D-SSIM}}\\) denotes the D-SSIM loss [21]. This process is interleaved with adaptive point densification, which is triggered when the cumulative gradient of the point reaches a certain threshold.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '3DGS [21] works well on small and object-centric scenes, but it struggles when scaled up to large environments due to video memory limitation, long optimization time, and appearance variations. In this paper, we extend 3DGS to large scenes for real-time and high-quality rendering. We propose to partition a large scene into multiple cells that are merged after individual optimization. In Sec. 4.1, we introduce a progressive data partitioning strategy with airspace-aware visibility calculation. Sec. 4.2 elaborates how to optimize individual cells, presenting our decoupled appearance modeling to capture appearance variations in images. Finally, we describe how to merge these cells in Sec. 4.3.\n' +
      '\n' +
      '### Progressive Data Partitioning\n' +
      '\n' +
      'We partition a large scene into multiple cells and assign parts of the point cloud \\(\\mathbf{P}\\) and views \\(\\mathbf{V}\\) to these cells for optimization. Each of these cells contains a smaller number of 3D Gaussians, which is more suitable for optimization with lower memory capacity, and requires less training time when optimized in parallel. The pipeline of our progressive data partitioning strategy is shown in Fig. 3.\n' +
      '\n' +
      '**Camera-position-based region division.** As shown in Fig. 3(a), we partition the scene based on the projected camera positions on the ground plane, and make each cell contain a similar number of training views to ensure balanced optimization between different cells under the same number of iterations. Without loss of generality, assuming that a grid of \\(m\\times n\\) cells fits the scene in question well, we first partition the ground plane into \\(m\\) sections along one axis, each containing approximately \\(|\\mathbf{V}|/m\\) views. Then each of these sections is further subdivided into \\(n\\) segments along the other axis, each containing approximately \\(|\\mathbf{V}|/(m\\times n)\\) views. Although here we take grid-based division as an example, our data partitioning strategy is also applicable to other geography-based division methods, such as sectorization and quadtrees.\n' +
      '\n' +
      '**Position-based data selection.** As illustrated in Fig. 3(b), we assign part of the training views \\(\\mathbf{V}\\) and point cloud \\(\\mathbf{P}\\) to each cell after expanding its boundaries. Specifically, let the \\(j\\)-th region be bounded in a \\(\\ell_{j}^{h}\\times\\ell_{j}^{w}\\) rectangle; the original boundaries are expanded by a certain percentage, \\(20\\%\\) in this paper, resulting in a larger rectangle of size \\((\\ell_{j}^{h}+0.2\\ell_{j}^{h})\\times(\\ell_{j}^{w}+0.2\\ell_{j}^{w})\\). We partition the training views \\(\\mathbf{V}\\) into \\(\\{\\mathbf{V}_{j}\\}_{j=1}^{m\\times n}\\) based on the expanded boundaries, and segment the point cloud \\(\\mathbf{P}\\) into \\(\\{\\mathbf{P}_{j}\\}\\) in the same way.\n' +
      '\n' +
      '**Visibility-based camera selection.** We find that the selected cameras in the previous step are insufficient for high-fidelity reconstruction, which can lead to poor detail or floater artifact. To solve this problem, we propose to add more relevant cameras based on a visibility criterion, as shown in Fig. 3(c). Given a yet-to-be-selected camera \\(\\mathcal{C}_{i}\\), let \\(\\Omega_{ij}\\) be the projected area of the \\(j\\)-th cell in the image \\(\\mathcal{I}_{i}\\), and let \\(\\Omega_{i}\\) be the area of \\(\\mathcal{I}_{i}\\); visibility is defined as \\(\\Omega_{ij}/\\Omega_{i}\\). Those cameras with a visibility value greater than a predefined threshold \\(T_{h}\\) are selected.\n' +
      '\n' +
      'Note that different ways of calculating \\(\\Omega_{ij}\\) result in different camera selections. As illustrated in Fig. 3(e), a natural and naive solution is based on the 3D points distributed on the object surface. They are projected on \\(\\mathcal{I}_{i}\\) to form a convex hull of area \\(\\Omega_{ij}^{\\text{surf}}\\). This calculation is airspace-agnostic because it takes only the surface into account. Therefore some relevant cameras are not selected due to its low visibility on the \\(j\\)-th cell in this calculation, which results in under-supervision for airspace, and cannot suppress floaters in the air.\n' +
      '\n' +
      'We introduce an airspace-aware visibility calculation, as shown in Fig. 3(f). Specifically, an axis-aligned bounding box is formed by the point cloud in the \\(j\\)-th cell, whose height is chosen as the distance between the highest point\n' +
      '\n' +
      'Figure 3: **Progressive data partitioning.****Top row**: (a) The whole scene is divided into multiple regions based on the 2D camera positions projected on the ground plane. (b) Parts of the training cameras and point cloud are assigned to a specific region according to its expanded boundaries. (c) More training cameras are selected to reduce floaters, based on an airspace-aware visibility criterion, where a camera is selected if it has sufficient visibility on this region. (d) More points of the point cloud are incorporated for better initialization of 3D Gaussians, if they are observed by the selected cameras. **Bottom row**: Two visibility definitions to select more training cameras. (e) A naive way: The visibility of the \\(i\\)-th camera on the \\(j\\)-th cell is defined as \\(\\Omega_{ij}^{\\text{surf}}/\\Omega_{i}\\), where \\(\\Omega_{i}\\) is the area of the image \\(\\mathcal{I}_{i}\\), and \\(\\Omega_{ij}^{\\text{surf}}\\) is the convex hull area formed by the surface points in the \\(j\\)-th cell that are projected to \\(\\mathcal{I}_{i}\\). (f) Our airspace-aware solution: The convex hull area \\(\\Omega_{ij}^{\\text{surf}}\\) is calculated on the projection of the \\(j\\)-th cellâ€™s bounding box in \\(\\mathcal{I}_{i}\\). (g) Floaters caused by depth ambiguity with improper point initialization, which cannot be eliminated without sufficient supervision from training cameras.\n' +
      '\n' +
      'and the ground plane. We project the bounding box onto \\(\\mathcal{I}_{i}\\) and obtain a convex hull area \\(\\Omega_{ij}^{\\text{air}}\\). This airspace-aware solution takes into account all the visible space, which ensures that given a proper visibility threshold, the views with significant contributions to the optimization of this cell are selected and provide enough supervision for the airspace.\n' +
      '\n' +
      '**Coverage-based point selection.** After adding more relevant cameras to the \\(j\\)-th cell\'s camera set \\(\\mathbf{V}_{j}\\), we add the points covered by all the views in \\(\\mathbf{V}_{j}\\) into \\(\\mathbf{P}_{j}\\), as illustrated in Fig. 3(d). The newly selected points can provide better initialization for the optimization of this cell. As illustrated in Fig. 3(g), some objects outside the \\(j\\)-th cell can be captured by some views in \\(\\mathbf{V}_{j}\\), and new 3D Gaussians are generated in wrong positions to fit these objects due to depth ambiguity without proper initialization. However, by adding these object points for initialization, new 3D Gaussians in correct positions can be easily created to fit these training views, instead of producing floaters in the \\(j\\)-th cell. Note that the 3D Gaussians generated outside the cell are removed after the optimization of the cell.\n' +
      '\n' +
      '### Decoupled Appearance Modeling\n' +
      '\n' +
      'There are obvious appearance variations in the images taken in uneven illumination, and 3DGS tends to produce floaters to compensate for these variations across different views, as shown in Fig. 2(a-d).\n' +
      '\n' +
      'To address this problem, some NeRF-based methods [29, 41, 44, 61] concatenate an appearance embedding to point-based features in pixel-wise ray-marching, and feed them into the radiance MLP to obtain the final color. This is not suitable for 3DGS, whose rendering is performed by frame-wise rasterization without MLPs. Instead, we introduce decoupled appearance modeling into the optimization process, which produces a transformation map to adjust the rendered image to fit the appearance variations in the training image, as shown in Fig. 4. Specifically, we first downsample the rendered image \\(\\mathcal{I}_{i}^{r}\\) to not only prevent the transformation map from learning high-frequency details, but also reduce computation burden and memory consumption. We then concatenate an appearance embedding \\(\\ell_{i}\\) of length \\(m\\) to every pixel in the three-channel downsampled image, and obtain a 2D map \\(\\mathcal{D}_{i}\\) with \\(3+m\\) channels. \\(\\mathcal{D}_{i}\\) is fed into a convolutional neural network (CNN), which progressively upsamples \\(\\mathcal{D}_{i}\\) to generate \\(\\mathcal{M}_{i}\\) that is of the same resolution as \\(\\mathcal{I}_{i}^{r}\\). Finally, the appearance-variant image \\(\\mathcal{I}_{i}^{a}\\) is obtained by performing a pixel-wise transformation \\(T\\) on \\(\\mathcal{I}_{i}^{r}\\) with \\(\\mathcal{M}_{i}\\):\n' +
      '\n' +
      '\\[\\mathcal{I}_{i}^{a}=T(\\mathcal{I}_{i}^{r};\\mathcal{M}_{i}). \\tag{2}\\]\n' +
      '\n' +
      'In our experiments, a simple pixel-wise multiplication works well on the datasets we use. The appearance embeddings and CNN are optimized along with the 3D Gaussians, using the loss function modified from Eq. (1):\n' +
      '\n' +
      '\\[\\mathcal{L}=(1-\\lambda)\\mathcal{L}_{1}(\\mathcal{I}_{i}^{a},\\mathcal{I}_{i})+ \\lambda\\mathcal{L}_{\\text{D-SSIM}}(\\mathcal{I}_{i}^{r},\\mathcal{I}_{i}). \\tag{3}\\]\n' +
      '\n' +
      'Since \\(\\mathcal{L}_{\\text{D-SSIM}}\\) mainly penalizes the structural dissimilarity, applying it between \\(\\mathcal{I}_{i}^{r}\\) and the ground truth \\(\\mathcal{I}_{i}\\) makes the structure information in \\(\\mathcal{I}_{i}^{r}\\) close to \\(\\mathcal{I}_{i}\\), leaving the appearance information to be learned by \\(\\ell_{i}\\) and the CNN. The loss \\(\\mathcal{L}_{1}\\) is applied between the appearance-variant rendering \\(\\mathcal{I}_{i}^{a}\\) and \\(\\mathcal{I}_{i}\\), which is used to fit the ground truth image \\(\\mathcal{I}_{i}\\) that may have appearance variations from other images. After training, \\(\\mathcal{I}_{i}^{r}\\) is expected to have a consistent appearance with other images, from which the 3D Gaussians can learn an average appearance and correct geometry of all the input views. This appearance modeling can be discarded after optimization, without slowing down the real-time rendering speed.\n' +
      '\n' +
      '### Seamless Merging\n' +
      '\n' +
      'After optimizing all the cells independently, we need to merge them to get a complete scene. For each optimized cell, we delete the 3D Gaussians that are outside the original region (Fig. 3(a)) before boundary expansion. Otherwise, they could become floaters in other cells. We then merge the 3D Gaussians of these non-overlapping cells. The merged scene is seamless in appearance and geometry without obvious border artifacts, because some training views are common between adjacent cells in our data partitioning. Therefore, there is no need to perform further appearance adjustment like Block-NeRF [41]. The total number of 3D Gaussians contained in the merged scene can greatly exceed that of the scene trained as a whole, thus improving the reconstruction quality.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '**Efficiency and memory.** In Tab. 2, we report the training time, video memory consumption during optimization, and rendering speed. Mega-NeRF, Switch-NeRF and Vast-Gaussian are trained on 8 Tesla V100 GPUs, while Grid-NeRF and Modified 3DGS on a single V100 GPU as they do not perform scene decomposition. The rendering speeds are tested on a single RTX 3090 GPU. Our VastGaussian requires much shorter time to reconstruct a scene with photo-realistic rendering. Compared to Modified 3DGS, Vast-Gaussian greatly reduces video memory consumption on a single GPU. Since VastGaussian has more 3D Gaussians in the merged scene than Modified 3DGS, its rendering speed is slightly slower than Modified 3DGS, but is still much faster than the NeRF-based methods, achieving real-time rendering at 1080p resolution.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'We perform ablation study on the _Sci-Art_ scene to evaluate different aspects of VastGaussian.\n' +
      '\n' +
      '**Data partition.** As shown in Fig. 6 and Tab. 3, both visibility-based camera selection (VisCam) and coverage-based point selection (CovPoint) can improve visual quality. Without each or both of them, floaters can be created in the airspace of a cell to fit the views observing regions out\n' +
      '\n' +
      'Figure 5: Qualitative comparison between VastGaussian and previous work. Floaters are pointed out by green arrows.\n' +
      '\n' +
      'side the cell. As shown in Fig. 7, the visibility-based camera selection can ensure more common cameras between adjacent cells, which eliminates the noticeable boundary artifact of appearance jumping when it is not implemented.\n' +
      '\n' +
      '**Airspace-aware visibility calculation.** As illustrated in the \\(4\\)th row of Tab. 3 and Fig. 8, the cameras selected based on the airspace-aware visibility calculation provide more supervision for the optimization of a cell, thus not producing floaters that are presented when the visibility is calculated in the airspace-agnostic way.\n' +
      '\n' +
      '**Decoupled appearance modeling.** As shown in Fig. 2 and the \\(5\\)th row of Tab. 3, our decoupled appearance modeling reduces the appearance variations in the rendered images. Therefore, the 3D Gaussians can learn consistent geometry and colors from training images with appearance variations, instead of creating floaters to compensate for these variations. Please also see the videos in the supplement.\n' +
      '\n' +
      '**Different number of cells**. As shown in Tab. 4, more cells reconstruct better details in VastGaussian, leading to better SSIM and LPIPS values, and shorter training time when the cells are optimized in parallel. However, when the cell number reaches \\(16\\) or bigger, the quality improvement becomes marginal, and PSNR slightly decreases because there may be gradual brightness changes in a rendered image from cells that are far apart.\n' +
      '\n' +
      '## 6 Conclusion and Limitation\n' +
      '\n' +
      'In this paper, we propose VastGaussian, the first high-quality reconstruction and real-time rendering method on large-scale scenes. The introduced progressive data partitioning strategy allows for independent cell optimization and seamless merging, obtaining a complete scene with sufficient 3D Gaussians. Our decoupled appearance modeling decouples appearance variations in the training images, and enables consistent rendering across different views. This module can be discarded after optimization to obtain faster rendering speed. While our VastGaussian can be applied to spatial divisions of any shape, we do not provide an optimal division solution that should consider the scene layout, the cell number and the training camera distribution. In addition, there are a lot of 3D Gaussians when the scene is huge, which may need a large storage space and significantly slow down the rendering speed.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c} \\hline \\hline \\#Cell & \\#GPU & SSIM & PSNR & LPIPS & Training \\\\ \\hline\n' +
      '4 & 4 & 0.870 & 26.39 & 0.136 & 2h46m \\\\\n' +
      '8 & 8 & 0.885 & **26.81** & 0.121 & 2h39m \\\\\n' +
      '16 & 16 & 0.888 & 26.80 & 0.116 & 2h30m \\\\\n' +
      '24 & 24 & **0.892** & 26.64 & **0.110** & **2h19m** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Effect of different cell numbers.\n' +
      '\n' +
      'Figure 8: Heavy floaters appear when the visibility is calculated in the airspace-agnostic way.\n' +
      '\n' +
      'Figure 6: The visibility-based camera selection can clearance-based point selection can reduce floaters in the airspace.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Model setting & SSIM & PSNR & LPIPS \\\\ \\hline\n' +
      '1) w/o VisCam & 0.694 & 20.05 & 0.261 \\\\\n' +
      '2) w/o CovPoint & 0.874 & 26.14 & 0.128 \\\\\n' +
      '3) w/o VisCam \\& CovPoint & 0.699 & 20.35 & 0.253 \\\\\n' +
      '4) airspace-aware \\(\\rightarrow\\) agnostic & 0.855 & 24.54 & 0.128 \\\\\n' +
      '5) w/o Decoupled AM & 0.858 & 25.08 & 0.148 \\\\ \\hline Full model & **0.885** & **26.81** & **0.121** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablation on data partition, visibility calculation and decoupled appearance modeling (Decoupled AM).\n' +
      '\n' +
      'Figure 7: The visibility-based camera selection can eliminate the appearance jumping on the cell boundaries.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. _Communications of the ACM_, 2011.\n' +
      '* [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _ICCV_, 2021.\n' +
      '* [3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _CVPR_, 2022.\n' +
      '* [4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In _ICCV_, 2023.\n' +
      '* [5] Piotr Bojanowski, Armand Joulin, David Lopez-Pas, and Arthur Szlam. Optimizing the latent space of generative networks. In _ICML_, 2018.\n' +
      '* [6] Ilker Bozcan and Erdal Kayacan. Au-air: A multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance. In _ICRA_, 2020.\n' +
      '* [7] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In _CVPR_, 2023.\n' +
      '* [8] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, and Jian Ren. Real-time neural light field on mobile devices. In _CVPR_, 2023.\n' +
      '* [9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In _ECCV_, 2022.\n' +
      '* [10] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated neural radiance fields in the wild. In _CVPR_, 2022.\n' +
      '* [11] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenferl: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In _CVPR_, 2023.\n' +
      '* [12] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. _arXiv preprint arXiv:2309.16585_, 2023.\n' +
      '* [13] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In _ECCV_, 2018.\n' +
      '* [14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _CVPR_, 2022.\n' +
      '* [15] Sara Fridovich-Keil, Giacomo Maein, Frederik Rahbaek Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In _CVPR_, 2023.\n' +
      '* [16] Christian Fruh and Avideh Zakhor. An automated method for large-scale, ground-based city model acquisition. _IJCV_, 2004.\n' +
      '* [17] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and Richard Szeliski. Towards internet-scale multi-view stereo. In _CVPR_, 2010.\n' +
      '* [18] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: A reality check. In _NeurIPS_, 2022.\n' +
      '* [19] Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven M Seitz. Multi-view stereo for community photo collections. In _ICCV_, 2007.\n' +
      '* [20] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In _ICCV_, 2021.\n' +
      '* [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM ToG_, 2023.\n' +
      '* [22] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads: Augmented autonomous driving simulation using data-driven algorithms. _Science robotics_, 2019.\n' +
      '* [23] Xiaowei Li, Changchang Wu, Christopher Zach, Svetlana Lazebnik, and Jan-Michael Frahm. Modeling and recognition of landmark image collections using iconic scene graphs. In _ECCV_, 2008.\n' +
      '* [24] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In _CVPR_, 2023.\n' +
      '* [25] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In _SIGGRAPH Asia_, 2022.\n' +
      '* [26] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, reconstructing, and simulating: the urbanscene3d dataset. In _ECCV_, 2022.\n' +
      '* [27] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In _CVPR_, 2023.\n' +
      '* [28] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. _arXiv preprint arXiv:2308.09713_, 2023.\n' +
      '* [29] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In _CVPR_, 2021.\n' +
      '* [30] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. In _CVPR_, 2019.\n' +
      '* [31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [32] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM ToG_, 2022.\n' +
      '* [33] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic scenes. In _CVPR_, 2021.\n' +
      '\n' +
      '* [34] Marc Pollefeys, David Nister, J-M Frahm, Amir Akbarzadeh, Philippos Mordohai, Brian Clipp, Chris Engels, David Gallup, S-J Kim, Paul Merrell, et al. Detailed real-time urban 3d reconstruction from video. _IJCV_, 2008.\n' +
      '* [35] Ravi Ramamoorthi. Nerfs: The search for the best 3d representation. _arXiv preprint arXiv:2308.02751_, 2023.\n' +
      '* [36] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonfer: Speeding up neural radiance fields with thousands of tiny mlps. In _ICCV_, 2021.\n' +
      '* [37] Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. _ACM ToG_, 2023.\n' +
      '* [38] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _CVPR_, 2016.\n' +
      '* [39] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In _SIGGRAPH_. 2006.\n' +
      '* [40] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _CVPR_, 2022.\n' +
      '* [41] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _CVPR_, 2022.\n' +
      '* [42] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. _arXiv preprint arXiv:2309.16653_, 2023.\n' +
      '* [43] Jiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Errui Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. In _ICCV_, 2023.\n' +
      '* [44] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In _CVPR_, 2022.\n' +
      '* [45] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In _CVPR_, 2022.\n' +
      '* [46] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for efficient novel view synthesis. In _ECCV_, 2022.\n' +
      '* [47] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _NeurIPS_, 2021.\n' +
      '* [48] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training with free camera trajectories. In _CVPR_, 2023.\n' +
      '* [49] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. In _ICCV_, 2023.\n' +
      '* [50] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Humanerf: Free-viewpoint rendering of moving people from monocular video. In _CVPR_, 2022.\n' +
      '* [51] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. _arXiv preprint arXiv:2310.08528_, 2023.\n' +
      '* [52] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In _ECCV_, 2022.\n' +
      '* [53] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In _CVPR_, 2023.\n' +
      '* [54] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Sean Rafferty, and Henrik Kretzschmar. Surfelgan: Synthesizing realistic sensor data for autonomous driving. In _CVPR_, 2020.\n' +
      '* [55] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. _arXiv preprint arXiv:2309.13101_, 2023.\n' +
      '* [56] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. _arXiv preprint arXiv:2310.10642_, 2023.\n' +
      '* [57] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In _NeurIPS_, 2021.\n' +
      '* [58] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. _arXiv preprint arXiv:2302.14859_, 2023.\n' +
      '* [59] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-dreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. _arXiv preprint arXiv:2310.08529_, 2023.\n' +
      '* [60] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plencotrees for real-time rendering of neural radiance fields. In _ICCV_, 2021.\n' +
      '* [61] MI Zhenxing and Dan Xu. Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In _ICLR_, 2022.\n' +
      '* [62] Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very large-scale global sfm by distributed motion averaging. In _CVPR_, 2018.\n' +
      '* [63] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In _VIS_, 2001.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'Figure 10: Evolution of the transformation maps (\\(\\mathcal{M}_{i}\\) and \\(\\mathcal{M}_{i+1}\\)), the rendered images (\\(\\mathcal{I}_{i}^{r}\\) and \\(\\mathcal{I}_{i+1}^{r}\\)), and the adjusted images (\\(\\mathcal{I}_{i}^{a}\\) and \\(\\mathcal{I}_{i+1}^{a}\\)) during the optimization process.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>