<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Stop Regressing: Scalable Deep RL을 위한 분류를 통한 훈련 값 함수\n' +
      '\n' +
      'Jesse Farebrother1,2,*, Jordi Orbay1,+, Quan Vuong1,+, Adrien Ali Taiga1,+, Yevgen Chebotar1, Ted Xiao1, Alex Irpan1, Sergey Levine1, Pablo Samuel Castro1,3,+, Aleksandra Faust1, Aviral Kumar1,+, Rishabh Agarwal1,3,*\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '가치 함수는 심층 강화 학습(RL)의 중심 구성요소이다. 신경망에 의해 매개변수화된 이러한 함수는 부트스트랩된 목표 값과 일치하도록 평균 제곱 오차 회귀 목표를 사용하여 훈련된다. 그러나 대용량 트랜스포머와 같은 대규모 네트워크에 회귀를 사용하는 스케일링 값 기반 RL 방법은 어려운 것으로 입증되었다. 이러한 어려움은 지도 학습과 극명한 대조를 이룬다: 교차 엔트로피 분류 손실을 활용하여 지도 방법이 대규모 네트워크에 안정적으로 확장되었다. 이러한 불일치를 관찰하여 본 논문에서는 학습 가치 함수에 대한 회귀 대신 분류를 사용하여 단순히 심층 RL의 확장성을 향상시킬 수 있는지 여부를 조사한다. 우리는 범주형 교차 엔트로피로 훈련된 가치 함수가 다양한 도메인에서 성능과 확장성을 크게 향상시킨다는 것을 보여준다. 여기에는 SoftMoEs가 있는 Atari 2600 게임에서 단일 작업 RL, 대규모 ResNets가 있는 Atari에서 다중 작업 RL, Q-트랜스포머를 사용한 로봇 조작, 검색 없이 체스 게임, 대용량 트랜스포머를 사용한 언어 에이전트 워들 작업이 포함되어 이러한 도메인에 대한 최첨단 결과를 달성한다. 세심한 분석을 통해 범주형 교차 엔트로피의 이점은 주로 노이즈 표적 및 비정상성과 같은 가치 기반 RL에 고유한 문제를 완화할 수 있는 능력에서 비롯됨을 보여준다. 전반적으로 범주형 교차 엔트로피가 있는 훈련 값 함수로의 단순한 이동은 적은 비용으로 심층 RL의 확장성에 상당한 개선을 가져올 수 있다고 주장한다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '알렉스넷(Krizhevsky et al., 2012)에서 트랜스포머(Vaswani et al., 2017)에 이르기까지 딥 러닝 돌파구에서 명확한 패턴이 등장하는데, 분류 문제는 특히 큰 신경망을 사용한 효과적인 훈련에 적응할 수 있는 것으로 보인다. 회귀 접근법이 자연스럽게 나타나는 시나리오에서도 문제를 분류 문제로 대신 프레이밍하는 것은 종종 성능을 향상시킨다(Torgo and Gama, 1996; Rothe et al., 2018; Rogez et al., 2019). 여기에는 실제 값 대상을 범주형 레이블로 변환하고 평균 제곱 오차가 아닌 범주형 교차 엔트로피를 최소화하는 것이 포함된다. 안정적인 기울기(Imani and White, 2018; Imani et al., 2024), 더 나은 표현(Zhang et al., 2023), 암시적 편향(Stewart et al., 2023), 불균형 데이터를 다루는(Pintea et al., 2023) 등 이 접근법의 우월성을 설명하기 위해 몇 가지 가설이 제시되었다.\n' +
      '\n' +
      '지도 학습의 경향과 달리 가치 기반 강화 학습(RL) 방법은 주로 회귀에 의존한다. 예를 들어, 딥 Q-러닝(Mnih et al., 2015) 및 액터-크리틱(Mnih et al., 2016)과 같은 딥 RL 방법들은 연속 스칼라 타겟들로부터 값 함수를 트레이닝하기 위해 평균 제곱 에러와 같은 회귀 손실을 사용한다. 회귀 손실에 의해 구동되는 이러한 가치 기반 심층 RL 방법은 높은 주목을 받는 결과로 이어졌지만(Silver et al., 2017), 대용량 변압기와 같은 대규모 네트워크로 확장하기가 어려웠다. 이러한 확장성 부족은 여러 문제(Kumar et al., 2021, 2022; Agarwal et al., 2021; Lyle et al., 2022; Le Lan et al., 2023; Obando-Ceron et al., 2024)에 기인하지만, 단순히 회귀 문제를 분류로 재구성하는 것이 지도 학습에서 달성되는 동일한 수준의 확장성을 가능하게 할 수 있다면?_ 본 논문에서는 범주형 교차 엔트로피 손실이 있는 가치 함수를 훈련하기 위한 분류 레이블을 도출하는 다양한 방법의 유효성을 평가하여 이 질문에 답하기 위한 광범위한 연구를 수행한다. 우리의 연구 결과는 교차 엔트로피가 있는 훈련 가치 함수가 전통적인 회귀 기반 접근법에 비해 심층 RL 방법(그림 1)의 성능, 견고성 및 확장성을 실질적으로 향상시킨다는 것을 보여준다. 가장 주목할 만한 방법(HL-Gauss; Imani and White, 2018)은 Mixture-of-Experts with Aatari(Obando-Ceron et al., 2024); \\(\\mathbf{1.8-2.1x}\\)의 성능을 Atari(Kumar et al., 2023; Ali Taiga et al., 2023); \\(\\mathbf{40\\%}\\)의 언어-에이전트 작업에서 워드(Snell et al., 2023); \\(\\mathbf{70\\%}\\)의 성능 향상 (Ruoss et al., 2024); 및 \\(\\mathbf{67\\%}\\)의 성능 향상 (Chebotar et al., 2023); 을 포함한다. 다양한 도메인, 네트워크 아키텍처 및 알고리즘에 걸친 일관된 경향은 심층 RL에서 회귀를 분류로 처리하는 상당한 이점을 강조하며, 가치 기반 RL을 확장하는 방향으로 이동할 때 회귀의 중요한 구성 요소로서의 잠재력을 강조한다.\n' +
      '\n' +
      '심층 RL**에서 평균 제곱 오차(MSE) 회귀 손실에 대한 "드롭인" 대체로서 교차 엔트로피의 사용을 지원하기 위한 **강력한 경험적 결과를 사용하여 이러한 경험적 이득의 출처를 이해하려고 시도한다. 주의 깊은 진단 실험을 기반으로 범주형 교차 엔트로피 손실이 평균 제곱 회귀에 비해 많은 이점을 제공한다는 것을 보여준다. 우리의 분석은 범주형 교차 엔트로피 손실이 잡음이 많은 표적에 대한 견고성과 네트워크가 고정적이지 않은 표적에 맞는 용량을 더 잘 사용할 수 있도록 하는 것을 포함하여 심층 RL에 내재된 여러 문제를 완화한다는 것을 시사한다. 이러한 결과는 심층 RL에서 범주형 교차 엔트로피의 강력한 경험적 이점을 설명하는 데 도움이 될 뿐만 아니라 해당 분야에 더 효과적인 학습 알고리즘을 개발하는 데 통찰력을 제공한다.\n' +
      '\n' +
      '##2 예비 및 배경\n' +
      '\n' +
      '**분류로서의 회귀** 우리는 주어진 입력\\(x\\in\\mathbb{R}^{d}\\)이 벡터\\(\\theta\\in\\mathbbb{R}^{k}\\)에 의해 매개변수화된 일부 고정 분산\\(\\sigma^{2}\\)과 예측 함수\\(\\hat{y}:\\mathbbb{R}^{d}\\times\\mathbbb{R}^{k}\\)에 대한 조건부 분포\\(Y\\mid x\\sim\\mathcal{N}(\\mu=\\hat{y}(x;\\theta),\\sigma^{2})\\)로 표적을 모델링하고자 한다. 상기 최대값은\n' +
      '\n' +
      '그림 1: **MoE(§4.2.1), ResNets(§4.2) 및 Transformers(§4.3)를 포함한 현대 아키텍처를 가진 훈련 가치-네트워크에 대한 MSE 회귀 손실에 대한 HL-Gauss 교차 엔트로피 손실(§3.1)의 성능 이득. x축 레이블은 도메인 이름에 해당하며, 괄호 안의 학습 방법이 있습니다. 다중 작업 RL 결과의 경우 실험에서 가장 큰 네트워크인 ResNet-101 백본으로 이득을 보고한다. 체스의 경우 270M 변압기에 대해 스톡피쉬 엔진에 비해 성능 격차가 개선되었음을 보고한다. Wordle의 경우 0.1.**의 행동 규칙화로 결과를 보고한다.\n' +
      '\n' +
      'likelihood estimator for data \\(\\left\\{x_{i},y_{i}\\right\\}_{i=1}^{N}\\)은 **mean-squared error (MSE)** objective,\n' +
      '\n' +
      '\\[\\min_{\\theta}\\;\\sum_{i=1}^{N}\\left(\\hat{\\phi}(x_{i};\\theta)-y_{i}\\right)^{2}\\,,\\]\n' +
      '\n' +
      '최적 예측 변수는 \\(\\hat{y}(x;\\theta^{*})=\\mathbb{E}\\left[Y\\,|\\,x\\right]\\이다.\n' +
      '\n' +
      '조건부 분포의 평균을 직접 학습하는 대신 목표값에 대한 분포를 학습한 다음, 예측 \\(\\hat{y}\\)을 분포의 통계량으로 복원하는 대체 접근법이 있다. 이를 위해 확률밀도함수 \\(p(y\\,|\\,x)\\)를 갖는 표적분포 \\(Y\\,|\\,x\\)을 구성하여 스칼라 표적이 이 분포의 평균으로 복원될 수 있도록 한다. 우리는 이제 회귀 문제를 목표 \\(p(y\\,|\\,x)\\)로의 KL 발산을 최소화하는 매개변수화된 분포 \\(\\hat{p}(y\\,|\\,x;\\theta)\\)를 학습하는 것으로 프레임화할 수 있다.\n' +
      '\n' +
      '\\[\\min_{\\theta}\\;\\sum_{i=1}^{N}\\int_{\\mathbf{y}p(y\\,|\\,x_{i})\\log\\left(\\hat{p}(y\\,|\\,x_{i};\\theta)\\right)dy\\tag{2.1}\\.\n' +
      '\n' +
      '이는 교차 엔트로피 목표이다. 마지막으로, 예측은 \\(\\hat{y}(x;\\theta)=\\mathbb{E}_{\\hat{p}}\\left[Y\\,|\\,x;\\theta\\,\\right]\\으로 복원될 수 있다.\n' +
      '\n' +
      '이 새로운 문제 공식을 고려할 때, 분포 학습 문제를 다루기 쉬운 손실로 변환하기 위해 \\(\\hat{p}\\)을 \\(m\\) 등간격 위치 또는 "classes"로 정의된 \\([v_{\\min},v_{\\max}]\\)에 지원되는 범주형 분포 집합으로 제한한다.\n' +
      '\n' +
      '\\[\\mathcal{Z}=\\left\\{\\sum_{i=1}^{m}p_{i}\\,\\delta_{z_{i}}\\;:\\;p_{i}\\geq 0,\\sum_{i=1}^{m}p_{i}=1\\right\\\\,\\tag{2.2}\\;\n' +
      '\n' +
      '여기서 \\(p_{i}\\)는 위치 \\(z_{i}\\)와 연관된 확률이고 \\(\\delta_{z_{i}}\\)는 위치 \\(z_{i}\\)에서의 디락 델타 함수이다. 최종적인 장애물은 목표분포\\(Y\\,|\\,x\\)와 그와 관련된 투영을 범주분포\\(\\mathcal{Z}\\)의 집합에 구성하는 절차를 정의하는 것이다. 우리는 이 논의를 RL의 맥락에서 이러한 단계를 수행하기 위한 다양한 방법에 대해 논의하는 SS3으로 연기한다.\n' +
      '\n' +
      '본 논문에서는 환경 전이 확률에 따라 에이전트가 현재 상태(S_{t}\\in\\mathcal{S}\\)에서 행동(A_{t}\\in\\mathcal{A}\\)을 취한 후 다음 상태(S_{t+1}\\in\\mathbb{R}\\)로 전이하기 전에 보상(R_{t+1}\\in\\mathcal{S}\\)을 처방하여 환경과 상호작용하는 강화학습(RL) 문제를 고려한다. 수익률은 일련의 행동의 품질을 누적 할인된 보상 합(G_{t}=\\sum_{k=0}^{\\infty}y^{k}R_{t+k+1}\\)으로 수치화하며, 여기서 \\(y\\in[0,1)\\)은 할인 요인이다. 에이전트의 목표는 기대수익을 최대화하는 정책\\(\\pi:\\mathcal{S}\\rightarrow\\mathcal{P}(\\mathcal{A})\\)을 배우는 것이다. 액션-값 함수는 액션을 취한 상태(s\\)에서 액션을 취한 후 정책(\\pi\\)에 따라 예상되는 수익을 쿼리할 수 있다. \\(q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[G_{t}\\,|\\,S_{t}=s,\\,A_{t}=a\\right]\\.\n' +
      '\n' +
      'Deep Q-Networks (DQN; Mnih et al., 2015)는 \\(\\theta\\)에 의해 파라미터화된 신경망으로 근사 최적 상태-행동 값 함수 \\(Q(s,a;\\theta)\\approx q_{\\pi^{*}}(s,a)\\)를 학습할 것을 제안한다. 구체적으로,\n' +
      '\n' +
      '그림 2: **분류로서의 회귀.** 데이터 포인트 \\(x_{i}\\)는 신경망에 의해 변환되어 소프트맥스를 통해 범주형 분포를 생성한다. 예측은 이 범주형 분포에 대한 예측으로 간주한다. 평균이 회귀 목표 \\(y_{i}\\)인 목표 분포에 대한 교차 엔트로피 손실에 대한 기울기 하강에 의해 네트워크의 로짓이 강화된다. 그림 3은 RL에서 목표 분포를 구성하고 투영하는 세 가지 방법을 보여준다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '스칼라로부터 범주형 분포를 구성하는###\n' +
      '\n' +
      '첫 번째 방법의 개요는 스칼라 목표 \\((\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-})\\)을 \\(\\{z_{i}\\}_{i=1}^{m}\\)에서 지원되는 범주형 분포에 투영할 것이다. 프로젝션 단계에 대한 널리 퍼졌지만 순진한 접근법은 스칼라를 \\(m\\) 빈 중 하나로 이산화하는 것을 포함하며, 여기서 \\(z_{i}\\)은 빈의 중심을 나타낸다. 결과적인 원-핫 분포는 "손실"이며 \\(Q\\) 함수의 오류를 유발한다. 이러한 오류는 더 많은 벨만 백업을 수행하여 더 편향된 추정치를 생성하고 더 나쁜 성능을 초래할 가능성이 있으므로 복합된다. 이에 대처하기 위해, 먼저 표적이 사이에 놓여 있는 두 위치에 0이 아닌 밀도를 두는 고유한 범주 분포를 통해 스칼라 표적 _exactly_을 나타내는 "two-hot" 접근법(Schrittwieser et al., 2020)을 고려한다(도 3 참조; 왼쪽).\n' +
      '\n' +
      '2-Hot Categorical Distribution.z\\(z_{i}\\)와 \\(z_{i+1}\\)은 TD 목표인 \\(z_{i}\\leq(\\widehat{\\mathcal{T}Q)(S_{t},A_{t};\\theta^{-})\\leq z_{i+1}\\)를 하한과 상한으로 하는 위치이다. 그 다음, 이들 위치에 놓일 확률 \\(p_{i}\\) 및 \\(p_{i+1}\\)은 다음과 같다.\n' +
      '\n' +
      '\\frac{(\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-})=\\frac{(\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-});z_{i}}{z_{i+1}-z_{i}},\\qquad p_{i+1}(S_{t},A_{t};\\theta^{-})= \\frac{z_{i+1}-(\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-}}}{z_{i+1}-z_{i}}. \\tag{3.2}\\cqad p_{i+1}(S_{t},A_{t};\\theta^{-}}}{z_{i+1}}(\\widehat{\\mathcal{T}}Q\n' +
      '\n' +
      '다른 모든 위치에 대해 범주형 분포로 규정된 확률은 정확히 0입니다. 원칙적으로, 이 Two-Hot 변환은 범주형 분포에 대한 스칼라 TD 타겟의 고유하게 식별 가능하고 손실되지 않는 표현을 제공한다. 그러나 Two-Hot는 이산 회귀의 서수 구조를 완전히 활용하지 않는다. 구체적으로, 클래스는 독립적이지 않고 대신 자연스러운 순서를 가지며, 여기서 각 클래스는 본질적으로 이웃과 관련이 있다.\n' +
      '\n' +
      'Imani and White(2018)가 소개한 Histogram Losses 클래스는 지도 분류에서 레이블 평활화와 유사한 확률 질량을 이웃 빈에 분배함으로써 회귀 작업의 서수 구조를 이용하고자 한다(Szegedy et al., 2016). 이는 표적 값의 잡음 버전을 두 위치로 제한되기보다는 확률 질량이 표적 근처의 여러 빈에 걸쳐 있을 수 있는 범주형 분포로 변환함으로써 수행된다(도 3 참조; 중심).\n' +
      '\n' +
      '히스토그램은 범주분포로서 형식적으로는 확률밀도\\(f_{Y|S_{t},A_{t}\\)와 누적분포함수\\(F_{Y|S_{t},A_{t}\\)을 갖는 확률변수\\(Y\\,|\\,S_{t_{1}},A_{t}\\)을 정의하며, 기대값은 \\((\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-})이다. 간격 \\([z_{i}-\\varsigma/2,z_{i}+\\varsigma/2]\\)에 걸쳐 적분함으로써 \\(z_{i}\\)을 중심으로 너비 \\(\\xi=(v_{\\max}-v_{\\min})/m\\)의 빈을 갖는 히스토그램에 분포 \\(Y\\,|\\,S_{t},A_{t}\\)을 투영하여 확률을 구하고,\n' +
      '\n' +
      '[p_{i}(S_{t},A_{t};\\theta^{-}=\\int_{z_{i}-\\varsigma/2}^{z_{i}+\\varsigma/2}f_{Y|S_{t},A_{t}(y\\,|\\,S_{t},A_{t})\\,dy\\] \\[=F_{Y|S_{t},A_{t}(z_{i}+\\varsigma/2\\,|\\,S_{t},A_{t})-F_{Y|S_{t},A_{t}(z_{i}-\\varsigma/2\\,|\\,S_{t},A_{t})\\;. \\tag{3.3}\\;\n' +
      '\n' +
      '그림 3: **교차 엔트로피 기반 TD 학습**에서 목표값 범주형 분포를 시각화함. Two-Hot(왼쪽, §3.1)이 정확히 두 위치에 확률 질량을 두는 반면, HL-Gauss(중간, §3.1)는 확률 질량을 (목표값을 평활화하는 것과 유사한) 이웃 위치에 분배한다. CDRL(오른쪽, §3.2)은 이웃 위치에 비례적으로 확률 질량을 분배하는 범주형 반환 분포를 모델링한다.\n' +
      '\n' +
      '우리는 이제 분포\\(Y\\left|\\,S_{t},A_{t}\\right.\\)을 선택할 수 있다. 가우시안 분포\\(Y\\left|\\,S_{t},A_{t}\\sim\\mathcal{N}(\\mu=(\\widehat{\\mathcal{T}}Q)(S_{t},A_{t};\\theta^{-}),\\sigma^{2})\\right.\\)를 이용하여 Imani와 White(2018)의 제안을 따른다. 여기서 분산 \\(\\sigma^{2}\\)은 결과 범주형 분포에 적용되는 레이블 평활화 양을 제어할 수 있는 하이퍼 매개변수입니다. 우리는 이 방법을 HL-Gauss라고 부른다.\n' +
      '\n' +
      'HL-Gauss는 bin width\\(\\varsigma\\)와 분포범위\\([v_{min},v_{max}]\\ 외에 표준편차\\(\\sigma\\)를 조정해야 한다. 표준분포에서 샘플링하여 얻은 샘플의 99.7%는 평균의 3 표준편차 이내에 있어야 하며, 이는 대략 \\(6\\cdot\\sigma/\\varsigma\\) 빈에 해당한다. 따라서, 튜닝을 권장하는 보다 해석 가능한 하이퍼파라미터는 \\(\\sigma/\\varsigma\\): 확률 질량을 \\(K/6\\)으로 설정하면 빈들 중 하나에 중심을 둔 평균값에 대해 대부분의 확률 질량을 \\([K]+1\\) 인접 위치로 분산시킨다. 달리 명시되지 않는 한, 우리는 약 6개의 위치에 질량을 분배하는 실험을 위해 \\(\\sigma/\\varsigma=0.75\\)을 설정했다.\n' +
      '\n' +
      '범주형 수익률 분포의 모델링\n' +
      '\n' +
      '이전 섹션에서는 예상 수익을 나타내는 일반적인 스칼라 회귀 목표에서 목표 분포를 구성하기로 결정했다. 또 다른 옵션은 분포 RL(Bellemare et al., 2023)에서 수행된 바와 같이 범주형 모델 \\(Z\\)을 사용하여 미래 수익에 대한 분포를 직접 모델링하는 것이다. 특히, 초기 분포 RL 접근법인 C51(Bellemare et al., 2017)은 예측된 분포\\(Z\\)와 TD 타겟의 분포 유사체 사이의 교차 엔트로피를 최소화하는 것과 함께 범주형 표현을 사용한다. 이를 위해, 교차 엔트로피 목표를 위한 목표 분포를 구성하기 위한 Two-Hot 및 HL-Gauss의 대안으로 C51을 또한 조사한다.\n' +
      '\n' +
      '**범주형 분포 RL.**범주형 수익률 분포를 모델링하는 첫 번째 단계는 \\(Z\\)에서 유사한 확률적 분포 벨만 연산자를 정의하는 것이고,\n' +
      '\n' +
      '\\[(\\widehat{\\mathcal{T}}Z)(s,a;\\theta^{-})\\overset{D}{=}\\sum_{i=1}^{m}\\hat{p}_{ i}(S_{t+1},A_{t+1};\\theta^{-})\\cdot\\delta_{R_{t+1}+7z_{i}}\\,\\left|\\,S_{t}=s,\\,A_{t}=a\\,,\\right.\\]\n' +
      '\n' +
      '여기서 \\(A_{t+1}=\\arg\\max_{a^{\\prime}}Q(S_{t+1},a^{\\prime})\\). 우리가 볼 수 있듯이 확률적 분포 벨만 연산자는 Bellemare et al.(2017)에 의해 처음 소개된 범주형 투영을 필요로 하는 위치 \\(z_{i}\\)을 이동 및 스케일링하는 효과가 있다. 높은 수준에서 이 투영은 가까운 이웃 위치\\(z_{j-1}\\leq R_{t+1}+yz_{i}\\leq z_{j}\\)에 비례적으로 확률을 분배한다(도 3 참조; 오른쪽). 이러한 주변 위치를 파악하기 위해 \\(\\left\\lfloor x\\right\\rfloor=\\arg\\max\\z_{i}:z_{i}\\leq x\\}\\)와 \\(\\left\\lceil x\\right\\rceil=\\arg\\min\\z_{i}:z_{i}\\geq x\\}\\)을 정의한다. 이제 위치 \\(z_{i}\\)에 대한 확률을 다음과 같이 쓸 수 있다.\n' +
      '\n' +
      '(S_{t},A_{t};\\theta^{-})=\\sum_{j=1}^{m}{hat{p}_{j}(S_{t+1},A_{t+1};\\theta^{-})\\cdot\\xi_{j}(R_{t+1}+yz_{i})\\tag{3.4}\\xi_{j}(x)=\\frac{z_{j-z_{j+1}-z_{rfloor=z_{j}+\\frac{z_{j+1}-z_{j}\\mathds{p}{m}\\hat{p}{m}{m}(S_{t+1},A_{t+1};\\theta^{-})\\cot\\xi_{j}(R_{t+1}+yz_{i})\\cot\\xi_{j}(x)=\\frac{z_{j+1}-z_{\n' +
      '\n' +
      '범주형 사영의 완전한 설명은 Bellemare et al.(2023, Chapter 5)을 참조한다.\n' +
      '\n' +
      '##4 RL에서의 분류 손실 평가\n' +
      '\n' +
      '이 섹션의 실험의 목표는 다양한 문제에 대한 가치 기반 심층 RL의 성능 및 확장성을 개선하는 범주형 교차 엔트로피 손실(3.1)과 결합된 섹션 3에서 논의된 다양한 목표 분포의 유효성을 평가하는 것이다. 여기에는 언어 에이전트, 체스 및 로봇 조작을 포함하는 아타리 이외의 도메인뿐만 아니라 아타리 2600 게임에서 여러 단일 작업 및 다중 작업 RL 문제가 포함된다. 이러한 작업은 온라인 및 오프라인 RL 문제로 구성된다. 각 작업에 대해 이전에 해당 작업에 대해 평가된 강력한 가치 기반 RL 접근법과 함께 교차 엔트로피 손실을 인스턴스화한다. 우리가 고려하는 각 도메인에 대한 하이퍼파라미터를 포함한 전체 실험 방법론은 부록 B에서 찾을 수 있다.\n' +
      '\n' +
      '아타리 게임에서의 단일 작업 RL\n' +
      '\n' +
      '먼저 아케이드 학습 환경에서 HL-Gauss, Two-Hot, C51(Bellemare et al., 2017)의 효과를 평가한다(Bellemare et al., 2013). 우리의 회귀 기준선에 대해 우리는 다른 회귀 기반 손실(Ceron and Castro, 2021)을 능가하는 것으로 나타난 평균 제곱 오차 TD 대물렌즈에 대해 DQN(Mnih et al., 2015)을 훈련한다. 각 방법은 Adam optimizer로 학습되며, 이는 회귀 기반 방법과 분포 RL 접근법 사이의 성능 불일치를 감소시키는 것으로 나타났다(Agarwal et al., 2021).\n' +
      '\n' +
      '평가** 아가왈 외(2021)의 권장 사항에 따라, 우리는 각각 여러 종자가 있는 게임에 걸쳐 집계된 95% 계층화된 부트스트랩 신뢰 구간(CI)으로 사분위간 평균(IQM) 정규화된 점수를 보고한다. 우리는 온라인 RL에 대해 60개의 아타리 게임에 걸쳐 인간 정규화된 집계 점수를 보고한다. 오프라인 RL의 경우 쿠마르 등(2021)의 프로토콜에 따라 17개 게임에 걸쳐 집계된 행동 정책 정규화 점수를 보고한다.\n' +
      '\n' +
      '**온라인 RL 결과** Mnih et al.(2015)의 설정에 따라, 전술한 손실과 함께 200M 프레임에 대해 DQN을 트레이닝한다. 우리는 그림 4에서 60개의 아타리 게임에서 인간 정규화된 IQM 성능과 최적성 격차를 집계했으며 HL-Gauss가 Two-Hot 및 MSE 손실보다 훨씬 우수하다는 것을 관찰한다. 흥미롭게도 HL-가우스는 수익률 분포를 모델링하지 않았음에도 불구하고 범주형 분포 RL(C51)에서도 개선된다. 이 결과는 수익 분포를 모델링하는 것과 비교하여 손실(범주형 교차 엔트로피)이 아마도 C51에 더 중요한 요소임을 시사한다.\n' +
      '\n' +
      '**오프라인 RL 결과** 자체 수집 상호 작용에서 학습하는 온라인 DQN을 사용한 HL-가우스의 강력한 성능은 오프라인 데이터 세트에서 학습하는 데에도 효과적인지 여부에 대한 질문을 제기한다. 이를 위해 6.25M 그래디언트 단계 동안 CQL(SS2)을 사용하여 10% Atari DQN 재생 데이터세트(Agarwal et al., 2020)에서 손실이 다른 에이전트를 훈련한다. 그림 4에서 볼 수 있듯이 HL-Gauss와 C51은 일관되게 MSE를 능가하는 반면 Two-Hot는 MSE에 비해 향상된 안정성을 보이지만 다른 분류 방법보다 성능이 낮다. 특히, HL-Gauss는 이 환경에서 다시 C51을 능가한다. 또한, 평균 제곱 회귀 손실을 활용하는 Kumar et al.(2021)의 결과와 일치하여 장기간의 훈련으로 성능 저하를 초래한다. 그러나 교차 엔트로피 손실(HL-Gauss와 C51 모두)은 이러한 열화를 나타내지 않으며 일반적으로 안정적으로 유지된다.\n' +
      '\n' +
      '### Scaling Value-based RL to Large Networks\n' +
      '\n' +
      '지도 학습에서, 특히 언어 모델링을 위해(Kaplan et al., 2020), 네트워크의 파라미터 카운트를 증가시키는 것은 전형적으로 성능을 향상시킨다. 그러나, 이러한 스케일링 거동은 값 기반 딥 RL 방법들에 대해 여전히 파악하기 어려우며, 여기서 _naive_ 파라미터 스케일링은 성능을 손상시킬 수 있다(Ali Taiga et al., 2023; Kumar et al., 2023; Obando-Ceron et al., 2024). 이를 위해 심층 RL에서 MSE 회귀 손실의 대안으로 가치 네트워크에 대한 매개변수 스케일링으로 더 나은 성능을 가능하게 하는 분류 방법의 유효성을 조사한다.\n' +
      '\n' +
      '도 4: (왼쪽) 온라인 RL 및 (오른쪽) 오프라인 RL에 대한 **회귀 대 교차 엔트로피 손실(§4.1). HL-Gauss와 CDRL은 MSE를 능가하며, HL-Gauss는 최고의 성능을 보인다. 또한, Two-Hot 손실은 MSE를 능가하지만 다른 교차 엔트로피 손실과 유사한 오프라인 RL에서 장기간의 훈련으로 더 안정적이다. 자세한 내용은 §4.1을 참조하십시오.**\n' +
      '\n' +
      '혼합 전문가와 4.2.1 스케일링\n' +
      '\n' +
      '최근, Obando-Ceron et al. (2024)은 컨볼루션 네트워크들을 사용한 파라미터 스케일링이 Atari 상의 단일-태스크 RL 성능을 해치는 반면, 그러한 네트워크들에서 Mixture-of-Expert (MoE) 모듈들을 통합하는 것이 성능을 향상시킨다는 것을 입증한다. 이를 위해 임팔라(Espeholt et al., 2018)가 사용하는 아키텍처에서 두 번째 레이어를 SoftMoE(Puigcerver et al., 2024) 모듈로 교체하고 \\(\\{1,2,4,8\\}\\)의 전문가 수를 달리한다. 각 전문가는 원래 두 번째 계층의 복사본이기 때문에 이 계층의 매개변수 카운트는 전문가 수와 동일한 요인만큼 증가한다. 우리가 만드는 유일한 변화는 Obando-Ceron et al.(2024)에 의해 채용된 바와 같이 SoftMoE DQN에서 MSE 손실을 HL-Gauss 교차 엔트로피 손실로 대체하는 것이다. 우리는 Obando-Ceron 등(2024)에 의해 사용된 20개의 아타리 게임의 동일한 하위 집합에 대해 훈련하고 그림 5에서 5개의 종자에 대한 집계 결과를 보고한다.\n' +
      '\n' +
      '그림 5와 같이 HL-Gauss는 전문가 수와 무관한 상수 요인에 의해 MSE에 비해 지속적으로 성능이 향상됨을 알 수 있다. 또한 SoftMoE + MSE가 MSE 단독으로 관찰된 부정적인 스케일링 효과 중 일부를 완화시키는 것으로 보인다는 것을 관찰할 수 있다. SoftMoE + MSE는 끝에서 두 번째 레이어에서 소프트맥스를 사용하기 때문에 분류 손실을 사용하는 것과 유사한 이점을 제공할 수 있지만 나중에 볼 것처럼 이러한 이점만으로는 소프트맥스의 추가로 설명할 수 없다.\n' +
      '\n' +
      '레즈네츠를 활용한 교육 일반주의 정책\n' +
      '\n' +
      '다음으로, 우리는 Atari에 대한 일반주의 비디오 게임-플레이링 정책을 훈련시키기 위해 오프라인 및 온라인 설정 모두에서 스케일링 값 기반 ResNets(He et al., 2016)를 고려한다. 각각의 경우에, 우리는 다중 작업 RL을 위해 서로 다른 크기의 Q-네트워크의 패밀리를 훈련시키고, 네트워크 크기의 함수로서 성능을 보고한다.\n' +
      '\n' +
      '**멀티태스크 온라인 RL.** Following Ali Taiga et al.(2023), 우리는 상이한 환경 역학 및 보상을 갖는 아타리 게임 변형을 플레이할 수 있는 멀티태스크 정책을 훈련한다(Farebrother et al., 2018). 우리는 두 개의 아타리 게임을 평가한다: 소행성의 경우 63개의 변종, 우주 침략자의 경우 29개의 변종. 분산 액터-크리틱 방법인 IMPALA(Espeholt et al., 2018)를 사용하였으며, 표준 MSE 비평가 손실과 교차 엔트로피 기반 HL-Gauss 손실을 비교한다. 실험에서는 Impala-CNN(\\(\\leq\\) 2M 파라미터)에서 ResNet-101(44M 파라미터)까지 더 큰 ResNets(He et al., 2016)로 이동할 때 이러한 손실의 스케일링 특성을 조사한다. 150억 프레임 동안 훈련한 후 다중 작업 성능을 평가하고 5개의 시드로 각 실험을 반복한다.\n' +
      '\n' +
      '소행성에 대한 결과는 그림 6에 나와 있고, 우주 침입자에 대한 추가 결과는 그림 D.3에 나와 있으며, 두 환경 모두에서 HL-Gauss가 일관되게 MSE보다 우수함을 관찰한다. 특히, HL-가우스는 특히 ResNet-18을 넘어 더 큰 네트워크로 성능을 약간 향상시키는 소행성에서 더 잘 스케일링하는 반면 MSE 성능은 크게 저하된다.\n' +
      '\n' +
      '다중 게임 오프라인 RL.** 배포 C51 대신 비 배포 HL-Gauss 손실을 사용하기 위해 레시피를 수정하는 Kumar et al.(2023)의 설정을 고려한다. 구체적으로, 우리는 각 게임에서 독립적으로 훈련된 온라인 RL 에이전트로부터 얻은 리플레이 버퍼로 구성된 "near-optimal" 훈련 데이터 세트로부터 학습할 때 40개의 다른 아타리 게임을 동시에 플레이하도록 단일 일반 정책을 훈련한다. 이러한 다중 게임 RL 설정은 원래 Lee et al.(2022)에 의해 제안되었다. 나머지 설계 선택사항(예: 피쳐 정규화, 네트워크의 크기)은 동일하게 유지됩니다.\n' +
      '\n' +
      '도 7에 도시된 바와 같이, HL-가우스는 쿠마르 외(2023)로부터의 C51 결과보다 훨씬 더 양호하게 스케일링되어, IQM 인간 정규화 스코어에 의해 측정된 바와 같이 ResNet-101(80M 파라미터)로 이용 가능한 최상의 사전 멀티 게임 결과에 비해 약 45% 개선된다(Agarwal 외, 2021). 또한 MSE 회귀 손실의 성능은 일반적으로 ResNet-34를 넘어 모델 용량을 증가시키면 안정되지만 HL-가우스는 이 용량을 활용하여 성능을 개선할 수 있어 분류 기반 교차 엔트로피 손실의 유효성을 나타낸다. 또한 DQN 에이전트에 의해 얻은 점수에 대해 정규화할 때 그림 D.4에서 성능 외에도 모델 척도가 증가함에 따른 개선 속도도 C51에 비해 HL-가우스 손실에 대해 더 큰 경향이 있음을 보여준다.\n' +
      '\n' +
      '변압기를 이용한### 값기반 RL\n' +
      '\n' +
      '다음으로, 아타리 이상의 HL-Gauss 교차 엔트로피 손실의 적용 가능성을 평가한다. 이를 위해 대용량 트랜스포머를 활용하는 몇 가지 작업, 즉 Wordle 재생, 추론-시간 탐색 없이 체스 재생, 로봇 조작 등의 언어 에이전트 작업을 고려한다.\n' +
      '\n' +
      '######4.3.1 언어 에이전트 : 단어\n' +
      '\n' +
      '언어 에이전트 벤치마크에서 분류 손실이 가치 기반 RL 접근법의 성능을 향상시키는지 평가하기 위해, Wordle1 게임을 수행하는 태스크에서 HL-Gauss와 MSE를 비교한다. Wordle은 에이전트가 6번의 단어 추측을 시도하는 단어 추측 게임이다. 각 턴에서 에이전트는 추측된 글자가 참 단어에 있는지 여부에 대한 환경 피드백을 받는다. 이 작업의 역학은 비결정적이다. 보다 일반적으로, 태스크는 턴-기반 구조를 따르고,\n' +
      '\n' +
      '도 7: Multi-game Atari(Offline RL).** IQM human normalized score for ResNet-\\((34,50,101)\\), with spatial embedding, to play 40 Atari games simultaneously using single value network(Kumar et al., 2023). HL-Gauss는 다중 게임 결정 트랜스포머(Lee et al., 2022) 뿐만 아니라 이전 작업에 의해 사용된 범주형 분포 RL(C51) 및 회귀(MSE) 손실을 실질적으로 능가하는 현저한 스케일링을 가능하게 한다. 더 자세한 내용은 §4.2.2를 참조하고 일반적으로 사용되는 또 다른 메트릭인 DQN 정규화된 점수로 보고된 이러한 결과의 버전은 그림 D.4를 참조한다.\n' +
      '\n' +
      '자연어 처리에서 대화 작업의 가속화. 이 실험은 오프라인 RL 설정에 있으며, 여기서 우리는 Snell 등이 제공한 차선책 게임 플레이의 데이터 세트를 활용한다(2023). 우리의 목표는 Q-네트워크를 나타내는 125M 파라미터로 GPT와 같은 디코더 전용 트랜스포머를 훈련하는 것이다. 변압기 모델이 이 게임을 수행하는 데 사용되는 방법은 그림 8(왼쪽)을 참조하십시오.\n' +
      '\n' +
      '본 논문에서는 DQN에서 Q-learning 업데이트와 CQL-style behavior regularizer(SS2)를 결합한 오프라인 RL 방법을 사용하여 20K Gradient 단계의 언어 기반 트랜스포머를 훈련한다. 이는 표준 다음 토큰 예측 손실(이 특정 문제)에 해당한다. 그림 8과 같이 HL-Gauss는 CQL 정규화의 강도를 제어하는 다중 계수에 대해 MSE보다 우수하다.\n' +
      '\n' +
      '검색이 없는 그랜드마스터급 체스 4.3.2\n' +
      '\n' +
      '트랜스포머는 증류를 통해 값비싼 추론-시간 계산을 효과적으로 상각하는 범용 알고리즘 근사자로서의 효과를 입증했다(Ruoss et al., 2024; Lehnert et al., 2024). 이러한 맥락에서, 우리는 스칼라 작용 값을 가치 함수를 증류하기 위한 분류 대상으로 변환하기 위해 HL-가우스를 사용하는 잠재적인 이점을 탐구한다. 루오스 et al. (2024)의 설정을 사용하여, 우리는 복잡한 휴리스틱과 명시적 검색의 조합을 사용하는 가장 강력한 사용 가능한 체스 엔진인 스톡피쉬 16의 액션-값 함수를 인과 변압기로 증류하기 위한 HL-가우스를 평가한다. 증류 데이터 세트는 스톡피시 엔진에 의해 주석이 달린 1,000만 개의 체스 게임으로 구성되어 150억 개의 데이터 포인트(그림 9, 왼쪽)를 산출한다.\n' +
      '\n' +
      '본 연구에서는 HL-Gauss 또는 1-Hot 분류표적을 이용하여 다양한 용량(9M, 137M, 270M 파라미터)의 3가지 변압기 모델을 학습하였다. 우리는 Ruoss et al.(2024)이 1-Hot 타겟들이 이 태스크에서 MSE를 능가한다는 것을 입증함에 따라 MSE를 생략한다. 각 모델의 효과는 리체스에서 10,000개의 체스 퍼즐을 해결하는 능력을 기반으로 평가되며, 알려진 솔루션과 비교하여 생성된 액션 시퀀스의 정확도로 측정되었다. 설정과 결과는 모두 그림 9(오른쪽)에 나와 있다. 루오스 외(2024)의 270M 트랜스포머를 사용한 원-핫 타겟이 탐색 없이 AlphaZero baseline을 능가하는 반면, HL-Gauss는 400 MCTS 시뮬레이션을 통해 실질적으로 더 강한 AlphaZero와의 성능 격차를 좁힌다(Schrittwieser 외, 2020).\n' +
      '\n' +
      '오프라인 데이터를 이용한 일반 로봇 조작\n' +
      '\n' +
      '마지막으로, 교차 엔트로피 손실이 Chebotar 등(2023)의 대규모 비전 기반 로봇 조작 제어 태스크 세트에 대한 성능을 향상시킬 수 있는지 평가한다. 이러한 작업은 카운터탑 표면 앞에 배치된 시뮬레이션된 7-DoF 이동 조작기를 제시한다. 목표는\n' +
      '\n' +
      '도 8: **Rression vs cross-entropy loss for Wordle (Offline RL.**) Comparing HL-Gauss cross-entropy loss with MSE regression loss for a transformer with offline RL on Wordle dataset (Snell et al., 2023). 여기서는 워드 게임(예를 들어, 왼쪽의 이미지)이 부분적으로 플레이되었을 때 단어를 한 번에 추측하는 성공률을 평가한다. HL-Gauss는 행동 규칙화의 다양한 강점에 대해 실질적으로 더 높은 성공률을 유도한다. 자세한 내용은 §4.3.1을 참조하십시오.\n' +
      '\n' +
      '이 매니퓰레이터를 제어하여 산만기 객체, 클러터 및 무작위 초기 포즈가 있는 상태에서 17개의 다른 주방 객체를 성공적으로 파악하고 들어올립니다. 우리는 인간 시연 데이터에 훈련된 행동 복제 정책의 배포 또는 평가 동안 얻은 실패한 자율 수집 롤아웃을 연상시키는 샘플링된 액션 노이즈가 추가된 전문가 시연을 다시 재생함으로써 소량의 인간 원격 조작 시연(\\(500,000\\)(성공 및 실패) 에피소드의 데이터 세트를 생성한다.\n' +
      '\n' +
      '우리는 Chebotar et al.(2023)의 레시피에 따라 60M 매개변수로 Q-Transformer 모델을 훈련하지만 MSE 회귀 손실을 HL-Gauss 분류 손실로 대체한다. 그림 10에서 볼 수 있듯이 HL-가우스는 회귀 기준선에 비해 67% 더 높은 피크 성능을 초래하는 반면 훨씬 더 표본 효율적이며 이전 회귀 기반 접근법의 핵심 한계를 해결한다.\n' +
      '\n' +
      '## 5. 분류 편익이 RL인 이유는 무엇인가?\n' +
      '\n' +
      '실험을 통해 분류 손실이 가치 기반 심층 RL의 성능과 확장성을 크게 향상시킬 수 있음을 보여준다. 이 섹션에서는 분류가 가치 기반 RL에 이점이 있는 이유를 이해하기 위해 통제된 실험을 수행한다. 구체적으로, 우리는 범주형 교차 엔트로피 손실이 표현 학습, 안정성 및 견고성을 포함하는 가치 기반 RL에 특정한 몇 가지 문제를 어떻게 해결할 수 있는지 이해하려고 시도한다. 우리는 또한 다른 범주형 표적에 비해 HL-가우스가 우월한 이유를 밝히기 위해 절제 실험을 수행할 것이다.\n' +
      '\n' +
      '그림 10: 오프라인 데이터를 사용한 **일반주의 로봇 조작: 시뮬레이션된 시각 기반 조작에 대한 (왼쪽) 로봇 플랫폼 및 (오른쪽) HL-가우스 대 MSE. 로봇 조작 문제(§4.3.3)는 Chebotar et al.(2023)로부터의 설정을 사용한다. 왼쪽의 이미지는 이러한 실험에 사용된 7자유도 이동 조작기 로봇을 보여준다. 그림에서 오차 막대는 95% CI를 보여준다. HL-Gauss를 사용하면 더 나은 점으로 훨씬 더 빠른 학습을 할 수 있습니다.**\n' +
      '\n' +
      '그림 9: **검색이 없는 그랜드마스터급 체스 (왼쪽) 체스에서 Q-값 증류를 위한 데이터세트 생성. (오른쪽) 축척 곡선입니다. 루스 등(2024)의 설정에 따라 트랜스포머 모델이 스톡피쉬 16 Q-값에 대한 지도 학습을 통해 체스를 하도록 훈련한 다음 평가를 위해 탐욕스러운 정책을 따른다. 결과에서 볼 수 있듯이 HL-Gauss는 Ruoss et al.(2024)이 사용하는 One-hot 타겟을 능가하며 AlphaZero의 성능과 트리 탐색과 거의 일치함을 알 수 있다.**\n' +
      '\n' +
      '### 절제 연구: 분류 손실의 어떤 구성 요소입니까?\n' +
      '\n' +
      '이 논문에서 제시된 분류 손실은 값 기반 RL에서 사용되는 전통적인 회귀 손실과 두 가지 방식으로 다르다: **(1)** 값 네트워크의 출력을 스칼라 대신 범주형 분포가 되도록 매개변수화하고 **(2)** 스칼라 대상을 범주형 대상으로 변환하는 전략이다. 우리는 이제 교차 엔트로피 손실의 성능에 대한 이러한 단계의 상대적 기여도를 이해할 것이다.\n' +
      '\n' +
      '#### 5.1.1 범주의 표상이 더 공연적인가?\n' +
      '\n' +
      'SS3.1에서 논의한 바와 같이 "소프트맥스" 연산자를 적용하여 범주형 분포의 확률로 변환된 로짓이 출력되도록 Q-네트워크를 매개변수화한다. 소프트맥스를 사용하는 것은 바운디드 Q-값들 및 바운디드 출력 그래디언트들로 이어지며, 이는 아마도 RL 트레이닝 안정성을 향상시킬 수 있다(Hansen et al., 2024). Q값 매개변수화만으로 교차 엔트로피 손실이 필요 없이 성능이 향상되는지 여부를 조사하기 위해 Eq(3.1)와 동일한 매개변수화이지만 MSE로 Q 함수를 훈련한다. 우리는 온라인(그림 11)과 오프라인 RL(그림 12) 모두에서 MSE 손실과 함께 소프트맥스를 사용하여 얻은 이득을 관찰하지 않는다. 이는 교차 엔트로피 손실의 사용이 성능 개선의 대부분을 초래한다는 것을 강조한다.\n' +
      '\n' +
      '#### 5.1.2 왜 일부 교차 엔트로피 손실이 다른 것보다 더 잘 작동하는가?\n' +
      '\n' +
      '우리의 결과는 HL-Gauss가 교차 엔트로피 손실을 사용하는 두 방법에도 불구하고 Two-Hot를 능가한다는 것을 나타낸다. 우리는 HL-Gauss의 이점이 두 가지 이유로부터 기인할 수 있다고 가정한다: 1) HLGauss는 확률 질량을 이웃 위치로 확산시킴으로써 과적합을 감소시키고, 2) HL-Gauss는 회귀 문제에서 서수 구조를 이용하여 특정 범위의 목표 값에 걸쳐 일반화된다. 첫 번째 가설은 레이블 평활화가 분류 문제에서 과적합을 해결하는 방법과 더 일치할 것이다(Szegedy et al., 2016).\n' +
      '\n' +
      '우리는 13개의 아타리 게임의 하위 집합에 걸쳐 온라인 RL 설정에서 이러한 가설을 테스트한다. 이를 위해 범주형 빈의 수를 \\(\\{21,51,101,201\\}\\)으로 변화시키면서 값 범위 \\([v_{\\min},v_{\\max}]\\)을 고정하고, \\(\\{0.25,0.5,0.75,1.0,2.0\\}\\(\\{21,51,101,201\\}\\)에서 편차 \\(\\sigma\\) 대 빈 폭 \\(\\varsigma\\)의 비율을 동시에 변화시켰다. 우리는 HL-Gauss에 대한 광범위한 \\(\\sigma\\) 값이 Two-Hot를 능가하는 것을 발견했으며, 이는 인접 위치로 확산 확률 질량이 과소 적합을 초래할 가능성이 있음을 나타낸다. 흥미롭게도, 우리는 \\(\\sigma\\)의 최적 값이 빈 수와 무관한 것으로 보이기 때문에 두 번째 가설도 작동한다는 것을 알게 되었으며, 이는 HL-가우스가 특정 범위의 목표 값에 걸쳐 가장 잘 일반화되고 실제로 회귀 문제의 순서적 특성을 활용하고 있음을 나타낸다. 따라서, HL-가우스의 이득은 라벨 평활화의 경우라고 믿어지는 바와 같이 완전히 과적합에 기인할 수 없다.\n' +
      '\n' +
      '가치기반 RL에서 분류주소는 무엇인가?\n' +
      '\n' +
      '교차 엔트로피 손실의 성능 이득이 값의 범주형 표현과 분산 목표의 사용 모두에서 비롯된다는 것을 본 우리는 이제 가치 기반 RL 교차 엔트로피 손실에서 어떤 도전을 해결하거나 적어도 부분적으로 완화하는지 이해하려고 시도한다.\n' +
      '\n' +
      '#### 5.2.1 분류가 잡음 표적에 대해 더 강한가?\n' +
      '\n' +
      '분류는 정확한 수치적 관계보다는 입력과 표적 사이의 범주적 관계에 초점을 맞추기 때문에 회귀보다 잡음이 많은 표적에 과적합하는 경향이 적다. 우리는 분류가 RL의 확률에 의해 유도된 노이즈를 더 잘 처리할 수 있는지 여부를 조사한다.\n' +
      '\n' +
      'Noisy Rewards** 보상의 확률성에 대한 분류의 견고성을 테스트하기 위해, 우리는 각 데이터세트 보상\\(r_{t}\\)에 랜덤 노이즈\\(\\epsilon_{t}\\)에서 균일하게 샘플링된 랜덤 노이즈를 추가하는 오프라인 RL 설정을 고려한다. 잡음 스케일\\(\\eta\\in\\{0.1,0.3,1.0\\}\\)을 변화시키고 교차 엔트로피 기반 HL-Gauss의 성능을 MSE 손실과 비교한다. 그림 14와 같이 HL-Gauss의 성능은 잡음 스케일이 증가함에 따라 MSE보다 더 우아하게 저하된다.\n' +
      '\n' +
      '**(b) Dynamics**에서의 확률. 마차도 외(2018)에 이어, 우리의 아타리 실험은 끈적끈적한 동작을 사용하는데, 25% 확률로, 환경은 에이전트의 실행된 동작 대신에 이전의 동작을 다시 실행할 것이고, 그 결과 비결정론적 역학을 초래한다. 여기서, 우리는 결정론적 아타리(60게임)에서 서로 다른 패배를 비교하기 위해 끈적끈적한 행동을 끕니다. 그림 15에서 볼 수 있듯이 교차 엔트로피 기반 HL-가우스는 확률적 역학으로 MSE를 능가하지만 결정론적 역학에서는 비교 가능하게 수행하면서 분포 C51을 능가한다.\n' +
      '\n' +
      '전반적으로 교차 엔트로피 손실의 이점은 부분적으로 확률적 역학 또는 보상이 있는 RL 환경에 고유한 문제인 시끄러운 표적에 덜 과적합하기 때문일 수 있다. 이러한 확률 문제는 실제 구현된 RL 문제에서 역학의 잘못된 지정 또는 행동 지연의 결과로 발생할 수도 있으며, 이는 교차 엔트로피 손실이 이러한 문제에서 우수한 선택임을 의미한다.\n' +
      '\n' +
      '#### 5.2.2 분류는 표현 표현을 더 많이 배우나요?\n' +
      '\n' +
      '평균 제곱 회귀 오차만을 사용하는 것은 값 기반 RL에서 유용한 표현을 생성하지 않으며, 종종 후속 훈련 동안 관찰된 목표 값을 피팅할 수 없는 저용량 표현(Kumar et al., 2021)을 초래한다는 것은 잘 알려져 있다. 스칼라 타겟이 아닌 범주형 분포를 예측하는 것은 더 나은 표현(Zhang et al., 2023)으로 이어질 수 있으며, 이는 가치 학습 과정에서 직면할 수 있는 임의의 정책의 가치 함수를 모델링하는 표현력을 유지한다(Dabney et al., 2021). Lyle et al.(2019)은 C51로부터의 이득이 개선된 표현들에 부분적으로 기인할 수 있음을 보여주었지만, 그것들이 수익들의 분배들을 백업하는 것 또는 교차-엔트로피 손실의 사용으로부터 기인하는지 여부는 알려지지 않았다.\n' +
      '\n' +
      '이 문제를 조사하기 위해, Farebrother et al.(2023)의 프로토콜에 따라, 우리는 200M 프레임 동안 Atari에서 온라인으로 훈련된 값-네트워크로부터 획득된 끝 두 번째 특징 벡터에 대응하는 학습된 표현이 처음부터 정책을 재학습하기 위해 여전히 필요한 정보를 보유하는지 여부를 연구한다. 이를 위해, 우리는 자기 감독 표현이 시각에서 어떻게 평가되는지(He et al., 2020)와 유사한 동결된 표현 위에 단일 선형 층을 갖는 Q-함수를 트레이닝한다(Farebrother et al., 2023). 그림 16과 같이 교차 엔트로피 손실은 선형 프로빙으로 더 나은 성능을 초래한다. 이것은 그들의 학습된 표현들이 처음부터 훈련된 정책의 가치-개선 경로를 지원하는 측면에서 실제로 더 낫다는 것을 나타낸다(Dabney et al., 2021).\n' +
      '\n' +
      '분류가 더 나은 Amidst Non-stationarity를 수행하는가?\n' +
      '\n' +
      '비안정성은 목표 계산이 끊임없이 진화하는 argmax 정책과 값 함수를 포함하기 때문에 값 기반 RL에 내재한다. Bellemare et al.(2017)은 분류가 비정상적 정책에서 학습의 어려움을 완화할 수 있다고 가정했지만 경험적으로 검증하지는 않았다. 여기서는 분류가 회귀 분석보다 목표 비안정성을 더 잘 처리할 수 있는지 여부를 조사한다.\n' +
      '\n' +
      '**Synthetic setup**: 우리는 먼저 Lyle 등에 제시된 CIFAR10에 대한 합성 회귀 태스크를 고려한다.\n' +
      '\n' +
      '도 16: 아타리 상의 선형 프로빙(§5.2.2)을 사용하여 표현들을 평가하는 ** 이 실험은 FareBrother et al.(2023)의 프로토콜을 따른다. 최적성 격차는 인간 수준의 성능과의 거리를 말하며, 낮은 것이 더 좋다. 두 도표 모두에서 HL-가우스가 가장 좋은 점수를 받아 학습된 표현이 다운스트림 작업에 가장 도움이 됨을 나타낸다.**\n' +
      '\n' +
      '(2024) 여기서 회귀 표적은 무작위 초기화된 신경망 \\(f_{\\theta^{-}}\\)을 통해 입력 이미지 \\(x_{i}\\)을 매핑하여 고주파 표적 \\(y_{i}=\\sin(10^{5}\\cdot f_{\\theta^{-}}(x_{i}))+b\\)을 생성하는 것에 해당하며, 여기서 \\(b\\)은 표적의 크기를 제어할 수 있는 일정한 바이어스이다. TD로 값 함수를 학습할 때 예측 대상은 비정상적이며 정책이 개선됨에 따라 시간이 지남에 따라 크기가 증가하는 경우가 많다. 증가하는 바이어스 수열 \\(b\\in\\{0,8,16,24,32\\}\\)에서 손실이 다른 네트워크를 피팅하여 이 설정을 시뮬레이션한다. 부록 B.4의 세부사항을 참조하십시오. 그림 17과 같이 분류 손실은 회귀에 비해 비정상 목표에서 더 높은 가소성을 유지합니다.\n' +
      '\n' +
      '**오프라인 RL:** RL 컨텍스트에서 비정상을 제어하기 위해, 우리는 Kumar 등의 프로토콜에 따라 고정된 데이터-수집 정책의 값을 추정하는 오프라인 SARSA를 실행한다(2022). 벨만 표적(SS2)을 계산하기 위해 다음 상태(S_{t+1}\\)에서 학습된 Q값을 최대화하는 액션을 사용하는 Q-러닝과 달리 SARSA는 오프라인 데이터셋에서 다음 타임스텝(S_{t+1},A_{t+1})에서 관찰된 액션을 사용한다. 그림 18과 같이 MSE 손실에 비해 HL-Gauss의 편익은 오프라인 SARSA 설정에서 대부분 사라지며, 분류의 편익 중 일부가 가치 기반 RL에서 비정상성을 다루는 데서 비롯된다는 증거를 추가한다.\n' +
      '\n' +
      '**를 요약하자면, 우리는 교차 엔트로피 손실의 사용 자체가 가치 기반 RL에서 좋은 성능을 얻기 위해 중심적이라는 것을 발견했으며, 이러한 방법은 특정 문제를 해결하지 않지만, 가치 기반 RL 방법이 비정상을 더 잘 처리하고, 고도로 표현된 표현을 유도하고, 잡음이 많은 목표 값에 대해 견고함을 제공할 수 있도록 한다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '표 회귀(Weiss and Indurkhya, 1995; Torgo and Gama, 1996) 및 컴퓨터 비전(Van Den Oord et al., 2016; Kendall et al., 2017; Rothe et al., 2018; Rogez et al., 2019)의 이전 작업들은 성능을 향상시키기 위해 회귀를 분류로 대체했다. 특히, 이마니와 화이트(2018)는 HL-Gauss 교차 엔트로피 손실을 회귀에 제안하고 RL 이외의 소규모 감독 회귀 작업에 대한 효율성을 보여준다. 우리의 연구는 교차 엔트로피, 특히 HL-가우스로 훈련된 분류 목적이 아타리, 로봇 조작, 체스 및 워들을 포함한 다양한 도메인에서 가치 기반 RL에 대한 효과적인 스케일링을 가능하게 할 수 있음을 처음으로 보여줌으로써 이러한 이전 작업을 보완한다.\n' +
      '\n' +
      'RL의 여러 최첨단 방법은 "ad-hoc" 트릭(Schrittwieser et al., 2020), 희소 보상에 대한 이점을 인용하거나(Hafner et al., 2023), 단순히 민속 지혜에 의존하는 것(Hessel et al., 2021; Hansen et al., 2024)으로 분석 없이 Two-Hot 교차 엔트로피 손실을 사용했다. 그러나 실험에서 Two-Hot는 다른 교차 엔트로피 손실 및 MSE보다 성능이 좋지 않다. 우리는 Two-Hot가 C51과 HL-Gauss와 달리 이웃 클래스에 확률을 효과적으로 분배하지 못하기 때문이라고 생각한다(경험적 조사를 위해 SS5.1.2 참조).\n' +
      '\n' +
      '밀접하게 관련된 것은 범주형 분포 RL에 대한 작업 라인이다. 특히, Achab et al.(2023)은 범주형 1단계 분포 RL의 분석을 제공하며, 이는 이전에 인식되지 않는 이들 두 접근법의 유사성으로 본 명세서에서 논의된 Two-Hot 알고리즘에 정확하게 대응한다. 또한 Bellemare et al.(2017)의 연구는 C51 알고리즘을 개척했으며, 분류로서 프레이밍 RL에 대한 주요 초점이 없었지만, 우리의 연구 결과는 사용된 특정 손실 함수가 반환 분포 자체를 모델링하는 것보다 알고리즘의 성공에 더 중요한 역할을 할 수 있음을 시사한다. 여러 방법은 범주형 분포 RL 손실이 오프라인 가치 기반 RL을 스케일링하는 데 중요하다는 것을 발견했지만(Kumar et al., 2023; Springenberg et al., 2024), 이러한 작업은 이 패러다임의 어떤 구성 요소가 긍정적인 스케일링 경향을 달성하는 데 중요한지 분리하려고 시도하지 않는다. 우리는 또한 이러한 발견들이 교차 엔트로피 목적 또는 범주적 표현의 사용에 직교하는 표준 RL보다 분포 RL이 통계적 이점을 가져온다고 주장하는 최근의 이론적 작업(Wang et al., 2023; Rowland et al., 2023)과 모순되지 않는다는 점에 주목한다.\n' +
      '\n' +
      '이전 작업들은 TD-학습에 의해 학습된 표현들을 특징지어 왔다(Bellemare et al., 2019; Lyle et al., 2021; Le Lan et al., 2022, 2023; Kumar et al., 2021, 2022). 그러나 이러한 이전 작업들은 RL에서 교차 엔트로피 기반 손실들에 의해 학습된 표현들을 분석하는 작업들이 거의 또는 전혀 없이 MSE 손실들에 전적으로 초점을 맞추고 있다. SS5.2.2의 선형 탐색 실험은 교차 엔트로피 손실로 훈련된 가치 네트워크가 회귀보다 더 나은 표현을 학습한다는 것을 보여주면서 이 공백을 채우려고 한다. 이 발견은 이마니와 화이트(2018)가 지도 회귀에서 MSE에 비해 HL-가우스의 표현적 이점을 찾지 못했기 때문에 특히 중요하며, 이는 교차 엔트로피 사용이 특히 TD 기반 학습 방법에 상당한 이점을 가질 수 있음을 나타낸다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 논문에서는 프레임 회귀를 분류로 하고 평균 제곱 오차 대신 범주형 교차 엔트로피(categoryical cross-entropy)를 최소화하는 것이 여러 신경망 아키텍처와 함께 다양한 작업에서 가치 기반 RL 방법의 성능과 확장성에 큰 개선을 가져온다는 것을 보여주었다. 이러한 개선의 원인을 분석했으며 특히 가치 기반 RL에서 더 많은 표현과 노이즈 및 비정상을 더 잘 처리할 수 있는 교차 엔트로피 손실의 능력에서 비롯된다는 것을 발견했다. 교차 엔트로피 손실만으로는 이러한 문제를 완전히 완화하지는 못하지만, 우리의 결과는 이 작은 변화가 가져올 수 있는 실질적인 차이를 보여준다.\n' +
      '\n' +
      '우리는 범주형 교차 엔트로피를 사용한 강력한 결과가 이론과 실제 모두에서 심층 RL에서 향후 알고리즘 설계에 영향을 미친다고 믿는다. 예를 들어, 가치 기반 RL 접근법은 가치 함수가 변압기 아키텍처로 표현될 때 확장 및 조정하기가 더 어려웠으며, 우리의 결과는 분류가 가치 기반 RL의 혁신을 변압기로 변환하기 위한 부드러운 접근법을 제공할 수 있음을 암시한다. 이론적 관점에서 교차 엔트로피의 최적화 역학을 분석하는 것은 개선된 손실 또는 목표 분포 표현을 고안하는 데 도움이 될 수 있다. 마지막으로, 여러 설정을 탐색했지만 사전 훈련, 미세 조정 또는 지속적인 RL과 같은 다른 RL 문제에서 분류 손실의 유효성을 평가하기 위해서는 추가 작업이 필요하다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '이 논문의 초기 버전에 대한 피드백을 제공해 주신 윌 다브니 씨께 감사드립니다. 클레어 라일, 마크 롤랜드, 마크 벨레마어, 맥스 슈워저, 피에루카 도로, 네이트 랜, 할리 윌처, 웨슬리 청, 데일 슈어먼스에게도 유익한 토론에 감사드립니다. 우리는 또한 애니안 루스, 그레고이어 델레탕, 팀 제네바인이 체스 훈련 인프라에 도움을 준 것을 인정하고 싶습니다. 이 연구는 구글 딥마인드의 TPU 리소스에 의해 지원되었으며 저자들은 도이나 프리컵과 조엘 바랄의 지원에 감사한다.\n' +
      '\n' +
      '## Author Contributions\n' +
      '\n' +
      'JF는 프로젝트를 주도하고 히스토그램 기반 방법을 구현했으며 아타리에 대한 모든 단일 작업 온라인 RL 실험, 체스에 대한 Q-증류, 대부분의 분석 실험을 공동으로 제안 및 실행했으며 논문 작성에 크게 기여했다.\n' +
      '\n' +
      'JO와 AAT는 다중 작업 RL 실험을 설정하고 실행했으며 글쓰기를 도왔다. QV는 로봇 조작 실험을 실행했고 YC는 초기 설정을 도왔다. TX는 종이 작성에 도움을 주었고 AI는 토론에 참여했습니다. SL은 로봇 공학 및 워드 실험에 대해 조언하고 피드백을 제공했다. PSC는 SoftMoE 실험을 설정하고 GDM에서 Jesse를 주최하는 것을 도왔다. PSC와 AF는 이 프로젝트를 후원하고 토론에 참여했다.\n' +
      '\n' +
      'AK는 프로젝트를 조언하고, 비정상성과 재현성 학습을 위한 오프라인 RL 분석을 제안했으며, 글쓰기, 수정, 내러티브에 크게 기여했으며, 로봇 공학 및 멀티 게임 스케일링 실험을 설정했다. RA는 연구 방향을 제시하고 프로젝트를 조언하며 논문 작성을 주도하고 오프라인 RL 및 Wordle 실험을 실행했으며 멀티 태스크 스케일링 및 비아타리 실험을 모두 설정하는 데 도움이 되었다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achab et al. (2023) Mastane Achab, Reda Alami, Yasser Abdelaziz Dahou Djilali, Kirill Fedyanin, and Eric Moulines. 일단계 분산 강화 학습. _ CoRR_, abs/2304.14421, 2023.\n' +
      '* Agarwal et al. (2020) Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. 오프라인 강화학습에 대한 낙관적 관점. In _International Conference on Machine Learning (ICML)_, 2020.\n' +
      '* Agarwal 등(2021) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G. Bellemare. 통계적 벼랑 끝에 있는 심층 강화 학습 Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* Taiga et al. (2023) Adrien Ali Taiga, Rishabh Agarwal, Jesse FareBrother, Aaron Courville, and Marc G. Bellemare. 강화학습에서 멀티태스크 프리트레이닝과 일반화에 대한 고찰 _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* Bellemare et al. (2013) Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. 아케이드 학습환경: 일반 에이전트를 위한 평가 플랫폼. _ Journal of Artificial Intelligence Research (JAIR)_, 47:253-279, 2013.\n' +
      '* Bellemare et al. (2017) Mark G. Bellemare, Will Dabney, and Remi Munos. 강화 학습에 대한 분배적 관점 In _International Conference on Machine Learning (ICML)_, 2017.\n' +
      '* Bellemare et al. (2019) Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, and Clare Lyle. 강화 학습을 위한 최적의 표현에 대한 기하학적 관점입니다. _Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* Bellemare et al. (2023) Marc G. Bellemare, Will Dabney, and Mark Rowland. _ 배포 강화 학습_. MIT 프레스, 2023\n' +
      '\n' +
      '* Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL[http://github.com/google/jax](http://github.com/google/jax).\n' +
      '* Castro et al.(2018) Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. 도파민: 심층 강화 학습을 위한 연구 프레임워크. _ CoRR_, abs/1812.06110, 2018.\n' +
      '* Ceron and Castro (2021) Johan Samir Obando Ceron and Pablo Samuel Castro. 무지개 다시 보기: 보다 통찰력 있고 포괄적인 심층 강화 학습 연구를 촉진합니다. In _International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* Chebotar et al. (2023) Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, et al. Q-transformer: scalable offline reinforcement learning via autoregressive q-functions. In _Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* Dabney et al. (2021) Will Dabney, Andre Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G. Bellemare, and David Silver. 가치 향상 경로: 강화 학습을 위한 더 나은 표현을 지향한다. 2021년 인공 지능에 관한 AAAI 회의에서.\n' +
      '* Espeholt et al. (2018) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. 임팔라: 중요도가 가중된 배우-학습자 아키텍처와 함께 확장 가능한 분산 딥-rl. In _International Conference on Machine Learning (ICML)_, 2018.\n' +
      '* FareBrother et al. (2018) Jesse FareBrother, Marlos C. Machado, and Michael Bowling. DQN에서의 일반화와 정규화 CoRR_, abs/1810.00123, 2018.\n' +
      '* FareBrother et al. (2023) Jesse FareBrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin, Pablo Samuel Castro, and Marc G. Bellemare. 프로토-밸류 네트워크: 보조 작업으로 표현 학습을 스케일링합니다. _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy P. Lillicrap. 세계 모델을 통해 다양한 도메인을 마스터합니다. _ CoRR_, abs/2301.04104, 2023.\n' +
      '* Hansen et al. (2024) Nicklas Hansen, Hao Su, and Xiaolong Wang. TD-MPC2: 연속 제어를 위한 확장 가능하고 견고한 세계 모델. _International Conference on Learning Representations (ICLR)_, 2024.\n' +
      '* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 이미지 인식을 위한 딥 잔차 학습. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* He et al. (2020) Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 지도하지 않은 시각적 표현 학습을 위한 운동량 대비. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2020.\n' +
      '* Hessel et al.(2021) Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt, Laurent Sifre, Theophane Weber, David Silver, and Hado van Hasselt. Muesli: 정책 최적화의 개선을 결합합니다. In _International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* Ho et al. (2021) Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. 레티너건: SIM-to-real 전송을 위한 객체 인식 접근법. In _IEEE International Conference on Robotics and Automation (ICRA)_, 2021.\n' +
      '\n' +
      '* 이마니와 화이트(2018) 에산 이마니와 마사 화이트. 분포 손실로 회귀 성능을 개선합니다. In _International Conference on Machine Learning (ICML)_, 2018.\n' +
      '* Imani et al. (2024) Ehsan Imani, Kai Luedemann, Sam Scholnick-Hughes, Esraa Elelimy, and Martha White. 회귀 분석에서 히스토그램 손실을 조사합니다. _ CoRR_, abs/2402.13425, 2024.\n' +
      '* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델의 법칙을 확장합니다. _ CoRR_, abs/2001.08361, 2020.\n' +
      '* Kendall et al. (2017) Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. 딥 스테레오 회귀를 위한 기하학 및 컨텍스트의 종단간 학습. In _IEEE International Conference on Computer Vision (ICCV)_, 2017.\n' +
      '* Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 심층 컨볼루션 신경망을 이용한 Imagenet 분류 Neural Information Processing Systems (NeurIPS)_, 2012.\n' +
      '* Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 오프라인 강화학습을 위한 보수적 q-learning. _ Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* Kumar et al. (2021) Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. 암시적 과소 모수화는 데이터 효율적인 심층 강화 학습을 억제합니다. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* Kumar et al. (2022) Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. Dr3: 가치 기반 심층 강화 학습은 명시적인 정규화가 필요하다. _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* Kumar et al. (2023) Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. 다양한 멀티태스크 데이터에 대한 오프라인 Q-러닝의 규모와 일반화 _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* Lan et al. (2022) Charline Le Lan, Stephen Tu, Adam Oberman, Rishabh Agarwal, and Marc G. Bellemare. 강화학습에서 표상의 일반화에 관한 것이다. _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2022.\n' +
      '* Lan et al. (2023) Charline Le Lan, Stephen Tu, Mark Rowland, Anna Harutyunyan, Rishabh Agarwal, Marc G. Bellemare, and Will Dabney. 강화 학습에서 부트스트랩된 표현 In _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* Lee et al. (2022) Kuang-Huei Lee, Ofir Nachum, Mengjiao (Sherry) Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, and Igor Mordatch. 다중 게임 결정 변압기. _Neural Information Processing Systems(NeurIPS)_에서, 2022.\n' +
      '* Lehnert et al. (2024) Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Search dynamics bootstrapping을 통한 트랜스포머와의 보다 나은 계획 CoRR_, abs/2402.14083, 2024.\n' +
      '* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 오프라인 강화학습: 개방형 문제에 대한 튜토리얼, 리뷰 및 관점 CoRR_, abs/2005.01643, 2020.\n' +
      '* Lyle et al.(2019) Clare Lyle, Marc G. Bellemare, and Pablo Samuel Castro. 기대 강화 학습과 분배 강화 학습의 비교 분석. 2019년 인공 지능에 관한 AAAI 회의에서.\n' +
      '\n' +
      '* Lyle et al. (2021) Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. 보조 과제가 표현 역학에 미치는 영향에 관한 것이다. 2021년 _International Conference on Artificial Intelligence and Statistics (AISTATS)_에서\n' +
      '* Lyle et al. (2022) Clare Lyle, Mark Rowland, and Will Dabney. 강화 학습에서 용량 손실을 이해하고 예방합니다. _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* Lyle et al. (2024) Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan Pascanu, James Martens, and Will Dabney. 신경망에서 가소성 손실의 원인을 분리한다. _ CoRR_, abs/2402.18762, 2024.\n' +
      '* Machado et al. (2018) Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. 아케이드 학습환경 재검토: 일반 에이전트에 대한 평가 프로토콜 및 개방형 문제. _ Journal of Artificial Intelligence Research (JAIR)_, 61:523-562, 2018.\n' +
      '* Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. 피드즐랜드, 게오르크 오스트로브스키, 스티그 피터슨, 찰스 베티, 아미르 사딕, 이오안니스 안토노글루, 헬렌 킹, 다산 쿠마란, 다안 위에르스트라, 셰인 레그, 데미스 하사비스. 심층 강화 학습을 통한 인간 수준의 제어 Nature_, 518(7540):529-533, 2015.\n' +
      '* Mnih et al.(2016) Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 심층 강화 학습을 위한 비동기식 방법. In _International Conference on Machine Learning (ICML)_, 2016.\n' +
      '* Obando-Ceron et al. (2024) Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse FareBrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, and Pablo Samuel Castro. 전문가들의 혼합물은 깊은 rl에 대한 파라미터 스케일링을 해제한다. _ CoRR_, abs/2402.08609, 2024.\n' +
      '* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Kimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 파이토치: 명령적인 스타일, 고성능 딥러닝 라이브러리. _Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* Pintea et al.(2023) Silvia L. 핀티아, 얀콩 린, 주케 다이크스트라, 얀 C. 반 게머트. 분류가 회귀에 도움이 되는 이유를 이해하는 단계입니다. _IEEE International Conference on Computer Vision (ICCV)_에서, 페이지 19972-19981, 2023.\n' +
      '* Puigcerver et al. (2024) Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, and Neil Houlsby. 전문가의 드문드문부터 부드러운 혼합물까지. _International Conference on Learning Representations (ICLR)_, 2024.\n' +
      '* Rogez et al. (2019) Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net++: 자연 영상에서 다인 2d 및 3d 포즈 검출. _ IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)_, 42(5):1146-1161, 2019.\n' +
      '* Rothe et al. (2018) Rasmus Rothe, Radu Timofte, and Luc Van Gool. 얼굴 랜드마크가 없는 단일 이미지에서 실제 및 명백한 연령에 대한 깊은 기대. _ International Journal of Computer Vision (IJCV)_, 126(2-4):144-157, 2018.\n' +
      '* Rowland et al. (2023) Mark Rowland, Yunhao Tang, Clare Lyle, Remi Munos, Marc G. Bellemare, and Will Dabney. 가치 추정을 위한 분위 시간차 학습의 통계적 이점. In _International Conference on Machine Learning (ICML)_, 2023.\n' +
      '\n' +
      '* Ruoss et al. (2024) Anian Ruoss, Gregoire Deletang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, and Tim Genewein. 수색 없는 그랜드마스터급 체스 CoRR_, abs/2402.04494, 2024.\n' +
      '* Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. 학습된 모델로 계획을 세워 아타리, 고, 체스, 소기를 마스터합니다. _ Nature_, 588(7839):604-609, 2020.\n' +
      '*실버 등(2017) David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. 인간의 지식 없이 바둑을 익히는 거죠 Nature_, 550(7676):354-359, 2017.\n' +
      '* Snell et al. (2023) Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. 암시적 언어 q 학습을 통한 자연어 생성을 위한 오프라인 RL. _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* Springenberg et al. (2024) Jost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch, Thomas Lampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, et al. Offline actor-critic reinforcement learning scales to large models. _ CoRR_, abs/2402.05546, 2024.\n' +
      '* Stewart et al. (2023) Lawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. 분류로서의 회귀: 작업 공식이 신경망 특징에 미치는 영향. 2023년 _International Conference on Artificial Intelligence and Statistics (AISTATS)_에서\n' +
      '* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 컴퓨터 비전을 위한 초기 아키텍처를 재고하고 있습니다. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* Torgo and Gama (1996) Luis Torgo and Joao Gama. 분류별 회귀 분석 \'인공지능에 관한 브라질 심포지엄\'에서 51-60쪽, 스프링어, 1996년\n' +
      '* Van Den Oord et al. (2016) Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 픽셀 순환 신경망입니다. In _International Conference on Machine Learning (ICML)_, 2016.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목해 주세요 Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* Wang et al. (2023) Kaiwen Wang, Kevin Zhou, Runzhe Wu, Nathan Kallus, and Wen Sun. 분배의 이점: 강화 학습을 위한 작은 손실 한계. _Neural Information Processing Systems(NeurIPS)_에서, 2023.\n' +
      '* Weiss and Indurkhya (1995) Sholom M Weiss and Nitin Indurkhya. 기능 예측을 위한 규칙 기반 기계 학습 방법. _ Journal of Artificial Intelligence Research (JAIR)_, 3:383-403, 1995.\n' +
      '* Zhang et al. (2023) Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, and Angela Yao. 서수 엔트로피로 심층 회귀를 개선합니다. _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '\n' +
      '## 부록 A 참조 구현\n' +
      '\n' +
      '```\n' +
      'importjax importjax.scipy.special importjnp.sum(probs*centers) returntransform_to_probs(target:jax.Array)->jax.support=jnp.linspace(min_value,max_value,num_bins+1,dtype=jnp.float32) >jax.Array: cdf_evals=jax.scipy.special.erf((support-target)/(jnp.sqrt(2))\n' +
      '```\n' +
      '\n' +
      '목록 1: Jax(Bradbury et al., 2018)에서 HL-Gauss(Imani and White, 2018)의 구현.\n' +
      '\n' +
      '```\n' +
      'IMporttorch importtorch. special importtorch.nnasn importtorch.nnfunctionalasF\n' +
      '`n.HLGaussLoss(nn.Module): def_init_(self,min_value:float,max_value:float,num_bins:int,sigma:float)->torch.Tensor:  center=(self.support[:-1]+self.support[1:torch.Tensor]>torch.Tensor: cdf_evals[...,1]returntorch.sum(probs*value,max_value,num_bins=sigma self.support=torch.linspace(min_value,max_value,num_bins+1,dtype=torch.float32) defforward(self,logits:torch.Tensor: retf_evals(torch.tensor(2.0))*self.sigma))z=cdf_evals[...,1]\n' +
      '\n' +
      '목록 2: PyTorch(Paszke et al., 2019)에서 HL-Gauss(Imani and White, 2018)의 구현.\n' +
      '\n' +
      '## 부록 B 실험방법\n' +
      '\n' +
      '후속 섹션에서는 본 명세서의 각 도메인에 대한 실험 방법론을 요약한다.\n' +
      '\n' +
      '### Atari\n' +
      '\n' +
      '우리의 온라인 및 오프라인 RL 회귀 기준선은 모두 Dopamine(Castro et al., 2018)에서 DQN+Adam의 Jax(Bradbury et al., 2018) 구현을 기반으로 한다. 마찬가지로, 각각의 분류 방법(즉, HL-Gauss 및 Two-Hot)은 Dopamine(Castro et al., 2018)에서 C51의 Jax(Bradbury et al., 2018) 구현에 기초하여 구축되었다. DQN+Adam에 대한 하이퍼파라미터는 C51(표 B.2), Two-Hot(표 B.2) 및 HL-Gauss(표 B.3)에 대한 하이퍼파라미터 차이와 함께 표 B.1에 제공된다. 달리 명시되지 않는 한 논문의 온라인 RL 결과는 게임당 5개의 종자가 있는 60개의 아타리 게임에서 200M 프레임 동안 실행되었다. 오프라인 RL 결과는 게임당 3개의 종자를 가지고 Kumar et al.(2021)의 17개 게임에서 실행되었다. 온라인 및 오프라인 결과 모두에 대한 네트워크 아키텍처는 액션-값을 출력하기 전에 3개의 컨볼루션 계층에 이어 단일 비선형 완전 연결 계층을 사용하는 표준 DQN Nature 아키텍처이다.\n' +
      '\n' +
      '전문가들의 혼합\n' +
      '\n' +
      '모든 실험은 SoftMoE를 사용하여 Obando-Ceron et al.(2024)의 실험 방법론을 재사용하였다. 구체적으로, 우리는 Dopamine(Castro et al., 2018)에서 DQN+Adam의 마지막 두 번째 층을 SoftMoE(Puigcerver et al., 2024) 모듈로 대체한다. MoE 결과는 Impala ResNet 아키텍처(Espeholt et al., 2018)로 실행되었다. 우리는 Obando-Ceron et al.(2024)로부터 동일한 20개의 게임 세트를 재사용하고 각 구성을 게임당 5개의 씨드에 대해 실행한다. 모든 분류 방법은 C51의 경우 표 B.2 및 HL-가우스의 경우 2-Hot 또는 표 B.3의 매개변수를 재사용했다.\n' +
      '\n' +
      '1.2 멀티태스크와 멀티게임\n' +
      '\n' +
      '다중 작업 및 다중 게임 결과는 각각 Ali Taiga et al. (2023) 및 Kumar et al. (2023)에 설명된 방법론을 정확하게 따른다. 표 B.3에 요약된 HL-Gauss에 대한 하이퍼파라미터를 재사용하고, 다중 작업 결과를 위해 각 에이전트는 게임당 5개의 씨앗에 대해 실행된다. 다중 게임 설정의 엄청난 계산으로 인해 단일 시드에 대해 각 구성을 실행합니다.\n' +
      '\n' +
      '### Chess\n' +
      '\n' +
      '우리는 루오스 등(2024)의 설정을 정확히 따르며, 유일한 차이점은 평활비 \\(\\sigma/\\varsigma=0.75\\)의 HL-Gauss를 사용하는 것이다. 특히, 우리는 스톡피쉬가 생성한 행동 값을 취하고 HL-가우스를 사용하여 범주형 분포를 투영한다. Ruoss et al.(2024)이 이미 분류를 수행함에 따라 범주형 분포의 매개변수를 재사용하고 있는데, 이 매개변수는 \\(m=128\\)개의 빈을 범위 \\([0,1]\\)으로 균등하게 나눈다. 각 매개변수 구성에 대해 단일 에이전트를 훈련하고 평가 퍼즐 정확도를 보고한다. 원-핫 및 AlphaZero w/MCTS에 대한 노즐 정확도 번호는 Ruoss 등(2024, 표 6)으로부터 직접 취하였다.\n' +
      '\n' +
      '로봇 조작 실험\n' +
      '\n' +
      '우리는 그림 10(왼쪽)에서 시각화된 7 자유도의 모바일 조작기 로봇에 대한 대규모 비전 기반 로봇 조작 설정을 연구한다. 테이블탑 로봇 조작 도메인은 조리대 위에 산란된 다양한 무작위 객체가 있는 테이블탑으로 구성된다. RetinaGAN은 (Ho et al., 2021)의 방법에 따라 시뮬레이션 이미지를 실제 이미지 분포에 더 가깝게 변환하기 위해 적용된다. 우리는 (Chebotar et al., 2023)의 절차에 따라 Q-Transformer 정책을 구현한다. 구체적으로 행동 차원당 \\(Q\\)값을 학습하여 자기회귀 \\(Q\\)-학습을 통합하고, 차선의 데이터로부터 효과적으로 학습하기 위해 보수적 정규화를 통합하며, 몬테카를로 수익률을 활용한다.\n' +
      '\n' +
      '### 회귀 목표 규모 및 소성 손실\n' +
      '\n' +
      '크기가 증가하는 비정상 목표를 학습할 때 분류 손실이 더 강력한지 평가하기 위해 우리는 Lyle 등(2024)의 합성 설정을 활용한다. 구체적으로, CIFAR 10개의 영상(x_{i}\\)을 입력으로 하여 스칼라 예측( \\(f_{\\theta}:\\mathbb{R}^{32\\times 32\\times 3}\\rightarrow\\mathbb{R}\\)을 출력하는 합성곱 신경망을 학습한다. 상기 회귀 타겟을 맞추는 것은,\n' +
      '\n' +
      '\\[y_{i}=\\sin(mf_{\\theta^{-}}(x_{i}))+b\\]\n' +
      '\n' +
      '여기서 \\(m=10^{5}\\), \\(\\theta^{-}\\)은 동일한 컨볼루션 아키텍처에 대해 랜덤하게 샘플링된 타겟 파라미터들의 집합이고, \\(b\\)은 예측 타겟들의 크기를 변화시키는 바이어스이다. 증가하는 \\(b\\)이 더 어려운 회귀 작업을 초래해서는 안 된다는 것은 분명하다.\n' +
      '\n' +
      'TD 방법으로 값 함수를 학습할 때 회귀 대상은 비정상적이며 크기가 증가하기를 희망한다(개선 정책에 해당). 이 설정을 시뮬레이션하기 위해 증가하는 수열(b\\in\\{0,8,16,24,32\\}\\)에 네트워크(f_{\\theta}\\)를 맞추는 것을 고려한다. 각 값에 대해 새로운 목표 파라미터 집합 \\(\\theta^{-}\\)을 샘플링하고, \\(10^{-3}\\)의 학습률을 사용하여 Adam 최적화기를 사용하여 배치크기가 \\(512\\)인 \\(5,000\\) 구배 단계에 대해 \\(y_{i}\\)으로 회귀한다.\n' +
      '\n' +
      '2-Hot, HL-Gauss, L2 회귀의 세 가지 방법에 대해 훈련 전반에 걸쳐 평균 제곱 오차(Mean-Squared Error, MSE)를 평가한다. Two-Hot와 HL-Gauss 모두 \\([-40,40]\\)의 지지대와 \\(101\\)의 빈을 사용한다. 그림 17은 각 방법에 대해 평균 \\(30\\) 종자에 대한 훈련 전반에 걸친 MSE를 보여준다. L2 회귀로 훈련된 네트워크는 실제로 Lyle 등(2024)과 일치하여 증가하는 크기의 표적에 빠르게 맞는 능력을 느슨하게 한다는 것을 알 수 있다. 반면에, 분류 방법은 목표 크기\\(b\\)에 관계없이 더 견고하고 동일한 MSE로 수렴하는 경향이 있다. 또한, 우리는 HL-Gauss가 이전 연구 결과와 일치하는 Two-Hot를 능가한다는 것을 알 수 있다. 이러한 결과는 분류가 회귀를 능가하는 이유 중 하나가 비정상 목표에서 네트워크가 더 "플라스틱"이기 때문일 수 있다는 몇 가지 증거를 제공하는 데 도움이 된다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:26]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:27]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>