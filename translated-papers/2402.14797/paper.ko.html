<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 스냅 비디오: 텍스트-비디오 합성을 위한 스케일링된 시공간 트랜스포머\n' +
      '\n' +
      'Willi Menapace1,2,*Aliaksandr Siarohin1 Ivan Skorokhodov1 Ekaterina Deyneka1 Tsai-Shien Chen1,3,*Anil Kag1 Yuwei Fang1 Aleksei Stoliar1 Elisa Ricci2,4 Jian Ren1 Sergey Tulyakov1\n' +
      '\n' +
      '스냅(주) 트렌토2 UC 메르세드3 폰다지온 브루노 케슬러4\n' +
      '\n' +
      'snap-research.github.io/snapvideo\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이미지를 생성하기 위한 현대 모델은 놀라운 품질과 다양성을 보여준다. 이러한 장점으로 인해 연구 커뮤니티는 비디오를 생성하기 위해 이를 용도 변경한다. 비디오 콘텐츠는 매우 중복적이기 때문에 비디오 생성 도메인에 이미지 모델의 발전을 순진하게 가져오면 모션 충실도가 감소하고 시각적 품질이 저하되며 확장성이 손상된다고 주장한다. 이 작업에서는 이러한 문제를 체계적으로 해결하는 비디오 우선 모델인 스냅 비디오를 구축한다. 이를 위해 먼저 EDM 프레임워크를 확장하여 공간적, 시간적 중복 픽셀을 고려하고 자연스럽게 비디오 생성을 지원한다. 둘째, U-Net(이미지 생성 뒤에 있는 워크호스)이 비디오를 생성할 때 스케일이 좋지 않아 상당한 계산 오버헤드가 필요하다는 것을 보여준다. 따라서 본 논문에서는 U-Nets에 비해 3.31배 빠르게 학습하고, 추론 시 \\(\\sim\\)4.5 더 빠르게 학습하는 새로운 트랜스포머 기반 구조를 제안한다. 이를 통해 우리는 처음으로 수십억 개의 파라미터를 갖는 텍스트-비디오 모델을 효율적으로 트레이닝하고, 다수의 벤치마크에서 최신의 결과에 도달하며, 실질적으로 더 높은 품질, 시간적 일관성 및 모션 복잡성을 갖는 비디오를 생성할 수 있다. 사용자 연구에 따르면 우리의 모델은 가장 최근의 방법보다 큰 차이로 선호되었다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '시각적 콘텐츠를 만들고 공유하는 것은 사람들이 디지털 세계에서 자신을 표현하는 핵심 방법 중 하나이다. 과거 전문가에게만 접근할 수 있는, 놀라운 품질과 사실성을 가진 [30, 40, 43, 69] 이미지를 만들고 [6, 37, 42] 이미지를 편집할 수 있는 능력은 큰 텍스트 대 이미지 모델과 그 변형의 출현으로 모두에게 잠금 해제되었다.\n' +
      '\n' +
      '이러한 진전에 힘입어 대규모 텍스트-비디오 모델[4, 13, 21, 48, 62]도 빠르게 발전하고 있다. 현재의 대규모 확산 기반 비디오 생성 프레임워크는 그들의 이미지 대응물에 강하게 뿌리를 두고 있다[4, 13]. U-Nets[41]과 같은 통합 이미지 생성 아키텍처의 가용성은 공개 이미지 사전 훈련 모델[40]을 사용하여 시간적 종속성을 포착하기 위해 Ad-hoc 레이어의 삽입에 초점을 맞춘 주요 아키텍처 수정으로 대규모 비디오 생성기를 구축할 수 있는 논리적 토대가 되었다[4, 13, 21, 48, 62]. 유사하게, 트레이닝은 이미지-기반 확산 프레임워크 하에서 수행되고, 모델은 결과들의 다양성을 개선하기 위해 비디오들 및 별개의 이미지 세트 모두에 적용된다[13, 21, 22, 48].\n' +
      '\n' +
      '우리는 이러한 접근법이 이 작업에서 체계적으로 다루는 여러 측면에서 차선책이라고 주장한다. 먼저, 이미지 및 비디오 모달리티들은 연속적인 비디오 프레임들에서 콘텐츠의 유사성에 의해 주어지는 본질적인 차이들을 제시한다[7, 13]. 유추에 의해, 이미지 및 비디오 압축 알고리즘들은 엄청나게 상이한 접근법들에 기초한다[33]. 이 문제를 해결하기 위해 고해상도 비디오에 초점을 맞춰 EDM[25] 프레임워크를 다시 작성한다. 비디오가 이미지의 시퀀스로 처리된 과거 작업과 달리, 우리는 순수 이미지 기반 훈련 내에서 시간적 차원의 부재로 인해 도입된 모달리티 불일치를 피하기 위해 이미지를 _high frame-rate video_로 처리하여 공동 비디오-이미지 훈련을 수행한다. 둘째, 각 비디오 프레임을 완전히 처리하기 위해서는 널리 채택된 U-Net[41] 아키텍처가 필요하다. 이것은 순수 텍스트 대 이미지 모델에 비해 계산 오버헤드를 증가시켜 모델 확장성에 매우 실용적인 한계를 제기한다. 후자는 높은 품질의 결과를 얻는 데 중요한 요소이다[13, 21]. 공간 및 시간 차원을 자연스럽게 지원하기 위해 U-Net 기반 아키텍처를 확장하려면 엄청난 계산 요구 사항이 있는 볼륨 주의 연산이 필요하다. 그렇게 할 수 없는 것은 출력에 영향을 주어, 일관성 있고 다양한 동작을 갖는 비디오 대신에 _dynamic image_ 또는 모션 아티팩트가 생성된다.\n' +
      '\n' +
      '본 논문에서 제안하는 압축 비유를 통해 프레임 간의 반복을 활용하고, 공간적 차원과 시간적 차원을 하나의 압축된 1차원 잠재 벡터로 취급하는 확장 가능한 트랜스포머 구조를 소개한다. 이 고도로 압축된 표현은 우리가 공동으로 시공간 계산을 수행할 수 있게 하고 복잡한 움직임의 모델링을 가능하게 한다. 우리의 아키텍처는 FIT[8]에서 영감을 받아 처음으로 수십억 개의 매개변수로 확장됩니다. 본 논문에서 제안한 모델은 U-Nets에 비해 학습시간(3.31\\times\\)과 추론시간(4.49\\times\\)을 크게 감소시키면서 높은 생성품질을 얻을 수 있다.\n' +
      '\n' +
      '우리는 널리 채택된 UCF101 [55] 및 MSR-VTT [65] 데이터 세트에서 스냅 비디오를 평가한다. 제너레이터는 생성된 모션의 품질과 관련하여 벤치마크 범위에 걸쳐 최첨단 성능을 보여줍니다. 가장 흥미롭게도, 우리는 가장 최근의 오픈 소스 및 클로즈 소스 방법에 대해 여러 사용자 연구를 수행했으며 연구 참가자에 따르면 우리 모델은 피카[1] 및 플로어33[17]보다 훨씬 나은 반면 Gen-2[11]에 필적하는 사실주의를 특징으로 한다는 것을 발견했다. 가장 흥분스럽게도, 사용자 연구 참가자의 선호도는 텍스트 정렬과 동작 품질을 평가할 때 스냅 비디오를 큰 여백으로 선호했다. 즉시 비디오 정렬에 대한 Gen-2[11]에 비해 본 모델은 81%(Pika[1]에 대해 80%, Floor33[17]에 대해 81%)에서 선호되었으며, 대부분의 움직임 양이 있는 대부분의 동적 비디오를 생성했으며(Gen2[11]에 대해 96%, Pika[1]에 대해 89%, Floor33[17]에 대해 88%) 가장 좋은 움직임 품질을 보였다(Gen-2[11]에 대해 79%, Pika[1]에 대해 71%, Floor33[17]에 대해 79%).\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**비디오 생성** 비디오 생성은 어렵고 오랫동안 연구된 작업입니다. 복잡성으로 인해 많은 작업이 좁은 도메인[5, 9, 12, 28, 35, 44, 47, 49, 56, 58, 59, 66, 70, 71]을 모델링하고 적대적 훈련[5, 9, 28, 44, 47, 49, 58, 59, 71] 또는 자기회귀 생성 기술[12, 35, 56, 66, 70]을 채택한다. 좁은 도메인 제한을 해결하기 위해 텍스트 대 비디오 생성 작업이 [34] 제안되었으며 두 자동 회귀 모델 [23, 34, 61, 63, 64] 및 GAN [29]가 모두 나타났다.\n' +
      '\n' +
      '텍스트-이미지 생성[3, 40, 43]의 맥락에서 확산 모델의 최근 성공은 과제 [2, 4, 13, 16, 17, 21, 22, 32, 48, 62, 67, 72]에서 엄청난 발전을 촉진했다. ImagenVideo[21] 및 Make-A-Video[48]은 비디오들을 생성하고 이미지 및 비디오 데이터세트들에 대해 그들의 모델들을 공동으로 트레이닝하기 위해 시간 및 공간 업샘플러들의 깊은 캐스케이드를 제안한다. PYoCo[13]은 비디오 프레임들 사이의 유사성을 캡처하기 위해 상관 잡음 모델을 도입한다. 비디오 LDM[4]는 미리 훈련된 잠재 이미지 생성기와 잠재 디코더를 미세 조정하여 시간적으로 일관된 비디오를 생성하는 잠재 확산 패러다임을 채택한다. AnimateDiff[16]는 미리 학습된 잠재 영상 생성기를 얼리고 새로 삽입된 모션 모델링 모듈만을 학습시킨다. 이러한 작업들은 분리 가능한 공간 및 시간 계산이 가능한 U-Nets를 채용하여 모션 모델링 능력에 한계를 가지고 있다. 비디오 팩토리[62]는 3D 창을 따라 공간 및 시간 양식 간의 상호 작용을 개선하는 스왑된 시공간 교차 주의를 제안함으로써 이 패러다임을 개선한다.\n' +
      '\n' +
      'U-Net[41] 아키텍처를 비디오 생성 태스크에 적용한 이 작업들의 코퍼스와는 달리, 트랜스포머 기반 FIT[8] 아키텍처를 사용하면 학습 가능한 압축 비디오 표현 덕분에 상당한 훈련 시간 절약, 확장성 향상 및 성능이 증가한다는 것을 보여준다. 특히, 압축된 비디오 표현에 의해 가능하게 된 글로벌 조인트 시공간 모델링 전략이 시간적 일관성과 모션 모델링 능력에서 상당한 개선을 가져온다는 것을 보여준다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '**고해상도 생성**고해상도 출력의 생성을 가능하게 하기 위해 상이한 접근법들이 제안되었다. 캐스케이드 확산 모델[3, 13, 21, 43, 48]은 이전 단계의 결과를 연속적으로 업샘플링하도록 설계된 독립 확산 모델 세트를 채택한다. 잠재 확산 모델[2, 4, 17, 40, 72]은 미리 훈련된 오토인코더를 사용하여 입력을 저차원 잠재 벡터 집합으로 인코딩하고 이 잠재 표현에 대한 확산 모델을 학습한다.\n' +
      '\n' +
      '다른 방법의 계열은 모델의 캐스케이드 또는 잠재 확산을 사용하지 않고 고해상도 출력을 종단 간 생성한다. 단순 확산[24]과 _Chen_[7]은 확산 과정의 잡음 스케줄에 적응하여 고해상도의 영상을 직접 생성한다. f-DM[14] 및 RDM[57]은 상이한 해상도들 사이에서 매끄럽게 전이되는 확산 프로세스를 설계한다. MDM[15]은 단일 모델이 점진적으로 더 높은 해상도로 입력을 동시에 잡음 제거하도록 훈련되는 전략을 제안한다.\n' +
      '\n' +
      '본 연구에서는 두 가지 고려 사항 중 (i) 잠재 오토인코더에 의해 도입될 수 있는 고주파 디테일의 깜박임 형태의 시간적 불일치를 회피하는 2단계 캐스케이드 모델을 채택한다[4], (ii) 모션 모델링과 장면 구조에 초점을 맞춘 저해상도 모델과 고주파 디테일에 초점을 맞춘 고해상도 모델의 두 가지 특수 모델을 만들어 종단간 모델에 대한 모델 용량을 증가시킨다.\n' +
      '\n' +
      '**확산 프레임워크**확산 생성 모델은 한 쌍의 프로세스로서 생성을 모델링하는 기술들의 집합으로서, 샘플을 노이즈로 점진적으로 파괴하는 순방향 프로세스와 샘플의 점진적인 잡음 제거로서 역방향 프로세스 모델링 생성이다. 확산 모델의 다양한 공식이 문헌에 제시되었다. 디노이징 확산 확률 모델(DDPM) [20, 50]은 전방 및 후방 프로세스를 마르코프 체인으로 공식화한다. 점수 기반 생성 모델(SGMs, Score-based Generative Models) [51, 52]는 증가하는 노이즈 레벨, _i.e_에 의해 교란된 일련의 데이터 분포의 확률 밀도 함수의 점수를 모델링한다. 데이터 로그 확률 밀도 함수의 최대 증가 방향입니다. 작업의 길[53, 54]은 확률 미분 방정식(SDE)을 통해 DDPM과 SGM을 무한 노이즈 수준으로 일반화한다. 이 작업에서는 고해상도 비디오 생성을 위해 재구성하는 EDM[25]의 SGM 프레임워크를 채택한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '고차원 입력을 위한 EDM[25] 확산 프레임워크를 재작성하여 고해상도 비디오를 생성하고, 수십억 개의 파라미터와 수만 개의 입력 패치로 확장하는 FITs[8] 기반의 효율적인 트랜스포머 구조를 제안한다. Sec. 3.1은 EDM 프레임워크에 대한 소개를 제공하고 Sec. 3.2는 확산 프레임워크를 고차원 입력에 적용하는 문제를 강조하고 재방문된 EDM 기반 확산 프레임워크를 제안한다. Sec. 3.3에서는 관절 훈련을 위한 영상과 영상 양식의 간격을 줄이는 방법을 제안한다. 마지막으로 Sec. 3.4는 확장 가능한 비디오 생성 아키텍처를 설명하고 Sec. 3.5와 Sec. 3.6은 각각 훈련 및 추론 절차를 설명한다.\n' +
      '\n' +
      '### EDM 도입\n' +
      '\n' +
      '확산 모델은 이미지와 비디오 생성에서 놀라운 성공을 거두었다. 제안된 프레임워크 중 _Karras et al_. [25] 공통 확산 프레임워크의 통일된 관점을 제공하고 EDM을 공식화한다. EDM은 분산-폭발 순방향 확산 프로세스 \\(p(\\mathbf{x}_{\\mathbf{\\sigma}|\\mathbf{x})\\sim\\mathcal{N}(\\mathbf{x},\\mathbf{\\sigma}^{2}\\mathbf{I})\\)를 정의하며, 여기서 \\(\\mathbf{\\sigma}\\in[\\mathbf{\\sigma}_{\\text{min},\\mathbf{\\sigma}_{\\text{max}]\\)는 적용된 잡음의 표준 편차와 일치하는 확산 타임스텝을 나타내며, \\(\\mathbf{x}_{\\mathbf{\\sigma}}\\sigma}\\sigma}는 현재 잡음 레벨에서의 데이터를 나타낸다. 디노이저 함수\\(\\mathcal{D}_{\\theta}\\)는 디노이징 목적을 사용하여 역과정을 모델링하기 위해 학습된다:\n' +
      '\n' +
      '\\mathcal{L}(\\mathcal{D}_{\\theta})=\\mathbbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}\\Big{[}\\lambda(\\mathbf{\\sigma});\\big{\\|}\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma})-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{},\\tag{1}\\cambda(\\mathbf{\\sigma}}\\cambda(\\mathbf{\\sigma}}}\\cambda(\\mathbf{x}_{\\sigma}}}\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{},\\tag{1}\\cambda(\\mathbf{x}\\cambda(\\mathbf{\\sigma}}}\\cambda(\\mathbf\n' +
      '\n' +
      '여기서 \\(\\lambda\\)는 손실 가중 함수이고, \\(\\mathbf{x}\\sim p_{\\text{data}\\)는 데이터 샘플이고, \\(\\mathbf{\\epsilon}\\)는 가우시안 잡음이고, \\(\\mathbf{\\sigma}\\sim p_{\\text{train}\\)는 트레이닝 분포로부터 샘플링된다. \\\\ (\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma})\\)로 정의된다:\n' +
      '\n' +
      '\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}))=c_{\\text{out}(\\mathbf{\\sigma})\\mathcal{F}_{\\theta}\\left(c_{\\text{in}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}\\right)+c_{\\text{skip}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}, \\tag{2}\\mathbf{x}_{\\mathbf{\\sigma}}, \\tag{2}\\mathbf{x}\\text{sigma}\\mathbf{x}\\text{sigma}\\mathbf{x}\\text{sigma}, \\tag{2}\\mathbf{x}\\text{sigma}\\mathbf{x}\\text{sigma}\\text{sigma}\\mathbf{x}\\text{sigma}\n' +
      '\n' +
      '여기서 \\(\\mathcal{F}_{\\theta}\\)는 신경망이고, \\(c_{\\text{out}\\), \\(c_text{skip}}\\) 및 \\(c_{\\text{in}\\)은 스케일링 함수를 나타낸다. 특히, 잡음제거 목적 \\(\\mathcal{L}(\\mathcal{F}_{\\theta})\\)는 다음과 같이 \\(\\mathcal{F}_{\\theta}\\)으로 등가적으로 표현될 수 있다.\n' +
      '\n' +
      '\\mathcal{L}(\\mathcal{F}_{\\theta})=\\mathbbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}\\Big{[}w(\\mathbf{\\sigma};\\big{\\|}\\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})}\\mathbf{x}_{\\mathbf{\\sigma})-c_{\\text{mm}(\\mathbf{\\sigma}}\\big{\\|}_{2}^{2}\\Big{},\\tag{3}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma}\\big{\\sigma\n' +
      '\n' +
      '여기서 \\(\\mathcal{F}_{\\text{tgt}}\\)는 훈련 대상을 나타내고, \\(c_{\\text{mm}}\\)는 정규화 인자이며, \\(w\\)는 가중 함수이다. Appx에서 파생된 이러한 양식입니다. D는 탭 1에 나와 있습니다.\n' +
      '\n' +
      '확산 과정을 역전시키기 위한 2차 Runge-Kutta 샘플러는 가우시안 잡음(\\mathbf{x}_{\\mathbf{\\sigma}_{\\text{max}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\sigma}_{\\text{max}^{2}\\mathbf{I})에서 출발하여 샘플\\(\\mathbf{x}\\)을 생성한다.\n' +
      '\n' +
      '고해상도 비디오 생성을 위한### EDM\n' +
      '\n' +
      'EDM은 원래 이미지 생성 프레임워크로 제안되었으며 그 파라미터는 \\(64\\times 64\\)px 이미지 생성에 최적화된다. 공간 해상도의 변경 또는 프레임 간의 공유 콘텐츠를 갖는 비디오의 도입은 잡음 제거 네트워크가 더 높은 신호 대 잡음 비(\\(SNR\\))로 원래의 해상도에서 노이즈 프레임을 사소하게 복구할 수 있게 하는데, 이는 원래의 프레임워크가 더 낮은 잡음 레벨에서 볼 수 있도록 설계되었다. 이유를 알아보기 위해 잡음 비디오\\(\\mathbf{x}_{\\mathbf{\\sigma}\\in\\mathbbb{R}^{T\\times s\\cdot H\\times s\\cdot W\\sim\\mathcal{N}(\\mathbf{x},\\mathbf{\\sigma}^{2}\\mathbf{I})를 고려하며, 여기서 \\(T\\)은 프레임 수이고 \\(s\\)은 업샘플링 인자이다. 픽셀의 각 블록에서 평균 값을 구하여 원래의 해상도\\(\\mathbf{\\tilde{x},\\mathbf{\\tilde{x}}_{\\mathbf{\\sigma}\\in\\mathbbb{R}^{1\\times H\\times W}\\)에서 깨끗한 프레임과 잡음 프레임을 생성한다. 평균화의 결과로, 상기 잡음은\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '공동 시공간 모델링에 대한 가능성을 유사하게 제한한다[62]. 우리는 공간 및 시간 모델링을 분리 가능한 방법으로 처리하는[4, 13, 21, 48]이 움직임이 생생한 비디오가 아닌 움직임 아티팩트, 시간적 불일치 또는 _dynamic image_의 생성을 야기한다고 주장한다. 그러나 비디오 프레임은 압축에 적합한 공간적 및 시간적 중복 콘텐츠를 포함한다[33]. 우리는 압축된 비디오 표현에 대해 학습하고 작동하고 공간 및 시간 차원을 공동으로 모델링하는 것이 고품질 비디오 생성에 필요한 확장성 및 모션 모델링 기능을 달성하기 위해 필요한 단계라고 주장한다.\n' +
      '\n' +
      'FITs[8]은 최근 고해상도 영상 합성 및 영상 생성을 위해 제안된 효율적인 트랜스포머 기반 아키텍처이다. 그들의 주요 아이디어는 그림 1에 요약되어 있다. 도 3은 학습 가능한 잠재 토큰들의 세트를 통해 그들의 입력의 압축된 표현을 학습하고, 이 학습 가능한 잠재 공간에 계산을 집중함으로써, 입력 차원성이 거의 성능 페널티 없이 성장할 수 있게 하는 것이다. 먼저, FIT는 입력의 패치화를 수행하고 나중에 그룹으로 분할된 패치 토큰의 시퀀스를 생성한다. 이어서, 잠재 토큰들의 세트가 인스턴스화되고 계산 블록들의 시퀀스가 적용된다. 각 블록은 먼저 잠재 토큰들과 확산 타임스테프와 같은 컨디셔닝 신호들 사이의 교차 어텐션 "읽기" 동작을 수행한 다음, 패치 정보를 압축하기 위해 대응하는 그룹들의 잠재 토큰과 패치 토큰들 사이의 추가적인 그룹별 "읽기" 교차 어텐션 동작을 수행하고, 잠재 토큰들에 일련의 자기 어텐션 동작들을 적용하고, 패치 토큰들을 업데이트하기 위해 잠재 토큰들 내의 정보를 압축해제하는 그룹별 "쓰기" 교차 어텐션 동작을 수행한다. 마지막으로, 패치 토큰들은 출력을 형성하기 위해 픽셀 공간으로 다시 투영된다. 자기 컨디셔닝은 이전의 샘플링 단계들에서 계산된 압축된 비디오 표현을 보존하기 위해 잠재 토큰들의 세트 상에 적용된다.\n' +
      '\n' +
      '유망하지만 이러한 아키텍처는 아직 최첨단 U-Net 기반 비디오 생성기의 10억 매개 변수 크기로 확장되지 않았으며 고해상도 비디오 생성에도 적용되지 않았다. 이하에서는 이러한 목표를 달성하기 위해 필요한 건축적 고려 사항을 강조한다. 시간 모델링은 고품질 비디오 생성기의 근본적인 측면이다. FITs는 공간 차원 및 시간 차원 모두에 걸쳐 있는 크기\\(T_{p}\\times H_{p}\\times W_{p}\\)의 3차원 패치를 고려하여 패치 토큰을 생성한다. 시간 모델링 성능을 제한하기 위해 \\(T_{p}>1\\)의 값을 찾으므로 공간 차원에만 걸쳐 있는 패치를 고려한다. 또한, 패치와 유사하게 FITs는 시간 차원 및 공간 차원 모두에 걸쳐 있는 그룹으로 패치 토큰을 그룹화하고 그룹별로 크로스 어텐션 연산을 수행한다. 각 그룹의 시간적 크기는 최적의 시간적 모델링을 위해 각 그룹이 모든 \\(T\\) 비디오 프레임을 커버하도록 구성되어야 한다는 것을 관찰한다. 또한, 비디오는 시간적 차원의 존재로 인해 이미지에 대한 정보를 더 많이 포함하므로, 공동 시공간 연산이 수행되는 압축 공간의 크기를 나타내는 잠재 토큰의 수를 증가시킨다. 마지막으로, FIT는 동일한 그룹에 해당하는 패치 토큰에 대해 셀프 어텐션 연산을 수행하는 로컬 계층을 사용한다. 우리는 이 연산이 많은 양의 패치 토큰(최대 해상도의 경우 147.456)에 대해 계산적으로 비싸다는 것을 발견하고 각각의 교차 주의 "읽기" 또는 "쓰기" 연산 후에 피드 포워드 모듈로 대체한다.\n' +
      '\n' +
      '그림 3: (a-left) U-Net 기반 텍스트 대 이미지 아키텍처는 공간 레이어로 순차적으로 적용된 시간 레이어를 삽입하여 분리 가능한 시공간 블록을 생성함으로써 비디오 생성을 수행하도록 적응된다. 공간 계산은 각 프레임에 대해 독립적으로 반복되어 확장성을 제한한다. (a-right) 우리의 확장성 있는 트랜스포머 기반 모델은 향상된 모션 모델링 및 확장성을 위해 학습 가능한 압축 비디오 표현에 대해 공간 및 시간 계산을 공동으로 수행한다. (b) 제안된 스냅 비디오 FIT 아키텍쳐. 잡음 입력 비디오\\(\\mathbf{x_{\\sigma}\\)이 주어졌을 때, 모델은 FIT 블록을 반복 적용하여 잡음 제거된 비디오\\(\\mathbf{\\hat{x}_{\\sigma}\\)을 추정한다. 각각의 블록은 패치 토큰들로부터 계산이 수행되는 작은 잠재 토큰들의 집합으로 정보를 판독한다. 결과는 패치 토큰에 기록됩니다. 추가적인 읽기 동작을 통해 텍스트 임베딩, 노이즈 레벨\\(\\sigma\\), 프레임 레이트\\(\\nu\\) 및 해상도\\(r\\) 형태의 컨디셔닝 정보를 제공한다.\n' +
      '\n' +
      '우리의 모델은 생성 프로세스를 제어하기 위해 일련의 컨디셔닝 토큰으로 표현되는 컨디셔닝 정보를 사용한다. 본 논문에서는 텍스트 컨디셔닝을 가능하게 하기 위해 현재 (\\boldsymbol{\\sigma}\\)를 나타내는 토큰 외에 입력 텍스트로부터 텍스트 임베딩을 추출하는 T5-11B [39] 텍스트 인코더를 소개한다. 가변 비디오 프레이머레이트 및 트레이닝 데이터의 해상도 및 종횡비의 큰 차이를 지원하기 위해, 현재 입력의 프레이머레이트 및 원래 해상도를 나타내는 추가 토큰을 연결한다.\n' +
      '\n' +
      '고해상도 출력을 생성하기 위해 1단계 모델(36\\times 64\\)px 비디오 생성과 2단계 업샘플링 모델(288\\times 512\\)px 비디오 생성으로 구성된 모델 캐스케이드를 구현하였다. 업샘플링 품질을 향상시키기 위해 훈련 중 가변 수준의 노이즈로 2단계 저해상도 입력을 손상시키고[21, 43] 추론 중 하이퍼파라미터 검색을 통해 얻은 1단계 출력에 노이즈 수준을 적용한다.\n' +
      '\n' +
      '우리는 Appx. A에 상세한 모델 하이퍼파라미터를 제시한다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '스냅 비디오는 LAMB[68] 최적화기를 사용하여 학습률 \\(5e^{-3}\\), 코사인 학습 스케줄, 총 배치 크기 2048 비디오와 2048 이미지의 스케일러블 비디오 생성기 구조로 학습한다. 우리는 550k 단계 이상의 1단계 모델을 훈련하고 370k 반복에 대한 1단계 모델 가중치에서 시작하여 고해상도 비디오에서 2단계 모델을 미세 조정한다. Sec 3.2에서 관측한 후, 우리는 \\(\\boldsymbol{\\sigma}_{\\text{in}=s\\sqrt{T}\\)의 자세를 취한다. EDM이 설계된 원(64\\)px 해상도와 (T=16\\) 프레임 비디오들을 고려하여, 1단 모델은 \\(\\boldsymbol{\\sigma}_{\\text{in}=4\\), 2단 모델은 \\(\\boldsymbol{\\sigma}_{\\text{in}=32\\)으로 설정하였다.\n' +
      '\n' +
      '우리는 Appx. B에서 훈련 세부 사항과 매개변수를 제시한다.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      '우리는 [25]의 결정론적 샘플러와 2단계 캐스케이드를 사용하여 가우시안 잡음 및 사용자 제공 컨디셔닝 정보로부터 비디오 샘플을 생성한다. 우리는 1단계에서는 256개의 샘플링 단계를 사용하고 2단계에서는 40개의 샘플링 단계를 사용하며, 달리 명시되지 않는 한 텍스트-비디오 정렬(Appx. C.1 참조)을 개선하기 위해 분류기 무료 안내[19]를 사용한다. 샘플 품질을 일관되게 개선하기 위해 동적 임계화[43] 및 진동 유도[21]를 찾는다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '이 섹션에서는 기준선에 대한 스냅 비디오의 평가를 수행하고 설계 선택을 검증합니다. Sec. 4.1은 사용된 데이터셋을 소개하고, Sec. 4.2는 평가 프로토콜을 정의하고, Sec. 4.3은 확산 프레임워크와 아키텍처 선택의 삭제를 보여주고, Sec. 4.4는 우리의 방법을 최첨단 대형 비디오 생성기와 정량적으로 비교하고, Sec. 4.5는 정성적 평가를 수행한다. 우리는 _Appendix_ 및 _Website_에서 샘플을 보여줌으로써 평가를 보완한다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '1.265M 이미지와 238k 시간의 비디오로 구성된 내부 데이터 세트에서 모델을 훈련하고 각각 해당 텍스트 캡션을 사용한다. 비디오에 대한 고품질 캡션 획득의 어려움으로 인해, 우리는 이러한 주석이 누락된 데이터 세트에서 비디오 부분에 대한 합성 비디오 캡션을 생성하기 위해 사용하는 비디오 캡션 모델을 개발한다.\n' +
      '\n' +
      '우리는 훈련 중에 관찰되지 않는 평가를 위해 다음 데이터 세트를 사용한다:\n' +
      '\n' +
      '**UCF-101**[55]는 101개의 액션 카테고리로부터 13.320 \\(320\\times 240\\)px Youtube 비디오를 포함하는 비디오 데이터세트이다.\n' +
      '\n' +
      '**MSR-VTT**[65]는 10.000 \\(320\\times 240\\)px 웹 크롤링된 비디오를 포함하는 데이터 세트이며, 각각은 20개의 텍스트 캡션을 사용하여 수동으로 주석을 달았다. 테스트 세트에는 2.990개의 비디오와 59.800개의 해당 캡션이 포함되어 있습니다.\n' +
      '\n' +
      '### Evaluation Protocol\n' +
      '\n' +
      '확산 프레임워크와 모델 아키텍처에서 동작되는 선택사항을 검증하기 위해 1단계 모델만을 사용하여 \\(64\\times 36\\)px 해상도로 수행되는 방법 삭제를 제시하고, 생성된 50k 비디오에 대한 내부 데이터 세트의 테스트 세트에 대해 FID[18], FVD[60] 및 CLIPSIM[63] 메트릭을 계산한다.\n' +
      '\n' +
      '기준선에 대한 방법을 평가하기 위해 [4, 13, 32, 48, 62, 72]에서 강조 표시된 프로토콜을 0으로 따른다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) & Train Thr. \\(\\downarrow\\) & Inf. Thr. \\(\\downarrow\\) \\\\ \\hline U-Net 85M [10] & 8.21 & 45.94 & 0.2319 & 133.2 & 49.6 \\\\ U-Net 284M [10] & 4.90 & 23.76 & 0.2391 & 230.3 & 105.1 \\\\ Snap Video FIT 500M & 3.07 & 27.79 & 0.2459 & 69.5 & 254.1 \\\\ Snap Video FIT 3.9B & **2.51** & **12.31** & **0.2579** & 526.0 & 130.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: \\(64\\times 36\\)px 해상도에서 내부 데이터 세트에 대한 다양한 아키텍처 및 모델 크기의 성능. 우리는 스케일링과 함께 강력한 성능 향상을 관찰하고 FITs가 U-Nets와 관련하여 향상된 속도로 더 나은 성능을 제공한다는 점에 주목한다. ms/video/GPU에서 트레인 및 추론 처리량.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\(\\boldsymbol{\\sigma}_{\\text{data}}\\) & \\(\\boldsymbol{\\sigma}_{\\text{in}}\\) & Imgs. as Videos & FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) \\\\ \\hline (i) & 0.5 & 1.0 & ✓ & 6.58 & 39.95 & 0.2370 \\\\ (ii) & 0.5 & 4.0 & ✓ & 4.03 & 31.00 & 0.2449 \\\\ \\hline (iv) & 1.0 & 2.0 & ✓ & 4.45 & 34.89 & 0.2428 \\\\ \\hline (iii) & 1.0 & 1/4.0 & ✗ & 3.50 & 24.88 & 0.2469 \\\\ \\hline Ours & 1.0 & 4.0 & ✓ & 3.07 & 27.79 & 0.2459 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다양한 확산 프로세스 구성의 삭제는 \\(\\boldsymbol{\\sigma}_{\\text{data}\\), 입력 스케일링 \\(\\boldsymbol{\\sigma}_{\\text{in}\\), 이미지를 무한 프레임 비디오로 처리하여 \\(64\\times 36\\)px 해상도로 내부 데이터셋에 대해 평가하였다.\n' +
      '\n' +
      'UCF-101 [55] 및 MSR-VTT [65] 데이터 세트에 대한 샷 평가. 모든 설정에서 24fps에서 16 프레임 비디오(512\\times 288\\px)를 생성한다. 본 논문에서는 16:9의 종횡비를 갖는 고유 \\(512\\times 288\\)px 해상도와 이러한 벤치마크에서 일반적으로 사용되는 \\(288\\times 288\\)px 정사각형 종횡비에서의 성능을 평가한다. 우리는 [4, 13, 32, 48, 62, 72]의 평가 프로토콜이 생성된 샘플의 수, 클래스 레이블의 분포, 텍스트 프롬프트의 선택에 대해 서로 다른 선택을 제시한다는 점에 주목한다. 다음 평가 변수를 사용합니다.\n' +
      '\n' +
      '**Zero-shot UCF-101**[55] 원본 데이터세트와 동일한 분포를 갖는 10.000개의 비디오 [4, 62] 샘플링 클래스를 생성한다. 각 클래스 레이블[13]에 대한 텍스트 프롬프트를 생성하고 FVD[60] 및 인셉션 점수[46]를 계산한다.\n' +
      '\n' +
      '**Zero-shot MSR-VTT**[65] 59.800 테스트 프롬프트[13, 48] 각각에 대해 비디오 샘플을 생성하고 CLIP-FID[27] 및 CLIPSIM[63]을 계산한다.\n' +
      '\n' +
      '이러한 벤치마크에 대한 결과를 보고하지 않는 최첨단 폐쇄 소스 방법과 보다 완전한 성능 평가를 제공하고 비교하기 위해, 우리는 실제성, 비디오 텍스트 정렬 및 생성된 모션의 양과 품질, _dynamic 이미지의 생성을 시그널링할 수 있는 비디오 생성기의 중요한 특성, 생생하고 고품질의 모션이 있는 비디오보다 딤 모션이 있는 비디오 또는 모션 아티팩트를 평가하는 사용자 연구를 수행한다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '제안된 FIT 아키텍처를 평가하기 위해 시간적 어텐션 연산을 인터리빙하여 비디오 생성 설정에 적응하는 [10]의 U-Net을 고려한다. 우리는 두 아키텍처의 확장성을 평가하기 위해 서로 다른 용량의 두 개의 U-Net 변형과 FIT의 더 작은 변형을 고려한다. 우리는 Appx의 아키텍처를 자세히 설명합니다. A 및 결과를 탭에 표시합니다. 2.\n' +
      '\n' +
      '본 논문에서 제안하는 500M 파라미터 FIT는 기준선 284M 파라미터 U-Net보다 3.31\\(\\times\\) 빠르게 학습하고, FID와 CLIPSIM 측면에서 4.49\\(\\times\\) 빠르게 추론을 수행하며, 이를 능가한다. 또한, FITs와 U-Nets 모두 스케일링과 함께 강한 성능 이득을 보인다. 우리의 가장 큰 FIT는 284M U-Net에 대해 추론 시간이 1.24\\(\\times\\) 증가하는 3.9B 매개변수로 확장된다.\n' +
      '\n' +
      '확산 프레임워크에서 작동하는 선택 사항을 평가하기 위해 500M FIT 아키텍처를 사용하여 확산 프로세스의 다양한 구성을 삭제한다. 본 논문에서는 (i) 원본 EDM 프레임워크, (ii) EDM(\\mathbf{\\sigma}_{\\text{data}\\)을 사용한 확장 확산 프레임워크, (iii) 무한 프레임 레이트 비디오로 처리되지 않은 이미지를 사용한 확장 확산 프레임워크, (iv) 무한 프레임 레이트 비디오로 처리되지 않은 이미지를 사용한 확장 확산 프레임워크, (iii) EDM(\\mathbf{\\sigma}_{\\text{in}}\\)을 사용하여 확장 확산 프레임워크를 생성한다. 우리의 프레임워크는 모든 메트릭(i)에서 EDM에 비해 개선되며, 훈련 목표 생성과 널리 사용되는 \\(\\mathbf{v}\\)-Salimans et al_. [45]의 손실 가중치에 매칭되는 효과인 \\(\\mathbf{\\sigma}_{\\text{data}=1\\) 설정의 이점을 보여준다. (탭. 1 참조). \\(\\mathbf{\\sigma}_{\\text{in}<s\\sqrt{T}\\)을 사용하면 성능(iii)이 손상된다. 마지막으로 이미지를 무한 프레임 속도로 처리하면 FID가 지속적으로 개선됩니다.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      '우리는 탭에서 UCF101 [55] 및 MSR-VTT [65] 데이터 세트에 대해 스냅 비디오의 비교를 수행한다. 4 및 Tab. 5. FID 및 FVD 비디오 품질 메트릭은 본 아키텍처에서 수행한 확산 프레임워크 및 공동 시공간 모델링에 기인하는 기준선보다 개선됨을 보여준다. 이 방법은 UCF101에서 두 번째로 좋은 IS(38.89\\)를 생성하여 비디오 텍스트 정렬을 잘 보여준다. 우리의 방법은 UCF101에서 Make-A-Video[48]를 능가하지만 MSR-VTT에서 더 낮은 CLIPSIM 점수를 생성한다는 점에 주목한다. 우리는 이러한 행동을 유사한 CLIPSIM에도 불구하고 더 높은 텍스트 이미지 정렬을 생성하기 위해 [43]에서 관찰된 일반적으로 사용되는 CLIP [38] 임베딩 대신 T5 [39] 텍스트 임베딩을 사용한 것으로 간주한다.\n' +
      '\n' +
      '종합적인 평가를 제공하기 위해 사용자 연구를 통해 비디오 생성기의 중요한 측면인 실사, 비디오-텍스트 정렬, 움직임의 양과 움직임의 질을 평가한다. 공개적으로 접근할 수 있는 세 가지 최첨단 기술\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & FVD \\(\\downarrow\\) & FID \\(\\downarrow\\) & IS \\(\\uparrow\\) \\\\ \\hline CogVideo [23] (Chinese) & 751.3 & - & 23.55 \\\\ CogVideo [23] (English) & 701.6 & - & 25.27 \\\\ MagicVideo [72] & 655 & - & - \\\\ LVDM [17] & 641.8 & - & - \\\\ Video LDM [4] & 550.6 & - & 33.45 \\\\ VideoFactory [62] & 410.0 & - & - \\\\ Make-A-Video [48] & 367.2 & - & 33.00 \\\\ PYCo [13] & 355.2 & - & 47.46 \\\\ \\hline Snap Video (\\(288\\times 288\\) px) & 260.1 & 39.0 & 38.89 \\\\ Snap Video (\\(512\\times 288\\) px) & 200.2 & 28.1 & 38.89 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: UCF101[55]에 대한 제로샷 평가 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & CLIP-FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) \\\\ \\hline NUWA [64] (Chinese) & 47.68 & - & 0.2439 \\\\ CogVideo [23] (Chinese) & 24.78 & - & 0.2614 \\\\ CogVideo [23] (English) & 23.59 & - & 0.2631 \\\\ MagicVideo [72] & - & 998 & - \\\\ LVDM [17] & - & - & 0.2381 \\\\ Latent-Shift [2] & 15.23 & - & 0.2773 \\\\ Video LDM [4] & - & - & 0.2929 \\\\ VideoFactory [62] & - & - & 0.3005 \\\\ Make-A-Video [48] & 13.17 & - & 0.3049 \\\\ PYCo [13] & 9.73 & - & - \\\\ \\hline Snap Video (\\(288\\times 288\\) px) & 8.48 & 110.4 & 0.2793 \\\\ Snap Video (\\(512\\times 288\\) px) & 9.35 & 104.0 & 0.2793 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MSR-VTT[65]에 대한 제로샷 평가 결과.\n' +
      '\n' +
      '비디오 생성기가 고려된다: Gen-2[11], PikaLabs[1] 및 Floor33[17]. 우리는 생생한 동작으로 장면을 묘사하는 [31]에서 65개의 프롬프트 세트를 필터링하고 기본 옵션으로 각 방법에 대한 비디오를 생성한다. 참가자들에게 스냅 비디오의 짝을 이룬 샘플과 각 기준선 사이의 선호도를 표현하여 각 샘플에 대해 5명의 사용자의 표를 수집하도록 요청한다. 결과는 탭에 나와 있습니다. 도 6 및 비디오 샘플은 Appx. C.2 및 _웹사이트_에서 사용된 프롬프트 목록과 함께 제공된다. 이 방법은 Gen-2에 필적하는 광실감을 갖는 결과를 생성하면서, PikaLab과 Floor33을 능가하며, 비디오-텍스트 정렬과 관련하여 모든 기준선을 능가한다. 가장 중요한 것은 베이스라인이 종종 _동적 이미지_, 흐릿한 모션이 있는 비디오 또는 모션 아티팩트가 있는 비디오를 생성한다는 점에 주목하며, 이는 큰 모션 모델링의 어려움에 기인한다. 대조적으로, 우리의 방법은 관절 시공간 모델링 접근법 덕분에 움직임 메트릭에 의해 보여지는 바와 같이 생생하고 고품질의 움직임을 생성한다.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      '이 섹션에서는 프레임워크의 정성적 평가를 수행한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 4, Appx. C.3 및 _웹사이트_, 우리는 저자가 공개적으로 공개한 샘플에 대해 우리의 방법을 최첨단 발전기 [4, 13, 21, 48]와 비교한 정성적 결과를 제시한다. 이러한 프롬프트는 기준선의 강도를 강조하기 위해 선택되었을 수 있지만 본 방법은 텍스트 설명에 정렬된 보다 사실적인 샘플을 생성한다. 가장 중요한 것은 우리의 샘플이 시간적 불일치로 인해 기준선에 존재하는 깜박이는 아티팩트를 피하는 생생하고 고품질 모션을 제공한다는 것이다. 우리는 Appx. C.2의 동일한 샘플 세트에 대해 수행된 사용자 연구와 함께 정성적 평가를 동반한다.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '본 연구에서는 텍스트-비디오 생성에서 일반적으로 사용되는 확산 과정과 아키텍처의 단점을 강조하고, 비디오를 일류 시민으로 취급하여 체계적으로 다룬다. 먼저, 고해상도 비디오 생성을 위한 EDM[25] 확산 프레임워크의 수정을 제안하고 이미지-비디오 모달리티 불일치를 피하기 위해 이미지를 높은 프레임 레이트 비디오로 처리한다. 둘째, U-Nets[41]을 효율적인 변압기 기반 FITs[8]로 대체하여 수십억 개의 매개변수로 확장한다. 비디오의 학습 가능한 압축 표현 덕분에 압축 표현에 대한 공동 시공간 모델링으로 인해 시간 일관성 및 모션 모델링 능력과 관련하여 특히 훈련 시간, 확장성 및 성능을 크게 개선한다. UCF101[55] 및 MSR-VTT[65] 및 사용자 연구에서 평가될 때, 스냅 비디오는 모델링된 모션의 품질과 관련하여 특히 최첨단 성능을 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{5}{c}{Photorealism Video-Text Align. Mot. Quant. Mot. Qual.} \\\\ \\hline Gen-2 [11] & 44.3 & 81.0 & 96.0 & 78.7 \\\\ PikaLab [1] & 61.5 & 80.3 & 89.2 & 70.5 \\\\ Floor33 [17] & 76.3 & 80.9 & 88.0 & 79.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 65개의 동적 장면 프롬프트에 대한 공개 액세스 가능한 비디오 생성기에 대한 실사, 비디오-텍스트 정렬, 모션 수량 및 품질에 대한 사용자 연구. % 우리 방식에 찬성하는 표.\n' +
      '\n' +
      '그림 4: 공개된 샘플에서 스냅 비디오를 최신 비디오 생성기와 비교한 질적 결과. 베이스라인 방법은 움직임 아티팩트(좌상단, 우상단, 우하단)를 제시하거나 _dynamic image_(좌하단)를 생성하는 반면, 본 방법은 더 시간적으로 일관성 있는 움직임을 생성한다. 웹사이트에서 가장 잘 볼 수 있습니다.\n' +
      '\n' +
      '## 6 Acknowledgements\n' +
      '\n' +
      '올렉스실 포포프, 아르템 시니친, 안톤 쿠즈멘코, 비탈리 크라브추크, 바딤 헤르베니크, 그리고리 코즈헤미아크, 테티아나 슈르바코바, 스비틀라나 하르쿠샤, 올렉산드르 유르차크, 안드리 부니아코프, 메리나 마리엔코, 막심 가르쿠샤, 브렛 크롱, 아나스타시아 본다큐크, 비디오 프레젠테이션, 이야기 및 그래픽 자산 실현에 도움을 준 콜린 엘레스, 드리티만 사가르, 비탈리 오시코프, 에릭 후, 주석 작업에 도움을 준 메리나 디아코노바에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Pika lab discord server. [https://www.pika.art/](https://www.pika.art/). Accessed: 2023-11-01.\n' +
      '* [2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv_, 2023.\n' +
      '* [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. _ArXiv_, 2022.\n' +
      '* [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [5] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A Efros, and Tero Karras. Generating long videos of dynamic scenes. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [7] Ting Chen. On the importance of noise scheduling for diffusion models. _arXiv_, 2023.\n' +
      '* [8] Ting Chen and Lala Li. Fit: Far-reaching interleaved transformers. _arXiv_, 2023.\n' +
      '* [9] Aidan Clark, Jeff Donahue, and Karen Simonyan. Efficient video generation on complex datasets. _arXiv_, 2019.\n' +
      '* [10] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [11] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [12] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In _Proceedings of the European Conference of Computer Vision (ECCV)_, 2022.\n' +
      '* [13] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [14] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Josh Susskind. f-dm: A multi-stage diffusion model via progressive signal transformation. _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [15] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and Navdeep Jaitly. Matryoshka diffusion models. _arXiv_, 2023.\n' +
      '* [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv_, 2023.\n' +
      '* [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. _arXiv_, 2023.\n' +
      '* [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv_, 2022.\n' +
      '* [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. _arXiv_, 2022.\n' +
      '* [22] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _ICLR Workshop on Deep Generative Models for Highly Structured Data_, 2022.\n' +
      '* [23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideto: Large-scale pretraining for text-to-video generation via transformers. _arXiv_, 2022.\n' +
      '* [24] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* [25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv_, 2015.\n' +
      '* [27] Tuomas Kynkuanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes infrechet inception distance. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [28] Alex X. Lee, Richard Zhang, Frederik Ebert, P. Abbeel, Chelsea Finn, and S. Levine. Stochastic adversarial video prediction. _arXiv_, abs/1804.01523, 2018.\n' +
      '* [29] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2018.\n' +
      '* [30] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemergys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. _arXiv_, 2023.\n' +
      '* [31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. _arXiv_, 2023.\n' +
      '* [32] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [33] Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui Zhao, Shiqi Wang, and Shanshe Wang. Image and video compression with neural networks: A review. _IEEE Transactions on Circuits and Systems for Video Technology_, 2019.\n' +
      '* [34] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _Proceedings of the 25th ACM International Conference on Multimedia_, 2017.\n' +
      '* [35] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. _PyTorch: An Imperative Style, High-Performance Deep Learning Library_. 2019.\n' +
      '* [37] Chenyang QI, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 2022.\n' +
      '* [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. _arXiv_, 2021.\n' +
      '* [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention (MICCAI)_, 2015.\n' +
      '* [42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv_, 2022.\n' +
      '* [44] Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan, 2020.\n' +
      '* [45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.\n' +
      '* [47] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion styles. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. _arXiv_, 2022.\n' +
      '* [49] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning (ICML)_, 2015.\n' +
      '* [51] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* [52] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '\n' +
      '* [53] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models, 2021.\n' +
      '* [54] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* [55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _arXiv_, 2012.\n' +
      '* [56] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In _International Conference on Machine Learning (ICML)_, 2015.\n' +
      '* [57] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. _arXiv_, 2023.\n' +
      '* [58] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* [59] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '* [60] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv_, 2018.\n' +
      '* [61] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and D. Erhan. Phenaki: Variable length video generation from open domain textual description. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [62] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. _arXiv_, 2023.\n' +
      '* [63] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _ArXiv_, 2021.\n' +
      '* [64] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In _Proceedings of the European Conference of Computer Vision (ECCV)_, 2022.\n' +
      '* [65] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n' +
      '* [66] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogptv: Video generation using vq-vae and transformers. _arXiv_, 2021.\n' +
      '* [67] Sheng-Siang Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, and Nan Duan. Nuwa-xl: Diffusion over diffusion for extremely long video generation. In _Annual Meeting of the Association for Computational Linguistics_, 2023.\n' +
      '* [68] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In _International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* [69] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _Transactions on Machine Learning Research_, 2022.\n' +
      '* [70] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [71] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '파라미터. Sec. C.2는 사용자 연구 결과를 제시한다. Sec. C.3 및 Sec. C.4에서는 기준선에 대한 샘플과 본 방법의 추가 샘플을 보여준다. Sec. C.5에서는 높은 프레임레이트에서 더 긴 비디오를 생성하는 방법에 대해 설명합니다. 마지막으로 Sec. C.6은 UCF101[55]에 대한 평가 세부사항을 제시한다.\n' +
      '\n' +
      '###샘플러 파라미터 보정\n' +
      '\n' +
      '샘플링 매개변수의 선택은 확산 모델의 성능에 상당한 영향을 미칠 수 있다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 5를 참조하면, 분류기 자유 안내[19]가 모델의 성능에 미치는 영향을 보여준다. 1단계 모델을 사용하여 10k 샘플을 생산하고 40단계 결정론적 샘플러를 사용하여 10k 샘플에 대해 계산된 다양한 지침 값에서 FVD 및 CLIPSIM을 보고한다. 분류기 자유 안내는 FVD와 CLIPSIM을 모두 개선할 수 있지만 높은 분류기 자유 안내 척도에서는 샘플 포화도가 증가한다는 것을 알 수 있다. 우리는 동적 임계화 및 진동 분류기 자유 지침을 채택하여 현상을 줄이는 데 효과적임을 발견했다[43].\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 6, 우리는 동일한 설정에서 모델의 성능을 평가하지만 샘플링 단계의 수를 변경한다. 우리는 모델이 이미 64단계에서 고품질 샘플을 생산하고 FVD가 256단계까지 개선된다는 것을 발견했다.\n' +
      '\n' +
      '### User Studies\n' +
      '\n' +
      '평가를 보완하기 위해 Make-A-Video[48], PYoCo[13], Video LDM[4] 및 Imagen Video[21]의 공개 샘플 세트에 대한 사용자 연구를 실행했다. 각 방법에 대해 공개 샘플 및 프롬프트를 수집하고 스냅 비디오에서 해당 샘플을 생성하고 참가자에게 사진 사실성, 비디오 텍스트 정렬, 모션 양 및 품질 측면에서 선호도를 표현하여 각 샘플에 대해 5표를 수집하도록 요청한다. 결과는 탭에 나와 있습니다. 도 10 및 샘플은 도 10에 제공된다. 도 4를 참조하면, 도 7 및 _웹사이트_. 제안하는 방법은 기준선에 대해 향상된 실사성을 보여주고, Imagen Video를 제외한 모든 방법에 대해 높은 비디오-텍스트 정렬을 보여주며, 움직임 품질 측면에서 기준선을 지속적으로 능가한다(깜박이는 아티팩트 및 카메라 움직임이 큰 장면에서 시간적으로 일관되지 않은 배경 참조).\n' +
      '\n' +
      '기준선에 대한 정성적 결과\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 7 및 _웹사이트_, Make-A-Video[48], PYoCo[13], Video LDM[4] 및 Imagen Video[21]의 저자들이 공개적으로 발표한 샘플에 대한 기준선에 대한 방법의 정성적 결과를 제시한다. 제안하는 방법은 자연스러운 움직임으로 비디오를 생성하고, 시간적 일관성을 유지하면서 큰 움직임과 카메라 변화로 장면을 처리할 수 있다. 반면, 베이스라인은 움직임이 큰 경우 깜박이는 아티팩트와 시간적으로 불일치하는 객체를 자주 나타내는 것을 관찰한다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 8 및 _웹사이트_, Gen-2[11], PikalLab[1] 및 Floor33[17]을 포함하는 공개적으로 액세스 가능한 최첨단 비디오 생성기와 우리의 방법을 비교한 정성적 결과를 제공한다. 제안된 방법은 프롬프트에 더 정렬된 결과를 생성하고, 종종 동적 이미지를 생성하는 베이스라인과 달리 많은 양의 움직임을 갖는 시간적으로 일관성 있는 비디오를 생성한다.\n' +
      '\n' +
      '#추가적인 정성적 결과\n' +
      '\n' +
      '**복합 프롬프트** 그림 1에 나와 있습니다. 도 9를 참조하면, 도 10 및 _웹사이트_ 추가 샘플은 ChatGPT, Make-A-Video[48], PYoCo[13], Video LDM[4], Imagen Video[21] 및 Evalcrafter[31] 벤치마크에서 수집된 프롬프트 세트에 대해 우리의 방법으로 생성되었다. 우리의 방법은 합성할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{5}{c}{Photorealism Video-Text Align. Mot. Quant. Mot. Qual.} \\\\ \\hline Imagen Video [21] & 66.9 & 54.3 & 49.4 & 56.3 \\\\ PYoCo [13] & 63.3 & 64.9 & 57.1 & 63.9 \\\\ Video LDM [4] & 61.7 & 64.4 & 62.2 & 65.8 \\\\ Make-A-Video [48] & 80.0 & 82.2 & 82.2 & 75.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 폐쇄 소스 방법들로부터 공개된 샘플들에 대한 사실성, 비디오-텍스트 정렬, 모션 양 및 품질에 대한 사용자 연구. % 우리 방식에 찬성하는 표.\n' +
      '\n' +
      '그림 5: 분류기 자유 유도 가중치의 함수로서 \\(64\\times 36\\)px 해상도의 내부 데이터 세트에 대한 FVD 및 CLIPSIM. 점들은 \\([0,0.5,1,1.5,2,3,4,5,6,7,8,10,12,14,16]\\)의 가중치들을 나타낸다.\n' +
      '\n' +
      '그림 6: 샘플링 단계와 샘플러 유형의 함수로 \\(64\\times 36\\)px 해상도의 내부 데이터 세트에 대한 FVD.\n' +
      '\n' +
      '여러 가지 다른 개념의 크기를 조정합니다. 가장 중요한 것은 큰 카메라 움직임, POV 비디오, 빠르게 움직이는 물체의 비디오를 포함하여 도전적인 움직임을 가진 비디오를 제작할 수 있다는 것이다. 특히, 이 방법은 시간적 일관성을 유지하고 비디오 깜박임 아티팩트를 방지한다. 이러한 모션 모델링 기능은 아키텍처에서 수행한 공동 시공간 모델링에 기인한다.\n' +
      '\n' +
      '**새로운 뷰** 새로운 객체 뷰를 생성하는데 있어서 제안된 방법의 성능을 정성적으로 평가한다. 이를 위해 다음과 같은 템플릿 _Camera\\(\\langle\\)object\\(\\rangle\\)_,\\(\\langle\\)object\\(\\rangle\\)_,\\(\\langle\\)object\\(\\rangle\\)_ 또는\\(\\langle\\)object\\(\\rangle\\)_를 중심으로 회전하는 _Camera를 사용하여 프롬프트를 구축한다. 여기서 \\(\\langle\\)object\\(\\rangle\\)은 객체가 생성하는 자리 표시자를 나타낸다. 결과는 그림 1에 나와 있다. 11과 _웹사이트_. 본 논문에서 제안한 모델은 모델링된 객체들의 3차원 기하구조에 대한 이해도를 가지고 있음을 보여줌으로써, 객체들에 대한 그럴듯한 새로운 뷰를 생성할 수 있음을 보인다.\n' +
      '\n' +
      '**샘플 다양성** 각 프롬프트에 대해 다양한 샘플을 생성하는 방법의 능력을 정성적으로 평가한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 12 및 _웹사이트_ 우리는 일련의 프롬프트에 대해 생성된 세 가지 샘플을 보여주고 이 방법이 다양한 샘플을 생성할 수 있음을 주목한다.\n' +
      '\n' +
      '**UCF 101** 도 13 및 _웹사이트_, 우리는 제로샷 UCF101 [55] 평가를 위해 우리의 mehthod에 의해 생성된 정성적 결과를 제시한다.\n' +
      '\n' +
      '### 계층적 비디오 생성\n' +
      '\n' +
      '우리는 고정된 수의 프레임과 가변 프레임레이트로 비디오를 생성하는 방법을 훈련한다. 본 논문에서는 이러한 특성을 이용하여 비디오의 지속시간을 증가시키고, 이전에 생성된 프레임들에 대한 컨디셔닝 생성을 통해 프레임레이팅을 위한 계층적 생성 전략을 고안한다. 특히, 이미 사용 가능한 프레임들에 대한 조건 생성에 대해서는 _Ho et al._[22]의 _reconstruction guidance_ 방법을 채택한다. 우리는 점진적으로 증가하는 프레임레이트의 계층을 정의하고 마지막으로 생성된 프레임을 컨디셔닝으로 사용하여 각 단계에서 가장 낮은 프레임레이트에서 원하는 길이의 비디오를 자동으로 점진적으로 생성함으로써 시작한다. 그 후, 계층 구조의 각 연속 프레임레이트에 대해 동일한 길이의 비디오를 자동으로 생성하지만 하위 프레임레이트에서 이미 생성된 모든 프레임에서 모델을 조정한다. 우리는 _웹사이트_에 샘플을 보여준다.\n' +
      '\n' +
      '### Zero Shot UCF101 평가\n' +
      '\n' +
      'UFC101 [55]는 저해상도 유튜브 동영상의 데이터셋이다. 생성된 출력을 분포에 더 잘 일치시키기 위해, 우리는 해상도 조절 메커니즘을 통해 낮은 원본 해상도로 비디오를 생성하도록 모델을 조정한다(3.4절 참조). [13]에 따르면, UCF101 클래스 레이블은 항상 충분히 설명적인 이름을 가지고 있지 않기 때문에, 각 클래스에 대한 프롬프트를 생성한다. _applying eye makeup_, _applying lipstick_, _gymnast with the balance beam_, _baby crawling_, _baseball pitcher throwing baseball_, _a basketball player shooting basketball_, _a person in the floor_, _a person in the basketball game_, _a person in the tall jump_, _a person in the basketball game_, _a person to the spring_, _haircuting_, _a person in the swimming pool using a hair dryer_, _body weight squats_, _billiards_, _a person in the baseball.\n' +
      '\n' +
      '도 7: Make-A-Video[48], PYoC[13], Video LDM[4] 및 Imagen Video[21]의 공개된 샘플에 대한 스냅 비디오의 비교. 제안된 방법은 비디오 깜박임을 피하면서 시간적으로 일관성 있는 움직임을 생성한다. 웹사이트에서 가장 잘 봤습니다.\n' +
      '\n' +
      '도 8: 스냅 비디오와 공개적으로 액세스 가능한 최첨단 비디오 생성기 Gen-2[11], PikaLab[1] 및 Floor33[17]의 비교. 제안된 방법은 동적 영상을 생성하기 보다는 시간적으로 일관성 있는 움직임이 많은 영상을 생성한다. 웹사이트에서 가장 잘 봤습니다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '그림 10: ChatGPT, 기준선 방법 및 Evalcrafter [31] 벤치마크에서 수집한 프롬프트 모음에서 스냅 비디오에서 생성된 추가 샘플. 웹사이트에서 가장 잘 봤습니다.\n' +
      '\n' +
      '그림 11: 스냅 비디오가 각 객체 주변의 원형 카메라 모션을 유도하는 프롬프트를 위해 제작한 비디오. 웹사이트에서 가장 잘 봤습니다.\n' +
      '\n' +
      '도 12: 동일한 프롬프트를 위해 스냅 비디오에 의해 생성된 샘플에서의 다양성. _웹사이트_의 추가 샘플을 참조하십시오.\n' +
      '\n' +
      '도 13: UCF101에 대한 제로-샷 평가를 위해 스냅 비디오에 의해 생성된 샘플들[55]. 웹사이트에서 가장 잘 봤습니다.\n' +
      '\n' +
      '## 부록 D 도출 EDM 잡음제거 목표\n' +
      '\n' +
      '입력 스케일링 인자\\(\\mathbf{\\sigma}_{\\text{in}\\)를 도입하는 순방향 프로세스를 수정하는 EDM 프레임워크에 대한 \\(\\mathcal{F}_{\\theta}\\)의 표현으로 표현된 디노이징 목적 \\(\\mathcal{L}(\\mathcal{F}_{\\theta})\\)을 유도한다:\n' +
      '\n' +
      '\\mathcal{L}(\\mathcal{F}_{\\theta})=\\mathbbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}\\Big{[}w(\\mathbf{\\sigma}\\big{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma}})\\mathbf{x}_{\\mathbf{\\sigma})-c_{\\text{nrm}(\\mathbf{\\sigma}}\\text{2}^{2}\\Big{},\\tag{4}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\text{2}\\\n' +
      '\n' +
      '우리는 원래 공식에서와 같이 잡음제거 목적 \\(\\mathcal{L}(\\mathcal{D}_{\\theta})\\)에서 시작한다:\n' +
      '\n' +
      '\\mathcal{L}(\\mathcal{D}_{\\theta}) = \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}\\Big{[}\\lambda(\\mathbf{\\sigma}\\big{\\theta}(\\frac{\\mathbf{x}{\\mathbf{\\sigma}_{\\text{in}}+\\mathbf{x}\\big{\\sim}_{2}^{2}\\Big{},\\tag{5}\\mathbf{x}\\big{\\sim}\\text{in}}\\mathbf{x}\\big{\\sim}\\text{in}},\\tag{5}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\\mac}\n' +
      '\n' +
      '여기서 \\(\\mathcal{D}_{\\theta}\\)의 정의를 회상한다:\n' +
      '\n' +
      '\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}))=c_{\\text{out}(\\mathbf{\\sigma})\\mathcal{F}_{\\theta}\\left(c_{\\text{in}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}\\right)+c_{\\text{skip}(\\mathbf{\\sigma})(\\mathbf{x}_{\\mathbf{\\sigma}})}. \\tag{6}\\text{6}\\mathbf{x}_{\\mathbf{\\sigma}}\n' +
      '\n' +
      '또한 \\(c_{\\text{in}}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}}(\\mathbf{\\sigma})\\), \\(\\lambda(\\mathbf{\\sigma})\\):\n' +
      '\n' +
      '\\frac{(}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}^{2}},\\quad\\lambda(\\mathbf{\\sigma}^{2}}\\mathbf{\\sigma}^{2}},\\tqrt{\\sigma}^{2}}\n' +
      '\n' +
      '\\(\\mathcal{D}_{\\theta}\\)와 \\(c_{\\text{in}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}(\\mathbf{\\sigma})\\)의 정의를 식에 삽입한다. (4) 우리는,\n' +
      '\n' +
      'bb{E}{\\mathbf{\\sigma},\\mathbf{\\sigma}{\\text{data}{\\text{in}}\\frac{\\mathbf{\\sigma}^{2}{\\text{in}}\\frac{\\mathbf{\\sigma}^{2}}\\big{{+}\\frac{\\mathbf{\\sigma}{\\text{in}}\n' +
      '\n' +
      '그 다음에는\n' +
      '\n' +
      '\\mathbf{x}-\\mathbf{\\sigma}_{\\text{data}^{2}\\mathbf{\\epsilon}+\\frac{\\mathbf{\\sigma}_{\\text{data}^{2}(\\mathbf{\\sigma}_{\\text{in}-1}{\\mathbf{in}\\mathbf{x},\\quad c_{\\text{nrm}(\\mathbf{\\sigma}}\\frac{1}{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}^{2}}},\\quad w(\\mathbf{\\sigma}}\\sqrt{\\mathbf{\\sigma}^{2}}+\\mathbf{\\sigma}^{2}}}^{2}},\\quad w(\\mathbf{\\sigma}}{\\sigma}^{2}}\\mathbf{\\sigma}}^{2}}\\mathbf{\\sigma}^{2}}\\mathbf{\\s\n' +
      '\n' +
      '학습 대상은 0에 가까울수록 무한대에 접근하는 스퓨리어스 항\\(\\frac{\\mathbf{\\sigma}_{\\text{data}^{2}(\\mathbf{\\sigma}_{\\text{in}-1)}{\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{x}\\mathbf{x}를 갖는다.\n' +
      '\n' +
      '이 공식으로부터 우리는 또한 EDM과 \\(\\mathbf{v}\\)-prediciton 프레임워크 사이의 연결에 주목한다. 먼저, 훈련목표는 \\(\\mathbf{v}=\\mathbf{\\sigma}_{\\text{data}^{2}\\mathbf{\\epsilon}-\\mathbf{\\sigma}\\mathbf{x}\\(\\mathbf{v}}=\\mathbf{\\sigma}_{\\text{data}^{2}\\mathbf{\\sigma}\\mathbf{x}\\(\\mathbf{v}})로 재구성되고 부정화된 \\(\\mathbf{v}} 예측목표로 구성된다. 둘째, 손실 가중치는 재가중치 \\(1+SNR\\)\\(\\mathbf{v}\\)-예측 프레임워크 [45] 가중치와 같다:\n' +
      '\n' +
      '\\frac{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}\\mathbf{\\sigma}^{2}}{\\big{(}\\mathbf{\\sigma}\\text{data}\\big{}=\\frac{1}{\\mathbf{\\sigma}^{2}+\\frac{1}{\\mathbf{\\sigma}^{2}}+SNR.\n' +
      '\n' +
      '따라서, \\(\\mathbf{\\sigma}_{\\text{data}=1\\)일 때, EDM은 \\(\\mathbf{v}\\)-prediciton 공식과 동등하다. 이러한 관측으로부터 출발하여, 스퓨리어스 항 \\(\\frac{\\mathbf{\\sigma}_text{data}^{2}(\\mathbf{\\sigma}_{\\text{in}-1)을 회피함으로써 \\(\\mathbf{\\sigma})의 모든 값에 대해 잘 형성된 \\(\\mathcal{F}_text{tgt}\\)을 나타내도록 프레임워크를 재작성한다.\n' +
      '\n' +
      '## 부록 E 우리의 확산 프레임워크 도출\n' +
      '\n' +
      '본 논문에서 제안하는 확산 프레임워크의 도출은 학습목표가 원 EDM(\\mathbf{\\sigma}_{\\text{in}\\mathbf{\\sigma}_{\\text{data}^{2}(\\mathbf{\\sigma}_{\\text{in}-1)\\mathbf{\\sigma}\\text{in}\\mathbf{\\sigma}\\mathbf{x}\\mathbf{x}\\(Appx. D 참조)에 대한 원 공식에 영향을 미치는 스퓨리어스 항(\\frac{\\mathbf{\\sigma}_{\\text{in}\\text{in})이 없는 모든 값(\\mathbf{\\sigma}_{\\text{in}\\text{in}\\text{in})에 대해 원래의 EDM(\\mathbf{\\sigma}\\text{in}\\text{in}\\text{in})의 모든 값에 대해 원래의 EDM(\\mathbf{\\sigma}\\text{in}\\text{in}\\text{in})의 예측목표와 동일하다고 가정함으로써 시작된다.\n' +
      '\n' +
      '\\[\\mathcal{F}_{\\text{tgt}=\\mathbf{v}=\\mathbf{\\sigma}_{\\text{data}^{2}\\mathbf{\\epsilon}-\\mathbf{\\sigma}\\mathbf{x}. \\tag{15}\\\n' +
      '\n' +
      '그리고 나서 \\(c_{\\text{arm}}(\\mathbf{\\sigma})\\(c_{\\text{arm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}}), \\(\\mathcal{F}_{\\theta}\\)으로 근사된 함수가 단위 분산을 갖도록 \\(c_{\\text{arm}}(\\mathbf{\\sigma})\\)을 유도한다:\n' +
      '\n' +
      '\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\sigma}\\mathbf{\\sigma}\\text{data}\\big{(\\mathbf{\\sigma}_{\\text{data}}\\frac{1}{\\mathbf{\\sigma}}^{2}(\\mathbf{\\sigma}}^{2}+\\mathbf{\\sigma}^{2}}}(18)}\\frac{1}{\\mathbf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\\cf{\\sigma}\n' +
      '\n' +
      '표준 정규화 방법에 따라 모델 입력이 단위 분산을 갖도록 \\(c_{\\text{in}}(\\mathbf{\\sigma})\\)을 정의한다.\n' +
      '\n' +
      '\\frac{Var}_{\\mathbf{x},\\mathbf{\\sigma}_{\\text{in}}\\frac{1}{\\sqrt{\\mathbf{\\sigma}_{\\text{in}}^{2}}{\\mathbf{\\sigma}}^{2}}+\\mathbf{\\sigma}^{2}}(22)}\n' +
      '\n' +
      '나머지 프레임워크 구성 요소를 도출하기 위해 먼저 우리의 순방향 프로세스의 정의를 회상한다:\n' +
      '\n' +
      '\\[\\mathbf{x}_{\\mathbf{\\sigma}=\\frac{\\mathbf{x}{\\mathbf{\\sigma}_{\\text{in}}+\\mathbf{\\sigma}\\mathbf{\\epsilon}, \\tag{25}\\\n' +
      '\n' +
      '그리고 \\(\\mathbf{x}\\)는 \\(\\mathbf{x}_{\\mathbf{\\sigma}\\) 및 \\(\\mathbf{v}\\)로부터 회수될 수 있음을 유의한다:\n' +
      '\n' +
      '\\frac{\\mathbf{x}_{\\mathbf{\\sigma}-\\frac{\\mathbf{\\sigma}{\\text{data}^{2}\\mathbf{v}{\\frac{1}{\\mathbf{\\sigma}_{\\text{in}}+\\frac{\\mathbf{\\sigma}^{2}{\\mathbf{\\sigma}_{\\text{data}^{2}}, \\tag{26}\\tag{1}{\\mathbf{\\sigma}^{2}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}}{\\text{n}{\\text{n}}{\\text{n}}{\\text{n}{\\text{n}}{\\text{n}}{\\text{n}{\\text{n}}{\\text{n}{\\text{n}}{\\text{n}{\\text{n}}{\\text{n}\n' +
      '\n' +
      'and consequently\n' +
      '\n' +
      '\\frac{\\mathbf{\\sigma}_{\\text{data}^{2}\\mathbf{x}_{\\mathbf{\\sigma}}-(\\frac{\\mathbf{\\sigma}_{\\text{data}^{2}}{\\mathbf{\\sigma}}+\\mathbf{\\sigma}^{2}\\mathbf{x}{\\mathbf{\\sigma}}. \\tag{27}\\text{2}\\text{2}\\mathbf{x}{\\mathbf{\\sigma}}\n' +
      '\n' +
      '\\(c_{\\text{skip}}(\\mathbf{\\sigma})\\)와\\(c_{\\text{out}(\\mathbf{\\sigma})\\)을 복원하기 위해 \\(\\mathcal{F}_{\\text{tgt}=\\mathbf{v}\\)의 정의와 Eq. (4) 0에 가까울수록 다음이 성립한다:\n' +
      '\n' +
      '\\mathcal{F}_{\\theta}(\\mathbf{\\sigma})\\mathbf{x}_{\\theta}(\\mathbf{\\sigma})\\mathbf{x}_{\\theta}(\\mathbf{\\sigma}}^{2}}\\mathcal{F}[\\mathbf{v}=\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}^{2}}\\mathcal{F}[\\mathbf{v}=\\mathbf{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\sigma}{\\s Eq를 대체합니다. (30)을 Eq. (26) 을 얻는다:\n' +
      '\n' +
      '\\frac{x}{\\mathbf{\\sigma}{\\text{in}}{\\frac{1}{\\sigma}{\\text{in}}{\\frac{bf{\\sigma}^{2}}{\\frac{1}{\\sigma}^{in}}+\\frac{bf{\\sigma}^{in}}\n' +
      '\n' +
      '우리가 아는 바로는\n' +
      '\n' +
      '\\frac{\\text{skip}(\\mathbf{\\sigma}_{\\text{in}}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}^{2},\\quad c_{\\text{in}\\mathbf{\\sigma}_{\\sigma}^{2}}\\frac{\\sqrt{\\sigma}^{2}+\\mathbf{\\sigma}^{2}}}{\\sigma}^{2}}{\\sigma}^{2}}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}{\\sigma}{\\sigma}{\\sigma}^{2}}\n' +
      '\n' +
      '손실 가중치\\(\\lambda(\\mathbf{\\sigma})\\)를 EDM과 동일한 값으로 설정했다.\n' +
      '\n' +
      '\\[\\lambda(\\mathbf{\\sigma})=\\frac{1}{\\mathbf{\\sigma}_{\\text{data}^{2}}+\\frac{1}{\\mathbf{\\sigma}^{2}. \\tag{36}\\tag{36}}\n' +
      '\n' +
      '\\(w(\\mathbf{\\sigma})\\) 복구를 위해 \\(\\mathcal{D}_{\\theta}\\)의 정의를 삽입한다. (6)과 \\(c_{\\text{in}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}(\\mathbf{\\sigma})\\), \\(\\lambda(\\mathbf{\\sigma})\\). (4) 우리는,\n' +
      '\n' +
      'bf{E}_{\\mathbf{\\sigma},\\mathbf{x}epsilon}\\big{[\\mathbf{\\sigma}{\\sigma}\\mathbf{\\sigma}{\\sigma}\\mathbf{\\sigma}{\\sigma}\\mathbf{\\sigma}^{2}}^{2}\n' +
      '\n' +
      'from which:\n' +
      '\n' +
      '\\frac{(\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}^{2})^{2}{(\\mathbf{\\sigma}^{2}+\\frac{\\mathbf{\\sigma}_{\\text{data}^{2}}{\\mathbf{\\sigma}_{\\text{in}}}}}^{2}}. \\tag{43}\\tag{43}}}}\n' +
      '\n' +
      '결론적으로, 제안된 확산 프레임워크는 입력 스케일링 인자\\(\\mathbf{\\sigma}_{\\text{in}\\)의 모든 값에 대해 학습 목표\\(\\mathcal{F}_{\\text{tgt}\\)를 원래의 EDM 학습 목표값과 동일하게 유지하여, 입력 신호에 존재하는 신호의 양을 제어하면서 동일한 잡음 제거 함수를 학습하도록 한다. 또한, 본 프레임워크는 기존의 손실 가중치\\(\\lambda(\\mathbf{\\sigma})\\)를 보존하여 잡음 레벨에 따른 손실 가중치에 영향을 주지 않으면서 입력 스케일링 팩터를 제어할 수 있다.\n' +
      '\n' +
      '## 부록 F 토론\n' +
      '\n' +
      '이 섹션에서는 프레임워크의 한계를 설명하고(Sec. F.1 참조) 사회적 영향에 대해 논의한다(Sec. F.2 참조).\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '스냅 비디오는 이 섹션에서 논의한 한계를 제시한다.\n' +
      '\n' +
      '**텍스트 렌더링** 우리는 종종 텍스트의 철자를 잘못 쓰는 우리의 프레임워크를 발견한다. 우리는 이 발견이 표시된 텍스트의 정확한 설명에 의해 일치하는 텍스트를 묘사하는 고품질 비디오의 부족 때문이라고 생각한다. 이 문제를 해결하기 위해 OCR을 위한 자동화된 파이프라인이 훈련 데이터 세트에 사용될 수 있다.\n' +
      '\n' +
      '**객체 카운트** 유사하게, 생성기는 특히 객체의 카디널리티가 높은 경우 요청된 개체 수를 렌더링하지 않을 수 있다. 행동은 비디오 데이터로부터 정확한 객체 카운트를 학습하는 어려움으로 설명될 수 있으며, 여기서 설명은 시끄러울 수 있고, 객체는 장면으로 들어가고 나갈 수 있으며 카메라는 넓게 움직일 수 있어 각 프레임에서 객체의 카디널리티를 변경한다.\n' +
      '\n' +
      '**위치 이해** 생성기가 프롬프트에 의해 요청된 위치에 객체를 배치할 수 있지만, 우리는 _"3개의 큐브의 스택과 같은 다수의 엔티티들 사이의 복잡한 위치 관계를 수반하는 프롬프트에 대응하는 비디오를 신뢰성 있게 합성할 수 없다는 것을 발견한다: 상단 하나는 파란색, 하단 하나는 녹색, 중간 하나는 빨간색"_.\n' +
      '\n' +
      '**스타일화** 우리 모델이 스타일화된 콘텐츠를 생성할 수 있음을 발견한다(도 9 참조). 그러나 애니메이션이 아닌 장면에서만 스타일 사양이 무시되거나 스타일화된 콘텐츠가 번역되는 실패 사례를 제시할 수 있다. 우리는 이 발견이 더 높은 확률로 예술적 스타일 및 시각적 스타일과 관련된 텍스트 설명을 포함하는 높은 미적 점수를 제시하는 필터링된 데이터 세트에 대한 모델 교육이 부족하기 때문이라고 생각한다.\n' +
      '\n' +
      '**네거티브** _"바나나가 들어 있지 않은 플레이트 옆에 주스 한 잔"_와 같은 몇 가지 도전적인 프롬프트는 모델이 부정을 무시하도록 유도하여 모든 엔티티의 생성을 초래할 수 있다.\n' +
      '\n' +
      '**블록 아티팩트** 비디오는 매우 많은 양의 움직임을 갖는 콘텐츠를 포함할 수 있으며, 이는 고정된 크기의 잠재된 표현으로 그 콘텐츠를 압축하는 데 더 큰 어려움을 초래한다. 그러한 상황들에서, 우리는 모델이 완벽한 방식으로 함께 블렌딩되지 않는 패치 토큰들을 생성할 수 있다는 것을 발견하여, 비디오 압축 아티팩트들과 유사한, 비디오들에서 일부 가시적인 패치들을 생성한다.\n' +
      '\n' +
      '**해상도** 우리의 2단계 모델 캐스케이드는 \\(512\\times 288\\)px 해상도로 비디오를 생성한다. 주어진 프롬프트에 맞춰 비디오 콘텐츠를 생성하고 시간적으로 일관된 움직임을 제시하는 것은 비디오 생성에서 가장 중요한 문제이며, 이러한 범주에서 가능한 아티팩트는 기준선과의 비교에서 볼 수 있듯이 \\(512\\times 288\\)px 해상도에서 이미 볼 수 있다. 우리는 또한 캐스케이드 모델 단계가 사용된 이전 단계 생성기에 독립적으로 훈련되고 불가지론적이라는 점에 주목한다. 따라서, 업샘플러가 \\(512\\times 288\\)px에서 더 높은 분해능으로 주어질 때, 기준선에 대해 \\(512\\times 288\\)px 분해능으로 나타난 모든 개선은 그에 상응하는 향상된 품질의 더 높은 분해능 결과를 생성할 것으로 예상된다. 추가 캐스케이드 단계의 통합을 향후 작업의 흥미로운 장소로 간주한다.\n' +
      '\n' +
      '### Societal Impact\n' +
      '\n' +
      '텍스트-비디오 생성 모델은 빠르게 진화하고 있으며[4, 13, 21, 62] 아티스트 및 디지털 콘텐츠 크리에이터와 같은 훈련된 전문가만 접근하면 창의성을 표현할 수 있는 새롭고 강력한 방법으로 사용자에게 권한을 부여하겠다는 약속을 가지고 있다. 이러한 개선으로 인해 생성된 결과가 사악한 개인이 유해하거나 속이는 콘텐츠를 생성할 가능성과 함께 실제로 인식될 수 있는 더 큰 위험이 발생한다. 우리의 모델은 훈련 중에 광범위한 개념에 노출되며 여과되지 않은 인터넷 데이터에 대해 훈련된 T5 [39] 텍스트 인코더를 사용하여 이러한 가능한 사용으로부터 보호할 필요가 있다. 또한, 모델은 데이터 세트에 존재할 수 있는 잠재적인 바이어스가 모델 출력에 반영될 수 있음을 암시하는 학습 데이터 분포에 따라 데이터를 생성한다. 오용을 방지하기 위해 모델을 공개적으로 액세스하지 않고 데이터 청소, 신속한 필터링 및 출력 필터링 기술 및 추가 보호 장치로 워터마킹을 배치할 계획이다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>