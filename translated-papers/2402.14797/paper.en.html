<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis\n' +
      '\n' +
      'Willi Menapace1,2,* Aliaksandr Siarohin1 Ivan Skorokhodov1 Ekaterina Deyneka1 Tsai-Shien Chen1,3,* Anil Kag1 Yuwei Fang1 Aleksei Stoliar1 Elisa Ricci2,4 Jian Ren1 Sergey Tulyakov1\n' +
      '\n' +
      'Snap Inc.1 University of Trento2 UC Merced3 Fondazione Bruno Kessler4\n' +
      '\n' +
      'snap-research.github.io/snapvideo\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Contemporary models for generating images show remarkable quality and versatility. Swayed by these advantages, the research community repurposes them to generate videos. Since video content is highly redundant, we argue that naively bringing advances of image models to the video generation domain reduces motion fidelity, visual quality and impairs scalability. In this work, we build Snap Video, a video-first model that systematically addresses these challenges. To do that, we first extend the EDM framework to take into account spatially and temporally redundant pixels and naturally support video generation. Second, we show that a U-Net--a workhorse behind image generation--scales poorly when generating videos, requiring significant computational overhead. Hence, we propose a new transformer-based architecture that trains 3.31 times faster than U-Nets (and is \\(\\sim\\)4.5 faster at inference). This allows us to efficiently train a text-to-video model with billions of parameters for the first time, reach state-of-the-art results on a number of benchmarks, and generate videos with substantially higher quality, temporal consistency, and motion complexity. The user studies showed that our model was favored by a large margin over the most recent methods.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Creating and sharing visual content is one of the key ways for people to express themselves in the digital world. Accessible to only professionals in the past, the capability to create [30, 40, 43, 69] and edit [6, 37, 42] images with stunning quality and realism was unlocked to everyone by the advent of large text-to-image models and their variations.\n' +
      '\n' +
      'Fueled by this progress, large-scale text-to-video models [4, 13, 21, 48, 62] are rapidly advancing too. Current large-scale diffusion-based video generation frameworks are strongly rooted into their image counterparts [4, 13]. The availability of consolidated image generation architectures such as U-Nets [41] with publicly-available image-pretrained models [40] made them a logical foundation onto which to build large-scale video generators with the main architectural modifications focusing on the insertion of ad-hoc layers to capture temporal dependencies [4, 13, 21, 48, 62]. Similarly, training is performed under image-based diffusion frameworks with the model being applied both to videos and to a separate set of images to improve the diversity of the results [13, 21, 22, 48].\n' +
      '\n' +
      'We argue that such an approach is suboptimal under multiple aspects which we systematically address in this work. First, image and video modalities present intrinsic differences given by the similarity of content in successive video frames [7, 13]. By analogy, image and video compression algorithms are based on vastly different approaches [33]. To address this issue, we rewrite the EDM [25] framework with a focus on high-resolution videos. Differently from past work where videos were treated as a sequence of images, we perform joint video-image training by treating images as _high frame-rate videos_ to avoid modality mismatches introduced by the absence of the temporal dimension within purely image-based training. Second, the widely adopted U-Net [41] architecture is required to fully processes each video frame. This increases computational overhead compared to purely text-to-image models, posing a very practical limit on model scalability. The latter is a critical factor in obtaining high-quality of results [13, 21]. Extending U-Net-based architectures to naturally support spatial and temporal dimensions requires volumetric attention operations, which have prohibitive computational demands. Inability to do so affects the outputs, resulting in _dynamic images_ or motion artifacts being generated instead of videos with coherent and diverse actions.\n' +
      '\n' +
      'Following our compression analogy, we propose to leverage repetition between frames and introduce a scalable transformer architecture that treats spatial and temporal dimensions as a single, compressed, 1D latent vector. This highly compressed representation allows us to perform spatio-temporal computation jointly and enables modelling of complex motions. Our architecture is inspired by FIT [8], which we scale to billions of parameters for the first time. Compared to U-Nets, our model features a significant \\(3.31\\times\\) reduction in training time and \\(4.49\\times\\) reduction in inference time while achieving higher generation quality.\n' +
      '\n' +
      'We evaluate Snap Video on the widely-adopted UCF101 [55] and MSR-VTT [65] datasets. Our generator shows state-of-the-art performance across the range of benchmarks with particular regard to the quality of the generated motion. Most interestingly, we performed a number of user studies against the most recent open- and close-source methods and found that according to the participants of the study our model features photorealism comparable to Gen-2 [11], while being significantly better than Pika [1] and Floor33 [17]. Most excitedly, the preference of user-study participants favoured Snap Video by a large margin when text alignment and motion quality were assessed. Compared to Gen-2 [11] on prompt-video alignment our model was preferred in 81% of cases (80% against Pika [1], 81% against Floor33 [17]), generated most dynamic videos with most amount of motion (96% against Gen2 [11], 89% against Pika [1], 88% against Floor33 [17]) and had the best motion quality (79% against Gen-2 [11], 71% against Pika [1], 79% against Floor33 [17]).\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Video Generation** Video generation is a challenging and long-studied task. Due to its complexity, a large number of works focus on modeling narrow domains [5, 9, 12, 28, 35, 44, 47, 49, 56, 58, 59, 66, 70, 71] and adopt adversarial training [5, 9, 28, 44, 47, 49, 58, 59, 71] or autoregressive generation techniques [12, 35, 56, 66, 70]. To address the narrow domain limitation, the task of text-to-video generation was proposed [34] and both autoregressive models [23, 34, 61, 63, 64] and GANs [29] emerged.\n' +
      '\n' +
      'The recent success of diffusion models in the context of text-to-image generation [3, 40, 43] fostered tremendous progress in the task [2, 4, 13, 16, 17, 21, 22, 32, 48, 62, 67, 72]. ImagenVideo [21] and Make-A-Video [48] propose a deep cascade of temporal and spatial upsamplers to generate videos and jointly train their models on image and video datasets. PYoCo [13] introduces a correlated noise model to capture similarities between video frames. Video LDM [4] adopts a latent diffusion paradigm where a pre-trained latent image generator and latent decoder are fine-tuned to generate temporally coherent videos. AnimateDiff [16] freezes a pre-trained latent image generator and trains only a newly inserted motion modeling module. These works employ U-Nets with separable spatial and temporal computation which poses a limitation on motion modeling capabilities. VideoFactory [62] improves upon this paradigm by proposing a Swapped Spatiotemporal Cross-Attention that improves interactions between the spatial and temporal modalities along 3D windows.\n' +
      '\n' +
      'Differently from this corpus of works which adapts the U-Net [41] architecture to the video generation task, we show that employing transformer-based FIT [8] architectures results in significant training time savings, scalability improvements, and performance increase thanks to their learnable compressed video representation. In particular, we show that the global joint spatiotemporal modeling strategy enabled by our compressed video representation results in significant improvements in temporal consistency and motion modeling capabilities.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '**High-Resolution Generation** Different approaches have been proposed to enable the generation of high-resolution outputs. Cascaded diffusion models [3, 13, 21, 43, 48] adopt a set of independent diffusion models designed to successively upsample the results of the previous step. Latent diffusion models [2, 4, 17, 40, 72] make use of a pretrained autoencoder to encode the input into a low-dimensional set of latent vectors and learn a diffusion model on this latent representation.\n' +
      '\n' +
      'A different family of methods generates high-resolution outputs end-to-end without employing cascades of models or latent diffusion. Simple Diffusion [24] and _Chen_[7] directly generate high-resolution images by adapting the noise schedule of the diffusion process. f-DM [14] and RDM [57] design a diffusion process that seamlessly transitions between different resolutions. MDM [15] proposes a strategy where a single model is trained to simultaneously denoise inputs at progressively higher resolutions.\n' +
      '\n' +
      'In this work, we adopt a two-stage cascaded model out of two considerations: (i) it avoids temporal inconsistencies in the forms of flickering of high-frequency details that may be introduced by latent autoencoders [4], (ii) it increases model capacity with respect to an end-to-end model by creating two specialized models, one for the low resolution focusing on motion modeling and scene structure, and one for the high-resolution, focusing on high-frequency details.\n' +
      '\n' +
      '**Diffusion Frameworks** Diffusion generative models are a set of techniques modeling generation as a pair of processes: a forward process progressively destructing a sample with noise, and a reverse process modeling generation as the progressive denoising of a sample. Different formulations of diffusion models have been proposed in the literature. Denoising Diffusion Probabilistic Models (DDPMs) [20, 50] formulate the forward and backward process as Markov chains. Score-based Generative Models (SGMs) [51, 52] model the score of the probability density function of a series of data distributions perturbed with increasing levels of noise, _i.e_. the direction of largest increase in the data log probability density function. An avenue of works [53, 54] generalizes DDPMs and SGMs to infinite noise levels through Stochastic Differential Equations (SDEs). In this work, we adopt the SGM framework of EDM [25] which we reformulate for the generation of high-resolution videos.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      'We propose the generation of high-resolution videos by rewriting the EDM [25] diffusion framework for high-dimensional inputs and proposing an efficient transformer architecture based on FITs [8] which we scale to billions of parameters and tens of thousands input patches. Sec. 3.1 provides an introduction to the EDM framework, Sec 3.2 highlights the challenges of applying diffusion frameworks to high dimensional inputs and proposes a revisited EDM-based diffusion framework. Sec. 3.3 proposes a method to reduce the gap between image and video modalities for joint training. Finally, Sec. 3.4 describes our scalable video generation architecture, while Sec. 3.5 and Sec. 3.6 respectively describe the training and inference procedures.\n' +
      '\n' +
      '### Introduction to EDM\n' +
      '\n' +
      'Diffusion models have achieved remarkable success in image and video generation. Among the proposed frameworks, _Karras et al_. [25] provide a unified view of common diffusion frameworks and formulate EDM. EDM defines a variance-exploding forward diffusion process \\(p(\\mathbf{x}_{\\mathbf{\\sigma}}|\\mathbf{x})\\sim\\mathcal{N}(\\mathbf{x},\\mathbf{\\sigma}^{2}\\mathbf{ I})\\), where \\(\\mathbf{\\sigma}\\in[\\mathbf{\\sigma}_{\\text{min}},\\mathbf{\\sigma}_{\\text{max}}]\\) represents the diffusion timestep coinciding with the standard deviation of the applied noise, and \\(\\mathbf{x}_{\\mathbf{\\sigma}}\\) represents the data at the current noise level. A denoiser function \\(\\mathcal{D}_{\\theta}\\) is learned to model the reverse process using the denoising objective:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{D}_{\\theta})=\\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{ \\epsilon}}\\Big{[}\\lambda(\\mathbf{\\sigma})\\;\\big{\\|}\\mathcal{D}_{\\theta}(\\mathbf{x}_{ \\mathbf{\\sigma}})-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\lambda\\) is the loss weighting function, \\(\\mathbf{x}\\sim p_{\\text{data}}\\) is a data sample, \\(\\mathbf{\\epsilon}\\) is gaussian noise, and \\(\\mathbf{\\sigma}\\sim p_{\\text{train}}\\) is sampled from a training distribution. \\(\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}})\\) is defined as:\n' +
      '\n' +
      '\\[\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}})=c_{\\text{out}}(\\mathbf{ \\sigma})\\mathcal{F}_{\\theta}\\left(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{ \\sigma}}\\right)+c_{\\text{skip}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}, \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathcal{F}_{\\theta}\\) is a neural network, and \\(c_{\\text{out}}\\), \\(c_{\\text{skip}}\\) and \\(c_{\\text{in}}\\) represent scaling functions. In particular, the denoising objective \\(\\mathcal{L}(\\mathcal{F}_{\\theta})\\) can equivalently be expressed in terms of \\(\\mathcal{F}_{\\theta}\\) as:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{F}_{\\theta})=\\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{ \\epsilon}}\\Big{[}w(\\mathbf{\\sigma})\\;\\big{\\|}\\mathcal{F}_{\\theta}(c_{\\text{in}}( \\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}})-c_{\\text{mm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text {tgt}}\\big{\\|}_{2}^{2}\\Big{]}, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\mathcal{F}_{\\text{tgt}}\\) represents the training target, \\(c_{\\text{mm}}\\) is a normalization factor, and \\(w\\) is a weighting function. These forms, derived in Appx. D, are presented in Tab. 1.\n' +
      '\n' +
      'A second order Runge-Kutta sampler is proposed to reverse the diffusion process and produce sample \\(\\mathbf{x}\\) starting from gaussian noise \\(\\mathbf{x}_{\\mathbf{\\sigma}_{\\text{max}}}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{\\sigma}_{\\text{ max}}^{2}\\mathbf{I})\\).\n' +
      '\n' +
      '### EDM for High-Resolution Video Generation\n' +
      '\n' +
      'EDM is originally proposed as an image generation framework and its parameters are optimized for \\(64\\times 64\\)px image generation. Alterations in spatial resolution or the introduction of videos with shared content between frames allow the denoising network to trivially recover a noisy frame in the original resolution with higher signal-to-noise-ratio (\\(SNR\\)), which the original framework was designed to see at lower noise levels. To see why, consider a noisy video \\(\\mathbf{x}_{\\mathbf{\\sigma}}\\in\\mathbb{R}^{T\\times s\\cdot H\\times s\\cdot W}\\sim \\mathcal{N}(\\mathbf{x},\\mathbf{\\sigma}^{2}\\mathbf{I})\\) where \\(T\\) is the number of frames and \\(s\\) is an upsampling factor. We build the corresponding clean and noisy frames at original resolution \\(\\mathbf{\\tilde{x}},\\mathbf{\\tilde{x}}_{\\mathbf{\\sigma}}\\in\\mathbb{R}^{1\\times H\\times W}\\) by averaging values in each \\(T\\times s\\times s\\) block of pixels. As a consequence of averaging, the noise\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      'similarly limit possibilities for joint spatio-temporal modeling [62]. We argue that treating spatial and temporal modeling in a separable way [4, 13, 21, 48] causes motion artifacts, temporal inconsistencies or generation of _dynamic images_ rather than videos with vivid motion. Video frames, however, contain spatially and temporally redundant content that is amenable to compression [33]. We argue that learning and operating on a compressed video representation and jointly modeling the spatial and temporal dimensions are necessary steps to achieve the scalability and motion-modeling capabilities required for high-quality video generation.\n' +
      '\n' +
      'FITs [8] are efficient transformer-based architectures that have recently been proposed for high-resolution image synthesis and video generation. Their main idea, summarized in Fig. 3 is that of learning a compressed representation of their input through a set of learnable latent tokens and of focusing computation on this learnable latent space, allowing input dimensionality to grow with little performance penalty. First, FITs perform patchification of the input and produce a sequence of patch tokens which are later divided into groups. A set of latent tokens is then instantiated and a sequence of computational blocks is applied. Each block first performs a cross attention "read" operation between latent tokens and conditioning signals such as the diffusion timestep, then an additional groupwise "read" cross attention operation between latent and patch tokens of corresponding groups to compress patch information, applies a series of self attention operations to the latent tokens, and performs a groupwise "write" cross attention operation that decompresses information in the latent tokens to update the patch tokens. Finally, the patch tokens are projected back to the pixel space to form the output. Self conditioning is applied on the set of latent tokens to preserve the compressed video representation computed in previous sampling steps.\n' +
      '\n' +
      'While promising, these architectures have not yet been scaled to the billion-parameters size of state-of-the-art U-Net-based video generators, nor they have been applied to high-resolution video generation. In the following, we highlight the architectural considerations necessary to achieve these goals. Temporal modeling is a fundamental aspect of a high-quality video generator. FITs produce patch tokens by considering three dimensional patches of size \\(T_{p}\\times H_{p}\\times W_{p}\\) spanning both the spatial and temporal dimensions. We find values of \\(T_{p}>1\\) to limit temporal modeling performance, so we consider patches spanning the spatial dimension only. In addition, similarly to patches, FITs group patch tokens into groups spanning both the temporal and spatial dimensions, and perform cross attention operations group by group. We observe that the temporal size of each group should be configured so that each group covers all \\(T\\) video frames for best temporal modeling. Furthermore, videos contain more information with respect to images due to the presence of the temporal dimension, thus we increase the number of latent tokens representing the size of the compressed space in which joint spatiotemporal computation is performed. Finally, FITs make use of local layers which perform self attention operations on patch tokens corresponding to the same group. We find this operation to be computationally expensive for large amounts of patch tokens (147.456 for our largest resolution) and replace it with a feed forward module after each cross attention "read" or "write" operation.\n' +
      '\n' +
      'Figure 3: (a-left) U-Net-based text-to-image architectures are adapted to do video generation by inserting temporal layers applied sequentially with spatial layers, creating separable spatiotemporal blocks. Spatial computation is repeated for each frame independently, limiting scalability. (a-right) Our scalable transformer-based model jointly performs spatial and temporal computation on a learnable compressed video representation for improved motion modeling and scalability. (b) The proposed Snap Video FIT architecture. Given a noisy input video \\(\\mathbf{x_{\\sigma}}\\), the model estimates the denoised video \\(\\mathbf{\\hat{x}_{\\sigma}}\\) by recurrent application of FIT blocks. Each block reads information from the patch tokens into a small set of latent tokens on which computation is performed. The results are written to the patch tokens. Conditioning information in the form of text embeddings, noise level \\(\\sigma\\), frame-rate \\(\\nu\\) and resolution \\(r\\) is provided through an additional read operation.\n' +
      '\n' +
      'Our model makes use of conditioning information represented by a sequence of conditioning tokens to control the generation process. In addition to the token representing the current \\(\\boldsymbol{\\sigma}\\), to enable text conditioning, we introduce a T5-11B [39] text encoder extracting text embeddings from the input text. To support variable video framerates and large differences in resolution and aspect ratios in the training data, we concatenate additional tokens representing the framerate and original resolution of the current input.\n' +
      '\n' +
      'To generate high-resolution outputs, we implement a model cascade consisting of a first-stage model producing \\(36\\times 64\\)px videos and a second-stage upsampling model producing \\(288\\times 512\\)px videos. To improve upsampling quality, we corrupt the second-stage low-resolution inputs with a variable level of noise during training [21, 43] and during inference apply a level of noise to the first-stage outputs obtained by hyperparameter search.\n' +
      '\n' +
      'We present detailed model hyperparameters in Appx. A.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      'We train Snap Video using the LAMB [68] optimizer with a learning rate of \\(5e^{-3}\\), a cosine learning schedule and a total batch size of 2048 videos and 2048 images, achievable thanks to our scalable video generator architecture. We train the first-stage model over 550k steps and finetune the second-stage model on high-resolution videos starting from the first-stage model weights for 370k iterations. Following the observations in Sec 3.2, we pose \\(\\boldsymbol{\\sigma}_{\\text{in}}=s\\sqrt{T}\\). Considering videos with \\(T=16\\) frames and the original \\(64\\)px resolution for which EDM was designed, we set \\(\\boldsymbol{\\sigma}_{\\text{in}}=4\\) for the first-stage and \\(\\boldsymbol{\\sigma}_{\\text{in}}=32\\) for the second-stage model.\n' +
      '\n' +
      'We present training details and parameters in Appx. B.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      'We produce video samples from gaussian noise and user-provided conditioning information using the deterministic sampler of [25] and our two-stage cascade. We use 256 sampling steps for the first-stage and 40 for the second-stage model, and employ classifier free guidance [19] to improve text-video alignment (see Appx. C.1) unless otherwise specified. We find dynamic thresholding [43] and oscillating guidance [21] to consistently improve sample quality.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      'In this section, we perform evaluation of Snap Video against baselines and validate our design choices. Sec. 4.1 introduces the employed datasets, Sec. 4.2 defines the evaluation protocol, Sec. 4.3 shows ablations of our diffusion framework and architectural choices, Sec. 4.4 quantitatively compares our method to state-of-the-art large-scale video generators and Sec. 4.5 performs qualitative evaluation. We complement evaluation by showcasing samples in the _Appendix_ and _Website_.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'We train our models on an internal dataset consisting of 1.265M images and 238k hours of videos, each with a corresponding text caption. Due to the difficulty in acquiring high-quality captions for videos, we develop a video captioning model that we use to produce synthetic video captions for the portion of videos in the dataset missing such annotation.\n' +
      '\n' +
      'We make use of the following datasets for evaluation which are never observed during training:\n' +
      '\n' +
      '**UCF-101**[55] is a video dataset containing 13.320 \\(320\\times 240\\)px Youtube videos from 101 action categories.\n' +
      '\n' +
      '**MSR-VTT**[65] is a dataset containing 10.000 \\(320\\times 240\\)px web-crawled videos, each manually annotated with 20 text captions. The test set contains 2.990 videos and 59.800 corresponding captions.\n' +
      '\n' +
      '### Evaluation Protocol\n' +
      '\n' +
      'To validate the choices operated on the diffusion framework and on model architecture, present method ablations performed in \\(64\\times 36\\)px resolution using the first-stage model only, and compute FID [18], FVD [60] and CLIPSIM [63] metrics against the test set of our internal dataset on 50k generated videos.\n' +
      '\n' +
      'To evaluate our method against baselines, we follow the protocols highlighted in [4, 13, 32, 48, 62, 72] for zero\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) & Train Thr. \\(\\downarrow\\) & Inf. Thr. \\(\\downarrow\\) \\\\ \\hline U-Net 85M [10] & 8.21 & 45.94 & 0.2319 & 133.2 & 49.6 \\\\ U-Net 284M [10] & 4.90 & 23.76 & 0.2391 & 230.3 & 105.1 \\\\ Snap Video FIT 500M & 3.07 & 27.79 & 0.2459 & 69.5 & 254.1 \\\\ Snap Video FIT 3.9B & **2.51** & **12.31** & **0.2579** & 526.0 & 130.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Performance of different architectures and model sizes on our internal dataset in \\(64\\times 36\\)px resolution. We observe strong performance gains with scaling and note that FITs present better performance with improved speed with respect to U-Nets. Train and inference throughputs in ms/video/GPU.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline  & \\(\\boldsymbol{\\sigma}_{\\text{data}}\\) & \\(\\boldsymbol{\\sigma}_{\\text{in}}\\) & Imgs. as Videos & FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) \\\\ \\hline (i) & 0.5 & 1.0 & ✓ & 6.58 & 39.95 & 0.2370 \\\\ (ii) & 0.5 & 4.0 & ✓ & 4.03 & 31.00 & 0.2449 \\\\ \\hline (iv) & 1.0 & 2.0 & ✓ & 4.45 & 34.89 & 0.2428 \\\\ \\hline (iii) & 1.0 & 1/4.0 & ✗ & 3.50 & 24.88 & 0.2469 \\\\ \\hline Ours & 1.0 & 4.0 & ✓ & 3.07 & 27.79 & 0.2459 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Ablation of different diffusion process configurations varying \\(\\boldsymbol{\\sigma}_{\\text{data}}\\), input scaling \\(\\boldsymbol{\\sigma}_{\\text{in}}\\), and treatment of images as infinite-framerate videos, evaluated on our internal dataset in \\(64\\times 36\\)px resolution.\n' +
      '\n' +
      'shot evaluation on the UCF-101 [55] and MSR-VTT [65] datasets. We generate 16 frames videos in \\(512\\times 288\\)px resolution at 24fps for all settings. We evaluate both at the native \\(512\\times 288\\)px resolution with 16:9 aspect ratio and in the \\(288\\times 288\\)px square aspect ratio typically employed on these benchmarks. We note that the evaluation protocols of [4, 13, 32, 48, 62, 72] present different choices regarding the number of generated samples, distribution of class labels, choice of text prompts. We make use of the following evaluation parameters:\n' +
      '\n' +
      '**Zero-shot UCF-101**[55] We generate 10.000 videos [4, 62] sampling classes with the same distribution as the original dataset. We produce a text prompt for each class label [13] and compute FVD [60] and Inception Score [46].\n' +
      '\n' +
      '**Zero-shot MSR-VTT**[65] We generate a video sample for each of the 59.800 test prompts [13, 48] and compute CLIP-FID [27] and CLIPSIM [63].\n' +
      '\n' +
      'To provide a more complete performance assessment and compare against state-of-the-art closed-source methods not reporting results for these benchmarks, we perform a user study evaluating photorealism, video-text-alignment and, most importantly, the quantity and quality of the generated motion, important characteristics of a video generator that may signal the generation of _dynamic images_, videos with dim motion, or motion artifacts rather than videos with vivid and high-quality motion.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      'To evaluate the proposed FIT architecture, we consider the U-Net of [10], which we adapt to the video generation setting by interleaving temporal attention operations. We consider two U-Net variants of different capacities and a smaller variant of our FIT to evaluate the scalability of both architectures. We detail the architectures in Appx. A and show results in Tab. 2.\n' +
      '\n' +
      'Our 500M parameters FIT trains 3.31\\(\\times\\) faster than the baseline 284M parameters U-Net, performs inference 4.49\\(\\times\\) faster and surpasses it in terms of FID and CLIPSIM. In addition, both FITs and U-Nets show strong performance gains with scaling. Our largest FIT scales to 3.9B parameters with only a 1.24\\(\\times\\) increase in inference time with respect to the 284M U-Net.\n' +
      '\n' +
      'To evaluate the choices operated on our diffusion framework, we ablate different configurations of the diffusion process using our 500M FIT architecture. We produce the following variations: (i) the original EDM framework, (ii) our scaled diffusion framework with EDM \\(\\mathbf{\\sigma}_{\\text{data}}\\), (iii) our framework with a reduced value of \\(\\mathbf{\\sigma}_{\\text{in}}\\), (iv) our framework with images not treated as infinite-frame-rate videos. Our framework improves over EDM under all metrics (i) and shows benefits in setting \\(\\mathbf{\\sigma}_{\\text{data}}=1\\), an effect that we attribute to the creation of a training target and loss weighting matching the widely used \\(\\mathbf{v}\\)-prediction formulation of _Salimans et al_. [45] (see Tab. 1). Using \\(\\mathbf{\\sigma}_{\\text{in}}<s\\sqrt{T}\\) (see Sec. 3.2) impairs performance (iii). Finally, treating images as infinite-frame-rate videos consistently improves FID.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      'We perform comparison of Snap Video against baselines on the UCF101 [55], and MSR-VTT [65] datasets respectively in Tab. 4 and Tab. 5. FID and FVD video quality metrics show improvements over the baselines which we attribute to the employed diffusion framework and joint spatiotemporal modeling performed by our architecture. On UCF101, our method produces the second-best IS of \\(38.89\\), demonstrating good video-text alignment. While our method surpasses Make-A-Video [48] on UCF101, we note that it produces a lower CLIPSIM score on MSR-VTT. We attribute this behavior to the use of T5 [39] text embeddings in place of the commonly used CLIP [38] embeddings which were observed [43] to produce higher text-image alignment despite similar CLIPSIM.\n' +
      '\n' +
      'To provide a comprehensive evaluation we run a user study to evaluate photorealism, video-text alignment, quantity of motion and quality of motion, important aspects of a video generator. Three publicly-accessible state-of-the-art\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & FVD \\(\\downarrow\\) & FID \\(\\downarrow\\) & IS \\(\\uparrow\\) \\\\ \\hline CogVideo [23] (Chinese) & 751.3 & - & 23.55 \\\\ CogVideo [23] (English) & 701.6 & - & 25.27 \\\\ MagicVideo [72] & 655 & - & - \\\\ LVDM [17] & 641.8 & - & - \\\\ Video LDM [4] & 550.6 & - & 33.45 \\\\ VideoFactory [62] & 410.0 & - & - \\\\ Make-A-Video [48] & 367.2 & - & 33.00 \\\\ PYCo [13] & 355.2 & - & 47.46 \\\\ \\hline Snap Video (\\(288\\times 288\\) px) & 260.1 & 39.0 & 38.89 \\\\ Snap Video (\\(512\\times 288\\) px) & 200.2 & 28.1 & 38.89 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Zero-shot evaluation results on UCF101 [55].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & CLIP-FID \\(\\downarrow\\) & FVD \\(\\downarrow\\) & CLIPSIM \\(\\uparrow\\) \\\\ \\hline NUWA [64] (Chinese) & 47.68 & - & 0.2439 \\\\ CogVideo [23] (Chinese) & 24.78 & - & 0.2614 \\\\ CogVideo [23] (English) & 23.59 & - & 0.2631 \\\\ MagicVideo [72] & - & 998 & - \\\\ LVDM [17] & - & - & 0.2381 \\\\ Latent-Shift [2] & 15.23 & - & 0.2773 \\\\ Video LDM [4] & - & - & 0.2929 \\\\ VideoFactory [62] & - & - & 0.3005 \\\\ Make-A-Video [48] & 13.17 & - & 0.3049 \\\\ PYCo [13] & 9.73 & - & - \\\\ \\hline Snap Video (\\(288\\times 288\\) px) & 8.48 & 110.4 & 0.2793 \\\\ Snap Video (\\(512\\times 288\\) px) & 9.35 & 104.0 & 0.2793 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Zero-shot evaluation results on MSR-VTT [65].\n' +
      '\n' +
      'video generators are considered: Gen-2 [11], PikaLabs [1] and Floor33 [17]. We filter a set of 65 prompts from [31] describing scenes with vivid motions, and generate a video for each method with default options. We ask the participants to express preference between paired samples from Snap Video and each baseline, gathering votes from 5 users for each sample. Results are shown in Tab. 6 and video samples provided along with the employed prompt list in Appx. C.2 and in the _Website_. Our method produces results with photorealism comparable to Gen-2, while surpassing PikaLab and Floor33, and outperforms all baselines with respect to video-text alignment. Most importantly, we note that baselines often produce _dynamic images_, videos with dim motion, or videos with motion artifacts, a finding we attribute to the challenges in modeling large motion. In contrast, our method, thanks to the joint spatiotemporal modeling approach, produces vivid and high-quality motion as shown by the motion metrics.\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      'In this section, we perform qualitative evaluation of our framework. In Fig. 4, Appx. C.3 and the _Website_, we present qualitative results comparing our method to state-of-the-art generators [4, 13, 21, 48] on samples publicly released by the authors. While such prompts might have been selected to highlight strengths of the baselines, our method produces more photorealistic samples aligned to the text descriptions. Most importantly, our samples present vivid and high-quality motion avoiding flickering artifacts that are present in the baselines due to temporal inconsistencies. We accompany qualitative evaluation with a user study performed on the same set of samples in Appx. C.2.\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      'In this work, we highlight the shortcomings of diffusion processes and architectures commonly used in text-to-video generation, and systematically address them by treating videos as first-class citizens. First, we propose a modification to the EDM [25] diffusion framework for the generation of high-resolution videos and treat images as high frame-rate videos to avoid image-video modality mismatches. Second, we replace U-Nets [41] with efficient transformer-based FITs [8] which we scale to billions of parameters. Thanks to their learnable compressed representation of videos, they significantly improve training times, scalability and performance with particular regards to temporal consistency and motion modeling capabilities due to the joint spatiotemporal modeling on the compressed representation. When evaluated on UCF101 [55] and MSR-VTT [65] and in user studies, Snap Video attains state-of-the-art performance with particular regard to the quality of the modeled motion.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{5}{c}{Photorealism Video-Text Align. Mot. Quant. Mot. Qual.} \\\\ \\hline Gen-2 [11] & 44.3 & 81.0 & 96.0 & 78.7 \\\\ PikaLab [1] & 61.5 & 80.3 & 89.2 & 70.5 \\\\ Floor33 [17] & 76.3 & 80.9 & 88.0 & 79.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: User study on photorealism, video-text alignment, motion quantity and quality against publicly-accessible video generators on 65 dynamic scene prompts. % of votes in favor of our method.\n' +
      '\n' +
      'Figure 4: Qualitative results comparing Snap Video to state-of-the-art video generators on publicly available samples. While baseline methods present motion artifacts (top-left, top-right, bottom-right) or produce _dynamic images_ (bottom-left), our method produces more temporally coherent motion. Best viewed in the _Website_.\n' +
      '\n' +
      '## 6 Acknowledgements\n' +
      '\n' +
      'We would like to thank Oleksksil Popov, Artem Sinitsyn, Anton Kuzmenko, Vitalii Kravchuk, Vadym Hrebennyk, Grygorii Kozhemiak, Tetiana Shcherbakova, Svitlana Harkusha, Oleksandr Yurchak, Andrii Buniakov, Maryna Marienko, Maksym Garkusha, Brett Krong, Anastasiia Bondarchuk for their help in the realization of video presentations, stories and graphical assets, Colin Eles, Dhritiman Sagar, Vitalii Osykov, Eric Hu for their supporting technical activities, Maryna Diakonova for her assistance with annotation tasks.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Pika lab discord server. [https://www.pika.art/](https://www.pika.art/). Accessed: 2023-11-01.\n' +
      '* [2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. _arXiv_, 2023.\n' +
      '* [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. _ArXiv_, 2022.\n' +
      '* [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [5] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei A Efros, and Tero Karras. Generating long videos of dynamic scenes. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [6] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [7] Ting Chen. On the importance of noise scheduling for diffusion models. _arXiv_, 2023.\n' +
      '* [8] Ting Chen and Lala Li. Fit: Far-reaching interleaved transformers. _arXiv_, 2023.\n' +
      '* [9] Aidan Clark, Jeff Donahue, and Karen Simonyan. Efficient video generation on complex datasets. _arXiv_, 2019.\n' +
      '* [10] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [11] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [12] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In _Proceedings of the European Conference of Computer Vision (ECCV)_, 2022.\n' +
      '* [13] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [14] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, and Josh Susskind. f-dm: A multi-stage diffusion model via progressive signal transformation. _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [15] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, and Navdeep Jaitly. Matryoshka diffusion models. _arXiv_, 2023.\n' +
      '* [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv_, 2023.\n' +
      '* [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. _arXiv_, 2023.\n' +
      '* [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.\n' +
      '* [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv_, 2022.\n' +
      '* [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. _arXiv_, 2022.\n' +
      '* [22] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In _ICLR Workshop on Deep Generative Models for Highly Structured Data_, 2022.\n' +
      '* [23] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideto: Large-scale pretraining for text-to-video generation via transformers. _arXiv_, 2022.\n' +
      '* [24] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, 2023.\n' +
      '* [25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [26] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv_, 2015.\n' +
      '* [27] Tuomas Kynkuanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes infrechet inception distance. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [28] Alex X. Lee, Richard Zhang, Frederik Ebert, P. Abbeel, Chelsea Finn, and S. Levine. Stochastic adversarial video prediction. _arXiv_, abs/1804.01523, 2018.\n' +
      '* [29] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2018.\n' +
      '* [30] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemergys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. _arXiv_, 2023.\n' +
      '* [31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. _arXiv_, 2023.\n' +
      '* [32] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan. Videofusion: Decomposed diffusion models for high-quality video generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [33] Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui Zhao, Shiqi Wang, and Shanshe Wang. Image and video compression with neural networks: A review. _IEEE Transactions on Circuits and Systems for Video Technology_, 2019.\n' +
      '* [34] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In _Proceedings of the 25th ACM International Conference on Multimedia_, 2017.\n' +
      '* [35] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. _PyTorch: An Imperative Style, High-Performance Deep Learning Library_. 2019.\n' +
      '* [37] Chenyang QI, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 2021.\n' +
      '* [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research (JMLR)_, 2022.\n' +
      '* [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. _arXiv_, 2021.\n' +
      '* [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention (MICCAI)_, 2015.\n' +
      '* [42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv_, 2022.\n' +
      '* [44] Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal gan, 2020.\n' +
      '* [45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.\n' +
      '* [47] Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Mostgan-v: Video generation with temporal motion styles. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [48] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. _arXiv_, 2022.\n' +
      '* [49] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning (ICML)_, 2015.\n' +
      '* [51] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* [52] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '\n' +
      '* [53] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models, 2021.\n' +
      '* [54] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* [55] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _arXiv_, 2012.\n' +
      '* [56] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In _International Conference on Machine Learning (ICML)_, 2015.\n' +
      '* [57] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. _arXiv_, 2023.\n' +
      '* [58] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In _International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '* [59] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '* [60] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv_, 2018.\n' +
      '* [61] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and D. Erhan. Phenaki: Variable length video generation from open domain textual description. In _International Conference on Learning Representations (ICLR)_, 2023.\n' +
      '* [62] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation. _arXiv_, 2023.\n' +
      '* [63] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. _ArXiv_, 2021.\n' +
      '* [64] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual synthesis pretraining for neural visual world creation. In _Proceedings of the European Conference of Computer Vision (ECCV)_, 2022.\n' +
      '* [65] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n' +
      '* [66] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogptv: Video generation using vq-vae and transformers. _arXiv_, 2021.\n' +
      '* [67] Sheng-Siang Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, and Nan Duan. Nuwa-xl: Diffusion over diffusion for extremely long video generation. In _Annual Meeting of the Association for Computational Linguistics_, 2023.\n' +
      '* [68] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. In _International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* [69] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. _Transactions on Machine Learning Research_, 2022.\n' +
      '* [70] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [71] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      'parameters. Sec. C.2 presents user study results. In Sec. C.3 and Sec. C.4 we show samples against baselines and additional samples from our method. In Sec. C.5, we describe how to generate longer videos at high framerate. Finally, Sec. C.6 presents evaluation details for UCF101 [55].\n' +
      '\n' +
      '### Sampler Parameters Ablations\n' +
      '\n' +
      'Choices in the sampling parameters can affect the performance of diffusion models significantly. In Fig. 5, we show the impact of classifier free guidance [19] on model\'s performance. We produce 10k samples using the first-stage model and report FVD and CLIPSIM at various guidance values, computed on 10k samples with a 40 step deterministic sampler. We find that classifier free guidance can improve both FVD and CLIPSIM, but notice increased sample saturation at high classifier free guidance scales. We adopt dynamic thresholding and oscillating classifier free guidance [43] which we find effective in reducing the phenomenon.\n' +
      '\n' +
      'In Fig. 6, we evaluate performance of the model under the same setting, but varying the number of sampling steps. We find that the model produces high-quality samples already at 64 steps and that FVD improves until 256 steps.\n' +
      '\n' +
      '### User Studies\n' +
      '\n' +
      'To complement the evaluation, we run a user study on a set of publicly released samples from Make-A-Video [48], PYoCo [13], Video LDM [4] and Imagen Video [21]. For each method, we collect publicly available samples and prompts, generate corresponding samples from Snap Video, and ask participants to express a preference in terms of photorealism, video-text alignment, motion quantity and quality, collecting 5 votes for each sample. Results are shown in Tab. 10 and samples are provided in Fig. 4, Fig. 7 and the _Website_. Our method shows increased photorealism with respect to the baselines, presents higher video-text alignment with respect to all methods except Imagen Video and consistently surpasses baselines in terms of motion quality (see flickering artifacts and temporally inconsistent backgrounds in scenes with large camera motion), a finding we attribute to our joint spatiotemporal modeling approach.\n' +
      '\n' +
      '### Qualitative Results Against Baselines\n' +
      '\n' +
      'In Fig. 7 and the _Website_, we present qualitative results of our method against baselines on samples publicly released by the authors of Make-A-Video [48], PYoCo [13], Video LDM [4] and Imagen Video [21]. Our method produces videos with natural motion and can handle scenes with large motion and camera changes while preserving temporal consistency. On the other hand, we observe that baselines often present flickering artifacts and temporally inconsistent objects in case of large motion.\n' +
      '\n' +
      'In Fig. 8 and the _Website_, we provide qualitative results comparing our method to publicly accessible state-of-the-art video generators including Gen-2 [11], PikalLab [1] and Floor33 [17]. Our method produces results that are more aligned to the prompts and, differently from the baselines, which often produce _dynamic images_, our method produces temporally coherent videos with large amounts of motion.\n' +
      '\n' +
      '### Additional Qualitative Results\n' +
      '\n' +
      '**Complex Prompts** We present in Fig. 9, Fig. 10 and the _Website_ additional samples generated by our method on a set of prompts gathered from ChatGPT, Make-A-Video [48], PYoCo [13], Video LDM [4], Imagen Video [21], and the Evalcrafter [31] benchmark. Our method can synthe\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multicolumn{5}{c}{Photorealism Video-Text Align. Mot. Quant. Mot. Qual.} \\\\ \\hline Imagen Video [21] & 66.9 & 54.3 & 49.4 & 56.3 \\\\ PYoCo [13] & 63.3 & 64.9 & 57.1 & 63.9 \\\\ Video LDM [4] & 61.7 & 64.4 & 62.2 & 65.8 \\\\ Make-A-Video [48] & 80.0 & 82.2 & 82.2 & 75.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: User study on photorealism, video-text alignment, motion quantity and quality on publicly available samples from closed-source methods. % of votes in favor of our method.\n' +
      '\n' +
      'Figure 5: FVD and CLIPSIM on our internal dataset in \\(64\\times 36\\)px resolution as a function of the classifier free guidance weight. Points represent weights of \\([0,0.5,1,1.5,2,3,4,5,6,7,8,10,12,14,16]\\).\n' +
      '\n' +
      'Figure 6: FVD on our internal dataset in \\(64\\times 36\\)px resolution as a function of the sampling steps and sampler type.\n' +
      '\n' +
      'size a large number of different concepts. Most importantly, it can produce videos with challenging motion including large camera movement, POV videos, and videos of fast-moving objects. Notably, the method maintains temporal consistency and avoids video flickering artifacts. We ascribe these motion modeling capabilities to the joint spatiotemporal modeling performed by our architecture.\n' +
      '\n' +
      '**Novel Views** We qualitatively evaluate the capabilities of the proposed method in producing novel views of objects. To do so, we build prompts using one of the following templates _Camera circling around a \\(\\langle\\)object\\(\\rangle\\)_, _Camera orbiting around a \\(\\langle\\)object\\(\\rangle\\)_ or _Camera moving around a \\(\\langle\\)object\\(\\rangle\\)_, where \\(\\langle\\)object\\(\\rangle\\) represents a placeholder for the object to generate. Results are shown in Fig. 11 and the _Website_. We find that the model is capable of generating plausible novel views of objects, suggesting that it possesses an understanding of the 3D geometry of the modeled objects.\n' +
      '\n' +
      '**Samples Diversity** We qualitatively assess the capabilities of the method of generating diverse samples for each prompt. In Fig. 12 and the _Website_ we show three samples produced for a set of prompts and note that the method is capable of producing diverse samples.\n' +
      '\n' +
      '**UCF 101** In Fig. 13 and the _Website_, we present qualitative results produced by our mehthod for zero-shot UCF101 [55] evaluation.\n' +
      '\n' +
      '### Hierarchical Video Generation\n' +
      '\n' +
      'We train our method to generate videos with a fixed number of frames and variable framerate. We exploit this characteristic and devise a hierarchical generation strategy to increase video duration and framerate by conditioning generation on previously generated frames. In particular, to condition generation on already available frames, we adopt the _reconstruction guidance_ method of _Ho et al._[22]. We define a hierarchy of progressively increasing framerate and start by autoregressively generating a video of the desired length at the lowest framerate, at each step using the last generated frame as the conditioning. Subsequently, for each successive framerate in the hierarchy, we autoregressively generate a video of the same length but conditioning the model on all frames that have already been generated at the lower framerate. We show samples in the _Website_.\n' +
      '\n' +
      '### Zero-Shot UCF101 Evaluation\n' +
      '\n' +
      'UFC101 [55] is a dataset of low-resolution Youtube videos. To better match our generated outputs to its distribution, we condition the model to produce videos with a low original resolution through the resolution conditioning mechanism (see Sec. 3.4). Following [13], since UCF101 class labels do not always have sufficiently descriptive names, we produce a prompt for each class which we report in the following: _applying eye makeup_, _applying lipstick_, _a person shooting with a bow_, _baby crawling_, _gymnast performing on a balance beam_, _band marching_, _baseball pitcher throwing baseball_, _a basketball player shooting basketball_, _dunking basketball in a basketball match_, _bench press in a gym_, _a person riding a bicycle_, _billiards_, _a woman using a hair dryer_, _a kid blowing candles on a cake_, _body weight squats_, _a person bowling on bowling alley_, _boxing punching bag_, _boxing training on speed bag_, _swimmer doing breast stroke_, _brushing teeth_, _a person doing clean and jerk in a gym_, _cliff diving_, _bowling in cricket match_, _batting in cricket match_, _cutting in kitchen_, _diver diving into a swimming pool from a springboard_, _drumming_, _two fencers have fencing match indoors_, _field hockey match_, _gymnast performing on the floor_, _group of people playing frisbee on the playground_, _swimmer doing front crawl_, _golfer swings and strikes the ball_, _haircuting_, _a person hammering a nail_, _an athlete performing the hammer throw_, _an athlete doing handstand push up_, _an athlete doing handstand walking_, _massagist doing head massage to man_, _an athlete doing high jump_, _group of people racing horse_, _person riding a horse_, _a woman doing hula hoop_, _man and woman dancing on the ice_, _athlete practicing javelin throw_, _a person juggling with balls_, _a young person doing jumping jacks_, _a person skipping with jump rope_, _a person kayaking in rapid water_, _knitting_, _an athlete doing long jump_, _a person doing lunges exercise in a gym_, _a group of soldiers marching in a parade_, _mixing in the kitchen_, _mopping floor_, _a person practicing munchuck_, _gymnast performing on parallel bars_, _a person tossing pizza dough_, _a musician playing the cello in a room_, _a musician playing the daf drum_, _a musician playing the indian dhol_, _a musician playing the flute_, _a musician playing the guitar_, _a musician playing the piano_, _a musician playing the sitar_, _a musician playing the tabla drum_, _a musician playing the violin_, _an athlete jumps over the bar_, _gymnast performing pommel horse exercise_, _a person doing pull ups on bar_, _boxing match_, _push ups_, _group of people rafting on fast moving river_, _rock climbing indoor_, _a person lifting on a rope in a gym_, _several people rowing a boat on the river_, _a man and a woman are salsa dancing_, _young man shaving beard with razor_, _an athlete practicing shot put throw_, _a teenager skateboarding_, _skier skiing down_, _jet ski on the water_, _a person is skydiving in the sky_, _soccer player juggling football_, _soccer player doing penalty kick in a soccer match_, _gymnast performing on still rings_, _sumo wrestling_, _surfing_, _kids swing at the park_, _a person playing table tennis_, _a person doing TaiChi_, _a person playing tennis_, _an athlete practicing discus throw_, _trampoline jumping_, _typing on computer keyboard_, _a gymnast performing on the uneven bars_, _people playing volleyball_, _walking with dog_, _a person standing doing pushups on the wall_, _a person writing on the blackboard_, _a person at a Yo-Yo competition_.\n' +
      '\n' +
      'Figure 7: Comparison of Snap Video to publicly released samples from Make-A-Video [48], PYoC [13], Video LDM [4] and Imagen Video [21]. Our method produces temporally coherent motion while avoiding video flickering. Best viewed on the _Website_.\n' +
      '\n' +
      'Figure 8: Comparison of Snap Video to the publicly accessible state-of-the-art video generators Gen-2 [11], PikaLab [1] and Floor33 [17]. Rather than producing _dynamic images_, our method generates videos with large amounts of temporally coherent motion. Best viewed on the _Website_.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      'Figure 10: Additional samples generated from Snap Video on a collection of prompts gathered from ChatGPT, baseline methods and the Evalcrafter [31] benchmark. Best viewed on the _Website_.\n' +
      '\n' +
      'Figure 11: Videos produced by Snap Video for prompts eliciting circular camera motion around each object. Best viewed on the _Website_.\n' +
      '\n' +
      'Figure 12: Diversity in samples produced by Snap Video for the same prompt. See additional samples on the _Website_.\n' +
      '\n' +
      'Figure 13: Samples generated by Snap Video for zero-shot evaluation on UCF101 [55]. Best viewed on the _Website_.\n' +
      '\n' +
      '## Appendix D Derivation of EDM Denoising Objective\n' +
      '\n' +
      'We derive the denoising objective \\(\\mathcal{L}(\\mathcal{F}_{\\theta})\\) expressed in terms of \\(\\mathcal{F}_{\\theta}\\) for the EDM framework where we modify the forward process introducing the input scaling factor \\(\\mathbf{\\sigma}_{\\text{in}}\\):\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{F}_{\\theta})=\\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}} \\Big{[}w(\\mathbf{\\sigma})\\ \\big{\\|}\\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}})-c _{\\text{nrm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}}\\big{\\|}_{2}^{2}\\Big{]}, \\tag{4}\\]\n' +
      '\n' +
      'We start from the denoising objective \\(\\mathcal{L}(\\mathcal{D}_{\\theta})\\) as in the original formulation:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\mathcal{D}_{\\theta}) = \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\lambda(\\mathbf{ \\sigma})\\ \\big{\\|}\\mathcal{D}_{\\theta}(\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}+\\mathbf{ \\sigma}\\mathbf{\\epsilon})-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]}, \\tag{5}\\]\n' +
      '\n' +
      'Where we recall the definition of \\(\\mathcal{D}_{\\theta}\\):\n' +
      '\n' +
      '\\[\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}})=c_{\\text{out}}(\\mathbf{\\sigma}) \\mathcal{F}_{\\theta}\\left(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}} \\right)+c_{\\text{skip}}(\\mathbf{\\sigma})(\\mathbf{x}_{\\mathbf{\\sigma}}). \\tag{6}\\]\n' +
      '\n' +
      'We also recall the definitions of \\(c_{\\text{in}}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}}(\\mathbf{\\sigma})\\), \\(\\lambda(\\mathbf{\\sigma})\\):\n' +
      '\n' +
      '\\[c_{\\text{in}}(\\mathbf{\\sigma})=\\frac{1}{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text {data}}^{2}}},\\quad c_{\\text{out}}(\\mathbf{\\sigma})=\\frac{\\mathbf{\\sigma}\\cdot\\mathbf{ \\sigma}_{\\text{data}}}{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}}, \\quad c_{\\text{skip}}(\\mathbf{\\sigma})=\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}}{ \\big{(}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}\\big{)}},\\quad\\lambda( \\mathbf{\\sigma})=\\frac{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma} ^{2}\\mathbf{\\sigma}_{\\text{data}}^{2}}. \\tag{7}\\]\n' +
      '\n' +
      'Inserting the definition of \\(\\mathcal{D}_{\\theta}\\) and of \\(c_{\\text{in}}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}}(\\mathbf{\\sigma})\\) into Eq. (4) we obtain:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\lambda(\\mathbf{ \\sigma})\\big{\\|}c_{\\text{skip}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\sigma_{\\text{in}} }{+}\\mathbf{\\sigma}\\mathbf{\\epsilon})+c_{\\text{out}}(\\mathbf{\\sigma})\\mathcal{F}_{\\theta} \\big{(}c_{\\text{in}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}{+} \\mathbf{\\sigma}\\mathbf{\\epsilon})\\big{)}-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]} \\tag{8}\\] \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\frac{\\mathbf{ \\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}^{2}\\sigma_{\\text{data} }^{2}}\\big{\\|}\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}^{2}+\\mathbf{ \\sigma}_{\\text{data}}^{2}}\\big{(}\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}{+} \\mathbf{\\sigma}\\mathbf{\\epsilon}\\big{)}+\\frac{\\mathbf{\\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}_{\\text{ data}}}{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}}\\mathcal{F}_{ \\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}{ +}\\mathbf{\\sigma}\\mathbf{\\epsilon})\\big{)}-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]}\\] (9) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}1\\cdot\\big{\\|} \\frac{\\mathbf{\\sigma}_{\\text{data}}}{\\mathbf{\\sigma}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_ {\\text{data}}^{2}}}\\big{(}\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}{+}\\mathbf{ \\sigma}\\mathbf{\\epsilon}\\big{)}+\\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{ \\sigma})(\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}{+}\\mathbf{\\sigma}\\mathbf{\\epsilon}) \\big{)}-\\frac{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}}{\\mathbf{ \\sigma}\\mathbf{\\sigma}\\mathbf{\\sigma}_{\\text{data}}}\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]}\\] (10) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}1\\cdot\\big{\\|} \\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\mathbf{ \\sigma}_{\\text{in}}}{+}\\mathbf{\\sigma}\\mathbf{\\epsilon})\\big{)}-\\frac{\\mathbf{\\sigma}_{ \\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}{+}\\mathbf{\\sigma}\\mathbf{\\epsilon})}{\\mathbf{ \\sigma}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}}(\\mathbf{x}-\\frac{\\mathbf{ x}}{\\mathbf{\\sigma}_{\\text{in}}})-\\frac{\\mathbf{\\sigma}\\mathbf{x}-\\mathbf{\\sigma}_{\\text{ data}}^{2}\\mathbf{\\epsilon}}{\\mathbf{\\sigma}_{\\text{data}}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{ \\sigma}_{\\text{data}}^{2}}}\\big{\\|}_{2}^{2}\\Big{]}\\] (11) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}1\\cdot\\big{\\|} \\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\mathbf{ \\sigma}_{\\text{in}}}{+}\\mathbf{\\sigma}\\mathbf{\\epsilon})\\big{)}-\\frac{\\frac{\\mathbf{\\sigma}_{ \\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}{-}1)}{\\mathbf{\\sigma}_{\\text{in}}\\mathbf{ \\sigma}}\\mathbf{x}+\\mathbf{\\sigma}\\mathbf{x}-\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{\\epsilon}}{\\mathbf{ \\sigma}_{\\text{data}}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}} \\big{\\|}_{2}^{2}\\Big{]}. \\tag{12}\\]\n' +
      '\n' +
      'From which follows that:\n' +
      '\n' +
      '\\[\\mathcal{F}_{\\text{tgt}}=\\mathbf{\\sigma}\\mathbf{x}-\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{ \\epsilon}+\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}-1)}{\\mathbf{ \\sigma}_{\\text{in}}\\mathbf{\\sigma}}\\mathbf{x},\\quad c_{\\text{nrm}}(\\mathbf{\\sigma})=\\frac{1} {\\mathbf{\\sigma}_{\\text{data}}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}}, \\quad w(\\mathbf{\\sigma})=1. \\tag{13}\\]\n' +
      '\n' +
      'Note that the training target has a spurious term \\(\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}-1)}{\\mathbf{\\sigma}_{ \\text{in}}\\mathbf{\\sigma}}\\mathbf{x}\\) which approaches infinity as \\(\\mathbf{\\sigma}\\) approaches 0.\n' +
      '\n' +
      'From this formulation we also notice the link between EDM and the \\(\\mathbf{v}\\)-prediciton framework. First, the training target consists in a rescaled and negated \\(\\mathbf{v}\\)-prediction objective with \\(\\mathbf{v}=\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{\\epsilon}-\\mathbf{\\sigma}\\mathbf{x}\\). Second the loss weight equals to a reweighted \\(1+SNR\\)\\(\\mathbf{v}\\)-prediction framework [45] weighting:\n' +
      '\n' +
      '\\[\\lambda(\\mathbf{\\sigma}) = \\frac{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\big{(}\\mathbf{ \\sigma}\\mathbf{\\sigma}_{\\text{data}}\\big{)}^{2}}=\\frac{1}{\\mathbf{\\sigma}_{\\text{ data}}^{2}}+\\frac{1}{\\mathbf{\\sigma}^{2}}=\\frac{1}{\\mathbf{\\sigma}_{\\text{data}}^{2}}+SNR. \\tag{14}\\]\n' +
      '\n' +
      'Thus, when \\(\\mathbf{\\sigma}_{\\text{data}}=1\\), EDM is equivalent to the \\(\\mathbf{v}\\)-prediciton formulation. Starting from these observations, we rewrite the framework so that it exhibits a well-formed \\(\\mathcal{F}_{\\text{tgt}}\\) for all values of \\(\\mathbf{\\sigma}\\) by avoiding the spurious term \\(\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}-1)}{\\mathbf{\\\n' +
      '\n' +
      '## Appendix E Derivation of Our Diffusion Framework\n' +
      '\n' +
      'We start the derivation of our diffusion framework by imposing that the training target equals the original EDM \\(\\mathbf{v}\\)-prediction objective for all values of \\(\\mathbf{\\sigma}_{\\text{in}}\\), without the spurious term \\(\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{in}}-1)}{\\mathbf{\\sigma}_{ \\text{in}}\\mathbf{\\sigma}}\\mathbf{x}\\) affecting the original formulation for \\(\\mathbf{\\sigma}_{\\text{in}}\\neq 1\\) (see Appx. D).\n' +
      '\n' +
      '\\[\\mathcal{F}_{\\text{tgt}}=\\mathbf{v}=\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{\\epsilon}- \\mathbf{\\sigma}\\mathbf{x}. \\tag{15}\\]\n' +
      '\n' +
      'We then derive \\(c_{\\text{arm}}(\\mathbf{\\sigma})\\) such that \\(c_{\\text{arm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}}\\), the function approximated by \\(\\mathcal{F}_{\\theta}\\), has unit variance:\n' +
      '\n' +
      '\\[\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\epsilon}}\\left[c_{\\text{arm}}(\\mathbf{\\sigma })\\mathcal{F}_{\\text{tgt}}\\right] = 1 \\tag{16}\\] \\[\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\epsilon}}\\left[c_{\\text{mm}}(\\mathbf{ \\sigma})\\big{(}\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{\\epsilon}-\\mathbf{\\sigma}\\mathbf{x} \\big{)}\\right] = 1\\] (17) \\[c_{\\text{nm}}(\\mathbf{\\sigma})^{2} = \\frac{1}{\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\epsilon}}\\left[\\big{(}\\mathbf{ \\sigma}_{\\text{data}}^{2}\\mathbf{\\epsilon}-\\mathbf{\\sigma}\\mathbf{x}\\big{)}\\right]}\\] (18) \\[c_{\\text{nm}}(\\mathbf{\\sigma})^{2} = \\frac{1}{\\mathbf{\\sigma}_{\\text{data}}^{2}(\\mathbf{\\sigma}_{\\text{data}} ^{2}+\\mathbf{\\sigma}^{2})}\\] (19) \\[c_{\\text{nm}}(\\mathbf{\\sigma}) = \\frac{1}{\\mathbf{\\sigma}_{\\text{data}}\\sqrt{(\\mathbf{\\sigma}_{\\text{data }}^{2}+\\mathbf{\\sigma}^{2})}}. \\tag{20}\\]\n' +
      '\n' +
      'Following standard normalization practices, we define \\(c_{\\text{in}}(\\mathbf{\\sigma})\\) so that the model input has unit variance:\n' +
      '\n' +
      '\\[\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\epsilon}}\\left[c_{\\text{in}}(\\mathbf{\\sigma })(\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}+\\mathbf{\\sigma}\\mathbf{\\epsilon})\\right] = 1 \\tag{21}\\] \\[c_{\\text{in}}(\\mathbf{\\sigma})^{2} = \\frac{1}{\\mathrm{Var}_{\\mathbf{x},\\mathbf{\\epsilon}}\\left[\\frac{\\mathbf{x}}{ \\mathbf{\\sigma}_{\\text{in}}}+\\mathbf{\\sigma}\\mathbf{\\epsilon}\\right]}\\] (22) \\[c_{\\text{in}}(\\mathbf{\\sigma})^{2} = \\frac{1}{\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{\\text {in}}^{2}}+\\mathbf{\\sigma}^{2}}\\] (23) \\[c_{\\text{in}}(\\mathbf{\\sigma}) = \\frac{1}{\\sqrt{\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{ \\text{in}}^{2}}+\\mathbf{\\sigma}^{2}}}. \\tag{24}\\]\n' +
      '\n' +
      'To derive the remaining framework components, we first recall the definition of our forward process:\n' +
      '\n' +
      '\\[\\mathbf{x}_{\\mathbf{\\sigma}}=\\frac{\\mathbf{x}}{\\mathbf{\\sigma}_{\\text{in}}}+\\mathbf{\\sigma}\\mathbf{ \\epsilon}, \\tag{25}\\]\n' +
      '\n' +
      'and note that \\(\\mathbf{x}\\) can be recovered from \\(\\mathbf{x}_{\\mathbf{\\sigma}}\\) and \\(\\mathbf{v}\\) as:\n' +
      '\n' +
      '\\[\\mathbf{x}=\\frac{\\mathbf{x}_{\\mathbf{\\sigma}}-\\frac{\\mathbf{\\sigma}}{\\mathbf{\\sigma}_{\\text{data }}^{2}}\\mathbf{v}}{\\frac{1}{\\mathbf{\\sigma}_{\\text{in}}}+\\frac{\\mathbf{\\sigma}^{2}}{\\mathbf{ \\sigma}_{\\text{data}}^{2}}}, \\tag{26}\\]\n' +
      '\n' +
      'and consequently\n' +
      '\n' +
      '\\[\\mathbf{v}=\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{x}_{\\mathbf{\\sigma}}-(\\frac{\\mathbf{ \\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{\\text{in}}}+\\mathbf{\\sigma}^{2})\\mathbf{x}}{ \\mathbf{\\sigma}}. \\tag{27}\\]\n' +
      '\n' +
      'To recover \\(c_{\\text{skip}}(\\mathbf{\\sigma})\\) and \\(c_{\\text{out}}(\\mathbf{\\sigma})\\) we note from the definition of \\(\\mathcal{F}_{\\text{tgt}}=\\mathbf{v}\\) and the loss expressed in Eq. (4) that as it approaches zero the following holds:\n' +
      '\n' +
      '\\[c_{\\text{arm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}} = \\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma }}) \\tag{28}\\] \\[\\mathbf{v} = \\frac{\\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\bm {\\sigma}})}{c_{\\text{arm}}(\\mathbf{\\sigma})}\\] (29) \\[\\mathbf{v} = \\mathbf{\\sigma}_{\\text{data}}\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text {data}}^{2}}\\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma }}). \\tag{30}\\]Substituting Eq. (30) into Eq. (26) we obtain:\n' +
      '\n' +
      '\\[\\mathcal{D}_{\\theta}(\\mathbf{x}_{\\mathbf{\\sigma}})=\\mathbf{x} = \\frac{\\mathbf{x}_{\\mathbf{\\sigma}}-\\frac{\\mathbf{\\sigma}}{\\sigma_{\\text{dis}}^{ 2}}\\mathbf{v}}{\\frac{1}{\\sigma_{\\text{in}}}+\\frac{\\mathbf{\\sigma}^{2}}{\\sigma_{\\text{ in}}^{2}}} \\tag{31}\\] \\[= \\frac{\\mathbf{x}_{\\mathbf{\\sigma}}}{\\frac{1}{\\sigma_{\\text{in}}}+\\frac{ \\mathbf{\\sigma}^{2}}{\\sigma_{\\text{dis}}^{2}}}-\\frac{\\frac{\\mathbf{\\sigma}}{\\sigma_{ \\text{in}}^{2}}\\mathbf{v}}{\\frac{1}{\\sigma_{\\text{in}}}+\\frac{\\mathbf{\\sigma}^{2}}{ \\sigma_{\\text{out}}^{2}}}\\] (32) \\[= \\frac{\\mathbf{x}_{\\mathbf{\\sigma}}}{\\frac{1}{\\sigma_{\\text{in}}}+\\frac{ \\mathbf{\\sigma}^{2}}{\\sigma_{\\text{dis}}^{2}}}-\\frac{\\mathbf{\\frac{\\sigma}{\\sigma_{ \\text{in}}}}\\mathbf{\\sigma}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\frac{1 }{\\sigma_{\\text{in}}}+\\frac{\\mathbf{\\sigma}^{2}}{\\sigma_{\\text{in}}^{2}}}\\] (33) \\[= c_{\\text{skip}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}+c_{\\text{out}}( \\mathbf{\\sigma})\\mathcal{F}_{\\theta}(c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma} }), \\tag{34}\\]\n' +
      '\n' +
      'from which we recognize:\n' +
      '\n' +
      '\\[c_{\\text{skip}}(\\mathbf{\\sigma})=\\frac{\\mathbf{\\sigma}_{\\text{in}}\\mathbf{\\sigma}_{\\text {data}}^{2}}{\\mathbf{\\sigma}_{\\text{in}}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data} }^{2}},\\quad c_{\\text{out}}(\\mathbf{\\sigma})=-\\mathbf{\\sigma}_{\\text{in}}\\mathbf{\\sigma} \\mathbf{\\sigma}_{\\text{data}}\\frac{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{ data}}^{2}}}{\\sigma_{\\text{data}}^{2}+\\mathbf{\\sigma}_{\\text{in}}\\mathbf{\\sigma}^{2}}. \\tag{35}\\]\n' +
      '\n' +
      'We set the loss weight \\(\\lambda(\\mathbf{\\sigma})\\) to the same value as EDM:\n' +
      '\n' +
      '\\[\\lambda(\\mathbf{\\sigma})=\\frac{1}{\\mathbf{\\sigma}_{\\text{data}}^{2}}+\\frac{1}{\\mathbf{ \\sigma}^{2}}. \\tag{36}\\]\n' +
      '\n' +
      'To recover \\(w(\\mathbf{\\sigma})\\), inserting the definition of \\(\\mathcal{D}_{\\theta}\\) (Eq. (6)) and of \\(c_{\\text{in}}(\\mathbf{\\sigma})\\), \\(c_{\\text{out}}(\\mathbf{\\sigma})\\), \\(c_{\\text{skip}}(\\mathbf{\\sigma})\\), \\(\\lambda(\\mathbf{\\sigma})\\) into Eq. (4) we obtain:\n' +
      '\n' +
      '\\[\\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\lambda(\\mathbf{ \\sigma})\\big{\\|}c_{\\text{skip}}(\\mathbf{\\sigma})(\\frac{\\mathbf{x}}{\\sigma_{\\text{in }}}+\\mathbf{\\sigma}\\mathbf{\\epsilon})+c_{\\text{out}}(\\mathbf{\\sigma})\\mathcal{F}_{\\theta} \\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}\\big{)}-\\mathbf{x}\\big{\\|}_ {2}^{2}\\Big{]} \\tag{37}\\] \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\lambda(\\mathbf{ \\sigma})\\Big{\\|}\\frac{\\mathbf{\\sigma}_{\\text{in}}\\sigma_{\\text{data}}^{2}}{\\mathbf{ \\sigma}_{\\text{in}}\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2}}(\\frac{\\mathbf{x }}{\\sigma_{\\text{in}}}+\\mathbf{\\sigma}\\mathbf{\\epsilon})-\\mathbf{\\sigma}_{\\text{in}}\\mathbf{ \\sigma}\\mathbf{\\sigma}_{\\text{data}}\\frac{\\sqrt{\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{ \\text{data}}^{2}}}{\\mathbf{\\sigma}_{\\text{data}}^{2}+\\mathbf{\\sigma}_{\\text{in}}\\mathbf{ \\sigma}^{2}}\\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{ \\sigma}}\\big{)}-\\mathbf{x}\\big{\\|}_{2}^{2}\\Big{]}\\] (38) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\frac{\\lambda( \\mathbf{\\sigma})}{(\\mathbf{\\sigma}^{2}+\\frac{\\sigma_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{ \\text{in}}})^{2}}\\big{\\|}\\frac{\\mathbf{\\sigma}}{c_{\\text{mm}}(\\mathbf{\\sigma})} \\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}} \\big{)}+\\mathbf{\\sigma}^{2}\\mathbf{x}-\\mathbf{\\sigma}_{\\text{data}}^{2}\\mathbf{\\sigma}\\mathbf{ \\epsilon}\\big{\\|}_{2}^{2}\\Big{]}\\] (39) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\frac{\\mathbf{ \\sigma}^{2}}{(\\mathbf{\\sigma}^{2}+\\frac{\\sigma_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{ \\text{in}}})^{2}}\\frac{\\lambda(\\mathbf{\\sigma})}{c_{\\text{mm}}(\\mathbf{\\sigma})^{2}} \\big{\\|}\\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma }}\\big{)}+c_{\\text{mm}}(\\mathbf{\\sigma})(\\mathbf{\\sigma}\\mathbf{x}-\\mathbf{\\sigma}_{\\text{ data}}^{2}\\mathbf{\\epsilon})\\big{\\|}_{2}^{2}\\Big{]}\\] (40) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}\\frac{(\\mathbf{ \\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2})^{2}}{(\\mathbf{\\sigma}^{2}+\\frac{\\mathbf{ \\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{\\text{in}}})^{2}}\\big{\\|}\\mathcal{F}_{ \\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma}}\\big{)}+c_{\\text{mm} }(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}}\\big{\\|}_{2}^{2}\\Big{]}\\] (41) \\[= \\mathbb{E}_{\\mathbf{\\sigma},\\mathbf{x},\\mathbf{\\epsilon}}\\Big{[}w(\\mathbf{\\sigma}) \\big{\\|}\\mathcal{F}_{\\theta}\\big{(}c_{\\text{in}}(\\mathbf{\\sigma})\\mathbf{x}_{\\mathbf{\\sigma }}\\big{)}+c_{\\text{mm}}(\\mathbf{\\sigma})\\mathcal{F}_{\\text{tgt}}\\big{\\|}_{2}^{2} \\Big{]}, \\tag{42}\\]\n' +
      '\n' +
      'from which:\n' +
      '\n' +
      '\\[w(\\mathbf{\\sigma})=\\frac{(\\mathbf{\\sigma}^{2}+\\mathbf{\\sigma}_{\\text{data}}^{2})^{2}}{(\\mathbf{ \\sigma}^{2}+\\frac{\\mathbf{\\sigma}_{\\text{data}}^{2}}{\\mathbf{\\sigma}_{\\text{in}}})^{2}}. \\tag{43}\\]\n' +
      '\n' +
      'In conclusion, the proposed diffusion framework maintains the training target \\(\\mathcal{F}_{\\text{tgt}}\\) equal to the original EDM training target for all values of the input scaling factor \\(\\mathbf{\\sigma}_{\\text{in}}\\), ensuring that \\(\\mathcal{F}_{\\theta}\\) learns the same denoising function while giving control on the quantity of the signal present in the input. In addition, our framework preserves the original loss weight \\(\\lambda(\\mathbf{\\sigma})\\), giving control over the input scaling factor without affecting the weight of the loss over the different noise levels.\n' +
      '\n' +
      '## Appendix F Discussion\n' +
      '\n' +
      'In this section, we describe the limitations of our framework (see Sec. F.1) and discuss societal impact (see Sec. F.2).\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'Snap Video presents limitations which we discuss in this section.\n' +
      '\n' +
      '**Text Rendering** We find our framework to often spell text incorrectly. We attribute this finding to a lack of high-quality videos depicting text matched by the exact description of the displayed text. Automated pipelines for OCR can be employed on the training dataset to address this issue.\n' +
      '\n' +
      '**Object count** Similarly, the generator may not render the requested number of entities, especially when the cardinality of the objects is high. The behavior can be explained by the difficulties in learning correct object counts from video data, where descriptions can be noisy, objects can enter and exit the scene and the camera can move widely, changing the cardinality of objects in each frame.\n' +
      '\n' +
      '**Positional understanding** While the generator can place objects in positions requested by the prompts, we find it can not reliably synthesize videos corresponding to prompts entailing complex positional relationships between multiple entities such as _"A stack of three cubes: the top one is blue, the bottom one green and the middle one is red"_.\n' +
      '\n' +
      '**Stylization** We find that our model can generate stylized content (see Fig. 9), but may present failure cases where the style specification is ignored or the stylized contents only translate in the scene rather than being animated. We attribute this finding to the lack of model training on a filtered set of data presenting high aesthetic scores which we find to contain textual descriptions related to artistic and visual styles with higher probability.\n' +
      '\n' +
      '**Negation** Some challenging prompts such as _"A glass of juice next to a plate with no bananas in it"_ may lead the model to ignore the negation, resulting in the generation of all entities.\n' +
      '\n' +
      '**Block artifacts** Videos may contain content with a very large amount of motion, leading to a greater difficulty in compressing its content to a latent representation of fixed size. In such situations we find that the model may produce patch tokens that do not blend together in a perfect manner, resulting in some visible patches in the videos, akin to video compression artifacts.\n' +
      '\n' +
      '**Resolution** Our two-stage model cascade generates videos in \\(512\\times 288\\)px resolution. We note that generating video content aligning to the given prompt and presenting temporally coherent motion are the most critical problems in video generation, and possible artifacts in these categories are already visible in \\(512\\times 288\\)px resolution, as shown in our comparisons to baselines. We also note that cascaded model stages are independently trained and agnostic to the employed previous-stage generator. Thus, given an upsampler from \\(512\\times 288\\)px to a higher resolution, any improvement shown in \\(512\\times 288\\)px resolution with respect to baselines is expected to produce higher resolution results of correspondingly improved quality. We consider the integration of additional cascade stages as an interesting venue for future work.\n' +
      '\n' +
      '### Societal Impact\n' +
      '\n' +
      'Text-to-video generative models are evolving rapidly [4, 13, 21, 62] and hold promise to empower users with new and powerful ways to express their creativity once accessible only to trained experts such as artists and digital content creators. With such improvements comes a greater risk that generated results may be perceived as real with the potential for nefarious individuals to generate harmful or deceiving content. Our model is exposed to a broad range of concepts during training and makes use of a T5 [39] text encoder that was trained on unfiltered internet data, making it necessary to guard it against such possible uses. In addition, the model generates data following its training data distribution which implies that potential biases that may be present in the dataset can be reflected in the model outputs. To avoid misuse, we do not make the model publicly accessible and plan to put in place data cleaning, prompt filtering and output filtering techniques and watermarking as additional safeguards.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>