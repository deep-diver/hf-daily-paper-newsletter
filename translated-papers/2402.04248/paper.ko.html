<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '맘바는 배우는 법을 배울 수 있을까?\n' +
      '\n' +
      '컨텍스트 학습 과제 비교 연구\n' +
      '\n' +
      ' 종호박({}^{1}\\), 재승박({}^{2}\\), 즈양양이온({}^{3}\\), 나영이({}^{3}\\), 재웅조({}^{1}\\)\n' +
      '\n' +
      '**Samet Oymak\\({}^{4}\\), Kangwook Lee\\({}^{1,3}\\), Dimitris Papailiopoulos\\({}^{1,3}\\)**\n' +
      '\n' +
      'KRAFTON, \\({}^{1}\\) 서울대학교,\n' +
      '\n' +
      '위스콘신-매디슨 대학교, 미시간 대학교, 앤 아버\n' +
      '\n' +
      '이 작업은 KRAFTON.Email: <jongho.park@krafton.com>에서 인턴을 하는 동안 이루어졌다. 대응: <dimitris@papail.io> 트랜스포머 언어 모델이 현재 실제로 ICL이 가능하다고 보고된 유일한 대형 모델이기 때문에, 이것은 질문을 제기한다:\n' +
      '\n' +
      '무관심 모델들은 ICL?___Can 무관심 모델들이 ICL?__Can 무관심 모델들을 ICL?__Can 무관심 모델들이 ICL?__Can 무관심 모델들을 ICL?_\n' +
      '\n' +
      '이 질문은 특히 최근 여러 연구가 2차 비용 때문에 주의 기반 네트워크를 넘어 이동하려고 시도했다는 점을 고려할 때 장점이 있다(Gu et al., 2022; Dao et al., 2022; Gu and Dao, 2023; Poli et al., 2023; Peng et al., 2023; Sun et al., 2023; Yang et al., 2023b). 이 작업에서 우리는 특히 국가 공간 모델(SSM), 특히 암파(구 및 도, 2023)에 초점을 맞춘다. Mampa는 최근 Pile(Gao et al., 2020)과 같은 표준 사전 훈련 언어 데이터 세트에서 거의 최첨단 성능을 달성하면서 매우 효율적인 것으로 입증되었지만 더 작은 모델 규모(예: 최대 30억 매개변수)에서는 다양한 언어 및 비언어 작업에 걸쳐 변압기 및 기타 주의 없는 아키텍처를 능가한다. 그러나 ICL 기능은 보통 30억 매개 변수를 초과하는 규모로 나타난다. 결과적으로, 이러한 가설을 테스트하려면 일반적으로 70억 매개변수 수준을 초과하는 스케일링이 필요하기 때문에 ICL을 수행하기 위한 이러한 주의 없는 모델의 잠재력은 미개척 상태로 남아 있다. 그럼에도 불구하고, 우리는 Garg et al.(2022)의 접근법에 따라, 인-컨텍스트 학습을 수행하기 위한 모델을 구체적으로 트레이닝함으로써 여전히 소규모 ICL 능력을 조사할 수 있다.\n' +
      '\n' +
      '본 연구에서는 Mampa 및 S4(Gu et al., 2022b)와 같은 최신 모델을 포함한 트랜스포머 및 다양한 SSM의 성능을 평가하기 위한 다양한 ICL 태스크 세트를 소개한다. 본 연구 결과는 대부분의 SSM이 여러 작업에서 트랜스포머의 성능과 일치하는 ICL을 효과적으로 수행할 수 있음을 보여준다. 그러나, Mampa는 (Arora et al., 2023)에 의해 또한 언급된 바와 같이) 결정 트리 및 검색 태스크를 학습하는데 몇 가지 한계를 입증하지만, 트랜스포머 모델이 고전하는 희소 패리티와 같은 다른 복잡한 ICL 태스크에서 트랜스포머를 능가할 수 있다. 각 과제에 대한 서로 다른 모델의 성능은 표 1에 요약되어 있다.\n' +
      '\n' +
      '모델 중 어느 하나의 패밀리가 더 나은 작업이 있는 것처럼 보이기 때문에, 우리는 (구 및 도, 2023)과 유사하게 다중 헤드 주의 블록으로 SSM 블록을 인터리빙하는 것의 영향을 탐구한다. 본 논문에서는 그림 1과 같이 Mampa와 Attention layer를 통합한 새로운 하이브리드 구조인 Mampa former를 소개한다. Mampa former는 Mampa와 Transformers의 장점을 이용하여 ICL 작업에서 우수한 성능을 보이며, 희소 패리티와 검색을 동시에 학습할 수 있다.\n' +
      '\n' +
      '우리는 우리의 연구 결과가 주목받지 않는 아키텍처의 맥락에서 상당한 진전이 이루어졌기 때문에 트랜스포머를 넘어 ICL에 대한 이해의 폭을 넓히는 것의 중요성을 강조한다고 믿는다.\n' +
      '\n' +
      '우리는 연구의 한계가 비언어 ICL 작업과 더 작은 모델에 초점을 맞추고 있다는 것을 인정한다. 더 높은 매개변수 카운트에서 실제 언어 설정에서 더 일반적인 ICL 작업에 대한 SSM과 변압기 간의 아키텍처 비교는 여기에서 제공하는 것과 동일한 관찰을 산출하지 않을 수 있다. 그럼에도 불구하고, 우리의 결과는 (Arora et al., 2023)에 의해 언급된 것과 유사한 일부 검색 작업의 어려움을 제외하고, Mampa가 문맥 내 학습을 수행하는 데 근본적인 장애물이 없는 것으로 보인다는 것을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Transformer & Mampa & MampaFormer \\\\ \\hline Linear regression & ✓ & ✓ & ✓ \\\\ Sparse linear regression & ✓ & ✓ & ✓ \\\\\n' +
      '2NN regression & ✓ & ✓ & ✓ \\\\ Decision Tree & ✓ & ✗ & ✓ \\\\ Orthogonal-outlier regression & ✓ & ✗ & ✓ \\\\ Many-outlier regression & ✗ & ✓ & ✓ \\\\ Sparse parity & ✗ & ✓ & ✓ \\\\ Chain-of-Thought I/O & ✓ & ✓ & ✓ \\\\ Vector-valued MQAR & ✓ & ✗ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 ICL 태스크에 대한 모델 성능. 우리는 모델이 다른 기준 모델과 동등하게 수행하면 ✓, 모델이 작업을 학습하지 못하면 ✗, 다른 모델에 비해 성능 격차가 있으면 ✗로 모델의 성능을 레이블링한다. 변환기는 희소 패리티를 학습하는 데 실패하고 Mamba는 벡터 값 MQAR에서 실패한다. 제안된 Mampa former는 모든 작업에서 성공합니다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      'Transformer-based in-context Learning.ICL에서 주의의 역할은 이론적 연구와 실증적 연구의 초점이 되어 왔다. 연구는 주로 ICL을 위해 명시적으로 훈련하는 메타 학습(Ravi and Larochelle, 2016; Min et al., 2022)에 초점을 맞추었다. 특히, Garg et al. (2022)은 선형 회귀 학습에서 학습 결정 트리에 이르기까지 문맥 내 회귀 태스크에서 변압기를 조사했다. 후속 작업들은 관심이 다양한 최적화 알고리즘들을 모방할 수 있음을 시사했다(Akyurek et al., 2022; von Oswald et al., 2023; Dai et al., 2023). 사실, Ahn et al. (2023); Mahankali et al. (2023)은 선형 주의를 위한 선형 회귀 ICL에서 경사 하강이 최적임을 입증 가능하게 보여주었다.\n' +
      '\n' +
      '이러한 설정은 언어 모델에서 단순하고 분리되어 보일 수 있지만, Bhattacharya et al.(2023)은 동결된 GPT-2가 최근접 이웃 알고리즘을 구현할 수 있음을 보여주었으며, 기존의 언어 모델에서 ICL과 무작위 초기화에서 ICL에 대한 훈련의 양식화된 설정 사이의 연결을 그렸다. 또한, Olsson et al.(2022)은 또한 간단한 검색 문제를 해결하는 주의 헤드인 "유도 헤드"가 ICL 거동과 상관되어 검색과 ICL 사이의 강한 연결을 제공한다는 것을 경험적으로 입증한다.\n' +
      '\n' +
      '하위 2차 아키텍처.어텐션 레이어의 유효 부동 소수점 연산 횟수는 입력 시퀀스 길이에 대해 2차적으로 스케일링된다. 2차 의존성을 극복하기 위해 수많은 근사치 또는 대체 모델 아키텍처가 제안되었다. 이러한 범위는 어텐션 메커니즘(Beltagy et al., 2020; Wang et al., 2020)을 근사화하는 것에서부터 구조화된 상태 공간 모델(Gu et al., 2022)과 같은 새로운 순환 컨볼루션 모델의 개발에 이르기까지 다양하다.\n' +
      '\n' +
      'S4(Gu et al., 2022)는 이산화된 상태-공간 모델을 특징으로 하는 시퀀스 모델들의 패밀리이다.\n' +
      '\n' +
      '\\overline{\\mathbf{h}_{t}=\\overline{\\mathbf{A}\\mathbf{h}_{t-1}+\\overline{\\mathbf{B}\\mathbf{x}_{t},\\y_{t}=\\mathbf{C}\\mathbf{h}_{t},\\tag{1}\\tag}\n' +
      '\n' +
      '여기서 \\(\\mathbf{h}_{t}\\)는 숨겨진 상태를 나타내고 \\((\\overline{\\mathbf{A}},\\overline{\\mathbf{B},\\mathbf{C})\\)는 입력 독립적인 (변환) 파라미터이다. 재발은 컨볼루션으로 표현되어 고속 푸리에 변환을 사용하여 거의 선형적인 복잡성을 가능하게 한다. 이러한 프레임워크에서 볼 때, 소프트맥스 없이 선형 주의를 사용하는 선형 트랜스포머(Katharopoulos et al., 2020)는 선형 SSM의 변형으로 볼 수 있다.\n' +
      '\n' +
      '이러한 개념을 기반으로, H3(Dao et al., 2022)는 이중 게이트 연결과 S4를 통합한다. 최근 Mamba(Gu and Dao, 2023)는 선정을 도입하여 표준 SSM에서 출발한다.\n' +
      '\n' +
      '그림 1: Mamba former는 변압기 내의 MLP 블록을 Mamba 블록으로 대체하는 하이브리드 아키텍처이다. 중요하게도, 이 아키텍처는 또한 맘바 블록으로부터 시작하고 위치 인코딩을 사용하지 않는다. ICL 평가에서, 우리는 Mamba former가 트랜스포머와 Mamba에 비해 일관되게 최고의 성능을 달성한다는 것을 발견했다.\n' +
      '\n' +
      '식 (1)에서 \\((\\mathbf{\\overline{A}},\\mathbf{\\overline{B},\\mathbf{C})\\)를 \\(\\mathbf{x}_{t}\\)에 의존하게 만드는 메커니즘은 입력 의존적 시퀀스 혼합을 허용한다.\n' +
      '\n' +
      '하이에나(Poli et al., 2023), RWKV(Peng et al., 2023), RetNet(Sun et al., 2023), 및 GLA(Yang et al., 2023b)와 같은 다른 주목할만한 주의없는 모델들이 있다. Mamba와 같은 모델들에 대한 최신 성능에도 불구하고, Arora et al.(2023)은 서브쿼리틱 모델들이 유도 헤드 태스크의 일반화인 멀티쿼리 리콜 태스크들에 대한 관심에 여전히 뒤처진다는 것을 입증하였다(Olsson et al., 2022).\n' +
      '\n' +
      'Xie et al.(2021)에서 저자들은 합성 언어 기반 문맥 내 학습 데이터 세트를 제안했으며 변압기와 LSTM이 ICL이 가능함을 보여준다. 또한, Akyurek et al. (2024)는 랜덤 유한 오토마타에 의해 생성된 정규 언어에 대한 트레이닝을 통해 랭게이지 기반 ICL 벤치마크를 제안했으며 트랜스포머와 서브쿼드 복잡도 모델 간의 격차를 강조했다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '본 연구에서는 SSM과 Transformer의 ICL 성능을 평가하기 위해 3.1절 3.2절에 자세히 설명되어 있는 각 특정 작업에 대해 각 모델을 처음부터 학습하여 ICL 및 관련 작업을 요약한다. 우리는 다음 표 2에 우리의 작업에 대한 간략한 요약을 제공한다.\n' +
      '\n' +
      '상황내 학습을 위한### 모델 학습\n' +
      '\n' +
      '우리는 특정 함수 클래스 \\(\\mathcal{F}\\) in-context를 학습하도록 모델을 훈련시킨다. 학습은 분포(\\mathcal{D}_{\\mathcal{F}\\)로부터 함수(f\\in\\mathcal{F}\\)를 선택하고, 무작위 입력(\\mathbf{x}_{1},\\dots,\\mathbf{x}_{N}\\in\\mathbb{R}^{d}\\)의 시퀀스를 샘플링하는 무작위 프롬프트를 생성하는 것으로 시작된다. 여기서 \\(N\\)과 \\(d\\)은 각각 문맥 내 예제의 수와 \\(\\mathbf{x}_{i}\\)의 차원을 나타낸다. 이들 입력은 프롬프트 \\(P=(\\mathbf{x}_{1},f(\\mathbf{x}_{1}),\\dots,\\mathbf{x}_{N},f(\\mathbf{x}_{N}))를 생성한다. 우리는 모든 프롬프트에 대한 예상 손실을 최소화하여 \\(\\theta\\)으로 매개변수화된 모델 \\(f_{\\theta}\\)을 훈련한다.\n' +
      '\n' +
      '\\\\[\\min_{\\theta}\\mathbb{E}_{P}\\left[\\frac{1}{N}\\sum_{i=1}^{N-1}\\ell(f_{\\theta}(P^{ i}),f(\\mathbf{x}_{i}))\\right], \\tag{2}\\t.\n' +
      '\n' +
      '여기서 \\(P^{i}:=(\\mathbf{x}_{1},f(\\mathbf{x}_{1}),\\dots,\\mathbf{x}_{i},f(\\mathbf{x}_{i}),\\mathbf{x}_{i+1})\\) 및 \\(\\ell(\\cdot,\\cdot)\\)은 손실 함수이다. \\(f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}\\)에 대해 \\(d-1\\)0을 \\(f(\\mathbf{x})\\)에 추가한다. 각 ICL 태스크에 적합한 손실 함수를 사용합니다.\n' +
      '\n' +
      'Model architecture.We mainly focus for SSMs including (1) Mamba (Gu and Dao, 2023), the state-of-the-art SSM model with selection mechanism; (2) S4 (Gu et al., 2022a), the linear time-invariant counterpart of Mamba; and (3) S4-Mamba, the variant of Mamba\'s input-dependent S6 is replaced with input-independent S4. The primary differences of S4 models are in application of multiplicative gating and the module order.2\n' +
      '\n' +
      '각주 2: [https://github.com/state-spaces/s4/blob/main/models/s4](https://github.com/state-spaces/s4/blob/main/models/s4)\n' +
      '\n' +
      '트레이닝.각 트레이닝 단계에서 랜덤 프롬프트의 배치를 샘플링하고 Adam Optimizer(Kingma and Ba, 2014)를 사용하여 모델 파라미터를 업데이트함으로써 각 모델을 트레이닝한다. 우리는 64의 배치 크기를 사용하고 500,000번의 반복(벡터 값 MQ를 제외하고)에 대해 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c} \\hline \\hline Task & dim \\((d)\\) & points \\((N)\\) & Example/Function Sampling & Task-specific \\\\ \\hline Linear regression & 20 & 41 & \\(\\mathbf{x},\\mathbf{w}\\sim\\mathcal{N}(0,\\mathbf{I}_{d})\\) & – \\\\ Sparse Linear regression & 20 & 101 & \\(\\mathbf{x},\\mathbf{w}\\sim\\mathcal{N}(0,\\mathbf{I}_{d})\\), \\(\\mathtt{sparsity}(\\mathbf{w})\\gets k\\) & \\(k=3\\) \\\\\n' +
      '2NN regression & 20 & 101 & \\(\\mathbf{W}_{1}^{(d)},\\mathbf{W}_{ij}^{(d)}\\sim\\mathcal{N}(0,1)\\) & – \\\\ Decision Tree & 20 & 101 & \\(\\mathbf{x},\\mathrm{Leaf}\\sim\\mathcal{N}(0,1),\\mathrm{non\\_leaf}\\sim\\{1,...,d\\}\\) & \\(\\mathrm{depth}=4\\) \\\\ Orthogonal-outrigent regression & 20 & 101 & \\(\\mathbf{x},\\mathbf{w}\\sim\\mathcal{N}(0,\\mathbf{I}_{d})\\), \\(\\mathbf{w}\\sim\\mathbf{w}^{-1}\\) & \\(p=0.5\\) \\\\ Many-outlier regression & 20 & 512 & \\(\\mathbf{x}\\sim\\mathcal{N}(0,1),\\mathbf{w}\\cdot\\mathbf{1},\\cdot\\mathbf{p}=1, \\cdot\\mathbf{p}=\\mathbf{c}(\\mathbf{x},y)=(\\mathbf{1},1)\\) & \\(p=0.9\\) \\\\ Sparse Parity & 10 & 140 & \\(\\mathbf{x}\\sim\\{-1,1\\}^{d},y=\\prod_{j\\in I}\\mathbf{x}[j]\\) & \\(k=2\\) \\\\ Chain-of-Thought I/O & 10 & 101 & \\(\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_{d}),\\mathbf{W}_{ij}\\sim\\mathcal{N}(0,2/k), \\mathbf{v}\\sim\\mathcal{N}(0,\\mathbf{I}_{k})\\) & \\(h=8\\) \\\\ Vector-valued MQAR & 20 & 128 & \\(\\mathbf{k},\\mathbf{v}\\sim\\mathrm{Unif}(\\mathcal{S}^{d-1})\\) & 32 k-v pairs \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 태스크 요약. 모든 모델은 500,000번의 반복(벡터 값 MQAR 제외)에 대해 훈련됩니다. 부록 B.2를 참조하십시오.\n' +
      '\n' +
      '평가.훈련과 일치하는 과제 및 데이터 분포 \\(\\mathcal{D}_{\\mathcal{F}\\) 및 \\(\\mathcal{D}_{\\mathcal{X}\\)을 사용하여 상황 내 학습에 대한 모델 성능을 평가한다. 함수 및 입력 시퀀스는 각각 \\(\\mathcal{D}_{\\mathcal{F}\\) 및 \\(\\mathcal{D}_{\\mathcal{X}\\)으로부터 샘플링되어 테스트 프롬프트 \\(P_{\\text{test}=(\\mathbf{x}_{1},f(\\mathbf{x}_{1}),\\dots,\\mathbf{x}_{N},f(\\mathbf{x}_{N}))를 생성한다. 우리는 1,280개의 프롬프트를 만들고 Eq의 경험적 평균을 측정한다. (2) In-context 학습 수행을 위해 그들을 가로질러.\n' +
      '\n' +
      '모델 용량이 증가함에 따라 성능을 나타내기 위해 훈련에 사용되는 총 부동 소수점 연산(FLOP)을 계산합니다. 트랜스포머 및 맘바에 대한 계산은 부록 C에서 찾을 수 있으며, 이는 (Kaplan et al., 2020; Gu and Dao, 2023)에 기초한다. 모델 구성 및 교육 구현 세부 사항은 부록 A에 나와 있다.\n' +
      '\n' +
      '### 상황 내 학습 과제\n' +
      '\n' +
      '이 연구에서 조사된 ICL 및 관련 작업에 대한 개요를 제공한다. 일부 작업은 (Garg et al., 2022)에서 수정되었으며 작업에서 설명된 설정을 따른다. 과제는 표 2에 요약되어 있다.\n' +
      '\n' +
      '###### 3.2.1 학습회귀\n' +
      '\n' +
      '모든 회귀 작업에 대해, 문맥 내 예제 \\(\\mathbf{x}_{i}\\)는 가우스 분포 \\(\\mathcal{N}(0,\\mathbf{I}_{d})\\)에서 샘플링되며, 여기서 \\(\\mathbf{I}_{d}\\)은 \\(d\\times d\\) 동일성 매트릭스이다. 우리는 모델 훈련에 제곱 오차 손실을 사용한다.\n' +
      '\n' +
      '선형회귀(Linear Regression.We examine class of linear functions \\(\\mathcal{F}=\\{f|f(\\mathbf{x})=\\mathbf{w}^{\\top}\\mathbf{x},\\mathbf{w}\\in\\mathbbb{R}^{d}\\}) 여기서 \\(\\mathbf{w}\\)는 가우시안 분포 \\(\\mathcal{N}(0,\\mathbf{I}_{d})\\에서 샘플링된다. 우리는 \\(d=20\\)으로 설정하였다.\n' +
      '\n' +
      '희소 선형 회귀.이 설정은 \\(\\mathbf{w}\\)을 \\(\\mathcal{N}(0,\\mathbf{I}_{d})\\)에서 샘플링한 후 \\(k\\) 좌표를 \\(\\mathbf{w}\\)에 랜덤하게 유지하고 나머지는 0으로 설정하는 것을 제외하고는 선형 회귀와 동일하다. 우리는 \\(k=3\\)으로 설정하였다.\n' +
      '\n' +
      '2층 신경망.2층 ReLU 신경망의 클래스를 고려한다. 2층 ReLU 신경망의 클래스는 \\(\\mathcal{F}=\\{f|f(\\mathbf{x})=\\mathbf{W}^{(2)}\\sigma\\left(\\mathbf{W}^{(1)}\\mathbf{x}\\right)\\}\\), 여기서 \\(\\mathbf{W}^{(2)}\\in\\mathbbb{R}^{1\\times h},\\mathbbb{W}^{(1)}\\in\\mathbb{R}^{h\\times d}\\), \\(\\sigma(\\cdot)=\\max(0,\\cdot)\\)이다. 가중치 행렬의 각 요소는 \\(\\mathcal{N}(0,1)\\)에서 독립적으로 그려진다. 우리는 \\(d=20\\)과 \\(h=100\\)을 사용한다.\n' +
      '\n' +
      '결정 트리는 고정된 깊이와 입력 \\(\\mathbf{x}\\in\\mathbb{R}^{d}\\)을 갖는 완전 이진 트리를 고려한다. 잎 노드 값은 \\(\\mathcal{N}(0,1)\\)에서 샘플링되고 나머지는 \\(\\{1,...,d\\}\\)에서 균일하게 샘플링되어 \\(\\mathbf{x}\\)의 인덱스로 기능한다. 주어진 비-리프 노드에서, 우리는 \\(x[i]>0\\)이면 오른쪽으로 이동하고, 여기서 i는 샘플링된 인덱스이고, 그렇지 않으면 왼쪽으로 이동한다. \\ (y\\)는 횡단이 종료될 때의 리프 노드 값이다.\n' +
      '\n' +
      '###### 3.2.2 이상치를 이용한 학습\n' +
      '\n' +
      '이 가족에 속하는 문제들은 표준 선형 회귀 과제의 기본 설정을 채택하고 있다. 프롬프트에 있는 \\((\\mathbf{x}_{i},f(\\mathbf{x}_{i}))\\)의 각 쌍을 학습 분포에서 벗어난 "더미" 벡터 또는 작업의 복잡도를 높이기 위해 설계된 교란 변수로 대체한다. 우리는 다음과 같은 작업에 대한 대체 확률로 \\(p\\in\\{0.5,0.9\\}\\)을 테스트한다. 교육 중에는 교체된 특이치에 대한 손실을 계산하지 않습니다.\n' +
      '\n' +
      '직교-아웃라이어 회귀. \\((\\mathbf{x}_{i},f(\\mathbf{x}_{i})))의 각 쌍은 \\(((a_{x}\\mathbf{u}+b_{x}\\mathbf{v})/(a_{x}^{2}+b_{x}^{2}),(a_{y}\\mathbf{u}+b_{ y}\\mathbf{v})/(a_{x}^{2}))으로 랜덤하게 대체된다. 여기서 \\(\\mathbf{u},\\mathbf{v}\\in\\mathbf{w}^{perp}\\). ((\\mathbf{u},\\mathbf{v}):=(\\mathbf{w}_{1}-\\text{proj}_{\\mathbf{w}(\\mathbf{w}_{1}),\\mathbf{w}_{2}-\\text{proj}_{\\mathbf{w}(\\mathbf{w}_{2}))) 및 \\(\\mathbf{w}_{1}\\) 및 \\(\\mathbf{w}_{2}\\)으로부터 샘플링되고 \\(\\mathcal{N}(0,\\mathbf{I}_{d}))로부터 계수 \\(a_{x},b_{x},a_{y},b_{y}\\)는 독립적으로 \\(\\mathcal{N}(0,1)\\)으로부터 샘플링된다.\n' +
      '\n' +
      '이 환경에서 \\(\\mathbf{x}_{i}\\)와 \\(f(\\mathbf{x}_{i})\\)은 각각 1\\(\\{1\\}^{d}\\)의 \\(d\\)차원 벡터와 1-hot 벡터 \\([1,0,\\dots,0]\\)의 확률 90%로 랜덤하게 대체된다. 여기서, 우리는 \\(N=512\\)의 더 긴 시퀀스를 테스트한다.\n' +
      '\n' +
      '###### 3.2.3 학습 이산 함수\n' +
      '\n' +
      'Sparse parity.Bhattacharya et al.(2023)의 설정에 따라, 함수들의 클래스 \\(\\mathcal{F}=\\{f|f(\\mathbf{x})=\\prod_{j\\in\\mathcal{S}}\\mathbff{x}_{i}[j]\\}\\)를 고려하였으며, 여기서 \\(\\mathbf{x}_{i}[j]\\)는 벡터 \\(\\mathbf{x}_{i}\\)의 \\(j\\)번째 요소를 나타내고 \\(\\mathcal{S}\\)은 크기 \\(k\\)인 \\(\\{1,\\dots,d\\}\\)의 부분집합이다. 각 \\(\\mathbf{x}_{i}\\)은 랜덤으로 균일하게 샘플링되고 \\(\\mathcal{S}\\) 크기의 \\(k\\)은 세트 \\(\\{1,\\ldots,d\\}\\)에서 랜덤으로 샘플링된다. 이 작업을 위해 교차 엔트로피 손실을 사용하여 모델을 훈련하고 정확성을 위해 이진 표시기를 사용하여 모델을 평가하며, 예측은 1을 수정하고 예측은 0을 부정확한 것으로 할당한다.\n' +
      '\n' +
      '3.2.4 사고력 학습 사슬\n' +
      '\n' +
      'Li 등(2023b)의 설정에 따라 이층 ReLU 신경망 \\(\\mathcal{F}=\\{f|f(\\mathbf{x})=\\mathbf{W}^{(2)}\\sigma\\left(\\mathbf{W}^{(1)}\\mathbff{x}\\right)\\}\\), 여기서 \\(\\mathbf{W}^{(2)}\\in\\mathbbb{R}^{1\\times h},\\mathbbb{R}^{h\\times d}\\,\\(\\cdot)\\)은 \\(\\mathrm{ReLU}\\) 함수이다. 우리는 \\(d=10\\), \\(h=8\\)을 설정하였다. 또한 입력 학습 시퀀스에 중간 은닉 특징 \\(\\mathbf{s}_{i}=\\sigma\\left(\\mathbf{W}^{(1)}\\mathbf{x}_{i}\\right)\\)을 사상 체인 스타일로 인터리브한다. 입력 시퀀스\\((\\mathbf{x}_{1},\\mathbf{s}_{1},f(\\mathbf{x}_{1}),\\cdots,\\mathbf{x}_{N},\\mathbf{s}_{N},f(\\mathbf{x}_{N}),\\mathbf{x}_{text{test})가 주어지면, 입력 시퀀스와 중간 레이어 예측\\(\\hat{\\mathbf{s}_{text{test}})에 기초하여 최종 출력 예측\\(\\hat{\\mathbf{y}}\\)에 대해 모델을 평가한다.\n' +
      '\n' +
      '###### 3.2.5 학습 검색\n' +
      '\n' +
      'Vector-valued multi-query associative recallWe test the model of ability to multi-query associative recall (MQAR) (Arora et al., 2023). MQAR은 ICL 태스크가 아니지만, 모델의 연관 회상(AR) 수행 능력은 모델의 인컨텍스트 학습 능력과 높은 관련이 있다(Olsson et al., 2022). 문맥에서 정보를 검색하는 모델의 능력을 더 잘 측정하기 위해 키와 값이 벡터 값으로 계산되어 각 벡터가 "고유 토큰"으로 보일 수 있도록 MQAR의 변형을 고려하고 검색된 벡터와 대상 벡터 간의 평균 제곱 오차로 검색 정확도를 측정할 수 있다. 구체적으로 이 과제에서 모델은 벡터\\(\\{\\mathbf{k}_{1},\\mathbf{v}_{1},...,\\mathbf{k}_{n},\\mathbf{v}_{n}\\})의 키-값 쌍들의 시퀀스가 주어지며, 여기서 \\(\\mathbf{k}_{i},\\mathbf{v}_{i}\\in\\mathcal{S}^{d-1}\\)은 단위\\(d\\)-구로부터 균일하게 샘플링된다. 질의는 벡터\\(\\{\\mathbf{q}_{1},...,\\mathbf{q}_{m}\\})의 시퀀스로 구성된다. 각 질의 \\(\\mathbf{q}_{j}\\)에는 \\(1\\leq l\\leq n\\)이 존재하여 \\(\\mathbf{q}_{j}=\\mathbf{k}_{l}\\)이 존재한다. 모델은 질의와 관련된 \\(\\mathbf{v}_{l}\\)의 출력을 각 질의에 대해 학습해야 하며, 총 \\(m\\)의 출력을 생성한다. 우리는 제곱 오차(square error)를 이용하여 모델을 학습시킨다.\n' +
      '\n' +
      '##4 실험 결과\n' +
      '\n' +
      '이 섹션에서는 맘바가 다양한 ICL 작업을 수행하기 위해 처음부터 훈련될 수 있음을 보여준다. 또한, 한 모델이 다른 모델보다 더 잘 수행하고 그 반대의 경우 특정 작업을 식별한다.\n' +
      '\n' +
      '#Mamba _can_ in-context learn!\n' +
      '\n' +
      '도 2에 도시된 바와 같이, Mamba는 보다 단순한 대응물 S4-Mamba 및 S4를 일관되게 능가한다. 선형 회귀와 같은 간단한 작업에서, Mamba와 S4-Mamba 사이의 갭은 S4-Mamba 및 S4의 갭보다 훨씬 작다. Mamba와 S4-Mamba의 주요 차이가 입력 의존적 선택 메커니즘이라는 점을 감안할 때, MLP의 적절한 게이팅 및 적층(_i.e._, S4-Mamba와 S4의 차이)이 이러한 작업에 더 중요한 것으로 보인다. 그러나 이에 비해 입력 종속 선택은 2NN 회귀 및 학습 결정 트리와 같은 보다 복잡한 작업에 대해 의미 있는 진전을 이룬다.\n' +
      '\n' +
      'Mamba는 또한 전체 FLOP가 확장되는 경우에도 트랜스포머와 동등하게 수행할 수 있다. 트랜스포머와 관심이 독특한 ICL 기능으로 많은 이전 작업의 초점이었다는 점을 감안할 때 이는 놀라운 일이다. 더욱이, Mamba는 동일한 깊이, _i.e._, 주의의 수, MLP 및 Mamba 블록을 동등하게 유지할 때 더 작은 파라미터 설정에서 더 나은 성능을 수행하는 경향이 있다.\n' +
      '\n' +
      '더 복잡한 ICL 태스크의### 성능 격차\n' +
      '\n' +
      '우리는 또한 더 복잡한 ICL 과제의 패밀리, 즉 학습 결정 트리, 희소 패리티 및 사고 사슬을 고려한다(그림 2 및 4). 그림은 트랜스포머가 의사 결정 트리와 벡터 값 MQAR을 해결할 수 있는 반면 맘바는 해결할 수 없음을 보여준다. 그러나 그림 5의 희소패리티 과제에서 Transformer는 Mamba가 할 수 있는 반면 함수 패밀리를 학습할 수 없다.\n' +
      '\n' +
      '회귀분석에서 이상치 필터링.직교-이상치 회귀와 다수-이상치 회귀는 다른 이상치 태스크와 마찬가지로 \\(\\mathbf{x}_{i}\\in\\mathbf{w}^{\\perp}\\) 또는 \\(\\mathbf{y}_{i}\\)이 제로 패딩 스칼라 값이 아닌 벡터라는 사실에 의해 더미 벡터를 무시하는 모델의 학습 능력에 초점을 맞춘다. 이것은 모델들이 이전의 입력 시퀀스들을 살펴보고, 트레이닝 프롬프트가 나타내는 함수들의 클래스를 학습하면서 더미 벡터들을 트레이닝 예들로부터 구별하는 속성들을 발견하도록 명시적으로 요구한다.\n' +
      '\n' +
      '시퀀스 길이가 101(작업 설명은 표 2 참조)로 비교적 짧은 직교 이상치 회귀 작업의 경우, Mamba는 그림 3에서 볼 수 있듯이 Transformer와 동등하게 수행합니다. 흥미롭게도 512 및 90% 전-온 대체 시퀀스 길이를 테스트하는 다 이상치 회귀의 경우, Mamba는 Transformer보다 상당히 우수합니다. 이는 구와 도(2023)가 무엇이었는지와도 일맥상통한다.\n' +
      '\n' +
      '그림 2: ICL 작업 세트는 트랜스포머, 맘바, S4 및 하이브리드 아키텍처를 위해 실행되었으며 각 색상이 다른 아키텍처를 나타낸다. 더 투명한 점은 훈련의 초기 단계를 나타내며, 플롯된 모델은 [100k, 500k] 반복 사이에서 훈련된다. 우리는 각 모델을 훈련에 사용되는 부동 소수점 연산(FLOP)의 수로 표현한다.\n' +
      '\n' +
      '보고서에 따르면, 맘바는 긴 시퀀스 길이에 대한 유도 작업에 더 좋다. 이 두 결과는 Mamba가 선형 회귀를 맥락 내에서 학습할 수 있는 능력을 유지하면서 불필요한 정보를 걸러내는 데 큰 문제가 없음을 나타낸다.\n' +
      '\n' +
      'Cain-of-Thought I/O.그림 4는 Mamba 모델이 테스트된 구성 전반에 걸쳐 트랜스포머 모델과 비교 가능하게 수행하면서 Context-of-thought 방식으로 학습할 수 있음을 보여준다. 더 작은 모델 구성에서, 맘바 모델은 트랜스포머 모델에 비해 우수한 성능을 나타낸다. 그러나 모델 크기가 증가함에 따라 트랜스포머 모델은 맘바 모델을 능가하기 시작한다. 트랜스포머 모델의 성능은 다른 문제 크기에 걸쳐 비교적 안정적으로 유지되는 반면, 맘바 모델의 성능은 은닉층의 크기에 크게 영향을 받는다. 구체적으로, Mamba 모델은 더 작은 문제 크기(_i.e._, 더 작은 숨겨진 차원)에서 Transformer 모델보다 우수하지만, 문제 크기가 확장됨에 따라 이점이 감소한다.\n' +
      '\n' +
      '패러티와 리트리브에서의 도전\n' +
      '\n' +
      '벡터값 MQAR은 (1)\\(32\\) 키-값 쌍과 \\(16\\) 질의와 (2)\\(32\\) 키-값 쌍과 \\(4\\) 질의의 두 가지 설정에서 실행된다. 표 3에서 우리는 모든 경우에 정규화된 벡터를 검색하기 위한 평균 제곱 오차가 \\(0.1\\)보다 커서 Mamba가 벡터를 정확하게 검색하지 못한다는 것을 알 수 있다.\n' +
      '\n' +
      'Sidenote로서, \\(16\\) 질의로 훈련된 모든 모델은 \\(4\\) 질의로 훈련된 모델보다 테스트 손실이 낮다. 가능한 설명은 MQAR 태스크를 나타내는 데이터의 단일 시퀀스에 대해, 각 \\((\\mathbf{q},\\mathbf{v})\\) 쌍을 "트레이닝 샘플"로 생각할 수 있기 때문에, \\(16\\) 쿼리가 있는 시퀀스는 \\(4\\) 쿼리가 있는 시퀀스보다 더 많은 "트레이닝 샘플"을 포함한다는 것이다. 이것은 또한 더 많은 질의를 갖는 것이 반드시 작업을 더 어렵게 만드는 것은 아님을 보여준다.\n' +
      '\n' +
      '그림 4: Chain-of-Thought-I/O 작업에서 Transformer와 Mamba 모델의 성능. 모델 크기를 변경하는 실험(왼쪽) 및 숨겨진 차원을 변경하는 실험(오른쪽)\n' +
      '\n' +
      '그림 3: 직교 이상치 회귀 작업에서 FLOP의 총 수에 대한 곡선을 스케일링합니다.\n' +
      '\n' +
      '맘보는 MQAR과 같은 간단한 검색 작업에서 실패하지만, 표는 희소 패리티를 학습하는 작업으로 돌아간다(그림 5). Transformer는 Bhattamishra et al.(2023)의 경험적 증거에 따라 무작위 추측보다 더 잘 하지 못한다. 최대 100만 번의 반복 훈련 시 임베딩 차원이 최대 768개, 최대 24개 레이어인 트랜스포머 크기의 경우임을 확인한다. 그러나, Mambo는 2개의 층만큼 작은 네트워크를 갖는 \\((d,k)=(10,2)\\)에 대한 희소 패리티를 쉽게 풀며 이 작업에 성공한다. 더욱 놀랍게도, S4-맘보는 패리티도 해결할 수 있다; 이것은 적절한 컨볼루션 또는 게이팅이 입력 종속 선택보다 더 중요할 수 있다는 것을 의미할 수 있다. 본 연구의 결과는 Mambo가 주의계층 이전에 제공하는 초기(인과적) 컨볼루션이 컴퓨터 비전 태스크에서 비전 트랜스포머에 대해 관찰되는 유사한 현상인 패리티를 해결하는 데 중요할 수 있음을 암시한다(Yu et al., 2022).\n' +
      '\n' +
      '패리티 학습을 위한 알고리즘은 \\(\\omega(d)\\)의 초선형 메모리 또는 \\(d\\)(Raz, 2016; Kol et al., 2017)의 초다항식 샘플 수를 필요로 하는 것으로 알려져 있다. 트랜스포머는 2차 주의 메커니즘으로 인해 메모리가 더 나은 것으로 알려져 있지만 희소 패리티를 학습한 결과는 서로 다른 아키텍처가 메모리를 어떻게 측면에서 다르게 활용할 수 있는지에 대한 질문을 제기한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Embedding dimension (\\(d\\)) & 64 & 128 \\\\ \\hline Mambo & 7.23e-1 & 1.50e-1 \\\\\n' +
      '6 MambaBlocks + 1 Standard Hybrid & 1.54e-3 & 5.86e-5 \\\\ Transformer w/o PE & 7.61e-5 & 5.55e-5 \\\\ Transformer w/ PE & 3.99e-5 & **2.46e-7** \\\\ MambaFormer & **1.03e-5** & 3.79e-7 \\\\ \\hline \\hline \\end{tabular} (a) \\(32\\) key-value pairs with \\(16\\) queries.\n' +
      '\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Embedding dimension (\\(d\\)) & 64 & 128 \\\\ \\hline Mambo & 8.64e-1 & 1.64e-1 \\\\\n' +
      '6 MambaBlocks + 1 Standard Hybrid & 1.99e-2 & 1.37e-2 \\\\ Transformer w/o PE & 1.14e-3 & 8.66e-5 \\\\ Transformer w/ PE & **5.17e-6** & **8.76e-7** \\\\ MambaFormer & 7.30e-6 & 3.37e-6 \\\\ \\hline \\hline \\end{tabular} (b) \\(32\\) key-value pairs with \\(4\\) queries.\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: 벡터값 MQAR 및 각각의 모델 구성에 대한 테스트 손실(평균 제곱 오차) Positional Encoding (PE)을 사용하는 트랜스포머와 사용하지 않는 트랜스포머를 시험한다. 모든 모델은 대략 동일한 수의 매개변수를 갖는 \\(4\\) 레이어를 갖는다. 예외는 "\\(6\\) MambaBlocks + 1 표준 하이브리드" 모델이지만, 우리는 여전히 [\\(6\\) MambaBlocks]이 그림 6에 설명된 바와 같이 \\(3\\) Mamba Layer와 동등하기 때문에 \\(4\\) Layer를 갖는 (등가적으로) 것으로 간주한다.\n' +
      '\n' +
      '그림 5: 트랜스포머가 수렴하지 못하지만, Mamba와 S4-Mambo는 \\(d=10\\)과 \\(k=2\\)의 희소 패리티를 학습할 수 있다. 각 모델은 256의 임베딩 차원과 12개의 레이어(약 1,000만 파라미터)의 깊이로 최대 500k 반복으로 학습된다. 트랜스포머는 768, 24레이어의 임베딩 차원까지 학습하지 못했다.\n' +
      '\n' +
      '의 함수 근사치를 갖는 것을 특징으로 하는 방법. 우리는 어떤 건축적 요소가 더 많은 연구를 위한 길목으로서 학습 패리티를 허용하는지에 대한 이론적, 경험적 질문을 남긴다.\n' +
      '\n' +
      '##5 상황내 학습을 위한 하이브리드 구조의 장점\n' +
      '\n' +
      '이전 섹션에서는 트랜스포머가 학습 결정 트리 또는 검색과 같은 일부 작업에서 SSM보다 더 나은 성능을 보이는 반면, SSM은 학습 희소 패리티 또는 학습 중이상치 선형 회귀와 같은 다른 작업에서 재발성 때문일 수 있음을 관찰했다. 그러나 ICL 작업 세트에서 성능을 희생하지 않고 두 세계의 최고를 달성할 수 있습니까?\n' +
      '\n' +
      '우리는 이것을 긍정적으로 답한다; 우리는 트랜스포머와 맘바에 버금가는 성능을 달성하면서 동시에 실패하는 특정 작업에서 탁월하면서 ICL 작업 세트에서 경쟁력 있는 성능에 도달할 수 있다는 것이다. 우리는 주목과 맘바를 인터리빙함으로써 강력한 성능을 얻을 수 있는데, 여기서 핵심 요소는 맘바를 첫 번째 레이어로 가지고 있다.\n' +
      '\n' +
      '본 절에서는 Transformer와 Mamba를 결합한 두 가지 하이브리드 아키텍처, 즉 그림 6과 같이 Standard Hybrid와 Mamba former를 조사한다. Standard Hybrid는 MLP 블록을 Mamba로 대체하여 MHA와 Mamba를 인터리빙하는 아키텍처이다. Mamba former는 Standard Hybrid와 거의 동일하지만 초기 계층으로 추가 Mamba 블록이 있으며 특별한 위치 인코딩이 없다. 많은 작품들이 인터리빙 멀티헤드 어텐션 및 LTI SSM이 유익하다는 것을 발견했지만(Zuo et al., 2022; Mehta et al., 2022; Pilault et al., 2023), 흥미롭게도 Gu 및 Dao(2023)는 인터리빙의 상당한 이점을 발견하지 못했다. 다음 결과에서, Mamba를 초기 계층으로 인터리빙하는 것이 Mamba 및 Transformer가 풀 수 없는 희소 패리티 및 검색 모두를 해결하는 데 도움이 될 수 있음을 보여준다.\n' +
      '\n' +
      '### 패러티와 검색을 동시에 학습하는 것\n' +
      '\n' +
      'Bhattamishra et al. (2023); Barak et al. (2022)에서 강조된 바와 같이, 트랜스포머 및 Hyena와 같은 일부 SSM에서는 희소 패리티 인-컨텍스트를 학습하는 것이 어려운 것으로 보인다. 그러나 흥미롭게도 그림 7에서 볼 수 있듯이 Mamba former는 샘플 복잡성 측면에서 Mamba만큼 빠르게 패리티를 성공적으로 학습한다. 표준 하이브리드 모델도 가능하지만 훨씬 더 나쁜 샘플 효율성을 나타냅니다.\n' +
      '\n' +
      '우리는 트랜스포머에 위치 인코딩 없이 초기 Mamba 블록을 장착하여 절제 연구를 수행한다. 이 변형 트랜스포머는 표준 하이브리드보다 Mamba 블록이 적지만 Mamba만큼 패리티를 효율적으로 해결합니다. 이는 Press et al. (2022)에 도시된 바와 같이 인터리빙 물질 내의 층들의 순서를 우리에게 보여줄 뿐만 아니라, Mamba가 ICL에서의 성능을 해치지 않고 Transformer를 보완할 수 있다는 것을 보여준다. 이 결과는 주의 집중과 맘바의 기능 학습 능력 사이에 흥미로운 차이를 가져오며, 이 질문은 추가 연구를 위해 남겨둔다.\n' +
      '\n' +
      '도 6: 모델 아키텍처. (a) 및 (b)는 표준 트랜스포머 및 맘바 아키텍처를 나타낸다. (c)는 구와 도(2023)에서 제안된 설계에 따른, 맘바와 어텐션 블록의 하이브리드 아키텍처를 나타낸다. (d) 제안된 구조, 즉 MambaFormer는 Positional Encoding을 Mamba 블록으로 대체한다. 편의상, 우리는 Mamba, Multi-head Attention 또는 Feed Forward Network의 2개의 블록을 1개의 레이어로 표시한다.\n' +
      '\n' +
      '벡터값 MQAR 태스크에서 Mamba와 Transformer 사이의 갭은 Mamba가 출력을 생성할 때 문맥을 더 작은 상태로 압축하는 반면, Transformer에서의 Attention 메커니즘은 문맥을 압축하지 않기 때문이다. Mamba가 각각의 상태에서 갖는 컨텍스트에 관한 정보의 양은 숨겨진 상태의 차원에 의존하며(숨겨진 상태들이 컨텍스트에서 중요한 정보를 캡처하기 때문에) 태스크가 컨텍스트 뒤에 배치되는 쿼리에 의해 컨텍스트의 특정 부분을 정확하게 검색하는 것인지는 어렵다.\n' +
      '\n' +
      'Mamba와 Transformer 사이의 벡터값 MQAR 태스크의 갭을 좁히고, 효율을 너무 많이 희생시키지 않고, Mamba 블록의 레이어 내에 하나의 주의 레이어를 추가한다. 특히, \\(4\\) 층의 Mamba 모델(\\(8\\) Mamba 블록을 균질하게 적층)에서, 중간 두 블록을 표준 하이브리드(w/o 위치 임베딩)로 대체한다. 표 3에 나타난 바와 같이, 맘바 모델은 하나의 표준 하이브리드를 가짐으로써 벡터 값 MQAR에서 상당한 개선을 얻는다. 또한 동일한 작업에서 Mamba former를 테스트하고 벡터 값 MQAR 작업에서 Mamba former가 변압기와의 간격을 거의 완전히 닫는다는 것을 발견했다.\n' +
      '\n' +
      '### 일체형 ICL 성능\n' +
      '\n' +
      'Mamba former는 Transformer용 Mamba가 어렵다고 여겨졌던 두 가지 작업을 성공하지만, 나머지 ICL 작업 세트에서 Transformer와 Mamba뿐만 아니라 동등하게 수행합니다. 그림 2에서 우리는 MambaFormer와 Standard Hybrid가 더 큰 매개변수 크기에서도 트랜스포머뿐만 아니라 의사 결정 트리를 모두 학습한다는 것을 알 수 있다. 더 놀랍게도, Mamba former는 100k 반복(\\(<10^{17}\\) FLOP)으로 훈련된 Mamba former가 FLOP의 10배 수로 훈련된 모델뿐만 아니라 수행함에 따라 많은 이상치 회귀에서 90% 노이즈 데이터가 있는 경우에도 두 모델보다 선형 회귀를 효율적으로 학습한다.\n' +
      '\n' +
      '결론적으로, 우리는 다른 ICL 작업에서 트랜스포머와 맘바와 동등하게 수행하면서 검색과 패리티만큼 어려운 문제를 해결할 수 있는 하이브리드 아키텍처인 다양한 ICL 작업에서 두 세계 중 최고를 찾는다. 우리의 결과들을 고려할 때, 하이브리드 아키텍처들이 다른 종류의 ICL 태스크들에서 어떻게 수행되는지 보는 것은 흥미로울 것이다(Xie et al., 2021; Akyurek et al., 2024).\n' +
      '\n' +
      '그림 7: 최대값에 대한 랜덤 시드 5개 이상의 패리티 학습 중간 수렴 시간. 500k 반복, 여기서 500k 수렴 시간은 실패한 학습을 의미한다. 초기 계층을 맘바(Mamba)로 갖는 것은 패리티를 효율적으로 학습하는 데 필수적이다. 테스트한 모델 구성은 부록 A에 명시되어 있습니다.\n' +
      '\n' +
      'Discussion\n' +
      '\n' +
      '본 연구에서는 상태 공간 모델(State-space model, SSM)을 이용한 상황 내 학습에 대한 종합적인 조사를 제공하고 이를 변압기 구조와 비교하였다. 우리의 연구는 SSM, 특히 맘바가 맥락 내 학습자가 가능하다는 것을 밝혀냈다. 다른 한편으로, 우리의 평가들은 SSM들 또는 트랜스포머들 모두 모든 태스크들에서 훌륭하지 않다는 것을 밝혀냈는데, 구체적으로, SSM들은 의사 결정 트리 학습 및 검색 태스크들에 대해 어려움을 겪는 반면, 트랜스포머들은 희소 패리티에 대해 어려움을 겪는다는 것을 밝혀냈다. 이를 통해 ICL 스위트룸에서 두 가지 세계 최고의 성능을 달성하는 하이브리드 아키텍처 Mamba former로 연결되었습니다.\n' +
      '\n' +
      '향후 연구 방향은 (1) ICL 제품군의 성능이 표준 NLP 벤치마크에 대한 복잡성과 같은 일반적인 언어 모델링 능력과 어떻게 상관관계가 있는지 탐구하는 것, (2) 변압기, SSM 및 게이팅 메커니즘의 요소를 통합하여 보다 효과적인 아키텍처를 개발할 수 있는 가능성, (3) 효과적인 컨텍스트 내 학습에 기여하는 아키텍처 특징을 식별하는 것, (4) Mamba former 및 기타 혁신적인 아키텍처가 언어 모델링 성능에 미치는 영향을 평가하는 것을 포함한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. 트랜스포머는 상황 내 학습을 위해 사전 조건화된 경사 하강을 구현하는 것을 학습한다. _ arXiv preprint arXiv:2306.00297_, 2023.\n' +
      '* Akyurek et al. (2022) Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. what learning algorithm is in-context learning? 선형 모델을 사용한 조사. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Akyurek et al. (2024) Akyurek, E., Wang, B., Kim, Y., and Andreas, J. In-context 언어 학습: Architecture and algorithms. _ arXiv preprint arXiv:2401.12973_, 2024. URL[https://arxiv.org/abs/2401.12973](https://arxiv.org/abs/2401.12973).\n' +
      '* Arora et al. (2023) Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and Re, C. Zoology: measurement and improving recall in efficient language models. _ arXiv preprint arXiv:2312.04927_, 2023. URL[https://arxiv.org/abs/2312.04927](https://arxiv.org/abs/2312.04927).\n' +
      '* Bai et al. (2023) Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. 통계학자로서의 트랜스포머: 상황 내 알고리즘 선택으로 상황 내 학습을 제공할 수 있다. _ arXiv preprint arXiv:2306.04637_, 2023.\n' +
      '* Barak et al. (2022) Barak, B., Edelman, B., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden progress in deep learning: Sgd learn parities near the computational limit. _ 신경 정보 처리 시스템_, 35:21750-21764, 2022에서의 발전.\n' +
      '* Beltagy et al. (2020) Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. _ arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* Bhattamishra et al. (2023) Bhattamishra, S., Patel, A., Blunsom, P., and Kanade, V. 이산 함수를 학습하는 학습을 통해 변압기와 llms에서의 문맥 내 학습에 대한 이해 arXiv preprint arXiv:2310.03016_, 2023. URL[https://arxiv.org/abs/2310.03016](https://arxiv.org/abs/2310.03016)이다.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Chan et al. (2022) Chan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F. Data Distributional properties drive emergent in-context learning in transformers. _ 신경 정보 처리 시스템_, 35:18878-18891, 2022에서의 발전.\n' +
      '* Dai et al. (2023) Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn in-context? 언어 모델은 메타 최적화 도구로서 기울기 하강을 비밀리에 수행한다. In _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 4005-4019, 2023.\n' +
      '* Dao et al. (2022) Dao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry huppos: with language modeling with state space models. _ arXiv preprint arXiv:2212.14052_, 2022. URL[https://arxiv.org/abs/2212.14052](https://arxiv.org/abs/2212.14052).\n' +
      '* Gao et al. (2020) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. pile: An 800gb dataset of diverse text for language modeling, 2020.\n' +
      '* Garg et al. (2022) Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. Transformers learn in-context? 단순 함수 클래스 사례 연구. _ 신경 정보 처리 시스템_, 35:30583-30598, 2022에서의 발전.\n' +
      '* Gu & Dao(2023) Gu, A. and Dao, T. Mamba: 선택적 상태 공간을 갖는 선형-시간 시퀀스 모델링 _ arXiv preprint arXiv:2312.00752_, 2023. URL[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)이다.\n' +
      '* Gu et al. (2022) Gu, A., Goel, K., Gupta, A., and Re, C. On the parameterization and initialization of diagonal state space models. In _Advances in Neural Information Processing Systems_, volume 35, pp. 35971-35983, 2022a.\n' +
      '* Gu et al. (2020)Gu, A., Goel, K., and Re, C. Efficiently modeling long sequence with structured state spaces. _The Tenth International Conference on Learning Representations, ICLR 2022_, 2022b. URL[https://openreview.net/forum?id=uYLFo2lv1AC](https://openreview.net/forum?id=uYLFo2lv1AC).\n' +
      '* Kaplan 등 (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Katharopoulos et al. (2020) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformer with linear attention. 제37회 국제기계학습회의의 _Proceedings of the 37th International Conference on Machine Learning, ICML 2020_, vol 119 of _Proceedings of Machine Learning Research_, pp. 5156-5165, 2020. URL[http://proceedings.mlr.press/v119/katharopoulos20a.html](http://proceedings.mlr.press/v119/katharopoulos20a.html)에 개시되어 있다.\n' +
      '* Kingma & Ba(2014) Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Kol et al. (2017) Kol, G., Raz, R., and Tal, A. Time-space hardness of learning sparse parities. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, pp. 1067-1080, 2017.\n' +
      '* Li et al. (2023a) Li, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. 알고리즘으로서의 트랜스포머: In-context learning에서의 일반화 및 안정성, 2023a.\n' +
      '* Li et al. (2023b) Li, Y., Sreenivasan, K., Giannou, A., Papailiopoulos, D., and Oymak, S. 해부 사상 연쇄: 문맥 내 필터링 및 학습을 통한 구성성. 30-7차 신경 정보 처리 시스템 회의에서_, 2023b.\n' +
      '* Mahankali et al. (2023) Mahankali, A., Hashimoto, T. B., and Ma, T. 경사 하강의 한 단계는 선형 자기 주의의 한 층을 갖는 최적의 상황 내 학습기이다. _ arXiv preprint arXiv:2307.03576_, 2023.\n' +
      '* Mehta et al. (2022) Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated state spaces. _ ArXiv:2206.13947_, 2022.\n' +
      '* Min et al. (2022a) Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. Metaicl: Learning to learn in context. In _Proceedings of the 2022 Conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 2791-2809, 2022a.\n' +
      '* Min et al. (2022b) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. 시연의 역할에 대한 재고: 상황 내 학습이 작동하는 이유는 무엇인가? arXiv preprint arXiv:2202.12837_, 2022b. URL[https://arxiv.org/abs/2202.12837](https://arxiv.org/abs/2202.12837).\n' +
      '* Muennighoff et al. (2023) Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T., and Raffel, C. Scaling data-constrained language models. _30-7th Conference on Neural Information Processing Systems_, 2023. URL[https://openreview.net/forum?id=j5BuTrEj35](https://openreview.net/forum?id=j5BuTrEj35).\n' +
      '*Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., and et al., A. C. In-context learning and induction head. _ arXiv preprint arXiv:2209.11895_, 2022. URL[https://arxiv.org/abs/2209.11895](https://arxiv.org/abs/2209.11895).\n' +
      '* Peng et al. (2023) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., Kiran, K. G., et al. Rwkv: Reinventing rnns for the transformer era. _ arXiv preprint arXiv:2305.13048_, 2023. URL[https://arxiv.org/abs/2305.13048](https://arxiv.org/abs/2305.13048)이다.\n' +
      '* Pilault et al. (2023) Pilault, J., Fathi, M., Firat, O., Pal, C., Bacon, P.-L., and Goroshin, R. 블록-상태 변압기. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '*Poli et al. (2023) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and Re, C. Hyena hierarchy: Towards of larger convolutional language models. _ arXiv preprint arXiv:2302.10866_, 2023. URL[https://arxiv.org/abs/2302.10866](https://arxiv.org/abs/2302.10866).\n' +
      '* Press et al. (2022) Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. 언어모델에서의 조성격차를 측정하고 좁히는 단계; _ arXiv preprint arXiv:2210.03350_, 2022.\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.\n' +
      '* Radford et al. (2019)Ravi, S. and Larochelle, H. Optimization as a model for few-shot learning. _International conference on learning representations_, 2016.\n' +
      '* Raz[2016] Raz, R. 빠른 학습은 좋은 기억을 필요로 한다: 패리티 학습을 위한 시간-공간 하한. In _2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)_, pp. 266-275. IEEE, 2016.\n' +
      '* Schaeffer et al. [2023] Schaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models a mirage? In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=ITw9edRDID](https://openreview.net/forum?id=ITw9edRDID).\n' +
      '* Schmidhuber et al. [1997] Schmidhuber, J., Zhao, J., and Wiering, M. Shifting inductive bias with success-story algorithm, adaptive levin search, and incremental self-improvement. _Machine Learning_, 28:105-130, 1997.\n' +
      '* Sun et al. [2023] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models. _arXiv preprint arXiv:2307.08621_, 2023. URL [https://arxiv.org/abs/2307.08621](https://arxiv.org/abs/2307.08621).\n' +
      '* Vaswani et al. [2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017_, pp. 5998-6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).\n' +
      '* von Oswald et al. [2023a] von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. 트랜스포머는 기울기 하강에 의해 문맥 내에서 학습한다. In _International Conference on Machine Learning_, pp. 35151-35174. PMLR, 2023a.\n' +
      '* von Oswald et al. [2023b] von Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Vladymyrov, M., Pascanu, R., et al. Uncovering mesa-optimization algorithms in transformer. _ arXiv preprint arXiv:2309.05858_, 2023b. URL[https://arxiv.org/abs/2309.05858](https://arxiv.org/abs/2309.05858).\n' +
      '* Wang et al. [2020] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.\n' +
      '* Wei et al. [2022] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.\n' +
      '* Xie et al. [2021] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. In _International Conference on Learning Representations_, 2021.\n' +
      '* Yang et al. [2023a] Yang, L., Lee, K., Nowak, R., and Papailiopoulos, D. Looped Transformers is better to learning learning algorithms, 2023a.\n' +
      '* Yang et al. [2023b] Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. 하드웨어-효율적인 트레이닝을 갖는 게이티드 선형 주의 변압기. _ arXiv preprint arXiv:2312.06635_, 2023b.\n' +
      '* Yu et al. [2022] Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10819-10829, 2022.\n' +
      '* Zuo et al. [2022] Zuo, S., Liu, X., Jiao, J., Charles, D., Manavoglu, E., Zhao, T., and Gao, J. Efficient long sequence modeling via state space augmented transformer. _arXiv preprint arXiv:2212.08136_, 2022.\n' +
      '\n' +
      'Experimental Setup\n' +
      '\n' +
      '### Model Architectures\n' +
      '\n' +
      '우리는 디코더 전용 트랜스포머 모델들, 특히 GPT-2 패밀리(Radford et al., 2019), 맘바(Gu & Dao, 2023), 및 그들의 하이브리드 변형들, 이를 포함하는 표준 및 맘바 이전의 구성들에 초점을 맞춘다. 이러한 모델은 표 4에 자세히 설명된 대로 다양한 크기에 걸쳐 평가된다. 변압기 층은 MHA(Multi-Head Attention) 블록에 이어 MLP(Multilayer Perceptron) 블록으로 구성된다. 맘바 모델은 층당 두 개의 맘바 블록으로 구성된다. 하이브리드 변형은 단일 MHA 블록을 맘바 블록과 결합하여 이러한 접근법을 병합한다. MHA 블록의 경우 8개의 헤드를 사용합니다. 고려된 아키텍처의 시각화는 그림 6을 참조하십시오.\n' +
      '\n' +
      '### Model Training\n' +
      '\n' +
      '우리는 모든 작업에 대한 50만 번의 교육 단계를 위해 A100-SXM4-40GB GPU에서 모든 모델을 교육합니다. 우리는 고정된 학습률을 갖는 Adam Optimizer Kingma & Ba(2014)를 사용한다. 기본값은 Garg et al.(2022)의 기본 학습률에 따라 \\(0.0001\\)으로 설정하고, \\(\\{5e-5,1e-4,2e-4,4e-4\\}\\)에서 다양한 학습률을 검색한다. 우리는 훈련 절차가 올바른 학습률을 선택하는 데 가장 민감하다는 것을 관찰한다. 특히, 모델의 매개변수 수가 증가함에 따라 훈련 절차는 특히 맘바와 하이브리드 아치컷에서 구배 폭발이 발생하기 쉽다. 따라서 우리는 \\(\\{5.0,10.0,50.0\\}\\)의 값을 갖는 기울기 규범을 클리핑한다.\n' +
      '\n' +
      '열차 및 시험 데이터는 \\(\\mathbf{x}\\)의 치수를 \\(20\\)으로 고정하고 배치 크기를 \\(64\\)으로 고정한다. Garg et al. (2022)에서 제시된 바와 같이, 우리는 또한 특정 ICL 과제에서 커리큘럼이 중요하다는 것을 관찰한다. 우리는 \\(\\mathbf{x}\\)의 차원과 포인트 수(교육 프롬프트 길이의 절반)에 대해 2000단계마다 15단계의 교육과정을 채택한다.\n' +
      '\n' +
      '## 부록 B 구현 상세\n' +
      '\n' +
      '이 섹션에서는 섹션 3의 작업 설명에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Chain-of-Thought-I/O results\n' +
      '\n' +
      '표 5는 Li 등(2023)에 의해 설명된 설정에 따라, 2-레이어 ReLU 신경망을 사용하는 Chain-of-Thought-I/O 태스크에 대한 구성들을 제시한다. 모델 스케일 실험에서 입력 차원 \\(d=10\\)과 은닉층 차원 \\(k=8\\)은 모델 스케일을 변화시키면서 일정하게 유지된다. 또한, 문제척도의 영향을 파악하기 위해 모형척도를 작게 고정시키면서 은닉차원 \\(k\\)을 \\(4,8,16\\)으로 변화시켰다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\# layers & embed dim \\\\ \\hline Standard & 12 & 768 \\\\ Small & 8 & 512 \\\\ X-small & 4 & 256 \\\\ XX-small & 2 & 128 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 모델의 상이한 구성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Model & \\# layers & embed dim & \\# heads (MHA) \\\\ \\hline Standard & 12 & 256 & 8 \\\\ Small & 6 & 128 & 4 \\\\ Tiny & 3 & 64 & 2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: Chain-of-Thought-I/O 실험을 위한 모델 구성.\n' +
      '\n' +
      '### Vector-valued MQAR\n' +
      '\n' +
      '트레이닝 세트는 \\(300,000\\) 트레이닝 샘플로 구성된다. 우리는 배치크기가 \\(64\\)인 \\(64\\) 에폭에 대해 훈련하고 \\(3,000\\) 샘플들의 테스트 세트에서 평가한다. 각 설정에 대해 np.logspace(-4, -2, 4)에서 학습률로 스윕하고 모든 학습률 중 가장 좋은 결과를 보고한다.\n' +
      '\n' +
      '### Orthogonal-outlier Regression\n' +
      '\n' +
      '우리는 다양한 수의 매개변수를 사용하여 네 가지 모델 아키텍처 모두에 대해 직교 이상치 회귀를 실행합니다. 위의 곡선은 일반적으로 맘바와 트랜스포머가 서로 동등하게 수행하는 추세를 포착하며, 스탠다드 하이브리드 및 맘바전르는 바닐라 모델에 비해 더 나은 곡선을 보여준다.\n' +
      '\n' +
      '눈에 띄는 곡선은 더 작은 체제에서 맘바의 손실 곡선일 것이다. 향후 방향으로서 표 6에서 가장 작은 맘바 모델 구성으로 총 매개변수 수를 고정하면서 깊이와 너비를 트레이드오프하는 것을 테스트했다.\n' +
      '\n' +
      '가장 성능이 좋은 모델 구성은 16개의 레이어와 64개의 임베딩 차원으로 구성되었으며, 이는 고정된 수의 파라미터로 인해 깊이가 증가하면 ICL 태스크에서 다운스트림 태스크 성능이 향상될 수 있음을 시사한다.\n' +
      '\n' +
      '## 부록 C FLOPs 계산\n' +
      '\n' +
      '표 7과 표 8의 Mamba 블록과 Transformer 블록에서 곱셈 횟수를 계산하고 배치 크기 \\(B=1\\)로 가정한다. FLOP를 계산하기 위해 우리는 앞통과 뒤통과 모두에서 곱하기 누적 비용을 설명하기 위해 곱셈 횟수에 6을 곱한다. 스탠다드 하이브리드 블록은 맘바 블록이 적층된 어텐션 블록이므로, 스탠다드 하이브리드 블록의 곱셈 횟수는 선형 항을 무시하고 \\(10LD^{2}+2L^{2}D\\)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Model & \\# layers & embed dim \\\\ \\hline Standard & 4 & 128 \\\\  & 2 & 192 \\\\  & 8 & 96 \\\\  & 16 & 64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 파라미터 수가 고정된 다양한 맘바 모델 구성.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline  & Number of multiplications \\\\ \\hline QKV projection & \\(3LD^{2}\\) \\\\ Outer product and multiply \\(V\\) & \\(2L^{2}D\\) \\\\ Outer projection & \\(LD^{2}\\) \\\\ \\hline FFN with ffw\\_width=4 & \\(8LD^{2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 트랜스포머 블록에서의 곱셈 횟수.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline  & Number of multiplications \\\\ \\hline Input projection & \\(2LED^{2}\\) \\\\ SSM & \\(7LEDN+2RLED\\) \\\\ Output projection & \\(LED^{2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 맘바 블록에서의 곱셈 횟수. 달리 명시되지 않는 한, 우리는 \\(E=2\\) 및 \\(R=2\\)을 가정한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>