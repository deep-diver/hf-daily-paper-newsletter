<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LLaVA-UHD: 임의의 종횡비와 고해상도 영상을 인식하는 LMM\n' +
      '\n' +
      ' Ruyi Xu\\({}^{1}\\) Yuan Yao\\({}^{2}\\) Zonghao Guo\\({}^{3}\\) Junbo Cui\\({}^{1}\\) Zanlin Ni\\({}^{1}\\) Chunjiang Ge\\({}^{1}\\)\n' +
      '\n' +
      'Tat-Seng Chua \\({}^{2}\\) Zhiyuan Liu \\({}^{1}\\) Maosong Sun \\({}^{1}\\) Gao Huang \\({}^{1}\\)1**\n' +
      '\n' +
      '싱가포르 국립대학교\n' +
      '\n' +
      '중국과학원\n' +
      '\n' +
      'xrorrim@gmail.com yaoyuanthu@gmail.com gaohuang@tsinghua.edu.cnn\n' +
      '\n' +
      '[https://github.com/thunlp/LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)\n' +
      '\n' +
      'Corresponding Authors\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '시각적 인코딩은 시각적 세계를 이해하는 데 있어 큰 멀티모달 모델(LMM)의 기초를 구성한다. 기존의 LMM은 고정된 크기와 제한된 해상도로 이미지를 처리하는 반면, 최근 이러한 방향으로의 탐색은 적응성, 효율성 및 심지어 정확성에 한계가 있다. 이 작업에서 우리는 먼저 GPT-4V와 LLaVA-1.5를 대표적인 예로 들고 시각적 인코딩 전략에 뿌리를 둔 체계적인 결함을 노출한다. 이 문제를 해결하기 위해, 우리는 어떤 종횡비와 고해상도에서도 영상을 효율적으로 인식할 수 있는 대형 멀티모달 모델인 LLaVA-UHD를 제시한다. LLaVA-UHD는 세 가지 핵심 구성 요소를 포함한다: (1) 효율적이고 확장 가능한 인코딩을 위해 네이티브 해상도 이미지를 더 작은 가변 크기 슬라이스로 분할하는 이미지 모듈화 전략, (2) 시각적 인코더로부터 이미지 토큰을 추가로 응축하는 압축 모듈, (3) LLM을 위한 슬라이스 토큰을 조직하기 위한 공간 스키마. 종합 실험을 통해 LLaVA-UHD가 9개의 벤치마크에서 2-3배 더 많은 데이터로 훈련된 확립된 LMM보다 성능이 우수함을 보여준다. 특히, LLaVA-1.5\\({}_{336\\times 336}\\)에 구축된 모델은 94%의 추론 연산만으로 6배(즉, 672\\(\\times\\)1088)의 해상도 영상을 지원하며, TextVQA에서 6.4의 정확도 향상을 달성한다. 또한, 모델은 8개의 A100 GPU(vs. 26시간 LLaVA-1.5)에서 23시간 이내에 학술 환경에서 효율적으로 훈련될 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 대규모 멀티모달 모델(LMM)[26; 11; 23; 28; 5]의 발전은 시각 언어 이해, 추론 및 상호 작용 능력의 현저한 급증을 목격했다. 이는 시각 신호를 LLM(Large Language Models)에 투영하여 시각 인코딩 전략이 근본적인 역할을 하는 세계에 대한 시각 인식을 가능하게 함으로써 달성된다[21; 3; 28]. 실세계 이미지들은 광범위한 종횡비들 및 해상도들에 상주하는 것으로 알려져, 다양한 애플리케이션들에서 LMM들에 대한 상당한 도전들을 제시한다.\n' +
      '\n' +
      '그러나, 대부분의 기존 LMM들[8; 11; 28]은 고정된 종횡비(즉, 1:1) 및 낮은 해상도(즉, 224\\(\\times\\)224)로 이미지들을 인식한다. 이러한 단순화된 설정에 대한 절충은 전형적으로 이미지 콘텐츠의 심각한 형상 왜곡 및 흐림을 초래한다. 이 문제는 특히 작은 객체 이해[20] 및 광학 문자 인식[42; 5; 17]과 같은 세밀한 능력에 대해 LMM의 능력을 상당히 손상시킨다. 더욱이, 모델은 흐릿한 이미지에 대해 최선의 추측을 하는 것만을 배울 수 있기 때문에, 이 문제는 또한 환각 문제를 악화시킨다(즉, 이미지에 사실적으로 근거하지 않은 텍스트 응답을 생성하는 것).\n' +
      '\n' +
      'LMM에 대한 다양한 종횡비와 고해상도로 이미지 인식을 달성하기 위해서는 (1) 적응성(Adaptivity)이라는 두 가지 주요 과제가 있다. 시각 인코더(예를 들어, CLIP-ViT[34])는 고정된 해상도로 사전 훈련되기 때문에, 광범위한 종횡비 및 해상도로 이미지를 다루는 것이 어려울 수 있다. 사전 훈련 시나리오에서 멀리 벗어나는 단순한 이미지 보간은 배포되지 않는 문제를 초래할 수 있다. (2) 효율. 비전 트랜스포머[13]를 이용하여 고해상도 영상을 직접 부호화하려면 영상 크기에 따른 2차 연산 비용이 필요하다. 또한, 고해상도 이미지(ViT-L/14에서 896(\\times\\)896 이미지에 대해 4096 토큰)에서 많은 수의 시각적 토큰을 처리하는 것은 LLM에 훨씬 더 많은 비용이 들 수 있다.\n' +
      '\n' +
      '또한, 부주의한 시각적 인코딩 전략은 정확성에 대한 체계적인 결함까지 초래할 수 있다. 예를 들어, 다양한 측면에서의 강력한 능력에도 불구하고, GPT-4V [2]는 객체의 수를 식별하는 것과 같은 일부 기본 능력에서 놀랍도록 투쟁할 수 있다는 것이 일반적으로 보고되었다[41]. 그러한 당혹감에 대한 기계론적 원인은 거의 알려지지 않았다. 본 연구에서는 비주얼 인코딩 전략의 관점에서 GPT-4V 결함의 첫 번째 기계론적 조사를 수행한다. GPT-4V 탐사에 대한 제어된 실험은 이 문제가 고해상도 이미지를 다루는 시각적 인코딩 전략에 부분적으로 뿌리를 내릴 수 있음을 보여준다. 대표적인 오픈 소스 LMM인 LLaVA-1.5[27]에 대한 조사도 정확성에서 체계적인 문제를 보여 적대적 공격에 대한 잠재적 취약성을 나타낸다.\n' +
      '\n' +
      '이 문제를 해결하기 위해, 우리는 종횡비와 고해상도 이미지를 효율적으로 인식하는 대형 멀티모달 모델인 LLaVA-UHD를 제시한다. 이 모델은 세 가지 핵심 요소를 가지고 있다. (1) LLaVA-UHD의 핵심은 효율적이고 확장 가능한 인코딩을 위해 네이티브 해상도 이미지를 더 작은 가변 크기 슬라이스로 분할하는 이미지 모듈화 전략이다. LLaVA-UHD의 가변 크기 슬라이스는 여러 가지 고정된 종횡비 및 해상도에 이미지를 맞추는 최근의 작업과 비교하여 패딩 또는 형상 왜곡 크기 조정 없이 네이티브 해상도 이미지에 대한 완전한 적응성을 가능하게 한다. 이것은 물방울 대 물방울 사용의 더 나은 적응성과 유사하다. 풀 필링 가변 사이즈 안경에 큐브를 맞춥니다. 또한 시각적 인코더의 사전 훈련 설정에서 약간의 편차를 보장하여 성능을 최대한 유지한다는 것을 보여준다. (2) 시각적 토큰들은 압축 계층에 의해 적당한 길이로 응축되어, LLM들에 대한 계산을 크게 감소시킨다. (3) 마지막으로 압축된 슬라이스 토큰들을 공간 스키마로 구성하여 LLM들에게 이미지 내의 슬라이스 위치들을 알린다.\n' +
      '\n' +
      '9개의 벤치마크에 대한 포괄적인 실험은 LLaVA-UHD가 LMM의 성능을 크게 향상시켜 2-3배 더 많은 데이터로 훈련된 확립된 대응물을 능가한다는 것을 보여준다. 특히, LLaVA-1.5\\({}_{336\\times 336}\\)에 구축된 모델은 94%의 추론 연산만으로 672\\(\\times\\)1088 해상도의 영상을 지원하며, TextVQA에서는 6.4의 정확도 향상, POPE에서는 3.2의 정확도 향상을 보인다. 이 장점은 더 극단적인 종횡비로 확대됩니다. 또한 ViT 매개변수에 대한 명령어 튜닝이 광범위한 이미지에 적응하기에 충분하다는 것을 보여준다. 더욱이, 모델은 23시간(vs. LLaVA-1.5의 26시간) 이내에 학업 환경에서 효율적으로 훈련될 수 있다. 8 A100 GPU에서.\n' +
      '\n' +
      '본 연구의 기여도는 크게 세 가지로 요약할 수 있다. (1) GPT-4V에 대한 시각적 인코딩 전략의 관점에서 최초의 기계론적 조사를 수행하고 체계적인 결함을 노출한다. (2) 임의의 종횡비와 고해상도 영상을 효율적으로 인식할 수 있는 대형 멀티모달 모델인 LLaVA-UHD를 제시한다. (3) 9개의 인기 벤치마크에서 LLaVA-UHD의 효과를 입증하기 위한 포괄적인 실험을 수행하고 모델에 대한 더 깊은 이해를 위한 분석을 제공한다.\n' +
      '\n' +
      '##2 파일럿 실험\n' +
      '\n' +
      'GPT-4V [2]와 LLaVA-1.5 [27]을 대표적인 예로 사용하여 기존 LMM의 시각적 인코딩 전략에 대한 파일럿 실험으로 시작한다. GPT-4V는 강력하고 가장 인정받는 독점 LMM인 반면 LLaVA-1.5는 가장 영향력 있는 오픈 소스 LMM 중 하나이다. 여러 측면에서 그들의 강력한 성과에도 불구하고, 일부 기본 역량에서 딜레마에 직면할 수 있다는 것이 흔히 보고되고 있다[41]. 예를 들어, GPT-4V는 이미지에서 객체 번호를 잘못 계산하는 경향이 있는 반면, 원인은 거의 알려지지 않았다.\n' +
      '\n' +
      '본 연구에서는 비주얼 인코딩 전략의 관점에서 GPT-4V 결함의 첫 번째 기계론적 조사를 수행한다. 핵심 아이디어는 합성 이미지를 연속 프로브로 사용하여 GPT-4V의 행동을 고도로 통제된 방식으로 평가하여 근본적인 원인을 식별할 수 있다는 것이다. 우리의 실험 결과는 GPT-4V의 일부 체계적인 결함이 적대적 공격에 잠재적으로 악용될 수 있는 시각적 인코딩 전략에 뿌리를 두고 있음을 나타낸다.\n' +
      '\n' +
      '### GPT-4V Experiments\n' +
      '\n' +
      '** 예비.** OpenAI,2 GPT-4V의 공개된 정보에 따르면, 낮은 해상도와 높은 해상도의 두 가지 이미지 처리 모드를 사용한다. (1) 저해상도 모드에서, 치수 W 및 H를 갖는 원본 이미지에 대해, 모델은 저해상도 개관 이미지만을 처리한다. (2) 고해상도 모드에서 GPT-4V는 원본 고해상도 영상의 추가 슬라이스를 처리하는데, 이때 각 슬라이스는 \\(512\\times 512\\) 해상도를 가지며, 결과적으로 총 \\(\\lceil\\frac{W}{512}\\rceil\\times\\lceil\\frac{H}{512}\\rceil\\) 슬라이스가 생성된다. GPT-4V의 새로운 고해상도 모드에 대한 실험에서 흥미로운 오류 패턴이 관찰되어 GPT-4V의 기본 시각적 인코딩 로직에 대한 탐사를 촉발한다.\n' +
      '\n' +
      '각주 2: [https://platform.openai.com/docs/guides/vision](https://platform.openai.com/docs/guides/vision)\n' +
      '\n' +
      '**이미지의 위치가 GPT-4V의 행동에 어떻게 영향을 미치는가?** 우리의 실험은 그림과 같은 이미지를 감안할 때 간단한 사례로 시작한다. 도 1의 (a)를 참조하면, GPT-4V: "이미지 내에 얼마나 많은 원이 존재하는가?" 우리는 이미지 내의 원의 위치를 변경함으로써 일련의 이미지 변형을 합성하고, 텍스트 프롬프트를 변경하지 않고 유지한다. 더 나은 신뢰성을 위해, 우리는 또한 {적색, 녹색, 흰색}\\(\\times\\){circle, 삼각형, 사각형}에서 다른 색상과 모양을 사용하여 이미지를 합성한다. 각 인스턴스에 대해 실제 반응 분포를 더 잘 근사화하기 위해 15번 쿼리합니다.\n' +
      '\n' +
      '이미지 내 각 위치에 대해 GPT-4V로 응답한 평균 수를 계산하고 그림 1의 히트맵을 보고한다. 1(b) 우리는 그 결과가 이미지 내의 객체 위치와 높은 상관 관계를 갖는다는 것을 관찰할 수 있다. 구체적으로, 패턴은 \\(256\\times 256\\) 제곱으로 분할되며, 세 가지 흥미로운 패턴을 식별할 수 있다: (1) 중앙 사각형이 가장 높은 응답 수를 나타내고, (2) 중간 모서리가 더 낮은 수를 나타내고, (3) 모서리가 접지 진실에 가장 가깝다.\n' +
      '\n' +
      '원인을 조사하기 위해 모델 반응을 수별로 추가로 분리하고 그림 1의 각 반응에 대해 위치에 따른 분포를 보고한다. 1(c), (d), (f), (g) 및 (h)를 포함한다. 흥미로운 점은 정답(4:66.1%)과 근접답(5:16.6%, 3:10.2%) 외에 지상진실을 2배, 4배인 나머지 2개의 이상답(8:5.2%, 16:1.9%)이 오류 패턴을 설명하는 것으로 나타났다. 1(b) 결과를 오픈AI의 공개 정보와 결합하여 이미지 해상도가 512.3으로 분할되지 않을 때 GPT-4V 슬라이스에 중복이 있다는 가장 가능성 있는 원인을 가정한다. 도 1(e)를 참조하면, 두 슬라이스 간의 중첩 영역은 숫자를 두 배로 하고, 네 슬라이스 간의 중첩 영역은 숫자를 네 배로 할 것이다.4\n' +
      '\n' +
      '각주 3: GPT-4V에서의 중첩은 상이한 해상도 이미지들에 걸쳐 일관성이 없기 때문에, 이슈는 CNN들에서의 중첩 슬라이딩 윈도우들과 상이하다는 것에 유의한다.\n' +
      '\n' +
      '각주 4: 시각적 인코딩 전략 외에도 모델 행동도 축적된 훈련 역학 및 RLHF에 의해 영향을 받는다는 점에 유의한다. 따라서 이중/4중 효과는 결과를 지배하지 않는다. 모든 결과는 03-05-2024의 GPT-4V에서 나온 것이다.\n' +
      '\n' +
      '그림 1: 객체의 수를 식별하는 GPT-4V의 실험 결과. (a)의 점선은 단지 설명을 위한 것이며, GPT-4V에 제시되지 않았다는 점에 유의한다.\n' +
      '\n' +
      '**이미지 해상도가 GPT-4V의 행동에 어떤 영향을 미치는가?** 가설을 검증하기 위해 지속적으로 변화하는 이미지 해상도를 통해 GPT-4V를 추가로 조사한다. 구체적으로, 우리는 그림 1의 이미지를 비례적으로 조정한다. 도 2의 (a)는 서로 다른 해상도로, 동일한 방식으로 객체 번호에 대해 질의한다. 각 해상도에 대해 더 나은 신뢰성을 위해 30회 반복 질의한다.\n' +
      '\n' +
      '우리는 그림 1에 실험 결과를 보고한다. 2(b). 모델 응답은 이미지 해상도에 따라 상당한 위상 변화를 보이는 것을 관찰한다: (1) 1단계에서는 이미지 슬라이스가 없기 때문에 대부분의 답변이 정확하다; (2) 2단계에서는 각 슬라이스의 불완전한 원으로 인해 답변 12가 응답을 지배한다. (3) 단계 3은 9, 12 및 16의 혼합된 답변을 나타낸다. 16은 그림의 오류 패턴에 의해 잘 설명될 수 있음에 유의한다. 1(e). 우리는 각 단계에 대한 더 자세한 설명을 위해 A절을 참조한다. 게다가, 우리는 또한 Fig.에서 많은 비정상적인 현상들을 알아차린다. 2(b)는 아직 완벽하게 설명할 수 없으며, 이는 우리가 향후 작업을 위해 떠난다.\n' +
      '\n' +
      '결론적으로, 이러한 실험 결과는 고해상도 이미지 처리에서 GPT-4V의 잠재적인 취약성을 조명하여 이러한 취약성의 함의에 대한 추가 조사와 LMM에 대한 잠재적인 적대적 공격에 대응하기 위한 전략 개발을 보증한다.\n' +
      '\n' +
      '### LLaVA-1.5 Experiments\n' +
      '\n' +
      '다양한 종횡비를 갖는 이미지들을 다루기 위해, LLaVA-1.5는 입력 이미지들을 시각적 인코더에 공급하기 전에 정사각형들로 패드한다. 이러한 인코딩 방법은 비정방형 이미지에 대한 계산 낭비를 초래한다. 예를 들어, 1:4 이미지는 정사각형으로 패딩한 후 25%의 유효 연산만을 갖는다. 영향을 정량화하기 위해 2D 보간법을 사용하여 입력 이미지의 종횡비에 ViT 위치 임베딩을 피팅함으로써 패딩되지 않은 버전의 LLaVA-1.5를 트레이닝한다. 결과 이미지 토큰은 LLaVA-1.5에서와 같이 576 이하를 유지한다(섹션 3.1 참조). 표 2의 실험 결과로부터, 패딩 없이 적응적 종횡비 인코딩이 LLaVA-1.5의 성능을 일관되게 향상시키는 것을 관찰한다.\n' +
      '\n' +
      '패딩의 또 다른 문제는, 패딩-유사 픽셀들이 이미지 전처리로부터 오는 것인지 또는 원래의 입력 이미지의 실제 부분으로부터 오는 것인지를 모델이 본질적으로 알 수 없다는 것이다. 이 문제를 입증하기 위해 그림 1과 같이 일련의 입력 이미지를 합성한다. 3(오른쪽)에서 다양한 종횡비의 청색/녹색/적색 직사각형은 회색(즉, LLaVA-1.5의 패딩 RGB 값의 색상)으로 둘러싸여 있다. 입력 이미지가 주어졌을 때, 우리는 "좌/우/상/하 대부분의 영역의 색은 무엇인가?"라고 프롬프트한다.\n' +
      '\n' +
      '그림 3: 패딩 픽셀이 포함된 입력 영상을 이용하여 LLaVA-1.5를 적대적으로 공격한 실험 결과. 왼쪽: LLaVA-1.5가 회색 영역을 무시하고 중앙 직사각형(예: 녹색)의 색상에 답하는 공격 성공률. 오른쪽: (1) 다양한 종횡비의 직사각형 및 (2) 패딩 픽셀을 포함하는 입력 이미지를 합성한다.\n' +
      '\n' +
      '그림 2: 이미지 해상도를 지속적으로 변경하여 GPT-4V를 조사한 결과.\n' +
      '\n' +
      '의 결과를 나타낸다. 3(왼쪽)에서 우리는 LLaVA-1.5가 회색 입력 영역(패딩으로 간주)을 무시하고 중심 사각형의 색상으로 충실히 응답하는 것을 관찰한다.\n' +
      '\n' +
      '파일럿 실험에 대한### 결론\n' +
      '\n' +
      '요약하면, GPT-4V 및 오픈 소스 LLaVA-1.5와 같은 강력한 독점 LMM은 기본 시각적 인코딩 전략에 체계적인 문제가 있다. 결과는 시각적 전략이 주의 깊게 설계되어야 함을 보여준다. 패딩, 모양 왜곡 크기 조정 및 반복적인 슬라이싱과 같은 일반적인 관행은 계산의 낭비, 모델 능력의 손실 및 적대적 공격에 대한 취약성을 초래할 수 있다. 따라서, 보다 적응적이고 효율적인 시각적 부호화 방법이 절실히 요구되고 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '파일럿 실험을 통해 학습된 원리를 바탕으로 임의의 종횡비와 고해상도 영상을 효율적으로 인식할 수 있는 대형 멀티모달 모델인 LLaVA-UHD를 제안한다. 도 1에 도시된 바와 같다. 도 4에 도시된 바와 같이, 모델은 세 가지 핵심 컴포넌트들을 포함한다: (1) 효율적이고 확장 가능한 인코딩을 위해 네이티브-해상도 이미지들을 더 작은 가변-크기 슬라이스들로 분할하는 이미지 모듈화 전략, (2) 시각적 인코더들로부터 이미지 토큰들을 추가로 응축하는 압축 모듈, 및 (3) LLM들에 대한 슬라이스 토큰들을 조직하기 위한 공간 장식 스키마.\n' +
      '\n' +
      '### 모듈화된 비주얼 인코딩\n' +
      '\n' +
      '다양한 종횡비를 갖는 고해상도 영상을 다루기 위해, 순진한 접근법은 전체적으로 직접 인코딩을 위해 ViT의 위치 임베딩을 목표 형상에 보간하는 것이다. 그러나 이 방법은 2차 계산 비용과 분산 외 문제로 인한 성능 저하로 인해 차선책이다. 이 문제를 해결하기 위해 모듈화된 시각적 인코딩 전략을 제시한다. 기본 아이디어는 네이티브-해상도 이미지들을 더 작은 가변-크기 슬라이스 슬라이스들로 분할하는 것인데, 여기서 각 슬라이스의 형상은 ViT의 표준 프리트레이닝 설정으로부터 너무 멀리 벗어나지 않는다. 가변 크기의 슬라이스 슬라이스로, LLaVA-UHD는 패딩 또는 형상 왜곡 재성형 없이 네이티브 해상도 이미지에 대한 완전한 적응성을 달성할 수 있다.\n' +
      '\n' +
      '그림 4: LLaVA-UHD 프레임워크. 왼쪽: 고해상도 이미지가 주어지면, LLaVA-UHD는 먼저 이상적인 슬라이스 수를 계산한 다음, 가능한 인수분해로부터 최상의 분할을 선택하여 고해상도 이미지를 다양한 크기의 슬라이스로 분할한다. 오른쪽: 슬라이스들은 위치 임베딩들 상에서 2D 보간법에 의해 네이티브 종횡비들로 인코딩된 후, LLM 프로세싱을 위해 공간 스키마로 압축되고 배열된다.\n' +
      '\n' +
      '**고해상도 이미지 분할 전략**이미지 슬라이싱 전략의 목표는 각 슬라이스의 해상도에 대한 최소한의 변화로 고해상도 이미지의 분할을 결정하는 것이다. 해상도 \\((W_{I},H_{I})\\)의 이미지와 해상도 \\((W_{v},H_{v})\\)의 ViT가 주어지면, 우리는 먼저 이미지를 처리하는 데 필요한 슬라이스 수(즉, 이상적인 계산)를 결정한다: \\(N=\\lceil\\frac{W_{I}\\times H_{I}}{W_{v}\\times H_{v}}\\rceil\\). 그리고 슬라이스 수 \\(N\\)을 \\(m\\) 열과 \\(n\\) 행으로 인수분해한다. \\(\\mathbb{C}_{N}=\\{(m,n)|m\\times n=N,m\\in\\mathbb{N},n\\in\\mathbb{N}\\}\\}\\\\. 가장 적절한 파티션을 선택하기 위해 ViT의 표준 사전 훈련 설정으로부터의 편차를 측정하기 위한 점수 함수를 정의한다:\n' +
      '\n' +
      '[S(W_{I},H_{I},W_{v},H_{v},m,n=-\\left|\\log\\frac{W_{I}\\times n}{H_{I}\\times m}-\\log\\frac{W_{v}}{H_{v}}\\right|,\\tag{1}\\times\n' +
      '\n' +
      '여기서 더 높은 점수 \\(S(\\cdot)\\)는 ViT의 표준 설정으로부터의 더 작은 편차를 나타내고, 따라서 바람직하다. 따라서, 다음과 같이 격벽을 얻을 수 있다:\n' +
      '\n' +
      '\\[m^{*},n^{*}=\\operatorname*{arg\\,max}_{(m,n)\\in\\bar{\\mathbb{C}}}S(W_{I},H_{I},W_{v},H_{v},m,n), \\tag{2}\\}\n' +
      '\n' +
      '여기서 후보집합 \\(\\bar{\\mathbb{C}}=\\mathbb{C}_{\\mathbb{N}}). 실제로, 우리는 경우에 따라 \\(N\\), 특히 소수에 대한 몇 가지 가능한 인수분해 방식이 있을 수 있으며, 이는 제한된 선택으로 이어질 수 있고 따라서 이미지의 극단적인 분할을 초래할 수 있다는 것을 알아챘다. 예를 들어, \\(N=7\\)은 1:7과 7:1의 두 가지 극단적인 분할 선택만을 가지고 있다. 이 문제를 해결하기 위해 이상적인 슬라이스 수 \\(N\\) 외에 슬라이스 수 \\(N-1,N+1\\)의 적당한 변경을 허용하여 더 그럴듯한 분할 선택들을 통합한다. 따라서, 최종 분할은 수학식 2에 의해 주어지며, 여기서 \\(\\bar{\\mathbb{C}=\\mathbb{C}_{N-1}\\cup\\mathbb{C}_{N}\\cup\\mathbb{C}_{N+1}\\cup\\mathbb{C}_{N+1}}이다.\n' +
      '\n' +
      '이론적으로, 분할 전략은 각 슬라이스에 대한 표준 사전 훈련 해상도 \\((W_{v},H_{v})\\)에 대해 약간의 예상 변경과 약간의 최악의 경우 변경을 보장한다는 것을 보여준다. 구체적으로, 입력 영상에 대해 \\(N\\leq 20\\)과 \\([1:6,6:1]\\)의 종횡비, 각 슬라이스의 종횡비가 \\([1:2,2:1]\\) 내에 존재하고, 각 슬라이스의 면적이 \\([0.33W_{I}H_{I}, 1.5W_{I}H_{I}]]\\ 내에 존재함을 보인다. 우리는 완전한 증명 세부 사항에 대해 독자들을 섹션 B에 참조합니다.\n' +
      '\n' +
      '**임의 종횡비 슬라이스 인코딩.** 대부분의 기존 LMM은 이미지 슬라이스 인코딩[5; 27; 11]을 위해 정적 해상도를 이용한다. 이것은 본질적으로 기본 해상도에 대한 완전한 적응성을 방지하는데, 이는 단지 몇 개의 미리 정의된 고정-형상 슬라이스만이 이용가능하기 때문이다. 더욱이, 정적 슬라이스 해상도는 필연적으로 패딩 또는 형상 왜곡 리사이징을 발생시키며, 이는 섹션 2에서 논의된 바와 같이 성능, 효율성 및 심지어 정확성에 손상을 준다.\n' +
      '\n' +
      '이 문제를 해결하기 위해, 우리는 분할 전략에 의해 주어진 종횡비로 이미지 슬라이스를 그대로 인코딩하는 것을 제안한다. 구체적으로, 사전 훈련 예산 \\(M\\) (즉, ViT의 위치 임베딩 수) 내에 패치의 수가 최대가 되도록 종횡비에 따라 원본 이미지의 크기를 비례적으로 조정한다. 그런 다음 ViT의 사전 훈련된 1D 위치 임베딩 시퀀스를 사전 훈련 설정에 따라 2D 형식 \\(P\\in\\mathbb{R}^{q\\times q\\times l}\\)으로 재구성하고, 여기서 \\(M=q\\times q\\) 및 \\(l\\)은 위치 임베딩의 차원이다. 그 후, 시각적 부호화를 위한 분할 전략에 의해 주어진 슬라이스 해상도에 맞도록 2D 보간한다. 실험에서, ViT 및 위치 임베딩 파라미터는 사전 훈련 동안 동결 상태로 유지될 수 있고, 명령어-튜닝 단계 동안 이러한 파라미터를 업데이트하는 것은 양호한 성능을 위해 충분하다는 것을 보여준다. 슬라이스 외에도 네이티브 종횡비의 저해상도 개관 이미지도 제공합니다. 개요 이미지는 이미지에서 거친-그레인된 정보 및 글로벌 시맨틱 연결을 제공할 수 있다.\n' +
      '\n' +
      '### Compression Layer\n' +
      '\n' +
      '고해상도 이미지는 LLM이 훨씬 더 많은 시각적 토큰을 처리해야 하며, 이는 계산의 주요 부분을 차지한다. 예를 들어, \\(672\\times 1008\\) 해상도 이미지는 LLaVA-1.5 [27]에 대해 3,456개의 시각적 토큰을 생성할 것이다. 이 문제를 해결하기 위해, 우리는 공유된 perceiver resampler 레이어를 사용하여 각 이미지 슬라이스의 시각적 토큰을 압축한다[3]. 구체적으로, 시각적 인코더가 출력하는 이미지 토큰은 교차 어텐션(실험에서 \\(576\\)에서 \\(64\\)까지)을 통해 쿼리 벡터 세트를 사용하여 더 낮은 수로 재샘플링된다. 일반적인 MLP 기반 시각적 프로젝션 접근법[27; 26; 39]과 비교하여, 수신기 리샘플러는 이미지 해상도에 관계없이 고정적이고 저렴한 수의 시각적 토큰을 유지하고, 따라서 고해상도 이미지 이해와 더 호환된다. 그 결과, LLaVA-UHD는 336\\(336\\times 336\\) 해상도 영상을 부호화할 때 LLaVA-1.5보다 훨씬 낮은 계산 비용을 사용하여 672\\times 1008\\의 해상도 영상을 부호화할 수 있었다.\n' +
      '\n' +
      '### 이미지 조각을 위한 공간 스키마\n' +
      '\n' +
      '이미지 파티션은 상이한 이미지들에 걸쳐 동적이기 때문에, 이미지 슬라이스들의 공간 조직들을 LLM에 알릴 필요가 있다. [6]에서 영감을 얻은 두 개의 특수 토큰을 사용하여 이미지 슬라이스의 상대적 위치를 알리기 위한 공간 스키마를 설계한다. 구체적으로, 우리는 ""을 사용하여 한 행의 슬라이스 표현을 분리하고, "n"을 사용하여 다른 행을 분리한다. 실험 결과, 간단한 스키마가 동적 파티션을 효과적으로 알려주어 좋은 성능을 얻을 수 있음을 알 수 있었다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 절에서는 LLaVA-UHD의 효과를 실증적으로 조사한다. 먼저 구현 세부 사항을 제공하고, 강력한 기준선과 비교하여 9개의 공통 벤치마크에 대한 평가 결과를 보고한다. 그런 다음 모델을 더 잘 이해하기 위한 분석 결과를 제공한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '**모델 구성** 이 작업에서 우리는 LLaVA-1.5 [27]의 구현에 따라 LLaVA-UHD를 구축했다. 특히, CLIP-ViT-L/14를 Visual Encoder(default resolution \\(336\\times 336\\)), Vicuna-13B[9]를 LLM으로 사용하고, 공유 Visual Resampler[5]를 프로젝터로 사용하여 Visual Encoder와 LLM을 연결하였다. 이미지 슬라이스의 인코딩 동안, 슬라이스를 패치에 맞추기 위해 절반 패치(최대 7-8 픽셀) 내의 작은 재형성이 수행될 수 있다. 리샘플러에서 학습 가능한 질의의 수는 64개로 설정하였으며, LLM에 입력되는 시각적 토큰의 수는 64(N+1)\\(64\\times)\\, 저해상도 개관 이미지의 토큰이다. 최대 6(672\\times 1008\\) 해상도의 영상을 지원하는 실험에서 최대 6(N\\)을 설정하였다. LLaVA-1.5에 이어 다음과 같이 2단계 훈련을 수행한다.\n' +
      '\n' +
      '**단계 1: 프리트레이닝 세부사항.** 이 단계 동안, CC-595K 데이터세트[28]와 함께, 학습률 \\(1e^{-3}\\) 및 코사인 학습률 스케쥴을 갖는 AdamW 최적화기를 사용하여, 1 에폭 동안 퍼시버 리샘플러만이 튜닝된다. 전체 배치 크기는 256으로 설정되었으며, 이 단계의 훈련 비용은 8\\(\\times\\)A100 GPU를 사용하여 \\(\\sim\\)5시간이다.\n' +
      '\n' +
      '**단계 2: Instruction-tuning details.** 이 단계 동안, 시각적 인코더는 동결되고, 우리는 LLaVA-Instruct[28], TextVQA[36], GQA[18], OCR-VQA[32] 및 Visual Genome[19]을 포함하는 656K 혼합물 데이터세트[27]로 시각적 리샘플러 및 LLM을 미세 조정한다. 학습률은 \\(2e^{-5}\\)이고 배치 크기는 128이며, 다른 설정은 1단계와 동일하다. 이 단계의 학습 비용은 8\\(\\times\\)A100 GPU를 사용하여 \\(\\sim\\)18시간이다.\n' +
      '\n' +
      '### Experimental Setting\n' +
      '\n' +
      '벤치마크, 평가 메트릭 및 기준선을 포함한 실험 설정을 소개합니다.\n' +
      '\n' +
      '**Benchmarks.** 우리는 (1) VQA-V2[4], GQA[18], ScienceQA[30], VizWiz[15]와 같은 일반적인 시각적 질문 응답 벤치마크들; (2) TextVQA[36]와 같은 광학 문자 기반 시각적 질문 응답 벤치마크들; (3) POPE[22]와 같은 환영 벤치마크들; (4) MME[14], MMBench[29], MMBench-CN[29]과 같은 포괄적인 벤치마크들을 포함하여 우리의 모델을 평가하기 위해 9개의 인기 벤치마크들을 채택한다.\n' +
      '\n' +
      '**Evaluation Metrics.** 인기 벤치마크에 대한 성능 외에도 최대 지원 해상도로 이미지를 처리하는 데 있어 계산 비용(TFLOP)도 보고한다. 계산 비용은 시각적 인코더, 프로젝터 및 LLM에서 집계된다. 또한 참조를 위한 축적된 멀티모달 학습 데이터 볼륨을 보고하며, 이는 관련 및 명령어 튜닝 동안 사용되는 이미지-텍스트 쌍을 포함한다. 기존 멀티모달 모델에서 백본으로 사후 훈련된 모델의 경우 백본의 훈련 데이터도 포함한다.\n' +
      '\n' +
      '**기준.** 모델을 강력한 기준선과 비교합니다. (1) 일반 기준선. Qwen-VL[5], LLaVA-1.5[27], MiniGPT-v2[7], Shikra[8], BLIP-2[21] 및 InstructBLIP[11]을 대표적인 일반 기준선으로 채택한다. LLaVA-UHD의 구현은 LLaVA-1.5와 고도로 정렬되어 있기 때문에 가장 직접적인 기준선 역할을 한다. (2) 고해상도 LMM. SPHINX[24] 및 mPLUG-Owl2[43]은 고정된 해상도로 이미지를 인코딩하고; Ureader[42] 및 Monkey[23]은 열거된 해상도 유형(몇 개의 미리 정의된 고정된 형상 슬라이스)을 지원하고; Fuyu-8B[6] 및 OtterHD-8B[20]은 임의의 해상도로 이미지를 인코딩할 수 있다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '표 1의 주요 실험 결과를 보고하며, 그 결과 다음과 같은 관찰이 있다. (1) LLaVA-UHD가 인기 벤치마크에서 강력한 기준선보다 우수하다. 여기에는 Qwen-VL 및 InstructBLIP와 같은 2-3배 이상의 데이터에 대해 훈련된 강력한 일반 기준선과 Fuyu-8B, OtterHD-8B, Monkey 및 SPHINX-2k와 같은 훨씬 더 많은 계산을 필요로 하는 고해상도 LMM이 포함된다. 그 결과, LLaVA-UHD는 좋은 데이터 및 계산 효율뿐만 아니라 강한 성능을 위해 네이티브 해상도의 이미지를 적절하게 처리할 수 있음을 보여준다. (2) LLaVA-UHD는 LLaVA-1.5 백본에 비해 상당한 개선을 달성한다. 특히, LLaVA-UHD는 단순히 네이티브 고해상도 영상을 인식함으로써 TextVQA에서 6.4의 정확도 향상과 POPE에서 3.2의 정확도 향상을 달성한다. 그 이유는 저해상도 이미지에서 흐릿한 콘텐츠는 LMM이 어려운 세립 객체 및 광학 문자를 정확하게 식별하는 것을 방지할 수 있기 때문이다. 이 결과는 다양한 멀티모달 작업에서 고유 고해상도 이미지를 인식하는 근본적인 역할과 문제를 해결하는 데 LLaVA-UHD의 효과를 보여준다. (3) 해상도 및 효율 측면에서, LLaVA-1.5 관련 고정 \\(336\\times 336\\) 해상도와 비교하여, LLaVA-UHD는 94% 추론 연산만을 사용하여 임의의 종횡비에서 672\\(\\times\\)1088 해상도 영상을 지원한다. 결과는 향후 잠재적으로 더 큰 해상도에 대한 LLaVA-UHD의 유망한 확장성을 나타낸다.\n' +
      '\n' +
      '### Analytic Results\n' +
      '\n' +
      '우리는 대체 구성 요소에 대한 절제, 더 극단적인 종횡비를 가진 이미지에 대한 평가, 동결/훈련 가능한 매개변수에 대한 모범 사례 및 사례 연구를 포함한 추가 분석 결과를 제공한다.\n' +
      '\n' +
      '**절제 연구.** 표 2에서 대체 성분에 대한 절제 연구를 수행한다. (1) 동일한 최대 해상도를 유지하면서 임의의 종횡비를 지원하는 LLaVA-1.5의 패딩 전략을 LLaVA-UHD의 적응적 인코딩 전략으로 대체한다. 패딩으로 인한 계산 낭비가 방지되므로 일관된 개선을 관찰할 수 있다. (2) LLaVA-UHD의 perceiver resampler를 LLaVA-1.5의 2-layer MLP로 대체하였으며, 12.9%의 계산 비용만을 사용하여 MLP와 비교하거나 우수한 성능을 보였다. (3) 우리는 LLaVA-UHD 이미지 분할 전략을 나이브 분할 전략 [24] (즉, 고정된 \\(2\\times 2\\) 슬라이스)로 추가로 대체한다. 결과는 LLaVA-UHD가 이미지를 슬라이스로 더 적절하게 분할할 수 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline Model & \\#Data & MaxRes. & AR. & TFLOPs & VQA\\({}^{\\text{A}2}\\) & GQA & VQA\\({}^{\\text{T}}\\) & POPE & SQA & VizWiz & MME & MMB\\({}^{\\text{CN}}\\) \\\\ \\hline BLIP-2 [21] & 129M & 224\\(\\times\\)224 & Fix & 1.0 & 41.0 & 41.0 & 42.5 & 85.3 & 61.0 & 19.6 & 1293.8 & - & - \\\\ InstructBLIP [11] & 130M & 224\\(\\times\\)224 & Fix & 1.0 & - & 49.5 & 50.7 & 78.9 & 63.1 & 33.4 & 1212.8 & - & - \\\\ Shikz [8] & 6M & 224\\(\\times\\)224 & Fix & 8.0 & 77.4 & - & - & - & - & - & - & 58.8 & - \\\\ Qwen-VL [5] & 1.48 & 448\\(\\times\\)448 & Fix & 9.2 & 78.8 & 59.3 & 63.8 & - & 67.1 & 35.2 & - & 38.2 & 7.4 \\\\ SPHINX [24] & 1.08 & 448\\(\\times\\)448 & Fix & 39.7 & 78.1 & 62.5 & 51.6 & 80.7 & 69.3 & 39.9 & 1476.1 & 66.9 & 56.2 \\\\ SPHINX-2k [24] & 1.08 & 762\\(\\times\\)762 & Fix & 69.4 & 80.7 & 63.1 & 61.2 & 87.2 & 70.6 & 44.9 & 1470.7 & 65.9 & 57.9 \\\\ MiniGPY-2 [27] & 3264M & 448\\(\\times\\)448 & Fix & 4.3 & - & 60.1 & - & 74.1 & - & 53.6 & - & 53.6 & 10.7 & - \\\\ Fungi-8B [6] & - & 1024\\(\\times\\)1024 & Any & 21.3 & - & - & 86.0 & - & - & 128.6 & 10.7 & - \\\\ QuerHD-8B [20] & 1024\\(\\times\\)1024 & Any & 21.3 & - & - & 86.0 & - & - & 128.4 & 58.3 & - \\\\ In-PLG-QA [43] & 401M & 448\\(\\times\\)448 & Fix & 1.7 & 79.4 & 56.1 & 58.2 & 86.2 & 68.7 & 54.5 & 1450.2 & 64.5 & - \\\\ URteader [24] & 86M & 896\\(\\times\\)1120 & Enum & 26.0 & - & - & 57.6 & - & - & - & - & - & - \\\\ Monkey [23] & 1.08 & 896\\(\\times\\)1344 & Enum & 65.3 & 80.3 & 60.7 & 67.6 & 69.4 & **61.2** & - & - & - \\\\ LLaVA-1.5 [27] & 1.2M & 336\\(\\times\\)336 & Fix & 15.5 & 80.0 & 63.2 & 61.3 & 85.9 & 71.6 & 53.6 & 1531.3 & 67.7 & 63.6 \\\\ \\hline LLaVA-UHD (ours) & 1.2M & 672\\(\\times\\)1008 & Any & 14.6 & **81.7** & **65.2** & **67.7** & **89.1** & **72.0** & 56.1 & **1538.0** & **68.0** & **64.8** \\\\ \\(\\Delta\\) & - & \\(\\times\\)6 times & - & -0.9 & +1.7 & +1.9 & +6.4 & +3.2 & +0.4 & -2.5 & +3.7 & +0.3 & +1.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 9개의 인기 벤치마크에 대한 주요 결과. Data: accumulated multimodal training data volume, MaxRes.: maximum resolution supported, TFLOPs: computation cost of processing maximum resolution images, AR.: aspect ratio supported, \\(\\Delta\\): improvement over LLaVA-1.5 backbone.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Model & \\#TFLOPs & VQA\\({}^{\\text{V}2}\\) & GQA & VQA\\({}^{\\text{T}}\\) & POPE & SQA & VizWiz \\\\ \\hline LLaVA-1.5 & 15.50 & 80.0 & 63.3 & 61.3 & 85.9 & 71.6 & 53.6 \\\\ w/ adaptive enc. & 15.50 & **80.1** & **64.1** & **61.8** & **86.7** & **72.0** & **54.2** \\\\ \\hline LLaVA-UHD & **14.63** & **81.7** & 65.2 & **67.7** & 89.1 & **72.0** & 56.1 \\\\ w/ MLP & 113.65 & 81.6 & **65.4** & 66.9 & **89.2** & 71.5 & **56.3** \\\\ w/ MLP \\& FP. [24] & 80.10 & 81.2 & 64.3 & 66.1 & 89.1 & 71.1 & 54.3 \\\\ w/o spatial schema & 80.07 & 79.3 & 63.9 & 65.4 & 88.3 & 70.3 & 53.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 절제 결과. FP: 고정 이미지 분할 전략.\n' +
      '\n' +
      '더 나은 성능을 위해. (4) LLaVA-UHD에서 공간 스키마를 제거한다. 성능저하는 LMM의 동적 슬라이스 위치를 알려주는 공간 스키마의 효율성과 필요성을 보여준다.\n' +
      '\n' +
      '**LLaVA-UHD는 극단적인 종횡비를 갖는 이미지에 일반화된다.** 기존 벤치마크의 확장된 버전을 구성하여 LLaVA-UHD의 일반화 능력을 추가로 조사한다. 구체적으로 패딩을 통해 더 긴 변의 길이를 두 배로 늘려 영상의 종횡비를 확장한다. 표 3의 결과로부터 LLaVA-1.5와 대안들에 비해 LLaVA-UHD의 장점이 증가함을 알 수 있다. 그 이유는 LLaVA-UHD가 이미지를 고유 종횡비로 인식하기 때문이다. 이에 비해, 고정된 종횡비로 이미지를 인코딩하는 LMM은 콘텐츠 형상에서 상당한 왜곡을 겪을 것이다. 더욱이, 이것은 또한 계산이 이미지 콘텐츠의 폭 및 높이를 따라 불균일하게 분포되게 한다.\n' +
      '\n' +
      '**인스트럭션-튜닝 ViT 파라미터들은 적응에 충분하다.** 프리트레이닝 및 명령어-튜닝을 포함하는 상이한 트레이닝 스테이지들에서 ViT 파라미터들을 튜닝하는 효과를 조사한다. 표 4의 결과로부터, 우리는 (1) 명령어-튜닝 동안 ViT 업데이트가 양호한 성능을 달성하기에 충분하다는 것을 관찰한다. 실제로, 우리는 프리트레이닝과 명령어 튜닝 모두에서 ViT 파라미터가 동결된 경우에도 LLaVA-UHD가 LLaVA-1.5보다 향상될 수 있음을 발견했다. (2) 프리트레이닝 동안 ViT를 추가로 업데이트하는 것은 더 나은 결과로 이어지지 않는다. 우리는 제한된 사전 훈련 데이터에 대해 ViT와 리샘플러를 (처음부터) 공동으로 훈련하면 불안정 문제를 유발할 수 있기 때문이라고 가정한다.\n' +
      '\n' +
      '**Case Study.** 고해상도 이미지를 다룰 때 LMM의 능력에 대한 보다 직관적인 이해를 제공하기 위해 그림 5의 LLaVA-UHD와 LLaVA-1.5에 대한 정성적인 결과를 제공한다. LLaVA-UHD는 시간표(Case 1), 소형 포스터의 텍스트(Case 2), 전화의 아이콘과 텍스트(Case 3)에서 세밀한 인식 및 추론을 위해 조밀한 내용을 정확하게 식별할 수 있음을 알 수 있다. 이에 비해 LLaVA-1.5는 거친 정보만 인식할 수 있으므로 이러한 어려운 시나리오에서 정보가 없는 경우(사례 1 및 사례 2) 또는 오답/환각된 답변(사례 3)을 제공하는 경향이 있다. 그 결과, LLaVA-UHD가 세립형 멀티모달 성능에 대한 고유 종횡비 및 고해상도 이미지 인식에서 효과 및 이점을 입증한다.\n' +
      '\n' +
      '## 5 관련 업무\n' +
      '\n' +
      '**Visual Encoding in LMMs.** ChatGPT[1] 및 GPT-4[2]의 출현은 수많은 오픈 소스 대형 언어 모델(LLM)의 개발에 박차를 가했다[9; 38; 10]. LLM을 언어 인코더 및 디코더로 사용하면 시각적 이미지를 이해하는 것을 목표로 많은 LMM[21; 11; 3; 28; 5; 17]이 나타난다. 따라서 비전 기능을 LLM으로 인코딩하는 방법은 커뮤니티의 핵심 문제가 된다. 다행히 CLIP[34]는 BERT[12]와 같은 언어 모델과 ViT[13] 및 CNN[16]과 같은 비전 모델을 사용하여 시각적 특징을 사용하여 언어 임베딩을 각각 추출할 것을 제안하고,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c c} \\hline \\hline \\multicolumn{2}{c|}{Update ViT} & \\multicolumn{1}{c}{VQA\\({}^{\\text{v2}}\\)} & \\multicolumn{1}{c}{GQA} & \\multicolumn{1}{c}{VQA\\({}^{\\text{T}}\\)} & \\multicolumn{1}{c}{POPE} & \\multicolumn{1}{c}{SQA} & \\multicolumn{1}{c}{VizWiz} \\\\ \\hline pre-training & Fine-tuning & & & & & & \\\\ \\hline  & ✓ & **81.7** & **65.2** & **67.7** & **89.1** & **72.0** & **56.1** \\\\ ✓ & & 78.2 & 61.1 & 58.9 & 83.9 & 68.6 & 51.4 \\\\  & & 79.4 & 64.5 & 65.7 & 87.3 & 71.9 & 55.4 \\\\ ✓ & ✓ & 80.2 & 63.7 & 62.6 & 87.2 & 71.6 & 55.1 \\\\ \\hline \\multicolumn{2}{c|}{LLaVA-1.5 [27]} & 80.0 & 63.3 & 61.3 & 85.9 & 71.6 & 53.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 상이한 트레이닝 스테이지들에서 시각적 인코더를 튜닝하는 것의 효과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Model & \\#TFLOPs & VQA\\({}^{\\text{v2}}\\) & GQA & VQA\\({}^{\\text{T}}\\) & POPE & SQA & VizWiz \\\\ \\hline LLaVA-1.5 & 15.50 & 74.6 (-5.4) & 57.9 (-5.4) & 58.4 (-3.9) & 81.1 (-4.8) & 66.3 (-5.3) & 50.1 (-3.5) \\\\ w/ adaptive enc. & 15.50 & **74.9** (-5.2) & **62.5** (-1.6) & **60.7** (-1.1) & **82.3** (-4.4) & **66.9** (-5.1) & **50.9** (-3.3) \\\\ \\hline LLaVA-UHD & 14.63 & **81.4** (-0.3) & 61.8 (-3.4) & **64.5** (-3.2) & 85.1 (-4.0) & **71.5** (-0.5) & 54.0 (-2.1) \\\\ w/ MLP & 113.65 & 81.3 (-0.3) & **62.0** (-3.4) & 63.9 (-3.0) & **85.2** (-4.0) & 70.9 (-0.6) & **54.3** (-2.0) \\\\ w/ MLP \\& FP. [24] & 80.10 & 79.6 (-1.6) & 61.9 (-2.4) & 58.5 (-7.6) & 84.4 (-4.7) & 69.4 (-1.7) & 52.2 (-2.1) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 극단적 종횡비 영상에 대한 실험 결과. 표 2의 표준 벤치마크와 비교한 절대 성능 및 저하가 보고된다.\n' +
      '\n' +
      '상당한 이미지-텍스트 쌍[35]을 사용하여 대조적인 학습 방식으로 정렬하여 시각적 임베딩이 언어를 향해 잘 정렬되도록 한다.\n' +
      '\n' +
      'LLM에 대한 기존의 시각적 투영 접근법은 세 가지 범주로 나눌 수 있다. (1) 플라밍고[3]은 교차 주의 동작에 의해 시각적 특징을 캡처하기 위해 고정된 수의 쿼리를 사용하여 이미지/비디오 이해를 위해 LLM에 공급하는 수신기 리샘플러를 제안한다. (2) BLIP-2[21]는 이미지 인코더 및 LLMs를 브릿지하기 위해 Q-전이를 프리트레인한다. (3) LLaVA [28]은 언어와 비전 특징 공간을 연결하기 위해 MLP 모듈을 이용할 뿐이다. 이 외에도 SPHINX[24]는 DINO-V2[33], CLIP-ViT[34] 및 CLIP-CNN[34], Q-Former 등 많은 종류의 시각적 특징을 혼합하여 시각적 표현을 증강한다. 가변 [40]은 문서/차트 인식 및 이해에 맞춘 시각적 모델을 미리 훈련하고 추가 기능 향상을 위해 LLaVA [28]의 시각적 특징과 통합한다.\n' +
      '\n' +
      '그러나, 이러한 LMM들은 고정된 해상도 이미지를 입력으로 요구하는 CLIP-ViT에 의존하기 때문에, LMM들이 더 높은 해상도 또는 임의의 종횡비를 갖는 이미지들을 핸들링하는 것을 방해하고, 광학 문자 인식 또는 작은 객체 이해와 같은 세밀한 다운스트림 태스크들을 약화시킨다.\n' +
      '\n' +
      '** 고해상도 LMM.** 더 높은 해상도로 이미지를 인식하려면 최근 작업을 네 가지 범주로 나눌 수 있습니다. (1) Up-Resize. Qwen-VL[5]는 ViT의 위치 임베딩을 224\\(\\times\\)224에서 448\\(\\times\\)448로 보간하고 ViT를 미세 조정하기 위한 훈련 단계를 추가로 실행한다. CogAgent[17] 및 LLaVA-HR[31]은 작은 고해상도 이미지를 갖는 큰 저해상도 인코더와 결혼한다. MiniGPT-v2[7]은 시각 인코더를 미세 조정하지 않고 위치 임베딩만 조정한다.\n' +
      '\n' +
      '그림 5: 세립 인식 및 추론 능력에서 LLaVA-UHD와 LLaVA-1.5의 질적 비교.\n' +
      '\n' +
      '명령어 튜닝. 이러한 방법들은 CLIP-ViT[34]의 원래의 시각적 위치 인코딩을 극적으로 변화시키며, 이는 차선의 시각적 표현을 야기할 수 있다. (2) Fix+Crop. 상기 문제를 해결하기 위해 SPHINX[24]는 고정된 윈도우 크기(224\\(\\times\\)224)를 사용하여 패딩된 이미지(448\\(\\times\\)448)를 4개의 슬라이스로 자르고, 이를 다운 샘플링된 224\\(\\times\\)224 이미지와 연결한다. 원숭이[23]은 이러한 개념을 따르면서도 접근 가능한 이미지 크기를 896\\(\\times\\)1344로 증가시키고, 공유된 리샘플러를 사용하여 각 슬라이스를 변환한다. (3) fix+Enumerated-Crop. UReader[42], LLaVA-1.6[26] 및 infiMM-HD[25]는 SPHINX[24]에서와 같이 고정된 정사각형 비율(예를 들어, 2\\(\\times\\)2)을 사용하는 것이 아니라 크기 조정에 유사한 종횡비를 열거한다. 피할 수 없는 이미지 크기 조정 및 패딩 작업은 각각 이미지 변형과 계산 낭비를 초래할 수 있다. (4) 임의의 것이다. Fuyu-8B[6] 및 Otter-HD[20]는 LLM을 직접 활용하여 비전 트랜스포머 대신 시각적 특징을 인코딩한다. 그들은 이미지를 패치로 분할하고 LLM에 공급하기 전에 선형 레이어를 사용하여 투영한다. 이미지 패치와 관련하여 시퀀스로서 이미지 패치는 연속적인 해상도로 이미지를 처리할 수 있게 한다. 그러나, 이미지 인코더의 제거는 불충분한 시각적 표현을 의미하며, 이는 이러한 방법들을 만족스럽지 못한 성능에서 제한한다.\n' +
      '\n' +
      '이에 비해, LLaVA-UHD는 임의의 종횡비 및 고해상도의 이미지를 지원한다. LLaVA-UHD는 모듈화 및 적응 이미지 인코딩의 장점뿐만 아니라 수신기 리샘플러를 통합함으로써 향상된 계산 효율로 강력한 성능을 달성할 수 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '본 논문에서는 임의의 종횡비와 고해상도 영상을 효율적으로 인지하는 대형 멀티모달 모델인 LLaVA-UHD를 제안한다. 9개의 인기 벤치마크에 대한 포괄적인 실험 결과는 특히 세립 다중 모드 기능에서 LLaVA-UHD의 효과를 보여준다. 해석적 평가 결과는 모델에 대한 더 깊은 이해를 위해 제공된다. 본 연구에서는 LLaVA-UHD의 해상도를 최대(672\\times 1008\\)로 제한하였다. 향후, 유망한 효율성과 확장성을 고려하여 고해상도 이미지와 작은 객체 검출 및 분할과 같은 더 어려운 작업을 탐색할 것이다. 또한, 이미지 슬라이스는 현재 LLM에서만 상호 작용이 있는 독립적으로 인코딩된다. 우리는 세밀한 글로벌 정보 상호작용을 위한 개선된 시각적 인코딩 전략을 통해 이미지 슬라이스 간의 효율적인 연결을 구축할 계획이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Introducing ChatGPT. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt). 2022.\n' +
      '* [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In _NeurIPS_, 2022.\n' +
      '* [4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In _IEEE ICCV_, pages 2425-2433, 2015.\n' +
      '* [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [6] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani,, and Sagnak Tasrrlar. Introducing our multimodal models. adept.ai/blog/fyu-8b. 2023.\n' +
      '* [7] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.\n' +
      '* [8] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.\n' +
      '\n' +
      '* [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. 2023.\n' +
      '* [10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. _arXiv preprint arXiv:2305.06500_, 2023.\n' +
      '* [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _NAACL_, pages 4171-4186, 2019.\n' +
      '* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.\n' +
      '* [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. MME: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.\n' +
      '* [15] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. VizWiz grand challenge: Answering visual questions from blind people. In _IEEE CVPR_, pages 3608-3617, 2018.\n' +
      '* [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE CVPR_, pages 770-778, 2016.\n' +
      '* [17] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. CogAgent: A visual language model for gui agents. _arXiv preprint arXiv:2312.08914_, 2023.\n' +
      '* [18] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _IEEE CVPR_, pages 6700-6709, 2019.\n' +
      '* [19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 123:32-73, 2017.\n' +
      '* [20] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. OtterHD: A high-resolution multi-modality model. _arXiv preprint arXiv:2311.04219_, 2023.\n' +
      '* [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _ICML_, 2023.\n' +
      '* [22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. _arXiv preprint arXiv:2305.10355_, 2023.\n' +
      '* [23] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _arXiv preprint arXiv:2311.06607_, 2023.\n' +
      '* [24] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. SPHINX: the joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. _arXiv preprint arXiv:2311.07575_, 2023.\n' +
      '* [25] Haogeng Liu, Quanzeng You, Xiaotian Han, Yiqi Wang, Bohan Zhai, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, and Hongxia Yang. InfMM-HD: A leap forward in high-resolution multimodal understanding. _arXiv preprint arXiv:2403.01487_, 2024.\n' +
      '\n' +
      '* [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge. [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/). 2024.\n' +
      '* [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _NeurIPS_, 36, 2024.\n' +
      '* [29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [30] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. _Advances in Neural Information Processing Systems_, 35:2507-2521, 2022.\n' +
      '* [31] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiauw Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. _arXiv preprint arXiv:2403.03003_, 2024.\n' +
      '* [32] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual question answering by reading text in images. In _IEEE ICDAR_, pages 947-952, 2019.\n' +
      '* [33] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOV2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.\n' +
      '* [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. _NeurIPS_, pages 25278-25294, 2022.\n' +
      '* [36] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In _IEEE CVPR_, pages 8317-8326, 2019.\n' +
      '* [37] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented RLHF. _arXiv preprint arXiv:2309.14525_, 2023.\n' +
      '* [38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* [39] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. CogVLM: Visual expert for large language models. 2023.\n' +
      '* [40] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. _arXiv preprint arXiv:2312.06109_, 2023.\n' +
      '* [41] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of LMMs: Preliminary explorations with gpt-4v (ision). _arXiv preprint arXiv:2309.17421_, 9(1): 1, 2023.\n' +
      '* [42] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. UReader: Universal OCR-free visually-situated language understanding with multimodal large language model. _arXiv preprint arXiv:2310.05126_, 2023.\n' +
      '* [43] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _arXiv preprint arXiv:2311.04257_, 2023.\n' +
      '* [44] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwan He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. RLHF-V: Towards trustworthy MLLMs via behavior alignment from fine-grained correctional human feedback. In _CVPR_, 2024.\n' +
      '\n' +
      '## 부록 GPT-4V 화상에 대한 상세 삽화\n' +
      '\n' +
      '그림의 파일럿 실험 결과로부터. 도 6을 참조하면, GPT-4V 응답은 이미지 해상도에 따라 상당한 위상 변화를 보인다는 것을 알 수 있다. 여기에서 우리는 시각적 인코딩의 관점에서 가설된 원인에 대한 자세한 그림을 제공한다.\n' +
      '\n' +
      '(1) 1단계에서, 이미지 슬라이스가 하나밖에 없기 때문에, 대부분의 답변이 정확하다. 보다 구체적으로, 512 해상도 하에서 입력 이미지들을 다룰 때, 이미지들이 512로 리사이징되면, 동작은 1 단계 내에서 동일할 것이다. 그러나, 동작은 1 단계 내에서 상당히 변화하기 때문에, 우리는 입력 이미지들이 그림과 같이 512 해상도로 패딩될 가능성이 가장 높다고 의심한다. 7(a)\n' +
      '\n' +
      '(2) 2단계에서 답변 12는 그림 1과 같이 각 슬라이스의 불완전한 원으로 인해 응답을 지배한다. 7(b).\n' +
      '\n' +
      '(3) Phase 3은 9, 12, 16의 혼합 답변을 나타낸다. 이 응답들 중, 답변 16은 Fig.의 슬라이스 전략에 의해 잘 설명될 수 있다. 7(c). 게다가, 우리는 또한 Fig.에서 많은 비정상적인 현상들을 알아차린다. 2(b)는 아직 완벽하게 설명할 수 없으며, 이는 우리가 향후 작업을 위해 떠난다.\n' +
      '\n' +
      '## 부록 B 증명\n' +
      '\n' +
      '이 절에서는 이미지 분할 전략에 대한 증명을 제공합니다. 우리는 슬라이스 해상도가 ViT의 원래 해상도에 약간의 변화를 보인다는 것을 보여준다.\n' +
      '\n' +
      '**슬라이스 종횡비의 범위.**슬라이스의 종횡비는 다음과 같이 나타낼 수 있다:\n' +
      '\n' +
      '\\[\\frac{W_{v}}{H_{v}}=\\frac{W_{I}}{m}:\\frac{H_{I}}{n},\\]\n' +
      '\n' +
      '여기서 \\(W_{v}\\), \\(H_{v}\\)은 슬라이스의 폭과 높이, \\(W_{I}\\), \\(H_{I}\\)은 원본 이미지의 크기, (m,n)은 가장 좋은 파티션이다. 종횡비 \\(r=\\frac{W_{v}}{H_{v}}\\in[\\frac{1}{2},2]\\)를 제한하는 것은 \\(|\\log(\\mathsf{r})|\\leq|\\log 2|\\)와 같다.\n' +
      '\n' +
      '그림 6: 연속적으로 변화하는 이미지 해상도를 통해 GPT-4V를 조사한 결과.\n' +
      '\n' +
      '도 7: GPT-4V 단계(가설화)에 대한 일러스트레이션. 빨간 정사각형은 슬라이스를 나타냅니다.\n' +
      '\n' +
      '또한 \\(\\left|\\log\\left(\\frac{W_{I}}{H_{I}}}\\right)-\\log(\\frac{n}{m})\\right|\\leq|\\log(2)|\\)와 동등하다. 증명해야 해\n' +
      '\n' +
      '\\[\\forall\\frac{W_{I}}{H_{I}}\\in[\\frac{1}{6},6],N\\leq 20\\]\n' +
      '\n' +
      '\\[\\exists(\\mathsf{m},\\mathsf{n})\\in\\bar{\\mathbb{C}},\\left|\\log\\left(\\frac{W_{I}} {H_{I}}\\right)-\\log(\\frac{n}{m})\\right|\\leq|\\log(2)|,\\]\n' +
      '\n' +
      '와 동등하다.\n' +
      '\n' +
      '\\[\\forall N\\leq 20,(n_{i},m_{i})\\in\\bar{\\mathbb{C}\\]\n' +
      '\n' +
      '[\\exists(n_{j},m_{j})\\in\\bar{\\mathbb}},\\left|\\left(\\log\\left(\\frac{n_{i}}{m_{i}}\\right)-\\log\\left(\\frac{n_{j}}{m_{j}}\\right)\\right|\\leq 2\\cdot|\\log(2)|\\\\,\\\\\n' +
      '\n' +
      '이것은 \\(\\bar{\\mathbb{C}=\\mathbb{C}_{N-1}\\cup\\mathbb{C}_{N}\\cup\\mathbb{C}_{N+1}\\(N\\leq 20\\)에 대한 \\(\\bar{\\mathbb}=\\mathbb{C}_{N-1}}=\\mathbb{C}_{N-1}})의 모든 가능한 인수분해를 열거함으로써 검증될 수 있다. 그 결과 각 슬라이스의 종횡비는 \\([\\frac{1}{2},2]\\) 내에 존재함을 알 수 있었다.\n' +
      '\n' +
      '** 기대 종횡비.** 원본 이미지의 비율이 1보다 크다고 가정한다(즉, \\(H_{I}>W_{I}\\)). 상황은 \\(H_{I}<W_{I}\\)에 대해서도 동일하다. 원영상의 종횡비는 1,6]\\(P(W_{I},W_{H},n,m)=\\frac{1}{20}\\cdot\\frac{1}{5}\\(W_{I},W_{H},n,m)=\\frac{1}{20}\\cdot\\frac{1}{5}\\(W_{I},W_{H},n,m)=\\frac{1}{H}{I}\\in[1,6]\\(W_{I},W_{H},n,m)=\\frac{1}{20}\\cdot\\frac{1}{5}\\(W_{I},W_{H},n,m)=\\frac{1}{H}{I}\\in[1,6]\\(W_{I},W_{H},n,m)=\\frac{1}{1}{5}\\(W_{I},W_{H},n,m)=\\frac{1}{ 예상되는 종횡비는 다음과 같이 구할 수 있다:\n' +
      '\n' +
      'f{E}(\\frac{m\\times W_{I}}{n\\times H_{I}}}}{iint\\begin{array}{c}\\frac{W_{I}}{H_{I}}\\in[1,6]\\\\cdot H_{I}\\cdot H_{I}\\in[0,20s]\\end{array}(\\frac{m\\times W_{I}}{n\\times H_{I}}}\\cdot P(W_{I},H_{I},n,m)\\,dW_{I}dH_{I},\\\n' +
      '\n' +
      '여기서 \\(s\\)은 ViT의 표준 해상도의 영역이다. 계산 후 \\(\\mathsf{E}(r)=1.258\\), \\(\\text{Var}(r)=0.048\\을 얻는다. 결과는 슬라이스의 예상 종횡비가 1:1.258로 ViT의 설정과 관련된 표준에 가깝다는 것을 보여준다. 영상은 \\([1,3]\\) 사이에 균일하게 분포하며, 종횡비는 \\([1,2]\\) 사이에 균일하게 분포한다고 가정하면 \\(\\mathsf{E}(r)=1.147\\), \\(\\text{Var}(r)=0.011\\)으로 더 작은 변화를 나타낸다.\n' +
      '\n' +
      'Slice Area.** Let\\(n=\\frac{W_{I}}{W_{text{s}}}\\times\\frac{H_{I}}{H_{text{s}}\\)으로 이어지며, 이는 \\(N=\\lceil n\\rceil\\)으로 이어진다. 우리는 이미지를 \\(\\{N-1,N,N+1\\}\\) 슬라이스로 나누는 것을 고려한다. 따라서, 각 슬라이스의 최대값은 \\(\\text{S}_{\\text{max}}=\\frac{n}{N-1}\\)( \\(N\\neq 2\\)), \\(\\text{S}_{\\text{max}=\\frac{n}{N}\\)( \\(N=2\\))이다. 최소값\\(\\text{S}_{\\text{min}}=\\frac{n}{N+1}\\). \\(n\\)은 \\(3^{-}\\)에 접근하는데, 여기서 \\(N=3\\), \\(\\text{S}_{\\text{max}\\)는 \\(1.5\\)의 최대값을 달성한다. 유사하게, \\(n\\)이 \\(1^{+}\\)에 접근함에 따라, \\(N=2\\), \\(\\text{S}_{\\text{min}\\)은 \\(0.33\\)의 최소값을 달성한다.\n' +
      '\n' +
      '예상 슬라이스 면적.** 이미지의 크기가 \\(N\\in[0,20]\\) 내에 균일하게 분포한다고 가정하면서도 이미지의 종횡비는 \\(\\frac{W_{I}}{H_{I}}\\in[\\frac{1}{6},6]\\)이다. 슬라이스의 예상 영역은 다음에 의해 획득될 수 있다:\n' +
      '\n' +
      '{ccc}\\mathsf{E}(\\frac{W_{I}\\times H_{I}}{n\\times m})=\\iint\\begin{array}{c}\\frac{W_{I}}{H_{I}}\\in[1,6]\\cdot H_{I}\\cdot H_{I}\\in[0,20s]\\end{array}(\\frac{W_{I}\\times H_{I}}{n\\times m}}}\\cdot P(W_{I},H_{I},n,m)dW_{I}dH_{I}.\\n,m=\\arg\\max S(\\cdot\\end{array}\\c}\\frac{W_{I}}\\c}\\frac{W_{I}}\\in[1,6]\\cdot H_{I}\\in[0,20s]\\end{array}(\\frac{W_{I}\\times H_{I\n' +
      '\n' +
      '계산 후 \\(\\fracsf{E}(\\frac{W_{I}\\times H_{I}}{n\\times m})=1.057\\), \\(\\text{Var}(\\fracsf{W_{I}\\times H_{I}}{n\\times m})=0.016\\). 이것은 우리의 슬라이스 영역이 ViT의 원래 해상도와 유사하게 상대적으로 집중되어 있음을 보여준다.\n' +
      '\n' +
      '## 부록 C 토론\n' +
      '\n' +
      '이 작업의 한계와 잠재적인 부정적인 영향에 대한 논의를 제공합니다.\n' +
      '\n' +
      '**제한 및 미래 작업**(1) 더 높은 해상도 본 연구에서는 LLaVA-UHD의 해상도를 최대(672\\times 1008\\)로 제한하였다. 이 해상도는 표준 LLaVA-1.5 해상도를 6배 증가시키지만, 4K 이미지 및 원격 감지 이미지와 같은 고해상도 이미지는 여전히 손이 닿지 않는다. 향후, 유망한 효율성과 확장성을 고려하여 고해상도 이미지와 작은 객체 검출 및 분할과 같은 더 어려운 작업을 탐색할 것이다. (2) 조인트 슬라이스 인코딩. 현재 이미지 슬라이스는 현재 LLM에서만 상호 작용이 있는 독립적으로 인코딩된다. 우리는 세밀한 글로벌 정보 상호작용을 위한 개선된 시각적 인코딩 전략을 통해 이미지 슬라이스 간의 효율적인 연결을 구축할 계획이다.\n' +
      '\n' +
      '** potential Negative Impact.** 이 연구에서는 GPT-4V와 LLaVA-1.5에 대한 실패 패턴과 근본적인 원인을 조사한다. 이 메커니즘은 잠재적으로 이러한 모델에 대한 적대적 공격에 사용될 수 있다. 이 작업의 목표는 LMM의 취약성에 대한 관심을 높이고 시각적 인코딩 전략의 중요성에 대한 더 깊은 이해를 제공하는 것이다. 이 작업은 LMM의 견고성과 안전성을 보장하기 위해 밝혀진 문제를 완화하기 위한 추가 노력을 요구한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>