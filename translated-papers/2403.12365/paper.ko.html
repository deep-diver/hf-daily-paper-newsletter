<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '정량적, 정성적 평가를 통해 4D 생성과 4D 신규 뷰 합성의 두 가지 과제에서 우리의 방법이 최첨단 결과를 달성한다는 것을 보여준다.\n' +
      '\n' +
      '프로젝트 페이지: [https://zerg-overmind.github.io/GaussianFlow.github.io/](https://zerg-overmind.github.io/GaussianFlow.github.io/]\n' +
      '\n' +
      '키워드:4D 생성 4D 새로운 뷰 합성 3D 가우시안 스플래팅 동적 장면 광학 흐름.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '단안 또는 다시점 동영상으로부터 4D 동적 콘텐츠 생성은 가상현실/증강현실, 디지털 게임, 영화 산업 등에 폭넓게 적용 가능하여 학계 및 산업계의 큰 관심을 받고 있다. 연구 [19, 36, 37, 39]는 4D 동적 신경 복사 필드(NeRF)에 의해 4D 장면을 모델링하고 입력 멀티뷰 또는 단안 비디오를 기반으로 최적화한다. 일단 최적화되면, 4D 필드는 볼륨 렌더링을 통해 선호되는 시간 단계들에서 신규 카메라 포즈들로부터 볼 수 있다. 보다 도전적인 과제는 텍스트-투-비디오 또는 이미지-투-비디오 모델들에 의해 생성된 교정되지 않은 단안 비디오들 또는 합성 비디오들에 기초하여 360도 4D 콘텐츠를 생성하는 것이다. 단안 입력은 충분한 멀티뷰 신호를 제공할 수 없고 교합으로 인해 관찰되지 않은 영역은 감독되지 않기 때문에 연구 [15, 48, 70]은 생성 모델을 활용하여 그럴듯하고 시간적으로 일관된 3D 구조와 외관을 생성함으로써 4D 동적 NeRF를 최적화한다. 4D NeRF의 최적화는 볼륨 렌더링을 필요로 하므로 공정 시간이 많이 소요된다. 또한, 최적화된 4D NeRF의 실시간 렌더링은 특별한 설계 없이 거의 이루어지지 않는다. 보다 효율적인 대안은 4D 가우스 스플래팅(GS)[30, 61]에 의해 4D 복사 필드를 모델링하는 것이며, 이는 시간 차원으로 3D 가우스 스플래팅[18]을 확장한다. 3D GS의 효율적인 렌더링을 활용하면 4D Radiance Field의 긴 훈련 시간을 획기적으로 줄일 수 있으며, 추론 시 실시간 속도를 얻을 수 있다[42, 67].\n' +
      '\n' +
      '4D 가우시안 필드의 최적화는 주요 감독으로서 측광 손실을 취한다. 그 결과, 장면 역학은 일반적으로 구속력이 낮다. 4D NeRFs[36, 21, 39]와 유사하게, 가우시안들의 복사 특성들 및 시변 공간 특성들(위치, 스케일, 및 배향들)은 렌더링된 프레임들과 입력 비디오 프레임들 사이의 측광 평균 제곱 오차(MSE)를 감소시키도록 둘 다 최적화된다. 외관, 기하학 및 역학에 대한 모호성은 프로세스에서 도입되었으며 희소 뷰 또는 단안 비디오 입력으로 두드러진다. 프레임별 스코어 증류 샘플링(SDS; Per-frame Score Distillation Sampling) [53]은 잠재 도메인에서 멀티뷰 감독을 수반함으로써 외관-형상 모호성을 어느 정도 감소시킨다. 그러나 단안 측광 감독과 SDS 감독 모두 장면 역학을 직접 감독하지는 않는다.\n' +
      '\n' +
      '빠른 움직임으로 인한 시간적 불일치를 피하기 위해, 일관성 4D[15]는 보간된 프레임과 생성된 프레임 사이에 측광 일관성을 부여하는 비디오 보간 블록을 이용하는데, 이는 피팅하기 위해 더 많은 프레임을 의사 그라운드 진리로 포함시키는 비용을 들인다. 유사하게, AYG[23]은 미리 설정된 프레임 레이트와 모션 크기 및 시간적 일관성의 균형을 맞추기 위해 텍스트-비디오 확산 모델을 사용한다. 4D NeRF 모델 [21]은 참조 비디오들 상의 광학 흐름들이 강한 모션 큐들이고 장면 역학에 상당히 유익할 수 있다는 것을 증명하였다. 그러나, 4D GS의 경우, 4D 가우시안 모션들을 광학 흐름들과 연결하는 것은 다음의 두 가지 과제들을 갖는다. 첫째, 가우시안 모션은 3차원 공간이지만 렌더링된 픽셀에 기여하는 2차원 스플랫이다. 둘째, 다수의 3D 가우시안들은 렌더링에서 동일한 픽셀에 기여할 수 있으며, 각 픽셀의 흐름은 어느 하나의 가우시안 모션과 동일하지 않다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 연속 프레임 사이의 3D 가우시안 역학 및 픽셀 속도를 연결하는 새로운 개념인 가우시안 흐름을 소개한다. 특히, 영상공간에서 각 픽셀의 광학적 흐름은 가우시안(Gaussians)에 의해 영향을 받는다고 가정한다. 각 픽셀의 가우시안 흐름은 2D에서 이러한 가우시안 움직임의 가중 합으로 간주된다. 가우시안 스플래팅의 속도 이점을 잃지 않고 각 픽셀에서 가우시안 흐름 값을 얻기 위해 3차원 공간에서의 스케일링, 회전 및 병진을 포함한 3차원 가우시안 동역학을 복사 특성과 함께 이미지 평면에 스플래팅한다. 전체 프로세스가 종단간 미분가능하기 때문에, 3D 가우시안 역학은 입력 비디오 프레임들 상의 광학 흐름과 가우시안 흐름을 매칭시킴으로써 직접 감독될 수 있다. 이러한 흐름 감독은 4D 콘텐츠 생성과 4D 새로운 뷰 합성에 모두 적용되며, 특히 기존 방법으로 다루기 어려운 움직임이 풍부한 콘텐츠에 대해 제안된 방법의 이점을 보여준다. 흐름 유도 과시안 역학은 또한 4D 생성에서 일반적으로 관찰되는 색상 표류 인공물을 해결한다. 우리의 공헌을 다음과 같이 요약한다.\n' +
      '\n' +
      '* 우리는 3D 가우시안 역학을 결과 픽셀 속도에 처음으로 연결하는 새로운 개념인 가우시안 흐름을 소개한다. 가우시안 흐름을 광학 흐름과 일치시키면 3D 가우시안 역학이 직접 감독될 수 있다.\n' +
      '* 가우시안 흐름은 가우시안 다이내믹스를 영상 공간에 스플래팅하여 얻을 수 있다. 기존의 3차원 가우시안 스플래팅에 의한 타일 기반 설계에 이어, 최소 오버헤드로 CUDA에서 다이내믹 스플래팅을 구현한다. 3차원 가우시안 동역학으로부터 조밀한 가우시안 흐름을 생성하기 위한 연산은 매우 효율적이고 종단간 미분 가능하다.\n' +
      '* 가우시안 흐름에서 광학 흐름 매칭으로, 우리의 모델은 특히 빠른 움직임의 장면 시퀀스에 대해 기존의 방법들에 비해 획기적으로 개선된다. 색상 표류는 또한 개선된 가우시안 역학으로 해결된다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '**3D Generation.**3D 세대는 다양한 2D 또는 3D 인식 확산 모델[26, 27, 43, 47]과 대형 비전 모델[16, 35, 40]의 진보로 엄청난 관심을 끌었다. 대규모 멀티뷰 이미지 데이터세트[8, 9, 68]의 가용성 덕분에 객체 레벨 멀티뷰 큐는 생성 모델로 인코딩될 수 있고 생성 목적으로 사용된다. NeRFs를 통해 사실적인 콘텐츠를 2D에서 3D로 들어올리기 위해 먼저 SDS(Score Distillation Sampling) 손실을 제안하는 DreamFusion[38]에 의해 개척된 텍스트 또는 이미지 입력으로부터 3D 콘텐츠 생성이 번성했다. 이 진행은 온라인 최적화[22, 41, 53, 60] 및 피드포워드 방법들[13, 24, 25, 59, 62]에 기초한 접근법들, 예를 들어 NeRFs[32], 트라이플레인[6, 7, 12] 및 3D 가우시안 스플래팅[18]과 같은 상이한 표현들을 포함한다. 3D 생성은 SDS 감독으로서 멀티뷰 제약[47] 및 3D 인식 확산 모델[26]을 포함시킴으로써 보다 일관된 멀티뷰가 된다. 고품질 렌더링에 국한되지 않고, 일부 작업 [29, 52]는 또한 정규 큐를 통합함으로써 생성된 3D 지오메트리의 품질을 향상시키는 것을 탐구한다.\n' +
      '\n' +
      '#### 3.1.2 4D 새로운 뷰 합성 및 재구성\n' +
      '\n' +
      '추가 변수로서 타임스탬프를 추가함으로써, 동적 NeRF[11, 19, 20, 36, 37, 54, 57], 동적 트라이플레인[5, 10, 45] 및 4D 가우시안 스플래팅[61, 67]과 같은 상이한 동적 표현들을 갖는 최근의 4D 방법들이 캘리브레이션된 멀티뷰 또는 캘리브레이션되지 않은 RGB 단안 비디오 입력들로부터 고품질의 4D 모션들 및 장면 콘텐츠 재구성을 달성하기 위해 제안된다. 또한, RGB-D 센서로 강체 및 비강체 장면 콘텐츠를 재구성하는 일부 작업[33, 34, 71]이 있으며, 이는 깊이 큐를 포함함으로써 3D 모호성을 해결하는 데 도움이 된다. 정적 3D 재구성 및 새로운 뷰 합성과는 달리, 강성 및 비강성 변형 둘 다로 구성된 4D 신규 뷰 합성은 RGB 단안 입력만으로 도전적이고 비포즈된 것으로 악명 높다. 일부 진행[11, 20, 54, 56]은 시간적 측광 일관성 및 4D 동작을 더 잘 정규화하기 위해 시간적 사전 및 모션 큐(예를 들어, 광학 흐름)를 포함한다. 최근 연구 중 하나인 [57]은 후진 변형 함수를 세계 좌표에서 표준 좌표로 반전시키지 않고 변형 가능한 NeRF에 대한 유동 감독에 대한 해석 솔루션을 제공한다. 여러 작품[63, 64, 65, 66]은 광학 흐름으로 단안 비디오에서 객체 수준의 메쉬 복구를 탐구한다.\n' +
      '\n' +
      '####3.1.3 4D 세대.\n' +
      '\n' +
      '텍스트 프롬프트들 또는 단일 이미지들로부터의 3D 생성과 유사하게, 텍스트 프롬프트들 또는 단안 비디오들로부터의 4D 생성은 또한 미리 트레이닝된 확산 모델들로부터의 프레임별 멀티뷰 큐들에 의존한다. 또한, 4D 생성 방법은 시간적인 일관성을 보장하기 위해 항상 비디오 확산 모델 또는 비디오 보간 블록에 의존한다. Animate124[70], 4D-fy[2] 및 가장 초기의 작업들 중 하나[48]는 동적 NeRF들을 4D 표현들로서 사용하고 텍스트-대-비디오 확산 모델들과 시간적 일관성을 달성하며, 이는 제어된 프레임 레이트들을 갖는 비디오들을 생성할 수 있다. 동적 NeRF를 사용하는 대신, Align Your Gaussians[23]과 DreamGaussian4D[42]는 3D Gaussian Splatting으로 생생한 4D 콘텐츠를 생성하지만, 자유 프레임 속도 제어를 위해 텍스트-비디오 확산 모델에 의존한다. 텍스트-비디오 확산 모델의 사용 없이, Consistent4D[15]는 기성 비디오 보간 모델과 일관성 있는 4D 생성을 달성한다[14]. 이 방법은 흐름 감독과 특수 시간 일관성 네트워크의 필요 없이 4D 가우시안 표현을 통해 이득을 얻는다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '2차원 영상에서 가우시안 모션과 픽셀 흐름의 관계를 더 잘 설명하기 위해 먼저 3차원 가우시안 스플래팅의 렌더링 과정을 요약한 후 4차원 사례를 조사한다.\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      '####3.1.1 3D Gaussian Splatting.\n' +
      '\n' +
      '초기화된 3차원 가우시안 프리미티브 집합으로부터 3차원 가우시안 스플래팅은 입력 영상(\\(\\{I\\}}_{m}\\)과 렌더링 영상(\\(\\{I\\}}_{r}}_{m}\\) 사이의 측광 손실을 최소화하여 3차원 장면을 복원하는 것을 목표로 한다. 각 픽셀에 대해 렌더된 색상 \\(C\\)은 Eq에서와 같이 점 기반 \\(\\alpha\\)-블렌딩에 의해 광선을 따라 깊이 순서대로 여러 가우시안 색상 \\(c_{i}\\)의 가중합이다. 1,\n' +
      '\n' +
      '\\[C=\\sum_{i=1}^{N}T_{i}\\alpha_{i}c_{i}, \\tag{1}\\]\n' +
      '\n' +
      '…을 지정하는 가중치들\n' +
      '\n' +
      '\\mathbf{x}-\\boldsymbol{\\mu}_{i}=o_{i}e^{-\\frac{1}{2}(\\alpha_{i}=o_{i}e^{-\\frac{2}(\\alpha_{i})^{T}\\boldsymbol{\\Sigma}_{i}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{i}}}\\quad\\text{and}\\quad T_{i}=\\sum_{j=1}^{i-1}(1-\\alpha_{i})}. \\tag{2}\\mbol{\\Sigma}_{i}}(\\mathbf{x}-\\boldsymbol{\\mu}_{i}=\\sum_{j=1}^{i-1}(1-\\alpha_{i}).\n' +
      '\n' +
      '그림 2: 연속된 두 프레임 사이에 2D 가우시안(i\\)의 움직임(i^{t_{1}}\\to i^{t_{2}}\\)에 의해 픽셀(x_{t_{1}}\\to x_{i,t_{2}}\\)을 향해 밀리게 된다. 우리는 가우스 \\(i\\)에서 \\(x_{t_{1}}\\)을 정규화된 가우스 공간으로 \\(\\hat{x}_{i}\\)으로 정규화하고 이미지 공간으로 비정규화하여 \\(x_{i,t_{2}}\\)을 구할 수 있다. 여기서 우리는 가우시안 \\(i\\)의 이동 기여도를 \\(flow_{i,t_{1},t_{2}}^{G}\\)으로 나타낸다. 가우스 흐름\\(flow_{t_{1},t_{2}}^{G}(x_{t_{1}})\\)은 픽셀\\(x_{t_{1}}\\)을 커버하는 모든 가우시안들의 이동 기여도의 가중 합으로 정의된다. 가중 인자는 알파 구성 가중치를 이용한다. 전체 이미지의 가우시안 흐름은 3D 가우시안 다이내믹스를 스플래팅하고 알파 구도로 렌더링함으로써 효율적으로 얻어질 수 있으며, 이는 원래의 3D 가우시안 스플래팅의 파이프라인과 유사하게 구현될 수 있다[18].\n' +
      '\n' +
      '여기서 \\(o_{i}\\in[0,1]\\), \\(\\mathbf{\\mu}\\in\\mathbbb{R}^{3\\times 1}\\) 및 \\(\\mathbf{\\Sigma}_{i}\\in\\mathbb{R}^{3\\times 3}\\)은 각각 \\(i\\)번째 가우시안의 불투명도, 3D 평균 및 3D 공분산 행렬이다. 그리고 \\(\\mathbf{x}\\)는 픽셀 광선과 \\(i\\)번째 가우시안 사이의 교차점이다. Eq.에 나타낸 바와 같다. 도 1에 도시된 바와 같이, 렌더링된 픽셀과 3D 가우시안들 사이의 관계는 양사형이 아니다.\n' +
      '\n' +
      '#### 3.1.3 3D Gaussian Splitting in 4D.\n' +
      '\n' +
      '3D 가우시안 스플래팅으로 4D 모션들을 모델링하는 것은 직접 멀티-뷰 피팅[30] 또는 시변 변형 필드를 갖는 3D 가우시안들을 이동하는 것[42, 23] 또는 시간을 갖는 3D 가우시안들을 파라미터화하는 것[67]을 통해 프레임 단위로 수행될 수 있다. 단안 입력의 경우, 상이한 가우시안 모션이 동일한 렌더링된 컬러로 이어질 수 있기 때문에 가우시안 모션은 과소 구속되고, 따라서 장기 지속 트랙이 손실된다[30]. 가우시안 모션의 전역적 자유도를 줄이기 위해 국부적 강직 손실[30, 23]이 제안되었지만, 초기화가 불량하거나 도전적이며 멀티 뷰 감독이 부족하여 심각한 문제를 야기하기도 한다. 도 1에 도시된 바와 같다. 6, 두개골 입을 닫은 상태에서 초기화된 3D 가우시안들은 국소 강직 손실로 입을 열 때 갈라지기 어렵다.\n' +
      '\n' +
      '### GaussianFlow\n' +
      '\n' +
      '가우시안 모션은 3차원 공간에 있기 때문에 4차원 가우시안 모션과 픽셀 모션을 연결하는 것은 어렵지만, 2차원 프로젝션(2d Gaussian)은 렌더링된 픽셀에 기여한다. 따라서 본 논문에서는 이러한 문제를 해결하기 위해 연속되는 두 프레임 사이의 영상 평면에 접선인 가우시안 회전 계수가 매우 작다고 가정한다. 이러한 가정을 통해 연속된 두 프레임으로부터 3차원 가우시안 2차원 투영을 시간에 따라 동일한 2차원 가우시안 변형(2D translation, rotation, scaling)으로 처리할 수 있다. 이 가정은 영상 공간에서 가우시안 동역학을 추적하는 것을 훨씬 앞으로 나아가게 하며 일반적으로 자연 장면에 대해 유효하다.\n' +
      '\n' +
      '우리는 각 시간 단계에서 1) 스케일링, 2) 회전 및 3) 병진을 포함하는 4D 필드에서 각 가우시안 모션의 완전한 자유도를 고려한다. 시간 변화에 따라, 그림 2와 같이 \\(t=t_{1}\\)에서 질의된 픽셀을 덮는 가우시안들은 \\(t=t_{2}\\)에서 다른 곳으로 이동할 것이다. \\(t=t_{2}\\)에서 새로운 픽셀 위치 \\(\\mathbf{x}_{t_{2}}\\)을 지정하기 위해, 먼저 모든 3D 가우시안들을 2D Gaussians로 2D 이미지 평면에 투영하고 픽셀 이동에 대한 모션의 영향을 계산한다.\n' +
      '\n' +
      'Single Gaussian에서 3.2.1 Flow\n' +
      '\n' +
      '가우시안 모션에 의한 픽셀 이동(흐름)을 추적하기 위해, 변형된 2D 가우시안에서 픽셀의 상대적 위치가 동일하게 유지되도록 한다. 이 설정은 가우시안 좌표계에서 질의된 픽셀 위치에서의 확률을 연속적인 두 시간 단계에서 변경하지 않게 한다. Eq에 따르면 도 2에 도시된 바와 같이, 변하지 않은 확률은 2D 가우시안이 변형되더라도 2D 가우시안으로부터 동일한 광도와 불투명도 기여를 갖는 픽셀을 부여할 것이다.\n' +
      '\n' +
      '픽셀 시프트(flow)는 두 시간 스텝에서 동일한 픽셀의 이미지 공간 거리이다. 우리는 먼저 픽셀을 덮는 단일 2D 가우시안(Gaussian)에 의해 영향을 받는 픽셀 이동을 계산한다. 우리는 화소를 정규화하여 \\(t_{2}\\)에서 \\(\\mathbf{x}\\)의 위치를 찾을 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '다시 말해서, 등방성 가우시안 필드들에 대해, 두 개의 상이한 시간 단계들 사이의 가우시안 흐름은 2D 가우시안들의 개별 번역의 가중 합으로서 근사화될 수 있다.\n' +
      '\n' +
      '두 식 중 하나를 따릅니다. 8 또는 Eq. 도 9를 참조하면, 가우시안 흐름은 각 픽셀에서 조밀하게 계산될 수 있다. (t=t_{1}\\)에서 \\(t=t_{2}\\)까지의 픽셀 \\(\\mathbf{x}_{t_{1}}\\)에서의 흐름 감독은 다음과 같이 지정될 수 있다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{flow}=||flow^{o}_{t_{1}t_{2}}(\\mathbf{x}_{t_{1}})-flow^{G}_{t_{1}t_{2}}||, \\tag{10}\\}\n' +
      '\n' +
      '여기서 광류\\(flow^{o}_{t_{1}t_{2}}\\)는 유사 지상진리로서 기성 방법에 의해 계산될 수 있다.\n' +
      '\n' +
      '#####4D 컨텐츠 생성\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 3에 도시된 바와 같이, 가우시안 표현을 갖는 4D 콘텐츠 생성은 4D 가우시안 필드를 입력 및 출력으로서 텍스트-투-비디오 또는 이미지-투-비디오 모델로부터 실제 캡처 또는 생성함으로써 교정되지 않은 단안 비디오를 취한다. 3D 가우시안들은 렌더링된 영상과 입력 영상 사이의 측광 감독과 다시점 SDS 감독을 위한 3D 인식 확산 모델[26]을 사용하여 첫 번째 비디오 프레임부터 초기화된다. 본 논문에서 제안하는 방법은 3차원 가우시안 초기화가 가능하다.\n' +
      '\n' +
      '도 3: 우리의 4D 콘텐츠 생성 파이프라인의 개요. 우리의 모델은 보정되지 않은 단안 비디오 또는 이미지로부터 생성된 비디오를 입력으로 취할 수 있다. 첫 번째 프레임을 기준 뷰에서 광도계로 매칭하고 새로운 뷰에서 필드를 감독하기 위해 3D 인식 SDS 손실[26]을 사용하여 3D 가우시안 필드를 최적화한다. 그런 다음, 각 프레임에 대해 동일한 두 손실을 갖는 3D 가우시안들의 동역학을 최적화한다. 가장 중요한 것은 연속되는 두 시간 단계에 대해 기준 뷰 상에서 가우시안 흐름을 계산하고 이를 입력 비디오의 미리 계산된 광학 흐름과 일치시킨다는 것이다. 흐름 매칭의 기울기는 동역학 스플래팅 및 렌더링 과정을 통해 다시 전파되어 자연스럽고 부드러운 움직임을 갖는 4D 가우시안 필드가 생성된다.\n' +
      '\n' +
      'One-2-3-45[25] 또는 DreamGaussian[53]에 의해 수행된다. 초기화 후, 4D 가우시안 필드는 Eq에서와 같이 프레임당 측광 감독, 프레임당 SDS 감독 및 흐름 감독으로 최적화된다. 10. 4D 가우시안 필드 최적화를 위한 손실 함수는 다음과 같이 기입될 수 있다:\n' +
      '\n' +
      '\\mathcal{L}_{photometric}+\\lambda_{1}\\mathcal{L}_{flow}+\\lambda_{2}\\mathcal{L}_{sds}+\\lambda_{3}\\mathcal{L}_{other}, \\tag{11}\\mathcal{L}_{flow}\n' +
      '\n' +
      '여기서 \\(\\lambda_{1}\\), \\(\\lambda_{2}\\) 및 \\(\\lambda_{3}\\)은 하이퍼파라미터이다. \\\\ (\\mathcal{L}_{other}\\)는 선택적이고 방법 의존적이다. 우리의 방법에는 사용되지 않았지만 완전성을 위해 남겨둡니다.\n' +
      '\n' +
      '##### 4D novel view Synthesis\n' +
      '\n' +
      '3D 인식 확산 모델로부터 다시점 객체-레벨을 갖는 4D 콘텐츠 생성과는 달리, 4D 신규 뷰 합성은 장면-레벨의 사전 없이 측광 감독을 위해 다시점 또는 단안 입력 비디오 프레임만을 취한다. 3D 가우시안들은 일반적으로 입력 비디오들로부터 sfm[44, 49]에 의해 초기화된다. 초기화 후, 4D 가우시안 필드는 프레임당 측광 감독 및 흐름 감독으로 최적화된다. 우리는 [67]의 4D 가우시안 필드를 채택한다. 4D 가우시안 필드 최적화를 위한 손실 함수는 다음과 같이 기입될 수 있다:\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathcal{L}_{photometric}+\\lambda_{1}\\mathcal{L}_{flow}+\\lambda_{3}\\mathcal{L}_{other}, \\tag{12}\\]\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 먼저 제안된 방법의 구현 세부 사항을 제공하고 (1) 4D 생성 및 (2) 4D 새로운 뷰 합성을 사용한 4D 가우시안 표현에서 우리의 방법을 유효하게 한다. 우리는 정량적 평가와 정성적 평가를 위해 Consistent4D 데이터세트[15]와 플렌옵틱 비디오 데이터세트[19]에 대해 테스트한다. 우리의 방법은 두 가지 작업 모두에서 최첨단 결과를 달성합니다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '우리는 \\(t_{1}\\)의 다음 타임스텝으로 \\(t_{2}\\)을 취하여 모든 실험에서 두 이웃 프레임 사이의 광 흐름을 계산한다. 가우시안 다이내믹 스플래팅의 CUDA 구현에 있어서, 각 픽셀 광선을 따라 가우시안 수\\(K\\)는 보통 다르지만, 속도와 효율의 균형을 맞추기 위해 \\(K=20\\)을 사용한다. 더 큰 \\(K\\)는 더 많은 수의 가우시안들을 의미하며 그들의 구배는 역전파를 통해 계산될 것이다. 크기 \\(H\\times W\\times 3\\)의 비디오 프레임들에 대해, 깊이 순서대로 정렬된 top-\\(K\\) 가우시안들의 인덱스들을 기록하기 위해 두 개의 (H\\times W\\times K\\) 텐서들을 유지함으로써 모든 이웃 타임스템들 \\(t_{1}\\)과 \\(t_{2}\\) 사이의 가우시안들의 움직임을 추적한다. 각 픽셀에 대해 top-\\(K\\) 가우시안들의 렌더링된 가중치 \\(w_{i}\\)과 크기 \\(H\\times W\\times K\\times 2\\)의 다른 텐서들은 픽셀 좌표와 2D 가우시안 평균 \\(\\mathbf{x}_{t_{1}}-\\boldsymbol{\\mu}},t_{1}}\\) 사이의 거리를 각각 나타낸다. 또한, 2D 평균\\(\\boldsymbol{\\mu}_{i,t_{1}}\\)과 2D 공분산 행렬\\(\\boldsymbol{\\Sigma}_{i,t_{1}}\\)과 \\(\\boldsymbol{\\Sigma}_{i,t_{2}}\\)은 카메라 투영을 통해 접근할 수 있다[18].\n' +
      '\n' +
      '### Dataset\n' +
      '\n' +
      '**Consistent4D Dataset.** 이 데이터 세트는 14개의 합성 및 12개의 야생 단안 비디오를 포함한다. 모든 비디오에는 흰색 이동 객체가 하나뿐입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{**Pistol**} & \\multicolumn{2}{c}{**Cuppie**} & \\multicolumn{2}{c}{**Crocodile**} & \\multicolumn{2}{c}{**Monster**} & \\multicolumn{2}{c}{**Skull**} & \\multicolumn{2}{c}{**Trump**} & \\multicolumn{2}{c}{**Aurorus**} & \\multicolumn{2}{c}{**Mean**} \\\\ \\cline{2-13}  & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) & LPIPS\\({}_{1}\\) & CLIP\\({}_{1}\\) \\\\ \\hline D-NeRF [28] & 0.52 & 0.66 & 0.32 & 0.76 & 0.54 & 0.61 & 0.52 & 0.79 & 0.53 & 0.72 & 0.55 & 0.60 & 0.56 & 0.66 & 0.51 & 0.68 \\\\ K-planes [10] & 0.40 & 0.74 & 0.29 & 0.75 & 0.19 & 0.75 & 0.47 & 0.73 & 0.41 & 0.72 & 0.51 & 0.66 & 0.37 & 0.67 & 0.38 & 0.72 \\\\ Consistent4D [15] & **0.10** & 0.90 & 0.12 & 0.90 & 0.12 & 0.82 & 0.18 & 0.90 & **0.17** & 0.88 & 0.23 & **0.85** & 0.17 & 0.85 & 0.16 & 0.87 \\\\ DCAID [24] & 0.12 & 0.92 & 0.12 & 0.91 & 0.12 & 0.88 & 0.19 & 0.90 & 0.18 & 0.90 & 0.22 & 0.83 & 0.17 & 0.86 & 0.16 & 0.87 \\\\ Ours & **0.10** & **0.94** & **0.10** & **0.93** & **0.10** & **0.90** & **0.17** & **0.92** & **0.17** & **0.92** & **0.20** & **0.85** & **0.15** & **0.89** & **0.14** & **0.91** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Consistent4D 데이터 세트에 대한 우리의 것과 다른 것의 정량적 비교.\n' +
      '\n' +
      '그림 4: Consistent4D 데이터셋에 대한 질적 결과.\n' +
      '\n' +
      '그림 5: Consistent4D[15](Con4D)와 우리의 질적 비교. 동적 NeRF 기반 방법으로 Consistent4D는 새로운 뷰에서 "버블과 같은" 텍스처와 일치하지 않는 기하학을 보여준다.\n' +
      '\n' +
      '배경. 합성 영상 중 7편에는 정량적 평가를 위한 다시점 지상진실이 제공된다. 정적 카메라를 이용한 입력 단안 영상은 방위각(0^{\\circ}\\)으로 설정하였다. 지상-진상 영상은 방위각 -75\\({}^{\\circ}\\), 15\\({}^{\\circ}\\), 105\\({}^{\\circ}\\), 195\\({}^{\\circ}\\)의 4개의 뚜렷한 뷰를 포함하고 있으며, 입력 카메라와 고도, 반경 및 기타 카메라 매개변수는 동일하게 유지한다.\n' +
      '\n' +
      '**Plenoptic Video Dataset.** 고품질 실세계 데이터셋은 30FPS와 2028 \\(\\times\\)2704 해상도의 6개의 장면으로 구성되어 있습니다. 훈련용 카메라 뷰는 장면당 15~20개, 테스트용 카메라 뷰는 1개입니다. 데이터세트에는 다시점 동기 카메라가 있지만, 모든 시점은 대부분 장면의 정면 부분으로 제한된다.\n' +
      '\n' +
      '### 결과 및 분석\n' +
      '\n' +
      '**4D Generation.** Tab에서 최근 4D Gaussian 기반 최첨단 생성 모델인 DreamGaussian4D[42]와 open-sourced code, dynamic NeRF 기반 방법을 비교 평가한다. 1은 우리와 일치하는 4D 데이터 세트에 있다. 개별 비디오에 대한 점수는 위에서 언급한 4개의 새로운 뷰에 대해 계산되고 평균화된다. 흐름 감독은 효과적이며 4D 생성 가우시안 표현에 도움이 된다는 점에 유의한다. 그림 4에서 우수한 질적 결과를 보여준다. DreamGaussian4D에 비해 우리의 방법은 그림 4와 같이 더 나은 품질을 보여준다. 동일한 수의 훈련 반복 후에 도 6을 참조한다. 그림 1에 표시된 두 개의 단단한 동적 장면에 대해. 도 6을 참조하면, 본 논문에서 제안한 방법은 흐름감독의 장점을 가지며, 바람직한 동작을 생성하는 반면, DG4D는 새로운 시각에서 두드러진 아티팩트를 보여준다. 또한, 제안된 방법은 동적 방법에 비해 색상 표류가 적음을 보여준다.\n' +
      '\n' +
      '그림 6: DreamGaussian4D[42], 흐름 손실이 없는 방법, 흐름 손실이 없지만 국소 강직 손실(Ours-r)과 우리의 방법 간의 정성적 비교.\n' +
      '\n' +
      'NeRF 기반 방법은 그림 4D와 일치한다. 5, 그리고 우리의 결과는 질감과 기하학 측면에서 더 일관적이다.\n' +
      '\n' +
      '**4D 새로운 뷰 합성.** 매우 최근의 최첨단 4D 가우스 방법 RT-4DGS[67]의 렌더링된 이미지와 깊이 맵을 그림에서 흐름 감독이 있는(노란색)과 없는(빨간색)으로 시각화한다. 또한, 도 6의 (a) 및 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6의 (a)와 도 6 6(b). 줌-인 비교에 따르면, 본 방법은 정반경이 강조된 광택 물체에서도 사실적인 동작을 일관되게 모델링하고 구조를 수정할 수 있다. 이러한 지역은 적절한 멀티뷰 감독하에서도 대부분의 방법에 대해 도전적인 것으로 알려져 있다[28, 55]. 제안된 방법은 모션 큐를 포함함으로써 측광 감독에서 모호성을 줄일 수 있으며 프레임 간에 일관되게 효과적인 것으로 나타났다. 외장형 광학 흐름 알고리즘[46]을 사용하여 플렌옵틱 비디오 데이터셋에서 얻은 이미지 픽셀의 1%에서 2%만이 하나의 픽셀보다 큰 광학 흐름 값을 갖는다는 것을 발견했다. 제안된 방법은 4차원 가우시안 기반 방법에 비해 움직임이 큰 영역에 더 많은 장점을 가지므로, Tab. 2에서 전체 장면 복원과 동적 영역(광학 흐름 값 \\(>1\\)) 모두에 대해 PSNR 수치를 보고한다. 제안된 흐름 감독은 모든 장면에서 더 좋은 성능을 보이고 동적 영역에서 더 큰 이득을 보인다. 결과적으로, 우리의 방법은 또한 4D 새로운 뷰 합성에 대한 최신 결과를 달성한다.\n' +
      '\n' +
      '## 5 Abablation Study\n' +
      '\n' +
      '그림 6에 표시된 정성적 비교를 통해 흐름 감독을 검증한다. Ours(흐름 없음) 및 Ours와 비교하여 제안된 흐름 감독은 움직이는 부품에 대한 효과를 보여준다. 두개골의 경우, \\(t=t_{1}\\)에서 초기화된 3차원 가우시안들은 서로 매우 가깝고 \\(t=t_{2}\\)일 때 완전히 분리되기 어렵다. 관측점 0에서 측광 MSE가 작기 때문에 부정확하게 그룹화된 가우시안들의 기울기가 작기 때문에, SDS 감독은 잠재 도메인에서 작동하고 픽셀 기반 감독을 제공할 수 없다. 그리고 지역 강직 손실을 포함할 때 문제가 더 심각해진다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Method & Coffee Martini & Spinach & Cut Beef & Flame Salmon & Flame Steak & Sear Steak & Mean \\\\ \\hline HexPlane [5] & - & 32.04 & 32.55 & 29.47 & 32.08 & 32.39 & 31.70 \\\\ K-Planes [10] & **29.99** & 32.60 & 31.82 & 30.44 & 32.38 & 32.52 & 31.63 \\\\ MixVoxels [58] & 29.36 & 31.61 & 31.30 & 29.92 & 31.21 & 31.43 & 30.80 \\\\ NeRFIlayer [50] & 31.53 & 30.56 & 29.35 & **31.65** & 31.93 & 29.12 & 30.69 \\\\ HyperReel [1] & 28.37 & 32.30 & 32.92 & 28.26 & 32.20 & 32.57 & 31.10 \\\\\n' +
      '4DGS [61] & 27.34 & 32.46 & 32.90 & 29.20 & 32.51 & 32.49 & 31.15 \\\\ RT-4DGS [67] & 28.33 & 32.93 & 33.85 & 29.38 & 34.03 & 33.51 & 32.01 \\\\ Ours & 28.42 & **33.68** & **34.12** & 29.36 & **34.22** & **34.00** & **32.30** \\\\ \\hline \\hline \\multicolumn{8}{c}{Dynamic Region Only} \\\\ \\hline RT-4DGS [67] & 27.36 & 27.47 & 34.48 & 23.16 & 26.04 & 29.52 & 28.00 \\\\ Ours & **28.02** & **28.71** & **35.16** & **23.36** & **27.53** & **31.15** & **28.99** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: DyNeRF 데이터 세트에 대한 우리와 다른 방법 간의 정량적 평가[19]. 우리는 지상-진실 광학 흐름 값이 한 픽셀보다 큰 풀-씬 새로운 뷰 합성 및 동적 영역 모두에 대한 PSNR 수를 보고한다. "Ours"는 제안된 흐름 감독이 있는 RT-4DGS를 나타낸다.\n' +
      '\n' +
      '도 7: DyNeRF 데이터세트 [19]에 대한 질적 비교. 왼쪽 열은 4D 가우시안 방법[67]의 새로운 뷰 렌더링 이미지와 깊이 맵을 보여주는데, 이는 동적 영역에서 아티팩트를 겪으며 움직이는 광택 객체에 대한 시변 정반사 효과를 거의 처리할 수 없다. 오른쪽 열은 훈련 중 흐름 감독으로 최적화하면서 동일한 방법의 결과를 보여준다. 우리는 더 많은 비교를 위해 보충 자료를 참조합니다.\n' +
      '\n' +
      '(t=t_{1}\\)에서 초기화된 3차원 가우시안들의 움직임은 이웃에 의해 구속되고 가우시안들은 \\(t=t_{1}\\)에서 분리되기 어렵기 때문에 Ours-r과 Ours를 비교한다. 유사하게, 새의 경우, 새의 부리와 같은 얇은 구조로 구성된 영역은 우리의 흐름 감독 없이 프레임에 걸쳐 완벽하게 유지될 수 없다. 원래 4D 가우시안 필드[30]에서 동작 중 구조적 일관성을 유지하기 위해 사용되었지만, 동작 제약으로서 국부적 강성 손실은 가우시안들을 잘못 그룹화할 수 있으며, 우리의 흐름 감독보다 덜 효과적이다.\n' +
      '\n' +
      '또한 그림 8의 흐름 감독 유무에 따른 광학적 흐름\\(흐름^{o}_{t_{1}t_{2}\\)과 가우시안 흐름\\(흐름^{G}_{t_{1}t_{2}\\)을 시각화하였다. 두 경우 모두 입력 뷰 상에서 렌더링된 이미지들 사이의 광학적 흐름\\(흐름^{o}_{t_{1}t_{2}\\)은 서로 매우 유사하며(#1과 #4 열에 표시됨), 입력 뷰 상에서 직접 측광 감독으로 인해 지상-진실 운동에 정렬된다. 그러나 #3과 #6과 같이 새로운 뷰에 대한 광학 흐름을 비교하면, 새로운 뷰에 대한 측광 감독 없이 일관되지 않은 가우시안 모션이 우리의 흐름 감독 없이 목격된다. 가우시안 흐름\\(flow^{G}_{t_{1}t_{2}}\\)을 #2 열과 같이 시각화하면 일관성 없는 가우시안 움직임도 나타난다. 잘못된 가우시안 모션은 여전히 입력 뷰 상에서 정확한 이미지 프레임들을 환각할 수 있다. 그러나 이러한 모션-출현 모호성은 새로운 뷰에서 비현실적인 모션(불평활한 흐름 색상)으로 이어질 수 있다.\n' +
      '\n' +
      '도 8: 입력 뷰 및 신규 뷰 상에 광학 및 가우시안 흐름의 시각화. "우리(흐름 없음)"는 흐름 감독이 없는 모델을 나타내는 반면 "우리"는 전체 모델이다. 조밀한 광학 흐름 알고리즘은 배경 픽셀 간의 대응 관계를 계산하기 때문에 배경의 광학 흐름 값은 무시해야 한다. 렌더링된 시퀀스에 대해 오토플로우(autoflow)를 이용하여 광학흐름(optical flow)\\(flow^{o}_{t_{1}t_{2}}\\)을 계산한다. #1 및 #4 열로부터, 입력 뷰 상에서 렌더링된 시퀀스들 모두는 정확한 모션들 및 외관을 나타내는 고품질 광학 흐름을 갖는다는 것을 알 수 있다. #2 열과 #5 열에서 가우시안 흐름을 비교하면 밑줄 친 가우시안들이 흐름 감독 없이 일관되지 않게 움직일 것임을 알 수 있다. 이는 단일 입력 뷰에서 측광 손실에 의해서만 최적화되면서 외관과 모션의 모호성에 기인한다. 광학 흐름에 가우시안 흐름을 정렬하면 불규칙한 움직임(#3 열)을 획기적으로 개선하고 새로운 뷰에서 고품질 동적 움직임(#6 열)을 생성할 수 있다.\n' +
      '\n' +
      '상기 #3)에서 부품을 이동시키는 단계를 포함하는, 방법. #5는 일관된 가우시안 흐름을 보여주며, 흐름 감독과 일관된 가우시안 움직임을 나타낸다.\n' +
      '\n' +
      '##6 결론 및 향후 과제\n' +
      '\n' +
      '본 논문에서는 2차원 광학 흐름으로 스케일링, 회전, 병진운동을 포함한 3차원 가우시안 동역학을 감독할 수 있는 해석적 해법인 가우시안 플로우를 제시한다. 광범위한 정성적 및 정량적 비교는 우리의 방법이 4D 생성 및 4D 새로운 움직임 뷰 합성 모두에 대해 가우시안 기반 표현에 일반적이고 유익하다는 것을 보여준다. 이 논문에서는 모든 실험에서 모든 두 이웃 프레임 간의 단기 흐름 감독만을 고려한다. 여러 프레임에 걸친 장기 흐름 감독은 더 좋고 원활할 것으로 예상되며, 이는 향후 작업으로 남겨둔다. 또 다른 유망한 미래 방향은 4D 생성 작업에서 새로운 뷰에 대한 가우시안 흐름을 감독하기 위해 뷰 조절 흐름 SDS를 탐색하는 것이다.\n' +
      '\n' +
      '## 7 Acknowledgments\n' +
      '\n' +
      '사려 깊고 가치 있는 논의에 대해 리정치와 진천천에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Attal, B., Huang, J.B., Richardt, C., Zollhoefer, M., Kopf, J., O\'Toole, M., Kim, C.: Hyperreel: High-fidelity 6-dof video with ray-conditioned sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16610-16620 (2023)\n' +
      '* [2] Bahmani, S., Skorokhodov, I., Rong, V., Wetzstein, G., Guibas, L., Wonka, P., Tulyakov, S., Park, J.J., Tagliasacchi, A., Lindell, D.B.: 4d-fy: Text-to-4d generation using hybrid score distillation sampling. arXiv preprint arXiv:2311.17984 (2023)\n' +
      '* [3] Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Li, Y., Michaeli, T., et al.: Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945 (2024)\n' +
      '* [4] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video generation models as world simulators (2024), [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)\n' +
      '* [5] Cao, A., Johnson, J.: Hexplane: A fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 130-141 (2023)\n' +
      '* [6] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16123-16133 (2022)\n' +
      '* [7] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision. pp. 333-350. Springer (2022)* [8] Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Obijavers: A universe of annotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13142-13153 (2023)\n' +
      '* [9] Downs, L., Francis, A., Koenig, N., Kinman, B., Hickman, R., Reymann, K., McHugh, T.B., Vanhoucke, V.: Google scanned objects: A high-quality dataset of 3d scanned household items. In: 2022 International Conference on Robotics and Automation (ICRA). pp. 2553-2560. IEEE (2022)\n' +
      '* [10] Fridovich-Keil, S., Meanti, G., Warburg, F.R., Recht, B., Kanazawa, A.: K-planes: Explicit radiance fields in space, time, and appearance. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12479-12488 (2023)\n' +
      '* [11] Gao, C., Saraf, A., Kopf, J., Huang, J.B.: Dynamic view synthesis from dynamic monocular video. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5712-5721 (2021)\n' +
      '* [12] Gao, Q., Xu, Q., Su, H., Neumann, U., Xu, Z.: Strivec: Sparse tri-vector radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 17569-17579 (2023)\n' +
      '* [13] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)\n' +
      '* [14] Huang, Z., Zhang, T., Heng, W., Shi, B., Zhou, S.: Real-time intermediate flow estimation for video frame interpolation. In: European Conference on Computer Vision. pp. 624-642. Springer (2022)\n' +
      '* [15] Jiang, Y., Zhang, L., Gao, J., Hu, W., Yao, Y.: Consistent4d: Consistent 360 {\\(\\backslash\\)deg} dynamic object generation from monocular video. arXiv preprint arXiv:2311.02848 (2023)\n' +
      '* [16] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023)\n' +
      '* [17] Keetha, N., Karhade, J., Jatavallabhula, K.M., Yang, G., Scherer, S., Ramanan, D., Luiten, J.: Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126 (2023)\n' +
      '* [18] Kerbl, B., Kopanas, G., Leimkuhler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics **42**(4) (2023)\n' +
      '* [19] Li, T., Slavcheva, M., Zollhoefer, M., Green, S., Lassner, C., Kim, C., Schmidt, T., Lovegrove, S., Goesele, M., Newcombe, R., et al.: Neural 3d video synthesis from multi-view video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5521-5531 (2022)\n' +
      '* [20] Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time view synthesis of dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6498-6508 (2021)\n' +
      '* [21] Li, Z., Wang, Q., Cole, F., Tucker, R., Snavely, N.: Dynibar: Neural dynamic image-based rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4273-4284 (2023)\n' +
      '* [22] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 300-309 (2023)\n' +
      '* [23] Ling, H., Kim, S.W., Torralba, A., Fidler, S., Kreis, K.: Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint arXiv:2312.13763 (2023)* [24] Liu, M., Shi, R., Chen, L., Zhang, Z., Xu, C., Wei, X., Chen, H., Zeng, C., Gu, J., Su, H.: One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885 (2023)\n' +
      '* [25] Liu, M., Xu, C., Jin, H., Chen, L., Varma T, M., Xu, Z., Su, H.: One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [26] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298-9309 (2023)\n' +
      '* [27] Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint arXiv:2309.03453 (2023)\n' +
      '* [28] Liu, Y., Wang, P., Lin, C., Long, X., Wang, J., Liu, L., Komura, T., Wang, W.: Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images. arXiv preprint arXiv:2305.17398 (2023)\n' +
      '* [29] Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008 (2023)\n' +
      '* [30] Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023)\n' +
      '* [31] Matsuki, H., Murai, R., Kelly, P.H., Davison, A.J.: Gaussian splatting slam. arXiv preprint arXiv:2312.06741 (2023)\n' +
      '* [32] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM **65**(1), 99-106 (2021)\n' +
      '* [33] Newcombe, R.A., Fox, D., Seitz, S.M.: Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 343-352 (2015)\n' +
      '* [34] Newcombe, R.A., Izadi, S., Hilliges, O., Molyneaux, D., Kim, D., Davison, A.J., Kohi, P., Shotton, J., Hodges, S., Fitzgibbon, A.: Kinectfusion: Real-time dense surface mapping and tracking. In: 2011 10th IEEE international symposium on mixed and augmented reality. pp. 127-136. Ieee (2011)\n' +
      '* [35] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022)\n' +
      '* [36] Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin-Brualla, R.: Nerfies: Deformable neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5865-5874 (2021)\n' +
      '* [37] Park, K., Sinha, U., Hedman, P., Barron, J.T., Bouaziz, S., Goldman, D.B., Martin-Brualla, R., Seitz, S.M.: Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228 (2021)\n' +
      '* [38] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n' +
      '* [39] Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural radiance fields for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10318-10327 (2021)\n' +
      '* [40] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)* [41] Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S., Aberman, K., Rubinstein, M., Barron, J., et al.: Dreambooth3d: Subject-driven text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023)\n' +
      '* [42] Ren, J., Pan, L., Tang, J., Zhang, C., Cao, A., Zeng, G., Liu, Z.: Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023)\n' +
      '* [43] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684-10695 (2022)\n' +
      '* [44] Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4104-4113 (2016)\n' +
      '* [45] Shao, R., Zheng, Z., Tu, H., Liu, B., Zhang, H., Liu, Y.: Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16632-16642 (2023)\n' +
      '* [46] Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint arXiv:2303.08340 (2023)\n' +
      '* [47] Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023)\n' +
      '* [48] Singer, U., Sheynin, S., Polyak, A., Ashual, O., Makarov, I., Kokkinos, F., Goyal, N., Vedaldi, A., Parikh, D., Johnson, J., et al.: Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280 (2023)\n' +
      '* [49] Snavely, N., Seitz, S.M., Szeliski, R.: Photo tourism: exploring photo collections in 3d. In: ACM siggraph 2006 papers, pp. 835-846 (2006)\n' +
      '* [50] Song, L., Chen, A., Li, Z., Chen, Z., Chen, L., Yuan, J., Xu, Y., Geiger, A.: Nerf-player: A streamable dynamic scene representation with decomposed neural radiance fields. IEEE Transactions on Visualization and Computer Graphics **29**(5), 2732-2742 (2023)\n' +
      '* [51] Sun, D., Vlasic, D., Herrmann, C., Jampani, V., Krainin, M., Chang, H., Zabih, R., Freeman, W.T., Liu, C.: Autoflow: Learning a better training set for optical flow. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10093-10102 (2021)\n' +
      '* [52] Sun, J., Zhang, B., Shao, R., Wang, L., Liu, W., Xie, Z., Liu, Y.: Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818 (2023)\n' +
      '* [53] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023)\n' +
      '* [54] Tretschk, E., Tewari, A., Golyanik, V., Zollhofer, M., Lassner, C., Theobalt, C.: Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12959-12970 (2021)\n' +
      '* [55] Verbin, D., Hedman, P., Mildenhall, B., Zickler, T., Barron, J.T., Srinivasan, P.P.: Ref-nerf: Structured view-dependent appearance for neural radiance fields. in 2022 ieee. In: CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 5481-5490 (2022)\n' +
      '* [56] Wang, C., Eckart, B., Lucey, S., Gallo, O.: Neural trajectory fields for dynamic novel view synthesis. arXiv preprint arXiv:2105.05994 (2021)\n' +
      '* [57] Wang, C., MacDonald, L.E., Jeni, L.A., Lucey, S.: Flow supervision for deformable nerf. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21128-21137 (2023)* [58] Wang, F., Tan, S., Li, X., Tian, Z., Song, Y., Liu, H.: Mixed neural voxels for fast multi-view video synthesis. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 19706-19716 (2023)\n' +
      '* [59] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023)\n' +
      '* [60] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [61] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023)\n' +
      '* [62] Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wetzstein, G., Xu, Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217 (2023)\n' +
      '* [63] Yang, G., Sun, D., Jampani, V., Vlasic, D., Cole, F., Chang, H., Ramanan, D., Freeman, W.T., Liu, C.: Lasr: Learning articulated shape reconstruction from a monocular video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15980-15989 (2021)\n' +
      '* [64] Yang, G., Sun, D., Jampani, V., Vlasic, D., Cole, F., Liu, C., Ramanan, D.: Viser: Video-specific surface embeddings for articulated 3d shape reconstruction. Advances in Neural Information Processing Systems **34**, 19326-19338 (2021)\n' +
      '* [65] Yang, G., Wang, C., Reddy, N.D., Ramanan, D.: Reconstructing animatable categories from videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16995-17005 (2023)\n' +
      '* [66] Yang, G., Yang, S., Zhang, J.Z., Manchester, Z., Ramanan, D.: Ppr: Physically plausible reconstruction from monocular videos. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3914-3924 (2023)\n' +
      '* [67] Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023)\n' +
      '* [68] Yu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Zhu, C., Xiong, Z., Liang, T., et al.: Mvimgnet: A large-scale dataset of multi-view images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9150-9161 (2023)\n' +
      '* [69] Yugay, V., Li, Y., Gevers, T., Oswald, M.R.: Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070 (2023)\n' +
      '* [70] Zhao, Y., Yan, Z., Xie, E., Hong, L., Li, Z., Lee, G.H.: Animate124: Animating one image to 4d dynamic scene. arXiv preprint arXiv:2311.14603 (2023)\n' +
      '* [71] Zollhofer, M., Niessner, M., Izadi, S., Rehmann, C., Zach, C., Fisher, M., Wu, C., Fitzgibbon, A., Loop, C., Theobalt, C., et al.: Real-time non-rigid reconstruction using an rgb-d camera. ACM Transactions on Graphics (ToG) **33**(4), 1-12 (2014)\n' +
      '\n' +
      '**Appendix**\n' +
      '\n' +
      '## 부록 0. 추가 구현 상세사항\n' +
      '\n' +
      '제안된 흐름감독을 위한 상세한 의사코드는 알고리즘 1에서 찾을 수 있으며, 투영된 가우시안 동역학을 추출하고 이 동역학을 렌더링하여 최종 가우시안 흐름을 얻는다. 3차원 가우시안 스플래팅(Gaussian Splatting, 18)의 원본 CUDA 커널 코드를 수정하여 픽셀당 가우시안(Gaussians)의 가중치 및 top-\\(K\\) 지수를 포함한 변수를 CUDA에서 계산한다. 그리고 Gaussian flow\\(flow^{G}\\)는 PyTorch와 Eq.8에 의해 계산된다.\n' +
      '\n' +
      '본 논문에서 제안한 4D 생성 실험에서는 3D 가우시안 필드들을 16의 배치 크기로 초기화하기 위해 500번의 반복 정적 최적화를 수행하였고, SDS의 Tmax는 0.98에서 0.02까지 선형적으로 감소되었으며, 동적 표현을 위해 DG4D[42]와 우리의 DG4D[42] 모두에 대해 4의 배치 크기로 600번의 반복을 수행하였다. Eq.에서의 유량감소량\\(\\lambda_{1}\\) 본 논문의 11은 1.0입니다.\n' +
      '\n' +
      '4D 새로운 뷰 합성 실험에서 우리는 모든 카메라에 대해 제안된 흐름 감독을 추가하는 것을 제외하고 RT-4DGS[67]를 따른다. Eq.에서의 유량감소량\\(\\lambda_{1}\\) 본 논문의 11은 0.5입니다.\n' +
      '\n' +
      '## 부록 0.B 추가 결과\n' +
      '\n' +
      '4D 생성에서 더 많은 가우시안 흐름\n' +
      '\n' +
      '렌더링된 영상에 대한 가우시안 흐름\\(flow^{G}\\)과 광학 흐름\\(flow^{o}\\)의 비교가 그림 1에 나와 있다. 각 예시의 첫 번째 행은 최적화된 4D 가우시안 필드로부터 렌더링된 rgb 프레임이다. 각 시간 단계마다 카메라를 회전시켜 물체가 최적화된 대로 움직일 수 있도록 하고 카메라가 동시에 움직여 다른 각도에서 장면을 보여줍니다. 각 예의 두 번째 행은 시각화된 가우시안 흐름을 보여준다. 이러한 가우시안 흐름들은 각각의 카메라 뷰에서 연속적인 시간 단계들의 렌더링된 이미지들에 의해 계산되며, 따라서 흐름 값들에 카메라 모션이 포함되지 않는다. 세 번째 행은 각각의 카메라 뷰에서 연속적인 시간 스텝들의 렌더링된 이미지들 사이의 추정된 광학 흐름들이다. 추정을 위해 기성품 AutoFlow[51]를 사용한다. 단일 입력 뷰에서 흐름 감독에 의해 강화된 것을 볼 수 있으며, 우리의 4D 생성 파이프라인은 건 해머의 폭발 운동과 같은 빠른 운동을 모델링할 수 있다(도 1의 마지막 예 참조).\n' +
      '\n' +
      'DyNeRF 데이터셋에 대한 결과가 더 있습니다.\n' +
      '\n' +
      'DyNeRF 데이터 세트 [19]에 대한 보다 정성적 결과는 그림 1에서 찾을 수 있다. 2와 우리의 비디오.\n' +
      '\n' +
      '```\n' +
      '입력:\\(flow^{o}_{t_{k},t_{k+1}:\\) Pseudo ground-truth optical flow, 여기서 \\(k=0,1,...,T\\); (renderer\\(Gaussians_{t_{k}}}), \\(Gaussians_{t_{t_{k}}}=renderer_{t_{k}}},dim=-.n\\) timestep \\(t_{t_{k}})에 대한 \\(H\\times W\\t_{k}}=renderer_{t_{k}}}/sum(w_{t_{k}}) ["(index\\)"); #\\(H\\times W\\times K\\), \\(gaussians_{t_{k}}=renderer_{t_{k}}) 및 \\(cam_{t_{k+1}})에 대한 카메라 파라미터\n' +
      '```\n' +
      '\n' +
      '알고리즘 1**가우시안 플로우 그림 1에 대한 상세한 의사 코드: 서로 다른 뷰에서 렌더링된 시퀀스에 대한 가우시안 플로우\\(플로우^{G}\\) 및 옵티컬 플로우\\(플로우^{o}\\)의 시각화.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '도 2: DyNeRF 데이터세트 [19]에 대한 질적 비교. 왼쪽 열은 4D 가우시안 방법의 새로운 뷰 렌더링된 이미지 및 깊이 맵을 보여준다[67]. 오른쪽 열은 훈련 중 흐름 감독으로 최적화하면서 동일한 방법의 결과를 보여줍니다.\n' +
      '\n' +
      '도 4: DyNeRF 데이터셋에 대한 정성적 비교[19]. 우리는 \\(Flame\\ Salmon\\)에 대한 깊이 맵의 세부 정보를 인식하기 어렵기 때문에 렌더링된 이미지만을 비교한다. 왼쪽 열은 4D 가우시안 방법의 새로운 뷰 렌더링된 이미지들을 보여준다[67]. 오른쪽 열은 훈련 중 흐름 감독으로 최적화하면서 동일한 방법의 결과를 보여줍니다.\n' +
      '\n' +
      '도 3: \\(화염\\연어\\)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>