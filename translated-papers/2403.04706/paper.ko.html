<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Common 7B 언어 모델은 이미 강력한 수학 능력을 보유한다\n' +
      '\n' +
      '천리({}^{1,4}\\), 위치왕({}^{2,4}\\), 징청후({}^{3,4}\\), 이천위({}^{3,4}\\),\n' +
      '\n' +
      'Nanning Zheng\\({}^{1}\\), Han Hu\\({}^{4}\\), Zheng Zhang\\({}^{4}\\), Houwen Peng\\({}^{4}\\)**\n' +
      '\n' +
      '중국과학기술대학교\n' +
      '\n' +
      '싱화대학교 마이크로소프트 연구 아시아\n' +
      '\n' +
      'stu.xjtu.edu.cn(v-weiqiwang, t-jingchu, t-yixuanwei, zhez, houwen.peng)@microsoft.com nnzheng@xjtu.edu.cn ancientmooner@gmail.com\n' +
      '\n' +
      '프로젝트 리더 첸, 웨이치, 징청, Yixuan은 MSRA의 인턴이다. GitHub: Xwin-Math 이 리포지토리는 지속적으로 업데이트됩니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '수학 능력은 이전에 매우 큰 규모로만 공통 언어 모델에서 등장하거나 광범위한 수학 관련 사전 훈련이 필요한 것으로 여겨졌다. 본 논문은 일반적인 사전 훈련을 가진 LLaMA-2 7B 모델이 256개의 무작위 세대에서 최상의 응답을 선택할 때 GSM8K 및 MATH 벤치마크에서 각각 97.7% 및 72.0%의 인상적인 정확도로 입증된 바와 같이 이미 강력한 수학적 능력을 나타냄을 보여준다. 현재 기반 모델의 주요 문제는 고유한 수학적 능력을 일관되게 이끌어내기 어렵다는 것이다. 특히, 첫 번째 답변의 정확도는 GSM8K 및 MATH 벤치마크에서 각각 49.5% 및 7.9%로 떨어진다. 우리는 단순히 SFT 데이터를 확장하면 정답 생성의 신뢰성을 크게 향상시킬 수 있다는 것을 발견했다. 그러나 광범위한 스케일링의 가능성은 공개적으로 사용할 수 있는 수학 문제의 부족에 의해 제한된다. 이러한 한계를 극복하기 위해 실제 데이터만큼 효과적인 것으로 판명된 합성 데이터를 사용하고 약 100만 샘플까지 확장할 때 명확한 포화도를 나타내지 않는다. 이 간단한 접근법은 LLaMA-2 7B 모델을 사용하여 GSM8K에서 82.6%, MATH에서 40.6%의 정확도를 달성하여 이전 모델을 각각 14.2% 및 20.8% 능가한다. 또한 다양한 추론 복잡성과 오류 유형에 걸친 스케일링 행동에 대한 통찰력을 제공합니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '수학적 역량은 오랫동안 매우 큰 규모로만 공통 언어 모델에서 등장할 정도로 어려운 것으로 여겨져 왔다. 예를 들어, (Wei et al., 2022, 2020)의 연구에 따르면 500억 매개 변수를 초과하는 크기의 모델만이 수학 문제에 대한 의미 있는 정확도를 얻거나 사고 연쇄 처리의 이점을 얻을 수 있다. 더 작은 언어 모델을 수학적 능력을 갖추기 위한 전략은 수천억 개의 수학 관련 사전 훈련 데이터에 대해 훈련된 수학 특정 기반 모델을 생성하는 것을 포함한다(Lewkowycz et al., 2022; Azerbayev et al., 2023). 그러나, 그러한 모델들의 정확도는 여전히 완만하다; 예를 들어, Lemma-7B(Azerbayev et al., 2023)는 GSM8K 데이터세트(Cobbe et al., 2021) 상에서 36.4% 및 MATH 데이터세트(Hendrycks et al., 2021) 상에서 18.0%만을 달성한다.\n' +
      '\n' +
      '본 논문에서는 LLaMA-2 7B 모델(Touvron et al., 2023)과 같이 크기가 작은 공통 언어 모델이 수학 관련 데이터에 대한 특정 사전 훈련 없이 이미 강력한 수학적 능력을 가지고 있음을 보여준다. 놀랍게도, 우리는 단지 수천 개의 수학 문제들에 대한 감독적인 미세 조정을 통해(SFT 단계가 언급된 바와 같이 능력을 향상시키지 않는다는 것을 언급함) 발견한다.\n' +
      '\n' +
      '그림 1: 주황색 별 표지자는 LLaMA-2 7B 모델의 256개 무작위 세대 중 최상의 응답을 선택하여 달성한 정확도를 나타낸다. MATH(왼쪽) 벤치마크와 GSM8K(오른쪽) 벤치마크(각각 72.0%와 97.7%)의 높은 정확도는 LLaMA-2 7B가 정답 생성의 안정성을 높일 수 있지만 이미 강력한 수학적 능력을 가지고 있음을 시사한다. 이 논문은 합성 SFT 데이터를 스케일링함으로써 곡선에 의해 입증된 바와 같이 안정성이 크게 향상될 수 있음을 보여준다. 이러한 SFT 데이터의 간단한 스케일링을 통해 상위 성능 모델은 MATH 벤치마크에서 초기 GPT-4 모델을 10.3% 초과했다.\n' +
      '\n' +
      '(Bai et al., 2022; Ouyang et al., 2022)) 모델은 그림 1의 주황색 별 표시로 표시된 바와 같이 256개의 무작위 세대에서 최상의 답을 선택할 때 GSM8K 질문의 97.7%와 MATH 질문의 72.0%를 올바르게 해결할 수 있다. 정확도가 GSM8K에서 92.0%, MATH 1에서 42.5%를 달성한 GPT-4 모델에 대해 보고된 것보다 훨씬 우수하다는 점은 주목할 만하다. 따라서 LLaMA-2 7B 모델이 실제로 강력한 수학적 능력을 개발했다고 결론지었다. 대부분의 세대가 부정확하기 때문에 정답이 발굴될 것이라는 보장의 부족이 주요 쟁점이다. 사실, 질문당 하나의 무작위 생성만을 고려할 경우 GSM8K에서는 49.5%, MATH에서는 7.9%로 정확도가 떨어진다. 이를 _instability issue_라고 한다.\n' +
      '\n' +
      '각주 1: 정확도는 GPT-4 기술 보고서(OpenAI, 2023b)에 보고되어 있다. GPT-4 모델은 지속적으로 개선되고 있다. 최신 GPT-4 터보(1106) API는 GSM8K에서 94.8%, MATH에서 64.5%로 정확도를 높였다. 그러나 256세대 중 최고를 사용하는 LLaMA-2 7B 모델은 여전히 최신 GPT-4 모델보다 우수하다.\n' +
      '\n' +
      '불안정 문제를 해결하기 위해, 우리는 먼저 SFT(supervised fine-tuning) 데이터가 기하급수적으로 증가함에 따라 정확도가 선형 또는 심지어 초선형에서 거의 개선된다는 것을 관찰한다. 더욱이, 우리는 사용 가능한 모든 GSM8K 및 MATH 트레이닝 데이터(표 1에 나타낸 바와 같이)를 활용할 때 정확도가 고원에 도달하는 것과는 거리가 멀다는 점에 주목한다. 이 관찰은 우리가 SFT 데이터를 더 확장하도록 장려합니다. 그러나 이러한 지속적인 스케일링을 지원하기 위해 공개적으로 액세스할 수 있는 실제 데이터가 부족하기 때문에 우리는 도전에 직면해 있다.\n' +
      '\n' +
      '이러한 한계를 극복하기 위해, 우리는 합성 수학 문제를 생성하기 위해 명문 언어 모델인 GPT-4 터보(Turbo)를 사용하는 합성 데이터로 전환한다. 우리는 GPT-4 Turbo가 선호 질문을 기반으로 완전히 새로운 질문을 생성하도록 유도하고 간단한 검증기(GPT-4 Turbo 기반)를 적용하는 간단한 "브랜드-뉴" 생성 전략이 매우 효과적이라는 것을 발견했다. 구체적으로, 표 1에 나타낸 바와 같이, 합성적으로 생성된 수학 질문의 사용은 실제 질문과 거의 동등한 정확도를 달성할 수 있으며, 스케일링 목적을 위한 합성 SFT 수학 질문의 잠재력을 강조한다.\n' +
      '\n' +
      '합성 데이터를 활용하면 SFT 데이터를 GSM8K의 경우 7.5K에서 960K로, MATH의 경우 7.5K에서 480K로 크게 확장할 수 있다. 이 데이터 스케일링은 그림 1에 그려진 것과 같이 거의 완벽한 스케일링 동작을 보여준다. 구체적으로, SFT 데이터를 간단히 스케일링함으로써, 본 모델은 표준 LLaMA-2 7B 기본 모델(각각 82.6% 및 40.6% 달성)2를 사용하여 GSM8K 및 MATH에서 각각 80% 및 40% 정확도를 초과하는 최초의 모델이 되었다.\n' +
      '\n' +
      '각주 2: Concurrently, DeepSeek-MATH-7B(Shao et al., 2024)도 80% 정확도를 능가한다. 그러나, 그들의 접근법은 수학 관련 말뭉치에서 광범위하게 사전 훈련된 훨씬 더 강력한 기본 모델과 정교한 RL 알고리즘에 의존한다. 우리의 결과는 그들의 결과와 보완적이다.\n' +
      '\n' +
      '간단한 합성 SFT 데이터는 GSM8K에서 90.6%, MATH에서 52.8%를 달성하는 LLaMA-2 7OB와 같은 더 강력한 기본 모델에서도 효과적임을 입증한다. 우리가 아는 한, 이것은 GSM8K에서 90%의 정확도를 초과하는 최초의 오픈 소스 모델이다. 또한 MATH 벤치마크에서 GPT-4(즉, GPT-4-0314)를 능가하는 최초의 오픈 소스 모델이며, 간단한 합성 스케일링 방법의 효능을 보여준다.\n' +
      '\n' +
      '강한 결과 외에도 접근법의 효과에 대한 통찰력을 수집했다: 1) SFT 데이터의 규모가 증가함에 따라 256개의 시도를 사용할 때 모델의 정확도가 안정되는 경향이 있지만 1개의 응답을 사용하여 현저한 증가가 있다. 이는 모델의 능력 상한이 상당히 일정하게 유지되지만 성능 향상은 주로 정답 생성의 안정성 향상으로 인한 것임을 나타낸다. 2) 수학 문제 해결의 정확도는 SFT 데이터 양이 다른 CoT(chain-of-thought) 단계의 수에 대한 멱 법칙을 따른다. 확장된 SFT 데이터셋은 각 추론 단계의 신뢰도를 향상시키며, 재샘플링을 통해 CoT 단계가 긴 훈련 샘플의 비율을 더 높이면 어려운 문제에 대한 모델의 정확도를 크게 향상시킬 수 있다. 3) 스케일링 과정에서 오류 유형을 분석하면 계산 오류가 추론 오류에 비해 쉽게 완화된다는 것을 알 수 있다.\n' +
      '\n' +
      '##2 언어 모델의 수학 능력 시험\n' +
      '\n' +
      '메트릭스는 언어 모델의 수학 능력을 조사하기 위해 두 가지 메트릭을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline Data size & GSM8K-real & GSM8K-syn & MATH-real & MATH-syn \\\\ \\hline\n' +
      '0.94K & 26.7 & 25.9 & 4.2 & 3.9\\\\\n' +
      '1.88K & 32.8 & 31.9 & 5.6 & 4.9\\\\\n' +
      '3.75K & 43.3 & 42.2 & 6.6 & 6.0\\\\\n' +
      '7.50K & 50.2 & 49.5 & 8.4 & 7.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: SFT 데이터 스케일링과 실제 수학 문제 대 합성 수학 문제의 비교. 합성 수학 문제가 실제 문제만큼 효과적임을 드러낸다.\n' +
      '\n' +
      '상기 첫번째는 Pass@N 메트릭\n' +
      '\n' +
      '\\[\\text{Pass@N}=\\operatorname*{\\mathbb{E}}_{\\text{Problems}}\\left[\\min(c,1)\\right], \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(c\\)는 \\(N\\) 응답 중에서 정답의 수를 나타낸다. 이 메트릭은 적어도 하나의 정답이 \\(N\\) 랜덤 세대에서 생성되면 풀어야 할 질문을 고려한다. 우리는 수학 문제를 풀 때 모델의 잠재력이나 능력을 반영하기 위해 이 척도를 사용한다. 우리는 \\(N\\) 세대의 다양성을 향상시키기 위해 생성 과정의 온도를 0.73으로 설정했다.\n' +
      '\n' +
      '각주 3: 대부분의 수학 모델은 온도를 0으로 설정한 탐욕스러운 세대 전략을 사용하지만 이러한 차이의 영향은 미미하다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '두 번째는 PassRatio@N 메트릭\n' +
      '\n' +
      '\\[\\text{PassRatio@N}=\\operatorname*{\\mathbb{E}}_{\\text{Problems}}\\left[\\frac{ c}{N}\\right], \\tag{2}\\text{E}}\n' +
      '\n' +
      '이것은 \\(N\\)에서 생성된 정답의 백분율을 측정합니다. 이 메트릭은 Pass@1과 다소 동일하지만 분산이 감소했습니다.\n' +
      '\n' +
      '이 두 가지 메트릭을 기반으로, 우리는 그림 1과 같이 GSM8K와 MATH 벤치마크 4에서 LLaMA-2 모델의 성능을 검토한다. 이 두 벤치마크에 대한 모델을 명령어 추적 설정에서 적응시키기 위해 제한된 양의 SFT 데이터(즉, 7.5K)로 훈련된 SFT 버전을 사용한다. Bai 등(2022); Ouyang 등(2022)에서 입증된 바와 같이, SFT 스테이지는 능력을 향상시키지 않는다(그리고 심지어 "정렬 세금"의 맥락에서 언급된 바와 같이, 감소를 초래할 수 있다). 따라서 SFT 버전을 사용하면 모델의 수학적 능력을 공정하게 평가할 수 있다.\n' +
      '\n' +
      '각주 4: Following Lightman et al. (2023), 우리는 실험 효율을 위해 MATH 벤치마크로부터 500개의 테스트 샘플들의 서브세트를 활용한다.\n' +
      '\n' +
      '먼저 두 벤치마크에서 LLaMA-2 7B 모델에 대한 Pass@256 메트릭이 GSM8K에서 97.7%, MATH에서 72.0%로 현저하게 높다는 것을 관찰한다. 이는 LLaMA-2 7B 모델이 수학적 문제를 해결하는 강력한 능력을 가지고 있음을 시사한다.\n' +
      '\n' +
      '그런 다음 PassRatio@256이 GSM8K에서 48.2%, MATH에서 7.9%로 Pass@256보다 현저히 낮다는 것을 알 수 있다. 이것은 대부분의 수학 문제에 대한 정답이 256개의 무작위 세대 내에 존재하지만, 우리가 "불안정한 문제"라고 지칭하는 현상인 정답이 일관되게 추출될 것이라는 보장이 없음을 시사한다.\n' +
      '\n' +
      '이하에서, 우리는 _instability issue_를 상당히 감소시키기 위한 간단한 접근법을 제시할 것이다.\n' +
      '\n' +
      '##3 합성수학 질문을 이용한 스케일링 SFT 데이터\n' +
      '\n' +
      '이 섹션에서는 먼저 제한된 실제 SFT 데이터를 확장하면 불안정 문제를 크게 완화할 수 있음을 보여준다. 또한 전체 사용 가능한 GSM8K 및 MATH 훈련 데이터를 사용할 때 정확도가 아직 안정되지 않았음을 관찰한다. 우리는 합성 수학 문제를 사용하여 SFT 데이터를 추가로 확장하는 것을 고려한다. 이를 위해 GPT-4 Turbo API를 이용한 합성 데이터 생성을 위한 간단한 방법을 소개한다. 합성 데이터는 실제 수학 문제만큼 효과적이라는 것을 증명한다. 결과적으로 합성 SFT 데이터를 GSM8K에서 960K, MATH에서 480K로 각각 과감하게 스케일링하여 거의 완벽한 스케일링 동작을 수행하고 최첨단 정확도에 도달했다.\n' +
      '\n' +
      'Real Math Questions를 이용한 스케일링 우리는 전체 GSM8K 및 MATH 트레이닝 세트에 걸쳐 실제 수학 질문들의 스케일링 거동을 조사하는 것으로 시작한다. 표 1에 표시된 바와 같이, 우리는 GSM8K에서 26.7%에서 50.2%로, MATH에서 4.2%에서 8.4%로 증가하는 일관된 정확도 향상을 관찰하며 포화 징후는 없다.\n' +
      '\n' +
      '합성 SFT 데이터 생성은 실제 데이터가 소진되었기 때문에 합성적으로 생성된 수학 문제를 사용하여 SFT 데이터를 더 확장하는 것을 고려한다.\n' +
      '\n' +
      'GPT-4 터보 API의 도움으로 간단한 3단계 접근법을 소개한다.\n' +
      '\n' +
      '* 단계 1. 새로운 수학 문제를 생성한다. GPT-4 Turbo API를 요청하여 기준 수학 문항을 출발점으로 하여 새로운 문항을 생성한다. 새 질문의 타당성을 높이기 위해 세 가지 규칙을 프롬프트에 통합한다. 첫째, 새 질문은 상식을 따라야 하며 둘째, 원래 질문과 독립적으로 해결할 수 있어야 하며 셋째, 응답 응답을 포함하지 않아야 한다. 또한, 다양한 타겟 데이터 세트에 맞춘 질문 및 답변에 대한 특정 포맷팅 요구 사항을 설정했습니다.\n' +
      '* 단계 2. 질문을 검증한다. 우리는 시도된 해결책을 통해 이를 검증하고 정제함으로써 생성된 질문의 품질을 더욱 향상시킨다. 해결 및 검증 단계를 단일 프롬프트로 통합함으로써 이 접근법이 다양한 벤치마크에 걸쳐 질문의 유효성을 일관되게 향상시킨다는 것을 발견했다.\n' +
      '* 3단계. CoT(Cain-of-thought) 답변을 생성한다. 우리는 GPT-4 Turbo에게 새롭게 생성된 각 질문에 대한 CoT(chain-of-thought) 답변 응답을 생성하도록 요청한다.\n' +
      '\n' +
      '자세한 프롬프트 디자인은 부록 A에 나와 있습니다.\n' +
      '\n' +
      '합성 SFT 데이터와 실제 데이터를 비교하여 합성적으로 생성된 수학 문제의 품질을 평가하기 위해 표 1에 자세히 설명된 대로 LLaMA-2 7B 모델을 사용하여 GSM8K 및 MATH 훈련 세트의 실제 질문에 대한 효과를 평가했으며 그 결과는 합성 수학 문제가 실제 문제만큼 효과적임을 나타낸다.\n' +
      '\n' +
      '또한 Xu et al. (2023); Yu et al. (2023); An et al. (2023). 이러한 방법은 그림 6과 같이 우리의 접근법보다 약간 적지만 효과적인 것으로 입증된다.\n' +
      '\n' +
      '약 백만 SFT 수학 데이터로의 확장 합성 접근법의 효율성을 고려하여 GSM8K 문제와 MATH 문제 모두에 대한 SFT 데이터의 규모를 각각 960K와 480K로 크게 늘렸다. 그림 1은 LLaMA-2 계열의 다양한 크기를 활용한 주요 결과를 제시하고 있다. 간단한 스케일링 전략은 최첨단 정확도를 제공합니다.\n' +
      '\n' +
      '정확도가 아직 정점에 이르지 못했다는 점도 주목할 필요가 있다. 추가적인 스케일링의 효과를 탐색하는 것은 우리의 향후 연구로 남겨질 것이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### 데이터세트 및 평가\n' +
      '\n' +
      '제안된 방법의 유효성을 평가하기 위해 5개의 벤치마크에 대한 실험을 수행한다.\n' +
      '\n' +
      '**GSM8K**Cobbe 등(2021). 이것은 수학 지식이 주로 초등학교 수준을 다루는 고품질의 언어학적으로 다양한 수학 데이터 세트이다. 교육예 7,473건, 시험예 1,319건을 포함하고 있다. 이 작업에서 우리는 새로운 합성 데이터를 생성하기 위해 주어진 질문으로 훈련 세트를 사용한다.\n' +
      '\n' +
      '**MATH**Hendrycks et al. (2021). 이 데이터 세트는 높은 수준의 추론 능력과 수학적 지식이 필요한 경쟁 수준의 수학 문제에 초점을 맞춘다. 7,500개의 훈련 사례와 5,000개의 테스트 사례로 구성되어 있습니다. 우리는 합성 데이터를 생성하기 위해 훈련 예제를 사용한다.\n' +
      '\n' +
      '**SVAMP**Patel et al. (2021). 이 데이터 세트는 초등 수준의 수학 문제를 포함한다. 우리는 모델의 교차 데이터 세트 성능을 평가하기 위해 1,000개의 테스트 케이스를 모두 활용합니다.\n' +
      '\n' +
      '**ASDiv**Miao et al.(2021) 이 데이터 세트에는 다양한 언어 패턴과 질문 유형이 있는 수학 문제 세트가 포함되어 있다. 우리는 2,305개의 문제의 테스트 세트를 평가 벤치마크로 채택한다.\n' +
      '\n' +
      '**헝가리 국립 고등학교 시험** 이 평가 벤치마크는 수학 모델의 영역 외 능력을 평가하기 위해 설계된 Grok-1(xAI, 2023)에 의해 처음 도입된다. 그것은 33개의 도전적인 문제들로 구성되어 있다.\n' +
      '\n' +
      '헝가리 국립고 시험 데이터 세트의 최종 답변은 사람에 의해 주석이 달린 반면, 다른 벤치마크는 이전 작품 Luo et al.(2023); Gou et al.(2023)과 유사한 자동 스크립트를 사용하여 라벨링된다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '데이터 합성에서는 GPT-4 Turbo API를 활용하여 질문과 답변 생성 모두에 대해 온도를 1.0으로 설정한다.\n' +
      '\n' +
      '감독 미세 조정을 위해 총 3단계의 훈련에 걸쳐 코사인 학습 속도 스케줄이 있는 Adam 최적화기를 사용한다. 최대 학습률은 2e-5(미스트랄-7b 모델의 경우 2e-6 제외)로 설정되며 4% 선형 워밍업이 있다. 최대 토큰 길이는 2048로 설정되며, Vicuna-v1.1 Zheng et al.(2023) 시스템 프롬프트가 사용된다. 모든 실험은 8\\(\\times\\)Nvidia H100 GPU에서 수행되었다. 70B 모델과 960K 데이터 포인트를 포함하는 가장 자원 집약적인 실험은 1900H100 GPU 시간이 소요됩니다.\n' +
      '\n' +
      '평가를 위해 SFT에서 사용된 것과 동일한 프롬프트를 사용하고 최대 시퀀스 길이를 2048로 설정하며, vLLM Kwon et al.(2023)을 답안 생성에 사용한다.\n' +
      '\n' +
      '### 주요 결과 및 최신 모델과의 비교\n' +
      '\n' +
      '이 비교에서 우리는 도메인 내 벤치마크, GSM8K/MATH 및 헝가리 국립 고등학교 시험과 같은 도메인 외 벤치마크를 모두 조사한다. 각 벤치마크의 도메인 내 평가를 위해 각 훈련 샘플에서 합성된 데이터를 활용한다. GSM8K의 경우 960K 합성 데이터가 사용되는 반면 MATH의 경우 480K 합성 데이터가 사용된다. 외부 도메인 평가를 위해 GSM8K, MATH 또는 두 합성 세트의 혼합을 사용하여 훈련된 모델을 테스트한다.\n' +
      '\n' +
      '기본 모델의 경우 제안된 접근법의 일반성을 평가하기 위해 공통 언어 모델, 즉 LLaMA-2 7B/13B/70B/Mistral-7B와 Llemma-7B와 같은 수학 특정 모델을 모두 고려한다.\n' +
      '\n' +
      '도메인 내 결과 표 2는 제안된 접근법과 최첨단 오픈 소스 및 폐쇄 소스 모델을 비교한 것이다. 모든 기본 모델들에 걸쳐, 우리의 방법은 동일한 사전 훈련된 기본 모델을 사용하는 이전의 최상의 접근법들보다 상당히 능가한다.\n' +
      '\n' +
      'LLaMA-2-7B 상에서, 우리의 접근법은 GSM8K(MuggleMath-7B(Li et al., 2023)) 상에서 절대적으로 +14.2만큼, 그리고 MATH(MetaMath-7B(Yu et al., 2023)) 상에서 각각 +20.8만큼 이전 최고를 초과한다. 그것은 심지어 WizardMath-70B (Luo et al., 2023) (82.6 대 81.6 on GSM8K)와 같은 수학 능력들을 전용하는 몇몇 최신 70B 모델들을 능가한다. LLaMA-2-13B 상에서, 개선들은 GSM8K(MuggleMath-13B(Li et al., 2023)) 상에서 +14.1이고 MATH(MetaMath-13B(Yu et al., 2023)) 상에서 +22.5이다. LLaMA-2-70B에서 이득은 GSM8K(LEMA-LLaMA-2-70B(An et al., 2023))에서 +7.1이고 MATH(MetaMath-70B(Yu et al., 2023))에서 +26.2이다.\n' +
      '\n' +
      '더 강한 공통 언어 모델, 즉 Mistral-7B 상에서, 개선은 GSM8K 상에서 +6.0이고 MATH 상에서 +10.7이다(WizardMath-7B-v1.1(Luo et al., 2023))에 비해 각각 개선된다.\n' +
      '\n' +
      'Llemma-7B와 같은 수학-특정 기본 모델에서 이득은 GSM8K에서 +15.0이고 MATH에서 +17.2이다(MetaMath-Llemma-7B(Luo et al., 2023))와 비교된다.\n' +
      '\n' +
      '또한 우리의 LLaMA-2-70B 모델이 GSM8K 및 MATH에서 초기 버전의 GPT-4와 경쟁 정확도를 달성한다는 점도 주목할 만하다. 알고 있는 범위 내에서는, 이것은 MATH에서 GPT-4-0314를 능가하는 최초의 LLaMA 기반 모델이다.\n' +
      '\n' +
      '이러한 결과는 합성 수학 SFT 데이터의 스케일링의 상당한 효과와 광범위한 적용 가능성을 보여준다.\n' +
      '\n' +
      '도메인 외 결과(xAI, 2023)의 실습에 따라 도메인 외 벤치마크인 헝가리 국립 고등학교 시험 테스트에서 GSM8K, MATH 또는 두 개의 합성 세트를 혼합하여 훈련된 모델을 테스트한다.\n' +
      '\n' +
      '결과를 표 3 에 나타낸다. 혼합 데이터(240K MATH 합성 데이터 + 240K GSM8K 합성 데이터)에 대해 훈련된 모델은 GPT-4 바로 뒤에 있고 다른 모델보다 훨씬 더 나은 두 번째로 순위가 매겨졌다. 또한, 부록 B에서 GSM8K와 헝가리 국가 고등학교 시험 점수 사이의 상관 관계를 도표화했는데, 그 결과는 우리 모델에서 유의미한 벤치마크 과적합이 없음을 보여준다.\n' +
      '\n' +
      '그림. 도 2(Left)는 모델의 결과를 제시한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & GSM8K & MATH \\\\ \\hline \\hline \\multicolumn{3}{c}{_Closed-source models_} \\\\ GPT-4 Turbo (1106) & 94.8 & 64.5 \\\\ GPT-4-0314 & 94.7 & 52.6 \\\\ GPT-4 (Achiam et al., 2023) & 92.0 & 42.5 \\\\ Claude-2 (Anthropic, 2023) & 88.0 & - \\\\ GPT-3.5-Turbo (OpenAI, 2023a) & 80.8 & 34.1 \\\\ \\hline \\multicolumn{3}{c}{_Open-source models LLaMA-2-7B_} \\\\ WizardMath-7B (Luo et al., 2023) & 54.9 & 10.7 \\\\ MuggleMath-7B (Li et al., 2023) & 68.4 & - \\\\ MetaMath-7B (Yu et al., 2023) & 66.5 & 19.8 \\\\ LEMA-LLaMA-2-7B (An et al., 2023) & 54.1 & 9.4 \\\\ \\hline \\multicolumn{3}{c}{_Open-source models Mistral-7B_} \\\\ WizardMath-7B-v1.1 (Luo et al., 2023) & 83.2 & 33.0 \\\\ MetaMath-Mistral-7B (Yu et al., 2023) & 77.4 & 28.2 \\\\ \\hline \\multicolumn{3}{c}{_Open-source models Lema-7B_} \\\\ MetaMath-Llemma-7B (Yu et al., 2023) & 69.2 & 30.0 \\\\ \\hline \\multicolumn{3}{c}{_Open-source models LLaMA-2-13B_} \\\\ WizardMath-13B (Luo et al., 2023) & 63.9 & 14.0 \\\\ MuggleMath-13B (Li et al., 2023) & 74.0 & - \\\\ MetaMath-13B (Yu et al., 2023) & 72.3 & 22.4 \\\\ LEMA-LLaMA-2-13B (An et al., 2023) & 65.7 & 12.6 \\\\ \\hline \\multicolumn{3}{c}{_Open-source models LLaMA-2-70B_} \\\\ WizardMath-70B (Luo et al., 2023) & 81.6 & 22.7 \\\\ MuggleMath-70B (Li et al., 2023) & 82.3 & - \\\\ MetaMath-70B (Yu et al., 2023) & 82.3 & 26.6 \\\\ LEMA-LLaMA-2-70B (An et al., 2023) & 83.5 & 25.0 \\\\ \\hline \\multicolumn{3}{c}{_Xuin-Math-70B (ours)_} & **90.6** & **52.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 다양한 LLM의 수학 추론 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline Model & Test Score (\\%) \\\\ \\hline GPT-4 (Achiam et al., 2023) & 68 \\\\ Grok-1 (xAI, 2023) & 59 \\\\ Claude-2 (Anthropic, 2023) & 55 \\\\ GPT-3.5 Turbo (OpenAI, 2023a) & 41 \\\\ DeepSeek-LLM-67B-Chat (Bi et al., 2024) & 58 \\\\ \\hline Xwin-Math-70B (480K GSM8K) & 22 \\\\ Xwin-Math-70B (120K MATH) & 51 \\\\ Xwin-Math-70B (480K MATH) & 59 \\\\ Xwin-Math-70B (480K Mix) & 65 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다양한 LLM의 헝가리 전국 고등학교 시험 결과.\n' +
      '\n' +
      'GSM8K 합성 데이터로 훈련된 반면 그림. 2(Middle)는 MATH로 학습된 모델의 결과를 제시하며, GSM8K 또는 MATH 합성 데이터로 학습된 모델의 데이터 양이 증가함에 따라 다른 벤치마크의 정확도도 향상됨을 알 수 있으며, GSM8K 및 MATH 모델에 대해 일반화 행동이 다르다는 것을 알 수 있다: 1) SVAMP와 ASDiv는 MATH 모델보다 GSM8K 모델에서 더 많은 이점을 얻는다. 2) GSM8K 벤치마크에서 MATH 모델이 상대적으로 좋은 성능을 보이는 반면, GSM8K 모델은 MATH 벤치마크에서 상당히 더 나쁜 성능을 보인다.\n' +
      '\n' +
      '그림. 도 2의 (Right)는 GSM8K와 MATH를 1:1 비율로 혼합한 모델을 사용한 결과를 나타낸다. 이러한 모델은 도메인 내 및 도메인 외 벤치마크 모두에서 균형 잡힌 스케일링 동작을 나타낸다.\n' +
      '\n' +
      '성능 개선 이면에 무슨 일이 일어나는가?\n' +
      '\n' +
      '**Pass@256 _v.s._ PassRatio@256 성능 개선 이면의 이해를 심화시키기 위해 서로 다른 데이터 크기 하에서 Pass@N 메트릭과 PassRatio@N 메트릭을 추적하였다. 결과는 그림 3과 같다. 매우 제한된 합성 데이터(_e.g._7.5K 샘플)로 Xwin-Math-70B 모델은 이미 Pass@256이 매우 높아 여러 번의 시도를 통해 정답을 생성하는 능력이 강함을 알 수 있다. 한편 Pass@256 메트릭은 사용 데이터의 양이 증가함에 따라 약간만 변경되었다. 이와는 대조적으로, 정답을 생성하기 위한 안정성을 반영하는 PassRatio@256은 합성 데이터의 양에 따라 크게 증가하며, 그 성장 추세는 Pass@1과 유사하다. 이러한 결과는 성능 향상이 주로 질문에 대한 더 강한 답변 능력보다는 답변 생성의 더 나은 안정성에 기인한다는 가설을 확인시켜준다.\n' +
      '\n' +
      '**추정된 단일 단계 추론 정확성** 추론에 CoT(Chain-of-Thought)가 채택되기 때문에, 수학적 문제에 답하는 과정은 다단계 추론 과정에 의해 완료된다. 따라서, 최종 답변 정확도의 증가는 단일 단계 추론 정확도의 향상으로 해석될 수 있다고 가정한다. 이 가정에 기초하여, CoT의 \\(s\\) 추론 단계에 의해 하나의 질문이 이론적으로 답변될 수 있다면, 최종 답변 정확도는 단일 단계 추론 정확도의 전력 함수에 의해 근사적일 수 있다:\n' +
      '\n' +
      '\\[\\text{Acc}_{\\text{final}}=\\text{Acc}_{\\text{step}}^{s} \\tag{3}\\]\n' +
      '\n' +
      '이 방정식으로 최종 답안 정확도로부터 단계 정확도를 추정할 수 있다. GSM8K를 실험했습니다. 검정의 각 문제에 대해\n' +
      '\n' +
      '그림 3: GSM8K 및 MATH 벤치마크에서 데이터 크기가 증가하는 Pass@256 및 PassRatio@256 곡선.\n' +
      '\n' +
      '그림 2: 단일 데이터 세트 또는 혼합 데이터 세트를 사용하여 SFT 데이터 척도의 증가를 비교한다.\n' +
      '\n' +
      '셋, 우리는 256개의 응답을 생성하고 이론적인 CoT 단계로 GSM8k 테스트 세트의 CoT 주석의 단계 수를 사용했다. CoT 추론 단계의 수와 평균 최종 답변 정확도 사이의 관계를 보여주기 위해 곡선을 그리고 방정식을 기반으로 적합 곡선을 보여준다. 3. 서로 다른 합성 데이터를 사용하여 Xwin-Math-7B 모델을 테스트하고 결과를 그림 4에 나타낸다. 실선은 7개의 점 모두를 사용하여 적합되며 표 4는 모든 데이터 점을 사용하여 서로 다른 양의 데이터를 사용할 때 추정된 단일 단계 정확도를 나타내며 더 많은 데이터로 단일 단계 정확도가 크게 향상됨을 알 수 있다.\n' +
      '\n' +
      '그러나, 우리가 식에 기초하여 적합할 때. 3에서 처음 4개의 점, 점선에 표시된 것처럼 후자의 3개의 점이 곡선 아래에 유의하게 있음을 발견했다. 우리는 이 현상이 훈련 데이터에서 더 복잡한 문제의 더 작은 비율과 관련이 있을 수 있다고 믿는다. 따라서, 우리는 CoT 솔루션의 문장 수에 따라 960K 합성 데이터를 재샘플링했다. 도 4(오른쪽)에서 알 수 있는 바와 같이, 복잡한 문제들의 비율이 증가할 때, 더 간단한 문제들에 대한 정확도는 사실상 변하지 않지만, 더 복잡한 문제들에 대한 정확도는 상당히 개선될 수 있다. 또한 데이터 리샘플링의 활용은 모델의 PassRatio@256을 71.1에서 72.8로 증가시킬 수 있으며, 이 실험 결과는 수학적 추론 작업에 대한 데이터 선택에 대한 새로운 통찰력을 제공한다.\n' +
      '\n' +
      '또한 GPT-4 Turbo를 사용하여 우리 답안의 첫 번째 단계가 틀린 위치를 찾고 그 위치를 각 답안의 총 단계 수로 정규화했다. 추정된 단일 단계 정확도가 높아짐에 따라 정규화의 첫 번째 오류 위치는 연기된다.\n' +
      '\n' +
      '**수치 계산의 정확도 향상은 논리 추론**보다 더 중요하다. 모델의 성능은 합성 데이터가 증가함에 따라 점진적으로 향상된다. 더 깊은 이해를 위해 GSM8K에 대한 다양한 유형의 오류에 대한 오류 비율을 분석한다. 우리는 오류를 추론 오류와 계산 오류 두 가지 유형으로 분류했다. 추론 오류는 주로 조건의 손실 및 개념 혼란과 같은 문제를 포괄하는 반면 계산 오류는 정량적 관계에 대한 잘못된 분석 및 수치 계산 오류를 포함한다. 그림 5에 예시된 실험 결과를 바탕으로 계산 오차의 백분율이 점진적으로 감소하는 것을 관찰하여 GSM8K가 추론 오차보다 빠른 속도로 계산 오차를 보정하고 있음을 알 수 있다.\n' +
      '\n' +
      '다른 데이터 합성 방법과 데이터 합성 스키마 비교에 대한### 수정\n' +
      '\n' +
      '우리는 우리의 접근 방식을 일반적인 데이터 합성 방법과 비교했다.\n' +
      '\n' +
      '_Add Constraint.__Add Constraint. 위저드마쓰와 머글마쓰에서 사용되는 다른 질문을 변경하지 않고 그대로 유지하면서 원래 질문에 하나를 더 추가하면 제한한다.\n' +
      '\n' +
      'Numbers.__Change Numbers 문맥을 그대로 유지하면서 문제에 나타나는 숫자를 변경합니다. 그것은 머글수학에서 사용된다.\n' +
      '\n' +
      '배경 변경____ 배경 변경 에서 배경 변경\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline Data size & Estimated Acc\\({}_{\\text{sup}}\\) & Normalized first error position \\\\ \\hline\n' +
      '7.5K & 78.9 & 67.1\\\\\n' +
      '120K & 89.7 & 83.9\\\\\n' +
      '960K & 94.2 & 90.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: GSM8K 벤치마크 상의 Xwin-Math-7B에서 GPT-4 Turbo에 의한 추정된 단일 단계 추론 정확도와 평균 정규화된 첫 번째 오류 위치.\n' +
      '\n' +
      '그림 4: 왼쪽: GSM8K에 대한 평균 정확도와 데이터가 증가함에 따라 주석이 달린 CoT 단계의 수 사이의 관계. 실선은 7개의 점을 모두 사용하여 적합되고 점선은 처음 4개의 점을 사용하여 적합됩니다. Right: 훈련 데이터의 CoT lengeh를 증가시키기 위해 리샘플링을 사용할 때 평균 정확도의 변화.\n' +
      '\n' +
      '그림 5: 자료 중 계산과 추론 오류의 비중 변화가 증가하였다.\n' +
      '\n' +
      '다른 사람들은 그대로 유지하면서 질문.\n' +
      '\n' +
      '_Change Number and Background.__The Combination of change number and Background.__ 숫자와 배경을 모두 바꾸는 하이브리드 접근법입니다.\n' +
      '\n' +
      '___MetaMath Approach.____MetaMath Approach. MetaMath에서 제안된 합성 방법은 답변 증강, 재구문, 자기 검증 질문 및 FOBAR 질문을 포함한다. 실험에서는 MetaMath의 구현을 따르지만, GPT-3.5 Turbo 대신 GPT-4 Turbo를 사용하여 공개된 질문을 사용하여 응답 데이터를 생성한다.\n' +
      '\n' +
      '그림 6의 실험 결과는 데이터 크기가 _e.g._, 7.5k 및 30k 샘플로 상대적으로 작을 때 서로 다른 방법 간의 성능 차이는 무시할 수 있음을 보여준다. 그러나 데이터 크기가 증가함에 따라 본 논문에서 제안한 방법과 제약조건을 추가한 방법이 더 강한 성능을 보인다. 이는 데이터 크기가 증가함에 따라 데이터 합성 전략의 선택이 더 중요해지고 일부 방법이 데이터를 더 효율적으로 스케일링하여 성능을 향상시킬 수 있음을 시사한다.\n' +
      '\n' +
      '**질문 검증의 효과**질문 검증은 생성 품질을 더욱 향상시키기 위해 사용된다. 실험에서 MATH 벤치마크에서 성능을 향상시킬 수 있음을 발견했으며 결과는 표 5에 나와 있지만 GSM8K 데이터 세트에 큰 영향을 미치지 않는다.\n' +
      '\n' +
      '##5 관련 작품\n' +
      '\n' +
      'Large Language ModelsLarge language modelsLarge language models Brown et al. (2020); Achiam et al. (2023); Touvron et al. (2023); Tuvron et al. (2023)은 광범위한 태스크들에서 인상적인 성능으로 상당한 업적을 달성하였다. 현재 GPT Brown et al. (2020); Achiam et al. (2023), Gemini Team et al. (2023), Groke (xAI, 2023) 및 Claude-2 (Anthropic, 2023)로 대표되는 폐쇄 소스 대형 언어 모델은 성능 면에서 가장 진보된 모델이다. 그러나, LLaMA Touvron et al. (2023), LLaMA-2 Touvron et al. (2023) 및 Mixtral Jiang et al. (2024)로 대표되는 오픈 소스 모델도 빠르게 진행되었으며, 일부 태스크에서 클로즈드 소스 모델들과 경쟁적인 성능을 보여주었다. 본 연구는 오픈소스 LLM을 합성 데이터로 미세 조정함으로써 수학적 작업에 대한 성능을 향상시키는 것을 목표로 한다.\n' +
      '\n' +
      'Reasoning Framework for Improving Mathematical CapabilityChain-of-thoughts Wei et al. (2022)은 LLM들이 특정한 설계 프롬프트에 의해 다단계 추론을 수행하도록 장려하고 추론 성능을 향상시킬 수 있다. 이 작품을 바탕으로 Fu et al. (2022); Zhang et al. (2022); Kojima et al. (2022). 위의 작업은 모델을 미세 조정하지 않고 보다 신속한 설계 또는 추론 전략을 통해 성능을 개선하는 방법에 중점을 두는 반면, 우리의 작업은 모델 자체를 개선하는 방법에 중점을 두고 있으므로 이러한 접근 방식은 우리의 것과 보완적이다.\n' +
      '\n' +
      'MathAnother sort of work Lightman et al. (2023); Luo et al. (2023); Azerbayev et al. (2023); Yue et al. (2023); Yu et al. (2023); An et al. (2023); Li et al. (2023); Gou et al. (2023)은 수학적 데이터에 대한 모델을 트레이닝함으로써 직접 성능을 개선하려고 노력한다. 직접적인 방법은 미세 조정을 사용하여 모델을 개선하는 것이다. 널리 사용되는 방법 중 하나는 합성 데이터를 사용하는 것인데, 이는 우리의 접근법에 매우 가깝다: MetaMath Yu et al.(2023)은 데이터를 증강하기 위해 질문들을 부트스트랩하기 위해 제시한다. LeMA An et al.(2023)은 GPT-4를 보정기로 사용하여 오차 보정 데이터 쌍을 수집한다. 그리고 MuggleMath Li et al.(2023)은 GPT-4를 일련의 미리 정의된 동작들과 통합함으로써 GSM8K 데이터세트를 증강한다. 이러한 합성 데이터 기반 노력에 비해 우리의 데이터 합성 방법은 이전 및 제약이 적기 때문에 훨씬 간단하고 확장 가능하다.\n' +
      '\n' +
      '최근 SFT 데이터 스케일링은 지도 미세 조정을 위한 데이터 스케일에 초점을 맞추고 있다. 예를 들어, LIMA Zhou et al.(2023)은 1,000개의 고품질로 미세 조정하는 것을 언급한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c} \\hline \\hline Model & Pass@1 (\\%) \\\\ \\hline Xwin-Math-70B (7.5K data) & 28.9 \\\\ Xwin-Math-70B (7.5K data) w/o verification & 28.1 (-0.8) \\\\ Xwin-Math-70B (30K data) & 37.6 \\\\ Xwin-Math-70B (30K data) w/o verification & 36.6 (-1.0) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MATH에 대한 질문 검증의 제거.\n' +
      '\n' +
      '그림 6: GSM8K 및 다양한 합성 방법의 MATH 성능.\n' +
      '\n' +
      '지시는 다양한 일반적인 작업에서 인상적인 결과를 가져올 수 있습니다. 다른 연구에서는 수학적 및 코딩 작업 동 등(2023)에서 데이터 크기를 갖는 성능 척도가 나타났다. 최근 연구 Bi 등(2024)은 상위 성능을 얻기 위해 미세 조정을 지시하기 위해 150만 데이터까지 사용한다. 그러나 이러한 스케일링 효과의 본질적인 이유는 철저히 조사되지 않았다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 연구는 LLaMA-2 7B와 같은 일반적인 7B 언어 모델이 이미 강력한 수학적 능력을 보여 고급 수학적 추론이 더 크고 광범위하게 사전 훈련된 모델에만 배타적이라는 이전의 믿음에 도전한다는 것을 보여준다. SFT 데이터를 크게 확장함으로써 모델의 수학적 문제 해결 능력의 안정성을 현저하게 향상시켰다. 우리의 방법론은 Xwin-Math 모델이 더 큰 모델의 성능과 비슷하고 어떤 경우에는 능가하는 성능 수준에 도달할 수 있도록 했다. 우리의 분석은 또한 향상이 주로 단일 단계 추론에서 향상된 정확도에 기인하며 훈련 데이터의 추가 리샘플링이 더 어려운 질문의 정확도를 향상시킬 수 있음을 나타낸다. 또한, 논리적 추론 오류보다 계산 오류를 더 크게 줄일 수 있다. 우리의 연구는 대규모 언어 모델의 수학적 능력에 대한 귀중한 통찰력을 제공한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '첸 리와 난닝 정은 NSFC의 보조금 제62088102호에 의해 부분적으로 지원되었다. 이 작품에 대한 그의 귀중한 조언에 대해 시안 자오통 대학교 IAIR의 쉔난 안에게 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 기술보고서; _ arXiv preprint arXiv:2303.08774_.\n' +
      '* An et al. (2023) Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. 2023. 실수로부터 배우는 것은 llm을 더 나은 추론자로 만든다. _ arXiv preprint arXiv:2310.20689_.\n' +
      '*인체(2023)인체. 2023. 모델 카드 및 클로드 모델에 대한 평가.\n' +
      '* Azerbayev et al. (2023) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: 수학을 위한 개방형 언어 모델. _ arXiv preprint arXiv:2310.10631_.\n' +
      '* Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan. 2022. 인간 피드백으로부터 강화 학습으로 도움되고 무해한 어시스턴트를 훈련시킨다.\n' +
      '* Bi et al. (2024) Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. _ arXiv preprint arXiv:2401.02954_.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 소수의 학습자를 의미한다. _ 신경 정보 처리 시스템들_, 33:1877-1901의 진보들.\n' +
      '* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. 수학 단어 문제를 해결하기 위한 훈련 검증자 __ arXiv preprint arXiv:2110.14168_.\n' +
      '* Dong et al. (2023) Guanting Dong, Hongyi Yuan, Keming Lu, Cheng-peng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, 및 Jingren Zhou. 2023. 대규모 언어 모델에서의 능력이 지도된 미세 조정 데이터 구성에 의해 영향을 받는 방법. _ arXiv preprint arXiv:2310.05492_.\n' +
      '* Fu et al. (2022) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. 다단계 추론을 위한 복잡도 기반 프롬프트. _ arXiv preprint arXiv:2210.00720_.\n' +
      '* Fu et al. (2020)Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. 2023. Tora: tool-integrated reasoning agent for mathematical problem solving. _ arXiv preprint arXiv:2309.17452_.\n' +
      '* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. 수학 데이터셋으로 수학 문제 풀이를 측정하는 단계. _ arXiv preprint arXiv:2103.03874_.\n' +
      '* Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand et al. 2024. Mixtral of experts. _ arXiv preprint arXiv:2401.04088_.\n' +
      '* Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. 대형 언어 모델들은 제로 샷 추론기들이다. _ 신경 정보 처리 시스템들_, 35:22199-22213의 진보들.\n' +
      '* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. 페이지 어텐션으로 서빙하는 대형 언어 모델에 대한 효율적인 메모리 관리. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_.\n' +
      '* Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. 언어 모델들로 정량적 추론 문제들을 해결하는 단계.\n' +
      '* Li et al. (2023) Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2023. 질의 및 응답 증강은 영역 외 수학 추론 일반화를 도울 수 없다. _ arXiv preprint arXiv:2310.05506_.\n' +
      '* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. 단계별로 검증하자. _ arXiv preprint arXiv:2305.20050_.\n' +
      '* Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empower mathematical reasoning for large language models via reinforced evol-instruct. _ arXiv preprint arXiv:2308.09583_.\n' +
      '* Miao et al. (2021) Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. 영어 수학 단어 문제 해결기의 평가 및 개발을 위한 다양한 말뭉치. _ arXiv preprint arXiv:2106.15772_.\n' +
      '* OpenAI(2023a) OpenAI. 2023a. Gpt-3.5 터보 미세 조정 및 api 업데이트.\n' +
      '* OpenAI(2023b) OpenAI. 2023b. GPT-4 기술 보고서입니다 CoRR_, abs/2303.08774.\n' +
      '* Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. 2022. 인간의 피드백으로 지시를 따르도록 언어 모델을 트레이닝한다.\n' +
      '* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. 2021. nlp 모델들이 정말 간단한 수학 단어 문제를 해결할 수 있는가? _ arXiv preprint arXiv:2103.07191_.\n' +
      '* Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. 우, 다야 궈 2024. Deepseekmath: 개방 언어 모델에서 수학적 추론의 한계를 밀어내는 것.\n' +
      '* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: High capable multiimodal model의 가족. _ arXiv preprint arXiv:2312.11805_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. 개방적이고 효율적인 기초 언어 모델 arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b)Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ arXiv preprint arXiv:2307.09288_.\n' +
      '* Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. 대형 언어 모델의 새로운 능력.\n' +
      '* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. 생각 연결 프롬프트는 큰 언어 모델에서 추론을 이끌어냅니다. _ 신경 정보 처리 시스템_, 35:24824-24837의 발전.\n' +
      '* AAI(2023) xAI. 2023. Grok-1.\n' +
      '*Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. 마법사: 복잡한 명령어를 따르도록 큰 언어 모델을 엠파워링하는 단계 _ arXiv preprint arXiv:2304.12244_.\n' +
      '* Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. 메타매스: 큰 언어 모델에 대해 자신의 수학적 질문을 부트스트랩한다. _ arXiv preprint arXiv:2309.12284_.\n' +
      '* Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. 매머드: 하이브리드 명령어 튜닝을 통한 수학 일반 모델 구축 arXiv preprint arXiv:2309.05653_.\n' +
      '* Zhang et al. (2022) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. 대규모 언어 모델에서의 사고 프롬프트의 자동 체인. _ arXiv preprint arXiv:2210.03493_.\n' +
      '* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. 2023년, mtbench와 챗봇 아레나로 판결합니다\n' +
      '* Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Lessly is more for alignment. _ arXiv preprint arXiv:2305.11206_.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n' +
      'Additional Results\n' +
      '\n' +
      '그림 7: 이 두 벤치마크에 대한 Xwin-Math의 집계 성능은 GPT-4에 이어 두 번째이며, 이는 우리 모델의 강력한 일반화 능력을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c} \\hline \\hline Dataset & \\(L_{\\text{test-regen}}\\) & \\(L_{\\text{test-ref}}\\) & \\(L_{\\text{train}}\\) & \\(L_{\\text{train-regen}}\\) & \\(\\Delta_{1}\\) & \\(\\Delta_{2}\\) \\\\ \\hline GSM8K & 0.52 & 0.50 & 0.11 & 0.33 & 0.02 & 0.19 \\\\ MATH & 0.59 & 0.58 & 0.23 & 0.39 & 0.01 & 0.20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 데이터 생성 중 벤치마크 데이터 유출을 검증하기 위해 LM 손실을 비교한다: 1) 모든 합성 데이터에서 256개의 샘플이 있는 작은 하위 집합인 학습 하위 집합; 2) 재생성된 학습 하위 집합에서 원래 질문을 유지하고 GPT-4 Turbo를 사용하여 답변을 재작성하는 단계; 3) 평가 메트릭에서 질문을 변경하지 않고 GPT-4 Turbo를 사용하여 답변을 재작성하는 재생성 테스트 세트; 4) 참조 테스트 세트를 시드로 사용하여 GPT-4 Turbo를 통해 새로운 질문과 답변을 생성하는 단계. 또한 Skywork를 참조하면, 두 가지 주요 메트릭인 \\(\\Delta_{1}=L_{\\text{test-regen}-L_{\\text{test-ref},\\Delta_{2}=L_{\\text{test- regen}-L_{\\text{train-regen}}, As\\(\\Delta_{1}\\)은 0에 가깝고, \\(\\Delta_{2}\\)은 0보다 크게 나타나 데이터 합성 과정에서 누출이 없는 것으로 판단된다.\n' +
      '\n' +
      '합성자료의 사례연구\n' +
      '\n' +
      '### GSM8K\n' +
      '\n' +
      'Original Question\n' +
      '\n' +
      '랜디는 그의 농장에 60그루의 망고나무를 가지고 있다. 그는 또한 망고 나무보다 5개 미만의 코코넛 나무를 가지고 있습니다. 랜디는 농장에 나무가 몇 그루나 있죠?\n' +
      '\n' +
      '충분한 질문 w/o 검증\n' +
      '\n' +
      '마리아는 과수원에 사과나무 80그루를 가지고 있다. 그녀는 또한 사과나무보다 3분의 1 이하의 벚나무를 가지고 있다. 마리아는 과수원에 나무가 몇 그루나 있죠?\n' +
      '\n' +
      '충분한 질문 w/o 검증\n' +
      '\n' +
      '마리아는 사과나무가 80그루야\n' +
      '\n' +
      '사과나무보다 체리나무가 3분의 1도 안 돼 사과나무의 3분의 1은 80/3이며, 이는 약 26.67이다.\n' +
      '\n' +
      '그보다 4는 약 22.67이지만 나무의 일부를 가질 수 없기 때문에 가장 가까운 정수인 27로 반올림한다.\n' +
      '\n' +
      '- 수정된 문항은 십진법 없이 벚나무의 수를 물어봐야 한다. 그래서 우리는 그 결과로 정수를 갖도록 문제를 조정한다.\n' +
      '\n' +
      '- 80은 3으로 나눌 수 없기 때문에 벚나무의 정수를 얻기 위해 3으로 나눌 수 있도록 수를 바꿀 수 있다. 80이 아닌 81개의 사과나무를 사용하자.\n' +
      '\n' +
      '확인을 통한 충분한 질문\n' +
      '\n' +
      '마리아는 과수원에 81그루의 사과나무를 가지고 있다. 그녀는 또한 사과나무보다 3분의 1 이하의 벚나무를 가지고 있다. 마리아는 과수원에 나무가 몇 그루나 있죠?\n' +
      '\n' +
      '충분한 해결책과 최종 정답\n' +
      '\n' +
      '먼저 마리아가 가지고 있는 벚나무의 수를 정하자. 우리는 그녀가 사과나무보다 3분의 1 이하의 벚나무를 가지고 있다고 들었다.\n' +
      '\n' +
      '마리아는 사과나무가 81그루니까 그 숫자의 3분의 1을 찾아보자\n' +
      '\n' +
      '81개의 사과나무 중 1/3 = 81개의 사과나무/3 = 27개의 사과나무.\n' +
      '\n' +
      '이제, 그녀가 벚나무에 이 양보다 4개가 적기 때문에, 우리는 벚나무의 수를 찾기 위해 27에서 4개를 뺍니다:\n' +
      '\n' +
      '27개의 벚나무 - 4 = 23개의 벚나무.\n' +
      '\n' +
      '마리아가 과수원에 가지고 있는 나무의 총 수를 찾기 위해, 우리는 벚나무의 수에 사과나무의 수를 더한다:\n' +
      '\n' +
      '81개의 사과나무 + 23개의 벚나무 = 104개의 나무.\n' +
      '\n' +
      '답은 [104]이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '모형반응 사례연구\n' +
      '\n' +
      '### GSM8K\n' +
      '\n' +
      '질문과 진실솔루션\n' +
      '\n' +
      '보물 사냥꾼이 보석으로 가득 찬 보물 상자를 발견했어 다이아몬드는 175개였고, 다이아몬드보다 루비가 35개 적었고, 루비보다 에메랄드 수가 두 배나 많았다. 가슴에 보석이 몇 개 있었어?\n' +
      '\n' +
      '**해결책:** 보물상자에 175 - 35 = 140루비가 있었다. 보물 상자에는 140 x 2 = 280 에메랄드가 있었다. 그래서 보물상자에 있는 보석의 총 개수는 175+140+280=[595]이다.\n' +
      '\n' +
      '**Response - Xothing-Xothing-75 (755 (400))** 그레이 데스\n' +
      '\n' +
      '먼저 루비의 수를 알아봅시다. 우리는 다이아몬드보다 루비가 35개 더 적다고 들었다. 다이아몬드가 175개이므로 루비의 수를 다음과 같이 계산할 수 있다.\n' +
      '\n' +
      '루비의 수 = 다이아몬드 수 - 35 = 175 - 35 = 140\n' +
      '\n' +
      '다음으로, 우리는 에메랄드의 수를 알아내야 한다. 우리는 루비보다 에메랄드의 수가 두 배라고 들었다. 그래서 우리는 다음과 같이 에메랄드의 수를 계산할 수 있다:\n' +
      '\n' +
      '에메랄드의 수 = 2 x 루비의 수 = 2 x 140 = 280\n' +
      '\n' +
      '이제 우리는 다이아몬드, 루비, 에메랄드의 수를 함께 더하면 총 보석의 수를 찾을 수 있다.\n' +
      '\n' +
      '총 보석 수 = 다이아몬드 수 + 루비 수 + 에메랄드 수 = 175 + 140 + 280 = 605이다.\n' +
      '\n' +
      '**Response - Xothing - Xothing - 75 (755 (400))** 온도로부터의 한 샘플=0.7\n' +
      '\n' +
      '가슴에 있는 보석의 종류별 개수를 단계별로 찾아보자.\n' +
      '\n' +
      '먼저, 우리는 다이아몬드의 수를 받는다:\n' +
      '\n' +
      '다이아몬드 수 = 175\n' +
      '\n' +
      '다음으로, 우리는 다이아몬드보다 루비가 35개 더 적다고 들었다. 루비의 수를 찾기 위해 우리는 다이아몬드 수에서 35를 뺀다:\n' +
      '\n' +
      '루비의 수 = 다이아몬드 수 - 35 = 175 - 35 = 140\n' +
      '\n' +
      '그리고 나서, 우리는 루비보다 에메랄드의 수가 두 배라고 들었다. 에메랄드의 수를 찾기 위해 우리는 루비의 수에 2를 곱한다:\n' +
      '\n' +
      '에메랄드의 수 = 2 * 루비의 수 = 2 * 140 = 280\n' +
      '\n' +
      '이제 다이아몬드, 루비, 에메랄드의 수를 합쳐서 가슴에 있는 보석의 총 수를 찾아보자\n' +
      '\n' +
      '총 보석 수 = 다이아몬드 수 + 루비 수 + 에메랄드 수 = 175 + 140 + 280 = 595\n' +
      '\n' +
      '답은 다음과 같다[595]. 정확합니다\n' +
      '\n' +
      '**Response - Xothing - Xothing - 75 (755 (400))** 온도로부터의 한 샘플=0.7\n' +
      '\n' +
      '문제에서 주어진 정보를 단계별로 분해해 보자.\n' +
      '\n' +
      '1. 우리는 175개의 다이아몬드가 있다는 것을 안다.\n' +
      '\n' +
      '2. 다이아몬드보다 루비가 35개 적다. 다이아몬드가 175개이기 때문에 우리는 175개에서 35개를 빼서 루비의 수를 계산할 수 있다.\n' +
      '\n' +
      '루비의 수 = 다이아몬드 수 - 35 = 175 - 35 = 140\n' +
      '\n' +
      '3. 에메랄드 수는 루비보다 두 배 많다. 우리는 루비의 수에 2를 곱하면 에메랄드의 수를 구할 수 있다.\n' +
      '\n' +
      '에메랄드의 수 = 2 x 루비의 수 = 2 x 140 = 280\n' +
      '\n' +
      '이제, 가슴에 있는 보석의 총 수를 찾기 위해, 우리는 다이아몬드, 루비, 에메랄드의 수를 함께 추가합니다.\n' +
      '\n' +
      '총 보석 수 = 다이아몬드 수 + 루비 수 + 에메랄드 수 = 175 + 140 + 280 = 595\n' +
      '\n' +
      '답은 다음과 같다[595]. 정확합니다\n' +
      '\n' +
      '### Math\n' +
      '\n' +
      '**질문 및 근거 진실 해결**\n' +
      '\n' +
      '**질문:** \\(f(x)=\\frac{3x-2}{x-2}\\이면 \\(f(-2)+f(-1)+f(0)\\)의 값은 얼마인가? 답변을 공통 분수로 표현합니다.\n' +
      '\n' +
      '**Solution:**\\(f(-2)+f(-1)+f(0)=\\frac{3(-2)-2}{-2-2}+\\frac{3(-1)-2}{-1-2}+\\frac{3(0)-2}{0-2}= \\frac{-8}{-4}+\\frac{-5}{-3}+\\frac{-2}{-2}=2+\\frac{5}{3}+1=\\framebox{\\frac{14}{3}}\\)\n' +
      '\n' +
      '**Response-Xwithe-Yeth-75 (75-14) Greedy Decortic**\n' +
      '\n' +
      '우리는 \\(f(-2)+f(-1)+f(0)\\의 값을 구하기 위해 \\(x\\)의 세 가지 값에서 함수 \\(f(x)\\)을 평가해야 한다.\n' +
      '\n' +
      '먼저 \\(f(-2)\\): \\(f(x)=\\frac{3x-2}{x-2}\\) plug in \\(x=-2\\): \\(f(-2)=\\frac{3(-2)-2}{-2-2}=\\frac{-6-2}{-4}=\\frac{-8}{-4}\\)\n' +
      '\n' +
      '이제 \\(f(-1)\\): 플러그인 \\(x=-1\\): \\(f(-1)=\\frac{3(-1)-2}{-1-2}=\\frac{-3-2}{-3}=\\frac{-5}{-3}\\\n' +
      '\n' +
      '마지막으로 \\(f(0)\\): \\(x=0\\): \\(f(0)=\\frac{3(0)-2}{0-2}=\\frac{-2}{-2}=1\\을 구하자.\n' +
      '\n' +
      '이제 우리는 \\(f(-2)+f(-1)+f(0)=\\frac{-8}{-4}+\\frac{-5}{-3}+1\\)의 합을 구할 수 있다.\n' +
      '\n' +
      '이 분수를 더하기 위해서는 세 분모(-4, -2, -3)의 최소공배수(LCM)인 공통분모를 찾아야 한다. 이들 분모의 LCM은 -12이다.\n' +
      '\n' +
      '이제, 우리는 공통분모로 각 분수를 다시 쓸 수 있다.\n' +
      '\n' +
      '\\(\\frac{-8}{-4}=\\frac{4}{3}\\), \\(\\frac{-5}{-3}=\\frac{5}{3}\\), \\(1=\\frac{12}{12}\\)\n' +
      '\n' +
      '**Noneasonable**\n' +
      '\n' +
      '이제 우리는 \\(\\frac{4}{3}+\\frac{5}{3}+\\frac{12}{12}=\\frac{4+5+12}{3}=\\frac{19}{3}\\frac{5}{3}+\\frac{5}{3}+\\frac{12}=\\frac{4+5+12}{3}=\\frac{19}{3}\\frac{5}{3}+\\frac{5}{3}+\\frac{12}=\\frac{4+5+12}{3}=\\frac{19}{3}\\frac{5}{3}+\\frac{5}{3}+\\frac{12}=\\frac{4+5+12}{3}=\\frac{5}{3}+\\frac{5}{3}+\\frac{5}{3}+\\frac{5}{3}+\\frac{5}{3}+\\frac{5}{\n' +
      '\n' +
      '답은 "\\(\\framebox{\\frac{19}{3}}\\)**Incorrect**\n' +
      '\n' +
      '**Wrong calculation**\n' +
      '\n' +
      '**Response-Xwithe-Yeth-75(75-14) 온도 0.7**로부터의 시료 1개\n' +
      '\n' +
      '\\(f(-2)+f(-1)+f(0)\\의 값을 구하려면 먼저 x의 특정 값에서 각 함수를 평가해야 한다.\n' +
      '\n' +
      '\\(f(-2)\\): \\(f(x)=\\frac{3x-2}{x-2}\\)을 방정식으로 대체한다. \\(f(-2)=\\frac{3(-2)-2}{-2-2}=\\frac{-6-2}{-4}=\\frac{-8}{-4}=2\\).\n' +
      '\n' +
      '\\(f(-1)\\): \\(f(x)=\\frac{3x-2}{x-2}\\)을 방정식으로 대체한다. \\(f(-1)=\\frac{3(-1)-2}{-1-2}=\\frac{-3-2}{-3}=\\frac{-5}{-3}=\\frac{5}{3}\\).\n' +
      '\n' +
      '\\(f(0)\\)에 대해: \\(f(x)=\\frac{3x-2}{x-2}\\)을 방정식으로 대체한다: \\(f(0)=\\frac{3(0)-2}{0-2}=\\frac{-2}{-2}=1\\).\n' +
      '\n' +
      '이제 \\(f(-2)\\), \\(f(-1)\\), \\(f(0)\\): \\(f(-2)+f(-1)+f(0)=2+\\frac{5}{3}+1=\\frac{14}{3}\\)의 값을 더할 수 있다.\n' +
      '\n' +
      '답은: \\(\\framebox{\\frac{14}{3}}\\)**Correct**\n' +
      '\n' +
      '**Response-Xwithe-Yeth-75 (75-14) Greedy Decortic**\n' +
      '\n' +
      '우리는 \\(f(-2)+f(-1)+f(0)\\)의 값을 구하기 위해 주어진 점들 각각에서 함수 \\(f(x)\\)을 구한 후 그 결과를 합해야 한다.\n' +
      '\n' +
      '먼저 \\(f(-2)\\): \\(f(-2)=\\frac{3(-2)-2}{(-2)-2}=\\frac{-6-2}{-2-2}=\\frac{-8}{-4}=2\\)\n' +
      '\n' +
      '다음으로 \\(f(-1)\\): \\(f(-1)=\\frac{3(-1)-2}{(-1)-2}=\\frac{-3-2}{-1-2}=\\frac{-5}{-3}=\\frac{5}{3}\\\n' +
      '\n' +
      '마지막으로 \\(f(0)\\): \\(f(0)=\\frac{3(0)-2}{0-2}=\\frac{0-2}{-2\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>