<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# TnT-LLM: Text Mining at Scale with Large Language Models\n' +
      '\n' +
      'Mengting Wan\n' +
      '\n' +
      'Som of the information in this document relates to pre-released content which may be subsequently modified. Microsoft makes no warrantes, express or implied, with respect to the information provided here. This document is provided "as-sr". Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. Some examples depicted herein are provided for illustration only and are fictitious. No real association or connection is intended or should be inferred. This document does not provide you with any legal rights to any intellectual property in any Microsoft product:0 2024 Microsoft. All rights reserved.\n' +
      '\n' +
      'Tara Safavi\n' +
      '\n' +
      'Som, Suzy Kumar Jauhar\n' +
      '\n' +
      'Yuin Kim\n' +
      '\n' +
      'Scott Counts\n' +
      '\n' +
      'Jennifer Neville\n' +
      '\n' +
      'Siddharth Suri\n' +
      '\n' +
      'Chirag Shah\n' +
      '\n' +
      'Som, Ryen W. White\n' +
      '\n' +
      'Longqi Yang\n' +
      '\n' +
      'Reid Andersen\n' +
      '\n' +
      'Georg Buscher\n' +
      '\n' +
      'Dhruv Joshi\n' +
      '\n' +
      'Nagu Rangan\n' +
      '\n' +
      'Microsoft Corporation\n' +
      '\n' +
      'University of Washington\n' +
      '\n' +
      '{mengting.wan,tarasafavi}@microsoft.com\n' +
      '\n' +
      '###### Abstract.\n' +
      '\n' +
      'Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose **TnT-LLM**, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with _minimal human effort_ for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply **TnT-LLM** to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that **TnT-LLM** generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.\n' +
      '\n' +
      '+\n' +
      'Footnote †: dagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: dagger}\\)Work done while working at Microsoft.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddaggeragger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddaggeragger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddaggeragger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddaggeragger}\\)Corresponding authors.\n' +
      '\n' +
      '+\n' +
      'Footnote †: ddagger}\\)Corresponding authors.\n' +
      '\n' +
      'To address these challenges, in this paper we propose **TnT-LLM**, a novel framework that combines the interpretability of manual approaches with the scale of automatic text clustering and topic modeling. **TnT-LLM** is an end-to-end two-phase framework for joint **T**axonomy **G**eneration **and** Text Classification that relies on the unique strengths of instruction following Large Language Models (**LLMs**) in both phases. First, in the taxonomy generation phase, we devise a zero-shot multi-stage reasoning approach that prompts an LLM to produce and refine a label taxonomy iteratively with respect to the corpus for a given use-case (e.g., intent detection). Second, in the text classification phase, we adopt LLMs as data augmentors to scale up the creation of training data, which in turn is used to train lightweight classifiers capable of large-scale labeling. This framework is adaptable and modular, and can be customized to different use cases, text corpora, LLMs, and classifiers, while requiring little human intervention or input. In summary, our main contributions are as follows:\n' +
      '\n' +
      '* We introduce **TnT-LLM**, an end-to-end two-phase framework to automate and scale the process of taxonomy generation and text classification with representative and interpretable labels.\n' +
      '* We present a series of quantifiable and traceable evaluation strategies to validate each stage of this framework, including _deterministic automatic_ metrics, _human evaluation_ metrics, as well as _LLM-based_ evaluations.\n' +
      '* We use **TnT-LLM** to analyze conversations from Bing Copilot (formerly Bing Chat), a web-scale, multilingual, and open-domain conversational agent. Our results show that the proposed framework can produce more accurate and relevant label taxonomies compared to the state-of-the-art text clustering approaches. We also demonstrate that the lightweight label classifiers trained on LLM annotations can achieve comparable (and sometimes better) performance than directly using LLMs as classifiers, but with much higher scalability and model transparency. Through quantitative and qualitative analysis, we provide insights and recommendations for applying LLMs on large-scale text mining.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      '**Taxonomy Generation.** Prior work in taxonomy generation falls into manual and automatic approaches. Handcrafted taxonomies, beyond being expensive to construct, tend to be developed for specific downstream tasks (e.g., web search intent analysis (Han et al., 2017; Chen et al., 2018), chatbot intent detection (Tang et al., 2018)), or tied to the development of specific datasets (Kang et al., 2019; Krizhevsky et al., 2019). On the other hand, automated approaches scale better but either rely on term extraction from corpora to obtain labels, which may hinder interpretability and/or coverage (Krizhevsky et al., 2019; Wang et al., 2019), or else require a set of seeds for the taxonomy in order to generate new labels (Krizhevsky et al., 2019). **TnT-LLM**, in contrast, is automatic, abstractive (i.e., labels describe the corpus but need not be directly extracted from it), and does not require any seed labels. Moreover, **TnT-LLM** treats taxonomy generation and text classification as interrelated problems in an end-to-end pipeline, whereas prior work has tended to focus mainly on the quality of the taxonomy produced, without considering its downstream utility for classification.\n' +
      '\n' +
      '**Text Clustering and Topic Modeling.** Text clustering and topic modeling "invert" the traditional approach of defining a label set, then applying the labels on the corpus. Given a set of documents, such approaches first group the documents into topical clusters using various definitions of textual similarity, then post-hoc label or summarize the clusters (Han et al., 2017; Chen et al., 2018). While traditional approaches in theory accomplish the same goals as **TnT-LLM**, they suffer due to a lack of interpretability (Chen et al., 2018), as they typically do not assign intelligible labels to clusters. More recently, attempts have been made to overcome these problems by using LLMs for topic modeling (Krizhevsky et al., 2019; Wang et al., 2019), though these approaches still require supervision through either a predefined taxonomy (Wang et al., 2019) or a seed set of topics (Krizhevsky et al., 2019).\n' +
      '\n' +
      '**LLMs as Annotators.** Recent work has explored using LLMs to replace human annotation for labor-intensive tasks such as search relevance quality labeling (Krizhevsky et al., 2019), topic and stance detection (Chen et al., 2018), and various computational social science labeling tasks (Wang et al., 2019). These studies have found that, in general, LLMs perform on par or even better than crowd-workers (Krizhevsky et al., 2019), often at a fraction of the cost. In the same vein, we explore using LLMs as annotators for text classification, although our main goal is to scale the process by distilling LLMs\' label-specific capabilities into more efficient, lightweight classifiers.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'We begin with a high-level overview of **TnT-LLM**, our proposed two-phase framework for 1) **LLM-powered taxonomy generation** and 2) **LLM-augmented text classification**. In the first phase, we sample a small-scale representative subset of a corpus and perform zero-shot multi-stage taxonomy generation in an iterative manner inspired by stochastic gradient descent (Beng et al., 2017). In the second phase, we sample a larger dataset and leverage LLMs with the taxonomy produced by Phase 1 to classify each instance. These LLM labels are then treated as "pseudo-labels" for training a lightweight text classifier. Once training is complete, the lightweight classifier is deployed to label the entire corpus offline, and may also serve for online real-time classification.\n' +
      '\n' +
      '### Phase 1: Taxonomy Generation\n' +
      '\n' +
      'Phase 1 of **TnT-LLM** is inspired by the classic mixture model clustering process (Krizhevsky et al., 2019), but implemented in a prompt-based manner. We leverage a "stochastic optimization" approach (Krizhevsky et al., 2019) to iteratively update the intermediate taxonomy outcome, so that a large and dynamic corpus sample can be effectively handled. Depending on the desired granularity of the taxonomy, we suggest using a "small-to-medium" corpus sample that is representative of the corpus in this phase, such that the sample size is sufficient to capture the diversity of the corpus, but not too large to incur unnecessary costs.\n' +
      '\n' +
      '* **Stage 1: Summarization.** In order to normalize all text samples and extract their most salient information, we first generate concise and informative summaries of each document in the sample. Specifically, we prompt an LLM to summarize each document by providing a short blurb about the intended use-case for the summary (e.g., intent detection) and a target summary length (e.g., 20 words); the full prompt template is provided in Figure 8 in the supplemental details. This stage helps reduce the size and variability of the input documents while also extracting the aspects of the document most relevant to the use-case, which we find is especially important for label spaces that are not evident from surface-level semantics (e.g., user intent). Note that this stage is relatively fast, as it may be executed concurrently for each input document with a cost-efficient LLM like GPT-3.5-Turbo.\n' +
      '* **Stage 2: Taxonomy Creation, Update, and Review**. We next create and refine a label taxonomy using the summaries from the previous stage. Similar to SGD, we divide the summaries into equal-sized minibatches. We then process these minibatches with three types of zero-shot LLM reasoning prompts in sequence. The first, an _initial generation prompt_, takes the first minibatch and produces an initial label taxonomy as output. The second, a _taxonomy update prompt_, iteratively updates the intermediate label taxonomy with new minibatches, performing three main tasks in each step: 1) evaluating the given taxonomy on the new data; 2) identifying issues and suggestions based on the evaluation; and 3) modifying the taxonomy accordingly. Finally, after the taxonomy has been updated a specified number of times, we apply a _review prompt_ that checks the formatting and quality of the output taxonomy, of which the output is regarded as the final taxonomy output by Stage 1. In all three prompts, we provide the use-case instruction, which specifies the goal and the format of the desired label taxonomy (e.g., the desired number of labels, the target number of words per label), alongside the minibatch. The full prompt templates are provided in Figure 10 in the supplemental details.\n' +
      '\n' +
      'Notice that this process naturally lends itself to hierarchy: After a first round of taxonomy generation, we can rerun Stage 2 for each subgroup of categorized samples to create new, more granular levels in the taxonomy. An overview of our proposed approach is presented in Figure 2.\n' +
      '\n' +
      '**Connection to Mixture Models & Stochastic Optimization.** Here we present an analogy between our pipeline and the Mixture Model family (e.g., Gaussian Mixture Model) for text clustering. We assume each text data point \\((x_{i})\\) follows a mixture distribution \\(x_{i}\\sim\\sum w_{k}\\mathcal{N}(\\mu_{k},\\Sigma_{k})\\), where \\(\\mathcal{N}(\\mu_{k},\\Sigma_{k})\\) defines the distribution of the \\(k\\)-th component, i.e., a Gaussian distribution with a mean \\(\\mu_{k}\\) and variance \\(\\Sigma_{k}\\). Given a corpus sample \\(\\{x_{i}\\}\\), this mixture model can be learned through Maximum Likelihood Estimation (MLE), equivalent to minimizing the negative of the log-likelihood loss, i.e.,\n' +
      '\n' +
      '\\[\\begin{split}&\\max\\,\\prod_{i}\\Big{(}\\sum w_{k}\\mathcal{N}(\\mu_{k},\\Sigma_{k};x_{i})\\Big{)}\\\\ &\\Leftrightarrow\\min\\,-\\sum_{i}\\log\\Big{(}\\sum w_{k}\\mathcal{N}( \\mu_{k},\\Sigma_{k};x_{i})\\Big{)}\\Leftrightarrow\\min\\,\\sum_{i}\\mathcal{L}( \\Theta,x_{i}).\\end{split} \\tag{1}\\]\n' +
      '\n' +
      'Mapping back to our prompt-based approach, we take a corpus sample and a use-case instruction as input. Our goal is to "learn" a taxonomy that is relevant to the instruction and best fits the input corpus sample; this taxonomy must consist of category labels with names and brief descriptions. We can represent our desired label taxonomy as a parameter set \\(\\Theta=\\{\\mathbf{\\mu},\\Sigma\\}\\), following the definition of the mixture model, where \\(\\mathbf{\\mu}=\\{\\mu_{k}\\}\\) are the names of labels \\(k\\) which represent the "cluster centroids," and \\(\\Sigma=\\{\\Sigma_{k}\\}\\) are the descriptions that specify the "shape" of cluster \\(k\\). We assume the mixture weights \\((w_{k})\\) are implicitly captured by the LLM that generates the label taxonomy in this study. We can then map our taxonomy creation and refinement stages to stochastic optimization as follows:\n' +
      '\n' +
      '* **Stage 1: Feature Representation.** Our summarization stage is analogous to the featurization step in classic machine learning, where raw text inputs are projected onto a vector space via a feature transformation such as an embedding model. In our case, the output summary of each data point can be viewed as a concise and informative feature representation of the original text \\((x_{i})\\).\n' +
      '* **Stage 2: Stochastic Gradient Descent.** The main taxonomy creation and update stage resembles prompt optimization with Stochastic Gradient Descent (SGD) (Glorot et al., 2016), where the generation prompt is used to initialize the taxonomy (i.e., the parameters \\(\\Theta_{0}\\)), which is then optimized via SGD through the update prompt-chain. In each update prompt, we assess how the current taxonomy (\\(\\Theta_{\\mathbf{m}}\\)) fits the given batch of data (i.e., calculating the loss function defined in Eq. (1)), then analyze and "backpropagate" the errors to update the taxonomy, i.e., \\(\\Theta_{m+1}=\\Theta_{m}-\\eta\\nabla\\mathcal{L}(\\Theta_{m})\\), where \\(\\eta\\) refers to the learning rate which we assume is implicitly adjusted by the LLM.\n' +
      '\n' +
      '### Phase 2: LLM-Augmented Text Classification\n' +
      '\n' +
      'After the taxonomy is finalized, we next train a text classifier that can be reliably deployed to perform label assignments at very large-scale and in real-time. Following recent work that shows the strengths of LLMs as annotators of training data (Krizhevsky et al., 2015; Krizhevsky et al., 2015), we propose to leverage LLMs to obtain a "pseudo-labeled" corpus set\n' +
      '\n' +
      'Figure 2. An illustration of the LLM-powered taxonomy generation phase (Phase 1).\n' +
      '\n' +
      'using the taxonomy yielded in Phase 1, then use these labels to train more efficient classifiers at scale. Specifically, we prompt an LLM to infer the primary label (as a multiclass classification task) and all applicable labels (as a multilabel classification task) on a "medium-to-large" scale corpus sample that covers the range of labels in the taxonomy, creating a representative training dataset that can be used to build a lightweight classifier, such as a Logistic Regression model or a Multilayer Perceptron classifier. In this way, we can induce "pseudo labels" from the LLM classifier and transfer its knowledge to a more efficient and manageable model that can be deployed and served at scale. An illustrative figure of this phase is presented in Figure 3.\n' +
      '\n' +
      '## 4. Evaluation Suite\n' +
      '\n' +
      'Due to the unsupervised nature of the problem we study and the lack of a benchmark standard, performing quantitative evaluation on end-to-end taxonomy generation and text classification can be challenging. We therefore design a suite of strategies to evaluate **TnT-LLM**. Our evaluation strategies may be categorized into three buckets, depending on the type and source of the evaluation criteria. The three categories are as follows:\n' +
      '\n' +
      '* **Deterministic automatic evaluation**: This type of approach is scalable and consistent, but requires well-defined, gold standard rules and annotations. It is less applicable for evaluating the abstract aspects studied in this paper, such as the quality and usefulness of a label taxonomy.\n' +
      '* **Human evaluation**: These approaches are useful for evaluating the abstract aspects that the automatic evaluations cannot address. However, they are also time-consuming, expensive, and may encounter data privacy and compliance constraints.\n' +
      '* **LLM-based evaluations**: Here, LLMs are used to perform the same or similar tasks as human evaluators. This type of evaluation is more scalable and cost-effective than human evaluation, albeit potentially subject to biases and errors if not applied properly. We therefore aim to combine and validate LLM-based evaluation with human evaluation metrics on small corpora so that we can extrapolate conclusions with sufficient statistical power.\n' +
      '\n' +
      '### Phase 1 Evaluation Strategies\n' +
      '\n' +
      'Following prior studies (Wang et al., 2018; Wang et al., 2019), we evaluate a label taxonomy on three criteria: Coverage, accuracy, and relevance to the use-case instruction. Note that we require implementing the _native_ primary label assignment to apply these metrics. For clustering-based methods, this is instantiated through the clustering algorithm. For **TnT-LLM**, this is done by a label assignment prompt as described in Section 3.2. We also note that the label accuracy and use-case relevance metrics discussed here are applicable to both **human** and **LLM** raters.\n' +
      '\n' +
      '**Taxonomy Coverage**. This metric measures the comprehenses of the generated label taxonomy for the corpus. Conventional text clustering approaches (e.g., embedding-based k-means) often achieve 100% coverage by design. In our LLM-based taxonomy generation pipeline, we add an \'Other\' or \'Undefined\' category in the label assignment prompt by design and measure the proportion of data points assigned to this category. The lower this proportion, the higher the taxonomy coverage.\n' +
      '\n' +
      '**Label Accuracy**. This metric quantifies how well the assigned label reflects the text data point, relative to other labels in the same taxonomy. Analogous to mixture model clustering, the primary label should be the most probable one given the text. We assume human and LLM raters can assess the label fit by its name and description. We treat accuracy as a pairwise comparison task: for each text, we obtain the primary label and a random negative label from the same taxonomy, and ask a rater to choose the more accurate label based on their names and descriptions.1 If the rater correctly identifies the positive label, we consider it as a "Hit" and report the average hit rate as the label accuracy metric. We do not explicitly evaluate the overlap across category labels and rather expect it to be implicitly reflected in the pairwise label accuracy metric.\n' +
      '\n' +
      'Footnote 1: Raters are also offered a “None’ option besides the pair, but are instructed to minimize the use of it.\n' +
      '\n' +
      '**Relevance to Use-case Instruction.** This metric measures how relevant the generated label taxonomy is to the use-case instruction. For example, "Content Creation" is relevant to an instruction to "understand user intent in a conversation", while "History and Culture" is not. We operationalize this as a binary rating task: for each instance, we provide its primary label name and description to a human or LLM rater, and ask them to decide if the label is relevant to the given use-case instruction or not. Note that we instruct the rater to use the presented instance as the context, and rate the relevance conditioned on the label\'s ability to accurately describe some aspect of the text input. The goal of this metric is not to evaluate the label accuracy, but rather to rule out the randomness introduced by taxonomies that are seemingly relevant to the use-case instruction, but irrelevant to the corpus sample - and therefore useless for downstream applications.\n' +
      '\n' +
      '### Phase 2 Evaluation Strategies\n' +
      '\n' +
      'To quantitatively evaluate text classification, we create a benchmark dataset with reliable ground-truth annotations as follows:\n' +
      '\n' +
      '**Task and Annotation Reliability.** We first assess the reliability of the label assignment task and the human annotations by\n' +
      '\n' +
      'Figure 3. An illustration of the LLM-augmented text classification phase (Phase 2).\n' +
      '\n' +
      'involving multiple human annotators and calculating the inter-rater agreement (Cohen\'s Kappa (Cohen, 1998) between two raters and Fleiss\' Kappa (Fleiss, 1998) among multiple raters). We then resolve disagreements between human annotations by either voting or deliberation, and obtain a consensus human annotation for each instance. Then we use an LLM as an additional annotator to perform the same label assignment task, and measure the agreement between the LLM annotation and the consensus human label. Intuitively, this agreement captures how well the LLM is aligned with (the majority of) human annotators and how reliable it is for this label assignment task.\n' +
      '\n' +
      '**Classification Metrics.** We apply both human and LLM annotations on a small-scale corpus sample and calculate the conventional multiclass and multilabel classification metrics (e.g., Accuracy, F1) with human annotations as the ground truth. These metrics evaluate how the label classifier is aligned with human preferences on a small subset of the corpus. We then apply the LLM annotator on a larger-scale corpus sample and leverage the resulting annotations as the oracle to calculate the same classification metrics. These metrics enable a comprehensive diagnosis of the label classifier performance at scale on different aspects of the corpus, such as domains, languages, and time ranges.\n' +
      '\n' +
      'In practice, we recommend leveraging both human evaluation and LLM-based metrics as a holistic evaluation suite, while also taking into account the task and annotation reliability. This approach can help us identify and mitigate the possible bias that may arise from either method or be affected by the task complexity, and enable us to scale up the evaluation and annotation to a large corpus sample with confidence, thus obtaining more robust and informative evaluation results.\n' +
      '\n' +
      '## 5. Experiments\n' +
      '\n' +
      'We showcase the utility of **TnT-LLM** for two text mining tasks of special interest in today\'s LLM era: **User intent detection** and **conversational domain labeling** over human-AI chat transcripts.\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      'Our conversation transcripts are taken from Microsoft\'s Bing Consumer Copilot system, which is a multilingual, open-domain generative search engine that assists users through a chat experience. We randomly sample 10 weeks of conversations from 8/6/2023 to 10/14/2023, with 1k conversations per week for Phase 1, where we perform a random 60%-20%-20% split for "learning" the label taxonomy, validation, and testing respectively. We then sample another 5k conversations per week from the same time range for Phase 2, and apply the same train/validation/test data split.\n' +
      '\n' +
      'We perform two steps of filtering to ensure the quality and privacy of the data. First, we apply an in-house privacy filter that scrubs all personal information (e.g., addresses, phone numbers) from the original conversation content. Second, we apply a content filter that removes all conversations that contain harmful or inappropriate content that should not be exposed to annotators or downstream analyses. After applying these filters, we obtain 9,592 conversations for Phase 1 and 48,160 conversations for Phase 2. We leverage the FastText language detector (Fleiss, 2016; Fleiss, 2017) to identify the primary language of each conversation, where we find around half of the conversations in our corpus are in English.\n' +
      '\n' +
      'In the remainder of this section, we will report results on the following datasets:\n' +
      '\n' +
      '* **BingChat-Phase1-L-MULTI**: The test set used in the taxonomy generation phase, which includes around 2k conversations.\n' +
      '* **BingChat-Phase2-L-MULTI**: The test set used in the label assignment phase, which includes around 10k conversations.\n' +
      '\n' +
      'Besides the above datasets, we also reserve two separate English-only conversation datasets to perform human evaluations, with the same privacy and content filter applied.\n' +
      '\n' +
      '* **BingChat-Phase1-S-Eng** includes 200 English conversations to evaluate label taxonomy.\n' +
      '* **BingChat-Phase2-S-Eng** includes 400 English conversations to evaluate label assignment.\n' +
      '\n' +
      '### Taxonomy Generation\n' +
      '\n' +
      '#### 5.2.1. Methods\n' +
      '\n' +
      'To evaluate the effectiveness of **TnT-LLM**, we compare it with baseline methods that rely on embedding-based clustering to group conversations and then assigns LLM-generated labels to each cluster. We use two state-of-the-art LLMs, **GPT-4 (0613)** and **GPT-3.5-Turbo (0613)**, as label generators and evaluators, and two different embedding methods, **ada2**2 and **Instructor-XL**(Yang et al., 2017), to represent the conversations. The methods considered in our experiments are as follows:\n' +
      '\n' +
      'Footnote 2: [https://openai.com/blog/new-and-improved-embedding-modal](https://openai.com/blog/new-and-improved-embedding-modal)\n' +
      '\n' +
      '* **GPT-4 (TnT-LLM)**: the proposed **TnT-LLM** with GPT-4 to perform label taxonomy generation and assignment.\n' +
      '* **GPT-3.5 (TnT-LLM)**: the proposed **TnT-LLM** with GPT-3.5-Turbo to perform label taxonomy generation and assignment.\n' +
      '* **ada2 + GPT-4**: the embedding-based clustering approach where conversations are represented via **ada2** and K-means algorithm is applied to generate clusters. We randomly sample 200 conversations within each cluster, prompt GPT-4 to summarize each conversation, then ask it to produce a label name and description from these summaries, conditioned on the use-case instruction.\n' +
      '* **ada2 + GPT-3.5-Turbo**: similar to the above method, with GPT-3.5-Turbo as the label generator.\n' +
      '* **Instructor-XL + GPT-4**: similar to the above embedding-based methods, with Instructor-XL and GPT-4 as the underlying embedding and the label generator respectively.\n' +
      '* **Instructor-XL + GPT-3.5-Turbo**: similar to the above method, with GPT-3.5-Turbo as the label generator.\n' +
      '\n' +
      'Note that all the taxonomies evaluated in this section are fully automatic and do not involve any human intervention.\n' +
      '\n' +
      '#### 5.2.2. Implementation Details\n' +
      '\n' +
      'We instruct our LLMs to generate 10 intent categories and 25 domain categories for taxonomy generation. Likewise, we learn 10 intent clusters and 25 domain clusters with our embedding-based baselines. We use a minibatch size of 200 for our proposed taxonomy generation pipeline. We also apply a minibatch version of the K-means algorithm in all embedding-based clustering approaches, where the same batch size is used with a K-means++ (Chen et al., 2017) initialization. We run 10 different trials of the clustering algorithm and select the best one based on the Silhouette coefficient (Srivastava et al., 2016) on the validation set. We also devise a "model" selection prompt, which takes a batch of conversation summaries,multiple label taxonomies, a use-case instruction as input, then outputs the index of the taxonomy that best fits the data and the instructional desiderata. We then run **TnT-LLM** 10 trials and select the best outcome based on its performance on the validation set.\n' +
      '\n' +
      '**Human Evaluation.** To evaluate the quality of generated taxonomies from methods listed above, three of the authors performed the label accuracy and use-case relevance tasks; each conversation was evaluated by all three raters. While raters possessed a high degree of familiarity with the Bing Copilot system, as well as the desired use-cases, they were unaware of the correspondence between methods and their generated labels. The position of the options in the pairwise comparison label accuracy task is also fully randomized. We also use two LLM systems, GPT-4 and GPT-3.5-Turbo, to perform the same evaluation tasks as the human raters. However, we note that the LLM systems tend to exhibit a position bias (Han et al., 2017) for the pairwise comparison task, where they favor one option over another based on its position in the prompt. This bias is more evident when the taxonomy quality is low and the task is more challenging. To mitigate this, we average the results over multiple runs with randomized positions of the options in our experiments.\n' +
      '\n' +
      '#### 5.2.3. Results\n' +
      '\n' +
      'We first calculate the **coverage** of the LLM-generated taxonomies on the **BingChat-Phase1-L-Multi** dataset, where both LLM systems achieve very high coverage (\\(>\\)99.5%) on both user intent and conversational domain taxonomies.\n' +
      '\n' +
      'We then conduct the accuracy and relevance evaluation tasks to assess the quality of the taxonomies generated by different methods on the small English-only evaluation dataset **BingChat-Phase1-S-Eng**. We report the inter-rater agreement (Cohen\'s Kappa (Cohen, 2018) between two raters and Fleiss\' Kappa (Cohen, 2018) among multiple raters) in Table 1. The agreement is _moderate_ (\\(\\kappa>0.4\\)) on intent and domain accuracy as well as intent relevance, while the agreement on domain relevance is _fair_ (_Fleiss\'\\(\\kappa=0.379\\)_).3 Interestingly, for the tasks with _moderate_ agreement, the GPT-4 evaluator agrees more with the human majority than the humans do among themselves. This suggests that GPT-4 can be a consistent and reliable evaluator.\n' +
      '\n' +
      'Footnote 3: Note that these evaluation tasks are cognitively challenging, especially for low-quality taxonomies (e.g., from some baseline methods).\n' +
      '\n' +
      'Figure 3(a) shows the main results on **label accuracy and use case relevance** from human evaluations on **BingChat-Phase1-S-Eng**. We observe our **TnT-LLM** using GPT-4 outperforms other methods in most cases. Compared to GPT4, we find that GPT-3.5-Turbo tends capture conversation topics (domains) well, but often fails to generate labels that are aligned with the user intent instruction. Likewise, we notice that some embedding methods (**ada2 + GPT-4**, **instructor-xl + GPT-4**) perform well in terms of producing accurate domain labels, on par with **TnT-LLM** instantiated with GPT-3.5-Turbo, but fail to capture the user intent behind the conversations. This is likely because the domain labels reflect the topical theme of the conversations, which can be easily derived from the semantic information captured by unsupervised embeddings, while intent labels require deeper reasoning and understanding of the use-case instruction.\n' +
      '\n' +
      'With regard to our baselines, we find that GPT-4 consistently outperforms GPT-3.5-Turbo in producing more accurate labels when using the same embedding method for clustering. For the intent use-case, GPT-4 generates more relevant labels than GPT-3.5-Turbo, while the difference is less noticeable for the domain use case; again, this may be because GPT-3.5-Turbo is better at capturing topical information in conversations than reasoning about user intent.\n' +
      '\n' +
      'Finally, given the high agreement between GPT-4 and human raters on the label accuracy task, we use GPT-4 to evaluate the label accuracy on the larger multilingual dataset **BingChat-Phase1-L-Multi** (Figure 3(b)). We observe similar patterns as those in our\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline \\multirow{2}{*}{**Metric**} & \\multirow{2}{*}{**Use Case**} & \\multicolumn{2}{c}{**Among Humans**} & \\multicolumn{2}{c}{**LLM vs. Human**} \\\\ \\cline{3-6}  & & Overall & Avg. pairwise & GPT-3.5-Turbo & GPT-4 \\\\  & & (Fleiss) & (Cohen) & (Cohen) & (Cohen) \\\\ \\hline \\multirow{3}{*}{Accuracy} & Intent & 0.476\\({}^{*}\\) & 0.477\\({}^{*}\\) & 0.376 & 0.558\\({}^{*}\\) \\\\  & Domain & 0.478\\({}^{*}\\) & 0.484\\({}^{*}\\) & 0.260 & 0.578\\({}^{*}\\) \\\\ \\hline \\multirow{3}{*}{Relevance} & Intent & 0.466\\({}^{*}\\) & 0.481\\({}^{*}\\) & 0.333 & 0.520\\({}^{*}\\) \\\\  & Domain & 0.379 & 0.399 & 0.177 & 0.288 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. Inter-rater reliability (Fleiss’ Kappa and Cohen’s Kappa) among human raters and between LLM raters and the resolved human rating through majority voting. Agreement considered as _moderate_ and above (\\(>0.4\\)) are highlighted with \\({}^{*}\\). Evaluation is performed on **BingChat-Phase1-S-Eng**.\n' +
      '\n' +
      'Figure 4. Taxonomy evaluation results on **BingChat-Phase1-S-Eng** from human raters and the GPT-4 rater, where error bars indicate 95% confidence intervals.\n' +
      '\n' +
      'human evaluation, where our **TnT-LLM** achieves the highest accuracy, and in particular the instantiation that uses GPT-4.\n' +
      '\n' +
      '### LLM-Augmented Text Classification\n' +
      '\n' +
      'At the end of the label taxonomy generation phase, we conduct a lightweight human calibration (Liu et al., 2018) on the intent taxonomy and domain taxonomy generated from **TnT-LLM** with GPT-4 to improve their clarity. These calibrated taxonomies are then utilized in the label assignment phase. The full label and description texts of each taxonomy are provided in Table 5 and Table 6. As a reminder, our main goal in this section is to compare how distilled lightweight classifiers trained on LLM labels compare to a full LLM classifier; our goal is to achieve a favorable tradeoff of accuracy and efficiency compared to a more expensive but potentially more powerful LLM.\n' +
      '\n' +
      '#### 5.3.1. Methods\n' +
      '\n' +
      'We apply GPT-4 as an automated annotator to assign both the primary label and any other relevant labels to each conversation in the corpus. We then train classifiers based on the GPT-4 annotated training and validation sets. We extract features from each conversation using two embedding methods: **ADA2** and **INstructor-XL**. For each embedding method, we train three types of classifiers with the GPT-4 labels: **Logistic Regression**, the gradient boosting **LightGBM**(Liu et al., 2018), and a two-layer **MultiLayer****Perceptron (MLP)**(Chen et al., 2019). We use multinomial logit in **logistic regression** for the primary label classification, and a standard \'one-vs-all\' scheme for the multilabel classification with all three classifiers.\n' +
      '\n' +
      'Additionally, four of the authors manually labeled 400 English conversations (**BingChat-Phase2-S-Eng**) with the given intent and domain taxonomy. Each conversation was labeled by three annotators, and the majority vote determined the final labels. For a few conversations (\\(<\\)10%), where all three annotators disagreed on the primary label the fourth annotator was used as a tie-breaker.\n' +
      '\n' +
      'We thus obtain two annotated test sets: **BingChat-Phase2-S-Eng** with 400 English conversations with both human and GPT-4 annotations, and **BingChat-Phase2-L-Multi** with around 10k conversations with GPT-4 annotations only.\n' +
      '\n' +
      '#### 5.3.2. Results\n' +
      '\n' +
      'We first evaluate the agreement between annotators to assess the task complexity and reliability. As Table 2 shows, human annotators have _substantial_ agreement on the primary domain label (\\(\\kappa>0.6\\)), and _moderate_ agreement on the primary intent label (\\(Fleiss^{\\prime}\\kappa=0.553\\)). Both of these values indicate a high degree of mutual understanding among raters and clarity in the instructions and taxonomies. We also note that the domain taxonomy has more categories (25) than the intent taxonomy (10). One might expect a larger taxonomy to be more difficult to comprehend, but we find the smaller intent taxonomy to be more challenging for humans to agree on. We attribute this to the task complexity and ambiguity, as it requires more reasoning; this observation aligns well with our observation in the previous evaluation that GPT4 greatly outperforms GPT-3.5-Turbo on intent detection, as GPT4 is generally considered to be a stronger reasoner.\n' +
      '\n' +
      'Similar to the label accuracy evaluation (Table 1), GPT-4 agrees more with the resolved human labels than humans do among themselves on the primary label assignment. We observe that human agreement on all applicable labels is _moderate_ (\\(\\kappa>0.4\\)) with both intent and domain taxonomies, which is surprisingly good considering such an agreement is calculated based on exact match (i.e., an agreement is counted only if all selected labels are matched). However, the agreement between GPT-4 and human annotations on this task is much lower. A closer inspection reveals that GPT-4 tends to be more liberal than humans on label assignment, applying all relevant categories, resulting in a low precision but high recall.\n' +
      '\n' +
      'We then evaluate the classification performance of the distilled embedding-based classifiers on two datasets: **BingChat-Phase2-S-Eng**, where human annotations are the oracle, and **BingChat-Phase2-L-Multi**, where GPT-4 annotations are the oracle. The results for the primary label classification are presented in Table 3, where we observe that lightweight embedding-based classifiers can achieve promising results. In particular, **ADA2** embeddings achieve strong results with logistic regression; nonlinearity does not seem to improve performance significantly in most cases. When using human annotations as the gold standard, we find that the performance of these lightweight models are comparable to, and sometimes slightly better than, directly using GPT-4 as a classifier on **BingChat-Phase2-S-Eng**. We also perform evaluation on the multilingual test set **BingChat-Phase2-L-Multi**, where GPT-4 annotations are considered as oracle. We observe the performance on non-English conversations is lower than that on English conversations (Table 3), especially on the **INstructor** embedding, indicating the importance of choosing an appropriate embedding method that suits the characteristics of the corpus.\n' +
      '\n' +
      'On the multilabel classification task (Table 4), we observe that the distilled classifiers achieve higher precision at the expense of some recall compared to GPT-4. Here, nonlinearity also seems to help more, as MLP-based classifiers achieve the highest accuracy and precision.\n' +
      '\n' +
      '### Summary of Findings and Suggestions\n' +
      '\n' +
      'We have shown that our novel **TnT-LLM** framework is capable of generating high-quality label taxonomies from unstructured text corpora with very little human instruction or intervention. In our evaluation of this approach on real-world AI chat conversations, we demonstrated that it can be used to find structure and organization in unstructured text. Our method outperforms the conventional embedding-based clustering approach, especially when deeper reasoning beyond surface-level semantics is required. Finally we found that while embedding-based clustering can still be effective, it is\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline \\multirow{2}{*}{**Metric**} & \\multirow{2}{*}{**Use Case**} & \\multicolumn{2}{c}{**Among Humans**} & **LLM vs. Human** \\\\ \\cline{3-5}  & & Overall & Avg. pairwise & GPT-4 \\\\  & & (Fleiss) & (Cohen) & (Cohen) \\\\ \\hline \\multirow{2}{*}{Primary Label} & Intent & 0.553* & 0.559* & 0.572* \\\\  & Domain & 0.624** & 0.624** & 0.695** \\\\ \\hline \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} All Labels (exact match) \\\\ \\end{tabular} } & Intent & 0.422* & 0.427* & 0.271 \\\\  & Domain & 0.467* & 0.467* & 0.102 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2. Inter-rater reliability (Fleiss’ Kappa and Cohen’s Kappa) among human annotators and between LLM annotations and the resolved human annotations. Agreement considered as _moderate_ (\\((0.4,0.6]\\)) are highlighted with *, _substantial_ and above (\\(>0.6\\)) are highlighted with **.\n' +
      '\n' +
      'more susceptible to modeling choices or artifacts, such as cluster granularity and alignment of use-case with inputs.\n' +
      '\n' +
      'We further explored the use of LLMs as raters or evaluators, demonstrating that they effectively approximate the collective opinion of humans on some evaluation tasks. Additionally, we found that LLMs excel at single-choice questions (e.g., pairwise label accuracy evaluation task) where they are forced to indicate preference on one option over another, but they can struggle on multiple-choice questions that involve subjective and nuanced judgments with implicit standards. We suggest using LLMs as an alternative strategy for human evaluation, but with caution and verification by measuring agreement with human preferences.\n' +
      '\n' +
      'Lastly, we proposed a perspective of using LLMs as "annotators" rather than classifiers, harnessing their ability to create abundant data. By utilizing LLMs to generate pseudo labels for unlabeled data, we can distill a lightweight classifier that can be reliably deployed at scale. In our experiments, such a classifier achieved competitive results, and matched or even surpassed the performance of GPT-4 as a classifier. We advocate for a careful assessment of the potential use cases of LLMs, balancing performance and efficiency, while exploiting both their power to generalize with the maturity, speed, and cost of conventional machine learning classifiers.\n' +
      '\n' +
      '## 6. Discussion and Future Work\n' +
      '\n' +
      'This work has the potential to create significant impact for research and application of AI technologies in text mining. Our framework has demonstrated the ability to use LLMs as taxonomy generators, as well as data labelers and evaluators. These automations could lead to significant efficiency gains and cost savings for a variety of domains and applications that rely on understanding, structuring and analyzing massive volumes of unstructured text. It could also broadly democratize the process of mining knowledge from text, empowering non-expert users and enterprises to interact with and interpret their data through natural language, thereby leading to better insights and data-driven decision making for a range of industries and sectors. Additionally, our framework and research findings relate to other work that leverages LLMs for taxonomy creation and text clustering, and has important empirical lessons for the efficient use of instruction-following models in these scenarios.\n' +
      '\n' +
      'Despite these initial successes, there are some important challenges and future directions that are worth exploring. As we have already noted, LLMs are expensive and slow. In future work, we hope to explore ways to improve the speed, efficiency and robustness of our framework, through hybrid approaches that further\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & \\multirow{2}{*}{Accur.} & \\multicolumn{3}{c}{Micro} & \\multicolumn{3}{c}{Macro} \\\\ \\cline{3-8}  & & Precision & Recall & F1 & Precision & Recall & F1 \\\\ \\hline \\multicolumn{8}{c}{**User Intent**} \\\\ \\hline GPT-4 & 0.320 & 0.518 & **0.743** & 0.610 & 0.613 & **0.644** & **0.537** \\\\\n' +
      '**ADA2 +** & & & & & & \\\\ LogisticReg & 0.388 \\({}^{\\dagger}\\) & 0.574 \\({}^{\\dagger}\\) & 0.736 \\({}^{\\circ}\\) & **0.645** & 0.593 & 0.607 & **0.537** \\\\ LightGBM & 0.380 \\({}^{\\dagger}\\) & 0.587 \\({}^{\\dagger}\\) & 0.669 \\({}^{\\dagger}\\) & 0.626 & 0.610 & 0.486 & 0.456 \\\\ MLP & **0.418**\\({}^{\\dagger}\\) & 0.599 \\({}^{\\dagger}\\) & 0.657 \\({}^{\\dagger}\\) & 0.627 & **0.626** & 0.513 & 0.499 \\\\\n' +
      '**Instructor-XL +** & & & & & & \\\\ LogisticReg & 0.358 \\({}^{\\dagger}\\) & 0.559 \\({}^{\\dagger}\\) & 0.688 \\({}^{\\dagger}\\) & 0.617 & 0.583 & 0.540 & 0.51 \\\\ LightGBM & 0.335 \\({}^{\\Box}\\) & 0.557 \\({}^{\\dagger}\\) & 0.644 \\({}^{\\dagger}\\) & 0.597 & 0.571 & 0.479 & 0.465 \\\\ MLP & 0.410 \\({}^{\\dagger}\\) & **0.606**\\({}^{\\dagger}\\) & 0.642 \\({}^{\\dagger}\\) & 0.623 & 0.623 & 0.480 & 0.495 \\\\ \\hline \\multicolumn{8}{c}{**Conversation Domain**} \\\\ \\hline GPT-4 & 0.110 & 0.442 & **0.753** & 0.557 & 0.565 & **0.687** & 0.576 \\\\\n' +
      '**ADA2 +** & & & & & & \\\\ LogisticReg & 0.188 \\({}^{\\dagger}\\) & 0.493 \\({}^{\\dagger}\\) & 0.732 \\({}^{\\dagger}\\) & **0.589** & 0.644 & 0.624 & **0.558** \\\\ LightGBM & 0.182 \\({}^{\\dagger}\\) & 0.469 \\({}^{\\dagger}\\) & 0.576 \\({}^{\\dagger}\\) & 0.517 & 0.621 & 0.440 & 0.452 \\\\ MLP & 0.242 \\({}^{\\dagger}\\) & 0.532 \\({}^{\\dagger}\\) & 0.625 \\({}^{\\ddagger}\\) & 0.575 & 0.667 & 0.490 & 0.509 \\\\\n' +
      '**Instructor-XL +** & & & & & & \\\\ LogisticReg & 0.210 \\({}^{\\dagger}\\) & 0.495 \\({}^{\\dagger}\\) & 0.714 \\({}^{\\ddagger}\\) & 0.585 & **0.655** & 0.602 & 0.574 \\\\ LightGBM & 0.172 \\({}^{\\dagger}\\) & 0.479 \\({}^{\\dagger}\\) & 0.592 \\({}^{\\dagger}\\) & 0.530 & 0.586 & 0.453 & 0.469 \\\\ MLP & **0.262**\\({}^{\\dagger}\\) & **0.550**\\({}^{\\dagger}\\) & 0.602 \\({}^{\\ddagger}\\) & 0.575 & 0.738 & 0.475 & 0.511 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4. Lightweight distilled classifiers perform on par with or better than GPT-4 on multilabel classification: Results on BingChat-Phase2-S-Eng using human-annotated gold labels.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Oracle** & \\multicolumn{3}{c}{**Human Annot.**} & \\multicolumn{3}{c}{**GPT-4 Annot.**} \\\\ \\cline{3-6}  & Accur. & F1 macro & \\multicolumn{3}{c}{Accuracy} \\\\ \\cline{3-6}  & & All & English & Non-Eng. \\\\ \\hline \\multicolumn{6}{c}{**User Intent**} \\\\ \\hline GPT-4 & 0.655 & **0.640** & & & \\\\\n' +
      '**ADA2 +** & & & & \\\\ LogisticReg & **0.658**\\({}^{\\Box}\\) & 0.639 & **0.746** & **0.763**\\({}^{\\circ}\\) & 2.35 & **0.725**\\({}^{\\circ}\\) \\\\ LightGBM & 0.642 \\({}^{\\Box}\\) & 0.536 & 0.702 & 0.716 \\({}^{\\circ}\\) & 2.05 & 0.686 \\({}^{\\circ}\\) \\\\ MLP & 0.658 \\({}^{\\Box}\\) & 0.602 & 0.744 & 0.762 \\({}^{\\circ}\\) & 2.45 & 0.722 \\({}^{\\circ}\\) \\\\\n' +
      '**Instructor-XL +** & & & & & \\\\ LogisticReg & 0.655 \\({}^{\\Box}\\) & 0.611 & 0.687 & 0.745 \\({}^{\\circ}\\) & 8.45 & 0.619 \\({}^{\\circ}\\) \\\\ LightGBM & 0.602 \\({}^{\\ddagger}\\) & 0.455 & 0.652 & 0.705 \\({}^{\\circ}\\) & 8.11 \\({}^{\\circ}\\) & 0.589 \\({}^{\\circ}\\) \\\\ MLP & 0.650 \\({}^{\\Box}\\) & 0.593 & 0.691 & 0.750 \\({}^{\\circ}\\) & 0.521 \\({}^{\\circ}\\) & 10.15 \\\\ \\hline \\multicolumn{6}{c}{**Conversation Domain**} \\\\ \\hline GPT-4 & 0.638 & **0.603** & & & & \\\\\n' +
      '**ADA2 +** & & & & & \\\\ LogisticReg & 0.640 \\({}^{\\Box}\\) & 0.573 & **0.705** & **0.733**\\({}^{\\circ}\\) & **0.673**\\({}^{\\circ}\\) & 4.67 \\\\ LightGBM & 0.560 \\({}^{\\ddagger}\\) & 0.476 & 0.633 & 0.656 \\({}^{\\circ}\\) & 3.85 & 0.605 \\({}^{\\circ}\\) & -4.45 \\\\ MLP & **0.650**\\({}^{\\Box}\\) & 0.583 & 0.703 & 0.731 \\({}^{\\circ}\\) & 4.15 & 0.669 \\({}^{\\circ}\\) & -4.85 \\\\\n' +
      '**Instructor-XL +** & & & & & \\\\ LogisticReg & 0.622 \\({}^{\\Box}\\) & 0.562 & 0.639 & 0.711 \\({}^{\\ddagger}\\) & 11.35 & 0.553 \\({}^{\\circ}\\) & -13.35 \\\\ LightGBM & 0.588 \\({}^{\\ddagger}\\) & 0.505 & 0.583 & 0.646 \\({}^{\\circ}\\) & 10.95 & 0.508 \\({}^{\\circ}\\) & -12.85 \\\\ MLP & 0.648 \\({}^{\\Box}\\) & 0.569 & 0.639 & 0.712 \\({}^{\\ddagger}\\) & 0.553 \\({}^{\\circ}\\) & -13.45 \\\\ \\hline \\hline \\end{tabularexplore the combination of ILMs with embedding-based methods, or model distillation that fine-tunes a smaller model through instructions from a larger one. Evaluation continues to be a crucial and open challenge for future work, and we plan to explore ways of performing more robust LLM-aided evaluations in future work, for example by fine-tuning a model to expand its reasoning capabilities beyond pairwise judgement tasks. While this work has focused largely on text mining in the conversational domain, we also hope to explore the extensibility of our framework to other domains as well. Finally, many domains have ethical considerations from the perspective of privacy and security that must be taken into account when performing large-scale automated text mining, and we hope to engage with these challenges more deeply in future work.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* C Aggarwal and Zhai (2012) Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering algorithms. Mining text data. (2012), 77-128.\n' +
      '* Arthur et al. (2007) David Arthur, Sergei Vasilvitskii, et al. 2007. k-means++: The advantages of careful seeding. In _Soda_, Vol. 7. 1027-1035.\n' +
      '* Bottou (1998) Leon Bottou. 1998. Online algorithms and stochastic approximations. _Online learning in neural networks_ (1998).\n' +
      '* Cambazoglu et al. (2021) B B Baria Cambazoglu, Leila Tavakoli, Falk Scholer, Mark Sanderson, and Bruce Croft. 2021. An intent taxonomy for questions in asked in web search. In _Proceedings of the 2021 Conference on Human Information Interaction and Retrieval_. 85-94.\n' +
      '* Chang et al. (2009) Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. 2009. Reading texts: How humans interpret topic models. _Advances in neural information processing systems_ 22 (2009).\n' +
      '* Cohen (1960) Jacob Cohen. 1960. A coefficient of agreement for nominal scales. _Educational and psychological measurement_ 20, 1 (1960), 37-46.\n' +
      '* Fleiss and Cohen (1973) Joseph I. Fleiss and Jacob Cohen. 1973. The equivalence of weighted kappa and the intraclass correlation coeffact measures of reliability. _Educational and psychological measurement_ 33, 3 (1973), 613-619.\n' +
      '* Glindysr & Alizadeh (2023) Fabrizio Glindysr, Metsz Alizadeh, and Mali Kublik. 2023. Chatopt outperforms crowd-workers for text-annotation tasks. _arXiv preprint arXiv:2303.1506_ (2023).\n' +
      '* Haykin (1998) Simon Haykin. 1998. _Neural networks: a comprehensive foundation_. Prentice Hall PTR.\n' +
      '* Hotho et al. (2005) Andreas Hotho, Andreas Nurnberger, and Gerhard Paafl. 2005. A brief survey of text mining. _Journal for Language Technology and Computational Linguistics_ 20, 1 (2005), 19-62.\n' +
      '* Joulin et al. (2016) Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. 2016. FastTextrap: Compressing text classification models. _arXiv preprint arXiv:1612.0867_ (2016).\n' +
      '* Joulin et al. (2016) Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efficient Text Classification. _arXiv preprint arXiv:1607.0759_ (2016).\n' +
      '* Ko et al. (2017) Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting decision tree. _Advances in neural information processing systems_ 30 (2017).\n' +
      '* Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. _CoRR_ abs/1412.6980 (2014). [https://api.semanticscholar.org/CorpusID:6628106](https://api.semanticscholar.org/CorpusID:6628106)\n' +
      '* Lee et al. (2023) Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryon White, and Sujay Jauhar. 2023. Making Large Language Models Better Data Creators. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Houda Boumor, Juan Pinno, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 15349-15360. [https://doi.org/10.18635/v1/2023.emnlp-main.948](https://doi.org/10.18635/v1/2023.emnlp-main.948)\n' +
      '* Liu et al. (2023) Nelson F Lu, Kevin Lim, John Hewitt, Ashwin Parangang, Michele Bevlacqua, Fabio Perroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. _arXiv preprint arXiv:2307.03172_ (2023).\n' +
      '* McLachlan and Barford (1988) Geoffrey J McLachlan and Kay E Bastford. 1988. _Mature models: Inference and applications to clustering_, Vol. 38. M. Dekker New York.\n' +
      '* Pham et al. (2023) Chau Minh Pham, Alexander Hoyle, Simeg Sun, and Mohit Iyyer. 2023. TopicCPT: A Prompt-based Topic Modeling Framework. _arXiv preprint arXiv:2311.01449_ (2023).\n' +
      '* Pryzant et al. (2023) Reid Priyzant, Dan Iker, Jerry Li, Yin Lee, Chengquang Zhu, and Michael Zeng. 2023. Automatic Prompt Optimization with "Gradient Descent" and Beam Search. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Houda Boumor, Juan Pinno, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 7957-7968. [https://doi.org/10.18653/v1/2023.emnlp-main.494](https://doi.org/10.18653/v1/2023.emnlp-main.494)\n' +
      '* Rose and Levinson (2004) Daniel E Rose and Danny Levinson. 2004. Understanding user goals in web search. In _Proceedings of the 13th international conference on World Wide Web_, 13-19.\n' +
      '* Rousseeuw (1987) Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. _Journal of computational and applied mathematics_ 20 (1987), 53-65.\n' +
      '* Sandhaus (2008) Evan Sandhaus. 2008. The new york times annotated corpus. _Linguistic Data Consortium, Philadelphia_ 6, 12 (2008), e26752.\n' +
      '* Shah et al. (2023) Chirag Shah, Ryon W White, Reid Andersen, Georg Buscher, Scott Counts, Sarkar Singhala Sarathi Das, Ali Muncar Sathiam, Jennifer Neville, Xiaochuan Ni, et al. 2023. Using large language models to generate, validate, and apply user intent taxonomies. _arXiv preprint arXiv:2309.13063_ (2023).\n' +
      '* Shang et al. (2020) Jingbong Shang, Xinyang Zhang, Liyuan Liu, Sha Li, and Jiawei Han. 2020. Nettaxo: Automated topic taxonomy construction from text-rich models. In _Proceedings of the Web Conference_ 2020. 1908-1919.\n' +
      '* Socher et al. (2013) Richard Socher, Alex Perelylyin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_. 1631-1642.\n' +
      '* Su et al. (2022) Honggin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. 2022. One Embedder: Any Task-Inversion-Fintend Text Embeddings. [https://arxiv.org/abs/2212.09741](https://arxiv.org/abs/2212.09741)\n' +
      '* Tan et al. (1999) Ah-Hwee Tan et al. 1999. Text mining: The state of the art and the challenges. In _Proceedings of the PAKDD 1999 workshop on knowledge discovery from advanced databases_, Vol. 8. 65-70.\n' +
      '* Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2023. Large language models can accurately predict searched preferences. _arXiv preprint arXiv:2309.10621_ (2023).\n' +
      '* Yvansany and Kumar (2020) The Xvansany and Satish AP Kumar. 2020. A review of topic modeling methods. _Information Systems_ 94 (2020), 101582.\n' +
      '* Wang et al. (2023) Zihan Wang, Jingbo Shang, and Ruiqi Zhong. 2023. Goal-Driven Explainable Clustering via Language Descriptions. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, Houda Boumor, Juan Pinno, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 10626-1064. [https://doi.org/10.18653/v1/2023.emnlp-main.657](https://doi.org/10.18653/v1/2023.emnlp-main.657)\n' +
      '* Welivitz and Pur (2020) Amuradha Welivitz and Pearl Pur. 2020. A Taxonomy of Empathetic Response. In Human Social Conversations. In _Proceedings of the 28th International Conference on Computational Linguistics_. 4886-4899.\n' +
      '* Zeng et al. (2021) Qingkai Zeng, Jinfeng Lin, Wenhhao Yu, and Cleland-Huang, and Meng Jiang. 2021. Enhancing taxonomy completion with concept generation via fusing relational representations. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_. 2104-2113.\n' +
      '* Zhang et al. (2018) Chao Zhang, Fangbo Tao, Xiusi Chen, Jianting Shen, Meng Jiang, Brian Sadler, Michelle Vanni, and Jiawei Han. 2018. TaxoSP: Unsupervised topic taxonomy construction by adaptive term embedding and clustering. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 2701-2709.\n' +
      '* Zijms et al. (2023) Cablez Zijms, William Held, Omar Shaikh, Jiaoa Chen, Zhehao Zhang, and Diyi Yang. 2023. Can Large Language Models Transform Computational Social Science? _arXiv preprint arXiv:2305.03514_ (2023).\n' +
      '\n' +
      '## Appendix A Taxonomies\n' +
      '\n' +
      'The user intent taxonomy and conversation domain taxonomy used in the label assignment phase are provided in Tables 5 and 6. Note although the label name and the majority of label description are automatically generated through our **TnT-LLM** framework, we did perform a lightweight human calibration on these generated taxonomies and added artificial examples. These examples are purely for illustration purpose and do not link to any particular data point in our corpus.\n' +
      '\n' +
      '## Appendix B Additional Results\n' +
      '\n' +
      'We present additional results from the experiments conducted for taxonomy generation phase and the label assignment phase.\n' +
      '\n' +
      '### Phase 1: Taxonomy Generation\n' +
      '\n' +
      'In addition to the taxonomy evaluation results on **BingChatPhase1-S-Eng**, we also investigate how the label taxonomy outcome from our proposed **TnT-LLM** framework perform across different languages. We present the label accuracy results from the GPT-4 rator in Figure 5, where we generally do not find significant differences of its performance on English conversations and non-English conversations.\n' +
      '\n' +
      '### Phase 2: Label Assignment\n' +
      '\n' +
      '#### b.2.1. Annotation Agreement Analysis\n' +
      '\n' +
      'We conduct in-depth investigation on the agreement results among human annotators and the LLM annotator for the label assignment task. The agreement results between different pairs of human annotators are presented in Figure 6. The confusion matrix between the GPT-4 annotations and (resolved) human annotations for the primary label on **BingChatPhase2-S-Eng** dataset is provided in Figure 7. We notice that for user intent, most disagreements occur at the boundary between "Fact-based information seeking" and "Clarification and concept explanation", "General solution and advice seeking" and "Technical assistance and problem solving". This suggests that human annotators and the GPT-4 annotator have different judgments on how "technical" or how much elaboration a user query requires. Note all our human annotators have high technical expertise, which may lead them to apply different implicit standards than the general population, resulting in potentially biased annotations. We observe similar patterns in the domain label assignment task, where "General digital support" and "Software development and hardware issues" are often confused, and the GPT-4 annotator has a high false positive rate on the "Software development and hardware issues" if human annotations are considered as oracle. We argue that this kind of analysis can help us identify and reduce potential biases in both human annotations and LLM annotations, and thus improve the clarity of the label description in the taxonomy and the consistency of label annotations.\n' +
      '\n' +
      '#### b.2.2. Full Classification Results\n' +
      '\n' +
      'We present the full multiclass classification results from predicting the primary label of a conversation in Figure 11, the full multilabel classification results from predicting all applicable labels in Figure 12, and the by language classification results in Figure 13. We confirm that the conclusions in Section 5.3 still hold.\n' +
      '\n' +
      '## Appendix C Implementation Details\n' +
      '\n' +
      '### Pipeline Design and Detailed Techniques\n' +
      '\n' +
      'We discuss the details of our LLM-based framework in this section. The rationale of these design details is to ensure that our proposed framework is executable, robust, and can be validated via quantitative metrics.\n' +
      '\n' +
      '**Executability and Robustness.** A key challenge is how to reliably execute the framework, especially when a prompt chain is involved where the states are dependent on the previous outputs. To address this, we explicitly state the output format in our prompts using predefined xml tags, such as "<output-output taxonomy in markdown table format</output>. This allows us to parse the outcomes from each step of the prompt chain and feed them to the next step. Moreover, we instruct the LLMs to format the taxonomy as a markdown table with a predefined schema, which includes the name, description, and index of each label. By asking the LLMs to output the name and the index of the assigned label together, we improve the consistency of the label assignment outputs and reduce the potential post-processing effort.\n' +
      '\n' +
      'However, we acknowledge that LLMs may not always follow the format instruction perfectly. Therefore, we propose the following strategy to increase the robustness of the pipeline execution. Specifically, we design a few guardarial tests for each type of LLM prompts. These tests include: 1) checking whether the output from a prompt adheres to the specified format that can be successfully parsed; 2) verifying whether the output is in the correct language (English) specified in the prompt, especially for the summarization prompt; 3) ensuring whether the output satisfies a key verifiable requirement given in the prompt instruction, such as the maximum number of labels in the output taxonomy. These metrics not only measure the **instruction-following** ability of an LLM system, but also provide\n' +
      '\n' +
      'Figure 5. Taxonomy evaluation results by language on multilingual conversations (**BingChat-Phase1-L-Multi**) from the GPT-4 rater.\n' +
      '\n' +
      'Figure 6. Pairwise agreement (in Cohen’s Kappa) between human annotators on the label assignment task.\n' +
      '\n' +
      'a quality assurance test suite to enhance the executability of the framework.\n' +
      '\n' +
      'We also specify a maximum number of retries (5 in our experiments) and a base temperature for each LLM call. If the outcome from an LLM call cannot pass the guardrail tests, we increase the temperature by 0.1 and allow it to try again until reaching the limit. Although there are still cases where LLMs fail to follow the instruction after exhausting the retry quota, empirically we find that this strategy largely increases the executability of our LLM-based framework.\n' +
      '\n' +
      '**"Model" Selection.** We draw inspiration from the practice of applying stochastic gradient descent in classic machine learning optimization. Our taxonomy generation and update pipeline does not guarantee the convergence to a global, but we can leverage an external validation step to perform\'model\' selection in a more principled way. To this end, we devise an evaluation prompt that takes as input a pair of or multiple taxonomies, a batch of text summaries, a use case instruction along with the taxonomy requirements, and then outputs the index of the taxonomy that best fits the data and complies with the requirements.4 We apply the evaluation prompt on a validation set that is separate from the training set used by the update prompts. After each or every few update steps, we compare the updated taxonomy and the best taxonomy that has been tracked on the validation set using the evaluation prompt. Once the update prompt chain is completed, the best taxonomy is passed to the final review step. This process simulates the conventional stochastic optimization practices, where the \'early stopping\' criteria can also be applied.\n' +
      '\n' +
      'Footnote 4: Note to mitigate the potential position bias [16] in such kind of single-choice or pairwise selection evaluation, we always randomize the position of each option and run the evaluation multiple times in all of our experiments.\n' +
      '\n' +
      '**Efficiency Analysis and Sample Size Suggestion.** The efficiency of our pipeline depends on the choice of the corpus sample size and the LLM system for each phase. For the taxonomy generation phase (Phase 1), we suggest using a\'small-to-medium\' size corpus sample that is representative of the whole corpus. The sample size (\\(N\\)) should be large enough to capture the diversity of the corpus, but not too large to incur unnecessary computational costs. In our experiments, we found that a sample size around 10k was sufficient to produce a high quality taxonomy with no more than 100 labels. The most computationally intensive stage of this phase is the summarization stage, which requires calling an LLM at least \\(N\\) times to generate summaries for the entire corpus sample. This stage can be skipped if the input texts are short and normative, or replaced by a cheaper or more specialized summarization model. The generation and update prompt chain requires an LLM system with high reasoning capacity and large context window. We used GPT-4 (with 32k context window) and GPT-3.5-Turbo (with 16k context window) in our experiments, and was able to achieve efficiency with proper batching (with a batch size of 200). We observed that GPT-3.5-Turbo was 5x-10x faster than GPT-4, but may compromise the quality of the final label taxonomy outcome.\n' +
      '\n' +
      'For the label assignment and classifier development phase (Phase 2), we recommend using a\'medium-to-large\' size corpus sample that covers the range of labels in the taxonomy. The sample size needed also depends on the difficulty of the classification task and the effectiveness of the representation model used. Since this phase involves applying an LLM on the entire sample, we suggest starting with a\'medium\' size sample for model development, and increasing it as needed.\n' +
      '\n' +
      'Figure 7: **The confusion matrix of the primary labels assigned by human annotators and the GPT-4 annotator.**\n' +
      '\n' +
      '### Experiment Details\n' +
      '\n' +
      '**LLM Configurations.** We used the following fixed parameter configurations for all prompts applied in this work: frequency_penalty=0, presence_penalty=0, top_p=0. 5. We purposely apply a higher temperature for the taxonomy generation prompt chain to elicit the generation power of LLMs. The base temperature is set to 0.5 for the "generation" prompt, and 0.2 for the "update" prompt. Base temperature is set to 0 for all other prompts in our experiments.\n' +
      '\n' +
      '**Hyperparameter Selection.** For classifiers presented in Section 5.3, we perform grid search based on their accuracy performance on the validation set as the follows.\n' +
      '\n' +
      '* **Logistic Regression**: An \\(\\ell_{2}\\) regularizer is applied and \\(\\lambda\\) is selected from \\([0.01,0.1,1,10]\\).\n' +
      '* **LightGBM**: We use the default number of leaves (31) in the official **LightGBM** package and the maximum depth is selected from \\([3,5,7,9]\\).\n' +
      '* **MLP**: We apply an Adam (King and Ba, 2014) optimizer with weight decay set to \\(1e-5\\) and a learning rate 0.001. The size of the hidden layer is selected from \\([32,64,128,256]\\).\n' +
      '\n' +
      '**Instruction Following Results.** In addition to the results reported in Sections 5.2 and 5.3, we also evaluate the instruction following ability of the two LLM systems applied in our experiments. For the first summarization stage of our proposed taxonomy generation pipeline (Stage 1 in Section 3.1), we primarily evaluate 1) if the output can be successfully parsed based on the predefined format in the prompt (i.e., format check) and 2) if the output complies with the language specified in the prompt (i.e., English). We found that GPT-4 performed flawlessly, passing 100% of the format and language checks. GPT-3.5-Turbo, on the other hand, had a very low failure rate for the format check (-0.01%) and a slightly higher failure rate for the language check (around 2%). However, we also notice that 0.3% of GPT-3.5-Turbo outputs passed the strict format check, but copied the instruction into the XML tags. Given the overall instruction following success rate is high and our taxonomy generation pipeline is relatively robust to minor perturbations of the input batch, we discard the conversations that did not pass the instruction following test for GPT-3.5-Turbo in the subsequent stage. For the taxonomy generation and update stage (Stage 2 in Section 3.1), we evaluate if the prompt chain can successfully complete for each of 10 epoch runs, which requires that all the intermediate taxonomy outcomes 1) can be successfully parsed (i.e., format check) and 2) comply with the predefined taxonomy size limit (i.e., max number of generated labels). GPT-4 again performed flawlessly, completing 10 out of 10 epochs for both taxonomies. GPT-3.5-Turbo, however, struggled on this stage, primarily because of it persistently exceeding the taxonomy size limit in the \'Update\' step. At the end, it only completed 4 out of 10 epochs for the intent taxonomy and 1 out of 10 epochs for the domain taxonomy. For the native label assignment stage, we find both GPT-4 and GPT-3.5-Turbo are able to pass the format check close to 100%.\n' +
      '\n' +
      '## Appendix D Prompt Templates\n' +
      '\n' +
      'In this section, we present the prompt templates that were used for conversation summarization (Figure 8), label assignment (Figure 9), and taxonomy generation (Figure 10a), updation (Figure 10b) and review (Figure 10c).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      'Figure 11. Results from predicting the primary label.\n' +
      '\n' +
      'Figure 12. Results from predicting all applicable labels.\n' +
      '\n' +
      '## 6. Conclusion\n' +
      '\n' +
      'Figure 13. Results by language (English vs. non-English conversations) from predicting both the primary label and all applicable with GPT-4 annotations as the oracle on the large multilingual test set.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Label Name & Label Description \\\\ \\hline Website Navigation Requests & User seeks to visit a very specific website or web page by providing a URL, or keywords that ”indicate the name or domain of the website”, e.g., ”amazon.com”, ”gmail login”. \\\\ Fact-Based Information Seeking & User seeks factual and descriptive information on a specific topic, product, or service. These user queries can be answered by retrieving the factual information that ”already exists in the sources” and require ”a high level of specificity” and ”low level of subjectivity”, e.g., ”What is the capital of France?”. \\\\ Clarification and Concept Explanation & User asks AI to explain various topics or concepts, or seeks clarification or confirmation on a matter, by providing a question that requires more than a factual or a descriptive answer, but rather “an interpretation, definition, or elaboration”, e.g., ”What is the difference between AI and machine learning?”. \\\\ General Solution and Advice Seeking & User seeks general solutions, advice, instructions, or steps on a ”non-technical” topic, product, or service, by providing a problem, goal, or scenario that requires more than a factual or descriptive answer, but rather ”a recommendation, suggestion, or guidance”, e.g., ”What should I buy for my friend’s birthday?”. \\\\ Technical Assistance and Problem Solving & User seeks help with ”technical” issues or problem-solving related to a product, service, or system, by providing a description of the issue, error, or challenge that requires more than a factual or descriptive answer, but rather ”a diagnosis, solution, or workaround”, e.g., ”How to fix the bug in my code?”. \\\\ Language Translation Requests & User requests translation or interpretation of a phrase or sentence “from one language to another”, e.g., ”Hello” in Spanish”. \\\\ Content Creation and Storytelling Requests & User requests the ”creation of original content” such as images, stories, instructions, summaries, or narratives on a specific topic or theme, e.g., ”Create an image of a unicorn in a forest”. \\\\ Planning and Scheduling & User seeks assistance with planning an event, trip, or schedule, e.g., ”Plan a birthday party for my mom”. \\\\ Data Analysis and Calculation Requests & User asks for quantitative data analysis, calculations, or statistical interpretations, by providing the source of the data and the desired operation or result, e.g., ”Calculate the average of these numbers”, ”Analyze the sales data for last quarter”. \\\\ Greetings and Social Interactions & User greets the AI agent or engages in social interactions, by providing a salutation, expression, or remark, or requesting to play games with the AI, which ”does not require a factual, descriptive, or technical answer”, but rather an engaging, polite or humorous response, e.g., “Hello, how are you?”, ”You’re very smart”. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5. The user intent taxonomy used in the label assignment experiments. Note all presented examples are artificial and do not link to any particular data point in our corpus.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Label Name & Label Description \\\\ \\hline Academic Resources & Requests for educational resources, explanations of “general” academic concepts, and academic advice, e.g., \\\\  & *Best college for computer science*, “How to prepare for the SAT?”. \\\\  & *Requests for translations, text editing, or discussions about grammar, syntax, and other linguistic concepts, e.g., “Translate “Hello’ to French”, “What is the difference between ‘affect’ and ‘effect’?”. \\\\  & Mathematics, Logics and Data Science \\\\  & *Queries and discussions related to concepts, theories, and problems in the fields of mathematics and logics, or related to machine learning and data science, e.g., “How to calculate standard deviation?”, “What is the difference between boosting and bagging?”. \\\\  & *Queries and discussions related to concepts, theories, and problems in the fields of physics and chemistry, e.g., “What is the speed of light?”, “What is the atomic number of carbon?”. \\\\  & *Discussions about “business operations”, “industry developments”, and related information, e.g., “What is the best business strategy for a startup?”, “Generate a FAQ page for a healthcare product website”. \\\\  & *Economics and Finance \\\\  & *Discussions about economic concepts and theories, financial products, investment advice, and related queries, e.g., “What is the current inflation rate?”, “What is the best investment strategy in 2024?”. \\\\  & *Requests for job applications, career advice, and related information, e.g., “What is the best career path for a data scientist?”. \\\\  & *Queries about legal terms, regulations, and related information, e.g., “What is the legal drinking age in the US?”, “What are the regulations for AI development in EU countries?”. \\\\ Art, Design and Creativity & *Requests for ‘image creation and creative writing”, or discussions about “art, design and creative concepts*, e.g., “Create a logo for my company”, “What is the difference between modern art and contemporary art?”. \\\\  & *Discussions about movies, music, games, game development, and other forms of entertainment, e.g., “Who is the director of the movie “Oppenheimer’?”. \\\\  & *Requests for playing games, or engaging in “interactive activities with the AI’, e.g., “Play a game with me”, “Tell me a joke”. \\\\  & *Coverstations about “personal” hobbies, lifestyle choices, and individual interests, e.g., “How to learn to play the guitar as a beginner?”. \\\\  & *Sports and Fitness \\\\  & *Conversations about “sports events”, “fitness advice”, and related topics, e.g., “Who will play in the NBA finals?”, “Training tips for marathon”. \\\\  & *Conversations about food recommendations, nutritional information, and cooking advice., e.g., “How to make a pizza?”. \\\\  & *Health and Wellness \\\\  & *Discussions about health conditions, treatments, and wellness information, e.g., “Is cancer curable?”, “Best practices to improve sleep quality”. \\\\  & *General Digital Support \\\\  & *Conversations related to the AI’s abilities, limitations, functionality, task requests, and technical support for “general” digital products or services, e.g., “What can Bing Chat do?”, “How to take a screenshot on mackob?”. \\\\  & *Software Development and Hardware Issues \\\\  & *Conversations about “coding”, “software configuration*, “development tools*, and specific software or “hardware issues* and their solutions, e.g., “How to install python on mackob?”, “How to fix a broken external hard drive?”. \\\\  & *Home and Household Issues \\\\  & *Queries about home maintenance, household issues, and related advice, e.g., “How to clean a microwave oven?”. \\\\  & *Animals and Nature \\\\  & *Queries about animals, nature, and related information, e.g., “What is the pH value of water?”, “What is the average lifespan of a cat?”. \\\\  & *Geography, Climate and Environment \\\\  & *Geography, Climate and Environment \\\\  & *Geography, Climate and Environment \\\\  & *History and Culture \\\\  & *History and Culture \\\\  & *Personal Counseling and Emotional Support \\\\  & *Social and Political Issues \\\\  & *Product and Shopping Queries \\\\  & *Travel and Tourism \\\\  &\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>