<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion\n' +
      '\n' +
      'Tairan He\\({}^{\\dagger\\dagger}\\) Chong Zhang\\({}^{2\\ddagger}\\) Wenli Xiao\\({}^{1}\\) Guanqi He\\({}^{1}\\) Changliu Liu\\({}^{1}\\) Guangya Shi\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Carnegie Mellon University \\({}^{2}\\)ETH Zurich\n' +
      '\n' +
      '\\({}^{\\dagger}\\)Equal Contributions [https://agile-but-safe.github.io](https://agile-but-safe.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Legged robots navigating cluttered environments must be jointly _agile_ for efficient task execution and _safe_ to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (\\(<1.0\\) m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an extereception representation network, all in simulation. These trained modules can be directly deployed in the real world with onboard sensing and computation, leading to high-speed and collision-free navigation in confined indoor and outdoor spaces with both static and dynamic obstacles (Figure 1).\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'Agile locomotion of legged robots in cluttered environments presents a non-trivial challenge due to the inherent trade-off between agility and safety, and is crucial for real-world applications that require both robustness and efficiency. Existing works typically exhibit limited agility (velocity \\(<1\\) m/s) to ensure safety [35, 17, 7, 22, 27, 67, 12, 46, 41, 72], or focus solely on maximizing agility without considering safety in navigation scenarios [45, 53]. Our work distinguishes itself by achieving high-speed (max velocity \\(>3\\) m/s), collision-free quadrupedal locomotion in cluttered environments.\n' +
      '\n' +
      'The agility limitations in existing works in the navigation domain stem from varied factors. Regarding the formulation, some decouple locomotion and navigational planning into two subtasks and build hierarchical systems [35, 17, 7, 27, 46, 72]. Such decoupling not only constrains the controller from the optimal solution [55], but also results in conservative behaviors to ensure safety, thereby limiting the system from fully unleashing the locomotion agility. This work, instead, learns end-to-end controllers that directly output joint-level actions for collision-free locomotion to reach specified goal positions. Our approach is inspired by recent works [66, 50, 71, 28] where robots learn end-to-end controllers to overcome challenging terrains by integrating locomotion with navigation.\n' +
      '\n' +
      'Regarding the controller, some works employ model-based methods with simplified models, such as model predictive control (MPC) and barrier functions, for guaranteed safety [22, 12, 41]. The model mismatch and potential constraint violations such as slippage, together with the online computational burden, limit these controllers from agile motions and stable deployment in the wild [22, 32, 38]. On the other hand, recent progress of model-free reinforcement learning (RL) in legged locomotion has demonstrated remarkable agile motor skills that model-based controllers have not achieved [45, 53, 28, 38, 47, 34, 75, 43, 69, 11], although potentially unsafe in cluttered environments. We harness the flexibility and agility of model-free RL and further safeguard it using control-theoretic tools.\n' +
      '\n' +
      'Named ABS, our framework goes beyond a single RL policy. First, we have a model-free perceptive agile policy that incorporates collision avoidance into locomotion, as presented in Section IV, enabling our Go1 robot to achieve peak speeds up to \\(3.1\\) m/s while being aware of collisions. However, the RL policy does not guarantee safety, so we safeguard the robot with another recovery policy (see Section VI) when the agile policy may fail. To decide which policy to take control, we use a policy-conditioned reach-avoid (RA) value network to quantify the risk level of the agile policy. This is inspired by [29] where model-free RA values can be efficiently learned based on the Hamilton-Jacobi reachability theory [3]. The RA value network is trained by a discounted RA Bellman equation, with data collected by the learned agile policy in simulation. Beyond being a threshold, the differentiable RA value network also provides gradient information to guide the recovery policy, thus closing the loop, which will be further presented in Section V.\n' +
      '\n' +
      'To get collision avoidance behaviors that can generalize in different scenarios, we use a low-dimensional exteroceptive feature for policy and RA value training: the traveling distances of several rays cast from the robot to obstacles. In order to achieve this, we additionally train an exteroception representation (or ray-prediction) network with simulated data, which maps depth images to ray distances as detailed in Section VII. By doing so, we achieve robust collision avoidance in high-speed locomotion with onboard sensing and computation.\n' +
      '\n' +
      'Briefly, we identify our contributions as follows:\n' +
      '\n' +
      '1. A perceptive agile policy for obstacle avoidance in high-speed locomotion with novel training methods.\n' +
      '2. A novel control-theoretic data-driven method for RA value estimation conditioned on the learned agile policy.\n' +
      '3. A dual-policy setup where an agile policy and a recovery policy collaborates for high-speed collision-free locomotion, and the RA values govern the policy switch and guide the recovery policy.\n' +
      '4. An exteroception representation network that predicts low-dimensional obstacle information for generalizable collision avoidance capability.\n' +
      '5. Validation of ABS\'s superior safety measures and state-of-the-art agility amidst obstacles both indoors and outdoors (Figure 1).\n' +
      '\n' +
      '## II Related Works\n' +
      '\n' +
      '### _Agile Legged Locomotion_\n' +
      '\n' +
      'Model-based methods such as MPC use simplified models and handcrafted gaits to enable dynamic legged locomotion [5, 14, 15, 24, 33, 25]. Despite their impressive performance in simulation and under laboratory conditions, these methods struggle in the wild due to model mismatch and unexpected slippage [32, 38]. The online computational burden also limits perceptive model-based controllers from agile motions [12].\n' +
      '\n' +
      'Recently, RL-based controllers have shown promising results for robust locomotion [66, 38, 47, 23] and agile motor skills including high-speed running [31, 45, 53], challenging terrain traversal [71, 28, 34, 75, 11], jumping [39, 69], and fall recovery [66, 43, 62, 70]. However, existing works on agile locomotion mostly show to achieve fast speeds for racing or skillful motions to overcome challenging terrains. In cluttered environments, these methods necessitate a high-level navigation module for collision avoidance, which is typically conservative and greatly constrains the motion far below the motor limit [63, 72]. In contrast, this paper studies agile collision avoidance for versatile navigation.\n' +
      '\n' +
      '### _Legged Collision Avoidance_\n' +
      '\n' +
      'Classical methods tackle collision avoidance in legged robots with collision-free motion planning [35, 17, 7] in the configuration space without considering the robot dynamics, leading to slow and statically stable gaits. MPC-based methods [22, 12, 57, 41] integrate planning and control by treating distances to obstacles as optimization constraints. However, they suffer from the aforementioned drawbacks of model-based controllers and slow movements (velocity \\(<0.5\\) m/s).\n' +
      '\n' +
      'Learning-based methods are another choice. Hoeller et al. [27] and Zhang et al. [72] train RL-based policies that output twist commands to be tracked by the locomotion controller, while the velocity commands are bounded by \\(1\\) m/s to ensure safety. However, the decoupling of navigation planning and locomotion control makes high-speed locomotion risky, as the high-level planner is unaware of the low-level tracking error.\n' +
      '\n' +
      'Yang et al. [67] instead provides an end-to-end RL-based solution that maps depth images and proprioceptive data directly to joint actions, but the robot can only walk forward and the velocity is limited to \\(\\sim 0.4\\) m/s. In contrast, our work deploys an end-to-end agile policy for omnidirectional rapid locomotion with collision avoidance, and safeguards the robot with RA values and a recovery policy. To the best of our knowledge, our work is the first to validate collision-free quadruped locomotion with maximum velocity up to \\(3.1\\) m/s. Even in tight space with dynamic adversarial obstacles, our system can still reach a peak velocity of \\(2.5\\) m/s and an average speed of \\(1.5\\) m/s (Figure 1 (f)).\n' +
      '\n' +
      '### _Safe Reinforcement Learning_\n' +
      '\n' +
      'There are two main categories of methods to perform safe RL [74]: 1) _end-to-end_ methods and 2) _hierarchical_ methods. Lagrangian-based methods [49, 4, 40, 58] are the most representative _end-to-end_ safe RL methods that solve a primal-dualoptimization problem to satisfy safety constraint where the Lagrange multipliers can be optimized along with the policy parameters. However, the constraint is often enforced before convergence, hindering exploration and lowering returns [49].\n' +
      '\n' +
      '_Hierarchical_ safe RL methods safeguard unsafe RL actions using structures of underlying dynamics [68, 65, 13] and control-theoretic safety certificates [48, 73, 10]. These methods typically build on the assumptions of available dynamics or safety certificate functions before learning, which heavily limits the scalability to high-dimensional complex systems. Some recent works learn safety prediction networks (or safety critics) and safety backup policies to safeguard RL when the safety critics indicate the nominal policy is unsafe [59, 30]. Nevertheless, these frameworks lack interplay between safety critics and backup policies, relying on the demanding assumption that the backup policy can restore safety without explicit optimization to satisfy the safety critics.\n' +
      '\n' +
      'Our approach aligns with the _hierarchical_ methods, yet it stands out with a distinctive strategy. We focus on estimating the reach-avoid values of the agile policy and feed the reach-avoid values\' gradient information back into the system to guide the recovery policy within a closed loop. This innovative approach enables a dynamic and adaptive recovery process. Notably, all our modules are trained in simulation using a model-free approach, enhancing the generalizability and scalability of our method.\n' +
      '\n' +
      '### _Reach-Avoid Problems and Hamilton-Jacobi Analysis_\n' +
      '\n' +
      'Reach-avoid (RA) problems involve navigating a system to reach a target while avoiding certain undesirable states. Hamilton-Jacobi (HI) reachability analysis [3] solves this problem by analyzing the associated Hamilton-Jacobi partial differential equation, which provides a set of states that the system must stay out of in order to remain safe.\n' +
      '\n' +
      'HJ reachability analysis faces computational challenges, which escalate exponentially with the system\'s dimensionality [9]. Recent learning-based methods [2] try to scale HJ reachability analysis to high-dimensional systems by learning value networks that satisfy the associated HJ partial differential equations and constraints. However, they still require explicit system Hamiltonian expression before learning.\n' +
      '\n' +
      'Our method builds on another line of works [29, 20] that leverage contraction properties to derive a time-discounted reach-avoid Bellman equation. However, contrary to previous works that learn policy-agnostic RA values during RL training, we instead learn a policy-conditioned RA value network. This not only reduces the computational burden by avoiding the identifiability issue of the global RA set but also best suits our trained agile policy.\n' +
      '\n' +
      '## III Overview and Preliminaries\n' +
      '\n' +
      '### _Nomenclature_\n' +
      '\n' +
      'We present important symbols and abbreviations that are used across this paper in Table I for reference.\n' +
      '\n' +
      '### _Problem Formulation_\n' +
      '\n' +
      '#### Iii-B1 Dynamics\n' +
      '\n' +
      'Let \\(s_{t}\\in\\mathcal{S}\\subset\\mathbb{R}^{n_{s}}\\) be the state at time step \\(t\\), where \\(n_{s}\\) is the dimension of the state space \\(\\mathcal{S}\\); \\(a_{t}\\in\\mathcal{A}\\subset\\mathbb{R}^{n_{a}}\\) be the control input at time step \\(t\\), where \\(n_{a}\\) is the dimension of the action space \\(\\mathcal{A}\\). The system dynamics are defined as:\n' +
      '\n' +
      '\\[s_{t+1}=f(s_{t},a_{t}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(f:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}\\) is a function that maps the current robot state and control to the next state. For simplicity, this paper considers deterministic dynamics that can be without an analytical form. We denote the robot observations from proprioception and/or extereception as \\(o_{t}=h(s_{t})\\) where \\(h:\\mathcal{S}\\rightarrow\\mathcal{O}\\) is the sensor mapping. Detailed observation and action space of the agile policy and recovery policy will be introduced in Section IV and Section VI.\n' +
      '\n' +
      '#### Iii-B2 Goal and Policy\n' +
      '\n' +
      'Goal-conditioned reinforcement learning [42] learns to reach goal states \\(G\\in\\Gamma\\) via a goal-conditioned policy \\(\\pi:\\mathcal{O}\\times\\Gamma\\rightarrow\\mathcal{A}\\). With the reward function \\(r:\\mathcal{S}\\times\\mathcal{A}\\times\\Gamma\\rightarrow\\mathbb{R}\\) and the discount factor \\(\\gamma_{\\text{RL}}\\). The policy is learned to maximize the expected cumulative return over the goal distribution \\(p_{G}\\):\n' +
      '\n' +
      '\\[J(\\pi)=\\mathbb{E}_{u_{t}\\sim\\pi(\\cdot|o_{t},G),G\\sim p_{G}}\\left[\\sum_{t}\\gamma_{ \\text{RL}}^{t}r(s_{t},a_{t},G)\\right]. \\tag{2}\\]\n' +
      '\n' +
      '#### Iii-B3 Failure Set, Target Set and Reach-Avoid Set\n' +
      '\n' +
      'We denote the _failure set_\\(\\mathcal{F}\\subseteq\\mathcal{S}\\) as unsafe states (e.g., collision) where the robot is not allowed to enter. The _failure set_ can be represented by the zero-sublevel set of a Lipschitz-continuous function \\(\\zeta:\\mathcal{S}\\rightarrow\\mathbb{R}\\), i.e., \\(s\\in\\mathcal{F}\\Leftrightarrow\\zeta(s)>0\\). The _target set_\\(\\Theta\\subset\\mathcal{S}\\) is defined as desired states (i.e., goal states). Similarly, the _target set_ can be represented by the zero-sublevel set of a Lipschitz-continuous function \\(l:\\mathcal{S}\\rightarrow\\mathbb{R}\\), i.e., \\(s\\in\\Theta\\Leftrightarrow l(s)\\leq 0\\). We denote \\(\\xi_{s_{t}}^{\\pi}(\\cdot)\\) as the future trajectory rollout from state \\(s_{t}\\) (\\(\\xi_{s_{t}}^{\\pi}(0)=s_{t}\\)) using policy \\(\\pi\\) up to \\(s_{T}\\). The _reach-avoid set_ conditioned on policy \\(\\pi\\) is defined as\n' +
      '\n' +
      '\\[\\mathcal{RA}^{\\pi}(\\Theta;\\mathcal{F}):=\\{s_{t}\\in\\mathcal{S} \\left|\\xi_{s_{t}}^{\\pi}(T-t)\\in\\Theta\\wedge\\right. \\tag{3}\\] \\[\\left.\\forall t^{\\prime}\\in[0,T-t],\\xi_{s_{t}}^{\\pi}(t^{\\prime}) \\notin\\mathcal{F}\\},\\right.\\]\n' +
      '\n' +
      'which represents the set of states governed by policy \\(\\pi\\) capable of leading the system to \\(\\Theta\\) while consistently avoiding \\(\\mathcal{F}\\) in all prior timesteps.\n' +
      '\n' +
      '#### Iii-B4 Reach-Avoid Value and Time-Discounted Reach-Avoid Bellman Equation\n' +
      '\n' +
      'We define policy-conditioned reach-avoid values as: \\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\leq 0\\Leftrightarrow s\\in\\mathcal{RA}^{\\pi}( \\Theta;\\mathcal{F})\\). Following the proof (Appendix A in [29]), it can be easily extended that value function \\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\) satisfies the fixed-point reach-avoid Bellman equation (our policy-conditioned value function is a special case of the general value function):\n' +
      '\n' +
      '\\[V_{\\text{RA}^{\\pi}}^{\\pi}(s)=\\max\\left\\{\\zeta(s),\\min\\left\\{l(s),V_{\\text{RA }^{\\pi}}^{\\pi}\\left(f\\left(s,\\pi(s)\\right)\\right)\\right\\}\\right\\}\\,. \\tag{4}\\]\n' +
      '\n' +
      'However, there is no assurance that Equation (4) will result in a contraction in the space of value functions. To make it accessible to data-driven approximation, we leverage time-discounted reach-avoid Bellman equation [29] to make a contraction on the discounted policy-conditioned reach-avoid values \\(V_{\\text{RA}}^{\\pi}(s)\\) defined as\n' +
      '\n' +
      '\\[V_{\\text{RA}}^{\\pi}(s)= \\gamma_{\\text{RA}}\\max\\left\\{\\zeta(s),\\min\\left\\{l(s),V_{\\text{ RA}}^{\\pi}\\left(f\\left(s,\\pi(s)\\right)\\right)\\right\\}\\right\\} \\tag{5}\\] \\[+(1-\\gamma_{\\text{RA}})\\max\\left\\{l(s),\\zeta(s)\\right\\}\\,.\\]\n' +
      '\n' +
      'Following [29], it can be shown that \\(V_{\\text{RA}}^{\\pi}(s)\\) is always an under-approximation of \\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\) for \\(\\gamma_{\\text{RA}}\\in[0,1)\\), and \\(V_{\\text{RA}}^{\\pi}(s)\\) converges to \\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\) as \\(\\gamma_{\\text{RA}}\\) approaches \\(1\\). Note that the under-approximation of \\(V_{\\text{RA}}^{\\pi}(s)\\) to \\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\) means that \\(V_{\\text{RA}}^{\\pi}(s)\\leq 0\\Rightarrow s\\in\\mathcal{RA}^{\\pi}(\\Theta; \\mathcal{F})\\), which enables that shielding methods on thresholds of \\(V_{\\text{RA}}^{\\pi}(s)\\) could make the system stay in the control-theoretic _reach-avoid set_\\(\\mathcal{RA}^{\\pi}(\\Theta;\\mathcal{F})\\).\n' +
      '\n' +
      'Fig. 2: Overview of ABS: (a) There are four trained modules within the ABS framework: 1) Agile Policy (introduced in Section IV) is trained to achieve the maximum agility amidst obstacles; 2) Reach-Avoid Value Network (introduced in Section V) is trained to predict the RA values conditioned on the agile policy as safety indicators; 3) Recovery Policy (introduced in Section VI) is trained to track desired twist commands (2D linear velocity \\(v_{x}^{\\pi},v_{y}^{\\pi}\\) and yaw angular velocity \\(v_{x}^{\\pi}\\)) that lower the RA values; 4) Ray-Prediction Network (introduced in Section VII) is trained to predict ray distances as the policies; asteroceptive inputs given depth images. (b) Illustration of the ABS deployment architecture. The dual policy setup switches between the _agile policy_ and the _recovery policy_ based on the estimated \\(\\hat{V}\\) from the _RA value network_: 1) if \\(\\hat{V}<V_{\\text{threshold}}\\), the _agile policy_ is activated to navigate amidst obstacles; 2) if \\(\\hat{V}>V_{\\text{threshold}}\\), the _recovery policy_ is activated to track twist commands that lower the _RA values_ via constrained optimization.\n' +
      '\n' +
      '### _System Structure_\n' +
      '\n' +
      'As shown in Figure 2, our proposed ABS framework involves a dual-policy setup where the agile policy \\(\\pi^{\\text{Agile}}\\) and the recovery policy \\(\\pi^{\\text{Recovery}}\\) work together to enable agile and safe locomotion skills. The agile policy performs agile motor skills (up to \\(3.1\\) m/s on Unitree Go1) to navigate the robot based on goal commands (target 2D positions and headings) with basic collision-avoidance ability (see also Section IV). The recovery policy is responsible for safeguarding the agile policy by rapidly tracking twist commands (2D linear velocity \\(v_{x}^{c},v_{y}^{c}\\) and yaw rate \\(\\omega_{z}^{c}\\)) that can avoid collisions (see also Section V-C and Section VI). Both policies output joint targets that are tracked by a PD controller.\n' +
      '\n' +
      'During deployment, the policy switch is governed by RA values conditioned on the agile policy, estimated using a neural network \\(\\hat{V}\\) (see also Section V). With a safety threshold \\(V_{\\text{threshold}}=-\\epsilon\\) where \\(\\epsilon\\) is a small positive number, we have:\n' +
      '\n' +
      '* If \\(\\hat{V}\\geq V_{\\text{threshold}}\\), we search for a twist command that drives the robot closer to the goal while maintaining safety based on \\(\\hat{V}\\) (see also Equation (21)). The recovery policy takes control and tracks the searched twist command.\n' +
      '* If \\(\\hat{V}<V_{\\text{threshold}}\\), the agile policy takes control.\n' +
      '\n' +
      'We expect the system to activate the agile policy in most time, and use the recovery policy as a safeguard in risky situations until it is safe again for the agile policy, _i.e._, \\(\\hat{V}<V_{\\text{threshold}}\\).\n' +
      '\n' +
      'For collision avoidance, both the agile policy and the RA value networks need exteroceptive inputs. Inspired by [28, 16, 1], we choose to use a low-dimensional exteroception representation: the distances that 11 rays travel from the robot to obstacles, similar to sparse LiDAR readings. We train a network that maps raw depth images to predicted ray distances (see also Section VII), and the ray distances serve as part of the observations for the agile policy and the RA value network.\n' +
      '\n' +
      'To summarize, as shown in Figure 2 (a), ABS needs to train four modules all in simulation:\n' +
      '\n' +
      '1. The _agile policy_ (Section IV) is trained via RL to reach the goal without collisions. We design goal-reaching rewards to encourage the most agile motor skills.\n' +
      '2. The _RA value network_ (Section V) is trained to indicate the safety for the agile policy. We use a data-driven method to train it based on the RA bellman equation (Equation (5)), and collect data in simulation by rolling out the agile policy.\n' +
      '3. The _recovery policy_ (Section VI) is trained to track twist commands rapidly from high-speed movements.\n' +
      '4. The _ray-prediction network_ (Section VII) is trained to predict ray distance observations from depth images. We collect synthetic depth images and ray distances in simulation by rolling out the agile policy.\n' +
      '\n' +
      'All of the four modules are directly deployed in the real world after training.\n' +
      '\n' +
      '## IV Learning Agile Policy\n' +
      '\n' +
      'As mentioned in Section III-C, we train an agile policy to achieve high agility amidst obstacles. Previous works on learning agile locomotion typically employ the velocity-tracking formulation [45, 53], _i.e._, to track velocity commands on open, flat terrains. However, designing a navigation planner for these velocity-tracking policies in cluttered environments can be non-trivial. To ensure safety, the planner may have to be conservative and unable to fully unleash the locomotion policy\'s agility.\n' +
      '\n' +
      'Instead, we use the goal-reaching formulation to maximize the agility, inspired by [50, 71]. Specifically, we train the robot to develop sensorimotor skills that enable it to reach specified goals within the episode time without collisions. The agility is also encouraged by a reward term pursuing high velocity in the base frame. By doing so, the robot naturally learns to achieve maximum agility while avoiding collisions.\n' +
      '\n' +
      'This section presents the details of our agile policy learning. A detailed comparison between goal-reaching and velocity-tracking formulations for agility will be presented in Section IX-A1.\n' +
      '\n' +
      '### _Observation Space and Action Space_\n' +
      '\n' +
      'The observation space of the agile policy consists of the foot contacts \\(c_{f\\in\\{1,2,3,4\\}}\\), the base angular velocities \\(\\omega\\), the projected gravity in the base frame \\(g\\), the goal commands \\(G^{c}\\) (i.e., the relative position and heading of the goal) in the base frame, the time left of the episode \\(T-t\\), the joint positions \\(q\\), the joint velocities \\(\\dot{q}\\), the actions \\(a\\) of the previous frame, and the exteroception (i.e., log values of the ray distances) \\(R\\). Here we omit the step-based timestamps (\\(t-1\\) for the actions and \\(t\\) otherwise) for brevity. We refer to the collection of all these variables as \\(o^{\\text{Agile}}\\).\n' +
      '\n' +
      'Among these observations, only \\(g\\) and \\(G^{c}\\) require the state estimators for respectively orientation and odometry. All other values are available from raw sensor data without cumulative drifts. The IMU-based orientation estimation for \\(g\\) (i.e., roll and pitch) is usually very accurate, and our policy can effectively handle the odometry drift (as we can even suddenly change the goals in the run, see Section IX-C). Therefore, our agile policy is robust to inaccurate state estimators which can be problematic for model-based controllers [6, 32, 18].\n' +
      '\n' +
      'The action space of the agile policy consists of 12-d joint targets. A PD controller tracks these joint targets \\(a\\) by converting them to joint torques:\n' +
      '\n' +
      '\\[\\tau=K_{p}(a-q)-K_{d}\\dot{q}. \\tag{6}\\]\n' +
      '\n' +
      'A fully-connected MLP maps the observations \\(o^{\\text{Agile}}\\) to the actions \\(a\\).\n' +
      '\n' +
      '### _Rewards_\n' +
      '\n' +
      'Our reward function is the summation of multiple terms:\n' +
      '\n' +
      '\\[r=r_{\\text{penalty}}+r_{\\text{task}}+r_{\\text{regularization}}, \\tag{7}\\]\n' +
      '\n' +
      'where each term can be further divided into subterms as follows.\n' +
      '\n' +
      '#### Iii-B1 Penalty Rewards\n' +
      '\n' +
      'We use a simple penalty design:\n' +
      '\n' +
      '\\[r_{\\text{penalty}}=-100\\cdot\\mathds{1}(\\text{undesired collision}), \\tag{8}\\]\n' +
      '\n' +
      'where undesired collision refers to collisions on the base, thighs, and calves, and horizontal collisions on the feet.\n' +
      '\n' +
      '#### Iii-B2 Task Rewards\n' +
      '\n' +
      'The task rewards are:\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{task}}&=60\\cdot r_{\\text{ possort}}+60\\cdot r_{\\text{postit}}+30\\cdot r_{\\text{heading}}\\\\ &-10\\cdot r_{\\text{stand}}+10\\cdot r_{\\text{agile}}-20\\cdot r_{ \\text{stall}},\\end{split} \\tag{9}\\]\n' +
      '\n' +
      'i.e., a soft position tracking term \\(r_{\\text{possort}}\\) to encourage the exploration for goal reaching, a tight position tracking term \\(r_{\\text{postit}}\\) to reinforce the robot to stop at the goal, a heading tracking term \\(r_{\\text{heading}}\\) to regulate the robot\'s heading near the goal, a standing term \\(r_{\\text{stand}}\\) to encourage a standing posture at the goal, an agile term \\(r_{\\text{agile}}\\) to encourage high velocities, and a stall term \\(r_{\\text{stall}}\\) to penalize the waiting behaviors. These terms ensure that the robot should reach the goal with appropriate heading and posture as fast as possible while wasting no time.\n' +
      '\n' +
      'To be specific, our tracking terms (\\(r_{\\text{possoft}}\\), \\(r_{\\text{postit}}\\), \\(r_{\\text{heading}}\\)) are in the same form as shown below, inspired by [72] where RL-based navigation planners are learned:\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{track (posoft/postith/heading)}}&=\\frac{1}{1+ \\left\\lVert\\frac{\\text{error}}{\\sigma}\\right\\rVert^{2}}\\cdot\\frac{\\mathds{1} (t>T-T_{r})}{T_{r}},\\end{split} \\tag{10}\\]\n' +
      '\n' +
      'where \\(\\sigma\\) normalizes the tracking errors, \\(T\\) is the episode length, and \\(T_{r}\\) is a time threshold. By doing so, the robot only needs to reach the goal before \\(T-T_{r}\\) to maximize the tracking rewards, free from explicit motion constraints such as target velocities that may limit the agility. For the soft position tracking, we have \\(\\sigma_{\\text{soft}}=2\\ \\mathrm{m}\\) and \\(T_{r}=2\\ \\mathrm{s}\\) with the error being the distance to the goal. For the tight position tracking, we have \\(\\sigma_{\\text{tight}}=0.5\\ \\mathrm{m}\\) and \\(T_{r}=1\\ \\mathrm{s}\\). For the heading tracking, we have \\(\\sigma_{\\text{heading}}=1\\ \\mathrm{rad}\\) and \\(T_{r}=2\\ \\mathrm{s}\\) with the error being the relative yaw angle to the goal heading. We further disable \\(r_{\\text{heading}}\\) when the distance to the goal is larger than \\(\\sigma_{\\text{soft}}\\) so that collision avoidance is not affected.\n' +
      '\n' +
      'The standing term is defined as\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{stand}}=\\|q-\\bar{q}\\|_{1}\\cdot\\frac{ \\mathds{1}(t>T-T_{r,\\text{stand}})}{T_{r,\\text{stand}}}\\cdot\\mathds{1}(d_{ \\text{goal}}<\\sigma_{\\text{tight}}),\\end{split} \\tag{11}\\]\n' +
      '\n' +
      'where \\(\\bar{q}\\) is the nominal joint positions for standing, \\(T_{r,\\text{stand}}=1\\ \\mathrm{s}\\), and \\(d_{\\text{goal}}\\) is the distance to the goal.\n' +
      '\n' +
      'The agile term is the core term that encourages the agile locomotion. It is defined as\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{agile}}&=\\max\\big{\\{}\\mathrm{ ReLU}(\\frac{\\mathrm{v_{x}}}{\\mathrm{v_{max}}})\\cdot\\mathds{1}(\\text{correct direction}),\\\\ &\\mathds{1}(d_{\\text{goal}}<\\sigma_{\\text{tight}})\\big{\\}}, \\end{split} \\tag{12}\\]\n' +
      '\n' +
      'where \\(v_{x}\\) is the forward velocity in the robot base frame, \\(v_{\\max}=4.5\\ \\mathrm{m}/\\mathrm{s}\\) is an upper bound of \\(v_{x}\\) that cannot be reached, and the "correct direction" means that the angle between the robot heading and the robot-goal line is smaller than 105\\({}^{\\circ}\\). To maximize this term, the robot has to either run fast or stay at the goal.\n' +
      '\n' +
      'The stall term \\(r_{\\text{stall}}\\) is \\(1\\) if the robot stays static when \\(d_{\\text{goal}}>\\sigma_{\\text{soft}}\\) and the robot is not in the "correct direction". This term penalizes the robot for time waste.\n' +
      '\n' +
      '#### Iii-B3 Regularization Rewards\n' +
      '\n' +
      'The regularization rewards are:\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{regularization}}=&-2\\cdot v_{z }^{2}-0.05\\cdot(\\omega_{x}^{2}+\\omega_{y}^{2})-20\\cdot(g_{x}^{2}+g_{y}^{2})\\\\ &-0.0005\\cdot\\|\\tau\\|_{1}^{2}-20\\cdot\\sum\\nolimits_{i=1}^{12} \\mathrm{ReLU}\\left(|\\tau_{i}|-0.85\\cdot\\tau_{i,\\lim}\\right)\\\\ &-0.0005\\cdot\\|\\dot{q}\\|_{2}^{2}-20\\cdot\\sum\\nolimits_{i=1}^{12} \\mathrm{ReLU}\\left(|\\dot{q}_{i}|-0.9\\cdot\\dot{q}_{i,\\lim}\\right)\\\\ &-20\\cdot\\sum\\nolimits_{i=1}^{12}\\mathrm{ReLU}\\left(|q_{i}|-0.95 \\cdot q_{i,\\lim}\\right)\\\\ &-2\\times 10^{-7}\\cdot\\|\\dot{q}\\|_{2}^{2}-4\\times 10^{-6}\\cdot\\| \\dot{a}\\|_{2}^{2}-20\\cdot\\mathds{1}(\\text{fly}),\\end{split} \\tag{13}\\]\n' +
      '\n' +
      'where \\(\\tau\\) is the joint torques, \\(\\tau_{\\lim}\\) is the hardware torque limits, \\(\\dot{q}_{\\lim}\\) is the hardware joint velocity limits, \\(q_{\\lim}\\) is the hardware joint position limits, and "fly" refers to when the robot has no contact with the ground. We penalize the "fly" cases as they make the robot base uncontrollable, threatening the system\'s safety.\n' +
      '\n' +
      '### _Training in Simulation_\n' +
      '\n' +
      '#### Iii-C1 Simulator\n' +
      '\n' +
      'We use the GPU-based Isaac Gym simulator [44] which supports us to train 1280 environments in parallel with the PPO algorithm [52].\n' +
      '\n' +
      '#### Iii-C2 Terrains\n' +
      '\n' +
      'We train the agile policy with randomized terrains following a curriculum to facilitate learning. To prevent unstable gaits that over-exploits the simulation dynamics, the terrains are randomly sampled to be flat, rough, or low stumbling blocks, as shown in Figure 3. As the difficulty level goes up from 0 to 9, the rough terrains and the stumbling blocks have larger height difference from 0 \\(\\mathrm{cm}\\) to 7 \\(\\mathrm{cm}\\).\n' +
      '\n' +
      '#### Iii-C3 Obstacles\n' +
      '\n' +
      'We train the policy with cylinders of 40 \\(\\mathrm{cm}\\) radius. For each episode we have 0\\(\\sim\\)8 obstacles randomly distributed in a 11 \\(\\mathrm{m}\\times\\) 5 \\(\\mathrm{m}\\) rectangle that covers the origin and the goal. To facilitate learning, we also apply a curriculum where higher difficulty levels have more obstacles.\n' +
      '\n' +
      '#### Iii-C4 Domain Randomization\n' +
      '\n' +
      'We do domain randomization [60] for sim-to-real transfer. The randomized settings are listed in Table II. Among these few terms, two are critical: the illusion, and the ERFI-50. The illusion makes the policy more robust to unseen geometries such as walls: it overwrites the observed ray distances by random values subject to \\(\\mathcal{U}(d_{\\text{goal}}+0.3,\\text{ray distance})\\) if they are larger than \\(d_{\\text{goal}}+0.3\\). The ERFI-50 proposed by Campanaro et al. [8] implicitly models the motor sim-to-real gaps with random torque perturbations, and we add a curriculum in our work to avoid impeding the early stage of learning. We also randomly bias the joint positions to model the motor encoders\' offset errors.\n' +
      '\n' +
      'Fig. 3: Example training environments. The magenta points indicate the goals, and bluegreen lines indicate the extereoceptive ray observations. Terrains from left to right: flat, low stumbling blocks, and rough.\n' +
      '\n' +
      '#### Iv-B5 Curriculum\n' +
      '\n' +
      'As mentioned above, we apply a curriculum where difficulty levels can change the terrains, the obstacle distribution, and the domain randomization. For the assignment of difficulty levels, we follow the design of Zhang et al. [71]: when an episode terminates, the robot gets promoted to a higher level if \\(d_{\\text{goal}}<\\sigma_{\\text{tight}}\\), and gets demoted to a lower level if \\(d_{\\text{goal}}>\\sigma_{\\text{soft}}\\). If the robot gets promoted at the highest level, it will go to a random level, following [51].\n' +
      '\n' +
      '## V Learning and Using Reach-Avoid Values\n' +
      '\n' +
      'Although the agile policy learns certain collision avoidance behaviors via corresponding rewards, it does not ensure safety. To safeguard the robot, we propose to use RA values to predict the failures, and then a recovery policy can save the robot based on the RA values.\n' +
      '\n' +
      'Inspired by Hsu et al. [29], we learn RA values in a model-free way, contrasting typical approaches of model-based reachability analysis [2]. This better suits the model-free RL-based policies. Also different from [29], we do not learn the global RA values, but make it policy-conditioned, as mentioned in Section III-B4. The learned RA value function will predict only the agile policy\'s failures based on the observations.\n' +
      '\n' +
      '### _Learning RA Values_\n' +
      '\n' +
      'To avoid overfitting in high dimensions and make the RA values generalize, we use a reduced set of observations as the inputs of the RA value function:\n' +
      '\n' +
      '\\[o^{\\text{RA}}=\\left[\\left[v;\\omega\\right];G^{c}_{x,y};R\\right], \\tag{14}\\]\n' +
      '\n' +
      '_i.e._, the base twists, the goal \\((x,y)\\) position in the robot frame, and the exteroception. These components are centroidal observations that significantly influence safety and goal reaching. On the other hand, we don\'t use joint-level observations (such as \\(q\\) and \\(\\dot{q}\\)) here because they are high-dimensional and less pertinent to goal reaching. We train an RA value network \\(\\hat{V}\\) to approximate the RA values:\n' +
      '\n' +
      '\\[V^{\\pi^{\\text{Agile}}}_{\\text{RA}}(s)\\approx\\hat{V}(o^{\\text{RA}}). \\tag{15}\\]\n' +
      '\n' +
      'Based on Equation (5), we minimize the following loss for each episode with gradient descent:\n' +
      '\n' +
      '\\[L=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\hat{V}(o^{\\text{RA}}_{t})-\\hat{V}^{\\text{ target}}\\right)^{2}, \\tag{16}\\]\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\[\\begin{split}\\hat{V}^{\\text{target}}=&\\gamma_{\\text {RA}}\\max\\left\\{\\zeta(s_{t}),\\min\\left\\{l(s_{t}),\\hat{V}^{\\text{old}}(o^{ \\text{RA}}_{t+1})\\right\\}\\right\\}\\\\ &+(1-\\gamma_{\\text{RA}})\\max\\left\\{l(s_{t}),\\zeta(s_{t})\\right\\},\\end{split} \\tag{17}\\]\n' +
      '\n' +
      'and we set the discount factor \\(\\gamma_{\\text{RA}}=0.99999\\) to best approximate \\(\\mathcal{RA}^{\\pi}(\\Theta;\\mathcal{F})\\) since \\(V^{\\pi}_{\\text{RA}}(s)\\) converges to \\(V^{\\pi}_{\\text{RA}}(s)\\) as \\(\\gamma_{\\text{RA}}\\) approaches 1. \\(\\hat{V}^{\\text{old}}\\) refers to \\(\\hat{V}\\) from previous iteration, and we set \\(\\hat{V}^{\\text{old}}(o^{\\text{RA}}_{T+1})=+\\infty\\).\n' +
      '\n' +
      'Differing from [29], our approach learns policy-conditioned reach-avoid values instead of solving policy-agnostic global reach-avoid value of the entire system dynamics which involves another value minimization problem over \\(\\mathcal{A}\\). Our method offers several advantages: 1) simplicity: as highlighted in Equation (5), this simplicity arises from avoiding the need to solve for the lowest value of the next state across the entire action space. 2) two-stage offline learning: our approach can be learned in a two-stage offline manner. This involves first collecting policy trajectories and then training the policy-conditioned reach-avoid value. This two-stage process enhances stability compared to the online training method presented in [29].\n' +
      '\n' +
      '### _Implementation_\n' +
      '\n' +
      'According to [29], \\(l(s)\\) and \\(\\zeta(s)\\) should be Lipschitz-continuous for theoretical guarantees. In our implementation, we define the \\(l(s)\\) as\n' +
      '\n' +
      '\\[l(s)=\\tanh\\log\\frac{d_{\\text{goal}}}{\\sigma_{\\text{tight}}}, \\tag{18}\\]\n' +
      '\n' +
      'thereby making it Lipschitz-continuous, bounding it with \\((-1,1)\\), and setting \\(d_{\\text{goal}}\\leq\\sigma_{\\text{tight}}\\) as "reach".\n' +
      '\n' +
      'Regarding failures, we naturally have\n' +
      '\n' +
      '\\[\\zeta(s)=2*\\mathds{1}(\\text{undesired collision})-1. \\tag{19}\\]\n' +
      '\n' +
      'However, this definition violates the Lipschitz continuity. Hence, we soften the function in a hindsight way: when an undesired collision happens, the \\(\\zeta\\) values for the last 10 timesteps are relabelled to be \\(-0.8,-0.6,\\dots,0.8,1.0\\).\n' +
      '\n' +
      'For RA dataset sampling, we make the obstacles distribute as in the highest difficulty level during the training of the agile policy. We roll out our trained agile policy for 200k episodes, and collect these trajectories for RA learning.\n' +
      '\n' +
      'Figure 4 visualizes the learned RA values for a specific set of obstacles. As the robot\'s velocity changes, the landscape of RA values changes accordingly. The sign of the RA values reasonably indicates the safety for the agile policy.\n' +
      '\n' +
      '### _Using RA Values for Recovery_\n' +
      '\n' +
      'RA values provide a failure prediction conditioned on the agile policy, and we propose to use RA values to guide the recovery policy. To be specific, the robot decides the optimal twist to avoid collisions using the RA value function, and employs the recovery policy to track these twist commands. The recovery policy is triggered as a back-up shielding policy if and only if \\(\\hat{V}(o^{\\text{RA}})\\geq V_{\\text{threshold}}\\). We set \\(V_{\\text{threshold}}=-0.05\\) to compensate for learning errors without causing over-conservative shielding.\n' +
      '\n' +
      'During recovery, we assume that the recovery policy is well-trained so that the robot twist is close to the command\n' +
      '\n' +
      '\\[tw^{c}=[v_{x}^{c},v_{y}^{c},0,0,0,\\omega_{z}^{c}], \\tag{20}\\]\n' +
      '\n' +
      'and the robot should try to get closer to the goal if its twist is safe given the goal and the exteroception. Therefore, the twist command is obtained from the optimization:\n' +
      '\n' +
      '\\[tw^{c}=\\arg\\min d_{\\text{goal}}^{\\text{future}}\\text{ s.t. }\\hat{V}([tw^{c};G_{x,y}^{c};R])<V_{\\text{ threshold}}, \\tag{21}\\]\n' +
      '\n' +
      'and \\(d_{\\text{goal}}^{\\text{future}}\\) refers to the approximate distance to the goal after tracking the twist command for a small amount of time \\(\\delta t=0.05\\) s. This is calculated based on the linearized integral of the robot displacement in the base frame:\n' +
      '\n' +
      '\\[\\begin{split}\\delta x&=v_{x}^{c}\\delta t-0.5v_{y} ^{c}\\omega_{z}^{c}\\delta t^{2},\\\\ \\delta y&=v_{y}^{c}\\delta t+0.5v_{x}^{c}\\omega_{z}^{ c}\\delta t^{2}.\\end{split} \\tag{22}\\]\n' +
      '\n' +
      'In our practice, gradient descent with a Lagrangian multiplier on the constraint can solve Equation (21) within 5 steps when initialized with the current twist, thereby enabling real-time deployment. A visualization of the twist optimization process is given in Figure 8 where the searched twist consistently satisfies the safety constraint (i.e., \\(\\hat{V}<V_{\\text{threshold}}\\)).\n' +
      '\n' +
      '## VI Learning Recovery Policy\n' +
      '\n' +
      'The recovery policy is intended to make the robot track a given twist command as fast as possible so that it can function as a backup shielding policy, as mentioned in Section V.\n' +
      '\n' +
      '### _Observation Space and Action Space_\n' +
      '\n' +
      'The observation space of the recovery policy differs from the agile policy in that it tracks twist commands and it does not need exteroception. The recovery policy\'s observation \\(o^{\\text{Rec}}\\) consists of: the foot contacts \\(c_{f}\\), the base angular velocities \\(\\omega\\), the projected gravity in the base frame \\(g\\), the twist commands \\(tw^{c}\\) (only non-zero variables), the joint positions \\(q\\), the joint velocities \\(\\dot{q}\\), and the actions \\(a\\) of the previous frame.\n' +
      '\n' +
      'The action space of the recovery policy is exactly the same as that of the agile policy: the 12-d joint targets. We also use an MLP as the policy network.\n' +
      '\n' +
      '### _Rewards_\n' +
      '\n' +
      'Similar to the agile policy, the reward functions for the recovery policy also consist of three parts: the penalty rewards, the task rewards, and the regularization rewards. The regularization rewards and the penalty rewards remain the same, except that we allow knee contacts with the ground for maximum deceleration (e.g., Figure 1 (a)).\n' +
      '\n' +
      'The task rewards are for twist tracking:\n' +
      '\n' +
      '\\[r_{\\text{task}}=10\\cdot r_{\\text{invel}}-0.5\\cdot r_{\\text{angvel}}+5\\cdot r_{ \\text{alive}}-0.1\\cdot r_{\\text{posture}}, \\tag{23}\\]\n' +
      '\n' +
      '_i.e._, a term for tracking \\(v_{x}^{c}\\) and \\(v_{y}^{c}\\), a term for tracking \\(\\omega_{z}^{c}\\), a term for staying alive, and a term for maintaining a posture to seamlessly switch back to the agile policy.\n' +
      '\n' +
      'To be specific, we have\n' +
      '\n' +
      '\\[r_{\\text{invel}}=\\exp\\left[-\\frac{(v_{x}-v_{x}^{c})^{2}+(v_{y}-v_{y}^{c})^{2}}{ \\sigma_{\\text{invel}}^{2}}\\right], \\tag{24}\\]\n' +
      '\n' +
      'where we set \\(\\sigma_{\\text{invel}}=0.5\\ \\mathrm{m/s}\\). For the angular velocity,\n' +
      '\n' +
      '\\[r_{\\text{angvel}}=\\|\\omega_{z}-\\omega_{z}^{c}\\|_{2}^{2}, \\tag{25}\\]\n' +
      '\n' +
      'which provides a softer landscape near the command than \\(r_{\\text{invel}}\\). The alive term is simply\n' +
      '\n' +
      '\\[r_{\\text{alive}}=1\\cdot\\mathds{1}(\\text{alive}). \\tag{26}\\]\n' +
      '\n' +
      'The posture term is\n' +
      '\n' +
      '\\[r_{\\text{posture}}=\\|q-\\bar{q}_{\\text{rec}}\\|_{1}, \\tag{27}\\]\n' +
      '\n' +
      'where \\(\\bar{q}_{\\text{rec}}\\) is a nominal standing pose with low height allowing the robot to switch back to the agile policy seamlessly.\n' +
      '\n' +
      '### _Training in Simulation_\n' +
      '\n' +
      'The simulation settings for training the recovery policy are similar to those for the agile policy. The differences lie in:\n' +
      '\n' +
      '#### Vi-C1 Domain Randomization\n' +
      '\n' +
      'The observation noises and the dynamic randomization do not change. The episode length is changed to 2 s, and there are randomized initial roll and pitch angles subject to \\(\\mathcal{U}(-\\pi/6,\\pi/6)\\) rad. The randomization ranges are also changed for initial \\(v_{x}\\sim\\mathcal{U}(-0.5,5.5)\\) m/s and initial \\(\\omega\\sim\\mathcal{U}(-1.0,1.0)\\) rad/s. These changes better accommodate the states that can trigger the recovery policy during the agile running. The ranges of sampling commands are \\(v_{x}^{c}\\sim\\mathcal{U}(-1.5,1.5)\\) m/s, \\(v_{y}^{c}\\sim\\mathcal{U}(-0.3,0.3)\\) m/s, and \\(\\omega_{z}^{c}\\sim\\mathcal{U}(-3.0,3.0)\\) rad/s.\n' +
      '\n' +
      '#### Vi-C2 Curriculum\n' +
      '\n' +
      'The curriculum still exists for terrains and domain randomization. However, the assignment is changed: the robot gets promoted if the velocity tracking error is smaller than \\(0.7\\sigma_{\\text{invel}}\\), and gets demoted if it falls over.\n' +
      '\n' +
      'Fig. 4: Visualization of \\(\\hat{V}\\) with different linear velocities and 2D positions relative to the \\(3\\) fixed obstacles. The angular velocities are set to zero, and the relative goal commands are set to \\(5\\) m ahead of the robot. The grey circles represent the obstacles, and the colors represent the values of \\(\\hat{V}\\) at corresponding 2D positions.\n' +
      '\n' +
      '## VII Perception\n' +
      '\n' +
      'As mentioned in Section IV and Section V, both the agile policy and the RA value network use the exteroceptive 11-d ray distances as part of the observations, with access to their ground truth values during training. These rays are horizontally cast from the robot base, with directions evenly spaced in \\([-\\frac{\\pi}{4},\\frac{\\pi}{4}]\\).\n' +
      '\n' +
      'However, such ray distances are not directly available during deployment, and we need to train a ray-prediction network to predict them from depth images, as mentioned in Section III-C. Such a design leads to the following benefits:\n' +
      '\n' +
      '1. We only need to tune the ray-prediction network to handle high-dimensional image noises by data augmentation.\n' +
      '2. The representation is highly interpretable, allowing humans to supervise.\n' +
      '3. The agile policy and the RA value network are easier to train with low-dimensional inputs.\n' +
      '4. Compared to costly image rendering in simulation, the ray distances are easy to compute and save training time.\n' +
      '\n' +
      'Besides, although ray distances are similar to sparse LiDAR readings, we use cameras instead of LiDARs because a lightweight low-cost camera can easily reach a high FPS, which is important in high-speed collision avoidance.\n' +
      '\n' +
      'We present details for training the ray-prediction network in this section.\n' +
      '\n' +
      '### _Data Collection_\n' +
      '\n' +
      'To train our ray-prediction network, we collect a dataset of pairs of depth images and ray distances (as shown in Figure 2 (a)) by running the agile policy in simulation. The ray-prediction network can then be trained in a supervised way. To facilitate generalization, as shown in Figure 5, we replace the cylinders with objects of different shapes during data collection.\n' +
      '\n' +
      '### _Data Augmentation for Sim-to-Real Transfer_\n' +
      '\n' +
      'The real-world depth images collected from cameras are far more noisy than the rendered depth images in simulation [27]. To make the ray-prediction network adapt better to real-world depth images, we apply four data augmentation techniques during training, as shown in Figure 6: 1) horizontal flip; 2) random erase; 3) Gaussian blur; 4) Gaussian noise. For deployment, we apply hole filling [56] to further reduce the gap of depth images between the simulation and the real world.\n' +
      '\n' +
      '### _Other Implementation Details_\n' +
      '\n' +
      'To make the network focus more on close obstacles, we take the logarithm of depth values as the NN inputs, and the logarithm of ray distances as the outputs, with the mean squared error as the loss function.\n' +
      '\n' +
      'We finetune ResNet-18 [26] with pretrained weights to train the model. The images are downsampled to \\([160,90]\\) resolution both in simulation and during deployment.\n' +
      '\n' +
      '## VIII Experiments\n' +
      '\n' +
      '### _Baselines_\n' +
      '\n' +
      'For experimental results, we consider three settings:\n' +
      '\n' +
      '1. Our ABS system, with both the agile policy and the recovery policy;\n' +
      '2. Our agile policy \\(\\pi^{\\text{Agile}}\\) only;\n' +
      '3. "LAG": we use PPO-Lagrangian [49] to train end-to-end safe RL policies with the agile policy\'s formulation.\n' +
      '\n' +
      'By comparing (2) and (3), we can see how agility and safety trade off without external modules, forming a boundary of agility and safety. With the help of RA values and the recovery policy, we expect (1) to break this boundary with a high safety gain: it should be as agile as (2) in safe cases, and shield the robot in risky cases.\n' +
      '\n' +
      '_Note that the three settings here are all based on what we propose. A detailed comparison between our agile policy and the previous state-of-the-art (SOTA) agile running policy [45] is made in Section IX-A1._\n' +
      '\n' +
      '### _Simulation Experiments_\n' +
      '\n' +
      '#### Vi-B1 Quantitative results\n' +
      '\n' +
      'We test the policies trained with different settings in simulation. To better show the agility-safety boundary, we introduce 3 variants for each setting: an aggressive one ("-a") doubling the agile reward term \\(r_{\\text{agile}}\\), a nominal one ("-n"), and a conservative one ("-c") halving the \\(r_{\\text{agile}}\\). Regarding the obstacles, we distribute eight obstacles within a 5.5 m\\(\\times\\) 4 m rectangle (during training it was 11 m\\(\\times\\) 5 m), so the test cases are in distribution but much harder than most cases during training.\n' +
      '\n' +
      'The results are reported in Table III and Figure 7. There are three possible outcomes for an episode: success, collision, or timeout. Trajectories that do not trigger success or collision criteria within the episode length are labelled as "timeout". We report the success rate, the collision rate, the timeout rate, the average peak velocity for success cases, and the average speed for success cases as the metrics. For each setting, the\n' +
      '\n' +
      'Fig. 5: Various obstacles used for ray-prediction data collection.\n' +
      '\n' +
      'Fig. 6: Illustration of four kinds of image augmentation used for depth-based ray-prediction training.\n' +
      '\n' +
      'mean and std values are calculated over 3 policies trained with different seeds, and the metrics are obtained via testing for 10k random episodes.\n' +
      '\n' +
      'The results indicate that, no matter how the reward weights are tuned or whether the RL algorithm is constrained by safe exploration, the agility and the safety trade off within a boundary. Yet, with our RA values and the recovery policy as a safeguard, we can break this boundary and get a substantial improvement in safety at the cost of only a minor decrease in agility.\n' +
      '\n' +
      '_Note that the variants are only introduced to show the boundary here. In the following parts, we will only use the nominal ones._\n' +
      '\n' +
      '#### V-B2 Example Case\n' +
      '\n' +
      'We present an example case of ABS and other baselines in simulation, where the robot starting from \\((0,0)\\) needs to run through \\(8\\) obstacles to reach the goal \\((7,0)\\), as shown in Figure 8. The robot needs to first go through an open space, followed by two tight spaces, and then another open space. In this case, the \\(\\pi^{\\text{Agile}}\\) baseline runs fast but dies near the second tight space. The LAG baseline runs much slower than ABS. In contrast, our proposed ABS runs fast in the open spaces, and slows down in the tight spaces for safety thanks to the shielding of RA values and the recovery policy. Figure 8 (c) demonstrates the RA value landscape with respect to twist commands when the recovery policy is activated, where the searched twist consistently satisfies the safety constraint (i.e., \\(\\tilde{V}<V_{\\text{threshold}}\\)).\n' +
      '\n' +
      '### _Real-World Experiments_\n' +
      '\n' +
      '#### V-C1 Hardware setup\n' +
      '\n' +
      'We use the Unitree Go1 for our experiments. The robot is equipped a Jetson Orin NX for onboard computation and a ZED Mini Stereo Camera for depth and odometry sensing. We employ the ZED odometry module to\n' +
      '\n' +
      'Fig. 8: An example case in simulation where \\(\\pi^{\\text{Agile}}\\) fails to reach the goal. a) Trajectories of ABS and other baselines, with RA values visualized for ABS. b) The velocity-time curves showing that ABS is much faster than the LAG baseline. c) Illustrations of the RA value landscape when the recovery policy is triggered at (I) and (II), projected in the \\(v_{x}-\\omega_{z}\\) plane and the \\(v_{x}-v_{y}\\) plane. We show the initial twist before search (_i.e._, the current twist of the robot base) and the searched commands based on Equation (21).\n' +
      '\n' +
      'Fig. 7: Illustration of agility-safety trade-off in benchmarked comparison. Agility is quantified by the average speed achieved in success cases while safety is represented by the non-collision rate. Points indicate the mean values, and error bars indicate the std values.\n' +
      '\n' +
      'online update the relative goal commands for the agile policy, with its difference as the velocity estimation. We use Unitree\'s built-in PD controller, with \\(K_{p}=30\\) and \\(K_{d}=0.65\\).\n' +
      '\n' +
      '#### V-A2 Results\n' +
      '\n' +
      'Across two indoor and one outdoor testbeds, ABS demonstrates superior overall performance as shown in Figure 9, achieving the highest success rates and the lowest collision rates. Specifically, ABS consistently scores either 9 or 10 out of 10 in success rates across all environments, with minimal collisions, indicating robustness and reliability in the real world.\n' +
      '\n' +
      'Without the safety shield, the agile policy \\(\\pi_{\\text{Agile}}\\) achieves the fastest running speed at the cost of more collisions. LAG outperforms \\(\\pi_{\\text{Agile}}\\) in safety but has slower speeds, and falls short in both safety and agility compared to ABS. ABS achieves high speed with high safety, and generalizes to dynamic obstacles, as shown in Figure 1.\n' +
      '\n' +
      '#### V-A3 Robustness\n' +
      '\n' +
      'Our ABS system can work on the slippery scipy snow, bear a \\(12\\)-kg payload (equal to its own weight), and withstand perturbations, as shown in Figure 10. These tests demonstrate the robustness of our system.\n' +
      '\n' +
      '## IX Extensive Studies and Analyses\n' +
      '\n' +
      '### _Maximizing Agility_\n' +
      '\n' +
      '#### IX-A1 Goal-Reaching v.s. Velocity-Tracking\n' +
      '\n' +
      'Velocity-tracking is the most commonly used formulation of locomotion controllers [51, 45, 47, 38], and is also adopted for our recovery policy. However, for the agile policy, we claim that goal-reaching is a better choice because it does not decouple locomotion and navigation for collision avoidance and can fully unleash the agility that is learned. Moreover, we empirically find that the goal-reaching formulation benefits sim-to-real transfer as it finds a better gait pattern for high-speed running.\n' +
      '\n' +
      'With the SOTA agile velocity-tracking policy in [45] as a baseline (referred to as "rapid"), we make detailed comparisons in Table IV. For fair comparison, we train "rapid" with the combination of our regularization rewards and the task rewards in [45], use the same action space for two policies, and remove the temporal information and system identification in [45].\n' +
      '\n' +
      '#### IX-A2 Effects of illusion and ERFI-50 randomization\n' +
      '\n' +
      'Two key components we add in domain randomization to help sim-to-real transfer is the illusion and the ERFI-50. As shown in Figure 11, without the illusion, the robot will sometimes tremble near a wall which it has never seen in simulation. Without ERFI-50, the robot will hit the ground with its head during running due to the sim-to-real gap in motor dynamics.\n' +
      '\n' +
      '### _Enhancing Perception Training_\n' +
      '\n' +
      'In refining our ray-prediction network training, we systematically examine several factors: 1) network architecture, 2)\n' +
      '\n' +
      'Fig. 11: Effects of illusion and ERFI-50 randomization. The robot will tremble near a wall without illusion randomization and will hit the ground during running without ERFI-50 randomization.\n' +
      '\n' +
      'Fig. 10: Robustness Tests of our ABS system, a) in snowy terrain,s b) bearing a \\(12\\)-kg payload, c) against a ball hit when running, and d) withstand a kick when standing at the goal.\n' +
      '\n' +
      'Fig. 9: We evaluate ABS and other baselines in the real world with two indoor testbeds, one outdoor testbed, and repetitive speed tests. Indoor (a) is a dim and narrow corridor, Indoor (b) is a hall with furnitures, and Outdoor is an open space on the playground with few obstacles. ABS achieves the best safety across three testbeds, with faster speeds compared to the LAG baseline.\n' +
      '\n' +
      'pretrained weights, and 3) data augmentation. The comparative results, detailed in Table V, underscore the significance of both pretrained weights and data augmentation in enhancing the accuracy of the network.\n' +
      '\n' +
      'Regarding the network size, larger networks improve the prediction accuracy at the cost of inference time. Note that the reported inference time in Table V was measured on Jetson Orin NX exclusively for perception inference. In practical deployment where computational resources are shared among various tasks, the actual update frequency can be considerably lower. For real-time high-speed locomotion, we opt for ResNet-18, balancing accuracy and responsiveness in dynamic environments.\n' +
      '\n' +
      '### _Instant Steering via Commands_\n' +
      '\n' +
      'As mentioned in Section IV, we can change our goal commands even in the run time. Thanks to our single-frame observations and randomization settings, we can easily overwrite goal commands to achieve instant agile steering, as presented in Table VI. This enables direct human involvement similar to the velocity-tracking formulation, and Figure 12 showcases such operations in the real world.\n' +
      '\n' +
      '### _Failure Cases and Limitations_\n' +
      '\n' +
      'First, when the obstacles are too dense and form a local minimum, our policy can easily fail, which is also reflected by the high timeout rates in the results. This is common in local navigation planners [72, 46] though, and a potential solution can be to add memory [64] or introduce a global hint [61].\n' +
      '\n' +
      'Second, our generalization to dynamic environments are due to the shielding of RA values and the recovery policy. The RA values are learned with static obstacles, and can only generalize to quasi-static environments. If a dynamic object moves faster than the velocity limit of the recovery policy, the collision may happen. A potential solution is to predict the motions of the obstacles in the future [37, 54].\n' +
      '\n' +
      'Third, we limit the robot behaviors to only 2D locomotion and constrain the motions to have no flying phase. For 3D terrains such as stairs and gaps, the problem can be far more challenging because the locomotion skills and the collision avoidance are coupled.\n' +
      '\n' +
      'Fourth, implicit system identification techniques [38, 47, 45, 36] can leverage temporal information to represent real-world dynamics and facilitate sim-to-real transfer, but it is non-trivial to incorporate them into our system. This requires a latent embedding of the temporal information, which is hard to deal with for the RA module. The policy switch can also make the embedding out of distribution for the policies.\n' +
      '\n' +
      'Fifth, the vision system needs further improvement. In the Indoor (a) testbed, the only collision of ABS is due to the "undetected" objects by the ray-prediction network as the corridor is quite dim. Beside the network, the system can also be completed by adding more cameras around the body. In this way, the robot may also dodge the obstacles come from behind or the side. Event cameras may also help in highly dynamic scenarios, e.g., when dodging a high-speed ball [19]\n' +
      '\n' +
      '## X Conclusion\n' +
      '\n' +
      'In this paper, we achieve safe high-speed quadrupedal locomotion in cluttered environments. Our framework ABS employs a dual-policy setup where the agile policy enables the robot to run fast, and the recovery policy safeguards the robot. The learned reach-avoid values govern the policy switch and guide the recovery policy. A ray-prediction network provides exteroception representation for the agile policy and the RA value network. Some key takeaways are:\n' +
      '\n' +
      '1. For agility in collision avoidance, the best practice is to integrate locomotion and navigation rather than decoupling them as separate subtasks.\n' +
      '2. A low-dimensional exteroception representation can facilitate policy learning and generalization.\n' +
      '3. With the same reward terms and training settings, adjusting reward weights or choosing between RL and Lagrangian-based RL trades off agility and safety, while external shielding modules can help break the trade-off boundary.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We appreciate Wennie Tabib for supporting hardware experiments, and thank Yuxiang Yang, Yiyu Chen, Yikai Wang and Xialin He for their advice on hardware debugging. Special thanks to Andrea Bajcsy and Ziqiao Ma for their assistance in graphics design.\n' +
      '\n' +
      'Fig. 12: Steering the robot with a command sequence Forward-Rapid Right Turn-Forward. The robot can reach \\(>3\\) m/s when running forward and \\(>6\\) rad/s when turning rapidly.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Fernando Acero, Kai Yuan, and Zhibin Li. Learning perceptual locomotion on uneven terrains using sparse visual observations. _IEEE Robotics and Automation Letters_, 7(4):8611-8618, 2022.\n' +
      '* [2] Somil Bansal and Claire J Tomlin. Deepreach: A deep learning approach to high-dimensional reachability. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1817-1824. IEEE, 2021.\n' +
      '* [3] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief overview and recent advances. In _2017 IEEE 56th Annual Conference on Decision and Control (CDC)_, pages 2242-2253. IEEE, 2017.\n' +
      '* [4] Shalabh Bhatnagar and K Lakshmanan. An online actor-critic algorithm with function approximation for constrained markov decision processes. _Journal of Optimization Theory and Applications_, 153:688-708, 2012.\n' +
      '* [5] Gerardo Bledt, Matthew J Powell, Benjamin Katz, Jared Di Carlo, Patrick M Wensing, and Sangbae Kim. Mit cheetah 3: Design and control of a robust, dynamic quadruped robot. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2245-2252. IEEE, 2018.\n' +
      '* [6] Michael Bloesch, Christian Gehring, Peter Fankhauser, Marco Hutter, Mark A Hoepflinger, and Roland Siegwart. State estimation for legged robots on unstable and slip-pery terrain. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 6058-6064. IEEE, 2013.\n' +
      '* [7] Russell Buchanan, Lorenz Wellhausen, Marko Bjelonic, Tirthankar Bandyopadhyay, Navinda Kottege, and Marco Hutter. Perceptive whole-body planning for multilegged robots in confined spaces. _Journal of Field Robotics_, 38(1):68-84, 2021.\n' +
      '* [8] Luigi Campanaro, Siddhant Gangapurwala, Wolfgang Merkt, and Ioannis Havoutis. Learning and deploying robust locomotion policies with minimal dynamics randomization. _arXiv preprint arXiv:2209.12878_, 2022.\n' +
      '* [9] Mo Chen, Sylvia Herbert, and Claire J Tomlin. Fast reachable set approximations via state decoupling disturbances. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 191-196. IEEE, 2016.\n' +
      '* [10] Richard Cheng, Gabor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3387-3395, 2019.\n' +
      '* [11] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. _arXiv preprint arXiv:2309.14341_, 2023.\n' +
      '* [12] Jia-Ruei Chiu, Jean-Pierre Sleiman, Mayank Mittal, Farbod Farshidian, and Marco Hutter. A collision-free mpc for whole-body dynamic locomotion and manipulation. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 4686-4693. IEEE, 2022.\n' +
      '* [13] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _arXiv preprint arXiv:1801.08757_, 2018.\n' +
      '* [14] Jared Di Carlo, Patrick M Wensing, Benjamin Katz, Gerardo Bledt, and Sangbae Kim. Dynamic locomotion in the mit cheetah 3 through convex model-predictive control. In _2018 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 1-9. IEEE, 2018.\n' +
      '* [15] Yanran Ding, Abhishek Pandala, and Hae-Won Park. Real-time model predictive control for versatile dynamic motions in quadrupedal robots. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8484-8490. IEEE, 2019.\n' +
      '* [16] Helei Duan, Bikram Pandit, Mohitvishnu S Gadde, Bart Jaap van Marum, Jeremy Dao, Chanho Kim, and Alan Fern. Learning vision-based bipedal locomotion for challenging terrain. _arXiv preprint arXiv:2309.14594_, 2023.\n' +
      '* [17] Thomas Dudzik, Matthew Chignoli, Gerardo Bledt, Bryan Lim, Adam Miller, Donghyun Kim, and Sangbae Kim. Robust autonomous navigation of a small-scale quadruped robot in real-world environments. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3664-3671. IEEE, 2020.\n' +
      '* [18] Shamel Fahmi, Geoff Fink, and Claudio Semini. On state estimation for legged locomotion over soft terrain. _IEEE Sensors Letters_, 5(1):1-4, 2021.\n' +
      '* [19] Davide Falanga, Kevin Kleber, and Davide Scaramuzza. Dynamic obstacle avoidance for quadrotors with event cameras. _Science Robotics_, 5(40):eaaz9712, 2020.\n' +
      '* [20] Jaime F Fisac, Neil F Lugovoy, Vicenc Rubies-Royo, Shromona Ghosh, and Claire J Tomlin. Bridging hamilton-jacobi safety analysis and reinforcement learning. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8550-8556. IEEE, 2019.\n' +
      '* [21] Kunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold elements. _IEEE Transactions on Systems Science and Cybernetics_, 5(4):322-333, 1969.\n' +
      '* [22] Magnus Gaertner, Marko Bjelonic, Farbod Farshidian, and Marco Hutter. Collision-free mpc for legged robots in static and dynamic scenes. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 8266-8272. IEEE, 2021.\n' +
      '* [23] Siddhant Gangapurwala, Mathieu Geisert, Romeo Orsolino, Maurice Fallon, and Ioannis Havoutis. Rloc: Terrain-aware legged locomotion using reinforcement learning and optimal control. _IEEE Transactions on Robotics_, 38(5):2908-2927, 2022.\n' +
      '* [24] Ruben Grandia, Farbod Farshidian, Alexey Dosovitskiy, Rene Ranftl, and Marco Hutter. Frequency-aware model predictive control. _IEEE Robotics and Automation Letters_, 4(2):1517-1524, 2019.\n' +
      '* [25] Ruben Grandia, Fabian Jenelten, Shaohui Yang, Farshidian, and Marco Hutter. Perceptive locomotion through nonlinear model-predictive control. _IEEE Transactions on Robotics_, 2023.\n' +
      '* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [27] David Hoeller, Lorenz Wellhausen, Farbod Farshidian, and Marco Hutter. Learning a state representation and navigation in cluttered and dynamic environments. _IEEE Robotics and Automation Letters_, 6(3):5081-5088, 2021.\n' +
      '* [28] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco Hutter. Anymal parkour: Learning agile navigation for quadrupedal robots. _arXiv preprint arXiv:2306.14874_, 2023.\n' +
      '* [29] Kai-Chieh Hsu, Vicenc Rubies Royo, Claire J. Tomlin, and Jaime F. Fisac. Safety and liveness guarantees through reach-avoid reinforcement learning. In _Robotics: Science and Systems XVII_, 2021.\n' +
      '* [30] Kai-Chieh Hsu, Allen Z Ren, Duy P Nguyen, Anirudha Majumdar, and Jaime F Fisac. Sim-to-lab-to-real: Safe reinforcement learning with shielding and generalization guarantees. _Artificial Intelligence_, 314:103811, 2023.\n' +
      '* [31] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. _Science Robotics_, 4(26):eaau5872, 2019.\n' +
      '* [32] Fabian Jenelten, Jemin Hwangbo, Fabian Tresoldi, C Dario Bellicoso, and Marco Hutter. Dynamic locomotion on slippery ground. _IEEE Robotics and Automation Letters_, 4(4):4170-4176, 2019.\n' +
      '* [33] Fabian Jenelten, Ruben Grandia, Farbod Farshidian, and Marco Hutter. Tamols: Terrain-aware motion optimization for legged systems. _IEEE Transactions on Robotics_, 38(6):3395-3413, 2022.\n' +
      '* [34] Fabian Jenelten, Junzhe He, Farbod Farshidian, and Marco Hutter. Dtc: Deep tracking control. _Science Robotics_, 9(86):eadh5401, 2024.\n' +
      '* [35] Donghyun Kim, Daniel Carballo, Jared Di Carlo, Benjamin Katz, Gerardo Bledt, Bryan Lim, and Sangbae Kim. Vision aided dynamic exploration of unstructured terrain with a small-scale quadruped robot. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 2464-2470. IEEE, 2020.\n' +
      '* [36] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In _Robotics: Science and Systems_, 2021.\n' +
      '* [37] Frederic Large, Dizan Vasquez, Thierry Fraichard, and Christian Laugier. Avoiding cars and pedestrians using velocity obstacles and motion prediction. In _IEEE Intelligent Vehicles Symposium, 2004_, pages 375-379, 2004.\n' +
      '* [38] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. _Science robotics_, 5(47):eabc5986, 2020.\n' +
      '* [39] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Robust and versatile bipedal jumping control through reinforcement learning. In _Robotics: Science and Systems_, 2023.\n' +
      '* [40] Qingkai Liang, Fanyu Que, and Eytan Modiano. Accelerated primal-dual policy optimization for safe reinforcement learning. _arXiv preprint arXiv:1802.06480_, 2018.\n' +
      '* [41] Qiayuan Liao, Zhongyu Li, Akshay Thirupanam, Jun Zeng, and Koushil Sreenath. Walking in narrow spaces: Safety-critical locomotion control for quadrupedal robots with duality-based optimization. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2723-2730. IEEE, 2023.\n' +
      '* [42] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence_, 2022.\n' +
      '* [43] Yuntao Ma, Farbod Farshidian, and Marco Hutter. Learning arm-assisted fall damage reduction and recovery for legged mobile manipulators. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 12149-12155. IEEE, 2023.\n' +
      '* [44] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu based physics simulation for robot learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.\n' +
      '* [45] Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid locomotion via reinforcement learning. _arXiv preprint arXiv:2205.02824_, 2022.\n' +
      '* [46] Matias Mattamala, Nived Chebrolu, and Maurice Fallon. An efficient locally reactive controller for safe navigation in visual teach and repeat missions. _IEEE Robotics and Automation Letters_, 7(2):2353-2360, 2022.\n' +
      '* [47] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. _Science Robotics_, 7(62):eabk2822, 2022.\n' +
      '* [48] Yashwanth Kumar Nakka, Anqi Liu, Guanya Shi, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Chance-constrained trajectory optimization for safe exploration and learning of nonlinear systems. _IEEE Robotics and Automation Letters_, 6(2):389-396, 2020.\n' +
      '* [49] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. [https://openai.com/research/benchmarking-safe-explorati](https://openai.com/research/benchmarking-safe-explorati) on-in-deep-reinforcement-learning, 2019.\n' +
      '* [50] Nikita Rudin, David Hoeller, Marko Bjelonic, and Marco Hutter. Advanced skills by learning locomotion and local navigation end-to-end. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2497-2503. IEEE, 2022.\n' +
      '\n' +
      '* Rudin et al. [2022] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In _Conference on Robot Learning_, pages 91-100. PMLR, 2022.\n' +
      '* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n' +
      '* Shin et al. [2023] Young-Ha Shin, Tae-Gyu Song, Gwanghyeon Ji, and Hae-Won Park. Actuator-constrained reinforcement learning for high-speed quadrupedal locomotion. _arXiv preprint arXiv:2312.17507_, 2023.\n' +
      '* Si et al. [2019] Wenwen Si, Tianhao Wei, and Changliu Liu. Agen: Adaptable generative prediction networks for autonomous driving. In _2019 IEEE Intelligent Vehicles Symposium (IV)_, pages 281-286. IEEE, 2019.\n' +
      '* Song et al. [2023] Yunlong Song, Angel Romero, Matthias Muller, Vladlen Koltun, and Davide Scaramuzza. Reaching the limit in autonomous racing: Optimal control versus reinforcement learning. _Science Robotics_, 8(82):eadg1462, 2023.\n' +
      '* depth settings. [https://www.stereolabs.com/docs/depth-sensing/depth-settings#fill-m](https://www.stereolabs.com/docs/depth-sensing/depth-settings#fill-m) ode, 2024.\n' +
      '* Teng et al. [2021] Sangli Teng, Yukai Gong, Jessy W. Grizzle, and Maani Ghaffari. Toward safety-aware informative motion planning for legged robots. _CoRR_, abs/2103.14252, 2021.\n' +
      '* Tessler et al. [2018] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. _arXiv preprint arXiv:1805.11074_, 2018.\n' +
      '* Thananjeyan et al. [2021] Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. Recovery rl: Safe reinforcement learning with learned recovery zones. _IEEE Robotics and Automation Letters_, 6(3):4915-4922, 2021.\n' +
      '* Tobin et al. [2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.\n' +
      '* Truong et al. [2023] Joanne Truong, April Zitkovich, Sonia Chernova, Dhruv Batra, Tingnan Zhang, Jie Tan, and Wenhao Yu. Indoorsim-to-outdoorreal: Learning to navigate outdoors without any outdoor experience. In _arXiv preprint arXiv:2305.01098_, 2023.\n' +
      '* Wang et al. [2023] Yikai Wang, Mengdi Xu, Guanya Shi, and Ding Zhao. Guardians as you fall: Active mode transition for safe falling. _arXiv preprint arXiv:2310.04828_, 2023.\n' +
      '* 434, 2023-03. ISSN 2771-3989. doi: 10.3929/ethz-b-000614683.\n' +
      '* Wijmans et al. [2023] Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, and Dhruv Batra. Emergence of maps in the memories of blind navigation agents. In _International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=IT](https://openreview.net/forum?id=IT)!4KjHSyI.\n' +
      '* Xiao et al. [2023] Wenli Xiao, Tairan He, John Dolan, and Guanya Shi. Safe deep policy adaptation. _arXiv preprint arXiv:2310.08602_, 2023.\n' +
      '* Yang et al. [2020] Chuanyu Yang, Kai Yuan, Qiuguo Zhu, Wanming Yu, and Zhibin Li. Multi-expert learning of adaptive legged locomotion. _Science Robotics_, 5(49):eabb2174, 2020.\n' +
      '* Yang et al. [2022] Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, and Xiaolong Wang. Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=nhnJ3oo6AB](https://openreview.net/forum?id=nhnJ3oo6AB).\n' +
      '* Yang et al. [2022] Tsung-Yen Yang, Tingnan Zhang, Linda Luu, Sehoon Ha, Jie Tan, and Wenhao Yu. Safe reinforcement learning for legged locomotion. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2454-2461. IEEE, 2022.\n' +
      '* Yang et al. [2023] Yuxiang Yang, Guanya Shi, Xiangyun Meng, Wenhao Yu, Tingnan Zhang, Jie Tan, and Byron Boots. Cajun: Continuous adaptive jumping using a learned centroidal controller. _arXiv preprint arXiv:2306.09557_, 2023.\n' +
      '* Zhang et al. [2022] Chong Zhang, Wanming Yu, and Zhibin Li. Accessibility-based clustering for efficient learning of locomotion skills. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 1600-1606. IEEE, 2022.\n' +
      '* Zhang et al. [2023] Chong Zhang, Nikita Rudin, David Hoeller, and Marco Hutter. Learning agile locomotion on risky terrains. _arXiv preprint arXiv:2311.10484_, 2023.\n' +
      '* Zhang et al. [2021] Chong Zhang, Jin Jin, Jonas Frey, Nikita Rudin, Matias Eduardo Mattamala Aravena, Cesar Cadena, and Marco Hutter. Resilient legged local navigation: Learning to traverse with compromised perception end-to-end. In _41st IEEE Conference on Robotics and Automation (ICRA 2024)_, 2024.\n' +
      '* Zhao et al. [2021] Weiye Zhao, Tairan He, and Changliu Liu. Model-free safe control for zero-violation reinforcement learning. In _5th Annual Conference on Robot Learning_, 2021.\n' +
      '* Zhao et al. [2023] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu. State-wise safe reinforcement learning: A survey. In Edith Elkind, editor, _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 6814-6822. International Joint Conferences on Artificial Intelligence Organization, 8 2023. URL [https://doi.org/10.24963/ijcai.2023/763](https://doi.org/10.24963/ijcai.2023/763).\n' +
      '* Zhuang et al. [2023] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwerfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. _arXiv preprint arXiv:2309.05665_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>