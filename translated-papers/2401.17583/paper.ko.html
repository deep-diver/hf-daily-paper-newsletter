<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 민첩하지만 안전: 충돌 없는 고속 다리 이동 학습\n' +
      '\n' +
      'Tairan He\\({}^{\\dagger\\dagger}\\) Chong Zhang\\({}^{2\\ddagger}\\) Wenli Xiao\\({}^{1}\\) Guanqi He\\({}^{1}\\) Changliu Liu\\({}^{1}\\) Guangya Shi\\({}^{1}\\)\n' +
      '\n' +
      '({}^{1}\\)ETH Zurichie Mellon University \\({}^{2}\\)ETH Zurichie Mellon University\n' +
      '\n' +
      '\\({}^{\\dagger}\\)equal Contributions[https://agile-but-safe.github.io](https://agile-but-safe.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '어수선한 환경을 항해하는 다리 로봇은 효율적인 작업 수행을 위해 공동으로 _agile_와 장애물 또는 인간과의 충돌을 피하기 위해 _safe_가 있어야 한다. 기존 연구는 안전성을 보장하기 위해 보수적인 제어기(\\(<1.0\\) m/s)를 개발하거나 잠재적으로 치명적인 충돌을 고려하지 않고 민첩성에 중점을 둔다. 본 논문에서는 4족 로봇의 민첩하고 충돌 없는 운동을 가능하게 하는 학습 기반 제어 프레임워크인 ABS(Agile But Safe)를 소개한다. ABS는 장애물 속에서 민첩한 운동 기술을 실행하기 위한 민첩한 정책과 장애를 방지하기 위한 복구 정책을 포함하며, 협동적으로 고속 및 충돌 없는 항법을 달성한다. ABS의 정책 전환은 학습된 제어 이론 도달 회피 가치 네트워크에 의해 제어되며, 이는 또한 목적 함수로서 복구 정책을 안내하여 폐루프에서 로봇을 보호한다. 훈련 과정은 애자일 정책, 도달 회피 가치 네트워크, 복구 정책 및 확장 표현 네트워크의 학습을 모두 시뮬레이션에 포함한다. 이러한 훈련된 모듈은 온보드 감지 및 계산으로 실제 세계에 직접 배치될 수 있으며, 이는 정적 및 동적 장애물 모두를 갖는 제한된 실내 및 실외 공간에서 고속 및 충돌 없는 내비게이션으로 이어진다(도 1).\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '어수선한 환경에서 다리 로봇의 민첩한 움직임은 민첩성과 안전 사이의 고유한 상충 관계로 인해 사소한 문제가 없으며 견고성과 효율성을 모두 요구하는 실제 응용 분야에 중요하다. 기존 작업은 일반적으로 안전성을 보장하기 위해 제한된 민첩성(속도 \\(<1\\) m/s)을 나타내거나[35, 17, 7, 22, 27, 67, 12, 46, 41, 72], 또는 내비게이션 시나리오 [45, 53]에서 안전성을 고려하지 않고 민첩성을 최대화하는 데만 초점을 맞춘다. 우리의 작업은 고속(최대 속도 \\(>)을 달성함으로써 스스로를 구별한다. 3\\) m/s), 충돌 없는 4족 보행은 어수선한 환경에서 이루어진다.\n' +
      '\n' +
      '항법 영역에서 기존 작업의 민첩성 한계는 다양한 요인에서 비롯된다. 공식과 관련하여, 일부 디커플 운동 및 항해 계획을 두 개의 하위 작업으로 나누고 계층 시스템[35, 17, 7, 27, 46, 72]을 구축한다. 이러한 디커플링은 최적 솔루션[55]으로부터 제어기를 구속할 뿐만 아니라, 안전을 보장하기 위한 보수적인 거동을 초래하여, 시스템이 운동 민첩성을 완전히 해제하는 것을 제한한다. 이 작업은 대신 충돌 없는 운동을 위해 조인트 레벨 동작을 직접 출력하여 지정된 목표 위치에 도달하는 엔드 투 엔드 컨트롤러를 학습한다. 우리의 접근법은 로봇이 주행과 내비게이션을 통합함으로써 어려운 지형을 극복하기 위해 엔드 투 엔드 컨트롤러를 학습하는 최근 작업[66, 50, 71, 28]에서 영감을 얻었다.\n' +
      '\n' +
      '제어기와 관련하여 일부 작업은 안전성을 보장하기 위해 모델 예측 제어(MPC) 및 장벽 기능과 같은 단순화된 모델을 사용하는 모델 기반 방법을 사용한다[22, 12, 41]. 모델 불일치 및 미끄러짐과 같은 잠재적 제약 위반은 온라인 계산 부담과 함께 이러한 컨트롤러를 민첩한 움직임 및 야생에서의 안정적인 배치로부터 제한한다[22, 32, 38]. 반면에, 다리 운동에서 모델 없는 강화 학습(RL)의 최근 진행은 모델 기반 컨트롤러가 어수선한 환경에서 잠재적으로 안전하지 않지만 [45, 53, 28, 38, 47, 34, 75, 43, 69, 11]을 달성하지 못한 놀라운 민첩한 운동 기술을 보여주었다. 우리는 모델이 없는 RL의 유연성과 민첩성을 활용하고 제어 이론 도구를 사용하여 이를 추가로 보호한다.\n' +
      '\n' +
      'ABS라는 명칭으로, 우리의 프레임워크는 단일 RL 정책을 넘어선다. 첫째, 섹션 IV에 제시된 바와 같이 충돌 회피를 운동에 통합하는 모델 없는 지각 민첩성 정책을 통해 Go1 로봇이 충돌을 인지하면서 최대 \\(3.1\\)m/s의 속도를 달성할 수 있다. 그러나 RL 정책은 안전을 보장하지 않으므로 민첩한 정책이 실패할 수 있는 경우 다른 복구 정책으로 로봇을 보호한다(섹션 VI 참조). 어떤 정책을 통제할지 결정하기 위해, 우리는 민첩한 정책의 위험 수준을 정량화하기 위해 정책 조건 도달 회피(RA) 가치 네트워크를 사용한다. 이것은 모델 없는 RA 값이 해밀턴-자코비 도달 가능성 이론[3]에 기초하여 효율적으로 학습될 수 있는 [29]에서 영감을 받는다. RA 가치 네트워크는 시뮬레이션에서 학습된 애자일 정책에 의해 수집된 데이터와 함께 할인된 RA 벨만 방정식에 의해 훈련된다. 임계값을 넘어 미분 가능한 RA 값 네트워크는 복구 정책을 안내하는 기울기 정보를 제공하여 루프를 폐쇄하며, 이는 섹션 V에서 추가로 제시될 것이다.\n' +
      '\n' +
      '다양한 시나리오에서 일반화할 수 있는 충돌 회피 행동을 얻기 위해, 우리는 정책 및 RA 가치 훈련에 저차원 외부 수용 기능을 사용한다: 로봇에서 장애물까지의 여러 광선들의 이동 거리. 이를 달성하기 위해 섹션 VII에 자세히 설명된 대로 깊이 이미지를 광선 거리에 매핑하는 시뮬레이션 데이터를 사용하여 엑테로셉션 표현(또는 광선 예측) 네트워크를 추가로 훈련한다. 이를 통해 온보드 센싱과 연산으로 고속 이동에서 강력한 충돌 회피를 달성한다.\n' +
      '\n' +
      '간단히 말해서, 우리는 다음과 같이 우리의 기여를 식별한다:\n' +
      '\n' +
      '1. 새로운 훈련 방법을 가진 고속 이동에서 장애물 회피를 위한 통찰력 있는 민첩성 정책.\n' +
      '2. 학습된 애자일 정책에 기반한 RA 값 추정을 위한 새로운 제어 이론 데이터 기반 방법.\n' +
      '3. 고속 충돌 없는 이동을 위해 애자일 정책과 복구 정책이 협력하고 RA 값이 정책 스위치를 제어하고 복구 정책을 안내하는 이중 정책 설정.\n' +
      '4. 일반화 가능한 충돌 회피 능력을 위한 저차원 장애물 정보를 예측하는 익스테로셉션 표현 네트워크.\n' +
      '5. 실내 및 실외 모두에서 장애물 가운데 ABS의 우수한 안전 조치 및 최첨단 민첩성의 검증(도 1).\n' +
      '\n' +
      '## II 관련 작품들\n' +
      '\n' +
      '### _Agile Legged Locomotion__\n' +
      '\n' +
      'MPC와 같은 모델 기반 방법은 단순화된 모델 및 수작업된 가트를 사용하여 동적 다리 운동[5, 14, 15, 24, 33, 25]을 가능하게 한다. 시뮬레이션과 실험실 조건에서 인상적인 성능에도 불구하고 이러한 방법은 모델 불일치와 예상치 못한 미끄러짐으로 인해 야생에서 어려움을 겪는다[32, 38]. 온라인 계산 부담은 또한 지각 모델 기반 컨트롤러를 민첩한 움직임으로부터 제한한다[12].\n' +
      '\n' +
      '최근, RL 기반 제어기는 고속 주행[31, 45, 53], 도전적인 지형 횡단[71, 28, 34, 75, 11], 점프[39, 69], 낙상 회복[66, 43, 62, 70]을 포함한 강인한 운동[66, 38, 47, 23] 및 민첩한 운동 기술에 대해 유망한 결과를 보여주었다. 그러나 기존의 민첩한 이동 작업은 대부분 경주 또는 어려운 지형을 극복하기 위한 능숙한 동작을 위해 빠른 속도를 달성하는 것으로 나타났다. 어수선한 환경에서 이러한 방법은 충돌 회피를 위한 높은 수준의 항법 모듈을 필요로 하며, 이는 일반적으로 보수적이고 모터 한계보다 훨씬 낮은 운동을 크게 제한한다[63, 72]. 이에 반해 본 논문에서는 다용도 항해를 위한 민첩한 충돌 회피를 연구한다.\n' +
      '\n' +
      '### _Legged 충돌회피_\n' +
      '\n' +
      '고전적인 방법은 로봇의 동역학을 고려하지 않고 구성 공간에서 충돌 없는 모션 계획[35, 17, 7]을 가진 다리의 로봇에서 충돌 회피를 해결하여 느리고 정적으로 안정적인 걸음걸이로 이어진다. MPC 기반 방법[22, 12, 57, 41]은 장애물까지의 거리를 최적화 제약으로 처리하여 계획과 제어를 통합한다. 그러나 모델 기반 제어기의 단점 및 느린 움직임(속도 \\(<0.5\\) m/s)으로 인해 어려움을 겪고 있다.\n' +
      '\n' +
      '학습 기반 방법은 또 다른 선택입니다. Hoeller et al. [27] 및 Zhang et al. [72]는 주행 제어기에 의해 추적될 트위스트 명령을 출력하는 RL 기반 정책을 트레인하는 반면, 속도 명령은 안전을 보장하기 위해 \\(1\\) m/s로 제한된다. 그러나, 높은 수준의 설계자는 낮은 수준의 추적 오류를 인식하지 못하기 때문에, 항법 계획과 이동 제어의 디커플링은 고속 이동을 위험하게 만든다.\n' +
      '\n' +
      'Yang et al. [67]은 대신 깊이 영상과 고유수용성 데이터를 관절 동작에 직접 매핑하는 종단간 RL 기반 솔루션을 제공하지만 로봇은 앞으로만 걸을 수 있고 속도는 \\(\\sim 0.4\\) m/s로 제한된다. 이와는 대조적으로, 본 연구는 충돌 회피와 함께 전방위 신속 주행을 위한 엔드 투 엔드 애자일 정책을 도입하고 RA 값과 복구 정책으로 로봇을 보호한다. 우리가 아는 한, 우리의 연구는 \\(3.1\\) m/s까지의 최대 속도를 가진 충돌 없는 4족 운동을 최초로 검증한다. 동적 적대적 장애물이 있는 좁은 공간에서도, 우리의 시스템은 여전히 \\(2.5\\) m/s의 피크 속도와 \\(1.5\\) m/s의 평균 속도에 도달할 수 있다(그림 1(f)).\n' +
      '\n' +
      '### _Safe reinforcement learning_\n' +
      '\n' +
      '안전한 RL[74]을 수행하기 위한 방법에는 크게 두 가지 범주가 있다: 1) _end-to-end_ 방법 및 2) _hierarchical_ 방법. 라그랑지안 기반 방법[49, 4, 40, 58]은 정책 파라미터와 함께 라그랑지 승수가 최적화될 수 있는 안전 제약 조건을 만족시키기 위해 원시-이중 최적화 문제를 해결하는 가장 대표적인 _end-to-end_ 안전 RL 방법이다. 그러나, 제약은 종종 수렴 전에 시행되어 탐색을 방해하고 수익을 낮춘다[49].\n' +
      '\n' +
      '_Hierarchical_ safe RL 방법들은 기본 역학들[68, 65, 13] 및 제어-이론적 안전 인증서들[48, 73, 10]의 구조들을 사용하여 안전하지 않은 RL 액션들을 보호한다. 이러한 방법은 일반적으로 학습 전에 사용 가능한 역학 또는 안전 인증서 기능의 가정을 기반으로 하며, 이는 고차원 복잡한 시스템에 대한 확장성을 크게 제한한다. 최근 일부 연구에서는 안전평가가 명목상의 정책이 안전하지 않다는 것을 나타낼 때 RL을 보호하기 위해 안전 예측 네트워크(또는 안전 평론가) 및 안전 백업 정책을 학습한다[59, 30]. 그럼에도 불구하고 이러한 프레임워크는 안전 비평가와 백업 정책 간의 상호 작용이 부족하여 백업 정책이 안전 비평가들을 만족시키기 위해 명시적인 최적화 없이 안전을 회복할 수 있다는 까다로운 가정에 의존한다.\n' +
      '\n' +
      '우리의 접근법은 _hierarchical_ 방법과 일치하지만 독특한 전략이 두드러진다. 우리는 애자일 정책의 도달-회피 값을 추정하는 데 초점을 맞추고 도달-회피 값의 기울기 정보를 시스템에 피드백하여 폐루프 내에서 복구 정책을 안내한다. 이 혁신적인 접근 방식은 역동적이고 적응적인 복구 프로세스를 가능하게 합니다. 특히, 모든 모듈은 모델이 없는 접근법을 사용하여 시뮬레이션으로 훈련되어 본 방법의 일반화 가능성과 확장성을 향상시킨다.\n' +
      '\n' +
      '###_Reach-Avoid Problems and Hamilton-Jacobi Analysis__\n' +
      '\n' +
      '접근 회피(RA: Reach-avoid) 문제는 특정 바람직하지 않은 상태를 피하면서 목표물에 도달하기 위해 시스템을 탐색하는 것을 포함한다. 해밀턴-자코비(HI) 도달성 분석[3]은 관련된 해밀턴-자코비 편미분 방정식을 분석하여 이 문제를 해결하며, 이는 시스템이 안전하게 유지되기 위해 빠져 있어야 하는 일련의 상태를 제공한다.\n' +
      '\n' +
      'HJ 도달성 분석은 시스템의 차원[9]과 함께 기하급수적으로 증가하는 계산 문제에 직면한다. 최근의 학습 기반 방법 [2]는 연관된 HJ 편미분 방정식과 제약 조건을 만족하는 가치 네트워크를 학습하여 HJ 도달 가능성 분석을 고차원 시스템으로 확장하려고 한다. 그러나, 그들은 학습하기 전에 여전히 명시적인 시스템 해밀토니안 표현을 필요로 한다.\n' +
      '\n' +
      '우리의 방법은 수축 속성을 활용하여 시간 할인된 도달 회피 벨만 방정식을 유도하는 또 다른 작업[29, 20]을 기반으로 한다. 그러나 RL 훈련 중 정책 진단 RA 값을 학습하는 이전 작업과 달리 정책 조건 RA 값 네트워크를 학습한다. 이는 글로벌 RA 세트의 식별 가능성 문제를 회피함으로써 계산 부담을 줄일 뿐만 아니라 훈련된 민첩성 정책에 가장 적합하다.\n' +
      '\n' +
      '## III 개요 및 예비\n' +
      '\n' +
      '### _Nomenclature_\n' +
      '\n' +
      '우리는 참조를 위해 표 I에서 이 논문 전반에 걸쳐 사용되는 중요한 기호와 약어를 제시한다.\n' +
      '\n' +
      '### _Problem Formulation_\n' +
      '\n' +
      '#### Iii-B1 Dynamics\n' +
      '\n' +
      '(s_{t}\\in\\mathcal{S}\\subset\\mathbb{R}^{n_{s}\\)을 시간 단계 \\(t\\)에서의 상태라고 하자, 여기서 \\(n_{s}\\)은 상태 공간의 차원이다. \\(a_{t}\\in\\mathcal{A}\\subset\\mathbb{R}^{n_{a}\\)은 시간 단계 \\(t\\)에서의 제어 입력이고, 여기서 \\(n_{a}\\)은 동작 공간의 차원이다. 상기 시스템 역학은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[s_{t+1}=f(s_{t},a_{t}), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(f:\\mathcal{S}\\times\\mathcal{A}\\rightarrow\\mathcal{S}\\)는 현재의 로봇 상태와 제어를 다음 상태로 매핑하는 함수이다. 단순화를 위해 이 논문은 분석적 형태가 없을 수 있는 결정론적 역학을 고려한다. 우리는 고유 감각 및/또는 외각 감각으로부터 로봇 관찰을 \\(o_{t}=h(s_{t})\\)로 표시하며, 여기서 \\(h:\\mathcal{S}\\rightarrow\\mathcal{O}\\)은 센서 매핑이다. 민첩 정책과 회복 정책의 구체적인 관찰과 실천 공간은 Ⅳ절과 Ⅵ절에서 소개될 것이다.\n' +
      '\n' +
      '###### Iii-B2 목표 및 정책\n' +
      '\n' +
      '목표조건 강화학습[42]은 목표조건 정책\\(\\pi:\\mathcal{O}\\times\\Gamma\\rightarrow\\mathcal{A}\\)을 통해 목표상태에 도달하도록 학습한다. 보상함수 \\(r:\\mathcal{S}\\times\\mathcal{A}\\times\\Gamma\\rightarrow\\mathbb{R}\\)와 할인계수 \\(\\gamma_{\\text{RL}\\)을 사용하여 정책은 목표 분포 \\(p_{G}\\)에 대한 기대 누적 수익률을 최대화하기 위해 학습된다:\n' +
      '\n' +
      '[J(\\pi)=\\mathbb{E}_{u_{t}\\sim\\pi(\\cdot|o_{t},G),G\\sim p_{G}}\\left[\\sum_{t}\\gamma_{\\text{RL}}}^{t}r(s_{t},a_{t},G\\right]\\tag{2}\\right]\n' +
      '\n' +
      '###### Iii-B3 실패 세트, 목표 세트 및 도달 회피 세트\n' +
      '\n' +
      '우리는 로봇이 들어갈 수 없는 안전하지 않은 상태(예: 충돌)로 _failure set_\\(\\mathcal{F}\\subseteq\\mathcal{S}\\)를 나타낸다. _failure set_는 Lipschitz-continuous function \\(\\zeta:\\mathcal{S}\\rightarrow\\mathbb{R}\\), 즉 \\(s\\in\\mathcal{F}\\Leftrightarrow\\zeta(s)>0\\)의 영하위 집합으로 나타낼 수 있다. 상기 _target set_\\(\\Theta\\subset\\mathcal{S}\\)는 원하는 상태(즉, 목표 상태)로 정의된다. 마찬가지로, _target set_는 Lipschitz-continuous function \\(l:\\mathcal{S}\\rightarrow\\mathbb{R}\\), 즉 \\(s\\in\\Theta\\Leftrightarrow l(s)\\leq 0\\)의 제로-서브레벨 집합으로 표현될 수 있다. 우리는 상태로부터의 미래 궤적 롤아웃으로서 \\(\\xi_{s_{t}^{\\pi}(\\cdot)\\)을 \\(\\xi_{s_{t}^{\\pi}(0)=s_{t}\\))까지 정책 \\(\\pi\\\\(s_{T}\\)을 사용하여 표시한다. _reach-avoid set_ conditioned on policy \\(\\pi\\)는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{RA}^{\\pi}(\\theta;\\mathcal{F}):=\\{s_{t}\\in\\mathcal{S}\\left|\\xi_{s_{t}}^{\\pi}(T-t)\\in\\theta\\wedge\\right.\\tag{3}\\t} \\[\\left.\\ forall t^{\\prime}\\in[0,T-t],\\xi_{s_{t}}^{\\pi}(t^{\\prime}) \\notin\\mathcal{F}\\},\\right.\\\n' +
      '\n' +
      '이는 모든 이전 타임스텝에서 일관되게 \\(\\mathcal{F}\\)을 피하면서 시스템을 \\(\\theta\\)으로 이끌 수 있는 정책 \\(\\pi\\)에 의해 지배되는 상태 집합을 나타낸다.\n' +
      '\n' +
      '###### Iii-B4 Reach-Avoid Value와 Time Discounted Reach-Avoid Bellman Equation\n' +
      '\n' +
      '정책 조건 도달 회피 값은 다음과 같다. \\(V_text{RA}^{\\pi}^{\\pi}(s)\\leq 0\\Leftrightarrow s\\in\\mathcal{RA}^{\\pi}( \\theta;\\mathcal{F)\\. [29]의 부록 A에 따르면, 값 함수\\(V_{\\text{RA}^{\\pi}}^{\\pi}(s)\\)가 고정점 도달 회피 벨만 방정식을 만족한다는 것은 쉽게 확장될 수 있다. (우리의 정책 조건화된 값 함수는 일반적인 값 함수의 특별한 경우):\n' +
      '\n' +
      '\\[V_{\\text{RA}^{\\pi}}^{\\pi}(s)=\\max\\left\\{\\zeta(s),\\min\\left\\{l(s),V_{\\text{RA}^{\\pi}}^{\\pi}\\left(f\\left(s,\\pi(s)\\right)\\right\\\\right\\,.\\tag{4}\\\\)\n' +
      '\n' +
      '그러나 식 (4)가 가치함수의 공간에서 수축을 초래할 것이라는 보장은 없다. 데이터 기반 근사화에 접근하기 위해 시간 할인된 도달 회피 벨만 방정식[29]을 활용하여 할인된 정책 조건 도달 회피 값\\(V_{\\text{RA}}^{\\pi}(s)\\)으로 정의되는 할인된 정책 조건 도달 회피 값을 수축한다.\n' +
      '\n' +
      '\\gamma_{\\text{RA}}^{\\pi}(s)= \\gamma_{\\text{RA}}\\max\\left\\zeta(s),\\min\\left\\{l(s),V_{\\text{RA}}^{\\pi}\\left(f\\left(s,\\pi(s)\\right)\\right\\\\tag{5}\\]\\[+(1-\\gamma_{\\text{RA}})\\max\\left\\{l(s),\\zeta(s)\\right\\\\.\n' +
      '\n' +
      '[29]에 이어, \\(\\gamma_{\\text{RA}}^{\\pi}(s)\\)에 대한 \\(V_{\\text{RA}}^{\\pi}(s)\\)은 항상 \\(\\gamma_{\\text{RA}}\\in[0,1)\\)에 대한 \\(V_{\\text{RA}}^{\\pi}(s)\\)의 과소근사임을 알 수 있으며, \\(\\gamma_{\\text{RA}}(s)\\)이 \\(1\\\\)에 접근함에 따라 \\(V_{\\text{RA}}^{\\pi}(s)\\)으로 수렴함을 알 수 있다. \\(V_{\\text{RA}}^{\\pi}(s)\\)에서 \\(V_{\\text{RA}}^{\\pi}(s)\\)까지의 과소근사는 \\(V_{\\text{RA}}^{\\pi}(s)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(s)\\)\\(V_{\\text{RA}}^{\\pi}(\\theta;\\mathcal{F)\\)의 임계값에 대한 차폐 방법이 시스템을 제어-이론 _reach-avoid set_\\(\\mathcal{F)\\(\\theta;\\math\n' +
      '\n' +
      '도. 2: ABS의 개요: (a) ABS 프레임워크 내에 4개의 훈련된 모듈이 있다: 1) 애자일 정책(IV절에서 도입됨)은 장애물에 대한 최대 민첩성을 달성하기 위해 훈련된다; 2) 애자일 정책에 조건화된 RA 값을 안전 지표로 예측하기 위해 훈련된다; 3) 복구 정책(VI절에서 도입됨)은 RA 값을 낮추는 원하는 트위스트 명령(2D 선형 속도\\(v_{x}^{\\pi},v_{y}^{\\pi}\\) 및 요 각속도\\(v_{x}^{\\pi}\\)을 추적하기 위해 훈련된다; 4) 광선 예측 네트워크(VII절에서 도입됨)는 광선 거리를 정책으로 예측하기 위해 훈련된다; 깊이 영상이 주어진 경우, 광선 거리를 예측하기 위해 훈련된다. (b) ABS 전개 아키텍처의 일러스트레이션. 이중 정책 설정은 _RA 값 네트워크로부터 추정된 \\(\\hat{V}\\)에 기초하여 _agile policy_와 _recovery policy_ 사이를 전환한다: 1) \\(\\hat{V}<V_{text{threshold}\\)이면 _agile policy_가 장애물 속에서 탐색하도록 활성화되고, 2) \\(\\hat{V}>V_{\\text{threshold}\\)이면 _recovery policy_가 활성화되어 제약된 최적화를 통해 _RA 값_를 낮추는 트위스트 명령을 추적한다.\n' +
      '\n' +
      '### _System Structure_\n' +
      '\n' +
      '도 2에 도시된 바와 같이, 제안된 ABS 프레임워크는 애자일 정책\\(\\pi^{\\text{Agile}}\\)과 복구 정책\\(\\pi^{\\text{Recovery}}\\)이 함께 작동하여 애자일 및 안전한 이동 기술을 가능하게 하는 이중 정책 설정을 포함한다. 애자일 정책은 기본 충돌 회피 능력을 가진 목표 명령(목표 2D 위치 및 제목)을 기반으로 로봇을 탐색하기 위해 유닛리 Go1에서 애자일 모터 기술(최대 \\(3.1\\) m/s)을 수행한다(섹션 IV 참조). 복구 정책은 충돌을 피할 수 있는 트위스트 명령(2D 선형 속도\\(v_{x}^{c},v_{y}^{c}\\) 및 요레이트\\(\\omega_{z}^{c}\\)을 신속하게 추적함으로써 민첩한 정책을 보호하는 역할을 한다(V-C 및 VI 섹션 참조). 두 정책 모두 PD 컨트롤러에 의해 추적되는 공동 대상을 출력합니다.\n' +
      '\n' +
      '배치 중에 정책 스위치는 신경망 \\(\\hat{V}\\)을 사용하여 추정된 민첩한 정책에 조건화된 RA 값에 의해 제어된다(섹션 V 참조). 안전 임계값 \\(V_{\\text{threshold}}=-\\epsilon\\)에서 \\(\\epsilon\\)은 작은 양수이다.\n' +
      '\n' +
      '* \\(\\hat{V}\\geq V_{\\text{threshold}}\\)이면 \\(\\hat{V}\\)을 기준으로 안전을 유지하면서 로봇을 목표에 더 가깝게 구동하는 트위스트 명령을 탐색한다(식 (21)도 참조). 복구 정책은 검색된 트위스트 명령을 제어하고 추적합니다.\n' +
      '* \\(\\hat{V}<V_{\\text{threshold}}\\)일 경우 민첩한 정책이 통제한다.\n' +
      '\n' +
      '이 시스템은 대부분의 시간 내에 애자일 정책을 활성화하고, 애자일 정책인 _i.e., \\(\\hat{V}<V_{\\text{threshold}}\\)에 대해 다시 안전해질 때까지 위험 상황에서 복구 정책을 안전 장치로 사용할 것으로 기대한다.\n' +
      '\n' +
      '충돌 회피를 위해 민첩한 정책과 RA 가치 네트워크 모두 외부 수용적 입력이 필요하다. [28, 16, 1]에서 영감을 얻은 우리는 희박한 LiDAR 판독과 유사하게 11개의 광선이 로봇에서 장애물까지 이동하는 거리인 저차원 익스테로셉션 표현을 사용하도록 선택한다. 우리는 원시 깊이 이미지를 예측된 광선 거리에 매핑하는 네트워크를 훈련하고(섹션 VII 참조), 광선 거리는 애자일 정책과 RA 가치 네트워크에 대한 관찰의 일부로 사용된다.\n' +
      '\n' +
      '요약하자면, 도 2의 (a)에 도시된 바와 같이, ABS는 시뮬레이션에서 모두 4개의 모듈을 훈련시킬 필요가 있다:\n' +
      '\n' +
      '1. _agile policy_(섹션 IV)는 충돌 없이 목표에 도달하도록 RL을 통해 트레이닝된다. 가장 민첩한 운동 기술을 장려하기 위해 목표 달성 보상을 설계합니다.\n' +
      '2. _RA 값 네트워크_(섹션 V)는 애자일 정책에 대한 안전성을 나타내도록 트레이닝된다. 우리는 RA bellman 방정식(식 (5))을 기반으로 데이터 구동 방법을 사용하여 훈련하고, 애자일 정책을 실행함으로써 시뮬레이션에서 데이터를 수집한다.\n' +
      '3. _recovery policy_(Section VI)는 고속 움직임으로부터 트위스트 명령을 빠르게 추적하도록 훈련된다.\n' +
      '4. _ray-예측 네트워크_(섹션 VII)는 깊이 이미지들로부터 광선 거리 관측들을 예측하도록 트레이닝된다. 시뮬레이션은 민첩성 정책을 적용하여 합성 깊이 영상과 광선 거리를 수집한다.\n' +
      '\n' +
      '4개의 모듈 모두 훈련 후 실제 세계에 직접 배치됩니다.\n' +
      '\n' +
      '## IV 학습 민첩성 정책\n' +
      '\n' +
      'III-C절에서 언급했듯이, 우리는 장애물 속에서 높은 민첩성을 달성하기 위해 민첩성 정책을 훈련한다. 민첩한 운동을 학습하기 위한 이전 작업은 일반적으로 개방되고 평평한 지형에서 속도 명령을 추적하기 위해 속도 추적 공식[45, 53], _i.e._를 사용한다. 그러나 복잡한 환경에서 이러한 속도 추적 정책을 위한 탐색 플래너를 설계하는 것은 간단하지 않을 수 있다. 안전을 보장하기 위해 설계자는 보수적이어야 하며 이동 정책의 민첩성을 완전히 발휘할 수 없을 수 있습니다.\n' +
      '\n' +
      '대신 [50, 71]에서 영감을 받아 민첩성을 극대화하기 위해 목표 도달 공식을 사용합니다. 특히, 충돌 없이 에피소드 시간 내에 지정된 목표에 도달할 수 있는 감각 운동 기술을 개발하기 위해 로봇을 훈련한다. 민첩성은 또한 기본 프레임에서 높은 속도를 추구하는 보상 용어에 의해 장려된다. 이렇게 함으로써, 로봇은 충돌을 피하면서 자연스럽게 최대한의 민첩성을 얻을 수 있도록 학습한다.\n' +
      '\n' +
      '이 섹션에서는 민첩한 정책 학습의 세부 사항을 설명합니다. 민첩성을 위한 목표 도달 및 속도 추적 공식 간의 자세한 비교는 섹션 IX-A1에 나와 있다.\n' +
      '\n' +
      '###_관찰공간 및 동작공간_###\n' +
      '\n' +
      '애자일 정책의 관측공간은 발접촉(c_{f\\in\\{1,2,3,4\\}}\\), 기저각속도\\(\\omega\\), 기저프레임에서의 투사중력\\(g\\), 골명령(G^{c}\\)(즉, 골의 상대위치와 헤딩), 기저프레임에서의 에피소드(T-t\\), 관절위치\\(q\\), 관절속도\\(\\dot{q}\\), 이전 프레임의 동작\\(a\\), 외감각(즉, 광선거리의 로그값)으로 구성된다. 여기서는 간단한 동작을 위한 단계 기반 타임스탬프(\\(t-1\\)와 그렇지 않은 경우 \\(t\\)을 생략하였다. 이 모든 변수들의 집합을 \\(o^{\\text{Agile}}\\)이라고 한다.\n' +
      '\n' +
      '이들 관측치 중 \\(g\\)과 \\(G^{c}\\)은 각각 방향 및 오도메트리에 대한 상태 추정치가 필요하다. 다른 모든 값은 누적 드리프트 없이 원시 센서 데이터에서 사용할 수 있습니다. \\(g\\)(즉, 롤 및 피치)에 대한 IMU 기반 방향 추정은 일반적으로 매우 정확하며, 우리의 정책은 오도메트리 드리프트를 효과적으로 처리할 수 있다(실행 시 갑자기 목표를 변경할 수도 있으므로 섹션 IX-C 참조). 따라서, 우리의 민첩한 정책은 모델 기반 제어기[6, 32, 18]에 문제가 될 수 있는 부정확한 상태 추정기에 강건하다.\n' +
      '\n' +
      '민첩한 정책의 행동 공간은 12-d 공동 목표로 구성된다. PD 제어기는 이들 조인트 타겟들을 조인트 토크로 변환하여 \\(a\\) 추적한다:\n' +
      '\n' +
      '\\[\\tau=K_{p}(a-q)-K_{d}\\dot{q}. \\tag{6}\\]\n' +
      '\n' +
      '완전히 연결된 MLP는 관측치(o^{\\text{Agile}}\\)를 액션(a\\)에 매핑한다.\n' +
      '\n' +
      '### _Rewards_\n' +
      '\n' +
      '우리의 보상 함수는 여러 항을 합한 것이다:\n' +
      '\n' +
      '\\[r=r_{\\text{penalty}}+r_{\\text{task}}+r_{\\text{regularization}}, \\tag{7}\\]\n' +
      '\n' +
      '여기서 각 항은 다음과 같이 하위 항으로 더 나누어질 수 있다.\n' +
      '\n' +
      '###### Iii-B1 벌칙 보상\n' +
      '\n' +
      '간단한 패널티 디자인을 사용합니다:\n' +
      '\n' +
      '\\[r_{\\text{penalty}}=-100\\cdot\\mathds{1}(\\text{undesired collision}), \\tag{8}\\]\n' +
      '\n' +
      '여기서 원하지 않는 충돌은 베이스, 허벅지 및 종아리에서의 충돌, 및 발에서의 수평 충돌을 지칭한다.\n' +
      '\n' +
      '###### Iii-B2 과제 보상\n' +
      '\n' +
      '상기 작업 보상은:\n' +
      '\n' +
      '\\[\\begin{split} r_{\\text{task}}&=60\\cdot r_{\\text{ possort}}+60\\cdot r_{\\text{postit}+30\\cdot r_{\\text{heading}}\\\\&-10\\cdot r_{\\text{stand}}+10\\cdot r_{\\text{agile}-20\\cdot r_{\\text{stall},\\end{split} \\tag{9}\\}\n' +
      '\n' +
      '즉, 목표 도달에 대한 탐사를 장려하기 위한 소프트 포지션 트랙킹 용어(r_{\\text{possort}\\), 목표 도달에 정지하기 위한 로봇을 보강하기 위한 타이트 포지션 트랙킹 용어(r_{\\text{postit}\\), 목표 근접에서 로봇의 헤딩을 규제하기 위한 헤딩 트랙킹 용어(r_{\\text{heading}\\), 목표 도달에 서 있는 자세를 장려하기 위한 스탠딩 용어(r_{\\text{stand}\\), 높은 속도를 장려하기 위한 애자일 용어(r_{\\text{agile}}\\), 대기 행동에 불이익을 주기 위한 스톨 용어(r_{\\text{stall}\\)를 포함한다. 이러한 용어는 로봇이 시간을 낭비하지 않으면서 가능한 한 빨리 적절한 헤딩과 자세로 목표에 도달해야 함을 보장한다.\n' +
      '\n' +
      '구체적으로, 우리의 추적 용어(\\(r_{\\text{possoft}}\\), \\(r_{\\text{postit}}\\), \\(r_{\\text{heading}}\\))는 RL 기반 내비게이션 플래너가 학습되는 [72]에서 영감을 받아 아래와 같은 형태이다:\n' +
      '\n' +
      '\\[\\begin{split}r_{\\text{track(posoft/postith/heading)]}&=\\frac{1}{1+\\left\\lVert\\frac{\\text{error}{\\sigma}\\right\\rVert^{2}\\cdot\\frac{\\mathds{1}(t>T-T_{r}}{T_{r}},\\end{split}\\tag{10}}}\n' +
      '\n' +
      '여기서 \\(\\sigma\\)은 추적 에러를 정규화하고, \\(T\\)은 에피소드 길이이고, \\(T_{r}\\)은 시간 임계치이다. 이렇게 함으로써, 로봇은 민첩성을 제한할 수 있는 목표 속도와 같은 명시적인 움직임 제약으로부터 벗어나 추적 보상을 최대화하기 위해 \\(T-T_{r}\\) 이전에 목표에 도달하기만 하면 된다. 소프트 위치 추적을 위해 목표까지의 거리를 오차로 하는 \\(\\sigma_{\\text{soft}}=2\\mathrm{m}\\)과 \\(T_{r}=2\\mathrm{s}\\)을 갖는다. 밀착 위치 추적을 위해 우리는 \\(\\sigma_{\\text{tight}}=0.5\\\\mathrm{m}\\)과 \\(T_{r}=1\\\\mathrm{s}\\)을 갖는다. 헤딩 트래킹을 위해 \\(\\sigma_{\\text{heading}}=1\\\\mathrm{rad}\\)과 \\(T_{r}=2\\\\mathrm{s}\\)이 있으며, 오차는 목표 헤딩에 대한 상대 요각이다. 또한 목표까지의 거리가 \\(\\sigma_{\\text{soft}}\\)보다 클 때 \\(r_{\\text{heading}}\\)을 비활성화함으로써 충돌 회피에 영향을 미치지 않도록 한다.\n' +
      '\n' +
      '상기 스탠딩 항은,\n' +
      '\n' +
      '\\\\[\\begin{split} r_{\\text{stand}=\\|q-\\bar{q}\\|_{1}\\cdot\\frac{ \\mathds{1}(t>T-T_{r,\\text{stand}}}{T_{r,\\text{stand}}\\cdot\\mathds{1}(d_{\\text{goal}<\\sigma_{\\text{tight}}),\\end{split}\\tag{11}\\text{11}}}\n' +
      '\n' +
      '여기서 \\(\\bar{q}\\)는 서기 위한 공칭 조인트 위치이고, \\(T_{r,\\text{stand}}=1\\\\mathrm{s}\\) 및 \\(d_{text{goal}}\\)은 목표까지의 거리이다.\n' +
      '\n' +
      '민첩한 용어는 민첩한 운동을 장려하는 핵심 용어이다. 로 정의된다.\n' +
      '\n' +
      '\\frac{\\mathrm{v_{x}}{\\mathrm{ReLU}(\\frac{\\mathrm{v_{x}}{\\mathrm{v_{max}}})\\cdot\\mathds{1}(\\text{correct direction}),\\\\\\\\mathds{1}(d_{\\text{goal}<\\sigma_{\\text{tight}})\\big}, \\end{split}\\tag{12}}}\n' +
      '\n' +
      '여기서 \\(v_{x}\\)는 로봇 베이스 프레임에서의 전방 속도이고, \\(v_{\\max}=4.5\\\\mathrm{m}/\\mathrm{s}\\)은 도달하지 못하는 \\(v_{x}\\의 상한이며, "올바른 방향"은 로봇 헤딩과 로봇-목표 라인 사이의 각도가 105\\({}^{\\circ}\\)보다 작음을 의미한다. 이 용어를 최대화하려면 로봇이 빠르게 달리거나 목표에 머물러야 합니다.\n' +
      '\n' +
      '(d_{\\text{goal}>\\sigma_{\\text{soft}\\) 로봇이 "올바른 방향"에 있지 않을 때 로봇이 정적 상태를 유지한다면, 정지항(r_{\\text{stall}}\\)은 \\(1\\)이다. 이 용어는 시간 낭비에 대해 로봇에 불이익을 준다.\n' +
      '\n' +
      '###### Iii-B3 정규화 보상\n' +
      '\n' +
      '상기 정규화 보상은:\n' +
      '\n' +
      'ReLU}\\left(|\\tau_{i}|-0.95 \\cdot\\tau_{i,\\lim}\\right)\\cdot\\sum\\nolimits_{i=1}^{12}\\mathrm{ReLU}\\left(|\\tau_{i}|-0.95 \\cdot\\tau_{i,\\lim}\\right)\\cdot\\sum\\nolimits_{i=1}^{12}\\mathrm{ReLU}\\left(|\\tau_{i}|-0.95 \\cdot\\tau_{i,\\lim}\\right)\\cdot\\molimits_{i=1}^{2}-0.0005\\cdot\\molimits_{i=1}^{2}\\tot\\molimits_{i=1}^{2}-0.0005\\cdot\\molimits_{i=1}^{2}\\tot\\molimits_{i=1}^{12}\n' +
      '\n' +
      '여기서 \\(\\tau\\)은 관절 토크, \\(\\tau_{\\lim}\\)은 하드웨어 토크 한계, \\(\\dot{q}_{\\lim}\\)은 하드웨어 관절 속도 한계, \\(q_{\\lim}\\)은 하드웨어 관절 위치 한계, "플라이"는 로봇이 지면과 접촉하지 않은 경우를 나타낸다. 우리는 로봇 기지를 통제할 수 없게 하여 시스템의 안전을 위협하는 "비행" 사례를 처벌한다.\n' +
      '\n' +
      'Simulation_### _ Training in Simulation_\n' +
      '\n' +
      '#### Iii-C1 Simulator\n' +
      '\n' +
      'GPU 기반의 Isaac Gym 시뮬레이터[44]를 사용하여 PPO 알고리즘[52]과 병행하여 1280개의 환경을 훈련할 수 있도록 지원한다.\n' +
      '\n' +
      '#### Iii-C2 Terrains\n' +
      '\n' +
      '우리는 학습을 용이하게 하기 위해 커리큘럼에 따라 무작위 지형으로 민첩한 정책을 훈련한다. 시뮬레이션 역학을 과도하게 이용하는 불안정한 걸음새를 방지하기 위해 그림 3과 같이 지형이 평평하거나 거칠거나 낮은 비틀림 블록으로 무작위로 샘플링된다. 난이도가 0에서 9로 올라갈수록 거친 지형과 비틀림 블록은 0\\(\\mathrm{cm}\\)에서 7\\(\\mathrm{cm}\\)으로 높이 차이가 더 크다.\n' +
      '\n' +
      '#### Iii-C3 Obstacles\n' +
      '\n' +
      '우리는 반지름 40\\(\\mathrm{cm}\\)의 실린더로 정책을 훈련한다. 각 에피소드마다 원점과 목표를 포함하는 11\\(\\mathrm{m}\\times\\)5\\(\\mathrm{m}\\) 직사각형에 0\\(\\sim\\)8개의 장애물이 무작위로 분포되어 있다. 학습을 용이하게 하기 위해 난이도가 높을수록 장애 요인이 많은 교육과정도 적용한다.\n' +
      '\n' +
      '###### Iii-C4 도메인 랜덤화\n' +
      '\n' +
      '우리는 sim-to-real 전송을 위해 도메인 랜덤화[60]를 수행한다. 무작위 설정은 표 II에 나열되어 있다. 이 몇 가지 용어 중 두 가지는 중요한 것으로, 착시와 ERFI-50이다. 착시는 벽과 같은 보이지 않는 기하학적 구조에 대해 정책을 더 강력하게 만든다. 착시는 관찰된 광선 거리를 \\(d_{\\text{goal}}+0.3,\\text{ray distance})\\(d_{\\text{goal}}+0.3\\)보다 크면 \\(\\mathcal{U}(d_text{goal}}+0.3,\\text{ray distance})의 임의의 값에 의해 덮어씌운다. Campanaro et al. [8]이 제안한 ERFI-50은 무작위 토크 섭동과 함께 모터 심-대-실제 갭을 암묵적으로 모델링하며, 학습의 초기 단계를 방해하지 않기 위해 작업에 커리큘럼을 추가한다. 또한 모터 인코더의 오프셋 오차를 모델링하기 위해 조인트 위치를 무작위로 편향시킨다.\n' +
      '\n' +
      '도. 3: 예제 훈련 환경. 마젠타 포인트는 목표를 나타내고 청록색 선은 외수용성 광선 관찰을 나타낸다. 좌에서 우로 이어지는 지형: 평지, 낮은 걸림돌, 거칠다.\n' +
      '\n' +
      '#### Iv-B5 Curriculum\n' +
      '\n' +
      '위에서 언급한 바와 같이 난이도 조절이 지형, 장애물 분포, 도메인 랜덤화 등을 변화시킬 수 있는 교육과정을 적용한다. 난이도 배정을 위해 Zhang et al. [71]의 설계를 따르며, 에피소드가 종료되면 로봇은 \\(d_{\\text{goal}}<\\sigma_{\\text{tight}\\)일 경우 더 높은 레벨로 승격되고, \\(d_{\\text{goal}>\\sigma_{\\text{soft}\\)일 경우 더 낮은 레벨로 강등된다. 로봇이 최고 레벨에서 승격되면 [51]에 이어 랜덤 레벨로 이동한다.\n' +
      '\n' +
      '## V 학습 및 도달 회피 값 사용\n' +
      '\n' +
      '애자일 정책은 해당 보상을 통해 특정 충돌 회피 행동을 학습하지만 안전을 보장하지는 않는다. 로봇을 보호하기 위해 RA 값을 사용하여 오류를 예측하고, RA 값을 기반으로 로봇을 복구하는 정책을 제안한다.\n' +
      '\n' +
      'Hsu et al. [29]에 의해 영감을 받아, 우리는 모델 기반 도달 가능성 분석의 전형적인 접근법과 대조되는 모델 없는 방식으로 RA 값을 학습한다[2]. 이것은 모델이 없는 RL 기반 정책에 더 잘 맞습니다. 또한 [29]와 달리, 우리는 글로벌 RA 값을 학습하지 않고 섹션 III-B4에서 언급한 바와 같이 정책 조건으로 만든다. 학습된 RA 값 함수는 관찰에 기초하여 민첩한 정책의 실패만을 예측할 것이다.\n' +
      '\n' +
      '###_Learning RAValues_\n' +
      '\n' +
      '고차원에서 과적합을 피하고 RA 값을 일반화하기 위해 RA 값 함수의 입력으로 감소된 관찰 세트를 사용한다:\n' +
      '\n' +
      '\\[o^{\\text{RA}}=\\left[\\left[v;\\omega\\right];G^{c}_{x,y};R\\right], \\tag{14}\\]\n' +
      '\n' +
      '즉, 베이스 트위스트, 로봇 프레임 내의 목표 \\((x,y)\\) 위치 및 엑테로셉션. 이러한 구성 요소는 안전과 목표 도달에 상당한 영향을 미치는 중심 관찰이다. 반면에, 우리는 높은 차원과 목표 도달에 덜 적절하기 때문에 여기서 (\\(q\\) 및 \\(\\dot{q}\\)과 같은 공동 수준 관측치를 사용하지 않는다. RA 값을 근사화하기 위해 RA 값 네트워크 \\(\\hat{V}\\)를 훈련한다:\n' +
      '\n' +
      '\\[V^{\\pi^{\\text{Agile}}}_{\\text{RA}}(s)\\approx\\hat{V}(o^{\\text{RA}}). \\tag{15}\\]\n' +
      '\n' +
      '식 (5)에 기초하여, 기울기 하강을 갖는 에피소드마다 다음의 손실을 최소화한다:\n' +
      '\n' +
      '[L=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\hat{V}(o^{\\text{RA}}_{t})-\\hat{V}^{\\text{target}}\\right}^{2}, \\tag{16}\\left)\n' +
      '\n' +
      'where\n' +
      '\n' +
      '{split}\\hat{V}^{\\text{target}=&\\gamma_{\\text{RA}}\\max\\left\\zeta(s_{t}),\\min\\left\\{l(s_{t}),\\hat{V}^{\\text{old}(o^{\\text{RA}}_{t+1})\\right\\\\right\\\\\\\\&+(1-\\gamma_{\\text{RA}})\\max\\left\\{l(s_{t}),\\zeta(s_{t}}\\end{split}\\tag{17}\\\\t})\n' +
      '\n' +
      '그리고 할인계수 \\(\\gamma_{\\text{RA}}=0.99999\\)를 가장 근사한 \\(\\mathcal{RA}}(\\theta;\\mathcal{F})\\)으로 설정하였으며, \\(V^{\\pi}_{\\text{RA}}(s)\\)이 \\(V^{\\pi}_{\\text{RA}}(s)\\)으로 수렴하므로 \\(\\hat{V}^{\\text{RA}}=0.99999\\)을 이전 반복에서 \\(\\hat{V}}(o^{\\text{RA}}_{T+1})=+\\infty\\)로 설정하였다.\n' +
      '\n' +
      '[29]와 달리, 우리의 접근법은 \\(\\mathcal{A}\\)에 대한 또 다른 값 최소화 문제를 포함하는 전체 시스템 다이내믹스의 정책-진단 글로벌 도달-회피 값을 해결하는 대신 정책-조건 도달-회피 값을 학습한다. 우리의 방법은 몇 가지 이점을 제공한다: 1) 단순성: 식 (5)에서 강조된 바와 같이 , 이러한 단순성은 전체 동작 공간에 걸쳐 다음 상태의 가장 낮은 값에 대한 해결의 필요성을 회피하는 것으로부터 발생한다. 2) 2단계 오프라인 학습: 우리의 접근법은 2단계 오프라인 방식으로 학습될 수 있다. 여기에는 먼저 정책 궤적을 수집한 다음 정책 조건 도달 회피 값을 훈련하는 것이 포함된다. 이 2단계 과정은 [29]에서 제시한 온라인 교육 방법에 비해 안정성을 높인다.\n' +
      '\n' +
      '### _Implementation_\n' +
      '\n' +
      '[29]에 따르면 이론적인 보장을 위해 \\(l(s)\\)과 \\(\\제타(s)\\)는 립시츠 연속이어야 한다. 구현시 \\(l(s)\\)을 다음과 같이 정의한다.\n' +
      '\n' +
      '\\[l(s)=\\tanh\\log\\frac{d_{\\text{goal}}}{\\sigma_{\\text{tight}}}, \\tag{18}\\]\n' +
      '\n' +
      '이를 Lipschitz-continuous로 만들고, \\((-1,1)\\으로 경계짓고, \\(d_text{goal}\\leq\\sigma_{\\text{tight}\\)을 "reach"로 설정한다.\n' +
      '\n' +
      '실패에 대해, 우리는 당연히\n' +
      '\n' +
      '\\[\\zeta(s)=2*\\mathds{1}(\\text{undesired collision})-1. \\tag{19}\\\n' +
      '\n' +
      '그러나, 이 정의는 립시츠 연속성을 침해한다. 따라서, 의도하지 않은 충돌이 발생할 때, 마지막 10개의 타임스텝에 대한 \\(\\zeta\\) 값은 \\(-0.8, -0.6,\\dots,0.8, 1.0\\)으로 다시 표시된다.\n' +
      '\n' +
      'RA 데이터 세트 샘플링을 위해, 우리는 민첩한 정책을 훈련하는 동안 가장 높은 난이도와 같이 장애물을 분산시킨다. 우리는 200k 에피소드에 대해 훈련된 민첩한 정책을 펼치고 RA 학습을 위해 이러한 궤적을 수집한다.\n' +
      '\n' +
      '그림 4는 특정 장애물 세트에 대한 학습된 RA 값을 시각화한다. 로봇의 속도가 변함에 따라 RA 값의 경관이 그에 따라 변한다. RA 값의 부호는 민첩한 정책에 대한 안전성을 합리적으로 나타낸다.\n' +
      '\n' +
      '### _ Recovery_를 위한 RA 값을 사용하는 것\n' +
      '\n' +
      'RA 값은 애자일 정책을 기반으로 한 장애 예측을 제공하며, RA 값을 사용하여 복구 정책을 안내할 것을 제안한다. 구체적으로, 로봇은 RA 값 함수를 사용하여 충돌을 피하기 위해 최적의 트위스트를 결정하고, 이러한 트위스트 명령을 추적하기 위해 복구 정책을 사용한다. 복구 정책은 \\(\\hat{V}(o^{\\text{RA}})\\geq V_{\\text{threshold}}의 경우에만 백업 차폐 정책으로 트리거된다. 과보존적 차폐를 유발하지 않으면서 학습 오류를 보완하기 위해 \\(V_{text{threshold}}=-0.05\\)을 설정하였다.\n' +
      '\n' +
      '복구 중에는 복구 정책이 잘 훈련되어 로봇 꼬임이 명령에 가깝다고 가정합니다.\n' +
      '\n' +
      '\\[tw^{c}=[v_{x}^{c},v_{y}^{c},0,0,0,\\omega_{z}^{c}], \\tag{20}\\]\n' +
      '\n' +
      '그리고 로봇은 골과 외향성을 고려할 때 비틀림이 안전하다면 목표에 더 가까이 가도록 노력해야 한다. 따라서, 트위스트 커맨드는 최적화로부터 얻어진다:\n' +
      '\n' +
      '\\[tw^{c}=\\arg\\min d_{\\text{goal}}^{\\text{future}}\\text{ s.t.}\\hat{V}([tw^{c};G_{x,y}^{c};R])<V_{\\text{ threshold}, \\tag{21}\\hat{V}([tw^{c};G_{x,y}^{c};R])\n' +
      '\n' +
      '그리고 \\(d_{\\text{goal}}^{\\text{future}}\\)는 적은 시간 동안 트위스트 명령을 추적한 후 목표까지의 대략적인 거리를 의미한다. 이는 베이스 프레임에서의 로봇 변위의 선형화된 적분에 기초하여 계산된다:\n' +
      '\n' +
      '{split}\\delta x&=v_{x}^{c}\\delta t-0.5v_{y}\\omega_{z}^{c}\\delta t^{2},\\\\delta y&=v_{y}\\delta t+0.5v_{x}^{c}\\omega_{z}\\delta t^{2}.\\end{split}\\tag{22}\\delta y&=v_{y}\\delta t+0.5v_{x}^{c}\\delta t^{2}.\n' +
      '\n' +
      '우리의 실무에서 제약 조건 상의 라그랑지안 승수를 갖는 경사 하강은 전류 트위스트로 초기화될 때 식 (21)을 5단계 내에서 해결할 수 있어 실시간 배포가 가능하다. 트위스트 최적화 프로세스의 시각화는 탐색된 트위스트가 안전 제약 조건(즉, \\(\\hat{V}<V_{\\text{threshold}}\\))을 일관되게 만족시키는 그림 8에 주어진다.\n' +
      '\n' +
      'VI 학습 회복 정책\n' +
      '\n' +
      '복구 정책은 V절에서 언급한 바와 같이 백업 차폐 정책으로서 기능할 수 있도록 로봇이 주어진 트위스트 명령을 가능한 한 빨리 추적하도록 하기 위한 것이다.\n' +
      '\n' +
      '###_관찰공간 및 동작공간_###\n' +
      '\n' +
      '복구 정책의 관찰 공간은 트위스트 명령을 추적하고 엑테로셉션이 필요하지 않다는 점에서 민첩한 정책과 다르다. 회복정책의 관측치\\(o^{\\text{Rec}\\)는 발 접촉\\(c_{f}\\), 베이스 각속도\\(\\omega\\), 베이스 프레임에 투영된 중력\\(g\\), 트위스트 명령\\(tw^{c}\\) (0이 아닌 변수만), 관절 위치\\(q\\), 관절 속도\\(\\dot{q}\\), 이전 프레임의 동작\\(a\\)으로 구성된다.\n' +
      '\n' +
      '회복 정책의 행동 공간은 민첩성 정책의 행동 공간인 12-d 공동 목표와 정확히 동일하다. 또한 정책 네트워크로 MLP를 사용합니다.\n' +
      '\n' +
      '### _Rewards_\n' +
      '\n' +
      '애자일 정책과 유사하게 회복 정책에 대한 보상 기능 역시 벌칙 보상, 과제 보상, 정례화 보상의 세 부분으로 구성되어 있다. 정규화 보상과 페널티 보상은 최대 감속을 위해 지면과의 무릎 접촉을 허용하는 것을 제외하고는 동일하게 유지된다(예를 들어, 도 1의 (a)).\n' +
      '\n' +
      '상기 태스크 보상은 트위스트 추적을 위한 것인,\n' +
      '\n' +
      '\\[r_{\\text{task}}=10\\cdot r_{\\text{invel}}-0.5\\cdot r_{\\text{angvel}}+5\\cdot r_{\\text{alive}}-0.1\\cdot r_{\\text{posture}, \\tag{23}\\}\n' +
      '\n' +
      '즉, \\(v_{x}^{c}\\) 및 \\(v_{y}^{c}\\)을 추적하기 위한 용어, \\(\\omega_{z}^{c}\\)을 추적하기 위한 용어, 살아있는 상태를 유지하기 위한 용어, 그리고 자세를 유지하기 위한 용어는 민첩한 정책으로 원활하게 다시 전환한다.\n' +
      '\n' +
      '구체적으로 말씀드리자면\n' +
      '\n' +
      '\\[r_{text{invel}}=\\exp\\left[-\\frac{(v_{x}-v_{x}^{c})^{2}+(v_{y}-v_{y}^{c})^{2}}{\\sigma_{\\text{invel}}^{2}}\\right], \\tag{24}\\c}}\n' +
      '\n' +
      '여기서 \\(\\sigma_{\\text{invel}}=0.5\\\\mathrm{m/s})로 설정한다. 각속도에 대해,\n' +
      '\n' +
      '\\[r_{\\text{angvel}}=\\|\\omega_{z}-\\omega_{z}^{c}\\|_{2}^{2}, \\tag{25}\\]\n' +
      '\n' +
      '이것은 \\(r_{\\text{invel}}\\)보다 명령 근처에 더 부드러운 풍경을 제공한다. 살아있는 용어는 간단히\n' +
      '\n' +
      '\\[r_{\\text{alive}}=1\\cdot\\mathds{1}(\\text{alive}). \\tag{26}\\]\n' +
      '\n' +
      '상기 자세 용어는,\n' +
      '\n' +
      '\\[r_{\\text{posture}}=\\|q-\\bar{q}_{\\text{rec}}\\|_{1}, \\tag{27}\\]\n' +
      '\n' +
      '여기서 \\(\\bar{q}_{\\text{rec}\\)는 로봇이 민첩한 정책으로 원활하게 다시 전환할 수 있도록 낮은 높이를 가진 명목상의 서 있는 자세이다.\n' +
      '\n' +
      'Simulation_### _ Training in Simulation_\n' +
      '\n' +
      '복구 정책을 훈련하기 위한 시뮬레이션 설정은 민첩한 정책에 대한 시뮬레이션 설정과 유사합니다. 차이점은 다음과 같다.\n' +
      '\n' +
      '###### Vi-C1 도메인 랜덤화\n' +
      '\n' +
      '관측 잡음과 동적 랜덤화는 변하지 않는다. 에피소드 길이는 2초로 변경되었으며, \\(\\mathcal{U}(-\\pi/6,\\pi/6)\\) rad를 받는 무작위 초기 롤 및 피치 각도가 있다. 랜덤화 범위는 초기 \\(v_{x}\\sim\\mathcal{U}(-0.5,5.5)\\) m/s와 초기 \\(\\omega\\sim\\mathcal{U}(-1.0,1.0)\\) rad/s에 대해서도 변경되었다. 이러한 변경사항은 애자일 실행 중에 복구 정책을 트리거할 수 있는 상태를 더 잘 수용합니다. 샘플링 명령의 범위는 \\(v_{x}^{c}\\sim\\mathcal{U}(-1.5,1.5)\\) m/s, \\(v_{y}^{c}\\sim\\mathcal{U}(-0.3,0.3)\\) m/s, \\(\\omega_{z}^{c}\\sim\\mathcal{U}(-3.0,3.0)\\) rad/s이다.\n' +
      '\n' +
      '#### Vi-C2 Curriculum\n' +
      '\n' +
      '지형과 도메인 랜덤화에 대한 커리큘럼은 여전히 존재합니다. 그러나, 속도 추적 오차가 \\(0.7\\sigma_{\\text{invel}}\\)보다 작으면 로봇이 승격되고, 넘어지면 강등되는 과제가 변경된다.\n' +
      '\n' +
      '도. 4: \\(3\\) 고정 장애물에 대한 선형 속도와 2D 위치가 다른 \\(\\hat{V}\\)의 시각화. 각속도는 0으로 설정하고 상대 목표 명령은 로봇 앞에 \\(5\\)m로 설정한다. 회색 원은 장애물을 나타내고 색상은 해당 2D 위치에서 \\(\\hat{V}\\)의 값을 나타낸다.\n' +
      '\n' +
      '## VII Perception\n' +
      '\n' +
      '섹션 IV 및 섹션 V에서 언급한 바와 같이 민첩성 정책과 RA 가치 네트워크는 모두 훈련 중 그라운드 트루스 값에 대한 접근과 함께 관찰의 일부로 엑테로셉티브 11d 광선 거리를 사용한다. 이 광선들은 로봇 베이스로부터 수평으로 주조되며, 방향은 \\([-\\frac{\\pi}{4},\\frac{\\pi}{4}]\\)으로 균일하게 이격된다.\n' +
      '\n' +
      '그러나 이러한 광선 거리는 배치 중에 직접 사용할 수 없으며 섹션 III-C에서 언급한 바와 같이 깊이 이미지에서 이를 예측하기 위해 광선 예측 네트워크를 훈련해야 한다. 이러한 설계는 다음과 같은 이점을 가져온다:\n' +
      '\n' +
      '1. 데이터 증강에 의한 고차원 영상잡음을 처리하기 위해 광선 예측 네트워크만 조정하면 된다.\n' +
      '2. 표상이 매우 해석가능하여 인간이 감독할 수 있다.\n' +
      '3. 민첩한 정책과 RA 가치 네트워크는 저차원 입력으로 훈련하기가 더 쉽다.\n' +
      '4. 시뮬레이션에서 값비싼 이미지 렌더링에 비해 광선 거리는 계산이 쉽고 훈련 시간을 절약할 수 있다.\n' +
      '\n' +
      '또한, 레이 거리는 희소 LiDAR 판독값과 유사하지만, 경량 저가의 카메라는 고속 충돌 회피에 중요한 높은 FPS에 쉽게 도달할 수 있기 때문에 LiDAR 대신 카메라를 사용한다.\n' +
      '\n' +
      '이 섹션에서는 광선 예측 네트워크를 훈련하기 위한 세부 사항을 제시한다.\n' +
      '\n' +
      '### _Data Collection_\n' +
      '\n' +
      '광선 예측 네트워크를 훈련하기 위해 시뮬레이션에서 애자일 정책을 실행하여 깊이 이미지와 광선 거리 쌍(그림 2(a)와 같이)의 데이터 세트를 수집한다. 그 후, 광선-예측 네트워크는 감독 방식으로 훈련될 수 있다. 일반화를 촉진하기 위해 그림 5와 같이 데이터 수집 중에 실린더를 다른 모양의 객체로 교체한다.\n' +
      '\n' +
      'Simto-Real Transfer__를 위한### _Data Augmentation\n' +
      '\n' +
      '카메라로부터 수집된 실제 깊이 이미지는 시뮬레이션에서 렌더링된 깊이 이미지보다 훨씬 더 시끄럽다[27]. 광선 예측 네트워크가 실제 깊이 영상에 더 잘 적응하도록 하기 위해, 그림 6과 같이 훈련 동안 4가지 데이터 증강 기술을 적용한다: 1) 수평 플립; 2) 랜덤 소거; 3) 가우시안 블러; 4) 가우시안 노이즈. 전개를 위해 시뮬레이션과 실제 세계 사이의 깊이 이미지 간격을 더 줄이기 위해 홀 채움[56]을 적용한다.\n' +
      '\n' +
      '###_기타 구현 상세__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n' +
      '\n' +
      '네트워크가 가까운 장애물에 더 집중하도록 하기 위해 깊이 값의 로그를 NN 입력으로, 광선 거리의 로그를 출력으로 하고 평균 제곱 오차를 손실 함수로 한다.\n' +
      '\n' +
      '우리는 모델을 훈련시키기 위해 사전 훈련된 가중치로 ResNet-18[26]을 세밀하게 조정한다. 이미지는 시뮬레이션과 배포 중에 모두 \\([160,90]\\) 해상도로 다운샘플링된다.\n' +
      '\n' +
      '## VIII Experiments\n' +
      '\n' +
      '### _Baselines_\n' +
      '\n' +
      '실험 결과를 위해 세 가지 설정을 고려한다.\n' +
      '\n' +
      '1. 애자일 정책과 복구 정책을 모두 갖춘 우리의 ABS 시스템;\n' +
      '2. 우리의 애자일 정책\\(\\pi^{\\text{Agile}}\\);\n' +
      '3. "LAG": PPO-Lagrangian[49]를 사용하여 민첩한 정책의 공식화로 종단간 안전한 RL 정책을 훈련한다.\n' +
      '\n' +
      '(2)와 (3)을 비교하여 외부 모듈 없이 민첩성과 안전이 어떻게 상충되어 민첩성과 안전의 경계를 형성하는지 알 수 있다. RA 값과 복구 정책의 도움으로 우리는 (1) 안전한 경우 (2)만큼 민첩해야 하고 위험한 경우 로봇을 보호해야 하는 높은 안전 이득으로 이 경계를 깰 것으로 기대한다.\n' +
      '\n' +
      '_여기서 세 가지 설정은 모두 우리가 제안하는 것에 기초한다는 점에 유의한다. 우리의 애자일 정책과 이전의 최신(SOTA) 애자일 실행 정책 [45]의 상세한 비교는 섹션 IX-A1._\n' +
      '\n' +
      '### _Simulation Experiments_\n' +
      '\n' +
      '###### Vi-B1 정량적 결과\n' +
      '\n' +
      '시뮬레이션에서 다양한 설정으로 훈련된 정책을 테스트합니다. 민첩성과 안전성의 경계를 더 잘 보여주기 위해, 각 설정에 대해 공격적인 것("-a")과 애자일 보상 항을 두 배로 늘리는 것("r_{text{agile}}\\), 명목적인 것("-n") 및 보수적인 것("-c")의 세 가지 변형을 도입한다. 장애물에 대해서는 5.5 m\\(\\times\\) 4 m 직사각형(훈련 중 11 m\\(\\times\\) 5 m) 내에 8개의 장애물을 분포시켰기 때문에 테스트 사례는 분포되지만 훈련 중 대부분의 사례보다 훨씬 어렵다.\n' +
      '\n' +
      '결과는 표 III 및 그림 7에 보고되어 있다. 에피소드에 대해 성공, 충돌 또는 타임아웃의 세 가지 가능한 결과가 있다. 에피소드 길이 내에서 성공 또는 충돌 기준을 트리거하지 않는 궤적은 "타임아웃"으로 레이블이 지정됩니다. 우리는 성공률, 충돌률, 타임아웃률, 성공 사례의 평균 피크 속도, 성공 사례의 평균 속도를 메트릭으로 보고한다. 각 설정에 대해,\n' +
      '\n' +
      '도. 5: 광선 예측 데이터 수집에 사용되는 다양한 장애물.\n' +
      '\n' +
      '도. 6: 깊이 기반 광선 예측 훈련에 사용되는 4가지 종류의 영상 증강의 일러스트레이션.\n' +
      '\n' +
      '평균 및 std 값은 서로 다른 종자로 훈련된 3개의 정책에 대해 계산되며 메트릭은 10k 무작위 에피소드에 대한 테스트를 통해 획득된다.\n' +
      '\n' +
      '결과는 보상 가중치가 어떻게 조정되는지 또는 RL 알고리즘이 안전한 탐사에 의해 제약되는지 여부에 관계없이 경계 내에서 민첩성과 안전 트레이드 오프임을 나타낸다. 그러나 우리의 RA 가치와 복구 정책이 안전장치로서, 우리는 이 경계를 깨고 민첩성의 약간의 감소만을 대가로 안전의 실질적인 개선을 얻을 수 있다.\n' +
      '\n' +
      '_주, 변형들은 단지 여기서 경계를 보여주기 위해 도입된다는 것에 유의한다. 다음 부분에서는 명목상의 것들만 사용할 것이다._\n' +
      '\n' +
      '####V-B2 실시예 사례\n' +
      '\n' +
      '본 논문에서는 시뮬레이션에서 ABS와 다른 기준선의 예를 제시하는데, 그림 8과 같이 \\((0,0)\\)에서 시작하는 로봇이 목표 \\((7,0)\\)에 도달하기 위해 \\(8\\)의 장애물을 통과해야 한다. 로봇은 먼저 열린 공간을 통과해야 하고, 그 다음 두 개의 좁은 공간을 통과해야 하며, 그 다음 또 다른 열린 공간을 통과해야 한다. 이 경우, \\(\\pi^{text{Agile}}\\) 기준선은 빠르게 진행되지만 두 번째 좁은 공간 근처에서 죽는다. LAG 기준선은 ABS보다 훨씬 느리게 작동합니다. 대조적으로, 제안된 ABS는 개방 공간에서 빠르게 실행되고 RA 값의 차폐 및 복구 정책 덕분에 안전을 위해 좁은 공간에서 느려진다. 도 8의 (c)는 복구 정책이 활성화될 때 트위스트 명령들에 대한 RA 값 랜드스케이프를 보여주는데, 여기서 탐색된 트위스트는 안전 제약 조건(즉, \\(\\tilde{V}<V_{\\text{threshold}}\\))을 일관되게 만족시킨다.\n' +
      '\n' +
      '### _Real-World Experiments_\n' +
      '\n' +
      '###### V-C1 하드웨어 설정\n' +
      '\n' +
      '우리는 실험을 위해 유니트리 고1을 사용한다. 로봇에는 온보드 계산을 위한 제트슨 오린 NX와 깊이 및 오도메트리 감지를 위한 ZED 미니 스테레오 카메라가 장착되어 있습니다. 우리는 ZED 오도메트리 모듈을 사용하여\n' +
      '\n' +
      '도. 8: \\(\\pi^{\\text{Agile}}\\)이 목표에 도달하지 못하는 시뮬레이션의 예. a) ABS 및 기타 기준선의 궤적, ABS에 대해 RA 값이 시각화되었다. b) ABS가 LAG 베이스라인보다 훨씬 빠르다는 것을 보여주는 속도-시간 곡선. c) (I) 및 (II)에서 복구 정책이 트리거될 때 RA 값 경관의 삽화는 \\(v_{x}-\\omega_{z}\\) 평면 및 \\(v_{x}-v_{y}\\) 평면에 투영된다. 우리는 탐색 전의 초기 비틀림(_i.e._, 로봇 베이스의 현재 비틀림)과 탐색된 명령을 식 (21)을 기반으로 보여준다.\n' +
      '\n' +
      '도. 7: 벤치마킹된 비교에서 민첩성-안전 트레이드오프의 도면. 민첩성은 성공 사례에서 달성된 평균 속도로 정량화되고 안전성은 충돌하지 않는 비율로 표시된다. 점은 평균값을 나타내고 오차 막대는 std 값을 나타낸다.\n' +
      '\n' +
      '온라인으로 민첩한 정책에 대한 상대 목표 명령을 업데이트하고 그 차이를 속도 추정으로 설정합니다. 우리는 Unitree의 내장형 PD 제어기를 사용하였으며, \\(K_{p}=30\\)와 \\(K_{d}=0.65\\)을 사용하였다.\n' +
      '\n' +
      '#### V-A2 Results\n' +
      '\n' +
      '2개의 실내 및 1개의 실외 테스트베드에서 ABS는 그림 9와 같이 우수한 전체 성능을 보여 가장 높은 성공률과 가장 낮은 충돌률을 달성한다. 구체적으로 ABS는 모든 환경에 걸쳐 성공률에서 10점 만점에 9점 또는 10점을 일관되게 기록하며, 충돌은 최소화되어 실제 세계에서 견고성과 신뢰성을 나타낸다.\n' +
      '\n' +
      '안전보호막이 없는 애자일 정책\\(\\pi_{\\text{Agile}}\\)은 더 많은 충돌의 비용으로 가장 빠른 주행 속도를 달성한다. LAG는 안전성은 \\(\\pi_{\\text{Agile}}\\)보다 뛰어나지만 속도는 느리고 ABS에 비해 안전성과 민첩성 모두에서 떨어진다. ABS는 높은 안전성으로 빠른 속도를 달성하고 그림 1과 같이 동적 장애물로 일반화한다.\n' +
      '\n' +
      '#### V-A3 Robustness\n' +
      '\n' +
      'ABS 시스템은 미끄러운 눈 위에서 작동할 수 있고, (자체 중량과 동일한) \\(12\\)-kg 페이로드를 견딜 수 있으며, 그림 10과 같이 섭동을 견딜 수 있다. 이러한 테스트는 시스템의 견고성을 보여준다.\n' +
      '\n' +
      '## IX 광범위 연구 및 분석\n' +
      '\n' +
      '### _Maximizing Agility_\n' +
      '\n' +
      '목표 도달 v.s. 속도 추적\n' +
      '\n' +
      '속도 추적은 운동 제어기[51, 45, 47, 38]의 가장 일반적으로 사용되는 공식이며 복구 정책에도 채택된다. 그러나 민첩성 정책은 충돌 회피를 위한 이동과 항법을 분리하지 않고 학습된 민첩성을 충분히 발휘할 수 있기 때문에 목표 도달이 더 나은 선택이라고 주장한다. 또한, 목표 도달 공식이 고속 달리기에 더 나은 보행 패턴을 찾기 때문에 실제 전달에 도움이 된다는 것을 경험적으로 발견했다.\n' +
      '\n' +
      '[45]의 SOTA 민첩한 속도 추적 정책을 기준선("빠른"이라고 함)으로 사용하여 표 IV에서 자세한 비교를 한다. 공정한 비교를 위해 [45]에서 정규화 보상과 과제 보상의 조합으로 "빠르게" 훈련하고, 두 정책에 대해 동일한 액션 공간을 사용하고 [45]에서 시간 정보와 시스템 식별을 제거한다.\n' +
      '\n' +
      '환상과 ERFI-50 무작위화의 효과\n' +
      '\n' +
      '심-투-리얼 전송을 돕기 위해 도메인 랜덤화에 추가한 두 가지 주요 구성 요소는 착시와 ERFI-50이다. 그림 11과 같이 착시 없이 로봇은 간혹 시뮬레이션에서 보지 못한 벽 근처에서 떨릴 것이다. EMFI-50이 없으면, 로봇은 모터 다이내믹스의 심-대-리얼 갭으로 인해 주행 중에 머리로 지면을 타격할 것이다.\n' +
      '\n' +
      '### _Enhanced Perception Training_\n' +
      '\n' +
      '광선 예측 네트워크 교육을 개선함에 있어서, 우리는 1) 네트워크 아키텍처, 2) 몇 가지 요소를 체계적으로 검토한다.\n' +
      '\n' +
      '도. 11: 착시의 효과 및 ERFI-50 랜덤화. 로봇은 착시 무작위화 없이 벽 근처에서 떨고 ERFI-50 무작위화 없이 달리는 동안 지면에 부딪힐 것이다.\n' +
      '\n' +
      '도. 10: ABS 시스템의 강인성 테스트, a) 눈 덮인 지형에서, sb) a\\(12\\)-kg 페이로드를 베어링하고, c) 달리기 시 공에 대항하고, d) 골대에 서 있을 때 킥을 견딥니다.\n' +
      '\n' +
      '도. 9: 실내 테스트베드 2개, 실외 테스트베드 1개, 반복 속도 테스트로 현실 세계의 ABS 및 기타 기준선을 평가한다. 실내(a)는 어둡고 좁은 복도, 실내(b)는 가구가 있는 홀, 실외는 장애물이 거의 없는 운동장의 열린 공간이다. ABS는 LAG 기준선에 비해 빠른 속도로 3개의 테스트베드에서 최고의 안전성을 달성합니다.\n' +
      '\n' +
      '사전 훈련된 가중치, 및 3) 데이터 증강. 표 V에 자세히 설명된 비교 결과는 네트워크의 정확도를 향상시키는 데 있어 사전 훈련된 가중치와 데이터 증강의 중요성을 강조한다.\n' +
      '\n' +
      '네트워크 크기와 관련하여, 더 큰 네트워크는 추론 시간에 대한 비용으로 예측 정확도를 향상시킨다. 표 V의 보고된 추론 시간은 지각 추론만을 위해 제트슨 오린 NX에서 측정되었다는 점에 유의한다. 다양한 작업들 사이에서 계산 자원들이 공유되는 실제 배치에서, 실제 업데이트 빈도는 상당히 낮을 수 있다. 실시간 고속 이동을 위해 동적 환경에서 정확도와 응답성의 균형을 맞추는 ResNet-18을 선택합니다.\n' +
      '\n' +
      '### _Commands_를 통한 인스턴트 스티어링\n' +
      '\n' +
      '섹션 IV에서 언급했듯이 실행 시간에도 목표 명령을 변경할 수 있다. 단일 프레임 관찰 및 무작위화 설정 덕분에 표 VI에 제시된 대로 즉시 민첩한 조향을 달성하기 위해 목표 명령을 쉽게 덮어쓸 수 있다. 이는 속도 추적 공식과 유사한 직접적인 인간 관여를 가능하게 하며, 그림 12는 실제 세계에서 이러한 동작을 보여준다.\n' +
      '\n' +
      '### _ failure case and Limitations_\n' +
      '\n' +
      '첫째, 장애물이 너무 밀집되어 국소 최소값을 형성할 때 우리의 정책은 쉽게 실패할 수 있으며, 이는 결과에 높은 제한 시간 비율에 의해 반영된다. 그러나 이것은 로컬 내비게이션 플래너들[72, 46]에서 흔하며, 잠재적인 해결책은 메모리를 추가하거나 전역 힌트를 도입하는 것일 수 있다[64].\n' +
      '\n' +
      '둘째, 동적 환경에 대한 일반화는 RA 값의 차폐와 복구 정책 때문이다. RA 값은 정적 장애물로 학습되며 준정적 환경으로만 일반화할 수 있다. 동적 객체가 복구 정책의 속도 제한보다 빠르게 이동하는 경우 충돌이 발생할 수 있다. 잠재적인 해결책은 미래에 장애물의 움직임을 예측하는 것이다[37, 54].\n' +
      '\n' +
      '셋째, 로봇 동작을 2차원 이동으로만 제한하고, 비행 위상이 없도록 제한한다. 계단이나 틈과 같은 3차원 지형의 경우, 이동 기술과 충돌 회피가 결합되어 있기 때문에 문제가 훨씬 더 어려울 수 있다.\n' +
      '\n' +
      '넷째, 암시적 시스템 식별 기법[38, 47, 45, 36]은 시간 정보를 활용하여 실제 동역학을 표현하고 심투리얼 전송을 용이하게 할 수 있지만, 이를 시스템에 통합하는 것은 간단하지 않다. 이것은 RA 모듈에 대해 다루기 어려운 시간적 정보의 잠재 임베딩을 필요로 한다. 정책 전환은 또한 정책에 대한 배포를 포함시킬 수 있습니다.\n' +
      '\n' +
      '다섯째, 비전 시스템의 추가 개선이 필요하다. 실내(a) 테스트베드에서 ABS의 유일한 충돌은 복도가 상당히 어둡기 때문에 광선 예측 네트워크에 의해 "검출되지 않은" 물체 때문이다. 네트워크 외에도 본체 주변에 더 많은 카메라를 추가하여 시스템을 완성할 수도 있습니다. 이러한 방식으로, 로봇은 또한 후방 또는 측면에서 오는 장애물을 피할 수 있다. 이벤트 카메라는 또한 매우 역동적인 시나리오들, 예를 들어 고속 공을 피하는 경우[19]에 도움이 될 수 있다.\n' +
      '\n' +
      '## X Conclusion\n' +
      '\n' +
      '본 논문에서는 어수선한 환경에서 안전한 고속 4족 보행을 달성한다. 우리의 프레임워크 ABS는 민첩한 정책을 통해 로봇이 빠르게 실행되고 복구 정책이 로봇을 보호하는 이중 정책 설정을 사용합니다. 학습된 도달 회피 값은 정책 스위치를 제어하고 복구 정책을 안내한다. 레이 예측 네트워크는 애자일 정책과 RA 가치 네트워크에 대한 익스테로셉션 표현을 제공한다. 어떤 주요 테이크아웃은 다음과 같다.\n' +
      '\n' +
      '1. 충돌회피에 있어서 민첩성을 위해서는 이동과 항행을 별도의 하위작업으로 분리하지 않고 통합하는 것이 가장 좋은 관행이다.\n' +
      '2. 저차원 엑테로셉션 표현은 정책 학습 및 일반화를 용이하게 할 수 있다.\n' +
      '3. 동일한 보상 조건 및 훈련 설정으로, 보상 가중치를 조정하거나 RL과 라그랑지안 기반 RL 사이에서 선택하는 것은 민첩성과 안전성을 거래하는 반면, 외부 차폐 모듈은 트레이드 오프 경계를 깨는 데 도움이 될 수 있다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '하드웨어 실험을 지원해준 Wennie Tabib에 감사드리며, 하드웨어 디버깅에 대한 조언을 해준 Yang Yuxiang, Yiyu Chen, Yikai Wang 및 Xialin He에 감사드립니다. 안드레아 바흐시와 지카오 마가 그래픽 디자인에 도움을 준 것에 대해 특별한 감사를 표합니다.\n' +
      '\n' +
      '도. 12: 명령 시퀀스 "Forward"-Rapid Right Turn"-Forward로 로봇을 조향한다. 이 로봇은 앞으로 달릴 때 \\(>3\\)m/s, 빠르게 돌릴 때 \\(>6\\)rad/s에 도달할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Fernando Acero, Kai Yuan, and Zhibin Li. Learning perceptual locomotion on uneven terrains using sparse visual observations. _IEEE Robotics and Automation Letters_, 7(4):8611-8618, 2022.\n' +
      '* [2] Somil Bansal and Claire J Tomlin. Deepreach: A deep learning approach to high-dimensional reachability. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1817-1824. IEEE, 2021.\n' +
      '* [3] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief overview and recent advances. In _2017 IEEE 56th Annual Conference on Decision and Control (CDC)_, pages 2242-2253. IEEE, 2017.\n' +
      '* [4] Shalabh Bhatnagar and K Lakshmanan. An online actor-critic algorithm with function approximation for constrained markov decision processes. _Journal of Optimization Theory and Applications_, 153:688-708, 2012.\n' +
      '* [5] Gerardo Bledt, Matthew J Powell, Benjamin Katz, Jared Di Carlo, Patrick M Wensing, and Sangbae Kim. Mit cheetah 3: Design and control of a robust, dynamic quadruped robot. In _2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2245-2252. IEEE, 2018.\n' +
      '* [6] Michael Bloesch, Christian Gehring, Peter Fankhauser, Marco Hutter, Mark A Hoepflinger, and Roland Siegwart. State estimation for legged robots on unstable and slip-pery terrain. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 6058-6064. IEEE, 2013.\n' +
      '* [7] Russell Buchanan, Lorenz Wellhausen, Marko Bjelonic, Tirthankar Bandyopadhyay, Navinda Kottege, and Marco Hutter. Perceptive whole-body planning for multilegged robots in confined spaces. _Journal of Field Robotics_, 38(1):68-84, 2021.\n' +
      '* [8] Luigi Campanaro, Siddhant Gangapurwala, Wolfgang Merkt, and Ioannis Havoutis. Learning and deploying robust locomotion policies with minimal dynamics randomization. _arXiv preprint arXiv:2209.12878_, 2022.\n' +
      '* [9] Mo Chen, Sylvia Herbert, and Claire J Tomlin. Fast reachable set approximations via state decoupling disturbances. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 191-196. IEEE, 2016.\n' +
      '* [10] Richard Cheng, Gabor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 33, pages 3387-3395, 2019.\n' +
      '* [11] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. _arXiv preprint arXiv:2309.14341_, 2023.\n' +
      '* [12] Jia-Ruei Chiu, Jean-Pierre Sleiman, Mayank Mittal, Farbod Farshidian, and Marco Hutter. A collision-free mpc for whole-body dynamic locomotion and manipulation. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 4686-4693. IEEE, 2022.\n' +
      '* [13] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _arXiv preprint arXiv:1801.08757_, 2018.\n' +
      '* [14] Jared Di Carlo, Patrick M Wensing, Benjamin Katz, Gerardo Bledt, and Sangbae Kim. Dynamic locomotion in the mit cheetah 3 through convex model-predictive control. In _2018 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 1-9. IEEE, 2018.\n' +
      '* [15] Yanran Ding, Abhishek Pandala, and Hae-Won Park. Real-time model predictive control for versatile dynamic motions in quadrupedal robots. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8484-8490. IEEE, 2019.\n' +
      '* [16] Helei Duan, Bikram Pandit, Mohitvishnu S Gadde, Bart Jaap van Marum, Jeremy Dao, Chanho Kim, and Alan Fern. Learning vision-based bipedal locomotion for challenging terrain. _arXiv preprint arXiv:2309.14594_, 2023.\n' +
      '* [17] Thomas Dudzik, Matthew Chignoli, Gerardo Bledt, Bryan Lim, Adam Miller, Donghyun Kim, and Sangbae Kim. Robust autonomous navigation of a small-scale quadruped robot in real-world environments. In _2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 3664-3671. IEEE, 2020.\n' +
      '* [18] Shamel Fahmi, Geoff Fink, and Claudio Semini. On state estimation for legged locomotion over soft terrain. _IEEE Sensors Letters_, 5(1):1-4, 2021.\n' +
      '* [19] Davide Falanga, Kevin Kleber, and Davide Scaramuzza. Dynamic obstacle avoidance for quadrotors with event cameras. _Science Robotics_, 5(40):eaaz9712, 2020.\n' +
      '* [20] Jaime F Fisac, Neil F Lugovoy, Vicenc Rubies-Royo, Shromona Ghosh, and Claire J Tomlin. Bridging hamilton-jacobi safety analysis and reinforcement learning. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 8550-8556. IEEE, 2019.\n' +
      '* [21] Kunihiko Fukushima. Visual feature extraction by a multilayered network of analog threshold elements. _IEEE Transactions on Systems Science and Cybernetics_, 5(4):322-333, 1969.\n' +
      '* [22] Magnus Gaertner, Marko Bjelonic, Farbod Farshidian, and Marco Hutter. Collision-free mpc for legged robots in static and dynamic scenes. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 8266-8272. IEEE, 2021.\n' +
      '* [23] Siddhant Gangapurwala, Mathieu Geisert, Romeo Orsolino, Maurice Fallon, and Ioannis Havoutis. Rloc: Terrain-aware legged locomotion using reinforcement learning and optimal control. _IEEE Transactions on Robotics_, 38(5):2908-2927, 2022.\n' +
      '* [24] Ruben Grandia, Farbod Farshidian, Alexey Dosovitskiy, Rene Ranftl, and Marco Hutter. Frequency-aware model predictive control. _IEEE Robotics and Automation Letters_, 4(2):1517-1524, 2019.\n' +
      '* [25] Ruben Grandia, Fabian Jenelten, Shaohui Yang, Farshidian, and Marco Hutter. Perceptive locomotion through nonlinear model-predictive control. _IEEE Transactions on Robotics_, 2023.\n' +
      '* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [27] David Hoeller, Lorenz Wellhausen, Farbod Farshidian, and Marco Hutter. Learning a state representation and navigation in cluttered and dynamic environments. _IEEE Robotics and Automation Letters_, 6(3):5081-5088, 2021.\n' +
      '* [28] David Hoeller, Nikita Rudin, Dhionis Sako, and Marco Hutter. Anymal parkour: Learning agile navigation for quadrupedal robots. _arXiv preprint arXiv:2306.14874_, 2023.\n' +
      '* [29] Kai-Chieh Hsu, Vicenc Rubies Royo, Claire J. Tomlin, and Jaime F. Fisac. Safety and liveness guarantees through reach-avoid reinforcement learning. In _Robotics: Science and Systems XVII_, 2021.\n' +
      '* [30] Kai-Chieh Hsu, Allen Z Ren, Duy P Nguyen, Anirudha Majumdar, and Jaime F Fisac. Sim-to-lab-to-real: Safe reinforcement learning with shielding and generalization guarantees. _Artificial Intelligence_, 314:103811, 2023.\n' +
      '* [31] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. _Science Robotics_, 4(26):eaau5872, 2019.\n' +
      '* [32] Fabian Jenelten, Jemin Hwangbo, Fabian Tresoldi, C Dario Bellicoso, and Marco Hutter. Dynamic locomotion on slippery ground. _IEEE Robotics and Automation Letters_, 4(4):4170-4176, 2019.\n' +
      '* [33] Fabian Jenelten, Ruben Grandia, Farbod Farshidian, and Marco Hutter. Tamols: Terrain-aware motion optimization for legged systems. _IEEE Transactions on Robotics_, 38(6):3395-3413, 2022.\n' +
      '* [34] Fabian Jenelten, Junzhe He, Farbod Farshidian, and Marco Hutter. Dtc: Deep tracking control. _Science Robotics_, 9(86):eadh5401, 2024.\n' +
      '* [35] Donghyun Kim, Daniel Carballo, Jared Di Carlo, Benjamin Katz, Gerardo Bledt, Bryan Lim, and Sangbae Kim. Vision aided dynamic exploration of unstructured terrain with a small-scale quadruped robot. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 2464-2470. IEEE, 2020.\n' +
      '* [36] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In _Robotics: Science and Systems_, 2021.\n' +
      '* [37] Frederic Large, Dizan Vasquez, Thierry Fraichard, and Christian Laugier. Avoiding cars and pedestrians using velocity obstacles and motion prediction. In _IEEE Intelligent Vehicles Symposium, 2004_, pages 375-379, 2004.\n' +
      '* [38] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. _Science robotics_, 5(47):eabc5986, 2020.\n' +
      '* [39] Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Robust and versatile bipedal jumping control through reinforcement learning. In _Robotics: Science and Systems_, 2023.\n' +
      '* [40] Qingkai Liang, Fanyu Que, and Eytan Modiano. Accelerated primal-dual policy optimization for safe reinforcement learning. _arXiv preprint arXiv:1802.06480_, 2018.\n' +
      '* [41] Qiayuan Liao, Zhongyu Li, Akshay Thirupanam, Jun Zeng, and Koushil Sreenath. Walking in narrow spaces: Safety-critical locomotion control for quadrupedal robots with duality-based optimization. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2723-2730. IEEE, 2023.\n' +
      '* [42] Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence_, 2022.\n' +
      '* [43] Yuntao Ma, Farbod Farshidian, and Marco Hutter. Learning arm-assisted fall damage reduction and recovery for legged mobile manipulators. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 12149-12155. IEEE, 2023.\n' +
      '* [44] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu based physics simulation for robot learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021.\n' +
      '* [45] Gabriel B Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid locomotion via reinforcement learning. _arXiv preprint arXiv:2205.02824_, 2022.\n' +
      '* [46] Matias Mattamala, Nived Chebrolu, and Maurice Fallon. An efficient locally reactive controller for safe navigation in visual teach and repeat missions. _IEEE Robotics and Automation Letters_, 7(2):2353-2360, 2022.\n' +
      '* [47] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. _Science Robotics_, 7(62):eabk2822, 2022.\n' +
      '* [48] Yashwanth Kumar Nakka, Anqi Liu, Guanya Shi, Anima Anandkumar, Yisong Yue, and Soon-Jo Chung. Chance-constrained trajectory optimization for safe exploration and learning of nonlinear systems. _IEEE Robotics and Automation Letters_, 6(2):389-396, 2020.\n' +
      '* [49] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. [https://openai.com/research/benchmarking-safe-explorati](https://openai.com/research/benchmarking-safe-explorati) on-in-deep-reinforcement-learning, 2019.\n' +
      '* [50] Nikita Rudin, David Hoeller, Marko Bjelonic, and Marco Hutter. Advanced skills by learning locomotion and local navigation end-to-end. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2497-2503. IEEE, 2022.\n' +
      '\n' +
      '* Rudin et al. [2022] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In _Conference on Robot Learning_, pages 91-100. PMLR, 2022.\n' +
      '* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n' +
      '* Shin et al. [2023] Young-Ha Shin, Tae-Gyu Song, Gwanghyeon Ji, and Hae-Won Park. Actuator-constrained reinforcement learning for high-speed quadrupedal locomotion. _arXiv preprint arXiv:2312.17507_, 2023.\n' +
      '* Si et al. [2019] Wenwen Si, Tianhao Wei, and Changliu Liu. Agen: Adaptable generative prediction networks for autonomous driving. In _2019 IEEE Intelligent Vehicles Symposium (IV)_, pages 281-286. IEEE, 2019.\n' +
      '* Song et al. [2023] Yunlong Song, Angel Romero, Matthias Muller, Vladlen Koltun, and Davide Scaramuzza. Reaching the limit in autonomous racing: Optimal control versus reinforcement learning. _Science Robotics_, 8(82):eadg1462, 2023.\n' +
      '* 깊이 설정. [https://www.stereolabs.com/docs/depth-sensing/depth-settings#fill-m] (https://www.stereolabs.com/docs/depth-sensing/depth-settings#fill-m) ode, 2024.\n' +
      '* Teng et al. [2021] Sangli Teng, Yukai Gong, Jessy W. Grizzle, and Maani Ghaffari. Toward safety-aware informative motion planning for legged robots. _CoRR_, abs/2103.14252, 2021.\n' +
      '* Tessler et al. [2018] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. _arXiv preprint arXiv:1805.11074_, 2018.\n' +
      '* Thananjeyan et al. [2021] Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph E Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. Recovery rl: Safe reinforcement learning with learned recovery zones. _IEEE Robotics and Automation Letters_, 6(3):4915-4922, 2021.\n' +
      '* Tobin et al. [2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pages 23-30. IEEE, 2017.\n' +
      '* Truong et al. [2023] Joanne Truong, April Zitkovich, Sonia Chernova, Dhruv Batra, Tingnan Zhang, Jie Tan, and Wenhao Yu. Indoorsim-to-outdoorreal: Learning to navigate outdoors without any outdoor experience. In _arXiv preprint arXiv:2305.01098_, 2023.\n' +
      '* Wang et al. [2023] Yikai Wang, Mengdi Xu, Guanya Shi, and Ding Zhao. Guardians as you fall: Active mode transition for safe falling. _arXiv preprint arXiv:2310.04828_, 2023.\n' +
      '* 434, 2023-03. ISSN 2771-3989. doi: 10.3929/ethz-b-000614683.\n' +
      '* Wijmans et al. [2023] Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, and Dhruv Batra. Emergence of maps in the memories of blind navigation agents. In _International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=IT](https://openreview.net/forum?id=IT)!4KjHSyI.\n' +
      '* Xiao et al. [2023] Wenli Xiao, Tairan He, John Dolan, and Guanya Shi. Safe deep policy adaptation. _arXiv preprint arXiv:2310.08602_, 2023.\n' +
      '* Yang et al. [2020] Chuanyu Yang, Kai Yuan, Qiuguo Zhu, Wanming Yu, and Zhibin Li. Multi-expert learning of adaptive legged locomotion. _Science Robotics_, 5(49):eabb2174, 2020.\n' +
      '* Yang et al. [2022] Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, and Xiaolong Wang. Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=nhnJ3oo6AB](https://openreview.net/forum?id=nhnJ3oo6AB).\n' +
      '* Yang et al. [2022] Tsung-Yen Yang, Tingnan Zhang, Linda Luu, Sehoon Ha, Jie Tan, and Wenhao Yu. Safe reinforcement learning for legged locomotion. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 2454-2461. IEEE, 2022.\n' +
      '* Yang et al. [2023] Yuxiang Yang, Guanya Shi, Xiangyun Meng, Wenhao Yu, Tingnan Zhang, Jie Tan, and Byron Boots. Cajun: Continuous adaptive jumping using a learned centroidal controller. _arXiv preprint arXiv:2306.09557_, 2023.\n' +
      '* Zhang et al. [2022] Chong Zhang, Wanming Yu, and Zhibin Li. Accessibility-based clustering for efficient learning of locomotion skills. In _2022 International Conference on Robotics and Automation (ICRA)_, pages 1600-1606. IEEE, 2022.\n' +
      '* Zhang et al. [2023] Chong Zhang, Nikita Rudin, David Hoeller, and Marco Hutter. Learning agile locomotion on risky terrains. _arXiv preprint arXiv:2311.10484_, 2023.\n' +
      '* Zhang et al. [2021] Chong Zhang, Jin Jin, Jonas Frey, Nikita Rudin, Matias Eduardo Mattamala Aravena, Cesar Cadena, and Marco Hutter. Resilient legged local navigation: Learning to traverse with compromised perception end-to-end. In _41st IEEE Conference on Robotics and Automation (ICRA 2024)_, 2024.\n' +
      '* Zhao et al. [2021] Weiye Zhao, Tairan He, and Changliu Liu. Model-free safe control for zero-violation reinforcement learning. In _5th Annual Conference on Robot Learning_, 2021.\n' +
      '* Zhao et al. [2023] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu. State-wise safe reinforcement learning: A survey. In Edith Elkind, editor, _Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23_, pages 6814-6822. International Joint Conferences on Artificial Intelligence Organization, 8 2023. URL [https://doi.org/10.24963/ijcai.2023/763](https://doi.org/10.24963/ijcai.2023/763).\n' +
      '* Zhuang et al. [2023] Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwerfeger, Chelsea Finn, and Hang Zhao. Robot parkour learning. _arXiv preprint arXiv:2309.05665_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>