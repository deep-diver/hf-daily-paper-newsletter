<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ReFT.\n' +
      '\n' +
      ' 진, 항리리 루엉\\({}^{**}\\), 신보 장\\({}^^{**}\\), 한밍 재즈({}^^{{{**}\\), 장셍\\({}^^{{**}\\), 장셴\\({}^^{{**}\\), 자이\'({}^{{ty{{**}\\) 등 펭한밍 재즈({.\n' +
      '\n' +
      'ByteDance Research\n' +
      '\n' +
      '루롱, 장신보 파레야, 자락신보 프레야, 알란}@bytedance.comtrung,@bytedance.comtrung.luong, zhangxinbo.freya, zhangxinbo.freya, 알란}@bytedance.comtrung,@bytedance.comtrung.\n' +
      '\n' +
      '간진, 간항.lh}@bytedance.comwanhesong, xiaoran.\n' +
      '\n' +
      '원자격은 동일한 기여도를 나타내는 상응하는 저자이고, \\(다거\\)는 동일한 기여도를 나타낸다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)의 추론 능력을 향상시키는 한 가지 방법은 사상(CoT) 주석을 사용하여 슈퍼엔드 파인 트닝(SFT)을 수행하는 것이다. 그러나 이 접근법은 주어진 CoT 데이터에만 의존하기 때문에 충분히 강력한 일반화 능력을 나타내지 않는다. 수학 문제 해결에서, 예를 들어, 학습 데이터에는 보통 각 질문에 대해 하나의 주석을 달았던 추론 경로가 있다. 직관적으로, 알고리즘이 질문을 받은 여러 개의 주석이 달린 추론 경로로부터 배우는 것이 더 좋을 것이다. 이 문제를 해결하기 위해 수학 문제 해결을 예로 하여 추론용 LLM의 일반화 가능성을 높이기 위해 _Rein 강화한다 Fine-Tuning_(ReFT)라는 단순하면서도 효과적인 접근법을 제안한다. ReFT는 먼저 SFT로 모델을 따뜻하게 한 다음, 본 논문의 온라인 강화 학습, 특히 PPO 알고리즘을 사용하여 질문을 통해 다양한 추론 경로가 자동으로 샘플링되고 보상은 지상 신뢰 답변에서 자연적으로 파생된 모델에 추가 미세 조정한다. GSM8K, MathQA 및 SVAMP 데이터셋에 대한 광범위한 실험은 ReFT가 SFT를 상당히 능가하고 다수의 투표와 재순위 등 추론 시간 전략을 결합하여 성능을 잠재적으로 더욱 높일 수 있음을 보여준다. rFT는 추가적인 또는 증강된 훈련 질문에 의존하지 않고 SFT와 동일한 훈련 질문에서 학습하여 개선을 획득한다는 점에 유의한다. 이것은 ReFT에 대한 우수한 일반화 능력을 나타낸다. 이 작업의 코드는 공개적으로 이용 가능한 1입니다.\n' +
      '\n' +
      '폐경 1: [https://github.com/lqtrung1998/mmp_ReFT](https://github.com/lqtrung1998/mmp_ReFT)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '수학 문제 해결을 위한 최첨단 접근 방식(Uesato et al., 2022; 루오 et al., 2023; 왕 et al., 2023)은 슈퍼브레이팅 Fine-Tuning(SFT)을 사용하여 아이디어(CoT) 주석(Wei et al., 2022)을 사용하여 모델을 훈련시킨다. 그림 1과 같이 CoT 주석에서는 수학 문제 해결을 위한 중간 추론 단계를 정리한다.\n' +
      '\n' +
      '일반적으로 훈련 데이터에 각 질문에 대한 하나의 CoT 주석, 즉 SFT에서 사용되는 하나의 올바른 추론 경로가 있다. 우리는 이것이 SFT 모델의 상대적으로 약한 일반화 능력을 초래할 수 있음을 관찰한다. 동일한 질문(Cobbe et al, 2021, Zhang et al, 2023)에 대해 다수의 유효한 CoT 주석들이 존재하는 경우가 종종 있으며, 이는 보다 강력한 미세 조정 접근의 필요성을 강조한다. 이 문제를 해결하기 위해 그림 1의 하단에 묘사된 _Rein 강화된 Fine-Tuning_(ReFT)라는 단순하면서도 효과적인 접근법을 제안한다.\n' +
      '\n' +
      'ReFT는 1~2개의 에포치(그림 1, 음영 박스)에서 슈퍼엔드 파인튜닝(SFT)과 관련된 평가 단계로 시작된다. 이 초기 단계.\n' +
      '\n' +
      '<그림 1>은 GSM8K(Cobbe et al 2021)에서 질문(x\\), CoT(e\\) 및 답변(\\(y\\))의 예시이다. SFT 프로세스는 학습 데이터에 대해 여러 에포크를 반복한다. 제안된 SFT로부터의 ReFT 평가 및 동일한 데이터에 대한 RL 학습을 수행한다.\n' +
      '\n' +
      '사전 작업 Cobbe 등(2021)에서 입증된 바와 같이 수학적 문제에 대한 올바른 반응을 어느 정도 생성할 수 있는 능력을 가진 모델을 동일하게 한다. 다음으로, ReFT는 본 논문에서 온라인 강화 학습(RL) 알고리즘 Sutton and Barto(2018), 구체적으로 동일시 정책 최적화(PPO) Schulman 등(2017)의 활용을 통해 모델을 더욱 개선한다. 이와 같이 ReFT는 다수의 올바른 추론 경로 또는 CoT 주석을 샘플링하여 그로부터 학습할 수 있다(그림 2, 우측).\n' +
      '\n' +
      '학습데이터에는 지상진실답이 포함되므로 PPO 훈련 시 자연스럽게 황금보상이 도출될 수 있다. 결과적으로 별도로 훈련된 보상 모델에 대한 요구 사항은 없다. 대조적으로, RLHF Ouyang et al.(2022)는 인간 표지된 데이터로부터 학습된 보상 모델을 활용해야 한다.\n' +
      '\n' +
      '평가 단계 동안 ReFT는 지도 학습으로 일정 수준의 정확도를 획득한다. RL 단계에서 ReFT는 다양한 CoT 추론 경로 샘플링을 통한 강화 학습으로 능력을 더욱 향상시킨다. 이러한 방식으로 ReFT는 SFT보다 훨씬 더 풍부한 감독 신호를 얻는다. 이 접근법은 ReFT가 수학 문제 해결 가오 등의 일반화를 크게 향상시킬 수 있게 한다(2018년), 브라운 등(2020년). WFT는 추가 또는 증강 훈련 질문에 의존하지 않고 SFT와 동일한 훈련 질문을 사용하여 SFT를 능가한다. 실제로 ReFT는 이러한 데이터 엔지니어링과 충돌하지 않으며, 이를 원활하게 결합할 수 있다.\n' +
      '\n' +
      '우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* 우리는 수학 문제를 해결하기 위해 강화 학습을 활용하는 새로운 미세 조정 접근, 강화 미세 조정(ReFT)을 소개한다. ReFT는 동일한 데이터셋에서 학습되었을 때 기존의 감독 미세 조정(SFT)에 비해 향상된 일반화 능력을 나타낸다.\n' +
      '* 우리는 GSM8K Cobbe et al.(2021), MathQA Amini et al.(2019) 및 SVAMP Patel et al.(2021)의 세 가지 표준 수학적 데이터 세트에 대해 두 가지 발견된 모델인 CodeLLAMA Touvron et al.(2023); 로지레 et al.(2023) 및 갈락시스 테일러 et al.(2022)를 사용하여 광범위한 실험을 수행한다. 우리의 실험은 자연 언어와 프로그램 기반 CoT를 모두 포괄하여 ReFT의 상당히 향상된 성능과 일반화 능력을 보여준다.\n' +
      '*각에서 우리는 ReFT가 다수결 왕(2023)의 이익을 얻고 보상 모델이 추론 시간에 우사토 등(2022년)을 다시 대표하여 성과를 더욱 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '최근 연구 노력을 해결하는 수학 문제 해결은 CoT 프롬프트 설계 및 데이터 엔지니어링에 중점을 둔다. 대부분 CoT를 종합적이고 미세하게 만들어 단계적 추론 솔루션 노예(2021), 푸 등(2023), 저우 등(2023), 카이트 등(2023), 이마니 등(2023), 미오 등(2023)을 제시하려고 시도했다. Gao et al.(2023)는 파이썬 프로그램을 CoT 프롬프트로 사용하도록 추가로 제안하여 자연 언어 CoT Wei et al.(2022)보다 더 정확한 추론 단계와 상당한 개선을 보여준다. 주 등은 GPT-4 오픈AI(2023)로 중간 추론 단계를 확인하기 위해 코드를 생성하는 프롬프트 방식을 도입하여 GSM8K 코브 등(2021)과 MATH 헨드롭 등(2021)에서 최첨단 성능을 달성했다. 또 다른 작업 라인은 CoT 왕 등의 품질 향상(2023), 류 등은 알(2023), 유 등은 CoT 데이터 루노(2023)의 양을 늘리고 OpenAI의 ChatGPT(gpt-3.5-turbo) 또는 GPT-42로부터 CoT 데이터 루노(2023)의 양을 늘리는 데 중점을 둔다.\n' +
      '\n' +
      '부타주 2: [부신차트[부신차트 오픈카이.com/] (부시아나이. 오프바이.com/)].\n' +
      '\n' +
      '우리의 연구는 인간 선호 오양 등의 정렬을 위한 자연어 과정에 PPO Schulman et al.(2017)을 적용하는 최근 작업과 대체로 관련이 있다. 이후 직접 선호도 최적화(DPO) 라파일로프(2023), 동일성 선호 최적화(IPO) 아자일로프(2023) 등 정렬을 효율적으로 개선하기 위해 여러 훈련 알고리즘이 제시되었다.\n' +
      '\n' +
      '그림 2: CoT 대안이 있는 경우 SFT와 ReFT 사이의 비교이다.\n' +
      '\n' +
      '야라자 등 2023년. 정렬의 목적 이외의 기존 감독 미세 조정보다 성능을 향상시키기 위한 미세 조정 패러다임으로서 강화 학습을 채택하는 것을 목표로 한다.\n' +
      '\n' +
      '구체적으로 수학 문제 해결을 위해 우사토 등(2022년)과 라이트맨(2023년)은 SFT와 다수결 왕(2023년)에 비해 훨씬 더 나은 성과를 내기 위해 코베 등(2021년)을 재위화하는 결과 기반 또는 프로세스 기반 보상 모델을 훈련했다. 우리의 접근법은 정책 자체의 성과를 향상시키는 것을 목표로 하지만 이러한 보상 모델 재위 접근 방식은 결과적인 정책 모델에 쉽게 통합될 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 연구에서 우리는 파이썬을 사용하여 _자연 언어 CoT_(**N-CoT**) Wei et al.(2022)(그림 1) 및 _프로그램 기반 CoT_Gao et al.(2023)(**P-CoT**)에 중점을 둔다. 가오 등은 수학 문제 해결을 위한 프로그램 기반 CoT를 제안하였다(2023). 우리는 간단히 프로그램을 실행하여 답을 얻을 수 있습니다. 명확성을 보장하고 모호성을 피하기 위해 N-CoT 및 P-CoT를 사용하여 본 논문의 나머지 부분에서 각각 자연어 및 프로그램 기반 CoT를 나타낸다.\n' +
      '\n' +
      '### Reinforced Fine-Tuning\n' +
      '\n' +
      '제안된 재강화 파인튜닝(ReFT) 과정은 평가단계와 강화학습단계의 두 단계로 구성된다. 전체 알고리즘은 알고리즘 1에 나와 있다.\n' +
      '\n' +
      '이 단계에서 정책은 "(_question_, _CoT_)" tuples: \\(\\mathbf{x},\\mathbf{e})\\로 구성된 데이터세트 상에서 몇 개의 epoch에 대해 미세 조정된다. 모델이 질문3에 대한 적절한 응답을 생성하기 위해 기본적인 문제 해결 능력을 가질 수 있도록 하고, 형식적으로는 CoT 생성 과정을 다음 토큰 예측 행동의 시퀀스로 분해할 수 있다. 마지막 액션 토큰 <eos>는 생성 과정을 신호를 하여 종료한다. CoT \\(\\mathbf{e}\\)는 그대로 작성된다.\n' +
      '\n' +
      '발주 3: 기본 개념은 검증자 훈련 코베 등(2021)과 유사하여 다중 솔루션을 생성한다.\n' +
      '\n' +
      '\\[\\mathbf{e}=[a_{1},a_{2},...,a_{L-1},a_{L}\\textlessless]\\]\n' +
      '\n' +
      '여기서 \\(L\\)는 최대 길이를 나타낸다. 타임스메프(t\\)에서 액션 \\(a_{t}\\)는 정책 \\(\\mathbf{\\pi_{\\theta}}(\\cdot|s_{t}})에서 샘플링되며, 여기서 \\(a_{t}\\)은 어휘에서 임의의 토큰이 될 수 있고 상태 \\(s_{t}\\)는 질문에서 모든 토큰으로 구성되어 있으며 지금까지 생성된 모든 토큰(s_{t}\\)은 질문과 모든 토큰으로 구성된다. 각 작용 후, 결과 상태 \\(s_{t+1}\\)는 현재 상태 \\(s_{t}\\)와 액션 \\(a_{t}\\)의 연결이다.\n' +
      '\n' +
      '\\[s_{t+1}=\\begin{case}\\mathbf{x},&t=0\\\\,&1\\leq t\\leq L\\end{case}.\\]]]].\n' +
      '\n' +
      '생산된 액션이 <eos> 토큰에 해당함에 따라, 결과 상태 \\(s_{L+1}\\)는 단자 상태이고 생성 과정이 종료된다. 이러한 표기법으로 표본에 대한 손실함수는 식 1과 같이 기입될 수 있다.\n' +
      '\n' +
      '{S}(\\mathbf{\\o}}\\sum_{{:\\math\\mathcal{D}}\\mathbf{\\pi_{\\ta})\n' +
      '\n' +
      '이 단계에서 정책은 (_question_, _ansuro_) tuples: \\(\\mathbf{x},\\mathbf{y})\\로 구성된 데이터 세트를 사용하여 온라인 자기 학습의 형태를 통해 성능을 향상시킨다. 구체적으로 정책모형은 반복적으로 샘플링 응답(그림 2)을 통해 학습하며, 응답의 정답을 평가하고, 온라인 패션(알고리즘 1의 라인 7-14)에서 그 파라미터를 업데이트한다. 우리는 훈련을 위한 클램핑된 객관적인 알고리즘을 사용하여 PPO Schulman et al.(2017)를 사용한다. 지글러(2019)에 이어 평가 단계 이후 모델인 정책 모델 \\(\\pi_{\\theta}\\)의 마지막 숨겨진 상태 위에 선형값 헤드를 적용함으로써 가치 모델 \\(V_{\\파이}\\)을 구축한다. 0의 보상은 비말단 상태를 초래하는 모든 작용에 대해 주어진다. 단말 상태에서 우리는 국가의 CoT에서 추출한 답변과 지상 진실한 답변 \\(\\mathbf{y}\\)를 직접 비교하는 보상 함수를 사용한다. 여기서, 보상함수는 정답이 옳다고 판단되면 1을 반환하고, 그렇지 않으면 0을 반환한다. 답변이 모두 숫자인 데이터세트에서 _부분 보상_Z홍 등(2017); 0.1의 Le et al.(2022)은 답을 추출할 수 있는 경우와 숫자 유형일 때 적용될 수 있다. \\(1\\leq t\\leq L\\)에 대해 기록합니다.\n' +
      '\n' +
      '(s_{t+1})\\neq\\texttt{EXTRACT}=\\neq\\mathbf{y},\\neq\\mathbf{y}}\\.\n' +
      '\n' +
      '이러한 부분 보상은 희박한 보상 라이더러 등(2018년)에서 학습의 효과를 줄이는 데 도움이 될 수 있다(2019년). 또한 정(2023)에 이어 우리의 총보상은 보상 기능 점수와 풀백-리블러(KL) 발산 쿨백과 라이프블러(1951)의 합으로 학습된 RL 정책과 계수 계수 \\(\\beta\\)로 스케일링된 초기 정책이다.\n' +
      '\n' +
      '\\[\\mathbf{pi},\\_{t}}(\\cathbf{t}),\\mathbf{pi}(\\cath_{t}),\\mathbf{{{(\\cot|_{t})\\(\\cot|_{t})\n' +
      '\n' +
      '우위 계산을 위해 Schulman et al.(2018)의 일반화된 장점 추정치를 사용한다.\n' +
      '\n' +
      '\\[\\hat{A}_{t}=\\sum_{l=0}^{L-t}(\\gamma\\lambda)^{l}\\delta_{t+l},\\]\n' +
      '\n' +
      'TD(multporal Difference, TD)가 프로파일링 Difference(TD)인 경우, 암호화 Difference(TD)로 정의된다.\n' +
      '\n' +
      '}=V_{t\\prime}(_{t^{\\prime}})+{t^{\\prime}}+{t^{{\\prime}(_{t^{\\prime}+1})\\.\n' +
      '\n' +
      '말단 상태 값 \\(V_{\\파이}(s_{L+1}):=0\\), \\(\\lambda\\in(0,1]\\)는 보상을 위한 할인 요소이며, \\(\\gamma\\in[0,1]\\)는 TD의 할인 요소이다. 수익률 추정을 위해 일반화된 장점 추정치와 가치 추정치의 합으로 표기할 수 있는 \\(\\lambda\\)-레턴 \\(\\hat{R}_{t}\\)를 인용한다.\n' +
      '\n' +
      '\\[\\hat{R}_{t}=\\hat{A}_{t}+V_{\\mathbf{\\phi}}(s_{t})\\]\n' +
      '\n' +
      '마지막으로 정책가치목표는 아래 두 식과 같이 표기할 수 있다.\n' +
      '\n' +
      '}}\\frac{\\mathbf{\\pi}}}\\Biggf{\\mathbf{\\pi}} (a_{\\mathbf{\\f{\\ta}})=_{\\mathbf{\\o}}(a_{t}{\\math{f}}:{\\math{f{\\d{\\math{f{\\math{f{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\mathbf{\\math{\\f{\\mathbf{\\math{\\math{\\math{\\bf{\\math{\\f{\\math{\\f{\\math{\\f{\\f{\\f}}}}}}}_{\\math{\\bf{\\f}}}}}}}_{\\math{\\f{\\f}}}}}}} <{\\math{f{\\f}}}}} [좌표].\\left.\\[\\left. <\\mathbf{e}} <\\math{f}}} <\\math{f}} <\\math{f}}}> <\\math{{t}}> <\\math{{t}} <\\math{{t}> <\\math{{t}} <\\math{{t}>>}. 좌측\\|\\text{clip}\\lele\\\\text{clip}\\left(V_{\\파이}(s_{t})-\\hat{R}_{t},\\hat{A} _{t}-\\epsilon,\\hat{A}_{t}_{t}_{t}_{t}+\\epsilon\\ar)\\ 오른쪽 <\\ar]\\ar.\\ar.\\ar.\\ar:\\ar)\\\\|^{clip}(s_{t},\\hat{A}_{t},\\hat{A}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}_{t}+\\epsilon 및+\\epsilon\\ar:+\\epsilon\\ar)\\ar.\\ar.\\ar.\\ar)\\ar.\\ar.\\ar.\\ar.\\ar)\\ar.\\ar.\\ar.\\ar.\\ar.\n' +
      '\n' +
      '\\(\\mathbf{\\pi}_{\\theta_{\\text{old}}}}), \\(V_{\\phi_{\\text{old}}}}})는 CoT 및 컴퓨팅(\\hat{A}_{t}_{t}\\), \\(\\hat{R}_{t}_{t}\\)에 사용된다. 통일 손실 함수는 위의 목적들의 가중 합이다.\n' +
      '\n' +
      'cal{L}(\\mathbf{\\mathcal{})\n' +
      '\n' +
      '여기서 \\(\\alpha\\)는 가치함수 손실에 대한 계수이다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'GSM8K 코베 등 3개 수학 문제 데이터셋(2021), SVAMP 파텔 등(2021), MathQA 아미니 등(2019)에 대한 실험을 진행한다. GSM8K와 SVAMP 모두 답변 형식은 숫자 값이다. MathQA에서 형식은 대신 다중 선택 목록(즉, ABCD)이다. 표 1은 모든 데이터 세트의 통계를 제시한다. 우리는 몇 번의 샷 프롬프트 웨이(2022)를 수행하며, GPT-3.5-트루보(2023)를 사용하여 N-CoT 및 P-CoT 주석4를 모두 얻기 위해 Gao et al. Jie et al.(2023)에 따라 N-CoT 및 P-CoT 주석을 얻는다. 우리는 또한 형식이 숫자 값인 MathQA(Jie and Lu, 2023) 숫자 버전에 대한 추가 실험을 수행했다. 이러한 실험은 MathQA(SS4.4)에 대한 잠재적인 보상 해킹 현상(Skalse et al., 2022)의 가정을 입증하는 데 사용된다.\n' +
      '\n' +
      '### Baseline\n' +
      '\n' +
      '우리는 ReFT와 SFT 및 자가 훈련(Xie et al., 2020; Amini et al., 2022) 기저부를 비교한다. SFT는 단순히 학습 데이터에 대한 언어 모델을 미세 조정한다. 자가 훈련 방법을 사용한 실험은 이러한 모든 방법이 모델에서 생성된 샘플을 사용하는 메커니즘을 공유하기 때문에 비교적 공정한 비교를 보장한다.\n' +
      '\n' +
      '커뮤니케이션 셀프 트레이닝(**Offline-ST**)과 온라인(Hoi et al., 2021), 셀프 트레이닝(** 온라인-ST***)을 구현했다. 오프라인-ST 방법은 전문가 반복(Anthony et al, 2017, Uesato et al., 2022)과 유사하다. 먼저 초기 체크포인트부터 SFT 체크포인트를 사용하여 CoT를 샘플링하고 근거 진실에 대해 확인합니다. 우리는 정답이 있는 전문가 샘플만 유지합니다. 원래 훈련 데이터와 전문가 샘플의 조합에 대해 감독된 미세 조정 작업을 수행한다.\n' +
      '\n' +
      '온라인-ST 방법은 ReFT와 밀접하게 비교할 수 있도록 만들어졌다. BSFT에 이어 온라인-ST는 동일한 평가 과정을 가지고 있다. 그 후, 우리는 플라이에 생성된 샘플과 지속적인 훈련을 수행한다. 각 훈련 단계에서 모델은 배치에 대한 CoT를 먼저 샘플링하고 정답이 있는 샘플만 유지한다. 생성된 배치는 샘플링된 CoT와 접지-진실 CoT로 구성된다. 그런 다음 감독된 미세 조정 목적 \\(\\mathcal{L}_{SFT}\\)로 이 배치의 모델 파라미터를 업데이트한다. 온라인-ST는 ReFT와 비교하여 부정적인 반응(오답으로)을 사용하지 않고, 모델이 초기 모델과 크게 분기되는 것을 방지하기 위한 전용 메커니즘을 가지고 있지 않으며, 이는 과제별 과적합과 훈련 불안정성으로 나타날 수 있다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리는 갈락티카-6.7B5(타일러 et al., 2022)와 코델라마-7B6(Roziere et al., 2023)의 두 가지 기초 모델을 사용한 실험을 수행한다. 두 모델 모두 수학 문제 해결에 강한 성과를 보이는 것으로 보고되고 있으며, 추론 과제에 대한 최근 문헌(Yue et al., 2023; Luo et al., 2023)에서 일반적으로 채택된다. 바젤과의 비교 외에도 GSM8K에 공통 기법, 다수결(왕 등 알, 2023) 및 보상 모델 재위(라이트맨 등 2023)를 적용한다.\n' +
      '\n' +
      '부타주 5: [국무신경부당 표면.코/면책/갈락티카-6.7b] (국무신경부당 표면.co/비대북/갈락시카/갈락토카-6.7b)\n' +
      '\n' +
      '부타주 6: [국경://hugging.co/코델라마/코델라마/코델라마/코델라마-7b-hf] (https://hugging/Codellama/Codellama-7b-hf)\n' +
      '\n' +
      '모든 실험에서 하이퍼-파라미터는 딥스피드(라자바리 et al., 2020; 라슬리 et al., 2020) 제로 스테이지 2와 휴깅페이스 엑셀레이트(Gugger et al, 2022)를 사용하여 8 A100-80GB GPU로 트레이닝을 수행한다. BSFT의 평가 단계에서 우리는 0.1의 평가 배급으로 AdamW(Loshchilov and Hutter, 2017) 최적화를 사용한다. 배치 크기는 48로 설정하고 학습율은 \\(1e\\)-\\(5\\)이다. 최대 길이는 \\(1024\\)로 설정됩니다. 선별 단계의 epoch 수는 MathQA\\({}_{\\text{MCQ}}\\) 및 MathQA\\({}_{\\text{numeric}}\\)를 제외한 모든 환경에서 \\(1\\) 또는 \\(2\\)로 각각 최대 5와 10을 사용한다. 모델은 \\(3e\\)-\\(7\\)의 학습률을 갖는 \\(300\\) 에포치에 대해 훈련된다. Ziegler et al.(2019)에 이어 PPO의 \\(\\lambda\\), \\(\\gamma\\), \\(\\알파\\), \\(\\) 및 \\(U\\)는 각각 \\(1\\), \\(0.95\\), \\(5\\), \\(0.2\\), \\(2\\)로 설정된다. KL 계수 \\(\\beta\\)는 P-CoT에 대해 \\(0.01\\)로 설정되며 N-CoT 실험을 위해 \\(0.05\\)로 설정된다. rFT에 대한 추가 하이퍼파라미터 설정은 부록 B에서 찾을 수 있다.\n' +
      '\n' +
      'SFT 기준선을 위해 40epochs 모델을 훈련하고 최상의 성능으로 체크포인트를 선택합니다. 이 수의 에포크는 SFT 수렴을 보장하기 위해 충분히 많은 것으로 선택되었다. 오프라인-ST 베이스라인의 경우 ReFT 평가 단계의 체크포인트를 사용하여 CoT를 샘플링한다. 생성 온도 1.0과 최대 길이 1024를 사용하여 각 질문에 대해 100개의 CoT를 샘플링하고 정답만 있는 사람들을 유지한다. 그런 다음 싱 et al.(2023)에 이어 질문당 CoT를 10개의 무작위 고유 CoT로 하위 샘플링하여 질문의 어려움을 균형을 맞추었다. SS4.2에서 언급한 바와 같이, 온라인-ST 기준선은 ReFT와 동일한 설정을 모방하려고 한다. 우리는 동일한 평가 공정을 가지고 있으며 하이퍼파라미터 설정은 ReFT와 대략 동일하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline  & **GSM8k** & **SVAMP** & **MathQA\\({}_{\\text{MCQ}}\\)** & **MathQA\\({}_{\\text{numeric}}\\)** \\\\ \\hline\n' +
      '******N-CoT** & 7,465 & 14,862 & 8,955입니다.\n' +
      '3,356 & 15,250&3,672 \\\\*****P-CoT*********** 7,356 & 7,043&3,672 \\\\*********\n' +
      '**Test** & 1,319 & 1,000 & 1,605 & 1,605 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 1>은 훈련 집합과 시험 집합에서 두 종류의 CoT의 Dataset 통계량이다.\n' +
      '\n' +
      '모델 컴플로우.\n' +
      '\n' +
      '코브 등(2021년)에 이어 우사토 등(2022년)에 따라 CoT의 정확도를 결정하기 위해 보상 모델(RM)을 훈련한다. RM 학습 데이터를 구성하기 위해 평가 단계에서 모델을 사용하고 샘플링을 수행하여 훈련 세트에서 각 질문에 대한 100개의 CoT를 얻었다. CoT는 중복되고 이항 라벨은 추출된 답변과 근거 진리를 비교하여 얻을 수 있다.\n' +
      '\n' +
      '공통 관행으로서 보상 모델은 최고의 SFT 체크포인트 코브(Cobbe et al.(2021), 오양 등(2022)에서 초기화된 언어 모델이다. 결과 기반 보상 모델(ORM) Uesato et al.(2022)과 유사하게, 보상 모델은 "_c 보정_" 또는 "_incision_" 솔루션을 나타내는 이진 라벨을 예측하도록 훈련된다. 입력이 보상 모델을 통과하면 마지막 토큰의 히든 상태에 대한 선형 분류기로 분류를 수행한다. 마지막으로 후보 중 \'올바른\' 점수가 가장 높은 솔루션이 최종 답안으로 선정된다. 우리는 배치 크기 48과 최대 길이 700을 사용하여 3epochs에 대한 RM 모델을 훈련한다.\n' +
      '\n' +
      '#### Evaluation\n' +
      '\n' +
      '모든 데이터 세트에서 N-CoT와 P-CoT 모두에 대한 가치 정확도를 보고한다. 특히 다수결 및 재순위(표 4)에 대해서는 평가를 위해 100개의 CoT를 표본으로 한다. 투표에서 다수의 카운트를 갖는 유효한 답변은 컴퓨팅 정확도를 위한 최종 답으로 선택된다. 재위권에서는 가장 높은 점수를 가진 CoT를 선택하여 답을 추출한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '세븐은 SFT입니다.\n' +
      '\n' +
      '표 2는 기저부 간의 성능을 비교하고 GSM8K, SVAMP 및 MathQA 데이터 세트에 대한 ReFT를 제안했다. 우리는 ReFT가 MathQAMCQ N-CoT를 제외하고 SFT와 자기 훈련 가족 접근법에 비해 훨씬 더 나은 성능을 일관되게 달성한다는 것을 관찰할 수 있다. 구체적으로 GSM8K N-CoT 및 P-CoT에서 코드LLAMA가 있는 SFT에 비해 각각 \\(9\\)-포인트 및 \\(8\\)포인트 개선이 더 많다. 평균적으로 N-CoT 및 P-CoT의 모든 데이터 세트에 대해 코드LLAMA로 각각 3.7점 및 5.9점 개선을 달성했다. 더 중요한 것은 ReFT에서 추가 주석이나 보상 모델이 사용되지 않는다는 것이다. 이러한 강력한 결과는 ReFT(분석 SS5.1 참조)의 강력한 일반화 및 강화 학습 루 등(2023)으로 훈련 데이터를 더 탐색할 수 있는 엄청난 잠재력을 보여준다.\n' +
      '\n' +
      '커뮤니케이션 셀프 트레이닝은 미세 조정의 초기 정책으로부터의 샘플링 데이터를 포함한다. 이 간단한 기준선은 때때로 SFT He et al.(2020)에 비해 성능을 향상시킬 수 있지만 Gulcehre et al.(2023)는 ReFT가 만든 것보다 훨씬 뒤처져 있다. 이러한 비교는 ReFT에서 좋은 성능을 가지려면 "_exploring_"가 필수적임을 나타낸다. 온라인 셀프트레이닝은 갈락티카로 약간의 개선을 달성하지만, 여전히 평균적으로는 ReFT에 크게 뒤처져 있다. 이 결과는 잘못된 사례도 더 나은 탐구를 위해 모델을 안내하는 데 매우 필수적임을 나타낸다. 자가 훈련과의 비교에서도 표준 데이터 증강 접근법보다 온 정책 샘플링과 강화 학습이 제안된 접근법이 더 낫다고 제안한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**GSM8K**} & \\multicolumn{2}{c}{**SVAMP**} & \\multicolumn{2}{c}{**MathQA**MCQ} & \\multicolumn{2}{c}{**Average**} \\\\  & & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** & **N-CoT** & **P-CoT** \\\\ \\hline Galactica + SFT & 6.7B & \\(41.0\\) & \\(57.1\\) & \\(53.8\\) & \\(69.3\\) & \\(58.7\\) & \\(64.8\\) & \\(51.2\\) & \\(63.7\\) \\\\ Galactica + Offline Self-Training & 6.7B & \\(45.0\\) & \\(61.0\\) & \\(56.5\\) & \\(70.8\\) & \\(\\mathbf{60.7}\\) & \\(67.5\\) & \\(54.1\\) & \\(66.5\\) \\\\ Galactica + Online Self-Training & 6.7B & \\(45.7\\) & \\(61.9\\) & \\(58.5\\) & \\(73.7\\) & \\(59.7\\) & \\(62.4\\) & \\(54.6\\) & \\(66.0\\) \\\\ Galactica + ReFT & 6.7B & \\(\\mathbf{46.8}\\) & \\(\\mathbf{68.4}\\) & \\(\\mathbf{62.3}\\) & \\(\\mathbf{73.9}\\) & \\(58.3\\) & \\(\\mathbf{70.4}\\) & \\(\\mathbf{55.8}\\) & \\(\\mathbf{70.9}\\) \\\\ \\hline \\hline CodeLLAMA + SFT & 7B & \\(44.0\\) & \\(64.4\\) & \\(59.6\\) & \\(76.2\\) & \\(56.5\\) & \\(64.2\\) & \\(53.4\\) & \\(68.3\\) \\\\ CodeLLAMA + Offline Self-Training & 7B & \\(38.8\\) & \\(65.0\\) & \\(54.2\\) & \\(72.5\\) & \\(57.6\\) & \\(62.8\\) & \\(50.2\\) & \\(66.8\\) \\\\ CodeLLAMA + Online Self-Training & 7B & \\(40.0\\) & \\(64.3\\) & \\(59.7\\) & \\(75.4\\) & \\(55.5\\) & \\(68.2\\) & \\(53.1\\) & \\(69.3\\) \\\\ CodeLLAMA + ReFT & 7B & \\(\\mathbf{53.5}\\) & \\(\\mathbf{72.8}\\) & \\(\\mathbf{60.0}\\) & \\(\\mathbf{78.4}\\) & \\(\\mathbf{57.9}\\) & \\(\\mathbf{71.5}\\) & \\(\\mathbf{57.1}\\) & \\(\\mathbf{74.2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 기저부의 가치 정확도 비교 및 모든 데이터셋에 두 개의 기반 모델로 미세 조정된 ReFT 방법을 제안했다.\n' +
      '\n' +
      '그림 3: MathQAMCQ의 실시예 예측은 보상 해킹을 보여준다.\n' +
      '\n' +
      'MathQA\\({}_{\\text{MCQ}}\\)에 대한 음성 결과에 대한 우리의 조사 결과는 ReFT가 훈련 중 선택형 질문에 대한 보상 해킹(Skalse et al., 2022)에 시달리고 있음을 나타낸다. 그림 3은 샘플링된 솔루션이 RL 훈련이 겪는 "_누적 보상_"을 생성하는 방법을 보여준다. 우리가 볼 수 있듯이, 샘플링된 CoT는 "_18_" 및 "22"의 산물이 아닌 잘못된 답변 "_344_"를 얻는다. 그러나 최종 추론 단계는 모델이 중간 CoT7의 정확성에 관계없이 항상 {A, B, C, D, E}로부터 옵션 중 하나를 예측할 것이기 때문에 최종 답으로 "_C_" 옵션을 예측하는데, 이러한 오해의 소지가 있는 CoT는 이를 올바른 CoT로 처리하기 위한 양의 보상 "1"을 받고 모델을 오도할 것이다. 기본 보상 해킹 현상은 모델 훈련(Everitt et al, 2021)을 심각하게 변조한다. 이는 보상 해킹 효과를 줄이기 위해 MathQA에 대한 평가 단계가 더 긴 체크포인트를 선택한 이유이기도 하다.\n' +
      '\n' +
      '부츠 7: 프로그램 기반 CoT가 자연어보다 엄격해 고통을 받을 가능성이 낮다는 것을 발견했다.\n' +
      '\n' +
      'MCQ 문항의 부정적인 영향을 추가로 입증하기 위해 Jie와 Lu(2023), MathQA\\({}_{\\text{numeric}}\\)에 의한 MathQA 변이체에 대한 실험을 수행하여 질문의 옵션을 제거하고 숫자 답을 직접 예측한다. 표 3은 SFT에 대한 비교를 나타낸다. 우리는 ReFT가 Galactica와 CodeLLAMA를 모두 사용하여 SFT를 일관되게 능가한다는 것을 관찰할 수 있다.\n' +
      '\n' +
      '주요 투표와 리랭킹 벤비트 레이즐링 왕 등(2023년), 우사토 등(2022년), 라이트맨 등(2023년)은 또한 다수의 투표와 보상 모델을 수행하여 ReFT가 이러한 공통 기술로부터 이익을 얻을 수 있음을 보여준다. 구체적으로 SFT 및 ReFT 정책 모두에서 샘플링을 수행한다. 각 질문에 대한 \\(100\\) CoT 솔루션을 샘플하고 SS4.3. 표 4에 설명된 보상 모델을 적용하여 ReFT가 보상 모델 재위치에 의해 GSM8K에서 최상의 성능을 일관되게 달성한다는 것을 보여준다. ReFT + 투표는 모든 환경에서 평균적으로 SFT + \\(9.2\\) 포인트를 상당히 능가한다. 평균적으로 \\(3.3\\) 포인트 재위임이 있는 재위권 SFT를 능가한다.\n' +
      '\n' +
      '기존 오픈소스 접근법(Luo et al., 2023; 왕 et al., 2023; Yue et al., 2023)과 비교하여, 우리의 최고의 P-CoT 변이체는 GSM8K에서 정확도 \\(79.3\\)로 최상의 성능을 달성한다. 또한 이러한 접근법에는 주로 ChatGPT에서 생성된 추가 데이터가 포함되어 미세 조정 중에 증류를 수행한다. 이에 비해 기존 학습자료의 잠재력을 활용하고 정책성과의 한계를 추진하여 정책 자체를 개선하고자 한다. 표 4에서 보고된 가장 좋은 결과, 즉 P-CoT 설정으로 코드LLAMA + ReFT + Reranking은 GPT-3.5-turbo를 약간 능가한다. 그러나 7B의 크기에만 있는 모델로 결과를 얻습니다.\n' +
      '\n' +
      '발주 8: 은어는 원래 논문에서 가져옵니다. MAmmoTH-Coder에 대한 N-CoT 및 P-CoT 결과는 부록에서 보고된다.\n' +
      '\n' +
      '직관적으로 작은 모델에 대한 실험은 소규모 언어 모델로 불완전한 시연으로 이어질 수 있다. 우리는 수술을 합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Size**} & \\multicolumn{2}{c}{**GSM8K**} \\\\  & & \\multicolumn{1}{c}{**N-CoT**} & **P-CoT** \\\\ \\hline Galactica + SFT + Voting & 6.7B & 50.8 & 61.1 \\\\ Galactica + ReFT + Voting & 6.7B & 55.7 & 70.7 \\\\ Galactica + SFT + Reranking & 6.7B & 56.5 & 72.4 \\\\ Galactica + ReFT + Reranking & 6.7B & **62.8** & **76.6** \\\\ \\hline \\hline CodeLLAMA + SFT + Voting & 7B & 53.8 & 67.9 \\\\ CodeLLAMA + ReFT + Voting & 7B & 65.1 & 75.0 \\\\ CodeLLAMA + SFT + Reranking & 7B & 61.5 & 77.6 \\\\ CodeLLAMA + ReFT + Reranking & 7B & **65.7** & **79.3** \\\\ \\hline \\hline Extra Training Unit & -1 & -1 & -1 \\\\ WizardMath (Luo et al., 2023) & 7B & 54.9 & - \\\\ WizardMath (Luo et al., 2023) & 13B & 63.9 & - \\\\ MathCoT (Wang et al., 2023) & 7B & 67.8 & - \\\\ MAmmoTH-Coder (Yue et al., 2023) & 7B & 22.2 & 58.8 \\\\ MAmmoTH-Coder (Yue et al., 2023) & 70B & 72.4 & 76.7 \\\\ \\hline \\hline GPT-3.5-turbo (Jie et al., 2023) & N.A. & \\(75.3\\) & \\(78.0\\) \\\\ GPT-4 (OpenAI, 2023; Zhou et al., 2023a) & N.A. & \\(93.0\\) & \\(97.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: GSM8K에 대한 SFT 및 ReFT에 대한 다수결 및 보상 모델의 정확성 판별 모델 재순위이다. 우리는 또한 비교를 위한 기존 접근법을 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline \\multicolumn{2}{c}{**Method**} & \\multicolumn{2}{c}{**N-CoT**} \\\\ \\hline \\multirow{2}{*}{**Galactica**} & SFT & \\(41.1\\) \\\\  & ReFT & \\(44.9\\) \\\\\n' +
      '**Codellama** & SFT & \\(36.3\\) \\\\  & ReFT & \\(41.0\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 갈락티카-125M9를 사용한 P-CoT 데이터에 대한 MathQA\\({}_{\\text{numeric}}\\) 벤치마킹에 대한 2개의 기반 모델을 사용한 SFT 및 ReFT의 측정 결과 SFT와 ReFT의 성능 비교는 표 5에 나와 있다. 놀랍게도, ReFT는 작은 모델에서도 3개의 데이터 세트에서 SFT를 여전히 능가한다. 이러한 개선은 합리적인 프로그램의 탐색 동안 ReFT의 견고성을 보여준다.\n' +
      '\n' +
      '부츠 9: 갈락티카에서 사용할 수 있는 가장 작은 모델 크기[https://huggingface.co/facebook/갈락토카-125m] (https://huggingface.co/facebook/갈락토카-125m)\n' +
      '\n' +
      '우리는 GSM8K P-CoT에 대한 코드LLAMA를 사용하여 절제 연구를 수행한다(표 6). 부분 보상 없이 ReFT는 더 낮은 정확도 \\(70.9\\)를 얻지만 SFT보다 여전히 훨씬 낫다. SS3.1에서 언급한 바와 같이 이러한 부분 보상은 훈련 중 희소 보상(Trott et al., 2019)의 효과를 줄이는 데 도움이 될 수 있다. 또한, KL 계수 \\(\\beta\\)를 \\(0\\)로 설정하면 정책 분포가 쉽게 붕괴되어 예상치 못한 결과(즉, \\(0\\) 정확도를 발생시킬 것이다. 정책이 탐구하는 공간(오양 등 2022년)에 제약을 가하는 것이 분명히 중요하다. 초기 평가 단계는 본질적으로 그러한 제약을 만들고 정책은 \\(\\beta\\)에 의해 지배되는 범위 내에서 더 탐색할 수 있게 한다. 마지막으로 정책 모델(그리고 2021년, 코브 등, 2021년)과 공유하는 파라미터가 없는 가치 모델을 실험한다. 개별 가치 모형은 매개 변수를 정책 모형과 동일하게 초기화한다. 이러한 설정이 모델이 더 빠르게 수렴할 수 있고 결국 동등한 성능에 도달하지만 각 배치에 대해 두 번 전진 패스를 수행해야 하므로 원래 계산 오버헤드의 두 번 희생한다는 것을 발견했다.\n' +
      '\n' +
      '## 5 Analysis\n' +
      '\n' +
      '### Generalization\n' +
      '\n' +
      '그림 4는 GSM8K P-CoT에서 ReFT10을 훈련하는 동안 평균 보상, 평가 정확도 및 KL 분기를 보여준다. SFT는 40\\({}^{th}\\) 에포치에 접근할 때 수렴하여 과대 적합해진다. 그러나 40\\({}^{th}\\) epoch에서 ReFT 정책에 대해 평균 보상은 약 80~90%이며, 가치 정확성도 증가하고 있다. 또한 KL 발산(그림 4 (c))이 초기에 매우 큰 후 \\(0\\)와 \\(10\\) 사이의 합리적인 값을 유지하는 것을 알 수 있다. 안정적인 KL 분기는 우리의 정책이 적절한 프로그램을 포함하는 공간 내에서 탐색을 수행하는 것을 나타낸다. 기본 강화 학습 메커니즘은 ReFT(브라운 et al., 2020)의 일반화 능력을 크게 향상시킵니다.\n' +
      '\n' +
      '부츠 10: 일러스트 목적의 경우 \\(60\\) epochs에 대한 평균 보상 및 KL만 보여준다.\n' +
      '\n' +
      '### ReFT가 SFT를 뛰어넘을 때?\n' +
      '\n' +
      '우리는 ReFT와 SFT 사이의 관계를 추가로 조사하기 위해 SFT의 다른 수의 평가 단계로 ReFT 교육을 수행한다. 그림 5는 SFT11에 대한 다양한 ReFT 변이체의 가치 정확도를 보여주고 있으며, 특히 평가 단계가 \\(3\\)인 경우 정책 초기화(3^{rd}\\)-포흐 SFT 체크포인트로부터 정책 초기화를 의미한다. 우리는 모든 ReFT 정책이 에포치가 \\(8\\) 미만인 초기에 더 나쁜 성과를 보이고 있음을 알 수 있다. 공유값 모델의 선형층은 무작위로 초기화되어 분포 조절을 위해 몇 개의 에포크를 취할 수 있기 때문이다. I\\(30^{th}\\) 에포치를 시작으로 SFT 수렴과 모든 ReFT 변이체가 여전히 개선되고 있다. 또한 모든 변이체가 상당한 마진으로 SFT를 능가하고 특정 ReFT 변이체의 명백한 이점이 없음을 볼 수 있다.\n' +
      '\n' +
      '부츠 11: 일러스트 목적으로 60 에포치만 보여줍니다. 후기 에포치에 대한 공연은 부록에서 보여질 것이다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '수학 문제를 해결하기 위한 미세 조정 모델의 새로운 방법으로 강화된 미세 조정(ReFT)을 도입했다. SFT와 달리 ReFT는 단일 CoT 주석에 의존하기보다는 정답 검색에서 다중 CoT 주석을 탐색하여 구별할 수 없는 목적을 최적화한다.\n' +
      '\n' +
      '두 개의 기초 모델을 사용하여 세 개의 데이터 세트에 대한 광범위한 실험을 통해 ReFT가 성능 및 일반화 능력 측면에서 SFT를 능가한다는 것을 입증했다. 더욱이, 우리는 다수의 투표(왕 등 2023), 보상 모델 재위(Cobbe et al, 2021; Uesato et al., 2022)와 같은 기술로 ReFT로 훈련된 모델의 호환성을 보여주었다.\n' +
      '\n' +
      '또한, ReFT는 수학에서 유사한 크기의 공개적으로 이용 가능한 여러 오픈 소스 모델에 비해 우수한 성능을 나타냈다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Model Setting** & **Accuracy** \\\\ \\hline CodeLLAMA + ReFT & \\(72.7\\) \\\\ – remove partial reward & \\(70.9\\) \\\\ – KL coefficient \\(\\beta=0\\) & _collapse_ \\\\ – non-shared value model & \\(72.6\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: GSM8K P-CoT에 대한 Ablation 연구는 다음과 같다.\n' +
      '\n' +
      '문제 해결. 이는 ReFT 접근법의 효과와 실질적인 가치를 보여준다.\n' +
      '\n' +
      '7개의 미래 작업을 합니다.\n' +
      '\n' +
      '우리는 수학 문제 해결을 위해 LLM을 미세 조정하기 위해 강화 학습, 특히 PPO 알고리즘 Schulman et al.(2017)을 적용하려는 첫 번째 시도를 했다. 우리의 미래 작업은 오프라인 강화 학습 기법 Levine et al.(2020)의 활용, Gulcehre et al.(2023), 훈련 효율성과 성능을 높이기 위한 _Å-up 자유_ 방법의 개발, 이에 따라 재위법과의 격차를 줄이는 것을 포함한다. 또한 라이트맨 등(2023)은 잘 훈련된 프로세스 기반 보상 모델(PRM)이 성능을 크게 향상시킬 수 있음을 시사한다. 따라서 강화 학습 훈련에서 과정 기반 보상의 구현을 탐색할 가치가 있을 것이다. 마지막으로 ReFT가 다재다능한 접근인 만큼 CoT로 추론을 공식화할 수 있는 보다 일반적인 추론 작업에 적용하고자 한다.\n' +
      '\n' +
      'Limitations\n' +
      '\n' +
      '그림 4(b)에 묘사된 바와 같이, ReFT는 SFT에 비해 융합에 도달하기 위해 더 많은 수의 에포크를 필요로 하는 것이 분명하다. 이는 주로 ReFT가 미분할 수 없는 목표를 최적화하고 정답을 얻기 위해 생성 공간을 탐색해야 한다는 데 기인한다. 더 큰 학습율은 융합을 빠르게 할 수 있지만 정책은 불안정성과 잠재적 붕괴에 더 취약하게 만든다. 대안적으로, 더 큰 배치 크기를 사용하는 것은 실행 가능한 옵션이지만, 계산 비용 증가를 희생시킨다.\n' +
      '\n' +
      '리워드 해킹 보상 기능은 보상을 결정하기 위해 최종 답변에만 의존한다. 그러나 MathQAMCQ N-CoT 데이터셋에 대해 수행된 실험에서 입증된 바와 같이 A,B,C,D와 같이 최종 답변의 가능한 공간이 제한되면 정책을 쉽게 조작할 수 있다. 보상 해킹 문제를 완화하기 위해서는 더 넓은 범위의 요소를 고려한 보다 세부적이거나 프로세스 기반 보상 기능을 채용할 필요가 있을 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. 아미니, S. 가브리엘, S. 린, R. Koncel-Kedziorski, Y. 최씨와 H. 하지시르지(2019)MathQA: 운영 기반 형식과의 해석 가능한 수학어 문제 해결에 관한 것이다. NAACL의 개시에서 SS1, SS2에 의해 계산된다.\n' +
      '*M. 아리니, V. 로파노프, L. 파쿨레토, E. Devijver 및 Y. 맥시모프(2022) 자기교육: 설문 조사. arXiv 프리프린트 arXiv:2202.12040: SS1, SS2.\n' +
      '*M. 안트리코위즈, A. 라히륵, P. 스티키지크, M. 오리신이, S. 기긴, R. 마니니에, L. Hussenot, M. 지스트, O. 피테퀴인, M. 미칼스키, 예를 들어 (2021) 온정책 강화 학습에는 어떤 사항이 있습니까? 대규모 실증연구입니다. ICLR의 개시에서: SS1, SS2가 계산합니다.\n' +
      '* T. 안토니, Z. 티안, D. 바버(2017)는 딥러닝과 트리 검색으로 빠르고 느립니다. NurIPS의 모집에서 SS1, SS2에 의해 계산된다.\n' +
      '*\n' +
      '\n' +
      '그림 4는 GSM8K P-CoT에 대한 훈련 epoch에 대한 ReFT, 평가 정확도, KL의 훈련 보상이다.\n' +
      '\n' +
      '그림 5: 평가-업 에포치의 수가 다른 SFT와 ReFT 간의 친화도 비교이다.\n' +
      '\n' +
      '* 아자 등은 (2023) 모하마드 접착제힐라기 아자르, 마크 로웨랜드, 빌랄 푸엇, 다니엘 구오, 다에글 칼란디넬로, 미칼 발코, 로미 문오스 등이다. 인류 선호로부터 학습을 이해하기 위한 일반적인 이론적 패러다임은2023. __ 인간 선호로부터 학습을 이해하는 일반적인 이론적 패러다임이다. arXiv 프리프린트 arXiv:2310.12036_.\n' +
      '* 브라운 등은 (2020) 다니엘 S 브라운, 원준 고, 스콧 니킷 등이 있다. 2020. 베터-시범자 모방 학습은 자동 순위 시연을 통해 학습한다. 로봇 학습_에 관한 회의의 _발표에서 페이지 330-359.\n' +
      '* 코브(2021a) 카를 코베, 비네트 코사라주, 모하마드 바바리아, 마크 첸, 희우준, 루카즈 카이저, 마타시아 플라퍼트, 제리 트레오레크, 제이콥 힐튼, 레이히로 나카노 등 2021a. 수학어 문제 해결을 위한 교육 검증자 __ 훈련 검증자가 수학어 문제를 해결하기 위한 교육 검증자 __. arXiv 프리프린트 arXiv:2110.14168_.\n' +
      '* 코베 등은 (2021b) 카를 W 코베, 제이콥 힐튼, 오리브 클리모프, 존 스철만 등이 있다. 2021b. 파스릭 정책 구절. ICML_검토에서.\n' +
      '* 에마야라자 등은 (2023) 가와인 에마야라자, 위니 주, 단주라프스키, 다우웨 킬라 등이 있다. 2023년 인간 중심 손실 기능(할로스). 기술 보고서, 콘텍스트 AI.\n' +
      '* 에버트 등은 (2021) 톰 에버트, 마커스 허터, 라마나 쿠타르, 빅토리아 카락코프나 등이 있다. 강화학습의 문제 및 해결방안, 즉 인과관계 영향 다이어그램 관점 __omplete tampering 문제와 해결방안 강화학습의 해결방안. _rward tampering 문제와 해결책 : 인과관계 영향 다이어그램 관점. 합성_, 198(공급 27):6435-6467이다.\n' +
      '* Fu 등은 (2023) 요오푸, 하오펑, 아쉬쉬 사바왈, 피터 클락, 투샤르 킬이다. 2023년 복합성 기반 다단계 추론을 촉발합니다. ICLR_의 _수익률.\n' +
      '* 가오 등은 (2023) 루유 가오, 아만 마다안, 슈옌 주, 우리 알론, 펭페리 류, 유밍 양, 제이미 칼란, 그레이엄 네비그 등이다. 2023. PAL: 프로그램 유도 언어 모델. ICML_검토에서.\n' +
      '* 가오 등은 (2018) 양가오, 화즈허 주, 지린, 피셔 유, 세르게이 레빈, 트레보 다렐 등이 있다. 2018. _2018. 불완전한 시위에 의한 강화 학습. arXiv 프리프린트 arXiv:1802.05313_.\n' +
      '* 구거 등은 (2022) 실바인 구거, 리칸드르 드부트, 토마스 볼프, 필리 슈미트, 자차 뮬러, 소라브 망룰카, 마쿨카르, 벤자민 보산 등이 있다. 규모의 훈련과 추론은 간단하고 효율적이고 적응 가능하게 하였다. (https://github.com/huggingface/accelerate)\n' +
      '* 굴체레 등은 2023년 언어 모델링을 위한 Caglar Gulcehre, 톰 Le Paine, Srivatsan Srinivasan, Ksenia Konyushamethox, 롯데 위츠, Abhishek Sharma, Aditya Siddhant, 알렉스 아우르, Mosen Wang, Chenjie Gu, et al. arXiv 프리프린트 arXiv:2308.08998_.\n' +
      '* He et al. (2020) Junxian He, 지아토 구, 지준 선, 마크의 아우렐리오 라나자토. 2020년 신경 서열 생성을 위한 자체 교육을 강화하고 있다. ICLR_의 _수익률.\n' +
      '* 헨드렉스 등은 (2021) 단켄드랙스, 콜린 번스, 소라바 카다브리스, 아쿨 아로라, 스티븐 바마트, 에릭 탕, 전당송, 제이콥 슈타인하르트 등이다. 2021. 수학 데이터셋으로 수학적 문제 해결을 해결합니다. 신경 정보 처리 시스템 Datasets 및 벤치마크 트랙(Round 2)_에 관한 제5차 회의의 _검토에서.\n' +
      '* 호이 등은 (2021) 스티븐 CH호이, 도젠 사후, 징루, 피린 자오 등이 있다. 온라인 학습 : 온라인 학습 : 종합 조사. ___2021. 온라인 학습. 신경컴퓨팅_ 459:249-289.\n' +
      '* 이마니 등은 (2023) 샤마 이마니, 리앙두, 호스 시바스타바 등이 있다. 대언어 모델을 이용한 수학 추론 __수학 추론 __수학 추론. arXiv 프리프린트 arXiv:2303.05398_입니다.\n' +
      '* Jie와 루(2023) 자밍 지와 웨루. 2023. 수적 추론을 위해 몇 샷으로 훈련 데이터를 추적합니다.\n' +
      '* Jie et al.(2023) 자밍 지, 트웅 Quoc 루룽, 신장 장, 샤오란 진, 항리. 수학 문제 해결에서 항문 체인의 설계. __.2023. __ 수학 문제 해결에서 체인의 설계. arXiv 프리프린트 arXiv:2309.11054_.\n' +
      '* 쿠핫 등은 (2023) 투샤르 킬, 호스 트리베디, 마테와 핀레이슨, 야오 후, 카일 리처드슨, 피터 클락, 아쉬쉬 사바왈 등이 있다. 2023. 복잡한 작업을 해결하기 위한 모듈식 접근 방식을 촉발했다. ICLR_의 _수익률.\n' +
      '* 쿨백과 라이프블러(1951) 솔로몬 풀백, 리처드 아리블러 등이다. 1951. __1951. 정보 및 충분성. 수학적 통계_, 22(1):79-86의 기록.\n' +
      '* Le et al.(2022) 헝가리 르, Yue Wang, Akhilesh 딥크 고트마어, 실비오 사바레스, 스티븐 추 홍호이. 2022. Coderl: 전처리된 모델과 딥러닝을 통한 마스터링 코드 생성. NurIPS_의 _검토에서.\n' +
      '* 레빈 등은 (2020) 세르게이 레빈, 아비랄 쿠마르, 조지 터커, 저스틴 푸. 2020년 오프라인 강화 학습: 공개 문제에 대한 튜토리얼, 리뷰 및 관점: 개방 문제에 대한 튜토리얼, 리뷰 및 시각. __I라인 강화 학습. arXiv 프리프린트 arXiv:2005.01643_입니다.\n' +
      '* 라이트맨 등은 (2023) 헌터 라이트맨, 비네트 코사라주, 유라 버다, 하리 에드워즈, 보웬 베이커, 테디 리, 얀 레이이크, 존 셸만, 아이라이아 세이츠케버, 칼 코베 등이 있다. 단계별로 1단계 확인. __2023.2023. arXiv 프리프린트 arXiv:2305.20050_입니다.\n' +
      '* 류 등은 (2023) 빙빈 류, 세바스티엔 벙크, 로네노반, 얀가르드한 굴카리, 원히 리, 안 응우옌, 라첼워드, 이장 등이 있다. 2023. Tinygsm: 작은 언어 모델이 있는 gsm8k에서 80%의\\(\\geqslant\\)를 달성(\\geqslant\\)한다. arXiv 프리프린트 arXiv:2312.09241_.\n' +
      '* 로쉬칠로프와 후터(2017) 아이리카 로쉬칠로프와 프랑크 홉터. 기준 체중 붕괴 규칙화 __2017. 12. 체중 붕괴 규칙화. __2017. 12. 체중 붕괴 규칙화. arXiv 프리프린트 arXiv:1711.05101_입니다.\n' +
      '* 루 등은 (2023) 시우위안 루, 벤자민 반 로이, 비크란트 다워머라, 모테자 이브라히미, 이안 오스밴드, 정웬 등 2023. 기계 학습_, 16(6):733-865의 구성 및 동향(r)이다.\n' +
      '\n' +
      '해펑 루오, 칭펑선, 칸 잔고, 푸 자오, 지구강 루, 충양 도, 시부고, 칭웨이 린, 시펑 첸, 동메이 장 등이 있다. 2023. 위즈-아드민스: 강화 진화 구성을 통해 대형 언어 모델에 대한 수학적 추론을 황후한다. arXiv 프리프린트 arXiv:2308.09583_입니다.\n' +
      '* 미오 등은 (2023) 닝 미오, 예 휘 테, 톰 레인플스 등이 있다. <2023.2023> 자 확인: lms를 사용하여 제로샷을 통해 자신의 단계별 추론을 확인할 수 있습니다. __ 제로샷은 자신의 단계별 추론을 확인한다. arXiv 프리프린트 arXiv:2308.00436_입니다.\n' +
      '*노예(2021) 맥스웰나예, 안데르 요한 안드레센, 과이 구르레산, 헨리크 미칼레세키, 제이콥 오스틴, 다비드 비버, 다비드 도한, 아이터 로위즈, 마르트엔 보스마, 다비드 루안 등 2021년 작업. arXiv 프리프린트 arXiv:2112.00114_입니다.\n' +
      '* 오픈AI(2023) 오픈AI입니다. 2023. GPT-4 기술 보고서.\n' +
      '*오양(2022) 롱오우양, 제프리우, 주장, 디아고 알미다, 카롤 웨인웨이드, 파멜라 미시킨, 총 장, 샌히니 아가왈, 카타리나 슬라, 알렉스 레이 등 2022년 언어 모델을 통해 인간의 피드백을 받아 지침을 따르도록 했다. NurIPS_의 _검토에서.\n' +
      '* 파텔 등은 알(2021) 아르킬로 파텔, 사위크 바타미슈라, 나빈 고달이 있다. 2021. nlp 모델은 정말 간단한 수학 단어 문제를 해결할 수 있습니까? NAACL_의 _수익에서.\n' +
      '* 라파일로프(2023) 라파엘 라파일로프, 아치트 샤마, 에릭 미첼, 스테파노 에르몬, 크리스토퍼 디 매니닝, 첼시 핀 등이 있다. 2023. 다이렉트 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. NurIPS_의 _검토에서.\n' +
      '* 라자베나리 등은 (2020) 사미암 라자베나리, 제프 라슬리, 올라툰지 루위즈, 유시온가 등이 있다. 2020.제로: 메모리 최적화는 1조 매개변수 모델을 교육한다. _SC20: 고성능 컴퓨팅, 네트워크링, 저장 및 분석_ 국제 컨퍼런스입니다.\n' +
      '* 라슬리 등은 (2020) 제프 라슬리, 삼야암 라즈베나리, 올라툰지 루위즈, 유시온그 헤 등이 있다. 2020년 딥속: 시스템 최적화는 1,000억 개 이상의 매개변수를 가진 딥러닝 모델을 트레이닝할 수 있다. SIGKDD_의 _수익에서.\n' +
      '* 라에밀러 등은 (2018) 마르틴 러블러, 롤랑 하프너, 토마스 람페, 마이클 네베르트, 조나스 데그레브, 톰 위글, 블라디 미니, 니콜라스 헤센, 조스트 토비타스 스프링켄버그 등이다. 2018학년도에는 처음부터 희박한 보상 과제를 해결하여 학습합니다. ICML_검토에서.\n' +
      '* 로지레 등 (2023) 침례자 로지레, 조나스 게히어, 파비안 글로클, 스톤 스투클, 이타이 가트, 샤오킹 엘리탄, 요시 아디, 진유 리무, 탈 레즈, 제레미 라핀 등의 코드 오픈 기반 모델. arXiv 프리프린트 arXiv:2308.12950_입니다.\n' +
      '*삭만 등은 (2018) 존 셸만, 필리필 모리츠, 세르게이 레빈, 마이클 요르단, 피에테르 압벨 등이 있다. 2018년 일반화된 장점 추정을 이용한 고차원 연속 제어이다.\n' +
      '*삭만 등은 (2017) 존 스철만, 필리핀 웰스키, 프라풀라 다라리왈, 알레크 라드포드, 오레그 클리모프 등이다. 시동 정책 최적화 알고리즘 __2017. 시동 정책 최적화 알고리즘. __. arXiv 프리프린트 arXiv:1707.06347_입니다.\n' +
      '알렉산 아틀레키 리, 압스네르키, 알렉산드 라예스키, 아즈네르키, 아자네트 나르네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자우, 이즈네트, 베냐에, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아자르네, 아 바일치, 노아 콘스탄트, 루마니아 노바크, 로산네 류, 트리스 와르센틴, 예디칸탈, 야미니 반살, 이단 다이어, 베남 네세르, 자스차 소힐-디케슈타인, 노아 피젤 등이 있다. 2023. 인간 데이터를 넘어 언어 모델과의 문제 해결을 위한 자기 학습을 수행한다.\n' +
      '* 스칼세 등 (2022) 조르 스칼세, 니콜로우스 하네, 디트리이 크라스텐니코프, 데이비드 케에거 등이다. 2022. 보상 게임을 수정하고 특성화할 수 있습니다. NurIPS_의 _검토에서.\n' +
      '* 수튼과 바토(2018) 리처드 S 수튼과 앤드루 G 바토. 2018. _강화학습: An 도입_. MIT 언론.\n' +
      '* 테일러 등은 (2022) 로스 테일러, 마신 카다스, 구일리렘 쿠쿠렐, 토마스 인텀, 앤서니 하트셔린, 엘비스 사라비아, 앤드루 푸울턴, 비코르 케르케즈, 로버트 스토히닉 등이 있다. 과학에 대한 큰 언어 모델 __2022. 갈락티카: 과학에 대한 큰 언어 모델. __2022. 갈락티카: 과학에 대한 큰 언어 모델. arXiv 프리프린트 arXiv:2211.09085_입니다.\n' +
      '* 타우브론 등은 (2023) 허고 투브론, 루이 마틴, 케빈 스톤, 피터 알베르트, 암자드 알마헤이, 야스민 바베이이, 니콜레이 바시코프, 소우미아 바트라, 프라자왈 바하바바, 샤우티 보세일 등 2023. 로메드 채팅 모델. arXiv 프리프린트 arXiv:2307.09288_.\n' +
      '* 트로트 등은 (2019) 알렉산더 트로트, 스테판 정, 커밍 시온그, 리처드 소셔 등이 있다. 2019. 거리를 따라, 자기 균형 모양의 보상을 사용하여 희박한 보상 과제를 해결하세요. NurIPS_의 _검토에서.\n' +
      '* 우사토 등은 (2022) 제나사탄 우세사토, 네이트 쿠슈만, 라마나 쿠마르, 프란치스코 송, 노아 시겔, 리사 왕, 안토니아 크레웰, 거프리 어빙, 이리나 히기원스 등이다. 과정 및 결과 기반 피드백으로 수학 단어 문제를 해결. __2022. 프로세스 및 결과 기반 피드백으로 수학 단어 문제를 해결한다. arXiv 프리프린트 arXiv:2211.14275_.\n' +
      '* 왕(2023a) Ke왕, 허싱 르, 아오준 주, 지무 루, 시쿤 루노, 위쿠앙 시, 르루이 장, 린치 송, 명지 자한, 홍정 리가 있다. 2023a. Math-coder: 향상된 수학적 추론을 위한 llms의 무질서한 코드 통합. __ath-coder: 향상된 수학적 추론을 위한 llms의 무질서한 코드 통합. arXiv 프리프린트 arXiv:2310.03731_.\n' +
      '* 왕 등은 왕(2023b) 자에히 왕, 제이슨 웨이, 데일 슈무르만스, 퀀오 V 르, 에드 하치, 샤란 나랑, 아악카 초위헤이, 데니 저우 등이 있다. 2023b. 자기 대응성은 언어 모델에서 사고 추론의 사슬을 향상시킨다. ICLR_의 _수익률.\n' +
      '\n' +
      '제손 위, 제에히 왕, 데일 슈무만스, 마르텐 보스마, 페아샤, 에드치, 퀘오 비 르, 데니 주 등은 2022년 대형 언어 모델에서 추론을 이끌어냈다. NurIPS_의 _검토에서.\n' +
      '* Xie et al. (2020) Qizhe Xie, Minh-Thang 루룽, 에듀드 호비 및 Quoc V Le. 2020학년도, 시끄러운 학생과의 자기 교육은 상상력 분류를 향상시킵니다. CVPR_의 _수익에서 페이지 10687-10698.\n' +
      '*유 등은 (2023) 롱휘유, 웨젠장, 한시, 진청유, 조누잉리, 유장, 제임스 T커옥, 조누고 리, 아드리안 웨셀러, 웨양 류 등이 있다. 큰 언어 모델에 대한 수학 질문 __부트스트랩: 큰 언어 모델에 대해 자신의 수학적 질문을 검색한다. arXiv 프리프린트 arXiv:2309.12284_.\n' +
      '*유 등은 (2023) 샤랑유, 시웨이 Qu, 거장, 야오후, 원하오황, 후안순, 유수, 원후첸 등이다. 하이브리드 명령어 튜닝을 통한 빌딩 수학 일반주의 모델 __Mammoth: __Mammoth: 하이브리드 명령어 튜닝을 통한 빌딩 수학 일반주의 모델. arXiv 프리프린트 arXiv:2309.05653_입니다.\n' +
      '*장 등은 (2023) 풍수장, 지차오왕, 지차오양, 위치펑, 앤드루란 등이 있다. 2023년 단계 계획을 통해 해석 가능한 수학 단어 문제 해결 솔루션을 생성합니다. ACL_의 _수익에서.\n' +
      '진(2023)의 류우·위위·위위·위위·위위·위위·위위·위위·위위·위오·위오·위오·위오·위오·진주·위오·위이·위빈·명하이·명하우·명하후·청창·장장·장위·장위·장위·위산위·위앙·위앙·위앙·위앙·장·장·장·장·장·장·장·위장·장·장·장·위장·장·장·장·장·위장·장·장·장·장·장·장·장·장·장·장·위장·장·장·장·장·장·장·장·장·장·장·장·장·위장·장·장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위장·위젠젠젠젠젠젠젠젠젠젠젠젠젠젠젠젠·하앙· 2023. 대형 언어 모델 부분(i: Ppo)에서 로프의 아쉬움이 있다.\n' +
      '*중 등은 (2017) 빅토르 중, 커밍 시온그, 리처드 소셔 등을 맡았다. 강화 학습을 이용하여 자연어의 구조 질의를 설명하는 __2017. Seq2sql: 강화 학습을 사용하여 자연어의 구조 질의를 설정합니다. arXiv 프리프린트 arXiv:1709.00103_입니다.\n' +
      '* 저우(2023a) 아오준 주, 케이 왕, 지무 루, 위쑤시, 시쿤 루노, 지펑 진, 샤오킹 루, 안야 자아, 린치 송, 명지 자한 등 2023a. 코드 기반 자기 검증으로 Gpt-4 코드 인터프리터를 사용하여 도전적인 수학어 문제를 해결하는데 __gpt-4 코드 인터프리터를 사용하여 도전적인 수학어 문제를 해결한다. arXiv 프리프린트 arXiv:2308.07921_입니다.\n' +
      '* 저우(2023b) 덴니 주, 나타넬 사차리, 르 하우, 제이슨 웨이, 나스안 스칼레스, 쉬에즈 왕, 데일 슈무만스, 클레이어 쿠이, 올리비에 부케, 퀀크 르 등. 2023b. 최소한의 프롬프트는 큰 언어 모델에서 복잡한 추론을 가능하게 한다. ICLR_의 _수익률.\n' +
      '* 지글러 등은 (2019) 다니엘 미 지글러, 니산 스티논, 제프리 우, 톰 B 브라운, 알레크 라드포드, 다리오 암데이, 폴 크리스토, 거프리 어빙 등이다. 2019. __2019 식어 모델은 인간의 선호에 따른 언어 모델입니다. arXiv 프리프린트 arXiv:1909.08593_입니다.\n' +
      '* 기스타인(2020)***** 질문:* 윈은 바베키스트링의 경우 1시간 12달러를 귀착한다. 에스테르데이, 그녀는 50분 동안 바베키스트링을 했어요: 얼마를 벌었나요?\n' +
      '\n' +
      '**[N-CoT]:** 우리는 그녀의 시간당 비율을 계산한 다음 자신이 일했던 시간만큼 곱해야 한다.\n' +
      '\n' +
      '먼저 50분부터 몇 시간까지 전환해야 합니다.\n' +
      '\n' +
      '한 시간 안에 60분이 있어 50분은 50/60 = 55시간과 같다.\n' +
      '\n' +
      '다음으로, 우리는 월의 수익을 시간당 비율에 근무 시간만큼 곱하여 계산할 수 있다.\n' +
      '\n' +
      '12시간 x 5/6시간 = $10입니다.\n' +
      '\n' +
      '따라서 웡은 바베키스트링 50분 동안 10달러를 벌었습니다.\n' +
      '\n' +
      '답은 10입니다.\n' +
      '\n' +
      '**[P-CoT]:**\n' +
      '\n' +
      'def solution():\n' +
      '\n' +
      '“어제는 바베키스터링에 1시간12달러를 버텨줬어요. _예, 그녀는 50분 동안 바베키스터링을 했어요.\n' +
      '\n' +
      '시간별 = 12분\n' +
      '\n' +
      '##는 N-CoT 및 P-CoT 설명서 A 예.\n' +
      '\n' +
      '그림 6의 GSM8K 데이터세트로부터의 자연어 CoT 및 프로그램 기반 CoT의 예를 제시하며, 우리는 Jie et al.(2023)에 따라 소수의 샷 프롬프트를 수행하고 CoT 표현을 얻는다. 자연어 CoT는 일반적으로 Wei et al.(2022)에 제시된 것과 동일하다. 포맷 프로그램 기반 CoT는 우리가 문제를 해결하기 위한 기능을 사용하는 PAL(Gao et al., 2023)의 것과 유사하다. 그 기능은 질문을 반복한 다음 추론 단계로 진술 목록을 나열하는 파이썬 도스트링에서 시작한다.\n' +
      '\n' +
      '부록 B.\n' +
      '\n' +
      '우리는 배치 크기가 48이고 최대 길이가 \\(1024\\)인 40epoch에 대한 모델을 훈련시킨다. 소규모 모델의 경우 학습률을 \\(2e\\)-\\(5\\)로 높이고 MathQA\\({}_{\\text{MCQ}}\\)을 훈련시키기 위한 epoch의 수를 100 epoch로 증가시킨다.\n' +
      '\n' +
      '갈락티카의 경우 N-CoT와 P-CoT 모두에 대해 GSM8K, SVAMP에서 2epochs에 대한 평가 작업을 수행한다. MathQA\\({}_{\\text{MCQ}}\\) 측면에서 MathQA\\({}_{\\text{MCQ}}\\) N-CoT 및 MathQA\\({}_{\\text{MCQ}}}}}) P-CoT에서 5epochs에 대한 평가 작업을 수행한다. 코드LLAMA의 경우 SVAMP, GSM8K에서 2epoch, MathQA\\에서 5epochs, MathQA\\({}_{\\text{MCQ}}\\) N-CoT 및 MathQA\\({}_{\\text{MCQ}}\\) P-CoT에서 2epoch에 대한 평가 작업을 수행한다. 특히 MathQA\\({}_{\\text{numeric}}\\)에 대해 구체적으로 수행합니다.\n' +
      '\n' +
      '그림 6: N-CoT 및 P-CoT 예를 GSM8Km8K그림 6: N-CoT 및 P-CoT 예시를 그림 6: N-CoT 및 P-CoT 예이다.\n' +
      '\n' +
      '이 데이터셋이 훨씬 더 어렵고 추론 사슬의 수가 다른 데이터세트보다 더 길기 때문에 10epochs에 대한 데일리업. 작은 모델의 경우, 우리는 GSM8K 및 SVAMP의 경우 10epoch이며 MathQA\\({}_{\\text{MCQ}}\\)의 경우 40 epoch이다.\n' +
      '\n' +
      '질문에 대한 최대 길이는 300으로 설정되며 샘플링 중 최대 길이는 \\(700\\)로 설정된다. 배치 크기는 32로 값 모델의 추가 메모리 소비로 인해 SFT보다 작다. RL 단계(즉, ppo epoch)당 업데이트 횟수는 2(Ziegler et al., 2019)로 설정된다. 지글러 등(2019년)에 이어 체중 붕괴와 탈락은 사용하지 않습니다. 작은 모델의 경우 \\(3e\\)-\\(6\\)의 학습률과 256의 글로벌 배치 크기로 700epoch를 훈련합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>