<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '*1 소개\n' +
      '* 2 데이터 구축\n' +
      '	* 2.1 비전-언어 사전 학습 데이터\n' +
      '	* 2.2 감독 미세 조정 데이터\n' +
      '* 3 접근\n' +
      '	* 3.1 건축\n' +
      '	* 3.2 트레이닝 파이프라인\n' +
      '		* 3.2.1 Stage 1 : 트레이닝 비전-언어 어댑터\n' +
      '		* 3.2.2 Stage 2: Joint Vision-anguage Preraining\n' +
      '		* 3.2.3 Stage 3 : 감독 Fine-tuning\n' +
      '	* 3.3 하이퍼파라미터 및 인프라\n' +
      '* 4 평가\n' +
      '	* 4.1 공공 멀티모달 벤치마크 평가\n' +
      '	* 4.2 공용어 벤치마크 평가\n' +
      '	* 4.3 인체 평가\n' +
      '	* 4.4 절제 연구\n' +
      '* 5 결론, 한계 및 향후 작업\n' +
      '* 부록\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)(Anthropic, 2023; Google, 2023; OpenAI, 2022, 2023a)의 놀라운 성공은 언어를 넘어 여러 모달리티를 처리할 수 있는 다용도 인터페이스에 대한 수요를 촉진했다. 이러한 요구에 부응하여 GPT-4V(OpenAI, 2023b) 및 Gemini(Team et al., 2023)와 같은 대규모 멀티모달 모델(LMM)이 출현했으며, 이는 시각과 언어에 걸친 명령을 이해하고 수행할 수 있는 다재다능한 어시스턴트 역할을 한다. 이러한 모델은 복잡하고 다양한 실제 작업을 실행하는 데 상당한 가능성을 보여 더 자연스럽고 인간과 유사한 상호 작용을 가능하게 한다.\n' +
      '\n' +
      '최근 독점적 대응물과의 격차를 줄이기 위한 오픈소스 대형 멀티모달 모델이 급증하고 있다. 특히 벤치마크 성능에서는 상당한 발전이 이루어졌지만, 실제 성능 및 사용자 경험에 관해서는 대부분의 오픈 소스 모델과 최첨단 폐쇄 소스 모델(Bai et al., 2023; Bavishi et al., 2023; OpenAI, 2023b; Team et al., 2023) 간에 상당한 격차가 지속된다. 오픈 소스 커뮤니티가 실제 애플리케이션을 위한 강력한 일반 멀티모달 기능을 가진 모델을 개발하는 것은 여전히 어려운 일이다.\n' +
      '\n' +
      '가장 오픈 소스 모델과 독점 모델 간의 성능 격차는 주로 다음 이유로 인해 실제 시나리오에서 크게 두드러진다.\n' +
      '\n' +
      '* 많은 오픈 소스 솔루션들은 명령어 튜닝 단계에 상당한 비율의 계산 자원들을 할당한다. 그러나 강력한 언어 모델을 훈련한 경험은 일반 지능의 발달에서 광범위한 사전 훈련의 중요성을 강조한다. 풍부한 세계 지식을 가진 멀티모달 모델을 주입하기 위해서는 광범위한 비전 언어 데이터를 활용하는 포괄적인 사전 훈련에 중점을 두어야 한다.\n' +
      '* 일반적인 관행은 명령어 튜닝 동안 다양한 학술 데이터 세트를 병합하는 것이다. 이러한 접근법은 좋은 벤치마크 결과를 산출할 수 있지만, 종종 진정한 실제 사용 경험을 제공하는 데 부족하다.\n' +
      '* 모델 아키텍처 측면에서, 선행 작업들은 대부분 비전 트랜스포머, 전형적으로 텍스트 정렬을 사전 훈련된 언어 모델에 적응시킨다. 그러나 대부분의 모델들은 336\\(\\times\\)336 또는 448\\(\\times\\)448과 같은 비교적 낮은 해상도로 동작하며, 광학 문자 인식이나 작은 객체 식별과 같은 복잡한 실제 시나리오의 복잡성은 높은 해상도의 처리 능력을 요구한다.\n' +
      '* 일부 모델들(01-ai, 2024; Lin et al., 2023; Sun et al., 2023; Wang et al., 2023b)이 사전 트레이닝을 이용하기 시작했지만, 그들은 종종 언어 기술의 보존을 간과한다. 종종, 장기간의 멀티모달 훈련 후에 언어 능력의 저하가 있다. 두 양식 모두에서 강한 능력을 지닌 일반주의자를 지향하기 때문에 새로운 양식 능력을 개발할 때 언어 능력을 잘 보존하는 훈련 전략이 있어야 한다.\n' +
      '\n' +
      '이를 고려하여 DeepSeek 언어 모델 시리즈를 기반으로 하는 오픈 소스 대형 멀티모달 모델인 DeepSeek-VL을 제시한다. 우리는 광범위한 사전 훈련, 유스 케이스 분류에 기반한 세심한 데이터 큐레이션, 고해상도 처리를 위한 모델 아키텍처 설계 및 다중 모달리티의 균형을 맞추는 훈련 전략을 포함하는 실제 시나리오에서 능숙한 성능을 추구하는 모델을 개발한다. 이 외에도 모델 스케일링을 1B에서 7B로 조정하는 훈련 방법론을 개발한다. 이러한 포괄적인 탐색은 유사한 크기의 다른 대형 멀티모달 모델(LMM)에 비해 실제 환경에서 상당한 성능 이점을 제공한다.\n' +
      '\n' +
      '그림 1 | DeepSeek-VL은 논리 다이어그램, 웹 페이지, 공식 인식, 과학 문헌, 자연 이미지 및 복잡한 시나리오에서 체화된 지능을 처리할 수 있는 일반적인 멀티모달 이해 기능을 가지고 있다.\n' +
      '\n' +
      'DeepSeek-VL의 사전 훈련 데이터 세트는 커먼 크롤, 웹 코드, 전자책, 교육 자료 및 arXiv 기사를 포함하지만 이에 국한되지 않는 다양한 출처에서 컴파일된다. 이 컬렉션은 웹 스크린샷, PDF, OCR, 차트 및 지식 기반 콘텐츠(전문가, 교과서)와 같은 실제 시나리오를 철저히 포괄하며 확장성을 유지하면서 광범위하고 실용적인 표현을 목표로 한다.\n' +
      '\n' +
      '사전 훈련 데이터는 광범위한 세계 지식을 포함하지만 실제 사용 시나리오를 반영하기 위해 명령어 조정 데이터 세트를 세심하게 큐레이션한다. 이를 위해 GPT-4V와 Gemini에 대한 실제 테스트 케이스를 인터넷에서 수동으로 수집한다. 이러한 사례들은 종합분류법으로 체계적으로 정리되었다. 이 구조화된 분류법을 사용하여 각 테스트 이미지에 대한 프롬프트를 선택하여 실용적이고 관련 있는 명령어 튜닝 데이터 세트를 보장한다. 이 분류법은 또한 실제 성능을 효과적으로 평가하는 평가 데이터 세트를 만드는 데 사용된다.\n' +
      '\n' +
      '비주얼 모듈은 추론 비용을 효과적으로 관리하기 위해 고정된 토큰 예산 내에 유지하면서 고해상도 비주얼 입력의 활용을 최적화하도록 설계되었다. 따라서 본 논문에서는 텍스트 정렬 엔코더를 사용하여 \\(384\\times 384\\) 해상도의 거친 의미 추출과 \\(1024\\times 1024\\) 해상도의 상세한 시각 정보를 캡처하는 고해상도 엔코더를 결합한 하이브리드 비전 엔코더를 사용한다. 이 두 인코더를 융합함으로써, 제안된 하이브리드 방식은 576개의 토큰으로 \\(1024\\times 1024\\) 해상도 이미지(대부분의 경우 충분함)를 효율적으로 압축한다. 이 토큰 카운트는 풍부한 시각적 표현과 토큰 이코노미 사이의 균형을 이루어 텍스트-이미지 인터리빙 및 다중 턴 추론 시나리오 모두에 대해 실현 가능하다.\n' +
      '\n' +
      '멀티모달 모델의 사전 훈련 동안 직면하는 일반적인 과제는 훈련 과정이 시각 언어 데이터에 과도하게 의존할 때 언어 능력의 잠재적인 저하이다. 우리의 연구는 모델 내에서 언어 지식의 무결성을 보존하기 위해 상당한 비율의 언어 데이터, 특히 최소 70%를 유지하는 것이 필수적이라는 것을 보여준다. 이 균형은 언어 성능을 손상시키지 않는 강력한 멀티모달 능력을 달성하는 데 중요하다. 또한 새로운 "모달리티 워밍업" 전략을 소개합니다. 이 접근법은 훈련 중 양식의 비율을 신중하게 조정하여 점차적으로 더 많은 비전 언어 데이터를 통합한다. 준비 전략과 함께 촬영장비 비율을 신중하게 조정하면 두 촬영장비의 균형 잡힌 성능을 얻을 수 있다.\n' +
      '\n' +
      '모델에서 반복할 때 더 큰 모델 크기로 스케일링하기 전에 작은 규모로 실험을 수행한다. 그러나, 더 작은 모델, 예를 들어, 1B 모델은 벤치마크에서 합리적인 성능을 입증할 수 없으며(Schaeffer et al., 2024), 모델의 성능을 충실히 반영한다. 우리는 이것을 해결하기 위해 두 가지 접근법을 채택한다. 먼저, 옵션의 복잡성을 비교하기 위해 다중 선택에서 평가 프로토콜을 수정한다. 또한, 명령어 추종 능력이 병목 현상이 되는 것을 방지하기 위해 사전 훈련 단계에서 적은 비율의 명령어 튜닝 데이터를 혼합한다. 이러한 방법으로 1B 모델을 사용하여 합리적인 성능을 달성하고 실험 중 각 반복의 영향을 보다 정확하게 측정할 수 있다.\n' +
      '\n' +
      '일반적인 시각 및 언어 벤치마크에 대한 광범위한 평가를 통해 DeepSeek-VL 패밀리는 실제 응용 프로그램에서 우수한 사용자 경험을 보여주고 동일한 모델 크기로 광범위한 시각 언어 벤치마크에 걸쳐 최첨단 또는 경쟁적인 성능을 달성하면서도 강력한 언어 중심 성능을 유지합니다. 혁신을 촉진하고 광범위한 응용 프로그램을 가능하게 하기 위해 다양한 계산 능력의 요구를 용이하게 하기 위해 공개적으로 액세스할 수 있는 1.3B 및 7B의 두 가지 버전을 만들었습니다.\n' +
      '\n' +
      '##2 데이터 구축\n' +
      '\n' +
      '다양하고 큰 데이터 세트는 시각적 언어 모델 훈련의 가장 중요한 요소이다. 데이터 세트는 비전 언어 사전 훈련 데이터와 비전 언어 감독 미세 조정 데이터의 두 부분으로 나눌 수 있다. VL 사전 훈련 데이터는 다양한 소스의 시각적 텍스트 데이터로 구성되어 모델의 근본적인 교차 모드 이해 능력을 향상시키는 것을 목표로 하는 반면, VL 감독 미세 조정 데이터는 상대적으로 작은 크기를 가지며 특정 다운스트림 작업을 완료하도록 모델을 가르치는 것을 목표로 한다. VL 프리트레이닝 데이터는 훈련 단계 \\(1\\)에서 비전 언어 어댑터를 예열하고, 훈련 단계 \\(2\\)에서 비전 언어 모델을 공동으로 프리트레이닝하기 위해 사용되며, VL Supervised Fine-Tuning 데이터는 훈련 단계 \\(3\\), 즉 비전 언어 Supervised Fine-Tuning에서 활용된다.\n' +
      '\n' +
      '### 시각언어 사전학습 데이터\n' +
      '\n' +
      '우리 연구에 사용된 사전 훈련 데이터 세트는 독점 데이터 선택 외에도 공개적으로 액세스할 수 있는 다양한 소스를 포함한다. 우리는 표 1의 공동 시각 및 언어 사전 훈련 단계에서 사용된 데이터 소스에 대한 포괄적인 개요를 제공하며, 이러한 데이터 세트는 이미지에 묘사된 엔티티에 대한 LLM의 이해를 용이하게 할 수 있다.\n' +
      '\n' +
      '또한, 우리는 다음 범주로 구성된 전체 데이터 세트에 대한 자세한 분석을 제시한다.\n' +
      '\n' +
      '**인터리빙된 이미지-텍스트** 데이터는 모델이 다중 모달리티 입력의 인-컨텍스트 학습을 위한 더 나은 능력을 가질 수 있게 하며, 3개의 공개 데이터 세트 MMC4(Zhu et al., 2024), 위키(Burns et al., 2023) 및 위키호(Yang et al., 2021)를 활용한다.\n' +
      '\n' +
      '**이미지 캡션** 데이터는 Capsfusion(Yu et al., 2023a), TaiSu(Liu et al., 2022b) 및 Detailed Caption(echo840, 2024)의 세 가지 고품질 이미지-텍스트 쌍을 이루는 데이터 세트로부터 나온다.\n' +
      '\n' +
      '**표 및 차트** 데이터를 사용하면 모델이 일반 표 및 차트 이미지 이해 기능을 학습할 수 있습니다. 이는 Chart2text(Kantharaj et al., 2022), Geo170K(Gao et al., 2023), Unichart(Masry et al., 2023), Ureader(Ye et al., 2023), M-paper(mPLUG, 2024), ScienceQA(Lu et al., 2022b), ScreenQA(Hsiao et al., 2022), SciGraphQA-295K(Li and Tajbakhsh, 2023), Paper2figure100k(Rodriguez et al., 2023), Widget Captioning(Li et al., 2020), Screen2words(Wang et al., 2021), Refexp(Mao et al., 2016).\n' +
      '\n' +
      '**웹 코드** 데이터는 그래픽 인터페이스 또는 시각적 플롯으로부터 코드를 재구성할 수 있는 능력을 모델에 부여한다. Websight(HuggingFaceM4, 2024) for UI Inverse Rendering, we adopted a strategy of the MATCHA(Liu et al., 2022a) for visual plot inverse rendering. 여기에는 스택 데이터 세트(Kocetkov et al., 2023)에서 약 146만 개의 Jupyter 노트북의 처리가 포함되었다. 이 노트북을 추출하고 해당 이전 코드 세그먼트와 함께 모든 다이어그램을 대조함으로써 200만 쌍의 이미지와 코드가 포함된 컬렉션을 큐레이션하는 데 성공했습니다. 더 나은 데이터 품질을 위해 최소 5줄의 코드와 결합된 단일 이미지를 포함하는 110만 인스턴스를 필터링하여 기본 학습 데이터 세트를 구성한다.\n' +
      '\n' +
      '**문서 광학 문자 인식(OCR)** 데이터는 도전적인 실제 시나리오에서도 문서 레벨에서 광학 문자의 인식을 용이하게 한다. 우리가 아는 한, 현재 영어와 중국 문서를 모두 포함하는 공개적으로 사용할 수 있는 대규모 데이터 세트는 없다. 공개적으로 접근 가능한 소규모 데이터셋 Latex-OCR(Blecher, 2024)이 존재함에도 불구하고 영어와 중국어 문서 OCR 데이터셋을 추가로 구축하였으며, 1)의 두 부분으로 구성하였다. ** **arXiv 기사:** 우리는 140만 개의 arXiv 기사로부터 소스 코드를 수집하고 PDF를 컴파일했다. Nougat(Blecher et al., 2023)의 전처리 도구를 사용하여, 우리는 이 기사들을 쌍을 이루는 이미지와 텍스트로 렌더링했다; 2): **E-북 및 교육 자료:** 우리는 수백만 K-12 교육 시험 문제와 함께 Anna\'s Archive(Anna\'s Archive, 2024)로부터 860K 영어 및 180K 중국 e-북을 청소했다. 그 후, HTML 렌더링 도구(Kulkarni와 Truelsen)를 사용하여 서로 다른 템플릿을 가진 HTML 파일을 쌍으로 된 이미지와 텍스트 형식으로 변환했다.\n' +
      '\n' +
      '**장면 텍스트 OCR** 데이터는 텍스트가 환경에 통합된 이미지들로부터 텍스트를 인식하고 추출하는 모델의 능력을 증대시킨다. 상기 데이터셋은 다중으로 구성되는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c} \\hline \\hline\n' +
      '**Category** & **Dataset** & **Ratio** \\\\ \\hline Interleaved image-text & MMC4 (Zhu et al., 2024) & 15.2\\% \\\\  & Wikipedia EN\\& CN (Foundation) \\\\  & Wikihow (Yang et al., 2021) & \\\\  & in-house PDF and Epub textbooks & \\\\ \\hline Image caption & Capsfusion (Yu et al., 2023a) & 11.1\\% \\\\  & TaiSu (Liu et al., 2022b) & \\\\  & Detailed Caption (echo840, 2024) & \\\\ \\hline Table and chart & Chart2text (Kantharaj et al., 2022) & 2.1\\% \\\\  & Geo170K (Gao et al., 2023) & \\\\  & Ureader (Ye et al., 2023) & \\\\  & Unichart (Masry et al., 2023) & \\\\  & M-paper (mPLUG, 2024) & \\\\  & ScienceQA (Lu et al., 2022b) & \\\\  & ScreenQA (Hsiao et al., 2022) & \\\\  & SciGraphQA-295K (Li and Tajbakhsh, 2023) & \\\\  & Paper2figure100k (Rodriguez et al., 2023) & \\\\  & Widget Captioning (Li et al., 2020) & \\\\  & Screen2words (Wang et al., 2021) & \\\\  & Refexp (Mao et al., 2016) & \\\\ \\hline Web Code & Websight (HuggingFaceM4, 2024) & 0.4\\% \\\\  & python plots scraped from Github notebook & \\\\ \\hline Scene text OCR & ArT (Chng et al., 2019) & 1.2\\% \\\\  & MLT-17 (Nayef et al., 2017) & \\\\  & LSVT (Sun et al., 2019) & \\\\  & UberText (Zhang et al., 2017) & \\\\  & Coco-text (Veit et al., 2016) & \\\\  & RCTW-17 (Shi et al., 2017) & \\\\  & ReCTS (Zhang et al., 2019) & \\\\  & TextOCR (Singh et al., 2021) & \\\\  & OpenVINO (Krylov et al., 2021) & \\\\  & HierText (Long et al., 2022) & \\\\ \\hline Document OCR & arXiv rendered markdown & \\\\ \\hline Text-only corpus & DeepSeek-LLM 2T text copus (DeepSeek-AI, 2024) & 70.0\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 관절 비전 및 언어 사전 훈련 단계에서 사용되는 데이터셋의 요약.\n' +
      '\n' +
      '공개 데이터 세트는 ArT(Chng et al., 2019), MLT-17(Nayef et al., 2017), LSVT(Sun et al., 2019), UberText(Zhang et al., 2017), Coco-text(Veit et al., 2016), RCTW-17(Shi et al., 2017), ReCTS(Zhang et al., 2019), TextOCR(Singh et al., 2021), OpenVINO(Krylov et al., 2021) 및 HierText(Long et al., 2022)를 포함한다.\n' +
      '\n' +
      '**텍스트 전용 말뭉치**는 언어 중심 작업의 숙련도를 유지하는 역할을 한다. 본 연구에서는 DeepSeek-LLM(DeepSeek-AI, 2024)과 동일한 텍스트 코퍼스를 사용한다.\n' +
      '\n' +
      '### 감독 미세 조정 데이터\n' +
      '\n' +
      '본 연구에서 사용된 지도 미세 조정 데이터 세트는 ShareGPT4V(Chen et al., 2023), LAION-GPTV(Laion, 2023), LVIS-Instruct4V(Wang et al., 2023a), textOCR-GPT4V(Carter, 2024), LLaVA1.6-GPT4V(Liu et al., 2024a) 및 IconQA(Lu et al., 2021)와 같은 잘 알려진 오픈 소스 공유 gpt4v 데이터 세트를 포함하여 다양한 멀티모달리티 및 언어 데이터 소스를 포함한다. 또한 Ureader (Ye et al., 2023), ScreenQA (Hsiao et al., 2022), Geo170K (Gao et al., 2023), ScienceQA (Lu et al., 2022b)와 같은 사전 훈련 데이터 세트에서 추출된 부분 테이블과 차트 데이터를 통합한다. 또한, Screen-to-code(Abi, 2024) 작업에서 얻은 UI Code 데이터셋을 통합한다. 다중 모달리티 SFT 데이터의 품질을 향상시키기 위해 고품질 사내 다중 모달리티 SFT 데이터의 일부를 선별했으며 그 중 일부는 중국어로 되어 있습니다. 사내 명령어 조정 데이터 세트는 실제 사용 시나리오를 반영하고 광범위한 작업을 포함하도록 세심하게 설계되었습니다. 우리는 다양한 온라인 소스에서 GPT-4V 및 제미니에 대한 다양한 정품 테스트 케이스 세트를 수집하는 것으로 시작한다. 이러한 테스트 사례는 표 3에 자세히 설명된 바와 같이 인식, 변환, 분석, 추론, 평가 및 안전성과 같은 여러 범주를 포함하는 포괄적인 분류법으로 신중하게 분석 및 정리된다. 이러한 구조화된 분류법은 각 테스트 이미지에 대한 대표적인 프롬프트를 선택하기 위한 지침 역할을 하여 우리의 지시-튜닝 데이터 세트가 실제 애플리케이션과 모두 실용적이고 관련성이 있음을 보장한다. 또한, 이 분류법은 균형 있고 포괄적인 평가 데이터 세트를 구성하는 데 사용되며, 이를 통해 다양한 작업 및 범주에서 모델의 성능을 효과적으로 평가할 수 있다. 이러한 체계적인 접근 방식을 통해 사내 멀티모달리티 SFT 데이터가 다루는 범주가 분류법과 실제 사용 시나리오를 대표하는 것과 잘 일치하는지 확인합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Class** & **Dataset** & **Ratio** \\\\ \\hline In-house Data & SFT data based on taxonomy (Figure 3) & 10.5\\% \\\\ \\hline General Multi-modality & ShareGPT4V (Chen et al., 2023) & 35.5\\% \\\\  & LAION-GPTV (LaiON, 2023) & \\\\  & LVIS-Instruct4V (Wang et al., 2023a) & \\\\  & textOCR-GPT4V (Carter, 2024) & \\\\  & LLaVA1.6-GPT4V (Liu et al., 2024a) & \\\\  & IconQA (Lu et al., 2021) & \\\\ \\hline Table and chart & Ureader (Ye et al., 2023) & 4.1\\% \\\\  & Geo170K (Gao et al., 2023) & \\\\  & ScienceQA (Lu et al., 2022b) & \\\\ \\hline Web Code & Screen-to-code (Abi, 2024) & 2.0\\% \\\\  & ScreenQA (Hsiao et al., 2022) & \\\\ \\hline Text-only SFT & DeepSeek-LLM (DeepSeek-AI, 2024) & 47.9\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 우리의 공동 시각 및 언어 감독 미세 조정 단계에서 사용된 데이터의 요약.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Main Category & Description & Secondary Category & Tertiary Category \\\\ \\hline Recognition & This part of the use cases mainly examines the understanding and description ability of large models for image content, which does not require high knowledge reserve and reasoning ability of the model, and some tasks can be completed using traditional machine learning models. & The Description, Event/Behavire Description, Location/Scene Description, Emotion/Mood Description, Style Recognition, Food Recognition, Others \\\\ \\hline \\hline \\multirow{2}{*}{Local Description} & Pointing Description, Position Description, Person Recognition, Object Attribute Description, Logo Recognition, Counting, Currency Recognition \\\\ \\hline OCR and Transcription & Printed Text Transcription, Handwritten Text Transcription, Specified Format Transcription \\\\ \\hline \\hline \\multirow{2}{*}{Lose to Code} & UI to Code, Chart to Code, Photo to SVG/p64 Encoding, Formula to Code, Flowchart to Code \\\\ \\hline \\hline \\multirow{2}{*}{Image to Text} & Image to Prompt, Text Summary, Image-based Creation, Text Interpretation \\\\ \\hline \\hline \\multirow{2}{*}{Data Chart Analysis} & Graph Interpretation, Table Interpretation \\\\ \\hline Professional Chart Analysis & Circuit Diagram, Flowchart, Map, Music Score, Financial Chart, Flooc Plan, Others \\\\ \\hline \\hline \\multirow{2}{*}{Professional Image Analysis} & Sensor Image. Biological and Medical Image, Viewprint Image, Point Cloud Image \\\\ \\hline \\hline \\multirow{2}{*}{Encyclopedia Knowledge Analysis} & Art and Culture Knowledge, Natural Environment Knowledge, Food/Cothing/Houging/Transportation Historical Knowledge \\\\ \\hline \\hline \\multirow{2}{*}{Loseical Reasoning} & Relationship Reasoning & Interpersonal Relationship, Spatial Relationship, Size Relationship, Species Relationship \\\\ \\hline \\hline \\multirow{2}{*}{Logical Reasoning} & Hardware Function Reasoning, Software Function Reasoning \\\\ \\hline \\hline \\multirow{2}{*}{Anomaly Reasoning} & Environment State Analysis, Environment-based Behavior Reasoning, Embodied Intelligence \\\\ \\hline \\hline \\multirow{2}{*}{Humor Reasoning} & Identifying Anomalies in Images, Defect Detection, cident Judgment \\\\ \\hline \\hline \\multirow{2}{*}{Oher Commonsense Reasoning} & State Reasoning, Cause Reasoning, Attribute Comparison, Optical Illusion, Fun Games, Intention Interpretation, Behavior Prediction \\\\ \\hline \\hline \\multirow{2}{*}{Multi-graph} & This type of use case requires the model to combine the understanding of images, comprehensively use domain knowledge and logical reasoning ability to complete corresponding tasks. & Mathematical Reasoning \\\\ \\hline \\hline \\multirow{2}{*}{Evaluation} & This type of use case requires the model to evaluate the image content according to specific criteria. & - \\\\ \\hline \\hline \\multirow{2}{*}{Multi-graph} & This type of use case examines the model’s ability to analyze and understand multiple images. & Temporal Sequence Understanding - Event Prediction, Image Sequencing, Behavior Analysis \\\\ \\cline{2-2}  & Multi-graph Comparison & Attribute Comparison, Image-Text Matching, Finding Associations, Spotting Differences, Image Discrimination \\\\ \\hline \\hline \\multirow{2}{*}{Safety} & This type of use case examines the model’s performance in terms of safety. & Suggestive Questioning, Counterfactual Questioning, Prompt Injection \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 사내 SFT 데이터에 대한 우리의 분류법. 우리의 고품질 사내 다중 양식 SFT 데이터가 다루는 범주는 이 분류법에 포괄적으로 표시된다.\n' +
      '\n' +
      '또한, 딥시크-LLM(DeepSeek-AI, 2024)에 사용된 텍스트 전용 SFT 데이터를 공동 비전 및 언어 SFT 데이터의 일부로 포함한다.\n' +
      '\n' +
      '## 3 Approach\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '본 시스템은 하이브리드 비전 인코더, 비전 어댑터 및 언어 모델의 세 가지 모듈을 포함한다. 우리는 이 섹션에서 각 부분을 소개한다.\n' +
      '\n' +
      '**하이브리드 비전 인코더.** 시각 입력으로부터 높은 레벨의 의미 특징 표현을 추출하기 위해 비전 인코더로서 SigLIP를 채용한다. 그러나 우리는 단일 SigLIP 인코더가 모든 실제 질문을 포괄적으로 해결하기 위해 고군분투하는 것을 관찰한다. SigLIP를 포함하는 CLIP 패밀리의 비전 인코더는 주로 의미론적 시각적 표현을 위해 설계되지만 모호한 인코딩에 의해 도전받아, 시각적으로 구별되는 이미지가 "CLIP-블라인드 페어" 통 등으로 인해 유사한 것으로 인코딩된다(2024). 한편, CLIP 모델의 패밀리는 상대적으로 저해상도 입력(예를 들어, 224 x 224, 336 x 336, 384 x 384, 512 x 512)에 의해 제한되며, 이는 조밀한 OCR 및 시각적 접지 태스크와 같은 보다 상세한 하위 레벨 특징을 요구하는 태스크를 처리하는 능력을 방해한다.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해, 우리는 고해상도 1024 x 1024 이미지 입력을 수용하는 낮은 수준의 특징을 처리하기 위해 사전 훈련된 ViTDet(Li et al., 2022) 이미지 인코더인 SAM-B(Kirillov et al., 2023)에 기반한 비전 전용 인코더를 추가로 활용한다. SAM-B 인코더 외에도 저해상도 384 x 384 이미지 입력으로 SigLIP-L 비전 인코더를 유지한다. 결과적으로, 제안된 하이브리드 비전 인코더는 SAM-B와 SigLIP-L 인코더를 결합하여 고해상도 1024 x 1024 이미지를 효율적으로 인코딩하면서 의미 정보와 세부 정보를 모두 보존한다. 구체적으로, 고해상도 SAM-B 비전 인코더는 먼저 이미지를 1024 x 1024로 리사이징하고 64 x 64 x 256 특징 맵을 생성한다. 그런 다음 96 x 96 x 256으로 보간하고 2의 보폭을 가진 두 개의 컨볼루션 레이어를 사용하여 24 x 24 x 1024의 특징 맵을 생성하고 최종적으로 576 x 1024로 재구성한다. 저해상도 SigLIP-L 비전 인코더는 먼저 이미지를 384 x 384로 크기를 조정하고 24 x 24 x 1024 특징 맵으로 인코딩하고 최종적으로 576 x 1024로 재구성한다.\n' +
      '\n' +
      'SAM-B에 의해 생성된 64 x 64 x 256 크기의 고해상도 특징 맵의 경우, VL 어댑터는 처음에 96 x 96 x 256의 크기로 보간한다. 이어서, 2의 스트라이드를 갖는 2개의 컨볼루션 레이어를 채용하여 24 x 24 x 1024의 특징 맵을 생성하고, 576 x 1024로 재구성한다. 이와 함께, SigLIP-L에 의해 생성된 576 x 1024 크기의 저해상도 특징 맵은 고해상도 특징들과 연결됨으로써, 2048 치수의 576 개의 시각적 토큰들을 생성한다. 이러한 시각적 토큰은 높은 수준의 의미론적 시각적 인식 및 낮은 수준의 시각적 접지 작업을 향상시키는 상당한 능력을 가지고 있다. 그런 다음 GeLU 활성화를 겪고 임베딩 계층을 통해 지시되어 언어 모델과의 연결을 설정한다.\n' +
      '\n' +
      '**비전-언어 어댑터.** 비전 인코더와 LLM을 브릿지하기 위해 2-계층 하이브리드 MLP를 사용한다. 초기에, 별개의 단일-층 MLP는 고해상도 피처들 및 저해상도 피처들을 개별적으로 프로세싱하기 위해 사용된다. 이어서, 이러한 특징들은 그들의 치수를 따라 연결된 다음, MLP의 다른 층을 통해 LLM의 입력 공간으로 변환된다.\n' +
      '\n' +
      '**Language Model.** 우리의 언어 모델은 LLaMA(Touvron et al., 2023a,b)의 설계를 마이크로 디자인이 크게 따르는 DeepSeek LLM(DeepSeek-AI, 2024)을 기반으로 구축되며, RMSNorm(Zhang and Sennrich, 2019) 기능을 갖는 Pre-Norm 구조를 채택하고 Feed-Forward Network(FFN)에 대한 활성화 함수로 SwiGLU(Shazeer, 2020)를 사용하며, 중간 층 차원은 \\(\\frac{8}{3}d_{model}\\)이다. 또한 위치 인코딩을 위해 Rotary Embedding(Su et al., 2024)을 통합하고 DeepSeek-LLM과 동일한 토큰화기를 사용한다. 딥시크-VL 모델 패밀리를 소개합니다. 다중 모드 및 언어로 공동 사전 훈련을 수행하는 목적을 고려하여 딥시크의 사전 훈련 모델에서 중간 체크포인트를 선택하여 사전 훈련을 계속한다.\n' +
      '\n' +
      '구체적으로 DeepSeek-VL-1B 모델은 DeekSeek-LLM-1B 모델을 기반으로 구축되는데, DeekSeek-LLM-1B 모델은 대략 5천억 개의 텍스트 토큰의 말뭉치로 훈련을 받았다. 그리고 DeekSeek-VL-7B 모델은 2조 개의 텍스트 토큰으로 학습된 DeepSeek-LLM-7B 모델을 활용하여 개발된다.\n' +
      '\n' +
      '도 2 | 시각화 결과. DeepSeek-VL은 작은 물체를 포착하고 조직적인 설명을 할 수 있다.\n' +
      '\n' +
      '### Training Pipelines\n' +
      '\n' +
      '우리는 그림 3과 같이 딥시크-VL을 비전 언어 어댑터 워밍업, 공동 비전 언어 사전 훈련 및 감독 미세 조정의 세 가지 연속 단계로 훈련한다. 현재 시각적 이해 능력에 초점을 맞추고 언어 부분에 대한 다음 토큰 예측 손실만 계산합니다.\n' +
      '\n' +
      '1단계 : 훈련 비전-언어 어댑터\n' +
      '\n' +
      '이 단계의 주요 목적은 임베딩 공간 내에서 시각적 요소와 언어적 요소 사이의 개념적 연결을 확립함으로써 LLM(Large Language Model)에 의해 이미지 내의 묘사된 엔티티에 대한 포괄적인 이해를 촉진하는 것이다. LLaVA(Liu et al., 2024) 및 Instruct-BLIP(Dai et al., 2023)에 의해 수행된 이전 연구와 일관되게, 우리는 비전 인코더와 LLM이 모두 이 단계 동안 동결된 채로 유지하면서, 비전 언어 어댑터 내에서 훈련 가능한 파라미터를 단독으로 허용하는 유사한 접근법을 채택한다. 우리는 VL 어댑터를 훈련하기 위해 250만 문서 OCR 렌더링 쌍과 함께 ShareGPT4V에서 얻은 125만 개의 이미지 텍스트 쌍 캡션을 포함하는 데이터 세트를 활용한다.\n' +
      '\n' +
      '그럼에도 불구하고, LLM(Large Language Models)에 비해, 비전-언어 어댑터(예를 들어, 2-레이어 MLP)는 상당히 더 작은 파라미터 용량을 갖는다. 이러한 모델 용량의 제한은 이 단계에서 학습할 수 있는 능력을 제한한다. 자연스러운 질문이 발생한다: **데이터 스케일링의 법칙이 이 단계에서 효과적일 수 있는가?** 이 질문을 해결하기 위해 표 8의 간단한 실험을 수행했다. 결과는 이 단계에서 데이터 스케일을 확장하는 것이 이점을 제공하지 않고 심지어 성능 저하로 이어질 수 있음을 보여준다. 결과적으로, 우리는 2단계 동안 대규모 언어 모델(LLM)을 동결 해제하고 효율적인 비전 언어 사전 훈련 접근법을 조사한다.\n' +
      '\n' +
      '2단계 : Joint Vision-anguage Preraining\n' +
      '\n' +
      '이 단계에서는 대규모 언어 모델(LLM)이 멀티모달 입력을 이해할 수 있는 추가 단계로 간주할 수 있는 효과적인 사전 훈련 전략을 탐색한다. 비젼 인코더를 냉동 상태로 유지하고 언어 모델과 VL 어댑터를 최적화합니다.\n' +
      '\n' +
      '처음에 우리는 멀티모달 데이터로 LLM을 직접 훈련시키려고 시도한다. 그러나, 우리는 멀티모달 성능에 대한 메트릭이 점진적으로 개선되었지만, 그림 4(멀티모달:언어=100%:0%)에 예시된 바와 같이 언어 메트릭이 극명하고 심각하게 감소한다는 것을 발견했다. 이것은 LLM을 기반으로 하는 다중 모드 사전 훈련을 직접 수행하는 데 내재된 도전을 강조하며, 다중 모드 능력을 향상시키는 것과 언어 능력을 보존하는 것 사이의 중요한 균형을 드러낸다.\n' +
      '\n' +
      '우리는 관찰된 현상이 두 가지 주요 요인에서 비롯된다고 가정한다: 첫째, 다중 모드 말뭉치의 대부분은 지나치게 단순하며 언어 데이터의 복잡성과 분포로부터 상당한 차이를 나타낸다. 둘째, 멀티모달과 언어 양식 사이에는 경쟁적인 역학 관계가 있는 것으로 보이며, 이는 LLM 내에서 언어 능력의 치명적인 망각으로 설명될 수 있다.\n' +
      '\n' +
      '**공동 언어-복합 훈련** 이 과제를 해결하기 위해, 우리는 간단하면서도 효과적인 공동 언어-복합 훈련 전략을 고안한다. 훈련 중에, 우리는 멀티모달 데이터 훈련에 참여할 뿐만 아니라 많은 비율의 언어 데이터를 훈련에 통합한다. 이 접근법은 관찰된 부작용을 완화하면서 훈련 초점의 균형을 맞추는 것을 목표로 한다. 촬영물 혼합 비율의 변화에 따른 영향을 조사하기 위해 그림 4의 DeepSeek-VL 1B 모델에 대한 실험을 수행한다.\n' +
      '\n' +
      '그래프의 분석은 몇 가지 핵심 결론을 도출한다: (1). 언어 데이터를 통합하면 언어 능력의 저하가 크게 완화되어 모델의 언어 성능이 크게 향상되었음을 보여준다. (2). 언어 데이터의 포함은 멀티모달 성능에 큰 손실을 초래하지 않으며, 이는 모델이 멀티모달 처리 능력을 보유함을 나타낸다. (3). 서로 다른 모달리티의 성능은 훈련 데이터 세트의 각 비율과 강한 상관관계가 있어 두 모달리티 간의 경쟁 관계를 입증한다. 궁극적으로, 우리는 최종 모델에 대해 대략 7:3의 멀티모달 데이터에 대한 언어의 훈련 비율을 선택한다. 이 비율은 모델이 언어 능력을 유지하면서 동시에 멀티모달 데이터에 대한 더 나은 사전 훈련을 달성하여 언어 및 멀티모달 효율성 모두의 개발을 효과적으로 균형 있게 할 수 있도록 한다.\n' +
      '\n' +
      '**스케일링 비전-언어 프리트레이닝** 그럼에도 불구하고, 모델의 프리트레이닝 단계는 상당한 계산 비용을 초래하고, 7B 모델에 대한 반복을 수행하는 것은 과도한 양의 컴퓨팅 파워 및 시간을 요구한다. 한 가지 적절한 전략은 더 작은 모델, 특히 1.3B 모델에 대한 실험을 수행한 다음 7B 모델까지 확장하는 것이다. 다행히도, 우리는 1.3B 모델에서 얻은 결과의 상당 부분이 SFT 활용(예: 인코더 설계)을 통해 7B 모델로 효과적으로 전달될 수 있음을 관찰했다. 그러나 2단계 훈련 단계에서는 1.3B 모델의 생성 메트릭에 상당한 변동이 발생하여 훈련 프로세스를 효과적으로 감독하기가 어렵다. 그리고 Schaeffer et al. (2024)에서 "모델 패밀리의 토큰당 오류율이 규모가 증가함에 따라 원활하고 지속적이며 예측 가능하게 변화함에도 불구하고 연구자의 측정 선택에 의해 날카롭고 예측할 수 없는 변화가 유발될 수 있다." 후속 실험으로 인해 이 문제의 근본 원인을 식별하게 되었다. 즉, 1.3B 모델의 제한된 용량과 훈련 데이터 세트 내의 SFT 데이터의 부재는 두 모델 모두 지침을 정확하게 따르는 능력을 방해한다. 모델이 올바른 옵션에 대한 지식을 가지고 있을 때에도 정확하게 생성하려고 애쓴다.\n' +
      '\n' +
      '이러한 문제를 완화하기 위해 이중 갈래의 접근법을 채택한다. 먼저, 다중 선택 PPL 방법론을 사용하여 모델의 진행 상황을 모니터링한다. 이것은 프롬프트와 이미지를 네트워크에 입력하는 것뿐만 아니라 질문과 관련된 모든 답변을 입력하는 것을 포함한다. 이어서, 각 답변 위치(예: A, B, C, D)에 대한 PPL을 계산하고 모델에 의해 정답으로 간주되는 옵션을 최종 답변으로 선택한다. 둘째, SFT 데이터를 최소 비율로 학습 데이터 세트에 도입하여 모델이 다음 지침에 대한 약간의 숙련도를 얻을 수 있도록 한다. 이 두 가지 접근법의 조합은 1.3B 모델에 대한 안정적인 훈련 메트릭의 유지를 보장하고 스테이지 3 이후에 더 나은 성능을 가져온다.\n' +
      '\n' +
      'IMT2000 3GPP - 3단계 : 감독 미세조정\n' +
      '\n' +
      '이 단계에서는 명령어 기반 미세 조정으로 사전 훈련된 DeepSeek-VL 모델을 미세 조정하여 명령어를 따르고 대화에 참여하는 능력을 강화하여 대화형 DeepSeek-VL-Chat 모델을 만드는 데 최고조에 달한다. 언어 모델, VL 어댑터 및\n' +
      '\n' +
      '그림 4: 서로 다른 촬영장비 융합비에 대한 비교 성능 결과. 멀티모달 데이터의 지나치게 많은 비율(멀티모달:언어=100%:0%)은 LLM의 언어 능력을 상당히 잊게 한다. 적절한 비율(멀티모달:언어=70%:30%)은 언어 망각의 문제를 효과적으로 완화하면서 동시에 모델의 멀티모달 능력을 향상시킬 수 있다.\n' +
      '\n' +
      '표 2와 같이 비전 언어 SFT 데이터를 갖는 하이브리드 비전 인코더는 제한된 GPU 메모리로 인해 SAM-B가 동결된 상태로 유지된다. 우리는 답변과 특수 토큰만 감독하고 시스템과 사용자 프롬프트를 마스킹합니다. 대화에 대한 모델의 포괄적인 숙련도를 보장하기 위해 DeepSeek-LLM에서 사용되는 멀티모달 데이터와 순수 텍스트 대화 데이터의 혼합을 활용한다. 이 접근법은 다양한 대화 시나리오에 걸쳐 모델의 다양성을 보장한다.\n' +
      '\n' +
      '### 하이퍼파라미터 및 인프라\n' +
      '\n' +
      '모든 단계의 세부 하이퍼파라미터는 표 4에 설명되어 있으며, 경량적이고 효율적인 분산 훈련 프레임워크인 HAI-LLM(High-flyer, 2023)으로 DeepSeek-VL을 훈련하고 평가한다. 시각적 인코더를 사용하여 이미지를 임베딩 벡터로 변환하고 이미지 임베딩과 텍스트 임베딩을 균일하게 처리하기 때문에 파이프라인 병렬성을 VL 모델 학습에 쉽게 적용할 수 있다. 우리는 시각적 인코더와 텍스트 임베딩을 단일 모듈로 보고 결과 모델의 첫 번째 레이어로 받아들이기만 하면 된다. 이 첫 번째 레이어는 복잡한 모델 구조를 가지며 표준 텐서 병렬 기법을 배제하지만, 다행히 상위 표준 변압기 블록에 비해 상대적으로 적은 연산량을 요구한다. 따라서 우리는 모든 텐서 병렬 순위에서 시각적 인코더 순방향 패스를 다시 계산한다. 또한 비주얼 인코더의 존재는 모델 레이어에 걸쳐 불균일한 실행 시간을 초래하므로, 더 나은 부하 균형과 처리량을 달성하기 위해 파이프라인 병렬 순위 사이에 모델 레이어를 다시 분할한다. DeepSeek-VL의 상층은 DeepSeek-LLM의 상층과 정확히 동일하다. 이러한 사소한 수정으로 우리는 이제 메가트론(Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019)에서와 같은 표준 3D 병렬화 기술을 수행할 수 있고 DeepSeek-LLM(DeepSeek-AI, 2024)에서와 같은 중첩 계산 및 통신을 수행할 수 있다. DeepSeek-VL-7B는 각각 8개의 카드를 포함하는 64개의 Nvidia A100 처리 장치 군집에서 5일을 소비한 반면, DeepSeek-VL-1B는 16개의 Nvidia A100 처리 장치를 포함하는 설정에서 7일을 소비했다.\n' +
      '\n' +
      '도 5: 시각화 결과. DeepSeek-VL은 파이썬 코드를 이해하고 상세하고 체계적인 설명을 제공할 수 있다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '### 공공 멀티모달 벤치마크 평가\n' +
      '\n' +
      '우리는 일련의 공개 벤치마크에 대해 우리의 모델을 평가한다:\n' +
      '\n' +
      '**멀티모달 종합 이해** 데이터세트: MMMU(Yue et al., 2023), CMMMU(Zhang et al., 2024), MMBench(Liu et al., 2023a), MMBench-CN(Liu et al., 2023a), SeedBench(Li et al., 2023a) 및 MMV(Yu et al., 2023b). 현재 공식 테스트 다운로드 링크가 더 이상 활성화되지 않기 때문에 딥시크-VL을 MMB/MMC-dev의 경쟁업체와 비교한다.\n' +
      '\n' +
      '**차트/테이블 이해** 데이터셋: OCRBench(Liu et al., 2023b);\n' +
      '\n' +
      '**Hallucination** 데이터셋: POPE(Li et al., 2023b);\n' +
      '\n' +
      '**Scientific problem** 데이터셋: ScienceQA(Lu et al., 2022a) and MathVista(Lu et al., 2023).\n' +
      '\n' +
      '본 논문에서는 그리디 디코딩을 이용한 세대 기반 평가를 적용한다. 여기서 생성 기반 평가는 모델이 자유 텍스트를 생성하도록 하고 생성된 텍스트로부터 파싱 결과를 생성하는 것을 의미한다. 표 5에 예시된 바와 같이 비교 결과는 DeepSeek-VL-7B가 광범위한 벤치마크에서 유사한 크기의 대부분의 오픈 소스 모델을 능가한다는 것을 보여준다.\n' +
      '\n' +
      'DeepSeek-VL은 MMB, MMC 및 SEEDbench와 같은 벤치마크에서 유사한 크기의 오픈 소스 모델보다 우수하며, 심지어 독점 모델에 접근한다(DeepSeek-VL 대 GPT-4V = 70.4 대 71.6 on seedbench). 강력한 자연 이미지 이해 능력을 보여줍니다. 이 모델은 또한 수학적 논리에서 모든 오픈 소스 모델을 능가하지만 여전히 GPT-4V(매스비스타의 36.1 대 47.8)와 같은 독점 모델에 크게 뒤쳐져 있다. 이 차이는 기본 모델 크기의 분산에 기인할 수 있다.\n' +
      '\n' +
      '또한 표 6에 나타난 바와 같이 DeepSeek-VL-1.3B는 비슷한 크기의 모델보다 상당히 우수하다. MMB 벤치마크 테스트에서 선도적인 오픈 소스 모델에 비해 우수한 성능을 보여주며, 매개변수의 절반에 가까운(1.3B 대 2.7B)만 활용함으로써 강력한 자연 이미지 이해 능력을 나타낸다. DeepSeek-VL-1.3B는 심지어 MathVista에서 7B 오픈 소스 모델과 유사한 결과를 달성하여 DeepSeek-VL 패밀리의 강력한 논리적 이해 능력을 추가로 검증한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c} \\hline \\hline  & \\multicolumn{3}{c}{DeepSeek-VL 1B} & \\multicolumn{3}{c}{DeepSeek-VL 7B} \\\\ Vision Encoder & \\multicolumn{3}{c}{SigLIP} & \\multicolumn{3}{c}{SigLIP+SAM} \\\\ \\hline\n' +
      '**Hyperparameters** & **Stage 1** & **Stage 2** & **Stage 3** & **Stage 1** & **Stage 2** & **Stage 3** \\\\ \\hline Learning rate & \\(1.0\\times 10^{-3}\\) & \\(3\\times 10^{-5}\\) & \\(2.0\\times 10^{-5}\\) & \\(1.0\\times 10^{-3}\\) & \\(4.2\\times 10^{-5}\\) & \\(2.0\\times 10^{-5}\\) \\\\ LR scheduler & Cosine & Step & Cosine & Cosine & Step & Cosine \\\\ Weight decay & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\ Gradient clip & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\\\ Optimizer & AdamW(\\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\)) & AdamW(\\(\\beta_{1}=0.9\\), \\(\\beta_{2}=0.95\\)) & \\\\ Warm-up steps & 128 & 2000 & 256 & 128 & 2000 & 256 \\\\ Training steps & 15000 & 96000 & 10000 & 15000 & 42000 & 10000 \\\\ Batch size & 256 & 1024 & 256 & 256 & 2304 & 256 \\\\ Sequence length & 512 & 4096 & 4096 & 512 & 4096 & 4096 \\\\ Sequence packing & \\(\\times\\) & \\(\\checkmark\\) & \\(\\times\\) & \\(\\times\\) & \\(\\checkmark\\) & \\(\\times\\) \\\\ Pipeline parallelism & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: DeepSeek-VL의 상세 하이퍼파라미터.\n' +
      '\n' +
      '### 공용어 벤치마크 평가\n' +
      '\n' +
      '다음 공용 언어 벤치마크에 대한 모델을 평가합니다.\n' +
      '\n' +
      '**MLU(Hendrycks et al., 2020)를 포함하는 다중-대상 다중-선택** 데이터세트.\n' +
      '\n' +
      '**언어 이해 및 추론** HellaSwag(Zellers et al., 2019)를 포함한 데이터셋들.\n' +
      '\n' +
      '**언어 모델링** 파일(Gao et al., 2020)을 포함하는 데이터셋들.\n' +
      '\n' +
      '**Math** 데이터 셋은 GSM8K(Cobbe et al., 2021)를 포함한다.\n' +
      '\n' +
      '**Code** MBPP(Austin et al., 2021)를 포함하는 데이터셋들.\n' +
      '\n' +
      '** AGIEval(Zhong et al., 2023)을 포함하는 표준화 시험**.\n' +
      '\n' +
      '우리는 여러 옵션에서 답을 선택해야 하는 데이터 세트에 복잡성 기반 평가를 적용한다. 이러한 데이터 세트에는 헬라 스웨그 및 MMLU가 포함된다. 여기서의 복잡도 기반 평가는 각 옵션의 복잡도를 계산하여 가장 낮은 것을 선택으로 하는 것을 말한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline  & LLM & MMMU & COMMU & MMB & MMC & SEED & OCRB & POPE & MathV & MMVet \\\\ \\hline \\hline\n' +
      '**Tiny Model**: & & & & & & & & & & \\\\ MobileVLM & 1.4B & - & - & 53.2 & - & - & - & 84.5 & - & - \\\\ MobileVLM & 2.7B & - & - & 59.6 & - & - & - & 84.9 & - & - \\\\ MobileVLM V2 & 1.4B & - & - & 59.6 & - & - & - & 84.3 & - & - \\\\ MobileVLM V2 & 2.7B & - & - & 63.2 & - & - & - & 84.7 & - & - \\\\ LLaVA-Phi & 2.7B & - & - & 59.5 & - & - & - & 85.0 & - & 28.9 \\\\ \\hline DeepSeek-VL (ours) & 1.3B & 32.2 & 27.4 & **64.6** & 61.3 & 66.7 & 409 & **87.6** & 31.1 & **34.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 소형 멀티모달 모델 간의 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline  & LLM & MMMU & COMMU & MMB & MMC & SEED & OCRB & POPE & MathV & MMVet \\\\ \\hline \\hline\n' +
      '**Close-source LMMs**: & & & & & & & & & & \\\\ Gemini Pro & Unk & 48.9 & - & 75.2 & 74.0 & 70.7 & 659 & - & 45.2 & 59.2 \\\\ GPT-4V & Unk & 56.8 & 42.5 & 75.0 & 74.7 & 71.6 & 659 & - & 47.8 & 49.9 \\\\ Qwen-VL-Plus & Unk & 45.2 & 39.5 & 66.2 & 69.6 & 72.7 & - & - & 43.3 & 55.7 \\\\ Qwen-VL-MAX & Unk & 51.4 & - & 78.1 & 76.4 & 72.7 & - & - & 51.0 & 61.8 \\\\ \\hline \\hline\n' +
      '**Open-source 13B LMMs**: & & & & & & & & & & \\\\ LLaVA-1.5 & 13B & 36.4 & - & 68.2 & 61.9 & 68.2 & 331 & 85.9 & 26.4 & 38.3 \\\\ VILA & 13B & - & - & 70.3 & 64.3 & - & - & 84.2 & - & 38.8 \\\\ LLaVA-Next & 13B & 36.2 & - & 70.0 & 64.4 & 71.9 & - & 86.7 & 35.3 & 48.4 \\\\ \\hline \\hline\n' +
      '**Open-source 7B LMMs**: & & & & & & & & & & \\\\ EMU2-Chat & 7B & 36.3 & 23.8 & 63.6 & 45.9 & 68.9 & - & - & 30.0 & 31.0 \\\\ Qwen-VL-Chat & 7B & 37.0 & - & 60.6 & 56.7 & 64.8 & - & - & 33.8 & 47.3 \\\\ CogVLM & 7B & 37.3 & 24.8 & 63.7 & 53.8 & 68.8 & - & - & 34.7 & **54.5** \\\\ LLaVA-Next & 7B & 35.8 & - & 67.4 & 60.0 & 70.2 & - & 86.5 & 34.6 & 43.9 \\\\ Yi-VL & 6B & **37.8** & 35.8 & 68.2 & 68.9 & 67.6 & - & - & 28.0 & 31.1 \\\\ \\hline DeepSeek-VL (ours) & 7B & 36.6 & **37.9** & **73.2** & **72.8** & **70.4** & 456 & **88.1** & **36.1** & 41.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 상이한 멀티모달 모델들 간의 비교. 상위 절반은 독점 모델이고 하위 절반은 오픈 소스 모델이다.\n' +
      '\n' +
      '모델 예측. 복잡도 기반 평가는 모델 예측 간의 미묘한 확률 차이를 구별하고 정확한 일치 스타일 평가의 불연속성을 방지하는 데 도움이 된다. GSM8K와 AGIEval에 대해 그리디 디코딩을 적용한 세대 기반 평가를 적용한다. 여기서 생성 기반 평가는 모델이 자유 텍스트를 생성하도록 하고 생성된 텍스트로부터 파싱 결과를 생성하는 것을 의미한다. 테스트 코퍼스에서 바이트당 비트 수를 계산하는 것을 의미하는 파일 테스트를 위해 언어 모델링 기반 평가를 적용한다. 그리고 결과는 표 7에 예시되어 있다\n' +
      '\n' +
      '대부분의 언어 벤치마크에서 DeepSeek-VL은 DeepSeek-7B와 비교하거나 능가하는 성능을 수행하는 것으로 관찰될 수 있다. 예를 들어, 68.4 대 68.4의 점수를 얻습니다. 일반적인 언어 능력을 평가하기 위한 일반적인 벤치마크 역할을 하는 HellaSwag에 대한 68.5 DeepSeek-VL은 MMLU 및 AGIEval과 같은 메트릭에서 DeepSeek-7B를 능가하여 멀티모달 트레이닝 방법이 언어 작업에도 도움이 될 수 있음을 나타낸다. 그럼에도 불구하고 DeepSeek-VL-7B는 수학의 어느 정도 감소(GSM8K)를 보이는데, 이는 시각과 언어 양식의 조화를 촉진하려는 노력에도 불구하고 여전히 이들 사이에 경쟁 관계가 존재함을 시사한다. 이는 제한된 모델 용량(7B)에 기인할 수 있으며 더 큰 모델이 이 문제를 크게 완화할 수 있다. 전반적으로 DeepSeek-VL은 이러한 문제를 해결하면서 언어 능력의 감소를 최소화하는 목표를 달성하기 위해 노력한다.\n' +
      '\n' +
      '### Human Evaluation\n' +
      '\n' +
      'DeepSeek-VL의 기능을 추가로 탐색하기 위해 수동 평가를 위한 데이터 세트를 독립적으로 구성한다. 이 데이터 세트는 각각 특정 작업을 포함하는 7개의 범주로 분할된 100개의 질문으로 구성된다. 이러한 범주 및 작업은 표 3과 같이 사내 SFT 데이터에 대한 분류와 동일합니다. 이 접근 방식은 테스트하는 작업이 보편적이며 다중 모드 모델에 대한 대부분의 사용 사례를 포함한다는 것을 보장한다.\n' +
      '\n' +
      '또한 기존 보고서에 기술된 범주 및 작업을 기반으로 유사한 이미지 자료를 수집하고 프롬프트를 개발했다. 이러한 이미지 자료의 출처에는 로열티가 없는 이미지 커뮤니티와 연구진이 촬영한 사진이 포함된다. 이 체계적인 수집 및 신속한 공식화 프로세스는 데이터 세트가 포괄적이고 실제 멀티모달 모델 애플리케이션을 대표한다는 것을 보장한다.\n' +
      '\n' +
      '우리는 그림 6과 같이 DeepSeek-VL 7B와 InternLM2-VL, CogVLM 및 GPT-4V를 비교한다(그리고 부록 A에서도 시각화 결과를 제공한다). GPT-4V는 대부분의 차원에 걸쳐 탁월한 성능을 보여줍니다. 모든 오픈 소스 모델은 논리적 추론에서 여전히 GPT-4V보다 훨씬 뒤처져 있어 대규모 언어 모델(LLM)의 크기를 확장할 필요성을 강조한다. DeepSeek-VL 7B는 인식, 변환 및 상식 추론에서 GPT-4V에 가까운 결과에 도달하여 전체 성능에서 더 나은 결과를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline \\hline  & Version & \\begin{tabular}{c} DeepSeek-VL \\\\ 1B Chat \\\\ \\end{tabular} & \\begin{tabular}{c} DeepSeek-VL \\\\ 7B Chat \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} DeepSeek-LLM \\\\ 7B Chat \\\\ \\end{tabular} \\\\  & Encoder & SigLIP & SigLIP+SAM & None \\\\ \\hline \\multirow{5}{*}{Benchmark} & HellaSwag & 56.0 & 68.4 & **68.5** \\\\  & MMLU & 32.5 & **52.4** & 49.4 \\\\  & GSM8K & 18.0 & 55.0 & **63.0** \\\\  & MBPP & 10.0 & **35.2** & **35.2** \\\\  & AGIEval & 14.0 & **27.8** & 19.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: | 언어 벤치마크에 대한 성능.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '**스케일업 프로젝터 트레이닝** 단계 1(프로젝터 워밍업)에 대한 데이터 세트를 확장한 후 감독 미세 조정을 적용한다. 그림 8에 표시된 결과는 훈련 데이터 볼륨을 증가시키는 것이 이 단계에서 성능을 향상시키지 않는다는 것을 보여준다. 이는 프로젝터의 용량이 본질적으로 제한되어 있어 멀티모달 작업에 필요한 광범위한 지식을 포착할 수 없음을 의미한다.\n' +
      '\n' +
      '**훈련 단계** 표 9에서 각 단계가 모델의 성능에 미치는 기여도를 살펴본다. 단계 1, 단계 2 및 단계 3을 결합하는 것이 단계 1과 단계 3만을 결합하는 것에 비해 모든 메트릭에서 상당히 더 나은 결과를 산출한다는 것이 명백하여 멀티모달 사전 훈련의 효과를 입증한다. 추가적으로 2단계와 3단계의 결합은 여전히 1단계, 2단계, 3단계의 결합 성능에 약간 뒤처져 있어 비전 언어 적응기 준비 단계가 의미 있게 남아 있음을 알 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Stage 1, Training Step & MMB & MMC & SEED & POPE & MMMU & Average \\\\ \\hline\n' +
      '2K & **59.0** & 54.0 & **61.8** & 82.3 & **30.3** & *57.5**\n' +
      '8K & 58.0 & 45.0 & 58.5 & **84.9** & 29.2 & 55.1\\\\\n' +
      '20K & 56.0 & 52.3 & 59.0 & 81.7 & 28.6 & 55.5\\\\\n' +
      '80K & 58.1 & **55.0** & 58.6 & 78.6 & 27.9 & 55.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 스케일링 업 스테이지 1 데이터에 대한 직접 SFT 성능 결과들을 비교한다. 결과는 이 단계에서 데이터 규모를 확장하는 것이 이점을 얻지 못하거나 심지어 더 나쁜 성능을 초래한다는 것을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline Stage 1 & Stage 2 & Stage 3 & MMB & MMC & SEED & POPE & MMMU & Average \\\\ \\hline ✓ & & ✓ & 59.4 & 54.2 & 61.4 & 82.5 & 29.2 & 57.4 \\\\  & ✓ & ✓ & 63.4 & 60.5 & 65.9 & 87.1 & 31.8 & 61.7 \\\\ ✓ & ✓ & ✓ & 64.3 & 61.3 & 66.7 & 87.6 & 32.2 & **62.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 훈련 단계에 걸친 모델 성능 분석.\n' +
      '\n' +
      '도 6: InternLM2-VL, CogVLM, DeepSeek-VL 및 GPT-4V에 대한 인간 평가 결과.\n' +
      '\n' +
      '**모달 그룹 훈련** 언어와 멀티모달 데이터를 혼합할 때 배치 수준에서 직접 혼합하면 훈련 효율이 크게 감소한다는 것을 관찰한다. 이러한 비효율성은 각 배치 구배 역전파 프로세스가 가장 느린 샘플이 완료되기를 기다리기 때문에 발생한다. 결과적으로, 주로 더 빠르게 처리되는 순수 언어 데이터는 멀티모달 샘플이 완료되기를 기다리며, 전체 트레이닝 효율의 감소로 이어진다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 각 글로벌 단계에서 서로 다른 데이터 양식을 그룹화하고 별개의 양식을 별도로 샘플링하는 실험을 수행한다. 이 접근법은 배치가 동일한 배치 내에서 혼합되는 것이 아니라, 상이한 훈련 단계에서 전적으로 언어 데이터 또는 전적으로 멀티모달 데이터로 구성되도록 훈련 데이터를 조직화하는 것을 포함한다.\n' +
      '\n' +
      '결과는 그림 7에 나와 있으며, 이 방법이 모델의 훈련 효율성을 20% 향상시키면서 모델의 성능을 손상시키지 않는다는 것을 관찰한다. 이 전략은 양식 간의 서로 다른 처리 시간으로 인한 병목 현상을 효과적으로 우회하여 교육 워크플로우를 최적화합니다.\n' +
      '\n' +
      '**모달리티 웜업** 우리의 접근 방식이 언어 모델의 기초에 대한 멀티모달 훈련을 포함한다는 점을 고려하면, 처음부터 고정된 비율로 멀티모달 데이터를 직접 혼합하면 모델을 불안정하게 만들 수 있다. 이 문제에 대응하기 위해 간단하면서도 효과적인 촬영 준비 전략을 제안합니다. 초기에는 언어 데이터 비율을 1로 설정한 다음 최종 모델 학습을 위한 목표 비율(예: 0.7)로 점진적으로 줄였다.\n' +
      '\n' +
      '그림 8에 도시된 바와 같이 우리의 실험은 이 전략이 훈련 초기에 언어 능력의 현저한 감소를 효과적으로 방지하는 동시에 언어 및 멀티모달 도메인 모두에 대해 최종 단계에서 비교적 우수한 결과를 산출한다는 것을 보여준다. 이러한 점진적 적응은 모델이 멀티모달 데이터의 통합에 보다 원활하게 적응할 수 있게 하여 전반적인 훈련 안정성과 성능을 향상시킨다.\n' +
      '\n' +
      '**비전 인코더 선택** 영상 정보를 더 잘 획득하고 활용하기 위해, 우리는 효율성을 위해 2단계의 훈련 단계를 8000 단계로 줄이는 것을 제외하고, 우리의 훈련 설정 하에서 상이한 비전 인코더의 훈련 손실을 비교한다. 그림 9에 예시된 바와 같이, 비전 전용 자가 감독 인코더의 통합은 훈련 손실에 대한 성능을 상당히 향상시키는 것으로 밝혀졌다. 본 연구에서는 고해상도 영상을 보다 효과적으로 처리하기 위해 하이브리드 비전 인코더(Hybrid Vision Encoder) 전략을 채택하여 SigLIP와 SAM을 결합한 모델을 구현하였다.\n' +
      '\n' +
      '**비전-언어 적응기 설계** 현재의 토큰 길이 제약 조건을 준수하면서 비주얼 인코더로부터 정보를 추출하는 효율을 향상시키기 위해, 조정들이 이루어질 수 있다\n' +
      '\n' +
      '그림 7: 언어(Pile-test)와 멀티모달(MMBench and MMBench_CN) 벤치마크에 대한 모달리티 워밍업의 비교 분석은 모달리티 그룹화가 언어 태스크에서 비그룹화 모달리티 접근 방식을 일관되게 능가하는 동시에 멀티모달 태스크(Multimodal:Language=60%:40%)에 대한 성능을 보존한다는 것을 보여준다.\n' +
      '\n' +
      '시각적 특징을 결합하기 위해 사용되는 방법과 MLP 어댑터의 설계 두 가지 주요 방식으로 비전 언어 어댑터를 사용할 수 있습니다.\n' +
      '\n' +
      '이전의 연구들(Tong et al., 2024)은, 시각적 특징 토큰들의 더 긴 시퀀스로 인해 증가된 계산 요건들의 트레이드-오프와 함께 제공되지만, 시퀀스 차원을 따라 시각적 특징들을 조합하는 것이 더 나은 모델 성능을 초래할 수 있음을 나타냈다. 표 10의 상단 섹션에서 입증된 바와 같이, 시퀀스 연결 전에 이미지의 폭 또는 높이 차원을 따라 시각적 특징을 적층함으로써 시퀀스 길이를 감소시키는 것은 시퀀스 길이를 일정하게 유지하기 위해 대부분의 메트릭에서 임베딩 차원을 따라 단순히 병합하는 것과 비교하여 더 나은 결과를 달성하지 못한다. 어댑터 아키텍처 측면에서, 각각의 비전 피처 인코더에 대해 별도의 MLP 어댑터를 사용하는 것은 시각적 피처들의 특정 값들 및 분포 패턴들에 대한 보다 정밀한 조정을 가능하게 하여, 보다 원활한 모델 트레이닝을 용이하게 한다. 반대로, 상이한 비전 인코더에 공유된 MLP 어댑터를 사용하는 것은 적절한 특징 융합에 기여한다. 우리는 혼합 전략을 채택하고 표 10의 하위 섹션에 요약된 바와 같이 안정적이고 개선된 성능을 보고한다.\n' +
      '\n' +
      '도 8: 모달리티 워밍업을 위한 언어(파일-테스트) 및 멀티모달(MMBench 및 MMBench_CN) 벤치마크에 대한 비교 성능 결과. 모달 워밍업은 평가된 모든 작업(멀티모달:언어=60%:40%)에서 모달 워밍업이 없는 접근법의 성능과 일관되게 일치하거나 능가한다.\n' +
      '\n' +
      '그림 9: 2단계의 훈련 손실에 대한 다양한 비전 인코더의 비교 분석.\n' +
      '\n' +
      '##5 결론, 한계 및 향후 작업\n' +
      '\n' +
      '이 기술 보고서에서 우리는 1.3B 및 6.7B 매개변수의 규모로 사용할 수 있는 일련의 멀티모달 대형 언어 모델인 DeepSeek-VL을 도입했다. 이 보고서는 딥섹-VL이 채택한 혁신적인 접근 방식을 위한 단계를 설정하면서 우세한 프로젝터 기반 사전 훈련 방법론에 내재된 한계를 드러냈다. 딥시크-VL은 공동 비전과 언어(VL) 사전 훈련 단계를 우선시함으로써 멀티모달 데이터의 통합이 대규모 언어 모델(LLM)의 언어 능력을 손상시키지 않도록 보장함으로써 전통적인 모델을 초월한다. 이는 전략적인 웜업 데이터 비율과 하이브리드 비전 인코더의 도입을 통해 달성되며, 이는 함께 시맨틱 풍부함을 잃지 않고 고해상도 이미지의 효율적인 처리를 가능하게 한다.\n' +
      '\n' +
      '제한된 토큰 예산 내에서 1024 x 1024 이미지를 처리할 수 있는 하이브리드 비전 인코더의 통합은 다양한 작업에 걸쳐 미묘한 세부 사항과 의미적 무결성을 보존하기 위한 우리의 약속을 강조한다. 결과적으로 DeepSeek-VL은 그 수업에서 일반주의 모델이 설정한 기준을 충족할 뿐만 아니라 능가하는 선구적 모델로 등장한다. 언어 중심 평가에서 어마어마한 숙련도를 유지하면서 광범위한 시각 중심 벤치마크에서 탁월한 성능을 보여줍니다.\n' +
      '\n' +
      'DeepSeek-VL을 공개적으로 사용할 수 있도록 하는 데 있어 우리는 연구 커뮤니티 내에서 추가 혁신 및 탐색을 촉매하여 향후 연구가 구축할 수 있는 강력한 기반을 제공하는 것을 목표로 한다. 이러한 개방의 제스처는 멀티모달 데이터를 다루는 데 있어 우리의 이해와 능력의 집단적 발전을 촉진하기 위한 것이다.\n' +
      '\n' +
      '앞을 내다보며, 우리는 딥섹-VL을 더 큰 규모로 확장하고, Mixture of Experts(MoE) 기술을 통합할 계획을 발표하게 되어 기쁩니다. 이러한 향후 확장은 모델의 효율성과 효율성을 더욱 향상시켜 AI 분야의 연구 및 적용을 위한 새로운 지평을 열 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]\n' +
      '\n' +
      '01-ai. Yi-34B 비전 언어 모델. [https://huggingface.co/01-ai/Yi-VL-34B] (https://huggingface.co/01-ai/Yi-VL-34B), 2024.\n' +
      '\n' +
      '아비 Screenshot to code. [https://github.com/abi/screenshot-to-code] (https://github.com/abi/screenshot-to-code), 2024.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline Architecture & MMB & MMC & SEED & POPE & ScienceQA & MMMU & OCRB & Average \\\\ \\hline \\multicolumn{8}{l}{**Sequence Concatenation:**} \\\\ \\hline Token Pooling - W & 61.2 & 59.6 & 61.6 & 86.5 & **57.7** & 31.6 & 304 & 55.5 \\\\ Token Pooling - H & 59.9 & 58.3 & 61.6 & 83.8 & 55.0 & **32.0** & 291 & 54.2 \\\\ \\hline \\multicolumn{8}{l}{**Embedding Concatenation:**} \\\\ \\hline Hybrid MLP & 61.7 & **60.1** & 62.9 & **87.8** & 56.6 & 31.3 & 309 & **55.9** \\\\ Shared MLP & **62.0** & 58.9 & 62.5 & 86.6 & 54.7 & 30.2 & **318** & 55.2 \\\\ Separate MLP & 57.5 & 58.7 & **63.1** & 86.5 & 56.6 & 29.0 & 299 & 54.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: SigLIP 및 SAM을 하이브리드 비전 인코더로서 사용하는 상이한 어댑터 아키텍처의 비교, 하이브리드 MLP는 시퀀스 연결 실험에 사용된다. **굵은** 항목은 가장 좋은 결과를 나타내는 반면 밑줄 친 항목은 두 번째로 좋은 결과를 나타낸다. 평균 점수를 계산하기 위해 OCRBench를 총 질문 수로 나눈다.\n' +
      '\n' +
      '애나의 기록 보관소 Anna\'s archive. [https://annas-archive.org/] (https://annas-archive.org/), 2024.\n' +
      '* 인류학[2023] 인류학. Claude, 2023. URL[https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)\n' +
      '* Austin et al. [2021] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.\n' +
      '* Bai et al. [2023] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* Ravishi et al. [2023] R. Ravishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Tasirlar. Introducing our multimodal models, 2023. URL [https://www.adept.ai/blog/fuyu-8b](https://www.adept.ai/blog/fuyu-8b).\n' +
      '* Blecher [2024] L. 브레처 후반반 GitHub repository, 2024. URL[https://github.com/lukas-blecher/LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR).\n' +
      '* Blecher et al. [2023] L. Blecher, G. Cucurull, T. Scialom, and R. Stojnic. Nougat: Neural optical understanding for academic documents. _arXiv preprint arXiv:2308.13418_, 2023.\n' +
      '* Burns et al. [2023] A. Burns, K. Srinivasan, J. Ainslie, G. Brown, B. A. Plummer, K. Saenko, J. Ni, and M. Guo. A suite of generative tasks for multi-level multimodal webpage understanding. In _The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, 2023. URL [https://openreview.net/forum?id=rwcLHjtUmn](https://openreview.net/forum?id=rwcLHjtUmn).\n' +
      '* 카터[2024] J. 카터. Textocr-gpt4v. [https://huggingface.co/datasets/jimmycarter/textocr-gpt4v] (https://huggingface.co/datasets/jimmycarter/textocr-gpt4v), 2024.\n' +
      '* Chen et al. [2023] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* Chng et al. [2019] C. K. Chng, Y. Liu, Y. Sun, C. C. Ng, C. Luo, Z. Ni, C. Fang, S. Zhang, J. Han, E. Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In _2019 International Conference on Document Analysis and Recognition (ICDAR)_, pages 1571-1576. IEEE, 2019.\n' +
      '* Cobbe et al. [2021] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Dai et al. [2023] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.\n' +
      '* DeepSeek-AI[2024] DeepSeek-AI. Deepseek llm: 장기주의를 가진 오픈 소스 언어 모델을 스케일링하는 것 _ arXiv preprint arXiv:2401.02954_, 2024. URL[https://github.com/deepseek-ai/DeepSeek-LM](https://github.com/deepseek-ai/DeepSeek-LM).\n' +
      '* echo840[2024] echo840. 상세한 캡션 데이터셋. [https://huggingface.co/datasets/echo840/Detailed_Caption] (https://huggingface.co/datasets/echo840/Detailed_Caption), 2024.\n' +
      '* Foundation[2023] W. 재단 위키미디어 다운로드 URL[https://dumps.wikimedia.org](https://dumps.wikimedia.org)\n' +
      '* Gao et al. [2023] J. Gao, R. Pi, J. Zhang, J. Ye, W. Zhong, Y. Wang, L. Hong, J. Han, H. Xu, Z. Li, et al. G-llava: Solving geometric problem with multi-modal large language model. _arXiv preprint arXiv:2312.11370_, 2023.\n' +
      '* Zhou et al. [2023]L. 가오성 비더만 블랙 L. 골딩, T 호피, C. 포스터, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of various text for language modeling. _ ArXiv:2101.00027_, 2020.\n' +
      '* 구글[2023] 구글. 우리의 AI 여정의 중요한 다음 단계인 2023. URL[https://blog.google/tech](https://blog.google/tech) nology/ai/bard-google-ai-search-updates/.\n' +
      '* Hendrycks et al. [2020] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* High-flyer[2023] High-flyer. 하이엘름 2023. URL[https://www.high-flyer.c](https://www.high-flyer.c)n/en/blog/hai-llm.\n' +
      '* Hsiao et al. [2022] Y.-C. Hsiao, F. Zubach, M. Wang, et al. Screenqa: Large-scale question-answer pairs over mobile app screenshots. _arXiv preprint arXiv:2209.08199_, 2022.\n' +
      '* HuggingFaceM4[2024] HuggingFaceM4. Websited dataset. [https://huggingface.co/datasets/HuggingFaceM4/WebSight] (https://huggingface.co/datasets/HuggingFaceM4/WebSight), 2024.\n' +
      '* Kantharaj et al. [2022] S. Kantharaj, R. T. Leong, X. Lin, A. Masry, M. Thakkar, E. Hoque, and S. Joty. Chart-to-text: A large-scale benchmark for chart summarization. In S. Muresan, P. Nakov, and A. Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4005-4023, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.277. URL [https://aclanthology.org/2022.acl-long.277](https://aclanthology.org/2022.acl-long.277).\n' +
      '* Kirillov et al. [2023] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al. Segment anything. _arXiv preprint arXiv:2304.02643_, 2023.\n' +
      '* Kocetkov et al. [2023] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, D. Bahdanau, L. von Werra, and H. de Vries. The stack: 3 tb of permissively licensed source code. In _Transactions on Machine Learning Research_, 2023.\n' +
      '* Korthikanti et al. [2023] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersson, M. Shoeybi, and B. Catanzaro. Reducing activation recomputation in large transformer models. _Proceedings of Machine Learning and Systems_, 5, 2023.\n' +
      '* Krylov et al. [2021] I. Krylov, S. Nosov, and V. Sovrasov. Open images v5 text annotation and yet another mask text spotter. In _Asian Conference on Machine Learning_, pages 379-389. PMLR, 2021.\n' +
      '* 쿨카르니와 트루엘센[2023] A. 쿨카르니와 J. 트루엘센. wkhtmltopdf. [https://wkhtmltopdf.org/] (https://wkhtmltopdf.org/). 애쉬 쿨카르니가 관리하는 프로젝트는 원래 야콥 트루엘센이 만들었다. 접속: 2024-02-22.\n' +
      '*레이온[2023]레이온. Gpt-4v 데이터세트[https://huggingface.co/datasets/laion/gpt4v-dataset] (https://huggingface.co/datasets/laion/gpt4v-dataset), 2023.\n' +
      '* Li 등 [2023a] B. Li, R. 왕건왕 지영 Ge, Y. 숀 종자 벤치: 생성적 이해도를 가진 다중 모드 llms를 벤치마킹 arXiv preprint arXiv:2307.16125_, 2023a.\n' +
      '* Li and Tajbakhsh [2023] S. Li와 N. 타지바흐쉬 Scigraphqa: 과학 그래프에 대한 대규모 합성 다중 턴 질문 응답 데이터 세트, 2023.\n' +
      '* Li et al. [2020] Y. Li, G. Li, L. He, J. Zheng, H. Li, and Z. Guan. Widget captioning: Generating natural language description for mobile user interface elements. _arXiv preprint arXiv:2010.04295_, 2020.\n' +
      '* Li 등 [2020]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:25]\n' +
      '\n' +
      'J. 마오, J. 황, A. 토셰프, O. Camburu, A. L. Yuille, K. 머피 명확한 객체 설명의 생성 및 이해. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 11-20, 2016.\n' +
      '* Masry et al. [2023] A. Masry, P. Kavehzadeh, X. L. Do, E. Hoque, and S. Joty. Unichart: A universal vision-language pretrained model for chart comprehension and reasoning. _arXiv preprint arXiv:2305.14761_, 2023.\n' +
      '* mPLUG[2024] mPLUG. M-paper dataset. [https://huggingface.co/datasets/mPLUG/M-Paper] (https://huggingface.co/datasets/mPLUG/M-Paper), 2024.\n' +
      '* Narayanan et al. [2021] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15, 2021.\n' +
      '* Nayef et al. [2017] N. Nayef, F. Yin, I. Bizid, H. Choi, Y. Feng, D. Karatzas, Z. Luo, U. Pal, C. Rigaud, J. Chazalon, et al. Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt. In _2017 14th IAPR international conference on document analysis and recognition (ICDAR)_, volume 1, pages 1454-1459. IEEE, 2017.\n' +
      '* OpenAI[2022] OpenAI. 채팅: 대화를 위한 언어 모델을 최적화합니다. 2022. URL[https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)\n' +
      '* OpenAI[2023a] OpenAI. GPT-4 기술 보고서입니다 arXiv_, 2023a.\n' +
      '* OpenAI[2023b] R. 오픈AI Gpt-4v(ision) 시스템 카드. 2023b.\n' +
      '* Rodriguez et al. [2023] J. A. Rodriguez, D. Vazquez, I. Laradji, M. Pedersoli, and P. Rodriguez. Ocr-vqgan: Taming text-within-image generation. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 3689-3698, 2023.\n' +
      '* Schaeffer et al. [2024] R. Schaeffer, B. Miranda, and S. Koyejo. Are emergent abilities of large language models a mirage? _Advances in Neural Information Processing Systems_, 36, 2024.\n' +
      '* Shazeer[2020]N. 셰이저 Glu 변형은 변압기를 개선한다. _ arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* Shi et al. [2017] B. Shi, C. Yao, M. Liao, M. Yang, P. Xu, L. Cui, S. Belongie, S. Lu, and X. Bai. Icdar2017 competition on reading chinese text in the wild (rctw-17). In _2017 14th iapr international conference on document analysis and recognition (ICDAR)_, volume 1, pages 1429-1434. IEEE, 2017.\n' +
      '* Shoeybi et al. [2019] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* Singh et al. [2021] A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8802-8812, 2021.\n' +
      '* Su et al. [2024] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n' +
      '* Sun et al. [2023] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang. Generative pretraining in multimodality. _arXiv preprint arXiv:2307.05222_, 2023.\n' +
      '* Sun et al. [2021]Y. 선종욱 니철규 장영 Liu, C. Luo, C. C. Ng, J. Han, E. Ding, J. Liu, D. Karatzas, et al. Icdar 2019 Competition on large-scale street view text with partial labeling-rrc-lsvt. _2019 국제 문서 분석 및 인식(ICDAR)_에서, 페이지 1557-1562. IEEE, 2019.\n' +
      '* Team et al.(2023) G. Team, R. 안성일 보르지우 우종범 알라락, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tong et al.(2024) S. 통주 유영 자이영 마영 르쿤과 S. 시 눈을 크게 감고? 멀티모달 llms의 시각적 단점을 탐구합니다. _ arXiv preprint arXiv:2401.06209_, 2024.\n' +
      '* Touvron et al.(2023a) H. Touvron, T. 라브릴, G. 이자카드, X. 마티넷 - A. 라초, T. 라크루아, B. 로지에르, N. Goyal, E. Hambro, F. Azhar, et al. LLAMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a.\n' +
      '* Touvron et al.(2023b) H. Touvron, L. 마틴기 스톤, P. 알버트, A. 알마하일리, Y. 바배이 바슐리코프 바트라, P. 바가바, S. 보살, D. 비켈, L. Blecher, C. Canton-Ferrer, M 첸, G. 쿠쿠럴, D. 에시오부, J. 페르난데스, J. 푸, W. Fu, B. Fuller, C. Gao, V. 고스와미 고열 A. 하트쇼른, S. 호세이니 허현인 카다스, V 커케즈 Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. 라초, T. 라브릴, J. 리, D. 리스코비치, Y. 류영 마오진 마티넷 미하일로프, P. 미쉬라, I. 몰리보그, Y. 니, A. 폴튼, J. 레이젠슈타인, R. 룽타 살라디, A. Schelten, R. 실바, E. M. 스미스, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. 얀인자로프 장아환 캄바두르 나랑 A. 로드리게스, R. 스톱닉 에두노프와 T 사이알롬 라마 2: 오픈 파운데이션 및 미세 조정 채팅 모델들_ CoRR_, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL[https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)\n' +
      '* Veit et al.(2016) A. Veit, T. 마테라, L. Neumann, J. Matas, S. 신부님 코코-텍스트: 자연 이미지에서의 텍스트 검출 및 인식을 위한 데이터세트 및 벤치마크 _ ArXiv:1601.07140_, 2016.\n' +
      '* Wang et al. (2021) B. Wang, G. Li, X. 주주 천태호 그로스먼과 Y 리 Screen2words: 멀티모달 학습을 이용한 자동 모바일 ui 요약. 제34회 ACM Symposium on User Interface Software and Technology_, pages 498-510, 2021.\n' +
      '* Wang et al. (2023a) J. Wang, L. 멍준 웽병하 Wu, Y. - G. 장 보는 것은 믿는 것이다: 더 나은 시각적 지시 튜닝을 위해 gpt-4v를 프롬프트하는 것; _ arXiv preprint arXiv:2311.07574_, 2023a.\n' +
      '* Wang et al.(2023b) W. 왕규 Lv, W. 유원 홍진기 왕종지 양룡 자오 Song, et al. Cogvlm: Visual expert for prerained language models. _ arXiv preprint arXiv:2311.03079_, 2023b.\n' +
      '* Yang et al.(2021) Y. 양아파나고풀루 류룡 장민 야츠카와 C. 캘리슨 버치 Wikihow를 이용한 시각적 목표단계 추론 arXiv preprint arXiv:2104.05845_, 2021.\n' +
      '* Ye et al. (2023) J. Ye, A. Hu, H. Xu, Q. 예민 Yan G. Xu, C. Li, J. Tian, Q. Qian, J. Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multiimodal large language model. _ arXiv preprint arXiv:2310.05126_, 2023.\n' +
      '* Yu et al.(2023a) Q. 유규 선선욱 장영 최창영 조욱 왕, 그리고 J. 류 캡스퓨전: 스케일에서 이미지-텍스트 데이터를 재싱크하는 단계; _ arXiv preprint arXiv:2310.20550_, 2023a.\n' +
      '* Yu et al.(2023b) W. 유진 양룡 이정왕 린지 류진 왕, L. 왕 Mm-vet: 통합 기능을 위한 대규모 멀티모달 모델 평가 arXiv preprint arXiv:2308.02490_, 2023b.\n' +
      '\n' +
      'X. 유영 니경 장태 정룡 류규장 스티븐스 D. 장원 렌영 Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. _ arXiv preprint arXiv:2311.16502_, 2023.\n' +
      '* Zellers et al.(2019) R. 젤러스, A. 홀츠만, Y. Bisk, A. Farhadi, Y. 최 헬라 스웨그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? A. 코호넨, D. R. 트라움 및 L. Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics_, ACL 2019, Florence, Italy, July 28-8월 2, 2019, Volume 1: Long Papers, pages 4791-4800. Association for Computational Linguistics, 2019. Doi: 10.18653/v1/p19-1472. URL[https://doi.org/10.18653/v1/p19-1472](https://doi.org/10.18653/v1/p19-1472](https://doi.org/10.18653/v1/p19-1472).\n' +
      '* Zhang and Sennrich (2019) B. Zhang and R. 센리히 Root mean square layer normalization. _ Neural Information Processing Systems_, 32, 2019에서의 발전\n' +
      '* Zhang et al.(2024) G. Zhang, X. 두병천 량태 루오태 정경 주영 청창수 Guo, et al. Cmmmu: A chinese massive multi-discipline multimodal understanding benchmark. _ arXiv preprint arXiv:2401.11944_, 2024.\n' +
      '* Zhang et al.(2019) R. 장영 주경호 장규 송남 이경 주림 왕동무 리아오 Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. 2019년 국제학술대회 논문 분석 및 인식(ICDAR)에서는 1577-1581페이지가 있다. IEEE, 2019.\n' +
      '* CVPR 2017, Hawaii, U.S.A._, 2017. URL[http://sunw.csail.mit.edu/abstract/uberText.pdf](http://sunw.csail.mit.edu/abstract/uberText.pdf)\n' +
      '* Zhong et al.(2023) W. 중락 최영 곽용 량상 류영 왕아현 천, N. 듀안 기초 모델을 평가하기 위한 인간 중심 벤치마크 CoRR_, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL[https://doi.org/10.48550/arXiv.2304.06364](https://doi.org/10.48550/arXiv.2304.06364).\n' +
      '* Zhu et al.(2024) W. J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. 유룡 슈미트, W. Y. Wang, Y. 최 멀티모달 c4: 텍스트가 인터리빙된 이미지들의 오픈, 억대 스케일 코퍼스 _ 신경 정보 처리 시스템_, 36, 2024의 발전.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      '그림 10 \\(|\\) 시각화 결과이다. DeepSeek-VL은 현실 세계의 아이들의 프로그래밍 다이어그램을 이해하고 상세하고 체계적인 설명을 제공할 수 있다.\n' +
      '\n' +
      '도 11 | 시각화 결과. DeepSee-VL은 실제 세계에서 코드와 차트에 대한 강력한 이해 능력을 가지고 있습니다.\n' +
      '\n' +
      '도 12: 시각화 결과. DeepSeek-VL은 현실 세계에 대한 광범위한 지식을 가지고 있다.\n' +
      '\n' +
      '도 13: 시각화 결과. DeepSeek-VL은 실제 테이블의 내용을 정확하게 읽을 수 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>