<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# GES : 효율적인 복사 필드 렌더링을 위한 일반화된 지수 스플래팅\n' +
      '\n' +
      ' Abdullah Hamdi\\({}^{1}\\) Luke Melas-Kyriazi\\({}^{1}\\) Guocheng Qian\\({}^{2,4}\\) Jinjie Mai\\({}^{2}\\)\n' +
      '\n' +
      'Ruoshi Liu\\({}^{3}\\)  Carl Vondrick\\({}^{3}\\)  Bernard Ghanem\\({}^{2}\\)  Andrea Vedaldi\\({}^{1}\\)**\n' +
      '\n' +
      '옥스퍼드대학교 시각기하학그룹\n' +
      '\n' +
      '압둘라 국왕과기대\n' +
      '\n' +
      'Columbia University \\({}^{3}\\)Snap Inc.\n' +
      '\n' +
      'abdullah.hamdi@eng.ox.ac.uk\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '3D 가우시안 스플래팅의 발전은 3D 재구성 및 생성을 상당히 가속화했다. 그러나, 그것은 상당한 메모리 풋프린트를 생성하는 많은 수의 가우시안들을 필요로 할 수 있다. 본 논문에서는 GES(Generalized Exponential Splatting)를 이용하여 3차원 장면을 모델링하는 새로운 표현 방법인 GES(Generalized Exponential Function)를 소개한다. GES는 일반화된 지수함수(Generalized Exponential Function)를 이용하여 3차원 장면을 표현하는데 훨씬 적은 수의 파티클이 필요하므로 가우시안 기반 유틸리티에 대한 플러그 앤 플레이 대체 능력으로 효율성 면에서 가우시안 Splatting 방법을 크게 능가한다. GES는 원리적인 1D 설정과 사실적인 3D 장면 모두에서 이론적으로 그리고 경험적으로 검증된다. 예리한 모서리를 가진 신호를 더 정확하게 나타내는 것으로 나타났으며, 이는 고유한 저역 통과 특성으로 인해 가우시안에게 일반적으로 도전적이다. 우리의 경험적 분석은 GEF가 자연 발생 신호(_e.g. 사각형, 삼각형, 포물선 신호)를 피팅하는 데 가우시안보다 우수하다는 것을 보여준다. 따라서, 가우시안 스플래팅의 메모리 풋프린트를 증가시키는 광범위한 분할 동작의 필요성을 감소시킨다. GES는 주파수 변조 손실의 도움으로 새로운 뷰 합성 벤치마크에서 경쟁적인 성능을 달성하면서 가우시안 스플래팅의 메모리 저장량의 절반 이하를 요구하며 렌더링 속도를 최대 39%까지 증가시킨다. 코드는 프로젝트 웹사이트 [https://abdullahhamdi.com/ges](https://abdullahhamdi.com/ges)에서 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '게임, 영화 및 메타버스에 걸쳐 더 매력적이고 몰입적인 가상 경험을 추구하려면 시각적 풍부함과 계산 효율성의 균형을 맞추는 3D 기술의 발전이 필요하다. 이와 관련하여, 3D 가우스 스플래팅(GS; 3D Gaussian Splatting) [27]은 3D 객체들 및 장면들을 학습하고 렌더링하기 위한 신경 래디언스 필드들[17, 40, 44, 45, 51, 81]에 대한 최근의 대안이다. GS는 작고 색깔이 있는 가우시안들의 큰 혼합으로 한 장면을 나타낸다. 그것의 핵심 장점은 매우 빠른 미분 가능한 렌더러의 존재이며, 이는 이러한 표현을 실시간 애플리케이션에 이상적으로 적합하게 하고 학습 비용을 상당히 감소시킨다. 특히, 학습 가능한 3D 표현의 빠른 렌더링은 고품질, 유동성 및 반응형 그래픽이 필수적인 게임과 같은 응용 분야에서 매우 중요하다.\n' +
      '\n' +
      '그러나 GS가 결점이 없는 것은 아니다. 특히 GS가 모델링된 신호의 특성에 대해 암묵적으로 가정한다는 점에 주목하며, 이는 차선책이다. 구체적으로, 가우시안들은 _low-pass filter_에 해당하지만, 대부분의 3D 장면들은 모양과 외관에 급격한 불연속성을 포함하기 때문에 로우-pass와는 거리가 멀다. 그림 2는 가우시안 기반 방법의 고유한 저역 통과 제한을 시연한다. 결과적으로 GS는 이러한 3D 장면을 표현하기 위해 매우 적은 수의 가우시안들을 사용해야 하며, 이는 더 적절한 기반이 선택되었다면 훨씬 더 많이 사용되어 메모리 활용에 부정적인 영향을 미친다.\n' +
      '\n' +
      '이러한 단점을 해결하기 위해 본 연구에서는 새로운 ap인 _GES_(Generalized Exponential Splatting)를 소개한다.\n' +
      '\n' +
      '도 1: **GES: Generalized Exponential Splatting** Gaussians 대신 Generalized 지수 함수(추가 학습 가능한 형상 파라미터 포함)에 의존하는 Gaussian Splatting[27]에 대한 더 빠르고 효율적인 대안을 제안한다.\n' +
      '\n' +
      '3D 장면 모델링을 위해 GEF(Generalized Exponential Function)를 활용하는 프로치(도 1). 제안된 방법은 신호, 특히 날카로운 특징을 갖는 신호를 효과적으로 표현하도록 설계되었으며, 이전의 가우시안 스플래팅 기법은 종종 평활화되거나 모델 [27]을 위해 광범위한 분할을 필요로 한다. 그림 3에서 입증된 바와 같이, 우리는 정사각형을 맞추기 위해 무작위로 초기화된 가우스인이 필요하지만 동일한 신호에 대해 \\(N=5\\) GEF만 필요함을 보여준다. 이것은 가우시안 혼합물이 저역 통과 주파수 영역을 갖는 반면 정사각형과 같은 많은 공통 신호는 대역 제한이 없다는 사실에서 비롯된다. 이러한 고대역 모델링은 가우시안 기반 방법에 대한 근본적인 도전을 구성한다. GES가 저주파에서 고주파까지 점진적으로 훈련할 수 있도록 특수 주파수 변조 이미지 손실을 제안한다. 이를 통해 GES는 기존의 새로운 뷰 합성 벤치마크에서 경쟁적인 성능을 유지하면서 가우시안 스플래팅의 메모리 요구량의 50% 이상 감소 및 렌더링 속도의 최대 39% 증가를 달성할 수 있다.\n' +
      '\n' +
      '우리의 공헌을 다음과 같이 요약한다.\n' +
      '\n' +
      '* 우리는 장면 모델링을 위해 가우시안 대신 GEF(Generalized Exponential Functions)의 사용을 동기화하는 원리적인 수치 시뮬레이션을 제시한다.\n' +
      '* 우리는 GEF를 활용하여 사실적이고 실시간적이며 메모리 효율적인 새로운 뷰 합성을 위한 스플래팅 기반 방법을 개발하는 새로운 3D 표현인 GES(Generalized Exponential Splatting)를 제안한다.\n' +
      '* 특수 주파수 변조 이미지 손실을 갖추고 새로운 뷰 합성에 대한 표준 벤치마크에 대한 광범위한 실험을 통해 GES는 가우시안 스플래팅 기반 실시간 래디언스 필드 렌더링을 위해 메모리 요구량이 50% 감소하고 렌더링 속도가 최대 39% 증가하는 것을 보여준다. GES는 _any_ Gaussian 기반 유틸리티에 대한 플러그 앤 플레이 대체 역할을 할 수 있다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**다시점 3D 재구성.**다시점 3D 재구성은 상이한 카메라 위치로부터 캡처된 그의 2D RGB 이미지로부터 장면의 3D 구조를 복구하는 것을 목표로 한다[1, 16]. 고전적 접근법은 일반적으로 SIFT 기반 [39] 포인트 매칭[61, 63]을 사용하여 장면의 기하학을 포인트 클라우드로서 복구한다. 보다 최근의 방법들은 특징 추출(_e.g_. [75, 76, 22, 83])을 위해 신경망들에 의존함으로써 이들을 향상시킨다. NeRF(Neural Radiance Fields) [37, 44]의 개발은 3D를 볼륨 레디언스[66]로 재구성하는 방향으로 전환하여 사진 사실주의 소설 뷰[69, 4, 5]를 합성할 수 있게 했다. 후속 작업에서도 수샷(_e.g_. [15, 23, 28])에서 NeRF의 최적화를 조사했다. 및 원샷(_e.g_. [7, 82]) 설정. NeRF는 어떠한 3D 기하학적 구조(밀도 필드만)도 명시적으로 저장하지 않으며, 몇몇 작업들은 소수의 샷 설정에서도 포함하는 장면의 표면[77, 78, 12, 33, 34, 71, 72]을 복구하기 위해 서명된 거리 함수를 사용할 것을 제안한다(_e.g_. [84, 85]).\n' +
      '\n' +
      '** 미분 가능한 렌더링.** 가우시안 스플래팅은 각도 복사 성분[80]에 대한 구면 조화 계수를 갖는 가우시안 함수(평균, 분산, 불투명도)로서 3D 점들을 파라미터화하는 점 기반 렌더링[2, 19] 알고리즘이다. 선행 연구들은 미분 래스터화를 광범위하게 연구해 왔으며, 일련의 연구[38, 26, 36]에서는 삼각형 메쉬에서 삼각형과 픽셀 사이의 미분 함수를 정의하는 기술을 제안하여 관찰로부터 삼각형 메쉬의 매개변수를 조정할 수 있다. 이러한 작업은 이미지 필터로 메쉬 처리를 위한 미분 가능한 렌더러를 제안하고[32], 주변 삼각형의 혼합 방식을 제안하고[48], 미분 가능한 래스터화를 대규모 실내 장면으로 확장한다[79]. 점 기반 렌더링[19] 측에서, 신경 점 기반 렌더링[26]은 기하학적 및 텍스처 정보를 위해 특징들이 3D 포인트들에 학습되고 저장될 수 있게 한다. Wiles _et al_. 신경 점 기반 렌더링과 더 나은 실사를 위해 적대적 손실을 결합[73]하는 반면, 이후 작업은 NeRF와 점 기반 렌더링[74, 86]을 결합하여 복사 필드를 나타내기 위해 점을 사용한다. 우리의 GES는 모든 점이 규모, 불투명도 및 모양을 가진 일반화된 지수를 나타내는 점 기반 래스터라이저이며 그에 따라 래스터화에 영향을 미친다.\n' +
      '\n' +
      '**Prior-based 3D reconstruction.** 현대의 Zero-shot text-to\n' +
      '\n' +
      '그림 2: **Gaussians**의 내재적 저역 통과 제한. 우리는 정사각형과 삼각형 신호와 비교하여 가우시안 함수의 대역폭 제약을 설명한다. 가우시안 함수의 저역 통과 특성은 무한대의 대역폭을 갖는 날카로운 모서리로 신호를 맞추는 능력을 제한한다. 이 제한은 고대역폭 3D 공간 데이터를 정확하게 피팅하는 데 있어 3D 가우시안 스플래팅[27]에 대한 도전을 구성한다.\n' +
      '\n' +
      '이미지 생성기[55, 56, 59, 3, 18, 60]는 더 강한 합성 사전[11, 42, 50, 70, 8, 70]을 제공함으로써 결과를 개선했다. DreamFusion[50]은 주어진 텍스트 질의에 대해 기성 확산 모델[60]을 NeRF[5, 44]로 증류하기 위해 제안된 중요한 작업이다. 텍스트 대 3D 합성(_e.g_. [9, 30])을 위한 수많은 후속 접근 방식을 촉발했다. 및 이미지-투-3D 복원(_e.g_. [13, 35, 41, 64])을 포함할 수 있다. 후자는 정면 카메라 위치[35] 및/또는 피사체 구동 확산 안내[30, 54] 상의 추가적인 재구성 손실을 통해 달성된다. 개발된 방법은 감독[35, 65]의 기본 3D 표현[9, 30, 67] 및 3D 일관성을 개선했으며 작업별 사전[21, 24, 58] 및 추가 대조군[43]을 탐색했다. 최근 가우시안 기반 방법[68]은 가우시안 스플래팅의 빠른 래스터화를 활용하여 3D 생성의 최적화 속도를 향상시켰다. 우리는 GES가 이 애플리케이션 및 기타 유틸리티에서 가우스 스플래팅을 위한 플러그 앤 플레이 대체물로 작동할 수 있는 방법을 보여준다.\n' +
      '\n' +
      '##3 일반화된 지수의 특성\n' +
      '\n' +
      '일반화된 지수함수\n' +
      '\n' +
      '** 예비.** 일반화된 지수 함수(GEF)는 일반화된 정규 분포(GND)의 확률 밀도 함수(PDF)와 유사하다[14]. 이 함수는 형상 매개변수 \\(\\beta\\in(0,\\infty))을 조정하여 다양한 데이터 형상에 보다 유연한 적응을 가능하게 한다. GEF는 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[f(x|\\mu,\\alpha,\\beta,A)=A\\exp\\left(-\\left(\\frac{|x-\\mu|}{\\alpha}\\right)^{\\beta}\\right) \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\mu\\in\\mathbb{R}\\)은 위치 파라미터, \\(\\alpha\\in\\mathbb{R}\\)은 스케일 파라미터, \\(A\\in\\mathbb{R}^{+}\\)은 양의 진폭을 정의한다. 이 함수의 거동은 그림 3에 설명되어 있다. \\(\\beta=2\\)에 대해 GEF는 스케일된 가우시안 \\(f(x|\\mu,\\alpha,\\beta=2,A)=Ae^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\alpha}\\right)^{2}\\)가 된다. 따라서 GEF는 저역 주파수 영역을 갖는 가우시안 혼합과 달리 \\(\\beta\\)을 변화시켜 광범위한 데이터를 모델링할 수 있는 다양한 프레임워크를 제공한다. 정사각형이나 삼각형과 같은 많은 공통 신호는 대역 제한이 없으며 가우스 기반 방법에 대한 근본적인 도전을 구성한다. 본 논문에서는 일반화된 3차원 표현을 위해 가우시안 스플래팅의 모든 성분에 대해 양의 \\(\\beta\\)을 얻고자 한다.\n' +
      '\n' +
      '**이론 결과.** 일반화 가능한 능력에도 불구하고 GEF의 거동은 형상 매개변수 \\(\\beta\\)에 의존하는 닫힌 형태가 없는 지수들의 복잡한 적분을 포함하기 때문에 분석적으로 쉽게 연구할 수 없다. 우리는 부록_의 정리 1에서 정사각형 신호와 같은 특정 경우에 대해 GEF가 \\(\\beta\\)를 적절하게 선택함으로써 해당 가우시안 함수보다 엄격하게 더 작은 근사 오차를 달성할 수 있음을 보여준다. 그 증명은 오차 계산을 단순화하기 위해 구형파 신호의 대칭성을 이용한다. 정리 1은 3D 가우시안 스플래팅[27] 대신 GES 표현에서 표준 가우시안 함수보다 GEF를 선호하기 위한 이론적 기초를 제공한다.\n' +
      '\n' +
      '시뮬레이션에서 1차원 GEF 혼합물 평가###\n' +
      '\n' +
      '다양한 1차원 신호 유형을 나타내는 데 있어 GEF의 혼합물의 효과를 평가한다. 이 평가는 모델을 일반적인 실제 신호의 특성 특성을 복제하는 합성 신호에 피팅하여 수행된다. 보다 상세한 내용과 추가적인 시뮬레이션 결과는 _the Appendix_에 제시되어 있다.\n' +
      '\n' +
      '**Simulation Setup.** 실험 프레임워크는 PyTorch[47]에서 구현된 일련의 파라메트릭 모델을 기반으로 했으며, PyTorch[47]의 혼합물을 사용하여 1D 신호를 근사하도록 설계되었다.\n' +
      '\n' +
      '도 3: **일반화된 지수함수(GEF) (_a_): \\(\\alpha=1,\\mu=0\\)에 대해 다른 \\(\\beta\\) 값을 갖는 GEFs \\(f_{\\beta}(x)=Ae^{-\\left(\\frac{|x-\\mu|}{\\alpha}\\right)^{\\beta}}\\)의 패밀리를 보여준다. \\(\\beta=2\\)일 때, 함수는 3차원 가우시안 스플래팅에서 뒤따르는 가우시안 함수로 감소한다[27]. GES에서 우리는 각 스플래팅 성분의 또 다른 매개변수로 \\(\\beta\\)을 학습한다. (_b_,_c_): 학습 가능한 \\(\\beta\\)을 갖는 제안된 GEF 혼합물은 기울기 기반 최적화를 사용하여 가우시안 함수에 비해 더 적은 성분으로 동일한 신호(제곱)에 적합하다. (_b_): 가우시안들이 _vs_를 사용할 때 \\(N=5\\) 성분을 갖는 피팅된 혼합물의 예를 보여준다. (_c_) 경우 GEF가 \\(N=2\\) 성분과 함께 사용된다. GEF는 적은 수의 성분으로 가우시안 대응(0.48 오차)보다 적은 오차 손실(0.44)을 달성하고 날카로운 에지를 더 잘 근사화한다. 최적화된 개별 구성요소(랜덤 매개변수로 초기화됨)는 수렴 후 녹색으로 표시됩니다.**\n' +
      '\n' +
      '가우시안(low-pass), 가우시안 차이(Difference of Gaussians, DoG), 가우시안 라플라시안(Laplacian of Gaussian, LoG) 및 GEF 혼합 모델과 같은 상이한 함수들. 각 모델은 평균, 분산(또는 척도) 및 가중치에 대한 매개변수를 구성했으며 일반화된 모델은 GEF 함수의 지수를 제어하기 위해 추가 매개변수 \\(\\beta\\)를 통합했다.\n' +
      '\n' +
      '**모델.** 이 섹션에서는 실제 신호를 근사화하기 위해 사용된 혼합물 모델을 간략하게 개관한다. 자세한 제형은 _the Appendix_에 나와 있다.\n' +
      '\n' +
      '**가우시안 혼합:** 이 모델은 다중 가우시안 함수의 조합을 사용한다. 각각의 가우시안들은 그 자체의 평균, 분산 및 가중치를 특징으로 한다. 전체 모델은 저역 통과 필터인 이러한 가우시안 함수의 가중 합이다.\n' +
      '\n' +
      '** 가우시안(DoG) 혼합물의 차이:** DoG 모델은 가우시안 혼합물의 변형이다. 미리 정의된 분산 비율을 갖는 가우시안 함수의 쌍들 간의 차이를 취함으로써 형성된다. 이 모델은 신호에서 콘트라스트를 강조하는데 특히 효과적이며 대역 통과 필터로 간주된다.\n' +
      '\n' +
      '**Laplacian of Gaussian (LoG) Mixture:** 이 모델은 Laplacian of Gaussian 함수의 특성을 결합한다. 혼합물의 각 구성 요소는 모양과 축척을 제어하는 특정 매개변수를 가지고 있습니다. DoG와 마찬가지로 LoG 모델은 신호에서 미세한 세부 사항을 포착하는 데 능숙하며 대역 통과 필터입니다.\n' +
      '\n' +
      '**GEF(Generalized Exponential) 혼합물:** 가우스 혼합물의 보다 유연한 버전, 이 모델은 추가적인 형상 파라미터 \\(\\beta\\)를 도입한다. 이 매개변수를 조정하면 신호의 특성에 더 잘 맞도록 모델을 미세 조정할 수 있다. GEF 혼합 주파수 응답은 형상 파라미터 \\(\\beta\\)에 따라 달라진다.\n' +
      '\n' +
      '**모델 구성** 모델은 다양한 수의 구성 요소로 구성되었으며 \\(N\\)\\(N=\\{2,5,8,10,15,20\\}\\)을 사용하여 테스트를 수행했다. 구성요소의 가중치는 양으로 선택됩니다. 모든 \\(N\\) 성분의 모든 매개변수가 학습되었다. 각 모델은 평균 제곱 오차 손실 함수와 함께 아담 최적화기를 사용하여 훈련되었다. 입력 \\(x\\)은 합성 신호의 영역을 나타내는 선형 간격 텐서이고, 타겟 \\(y\\)은 \\(x\\)의 각 지점에서 신호의 값이다. 훈련은 정해진 수의 에폭 동안 진행되었으며, 훈련 종료 시 손실이 기록되었다.\n' +
      '\n' +
      '**데이터 생성.** 합성 1D 신호는 주어진 데이터 크기와 신호 폭으로 지정된 범위에 걸쳐 다양한 신호 유형에 대해 생성되었다. 신호는 혼합 모델을 훈련하기 위한 지상 진리로 사용되었다. 실험에 사용된 Ground truth 신호는 신호처리 알고리즘을 평가하기 위한 벤치마크 역할을 하는 1차원(1D) 함수이다. 연구 중인 신호 유형은 _square_, _triangle_, _parabolic_, _half sinusoidal_, _Gaussian_ 및 _exponential_ 함수이다. 우리는 그림 3에 \\(N=5\\)일 때 가우시안과 \\(N=2\\)일 때 제곱 신호에 일반화된 혼합물을 피팅하는 예를 보여준다. 정사각형 신호가 sinc 함수에 의해 알려진 무한한 대역폭을 갖는 반면, 낮은 통과 대역폭을 갖는 가우시안들에 대해 얼마나 날카로운 에지들이 도전을 구성하는지에 주목한다[25].\n' +
      '\n' +
      '**시뮬레이션 결과.** 훈련 후 손실값을 기준으로 모델의 성능을 평가하였다. 또한 생성된 도표를 통해 입력 신호를 나타내는 모델의 능력을 시각적으로 검사했다. 결과의 분산을 설명하기 위해 구성당 여러 실행이 실행되었다. 종합적인 평가를 위해 각 구성을 여러 번 실행(구성당 20회 실행)하여 설명했다.\n' +
      '\n' +
      '그림 4: **다양한 혼합물의 수치 시뮬레이션 결과.** 다양한 신호 유형(a-f)에서 다양한 수의 성분에 걸쳐 구배 기반 최적화기로 최적화된 다양한 혼합물 모델에 대한 평균 손실의 비교를 보여준다. NaN 손실(경사 폭발)의 경우 그래프에는 결과가 표시되지 않는다. 전체 시뮬레이션 결과는 _the Appendix_\n' +
      '\n' +
      '훈련 과정에서의 가변성. 이러한 실행 동안 훈련이 \'난\' 손실을 초래한 경우의 수는 손실 도표에서 제거되었으므로 그림 4의 일부 도표는 일부 \\(N\\)에서 손실 값을 갖지 않는다. 그림 4에 도시된 바와 같이, GEF 혼합물은 성분 수에 걸쳐 일관되게 가장 낮은 손실을 산출했으며, 이는 많은 공통 신호, 특히 정사각형 및 삼각형과 같은 대역 제한 없는 신호의 효과적인 근사를 나타낸다. 유일한 예외는 가우시안 신호이며, 가우시안 혼합기에 (분명히) 더 잘 맞는다.\n' +
      '\n' +
      '##4 GES(Generalized Exponential Splatting)\n' +
      '\n' +
      'Eq의 GEF의 이점을 확립했습니다. (1) 가우스 함수에 걸쳐, 이제 GEF를 GES(Generalized Exponential Splatting) 프레임워크로 확장하는 방법을 시연하여 가우스 스플랫을 위한 플러그 앤 플레이 교체를 제공할 것이다. 또한 SfM(Structural from Motion) [62]를 통해 얻은 장면의 정적 이미지와 그에 상응하는 카메라 보정을 모아 희소점 구름을 추가로 제공한다. 가우시안 모델[27]을 넘어 GES는 스플랫의 초점을 맞추기 위해 지수\\(\\beta\\)를 채택하여 장면 간선의 묘사를 선명하게 한다. 이 기법은 메모리 사용에 더 효율적일 뿐만 아니라 새로운 뷰 합성을 위한 확립된 벤치마크에서 가우시안 스플래팅을 능가할 수 있다.\n' +
      '\n' +
      '# 미분 가능한 GES 제형\n' +
      '\n' +
      '우리의 목표는 정제된 장면 표현으로 새로운 뷰 합성을 향상시키는 것이다. 우리는 3차원 공간에서의 위치\\(\\mathbf{x}\\)와 양의 정행렬\\(\\mathbf{\\Sigma}\\)에 대한 일반화된 지수 형태를 이용한다.\n' +
      '\n' +
      '[L(\\mathbf{x};\\boldsymbol{\\mu},\\boldsymbol{\\Sigma},\\beta)=\\exp\\left\\{-\\frac{1}{2}\\big{(}(\\mathbf{x}-\\boldsymbol{\\mu}}}{\\intercal}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}}\\big{}}^{\\frac{\\beta}{2}\\right},\\tag{2}\\big{(}(\\mathbf{x}-\\boldsymbol{\\mu}}}\\frac{\\beta}{2}\\right},\\tag{2}\\big{(}(\\mathbf{x}-\\boldsymbol{\\mu}}}{\\intercal}\\boldsymbol{\\mu}}{\\intercal}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\\n' +
      '\n' +
      '여기서 \\(\\boldsymbol{\\mu}\\)는 위치 파라미터이고 \\(\\boldsymbol{\\Sigma}\\)는 가우시안 스플래팅에서의 공분산 행렬 등가값[27]. (\\beta\\)는 스플랫의 선명도를 조절하는 형상 파라미터이다. \\(\\beta=2\\)일 때, 이 제형은 가우시안 스플래팅[27]과 동등하다. 제안된 방법은 혼합을 위한 불투명도 측정치\\(\\kappa\\)를 유지하며, 가우시안 스플래팅과 유사하게 구면 고조파를 사용하여 채색을 수행한다[27].\n' +
      '\n' +
      '2차원 영상투영을 위해 Zwicker _et al_. [88]에 의한 기법을 적용하였다. 그러나 변수 지수 \\(\\beta\\)를 계속 추적합니다. 카메라 공간 공분산 행렬 \\(\\boldsymbol{\\Sigma}^{\\prime}\\)은 다음과 같이 변환된다. \\(\\boldsymbol{\\Sigma}^{\\prime}=\\mathbff{J}\\mathbf{W}\\boldsymbol{\\Sigma}\\mathbff{W}^{\\intercal}\\mathbff{J}^{\\intercal}\\), 여기서 \\(\\mathbf{J}\\)는 세계에서 카메라 공간으로 변환되는 자코비안이고, \\(\\mathbf{W}\\(\\mathbf{W}\\(\\mathbf{W}\\(\\mathbf{W}\\(\\mathbf{W}\\(\\mathbf{W}\\)(\\mathbf{W}\\(\\mathbf{J}^{\\intercal}\\)는 세상에서 카메라 공간으로 변환되는 자코비안이고, \\(\\mathbf{W}\\(\\mathbf{W}\\)는 세상에서 카메라 공간으로 변환되는 자코비안이고,\n' +
      '\n' +
      '그림 5: **새로운 뷰 합성에 대한 시각적 비교.** 제안된 방법과 확립된 기준선 간의 비교를 각각의 지상 진실 이미지와 함께 표시한다. 묘사된 장면은 Mip-NeRF360 데이터 세트의 가든 및 룸, 딥 블렌딩 데이터 세트의 Dř존슨, 탱크&템플의 기차 순으로 정렬된다. 줌인 디테일을 통해 렌더링 품질의 미묘한 차이가 강조된다. 이러한 특정 장면은 공정한 비교를 위해 가우신 분할[27]과 유사하게 선택되었다. 일반적으로 GES와 가우시안 사이의 차이를 보는 것은 거의 동일한 PSNR(GES가 50% 적은 메모리를 필요로 함에도 불구하고)을 갖기 때문에 어려울 수 있다.\n' +
      '\n' +
      '\\(\\mathbf{\\Sigma}\\)의 고유값. 우리는 스케일링 행렬\\(\\mathbf{S}\\)과 회전행렬\\(\\mathbf{R}\\)의 곱으로 수식화함으로써 최적화 전반에 걸쳐 양의 반정의를 유지할 수 있도록 하였으며, 이 성분들의 최적화는 별도의 3차원 스케일 벡터\\(\\mathbf{s}\\)와 쿼터니언 회전\\(\\mathbf{q}\\)을 통해 용이하게 하였다.\n' +
      '\n' +
      '### 일반 지수 스플랫을 위한 고속 미분 래스터라이저\n' +
      '\n' +
      '**볼륨 렌더링으로부터의 직관.** 신경 복사 필드(44)의 맥락에서 볼륨 렌더링의 개념은 장면을 통과하는 광선을 따라 방출된 복사선의 통합을 포함한다. 카메라 광선(\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}\\)의 예상 색상 \\(C(\\mathbf{r})\\)에 대한 적분방정식은 다음과 같다.\n' +
      '\n' +
      '[\\begin{split}C(\\mathbf{r})=\\int_{t_{n}}^{t_{f}}T(t)\\kappa(\\mathbf{r}(t))c(\\mathbf{r}(t),\\mathbf{d}\\,dt,\\\\text{where}\\quadT(t)=\\exp\\left(-\\int_{t_{n}}^{t}\\kappa(\\mathbf{r}(s))\\,ds\\right)\\end{split}\\tag{3}\\cappa(\\mathbf{r}(t))c(\\mathbf{r}(t),\\mathbf{d}\\,dt,\\\\text{where}\\quadT(t)=\\exp\\left(-\\int_{t_{n}}^{t}\\kappa(\\mathbf{r}(s))\\,ds\\right)\\end{split}\\tag{3}\\cappa(\\\n' +
      '\n' +
      '여기서, \\(t)\\는 \\(t_{n}\\)에서 \\(t\\)까지의 광선에 따른 투과율을 나타내며, \\(\\kappa(\\mathbf{r}(t))\\)는 체적 밀도이고, \\(c(\\mathbf{r}(t),\\mathbf{d})\\)는 \\(\\mathbf{r}(t)\\) 방향의 점 \\(\\mathbf{r}(t)\\)에서의 방사 광선을 나타낸다. 비공백을 가로지르는 광선에 의해 교차되는 총 거리 \\([t_{n},t_{f}]\\)는 손실된 에너지의 양을 지시하여 렌더링된 색상의 강도를 감소시킨다. 가우스 스플래팅 세계[27]에서 이 거리\\([t_{n},t_{f}]\\)는 광선 방향을 따라 각 성분의 투영된 분산\\(\\alpha\\)으로 구성된다. 우리의 GES of Eq. (2) 일부 개별 구성요소의 형상 파라미터 \\(\\beta\\)가 변하면 Eq에 유효한 영향을 미친다. (3)은 다음과 같이 변형 함수 \\(\\phi(\\beta)\\)에 의해 변형된 동일한 성분의 유효 분산 투영 \\(\\widehat{\\alpha}\\)에 의해 결정될 것이다:\n' +
      '\n' +
      '\\[\\widehat{\\alpha}(\\beta)=\\phi(\\beta)\\alpha\\quad. \\tag{4}\\]\n' +
      '\n' +
      '형상 매개변수 \\(\\beta\\)는 스플래팅 성분의 전역적 성질이므로 우리가 선택한 수정 함수 \\(\\phi\\)는 광선 방향에 의존하지 않으며, 많은 성분을 포함하는 장면을 가정한다. 다음으로 수정 함수 \\(\\phi\\)의 선택 및 가우스 스플래팅의 래스터화 프레임워크에 어떻게 적합한지를 다룬다[27].\n' +
      '\n' +
      '**대략적인 래스터화.** 주요 질문은 래스터화 프레임워크에서 GES를 어떻게 표현할 것인가이다. 실제로, 가우시안 스플래팅 [27]에서의 래스터화는 각 성분의 분산 스플랫에만 의존한다. 따라서, 우리는 GES의 래스터화를 얻기 위해 형상 파라미터 \\(\\beta\\)가 각 성분의 공분산에 미치는 영향을 시뮬레이션하기만 하면 된다. 이를 위해 각 성분의 공분산의 스케일 행렬을 스케일러 함수 \\(\\phi(\\beta)\\)에 의해 수정한다. 확률 이론으로부터 일반화된 지수 분포의 분산과 가우시안 분포의 분산 사이의 정확한 변환은 [14]와 같이 주어진다.\n' +
      '\n' +
      '\\[\\phi(\\beta)=\\frac{\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)} \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\Gamma\\)는 감마함수이다. 이 변환은 Eq.입니다. (5)는 PDF를 1로 적분하는 것을 보장한다. 유사한 방식으로, 적분은 식에 있다. (3) Eq. (4)는 Eq의 동일한 수정을 사용하여 가우시안 및 GES에 대해 동등한 것으로 나타낼 수 있다. (5). 수정은 우리가 지수 변화를 수행한 것처럼 래스터화 _에 영향을 미칠 것이다. 이것은 \\(\\beta\\) 지수를 취하지 않고 일반화된 지수 래스터화를 사용할 수 있는 트릭이다. 마찬가지로, 가우시안 스플래팅[27]은 강체 가우시안_을 학습하지 않고, 마치 이미지 평면에 스플래팅할 때 가우시안들이 배치된 것처럼 _act하는 포인트 클라우드의 속성을 학습한다. 우리의 GES와 가우시안 모두 같은 스플랫 정신에 있고 스플랫 성질로 3D를 표현한다. 그림 6은 카메라의 광선\\(\\mathbf{r}\\)과 교차하는 개별 스플래팅 성분에 대한 개념과 유효 분산 투영\\(\\widehat{\\alpha}\\) 개념을 보여준다. 그러나 그림 6에서와 같이 이 스케일러 수정 \\(\\phi(\\beta)\\)에는 몇 가지 뷰 의존적 경계 효과 오류(_e.g_)가 도입된다. 만약 ray\\(\\mathbf{r}\\)가 대각선을 통과한다면. 우리는 부록_에서 이 오차에 대한 상한 추정치를 제공한다.\n' +
      '\n' +
      'Eq에서 \\(\\Gamma\\) 함수의 불안정성으로 인해. (5) 다음 평활함수로 \\(\\phi(\\beta)\\)를 근사할 수 있다.\n' +
      '\n' +
      '\\[\\bar{\\phi}_{\\rho}(\\beta)=\\frac{2}{1+e^{-(\\rho\\beta-2\\rho)}}\\enspace. \\tag{6}\\]\n' +
      '\n' +
      '정확한 변형\\(\\phi(\\beta)\\)와 근사 \\(\\bar{\\phi}_{\\rho}(\\beta)\\)의 차이는 하이퍼파라미터 형상 강도 \\(\\rho\\)에 의해 제어된다. \\(\\beta=2\\)(가우시안 형상)에서 변형\\(\\phi\\)과 \\(\\bar{\\phi}\\)이 정확히 1이다.\n' +
      '\n' +
      '그림 6: **GES 성분의 유효 분산**. 본 논문에서는 형상 변형(\\(\\beta>2\\)) 하에서 카메라 광선과 교차하는 개별 스플래팅 성분에 대한 유효 분산 투영(\\widehat{\\alpha}(\\beta)\\)의 개념을 증명한다. \\(\\widehat{\\alpha}(\\beta)\\)는 원래 스플랫 투영 분산\\(\\alpha\\)의 축소된 버전이라는 점에 유의한다.\n' +
      '\n' +
      '매개변수화\\(\\bar{\\phi}_{\\rho}(\\beta)\\)는 각 성분의 분산이 양으로 유지되도록 한다.\n' +
      '\n' +
      '### 주파수 변조 이미지 손실\n' +
      '\n' +
      'GES의 광대역 특성을 효과적으로 활용하기 위해 주파수 변조 영상 손실(\\(\\mathcal{L}_{\\omega}\\)을 개선하였다. 이 손실은 처음에 가우시안 저역 대역 스플랫으로 구성된 GES가 훈련 초기 단계에서 주로 저주파 세부 사항에 집중해야 한다는 근거에 근거한다. 훈련이 진행됨에 따라, 스플랫 형성이 더 높은 주파수를 캡슐화하도록 적응하면서, 최적화의 강조는 이미지 내의 이러한 더 높은 주파수 대역으로 점진적으로 이동해야 한다. 이 개념은 3D 좌표 공간이 아닌 이미지 도메인 내에서 적용되지만 BARF[31]에서 사용되는 주파수 변조 접근법과 기술적 유사성을 가지고 있다. 손실은 정규화된 주파수\\(\\omega\\)로 변조된 영상 재구성 작업에서 에지 인식 최적화를 향상시키기 위해 DoG(Difference of Gaussians) 필터를 통해 구현된 주파수 조절 마스크에 의해 유도된다. DoG 필터는 대역 통과 필터로서 작용하여, 덜 흐릿한 다른 버전으로부터 이미지의 흐릿한 버전을 빼서 에지를 강조하고, 따라서 이미지의 제2 공간 도함수를 근사화한다. 이 연산은 다음과 같이 수학적으로 표현된다:\n' +
      '\n' +
      '(I)=G(I,\\sigma_{1})-G(I,\\sigma_{2}),\\\\0<\\sigma_{2}<\\sigma_{1}\\)\n' +
      '\n' +
      '여기서 \\(G(I,\\sigma)\\)는 표준 편차 \\(\\sigma\\)를 갖는 이미지 \\(I\\)에 대한 가우시안 블러 연산을 나타낸다. \\(\\sigma\\) 값들의 선택은 강조되어야 할 에지의 스케일을 결정함으로써 필터의 주파수 대역을 효과적으로 결정한다. 대역 통과 필터의 타당성을 확보하기 위해 \\(\\sigma_{1}=2\\sigma_{2}\\)을 선택하였으며, 여기서 \\(\\sigma_{2}\\)의 선택은 필터의 목표 주파수 대역을 결정한다. 본 논문에서는 저주파수에서는 소정의 목표 정규화 주파수\\(\\omega\\)(\\(\\omega=0\\%\\)에서 고주파수에서는 \\(\\omega=100\\%\\)을 사용한다. 필터와 합리적인 마스크의 안정성을 확보하기 위해 \\(\\sigma_{2}=0.1+10\\omega\\)를 선택하였다. 그런 다음 필터링된 이미지를 사용하여 다음과 같이 임계값(정규화 후)에 대한 픽셀별 비교를 통해 에지 인식 마스크\\(M_{\\omega}\\)를 생성한다.\n' +
      '\n' +
      '\\text{DoG}_{\\omega}=\\mathbb{1}\\left(\\text{DoG}_{\\omega}(I_{\\text{gt})_{\\text{normalized}}>\\epsilon_{\\omega}\\right)\\\\\\, \\tag{7}\\]\\[\\text{DoG}_{\\omega}(I)=G(I,0.2+20\\omega)-G(I,0.1+10\\omega\\)\n' +
      '\n' +
      '필터 DoG({}_{\\omega}\\)의 정규화된 응답에 대해 \\(0\\leq\\epsilon_{\\omega}\\leq 1\\)은 임계값(0.5)이고, \\(I_{\\text{gt}\\)은 그라운드 진리 이미지이며, \\(\\mathbb{1}\\)은 지표 함수이다. 마스크의 예는 그림 8을 참조하십시오. 에지 인식 주파수 변조 손실\\(\\mathcal{L}_{\\omega}\\)은 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\omega}=\\|(I-I_{\\text{gt}})\\cdot M_{\\omega}\\|_{1}, \\tag{8}\\]\n' +
      '\n' +
      '여기서 \\(I\\)은 복원된 영상이고, \\(\\|\\cdot\\|_{1}\\)은 L1 norm을 나타낸다. 이 용어는 나중에 보여지는 바와 같이 전체 손실에 통합된다. 마스크는 지정된 주파수\\(\\omega\\)를 대상으로 합니다. 우리는 식에서 이러한 목표 \\(\\omega\\) 값을 결정하기 위해 선형 스케줄을 사용한다. (8) 및 Eq. (7) GES 최적화 시 \\(\\omega=\\text{current iteration}\\) 손실(\\mathcal{L}_{\\omega}\\)은 장면의 특성에 따라 모양(\\beta\\)을 조정하는데 도움을 주는 것을 목표로 한다. 초기 값으로부터 \\(\\beta\\)를 튜닝하여 고주파에 초점을 맞추기 전에 훈련 중에 저역 통과 신호에 GES 성분을 먼저 집중함으로써 그렇게 한다. 이는 표 6에서 나중에 볼 수 있는 바와 같이 GES의 _efficiency_를 돕는다(거의 메모리의 9% 감소).\n' +
      '\n' +
      '높은 주파수에 대한 DoG 필터 감도 때문에, \\(0\\%<\\omega\\leq 50\\%\\) 마스크를 \\(50\\%<\\omega\\leq 100\\%\\)의 \\(1-M_{\\omega}\\)으로 정의한다. 이는 이미지의 모든 부분이 마스크들 중 하나(M_{\\omega}\\)에 의해 커버되는 반면, 최적화가 진행됨에 따라 세부사항들에 더 집중될 것을 보장한다.\n' +
      '\n' +
      '일반화된 지수 스플릿의### 최적화\n' +
      '\n' +
      '우리는 형상 속성에 따라 GES를 선택적으로 자두하여 가변 밀도 메커니즘이 필요하지 않은 형상 밀도를 제어하기 위한 새로운 접근법을 자세히 설명한다. 이 최적화 전략은 구면 고조파 계수를 통한 스플랫의 위치\\(\\mathbf{x}\\), 불투명도\\(\\kappa\\), 공분산 행렬\\(\\mathbf{\\Sigma}\\) 및 색상 표현뿐만 아니라 \\(\\beta\\) 매개변수를 포함한다[27]. 이러한 요소의 최적화는 확률적 기울기 하강을 사용하여 수행되며, 프로세스는 GPU 기반 계산 및 특수 CUDA 커널에 의해 가속화된다.\n' +
      '\n' +
      'SfM 점으로부터 \\(\\mathbf{\\Sigma}\\) 및 \\(\\mathbf{x}\\)에 대한 시작 추정치를 추론하는 반면, 모든 \\(\\beta\\) 값은 \\(\\beta=2\\)(순수 가우시안 스팔트)로 초기화된다. 손실 함수는 구조적 유사성 손실(SSIM)과 결합된 \\(\\mathcal{L}_{1}\\) 메트릭과 주파수 변조 손실(\\mathcal{L}_{\\omega}\\)을 통합한다:\n' +
      '\n' +
      '\\lambda_{\\text{L1}\\mathcal{L}_{1}+\\lambda_{\\text{ssim}\\mathcal{L}_{\\text{ssim}+\\lambda_{\\omega}\\mathcal{L}_{\\omega}, \\tag{9}\\mathcal}\n' +
      '\n' +
      '도 7: **수정함수 \\(\\phi(\\beta)\\)**. 우리는 식에서 근사함수\\(\\bar{\\phi}_{\\rho}(\\beta)\\)의 서로 다른 \\(\\rho\\) 형상강도 값을 보인다. (6) 및 Eq.에서의 정확한 수정 함수 \\(\\phi(\\beta)\\) (5). \\(\\beta=2\\)(가우시안 스플랫)에서 _all_ 함수는 1의 분산 수정을 가지며 GES는 가우시안 스플랫으로 감소한다. 극단적인 \\(\\rho=0\\)의 경우, GES는 _any_\\(\\beta\\)에 대한 가우시안 스플래팅으로 감소한다.\n' +
      '\n' +
      '여기서 \\(\\lambda_{\\text{ssim}}=0.2\\)은 모든 평가에서 균일하게 적용되고 \\(\\lambda_{\\text{L1}}=1-\\lambda_{\\text{ssim}-\\lambda_{\\omega}\\). 학습 알고리즘 및 기타 특정 절차 요소에 대한 확장된 세부 사항은 _the Appendix_에서 사용할 수 있다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '데이터셋과 메트릭스\n' +
      '\n' +
      '실험에서 우리는 다양한 데이터 세트를 사용하여 실제 장면을 렌더링하는 알고리즘의 효과를 테스트했다. 이 평가는 다양한 출처의 13개의 실제 장면을 포함했다. 특히 우수한 NeRF 렌더링 품질로 유명한 Mip-Nerf360 데이터 세트[5]의 장면과 탱크 & 템플 데이터 세트[29]의 선택된 장면 및 딥 블렌딩에서의 작업에 대해 Hedman 등이 제공한 인스턴스에 중점을 두었다. 이러한 장면은 제한된 실내 설정에서 광범위한 제한 없는 실외 환경에 이르기까지 다양한 캡처 스타일을 제시했다.\n' +
      '\n' +
      '우리 연구의 품질 벤치마크는 Mip-Nerf360[4]에 의해 설정되었으며, 이는 인스턴트NGP[45] 및 플레녹셀과 같은 다른 현대 고속 NeRF 방법과 비교되었다. 우리의 기차/테스트 분할은 테스트를 위해 매 8번째 사진을 사용하여 Mip-NeRF360에서 권장하는 방법론을 따랐다. 이 접근법은 기존 문헌에서 자주 사용되는 PSNR, L-PIPS 및 SSIM과 같은 표준 측정을 포함하여 일관되고 의미 있는 오류 메트릭 비교를 용이하게 했다(표 1 참조). 우리의 결과는 다양한 구성 및 반복을 포함하며, 최적화된 매개변수에 대한 훈련 시간, 렌더링 속도 및 메모리 요구 사항의 차이를 강조한다.\n' +
      '\n' +
      '### GES의 구현 세부사항\n' +
      '\n' +
      '우리의 방법론은 모든 장면에서 일관된 하이퍼파라미터 설정을 유지하여 평가의 균일성을 보장했다. 대부분의 테스트를 위해 A6000 GPU를 배치했습니다. GES(Generalized Exponential Splatting)는 40,000회 이상 구현되었으며 밀도 기울기 임계값은 0.0003으로 설정되었으며 형상 매개변수에 대한 학습률은 0.0015로 설정되었으며 형상 재설정 간격은 1000회, 형상 가지치기 간격은 100회였다. 형상 기반 가지치기 임계값은 0.5로 설정되었고 형상 강도 매개변수는 0.1로 결정되어 정확도와 계산 부하 사이의 균형을 제공했다. 또한 이미지 라플라시안 스케일 팩터는 0.2로 설정되었으며, 해당 \\(\\lambda_{\\omega}\\) 주파수 손실 계수는 0.5로 표시되어 이미지 재구성 작업에서 에지 강화 최적화를 보장한다. 가우스 분할[27]과 공유된 다른 하이퍼파라미터 및 설계 선택(불투명 분할 및 가지치기)은 동일하게 유지되었다. 보다 상세한 내용은 _the Appendix_에 나와 있다.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      '### 새로운 시점 합성 결과\n' +
      '\n' +
      '우리는 두 새로운 뷰 합성 작업에서 여러 최신 기술에 대해 _GES_를 평가했다. 표 1은 그림 5에 추가하여 비교 결과를 캡슐화한다. 표 1은 _GES_가 새로운 뷰 합성에서 높은 충실도와 효율 사이의 균형을 달성한다는 것을 입증한다. SSIM이나 PSNR에서 항상 다른 방법을 능가하는 것은 아니지만 메모리 사용과 속도 면에서 상당히 뛰어나다. 메모리가 377MB에 불과하고 처리 속도가 2분인 _GES_는 특히, 실질적으로 더 많은 메모리 또는 더 긴 처리 시간을 필요로 하는 3D 가우시안-30K 및 인스턴트 NGP와 비교할 때 매우 효율적인 방법으로서 두드러진다. 전반적으로 결과는 뛰어난 효율성으로 균형 잡힌 성능을 제공하는 _GES_의 능력을 강조하여 고품질 출력과 작동 속도 및 메모리 효율성을 모두 요구하는 실시간 애플리케이션에 실행 가능한 옵션이 된다.\n' +
      '\n' +
      '그림 5에서 GES와 가우시안 간의 _visual effects_의 차이는 PSNR은 거의 동일하지만 파일 크기가 다르기 때문에 보기 어렵다는 점에 유의한다(표 1). 공정한 시각적 비교를 위해 구성 요소의 수를 대략적으로 동일하게 제한하고 (가우시안 분할을 제어함으로써) 그림 9의 결과를 보여준다. GES가 장면 내기에 대해 작고 날카로운 모서리를 모델링할 수 있음을 분명히 보여준다.\n' +
      '\n' +
      '그림 8: **주파수 변조된 이미지 마스크. 왼쪽의 입력 예제 이미지에 대해, Sec.4.3에서 사용된 주파수 손실 마스크 \\(M_{\\omega}\\)의 예를 다른 수의 목표 정규화 주파수 \\(\\omega\\)에 대해 보여준다. (\\(\\omega=0\\%\\) 저주파수에서 \\(\\omega=100\\%\\) 고주파수에서 \\(\\omega=100\\%\\) 이 마스킹된 손실은 GES가 특정 주파수 대역을 학습하는 데 도움이 된다. 본 논문에서는 GES의 최적화에 필요한 목표 \\(\\omega\\) 값, \\(\\omega=\\frac{\\text{currentiteration}}{\\text{total iterations}}\\)을 결정하기 위해 선형 스케줄을 사용한다. 높은 주파수에 대한 DoG 필터 민감도로 인해, \\(0<\\omega\\leq 50\\%\\)에 대한 마스크는 \\(50<\\omega\\leq 100\\%\\)의 \\(1-M_{\\omega}\\)으로 정의된다. 이렇게 하면 이미지의 모든 부분이 마스크\\(M_{\\omega}\\) 중 하나에 의해 가려지는 반면 최적화가 진행됨에 따라 세부 사항에 더 중점을 둡니다.**\n' +
      '\n' +
      '가우시안보다 더\n' +
      '\n' +
      '### 절제술 및 분석\n' +
      '\n' +
      '**형상 매개변수.** 표 2에서 새로운 형상 매개변수와 관련된 중요한 하이퍼파라미터가 새로운 뷰 합성 성능에 미치는 영향을 탐구한다. 우리는 식에서 적절한 근사치\\(\\bar{\\phi}_{\\rho}\\)를 알 수 있다. (6)이 필요한 이유는 \\(\\bar{\\phi}_{\\rho}\\)에 대해 \\(\\rho=10\\)을 정확한 \\(\\phi(\\beta)\\)에 가깝게 설정하면 PSNR이 11.6으로 떨어지기 때문이다.\n' +
      '\n' +
      '**주파수 변조된 이미지 손실의 영향.** Sec.4.3에 소개된 주파수 손실\\(\\mathcal{L}_{\\omega}\\)이 \\(\\lambda_{\\omega}\\)의 변화에 따른 성능에 미치는 영향을 연구한다. 표 2와 그림 10에서 우리는 이\\(\\mathcal{L}_{\\omega}\\)을 추가하는 것이 큰 콘트라스트가 존재하는 영역 또는 매끄러운 배경이 렌더링되는 영역에서 최적화를 개선하고 GES의 효율성을 향상시키는 방법을 보여준다. GES에서 \\(\\lambda_{\\omega}\\)을 증가시키면 파일의 크기가 감소하지만 성능에 영향을 줄 수 있음을 알 수 있다. 성능 향상과 파일 크기 감소 사이의 중간 지점으로 \\(\\lambda_{\\omega}=0.5\\)을 선택했다.\n' +
      '\n' +
      '** 기억력 감소를 분석.** \\(\\beta\\) 학습 후 기억력 감소는 실제로 필요한 구성요소 수의 감소에 기인한다는 것을 발견했다. 예를 들어, "Train" 시퀀스에서, 성분들의 수는 가우시안 스플래팅 및 GES에 대해 각각 1,087,264 및 548,064이다. 이것은 GES를 사용할 때 파일 크기를 275MB에서 129.5MB로 줄이는 것으로 해석된다.\n' +
      '\n' +
      '**GES를 빠른 3D 생성에 적용하는 것** 최근 작업들은 Gaussian Splatting을 사용하여 DreamGaussian[68] 및 Gaussian Splatting[10]을 사용하여 Text-to-3D와 같은 3D 생성 파이프라인들에 가우시안 Splatting을 사용하도록 제안되었다. GES를 이러한 가우시안 기반 3D 생성 파이프라인에 통합하는 것은 가우시안 스플래팅 대신 GES의 플러그 앤 플레이 능력으로 빠르고 설득력 있는 결과를 산출했다(도 11 참조).\n' +
      '\n' +
      '##7 결론 및 논의\n' +
      '\n' +
      '본 논문에서는 가우시안 스플래팅 시 메모리 효율과 신호 표현, 특히 고주파 신호에 대해 향상된 3차원 장면 모델링을 위한 새로운 기법인 _GES_(Generalized Exponential Splatting)을 소개한다. 우리의 경험적 결과는 새로운 뷰 합성 및 3D 생성 작업에서 그 효능을 입증한다.\n' +
      '\n' +
      '**제한.** 우리의 접근법에서 명백한 한 가지 한계는 일반적으로 표현이 암기 효율적이고 가능한 한 컴팩트하도록 하기 위해 성능이 떨어진다는 것이다. 이것은 \\(\\beta\\)-튜닝에 의존하는 가지치기 작업으로 인해 더 복잡한 장면에서 더 두드러진다. 많은 컴포넌트들을 제거하는 것은 결국 PSNR 성능을 떨어뜨릴 수 있다(표 1 마지막 2행). 향후 연구는 더 복잡하고 역동적인 환경에서 GES의 성능을 향상시키고 3D 모델링에서 다른 기술과의 통합을 탐색하는 데 초점을 맞출 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building come in a day. _Communications of the ACM_, 54(10):105-112, 2011.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c c c c c c} \\hline \\hline Dataset & \\multicolumn{6}{c}{Mip-NeRF360 Dataset} & \\multicolumn{6}{c}{Deep Blending} \\\\ Method–Metric & \\(SSIM^{\\dagger}\\) & \\(PSNR^{\\ddagger}\\) & \\(TPIPS^{\\ddagger}\\) & Train\\({}^{\\ddagger}\\) & PS\\(SIIM^{\\dagger}\\) & \\(PSNR^{\\ddagger}\\) & \\(TPIPS^{\\ddagger}\\) & Train\\({}^{\\ddagger}\\) & PS\\(SIIM^{\\dagger}\\) & \\(PSNR^{\\ddagger}\\) & \\(TPIPS^{\\ddagger}\\) & Train\\({}^{\\ddagger}\\) & PS\\(SIIM^{\\dagger}\\) & \\(PSNR^{\\ddagger}\\) & \\(TPIPS^{\\ddagger}\\) & Train\\({}^{\\ddagger}\\) \\\\ \\hline Phenoels & 0.626 & 23.08 & 0.463 & 26nm & 6.79 & 2.16B & 0.719 & 21.08 & 0.379 & 25m & 13.0 & 2.36B & 0.795 & 23.06 & 0.510 & 28m & 11.2 & 2.7GB \\\\ INGP & 0.699 & 25.59 & 0.331 & 7.5m & 9.43 & 48.88M & 0.745 & 21.92 & 0.305 & 7m & 14.4 & 48.88M & 0.817 & 24.96 & 0.390 & 8m & 2.79 & **68MM** \\\\ WebRF360 & 0.792 & 27.69 & 0.237 & 48.0 & 0.86M & 0.759 & 22.22 & 0.257 & 48.1 & 48.1 & **0.68M** & 0.901 & 29.40 & 0.245 & 48.0 & 0.98 & **68MM** \\\\\n' +
      '3D Gaussians-7K & 0.770 & 25.60 & 0.279 & 6.5m & 160 & 52.3MB & 0.767 & 21.20 & 0.280 & 7m & 197.720MB & 0.755 & 27.78 & 0.317 & 4.5m & 1027 & 36.8MB\\\\\n' +
      '3D Gaussians-30K & 0.815 & 27.21 & 0.214 & 42m & 134 & 734MB & 0.841 & 25.14 & 0.183 & 26m & 154 & 411MB & 0.903 & 29.41 & 0.243 & 36m & 137 & 67.6MB \\\\ \\hline GES (ours) & 0.794 & 26.91 & 0.250 & 32m & 18.36 & 377MB & 0.836 & 23.35 & 0.198 & 21m & 20.222MB & 0.901 & 29.68 & 0.252 & 30m & 160 & 399MB \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **새로운 뷰 합성 기술의 비교 분석.** 이 표는 다양한 데이터 세트에 걸쳐 확립된 방법과 우리의 접근법을 종합적으로 비교한다. SSIM, PSNR 및 LPIPS를 포함한 메트릭은 훈련 기간, 초당 프레임 및 메모리 사용과 함께 성능 효능의 다차원 관점을 제공한다. 서로 다른 방법의 훈련 시간 번호는 서로 다른 GPU에서 계산될 수 있으며, 반드시 완벽하게 비교할 수는 없지만 여전히 유효하다. 비명시적 표현들(INGGP, Mip-NeRF360)은 디코딩을 위해 추가적인 느린 신경망들에 의존하기 때문에 낮은 메모리를 갖는다. 빨간색 결과가 최고입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Ablation Setup & \\(PSNR^{\\ddagger}\\) & \\(SSIM^{\\dagger}\\) & \\(LPIPS^{\\ddagger}\\) & **Size (MB)\\({}^{\\ddagger}\\)** \\\\ \\hline Gaussians & 27.21 & 0.815 & 0.214 & 734 \\\\ \\hline GES w/o approx. \\(\\phi_{\\rho}\\) & 11.60 & 0.345 & 0.684 & 364 \\\\ GES w/o shape reset & 26.57 & 0.788 & 0.257 & 374 \\\\ GES w/o \\(\\mathcal{L}_{\\omega}\\) loss & 27.07 & 0.800 & 0.250 & 411 \\\\ Full GES & 26.91 & 0.794 & 0.250 & 377 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **새로운 뷰 합성에 대한 절제 연구.** Mip-NeRF360 데이터 세트의 재구성 품질 및 파일 크기에 대한 GES의 여러 구성 요소의 영향을 연구한다.\n' +
      '\n' +
      '도 9: **Fair Visual Comparison.** 공정한 시각적 비교를 위해 동일한 수의 스플래팅 컴포넌트에 구속될 때 가우시안 [27] 및 GES의 예를 도시한다. 그것은 GES가 가우시안들보다 그 장면에 대해 작고 날카로운 에지들을 더 잘 모델링할 수 있다는 것을 분명히 보여준다.\n' +
      '\n' +
      '* [2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graphics. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII 16_, pages 696-712. Springer, 2020.\n' +
      '* [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffin: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [4] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5855-5864, 2021.\n' +
      '* [5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5470-5479, June 2022.\n' +
      '* [6] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. _ICCV_, 2023.\n' +
      '* [7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16123-16133, 2022.\n' +
      '* [8] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niessner. Text2tex: Text-driven texture synthesis via diffusion models. _arXiv preprint arXiv:2303.11396_, 2023.\n' +
      '* [9] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [10] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. _arXiv preprint arXiv:2309.16585_, 2023.\n' +
      '* [11] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [12] Francois Darmon, Benedicate Bascle, Jean-Clement Devaux, Pascal Monasse, and Mathieu Aubry. Improving neural implicit surfaces geometry with patch warping. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6260-6269, 2022.\n' +
      '* [13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Obiayares-xl: A universe of 10m+ 3d objects. _arXiv preprint arXiv:2307.05663_, 2023.\n' +
      '* [14] J Armando Dominguez-Molina, Graciela Gonzalez-Faras, Ramon M Rodriguez-Dagnino, and ITESM Campus Monterrey. A practical procedure to estimate the shape parameter in the generalized gaussian distribution. _available through link [https://www.cimat.mx/BiblioAdmin/RTAdmin/reportes/enlinea/10-18_eng.pdf_](https://www.cimat.mx/BiblioAdmin/RTAdmin/reportes/enlinea/10-18_eng.pdf_), 1, 2003.\n' +
      '* [15] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [16] Olivier D Faugeras. What can be seen in three dimensions with an uncalibrated stereo rig? In _Computer Vi\n' +
      '\n' +
      '도 11: **GES Application: Fast Image-to-3D Generation**. GES를 Gaussian 기반 3D 생성 파이프라인[68]과 결합하여 Co3D 이미지[57]에서 선택된 3D 생성 사례를 보여주고, GES의 플러그 앤 플레이 이점을 강조하여 Gaussian Splatting[27]을 대체한다.\n' +
      '\n' +
      '그림 10: **Frequency-Modulated Loss Effect.** 주파수-Modulated Image Loss\\(\\mathcal{L}_{\\omega}\\)이 새로운 뷰 합성에 미치는 성능에 미치는 영향을 보여준다. 이 \\(\\mathcal{L}_{\\omega}\\)을 추가하면 큰 콘트라스트가 존재하거나 매끄러운 배경이 렌더링되는 영역에서 최적화가 어떻게 개선되는지 주목하라.\n' +
      '\n' +
      'sion--ECCV\'92: Second European Conference on Computer Vision Santa Margherita Ligure, Italy, May 19-22, 1992 Proceedings 2_, pages 563-578. Springer, 1992.\n' +
      '* [17] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5501-5510, June 2022.\n' +
      '* [18] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 89-106, 2022.\n' +
      '* [19] Markus Gross and Hanspeter Pfister. _Point-based graphics_. Elsevier, 2011.\n' +
      '* [20] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. _ACM Trans. on Graphics (TOG)_, 37(6), 2018.\n' +
      '* [21] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Niessner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. _arXiv preprint arXiv:2303.11989_, 2023.\n' +
      '* [22] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2821-2830, 2018.\n' +
      '* [23] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5885-5894, 2021.\n' +
      '* [24] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3d: Learning articulated 3d animals by distilling 2d diffusion. _arXiv preprint arXiv:2304.10535_, 2023.\n' +
      '* [25] A.J. Jerri. The shannon sampling theorem--its various extensions and applications: A tutorial review. _Proceedings of the IEEE_, 65(11):1565-1596, 1977.\n' +
      '* [26] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3d mesh renderer. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3907-3916, 2018.\n' +
      '* [27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics_, 42(4), July 2023.\n' +
      '* [28] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12912-12921, 2022.\n' +
      '* [29] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. _ACM Transactions on Graphics_, 36(4), 2017.\n' +
      '* [30] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [31] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In _IEEE International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [32] Hsueh-Ti Derek Liu, Michael Tao, and Alec Jacobson. Paparazzi: surface editing by way of multi-view image processing. _ACM Trans. Graph._, 37(6):221-1, 2018.\n' +
      '* [33] Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis Park, Simon Stent, and Carl Vondrick. What you can reconstruct from a shadow. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17059-17068, 2023.\n' +
      '* [34] Ruoshi Liu and Carl Vondrick. Humans as light bulbs: 3d human reconstruction from thermal reflection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12531-12542, 2023.\n' +
      '* [35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. _arXiv preprint arXiv:2303.11328_, 2023.\n' +
      '* [36] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7708-7717, 2019.\n' +
      '* [37] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. _arXiv preprint arXiv:1906.07751_, 2019.\n' +
      '* [38] Matthew M Loper and Michael J Black. Opendr: An approximate differentiable renderer. In _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13_, pages 154-169. Springer, 2014.\n' +
      '* [39] David G Lowe. Distinctive image features from scale-invariant keypoints. _International Journal of Computer Vision (IJCV)_, 60:91-110, 2004.\n' +
      '* [40] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7210-7219, 2021.\n' +
      '* [41] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360{deg} reconstruction of any object from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [42] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. _arXiv preprint arXiv:2211.07600_, 2022.\n' +
      '*[43]*[43] 아리안 미카에일리, Or Perel, Daniel Cohen-Or, and Ali Mahdavi-Amiri. Sked: 스케치 유도 텍스트 기반 3D 편집_ arXiv preprint arXiv:2303.10735_, 2023.\n' +
      '* [44] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 405-421. Springer, 2020.\n' +
      '* [45] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):102:1-102:15, July 2022.\n' +
      '* [46] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [47] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.\n' +
      '* [48] Felix Petersen, Amit H Bermano, Oliver Deussen, and Daniel Cohen-Or. Pix2vex: Image-to-geometry reconstruction using a smooth differentiable renderer. _arXiv preprint arXiv:1903.11149_, 2019.\n' +
      '* [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n' +
      '* [50] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [51] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10318-10327, 2021.\n' +
      '* [52] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 8748-8763. PMLR, 2021.\n' +
      '* [54] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aherman, Michael Rubinstein, Jonathan Barron, et al. Dream-booth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.\n' +
      '* [56] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 8821-8831. PMLR, 2021.\n' +
      '* [57] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10901-10911, October 2021.\n' +
      '* [58] Elad Richardson, Gal Metzzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. _arXiv preprint arXiv:2302.01721_, 2023.\n' +
      '* [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, 2022.\n' +
      '* [60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:36479-36494, 2022.\n' +
      '* [61] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [62] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.\n' +
      '* [63] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In _European Conference on Computer Vision (ECCV)_, 2016.\n' +
      '* [64] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young Chun. Ditto-nerf: Diffusion-based iterative text to omni-directional 3d model. _arXiv preprint arXiv:2304.02827_, 2023.\n' +
      '* [65] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust text-to-3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [66] Andrea Tagliasacchi and Ben Mildenhall. Volume rendering digest (for nerf). _arXiv preprint arXiv:2209.02417_, 2022.\n' +
      '* [67] Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with stable-diffusion, 2022. [https://github.com/ashawkey/stable-dreamfusion](https://github.com/ashawkey/stable-dreamfusion).\n' +
      '* [68] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. _arXiv preprint arXiv:2309.16653_, 2023.\n' +
      '**[69] 도르 버빈, 피터 헤드만, 벤 밀덴홀, 토드 지클러, 조나단 T 배론, 프라툴 P 스리니바산. Ref-nerf: 신경 복사 필드에 대한 구조화된 뷰 의존적 외관. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5481-5490. IEEE, 2022.\n' +
      '* [70] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [71] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.\n' +
      '* [72] Yiqun Wang, Ivan Skorokhodov, and Peter Wonka. Hf-neus: Improved surface reconstruction using high-frequency details. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:1966-1978, 2022.\n' +
      '* [73] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from a single image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7467-7477, 2020.\n' +
      '* [74] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5438-5448, 2022.\n' +
      '* [75] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 767-783, 2018.\n' +
      '* [76] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5525-5534, 2019.\n' +
      '* [77] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:4805-4815, 2021.\n' +
      '* [78] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:2492-2502, 2020.\n' +
      '* [79] Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. _ACM Transactions on Graphics (TOG)_, 38(6):1-14, 2019.\n' +
      '* [80] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. _arXiv preprint arXiv:2112.05131_, (2):6, 2021.\n' +
      '* [81] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [82] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4578-4587, 2021.\n' +
      '* [83] Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 1949-1958, 2020.\n' +
      '* [84] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [85] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. Ners: neural reflectance surfaces for sparse-view 3d reconstruction in the wild. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:29835-29847, 2021.\n' +
      '* [86] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis. In _SIGGRAPH Asia 2022 Conference Papers_, pages 1-12, 2022.\n' +
      '* [87] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '* [88] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. _Proceedings Visualization, 2001. VIS\'01._, pages 29-538, 2001.\n' +
      '\n' +
      '일반화된 지수 이면의 이론\n' +
      '\n' +
      '일반화된 지수함수\n' +
      '\n' +
      '일반화된 지수함수(GEF)는 추가적인 진폭 파라미터 \\(A\\in\\mathbb{R}\\)를 갖는 일반화된 정규분포(GND) [14]의 확률밀도함수(PDF)와 유사하다. 이 함수는 형상 매개변수 \\(\\beta\\in(0,\\infty))을 조정하여 다양한 데이터 형상에 보다 유연한 적응을 가능하게 한다. GEF는 다음과 같이 주어진다.\n' +
      '\n' +
      '\\[f(x|\\mu,\\alpha,\\beta,A)=A\\exp\\left(-\\left(\\frac{|x-\\mu|}{\\alpha}\\right)^{\\beta}\\right) \\tag{10}\\]\n' +
      '\n' +
      '여기서 \\(\\mu\\in\\mathbb{R}\\)은 위치 파라미터, \\(\\alpha\\in\\mathbb{R}\\)은 스케일 파라미터, \\(A\\)은 진폭을 정의하고 \\(\\beta>0\\)은 형상 파라미터이다. \\(\\beta=2\\)에 대해, GEF는 스케일링된 가우시안 분포가 된다:\n' +
      '\n' +
      '\\[f(x|\\mu,\\alpha,\\beta=2,A)=\\frac{A}{\\alpha\\sqrt{2\\pi}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\alpha/\\sqrt{2}\\right)^{2}\\right)\\tag{11}\\]\n' +
      '\n' +
      '그리고 \\(\\beta=1\\)에 대해서는 Eq. 도 10은 스케일링된 라플라스 분포로 감소:\n' +
      '\n' +
      '\\[f(x|\\mu,\\alpha,\\beta=1,A)=\\frac{A}{2\\alpha}\\exp\\left(-\\frac{|x-\\mu|}{\\alpha}\\right) \\tag{12}\\]\n' +
      '\n' +
      '따라서 GEF는 저역 주파수 영역을 갖는 가우시안 혼합과 달리 \\(\\beta\\)을 변화시켜 광범위한 데이터를 모델링할 수 있는 다양한 프레임워크를 제공한다. 정사각형 또는 삼각형과 같은 많은 공통 신호는 대역 제한이 없으며 가우스 기반 방법에 대한 근본적인 도전을 구성한다(도 12 참조). 본 논문에서는 일반화된 3차원 표현을 위해 가우시안 스플래팅의 모든 성분에 대해 양의 \\(\\beta\\)을 얻고자 한다.\n' +
      '\n' +
      '### Theoretical Results\n' +
      '\n' +
      '일반화 가능한 기능에도 불구하고 GEF는 주파수 영역 측면에서 고정된 동작이 없다. GEF와 푸리에 영역의 오차함수는 형상매개변수 \\(\\beta\\)에 의존하는 닫힌 형태가 없는 지수들의 복소적분을 포함하기 때문에 해석적으로 연구할 수 없다. 예를 들어, GEF의 푸리에는 다음과 같다.\n' +
      '\n' +
      '\\[\\mathcal{F}(f)(\\xi)=\\int_{-\\infty}^{\\infty}A\\exp\\left(-\\left(\\frac{|x-\\mu|}{ \\alpha}\\right)^{\\beta}\\right)e^{-2\\pi ix\\xi}\\,dx\\\n' +
      '\n' +
      '일반 \\(\\beta\\)에 대한 닫힌 형태의 해는 없습니다. 우리는 정사각형 신호와 같은 특정 경우에 대해 GEF가 \\(\\beta\\)를 적절하게 선택함으로써 해당 가우시안 함수보다 더 작은 근사 오차를 얻을 수 있음을 보여준다. 정리 1은 3D 가우시안 스플래팅[27] 대신 GES 표현에서 표준 가우시안 함수보다 GEF를 선호하기 위한 이론적 기초를 제공한다.\n' +
      '\n' +
      '**정리 1** (Superiority of GEF Approximation Over Gaussian for Square Wave Signals). _Let \\(S(t)\\)는 \\(t=0\\)을 중심으로 진폭 \\(A>0\\)과 폭 \\(L>0\\)을 갖는 구형파 신호를 나타낸다. 축소된 가우시안 \\(G(t;\\alpha,A)=Ae^{-\\frac{t^{2}}{\\alpha^{2}}\\)와 일반화 지수 함수 \\(GEF(t;\\alpha,\\beta,A)=Ae^{-(|t|/\\alpha)^{\\beta}}\\)의 두 함수를 정의한다. 주어진 축척 매개변수\\(\\alpha\\)에 대해 근사오차\\(E_{f}=\\int_{-\\infty}^{\\infty}|S(t)-f(t)|dt\\)와 같은 형상 매개변수\\(\\beta\\)이 존재한다. of the square signal \\(S(t)\\) using GEF is strictly smaller than the Gaussian \\(G\\)._\n' +
      '\n' +
      'Proof.: \\(E_{f}=\\int_{-\\infty}^{\\infty}|S(t)-f(t)|dt\\)로 \\(f\\) 함수를 이용한 정사각형 신호 \\(S(t)\\) 근사에 대한 오차 메트릭 \\(E_{f}\\) 대칭과 \\(S(t)\\)의 정의, 그리고 \\(S(t)>G(t;\\alpha,A)\\)의 사실, 가우시안 근사화에 대한 오차는 다음과 같다.\n' +
      '\n' +
      '\\[E_{G}=2\\int_{0}^{L/2}A(1-e^{-\\frac{t^{2}}{\\alpha^{2}}})dt+2\\int_{L/2}^{\\infty} Ae^{-\\frac{t^{2}}{\\alpha^{2}}}dt.\\]\n' +
      '\n' +
      'GEF 근사에 대해, 오차는:\n' +
      '\n' +
      '\\[E_{GEF}=2\\int_{0}^{L/2}A(1-e^{-(t/\\alpha)^{\\beta}})dt+2\\int_{L/2}^{\\infty}Ae^{ -(t/\\alpha)^{\\beta}}dt.\\]\n' +
      '\n' +
      '오차(\\Delta E=E_{G}-E_{GEF}\\)의 차이는 \\(\\beta\\)을 적절히 선택하여 엄밀하게 양의 오차를 보이는 것을 목표로 한다. 오차 차이는 다음과 같이 설명될 수 있다.\n' +
      '\n' +
      '\\[\\Delta E=\\Delta E_{middle}+\\Delta E_{tail}\\]\n' +
      '\n' +
      '\\[\\Delta E_{middle}=2\\int_{0}^{L/2}A(1-e^{-\\frac{t^{2}}{\\alpha^{2}})dt\\,-\\,2\\int_{0}^{L/2}A(1-e^{-(t/\\alpha)^{\\beta}})dt\\)\n' +
      '\n' +
      '\\[\\Delta E_{tail}=2\\int_{L/2}^{\\infty}Ae^{-\\frac{t^{2}}{\\alpha^{2}}dt-2\\int_{L/2}^{\\infty}Ae^{-(t/\\alpha}^{\\beta}dt\\}\n' +
      '\n' +
      '지수항의 차이로 \\(\\text{err}(t)\\)을 정의하자:\n' +
      '\n' +
      '\\[\\text{err}(t)=e^{-\\frac{t^{2}}{\\alpha^{2}}}-e^{-(t/\\alpha)^{\\beta}}.\\]\n' +
      '\n' +
      '가우시안 근사치와 GEF 근사치에 대한 중간 오차항의 차이, \\(\\Delta E_{middle}\\)는 \\(\\text{err}(t)\\)을 이용하여 나타낼 수 있다.\n' +
      '\n' +
      '\\[\\Delta E_{middle}=2A\\int_{0}^{L/2}\\text{err}(t)\\,dt.\\]\n' +
      '\n' +
      '적분의 사다리꼴 근사를 사용하면, 이것은 다음과 같이 간단해진다:\n' +
      '\n' +
      '\\[\\Delta E_{middle}\\approx LA\\,\\text{err}(L/2)=LA\\left(e^{-\\frac{L^{2}}{4\\alpha ^{2}}}-e^{-(L/2\\alpha)^{\\beta}}\\right.)\\\\\\Delta E_{middle}\\approx LA\\,\\text{err}(L/2)=LA\\left(e^{-\\frac{L^{2}}{4\\alpha ^{2}}}-e^{-(L/2\\alpha)^{\\beta}}\\right.)\\\\\\delta E_{middle}\\approx LA\\,\\text{err}(L/2)=LA\\left(e^{-\\frac{L^{2}}{4\\alpha ^{2}}}}-e^{-(L/2\\alpha)^{\\beta}}\\right.)\\.\n' +
      '\n' +
      '음수지수가 단조적으로 감소하고 \\(\\Delta E_{middle}\\)이 항상 양수임을 확인하기 위해 \\(L/2\\)와 \\(\\alpha\\):*의 관계를 바탕으로 \\(\\beta\\)을 선택한다. \\(\\frac{L}{2}>\\alpha\\)(즉, \\(\\frac{L}{2\\alpha}>1\\)), \\(\\beta>2\\)은 \\(e^{-(L/2\\alpha)^{\\beta}<e^{-\\frac{L^{2}}{4\\alpha^{2}}}}}}를 선택한다.\n' +
      '* \\(\\frac{L}{2}<\\alpha\\)(즉 \\(\\frac{L}{2\\alpha}<1\\)), \\(0<\\beta<2\\)을 선택하면 \\(e^{-(L/2\\alpha)^{\\beta}<e^{-\\frac{L^{2}}{4\\alpha^{2}}}}}}의 결과가 된다.\n' +
      '\n' +
      '따라서 \\(\\Delta E_{middle}\\)은 \\(\\beta\\)을 적절히 선택함으로써 항상 양으로 만들 수 있으며, 이는 간격 \\([-L/2,L/2]\\)에서 GEF 근사의 오차가 항상 가우시안 근사의 오차보다 작음을 의미한다. 마찬가지로 꼬리오차의 차이도 \\(\\Delta E_{tail}\\)은 \\(\\beta\\)의 적절한 선택에 의해 양수가 될 수 있으며, 이는 총 오차 \\(E_{GEF}\\)이 \\(E_{G}\\)보다 엄격히 작음을 의미한다. 이상으로 증명을 마칩니다.\n' +
      '\n' +
      '구배 기반 1차원 혼합물의 수치 시뮬레이션\n' +
      '\n' +
      '**목적.** 이 수치 시뮬레이션의 주요 목적은 다양한 1차원(1D) 신호 유형을 표현하는데 있어서 일반화된 지수 모델의 유효성을 평가하는 것이다. 이 평가는 실제 세계에서 사용할 수 있는 기본 토폴로지의 비배타적 목록을 구성할 수 있는 정사각형, 삼각형, 포물선, 반 정현파, 가우스 및 지수 함수의 특성을 구현하기 위해 생성된 합성 신호에 모델을 피팅하여 수행되었다.\n' +
      '\n' +
      '**시뮬레이션 설정.** 실험 프레임워크는 가우시안, DoG(Difference of Gaussians), LoG(Laplacian of Gaussian) 및 일반화된 혼합물 모델과 같은 서로 다른 기능의 혼합물을 사용하여 1D 신호를 근사하도록 설계된 PyTorch에서 구현된 일련의 매개변수 모델을 기반으로 했다. 각 모델은 평균, 분산(또는 척도) 및 가중치에 대한 매개변수를 구성했으며 일반화된 모델은 가우스 함수의 지수를 제어하기 위해 추가 매개변수 \\(\\beta\\)를 통합했다.\n' +
      '\n' +
      '**모델.** 여기서, 우리는 실제 신호 형태를 근사화하는 데 사용되는 혼합물 모델을 설명한다.\n' +
      '\n' +
      '***가우시안 혼합 모델(GMM):** GMM은 여러 개의 가우시안 함수를 조합하며, 각각은 평균(\\(\\mu_{i}\\)), 분산(\\(\\sigma_{i}^{2}\\)), 및 가중치(\\(w_{i}\\))로 정의된다. \\(N\\) 가우스 함수의 집합에 대해, 혼합물 모델 \\(g(x)\\)은 다음과 같이 표현될 수 있다. \\[g(x)=\\sum_{i=1}^{N}w_{i}\\exp\\left(-\\frac{(x-\\mu_{i})^{2}}{2\\sigma_{i}^{2}+ \\epsilon}\\right), \\(\\epsilon\\)은 0으로 나눗셈을 피하기 위해 작은 상수이며, \\(\\epsilon=1e-8\\).\n' +
      '***Gaussians(DoG) 혼합 모델의 차이:** DoG 혼합 모델은 고정된 분산 비율 \\(\\nu\\)을 갖는 두 가우시안 함수의 차이를 나타내는 요소로 구성된다. \\(d)\\(x)=\\sum_{i=1}^{N}w_{i}D_{i}\\]\\[D_{i}= \\left(\\exp\\left(-\\frac{(x-\\mu_{i})^{2}}igma_{i}+ \\epsilon}\\right)-\\exp\\left(-\\frac{(x-\\mu_{i})^{2}}{2}/\\nu)+ \\epsilon}\\right),\\(\\nu\\)은 4로 고정된다.\n' +
      '***Laplacian of Gaussian (LoG) Mixture Model:** LoG mixture model은 일련의 Laplacian of Gaussian function에 의해 형성되며, 각각은 평균(\\(\\mu_{i}\\)), 척도(\\(\\gamma_{i}\\)), 및 가중치(\\(w_{i}\\))에 의해 정의된다. 혼합물 모델 \\(l(x)\\)은 \\[l(x)=\\sum_{i=1}^{N}w_{i}\\left(-\\frac{(x-\\mu_{i})^{2}}{\\gamma_{i}^{2}+1\\right)\\exp\\left(-\\frac{(x-\\mu_{i})^{2}}{2\\gamma_{i}^{2}+\\epsilon}\\right),\\] (15)\n' +
      '***일반화된 혼합 모델:** 이 모델은 형상 파라미터 \\(\\beta\\)를 도입하여 가우시안 혼합을 일반화한다. 모델 \\(h(x)\\)의 각 성분은 \\[h(x)=\\sum_{i=1}^{N}w_{i}\\exp\\left(-\\frac{|x-\\mu_{i}|^{\\beta}}{2\\sigma_{i}^{2}+ \\epsilon}\\right), \\(x)=\\sum_{i=1}^{N}w_{i}\\exp\\left(-\\frac{|x-\\mu_{i}|^{\\beta}}{i}}}}{2}+ \\epsilon}\\right), \\(x)=\\sum_{i=1}^{N}w_{i}\\exp\\left(-\\frac{|x-\\mu_{i}|^{beta}}{i}}}}{2}+\\epsilon}\\right), \\(x)=\\sum_{i=1}^{N}w_{i}\\exp\\left(-\\frac{|x-\\mu_{i} \\(\\beta=2\\)을 고정하면 식은 식이다. (16)은 식에 있는 것으로 감소한다. (13).\n' +
      '\n' +
      '**모델 구성** 모델은 다양한 수의 구성 요소로 구성되었으며 \\(N\\)\\(N=\\{2,5,8,10,15,20,50,100\\}\\)을 사용하여 테스트를 수행했다. 구성요소의 가중치는 양수이거나 제한이 없을 수 있습니다. 일반화된 모델의 경우 \\(\\beta\\) 파라미터를 학습할 수 있었다.\n' +
      '\n' +
      '**훈련 절차.** 각 모델은 평균 제곱 오차 손실 함수가 있는 아담 최적화기를 사용하여 훈련되었다. 입력 \\(x\\)은 합성 신호의 영역을 나타내는 선형 간격 텐서이고, 타겟 \\(y\\)은 \\(x\\)의 각 지점에서 신호의 값이다. 훈련은 정해진 수의 에폭 동안 진행되었으며, 훈련 종료 시 손실이 기록되었다.\n' +
      '\n' +
      '**데이터 생성.** 합성 1D 신호는 주어진 데이터 크기와 신호 폭으로 지정된 범위에 걸쳐 다양한 신호 유형에 대해 생성되었다. 신호는 혼합 모델을 훈련하기 위한 지상 진리로 사용되었다. 실험에 사용된 Ground truth 신호는 신호처리 알고리즘을 평가하기 위한 벤치마크 역할을 하는 1차원(1D) 함수이다. 각 신호 유형은 원점을 중심으로 지정된 너비 내에서 정의되며, 이 간격 외부의 값은 0이다(도 12 참조). 파라미터 폭\\(\\sigma\\)은 신호의 0이 아닌 부분의 유효 스팬을 지시한다. 6가지 신호 유형을 다음과 같이 정의한다. **정사각형 신호:** 정사각형 신호는 구간 내에서 값이 1이고 다른 곳에서는 0인 이진 함수이다. 수학적으로 \\[f_{\\text{square}}(x)=\\begin{cases}1&\\text{if}-\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\\\0&\\text{otherwise.\\ end{cases}\\] (17) 이 푸리에 변환은 \\[\\text{FT}\\\\text{Square Wave}\\}(f)=\\text{sinc}\\left(\\frac{f\\cdot\\sigma}{\\pi}\\right)\\] (18)\n' +
      '2. **Triangle Signal:** 이 신호는 간격의 왼쪽 가장자리에서 중앙으로 선형적으로 증가하고 오른쪽 가장자리로 대칭적으로 감소하여 삼각형 모양을 형성한다. \\[f_{\\text{triangle}}(x)=\\begin{cases}\\frac{\\sigma}{2}-|x|&\\text{if}-\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\\\0&\\text{otherwise.\\frac{\\sigma}{2}로 정의된다. end{cases}\\] (19) Its Fourier Transform is \\[\\text{FT}\\{\\text{Triangle Wave}\\}(f)=\\left(\\text{sinc}\\left(\\frac{f\\cdot\\sigma}{2\\pi}\\right)\\right)^{2}\\] (20)\n' +
      '3. **Parabolic Signal:** 이 신호는 간격 내에서 아래쪽으로 향하는 포물선을 형성하고, 그 표현식 \\[f_{\\text{parabolic}}(x)=\\begin{cases}(\\frac{\\sigma}{2}^{2}-x^{2}&\\text{if}-\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\0&\\text{otherwise.\\frac{\\sigma}{2} end{cases}\\] (21) 포물선 신호의 푸리에 변환은 \\[\\text{FT}\\text{Parabolic Wave}\\}(f)=\\frac{3\\cdot\\left(\\text{sinc}\\left(\\frac{f\\cdot\\sigma}{2\\pi}\\right)\\right)^{2}}{\\pi^{2}\\cdot f^{2}}\\] (22)\n' +
      '4. ** 반정현파 신호:** 정현파의 반주기가 구간 내에 포함되어 진폭이 0으로 시작되고 종료된다. 그 공식은 \\[f_{text{half\\_sinusoid}}(x)=\\begin{cases}\\begin{cases}\\sin\\left((x+\\frac{\\sigma}{2})\\frac{\\pi}{\\sigma}\\right) &\\text{if}-\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\\\0&\\text{otherwise.\\frac{\\sigma}\\text{if}\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\0&\\text{otherwise. end{cases}\\] (23) 이 푸리에 변환은 \\[\\text{FT}\\text{Half Sinusoid}\\}(f)=\\begin{cases}\\frac{\\sigma}2}&\\text{if}f=0\\\\frac{\\sigma\\cdot\\sin(\\pi\\cdot f\\cdot\\sigma}\\pi^{2}\\cdot f^{2}&\\text{ otherwise}\\end{cases}\\] (24)\n' +
      '\n' +
      '도 12: **Commony Signals used and their Fourier Transforms**. 가우시안 함수는 저역 통과 대역폭인 반면, 날카로운 에지를 갖는 정사각형 및 삼각형과 같은 공통 신호들은 무한대의 대역폭을 가지므로, 저역 통과 주파수 대역폭을 갖는 혼합물들로 피팅되기 어렵게 한다(_e.g._가우시안 혼합물들, 가우스 분할[27]로 나타냄).\n' +
      '\n' +
      '5. **지수 신호:** 원점을 중심으로 지수 감쇠를 나타내는 이 신호는 \\[f_text{exponential}(x)=\\begin{cases}\\exp(-|x|) &\\text{if}-\\frac{\\sigma}{2}<x<\\frac{\\sigma}{2},\\0&\\text{otherwise.}\\end{cases}\\] (25) 지수 신호에 대한 푸리에 변환은 \\[\\text{FT}\\text{exponential}(f)=\\frac{\\sigma}{f^{2}+\\left(\\frac{\\sigma}{2}\\right)^{2}\\] (26)\n' +
      '6. **가우시안 신호:** 다른 것과 달리, 가우시안 신호는 특정 간격 내에서 경계되지 않고 대신 \\(x\\)의 전체 범위에 걸쳐 확장되며, 그 진폭은 가우시안 분포에 의해 지배된다. \\[f_{\\text{Gaussian}}(x)=\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)으로 주어진다.\\] (27) 가우시안 신호의 푸리에 변환은 또한 가우시안이고, 표준편차\\(\\sigma\\)의 맥락에서 \\[\\text{FT}\\text{Gaussian}\\}(f)=\\sqrt{2\\pi}\\cdot\\sigma\\cdot\\exp\\left(-2\\pi^{2}\\sigma^{2}f^{2}\\right)\\] (28)\n' +
      '\n' +
      '도 12에 도시된 바와 같이, 가우시안 함수는 저역 통과 대역을 갖는 반면, 날카로운 에지를 갖는 정사각형 및 삼각형과 같은 신호들은 무한대의 대역폭을 가져, 저역 통과 주파수 대역폭을 갖는 혼합물들(_e.g_. 가우시안 혼합물들, 가우시안 스플래팅[27])에 대해 도전적이다.\n' +
      '\n' +
      '각 신호는 실험의 프레임워크 내에서 계산 조작 및 분석을 용이하게 하기 위해 PyTorch 텐서를 사용하여 이산 지점에서 샘플링된다. 그림 14,15,16,17,18,19,20,21,22,23,24 및 25에서 혼합물 _vs_에서 양의 가중치가 사용될 때 모든 혼합물을 관심 있는 모든 다른 신호 유형에 맞추는 예를 보여준다. 상기 수학식들의 조합들에서 실수 가중치를 허용할 때. 정사각형 신호가 sinc 함수에 의해 알려진 무한한 대역폭을 갖는 반면, 낮은 통과 대역폭을 갖는 가우시안들에 대해 얼마나 날카로운 에지들이 도전을 구성하는지에 주목한다[25].\n' +
      '\n' +
      '**Loss Evaluation.** 훈련 후 손실값을 기준으로 모델의 성능을 평가하였다. 또한 생성된 도표를 통해 입력 신호를 나타내는 모델의 능력을 시각적으로 검사했다. 결과의 분산을 설명하기 위해 구성당 여러 실행이 실행되었다.\n' +
      '\n' +
      '**안정성 평가.** 모델 안정성 및 성능은 다양한 신호 유형 및 혼합물 모델을 포함하는 일련의 실험을 사용하여 평가되었다. 각 모델은 모델 출력과 지상진리 신호 사이의 평균 제곱 오차(MSE) 손실을 최소화하는 것을 목표로 미리 정의된 신호 유형(정사각형, 삼각형, 포물선, 반 정현파, 가우스 및 지수)에 따라 생성된 1D 신호에 대해 훈련되었다. 혼합물 모델의 성분 수(\\(N\\))는 값 세트에 따라 다양했으며 모델도 양의 가중치에 구속되는지 여부에 따라 구별되었다. 종합적인 평가를 위해 훈련 과정의 변동성을 설명하기 위해 각 구성을 여러 번 실행(구성당 20회 실행)했다. 이러한 실행 동안 교육이 NaN 손실을 초래한 사례의 수는 안정성 문제의 지표로 기록되었다. 각 모델의 안정성은 성공적인 훈련 실행의 백분율(\\frac{\\text{Total\\;Rans}-\\text{NaN\\;Loss\\;Counts}{\\text{Total\\;Rans}\\times 100\\%\\)에 의해 정량화되었다. 손실이 NaN으로 분기되었기 때문에 실패한 실험은 실패했다. 최적화에서 이러한 전형적인 수치 불안정성은 0에 가까워질 수 있는 분산을 학습하여 지수 공식(Eq)을 생성한 결과이다. (10))을 극소수로 나눈다.\n' +
      '\n' +
      '성공적인 실행으로 인한 평균 MSE 손실은 모델 성능의 척도를 제공하기 위해 계산되었다. 이러한 실험의 결과를 도표화하여 각 신호 유형에 대한 구성 요소의 수와 모델의 안정성 및 손실 사이의 관계를 보여준다.\n' +
      '\n' +
      '** 시뮬레이션 결과.** 수행된 분석에서 양성 및 비양성 가중치를 갖는 다양한 혼합물 모델의 손실 및 안정성을 모두 상이한 형상을 갖는 신호에 대해 평가하였다. 그림 13에 묘사된 바와 같이 양의 가중치를 가진 가우시안 혼합 모델은 성분 수에 걸쳐 일관되게 가장 낮은 손실을 산출했으며, 이는 정사각형 특징의 효과적인 근사치를 나타낸다. 반대로 가우스 및 일반 모델에서 비양성 가중치가 더 높은 손실을 보여 가중치 사인온 모델 성능의 중요성을 강조했다. 이러한 발견은 낮은 손실과 높은 안정성을 모두 달성하는 데 있어 모델 복잡성과 무게 제약 사이의 복잡한 균형을 강조한다. GEF는 구성 요소가 거의 없는 정사각형을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다. 또한, 양의 중량 혼합물은 더 적은 수의 성분으로 더 낮은 손실을 달성하는 경향이 있지만 더 많은 수의 성분에 대해서는 덜 안정적이다.\n' +
      '\n' +
      '도 13: **다양한 혼합물의 수치 시뮬레이션 결과.** 다양한 구성 요소 및 중량 구성(양성 _vs_)에 걸쳐 구배 기반 최적화기로 최적화된 다양한 혼합물 모델에 대한 평균 손실 및 안정성(성공적인 실행의 백분율)의 비교를 보여준다. real weights) on various signal types (a-f.\n' +
      '\n' +
      '도 14: ** 양의 중량 혼합물을 갖는 피팅 스퀘어의 수치 시뮬레이션 예(N=2, 5, 8, 및 10)**. 우리는 양의 가중치 혼합이 있는 정사각형 신호에 대한 몇 가지 적합 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 정사각형을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 15: ** 실제 중량 혼합물과 피팅 스퀘어의 수치 시뮬레이션 예(N = 2, 5, 8, 10)**. 우리는 실수 가중치 혼합이 있는 정사각형 신호에 대한 몇 가지 적합 예를 보여준다(음수일 수 있음). 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 정사각형을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 16: **포지티브 가중치 혼합물을 갖는 피팅 포물선의 수치 시뮬레이션 예(N=2, 5, 8, 및 10)**. 우리는 양의 가중치 혼합물이 있는 포물선 신호에 대한 몇 가지 적합한 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 포물선을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 17: ** 실제 중량 혼합물을 갖는 피팅 포물선의 수치 시뮬레이션 예(N=2, 5, 8, 및 10)**. 우리는 실제 가중치 혼합물(음수일 수 있음)을 사용하여 포물선 신호에 대한 몇 가지 적합 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 포물선을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '도 18: ** 양의 가중치 혼합물을 갖는 지수를 피팅하는 수치 시뮬레이션 예(N = 2, 5, 8, 및 10)**. 우리는 양의 가중치 혼합물이 있는 지수 신호에 대한 몇 가지 적합한 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 지수 적합에 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 19: ** 실제 중량 혼합물(N=2, 5, 8, 및 10)을 갖는 지수 적합의 수치 시뮬레이션 예. 우리는 실수 가중치 혼합물(음수일 수 있음)을 사용하여 지수 신호에 대한 몇 가지 적합 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 지수 적합에 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.**\n' +
      '\n' +
      '도 20: ** 양의 무게 혼합체(N=2, 5, 8, 10)**로 삼각형을 맞추는 수치 시뮬레이션 예. 우리는 양의 가중치 혼합이 있는 삼각형 신호에 대한 몇 가지 적합 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 삼각형을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 21: **실중량 혼합체(N=2, 5, 8, 10)**로 삼각형을 맞추는 수치 시뮬레이션 예. 우리는 실수 가중치 혼합이 있는 삼각형 신호에 대한 몇 가지 적합 예를 보여준다(음수일 수 있음). 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 삼각형을 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 22: ** 양의 가중치 혼합물을 갖는 가우시안들을 피팅하는 수치 시뮬레이션 예(N=2, 5, 8, 및 10)**. 우리는 양의 가중치 혼합물을 갖는 가우시안 신호에 대한 몇 가지 피팅 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 성분이 적은 가우시안 피팅에 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 성분에 대해 더 안정적이다.\n' +
      '\n' +
      '도 23: **실제 중량 혼합체(N=2, 5, 8, 10)**로 가우시안들을 피팅하는 수치 시뮬레이션 예. 우리는 실제 가중치 혼합물(음수일 수 있음)을 갖는 가우시안 신호에 대한 몇 가지 피팅 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 성분이 적은 가우시안 피팅에 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 성분에 대해 더 안정적이다.\n' +
      '\n' +
      '도 24: **정현파를 양의 가중치 혼합물(N=2, 5, 8, 10)**로 피팅하는 수치 시뮬레이션 예. 우리는 양의 가중치 혼합물이 있는 반 정현파 신호에 대한 몇 가지 적합한 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 반 정현파를 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '도 25: **리얼 웨이트 믹스처(N=2, 5, 8, 10)**를 갖는 하프 정현파를 피팅하는 수치 시뮬레이션 예. 우리는 실수 가중치 혼합물(음수일 수 있음)이 있는 반 정현파 신호에 대한 몇 가지 적합 예를 보여준다. 왼쪽에서 오른쪽으로 사용되는 네 가지 혼합물은 가우시안, LoG, DoG 및 일반 혼합물이다. 위에서 아래로: N = 2, 8 및 10 성분입니다. 최적화된 개별 구성 요소는 녹색으로 표시됩니다. 일부 예는 가우시안 및 GEF 혼합물 모두에서 수치 불안정성으로 인해 최적화에 실패한다. GEF는 구성 요소가 거의 없는 반 정현파를 맞추는 데 매우 효율적인 반면 LoG 및 DoG는 더 많은 수의 구성 요소에 대해 더 안정적이다.\n' +
      '\n' +
      '### B. 유전자화된 지수 분할 상세\n' +
      '\n' +
      '근사 GES 래스터화에서 경계 시점 의존 오차에 대한 상위 경계\n' +
      '\n' +
      'Eq에 정의된 일반 지수 분할(GES) 함수가 주어집니다. (2) 및 식에 의해 주어진 우리의 근사 래스터화. (3), 4 및 5에서 GES 렌더링에서 근사치의 오차에 대한 상한을 설정하려고 한다. Eq로부터 각 개별 픽셀에 누적된 오차를 추정하는 것은 매우 어렵기 때문이다. (3) 모든 통과 광선의 에너지에 영향을 미치는 각 스플래팅 성분의 오차를 직접 추정하고자 한다.\n' +
      '\n' +
      '그림 6과 같이 대칭적인 성분을 갖는 간단한 2D 경우를 생각해 보자. 스케일링된 가우시안 성분과 원래의 GES 성분 사이의 오차는 광선의 에너지 손실과 관련이 있으며, 스케일링된 가우시안 영역의 면적 차이와 영역 사이의 _ratio_\\(\\eta\\)를 간단히 추정함으로써 나타낼 수 있다. 여기서는 각 성분의 면적에 대한 \\(\\eta\\)의 상한을 추정할 수 있음을 보인다.\n' +
      '\n' +
      '최악의 경우 \\(\\beta\\rightarrow\\infty\\)일 때, 우리는 근사치에 대한 두 가지 비중첩 조건을 고려한다: 정사각형이 외형인 경우와 원형이 정사각형을 덮는 경우이다. 정사각형의 변길이는 전자의 경우 \\(2r\\), 후자의 경우 \\(2r/\\sqrt{2}\\)이다. 원의 반지름 \\(r\\)은 Eq로부터 유효 투영 분산 \\(\\alpha\\)에 의해 결정된다. (4). 측면 길이가 \\(2r\\)인 정사각형과 반지름이 \\(r\\)인 원에 대해, 우리는 \\(A_{text{square}}=4r^{2},A_{text{circle}=\\pi r^{2}\\)을 갖는다. 측면 길이가\\(2r/\\sqrt{2}\\)인 정사각형에 대해 면적은\\(A_{\\text{square, covered}}=2r^{2}\\)이다.\n' +
      '\n' +
      '면적차 \\(\\Delta A\\)는:\n' +
      '\n' +
      '\\[\\Delta A_{\\text{square larger}}=A_{\\text{square}}-A_{\\text{circle}=4r^{2}-\\pi r^{2}, \\tag{29}\\] \\[\\Delta A_{\\text{circle larger}=A_{\\text{circle}}-A_{\\text{square, covered}}=\\pi r^{2}-2r^{2}. \\tag{30}\\]\n' +
      '\n' +
      '[\\(\\eta\\)]로 표시된 내부 모양의 면적에 대한 면적 차이의 비율은 다음과 같이 제한됩니다.\n' +
      '\n' +
      '{A_{\\text{square larger}}=\\frac{4r^{2}-\\pi r^{2}}{\\pi r^{2}\\approx 0.2732, \\tag{31}\\\\[\\eta_{\\text{circle larger}}=\\frac{\\delta A_{\\text{circle larger}}{A_{\\text{2}-2r^{2}}\\approx 0.3634. \\tag{32}\\approx 0.3634.\n' +
      '\n' +
      'GND [14]의 PDF 정규화 제약으로 인해 Eq에서 근사치를 따랐다. (4)와 5는 항상 \\(\\eta_{\\text{square larger}\\leq\\eta\\leq\\eta_{\\text{circle larger}\\)을 보장한다. 따라서, \\(\\beta\\)에 기초한 근사 분산 스케일링을 사용할 때 목표 비율 \\(\\eta\\)은 \\(0.2732\\leq\\eta\\leq 0.3634\\) 범위 내에 있어야 한다. 이것은 최악의 경우에, 우리의 GES 근사가 스플래팅 컴포넌트들을 통과하는 모든 광선들의 손실된 에너지에서 36.34% 에너지 에러를 초래할 것이라는 것을 의미한다. 실제로, 많은 수의 컴포넌트들 및 모든 스플래팅 컴포넌트들의 작은 스케일로 인해 에러는 훨씬 더 작아질 것이다.\n' +
      '\n' +
      '#### Implementation Details\n' +
      '\n' +
      'Eq의 DoG에 유의하십시오. (7)은 \\(\\sigma_{2}\\)이 클 때 매우 클 것이므로, Eq에서 손실을 계산하기 전에 그라운드 진리 이미지를 요인 \'\\(\\text{scale}_{\\text{im,freq}\\)\'으로 다운샘플링하고 마스크 \\(M_{\\omega}\\)을 유사하게 업샘플링한다. (8). GES(Generalized Exponential Splatting) 접근법의 구현에서 성능을 최적화하기 위해 여러 하이퍼파라미터를 미세 조정했다. 다음 목록은 구현에서 각 매개변수의 특정 값과 목적을 자세히 설명합니다.\n' +
      '\n' +
      '* 반복: 알고리즘은 총 40,000번의 반복에 대해 실행되었다.\n' +
      '* 학습률:\n' +
      '* 초기 위치 학습률((\\(\\text{lr}_{\\text{pos, init}}\\))은 0.00016으로 설정하였다.\n' +
      '* 최종 위치 학습률((\\text{lr}_{\\text{pos, final}}\\))은 0.0000016으로 감소하였다.\n' +
      '* 학습율 지연 곱셈기(\\(\\text{lr}_{\\text{delay\\,mut}}\\))는 0.01로 설정하였다.\n' +
      '* 위치 학습률에 대한 최대 단계(\\(\\text{lr}_{\\text{pos, max steps}}\\))는 30,000으로 설정되었다.\n' +
      '* 기타 학습률:\n' +
      '* 특징 학습률((\\(\\text{lr}_{\\text{feature}}\\))은 0.0025였다.\n' +
      '* 불투명도 학습률(\\(\\text{lr}_{\\text{opacity}}\\))은 0.05였다.\n' +
      '* 모양 및 회전 학습률(\\(\\text{lr}_{\\text{shape}\\) 및 \\(\\text{lr}_{\\text{rotation}\\))은 모두 0.001로 설정되었다.\n' +
      '* 스케일링 학습률((\\(\\text{lr}_{\\text{scaling}}\\))은 0.005였다.\n' +
      '* 밀도 및 프루닝 파라미터:\n' +
      '* 밀집점(\\(\\text{percent}_{\\text{dense}\\))의 백분율은 0.01이었다.\n' +
      '* 불투명도 및 형상 가지치기 임계값은 0.005로 설정되었다.\n' +
      '* 손실 가중치 및 간격:\n' +
      '* SSIM 손실 중량((\\(\\lambda_{\\text{ssim}}\\))은 0.2였다.\n' +
      '* 치밀화, 불투명도 재설정, 형상 재설정, 형상 가지치기 간격은 각각 100회, 3000회, 1000회, 100회 반복으로 설정하였다.\n' +
      '* 치밀화 상세:\n' +
      '* 치밀화는 반복 500에서 시작하여 반복 15,000까지 계속되었다.\n' +
      '* 치밀화를 위한 구배 임계값은 0.0003으로 설정되었다.\n' +
      '* 이미지 라플라시안 파라미터:\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:32]\n' +
      '\n' +
      '도 27: **New View Synthesis에 대한 Densification Threshold의 Ablation Study.** MipNeRF 데이터세트의 모든 장면에 걸쳐 평균화된, 우리의 방법에 대한 재구성 품질(LPIPS) 및 파일 크기(MB)에 대한 Densification Threshold의 영향. 밀도화 임계값은 파일 크기와 품질 모두에 상당한 영향을 미친다는 것을 알 수 있다. 전반적으로, 우리의 방법은 유사하거나 심지어 약간 개선된 성능으로 가우시안 스플래팅보다 더 작은 장면을 생성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l r r r r} \\hline \\hline \\(\\lambda_{\\text{freq}}\\) & Method & PSNR & LPIPS & SSIM & Size \\\\ \\hline \\multicolumn{6}{c}{_Deep Blending_} \\\\ \\hline \\multirow{2}{*}{0.05} & GES & 29.58 & 0.252 & 0.900 & 431 \\\\  & GES (fixed \\(\\beta=2\\)) & 29.53 & 0.251 & 0.901 & 433 \\\\ \\hline \\multirow{2}{*}{0.10} & GES & 29.54 & 0.252 & 0.901 & 428 \\\\  & GES (fixed \\(\\beta=2\\)) & 29.61 & 0.252 & 0.901 & 435 \\\\ \\hline \\multirow{2}{*}{0.50} & GES & 29.66 & 0.251 & 0.901 & 397 \\\\  & GES (fixed \\(\\beta=2\\)) & 29.61 & 0.252 & 0.901 & 437 \\\\ \\hline \\multirow{2}{*}{0.90} & GES & 27.21 & 0.259 & 0.899 & 366 \\\\  & GES (fixed \\(\\beta=2\\)) & 29.62 & 0.252 & 0.901 & 434 \\\\ \\hline \\multicolumn{6}{c}{_MipNeRF_} \\\\ \\hline \\multirow{2}{*}{0.05} & GES & 27.08 & 0.250 & 0.796 & 405 \\\\  & GES (fixed \\(\\beta=2\\)) & 27.05 & 0.250 & 0.795 & 411 \\\\ \\hline \\multirow{2}{*}{0.10} & GES & 27.05 & 0.250 & 0.795 & 403 \\\\  & GES (fixed \\(\\beta=2\\)) & 27.05 & 0.250 & 0.796 & 412 \\\\ \\hline \\multirow{2}{*}{0.50} & GES & 26.97 & 0.252 & 0.794 & 376 \\\\  & GES (fixed \\(\\beta=2\\)) & 27.09 & 0.250 & 0.796 & 415 \\\\ \\hline \\multirow{2}{*}{0.90} & GES & 25.82 & 0.255 & 0.792 & 364 \\\\  & GES (fixed \\(\\beta=2\\)) & 27.08 & 0.250 & 0.795 & 413 \\\\ \\hline \\multicolumn{6}{c}{_Tanks and Temples_} \\\\ \\hline \\multirow{2}{*}{0.05} & GES & 23.49 & 0.196 & 0.837 & 251 \\\\  & GES (fixed \\(\\beta=2\\)) & 23.55 & 0.196 & 0.836 & 255 \\\\ \\hline \\multirow{2}{*}{0.10} & GES & 23.54 & 0.196 & 0.837 & 247 \\\\  & GES (fixed \\(\\beta=2\\)) & 23.53 & 0.196 & 0.837 & 255 \\\\ \\hline \\multirow{2}{*}{0.50} & GES & 23.35 & 0.197 & 0.836 & 221 \\\\  & GES (fixed \\(\\beta=2\\)) & 23.65 & 0.196 & 0.837 & 256 \\\\ \\hline \\multirow{2}{*}{0.90} & GES & 22.65 & 0.200 & 0.834 & 210 \\\\  & GES (fixed \\(\\beta=2\\)) & 23.50 & 0.197 & 0.836 & 256 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: \\(\\lambda_{\\text{freq}\\)의 **Ablation.** \\(\\lambda_{\\text{freq}\\)의 다양한 값에 대한 성능(PSNR, LPIPS, SSIM)의 비교를 보여준다. GES에서 \\(\\lambda_{\\text{freq}}\\)을 증가시키면 파일의 크기가 감소하지만 성능에 영향을 미칠 수 있다는 점에 유의한다. 성능 향상과 파일 크기 감소 사이의 중간 지점으로 \\(\\lambda_{\\text{freq}=0.5\\)을 선택했다.\n' +
      '\n' +
      '도 26: **3D 생성을 위한 시각화**. 우리는 Realfusion15(_left_) 및 NeRF4 데이터세트(_middle_)로부터 GES에 의해 선택된 생성된 예를 보여준다. 또한, 초밥으로 만든 자동차와 천문대의 미켈란젤로 스타일 조각상이라는 두 가지 텍스트 프롬프트를 선택한 다음 StableDiffusion-XL[49]을 사용하여 참조 이미지를 생성한 후 GES를 사용한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:34]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '도 30 : **Gaussians** _vs._*의 수렴도 GES** 우리는 더 많은 훈련의 감소된 수익을 검사하기 위해 훈련이 최대 50K 반복을 계속하는 경우 GES와 가우시안 모두의 수렴 도표의 예를 보여준다. 융합을 위해 더 많은 반복이 필요함에도 불구하고 GES는 스플래팅 성분의 수가 적기 때문에 가우시안보다 더 빨리 훈련한다.\n' +
      '\n' +
      '도 31: **Frequency-Modulated Image Mask.** 왼쪽 상단의 입력 예제 이미지에 대해, 우리는 Sec.4.3에서 사용된 주파수 손실 마스크 \\(M_{\\omega}\\)의 예를 다른 수의 목표 정규화 주파수 \\(\\omega\\)에 대해 보여준다(\\(\\omega=0\\%\\) 낮은 주파수의 경우 \\(\\omega=100\\%\\) 높은 주파수의 경우 \\(\\omega=100\\%\\). 이 마스킹된 손실은 GES가 특정 주파수 대역을 학습하는 데 도움이 된다. 높은 주파수에 대한 라플라시안 필터 민감도로 인해, \\(0<\\omega\\leq 50\\%\\)에 대한 마스크는 \\(50<\\omega\\leq 100\\%\\)에 대한 \\(1-M_{\\omega}\\)으로 정의된다. 이는 이미지의 모든 부분이 마스크들 중 하나(M_{\\omega}\\)에 의해 커버되는 반면, 최적화가 진행됨에 따라 세부사항들에 더 집중될 것을 보장한다.\n' +
      '\n' +
      '그림 32: **방법 간 비교 시각화.** 표시됨은 제안된 방법과 각각의 지상 진실 이미지와 함께 확립된 기술 간의 나란히 비교이다. 묘사된 장면은 Mip-NeRF360 데이터 세트의 자전거, 가든, 스텀프, 카운터 및 룸, 딥 블렌딩 데이터 세트의 플레이룸 및 닥터존슨, 탱크&템플의 트럭 및 트레인과 같이 정렬된다. 렌더링 품질의 미묘한 차이는 줌인 디테일을 통해 강조된다. GES와 가우시안 간의 차이는 거의 동일한 PSNR(GES가 50% 적은 메모리를 필요로 함에도 불구하고)을 갖기 때문에 보기 어려울 수 있다.\n' +
      '\n' +
      '도 33: ** 상이한 반복 번호에 대한 MipNeRF 360에 대한 상세한 장면별 결과.** 우리는 GES의 MIPNeRF 360 데이터세트 [5]의 모든 장면에 대한 PSNR, LPIPS, SSIM 및 파일 크기, 결과를 보여주고 GES의 _exact same_ 하이퍼파라미터 및 상이한 반복 횟수로 가우시안 분할 [27] 기준선을 재실행한다.\n' +
      '\n' +
      '그림 34: **Frequency-Modulated Loss Effect.** 주파수-Modulated Image Loss\\(\\mathcal{L}_{\\omega}\\)이 새로운 뷰 합성에 미치는 성능에 미치는 영향을 보여준다. 이 \\(\\mathcal{L}_{\\omega}\\)을 추가하면 큰 콘트라스트가 존재하는 영역 또는 매끄러운 배경이 렌더링되는 영역에서 최적화가 어떻게 개선되는지 주목하라.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>