<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '더 크고 여러 복잡한 입력 소스를 동시에 처리하기 위해 규모로 작동해야 합니다. 따라서, 다양한 데이터세트 크기 및 계산 제약에 따라 조정하면서 효과적으로 업스케일 및 다운스케일을 모두 할 수 있는 DLRS의 필요성이 무엇보다 중요하다. 이러한 확장성은 "스케일링 법률"(Kaplan et al., 2020)로 알려진 것에 포함된다.\n' +
      '\n' +
      '현재까지 DLRS 업스케일링의 주요 경향은 _sparse scaling_를 통해, 즉 충돌을 줄이기 위해 더 많은 엔트리를 추가하고 표현성을 높이기 위해 차원을 넓힘으로써 임베딩 테이블의 크기를 확장하는 것이다. 결과적으로, DLRS는 파라미터 카운트를 지배하는 임베딩 테이블들과 함께 수조 개의 파라미터들(Kang et al., 2020; Mudigere et al., 2021; Lian et al., 2021)에 도달하였다.\n' +
      '\n' +
      '불행히도, 업-스케일링의 전통적인 방법은 몇 가지 실용적인 단점들을 가지고 있다. 모델의 희소 성분을 단순히 확장하는 것은 증가하는 수의 피쳐 간의 복잡한 상호 작용을 포착하는 능력을 향상시키지 않는다. 더욱이, 이러한 경향은 다음 세대 가속기의 대부분의 개선이 테이블 룩업을 임베딩할 수 없는 컴퓨트 용량(Luo et al., 2018, 2017)에 있기 때문에 하드웨어 발전 추세와 현저하게 다르다. 따라서, 단순히 임베딩 테이블을 확장하는 것은 특히 분산 환경에서 차선책 가속기 활용과 함께 엄청난 인프라 비용을 초래한다.\n' +
      '\n' +
      '우리의 연구는 LLM 도메인에서 확립된 것과 유사한 스케일링 법칙을 확립할 수 있는 추천 모델에 대한 대안적인 스케일링 메커니즘을 찾는 것을 목표로 한다. 즉, 우리는 시너지 전략으로 데이터 세트 크기, 계산 및 매개변수 예산과 함께 품질을 지속적으로 개선할 수 있는 통합 아키텍처를 고안하고자 한다.\n' +
      '\n' +
      '우리는 희소 스케일링으로 인한 품질 및 효율성 단점을 완화하기 위해 상향 스케일링 상호 작용 구성요소인 _dense scaling_에 중점을 둔다. 그러나, 기존의 모델들은 다양한 이유로 이러한 패러다임으로부터 이익을 얻을 수 없다. 예를 들어, DLRM은 복잡한 데이터 세트에 대한 효과를 제한하는 2차 상호작용만을 캡처하는 것으로 제한되는 확장성 제약에 직면한다. 더욱이, 기존 모델들은 스케일 업을 위한 포괄적인 프로세스가 부족하다. 예를 들어, DCNv2 및 AutoInt+는 특정 하이퍼파트너를 조정하는 데 중점을 둠으로써 스케일업 시 수익률이 급격히 감소합니다. 추가적으로, 잔여 연결(He et al., 2016), 레이어놈(Ba et al., 2016), 그라디언트 클립(Pascanu et al., 2013)), 업-스케일링 기존 모델들은 트레이닝 안정성 이슈(Tang et al., 2023)에 취약하다.\n' +
      '\n' +
      '추천 모델에 대한 스케일링 법칙을 확립하기 위해, 효과적인 밀집 스케일링 특성을 나타내는 단순하면서도 효과적인 상호작용 구조인 우콩을 제안한다. 이진 지수승의 원리에 영감을 받아, 우리의 핵심 혁신은 일련의 적층 인수분해 기계(FMs)를 사용하여 임의의 차수 피쳐 상호 작용을 효율적이고 확장 가능하게 캡처하는 것이다. 우리의 설계에서 각 FM은 입력과 관련하여 2차 상호작용을 캡처하는 역할을 하며, 이러한 FM으로부터의 출력은 후속적으로 MLP에 의해 상호작용 결과를 인코딩하고 다음 레이어로의 입력 역할을 하는 새로운 임베딩으로 변환된다.\n' +
      '\n' +
      '우리는 6개의 공개 데이터 세트와 대규모 내부 데이터 세트를 사용하여 우공의 성능을 평가했다. 그 결과, 우콩은 AUC 측면에서 모든 공공 데이터 세트에서 최첨단 모델을 능가하는 것으로 나타났으며, 이는 우콩의 아키텍처의 효과와 광범위한 추천 작업 및 데이터 세트에 걸쳐 일반화할 수 있는 능력을 나타낸다. 우리의 내부 데이터 세트에서 우공은 비교 가능한 수준의 복잡성에서 품질 측면에서 기존 모델을 크게 능가할 뿐만 아니라 모델 복잡성에서 두 가지 규모에 걸쳐 확장될 때 품질의 지속적인 향상을 보여주며, 100 GFLOP/예제를 넘어 또는 동등하게 선행 기술이 부족한 총 훈련 계산의 GPT-3/LLaMa-2 규모까지 확장된다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '딥러닝 추천 시스템(DLRS) 기존 DLRS는 유사한 구조를 공유한다. 일반적인 모델은 희소 및 조밀한 구성요소로 구성됩니다. 희소 성분은 본질적으로 희소 범주형 특징을 조밀한 임베딩으로 변환하는 룩업 테이블을 임베딩하는 반면, 조밀한 성분은 예측을 생성하기 위해 이러한 임베딩 간의 상호작용을 캡처하는 역할을 한다.\n' +
      '\n' +
      'Dense Interaction ArchitecturesCapturing interaction between features는 효과적인 DLRS의 핵심이며, 이러한 원인에 대해 다양한 선행기술들이 제안되었다. 예를 들어, AFN+(Cheng et al., 2020)는 상호작용의 임의의 순서를 캡처하기 위해 특징들을 로그 공간으로 변환하고; AutoInt+(Song et al., 2019)는 다중-헤드 자기 주의를 사용하고; DLRM 및 DeepFM(Naumov et al., 2019; Guo et al., 2017) 레버리지 Factorization Machines(FM)(Rendle, 2010)을 사용하여 2차 상호작용을 명시적으로 캡처하고; HOFM(Blondel et al., 2016)은 상호작용의 더 높은 순서를 효율적으로 캡처하기 위해 FM을 최적화하며; DCNv2(Wang et al., 2021)는 요소별 입력 주의의 형태로 볼 수 있는 적층된 특징 교차를 통해 상호작용들을 캡처하는 CrossNet을 사용한다. FinalMLP(Mao et al., 2023)는 2개의 MLP 스트림으로부터의 결과들을 집계하기 위해 쌍선형 융합을 채용하고, 각각은 스트림-특정 게이티드 피처들을 입력으로서 취한다. MaskNet(Wang et al., 2021)은 상호작용 캡쳐를 위해 일련의 MaskBlocks을 채택하여, 입력 자체에 "입력 주의"를 적용하고 DNN의 중간 활성화; xDeepFM(Lian et al., 2018)은 DNN과 압축 상호작용 네트워크를 결합하며, DNN은 외부 제품을 통해 상호작용을 캡쳐하고 그 결과를 요소별 합산으로 압축한다.\n' +
      '\n' +
      'DLRS 기존 문헌들을 스케일링 업하는 것은 주로 모델들의 희소 컴포넌트를 스케일링 업하는 것에 초점을 맞추고 있다(Kang et al., 2020; Mudigere et al., 2021; Lian et al., 2021). 밀집한 상호 작용 아키텍처의 확장성에 대한 논의는 제한적이다. 조사된 소수의 사람들 중에서, 그들의 확장성은 종종 만족스럽지 않다. 예를 들어, AutoInt+는 공개 데이터 세트에서 2개 이상의 레이어로 이득 감소를 관찰한다. 유사하게, DCNv2는 대규모 생산 데이터 세트에서 테스트되었음에도 불구하고 두 계층을 넘어 확장하거나 순위 크기를 증가시킬 때 수익률이 급격히 감소한다.\n' +
      '\n' +
      '우공##3 디자인\n' +
      '\n' +
      '우리는 우콩의 아키텍처를 설계할 때 두 가지 목표를 염두에 둔다: (1) 복잡한 고차 특징 상호 작용을 효과적으로 포착하기 위한 것, (2) 데이터 세트 크기, GFLOP/예제 및 매개변수 예산과 관련하여 우콩의 품질 규모를 우아하게 보장하기 위한 것이다.\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '우콩에서 범주형 및 조밀한 특징은 초기에 **Embedding Layer**(Sec. 3.2)를 통과하며, 이는 이러한 입력을 _Dense Embeddings_로 변환한다.\n' +
      '\n' +
      '도 2에 도시된 바와 같이, 우콩은 후속적으로 임베딩들 간의 상호작용을 캡처하기 위해 통합된 신경망 계층들의 스택인 **Interaction Stack**(Sec. 3.3)를 채택한다. 인터랙션 스택은 이진 지수승의 개념에서 영감을 끌어 각 연속 레이어가 기하급수적으로 높은 차수의 상호작용을 캡처할 수 있도록 한다. 인터랙션 스택의 각 레이어는 **Factorization Machine Block**(FMB, Sec. 3.4)와 **Linear Compression Block**(LCB, Sec. 3.5)로 구성된다. FMB와 LCB는 독립적으로 마지막 계층으로부터 입력을 받아들이고 그들의 출력은 현재 계층에 대한 출력으로서 앙상블된다. 상호작용 스택에 이어지는 것은 상호작용 결과를 예측으로 매핑하는 최종 다층 퍼셉트론(MLP) 계층이다.\n' +
      '\n' +
      '### Embedding Layer\n' +
      '\n' +
      '다중 핫 범주형 입력이 주어지면 임베딩 테이블은 이를 밀집된 임베딩으로 매핑한다. 이 프로세스는 일련의 룩업을 포함하며, 각각은 입력 내의 "핫" 치수에 대응한다. 그런 다음 룩업 결과는 풀링 연산을 사용하여 집계되며, 요약은 임베딩 영역에서 일반적인 방법이다.\n' +
      '\n' +
      '본 논문에서 제안하는 임베딩 차원은 전역 임베딩 차원\\(d\\)으로 알려진 Embedding Layer에 의해 생성된 모든 임베딩에 대해 표준화된다. 상이한 특징들의 다양한 유의성을 수용하기 위해, 유의미한 것으로 간주되는 각각의 특징들에 대해 다수의 임베딩들이 생성된다. 대조적으로, 덜 중요한 특징들은 더 작은 기본 임베딩 치수들을 할당받는다. 그런 다음 이러한 더 작은 임베딩은 집합적으로 그룹화되고, 연결되며, MLP를 사용하여 \\(d\\)차원 임베딩으로 변환된다.\n' +
      '\n' +
      '밀집 입력은 MLP에 의해 동일한 \\(d\\) 차원을 공유하는 잠재 임베딩으로 변환되고 범주형 입력의 임베딩 출력과 결합된다. 이것은 출력 텐서 크기\\(X_{0}\\in\\mathbb{R}^{n\\times d}\\)를 산출하며, 여기서 \\(n\\)은 조밀하고 희박한 부분의 전체 임베딩 수이다. \\\\ (X_{0}\\)는 이어서 인터렉션 스택에 의해 추가로 처리될 준비가 된다.\n' +
      '\n' +
      'DCN(Wang et al., 2021)과 같은 기존의 접근법과 달리, 우리는 각 임베딩 벡터를 전체 단위로 해석하고(상세 후), 따라서 \\(X_{0}\\in\\mathbb{R}^{n\\times d}\\)에 대한 표현과 \\(X_{0}\\in\\mathbb{R}^{nd}\\)에 대해 설명한다.\n' +
      '\n' +
      '### Interaction Stack\n' +
      '\n' +
      '교호작용 모듈은 동일한 교호작용 계층을 쌓는데, 각 계층은 인수분해 기계(FMs)를 사용하여 지수 비율로 점진적으로 고차 피쳐 교호작용을 포착한다.\n' +
      '\n' +
      '인터랙션 레이어는 두 개의 블록을 병렬로 가지고 있다: FMB( Factorization Machine Block)와 LCB(Linear Compression Block). FMB는 레이어의 입력 임베딩들 사이의 피처 상호작용들을 계산하며, LCB는 단순히 레이어의 선형 압축된 입력 임베딩들을 포워딩한다. 그런 다음 FMB 및 LCB의 출력이 연결된다.\n' +
      '\n' +
      '스택 내의 레이어 \\(i\\)의 경우, 그 결과는 1에서 \\(2^{i}\\)까지의 임의의 순서로 특징 상호작용을 포함할 수 있다. 이는 단순히 인덕션으로 나타낼 수 있다. 계층 \\(i\\)의 입력이 1에서 \\(2^{i-1}\\)까지의 순서의 상호작용을 포함한다고 가정하자. 이는 첫 번째 계층(즉, \\(i=1\\))에 해당된다. FMB는 \\((o_{1}+o_{2})\\)-order 특징 상호작용을 \\(o1\\)과 \\(o2\\)-order 상호작용으로 생성하므로, 우리는 1을 포함하는 계층 \\(i\\)의 출력을 즉시 얻는다.\n' +
      '\n' +
      '그림 2: 우콩은 특징 상호 작용을 포착하기 위해 상호 작용 스택을 사용한다. 스택의 각 레이어는 인수분해 기계 블록과 선형 압축 블록으로 구성됩니다.\n' +
      '\n' +
      '(2^{i}\\)-차 상호작용으로, LCB의 출력으로부터 하한이 달성되고 입력으로부터 두 \\(2^{i-1}\\)-차 상호작용이 FM에 의해 달성되는 상한이 달성된다.\n' +
      '\n' +
      '트레이닝을 안정화시키기 위해, 우리는 또한 계층들에 걸친 잔여 연결들을 채택하고, 이어서 계층 정규화(LN)를 채택한다. 모든 걸 종합해 보면\n' +
      '\n' +
      '\\[X_{i+1}=\\mathrm{LN}(\\mathrm{concat}(\\mathrm{FMB_{i}}(X_{i}),\\mathrm{LCB_{i}}(X_ {i}))+X_{i})\\]\n' +
      '\n' +
      'FMB와 LCB의 특정 구성에 따라 \\(X_{i}\\)는 첫 번째 레이어에서 일반적으로 발생하는 \\(X_{i+1}\\)과 다른 수의 임베딩을 가질 수 있다. 이 경우 처리를 위해 잔차를 선형으로 압축하여 모양과 일치시킬 수 있습니다.\n' +
      '\n' +
      '### 분해 기계 블록(FMB)\n' +
      '\n' +
      'FMB에는 인수분해 기계(FM)와 MLP가 포함됩니다. FM은 입력 임베딩들의 명시적인 특징 상호작용들을 캡처하는 데 사용되며, 출력은 2D 상호작용 매트릭스이고, 여기서 각각의 요소는 임베딩들의 쌍 사이의 상호작용을 나타낸다. 이 상호 작용 행렬은 MLP를 통해 \\((n_{F}\\times d)\\)의 모양을 가진 벡터로 평평하게 변환되고 나중에 사용하기 위해 \\(n_{F}\\) 임베딩으로 재형성된다.\n' +
      '\n' +
      '동작적으로, FMB는 다음을 수행한다:\n' +
      '\n' +
      '\\[\\mathrm{FMB}(X_{i})=\\mathrm{reshape}(\\mathrm{MLP}(\\mathrm{LN}(\\mathrm{flatten }(\\mathrm{FM}(X_{i})))))\\]\n' +
      '\n' +
      'Wukong의 FM 모듈은 완전히 사용자 정의할 수 있다. 예를 들어, 가장 기본적인 버전에서는 (Naumov et al., 2019)에서 FM 설계를 따랐는데, 즉 임베딩 벡터의 모든 쌍 사이의 내적을 취한다. \\(FM(X)=XX^{T}\\). 우리는 Sec. 3.6에서 보다 최적화된 FM 설계에 대해 논의한다.\n' +
      '\n' +
      '### 선형압축블록(LCB)\n' +
      '\n' +
      'LCB는 상호작용 차수의 증가 없이 임베딩들을 단순히 선형적으로 재조합하는데, 이는 상호작용 차수의 불변성이 계층 전체에 걸쳐 유지되는 것을 보장하는 데 중요하다. 구체적으로, \\(i\\)번째 상호작용 계층이 1에서 \\(2^{i}\\)까지의 상호작용 차수를 포착하는 것을 보장한다. LCB에 의해 수행되는 동작은 다음과 같이 설명될 수 있다:\n' +
      '\n' +
      '\\[\\mathrm{LCB}(X_{i})=W_{L}X_{i}\\]\n' +
      '\n' +
      '여기서 \\(W_{L}\\in\\mathbb{R}^{n_{L}\\times n_{i}\\)은 가중치 행렬이고, \\(n_{L}\\)은 압축된 임베딩의 수를 나타내는 하이퍼파라미터이며, \\(n_{i}\\)은 레이어의 입력 임베딩의 수이다.\n' +
      '\n' +
      '### Optimized FM\n' +
      '\n' +
      '쌍별 도트 곱이 사용되는 기본 형태에서, FM 계산 및 저장 복잡성은 임베딩 수에 따라 2차적으로 증가한다. 이것은 수천 개의 기능을 가진 실제 데이터 세트에서 빠르게 금지됩니다.\n' +
      '\n' +
      '계산 비용을 낮추면서 효과적인 특징 상호 작용을 허용하기 위해, 우리는 많은 실제 데이터 세트에서 관찰된 쌍별 내적 매트릭스에서 낮은 순위 속성을 활용하는 (Sharma, 2023; Anonymous, 2019)와 유사한 방식을 채택한다(Wang et al., 2021).\n' +
      '\n' +
      'd<=n\\일 때, dot-product interaction \\(XX^{T}\\)은 \\(d\\)-rank 행렬로서, 특징들의 수가 임베딩 차원보다 큰 큰 데이터 세트들에서 종종 그러하다. 따라서, 이론상 정보의 손실 없이 \\(n\\times n\\)에서 \\(n\\times k\\)으로 출력행렬의 크기를 \\(n\\times n\\)에서 \\(n\\times k\\)으로 효과적으로 줄일 수 있다. 여기서 \\(k\\)은 하이퍼파라미터인 \\(XX^{T}\\)에 학습 가능한 형상 \\(n\\times k\\)(즉, 계산 \\(XX^{T}Y\\))의 투영행렬을 곱함으로써 \\(n\\times n\\)에서 \\(n\\times k\\)으로 효과적으로 줄일 수 있다. 이것은 상호 작용 매트릭스를 저장하기 위한 메모리 요구 사항을 감소시킨다. 우리는 먼저 결합법칙을 이용하여 \\(X^{T}Y\\)을 계산할 수 있으며, 계산 복잡도를 \\(O(n^{2}d)\\)에서 \\(k<<n\\)으로 더 줄일 수 있다.\n' +
      '\n' +
      '또한, 모델 품질을 향상시키기 위해 MLP를 통해 선형 압축된 입력을 처리함으로써 투영 행렬 \\(Y\\)을 입력에 주의 깊게 다룰 수 있다. 달리 언급되지 않는 한 기본적으로 다음 실험에서 최적화된 FM을 사용한다.\n' +
      '\n' +
      '### Complexity Analysis\n' +
      '\n' +
      '상호작용 스택의 각 레이어는 동일한 하이퍼파라미터를 사용하고, MLP에서 가장 큰 FC는 크기\\(h\\)를 갖는다고 가정한다.\n' +
      '\n' +
      '첫 번째 층에서 FMB의 시간 복잡도는 FM과 MLP의 합으로 각각 \\(O(nkd)\\approx O(ndh)\\)와 \\(O(nkh+h^{2}+n_{F}dh)\\approx O(ndh+h^{2})\\)이다. LCB의 시간 복잡도는 \\(O(nn_{L}d)\\approx O(ndh)\\이다. 후속 층의 경우, 시간 복잡도는 \\(O(n^{\\prime}dh+h^{2})\\), 여기서 \\(n^{\\prime}=n_{L}+n_{F}\\)이다. 따라서 우공의 전체 시간 복잡도는 \\(O(ndh+ln^{\\prime}dh+h^{2})\\approx O(ndhogn+h^{2})\\이다.\n' +
      '\n' +
      '### Scaling Wukong\n' +
      '\n' +
      '우콩 아키텍처를 정의한 후, 우리는 스케일 업과 관련된 주요 하이퍼파라미터를 요약하고 나중에 이러한 하이퍼파라미터에 대해 우콩을 업스케일링하기 위한 노력을 설명한다.\n' +
      '\n' +
      '* [noitemsep,topsep=0pt]\n' +
      '*\\(l\\): 인터렉션 스택 내의 층수.\n' +
      '*\\(n_{F}\\): FMB에 의해 생성된 임베딩 수\n' +
      '*\\(n_{L}\\): LCB에 의해 생성된 임베딩 수\n' +
      '* \\(k\\): 최적화된 FM에서의 압축 임베딩 수\n' +
      '*\\(MLP\\): FMB의 MLP에서 층수 및 FC 크기\n' +
      '\n' +
      '확장하는 동안 처음에는 모델이 고차 상호 작용을 포착할 수 있도록 \\(l\\)을 늘리는 데 중점을 둡니다. 그 후, 우리는 더 넓은 범위의 상호 작용을 포착하는 모델의 능력을 증가시키기 위해 다른 하이퍼 파라미터를 확대한다.\n' +
      '\n' +
      '우콩의 향상된 효과성 이면의 직관\n' +
      '\n' +
      'FM을 주요 상호 작용 아키텍처로 사용하는 기존 작업에 비해 우콩의 혁신적인 FM 스택 접근 방식은 기존 FM 성능을 크게 향상시킨다. 이를 통해 우공은 어떤 차수의 상호작용도 포착할 수 있어 고차 추론이 필요한 대규모의 복잡한 데이터셋에 매우 효과적이다. 고차 FM에 대한 노력이 있지만 우콩의 기하급수적인 고차 상호작용 포착 속도는 HOFM에서 볼 수 있는 선형 복잡성을 무시하고 xDeepInt에서 비용이 많이 드는 외부 제품을 피하는 큰 효율성을 제공한다.\n' +
      '\n' +
      'MLP는 상호 작용을 암묵적으로 포착하는 데 한계를 보였지만(Beutel et al., 2018), 우콩은 상호 작용 포착을 위해 MLP에 의존하는 접근법에서 분기한다. 대신 우콩은 주로 MLP를 사용하여 상호 작용의 결과를 임베딩 표현으로 변환하고 이를 추가 상호 작용에 사용한다. MLP의 이러한 뚜렷한 사용은 복잡하고 이질적인 특징을 효과적으로 처리하고 해석하는 모델의 능력을 향상시킨다.\n' +
      '\n' +
      '추가적으로 우콩은 임베딩-와이즈 상호작용을 중심으로 각각의 임베딩을 하나의 단위로 취급한다. 이 접근법은 덜 유용한 내부 임베딩 및 교차 차원 상호작용을 피하기 때문에 요소별 상호작용을 캡처하는 아키텍처에 비해 계산 요구를 상당히 감소시킨다. 결과적으로 우공은 추천 시스템의 효율성을 높일 뿐만 아니라 관련 특징 상호 작용을 포착하는 데에도 그 효과를 유지한다.\n' +
      '\n' +
      '## 4 Implementation\n' +
      '\n' +
      '이 절에서는 대규모 데이터 세트에서 복잡도가 높은 우콩을 효과적으로 훈련하기 위한 관행에 대해 논의한다.\n' +
      '\n' +
      '전반적으로 우콩 교육을 실현 가능하게 하기 위해서는 분산 교육이 필요하다. 임베딩 레이어는 Neo(Mudigere et al., 2021) 및 NeuroShard(Zha et al., 2023)에서 제공하는 컬럼-와이즈 샤드 임베딩 백 구현을 사용한다. 조밀한 부분에서는 FSDP(Zhao et al., 2023)를 채택하여 성능과 메모리 용량 사이의 트레이드 오프 균형을 맞추고 샤딩 팩터를 조정하여 모델이 너무 많은 중복을 생성하지 않고 메모리에 적합하도록 한다.\n' +
      '\n' +
      '훈련 효율을 높이기 위해 Torch.fx(Reed et al., 2022)를 통한 자동 연산자 융합을 이용하여 훈련 성능을 향상시킨다. 또한, 계산, 메모리, 통신 오버헤드를 동시에 줄이기 위해 양자화를 적극적으로 적용한다. 구체적으로, FP16에서 Wukong의 임베딩 테이블을 학습하고, 임베딩 룩업 결과를 Forward Pass에서는 FP16, Backward Pass에서는 BF16으로 통신하며, Backward Pass에서는 밀집 파라미터에 대한 BF16 양자화 및 기울기 감소 과정을 이용한다.\n' +
      '\n' +
      '##5 평가 개요\n' +
      '\n' +
      '우리는 6개의 공개 데이터 세트와 내부 데이터 세트를 사용하여 우콩을 평가하며, 그 세부 정보는 표 1에 요약되어 있으며, 이러한 평가 결과는 두 섹션으로 구성된다.\n' +
      '\n' +
      '섹션 6에서는 낮은 복잡성 영역에서 우공의 효과를 입증하는 데 중점을 두고 6개의 공공 데이터 세트에 대해 평가한다. 우리의 결과는 **우콩이 6개의 데이터 세트 모두에서 이전의 최첨단 방법을 능가하여 그 효과를 입증한다는 것을 보여준다.**\n' +
      '\n' +
      '7절에서는 우콩의 확장성을 입증하기 위해 대규모 사내 데이터 세트에 대해 평가한다. 데이터 세트에는 가장 큰 데이터 세트인 Criteo에 비해 30배 더 많은 샘플과 20배 더 많은 기능이 포함되어 있습니다. 본 연구의 결과는 **(1) 우콩이 모델 품질과 런타임 속도 측면에서 모든 기준 모델보다 일관되게 우수하다는 것을 보여주며, 모든 복잡도 척도에서 이러한 우월성을 유지한다는 것을 보여준다. (2) 우콩은 기준 모델에 비해 더 나은 스케일링 경향을 보인다. 또한, 우콩 내 각 구성 요소의 개별 기여도와 효과에 대한 이해를 얻기 위해 절제 연구를 수행한다.\n' +
      '\n' +
      '##6 공공데이터세트 평가\n' +
      '\n' +
      '이 섹션에서는 다양한 공개 데이터 세트에 걸쳐 우공의 효과를 입증하는 것을 목표로 한다. 달리 언급되지 않는 한, 우리는 이전 작업과의 일관성을 위해 BARS 벤치마크(Zhu et al., 2022)에 의해 제공된 프리록(preproc)을 사용한다.\n' +
      '\n' +
      '일반 평가 설정\n' +
      '\n' +
      '#### 6.1.1 Datasets\n' +
      '\n' +
      '**프라페(발트루나스)**는 앱 사용 로그입니다. 이 데이터 세트는 사용자가 주어진 컨텍스트와 함께 앱을 사용하는지 여부를 예측한다.\n' +
      '\n' +
      '**MicroVideo(Chen et al., 2018)**는 사용자와 마이크로 비디오 간의 상호 작용을 포함하는 THACIL 작업에 의해 제공되는 콘텐츠 이해 기반 데이터세트이다. 이 로그에는 기존 피쳐와 함께 멀티모달 임베딩이 포함되어 있습니다.\n' +
      '\n' +
      '**MovieLens Latest(Harper and Konstan, 2015)**는 영화에 대한 사용자들의 평점을 포함하는 잘 알려진 데이터셋이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & **\\#Samples** & **\\#Features** \\\\ \\hline Frappe & 0.29M & 10 \\\\ MicroVideo & 1.7M & 7 \\\\ MovieLens Latest & 2M & 3 \\\\ KuaiVideo & 13M & 8 \\\\ TaobaoAds & 26M & 21 \\\\ Criteo Terabyte & 4B & 39 \\\\ Internal & 146B & 720 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 평가 데이터 세트의 통계량.\n' +
      '\n' +
      'KualVideo(Kuaishou)는 Kuaishou가 공개한 경쟁 데이터셋이다. 데이터세트는 새로운 마이크로 비디오에서 사용자의 클릭 확률을 예측하는 데 사용된다. 이 데이터 세트에는 다른 범주형 및 플로트 피쳐와 함께 내용 이해 기반 임베딩도 포함되어 있습니다.\n' +
      '\n' +
      'TaobaoAds(Tianchi, 2018) 이 데이터세트에는 Taobao에 대한 광고 클릭율(CTR) 예측 8일이 포함된다.\n' +
      '\n' +
      'Criteo Terabyte(Criteo) 이 데이터 세트에는 24일의 광고 클릭 피드백이 포함되어 있습니다. 우리는 테스트를 위해 데이터의 마지막 날을 사용했다.\n' +
      '\n' +
      '#### 6.1.2 Baselines\n' +
      '\n' +
      '우리는 AFN+(Cheng et al., 2020), AutoInt+(Song et al., 2019), DLRM(Naumov et al., 2019), DCNv2(Wang et al., 2021a), FinalMLP(Mao et al., 2023), MaskNet(Wang et al., 2021b) 및 xDeepFM(Lian et al., 2018)을 포함하여 학계와 산업계 모두에서 널리 알려진 7개의 최신 모델에 대해 우콩을 벤치마킹한다.\n' +
      '\n' +
      '#### 6.1.3 Metrics\n' +
      '\n' +
      'AUC(AucArea Under the Curve)는 모든 임계값에 걸쳐 양성과 음성을 올바르게 분류하는 모델의 능력에 대한 집계된 척도를 제공한다. AUC 값이 높을수록 모델 성능이 향상됨을 나타냅니다.\n' +
      '\n' +
      'LogLoss log loss는 예측이 실제 라벨로부터 얼마나 멀리 떨어져 있는지에 기초하여 페널티를 정량화한다. 로그 손실이 적을수록 모형이 좋습니다.\n' +
      '\n' +
      '### Model-Specific Setup\n' +
      '\n' +
      '5개의 작은 데이터 세트에 대해 Criteo를 제외하고 공개 BARS 평가 프레임워크(Zhu et al., 2022; 2021)를 채택했다. 우리는 가능한 한 BARS에서 가장 잘 검색된 모델 구성을 직접 사용하고 나머지는 제공된 모델 기본 하이퍼파라미터를 사용한다. 프레임워크에 제공된 기본 임베딩 차원 외에도 128의 임베딩 차원을 추가로 테스트하고 이 두 구성 중 어느 것이 더 나은 결과를 산출했는지 보고한다. 우공의 경우, LCB의 드롭아웃 비율과 최적화 설정 및 압축을 조정하여 특징의 수에 적응한다.\n' +
      '\n' +
      '우리는 원패스 훈련이 수행되는 현실적인 온라인 추천 시스템에서 모델 성능을 평가하기 위해 더 큰 Criteo 데이터 세트를 활용합니다. 새로운 훈련 설정에 비추어 Sec에 설명된 시스템을 사용하여 광범위한 그리드 검색을 수행했다. 4는 모든 기준선과 우공에 대해 공정한 비교를 용이하게 한다. 이 철저한 과정에는 거의 3000개의 개별 실행이 포함되었다. 우리는 부록 A에서 모델별 검색 공간을 제공하며, 가장 잘 검색된 모델 하이퍼파라미터는 Sec 7에서 base config으로 사용되었다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '그 결과를 표 2에 요약하였다. 전반적으로 우공은 모든 공공 데이터 세트에서 AUC 측면에서 최첨단 결과를 얻을 수 있다. 이 결과는 우콩의 아키텍처의 효과와 다양한 데이터 세트를 이해하고 광범위한 추천 작업에 걸쳐 일반화할 수 있는 능력을 보여준다.\n' +
      '\n' +
      '##7 내부 데이터세트 평가\n' +
      '\n' +
      '이 섹션에서는 대규모 데이터 세트를 사용하여 우콩의 확장성을 입증하고 우콩의 다양한 개별 구성 요소가 그 효과에 어떻게 기여하는지 깊이 이해한다.\n' +
      '\n' +
      '### Evaluation Setup\n' +
      '\n' +
      '#### 7.1.1 Dataset\n' +
      '\n' +
      '이 데이터 세트는 총 146B 항목을 포함하고 720개의 고유한 기능을 가지고 있다. 각 기능은 항목 또는 사용자의 속성을 설명합니다. 이 데이터세트와 관련된 두 가지 태스크가 있다: _(Task1)_사용자가 아이템에 관심을 보였는지 여부를 예측하는 것(예를 들어, 클릭) 및 _(Task2)_변환이 일어났는지 여부(예를 들어, 좋아요, 팔로우)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**Frappe**} & \\multicolumn{2}{c}{**MicroVideo**} & \\multicolumn{2}{c}{**MovieLens L.**} & \\multicolumn{2}{c}{**KualVideo**} & \\multicolumn{2}{c}{**TaobaoAds**} & \\multicolumn{2}{c}{**Criteo TB**} \\\\  & AUC & LogLoss & AUC & LogLoss & AUC & LogLoss & AUC & LogLoss & AUC & LogLoss \\\\ \\hline \\multicolumn{11}{l}{_Baselines_} \\\\ AFN+ & 0.9812 & 0.2340 & 0.7220 & 0.4142 & 0.9648 & 0.3109 & 0.7348 & 0.4372 & 0.6416 & 0.1929 & 0.8023 & 0.1242 \\\\ AutoInt+ & 0.9806 & 0.1754 & 0.7155 & 0.4203 & 0.9693 & 0.2178 & 0.7297 & 0.4376 & 0.6437 & 0.1930 & 0.8073 & 0.1233 \\\\ DCNv2 & 0.9774 & 0.2325 & 0.7187 & 0.4162 & 0.9683 & 0.2169 & 0.7360 & 0.4383 & 0.6457 & 0.1926 & 0.8096 & 0.1227 \\\\ DLRM & 0.9846 & 0.1465 & 0.7173 & 0.4179 & 0.9685 & 0.2160 & 0.7357 & 0.4382 & 0.6430 & 0.1931 & 0.8076 & 0.1232 \\\\ FinalMLP & **0.9868** & 0.1280 & 0.7247 & 0.4147 & **0.9723** & 0.2211 & 0.7374 & 0.4435 & 0.6434 & 0.1928 & 0.8096 & 0.1226 \\\\ MaskNet & 0.9816 & 0.1701 & 0.7255 & 0.4157 & 0.9676 & 0.2383 & 0.7376 & 0.4372 & 0.6433 & 0.1927 & 0.8100 & 0.1227 \\\\ xDeepFM & 0.9780 & 0.2441 & 0.7167 & 0.4172 & 0.9667 & 0.2089 & 0.718 & 0.4565 & 0.6342 & 0.1961 & 0.8084 & 0.1229 \\\\ \\multicolumn{11}{l}{_Ours_} \\\\ Wukong & **0.9868** & 0.1757 & **0.7292** & 0.4148 & **0.9723** & 0.1794 & **0.7414** & 0.4367 & **0.6488** & 0.1954 & **0.8106** & 0.1225 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 6개의 공개 데이터셋에 대한 평가 결과. 각 데이터 세트에 **best AUC** 및 **best LogLoss**가 있는 모델이 강조 표시됩니다.\n' +
      '\n' +
      '#### 7.1.2 Metrics\n' +
      '\n' +
      'GFLOP/exampleGiga Floating Point Operations per example (GFLOP/example)는 모델 트레이닝 동안 계산 복잡도를 정량화한다.\n' +
      '\n' +
      'PF-days 총 훈련량은 1 PetaFLOP/s에서 1일 동안 작동하는 기계를 실행하는 것과 동일하다.\n' +
      '\n' +
      '#모형에 있는 모수의 수로 측정한 매개변수 모형 크기입니다. 희소 임베딩 테이블 크기는 627B 매개변수로 고정되었다.\n' +
      '\n' +
      '고정 기준선에 대한 상대적 로그로스 로그로스 개선. 기본 구성을 기준선으로 하는 DLRM을 사용하도록 선택한다. 0.02% 상대 로그로스 개선은 이 데이터 세트에서 중요한 것으로 간주된다. 온라인 교육 중 마지막 1B 창에 상대 로그로스를 보고합니다.\n' +
      '\n' +
      '#### 7.1.3 Baselines\n' +
      '\n' +
      '우리는 Sec. 6.1.2에 자세히 설명된 것과 동일한 기준 설정을 준수하지만 xDeepFM은 대규모 데이터 세트와 고가의 외부 제품 작동의 비호환성으로 인해 보고된 결과에 포함되지 않아 최소한의 설정에서도 지속적으로 메모리 외 문제를 유발한다.\n' +
      '\n' +
      '#### 7.1.4 Training\n' +
      '\n' +
      '조밀한 부분의 경우 lr=0.04, 조밀한 부분의 경우 베타1=0.9, 조밀한 부분의 경우 베타2=1, 희소 임베딩 테이블의 경우 lr=0.04인 Rowwise Adagrad와 같은 모든 실험에서 파일럿 연구에서 발견된 최상의 최적화 구성을 사용했다. 모델은 온라인 교육 방식으로 교육 및 평가되었다. 모든 런에서 임베딩 차원을 160으로 고정합니다.\n' +
      '\n' +
      '우리는 Sec에 설명된 Criteo Terabyte 평가에서 발견된 최상의 구성으로 하이퍼파라미터를 설정했습니다. 도 6을 출발점으로 하고, 각 모델에 대한 파라미터 카운트를 점진적으로 스케일링한다. 우리는 모든 실험에 262,144의 전체 배치 크기를 사용한다. 각 실험은 모델 크기에 따라 128개 또는 256개의 H100 GPU에서 실행되었다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '두 과제 모두 유사한 결과를 관찰하였고, 본문에서 _Task1_에 대한 결과를 보고하였으며, _Task2_에 대한 자세한 결과는 부록 B에 제시하였다.\n' +
      '\n' +
      '품질 대 품질. Compute Complexity in Fig. 1, 우리는 품질과 계산 복잡성 사이의 관계를 묘사한다. 결과는 우공이 다양한 복잡성 수준에서 모든 기준선을 일관되게 능가하여 LogLoss에서 0.2% 이상의 개선을 달성함을 보여준다. 특히, 우콩은 모델 복잡도에서 두 배의 크기에 걸쳐 스케일링 법칙을 가지고 있다 - 우리는 전체 훈련에서 GPT-3/LAMa-2 척도까지 200배 증가하면서 품질의 상당한 0.4% 향상을 관찰하는데, 이는 복잡도의 4배마다 대략 0.1% 개선으로 해석된다. 기준선 중 AFN+, DLRM 및 FinalMLP는 일정 복잡도 수준 이후에 안정기에 도달하는 경향이 있는 반면 AutoInt+, DCNv2 및 MaskNet은 품질 2를 더 향상시키지 못했지만, 최고 성능 기준선인 DCNv2조차도 우콩의 품질에 맞추기 위해 복잡도가 40배 증가해야 한다.\n' +
      '\n' +
      '각주 1: 160 PF일의 총 트레이닝을 프로젝팅하는 것은 지속적인 온라인 트레이닝의 사용을 가정하여 내부 데이터세트 상에서 365일 기간으로 계산한다.\n' +
      '\n' +
      '각주 2: AutoInt+ 및 DCNv2는 추가 확장 시 지속적으로 상당한 훈련 불안정 문제에 직면했다. AutoInt+는 모델 품질이 감소했지만 손실 폭발에서 회복되었으며 DCNv2는 회복되지 않았으며 폭발 전 성능에서 품질을 추정했다. 마스크넷은 과도한 메모리 소비로 인해 방해를 받아 메모리 외 오류가 발생하여 추가 스케일 업을 차단했다.\n' +
      '\n' +
      '품질 대 품질. Model SizeIn Fig. 도 3에서는 모델 품질과 모델 크기 사이의 상관 관계를 설명한다. 위의 계산 복잡도 스케일링에서 관찰된 경향을 반영하여 우공은 모델 크기의 모든 척도에서 일관되게 0.2%의 거칠게 모든 기준선을 능가한다. 최대 6,370억 매개 변수까지 꾸준한 개선 추세를 보여줍니다.\n' +
      '\n' +
      '스케일링 프로세스 전반에 걸쳐 모델 특정 스케일링을 통해 모델당 고유한 전략을 사용했다. 각 실행에 대한 상세한 하이퍼파라미터 설정은 부록 B에 제공된다. 각 모델의 스케일링 프로세스는 다음과 같이 요약된다:\n' +
      '\n' +
      '우리는 Sec. 3.8에 자세히 설명된 하이퍼파라미터를 조정하여 우콩을 확장했다.\n' +
      '\n' +
      'AFN의 은닉층, 앙상블 DNN 및 로그 뉴런 수를 확장했다. 그 결과 AFN을 확장해도 모델 품질이 향상되지 않는다는 것을 알 수 있다.\n' +
      '\n' +
      '우리는 멀티헤드 어텐션과 앙상블 DNN을 확장했다. 이 모델의 모델 품질은 처음에 다른 모델보다 나쁘지만 확장하면 눈에 띄게 개선됩니다.\n' +
      '\n' +
      '도 3: #파라미터에 대한 우공의 확장성.\n' +
      '\n' +
      'DLRM 우리는 상위 MLP를 확대했다. 결과는 품질이 31 GFLOP/예제를 넘어 포화되기 시작한다는 것을 보여준다.\n' +
      '\n' +
      'DCNv2 우리는 크로스 네트워크와 딥 네트워크를 모두 확장했다. 크로스 네트워크를 확장해도 품질 향상이 나타나지 않았습니다. DCNv2의 훈련 안정성은 다른 모델보다 좋지 않으며 엄격한 기울기 클리핑을 적용했다.\n' +
      '\n' +
      '최종 MLPWe는 두 개의 MLP 스트림과 특징 선택 모듈을 확장했다. 결과는 모델 품질이 낮은 복잡도 영역에서 개선되지만 36 GFLOP/예제를 초과하여 포화되기 시작한다는 것을 보여준다.\n' +
      '\n' +
      'MaskNet은 Parallel MaskNet과 Serial MaskNet을 모두 테스트했으며, Parallel 변종이 더 낫다는 것을 발견했다. 모델이 실행 가능한 크기를 갖도록 초기 축소 비율을 줄이고 마스크 블록의 수, DNN 및 축소 비율을 점진적으로 확대했다.\n' +
      '\n' +
      '### Ablation\n' +
      '\n' +
      '개별 구성요소의 의의 우리의 목표는 우콩의 상호작용 스택에서 FMB, LCB 및 잔류 연결의 중요성을 입증하는 것이다. 이를 위해 각 구성 요소의 결과를 제로화하여 개별적으로 비활성화하는 실험을 수행했다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 4를 참조하면, FMB를 무효화하는 것은 큰 품질 저하를 초래한다. 흥미롭게도 LCB 또는 잔차의 비활성화는 품질의 약간의 저하만을 초래하는 반면 둘 다 비활성화하면 상당한 저하가 발생한다. 이 관찰은 제로 패딩 FMB 출력 및 잔류 연결을 통합함으로써 LCB를 단순화할 수 있음을 의미한다.\n' +
      '\n' +
      '개별 구성요소 축척의 영향은 우콩 내에서 각 하이퍼파라미터를 확장할 때 모델 품질의 기여도를 분석하는 것을 목표로 한다. 우리는 기본 구성에서 시작하여 각 하이퍼파라미터를 점진적으로 두 배로 늘렸다. 그 결과는 그림 5에 나와 있으며, 우콩 층수\\(l\\)가 증가하면 고차 상호작용이 포착되어 모델 품질이 크게 향상되는 것을 관찰했다. 또한, MLP 크기를 증가시키는 것은 상당한 성능 향상을 초래한다. \\(k\\)과 \\(n_{F}\\)의 상승은 유익한 반면, \\(n_{L}\\)은 기본 구성에 대해 안정되었다. 특히, \\(k,n_{F},n_{L}\\)의 결합된 스케일 업은 각각 개별적으로 스케일링하는 것보다 더 뚜렷한 품질 개선을 제공한다.\n' +
      '\n' +
      '## 8 Discussions\n' +
      '\n' +
      '실제적으로 높은 복잡도까지 확장되는 확장 모델 제공은 실시간 서비스를 위한 주목할만한 과제를 제시한다. 잠재적인 해결책은 비용을 상각하기 위해 다중 작업 기반 모델을 훈련하는 것을 포함한다: 큰 모델에서 서비스를 제공하기 위한 작고 효율적인 모델로 지식을 증류하는 것.\n' +
      '\n' +
      '자원 제한 연구 및 비즈니스 우콩의 효율성과 확장성에 대한 시사점은 자원이 제한된 연구자와 비즈니스에 상당한 이점을 제공한다. 40배 적은 컴퓨팅 자원을 가진 다른 모델의 정확도를 맞추는 기능은 제한된 컴퓨팅 자원을 가진 환경에서도 최첨단 추천 연구 및 응용 프로그램을 가능하게 한다.\n' +
      '\n' +
      '제한과 미래 작업 또한 미래의 작업에서 목표가 될 수 있는 우리의 작업에 제한과 주의를 주목한다.\n' +
      '\n' +
      '우콩의 확장성의 정확한 한계를 이해하는 것은 중요한 연구 분야이다. 대규모 계산 요구 사항으로 인해 한계가 적용되는 복잡성 수준에 도달하지 못했다.\n' +
      '\n' +
      '우공은 다양한 평가에서 우수한 품질을 보여주지만, 특히 적층 내적 구조를 공유하는 변압기와 같은 아키텍처와 달리 기본 원칙에 대한 포괄적인 이론적 이해는 추가 탐구가 필요한 영역으로 남아 있다.\n' +
      '\n' +
      '또한, 특히 추천에서 뚜렷한 특징과 유사한 이질적인 입력 데이터 소스를 포함하는 도메인에서 추천을 넘어서는 우콩의 일반화 가능성은 추가로 탐색되고 이해되어야 한다.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      '우리는 우콩이라는 효과적인 네트워크 구조를 제안했다. 우리는 우콩이 이전에 관찰되지 않았던 권고 영역에서 스케일링 법칙을 확립한다는 것을 증명했다 - 우콩은 컴퓨트 컴에서 두 개의 크기 순서에 걸쳐 효율적으로 위아래로 스케일링할 수 있다.\n' +
      '\n' +
      '그림 4: 개별 구성요소의 의미.\n' +
      '\n' +
      '도 5: 개별 구성요소의 스케일링의 영향.\n' +
      '\n' +
      '복잡성은 다른 최첨단 모델보다 경쟁력을 유지하면서 확장 가능한 아키텍처로, 광범위한 작업 및 데이터 세트에 걸쳐 작은 수직 모델부터 큰 기초 모델까지 백본 역할을 할 수 있습니다.\n' +
      '\n' +
      '## Impact Statements\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리의 작업에는 많은 잠재적인 사회적 결과가 있으며, 우리가 특별히 강조해야 한다고 느끼는 것은 없다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*익명(2019)익명. 기계 학습을 위한 점 행렬 압축. _ 기술 공개 커먼즈_, 2019.\n' +
      '* Ba et al. (2016) Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer Normalization. _ ArXiv:1607.06450_, 2016.\n' +
      '* 모바일 앱 사용. URL[https://www.baltrunas.info/context-aware](https://www.baltrunas.info/context-aware)\n' +
      '* Beutel et al. (2018) Beutel, A., Covington, P., Jain, S., Xu, C., Li, J., Gatto, V., and Chi, E. H. Latent cross: Making use of context in recurrent recommender systems. In _Proceedings of the eleventh ACM international conference on web search and data mining_, pp. 46-54, 2018.\n' +
      '* Blondel et al. (2016) Blondel, M., Fujino, A., Ueda, N., and Ishihata, M. 고차 인수분해 기계 신경 정보 처리 시스템_, 29, 2016의 발전.\n' +
      '* Bommasani et al. (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* Chen et al. (2018) Chen, X., Liu, D., Zha, Z. - J., Zhou, W., Xiong, Z., and Li, Y. 마이크로 비디오 클릭-스루 예측을 위한 카테고리- 및 아이템-레벨에서의 시간적 계층적 주의. 2018년 _MM_에서.\n' +
      '* Cheng et al.(2020) Cheng, W., Shen, Y., and Huang, L. 적응적 인수분해 네트워크: 적응-순서 특징 상호 작용을 학습합니다. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pp. 3609-3616, 2020.\n' +
      '* Covington et al. (2016) Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube recommendations. In _Proceedings of the 10th ACM conference on recommender systems_, pp. 191-198, 2016.\n' +
      '* Criteo(2017) Criteo. Criteo 1tb 클릭 로그 데이터세트. [https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/] (https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/).\n' +
      '* Guo et al. (2017) Guo, H., Tang, R., Ye, Y., Li, Z., and He, X. Deepfm: ctr 예측을 위한 인수분해-기계 기반 신경망. _ ArXiv:1703.04247_, 2017.\n' +
      '* Harper & Konstan (2015) Harper, F. M. and Konstan, J. A. The movielens dataset: History and context. _ ACM Trans. Interact. 인텔 Syst._ , 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL[https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872).\n' +
      '* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.\n' +
      '* Kang et al. (2020) Kang, W. -C., Cheng, D. Z., Yao, T., Yi, X., Chen, T., Hong, L., and Chi, E. H. Learning to embedding table without embedding table for recommendation. _ arXiv preprint arXiv:2010.10784_, 2020.\n' +
      '* Kaplan 등 (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kuaishou(2018) Kuaishou. URL[https://www.kuaishou.com/activity/uimc](https://www.kuaishou.com/activity/uimc).\n' +
      '* Lian et al. (2018) Lian, J., Zhou, X., Zhang, F., Chen, Z., Xie, X., and Sun, G. xdeepfm: Combining explicit feature interaction for recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pp. 1754-1763, 2018.\n' +
      '* Lian et al. (2021) Lian, X., Yuan, B., Zhu, X., Wang, Y., He, Y., Wu, H., Sun, L., Lyu, H., Liu, C., Dong, X., Liao, Y., Liao, Y., Zhang, C., Xie, J., Li, H., Chen, L., Huang, R., Lin, J., Shu, C., Qiu, X., Liu, Z., Kong, D., Yuan, L., Yu, H., Yang, S., Zhang, C., and Liu, J. Persia: 100조 파라미터까지의 오픈, 하이브리드 시스템 스케일링 딥러닝 기반 추천자. 2021년 11월\n' +
      '*Liu et al. (2022) Liu, Z., Zou, L., Zou, X., Wang, C., Zhang, B., Tang, D., Zhu, B., Zhu, Y., Wu, P., Wang, K., et al. Monolith: 충돌 없는 임베딩 테이블을 갖는 실시간 추천 시스템. corr abs/2209.07663 (2022), 2022.\n' +
      '* Luo et al. (2017) Luo, L., Liu, M., Nelson, J., Ceze, L., Phanishayee, A., and Krishnamurthy, A. Motivating in-network aggregation for distributed deep neural network training. 2017년 Stack_ 전반에 걸친 Approximate Computing에 관한 _Workshop.\n' +
      '* Luo et al. (2018) Luo, L., Nelson, J., Ceze, L., Phanishayee, A., and Krishnamurthy, A. Parameter hub: a rack-scale parameter server for distributed deep neural network training. In _Proceedings of the ACM Symposium on Cloud Computing_, pp. 41-54, 2018.\n' +
      '\n' +
      '* Mao et al. (2023) Mao, K., Zhu, J., Su, L., Cai, G., Li, Y., and Dong, Z. 최종 mlp: ctr 예측을 위한 향상된 2-스트림 mlp 모델 _ arXiv preprint arXiv:2304.00902_, 2023.\n' +
      '* Mudigere et al. (2021) Mudigere, D., Hao, Y., Huang, J., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., et al. arXiv preprint arXiv:2104.05158_, 2021.\n' +
      '* Naumov et al. (2019) Naumov, M., Mudigere, D., Shi, H.-J. M., Huang, J., Sundaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.-J., Azzolini, A. G., et al. ArXiv preprint arXiv:1906.00091_, 2019.\n' +
      '* Pascanu et al. (2013) Pascanu, R., Mikolov, T., and Bengio, Y. 순환 신경망을 훈련하는 것의 어려움에 대해. In _International conference on machine learning_, pp. 1310-1318. Pmlr, 2013.\n' +
      '*Reed et al. (2022) Reed, J., DeVito, Z., He, H., Ussey, A., and Ansel, J. Torch. fx: python에서의 딥러닝을 위한 실용적 프로그램 캡쳐 및 변환. _ Proceedings of Machine Learning and Systems_, 4:638-651, 2022.\n' +
      '*렌들(2010)렌들, S. 공장화 기계야 In _2010 IEEE International Conference on Data Mining_, pp. 995-1000. ieeeexplore.ieee.org, December 2010.\n' +
      '* 샤르마(2023) 샤르마, S. 시작하지 않은 사람들을 위한 특징 융합! 싯다르트 샤르마! 중. [https://siddharth-1729-65206.medium.com/feature-fusion-for-the-uninitiated-4c593802922] (https://siddharth-1729-65206.medium.com/feature-fusion-for-the-initiated-4c593802922). (01/24/2024에 액세스됨).\n' +
      '* Song et al. (2019) Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., and Tang, J. Autoint: self-attentive neural networks via automatic feature interaction learning. In _Proceedings of the 28th ACM international conference on information and knowledge management_, pp. 1161-1170, 2019.\n' +
      '* Tang et al. (2023) Tang, J., Drori, Y., Chang, D., Sathiamoorthy, M., Gilmer, J., Wei, L., Yi, X., Hong, L., and Chi, E. H. Improving training stability for multitask ranking models in recommender systems. _ arXiv preprint arXiv:2302.09178_, 2023.\n' +
      '* 톈치(2018) 톈치. Ad display/click data on taobao.com, 2018. URL[https://tianchi.aliyun.com/dataset/dataDetail?dataId=56](https://tianchi.aliyun.com/dataset/dataDetail?dataId=56).\n' +
      '* Wang et al. (2021) Wang, R., Shivanna, R., Cheng, D., Jain, S., Lin, D., Hong, L., and Chi, E. Dcn v2: 개선된 deep & cross network and practical lessons for web-scale learning to rank systems. In _Proceedings of the web conference 2021_, pp. 1785-1797, 2021a.\n' +
      '* Wang et al. (2021) Wang, Z., She, Q., and Zhang, J. Masknet: Introduction feature-wise multiplication to ctr ranking models by instance-guided mask. _ arXiv preprint arXiv:2102.07619_, 2021b.\n' +
      '* Zha et al. (2023) Zha, D., Feng, L., Luo, L., Bhushanam, B., Liu, Z., Hu, Y., Nie, J., Huang, Y., Tian, Y., Kejariwal, A., et al. Pre-train and search: Efficient embedding table sharding with the pre-trained neural cost models. _ Proceedings of Machine Learning and Systems_, 5, 2023.\n' +
      '* Zhao et al. (2023) Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch fsdp: experience on scaling fully sharded data parallel. _ arXiv preprint arXiv:2304.11277_, 2023.\n' +
      '* 5, 2021_, pp. 2759-2769. ACM, 2021. doi: 10.1145/3459637.3482486. URL[https://doi.org/10.1145/3459637.3482486](https://doi.org/10.1145/3459637.3482486)\n' +
      '*15, 2022_, pp. 2912-2923. ACM, 2022a. doi: 10.1145/3477495.3531723. URL[https://doi.org/10.1145/3477495.3531723](https://doi.org/10.1145/3477495.3531723)\n' +
      '\n' +
      '## Criteo의 모델별 격자 탐색 공간 부록\n' +
      '\n' +
      '조밀한 아치 최적화를 위해 Adam을 사용하고 첫 번째 10% 단계에 대해 선형 예열 기간을 갖는 희소 아치 최적화를 위해 Rowwise AdaGrad를 사용한다. 전체 배치 크기는 \\(8*16384=131,072\\)을 사용한다. 모든 모델은 활성화를 위해 ReLU를 사용합니다. 파일럿 실험에서 모든 모델에서 더 나은 결과를 보여주기 때문에 128을 임베딩 차원으로 사용하도록 했다. 모든 실행에서 FP32를 사용합니다. 데이터세트 볼륨과 모델 크기로 인해, 우리는 밀도 동기화를 위한 희소 분산 학습 프레임워크와 데이터 병렬로 Mudigere et al.(2021)을 사용한다.\n' +
      '\n' +
      '공정한 비교를 용이하게 하기 위해 Criteo Dataset에서 일반 하이퍼 파라미터와 모델별 구성 모두에 대해 광범위한 그리드 검색(>3000 실행)을 수행했다.\n' +
      '\n' +
      '모든 모델에서 희소 학습률과 조밀 학습률은 각각 \\(\\{1e^{-3}, \\e^{-2}, \\e^{-1}\\})으로 조정되었다. 모든 모델에서 은닉층의 수는 \\(\\{1,2,3,4\\}\\)이고, 층의 크기는 \\(\\{512,1024,2048\\}\\)이다. 본 논문에서는 지나치게 큰 탐색 공간을 줄이기 위해 최적화 하이퍼파라미터에 대한 파일럿 실험을 수행하였으며, 학습률을 조밀도는 \\(1e^{-3}\\), 희소도는 \\(1e^{-1}\\)으로 설정하는 것이 모든 모델에서 가장 효과적임을 알 수 있었다. 다음 실행에서 학습률을 고정했습니다. 이제 모델별 검색 공간을 설명합니다.\n' +
      '\n' +
      '**AFN+** AFN 히든 유닛과 DNN 히든 유닛은 모든 런에서 동일하고, 일반적인 MLP 탐색 공간을 따른다. 대수 뉴런의 수는 \\({128,256,512,1024\\}\\)이다.\n' +
      '\n' +
      '**AutoInt+** 논문 Song et al.(2019)에 보고된 최상의 구성을 기반으로 검색 공간을 생성하였으며 하이퍼파라미터당 더 큰 값을 추가적으로 고려하였다. 어텐션 층수는 \\(\\{3,4\\}\\), 어텐션 딤은 \\(\\{256,512\\}\\) 범위였다. 주의 헤드 수는 \\(\\{4,8\\}\\)이다. DNN 히든 유닛들은 일반적인 MLP 탐색 공간을 따른다.\n' +
      '\n' +
      '**DCNv2** 교차층의 수는 1에서 4까지의 범위였다. 전체 순위 또는 \\(512\\)에서 검색된 순위이다.\n' +
      '\n' +
      '**DLRM** 하단 MLP 레이어 크기 및 숫자는 \\([512,256]\\)으로 설정하였다.\n' +
      '\n' +
      '**FinalMLP** 우리는 퍼블릭 벤치마크 셋업 Zhu et al.(2022)을 따랐고, 특징 선택(FS)을 하나의 스트림에 대한 모든 플로트 피처들로 설정하고, 다른 스트림에 대해 8개의 선택된 희소 피처들 중 하나를 탐색함으로써 수행되었다. FS MLP는 \\([800]\\)으로 설정된다. 헤드 수는 \\(256\\)으로 고정됩니다.\n' +
      '\n' +
      '**MaskNet** Parallel MaskNet과 Serial MaskNet을 모두 테스트했습니다. 병렬변형의 경우 \\(\\{1,8,16\\}\\)의 블록 수와 \\(\\{64,128\\}\\)의 블록 차원을 고려한다. 직렬변형의 경우, \\(\\{64,256,1024\\}\\)의 층수와 \\(\\{1,4,8\\}\\)의 층수를 고려한다. 우리는 두 변형 모두에 대해 감소 비율을 1로 고정했다.\n' +
      '\n' +
      '**xDeepInt** 우리는 \\(\\{3,4\\}\\)의 레이어 수와 \\(\\{16,32,64\\}\\)의 레이어 차원과 압축 상호작용 네트워크(CIN)를 고려했다.\n' +
      '\n' +
      '**Wukong** 하부 MLP 층 크기 및 수는 \\([512,256]\\)으로 설정하였다. \\ (l\\) 범위는 \\(1\\)에서 \\(4\\); \\(n_{F}\\) 및 \\(n_{L}\\)은 \\(\\{8,16\\}\\) 범위인 동일한 값으로 설정된다. \\(1\\) (k\\)는 \\(24\\)으로 고정된다.\n' +
      '\n' +
      '## 부록 B 모델-특정 스케일링-업 구성\n' +
      '\n' +
      '자세한 내용은 <표 3>을 참조하시기 바랍니다.\n' +
      '\n' +
      '우콩 고차 상호작용의## 부록 C 분석\n' +
      '\n' +
      '전통적인 인수분해 기계 접근법은 Naumov et al.(2019)을 최소화함으로써 2차 상호작용 문제를 해결한다:\n' +
      '\n' +
      '\\[\\min\\underset{i,j\\in S}{\\Sigma}r_{ij}-X^{1}X^{1}{}^{T}\\]\n' +
      '\n' +
      '여기서, \\(r_{ij}\\in R\\)는 \\(i\\) = 1,..., m 및 \\(j\\) = 1,..., n에 대한 \\(j\\)번째 사용자에 의한 \\(i\\)번째 제품의 등급이고; X는 사용자 및 아이템 표현(embedding)을 나타내고, 위첨자 \\(1\\)은 임베딩에 \\(1\\)번째 순서 정보를 포함한다. 이러한 임베딩 벡터의 내적은 2차 상호작용에 대한 후속 등급에 대한 의미 있는 예측을 산출한다. 우콩에서는 이러한 의미 있는 상호작용을 MLP를 이용하여 2차 상호작용 표현 \\(X^{2}\\)으로 변환한다. 2층 FMB에서 잔류와 LCB 연결이 있는 경우, \\((X^{1}+X^{2})(X^{1}+X^{2})^{T}\\)의 내적이 1차부터 4차까지 의미 있는 상호작용을 얻을 수 있다. 유추에 의해, \\(l\\)층 우공은 문제를 최소화함으로써 해결한다:\n' +
      '\n' +
      '\\[\\min\\underset{i,j\\in S}{\\Sigma}(r_{ij}-\\underset{k\\in 1,2,\\dots,2^{l-1}}{ \\Sigma}X^{k}{}^{T})\\]\n' +
      '\n' +
      '따라서 전통적인 인수분해 접근법에 비해 우공은 보다 충분한 상호작용 명령으로 추천 문제를 해결할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Hyperparameters** & **GFLOP/example** & **\\#Params** & **Relative LogLoss** & **Relative LogLoss** \\\\  & & & (Task1) & (Task2) \\\\ \\hline _AFN+_ & & & & \\\\ DNN=4x2048, afn=4x2048, nlog=1024 & 4.41 & 628.22 & 0.11 & 0.05 \\\\ DNN=4x4096, afn=4x2048, nlog=1024 & 7.65 & 628.74 & 0.12 & 0.06 \\\\ DNN=4x4096, afn=4x4096, nlog=2048 & 13.08 & 629.46 & 0.21 & 0.14 \\\\ DNN=4x8192, afn=4x8192, nlog=4096 & 43.4 & 633.95 & 0.12 & 0.06 \\\\ _AutoInt+_ & & & & \\\\ Attention=3x256, nhead=4, DNN=2x256 & 7.72 & 627.73 & 0.39 & 0.24 \\\\ Attention=3x512, nhead=4, DNN=2x256 & 18.58 & 627.77 & 0.15 & 0.05 \\\\ Attention=3x512, nhead=8, DNN=3x8192 & 42.53 & 631.49 & -0.09 & -0.16 \\\\ Attention=3x512, nhead=16, DNN=3x10240 & 49.58 & 632.59 & -0.1 & -0.2 \\\\ Attention=3x512, nhead=16, DNN=3x16384 & 68.83 & 635.57 & 0.13 (lossX) & 0.01 (lossX) \\\\ _DCN_ & & & & \\\\ |=2, rank=512, MLP=4x2048 & 3 & 628.11 & -0.27 & -0.27 \\\\ |=2, rank=512, MLP=4x4096 & 4.67 & 628.37 & -0.29 & -0.32 \\\\ |=2, rank=512, MLP=4x16384 & 17.85 & 630.42 & -0.38 & -0.41 \\\\ |=2, rank=512, MLP=4x32768 & 43.88 & 634.46 & -0.43 & -0.45 \\\\ |=2, rank=512, MLP=4x51200 & 84.71 & 640.79 & (LossX) & (LossX) \\\\ _DLRM_ & & & & \\\\ TopMLP=2x512 & 1.37 & 627.78 & (Baseline) & (Baseline) \\\\ TopMLP=4x512 & 1.37 & 627.78 & -0.11 & -0.08 \\\\ TopMLP=4x2048 & 3.85 & 628.17 & -0.23 & -0.21 \\\\ TopMLP=4x4096 & 7.29 & 628.7 & -0.28 & -0.27 \\\\ TopMLP=4x8192 & 14.61 & 629.84 & -0.32 & -0.31 \\\\ TopMLP=4x16384 & 31 & 632.39 & -0.37 & -0.35 \\\\ TopMLP=4x32768 & 71.23 & 638.62 & -0.36 & -0.34 \\\\ _FinalMLP_ & & & & \\\\ MLP1=4x4096, MLP2=2x1024, output_dim=64, & 3.93 & 628.25 & -0.11 & -0.16 \\\\ no\\_fs & & & & \\\\ MLP1=4x4096, MLP2=2x1024, output_dim=64, & 8.17 & 628.91 & -0.23 & -0.27 \\\\ f=15/05601, f8=5[75600,115200], fs_MLP=1x2048 & & & & \\\\ MLP1=4x8192, MLP=2x2048, output_dim=64, & 16.9 & 630.27 & -0.34 & -0.36 \\\\ f=15/05601, f8=5[75600,11520], fs_MLP=1x4096 & & & & \\\\ MLP1=8x8192, MLP=2x2048, output_dim=64, & 18.77 & 630.56 & -0.37 & -0.38 \\\\ f=15/05601, f8=5[75600,115200], fs_MLP=2x4096, & & & & \\\\ MLP1=4x6384, MLP2=2x4096, output_dim=64, & 36.26 & 633.27 & -0.34 & -0.34 \\\\ f=15/05601, f8=5[75600,115200], fs_MLP=1x8192 & & & & \\\\ _MaskNet_ & & & & \\\\ MLP=1x512, nblock=1, dim=128, reduction=0.01 & 1.76 & 627.92 & -0.09 & -0.12 \\\\ MLP=1x512, nblock=4, dim=128, reduction=0.01 & 6.8 & 628.7 & -0.22 & -0.25 \\\\ MLP=3x2048, nblock=4, dim=128, reduction=0.01 & 6.88 & 628.71 & -0.28 & -0.3 \\\\ MLP=3x2048, nblock=4, dim=128, reduction=0.05 & 32.36 & 632.67 & -0.37 & -0.37 \\\\ MLP=3x2048, nblock=4, dim=128, reduction=0.1 & 64.21 & 637.61 & -0.4 & -0.4 \\\\ _Wukong_ & & & & & \\\\ |=2, nrl=8, nf=8, k=24, MLP=3x2048 & 0.53 & 627.74 & -0.35 & -0.32 \\\\ |=4, nrl=32, nf=32, k=24, MLP=3x2048 & 1.25 & 627.82 & -0.45 & -0.43 \\\\ |=8, nrl=32, nf=32, k=24, MLP=3x2048 & 2.12 & 627.95 & -0.53 & -0.49 \\\\ |=8, nrl=48, nf=48, k=48, MLP=3x4096 & 5.6 & 628.46 & -0.6 & -0.6 \\\\ |=8, nrl=96, nf=96, k=96, MLP=3x8192\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>