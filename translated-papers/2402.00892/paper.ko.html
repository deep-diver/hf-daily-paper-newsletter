<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# EVA-GAN: 향상된 다양한 오디오 생성 via\n' +
      '\n' +
      '확장 가능한 생성적 적대적 네트워크\n' +
      '\n' +
      'Shijia Liao\n' +
      '\n' +
      'Shiyi Lan\n' +
      '\n' +
      '아룬 조지 자카리아\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 모델의 출현은 머신 러닝의 새로운 시대를 나타내며, 복잡한 패턴을 캡처하고 합성하기 위해 방대한 데이터 세트를 활용하여 소규모 모델을 크게 능가한다. 이러한 발전에도 불구하고, 특히 오디오 생성 영역에서 스케일링에 대한 탐색은 여전히 제한적이며, 이전의 노력은 높은 충실도(HiFi) 44.1kHz 영역으로 확장되지 않았고, 영역 외 데이터에 대한 견고성의 부족과 함께 고주파 영역에서 스펙트럼 불연속성과 흐림성 모두에 시달렸다. 이러한 한계는 음악 및 노래 생성을 포함한 다양한 사용 사례에 모델의 적용 가능성을 제한한다. 본 논문에서는 EVA-GAN(Enhanced Various Audio Generation via Scalable Generative Adversarial Networks)을 통해 향상된 다양한 오디오 생성을 소개하고, 스펙트럼 및 고주파 재구성에서 기존 최신 기술에 비해 상당한 개선 효과를 얻으며, 도메인 외 데이터 성능에서도 강건성을 제공하여, 44.1kHz 오디오의 36,000시간의 방대한 데이터 세트, 상황 인식 모듈, Human-In-The-Loop artifact 측정 툴킷을 사용하여 HiFi 오디오 생성을 가능하게 하며, 약 2억 개의 파라미터로 모델을 확장한다. 우리 작업의 시연은 [https://double-blind-eva-gan.cc](https://double-blind-eva-gan.cc)에서 가능하다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 GAN 기반 신경 보코더는 음성 합성, 음성 변환 및 오디오 향상에서 광범위한 응용으로 음향 특성으로부터 오디오 파형 생성에 혁명을 일으켰다. 이들에 의해 제공되는 샘플링 및 메모리 최적화의 효율성에도 불구하고, 그들은 고주파 영역에서 **스펙트럼 불연속성** 및 **블러리니스**와 같은 도전에 직면하여 고품질 음악 및 노래 음성 생성을 차단한다. 도 2는 HiFiGAN(Kong et al., 2020) 및 BigVGAN(gil Lee et al., 2023)을 포함하는 기존의 보코더를 사용하여 노래 데이터를 생성할 때 스펙트럼 중단을 도시한 것으로, 이는 현재의 최신 기술을 나타낸다. 우리는 이러한 문제를 데이터 다양성 부족, 제한된 모델 용량, 훈련 및 추론 동안 사용된 컨텍스트 창 크기 간의 불일치, 이러한 아티팩트를 측정하기 위한 객관적인 메트릭의 부재로 돌린다.\n' +
      '\n' +
      '이 영역에서 또 다른 중요한 과제는 특히 음악과 노래 합성에 사용되는 고품질 오디오 충실도를 달성하는 것이며, 이는 불충분하게 해결되지 않은 채로 남아 있다. BigVGAN(gil Lee et al., 2023)은 최신 OOD(out-of-domain) 성능으로의 경로로서 데이터세트 및 모델의 스케일링을 제안했다. 그러나 저충실도(24kHz) 음성 데이터에 국한된 LibriTTS 데이터세트(Zen et al., 2019)에 대한 의존도는 사실적인 음악 및 노래 생성에 필요한 다양하고 풍부한 음향 특성을 포착하는 데 미치지 못한다. 모델의 스케일링은 성능 향상을 위한 중추적인 전략으로 인식되고 있지만, 대다수의 보코더에는 2천만 개 미만의 매개변수가 장착되어 있다. 이러한 한계는 주로 그라디언트 풋프린트를 관리하는 것과 관련된 상당한 계산 비용 및 메모리 요구 사항으로 인한 것이며, 따라서 고충실도 음악 및 노래 오디오 합성의 발전을 방해한다.\n' +
      '\n' +
      '더욱이, 스펙트로그램 품질에 대한 민감도가 감소된 것으로 알려져 있는 음성 데이터 세트에 대한 기존 작업의 일반적인 평가는 신경 보코더에서 고주파수 ** 아티팩트** 및 스펙트로그램 불연속의 자동 평가를 위한 효과적인 객관적 메트릭의 결핍으로 이어졌다. 우리는 현재 메트릭이 미묘함에도 불구하고(예: 20밀리초만 지속되는 불연속성) 인간에게 매우 감지할 수 있는**인 인공물을 감지하지 못한다는 것을 확인했다. 이는 훈련 과정에서 이러한 뉘앙스를 파악할 수 있는 강건한 평가 방법의 필요성을 강조한다.\n' +
      '\n' +
      '오디오 생성의 현재 문제는 자연어 처리(NLP)의 초기 문제와 매우 유사하다. GPT-3와 같은 대규모 언어 모델(LLM)이 등장하기 전에 NLP 분야의 합의는 문제를 해결하기 위해서는 전문적인 설계와 기술이 필요하다고 믿었다. 유사하게, 노래 합성, 텍스트 투 스피치, 및 음악 합성과 같은 오디오 생성 태스크는 전형적으로 상이한 모델 아키텍처를 사용하였다. 그러나 GPT 시리즈의 도입은 광범위한 확장 가능한 솔루션이 작업별 조정이나 고유한 모델 설계가 필요 없이 다양한 NLP 작업을 효과적으로 해결할 수 있음을 보여주는 중요한 패러다임 변화를 나타냈다. 이러한 변화에 착안하여, 본 논문에서는 EVA-GAN(Scalable Generative Adversarial Networks)을 통한 향상된 다양한 오디오 생성을 제안한다. EVA-GAN은 모델 및 데이터 스케일링에 초점을 맞추어 오디오 생성을 현대화하여 산업 표준 딥러닝 방법에 따라 일반화되고 견고한 보코더를 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '우리의 기여는 다면적이다:\n' +
      '\n' +
      '1. (gil Lee et al., 2023)에 비해 EVA-GAN을 2억 개의 파라미터로 확장하고, 전체 36,000시간 HiFi(44.1kHz) 데이터 세트를 활용했는데, 이는 **최대** 모델 및 신경망 보코더에서 사용되는 데이터이다.\n' +
      '2. 우리는 모델 성능에서 상당한 도약을 나타내는 CAM이라고 하는 새로운 상황 인식 모듈을 소개하며, 표 1에 문서화된 바와 같이 추가 계산 부담이 거의 없이 뛰어난 발전을 달성한다.\n' +
      '3. 보다 긴 컨텍스트 윈도우(약 3초), Encodec(Defossez et al., 2022)에 의해 초기에 도입된 손실 밸런서, 통합된 그래디언트 체크포인팅, 및 트레이닝 안정성을 증진시키고, 메모리 사용량을 감소시키고, 수동 하이퍼파라미터 튜닝의 필요성을 최소화하기 위해 개선된 활성화 함수를 포함하는 혁신적인 트레이닝 파이프라인을 제안한다.\n' +
      '4. 새로운 Human-In-The-Loop SMOS(Similarity Mean Option Score) 평가 툴킷을 구축하고, 인공물 모니터링을 가능하게 하며, 인간의 주관적 인식과 비교할 수 없는 정렬을 보장한다.\n' +
      '\n' +
      '전반적으로, 우리는 고품질 오디오 생성에 특히 적합한 최첨단 44.1kHz 보코더, EVA-GAN을 만들어 이 도메인에서 새로운 산업 벤치마크를 구축했다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '딥러닝을 이용하여 음향 특성으로부터 오디오 파형을 생성하는 신경 보코더는 음성 합성, 음성 변환, 음성 향상 등에서 없어서는 안 될 존재가 되었다. 신경 보코더 기술의 진화는 자기 회귀(AR) 또는 흐름 기반 모델(van den Oord et al., 2016; Kalchbrenner et al., 2018; Prenger et al., 2019), GAN 중심 접근법(Yamamoto et al., 2020; Yang et al., 2020; Kong et al., 2020; Jang et al., 2021), 및 직접 스펙트럼 생성(DSG) 전략(Siazdak, 2023; Kaneko et al., 2022)의 세 가지 별개의 단계로 분할될 수 있다.\n' +
      '\n' +
      '###GAN 기반 신경망 보코더\n' +
      '\n' +
      '이 중 현재 동급 최고 수준의 GAN 기반(Kong et al., 2020; gil Lee et al., 2023; Kaneko et al., 2022; Siuzdak,\n' +
      '\n' +
      '그림 1: EVA-GAN 생성기는 컨텍스트 인식 블록과 업샘플 병렬 재블록의 두 가지 주요 섹션으로 구성된다. 본 논문의 새로운 소개인 _Context Aware Blocks_는 잔여 연결과 큰 컨볼루션 커널을 활용하여 최소한의 계산 오버헤드로 모듈의 컨텍스트 윈도우와 용량을 증가시킨다. HiFi-GAN의 MRF(multi-receptive field fusion) 블록(Kong et al., 2020)으로부터 적응된 _Upsample Parallel Resblocks_는 특징을 파형으로 디코딩하기 위해 _SiLU_(Elfwing et al., 2017) 활성화 함수를 이용한다.\n' +
      '\n' +
      '2023) 보코더는 인상적인 샘플링 효율과 메모리 최적화를 높입니다. 그러나 스펙트럼 불일치, 고주파수 및 저주파수 범위 아티팩트, 디콘볼루션 체커보드 아티팩트와 같은 자체 챌린지 세트가 함께 제공되어 오디오 품질이 저하됩니다. 나아가, 기본 HiFi-GAN(Kong et al., 2020)은 음악 데이터, 고음역대 오디오, OOD(Out-of-distribution) 콘텐츠 등을 다룰 때 부족한 경향이 있어 오디오의 혼란을 야기하고 있다.\n' +
      '\n' +
      '### 발전기 및 손실항 개발\n' +
      '\n' +
      'UnivNet(Jang et al., 2021)은 Mel-Spectrogram 손실을 다중 스케일 STFT 손실로 대체하고, 고주파 영역 복구를 부스팅하기 위해 다중 해상도 판별기를 채택한다. 한편, BigVGAN(gil Lee et al., 2023)은 스펙트로그램 품질 및 OOD(Out-of-distribution) 성능을 향상시키기 위해 Snake Activation을 사용하고자 하였다. 한편, NSF-HiFi-GAN(Zhao et al., 2020; Kong et al., 2020; Openvpi, 2022), RefineGAN(Xu et al., 2021) 및 SingGAN(Huang et al., 2022)과 같은 모델은 오디오 품질 및 스펙트럼 연속성을 높이기 위해 f0 소스를 통합하지만, 이는 크고 다양한 데이터 세트를 활용하는 것을 제한한다.\n' +
      '\n' +
      '차별자 개선을 통한 아티팩트 감소\n' +
      '\n' +
      '판별자는 보코더 훈련에서 중요한 역할을 하며, 인간에 민감한 인공물을 줄이는 것과 손실과 같은 객관적인 점수를 최적화하는 것 사이의 균형을 이룬다. MPD, MSD, MRD, MS-STFTD 및 MS-SBCQTD를 포함한 다양한 접근법이 이전 작업에서 탐색되었으며, 모두 이러한 아티팩트를 최소화하고 고주파 영역 재구성을 향상시키는 것을 목표로 한다.\n' +
      '\n' +
      'HiFi-GAN(Kong et al., 2020)은 MPD(Multi-Period Discriminator)를 도입하여 파형을 다양한 높이의 2D 표현으로 변환하고 주기적인 구조를 분석하기 위해 2D 컨볼루션을 사용한다. 같은 맥락에서 HiFi-GAN의 MSD(Multi-Scale Discriminator)는 파형을 다른 스케일로 여러 개의 1D 표현으로 처리하여 시간 영역 정보에 대한 상세한 분석이 가능하다.\n' +
      '\n' +
      'UnivNet(Jang et al., 2021)은 STFT(Short-Term Fourier Transform)를 통해 다해상도 시간-주파수 영역을 중심으로 MSRD 또는 MRD(Multi-Resolution Spectrogram Discriminator)를 제안하였다. 마찬가지로, Encodec(Defossez et al., 2022)은 오디오 생성 품질을 향상시키기 위해 Multi-Scale STFT Discriminator(MS-STFTD)를 주창하였다.\n' +
      '\n' +
      '이러한 혁신을 더 나아가, (Gu et al., 2023)은 다중-스케일 서브-대역 상수-Q 변환 판별기(MS-SBCQTD)를 도입하였다. 이 새로운 방법은 기존의 STFT의 대안인 Constant Q Transform을 이용하여 고주파 성분을 보다 효과적으로 복원할 수 있도록 발전기를 지원한다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '이 원고는 생성적 적대 네트워크(GAN) 기반 보코더에 대한 기초 작업을 확장하며, 특히 HiFiGAN(Kong et al., 2020)에 의해 도입된 아키텍처 패러다임을 활용한다. 이 프레임워크의 중요한 요소에 대한 설명은 후속 개발을 이해하는 데 필수적이다. 최소 제곱 GAN(Mao et al., 2017)에 의한 특정 손실 메트릭의 개시는 HiFiGAN의 구현으로 두드러지게 예시된 GAN 기반 보코더에서의 그들의 채택을 예고했다.\n' +
      '\n' +
      '### Generator\n' +
      '\n' +
      'GAN 기반 보코더 프레임워크 내의 생성기는 Mel-Spectrogram을 처리되지 않은 파형으로 변환하는 작업을 수행한다. HiFiGAN(Kong et al., 2020)에서 생성기의 손실 메트릭은 Mel-Spectrogram 손실\\(L_{mel}\\), 적대적 손실\\(L_{adv}\\), 및 특징 매칭 손실\\(L_{fm}\\)을 캡슐화하며, 다음과 같이 아티큘레이션된다:\n' +
      '\n' +
      '\\[L_{mel}(G)=\\mathbb{E}\\Big{[}\\|\\phi(x)-\\phi(G(s))\\|_{1}\\Big{]} \\tag{1}\\]\n' +
      '\n' +
      '\\[L_{adv}(G)=\\mathbb{E}\\Big{[}(D(G(s))-1)^{2}\\Big{]} \\tag{2}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Experiment & Total Params & Generator Params & Train Mem & Infer Mem & Infer Time & PESQ \\\\ \\hline HiFi-GAN-base & 13.6 M & 13.6 M & 43.2 GB & 2.1 GB & 177 ms & 3.5486 \\\\ EVA-GAN-base & 34.85 M & 16.3 M & 46 GB & 2.2 GB & 193 ms & 4.0330 \\\\ EVA-GAN-big & 192.99 M & 174.44 M & 68 GB & 2.7 GB & 402 ms & 4.3536 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 모델들에 걸쳐 측정된 속도, 결과는 fp32 포맷의 오디오의 512 프레임(약 6초)을 갖는 16의 배치 크기를 사용하여 단일 A100 GPU 상에서 획득되었다. 기존 HiFi-GAN(Kong et al., 2020)과 비교하여 leaky ReLU를 SiLU(Elfwing et al., 2017)로 대체하고, in-place 활성화 최적화를 적용하고, 44.1k 세대에 대한 커널 크기를 최적화하였다. EVA-GAN-base는 Context Aware Module로 강화된 HiFi-GAN-base인 반면, EVA-GAN-large는 174M 파라미터로 발전기의 축소된 버전을 나타낸다. 보고된 트레이닝 메모리는 옵티마이저의 적용을 제외하고, 순방향 및 역방향 메모리 사용을 모두 포함한다. 속도 메트릭은 10개의 초기 워밍업 실행에 이어 평균 100개의 추론을 기반으로 한다. PESQ(광대역)는 LibriTTS dev 세트에서 관찰된다.\n' +
      '\n' +
      '[L_{fm}(G)=\\mathbb{E}\\Big{[}\\sum_{i=1}^{T}\\frac{1}{N_{i}}\\|D^{i}(x)-D^{i}(G(s))\\|_{1}\\Big{}}\\tag{3}\\frac{1}{N_{i}(x)\n' +
      '\n' +
      '여기서 \\(x\\)은 Ground truth를 나타내고, \\(\\phi\\)은 Mel-Spectrogram 함수를 나타내며, \\(s\\)은 오디오 신호에 해당하는 Mel-Spectrogram을 나타내고, \\(T\\)은 판별기의 층수를 나타내고, \\(D\\)과 \\(G\\)은 각각 판별기와 생성기를 나타내며, \\(N_{i}\\)은 판별기의 \\(i\\)번째 층에서의 특징의 수를 나타낸다.\n' +
      '\n' +
      '### Discriminator\n' +
      '\n' +
      '멜-스펙트로그램 손실 외에도 현대 GAN 기반 보코더는 지각 왜곡을 최소화하기 위해 여러 판별기를 통합하며, 이는 스펙트로그램 충실도와 같은 객관적인 측정을 잠재적으로 향상시켰음에도 불구하고 인지적으로 식별할 수 있다. 특히, 이러한 판별자는 다해상도 특징 분석을 보편적으로 채택한다.\n' +
      '\n' +
      '각각의 판별기는 다음의 적대적 손실 메트릭을 사용하여 평가된다:\n' +
      '\n' +
      '\\[L_{adv}(D)=\\mathbb{E}\\Big{[}(D(x)-1)^{2}+(D(G(s)))^{2}\\Big{]}, \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(x\\)은 Ground truth를 나타내고, \\(s\\)은 오디오 신호와 관련된 Mel-Spectrogram을 나타내며, \\(D\\)과 \\(G\\)은 각각 Discriminator와 Generator를 나타낸다.\n' +
      '\n' +
      '## 4 Eva-Gan\n' +
      '\n' +
      '우리는 EVA-GAN을 HiFi-GAN(Kong et al., 2020)에 대한 진보로 포지셔닝하며, 더 큰 컨텍스트 윈도우, 개선된 구조, 증가된 용량 및 확장된 데이터세트를 특징으로 한다.\n' +
      '\n' +
      '### Data Scaling\n' +
      '\n' +
      '전통적으로 보코더의 훈련은 LJSpeech(Ito & Johnson, 2017), LibriTTS(Zen et al., 2019), VCTK(Veaux et al., 2017), M4Singer(Zhang et al., 2022) 등의 데이터셋을 이용하는 것을 포함한다. 그러나 이러한 데이터 세트는 낮은 샘플링 비율(24k)로 고통받거나 다양성이 부족하여 제한된 범위의 스피커와 언어만 포함한다. 데이터 스케일링의 중요성은 BigVGAN 연구(gil Lee et al., 2023)에 의해 강조되었으며, 이는 LibriTTS(Zen et al., 2019) 데이터세트로 스케일링될 때 상당한 분포 외 개선을 보여주었다. 이를 기반으로 모델 견고성을 강화하기 위해 규모를 더욱 늘리는 것을 목표로 합니다.\n' +
      '\n' +
      '이를 위해 피쉬오디오 커뮤니티는 유튜브뮤직과 네티즈뮤직(HiFi-16000h)의 종합 1만6000시간 고충실도 음악과 노래 데이터셋을 작성했다. 이 데이터 세트는 중국어, 영어, 일본어를 포함한 다양한 언어를 포함하며 광범위한 악기를 특징으로 한다. 알고 있는 범위 내에서는, 이것은 지금까지 가장 큰 충실도 오디오 데이터 세트이며 도메인 외 샘플 문제를 효과적으로 해결한다.\n' +
      '\n' +
      '또한 음성 성능을 높이기 위해 방송 플랫폼 플레이어 FM(Termed PlayerFM-20000h)의 다양한 언어 오디오를 피쉬 오디오 커뮤니티에서 추가로 20,000시간 추가했다. 샘플 다양성을 보장하기 위해 HiFi-16000h에서 50%, PlayerFM-20000h에서 50%의 균형 잡힌 분포를 유지했다.\n' +
      '\n' +
      '우리의 결과는 훈련 데이터 세트에서 규모와 다양성의 중요한 역할을 강조한다. 이 광범위한 36,000시간 데이터 세트에 대해 훈련된 우리의 기준 HiFi-GAN은 스펙트럼 불연속성을 효과적으로 줄이고 광범위한 오디오 유형을 복제할 수 있는 능력을 보여준다. 여기에는 노래, 말하기, 베이스, 피아노 및 헬리콥터 소음 및 끓는 주전자 소리와 같은 고유한 소리가 포함되지만 이에 제한되지 않는다.\n' +
      '\n' +
      '### Model Scaling\n' +
      '\n' +
      '빅VGAN의 연구 결과(gil Lee et al., 2023)는 비교적 적당한 크기의 LibriTTS 데이터 세트(Zen et al., 2019)(약 1000시간)에서도 더 작은 모델보다 더 큰 모델의 우수한 성능을 강조하는 것으로, 확장 생성기를 사용하여 견고성과 전체 성능의 현저한 개선을 관찰했다. 따라서 기본 EVA-GAN 기반 모델의 생성기를 1630만에서 17440만 매개변수로 확대하여 보다 강력한 EVA-GAN-빅 변형을 생성했다. EVA-GAN-big의 향상된 능력은 표 1에서 분명하고 특히 고주파 영역에서 연속 및 분해능 측면에서 그림 2에서 입증되었다.\n' +
      '\n' +
      'MPD(Kong et al., 2020) 및 MRD(Jang et al., 2021)와 같은 판별자를 확대하는 것은 비례적인 이익을 산출하지 않았다. 또한, MS-STFT(Defossez et al., 2022) 및 MS-SBCQTD(Gu et al., 2023)와 같은 새로운 판별기를 통합하는 것은 모델 성능을 더 향상시키지 않았다. 우리는 이 결과가 저주파 및 고주파 범위 모두에서 지상 진실과 생성된 오디오 사이의 미묘한 차이를 효과적으로 포착하는 모델의 상당한 용량에 기인한다고 가정한다. 이 능력은 손실 함수에서 특정 주파수 범위에 대한 임의의 트레이드-오프 또는 과대 강조의 필요성을 부정한다.\n' +
      '\n' +
      '### 무료급식: 상황인식모듈(CAM)\n' +
      '\n' +
      'HiFi-GAN(Kong et al., 2020) 생성기를 확장할 때 계산 자원과 메모리에 대한 수요가 증가했음에도 불구하고, 긴 오디오 시퀀스와 실질적인 기울기 풋프린트로 인해 제기된 어려움으로 인해 ConvNeXt(Liu et al., 2022)에서 파생된 빌딩 블록을 활용하는 1D Convolution 기반 컨텍스트 인식 모듈 CAM은 훨씬 적은 메모리와 계산 자원을 필요로 한다는 것을 발견했다. 이 모듈은 자원 효율적일 뿐만 아니라 표 1에서 설명한 대로 객관적인 평가와 주관적인 평가 모두에서 눈에 띄는 개선을 제공한다.\n' +
      '\n' +
      '##갱신 훈련 패러다임\n' +
      '\n' +
      '모델, 데이터 및 컨텍스트 길이를 직관적으로 스케일링하는 것은 성능을 향상시킬 수 있지만, 과제는 제한된 계산 자원의 범위 내에서 이를 달성하고 훈련 안정성을 유지하는 데 있다. 이전에 가장 큰 보코더(gil Lee et al., 2023)인 BigVGAN은 훈련 불안정성 및 계산 제약으로 인해 추가 스케일링에서 장애물에 직면했다. 이러한 문제를 완화하기 위해 컨텍스트 길이를 줄임으로써 안정성과 자원 활용 간의 타협이 이루어졌으며, 이는 자원 집약도가 낮지만 안정성을 희생해야 한다.\n' +
      '\n' +
      '우리의 조사는 기존의 신경 보코더에서 몇 가지 효율적인 기술을 구현하는 데 차이가 있음을 보여주었다.\n' +
      '\n' +
      '######4.4.1 대형 상황창\n' +
      '\n' +
      'HiFi-GAN(Kong et al., 2020) 및 BigVGAN(gil Lee et al., 2023)에서 사용되는 컨텍스트 윈도우들(전형적으로 32 또는 64 프레임들)과 비교하여, 윈도우를 256 프레임들로 확장하는 것은 매우 유익한 것으로 입증되었다. 이러한 증가는 더 빠른 모델 수렴, 더 높은 GPU 활용도를 돕고 스펙트로그램 불연속성을 상당히 감소시킨다. 그러나, 이러한 확장은 또한 GPU 메모리 소비를 증가시키고 트레이닝 속도를 늦춘다. 이를 완화하기 위해 아래에 설명된 대로 여러 최적화를 구현했다.\n' +
      '\n' +
      '######4.4.2 SiLU In-place Activation\n' +
      '\n' +
      '우리는 Leaky ReLU 활성화 함수를 SiLU(Elfwing et al., 2017)로 대체하는 것이 모델 수렴을 가속화할 뿐만 아니라 최종 성능을 보존하는 것을 관찰했다. 또한 가능한 한 제너레이터와 판별기 모두에 제자리 활성화를 사용하면 GPU 메모리 사용량이 약 30% 감소했다.\n' +
      '\n' +
      '######4.4.3 기울기 체크포인팅\n' +
      '\n' +
      '표 1에 표시된 바와 같이, 큰 배치 크기는 A100 GPU에서도 256-프레임 컨텍스트 윈도우로 실현 가능하지 않다. 따라서, 생성기 및 판별기 모두에 기울기 체크포인팅(Chen et al., 2016)을 적용하여 기울기 메모리 풋프린트를 상당히 감소시켰다. 예를 들어, EVA-GAN-base 생성기의 메모리 사용량은 훈련 속도가 30% 감소했지만 46GB에서 16GB로 감소했다.\n' +
      '\n' +
      '#### 4.4.4 TensorFlow-32\n' +
      '\n' +
      'fp16과 bf16을 사용한 혼합 정밀 훈련은 큰 언어 모델과 컴퓨터 비전 작업에서 일반적이지만, fp16(특히 큰 기울기 규범)의 불안정성과 낮은 정밀도로 인해 bf16의 성능 저하에 직면했다. 따라서 A100 GPU를 활용하여 fp32의 약 2배 속도를 제공하는 텐서플로우-32를 훈련용으로 선택했다.\n' +
      '\n' +
      '######4.4.5 Loss Balancer\n' +
      '\n' +
      '모델을 확장하고 44.1k 구성을 채택할 때 특징 매칭 손실, 적대적 손실, 멜-스펙트로그램 손실 및 다중 스케일 STFT 손실과 같은 다양한 손실의 균형을 맞추는 데 어려움을 겪었다. 엔코덱(Defossez et al., 2022)에 의해 보코더 분야에 도입된 개념인 손실 밸런서가 해결책으로 등장하였다. 이 기술은 기울기에 따라 이러한 손실의 균형을 자동으로 유지하여 각각이 매개변수 업데이트 프로세스에 동등하게 기여할 수 있다. 이 균형이 없는 경우 판별자 손실의 무게를 적절하게 조정할 수 없기 때문에 l1 멜-스펙트로그램 거리와 같은 겉보기에 허용 가능한 객관적인 결과에도 불구하고 인간이 감지할 수 있는 상당한 고주파 인공물을 관찰했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c|c} \\hline \\hline \\multicolumn{1}{l|}{LibriTTS} & M-STFT (\\(\\downarrow\\)) & Periodicity (\\(\\downarrow\\)) & V/UV F1 (\\(\\uparrow\\)) & PESQ (\\(\\uparrow\\)) & SMOS (\\(\\uparrow\\)) \\\\ \\hline Ground Truth & - & - & - & - & 4.909 \\\\ \\hline Vocos & 0.8580 & 0.1103 & 0.9555 & 3.6328 & 4.8577 \\\\ UnivNet-c32 & 0.8959 & 0.1333 & 0.9444 & 3.2566 & 4.8042 \\\\ HiFi-GAN (V1) & 1.3647 & 0.1600 & 0.9309 & 2.9110 & 4.7596 \\\\ \\hline BigVGAN-base & 0.8788 & 0.1287 & 0.9459 & 3.5190 & 4.8545 \\\\ BigVGAN-big & 0.7997 & 0.1018 & 0.9598 & 4.0270 & 4.8786 \\\\ \\hline HiFi-GAN-base & 1.0269 & 0.1230 & 0.9523 & 3.5485 & 4.8345 \\\\ EVA-GAN-base & 0.9485 & 0.0942 & 0.9658 & 4.0330 & 4.8687 \\\\ EVA-GAN-big & **0.7982** & **0.0751** & **0.9745** & **4.3536** & **4.9134** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: LibriTTS 개발 세트에 대해 EVA-GAN의 객관적 평가를 수행하였고, BigVGAN 연구(gil Lee et al., 2023)와 유사한 테스트 세트를 사용하여 주관적 평가를 수행하였다. 이러한 벤치마크에 맞추기 위해 EVA-GAN의 출력을 24kHz로 다운 샘플링했다. 보코스(Siazdak, 2023) 중량은 제멜로-아이/보코스로부터 얻었다. UnivNet-c32(Jang et al., 2021) 모델 가중치는 maum-ai/univnet으로부터 조달되었다. 22kHz에서 LJSpech, VCTK, 및 LibriTTS에 대해 트레이닝된 HiFi-GAN(Kong et al., 2020)(V1) 버전은 jik876/hifi-gan으로부터 소스되었다. BigVGAN은 논문(gil Lee et al., 2023)에서 직접 객관적인 결과를 얻었고, 주관적 평가는 NVIDIA/BigVGAN에서 획득한 500만 단계에 대해 훈련된 가중치를 기반으로 했다.\n' +
      '\n' +
      '### Human In-The-Loop Artifact 측정\n' +
      '\n' +
      'PESQ(Rix et al., 2001) 및 Mel 거리와 같은 메트릭은 생성된 오디오와 지상 진실 사이의 불일치를 정량화하지만, 그들의 높은 점수는 반드시 주관적인 평가 결과와 상관관계가 있는 것은 아니다. 이러한 오정렬은 이러한 메트릭이 특히 인간 청취자가 인지할 수 있는 아티팩트, 특히 고주파 영역에서 발생하는 아티팩트 또는 불과 몇 밀리초 동안 지속되는 단기 스펙트로그램 중단을 부적절하게 포착하기 때문에 발생한다. 이러한 뉘앙스는 오디오 품질에 중요하지만 기존의 메트릭에 의해 간과된다. 이러한 격차를 해결하는 데 있어 HiFiGAN(Kong et al., 2020)과 같은 모델의 판별자는 객관적인 메트릭에 항상 반영되지는 않지만 이러한 미묘한 차이를 식별하는 능력을 강조하면서 도구적이었다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해, 우리는 인간 지각 표준에 대해 생성된 오디오의 품질을 지속적으로 모니터링하고 평가하기 위해 SMOS(Similarity Mean Option Score) 주석 도구와 CLI를 통합한 Human-In-The-Loop Artifact Measurement 툴킷을 고안했다.\n' +
      '\n' +
      '## 5 실험 설정\n' +
      '\n' +
      'BigVGAN(gil Lee et al., 2023)과 유사한 접근법을 채택하여, 우리는 기본 학습률을 1e-4로 설정하고 1000의 규범에서 그래디언트 클리핑을 적용하여 그래디언트 스파이크 및 폭발을 관리한다. 우리가 선택한 최적기는 베타가 \\([0.8,0.99]\\)으로 설정된 AdamW였다. LLM에서 일반적으로 실행되는 것에 따라 HiFi-GAN(Kong et al., 2020) 및 그 파생어에서 사용되는 에포크 기반 접근법과 달리 단계적 학습 속도 붕괴를 선택했다. 구체적으로, 각 단계 후에 \\(0.999999\\)의 지수 학습률 감쇠를 구현하였다.\n' +
      '\n' +
      '본 모델은 512x 업샘플링 속도를 사용하여 2048 n.fft, 512 홉 길이 및 2048 승 길이로 160빈 멜-스펙트로그램 입력을 수용한다. 앞서 언급한 바와 같이, 우리는 더 큰 컨텍스트 윈도우가 매우 유익하다는 것을 발견하여, HiFi-GAN(Kong et al., 2020) 및 BigVGAN(gil Lee et al., 2023)과 같은 이전 모델에서 사용된 32 또는 64 프레임 대신에 256 프레임 컨텍스트 윈도우(대략 3초)를 사용하였다. 메모리 제약으로 인해 총 배치 크기는 이전 모델에서 사용된 32개와 달리 16개로 설정되었다. 그러나 모델의 더 큰 프레임 크기를 감안할 때, 우리는 여전히 배치당 더 높은 유효 프레임 수를 달성하여 훈련 안정성을 더욱 향상시켰다.\n' +
      '\n' +
      'Multi Period Discriminator는 HiFiGAN(Kong et al., 2020)에 의해 소개된 바와 같이 \\([3,5,7,11,17,23,37]\\)의 기간을 사용하였다. UniNet(Jang et al., 2021)이 소개한 다해상도 판별기 및 다해상도 STFT 손실은 [[2048, 512, 2048], [1024, 120, 600], [2048, 240, 1200], [4096, 480, 2400], [512, 50, 240]의 해상도로 설정하였다. 이 설정은, 이전 작품들에 의해 제공된 24k 구성과 비교된다(Kong et al., 2020; gil Lee et al., 2023; Jang et al., 2021; Siuzdak, 2023),\n' +
      '\n' +
      '도 2: HiFi-GAN(Kong et al., 2020), BigVGAN(gil Lee et al., 2023), 및 우리의 EVA-GAN의 베이스 및 빅 버전 둘 다에 의해 생성된 44.1kHz 노래 음성에 대한 스펙트로그램 시각화는 스펙트로그램 연속성 및 고주파 디테일의 차이를 설명하기 위해 고주파 영역에 대한 줌-인 뷰를 포함하여 제시된다. 22kHz에서 LJSpeech, VCTK 및 LibriTTS 데이터세트에 대해 트레이닝된 HiFi-GAN(Kong et al., 2020)(V1) 모델은 jik876/hifi-gan으로부터 획득되었다. 빅VGAN의 가중치는 공식 저장소 NVIDIA/BigVGAN에서 조달되었다.\n' +
      '\n' +
      '고주파 영역에서 모델의 성능을 향상시켰습니다.\n' +
      '\n' +
      '앞서 언급한 설정과 tf32 정밀도를 사용하여 기울기가 폭발하거나 충돌하는 문제 없이 안정적인 훈련을 경험했다. 달리 명시되지 않는 한 모든 모델은 NVIDIA A100 GPU에서 100만 단계에 대해 50% HiFi-16000h 및 50% PlayerFM-20000h로 구성된 36,000시간 데이터 세트에 대해 훈련되었다.\n' +
      '\n' +
      'HiFiGAN(Kong et al., 2020) v1 구성을 시작으로 Leaky ReLU 활성화를 SiLU 활성화로 대체하고 44.1k baseline을 설정하여 **HiFi-GAN-Base**를 생성하였다. 우리는 \\([8,8,2,2,2]\\)의 업샘플 비율, \\([16,16,4,4,4]\\)의 업샘플 커널 크기 및 \\([3,7,11]\\)의 병렬 블록(Kong et al., 2020)) 커널 크기를 선택했다. 이 구성은 13.6M 생성기로 이어졌다.\n' +
      '\n' +
      '**HiFi-GAN-Base**를 기반으로, \\([3,3,9,3]\\) 및 \\([128,256,384,512]\\), 0.2 드롭 경로 비율, 7의 커널 크기에서 각 블록에 대한 깊이와 딤을 생성하는 Context Aware Module을 생성기에 추가하였다. 이는 입력 차원이 160에서 512로 증가한 것을 고려하여 16.3M 생성기를 갖는 **EVA-GAN-base**와 Context Aware Module에 18.6M을 추가하였다.\n' +
      '\n' +
      '*EVA-GAN-big**을 개발하기 위해 **EVA-GAN-base**에서 모든 설정을 유지했지만 업샘플 비율을 \\([4,4,2,2,2,2,2]\\), 업샘플 커널 크기를 \\([8,8,4,4,4,4,4,4]\\), 병렬 블록 커널 크기를 \\([3,7,11,13]\\)으로 변경했다. 업샘플링을 위한 초기 채널은 1536으로 설정되었으며, 이 스케일링은 발전기 크기를 174.4M으로 증가시켜 총 파라미터 수를 193M으로 증가시켰다.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      '우리는 EVA-GAN의 성능을 평가하고 LibriTTS(24k speech) 및 DSD-100(48k music)을 포함한 여러 작업에 걸쳐 기존 방법과 비교했으며 상당한 성능 향상을 관찰했다.\n' +
      '\n' +
      '표 1은 최적화된 HiFi-GAN(Kong et al., 2020) 베이스라인과 비교하여, Context Aware Module이 추론 속도 및 트레이닝 메모리에서 최소한의 오버헤드를 추가하지만, 데이터세트 상에서 트레이닝될 때 성능을 상당히 향상시킨다는 것을 입증한다. 이는 추가적인 파라미터들이 과도한 자원을 요구하지 않고 네트워크 용량을 효율적으로 향상시킨다는 것을 시사한다. 또한, EVA-GAN-big는 EVA-GAN-base보다 6배 큰 반면, 학습 메모리를 2배만 증가시키고 추론 시간을 1배 느리게 하여 배치 크기가 16인 실시간보다 250배 빠른 속도를 유지한다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      'BigVGAN의 방법론(gil Lee et al., 2023)에 따라, 우리는 LibriTTS 평가를 위해 다음과 같은 객관적인 메트릭을 사용했다:\n' +
      '\n' +
      '* Multi-resolution STFT (M-STFT), Parallel WaveGAN (Yamamoto et al., 2020)에 제공된 바와 같이, csteinmetz1/auraloss (Steinmetz & Reiss, 2020)로부터의 오픈 소스 구현을 사용한다.\n' +
      '* 주기성 에러(CREPE에 기초함) 및 유성/무성 분류 F1 스코어(V/UV F1)를 통해, 소스 모듈이 없는 신경 보코더에서 공통 아티팩트를 강조한다. CARGAN(Morrison et al., 2022) code available at descriptive/cargan을 사용하였다.\n' +
      '* Ludlows/PESQ에서 이용 가능한, 잘 알려진 자동화된 음성 품질 평가 툴의 광대역 버전(16,000 Hz)을 사용하는, 음성 품질의 지각 평가(PESQ)(Rix et al., 2001).\n' +
      '\n' +
      'BigVGAN(gil Lee et al., 2023)에 따르면, 대부분의 수신된 높은 점수와 사소한 데이터 노이즈가 평균 점수를 크게 변경할 수 있기 때문에 5-스케일 평균 의견 점수(MOS)가 각 모델의 성능을 정확하게 반영하지 못한다는 것을 관찰했다. 우리의 작업은 보코더에 대한 복사 합성을 포함하기 때문에, 우리는 모델이 입력 멜-스펙트로그램과 매우 유사한 출력을 생성하는 것을 목표로 한다. 잡음 입력의 경우, 우리는 신경 보코더가 오디오 품질을 훈련 데이터에 편향시키는 것을 원하지 않으며, 이는 인위적으로 높은 청취자 점수를 초래할 수 있다.\n' +
      '\n' +
      '따라서, 우리는 (gil Lee et al., 2023)과 유사한 5-스케일 유사성 평균 의견 점수(SMOS) 접근법을 채택했다. 구체적으로, 참가자들은 생성된 오디오의 품질을 평가하기 위해 참조 오디오(Ground Truth)와 생성된 오디오를 모두 제공받았다. 모든 참가자는 보다 정확한 순위를 위해 조용한 환경에서 헤드폰을 착용하고 오디오를 들어야 했다.\n' +
      '\n' +
      '### LibriTTS\n' +
      '\n' +
      '표 2에서는 24kHz 음성 데이터세트인 LibriTTS(Zen et al., 2019)에 대한 객관적 결과와 주관적 결과를 모두 제시한다. 객관적 평가 메트릭 M-STFT, 주기성, V/UV F1 및 PESQ는 빅VGAN(gil Lee et al., 2023) 및 HiFiGAN(Kong et al., 2020)의 훈련에서 보이지 않는 스피커와 함께 깨끗하고 \'기타\' 범주를 모두 포함하는 LibriTTS 개발 세트에 대해 계산되었다. 주관적 평가를 위해 LibriTTS 테스트 세트에서 무작위로 선택된 100개의 파일 세트에 대해 SMOS 테스트를 수행했다.\n' +
      '\n' +
      '표 2에 자세히 설명된 우리의 연구 결과는 EVA-GAN이 본질적으로 44.1kHz이고 LibriTTS에 대해 훈련되지 않았음에도 불구하고 24kHz로 다운샘플링한 후 모든 객관적이고 주관적인 메트릭에서 현재 최첨단 빅VGAN(gil Lee et al., 2023)보다 훨씬 우수하다는 것을 보여준다. 특히, HiFi-GAN-베이스는 훈련 레시피, 활성화 함수 및 우리의 큰 데이터세트의 사용의 변화만이 있는 HiFiGAN(Kong et al., 2020) 베이스라인에 비해 현저한 개선을 보여준다. 이것은 우리의 새로운 훈련 전략과 더 다양한 데이터 세트의 중요성을 강조한다.\n' +
      '\n' +
      'EVA-GAN-base는 CAM을 통합함으로써 HiFi-GAN-base의 객관적인 결과를 더욱 향상시킨다. 빅VGAN 기반과 비교할 때, EVA-GAN 기반은 메모리를 덜 사용하고 더 빠른 추론 속도를 제공하면서 대부분의 객관적인 메트릭에서 더 나은 성능을 달성한다. 우리의 가장 큰 모델인 EVA-GAN-빅은 EVA-GAN이 LibriTTS에 대해 훈련되지 않았음에도 불구하고 기존의 모든 모델을 능가하는 확장 효과를 검증한다.\n' +
      '\n' +
      '### Dsd-100\n' +
      '\n' +
      '자원 제약 조건을 관리하기 위해 DSD-100 (Liutkus et al., 2017) 데이터 세트에서 44.1kHz 혼합 오디오(Mixture, Bass, Drums, Vocals, Others)의 5가지 범주 각각에서 10개의 트랙을 선택하여 무작위 샘플링 전략을 사용했다. 각 트랙에 대해 평가를 위해 무작위 비침묵 섹션에서 5초 클립을 선택했으며 결과는 표 3에 자세히 설명되어 있다.\n' +
      '\n' +
      '이 데이터 세트를 분석하는 것은 중요한 음악 구성요소를 포괄한다는 점에서 음악 생성에서 신경 보코더의 복원력과 효능에 대한 독특한 관점을 제공했다. 추론은 DSD-100 데이터 세트에 대한 모델의 숙련도가 고충실도 음악과 음성 합성에서 향상된 성능으로 해석될 가능성이 있다는 것이다.\n' +
      '\n' +
      '극도로 OOD 작업에 대한 추가 결과를 포함하여 여기에서 데모에서 사용할 수 있습니다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '**New Training Recipe and Larger Dataset:** HiFiGAN v1(Kong et al., 2020) baseline에 비해, 우리의 HiFiGAN-base는 섹션 4에서 논의되고 표 2 및 그림 2에서 입증된 바와 같이 훈련 레시피 및 데이터세트를 최적화함으로써 객관적인 메트릭을 상당히 향상시킨다.\n' +
      '\n' +
      '**컨텍스트 인식 모듈:** EVA-GAN 기반에 컨텍스트 인식 모듈을 도입하면 객관적인 메트릭이 크게 향상되어 최소한의 오버헤드가 추가되었습니다. 이를 표 1, 표 2, 표 3 및 도 2에서 설명한다.\n' +
      '\n' +
      '**더 큰 모델:** 표 2, 3 및 그림 2에 자세히 설명된 EVA-GAN-big를 생성하기 위한 EVA-GAN의 스케일링 업은 모델 크기 증가의 효과를 보여준다. EVA-GAN-big는 EVA-GAN-base보다 6배 크지만 훈련 하이퍼파라미터나 데이터세트의 변화 없이 음성 및 음악 도메인 모두에서 우수한 성능을 달성하고 다양한 유형의 오디오에 걸쳐 견고함을 나타낸다. OOD 성능을 입증하는 추가 결과는 여기에서 찾을 수 있다.\n' +
      '\n' +
      '## 7 Conclusions\n' +
      '\n' +
      '본 논문에서는 신경 보코더 영역에서 새로운 벤치마크를 설정하는 획기적인 오디오 생성 모델인 **EVA-GAN**을 소개하였다. 광범위한 데이터 세트를 활용하고 CAM과 같은 혁신적인 기능을 통합하고 모델을 약 200M 매개변수로 스케일링함으로써 **EVA-GAN**은 스펙트럼 연속성, 고주파 재구성 및 분산 외 데이터 성능에서 견고성 측면에서 기존 모델보다 크게 우수하다.\n' +
      '\n' +
      '현재까지 가장 큰 데이터 세트를 포함한 다양한 범위의 오디오 데이터에 대해 수행된 포괄적인 실험은 다양한 도메인에 걸쳐 고품질 사실적인 오디오를 생성하는 **EVA-GAN**의 우수한 능력을 보여준다. 이러한 성과는 오디오 합성의 상당한 발전을 나타낼 뿐만 아니라 음성 합성, 음악 생성 및 그 이상의 향후 연구 및 응용 분야에 새로운 길을 열어준다.\n' +
      '\n' +
      '**EVA-GAN**의 최첨단 결과와 효율성이 강조된 놀라운 성능은 오디오 생성에서 새로운 금본위제로 확립되어 오디오 도메인에서 광범위한 응용 프로그램을 향상시킬 것을 약속한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline DSD100 & Mixture (\\(\\uparrow\\)) & Bass (\\(\\uparrow\\)) & Drums (\\(\\uparrow\\)) & Vocals (\\(\\uparrow\\)) & Other (\\(\\uparrow\\)) \\\\ \\hline Ground Truth & 4.7778 & 4.7237 & 4.8533 & 4.7531 & 4.8313 \\\\ \\hline Vocos & 3.1519 & 3.2716 & 3.9857 & 4.2078 & 3.0694 \\\\ UnivNet-c32 & 2.2778 & 2.9241 & 3.5507 & 3.2963 & 2.3247 \\\\ HiFi-GAN (V1) & 2.4023 & 3.0833 & 3.5821 & 3.3188 & 2.5769 \\\\ \\hline BigVGAN-base & 3.3537 & 3.2821 & 4.2464 & 4.3846 & 3.6892 \\\\ BigVGAN-big & 4.0854 & 3.8642 & 4.0909 & 4.5000 & 3.9747 \\\\ \\hline HiFi-GAN-base & 4.5658 & 4.1940 & **4.5493** & 4.6944 & 4.4605 \\\\ EVA-GAN-base & 4.4133 & 4.2405 & 4.5467 & 4.6627 & 4.5634 \\\\ EVA-GAN-big & **4.6197** & **4.4675** & 4.4658 & **4.7467** & **4.6053** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: DSD100 테스트 세트에 대해 평가된 SMOS. 500만 단계에 대해 훈련된 BigVGAN(gil Lee et al., 2023) 웨이트는 공식 저장소 NVIDIA/BigVGAN으로부터 획득되었다. 모든 샘플은 청취자를 위해 44.1kHz로 재샘플링됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Chen et al. (2016) Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep net with sublinear memory cost, 2016.\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. 고충실도 신경 오디오 압축 2022년\n' +
      '* Elfwing et al. (2017) Elfwing, S., Uchibe, E., and Doya, K. 강화 학습에서 신경망 함수 근사화를 위한 시그모이드 가중 선형 단위, 2017.\n' +
      '* gil Lee et al. (2023) gil Lee, S., Ping, W., Ginsburg, B., Catanzaro, B., and Yoon, S. 빅번: 2023년 대규모 훈련을 받은 범용 신경 보코더.\n' +
      '* Gu et al. (2023) Gu, Y., Zhang, X., Xue, L., and Wu, Z. 고충실도 보코더를 위한 다중 스케일 서브밴드 상수-q 변환 판별기, 2023.\n' +
      '* Huang et al. (2022) Huang, R., Cui, C., Chen, F., Ren, Y., Liu, J., Zhao, Z., Huai, B., and Wang, Z. Singgan: Generative adversarial network for high-fidelity singing voice generation, 2022.\n' +
      '* Ito & Johnson (2017) Ito, K. 및 존슨, L. lj 음성 데이터세트. [https://keithito.com/LJ-Speech-Dataset/] (https://keithito.com/LJ-Speech-Dataset/), 2017.\n' +
      '*Jang et al. (2021) Jang, W., Lim, D., Yoon, J., Kim, B., and Kim, J. Univnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation, 2021.\n' +
      '* Kalchbrenner et al. (2018) Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., van den Oord, A., Dieleman, S., and Kavukcuoglu, K. 효율적인 신경 오디오 합성입니다. 2018년 2월\n' +
      '* Kaneko et al. (2022) Kaneko, T., Tanaka, K., Kameoka, H., and Seki, S. isftIntet: Fast and lightweight mel-spectrogram vocoder incorporating inverse short-time fourier transform, 2022.\n' +
      '* Kong et al. (2020) Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis, 2020.\n' +
      '* Liu et al. (2022) Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. 2020년, 2022년 경마장\n' +
      '* 12th International Conference, LVA/ICA 2015, Liberec, Czech Republic, August 25-28, 2015, Proceedings_, pp. 323-332, Cham, 2017. Springer International Publishing.\n' +
      '* Mao et al. (2017) Mao, X., Li, Q., Xie, H., Lau, R. Y. K., Wang, Z., and Smolley, S. P. Least squares generatingative adversarial networks, 2017.\n' +
      '* Morrison et al. (2022) Morrison, M., Kumar, R., Kumar, K., Seetharaman, P., Courville, A., and Bengio, Y. 2022년 조건부 파형 합성을 위한 청크형 자기회귀 gan.\n' +
      '* Openvpi(2022) Openvpi. 44.1khz 샘플링 속도로 nsf-히피건을 방출한다. openvpi/vocoders, 2022년 12월. URL[https://github.com/openvpi/vocoders/releases/tag/nsf-hifigan-v1](https://github.com/openvpi/vocoders/releases/tag/nsf-hifigan-v1).\n' +
      '* Prenger et al. (2019) Prenger, R., Valle, R., and Catanzaro, B. Waveglow: A flow-based generatingative network for speech synthesis. 2019년 3월\n' +
      '* Rix et al. (2001) Rix, A., Beerends, J., Hollier, M., and Hekstra, A. Perceptual evaluation of speech quality (pesq): A new method for speech quality assessment of telephone networks and codecs. volume 2, pp. 749-752 vol.2, 02 2001. ISBN 0-7803-7041-4. doi: 10.1109/ICASSP.2001.941023.\n' +
      '* Siuzdak(2023) Siuzdak, H. Vocos: 고품질 오디오 합성을 위한 시간-도메인 및 푸리에-기반 신경 보코더 사이의 갭을 폐쇄, 2023.\n' +
      '* Steinmetz & Reiss(2020) Steinmetz, C. J. and Reiss, J. D. auraloss: Audio focused loss functions in PyTorch. _Digital Music Research Network One-day Workshop (DMRN+15)_, 2020.\n' +
      '* van den Oord et al. (2016) van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. Wavenet: 원시 오디오에 대한 생성 모델입니다. 2016년 1월\n' +
      '* Veaux et al. (2017) Veaux, C., Yamagishi, J., and MacDonald, K. Cstr vctk 말뭉치: cstr 음성 복제 툴킷을 위한 영어 다중 화자 말뭉치. 2017년\n' +
      '* Xu et al. (2021) Xu, S., Zhao, W., and Guo, J. Refinegan: Universally generating waveform better with ground truth with high accurate pitch and intensity responses. _ arXiv preprint arXiv:2111.00962_, 2021.\n' +
      '* Yamamoto et al. (2020) Yamamoto, R., Song, E., and Kim, J.-M. 병렬 파동: 다중 해상도 스펙트로그램을 갖는 생성적 적대 네트워크를 기반으로 한 빠른 파형 생성 모델이다. 2020년 4월\n' +
      '* Yang et al. (2020) Yang, Y. R., Chen, Y. - A, Tsao, Y. 다중 대역 멜간: 고품질 텍스트 투 스피치를 위한 더 빠른 파형 생성. [5] 2020년\n' +
      '* Zen et al. (2019) Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. 라이브릿: 2019년 텍스트 투 스피치용 librispecech에서 파생된 코퍼스.\n' +
      '* Zhang et al. (2019) Zhang, L., Li, R., Wang, S., Deng, L., Liu, J., Ren, Y., He, J., Huang, R., Zhu, J., Chen, X., and Zhao, Z. M4singer: Multi-style, Multi-singer and musical score provided mandarin singing corpus. 제36차 컨퍼런스 신경 정보 처리 시스템 데이터 세트 및 벤치마크 Track_에서 2022년.\n' +
      '* Zhao et al. (2020) Zhao, Y., Wang, X., Juvela, L., and Yamagishi, J. Trans transferring neural speech waveform synthesizers to musical instrument sound generation. Proc에서. ICASSP_, pp. 6269-6273, 2020. doi: 10.1109/ICASSP40776.2020.9053047. URL[https://ieeexplore.ieee.org/document/9053047/](https://ieeexplore.ieee.org/document/9053047/).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>