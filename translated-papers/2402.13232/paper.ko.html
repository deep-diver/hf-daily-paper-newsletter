<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 멀티모달 정렬을 위한 터치, 비전 및 언어 데이터 세트\n' +
      '\n' +
      'Letian Fu\n' +
      '\n' +
      'Gaurav Datta\n' +
      '\n' +
      'Huang Huang\n' +
      '\n' +
      '윌리엄 청호 패니치\n' +
      '\n' +
      'Jaimyn Drake\n' +
      '\n' +
      'Joseph Ortiz\n' +
      '\n' +
      'Mustafa Mukadam\n' +
      '\n' +
      'Mike Lambeta\n' +
      '\n' +
      'Roberto Calandra\n' +
      '\n' +
      'Ken Goldberg\n' +
      '\n' +
      '동일한 기여도 \\({}^{1}\\)UC Berkeley \\({}^{2}\\)Meta AI \\({}^{3}\\)TU 드레스덴. 대응: Letian Fu \\(<\\)max.fu.letian@berkeley.edu\\(>\\)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '터치는 인간에게 중요한 감지 양식이지만 아직 멀티모달 생성 언어 모델에 통합되지 않았다. 이는 부분적으로 촉각 데이터에 대한 자연어 라벨을 얻는 것의 어려움과 촉각 판독을 시각적 관찰 및 언어 설명 둘 다와 정렬하는 복잡성 때문이다. 그 간극을 메우기 위한 단계로, 이 연구는 인간에 의해 주석이 달린 영어 라벨(10%)과 GPT-4V(90%)의 텍스트 의사 라벨(텍스트 의사 라벨)이 있는 44K 와일드 비전 터치 쌍의 새로운 데이터 세트를 소개한다. 이 데이터셋을 이용하여 개방형 어휘 분류를 위한 비전 언어 정렬 촉각 인코더와 학습된 인코더를 이용하여 텍스트 생성을 위한 터치 비전 언어(TVL) 모델을 학습한다. 결과는 터치를 통합함으로써 TVL 모델이 모든 모달리티 쌍에 대해 훈련된 기존 모델보다 (+29% 분류 정확도) 터치-비전-언어 정렬을 향상시킨다는 것을 시사한다. 데이터 세트의 작은 부분만 인간 레이블이 지정되었지만 TVL 모델은 새로운 터치 비전 이해 벤치마크에서 GPT-4V(+12%) 및 오픈 소스 비전 언어 모델(+32%)에 비해 향상된 시각적 촉각 이해도를 보여준다. 코드 및 데이터: [https://tactile-vlm.github.io](https://tactile-vlm.github.io)\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '거의 모든 생물학적 인식은 본질적으로 멀티모달(Bertelson and De Gelder, 2004; Turk, 2014; Bruck et al., 2022)이며, 에이전트가 여러 정보 스트림에 기초하여 추론하고 결정을 내릴 수 있게 한다. 인공 멀티모달 표현 학습에서의 최근 연구는 비전, 언어, 오디오, 온도, 및 로봇 액션과 같은 모달리티들을 연결하는 것을 탐구하였다(Radford et al., 2021; Girdhar et al., 2023; Guzhov et al., 2021; Brohan et al., 2023; Radosavovic et al., 2023). 그러나 촉각적 양식은 복합적 이해에 대한 미개척으로 남아 있다. 터치는 인간이 표면 텍스처, 물체 재료, 치수, 및 접촉력을 구별할 수 있게 한다(Johansson and Flanagan, 2009; Dahiya et al., 2009; Klatzky and Lederman, 2003). 촉각 인식은 또한 로봇 애플리케이션들, 특히 접촉-풍부 조작 태스크들에 대해 유용한 것으로 입증되었다(Lambeta et al., 2020; Dahiya et al., 2009; Calandra et al., 2018; Yuan et al., 2017; Dave et al., 2024; Qi et al., 2023).\n' +
      '\n' +
      '많은 작품들은 또한 시각적 촉각 연관을 탐색하고, 교차-모달 생성기들을 구축하며, 폐쇄된 어휘 세트 상의 재료 특성, 표면 텍스처, 및 천 분류에 관한 레버리지 교차-모달을 탐색한다(Yang et al., 2022; Dave et al., 2024; Li and Adelson, 2013; Ojala et al., 2002; Kampouris et al., 2016; Yuan et al., 2018; Kerr et al., 2023).\n' +
      '\n' +
      '그러나, 인간의 촉각 지각은 촉각-시각적 연관성보다 _more_를 포착한다; 촉각 모달리티는 다양한 의미 정보를 포착하고 언어와의 깊은 통합을 입증한다(Schmidt et al., 2019; Speed et al., 2021; Miller et al., 2018; ajbarnett, 2023). 터치와 언어의 통합에 대한 한 가지 주요 장애물은 다양한 데이터의 부족이다. 최근 연구는 촉각 기반 질감 또는 재료 분류를 위해 쌍을 이루는 촉각 및 시각적 관찰의 데이터 세트와 인간 라벨 데이터 세트를 모두 수집했지만 개방형 어휘 언어 레이블을 포함하는 촉각 데이터 세트는 알지 못한다. 따라서 우리는 커스텀을 개발합니다\n' +
      '\n' +
      '그림 1: 구현된 에이전트가 시각 및 언어와 접촉을 통합할 수 있습니까? 우리가 아는 한, 본 연구는 첫 번째 개방형 어휘 촉각-시각-언어 데이터 세트를 제시하고, 1) 시각-언어 정렬 촉각 인코더와 2) 촉각 감각을 묘사하기 위한 촉각-시각-언어 모델(TVLM)을 훈련한다.\n' +
      '\n' +
      '제어된 실험실 설정 외부에서 동기화된 "야생형" 터치-비전 데이터 수집을 위한 핸드-헬드 장치(도 2). 이 설정을 통해 다양한 전경 표면과 다양한 배경을 가진 물체를 누르고 슬라이딩하면서 _close-up_ 시각 관찰 및 촉각 판독을 캡처할 수 있다. 또 다른 과제는 인간 라벨링이 비용이 많이 들 수 있고 촉각 경험에 대한 언어 설명이 주관적이며 개인마다 다르다는 것이다. 이러한 문제를 해결하기 위해, 본 논문에서는 대규모 언어 모델(LLM) 및 비전 언어 모델(VLM)을 훈련하는 이전 작업으로부터 영감을 얻는다(Taori et al., 2023; Wang et al., 2022b; Liu et al., 2023b; Chen et al., 2023b). 이는 자체 또는 기존의 LLM에 의해 합성된 데이터에 대한 훈련에 의해 비전 언어 이해를 입증하는 것이다. 본 논문에서는 Off-the-shelf LLM (GPT-4V (OpenAI et al., 2023))을 이용하여 시각적 관찰로부터 촉각적 설명을 생성하고, 이것이 라벨링된 촉각 언어 데이터의 희소성을 완화하는 효과적인 캡션 역할을 할 수 있다고 가정한다.\n' +
      '\n' +
      '이 작업에서 우리는 44K 쌍의 시력-촉각 관찰로 구성된 새로운 데이터 세트인 **T**ouch-**V**ision-**L** 언어(**TVL**) 데이터 세트를 제시하며, 여기서 데이터의 10%는 사람이 주석을 달고 나머지는 GPT-4V에 의해 레이블이 지정된다. 모든 모달리티를 시각에 바인딩하는 대신(Girdhar et al., 2023), 세 모달리티 모두에서 쌍별 대조 학습을 수행하여 이 데이터 세트에서 촉각 인코더를 훈련한다. 우리는 OpenCLIP(Ilharco et al., 2021)의 기존 비전 및 언어 인코더를 활용하여 텍스트 및 시각적 양식에 모두 정렬된 촉각 인코더를 훈련한다. 본 논문에서는 터치-비전과 터치-언어 분류를 위한 인코더의 기능을 이용하여 정렬을 평가한다.\n' +
      '\n' +
      '데이터세트 및 트레이닝된 촉각 인코더를 활용하여, 우리는 후속적으로 LLaMA2 7B(Touvron et al., 2023)를 미세조정하여 시각적 및 촉각적 관찰에 기초하여 촉각적 이미지의 텍스트 설명을 생성한다(도 1). 이 모델을 평가하기 위해, 우리는 촉각적 설명을 생성하기 위해 멀티모달 모델을 쿼리하고 LLM을 사용하여 그라운드 트루스 인간 주석과의 일관성을 평가하는 Touch-Vision-Language Benchmark를 제안한다.\n' +
      '\n' +
      '제안된 터치 비전 언어 모델은 소량의 인간 라벨 데이터에 대해서만 훈련되었으며, 라벨 생성 모델인 오픈 소스 VLM(+32% 개선) 및 GPT-4V(+12% 개선)와 비교할 때 TVL 벤치마크에서 통계적으로 유의미한 성능 향상을 보여준다.\n' +
      '\n' +
      '이 논문은 다음과 같은 기여를 한다.\n' +
      '\n' +
      '1. **TVL**, 인간 또는 VLM 중 하나로 주석이 달린 44K 쌍을 이루는 촉각-시각 관찰을 포함하는 새로운 데이터세트는 언어 주석이 달린 촉각 데이터의 부족을 해결하는 촉각 기술 생성;\n' +
      '\n' +
      '2. **시각 및 촉각 입력 둘 다로부터 촉각 설명을 생성할 수 있는 멀티모달 모델인, 세 모달리티들 사이의 쌍별 대조적 학습을 통해 TVL 데이터세트 상에서 트레이닝된 시각 및 언어 정렬 촉각 인코더**와 **터치-비전-언어 모델**;\n' +
      '\n' +
      '3. TVL 벤치마크에 대한 실험은 인간 주석과 VLM 의사 레이블의 혼합이 터치 비전 언어 이해에서 모델 성능을 향상시켜 기존 VLM을 최소 12% 능가함을 시사한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '#학습 멀티모달 인코더\n' +
      '\n' +
      '멀티 모달 인코더 사전 훈련은 제로 샷 크로스 모달 추론을 수행하기 위해 잠재 공간을 자연스럽게 구조화할 수 있기 때문에 멀티 태스크 학습을 위해 필요한 단계이다. CLIP(Radford et al., 2021; Ilharco et al., 2021)는 비전과 텍스트 사이의 공동 임베딩 공간을 학습하기 위해 대조적 사전 훈련을 수행하기 위해 인터넷 스케일 데이터를 가장 먼저 활용하는 것 중 하나이다. Guzhov et al. (2021) 및 Zhang et al. (2021); Guo et al. (2023)은 오디오 및 포인트 클라우드를 포함하도록 CLIP를 확장한다. ImageBind(Girdhar et al., 2023)는 이미지 쌍을 이루는 데이터만을 사용하여 6개의 모달리티에 대한 인코더를 대조적으로 훈련시킨다. 많은 작품들이 멀티모달 프리트레이닝(Bachmann et al., 2022; Li et al., 2023b; Geng et al., 2022)을 위한 대안 전략으로 마스킹을 탐색하기도 하였다. 이 연구에서 우리는 촉각적 양식을 CLIP 잠재 공간과 정렬하여 이미지 관찰 및 인간의 촉각성에 대한 자연어 설명과의 관계를 포착한다.\n' +
      '\n' +
      '### Tactile Perception\n' +
      '\n' +
      '인간의 지각에서 시각과 촉각의 동시 사용에 영감을 받은 촉각과 시각을 통합하는 것(Bresciani et al., 2006; Ittyerah and Marks, 2007; Jones et al., 2005; Camponogara and Volcic, 2021; Stone and Gonzalez, 2015)은 로봇 공학 및 체화된 AI 모두에서 활발한 연구 영역이다(Goldberg and Bajcsy, 1984; Pacchierotti et al., 2017). 이 분야의 작업은 저비용의 비전 기반 촉각 센서에 의해 촉진된다(Chorley et al., 2009; Yamaguchi and Atkeson, 2016; Yuan et al., 2017; Lambeta et al., 2020; Sferrazza and D\'Andrea, 2019; Shimonomura, 2019). 최근 몇 가지 연구는 vi의 조합을 활용하는 것을 발견했다.\n' +
      '\n' +
      '도 2: (1) 우리는 "in-the-wild" (2) 촉각 및 비전 관찰을 동기적으로 수집하기 위해 DIGIT 촉각 센서와 웹캠을 사용하여 3D 인쇄 데이터 수집 장치를 설계했다. (3) 데이터 수집을 위해 표면과 물체의 장치를 누르고 미끄러뜨린다.\n' +
      '\n' +
      '시온 및 터치는 힘 및 센서 포즈 추정을 돕고(Suresh et al., 2022), 교차-모달 이미지 생성 및 예측을 돕는다(Higiore et al., 2023; Zhong et al., 2022; Yang et al., 2022; Li et al., 2019), 덱스터러스 조작(Calandra et al., 2018; Fu et al., 2023; Zhang and Demiris, 2023; Chen et al., 2022; Qi et al., 2023; Kerr et al., 2023), 촉각, 비전 및 오디오 데이터를 포함하는 데이터 세트를 생산했다(Gao et al., 2021; 2022).\n' +
      '\n' +
      '많은 작품들이 표면 질감, 물체 소재, 의류를 분류하기 위한 촉각 감지의 사용에 대해 연구하고 있다. Li and Adelson(2013)은 비학습 기반 질감 분류 방법을 사용하여 촉각 관찰로부터 40개의 재료 특성을 분류한다(Ojala et al., 2002); 후속 작업은 의복 분류를 위한 학습 기반 방법을 사용한다(Kampouris et al., 2016; Yuan et al., 2018). "in-the-wild" 데이터를 수집함으로써, Yang et al.(2022)은 촉각 관찰 다양성을 확장하고 재료 분류기를 훈련시켰다. 이 모든 작업은 전체 데이터 세트의 폐쇄 어휘 _인간_ 주석을 사용하는 반면, 우리는 "야생에서" 수집된 데이터 세트에 레이블을 지정하고 개방형 어휘 작업에 대해 테스트하기 위해 비전 언어 모델을 사용한다. 이 작업과 동시에, Yang et al.(2024)은 비전 모달리티에 터치를 바인딩하고, 촉각, 비전 및 언어 모달리티에 걸쳐 개방 어휘 분류를 수행하고, 이미지 바인드-LLM을 미세 조정하지 않고 텍스트 생성을 위한 언어 모델과 촉각 입력을 정렬한다(Han et al., 2023).\n' +
      '\n' +
      '### LLMs의 멀티모달 정렬\n' +
      '\n' +
      '미리 훈련된 멀티모달 인코더는 언어 모델과 정렬될 때 언어 모델이 텍스트가 아닌 모달리티로 추론할 수 있게 한다. LLM(Large Language Models), Unified-IO 2(Lu et al., 2023), Generalist Agent(Reed et al., 2022), Robot Transformer 2(Brohan et al., 2023), 및 PaLM-E(Driess et al., 2023)의 능력에 기초하여, 인터넷 및 다수의 도메인으로부터의 시각적 데이터를 갖는 종단간 피네튠 언어 모델. 최근의 작업은 정렬을 더 빠르고 더 효율적인 파라미터로 만들기 위해 시도한다(Zhu et al., 2023; Moon et al., 2023; Dai et al., 2023; Lin et al., 2023; Chen et al., 2023; Cai et al., 2023; Bai et al., 2023; Hu et al., 2022). 오픈 소스 언어 모델들이 GPT 생성 데이터 상에서 트레이닝하는 방법과 유사하게(Taori et al., 2023), 많은 비전-언어 모델들(Liu et al., 2023; Ma et al., 2023; Gao et al., 2023; Chen et al., 2023)은 GPT-4에 의해 생성된 언어-이미지 명령-추종 데이터 상에서 모델을 미세화하고(OpenAI et al., 2023), 일반적인 시각적 추론 능력을 보여준다. ImageBind-LLM(Han et al., 2023) 및 PandaGPT(Su et al., 2023)는 ImageBind 인코더를 이용한 멀티모달 추론 능력을 소개한다. 보다 최근의 작업은 미리 훈련된 LLM, 인코더 및 디코더를 정렬하여 멀티모달 데이터를 이해하고 생성할 수 있는 모델을 미세 조정한다(Wu et al., 2023; Tang et al., 2023; Sun et al., 2023). Imagebind-LLM과 유사하게, 이 작업은 멀티모달 인코더를 미리 훈련된 LLaMA-2(Touvron et al., 2023)와 정렬한다.\n' +
      '\n' +
      '의사표지를 이용한###훈련\n' +
      '\n' +
      '지도 학습의 효과는 레이블이 지정된 데이터의 가용성에 의해 제한되는 경우가 많다. 라벨링된 데이터의 작은 세트에 대해 훈련된 교사 모델은 의사 라벨 형태의 저렴한 감독 소스를 제공할 수 있다. 그런 다음 학생 모델은 교사 모델에 의해 생성된 의사-라벨로부터 대량의 라벨링되지 않은 데이터에 대해 학습한다(Sohn et al., 2020; Lee et al., 2013; Wang et al., 2022; Rosenberg et al., 2005; McLachlan, 1975). 이전 작업은 레이블이 지정된 데이터 세트에서 훈련 교사 모델을 활용하는 반면, 비전과 언어 문헌 모두에서 최근 작업은 대규모 사전 훈련 모델을 활용한다. CutLER(Wang et al., 2023)은 경계 박스들을 생성하기 위해 DINO(Caron et al., 2021) 특징들을 사용하여, 객체 검출 및 분할 모델들의 비감독 트레이닝을 가능하게 한다. InstructPix2Pix 및 InstructNeRF2NeRF(Brooks et al., 2023; Haque et al., 2023)는 GPT(Brown et al., 2020) 및 Stable Diffusion(Rombach et al., 2022)을 사용하여 이미지 편집 예제의 데이터세트를 생성하고 후속적으로 이러한 예제에 기초하여 확산 모델을 트레이닝한다. 최근의 LLMs 및 VLMs(Wang et al., 2022; Taori et al., 2023; Liu et al., 2023;b)는 GPT 모델들에 의해 생성된 의사-라벨들을 사용하여 트레이닝된다(Brown et al., 2020; OpenAI et al., 2023). 그러나 이러한 작업에서 교사와 학생 모델은 동일한 입력 및 출력 양식을 공유한다. Burnel 등에 의해 제안된 프레임워크(2023)와 유사하게, 우리는 비전 데이터로부터 텍스트 라벨들을 생성하기 위해 비전 전용 멀티모달 모델을 사용하며, 이는 차례로 언어 정렬 촉각 인코더 및 TVL 모델을 트레이닝하기 위해 촉각 데이터와 매칭된다. 우리가 사용하는 교사(GPT-4V)는 학생 과제만으로 훈련된 전문가 모델보다 더 일반적이다.\n' +
      '\n' +
      '## 3 TVL 데이터세트\n' +
      '\n' +
      'TVL 데이터세트(도 3의 예)는 자연어로 촉각 감각으로 라벨링된 쌍을 이루는 촉각 및 시각 관찰을 포함한다. 여기서는 데이터 수집, 청소 및 라벨링에 사용되는 하드웨어와 절차에 대해 설명한다.\n' +
      '\n' +
      '### Data Collection\n' +
      '\n' +
      'TVL은 Logitech BRIO 웹캠의 비전 데이터와 내부 변형 가능한 표면의 RGB 이미지 형태로 고해상도 촉각 관찰을 제공하는 저비용, 컴팩트, 오픈 소스 촉각 센서인 DIGIT의 촉각 데이터를 사용한다(Lambeta et al., 2020). 상기 원시 비전-촉각 데이터세트는 두 개의 별개의 서브세트를 병합한다: 1) 상기 **S**自-**V**isuo-**T**actile **P**retraining (SSVTP)(Kerr et al., 2023) 데이터세트 및 2) 상기 **H**uman **C**llected **T**actile (HCT) 데이터세트. 상기 SSVTP 데이터세트(4,587개의 이미지-터치 쌍)는 UR5 로봇에 의해 수집되며, 이는 먼저 객체들의 세트가 미리 배열된 작업 표면 위에서 하향식 이미지들을 캡처하고, 이어서 작업 공간의 해당 위치로 DIGIT 센서를 누르는 것과 같은 두 가지 제한들에 직면한다. 특히 데이터가 수집되는 동안 대상체가 로봇에 의해 부주의하게 이동되는 경우 정렬이 잘못됩니다. 이러한 문제를 해결하기 위해 HCT는 캡처된 감각 정보에서 정렬을 보장하기 위해 촉각 및 시각 데이터의 동기 획득을 강조한다.\n' +
      '\n' +
      'HCT는 그림 2에 표시된 휴대용 3D 프린팅 데이터 수집 장치를 사용하여 총 20시간 동안 5명의 인간이 수집한 현장 데이터 시각 촉각 데이터 예로 구성된다. 장치는 30Hz에서 시각 및 촉각 관찰을 모두 기록한다. 데이터 프레임은 터치의 "궤적"에 수집되는데, 각각의 궤적은 촉각 센서와 물체의 접근, 접촉, 슬라이딩 및 인출로 구성된다. 우리는 터치 비전 쌍을 표면과 접촉 중 또는 비접촉으로 분류한다. 시각 데이터는 촉각 센서 및 접촉 지점이 항상 카메라의 시야 내에 있도록 비스듬한 각도로 수집되어 비전-터치 동기성을 보존한다. 이 데이터 세트 내의 다양성을 개선하기 위해 인간 수집가들은 질감 및 가장자리와 같은 흥미롭고 새로운 실제 촉각 사례를 검색하도록 지시받았다. HCT의 작은 고정 테스트 세트(쌍의 1%)는 손으로 주석을 달고 나머지는 섹션 3.3에 설명된 대로 GPT-4V에 의해 의사 표지된다.\n' +
      '\n' +
      '청소 후보 촉각 이미지\n' +
      '\n' +
      '수집된 데이터를 SSVTP(Kerr et al., 2023)의 사전 훈련된 촉각 인코더를 사용하여 비접촉 및 비접촉 프레임으로 분류한다. 모든 터치 궤적에 대해, 초기 프레임과 최종 프레임이 비접촉이라는 가정 하에, 우리는 기준 배경 이미지를 생성하기 위해 이러한 프레임들의 평균을 계산한다. 이어서, 이 이미지는 잠재 표현을 획득하기 위해 미리 훈련된 촉각 인코더에 의해 임베딩된다. 터치 궤적의 프레임이 접촉하는지 여부를 결정하기 위해, 촉각 잠재 임베딩과 추정된 배경 프레임의 코사인 유사도를 계산한다. 우리는 코사인 유사도가 0.6 이하로 떨어질 때 촉각 프레임이 접촉하는 것으로 간주한다(Kerr et al., 2023). 수집된 데이터는 43,741쌍의 비접촉 프레임과 169,292쌍의 비접촉 프레임을 포함한다.\n' +
      '\n' +
      '### Language Labeling\n' +
      '\n' +
      '**인간 레이블링** SSVTP 데이터 세트는 강력한 시각-촉각 정렬을 보여주기 때문에 터치와 언어를 정렬하기 위한 기반으로 사용하므로 각 데이터 포인트에 의해 캡처된 촉각에 대한 자연어 설명으로 데이터 세트에 수동으로 주석을 달 수 있다. 우리는 인간 주석자에게 400개 단어(ajbarentt, 2023)의 촉각 어휘 목록을 제공하여 쌍에 대한 물질적 특성과 촉각적 느낌에 대한 언어 설명을 생성한다.\n' +
      '\n' +
      '도 3: **TVL Dataset**는 두 데이터세트, 즉 SSVTP(Kerr et al., 2023)(4,587 이미지-터치 쌍) 및 HCT(39,154 이미지-터치 쌍)를 조합하여 시작하며, 시각적 관찰과 촉각적 입력이 동기적으로 캡처되도록 우리가 수집한 새로운 데이터세트이다. SSVTP 데이터 세트의 경우 데이터(첫 번째 행에 표시된 예제)에 수동으로 레이블을 지정합니다. 새로 수집된 데이터 세트에 대해, 우리는 GPT-4V(부록 C.4 참조)에 데이터 세트에 레이블을 지정하도록 촉구한다(행 2-4에 표시된 예제). GPT-4V는 접촉 패치가 센서에 의해 폐색될 때, 또는 촉감을 추정하기에 충분한 정보가 없을 때 정확한 촉각 라벨(행 4)을 제공하는 데 실패할 것이라는 점에 유의한다. 전체적으로, 이것은 오픈 어휘 언어 라벨이 있는 43,741개의 이미지 터치 쌍을 포함하는 데이터 세트를 생성한다.\n' +
      '\n' +
      'SSVTP 데이터세트. 이러한 주석자는 각 시각-촉각 쌍에 표시된 촉각 패턴을 가장 정확하게 설명하는 적용 가능한 형용사를 최대 5개 선택하도록 지시된다.\n' +
      '\n' +
      'GPT-4V를 사용한 의사 레이블 생성은 촉각 느낌을 설명하는 언어 레이블을 생성하기 위해 GPT-4V를 사용하여 HCT 데이터 세트에서 접촉하는 부분에 의사 레이블링을 수행한다. 우리는 전체 이미지와 접촉 지점 주위에 크롭된 로컬화된 버전을 모두 제공하는 것이 GPT-4V가 인간의 것과 정렬된 텍스트 라벨을 생성하도록 장려한다는 것을 경험적으로 발견했는데, 이는 전체 이미지가 수많은 산만자와 비접촉 물체를 포함할 수 있기 때문이다(도 3의 성공 및 실패 사례 참조). 의사 라벨 생성을 위해 GPT-4V에 제공된 특정 프롬프트는 부록 C.4에 보고되어 있다.\n' +
      '\n' +
      '때때로, GPT-4V는 모션 블러링 또는 낮은 조명 이미지에 대한 촉각 라벨의 생성에 실패하거나 거부한다. 이러한 경우, 우리는 먼저 동일한 궤적의 다른 이미지에 대한 레이블을 생성하려고 시도하고, 동일한 궤적 내의 다른 접촉 이미지에 적용된 단어 세트에서 무작위로 샘플링하여 누락된 레이블을 채운다. 궤적 내의 _no_ 이미지가 성공적으로 라벨링될 수 있다면, 그 궤적은 데이터세트의 트레이닝 부분으로부터 제외된다. 이 과정이 끝나면 39,154개의 유사 라벨 이미지가 남게 된다.\n' +
      '\n' +
      '### Dataset Statistics\n' +
      '\n' +
      'SSVTP 구성요소는 4,587개의 독립적인 이미지 터치 쌍을 포함한다. HCT 컴포넌트는 새롭게 수집된 39,154개의 대응하는 접촉식 이미지-촉각 프레임 쌍들 및 169,292개의 접촉식 외부 데이터 쌍들로 구성된다. 전자의 데이터세트는 각각의 데이터 포인트에 대한 고유한 터치 궤적을 포함하는 반면, 후자는 1,486개의 고유한 연속 궤적으로서 수집되며, 이들 각각은 관심 객체와의 하나 이상의 접촉 이벤트로 구성된다. 데이터세트의 인간 및 GPT-4V 라벨이 붙은 부분 모두에 걸쳐 주석자는 254개의 고유한 촉각 형용사를 사용한다. 우리는 두 데이터 세트 모두에 대해 99%-1% 트레인 테스트 분할을 수행하며, 인간 주석자는 두 데이터 세트에 대해 테스트 세트(402 이미지 터치 쌍)에 수동으로 레이블을 지정한다. 평균적으로 GPT-4V는 HCT 상의 촉감을 묘사하기 위해 4.25 형용사를 사용하는 반면, 인간 주석기는 평균 2.70 형용사를 사용한다. 설명에 대한 보다 자세한 설명은 부록 C.3에 나와 있다.\n' +
      '\n' +
      '##4 촉각-시각-언어 모델\n' +
      '\n' +
      '먼저 ImageBind와 ImageBind-LLM의 공식화를 재검토한다. 그런 다음 촉각 인코더 훈련을 위한 쌍별 대조 접근법을 설명하고 마지막으로 정렬된 TVL 모델의 훈련 레시피에 대해 논의한다.\n' +
      '\n' +
      '### Preliminary\n' +
      '\n' +
      'ImageBind(Girdhar et al., 2023)는 이미지, 텍스트, 오디오, 깊이, 열 및 IMU 데이터의 6가지 다른 모달리티에 걸쳐 조인트 임베딩을 학습하는 멀티모달 모델이다. 비전과 다른 양식 중 하나로 구성된 데이터 쌍을 활용하여 모두 비전에 "바인드"된다. 비전 및 언어 인코더는 OpenCLIP(Ilharco et al., 2021)로부터 초기화되고 동결된 상태로 유지되는 반면, 다른 모달리티에 대한 인코더는 랜덤하게 초기화된다. 각 인코더는 끝에서 작고 훈련 가능한 어댑터 네트워크를 사용하여 동일한 차원의 잠재 공간에 입력을 투영합니다. 인코더는 InfoNCE 손실을 사용하여 정규화된 잠재 임베딩에 대한 대비 학습을 통해 공동으로 훈련된다.\n' +
      '\n' +
      'LLaMA-Adapter(Zhang et al., 2023)와 ImageBind-LLM(Han et al., 2023)은 VLM에 대한 효율적인 명령어 미세 조정 접근법을 제공하며, 새로운 모달리티를 인코딩하기 위해 사전 훈련된 멀티모달 모델을 활용한다. 이러한 방법의 효율성은 (1) 단일 토큰에서 멀티모달 관측치를 평균화하고 (2) 멀티모달 토큰을 언어 모델과 적응적으로 융합하는 제로 초기화 게이트에서 비롯된다. LLaMA-Adapter는 먼저 zero-initialized gate와 프로젝터를 인코더에서 언어 모델로 프리트레인한 후 LoRA로 언어 모델을 미세 조정한다(Hu et al., 2022).\n' +
      '\n' +
      '도 4: **Method.**(Left) TVL은 ImageBind(Girdhar et al., 2023)와 상이하며, ImageBind는 비전 모달리티와 모든 다른 모달리티 사이의 손실만을 고려한다. TVL은 새로운 양식(촉각)과 언어 사이의 손실을 포함하여 모든 양식 쌍 간의 손실을 계산합니다. 경험적으로, 우리는 그러한 손실을 포함하는 것이 촉각적 의미론을 포착하는 모델의 능력을 향상시킬 수 있음을 보여준다. (Right) Following Han et al. (2023), We average the latent from tactile and vision modality and finetune the language model.\n' +
      '\n' +
      '### Tactile Encoder\n' +
      '\n' +
      '모든 모달리티를 시각에 독립적으로 바인딩하는 ImageBind와 달리 각 모달리티 쌍을 바인딩하여 촉각 모달리티에 대한 강력한 감독을 제공한다. 각 데이터 배치에 대해 비전 언어, 촉각 언어 및 촉각 비전 쌍 간의 대비 손실을 계산한다. Tactile encoder를 Vision Transformer (ViT) (Dosovitskiy et al., 2020)로 랜덤하게 초기화하고, ViT-Tiny (5.7M paraeters), ViT-Small (22M), ViT-Base (86M)의 세 가지 모델 크기에 대해 테스트한다. 이미지바운드 학습 레시피를 직접 채택하면 44K 쌍의 비접촉 데이터로 구성된 비교적 작은 학습 데이터 세트가 과도하게 적합된다는 것을 알 수 있다. 이전 작업들(Kerr et al., 2023; Yang et al., 2022; Dave et al., 2024)과 달리, 우리는 촉각 센서가 표면과 접촉하지 않는 데이터를 레버리지하는 것이 이러한 과적합 문제를 완화하고 시각적 데이터 다양성을 개선함으로써 촉각 표현 학습을 향상시킬 수 있다는 것을 발견한다(부록의 도 6 참조). 따라서 훈련 데이터의 분율\\(\\gamma=10\\%\\)에 대해 센서가 접촉하지 않는 것을 보장하고, 이러한 예에 "배경"의 텍스트 레이블을 할당한다. 또한, 비전 및 언어 인코더에서 프로젝터를 제거하여 촉각 인코더가 원본 CLIP의 공통 잠재 공간으로 직접 투영되도록 한다. 마지막으로 언어 레이블의 다양성을 높이기 위해 각 이미지에 대한 촉각 기술에서 단어의 하위 집합을 무작위로 섞고 선택한다. 이러한 방법은 과적합을 완화하는데 도움이 된다(부록 B.1 참조).\n' +
      '\n' +
      '### 언어 모델을 이용한 정렬\n' +
      '\n' +
      '본 논문에서는 ImageBindLLM(Han et al., 2023)에서 제안한 2단계 훈련에 따라 ImageBind 인코더를 TVL 인코더와 교환한다. 우리는 LLaVA Visual Instruct CC3M (Liu et al., 2023b) 595K 서브세트와 TVL 데이터세트 모두에 대해 사전 훈련한다. CC3M 서브세트의 경우, 우리는 촉각적 양식에 빈 촉각적 이미지를 제공한다. 미세 조정 동안, 우리는 TVL, Alpaca(Taori et al., 2023) 및 LLaVA Visual Instruct 150K(Liu et al., 2023b)의 조합을 사용한다. 경험적으로, 우리는 우리의 데이터세트 훈련만으로는 LLaMA2(Touvron et al., 2023)의 안전 미세 조정을 극복하기에 충분하지 않다는 것을 발견했으며, 그 결과 모델이 촉각 감각에 관한 질문에 답하는 것을 거부했다. 지침 미세 조정을 위한 TVL 프롬프트에 대한 자세한 내용은 부록 C.2에 나와 있습니다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      'TVL 모델의 멀티모달 능력을 크로스모달 분류 태스크와 촉각-의미 기술 태스크의 두 가지 실험 설정에서 정량적으로 평가한다.\n' +
      '\n' +
      '### 평가 및 측정\n' +
      '\n' +
      '**Open Vocabulary Tactile Classification** 인간 라벨 TVL 테스트 세트를 402-way 분류 문제로 캐스팅하고 촉각-비전과 촉각-언어 분류 모두에 대해 Top-1과 Top-5 정확도를 측정하여 촉각 인코더의 성능을 평가한다. 많은 술집들이\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c c} \\hline \\hline  & \\multicolumn{2}{c|}{**Tactile-Text**} & \\multicolumn{2}{c|}{**Tactile-Vision**} & \\multicolumn{2}{c}{**Vision-Text**} \\\\  & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\\\ \\hline CLIP & - & - & - & 28.4\\% & 64.9\\% \\\\ SSVTP & - & - & 0.2\\% & 0.3\\% & \\\\ TVL & 36.7\\% & 70.3\\% & 79.5\\% & 95.7\\% & 28.4\\% & 64.9\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다른 촬영장비 쌍에 걸쳐 **Top-1 및 Top-5 정확도**. 우리는 훈련된 TVL 인코더(ViT-Tiny)가 OpenCLIP의 비전 언어 정렬보다 더 나은 촉각 언어 정렬을 보여주며, 이는 바닐라 CLIP가 촉각 의미론을 잘 포착하지 못할 수 있음을 시사한다. SSVTP는 TVL 데이터세트의 하위 집합에 대해 훈련되기 때문에 전체 TVL 데이터세트에 걸쳐 잘 일반화되지 않아 촉각-비전 데이터세트를 확장할 필요가 있다.\n' +
      '\n' +
      '그림 5: _Left:_ 402 촉각, 이미지 및 언어 트리플렛을 포함하는 전체 테스트 세트에서 촉각과 언어 사이의 코사인 유사성을 측정한다. 그러나, 상이한 촉각 관찰은 동의어 설명을 가질 수 있기 때문에, 5.1에서 이를 고려하기 위해 top-1 및 top-5 정확도 계산을 업데이트한다. _ Right:_ GPT-4V 및 TVL-LLaMA 세대는 인간 라벨에 기초하여 GPT-4에 의해 평가된 점수를 갖는다. GPT-4V는 촉각을 고려하지 않아 접촉하지 않는 물체에 의해 산만해질 수 있으며, 관찰이 분산되지 않기 때문에 촉각을 촉구할 때 촉각 관찰을 포함할 때 개선이 없음을 경험적으로 발견했다. TVL-LLaMA는 GPT-4V 의사 레이블로 훈련됨에 따라 동일한 고장 모드를 겪는다.\n' +
      '\n' +
      '타일 관찰은 다수의 의미적으로 유사한 방식(_e.g._ rigid is synonymous with stiff)으로 기술될 수 있고 CLIP 언어 임베딩은 순열 불변성이 아니다(_e.g._ "soft, smooth" and "smooth, soft" is different embeddings), 우리는 촉각 언어 분류를 위한 지상 진실 라벨을 계산하기 위한 대안적인 방법을 제안한다.\n' +
      '\n' +
      '우리는 먼저 GPT-4가 SSVTP 데이터세트의 인간 주석자가 사용하는 기술자 세트에서 각 단어에 대한 5(촉각 의사 라벨의 평균 길이) 동의어 세트를 생성하도록 프롬프트하여 촉각을 설명하는 799개의 별개의 형용사를 생성한다. 이러한 형용사에 대한 CLIP 언어 임베딩을 구하고, 생성된 각 동의어와 각 원본 기술자의 코사인 유사도를 계산한다. 우리는 이러한 코사인 유사성의 최소 \\(\\phi\\)를 의미적으로 유사한 어휘의 임계값으로 간주한다. 각 촉각 이미지에 대해, 이미지의 원래 언어 라벨과 코사인 유사도가 \\(\\phi\\)을 초과하는 테스트 세트에서 올바른 언어 라벨 세트를 모든 라벨로 정의한다. 이러한 레이블을 사용하여 상위 1 및 상위 5 정확도를 계산한다. 경험적으로 우리는 \\(\\phi=0.636\\)을 발견한다. 또한 표 6의 코사인 유사성의 25번째, 50번째 및 75번째 백분위수를 임계값으로 사용하여 상위 1 및 상위 5 정확도를 보고한다.\n' +
      '\n' +
      '**TVL Benchmark** TVL 테스트 세트에서 촉각적 설명을 생성하기 위한 LLM의 기능을 평가한다. 시각적 입력 이미지, 촉각 센서를 중심으로 한 크롭된 시각적 이미지 및 대응하는 촉각 이미지가 주어지면, 우리는 모델에 5개 이하의 형용사의 세트로 문제의 물체의 촉각 감각을 기술하도록 요청한다.\n' +
      '\n' +
      '수치 비교를 얻기 위해, 텍스트 전용 GPT-4가 1에서 10의 척도로 인간 주석이 달린 지상 진리 의미 라벨에 대한 모델의 응답의 유사성을 점수화하도록 프롬프트하고(여기서 더 높은 점수는 더 나은 지시-추종 및 더 가까운 서술적 매칭을 나타냄), 그리고 선행 작업들과 유사하게 주어진 점수를 설명한다(Liu et al., 2023b; Chiang et al., 2023). 모델 출력의 샘플이 도 5에 제공되고, 생성 및 평가에 사용되는 프롬프트가 부록 C.4에 보고된다. 우리는 기존의 오픈-소스 VLM(Liu et al., 2023a; Cai et al., 2023b; Li et al., 2023a; Dai et al., 2023) 및 GPT-4V와 비교한다. 추가적인 베이스라인으로서, 우리는 언어 모델을 미세 조정하기 위해 SSVTP(Kerr et al., 2023) 촉각 및 이미지 인코더를 사용한다; 우리는 결과 모델 SSVTP-LLaMA를 호출한다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '#### Classification\n' +
      '\n' +
      '영상과 언어 관찰을 인코딩하기 위해 OpenCLIP를 사용하기 때문에 TVL 인코더는 시각-언어 정확도 점수를 OpenCLIP와 공유한다. 우리는 Kerr et al. (2023)에 대한 우리의 인코더의 촉각-비전 정확도를 비교한다; 그들이 실험실 설정에서 수집된 작은 데이터세트에서 트레이닝하기 때문에, 그들의 모델은 SSVTP 데이터세트에서 잘 수행되지만, 새로운 "in-the-wild" 데이터세트에는 잘 일반화되지 않는다. 택타일 인코더는 택타일에 대한 언어 설명에 정렬되기 때문에 OpenCLIP의 비전-텍스트 정렬보다 더 나은 택타일-텍스트 정렬을 보여준다.\n' +
      '\n' +
      '**TVL Benchmark** 표 2의 촉각-의미 생성 결과에 대한 요약 통계를 제시한다. 제안된 벤치마크에서 오픈 소스 VLM은 제한된 다양성과 훈련된 시각적 데이터에서 인간의 촉각성에 대한 초점 부족으로 인해 GPT-4V보다 더 나쁜 성능을 나타낸다. 한편, 모든 버전의 TVL은\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c c} \\hline \\hline  & \\multicolumn{2}{c|}{**Encoder Pre-training Modalities**} & \\multicolumn{3}{c}{**Score** (1-10)} & \\multicolumn{2}{c}{\\(p\\)-value} \\\\ \\cline{2-7}  & Vision & Tactile & Language & SSVTP & HCT & TVL & (d.f. = 401) \\\\ \\hline LLaVA-1.5 7B & ✓ & - & ✓ & 3.64 & 3.55 & 3.56 & \\(1.21\\times 10^{-9}\\) \\\\ LLaVA-1.5 13B & ✓ & - & ✓ & 3.55 & 3.63 & 3.62 & \\(1.49\\times 10^{-9}\\) \\\\ ViP-LLaVA 7B & ✓ & - & ✓ & 2.72 & 3.44 & 3.36 & \\(8.77\\times 10^{-16}\\) \\\\ ViP-LLaVA 13B & ✓ & - & ✓ & 4.10 & 3.76 & 3.80 & \\(1.72\\times 10^{-6}\\) \\\\ LLaMA-Adapter & ✓ & - & ✓ & 2.56 & 3.08 & 3.02 & \\(2.68\\times 10^{-17}\\) \\\\ BLIP-2 Opt-6.7b & ✓ & - & ✓ & 2.02 & 2.72 & 2.64 & \\(1.92\\times 10^{-31}\\) \\\\ InstructBLIP 7B & ✓ & - & ✓ & 1.40 & 1.30 & 1.31 & \\(1.07\\times 10^{-84}\\) \\\\ InstructBLIP 13B & ✓ & - & ✓ & 1.44 & 1.21 & 1.24 & \\(4.64\\times 10^{-88}\\) \\\\ GPT-4V & ✓ & - & ✓ & 5.02 & 4.42 & 4.49 & - \\\\ \\hline SSVTP-LLaMA & ✓ & ✓ & - & 2.58 & 3.67 & 3.54 & \\(1.79\\times 10^{-9}\\) \\\\ \\hline TVL-LLaMA (ViT-Tiny) & ✓ & ✓ & ✓ & 6.09 & 4.79 & 4.94 & \\(4.24\\times 10^{-5}\\) \\\\ TVL-LLaMA (ViT-Small) & ✓ & ✓ & ✓ & 5.81 & 4.77 & 4.89 & \\(6.02\\times 10^{-4}\\) \\\\ TVL-LLaMA (ViT-Base) & ✓ & ✓ & ✓ & **6.16** & **4.89** & **5.03** & \\(3.46\\times 10^{-6}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **TVL 벤치마크 성능.** SSVTP 촉각 비전 인코더를 사용하여 미세 조정된 모델인 기존 VLM 및 SSVTP-LLaMA에 대해 TVL-LLaMA를 벤치마킹하여 촉각 이미지 관찰에서 촉각 설명을 생성하고 GPT-4를 사용하여 TVL 테스트 세트의 각 구성 부분에 대한 성능을 수치적으로 점수화했다. 우리는 촉각-의미 과제에 대한 GPT-4V의 점수에 대한 각 모델의 점수에 대한 양측 쌍 표본 \\(t\\)-검정에서 \\(p\\)-값을 보고한다.\n' +
      '\n' +
      'LLaMA는 GPT-4V를 능가하여 훈련된 모델이 데이터 세트의 일부로 제공된 인간 라벨의 작은 부분을 넘어 일반화할 수 있음을 시사한다. 이 두 결과는 \\(\\alpha=0.05\\) 수준에서 통계적으로 유의하다. 결과는 또한 사전 훈련 동안 촉각 및 시각 양식만을 사용하는 SSVTP-LLaMA의 낮은 점수에서 알 수 있듯이 촉각 언어 정렬이 필요함을 시사한다.\n' +
      '\n' +
      '전반적으로, 우리의 실험은 1) TVL 데이터세트에서 훈련된 TVL 촉각 인코더가 시각 촉각 사전 훈련 인코더 및 일반 시각 언어 인코더(OpenCLIP)에 비해 언어 잠재 공간과 정렬되고 분류 작업에서 더 높은 점수(+29%)를 얻었고, 2) 시각 및 촉각 관찰에서 촉각 언어 설명을 생성하도록 훈련된 TVL-LLaMA 모델이 기존 VLM에 비해 새로운 TVL 벤치마크(최소 +12%)에서 인간 설명과 더 밀접하게 일치함을 시사한다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '이 섹션에서는 모델 크기와 제안된 데이터 세트가 인코더의 다중 모드 분류 성능에 미치는 영향을 조사하는 표 3에 표시된 6개의 절제 및 민감도 분석을 제시한다. 더 많은 절제술이 부록에 포함되어 있다.\n' +
      '\n' +
      '**모델 크기**(표 2(a)) 성능은 인코더 크기에 따라 크게 다르다. ViT-Base는 가장 높은 검증 정확도를 갖지만 분포 이동으로 인해 테스트 세트에서 지연되며, GPT-4V의 훈련 라벨은 인간 주석이 달린 테스트 데이터에 비해 덜 상세하고 정확하다. 그러나, 동기화된 데이터에 대한 촉각적 비전 분류에서 ViT-Base는 두 가지 작은 모델 모두보다 우수하다.\n' +
      '\n' +
      '**Disable Tactile-Text Loss**(표 2(b))는 ImageBind(Girdhar et al., 2023)의 설정과 유사하며, 여기서 세 가지 양식의 데이터가 모두 고려되지만 촉각-Text 손실은 생략된다. 결과는 촉각 인코더를 감독하기 위해 언어를 사용하는 것이 두 가지 양식을 더 잘 정렬함을 시사한다.\n' +
      '\n' +
      '**Data**(표 2(c)-f) 훈련용 데이터셋의 서로 다른 구성에 대해 4가지 민감도 분석을 수행한다. 우리는 세 가지 양식 모두에서 데이터를 활용하는 것이 촉각 언어 정렬을 향상시킨다는 것을 발견했다. 비접촉 데이터를 추가하면 모델이 훈련 세트에 과도하게 맞추는 것을 방지하지만 테스트 세트 성능은 접촉 데이터만 갖는 것과 비슷합니다. 또한 바닐라 CLIP 훈련(Radford et al., 2021)에서 사용되는 프롬프트를 실험하여 정확도의 한계 개선을 가져왔다. 마지막으로 SSVTP와 HCT에 대한 모델을 별도로 학습하고, 의사 레이블 데이터 세트는 전체 데이터 세트에 대한 학습과 유사한 성능을 제공할 수 있음을 발견했으며, 이는 TVL의 촉각 인코더가 자기 지도 학습을 효과적으로 활용하여 태스크 성능을 유지하면서 대규모의 완전 레이블 데이터 세트에 대한 의존성을 줄일 수 있음을 시사한다.\n' +
      '\n' +
      '##6 토론 및 결론\n' +
      '\n' +
      '제시된 연구는 몇 가지 한계점을 가지고 있다. 이 연구는 촉각 데이터를 라벨링하기 위한 VLM의 사용을 강조하지만, 시각 지각과 비교하여 터치의 뚜렷한 특성은 시각에서만 파생된 촉각 라벨의 정확성에 한계를 시사한다. 데이터 수집 하드웨어로 인해, 카메라는 촉각 센서가 접촉하는 표면 또는 물체의 미폐색 뷰를 갖지 않을 수 있고, 이는 시각과의 터치 정렬의 어려움을 증가시키고 이미지로부터 생성된 의사-라벨의 품질을 감소시킬 수 있다. 향후 연구에서는 멀티모달 정렬을 개선하기 위해 터치 비전 언어 데이터 세트의 규모를 더욱 증가시킬 수 있기를 바란다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: TVL 촉각 인코더에 대한 Ablations and Sensitivity Analysis. ViT-Small을 사용하여 Top-1 및 Top-5 촉각 텍스트 및 촉각 비전 분류 정확도를 보고한다. 베이스라인은 TVL 촉각 인코더를 트레이닝하기 위한 디폴트 설정을 나타내며, 이는 달리 언급되지 않는 한 _validation set_ 상에서 가장 성능이 좋은 모델이다. **Bold**는 _test set_에서 가장 높은 정확도를 나타낸다. 이러한 성능의 불일치는 섹션 5.3에 설명되어 있다.\n' +
      '\n' +
      '요약하면, 본 연구는 촉각과 언어 양식을 정렬하기 위해 촉각, 시각 및 촉각-의미 설명을 특징으로 하는 데이터 세트인 TVL을 소개한다. 데이터 세트를 사용하여 시각과 자연 언어 모두에 정렬된 촉각 인코더를 훈련한다. 우리는 훈련된 촉각 인코더를 사용하여 TVL-LLaMA가 기존의 VLM에 의해 생성된 것보다 인간 설명과 더 밀접하게 정렬되는 자연 언어로 촉각 설명을 생성할 수 있음을 입증한다.\n' +
      '\n' +
      '## 7 영향 진술\n' +
      '\n' +
      '이 문서에 있는 데이터는 익명화됩니다. 이 연구는 터치를 감지 양식으로 고려하는 미래의 대규모 생성 모델에 도움이 될 수 있으며 의사 레이블 기반 학습 방법을 연구하는 연구자에게 유용할 수 있다. 동시에 소개된 모델은 터치의 더 나은 디지털화와 로봇 공학에서 터치의 사용을 달성하는 데 기여할 것이다. 본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리 작업에는 많은 잠재적인 사회적 이점이 있으며, 여기에서 특별히 강조되어야 한다고 느끼는 것은 없습니다.\n' +
      '\n' +
      '## 8 Acknowledgments\n' +
      '\n' +
      '이 연구는 메타와 함께 BAIR 개방형 연구 공통 프로젝트로 지원되었다. 이 연구는 버클리 AI 연구(BAIR) 연구소와 CITRIS "사람과 로봇"(CPAR) 이니셔티브와 함께 UC 버클리의 AUTOLAB에서 수행되었다. UC 버클리, 레티안 푸, 가우라브 다타, 황 황, 윌리엄 청호 파니치, 자이민 드레이크, 켄 골드버그에서의 학문적 역할은 부분적으로 메타, 구글, 오토데스크, 지멘스, 도요타 연구소, 보쉬의 기부와 포토네오, 엔비디아, 직관 수술의 장비 보조금으로 지원된다. 로베르토 칼란드라는 독일의 우수전략의 일환으로 독일연구재단(DFG, Dutsche Forschungsgemeinschaft)의 자금 지원을 받고 있으며 - EXC 2050/1 - 프로젝트 ID 390696704 - 테크니셰 유니버시타트 드레스덴의 "인간과 루프를 가진 촉각 인터넷을 위한 센터"(CeTI)와 프로젝트 57616814(SECAI, 임베디드 및 복합 AI 학교)의 분데스인티윰 퍼 빌둥 운드 포르슈충(BMBF)과 독일 학술교류서비스(DAAD)가 자금을 지원하고 있다. 유익한 토론과 피드백에 저스틴 커, 정민 김, 라이언 호크, 슈동 왕에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]J. 바흐만, D. 미즈라히, A. 아타노프, A. 자미르(2022) 멀티매: 멀티모달 멀티태스크 마스킹 오토인코더. ArXiv:2204.01678. 인용: SS1.\n' +
      '*[2]J. 배승 배승 양승 왕승 Tan, P. Wang, J. Lin, C. Zhou 및 J. Zhou(2023)Qwen-vl: 이해, 현지화, 텍스트 읽기 및 그 이상을 위한 다목적 비전 언어 모델. 인용: SS1.\n' +
      '*[3]P. Bertelson and B. De Gelder (2004)의 심리학은 복합적 인식의 심리학이다. Crossmodal space and crossmodal attention, pp. 141-177. Cited by: SS1.\n' +
      '*[4]J. Bresciani, F. Dammeier, and M. O. Ernst (2006)Vision and Touch는 이벤트 시퀀스의 인식을 위해 자동으로 통합된다. Journal of vision6(5), pp.2-2. Cited by: SS1.\n' +
      '*[5]A. 브로한남 브라운, J. 카르바할, Y. 체보타르, X 천경 초롬스키 Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. 고팔라크리쉬난 한경호 Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. 조시 줄리안, D. 칼라쉬니코프, Y. 광일렬 이태은 류영 미칼레우스키, I. 모드매치, K. 퍼치경 라오경 레이만 류지 살라자르, P. 산케티, P. 서머넷, J. 싱, A. 싱, R. Soricut, H. Tran, V. 바누케, 큐 부엉아와히드 Welker, P. Wolhart, J. Wu, F. Xia, T. 샤오평수 Yu, and B. Zitkovich (2023)Rt-2: Vision-language-action model transfer web knowledge to robotic control. ArXiv 프리프린트에서 arXiv:2307.15818, 인용: SS1.\n' +
      '*[6]T. Brooks, A. Holynski, and A. A. Efros (2023)Instructpix2pix: 이미지 편집 지시를 따르는 것을 배우는 것. 인용: SS1.\n' +
      '*[7]T. B. Brown, B. Mann, N. 라이더 Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. 헤니건 어린이, A. 라메시, D. M. 지글러, J. 우, C. 윈터, C. 헤세, M. 첸 E. 시글러 M. 리트윈 그레이, B. 체스, J. 클라크, C. 버너, S. McCandlish, A. Radford, I. Sutskever, 및 D. Amodei(2020) 언어 모델은 소수의 학습자이다. 인용: SS1.\n' +
      '*[8]J. N. Bruck, S. F. Walmsley, and V. M. Janik(2022)Cross-modal perception of identity by sound and taste by bottlenos dolphins. Science Advances8(20), pp. eabm7684. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* Cai et al. (2023b) Cai, M., Liu, H., Mustikovela, S. K., Meyer, G. P., Chai, Y., Park, D., and Lee, Y. J. Making large multimodal models understand arbitrary visual prompts. _arXiv:2312.00784_, 2023b.\n' +
      '* Calandra et al. (2018) Calandra, R., Owens, A., Jayaraman, D., Lin, J., Yuan, W., Malik, J., Adelson, E. H., and Levine, S. 느낌 이상: 시각과 촉각을 이용하여 움켜쥐고 재기하는 법을 배우는 것. _ IEEE Robotics and Automation Letters_, 3(4):3300-3307, 2018.\n' +
      '* Camponogara & Volcic (2021) Camponogara, I. and Volcic, R. 인간의 다감각적 파악에서 햅틱과 시각의 통합 Cortex_, 135:173-185, 2021.\n' +
      '* Caron et al. (2021) Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformer, 2021.\n' +
      '* Chen et al. (2023a) Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: 멀티모달 llm의 지시적 대화 마법, 2023a.\n' +
      '* Chen et al. (2023b) Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captionions, 2023b.\n' +
      '* Chen et al. (2020) Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I. Generative pretraining from pixels. 2020년\n' +
      '* Chen et al. (2022) Chen, Y., Sipos, A., der Merwe, M. V., and Fazeli, N. 2022년 조작용 시각적 촉각 트랜스포머입니다.\n' +
      '* Chiang et al. (2023) Chiang, W. -L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zu, H., Zang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: 90%* chatpity, March 2023. URL[https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)을 갖는 gpt-4를 인상하는 오픈 소스 챗봇.\n' +
      '* Chorley et al. (2009) Chorley, C., Melhuish, C., Pipe, T., and Rossiter, J. Development of a tactile sensor based on biologically inspired edge encoding. In _2009 International Conference on Advanced Robotics_, pp. 1-6. IEEE, 2009.\n' +
      '* Daiya et al. (2009) Daiya, R. S., Metta, G., Valle, M., and Sandini, G. Tactile sensing--from human to humanoids. _ IEEE transactions on robotics_, 26(1):1-20, 2009.\n' +
      '* Dai et al. (2023) Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. 인스트럭션 블립: 명령어 튜닝이 있는 범용 비전 언어 모델에 대해, 2023.\n' +
      '* Dave et al. (2024) Dave, V., Lygerakis, F., and Rueckert, E. Multimodal visual-tactile representation learning through self-supervised contrastive pre-training. _ arXiv preprint arXiv:2401.12024_, 2024.\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. 이미지는 16x16 단어들의 가치: 스케일에서 이미지 인식을 위한 트랜스포머들이다. 2020년\n' +
      '* Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* Fu et al. (2023) Fu, L., Huang, H., Berscheid, L., Li, H., Goldberg, K., and Chitta, S. 2023년 산업 삽입을 위한 시각 촉각 피드백 정책의 실제에서 안전한 자가 지도 학습.\n' +
      '*Gao et al. (2023) Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y. Llama-adapter v2: Parameterefficient visual instruction model. _ arXiv preprint arXiv:2304.15010_, 2023.\n' +
      '* Gao et al. (2021) Gao, R., Chang, Y. - Y., Mall, S., Fei-Fei, L., and Wu, J. Objectfolder: 암시적 시각적, 청각적, 촉각적 표현을 갖는 객체들의 데이터세트. 2021년 로봇 학습 관련 회의.\n' +
      '*Gao et al. (2022) Gao, R., Si, Z., Chang, Y. -Y., Clarke, S., Bohg, J., Fei-Fei, L., Yuan, W., and Wu, J. Objectfolder 2.0: sim2real transfer를 위한 다중감각 객체 데이터세트. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10598-10608, June 2022.\n' +
      '* Geng et al. (2022) Geng, X., Liu, H., Lee, L., Schuurams, D., Levine, S., and Abbeel, P. Multimodal masked autoencoders learn transferable representations. _ ArXiv:2205.14204_, 2022.\n' +
      '* Girdhar et al. (2023) Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., and Misra, I. Imagebind: One embedding space to bind them all. _CVPR_, 2023.\n' +
      '* Goldberg & Bajcsy (1984) Goldberg, K. Y. and Bajcsy, R. 능동 터치와 로봇 인식 Cognition and Brain Theory_, 7(2):199-214, 1984.\n' +
      '* Goyal et al. (2017) Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. 정확하고 큰 미니 배치 sgd: 1시간 안에 이미네를 훈련시켜라 _ ARXiv:1706.02677_, 2017.\n' +
      '* Guo et al. (2023) Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., and Heng, P.-A. 포인트 바인딩 및 포인트-llm: 포인트 클라우드를 3D 이해, 생성 및 지시에 대한 다중 모달리티와 정렬, 2023.\n' +
      '\n' +
      'Guzhov, A., Raue, F., Hees, J., and Dengel, A. Audioclip: Extending clip to image, text and audio, 2021.\n' +
      '* Han et al. (2023) Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., Lu, X., Ren, S., Wen, Y., Chen, X., Yue, X., Li, H., and Qiao, Y. Imagebind-llm: Multi-modality instruction tuning, 2023.\n' +
      '* Haque et al. (2023) Haque, A., Tancik, M., Efros, A., Holynski, A., and Kanazawa, A. Instruct-nerf2nerf: Editing 3d scenes with instructions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023.\n' +
      '* Higuera et al. (2023) Higuera, C., Boots, B., and Mukadam, M. 점자 읽기 학습: 확산 모델로 촉각 현실 격차를 해소합니다. 2023년\n' +
      '* Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: 대형 언어 모델의 낮은 순위 적응. _International Conference on Learning Representations_, 2022. URL[https://openreview.net/forum?id=n2eVKeeFYF9](https://openreview.net/forum?id=n2eVKeeFYF9)이다.\n' +
      '* Ilharco et al. (2021) Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, July 2021. URL[https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773). 이 소프트웨어를 사용하신다면 아래와 같이 인용해 주시기 바랍니다.\n' +
      '* Ittyerah & Marks (2007) Ittyerah, M. and Marks, L. E. Memory for curvature of objects: Haptic touch vs. vision. _ 영국 심리학 저널_, 98(4):589-610, 2007.\n' +
      '* Johansson & Flanagan (2009) Johansson, R. S. and Flanagan, J. R. Coding and use of tactile signals from the finger tip from object manipulation tasks. _ Nature Reviews Neuroscience_, 10(5):345-359, 2009.\n' +
      '* Jones et al. (2005) Jones, M. G., Bokinsky, A., Tretter, T., and Negishi, A. A. Learning with haptic and visual modalities. 2005년\n' +
      '* Kampouris et al. (2016) Kampouris, C., Mariolis, I., Peleka, G., Skartados, E., Kargakos, A., Triantafyllou, D., and Malassiotis, S. 제약 없는 환경에서 의복과 의복의 재료 특성에 대한 다중 센서 및 탐구적 인식 In _2016 IEEE international conference on robotics and automation (ICRA)_, pp. 1656-1663. IEEE, 2016.\n' +
      '* Kerr et al. (2023) Kerr, J., Huang, H., Wilcox, A., Hoque, R., Ichnowski, J., Calandra, R., and Goldberg, K. 의복 기능을 찾고 추적하기 위한 자체 감독 시각 촉각 사전 훈련, 2023년.\n' +
      '* Klatzky & Lederman (2003) Klatzky, R. L. and Lederman, S. J. the skin and its receptors 148 pathway to cortex and major cortical area. _ 심리학 편람, 실험 심리학_, 4:147, 2003.\n' +
      '*Lambeta et al. (2020) Lambeta, M., Chou, P.-W., Tian, S., Yang, B., Maloon, B., Most, V. R., Stroud, D., Santos, R., Byagowi, A., Kammerer, G., Jayaraman, D., and Calandra, R. Digit: in-hand manipulation에 응용되는 저가의 소형 고해상도 촉각 센서를 위한 신규한 설계. _ IEEE Robotics and Automation Letters_, 5(3):3838-3845, 2020. doi: 10.1109/ICRA.2020.2977257.\n' +
      '* Lee et al. (2013) Lee, D.-H. et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. _Workshop on challenges in representation learning, ICML_, volume 3, pp. 896. Atlanta, 2013.\n' +
      '* Li 등(2023a) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoder and large language models, 2023a.\n' +
      '* Li & Adelson (2013) Li, R. 및 Adelson, E. H. 젤라이트 센서를 사용하여 표면 질감을 감지하고 인식한다. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 1241-1247, 2013.\n' +
      '* Li 등 (2019) Li, Y., Zhu, J.-Y., Tedrake, R., and Torralba, A. Connecting touch and vision via cross-modal prediction, 2019.\n' +
      '* Li 등(2023) Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. 마스킹을 통한 언어-이미지 사전 트레이닝의 스케일링. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 23390-23400, 2023b.\n' +
      '* Lin et al. (2023) Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., Han, J., Huang, S., Zhang, Y., He, X., Li, H., and Qiao, Y. 스핑크스: 멀티모달 대형 언어 모델에 대한 가중치, 작업 및 시각적 임베딩의 공동 혼합, 2023.\n' +
      '*Liu et al. (2023a) Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning, 2023a.\n' +
      '*Liu et al. (2023b) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _NeurIPS_, 2023b.\n' +
      '* Loshchilov & Hutter (2017a) Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restart. 2017a.\n' +
      '* Loshchilov & Hutter (2017b) Loshchilov, I. and Hutter, F. Decoupled Weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017b.\n' +
      '* Lu et al. (2023) Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023.\n' +
      '* McLachlan(1975) McLachlan, G. J. Iterative 재분류 절차 in discrimininant analysis. _ Journal of the American Statistical Association_, 70(350):365-369, 1975.\n' +
      '\n' +
      '* Miller et al. (2018) Miller, T. M., Schmidt, T. T., Blankenburg, F., and Pulvermuller, F. Verbal label facilitate tactile perception. _ Cognition_, 171:172-179, 2018.\n' +
      '*Moon et al. (2023) Moon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith, M., Jain, S., Yeh, C.-F., Murugesan, P., Heidari, P., Liu, Y., Srinet, K., Damavandi, B., and Kumar, A. Anymal: An efficient and scalable any-modality augmented language model, 2023.\n' +
      '* Ojala et al. (2002) Ojala, T., Pietikainen, M., and Maenpaa, T. 다해상도 그레이스케일 및 회전 불변 텍스처 분류와 국부 이진 패턴. _ IEEE Transactions on pattern analysis and machine intelligence_, 24(7):971-987, 2002.\n' +
      '* OpenAI(2002) OpenAI, ;., Achiam, J., Adler, S., Agarwal, S., Anadkat, L., Akkaya, I., Balaji, S., Balcom, L., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, K., Cai, T., Chen, R., Deville, J., Eloundou, T., Felix, N., Goh, G., Ecoffet, R., Gross, J., J., Harris, J., H.\n' +
      '* Pacchierotti et al. (2017) Pacchierotti, C., Sinclair, S., Solazzi, M., Frisoli, A., Hayward, V., and Prattichizzo, D. Wearable haptic systems for the fingerertip and hand: Taxonomy, review, and perspectives. _ IEEE Transactions on Haptics_, 10(4):580-600, 2017. doi: 10.1109/TOH.2017.2689006.\n' +
      '* Qi et al. (2023) Qi, H., Yi, B., Ma, Y., Suresh, S., Lambeta, M., Calandra, R., and Malik, J. General In-Hand Object Rotation with Vision and Touch. In _Conference on Robot Learning (CoRL)_, 2023.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\n' +
      '* Radosavovic et al. (2022) Radosavovic, I., Shi, B., Fu, L., Goldberg, K., Darrell, T., and Malik, J. Robot learning with sensorimotor pre-training. _ arXiv preprint arXiv:2306.10007_, 2023.\n' +
      '*Reed et al. (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Kimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de Freitas, N. 2022년 일반 요원\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2022.\n' +
      '* Rosenberg et al. (2005) Rosenberg, C., Hebert, M., and Schneiderman, H. Semi-supervised self-training of object detection models. 2005년\n' +
      '\n' +
      '* Schmidt et al. (2019) Schmidt, T. T., Miller, T. M., Blankenburg, F., and Pulvermuller, F. Neuronal correlates of label facilitated tactile perception. _ Scientific Reports_, 9(1):1606, 2019.\n' +
      '* Sferrazza & D\'Andrea (2019) Sferrazza, C. and D\'Andrea, R. 풀레졸루션 광학 촉각 센서의 설계, 동기 및 평가 Sensors_, 19(4):928, 2019.\n' +
      '* Shimonomura(2019) Shimonomura, K. 카메라를 채용한 촉각 이미지 센서: A review. _ Sensors_, 19(18):3933, 2019.\n' +
      '* Sohn et al. (2020) Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simplifying semisupervised learning with consistency and confidence. _ arXiv preprint arXiv:2001.07685_, 2020.\n' +
      '* Speed et al. (2021) Speed, L. J., Croijmans, I., Dolscheid, S., and Majid, A. Crossmodal association with olfactory, auditory, tactile stimuli in children and adults. _ i-Perception_, 12(6):20416695211048513, 2021.\n' +
      '* Stone & Gonzalez (2015) Stone, K. D. and Gonzalez, C. L. The contributions of vision and haptics to reach and grasp. _ Frontiers in psychology_, 6:1403, 2015\n' +
      '* Su et al. (2023) Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., and Cai, D. Pandagpt: One model to instruction-follow them all, 2023.\n' +
      '*Sun et al. (2023) Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X. 생성적 멀티모달 모델은 맥락 내 학습자, 2023이다.\n' +
      '* Sureh et al. (2022) Suresh, S., Si, Z., Anderson, S., Kaess, M., and Mukadam, M. 미드스타우치: 슬라이딩 터치에 걸친 분포에 대한 몬테카를로 추론. _6th Annual Conference on Robot Learning_, 2022. URL[https://openreview.net/forum?id=JWROnOf4w-K](https://openreview.net/forum?id=JWROnOf4w-K).\n' +
      '* Tang 등(2023) Tang, Z., Yang, Z., Khademi, M., Liu, Y., Zhu, C., and Bansal, M. Codi-2: In-context, interleaved, and interactive any-to-any generation, 2023.\n' +
      '* Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Turk(2014) Turk, M. 멀티모달 상호작용: 리뷰_ 패턴 인식 문자_, 36:189-195, 2014.\n' +
      '* Wang et al. (2020) Wang, X., Lian, L., Miao, Z., Liu, Z., and Yu, S. X. Long-tail recognition by routing various distribution-aware experts. _ arXiv preprint arXiv:2010.01809_, 2020.\n' +
      '* Wang et al. (2022a) Wang, X., Wu, Z., Lian, L., and Yu, S. X. Debiased learning from naturally imbalanced pseudo-labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14647-14657, 2022a.\n' +
      '* Wang et al. (2023) Wang, X., Girdhar, R., Yu, S. X., and Misra, I. Cut and learn for unsupervised object detection and instance segmentation, 2023.\n' +
      '* Wang et al. (2022b) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions, 2022b.\n' +
      '* Wu et al. (2023) Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T. - S. Next-gpt: Any-to-any multimodal llm. _ CoRR_, abs/2309.05519, 2023.\n' +
      '* Yamaguchi & Atkeson (2016) Yamaguchi, A. and Atkeson, C. G. Combining finger vision and optical tactile sensing: 야채를 절단하면서 오류를 줄이고 취급한다. In _2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)_, pp. 1045-1051, 2016. doi: 10.1109/HUMANOIDS.2016.7803400.\n' +
      '* Yang et al. (2022) Yang, F., Ma, C., Zhang, J., Zhu, J., Yuan, W., and Owens, A. Touch and go: Learning from human-collected vision and touch. 제36차 신경망 정보 처리 시스템 데이터 세트 및 벤치마크 Track_에서 2022년.\n' +
      '* Yang et al. (2024) Yang, F., Feng, C., Chen, Z., Park, H., Wang, D., Dou, Y., Zeng, Z., Chen, X., Gangopadhyay, R., Owens, A., et al. Binding touch to everything: Learning unified multimodal tactile representation. _ arXiv preprint arXiv:2401.18084_, 2024.\n' +
      '* Yuan et al. (2017) Yuan, W., Dong, S., and Adelson, E. H. Gelsight: High resolution robot tactile sensors for estimating geometry and force. _ Sensors_, 17(12):2762, 2017.\n' +
      '* Yuan et al. (2018) Yuan, W., Mo, Y., Wang, S., and Adelson, E. H. Active clothing material perception using tactile sensing and deep learning. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pp. 4842-4849. IEEE, 2018.\n' +
      '* Zhang & Demiris(2023) Zhang, F. and Demiris, Y. 로봇 보조 드레싱을 위한 의복 전개에 대한 시각적 촉각 학습 IEEE Robotics and Automation Letters_, 8(9):5512-5519, 2023. doi: 10.1109/LRA.2023.3296371.\n' +
      '* Zhang et al. (2021) Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P., and Li, H. Pointclip: Point cloud understanding by clip, 2021.\n' +
      '\n' +
      '* Zhang et al. (2023) Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., and Qiao, Y. Llama-adapter: zero-init attention을 갖는 언어 모델의 효율적인 finetuning _ arXiv preprint arXiv:2303.16199_, 2023.\n' +
      '* Zhong et al. (2022) Zhong, S., Albini, A., Jones, O. P., Maiolino, P., and Posner, I. Touching a neRF: Leveraging neural radiance fields for tactile sensory data generation. _6th Annual Conference on Robot Learning_, 2022. URL[https://openreview.net/forum?id=No3mbanR1ZJ](https://openreview.net/forum?id=No3mbanR1ZJ).\n' +
      '* Zhu et al. (2023) Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '\n' +
      '## 부록 추가 결과\n' +
      '\n' +
      '데이터세트별### 성능\n' +
      '\n' +
      '이 섹션에서는 데이터 세트의 하위 집합당 결과를 보여줌으로써 TVU 벤치마크에서 모델 성능의 표 2의 세분화된 분해를 보여준다. SSVTP 하위 집합에 대한 모델의 성능은 표 4에 나열되어 있고 HCT 하위 집합에 대한 성능은 표 5에 나열되어 있다. 결과는 GPT-4V가 "in-the-wild"로 수집된 HCT보다 실험실 환경에서 수집된 SSVTP에서 더 잘 수행됨을 시사한다.\n' +
      '\n' +
      'GPT-4V 라벨의 큰 샘플로 훈련된 모델은 GPT-4V와 동일한 성능을 달성해야 한다. 표 5의 결과는 인간 라벨이 붙은 비전 터치 **의 작은 데이터 세트에 대한 훈련이 모델의 촉각 시각적 이해도를 향상시킨다는 것을 시사한다. 이 차이는 \\(\\alpha=0.05\\)에서 통계적으로 유의하다.\n' +
      '\n' +
      '### 개방형 어휘 촉각 분류 전체 결과\n' +
      '\n' +
      '우리는 동의어에 대한 서로 다른 코사인 유사성 임계값에서 표 6 및 표 7의 표 1에 제시된 결과를 제시한다. 우리는 ViT-Small이 데이터세트의 SSVTP 부분집합에 대해 잘 수행하지만, ViT-Tiny는 촉각 텍스트 분류 작업에서 더 큰 대응물(ViT-Small 및 ViT-Base)을 능가한다는 것을 발견했다. 그러나, 촉각-비전 분류에 대해(표 7), ViT-Base는 더 작은 모델들보다 우수한 성능을 수행한다. 더 많은 통찰력은 부록 B.1에 자세히 설명되어 있다.\n' +
      '\n' +
      '## 부록 B 훈련 세부사항 및 하이퍼파라미터\n' +
      '\n' +
      '이 섹션에서는 훈련 과정과 특정 하이퍼파라미터에 대한 더 많은 통찰력과 세부 정보를 제공합니다.\n' +
      '\n' +
      '###Pseudo-label에 과적합\n' +
      '\n' +
      'GPT-4V(gpt-4-vision-preview)에 의해 생성된 의사 레이블을 활용하는 핵심 장애물은 생성된 레이블에 대한 불확실한 추정치를 구축하기 위해 로짓이 제공되지 않는다는 것인데, 이는 모델 예측을 위해 의사 레이블을 활용하는 컴퓨터 비전에서의 선행 작업에 일반적으로 필요하다(_e.g._Sohn et al.(2020); Lee et al.(2013); Wang et al.(2022). 이것은 4K 인간 라벨이 도입되는 경우에도 접촉 전용 데이터세트에서 ViT-Small에 적합하기 어렵고 의사 라벨을 시끄럽게 만든다(도 6 참조).\n' +
      '\n' +
      '4.2에서 데이터의 10%가 접촉하도록 하여 이 문제를 해결한다. 우리는 훈련 시작 시 교체 없이 데이터의 10%를 무작위로 균일하게 샘플링한다. 이것은 모델이 (ViT-Tiny, ViT-Small, ViT-Base)의 세 가지 모델 크기 모두에서 과적합되는 것을 방지한다. 그러나, 테스트 세트는 모두 인간 주석자에 의해 라벨링되기 때문에, 분배 시프트는 더 나쁜 촉각-이미지, 및 촉각-언어 분류 성능(표 1에서 관찰됨)으로 이어진다. 절제 연구로서, 우리는 또한 촉각 언어 생성을 위해 접촉 데이터에 대해서만 훈련된 ViT-Small을 미세 조정했다. 테스트 세트 성능은 4.81로 비접촉 데이터로 훈련된 ViT-Small이 얻은 성능(4.89)보다 매우 미미하게 낮다. 미래의 작품들은 불확실한 추정치를 제시하지 않는 교사 모델로부터 시끄러운 투입으로 어떻게 스케일링을 할 것인지 또는 기존의 작품들을 학습에 활용하는지를 살펴볼 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r|c c} \\hline \\hline  & **Score** & \\(p\\)-value \\\\  & (1-10) & \\((\\mathrm{d.f.}=401)\\) \\\\ \\hline LLaVA-1.5 7B & 3.64 & \\(2.32\\times 10^{-3}\\) \\\\ LLaVA-1.5 13B & 3.55 & \\(1.30\\times 10^{-3}\\) \\\\ ViP-LLaVA 7B & 2.72 & \\(4.45\\times 10^{-8}\\) \\\\ ViP-LLaVA 13B & 4.10 & \\(3.76\\times 10^{-2}\\) \\\\ LLaMA-Adapter & 2.56 & \\(7.826\\times 10^{-6}\\) \\\\ BLIP-2 Opt-6.7b & 2.02 & \\(2.74\\times 10^{-9}\\) \\\\ InstructBLIP 7B & 1.40 & \\(1.49\\times 10^{-13}\\) \\\\ InstructBLIP 13B & 1.44 & \\(4.68\\times 10^{-14}\\) \\\\ GPT-4V & 5.02 & - \\\\ \\hline SSVTP-LLaMA & 2.58 & \\(9.33\\times 10^{-6}\\) \\\\ \\hline TVL-LLaMA (ViT-Tiny) & 6.09 & \\(2.65\\times 10^{-2}\\) \\\\ TVL-LLaMA (ViT-Small) & 5.81 & \\(1.02\\times 10^{-1}\\) \\\\ TVL-LLaMA (ViT-Base) & **6.16** & \\(1.67\\times 10^{-2}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: SSVTP에 대한 **TVL 벤치마크 성능.** 기존 VLM 및 SSVTP-LLaMA에 대해 TVL-LLaMA를 벤치마킹하고 SSVTP 데이터 세트에 대해서만 성능을 보여준다. 우리는 GPT-4V 점수에 대한 각 모델의 점수에 대한 양측 쌍 표본 \\(t\\)-검정에서 \\(p\\)-값을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r|c c} \\hline \\hline  & **Score** & \\(p\\)-value \\\\  & (1-10) & \\((\\mathrm{d.f.}=401)\\) \\\\ \\hline LLaVA-1.5 7B & 3.55 & \\(8.49\\times 10^{-8}\\) \\\\ LLaVA-1.5 13B & 3.63 & \\(1.74\\times 10^{-7}\\) \\\\ ViP-LLaVA 7B & 3.44 & \\(4.10\\times 10^{-11}\\) \\\\ ViP-LLaVA 13B & 3.76 & \\(1.57\\times 10^{-5}\\) \\\\ LLaMA-Adapter & 3.08 & \\(2.05\\times 10^{-13}\\) \\\\ BLIP-2 Opt-6.7b & 2.72 & \\(1.25\\times 10^{-24}\\) \\\\ InstructBLIP 7B & 1.30 & \\(8.02\\times 10^{-73}\\) \\\\ InstructBLIP 13B & 1.21 & \\(9.74\\times 10^{-76}\\) \\\\ GPT-4V & 4.42 & - \\\\ \\hline SSVTP-LLaMA & 3.67 & \\(3.24\\times 10^{-6}\\) \\\\ TVL-LLaMA (ViT-Tiny) & 4.79 & \\(5.79\\times 10^{-4}\\) \\\\ TVL-LLaMA (ViT-Small) & 4.77 & \\(2.64\\times 10^{-3}\\) \\\\ TVL-LLaMA (ViT-Base) & **4.89** & \\(6.82\\times 10^{-5}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: HCT에 대한 **TVL 벤치마크 성능.** 기존 VLM 및 SSVTP-LLaMA에 대해 TVL-LLaMA를 벤치마킹하고 HCT 데이터 세트에 대해서만 성능을 보여준다. 우리는 GPT-4V 점수에 대한 각 모델의 점수에 대한 양측 쌍 표본 \\(t\\)-검정에서 \\(p\\)-값을 보고한다.\n' +
      '\n' +
      '### 어블레이션: 배경 감산\n' +
      '\n' +
      '우리는 촉각, 시각 및 언어 사이에서 대조적 학습을 순진하게 수행하는 것이 제로 샷 분류에 효과적이라는 것을 발견했지만, 데이터 수집에 사용되는 다양한 촉각 센서에 걸쳐 일반화를 더욱 용이하게 하기 위해 촉각 센서의 정지 배경(즉, 접촉하지 않을 때 센서로부터의 판독값)을 활용하는 것이 해결책이다. 배경 감산을 수행하여 촉각 관찰을 전처리하고, 후처리한 데이터셋 통계량을 기반으로 입력 관찰을 정규화한다. 경험적으로, 우리는 이 방법이 비접촉 데이터와 공동으로 사용될 때 분류 정확도와 다운스트림 TVL-LLaMA의 성능을 향상시킨다는 것을 발견한다(표 8).\n' +
      '\n' +
      '### Ablation: (Zero-shot) Single Modality For Generation (Out of Distribution)\n' +
      '\n' +
      '우리는 TVL-LLaMA의 훈련 동안 잠재된 촉각적 잠재와 이미지를 순진하게 평균화하기 때문에, 시각과 촉각 임베딩 사이의 일관성을 보기 위한 제로 샷 실험으로서, _test_ 시간에 시각 또는 촉각 양식 중 하나를 임의로 드롭할 수 있다. 표 9의 결과를 보고한다. 더 큰 인코더는 더 표현적일 수 있지만, 더 큰 촉각 인코더는 이 실험 설정에서 더 나쁜 제로 샷 성능을 초래한다는 것을 발견하며, 이는 표 (a)a와 일치한다. 흥미롭게도, 배경 감산(부록 B.2)은 촉각에서 제로 샷 성능을 향상시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c c} \\hline \\hline \\multirow{2}{*}{Percentile} & \\multicolumn{2}{c}{SSVTP} & \\multicolumn{2}{c}{HCT} & \\multicolumn{2}{c}{TVL} \\\\ \\cline{3-8}  & & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\\\ \\hline \\multirow{3}{*}{0} & ViT-Tiny & 29.4\\% & 71.7\\% & 34.8\\% & 70.1\\% & 36.7\\% & 70.3\\% \\\\  & ViT-Small & 42.4\\% & 76.1\\% & 36.5\\% & 68.0\\% & 36.3\\% & 66.4\\% \\\\  & ViT-Base & 38.0\\% & 69.6\\% & 34.8\\% & 65.6\\% & 30.7\\% & 63.6\\% \\\\ \\hline \\multirow{3}{*}{25} & ViT-Tiny & 3.3\\% & 21.7\\% & 7.2\\% & 22.9\\% & 4.6\\% & 14.1\\% \\\\  & ViT-Small & 10.9\\% & 33.7\\% & 9.1\\% & 21.5\\% & 6.7\\% & 19.5\\% \\\\  & ViT-Base & 8.7\\% & 31.5\\% & 5.9\\% & 14.0\\% & 4.4\\% & 13.7\\% \\\\ \\hline \\multirow{3}{*}{50} & ViT-Tiny & 3.3\\% & 19.6\\% & 4.8\\% & 17.8\\% & 3.7\\% & 11.8\\% \\\\  & ViT-Small & 10.9\\% & 32.6\\% & 6.6\\% & 15.3\\% & 5.9\\% & 11.0\\% \\\\  & ViT-Base & 7.6\\% & 28.3\\% & 4.5\\% & 9.8\\% & 3.5\\% & 11.0\\% \\\\ \\hline \\multirow{3}{*}{75} & ViT-Tiny & 3.3\\% & 19.6\\% & 4.1\\% & 14.2\\% & 3.7\\% & 10.7\\% \\\\  & ViT-Small & 10.9\\% & 28.3\\% & 3.5\\% & 7.9\\% & 3.4\\% & 10.2\\% \\\\ \\cline{1-1}  & ViT-Base & 7.6\\% & 28.3\\% & 3.5\\% & 7.9\\% & 3.4\\% & 10.2\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 모델 아키텍처 및 유사성 임계값 \\(\\phi\\)이 **촉각-텍스트** 분류 정확도에 미치는 영향 각 백분위수에 대한 유사성 임계값 \\(\\phi\\)은 0.636(0th), 0.859(25th), 0.893(50th), 0.921(75th)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{SecVTP} & \\multicolumn{2}{c}{HCT} & \\multicolumn{2}{c}{TVL} \\\\ \\cline{2-5}  & Top-1 & Top-5 & Top-1 & Top-5 & Top-1 & Top-5 \\\\ \\hline ViT-Tiny & 34.8\\% & 70.7\\% & 85.3\\% & 99.0\\% & 79.5\\% & 95.7\\% \\\\ ViT-Small & 28.3\\% & 69.6\\% & 84.4\\% & 98.9\\% & 78.0\\% & 95.2\\% \\\\ ViT-Base & 34.8\\% & 66.3\\% & 87.8\\% & 99.7\\% & 81.7\\% & 95.7\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7:촉각 인코더 모델 아키텍처가 **촉각-비전** 분류에 미치는 영향.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline  & Zero-Shot & Zero-Shot & Tactile \\\\  & Tactile & Vision & \\& Vision \\\\ \\hline TVL-LLaMA & **4.56** & 4.66 & 4.94 \\\\ TVL-LLaMA & 3.50 & 4.81 & 4.89 \\\\ TVL-LLaMA & 2.80 & **4.85** & 5.03 \\\\ \\hline TVL-LLaMA & & & \\\\ (ViT-Small) & 4.52 & - & **5.06** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 한 모달리티(분포 외) 제로 샷 실험 떨어뜨리기\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline  & Zero-Shot & Zero-Shot & Tactile \\\\  & Tactile & Vision & \\& Vision \\\\ \\hline TVL-LLaMA & **4.56** & 4.66 & 4.94 \\\\ (ViT-Tiny) & & & \\\\ TVL-LLaMA & 3.50 & 4.81 & 4.89 \\\\ TVL-LLaMA & 2.80 & **4.85** & 5.03 \\\\ \\hline TVL-LLaMA & & & \\\\ (ViT-Small) & 4.52 & - & **5.06** \\\\ + Background Subtract & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: ViT-Small 촉각 인코더 훈련 중 비접촉 데이터와 배경 감산이 TVL 벤치마크에서 분류 정확도와 성능에 미치는 영향.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '다양한 시야각 및 범위에서 데이터를 수집합니다. 멀티모달 트레이닝을 위한 데이터 세트의 유용성을 보장하기 위해, 우리는 각 궤적 동안 촉각 센서와 관심 객체와의 접촉 지점이 카메라를 보도록 항상 상대적 위치를 설정한다. 핸들 디자인은 오토데스크 퓨전 360에서 개념화되었으며 밤부 랩 P1P 3D FDM 프린터에 인쇄되었습니다. CAD 파일은 오픈소싱됩니다.\n' +
      '\n' +
      '촉각 언어 생성을 위한 프롬프트 목록###\n' +
      '\n' +
      '촉각 언어 생성을 위해 우리의 언어 모델을 미세 조정할 때, 이를 시각적 명령어 튜닝 문제(Liu et al., 2023b)로 공식화한다. 우리는 질문으로 의미적으로 유사한 프롬프트의 다음 세트에서 무작위로 선택하고 인간 레이블 세트를 답으로 취급한다. 이는 훈련 시 보이는 데이터의 다양성을 높이는 역할을 한다.\n' +
      '\n' +
      '이 이미지는 이 이미지의 촉감적 느낌을 불러일으킨다 이 이미지의 촉감적 느낌을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다 이 이미지의 촉감적 감각을 불러일으킨다\n' +
      '\n' +
      '### 어휘 단어 분포\n' +
      '\n' +
      'TVL 데이터세트 내의 인간 라벨 및 의사-라벨의 리스트 및 카운트는 사전 포맷으로 여기서 재생된다(모든 오타가 데이터세트로부터 이월된다는 점에 유의). 시각적으로\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Tactile Statistics & Mean & Std. \\\\ \\hline \\multirow{3}{*}{With Background} & 0.292 & 0.188 \\\\  & 0.297 & 0.195 \\\\  & 0.291 & 0.219 \\\\ \\hline \\multirow{3}{*}{Background Subtracted} & -0.008 & 0.045 \\\\  & -0.019 & 0.044 \\\\ \\cline{1-1}  & -0.018 & 0.053 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 촉각 정규화 통계량\n' +
      '\n' +
      '도 8: 센서 홀더 CAD 모델의 대안적 관점: 페이스 다운 뷰(왼쪽) 및 분해 뷰(오른쪽)이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Image Statistics & Mean & Std. \\\\ \\hline \\multirow{3}{*}{OpenCLIP Statistics} & 0.481 & 0.269 \\\\  & 0.458 & 0.261 \\\\ \\cline{1-1}  & 0.408 & 0.276 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: RGB 정규화 통계 표현은 도 9에 제공된다.\n' +
      '\n' +
      '\'부드러운\'은 14577, \'조직화\'는 12443, \'경화\'는 10433, \'반사\'는 8643, \'부드러운\'은 8415, \'광택\'은 6411, \'강성\'은 5659, \'평탄\'은 5379, \'평탄\'은 4363, \'평탄\'은 3553, \'곡면\'은 3513, \'고형\'은 3337, \'부드러운\'은 2559, \'구부러운\'은 1587, \'구부러운\'은 1381, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은, \'구부러운\'은,\n' +
      '\n' +
      'Psuedo-Label 생성을 위한### Prompting\n' +
      '\n' +
      '우리는 촉각적 설명으로 이미지를 라벨링하기 위해 GPT-4V와 함께 다음 프롬프트를 사용한다:\n' +
      '\n' +
      '```\n' +
      '1SurfaceType:[Specifythesurfacetype,e.g.,"metal,"#fabric"]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline Config & Value \\\\ \\hline optimizer & AdamW (Loshchilov \\& Hutter, 2017b) \\\\ base learning rate & 1.5e-4 \\\\ learning rate schedule & cosine decay (Loshchilov \\& Hutter, 2017a) \\\\ batch size & 256 \\\\ weight decay & 0.05 \\\\ optimizer momentum & \\(\\beta_{1},\\beta_{2}\\) = 0.9, 0.95 (Chen et al., 2020) \\\\ warm up epoch (Goyal et al., 2017) & 10 \\\\ total epochs & 200 \\\\  & RandomHorizontalFlip, \\\\ RGB Augmentation & ColorJitter, \\\\  & RandomGrayscale, \\\\  & GaussianBlur \\\\ Tactile Augmentation & (Optional) Background Subtraction \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: Encoder Pretraining Hyperparameters*Image:Thefirstimageisfromacameraobservingthetactilesensor(shiny,nearthetopofthemimage)andthesurface.Thesecondimageisacroppedversionofthefirstimagethatfocousesonthecontactpatch.\n' +
      '*Example:Foracsmoothandcoldsurface,thedescriptionmightbe"slick,chilly,hard,unyielding,glossy."\n' +
      '*Task:Basedontheseimages,describethepossibletactilefeelingsofthecontactpatchusingsensoryadjectives.Limityourresponseuptofiveadjectives,separatedbycommas.\n' +
      '\n' +
      '### 평가용 프롬프트 GPT-4\n' +
      '\n' +
      'TVL 벤치마크에 다음과 같은 프롬프트를 사용합니다.\n' +
      '\n' +
      '```\n' +
      '1[UserQuestion]:[prompt]\n' +
      '2[AssistantResponse]:{assistant_response}\n' +
      '3[CorrectResponse]:{correct_response}\n' +
      '4\n' +
      '5WewouldliketorequestyourfeedbackontheperformanceofanAIassistantinresponsetotheuserquestiondisplayedabove.\n' +
      '6Theuserasksthequestiononobservinganimage.Theassistant\'sresponseisfollowedbythecorrectresponse.\n' +
      '7\n' +
      '8Pleaseevaluatetheassistant\'sresponsebasedonhowcloselyitmatchestthecorrectresponsewhichdescribestactilefeelings.Pleasecompareonlythesemanticsoftheanswers.DONOTconsidergrammaticalerrorsinscoringtheassistant.Theassistantreceivesanoverallscoreonascaleof1to10,whereahigherescoreindicatesbetteroverallperformance.\n' +
      '9\n' +
      '10Pleasefirstoutputasingalinecontainingonlyonevalueindicatingthescorefortheassistant.\n' +
      '11\n' +
      '12Inthesubsequentline,pleaseprovideaccomprehensiveexplanationofyourevaluation,avoidinganypotentialbias.\n' +
      '\n' +
      '도 9: TVL 데이터세트에서의 단어들의 분포: TVL 데이터세트는 254개의 고유한 촉각 기술자들을 포함하고, 일반적인 촉각 기술들(부드럽고, 단단하고, 단단함)로부터 특이하고 광학 기술자들까지 다양하다. 이러한 덜 일반적인 형용사에는 VLM에 의해 생성된 잘못된 철자와 비촉각 기술자의 작은 부분이 포함된다. 영상 분류에서 흔히 볼 수 있는 Long-right-tailed 분포(Wang et al., 2020)는 촉각-의미 데이터에 대한 예측 변수를 학습하기 위한 과제를 제시한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>