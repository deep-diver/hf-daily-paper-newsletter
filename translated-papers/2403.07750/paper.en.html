<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Synth\\({}^{2}\\): Boosting Visual-Language Models with\n' +
      '\n' +
      'Synthetic Captions and Image Embeddings\n' +
      '\n' +
      ' Sahand Sharifzadeh\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Google DeepMind, London, UK. Correspondence to: Sahand Sharifzadeh \\(<\\)sharifzadeh@google.com\\(>\\), Andrea Banino \\(<\\)abanino@google.com\\(>\\).\n' +
      '\n' +
      ' Christos Kaplanis\n' +
      '\n' +
      'Shreya Pathak\n' +
      '\n' +
      'Dharshan Kumaran\n' +
      '\n' +
      'Anastasija Ilic\n' +
      '\n' +
      'Jovana Mitrovic\n' +
      '\n' +
      'Charles Blundell\n' +
      '\n' +
      'Andrea Banino\n' +
      '\n' +
      'Equal contribution \\({}^{1}\\)Google DeepMind, London, UK. Correspondence to: Sahand Sharifzadeh \\(<\\)sharifzadeh@google.com\\(>\\), Andrea Banino \\(<\\)abanino@google.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'The creation of high-quality human-labeled image-caption datasets presents a significant bottleneck in the development of Visual-Language Models (VLMs). We propose a novel approach that leverages the strengths of Large Language Models (LLMs) and image generation models to create synthetic image-text pairs for efficient and effective VLM training. Our method employs pre-training a text-to-image model to synthesize image embeddings starting from captions generated by an LLM. These synthetic pairs are then used to train a VLM. Extensive experiments demonstrate that the VLM trained with synthetic data exhibits comparable performance on image captioning, while requiring a fraction of the data used by models trained solely on human-annotated data. In particular, we outperform the baseline by 17% through augmentation with a synthetic dataset. Furthermore, we show that synthesizing in the image embedding space is 25% faster than in the pixel space. This research introduces a promising technique for generating large-scale, customizable image datasets, leading to enhanced VLM performance and wider applicability across various domains, all with improved data efficiency and resource utilization.\n' +
      '\n' +
      'Machine Learning, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Visual-language models (VLMs) are quickly emerging as powerful tools for understanding visual and textual information. Their ability to combine these two modalities holds immense promise for application ranging from image captioning to visual question answering. While VLMs hold significant potential, their performance is often constrained by limited data availability. Recent breakthroughs demonstrate that pre-training VLMs on larger image-text pair datasets leads to significant improvements in downstream tasks (Li et al., 2022; Hu et al., 2022). However, creating such datasets poses several challenges such as scarcity of paired data, high curation costs, low diversity and high imbalance in semantics, and the potentially noisy nature of data sourced from the internet (e.g., LAION (Schuhmann et al., 2021)). These factors often lead to laborious filtering and extended training times due to low signal-to-noise ratio, thus increasing overall resource consumption.\n' +
      '\n' +
      'This work tackles these limitations by introducing a novel approach that leverages pre-trained generative text and image models to efficiently create synthetic paired data for VLMs. Our approach uniquely generates both text and images synthetically, overcoming the reliance on real-world data and addressing the challenges of scarcity, cost, and noise.\n' +
      '\n' +
      'Figure 1: Traditional dataset curation pipelines require a human in the loop to collect and annotate images (A). We study whether we can reverse this pipeline with generative models, i.e. by first sampling synthetic captions from an LLM and then synthetically generating images from those (B). By operating in the image embedding space, we also bypass computationally expensive encoder/decoder steps, optimizing and integrating the process within visual-language model (VLM) training (C).\n' +
      '\n' +
      'While synthetic data generation has been explored for various computer vision tasks such as image segmentation, optical flow prediction, or image classification (Mishra et al., 2022; Greff et al., 2022; Azizi et al., 2023; Fan et al., 2023; Li et al., 2023c), its application to both visual and textual modalities within VLM training is a significant advancement.\n' +
      '\n' +
      'Moreover, instead of using off-the-shelf image generation models, we employ a controlled image generation approach. That is, we pre-trained our image generator on a dedicated dataset (also used for VLM training) to prevent unintended knowledge transfer from the image generator into the VLM. This strategy ensures a fair evaluation of the impact of synthetic data.\n' +
      '\n' +
      'Finally, our framework can operate also at the embedding level; our text-to-image model generates image embeddings which feed directly into the VLM, bypassing pixel-space rendering. This paradigm shift dramatically reduces memory overhead and resource consumption while preserving the quality of synthetic training data.\n' +
      '\n' +
      'To sum up, our research explores the synergy between VLMs and text-to-image generation models, demonstrating a powerful framework for overcoming data limitations. This paper presents the following key contributions:\n' +
      '\n' +
      '* **Fully Synthetic Data Creation:** We introduce the first VLM training process that utilizes a fully synthetic dataset of high-quality text-image embedding pairs. These pairs are generated by pre-trained generative models, circumventing the need for massive amounts of real-world data for either modality.\n' +
      '* **Efficient Embedding Space Generation:** Our method works with images in embedding space, avoiding costly image rendering and dramatically improving efficiency, without comprising performance.\n' +
      '* **Fair Evaluation Through Control:** By pre-training a text-to-image model on the same dataset used for VLM training, instead of using a large, off-the-shelf model, we prevent knowledge transfer from a model trained on vast, external image-text pairs. This fosters a fair and focused assessment of the benefits of synthetic data in isolation.\n' +
      '* **Demonstrated Effectiveness:** Experiments will showcase significant performance gains in image captioning when using our synthetic data, assessed against real and synthetic-data baselines.\n' +
      '\n' +
      'In conclusion, we offer insights into the future of VLM training, highlighting the potential for creating customized and potentially infinitely expandable datasets using text-driven approaches.\n' +
      '\n' +
      '## 2 Related Works\n' +
      '\n' +
      'VLMsVisual-language models (VLMs) pretrained on large-scale image-text data, primarily sourced from the web, have shown remarkable capabilities across numerous vision-language tasks. These tasks include image captioning, visual question answering, and few-shot learning (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2022b). Architecturally, VLMs commonly employ an image encoder coupled with a large language model (LLM). Researchers explore diverse image encoder designs, ranging from convolutional neural networks (CNNs) (Alayrac et al., 2022) to transformer-based architectures (Chen et al., 2022). Additionally, the choice between pretraining the image encoder on separate image datasets (Alayrac et al., 2022) and training it from scratch alongside the VLM (Tsimpoukelli et al., 2021) remains an active area of investigation. Pre-training VLMs involves various objectives, including supervised learning approaches, as well as contrastive methods that focus either on aligning image and text representations (Radford et al., 2021) or on aligning different image representations (Chen et al., 2020b;a; Grill et al., 2020). Similar diversity exists in LLM choices, with both encoder-decoder (Chen et al., 2022) and decoder-only (Alayrac et al., 2022) architectures being utilized. The strategies for combining information extracted by the image encoder and the language model also exhibit substantial variation. Despite the success achieved with web-scraped datasets, there\'s a growing emphasis on designing VLM architectures that facilitate training using synthetically generated data. This ca\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|c|c|c|c|c|c|c} \\hline \\hline Generator & Method & Generator Model & Generator Range & Generated Pairs & Caption Class & Caption Type & Evaluation Setting \\\\ \\hline \\multirow{4}{*}{Simulation/Rendering Engines} & Mishra et al. (2022), & & & Mixed pairs such as & & & \\multirow{4}{*}{N/A} & \\multirow{4}{*}{Mix} \\\\  & Greff et al. (2022), & & & & (Segmentation, Images), & & & \\\\  & Zheng et al. (2020) & Mix & Mix & (Optimal Flow, Videos), & & & & \\\\  & de Melo et al. (2022) & & & (Depth Map, Images) & & & & \\\\ \\hline \\multirow{4}{*}{Off-the-shelf Image Generator} & Cascante-Bonilla et al. (2022) & Mix & Pixel & (Captions, Images) & Complex & Rule-based & Vision Encoder \\\\ \\cline{2-8}  & Azizi et al. (2023), & SD, & & & & & & \\\\ \\cline{1-1}  & Fan et al. (2023) & ImageNet, & Pixel & & (Class, Images) & Simple & ImageNet Classes & Classific \\\\ \\cline{1-1}  & Li et al. (2023c) & SD & Pixel & (Captions, Images) & Complex & Human Generated & VLM \\\\ \\hline \\multirow{2}{*}{Controlled Image Generator} & \\multirow{2}{*}{Synth\\({}^{2}\\)} & \\multirow{2}{*}{MUSE} & \\multirow{2}{*}{Embedding} & (Captions, Images) & Complex & Human Generated & VLM \\\\ \\cline{1-1} \\cline{4- pability holds particular significance for applications where data scarcity or resources availability pose significant challenges.\n' +
      '\n' +
      'Training From Synthetic DataThe generation of synthetic data for training machine learning models remains a highly active area of research. While numerous studies (Mishra et al., 2022; Cascante-Bonilla et al., 2023; Greff et al., 2022; Zheng et al., 2020; de Melo et al., 2022) explore the use of model-based rendering engines or simulators, the remarkable advancements in high-quality image generators have ignited a surge of interest in leveraging generative models for creating training data. This trend has significantly impacted a wide range of computer vision tasks, including semantic segmentation (Li et al., 2022; Ros et al., 2016; Baranchuk et al., 2021; Tritrong et al., 2021; Li et al., 2021; Chen et al., 2019), human motion understanding (Varol et al., 2017; Ma et al., 2022; Guo et al., 2022), and more recently image classification (Azizi et al., 2023; Fan et al., 2023). Our work investigates the potential of data-driven generative models within the domain of visual-language models (VLMs). We focus on their application in downstream tasks such as image captioning, where the ability to process complex scene descriptions is important. From this aspect, our approach is closest to a concurrent work by Li et al. (2023c) where the goal is to replace faulty images in a captioning pipeline with their synthetic versions. However, our work distinguishes itself from Li et al. (2023c) by achieving strong performance while utilizing 40 times less paired data and only a quarter of the parameters. This demonstrates the potential for achieving both performance and efficiency through our proposed approach. Furthermore, unlike the mentioned works (Azizi et al. (2023); Fan et al. (2023); Li et al. (2023c), we avoid using an off-the-self image generator trained on larger datasets. Our work can also be likened to the concept of cycle consistency (Zhu et al., 2017) in visual-language models. This principle, where image-to-text and text-to-image conversion can be employed as a form of synthetic data training, albeit with extra gradients throughout the network, exploring image-text cycle consistency during sampling Li et al. (2023a) or training Li et al. (2023b) has been explored in recent works, demonstrating promising results.\n' +
      '\n' +
      'Finally, we emphasize the efficiency of pipelines that seamlessly connect data generation and model training. While most prior work has concentrated on generating images in pixel space, we investigate the generation of image embeddings that can be directly integrated into the VLM. Our approach aligns with recent work in the scene graph classification domain (Sharifzadeh et al., 2021, 2022), which has explored synthetic scene graph embedding generation using pretrained object representations.\n' +
      '\n' +
      'Table 1 provides a taxonomy of some of the mentioned related work to clarify the differences better. As shown, our study is the first of its kind to scientifically explore the application of synthetic data generation within a VLM training pipeline and in particular using image embeddings and by generating synthetic captions.\n' +
      '\n' +
      '## 3 Synth\\({}^{2}\\)\n' +
      '\n' +
      'Given the synthetic generation of text and then images, we refer to our method as Synth\\({}^{2}\\). Synth\\({}^{2}\\) is a pipeline for training VLMs using generative models in addition to collected human data. In this section we introduce different components of this pipeline, namely Caption Generation, Image Generation and the full Synth\\({}^{2}\\) model.\n' +
      '\n' +
      '### Caption Generation\n' +
      '\n' +
      'We leverage the generative capabilities of LLMs to create synthetic captions. To ensure a wide variety of captions, we adopt a class-based prompting approach. First, we randomly select a class from the ImageNet21k dataset (Ridnik et al., 2021). The LLM (Gemini Pro Team et al., 2023)) is then prompted with the following text:\n' +
      '\n' +
      '_Make up a human-annotated description of an image that contains the following object: \\([object]\\). The caption should be around 30-40 words long. Describing the different components of the scene in an objective and unbiased way. Do not add subjective judgments about the image, it should be as factual as possible. Do not use fluffy, poetic language. Respond only with the caption itself, beginning with "This is an image of"._\n' +
      '\n' +
      'Figure 2: Example of synthetic captions and synthetic images generated by LLM and text-to-image generator.\n' +
      '\n' +
      'where we replace "[object]" with the randomly selected class. This class-based prompting encourages the LLM to generate captions that cover a broad range of visual concepts. Figure 2 shows some samples from the generated captions.\n' +
      '\n' +
      '### Image Generation\n' +
      '\n' +
      'Off-the-shelf image generator models, trained on vast image-text datasets, possess an unfair advantage for studying the effect of synthetic images in VLM training. Their exposure to extensive human annotated data might also enable them to produce images highly reminiscent of the original training set. To ensure fair evaluation, we propose pre-training a text-to-image generator in a controlled environment. This generator is trained on a human annotated image-caption dataset that will also be utilized for VLM training, alongside the synthetic data. By doing so, we effectively eliminate the effects of external data on the VLM training.\n' +
      '\n' +
      'Our architecture is similar to Chang et al. (2023) and was chosen for its superior inference efficiency due to its use of discrete image tokens and its parallel decoding capability, which dramatically reduces the number of sampling iterations compared to auto-regressive models. Additionally, MUSE leverages a transformer architecture, further enhancing efficiency (Chang et al., 2023). Figure 3 shows some examples of generated images compared with original ones.\n' +
      '\n' +
      '#### 3.2.1 Training\n' +
      '\n' +
      'To pretrain our image generator given paired image and texts, we embed the texts with a pre-trained language model and the images with a pre-trained VQ-GAN (Esser et al., 2021) (refer to A.1.1 in the appendix for the details). The pre-trained language model is the same used in our VLM and it is a reduced version of Chinchilla (Hoffmann et al., 2022) that is using 400 million parameters. Following the approach in Chang et al. (2023), we apply a masking procedure with a cosine scheduling function. This function gradually increases the masking ratio during training, biasing the model towards learning from images with larger masked regions. On average, around 64% of the VQ tokens are replaced with a special "dropped token". The noisy VQ tokens and the embedded text are then fed into a transformer model. The model\'s objective is to predict the original, unmasked VQ tokens. The loss function is the cross-entropy between the predicted VQ tokens and the masked ones:\n' +
      '\n' +
      '\\[L_{t2i}=\\prod_{t\\in M}p(z(x)_{t}|y,\\{z(x)_{u}|u\\notin M\\}),\\]\n' +
      '\n' +
      'where \\(z(x)\\) denotes the VQ tokens computed from the ground-truth image \\(x\\), \\(y\\) denotes the ground-truth caption, and \\(M\\) denotes the indices of the masked tokens.\n' +
      '\n' +
      'Our text-to-image generator was trained only on 10.1 millions text-image pairs from Conceptual Captions V2 (Changpinyo et al., 2021) (see Section 4 for details).\n' +
      '\n' +
      '#### 3.2.2 Inference\n' +
      '\n' +
      'During inference, we use an iterative procedure to generate images. At first iteration, the predicted VQ tokens are all initialized with the "dropped token" representing the masked entries. At each upcoming iteration, the text embedding\n' +
      '\n' +
      'Figure 3: Qualitatives showing images selected from the validation set of MS-COCO (Chen et al., 2015) (top row) and their synthetic versions generated by our text-to-image generator given the ground truth captions (bottom). The training of the image generator has been done on Conceptual Caption v2 (Changpinyo et al., 2021).\n' +
      '\n' +
      'and the predicted VQ tokens until that stage are fed into the transformer and the model predicts the masked tokens. A cosine schedule determines which masked tokens have the highest prediction confidence during decoding. These tokens are unmasked, progressively reducing the masked token set. This process repeats for 24 iterations, gradually refining the generated VQ-tokens.\n' +
      '\n' +
      'Additional details for our text-to-image generator model are reported in A.1.3 in the appendix.\n' +
      '\n' +
      '### Synth\\({}^{2}\\) VLM architecture\n' +
      '\n' +
      'As described in Section 3.2, our text-to-image model generates image embeddings (VQ tokens) through an iterative caption-conditioned denoising process. A VQ-GAN decoder could be used to convert these tokens into human-viewable images. However, we emphasize that the image embeddings themselves provide a compressed representation of an image, making the decoder step unnecessary for certain applications.\n' +
      '\n' +
      'Therefore, to enhance efficiency for training the VLM from synthetic data, we design the VLM to enable the bypass of pixel-space processing. This is achieved by setting its vision encoder to be identical to the VQ-GAN backbone used for our image generator. This enables seamless interaction between the VLM and synthetically generated image embeddings and eliminates the computationally expensive decoding stage of VQ-GAN for the image generator and the encoding stage for the VLM when training on synthetic data. At the same time, it allows us to train from images in pixel space when using human annotated data. Overall, our VQ-based design removes the need for costly conversions to and from pixel space, resulting in significant time and disk space savings as will be shown in Section 4. To harness the efficiency of discrete tokens during training and sampling, while maintaining the rich image understanding provided by soft embeddings, we convert each discrete token to its soft embedding using the codebook before feeding them to the VLM.\n' +
      '\n' +
      'On top of the VQ backbone in our VLM, we incorporate a Perceiver Resampler component (Jaegle et al., 2021) to cross-attend to the VQ tokens extracted from our backbone. Similar to Flamingo (Alayrac et al., 2022), we employ a frozen language model to cross-attend to the image representation and autoregressively generate text. While the self-attention layers in the language model remain frozen, the cross-attention layers between the language and vision components are trainable, allowing the model to learn effective multimodal representations.\n' +
      '\n' +
      'The VLM\'s objective is to predict the next token in a sequence, conditioned on the input image. This is formulated as a cross-entropy loss:\n' +
      '\n' +
      '\\[L_{VLM}=\\prod_{l=1}^{L}p(y_{l}|y_{<l},x),\\]\n' +
      '\n' +
      'where \\(x\\) denotes the image, \\(y_{l}\\) represents the l-th language token, and \\(y_{<l}\\) represents all tokens preceding \\(y_{l}\\).\n' +
      '\n' +
      'As shown in Figure 4 by combining the components introduced in the previous parts, we can train VLMs either from human annotated data or synthetic image-captions pairs such that the VLM is conditioned on the synthetic image embedding with the cross-entropy loss being between the synthetic captions and the VLM\'s predictions.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '#### 4.1.1 Datasets\n' +
      '\n' +
      'For training we used the following datasets in our experiments:\n' +
      '\n' +
      '* Conceptual Captions v2 (Changpinyo et al., 2021) is a dataset with 12 million image-text pairs that are collected by automatically extracting images and their corresponding "alt-text" descriptions from various websites. In our experiments we use Conceptual Captions v2 as the main source of human-annotated data whereas for the other datasets we might use synthetically generated images/texts depending on the experiment.\n' +
      '* WebLI (Chen et al., 2022) has around 350 million image and alt text pairs with high text quality in English.\n' +
      '\n' +
      'Figure 4: We introduce a VLM framework that leverages LLMs and image generation models to create synthetic image-text pairs for efficient training. We can train a VLM from both non-synthetic (A) and synthetic (B) data as shown in this figure. Our model trained with the added synthetic pairs demonstrates impressive image captioning performance, significantly reducing the need for human annotated images.\n' +
      '\n' +
      '* LTIP (Alayrac et al., 2022) has 312 million web collected image and text pairs. For more details refer to Alayrac et al. (2022).\n' +
      '* GenPair refers to 1 million fully synthetic captions that are created as described in Section 3.1, paired with synthetic images generated on-the-fly during the VLM training using our text-to-image generator.\n' +
      '\n' +
      'For evaluation we used:\n' +
      '\n' +
      '* MS-COCO (Chen et al., 2015): we evaluate the performance of our models on the commonly used test set of MS-COCO under zero-shot and fine-tuning settings. For the finetuning settings we use the training set MS-COCO. We use the Karpathy split (Karpathy and Fei-Fei, 2015) for evaluation. We use CIDEr score (Vedantam et al., 2015) as our metric.\n' +
      '* Flickr-30k (Plummer et al., 2015): again, we evaluate performances on the Karpathy split.\n' +
      '\n' +
      '#### 4.1.2 Training details\n' +
      '\n' +
      'For both the image generator and the VLM we use a pretrained and frozen VQ-GAN (see A.1.1 for details on the network). The images are input at a resolution of 256x256, with a patch size of 16, which results in a sequence length of 256 tokens (16x16). Both models use a pre-trained and frozen Chinchilla 400m (Hoffmann et al., 2022) as LLM. The VLM has a perceiver resampler (Alayrac et al., 2022; Jaegle et al., 2021) with 6 layers and 64 latent input queries that cross-attend to the image embeddings. There are cross attention layers between each layer in the LLM and the output of perceiver resampler similar to the original Flamingo architecture (please refer to A.1.4 for extra details).\n' +
      '\n' +
      'We use ViT-B (Dosovitskiy et al., 2020) for the main text-to-image model in the image generator (a transformer with 24 layers, embedding dimension of 769, 12 heads and an intermediate latent size of 3072, dropout rate of 0.1). We use a maximum text length of 64 tokens with guidance scale of 4.0 with 24 refining steps and sample choice temperature of 32.5. All the models are trained with AdamW (Loshchilov and Hutter, 2017) with a learning rate of 0.0001 with 5000 warmup steps, a batch size of 512. For finetuning settings (on COCO), we use a learning rate of 0.00001. Section A.1.3 reports extra details. We pre-train the image generator for 500k steps at which point it has a Frechet inception distance (FID) (Heusel et al., 2017) of 17.1 on MS-COCO test pairs. Our VLMs training experiments all run for 500k steps. Our models are all implemented in Jax and trained on 256 TPUs.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '#### 4.2.1 Synthetic Images\n' +
      '\n' +
      'To assess the effectiveness of synthetic images in VLM training, we conduct a study where human-written captions from existing datasets are paired with synthetic images generated by a text-to-image model. We train a VLM on these synthetic image-caption pairs and evaluate its performance against a model trained on the original real image-caption pairs. This comparison allows us to investigate whether synthetic images can effectively substitute for real images in the context of VLM training.\n' +
      '\n' +
      'Specifically, we generate synthetic images for the LTIP and WebLI datasets using their annotations. This provides us with _(Real Text, Synthetic Image)_ pairs to train a VLM with (**Synth\\({}^{2}\\)** in Table 2). We compare this model to the models trained on the original _(Real Text, Real Image)_ pairs (**Gold** in Table 2) and one trained from human-annotated data of Conceptual Caption v2 (CCv2) only, using no data from LTIP and WebLI (**Baseline** in Table 2). For consistency, all Synth\\({}^{2}\\) and Gold models are co-trained with CCv2, the same dataset used by our text-to-image generator and the baseline.\n' +
      '\n' +
      'As shown in Table 2, synthetic images significantly improve baseline model performance by over 25%. Importantly, they are effective for VLM training despite the smaller volume of human-annotated images used. Note that "Gold" represents the upper performance bound when using the full original dataset. For LTIP, synthetic images slightly outperform original images, likely due to increased data diversity. Because original pairs remain static during training, synthetic im\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l c c c c c} \\hline \\hline Method & Real & Synth & \\#Real Data & \\#Synth Data & CIDEr-COCO (\\(\\uparrow\\)) & CIDEr-Flickr-30 (\\(\\uparrow\\)) \\\\ \\hline Baseline & CCv2 & - & 10.1M & - & 22.1 & 11.3 \\\\ \\hline Synth\\({}^{2}\\) & CCv2 & LTIP & 10.1M & 330M & **28.7** & **12.9** \\\\  & CCv2 & WebLI & 10.1M & 350M & 27.6 & 12.3 \\\\ \\hline Gold & CCv2+LTIP & - & 340.1M & - & 27.6 & 13.2 \\\\  & CCv2+WebLI & - & 360.1M & - & 30.7 & 15.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Zero shot image captioning results when training with ground truth captions paired with either the original or synthetically generated images.\n' +
      '\n' +
      'ages introduce beneficial image augmentation (Azizi et al., 2023).\n' +
      '\n' +
      'These results suggest the potential of generative models to enhance VLM training, even with limited real image data. While previous work (Azizi et al., 2023; Fan et al., 2023) demonstrated the use of synthetic images for single-object image classification (i.e., ImageNet), our findings show that VLMs can be successfully trained with synthetic datasets containing complex, multi-object scenes and detailed captions. This highlights the potential of synthetic images to move beyond simple classification and significantly improve VLM performance in diverse, challenging scenarios.\n' +
      '\n' +
      '#### 4.2.2 Synthetic text and image pairs\n' +
      '\n' +
      'To further demonstrate the efficacy of using synthetic data for VLM training, we conducted additional experiments where the entire dataset including captions and their corresponding images are generated synthetically by an LLM and the image generator (see Section 3.3 for the explanation of how these caption where generate and Figure 2 for some examples).\n' +
      '\n' +
      'Comparing the first two rows in Table 3 (baseline vs. GenPair) shows that adding even a small fraction (1M) of purely synthetic image-caption data (GenPair) significantly improves performance (17.2% increase in CIDEr score on MS-COCO compared to the baseline model trained only on real data). Interestingly, the same improvement is not achieved by sampling an additional 1M data points from real datasets like WebLI or LTIP (rows 3 and 4). To investigate this, we assessed the semantic diversity and balance of each caption set. First, we used our language model to generate embeddings for the captions. Then, we employed k-means clustering to analyze these embeddings (see Appendix A.3 for details). Figure 5 visualizes the cluster distribution, with the x-axis representing the cluster index and the y-axis representing data volume from each dataset within a cluster. GenPair shows a more even distribution across clusters, indicating greater conceptual diversity. It also exhibits less variance in cluster size, suggesting better balance.\n' +
      '\n' +
      'Finally, to quantify the semantic concentration within each dataset, Table 3 presents the "Concentration" metric: the percentage of data belonging to the top-5 most populated clusters. GenPair has the lowest concentration, with only 57.5% of its captions in the top-5 clusters. This indicates lower semantic imbalance in the synthetically generated GenPair and contrasts with the higher imbalance found in the other datasets (69.9% and 83%). Also, the entropy over the full cluster distribution, confirms that GenPair has a more uniform distribution of data across clusters. We postulate that the inherent diversity within GenPair likely contributes to its robust performance on MS-COCO, as models trained on diverse data tend to generalize better. (See Appendix A.3 for extra details on how the cluster analysis was performed and for extra results.)\n' +
      '\n' +
      'This evaluation showcases Synth\\({}^{2}\\)\'s ability to utilize diverse text-based data, including purely synthetic ones. Overall, our experiments highlight the significant role that synthetic data can play in enhancing VLM performance by leveraging synthetic image generation and LLM-generated captions. This approach not only overcomes data limitations but also demonstrates the effectiveness of text-only captions in boosting model capabilities, further emphasizing the value of our technique.\n' +
      '\n' +
      '#### 4.2.3 Comparison with related work\n' +
      '\n' +
      'Furthermore, we investigated the potential of our approach compared to the state-of-the-art model while taking into\n' +
      '\n' +
      'Figure 5: Semantic diversity. Histogram represent the distribution of cluster sizes, with GenPair showing a more uniform coverage of semantic concepts. See A.3 for details on how the histogram was derived.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Real & Synth & \\#Real Data & \\#Synth Data & Concentration (\\(\\downarrow\\)) & Entropy (\\(\\uparrow\\)) & CIDEr-COCO (\\(\\uparrow\\)) \\\\ \\hline CCv2 & - & 10.1M & - & - & - & 22.1 \\\\ CCv2 & GenPair & 10.1M & 1M & **57.7\\%** & **3.81** & **25.9** \\\\ CCv2+WebLI & - & 10.1M+1M & - & 69.8\\% & 3.43 & 24.4 \\\\ CCv2+LTIP & - & 10.1M+1M & - & 83.0\\% & 2.92 & 23.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Zero shot image captioning results when using synthetically generated caption and image embedding pairs. Concentration is calculated as the cumulative distribution on the top-5 clusters, a lower value represent higher diversity (see Appendix A.3 for more details).\n' +
      '\n' +
      'account the amount of synthetic or real training data and the number of model parameters. We combined the two datasets with the highest diversity, namely, GenPair and WebLI. Table 4 compares our model\'s MS-COCO benchmark performance against related methods: IITIT (Li et al., 2023b) and DC (Li et al., 2023c). Note that, for consistency with these methods, we used Synth\\({}^{2}\\) finetuned on the MS-COCO training set (see A.2). IITIT employs cycle consistency for VLM improvement, while DC uses Stable Diffusion to generate images based on human-defined heuristics.\n' +
      '\n' +
      'Synth\\({}^{2}\\) and IITIT share similar parameter counts and paired data usage, making them directly comparable. As shown in Table 4, Synth\\({}^{2}\\) drastically outperforms IITIT on the CIDEr score. While DC achieves the highest raw performance, it requires significantly more parameters, relies on vast amount of real data, and the filtering procedure is heavily based on human heuristics.\n' +
      '\n' +
      'Synth\\({}^{2}\\) strikes a balance between performance and efficiency. By leveraging text-only data, it achieves competitive results using 40 times less paired data than DC. This comparison highlights the trade-offs between performance, efficiency, and data requirements. Synth\\({}^{2}\\) emerges as a compelling solution when resources are limited, achieving impressive results with reduced data and computational overhead.\n' +
      '\n' +
      '#### 4.2.4 Data efficiency and augmentation with fully synthetic data\n' +
      '\n' +
      'To further analyze the boost in performance afforded by augmentation with fully synthetic data, we characterized the performance of the baseline model (i.e. trained solely on CCv2), and Synth\\({}^{2}\\) which was additionally trained together with GenPair. Figure 6 visualizes the performance trends for both models. Notably, the light blue curve representing the baseline exhibits a gradual increase in performance with increasing training steps, eventually plateauing at 22.1 CIDER score. In contrast, the purple curve reveals Synth\\({}^{2}\\)\'s steeper performance improvement, achieving comparable performance to the paired only training regime, with roughly 1/3 of the training steps. This highlights the significant data efficiency gain achieved through fully synthetic data augmentation in Synth\\({}^{2}\\). Shaded regions surrounding each curve indicate standard deviation across three random seeds, demonstrating the robustness of the observed performance trends. These findings demonstrate Synth\\({}^{2}\\)\'s ability to effectively leverage fully synthetic data, resulting in remarkable performance gains.\n' +
      '\n' +
      'Our study also investigated the computational efficiency of Synth\\({}^{2}\\). We compared the case where captions are used to generate image embedding versus when captions are rendered into actual images. As shown in Table 5, Synth\\({}^{2}\\) trained using image embedding consistently demonstrates faster performance, running at 2.08 training steps per second, whereas Synth\\({}^{2}\\) trained from pixel runs at a slower pace of 1.66 steps/second. This performance advantage stems from Synth\\({}^{2}\\)\'s efficient utilization of text-only data and embedding-based image generation using parallel decoding, which reduces the computational overhead associated with processing pixel-based images as done in the Synth\\({}^{2}\\) model trained from pixels without affecting the performance.\n' +
      '\n' +
      '## 5 Limitations\n' +
      '\n' +
      'The three main limitations of the current work stem from the restricted quantity of fully synthetic data used, potential biases in the generative model and the limited exploration of text data sources. Firstly, whilst we were able to show a substantial gain when training was augmented with fully synthetic data, our experiments were limited to a relatively low quantity of data (i.e. 1M image-caption pairs). In the future, it will be important to examine whether using orders of magnitude more fully synthetic data (e.g. \\(\\sim\\)700M) will\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & \\(\\#\\)All Params\\(\\downarrow\\) & \\(\\#\\)Real Data\\(\\downarrow\\) & \\(\\#\\)Synth Data\\(\\downarrow\\) & CIDEr\\(\\uparrow\\) (FT) \\\\ \\hline IITIT & 11.2B & **3M** & **110M** & 103.5 \\\\ DC & 1.7B & 5.5B & 400M & **133.1** \\\\ Synth\\({}^{2}\\) & **632M** & 10.1M & 360M & 126.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Comparison with related work on COCO. The image-captioning performance is evaluated on the COCO Karpathy split. Bold indicates the best and underline the second best values.\n' +
      '\n' +
      'Figure 6: Performance as a function of training steps. The blue curve shows the baseline trained solely on paired data (CCv2). The purple curve demonstrates Synth\\({}^{2}\\)â€™s performance trained additionally on augmentation with fully synthetic data (GenPair). Synth\\({}^{2}\\) achieves parity with the baseline using roughly 1/3 of the training steps, showcasing its superior efficiency. Shaded regions represent standard deviation across 3 random seeds.\n' +
      '\n' +
      'result in performance gains that surpass that of using solely paired data. Secondly, the generative model responsible for text-to-image generation can introduce biases into the synthetic data, reflecting limitations inherent in its training data, architecture, or procedure. A possible future step would involve investigating this issue using various text-to-image models to assess and mitigate potential biases. Thirdly, the current work primarily explores a restricted set of text-only data sources. Further research is necessary to investigate the potential of diverse text sources, including investigating context-specific generation (e.g., medical data), which could be an exciting avenue for future exploration.\n' +
      '\n' +
      'By acknowledging and addressing these limitations, future research can leverage the strengths of the Synth\\({}^{2}\\) approach and enhance its generalizability, robustness, and efficiency for a broader range of visual-language modeling tasks.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      'Our study presents a novel approach for generating synthetic image-text pairs to enhance the training of visual language models. By harnessing the capabilities of large language models and text-to-image generation, we effectively address the limitations of manual image labeling. Our results conclusively demonstrate several significant findings:\n' +
      '\n' +
      '* **Improved VLM Performance:** The visual language model trained on our synthetic and human annotated datasets exhibits a marked improvement in image captioning tasks compared to a baseline trained exclusively on human-annotated data. This underscores the potential of our method to augment VLM capabilities efficiently. This is highly advantageous in scenarios where data acquisition and annotation are resource-intensive.\n' +
      '* **Data Efficiency:** Our approach yields comparable performance while utilizing only a fraction of human-labeled data.\n' +
      '* **Customization and Scalability:** Our method provides flexibility in generating customized image datasets tailored to specific domains. Additionally, the synthetic data generation process is scalable, making it suitable for supporting large-scale VLM development.\n' +
      '\n' +
      'This research introduces a novel technique that has the potential to transform visual language model training. Our findings highlight the value of synthetic data generation, paving the way for advancements in numerous fields where visual language understanding is crucial. Further explorations could investigate refining the synthetic data generation process with different objectives in mind.\n' +
      '\n' +
      '## 7 Broader Impact\n' +
      '\n' +
      'This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. In particular biases in generative models can have a drastic effect on the synthetic data. Similarly it is important to consider the challenges of privacy when using generative models.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We would like to thank Sander Dieleman, Ali Razavi, and Benigno Uria for their insights on VQGAN. We would also like to thank Aida Nematzadeh, Pinelopi Papalampidi, Han Zhang, Mateusz Malinowski, Valentin De Bortoli, Sahra Ghalebikesabi, Emanuele Bugliarello, Chris Knutsen, and Murray Shanahan for their in depth comments throughout the project.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. _Advances in Neural Information Processing Systems_, 35:23716-23736, 2022.\n' +
      '* Azizi et al. (2023) Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D. J. Synthetic data from diffusion models improves imagenet classification. _arXiv preprint arXiv:2304.08466_, 2023.\n' +
      '* Baranchuk et al. (2021) Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V., and Babenko, A. Label-efficient semantic segmentation with diffusion models. _arXiv preprint arXiv:2112.03126_, 2021.\n' +
      '* Cascante-Bonilla et al. (2023) Cascante-Bonilla, P., Shehada, K., Smith, J. S., Doveh, S., Kim, D., Panda, R., Varol, G., Oliva, A., Ordonez, V., Feris, R., et al. Going beyond nouns with vision & language models using synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 20155-20165, 2023.\n' +
      '* Chang et al. (2017) Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K., Freeman, W. T.,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Step/Sec\\(\\uparrow\\) & CIDEr(zs)\\(\\uparrow\\) \\\\ \\hline Synth\\({}^{2}\\)- Image Embedding & **2.08** & 25.9 \\\\ Synth\\({}^{2}\\)- Pixels & 1.66 & 26.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Model efficiency measured by computing the steps per second of training on the same hardware. Synth\\({}^{2}\\) in embedding space consistently outperforms the Synth\\({}^{2}\\) using pixels, demonstrating superior training efficiency.\n' +
      '\n' +
      'Rubinstein, M., et al. Muse: Text-to-image generation via masked generative transformers. _arXiv preprint arXiv:2301.00704_, 2023.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3558-3568, 2021.\n' +
      '* Chen et al. (2020a) Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pp. 1597-1607. PMLR, 2020a.\n' +
      '* Chen et al. (2020b) Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. _Advances in neural information processing systems_, 33:22243-22255, 2020b.\n' +
      '* Chen et al. (2015) Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.\n' +
      '* Chen et al. (2022) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. _arXiv preprint arXiv:2209.06794_, 2022.\n' +
      '* Chen et al. (2019) Chen, Y., Li, W., Chen, X., and Gool, L. V. Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 1841-1850, 2019.\n' +
      '* de Melo et al. (2022) de Melo, C. M., Torralba, A., Guibas, L., DiCarlo, J., Chellappa, R., and Hodgins, J. Next-generation deep learning based on simulators and synthetic data. _Trends in cognitive sciences_, 2022.\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* Esser et al. (2021) Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 12873-12883, 2021.\n' +
      '* Fan et al. (2023) Fan, L., Chen, K., Krishnan, D., Katabi, D., Isola, P., and Tian, Y. Scaling laws of synthetic images for model training... for now. _arXiv preprint arXiv:2312.04567_, 2023.\n' +
      '* Greff et al. (2022) Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D. J., Gnanapragasam, D., Golemo, F., Herrmann, C., et al. Kubric: A scalable dataset generator. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3749-3761, 2022.\n' +
      '* Grill et al. (2020) Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.\n' +
      '* Guo et al. (2022) Guo, X., Wu, W., Wang, D., Su, J., Su, H., Gan, W., Huang, J., and Yang, Q. Learning video representations of human motion from synthetic data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 20197-20207, 2022.\n' +
      '* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Hu et al. (2022) Hu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and Wang, L. Scaling up vision-language pre-training for image captioning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 17980-17989, 2022.\n' +
      '* Jaegle et al. (2021) Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pp. 4651-4664. PMLR, 2021.\n' +
      '* Karpathy & Fei-Fei (2015) Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3128-3137, 2015.\n' +
      '* Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Li et al. (2021) Li, D., Yang, J., Kreis, K., Torralba, A., and Fidler, S. Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8300-8311, 2021.\n' +
      '* Li et al. (2021)Li, D., Ling, H., Kim, S. W., Kreis, K., Fidler, S., and Torralba, A. Bigdataset: Synthesizing imagenet with pixel-wise annotations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 21330-21340, 2022a.\n' +
      '* Li et al. [2022a] Li, H., Gu, J., Koner, R., Sharifzadeh, S., and Tresp, V. Do dall-e and flamingo understand each other? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 1999-2010, 2023a.\n' +
      '* Li et al. [2022b] Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pp. 12888-12900. PMLR, 2022b.\n' +
      '* Li et al. [2023b] Li, T., Bhardwaj, S., Tian, Y., Zhang, H., Barber, J., Katabi, D., Lajoie, G., Chang, H., and Krishnan, D. Leveraging unpaired data for vision-language generative models via cycle consistency. _arXiv preprint arXiv:2310.03734_, 2023b.\n' +
      '* Li et al. [2023c] Li, W., Lotz, J. F., Qiu, C., and Elliott, D. Data curation for image captioning with text-to-image generative models. _arXiv preprint arXiv:2305.03610_, 2023c.\n' +
      '* Loshchilov and Hutter [2017] Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Ma et al. [2022] Ma, J., Bai, S., and Zhou, C. Pretrained diffusion models for unified human motion synthesis. _arXiv preprint arXiv:2212.02837_, 2022.\n' +
      '* Mishra et al. [2022] Mishra, S., Panda, R., Phoo, C. P., Chen, C.-F. R., Karlinsky, L., Saenko, K., Saligrama, V., and Feris, R. S. Task2sim: Towards effective pre-training and transfer from synthetic data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 9194-9204, 2022.\n' +
      '* Plummer et al. [2015] Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pp. 2641-2649, 2015.\n' +
      '* Radford et al. [2021] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ridnik et al. [2021] Ridnik, T., Ben-Baruch, E., Noy, A., and Zelnik-Manor, L. Imagenet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.\n' +
      '* Ros et al. [2016] Ros, G., Sellart, L., Materzynska, J., Vazquez, D., and Lopez, A. M. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3234-3243, 2016.\n' +
      '* Schuhmann et al. [2021] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Sharifzadeh et al. [2021] Sharifzadeh, S., Baharlou, S. M., and Tresp, V. Classification by attention: Scene graph classification with prior knowledge. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 5025-5033, 2021.\n' +
      '* Sharifzadeh et al. [2022] Sharifzadeh, S., Baharlou, S. M., Schmitt, M., Schutze, H., and Tresp, V. Improving scene graph classification by exploiting knowledge from texts. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pp. 2189-2197, 2022.\n' +
      '* Team et al. [2023] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tritrong et al. [2021] Tritrong, N., Rewatbowornwong, P., and Suwajanakorn, S. Repurposing gans for one-shot semantic part segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 4475-4485, 2021.\n' +
      '* Tsimpoukelli et al. [2021] Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. _Advances in Neural Information Processing Systems_, 34:200-212, 2021.\n' +
      '* Varol et al. [2017] Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., and Schmid, C. Learning from synthetic humans. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 109-117, 2017.\n' +
      '* Vedantam et al. [2015] Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 4566-4575, 2015.\n' +
      '* Zheng et al. [2020] Zheng, J., Zhang, J., Li, J., Tang, R., Gao, S., and Zhou, Z. Structured3d: A large photo-realistic dataset for structured 3d modeling. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pp. 519-535. Springer, 2020.\n' +
      '\n' +
      'Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pp. 2223-2232, 2017.\n' +
      '\n' +
      '## Appendix A Appendix.\n' +
      '\n' +
      '### Extra Implementation details\n' +
      '\n' +
      '#### a.1.1 Vq-Gan details\n' +
      '\n' +
      'VQGAN Architecture: Our VQGAN architecture is similar to the previous work (Esser et al., 2021). It consists of several residual blocks, downsample (encoder) and upsample (decoder) blocks. The main difference is that we remove the non-local block to make the encoder and decoder fully convolutional to support different image sizes. In the base VQGAN model, we apply 2 residual blocks in each resolution and the base channel dimension is 128. For the finetuned decoder, we apply 4 residual blocks in each resolution and we also make the base channel dimension to be 256.\n' +
      '\n' +
      '#### a.1.2 Optimization details for Synth\\({}^{2}\\)\n' +
      '\n' +
      '#### a.1.3 Text to image generator details\n' +
      '\n' +
      'Config param & Value\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} Config param & Value \\\\ \\hline Perceptual loss weight & 0.05 \\\\ Adversarial loss weight & 0.01 \\\\ Codebook size & 8192 \\\\ Optimizer & Adam (Kingma \\& Ba, 2014) \\\\ Discriminator learning rate & 1e-4 \\\\ Generator learning rate & 1e-4 \\\\ Optimizer momentum & \\(\\beta_{1}\\)=0.9 \\(\\beta_{2}\\)=0.99 \\\\ Batch Size & 256 \\\\ Learning rate schedule & Cosine \\\\ Decay (Loshchilov \\& Hutter, 2017) Warmup steps & 10000 \\\\ Training steps & 500000 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Configuration and training hyperparameters for VQGAN.\n' +
      '\n' +
      '### Finetuning details\n' +
      '\n' +
      'Following the pre-training stage, our model undergoes fine-tuning for various downstream tasks. The fine-tuning process employs the AdamW optimizer with similar Beta values as in pre-training. The learning rate was 1e-5.\n' +
      '\n' +
      'To enhance generalization capabilities during fine-tuning, we utilize regularization methods such as Dropout (set to 0.1).\n' +
      '\n' +
      'In accordance with established practices, we use the development split to determine optimal fine-tuning settings. The performance results are then reported based on the test split.\n' +
      '\n' +
      '### Clustering details\n' +
      '\n' +
      'To determine the optimal number of clusters for our analysis, we employed the Elbow Method in conjunction with the Within-Cluster Sum of Squares (WCSS) metric. We iteratively applied the K-means clustering algorithm while varying the number of clusters, calculating the WCSS for each iteration. A clear \'elbow\' point was observed in the WCSS plot, indicating a substantial decrease in variance as the number of clusters increased up to 20. Beyond this point, further increases in the number of clusters yielded diminishing returns in terms of WCSS reduction. Based on this analysis, we determined that 20 clusters provided a suitable balance between parsimony and capturing the underlying structure within our dataset.\n' +
      '\n' +
      'To analyze the diversity of cluster compositions, we employed a co-clustering approach with the K-means algorithm on the concatenated GenPair, WebLI, and LTIP datasets. For each of the resulting clusters, we calculated the normalized cluster sizes for each individual dataset. This allowed us to visualize the distribution among the datasets within each cluster, as illustrated in Figure 5.\n' +
      '\n' +
      'To calculate the concentration we performed a cumulative sum over the top-5 clusters to determine what percentage of the data points was present there. The idea is that, the higher the percentage in the top 5 clusters, the less uniform is the distribution across all clusters, potentially indicating a lower level of diversity. Here in Table 8 we also report top-3 and the entropy over the full 20 clusters. Both measures confirm a more uniform distribution of data across clusters for GenPair versus the WebLi and LTIP. Especially the entropy complement the other measures as it takes into account all clusters, even if they are smaller.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} Config param & Value \\\\ \\hline Num Layers & 6 \\\\ Num Heads & 16 \\\\ Embedding hidden dim & 1024 \\\\ Activation & GeLU \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Perceiver details\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline Real & Synth & \\#Real Data & \\#Synth Data & CIDEr-COCO (\\(\\uparrow\\)) & Concentration Top-3 (\\(\\downarrow\\)) & Concentration Top-5 (\\(\\downarrow\\)) & Entropy (\\(\\uparrow\\)) \\\\ \\hline CCv2+WebLI & - & 10.1M+1M & - & 24.4 & 58.2\\% & 69.8\\% & 3.43 \\\\ CCv2+LTIP & - & 10.1M+1M & - & 23.4 & 76.5\\% & 83.0\\% & 2.92 \\\\ CCv2 & GenPair & 10.1M & 1M & **25.9** & **40.5\\%** & **57.7\\%** & **3.81** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Zero shot image captioning results when using synthetically generated caption and image embedding pairs.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>