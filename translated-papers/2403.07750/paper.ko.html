<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Synth\\({}^{2}\\): Visual-Language Models을 Boosting with\n' +
      '\n' +
      '합성 캡션과 이미지 임베딩\n' +
      '\n' +
      ' 사하와 샤리프자데\n' +
      '\n' +
      '영국 런던의 구글 딥마인드도 같은 기여를 했다. 대응: Sahand Sharifzadeh \\(<\\)sharifzadeh@google.com\\(>\\), Andrea Banino \\(<\\)abanino@google.com\\(>\\).\n' +
      '\n' +
      ' 크리스토스 카플라니스\n' +
      '\n' +
      'Shreya Pathak\n' +
      '\n' +
      'Dharshan Kumaran\n' +
      '\n' +
      'Anastasija Ilic\n' +
      '\n' +
      'Jovana Mitrovic\n' +
      '\n' +
      'Charles Blundell\n' +
      '\n' +
      'Andrea Banino\n' +
      '\n' +
      '영국 런던의 구글 딥마인드도 같은 기여를 했다. 대응: Sahand Sharifzadeh \\(<\\)sharifzadeh@google.com\\(>\\), Andrea Banino \\(<\\)abanino@google.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '고품질 인간 라벨 이미지 캡션 데이터 세트의 생성은 시각적 언어 모델(VLMs) 개발에 상당한 병목 현상을 나타낸다. 효율적이고 효과적인 VLM 훈련을 위해 합성 이미지-텍스트 쌍을 생성하기 위해 LLM(Large Language Models)과 이미지 생성 모델의 장점을 활용하는 새로운 접근법을 제안한다. 제안된 방법은 LLM에 의해 생성된 캡션으로부터 시작하는 이미지 임베딩을 합성하기 위해 텍스트-이미지 모델을 사전 트레이닝한다. 그런 다음 이러한 합성 쌍은 VLM을 훈련하는 데 사용된다. 광범위한 실험은 합성 데이터로 훈련된 VLM이 이미지 캡션에서 유사한 성능을 나타내는 반면 인간 주석 데이터에만 훈련된 모델이 사용하는 데이터의 일부를 필요로 한다는 것을 보여준다. 특히 합성 데이터 세트를 사용한 증강을 통해 기준선을 17% 능가한다. 또한, 영상 임베딩 공간에서의 합성 속도가 픽셀 공간에서의 합성 속도보다 25% 빠르다는 것을 보인다. 본 연구는 대규모의 사용자 정의 가능한 이미지 데이터 세트를 생성하는 유망한 기술을 도입하여 다양한 도메인에 걸쳐 향상된 VLM 성능과 광범위한 적용 가능성을 유도하고 데이터 효율성과 리소스 활용도를 향상시킨다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '시각 언어 모델(VLM)은 시각 및 텍스트 정보를 이해하기 위한 강력한 도구로 빠르게 부상하고 있다. 이 두 가지 양식을 결합하는 능력은 이미지 캡션에서 시각적 질문 응답에 이르기까지 응용 분야에 대한 엄청난 가능성을 가지고 있다. VLM은 상당한 잠재력을 가지고 있지만, 그들의 성능은 종종 제한된 데이터 가용성에 의해 제한된다. 최근의 돌파구는 더 큰 이미지-텍스트 쌍 데이터세트 상에서 VLM을 사전 트레이닝하는 것이 다운스트림 태스크(Li 등, 2022; Hu 등, 2022)에서 상당한 개선으로 이어진다는 것을 입증한다. 그러나, 이러한 데이터 세트를 생성하는 것은 페어링된 데이터의 희소성, 높은 큐레이션 비용, 낮은 다양성 및 의미론의 높은 불균형, 및 인터넷으로부터 소싱된 데이터의 잠재적으로 시끄러운 성질(예를 들어, LAION(Schuhmann et al., 2021))과 같은 몇 가지 과제를 제기한다. 이러한 요인들은 종종 낮은 신호 대 잡음비로 인해 힘든 필터링 및 연장된 훈련 시간으로 이어지며, 따라서 전체 자원 소비를 증가시킨다.\n' +
      '\n' +
      '이 작업은 VLM에 대한 합성 쌍을 이루는 데이터를 효율적으로 생성하기 위해 사전 훈련된 생성 텍스트 및 이미지 모델을 활용하는 새로운 접근법을 도입함으로써 이러한 한계를 해결한다. 우리의 접근법은 실제 데이터에 대한 의존성을 극복하고 희소성, 비용 및 노이즈의 문제를 해결하면서 텍스트와 이미지를 합성적으로 모두 독특하게 생성한다.\n' +
      '\n' +
      '도 1: 전통적인 데이터세트 큐레이션 파이프라인은 이미지(A)를 수집하고 주석을 달기 위해 루프 내의 사람이 필요하다. 우리는 생성 모델로 이 파이프라인을 역전시킬 수 있는지, 즉 LLM에서 합성 캡션을 먼저 샘플링한 다음 그(B)에서 이미지를 합성적으로 생성할 수 있는지 연구한다. 영상 임베딩 공간에서 동작함으로써, 계산 비용이 많이 드는 인코더/디코더 단계를 우회하여, 시각 언어 모델(VLM) 트레이닝(C) 내에서 프로세스를 최적화하고 통합한다.\n' +
      '\n' +
      '합성 데이터 생성은 이미지 분할, 광학 흐름 예측 또는 이미지 분류와 같은 다양한 컴퓨터 비전 작업에 대해 탐색되었지만(Mishra et al., 2022; Greff et al., 2022; Azizi et al., 2023; Fan et al., 2023; Li et al., 2023c), VLM 훈련 내의 시각적 및 텍스트적 양식 모두에 대한 적용은 상당한 진보이다.\n' +
      '\n' +
      '또한, 기성 이미지 생성 모델을 사용하는 대신 제어 이미지 생성 접근법을 사용한다. 즉, 이미지 생성기에서 VLM으로의 의도하지 않은 지식 전달을 방지하기 위해 전용 데이터 세트(VLM 훈련에도 사용)에서 이미지 생성기를 사전 훈련했다. 이 전략은 합성 데이터의 영향에 대한 공정한 평가를 보장한다.\n' +
      '\n' +
      '마지막으로, 제안된 프레임워크는 임베딩 레벨에서도 동작할 수 있다. 텍스트-이미지 모델은 픽셀-공간 렌더링을 우회하여 VLM에 직접 공급되는 이미지 임베딩을 생성한다. 이러한 패러다임의 변화는 합성 학습 데이터의 품질을 유지하면서 메모리 오버헤드와 리소스 소비를 극적으로 감소시킨다.\n' +
      '\n' +
      '요약하자면, 본 연구는 VLM과 텍스트-이미지 생성 모델 간의 시너지 효과를 탐색하여 데이터 한계를 극복하기 위한 강력한 프레임워크를 보여준다. 본 논문은 다음과 같은 주요 기여도를 제시한다.\n' +
      '\n' +
      '***완전 합성 데이터 생성:** 고품질 텍스트 이미지 임베딩 쌍의 완전 합성 데이터 세트를 활용하는 첫 번째 VLM 트레이닝 프로세스를 소개합니다. 이 쌍은 미리 훈련된 생성 모델에 의해 생성되어 양식에 대한 방대한 양의 실제 데이터가 필요하지 않다.\n' +
      '**효율적인 임베딩 공간 생성:** 우리의 방법은 공간을 임베딩하는 이미지들과 함께 작동하며, 비용이 많이 드는 이미지 렌더링을 피하고 성능을 포함하지 않고 효율성을 획기적으로 향상시킨다.\n' +
      '컨트롤을 통한 공정한 평가:** VLM 훈련에 사용된 동일한 데이터 세트에서 텍스트 대 이미지 모델을 사전 훈련함으로써, 대형 기성 모델을 사용하는 대신, 방대한 외부 이미지 대 텍스트 쌍에 대해 훈련된 모델로부터 지식 전달을 방지한다. 이것은 격리된 합성 데이터의 이점에 대한 공정하고 집중적인 평가를 촉진한다.\n' +
      '**실증 효과:** 실험은 실제 및 합성 데이터 기준선에 대해 평가된 합성 데이터를 사용할 때 이미지 캡션에서 상당한 성능 향상을 보여줍니다.\n' +
      '\n' +
      '결론적으로, 우리는 텍스트 기반 접근법을 사용하여 맞춤형 및 잠재적으로 무한 확장 가능한 데이터 세트를 생성할 가능성을 강조하면서 VLM 교육의 미래에 대한 통찰력을 제공한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '주로 웹에서 조달되는 대규모 이미지 텍스트 데이터를 사전 훈련한 VLMs 비쥬얼 언어 모델(VLMs)은 수많은 비쥬얼 언어 작업에서 놀라운 능력을 보여주었다. 이러한 작업들은 이미지 캡션, 시각적 질문 응답, 및 소수 샷 학습(Alayrac et al., 2022; Chen et al., 2022; Li et al., 2022b)을 포함한다. 구조적으로 VLM은 일반적으로 LLM(Large Language Model)과 결합된 이미지 인코더를 사용한다. 연구자들은 합성곱 신경망(CNNs)(Alayrac et al., 2022)에서 변압기 기반 아키텍처(Chen et al., 2022)에 이르기까지 다양한 이미지 인코더 설계를 탐구한다. 또한, 별도의 이미지 데이터세트(Alayrac et al., 2022)에서 이미지 인코더를 사전 훈련하는 것과 VLM(Tsimpoukelli et al., 2021)과 함께 처음부터 훈련하는 것 사이의 선택은 여전히 활발한 조사 영역으로 남아 있다. 사전-트레이닝 VLM은 지도 학습 접근법들뿐만 아니라 이미지 및 텍스트 표현들을 정렬하는 것(Radford et al., 2021) 또는 상이한 이미지 표현들을 정렬하는 것(Chen et al., 2020b;a; Grill et al., 2020)에 초점을 맞추는 대조적 방법들을 포함하는 다양한 목적들을 포함한다. LLM 선택에는 유사한 다양성이 존재하며, 인코더-디코더(Chen et al., 2022) 및 디코더 전용(Alayrac et al., 2022) 아키텍처가 모두 활용된다. 이미지 인코더와 언어 모델에 의해 추출된 정보를 결합하는 전략도 상당한 변화를 나타낸다. 웹 스크래핑 데이터 세트를 사용하여 성공적으로 달성했음에도 불구하고, 합성적으로 생성된 데이터를 사용하여 훈련을 용이하게 하는 VLM 아키텍처를 설계하는 것이 점점 더 강조되고 있다. ♪ 이 캐 ♪\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|c|c|c|c|c|c|c} \\hline \\hline Generator & Method & Generator Model & Generator Range & Generated Pairs & Caption Class & Caption Type & Evaluation Setting \\\\ \\hline \\multirow{4}{*}{Simulation/Rendering Engines} & Mishra et al. (2022), & & & Mixed pairs such as & & & \\multirow{4}{*}{N/A} & \\multirow{4}{*}{Mix} \\\\  & Greff et al. (2022), & & & & (Segmentation, Images), & & & \\\\  & Zheng et al. (2020) & Mix & Mix & (Optimal Flow, Videos), & & & & \\\\  & de Melo et al. (2022) & & & (Depth Map, Images) & & & & \\\\ \\hline \\multirow{4}{*}{Off-the-shelf Image Generator} & Cascante-Bonilla et al. (2022) & Mix & Pixel & (Captions, Images) & Complex & Rule-based & Vision Encoder \\\\ \\cline{2-8}  & Azizi et al. (2023), & SD, & & & & & & \\\\ \\cline{1-1}  & Fan et al. (2023) & ImageNet, & Pixel & & (Class, Images) & Simple & ImageNet Classes & Classific \\\\ \\cline{1-1}  & Li et al. (2023c) & SD & Pixel & (Captions, Images) & Complex & Human Generated & VLM \\\\ \\hline \\multirow{2}{*}{Controlled Image Generator} & \\multirow{2}{*}{Synth\\({}^{2}\\)} & \\multirow{2}{*}{MUSE} & \\multirow{2}{*}{Embedding} & (Captions, Images) & Complex & Human Generated & VLM \\\\ \\cline{1-1} \\cline{4- pability holds particular significance for applications where data scarcity or resources availability pose significant challenges.\n' +
      '\n' +
      '합성 데이터로부터 학습 기계 학습 모델을 학습하기 위한 합성 데이터의 생성은 매우 활발한 연구 영역으로 남아 있다. 많은 연구(Mishra et al., 2022; Cascante-Bonilla et al., 2023; Greff et al., 2022; Zheng et al., 2020; de Melo et al., 2022)가 모델 기반 렌더링 엔진 또는 시뮬레이터의 사용을 탐구하는 동안, 고품질 이미지 생성기의 놀라운 발전은 훈련 데이터를 생성하기 위한 생성 모델을 활용하는 데 관심이 급증했다. 이러한 경향은 시맨틱 세분화(Li et al., 2022; Ros et al., 2016; Baranchuk et al., 2021; Tritrong et al., 2021; Li et al., 2021; Chen et al., 2019), 인간 동작 이해(Varol et al., 2017; Ma et al., 2022; Guo et al., 2022), 및 보다 최근에 이미지 분류(Azizi et al., 2023; Fan et al., 2023)를 포함하는 광범위한 컴퓨터 비전 작업에 상당한 영향을 미쳤다. 본 연구는 시각 언어 모델(VLM)의 도메인 내에서 데이터 기반 생성 모델의 잠재력을 조사한다. 우리는 복잡한 장면 설명을 처리하는 능력이 중요한 이미지 캡셔닝과 같은 다운스트림 작업에서 그들의 적용에 초점을 맞춘다. 이러한 측면에서, 우리의 접근법은 Li 등(2023c)에 의한 동시 작업에 가장 가깝고, 여기서 목표는 캡션 파이프라인의 결함 이미지를 그들의 합성 버전으로 대체하는 것이다. 그러나 본 연구는 40배 적은 페어드 데이터와 1/4의 매개변수만을 활용하면서 강력한 성능을 달성함으로써 Li 등(2023c)과 구별된다. 이는 제안된 접근 방식을 통해 성능과 효율성을 모두 달성할 수 있는 가능성을 보여준다. 또한, 언급된 작업들(Azizi et al. (2023); Fan et al. (2023); Li et al. (2023c)과 달리, 우리는 더 큰 데이터 세트들에 대해 트레이닝된 오프-더-셀프 이미지 생성기를 사용하는 것을 피한다. 우리의 작업은 또한 시각적 언어 모델에서 주기 일관성 개념(Zhu et al., 2017)에 비유될 수 있다. 이미지-텍스트 및 텍스트-이미지 변환이 합성 데이터 훈련의 한 형태로 사용될 수 있는 이 원리는 네트워크 전반에 걸쳐 추가 기울기가 있지만 샘플링 중 이미지-텍스트 주기 일관성을 탐색하는 Li 등(2023a) 또는 훈련 중 Li 등(2023b)이 최근 연구에서 탐색되어 유망한 결과를 보여준다.\n' +
      '\n' +
      '마지막으로 데이터 생성과 모델 학습을 원활하게 연결하는 파이프라인의 효율성을 강조한다. 대부분의 선행 연구는 픽셀 공간에서 이미지를 생성하는 것에 집중되어 있지만, 우리는 VLM에 직접 통합될 수 있는 이미지 임베딩의 생성을 조사한다. 우리의 접근법은 장면 그래프 분류 도메인(Sharifzadeh et al., 2021, 2022)의 최근 작업과 일치하며, 이는 사전 훈련된 객체 표현을 사용하여 합성 장면 그래프 임베딩 생성을 탐색했다.\n' +
      '\n' +
      '표 1은 차이점을 더 잘 명확히 하기 위해 언급된 관련 작업의 분류를 제공한다. 도시된 바와 같이, 본 연구는 VLM 트레이닝 파이프라인 내에서 그리고 특히 이미지 임베딩을 사용하고 합성 캡션을 생성함으로써 합성 데이터 생성의 적용을 과학적으로 탐구한 최초의 연구이다.\n' +
      '\n' +
      '## 3 Synth\\({}^{2}\\)\n' +
      '\n' +
      '텍스트와 이미지의 합성 생성을 고려하여 Synth\\({}^{2}\\)이라고 한다. Synth\\({}^{2}\\)는 수집된 인간 데이터 외에 생성 모델을 사용하여 VLM을 훈련하기 위한 파이프라인이다. 이 섹션에서는 이 파이프라인의 다양한 구성 요소, 즉 캡션 생성, 이미지 생성 및 전체 Synth\\({}^{2}\\) 모델을 소개한다.\n' +
      '\n' +
      '### Caption Generation\n' +
      '\n' +
      'LLM의 생성 기능을 활용하여 합성 캡션을 만듭니다. 다양한 캡션을 보장하기 위해 클래스 기반 프롬프트 접근 방식을 채택합니다. 먼저 ImageNet21k 데이터셋에서 임의로 클래스를 선택한다(Ridnik et al., 2021). 그 다음, LLM(Gemini Pro Team et al., 2023))이 다음의 텍스트로 프롬프트된다:\n' +
      '\n' +
      '_다음과 같은 객체를 포함하는 이미지에 대한 인간 주석이 부여된 설명을 구성한다. \\([object]\\). 캡션의 길이는 약 30~40 단어여야 합니다. 장면의 다른 구성 요소를 객관적이고 편견 없는 방식으로 묘사하는 것. 이미지에 대한 주관적인 판단을 추가하지 말고 가능한 한 사실적이어야 한다. 폭신폭신하고 시적인 언어를 사용하지 마세요. "This is a image of."__\n' +
      '\n' +
      '도 2: LLM 및 텍스트-이미지 생성기에 의해 생성된 합성 캡션 및 합성 이미지의 예.\n' +
      '\n' +
      '"[객체]"를 임의로 선택한 클래스로 대치합니다. 이 클래스 기반 프롬프트는 LLM이 광범위한 시각적 개념을 포함하는 캡션을 생성하도록 유도한다. 그림 2는 생성된 캡션에서 일부 샘플을 보여준다.\n' +
      '\n' +
      '### Image Generation\n' +
      '\n' +
      '광범위한 이미지 텍스트 데이터 세트에 대해 훈련된 기성 이미지 생성기 모델은 VLM 훈련에서 합성 이미지의 효과를 연구하는 데 불공정한 이점을 가지고 있다. 광범위한 인간 주석이 달린 데이터에 노출되면 원래 훈련 세트를 매우 연상시키는 이미지를 생성할 수도 있다. 공정한 평가를 보장하기 위해, 우리는 통제된 환경에서 텍스트-이미지 생성기를 사전 훈련하는 것을 제안한다. 이 생성기는 합성 데이터와 함께 VLM 훈련에도 사용될 인간 주석이 달린 이미지 캡션 데이터 세트에 대해 훈련된다. 이를 통해 외부 데이터가 VLM 훈련에 미치는 영향을 효과적으로 제거한다.\n' +
      '\n' +
      '제안된 구조는 Chang et al. (2023)과 유사하며, 이산 이미지 토큰의 사용과 병렬 디코딩 능력으로 인해 우수한 추론 효율로 선택되었으며, 이는 자동 회귀 모델에 비해 샘플링 반복 횟수를 극적으로 감소시킨다. 또한, MUSE는 변압기 아키텍처를 활용하여 효율성을 더욱 향상시킨다(Chang et al., 2023). 그림 3은 원본 이미지와 비교하여 생성된 이미지의 몇 가지 예를 보여준다.\n' +
      '\n' +
      '#### 3.2.1 Training\n' +
      '\n' +
      '페어링된 이미지와 텍스트가 주어진 이미지 생성기를 사전 학습하기 위해 사전 학습된 언어 모델과 사전 학습된 VQ-GAN(Esser et al., 2021)을 가진 이미지를 임베딩한다(자세한 내용은 부록에서 A.1.1 참조). 사전 훈련된 언어 모델은 우리의 VLM에서 사용된 것과 동일하며 4억 개의 파라미터를 사용하는 Chinchilla(Hoffmann et al., 2022)의 축소 버전이다. Chang et al.(2023)의 접근 방법에 따라, 코사인 스케줄링 기능을 갖는 마스킹 절차를 적용한다. 이 기능은 트레이닝 동안 마스킹 비율을 점진적으로 증가시켜 더 큰 마스킹 영역을 갖는 이미지로부터 학습을 향해 모델을 편향시킨다. 평균적으로 VQ 토큰의 약 64%가 특별한 "삭제된 토큰"으로 대체된다. 그런 다음 잡음이 많은 VQ 토큰과 내장된 텍스트가 변압기 모델에 공급된다. 모델의 목표는 마스킹되지 않은 원래 VQ 토큰을 예측하는 것입니다. 손실 함수는 예측된 VQ 토큰들과 마스킹된 것들 사이의 교차 엔트로피이다:\n' +
      '\n' +
      '\\[L_{t2i}=\\prod_{t\\in M}p(z(x)_{t}|y,\\{z(x)_{u}|u\\notin M\\}),\\\n' +
      '\n' +
      '여기서 \\(z(x)\\)는 Ground-truth 이미지에서 계산된 VQ 토큰 \\(x\\), \\(y\\)은 Ground-truth 캡션, \\(M\\)은 마스킹된 토큰의 인덱스를 나타낸다.\n' +
      '\n' +
      '우리의 텍스트-이미지 생성기는 Conceptual Captions V2(Changpinyo et al., 2021)에서 10.1백만 개의 텍스트-이미지 쌍에만 교육되었다(자세한 내용은 섹션 4 참조).\n' +
      '\n' +
      '#### 3.2.2 Inference\n' +
      '\n' +
      '추론하는 동안, 우리는 이미지를 생성하기 위해 반복적인 절차를 사용한다. 첫 번째 반복에서, 예측된 VQ 토큰들은 모두 마스킹된 엔트리들을 나타내는 "드롭된 토큰"으로 초기화된다. 각각의 다가오는 반복에서, 텍스트 임베딩은\n' +
      '\n' +
      '도 3: MS-COCO(Chen et al., 2015)(상단 행)의 유효성 검사 세트로부터 선택된 이미지들을 보여주는 퀄리티들 및 그라운드 트루스 캡션들(하단)이 주어진 우리의 텍스트-이미지 생성기에 의해 생성된 그들의 합성 버전들. 이미지 생성기의 트레이닝은 Conceptual Caption v2에 대해 수행되었다(Changpinyo et al., 2021).\n' +
      '\n' +
      '그리고 그 단계까지 예측된 VQ 토큰이 트랜스포머에 공급되고 모델은 마스킹된 토큰을 예측한다. 코사인 스케줄은 디코딩 동안 어떤 마스킹된 토큰들이 가장 높은 예측 신뢰도를 갖는지를 결정한다. 이러한 토큰은 마스킹되지 않으므로 마스킹된 토큰 집합을 점진적으로 줄입니다. 이 프로세스는 24번의 반복 동안 반복되어 생성된 VQ-토켄을 점진적으로 정제한다.\n' +
      '\n' +
      '텍스트 대 이미지 생성기 모델에 대한 추가 세부 정보는 부록의 A.1.3에 보고되어 있다.\n' +
      '\n' +
      '### Synth\\({}^{2}\\) VLM 아키텍처\n' +
      '\n' +
      '섹션 3.2에서 설명한 대로 텍스트 대 이미지 모델은 반복적인 캡션 조건 잡음 제거 프로세스를 통해 이미지 임베딩(VQ 토큰)을 생성한다. VQ-GAN 디코더는 이러한 토큰을 인간이 볼 수 있는 이미지로 변환하는 데 사용될 수 있다. 그러나, 우리는 이미지 임베딩 자체가 이미지의 압축된 표현을 제공하여 디코더 단계가 특정 애플리케이션에 불필요하게 만든다는 것을 강조한다.\n' +
      '\n' +
      '따라서 합성 데이터로부터 VLM을 학습하기 위한 효율성을 높이기 위해 픽셀 공간 처리를 우회할 수 있도록 VLM을 설계한다. 이는 비전 인코더가 이미지 생성기에 사용되는 VQ-GAN 백본과 동일하도록 설정함으로써 달성된다. 이것은 VLM과 합성적으로 생성된 이미지 임베딩 사이의 끊김없는 상호작용을 가능하게 하고 합성 데이터에 대한 트레이닝 시 이미지 생성기를 위한 VQ-GAN의 계산 비용이 많이 드는 디코딩 스테이지와 VLM을 위한 인코딩 스테이지를 제거한다. 동시에 인간의 주석이 달린 데이터를 사용할 때 픽셀 공간의 이미지에서 학습할 수 있습니다. 전반적으로, VQ 기반 설계는 픽셀 공간으로의 및 픽셀 공간으로의 비용이 많이 드는 변환의 필요성을 제거하여 섹션 4에서 보여지는 바와 같이 상당한 시간 및 디스크 공간 절감을 초래한다. 소프트 임베딩에 의해 제공되는 풍부한 이미지 이해도를 유지하면서, 트레이닝 및 샘플링 동안 이산 토큰의 효율성을 이용하기 위해, 각각의 이산 토큰을 코드북을 사용하여 소프트 임베딩으로 변환시킨 후 VLM에 공급한다.\n' +
      '\n' +
      'VLM에서 VQ 백본의 상단에, 우리는 Perceiver Resampler 컴포넌트(Jaegle et al., 2021)를 통합하여 백본에서 추출된 VQ 토큰에 교차 응답한다. 플라밍고(Alayrac et al., 2022)와 유사하게, 우리는 이미지 표현에 교차 참석하고 텍스트를 자동으로 생성하기 위해 냉동 언어 모델을 사용한다. 언어 모델 내의 자기-관심 레이어들이 동결된 채로 유지되는 동안, 언어와 비전 컴포넌트들 사이의 교차-관심 레이어들은 트레이닝가능하여, 모델이 효과적인 멀티모달 표현들을 학습할 수 있게 한다.\n' +
      '\n' +
      'VLM의 목적은 입력 이미지 상에서 조건화된 시퀀스에서 다음 토큰을 예측하는 것이다. 이것은 교차 엔트로피 손실로서 공식화된다:\n' +
      '\n' +
      '\\[L_{VLM}=\\prod_{l=1}^{L}p(y_{l}|y_{<l},x),\\]\n' +
      '\n' +
      '여기서 \\(x\\)은 이미지를 나타내고, \\(y_{l}\\)은 l번째 언어 토큰을 나타내며, \\(y_{<l}\\)은 \\(y_{l}\\)에 선행하는 모든 토큰을 나타낸다.\n' +
      '\n' +
      '그림 4에서 볼 수 있듯이 이전 부분에 도입된 구성 요소를 결합하여 인간 주석이 달린 데이터 또는 합성 이미지-캡션 쌍으로부터 VLM을 훈련할 수 있으므로 VLM이 합성 캡션과 VLM의 예측 사이에 교차 엔트로피 손실이 있는 합성 이미지 임베딩에 조건화된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '#### 4.1.1 Datasets\n' +
      '\n' +
      '훈련을 위해 우리는 실험에서 다음 데이터 세트를 사용했다.\n' +
      '\n' +
      '* Conceptual Captions v2(Changpinyo et al., 2021)는 다양한 웹사이트에서 이미지와 그에 대응하는 "alt-text" 설명을 자동으로 추출하여 수집되는 1,200만 개의 이미지-텍스트 쌍을 갖는 데이터셋이다. 실험에서 우리는 인간의 주석이 달린 데이터의 주요 소스로 개념 캡션 v2를 사용하는 반면 다른 데이터 세트의 경우 실험에 따라 합성적으로 생성된 이미지/텍스트를 사용할 수 있다.\n' +
      '* WebLI(Chen et al., 2022)는 영어에서 높은 텍스트 품질을 갖는 약 3억 5천만 개의 이미지 및 알트 텍스트 쌍을 갖는다.\n' +
      '\n' +
      '그림 4: 우리는 효율적인 훈련을 위해 합성 이미지-텍스트 쌍을 생성하기 위해 LLM과 이미지 생성 모델을 활용하는 VLM 프레임워크를 소개한다. 우리는 이 그림과 같이 비합성(A) 데이터와 합성(B) 데이터로부터 VLM을 훈련시킬 수 있다. 추가된 합성 쌍으로 훈련된 모델은 인상적인 이미지 캡션 성능을 보여 주석이 달린 이미지의 필요성을 크게 줄입니다.\n' +
      '\n' +
      '* LTIP(Alayrac et al., 2022)는 3억 2,200만 웹 수집 이미지 및 텍스트 쌍을 갖는다. 보다 상세한 내용은 Alayrac 등(2022)을 참조한다.\n' +
      '* GenPair는 섹션 3.1에 설명된 대로 생성된 100만 개의 완전 합성 캡션을 의미하며 텍스트 대 이미지 생성기를 사용하여 VLM 훈련 동안 즉석에서 생성된 합성 이미지와 쌍을 이룬다.\n' +
      '\n' +
      '평가를 위해 우리는 사용했다.\n' +
      '\n' +
      '* MS-COCO(Chen et al., 2015): 우리는 제로 샷 및 미세 조정 설정 하에서 MS-COCO의 일반적으로 사용되는 테스트 세트에 대한 모델의 성능을 평가한다. 미세 조정 설정을 위해 우리는 훈련 세트 MS-COCO를 사용한다. 우리는 평가를 위해 카파시 분할(카파시 및 페이-페이, 2015)을 사용한다. 우리는 CIDEr 점수(Vedantam et al., 2015)를 우리의 메트릭으로 사용한다.\n' +
      '* Flickr-30k (Plummer et al., 2015): 다시, 우리는 카파시 분할에 대한 성능을 평가한다.\n' +
      '\n' +
      '######4.1.2 훈련 세부사항\n' +
      '\n' +
      '이미지 생성기와 VLM 모두에 대해 사전 훈련되고 동결된 VQ-GAN을 사용한다(네트워크에 대한 자세한 내용은 A.1.1 참조). 이미지는 패치 크기가 16인 256x256의 해상도로 입력되며, 이는 256 토큰(16x16)의 시퀀스 길이를 초래한다. 두 모델 모두 LLM으로서 미리 훈련되고 동결된 Chinchilla 400m(Hoffmann et al., 2022)를 사용한다. VLM은 이미지 임베딩에 교차 응답하는 6개의 레이어 및 64개의 잠재 입력 쿼리를 갖는 퍼시버 리샘플러(Alayrac et al., 2022; Jaegle et al., 2021)를 갖는다. LLM의 각 계층과 원래 플라밍고 아키텍처와 유사한 수신기 리샘플러의 출력 사이에는 교차 주의 계층이 있다(추가 세부 사항은 A.1.4 참조).\n' +
      '\n' +
      '영상 생성기의 주요 텍스트 대 영상 모델은 ViT-B(Dosovitskiy et al., 2020)를 사용한다(24개의 레이어를 가진 변압기, 769개의 임베딩 차원, 12개의 헤드 및 3072개의 중간 잠재 크기, 0.1의 드롭아웃 비율). 전체 모델은 AdamW (Loshchilov and Hutter, 2017)로 학습하였으며, 학습률은 0.0001, 학습률은 5000 warmup, 배치 크기는 512, finetuning setting (on COCO)은 0.00001, 섹션 A.1.3은 추가 세부사항을 보고한다. 우리는 MS-COCO 테스트 쌍에서 FID (Frechet inception distance) (Heusel et al., 2017)가 17.1인 시점에서 500k 단계 동안 이미지 생성기를 사전 훈련한다. VLM 훈련 실험은 모두 500k 단계로 실행됩니다. 저희 모델은 모두 Jax로 구현되었으며 256개의 TPU에 대해 교육되었습니다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '4.2.1 합성영상\n' +
      '\n' +
      'VLM 훈련에서 합성 이미지의 효과를 평가하기 위해 기존 데이터 세트에서 사람이 작성한 캡션을 텍스트 대 이미지 모델에 의해 생성된 합성 이미지와 페어링하는 연구를 수행한다. 본 논문에서는 이러한 합성 영상-캡션 쌍에 대해 VLM을 학습하고, 실제 영상-캡션 쌍에 대해 학습된 모델에 대해 성능을 평가한다. 이 비교를 통해 합성 이미지가 VLM 훈련의 맥락에서 실제 이미지를 효과적으로 대체할 수 있는지 조사할 수 있다.\n' +
      '\n' +
      '특히, 주석을 이용하여 LTIP 및 WebLI 데이터세트에 대한 합성 이미지를 생성한다. 이는 표 2에서 VLM(**Synth\\({}^{2}\\)**)을 학습하기 위한 _(Real Text, Synthetic Image)_쌍을 제공한다. 이 모델을 LTIP와 WebLI(표 2의**Baseline**)의 데이터를 사용하지 않고, 원본 _(Real Text, Real Image)_쌍(표 2의**Gold**)과 개념 캡션 v2(CCv2)의 인간 주석 데이터로부터만 훈련된 모델과 비교한다. 일관성을 위해 모든 Synth\\({}^{2}\\) 모델과 Gold 모델은 텍스트-이미지 생성기와 기준선에서 사용된 동일한 데이터 집합인 CCv2와 함께 학습된다.\n' +
      '\n' +
      '표 2에 나타난 바와 같이 합성 영상은 기준선 모델 성능을 25% 이상 크게 향상시킨다. 중요한 것은, 사용된 인간 주석이 달린 이미지의 더 작은 볼륨에도 불구하고 VLM 훈련에 효과적이다. "Gold"는 전체 원본 데이터 세트를 사용할 때 상한 성능을 나타냅니다. LTIP의 경우 합성 이미지가 원본 이미지를 약간 능가하며, 이는 데이터 다양성 증가로 인한 것 같다. 원본 쌍이 훈련 중에 정적으로 남아 있기 때문에 합성 im\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l c c c c c} \\hline \\hline Method & Real & Synth & \\#Real Data & \\#Synth Data & CIDEr-COCO (\\(\\uparrow\\)) & CIDEr-Flickr-30 (\\(\\uparrow\\)) \\\\ \\hline Baseline & CCv2 & - & 10.1M & - & 22.1 & 11.3 \\\\ \\hline Synth\\({}^{2}\\) & CCv2 & LTIP & 10.1M & 330M & **28.7** & **12.9** \\\\  & CCv2 & WebLI & 10.1M & 350M & 27.6 & 12.3 \\\\ \\hline Gold & CCv2+LTIP & - & 340.1M & - & 27.6 & 13.2 \\\\  & CCv2+WebLI & - & 360.1M & - & 30.7 & 15.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 원본 또는 합성적으로 생성된 이미지와 쌍을 이루는 그라운드 트루스 캡션으로 훈련할 때의 제로 샷 이미지 캡션 결과.\n' +
      '\n' +
      'age는 유익한 이미지 증강을 도입한다(Azizi et al., 2023).\n' +
      '\n' +
      '이러한 결과는 제한된 실제 이미지 데이터에서도 VLM 훈련을 향상시키기 위한 생성 모델의 잠재력을 시사한다. 이전 작업(Azizi et al., 2023; Fan et al., 2023)이 단일 객체 이미지 분류(즉, ImageNet)를 위한 합성 이미지의 사용을 입증했지만, 우리의 연구 결과는 VLM이 복잡하고 다중 객체 장면 및 상세한 캡션을 포함하는 합성 데이터세트로 성공적으로 훈련될 수 있음을 보여준다. 이것은 합성 이미지가 단순 분류를 넘어 이동하고 다양하고 도전적인 시나리오에서 VLM 성능을 크게 향상시킬 가능성을 강조한다.\n' +
      '\n' +
      '######4.2.2 합성 텍스트 및 이미지 쌍\n' +
      '\n' +
      'VLM 훈련에 합성 데이터를 사용하는 효과를 추가로 입증하기 위해 LLM 및 이미지 생성기에 의해 캡션 및 해당 이미지를 포함하는 전체 데이터 세트가 합성적으로 생성되는 추가 실험을 수행했다(이러한 캡션이 생성되는 방법에 대한 설명은 섹션 3.3 참조 및 일부 예는 그림 2 참조).\n' +
      '\n' +
      '표 3의 처음 두 행(기준 대 기준)을 비교합니다. GenPair)는 순수 합성 이미지-캡션 데이터(GenPair)의 작은 부분(1M)이라도 추가하는 것이 실제 데이터에 대해서만 훈련된 베이스라인 모델에 비해 성능(MS-COCO에서 CIDEr 점수의 17.2% 증가)을 크게 향상시킨다는 것을 보여준다. 흥미롭게도 WebLI 또는 LTIP(행 3 및 4)와 같은 실제 데이터 세트에서 추가 1M 데이터 포인트를 샘플링하여 동일한 개선을 달성하지 못한다. 이를 조사하기 위해 각 캡션 집합의 의미적 다양성과 균형을 평가했다. 먼저, 언어 모델을 사용하여 캡션에 대한 임베딩을 생성했다. 그런 다음 이러한 임베딩을 분석하기 위해 k-평균 군집링을 사용했다(자세한 내용은 부록 A.3 참조). 그림 5는 클러스터 분포를 시각화하고, x축은 클러스터 인덱스를 나타내고 y축은 클러스터 내의 각 데이터 세트의 데이터 볼륨을 나타낸다. GenPair는 군집 전체에 걸쳐 더 고른 분포를 보여 개념적 다양성이 더 크다는 것을 나타낸다. 또한 클러스터 크기의 분산이 적어 더 나은 균형을 시사한다.\n' +
      '\n' +
      '마지막으로, 각 데이터 세트 내의 의미론적 집중도를 정량화하기 위해 표 3은 "집중도" 메트릭: 최상위 5개 가장 인구 밀도가 높은 군집에 속하는 데이터의 백분율을 제시한다. GenPair는 상위 5개 군집에서 캡션의 57.5%만이 가장 낮은 농도를 가지고 있다. 이것은 합성적으로 생성된 GenPair에서 더 낮은 의미 불균형을 나타내며 다른 데이터 세트에서 발견되는 더 높은 불균형(69.9% 및 83%)과 대조된다. 또한, 전체 군집 분포에 대한 엔트로피는 GenPair가 군집 전체에 걸쳐 보다 균일한 데이터 분포를 갖는다는 것을 확인한다. 다양한 데이터에 대해 훈련된 모델이 더 잘 일반화되는 경향이 있기 때문에 GenPair 내의 고유한 다양성이 MS-COCO에 대한 강력한 성능에 기여할 가능성이 있다고 가정한다. (클러스터 분석이 수행된 방법과 추가 결과에 대한 자세한 내용은 부록 A.3 참조)\n' +
      '\n' +
      '이 평가는 순수 합성 데이터를 포함한 다양한 텍스트 기반 데이터를 활용하는 Synth\\({}^{2}\\)의 능력을 보여준다. 전반적으로, 우리의 실험은 합성 이미지 생성 및 LLM 생성 캡션을 활용하여 합성 데이터가 VLM 성능을 향상시키는 데 중요한 역할을 강조한다. 이 접근법은 데이터 한계를 극복할 뿐만 아니라 모델 기능을 강화하는 데 있어 텍스트 전용 캡션의 효과를 입증하여 우리 기술의 가치를 더욱 강조한다.\n' +
      '\n' +
      '4.2.3 관련 업무의 비교\n' +
      '\n' +
      '또한, 우리는 최신 모델에 비해 접근법의 잠재력을 조사하면서 받아들였다.\n' +
      '\n' +
      '도 5: 의미론적 다양성. 히스토그램은 클러스터 크기의 분포를 나타내며 GenPair는 의미 개념의 보다 균일한 범위를 보여준다. 히스토그램이 파생된 방법에 대한 자세한 내용은 A.3을 참조하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Real & Synth & \\#Real Data & \\#Synth Data & Concentration (\\(\\downarrow\\)) & Entropy (\\(\\uparrow\\)) & CIDEr-COCO (\\(\\uparrow\\)) \\\\ \\hline CCv2 & - & 10.1M & - & - & - & 22.1 \\\\ CCv2 & GenPair & 10.1M & 1M & **57.7\\%** & **3.81** & **25.9** \\\\ CCv2+WebLI & - & 10.1M+1M & - & 69.8\\% & 3.43 & 24.4 \\\\ CCv2+LTIP & - & 10.1M+1M & - & 83.0\\% & 2.92 & 23.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 합성 생성된 캡션 및 이미지 임베딩 쌍을 사용할 때의 제로 샷 이미지 캡션 결과. 농도는 상위 5개 군집의 누적 분포로 계산되며, 더 낮은 값은 더 높은 다양성을 나타낸다(자세한 내용은 부록 A.3 참조).\n' +
      '\n' +
      '합성 또는 실제 학습 데이터의 양과 모델 매개 변수의 수를 설명합니다. 우리는 GenPair와 WebLI라는 가장 높은 다양성을 가진 두 데이터 세트를 결합했다. 표 4는 우리 모델의 MS-COCO 벤치마크 성능을 관련 방법: IITIT(Li et al., 2023b) 및 DC(Li et al., 2023c)와 비교한다. 이러한 방법들과의 일관성을 위해 MS-COCO 트레이닝 세트(A.2 참조)에서 미세 조정된 Synth\\({}^{2}\\)을 사용하였다. IITIT는 VLM 개선을 위해 주기 일관성을 사용하는 반면 DC는 인간 정의 휴리스틱을 기반으로 이미지를 생성하기 위해 안정적인 확산을 사용한다.\n' +
      '\n' +
      'Synth\\({}^{2}\\)와 IITIT는 유사한 파라미터 카운트와 쌍을 이루는 데이터 사용량을 공유하여 직접 비교할 수 있다. 표 4에 나타난 바와 같이, Synth\\({}^{2}\\)는 CIDEr 점수에서 IITIT를 크게 능가한다. DC는 가장 높은 원시 성능을 달성하지만 훨씬 더 많은 매개변수를 필요로 하며 방대한 양의 실제 데이터에 의존하며 필터링 절차는 인간 휴리스틱에 크게 기반한다.\n' +
      '\n' +
      'Synth\\({}^{2}\\)는 성능과 효율성의 균형을 이룬다. 텍스트 전용 데이터를 활용하여 DC보다 40배 적은 페어드 데이터를 사용하여 경쟁적인 결과를 달성합니다. 이 비교는 성능, 효율성 및 데이터 요구사항 간의 상충 관계를 강조합니다. Synth\\({}^{2}\\)는 자원이 제한적일 때 강력한 솔루션으로 등장하여 데이터 감소와 계산 오버헤드로 인상적인 결과를 얻을 수 있다.\n' +
      '\n' +
      '완전 합성 데이터를 이용한 데이터 효율성 및 증대\n' +
      '\n' +
      '완전 합성 데이터로 증강에 의해 제공되는 성능 증가를 추가로 분석하기 위해, 우리는 GenPair와 함께 추가로 훈련된 기준 모델(즉, CCv2에서만 훈련된)과 Synth\\({}^{2}\\)의 성능을 특성화했다. 그림 6은 두 모델에 대한 성능 추세를 시각화한 것이다. 특히, 기준선을 나타내는 밝은 파란색 곡선은 훈련 단계가 증가함에 따라 성능이 점진적으로 증가하여 결국 22.1 CIDER 점수로 안정된다. 이와는 대조적으로, 보라색 곡선은 Synth\\({}^{2}\\)의 더 가파른 성능 향상을 보여주며, 훈련 단계의 약 1/3으로 짝을 이루는 훈련 레짐과 유사한 성능을 달성한다. 이는 Synth\\({}^{2}\\)에서 완전 합성 데이터 증강을 통해 달성된 상당한 데이터 효율성 이득을 강조하고, 각 곡선을 둘러싼 음영 영역은 3개의 무작위 씨앗에 걸친 표준 편차를 나타내며, 관찰된 성능 경향의 견고성을 보여주며, 이러한 결과는 Synth\\({}^{2}\\)의 완전 합성 데이터를 효과적으로 활용하는 능력을 보여주어 현저한 성능 향상을 가져온다.\n' +
      '\n' +
      '또한 Synth\\({}^{2}\\)의 계산효율을 조사하였다. 자막을 사용하여 이미지 임베딩을 생성하는 경우와 실제 이미지로 자막을 렌더링하는 경우를 비교했다. 표 5에 나타난 바와 같이, 이미지 임베딩을 이용하여 훈련된 Synth\\({}^{2}\\)은 초당 2.08개의 훈련 단계에서 실행되는 반면, 픽셀로부터 훈련된 Synth\\({}^{2}\\)은 초당 1.66개의 느린 속도로 실행되는 더 빠른 성능을 일관되게 보여준다. 이러한 성능 장점은 Synth\\({}^{2}\\)의 효율적인 텍스트 전용 데이터 활용과 병렬 디코딩을 이용한 임베딩 기반 이미지 생성에서 기인하며, 이는 픽셀로부터 학습된 Synth\\({}^{2}\\) 모델에서 수행되는 픽셀 기반 이미지 처리와 관련된 계산 오버헤드를 감소시킴으로써 성능에 영향을 미치지 않는다.\n' +
      '\n' +
      '## 5 Limitations\n' +
      '\n' +
      '현재 작업의 세 가지 주요 한계는 사용된 완전한 합성 데이터의 제한된 양, 생성 모델의 잠재적인 편향 및 텍스트 데이터 소스의 제한된 탐색에서 비롯된다. 첫째, 완전 합성 데이터로 학습을 증강했을 때 상당한 이득을 볼 수 있었지만, 우리의 실험은 비교적 적은 양의 데이터(즉, 1M 이미지-캡션 쌍)로 제한되었다. 앞으로 더 완전한 합성 데이터(예: \\(\\sim\\)700M)를 사용하는지 여부를 조사하는 것이 중요할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & \\(\\#\\)All Params\\(\\downarrow\\) & \\(\\#\\)Real Data\\(\\downarrow\\) & \\(\\#\\)Synth Data\\(\\downarrow\\) & CIDEr\\(\\uparrow\\) (FT) \\\\ \\hline IITIT & 11.2B & **3M** & **110M** & 103.5 \\\\ DC & 1.7B & 5.5B & 400M & **133.1** \\\\ Synth\\({}^{2}\\) & **632M** & 10.1M & 360M & 126.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: COCO 관련 업무와의 비교. COCO Karpathy 분할에 대해 이미지 캡션 성능을 평가한다. 볼드체는 최상의 값을 나타내고 두 번째 최상의 값에 밑줄을 그습니다.\n' +
      '\n' +
      '도 6: 훈련 단계들의 함수로서의 성능. 파란색 곡선은 쌍을 이루는 데이터(CCv2)에만 훈련된 기준선을 보여준다. 자색 곡선은 완전 합성 데이터(GenPair)를 사용하여 증강에 대해 추가로 훈련된 Synth\\({}^{2}\\)의 성능을 보여준다. Synth\\({}^{2}\\)는 훈련 단계의 약 1/3을 사용하여 기준선과 패리티를 달성하여 우수한 효율성을 보여준다. 음영 영역은 3개의 무작위 씨앗에 걸친 표준 편차를 나타낸다.\n' +
      '\n' +
      '결과로 인해 쌍체 데이터만 사용하는 것보다 성능이 향상됩니다. 둘째, 텍스트-이미지 생성을 담당하는 생성 모델은 학습 데이터, 아키텍처 또는 절차에 내재된 한계를 반영하여 합성 데이터에 편향을 도입할 수 있다. 가능한 미래 단계는 잠재적인 편향을 평가하고 완화하기 위해 다양한 텍스트 대 이미지 모델을 사용하여 이 문제를 조사하는 것을 포함할 것이다. 셋째, 현재 작업은 주로 제한된 텍스트 전용 데이터 소스 세트를 탐구한다. 향후 탐구를 위한 흥미로운 방법이 될 수 있는 상황별 생성(예: 의료 데이터)을 조사하는 것을 포함하여 다양한 텍스트 소스의 잠재력을 조사하기 위해서는 추가 연구가 필요하다.\n' +
      '\n' +
      '이러한 한계를 인식하고 해결함으로써 향후 연구는 Synth\\({}^{2}\\) 접근법의 장점을 활용하고 광범위한 시각적 언어 모델링 작업에 대한 일반화 가능성, 견고성 및 효율성을 향상시킬 수 있다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 연구는 시각적 언어 모델의 학습을 향상시키기 위해 합성 이미지-텍스트 쌍을 생성하는 새로운 접근법을 제시한다. 대용량 언어 모델과 텍스트 대 이미지 생성 기능을 활용하여 수동 이미지 라벨링의 한계를 효과적으로 해결한다. 우리의 결과는 몇 가지 중요한 결과를 결정적으로 보여준다.\n' +
      '\n' +
      '***개선된 VLM 성능:** 합성 및 인간 주석 데이터 세트에 대해 훈련된 시각적 언어 모델은 인간 주석 데이터에 대해서만 훈련된 기준선과 비교하여 이미지 캡션 작업에서 현저한 개선을 나타낸다. 이것은 VLM 기능을 효율적으로 증강하기 위한 우리의 방법의 잠재력을 강조한다. 이는 데이터 획득 및 주석이 자원 집약적인 시나리오에서 매우 유리하다.\n' +
      '* **데이터 효율성:** 우리의 접근법은 인간 라벨 데이터의 일부만 활용하면서 유사한 성능을 산출한다.\n' +
      '***맞춤화 및 확장성:** 우리의 방법은 특정 도메인에 맞춘 맞춤형 이미지 데이터 세트를 생성하는 데 유연성을 제공한다. 또한 합성 데이터 생성 과정은 확장 가능하여 대규모 VLM 개발을 지원하기에 적합하다.\n' +
      '\n' +
      '본 연구는 시각 언어 모델 훈련을 변화시킬 수 있는 새로운 기법을 소개한다. 우리의 연구 결과는 합성 데이터 생성의 가치를 강조하여 시각적 언어 이해가 중요한 다양한 분야에서 발전할 수 있는 길을 열어준다. 추가적인 탐색은 다양한 목표를 염두에 두고 합성 데이터 생성 프로세스를 정제하는 것을 조사할 수 있다.\n' +
      '\n' +
      '##7 브로드캐스팅 효과\n' +
      '\n' +
      '본 논문은 머신러닝 분야의 발전을 목표로 하는 작업을 제시한다. 우리의 작업에는 많은 잠재적인 사회적 결과가 있으며, 우리가 특별히 강조해야 한다고 느끼는 것은 없다. 특히 생성 모델의 편향은 합성 데이터에 급격한 영향을 미칠 수 있다. 마찬가지로 생성 모델을 사용할 때 개인 정보 보호의 문제를 고려하는 것이 중요하다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'VQGAN에 대한 통찰력에 대해 샌더 디엘만, 알리 라자비, 베니그노 우리아에 감사드립니다. 우리는 또한 프로젝트 전반에 걸쳐 깊이 있는 논평에 대해 아이다 네마츠자데, 피넬로피 파팔람피디, 한 장, 마테우스 말리노프스키, 발렌틴 드 보톨리, 사드라 갈레비케사비, 에마누엘 부글리아렐로, 크리스 크누센, 머레이 섀너핸에게 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736, 2022에서의 발전.\n' +
      '* Azizi et al. (2023) Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D. J. Synthetic data from diffusion models improves imagenet classification. _ arXiv preprint arXiv:2304.08466_, 2023.\n' +
      '* Baranchuk et al. (2021) Baranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V., and Babenko, A. Label-efficient semantic segmentation with diffusion models. _ arXiv preprint arXiv:2112.03126_, 2021.\n' +
      '* Cascante-Bonilla et al. (2023) Cascante-Bonilla, P., Shehada, K., Smith, J. S., Doveh, S., Kim, D., Panda, R., Varol, G., Oliva, A., Ordonez, V., Feris, R., et al. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 20155-20165, 2023.\n' +
      '* Chang et al. (2017) Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M. - H., Murphy, K., Freeman, W. T.,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Model & Step/Sec\\(\\uparrow\\) & CIDEr(zs)\\(\\uparrow\\) \\\\ \\hline Synth\\({}^{2}\\)- Image Embedding & **2.08** & 25.9 \\\\ Synth\\({}^{2}\\)- Pixels & 1.66 & 26.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 동일한 하드웨어에서 훈련의 초당 단계를 계산하여 측정한 모델 효율성. 삽입 공간에서의 Synth\\({}^{2}\\)은 픽셀을 이용한 Synth\\({}^{2}\\)보다 일관되게 우수한 학습 효율을 보여준다.\n' +
      '\n' +
      'Rubinstein, M., et al. Muse: Text-to-image generation via masked Generative Transformers. _ arXiv preprint arXiv:2301.00704_, 2023.\n' +
      '* Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. 컨셉 12m: 롱테일 비주얼 컨셉을 인식하기 위해 웹 스케일 이미지-텍스트 사전 트레이닝을 푸시한다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3558-3568, 2021.\n' +
      '* Chen et al. (2020a) Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representation. In _International conference on machine learning_, pp. 1597-1607. PMLR, 2020a.\n' +
      '* Chen et al. (2020b) Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models is strong semi-supervised learners. _ 신경 정보 처리 시스템들_, 33:22243-22255, 2020b에서의 진보들.\n' +
      '* Chen et al. (2015) Chen, X., Fang, H., Lin, T. - Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. Microsoft 코코 캡션: 데이터 수집 및 평가 서버 _ arXiv preprint arXiv:1504.00325_, 2015.\n' +
      '* Chen et al. (2022) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. _ ArXiv:2209.06794_, 2022.\n' +
      '* Chen et al. (2019) Chen, Y., Li, W., Chen, X., and Gool, L. V. Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 1841-1850, 2019.\n' +
      '* de Melo et al. (2022) de Melo, C. M., Torralba, A., Guibas, L., DiCarlo, J., Chellappa, R., and Hodgins, J. Nextgeneration deep learning based on simulator and synthetic data. _ 2022년 인지과학의 동향\n' +
      '* Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* Esser et al. (2021) Esser, P., Rombach, R., and Ommer, B. Taming transformer for high-resolution image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 12873-12883, 2021.\n' +
      '* Fan et al. (2023) Fan, L., Chen, K., Krishnan, D., Katabi, D., Isola, P., and Tian, Y. 모델 트레이닝을 위한 합성 이미지의 스케일링 법칙은... 당분간은 arXiv preprint arXiv:2312.04567_, 2023.\n' +
      '* Greff et al. (2022) Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D. J., Gnanapragasam, D., Golemo, F., Herrmann, C., et al. Kubric: scalable dataset generator. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 3749-3761, 2022.\n' +
      '* Grill et al. (2020) Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. _ 신경 정보 처리 시스템들_, 33:21271-21284, 2020에서의 진보들.\n' +
      '* Guo et al. (2022) Guo, X., Wu, W., Wang, D., Su, J., Su, H., Gan, W., Huang, J., and Yang, Q. 합성 데이터로부터 인간의 움직임에 대한 비디오 표현을 학습하는 것. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 20197-20207, 2022.\n' +
      '* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. 2시간 척도 업데이트 규칙에 의해 훈련된 간스는 로컬 내쉬 평형에 수렴한다. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Hu et al. (2022) Hu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and Wang, L. 이미지 캡셔닝을 위한 비전 언어 사전 교육을 확대합니다. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 17980-17989, 2022.\n' +
      '* Jaegle et al. (2021) Jaegle, A., Kimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pp. 4651-4664. PMLR, 2021.\n' +
      '* Karpathy & Fei-Fei (2015) Karpathy, A. and Fei-Fei, L. 이미지 설명을 생성하기 위한 심층 시각적 의미 정렬입니다. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3128-3137, 2015.\n' +
      '* Kingma & Ba(2014) Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. _ arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* Li et al. (2021) Li, D., Yang, J., Kreis, K., Torralba, A., and Fidler, S. 생성 모델을 사용한 의미론적 분할: 준지도 학습과 강력한 영역 외 일반화. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8300-8311, 2021.\n' +
      '* Li et al. (2021)Li, D., Ling, H., Kim, S. W., Kreis, K., Fidler, S., and Torralba, A. Bigdataset: Syntizing imagenet with pixel-wise annotations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 21330-21340, 2022a.\n' +
      '* Li 등 [2022a] Li, H., Gu, J., Koner, R., Sharifzadeh, S., and Tresp, V. 달레이랑 플라밍고는 서로 이해하니? In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 1999-2010, 2023a.\n' +
      '* Li 등 [2022b] Li, J., Li, D., Xiong, C., and Hoi, S. Blip: 통일된 시각 언어 이해 및 생성을 위한 언어 이미지 사전 훈련. In _International Conference on Machine Learning_, pp. 12888-12900. PMLR, 2022b.\n' +
      '* Li et al. [2023b] Li, T., Bhardwaj, S., Tian, Y., Zhang, H., Barber, J., Katabi, D., Lajoie, G., Chang, H., and Krishnan, D. Leveraging unpaired data for vision-language generative models via cycle consistency. _ arXiv preprint arXiv:2310.03734_, 2023b.\n' +
      '* Li et al. [2023c] Li, W., Lotz, J. F., Qiu, C., and Elliott, D. Data 큐레이션 for image captioning with text-to-image generative models. _ arXiv preprint arXiv:2305.03610_, 2023c.\n' +
      '* Loshchilov and Hutter[2017] Loshchilov, I and Hutter, F. Decoupled Weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Ma et al. [2022] Ma, J., Bai, S., and Zhou, C. Pretrained diffusion models for unified human motion synthesis. _arXiv preprint arXiv:2212.02837_, 2022.\n' +
      '* Mishra et al. [2022] Mishra, S., Panda, R., Phoo, C. P., Chen, C.-F. R., Karlinsky, L., Saenko, K., Saligrama, V., and Feris, R. S. Task2sim: Towards effective pre-training and transfer from synthetic data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 9194-9204, 2022.\n' +
      '* Plummer et al. [2015] Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _Proceedings of the IEEE international conference on computer vision_, pp. 2641-2649, 2015.\n' +
      '* Radford et al. [2021] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Ridnik et al. [2021] Ridnik, T., Ben-Baruch, E., Noy, A., and Zelnik-Manor, L. Imagenet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.\n' +
      '* Ros et al. [2016] Ros, G., Sellart, L., Materzynska, J., Vazquez, D., and Lopez, A. M. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 3234-3243, 2016.\n' +
      '* Schuhmann et al. [2021] Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Sharifzadeh et al. [2021] Sharifzadeh, S., Baharlou, S. M., and Tresp, V. Classification by attention: Scene graph classification with prior knowledge. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 5025-5033, 2021.\n' +
      '* Sharifzadeh et al. [2022] Sharifzadeh, S., Baharlou, S. M., Schmitt, M., Schutze, H., and Tresp, V. Improving scene graph classification by exploiting knowledge from texts. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pp. 2189-2197, 2022.\n' +
      '* Team et al. [2023] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tritrong et al. [2021] Tritrong, N., Rewatbowornwong, P., and Suwajanakorn, S. Repurposing gans for one-shot semantic part segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 4475-4485, 2021.\n' +
      '* Tsimpoukelli et al. [2021] Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. _Advances in Neural Information Processing Systems_, 34:200-212, 2021.\n' +
      '* Varol et al. [2017] Varol, G., Romero, J., Martin, X., Mahmood, N., Black, M. J., Laptev, I., and Schmid, C. Learning from synthetic humans. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 109-117, 2017.\n' +
      '* Vedantam et al. [2015] Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 4566-4575, 2015.\n' +
      '* Zheng et al. [2020] Zheng, J., Zhang, J., Li, J., Tang, R., Gao, S., and Zhou, Z. Structured3d: A large photo-realistic dataset for structured 3d modeling. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pp. 519-535. Springer, 2020.\n' +
      '\n' +
      'Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pp. 2223-2232, 2017.\n' +
      '\n' +
      '## 부록 A 부록.\n' +
      '\n' +
      '### 추가 구현 세부사항\n' +
      '\n' +
      '####a.1.1 Vq-Gan 세부사항\n' +
      '\n' +
      'VQGAN Architecture: Our VQGAN architecture는 이전 작업과 유사하다(Esser et al., 2021). 여러 개의 잔차 블록, 다운샘플(인코더) 및 업샘플(디코더) 블록으로 구성된다. 주요 차이점은 비 로컬 블록을 제거하여 인코더와 디코더를 완전히 컨볼루션하여 서로 다른 이미지 크기를 지원한다는 것이다. 기본 VQGAN 모델에서는 각 해상도에 2개의 잔차 블록을 적용하고 기본 채널 차원은 128이며, finetuned 디코더에서는 각 해상도에 4개의 잔차 블록을 적용하고 기본 채널 차원은 256이 되도록 한다.\n' +
      '\n' +
      '#### a.1.2 Synth\\({}^{2}\\)에 대한 최적화 상세\n' +
      '\n' +
      '####a.1.3 텍스트 - 이미지 생성기 세부사항\n' +
      '\n' +
      'Configuration param & Value\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} Config param & Value \\\\ \\hline Perceptual loss weight & 0.05 \\\\ Adversarial loss weight & 0.01 \\\\ Codebook size & 8192 \\\\ Optimizer & Adam (Kingma \\& Ba, 2014) \\\\ Discriminator learning rate & 1e-4 \\\\ Generator learning rate & 1e-4 \\\\ Optimizer momentum & \\(\\beta_{1}\\)=0.9 \\(\\beta_{2}\\)=0.99 \\\\ Batch Size & 256 \\\\ Learning rate schedule & Cosine \\\\ Decay (Loshchilov \\& Hutter, 2017) Warmup steps & 10000 \\\\ Training steps & 500000 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: VQGAN에 대한 구성 및 훈련 하이퍼파라미터.\n' +
      '\n' +
      '### Finetuning details\n' +
      '\n' +
      '사전 훈련 단계에 이어, 우리의 모델은 다양한 다운스트림 작업에 대해 미세 조정을 거친다. 미세 조정 프로세스는 사전 훈련에서와 유사한 베타 값을 갖는 AdamW 최적화기를 사용한다. 학습률은 1e-5였다.\n' +
      '\n' +
      '미세 조정 시 일반화 기능을 강화하기 위해 드롭아웃(0.1로 설정)과 같은 정규화 방법을 사용합니다.\n' +
      '\n' +
      '기존 관행에 따라 개발 분할을 사용하여 최적의 미세 조정 설정을 결정한다. 그런 다음 테스트 분할을 기반으로 성능 결과를 보고합니다.\n' +
      '\n' +
      '### Clustering details\n' +
      '\n' +
      '분석을 위한 최적의 클러스터 수를 결정하기 위해 Elbow 방법을 WCSS(within-Cluster Sum of Squares) 메트릭과 함께 사용했다. 클러스터의 수를 변화시키면서 K-평균 클러스터링 알고리즘을 반복적으로 적용하여 각 반복에 대한 WCSS를 계산했다. WCSS 도표에서 명확한 \'팔꿈치\' 지점이 관찰되어 군집 수가 20개까지 증가함에 따라 분산이 크게 감소했으며, 이 지점 이후에는 군집 수가 더 증가하여 WCSS 감소 측면에서 수익률이 감소했다. 이 분석을 기반으로 20개의 군집이 간결함과 데이터 세트 내의 기본 구조를 캡처하는 사이에 적절한 균형을 제공한다고 결정했다.\n' +
      '\n' +
      '클러스터 구성의 다양성을 분석하기 위해 연결된 GenPair, WebLI 및 LTIP 데이터 세트에 K-means 알고리즘과 함께 공동 군집링 접근법을 사용했다. 결과 군집 각각에 대해 각 개별 데이터 세트에 대해 정규화된 군집 크기를 계산했다. 이를 통해 그림 5와 같이 각 군집 내 데이터 세트 간의 분포를 시각화할 수 있었다.\n' +
      '\n' +
      '농도를 계산하기 위해 상위 5개 군집에 걸쳐 누적 합계를 수행하여 데이터 포인트의 백분율을 결정했다. 이 아이디어는 상위 5개 군집에서 비율이 높을수록 모든 군집에 대한 분포가 덜 균일하여 잠재적으로 더 낮은 수준의 다양성을 나타낸다. 여기 표 8에서 우리는 또한 전체 20개의 군집에 대한 상위 3과 엔트로피를 보고한다. 두 측정 모두 WebLi 및 LTIP에 대한 GenPair에 대해 클러스터 전반에 걸쳐 데이터의 보다 균일한 분포를 확인한다. 특히 엔트로피는 더 작더라도 모든 클러스터를 고려하기 때문에 다른 측정치를 보완한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} Config param & Value \\\\ \\hline Num Layers & 6 \\\\ Num Heads & 16 \\\\ Embedding hidden dim & 1024 \\\\ Activation & GeLU \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7:수신자 세부사항\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline Real & Synth & \\#Real Data & \\#Synth Data & CIDEr-COCO (\\(\\uparrow\\)) & Concentration Top-3 (\\(\\downarrow\\)) & Concentration Top-5 (\\(\\downarrow\\)) & Entropy (\\(\\uparrow\\)) \\\\ \\hline CCv2+WebLI & - & 10.1M+1M & - & 24.4 & 58.2\\% & 69.8\\% & 3.43 \\\\ CCv2+LTIP & - & 10.1M+1M & - & 23.4 & 76.5\\% & 83.0\\% & 2.92 \\\\ CCv2 & GenPair & 10.1M & 1M & **25.9** & **40.5\\%** & **57.7\\%** & **3.81** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 합성 생성된 캡션 및 이미지 임베딩 쌍을 사용할 때의 제로 샷 이미지 캡션 결과.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>