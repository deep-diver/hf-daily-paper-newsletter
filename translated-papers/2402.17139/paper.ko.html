<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '실시간 의사결정을 위한 새로운 언어로서의 #비디오\n' +
      '\n' +
      'Sherry Yang\n' +
      '\n' +
      'Jacob Walker\n' +
      '\n' +
      'Jack Parker-Holder\n' +
      '\n' +
      'Yilun Du\n' +
      '\n' +
      'Jake Bruce\n' +
      '\n' +
      'Andre Barreto\n' +
      '\n' +
      'Pieter Abbeel\n' +
      '\n' +
      'Dale Schuurmans\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트와 비디오 데이터는 모두 인터넷에 풍부하며 다음 토큰이나 프레임 예측을 통한 대규모 자기 지도 학습을 지원한다. 그러나, 그들은 동등하게 활용되지 않았다: 언어 모델은 상당한 실제 영향을 미쳤지만, 비디오 생성은 미디어 엔터테인먼트에 크게 제한되었다. 그러나 비디오 데이터는 언어로 표현하기 어려운 물리적 세계에 대한 중요한 정보를 캡처한다. 이러한 격차를 해결하기 위해 우리는 현실 세계의 과제를 해결하기 위해 비디오 생성을 확장할 수 있는 과소 평가된 기회에 대해 논의한다. 우리는 언어와 유사하게 비디오가 인터넷 지식을 흡수하고 다양한 작업을 표현할 수 있는 통합 인터페이스 역할을 할 수 있는 방법을 관찰한다. 또한 언어 모델과 마찬가지로 비디오 생성이 인컨텍스트 학습, 계획 및 강화 학습과 같은 기술을 통해 플래너, 에이전트, 컴퓨팅 엔진 및 환경 시뮬레이터 역할을 할 수 있는 방법을 보여준다. 우리는 비디오 생성의 이러한 고급 기능이 도달 범위 내에 어떻게 그럴듯하게 있는지 보여주는 최근 작업에 의해 뒷받침되는 로봇 공학, 자율 주행 및 과학과 같은 도메인에서 주요 영향 기회를 식별한다. 마지막으로, 우리는 진행을 완화하는 비디오 생성의 주요 과제를 식별한다. 이러한 문제를 해결하면 비디오 생성 모델이 더 광범위한 AI 응용 프로그램에서 언어 모델과 함께 고유한 가치를 입증할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '지난 몇 년 동안 인터넷 텍스트 데이터 세트로부터 대규모 언어 모델(LLM)을 훈련하는 데 엄청난 진전이 있었다(Team et al., 2023; Achiam et al., 2023). 매우 다양한 작업에 대한 LLM의 인상적인 성능은 이러한 시스템을 확장하기 위해 인공지능 의제를 줄이는 것을 유혹하게 만든다. 그러나, 이것은 충분하지 않다. 첫째, 공개된 텍스트 데이터의 양은 추가 스케일링에 병목 현상이 되고 있다(Villalobos et al., 2022). 둘째, 그리고 아마도 더 중요한 것은, 자연 언어만으로는 모든 지능적인 행동을 묘사하기에 충분하지 않을 수 있다(Searle, 1980; Dennett, 1993; Minsky, 1988) 또는 우리가 살고 있는 물리적 세계에 대한 모든 정보를 캡처하거나(예를 들어, 누군가에게 단어만을 사용하여 매듭을 묶는 방법을 가르치는 것을 상상해보세요). 언어는 더 높은 수준의 추상화를 설명하는 강력한 도구이지만, 모든 풍부한 세부 사항에서 물리적 세계를 포착하기에 항상 충분한 것은 아니다.\n' +
      '\n' +
      '감사하게도, 인터넷에는 풍부한 비디오 데이터(예를 들어, 유튜브에서만 10,000년 이상의 연속 비디오 시청)가 있으며, 세계에 대한 지식이 풍부한 풍부한 정보를 요약한다. 그럼에도 불구하고, 오늘날 인터넷 텍스트 또는 비디오 데이터에 대해 훈련된 기계 학습 모델은 현저하게 다른 능력을 입증했다. LLM은 정교한 추론(Huang and Chang, 2022), 도구 사용(Mialon et al., 2023), 의사 결정(Yang et al., 2023c)이 필요한 복잡한 작업을 해결하는 데 발전했다. 대조적으로, 비디오 생성 모델은 주로 인간 소비를 위한 엔터테인먼트 비디오를 생성하는 것에 초점을 맞추어 덜 탐색되었다(Ho et al., 2022; Singer et al., 2022; Bar-Tal et al., 2024). 언어 모델링에서 전개되는 패러다임 변화를 감안할 때 로봇학, 자율주행, 과학 등 시각적 양식이 필요한 애플리케이션이 인터넷 시각적 지식과 사전 훈련된 비디오 모델의 보다 직접적인 혜택을 받을 수 있도록 비디오 생성 모델을 언어 모델과 유사한 자율 에이전트, 시뮬레이션 환경 및 계산 엔진 수준으로 높일 수 있는지 여부를 묻는 것이 중요하다.\n' +
      '\n' +
      '본 논문에서는 언어 모델링이 디지털 세계에 있는 것처럼 _video 생성이 물리적 세계에 있을 것이라는 입장을 취한다._ 이 위치에 도달하기 위해, 먼저 언어 모델들이 많은 실세계 태스크들을 해결할 수 있게 한 주요 컴포넌트들을 식별한다: (1) 인터넷으로부터 광범위한 정보를 흡수할 수 있는 _unified representation_ (즉, 텍스트), (2) 다양한 태스크들이 생성 모델링으로 표현될 수 있는 _unified interface_ (즉, 텍스트 생성), (3) 언어 모델들이 인간 피드백으로부터의 강화 학습(Ouyang et al., 2022), 계획(Huang et al., 2022), 검색(Yao et al., 2023), 및 최적화(Rafailovet al., 2023)와 같은 기법들을 통해 외부 피드백에 기초한 결정들을 수행하고 최적화함으로써 외부 환경들(예를 들어, 인간, 도구들, 및 다른 모델들)과_interact_할 수 있는 동작들을 취함으로써 외부 환경들(예를 들어, 인간, 도구들, 및 다른 모델들)과_interact_할 수 있는 동작들.\n' +
      '\n' +
      '언어 모델의 세 가지 측면에 자극을 받아, (1) 비디오는 물리적 세계에 대한 광범위한 정보를 흡수하는 통합된 표현 역할을 할 수 있고, (2) 컴퓨터 비전, 체화된 AI 및 과학의 다양한 작업은 비디오 생성 모델에 의해 표현되거나 지원될 수 있으며, (3) 사전 훈련 목적으로 비디오 생성은 대규모 비전 모델, 행동 모델 및 세계 모델에 대한 인터넷 규모의 감독을 도입하며, 이는 액션 추출, 환경 상호 작용 시뮬레이션 및 결정 최적화를 가능하게 한다.\n' +
      '\n' +
      '비디오 생성이 실제 애플리케이션에 지대한 영향을 미칠 수 있는 방법을 추가로 설명하기 위해 게임, 로봇 공학, 자율 주행 및 과학과 같은 환경에서 명령어 튜닝, 상황 내 학습, 계획 및 강화 학습(RL)과 같은 기술을 통해 비디오 생성을 작업 해결자, 질문에 대한 답변, 정책/에이전트 및 환경 시뮬레이터로 활용하는 최근 작업에 대한 심층 분석을 제공한다. 마지막으로, 비디오 생성을 둘러싼 주요 문제를 식별하고 이러한 문제를 해결하기 위한 그럴듯한 솔루션을 제안하여 실제 세계에서 비디오 생성의 잠재력을 최대한 발휘한다.\n' +
      '\n' +
      '## 2 Preliminaries\n' +
      '\n' +
      '우리는 비디오 생성 모델에 대한 간략한 개요와 조건부 생성을 통해 도메인별 설정에서 어떻게 사용되었는지 제공한다.\n' +
      '\n' +
      '조건부 비디오 생성\n' +
      '\n' +
      '우리는 비디오 클립을 이미지 프레임(\\textbf{x}=(x_{0},...,x_{t})의 시퀀스로 표시한다. 이미지는 그 자체로 \\(\\textbf{x}=(x_{0},)\\)의 단일 프레임으로 특수 동영상으로 취급될 수 있다. 조건부 비디오 생성은 조건부 확률 \\(p(\\textbf{x}|c)\\)을 모델링하며, 여기서 \\(c\\)은 조건부 변수이다. 조건부 확률\\(p(\\textbf{x}|c)\\)는 일반적으로 자기 회귀 모델(Razavi et al., 2019), 확산 모델(Ho et al., 2022) 또는 마스킹된 변압기 모델(Chang et al., 2022)에 의해 인수분해되었다. 인수분해에 따라 \\(p(\\textbf{x}|c)\\)에서 샘플링은 순차적으로 이미지(패치)를 예측하거나 모든 프레임 \\((x_{0},...,x_{t})\\)을 함께 반복적으로 예측하는 것에 해당한다.\n' +
      '\n' +
      '### Task-Specific Specialization\n' +
      '\n' +
      '조건부 변수 \\(c\\)에 무엇이 있는가에 따라, 조건부 비디오 생성은 다른 목적을 제공할 수 있다. 아래에서는 \\(c\\)의 일반적인 예와 그 사용 사례를 열거한다.\n' +
      '\n' +
      '*\\(p(\\textbf{x}|c=\\text{text})\\). 이것은 생성 미디어에 일반적으로 사용되는 텍스트 대 비디오 모델(Kondratyuk et al., 2023; Blattmann et al., 2023)에 대응하고, 여기서 텍스트는 종종 원하는 비디오의 일부 창의적인 기술(예를 들어, Singer et al. (2022)에서 "A teddy bear painting a portrait")이다. 텍스트-투-비디오는 영화(Zhu et al., 2023) 및 애니메이션(He et al., 2023; Guo et al., 2023)을 생성하는 데 주로 적용되어 왔다.\n' +
      '*\\(p(\\textbf{x}|c=\\{x_{0},\\text{text}\\})\\). 이는 텍스트 설명을 통합하면서 주어진 이미지 \\(x_{0}\\)에서 시작하는 비디오 롤아웃을 생성하는 것에 해당한다. 이러한 유형의 컨디셔닝은 장면-특정 시각적 상호작용들(Yang et al., 2023) 및 로봇 실행을 위한 시각적 계획들(Du et al., 2023)을 생성하기 위해 적용되었다. **x**가 미래 영상\\(x_{t}\\)만을 포함하는 경우, \\(p(x_{t}|c=\\{x_{0},\\text{text}\\})\\)는 로봇 조작에 대한 시각적 목표를 예측할 수 있다(Black et al., 2023; Yu et al., 2023). 목표 합성에 대한 이러한 접근법은 양식화된 이미지 생성 및 인페인팅(Efros and Freeman, 2023; Wang et al., 2023)에 대한 방대한 문헌에서 크게 영감을 받았다.\n' +
      '*(p(\\textbf{x}|c=\\{\\overline{\\textbf{x},\\text{text}\\})\\). **x** 및 \\(\\overline{\\textbf{x}}\\)이 동일한 기본 콘텐츠를 갖는 경우, 이는 텍스트-유도 비디오 편집 및 스타일화에 해당하며(Loeschcke et al., 2022; Yang et al., 2023), 이는 상이한 기상 조건에서 자율 주행 비디오를 생성하기 위해 적용되어 왔다(Hu et al., 2023). 또한 \\(\\overline{\\textbf{x}\\)는 **x**와 완전히 다를 수 있으며, 이 경우 \\(\\overline{\\textbf{x}\\)은 출력 비디오에서 특정 패턴을 이끌어내는 시각적 프롬프트 역할을 할 수 있다(Bai et al., 2023).\n' +
      '*\\(p(x_{i+1}|c=\\{x_{i},\\text{action}\\})\\). 이는 액션이 로봇 컨트롤(Yang et al., 2023), 키보드 입력(Hafner et al., 2020), 또는 시각적 공간의 변화를 야기하는 다른 모션 정보(Li et al., 2023)일 수 있는 시각적 역학 모델을 학습하는 것에 대응한다. 우리가 어떤 \\(t>1\\)에 대해 \\(x_{i+1}\\)을 \\(x_{i+t}\\)으로 대체한다면, 우리는 시간-추상 역학 모델을 갖는다 (Sutton et al., 1999). 이 경우 우리는 \\(x_{i}\\)을 \\((x_{i},x_{i+1},...,x_{i+t-1})\\의 임의의 서브시퀀스로 대체할 수도 있다.\n' +
      '\n' +
      '이러한 조건부 비디오 생성의 전문화는 광범위한 비디오 데이터를 흡수하고 비디오 생성을 사용하여 다양한 작업을 표현할 수 있는 일반적인 프레임워크가 존재할 수 있음을 시사한다.\n' +
      '\n' +
      '##3 Unified Representation and Task Interface\n' +
      '\n' +
      '이 섹션에서는 먼저 비디오가 광범위한 지식을 형성하기 위해 인터넷에서 다양한 유형의 정보를 캡처할 수 있는 _unified representation_인 방법에 대해 설명한다. 그런 다음 컴퓨터 비전과 체화된 AI의 다양한 작업이 어떻게 조건부 비디오 생성 문제로 공식화될 수 있는지 논의하여 비디오 생성과 함께 실제 의사 결정의 기초를 제공한다. 예제를 생성하는 데 사용된 모델의 세부 사항은 부록 A에서 찾을 수 있다. 추가로 생성된 비디오는 부록 B에서 찾을 수 있다.\n' +
      '\n' +
      '정보의 통합적 표현으로서의### 비디오\n' +
      '\n' +
      '인터넷 텍스트 데이터는 큰 언어 모델을 사용하여 디지털/지적 세계에 많은 가치를 제공했지만 텍스트는 물리적 세계의 낮은 수준의 세부 사항보다 높은 수준의 추상화를 캡처하는 데 더 적합하다. 아래에는 텍스트로 표현하기 어렵지만 동영상으로 쉽게 캡처할 수 있는 몇 가지 유형의 정보를 나열합니다.\n' +
      '\n' +
      '**Visual and Spatial Information**: 색상, 모양, 질감, 조명 효과 등의 시각적 세부 사항과, 사물이 공간에 어떻게 배치되는지, 상대적인 위치, 거리, 방위, 3D 정보 등의 공간적 세부 사항을 포함한다. 이러한 정보는 텍스트 형식이 아닌 이미지/비디오 형식으로 자연스럽게 존재한다.\n' +
      '**물리학 및 역학**: 이것은 물체 및 환경이 물리적 법칙에 의해 영향을 받는 충돌, 조작 및 다른 움직임과 같이 물리적으로 상호 작용하는 방법에 대한 세부사항을 포함한다. 텍스트는 하이 레벨(예를 들어, "거리를 따라 달리는 자동차")에서의 움직임을 기술할 수 있지만, 차량에 가해지는 토크 및 마찰과 같은 로우 레벨 세부사항을 캡처하기에는 종종 불충분하다. 비디오는 이 정보를 암묵적으로 캡처할 수 있습니다.\n' +
      '**행동 및 행동 정보**: 이는 사람의 행동 및 에이전트 행동과 같은 정보를 포함하며, 가구 한 조각을 조립하는 방법과 같은 작업을 수행하는 하위 수준의 세부 사항을 특성화한다. 다시 텍스트는 정확한 움직임과 움직임과 같은 세부 정보와는 반대로 태스크를 수행하는 방법에 대한 높은 수준의 설명을 대부분 캡처할 수 있다.\n' +
      '\n' +
      '왜 비디오?텍스트가 위의 정보를 캡처하기에 충분하지 않더라도 왜 비디오인지 궁금할 수 있다. 이 질문에 답하기 위해 우리는 비디오가 인터넷 규모에 존재하는 것 외에도 인간에게 (텍스트와 유사한) 해석 가능하므로 디버깅, 상호 작용 및 안전 추측이 쉽게 수행될 수 있음을 관찰한다. 더욱이, 비디오는 상이한 시공간 해상도들, 예를 들어, 옹스트롬 스케일(\\(10^{-10}\\)m)에서 이동하는 원자들(Kashin et al., 2021) 및 초당 a trillion 프레임으로 이동하는 광(Faccio and Velten, 2018)에서 정보를 특징짓을 수 있는 유연한 표현이다.\n' +
      '\n' +
      '통합 작업 인터페이스로서의### 비디오 생성\n' +
      '\n' +
      '광범위한 정보를 흡수할 수 있는 통일된 표현 외에도, 언어 모델링에서 우리는 단일 목적(예: 다음 토큰 예측)을 사용하여 다양한 작업을 표현할 수 있는 통일된 작업 인터페이스가 필요하다는 것을 알 수 있었고, 또한 광범위한 지식을 작업별 결정으로 전달할 수 있는 정보 표현(예: 텍스트)과 작업 인터페이스(예: 텍스트 생성) 간의 정렬이다. 이 절에서는 보다 광범위한 질문 응답, 추론, 문제 해결의 집합뿐만 아니라 다양한 비전 과제가 모두 동영상 생성 과제로 어떻게 표현될 수 있는지를 보여준다.\n' +
      '\n' +
      '고전적인 컴퓨터 비전 작업.자연어 처리에서, 많은 작업들(예를 들어, 기계 번역, 텍스트 요약, 질문 응답, 감정 분석, 명명된 개체 인식, 품사 태깅, 텍스트 분류, 대화 시스템)은 전통적으로 상이한 작업으로 간주되어 왔지만, 이제 모두 언어 모델링의 우산 하에 통합되었다. 이를 통해 업무 전반에 걸쳐 더 큰 일반화와 지식 공유가 가능해졌다. 유사하게, 컴퓨터 비전은 또한 시맨틱 세분화, 깊이 추정, 표면 정상 추정, 포즈 추정, 에지 검출 및 객체 추적에 걸쳐 광범위한 태스크 세트를 갖는다. 최근의 작업은 도 1에 도시된 바와 같이 다양한 비전 태스크들을 비디오 생성 태스크로 변환하는 것이 가능하다는 것을 보여주었다(Bai et al., 2023; Bar et al., 2022; Wang et al., 2023b). 그리고 비전 태스크들을 해결하기 위한 이러한 통합된 접근법은 모델 크기, 데이터 크기, 및 컨텍스트 길이로 양호하게 스케일링된다는 것을 보여주었다(Bai et al., 2023).\n' +
      '\n' +
      '비전 태스크들을 비디오 생성 태스크로 변환하는 것은 일반적으로 다음의 단계들을 수반한다: (1) 태스크(예를 들어, 분할 맵들, 깊이 맵들)의 입력과 출력을 통일된 이미지/비디오 공간으로 구조화하고, (2) 입력 이미지가 특정 태스크의 예상 출력 이미지(예를 들어, 정규 입력 이미지 다음에 깊이 맵)에 이어지도록 이미지 프레임들을 재정렬하고, (3) 원하는 태스크를 특정하기 위해 조건부 비디오 생성 모델에 입력으로서 예시적인 입력-출력 쌍들을 제공함으로써 컨텍스트 내 학습을 레버리지한다.\n' +
      '\n' +
      'Video as Answers.In traditional visual question answering(VQA)(Antol et al., 2015)에서 예상되는 답변은 텍스트로 되어 있다. 비디오 생성의 발전과 함께, 새로운 태스크는 비디오를 답변으로서 취급하는 것, 예를 들어, "종이접기 비행기를 만드는 방법"에 응답하여 비디오가 생성될 것이다(Soucek et al., 2023; Yang et al., 2023b). 언어 모델이 텍스트에서 인간 문의에 대한 맞춤형 응답을 생성할 수 있는 방법과 유사하게, 비디오 모델은 또한 매우 낮은 수준의 세부 사항을 갖는 방법-질문들에 대한 맞춤형 답변을 생성할 수 있다. 이러한 비디오 응답은 텍스트 응답보다 인간보다 더 바람직할 수 있다(Yadav et al., 2011). 그림 2에서 우리는 일련의 방법 문의에 대한 응답으로 텍스트 대 비디오 모델에 의해 생성된 비디오를 보여준다. 추가적으로, 사용자-특정 장면들에서 비디오 답변들을 합성하기 위해 초기 프레임 상의 컨디셔닝 생성을 고려할 수 있다. 엄청난 약속에도 불구하고\n' +
      '\n' +
      '도 1: **비디오 생성으로서의 태스크들. Bai et al.(2023)로부터의 도 8 (부분적 프롬프트를 보여주기 위해 단순화됨), 여기서 조인트/에지 검출, 깊이 추정, 및 세그먼트화와 같은 다양한 컴퓨터 비전 태스크는 단일 다음-프레임 예측 태스크로 변환될 수 있다.**\n' +
      '\n' +
      '오늘날의 텍스트-비디오 모델은 일반적으로 너무 짧거나 단순하여 사용자의 질문에 완전히 답할 수 있는 충분한 정보가 포함되어 있지 않다.\n' +
      '\n' +
      '사용자의 질문에 답하기 위해 비디오 프레임을 합성하는 문제는 언어 모델(Valmcekam et al., 2023)과의 계획과 유사하지만, 상태 및 하위 레벨 액션 공간 둘 다 이제 텍스트와 반대되는 픽셀이다. 언어 모델 또는 비전 언어 모델을 활용하여 상위 목표(예를 들어, "초밥 만드는 방법")를 특정 하위 목표(예를 들어, "먼저, 롤링 매트 위에 쌀을 올려놓기")로 분해하고, 합성된 계획의 타당성을 검증하면서 각 하위 목표에 대한 계획을 종합할 수 있다(Du et al., 2023c).\n' +
      '\n' +
      'Visual Reasoning과 Chain-of-Thought.Information의 통일된 표현과 통일된 태스크 인터페이스로, 언어 모델링에서 추론은 모델이 보다 복잡한 문제를 해결하기 위한 중간 단계로 관련 정보를 이끌어낼 수 있다(Wei et al., 2022). 유사하게, 비디오가 통합된 표현 및 태스크 인터페이스로서, 비디오 생성은 또한 그림 3(Bai et al., 2023)에 도시된 바와 같이, 이미지의 마스킹된 영역들을 예측함으로써 시각적 추론의 초기 징후들을 나타냈다. 오른쪽 보조 라인 세트로 비디오를 생성함으로써 Trinh et al.(2024)과 유사한 더 복잡한 기하학 문제를 해결하기 위해 다음 프레임 예측이 사용될 수 있는지 보는 것은 흥미로울 것이다.\n' +
      '\n' +
      '시각적 추론과 기하학 문제를 해결하기 위해 다음 프레임 예측을 활용하는 아이디어를 기반으로 비디오를 사용하여 추론 _process_(Himakunthala et al., 2023)와 알고리즘(Yang et al., 2022b)을 추가로 특성화할 수 있다. 구체적으로, Yang et al.(2022b)은 비디오들을 이용한 BFS(Breadth First Search) 알고리즘의 실행 상태를 특성화하였다. 이러한 맥락에서, 비디오를 생성하기 위한 학습은 도 4에 예시된 바와 같이, 검색하기 위한 학습에 대응한다(또한, Silver et al.(2017) 참조). 그림 3과 그림 4의 예는 창의적으로 보일 수 있지만 사전 훈련 작업으로서 비디오 생성이 언어 모델과 유사한 추론 유사 행동을 이끌어낼 수 있다는 초기 지표 역할을 하여 복잡한 추론 및 알고리즘 작업을 해결하기 위해 비디오 생성을 활용하는 기회를 드러낸다.\n' +
      '\n' +
      'Unified State-Action Space로서의 영상\n' +
      '\n' +
      '우리는 비디오 생성이 광범위한 지식을 흡수하고 다양한 비전 작업을 특성화할 수 있음을 보았습니다. 이 섹션에서는 비디오를 통합 표현 및 작업 인터페이스로 사용하는 구체화된 AI의 구체적인 예를 제공함으로써 이러한 관찰을 추가로 지원한다.\n' +
      '\n' +
      '체화된 AI에서 오랜 도전 중 하나는 데이터 단편화였으며, 여기서 한 세트의 작업을 수행하는 한 로봇에 의해 수집된 데이터 세트는 다른 로봇 또는 다른 세트의 작업에서 학습에 거의 유용하지 않다(Padalkar et al., 2023). 로봇과 작업에 걸친 지식 공유의 주요 어려움은 로봇과 작업의 각 유형이 별개의 상태-행동 공간을 가지고 있다는 점에 있다. 이러한 어려움을 해결하기 위해, Du et al.(2023b)은 작업들 및 환경들에 걸쳐 통합된 상태-동작 공간으로서 픽셀 공간의 사용을 옹호한다. 이러한 프레임워크 하에서, 체화된 계획은 조건부 비디오 생성 문제로 캐스팅될 수 있으며, 이에 따라 인터넷 사전 훈련된 비디오 생성 모델의 혜택을 받을 수 있다. 역동역학 모델(Du et al., 2023b), 목표-조건화된 정책(Black et al., 2023; Kang et al., 2023; Du et al., 2023c), 광학 흐름 네트워크(Ko et al., 2023), 또는 조밀한 그리드 포인트(Wen et al., 2023)와 같은 추가적인 모듈이 이후 하이-레벨 비디오 계획들로부터 로우-레벨 로봇 제어들을 복구하기 위해 채용될 수 있다. 우리는 그림 5(상단)에서 이전 작업에 의해 생성된 비디오 계획을 설명한다. 대부분의 기존 작업은 로봇당 하나의 비디오 생성 모델을 훈련하는데, 이는 체화된 학습을 위한 통합된 상태-행동 공간으로 비디오를 사용하는 잠재적인 이점을 감소시킨다. 우리는 그림에서 다양한 로봇 및 작업 세트를 사용하여 Open X-실시예 데이터셋(Padalkar et al., 2023)에서 비디오 생성 모델을 훈련하여 추가로 생성된 비디오 계획을 제공한다.\n' +
      '\n' +
      '도 4: 비디오 생성으로서 **BFS. Yang et al.(2022b)로부터의 도 14는 BFS 검색 절차를 에뮬레이팅하는 비디오 모델에 의해 생성된 중간 프레임들의 두 세트를 도시한다. 빨간색 셀과 녹색 셀은 시작 위치와 목표 위치를 나타냅니다. 백색 세포와 검은색 세포는 빈 공간과 장애물을 나타낸다. 블루 셀은 BFS 알고리즘을 실행하여 방문했을 셀을 나타낸다.**\n' +
      '\n' +
      '그림 3: **Next-Frame Generation으로서의 시각적 추론. Bai et al.(2023)의 그림 13은 다음 프레임 예측이 IQ 테스트에서와 같은 시각적 추론 작업을 해결할 수 있음을 보여준다.**\n' +
      '\n' +
      '그림 2: **생성된 비디오 방법. 비디오 생성 모델은 복잡한 작업을 수행하는 사람의 손에 대응하는 키 프레임을 합성할 수 있다. 그러나 생성된 프레임은 너무 일반적이며 사용자의 질문에 완전히 응답하기에 충분한 세부 정보를 캡처하지 못한다.**\n' +
      '\n' +
      'ure 5 (bottom). 이전 및 새로 생성된 비디오 계획 모두 매우 사실적으로 보이며 지정된 작업을 성공적으로 완료합니다.\n' +
      '\n' +
      '##4 시뮬레이션으로 비디오 생성\n' +
      '\n' +
      '비디오 생성 자체가 이전 섹션에서 설명한 바와 같이 이미 많은 작업을 해결할 수 있지만, 비디오 생성에서 또 다른 중요한 기회는 다양한 시스템 및 프로세스의 시각적 관찰을 시뮬레이션하는 것이므로 시뮬레이션 결과에 따라 시스템에 대한 제어 입력을 최적화할 수 있다. 이것은 풍부한 비디오 데이터가 수집될 수 있지만 근본적인 역학이 명시적으로 표현되기 어려운 애플리케이션(예를 들어, 클라우드 이동, 소프트 객체와의 상호작용)에 특히 유용하다. 이 절에서는 이러한 시각적 생성 시뮬레이터를 게임 환경에서 연구하는 것으로 시작하여 학습된 시뮬레이터의 품질을 확인하고 새로운 경험의 효과적인 생성에 대해 반복할 수 있는 그라운드 트루스 게임 엔진을 가질 수 있다. 그런 다음 로봇 상호 작용, 자율 주행 및 원자 수준 상호 작용과 같은 실제 프로세스를 시뮬레이션하는 예를 제공한다. 예제를 생성하는 데 사용된 생성 모델에 대한 자세한 내용은 부록 A에서 찾을 수 있다. 추가 생성 시뮬레이션 결과는 부록 B에서 찾을 수 있다.\n' +
      '\n' +
      '#생성적 게임 환경\n' +
      '\n' +
      '게임은 수십 년 동안 AI 알고리즘의 테스트베드로 사용되어 왔다(야나카키스 및 토겔리우스, 2018). 예를 들어, 아케이드 학습 환경(Bellemare et al., 2013)은 아타리 게임(Mnih et al., 2015)에서 인간 수준에 도달한 최초의 AI 에이전트인 딥 Q-러닝의 개발을 가능하게 하였다. 비슷한 맥락에서, 우리는 게임 엔진의 그라운드 트루스 시뮬레이션과 비교하여 생성 시뮬레이터의 품질을 테스트하기 위한 수단으로 게임을 고려할 수 있다. 미래에는 생성 모델을 사용하여 기존 인간이 설계한 시뮬레이션 환경에서 가능한 것을 능가할 수도 있다. 이 섹션에서는 단일 복잡한 환경을 시뮬레이션하는 것부터 완전히 새로운 환경을 생성하는 것까지 이러한 가능성에 대해 논의한다.\n' +
      '\n' +
      '복합 게임 시뮬레이션.액션 조절 비디오 생성은 마인크래프트와 같은 복잡한 컴퓨터 게임의 환경 역학을 시뮬레이션할 수 있다. 개념의 증명으로, 우리는 에피소드 히스토리를 기반으로 한 미래의 에이전트 행동 및 관찰을 예측하는 시간상 자기회귀식 트랜스포머 기반 아키텍처를 훈련했다. 우리는 인간이 게임과 상호작용하는 동안 수집된 궤적으로 구성된 베이커 등(2022)의 "계약자 데이터"를 사용했다. 관측치와 액션 모두 양자화된 토큰으로 모델 기반 롤아웃을 다음 토큰 예측으로 줄입니다. 이 경우 모델은 세계 모델 역할을 모두 수행합니다.\n' +
      '\n' +
      '도 5: **Generated Video Plans for Robots.** [Top] 기존 작업에 의해 생성된 Video plans(도 3 in Du et al. (2023b), 도 3 in Black et al. (2023), 도 3 in Du et al. (2023c), 도 5 in Ko et al. (2023), 도 14 in Yang et al. (2023b), 도 7 in Kang et al. (2023) [Bottom] Open X-실시예(Padalkar et al., 2023)에 트레이닝된 단일 비디오 생성 모델에 의해 생성된 비디오 계획.\n' +
      '\n' +
      '그림 6: **마인크래프트에서 생성된 게임 궤적.** 마인크래프트 데이터에 대해 훈련된 자기회귀 모델을 사용하여 액션과 관찰이 모두 생성된다. 맨 위 행에서 인벤토리가 열립니다. 가운데 줄은 돌덩이를 부수기 위해 곡괭이를 사용하는 것을 보여준다. 하단 행은 환경 전체의 움직임을 예측합니다.\n' +
      '\n' +
      '및 정책: 교번하는 관찰들의 시퀀스와 액션에서 끝나는 액션들이 주어지면, 모델은 다음 관찰(세계 모델)을 추론할 수 있고, 관찰에서 끝나는 유사한 시퀀스가 주어지면, 모델은 취할 다음 액션(정책)을 추론할 수 있다. 그림 6은 이 모델에서 생성된 몇 가지 궤적을 보여준다. 모델은 정교한 전략들에 대응하는 액션들 및 전이들을 생성할 수 있다(예를 들어, 돌 블록을 깨기 위해 피케이즈를 사용하는 것).\n' +
      '\n' +
      '이러한 정책 및 역학 백본으로, Dyna(Sutton, 1991), Dreamer(Hafner et al., 2020), MuZero(Schrittwieser et al., 2019; Antonoglou et al., 2022)와 같은 모델 기반 강화 학습 알고리즘이 정책을 개선하기 위해 사용될 수 있다. 이것은 역학 모델에서 광범위한 샘플링을 필요로 하며, 이는 차례로 생성 모델이 계산적으로 효율적이어야 한다. 비디오 생성 모델은 매우 일반적임에도 불구하고, 계획이 우려될 때, 세계 모델은 비디오 모델일 필요는 없으며, 잠재 상태 공간 모델은 종종 이전에 선호되었다(Ichter and Pavone, 2019; Hafner et al., 2020).\n' +
      '\n' +
      '새로운 게임 환경 생성.절차적으로 새로운 게임 콘텐츠 및 레벨을 생성하는 것은 게임 AI 커뮤니티(Summerville et al., 2018)에서 활발한 연구 분야이며, 이는 RL 에이전트의 훈련 및 평가 모두에 유용한 것으로 나타났다(Risi and Togelius, 2020; Justesen et al., 2018; Cobbe et al., 2020). 프레임(Bamford and Lucas, 2020)을 직접 예측하거나 배경을 수정하여 새로운 게임 레벨을 생성함으로써 게임 디자인을 위한 생성 모델을 활용하려는 시도가 있었다(Kim et al., 2020). 그러나 이러한 작업은 특권 시뮬레이션 데이터에 의존하며, 소규모에서만 시도되어 완전히 새로운 게임 환경을 생성할 수 있는 잠재력을 제한한다.\n' +
      '\n' +
      '최근 연구는 레이블이 지정되지 않은 인터넷 규모의 게임 플레이 데이터를 활용하여 _latent_ 액션을 학습한 다음 액션 제어 가능한 비디오 모델을 훈련하는 것이 가능하다는 것을 보여주었다(Bruce et al., 2024). 이는 신속한 이미지로부터 다양한 인터랙티브 환경의 무한한 가능성을 생성할 수 있게 한다. 그림 7은 두 개의 새로운 시작 프레임이 주어진 잠재 행동을 선택하는 인간 플레이어에 의해 제어되는 생성된 게임 궤적을 보여준다. 이 작업은 탐색적으로 남아 있지만, 완전한 생성 게임 환경에서 RL 에이전트를 훈련시키기 위해 학습된 보상 모델(Chan et al., 2023; Du et al., 2023; Escontrela et al., 2023)을 통합하는 것이 가능한 미래를 상상할 수 있다.\n' +
      '\n' +
      '로보틱스와 자가운전\n' +
      '\n' +
      'SE(3) Action Space를 시뮬레이션하는 것은 로봇 학습에서 오랜 과제 중 하나인 Sim-to-real transfer (Rusu et al., 2017)에 관한 것으로, 시뮬레이터에서 훈련된 정책들이 실제 로봇에서의 실행으로 전이되지 못하는 경우이다. Yang et al.(2023)은 언어 테이블 환경(Lynch et al., 2023)으로부터 실제 로봇 비디오 데이터에 대해 간단한 Cartesion action space로 액션 조건화된 다음 프레임 예측 모델을 학습할 수 있음을 증명하였다. 그림 8에서 다음 프레임 예측이 SE(3) 공간(Blanco-Claraco, 2021)에서 보다 일반적인 엔드 이펙터 액션의 시각적 효과를 예측할 수 있음을 보여준다.\n' +
      '\n' +
      '생성 SE(3) 시뮬레이터의 즉각적인 사용 사례 중 하나는 로봇 정책을 평가하는 것인데, 이는 실제 로봇 평가와 관련된 안전 고려 사항을 고려할 때 특히 유용하다. 평가와는 별개로, Yang et al.(2023)은 언어 테이블 환경에서 생성 시뮬레이터로부터의 롤아웃을 사용하여 RL 정책을 훈련시켰다. 흥미로운 다음 단계는 Dyna 스타일 알고리즘(Sutton, 1991)을 사용하여 시뮬레이션된 롤아웃과 실제 환경 모두에서 정책을 배우는 것이다. 이러한 환경에서 정책이 실행됨에 따라 실제 비디오가 수집되며, 이는 생성의 추가 시연 및 피드백 역할을 한다.\n' +
      '\n' +
      '도 8: **SE(3) 로봇 액션의 생성 시뮬레이션.** 로봇 정책의 실제 실행(상단), 동일한 정책의 모의 실행(중간), 동일한 액션을 반복하는 모의 실행(하단). 시뮬레이션된 롤아웃은 일반적으로 지상진실 롤아웃과 일치하지만 병이 사라짐에 따라 환각이 발생할 수 있다(하단 열).\n' +
      '\n' +
      '도 7: **Generated Interactive Game Environments: 두 개의 합성 이미지 프롬프트가 모델에 전달되었고(Bruce et al., 2024), 이는 이들을 대화형 환경으로 변환한다. 거기에서 플레이어 1과 2로 표시된 서로 다른 잠재 행동을 취하여 다양한 궤적을 생성할 수 있다.**\n' +
      '\n' +
      '시뮬레이터. 마지막으로 생성 시뮬레이터는 다양한 환경에서 비디오 롤아웃을 통해 멀티 태스크 및 멀티 환경 정책에 대한 효과적인 교육을 가능하게 할 수 있다. 정책은 일반적으로 한 번에 단일 실제 환경에만 접근할 수 있기 때문에 이전에는 불가능했다.\n' +
      '\n' +
      '도메인 랜덤화.로봇 공학, 네비게이션 및 자율 주행에 광범위하게 적용 가능한 생성 시뮬레이터의 또 다른 이점은 시뮬레이션에서 훈련된 정책의 실제 이전을 개선하기 위해 훈련 환경에 자연 무작위성을 도입하는 능력이다. 생성 모델 없이, 이것은 하드 코딩 렌더링 규칙(Tobin et al., 2017)에 의한 도메인 랜덤화를 통해 달성되며, 이는 지루하고 제한된 환경 변동 및 비현실적인 렌더링 효과를 초래한다. 생성 시뮬레이터를 사용하여, 최근의 작업은 상이한 운전 조건들(예를 들어, 맑음, 안개, 눈, 비, 밤에)이 시뮬레이터에 도입될 수 있다는 것을 보여주었다(Hu et al., 2023). 또한 인터넷 규모의 지식과 결합하여 다양한 위치와 기상 조건을 가진 자율 주행 정책을 훈련할 수 있는 그림 9와 같이 금문교에서 빗속에서 운전을 시뮬레이션하는 것과 같은 특정 위치에서 운전 조건을 시뮬레이션할 수 있다.\n' +
      '\n' +
      '### 과학 및 공학\n' +
      '\n' +
      '비디오는 의료 영상, 컴퓨터 이미지 처리 및 전산 유체 역학(Steinman, 2002)과 같은 연구 분야에 영향을 미치면서 광범위한 이공계 영역에 걸쳐 통합된 표현으로 작용할 수 있다. 시각 정보가 카메라에 의해 쉽게 캡처될 수 있지만 기본 동적 시스템이 식별하기 어려운 상황(예: 구름 이동, 전자 현미경 하에서의 원자 이동)에서, 제어 입력 상에 조건화된 비디오 생성 모델은 효과적인 시각적 시뮬레이터가 될 수 있으며, 이는 용어상 더 나은 제어 입력을 도출하기 위해 사용될 수 있다. 도 10에서, 우리는 Schwarzer et al.(2023)로부터 수집된 STEM 데이터를 사용하여 주사 투과 전자 현미경(STEM)의 전자 빔에 의해 자극될 때 탄소 원자의 단일 층 상의 실리콘 원자의 전이 역학을 예시한다. 우리는 생성 시뮬레이터가 픽셀 공간에서 실리콘 원자의 움직임을 특성화할 수 있음을 알 수 있다.\n' +
      '\n' +
      '제어 입력에 대한 응답으로 매우 사실적인 시각적 시뮬레이터를 사용하면 전자 현미경과 같은 특수 장비를 작동해야 하는 과학 연구 노력에서 제한된 하드웨어 접근 문제를 완화할 수 있다. 그러나 제어 입력 최적화를 위한 시각적 생성 시뮬레이터를 활용하려면 그 타당성과 유효성을 보장하기 위한 추가 조사가 필요하다.\n' +
      '\n' +
      '과학 과정을 시뮬레이션하는 데 있어 심 대 실제 격차를 줄이는 것 외에도 생성 시뮬레이터의 또 다른 이점은 전통적인 계산 방법이 어려울 때 유익할 수 있는 고정된 계산 오버헤드를 가지고 있다는 것이다. 예를 들어, 열량계 샤워를 시뮬레이션하려면 전자 간의 쌍별 상호작용을 계산해야 하는데, 그 복잡성은 전자 수가 많을 때 빠르게 비실용화된다(Mikuni and Nachman, 2022). 반면에 전자 샤워의 비디오는 샤워가 모델링되는 해상도에 비례하여 고정된 계산 오버헤드를 갖는다.\n' +
      '\n' +
      '## 5 Challenges\n' +
      '\n' +
      '비디오 생성은 큰 잠재력을 가지고 있지만 응용 프로그램에 대한 몇 가지 주요 과제는 여전히 남아 있다. 우리는 이 섹션에서 이러한 도전과 잠재적인 해결책을 요약한다.\n' +
      '\n' +
      '### Dataset Limitations\n' +
      '\n' +
      '제한된 커버리지.언어 모델링에서 특정 다운스트림 작업을 해결하기 위한 언어 데이터의 분포는 일반적으로 인터넷 텍스트 데이터의 분포 내에 있다. 그러나, 비디오의 경우는 그렇지 않다. 인터넷에 게시된 비디오는 인간의 관심에 맞춰져 있으며, 이는 반드시 다운스트림 작업에 유용한 비디오 데이터는 아니다. 예를 들어, 전산 유체 역학 모델은 물과 같은 유체의 이동에 초점을 맞춘 많은 긴 비디오를 필요로 할 것이며, 그러한 비디오는 인간에게 매우 흥미롭지 않을 것이며 따라서 인터넷에서는 부족할 것이다. 유사하게, 인터넷에서 특정 작업(예를 들어, 옷 접기)을 수행하는 특정 유형의 로봇(예를 들어, 프랑카 에미카 판다 로봇)을 찾는 것은 이례적이다. 이것은 도메인 특정 비디오 데이터를 수집하고 배포하기 위한 더 나은 촉진을 요구한다. 로보틱스를 위한 Open-X 실시예 데이터세트는 그러한 하나의 예이다(Padalkar et al., 2023).\n' +
      '\n' +
      '그림 10: **원자 수준 다음 프레임 예측. 조건부 프레임, 진정한 다음 프레임 및 생성된 다음 프레임은 전자 현미경의 전자 빔에 의해 자극된 그래핀 시트에 실리콘 원자의 시각적 역학을 반영한다. 생성 모델은 높은 충실도로 시각적 역학을 모델링할 수 있다.**\n' +
      '\n' +
      '그림 9: ** 자가 운전을 위한 생성 시뮬레이션. 인터넷 지식을 가지고, 우리는 "금문교에서 비", "요세미티에서 새벽", "요세미티로 가는 길에 눈"과 같은 특정 장소에서 다양한 운전 조건을 시뮬레이션할 수 있다.\n' +
      '\n' +
      '제한된 레이블.비디오 모델링의 또 다른 과제는 주석이 달린 비디오의 부족입니다. 예를 들어, MineDojo 데이터셋(Fan et al., 2022)은 30만 시간 이상의 인간이 게임 Minecraft를 플레이하고 있지만, 데이터셋은 언어 전사만 있을 뿐 게임 액션 레이블이 없어 이 데이터셋을 이용한 정책이나 환경 모델 훈련이 어렵다. 유사하게, 가장 큰 오픈-소스 로보틱스 데이터세트(Padalkar et al., 2023)에서, 많은 로봇 궤적은 수행되고 있는 작업들에 대한 언어 주석을 갖지 않거나, "임의의 객체와 상호작용"과 같은 일반 라벨만을 갖는다.\n' +
      '\n' +
      '더 많은 비디오 데이터를 라벨링하기 위해, 선행 작업은 텍스트-투-이미지/비디오 모델들을 트레이닝하기 위해 추가로 사용될 수 있는 추가적인 텍스트 라벨들을 제공하기 위해 이미지/비디오 캡션링 모델들을 활용하였다(Betker et al., 2023; Blattmann et al., 2023a). 이것은 비디오 프리트레이닝(VPT)(Baker et al., 2022)과 유사하지만, VPT는 텍스트 데이터와 대조적으로 액션 데이터로 비디오를 라벨링한다. 또 다른 가능성은 비디오로부터 추론된 잠재된 액션/기술을 레버리지하는 것이다(Edwards et al., 2019; Rybkin et al., 2018; Ye et al., 2022). 가장 큰 스케일 예는 Bruce et al.(2024)이다. 부록 B의 그림 13에서 잠재 행동의 예를 보여준다. 학습된 잠재 행동의 일관성에도 불구하고, 이 접근법이 더 복잡하고 다양한 역학으로 확장될 수 있는지에 대한 열린 질문으로 남아 있다.\n' +
      '\n' +
      '### Model Heterogeneity\n' +
      '\n' +
      '언어 모델이 자기 회귀 아키텍처에 수렴된 방식과 달리 비디오 생성은 아직 최상의 접근 방식에 정착하지 못했다. 자기 회귀 모델, 확산 모델 및 마스킹 모델은 각각 고유한 장점과 단점을 가지고 있다.\n' +
      '\n' +
      '확산 모델.확산 모델(Sohl-Dickstein et al., 2015; Ho et al., 2022)(섹션 3.3에서 사용된 모델 등)은 크게 두 가지 장점이 있다. 첫째, 토큰화를 요구하지 않고 연속적인 출력 공간을 쉽게 모델링할 수 있어 더 나은 생성 품질로 이어질 수 있다. 둘째, 다수의 프레임을 병렬로 샘플링할 수 있다. 그러나 확산 모델의 샘플링 속도는 여전히 상당히 느리기 때문에 실시간 시뮬레이션에 적용하기에는 한계가 있다. 또한, 확산 모델로 긴 비디오 시퀀스를 생성하는 방법은 불분명하다. 확산 모델들은 또한 잡음 스케줄들(Croitoru et al., 2023)과 같은 하이퍼파라미터들에 민감하여 트레이닝 및 스케일링을 어렵게 하는 것으로 알려져 있다.\n' +
      '\n' +
      '자동 회귀 모델. 토큰화된 출력 공간을 갖는 자동 회귀 모델(예: 섹션 4.1에서 언급된 모델)은 확산 모델보다 상대적으로 훈련하기가 더 쉽다. 토큰화는 또한 비디오 생성이 텍스트 또는 이산 액션 생성과 통합될 수 있게 하여, 멀티모달 생성을 필요로 하는 더 많은 애플리케이션들을 개방한다(Team et al., 2023). 추가적으로, 자기회귀 모델들은 컨텍스트 길이(Dai et al., 2019; Yan et al., 2023; Bai et al., 2023)로 잘 스케일링되어, 프레임들의 매우 긴 시퀀스들을 잠재적으로 모델링할 수 있게 한다. 그러나, 자기회귀 디코딩은 각각의 토큰이 순차적으로 예측되어야 하기 때문에 계산적으로 비용이 많이 든다. 또한, 자동 회귀 부트스트랩된 비디오는 표류 효과를 겪을 수 있다(Weng et al., 2023).\n' +
      '\n' +
      'Masked Models.models based on masked reconstruction(예: Section 4.1에서 새로운 게임 환경을 생성하기 위해 사용된 모델과 같은)는 확산의 이점 중 일부를 활용하고 이미지 토큰의 배치를 병렬로 샘플링함으로써 토큰-자동 회귀 모델링의 일부 문제를 완화할 수 있다(Chang et al., 2022). 이를 통해, Bruce et al.(2024)에서와 같이 수십 개의 모델 호출만으로 수천 개의 토큰으로 구성된 이미지를 샘플링할 수 있다. 그러나 이 접근법은 개별 샘플링 단계 내에서 독립성 가정에 의해 도입된 샘플링 편향과 같은 문제를 도입한다.\n' +
      '\n' +
      '더 나은 미래 모델.모델 이질성에 대한 잠재적인 해결책은 자기회귀 및 마스킹된 모델을 조합하는 것(Yan et al., 2023) 또는 자기회귀 및 확산 모델을 조합하는 것(Weng et al., 2023)과 같은 상이한 모델의 장점을 조합하는 것을 요구할 수 있다. 또한, 비디오 데이터는 공간적 및 시간적으로 중복 정보를 포함할 수 있다. 미래 모델은 중복성을 줄이기 위해 잠재 공간 학습을 고려할 수 있다. 더 나은 비디오 생성 모델은 또한 기존 모델에 걸친 생성 속도와 장기적인 일관성의 현재 문제를 해결해야 한다.\n' +
      '\n' +
      '### Hallucination\n' +
      '\n' +
      '비디오 생성에서 환각은 다양한 유형의 모델에 걸쳐 일반적이다. 예를 들어, 객체들은 무작위로 출현하거나 사라질 수 있다(도 8 하단 행 및 부록 B.5 참조). 이것은 물체들이 종종 작기 때문에 물체들에 대한 손실 중량이 배경들에 대한 손실 중량만큼 높지 않기 때문일 수 있다. 또 다른 유형의 공통 환각은 믿을 수 없는 역학, 예를 들어 컵을 잡는 로봇이 아닌 로봇 손으로 컵 "점프"하는 것을 포함한다. 이것은 정확한 모션-임계 프레임들을 캡처하지 않는 거친 시간적 주파수를 갖는 비디오들 때문일 수 있다. 나아가, 행동과 동역학을 동시에 모델링하는 생성 모델은 행동이나 동역학에 의한 시각적 변화를 구별하지 못할 수 있다(Yang et al., 2022). 환각은 또한 사용자 입력이 특정 장면, 예를 들어 "세척 손"이 테이블-탑 로봇에 주어질 때 비현실적일 때 발생할 수 있다. 그럼에도 불구하고, 영상 생성 모델은 그림 11과 같이 비현실적인 사용자 입력을 충족시키기 위해 자기 중심적 움직임을 활용하여 사실적인 영상을 생성하려고 시도하는 것을 볼 수 있다. 외부 피드백을 통한 강화 학습과 같은 방법은 영상 생성 모델에서 환각을 더욱 줄이기 위해 적용될 수 있다.\n' +
      '\n' +
      '### Limited Generalization\n' +
      '\n' +
      '임의의 이미지 및 텍스트 입력으로부터 비디오를 생성하는 것은 어려웠다. 이것은 특히 섹션 5.1에서 논의된 제한된 데이터 커버리지 도전으로 인해 훈련 데이터로 잘 표현되지 않는 도메인에 대해 해당되며, 이는 실제로 매우 일반적이다. 확산 모델을 예로 들면, 오버피팅을 방지하기 위해 낮은 해상도 비디오에 이어서 공간 초해상도로 트레이닝하는 것이 일반적이다(Ho et al., 2022; Bar-Tal et al., 2024; Xing et al., 2023). 우리는 고해상도 이미지/비디오가 인간의 눈에 보이지 않는 너무 많은 고주파 정보를 가지고 있으며, 그것에 초점을 맞추면 일반화가 부족하다고 가정한다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 비디오 생성이 디지털 세계에 대한 언어 모델링으로서 물리적 세계에 대한 것이라는 입장을 취했다. 우리는 언어 모델과 유사하게 비디오가 광범위한 정보와 작업을 표현할 수 있는 방법을 보여줌으로써 이러한 입장을 지지해 왔다. 추론, 상황 내 학습, 검색, 계획 및 강화 학습과 결합된 비디오 생성 응용 프로그램에 대한 이전 작업과 새로운 관점을 추가로 설명했다. 환각과 일반화와 같은 도전에도 불구하고, 비디오 생성 모델은 자율적인 에이전트, 기획자, 환경 시뮬레이터, 컴퓨팅 엔진이 될 가능성이 있으며, 결국 물리적 세계에서 사고하고 행동하는 인공 두뇌 역할을 할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Antol et al. (2015) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question answering. In _Proceedings of the IEEE international conference on computer vision_, pp. 2425-2433, 2015.\n' +
      '* Antonoglou et al. (2022) Antonoglou, I., Schrittwieser, J., Ozair, S., Hubert, T. K., and Silver, D. Planning in stochastic environment with a learned model. _International Conference on Learning Representations_. ICLR, 2022년\n' +
      '* Bai et al. (2023) Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A., Darrell, T., Malik, J., and Efros, A. A. Sequential Modeling enables scalable learning for large vision models. _ arXiv preprint arXiv:2312.00785_, 2023.\n' +
      '* Baker et al. (2022) Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video Preraining (vpt): 레이블이 없는 온라인 비디오를 시청하여 행동하는 학습. _ 신경 정보 처리 시스템_, 35:24639-24654, 2022에서의 발전.\n' +
      '* Bamford & Lucas (2020) Bamford, C. and Lucas, S. M. Neural game engine: 픽셀들로부터 일반화 가능한 전방 모델들의 정확한 학습. In _2020 IEEE Conference on Games (CoG)_, pp. 81-88, 2020. doi: 10.1109/CoG47356.2020.9231688.\n' +
      '* Bar et al. (2022) Bar, A., Gandelsman, Y., Darrell, T., Globerson, A., and Efros, A. Visual prompting via image inpainting. _ 신경 정보 처리 시스템_, 35:25005-25017, 2022에서의 발전.\n' +
      '* Bar-Tal et al. (2024) Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Li, Y., Michaeli, T., et al. Lumiere: 영상 생성을 위한 시공간 확산 모델. _ arXiv preprint arXiv:2401.12945_, 2024.\n' +
      '* Bellemare et al. (2013) Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. 아케이드 학습환경: 일반 에이전트를 위한 평가 플랫폼. _ Journal of Artificial Intelligence Research_, 47:253-279, 2013.\n' +
      '* Betker et al. (2023) Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. 더 나은 캡션을 갖는 이미지 생성 개선. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2:3, 2023.\n' +
      '* Black et al. (2023) Black, K., Nakamoto, M., Atreya, P., Walke, H., Finn, C., Kumar, A., and Levine, S. 사전 학습된 영상 편집 확산 모델을 이용한 제로 샷 로봇 조작 arXiv preprint arXiv:2310.10639_, 2023.\n' +
      '* Blanco-Claraco (2021) Blanco-Claraco, J. L. A tutorial on se(3) transformation parameterizations and onmanifold optimization. _ arXiv preprint arXiv:2103.15980_, 2021.\n' +
      '* Blattmann et al. (2023a) Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: latent video diffusion models to large datasets. _ arXiv preprint arXiv:2311.15127_, 2023a.\n' +
      '* Blattmann et al. (2023b) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. 잠복 시간 정렬: 잠재 확산 모델과 고해상도 비디오 합성 arXiv preprint arXiv:2304.08818_, 2023b.\n' +
      '* Bruce et al.(2020) Bruce, J., Dennis, M., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., Aytar, Y., Bechtle, S., Behbahani, F., Chan, S.,\n' +
      '\n' +
      '도 11: **비현실적 명령어로부터의 생성.** 비디오 생성 모델에 대한 입력 이미지는 로봇 핸드가 있는 테이블 상단이다. 언어 지침은 "손 씻기"이다. 비디오 모델은 언어 지시를 사실적으로 이행하기 위해 테이블 상단에서 주방 싱크대로 이동하기 위해 자기 중심적인 동작을 생성할 수 있다.\n' +
      '\n' +
      'Heess, N., Gonzalez, L., Osindero, S., Ozair, S., Reed, S., Zhang, J., Zolna, K., Clune, J., de Freitas, N., Singh, S., and Rocktaschel, T. 지니: 생성적 상호작용 환경, 2024.\n' +
      '*Chan et al. (2023) Chan, H., Mnih, V., Behbahani, F., Laskin, M., Wang, L., Pardo, F., Gazeau, M., Sahni, H., Horgan, D., Baumli, K., Schroecker, Y., Spencer, S., Steigerwald, R., Quan, J., Comanici, G., Flennerhag, S., Neitz, A., Zhang, L. M., Schaul, T., Singh, S., Lyle, C., Rocktaschel, T., Parker-Holder, J., and Holsheimer, K. 비전 언어 모델은 보상의 원천입니다. _Second Agent Learning in Open-Endedness Workshop_, 2023.\n' +
      '* Chang et al. (2022) Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked Generative Image Transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 11315-11325, 2022.\n' +
      '* Cobbe et al. (2020) Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leveraging procedural generation to benchmark reinforcement learning. In _International conference on machine learning_, pp. 2048-2056. PMLR, 2020.\n' +
      '* Croitoru et al. (2023) Croitoru, F.-A., Hondru, V., Ionescu, R. T., and Shah, M. 비전에서의 확산 모델: 설문조사. _ IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* Dai et al.(2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. 트랜스포머-xl: 고정 길이 컨텍스트를 넘어서는 주의 언어 모델. _ ArXiv preprint arXiv:1901.02860_, 2019.\n' +
      '* Dennett (1993) Dennett, D. C. _Consciousness explained_. 1993년 펭귄 우즈\n' +
      '* Dosovitskiy et al.(2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. 이미지는 16x16 단어의 가치가 있습니다: 스케일에서 이미지 인식을 위한 트랜스포머입니다. _International Conference on Learning Representations_, 2021.\n' +
      '* Du et al. (2023a) Du, Y., Konyushkova, K., Denil, M., Raju, A., Landon, J., Hill, F., de Freitas, N., and Cabi, S. 비젼 언어 모델은 성공 탐지기로 사용됩니다. In _Proceedings of The 2nd Conference on Lifelong Learning Agents_, pp.120-136, 2023a.\n' +
      '* Du et al. (2023b) Du, Y., Yang, M., Dai, B., Dai, H., Dai, H., Nachum, O., Tenenbaum, J. B., Schuurmans, D., and Abbeel, P. Learning universal policies via text-guided video generation. _ arXiv preprint arXiv:2302.00111_, 2023b.\n' +
      '* Du et al. (2023c) Du, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter, B., Sermanet, P., Yu, T., Abbeel, P., Tenenbaum, J. B., et al. Video language planning. _ arXiv preprint arXiv:2310.10625_, 2023c.\n' +
      '* Edwards et al. (2019) Edwards, A., Sahni, H., Schroecker, Y., and Isbell, C. Imitating latent policies from observation. In _International conference on machine learning_, pp. 1755-1763. PMLR, 2019.\n' +
      '* Efros & Freeman(2023) Efros, A. A. and Freeman, W. T. Image Quilting for texture synthesis and transfer. _Seminal Graphics Papers: Pushing the Boundaries, Volume 2_, pp. 571-576. 2023.\n' +
      '* Escontrela et al. (2023) Escontrela, A., Adeniji, A., Yan, W., Jain, A., Peng, X. B., Goldberg, K., Lee, Y., Hafner, D., and Abbeel, P. Video prediction models for reward for reinforcement learning. 30-7차 신경 정보 처리 시스템 회의에서_, 2023.\n' +
      '* Faccio & Velten (2018) Faccio, D. and Velten, A. A trillion frame per second: light-in-flight-in-flight photography의 기법 및 응용 Reports on Progress in Physics_, 81(10):105901, 2018.\n' +
      '* Fan et al. (2022) Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedjo: Building open-ended embodied agent with internet-scale knowledge. 제36차 신경망 정보 처리 시스템 데이터 세트 및 벤치마크 Track_에서 2022년.\n' +
      '* Guo et al. (2023) Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _ arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* Gupta et al. (2022) Gupta, A., Tian, S., Zhang, Y., Wu, J., Martin-Martin, R., and Fei-Fei, L. Maskvit: 비디오 예측을 위한 마스킹된 시각적 사전 훈련. _ arXiv preprint arXiv:2206.11894_, 2022.\n' +
      '* Hafner et al. (2020) Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mastering atari with discrete world models. _ arXiv preprint arXiv:2010.02193_, 2020.\n' +
      '* He et al. (2023) He, Y., Xia, M., Chen, H., Cun, X., Gong, Y., Xing, J., Zhang, Y., Wang, X., Weng, C., Shan, Y., et al. Animate-a-story: Storyelling with retrieval-augmented video generation. _ arXiv preprint arXiv:2307.06940_, 2023.\n' +
      '* Himakunthala et al. (2023) Himakunthala, V., Ouyang, A., Rose, D., He, R., Mei, A., Lu, Y., Sonar, C., Saxon, M., and Wang, W. Vip을 사용하여 프레임 단위로 생각합시다: 비디오 사고 사슬을 평가하기 위한 비디오 채우기 및 예측 데이터 세트. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 204-219, 2023.\n' +
      '* Ho & Salimans(2022) Ho, J. and Salimans, T. 분류자가 없는 확산 안내. _ ArXiv:2207.12598_, 2022.\n' +
      '* Ho et al. (2022a) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. _ arXiv preprint arXiv:2210.02303_, 2022a.\n' +
      '\n' +
      '* Ho et al. (2022) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. _ arXiv preprint arXiv:2204.03458_, 2022b.\n' +
      '* Hu et al. (2023) Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J., and Corrado, G. Gaia-1: A generative world model for autonomous driving. _ arXiv preprint arXiv:2309.17080_, 2023.\n' +
      '* Huang & Chang (2022) Huang, J. and Chang, K. C.-C. 대형 언어 모델에서의 추론에 관한 연구: 설문조사. _ ArXiv:2212.10403_, 2022.\n' +
      '* Huang et al. (2022) Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planner: Extraction actionable knowledge for embodied agent. In _International Conference on Machine Learning_, pp. 9118-9147. PMLR, 2022.\n' +
      '* Ichter & Pavone (2019) Ichter, B. and Pavone, M. 학습된 잠재공간에서의 로봇 동작 계획 IEEE Robotics and Automation Letters_, 4(3):2407-2414, 2019.\n' +
      '*Justesen et al. (2018) Justesen, N., Torrado, R. R., Bontrager, P., Khalifa, A., Togelius, J., and Risi, S. 절차적 수준 생성을 통한 심층 강화 학습에서의 일반화를 조명한다. _ CoRR_, abs/1806.10729, 2018.\n' +
      '* Kang et al. (2023) Kang, X., Ye, W., and Kuo, Y. - L. 계층적 목표 조건 정책에 대한 상상된 하위 목표. The _CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP)_, 2023.\n' +
      '* Kashin et al. (2021) Kashin, A. S., Boiko, D. A., and Ananikov, V. P. Neural Network analysis of electron microscopy video data reveals the temperature-driven microphase dynamics in the ions/water system. _ Small_, 17(24):2007726, 2021.\n' +
      '* Kim et al. (2020) Kim, S. W., Zhou, Y., Philion, J., Torralba, A., and Fidler, S. 게임GAN으로 동적 환경 시뮬레이션 학습 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun. 2020년\n' +
      '* Ko et al. (2023) Ko, P.-C., Mao, J., Du, Y., Sun, S. - H., and Tenenbaum, J. B. Learning to actionless video through dense correspondences, 2023.\n' +
      '* Kondratyuk et al. (2023) Kondratyuk et al. (2023) Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al. Videopoet: Zero-shot 비디오 생성을 위한 큰 언어 모델. _ arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* Li et al. (2023) Li, Z., Tucker, R., Snavely, N., and Holynski, A. Generative image dynamics. _ arXiv preprint arXiv:2309.07906_, 2023.\n' +
      '* Loeschcke et al. (2022) Loeschcke, S., Belongie, S., and Benaim, S. 비디오 객체의 텍스트 기반 스타일화. In _European Conference on Computer Vision_, pp. 594-609. Springer, 2022.\n' +
      '* Lynch et al. (2023) Lynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., and Florence, P. Interactive language: 실시간으로 로봇과 대화하는 언어 IEEE Robotics and Automation Letters_, 2023.\n' +
      '* Mialon et al. (2023) Mialon, G., Dessi, R., Lomeli, M., Nalmpantis, C., Passunuru, R., Raileanu, R., Roziere, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: survey. _ arXiv preprint arXiv:2302.07842_, 2023.\n' +
      '* Mikuni & Nachman (2022) Mikuni, V. and Nachman, B. Score based generatingative models for calorimeter shower simulation. _ Physical Review D_, 106(9):092009, 2022.\n' +
      '* 민스키(1988) 민스키, M. _ 마음의 사회. 1988년 사이먼과 슈스터\n' +
      '*Mnih et al.(2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. _ nature_, 518(7540):529-533, 2015.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744, 2022에서의 발전.\n' +
      '* Padalkar et al. (2023) Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models. _ arXiv preprint arXiv:2310.08864_, 2023.\n' +
      '* Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. _ arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* Razavi et al. (2019) Razavi, A., Van den Oord, A., and Vinyals, O. vq-vae-2. _Advances in neural information processing systems_, 32, 2019의 다양한 고충실도 영상을 생성한다.\n' +
      '* Risi & Togelius (2020) Risi, S. and Togelius, J. Increing generality in machine learning through procedure content generation. _ Nature Machine Intelligence_, 2, 08 2020. doi: 10.1038/s42256-020-0208-z.\n' +
      '* Rusu et al. (2017) Rusu, A. A., Vecerik, M., Rothorl, T., Heess, N., Pascanu, R., and Hadsell, R. 진행성 그물이 있는 픽셀에서 실제 로봇으로 학습합니다. In _Conference on robot learning_, pp. 262-270. PMLR, 2017.\n' +
      '* Rybkin et al. (2018) Rybkin, O., Pertsch, K., Derpanis, K. G., Daniilidis, K., and Jaegle, A. Learning what you can do anything before doing anything. _ arXiv preprint arXiv:1806.09655_, 2018.\n' +
      '* Schrittwieser et al. (2018) Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap, T. P., and Silver, D. Mastering atari, go, chess and shogi by planning with the learned model. _ Nature_, 588:604 - 609, 2019. URL[https://api.semanticscholar.org/CorpusID:208158225](https://api.semanticscholar.org/CorpusID:208158225).\n' +
      '* Schwarzer et al. (2023) Schwarzer, M., Farebrother, J., Greaves, J., Roccapriore, K., Cubuk, E., Agarwal, R., Courville, A., Bellemare, M., Kalinin, S., Mordatch, I., et al. Scanning Transmission Electron Microscope를 이용하여 그래핀에서 실리콘 도펀트 전이를 학습한다. In _AI for Accelerated Materials Design-NeurIPS 2023 Workshop_, 2023.\n' +
      '* Searle & Minds (1980) Searle, J. R. Minds, brains and programs. _ Behavioral and brain sciences_, 3(3):417-424, 1980.\n' +
      '* Silver et al. (2017) Silver, D., van Hasselt, H., Hessel, M., Schaul, T., Guez, A., Harley, T., Dulac-Arnold, G., Reichert, D. P., Rabinowitz, N. C., Barreto, A., and Degris, T. 예측자: 종단간 학습과 계획. In Precup, D. and Teh, Y. W. (eds.), _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pp. 3191-3199. PMLR, 2017.\n' +
      '* Singer et al. (2022) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al. Make-a-video: text-video 데이터가 없는 text-to-video 생성. _ arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 In _International Conference on Machine Learning_, pp. 2256-2265. PMLR, 2015.\n' +
      '* Soucek et al. (2023) Soucek, T., Damen, D., Wray, M., Laptev, I., and Sivic, J. Genhowto: Learning to generate action and state transformation from instruction video. _ arXiv preprint arXiv:2312.07322_, 2023.\n' +
      '* Steinman (2002) Steinman, D. A. Image-based computational fluid dynamics modeling in realistic arterial geometry. _ Annals of Biomedical engineering_, 30:483-497, 2002.\n' +
      '* Summerville et al. (2018) Summerville, A., Snodgrass, S., Guzdial, M., Holmgard, C., Hoover, A. K., Isaksen, A., Nealen, A., and Togelius, J. Procedural content generation via machine learning(PCGML). _ IEEE Trans. Games_, 10(3):257-270, 2018.\n' +
      '* Sutton & Dyna(1991) Sutton, R. S. Dyna, learning, planning, reaction을 위한 통합 아키텍처. _ ACM Sigart Bulletin_, 2(4):160-163, 1991.\n' +
      '* Sutton et al. (1999) Sutton, R. S., Precup, D., and Singh, S. MDPs와 semi-MDPs 사이에: 강화 학습에서 시간적 추상화를 위한 프레임워크. _ Artificial Intelligence_, 112:181-211, August 1999. doi:[http://dx.doi.org/10.1016/S0004-3702](http://dx.doi.org/10.1016/S0004-3702)(99)00052-1).\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: Family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tobin et al. (2017) Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P. Domain randomization for transferring deep neural networks from real world. In _2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)_, pp. 23-30. IEEE, 2017.\n' +
      '* Trinh et al. (2024) Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. 인간 시연 없이 티미아드 기하학 풀이 Nature_, 625(7995):476-482, 2024.\n' +
      '* Valmeekam et al. (2023) Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati, S. 계획 벤치: 변화에 대한 계획 및 추론에 대한 대규모 언어 모델을 평가하기 위한 확장 가능한 벤치마크입니다. IMT-2000 3GPP-신경정보처리시스템에 관한 제37차 회의 : 데이터세트 및 벤치마크 Track_, 2023\n' +
      '* Van Den Oord et al. (2017) Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Villalobos et al. (2022) Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. 데이터가 부족할 것인가? 머신 러닝에서 데이터 세트의 스케일링 한계에 대한 분석. _ arXiv preprint arXiv:2211.04325_, 2022.\n' +
      '* Wang et al. (2023a) Wang, S., Saharia, C., Montgomery, C., Pont-Tuset, J., Noy, S., Pellegrini, S., Onoe, Y., Laszlo, S., Fleet, D. J., Soricut, R., et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18359-18369, 2023a.\n' +
      '* Wang et al. (2023b) Wang, X., Wang, W., Cao, Y., Shen, C., and Huang, T. 이미지는 이미지로 말한다: 맥락 내 시각적 학습을 위한 일반주의 화가. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6830-6839, 2023b.\n' +
      '* Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '* Wen et al. (2023) Wen, C., Lin, X., So, J., Chen, K., Dou, Q., Gao, Y., and Abbeel, P. Any-point trajectory modeling for policy learning. _ arXiv preprint arXiv:2401.00025_, 2023.\n' +
      '* Weng et al. (2023) Weng, W., Feng, R., Wang, Y., Dai, Q., Wang, C., Yin, D., Zhao, Z., Qiu, K., Bao, J., Yuan, Y., Luo, C., Zhang, Y., and Xiong, Z. Art-v: 확산 모델을 사용한 자동 회귀 텍스트-비디오 생성, 2023.\n' +
      '\n' +
      '*Xing et al. (2023) Xing, Z., Feng, Q., Chen, H., Dai, Q., Hu, H., Xu, H., Wu, Z., and Jiang, Y. - G. 비디오 확산 모델에 대한 조사. _ arXiv preprint arXiv:2310.10647_, 2023.\n' +
      '* Yadav et al. (2011) Yadav, A., Phillips, M. M., Lundeberg, M. A., Koehler, M. J., Hilden, K., and Dirkin, K. H. 만약 사진이 천 단어 가치가 있다면 비디오는 백만 단어 가치가 있는가? 비디오 및 텍스트 사례의 정의적 및 인지적 처리의 차이. _ Journal of Computing in Higher Education_, 23:15-37, 2011.\n' +
      '* Yan et al. (2023) Yan, W., Hafner, D., James, S., and Abbeel, P. Temporally consistent transformer for video generation. In _International Conference on Machine Learning_, pp. 39062-39098. PMLR, 2023.\n' +
      '* Yang et al. (2022a) Yang, M., Schuurmans, D., Abbeel, P., and Nachum, O. 통제의 이분법: 통제할 수 있는 것과 통제할 수 없는 것을 분리합니다. _ arXiv preprint arXiv:2210.13435_, 2022a.\n' +
      '* Yang et al. (2023a) Yang, M., Du, Y., Dai, B., Schuurmans, D., Tenenbaum, J. B., and Abbeel, P. Probabilistic adaptation of text-to-video models. _ arXiv preprint arXiv:2306.01872_, 2023a.\n' +
      '* Yang et al. (2023b) Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuurmans, D., and Abbeel, P. Learning interactive real-world simulators. _ arXiv preprint arXiv:2310.06114_, 2023b.\n' +
      '* Yang et al. (2022b) Yang, M. S., Schuurmans, D., Abbeel, P., and Nachum, O. 프로시저 복제와 함께 생각의 끈 신경 정보 처리 시스템_, 35:36366-36381, 2022b에서의 발전.\n' +
      '* Yang et al. (2023c) Yang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., and Schuurmans, D. Foundation models for decision decisions: Problems, methods, opportunities. _ arXiv preprint arXiv:2303.04129_, 2023c.\n' +
      '* Yannakakis & Togelius (2018) Yannakakis, G. N. and Togelius, J. _Artificial Intelligence and Games_. Springer, 2018. [https://gameaibook.org](https://gameaibook.org)\n' +
      '* Yao et al. (2023) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. 생각의 나무: 큰 언어 모델을 사용하여 문제를 해결합니다. _ arXiv preprint arXiv:2305.10601_, 2023.\n' +
      '* Ye 등(2022) Ye, W., Zhang, Y., Abbeel, P., and Gao, Y. 순수 동영상 시청을 통해 제한된 데이터로 능숙한 플레이어가 되세요. _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* Yu et al. (2023) Yu, T., Xiao, T., Stone, A., Tompson, J., Brohan, A., Wang, S., Singh, J., Tan, C., Peralta, J., Ichter, B., et al. arXiv preprint arXiv:2302.11550_, 2023.\n' +
      '* Zhu et al. (2023) Zhu, J., Yang, H., He, H., Wang, W., Tuo, Z., Cheng, W. -H., Gao, L., Song, J., and Fu, J. Moviefactory: Automatic movie created from text using large generative models for language and images. _ arXiv preprint arXiv:2306.07257_, 2023.\n' +
      '\n' +
      'Appendix\n' +
      '\n' +
      '## 부록 주요 텍스트에서 예제 생성에 사용된 모델의 세부사항\n' +
      '\n' +
      '### Autoregressive Model\n' +
      '\n' +
      '섹션 4.1의 모델은 시간적으로는 자기회귀적이지만 TECO(Yan et al., 2023)와 유사한 방식으로 각 프레임에 대해 마스킹된 모델(Chang et al., 2022)을 사용한다. 픽셀 관찰의 주어진 궤적 \\(x_{t}\\)에 해당하는 액션 \\(a_{t}\\)에 대해, 우리는 인터리빙된 시퀀스 \\(z_{0},a_{0},z_{1},a_{1},...)을 모델링한다. 여기서 우리는 비전 트랜스포머(Dosovitskiy et al., 2021)와 결합하여 VQVAE(Van Den Oord et al., 2017)를 통해 픽셀 관찰 \\(x_{t}\\)을 토큰 \\(z_{t}\\)으로 인코딩한다. 우리는 VPT(Baker et al., 2022)에 따라 액션들을 토큰화한다. 우리는 Transformer-XL(Dai et al., 2019)을 이용하여 시간 궤적 \\(z_{0},a_{0},z_{1},a_{1},...)을 부호화한다. 시간적으로 정렬된 출력 \\(h_{z_{0}},h_{a_{0}},h_{z_{1}},h_{a_{1}},...)을 갖는다. 마지막 입력이 관측인 단계 즉, \\(h_{z_{t}\\)에 대해, 우리는 문맥 \\(h_{z_{t}\\)을 자기회귀 변압기 헤드에 대한 컨디셔닝 입력으로 사용하여 \\(a_{t}\\)을 예측한다. 마지막 입력이 액션이었다면, 문맥 \\(h_{a_{0}}\\)은 마스킹된 변압기 헤드에 컨디셔닝되어 \\(z_{t}\\)을 모델링한다. MaskGIT 구현은 코사인 마스킹 스케줄과 함께 8단계를 사용합니다. 인터리빙된 트랜스포머의 성능을 더욱 향상시키기 위해 이산화 없이 입력을 이용하여 인터리브된 시퀀스 \\(...,x_{-2},a_{-2},x_{-1},a_{-1}\\)에 별도로 훈련된 동일한 트랜스포머인 _past encoder_를 사용하여 메모리를 초기화한다.\n' +
      '\n' +
      '### Diffusion Model\n' +
      '\n' +
      '도 2, 도 5, 도 8, 도 9 및 도 10에서 예를 생성하기 위해 사용된 확산 모델은 공간 다운샘플링 패스 및 공간 업샘플링 패스에서 인터리브된 3D 어텐션 및 컨볼루션 레이어를 갖는 Ho 등(2022b;a)과 동일한 3D U-Net 아키텍처를 사용한다. 스킵 연결은 다운샘플링 패스 활성화에 적용된다. 모델은 잠재 공간 확산과 반대로 픽셀 공간 확산을 사용한다. 섹션 5에 기술된 바와 같은 비디오 확산에서의 관례들에 따라, 하부 해상도 비디오 생성 모델은 해상도(Baker et al., 2022; Liu et al., 2022)에서 동작하고, 이어서 목표 해상도를 갖는 두 개의 공간 초해상도 모델들(Zhu et al., 2022; Liu et al., 2022) 및 (Zhu et al., 2022; Zhu et al., 2022)이 동작한다. 텍스트 또는 액션 컨디셔닝을 위해 분류기 없는 안내(Ho and Salimans, 2022)가 적용되었다. 프레임 컨디셔닝을 위해, 우리는 컨디셔닝 프레임을 분류기 없는 안내에 사용되는 조건 및 무조건 모델 모두에 입력한다. 그림 8에 표시된 SE(3) 동역학을 시뮬레이션하기 위해 Yang et al. (2023b) 및 Padalkar et al. (2023)과 유사한 액션 이산화를 사용한다.\n' +
      '\n' +
      '### Masked Model\n' +
      '\n' +
      '섹션 4.1에서 새로운 게임 환경을 생성한 (Bruce et al., 2024)의 마스킹된 역학 모델은 제어 가능한 비디오 연속 모델이며, 트랜지션을 나타내는 감독되지 않은 잠재 변수를 조건으로 프레임 수준에서 자동으로 출력을 생성한다. 잠재변수는 VQ-VAE 코드(Van Den Oord et al., 2017)\\(\\tilde{a}_{1:T-1}\\)의 이산집합으로 구성되어 있으며, 프레임 \\(x_{1:T}\\)을 조건화하고 인과변압기로 \\(\\hat{x}_{2:T}\\)을 예측하는데 도움이 되도록 최적화하였다. 동역학 모델은 마스킹된 재구성 대물렌즈를 사용하여 훈련된 인터리브된 시간 및 공간 주의력을 갖는 트랜스포머(Gupta et al., 2022)로서, 다음과 같다(Chang et al., 2022). 비디오 토큰은 평균 75%의 비율로 독립적인 랜덤 베르누이 마스크로 마스킹되고, 다이내믹스 모델은 교차 엔트로피 목표를 최소화하여 누락된 토큰을 예측하도록 트레이닝된다.\n' +
      '\n' +
      '추론 시간에 MaskGIT(Chang et al., 2022)에 이어서 병렬로 토큰이 생성된다. 프레임 \\(x_{1:t-1}\\)과 완전 마스킹된 프레임 \\(x_{t}\\)에 대해 마스킹되지 않은 컨텍스트 토큰으로 시작하여 일련의 반복 단계가 수행되며, 각 단계는 \\(x_{1:t}\\)과 \\(\\tilde{a}_{1:t}\\)에 조건화된 모든 토큰에 대한 로짓들을 계산하고, 각각의 남은 마스킹된 위치에 대한 후보 토큰을 샘플링하고, 가장 높은 확률 샘플들은 향후 단계를 위해 잠긴다. (Bruce et al., 2024)에서 각각의 이미지는 920개의 토큰으로 구성되며, 이들은 모두 결국 25 MaskGIT 단계의 과정에 걸쳐 샘플링된다.\n' +
      '\n' +
      '모델은 대규모 비디오 데이터 세트에 대해 완전히 감독되지 않은 상태로 훈련되며, 예를 들어 10.7B 파라미터 모델로부터의 13개의 궤적을 참조하여 감독되지 않은 잠재 행동 목적이 다양한 시각적 프롬프트에 걸쳐 일관된 제어 변수를 초래한다는 것을 입증한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '그림 13: **추가 시뮬레이션 게임 다이내믹스. Bruce 등으로부터 생성된 프레임들(2024). 각각의 프레임은 텍스트-이미지 모델로부터의 초기 합성 프롬프트 이미지 및 감독되지 않은 잠재 액션으로부터 생성된다. 잠재적 행동의 감독되지 않은 특성에도 불구하고, 그들의 의미론은 초기 프레임들에 걸쳐 비교적 일관적이다. 모델의 한 가지 한계는 초기 이미지의 경계 밖에서 비교적 평범한 연속체를 생성하는 경향이며, 하단 행의 점프 열에서 가장 명확하게 입증되었다.**\n' +
      '\n' +
      '비디오를 위한### 추가 생성\n' +
      '\n' +
      '도 14: **추가 생성 방법 비디오.** 일부 생성 프레임은 인간 문의(첫 번째 행 및 마지막 행)에 응답하여 키 프레임을 합성할 수 있지만, 일부 다른 생성 프레임은 너무 일반적이고 사용자의 질문에 완전히 응답하기에 충분한 세부 정보를 캡처하지 못한다.\n' +
      '\n' +
      '### 자가운전 시뮬레이션 추가\n' +
      '\n' +
      '도 15: **Driving을 위한 추가 Generative Simulation.** Generative Simulator는 sunny, rain, snowy, night, dawn과 같이 낮의 다양한 기상 조건 및 시간에 주행을 생성할 수 있다.\n' +
      '\n' +
      '### 추가 로봇 SE(3) 시뮬레이션\n' +
      '\n' +
      '### 환각의 예\n' +
      '\n' +
      '도 16: ** SE(3) 로봇 동작의 추가 생성 시뮬레이션.** 로봇 정책의 실제 실행(빨간색) 및 동일한 정책(파란색)의 시뮬레이션 실행. 시뮬레이션된 롤아웃은 일반적으로 그라운드 트루스 롤아웃과 일치한다.\n' +
      '\n' +
      '그림 17: **세 가지 유형의 모델 모두에서 환각의 예.** 환각의 문제는 다른 유형의 비디오 생성 모델에 걸쳐 지속됩니다. 첫 번째 행에서 자기회귀 모형에 의해 생성된 영상은 재고를 닫은 후 가슴이 사라지는 것을 보여준다. 두 번째 행에서 확산 모델에 의해 생성된 비디오는 추첨에 들어간 후 오렌지가 사라지는 오렌지를 보여준다. 맨 아래 행에서 마스킹된 모델에 의해 생성된 비디오는 클라우드가 경계에서 갑자기 멈춘다는 것을 보여준다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>