<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# AnimateDiff-Lightning: Cross-Model Diffusion Distillation\n' +
      '\n' +
      '양산천린샤오\n' +
      '\n' +
      'ByteDance Inc.\n' +
      '\n' +
      '{peterlin, yangxiao.0}@bytedance.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '번개처럼 빠른 동영상 생성을 위한 AnimateDiff-Lightning을 소개합니다. 우리의 모델은 점진적 적대 확산 증류를 사용하여 몇 단계 비디오 생성에서 새로운 최첨단 기술을 달성한다. 우리는 비디오 촬영 방식에 맞게 수정하기 위해 논의합니다. 또한, 다중 기저 확산 모델의 확률 흐름을 동시에 증류하여 보다 광범위한 스타일 호환성을 갖는 단일 증류 개념 모듈을 생성할 것을 제안한다. 커뮤니티의 사용을 위해 증류된 AnimateDiff-Lightning 모델을 출시하게 되어 기쁩니다.\n' +
      '\n' +
      '+\n' +
      '각주 †: 모델: [https://huggingface.co/ByteDance/AnimateDiff-Lightning](https://huggingface.co/ByteDance/AnimateDiff-Lightning)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 비디오 생성 모델이 큰 관심을 받고 있습니다. 텍스트 대 비디오 모델[30, 36, 44, 6, 2, 4, 3, 4]은 아이디어에서 바로 비디오를 생성할 수 있게 한다; 이미지 대 비디오 모델[36, 4, 2, 4]은 콘텐츠 및 구성에 대한 보다 세밀한 제어를 가능하게 한다; 비디오 대 비디오 모델[4, 6]은 기존의 비디오를 애니메이션 또는 만화와 같은 상이한 스타일로 변환할 수 있다. 비디오 세대의 발전은 새로운 창조적 가능성을 가능하게 했다.\n' +
      '\n' +
      '모든 방법 중에서 AnimateDiff[6]은 가장 인기 있는 동영상 생성 모델 중 하나이다. 동결 이미지 생성 모델을 취하고 학습 가능한 시간적 모션 모듈을 네트워크에 주입한다. 이를 통해 모델은 이미지 이전을 상속하고 제한된 비디오 데이터 세트로부터 시간적으로 일관된 프레임을 생성하는 것을 학습할 수 있다. 이미지 모델의 아키텍처와 가중치는 변하지 않기 때문에, 그것은 놀라운 애니메이션과 만화 비디오인 _etc_를 만들기 위해 훈련 후 광범위한 스타일화된 모델과 교환될 수 있다. 또한, AnimateDiff는 ControlNet[42], T2I-Adapter[22], IP-Adapter[40], _etc_와 같은 이미지 제어 모듈과 호환되어 범용성을 더욱 향상시킨다.\n' +
      '\n' +
      '그러나 속도는 비디오 생성 모델이 더 광범위하게 채택되는 것을 방지하는 주요 장애물 중 하나이다. 최첨단 생성 모델은 반복 확산 프로세스로 인해 느리고 계산 비용이 많이 든다. 이 문제는 비디오 생성에서 더욱 악화됩니다. 예를 들어, AnimateDiff with ControlNet 및 스타일화된 이미지 모델을 사용하는 많은 비디오 스타일화 파이프라인들은 10초 비디오를 처리하는 데 최대 10분이 소요될 수 있다. 품질을 유지하면서 세대를 더 빠르게 만드는 것이 이 작업의 주요 초점입니다.\n' +
      '\n' +
      '확산 증류[11, 13, 17, 18, 20, 21, 28, 29, 31, 32, 35, 41, 43]는 이미지 생성에서 더 널리 연구되어 왔다. 최근 점진적 적대 확산 증류[13]는 몇 단계 이미지 생성의 최첨단 결과를 달성했다. 본 논문에서는 이를 비디오 모델에 처음으로 적용하여 비디오 촬영장비에 대한 이 방법의 적용성과 우수성을 입증한다. 비디오 모델 증류를 위해 특별히 만들어진 디자인과 변화에 대해 논의할 것입니다.\n' +
      '\n' +
      '또한, 다중 기저 확산 모델의 확률 흐름을 동시에 증류하는 것을 제안한다. 특히, 우리는 AnimateDiff가 다양한 양식화된 베이스 모델과 함께 널리 사용된다는 사실을 특별히 고려한다. 그러나, 기존의 모든 방법들은 디폴트 베이스 모델에 대해서만 증류를 수행하고, 새로운 베이스 상으로 스와핑한 후에 증류된 모션 모듈이 여전히 작동하기를 바랄 뿐이다. 실제로 추론 단계가 감소함에 따라 품질이 저하되는 것을 발견한다. 따라서, 우리는 공유된 모션 모듈을 서로 다른 베이스 모델에 명시적으로 동시에 증류하는 것을 제안한다. 우리는 이 접근법이 선택된 기본 모델뿐만 아니라 보이지 않는 기본 모델에서도 품질을 향상시킨다는 것을 발견했다.\n' +
      '\n' +
      '제안된 AnimateDiff-Lightning은 기존의 비디오 증류 방법 AnimateLCM[35]보다 더 적은 추론 단계에서 더 나은 품질의 비디오를 생성할 수 있다. 우리는 커뮤니티의 사용을 위해 증류된 AnimateDiff-Lightning 모델을 출시합니다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### Diffusion Model\n' +
      '\n' +
      '확산 모델[9, 33]은 대부분의 최첨단 비디오 생성 방법 뒤에 있다. 생성에는 잡음 분포\\(t=T\\)에서 데이터 분포\\(t=0\\)으로 샘플\\(x_{t}\\)을 점진적으로 전송하는 확률 흐름[16, 17, 33]이 포함된다. 신경망은 이 흐름의 임의의 위치에서 기울기를 예측하기 위해 학습된다. 흐름이 곡선이고 복잡하기 때문에 생성은 한 번에 기울기를 따라 작은 단계만 밟아야 하며 반복적으로 확장 신경망 평가를 호출한다. 확산 증류는 신경망이 다음 흐름 위치를 더 멀리 예측하도록 훈련시켜 더 큰 보폭과 더 적은 스텝으로 흐름을 횡단할 수 있게 한다.\n' +
      '\n' +
      '진보적 적대적 확산 증류\n' +
      '\n' +
      '진보적 적대적 확산 증류[13]는 점진적 증류[28]와 적대적 손실[5]을 결합하는 것을 제안한다. 구체적으로, 점진적 증류[28]는 교사 네트워크가 보폭의 \\(n\\) 단계를 거친 것처럼 현재의 흐름 위치 \\(x_{t-ns}\\)에서 다음 흐름 위치 \\(x_{t-ns}\\)을 직접 예측하도록 학생 네트워크를 훈련시킨다. 학생이 수렴한 후 교사로 활용되며 추가 증류를 위해 과정이 반복된다:\n' +
      '\n' +
      '\\mathbf{EulerSolver}(f_{\\mathrm{teacher}},x_{t},t,c,n,s) \\tag{1}\\] \\hat{x}_{t-ns} =\\mathbf{EulerSolver}(f_{\\mathrm{student},x_{t},t,c,1,ns)\\](2) \\[\\mathcal{L}_{t-ns}-x_{t-ns}\\|_{t-ns}\\|_{3}\\tag{3}\\hat{x}_{t-ns} =\\mathbf{EulerSolver}(f_{\\mathrm{student},x_{t},t,c,1,ns)\\](2) \\[\\mathcal{L}_{t-ns}-x_{t-ns}\\|_{mse}=\\|\\hat{x}_{t-ns}-x_{t-ns}\\|_{3\n' +
      '\n' +
      '그러나 이론 분석 [13]은 모델 용량 감소로 인해 식 (3)과 같은 평균 제곱 오차(MSE)와의 정확한 매칭이 불가능하므로 품질과 모드 커버리지 간의 트레이드오프에 적대적 손실을 도입한다는 것을 보여주었다. 이 방법은 먼저 흐름 궤적 보존을 강화하기 위해 \\(x_{t}\\)와 캡션 \\(c\\)에 조건화된 판별기 \\(D\\)로 증류하는 것을 제안한다:\n' +
      '\n' +
      '\\[p=D(x_{t},x_{t-ns},t,t-ns,c) \\tag{4}\\] \\[\\hat{p} =D(x_{t},\\hat{x}_{t-ns},t,t-ns,c) \\tag{5}\\]\n' +
      '\n' +
      '그런 다음 품질 향상을 위해 궤적 요구량을 완화하기 위해 \\(x_{t}\\) 조건 없이 판별기 \\(D^{\\prime}\\)로 증류한다:\n' +
      '\n' +
      '\\[p =D^{\\prime}(x_{t-ns},t-ns,c) \\tag{6}\\] \\[\\hat{p} =D^{\\prime}(\\hat{x}_{t-ns},t-ns,c) \\tag{7}\\]\n' +
      '\n' +
      '증류는 교대 반복에서 비포화 적대 손실[5]을 갖는 확산 모델 및 판별기를 훈련시킨다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{D} = -\\log(p)-\\log(1-\\hat{p}) \\tag{8}\\] \\[\\mathcal{L}_{G} = -\\log(\\hat{p}) \\tag{9}\\]\n' +
      '\n' +
      'SDXL-Lightning[13]은 이 증류법으로 1단계/소단계 텍스트-이미지 생성에서 새로운 최신 기술을 달성한다. 우리의 연구는 이 방법을 비디오 확산 증류에 최초로 적용하여 다른 양식에서 방법의 적용 가능성과 우수성을 보여준다.\n' +
      '\n' +
      '### 기타 확산 증류 방법\n' +
      '\n' +
      '확산 증류는 대부분 이미지 생성에서 연구된다. 가장 주목할 만한 것은 LCM(Latent Consistency Model) [20, 21]은 잠상 확산 모델에 대해 일관성 증류[32]를 적용하고; InstaFlow[18]은 샘플링 단계를 줄이기 위한 방법으로 정류 흐름(RF) [17]이라고 하는 기술을 사용하여 흐름을 점진적으로 더 안정하게 만들고; SDXL-Turbo[29]는 스코어 증류 샘플링(SDS) [24]과 함께 적대적 손실을 사용하여 생성을 한 단계로 밀어낸다. SDXL-번개 [13]은 증류에 대한 최신 연구이며 점진적 적대 증류를 사용한 이전 방법에 비해 훨씬 더 나은 품질을 달성한다.\n' +
      '\n' +
      '비디오 확산 증류에 대한 연구는 매우 부족하다. AnimateLCM[35]은 우리가 아는 한 지금까지 비디오 확산 증류에 대한 유일한 작업이다. AnimateDiff에 일관성 증류[32]를 적용하기 위해 LCM[20, 21]을 따른다. AnimateLCM은 8개의 추론 단계로 우수한 품질의 비디오를 생성할 수 있지만 4개의 추론 단계로 아티팩트를 보여주기 시작하고 4개의 추론 단계에서 결과가 흐릿하다.\n' +
      '\n' +
      '플러그러블 모듈로서의 증류\n' +
      '\n' +
      'LCM[21], AnimateLCM[35] 및 SDXL-Lightning[13]은 플러그 가능한 모듈로서 증류를 훈련하는 것을 탐구했다. 모듈에는 동결된 기본 모델 상단에 추가 매개변수가 포함되어 모듈이 훈련 후 다른 양식화된 기본 모델에 이식될 수 있다.\n' +
      '\n' +
      '그러나 증류 모듈은 기본 기본 모델에 대해서만 훈련되며 전체 접근법은 다른 양식화된 기본 모델이 유사한 가중치를 갖는다는 가정에 따라 달라진다. 경험적으로, 우리는 보이지 않는 기본 모델에서 추론 단계가 감소함에 따라 품질이 저하되는 것을 발견한다.\n' +
      '\n' +
      '본 논문에서는 증류 모듈을 여러 기본 모델에 대해 처음으로 명시적으로 동시에 증류하는 방법을 탐구한다. 이렇게 하면 선택한 기본 모델에 대한 품질 보증이 제공됩니다. 또한 보이지 않는 기본 모델의 호환성을 향상시킵니다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '우리는 AnimateDiff[6]를 위해 여러 개의 기저 모델에 대해 동시에 공유 증류 운동 모듈을 훈련할 것을 제안한다. 결과적인 모션 모듈은 상이한 베이스 모델들과의 더 나은 소수의 추론 호환성을 갖는다.\n' +
      '\n' +
      '### 모델 및 데이터 준비\n' +
      '\n' +
      '기본 안정 확산(SD) v1.5 기본 모델[26] 외에도 인기도에 따라 여러 개의 추가 목표 기본 모델을 선택한다. 사실적인 스타일은 RealisticVision v5.1[56]과 epiCRealism[49]를 선택한다. 애니메이션 스타일은 툰유 베타 6[58], IMP v1.0[51], 위조 v3.0[46]을 선택합니다.\n' +
      '\n' +
      '기존의 비디오 데이터셋 WebVid-10M [1]에는 사실적인 주식 비디오 영상만 포함되어 있다. 애니메이션 모델을 증류할 때 샘플이 특히 유통되지 않습니다. 따라서 선택된 모든 기본 모델에 AnimateDiff를 적용하여 대용량 데이터 샘플을 생성한다. 구체적으로 WebVid-10M [1]의 프롬프트를 이용하여 동영상 클립을 생성한다. 우리는 32단계의 DPM-Solver++[19]를 사용하고 음성 프롬프트 없이 분류기 없는 안내(CFG) 척도 7.5를 사용한다. 모든 클립은 16개의 프레임과 512\\(\\times\\)512의 해상도를 갖는다. 총 175만 개의 클립을 생성했습니다.\n' +
      '\n' +
      '### Cross-Model Distillation\n' +
      '\n' +
      'AnimateDiff 모델 \\(F_{i}\\)은 동결 영상 기반 모델 \\(f_{i}\\)과 공유 모션 모듈 \\(m\\)으로 구성되며, 여기서 \\(i\\)은 특정 기반 모델의 인덱스를 나타낸다.\n' +
      '\n' +
      '\\[F_{i}:=f_{i}\\circm\\tag{10}\\]\n' +
      '\n' +
      '증류에서는 모션 모듈의 가중치만 업데이트하고 이미지 베이스 모델의 가중치를 변경하지 않는다. 우리는 서로 다른 GPU 랭크에 서로 다른 이미지 베이스 모델 \\(f_{i}\\)을 로드하고 동일한 AnimateDiff v2 체크포인트로 모션 모듈 \\(m\\)을 초기화한다[6]. 구체적인 과제는 표 1과 같다.\n' +
      '\n' +
      '이 설계를 통해 모션 모듈을 여러 기본 모델에서 동시에 증류할 수 있습니다. GPU들에 걸쳐 상이한 베이스 모델들을 확산시키는 것은 각각의 GPU 상에서 베이스 모델들의 지속적인 스와핑의 필요성을 제거한다. PyTorch 분산 데이터 병렬(DDP) 프레임워크 [23]을 수정하여 동결 이미지 기반 모델의 동기화가 모델 할당을 지우는 것을 방지합니다. 수정 후 기존의 분산 훈련 메커니즘을 사용하여 기울기를 자동으로 누적하여 모든 기본 모델에 대한 정확한 증류를 위한 최적화를 보장한다.\n' +
      '\n' +
      '또한 이미지 베이스 모델에 따라 다른 증류 데이터 세트를 할당한다. 안정적인 확산 기반 모델을 증류하기 위해 WebVid-10M 데이터셋 [1]을 사용한다. 각 사실적 또는 애니메이션 모델을 증류하기 위해 다양성을 개선하기 위해 생성된 모든 데이터를 함께 풀링한다. 우리는 또한 표본 수를 두 배로 늘리기 위해 무작위 수평 플립을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Rank & Base Model & Dataset \\\\ \\hline\n' +
      '0 & 안정확산 v1.5 [26] & WebVid-10M [1] \\\\\n' +
      '1 & Stable Diffusion v1.5 [26] & Generated Realistic \\\\ \\hline\n' +
      '2 & RealisticVision v5.1 [56] & Generated Realistic \\\\\n' +
      '3 & epiCRealism [49] & \\\\ \\hline\n' +
      '4 & ToonYou Beta 6 [58] & \\\\\n' +
      '5 & ToonYou Beta 6 [58] & Generated Anime \\\\\n' +
      '6 & IMP v1.0 [51] & \\\\\n' +
      '7 & Counterfeit v3.0 [46] & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 단일 머신에서 8개의 GPU 랭크에 걸쳐 모델 및 데이터세트 할당. 동일한 구성이 추가 컴퓨터에 복제됩니다.\n' +
      '\n' +
      '### 플로우-조건부 비디오 판별기\n' +
      '\n' +
      '진보적 적대 확산증류[13]는 캡션 \\(c\\)이 주어졌을 때 \\(x_{t}\\)으로부터 \\(x_{t-ns}\\)의 학생 예측이 날카롭고 흐름을 보존할 수 있도록 판별기 \\(D\\)을 사용할 것을 제안한다. 증류는 이제 서로 다른 기본 모델의 여러 흐름을 포함하기 때문에 판별기를 흐름 조건으로 확장해야 한다. 구체적으로, 해당 기저 모델 인덱스 \\(i\\)를 판별기에 제공한다. 이러한 방식으로 판별기는 각각의 기본 모델에 대해 별개의 흐름 궤적을 학습하고 비평할 수 있다:\n' +
      '\n' +
      '{split}&D(x_{t},x_{t-ns},t,t-ns,c,i)\\\\&\\quad:=\\sigma\\bigg{(}\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c,i),d(x_{t},t,c,i)\\Big{}\\bigg{}\\end{split}\\tag{11}\\cad:\n' +
      '\n' +
      '우리는 확산 UNet [27] 인코더와 미드블록을 판별기 백본으로 사용하기 위해 선행 연구 [13, 15]를 따른다. 우리의 경우, SD v1.5 가중치[26]로 초기화된 이미지 베이스 모델과 AnimateDiff v2 가중치[6]로 초기화된 모션 모듈로 구성된 AnimateDiff 아키텍처[6]를 사용한다. 우리는 새로운 학습 가능한 임베딩으로 흐름 조건 \\(i\\)을 포함하고 이를 시간 임베딩에 추가한다. 공유 백본은 독립적으로 \\(d(x_{t-ns},t-ns,c,i)\\)와 \\(d(x_{t},t,c,i)\\)을 처리한다. 결과적인 미드블록 특징들은 예측 헤드로 전달되기 전에 채널 차원을 따라 연결된다. 예측 헤드는 커널 크기가 4이고 보폭이 2인 3D 컨볼루션의 블록, 그룹 정규화[37], SiLU 활성화[7, 25]로 구성되어 차원을 단일 값으로 더 감소시킨다. 마지막으로 시그모이드 함수 \\(\\sigma(\\cdot)\\)는 학생 대신 교사로부터 입력 \\(x_{t-ns}\\)이 생성될 확률을 나타내는 \\([0,1]\\) 범위로 값을 클램프한다. 등뼈를 포함한 전체 판별자가 훈련된다.\n' +
      '\n' +
      '또한 점진적 적대 확산 증류[13]은 각 단계에서 \\(x_{t}\\)의 조건 없이 모델을 더욱 미세화하여 흐름 궤적 보존 요구 사항을 완화하고 품질을 더욱 향상시킬 것을 제안한다. 그러나 흐름 궤적 보존이 완화되었음에도 불구하고 우리는 여전히 학생 예측이 목표 흐름의 분포 내에 있도록 강제해야 한다. 따라서 우리는 또한 이 판별기 \\(D^{\\prime}\\)를 흐름 \\(i\\)을 조건으로 수정한다.\n' +
      '\n' +
      '\\[\\begin{split}&D^{\\prime}(x_{t-ns},t-ns,c,i)\\\\&\\quad:=\\sigma\\bigg{(}\\mathrm{head}\\Big{(}d(x_{t-ns},t-ns,c,i) \\Big{}\\bigg{}\\end{split}\\tag{12}\\cad:\n' +
      '\n' +
      '### Distillation Procedure\n' +
      '\n' +
      '우리는 다음 단계 카운트 순서( \\(128\\~32\\~8\\~4\\~2\\)로 모델을 점진적으로 증류한다. 우리는 평균 제곱 오차(MSE)를 사용하고 분류기 없는 유도(CFG)를 \\(128\\~32\\) 증류에 적용한다. CFG 척도는 7.5로 설정되었으며 음성 프롬프트가 없습니다. 우리는 나머지 단계에서 적대적 손실을 사용한다. 데이터 생성은 32단계 동안 DPM-Solver++[19]를 사용합니다. DPM-Solver++는 오일러보다 더 좋은 품질을 생산하기 때문에, 우리는 여전히 추가 품질을 위해 128단계부터 증류를 시작하기로 결정했다.\n' +
      '\n' +
      '증류는 64 A100 GPU에서 수행된다. 각 GPU는 메모리 제약으로 인해 1의 배치 크기만 처리할 수 있으므로 4의 그래디언트 누적을 적용하여 총 256의 배치 크기를 달성한다. 학습률, _etc._ SDXL-Lightning [13]을 정확히 따르십시오. 우리는 원래 AnimateDiff에서 사용된 선형 스케줄[9]을 채택하지만, 제로 터미널 SNR[12]을 보장하기 위해 [13]에 후속하는 트레이닝 동안 모델 입력으로서 마지막 타임스텝에서의 순수 잡음을 사용한다.\n' +
      '\n' +
      'SDXL-Lightning [13]과 달리 1단계 생성을 위해 기본 모델을 동결한 상태에서 \\(x_{0}\\)-예측으로 전환할 수 없으므로 \\(\\epsilon\\)-예측에서 모델을 학습한다.\n' +
      '\n' +
      '먼저 영상 데이터 셋에 LoRA 모듈[10]로 영상 베이스 모델을 증류한 후 제한된 영상 데이터 셋에 영상 모션 모듈을 증류하여 데이터 부족에 대처하는 AnimateLCM[35]과 비교하여, 본 논문에서 제안하는 방법은 AnimateDiff 모델 전체를 증류한다. 또한, 움직임 모듈만으로 증류를 훈련할 수 있어 만족할 만한 품질을 얻을 수 있으며, 영상 기반 모델에 추가적인 LoRA 모듈이 필요하지 않음을 알 수 있다.\n' +
      '\n' +
      '## 4 Evaluation\n' +
      '\n' +
      '### Qualitative Evaluation\n' +
      '\n' +
      '그림 1은 원래 AnimateDiff[6] 및 AnimateLCM[35]에 대한 모델의 정성적 비교를 보여준다. 본 논문에서 제안하는 방법은 AnimateLCM에 비해 1단계, 2단계, 4단계 추론을 통해 더 나은 품질을 얻을 수 있다. 그 차이는 AnimateLCM이 날카로운 세부 사항을 생성하지 못함에 따라 1단계 및 2단계 추론을 사용할 때 특히 두드러진다. 또한 교차 모델 증류를 사용하는 방법은 기본 모델의 원래 스타일을 더 잘 유지할 수 있다. AnimateLCM은 8단계 추론을 사용할 때에도 때때로 베이스 모델의 스타일과 톤을 과도하게 노출하고 다르다.\n' +
      '\n' +
      '그림 1은 보이지 않는 기본 모델인 Mistoon Anime v1.0[54]에 적용했을 때 우리 모델의 결과를 보여준다. 추론 단계가 감소함에 따라 스타일이 원래 스타일에서 점차 벗어나지만, 우리의 모델은 캐릭터의 전반적인 애니메이션 스타일, 의류 및 헤어 색상 측면에서 AnimateLCM에 비해 여전히 원본에 더 가까운 결과를 생성한다는 점에 유의한다. 교차 모델 증류의 효과에 대한 더 많은 분석이 섹션 5.1에서 제공된다. 보이지 않는 모델에 대한 더 많은 분석이 섹션 5.2에서 제공된다.\n' +
      '\n' +
      '1단계 모델은 무거운 노이즈 아티팩트를 생성합니다. 이는 SDXL-Lightning[13]에서도 나타나는 엡실론 제형의 수치적 불안정성 때문일 가능성이 있다. 2단계 모델의 경우 더 뚜렷한 밝기 깜박임을 생성한다는 것을 알 수 있다. 깜빡임은 원래 애니메이트디프 모델 때부터 존재했습니다. 우리는 4단계 모델이 품질과 속도의 균형을 맞춘다는 것을 발견한다.\n' +
      '\n' +
      '그림 1: 질적 비교. 생성된 비디오 클립의 첫 번째, 중간, 마지막 프레임만 각 열에 표시한다. 우리의 모델은 1단계, 2단계, 4단계 추론을 사용하여 더 나은 결과를 생성한다. 또한, 저희 모델은 원래 모델의 스타일을 더 잘 유지할 수 있습니다. 이 페이지는 사실적인 스타일 생성에 중점을 둡니다. 애니메이션 스타일 세대의 다음 페이지를 참조하십시오.\n' +
      '\n' +
      '그림 1: 질적 비교. 마지막 페이지부터 이 페이지에서 애니메이션 스타일 생성 비교를 보여주고, 보이지 않는 기본 모델인 그림의 미스툰 애니메이션[54]에 모델을 적용하려고 한다. 1f. 추론 단계가 감소함에 따라 스타일 저하가 있지만, 본 모델은 캐릭터의 전반적인 애니메이션 스타일, 의상 및 헤어 색상 측면에서 원본에 비해 더 유사한 결과를 생성한다.\n' +
      '\n' +
      '그림 3: 보이지 않는 기본 모델에 대한 증류 결과. 여기 있는 모든 이미지 베이스 모델은 우리 모델과 AnimateLCM 모델을 증류하는 동안 보이지 않는다[35]. 우리의 결과는 세부적으로 더 좋고 원래 스타일에 더 가깝습니다. 우리는 이미지 기반 모델의 특수성과 가장 잘 일치하는 다른 프롬프트를 사용하지만 모델 비교 전반에 걸쳐 동일한 프롬프트와 시드가 사용된다. 생성된 비디오 클립의 첫 번째 프레임을 보여줍니다.\n' +
      '\n' +
      '그림 2: 교차 모델과 단일 모델 증류 간의 비교. 단일 모델 증류는 WebVid-10M [1] 데이터 세트를 사용하여 SD v1.5 [26] 기본 모델에 대해서만 훈련된다. 단일 모델 증류는 다른 기본 모델에서 품질을 유지하지 못한다. 생성된 비디오 클립의 첫 번째 프레임을 보여줍니다.\n' +
      '\n' +
      '그림 4: 우리의 모델은 세밀한 모션 제어를 위한 모션 LoRA 모듈[6]과 호환된다. 여기 툰유[58]의 4단계 모델 "웃고 있는 소녀"가 있습니다. 첫 번째 행은 시작 프레임이고 두 번째 행은 최종 프레임이다.\n' +
      '\n' +
      '도 5: 상이한 종횡비의 텍스트-대-비디오 생성. 여기에 표시된 예는 1:2, 2:3, 3:2 및 2:1 종횡비를 생성하는 2단계 및 4단계 모델이다. 생성된 비디오 클립에서 랜덤 프레임을 보여준다.\n' +
      '\n' +
      '도 6: ControlNet[42]을 이용한 비디오-투-비디오 생성. 예제 동영상은 ControlNet[42]과 함께 모델을 사용하여 직접 \\(576\\times 1024\\) 해상도로 생성된다. 초해상도 사용과 같은 보다 정교한 파이프라인은 품질을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '### Quantitative Evaluation\n' +
      '\n' +
      '표 2는 정량적 비교를 나타낸다. 먼저 WebVid-10M 데이터셋 [1]에서 무작위로 100개의 프롬프트를 선택한다. 그런 다음 4가지 다른 이미지 기반 모델을 사용하여 클립을 생성한다. 사실적, 아민적 스타일 모델로 RealisticVision[56]과 ToonYou[58]을 선정하고, 보이지 않는 사실적, 애니메이션 스타일 모델로 DreamShaper[47]와 DynaVision[48]을 선정한다. 각 프롬프트는 랜덤 시드를 사용하지만 동일한 프롬프트의 모델 간에 동일한 시드가 사용됩니다. 마지막으로, 음의 프롬프트 없이 32개의 오일러 스텝과 CFG 7.5를 사용하여 생성된 원래의 AnimateDiff 결과에 대해 FVD[34]를 계산한다. 우리와 AnimateLCM(35) 모두 CFG를 사용하지 않는다. 측정 기준은 우리의 모델이 AnimateLCM에 비해 더 나은 FVD를 가지므로 원래의 AnimateDiff에 더 가까운 결과를 생성한다는 것을 보여준다.\n' +
      '\n' +
      '## 5 Ablation\n' +
      '\n' +
      'Cross-Model 증류에서의### 효과\n' +
      '\n' +
      '본 논문에서는 WebVid-10M [1] 데이터셋에 대한 이미지 베이스 모델로 Stable Diffusion v1.5 [26]만을 사용하여 모델을 증류하는 비교 실험을 수행하였다. 이는 일반적인 단일 모델 증류 패러다임에 해당한다.\n' +
      '\n' +
      '그림 2는 단일 모델 증류가 기본 SD[26] 기본 모델에서만 최상의 품질을 유지할 수 있음을 보여준다. 사실적인 스타일이 유사한 RealisticVision[56]으로 전환하면 품질이 저하됩니다. 애니메이션 스타일이 확연히 다른 툰유[58]로 전환하면 품질이 크게 떨어진다.\n' +
      '\n' +
      '보이지 않는 기저 모델에 대한### 효과\n' +
      '\n' +
      '우리는 매우 다양한 인기 있는 이미지 기반 모델에 대해 모델을 테스트합니다. 이러한 염기 모델은 증류 과정에서 보이지 않는다. 그림 3은 우리의 증류된 모션 모듈이 보이지 않는 다른 기본 모델에 잘 일반화될 수 있음을 보여준다. 또한, 우리의 증류 모델은 AnimateLCM[35]에 비해 더 선명하고 원래 모델에 더 가까운 스타일로 결과를 생성한다.\n' +
      '\n' +
      'Motion LoRA의### 호환성\n' +
      '\n' +
      '그림 4는 우리의 모델이 Motion LoRA[6]와 호환된다는 것을 보여준다. 우리는 모든 모델에서 모션 LoRA를 테스트했으며 모든 단계 설정에서 작동한다는 것을 발견했습니다. 모션 LoRA가 소개하는 이슈인 워터마크를 피하기 위해 0.8의 강도로 모션 LoRA를 적용한다. 움직임 LoRA는 카메라 움직임의 세밀한 제어를 가능하게 하며, 생성된 비디오에서 움직임의 양을 크게 향상시킨다.\n' +
      '\n' +
      '양상-비율에 따른### 지원\n' +
      '\n' +
      '그림 5와 6은 증류가 정사각형 종횡비로만 수행되었음에도 불구하고 텍스트 대 비디오 및 비디오 대 비디오 시나리오 모두에서 서로 다른 종횡비의 비디오를 생성하는 능력을 본 모델이 보유하고 있음을 보여준다. 그러나 우리는 종횡비가 제곱에서 더 많이 벗어날수록 불량 사례가 발생할 확률이 더 높다는 것을 발견했다. 증류 훈련은 다중 종횡비로 수행될 수 있다. 우리는 이것을 미래의 개선에 맡긴다.\n' +
      '\n' +
      'ControlNet을 이용한### 비디오 투 비디오 생성\n' +
      '\n' +
      '애니메이트디프의 가장 인기 있는 용도 중 하나는 비디오 대 비디오 스타일링입니다. 소스 비디오가 주어지면, ControlNet[42]을 적용하여 사람의 움직임을 추출하고, AnimateDiff를 사용하여 서로 다른 스타일을 사용하여 움직임을 생성한다.\n' +
      '\n' +
      '도 6은 우리의 모델이 ControlNet[42]과 호환된다는 것을 보여준다. 여기서는 기본 설정만 적용하되 초해상도, 배경 교체 등 보다 정교한 파이프라인을 추가로 추가할 수 있다. 더 긴 비디오들을 생성하기 위해, 인기 있는 접근법은 16-프레임 컨텍스트 윈도우와 이전에 생성된 클립들을 오버랩하는 컨텍스트 오버랩이다. 우리는 우리의 모델이 컨텍스트가 겹치는 더 긴 비디오를 생성하는 것을 지원한다는 것을 테스트했습니다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '번개 빠른 동영상 생성 모델인 AnimateDiff-Lightning을 제시했습니다. 본 논문에서 우리는 점진적 적대 확산 증류가 비디오 촬영 방식에 적용될 수 있음을 보여주었다. 우리의 모델은 몇 단계 비디오 생성에서 새로운 최신 기술을 달성합니다. 또한, 다양한 양식화된 기본 모델로 일반화하는 증류 모듈의 능력을 더욱 향상시키기 위해 교차 모델 확산 증류를 제안했다. 본 논문에서는 AnimateDiff에 교차 모델 증류 기법을 적용하여 다양한 영상 기반 모델에서 가장 널리 사용되고 있다. 그러나 이 기술은 모든 양식에 대해 보다 보편적인 증류 플러그 가능한 모듈을 생성하기 위해 일반화될 수 있다. 마지막으로, 우리는 생성 AI의 발전을 촉진하고자 하는 희망으로 증류된 AnimateDiff-Lightning 모델을 출시합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r|r r|r r} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{Steps} & \\multicolumn{4}{c}{FVD \\(\\downarrow\\)} \\\\  & & RV [56] & TY [58] & DS [47] & DV [48] \\\\ \\hline \\multirow{4}{*}{AnimateLCM} & 1 & 1423.18 & 1825.24 & 1393.10 & 1652.32 \\\\  & 2 & 1041.61 & 917.61 & 1034.19 & 1045.49 \\\\  & 4 & 1171.54 & 784.81 & 1175.06 & 1097.66 \\\\  & 8 & 1300.41 & 804.21 & 1253.43 & 1115.95 \\\\ \\hline \\multirow{4}{*}{Ours} & 1 & 1135.43 & 1037.85 & 974.75 & 1501.34 \\\\  & 2 & 1024.13 & 801.04 & 918.74 & 1351.06 \\\\ \\cline{1-1}  & 4 & 1010.30 & 708.55 & 908.01 & 1175.29 \\\\ \\cline{1-1}  & 8 & 1058.58 & 690.65 & 865.29 & 979.94 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 상이한 이미지 베이스 모델들 상의 원래의 AnimateDiff에 대해 계산된 FVD. RV: RealisticVision, TY: ToonYou, DS: DreamShaper, DV: DynaVision.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Max Bain, Arsha Nagrani, Gul Gul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1708-1718, 2021.\n' +
      '* [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023.\n' +
      '* [3] A. Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22563-22575, 2023.\n' +
      '* [4] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Grasskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7312-7322, 2023.\n' +
      '*144, 2014.\n' +
      '* [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* [7] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023.\n' +
      '* [8] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022.\n' +
      '* [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc\'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.\n' +
      '* [10] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.\n' +
      '* [11] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* [12] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 5404-5411, January 2024.\n' +
      '* [13] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxllightning: Progressive adversarial diffusion distillation, 2024.\n' +
      '* [14] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. Robust high-resolution video matting with temporal guidance. _2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 3132-3141, 2021.\n' +
      '* [15] Shanchuan Lin and Xiao Yang. Diffusion model with perceptual loss, 2024.\n' +
      '* [16] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [17] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022.\n' +
      '* [18] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and qiang liu. Installow: One step is enough for high-quality diffusion-based text-to-image generation. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* [19] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.\n' +
      '* [20] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023.\n' +
      '* [21] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module, 2023.\n' +
      '* [22] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models, 2023.\n' +
      '* [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.\n' +
      '* [24] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [25] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017.\n' +
      '* [26] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesiswith latent diffusion models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10674-10685, 2021.\n' +
      '* [27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. _ArXiv_, abs/1505.04597, 2015.\n' +
      '* [28] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations_, 2022.\n' +
      '* [29] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation, 2023.\n' +
      '* [30] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* [31] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In _The Twelfth International Conference on Learning Representations_, 2024.\n' +
      '* [32] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In _International Conference on Machine Learning_, 2023.\n' +
      '* [33] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.\n' +
      '* [34] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _ArXiv_, abs/1812.01717, 2018.\n' +
      '* [35] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning, 2024.\n' +
      '* [36] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicvideo-v2: Multi-stage high-aesthetic video generation, 2024.\n' +
      '*755, 2018.\n' +
      '*18, 2015.\n' +
      '* [39] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4210-4220, 2023.\n' +
      '* [40] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models, 2023.\n' +
      '* [41] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2023.\n' +
      '* [42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3813-3824, 2023.\n' +
      '* [43] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation, 2024.\n' +
      '* [44] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models, 2023.\n' +
      '* [45] AbsoluteReality v1.8.1. [https://civitai.com/models/81458](https://civitai.com/models/81458).\n' +
      '* [46] Counterfeit v3.0. [https://civitai.com/models/4468](https://civitai.com/models/4468).\n' +
      '* [47] DreamShaper v8. [https://civitai.com/models/4384](https://civitai.com/models/4384).\n' +
      '* [48] DynaVision v2. [https://civitai.com/models/75549](https://civitai.com/models/75549).\n' +
      '* [49] epiCRealism. [https://civitai.com/models/25694](https://civitai.com/models/25694).\n' +
      '* [50] Exquisite Details Art. [https://civitai.com/models/118495](https://civitai.com/models/118495).\n' +
      '* [51] IMP v1.0. [https://civitai.com/models/56680](https://civitai.com/models/56680).\n' +
      '* [52] MajicMix Realistic v7. [https://civitai.com/models/43331](https://civitai.com/models/43331).\n' +
      '* [53] MajicMix Reverie v1. [https://civitai.com/models/65055](https://civitai.com/models/65055).\n' +
      '* [54] Mistoon Anime v1.0. [https://civitai.com/models/24149](https://civitai.com/models/24149).\n' +
      '* [55] RCNZ Cartoon 3d v2. [https://civitai.com/models/66347](https://civitai.com/models/66347).\n' +
      '* [56] Realistic Vision v5.1. [https://civitai.com/models/4201](https://civitai.com/models/4201).\n' +
      '* [57] ReV Animated v1.2.2. [https://civitai.com/models/7371](https://civitai.com/models/7371).\n' +
      '* [58] ToonYou Beta 6. [https://civitai.com/models/30240](https://civitai.com/models/30240).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>