<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '**Yi: 공개 기초 모델 by 01.AI**\n' +
      '\n' +
      '**01.AI**\n' +
      '\n' +
      '**Code:** [https://github.com/01-ai/Yi](https://github.com/01-ai/Yi)\n' +
      '\n' +
      '**Model:** [https://huggingface.co/01-ai](https://huggingface.co/01-ai)\n' +
      '\n' +
      '**Abstract**\n' +
      '\n' +
      '강한 다차원 기능을 발휘하는 일련의 언어 및 멀티모달 모델인 Yi 모델 패밀리를 소개합니다. Yi 모델 패밀리는 6B 및 34B 사전 훈련된 언어 모델을 기반으로 하고, 채팅 모델, 200K 긴 컨텍스트 모델, 깊이 상향 모델 및 비전 언어 모델로 확장한다. 우리의 기반 모델은 MMLU와 같은 광범위한 벤치마크에서 강력한 성능을 달성하고, 우리의 미세화된 채팅 모델은 알파카에벌 및 챗봇 아레나와 같은 주요 평가 플랫폼에서 강력한 인간 선호율을 제공한다. 확장 가능한 슈퍼 컴퓨팅 인프라와 고전적인 트랜스포머 아키텍처를 기반으로 Yi 모델의 성능은 주로 데이터 엔지니어링 노력의 결과 데이터 품질에 기인한다. 사전 학습을 위해 캐스케이드 데이터 중복 제거 및 품질 필터링 파이프라인을 사용하여 3.1조 토큰의 영어 및 중국어 코퍼스를 구성한다. 미세 조정을 위해, 우리는 모든 인스턴스가 기계 학습 엔지니어에 의해 직접 검증되도록 여러 번의 반복에 걸쳐 작은 규모(10K 미만) 명령어 데이터 세트를 연마한다. 비전 언어의 경우 채팅 언어 모델을 비전 트랜스포머 인코더와 결합하고 시각적 표현을 언어 모델의 의미 공간에 정렬하도록 모델을 훈련한다. 경량 연속 사전 훈련을 통해 컨텍스트 길이를 200K까지 확장하고 강력한 헤이스택 내 바늘 검색 성능을 입증한다. 지속적인 사전 훈련을 통해 사전 훈련된 체크포인트의 깊이를 확장하면 성능이 더욱 향상됨을 보인다. 현재 결과를 감안할 때 철저하게 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 훨씬 더 강력한 프론티어 모델이 될 것이라고 믿습니다.\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '*1 소개\n' +
      '* 2 사전 훈련\n' +
      '	* 2.1 데이터 처리\n' +
      '	* 2.2 토큰화\n' +
      '	* 2.3 모델 아키텍처\n' +
      '* 3 Finetuning\n' +
      '	* 3.1 데이터 전처리\n' +
      '	* 3.2 훈련방법\n' +
      '* 4 인프라\n' +
      '* 5 안전\n' +
      '* 6 평가\n' +
      '	* 6.1 베이스 모델 성능\n' +
      '		* 6.1.1 주요 결과\n' +
      '		* 6.1.2 토론\n' +
      '		* 6.1.3 In-Context Learning Study\n' +
      '	* 6.2 채팅 모델 성능\n' +
      '		* 6.2.1 자동평가\n' +
      '		* 6.2.2 인체 평가\n' +
      '* 7 능력 확장\n' +
      '	* 7.1 롱컨텍스트 모델링\n' +
      '	* 7.2 Vision-anguage\n' +
      '	* 7.3 Depth Upscaling\n' +
      '* 8 최종 토론\n' +
      '* 작성자 목록 및 기여도\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '최근 대형 언어 모델의 돌파구는 인공 지능의 전 분야에 혁명을 일으키고 잠재적으로 인간 사회 전반에 걸쳐 방사되고 있다. 대형 언어 모델에 대한 우리의 비전은 차세대 컴퓨팅 플랫폼으로 만들고 상당히 증폭된 지능으로 커뮤니티 전체에 힘을 실어주는 것이다. 이 미션을 위한 단계로 3.1T 고도로 설계된 대용량 데이터에 대해 처음부터 사전 훈련된 Yi 모델 시리즈, 6B 및 34B 언어 모델을 제시하고 작지만 세심하게 연마된 정렬 데이터에 대해 미세 조정한다. 향후 섹션에서 자세히 설명할 상당한 엔지니어링 노력으로 인한 데이터 품질로 인해 Yi는 GPT-3.5 벤치마크 점수와 인간 선호도에 가깝다.\n' +
      '\n' +
      'Yi 모델 시리즈를 설계할 때 우리는 주로 _모델 규모, 데이터 규모 및 데이터 품질_: (1)에 관한 다음 차원에 관심이 있다. 모델 척도를 선택할 때, 데시데라타는 경계 요소가 제한된 24G 메모리인 RTX 4090과 같은 소비자 등급 하드웨어에 추론할 수 있을 만큼 충분히 작은 모델을 갖지만 복잡한 추론과 비상 능력으로 충분히 큰 모델을 갖는 것이다. 이것이 우리가 34B가 좋은 성능-비용 균형을 제공한다는 것을 발견한 이유이다; (2). 34B가 Chinchilla[30] 및 LLaMA[77]에 의해 사용되는 종래의 70B보다 작기 때문에, 감소된 컴퓨트 플롭을 보상하기 위해 프리트레인 데이터 스케일을 3.1T 토큰으로 증가시킨다. 이것은 모델-데이터 스케일 조합이 사후 친칠라 최적 체제[64]에 빠지게 하고, 즉 계산 최적(약 1T)보다 더 많은 토큰(3T)에서 모델을 오버트레인한다. 인트4[81] 양자화 후 24G GPU 메모리에서 34B 채팅 모델을 거의 성능 저하 없이 서비스할 수 있다는 장점이 추론 측면에서 있다. (3) 우리의 데이터 엔지니어링 원칙은 사전 훈련과 미세 조정 모두에 대해 양보다 질을 촉진하는 것이다. 사전 훈련 데이터 품질은 캐스케이드 필터링 방법과 의도적으로 증가된 중복 제거 강도를 갖는 정교한 데이터 클리닝 파이프라인에 의해 보장된다; (4). 데이터를 미세 조정하기 위해 우리는 사용자 피드백을 기반으로 여러 번의 반복에 걸쳐 10K 미만의 명령어를 손으로 조작함으로써 품질을 크게 강조한다. 이 접근법은 FLAN[9] 및 UltraChat[19]와 같은 수량-스케일링 스타일링된 명령어 튜닝 작업에서 크게 벗어나지만 LIMA[94]와 같은 수공예 스타일링된 작업과 더 많이 정렬된다.\n' +
      '\n' +
      '사전 학습 데이터 클리닝 시스템은 언어, 휴리스틱 텍스트 특성, 복잡성, 의미, 주제 및 안전성에 기반한 정교한 필터링 파이프라인과 단락, 민해시 및 정확한 매칭에 기반한 연쇄 중복 제거 프로세스를 특징으로 한다. 이 철저한 파이프라인은 CCNet[80], RefinedWeb[56] 및 RedPajama[13]와 같은 기존 파이프라인보다 훨씬 높은 제거 비율을 가져오며, 이는 데이터 엔지니어링 성공의 열쇠라고 믿는다. 기본 원리는 사전 훈련이 데이터 스케일링을 필요로 하지만, 대규모 원시 데이터에 대한 모델을 훈련하는 것보다 사용된 데이터가 고품질인지 확인하고 싶기 때문에 광범위한 필터링 없이 10T 토큰보다 정교한 엔지니어링보다 3T 토큰을 선호한다. 모델 아키텍처는 Grouped-Query Attention (GQA) [1], SwiGLU [68] 활성화, RoPE ABF (RoPE ABF) [82]로 구성된 Transformer 아키텍처의 표준 구현을 사용한다. 이 설계 선택은 트랜스포머 원본 논문[78]에서 파생된 표준 접근법으로 나중에 GPT-3 및 친칠라[30]에 의해 수정된 다음 LLaMA[77], 바이촨[84], Qwen[3] 및 많은 관련 작업이 뒤따른다.\n' +
      '\n' +
      'GPT-3.5 매칭 인간 선호도에 접근하기 위해, 우리의 미세 조정 데이터 세트는 신중하게 선택된 다중 회전 명령-응답 쌍으로부터 큐레이션되며, 기계 학습 엔지니어 팀이 직접 주석을 달은 다음 사용자 피드백의 여러 반복에 걸쳐 연마된다. 위에서 언급한 바와 같이, 우리의 미세 조정 데이터 세트의 크기는 10K 미만이지만 모델 개발 타임라인에 걸쳐 계속해서 개선되었다. 데이터세트의 관리 가능한 크기에서 최적의 데이터 구성을 식별하고 다양성을 촉진하며 효과적인 하이퍼파라미터를 찾기 위해 광범위한 그리드 검색을 사용했다. 8비트 및 4비트 양자화 후에, 최종 채팅 모델은 bf16 포맷에 비해 성능 저하 없이 거의 소비자 등급 GPU에 배치될 수 있다.\n' +
      '\n' +
      '우리는 상황 확장, 시각 언어 적응 및 깊이 상향 조정이라는 세 가지 차원에서 Yi 모델 기능을 추가로 확장한다. 200K 컨텍스트 길이를 얻기 위해, 우리는 Fu 등의 동시 작업과 유사하게 약 5B 길이-업샘플링된 데이터에 대해 모델을 사전 훈련한다[22]. 이 모델을 비전 언어 작업에 적용하기 위해 비전 인코더를 통합하고 Liu et al. [47]의 실습을 따르고 개선하는 다단계 훈련 방법을 개발한다. 또한 깊이-업스케일링(depth-upscaling, 38)의 효과, 즉 지속적인 사전 훈련을 통해 모델을 더 깊게 만들고 모델 성능을 더욱 향상시키기 위한 효과를 확인한다.\n' +
      '\n' +
      '사전 훈련에서 미세 조정, 서빙까지. 사전 학습을 지원하기 위해, 제한된 스위칭 오버헤드를 갖는 실시간 가용 GPU 노드 교차 클러스터에 따라 태스크를 일괄적으로 실행할 수 있는 크로스 클라우드 탄성 태스크 스케줄링, 자동 장애 복구 및 토폴로지 인식 자원 할당을 개발한다. 미세 조정을 지원하기 위해, 우리는 상이한 모델(예를 들어, 정책 모델의 경우 메가트론[70], 보상 모델의 경우 딥스피드[60])에 대해 상이한 분산 백엔드를 지원하는 계층적 스케줄링 프레임워크를 구축한다. 효율적인 추론을 위해 4비트 모델과 8비트 KV 캐쉬 양자화를 사용하여 PagedAttention[41]과 Dynamic Batching을 결합한다.\n' +
      '\n' +
      '광범위한 실험은 Yi-34B가 성능과 효율성 모두에서 GPT-3.5와 일치할 수 있음을 보여준다. MMLU[27] 및 LMSys ELO 등급[93]과 같은 대부분의 표준 벤치마크에서, Yi-34B는 일반적으로 GPT-3.5와 동등한 점수를 달성한다. 모델 파라미터 및 KV 캐시 양자화 후에, 추론 비용은 또한 광범위한 커뮤니티가 비용 효율적인 디바이스들에 모델을 전개할 수 있도록 제어된다. 또한 다중 평가 벤치마크에서 상식 추론, 대학 시험, 수학, 코딩, 독해 및 인간 선호도 승률에 대한 Yi와 주요 LLM의 상세한 성능 비교를 보고한다.\n' +
      '\n' +
      '출시 이후, Yi 모델 시리즈는 다음과 같은 관점에서 커뮤니티에 도움이 되었다. 이는 GPT-3.5 매칭 품질이지만 비용 효율적인 모델을 연구자에게 제공하고 개발자가 언어 모델 기반 에이전트와 같은 AI 네이티브 애플리케이션을 구축할 수 있게 한다. (2) 최종 사용자에게 로컬 실행 가능한 챗봇을 제공하여 결과적으로 사용자 데이터 프라이버시를 보호하는 데 도움이 됩니다. 더 강력한 프론티어 모델을 달성하기 위해 추가 데이터와 모델 스케일링에 대한 방향을 조명합니다. 연구와 상업적 이용을 위해.\n' +
      '\n' +
      '## 2 Pretraining\n' +
      '\n' +
      '사전 훈련에 대한 우리의 접근법은 고도로 조작된 대규모 사전 훈련 코퍼스에서 표준 고밀도 변압기 아키텍처를 훈련하는 것인데, 우리의 기본 가정은 높은 품질의 광범위한 데이터에 대해 훈련될 때 표준 아키텍처가 고급 능력을 나타낼 수 있다는 것이다. 즉, 우리는 실제로 광범위한 예비 건축 실험을 수행했지만 많은 건축 수정이 필요하지 않을 수 있다. 다음 하위 섹션에서는 먼저 데이터 엔지니어링 파이프라인을 자세히 설명한 다음 모델 아키텍처에 대해 간략하게 설명합니다.\n' +
      '\n' +
      '### Data Processing\n' +
      '\n' +
      'Yi 데이터 혼합물은 그림 2에 나와 있으며 고품질 이중 언어 사전 훈련 데이터를 생성하기 위해 그림 1과 같이 캐스케이드 데이터 처리 파이프라인을 세심하게 설계했다. 이 파이프라인에는 품질과 다양성을 목표로 하는 일련의 데이터 청소 전략이 있다. 우리는 Common Crawl의 웹 문서로 시작하여 언어 식별 및 복잡성 채점을 위해 CCNet 파이프라인[79]을 사용한다. 그런 다음 아래에 자세히 설명된 대로 필터링 및 중복 제거 프로세스의 조합을 사용합니다.\n' +
      '\n' +
      '그림 1: Yi의 사전 훈련 데이터 클리닝 파이프라인.\n' +
      '\n' +
      '휴리스틱 규칙 필터 필터의 이 부분은 낮은 품질의 텍스트를 제거하는 것을 목표로 한다. 우리는 (1)을 기준으로 텍스트를 걸러낸다. URL, 도메인, 단어 블록리스트 및 가들링된 텍스트 필터; (2). 문서 길이, 특수 기호의 비율 및 짧은 선, 연속선 또는 불완전선의 비율(3)입니다. 반복된 단어, n-그램, 또는 단락 [58]; 필터링 임계값은 Nguyen et al. [52]에 기술된 바와 같이, 큰 문서 샘플의 통계적 분석에 기초한다. 또한, 이메일 주소 및 전화번호와 같은 개인 식별 정보(PII)를 식별하고 익명화한다.\n' +
      '\n' +
      '학습된 필터를 사용하여 표준 휴리스틱 규칙의 기능을 초과하는 미묘한 사례를 해결합니다. 특히 커먼 크롤에서 추출한 중국 콘텐츠는 특히 음란물 및 도박과 같은 부적절한 콘텐츠의 비율이 더 높기 때문에 독특한 문제를 제시한다. 전통적인 휴리스틱 룰 기반 필터는 모든 유해한 콘텐츠를 효과적으로 식별하고 제거하는 데 어려움을 겪는다. 필터링 프로세스를 향상시키기 위해, 우리는 필터링에 대한 학습된 점수자, 즉 당혹성 점수자, 품질 점수자, 안전 점수자 및 문서 일관성 점수자(1)를 통합했다. CCCNet[80]에 따라 KenLM 라이브러리를 사용하는 _Perplexity Scorer_는 방대한 웹 문서 배열을 평가하여 복잡도 점수가 평균보다 크게 높은 문서를 폐기한다. (2). _Quality Scorer_는 위키피디아와 유사한 페이지를 품질에서 인식하고 선호하며 그에 따라 점수를 할당하도록 훈련된 분류기이다. 품질 표준을 충족하지 못하는 문서는 이후 제거됩니다. (3). the _Document Coherence Scorer_는 이질적인 문장 또는 단락으로 구성된 저품질 웹 문서를 식별하여 일관성이 없다. 이러한 문서는 추가 분석을 위해 분할되거나 완전히 제거된다. (4). the _Safety Scorer_는 폭력, 음란물, 정치적 선전 등과 같은 독성 콘텐츠를 포함하는 웹 문서를 식별하고 제거한다.\n' +
      '\n' +
      '클러스터 기반 필터 우리는 웹 문서를 그룹화하기 위해 감독되지 않은 시맨틱 클러스터링을 추가로 사용한다. 이러한 클러스터링 과정은 유사한 의미적 특징을 공유하는 문서의 효율적인 식별 및 분석을 가능하게 한다. 클러스터링된 데이터는 후속적으로 품질 라벨로 주석 처리되어 Yi의 데이터 혼합 전략의 최적화를 위한 필수 참조를 제공한다. 자동 및 수동 검증을 통해 품질이 낮은 것으로 확인된 문서는 데이터셋에서 제외된다.\n' +
      '\n' +
      '필터링 후, 우리는 Penedo et al.(2023) [56]에서 절차에 따라 포괄적인 중복 제거 파이프라인을 구현한다. 이 파이프라인은 문서 수준 MinHash 중복제거와 하위 문서 정확일치 중복제거를 통합하여 문서 내 및 문서 전체에서 중복된 내용을 효과적으로 식별하고 제거합니다. 또한 뉴스, 광고 및 지식 기반 콘텐츠로 레이블을 예측하는 주제 모델을 사용하여 웹 문서를 특정 주제로 분류한다. 최종 사전 훈련 데이터 세트에서 우리는 정보 밀도를 보장하기 위해 도움이 덜 되는 콘텐츠, 대부분 광고를 다운 샘플링한다. Yi의 사전 훈련 데이터의 최종 구성은 그림 2에 나와 있다.\n' +
      '\n' +
      '### Tokenization\n' +
      '\n' +
      '우리는 사전 훈련 데이터를 토큰화하기 위해 SentencePiece 프레임워크[40]에서 구현된 바이트 쌍 인코딩(BPE) [69]를 사용한다. Yi의 어휘 크기는 계산 효율과 단어 이해의 균형을 맞추기 위해 64,000으로 설정되었다. 특히 숫자 데이터에 대한 더 나은 이해를 돕기 위해 숫자를 개별 숫자로 나눈다. 우리는 결함 허용을 보장하기 위해 희귀 문자를 유니코드 바이트 인코딩으로 되돌릴 수 있도록 한다. 우리는 모든 구두점을 반치폭 형식으로 옮기는 것을 피하기 위해 아이덴티티 토큰화기를 사용한다. 영어를 우선시하는LLM은 일반적으로 문장의 다른 위치에 동일한 단어를 일반화하기 위해 토큰화기에 더미 프리픽스(텍스트 시작 부분의 공백)를 사용한다. 우리는 가정이 영어 문맥, 특히 따옴표로 시작하는 문장의 경우에도 항상 유지되는 것은 아니며 중국어 문맥에서도 긍정적인 효과를 나타내지 않기 때문에 이 접근법을 사용하지 않는다.\n' +
      '\n' +
      '그림 2: Yi의 사전 훈련 데이터 혼합물. 전반적으로 우리의 데이터는 영어와 중국어로 3.1T 고품질 토큰으로 구성되며 다양한 출처에서 나온다. LLaMA[76] 및 Falcon[56]과 같은 기존의 알려져 있는 혼합물과의 주요 차이점은 우리가 이중 언어를 구사하고 더 엄격한 세척 파이프라인으로 인해 품질이 더 높다는 것입니다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      'Yi는 코드가 LLaMA의 [77] 구현에 기초하는 고전 디코더 전용 트랜스포머 아키텍처 [78]의 수정된 버전을 사용한다. 주요 파라미터 설정은 표 1에 요약되어 있다. LLaMA로부터 Yi로의 수정은 아래에 추가로 요약되어 있다:\n' +
      '\n' +
      'Attention MechanismLLaMA 2는 가장 큰 70B 모델에서만 Grouped-Query Attention(GQA) [1]을 사용하며, 7B와 13B는 완전한 attention을 사용한다. 우리는 Yi-6B와 Yi-34B 모두에 GQA를 통합한다. GQA는 질의 헤드를 G 그룹으로 분할하여 질의의 각 그룹 내에서 단일 키와 값 헤드를 공유한다[1]. 이 접근법은 원래의 Multi-Head Attention(MHA)[16; 57; 67]에 비해 트레이닝 및 추론 비용의 상당한 감소를 제공한다. 우리는 GQA를 6B 더 작은 모델에 적용한 후 성능 저하를 관찰하지 않는다.\n' +
      '\n' +
      '활성화 함수는 SwiGLU[68]을 Yi의 사후 주의 계층으로 사용하여 활성화 크기를 \\(4h\\)에서 \\(8/3h\\)으로 줄였다. 이 조정은 또한 GQA로 인한 매개변수 감소를 보상하여 전체 매개변수 카운트가 기존 7B 및 34B 모델과 호환되도록 한다.\n' +
      '\n' +
      '위치 임베딩과 긴 컨텍스트는 표준 구현에 따라 로터리 위치 임베딩(RoPE) [73]을 사용한다. Xiong et al. [82]에 소개된 기본 주파수(RoPE ABF)를 조정하여 기본 모델 자체가 4K 컨텍스트 길이로 훈련되는 200K까지의 긴 컨텍스트 윈도우를 지원한다. 기본 모델을 더 긴 컨텍스트에 적응시키기 위해 우리는 주로 책에서 약간 업샘플링된 긴 시퀀스로 사전 훈련 데이터 혼합물에서 10B 토큰에서 모델을 사전 훈련한다. 1-2B 토큰만이 4K-200K 길이의 낮은 손실로 수렴하기에 충분하며, 경량 미세 조정은 거의 완벽한 긴 컨텍스트 검색 성능을 추가로 유도한다는 것을 관찰한다. 이러한 관찰에 기초하여, 우리는 사전 훈련된 길이(4K)보다 더 긴 종속성을 모델링하는 능력이 내재적 능력(후열차에 의해 주입되는 것보다)이라고 보는 경향이 있다. 즉, 기본 모델은 모델이 더 짧게 트레이닝되더라도 4K 의존성보다 더 길게 모델링할 수 있는 능력을 이미 가지고 있고, 포스트-트레인/파인튜닝 절차는 단순히 이러한 능력을 방출한다.\n' +
      '\n' +
      '## 3 Finetuning\n' +
      '\n' +
      '우리의 미세 조정 방법은 수량보다 데이터 품질을 크게 강조한다. 우리의 접근 방식은 FLAN[9] 및 UltraChat[19]와 같은 기존 데이터 집약적 접근 방식을 따르며, 이는 SFT 데이터를 수백만 개의 항목으로 확장하지만 규모가 너무 크기 때문에 각 항목을 주의 깊게 조사하지 않을 수 있다. 대조적으로, 우리의 방법은 스케일링보다는 데이터 선택에 초점을 맞추는 LIMA[94] 및 DEITA[48] 접근법과 일치한다. 규모가 10K 미만인 경우 모든 단일 데이터 점_을 조사하고 최적화할 수 있다. 아래에서는 데이터 구축 및 교육 세부 사항에 대해 논의합니다.\n' +
      '\n' +
      '### Data Preprocessing\n' +
      '\n' +
      '품질 All You Need our finetuning dataset은 10K 미만의 멀티턴 명령어-응답 다이얼로그 쌍으로 구성되며, 엔트리 각각은 다중 반복 및 사용자 피드백에 걸쳐 구성되고 연마된다. 예비 실험에서 수십만 개의 엔트리의 오픈 소스 데이터와 비교하여 더 작고 수동으로 주석이 달린 데이터 세트의 결과가 우수하다는 것을 관찰하기 때문에 이 접근법을 취한다. 이러한 관찰은 Gemini Team et al. [23], Touvron et al. [77], Zhou et al. [94]에 보고된 것과 일치한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline Models & Hidden Size & Q-heads & KV-heads & Layers & Pretrain Seq. Len & Max LR \\\\ \\hline\n' +
      '6B & 4096 & 32 & 4 & 32 & 4096 & \\(3\\times 10^{-4}\\) \\\\\n' +
      '34B & 7168 & 56 & 8 & 60 & 4096 & \\(1.5\\times 10^{-4}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: Yi-6B 및 Yi-34B의 모델 구성. LR은 학습률을 의미한다.\n' +
      '\n' +
      '우리는 신속한 배포 선택, 응답 형식화 및 사고 연쇄 형식화를 개선하기 위해 다음과 같은 기술을 사용한다. 즉각적인 배포 선택, WizardLM[83]에서 영감을 얻기 위해 복합 지침을 개발하고 복잡성을 높이기 위해 점진적으로 진화했다. 이 접근법은 실험에서 SFT 데이터의 크기를 크게 줄였다. 응답 형식을 지정하기 위해 일반적으로 LIMA[94]에서 확장된 기본 스타일을 사용합니다. 전반적으로, 응답은 신체가 일반적으로 총알 포인트의 목록인 도입-신체-결론 형식으로 구성된다; (3). CoT 데이터 포맷팅을 위해, 우리는 독창적이고 더 구체적인 질문들에 대한 추론을 파고들기 전에 더 높은 레벨의 솔루션들을 공식화하기 위해 추상화를 수행함으로써 Zheng et al. [92]에서 영감을 얻은 "Step-Back" 패턴을 사용한다.\n' +
      '\n' +
      '우리는 환각과 반복을 줄이기 위해 추가적인 노력을 기울인다. (1) 환각을 줄이기 위해 우리는 응답에 대한 지식이 모델 내에 포함되지 않도록 조사하고 확인하고 암기로 이어질 수 있는 응답을 제거한다. (2). 반복을 줄이기 위해 우리는 보통 존재하지만 미세 조정 데이터에서 간과될 수 있는 응답의 반복 회전을 다시 쓴다.\n' +
      '\n' +
      '다양성과 혼합물은 다양한 능력의 범위를 보장하기 위해 질문 응답, 창의적 쓰기, 대화, 추론, 수학, 코딩, 안전, 이중 언어 능력 등과 같은 영역을 포함하는 광범위한 오픈 소스 프롬프트를 포함했다.\n' +
      '\n' +
      'InsTag[49]에서 영감을 받아 다양한 기능 방향을 세밀하게 제어할 수 있는 명령어 태깅 시스템을 개발한다. 다양성 중심의 샘플링 알고리즘을 설계함으로써 다양한 태그에 걸친 명령어 분포의 균형을 세심하게 맞추었다. 이 접근법은 향상된 교차 작업 견고성을 달성하는 것을 목표로 다양한 미세 조정 데이터 세트를 보장한다.\n' +
      '\n' +
      '성능의 서로 다른 방향의 균형을 맞추기 위한 최적의 데이터 비율을 얻기 위해 근사 그리드 검색을 사용하여 데이터 혼합을 결정합니다. 동 등에 의해 동기부여된 [20], 이 프로세스는 각각의 능력에 대해 {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} 비율로 실험하는 것을 수반하였다. 검색 프로세스는 검증 결과와 사내 인간 평가 세트에 의해 안내되었다.\n' +
      '\n' +
      'ChatML FormatBeyond the focus of data quality and diversity, our observations is revealed the format of data is substantially influenced the ultimate performance of the model. 이를 위해, 우리는 ChatML-스타일 포맷을 구현했다[53]. 이러한 구조화된 접근법은 모델이 시스템 구성, 사용자 입력 및 어시스턴트 응답과 같은 다양한 정보 유형을 구별할 수 있게 한다.\n' +
      '\n' +
      '### Training Method\n' +
      '\n' +
      '우리는 다음 단어 예측 손실을 미세 조정에 사용하고 응답에 대한 손실만 계산하지만 시스템 및 사용자 명령은 계산하지 않는다. 우리는 \\(\\beta_{1}\\)을 0.9로, \\(\\beta_{2}\\)을 0.999로, \\(\\epsilon\\)을 \\(10^{-8}\\)으로 설정한 AdamW 최적화기를 사용한다. 시퀀스 길이는 4096, 배치 크기는 64로 설정하였으며, 학습 속도는 일정(1\\times 10^{-5}\\), 가중치 감소율은 0.1, 기울기 클리핑은 최대 임계값 1.0, NEFTune[34]는 Yi-34B-Chat의 경우 45, Yi-6B-Chat의 경우 5로 설정하였다.\n' +
      '\n' +
      '## 4 Infrastructure\n' +
      '\n' +
      '우리는 풀스택 데이터 처리, 사전 훈련, 미세 조정 및 서비스를 지원하는 인프라를 구축합니다. 우리의 인프라 기능은 다음과 같습니다. 컴퓨팅 리소스를 자동으로 관리하고 모니터링하는 단계; (2). 최적화된 병렬 전략, 커널 효율성 및 긴 컨텍스트 지원으로부터 훈련 속도를 향상시켰다; (3). Direct Preference Optimization (DPO) [59] (4). 양자화, 연속 배치 및 호출된 주의와 같은 다양한 LLM 서빙 가속화에 의해 배치 비용을 감소시킨다. 아래에서는 이러한 기법들을 하나씩 설명한다.\n' +
      '\n' +
      '자원 관리 컴퓨팅은 수천 개의 GPU에서 수개월이 걸릴 수 있는 대규모 언어 모델 개발, 특히 사전 훈련을 효율적으로 스케줄링하기 위해, 서로 다른 우선순위의 사전 훈련, SFT 및 RLHF 작업을 관리하기 위해 매우 효율적인 다중 클라우드 작업 스케줄링 알고리즘을 구축한다. 또한 GPU 가용성을 기반으로 사전 훈련 작업을 다양한 노드 크기로 자동으로 탄력적으로 확장할 수 있는 고성능 사내 훈련 프레임워크를 구축한다. 더 중요한 것은, 모든 훈련 관련 하이퍼-파라미터들이 동시에 매끄럽게 스케일링될 것이다.\n' +
      '\n' +
      '대규모 언어 모델 학습 단계에서는 GPU 충돌에서 통신 패브릭 오류, 손실 스파이크에 이르기까지 광범위한 장애가 정기적으로 발생한다. 이러한 신뢰성 문제를 해결하기 위해 다음과 같은 전략을 사용한다. (1) 다양한 종류의 소프트웨어/하드웨어 오류 범주에 대해 노드의 자동화된 검사, 예측 및 레이블링을 적용한다. 오염된 것으로 표시된 노드는 오류가 제거될 때까지 리소스 풀에서 일시적으로 제거됩니다. (2) 사전 점검과 훈련 작업 중 장애 발생 시 빠르고 자동으로 복구할 수 있는 능력을 갖춘 작업 큐잉 시스템을 구현한다. (3) 사용자 친화적인 다중 작업 제출 및 관리 콘솔을 개발하여 개발자가 훈련 작업 및 하이퍼 파라미터를 원활하게 관리하고 추적할 수 있도록 한다.\n' +
      '\n' +
      '성능 및 비용 효율성_메모리_ 및 _통신_ 제한은 더 많은 GPU를 추가하는 것을 넘어 통합 솔루션을 요구하는 대규모 모델 훈련의 두 가지 주요 기술적 과제이다. 메모리 및 통신 제약을 해결하기 위해 (1) ZeRO-1[60]을 사용하고 개선한다. (2) 각 컴퓨팅 노드 내의 파이프라인 병렬[70]과 결합된 텐서 병렬은 노드 간 통신 병목 현상을 피하기 위해, 그리고 3D 병렬 전략은 활성화 체크포인팅을 사용하지 않고 파이프라인 버블을 최소화하기 위해 잘 설계 및 최적화된다. (3) 플래시 어텐션[15][14] 및 JIT 커널과 같은 커널 융합 기술은 중복 전역 메모리 액세스 및 소비를 줄이기 위해 사용된다. (4) 토폴로지 인식 자원 할당(순위 전략)은 일반적인 팻트리 토폴로지의 한계인 스위치 계층 간 통신을 최소화하기 위해 사용된다.\n' +
      '\n' +
      '사전 훈련과는 다른 미세 조정 프레임워크는 DPO[59] 및 PPO[54]의 관행과 마찬가지로 여러 모델의 조정을 요구할 수 있다. 이러한 트레이닝 작업에서, 전형적인 프로세스는 (또한 자명하지 않은 시간을 필요로 하는) 데이터의 배치를 예측하기 위해 참조/보상 모델을 사용하고, 그 다음 타겟 모델이 손실 및 업데이트 파라미터를 계산하기 위해 이 데이터를 사용하도록 하는 것이다. 이를 위해 단일 작업에서 서로 다른 LLM에 대해 다중 백엔드를 지원하는 다중 모델 스케줄링 프레임워크를 구축한다. 예를 들어, DPO로 언어 모델을 미세조정할 때, 참조 모델로부터의 중간 결과들이 캐싱되고 재사용될 수 있어, 훈련 속도 및 리소스 비용이 감독된 미세조정 대응물들에 근접하도록 개선된다.\n' +
      '\n' +
      '빠르고 효율적인 추론에서는 주로 양자화, 동적 배치 및 페이지 어텐션을 사용하여 디코딩 속도와 메모리 사용량을 개선한다. 우리는 메모리 풋프린트와 계산 요구량을 모두 줄이기 위해 양자화를 사용한다. 4-비트 모델 양자화[81]와 8-비트 KV 캐시 양자화[18]에 의해, MMLU/CMMLU 벤치마크에서 거의 0 이하의 성능 저하(예를 들어, \\(1\\%\\) 미만의 정확도 저하)로 상당한 GPU 메모리 절약을 달성할 수 있다. 동적 배치[86]를 사용하여 응답 시간을 최소화하고 배치 효율을 개선한다. PagedAttention[41]을 사용하여 메모리 사용률을 개선하고 디코딩을 개선합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline  & **Size** & **MMLU** & **BBH** & **C-Eval** & **CMMLU** & **Gaokao** & **CR** & **RC** & **Code** & **Math** \\\\ \\hline\n' +
      '**GPT-4** & - & **83.0** & **86.7** & 69.9 & 71.0 & 72.3 & *89.3** & **65.3** & *66.1**\n' +
      '**GPT-3.5** & - & 69.1 & 70.1 & 52.5 & 55.5 & 51.1 & 83.1 & - & 54.8 & 35.6 \\\\ \\hline\n' +
      '**Qwen** & 14B & 66.7 & 53.4 & 72.1 & 71.0 & 62.5 & 74.2 & 72.5 & 40.6 & 43.1 \\\\ \\hline\n' +
      '**Llama2** & 34B & 62.6 & 44.1 & - & - & - & 71.1 & 68.9 & 27.8 & 24.2 \\\\  & 70B & 69.7 & 64.9 & 50.1 & 53.3 & 23.3 & 72.7 & 72.3 & 38.4 & 35.2 \\\\ \\hline\n' +
      '**Baichuan-2** & 13B & 55.0 & 49.0 & 59.0 & 61.97 & 45.6 & 66.3 & 62.4 & 23.4 & 16.1\\\\\\\n' +
      '**InternLM** & 20B & 62.1 & 52.5 & 58.8 & 59.0 & 45.5 & 78.3 & - & 34.8 & 30.26 \\\\ \\hline\n' +
      '**Skywork** & 13B & 62.1 & 41.7 & 60.6 & 61.8 & 68.1 & 72.4 & 61.4 & 64.9 & 18.1 \\\\ \\hline\n' +
      '**Falcon** & 180B & 70.4 & 54.0 & 57.8 & 58.0 & 59.0 & 74.4 & - & - & - \\\\ \\hline \\multirow{2}{*}{**Yi**} & 6B & 63.2 & 42.8 & 72.0 & 75.5 & 72.2 & 72.2 & 68.7 & 21.1 & 18.6 \\\\  & 34B & 76.3 & 54.3 & **81.4** & **83.7** & **82.8** & 80.7 & **76.5** & 32.1 & 40.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 오픈 소스 기반 모델과 비교하여 그룹화된 학술 벤치마크에 대한 전반적인 성능. **CR**는 상식 추론의 약자입니다. **RC**는 읽기 이해의 약자입니다.\n' +
      '\n' +
      'Long-context Window Support 우리는 최대 200K 컨텍스트 길이 프리트레이닝 및 미세조정을 지원하기 위해 계산-통신 중첩, 시퀀스 병렬성, 통신 압축을 구현하고 개선한다. 컨텍스트 길이를 200K로 확장하는 방법은 공학을 기반으로 한 _solely_이다. 즉, 우리는 모델 아키텍처를 희소, 로컬 또는 슬라이딩 윈도우 어텐션과 같이 수정하지 않는다 - 모델은 입력이 200K일지라도 완전한 어텐션을 사용하여 남아 있다.\n' +
      '\n' +
      '## 5 Safety\n' +
      '\n' +
      '모델의 신뢰성과 안전성을 높이기 위해 풀스택 책임 AI 안전 엔진(RAISE)을 개발합니다. RAISE는 안전한 사전 훈련, 정렬 및 배치를 보장합니다. 이 섹션에서는 사전 훈련 및 정렬 단계에서 안전 조치에 대해 설명합니다.\n' +
      '\n' +
      '표준 사전 훈련 데이터 안전 관행[5; 58; 77]과 사전 훈련 데이터 정렬에서 안전성 개인 식별자와 개인 데이터가 포함된 텍스트를 제거하고 성적, 폭력적, 극단적 콘텐츠를 줄이기 위해 휴리스틱 규칙, 키워드 매칭 및 학습된 분류기를 기반으로 필터 세트를 구축한다.\n' +
      '\n' +
      '[24; 35]의 기존 연구에 의해 정보화된 정렬에서의 안전은 먼저 포괄적인 안전 분류법을 구축한다. 이 분류법은 환경 부조화, 미신, 종교적 민감성, 차별적 관행, 약물 남용, 폭력적 행동, 불법 활동, 혐오 발언, 윤리적 위반, 사생활 침해, 자해, 성적 명시적 내용, 정신 건강 문제 및 사이버 안보 위협을 포함한 광범위한 잠재적 우려를 포함한다. 강력한 정렬을 위해 이러한 범주를 반영하는 데이터 세트를 선별하고 대화 상자 SFT 데이터와 혼합한다. 또한 정렬 단계에서 공격 시나리오를 시뮬레이션하는 타겟팅된 프롬프트 세트를 포함하여 악의적인 사용에 대한 모델의 복원력을 효과적으로 개선했다.\n' +
      '\n' +
      '## 6 Evaluations\n' +
      '\n' +
      '이 모델 가족은 다양한 작업에서 고무적인 성능을 달성하고 GPT-3.5 사용자 선호율에 가깝다는 것을 보여준다. 먼저 표준 벤치마크에 대한 기본 모델 성능을 보고하고 채팅 모델 성능과 사용자 선호율에 대해 논의한다.\n' +
      '\n' +
      '##### 기본 모델 성능\n' +
      '\n' +
      '###### 6.1.1 주요 결과\n' +
      '\n' +
      '여기에서 표준 학술 벤치마크에 걸쳐 기본 모델과 여러 다른 잘 알려진 기본 모델에 대한 결과를 제시한다. 오픈 소스 모델을 벤치마킹하는 동안 파이프라인에 의해 생성된 결과와 공개 소스에 보고된 결과 사이의 불일치를 관찰했다. 이 차이에 대한 보다 심층적인 조사를 수행할 때, 대부분 다른 모델이 다른 프롬프트, 후처리 전략 및 샘플링 기술을 사용하기 때문이다. 이러한 차이는 잠재적으로 결과에 상당한 변화를 유발할 수 있다. 우리의 프롬프트 및 후처리 전략은 원래 벤치마크의 기본 설정[2; 4; 7; 8; 10; 11; 12; 27; 28; 42; 50; 61; 62; 63; 72; 74; 75; 89; 90]과 일치한다. 생성된 콘텐츠에 대한 후처리 없이 탐욕 디코딩을 사용한다. 공개적으로 보고되지 않은 점수(또는 다른 설정으로 보고된 점수)의 경우 파이프라인으로 결과를 얻으려고 합니다. 공적으로 찾을 수 있는 점수에 대해서는 기존 수치를 직접 보고합니다. 우리는 주로 LLaMA 2[77]의 관행에 따라 다음과 같은 벤치마크를 사용한다.\n' +
      '\n' +
      '상식 추론:**: 상식 추론을 평가하기 위해 PIQA[4], SIQA[63], HellaSwag[89], WinoGrande[62], ARC[11], OpenBookQA(OBQA)[50], CommonsenseQA(CSQA)[75]를 포함하였다. CSQA는 7-샷 설정을 사용하여 독점적으로 테스트되었으며 다른 모든 테스트는 0-샷 구성으로 수행되었다.\n' +
      '** 읽기 이해:**: 읽기 이해의 경우 SQuAD[61], QuAC[8] 및 BoolQ[10]에서 0-shot 평균을 보고한다.\n' +
      '**Math:**: GSM8K[12] (8 shot), MATH[28] (4 shot) 벤치마크에 대해 특별한 프롬프트 전략(예: Chain-of-Thought 프롬프트) 및 기타 앙상블 기법(예: 다수 투표) 없이 pass@1 정확도로 평균을 보고한다.\n' +
      '\n' +
      '**Code:**: 우리는 HumanEval[7] (Chen et al., 2021) 및 MBPP[2] (Austin et al., 2021)에 우리의 모델들의 평균 pass@1 스코어들을 보고한다.\n' +
      '**Popular Aggregated Benchmark:**: 우리는 MMLU[27](5-shot), CMMLU[42](5-shot), Gaokao-Bench[90](5-shot), BigBench[72] Hard(BBH[74])(3-shot)에 대한 전반적인 결과를 보고한다.\n' +
      '\n' +
      '이전 작업(보통 \\(\\leq 2\\)T)에 비해 훨씬 더 많은 수의 토큰(3.1T)에 대해 훈련함으로써 표 2와 같이 벤치마크 간에 상당한 성능 향상을 관찰했지만, 특히 수학 및 코딩과 관련된 작업에서 우리의 모델과 기존 오픈 소스 및 클로즈 소스 모델 간에 여전히 식별 가능한 차이가 있음을 주목하는 것이 중요하다. 이러한 영역의 성능은 지속적인 사전 훈련 및 명령어 미세 조정을 통해 크게 향상될 수 있으므로 초기 설계 선택 시 사전 훈련 코퍼스에 광범위한 수학적 및 코딩 콘텐츠를 통합하는 것을 자제했다. 향후 수학 및 코딩 기능이 강화된 모델을 출시할 계획입니다.\n' +
      '\n' +
      '#### 6.1.2 Discussions\n' +
      '\n' +
      '모델 척도에서 얻을 수 있다.**: Yi-34B가 Yi-6B에 비해 상당한 성능 향상이 있음을 관찰하지만 동일한 사전 훈련 말뭉치를 활용했다. 모델 크기가 클수록 탭(Tab)을 참조하여 코드 및 수학 벤치마크에서 성능이 향상됩니다. 3은 상식 추론, 읽기 이해 또는 지식에 초점을 맞춘 벤치마크와 비교된다.\n' +
      '**Data Quality.**: Yi-34B 또는 Qwen-14B와 같은 더 높은 품질의 프리트레인 데이터의 더 작은 모델은 일반적으로 더 큰 크기의 모델보다 더 나은 성능을 보여주지만(아마도) Falcon-180B와 같은 더 낮은 품질의 데이터(Falcon-180B의 초점은 스케일링 측면에 더 있을 수 있지만, 이는 분명히 그 자체로 중요한 가치가 있다).\n' +
      '**GPT-4와 오픈 소스 LLMs.** 사이의 갭: 탭에 기초하여. 2, 우리는 오픈 소스 LLM이 다양한 벤치마크에서 GPT-4 및 GPT-3.5의 성능에 여전히 뒤처져 있다는 점에 주목한다. 그러나 Qwen-14B 및 Yi-34B와 같은 대표적인 이중 언어 LLM은 C-Eval[31], CMMLU[42], 가오카오[90]를 포함한 중국 지식 관련 벤치마크에서 GPT-4의 성능과 일치하거나 심지어 능가할 수 있다. 그러나 BBH[72], 코드(HumanEval), 수학(MATH)과 같은 추론 관련 벤치마크에 대한 GPT-4와 오픈소스 모델 사이에는 여전히 엄청난 격차가 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **GSM8k** & **MATH** & **Human-Eval pass@1** & **MBPP pass@1** \\\\ \\hline\n' +
      '**GPT-3.5** & - & 57.1 & 14.0 & 48.1 & 61.4\\\\\n' +
      '**GPT-4** & - & **92.0** & **40.2** & **67.0** & **63.6** \\\\ \\hline\n' +
      '**Falcon** & 180B & 54.4 & - & 0.61 & 47.0 \\\\ \\hline\n' +
      '**Qwen** & 7B & 51.7 & 11.6 & 29.9 & 34.0 \\\\  & 14B & 61.3 & 24.8 & 32.3 & 48.9 \\\\ \\hline\n' +
      '**Baichuan 2** & 7B & 24.5 & 5.6 & 18.3 & 28.3 \\\\  & 13B & 22.1 & 10.1 & 20.7 & 26.1 \\\\ \\hline\n' +
      '**LLaMa 2** & 7B & 16.7 & 3.3 & 12.8 & 14.8 \\\\  & 34B & 42.2 & 6.2 & 22.6 & 33.0 \\\\  & 70B & 56.8 & 13.5 & 31.7 & 45.0 \\\\ \\hline\n' +
      '**Mistral** & 7B & 47.5 & 11.3 & 30.5 & 47.5 \\\\ \\hline\n' +
      '**InternLM** & 20B & 62.9 & 10.9 & 28.1 & 41.4 \\\\ \\hline\n' +
      '**Skywork** & 7B & 55.8 & 7.8 & 13.4 & 22.8 \\\\ \\hline\n' +
      '**Vi** & 6B & 32.5 & 4.6 & 15.9 & 26.3 \\\\  & 34B & 67.2 & 14.4 & 23.2 & 41.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: GSM8k, MATH, Human-Eval 및 MBPP에 대한 모델의 비교.\n' +
      '\n' +
      'Context Learning Study 6.1.3 InContext Learning Study\n' +
      '\n' +
      '본 논문에서는 몇 번의 입력 출력 시연을 통해 주어진 맥락 내 학습 능력, 즉 기본 함수를 추론할 수 있는 능력을 추가로 조사한다. 가중 합계의 선형 계수를 추론하는 작업을 고려한다. 구체적으로 \\(y=w_{1}x_{1}+w2x_{2}+...+ w_{n}x_{n}\\, 우리의 수샷 시연은 \\(x_{1},x_{2},...,x_{n},y\\)이며, 우리는 새로운 입력 집합이 주어졌을 때 \\(y\\)을 예측하여 (암묵적으로) \\(w_{1},w_{2},...,w_{n}\\)을 추론할 것을 모델에 요청한다. 우리는 (a)를 사용한다. 모델 예측(y\\(y\\)과 지상진리(y^{*}\\), 즉 \\(|y-y^{*}}|\\)의 절대차이를 연속척도로 사용하고 (b)를 사용한다. 불연속 측도로서 정확한 일치 \\(y==y^{*}\\)이다. 우리는 또한 대부분의 모델이 덧셈과 뺄셈에 대해 합리적으로 잘 수행하므로 교란 요인으로서 연산을 수행하는 능력을 배제할 수 있다는 점에 주목한다.\n' +
      '\n' +
      '결과는 그림 3과 같다. 선형 계수를 [1, -1]로 설정할 때 Yi 34B와 LLaMA-2 70B가 정답 정확도의 가장 좋은 항을 수행함을 알 수 있다. 우리가 선형 계수의 수를 [1, 1, 1, 1, 1]로 늘리면, 표적에 대한 차이는 더 연속적이지만 큰 모델(LLaMA-2 70B 및 Mixtral)만이 정확한 일치에서 좋은 점수를 얻을 수 있는 출현 행동을 관찰한다. 이러한 관찰은 In-context 학습에 대한 Yi-34B의 성능에 대한 측면 증거를 제공하며 추가 스케일링이 모델이 In-context 학습에 의해 더 복잡한 기능을 추론할 수 있음을 나타낸다.\n' +
      '\n' +
      '그림 3: 가중합의 선형 계수를 추론하여 언어 모델의 맥락 내 학습 능력을 평가한다. 창발 능력이 측정의 인공물인지 여부에 대한 논의[65]를 고려하여 표적과의 차이(표적 수 - 모형 예측)를 연속 측정으로, 정확 일치(표적 수 == 모형 예측)를 불연속 측정으로 사용한다. A: 두 개의 선형 계수가 있을 때 Yi-34B는 목표 수에 대한 차이로 측정할 때 가장 잘 수행한다. B: 선형 계수의 수를 5로 늘리면, 충분히 큰 모델(LLaMA2 70B 및 Mixtral 8x7B)만이 의미 있는 정확한 일치를 달성할 수 있으며, 이는 상황 내 학습 복합 함수가 창발적 능력임을 보여준다.\n' +
      '\n' +
      '########## 대화 모델 성능\n' +
      '\n' +
      '본 절에서는 채팅 모델의 자동 및 인간 선호도 평가를 보고한다. 우리는 탐욕스러운 디코딩을 사용하여 응답을 생성한다. 자동 평가 벤치마크를 위해 모델의 생성된 출력에서 답변을 추출하고 정확도를 계산한다. 평가 과정에서 서로 다른 프롬프트가 결과에 다양한 영향을 미친다는 것을 관찰했다. 따라서 동일한 질문 세트에 대해 동일한 프롬프트를 사용하여 모든 모델을 평가하여 가능한 공정하고 편견 없는 결과를 보장하는 것을 목표로 한다.\n' +
      '\n' +
      '1 자동평가\n' +
      '\n' +
      '자동 평가를 위해 Sec. 6.1.1에 자세히 설명된 기본 모델과 동일한 벤치마크를 사용하며, 제로 샷과 소수 샷 방법을 모두 사용하지만 일반적으로 제로 샷이 채팅 모델에 더 적합하다. 우리의 평가는 명시적으로 또는 암시적으로(몇 개의 샷 예제의 형식과 같은) 지침을 따르면서 응답을 생성하는 것을 포함한다. 그런 다음 생성된 텍스트에서 관련 답변을 분리한다. 기본 모델과 달리 GSM8K 및 BBH 데이터 세트에 대한 제로 샷 평가를 위해, 우리는 답을 얻기 전에 숙의에서 모델을 안내하는 CoT(Chain-of-Thought) 접근법을 사용한다.\n' +
      '\n' +
      '결과는 탭에 나와 있습니다. 4는 인간의 지시를 이해하고 적절한 지시를 따르는 응답을 생성하는 데 있어 우리의 채팅 모델의 효과를 보여준다. 특히 4비트 양자화는 메모리 요구량을 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4비트 양자화 결과를 강조한다. 이 관찰은 소비자 등급 장치에서 모델을 제공하는 기초가 된다.\n' +
      '\n' +
      '굿하트의 원칙에 따라 측정 메트릭이 우리의 추구의 대상이 될 때 신뢰할 수 있는 평가 기준 역할을 중단한다. 결과적으로 벤치마크에 대한 평가 결과는 정렬 교육이 기본 모델의 기본 지식과 능력에 해로운 영향을 미치지 않도록 하기 위해 독점적으로 사용된다. 우리는 벤치마크 성능을 향상시키기 위한 목적으로 채팅 모델의 목표 최적화에 참여하지 않습니다.\n' +
      '\n' +
      '우리 모델의 능력의 일반화 가능성을 추가로 평가하기 위해 xAI 그로크 팀이 먼저 제안한 다음 패스터가 재현한 2023년 헝가리 고등학교 수학 기말고사 문제를 적용하여 수학적 계산 능력에 대한 평가를 수행했다. 이 평가는 우리 모델이 수학적으로 지향된 훈련 데이터 세트에 과적합하는 징후를 나타내는지 여부를 결정하기 위한 목적으로 수행되었다. 그 결과는 다음과 같다. 도 4는 Yi-34B-Chat이 GSM8K와 헝가리 수학 시험 모두에서 고무적으로 수행한다는 것을 보여준다. 그러나, Yi-6B-Chat은 (GSM8K와 헝가리 수학 시험 모두에서) 강력한 수학적 능력을 나타내지 않는다. 우리는 더 작은 모델이 SFT 단계에서 해당 능력을 활성화하기 위해 더 많은 데이터가 필요할 수 있다고 추측한다.\n' +
      '\n' +
      '######6.2.2 인체평가\n' +
      '\n' +
      '이 절에서는 모델의 효과와 안전성을 보장하기 위해 측면을 고려하여 모델의 대화 능력에 대한 평가를 수행했다. 우리는 알파카-eval[21], 벨-eval[88], MT-bench[93]와 같은 커뮤니티의 오픈 소스 평가 데이터 세트를 수집했다. 또한 채팅 모델의 대화 능력을 종합적으로 평가하기 위해 다양한 난이도의 데이터를 수집하고 구성하여 유용하고 무해한 평가 데이터 세트를 구축했다.\n' +
      '\n' +
      '그러나 공공 평가 집합이든 자체 구축 평가 집합이든 평가 결과는 평가 기준과 프롬프트의 설계에 크게 영향을 받는다. 우리의 내부 평가 결과는 다른 모델에 불공평할 수 있으므로 모델의 진정한 능력 수준을 정확하게 나타내기 어렵다. 따라서 여기서는 채팅 모델의 현재 대화 능력을 입증하기 위해 외부 평가 결과만 제시한다. 우리는 (1)을 고려한다. AlapcaEval1[44]는 승률을 계산하기 위해 Davinci003[21]의 참조 응답에 지정된 모델의 응답을 비교하여 모델의 영어 회화 능력을 평가하도록 설계되었다. 대화 플랫폼을 통해 서로 다른 모델의 반응을 보여주는 LMSys2[93] 챗봇 아레나(Chatbot Arena)는 사용자에게 선호도에 따라 선택하도록 한 다음 Elo 점수를 계산한다; (3). 반면 슈퍼클루3는 모델의 중국어 능력을 종합적으로 평가하기 위한 리더보드다.\n' +
      '\n' +
      '각주 3: [https://www.superclueai.com/](https://www.superclueai.com/)\n' +
      '\n' +
      '탭 5는 3개의 제3자 평가에서 Yi-34B-Chat의 성능 결과를 제시하며, 그 결과에 대한 컷오프 날짜는 2023년 12월 21일이다. 데이터는 GPT-4에 비해 여전히 격차가 있지만, 우리의 모델이 능숙한 이중언어(중국어 및 영어) 대화 능력을 보여주고 사용자 선호도와 잘 일치한다는 것을 보여준다. 다양한 모델의 추가 비교 결과는 공식 웹사이트에서 검토할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **MMLU** & **CMMLU** & **C-Eval(val)** & **TruthfulQA** & **BBH** & **GSM8K** \\\\  & \\multicolumn{3}{c}{**0-shot / 5-shot 0-shot / 5-shot 0-shot / 5-shot**} & **0-shot 0-shot / 5-shot** & **0-shot / 3-shot** & **0-shot / 4-shot** \\\\ \\hline\n' +
      '1LaMA2-Chat & 13B & 50.9 / 47.3 & 27.5 / 35.1 & 27.9 / 35.9 & 36.8 & 32.9 / 58.2 & 36.9 / 2.7 \\\\  & 70B & 59.4 / 59.9 & 36.1 / 41.0 & 35.0 / 41.3 & 54.0 & 42.4 / 58.5 & 47.1 / 58.7 \\\\ \\hline Baichuan2-Chat & 13B & 55.1 / 50.1 & 58.6 / 59.5 & 56.0 / 54.8 & 49.0 & 38.8 / 47.2 & 45.7 / 23.3 \\\\ \\hline Qwen-Chat & 14B & 64.0 / 65.0 & 67.7 / 70.6 & 66.1 / 70.1 & 52.5 & 49.7 / 55.0 & 59.5 / 61.2 \\\\ \\hline InterLM-Chat & 20B & 55.6 / 57.4 & 53.6 / 53.8 & 51.2 / 53.6 & 51.8 & 42.4 / 36.7 & 15.7 / 43.4 \\\\ \\hline AquilaChat2 & 34B & 65.2 / 66.7 & 67.5 / 70.0 & **83.0 / 89.4** & **64.3** & 20.1 / 34.3 & 11.5 / 48.5 \\\\ \\hline Yi-Chat & 6B & 58.2 / 61.0 & 69.4 / 74.7 & 68.8 / 74.2 & 50.6 & 39.7 / 47.2 & 38.4 / 44.9 \\\\ Yi-Chat-8bits(GPTQ) & 6B & 58.3 / 61.0 & 69.2 / 74.7 & 69.2 / 73.9 & 49.9 & 40.4 / 47.3 & 39.4 / 44.9 \\\\ Yi-Chat-4bits(AWQ) & 6B & 56.8 / 59.9 & 67.7 / 73.3 & 67.5 / 72.3 & 50.3 & 37.7 / 43.6 & 35.7 / 38.4 \\\\ \\hline Yi-Chat & 34B & **67.6** / 73.5 & **79.1 / 81.3** & 77.0 / 78.5 & 62.4 & 51.4 / **71.7** & **71.7** / **76.0** \\\\ Yi-Chat-8bits(GPTQ) & 34B & 66.2 / **73.7** & 79.1 / 81.2 & 76.8 / 79.0 & 61.8 & **52.1** / 71.0 & 70.7 / 75.7 \\\\ Yi-Chat-4bits(AWQ) & 34B & 65.8 / 72.4 & 78.2 / 80.5 & 75.7 / 77.3 & 61.8 & 48.3 / 69.4 & 70.5 / 74.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 오픈 소스 채팅 모델과 비교하여 자동 벤치마크에 대한 전반적인 성능. 4비트 양자화는 메모리 요구량을 크게 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4비트 양자화 결과를 강조한다. 이 관찰은 RTX4090과 같은 소비자 등급 장치에서 모델을 제공하는 기초 역할을 한다.\n' +
      '\n' +
      '그림 4: 헝가리 수학 시험의 Lee의 결과.\n' +
      '\n' +
      '또한 데이터 스케일링 시 선호도 증가 속도를 비교하여 데이터 품질을 입증한다. 도 1에 도시된 바와 같다. 도 5를 참조하면, UltraChat[19] 및 그 클리닝된 버전 UltraChat200K와 비교할 때, Yi 데이터를 스케일링할 때 성능 개선의 분명한 경향을 보인다.\n' +
      '\n' +
      '##7 능력 확장\n' +
      '\n' +
      '이 절에서는 Yi 베이스 모델을 200K 롱컨텍스트로 확장하고 시각적 이해 능력을 갖추며 깊이 업스케일링을 통해 6B 모델을 향상시키기 위한 사후 훈련 방법에 대해 논의한다.\n' +
      '\n' +
      '### 긴 상황 모델링\n' +
      '\n' +
      '우리의 긴 컨텍스트 솔루션은 지속적인 사전 훈련과 미세 조정 단계로 구성되며 둘 다 가볍다. 200K 입력 컨텍스트 내에서 정보를 활용할 수 있는 잠재력은 이미 기본 모델( Fu et al. 22와 동일)에 존재한다는 기본 가설을 가지고 있으며, 이러한 능력은 Needle-in-a-Haystack 테스트에서 강력한 성능으로 입증되는 사전 훈련 단계 "잠금 해제"를 지속하고, 미세 조정 단계는 인간의 지시와 선호도에 따라 응답 스타일을 추가로 조정한다.\n' +
      '\n' +
      '계속 사전 훈련 우리는 시퀀스 병렬성[43]을 사용하여 완전 주의 모델을 사전 훈련하고 주의를 분산한다. 즉, 우리는 희박하거나 선형적인 주의를 사용하지 않고 완전한 주의의 무차별적인 힘 구현을 사용한다. 우리는 (1)의 데이터 혼합물에 Yi 6B/34B 기본 모델을 사전 훈련한다. 원본 사전 훈련 데이터는 섹션 2; (2)에 소개된 바와 같다. 길이 업샘플링된 긴 컨텍스트 데이터이며, 긴 문서는 대부분 책에서 가져온 것입니다. (3). 다중 문서 질문 응답 합성 데이터, 즉 질문에 답하기 전에 관련 단락의 암시가 포함된 QA 쌍을 구성한다. 우리의 데이터 접근 방식은 Fu et al. [22] 및 Yu et al. [87]의 데이터 엔지니어링 실습을 대부분 따른다. 우리는 100개의 최적화 단계로 번역되는 4M 배치 크기의 5B 토큰에서 모델을 사전 훈련한다. Fu et al. [22]의 동시 작업과 정렬하면, 우리는 그림 6에서 볼 수 있듯이 이러한 경량 연속 사전 훈련이 이미 Needle-in-a-Haystack 테스트에서 강력한 성능을 가능하게 할 수 있음을 관찰한다.\n' +
      '\n' +
      '감독 Finetuning우리는 짧은 컨텍스트 SFT 데이터와 긴 컨텍스트 문서 질문 응답 데이터를 혼합한다. 우리는 문서 QA를 구성하기 위해 모델 지원 자동화 방법(즉, 합성 데이터)을 사용한다. 구체적으로, 우리는 무작위로 여러 문서를 시퀀스로 연결하고, 긴 시퀀스로부터 하나 이상의 단락을 샘플링하고, 질문 및 답변 쌍을 구성하기 위해 채팅 모델을 요청한다.\n' +
      '\n' +
      '도 5: SFT 데이터 스케일링 곡선. 울트라챗 및 세척된 버전 울트라챗 200K와 비교하여 SFT 데이터는 명확한 스케일링 이점을 보여준다. 우리는 그것의 가파른 경사를 데이터 품질 탓으로 돌린다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Model** & **Size** & **AlpacaEval** & **LMSys Chatbot Arena** & **SuperClue** \\\\ \\hline GPT-4-Turbo & - & **97.7** & **1243** & **89.79** \\\\ GPT-3.5-Turbo & - & 89.37 & 1117 & 59.39 \\\\ LLaMA2-Chat & 70B & 92.66 & 1077 & - \\\\ Yi-Chat & 34B & 94.08 & 1110 & 71.87 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 다른 오픈 소스 채팅 모델과의 인간 평가 비교.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      'Yi-VL 모델들은 3단계 트레이닝 과정을 거친다:\n' +
      '\n' +
      '**단계 1:**: \\(224^{2}\\)의 이미지 해상도를 사용하여 ViT와 프로젝션 모듈의 파라미터를 학습한다. 트레이닝은 LAION-400M[66]으로부터 100만 개의 이미지-텍스트 쌍을 포함하는 실질적인 데이터 세트를 활용한다. 주요 목표는 지정된 아키텍처 내에서 ViT의 지식 습득을 강화하고 ViT와 LLM 간의 더 나은 정렬을 달성하는 것이다.\n' +
      '**단계 2:**: 우리는 ViT의 이미지 해상도를 \\(448^{2}\\)으로 확대함으로써 복잡한 시각적 세부 사항을 식별하는 모델의 능력을 더욱 향상시키는 것을 목표로 한다. 이 단계에서 사용된 데이터세트에는 LAION-400M에서 파생된 \\(20\\)백만 개의 이미지-텍스트 쌍이 포함된다. 또한, 다양한 소스에서 약 4.8\\(4.8\\)백만 개의 이미지-텍스트 쌍, 예를 들어, CLLaVA[45], LLaVAR[91], Flickr[85], VQAv2[25], RefCOCO[37], Visual7w[95] 등을 통합한다.\n' +
      '**단계 3:**: 전체 모델의 파라미터가 트레이닝된다. 주요 목표는 모델의 멀티모달 채팅 상호 작용에 대한 숙련도를 향상시켜 시각적 및 언어적 입력을 원활하게 통합하고 해석할 수 있는 능력을 부여하는 것이다. 이를 위해, 훈련 데이터 세트는 GQA[32], VizWiz VQA[26], TextCaps[71], OCR-VQA[51], Visual Genome[39], ShareGPT4V[6] 등 약 백만 개의 이미지-텍스트 쌍을 포함하는 다양한 소스들을 포함한다. 데이터 밸런싱을 보장하기 위해 단일 소스에서 최대 데이터 기여도를 상한으로 지정하여 \\(50,000\\) 쌍 이하로 제한한다.\n' +
      '\n' +
      '1단계와 2단계에서는 전체 배치 크기, 학습률, 기울기 클립 및 에폭의 수를 각각 \\(4096\\), \\(1\\mathrm{e}{-4}\\), \\(0.5\\) 및 \\(1\\)으로 설정하였다. 3단계에서는 이 변수들을 \\(256\\), \\(2\\mathrm{e}{-5}\\), \\(1.0\\), \\(2\\)으로 조정한다. 이 훈련은 엔비디아 A100 GPU를 소모한다. 총 훈련 시간은 Yi-VL-6B의 경우 약 3일, Yi-VL-34B의 경우 약 10일이었다.\n' +
      '\n' +
      '표 7은 Yi-VL의 릴리스에 의한 MMMU 테스트 세트 리더보드를 보여준다. 이 지역은 현재 커뮤니티의 발전에 따라 연구가 활발히 진행 중이며 업데이트 Yi-VL의 성능을 지속적으로 개선할 것이다.\n' +
      '\n' +
      '### Depth Upscaling\n' +
      '\n' +
      '스케일링 법칙[29; 30; 36]에 대한 최근 연구는 계산 예산, 모델 크기 및 데이터 크기가 증가함에 따라 예측 가능한 모델 성능 개선을 강조했다. 그러나 계산 예산을 확장할 때 모델과 데이터 크기 사이의 가장 효과적인 자원 분포를 식별하는 것은 스케일링 법칙 분야에서 만만치 않은 과제로 남아 있다. 또한, DeepSeek-AI 등에 의해 수행된 연구[17]는 모델 스케일링을 위한 증가된 계산 예산의 할당이 이용 가능한 데이터의 품질에 비례해야 한다는 것을 강조했다. 이러한 통찰력에 비추어, 우리는 일련의 단계적 훈련 프로세스를 통해 데이터와 모델 크기 간의 자원 할당을 동적으로 조정하는 것을 목표로 하는 새로운 접근법을 제안한다. 이 전략은 스케일링 법칙에 따라 데이터 특성과 모델 크기 간의 균형을 반복적으로 미세 조정하여 모델 훈련 효율성과 성능을 모두 향상시킨다.\n' +
      '\n' +
      '도 7: Yi-VL 모델들의 아키텍처. 기호는 3개의 훈련 단계에서 다양한 모듈의 훈련 상태를 나타내기 위해 사용된다: 화재 아이콘()은 모듈의 매개 변수가 훈련 가능한 반면 눈송이 아이콘()은 매개 변수가 동결된 것을 나타낸다. 각 단계에서 ViT에 사용되는 이미지 해상도는 \\(224^{2}\\) 또는 \\(448^{2}\\)으로 표시된다.\n' +
      '\n' +
      'Kim et al. [38]에 의해 요약된 방법론에 따르면, 우리의 목표는 원래 16개의 중간 레이어 12-28을 복제함으로써 32개의 레이어를 갖는 Yi-6B 베이스 모델을 48개의 레이어를 특징으로 하는 Yi-9B 베이스 모델이라는 9B 모델로 업스케일링하는 것이다. 깊이 업스케일링은 베이스 모델의 깊이를 확장하고 후속적으로 향상된 모델에 대한 사전 훈련 단계를 계속하는 것을 포함한다.\n' +
      '\n' +
      '우리의 조사는 각 계층의 입력과 출력 사이의 코사인 유사성 점수를 평가하여 복제할 계층에 대한 결정을 알릴 수 있음을 보여준다. 이러한 접근법은 추가 사전 훈련을 필요로 하지 않고 표적 모델 스케일링을 허용하여 최소한의 성능 영향만 초래한다. 성능에 대한 이러한 최소한의 영향은 그림 8에서 알 수 있듯이 복제된 레이어의 입력과 출력 사이에 접근하는 높은 코사인 유사성에 기인한다. 이 관찰은 이러한 레이어의 복제가 출력을 크게 변경하지 않는다는 것을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Overall** & **Art** & **Business** & **Science** & **Health** & **Society** & **Engineering** \\\\ \\hline GPT-4V & 55.7 & 65.3 & 64.3 & 48.4 & 63.5 & 76.3 & 41.7 \\\\ Yi-VL-34B & 41.6 & 56.1 & 33.3 & 32.9 & 45.9 & 66.5 & 36.0 \\\\ Qwen-VL-PLUS & 40.8 & 59.9 & 34.5 & 32.8 & 43.7 & 65.5 & 32.9 \\\\ Marco-VL & 40.4 & 56.5 & 31.0 & 31.0 & 46.9 & 66.5 & 33.8 \\\\ Yi-VL-6B & 37.8 & 53.4 & 30.3 & 30.0 & 39.3 & 58.5 & 34.1 \\\\ InfMIM-Zephyr-7B & 35.5 & 50.0 & 29.6 & 28.2 & 37.5 & 54.6 & 31.1 \\\\ SVIT & 34.1 & 48.9 & 28.0 & 26.8 & 35.5 & 50.9 & 30.7 \\\\ Emu2-Chat & 34.1 & 50.6 & 27.7 & 28.0 & 32.4 & 50.3 & 31.3 \\\\ BLIP-2 FLAN-T5-XXL & 34.0 & 49.2 & 28.6 & 27.3 & 33.7 & 51.5 & 30.4 \\\\ InstructBLIP-T5-XXL & 33.8 & 48.5 & 30.6 & 27.6 & 33.6 & 49.8 & 29.4 \\\\ LLAVA-1.5-13B & 33.6 & 49.8 & 28.2 & 25.9 & 34.9 & 54.7 & 28.3 \\\\ Qwen-VL-7B-Chat & 32.9 & 47.7 & 29.8 & 25.6 & 33.6 & 45.3 & 30.2 \\\\ SPHINX* & 32.9 & 50.9 & 27.2 & 25.3 & 34.1 & 51.2 & 27.8 \\\\ mPLUG-OWL2 & 32.1 & 48.5 & 25.6 & 24.9 & 32.8 & 46.7 & 29.6 \\\\ BLIP-2 FLAN-T5-XL & 31.0 & 43.0 & 25.6 & 25.1 & 31.8 & 48.0 & 27.8 \\\\ InstructBLIP-T5-XL & 30.6 & 43.3 & 25.2 & 25.2 & 29.3 & 45.8 & 28.6 \\\\ CogVLM & 30.1 & 38.0 & 25.6 & 25.1 & 31.2 & 41.5 & 28.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MMMU test set performance by the time of Yi-VL\'s release.\n' +
      '\n' +
      '도 8: 텍스트 "비트에 관한 퀴즈 작성"에 대한 레이어별 각 토큰의 입출력 코사인 유사도 점수. 아래 그림과 같이 새로 추가된 16개 층(층 28-44)의 코사인 유사성 점수는 거의 1인 것으로 관찰된다.\n' +
      '\n' +
      '원래 모델에서 생성된 로짓입니다. 이 방법은 레이어의 내부 처리 역학에 기초하여 아키텍처를 최적화함으로써 모델의 효율적인 스케일링을 보장한다.\n' +
      '\n' +
      '연속 훈련 데이터 세트는 두 단계에 걸쳐 약 8,000억 개의 토큰으로 구성되며, 약 70%가 최근에 수집되고 신중하게 선택되었다. 최종 단계에서 코드 커버리지를 향상시켜 코드 성능을 향상시켰습니다.\n' +
      '\n' +
      '학습 프로세스를 최적화하기 위해 3e-5의 일정한 학습률을 유지하고, 모델의 손실이 안정될 때마다 4M 토큰에서 배치 크기를 점진적으로 증가시키는 전략적 접근법을 채택한다. 설정된 Yi-6B 기본 모델 구성과 정렬하여 다른 모든 매개변수를 유지하는 것과 함께 배치 크기의 이러한 점진적 조정은 규모에서 훈련의 문제를 해결하는 데 중요한 역할을 했다.\n' +
      '\n' +
      '이러한 전략의 효과는 상식, 추론, 지식, 코딩 및 수학을 포함한 다양한 벤치마크에 걸쳐 Yi-9B 기본 모델의 성능을 자세히 설명하는 표 8에 나와 있다. 이는 특정 도메인에서 Yi-9B 기반 모델의 경쟁 우위를 강조하며, 데이터 특성과 모델 크기 간의 상호 작용을 최적으로 조정하여 모델 성능을 향상시키는 방법론의 유효성을 보여준다.\n' +
      '\n' +
      '## 8 최종 토론\n' +
      '\n' +
      '이 보고서에서는 Yi 언어 모델 패밀리의 풀 스택 개발에 대해 논의한다. Yi-34B는 GPT-3.5 매칭 성능을 달성하고 소비자 등급 장치에 배포(4/8 비트 양자화 감사) 가능하여 로컬 배포에 이상적인 모델이다.\n' +
      '\n' +
      'Lee 사전 훈련 절차의 주요 테이크아웃은 데이터 양과 품질에 관한 것이다: (1). 친칠라 최적보다 더 많은 양의 데이터로 모델을 훈련하면 명확하고 일관된 성능 이득을 얻을 수 있으며, 이는 모든 사전 훈련 팀에 적극 권장된다. 우리의 모델은 3.1T 토큰으로 훈련되지만 더 많은 양의 데이터로 인해 모델 성능을 계속 개선할 수 있다(즉, 모델이 3.1T에서 포화되지 않음); (2). 사전 훈련 데이터 품질에 관한 한, 우리는 가장 중요한 두 가지 요소는 데이터의 출처(예: 텍스트가 전문적인 사용을 위해 생산되는지 또는 일상적인 소셜 미디어 포스팅을 위해 생산되는지)와 데이터 청소의 세부 사항(예: 필터링 및 중복 제거의 강도)이라고 믿는다. 데이터 청소는 매우 복잡한 파이프라인이며 광범위한 그리드 검색 스타일 최적화를 수행하는 것이 매우 어렵기 때문에 현재 솔루션은 여전히 개선의 여지가 있을 수 있다.\n' +
      '\n' +
      'Lee finetuning 과정의 핵심 테이크아웃은 적은 양의 데이터(\\(\\leq\\) 10K), 사례별로, 다중 반복에 걸쳐, 기계 학습 엔지니어가 직접, 실제 사용자 피드백으로부터 크게 반복하는 것이다. 이 접근법은 처음에 FLAN 시리즈[9]에 이어 울트라챗 시리즈[19]에 의해 도입된 명령어-스케일링 접근법에서 분명히 벗어났다.\n' +
      '\n' +
      '본 연구의 결과를 통해 증명된 바와 같이, 언어 모델의 실제 배치를 위한 핵심 능력으로 간주되는 추론 능력은 사전 훈련 데이터의 양이 고정되어 있을 때 모델 규모와 강한 상관 관계가 있다. 현재 결과를 감안할 때 철저하게 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 다음 버전에서 훨씬 더 강력한 프론티어 모델이 될 것이라고 믿습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **Arc-C** & **HellaSwag** & **MMLU** & **Winogrande** & ** GSM8K** & **MATH** & **HumanEval** & **MBPP** \\\\ \\hline\n' +
      '**Vi-6B** & 50.3 & 74.4 & 63.2 & 71.3 & 32.5 & 4.6 & 15.9 & 26.3\\\\\n' +
      '**Vi-9B Init** & 52.1 & 73.3 & 63.0 & 69.4 & 31.3 & 4.1 & 12.8 & 25.8\\\\\n' +
      '**Vi-9B** & **55.6** & **76.4** & **68.4** & **73.0** & **52.3** & **15.9** & **39.0** & **54.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: Yi-6B와 Yi-9B 간의 성능: Arc Challenge(25-shot), HellaSwag(10-shot) MMLU(5-shot), Winogrande(5-shot), GSM8K(5-shot), MATH(4-shot), HumanEval pass@1, MBPP pass@1(3-shot) Yi-9B Init는 추가 훈련 없이 레이어 12-28을 복제함으로써 Yi-6B로부터 깊이 상향 스케일링하는 것이다.\n' +
      '\n' +
      '작성자 목록 및 기여도\n' +
      '\n' +
      '우리 팀원들은 다음과 같은 관점에서 Yi의 발전에 기여한다.\n' +
      '\n' +
      '* 프론티어 연구\n' +
      '* 머신 러닝 인프라\n' +
      '* Pretraining\n' +
      '* 파인튜닝 및 AI 얼라인먼트\n' +
      '\n' +
      '* Multimodal\n' +
      '* 안전 및 책임 있는 AI\n' +
      '* Deployment\n' +
      '\n' +
      '우리는 팀원들을 알파벳 순으로 나열한다. 모든 저자는 이 작업에 동등하게 기여했다.\n' +
      '\n' +
      '* 알렉스 영*\n' +
      '* Bei Chen\n' +
      '* 차오리\n' +
      '*hengen Huang\n' +
      '* Ge Zhang\n' +
      '*관웨이 장\n' +
      '*헝리\n' +
      '* 장청주\n' +
      '*젠쿤첸\n' +
      '*징창\n' +
      '* 카동유\n' +
      '* 팽류\n' +
      '* 치앙류\n' +
      '*숀 유\n' +
      '양선빈\n' +
      '* Multimodal\n' +
      '* 안전 및 책임 있는 AI\n' +
      '* Deployment\n' +
      '* Shimning Yang\n' +
      '*타오유\n' +
      '* Wen Xie\n' +
      '* 원하오황\n' +
      '*샤오후\n' +
      '* 샤오이 렌\n' +
      '* 신야오 니우\n' +
      '* 펭청니\n' +
      '* 유치슈\n' +
      '*유동류\n' +
      '*유왕\n' +
      '* 유슈안 카이\n' +
      '*진유구\n' +
      '* 지위안 류\n' +
      '*종홍대\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. _arXiv preprint arXiv:2305.13245_, 2023.\n' +
      '* [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program Synthesis With ILarge Language Models. _arXiv preprint arXiv:2108.07732_, 2021.\n' +
      '* [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen Technical Report. 09 2023. URL [https://arxiv.org/pdf/2309.16609.pdf](https://arxiv.org/pdf/2309.16609.pdf).\n' +
      '* [4] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. _ArXiv_, abs/1911.11641, 2019. URL [https://api.semanticscholar.org/CorpusID:208290939](https://api.semanticscholar.org/CorpusID:208290939).\n' +
      '* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '* [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code. _CoRR_, abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).\n' +
      '* [8] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC : Question Answering in Context, 2018.\n' +
      '* [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.\n' +
      '* [10] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, 2019.\n' +
      '* [11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018.\n' +
      '* [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. _arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '\n' +
      '* [13] Together Computer. Redpajama: an open dataset for training large language models, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).\n' +
      '* [14] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n' +
      '* [15] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.\n' +
      '* [16] Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference. _arXiv preprint arXiv:2212.08153_, 2022.\n' +
      '* [17] DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. 2024.\n' +
      '* [18] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm: int8 (): 8-bit matrix multiplication for transformers at scale. _arXiv preprint arXiv:2208.07339_, 2022.\n' +
      '* [19] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.\n' +
      '* [20] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition, 2023.\n' +
      '* [21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.\n' +
      '* [22] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. _arXiv preprint arXiv:2402.10171_, 2024.\n' +
      '* [23] Gemini Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* [24] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.\n' +
      '* [25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017.\n' +
      '* [26] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617, 2018.\n' +
      '\n' +
      '* [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. _CoRR_, abs/2009.03300, 2020. URL [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300).\n' +
      '* [28] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. _arXiv preprint arXiv:2103.03874_, 2021.\n' +
      '* [29] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewowo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. 2020.\n' +
      '* [30] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [31] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_, 2023.\n' +
      '* [32] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.\n' +
      '* [33] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL [https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773).\n' +
      '* [34] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Sompalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neptune: Noisy embeddings improve instruction finetuning. _arXiv preprint arXiv:2310.05914_, 2023.\n' +
      '* [35] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llvm via a human-preference dataset, 2023.\n' +
      '* [36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 2020.\n' +
      '* [37] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 787-798, 2014.\n' +
      '* [38] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling. 2023.\n' +
      '* [39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.\n' +
      '* [40] Taku Kudo and John Richardson. SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. _arXiv preprint arXiv:1808.06226_, 2018.\n' +
      '* [41] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. _arXiv preprint arXiv:2309.06180_, 2023.\n' +
      '\n' +
      '* [42] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring Massive Multitask Language Understanding in Chinese. _arXiv preprint arXiv:2306.09212_, 2023.\n' +
      '* [43] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. _arXiv preprint arXiv:2105.13120_, 2021.\n' +
      '* [44] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaceval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.\n' +
      '* [45] LinkSoul-AI. Chinese llava. [https://github.com/LinkSoul-AI/Chinese-LLAVA](https://github.com/LinkSoul-AI/Chinese-LLAVA), 2023.\n' +
      '* [46] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [48] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. _arXiv preprint arXiv:2312.15685_, 2023.\n' +
      '* [49] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models, 2023.\n' +
      '* [50] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, 2018.\n' +
      '* [51] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE, 2019.\n' +
      '* [52] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. _arXiv preprint arXiv:2309.09400_, 2023.\n' +
      '* [53] OpenAI. ChatML, 2022. URL [https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md).\n' +
      '* [54] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training Language Models to Follow Instructions with Human Feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* [55] Keiran Paster. Testing language models on a held-out high school national finals exam. [https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam](https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam), 2023.\n' +
      '* [56] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only, 2023.\n' +
      '* [57] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently Scaling Transformer Inference. _Proceedings of Machine Learning and Systems_, 5, 2023.\n' +
      '* [58] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. _arXiv preprint arXiv:2112.11446_, 2021.\n' +
      '\n' +
      '* [59] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* [60] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE, 2020.\n' +
      '* [61] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text, 2016.\n' +
      '* [62] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.\n' +
      '* [63] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. SocialIQA: Commonsense Reasoning about Social Interactions, 2019.\n' +
      '* [64] Nikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. _arXiv preprint arXiv:2401.00448_, 2023.\n' +
      '* [65] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? _Advances in Neural Information Processing Systems_, 36, 2024.\n' +
      '* [66] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [67] Noam Shazeer. Fast Transformer Decoding: One Write-Head is All You Need. _arXiv preprint arXiv:1911.02150_, 2019.\n' +
      '* [68] Noam Shazeer. GLU Variants Improve Transformer. _arXiv preprint arXiv:2002.05202_, 2020.\n' +
      '* [69] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte Pair Encoding: A Text Compression Scheme That Accelerates Pattern Matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.\n' +
      '* [70] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. _arXiv preprint arXiv:1909.08053_, 2019.\n' +
      '* [71] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.\n' +
      '* [72] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Erfat, Aykat Erdem, Ayla Karakas, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartomiej Bojanowski, Bathan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri Ramirez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Dami Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi,Daniel Levy, Daniel Mosegui Gonzalez, Danielle Perszyk, Danny Hernandez, Danqi Chen, and Daphne Ippolito et al. (351 additional authors not shown). Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 2023.\n' +
      '* Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 로포머: 회전 위치 임베딩이 있는 향상된 트랜스포머. _ arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challengeing BigBench Task and Whethergain-of-Thought can Solve Them. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '* Talmor et al. (2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: Question Answering Challenge Targeting Commonsense Knowledge, 2019.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Touvron et al. (2021) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Bhargava, Shruti Bhosale, Dan Bikel, Likas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Bucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Bynthia Kardas, Vedan Helun, Vedan Kelun, Saghar Hosseini, Rui Hungbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, E. Michael Smith, R. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J. J Llama 2: Open Foundation and Fine-Tuned 채팅 모델, 2023.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 주목만 하시면 됩니다 Advances in Neural Information Processing Systems_, 06 2017. URL[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf).\n' +
      '* Wenzek et al. (2019) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Web Crawl Data로부터 고품질 Monolingual Datasets을 추출하는 단계; _ arXiv preprint arXiv:1911.00359_, 11 2019. URL[https://arxiv.org/pdf/1911.00359.pdf](https://arxiv.org/pdf/1911.00359.pdf).\n' +
      '* Wenzek et al. (2019) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Web Crawl Data로부터 고품질 Monolingual Datasets을 추출하는 단계; _ ArXiv preprint arXiv:1911.00359_, 2019.\n' +
      '* Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. 변압기 모델을 위한 int4 양자화에 대한 이해: 지연 속도 향상, 컴포지토리성 및 고장 사례 _ arXiv preprint arXiv:2301.12017_, 2023.\n' +
      '* Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankaraman, Barlas Oguz, et al. Foundation model의 효과적인 long-context scaling. _ arXiv preprint arXiv:2309.16039_, 2023.\n' +
      '*Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: 복잡한 명령어를 따르기 위해 대규모 언어 모델의 권한을 부여합니다. _ arXiv preprint arXiv:2304.12244_, 2023.\n' +
      '*양 등(2021) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chhenxu Lv, Da Pan, Dian Wang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Hai Deng Zhang, Hohze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyo Ma, MangWang, Mickel Liu, Mingan Lin, Nuolan Nie, Peidong Sun, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Yiaoxi Chen, Yuphen Zhang, Zenan Zhou, Zieung Zhang, Huze Zhang, Hui Liu, Jiaming Jiu, Jian Xie, JunTao Dai, Luolan N 바이촨 2: 대규모 언어 모델을 엽니다. 09 2023. URL[https://arxiv.org/pdf/2309.10305.pdf](https://arxiv.org/pdf/2309.10305.pdf).\n' +
      '* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.\n' +
      '* Yu et al. [2022] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A Distributed Serving System for Transformer-Based Generative Models. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538, 2022.\n' +
      '* Yu et al. [2023] Yijiong Yu, Zhe Zhou, Zhixiao Qi, and Yongfeng Huang. Paraphrasing the original text makes high accuracy long-context qa. _arXiv preprint arXiv:2312.11193_, 2023.\n' +
      '* Yunjie et al. [2023] Ji Yunjie, Deng Yong, Gong Yan, Peng Yiping, Niu Qiang, Zhang Lei, Ma Baochang, and Li Xiangang. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. _arXiv preprint arXiv:2303.14742_, 2023.\n' +
      '* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine Really Finish Your Sentence?, 2019.\n' +
      '* Zhang et al. [2023] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the Performance of Large Language Models on GAOKAO Benchmark. _arXiv preprint arXiv:2305.12474_, 2023.\n' +
      '* Zhang et al. [2023] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_, 2023.\n' +
      '* Zheng et al. [2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models, 2023.\n' +
      '* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n' +
      '* Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xueze Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment, 2023.\n' +
      '* Zhu et al. [2016] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4995-5004, 2016.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>