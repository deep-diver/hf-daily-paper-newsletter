<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 척도에서의 코드 표현 학습\n' +
      '\n' +
      ' 데자오 장 와시 아흐마드\n' +
      '\n' +
      'dejiaoz,wuahmad@amazon.com\n' +
      '\n' +
      '(주)밍탄앤한천딩\n' +
      '\n' +
      '{mingtan,dhantian}@amazon.com\n' +
      '\n' +
      '\'라메시 날라파티 & 댄 로스 & 샤오페이 마 & 빙샹\'\n' +
      '\n' +
      '{rnallapa,drot,xiaofeim,bxiang}@amazon.com\n' +
      '\n' +
      'Equal Contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 연구에 따르면 코드 언어 모델은 규모 면에서 다운스트림 작업, 즉 코드 생성에 상당한 성능 향상을 보여준다. 그러나 기존의 대부분의 코드 표현 학습은 매우 제한된 사전 훈련 코퍼스를 사용하여 1억 매개 변수 척도로 모델을 학습시킨다. 본 논문에서는 2단계 사전 학습 기법을 통해 방대한 양의 코드 데이터를 갖는 코드 표현 학습에 연료를 공급한다. 먼저 마스킹 언어 모델링의 랜덤성과 프로그래밍 언어의 구조 측면을 모두 활용하는 믹스를 통해 인코더를 훈련한다. 그런 다음 비지도 방식으로 구성된 하드 네거티브 및 하드 포지티브와 대조 학습을 통해 표상을 향상시킨다. 우리는 다양한 다운스트림 작업에서 기존 모델보다 큰 마진으로 지속적으로 능가하는 기성 인코더 모델을 구축한다. 성공적인 코드 표현 학습에 기여하는 요인을 파악하기 위해 세부 삭제를 수행하고 소스 코드에 대한 맞춤형 및 효과적인 토큰 수준 잡음 제거 기법에 대한 연구 결과를 공유한다; _(ii)_ 하드 네거티브 및 하드 포지티브의 중요성; _(iii)_ 제안된 바이모달 대비 학습이 상호 언어 의미 검색 성능을 향상시키는 방법; 및 _(iv)_ 사전 훈련 기법이 모델 크기에 따라 다운스트림 태스크 성능 척도를 결정하는 방법에 대한 연구 결과를 공유한다. 1\n' +
      '\n' +
      '각주 1: 우리의 코드와 모델은 [https://github.com/amazon-science/CodeSage](https://github.com/amazon-science/CodeSage) 및 [https://huggingface.co/codesage](https://huggingface.co/codesage)에서 출시된다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대용량 소스 코드를 미리 훈련한 LLM(Large Language Model)은 코드 생성의 풍경을 재구성하였다 (Chen et al., 2021; Chowdhery et al., 2022; Li et al., 2023, _inter alia_). 예로서, 허용 라이선스 하에서 소스 코드를 포함하는 6TB 데이터세트(Kocetkov et al., 2022)의 최근 릴리스는 현재 코드 언어 모델의 발전을 촉진하는데 중추적인 역할을 한다. 그럼에도 불구하고 이러한 대규모 코퍼스는 범용 프로그래밍 언어(PL) 임베딩 모델을 개발하는 데 완전히 활용되지 않는다. 현재까지, 대부분의 PL 임베딩 모델들(Feng et al., 2020; Guo et al., 2021, 2022, _inter alia_)은 125M 파라미터들을 초과하지 않으며, 주로 수 백만 개의 트레이닝 예들, 예를 들어,_CodeSearchNet(Husain et al., 2019)에 대해 트레이닝된다.\n' +
      '\n' +
      '대규모 데이터의 부인할 수 없는 중요성에도 불구하고 사전 훈련 목적의 중요한 역할을 인정하는 것이 필수적이다. 표현을 학습하기 위해 양방향 트랜스포머 인코더를 사전 트레이닝하기 위한 기존의 접근법은 Devlin et al. (2019)에 의해 제안된 바와 같이, 마스킹 언어 모델링(MLM) 목적의 최적화를 통한 것이다. 표준 MLM 목표의 마스킹 기법은 80-10-10 연습.2를 따르지만, 이러한 마스킹 기법은 차선책 코드 임베딩 모델의 개발로 이어진다는 것을 알 수 있었다. 코드 스니펫들은 자연 언어(NL) 진술들(_i.e.,_ docstrings, 코멘트들) 및 순수 코드를 모두 포함하기 때문에, 따라서 마스킹된 토큰들을 80-10-10 협약에 따른 랜덤 토큰으로 교체하는 것은 NL 토큰을 PL 토큰으로 교체하는 결과를 초래할 수 있고, 그 반대의 경우도 가능하다(부록 A.3의 통계를 참조). 우리는 소스 코드의 구문 특성과 함께 PLand NL의 이러한 동시 발생을 추측하여 마스킹된 코드의 의미론과 구조를 더 쉽게 방해하여 언어 모델의 차선책 학습을 초래한다.\n' +
      '\n' +
      'MLM 사전 훈련은 맥락적 토큰 표현을 산출하지만, 대부분의 다운스트림 판별 작업은 주로 시퀀스 수준에서 기능한다. 순서 수준의 과제에서 즉각적인 적용을 위한 표현 판별력을 향상시키는 것이 목표일 때, 대조 학습(CL)이 자주 등장한다. 기존의 연구들은 표현 학습을 위해 유니모달 CL(Code-Code pair를 사용)(Guo et al., 2022; Jain et al., 2021) 또는 바이모달 CL(Text-Code pair를 사용)(Li et al., 2022)을 사용하였다. 유니모달 CL에서, 인기 있는 선택은 포지티브 코드 쌍들을 구성하기 위해 _dropout_ augmentation Gao et al.(2021)을 활용하는 것이다. 그러나, 우리는 _dropout_ augmentation이 Zhou et al.(2022)에 의해 보고된 긴 훈련 과정을 지원하는데 어려움을 겪는다는 것을 발견했다. 대조적으로, 바이모달 CL은 주로 자연 발생 쌍의 가용성 때문에 매력적인 선택이 된다. 이전 연구에서는 쌍봉 훈련 쌍을 설정하기 위해 기능과 해당 문서 문자열을 사용한다. 그럼에도 불구하고, 우리의 예비 실험은 문서 문자열과 함수 서명 사이의 상당한 중첩이 대조적 학습 프로세스를 단순화한다는 것을 나타낸다(부록 A.6의 통계를 참조).\n' +
      '\n' +
      '이를 위해 소스 코드에 대한 양방향 인코더 표현 모델인 CodeSage를 제시한다. 우리는 많은 양의 맞춤형 프리트레이닝 데이터를 갖는 2단계 트레이닝 스킴을 사용하여 CodeSage를 프리트레이닝한다(Kocetkov et al., 2022). 본 논문에서는 코드세이지의 핵심 구성 요소를 그림 1에 표현하였으며, 먼저 80-10-10 연습 없이 식별자 난독화(DOBF)와 MLM이라는 두 가지 목표를 상호 보완하는 혼합을 통해 양방향 인코더를 훈련한다. 인간 프로그래머와 유사하게 난독화된 식별자에 대한 의미 있는 이름을 찾는 것은 코드 의미와 구조에 대한 깊은 이해를 얻기 위한 모델을 필요로 한다. 한편, 보다 일반적인 목적으로서, MLM은 코드의 식별자를 넘어 다른 패싯을 커버하는데, 이는 특히 비정보 식별자 이름이 있는 데이터 예제의 경우 훈련 신호를 풍부하게 하는 데 중요하다. 두 번째 단계에서 우리는 쌍봉 대조 학습(CL)을 위해 _(텍스트, 코드)_ 쌍을 활용한다. 자연적으로 발생하는 텍스트와 코드 쌍에 주로 의존하는 기존의 접근법과 달리, 우리는 모델 학습 단축키의 가능성을 줄이는 전략을 제안한다. 우리의 접근법은 서명 및 반환 진술을 무시하면서 함수 본문을 독점적으로 활용하는 것을 포함한다. 우리는 임베딩 공간 내에서 식별된 하드 네거티브를 기반으로 CL을 추가로 활용한다. 우리는 이러한 단단한 긍정과 부정 구성 전략이 간단하지만 효과적인 쌍봉 대조 학습을 위해 필수적이라는 것을 보여준다.\n' +
      '\n' +
      '우리는 3가지 양방향 인코더 표현 모델, 즉 CodeSage-small(130M), CodeSage-base(356M) 및 CodeSage-large(1.3B)를 훈련한다. 우리는 코드세이지가 대부분의 작업에서 유사한 모델 크기를 가진 이전 최신 모델보다 실질적으로 우수한 다양한 차별 작업에 대한 접근법의 효과를 평가한다. 성공적인 코드 표현 학습에 기여하는 요인을 이해하기 위해 프레임워크의 핵심 구성 요소를 세심하게 분석하고 향후 연구를 위한 연구 결과를 제시한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '프로그래밍 언어에 대한 임베딩은 프로그래밍 언어에서 다양한 다운스트림 작업을 지원하기 위한 범용 표현 학습에 대한 관심이 급증하고 있다. Feng et al. (2020); Kanade et al. (2020); Li et al. (2023)은 텍스트에서의 성공의 영감을 취하고 선형화된 코드 상에서 Masking Language Modeling (MLM) 목적을 최적화한다.\n' +
      '\n' +
      '그림 1: 코드 표현 학습을 위한 CodeSage의 핵심 구성 요소에 대한 개요.\n' +
      '\n' +
      '데이터. 텍스트와 유사하게, 이들은 소스 코드에 대해 대체 토큰 검출 목적(Clark et al., 2020) 또는 다음 문장 예측 목적(Devlin et al., 2019)으로 추가로 최적화된다. 다른 작업 라인은 추가 훈련 신호를 제공하기 위해 코드의 구조 측면을 활용한다. 그 중 Guo et al.(2021)은 데이터 흐름을 활용하여 변수들 간의 "where-the-value-comes-from"의 관계를 인코딩한다. Wang et al. (2021); Jiang et al. (2021)은 변형 보조 목적들을 통해 추상 구문 트리(AST)로부터 구문 구조를 주입한다. 보다 최근의 작업(Guo et al., 2022)은 AST 구조를 직접 시퀀스로 평탄화하고 언어 모델링 목표를 통해 구문 정보를 인코딩한다. Wang et al. (2021); anne Lachaux et al. (2021)은 클래스, 함수, 및 변수 이름들이 특수 토큰들로 대체되는 식별자-난독화된 코드로부터 원래의 코드를 재구성하기 위해 시퀀스-투-시퀀스 언어 모델을 트레이닝한다. 탈난독화는 식별자의 이름을 올바르게 예측하기 위해 코드 구조뿐만 아니라 변수 간의 종속성을 이해해야 하기 때문에 보조 목적이나 깊은 계층을 가진 복잡한 입력을 포함하지 않고 데이터 흐름과 AST를 암시적으로 인코딩한다.\n' +
      '\n' +
      '대조적 학습은 Siamese(Hadsell et al., 2006) 네트워크에 의해 달성된 초기 성공 이후, 심층 신경망을 사용한 표현 학습에서 대조적 학습이 널리 채택되었다. Song et al.(2016)은 모든 인-배치 네거티브에 대해 각각의 포지티브 예를 대조함으로써 바닐라 삼중항 손실을 확장시키며, 이는 학습 효율을 크게 향상시켰고 SimCLR(Chen et al., 2020)에 의해 더욱 대중화되었다. 그러나, 입력 공간에서의 이미지들의 확률적 변환들에 의해 유효 긍정들이 획득될 수 있는 컴퓨트 버전 도메인과는 달리, 유효 데이터 증강은 입력의 이산적 특성 때문에 NLP에서 오랫동안 도전이었다. 이러한 도전은 최소 데이터 증강으로서 _dropout_(Srivastava et al., 2014)가 이산 입력 공간, _e.g.,_ 단어 삭제 및 대체에서 동작함으로써 얻어지는 것보다 종종 더 효과적임을 보여주는 Gao et al. (2021)에서 추가로 검증된다.\n' +
      '\n' +
      '대안적으로, 자연 발생 쌍들을 포지티브들로서 레버리지하기 위한 다양한 방법들이 제안되었다. Zhou et al. (2022)는 대화 데이터로부터 연속된 발화를 긍정으로 취급하고 Neelakantan et al. (2022)는 인터넷에서 채굴된 이웃 텍스트를 고려한다. 매우 최근의 작품(Wang et al., 2022)은 StackExchange와 Reddit의 질문과 답변 또는 댓글 쌍을 활용한다. 프로그래밍 언어에 대한 유사한 맥락에서, Guo et al. (2022); Wang et al. (2021); Neelakantan et al. (2022) leverage (text, code) pairs with text mined from the docstrings. 우리는 표현 학습의 핵심 요소이며 기성품 임베딩 모델을 얻을 수 있는 하드 포지티브 및 하드 네거티브 구성에 초점을 맞추어 한 걸음 더 나아간다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### 마스크 언어 모델링 및 난독화 사전 훈련\n' +
      '\n' +
      '\\(N\\) 토큰들을 갖는 입력 시퀀스, _i.e.,_\\(\\mathbf{x}=[\\mathbf{x}_{1},\\mathbf{x}_{2},\\ldots,\\mathbf{x}_{N_{s}]\\이 주어지면, 마스크 언어 모델링 목적(Devlin et al., 2019)은 다음과 같이 형성된다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{MLM}(\\mathbf{x})=-\\sum_{i\\in\\mathcal{M}}\\log\\mathbb{P}\\left(\\mathbf{x}_{i}|\\mathbf{x}^{\\mathcal{M}}\\right)\\tag{1}\\t]\n' +
      '\n' +
      '여기서 \\(\\mathcal{M}\\)은 주어진 입력 \\(\\mathbf{x}\\)에 적용된 마스크를 나타낸다. 식 (1)은 본질적으로 마스킹된 수열 \\(\\mathbf{x}^{\\mathcal{M}}\\)이 주어졌을 때 원래 토큰을 예측하는 작업에 대한 잡음 제거 목표이다.\n' +
      '\n' +
      '우리는 먼저 식별자의 마스킹된 이름을 예측하기 위해 모델을 미리 훈련시키는 식별자 디난독화(DOBF)를 고려한다. 인간 프로그래머와 유사하게 코드를 난독화(식별자를 예측)하기 위해 모델은 코드의 의미론과 구조를 모두 이해할 필요가 있다. 또한 자연어(NL) 토큰, 즉 _i.e.,_ docstring 및 코멘트는 코드 난독화에서 제외된다는 점에 유의한다. 모델이 식별자 이름을 예측하도록 훈련될 때, 이는 종종 코드의 풍부한 의미론을 운반하기 때문에 주석 또는 문서 스트링에서 NL 토큰을 보고 상관하는 것으로부터 이익을 얻을 수 있다. 결과적으로, 모델은 표 3의 랜덤 마스킹 전략보다 DOBF에 의해 달성된 더 나은 NL2Code 검색 성능에 의해 나타난 바와 같이 프로그래밍 언어와 자연 언어 사이의 개선된 공유 표현을 학습하도록 장려된다.\n' +
      '\n' +
      'DOBF는 처음에 Seq2Seq 모델들에 대해 제안된다(anne Lachaux et al., 2021; Wang et al., 2021). 우리가 아는 한, 우리는 그것을 인코더 전용 모델에 최초로 적용했다. 인코더 전용 모델을 위한 DOBF 채택의 주요 과제는 코드 토큰화(_i.e.,_tree-sitter_를 사용하는_) 및 모델 고유 토큰화(_i.e.,_sentencepiece_ tokenizer를 사용하는_i.e.)의 차이로 인해 일대일 매핑을 트윈 마스크 토큰(LM에 입력) 및 식별자 토큰(출력 라벨)으로 구성하는 것이다. 우리는 부록 A.5에서 그 도전에 대해 간단히 논의한다.\n' +
      '\n' +
      'Random Masking추가적으로, BERT Devlin et al.(2019)의 랜덤 토큰 마스킹 전략도 두 가지 주요 이유로 포함한다. 첫째, 식별자를 넘어 학습하도록 모델을 촉진함으로써 더 나은 표현을 촉진한다. 파이썬을 예로 들면, 식별자들과 연관된 코드 토큰들의 약 30%가 존재하며, 따라서 토큰들의 나머지 70%에 의해 운반되는 정보를 인코딩함으로써 더 나은 표현들이 달성될 수 있다. 둘째, 모든 프로그래머가 명명 규칙을 따르는 것은 아니며, \\(v1,v2,v3\\)과 같은 무의미한 변수 이름을 사용할 수 있다. 이러한 토큰을 예측하는 것은 불필요하게 어렵고 매우 제한된 훈련 신호를 제공한다.\n' +
      '\n' +
      '우리는 텍스트에 대한 표준 MLM에서 제안된 80-10-10 마스킹 규약을 따르지 않는다(Devlin et al., 2019). 소스 코드는 NL과 코드 토큰(즉, 식별자, 키워드, 연산자)으로 구성되기 때문에 토큰의 무작위 치환은 코드의 구조와 의미를 모두 해칠 수 있으며 표현 학습에서 악화로 이어질 수 있다.3 섹션 4.2.1에서 80-10-10 규칙이 지속적으로 다운스트림 작업에서 더 나쁜 성능을 초래한다는 것을 보여준다. 본 논문에서는 부록 A.4의 절제 연구를 통해 최적의 랜덤 마스킹 비율을 15%로 설정하였으며, 각 훈련 예제에 대해 DOBF 또는 랜덤 마스킹을 동일한 확률로 랜덤하게 선택한다.\n' +
      '\n' +
      '각주 3: 예를 들어, 토큰izer.convert_ids_to_tokens로부터 무작위로 두 개의 토큰을 마스킹하는 것은 토큰izer.convert_ids_to<mask>=mask>를 산출할 수 있지만, 랜덤 토큰 교체는 토큰izer.convert_jet_tobattokens를 초래할 수 있다. 결과적으로 코드 시멘틱은 크게 변경되고 자기 주의 메커니즘을 통한 표현 학습은 악화될 수 있다. 자세한 내용은 부록 A.3을 참조하십시오.\n' +
      '\n' +
      '경성 부정과 경성 긍정이 있는 쌍모달 대비 학습\n' +
      '\n' +
      '[\\(\\mathbf{x}_{i},\\mathbf{x}_{i^{+}}\\)는 양의 입력쌍을 나타내며, [\\(\\mathbf{h}_{i},\\mathbf{h}_{i^{+}}\\)는 인코더의 마지막 은닉층에 의해 출력되는 관련 표현이다. \\(\\mathcal{B}=\\{\\mathbf{h}_{1},\\mathbf{h}_{1^{+}},\\mathbf{h}_{2},\\mathbf{h}_{2},\\ldots,\\mathbf{h}_{N},\\mathbf{h}_{N^{+}}\\})는 \\(N\\)쌍으로 랜덤하게 샘플링된 배치의 표현을 나타내며, 다음 대칭 손실을 최소화한다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{CL}\\left(\\mathbf{h}_{i},\\mathbf{h}_{i}}} \\right=-\\left(\\log\\frac{\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{i}}}/\\tau}{\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{i}}}/\\tau)+\\sum_{k\\in\\mathcal{B}(i,i^{+}}\\gamma_{i}_{i}\\circ\\mathbf{h}_{k}/\\tau}\\right.\\gamma_{i}\\cdot\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{i}\\tau}\\gamma_{i}\\cdot\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{i} \\circ\\log\\frac{\\exp(\\left.+\\mathbf{h}_{i^{+}}\\circ\\mathbf{h}_{i^{+}}\\circ\\mathbf{h}_{i^{+}\\circ\\mathbf{h}_{i^{+}\\circ\\mathcal{B}(i,i^{+}}\\gamma_{i^{+}}}^{k}\\cdot\\exp(\\mathbf{h}_{i^{+}}\\circ\\mathbf{h}_{k}/\\tau)}{\\exp(\\mathbff{h}_{i}_{i}/\\tau)\\circ\\mathbff{h}_{i}/\\tau)+\\sum_{k\\mathcal{B}(i,i^{+}}\\gamma_{i}}}^{k}\\cdot\\exp(\\mathbff{h}_{i^{+}}\\circ\\mathb\n' +
      '\n' +
      '여기서 \\(\\tau\\)는 본 연구에서 0.05로 설정한 온도 하이퍼파라미터이다. \\ (\\diamond\\)는 두 표현 벡터 사이의 코사인 유사도를 나타낸다. \\ (\\gamma_{i}^{k}\\)는 다음으로 자세히 설명할 가중치 매개변수이다.\n' +
      '\n' +
      '감독 없이 하드 네거티브는 하드 네거티브를 식별하는 것이 까다롭다. 우리는 Zhang et al.(2021)에서 제안된 하드 네거티브의 거리 기반 비감독 근사치에 의존한다. 주어진 앵커\\(\\mathbf{h}_{i}\\)에 대해, 하드 네거티브는 의미적으로 다른 예를 언급하지만 표현 공간에서 \\(\\mathbf{h}_{i}\\)에 가깝게 매핑된다. 이에 따라, 표현공간에서 anchor \\(\\mathbf{h}_{i}\\)에 가까울수록 더 큰 \\(\\gamma\\) 값이 요구되며, 이는 다음과 같이 특징지어질 수 있다.\n' +
      '\n' +
      '\\frac{\\exp(\\gamma_{i}}\\circ\\mathbf{h}_{i}\\circ\\mathbf{h}_{k}/\\tau)}{\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{k}/\\tau)+\\sum_{j\\in\\mathcal{B}(i,i^{+},k}\\exp(\\mathbf{h}_{i}\\circ\\mathbf{h}_{j}/\\tau}}. \\tag{3}\\tau}\n' +
      '\n' +
      '즉, \\(\\gamma_{i}^{k}\\)은 모든 \\(2N\\)-2 회분식 부정형 중에서 \\(\\mathbf{h}_{i}\\)에 대한 \\(\\mathbf{h}_{k}\\)의 상대적 중요도를 근사화한다. 주어진 양의 쌍을 제외한 훈련 예제 간의 의미적 동등성에도 불구하고, 우리의 경우 위의 경음의 근사치는 여전히 유효하다. 이를 보기 위해 각 학습 배치가 전체 학습 데이터의 크기에 비해 훨씬 작은 크기로 무작위로 샘플링된다는 점에 유의한다. 따라서 각 배치 내에서 위음의 존재는 훈련 데이터가 충분히 크고 다양한 한 매우 작다. 본 논문에서는 회분식 크기(N)를 8K로 설정하여 다운스트림 작업에서 보고된 단조 증가 성능을 관찰한다.\n' +
      '\n' +
      'Hard PositiveWe는 자연 발생(텍스트, 함수)을 양의 쌍으로 간주하며, 여기서 텍스트는 함수 docstring으로부터 채굴된다(Husain et al., 2019). 문서 문자열에서 추출한 텍스트는 코드의 높은 수준의 의미를 잘 요약하는 경우가 많다. 따라서, bimodal 데이터인 _i.e. 텍스트와 함수 쌍에 대한 대조적 학습은 섹션 4.2.2에서 NL2Code 의미 검색 성능을 크게 향상시키며, 동일한 또는 다른 프로그래밍 언어에 관계없이 의미적으로 동등한 코드의 추출된 텍스트는 코드 자체에 비해 다양성이 떨어지는 경우가 많다. 이에 의해, 의미적으로 유사한 코드들은 동일하거나 매우 유사한 요약 텍스트를 통해 암시적으로 함께 그룹화될 수 있다. 우리의 추측은 섹션 4.2.2에서 인-언어 및 크로스-언어 코드2코드 검색 모두에 대한 대조적 학습에 의해 달성된 큰 성능 이득에 의해 검증된다.\n' +
      '\n' +
      '기능명과 입력 변수명은 특히 요약 텍스트와의 어휘 중복 측면에서 상당한 유사성을 공유하는 경우가 많다는 것도 쉽게 알 수 있다. 우리는 부록 A.6에 자세히 설명된 통계와 이러한 중복을 추가로 정량화한다. 이에 따라 함수 서명과 반환 진술을 모두 제거하여 하드 포지티브를 형성한다.4 섹션 4.2.2에서 입증된 바와 같이, 이러한 방식으로 형성된 하드 포지티브는 대비 학습의 성능을 효과적으로 높일 수 있다.\n' +
      '\n' +
      '각주 4: 함수 시그니처의 제거는 요약 텍스트와의 유사성으로 인해 바로 가기를 학습할 기회를 감소시킨다. 우리는 코드를 일반적인 코드 조각처럼 보이게 하기 위해 반환문을 제거한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'Training Data and Model ArchitectureWe trained our models on The Stack dataset (Kocetkov et al., 2022) over 9 languages - Python, Java, Javascript, Typescript, C#, C, Ruby, Go, 및 PHP. 전술한 바와 같이, 크기 130M(CodeSage-small), 356M(CodeSage-base), 및 1.3B(CodeSage-large) 파라미터를 갖는 세 가지 임베딩 모델을 트레이닝한다. 각 단계의 교육 세부 사항과 모델 하이퍼 파라미터는 부록 A를 참조하시기 바랍니다.\n' +
      '\n' +
      '평가 프로토콜 우리는 다운스트림 작업의 두 가지 주요 범주인 의미 검색 및 분류에 대해 모델의 성능을 평가한다. 우리의 목표는 감독된 미세 조정 데이터 수집 비용이 많이 드는 실제 시나리오에 대한 인코더 모델의 평가를 수행하는 것이다. 따라서 우리는 _zero-shot_ 시맨틱 검색에 초점을 맞추고 분류 작업을 위해 냉동 인코더의 상단에 선형 분류 계층을 미세 조정한다(Peters et al., 2019; Chen et al., 2020; Wang et al., 2022). 우리는 부록 B.3에서 완전히 미세 조정된 분류 결과와 미세 조정 하이퍼 파라미터를 보고한다.\n' +
      '\n' +
      '기준은 모델 선택에 대한 제안을 따라 4개의 범용 코드 표현 학습 인코더와 OpenAI-Ernbedding-Ado-002에 대해 모델을 비교한다.5 CodeBERT(Feng et al., 2020)와 GraphCodeBERT(Guo et al., 2021)는 CodeSearch(Husain et al., 2019)6을 사용하여 6개의 프로그래밍 언어에서 표준 MLM으로 훈련되고, 대체 토큰 탐지 목표(Clark et al., 2020)와 데이터 흐름 예측 목표는 각각 보조 목표로 채택된다. UnibCodef(Guo et al., 2022)는 동일한 데이터세트를 사용하여 3개의 언어 모델링 및 2개의 대조적 학습 목표를 통해 훈련된다. 보다 최근에, 스타엔코더(Li et al., 2023)는 더 스택(Kocetkov et al., 2022)으로부터 86개의 프로그래밍 언어에 대한 MLM 및 다음 문장 예측(Devlin et al., 2019)으로 트레이닝된다. 우리는 부록의 표 6의 각 기준 모델에 대해 더 자세한 내용을 제공한다. 우리는 또한 부록 B에서 디코더 전용 기준선을 고려한다.\n' +
      '\n' +
      '각주 5: OpenAI는 001 모델[https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings])보다 효율성과 우수한 성능으로 인해 OpenAI-Embedding-Ada-002를 사용하는 것을 제안한다.\n' +
      '\n' +
      '각주 6: 데이터 세트는 자연어 문서와 쌍을 이루는 2.3M 기능을 포함한다.\n' +
      '\n' +
      '###기준선과의 비교\n' +
      '\n' +
      '우리는 먼저 다음 작업에서 앞서 언급한 기준선과 CodeSage를 비교한다.\n' +
      '\n' +
      '**Code2Code** 시맨틱 검색은 코드 프래그먼트가 부여된 관련 코드 프래그먼트를 _query_로 검색하는 작업이다. 본 연구에서는 CodeNet에서 생성된 Code2Code 검색 평가 집합(Guo et al., 2022)을 C, C#, Javascript, Typescript, GO, PHP의 6개 언어로 확장하고, 이를 부록 B.2에서 세부사항을 요약한다. 질의어와 후보 코드가 동일한 언어인 경우, 표 1의 Code2Code 검색 결과를 보고한다.\n' +
      '\n' +
      '**NL2Code** 시맨틱 검색은 관련 코드를 검색하기 위한 질의로서 자연 언어를 사용하는 작업이다. 표 2에서 CoSQA(Huang et al., 2021), AdvTest(Lu et al., 2021), CSN(Guo et al., 2021)의 세 가지 벤치마크를 고려한다. 자세한 데이터 통계는 부록 B.2에서 확인할 수 있다.\n' +
      '\n' +
      '**분류** 세 가지 소스 코드 분류 작업을 고려합니다. Code Defect detection은 CodeXGLUE(Lu et al., 2021)의 C에서 벤치마크로서, 코드가 안전하지 않은지 여부를 나타내는 이진 라벨 및 임의의 공격 소프트웨어 시스템을 포함한다. 코드 복잡도 예측(Jeon et al., 2023)은 7개의 레이블 중 알고리즘 복잡도를 예측해야 하는 자바 벤치마크이다. 런타임 에러 예측(Bieber et al., 2023) 벤치마크는 불균형이 심한 분포를 갖는 29개의 가능한 라벨을 갖는다(부록의 표 10 참조). 보다 강력한 평가를 위해 "no_error" 클래스의 전체 학습 예를 다른 28개 클래스의 누적 카운트와 정렬하여 데이터 세트의 균형을 맞춘다.\n' +
      '\n' +
      '전체 성능 요약 코드2코드 검색에서 표 1은 CodeSage-small(130M)가 모든 언어에서 알려진 모델 크기(_i.e.,_ exclude OpenAI-Embedding-Ada-002)를 가진 모든 기준 모델보다 지속적으로 성능이 우수함을 보여주며, UnixCoder와 비교할 때 평균 성능에서 23.19% 상대(4.91% 절대) 개선되었다. 모델 크기가 증가함에 따라 CodeSage-base와 CodeSage-large가 각각 20.56% 상대(5.62% 절대)와 40.91% 상대(11.18% 절대)로 가장 우수한 베이스라인 모델인 _i.e.,_ OpenAI-Embedding-Ada-002(모델 크기 미지)를 능가했다.\n' +
      '\n' +
      '표 2에 도시된 바와 같이, CodeSage-small는 NL2Code 검색 상의 UnixCode에 비해 18.54% 내지 51.1% 상대(7.81% 내지 13.96% 절대) 개선을 달성한다. OpenAI-Embedding-Ada-002와 비교하여 CodeSage-small는 CosQA에서 12.86% 상대(5.69% 절대) 개선, AdvTest에서 8.4% 상대(3.12% 절대) 개선에 도달한다. 반면, OpenAI-Embedding-Ada-002는 CSN에서 CodeSage-large와 동일한 평균 성능을 달성한다. 그러나 우리는 함수 및 변수 이름이 더미 변수로 대체된 정규화된 파이썬 함수(CSN에서)를 포함하는 AdvTest에서 CodeSage에 의해 달성된 성능 이득을 강조하고자 한다(부록의 그림 9 참조). 이러한 방식으로 구성된 AdvTest는 모델이 주어진 자연어 질의에 대한 정확한 타겟 코드를 식별하기 위해 난독화된 코드가 무엇을 하는지 이해해야 하므로 일반화 성능을 더 잘 평가한다.\n' +
      '\n' +
      'UnixCoder와 OpenAI-Embedding-Ada-002에 비해, CodeSage는 표 2에서 큰 마진을 갖는 코드 복잡도 및 런타임 에러 예측에 대해 지속적으로 더 나은 성능을 보인다. 또한 CodeSage는 부록의 표 12에서 전체 모델을 미세조정할 때 더 나은 성능을 얻으면서 코드 결함 검출에 대해 두 모델 모두 낮은 성능을 보인다는 것을 알 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r r r r r r r r r r} \\hline \\hline Model & Python & Java & JS & TS & C\\# & C & Ruby & PHP & GO & Avg \\\\ \\hline CodeBERT & 14.40 & 7.62 & 5.47 & 6.05 & 3.66 & 5.53 & 13.55 & 10.28 & 6.27 & 8.09 \\\\ GraphCodeBERT & 19.23 & 10.78 & 7.38 & 8.65 & 5.54 & 8.48 & 19.69 & 15.67 & 9.65 & 11.68 \\\\ StarEncoder & 19.17 & 11.65 & 9.0 & 10.52 & 5.69 & 9.72 & 21.57 & 16.98 & 10.81 & 12.79 \\\\ UnixCoder & 30.77 & 16.45 & 21.32 & 21.95 & 6.19 & 15.62 & 32.33 & 31.93 & 13.94 & 21.17 \\\\ OpenAI-Ada-002 & 35.91 & 25.13 & 19.01 & 21.86 & 10.17 & 29.15 & 40.85 & 40.47 & 23.43 & 27.33 \\\\ \\hline CodeSage-small & 36.31 & 23.97 & 26.60 & 29.90 & 11.84 & 22.84 & 29.06 & 34.64 & 19.56 & 26.08 \\\\ CodeSage-base & **47.52** & 22.84 & 28.70 & 31.95 & 13.37 & 30.99 & 44.86 & 51.13 & 25.15 & 32.95 \\\\ CodeSage-large & 46.70 & **33.13** & **37.16** & **41.18** & **16.81** & **32.89** & **54.12** & **52.13** & **32.48** & **38.51** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 제로샷 코드 검색 태스크의 MAP 스코어(%). 맨 위 행에 언급된 언어 이름은 언어 쿼리와 후보자가 작성되었음을 나타냅니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r r r r|r r r} \\hline \\hline  & \\multicolumn{3}{c}{NL2Code} & \\multicolumn{3}{c}{Classification} \\\\ \\cline{2-7} Model & CosQA & AdvTest & CSN & Defect & Complexity & RunTime \\\\ \\hline CodeBERT & 0.24 & 0.06 & 0.10 & 51.82\\({}_{0.38}\\) & 35.60\\({}_{1.96}\\) & 6.2\\({}_{0.02}\\) \\\\ GraphCodeBERT & 16.20 & 5.58 & 11.26 & 55.26\\({}_{0.28}\\) & 55.54\\({}_{1.98}\\) & 10.63\\({}_{0.10}\\) \\\\ StarEncoder & 10.78 & 0.93 & 2.69 & 53.2\\({}_{0.11}\\) & 50.63\\({}_{3.33}\\) & 8.91\\({}_{0.05}\\) \\\\ UnixCoder & 42.11 & 27.32 & 46.39 & 60.28\\({}_{0.04}\\) & 76.45\\({}_{1.10}\\) & 20.87\\({}_{0.43}\\) \\\\ OpenAI-Ada-002 & 44.23 & 38.08 & **71.24** & **62.56\\({}_{0.11}\\)** & 79.82\\({}_{0.50}\\) & 20.84\\({}_{0.36}\\) \\\\ \\hline CodeSage-small & **49.92** & 41.28 & 63.86 & 57.52\\({}_{0.21}\\) & 79.76\\({}_{0.50}\\) & **25.05\\({}_{1.04}\\)** \\\\ CodeSage-base & 48.50 & 49.08 & 68.72 & 57.74\\({}_{0.09}\\) & 85.32\\({}_{1.72}\\) & 24.70\\({}_{0.40}\\) \\\\ CodeSage-large & 47.53 & **52.67** & **71.24** & 58.95\\({}_{0.13}\\) & **90.32\\({}_{2.10}\\)** & 24.42\\({}_{0.28}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **좌측. 영점 설정에서 NL2Code 검색의 MRR 점수(%)입니다. CSN의 경우 6개 언어에 대한 평균 성능을 보고한다(자세한 결과는 부록의 표 9 참조). 그래 분류 헤드를 미세 조정함으로써 달성되는 소스 코드 분류 태스크의 F1(매크로) 점수. 세 개의 종자를 사용하여 각 모델을 세분화하고 평균 및 표준 편차(아래 첨자)를 보고했다. 완전히 세분화된 결과는 부록 B.3.**에서 찾을 수 있습니다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '1 마스킹 전략 4.2.1\n' +
      '\n' +
      '1 마스킹 전략 4.2.1\n' +
      '\n' +
      '**80-10-10 vs. Full Mask** 입력 시퀀스가 주어지면, 표준 MLM(Devlin et al., 2019)은 먼저 자신의 토큰들의 서브세트를 랜덤하게 샘플링하며, 그 중 80%는 특수 토큰 "[MASK]"로 대체되고, 10%는 변경되지 않고 남겨지고, 나머지 10%는 어휘로부터의 랜덤 토큰으로 대체된다. 우리는 그림 2의 코드에 대해 원래 텍스트에 대해 제안된 이러한 협약의 유효성을 재검토한다. 놀랍게도, 선택된 모든 토큰들을 단순히 [MASK] 토큰, _i.e.,_ "Full Mask"로 대체하는 것에 비해, 80-10-10 마스킹 스킴은 그림 1(b)에 도시된 바와 같이, 상이한 다운스트림 태스크들에 걸쳐 큰 성능 저하를 야기한다. 유사한 발견이 텍스트에 대해 Gao et al.(2022)에서 보고되었다. 그러나 소스 코드의 경우 열화가 더 심하다. 도 1(a)가 나타내는 바와 같이, 랜덤 토큰들로 대체될 때, 마스킹된 코드의 의미론 및 구조 둘 다 크게 중단될 수 있으며, 이는 "[MASK]" 토큰들의 존재와 함께 학습을 너무 어렵게 만든다(더 많은 논의들을 위해 부록 A.3 참조). 우리는 과도한 부패가 그림 1(b)에서 80-10-10으로 훈련된 모델의 크기를 확장할 때 다운스트림 작업에서 관찰된 완만한 향상을 설명할 수도 있다고 가정한다. 이러한 스케일링 추세가 모델 크기와 훈련 데이터의 추가 증가와 함께 급격한 확장을 경험할 것인지 여부를 탐구하는 것은 흥미로울 것이며, 계산 자원이 그러한 조사를 허용한다면 잠재적으로 상전이 지점을 식별할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{CodeSAGE-small} & \\multicolumn{4}{c}{CodeSAGE-base} & \\multicolumn{4}{c}{CodeSAGE-large} \\\\ \\cline{2-13} Model & R & D & S & P & R & D & S & P & R & D & S & P \\\\ \\hline NL2Code & 6.6 & 19.9 & 22.7 & 25.8 & 12.2 & 22.5 & 22.0 & 23.3 & 19.4 & 23.3 & 29.4 & 30.5 \\\\ Code2Code (In) & 16.8 & 14.6 & 17.9 & 19.7 & 28.2 & 23.7 & 25.3 & 29.2 & 30.7 & 28.2 & 30.2 & 33.9 \\\\ Code2Code (Cross) & 5.7 & 6.7 & 8.8 & 9.6 & 17.2 & 14.1 & 14.6 & 19.7 & 20.5 & 18.0 & 19.0 & 24.6 \\\\ Classification & 51.2 & 53.9 & 53.5 & 53.4 & 53.8 & 55.6 & 54.8 & 55.4 & 52.0 & 55.6 & 57.2 & 56.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: DOBF(D)와 랜덤 마스킹(R)을 활용하여 상호 보완하는 두 가지 옵션을 탐색한다. (1) 순차(S): 랜덤 마스킹으로 모델을 먼저 트레이닝한 후 DOBF. (2) Parallel (P): 훈련 예제를 위해 DOBF 또는 랜덤 마스킹 중 하나를 랜덤하게 선택하는 것 – 우리의 전략.\n' +
      '\n' +
      '도 2: 80-10-10 vs. "완전 마스크"\n' +
      '\n' +
      'Deobfuscation & Random Masking Complement Each Other에 대해 DOBF와 "Full Mask"를 사용한 랜덤 마스킹 기반 MLM을 조사한다. DOBF는 분류에서 랜덤 마스킹을 지속적으로 능가하며, 이는 식별자의 이름을 예측하기 위해 코드 구조를 더 잘 캡처(이해)하기 위해 모델이 촉진된다는 동기를 검증한다. DOBF 또한 랜덤 마스킹보다 NL2Code 탐색에서 더 좋은 성능을 보인다. 잠재적인 이유는 코멘트 및 문서 문자열의 자연 언어일 수 있으며, 둘 다 DOBF에서 마스킹에서 제외되지만 코드의 풍부한 의미론을 가지고 있기 때문에 식별자 이름을 예측하기 위해 모델을 훈련할 때 자연 언어를 보고 상관하고 자연 언어와 프로그래밍 언어 사이의 더 나은 맥락화된 표현으로 이어질 것이다. 반면에, 랜덤 마스킹 전략("Full Mask")은 인-언어 및 크로스-언어 코드2코드 검색 작업 모두에서 DOBF보다 우수하다. 부록 A.3에서 살펴본 바와 같이 코드 스니펫에서 토큰의 상당 부분은 식별자가 아니다. 따라서, 랜덤 마스킹 전략은 모델이 식별자를 넘어 학습하고 표현으로 인코딩된 의미론을 풍부하게 할 수 있게 한다. 요약하면, 표 3은 DOBF와 무작위 마스킹을 공동으로 최적화하여 서로의 장점을 활용하여 상호 보완하는 우리의 전략을 검증한다.\n' +
      '\n' +
      '대비학습의 효과성에 관한 연구\n' +
      '\n' +
      'Hard Positive and Hard Negative Effectively Boost Performance 우리는 먼저 그림 2(a)에서 Hard positive and Hard negative 구축 전략의 효과를 입증한다. 알려진 바와 같이 하드 포지티브와 하드 네거티브는 모두 독립적으로 큰 마진만큼 성능을 향상시킬 수 있는 반면, 이들의 조합은 다양한 모델 크기에 걸쳐 지속적으로 더 나은 성능을 산출한다. 또한, 제안된 하드 네거티브 구축 전략으로부터 큰 모델 크기(_i.e.,_Codesage-base)가 더 많은 이점을 갖는다는 것을 관찰한다. 이 관찰은 더 큰 모델이 더 도전적이고 효과적인 학습 목표를 활용할 수 있는 더 많은 능력을 가지고 있기 때문에 놀라운 일이 아니다.\n' +
      '\n' +
      '유니모달 vs. 바이모달 대비 학습 그림 2(b)에서, 우리는 동일한 시퀀스의 두 개의 포워딩 패스에서 변압기의 서로 다른 드롭아웃 마스크를 레버리지하여 포지티브 쌍을 얻는 _Dropout_ 기반 유니모달 대비 학습 접근법과 비교한다(Gao et al., 2021; Guo et al., 2022). 공정한 비교를 위해 두 접근법 모두에 하드 네거티브 최적화가 적용된다. 우리는 _dropout_ 기반 단모달 대조 학습이 긴 훈련 과정을 지원하는 데 어려움을 겪으므로 표현들을 더욱 향상시키기 위해 많은 양의 사전 훈련 데이터를 효과적으로 활용할 수 없음을 알 수 있다. 유사한 발견이 (Zhou et al., 2022)에 의해 보고되었다. 실제로, Gao et al. (2021) 또는 Guo et al. (2022) 둘 다 - 각각 텍스트 및 코드에 대한 효과적인 증강으로서 드롭아웃을 입증하며, _dropout_ 기반 대조적 학습이 기준선에 대한 개선을 나타내는 도 2(b)의 처음 500회 반복(배치 크기 8K 포함)에서 트레이닝 데이터의 양에 의해 커버될 수 있는 몇 백만 개의 트레이닝 예만을 사용한다.\n' +
      '\n' +
      '사전 훈련의 2단계 동안 대조적 학습을 통해 달성된 성능 향상에 대한 더 깊은 이해를 얻기 위해, 우리는 의미론적 검색 성능 분석에 대해 조사한다. 그림 3(a)에서 볼 수 있듯이 대조적 학습은 교차 언어에서 상대적으로 더 큰 개선으로 검색 성능을 지속적으로 향상시킵니다.\n' +
      '\n' +
      '그림 3: **(a)** 하드 네거티브 및 하드 포지티브 중 어느 것도 적용되지 않는 기준선에 대해 독립적으로 성능을 높일 수 있다. 동시에 활용할 때 추가 개선이 달성됩니다. **(b)** 드롭아웃을 통해 얻은 양성을 가진 유니모달 대조 학습은 더 긴 훈련을 필요로 하므로 표현을 더욱 향상시키기 위해 방대한 양의 훈련 데이터를 활용할 수 없다.\n' +
      '\n' +
      'NL2Code 및 교차 언어 Code2Code 검색을 모두 포함하는 시나리오. 우리는 텍스트가 종종 코드의 높은 수준의 의미를 요약하므로 코드 자체보다 덜 다양할 가능성이 있으므로 문서 문자열에서 추출한 텍스트가 의미적으로 동일한 코드를 함께 그룹화하는 데 도움이 된다고 가정한다. 특히, 상이한 프로그래밍 언어로부터의 이러한 병렬 예들은 매우 유사하거나 심지어 동일한 요약을 공유할 수 있다. NL2Code의 경우, _(text, code)_를 양성으로 사용하는 쌍봉 대조 학습 목표와의 정렬에 더 큰 개선이 인정될 수 있다. 이러한 바이모달 목적은 또한 도 4b에서 NL 및 PL을 더 가깝게 한다. 스테이지-I에서만 훈련된 모델에 비해, 대조적 학습은 병렬 NL2Code 쌍과 교차 언어 코드2Code 병렬 예제 사이의 상대적 유사성 갭이 크게 감소하도록 NL과 PL을 함께 끌어당긴다.\n' +
      '\n' +
      '모델 크기에 따른 객체 및 다운스트림 성능 스케일링에 관한 연구\n' +
      '\n' +
      '그림 5에서, 우리는 다운스트림 태스크 수행이 다른 방식, 즉,_ token-level objective only (Stage-I), contrastive learning only (Stage-II), 그리고 Stage-I 다음에 Stage-II로 제안된 2단계 프레임워크로 미리 훈련될 때 모델 크기에 따라 어떻게 확장되는지 연구한다. 우리는 이 탐색을 위해 _zero-shot_ 다국어 내 언어 코드 검색 성능(평균 9개 언어 이상)을 사용한다. 우리는 대조적 학습만으로 처음부터 사전 훈련된 모델이 증가된 모델 크기와 함께 확장되지 않는다는 것을 알 수 있다. Neelakantan et al.(2022)은 대조적 목적이 그 자체로 유용한 표현들을 학습하기에 충분하지 않다는 유사한 발견을 보고한다. 대조 학습만으로 처음부터 훈련할 때 훈련 손실은 종종 큰 값으로 수렴한다는 것을 발견하며, 이는 모델이 각 긍정 쌍과 다른 배치 부정 쌍을 잘 구별할 수 없음을 나타낸다. 즉, 우수한 임베딩 기반을 제공하기 위해 토큰 레벨 노이즈 제거 목표를 활용하는 것은 대조적 학습이 효과적이고 시퀀스 레벨 프레젠테이션을 더욱 향상시키기 위해 필수적이다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 연구에서는 소스 코드에 대한 최첨단 인코더 표현 학습 모델인 CodeSage를 공개하였다. 우리는 9개 언어에 걸쳐 2억 3,700만 개의 코드 파일과 7,500만 개의 바이모달 코드 및 자연 언어 쌍으로 구성된 광범위한 데이터 세트를 사용하여 코드세이를 훈련시켰다. 우리의 연구 결과는\n' +
      '\n' +
      '도 4: CodeSage를 토큰-레벨 잡음제거 목적만으로 훈련된 것과 비교하여 대조적 학습(Stage-II)의 유효성을 검사한다(Stage-I). **(a)** 인-언어 코드2코드 검색과 비교하여, 대조적 학습은 NL2Code 및 크로스-언어 코드2코드 검색을 모두 포함하는 크로스-언어 검색을 위한 더 큰 성능 부스트로 지속적으로 이어진다. **(b)** 대비 학습은 병렬 및 랜덤 샘플링된 쌍들 사이의 대응하는 확대된 유사성 갭에 의해 표시되는 바와 같이, 보다 분산된 표현 공간으로 이어지는 동시에 NL2Code와 Code2Code 쌍들 사이의 상대적인 유사성 갭을 브리징한다.\n' +
      '\n' +
      '그림 5: 다른 훈련 방식 하에서 사전 훈련된 모델 크기를 갖는 다운스트림 작업 성능 스케일링에 관한 것이다.\n' +
      '\n' +
      '우리의 모델은 코드 검색 및 코드 분류와 관련된 작업에서 이전 모델을 훨씬 능가합니다. 또한 다양한 모델 크기에 걸쳐 향상된 코드 표현 학습에 기여하는 필수 요소를 조사한다. 우리의 작업이 소스 코드에 공개적으로 액세스할 수 있는 광범위한 코퍼스를 활용하여 코드 표현 학습에서 향후 작업에 영감을 줄 수 있기를 바랍니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Lachaux et al. (2021) Marie anne Lachaux, Baptiste Roziere, Marc Szafraniec, and Guillaume Lample. DOBF: 프로그래밍 언어에 대한 해독 사전 훈련 목표. A. 베이겔지머, Y. Dauphin, P. Liang, and J. Wortman Vaughan(eds.), _Advances in Neural Information Processing Systems_, 2021. URL[https://openreview.net/forum?id=3e29BSHNTNT](https://openreview.net/forum?id=3e29BSHNTNT).\n' +
      '* Bieber et al. (2023) David Bieber, Rishab Goel, Dan Zheng, Hugo Larochelle, and Daniel Tarlow. 외부 리소스 설명으로 프로그램을 실행하는 방법을 학습하여 런타임 오류를 정적 예측합니다. _The Eleventh International Conference on Learning Representations_, 2023. URL[https://openreview.net/forum?id=1lp-C5nTdJG](https://openreview.net/forum?id=1lp-C5nTdJG).\n' +
      '* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 코드 상에서 훈련된 대형 언어 모델들을 평가한다. _ ArXiv preprint_, abs/2107.03374, 2021. URL[https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).\n' +
      '* Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 시각적 표상의 대조적 학습을 위한 간단한 프레임워크. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1597-1607, 2020. URL[http://proceedings.mlr.press/v119/chen20j.html](http://proceedings.mlr.press/v119/chen20j.html).\n' +
      '* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ arXiv preprint arXiv:2204.02311_, 2022. URL[https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311).\n' +
      '* Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V. 르와 크리스토퍼 D. 매닝 Electra: 텍스트 인코더를 생성자가 아닌 판별자로 사전 훈련합니다. _International Conference on Learning Representations_, 2020. URL[https://openreview.net/forum?id=rlxMH1BtvB](https://openreview.net/forum?id=rlxMH1BtvB)이다.\n' +
      '* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019a. 컴퓨터 언어학과의 연관성 doi: 10.18653/v1/N19-1423. URL[https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).\n' +
      '* Devlin et al. (2019b) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, 2019b. doi: 10.18653/v1/N19-1423. URL[https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).\n' +
      '* Feng et al. (2020a) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: 프로그래밍 및 자연 언어에 대해 미리 훈련된 모델. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 1536-1547, 2020a. doi: 10.18653/v1/2020.findings-emnlp.139. URL[https://aclanthology.org/2020.findings-emnlp.139](https://aclanthology.org/2020.findings-emnlp.139).\n' +
      '* Feng et al. (2020b) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: 프로그래밍 및 자연 언어에 대해 미리 훈련된 모델. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 1536-1547, Online, November 2020b. 컴퓨터 언어학과의 연관성 doi: 10.18653/v1/2020.findings-emnlp.139. URL[https://aclanthology.org/2020.findings-emnlp.139](https://aclanthology.org/2020.findings-emnlp.139).\n' +
      '\n' +
      '* Gao et al. (2022) Jun Gao, Changlong Yu, Wei Wang, Huan Zhao, and Ruifeng Xu. Mask-then-fill: 이벤트 추출을 위한 유연하고 효과적인 데이터 증강 프레임워크. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 4537-4544, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.332. URL[https://aclanthology.org/2022.findings-emnlp.332](https://aclanthology.org/2022.findings-emnlp.332).\n' +
      '* Gao et al. (2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: 문장 임베딩의 단순 대조적 학습. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 6894-6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL[https://aclanthology.org/2021.emnlp-main.552](https://aclanthology.org/2021.emnlp-main.552)\n' +
      '* Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 그래프코드{bert}: 데이터 흐름을 갖는 사전-트레이닝 코드 표현들. _International Conference on Learning Representations_, 2021. URL[https://openreview.net/forum?id=jLoC4ez43PZ](https://openreview.net/forum?id=jLoC4ez43PZ).\n' +
      '* Guo et al. (2022) Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, 및 Jian Yin. UniXcoder: 코드 표현을 위한 Unified cross-modal pre-training. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 7212-7225, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.499. URL[https://aclanthology.org/2022.acl-long.499](https://aclanthology.org/2022.acl-long.499).\n' +
      '* Hadsell et al. (2006) Raia Hadsell, Sumit Chopra, and Yann LeCun. 불변 매핑을 학습하여 치수 감소 In _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\'06)_, volume 2, pp. 1735-1742. IEEE, 2006.\n' +
      '* Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan. CoSQA: 코드 검색 및 질문 응답을 위한 20,000개 이상의 웹 쿼리. _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 5690-5700, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.442. URL[https://aclanthology.org/2021.acl-long.442](https://aclanthology.org/2021.acl-long.442).\n' +
      '* Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet 챌린지: 시맨틱 코드 검색의 상태 평가. _ arXiv preprint arXiv:1909.09436_, 2019. URL[https://arxiv.org/abs/1909.09436](https://arxiv.org/abs/1909.09436).\n' +
      '* Jain et al. (2021) Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, and Ion Stoica. 대비 코드 표현 학습. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5954-5971, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.482. URL[https://aclanthology.org/2021.emnlp-main.482](https://aclanthology.org/2021.emnlp-main.482)\n' +
      '* Jeon et al.(2023) Mingi Jeon, Seung yeo Baik, Joonghyuk Hahn, Yo-Sub Han, and Sang-Ki Ko. 딥러닝 기반 소스코드 복잡도 예측, 2023. URL[https://openreview.net/forum?id=9irBKvxsw9](https://openreview.net/forum?id=9irBKvxsw9)\n' +
      '* Jiang et al. (2021) Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, and Lei Lyu. 트리버트: 프로그래밍 언어를 위한 트리 기반 사전 훈련된 모델. In _Uncertainty in Artificial Intelligence_, pp. 54-63. PMLR, 2021.\n' +
      '* Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 소스 코드의 문맥적 임베딩을 학습하고 평가한다. In _International conference on machine learning_, pp. 5110-5121. PMLR, 2020.\n' +
      '* Koectkov et al. (2022) Denis Koectkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. _ ARXiv 프리프린트 arXiv:2211.15533_, 2022.\n' +
      '\n' +
      '* Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source is with you! _ arXiv preprint arXiv:2305.06161_, 2023.\n' +
      '* Li et al. (2022) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. CodeRetriver: 코드 검색을 위한 대규모 대조적 사전 훈련 방법. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 2898-2910, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.187. URL[https://aclanthology.org/2022.emnlp-main.187](https://aclanthology.org/2022.emnlp-main.187)\n' +
      '* Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu 및 Shujie LIU. CodeXGLUE: 코드 이해 및 생성을 위한 머신 러닝 벤치마크 데이터세트. 제30차 신경정보처리시스템 데이터세트 및 벤치마크 추적회의(Round 1)_, 2021. URL[https://openreview.net/forum?id=61E4dQXAUcb](https://openreview.net/forum?id=61E4dQXAUcb)\n' +
      '* Neelakantan et al. (2022) Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. text and code embedding by contrastive pre-training. _ arXiv preprint arXiv:2201.10005_, 2022.\n' +
      '* Peters et al. (2019) Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. 조율을 하려고요, 안 하려고요? 다양한 작업에 미리 훈련된 표현을 적용합니다. In _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)_, pp. 7-14, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4302. URL[https://aclanthology.org/W19-4302](https://aclanthology.org/W19-4302).\n' +
      '* Song et al. (2016) Hyun O Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. 들어올린 구조화된 피쳐 임베딩을 통한 딥 메트릭 학습입니다. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pp. 4004-4012, 2016. doi: 10.1109/CVPR.2016.434. URL[https://doi.org/10.1109/CVPR.2016.434](https://doi.org/10.1109/CVPR.2016.434)\n' +
      '* Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 드롭아웃: 신경망이 과적합하는 것을 방지하는 간단한 방법. _ The journal of machine learning research_, 15(1):1929-1958, 2014.\n' +
      '* Wang et al. (2022) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 약 감독 대비 사전 훈련에 의한 텍스트 임베딩. _ ArXiv:2212.03533_, 2022.\n' +
      '* Wang et al. (2021a) Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, 및 Xin Jiang. Syncobert: 코드 표현을 위한 구문 유도 다중 모드 대비 사전 훈련. _ arXiv preprint arXiv:2108.04556_, 2021a.\n' +
      '* Wang et al. (2021b) Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: 코드 이해 및 생성을 위한 식별자 인식 통합 사전 훈련된 인코더-디코더 모델. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 8696-8708, 2021b. doi: 10.18653/v1/2021.emnlp-main.685. URL[https://aclanthology.org/2021.emnlp-main.685](https://aclanthology.org/2021.emnlp-main.685)\n' +
      '* Zhang et al. (2021) Dejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. 아놀드, 빙샹 쌍으로 지도된 문장 표현 대비 학습. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5786-5798, 2021. doi: 10.18653/v1/2021.emnlp-main.467. URL[https://aclanthology.org/2021.emnlp-main.467](https://aclanthology.org/2021.emnlp-main.467)\n' +
      '* Zhou et al. (2022) Zhihan Zhou, Dejiao Zhang, Wei Xiao, Nicholas Dingwall, Xiaofei Ma, Andrew Arnold, and Bing Xiang. 연속된 발화로부터 대화 표상을 배우는 것. In _Proceedings of the 2022 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies_, pp. 754-768, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.55. URL[https://aclanthology.org/2022.naacl-main.55](https://aclanthology.org/2022.naacl-main.55)\n' +
      '\n' +
      '## 부록 A 데이터, 모델 및 하이퍼-파라미터 상세\n' +
      '\n' +
      '### Pretraining Data\n' +
      '\n' +
      'MLM과 DOBF 모두에 대해 Masked Language Modeling (MLM)과 Identifier Deobsfucation (DOBF)은 Stack dataset (Kocetkov et al., 2022)을 사용한다. 우리는 연결과 함께 최대 시퀀스 길이를 1024로 설정하고 주의를 차단했다.\n' +
      '\n' +
      '대조학습(Contrastive Learning, CL)에서는 bimodal 데이터, 즉 _(text, function)_로 표기된_code와 자연어 쌍에 초점을 맞춘다. 텍스트는 함수의 docstring으로부터 첫 번째 문장으로 추출된다(Husain et al., 2019). 더 나은 해석을 위해 함수의 높은 수준의 의미를 요약하는 경우가 많기 때문에 이 섹션에서 "요약"과 같은 텍스트를 참조한다. 우리는 다음 관행을 기반으로 요약을 필터링하거나 수정합니다.\n' +
      '\n' +
      '1. 영어가 아니면 요약을 필터링한다.\n' +
      '2. 요약의 토큰 수가 \\(<\\)3 또는 \\(>\\)256인 경우 요약 필터링\n' +
      '3. 요약에서 URL, HTML 태그 및 교리를 제거합니다.\n' +
      '4. 요약에 불량 유니코드 텍스트를 수정합니다.\n' +
      '5. 기능 본문에 한 줄 이하의 코드로 기능을 필터링한다.\n' +
      '\n' +
      '우리는 그림 1과 표 4의 각 단계에서 사전 훈련 데이터의 통계를 요약한다.\n' +
      '\n' +
      '### 모델 및 하이퍼 매개 변수 훈련\n' +
      '\n' +
      '우리는 CodeSage-small, CodeSage-base 및 CodeSage-large라고 하는 세 가지 크기의 모델 아키텍처를 사전 훈련한다. 우리는 표 5의 모델 하이퍼-파라미터를 요약한다.\n' +
      '\n' +
      '### 토큰 분배 및 단계-I 사전 훈련 목표\n' +
      '\n' +
      '사전 연구에서는 자연어(NL) 토큰과 프로그래밍 언어(PL) 토큰의 비율을 조사하는 데이터 분석을 수행한다. 소스 코드에서 토큰은 크게 (1) 식별자, (2) 키워드, (3) 연산자, (4) 구분자, (5) 리터럴의 5개 그룹으로 분류된다. 우리는 _String 리터럴(즉,_ docstring, comments)을 NL 토큰으로 태그하는 반면, 다른 모든 토큰은 PL 토큰으로 간주된다. 우리는 소스 코드를 파싱하고 코드 토큰의 다섯 가지 범주를 추출하기 위해 _tree-sitter_를 사용한다. 그리고 Starcoder tokenizer (Li et al., 2023)를 이용하여 토큰화한다. Stack-Python corpora로부터 스타코더 토큰화된 토큰을 사용하여 다음과 같은 통계를 계산한다.\n' +
      '\n' +
      '1. _Approximate PL Tokens_: 토큰의 57.8%가 {identifier, keywords, delimiters, operators}에 속한다. 이 중 식별자에 속하는 토큰은 53.8%, 기타 토큰은 46.2%이다.\n' +
      '2. _Approximate NL Tokens_: 42.2%의 토큰 리터럴{Boolean, Numeric, String} 이 중 92.9%의 토큰이 문자열 리터럴에 속하고 7.1%의 토큰이 타인에 속한다.\n' +
      '\n' +
      '위의 숫자에서 알 수 있듯이, 대략적인 NL 토큰은 특정 프로그래밍 언어에 대한 전체 토큰의 약 40%를 차지한다. 따라서, 마스킹된 것을 교체하는 경우\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline Language & Total files & \\#Functions & \\#Func. w/ docstring & \\#Func. w/ summary \\\\ \\hline Python & 24,214,270 & 67,264,716 & 24,321,126 & 18,146,327 \\\\ Java & 42,429,211 & 84,828,833 & 17,613,636 & 13,118,303 \\\\ Javascript & 40,112,121 & 35,469,803 & 7,450,153 & 4,796,101 \\\\ C\\# & 21,702,269 & 37,284,300 & 9,325,665 & 7,350,191 \\\\ C & 21,383,832 & 16,253,435 & 4,392,973 & 2,958,699 \\\\ Ruby & 7,205,146 & 5,475,047 & 1,217,764 & 1,049,356 \\\\ GO & 11,653,185 & 31,067,259 & 11,247,051 & 9,739,861 \\\\ PHP & 34,851,418 & 42,373,766 & 22,191,329 & 13,416,574 \\\\ Typescript & 19,589,267 & 16,612,988 & 2,637,245 & 1,863,436 \\\\ \\hline Total & 237,961,548 & 367,905,026 & 105,760,862 & 75,389,347 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: Masked Language Modeling (MLM) 및 Identifier Deobsfucation (DOBF)을 통한 사전 훈련에 사용된 데이터의 통계, 그리고 대비 학습 (CL)이 뒤따른다. 데이터는 The Stack(Kocetkov et al., 2022)으로부터 수집된다.\n' +
      '\n' +
      '임의의 토큰을 갖는 kens는 PL 토큰을 NL 토큰으로 대체하는 결과를 초래할 수 있고, 그 반대의 경우도 마찬가지이다. 그러나, 많은 시나리오들에서 PL 토큰들, 예를 들어,_그러한 식별자 관련 토큰들은 종종 코드 스니펫들이 인간에 의해 해석가능하도록 명확한 의미론들을 운반할 것으로 예상되기 때문에, PL과 NL 토큰들 사이에 명확한 경계들이 종종 존재하지 않는다. 따라서 80-10-10 규칙에 따라 마스킹된 입력 토큰이 주어지면 모델이 어떤 토큰이 손상으로부터 발생하는지를 결정하는 것은 자명하지 않은 작업이 될 수 있다. 이는 PL의 구조 특성과 함께 랜덤 토큰이 코드의 의미론과 구조를 크게 방해하고 표현 학습이 효과적이기 어렵게 만드는 것을 가능하게 한다.\n' +
      '\n' +
      '예시를 위해 그림 6(오른쪽)의 예를 들어, 함수 이름 "binary_search"는 나타나는 세 곳 모두에서 랜덤 토큰으로 손상되고 있으며, 이는 코드의 의미적 의미를 변경할 가능성이 있다. 비록 모델이 손상된 코드로부터 "binary_search"를 올바르게 복구할 수 있을 것으로 기대할 수 있지만, (1) 코드의 신택스가 다른 랜덤 토큰 "getConfig"에 의해 파괴되었다; (2) \\(<\\)MASK \\(>\\) 토큰의 존재; 및 (3) 양방향 자기 주목 메커니즘이 모델을 구동하여 마스킹된 토큰들의 예측을 형성하기 위해 그 랜덤 토큰들을 레버리지할 수 있다.\n' +
      '\n' +
      '### Masking Rate\n' +
      '\n' +
      '80-10-10 손상 전략이 없는 "Full Mask", _i.e.,_ MLM을 사용하여 그림 7에서 최적의 마스킹 비율을 조사한다. 우리는 세 가지 일정한 마스킹 비율인 7.5%, 15% 및 30%를 고려한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & CodeSage-small & CodeSage-base & CodeSage-large \\\\ \\hline \\#layers & 6 & 24 & 24 \\\\ \\#heads & 8 & 8 & 16 \\\\ Model dim & 1024 & 1024 & 2048 \\\\ Vocab size & 49,152 & 49,152 & 49,152 \\\\ Max sequence length & 1024 & 1024 & 1024 \\\\ Total parameters & 130M & 356M & 1.3B \\\\ \\hline Stage1: Masked Language Modeling & & & \\\\ \\hline Dropout & 0.1 & 0.1 & 0.1 \\\\ Max steps & 250,000 & 250,000 & 250,000 \\\\ Warmup steps & 5000 & 5000 & 5000 \\\\ Batch size & 2048 & 2048 & 2048 \\\\ Base learning rate & 3e-4 & 3e-4 & 3e-4 \\\\ \\hline Stage2: Contrastive Learning & & & \\\\ \\hline Dropout & 0.1 & 0.1 & 0.1 \\\\ Max steps & 20,000 & 20,000 & 20,000 \\\\ Warmup steps & 500 & 500 & 500 \\\\ Batch size & 8192 & 8192 & 8192 \\\\ Bae learning rate & 5e-06 & 5e-06 & 5e-06 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 모델 아키텍처 및 사전 트레이닝 관련 하이퍼-파라미터.\n' +
      '\n' +
      '그림 6: 코드 스니펫(왼쪽) 및 그에 대응하는 마스킹된 버전이 15% 마스킹 레이트(오른쪽)로 80-10-10 연습을 사용하여 생성되었다.\n' +
      '\n' +
      '각각의 트레이닝 예에 대한 범위 [10%, 50%]로부터 마스킹 레이트가 랜덤하게 선택되는 것을 특징으로 하는 동적 마스킹 전략. 우리는 우리가 조사한 네 가지 변형 중에서 \\(15\\%\\)이 최적의 마스킹율로 남아 있음을 발견한다.\n' +
      '\n' +
      '### Identifier Obfuscation\n' +
      '\n' +
      '본 연구에서는 양방향 인코더 표현 모델을 학습하기 위해 식별자 난독화(DOBF) 목표를 사용한다. 이 접근법에 대한 우리의 영감은 Anne Lachaux et al.(2021)에 의해 도입된 DOBF 방법에서 나온 것이지만, 우리의 채택 전략은 그들의 것과 다르다. 그들의 작업에서, anne Lachaux et al.(2021)은 클래스, 함수 및 변수 이름이 특수 토큰으로 대체되는 난독화된 버전으로부터 원본 코드를 재구성하기 위해 시퀀스-투-시퀀스 언어 모델을 훈련시켰다. 대조적으로, 우리의 접근법은 이 기술을 인코더 전용 모델에 적용한다. 이러한 적응은 코드 토큰화(_i.e._, _tree-sitter_를 사용하는) 및 모델-특정 토큰화(_i.e._, _sentencepiece_ tokenizer를 사용하는)의 차이들로 인해 마스크 토큰들과 식별자 토큰들(마스크될 것이고 인코더들이 그들을 예측할 것) 사이의 1-1 매핑을 확립하기 위한 자명하지 않은 노력을 수반한다. 설명하기 위해 토큰화 프로세스를 고려해보자. 트리시터는 "_def function_name_(.)"를 \\(\\{def,function\\_name,(),:\\}\\)으로 토큰화하는 반면, 모델별 토큰화기는 \\(\\{def,function\\_,\\_name(),:\\}\\)으로 토큰화할 수 있다. 결과적으로, 우리는 식별자 토큰 "name"의 일부인 " token"을 건너뛰어 \\(\\{[mask],[mask],[mask]\\}\\rightarrow\\{function,\\_,name\\}\\)의 마스킹된 토큰에서 예측 토큰으로의 매핑을 구성해야 하는 문제에 직면한다. 난독화를 수행하고 _mask map_을 구성하기 위해 난독화 도구를 개발하였다.\n' +
      '\n' +
      'OBF ToolWe는 전체 소스 코드 파일 또는 함수를 입력으로 하여 토큰 맵과 함께 식별자가 난독화된 코드를 출력하는 툴을 개발하였다. 본 논문에서는 그림 8의 예를 제시한다. 코드 스니펫을 파싱하고 모든 식별자와 그 조상 노드 유형을 추출하기 위해 _tree-sitter_를 사용하였다. 노드 유형을 기반으로 클래스 이름, 함수 이름, 함수 인수 및 함수 호출을 식별합니다. 그런 다음 클래스 이름, 함수 이름 및 변수 이름에 대해 각각 특수 토큰(\\(c_{i},f_{i},v_{i}\\)으로 대체한다. 그런 다음 특수 토큰을 모델 토큰화기(_i.e._, Starcoder tokenizer)에 포함하고 특수 토큰이 출력 토큰에 유지되도록 난독화된 코드를 토큰화한다. 마지막으로 모델 토큰화기를 사용하여 식별자를 개별적으로 토큰화하고 특수 토큰(\\(c_{i},f_{i},v_{i}\\))을 식별자 토큰으로 대체한다.\n' +
      '\n' +
      '어휘중첩과 강경한 긍정설계에 관한 연구\n' +
      '\n' +
      '부록 A.1에 자세히 나와 있듯이, 우리는 함수 문서 문자열에서 첫 번째 문장을 요약 텍스트로 추출한다. 그런 다음 문서 문자열(요약)과 함수 서명 간의 어휘 중복과 문서 문자열(요약)과 함수 본문 간의 어휘 중복을 조사한다. 스택 파이썬 말뭉치에서\n' +
      '\n' +
      '1. 함수 서명에서 토큰의 22.3%, 함수 본문에서 토큰의 23.1%가 docstring 토큰과 겹친다.\n' +
      '2. 함수 서명에서 토큰의 12.3%, 함수 본문에서 토큰의 11.6%가 요약 토큰과 겹친다.\n' +
      '\n' +
      '그림 7: CodeSagebase에서 조사된 Maksing rate와 zero-shot Code2Code 검색 성능. 3가지 일정한 마스킹 비율(7.5%, 15%, 30%)과 마스킹 비율이 각 훈련 예제에 대해 [10%, 50%] 범위에서 무작위로 선택된 동적 마스킹 전략을 고려한다.\n' +
      '\n' +
      '이것은 함수의 문서 문자열 또는 요약이 종종 함수 서명과 큰 어휘 중복을 갖는다는 우리의 직관을 검증한다. 따라서, 전체 기능과 문서 문자열 또는 요약을 대조할 때 모델은 이러한 중첩을 활용하여 바로 가기를 학습하는 경향이 있으므로 표현에서 의미 동등성 개념을 포착하지 못한다. 결과적으로, 좋지 않은 일반화가 이루어진다.\n' +
      '\n' +
      '## 부록 B 다운스트림 태스크 평가\n' +
      '\n' +
      '### Baseline Models\n' +
      '\n' +
      '표 6에서 기준 모델 크기와 출력 표현 차원을 요약한다.\n' +
      '\n' +
      '### Code Search\n' +
      '\n' +
      '우리는 표 7의 NL2Code 및 Code2Code 벤치마크의 데이터 통계를 요약하며, 아래에서는 각 데이터 세트에 더 많은 컨텍스트를 제공한다.\n' +
      '\n' +
      '**Code2Code** 검색은 코드 프래그먼트가 부여된 관련 코드 프래그먼트를 _query_로 검색하는 작업이다. 본 논문에서는 CodeNet에서 생성된 코드2코드 검색 데이터셋(Guo et al., 2022)을 C, C#, Javascript, Typescript, GO, PHP의 6개 언어로 확장한다. 원래 데이터 세트에는 Java, Python 및 Ruby의 각 문제에 대해 2~10개의 솔루션이 포함되어 있습니다. 먼저 코드넷에서 문제 식별자와 6개 언어로 된 해를 모은다. 또한, 코드넷은 클러스터 내의 솔루션들이 서로 거의 중복되어 있는 각각의 솔루션에 대한 클러스터 식별자를 제공한다. 하나를 모으죠\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Model & Model Size & Embedding & Max Sequence & Training \\\\  & & Dimension & Length & Data Source \\\\ \\hline CodeBERT & 125M & 768 & 512 & CodeSearchNet \\\\ GraphCodeBERT & 125M & 768 & 512 & CodeSearchNet \\\\ StarEncoder & 125M & 768 & 1024 & The Stack \\\\ UnixCoder & 125M & 768 & 1024 & CodeSearchNet \\\\ OpenAI-Embedding-Ada-002 & Unknown & 1536 & 8191 & Unknown \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 임베딩의 모델 크기 및 치수. GitHub Code 데이터셋은 ([https://huggingface.co/datasets/codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code])에서 사용할 수 있다.\n' +
      '\n' +
      '그림 8: 우리가 개발한 난독화 도구에 의해 생성된 파이썬 코드(왼쪽) 및 해당 난독화 버전(오른쪽)의 예. 클래스 이름, 함수 이름 및 변수는 특수 토큰으로 대체됩니다. 제안된 OBF 도구는 왼쪽에 있는 코드를 고려하여 난독화된 코드와 식별자 맵( \\(\\{c_{0},v_{0},v_{1},v_{2},v_{3},v_{4},v_{5},f_{0},f_{1}\\} 우각부 노드, 자기,v, 데이터, 왼쪽, 오른쪽, 노드,\\_init\\_,printPostorder\\})을 생성한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '종단 간 분류 작업을 수행하고 표 12에 결과를 제시한다. 이러한 결과를 통해 CodeSage가 기준 모델보다 우수하다는 것을 알 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c c c c c c c c} \\hline \\hline  & CoSQA & AdvTest & \\multicolumn{6}{c}{CSN} \\\\ \\cline{2-10} Model & Python & Python & Python & Java & JS & PhP & Go & Ruby \\\\ \\hline CodeGen2.5 (7B) & 0.02 & 0.01 & 0.06 & 0.02 & 0.05 & 0.18 & 6.03 & 2.04 \\\\ Starcoder (15.5B) & 0.02 & 0.06 & 0.03 & 0.01 & 0.05 & 0.59 & 0.06 & 0.05 \\\\ CodeT5+ (16B) Encoder & 22.96 & 20.36 & 19.93 & 14.05 & 12.26 & 26.08 & 20.37 & 13.05 \\\\ CodeBERT & 0.24 & 0.06 & 0.05 & 0.03 & 0.04 & 0.02 & 0.14 & 0.34 \\\\ GraphCodeBERT & 16.20 & 5.58 & 10.37 & 8.59 & 7.29 & 8.07 & 12.47 & 20.79 \\\\ StarEncoder & 10.78 & 0.93 & 2.81 & 2.51 & 1.87 & 0.74 & 2.65 & 5.54 \\\\ UnixCoder & 42.11 & 27.32 & 42.17 & 43.92 & 40.46 & 35.21 & 61.39 & 55.52 \\\\ Openai-Embedding-Ada-002 & 44.23 & 38.08 & 68.02 & **71.49** & 67.50 & 60.62 & **85.63** & **74.20** \\\\ \\hline CodeSage-small & ************99.2** & 41.28 & 64.38 & 63.19 & 60.01 & 54.71 & 77.66 & 63.20 \\\\ CodeSage-base & 48.50 & 49.08 & 67.99 & 68.02 & 66.95 & 58.15 & 83.21 & 68.00 \\\\ CodeSage-large & 47.53 & **52.67** & **70.77** & 70.21 & **69.50** & **61.33** & 83.71 & 71.92 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 제로샷 코드 검색 태스크의 MAP 스코어(%). 맨 위 행에 언급된 언어 이름은 언어 쿼리와 후보자가 작성되었음을 나타냅니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c c c c c c c c c} \\hline \\hline Model & Python & Java & JS & TS & C\\# & C & Ruby & PHP & GO & Avg \\\\ \\hline CodeGen2.5(7B) & 16.5 & 10.2 & 7 & 8.5 & 4.2 & 8.0 & 17.3 & 15.6 & 9.4 & 10.7 \\\\ Starcode1(5.5B) & 7.1 & 3.9 & 3.2 & 4.4 & 1.7 & 2.4 & 6.8 & 6.1 & 3.3 & 4.3 \\\\ CodeT5+(16B) Encoder & 18.2 & 9.9 & 5.8 & 6.9 & 4.2 & 8.2 & 16.5 & 13.9 & 8.0 & 10.2 \\\\ CodeBERT & 14.40 & 7.62 & 5.47 & 6.05 & 3.66 & 5.53 & 13.55 & 10.28 & 6.27 & 8.09 \\\\ GraphCodeBERT & 19.23 & 10.78 & 7.38 & 8.65 & 5.54 & 8.48 & 19.69 & 15.67 & 9.65 & 11.68 \\\\ StarEncoder & 19.17 & 11.65 & 9.0 & 10.52 & 5.69 & 9.72 & 21.57 & 16.98 & 10.81 & 12.79 \\\\ UnixCoder & 30.77 & 16.45 & 21.32 & 21.95 & 6.19 & 15.62 & 32.33 & 31.93 & 13.94 & 21.17 \\\\ OpenAI-Embedding-Ada-002 & 35.91 & 25.13 & 19.01 & 21.86 & 10.17 & 29.15 & 40.85 & 40.47 & 23.43 & 27.33 \\\\ \\hline CodeSage-small & 36.31 & 23.97 & 26.60 & 29.90 & 11.84 & 22.84 & 29.06 & 34.64 & 19.56 & 26.08 \\\\ CodeSage-base & **47.52** & 22.84 & 28.70 & 31.95 & 13.37 & 30.99 & 44.86 & 51.13 & 25.15 & 32.95 \\\\ CodeSage-large & 46.70 & **33.13** & **37.16** & **41.18** & **16.81** & **32.89** & **54.12** & **52.13** & **32.48** & **38.51** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 제로 샷 설정에서의 NL2Code 검색의 MRR 스코어(%).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{r c c c c c c c} \\hline \\hline Target Class & Train \\# & Valid \\# & Test \\# & Target Class & Train \\# & Valid \\# & Test \\# \\\\ \\hline No error & 1,20,503 & 13,049 & 13,745 & ImportError & 259 & 37 & 22 \\\\ ZeroDivisionError & 25,087 & 3,087 & 2,828 & TabError & 74 & 4 & 3 \\\\ OSEror & 21540 & 2,427 & 2,422 & re.error & 62 & 6 & 11 \\\\ UnboundLocalError & 21,414 & 2,641 & 2,603 & AttributeError & 47 & 4 & 8 \\\\ decimal & 10,026 & 509 & 1,674 & StopIteration & 24 & 5 & 3 \\\\ ValueError & 8,585 & 991 & 833 & OverflowError & 19 & 2 & 2 \\\\ AssertionError & 7,816 & 1,072 & 691 & Timeout & 18 & 8 & 2 \\\\ FileNotFoundError & 7,676 & 727 & 797 & IndexError & 10 & 0 & 12 \\\\ IndentationError & 7,645 & 285 & 841 & ModuleNotFoundError & 8 & 7 & 1 \\\\ KeyError & 7,505 & 965 & 733 & RecursionError & 5 & 0 & 0 \\\\ NameError & 1,876 & 186 & 110 & EOFError & 3 & 0 & 0 \\\\ numpy.AxisError & 437 & 47 & 125 & SyntaxError & 3 & 0 & 1 \\\\ MathDomainError & 362 & 39 & 22 & RuntimeError & 2 & 0 & 1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 파이썬 런타임 에러 데이터세트 내의 타겟 클래스들의 분포.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>