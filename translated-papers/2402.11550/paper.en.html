<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration\n' +
      '\n' +
      'Jun Zhao\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Can Zu\\({}^{1}\\)1\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Hao Xu\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Yi Lu\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Wei He\\({}^{1}\\)\n' +
      '\n' +
      'Yiwen Ding\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Tao Gui\\({}^{2}\\)\n' +
      '\n' +
      'Corresponding authors.\n' +
      '\n' +
      'Qi Zhang\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Xuanjing Huang\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)School of Computer Science, Fudan University\n' +
      '\n' +
      '\\({}^{2}\\)Institute of Modern Languages and Linguistics, Fudan University\n' +
      '\n' +
      '{zhaoj19,qz,tgui}@fudan.edu.cn,czu22@m.fudan.edu.cn\n' +
      '\n' +
      'Footnote 1: footnotemark:\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over \\(100k\\) tokens, a phenomenon also known as _lost in the middle_. In this paper, we propose LongAgent, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In LongAgent, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members\' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an _intermember communication_ mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that LongAgent offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Benefiting from increased model scales and massive pre-training corpus, large language models (LLMs) like GPT-4 (OpenAI, 2023) and LLaMA (Touvron et al., 2023) have continuously improved their capabilities. However, due to the inherent quadratic complexity of attention mechanisms, LLMs are often pre-trained with a limited context window to avoid unbearable computational costs. Once the input text length exceeds this limit, LLMs suffer from severe performance degradation (Xiao et al., 2023; Peng et al., 2023). This significantly constrains the effectiveness of LLMs in many practical applications, such as querying information from books, analyzing legal documents, or scientific papers.\n' +
      '\n' +
      'Recently, many efforts have been dedicated to addressing the challenges of extending the context window of pre-trained LLMs. The first category of methods considers positional encoding as a crucial aspect to tackle this issue (Press et al., 2022; Chen et al., 2023; Peng et al., 2023; Chen et al., 2023). By extrapolation or interpolation, these methods enable positional encoding to adapt to positions "unseen" during the pre-training stage. However, this adaptation process may impair the performance on short sequences acquired during pre-training (Jin et al., 2024). Additionally, as the window size increases, positional biases may decrease the effectiveness of attention mechanisms\n' +
      '\n' +
      'Figure 1: LongAgent collaboration scheme. The input long text (left) is segmented into several chunks and assigned to corresponding members. The Leader receives user instruction (right), breaks them down into the simplest sub-problems, convenes members for discussion, ultimately obtaining answers to all sub-problems, and reasons to make the final response.\n' +
      '\n' +
      '(Liu et al., 2023), a phenomenon referred to as _lost in the middle_. The second category of methods employs intricate mechanisms such as designing recurrent structures (Zhou et al., 2023; Zhang et al., 2024), token selection (Mothashami and Jaggi, 2023; Tworkowski et al., 2023), or sliding windows (Xiao et al., 2023; Han et al., 2023), enabling limited context windows to handle longer input texts. While these methods offer advantages in computational efficiency, valuable long-term dependencies may gradually be overlooked with multiple recurrent iterations or window sliding.\n' +
      '\n' +
      'In this work, we introduce a promising novel method, termed LongAgent, to address the challenge of handling long texts. LongAgent achieves processing of documents exceeding \\(100k\\) through multi-agent collaboration. As illustrated in Figure 1, our agent team consists of a leader and multiple members. The leader is responsible for: 1) understanding user intent and organizing discussions among members; 2) supervising communication among members to resolve conflicting opinions; 3) gathering relevant information and reasoning answers. Each member is tasked with responding to the leader\'s instructions based on the content in the assigned text chunk. Due to members\' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. We address this issue through an inter-member communication mechanism. The leader detects members with conflicting opinions during discussions and allows them to interact directly to eliminate hallucinatory responses. In order to comprehensively evaluate LLMs\' long-text capabilities, we further extend _Needle in a Haystack_, a recently popular pressure test long-context LLMs. We change the simple fact retrieval to more challenging question-answering that may involve multiple documents. The entities related to answers in the documents have been modified to prevent models from taking shortcuts by relying on internal knowledge. We named the new test _Needle in a Haystack PLUS_.\n' +
      '\n' +
      'The main contributions of our work are as follows: 1) we propose LongAgent, scaling LLMs with \\(4k\\) context size to effectively handle long texts exceeding \\(100k\\) tokens; 2) we construct a larger benchmark, _Needle in the Haystack PLUS_, enabling more comprehensive evaluation on LLMs\' long-text capabilities. 3) experimental results show that LongAgent, built upon LLaMA-7B, exhibits potential surpassing GPT-4 in long text processing. This improvement strongly suggests that multi-agent collaboration is a promising direction for improving long-text processing.\n' +
      '\n' +
      '## 2 LongAgent for Long Text Processing\n' +
      '\n' +
      '### Method Overview\n' +
      '\n' +
      'As illustrated in Figure 2, we use long-text question answering as an example to elucidate the working mechanism of LongAgent. Given a long text \\(x\\) and a user query \\(q\\), LongAgent searches for relevant evidence from the long text \\(x\\) and reasons for the final response \\(r\\) through collaborative efforts of multiple agents. This process involves the following 4 steps:\n' +
      '\n' +
      '(1) **Selecting Members**. LongAgent, as a task-agnostic method for long-text processing, supports constructing customized agent teams based on the task to be processed. For example, given the task description \\(d=\\)"_Answer the question based on the given passages_", the leader selects the QA expert model to instantiate team members for more accurate and reliable responses. Then, we partition the long text \\(x\\) into \\(n\\) chunks \\(\\{c_{1},c_{2},...,c_{n}\\}\\) of predefined size and distribute them accordingly to \\(n\\) members \\(\\{m_{1},m_{2},...,m_{n}\\}\\) for processing.\n' +
      '\n' +
      '(2) **Collaborative Reasoning**. For a complex user query \\(q\\), the leader systematically breaks \\(q\\) down into multiple sub-questions and organizes members to collaborate in searching for clues from their respective chunks. As shown in fig 2, to answer \\(q=\\)"_Which team does the player named 2015 Diamond Head Classic\'s MVP play for?_", the leader first identifies who won the MVP of the 2015 Diamond Head Classic, and then further identifies which team this player play for. For more complex queries, collaborative reasoning will involve multiple rounds.\n' +
      '\n' +
      '(3) **Resolving Conflict**. Due to model hallucinations, some members may respond with false answers not mentioned in the document. Obviously, these false answers conflict with the correct one. The leader needs to identify such conflicts and ascertain the correct answer explicitly mentioned in the document.\n' +
      '\n' +
      '(4) **Deducing the Answer**. When the leader believes that the current discussion history is sufficient to derive the answer to the user query, it ends the discussion and provide the final response.\n' +
      '\n' +
      'In the following sections, we will elaborate on the details of the aforementioned four steps.\n' +
      '\n' +
      '### Selecting Experts to Instantiate Members\n' +
      '\n' +
      'The working mechanism of LongAgent involves coordinating members and integrating their processing results of respective chunks to form the final response. Different long-text tasks require members to possess diverse text processing capabilities. To address this challenge, we utilize expert models to construct task-specific agent teams, aiming to generate more accurate responses.\n' +
      '\n' +
      '**Construction of expert models**: This step aims to build a candidate set of expert models \\(E=\\{e_{1},e_{2},...,e_{s}\\}\\), where different experts excel in different tasks. For strong models (e.g., GPT-4, GPT-3.5), we employ a prompt-based approach to construct expert models. Specific prompt templates are used to elicit the internal capabilities of the model for the corresponding tasks. For example, "_You are an expert in answering questions, adept at searching for relevant information from given documents and providing answers._" The benefit of this approach is that all expert models can share the same parameters. For weak models (e.g., LLaMA-7b), we utilize a fine-tuning-based approach to inject task knowledge to make them experts in the corresponding tasks. The advantage of this method is that it can produce more accurate responses for specific tasks.\n' +
      '\n' +
      '**Member selection**: Given a natural language description \\(d\\) of a task to be processed, we prompt the leader to select a suitable expert \\(e_{i}\\in E\\) to play roles as team members. We assume that the task at hand requires only one particular expert to complete. For complex tasks that require collaboration among experts with different skill sets, we leave it as future work. The prompt template is illustrated in tab. 2. All members in the team share the parameters of the expert model \\(e_{i}\\).\n' +
      '\n' +
      '### Collaborative Reasoning\n' +
      '\n' +
      'To respond to user query \\(q\\), the leader needs to coordinate members to process text and acquire relevant information. As the core of the team, the leader continuously executes the following decision-making process:\n' +
      '\n' +
      '\\[a\\sim\\text{Leader}(a|S,q), \\tag{1}\\]\n' +
      '\n' +
      'where \\(q\\) denotes the user query, \\(S=\\{s_{1},s_{2},...,s_{m}\\}\\) represents the historical dialogue states, and \\(s_{i}\\) represents a round of dialogue composed of an instruction from the leader and responses from all members. The leader sample an action \\(a\\in\\{\\texttt{NEW\\_STATE},\\texttt{CONFLIICT},\\texttt{ANSWER}\\}\\) based on the dialogue history \\(S\\) and the user query \\(q\\). If \\(a=\\texttt{NEW\\_STATE}\\), it it indicates that the information contained in the preceding \\(i\\) rounds of dialogue history is insufficient to respond to query \\(q\\). Therefore, the leader initiates the next round of dialogue \\(s_{i+1}\\), and generates new instructions to further gather information. Conversely, if \\(a=\\texttt{ANSWER}\\), it signifies that the leader deems the currently collected information sufficient to derive the final answer, and the collaborative process terminates accordingly. CONFLIICT is a special state other than the two\n' +
      '\n' +
      'Figure 2: An Overview of the LongAgent. In step 1, the leader constructs a customized agent team based on the description of the task to be handled. In the second and third steps, the leader organizes the team to gather information from documents and resolve conflicts. This process may continue for multiple rounds until the leader deems enough information has been gathered to generate the final response, which is then exported in the step 4.\n' +
      '\n' +
      'aforementioned states, indicating that the leader perceives conflicting answers from the members in the current round of dialogue \\(s_{i}\\). We elaborate on this situation in the next subsection.\n' +
      '\n' +
      '### Resolving Conflicts\n' +
      '\n' +
      'Due to model hallucinations, members may respond with content not mentioned in their chunks. The dialogue in Step 2 of Figure 2 serves as an example, where two members respectively believe _Buddy Hield_ and _Mark Gibson_ to be the MVP of the 2015 Diamond Head Classic, despite the latter not being mentioned in the text chunk. We address this issue through _inter-member communication_, inspired by the following empirical findings: 1) When there is answer to the leader\'s instruction in the chunk, the member often provides correct responses rather than generating hallucinations; 2) When there are no answers in the chunk, the model frequently fabricates an answer instead of responding with \'no mention,\' even after supervised fine-tuning. Using this feature, the leader first identifies the member IDs where answers conflict and then requests these members to share chunks pairwise and provide answers again:\n' +
      '\n' +
      '\\[\\text{hallucination}=m_{i}(c_{i}), \\tag{2}\\] \\[\\text{Truth}=m_{j}(c_{j}),\\] (3) \\[\\text{Truth}=m_{j}(c_{j}\\oplus c_{i}) \\tag{4}\\]\n' +
      '\n' +
      'Here, \\(c_{i}\\) and \\(c_{j}\\) respectively represent two text chunks, where \\(c_{j}\\) contains the correct answer while \\(c_{i}\\) does not. \\(m_{i}\\) and \\(m_{j}\\) denote two members. Our experimental results demonstrate that sharing text chunks is a simple yet effective strategy. The majority of members experiencing hallucination tend to correct their original responses upon receiving the chunk containing the correct answers, resulting in accurate output. While we acknowledge some advanced mechanisms for mitigating hallucination issues, such as multi-agent debate Du et al. (2023) and reflection Shinn et al. (2023), these are not the focus of this paper; we leave them as avenues for future research.\n' +
      '\n' +
      '## 3 Experimental Setup\n' +
      '\n' +
      '### Evaluation Protocol\n' +
      '\n' +
      '**Needle-in-a-Haystacke PLUS**: The _Needle-in-a-Haystacke_Kamr addt2023 is currently one of the most popular testbed for evaluating the capability to handle long texts. In this setup, a fact or statement of interest (the _needle_) is placed within a lengthy distracting document (the _haystack_), and the model is tasked with retrieving this hidden key information. Performance is evaluated by varying the position of the needle within the distracting document and the length of the distracting document itself. To assess the long-text capabilities more comprehensively, we propose _Needle-in-a-Haystacke PLUS_, which shifts the focus from simple fact retrieval to more challenging single-document/multi-document question answering tasks. In _Needle-in-a-Haystacke PLUS_, the _needle_ represents the document(s) containing the answers, while the _haystacke_ comprises distracting documents. The model must locate one or more relevant documents scattered within the haystacke and reason the correct answers from them. For the purpose of evaluation, we deliberately select questions with definite answers as test data, such as questions where the answer is a specific entity or a simple yes/no response. To mitigate the risk of models relying on internal knowledge to answer, we replace entities directly related to the answer within the documents with fictional entities. In Appendix A, we elaborate on the collecting process of the test data, including single-document QA and multi-hop QA involving multiple documents.\n' +
      '\n' +
      '**Synthetic Tasks**: In addition to the _Needle-in-a-Haystacke PLUS_ test, we also selected two widely used long sequence evaluation tasks Mohtashami and Jaggi (2023); Liu et al. (2023); Zhang et al. (2023): long-text retrieval and numerical comparison. We choose them for the following reasons: (1) Similar to the needle-in-a-haystacke task, these synthetic tasks all use ACC as the evaluation metric, facilitating evaluation. The difference in metrics can directly reflect the difference in the model\'s long sequence processing capabilities. (2) We can automatically synthesize training data for fine-tuning open-source models. The long-text retrieval task includes the following three subtasks: 1) PassKey Retrieval: Retrieving hidden keys in a noisy long context; 2) Number Retrieval: Locating repeated hidden numbers in a noisy long context. 3) KV Retrieval: Finding the corresponding value from a dictionary and a key. Numerical comparison requires the model to find numbers that meet specific requirements from a numeric string of magnitude \\(100k\\) tokens, such as the top \\(K\\) numbers, median, etc., where \\(K\\) can be \\(1\\), \\(2\\), or \\(3\\).\n' +
      '\n' +
      '### Compared Methods\n' +
      '\n' +
      '**PI**[3]. Extending the context window sizes of RoPE-based pretrained large language models by position interpolation.\n' +
      '\n' +
      '**YARN**[14]. YaRN is an improved method to efficiently extend the context window. This work directly modifies the PE to expand to a theoretically infinite context length.\n' +
      '\n' +
      '**Claude2.1**[1]. The Claude 2.1 released by Anthropic Corporation features a context window of 200K tokens and has significantly reductions in rates of model hallucination.\n' +
      '\n' +
      '**GPT-4 Turbo**[1]. The GPT-4 Turbo model from OpenAI offers a context window of 128K and can process text exceeding 300 pages within a single prompt.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'To build an agent team, we perform supervised fine-tuning on LLaMA2-7b-base. Within the agent team, the Leader is responsible for coordinating Members to accomplish various tasks. We utilize GPT-4 to generate \\(1,000\\) interaction trajectories for each task to train the Leader, and manually verified the correctness of these interaction trajectories. Members are tasked with processing documents based on the Leader\'s instructions. To achieve this, we train QA experts, retrieval experts, and mathematical experts for instantiating members. Regardless of the number of members instantiated, they all share the parameters of a single expert model. Training data for QA experts are sourced from the SQuAD training set, consisting of \\(25,000\\) samples. Among these, \\(10,000\\) samples contain answers within the documents, while the remaining \\(15,000\\) samples do not, requiring the model to abstain from answering. We extended document lengths to \\(2500\\)-\\(3000\\) tokens through concatenation. Training data for retrieval experts and mathematical experts are synthesized automatically, with \\(10,000\\) documents generated for each task, ranging in length from \\(1k\\) to \\(3k\\) tokens, and information to be retrieved evenly placed at random positions within the documents. It\'s important to note that all training data is non-overlapping with the final evaluation data. Please refer to Appendix B for prompts and interaction trajectories for all tasks.\n' +
      '\n' +
      '## 4 Results and Discussion\n' +
      '\n' +
      '### Overall Performance\n' +
      '\n' +
      'To demonstrate the superiority of LongAgent in handling long texts, we compare it against powerful commercial models GPT-4 Turbo and Claude 2.1, as well as the state-of-the-art academic methods for long-text processing, PI and YARN.\n' +
      '\n' +
      '**Through multi-agent collaboration, fine-tuning LLaMA with only a 4k context window effectively handles contexts of up to 128k.**\n' +
      '\n' +
      'The results for the _Needle-in-a-Haystack PLUS_ are shown in Figure 3 and 4, respectively. LongAgent, constructed from fine-tuned LLaMA2-7B, significantly outperforms GPT-4 across document length ranging from 1k to 128k, with an average improvement of 19.53% (from 62.00% to 81.53%) under the single-document setting, and an average improvement of 4.96% (from 50.37% to 55.33%)\n' +
      '\n' +
      'Figure 3: The Comparison of Results of _Needle-in-a-Haystack PLUS_ in Single-Document Question Answering Setting. Under the LangAgent scheme, our fine-tuned LLaMA2-7B model achieved an average accuracy improvement of \\(19.53\\%\\) compared to GPT-4 across the range from 1k to 128k (increasing from \\(62.00\\%\\) to \\(81.53\\%\\)).\n' +
      '\n' +
      'under the multi-document setting. Considering that LongAgent is fine-tuned on downstream tasks, for fair comparison, we fine-tune PI and YARN on task data with lengths ranging from \\(1\\) to \\(16k\\) (training with longer contexts exceeds our hardware limitations). Experimental results demonstrate that when the length of the test document exceeds the maximum length trained on, PI and YARN fail to generate results properly. Even within the \\(0\\)-\\(16k\\) range (corresponding to the first two columns of the grid), the average performance of LongAgent surpasses that of PI and YARN. The results on the four synthetic tasks are shown in Table 1. From the table, we can observe that LongAgent supported by fine-tuned LLaMA2-7B model outperforms all baseline models, achieving or approaching \\(100\\%\\) accuracy on the three retrieval-type tasks. This demonstrates the superiority of LongAgent in handling various long-text tasks.\n' +
      '\n' +
      '**For LongAgent supported by more powerful models like GPT-3.5, fine-tuning is not necessary.**\n' +
      '\n' +
      'Through prompting, GPT-3.5 can simultaneously act as a leader and members with specific skills. Despite having only a \\(16k\\) context window, we found that the LongAgent supported by GPT-3.5 can effectively handle documents far exceeding \\(16k\\) in length. Specifically, in the _needle-in-a-haystack PLUS_ task, LongAgent achieved improvements of \\(6.780\\%\\) and \\(1.5\\%\\) over GPT-4 in single-doc and multi-doc settings, respectively. For the four synthetic tasks in Table 1, LongAgent also achieved perfect scores in two retrieval tasks with \\(100k\\) length documents. For KV retrieval and numerical comparison tasks, it also outperformed the majority of baselines.\n' +
      '\n' +
      '**Although we only tested inputs ranging from 1k to 128k, LongAgent demonstrates potential in handling inputs exceeding 128k in length.**\n' +
      '\n' +
      'In Figure 3 and 4, we observed the _lost in the middle_ phenomenon with Claude 2.1. Specifically, as the input length increases, Claude2.1\'s average accuracy gradually decreases. However, in the first and last rows of the Claude subfigure in Figure 3, and the third row of the Claude subfigure in Figure 4, relatively high accuracy is consistently maintained. This suggests that Claude2.1 can effectively model the beginning and end of long texts but fails to utilize key information in the middle effectively. LongAgent avoids direct processing of long texts through chunking. Regardless of the input length, the chunk size remains constant, thus avoiding the \'lost in the middle\' phenomenon. Although longer inputs\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c}{Retrieval} & \\multicolumn{1}{c}{Numerical} \\\\ \\cline{2-5}  & PassKey & Number & KV & Comparison \\\\ \\hline GPT-4 & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(0.890\\) & \\(0.600\\) \\\\ Kimi-Chat & \\(0.981\\) & \\(0.954\\) & \\(0.536\\) & \\(0.126\\) \\\\ Claude2.1 & \\(0.978\\) & \\(0.981\\) & \\(0.654\\) & \\(0.323\\) \\\\ YaRN & \\(0.927\\) & \\(0.566\\) & \\(-\\) & \\(0.171\\) \\\\ Ours-GPT3.5 & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(0.638\\) & \\(0.511\\) \\\\ Ours-LLAMA-7B & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(\\mathbf{0.966}\\) & \\(\\mathbf{0.625}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: The experimental results (accuracy) on four synthesis tasks.\n' +
      '\n' +
      'Figure 4: The Comparison of Results of _Needle-in-a-Haystack PLUS_ in Multi-Document Question Answering Setting. Under the LangAgent scheme, our fine-tuned LLaMA2-7B model achieved an average accuracy improvement of \\(4.96\\%\\) compared to GPT-4 across the range from \\(1k\\) to \\(128k\\) (increasing from \\(50.37\\%\\) to \\(55.33\\%\\)).\n' +
      '\n' +
      'may complicate agent interactions, experimental results show no significant performance decrease for LongAgent. Overall, LongAgent has the potential to handle inputs exceeding \\(128k\\) in length.\n' +
      '\n' +
      '### Hallucination Analysis\n' +
      '\n' +
      'We found that the errors of LongAgent are mainly due to a type of hallucination problem: when the chunk of a member does not contain information related to the instruction of the Leader, the member sometimes answers based on internal knowledge or fabricates a wrong answer as a response. In this subsection, we explore the impact of two key factors, the recipe of training data and chunk size, on model hallucination. As shown in Figure 5, with the increase of \'Reject\' type data in the training data, the proportion of the model correctly refusing to answer increased from \\(51.0\\%\\) to \\(78.6\\%\\). However, the increase of \'Reject\' data also slightly impairs the model\'s ability to answer questions. As shown in Figure 8, when the ratio of \'Answer:Reject\' increases from 2:1 to 2:3, the accuracy of the model decreases from \\(83.3\\%\\) to \\(78.3\\%\\), and there are also a small number of cases where the document contains the answer but refuses to answer.\n' +
      '\n' +
      'In addition to the data proportion, chunk size is also an important factor affecting model hallucination. As shown in Figure 5, when the chunk size increases from \\(500\\) to \\(2,000\\), the hallucination problem is alleviated. This is mainly because the length of our training data is about \\(3,000\\) tokens, and increasing the chunk size reduces the gap with the length of the training data. However, when the chunk size exceeds \\(2,000\\), further increasing the chunk size significantly exacerbates model hallucination. We speculate that this is because when the sequence length is too long, the model\'s inadequacy in document modeling becomes more prominent. Therefore, we believe that while researching how to construct larger context windows, we should not neglect the modeling of text within a \\(4k\\) window.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'In Section 2.4, we address conflicts between members through _inter-member communication_. To demonstrate the effectiveness of this mechanism, we calculate the difference in model accuracy before and after introducing this mechanism. As shown in Figure 6, the _inter-member communication_ mechanism leads to an average accuracy improvement of \\(18.9\\%\\) across a range of input text lengths from \\(1k\\) to \\(128k\\). Furthermore, the number of members increases with the length of the text, and the number of members experiencing hallucinations also grows. In this context, the improvement in accuracy brought about by conflict\n' +
      '\n' +
      'Figure 5: The influence of data recipe on model hallucinations. ‘Answer’ and ‘Reject’ represent two types of data. For the former, the documents contain answers to questions; whereas for the latter, they do not.\n' +
      '\n' +
      'Figure 6: Improved accuracy through _inter-member communication_ mechanism.\n' +
      '\n' +
      'resolution becomes even more evident.\n' +
      '\n' +
      '### Efficiency Advantage\n' +
      '\n' +
      'Thanks to chunking of long texts, LongAgent\'s time complexity for processing long texts is \\(\\mathcal{O}(N)\\). In this subsection, we empirically verify this point. As shown in Figure 7, the latency of LongAgent within the range of \\(1k\\)-\\(100k\\) almost grows linearly with length. For Full Attention, which has quadratic complexity, the inference latency increases rapidly regardless of the use of techniques such as flash attention. The latency of Full Attention when processing 10k tokens has already exceeded that of LongAgent processing \\(50k\\) tokens. Furthermore, without specific memory optimization techniques, a single A100 GPU with 80G memory can only support text inference up to 11k in length, and even with flash attention, this number can only be increased to \\(15k\\). Under the same settings, LongAgent can process contexts of around \\(100k\\) with less than 40G of memory.\n' +
      '\n' +
      '## 5 Related Works\n' +
      '\n' +
      '### Long-text Modeling\n' +
      '\n' +
      'Several methods have been proposed to extend the positional encoding (PE) for handling longer sequences. Initially, approaches like RoPE and PI [3] attempted to interpolate position indices within pre-trained limits, but neglected frequency variations. Recent advancements include "NTK-aware" [13] interpolation and "Dynamic NTK" [13] interpolation, which address high-frequency component losses. Additionally, "NTK-by-parts" [13] interpolation outperforms others when fine-tuned on longer-context data. Another popular approach for managing longer sequences involves constraining global causal attention to local attention. ReRoPE [23] truncates context lengths during pretraining and LM-Infinite [14] restricts attention to a chevron-shaped window. mohtashami2023neural insert landmark tokens after text fragments, while zhang2024neural propose beacon tokens for summarizing fragments. In contrast, our method effectively circumvents the risk of losing valuable contextual information while utilizing only a small amount (hundreds of agent interaction tracks) for fine-tuning, thereby reducing training costs.\n' +
      '\n' +
      '### LLM-Based Multi-Agent Systems\n' +
      '\n' +
      'In recent years, LLM-based multi-agent systems have garnered widespread attention in academia. Numerous efforts have been dedicated to leveraging cooperation among individuals to enhance the efficiency and accomplish more complex reasoning tasks [15, 16, 17, 18, 19]. To enable agents to effectively address a variety of dynamic tasks in the real world, researchers have also integrated external tools into the agents\' decision-making processes [13, 14, 15], enabling them to perform accurate computations and retrieve the latest information from databases or search engines. In these approaches, the most relevant ones to ours are PEARL [22] and MemWalker [3]. PEARL enhances the model\'s focus on relevant content within long texts by calling self-generated pseudo APIs. However, it can only handle long texts within the agent\'s context window and is ineffective for longer texts. Although MemWalker enables agents to process longer texts through a tree-based summarization approach, crucial information may be lost after multiple summarizations, causing the agent to get lost in irrelevant contexts.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      'This paper proposes LongAgent, a novel long-text processing approach based on multi-agent collaboration. LongAgent scaling LLMs with \\(4k\\) context size to effectively handle long texts exceeding \\(100k\\) tokens. The proposed _inter-member communication_ mechanism alleviates the member hallucination when they reading documents, thus facilitating effective management by the leader of dozens to hundreds of members. We have also\n' +
      '\n' +
      'Figure 7: LongAgent scheme exhibits significantly superior time and memory efficiency compared to directly perform full attention on long texts.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      'Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation.\n' +
      '* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n' +
      '* Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\n' +
      '* Su (2023) Jianlin Su. 2023. Rectified rotary position embeddings. [https://github.com/bojone/rerope](https://github.com/bojone/rerope).\n' +
      '* Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, and Xavier Martinet. 2023. Llama: Open and efficient foundation language models.\n' +
      '* Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. 2023. Focused transformer: Contrastive training for context scaling.\n' +
      '* Wang et al. (2024) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration.\n' +
      '* Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\n' +
      '* Zhang et al. (2024) Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. 2024. Soaring from 4k to 400k: Extending llm\'s context with activation beacon.\n' +
      '* Zhang et al. (2023) Xinrong Zhang, Yingfa Chen, Shengding Hu, Qihao Wu, Junhao Chen, Zihang Xu, Zhenning Dai, Xu Han, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2023. Infinitebench: 128k long-context benchmark for language models.\n' +
      '* Zhou et al. (2023) Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt: Interactive generation of (arbitrarily) long text.\n' +
      '\n' +
      '## Appendix A Collecting Needle-in-a-Haystack PLUS Testing Data\n' +
      '\n' +
      'The test data consists of two parts: single-document QA and multi-document QA. Below, we will elaborate on the construction process of each.\n' +
      '\n' +
      '### Single-document QA\n' +
      '\n' +
      'For single-document QA, the test data is constructed based on SQuAD Rajpurkar et al. (2016). SQuAD is a large-scale machine reading comprehension dataset containing one hundred thousand questions, selected from over 500 Wikipedia documents. Each question in the dataset has its answer as a text segment extracted from the given document. We randomly select \\(100\\) samples from the training set of SQuAD and replace the key entities in them. These \\(100\\) samples\' documents are used as _needles_, while documents from other samples in the training set are randomly selected as _haystacks_, which provide distractor information. The length of _haystack_ ranges from \\(1,000\\) to \\(128,000\\) tokens with equal intervals, totaling 15 different lengths. The depth of the needle ranges from \\(0\\%\\) to \\(100\\%\\) with equal intervals, totaling \\(10\\) different depths. Depth indicates the position of the needle in the _haystack_. A depth of \\(0\\%\\) means the _needle_ is at the beginning of the haystack, while \\(100\\%\\) indicates it\'s at the end. For each length and depth, we randomly select \\(10\\)_needles_ to construct \\(10\\) test samples.\n' +
      '\n' +
      '### Multi-document QA\n' +
      '\n' +
      'For multi-document QA, questions require information from two or more documents to reason the final answer. We construct test samples based on HotpotQA Yang et al. (2018), a widely adopted multi-document QA dataset. We select \\(60\\) questions from the validation set of HotpotQA that require information from two documents to answer. Each sample contains two _needles_, and the haystack is still composed of distractor documents. The length of _haystack_ ranges from \\(1,000\\) to \\(128,000\\) tokens with equal intervals, totaling \\(15\\) different lengths. The two _needles_ are randomly scattered at the depth of \\(0\\%\\), \\(33\\%\\), \\(66\\%\\), and \\(100\\%\\) of the haystack, resulting in \\(6\\) combinations: \\((0\\%,33\\%)\\), \\((0\\%,66\\%)\\), \\((0\\%,100\\%)\\), \\((33\\%,66\\%)\\), \\((33\\%,100\\%)\\), and \\((66\\%,100\\%)\\). For specific lengths and needle positions, we randomly select \\(10\\)_needles_ to construct \\(10\\) test samples for evaluation.\n' +
      '\n' +
      'Trajectory for Each Task\n' +
      '\n' +
      '### Single-document Question Answering\n' +
      '\n' +
      'Dataset: Squad\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      'First, the leader recruits members according to the task description as shown in Table 2.\n' +
      '\n' +
      'You need to recruit a team of members to solve a task. Select the appropriate member based on the task description.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '{task_description}\n' +
      '\n' +
      '**# Members List:**\n' +
      '\n' +
      'QA member: Good at solving Question Answering problems.\n' +
      '\n' +
      'KV member: Good at finding the corresponding value from a dictionary.\n' +
      '\n' +
      'NS member: Good at locating repeated hidden numbers in a noisy long context.\n' +
      '\n' +
      'PassKey member: Good at retrieving hidden keys in a noisy long context.\n' +
      '\n' +
      'Math member: Good at finding special integers in a lengthy list.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "member", "content": "your_chosen_member"}}\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_["type": "member", "content": "QA member"]_\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      'After recruiting members, the leader gives specific instruction as shown in Table 3.\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      'The prompt for the Member first input is shown in Tabel 4. The content of #Document is a part of the complete context, with variations for each member. The content of #Instruction originates from the first output of the leader.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      'The leader will exclude members who refuse to answer and group together members who provide the same answer.\n' +
      '\n' +
      '_Clusters: [\'the Wall Street Journal\': [1, 7, 8, 13, 15, 17, 18, 22, 23, 24, 28,,29 31, 32, 33, 34, 35, 37, 39, 42, 43], \'the Yale Law Journal\': [5], \'The Wall Street Journal\': [20]]_\n' +
      '\n' +
      '**Member next input**\n' +
      '\n' +
      'Prompt template for Member next input is the same as Member first input shown in Table 4. The content of #Document is the merge of the document from two members.\n' +
      '\n' +
      'Select member 43 randomly from Cluster 1 and member 5 randomly from Cluster 2. First, merge the documents of member 43 and member 5 to obtain the new result.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt}} \\hline \\hline You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is: \\\\\n' +
      '1. Only you know the task description and task objective; the other members do not. \\\\\n' +
      '2. But they will receive different documents that may contain answers, and you need to send them an instruction to query their document. \\\\\n' +
      '3. Your instruction need to include your understanding of the task and what you need them to focus on. If necessary, your instructions can explicitly include the task objective. \\\\\n' +
      '4. Finally, you need to complete the task based on the query results they return. \\\\\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. The answer must be extracted from the given passages. \\\\\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      'In which publication did Sander publish an article questioning racial preferences in law schools? \\\\\n' +
      '**\\# Generate Instruction for Members:**\n' +
      '\n' +
      'Now, you need to generate an instruction for all team members. You can ask them to answer a certain question, or to extract information related to the task, based on their respective documents.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 2: Prompt template for Leader first input on the Single-document Question Answering Task. The content of #Task Description is derived from user input.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt}} \\hline \\hline You output must following the JSON format: {{"type": "internet": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Prompt template for Leader next input on the Single-document Question Answering Task. The content of #Task Description and #Task Objective is derived from user input.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '**Leader second input**\n' +
      '\n' +
      'For multi-document Question Answering, the questions require answers that integrate the content of multiple documents. The details are shown in Table 6.\n' +
      '\n' +
      '**Leader second output**\n' +
      '\n' +
      'The leader needs to break down the original question, so the leader\'s second output is the first sub-question.\n' +
      '\n' +
      '_"type": "instruction", "content": "What was the population of Leshan in 2002?"_\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      'The prompt for the Member first input is shown in Table 7. The content of #Document is a part of the complete context, with variations for each member. The content of #Instruction originates from the first output of the leader.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      '_Member 1: 121,277 Member 2: The document does not contain the answer._\n' +
      '\n' +
      '_Member 3: The document does not contain the answer._\n' +
      '\n' +
      '_Member 30: The document does not contain the answer._\n' +
      '\n' +
      '_Member 31: The document does not contain the answer._\n' +
      '\n' +
      '**Leader third input**\n' +
      '\n' +
      'The prompt for the Leader third input is shown in Table 8. At this moment, Leader still retains the dialogue history from the first round.\n' +
      '\n' +
      '**Leader second output**\n' +
      '\n' +
      '_("type": "instruction", "content": "What was the population of Quanzhou in 2002?")_\n' +
      '\n' +
      '**Member second input**\n' +
      '\n' +
      'The prompt for the Member second input is shown in Table 9. The content of #Document is a part of the complete context, with variations for each member. The content of #Instruction originates from the second output of the leader.\n' +
      '\n' +
      '**Member second output**\n' +
      '\n' +
      'Exclude the members who refuse to respond, and cluster the members who provide the same answers.\n' +
      '\n' +
      '_Clusters: [\'1.5 million\': [7], \'658,798\': [11]]_\n' +
      '\n' +
      '**Member third input**\n' +
      '\n' +
      'The prompt for the Member third input is shown in Table 10. The content of #Document is the merge of the document from Member 7 and Member 11.\n' +
      '\n' +
      '**Member third output**\n' +
      '\n' +
      '_["type": "response", "content": "658,798"]_\n' +
      '\n' +
      'At this point, there is only one cluster left, thus all members have only one unique result.\n' +
      '\n' +
      '**Leader last input**\n' +
      '\n' +
      'The prompt for the Leader last input is shown in Table 11. At this moment, Leader still retains the dialogue history of the previous two rounds.\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      'Leader integrates the answers to the two sub-problems to obtain the final answer to the original problem.\n' +
      '\n' +
      '_["type": "answer", "content": "Quanzhou"]_\n' +
      '\n' +
      '### Retrieve\n' +
      '\n' +
      'The Retrieve-type tasks are divided into three types: Retrieve.KV, Retrieve.PassKey, and Retrieve.Number. Although the task descriptions vary for different tasks, the interaction trajectories are similar, and the prompt template is also the same. Therefore, Retrieve.KV task is chosen here as an example for demonstration.\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      'First, the leader recruits members according to the task description as shown in Table 2.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_["type": "member", "content": "KV member"]_\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      'The prompt for the Leader next input is shown in Table 12.\n' +
      '\n' +
      'The content of #Task Description and #Task Objective is derived from user input.\n' +
      '\n' +
      '**Leader next output**\n' +
      '\n' +
      '_[ "type": "instruction", "content": "Finding the corresponding value from a dictionary and a key. Key: "2b114db0-d87e-42d2-9b4c-0b1f115976ad" The value associated with the specified key is: " ]_\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      'The prompt for the Member first input is shown in Table 13.\n' +
      '\n' +
      'The content of #Document is a part of the complete context, with variations for each member.\n' +
      '\n' +
      'The content of #Instruction originates from the first output of the leader.\n' +
      '\n' +
      'You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is:\n' +
      '\n' +
      '1. Only you know the task description and task objective; the other members do not.\n' +
      '\n' +
      '2. But they will receive different documents that may contain answers, and you need to send them an instruction to query their document.\n' +
      '\n' +
      '3. Your instruction need to include your understanding of the task and what you need them to focus on. If necessary, your instructions can explicitly include the task objective.\n' +
      '\n' +
      '4. Finally, you need to complete the task based on the query results they return.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. Only give me the answer and do not output any other words.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'Did Leshan or Quanzhou have a population of 658,798 in 2002?\n' +
      '\n' +
      '**# Generate Instruction for Members:**\n' +
      '\n' +
      'Now, you need to generate an instruction for all team members. You can ask them to answer a certain question, or to extract information related to the task, based on their respective documents.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      'Except for the members who refuse to answer, member 1, member 9, and member 12 provide three different responses.\n' +
      '\n' +
      '_Clusters:_\n' +
      '\n' +
      '_[\'flec3dd8-b6e7-4af5-a589-fda0b7693f93\': [1], \'d3999999-9999-4999-8999-99999999999\': [9], \'d9999999-9999-8999-999999999999\': [12]]_\n' +
      '\n' +
      '**Member next input**\n' +
      '\n' +
      'The prompt template is the same as the prompt for \'Member next input\', while the content of #Document is the merge of the document from two members.\n' +
      '\n' +
      'First, merge the documents of member 1 and member 9 to obtain the new result.\n' +
      '\n' +
      '_Member 1 and Member 9 merge: flec3dd8-b6e7-4af5-a589-fda0b7693f93_\n' +
      '\n' +
      'Since the merged output is identical to the original output of member 1 but differs from the original output of member 9, member 9 has been removed. Next, merge the documents of member 1 and member 12 to obtain the new result.\n' +
      '\n' +
      '_Member 1 and Member 12 merge: flec3dd8-b6e7-4af5-a589-fda0b7693f93_\n' +
      '\n' +
      'Remove member 12 because the merged output is identical to the original output of member 1 and differs from the original output of member 12. At this point, all members have only one unique result. The leader will provide the final answer based on this result.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is:\n' +
      '\n' +
      '1. Only you know the task description and task objective; the other members do not.\n' +
      '\n' +
      '2. But they will receive different documents that may contain answers, and you need to send them an instruction to query their document.\n' +
      '\n' +
      '3. Your instruction need to include your understanding of the task and what you need them to focus on. If necessary, your instructions can explicitly include the task objective.\n' +
      '\n' +
      '4. Finally, you need to complete the task based on the query results they return.\n' +
      '\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. Only give me the answer and do not output any other words.\n' +
      '\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      'Did Leshan or Quanzhou have a population of 658,798 in 2002?\n' +
      '\n' +
      '**\\# Generate Instruction for Members:**\n' +
      '\n' +
      'Now, you need to generate an instruction for all team members. You can ask them to answer a certain question, or to extract information related to the task, based on their respective documents.\n' +
      '\n' +
      'Your output must following the JSON format: {"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 6: Prompt template for Leader second input on the Multi-document Question Answering Task. The content of #Task Description and #Task Objective is derived from user input.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline\n' +
      '**\\# Document:**\n' +
      '\n' +
      '{member_document} \\\\\n' +
      '**\\# Instruction:**\n' +
      '\n' +
      'What was the population of Leshan in 2002?\n' +
      '\n' +
      'You are an experienced writer; please summarize the content in the document related to the instructions in a <scratchpad> tag, then describe your response." Your output must following the JSON format: {["type": "response", "content": "your_response_content"}] \\\\ The "content" needs to be as concise as possible. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Prompt template for Member first input on the Multi-document Question Answering Task.\n' +
      '\n' +
      'Here are the responses from all the members. Each member sees different segments of a document, and these segments do not intersect with each other. The correct answer may appear in any one or several members\' responses.\n' +
      '\n' +
      'Note that if a minority of members find information relevant to the question while the majority reply that the document does not contain information relevant to the question, you should pay attention to the replies from those members who found relevant information.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      'Member 1: 121,277\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. Only give me the answer and do not output any other words.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'Did Leshan or Quanzhou have a population of 658,798 in 2002?\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      'Based on the above information, you need to determine if you can solve the task objective. You have two choices:\n' +
      '\n' +
      '1. If members\' responses cannot solve the task objective, or if their responses contain conflicting answers, provide a new instruction for them to answer again.\n' +
      '\n' +
      '2. Else, if the task objective can be solved, give your final answer as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "answer", "content": "your_answer_content"}} or {{"type": "instruction", "content": "your_instruction_content"}}}\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      '_"type": "answer", "content": "f1ec3dd8-b6e7-4af5-a589-fda0b7693/93"_\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline\n' +
      '**\\# Document:** \\\\ \\{member\\_document\\} \\\\ \\hline\n' +
      '**\\# Instruction:** \\\\ What was the population of Leshan in 2002? \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Prompt template for Leader third input on the Multi-document Question Answering Task.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline\n' +
      '**\\# Document:** \\\\ \\{member\\_document\\} \\\\ \\hline\n' +
      '**\\# Instruction:** \\\\ What was the population of Leshan in 2002? \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Prompt template for Member second input on the Multi-document Question Answering Task.\n' +
      '\n' +
      'Here are the responses from all the members. Each member sees different segments of a document, and these segments do not intersect with each other. The correct answer may appear in any one or several members\' responses.\n' +
      '\n' +
      'Note that if a minority of members find information relevant to the question while the majority reply that the document does not contain information relevant to the question, you should pay attention to the replies from those members who found relevant information.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      'Member 1: 658,798\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. Only give me the answer and do not output any other words.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'Did Leshan or Quanzhou have a population of 658,798 in 2002?\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      'Based on the above information, you need to determine if you can solve the task objective. You have two choices:\n' +
      '\n' +
      '1. If members\' responses cannot solve the task objective, or if their responses contain conflicting answers, provide a new instruction for them to answer again.\n' +
      '\n' +
      '2. Else, if the task objective can be solved, give your final answer as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "answer", "content": "your_answer_content"}} or {{"type": "instruction", "content": "your_instruction_content"}}}\n' +
      '\n' +
      '### Math.Find\n' +
      '\n' +
      '#### b.4.1 Find the largest number\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      'First, the leader recruits members according to the task description as shown in Table 2.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "member", "content": "Math member")_\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      'The prompt for the Leader next input is shown in Tabel 15.\n' +
      '\n' +
      'The content of #Task Description and #Task Objective is derived from user input.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "instruction", "content": "You should answer with only one number, no other words. The largest number of the list is: ")_\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      'The prompt for the Member first input is shown in Tabel 16.\n' +
      '\n' +
      'The content of #Document is a part of the complete context, with variations for each member.\n' +
      '\n' +
      'The content of #Instruction originates from the first output of the leader.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      'Each member returns the maximum value of the numbers in their documents. The results of each member are recorded and passed to the leader.\n' +
      '\n' +
      '**Leader last input**\n' +
      '\n' +
      'The prompt for the Leader last input is shown in Tabel 17.\n' +
      '\n' +
      'Leader finds the maximum value for the entire document based on the outputs of all members.\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      '_("type": "answer", "content": "94")_\n' +
      '\n' +
      '#### b.4.2 Find the second largest number\n' +
      '\n' +
      'For other tasks in Math.Find, the prompt template remains the same; it will be omitted from here on.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "instruction", "content": "You should answer with only one number, no other words. The largest number and second-largest number of the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline Here are the responses from all the members. Each member sees different segments of a document, and these segments do not intersect with each other. The correct answer may appear in any one or several members’ responses. \\\\ Note that if a minority of members find information relevant to the question while the majority reply that the document does not contain information relevant to the question, you should pay attention to the replies from those members who found relevant information. \\\\\n' +
      '**\\# Member Response:**\n' +
      '\n' +
      'Member 1: 658,798 \\\\\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      'Answer the question based on the given passages. Only give me the answer and do not output any other words. \\\\\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      'Did Leshan or Quanzhou have a population of 658,798 in 2002? \\\\\n' +
      '**\\# Determination:**\n' +
      '\n' +
      'Based on the above information, you need to determine if you can solve the task objective. You have two choices:\n' +
      '\n' +
      '1. If members’ responses cannot solve the task objective, or if their responses contain conflicting answers, provide a new instruction for them to answer again.\n' +
      '\n' +
      '2. Else, if the task objective can be solved, give your final answer as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "answer", "content": "your_answer_content"}} or {{"type": "instruction", "content": "your_instruction_content"}}}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 11: Prompt template for Leader last input on the Multi-document Question Answering Task.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      'Here are the responses from all the members. Each member sees different segments of a document, and these segments do not intersect with each other. The correct answer may appear in any one or several members\' responses.\n' +
      '\n' +
      'Note that if a minority of members find information relevant to the question while the majority reply that the document does not contain information relevant to the question, you should pay attention to the replies from those members who found relevant information.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      'Member 1: flec3dd8-b6e7-4af5-a589-fda0b7693f93\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      'Finding the corresponding value from a dictionary and a key.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'Key: "2b114db0-d87e-42d2-9b4c-0b1f115976ad"\n' +
      '\n' +
      'The value associated with the specified key is:\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      'Based on the above information, you need to determine if you can solve the task objective. You have two choices:\n' +
      '\n' +
      '1. If members\' responses cannot solve the task objective, or if their responses contain conflicting answers, provide a new instruction for them to answer again.\n' +
      '\n' +
      '2. Else, if the task objective can be solved, give your final answer as concisely as you can, using a single phrase if possible. Do not provide any explanation.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "answer", "content": "your_answer_content"}} or {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '## Appendix C Additional Results\n' +
      '\n' +
      'This section complements the experimental results mentioned in the main body of this paper.\n' +
      '\n' +
      'You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is:\n' +
      '\n' +
      '1. Only you know the task description and task objective; the other members do not.\n' +
      '\n' +
      '2. But they will receive different documents that may contain answers, and you need to send them an instruction to query their document.\n' +
      '\n' +
      '3. Your instruction need to include your understanding of the task and what you need them to focus on. If necessary, your instructions can explicitly include the task objective.\n' +
      '\n' +
      '4. Finally, you need to complete the task based on the query results they return.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      'Finding special integers in a lengthy list.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'You should answer with only one number, no other words. The largest number of the list is:\n' +
      '\n' +
      '**# Generate Instruction for Members:**\n' +
      '\n' +
      'Now, you need to generate an instruction for all team members. You can ask them to answer a certain question, or to extract information related to the task, based on their respective documents.\n' +
      '\n' +
      'Your output must following the JSON format: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>