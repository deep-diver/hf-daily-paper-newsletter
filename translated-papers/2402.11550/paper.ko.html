<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LongAgent: Multi-Agent 협업을 통한 128k 컨텍스트로의 언어 모델 스케일링\n' +
      '\n' +
      'Jun Zhao\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Can Zu\\({}^{1}\\)1\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Hao Xu\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Yi Lu\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Wei He\\({}^{1}\\)\n' +
      '\n' +
      'Yiwen Ding\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Tao Gui\\({}^{2}\\)\n' +
      '\n' +
      'Corresponding authors.\n' +
      '\n' +
      'Qi Zhang\\({}^{1}\\)\n' +
      '\n' +
      'Equal Contributions.\n' +
      '\n' +
      'Xuanjing Huang\\({}^{1}\\)\n' +
      '\n' +
      '푸단대학교 컴퓨터과학부\n' +
      '\n' +
      '푸단대학교 현대언어언어연구소\n' +
      '\n' +
      '{zhaoj19,qz,tgui}@fudan.edu.cn,czu22@m.fudan.edu.cn\n' +
      '\n' +
      '각주 1: 각주:\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 언어를 이해하고 복잡한 추론 작업을 실행하는 데 인상적인 성능을 보여주었다. 그러나, 긴 컨텍스트 윈도우를 갖는 LLM은 값비싼 트레이닝 비용과 높은 추론 지연으로 악명이 높다. GPT-4 및 Claude2와 같은 가장 진보된 모델조차도 \\(100k\\) 토큰의 입력을 처리할 때 종종 실수를 하는데, 이는 중간_에서 _lost라고도 하는 현상이다. 본 논문에서는 LLMs(예를 들어, LLaMA)를 128K 컨텍스트로 확장하고, GPT-4에 비해 긴 텍스트 처리에서 잠재적인 우월성을 입증하는 멀티 에이전트 협업 기반 방법인 LongAgent를 제안한다. LongAgent에서 리더는 사용자의 의도를 이해하고 팀원들이 문서로부터 정보를 획득하도록 지시하는 역할을 한다. 멤버들의 환각으로 인해 리더가 수십 명에서 수백 명의 멤버들의 반응으로부터 정확한 정보를 얻는 것은 사소한 일이 아니다. 이를 해결하기 위해 정보 공유를 통해 환각에 의한 대응 갈등을 해결하는 _intermember communication_ 메커니즘을 개발한다. 실험 결과는 LongAgent가 긴 텍스트 처리를 위한 유망한 대안을 제공한다는 것을 나타낸다. LLaMA-7B로 인스턴스화된 에이전트 팀은 GPT-4에 비해 128k 길이의 텍스트 검색, 다중 홉 질의 응답과 같은 작업에서 상당한 개선을 달성한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '증가된 모델 척도와 대규모 사전 훈련 코퍼스로 인해 GPT-4(OpenAI, 2023) 및 LLaMA(Touvron et al., 2023)와 같은 대규모 언어 모델(LLM)이 지속적으로 능력을 향상시켰다. 그러나 주의 메커니즘의 고유한 2차 복잡성으로 인해 LLM은 견딜 수 없는 계산 비용을 피하기 위해 제한된 컨텍스트 창으로 사전 훈련되는 경우가 많다. 일단 입력 텍스트 길이가 이 제한을 초과하면, LLM들은 심각한 성능 저하를 겪는다(Xiao et al., 2023; Peng et al., 2023). 이것은 책에서 정보를 조회하거나 법률 문서를 분석하거나 과학 논문을 분석하는 것과 같은 많은 실제 응용 프로그램에서 LLM의 효과를 크게 제한한다.\n' +
      '\n' +
      '최근에는 사전 훈련된 LLM의 컨텍스트 창을 확장하는 문제를 해결하기 위해 많은 노력을 기울이고 있다. 방법의 첫 번째 범주는 위치 인코딩을 이 문제를 해결하기 위한 중요한 측면으로 간주한다(Press et al., 2022; Chen et al., 2023; Peng et al., 2023; Chen et al., 2023). 외삽 또는 보간법에 의해, 이러한 방법들은 위치 인코딩이 사전-트레이닝 스테이지 동안 "보이지 않는" 위치들에 적응하는 것을 가능하게 한다. 그러나, 이러한 적응 과정은 사전 훈련 동안 획득된 짧은 시퀀스들에 대한 성능을 손상시킬 수 있다(Jin et al., 2024). 또한, 윈도우 크기가 증가함에 따라 위치 편향이 주의 메커니즘의 효과를 감소시킬 수 있다\n' +
      '\n' +
      '도 1: LongAgent 협업 방식. 입력된 긴 텍스트(왼쪽)는 여러 청크로 분할되어 해당 멤버에 할당된다. 리더는 사용자 지시를 받고(오른쪽), 가장 간단한 하위 문제로 분해하고, 토론을 위해 구성원을 소집하며, 궁극적으로 모든 하위 문제에 대한 답변을 얻고, 최종 응답을 할 이유를 얻는다.\n' +
      '\n' +
      '(Liu et al., 2023), middle_에서 _lost로 지칭되는 현상. 방법들의 두 번째 범주는 순환 구조들을 설계하는 것과 같은 복잡한 메커니즘들(Zhou et al., 2023; Zhang et al., 2024), 토큰 선택(Mothashami and Jaggi, 2023; Tworkowski et al., 2023), 또는 슬라이딩 윈도우들(Xiao et al., 2023; Han et al., 2023)을 사용하여, 제한된 컨텍스트 윈도우들이 더 긴 입력 텍스트들을 처리할 수 있게 한다. 이러한 방법은 계산 효율에서 이점을 제공하지만, 여러 번의 반복 반복 또는 윈도우 슬라이딩에서는 귀중한 장기 의존성이 점차 간과될 수 있다.\n' +
      '\n' +
      '이 연구에서는 긴 텍스트를 처리하는 문제를 해결하기 위해 LongAgent라고 하는 유망한 새로운 방법을 소개한다. LongAgent는 멀티에이전트 협업을 통해 \\(100k\\) 이상의 문서를 처리하는데, 그림 1과 같이 리더와 다수의 구성원으로 구성된다. 리더는 1) 사용자 의도를 이해하고 구성원 간의 토론을 조직하는 것, 2) 상충되는 의견을 해결하기 위해 구성원 간의 커뮤니케이션을 감독하는 것, 3) 관련 정보 및 추론 답변을 수집하는 중입니다. 각 구성원은 할당된 텍스트 청크의 내용에 따라 리더의 지시에 응답하는 작업을 수행합니다. 본 논문에서는 리더가 회원의 환각으로 인해 수십 명에서 수백 명의 회원에 대한 응답으로부터 정확한 정보를 얻는 것은 쉬운 일이 아니다. 본 논문에서는 리더가 회의 중에 서로 상반된 의견을 가진 회원을 탐지하고 직접 상호 작용하여 환각적 응답을 제거할 수 있도록 한다. 우리는 간단한 사실 검색을 여러 문서를 포함할 수 있는 더 어려운 질문 답변으로 변경한다. 문서의 답변과 관련된 엔티티는 내부 지식에 의존하여 모델이 바로 가기를 취하는 것을 방지하기 위해 수정되었다. 우리는 새로운 테스트의 이름을 Haystack PLUS_에서 _Needle로 지었다.\n' +
      '\n' +
      '본 연구의 주요 기여는 다음과 같다 : 1) LongAgent를 제안하며, \\(100k\\) 토큰을 초과하는 긴 텍스트를 효과적으로 처리하기 위해 LLMs을 \\(4k\\) 컨텍스트 크기로 스케일링한다; 2) Haystack PLUS_에서 더 큰 벤치마크인 _Needle을 구축하여 LLMs의 긴 텍스트 성능에 대한 보다 포괄적인 평가를 가능하게 한다. 3) 실험 결과는 LLaMA-7B를 기반으로 한 LongAgent가 긴 텍스트 처리에서 GPT-4를 능가할 가능성을 보여준다. 이러한 개선은 다중 에이전트 협업이 긴 텍스트 처리를 개선하기 위한 유망한 방향임을 강력하게 시사한다.\n' +
      '\n' +
      '##2 롱텍스트 처리를 위한 롱에이전트\n' +
      '\n' +
      '### Method Overview\n' +
      '\n' +
      '그림 2와 같이 LongAgent의 작동 메커니즘을 설명하기 위해 Long-Text 질문 답변을 예로 사용한다. 롱에이전트는 긴 텍스트(x\\)와 사용자 질의(q\\)가 주어졌을 때, 여러 에이전트의 협력적 노력을 통해 긴 텍스트(x\\)와 최종 응답 이유(r\\)로부터 관련 증거를 찾는다. 이 과정은 다음의 4단계를 포함한다:\n' +
      '\n' +
      '(1) ** Members** 선택. 롱에이전트는 긴 텍스트 처리를 위한 태스크 진단 방법으로 처리할 태스크를 기반으로 맞춤형 에이전트 팀을 구성할 수 있도록 지원한다. 예를 들어, 주어진 passages_에 기초하여 과제 설명\\(d=\\)"_질문에 답하는 것이 주어지면, 리더는 보다 정확하고 신뢰성 있는 응답을 위해 팀원들을 인스턴스화하기 위해 QA 전문가 모델을 선택한다. 그런 다음, 긴 텍스트 \\(x\\)을 미리 정의된 크기의 \\(n\\) 청크 \\(\\{c_{1},c_{2},...,c_{n}\\}\\)으로 분할하고 이를 처리하기 위해 \\(n\\) 멤버 \\(\\{m_{1},m_{2},...,m_{n}\\}\\}\\)에 분배한다.\n' +
      '\n' +
      '(2) **협업 추론**. 복잡한 사용자 질의(\\(q\\)에 대해 리더는 여러 개의 하위 질문으로 체계적으로 분할하고, 멤버들이 각각의 청크에서 단서를 찾기 위해 협력할 수 있도록 구성한다. 도 2에 도시된 바와 같이, \\(q=\\)"__2015 Diamond Head Classic이라는 선수의 MVP는 어느 팀에 출전하는가?_" 선두는 먼저 2015년 다이아몬드 헤드 클래식의 MVP를 수상한 선수를 파악한 후 이 선수가 어떤 팀에서 뛰는지 파악한다. 더 복잡한 질의의 경우, 협력 추론은 여러 라운드를 수반할 것이다.\n' +
      '\n' +
      '(3) ** 갈등 해결** 모델 환각으로 인해 일부 구성원은 문서에 언급되지 않은 거짓 답변으로 응답할 수 있다. 분명히, 이러한 잘못된 대답은 올바른 대답과 상충된다. 리더는 그러한 갈등을 파악하고 문서에 명시적으로 언급된 정답을 확인할 필요가 있다.\n' +
      '\n' +
      '(4) **Deducing the Answer**. 리더가 현재 토론 이력이 사용자 질의에 대한 답변을 도출하기에 충분하다고 믿는 경우, 토론을 종료하고 최종 응답을 제공한다.\n' +
      '\n' +
      '다음 섹션에서는 앞서 언급한 네 단계의 세부 사항에 대해 자세히 설명한다.\n' +
      '\n' +
      '회원 자격증 취득을 위한 전문가 선정\n' +
      '\n' +
      'LongAgent의 작업 메커니즘은 멤버들을 조정하고 각각의 청크들의 처리 결과들을 통합하여 최종 응답을 형성하는 것을 포함한다. 다양한 긴 텍스트 작업은 구성원이 다양한 텍스트 처리 기능을 보유하도록 요구한다. 이 문제를 해결하기 위해 전문가 모델을 활용하여 작업별 에이전트 팀을 구성하여 보다 정확한 응답을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '전문가 모델 구축**: 이 단계는 전문가 모델 \\(E=\\{e_{1},e_{2},...,e_{s}\\}\\)의 후보 집합을 구축하는 것을 목표로 하며, 여기서 서로 다른 전문가가 서로 다른 작업에서 탁월하다. 강력한 모델(예: GPT-4, GPT-3.5)의 경우 전문가 모델을 구성하기 위해 신속한 기반 접근법을 사용한다. 특정 프롬프트 템플릿은 해당 작업에 대한 모델의 내부 기능을 이끌어내는 데 사용됩니다. 예를 들어, "_당신은 질문에 답하는 전문가이며, 주어진 문서로부터 관련 정보를 검색하고 답변을 제공하는 데 능숙하다._" 이 접근법의 이점은 모든 전문가 모델이 동일한 매개변수를 공유할 수 있다는 것이다. 약한 모델(예: LLaMA-7b)의 경우, 태스크 지식을 주입하기 위해 미세 조정 기반 접근법을 사용하여 해당 태스크에 대한 전문가를 만든다. 이 방법의 장점은 특정 작업에 대해 더 정확한 응답을 생성할 수 있다는 것이다.\n' +
      '\n' +
      '구성원 선발**: 처리해야 할 작업에 대한 자연어 설명이 주어지면 리더에게 팀원 역할을 수행할 적합한 전문가\\(e_{i}\\in E\\)를 선택하도록 촉구한다. 우리는 당면한 작업이 완료되기 위해 오직 한 명의 특정 전문가만을 필요로 한다고 가정한다. 기술 세트가 다른 전문가들 간의 협업이 필요한 복잡한 업무는 추후 업무로 남겨둡니다. 즉시 템플릿은 탭에 설명되어 있다. 2. 팀의 모든 구성원은 전문가 모델의 매개변수를 공유한다.\n' +
      '\n' +
      '### Collaborative Reasoning\n' +
      '\n' +
      '사용자 질의에 응답하기 위해 리더는 텍스트를 처리하고 관련 정보를 획득하기 위해 구성원을 조정해야 한다. 리더는 팀의 핵심으로서 다음과 같은 의사결정 과정을 지속적으로 실행한다.\n' +
      '\n' +
      '\\[a\\sim\\text{Leader}(a|S,q), \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(q\\)은 사용자 질의를 나타내고, \\(S=\\{s_{1},s_{2},...,s_{m}\\})은 역사적 대화 상태를 나타내며, \\(s_{i}\\)은 리더로부터의 명령과 모든 구성원으로부터의 응답으로 구성된 대화 라운드를 나타낸다. 리더는 대화이력\\(S\\)과 사용자 질의\\(q\\)에 기초하여 액션\\(a\\in\\texttt{NEW\\_STATE},\\texttt{CONFLIICT},\\texttt{ANSWER}\\}\\)을 샘플링한다. 만약 \\(a=\\texttt{NEW\\_STATE}\\)인 경우, 이전 \\(i\\) 라운드의 대화 이력에 포함된 정보가 질의 \\(q\\)에 응답하기에 불충분하다는 것을 나타낸다. 따라서 리더는 다음 대화 라운드 \\(s_{i+1}\\)을 시작하고 정보를 더 수집하기 위한 새로운 지침을 생성한다. 반대로 \\(a=\\texttt{ANSWER}\\)이면 리더가 현재 수집된 정보를 최종 답변을 도출하기에 충분하다고 판단하고 이에 따라 협업 프로세스가 종료됨을 의미한다. CONFLIICT는 두 가지 이외의 특수한 상태이다.\n' +
      '\n' +
      '도 2: LongAgent 개요. 1단계에서는 리더가 처리할 과제에 대한 설명을 바탕으로 맞춤형 에이전트 팀을 구성한다. 두 번째와 세 번째 단계에서 리더는 문서를 통해 정보를 수집하고 갈등을 해결하기 위해 팀을 구성합니다. 이 프로세스는 리더가 최종 응답을 생성하기 위해 충분한 정보가 수집되었다고 생각할 때까지 여러 라운드 동안 계속될 수 있으며, 이는 단계 4에서 내보낸다.\n' +
      '\n' +
      '앞서 언급한 주들은 현재 대화 라운드 \\(s_{i}\\)에서 리더가 구성원들로부터 상충되는 답변을 인식하고 있음을 나타낸다. 우리는 다음 하위 섹션에서 이 상황에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Resolving Conflicts\n' +
      '\n' +
      '모델 환각으로 인해 구성원은 덩어리에 언급되지 않은 내용으로 응답할 수 있다. <그림 2>의 2단계에서의 대화는 본문 청크에서 후자가 언급되지 않았음에도 불구하고, 두 멤버가 각각 _Buddy Hield_와 _Mark Gibson_를 2015 다이아몬드 헤드 클래식의 MVP로 믿는 예제 역할을 한다. 첫째, 리더의 지시에 대한 대답이 청크에 있을 때, 회원은 환각을 일으키기 보다는 정확한 답변을 제공하는 경우가 많다. 둘째, 리더의 지시에 대한 대답이 없을 때, 모델은 감독된 미세 조정 후에도 \'언급하지 않음\'으로 대답하는 대신 답을 조작하는 경우가 많다. 이 특징을 이용하여, 리더는 먼저 답변들이 충돌하는 멤버 ID들을 식별한 다음, 이들 멤버들에게 청크들을 쌍방향으로 공유하도록 요청하고 답변들을 다시 제공한다:\n' +
      '\n' +
      '\\[\\text{hallucination}=m_{i}(c_{i}), \\tag{2}\\] \\[\\text{Truth}=m_{j}(c_{j}),\\](3) \\[\\text{Truth}=m_{j}(c_{j}\\oplus c_{i}) \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(c_{i}\\)와 \\(c_{j}\\)은 각각 두 개의 텍스트 청크를 나타내며, 여기서 \\(c_{j}\\)은 정답을 포함하고 \\(c_{i}\\)은 그렇지 않다. \\(c_{i}\\) (m_{i}\\) 및 \\(m_{j}\\)는 두 개의 부재를 나타낸다. 실험 결과는 텍스트 청크를 공유하는 것이 간단하면서도 효과적인 전략임을 보여준다. 환각을 경험하는 대다수의 구성원은 정답이 포함된 청크를 받았을 때 원래 응답을 수정하는 경향이 있어 정확한 출력이 나온다. 다중 에이전트 토론 Du et al. (2023) 및 반사 Shinn et al. (2023)과 같은 환각 문제를 완화하기 위한 몇 가지 고급 메커니즘을 인정하지만, 이는 본 논문의 초점이 아니며 향후 연구를 위한 방법으로 남겨둔다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '### Evaluation Protocol\n' +
      '\n' +
      '**Needle-in-a-Haystacke PLUS**: _Needle-in-a-Haystacke_Kamr addt2023은 현재 긴 텍스트를 다루는 능력을 평가하기 위한 가장 인기 있는 테스트베드 중 하나이다. 이 설정에서, 관심 사실 또는 진술(_needle_)은 긴 산만 문서(_haystack_) 내에 배치되고, 모델은 이 숨겨진 키 정보를 검색하는 임무를 받는다. 성능은 산만 문서 내의 니들의 위치 및 산만 문서 자체의 길이를 변화시킴으로써 평가된다. 본 논문에서는 긴 텍스트 성능을 보다 포괄적으로 평가하기 위해, 단순 사실 검색에서 보다 도전적인 단일 문서/다중 문서 질의 응답 태스크로 초점을 이동시키는 _Needle-in-a-Haystacke PLUS_를 제안한다. _Needle-in-a-Haystacke PLUS_에서, _needle_는 답변들을 포함하는 문서(들)를 나타내는 반면, _haystacke_는 산만하게 하는 문서들을 포함한다. 모델은 건초더미 내에 흩어져 있는 하나 이상의 관련 문서를 찾고 그들로부터 정답을 추론해야 한다. 평가의 목적을 위해 정답이 특정 개체이거나 단순한 예/아니오 응답인 질문과 같이 확실한 답이 있는 질문을 의도적으로 테스트 데이터로 선택한다. 내부 지식에 의존하는 모델의 위험을 완화하기 위해 문서 내 답변과 직접 관련된 개체를 허구 개체로 대체한다. 부록 A에서는 단일 문서 QA와 다중 문서를 포함하는 다중 홉 QA를 포함하여 테스트 데이터의 수집 과정에 대해 자세히 설명한다.\n' +
      '\n' +
      '**Synthetic Task**: _Needle-in-a-Haystacke PLUS_ 테스트 외에도 널리 사용되는 두 개의 긴 시퀀스 평가 작업 Mohtashami and Jaggi(2023); Liu et al. (2023); Zhang et al. (2023): 긴 텍스트 검색 및 수치 비교. (1) needle-in-a-haystacke 태스크와 유사하게, 이들 합성 태스크는 모두 ACC를 평가 메트릭으로 사용하여 평가를 용이하게 한다. 메트릭의 차이는 모델의 긴 시퀀스 처리 능력의 차이를 직접적으로 반영할 수 있다. (2) 오픈 소스 모델을 미세 조정하기 위한 훈련 데이터를 자동으로 합성할 수 있다. 긴 텍스트 검색 태스크는 다음과 같은 세 가지 하위 태스크를 포함한다: 1) PassKey 검색: 잡음이 많은 긴 컨텍스트에서 숨겨진 키를 검색하는 것; 2) Number 검색: 잡음이 많은 긴 컨텍스트에서 반복되는 숨겨진 번호를 찾는 것; 3) KV 검색: 사전과 키에서 해당 값을 찾습니다. 수치 비교를 위해서는 상위\\(K\\) 수, 중위수 등과 같은 크기\\(100k\\) 토큰의 숫자열에서 특정 요구 사항을 충족하는 숫자를 찾아야 하며, 여기서 \\(K\\)은 \\(1\\), \\(2\\), \\(3\\)일 수 있다.\n' +
      '\n' +
      '### Compared Methods\n' +
      '\n' +
      '3. 위치 보간을 통해 RoPE 기반 사전 훈련된 대형 언어 모델의 컨텍스트 윈도우 크기를 확장한다.\n' +
      '\n' +
      '14. YaRN은 컨텍스트 윈도우를 효율적으로 확장하기 위한 개선된 방법이다. 이 작업은 PE를 이론적으로 무한한 컨텍스트 길이로 확장하도록 직접 수정한다.\n' +
      '\n' +
      'claude2.1**[1]. 인류코퍼레이션이 출시한 클로드 2.1은 200K 토큰의 컨텍스트 창이 특징이며 모델 환각 비율이 크게 감소했다.\n' +
      '\n' +
      '**GPT-4 Turbo**[1]. OpenAI의 GPT-4 터보 모델은 128K의 컨텍스트 창을 제공하며 단일 프롬프트 내에서 300페이지를 초과하는 텍스트를 처리할 수 있다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '에이전트 팀을 구축하기 위해 LLaMA2-7b 기반에서 감독 미세 조정을 수행한다. 에이전트 팀 내에서 리더는 다양한 작업을 수행하기 위해 구성원을 조정하는 역할을 합니다. GPT-4를 활용하여 리더를 훈련시키기 위해 각 태스크에 대한 \\(1,000\\)의 상호작용 궤적을 생성하고, 이러한 상호작용 궤적의 정확성을 수동으로 검증하였다. 회원들은 지도자의 지시에 따라 문서를 처리하는 임무를 받는다. 이를 위해 QA 전문가, 검색 전문가 및 수학적 전문가를 교육하여 구성원을 인스턴스화한다. 인스턴스화된 구성원 수에 관계없이 모두 단일 전문가 모델의 매개변수를 공유한다. QA 전문가를 위한 훈련 데이터는 \\(25,000\\) 샘플로 구성된 SQuAD 훈련 세트에서 조달된다. 이 중 \\(10,000\\) 표본은 문서 내에 답변을 포함하고 나머지 \\(15,000\\) 표본은 그렇지 않아 모델이 답변을 자제할 필요가 있다. 우리는 연접을 통해 문서 길이를 \\(2500\\)-\\(3000\\) 토큰으로 확장했다. 검색 전문가와 수학 전문가를 위한 학습 데이터는 각 작업에 대해 생성된 \\(1k\\)에서 \\(3k\\) 토큰까지의 길이로 자동 합성되며, 검색될 정보는 문서 내의 임의의 위치에 고르게 배치된다. 모든 훈련 데이터가 최종 평가 데이터와 중첩되지 않는다는 점에 유의하는 것이 중요하다. 모든 작업에 대한 프롬프트 및 상호 작용 궤적은 부록 B를 참조하십시오.\n' +
      '\n' +
      '##4 결과 및 논의\n' +
      '\n' +
      '### Overall Performance\n' +
      '\n' +
      '롱에이전트의 장문 처리에 대한 우수성을 입증하기 위해 강력한 상용 모델인 GPT-4 Turbo 및 Claude 2.1과 장문 처리, PI 및 YARN을 위한 최첨단 학술 방법과 비교한다.\n' +
      '\n' +
      '**멀티 에이전트 협업을 통해 4k 컨텍스트 윈도우만으로 LLaMA를 미세 조정하면 최대 128k의 컨텍스트를 효과적으로 처리할 수 있다.**\n' +
      '\n' +
      '_Needle-in-a-Haystack PLUS_에 대한 결과는 각각 그림 3과 4에 나와 있다. 미세 조정된 LLaMA2-7B로 구성된 LongAgent는 단일 문서 설정에서 평균 19.53%(62.00%에서 81.53%)의 개선 및 평균 4.96%(50.37%에서 55.33%)의 개선으로 문서 길이 1k에서 128k 범위의 GPT-4보다 크게 우수하다.\n' +
      '\n' +
      '그림 3: 단일 문서 질의 응답 설정에서 _Needle-in-a-Haystack PLUS_의 결과 비교. LangAgent 기법에서는 1k에서 128k(62.00\\%\\에서 81.53\\%\\)의 범위에서 GPT-4에 비해 LLaMA2-7B 모델의 평균 정확도 향상을 보였다.\n' +
      '\n' +
      '다중 문서 설정 아래. 롱에이전트가 다운스트림 태스크에 대해 미세 조정된다는 점을 고려하여, 공정한 비교를 위해, 우리는 \\(1\\)에서 \\(16k\\)까지의 길이를 갖는 태스크 데이터에 대해 PI 및 YARN을 미세 조정한다(더 긴 컨텍스트를 갖는 트레이닝은 하드웨어 한계를 초과한다). 실험 결과는 테스트 문서의 길이가 훈련된 최대 길이를 초과할 때 PI 및 YARN이 제대로 결과를 생성하지 못한다는 것을 보여준다. 그리드의 첫 두 열에 해당하는 \\(0\\)-\\(16k\\) 범위 내에서도 LongAgent의 평균 성능은 PI와 YARN의 성능을 능가한다. 4개의 합성 태스크에 대한 결과는 표 1과 같다. 표로부터 LongAgent가 fine-tuned LLaMA2-7B 모델에 의해 지원되는 것은 3개의 검색형 태스크에서 모든 기준 모델보다 우수한 성능을 보임을 알 수 있다. 이는 LongAgent가 다양한 롱텍스트 태스크를 처리하는 데 있어서 우수함을 보여준다.\n' +
      '\n' +
      '**GPT-3.5와 같은 보다 강력한 모델에 의해 지원되는 LongAgent의 경우 미세 조정이 필요하지 않습니다.**\n' +
      '\n' +
      '프롬프트를 통해 GPT-3.5는 특정 기술을 가진 리더와 구성원으로 동시에 작용할 수 있다. GPT-3.5가 지원하는 LongAgent는 단지 \\(16k\\)의 컨텍스트 윈도우만을 가지고 있음에도 불구하고, 길이가 \\(16k\\)을 훨씬 초과하는 문서들을 효과적으로 처리할 수 있음을 알 수 있었다. 특히, _needle-in-a-haystack PLUS_ 태스크에서 LongAgent는 단일닥 및 다중닥 설정에서 GPT-4에 비해 각각 \\(6.780\\%\\) 및 \\(1.5\\%\\)의 개선을 달성하였다. 표 1의 4개의 합성 작업에 대해 LongAgent는 또한 \\(100k\\) 길이의 문서로 2개의 검색 작업에서 만점을 달성했다. KV 검색 및 수치 비교 작업에서도 대부분의 기준선을 능가했다.\n' +
      '\n' +
      '**1k에서 128k 범위의 입력만 테스트했지만 LongAgent는 길이가 128k를 초과하는 입력을 처리하는 데 잠재력을 보여준다.**\n' +
      '\n' +
      '도 3 및 도 4에서는 Claude 2.1로 middle_ 현상에서의 _lost를 관찰하였다. 구체적으로, 입력 길이가 증가함에 따라 Claude 2.1의 평균 정확도는 점차 감소하지만, 도 3의 Claude 서브피규어의 첫 번째 행과 마지막 행, 그리고 도 4의 Claude 서브피규어의 세 번째 행에서는 비교적 높은 정확도는 일관되게 유지된다. 이는 Claude 2.1이 긴 텍스트의 시작과 끝을 효과적으로 모델링할 수 있지만 중간에 있는 주요 정보를 효과적으로 활용하지 못함을 시사한다. LongAgent는 chunking을 통해 긴 텍스트의 직접적인 처리를 피하며, 입력 길이에 관계없이 청크 크기는 일정하게 유지되므로 \'lost in the middle\' 현상을 피할 수 있다. 더 긴 입력\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Methods**} & \\multicolumn{3}{c}{Retrieval} & \\multicolumn{1}{c}{Numerical} \\\\ \\cline{2-5}  & PassKey & Number & KV & Comparison \\\\ \\hline GPT-4 & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(0.890\\) & \\(0.600\\) \\\\ Kimi-Chat & \\(0.981\\) & \\(0.954\\) & \\(0.536\\) & \\(0.126\\) \\\\ Claude2.1 & \\(0.978\\) & \\(0.981\\) & \\(0.654\\) & \\(0.323\\) \\\\ YaRN & \\(0.927\\) & \\(0.566\\) & \\(-\\) & \\(0.171\\) \\\\ Ours-GPT3.5 & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(0.638\\) & \\(0.511\\) \\\\ Ours-LLAMA-7B & \\(\\mathbf{1.000}\\) & \\(\\mathbf{1.000}\\) & \\(\\mathbf{0.966}\\) & \\(\\mathbf{0.625}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 4가지 합성 작업에 대한 실험 결과(정확도)\n' +
      '\n' +
      '그림 4: 다중 문서 질의 응답 설정에서 _Needle-in-a-Haystack PLUS_의 결과 비교. LangAgent 기법을 이용한 미세조정 LLaMA2-7B 모델은 GPT-4에 비해 \\(1k\\)에서 \\(128k\\) (50.37\\%\\에서 \\(55.33\\%\\)의 평균 정확도 향상을 보였다.\n' +
      '\n' +
      '에이전트 상호 작용이 복잡해질 수 있으며, 실험 결과는 LongAgent에 대해 유의미한 성능 감소를 나타내지 않는다. 전반적으로 LongAgent는 길이가 \\(128k\\)을 초과하는 입력을 처리할 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '### Hallucination Analysis\n' +
      '\n' +
      '롱에이전트의 오류는 주로 환각 문제의 한 유형에 기인한다는 것을 발견했다: 구성원의 청크가 리더의 지시와 관련된 정보를 포함하지 않을 때, 구성원은 때때로 내부 지식을 기반으로 대답하거나 오답을 응답으로 조작한다. 이 하위 섹션에서는 훈련 데이터의 레시피와 청크 크기라는 두 가지 주요 요인이 모델 환각에 미치는 영향을 탐구한다. 그림 5와 같이 학습 데이터에서 \'거부\' 유형 데이터가 증가함에 따라 모델이 정답을 정확하게 거부한 비율은 \\(51.0\\%\\)에서 \\(78.6\\%\\)으로 증가했다. 그러나 \'거부\' 데이터의 증가는 모델의 질문에 대한 답변 능력도 약간 손상시킨다. <그림 8>에서 보는 바와 같이 \'Answer:Reject\'의 비율이 2:1에서 2:3으로 증가하면 모형의 정확도는 \\(83.3\\%\\)에서 \\(78.3\\%\\)으로 감소하며, 문서가 답변을 포함하고 있지만 답변을 거부하는 경우도 적지 않다.\n' +
      '\n' +
      '데이터 비율 외에도 청크 크기도 모델 환각에 영향을 미치는 중요한 요인이다. 그림 5와 같이 청크 크기가 \\(500\\)에서 \\(2,000\\)으로 증가하면 환각 문제가 완화된다. 이는 주로 학습 데이터의 길이가 약 \\(3,000\\) 토큰이고, 청크 크기를 증가시키면 학습 데이터의 길이와의 간격이 줄어들기 때문이다. 그러나 청크 크기가 \\(2,000\\)을 초과하면 청크 크기를 더 증가시키면 모델 환각이 크게 악화된다. 우리는 시퀀스 길이가 너무 길면 문서 모델링에 대한 모델의 부적절성이 더 두드러지기 때문이라고 추측한다. 따라서 우리는 더 큰 컨텍스트 윈도우를 구성하는 방법을 연구하면서 \\(4k\\) 윈도우 내에서 텍스트의 모델링을 소홀히 해서는 안 된다고 생각한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '섹션 2.4에서 우리는 구성원 간 커뮤니케이션_을 통해 구성원 간의 충돌을 해결한다. 이 메커니즘의 유효성을 입증하기 위해 이 메커니즘을 도입하기 전과 후의 모델 정확도의 차이를 계산한다. 그림 6에서 볼 수 있듯이, _inter-member communication_ 메커니즘은 입력 텍스트 길이가 \\(1k\\)에서 \\(128k\\)까지 범위에 걸쳐 \\(18.9\\%\\)의 평균 정확도 향상으로 이어진다. 나아가 텍스트의 길이에 따라 구성원의 수가 증가하고 환각을 경험하는 구성원의 수도 증가한다. 이러한 맥락에서 갈등으로 인한 정확성 향상\n' +
      '\n' +
      '그림 5: 데이터 레시피가 모델 환각에 미치는 영향. \'답변\'과 \'거부\'는 두 가지 유형의 데이터를 나타낸다. 전자의 경우 문서에는 질문에 대한 답변이 포함되어 있지만 후자의 경우 그렇지 않다.\n' +
      '\n' +
      '그림 6: _intermember communication_ mechanism을 통한 정확도 향상.\n' +
      '\n' +
      '해상도는 훨씬 더 분명해진다.\n' +
      '\n' +
      '### Efficiency Advantage\n' +
      '\n' +
      '롱에이전트의 긴 텍스트 처리 시간 복잡도는 롱에이전트의 긴 텍스트 처리 시간 복잡도는 \\(\\mathcal{O}(N)\\)이다. 이 하위 섹션에서 우리는 이 점을 경험적으로 검증한다. 그림 7에서 볼 수 있듯이 \\(1k\\)-\\(100k\\) 범위 내에서 LongAgent의 잠복기는 길이에 따라 거의 선형적으로 성장한다. 2차 복잡도를 갖는 Full Attention의 경우, 플래시 어텐션과 같은 기법들의 사용에 관계없이 추론 지연 시간이 급격히 증가한다. 10k 토큰 처리 시 Full Attention의 latency는 LongAgent 처리 \\(50k\\) 토큰의 latency를 이미 초과하였다. 또한, 특정 메모리 최적화 기술 없이 80G 메모리를 가진 단일 A100 GPU는 최대 11k 길이의 텍스트 추론만 지원할 수 있으며 플래시 주의에도 이 수는 \\(15k\\)으로 증가할 수 있다. 동일한 설정에서 LongAgent는 40G 미만의 메모리로 약 \\(100k\\)의 컨텍스트를 처리할 수 있다.\n' +
      '\n' +
      '##5 관련 작품\n' +
      '\n' +
      '### Long-text Modeling\n' +
      '\n' +
      '더 긴 시퀀스들을 다루기 위해 위치 인코딩(PE)을 확장하기 위한 여러 방법들이 제안되었다. 처음에 RoPE 및 PI [3]과 같은 접근법은 미리 훈련된 한계 내에서 위치 지수를 보간하려고 시도했지만 주파수 변동은 무시했다. 최근의 발전에는 고주파 성분 손실을 다루는 "NTK-aware" [13] 보간법과 "Dynamic NTK" [13] 보간법이 있다. 또한, "NTK-by-parts" [13] 보간은 더 긴 컨텍스트 데이터에서 미세 조정될 때 다른 보간보다 우수하다. 더 긴 시퀀스를 관리하기 위한 또 다른 인기 있는 접근법은 지역적 관심에 대한 글로벌 인과적 관심을 제한하는 것을 포함한다. ReRoPE[23]은 사전 훈련 동안 컨텍스트 길이를 절단하고 LM-무한[14]는 쉐브론 모양의 창에 대한 주의를 제한한다. mohtashami2023은 텍스트 조각 뒤에 랜드마크 토큰을 삽입하는 반면, zhang2024neural은 조각을 요약하기 위한 비콘 토큰을 제안한다. 이와는 대조적으로, 본 방법은 미세 조정을 위해 소량의 에이전트 상호 작용 트랙(수백 개의 에이전트 상호 작용 트랙)만을 활용하면서 가치 있는 상황 정보를 잃을 위험을 효과적으로 회피하여 훈련 비용을 절감한다.\n' +
      '\n' +
      '### LLM 기반 멀티 에이전트 시스템\n' +
      '\n' +
      '최근 몇 년 동안 LLM 기반 다중 에이전트 시스템은 학계에서 광범위한 관심을 받았다. 효율성을 높이고 보다 복잡한 추론 작업을 수행하기 위해 개인 간의 협력을 활용하는 데 많은 노력을 기울였다[15, 16, 17, 18, 19]. 에이전트가 실제 세계에서 다양한 동적 작업을 효과적으로 처리할 수 있도록 하기 위해 연구자들은 외부 도구를 에이전트의 의사 결정 프로세스[13, 14, 15]에 통합하여 정확한 계산을 수행하고 데이터베이스 또는 검색 엔진에서 최신 정보를 검색할 수 있도록 했다. 이러한 접근법에서 우리와 가장 관련이 있는 것은 PEARL[22]과 MemWalker[3]이다. PEARL은 자체 생성된 의사 API를 호출하여 긴 텍스트 내에서 관련 콘텐츠에 대한 모델의 초점을 향상시킨다. 그러나 에이전트의 컨텍스트 창 내에서 긴 텍스트만 처리할 수 있으며 긴 텍스트에는 효과가 없다. MemWalker는 트리 기반 요약 접근법을 통해 에이전트가 더 긴 텍스트를 처리할 수 있지만, 중요한 정보는 여러 요약 후에 손실되어 에이전트가 관련 없는 컨텍스트에서 손실될 수 있다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 논문에서는 다중 에이전트 협업에 기반한 새로운 긴 텍스트 처리 기법인 LongAgent를 제안한다. LongAgent scaling LLMs with \\(4k\\) context size to effectively handle the long text exceeds \\(100k\\) token. 제안된 _inter-member communication_ 메커니즘은 문서를 읽을 때 구성원 환각을 완화하여 수십에서 수백 명의 구성원의 리더에 의한 효과적인 관리를 용이하게 한다. 우리는 또한\n' +
      '\n' +
      '그림 7: LongAgent 스킴은 긴 텍스트에 대해 완전한 주의를 직접 수행하는 것에 비해 상당히 우수한 시간 및 메모리 효율을 나타낸다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '오피어 프레스, 노아 A 스미스, 마이크 루이스. 2022. Train short, test long: Attention with linear bias enables input length extrapolation.\n' +
      '* Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 텍스트의 기계 이해에 대한 100,000개 이상의 질문. _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas. 컴퓨터 언어학과의 연관성\n' +
      '* Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. 반사: 언어 강화 학습을 갖는 언어 에이전트.\n' +
      '* Su(2023) Jianlin Su. 2023. 정류 회전 위치 임베딩. [https://github.com/bojone/rerope] (https://github.com/bojone/rerope).\n' +
      '* Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: 대규모 언어 모델을 프롬프트하여 긴 문서를 통해 작업을 계획하고 실행합니다.\n' +
      '* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, and Xavier Martinet. 2023. 라마: 개방적이고 효율적인 기초 언어 모델들.\n' +
      '* Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milos. 2023. 포커싱된 트랜스포머: 콘텍스트 스케일링을 위한 대비 훈련.\n' +
      '* Wang et al. (2024) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. 대규모 언어모델에서 창발적 인지 시너지를 발휘하기: 다중 페르소나 자기협력을 통한 과제 해결 주체.\n' +
      '* Xiao et al.(2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. 어텐션 싱크가 있는 효율적인 스트리밍 언어 모델.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. 코헨, 루슬란 살라쿠티노프 크리스토퍼 매닝 2018. Hotpotqa: 다양하고 설명 가능한 멀티홉 질문 응답을 위한 데이터셋.\n' +
      '* Zhang et al. (2024) Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, 및 Zhicheng Dou. 2024. 4k에서 400k로 치솟기: 활성화 비콘으로 llm의 컨텍스트 확장.\n' +
      '* Zhang et al. (2023) Xinrong Zhang, Yingfa Chen, Shengding Hu, Qihao Wu, Junhao Chen, Zihang Xu, Zhenning Dai, Xu Han, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2023. Infinitebench: 128k long-context benchmark for language models.\n' +
      '* Zhou et al. (2023) Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt: (임의로) 긴 텍스트의 대화형 생성.\n' +
      '\n' +
      '## 부록 수집 Needle-in-a-Haystack PLUS 테스트 데이터\n' +
      '\n' +
      '테스트 데이터는 단일 문서 QA와 다중 문서 QA의 두 부분으로 구성된다. 아래에서는 각각의 시공 과정에 대해 자세히 설명하겠습니다.\n' +
      '\n' +
      '### Single-document QA\n' +
      '\n' +
      '단일 문서 QA에 대해, 테스트 데이터는 SQuAD Rajpurkar 등(2016)을 기반으로 구축된다. SQuAD는 500개 이상의 위키피디아 문서 중에서 선택된 10만 개의 질문을 포함하는 대규모 기계 판독 이해 데이터 세트이다. 데이터세트의 각 질문은 주어진 문서에서 추출된 텍스트 세그먼트로서 그 답변을 갖는다. 우리는 SQuAD의 훈련 집합에서 무작위로 \\(100\\)개의 샘플을 선택하고 그 안에 있는 주요 개체를 대체한다. 이들 \\(100\\) 샘플들의 문서들은 _needles_로 사용되는 반면, 트레이닝 세트 내의 다른 샘플들의 문서들은 산만자 정보를 제공하는 _haystacks_로 랜덤하게 선택된다. _haystack_의 길이는 동일한 간격으로 \\(1,000\\)에서 \\(128,000\\) 토큰까지 총 15개의 서로 다른 길이를 갖는다. 침의 깊이는 동일한 간격으로 \\(0\\%\\)에서 \\(100\\%\\)까지이며 총 10\\(10\\)의 깊이를 갖는다. 깊이는 _haystack_에서 바늘의 위치를 나타낸다. 깊이(0\\\\%\\)는 건초 더미의 시작 부분에 있는 반면, 깊이(100\\%\\)는 끝 부분에 있음을 나타낸다. 각 길이와 깊이에 대해 무작위로 \\(10\\)_needles_를 선택하여 \\(10\\) 테스트 샘플을 구성한다.\n' +
      '\n' +
      '### Multi-document QA\n' +
      '\n' +
      '다중 문서 QA의 경우, 질문은 최종 답변을 추론하기 위해 둘 이상의 문서의 정보를 필요로 한다. 우리는 널리 채택된 다중 문서 QA 데이터셋인 HotpotQA Yang et al.(2018)을 기반으로 테스트 샘플을 구성한다. 답변하기 위해 두 문서의 정보가 필요한 HotpotQA의 유효성 검사 집합에서 \\(60\\) 질문을 선택한다. 각 샘플에는 2개의 _needles_가 포함되어 있으며, 건초 더미는 여전히 산만 문서로 구성된다. _haystack_의 길이는 동일한 간격으로 \\(1,000\\)에서 \\(128,000\\)의 토큰 범위이며 총 길이가 15\\이다. 두 개의 바늘은 건초더미 깊이의 \\(0\\%\\), \\(33\\%\\), \\(66\\%\\), \\(100\\%\\)에 무작위로 흩어져 \\(0\\,33\\%)\\), \\(0\\,66\\%)\\), \\((0\\,100\\%)\\), \\(33\\,66\\%)\\), \\(33\\,100\\%)\\, \\(66\\,100\\%)\\, \\(66\\,100\\%)\\의 6개의 조합으로 구성되어 있다. 특정 길이와 바늘 위치에 대해 무작위로 \\(10\\)_needles_를 선택하여 평가를 위한 \\(10\\) 테스트 샘플을 구성한다.\n' +
      '\n' +
      '각 작업에 대한 궤적\n' +
      '\n' +
      '### 단일문서 질의응답\n' +
      '\n' +
      'Dataset: Squad\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      '먼저 리더는 <표 2>와 같이 과제설명에 따라 회원을 모집한다.\n' +
      '\n' +
      '당신은 과제를 해결하기 위해 팀원을 모집해야 합니다. 작업 설명을 기준으로 해당 구성원을 선택합니다.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '{task_description}\n' +
      '\n' +
      '**# Members List:**\n' +
      '\n' +
      'QA 회원: 질의응답 문제를 해결하는 데 능숙합니다.\n' +
      '\n' +
      'KV 멤버: 사전에서 해당 값을 찾는 데 능숙합니다.\n' +
      '\n' +
      'NS 멤버: 시끄러운 긴 컨텍스트에서 반복되는 숨겨진 숫자를 찾는 데 능숙합니다.\n' +
      '\n' +
      '패스키 멤버: 시끄러운 긴 컨텍스트에서 숨겨진 키를 검색하는 데 능숙합니다.\n' +
      '\n' +
      '수학 멤버: 긴 리스트에서 특별한 정수를 찾는 것을 잘한다.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "member", "content": "your_chosen_member"}}\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '"type": "member", "content": "QA member"]__["type"\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      '단원을 모집한 후 단장은 <표 3>과 같이 구체적인 지시를 한다.\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      '구성원 첫 번째 입력에 대한 프롬프트는 표 4에 나와 있습니다. #문서의 내용은 전체 컨텍스트의 일부이며 각 구성원에 대한 변형이 있습니다. #인스트럭션의 내용은 리더의 첫 번째 출력에서 비롯된다.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      '리더는 답변을 거부하는 구성원을 배제하고 동일한 답변을 제공하는 구성원을 함께 모이게 된다.\n' +
      '\n' +
      '클러스터: [\'월스트리트저널\' : [1, 7, 8, 13, 15, 17, 18, 22, 23, 24, 28, 29 31, 32, 33, 34, 35, 37, 39, 42, 43], \'예일법저널\' : [5], \'월스트리트저널\' : [20]_\n' +
      '\n' +
      '**Member next input**\n' +
      '\n' +
      '멤버 다음 입력에 대한 프롬프트 템플릿은 표 4에 도시된 멤버 첫 입력과 동일하다. #문서의 내용은 두 멤버로부터의 문서의 병합이다.\n' +
      '\n' +
      '클러스터 1에서 멤버 43을 랜덤으로, 클러스터 2에서 멤버 5를 랜덤으로 선택한다. 먼저, 멤버 43과 멤버 5의 문서를 병합하여 새로운 결과를 얻는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt}} \\hline \\hline You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is: \\\\\n' +
      '1. 작업 설명 및 작업 목표를 아는 사람만 알 수 있으며, 다른 구성원은 알 수 없습니다. \\\\\n' +
      '2. 그러나 그들은 답변을 포함할 수 있는 다른 문서들을 수신할 것이고, 당신은 그들에게 그들의 문서를 질의하기 위한 지시를 보내야 한다. \\\\\n' +
      '3. 당신의 지시에는 과제에 대한 이해와 그들이 집중할 필요가 있는 것이 포함되어야 한다. 필요한 경우 지시사항에는 작업 목표가 명시적으로 포함될 수 있습니다. \\\\\n' +
      '4. 마지막으로, 그들이 반환한 질의 결과에 기초하여 작업을 완료해야 한다. \\\\\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답은 주어진 구절에서 추출되어야 한다. \\\\\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      '샌더는 로스쿨에서 인종적 선호에 의문을 제기하는 기사를 어느 출판사에서 냈나요?\n' +
      '**\\#구성원을 위한 명령어 생성 :**\n' +
      '\n' +
      '이제 모든 팀 구성원에 대한 지침을 생성해야 합니다. 특정 질문에 답하도록 요청하거나 각 문서에 따라 작업과 관련된 정보를 추출할 수 있습니다.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 2: 단일 문서 질문 답변 작업에 대한 리더의 첫 번째 입력에 대한 프롬프트 템플릿입니다. #Task Description의 내용은 사용자 입력으로부터 도출된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{142.3pt}} \\hline \\hline You output must following the JSON format: {{"type": "internet": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 단일 문서 질문 답변 작업에 대한 리더 다음 입력에 대한 프롬프트 템플릿. #Task Description 및 #Task Objective의 콘텐츠는 사용자 입력으로부터 도출된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '**Leader second input**\n' +
      '\n' +
      '다중 문서 질문 답변의 경우, 질문에는 여러 문서의 내용을 통합하는 답변이 필요합니다. 자세한 내용은 표 6에 나와 있다.\n' +
      '\n' +
      '**Leader second output**\n' +
      '\n' +
      '리더는 원래 질문을 분해해야 하므로, 리더의 두 번째 산출물은 첫 번째 하위 질문이다.\n' +
      '\n' +
      '_"type": "instruction", "content": "What was the population of Leshan in 2002?"_\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      '회원 첫 번째 입력에 대한 프롬프트는 표 7에 나와 있다. #문서의 내용은 전체 컨텍스트의 일부이며, 각 회원에 대한 변형이 있다. #인스트럭션의 내용은 리더의 첫 번째 출력에서 비롯된다.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      '_회원 1: 121,277회원 2: 문서는 답변을 포함하지 않는다._\n' +
      '\n' +
      '_Member 3: 문서는 답변을 포함하지 않는다.__\n' +
      '\n' +
      '_Member 30: 문서는 답변을 포함하지 않는다.__\n' +
      '\n' +
      '_Member 31: 문서는 답변을 포함하지 않는다.__\n' +
      '\n' +
      '**Leader third input**\n' +
      '\n' +
      '리더 세 번째 입력에 대한 프롬프트는 표 8에 나와 있으며, 현재 리더는 1라운드부터 여전히 대화 이력을 유지하고 있다.\n' +
      '\n' +
      '**Leader second output**\n' +
      '\n' +
      '_("type": "instruction", "content": "What were population of Quanzhou in 2002)"\n' +
      '\n' +
      '**Member second input**\n' +
      '\n' +
      '멤버 두 번째 입력에 대한 프롬프트는 표 9에 나와 있다. #문서의 내용은 전체 컨텍스트의 일부이며, 각 멤버에 대한 변형이 있다. #인스트럭션의 내용은 리더의 두 번째 출력에서 비롯된다.\n' +
      '\n' +
      '**Member second output**\n' +
      '\n' +
      '응답을 거부하는 구성원을 제외하고 동일한 답변을 제공하는 구성원을 클러스터링합니다.\n' +
      '\n' +
      '[\'150만\': [7], \'658,798\': [11]_Clusters: [\'150만\': [7], \'658,798\': [11]\n' +
      '\n' +
      '**Member third input**\n' +
      '\n' +
      '회원 세 번째 입력에 대한 프롬프트는 표 10에 나타나 있다. #문서의 내용은 회원 7과 회원 11로부터의 문서의 병합이다.\n' +
      '\n' +
      '**Member third output**\n' +
      '\n' +
      '"type": "response", "content": "658,798"]__["type"\n' +
      '\n' +
      '이때 클러스터가 하나밖에 남지 않아 모든 구성원은 하나의 고유한 결과만 남는다.\n' +
      '\n' +
      '**Leader last input**\n' +
      '\n' +
      '리더 마지막 입력에 대한 프롬프트는 표 11과 같다. 현재 리더는 이전 두 라운드의 대화 이력을 여전히 유지한다.\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      '리더는 두 개의 하위 문제에 대한 답을 통합하여 원래의 문제에 대한 최종 답을 얻는다.\n' +
      '\n' +
      '"type": "답변", "content": "Quanzhou"]__["type": "답변", "content": "Quanzhou"\n' +
      '\n' +
      '### Retrieve\n' +
      '\n' +
      '검색형 작업은 검색.KV, 검색.PassKey, 검색.번호의 세 가지 유형으로 나뉜다. 작업 설명은 작업에 따라 다르지만 상호 작용 궤적은 유사하며 프롬프트 템플릿도 동일하다. 따라서, 여기서 검색.KV 태스크를 시범을 위한 예로 선택한다.\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      '먼저 리더는 <표 2>와 같이 과제설명에 따라 회원을 모집한다.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '"type": "member", "content": "KV member"]__["type"\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      '리더 다음 입력에 대한 프롬프트는 표 12에 나와 있습니다.\n' +
      '\n' +
      '#Task Description 및 #Task Objective의 콘텐츠는 사용자 입력으로부터 도출된다.\n' +
      '\n' +
      '**Leader next output**\n' +
      '\n' +
      '_[ "type" : "instruction", "content" : "Finding the corresponding value from a dictionary and key. Key: "2b114db0-d87e-42d2-9b4c-0b1f115976ad" : 지정된 키와 연관된 값은: " ]__\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      '회원 첫 번째 입력에 대한 프롬프트는 표 13에 나와 있다.\n' +
      '\n' +
      '#문서의 내용은 각 구성원에 대한 변형이 있는 완전한 컨텍스트의 일부입니다.\n' +
      '\n' +
      '#인스트럭션의 내용은 리더의 첫 번째 출력에서 비롯된다.\n' +
      '\n' +
      '당신은 {멤버_넘즈} 멤버로 구성된 팀의 리더입니다. 작업을 해결하기 위해 팀이 협력해야 합니다. 상기 규칙은:\n' +
      '\n' +
      '1. 작업 설명 및 작업 목표를 아는 사람만 알 수 있습니다. 다른 구성원은 알지 못합니다.\n' +
      '\n' +
      '2. 그러나 그들은 답변을 포함할 수 있는 다른 문서들을 수신할 것이고, 당신은 그들에게 그들의 문서를 질의하기 위한 지시를 보내야 한다.\n' +
      '\n' +
      '3. 당신의 지시에는 과제에 대한 이해와 그들이 집중할 필요가 있는 것이 포함되어야 한다. 필요한 경우 지시사항에는 작업 목표가 명시적으로 포함될 수 있습니다.\n' +
      '\n' +
      '4. 마지막으로, 그들이 반환하는 질의 결과에 기초하여 작업을 완료해야 한다.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답만 주시고 다른 단어는 출력하지 마세요.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      '2002년 레샨이나 취안저우의 인구는 65만 8천 798명이었는가?\n' +
      '\n' +
      '**#구성원을 위한 명령어 생성 :**\n' +
      '\n' +
      '이제 모든 팀 구성원에 대한 지침을 생성해야 합니다. 특정 질문에 답하도록 요청하거나 각 문서에 따라 작업과 관련된 정보를 추출할 수 있습니다.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      '회신을 거부하는 회원을 제외하고, 회원 1, 회원 9, 회원 12는 서로 다른 3가지 응답을 제공한다.\n' +
      '\n' +
      '_Clusters:_\n' +
      '\n' +
      '[1], \'d3999999-9999-4999-899999999\'\': [9], \'d9999999-99998-b6e7-4af5-a589-fda0b7693f93\': [1], \'d399999999-999999-499999-89999999999\': [12]_9999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999\n' +
      '\n' +
      '**Member next input**\n' +
      '\n' +
      '프롬프트 템플릿은 \'멤버 다음 입력\'에 대한 프롬프트와 동일한 반면, #문서의 내용은 두 멤버로부터 문서를 병합하는 것이다.\n' +
      '\n' +
      '먼저, 멤버 1과 멤버 9의 문서를 병합하여 새로운 결과를 얻는다.\n' +
      '\n' +
      '멤버 1 및 멤버 9 병합: flec3dd8-b6e7-4af5-a589-fda0b7693f93_a589-fda0b7693f93_\n' +
      '\n' +
      '병합된 출력은 멤버(1)의 원래 출력과 동일하지만 멤버(9)의 원래 출력과 다르기 때문에, 멤버(9)는 제거되었다. 다음으로, 멤버 1 및 멤버 12의 문서를 병합하여 새로운 결과를 획득한다.\n' +
      '\n' +
      '회원 1 및 회원 12 병합: flec3dd8-b6e7-4af5-a589-fda0b7693f93_a589-fda0b7693f93_\n' +
      '\n' +
      '병합된 출력이 멤버 1의 원래 출력과 동일하고 멤버 12의 원래 출력과 상이하기 때문에 멤버 12를 제거한다. 이 시점에서, 모든 멤버는 오직 하나의 고유한 결과를 갖는다. 리더는 이 결과를 바탕으로 최종 답변을 제공할 것입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline You are the leader of a team of {member_nums} members. Your team will need to collaborate to solve a task. The rule is:\n' +
      '\n' +
      '1. 작업 설명 및 작업 목표를 아는 사람만 알 수 있습니다. 다른 구성원은 알지 못합니다.\n' +
      '\n' +
      '2. 그러나 그들은 답변을 포함할 수 있는 다른 문서들을 수신할 것이고, 당신은 그들에게 그들의 문서를 질의하기 위한 지시를 보내야 한다.\n' +
      '\n' +
      '3. 당신의 지시에는 과제에 대한 이해와 그들이 집중할 필요가 있는 것이 포함되어야 한다. 필요한 경우 지시사항에는 작업 목표가 명시적으로 포함될 수 있습니다.\n' +
      '\n' +
      '4. 마지막으로, 그들이 반환하는 질의 결과에 기초하여 작업을 완료해야 한다.\n' +
      '\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답만 주시고 다른 단어는 출력하지 마세요.\n' +
      '\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      '2002년 레샨이나 취안저우의 인구는 65만 8천 798명이었는가?\n' +
      '\n' +
      '**\\#구성원을 위한 명령어 생성 :**\n' +
      '\n' +
      '이제 모든 팀 구성원에 대한 지침을 생성해야 합니다. 특정 질문에 답하도록 요청하거나 각 문서에 따라 작업과 관련된 정보를 추출할 수 있습니다.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 6: 다중 문서 질문 답변 작업에 대한 리더 두 번째 입력에 대한 프롬프트 템플릿. #Task Description 및 #Task Objective의 콘텐츠는 사용자 입력으로부터 도출된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{227.6pt}} \\hline \\hline\n' +
      '**\\# Document:**\n' +
      '\n' +
      '{member_document} \\\\\n' +
      '**\\# Instruction:**\n' +
      '\n' +
      '2002년 레샨의 인구는 얼마였나요?\n' +
      '\n' +
      'You are an experienced writer; please summarize the content in the document related to the instructions in a <scratchpad> tag, then describe your response." Your output must following the JSON format: {["type": "response", "content": "your_response_content"}] \\\\ The "content" needs to be as concise as possible. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 다중 문서 질문 답변 작업에 대한 구성원 첫 번째 입력에 대한 프롬프트 템플릿.\n' +
      '\n' +
      '여기 모든 멤버들의 답변이 있습니다. 각 구성원은 문서의 서로 다른 세그먼트를 보고 이러한 세그먼트는 서로 교차하지 않습니다. 정답은 어느 하나 또는 여러 구성원의 응답에 나타날 수 있다.\n' +
      '\n' +
      '소수의 구성원이 질문과 관련된 정보를 찾는 반면 다수가 문서에 질문과 관련된 정보가 포함되어 있지 않다고 답하는 경우 관련 정보를 찾은 구성원의 답변에 주의를 기울여야 한다.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      '회원 1: 121,277\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답만 주시고 다른 단어는 출력하지 마세요.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      '2002년 레샨이나 취안저우의 인구는 65만 8천 798명이었는가?\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      '위의 정보를 바탕으로 과제목적을 해결할 수 있는지 판단해야 한다. 두 가지 선택권이 있어\n' +
      '\n' +
      '1. 구성원의 응답이 과제 목표를 해결할 수 없거나, 자신의 응답이 상충되는 답변을 포함하고 있는 경우, 다시 답변할 수 있도록 새로운 지침을 제공한다.\n' +
      '\n' +
      '2. 둘째, 과제목표를 해결할 수 있는 경우에는 가능한 한 하나의 구문을 사용하여 최대한 간결하게 최종 답변을 한다. 어떤 설명도 제공하지 마세요.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "답변", "content": "your_answer_content"}} 또는 {{"type": "instruction", "content": "your_instruction_content"}}}}}\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      '_"형": "답변", "내용": "f1ec3dd8-b6e7-4af5-a589-fda0b7693/93"_1ec3dd8-b6e7-4af5-a589-fda0b7693/93"\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline\n' +
      '**\\# Document:** \\\\ \\{member\\_document\\} \\\\ \\hline\n' +
      '**\\# Instruction:** \\\\ What was the population of Leshan in 2002? \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 다중 문서 질문 답변 작업에 대한 리더 세 번째 입력에 대한 프롬프트 템플릿.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline\n' +
      '**\\# Document:** \\\\ \\{member\\_document\\} \\\\ \\hline\n' +
      '**\\# Instruction:** \\\\ What was the population of Leshan in 2002? \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 다중 문서 질문 답변 작업에 대한 구성원 두 번째 입력에 대한 프롬프트 템플릿.\n' +
      '\n' +
      '여기 모든 멤버들의 답변이 있습니다. 각 구성원은 문서의 서로 다른 세그먼트를 보고 이러한 세그먼트는 서로 교차하지 않습니다. 정답은 어느 하나 또는 여러 구성원의 응답에 나타날 수 있다.\n' +
      '\n' +
      '소수의 구성원이 질문과 관련된 정보를 찾는 반면 다수가 문서에 질문과 관련된 정보가 포함되어 있지 않다고 답하는 경우 관련 정보를 찾은 구성원의 답변에 주의를 기울여야 한다.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      '회원 1: 658,798\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답만 주시고 다른 단어는 출력하지 마세요.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      '2002년 레샨이나 취안저우의 인구는 65만 8천 798명이었는가?\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      '위의 정보를 바탕으로 과제목적을 해결할 수 있는지 판단해야 한다. 두 가지 선택권이 있어\n' +
      '\n' +
      '1. 구성원의 응답이 과제 목표를 해결할 수 없거나, 자신의 응답이 상충되는 답변을 포함하고 있는 경우, 다시 답변할 수 있도록 새로운 지침을 제공한다.\n' +
      '\n' +
      '2. 둘째, 과제목표를 해결할 수 있는 경우에는 가능한 한 하나의 구문을 사용하여 최대한 간결하게 최종 답변을 한다. 어떤 설명도 제공하지 마세요.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "답변", "content": "your_answer_content"}} 또는 {{"type": "instruction", "content": "your_instruction_content"}}}}}\n' +
      '\n' +
      '### Math.Find\n' +
      '\n' +
      '4.1 가장 많은 숫자 찾기\n' +
      '\n' +
      '**Leader first input**\n' +
      '\n' +
      '먼저 리더는 <표 2>와 같이 과제설명에 따라 회원을 모집한다.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "member", "content": "Math member")_\n' +
      '\n' +
      '**Leader next input**\n' +
      '\n' +
      '지시선 다음 입력에 대한 프롬프트는 표 15에 나와 있습니다.\n' +
      '\n' +
      '#Task Description 및 #Task Objective의 콘텐츠는 사용자 입력으로부터 도출된다.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "instruction", "content": "You should answer only one number, no other words. 가장 많은 수의 list는 ")__\n' +
      '\n' +
      '**Member first input**\n' +
      '\n' +
      '회원 첫 번째 입력에 대한 프롬프트는 표 16에 나와 있다.\n' +
      '\n' +
      '#문서의 내용은 각 구성원에 대한 변형이 있는 완전한 컨텍스트의 일부입니다.\n' +
      '\n' +
      '#인스트럭션의 내용은 리더의 첫 번째 출력에서 비롯된다.\n' +
      '\n' +
      '**Member first output**\n' +
      '\n' +
      '각 구성원은 문서에 있는 숫자의 최대값을 반환합니다. 각 구성원의 결과를 기록하여 리더에게 전달한다.\n' +
      '\n' +
      '**Leader last input**\n' +
      '\n' +
      '리더 마지막 입력에 대한 프롬프트는 표 17에 나와 있습니다.\n' +
      '\n' +
      '리더는 모든 구성원의 출력에 따라 전체 문서에 대한 최대값을 찾습니다.\n' +
      '\n' +
      '**Leader last output**\n' +
      '\n' +
      '_("type": "answer", "content": "94")_{type": "answer", "content": "94")\n' +
      '\n' +
      '###### b.4.2 두 번째로 큰 숫자 찾기\n' +
      '\n' +
      '수학의 다른 작업에 대해.찾기, 프롬프트 템플릿은 동일하게 유지됩니다. 여기서 생략됩니다.\n' +
      '\n' +
      '**Leader first output**\n' +
      '\n' +
      '_("type": "instruction", "content": "you should answer only one number, no other words." 가장 많은 숫자와 두 번째로 많은 숫자\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l} \\hline \\hline Here are the responses from all the members. Each member sees different segments of a document, and these segments do not intersect with each other. The correct answer may appear in any one or several members’ responses. \\\\ Note that if a minority of members find information relevant to the question while the majority reply that the document does not contain information relevant to the question, you should pay attention to the replies from those members who found relevant information. \\\\\n' +
      '**\\# Member Response:**\n' +
      '\n' +
      '회원 1: 658,798 \\\\\n' +
      '**\\# Task Description:**\n' +
      '\n' +
      '주어진 구절에 따라 질문에 대답하십시오. 답만 주시고 다른 단어는 출력하지 마세요. \\\\\n' +
      '**\\# Task Objective:**\n' +
      '\n' +
      '2002년에 레샨이나 취안저우의 인구는 658,798명이었나요?\n' +
      '**\\# Determination:**\n' +
      '\n' +
      '위의 정보를 바탕으로 과제목적을 해결할 수 있는지 판단해야 한다. 두 가지 선택권이 있어\n' +
      '\n' +
      '1. 구성원의 응답이 과제 목표를 해결할 수 없거나, 자신의 응답이 상충되는 답변을 포함하고 있는 경우, 다시 답변할 수 있는 새로운 지침을 제공한다.\n' +
      '\n' +
      '2. 둘째, 과제목표를 해결할 수 있는 경우에는 가능한 한 하나의 구문을 사용하여 최대한 간결하게 최종 답변을 한다. 어떤 설명도 제공하지 마세요.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "답변", "content": "your_answer_content"}} 또는 {{"type": "instruction", "content": "your_instruction_content"}}}}}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 11: 다중 문서 질문 답변 작업에 대한 리더의 마지막 입력에 대한 프롬프트 템플릿.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '여기 모든 멤버들의 답변이 있습니다. 각 구성원은 문서의 서로 다른 세그먼트를 보고 이러한 세그먼트는 서로 교차하지 않습니다. 정답은 어느 하나 또는 여러 구성원의 응답에 나타날 수 있다.\n' +
      '\n' +
      '소수의 구성원이 질문과 관련된 정보를 찾는 반면 다수가 문서에 질문과 관련된 정보가 포함되어 있지 않다고 답하는 경우 관련 정보를 찾은 구성원의 답변에 주의를 기울여야 한다.\n' +
      '\n' +
      '**# Member Response:**\n' +
      '\n' +
      '멤버 1: flec3dd8-b6e7-4af5-a589-fda0b7693f93\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '사전 및 키에서 해당 값을 찾는 중입니다.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      'Key: "2b114db0-d87e-42d2-9b4c-0b1f115976ad"\n' +
      '\n' +
      '상기 지정된 키와 연관된 값은:\n' +
      '\n' +
      '**# Determination:**\n' +
      '\n' +
      '위의 정보를 바탕으로 과제목적을 해결할 수 있는지 판단해야 한다. 두 가지 선택권이 있어\n' +
      '\n' +
      '1. 구성원의 응답이 과제 목표를 해결할 수 없거나, 자신의 응답이 상충되는 답변을 포함하고 있는 경우, 다시 답변할 수 있도록 새로운 지침을 제공한다.\n' +
      '\n' +
      '2. 둘째, 과제목표를 해결할 수 있는 경우에는 가능한 한 하나의 구문을 사용하여 최대한 간결하게 최종 답변을 한다. 어떤 설명도 제공하지 마세요.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "답변", "content": "your_answer_content"}} 또는 {{"type": "instruction", "content": "your_instruction_content"}}}\n' +
      '\n' +
      '## 부록 C 추가 결과\n' +
      '\n' +
      '본 절에서는 본 논문의 본문에서 언급한 실험 결과를 보완한다.\n' +
      '\n' +
      '당신은 {멤버_넘즈} 멤버로 구성된 팀의 리더입니다. 작업을 해결하기 위해 팀이 협력해야 합니다. 상기 규칙은:\n' +
      '\n' +
      '1. 작업 설명 및 작업 목표를 아는 사람만 알 수 있습니다. 다른 구성원은 알지 못합니다.\n' +
      '\n' +
      '2. 그러나 그들은 답변을 포함할 수 있는 다른 문서들을 수신할 것이고, 당신은 그들에게 그들의 문서를 질의하기 위한 지시를 보내야 한다.\n' +
      '\n' +
      '3. 당신의 지시에는 과제에 대한 이해와 그들이 집중할 필요가 있는 것이 포함되어야 한다. 필요한 경우 지시사항에는 작업 목표가 명시적으로 포함될 수 있습니다.\n' +
      '\n' +
      '4. 마지막으로, 그들이 반환하는 질의 결과에 기초하여 작업을 완료해야 한다.\n' +
      '\n' +
      '**# Task Description:**\n' +
      '\n' +
      '긴 리스트에서 특수 정수를 찾는 중입니다.\n' +
      '\n' +
      '**# Task Objective:**\n' +
      '\n' +
      '당신은 다른 단어가 아닌 숫자 하나로 대답해야 한다. 상기 리스트의 가장 많은 수는:\n' +
      '\n' +
      '**#구성원을 위한 명령어 생성 :**\n' +
      '\n' +
      '이제 모든 팀 구성원에 대한 지침을 생성해야 합니다. 특정 질문에 답하도록 요청하거나 각 문서에 따라 작업과 관련된 정보를 추출할 수 있습니다.\n' +
      '\n' +
      '당신의 출력은 JSON 포맷을 따라야 한다: {{"type": "instruction", "content": "your_instruction_content"}}\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:19]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>