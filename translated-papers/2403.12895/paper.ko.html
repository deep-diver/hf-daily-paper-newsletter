<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# mPLUG-DocOwl 1.5: OCR이 없는 문서 이해를 위한 통합 구조 학습\n' +
      '\n' +
      '안원후원, 해양수원, 자보예원, 명연원\n' +
      '\n' +
      '**양장2, 보장1, 천리1, 지장1, 진진2, 페이황1, 징렌주1**\n' +
      '\n' +
      '1Alibaba Group\n' +
      '\n' +
      '중국 2렌민대학\n' +
      '\n' +
      '{huanwen.haw,shuofeng.xhy,ym119608}@alibaba-inc.com\n' +
      '\n' +
      'Corresponding authors\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '구조 정보는 문서, 표 및 차트와 같이 텍스트가 풍부한 이미지의 의미를 이해하는 데 매우 중요합니다. 시각 문서 이해를 위한 기존의 MLLM(Multimodal Large Language Model)은 텍스트 인식 능력을 갖추고 있지만 텍스트가 풍부한 문서 이미지에 대한 일반적인 구조 이해 능력이 부족하다. 본 연구에서는 시각 문서 이해에서 구조 정보의 중요성을 강조하고, MLLM의 성능을 높이기 위한 통합 구조 학습을 제안한다. 통합 구조 학습은 문서, 웹 페이지, 테이블, 차트 및 자연 이미지의 5개 도메인에 걸쳐 구조 인식 구문 분석 작업과 다중 분류 텍스트 현지화 작업을 포함한다. 구조 정보를 더 잘 인코딩하기 위해, 우리는 레이아웃 정보를 유지할 수 있을 뿐만 아니라 컨볼루션(convolution)을 통해 수평 인접 패치를 병합함으로써 시각적 특징의 길이를 줄일 수 있어 LLM이 고해상도 이미지를 보다 효율적으로 이해할 수 있는 간단하고 효과적인 비전 투 텍스트 모듈 H-Reducer를 설계한다. 또한, 공개적으로 사용 가능한 텍스트가 풍부한 이미지를 위해 구조 인식 텍스트 시퀀스와 다중 집합 텍스트 쌍 및 경계 상자를 구성하여 포괄적인 텍스트를 구축한다.\n' +
      '\n' +
      '그림 1: 비슷한 크기의 일반인과 비교하여 DocOwl 1.5는 10개의 시각적 문서 이해 벤치마크에서 최첨단 OCR이 없는 성능을 달성합니다.\n' +
      '\n' +
      'training set DocStruct4M to support structure learning. 마지막으로, 문서 도메인에서 상세한 설명 능력을 트리거하기 위해 작지만 고품질의 추론 튜닝 데이터세트 DocReason25K를 구성한다. 모델 DocOwl 1.5는 10개의 시각적 문서 이해 벤치마크에서 최첨단 성능을 달성하여 7B LLM으로 MLLM의 SOTA 성능을 5/10 벤치마크에서 10점 이상 향상시킵니다. 우리의 코드, 모델 및 데이터 세트는 [https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5)에서 공개적으로 사용할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대언어 모델(LLM)[5; 46; 48; 62]의 강력한 언어 이해와 생성 능력을 활용하여 최근 일부 작품[57; 58; 27; 26; 64; 24]에서는 일반적인 시각 및 언어 이해를 위한 멀티모달 대언어 모델(MLLM)을 개발하였다. 미리 훈련된 시각적 인코더(예: CLIP[36]의 ViT/L-14[12])와 LLM을 V2T(Vision-to-Text) 모듈과 정렬함으로써, 이러한 모델은 일반적인 이미지를 이해하는 데 유망한 성능을 나타낸다. 그러나, 그들은 여전히 문서, 웹 페이지, 표 및 차트와 같은 풍부한 텍스트 정보를 가진 이미지로 큰 도전에 직면해 있다[28]. 이는 주로 시각적 인코더와 V2T 모듈이 일반적인 이미지-텍스트 쌍에 대해 훈련되고 텍스트가 풍부한 이미지에서 텍스트 및 구조적 정보를 나타내도록 특별히 최적화되지 않기 때문이다.\n' +
      '\n' +
      '이미지의 텍스트 정보는 많은 시각적 구조로 나타나며, 일반 텍스트의 단순성을 표의 체계적인 그리드 레이아웃에 확장하고 그래픽의 스펙트럼을 통합한다.\n' +
      '\n' +
      '도 2: 문서(a), 표(b), 웹페이지(c), 인포그래픽(d) 및 차트(e-f)에 대한 시각적 문서 이해에서 구조 정보의 중요도에 대한 예시도.\n' +
      '\n' +
      ' 파이, 선 및 막대 차트 등의 표현입니다. 이러한 요소는 포스터, 송장, 인포그래픽, 과학 보고서, 학술 및 뉴스 웹사이트 등에 걸쳐 풍부한 정보 아키텍처를 반영하여 문서 및 웹 페이지의 프레임워크 내에서 개별적으로 나타나거나 복잡하게 얽힐 수 있다. 도 1에 도시된 바와 같다. 도 2를 참조하면, 기본적인 텍스트 내용 외에 구조 정보도 Visual Document Understanding[53; 18; 45; 23]에서 큰 역할을 하고 있다. LLM 디코더를 통해 일반 이미지를 이해하고 구조화된 텍스트를 이해하는 기본 능력으로 MLLM은 텍스트가 풍부한 이미지에 대한 통일된 구조 학습을 달성할 수 있는 잠재력을 가지고 있다. MLLM을 사용한 보다 나은 시각적 문서 이해를 위해 일부 작업[55; 56; 3; 13]은 텍스트 인식 능력을 강화하기 위해 텍스트 읽기 작업을 설계하려고 시도하지만 구조 이해도를 무시하거나 웹 페이지[23] 또는 문서[13]와 같이 텍스트가 풍부한 이미지의 제한된 영역만 다룬다. 본 연구에서는 먼저 문서, 웹 페이지, 테이블, 차트, 자연 이미지의 5개 도메인에 걸쳐 MLLM에 대한 텍스트가 풍부한 이미지에 대해 통일된 구조 학습을 수행하는 것을 제안한다.\n' +
      '\n' +
      '더 나은 구조적 이해를 위해 먼저 간단하고 효과적인 비전 투 텍스트 모듈, 즉 H-Reducer를 설계한다. 시각적 특징을 학습 가능한 질의와 융합하지만 공간 정보에 영향을 주는 Resampler[1] 또는 Q-former[24]와 달리 H-Reducer는 컨볼루션(convolution)을 통해 이웃한 시각적 특징을 축적하여 상대적인 위치 관계를 유지한다. 선형 레이어[27; 26]만 있는 V2T 모듈과 비교하여 훨씬 적은 시각적 특징을 생성하므로 LLM이 고해상도 문서 이미지를 이해하는 데 더 효율적입니다. 문서 이미지의 텍스트가 왼쪽에서 오른쪽으로 가장 많이 구성되어 있음을 고려하여 H-Reducer는 수평 수준에서 시각적 특징을 병합한다. 통합 구조 학습은 구조 인식 구문 분석 작업과 다중 분류 텍스트 현지화 작업으로 구성된다. 텍스트 콘텐츠의 구성을 학습하기 위해 전자는 주로 모델에게 문서나 웹 페이지의 구조를 나타내기 위해 라인 피드 및 스페이스를 사용하고 표와 차트의 구조를 나타내기 위해 확장된 마크다운 구문을 사용하는 것과 같이 이미지 내의 텍스트를 구조 인식 스타일로 파싱하도록 가르친다. 다중-입체 텍스트 로컬화 태스크는 시각적으로 위치된 텍스트와 이미지 내의 구체적인 위치를 상관시키는 능력을 더욱 향상시킨다. 통합된 구조 학습을 지원하기 위해 공개된 데이터 세트를 기반으로 구조 인식 시퀀스와 텍스트 및 바운딩 박스의 다중 분할 쌍을 구성하여 포괄적인 학습 세트 DocStruct4M을 신중하게 구축한다. DocOwl 1.5는 통합 구조 학습에서 시작하여 다운스트림 작업 중 다중 작업 튜닝이 뒤따르는 2단계 프레임워크로 훈련된다. 마지막으로 비주얼 문서 이해에서 MLLM의 추론 능력을 트리거하기 위해 고품질 명령어 튜닝 데이터세트 DocReason25K를 구성한다. DocReason25K와 다운스트림 데이터셋에 대한 합동 훈련을 수행함으로써, DocOwl 1.5-Chat well balance는 간단한 답변이나 상세한 설명을 제공한다.\n' +
      '\n' +
      '이 작업에 대한 우리의 기여는 4배입니다.\n' +
      '\n' +
      '* 우리는 먼저 MLLM을 위한 텍스트가 풍부한 이미지에 대한 통합 구조 학습을 제안하고 5개의 도메인에 걸쳐 구조 인식 구문 분석 작업과 다중 분류 텍스트 현지화 작업을 모두 설계한다. 통합 구조 학습을 지원하기 위해 포괄적인 데이터 세트 DocStruct4M이 세심하게 구축됩니다.\n' +
      '* 구조 학습을 위한 간단하고 효과적인 비전 투 텍스트 모듈을 설계하고 그 유효성을 검증하기 위해 광범위한 실험을 수행한다.\n' +
      '* 우리는 시각적 문서 이해에 대한 MLLM의 추론 능력을 트리거하기 위해 고품질 명령어 튜닝 세트를 구성한다.\n' +
      '* DocOwl 1.5와 DocOwl 1.5-Chat은 10개의 Visual Document Understanding 태스크에서 최첨단 OCR 프리 성능을 달성하여 유사 크기의 모델 중 5/10 태스크에서 10점 이상의 향상을 달성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**시각적 언어 이해(Visual-situated Language Understanding; 23; 56)라고도 하는 시각 문서 이해(Visual Document Understanding; DVDU)**는 풍부한 텍스트 정보로 이미지를 이해하는 것을 목표로 한다. 이러한 이미지는 문서[30; 31; 42; 41; 60], 표[34; 8; 63], 차트[29; 19; 32; 21; 44; 17], 자연 이미지[39; 40; 16]에서 웹페이지 스크린샷[43; 9]까지 다양하며, 텍스트와 시각적 객체의 다양한 구성이 풍부한 정보를 포함한다. 멀티모달 문서 이해 성능을 평가하기 위해, 태스크 포맷들은 저-레벨 인식, 예를 들어 정보 추출[42; 41], 시각적 질문 응답[30; 31; 34; 29; 43; 40], 이미지 캡션[39; 21; 44], 및 자연 언어 추론[8]과 같은 고-레벨 의미 이해들을 포함한다. 이미지 내의 텍스트를 인식하기 위해 기성 OCR 시스템에 의존하는지에 따라, 시각적 문서 이해를 위한 모델은 OCR 종속 모델[45; 53; 18; 54]과 OCR이 없는 모델[22; 23]로 분류될 수 있다. OCR 시스템에서 인식된 텍스트를 활용하기 위해 OCR 종속 모델은 항상 텍스트 및 시각적 입력을 정렬하도록 훈련된다. 예를 들어, UDOP[45]는 마스킹된 텍스트 및 이미지가 주어진 레이아웃 정보 및 입력으로서 보유된 텍스트를 복구하도록 미리 트레이닝된다. OCR이 없는 방법은 텍스트 인식에 대한 과제를 가지고 훈련하는 것이 필수적이다. 마운트[22]는 구조 정보를 무시하는 연속적인 텍스트 시퀀스를 출력하도록 텍스트 판독 작업을 설계한다. 구조 정보를 활용하기 위해, Pix2Struct[23]는 웹페이지 스크린샷에 대한 HTML DOM 트리를 생성하기 위해 스크린샷 파싱 태스크를 설계하지만 다른 유형의 이미지에 적용하기 어렵다. 이 연구에서는 먼저 모든 이미지 유형에 대한 통합 구조 학습을 제안하고 레이아웃 학습을 지원하기 위해 포괄적인 데이터 세트를 신중하게 구축한다.\n' +
      '\n' +
      '**멀티모달 대형 언어 모델**(MLLM)은 자연스러운 이미지에 대해 강한 시각 이해와 개방형 대화 능력을 보여주었다[57; 58; 64; 10; 3; 15; 59]. 그들은 간단한 선형 계층[27; 26] 또는 Q-전[24]/리샘플러[1]/추상[57; 58]과 같은 비젼-투-텍스트 모듈에 의해 빅 언어 모델(LLM)[46; 48; 2]과 함께 ViT[12; 36]과 학습 가능한 쿼리를 연결하는 아키텍처 패러다임을 따른다. MLLM이 풍부한 텍스트로 이미지를 이해할 수 있도록 하기 위해서는 고해상도 이미지를 인코딩하는 방법과 시각적으로 위치한 텍스트를 이해하는 방법의 두 가지 과제가 있다. 고해상도 이미지를 다루기 위해 대부분의 작업은 [3; 13]을 추가로 훈련하거나 [15] 고해상도 비전 인코더를 추가하도록 선택한다. UReader[56]는 먼저 저해상도 비전 인코더를 유지하고 형상 적응형 크롭 모듈을 사용하여 로우 이미지를 저해상도의 다수의 서브 이미지로 크롭할 것을 제안한다. 시각적 텍스트 이해도를 높이기 위해 일부 작업은 구조의 중요성을 고려하지 않고 왼쪽 상단에서 오른쪽 하단으로 텍스트를 읽는 작업[56; 3]을 설계한다. CogAgent[15] 및 DocPedia[13]는 텍스트 접지 작업을 통해 문서, 웹 페이지 및 자연 이미지에 대한 레이아웃 이해도를 더욱 강화해 보십시오. 그러나 전체적인 구조에 대한 이해는 무시되고 표와 차트는 다루지 않는다. 본 연구에서는 UReader를 이용하여 고해상도 영상을 처리한다. 구조 이해를 강화하기 위해 문서, 표, 차트, 웹 페이지 및 자연 이미지를 포함하는 모든 유형의 이미지에 대해 구조 인식 칭찬 및 다중 분류 텍스트 현지화 작업을 설계한다. 컨볼루션에 의한 시각적 특징의 공간 정보를 더 잘 유지하기 위한 비전 투 텍스트 아키텍처를 제안한다. 마지막으로, 통합 구조 학습을 지원하기 위해 포괄적인 학습 데이터셋 DocStruct4M을 구축하고 시각적 문서 이해 성능을 크게 향상시킨다.\n' +
      '\n' +
      '##3 닥올 1.5\n' +
      '\n' +
      'DocOwl 1.5는 멀티모달 대형 언어 모델의 전형적인 아키텍처를 따르며, 이는 시각적 인코더, 시각-텍스트 모듈, 및 디코더로서 대형 언어 모델로 구성된다. 고해상도의 텍스트가 풍부한 이미지에서 텍스트 및 레이아웃 정보를 더 잘 유지하기 위해, 우리는 수평 시각적 특징을 앙상블하기 위한 비전 투 텍스트 모듈로 H-Reducer를 설계한다. 도 1에 도시된 바와 같다. 3(a) 텍스트 인식 및 구조 이해 능력을 향상시키기 위해 먼저 모든 유형의 이미지에 대해 구조 인식 구문 분석 및 다중 분류 텍스트 현지화 작업을 통한 통합 구조 학습을 수행한다. 그런 다음 비주얼 문서 이해의 여러 다운스트림 작업에 대해 모델을 공동으로 조정한다.\n' +
      '\n' +
      '### Model Architecture\n' +
      '\n' +
      '**고해상도 이미지 인코딩.** 이전 작업 [22; 23; 56]에 의해 증명된 바와 같이, 고해상도 이미지를 인코딩하는 능력은 디코더가 문서 이미지로부터 풍부한 텍스트 정보를 사용할 수 있도록 보장하는 데 중요하다. 도 1에 도시된 바와 같다. 도 3(b)를 참조하면, UReader[56]에 이어 파라미터가 없는 Shape-adaptive Cropping Module을 사용하여 형상 가변 고해상도 이미지 \\(I\\)을 여러 개의 고정 크기 서브 이미지 \\((I_{1},I_{2},...,I_{C})\\으로 크롭한다. 여기서 \\(C\\)은 작물의 수이다. 전체적인 레이아웃 정보를 유지하기 위해 원시 이미지는 전역 이미지\\(I_{0}\\)로서 저해상도 이미지로 리사이징된다. 그리고, 트랜스포머 기반의 Visual Encoder에 의해 각 영상(I_{i}=(V_{1}^{1},v_{i}^{2},...,v_{i}^{L}),0\\leqi\\leq C\\)을 각각 (I_{0},I_{1},...,I_{C})의 영상(I_{i}\\)을 독립적으로 부호화하고, 여기서 \\(v_{i}^{j},1\\leqj\\leq L\\)은 각 영상에 대한 시각적 특징의 길이이다.\n' +
      '\n' +
      '**Spatial-aware Vision-to-Text Module: H-Reducer.** Multimodal Large Language Model을 위한 두 종류의 인기 있는 Vision-to-Text Module: MLP[27; 26; 64] 또는 학습 가능한 질의가 있는 교차 주의 모듈[57; 3; 1; 24]이 있다. 두 가지 모두 고해상도 텍스트가 풍부한 이미지를 표현하기에 적합하지 않다. 전자의 프로젝트는 언어 임베딩 공간에 시각적 특징을 완성한다. 문서 이미지 내의 모든 공간 정보를 유지하지만, 고해상도 이미지를 처리할 때 너무 긴 원시 시각적 특징의 시퀀스 길이를 유지한다. 예를 들어, ViT/L-14로 1,344x1,344 이미지를 인코딩하면 9,216개의 시각적 토큰이 생성된다. 교차 주의 모듈은 시각적 시퀀스의 길이를 학습 가능한 질의의 수까지 크게 줄일 수 있지만, 의미 융합 동안 공간 정보를 잃을 수 있다.\n' +
      '\n' +
      '본 논문에서는 시각 문서 이해를 위한 보다 적절한 시각-텍스트 모듈, 즉 H-Reducer를 설계하여 시각 시퀀스 길이를 줄일 뿐만 아니라 공간 정보를 유지할 수 있도록 하였다. 도 1에 도시된 바와 같다. 도 3의 (b)를 참조하면, H-Reducer는 시퀀스 길이를 줄이기 위한 convolution layer와 언어 임베딩 공간에 시각적 특징을 투영하기 위한 fully-connected layer로 구성된다. 문서 이미지 내의 대부분의 텍스트 정보는 왼쪽에서 오른쪽으로 배열되기 때문에, 수평 텍스트 정보는 보통 의미적으로 일관적이다. 따라서, 컨볼루션 계층에서의 커널 크기 및 스트라이드 크기는 1x4 내지 앙상블 수평 4 시각적 특징으로 설정된다. 출력 채널은 입력 채널 \\(D\\)과 동일하게 설정된다. 컨볼루션 계산은 다음과 같다:\n' +
      '\n' +
      '(v_{i}=(v_{i}^{1},v_{i}^{2},...,v_{i}^{L}) \\tag{1}\\] \\[\\overline{v}_{i}^{j}=f(v_{i}^{4j-3},v_{i}^{4j-2},v_{i}^{4j},v_{i}^{4j}),1\\leq j\\leq L/4,\\] (2) \\[\\overline{v}_{i}=(\\overline{v}_{i}^{1},\\overline{v}_{i}^{L/4},..., \\tag{3}\\}}}\n' +
      '\n' +
      '여기서 \\(f\\)은 다중 채널에서 커널 가중치를 갖는 내적을 나타낸다. 컨볼루션 레이어 후, 이미지 \\(I_{i}\\)의 시각적 특징은 \\(\\overline{V}_{i}\\)으로 변환되며, 그 특징 길이는 \\(L/4\\)이다.\n' +
      '\n' +
      '그런 다음 언어 임베딩 공간에 시각적 특징을 정렬하기 위해 완전히 연결된 레이어로 \\(\\overline{V}_{i}\\)을 \\(\\hat{V}_{i}=(\\hat{v}_{i}^{1},\\hat{v}_{i}^{2},...,\\hat{v}_{i}^{L/4})\\으로 전달한다.\n' +
      '\n' +
      '**LLM을 사용한 멀티모달 모델링.** MLLM의 디코더로서, 대형 언어 모델은 이미지의 시각적 특징과 언어 지시의 텍스트적 특징을 모두 이해해야 한다. 에 따라,\n' +
      '\n' +
      '도 3: DocOwl 1.5의 2단계 트레이닝 프레임워크(a) 및 전체 아키텍처(b). 글로벌 이미지 및 크롭된 이미지는 Visual Encoder 및 H-Reducer에 의해 독립적으로 처리된다. <rowx-coly>는 원본 이미지에서 크롭된 이미지의 위치가 \\(x^{th}\\) 행과 \\(y^{th}\\) 열임을 나타내는 특수 텍스트 토큰이다.\n' +
      '\n' +
      'mPLUG-Owl2[58], 우리는 시각적 입력과 텍스트 입력을 더 잘 구별하기 위해 LLM에 Modality-adaptive Module(MAM)을 적용한다. 자기 주의 동안, MAM은 시각적 특징 및 텍스트 특징에 대한 키/값 투영을 별도로 수행하기 위해 두 세트의 선형 투영 레이어를 활용한다. LLM이 여러 개의 크롭된 하위 이미지를 상호 연관시키는 것을 돕기 위해 UReader[56]은 원시 이미지의 행 및 열 위치를 나타내기 위해 학습 가능한 크롭 위치 임베딩을 설계한다. 이 연구에서는 각 크롭 이미지의 시각적 특징 앞에 특수 텍스트 토큰 \'<row_col>>을 추가하는데, 여기서 \\(x\\)과 \\(y\\)은 각각 행과 열 인덱스를 나타낸다. 글로벌 이미지에 대해, 텍스트 지시자 토큰은 \'<global_img>\'이다. 이 설계는 추가 파라미터를 도입할 필요가 없고 LLM 디코더에 보다 친근하다. 우리의 실험은 그것이 작물 위치 임베딩과 유사한 효과를 달성한다는 것을 검증한다. 전체적으로, LLM의 디코딩은 다음과 같다:\n' +
      '\n' +
      '\\[Y=\\mathrm{LLM}([\\mathrm{T}_{0};\\hat{\\mathrm{V}}_{0},\\mathrm{T}_{1};\\hat{\\mathrm{V}}_{1},...,\\mathrm{T}_{\\mathrm{C};\\hat{\\mathrm{V}}_{\\mathrm{C};\\mathrm{X}]) \\tag{4}\\tag{1},...,\\mathrm{V}_{\\mathrm{C};\\hat{\\mathrm{V}}_{\\mathrm{C};\\mathrm{X}}}\\tag{4}\\tag{1};\\hat{\\mathrm{V}}_{0};\\hat{\\mathrm{V}}_{0};\\hat{\\mathrm{V}}_{0};\\hat{\\mathrm{V}}_{1},...,\\mathrm{V}}_{\\mathrm{C};\\hat{\\math\n' +
      '\n' +
      '여기서 \\([;]\\)은 연결 연산을 의미하고, \\(C\\)은 이미지의 크롭 넘버, \\(T_{j},0\\leq j\\leq C\\)은 전역 이미지 또는 크롭 이미지의 위치에 대한 특수 텍스트 지시자의 텍스트 임베딩, \\(\\hat{V}_{j}\\)은 전역 또는 크롭 이미지의 시각적 특징, \\(X\\)은 명령어의 텍스트 임베딩, \\(Y\\)은 예측 답이다.\n' +
      '\n' +
      '통합구조학습\n' +
      '\n' +
      '대부분의 멀티모달 대형 언어 모델[27; 58; 50]은 개념 캡션[7], LAION[37] 및 COYO[6]과 같은 시각적 인코더를 LLM과 정렬하기 위해 자연 이미지의 이미지-텍스트 쌍으로 훈련된다. 이러한 모델을 초기화하는 것은 얕은 텍스트 인식 능력을 계승할 수 있지만, 다양한 텍스트가 풍부한 이미지에서 복잡한 텍스트 및 구조적 정보를 이해하는 것과는 거리가 있다. 본 연구에서는 MLLM의 포괄적인 문서 이해 능력을 강화하기 위해 자연 이미지, 문서, 표, 차트 및 웹 페이지를 포함한 5개 도메인에 걸쳐 통합 구조 학습을 설계한다. 그림 4와 같이 구조 인식 구문 분석 작업과 다중 분류 텍스트 현지화 작업을 모두 포함한다.\n' +
      '\n' +
      '**Document Parsing.** 구조 정보를 나타내기 위해, Pix2Struct[23]은 HTML 소스 코드들에 기초하여 구축되고 문서들의 다른 포맷들 또는 웹페이지 스크린샷들, 예를 들어 PDF에 대해 이용가능하지 않은, 응축된 HTML DOM 트리들을 갖는 웹페이지 스크린샷들을 파싱한다. 문서나 웹 페이지에서는 텍스트 간의 가로 및 세로 거리가 주요 레이아웃 정보를 구성한다. 따라서 구조 인식 파싱 작업을 대부분의 문서 및 웹 페이지에 적용할 수 있도록 하기 위해\n' +
      '\n' +
      '그림 4: DocOwl 1.5의 통합구조학습 예시도.\n' +
      '\n' +
      '텍스트 시퀀스에 다른 선과 수평 거리를 나타내기 위해 추가 선 피드(\\({}^{\\star}\\backslash n^{\\star}\\))와 공간을 추가하기로 선택한다. 수평 거리가 클수록 공간 문자가 많아집니다.\n' +
      '\n' +
      '문서 파싱 작업을 지원하기 위해 CCpdf[47], RVL-CDIP[14], VisualMRC[43] 및 DUE-Benchmark[4](DocVQA[30], InfoVQA[31], DeepForm[42], KLC[41], WTQ[34], TabFact[8])에 캡슐화된 데이터 세트를 선택한다. CCpdf[47]은 Common Cramwl2의 웹 페이지에 구축된 다중 언어 PDF 데이터 세트이며 산업, 학술 및 의료와 같은 다양한 문서 영역을 포함한다. 이 작업에서는 주로 다른 언어로 탐지된 영어 문서 이해 및 드롭 PDF에 중점을 둡니다. RVL-CDIP는 \'편지\', \'이메일\', \'과학적 보고서\'와 같은 16개의 산업 문서를 포함하고 있으며, 또한 \'필기\', \'형식\'과 같은 텍스트를 뒤집거나 흐리게 하는 일부 범주를 제거한다. DUE-벤치마크는 테이블, 그래프, 목록 및 인포그래픽을 특징으로 하는 다양한 문서 도메인 및 레이아웃에 걸쳐 사용 가능하고 재구성된 데이터 세트의 집합이다. VisualMRC는 35개 웹사이트에 걸친 웹페이지 스크린샷 데이터세트이다. VisualMRC의 OCR 주석은 로컬 영역과 정렬되므로 이 구문 분석 작업에 대한 입력으로 스크린샷의 작물을 활용하기 위해 이를 따른다. CCpdf 및 DUE-Benchmark의 경우 PDF 파싱 도구 pdfplumber3를 직접 사용하여 PDF 페이지를 입력으로 하는 구조 인식 텍스트 시퀀스를 생성할 수 있다. RVL-CDIP 및 VisualMRC의 경우 PDF 파일이 없으며 텍스트 경계 상자의 주석만 있다. 대안으로서 LATIN-Prompt[51]과 유사하게 경계 상자의 수평 및 수직 거리를 계산하고 비교하여 라인 피드 및 공간을 삽입한다. 또한, 너무 많은 빈칸 문자를 피하기 위해 최대 연속 빈칸 수를 4개로 제한함으로써, 구조 인식 텍스트 시퀀스를 pdfumber와 같은 스타일로 구성할 수 있다.\n' +
      '\n' +
      '각주 2: [https://commoncrawl.org](https://commoncrawl.org)\n' +
      '\n' +
      '각주 3: [https://github.com/jsvine/pdfplumber](https://github.com/jsvine/pdfplumber)\n' +
      '\n' +
      '**표 파싱.** 문서 또는 웹 페이지와 달리, 테이블은 보다 표준화된 방식으로 구조화되며, 여기서 행 및 열 대응은 키-값 쌍을 나타낸다. HTML과 Markdown 코드는 주로 테이블을 표현하기 위해 사용되는 두 종류의 텍스트 시퀀스이다. HTML 코드는 여러 행과 격자에 걸쳐 있는 셀이 있거나 없는 모든 종류의 테이블을 나타낼 수 있지만, 쌍을 이루는 레이블(예: \'<tr></tr>\' 및 \'<td></td>\')이 너무 많아 텍스트 시퀀스가 너무 길어진다. 마크다운 코드는 간결한 텍스트 시퀀스를 가진 테이블을 나타낼 수 있지만, 여러 행과 열에 걸쳐 있는 셀을 나타낼 수 없다. 간결한 텍스트 시퀀스로 모든 테이블을 표현하기 위해 마크다운의 주요 문법을 따라 \'|\'와 라인 피드(\\({}^{\\star}\\backslash n^{\\star}\\))로 테이블 구조를 표현한다. 여러 행과 열에 걸쳐 있는 셀을 나타내기 위해 그림 4와 같이 값 앞에 특수 텍스트 토큰 \'<COLSPAN=x>\'과 \'<ROWSPAN=y>\'를 추가한다.\n' +
      '\n' +
      '구조 인식 테이블 파싱 작업을 수행하기 위해 TURL[11]과 PubTabNet[63]을 선택하며, 여기서 테이블은 위키피디아 페이지와 과학 기사로부터 각각 수집된다. 행과 열에 걸쳐 셀이 없으면 TURL의 테이블은 마크다운 코드로 직접 표현될 수 있다. TURL에서 테이블 이미지가 부족하기 때문에, 우리는 테이블을 HTML 코드로 옮기고 배경 색상과 폰트 크기의 변화가 있는 테이블 이미지를 렌더링한다. PubTabNet은 테이블 이미지와 HTML 코드 쌍을 포함한다. HTML 코드를 Markdown 스타일로 변환하고 \'<td>\' 라벨에 속성 \'rowspan=x\' 또는 \'colspan=y\'가 설정된 경우 값 앞에 \'<ROWSPAN=x>\' 또는 \'<COLSPAN=y>\'를 추가한다.\n' +
      '\n' +
      '**차트 파싱.** 문서 및 표와 달리 텍스트를 읽기 순서로 정리하는 것은 차트의 구조를 나타낼 수 없다. 차트가 테이블의 시각화 형태라는 점을 고려할 때, 차트를 테이블로 파싱하는 것은 차트의 수학적 특성을 가장 잘 유지할 수 있다. 이를 위해서는 모형에서 차트의 구조와 x/y 축의 정렬을 이해해야 합니다. 또한 표 파싱 작업과 일관성을 유지하기 위해 마크다운 코드를 사용하여 그림 4와 같이 차트의 데이터 테이블을 나타낸다.\n' +
      '\n' +
      '구조 인식 차트 파싱 작업을 지원하기 위해 PlotQA[32], FigureQA[20], DVQA[19], ChartQA[29]를 채택한다. 이러한 데이터 세트는 합성 [20, 19] 데이터와 실제 소스 [32, 29] 데이터의 차트를 모두 포함합니다. 차트 유형에는 수직 막대, 수평 막대, 선, 점선 및 파이 차트가 있습니다. 차트의 소스 데이터는 JSON[32, 20, 32] 또는 CSV 포맷[29]으로 제공되며, 둘 다 편리하게 마크다운 코드로 변환될 수 있다. 그러나 일부 원시 값은 차트에 나타낼 수 있는 유효 자릿수가 너무 많기 때문에 구문 분석을 위한 표준 답변으로 적합하지 않다. 따라서 값 추정의 어려움을 줄이고 모델을 구조적 이해에 더 중점을 두기 위해 모든 값에 대해 4자리 수를 유지한다.\n' +
      '\n' +
      '**자연 이미지 파싱.** 위에서 언급한 텍스트-지배적 이미지와 상당히 다른, 자연 이미지의 의미론은 자연 객체와 장면 텍스트의 결합이다. 따라서, 장면 텍스트를 정리하고 주요 이미지 내용을 언급하기 위해서는 자연스러운 이미지를 파싱하는 것이 필요하다. 객체와 장면 텍스트 사이의 관계를 설명하기 위해 수동으로 주석을 달는 캡션은 노동 및 재정 집약적이다. TAP[54]와 마찬가지로 일반 캡션을 OCR 텍스트와 연결하여 대상 구문 분석 시퀀스를 형성한다.\n' +
      '\n' +
      '우리는 자연 이미지 파싱 작업을 지원하기 위해 OCR-CC[54]를 활용한다. OCR-CC는 Microsoft Azure OCR 시스템에 의해 검출된 장면 텍스트들을 갖는 이미지들을 포함하는 개념 캡션[38]의 서브세트이다.\n' +
      '\n' +
      '**Multi-grained Text Localization.** 일반적인 이미지 이해에 대한 이전 작업[52; 49; 35]에서 증명된 바와 같이, 의미적 이해와 객체 접지 작업은 단일 모델로 잘 통일될 수 있다. 시각 문서 이해를 위해 구조 인식 구문 분석 작업은 주로 전체 구조에 따라 텍스트를 구성하는 데 중점을 두는 반면 특정 텍스트와 로컬 위치 간의 대응은 무시한다. 이미지에서 텍스트와 구체적인 위치를 연관시키는 것은 시각 문서에 대한 또 다른 기본적인 구조 이해 능력이다. 텍스트 위치 학습을 지원하기 위해, 우리는 두 가지 대칭 작업, 즉 다중 결정 텍스트 접지와 다중 결정 텍스트 인식을 설계한다. 전자는 시각에 입각한 텍스트가 주어졌을 때 경계 상자를 예측하는 것을 목표로 하는 반면 후자는 반대로 한다. 이 두 가지 작업에 대해 단어, 구, 선 및 블록의 네 가지 세분도를 설정했다. \'단어\'는 바운딩 박스의 가장 작은 입도로, 오직 1개의 단어만을 지칭한다. 단어가 가시적이고 정답이 고유하다는 것을 보장하기 위해, 너무 작은 단어(정규화된 영역 < 0.001)와 동일한 이미지에 여러 번 나타나는 단어는 후보에서 제외된다. \'선\'은 수직거리로 가로로 평행하다고 판단되는 텍스트로 구성되며, \'구\'는 같은 선 내에서 인접한 여러 단어로 구성된다. \'블록\'은 전체 라인의 2에서 절반에 이르는 다수의 연속적인 라인의 조합이다. 단어-레벨 및 구-레벨 질문 답변의 텍스트 시퀀스는 다른 둘보다 훨씬 짧다. 따라서 지역화를 보다 효율적으로 학습하기 위해 각 단어 수준 또는 구 수준 샘플은 동일한 이미지에 대해 최대 5개의 질문-답변 쌍으로 구성된다. 경계 상자의 표현은 정규화된 경계 상자의 각 연속 값을 0에서 999까지의 이산 위치 토큰으로 전달한다.\n' +
      '\n' +
      '경계 상자 주석은 다중 세그먼트 텍스트 위치 지정 작업에 대한 샘플을 구성하는 데 필요합니다. 따라서 문서, 표, 차트, 웹페이지 및 자연 이미지의 도메인에 걸쳐 DocVQA, InfoVQA, WTQ, TabFact, DeepForm, KLC, ChartQA, VisualMRC 및 TextVQA[40]을 이 작업에 사용한다.\n' +
      '\n' +
      '전반적으로 텍스트가 풍부한 이미지에 대한 통합 구조 학습을 지원하기 위해 공개적으로 사용 가능한 데이터 세트의 여러 훈련 세트를 앙상블하고 구조 인식 텍스트 시퀀스 또는 텍스트 위치 쌍을 대상으로 구성하여 DocStruct4M 데이터 세트를 구축한다. 각 과제에 대한 지시의 형태는 모델의 일반적인 지시-추종 능력을 개발하기 위해 매우 다양하다. 도. 도 5는 DocStruct4M의 상세 통계를 나타낸다.\n' +
      '\n' +
      '### Multi-task Fine-tuning\n' +
      '\n' +
      '통합 구조 학습을 통해 모델은 다양한 문서 이미지의 구조를 잘 이해할 수 있지만 정보와 같은 다양한 유형의 작업을 수행하기 위한 사용자의 지시를 따를 수 없다.\n' +
      '\n' +
      '도 5: DocStruct4M의 상세 통계.\n' +
      '\n' +
      '추출 또는 이미지 캡션. 따라서, 우리는 UReader로 시각 문서 이해의 일반주의자를 훈련시키기 위해 다중 작업 미세 조정을 추가로 수행한다[56].\n' +
      '\n' +
      '### Training Paradigm\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 3(a), DocOwl 1.5는 2단계 프레임워크로 훈련된다. LLM은 구조화된 텍스트에 대한 이해 능력이 강하다는 점을 고려할 때 [51; 61] 시각 문서 이해에서 MLLM의 주요 한계는 시각 기반 텍스트 및 구조 정보에 대한 시각 인코더 및 시각 대 텍스트 모듈의 표현 능력이라고 주장한다. 따라서, 통합 구조 학습 동안 LLM 매개변수를 동결하고 비주얼 인코더 및 H-리듀서를 조정한다. 또한 MAM은 LLM이 이미지와 구문 분석된 시각적 특징과 텍스트를 더 잘 구별할 수 있도록 최적화된다. Multi-task Fine-tuning 단계에서는 주로 사용자의 지시에 따라 1단계에서 습득한 시각적 상황 텍스트와 구조 이해 능력을 기반으로 답변을 하는 방법을 학습한다. 따라서 Visual Encoder는 동결되고 다른 모듈들은 튜닝된다.\n' +
      '\n' +
      '##4 닥올 1.5-챗\n' +
      '\n' +
      '기존의 벤치마크는 주로 간단한 문구로 질문에 답하고 상세한 설명을 소홀히 하여 문서 이해 능력을 평가한다. 본 연구에서는 시각적 문서 이해에 대한 대규모 언어 모델의 강력한 언어 추론 능력을 더 잘 활용하기 위해 텍스트가 풍부한 이미지 이해, 즉 DocReason25K에 대한 상세한 설명과 함께 작은 명령어 조정 세트를 구축한다. DocVQA[30], InfoVQA[31], WTQ[34], VisualMRC[43], ChartQA[29], TextVQA[40]의 원시 질문을 바탕으로 ChatGPT4로 상세한 설명을 수집한다. 텍스트 내용은 문서, 테이블 또는 웹페이지 스크린샷에 대한 주요 정보이다. 따라서 DocVQA, InfoVQA, WTQ, VisualMRC의 경우 gpt-3.5-turbo-0301의 입력으로 이미지의 구조 인식 텍스트 시퀀스를 취하고 간단한 답변과 자세한 설명으로 질문에 답하도록 프롬프트한다. ChartQA와 TextVQA는 이미지를 입력으로 하고 gpt-4-vision-preview를 활용하여 자세한 설명과 함께 질문에 답한다. ChartGPT가 잘못 대답하는 샘플을 걸러내기 위해 우리는 gpt-3.5-터보-0301을 추가로 자극하여 ChartGPT가 제공한 답변이 간결한 인간 주석이 달린 지상 진실 답변과 일치하는지 여부를 판단한다. 벤치마크 데이터 세트의 원시 질문과 비교하여 DocReason25K의 질문에는 \'자세한 설명과 함께 질문에 답하라\'라는 프롬프트가 추가된다. DocReason25K의 상세한 통계는 표 1에 제시되어 있다. DocOwl 1.5-Chat은 DocReason25K와 다운스트림 데이터 세트를 결합하고 통합 구조 학습 후 다중 작업 조정을 수행하여 훈련된다.\n' +
      '\n' +
      '각주 4: [https://openai.com/chatgpt](https://openai.com/chatgpt)\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'DocOwl 1.5는 mPLUG-Owl2[58]에서 초기화되며, ViT/L-14[12]를 Visual Encoder로 사용하고 Modality Adaptive Module을 언어 디코더로 사용하는 7B Large Language Model을 사용한다. 종횡비 및 해상도에 따라, 각각의 이미지는 448x448의 고정된 해상도로 최대 9개의 서브-이미지로 크롭된다. 각각의 서브-이미지는 ViT/L-14에 의해 1,024개의 특징들로 인코딩된 후 H-Reducer에 의해 256개의 특징들로 감소된다. 이 모델은 DocStruct4M에서 12,000번의 반복으로 학습되며 학습률과 배치 크기는 1e-4와 1,024로 설정되며 약 128 A100일이다. 다중 작업 미세 조정 동안, 모델은 배치 크기가 256으로 설정되고 학습 속도가 2e-5로 설정된 6,500번의 반복에 대해 트레이닝되며, 이는 약 24 A100일이 더 소요된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c|c} \\hline \\hline  & DocVQA & InfoVQA & WTQ & VisualMRC & ChartQA & TextVQA & ALL \\\\ \\hline Image & 1,491 & 1,614 & 850 & 1,927 & 1,252 & 1,612 & 8,746 \\\\ Sample & 5,119 & 5,421 & 5,994 & 5,263 & 1,827 & 2,253 & 25,877 \\\\ Avg Length & 79.2 & 95.4 & 77.7 & 103.4 & 106.9 & 88.0 & 89.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: DocReason25K의 상세 통계량. Avg Length는 답변의 평균 토큰 길이를 의미한다.\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '텍스트가 풍부한 10개의 이미지 벤치마크(DocVQA[30], InfoVQA[31], DeepForm[42], KLC[41]), 테이블(WTQ[34], TabFact[8]), 차트(ChartQA[29], 자연 이미지(TextVQA[40], TextCaps[39]) 및 웹 페이지 스크린샷(VisualMRC[43])을 대상으로 Visual Document Understanding 성능을 평가한다. 우리는 DocOwl 1.5를 텍스트를 인식하기 위해 적응된 멀티모달 대형 언어 모델과 문서 이해만을 위해 훈련된 훨씬 더 작은 모델을 모두 포함하여 최첨단 OCR이 없는 모델과 비교한다. 모델 설정의 상세한 비교는 표 2에서 찾을 수 있다. 표 3에 나타난 바와 같이, 7B 이상의 파라미터를 갖는 이전의 MLLM들은 1B 미만의 파라미터를 갖는 도메인-특정 모델들을 저성능으로 하여, 문서 이해는 여전히 기존의 MLLM들에 대한 단점임을 보여준다. DocOwl 1.5는 10개의 벤치마크 모두에서 유사한 크기의 도메인 특정 모델과 MLLM 모두를 능가합니다. 이는 DocOwl 1.5가 시각적 질문 응답, 정보 검색, 자연어 추론 및 이미지 캡션 작업을 포함하는 5개 도메인에 걸쳐 시각적 문서 이해에 훨씬 더 강하다는 것을 입증한다. 게다가, 훨씬 적은 부자연스러운 데이터(3M vs 9M) 및 파라미터(8.1B vs 17.3B)로, DocOwl 1.5는 InfoVQA 및 ChartQA에서 CogAgent[15]보다 우수하고, DocVQA에서 유사한 성능을 달성한다. 이는 DocStruct4M을 사용한 통합 구조 학습이 인쇄 텍스트 인식 및 문서 분석 방법을 학습하는 데 더 효율적임을 시사한다. 그러나 본 논문에서 제안한 모델은 TextVQA에서 CogAgent의 성능을 여전히 떨어뜨리며, 이는 장면 텍스트 인식 능력과 자연 객체에 대한 일반적인 지식을 필요로 한다. 주요한 이유는 장면 텍스트가 인쇄된 텍스트보다 모양이 더 다양하기 때문이며 CogAgent는 DocStruct4M의 자연 이미지(1M)보다 훨씬 많은 LAION-2B[37]와 COYO-700M[6]의 98M 장면 텍스트 인식 샘플에 대해 훈련된다. 이 작업에서는 주로 시각 문서의 통일된 구조 이해도를 향상시키는 데 중점을 두고 자연 장면에 대한 데이터를 향후 작업으로 추가 확장한다. 마지막으로, DocOwl 1.5-Chat은 또한 상세한 설명의 프롬프트를 제거함으로써 이러한 간결 응답 벤치마크에 대해 평가될 수 있다. DocOwl 1.5보다 비슷하거나 약간 더 나은 성능을 달성하여 소량의 상세한 설명 데이터가 모델이 텍스트가 풍부한 이미지의 의미를 이해하는 데 더 도움이 될 수 있음을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c|c c|c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Init**} & \\multirow{2}{*}{**Resolution**} & \\multicolumn{5}{c}{**OCR Learning**} \\\\  & & & **Text** & **Bbox** & **Size** & \\multicolumn{2}{c}{**Domain**} & **Open** \\\\ \\hline Donut [22] & - & 2560x1920 & ✓ & \\(\\times\\) & 13M & Synthetic, Doc & ✓ \\\\ Pix2Struct [23] & - & 21\\({}^{9}\\)(shape variable) & ✓ & \\(\\times\\) & 80M & Web & \\(\\times\\) \\\\ QwenVL [3] & - & 448x448 & ✓ & \\(\\times\\) & 24.8M & Synthetic, Doc, Web & \\(\\times\\) \\\\ Monkey [25] & QwenVL [3] & 396x896 & \\(\\times\\) & \\(\\times\\) & - & - & - \\\\ URReader [56] & Owl [57] & 224x224(20 crops) & ✓ & \\(\\times\\) & 0.1M & Doc, Table, Chart, Web, Natural & ✓ \\\\ DocPedia [13] & - & 2560x2560 & ✓ & ✓ & 0.9M & Doc & \\(\\times\\) \\\\ CogAgent [15] & CogVLM [50] & 1120x1120 & ✓ & ✓ & 107M & Synthetic, Nature, Doc, Web & \\(\\times\\) \\\\ \\hline DocOwl 1.5 & Owl2 [58] & 448x448(x9 crops) & ✓ & ✓ & 4M & Doc, Table, Chart, Web, Natural & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: OCR이 없는 시각적 문서 이해 모델의 다른 설정. \'오픈\'은 모든 OCR 학습 데이터가 오픈 소스인지 여부를 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c|c c|c c|c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Size**} & \\multirow{2}{*}{**Doc**} & \\multirow{2}{*}{**Info**} & \\multirow{2}{*}{**Deep**} & \\multirow{2}{*}{**KLC**} & \\multirow{2}{*}{**WTQ**} & \\multicolumn{2}{c|}{**Tab**} & \\multirow{2}{*}{**Chart**} & \\multirow{2}{*}{**Text**} & \\multirow{2}{*}{**Visual**} \\\\  & & **VQA** & & & & & & & & & & \\\\ \\hline Dessurt\\({}^{*}\\) & \\textless{}1B & 63.2 & - & - & - & - & - & - & - & - & - \\\\ Donut\\({}^{*}\\) & \\textless{}1B & 67.5 & 11.6 & 61.6 & 30.0 & 18.8 & 54.6 & 41.8 & 43.5 & 74.4 & 93.91 \\\\ Pix2Struct\\({}^{*}_{base}\\) & \\textless{}1B & 72.1 & 38.2 & - & - & - & 56.0 & - & 88.0 & - \\\\ Pix2Struct\\({}^{*}_{large}\\) & 1.3B & 76.6 & 40.0 & - & - & - & - & 58.6 & - & 95.5 & - \\\\ \\hline DocPedia & 7.0B & 47.1 & 15.2 & - & - & - & - & 46.9 & 60.2 & - & - \\\\ DocOwl & 7.1B & 62.2 & 38.2 & 42.6 & 30.3 & 26.9 & 60.2 & 57.4 & 52.6 & 111.9 & 188.8 \\\\ OwenVL & 9.6B & 65.1 & 35.4 & - & - & - & - & 65.7 & 63.8 & - & - \\\\ URReader & 7.1B & 65.4 & 42.2 & 49.5 & 32.8 & 29.4 & 67.6 & 59.3 & 57.6 & 118.4 & 221.7 \\\\ Monkey & 9.8B & 66.5 & 36.1 & 40.6 & 32.8 & 25.3 & - & - & 67.6 & 93.2 & - \\\\ CogAgent & 17.3B & 81.6 & 44.5 & - & - & - & - & 68.4 & **76.1** & - & - \\\\ \\hline DocOwl-1.5 & 8.1B & 81.6 & 50.4 & 68.8 & 37.9 & 39.8 & **80.4** & **70.5** & 68.8 & **132.0** & 239.5 \\\\ DocOwl-1.5-Chat & 8.1B & **82.2** & **50.7** & **68.8** & **38.7** & **40.6** & 80.2 & 70.2 & 68.6 & 131.6 & **246.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 다양한 유형의 텍스트가 풍부한 이미지 이해 작업에 대한 OCR이 없는 방법과의 비교. 위첨자 \'\\(*\\)\'는 일반주의자가 아닌 각 다운스트림 작업에 대해 별도로 미세 조정된 모델을 의미한다. \\(underline\\)은 \\(\\textless{}10\\text{B}\\)의 매개변수를 갖는 모델들 중에서 가장 좋은 성능을 의미한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '표 4에 나타난 바와 같이, 우리는 H-리듀서와 통합 구조 학습의 효과를 검증하기 위해 포괄적인 절제 연구를 추가로 수행한다.\n' +
      '\n' +
      '첫째, 보다 강력한 일반 MLLM에서 초기화하는 것은 텍스트가 풍부한 이미지(r2 vs r1)에서 더 나은 성능을 제공하여 일반적인 시각 및 언어 지식이 시각적 문서 이해에 도움이 된다는 것을 보여준다. 멀티-태스크 미세-튜닝 동안 비주얼 인코더를 튜닝하는 것은 문서 이해 성능(r3 vs r2)을 상당히 향상시킨다. 이는 문서 이미지의 시각적 표현이 MLLM의 주요 단점일 수 있음을 시사하며, 시각적인 위치에 있는 텍스트와 구조에 대한 시각적 인코더의 표현 능력을 향상시키기 위해 통합 구조 학습을 설계하도록 영감을 준다.\n' +
      '\n' +
      '**H-Reducer의 효과** Shape-adaptive Cropping Module 사용 시 MLLM에서 지원하는 이미지 해상도는 각 작물의 작부수와 기본 해상도의 곱이다. 추상화를 시각-텍스트 모듈로 사용하면, 자르기 수를 줄이면 문서에 대한 명백한 성능 감소(r4 vs r3)가 발생한다. 그러나, 자르기 수가 작을수록 H-Reducer가 Abstractor(r5 vs r3)보다 우수한 성능을 보여, 기존 벤치마크에서 허용 가능한 해상도인 \\(448^{2}\\times 9\\approx 2^{21}\\)이며, 시각 및 언어 특징 정렬 시 풍부한 텍스트 정보를 유지하는 데 H-Reducer가 더 강하다는 것을 보여준다. 또한, 컨볼루션 레이어에서 병합 모양에 대한 다른 설정을 추가로 비교한다. 병합된 토큰의 수가 동일한 경우, 1x4 병합 형태를 갖는 모델이 문서 및 테이블 데이터 세트에서 2x2 병합 형태를 갖는 모델보다 더 나은 성능을 달성하지만 차트 이해에서는 약간 더 나쁜 성능을 달성한다(r6 vs r5). 이는 차트의 의미 구조가 훨씬 유연하면서 문서와 표가 주로 텍스트를 좌우 순서로 정리한다는 상식과 일치한다. 정사각형 병합 모양은 막대, 선 또는 파이의 형태로 시각적 특징을 인코딩하는 데 더 적합하고 1x4 병합 모양은 일반적인 문서 이해에 더 적합하다. r7-r9에 도시된 바와 같이, 1x4 병합 형상을 수평 및 수직으로 추가로 확장하는 것은 시각적 특징의 길이를 감소시키지만 성능 저하를 초래한다. 텍스트가 풍부한 모든 이미지에 대한 전반적인 성능을 고려하여 최종적으로 H-Reducer의 병합 형태로 1x4를 선택한다.\n' +
      '\n' +
      '**통합구조학습의 효과.** 비전투텍스트 모듈을 결정한 후, 통합구조학습으로 2단계 학습을 수행한다. 구조 인식 구문 분석 작업만 있으면 다른 도메인(r10 대 r5)에서 상당한 개선이 있다. 이는 구조 인식 파싱 작업을 통해 비주얼 인코더와 H-리듀서를 미세 조정하면 MLLM이 텍스트가 풍부한 이미지를 이해하는 데 크게 도움이 된다는 것을 입증한다. LLM의 매개변수를 추가로 조정하면 약간의 개선(r11 대 r10)이 나타나며, 이는 일반적인 언어 지식이 시각적 문서 이해에 주요 장애물이 아님을 시사한다. 학습 가능한 크롭 위치 임베딩을 특수 텍스트 토큰으로 대체함으로써, 모델은 더 나은 성능(r12 대 r11)을 달성하며, 이는 LLM이 단순 텍스트 표시자만으로 다수의 크롭된 이미지의 상대적 위치를 잘 이해할 수 있음을 보여준다. 마지막으로, Multi-grained Text Localization 태스크를 도입함으로써 DocOwl 1.5가 가장 좋은 성능을 달성하여 시각적으로 위치한 텍스트와 구체적인 위치를 연관시키는 것이 문서를 더 정확하게 이해하는 데 도움이 된다는 것을 검증한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c|c c|c c|c c} \\hline \\hline  & \\multicolumn{3}{c|}{**Model Architecture**} & \\multicolumn{3}{c|}{**Multult-task Tuning**} & \\multicolumn{1}{c}{**DocVQA**} & \\multicolumn{1}{c}{**TabFact**} & \\multicolumn{1}{c}{**ChartQA**} \\\\  & \\multicolumn{1}{c}{**Init**} & \\multicolumn{1}{c}{**V2T**} & \\multicolumn{1}{c}{**Crop**} & \\multicolumn{1}{c|}{**CropPos**} & \\multicolumn{1}{c|}{**Learning**} & \\multicolumn{1}{c}{**VIT**} & \\multicolumn{1}{c|}{**LLM**} & \\multicolumn{1}{c}{**DocVQA**} & \\multicolumn{1}{c}{**TabFact**} & \\multicolumn{1}{c}{**ChartQA**} \\\\ \\hline r1 & Owl(224) & Abstractor & 20 & Emb & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & 65.4 & 67.6 & 59.3 \\\\ r2 & Owl(2448) & Abstractor & 20 & Emb & \\(\\times\\) & \\(\\times\\) & \\(\\times\\) & 66.3 & 69.8 & 60.6 \\\\ r3 & Owl(2448) & Abstractor & 20 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 71.4 & 70.3 & 64.2 \\\\ r4 & Owl(2448) & Abstractor & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 68.0 & 70.0 & 64.2 \\\\ \\hline r5 & Owl(2448) & H-Reducer(1x4) & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 72.8 & 72.9 & 65.0 \\\\ r6 & Owl(2448) & H-Reducer(2x2) & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 71.8 & 72.1 & 65.2 \\\\ r7 & Owl(2448) & H-Reducer(2x4) & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 71.4 & 71.1 & 66.0 \\\\ r8 & Owl(2448) & H-Reducer(1x8) & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 69.9 & 71.2 & 64.4 \\\\ r9 & Owl(2448) & H-Reducer(2x8) & 9 & Emb & \\(\\times\\) & ✓ & \\(\\times\\) & 69.2 & 70.2 & 65.6 \\\\ \\hline r10 & Owl(2448) & H-Reducer(1x4) & 9 & Emb & Parsing & \\(\\times\\) & \\(\\times\\) & 77.7 & 76.5 & 67.5 \\\\ r11 & Owl(2448) & H-Reducer(1x4) & 9 & Emb & Parsing & \\(\\times\\) & ✓ & 78.9 & 78.1 & 68.1 \\\\ r12 & Owl2(448) & H-Reducer(1x4) & 9 & Text & Parsing & \\(\\times\\) & ✓ & 79.8 & 77.7 & 69.1 \\\\ r13 & Owl2(448) & H-Reducer(1x4) & 9 & Text & Parsing+MTL & \\(\\times\\) & ✓ & 81.6 & 80.4 & 70.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 모델 설정의 절제 연구. ‘크롭’은 크롭된 이미지의 최대 수를 나타냅니다. ‘CropPos’는 크롭된 이미지의 위치를 나타내기 위해 학습 가능한 임베딩(Emb) 또는 텍스트 토큰(Text)을 사용하는 것을 의미한다. \'파싱\'과 \'MTL\'은 각각 구조 인식 파싱 태스크와 다중-그레인드 텍스트 위치 태스크를 의미한다. Owl(224)과 Owl2(448)는 각각 224 해상도의 mPLUG-Owl[57]과 448 해상도의 mPLUG-Owl2[58]을 나타낸다.\n' +
      '\n' +
      '**2단계 훈련의 효과.** 표 5와 같이 2단계 훈련 대신 구조 학습과 다운스트림 작업의 1단계 공동 훈련도 시도하고 DocStruct4M에서 샘플을 점진적으로 증가시킨다. 더 많은 반복으로 성능 향상을 관찰하지 못했기 때문에 에폭은 점차 감소한다. 조인트 트레이닝의 경우, 통합 구조 학습 샘플이 1M 미만일 때 증가함에 따라 DocVQA에서 모델이 크게 개선된다. 그러나 통합 구조 학습 샘플이 더욱 증가함에 따라 모델의 개선은 미묘해지고 2단계 학습을 사용하는 모델보다 성능이 좋지 않다. 이는 2단계 교육이 기본 텍스트 인식 및 구조 구문 분석 능력을 더 향상시킬 수 있으며 다운스트림 문서 이해에 더 유익하고 효율적임을 보여준다.\n' +
      '\n' +
      '### 텍스트 위치 결정 평가\n' +
      '\n' +
      '표 4에서 다운스트림 텍스트가 풍부한 이미지 이해 성능을 통해 H-Reducer의 효과를 입증하는 것 외에도, 우리는 공간 특징 보존의 우수성을 검증하기 위해 통합 구조 학습 후 텍스트 현지화 성능을 직접 비교한다. 4개의 입도에서 균형 잡힌 4,250개의 샘플을 사용하여 텍스트 인식 및 텍스트 접지 작업을 모두 포함하는 텍스트 현지화 평가 세트 DocLocal4K를 구축한다. DocLocal4K의 자세한 통계는 표 6과 같으며, 문서 이미지가 다른 이미지보다 훨씬 다양하고 복잡하다는 점을 고려할 때 이 도메인에는 다른 이미지보다 더 많은 샘플이 있다. IOU@0.5는 텍스트 접지 성능을 평가하는 데 사용된다. 텍스트 인식은 단어, 구, 선, 블록 입도를 각각 BLEU1, BLEU2, BLEU3, BLEU4[33]로 평가한다. 표 7에 도시된 바와 같이, 동일한 반복으로 훈련될 때, H-리듀서는 텍스트 인식 및 텍스트 접지 작업 모두에서 훨씬 더 나은 성능을 달성하며, 1x4 병합 형상을 갖는 H-리듀서가 이미지 내의 콘크리트 위치를 더 잘 이해하는 데 도움이 된다는 것을 보여준다.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '**간단한 구로 답변** 정량적인 결과 외에도 이미지의 다양한 도메인에 대한 시각적 문서 이해의 몇 가지 정성적 결과를 추가로 제시한다. 에 도시된 바와 같이\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Task**} & \\multicolumn{4}{c|}{**Text Granularity**} & \\multicolumn{4}{c}{**Image Domain**} \\\\  & **Word** & **Phrase** & **Line** & **Block** & **Doc** & **Table** & **Chart** & **Web** & **Natural** \\\\ \\hline Text Recognition & 622 & 499 & 522 & 482 & 1,004 & 491 & 229 & 267 & 134 \\\\ Text Grounding & 595 & 542 & 503 & 485 & 1,011 & 524 & 240 & 242 & 108 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: DocLocal4K의 상세 통계량.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Module**} & \\multirow{2}{*}{**Iter**} & \\multicolumn{4}{c|}{**Text Grounding**} & \\multicolumn{4}{c}{**Text Recognition**} \\\\  & & **Word** & **Phrase** & **Line** & **Block** & **ALL** & **Word** & **Phrase** & **Line** & **Block** & **ALL** \\\\ \\hline Abstractor & 1,800 & 10.92 & 25.83 & 34.59 & 87.01 & 37.69 & 30.68 & 28.58 & 40.12 & 32.73 & 33.03 \\\\ H-Reducer(2x2) & 1,800 & 14.19 & 34.87 & 43.94 & 89.07 & 43.94 & 37.20 & 38.33 & 48.68 & 41.99 & 41.55 \\\\ H-Reducer(1x4) & 1,800 & **17.82** & **39.30** & **53.28** & **90.52** & **48.28** & **39.60** & **41.84** & **55.37** & **49.84** & **46.66** \\\\ \\hline H-Reducer(1x4) & 12,000 & 70.42 & 76.38 & 85.88 & 91.34 & 80.38 & 70.10 & 67.86 & 73.88 & 70.70 & 70.63 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 상이한 비전-텍스트 모듈을 갖는 모델들의 다중-입체 텍스트 로컬화 성능.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c|c c} \\hline \\hline  & \\multicolumn{4}{c|}{**One-Stage**} & \\multicolumn{1}{c}{**Two-Stage**} \\\\ \\hline DocStruct4M samples & 0.0M & 0.5M & 1.0M & 2.0M & 4.0M & 4.0M \\\\ Benchmark samples & 0.6M & 0.6M & 0.6M & 0.6M & 0.6M & 0.6M \\\\ Epoch/iteration & 7/18k & 6/25k & 6/37k & 4/40k & 3/54k & 3/12k + 3/6.5k \\\\ Cost (A100 days) & 60.0 & 83.3 & 123.3 & 133.3 & 180.0 & 144.8 \\\\ \\hline DocVQA & 72.8 & 75.5 & 78.6 & 78.8 & 78.9 & 79.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: DocStruct4M의 증가하는 샘플과 2단계 훈련 및 1단계 관절 훈련의 비교. 공정한 비교를 위해 LLM은 2단계 및 1단계 훈련 모두에 대해 동결된다. 1단계 훈련의 배스 크기는 2단계 훈련에서 Multi-task Tuning과 동일한 256으로 항상 설정된다.\n' +
      '\n' +
      '도. 도 6의 (a) 및 (b)를 참조하면, 두 모델 모두 이미지 내의 텍스트로 질문에 응답한다. DocOwl 1.5는 두 문서의 구조를 더 잘 이해하고 정답을 줄 수 있다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 6(c)를 참조하면, 마크다운 코드를 갖는 파싱 차트의 학습으로 인해, DocOwl 1.5는 차트를 더 잘 이해할 수 있고 x/y 축을 성공적으로 상관시킬 수 있다. 도. 도 6의 (d)는 지면 진리와 일치하지 않지만, DocOwl 1.5는 테이블에 더 강한 구조 이해의 도움으로 또 다른 정답을 제공한다는 것을 보여준다.\n' +
      '\n' +
      '**자세한 설명과 함께 질문 답변.** 도 7 및 도 7을 참조하여 설명한다. 도 8은 상세한 설명의 질적 결과를 제시한다. 소량의 추론 훈련을 통해 DocOwl 1.5-Chat은 LLM의 추론 능력을 잘 계승하고 답변에 대한 상세한 설명을 제공할 수 있다. 그러나 그림 1에 제시된 바와 같다. 도 8(c)는 대부분의 일반적인 멀티모달 대형 언어 모델[57; 58; 3]과 마찬가지로, DocOwl 1.5-Chat도 시각적 문서 이해에서 환각 문제를 겪을 수 있다. 이 작업에서는 주로 MLLM의 통일된 구조 이해 능력을 높이는 데 초점을 맞추고 OCR이 없는 문서 이해에서 환각 문제를 해결하는 방법을 향후 작업으로 남겨둔다.\n' +
      '\n' +
      '**구조 인식 파싱.**에 도시된 바와 같다. 도 9에 도시된 바와 같이, DocOwl 1.5는 텍스트 콘텐츠의 구조를 나타내기 위해 라인 피드 및 스페이스를 사용하여 문서 이미지를 파싱할 수 있었다. 그림 1과 같이 전체 문서를 구문 분석하는 것 외에도. 10, 인간의 지시에 따라 이미지의 중간에서 텍스트를 구문 분석할 수도 있다. 도. 도 11은 다중 컬럼에 걸쳐 있는 셀이 있는 테이블에 확장된 마크다운 신택스를 통한 구조 인식 테이블 파싱의 정성적 결과를 제시한다. 나아가, 도 1을 참조하여 설명한다. 도 12는 수직 막대, 수평 막대, 파이, 및 선 차트를 포함하는 상이한 유형의 차트를 마크다운 코드로 파싱하는 일부 경우를 도시한다. 모든 데이터 포인트가 차트에 제시될 때, DocOwl 1.5는 통계 객체를 대응하는 숫자로 정확하게 정렬할 수 있다. 그것은 Fig.에서 약간의 실수를 한다. 도 12(d)는 데이터 포인트가 제공되지 않을 때 콘크리트 수를 추정하는 것이 상당히 어렵기 때문이다. 마지막으로, 도 1에 도시된 바와 같다. 도 13, DocOwl 1.5는 자연 이미지의 내용을 기술하고 장면 텍스트를 읽을 수 있다.\n' +
      '\n' +
      '**다중-그립 텍스트 로컬화.** 도 14 및 도 4를 참조하여 설명한다. 도 15는 단어, 구, 행 및 블록의 입도에서 텍스트 접지 및 텍스트 인식의 정성적 결과를 나타낸다. 이미지 도메인은 문서, 웹 페이지, 차트 및 표에서 자연 이미지에 이르기까지 다양합니다.\n' +
      '\n' +
      '그림 7: 상세한 설명과 함께 질의응답의 질적 결과. 일부 영역은 더 나은 시각화를 위해 확대됩니다.\n' +
      '\n' +
      '그림 8: 상세한 설명과 함께 질의응답의 질적 결과. 답안의 환각은 빨간색으로 표시되어 있다.\n' +
      '\n' +
      '그림 10: 이미지 중간부터 구조 인식 문서 파싱의 정성적인 결과. 빨간색 점선 상자는 더 나은 시각화를 위해 답변의 위치를 표시하는 데만 사용되며 입력 이미지에는 포함되지 않는다.\n' +
      '\n' +
      '도 9: 구조 인식 문서 파싱의 정성적 결과.\n' +
      '\n' +
      '도 11: 다중 열(a)에 걸쳐 있는 셀을 갖는 테이블에 대한 구조 인식 테이블 파싱 및 이미지(b)의 중간부터 구조 인식 테이블 파싱의 정성적 결과. 빨간색 점선 상자는 더 나은 시각화를 위해 답변의 위치를 표시하는 데만 사용되며 입력 이미지에는 포함되지 않는다.\n' +
      '\n' +
      '도 12: 수직 막대(a), 수평 막대(b), 파이(c) 및 라인(d)의 차트에 대한 구조 인식 차트 파싱의 정성적 결과. 답변에 잘못된 단어가 빨간색으로 표시되어 있습니다.\n' +
      '\n' +
      '도 13: 자연 이미지 파싱의 정성적 결과. 더 나은 시각화를 위해, 일부 영역은 확대되고 대응하는 장면 텍스트로 라벨링된다. 답변에 잘못된 단어가 빨간색으로 표시되어 있습니다.\n' +
      '\n' +
      '그림 14: 다중 입자 텍스트 접지의 질적 결과. 일부 영역은 더 나은 시각화를 위해 확대됩니다. DocOwl 1.5에서 예측한 경계 상자는 이미지에 솔리드 레드 상자로 그려진다.\n' +
      '\n' +
      '그림 15: 다중 분류 텍스트 인식의 질적 결과. 일부 영역은 더 나은 시각화를 위해 확대됩니다. 입력 경계 상자는 이미지에 솔리드 블루 상자로 그려집니다. 답변에서 잘못된 단어는 빨간색으로 표시됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. _ArXiv_, abs/2204.14198, 2022.\n' +
      '* [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.\n' +
      '* [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. _arXiv preprint arXiv:2308.12966_, 2023.\n' +
      '* [4] Lukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Michal Turski, Karolina Szyndler, and Filip Gralinski. DUE: end-to-end document understanding benchmark. In _NeurIPS Datasets and Benchmarks_, 2021.\n' +
      '* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n' +
      '* [6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* [7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In _CVPR_, pages 3558-3568. Computer Vision Foundation / IEEE, 2021.\n' +
      '* [8] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact : A large-scale dataset for table-based fact verification. In _International Conference on Learning Representations (ICLR)_, Addis Ababa, Ethiopia, April 2020.\n' +
      '* [9] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. Websrc: A dataset for web-based structural reading comprehension. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 4173-4185, 2021.\n' +
      '* [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _CoRR_, abs/2305.06500, 2023.\n' +
      '* [11] Xiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. TURL: table understanding through representation learning. _SIGMOD Rec._, 51(1):33-40, 2022.\n' +
      '* [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_. OpenReview.net, 2021.\n' +
      '* [13] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang Li, and Can Huang. Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding. _CoRR_, abs/2311.11810, 2023.\n' +
      '\n' +
      '* [14] Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In _ICDAR_, pages 991-995. IEEE Computer Society, 2015.\n' +
      '* [15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for GUI agents. _CoRR_, abs/2312.08914, 2023.\n' +
      '* [16] Anwen Hu, Shizhe Chen, and Qin Jin. Question-controlled text-aware image captioning. In _ACM Multimedia_, pages 3097-3105. ACM, 2021.\n' +
      '* [17] Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. mplug-paperowl: Scientific diagram analysis with the multimodal large language model. _arXiv preprint arXiv:2311.18248_, 2023.\n' +
      '* [18] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document AI with unified text and image masking. In _ACM Multimedia_, pages 4083-4091. ACM, 2022.\n' +
      '* [19] Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. DVQA: understanding data visualizations via question answering. In _CVPR_, pages 5648-5656. Computer Vision Foundation / IEEE Computer Society, 2018.\n' +
      '* [20] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. In _ICLR (Workshop)_. OpenReview.net, 2018.\n' +
      '* [21] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq R. Joty. Chart-to-text: A large-scale benchmark for chart summarization. In _ACL (1)_, pages 4005-4023. Association for Computational Linguistics, 2022.\n' +
      '* [22] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeong Yeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In _ECCV (28)_, volume 13688 of _Lecture Notes in Computer Science_, pages 498-517. Springer, 2022.\n' +
      '* [23] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In _ICML_, volume 202 of _Proceedings of Machine Learning Research_, pages 18893-18912. PMLR, 2023.\n' +
      '* [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. _CoRR_, abs/2301.12597, 2023.\n' +
      '* [25] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. _CoRR_, abs/2311.06607, 2023.\n' +
      '* [26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _CoRR_, abs/2310.03744, 2023.\n' +
      '* [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _CoRR_, abs/2304.08485, 2023.\n' +
      '* [28] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. _arXiv preprint arXiv:2305.07895_, 2023.\n' +
      '* [29] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In _ACL (Findings)_, pages 2263-2279. Association for Computational Linguistics, 2022.\n' +
      '\n' +
      '* Mathew et al. [2021] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images. In _WACV_, pages 2199-2208. IEEE, 2021.\n' +
      '* Mathew et al. [2022] Minesh Mathew, Viraj Bagal, Ruben Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. Infographicvqa. In _WACV_, pages 2582-2591. IEEE, 2022.\n' +
      '* Methani et al. [2020] Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In _WACV_, pages 1516-1525. IEEE, 2020.\n' +
      '* Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.\n' +
      '* Pasupat and Liang[2015] Panupong Pasupat and Percy Liang. 반구조화된 테이블에 대한 구성 시맨틱 파싱 _ACL(1)_에서, 페이지 1470-1480. The Association for Computer Linguistics, 2015.\n' +
      '* Peng et al. [2023] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _CoRR_, abs/2306.14824, 2023.\n' +
      '* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 8748-8763. PMLR, 2021.\n' +
      '* Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.\n' +
      '* Sharma et al. [2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypermyped, image alt-text dataset for automatic image captioning. In _ACL (1)_, pages 2556-2565. Association for Computational Linguistics, 2018.\n' +
      '* Sidorov et al. [2020] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: A dataset for image captioning with reading comprehension. In _ECCV (2)_, volume 12347 of _Lecture Notes in Computer Science_, pages 742-758. Springer, 2020.\n' +
      '* Singh et al. [2019] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In _CVPR_, pages 8317-8326. Computer Vision Foundation / IEEE, 2019.\n' +
      '* Stanislawek et al. [2021] Tomasz Stanislawek, Filip Gralinski, Anna Wroblewska, Dawid Lipinski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, and Przemyslaw Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In _ICDAR (1)_, volume 12821 of _Lecture Notes in Computer Science_, pages 564-579. Springer, 2021.\n' +
      '* Svetlichnaya[2020] S Svetlichnaya. 딥폼: 2020년 규모의 구조화된 문서를 이해합니다.\n' +
      '* Tanaka et al. [2021] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In _AAAI_, pages 13878-13888. AAAI Press, 2021.\n' +
      '* Tang et al. [2023] Benny J. Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: A benchmark for semantically rich chart captioning. In _ACL (1)_, pages 7268-7298. Association for Computational Linguistics, 2023.\n' +
      '* Tang et al. [2023] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document processing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19254-19264, 2023.\n' +
      '* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '\n' +
      '* [47] Michal Turski, Tomasz Stanislawek, Karol Kaczmarek, Pawel Dyda, and Filip Gralinski. Ccpdf: Building a high quality corpus for visually rich documents from web crawl data. In _ICDAR (3)_, volume 14189 of _Lecture Notes in Computer Science_, pages 348-365. Springer, 2023.\n' +
      '* [48] Vicuna. Vicuna: An open chatbot impressing gpt-4. [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat), 2023.\n' +
      '* [49] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In _ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 23318-23340. PMLR, 2022.\n' +
      '* [50] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogylm: Visual expert for pretrained language models. _CoRR_, abs/2311.03079, 2023.\n' +
      '* [51] Wenjin Wang, Yunhao Li, Yixin Ou, and Yin Zhang. Layout and task aware instruction prompt for zero-shot document image question answering. _CoRR_, abs/2306.00526, 2023.\n' +
      '* [52] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2E-VLP: end-to-end vision-language pre-training enhanced by visual learning. In _ACL/IJCNLP (1)_, pages 503-513. Association for Computational Linguistics, 2021.\n' +
      '* [53] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. Layoutlmv2: Multi-modal pre-training for visually-rich document understanding. In _ACL/IJCNLP (1)_, pages 2579-2591. Association for Computational Linguistics, 2021.\n' +
      '* [54] Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. TAP: text-aware pre-training for text-vqa and text-caption. In _CVPR_, pages 8751-8761. Computer Vision Foundation / IEEE, 2021.\n' +
      '* [55] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. _CoRR_, abs/2307.02499, 2023.\n' +
      '* [56] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Fei Huang. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. In _EMNLP (Findings)_, pages 2841-2858. Association for Computational Linguistics, 2023.\n' +
      '* [57] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality. _CoRR_, abs/2304.14178, 2023.\n' +
      '* [58] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. _CoRR_, abs/2311.04257, 2023.\n' +
      '* [59] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mmllms: Recent advances in multimodal large language models. _arXiv preprint arXiv:2401.13601_, 2024.\n' +
      '* [60] Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. MPMQA: multimodal question answering on product manuals. _CoRR_, abs/2304.09660, 2023.\n' +
      '* [61] Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for tables. _CoRR_, abs/2311.09206, 2023.\n' +
      '* [62] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. _CoRR_, abs/2303.18223, 2023.\n' +
      '\n' +
      '* [63] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno-Yepes. Image-based table recognition: Data, model, and evaluation. In _ECCV (21)_, volume 12366 of _Lecture Notes in Computer Science_, pages 564-580. Springer, 2020.\n' +
      '* [64] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>