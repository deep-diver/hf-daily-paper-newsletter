<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'concepts including objects, attributes, and relationships, the LLM agent initially decomposes it, which entails the extraction of individual objects, their associated attributes, and the prediction of a coherent scene layout. These individual objects can then be independently conquered. Subsequently, the agent performs reasoning by analyzing the text, plans and employs the tools to compose these isolated objects. The verification and human feedback mechanism is finally incorporated into our agent to further correct the potential attribute errors and refine the generated images. Guided by the LLM agent, we propose a tuning-free multi-concept customization model and a layout-to-image generation model as the tools for concept composition, and a local image editing method as the tool to interact with the agent for verification. The scene layout controls the image generation process among these tools to prevent confusion among multiple objects. Extensive experiments demonstrate the superiority of our approach for compositional text-to-image generation: CompAgent achieves more than 10% improvement on T2I-CompBench, a comprehensive benchmark for open-world compositional T2I generation. The extension to various related tasks also illustrates the flexibility of our CompAgent for potential applications.\n' +
      '\n' +
      '**Computing methodologies \\(\\rightarrow\\) Computer vision; Image manipulation.**\n' +
      '\n' +
      'Additional Key Words and Phrases: Image Generation, Compositional Text-to-Image Generation, Diffusion Models, LLM Agent\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Recent advancements in text-to-image generation (Chang et al., 2023; Chen et al., 2023; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022) have demonstrated the remarkable capability in creating diverse and high-quality images based on language text inputs. However, even state-of-the-art text-to-image models often fail to generate images that accurately align with complex text prompts, where multiple objects with different attributes or relationships are composed into one scene (Chefer et al., 2023; Feng et al., 2023; Huang et al., 2023). As can be seen in Fig. 1, existing methods cannot create correct objects, attributes, or relationships within the generated images given these complex text prompts.\n' +
      '\n' +
      'Addressing compositional text-to-image generation requires solving at least the following three issues: 1) _Object types and quantities_. Due to the presence of multiple objects, the generated images should accurately incorporate each object, avoiding issues such as incorrect object types, omissions of objects, and discrepancies in object quantities. 2) _Attribute binding_. Objects inherently possess distinctive attributes, like "color", "shape" or "texture". It should be guaranteed that these object attributes are meticulously preserved within the generated images, avoiding issues like attribute misalignment or leakage. 3) _Object relationships_. There can be interaction relationships among the multiple objects, such as spatial relationships like "left", "right" or non-spatial ones like "holding", "playing". The generation process should encapsulate and convey these relationships within the resultant images with precision and fidelity.\n' +
      '\n' +
      'While existing text-to-image models do not possess the capability to address the aforementioned three issues, they do demonstrate proficiency in the generation of single objects, encompassing their distinctive types and attributes. Motivated by this, we propose _CompAgent_, a training-free approach founded upon the principle of divide-and-conquer for compositional text-to-image generation, coordinated by a large language model (LLM) agent. The fundamental idea revolves around breaking down intricate textual sentences into their constituent individual objects, initially ensuring the correctness of these isolated objects, then composing them together to produce the final images. The overview is illustrated in Fig. 2.\n' +
      '\n' +
      'The core of our method is an AI agent implemented by LLM, which serves as the "brain" of the framework and is primarily responsible for the following tasks: 1) _Decomposition_. The agent decomposes the complex compositional sentence, extracting and cataloging all objects and their associated attributes. Simultaneously, it designs the layout of the scene, defining the positions of the objects through the specification of bounding boxes. The text-to-image generation models are then engaged to generate images for each individual object. 2) _Planning and Tool use_. The LLM agent conducts reasoning according to the complex text prompt and then formulates a strategic approach to image generation that is contingent upon the presence of object attributes and relationships. Then it employs external tools to perform image generation or editing. 3) _Verification and Feedback_. By leveraging the vision ability of LLM or other visual models, the agent further scrutinizes the generated images, discerns and rectifies potential attribute errors. Additionally, human feedback can be seamlessly incorporated into scene layout refinement, thereby enhancing the quality of the ultimate outputs.\n' +
      '\n' +
      'Guided by the LLM agent, we introduce three tools to compose multiple individual objects into a single cohesive image according to the scene layout. The first is about _tuning-free multi-concept customization_. Specifically, we impose spatial layout constraints by cross-attention map editing and a pre-trained ControlNet (Zhang et al., 2023) on a tuning-free single-concept customization network (Li et al., 2023) for supporting multiple objects. It regards previously generated images of individual objects as user-specified subjects to create customized images. In this way, the object attributes can be guaranteed. The second is a _layout-to-image_ model. Through latent updating (Xie et al., 2023), images are generated with the bounding box layout as the condition. The specification of object types and quantities is made feasible through the imposition of layout conditions, enabling the model to place emphasis on the\n' +
      '\n' +
      'Figure 2. **The overview of existing text-to-image generation methods (a) and our CompAgent (b).** Existing methods generate images from text prompts in a single step. In comparison, guided by the LLM agent, CompAgent divides complex text prompts into individual objects, conquers them separately, and composes them into final images with the tool library.\n' +
      '\n' +
      'accurate representation of object relationships. However, the layout-to-image model may not necessarily produce object attributes with absolute accuracy. Therefore, we further present the third tool, _local image editing_. The agent examines the objects with attribute errors in the generated images, which will be masked out through a segmentation model [14]. Through cross-attention control, subject-driven editing is conducted through the previous multi-concept customization network to replace the erroneous objects with their correct attribute counterparts. In addition to the aforementioned three tools, our toolkit also involves existing text-to-image generation models and vision-language multi-modal models, to handle simple text prompts and assess attribute correctness.\n' +
      '\n' +
      'Our main contributions can be summarized as follows:\n' +
      '\n' +
      '* We propose CompAgent to address compositional text-to-image generation through the divide-and-conquer approach. The LLM agent oversees the entire task, performing decomposition, reasoning and overall planning, and tool library use to solve complex cases in text-to-image generation.\n' +
      '* By employing both global and local layout control about the spatial arrangement, we propose a tuning-free multi-concept customization model to address the attribute binding problem. We also observe that the layout-to-image generation manner can ensure the faithfulness of object relationships.\n' +
      '* We introduce the verification and feedback mechanism into our LLM agent. By interacting with our designed local image editing tool, the potential attribute errors can be corrected and the generated images can be further refined.\n' +
      '\n' +
      'We evaluate CompAgent on the recent T2I-CompBench benchmark [15] for compositional text-to-image generation, involving extensive object attributes and relationships. Both qualitative and quantitative results demonstrate the superiority of our method. We achieve more than 10% improvement in the metrics about compositional text-to-image evaluation. Furthermore, CompAgent can be flexibly extended to various related applications, like multi-concept customized image generation, reference-based image editing, object placement, and so on.\n' +
      '\n' +
      '## 2. Related Work\n' +
      '\n' +
      'Text-to-image generationWith the development of diffusion models [13, 14], text-to-image generation has achieved remarkable success [12, 13, 15]. These models can typically generate highly natural and realistic images, but cannot guarantee the controllability of texts over images. Many subsequent works are thus proposed for controllable text-to-image generation. ControlNet [12] controls Stable Diffusion with various conditioning inputs like Canny edges, and [21] adopts sketch images for conditions. Layout-to-image methods [12, 13, 14, 15] synthesize images according to the given bounding boxes of objects. And some image editing methods [10, 11, 12, 13] edit images according to the user\'s instructions. Despite the success of these methods, they are still limited in handling complex text prompts for image generation.\n' +
      '\n' +
      'LLM agentLarge language models (LLMs), like ChatGPT, GPT-4 [14], Llama [15], have demonstrated impressive capability in natural language processing. The involvement of vision ability in GPT-4V [13] further enables the model to process visual data. Recently, LLMs begin to be adopted as agents for executing complex tasks. These works [14, 15, 16] apply LLMs to learn to use tools for tasks like visual interaction, speech processing, and more recent works have extended to more impressive applications like software development [13], gaming [13], or APP use [13]. Different from them, we focus on the task of compositional text-to-image generation, and utilize a LLM agent for complex text prompt analysis and method planning.\n' +
      '\n' +
      '## 3. Method\n' +
      '\n' +
      'The overview of our CompAgent is illustrated in Fig. 3. The LLM agent coordinates the entire framework. It decomposes the complex text prompt, analyzes the attributes within the text, and designs the plans for the tools to be used. Ultimately, it invokes tools to address the challenges inherent in compositional text-to-image generation.\n' +
      '\n' +
      '### LLM Agent\n' +
      '\n' +
      'The primary responsibilities of our LLM agent center around the execution of tasks including decomposition, planning, tool use, as well as the implementation of verification and feedback.\n' +
      '\n' +
      'Complex text prompt decompositionThe input text prompts usually contain multiple objects, each characterized by its distinct attributes. These objects may also interact with each other with specific relationships. Existing image synthesis models cannot capture all these attributes and relationships simultaneously, making it difficult to accurately generate images that align with the input texts.\n' +
      '\n' +
      'To address the challenges, the LLM agent decomposes the complex texts. It extracts the discrete objects embedded within the text prompts, along with their associated attributes. The text-to-image generation model is then harnessed to generate several images for these objects. Since only one object is involved, existing models can typically generate images that match the attributes. The agent also formulates the scene layout represented as captioned bounding boxes, where the position of each foreground object is specified in the (x, y, width, height) format. The layout will guide the image generation process when composing separate objects together.\n' +
      '\n' +
      'Tool useSince the language model does not have the ability for image generation, external tools are required to be utilized to compose separate objects together for image generation:\n' +
      '\n' +
      '1) Tuning-free multi-concept customization. It regards previously generated images of individual objects as the target images, and produces images that feature these specific objects according to the input text prompts. For these objects, the presence of multiple images, each characterizing them with accurate attributes, generally assures that the multi-concept customization model can effectively ensure the fidelity of object attributes. However, the model tends to focus excessively on preserving the features of the target images, thus could potentially overlook other information in the texts like object relationships. Therefore, this tool can effectively address the attribute binding problem, but may not guarantee the relationships.\n' +
      '\n' +
      '2) Layout-to-image generation. It generates images with the previously established scene layout as the condition, and does not utilize the target images of objects. The object types and quantities can be specified through the scene layout, thereby facilitating the model\'s enhanced attention to information beyond the objects, _i.e._ the object relationships. As a result, such a layout-to-image generation tool can address the object relationship problem well, but the layout guidance only is not enough to guarantee the object attributes.\n' +
      '\n' +
      '3) Local image editing. Since the layout-to-image generation model may not consistently produce objects with correct attributes, we further design the local image editing tool to replace the object with the correct attribute one. Previously generated images of individual objects are leveraged here as the reference for object replacement. This tool will interact reciprocally with the verification mechanism of the agent, collaboratively determining which object requires to be modified.\n' +
      '\n' +
      '4) Text-to-image generation. Existing text-to-image generation models are utilized to generate individual object images during the decomposition step. They will also be used as the tool to generate images given simple text prompts for non-compositional generation.\n' +
      '\n' +
      '5) Vision-text multi-modal model. It assesses whether the object attributes in the images are correct for the verification function by leveraging the visual question answering ability of these models.\n' +
      '\n' +
      '_Planning._ In light of the diverse proficiency exhibited by the tools, the strategic selection of tool deployment is a core responsibility of our LLM agent. It mainly analyzes the input text prompt. If the text primarily centers around object attributes and the relationships are relatively straightforward (like spatial relationships "left", "right"), the customization tool will be employed. If object relationships are contained while their attributes are simple (like the naive "white" snow color attribute), the layout-to-image generation tool is suitable. If the text involves both attributes and relationships, the LLM agent first employs the layout-to-image model to represent object relationships. Then it will leverage the vision-text multi-modal models to scrutinize the correctness of object attributes for verification, and decide whether to adopt the local image editing tool. For simple text prompts with only straightforward attributes or relationships, the text-to-image generation tool will be directly utilized.\n' +
      '\n' +
      '_Verification and feedback._ Considering the potential limitation of the layout-to-image generation tool in accurately rendering object attributes, a verification process becomes imperative. We employ existing vision-language multi-modal models like GPT-4V [17], MiniGPT-4 [17], LLVA [19], and query it about whether the attributes are correct. If the attributes of some objects are incorrect, the LLM agent will invoke the local image editing tool to substitute the objects with the correct ones.\n' +
      '\n' +
      'Besides, for too complicated text prompts with a higher number of objects, together with intricate attributes and relationships, relying solely on the agent to automatically decompose the text and design scene layout may not necessarily be entirely accurate. In this situation, human feedback can be involved. Humans can make adjustments to the scene layout, such as inappropriate object sizes, positions, missing or extra objects. They can also make modifications to planning and verification errors. Introducing human feedback in the form of layout can reduce the cost of human labor. The accommodation of our framework about human feedback makes our LLM agent more flexible for compositional generation.\n' +
      '\n' +
      '### Tuning-Free Multi-Concept Customization\n' +
      '\n' +
      'In this section, we mainly introduce our tuning-free multi-concept customization tool. Its overview is illustrated in Fig. 4. Training a tuning-free customization image generation model typically requires large-scale pre-training for subject representation learning. Currently, there are already tuning-free methods available that support single-concept customization. For our approach, we build upon the existing single-concept customization model, BLIP-Diffusion [19], and extend its capability to accommodate multiple concepts, aided by the incorporation of the scene layout. Remarkably, we eliminate the need for large-scale upstream pre-training and directly construct a tuning-free multi-concept customization model to uphold the integrity of object attributes.\n' +
      '\n' +
      'Fig. 3. **The framework of CompAgent.** Given the input containing the complex text prompt, the LLM agent conducts the decomposition and planning tasks to invoke external tools for image generation. It then performs verification or involves human feedback and interacts with the tools for image self-correction. The final image output will well satisfy the requirements from the input text prompt.\n' +
      '\n' +
      'Specifically, for each concept, we extract its subject prompt embedding with the BIIP-2 encoders and the multi-modal encoder [11]. To gain a comprehensive understanding of the precise object attributes, we harness the information contained within multiple images corresponding to a single concept. We collect all the embeddings derived from these images and compute their average, generating the definitive subject prompt embedding for subsequent use. They are concatenated with the text prompt embedding and forwarded into the U-Net of the single-concept customization model [11] for image generation.\n' +
      '\n' +
      'However, directly aggregating embeddings from multiple concepts can easily lead to interference between different objects during image generation, resulting in the issue of concept confusion. To avoid this, we leverage the scene layout to regulate the positioning of each object, thereby mitigating the risk of their interference. We employ two levels of layout control - globally and locally. As is seen in the top part of Fig. 4, we mask the background of the COCO dataset [11], and train a ControlNet [22] via a layout-to-image paradigm. The ControlNet is utilized to control the U-Net via residuals. It provides strong control at the global level, effectively distinguishing multiple objects, thus well avoiding their confusion. However, it can only control globally and cannot independently control the position of each individual object.\n' +
      '\n' +
      'For local layout control of individual objects separately, we further propose to edit the cross-attention map according to the scene layout, motivated by the fact that the cross-attention map directly affects the spatial layout of the generated image [14]. Specifically, we collect the cross-attention map of each object words and their attribute words. We add a positive constant number \\(\\alpha^{+}\\) to the regions corresponding to the presence of the object, while concurrently adding a negative constant number \\(\\alpha^{-}\\) to the rest regions. Compared to ControlNet, cross-attention editing realizes significantly weaker layout control but can independently govern the position of each object. Therefore, when synergistically integrated with ControlNet, it effectively controls the overall layout of the entire image. Ultimately, guided by the layout, different objects can be distinguished from each other, avoiding the confusion problem and achieving multi-concept customization.\n' +
      '\n' +
      '### Layout-to-Image Generation\n' +
      '\n' +
      'To guarantee object relationships, we generate images directly from the scene layout. While our previously employed ControlNet and cross-attention editing approach can indeed tackle the layout-to-image problem, they are characterized by the imposition of too strong constraints upon the layout. Once the scene layout deviates from the precise depiction of object relationships, it becomes challenging to guarantee the accurate representation of these relationships. Therefore, we utilize the strategy of latent updating by backwarding the box-constrained loss [23] for image generation from the layout. It provides a relatively loose control over the layout, thus allowing a flexible assurance of object relationships.\n' +
      '\n' +
      '### Local Image Editing\n' +
      '\n' +
      'To rectify objects with incorrect attributes, we introduce our local image editing tool, as illustrated in Fig. 5. By querying our LLM agent for verification, we can identify which object attributes are erroneous and require modification. We utilize the combination of Grounding DINO [11] and SAM [15] to segment the object out. The resulting segmented mask is utilized for cross-attention editing to provide position guidance for image editing. The image requiring editing is reconverted into the latent through DDIM inversion, serving as the initial latent for the subsequent image generation process. Images featuring objects characterized by correct attributes have already been generated earlier. These images, together with the text prompts, are processed in a manner analogous to the previous customization model, which serves as the conditional input for the U-Net. The process of image generation generally follows the previous multi-concept customization, with the image DDIM inversion as the initial latent. The segmentation masks are used as the guidance for cross-attention editing, while ControlNet is not employed. In this way, objects with incorrect attributes can be effectively substituted and rectified.\n' +
      '\n' +
      '## 4. Results\n' +
      '\n' +
      'We mainly conduct experiments on the recent T2I-CompBench benchmark [12], which mainly divides compositional text prompts into six sub-categories. For quantitative comparison,\n' +
      '\n' +
      'Figure 4. **Illustration of our tuning-free multi-concept customization tool.** ControlNet and cross-attention editing control the scene layout. The embeddings of multiple image concepts and the text prompt are concatenated together and forwarded into U-Net for image generation.\n' +
      '\n' +
      'Figure 5. **Illustration of our local image editing tool.** The concept images pass into the customization network for embedding extraction, and the masked segmentation map guides the image generation process.\n' +
      '\n' +
      'we following its setting, and utilize the BLIP-VQA metric for attribute binding evaluation, the UniDet-based metric for spatial relationship evaluation, CLIPscore for non-spatial relationship, and the 3-in-1 metric for complex prompts. We also include qualitative results in both the main paper and supplementary materials.\n' +
      '\n' +
      '### Quantitative Comparison\n' +
      '\n' +
      'We list the quantitative metric results of our CompAgent in Tab. 1. We compare with existing state-of-the-art text-to-image synthesis methods and models that are designed for complex text prompts. For text-to-image generation, we compare with the recent Stable Diffusion [12] v1.4, v2 and XL [13] model, DALL-E 2 [20], PixArt-\\(\\alpha\\)[3] and DALL-E 3 [14]. For compositional text-to-image generation methods, we compare with Composable Diffusion [15], StructureDiffusion [15], AttnMask-Control [16], GORS [17]. TokenCompose [16], Attn-Exct [3], ECLIPSE [14] and LMD [11] target at multiple objects within a sentence so we also compare with them.\n' +
      '\n' +
      'For attribute binding, our method achieves significantly higher BLIP-VQA metric compared to previous methods, 16.02%, 16.51% and 8.72% higher for the color, shape and texture attributes compared with PixArt-\\(\\alpha\\) respectively. Compared to DALL-E 3, the current state-of-the-art method for controllability in text-to-image generation, our approach performs on par with it in terms of the performance. For the color and shape attributes, our CompAgent is even 3.78% and 4.83% higher than it, based on the Stable Diffusion model. This demonstrates the capability of CompAgent to accurately generate object types as well as their attributes. Compared to single-step generation, the superiority of such the divide-and-conquer multi-step generation manner can thus be observed in attribute binding. For object relationships, CompAgent excels in both spatial and non-spatial relationships. In contrast, previous methods either lack such ability or are limited to handling only a single type of relationship. With the ability for both attribute binding and object relationship, our CompAgent can well address complex text prompts: we achieve the 48.63% 3-in-1 metric, which surpasses previous methods by more than 7%. The quantitative results well demonstrate that our method effectively addresses the challenges associated with compositional text-to-image generation.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'We further conduct ablation study in this section to analyze the effect of the LLM agent under our framework.\n' +
      '\n' +
      '_LLM agent planning and verification._ We first analyze the effect of the planning and verification mechanism of our LLM agent in Tab. 2. It can be observed that by leveraging the individual object images, the multi-concept customization tool performs well for attribute binding. However, because of the utilized ControlNet, the customization model can be inflexible for expressing the object relationships, which leads to its limited metric scores in complex compositions. In comparison, the layout-to-image generation tool can well generate images with object relationships, but cannot guarantee the accuracy of object attributes. Our LLM agent can analyze the complex text prompts, and plan the most suitable tool to use. As a result, LLM agent planning well helps address most situations for compositional text-to-image generation. The verification mechanism of our LLM agent further helps correct some attribute errors, especially in complex compositions where layout-to-image generation cannot handle the object attributes, which thus contributes to the 2.21% improvement. As we can see, by planning and verification, our LLM agent well utilizes the tools for compositional text-to-image generation.\n' +
      '\n' +
      'We provide some visualized examples in Fig. 6. As we can see, the LLM agent well plans and employs the most suitable tool. The customization tool is utilized to strictly constrain object attributes, and the layout-to-image tool can make appropriate adjustments to the layout for object relationships. Besides, the local image editing tool can assist in rectifying objects with incorrect attributes.\n' +
      '\n' +
      '_Human feedback._ We further provide some visualized examples in Fig. 7 to show the effect of human feedback. For the first example, although object types and attributes are correct, the size of the glass cup is too large. By involving human feedback to modify the scene layout, such a problem can be addressed. For the second example, the texts are quite complex and there are some mistakes in the scene layout - one less hot dog, small table and too small car. Humans can inspect and correct them, then CompAgent can generate accurate images. This also applies to the third example. Our CompAgent can incorporate human feedback, enabling it to generate more realistic images and handle more complex text prompts.\n' +
      '\n' +
      'The excellent ability of our method for compositional text-to-image generation can thus be further demonstrated.\n' +
      '\n' +
      'We then provide the qualitative comparison with existing text-to-image generation methods and compositional text-to-image generation methods in Fig. 11. For the text "a black guitar and a brown amplifier", existing methods are easy to confuse the color of the guitar and the amplifier. In the second example, where four objects exist in the text, the correct object number also cannot be guaranteed for existing methods. For some uncommon attributes, like the triangular shelf in the third example and the blue sink in the last example, existing models are also easy to make mistakes about the attributes. Besides, in the fifth example, most of existing methods cannot express the "left" relationship accurately. Our CompAgent generates accurately for all these text prompts. This further demonstrates the superiority of our method over existing models when it comes to compositional text-to-image generation.\n' +
      '\n' +
      '## 5. Conclusion\n' +
      '\n' +
      'In this paper, we propose a training-free approach, CompAgent, for compositional text-to-image generation. By decomposing, planning, verifying, and involving human feedback, the LLM agent coordinates the whole system and employs external tools to generate high-fidelity and accurate images according to the given complex text prompts. Extensive results demonstrate that CompAgent well addresses the object type and quantity, attribute binding, and object relationship problems in compositional text-to-image generation. We consider CompAgent as an important step towards the future of autonomous agents empowered by language models and the controllability in text-to-image generation.\n' +
      '\n' +
      'Figure 6. **Visualized examples of CompAgent to show the generated images from different tools and how LLM agent plans to use the tools.** The LLM agent analyzes the text prompt. It employs the customization tool to address attribute binding, and the layout-to-image tool to address object relationships. For complex composition, the layout-to-image tool is utilized for object relationship, then the local image editing tool is used for attribute correction.\n' +
      '\n' +
      'Figure 7. **Visualized examples to show the effect of human feedback.** The green boxes are well-generated by the LLM agent. The red boxes are the ones generated by the LLM agent with some issues, which are modified through human feedback into the blue boxes in the second row.\n' +
      '\n' +
      'Figure 8: **Visualized results of our method for attribute binding.**\n' +
      '\n' +
      'Figure 10: **Visualized results of our method for complex composition.**\n' +
      '\n' +
      'Figure 9: **Visualized results of our method for object relationship.**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:9]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      'It can be seen that as the language understanding ability of large language models improves, the performance of our CompAgent for compositional text-to-image generation also enhances. When adopting GPT-4 as the agent, the metric reaches the highest. This is because GPT-4 can not only decompose individual objects successfully, but also generate the scene layout where the shapes of bounding boxes are more suitable for the object types. Meanwhile, even utilizing the Llama-7B model as our agent, the compositional text-to-image generation metric is still higher than most of existing methods. This demonstrates the flexibility of our CompAgent across different large language models.\n' +
      '\n' +
      '### Image Generation with Layout Guidance\n' +
      '\n' +
      'For composing multiple individual objects into the image, the generated scene layouts guide the generation process under our CompAgent framework. We compare the performance of different layout-to-image generation methods and list the results in Tab. 4. We compare with previous methods, including LostGAN [21], LAMA [11], TwFA [22], Stable Diffusion [14], GLIGEN [11] and GLIGEN + BoxDiff [23]. Under our CompAgent framework, we mainly utilize latent updating, cross-attention editing and ControlNet three strategies. We conduct experiments on the benchmark proposed in [23]. We apply the YOLOv4 [1] to detect objects and obtain the YOLO score, including AP, AP\\({}_{50}\\) and AP\\({}_{75}\\) to evaluate the precision of the layout-to-image performance.\n' +
      '\n' +
      'It can be seen that by utilizing cross-attention editing only to control image generation through bounding boxes, the obtained YOLO score is quite low, only 0.06, slightly higher than Stable Diffusion. This demonstrates that although editing cross-attention maps can provide layout guidance, it is still insufficient to control image generation accurately. Utilizing our trained ControlNet is much more effective, achieving the 0.338 AP, 4.1% higher than GLIGEN. However, ControlNet can only provide global control, not object-level. After combining ControlNet and cross-attention editing, the YOLO score reaches to 0.508, 10.6% higher than GLIGEN + BoxDiff. This demonstrates that our design can control the positions of objects more accurately. This well avoids the confusion of multiple objects, thus is suitable for the attribute binding problem. However, the reliance on the scene layout is too strong in this way, making it inappropriate for generating correct object relationships. Instead, latent updating can well follow the scene layout and can also be flexible meanwhile. Therefore, we utilize the latent updating strategy for the object relationship issue.\n' +
      '\n' +
      '### Qualitative Comparison\n' +
      '\n' +
      'We then provide more visualized comparisons with existing state-of-the-art text-to-image generation methods in Fig. 12. Existing methods are highly prone to the following errors. 1) Attribute confusion. For example, for the text "a blue backpack and a red book", existing models are easy to confuse the color of the backpack and the red book, or generate a backpack with red parts. 2) Constrained by common attributes or scenarios. For example, for the second example, existing models tend to generate a white sink since it is more common in reality, rather than the brown one. For the third example, existing methods also tend to generate a common living room scenario, while ignoring the required objects and attributes. 3) Incorrect relationship. For example, the "left" relationship cannot be correctly expressed in the fifth example. In comparison, our CompAgent well avoids these problems, thus generates images that more accurately align with the description of input texts.\n' +
      '\n' +
      'We also provide visualized comparison with existing compositional text-to-image generation methods in Fig. 13. We can observe that existing compositional text-to-image methods also cannot address the above mentioned problems. As a result, the correct object types and quantities, object attributes and relationships cannot be guaranteed. CompAgent behaves equally well for these examples.\n' +
      '\n' +
      'More visualized examples are provided in Fig. 14, Fig. 15 and Fig. 16. These examples further demonstrate the excellent ability of our CompAgent for addressing the compositional text-to-image generation problem.\n' +
      '\n' +
      '## Appendix D Extension to Other Tasks\n' +
      '\n' +
      'Besides the compositional text-to-image generation task, our CompAgent can also be flexibly extended to other related image generation tasks with the help of the LLM agent and our toolkits. We mainly introduce about the multi-concept customization, the image editing and the object placement tasks.\n' +
      '\n' +
      '### Multi-Concept Customization\n' +
      '\n' +
      'We first conduct the multi-concept customization task to generate images according to the input text prompts containing the given subjects. We compare with existing state-of-the-art customization methods, including DreamBooth [15], Custom Diffusion [13] and Cones 2 [11]. The comparison results are provided in Fig. 17. It can be seen that DreamBooth and Custom Diffusion cannot generate the corresponding objects or their correct attributes. For example, for the first example, DreamBooth does not generate the use in the image, and the object attributes from the Custom Diffusion image are incorrect. For the second example, Custom Diffusion does not generate the cup, while DreamBooth generates the cup with the incorrect color. Cones 2 performs better than them, generating accurate images with a red book and a yellow was. However, it is also limited by common attributes. For example, it does not generate the correct color for the cow in the second example, which also applies to the car in the third example.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r} \\hline \\hline Methods & AP \\(\\uparrow\\) & AP\\({}_{50}\\) \\(\\uparrow\\) & AP\\({}_{75}\\) \\(\\uparrow\\) \\\\ \\hline LostGAN & 0.053 & 0.089 & 0.056 \\\\ LAMA & 0.102 & 0.153 & 0.117 \\\\ TwFA & 0.106 & 0.147 & 0.126 \\\\ Stable Diffusion & 0.028 & 0.092 & 0.011 \\\\ GLIGEN & 0.297 & 0.458 & 0.339 \\\\ GLIGEN + BoxDiff & 0.402 & 0.620 & 0.462 \\\\ \\hline latent updating & 0.224 & 0.468 & 0.178 \\\\ cross-attention editing & 0.060 & 0.190 & 0.021 \\\\ ControlNet & 0.338 & 0.521 & 0.339 \\\\ ControlNet + cross-attention editing & **0.508** & **0.778** & **0.534** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4. **Evaluation for layout-to-image generation**. We adopt the YOLO score to evaluate the correspondence of images and their layouts.\n' +
      '\n' +
      'Besides, in the fourth example, Cones 2 confuses the features of the cat and the dog, thus generates the image with two dogs. In comparison, our CompAgent accurately captures the subject features and avoids the object confusion problem, thus handles the multi-concept customization task better. Note that these previous methods are all tuning-based, while our CompAgent is tuning-free. Therefore, CompAgent can accurately address the customization task more efficiently.\n' +
      '\n' +
      '### Local Image Editing\n' +
      '\n' +
      'We then conduct the local image editing experiments and compare with the Paint-by-Example method [23]. It can be seen that although Paint-by-Example can perform the local image editing task, it cannot precisely catch the object attributes. For example, for the car example, the front window color from the Paint-by-Example generated image turns to blue. For the computer-desk example, Paint-by-Example does not edit the color of the table, and for the mirror-sink example, Paint-by-Example also does not modify the\n' +
      '\n' +
      'Fig. 12. **Qualitative comparison between our approach and existing state-of-the-art text-to-image generation methods.**Figure 13: **Qualitative comparison between our approach and existing compositional text-to-image generation methods.**\n' +
      '\n' +
      'Figure 16: **Visualized results of our method for complex composition.**\n' +
      '\n' +
      'Figure 14: **Visualized results of our method for attribute binding.**\n' +
      '\n' +
      'Figure 15: **Visualized results of our method for object relationship.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '* Chang et al. (2023) Huijwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. 2023. Muse: Text-to-image generation via masked generative transformers. _arXiv preprint arXiv:2301.00704_ (2023).\n' +
      '* Chierz et al. (2023) Hila Chierz, Yuval Allaf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excise: Attention-based semantic guidance for text-to-image diffusion models. _ACM Transactions on Graphics_ (2023).\n' +
      '* Chen et al. (2023) Junosheng Chen, Jiencheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhonglao Wang, James Kwok, Ping Liu, Harchuan Lu, et al. 2023. Pix++: Fast-F Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis. _arXiv preprint arXiv:2310.00426_ (2023).\n' +
      '* Chen et al. (2024) Ming-Chong Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control with cross-attention guidance. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_.\n' +
      '* Chen et al. (2023) Xi Chen, Langhua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. 2023. Anydoor: Zero-shot object-level image customization. _arXiv preprint arXiv:2307.09814_ (2023).\n' +
      '* Davidsiwal and Nichol (2021) Prafulla Davidsiwal and Alexander Nichol. 2021. Diffusion models beat gans image synthesis. _Advances in neural information processing systems_ (2021).\n' +
      '* Famael et al. (2023) Meta-Fundamental AI Research Diplomacy Team (FAIR). Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. _Science_ (2022).\n' +
      '* Feng et al. (2023) Weii Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2023. Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In _International Conference on Learning Representations_.\n' +
      '* Hertz et al. (2023) Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In _International Conference on Learning Representations_.\n' +
      '* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. _Advances in neural information processing systems_ (2020).\n' +
      '* Huang et al. (2023) Kaiyi Huang, Rajve Sun, Emre Xie, Zhengu Li, and Xihui Liu. 2023. I2s-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. _Advances in Neural Information Processing Systems_ (2023).\n' +
      '* Kirillov et al. (2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 2023. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_.\n' +
      '* Kurni et al. (2023) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtmann, and Jun-Yan Zhu. 2023. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.\n' +
      '* Li et al. (2023a) Dongru Li, Junnan Li, and Steven CH Hoi. 2023a. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_ (2023).\n' +
      '* Li et al. (2023b) Junnan Li, Dongru Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large-language models.\n' +
      '\n' +
      'Fig. 19: **Visualized results of our method for object placement.**\n' +
      '\n' +
      'Fig. 18: **Visualized results of our method for local image editing.**_arXiv preprint arXiv:2301.12597_ (2023).\n' +
      '* Li et al. (2021) Yuheng Li, Haofin Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jee Lee. 2023c: Gligen: Open-set grounded text-to-image generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.\n' +
      '* Li et al. (2021) Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. 2021. Image synthesis from layout with locality-aware mask adaption. In _Proceedings of the CVF/CT International Conference on Computer Vision_.\n' +
      '* Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2023. LIM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Datasets. _arXiv preprint arXiv:2305.1365_ (2023).\n' +
      '* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In _European conference on computer vision_.\n' +
      '* Liu et al. (2023b) Haofin Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. _Advances in Neural Information Processing Systems_ (2023b).\n' +
      '* Liu et al. (2022) Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022. Compositional visual generation with composable diffusion models. In _European Conference on Computer Vision_.\n' +
      '* Liu et al. (2023) Shliong Liu, Zhayong Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023c: Grounding dinco: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_ (2023).\n' +
      '* Lin et al. (2023) Zhayong Lin, Yinan He, Wenjian Wang, Weiyan Wang, Yi Wang, Shoufa Chen, Qing-long Zhang, Yang Yang, Qingyun Li, Jiahao Yu, et al. 2023a. Internect: Solving vision-centric tasks by interacting with chatboub beyond language. _arXiv preprint arXiv:2305.05662_ (2023).\n' +
      '* Liu et al. (2023) Zhiheng Liu, Tife Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deil Zhao, Jingren Zhou, and Yang Cao. 2023d: Concons 2: Customizable Image Synthesis with Multiple Subjects. _arXiv preprint arXiv:2305.19327_ (2023).\n' +
      '* Zong et al. (2023) Ozpan J. 2023. GIFT-T Technical Report. _arXiv preprint arXiv:2304.08774_ (2023).\n' +
      '* Parmazar et al. (2023) Gaurav Parmazar, Krishna Kumar Singh, Richard Zhang, Yiyun Li, Jingwan Lu, and Jun-Yan Zhu. 2023. Zero-shot image-to-image translation. In _ACM SIGGRAPH 2023 Conference Proceedings_.\n' +
      '* Patel et al. (2023) Maitrere Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. 2023. ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations. _arXiv preprint arXiv:2312.04653_ (2023).\n' +
      '* Pogl et al. (2023) Dostriol Pogl, Zion English, Kylee Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Romback. 2023. Ssdl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_ (2023).\n' +
      '* Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weizen Chen, Yusheng Su, Juyuan Xu, Zhiyun Liu, and Maosong Sun. 2023. Communicative agents for software development. _arXiv preprint arXiv:2307.07974_ (2023).\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Pratilla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2202.04612_ (2022).\n' +
      '* Bombach et al. (2022) Robin Bombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_.\n' +
      '* Ruiz et al. (2023) Natani Ruiz, Yuanzhen Li, Varun Jampani, Yael Pitch, Michael Rubinstein, and Kfir Aherman. 2023. Dreambon: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.\n' +
      '* Saharia et al. (2022) Chiuwan Saharia, William Chan, Surakhh Savenna, Lala Li, Jay Whang, Emily L Denton, Kamyar Chasemjour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_ (2022).\n' +
      '* Shen et al. (2023) Yongqing Shen, Katia Song, Xu Tan, Dongheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggspdf: Solving ai tasks with chatpdt and its friends in huggingface. _Advances in Neural Information Processing Systems_ (2023).\n' +
      '* Sun and Wu (2021) Wei Sun and Tianfu Wu. 2021. Learning layout and style reconfigurable gans for controllable image synthesis. _IEEE transactions on pattern analysis and machine intelligence_ (2021).\n' +
      '* Touwron et al. (2023) Hugo Touwron, Thibaut Lavrii, Gantier Iacacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Larcuix, Baptis Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Imam: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_ (2023).\n' +
      '* Touwron et al. (2023) Hugo Touwron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlyakov, Soumya Batra, Prajayil Bhargava, Shruti Bhosale, et al. 2023b. Imam: 2 open modulation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ (2023).\n' +
      '* Vyvov et al. (2023) Andrey Vyov, Kfir Aherman, and Daniel Cohen-Or. 2023. Sketch-guided text-to-image diffusion models. In _ACM SIGGRAPH 2023 Conference Proceedings_.\n' +
      '* Wang et al. (2023a) Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. 2023a. Compositional text-to-image synthesis with attention map control of diffusion models. _arXiv preprint arXiv:2308.19921_ (2023).\n' +
      '* Wang et al. (2023b) Zirui Wang, Zhihuo Shu, Zheng Ding, Yilin Wang, and Zhuowen Tu. 2023b. TokenCompose: Grounding Diffusion with Token-level Supervision. _arXiv preprint arXiv:2312.03628_ (2023b).\n' +
      '* Xie et al. (2023) Jinheng Xie, Yuexiang Li, Yawen Huang, Haofuo Liu, Wentan Zhang, Yefeng Zheng, and Mike Zheng Shen. 2023b. Rodiff: Text-to-image synthesis with training-free box-constrained diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_.\n' +
      '* Shi et al. (2023a) Binxin Wang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. 2023a. Paint by example: Exemplar-based image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.\n' +
      '* Yang et al. (2023b) Hui Yang, Sifu Yue, and Yunzhong He. 2023b. Auto-GIFT for Online Decision Making Benchmarks and Additional Opinions. _arXiv preprint arXiv:2306.0222_ (2023b).\n' +
      '* Yang et al. (2023) Zhengyuan Yang, Linje Li, Kevin Lin, Jianfeng Wang, Chuny-Ching Lin, Zichen Liu, and Lijun Wang. 2023b. The dawn of lmms: Preliminary explorations with gpt-tv (sion). _arXiv preprint arXiv:2309.11724_ (2023).\n' +
      '* Yang et al. (2020) Zuong Yang, Daqing Liu, Chaoyang Wang, Tezo Tang, and Dacheng Tao. 2020. Modeling image composition for complex scene generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_.\n' +
      '* Yang et al. (2023c) Zhao Yang, Jiauxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023c. AppAgent: Multimodal Agents as Smartphone Users. _arXiv preprint arXiv:2312.13717_ (2023c).\n' +
      '* Zhang et al. (2023) Lvmin Zhang, Anyi Rao, and Maneesh Agrawawala. 2023. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_.\n' +
      '* Zhu et al. (2023) Deyza Zhu, Jun Chen, Xiaozuan Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minipet-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_ (2023).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>