<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# High-Quality Image Restoration Following Human Instructions\n' +
      '\n' +
      'Marcos V. Conde \\({}^{1,2}\\), Gregor Geigle \\({}^{1}\\), Radu Timofte \\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\) Computer Vision Lab, CAIDAS & IFI, University of Wurzburg\n' +
      '\n' +
      '\\({}^{2}\\) Sony PlayStation, FTG\n' +
      '\n' +
      '[https://github.com/mv-lab/InstructIR](https://github.com/mv-lab/InstructIR)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Images often contain unpleasant effects such as noise, motion blur, haze, and low dynamic range. Such effects are commonly known in low-level computer vision as _degradations_. These can result from camera limitations or challenging environmental conditions _e.g_. low light.\n' +
      '\n' +
      'Image restoration aims to recover a high-quality image from its degraded counterpart. This is a complex inverse problem since multiple different solutions can exist for restoring any given image [16, 20, 44, 59, 102, 103].\n' +
      '\n' +
      'Some methods focus on specific degradations, for instance reducing noise (denoising) [64, 102, 103], removing blur (deblurring) [58, 105], or clearing haze (dehazing) [16, 66]. Such methods are effective for their specific task, yet they do not generalize well to other types of degradation. Other approaches use a general neural network for diverse tasks [10, 74, 82, 95], yet training the neural network for each specific task independently. Since using a separate model for each possible degradation is resourceintensive, recent approaches propose _All-in-One_ restoration models [42, 60, 61, 100]. These approaches use a single deep blind restoration model considering multiple degradation types and levels. Contemporary works such as PromptIR [61] or ProRes [49] utilize a unified model for blind image restoration using learned guidance vectors, also known as "prompt _embeddings_", in contrast to raw user prompts in text form, which we use in this work.\n' +
      '\n' +
      'In parallel, recent works such as InstructPix2Pix [4] show the potential of using text prompts to guide image generation and editing models. However, this method (or recent alternatives) do not tackle inverse problems. Inspired by these works, we argue that text guidance can help to guide blind restoration models better than the image-based degradation classification used in previous works [42, 60, 100]. Users generally have an idea about what has to be fixed (though they might lack domain-specific vocabulary) so we can use this information to guide the model.\n' +
      '\n' +
      'ContributionsWe propose the first approach that utilizes real human-written instructions to solve inverse problems and image restoration. Our comprehensive experiments demonstrate the potential of using text guidance for image restoration and enhancement by achieving _state-of-the-art_ performance on various image restoration tasks, including image denoising, deraining, deblurring, dehazing and low-light image enhancement. Our model, _InstructIR_, is able to generalize to restoring images using arbitrary human-written instructions. Moreover, our single _all-in-one_ model covers more tasks than many previous works. We show diverse restoration samples of our method in Figure 1.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      'Image Restoration.Recent deep learning methods [16, 44, 58, 64, 74, 95] have shown consistently better results compared to traditional techniques for blind image restoration [18, 29, 35, 37, 54, 73]. The proposed neural networks are based on convolutional neural networks (CNNs) and Transformers [76] (or related attention mechanisms).\n' +
      '\n' +
      'We focus on general-purpose restoration models [10, 44, 82, 95]. For example, SwinIR [44], MAXIM [74] and Uformer [82]. These models can be trained -independently-for diverse tasks such as denoising, deraining or deblurring. Their ability to capture local and global feature interactions, and enhance them, allows the models to achieve great performance consistently across different tasks. For instance, Restormer [95] uses non-local blocks [79] to capture complex features across the image.\n' +
      '\n' +
      'NAFNet [10] is an efficient alternative to complex transformer-based methods. The model uses simplified channel attention, and gating as an alternative to non-linear activations. The builing block (NAFBlock) follows a simple meta-former [92] architecture with efficient inverted residual blocks [31]. In this work, we build our _InstructIR_ model using NAFNet as backbone, due to its efficient and simple design, and high performance in several restoration tasks.\n' +
      '\n' +
      'All-in-One Image Restoration.Single degradation (or single task) restoration methods are well-studied, however, their real-world applications are limited due to the required resources _i.e_. allocating different models, and select the adequate model on demand. Moreover, images rarely present a single degradation, for instance noise and blur are almost ubiquitous in any image capture.\n' +
      '\n' +
      'All-in-One (also known as multi-degradation or multi-task) image restoration is emerging as a new research field in low-level computer vision [42, 49, 60, 61, 75, 91, 97, 98]. These approaches use a single deep blind restoration model to tackle different degradation types and levels.\n' +
      '\n' +
      'We use as reference AirNet [42], IDR [100] and ADMS [60]. We also consider the contemporary work PromptIR [61]. The methods use different techniques to guide the blind model in the restoration process. For instance, an auxiliary model for degradation classification [42, 60], or multi-dimensional guidance vectors (also known as "prompts") [49, 61] that help the model to discriminate the different types of degradation in the image.\n' +
      '\n' +
      'Despite it is not the focus of this work, we acknowledge that _real-world image super-resolution_ is a related problem [12, 44, 48, 106], since the models aim to solve an inverse problem considering multiple degradations (blur, noise and downsampling).\n' +
      '\n' +
      'Text-guided Image Manipulation.In the recent years, multiple methods have been proposed for text-to-image generation and text-based image editing works [4, 30, 34, 53, 70]. These models use text prompts to describe images or actions, and powerful diffusion-based models for generating the corresponding images. Our main reference is InstructPix2Pix [4], this method enables editing from _instructions_ that tell the model what action to perform, as opposed to text labels, captions or descriptions of the input or output images. Therefore, the user can transmit what to do in natural written text, without requiring to provide further image descriptions or sample reference images.\n' +
      '\n' +
      '## 3 Image Restoration Following Instructions\n' +
      '\n' +
      'We treat instruction-based image restoration as a supervised learning problem similar to previous works [4]. First, we generate over 10000 prompts using GPT-4 based on our own sample instructions. We explain the creation of the prompt dataset in Sec. 3.1. We then build a large paired training dataset of prompts and degraded/clean images. Finally, we train our _InstructIR_ model, and we evaluate it on a wide variety of instructions including real human-written prompts. We explain our text encoder in Sec 3.2, and our complete model in Sec. 3.3.\n' +
      '\n' +
      '### Generating Prompts for Training\n' +
      '\n' +
      '**Why instructions?** Inspired by InstructPix2Pix [4], we adopt human written instructions as the mechanism of control for our model. There is no need for the user to provide additional information, such as example clean images, or descriptions of the visual content. Instructions offer a clear and expressive way to interact, enabling users to pinpoint the unpleasant effects (degradations) in the images.\n' +
      '\n' +
      'Handling free-form user prompts rather than fixed degradation-specific prompts increases the usability of our model for laypeople who lack domain expertise. We thus want our model to be capable of understanding diverse prompts posed by users "in-the-wild" _e.g_. kids, adults, or photographers. To this end, we use a large language model (_i.e_., GPT-4) to create diverse requests that might be asked by users for the different degradations types. We then filter those generated prompts to remove ambiguous or unclear prompts (_e.g_., _"Make the image cleaner", "improve this image "_). Our final instructions set contains over 10000 different prompts in total, for 7 different tasks. We display some examples in Table 1. As we show in Figure 2 the prompts are sampled randomly depending on the input degradation.\n' +
      '\n' +
      '### Text Encoder\n' +
      '\n' +
      'The Choice of the Text Encoder.A text encoder maps the user prompt to a fixed-size vector representation (a text embedding). The related methods for text-based image generation [67] and manipulation [3, 4] often use the text encoder of a CLIP model [62] to encode user prompts as CLIP excels in visual prompts. However, user prompts for degradation contain, in general, little to no visual content (_e.g_. the use describes the degradation, not the image itself), therefore, the large CLIP encoders (with over 60 million parameters) are not suitable - especially if we require efficiency.\n' +
      '\n' +
      'We opt, instead, to use a pure text-based sentence encoder [63], that is, a model trained to encode sentences in a semantically meaningful embedding space. Sentence encoders -pre-trained with millions of examples- are compact and fast in comparison to CLIP, while being able to encode the semantics of diverse user prompts. For instance, we use the BGE-micro-v2 sentence transformer.\n' +
      '\n' +
      'Fine-tuning the Text Encoder.We want to adapt the text encoder \\(\\mathrm{E}\\) for the restoration task to better encode the required information for the restoration model. Training the full text encoder is likely to lead to overfitting on our small training set and lead to loss of generalization. Instead, we freeze the text encoder and train a projection head on top:\n' +
      '\n' +
      '\\[\\mathbf{e}=\\mathrm{norm}(\\mathbf{W}\\cdot\\mathrm{E}(t)) \\tag{1}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Degradation** & **Prompts** \\\\ \\hline Denoising & Can you clean the dots from my image? \\\\  & Fix the grainy parts of this photo \\\\  & Remove the noise from my picture \\\\ \\hline Deblurring & Can you reduce the movement in the image? \\\\  & My picture’s not sharp, fix it \\\\  & Deblur my picture, it’s too fuzzy \\\\ \\hline Dehazing & Can you make this picture clearer? \\\\  & Help, my picture is all cloudy \\\\  & Remove the fog from my photo \\\\ \\hline Deraining & I want my photo to be clear, not rainy \\\\  & Clear the rain from my picture \\\\  & Remove the raindrops from my photo \\\\ \\hline Super-Res. & Make my photo bigger and better \\\\  & Add details to this image \\\\  & Increase the resolution of this photo \\\\ \\hline Low-light & The photo is too dark, improve exposure \\\\  & Increase the illumination in this shot \\\\  & My shot has very low dynamic range \\\\ \\hline Enhancement & Make it pop! \\\\  & Adjust the color balance for a natural look \\\\  & Apply a cinematic color grade to the photo \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Examples of our curated GPT4-generated user prompts with varying language and domain expertise.\n' +
      '\n' +
      'Figure 2: We train our blind image restoration models using common image datasets, and prompts generated using GPT-4, note that this is (self-)supervised learning. At inference time, our model generalizes to human-written instructions and restores real images.\n' +
      '\n' +
      'where \\(t\\) is the text, \\(\\mathrm{E}(t)\\) represents the raw text embedding, \\(\\mathbf{W}\\in\\mathbb{R}^{d_{t}\\times d_{v}}\\) is a learned projection from the text dimension (\\(d_{t}\\)) to the input dimension for the restoration model (\\(d_{v}\\)), and \\(\\mathrm{norm}\\) is the l2-norm.\n' +
      '\n' +
      'Figure 3 shows that while the text encoder is capable out-of-the-box to cluster instructions to some extent (Figure 2(a)), our trained projection yields greatly improved clusters (Figure 2(b)). We distinguish clearly the clusters for deraining, denoising, dehazing, deblurring, and low-light image enhancement. The instructions for such tasks or degradations are very characteristic. Furthermore, we can appreciate that "super-res" and "enhancement" tasks are quite spread and between the previous ones, which matches the language logic. For instance _"add details to this image"_ could be used for enhancement, deblurring or denossising. In our experiments, \\(d_{t}\\!=\\!384\\), \\(d_{v}\\!=\\!256\\) and \\(\\mathbf{W}\\) is a linear layer.\n' +
      '\n' +
      'Intent Classification Loss.We propose a guidance loss on the text embedding \\(\\mathbf{e}\\) to improve training and interpretability. Using the degradation types as targets, we train a simple classification head \\(\\mathcal{C}\\) such that \\(\\mathbf{c}=\\mathcal{C}(\\mathbf{e})\\), where \\(\\mathbf{c}\\in\\mathbb{R}^{D}\\), being \\(D\\) is the number of degradations. In our experiments \\(D=7\\) and the classification head \\(\\mathcal{C}\\) is a simple two-layers MLP. Thus, we only need to train a projection layer and a simple MLP to capture the natural language knowledge. This allows the text model to learn meaningful embeddings as we can appreciate in Figure 3, not just guidance vectors for the main image processing model.\n' +
      '\n' +
      'We find that the model is able to classify accurately (_i.e_. over 95% accuracy) the underlying degradation in the user\'s prompt after a few epochs.\n' +
      '\n' +
      '### InstructIR\n' +
      '\n' +
      'Our method _InstructIR_ consists of an image model and a text encoder. We introduced our text encoder in Sec. 3.2. We use NAFNet [10] as the image model, an efficient image restoration model that follows a U-Net architecture [68]. To successfully learn multiple tasks using a single model, we use task routing techniques. Our framework for training and evaluating the model is illustrated in Figure 2.\n' +
      '\n' +
      'Text Guidance.The key aspect of _InstructIR_ is the integration of the encoded instruction as a mechanism of control for the image model. Inspired in _task routing_ for many-task learning [14, 69, 71], we propose an _"Instruction Condition Block" (ICB)_ to enable task-specific transformations within the model. Conventional task routing [71] applies task-specific binary masks to the channel features. Since our model does not know _a-priori_ the degradation, we cannot use this technique directly.\n' +
      '\n' +
      'Considering the image features \\(\\mathcal{F}\\), and the encoded instruction \\(\\mathbf{e}\\), we apply task routing as follows:\n' +
      '\n' +
      '\\[\\mathcal{F^{\\prime}}_{c}=\\mathrm{Block}(\\mathcal{F}_{c}\\odot\\mathbf{m}_{c})+ \\mathcal{F}_{c} \\tag{2}\\]\n' +
      '\n' +
      'where the mask \\(\\mathbf{m}_{c}=\\sigma(\\mathbf{W}_{\\mathbf{c}}\\cdot\\mathbf{e})\\) is produced using a linear layer -activated using the Sigmoid function- to produce a set of weights depending on the text embedding \\(\\mathbf{e}\\). Thus, we obtain a \\(c\\)-dimensional per-channel (soft-)binary mask \\(\\mathbf{m}_{c}\\). As [71], task routing is applied as the channel-wise multiplication \\(\\odot\\) for masking features depending on the task. The conditioned features are further enhanced using a NAFBlock [10] (Block). We illustrate our task-routing ICB block in Figure 4. We use "regular" NAFBlocks [10], followed by ICBs to condition the features, at both encoder and decoder blocks. The formulation is \\(F^{l+1}=\\mathrm{ICB}(\\mathrm{Block}(F^{l}))\\) where \\(l\\) is the layer. Although we do not condition explicitly the filters of the neural network, as in [71], the mask allows the model to select the most relevant channels depending on the image information and the instruction. Note that this formulation enables differentiable feature masking, and certain interpretability _i.e_. the features with high weights contribute the most to the restoration process. Indirectly, this also enforces to learn diverse filters and reduce sparsity [14, 71].\n' +
      '\n' +
      'Figure 4: _Instruction Condition Block (ICB)_ using an approximation of task routing [71] for many-tasks learning. See Eq. 2.\n' +
      '\n' +
      'Figure 3: We show t-SNE plots of the text embeddings before/after training _InstructIR_. Each dot represents a human instruction.\n' +
      '\n' +
      'Is _InstructIR_ a blind restoration model?The model does not use explicit information about the degradation in the image _e.g_. noise profiles, blur kernels, or PSFs. Since our model infers the task (degradation) given the image and the instruction, we consider _InstructIR_ a _blind_ image restoration model. Similarly to previous works that use auxiliary image-based degradation classification [42, 60].\n' +
      '\n' +
      '## 4 Experimental Results\n' +
      '\n' +
      'We provide extensive qualitative results using benchmark images in Figures 17, 18 and 19. We also evaluate our model on 9 well-known benchmarks for different image restoration tasks: image denoising, deblurring, deraining, dehazing, and image enhancement. We present extensive quantitative results in Table 2. Our _single_ model successfully restores images considering different degradation types and levels. We provide additional results and ablation studies in the supplementary material.\n' +
      '\n' +
      '### Implementation Details.\n' +
      '\n' +
      'Our _InstructIR_ model is end-to-end trainable. The image model does not require pre-training, yet we use a pre-trained sentence encoder as language model.\n' +
      '\n' +
      'Text Encoder.As we discussed in Sec. 3.2, we only need to train the text embedding projection and classification head (\\(\\approx 100K\\) parameters). We initialize the text encoder with BGE-micro-v2 1, a distilled version of BGE-small-en [85]. The BGE encoders are BERT-like encoders [13] pre-trained on large amounts of supervised and unsupervised data for general-purpose sentence encoding. The BGE-micro model is a 3-layer encoder with 17.3 million parameters, which we freeze during training. We also explore all-MiniLM-L6-v2 and CLIP encoders, however, we concluded that small models prevent overfitting and provide the best performance while being fast. We provide the ablation study comparing the three text encoders in the supplementary material.\n' +
      '\n' +
      'Footnote 1: [https://huggingface.co/TaylorAI/bge-micro-v2](https://huggingface.co/TaylorAI/bge-micro-v2)\n' +
      '\n' +
      'Image Model.We use NAFNet [10] as image model. The architecture consists of a 4-level encoder-decoder, with varying numbers of blocks at each level, specifically [2, 2, 4, 8] for the encoder, and [2, 2, 2, 2] for the decoder, from the level-1 to level-4 respectively. Between the encoder and decoder we use 4 middle blocks to enhance further the features. The decoder implements addition instead of concatenation for the skip connections.\n' +
      '\n' +
      'We use the _Instruction Condition Block (ICB)_ for task-routing [71] only in the encoder and decoder.\n' +
      '\n' +
      'The model is optimized using the \\(\\mathcal{L}_{1}\\) loss between the ground-truth clean image and the restored one. Additionally we use the cross-entropy loss \\(\\mathcal{L}_{ce}\\) for the intent classification head of the text encoder. We train use a batch size of 32 and AdamW [36] optimizer with learning rate \\(5e^{-4}\\) for 500 epochs (approximately 1 day using a single NVIDIA A100). We also use cosine annealing learning rate decay. During training, we utilize cropped patches of size \\(256\\times 256\\) as input, and we use random horizontal and vertical flips as augmentations. Since our model uses as input instruction-image pairs, given an image, and knowing its degradation, we randomly sample instructions from our prompt dataset (\\(>10\\)K samples). Our image model has only 16M parameters, and the learned text projection is just \\(100\\)k parameters (the language model is 17M parameters), thus, our model can be trained easily on standard GPUs such as NVIDIA RTX 2080Ti or 3090Ti in a couple of days. Furthermore, the inference process also fits in low-computation budgets.\n' +
      '\n' +
      '### Datasets and Benchmarks\n' +
      '\n' +
      'Following previous works [42, 61, 100], we prepare the datasets for different restoration tasks.\n' +
      '\n' +
      'Image denoising.We use a combination of BSD400 [2] and WED [50] datasets for training. This combined training set contains \\(\\approx 5000\\) images. Using as reference the clean images in the dataset, we generate the noisy images by adding Gaussian noise with different noise levels \\(\\sigma\\in\\{15,25,50\\}\\). We test the models on the well-known BSD68 [52] and Urban100 [32] datasets.\n' +
      '\n' +
      'Image deraining.We use the Rain100L [88] dataset, which consists of 200 clean-rainy image pairs for training, and 100 pairs for testing.\n' +
      '\n' +
      'Image dehazing.We utilize the Reside (outdoor) SOTS [41] dataset, which contains \\(\\approx 72\\)K training images. However, many images are low-quality and unrealistic, thus, we filtered the dataset and selected a random set of 2000 images - also to avoid imbalance _w.r.t_ the other tasks. We use the standard _outdoor_ testset of 500 images.\n' +
      '\n' +
      'Image deblurring.We use the GoPro dataset for motion deblurring [57] which consist of 2103 images for training, and 1111 for testing.\n' +
      '\n' +
      'Low-light Image Enhancement.We use the LOL [83] dataset (v1), and we adopt its official split of 485 training images, and 15 testing images.\n' +
      '\n' +
      'Image Enhancement.Extending previous works, we also study photo-realistic image enhancement using the MIT5K dataset [5]. We use 1000 images for training, and the standard split of 500 images for testing (as in [74]).\n' +
      '\n' +
      'Finally, as previous works [42, 61, 100], we combine all the aforementioned training datasets, and we train our unified model for all-in-one restoration.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:6]\n' +
      '\n' +
      'on this task using the MIT5K [5] Dataset is notable, while keeping the performance on the other tasks. We achieve similar performance to classical task-specific methods.\n' +
      '\n' +
      'We **summarize** the multi-task ablation study in Table 6. Our model can tackle multiple tasks without losing performance notably thanks to the instruction-based task routing.\n' +
      '\n' +
      'Comparison with Task-specific MethodsOur main goal is to design a powerful all-in-one model, thus, _InstructIR_ was not designed to be trained for a particular degradation. Nevertheless, we also compare _InstructIR_ with task-specific methods _i.e_. models tailored and trained for specific tasks.\n' +
      '\n' +
      'We compare with task-specific methods for image enhancement in Table 5, and for low-light in image enhancement in 7. We provide extensive comparisons for image denoising in Table 8. Also, in Table 9 we show comparisons with classical methods for deblurring and dehazing. Our multi-task method is better than most task-specific methods, yet it is still not better than SOTA.\n' +
      '\n' +
      '## 6 On the Effectiveness of Instructions\n' +
      '\n' +
      'Thanks to our integration of human instructions, users can control how to enhance the images. We show an example in Figure 5, where the input image has three different degradations, and we aim to focus on a particular one. Although these results do not offer great reconstruction, we believe it is a promising direction that illustrates the effectiveness of instruction guidance for image restoration and enhancement. We provide more results in Figures 6 and 7, where we show the potential of our method to restore and enhance images in a controllable manner.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Tasks** & **Rain** & **Noise (\\(\\sigma 15\\))** & **Blur** & **LOL** \\\\ \\hline\n' +
      '3D & 37.98/0.978 & 31.52/0.890 & - & - \\\\\n' +
      '5D & 36.84/0.973 & 31.40/0.887 & 29.40/0.886 & 23.00/0.836 \\\\\n' +
      '6D & 36.80 /0.973 & 31.39 /0.888 & 29.73/0.892 & 22.83 / 0.836 \\\\\n' +
      '**7D** & 36.75 /0.972 & 31.37 /0.887 & 29.70/0.892 & 22.81 / 0.836 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Summary ablation study** on the multi-task variants of _InstructIR_ that tackle from 3 to 7 tasks. We report PSNR/SSIM.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Methods** & **Dehazing** & **Deraining** & **Denoising ablation study (BSD68 [52])** & **Average** \\\\  & SOTS [41] & Rain100L [21] & \\(\\sigma=15\\) & \\(\\sigma=25\\) & \\(\\sigma=50\\) & \\\\ \\hline BRDNet [72] & 23.23/0.895 & 27.42/0.895 & 32.26/0.898 & 29.76/0.836 & 26.34/0.836 & 27.80/0.843 \\\\ LPNet [25] & 20.84/0.828 & 24.88/0.784 & 26.47/0.778 & 24.77/0.748 & 21.26/0.552 & 23.64/0.738 \\\\ FDGAN [19] & 24.71/0.924 & 29.89/0.933 & 30.25/0.910 & 28.81/0.868 & 26.43/0.776 & 28.02/0.883 \\\\ MPRNet [94] & 25.28/0.954 & 33.57/0.954 & 33.54/0.927 & 30.89/0.880 & 27.56/0.779 & 30.17/0.899 \\\\ DL[21] & 26.92/0.391 & 32.62/0.931 & 33.05/0.914 & 30.41/0.861 & 26.90/0.740 & 29.98/0.875 \\\\ AirNet [42] & 27.94/0.962 & 34.90/0.967 & 33.92/0.933 & 31.26/0.888 & 28.00/0.797 & 31.20/0.910 \\\\ PromptIR [61] & **30.58/0.974** & 36.37/0.972 & 33.98/0.933 & 31.31/0.888 & 28.06/0.799 & 32.06/0.913 \\\\ _InstructIR_-3D & 30.22/0.959 & **37.98/0.978** & **34.15/0.933** & **31.52/0.890** & **28.30/0.804** & **32.43/0.913** \\\\ _InstructIR_-5D & 27.10/0.956 & 36.84/0.973 & 34.00/0.931 & 31.40/0.887 & 28.15/0.798 & 31.50/0.909 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Comparisons of all-in-one restoration models for **three restoration tasks** (3D). We also show an ablation study for image denoising -the fundamental inverse problem- considering different noise levels. We report PSNR/SSIM metrics. Table based on [61].\n' +
      '\n' +
      'Figure 5: **Selective task**. _InstructIR_ can remove particular degradations or perform different transformations depending on the human instructions. This is a novel feature in image restoration, and it is possible thanks to the novel integration of textual descriptions.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Method** & **PSNR \\(\\uparrow\\)** & **SSIM \\(\\uparrow\\)** & \\(\\Delta E_{ab}\\downarrow\\) \\\\ \\hline UPE [77] & 21.88 & 0.853 & 10.80 \\\\ DPE [26] & 23.75 & 0.908 & 9.34 \\\\ HDRNet [11] & 24.32 & 0.912 & 8.49 \\\\\n' +
      '3DLLUT [96] & **25.21** & **0.922** & **7.61** \\\\ _InstructIR_-**7D** & 24.65 & 0.900 & 8.20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Image Enhancement** performance on MIT5K [5, 96].\n' +
      '\n' +
      'Figure 6: **Instruction-based Image Restoration**. _InstructIR_ understands a wide a range of instructions for a given task (first row). Given an _adversarial instruction_ (second row), the model performs an identity –we did not enforce this during training–. Images from BSD68 [52].\n' +
      '\n' +
      'Figure 7: **Multiple Real Instructions**. We can prompt multiple instructions (in sequence) to restore and enhance the images. This provides additional _control_. We show two examples of multiple instructions applied to the “Input” image -from left to right-.\n' +
      '\n' +
      'This implies an advancement _w.r.t_ classical (deterministic) image restoration methods. Classical deep restoration methods lead to a unique result, thus, they do not allow to control how the image is processed. We also compare _InstructIR_ with InstructPix2Pix [4] in Figure 8.\n' +
      '\n' +
      'Qualitative Results.We provide diverse qualitative results for several tasks. In Figure 9, we show results on the LOL [83] dataset. In Figure 10, we compare methods on the motion deblurring task using the GoPro [57] dataset. In Figure 11, we compare with different methods for the dehazing task on SOTS (outdoor) [41]. In Figure 12, we compare with image restoration methods for deraining on Rain100L [21]. Finally, we show denoising results in Figure 13. In this qualitative analysis, we use our single _InstructIR_-**5D** model to restore all the images.\n' +
      '\n' +
      'Discussion on Instruction-based RestorationIn Figure 8 we compare with InstructPix2Pix [4]. Our method is notably superior in terms of efficiency, fidelity and quality. We can conclude that diffusion-based methods [4, 53, 67] for image manipulation require complex "tuning" of several (hyper-)parameters, and strong regularization to enforce fidelity and reduce hallucinations. InstructPix2Pix [4] cannot solve inverse problems directly -although it has a good prior for solving Inpainting-, which indicates that such model require restoration-specific training (or fine-tuning).\n' +
      '\n' +
      'LimitationsOur method achieves _state-of-the-art_ results in five tasks, proving the potential of using instructions to guide deep blind restoration models. However, we acknowledge certain limitations. First, in comparison to diffusion-based restoration methods, our current approach would not produce better results attending to perceptual quality. Second, our model struggles to process images with more than one degradation (_real-world_ images), yet this is a common limitation among the related restoration methods. Third, as previous _all-in-one_ methods, our model only works with _in-distribution degradations_, thus it will not work on unseen artifacts. Nevertheless, these limitations can be surpassed with more realistic training data.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'We present the first approach that uses human-written instructions to guide the image restoration models. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. InstructIR achieves state-of-the-art results on several restoration tasks, demonstrating the power of instruction guidance. These results represent a novel benchmark for text-guided image restoration.n\n' +
      '\n' +
      'AcknowledgmentsThis work was partly supported by the The Humboldt Foundation (AvH). Marcos Conde is also supported by Sony Interactive Entertainment, FTG.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & LPNet & URetinex [43] & DeepLPF [55] & SCI [51] & LIME [27] & MF [23] & NPE [78] & SRIE [24] & SDD [28] & CDEF [40] & _InstructIR_ \\\\ \\hline\n' +
      '**PSNR \\(\\uparrow\\)** & 21.46 & 21.32 & 15.28 & 15.80 & 16.76 & 16.96 & 16.96 & 11.86 & 13.34 & 16.33 & 22.83 \\\\\n' +
      '**SSIM \\(\\uparrow\\)** & 0.802 & 0.835 & 0.473 & 0.527 & 0.444 & 0.505 & 0.481 & 0.493 & 0.635 & 0.583 & 0.836 \\\\ \\hline \\hline \\multirow{2}{*}{**Method**} & DRBN & KinD & RUAS & FIDE & EG & MS-RDN & Retinex & MIRNet & IPT & Uformer & **IAGC** \\\\  & [89] & [107] & [46] & [86] & [33] & [90] & -Net[83] & [93] & [8] & [82] & [81] \\\\ \\hline\n' +
      '**PSNR \\(\\uparrow\\)** & 20.13 & 20.87 & 18.23 & 18.27 & 17.48 & 17.20 & 16.77 & 24.14 & 16.27 & 16.36 & **24.53** \\\\\n' +
      '**SSIM \\(\\uparrow\\)** & 0.830 & 0.800 & 0.720 & 0.665 & 0.650 & 0.640 & 0.560 & 0.830 & 0.504 & 0.507 & **0.842** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Quantitative comparisons with _state-of-the-art_ methods on the **LOL dataset**[83] **(low-light enhancement)**. Table based on [81].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c||c} \\hline \\hline\n' +
      '**Deblurring GoPro**[57] & \\multicolumn{2}{c}{**Dehazing SOTS**[41]} \\\\\n' +
      '**Method** & **PSNR/SSIM** & **Method** & **PSNR/SSIM** \\\\ \\hline Xu _et al._[87] & 21.00/0.741 & DehazeNet [6] & 22.46/0.851 \\\\ DeblurGAN [38] & 28.70/0.858 & GFN [65] & 21.55/0.844 \\\\ Nah _et al._[57] & 29.08/0.914 & GCANet [7] & 19.98/0.704 \\\\ RNN [99] & 29.19/0.931 & MSBDN [17] & 23.36/0.875 \\\\ DeblurGAN-\\(V2\\)[39] & 29.55/0.934 & DuRN [47] & 24.47/0.839 \\\\ _InstructIR_-**5D** & 29.40/0.886 & _InstructIR_-**5D** & 27.10/0.956 \\\\ _InstructIR_-**5D** & **29.73**/**0.892** & _InstructIR_-**3D** & **30.22**/**0.959** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Comparison with general restoration and all-in-one methods (*) at **image denoising**. We report PSNR on benchmark datasets considering different \\(\\sigma\\) noise levels. Table based on [100].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c||c} \\hline \\hline\n' +
      '**Deblurring GoPro**[57] & \\multicolumn{2}{c}{**Dehazing SOTS**[41]} \\\\\n' +
      '**Method** & **PSNR/SSIM** & **Method** & **PSNR/SSIM** \\\\ \\hline Xu _et al._[87] & 21.00/0.741 & DehazeNet [6] & 22.46/0.851 \\\\ DeblurGAN [38] & 28.70/0.858 & GFN [65] & 21.55/0.844 \\\\ Nah _et al._[57] & 29.08/0.914 & GCANet [7] & 19.98/0.704 \\\\ RNN [99] & 29.19/0.931 & MSBDN [17] & 23.36/0.875 \\\\ DeblurGAN-\\(V2\\)[39] & 29.55/0.934 & DuRN [47] & 24.47/0.839 \\\\ _InstructIR_-**5D** & 29.40/0.886 & _InstructIR_-**5D** & 27.10/0.956 \\\\ _InstructIR_-**5D** & **29.73**/**0.892** & _InstructIR_-**3D** & **30.22**/**0.959** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Deblurring and Dehazing comparisons. We compare with task-specific classical methods on benchmark datasets.**Figure 8: **Comparison with InstructPix2Pix [4] for instruction-based restoration using the prompt. Images from the _RealSRet_[44, 80]. We use our 7D variant. We run InstructPix2Pix [4] using two configurations where we vary the weight of the image component hoping to improve fidelity: \\(S_{I}=5\\) and \\(S_{I}=7\\) (also known as Image CFG), this parameters helps to enforce fidelity and reduce hallucinations.**\n' +
      '\n' +
      'Figure 11: **Image Dehazing Results.** Comparison with other methods on SOTS [41] _outdoor_ (0150.jpg).\n' +
      '\n' +
      'Figure 12: **Image Deraining Results** on Rain100L [21] (035.png).\n' +
      '\n' +
      'Figure 13: **Image Denoising Results** on BSD68 [52] (0060.png).\n' +
      '\n' +
      'Figure 10: **Image Deblurring Results.** Comparison with other methods on the GoPro [57] dataset (GOR0854-11-00-00001.png).\n' +
      '\n' +
      'Figure 9: **Low-light Image Enhancement Results.** We compare with other methods on LOL [83] (748.png).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      'Figure 19: **Dehazing comparisons** for all-in-one methods on images from the SOTS outdoor dataset [41].\n' +
      '\n' +
      'Figure 16: **Comparison with InstructPix2Pix [4]** for instruction-based restoration using the prompt _“Remove the noise in this photo”_.\n' +
      '\n' +
      'Figure 17: **Denoising results** for all-in-one methods. Images from BSD68 [52] with noise level \\(\\sigma=25\\).\n' +
      '\n' +
      'Figure 18: **Image deraining comparisons** for all-in-one methods on images from the Rain100L dataset [21].\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]E. Agustsson and R. Timofte (2017) NTIRE 2017 challenge on single image super-resolution: dataset and study. In CVPR Workshops, Cited by: SS1, SS2.1, SS3.1, SS3.2.\n' +
      '* [2]P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik (2011) Contour detection and hierarchical image segmentation. TPAMI. Cited by: SS1.\n' +
      '* [3]Y. Bai, C. Wang, S. Xie, C. Dong, C. Yuan, and Z. Wang (2023) Textir: a simple framework for text-based editable image restoration. CoRRabs/2302.14736. Cited by: SS1.\n' +
      '* [4]T. Brooks, A. Holynski, and A. A. Efros (2023) Instructpix2pix: learning to follow image editing instructions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 18392-18402. Cited by: SS1.\n' +
      '* [5]V. Bychkovsky, S. Paris, E. Chan, and F. Durand (2011) Learning photographic global tonal adjustment with a database of input / output image pairs. In The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '* [6]B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao (2016) Dehazenet: an end-to-end system for single image haze removal. IEEE Transactions on Image Processing25 (11), pp. 5187-5198. Cited by: SS1.\n' +
      '* [7]D. Chen, M. He, Q. Fan, J. Liao, L. Zhang, D. Hou, L. Yuan, and G. Hua (2019) Gated context aggregation network for image dehazing and deraining. In 2019 IEEE winter conference on applications of computer vision (WACV), pp. 1375-1383. Cited by: SS1.\n' +
      '* [8]H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, and W. Gao (2021) Pre-trained image processing transformer. In CVPR, Cited by: SS1.\n' +
      '* [9]L. Chen, X. Lu, J. Zhang, X. Chu, and C. Chen (2021) Hinet: half instance normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 182-192. Cited by: SS1.\n' +
      '* [10]L. Chen, X. Chu, X. Zhang, and J. Sun (2022) Simple baselines for image restoration. In ECCV, Cited by: SS1.\n' +
      '* [11]Y. Chen, Y. Wang, M. Kao, and Y. Chuang (2018) Deep photo enhancer: unpaired learning for image enhancement from photographs with gans. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6306-6314. Cited by: SS1.\n' +
      '* [12]V. Cornillere, A. Djelouah, W. Yifan, O. Sorkine-Hornung, and C. Schroers (2019) Blind image super-resolution with spatially variant degradations. ACM Transactions on Graphics (TOG)38 (6), pp. 1-13. Cited by: SS1.\n' +
      '* [13]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019) BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-4186. Cited by: SS1.\n' +
      '* [14]C. Ding, Z. Lu, S. Wang, R. Cheng, and V. Naresh Boddeti (2023) Mitigating task interference in multi-task learning via explicit task routing with non-learnable primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7756-7765. Cited by: SS1.\n' +
      '* [15]C. Dong, C. C. Loy, K. He, and X. Tang (2015) Image super-resolution using deep convolutional networks. TPAMI. Cited by: SS1.\n' +
      '* [16]H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M. Yang (2020) Multi-scale boosted dehazing network with dense feature fusion. In CVPR, Cited by: SS1.\n' +
      '* [17]H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M. Yang (2020) Multi-scale boosted dehazing network with dense feature fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2157-2167. Cited by: SS1.\n' +
      '* [18]W. Dong, L. Zhang, G. Shi, and X. Wu (2011) Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. TIP. Cited by: SS1.\n' +
      '* [19]Y. Dong, Y. Liu, H. Zhang, S. Chen, and Y. Qiao (2020) Fd-gan: generative adversarial networks with fusion-discriminator for single image dehazing. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10729-10736. Cited by: SS1.\n' +
      '* [20]M. Elad and A. Feuer (1997) Restoration of a single superresolution image from several blurred, noisy, and undersampled measured images. IEEE transactions on image processing6 (12), pp. 1646-1658. Cited by: SS1.\n' +
      '* [21]Q. Fan, D. Chen, L. Yuan, G. Hua, N. Yu, and B. Chen (2019) A general decoupled learning framework for parameterized image operators. IEEE transactions on pattern analysis and machine intelligence43 (1), pp. 33-47. Cited by: SS1.\n' +
      '* [22]R. Franzen (1999) Kodak lossless true color image suite. Note: [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/) Cited by: SS1.\n' +
      '* [23]X. Fu, D. Zeng, Y. Huang, Y. Liao, X. Ding, and J. Paisley (2016) A fusion-based enhancing method for weakly illuminated images. 129, pp. 82-96. Cited by: SS1.\n' +
      '* [24]X. Fu, D. Zeng, Y. Huang, X. Zhang, and X. Ding (2016) A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, Cited by: SS1.\n' +
      '* [25]H. Gao, X. Tao, X. Shen, and J. Jia (2019) Dynamic scene deblurring with parameter selective sharing and nested skip connections. In CVPR, pp. 3848-3856. Cited by: SS1.\n' +
      '* [26]M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand (2019) Deep bilaterallearning for real-time image enhancement. _ACM Transactions on Graphics (TOG)_, 36(4):1-12, 2017.\n' +
      '* [27] Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. _IEEE TIP_, 26(2):982-993, 2016.\n' +
      '* [28] Shijie Hao, Xu Han, Yanrong Guo, Xin Xu, and Meng Wang. Low-light image enhancement with semi-decoupled decomposition. _IEEE TMM_, 22(12):3025-3038, 2020.\n' +
      '* [29] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. _TPAMI_, 2010.\n' +
      '* [30] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [31] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In _ICCV_, 2019.\n' +
      '* [32] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5197-5206, 2015.\n' +
      '* [33] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. _IEEE TIP_, 30:2340-249, 2021.\n' +
      '* [34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.\n' +
      '* [35] Kwang In Kim and Younghee Kwon. Single-image super-resolution using sparse regression and natural image prior. _TPAMI_, 2010.\n' +
      '* [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv:1412.6980_, 2014.\n' +
      '* [37] Johannes Kopf, Boris Neubert, Billy Chen, Michael Cohen, Daniel Cohen-Or, Oliver Deussen, Matt Uyttendaele, and Dani Lischinski. Deep photo: Model-based photograph enhancement and viewing. _ACM TOG_, 2008.\n' +
      '* [38] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. DeblurGAN: Blind motion deblurring using conditional adversarial networks. In _CVPR_, 2018.\n' +
      '* [39] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. DeblurGAN-v2: Deblurring (orders-of-magnitude) faster and better. In _ICCV_, 2019.\n' +
      '* [40] Xiaozhou Lei, Zixiang Fei, Wenju Zhou, Huiyu Zhou, and Minrui Fei. Low-light image enhancement using the cell vibration model. _IEEE TMM_, pages 1-1, 2022.\n' +
      '* [41] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking single-image dehazing and beyond. _IEEE Transactions on Image Processing_, 28(1):492-505, 2018.\n' +
      '* [42] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jianchao Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In _CVPR_, pages 17452-17462, 2022.\n' +
      '* [43] Jiaqian Li, Juncheng Li, Faming Fang, Fang Li, and Guixu Zhang. Luminance-aware pyramid network for low-light image enhancement. _IEEE TMM_, 23:3153-3165, 2020.\n' +
      '* [44] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwiniR: Image restoration using swin transformer. In _ICCV Workshops_, 2021.\n' +
      '* [45] Lin Liu, Lingxi Xie, Xiaopeng Zhang, Shankin Yuan, Xiangyu Chen, Wengang Zhou, Houqiang Li, and Qi Tian. Tape: Task-agnostic prior embedding for image restoration. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVIII_, pages 447-464. Springer, 2022.\n' +
      '* [46] Risheng Liu, Long Ma, Jiao Zhang, Xin Fan, and Zhongxuan Luo. Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. In _CVPR_, 2021.\n' +
      '* [47] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7007-7016, 2019.\n' +
      '* [48] Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan. Learning the degradation distribution for blind image super-resolution. In _CVPR_, pages 6063-6072, 2022.\n' +
      '* [49] Jiaqi Ma, Tianheng Cheng, Guoli Wang, Qian Zhang, Xinggang Wang, and Lefei Zhang. Prores: Exploring degradation-aware visual prompt for universal image restoration. _arXiv preprint arXiv:2306.13653_, 2023.\n' +
      '* [50] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. _TIP_, 2016.\n' +
      '* [51] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image enhancement. In _CVPR_, 2022.\n' +
      '* [52] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _ICCV_, 2001.\n' +
      '* [53] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [54] Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In _ICCV_, 2013.\n' +
      '* [55] Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory Slabaugh. Deepplf: Deep local parametric filters for image enhancement. In _CVPR_, 2020.\n' +
      '* [56] Chong Mou, Qian Wang, and Jian Zhang. Deep generalized unfolding networks for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17399-17410, 2022.\n' +
      '* [57] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _CVPR_, 2017.\n' +
      '* [58] Seungjun Nah, Sanghyun Son, Jaerin Lee, and Kyoung Mu Lee. Clean images are hard to reblur: Exploiting the ill-posed inverse task for dynamic scene deblurring. In _ICLR_, 2022.\n' +
      '* [59] Nhat Nguyen, Peyman Milanfar, and Gene Golub. Efficient generalized cross-validation with applications to parametric image restoration and resolution enhancement. _IEEE Transactions on image processing_, 10(9):1299-1308, 2001.\n' +
      '* [60] Dongwon Park, Byung Hyun Lee, and Se Young Chun. All-in-one image restoration for unknown degradations using adaptive discriminative filters for specific degradations. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5815-5824. IEEE, 2023.\n' +
      '* [61] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one blind image restoration. _arXiv preprint arXiv:2306.13090_, 2023.\n' +
      '* [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, pages 8748-8763. PMLR, 2021.\n' +
      '* [63] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 3980-3990. Association for Computational Linguistics, 2019.\n' +
      '* [64] Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao. Adaptive consistency prior based deep network for image denoising. In _CVPR_, 2021.\n' +
      '* [65] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang. Gated fusion network for single image dehazing. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3253-3261, 2018.\n' +
      '* [66] Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, and Ming-Hsuan Yang. Single image dehazing via multi-scale convolutional neural networks with holistic edges. _IJCV_, 2020.\n' +
      '* [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE, 2022.\n' +
      '* [68] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.\n' +
      '* [69] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. _arXiv preprint arXiv:1711.01239_, 2017.\n' +
      '* [70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [71] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring. Many task learning with task routing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1375-1384, 2019.\n' +
      '* [72] Chunwei Tian, Yong Xu, and Wangmeng Zuo. Image de-noising using deep cnn with batch renormalization. _Neural Networks_, 2020.\n' +
      '* [73] Radu Timofte, Vincent De Smet, and Luc Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In _ICCV_, 2013.\n' +
      '* [74] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. MAXIM: Multi-axis MLP for image processing. In _CVPR_, pages 5769-5780, 2022.\n' +
      '* [75] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal M Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions. In _CVPR_, pages 2353-2363, 2022.\n' +
      '* [76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [77] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6849-6857, 2019.\n' +
      '* [78] Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. _IEEE TIP_, 22(9):3538-3548, 2013.\n' +
      '* [79] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In _CVPR_, 2018.\n' +
      '* [80] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. ESRGAN: enhanced super-resolution generative adversarial networks. In _ECCV Workshops_, 2018.\n' +
      '* [81] Yinglong Wang, Zhen Liu, Jianzhuang Liu, Songcen Xu, and Shuaicheng Liu. Low-light image enhancement with illumination-aware gamma correction and complete image modelling network. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13128-13137, 2023.\n' +
      '* [82] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and Jianzhuang Liu. Uformer: A general u-shaped transformer for image restoration. _arXiv:2106.03106_, 2021.\n' +
      '\n' +
      '* [83] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In _British Machine Vision Conference_, 2018.\n' +
      '* [84] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wenhan Yang, and Jianmin Jiang. Uretinex-net: Retinex-based deep unfolding network for low-light image enhancement. In _CVPR_, 2022.\n' +
      '* [85] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-packaged resources to advance general chinese embedding. _CoRR_, abs/2309.07597, 2023.\n' +
      '* [86] Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau. Learning to restore low-light images via decomposition-and-enhancement. In _CVPR_, 2020.\n' +
      '* [87] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse representation for natural image deblurring. In _CVPR_, 2013.\n' +
      '* [88] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network for image super-resolution. In _CVPR_, 2020.\n' +
      '* [89] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. Band representation-based semi-supervised low-light image enhancement: bridging the gap between signal fidelity and perceptual quality. _IEEE TIP_, 30:3461-3473, 2021.\n' +
      '* [90] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. _IEEE TIP_, 30:2072-2086, 2021.\n' +
      '* [91] Mingde Yao, Ruikang Xu, Yuanshen Guan, Jie Huang, and Zhiwei Xiong. Neural degradation representation learning for all-in-one image restoration. _arXiv preprint arXiv:2310.12848_, 2023.\n' +
      '* [92] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10819-10829, 2022.\n' +
      '* [93] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In _ECCV_, 2020.\n' +
      '* [94] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In _CVPR_, 2021.\n' +
      '* [95] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In _CVPR_, 2022.\n' +
      '* [96] Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang. Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(4):2058-2073, 2020.\n' +
      '* [97] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. _arXiv preprint arXiv:2308.03021_, 2023.\n' +
      '* [98] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 2285-2293, 2023.\n' +
      '* [99] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Linchao Bao, Rynson WH Lau, and Ming-Hsuan Yang. Dynamic scene deblurring using spatially variant recurrent neural networks. In _CVPR_, 2018.\n' +
      '* [100] Jinghao Zhang, Jie Huang, Mingde Yao, Zizheng Yang, Hu Yu, Man Zhou, and Feng Zhao. Ingredient-oriented multi-degradation learning for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5825-5835, 2023.\n' +
      '* [101] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _TIP_, 2017.\n' +
      '* [102] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.\n' +
      '* [103] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior for image restoration. In _CVPR_, 2017.\n' +
      '* [104] Kai Zhang, Wangmeng Zuo, and Lei Zhang. FFDNet: Toward a fast and flexible solution for CNN-based image denoising. _TIP_, 2018.\n' +
      '* [105] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realistic blurring. In _CVPR_, pages 2737-2746, 2020.\n' +
      '* [106] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.\n' +
      '* [107] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In _ACM MM_, 2019.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>