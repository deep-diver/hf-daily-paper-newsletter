<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '인적 지시에 따른 고품질 이미지 복원\n' +
      '\n' +
      '마르코스 V Conde\\({}^{1,2}\\), Gregor Geigle\\({}^{1}\\), Radu Timofte\\({}^{1}\\)\n' +
      '\n' +
      'Wurzburg University CAIDAS & IFI 컴퓨터 비전 연구실\n' +
      '\n' +
      '소니 플레이스테이션, FTG\n' +
      '\n' +
      '[https://github.com/mv-lab/InstructIR](https://github.com/mv-lab/InstructIR)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '이미지 복원은 열화 관측으로부터 고품질의 깨끗한 이미지를 복구하는 것을 포함하는 근본적인 문제이다. 올인원 이미지 복원 모델은 복원 모델을 안내하라는 프롬프트로서 열화 특정 정보를 사용하여 다양한 유형 및 열화 레벨로부터 이미지를 효과적으로 복원할 수 있다. 본 논문에서는 이미지 복원 모델을 안내하기 위해 사람이 작성한 명령어를 사용하는 첫 번째 접근 방법을 제시한다. 자연 언어 프롬프트가 주어지면, 본 모델은 여러 가지 열화 유형을 고려하여 저하된 대응물로부터 고품질 이미지를 복구할 수 있다. 본 논문에서 제안하는 InstructIR 기법은 영상 잡음 제거, 잡음 제거, 디블러링, 안개 제거, (저조도) 영상 개선을 포함한 여러 복원 작업에서 최신의 결과를 얻을 수 있다. InstructIR은 기존의 올인원 복원 방법에 비해 +1dB 개선되었다. 또한, 데이터 셋과 결과는 텍스트 유도 이미지 복원 및 개선에 대한 새로운 연구를 위한 새로운 벤치마크를 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '이미지는 종종 노이즈, 모션 블러, 헤이즈, 및 낮은 동적 범위와 같은 불쾌한 효과를 포함한다. 이러한 효과는 일반적으로 낮은 수준의 컴퓨터 비전에서는 _degradations_로 알려져 있다. 이는 카메라 제한 또는 까다로운 환경 조건_e.g_로 인해 발생할 수 있다. 낮은 불빛\n' +
      '\n' +
      '이미지 복원은 손상된 이미지에서 고품질 이미지를 복구하는 것을 목표로 한다. 이것은 임의의 주어진 이미지를 복원하기 위해 다수의 상이한 해들이 존재할 수 있기 때문에 복잡한 역문제이다[16, 20, 44, 59, 102, 103].\n' +
      '\n' +
      '일부 방법은 특정 분해, 예를 들어 노이즈 감소(노이즈 제거) [64, 102, 103], 블러 제거(디블러링) [58, 105], 또는 헤이즈 제거(헤이즈 제거) [16, 66]에 초점을 맞춘다. 이러한 방법은 특정 작업에 효과적이지만 다른 유형의 저하에 잘 일반화되지 않는다. 다른 접근법들은 다양한 태스크들[10, 74, 82, 95]에 대해 일반적인 신경망을 사용하지만, 각각의 특정 태스크에 대해 신경망을 독립적으로 트레이닝한다. 각각의 가능한 열화에 대해 별도의 모델을 사용하는 것은 자원 집약적이기 때문에, 최근의 접근법들은 _All-in-One_ 복원 모델들[42, 60, 61, 100]을 제안한다. 이러한 접근 방식은 다중 열화 유형 및 수준을 고려한 단일 딥 블라인드 복원 모델을 사용한다. 프롬프트IR[61]이나 프로레스[49]와 같은 현대 저작물은 텍스트 형태의 원시 사용자 프롬프트와 달리 학습된 유도 벡터(prompt _embeddings_)를 사용하여 블라인드 이미지 복원을 위한 통합 모델을 활용한다.\n' +
      '\n' +
      '이와 병행하여 InstructPix2Pix[4]와 같은 최근 작품들은 텍스트 프롬프트를 이용하여 이미지 생성 및 편집 모델을 안내할 수 있는 가능성을 보여주고 있다. 그러나, 이러한 방법(또는 최근의 대안들)은 역문제를 해결하지 않는다. 이러한 연구들에서 영감을 받아, 우리는 텍스트 지침이 이전 작업들에서 사용된 이미지 기반 열화 분류보다 블라인드 복원 모델을 더 잘 안내하는 데 도움이 될 수 있다고 주장한다[42, 60, 100]. 사용자는 일반적으로 (도메인 특정 어휘가 부족할 수 있지만) 수정해야 하는 것에 대한 아이디어를 가지고 있으므로 이 정보를 사용하여 모델을 안내할 수 있다.\n' +
      '\n' +
      '본 논문에서는 역 문제와 이미지 복원을 해결하기 위해 실제 사람이 작성한 명령어를 활용하는 첫 번째 방법을 제안한다. 우리의 포괄적인 실험은 이미지 노이즈 제거, 데라잉, 디블러링, 데아징 및 저조도 이미지 개선을 포함한 다양한 이미지 복원 작업에서 _state-of-the-art_ 성능을 달성함으로써 이미지 복원 및 향상을 위한 텍스트 안내를 사용할 수 있는 가능성을 보여준다. 우리의 모델인 _InstructIR_는 임의의 사람이 작성한 명령어를 사용하여 이미지를 복원하는 것으로 일반화할 수 있다. 또한, 우리의 단일 _all-in-one_ 모델은 많은 이전 작업보다 더 많은 작업을 다룬다. 우리는 그림 1에서 우리의 방법의 다양한 복원 샘플을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이미지 복원.최근의 딥 러닝 방법들[16, 44, 58, 64, 74, 95]은 블라인드 이미지 복원을 위한 전통적인 기법들[18, 29, 35, 37, 54, 73]에 비해 일관되게 더 나은 결과들을 보여주었다. 제안된 신경망은 컨볼루션 신경망(CNN) 및 트랜스포머[76](또는 관련 주의 메커니즘)에 기초한다.\n' +
      '\n' +
      '우리는 범용 복원 모델[10, 44, 82, 95]에 초점을 맞춘다. 예를 들어, SwinIR[44], MAXIM[74] 및 Uformer[82]. 이러한 모델은 잡음 제거, 잡음 제거 또는 디블러링과 같은 다양한 작업에 대해 독립적으로 훈련될 수 있다. 로컬 및 글로벌 기능 상호 작용을 포착하고 이를 향상시키는 능력은 모델이 다양한 작업에서 일관되게 우수한 성능을 달성할 수 있도록 합니다. 예를 들어, Restormer[95]는 비 로컬 블록[79]을 사용하여 이미지 전체에 걸쳐 복잡한 피쳐를 캡처합니다.\n' +
      '\n' +
      'NAFNet[10]은 복잡한 변압기 기반 방법에 대한 효율적인 대안이다. 이 모델은 비선형 활성화의 대안으로 단순화된 채널 주의와 게이팅을 사용한다. 빌링 블록(NAFBlock)은 효율적인 역 잔차 블록을 갖는 간단한 메타-포머[92] 아키텍처를 따른다[31]. 본 논문에서는 NAFNet을 백본으로 사용하는 _InstructIR_ 모델을 구축한다.\n' +
      '\n' +
      'All-in-One Image Restoration.Single degradation (또는 single task) 복원 방법은 잘 연구되고 있으나, 필요한 자원으로 인해 실제 응용에 한계가 있다. 다른 모델을 할당하고 필요한 경우 적절한 모델을 선택합니다. 더욱이, 노이즈 및 블러와 같은 이미지는 단일 열화를 거의 나타내지 않으며, 모든 이미지 캡처에서 거의 유비쿼터스하다.\n' +
      '\n' +
      '올인원(다중 분해 또는 다중 작업이라고도 함) 이미지 복원은 저수준의 컴퓨터 비전[42, 49, 60, 61, 75, 91, 97, 98]에서 새로운 연구 분야로 부상하고 있다. 이러한 접근 방식은 다양한 열화 유형 및 수준을 해결하기 위해 단일 딥 블라인드 복원 모델을 사용한다.\n' +
      '\n' +
      '우리는 참조 AirNet[42], IDR[100] 및 ADMS[60]로 사용한다. 우리는 또한 현대 작품 PromptIR[61]을 고려한다. 방법들은 복원 과정에서 블라인드 모델을 안내하기 위해 다른 기법들을 사용한다. 예를 들어, 열화 분류를 위한 보조 모델[42, 60], 또는 모델이 이미지에서 다른 유형의 열화를 구별하는 데 도움이 되는 다차원 유도 벡터(또한 "프롬프트"라고도 함)[49, 61]이다.\n' +
      '\n' +
      '이 연구의 초점이 아님에도 불구하고, 우리는 모델들이 다중 분해(블러, 노이즈 및 다운샘플링)를 고려한 역 문제를 해결하는 것을 목표로 하기 때문에, _실세계 이미지 슈퍼-해상도_가 관련 문제임을 인정한다[12, 44, 48, 106].\n' +
      '\n' +
      '텍스트-유도 이미지 조작.최근에는 텍스트-대-이미지 생성 및 텍스트-기반 이미지 편집 작업[4, 30, 34, 53, 70]을 위한 다수의 방법들이 제안되고 있다. 이러한 모델은 텍스트 프롬프트를 사용하여 이미지 또는 동작을 설명하고 해당 이미지를 생성하기 위한 강력한 확산 기반 모델을 사용한다. 우리의 주요 참조는 InstructPix2Pix[4]이며, 이 방법은 입력 또는 출력 이미지의 텍스트 라벨, 캡션 또는 설명과 대조적으로 모델에 어떤 동작을 수행할지 알려주는 _instructions_로부터의 편집을 가능하게 한다. 따라서, 사용자는 추가적인 이미지 설명 또는 샘플 참조 이미지를 제공할 필요 없이, 자연스러운 필기 텍스트로 무엇을 할지를 전송할 수 있다.\n' +
      '\n' +
      '##3 영상복원 후 지시사항\n' +
      '\n' +
      '우리는 지도 기반 이미지 복원을 이전 작업과 유사한 지도 학습 문제로 취급한다[4]. 먼저, 자체 샘플 지침에 따라 GPT-4를 사용하여 10000개 이상의 프롬프트를 생성합니다. 우리는 Sec. 3.1에서 프롬프트 데이터 세트의 생성을 설명하고, 프롬프트와 열화/깨끗한 이미지의 대규모 쌍을 이루는 학습 데이터 세트를 구축한다. 마지막으로, 우리는 우리의 _InstructIR_ 모델을 훈련시키고, 실제 사람이 작성한 프롬프트를 포함한 매우 다양한 명령어에 대해 평가한다. 우리는 Sec 3.2에서 텍스트 인코더를 설명하고 Sec 3.3에서 완전한 모델을 설명한다.\n' +
      '\n' +
      '### 트레이닝을 위한 프롬프트 생성\n' +
      '\n' +
      '**왜 지시사항?** InstructPix2Pix[4]에서 영감을 받아 인간의 서면 지시사항을 모델에 대한 제어 메커니즘으로 채택합니다. 사용자가 예를 들어 깨끗한 이미지들, 또는 시각적 콘텐츠의 설명들과 같은 추가적인 정보를 제공할 필요가 없다. 지침은 상호 작용하는 명확하고 표현적인 방법을 제공하여 사용자가 이미지에서 불쾌한 영향(열화)을 정확하게 찾아낼 수 있도록 한다.\n' +
      '\n' +
      '고정적인 저하 특정 프롬프트가 아닌 자유 형식의 사용자 프롬프트를 처리하면 도메인 전문 지식이 부족한 일반인을 위한 모델의 사용성이 증가한다. 따라서 우리는 우리의 모델이 "in-the-wild" _e.g_ 사용자가 제기한 다양한 프롬프트를 이해할 수 있기를 바란다. 아이들, 어른들, 사진작가들 이를 위해 큰 언어 모델(_i.e_., GPT-4)을 사용하여 사용자가 다양한 분해 유형에 대해 요청할 수 있는 다양한 요청을 생성한다. 그런 다음 생성된 프롬프트를 필터링하여 모호하거나 불명확한 프롬프트를 제거한다(_e.g_., _"이미지 클리너를 만듭니다", "이 이미지 "_" 개선). 최종 지침 세트에는 7가지 작업에 대해 총 10000개 이상의 다른 프롬프트가 포함되어 있습니다. 표 1에 몇 가지 예를 보여준다. 그림 2에서 볼 수 있듯이 프롬프트는 입력 열화에 따라 무작위로 샘플링된다.\n' +
      '\n' +
      '### Text Encoder\n' +
      '\n' +
      '텍스트 인코더의 선택.텍스트 인코더는 사용자 프롬프트를 고정 크기 벡터 표현(텍스트 임베딩)에 매핑한다. 텍스트 기반 이미지 생성[67] 및 조작[3, 4]을 위한 관련 방법은 종종 CLIP 모델[62]의 텍스트 인코더를 사용하여 사용자 프롬프트를 시각적 프롬프트에서 CLIP가 우수함에 따라 인코딩한다. 그러나, 열화에 대한 사용자 프롬프트는 일반적으로 시각적 콘텐츠(_e.g_)를 거의 또는 전혀 포함하지 않는다. 용도는 이미지 자체가 아니라 열화를 설명하므로 (6천만 개 이상의 매개변수를 가진) 큰 CLIP 인코더는 적합하지 않습니다. 특히 효율성이 필요한 경우.\n' +
      '\n' +
      '대신 순수 텍스트 기반 문장 인코더[63]를 사용하기 위해, 즉 의미적으로 의미 있는 임베딩 공간에서 문장을 인코딩하도록 훈련된 모델을 선택한다. 수백만 개의 예제로 미리 훈련된 문장 인코더는 CLIP에 비해 작고 빠르면서도 다양한 사용자 프롬프트의 의미를 인코딩할 수 있다. 예를 들어, 우리는 BGE-micro-v2 문장 변환기를 사용한다.\n' +
      '\n' +
      '텍스트 인코더를 미세 조정한다. 우리는 복원 모델에 필요한 정보를 더 잘 인코딩하기 위해 텍스트 인코더\\(\\mathrm{E}\\)을 복원 작업에 적용하고자 한다. 전체 텍스트 인코더를 훈련하는 것은 우리의 작은 훈련 세트에 과적합을 초래하고 일반화의 손실로 이어질 가능성이 있다. 대신, 텍스트 인코더를 동결하고 상단에 프로젝션 헤드를 훈련한다:\n' +
      '\n' +
      '\\[\\mathbf{e}=\\mathrm{norm}(\\mathbf{W}\\cdot\\mathrm{E}(t)) \\tag{1}\\]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Degradation** & **Prompts** \\\\ \\hline Denoising & Can you clean the dots from my image? \\\\  & Fix the grainy parts of this photo \\\\  & Remove the noise from my picture \\\\ \\hline Deblurring & Can you reduce the movement in the image? \\\\  & My picture’s not sharp, fix it \\\\  & Deblur my picture, it’s too fuzzy \\\\ \\hline Dehazing & Can you make this picture clearer? \\\\  & Help, my picture is all cloudy \\\\  & Remove the fog from my photo \\\\ \\hline Deraining & I want my photo to be clear, not rainy \\\\  & Clear the rain from my picture \\\\  & Remove the raindrops from my photo \\\\ \\hline Super-Res. & Make my photo bigger and better \\\\  & Add details to this image \\\\  & Increase the resolution of this photo \\\\ \\hline Low-light & The photo is too dark, improve exposure \\\\  & Increase the illumination in this shot \\\\  & My shot has very low dynamic range \\\\ \\hline Enhancement & Make it pop! \\\\  & Adjust the color balance for a natural look \\\\  & Apply a cinematic color grade to the photo \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 언어 및 도메인 전문 지식을 가진 선별된 GPT4 생성 사용자 프롬프트의 예.\n' +
      '\n' +
      '그림 2: 우리는 일반적인 이미지 데이터 세트를 사용하여 블라인드 이미지 복원 모델을 훈련하고 GPT-4를 사용하여 생성된 프롬프트는 (자가 지도 학습)임을 유의한다. 추론 시간에, 본 모델은 인간이 작성한 명령어로 일반화하고 실제 이미지를 복원한다.\n' +
      '\n' +
      '여기서 \\(t\\)는 텍스트이고, \\(\\mathrm{E}(t)\\)는 원시 텍스트 임베딩을 나타내고, \\(\\mathbf{W}\\in\\mathbb{R}^{d_{t}\\times d_{v}\\)는 텍스트 차원(\\(d_{t}\\))에서 복원 모델에 대한 입력 차원(\\(d_{v}\\)), \\(\\mathrm{norm}\\)은 l2-norm이다.\n' +
      '\n' +
      '도 3은 텍스트 인코더가 어느 정도 명령어들을 클러스터링할 수 있는 반면(도 2(a)), 우리의 트레이닝된 투영은 크게 개선된 클러스터들을 산출한다는 것을 보여준다(도 2(b)). 우리는 노이즈 제거, 노이즈 제거, 디블러링 및 저조도 이미지 향상을 위한 클러스터를 명확하게 구별한다. 이러한 작업 또는 열화에 대한 지침은 매우 특징적이다. 더 나아가 언어 논리와 일치하는 이전 과제들 사이에서도 "초재"와 "향상" 과제가 상당히 확산되어 있음을 알 수 있다. 예를 들어, _"이 이미지에 세부사항을 추가"_는 향상, 디블러링 또는 디노싱에 사용될 수 있다. 실험에서 \\(d_{t}\\!=\\!384\\) , \\(d_{v}\\!=\\! 256\\) 및 \\(\\mathbf{W}\\)는 선형 층이다.\n' +
      '\n' +
      '의도 분류 손실.본 논문에서는 학습 및 해석성을 향상시키기 위해 텍스트 임베딩에 대한 안내 손실을 제안한다. 열화 유형을 대상으로 \\(\\mathbf{c}=\\mathcal{C}(\\mathbf{e})\\), \\(\\mathbf{c}\\in\\mathbb{R}^{D}\\), \\(D\\)가 열화의 수인 간단한 분류 헤드 \\(\\mathcal{C}\\)을 학습한다. 실험에서 D=7\\과 분류 헤드\\(\\mathcal{C}\\)는 간단한 2층 MLP이다. 따라서, 우리는 자연 언어 지식을 캡처하기 위해 투영 계층과 간단한 MLP를 훈련하기만 하면 된다. 이를 통해 텍스트 모델은 메인 이미지 처리 모델에 대한 안내 벡터만이 아니라 그림 3에서 알 수 있듯이 의미 있는 임베딩을 학습할 수 있다.\n' +
      '\n' +
      '우리는 모델이 정확하게 분류할 수 있다는 것을 발견했다 (_i.e_. 95% 이상의 정확도). 몇 번의 시대 이후 사용자의 프롬프트의 근본적인 저하\n' +
      '\n' +
      '### InstructIR\n' +
      '\n' +
      '제안하는 방법 _InstructIR_은 이미지 모델과 텍스트 인코더로 구성된다. 우리는 Sec. 3.2에 텍스트 인코더를 도입했다. 우리는 U-Net 아키텍처를 따르는 효율적인 이미지 복원 모델인 이미지 모델로 NAFNet[10]을 사용한다[68]. 단일 모델을 사용하여 여러 작업을 성공적으로 학습하기 위해 작업 라우팅 기법을 사용한다. 모델을 훈련하고 평가하기 위한 우리의 프레임워크는 그림 2에 나와 있다.\n' +
      '\n' +
      'Text Guidance._InstructIR_의 주요 측면은 이미지 모델에 대한 제어 메커니즘으로서 인코딩된 명령어의 통합이다. 다중 작업 학습을 위한 _task routing_에서 영감을 받은 [14, 69, 71], 우리는 모델 내에서 작업별 변환을 가능하게 하는 _"명령 조건 블록"(ICB)_을 제안한다. 종래의 태스크 라우팅[71]은 채널 피처들에 태스크-특정 바이너리 마스크들을 적용한다. 우리의 모델은 _a-priori_ 열화를 알지 못하기 때문에, 우리는 이 기술을 직접 사용할 수 없다.\n' +
      '\n' +
      '이미지 특징\\(\\mathcal{F}\\)과 인코딩된 명령어\\(\\mathbf{e}\\)을 고려하여 태스크 라우팅을 다음과 같이 적용한다.\n' +
      '\n' +
      '\\mathcal{F^{\\prime}}_{c}=\\mathrm{Block}(\\mathcal{F}_{c}\\odot\\mathbf{m}_{c})+\\mathcal{F}_{c}\\tag{2}\\mathrm{Block}(\\mathcal{F}_{c}\\odot\\mathbf{m}_{c})\n' +
      '\n' +
      '여기서 마스크\\(\\mathbf{m}_{c}=\\sigma(\\mathbf{W}_{\\mathbf{c}\\cdot\\mathbf{e})\\)는 시그모이드 함수를 사용하여 활성화된 선형 레이어를 사용하여 생성되어 텍스트 임베딩\\(\\mathbf{e}\\)에 따라 가중치 세트를 생성한다. 따라서, 우리는 \\(c\\)차원 채널당 (소프트)-이진 마스크 \\(\\mathbf{m}_{c}\\)을 얻는다. [71]과 같이 태스크에 따라 마스킹 특징을 위한 채널별 곱셈 \\(\\odot\\)으로 태스크 라우팅이 적용된다. 컨디셔닝된 특징들은 NAFBlock[10](Block)을 사용하여 더욱 강화된다. 우리는 그림 4에서 작업 라우팅 ICB 블록을 설명한다. 우리는 인코더와 디코더 블록 모두에서 특징을 조정하기 위해 "정규" NAFBlocks[10]을 사용하고 그 다음에 ICB를 사용한다. 제형은 \\(F^{l+1}=\\mathrm{ICB}(\\mathrm{Block}(F^{l}))\\)이고, 여기서 \\(l\\)은 층이다. 비록 신경망의 필터를 명시적으로 조건화하지는 않지만, [71]에서와 같이 마스크는 모델이 이미지 정보 및 지시에 따라 가장 관련성이 높은 채널을 선택할 수 있게 한다. 이 공식은 미분 가능한 특징 마스킹 및 특정 해석 가능성 _i.e_를 가능하게 한다는 점에 유의한다. 가중치가 높은 특징은 복원 과정에 가장 큰 기여를 한다. 간접적으로, 이것은 또한 다양한 필터를 학습하고 희소성을 감소시키는 것을 강제한다[14, 71].\n' +
      '\n' +
      '도 4: 다-작업 학습을 위한 작업 라우팅[71]의 근사치를 사용하는 _명령 조건 블록(ICB)_. 식 2를 참조한다.\n' +
      '\n' +
      '그림 3: 훈련 전/후 텍스트 임베딩의 t-SNE 플롯 _InstructIR_을 보여준다. 각 점은 인간의 지시를 나타낸다.\n' +
      '\n' +
      '_InstructIR_이 블라인드 복원 모델인가? 모델은 이미지 _e.g_에서 열화에 대한 명시적인 정보를 사용하지 않는다. 노이즈 프로파일, 블러 커널 또는 PSF. 우리의 모델은 이미지와 지시가 주어진 작업(열화)을 추론하기 때문에 _InstructIR_를 _blind_ 이미지 복원 모델로 간주한다. 보조 영상 기반 열화 분류[42, 60]를 사용하는 이전 작업과 유사하다.\n' +
      '\n' +
      '## 4 실험 결과\n' +
      '\n' +
      '우리는 그림 17, 18 및 19의 벤치마크 이미지를 사용하여 광범위한 정성적 결과를 제공하고, 이미지 노이즈 제거, 디블러링, 데라잉, 데아징 및 이미지 향상 등 다양한 이미지 복원 작업에 대해 잘 알려진 벤치마크 9개에 대해 모델을 평가한다. 우리는 표 2에 광범위한 정량적 결과를 제시한다. 우리의 _single_ 모델은 다양한 열화 유형과 수준을 고려하여 이미지를 성공적으로 복원한다. 우리는 보충 재료에 추가 결과와 절제 연구를 제공한다.\n' +
      '\n' +
      '### Implementation Details.\n' +
      '\n' +
      '우리의 _InstructIR_ 모델은 엔드 투 엔드 트레이닝이 가능하다. 이미지 모델은 사전 훈련이 필요하지 않지만 사전 훈련된 문장 인코더를 언어 모델로 사용한다.\n' +
      '\n' +
      '텍스트 인코더.Sec.3.2에서 논의된 바와 같이, 텍스트 임베딩 프로젝션 및 분류 헤드(\\(\\approx 100K\\) 파라미터들)를 트레이닝하기만 하면 된다. 우리는 BGE-small-en[85]의 증류 버전인 BGE-micro-v2 1로 텍스트 인코더를 초기화한다. BGE 인코더는 범용 문장 인코딩을 위해 많은 양의 감독 및 비감독 데이터에 대해 사전 훈련된 BERT 유사 인코더[13]이다. BGE-마이크로 모델은 훈련 중에 동결되는 1,730만 개의 파라미터를 갖는 3-레이어 인코더이다. 우리는 또한 all-MiniLM-L6-v2 및 CLIP 인코더를 탐색하지만 소형 모델이 과적합을 방지하고 빠르면서도 최상의 성능을 제공한다는 결론을 내렸다. 우리는 보조 자료의 세 텍스트 인코더를 비교하는 절제 연구를 제공한다.\n' +
      '\n' +
      '각주 1: [https://huggingface.co/TaylorAI/bge-micro-v2](https://huggingface.co/TaylorAI/bge-micro-v2)\n' +
      '\n' +
      '이미지 모델.우리는 이미지 모델로 NAFNet[10]을 사용한다. 이 아키텍처는 4-레벨 인코더-디코더로 구성되며, 각 레벨, 구체적으로 인코더의 경우 [2, 2, 4, 8], 디코더의 경우 [2, 2, 2, 2]에서 레벨-4까지 각각 다양한 수의 블록이 있다. 인코더와 디코더 사이에는 4개의 중간 블록을 사용하여 특징을 더욱 향상시킨다. 디코더는 스킵 연결에 대한 연결 대신 덧셈을 구현한다.\n' +
      '\n' +
      '우리는 인코더와 디코더에서만 태스크 라우팅[71]을 위해 _Instruction Condition Block(ICB)_를 사용한다.\n' +
      '\n' +
      '이 모델은 지상-진실영상과 복원된 영상 사이의 \\(\\mathcal{L}_{1}\\) 손실을 이용하여 최적화된다. 또한 텍스트 인코더의 의도 분류 헤드에는 교차 엔트로피 손실\\(\\mathcal{L}_{ce}\\)을 사용한다. 학습률은 5e^{-4}\\(5e^{-4}\\)인 32개의 batch size와 AdamW[36] optimizer를 이용하여 NVIDIA A100을 이용하여 약 1일 동안 학습하였다. 또한 코사인 어닐링 학습 속도 감쇠를 사용한다. 트레이닝 동안, 입력으로서 크기 \\(256\\times 256\\)의 크롭된 패치를 사용하고, 증강으로서 랜덤 수평 및 수직 플립을 사용한다. 모델은 입력 지시-이미지 쌍으로 사용되며, 이미지가 주어지고, 그 열화를 알기 때문에, 프롬프트 데이터세트(\\(>10\\)K 샘플)로부터 명령을 랜덤하게 샘플링한다. 본 논문의 이미지 모델은 16M 파라미터만을 가지고 있으며, 학습된 텍스트 투영은 \\(100\\)k 파라미터(언어 모델은 17M 파라미터)이므로, NVIDIA RTX 2080Ti 또는 3090Ti와 같은 표준 GPU에서 며칠 내에 쉽게 학습될 수 있다. 또한, 추론 과정은 낮은 계산 예산에도 적합하다.\n' +
      '\n' +
      '데이터셋과 벤치마크\n' +
      '\n' +
      '이전 작업[42, 61, 100]에 이어 다양한 복원 작업에 대한 데이터 세트를 준비한다.\n' +
      '\n' +
      '이미지 노이즈 제거.우리는 훈련을 위해 BSD400[2]와 WED[50] 데이터 세트의 조합을 사용한다. 이 결합 훈련 세트는 \\(\\approx 5000\\) 이미지를 포함한다. 데이터세트에서 깨끗한 영상을 참조하기 위해 잡음레벨이 다른 가우시안 잡음(\\sigma\\in\\{15,25,50\\}\\)을 추가하여 잡음영상을 생성한다. 우리는 잘 알려진 BSD68[52] 및 Urban100[32] 데이터 세트에 대한 모델을 테스트한다.\n' +
      '\n' +
      '이미지 디레인닝.우리는 훈련을 위해 200개의 깨끗한 비 이미지 쌍과 테스트를 위해 100개의 쌍으로 구성된 Rain100L [88] 데이터 세트를 사용한다.\n' +
      '\n' +
      '이미지 디헤징.우리는 \\(\\approx 72\\)K 트레이닝 이미지를 포함하는 Reside (outdoor) SOTS [41] 데이터세트를 활용한다. 그러나 많은 이미지들은 품질이 낮고 비현실적이기 때문에, 우리는 데이터 세트를 필터링하고 2000개의 이미지들의 랜덤 세트를 선택했으며, 또한 다른 작업들의 불균형을 피하기 위해서이다. 우리는 500개의 이미지의 표준 _outdoor_ 테스트 세트를 사용한다.\n' +
      '\n' +
      '영상 디블러링(image deblurring.GoPro dataset for motion deblurring[57])은 학습용 2103개의 영상과 테스트용 1111개의 영상으로 구성되어 있다.\n' +
      '\n' +
      'Low-light Image Enhancement.LOL[83] 데이터셋(v1)을 사용하여 485개의 학습 영상과 15개의 테스트 영상을 공식 분할한다.\n' +
      '\n' +
      '이미지 향상.이전 연구를 확장하여 MIT5K 데이터셋 [5]를 이용한 사진-실감 이미지 향상도 연구한다. 우리는 훈련을 위해 1000개의 이미지를 사용하고 테스트([74]에서와 같이)를 위해 500개의 이미지의 표준 분할을 사용한다.\n' +
      '\n' +
      '마지막으로, 이전 작업 [42, 61, 100]과 같이 앞서 언급한 모든 학습 데이터셋을 결합하고, 올인원 복원을 위한 통합 모델을 학습한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:6]\n' +
      '\n' +
      'MIT5K [5] 데이터셋을 사용하여 이 작업을 수행하면 다른 작업에 대한 성능을 유지하면서 주목할 만하다. 우리는 고전적인 과제별 방법과 유사한 성능을 달성한다.\n' +
      '\n' +
      '표 6의 다중 작업 절제 연구를 **요약한다. 본 모델은 명령어 기반 작업 라우팅 덕분에 성능을 크게 잃지 않고 여러 작업을 처리할 수 있다.\n' +
      '\n' +
      '작업별 방법과의 비교 우리의 주요 목표는 강력한 올인원 모델을 설계하는 것이므로 _InstructIR_는 특정 열화에 대해 훈련되도록 설계되지 않았다. 그럼에도 불구하고, 우리는 또한 _InstructIR_와 태스크 특정 방법 _i.e_를 비교한다. 특정 작업에 맞게 조정되고 훈련된 모델입니다.\n' +
      '\n' +
      '표 5에서는 영상 개선을 위한 태스크별 방법과, 7에서는 저조도 영상 개선을 위한 태스크별 방법을 비교하고, 표 8에서는 영상 노이즈 제거를 위한 광범위한 비교를 제공하며, 표 9에서는 디블러링 및 디헤징을 위한 고전적인 방법과 비교한다. 우리의 다중 작업 방법은 대부분의 작업별 방법보다 낫지만 여전히 SOTA보다 낫지 않다.\n' +
      '\n' +
      '##6 지침의 효과성에 관한 연구\n' +
      '\n' +
      '인간 지침의 통합 덕분에 사용자는 이미지를 향상시키는 방법을 제어할 수 있습니다. 우리는 그림 5에서 입력 영상이 세 가지 다른 열화를 갖는 예를 보여주며, 우리는 특정 영상에 초점을 맞추는 것을 목표로 한다. 이러한 결과는 큰 재구성을 제공하지 못하지만, 영상 복원 및 개선을 위한 지도 지침의 효과를 보여주는 유망한 방향이라고 생각한다. 우리는 그림 6과 7에서 더 많은 결과를 제공하며, 여기서 제어 가능한 방식으로 이미지를 복원하고 향상시키는 방법의 잠재력을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Tasks** & **Rain** & **Noise (\\(\\sigma 15\\))** & **Blur** & **LOL** \\\\ \\hline\n' +
      '3D & 37.98/0.978 & 31.52/0.890 & -\\\\\n' +
      '5D & 36.84/0.973 & 31.40/0.887 & 29.40/0.886 & 23.00/0.836 \\\\\n' +
      '6D & 36.80 /0.973 & 31.39 /0.888 & 29.73 /0.892 & 22.83 / 0.836 \\\\\n' +
      '**7D** & 36.75 /0.972 & 31.37 /0.887 & 29.70/0.892 & 22.81 / 0.836 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **요약 절제 연구** 3 내지 7개의 태스크들을 태클하는 _InstructIR_의 다중-태스크 변형들에 관한 우리는 PSNR/SSIM을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Methods** & **Dehazing** & **Deraining** & **Denoising ablation study (BSD68 [52])** & **Average** \\\\  & SOTS [41] & Rain100L [21] & \\(\\sigma=15\\) & \\(\\sigma=25\\) & \\(\\sigma=50\\) & \\\\ \\hline BRDNet [72] & 23.23/0.895 & 27.42/0.895 & 32.26/0.898 & 29.76/0.836 & 26.34/0.836 & 27.80/0.843 \\\\ LPNet [25] & 20.84/0.828 & 24.88/0.784 & 26.47/0.778 & 24.77/0.748 & 21.26/0.552 & 23.64/0.738 \\\\ FDGAN [19] & 24.71/0.924 & 29.89/0.933 & 30.25/0.910 & 28.81/0.868 & 26.43/0.776 & 28.02/0.883 \\\\ MPRNet [94] & 25.28/0.954 & 33.57/0.954 & 33.54/0.927 & 30.89/0.880 & 27.56/0.779 & 30.17/0.899 \\\\ DL[21] & 26.92/0.391 & 32.62/0.931 & 33.05/0.914 & 30.41/0.861 & 26.90/0.740 & 29.98/0.875 \\\\ AirNet [42] & 27.94/0.962 & 34.90/0.967 & 33.92/0.933 & 31.26/0.888 & 28.00/0.797 & 31.20/0.910 \\\\ PromptIR [61] & **30.58/0.974** & 36.37/0.972 & 33.98/0.933 & 31.31/0.888 & 28.06/0.799 & 32.06/0.913 \\\\ _InstructIR_-3D & 30.22/0.959 & **37.98/0.978** & **34.15/0.933** & **31.52/0.890** & **28.30/0.804** & **32.43/0.913** \\\\ _InstructIR_-5D & 27.10/0.956 & 36.84/0.973 & 34.00/0.931 & 31.40/0.887 & 28.15/0.798 & 31.50/0.909 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **3개의 복원 작업에 대한 올인원 복원 모델의 비교**(3D). 또한 다양한 잡음레벨을 고려한 영상 잡음제거-기본 역문제-에 대한 제거 연구를 보인다. 우리는 PSNR/SSIM 메트릭을 보고한다. [61]을 기준으로 하는 테이블.\n' +
      '\n' +
      '도 5 : **선택적 작업**. _ InstructIR_는 인간의 지시에 따라 특정 열화를 제거하거나 상이한 변환을 수행할 수 있다. 이것은 이미지 복원에 있어서 새로운 특징이며, 텍스트 기술의 새로운 통합 덕분에 가능하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline\n' +
      '**Method** & **PSNR \\(\\uparrow\\)** & **SSIM \\(\\uparrow\\)** & \\(\\Delta E_{ab}\\downarrow\\) \\\\ \\hline UPE [77] & 21.88 & 0.853 & 10.80 \\\\ DPE [26] & 23.75 & 0.908 & 9.34 \\\\ HDRNet [11] & 24.32 & 0.912 & 8.49 \\\\\n' +
      '3DLLUT [96] & **25.21** & **0.922** & **7.61** \\\\ _InstructIR_-**7D** & 24.65 & 0.900 & 8.20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **Image Enhancement** MIT5K [5, 96]에 대한 성능\n' +
      '\n' +
      '도 6: **명령어 기반 이미지 복원**; _ InstructIR_는 주어진 태스크(첫 번째 행)에 대한 광범위한 명령어들을 이해한다. 반대 지시_(두 번째 행)가 주어지면 모델은 ID를 수행합니다. 훈련 중에 이를 시행하지 않았습니다. BSD68[52]의 이미지.\n' +
      '\n' +
      '도 7: **Multiple Real Instructions** 이미지를 복원하고 향상시키기 위해 여러 가지 지시(순서대로)를 프롬프트할 수 있습니다. 이것은 추가적인 _control_를 제공한다. 우리는 "입력" 이미지에 적용된 두 가지 명령의 예를 왼쪽에서 오른쪽으로 보여준다.\n' +
      '\n' +
      '이것은 진보 _w.r.t_ 고전적(결정론적) 이미지 복원 방법을 의미한다. 고전적인 심층 복원 방법은 독특한 결과를 가져오기 때문에 이미지가 처리되는 방식을 제어할 수 없다. 또한 그림 8의 InstructPix2Pix[4]와 _InstructIR_를 비교한다.\n' +
      '\n' +
      '질적 결과.우리는 여러 과제에 대해 다양한 질적 결과를 제공한다. 그림 9에서 LOL [83] 데이터 세트에 대한 결과를 보여준다. 그림 10에서 GoPro[57] 데이터셋을 사용하여 모션 디블러링 태스크에 대한 방법을 비교한다. 그림 11에서 우리는 SOTS(옥외)에 대한 안개 제거 작업에 대해 다른 방법과 비교한다[41]. 도 12에서는 Rain100L에서 Deaining을 위한 이미지 복원 방법과 비교한다[21]. 마지막으로, 우리는 그림 13에서 잡음 제거 결과를 보여준다. 이 정성적 분석에서는 모든 이미지를 복원하기 위해 단일 _InstructIR_-**5D** 모델을 사용한다.\n' +
      '\n' +
      '[그림 8]에서 명령어 기반 복원에 대한 논의는 InstructPix2Pix[4]와 비교한다. 우리의 방법은 효율성, 충실도 및 품질 측면에서 현저하게 우수하다. 이미지 조작을 위한 확산 기반 방법[4, 53, 67]은 여러 (하이퍼-파라미터)의 복잡한 "튜닝"과 충실도를 강화하고 환각을 줄이기 위한 강력한 규칙화가 필요하다는 결론을 내릴 수 있다. InstructPix2Pix[4]는 Inpainting을 해결하기 위한 좋은 선행을 가지고 있음에도 불구하고 역문제를 직접 해결할 수 없으며, 이는 이러한 모델이 복원-특정 훈련(또는 미세 조정)을 필요로 한다는 것을 나타낸다.\n' +
      '\n' +
      '본 논문에서 제안하는 방법은 5개의 태스크에서 _state-of-the-art_ 결과를 얻음으로써, 딥 블라인드 복원 모델을 안내하는 명령어 사용의 가능성을 증명한다. 그러나 우리는 특정 한계를 인정한다. 첫째, 확산 기반 복원 방법과 비교하여 현재 접근 방식은 지각 품질에 대한 더 나은 결과를 얻지 못할 것이다. 둘째, 본 논문에서 제안하는 모델은 두 가지 이상의 열화(_real-world_ image)가 있는 영상을 처리하는데 어려움을 겪지만, 이는 관련 복원 방법 중 공통적인 한계이다. 셋째, 기존의 _all-in-one_ 방법들로서, 우리의 모델은 _in-distribution degradations_에서만 동작하므로, 보이지 않는 아티팩트에서는 동작하지 않을 것이다. 그럼에도 불구하고 이러한 한계는 보다 현실적인 훈련 데이터로 능가할 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '우리는 이미지 복원 모델을 안내하기 위해 사람이 작성한 지침을 사용하는 첫 번째 접근법을 제시한다. 자연 언어 프롬프트가 주어지면, 본 모델은 여러 가지 열화 유형을 고려하여 저하된 대응물로부터 고품질 이미지를 복구할 수 있다. InstructIR은 여러 복구 작업에 대한 최첨단 결과를 달성하여 지시 지침의 힘을 보여준다. 이러한 결과는 텍스트 유도 이미지 복원을 위한 새로운 벤치마크를 나타낸다.\n' +
      '\n' +
      '이 작업은 Humboldt Foundation (AvH)에 의해 부분적으로 지지되었다. 마르코스 콘데는 또한 소니 인터렉티브 엔터테인먼트, FTG의 지원을 받는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & LPNet & URetinex [43] & DeepLPF [55] & SCI [51] & LIME [27] & MF [23] & NPE [78] & SRIE [24] & SDD [28] & CDEF [40] & _InstructIR_ \\\\ \\hline\n' +
      '**PSNR \\(\\uparrow\\)** & 21.46 & 21.32 & 15.28 & 15.80 & 16.76 & 16.96 & 16.96 & 11.86 & 13.34 & 13.33 & 22.83 \\\\\\row\n' +
      '**SSIM \\(\\uparrow\\)** & 0.802 & 0.835 & 0.473 & 0.527 & 0.444 & 0.505 & 0.481 & 0.493 & 0.635 & 0.583 & 0.836 \\\\ \\hline \\hline \\multirow{2}{*}{**Method**} & DRBN & KinD & RUAS & FIDE & EG & MS-RDN & Retinex & MIRNet & IPT & Uformer & **IAGC** \\\\  & [89] & [107] & [46] & [86] & [33] & [90] & -Net[83] & [93] & [8] & [82] & [81] \\\\ \\hline\n' +
      '**PSNR \\(\\uparrow\\)** & 20.13 & 20.87 & 18.23 & 18.27 & 17.48 & 17.20 & 16.77 & 24.14 & 16.27 & 16.36 & **24.53**\n' +
      '**SSIM \\(\\uparrow\\)** & 0.830 & 0.800 & 0.720 & 0.665 & 0.650 & 0.640 & 0.560 & 0.830 & 0.504 & 0.507 & **0.842** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **LOL dataset**[83] **(low-light enhancement)** 상의 _state-of-the-art_ 방법들과의 정량적 비교. [81]을 기준으로 하는 테이블.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c||c} \\hline \\hline\n' +
      '**Deblurring GoPro**[57] & \\multicolumn{2}{c}{**Dehazing SOTS**[41]}\\\\\n' +
      '**Method** & **PSNR/SSIM** & **Method** & **PSNR/SSIM** \\\\ \\hline Xu _et al._[87] & 21.00/0.741 & DehazeNet [6] & 22.46/0.851 \\\\ DeblurGAN [38] & 28.70/0.858 & GFN [65] & 21.55/0.844 \\\\ Nah _et al._[57] & 29.08/0.914 & GCANet [7] & 19.98/0.704 \\\\ RNN [99] & 29.19/0.931 & MSBDN [17] & 23.36/0.875 \\\\ DeblurGAN-\\(V2\\)[39] & 29.55/0.934 & DuRN [47] & 24.47/0.839 \\\\ _InstructIR_-**5D** & 29.40/0.886 & _InstructIR_-**5D** & 27.10/0.956 \\\\ _InstructIR_-**5D** & **29.73**/**0.892** & _InstructIR_-**3D** & **30.22**/**0.959** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: **영상 잡음 제거**에서 일반 복원 및 올인원 방법(*)과의 비교. 우리는 서로 다른 \\(\\sigma\\) 잡음 수준을 고려한 벤치마크 데이터 세트에 대한 PSNR을 보고한다. [100]을 기준으로 하는 테이블.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c||c} \\hline \\hline\n' +
      '**Deblurring GoPro**[57] & \\multicolumn{2}{c}{**Dehazing SOTS**[41]}\\\\\n' +
      '**Method** & **PSNR/SSIM** & **Method** & **PSNR/SSIM** \\\\ \\hline Xu _et al._[87] & 21.00/0.741 & DehazeNet [6] & 22.46/0.851 \\\\ DeblurGAN [38] & 28.70/0.858 & GFN [65] & 21.55/0.844 \\\\ Nah _et al._[57] & 29.08/0.914 & GCANet [7] & 19.98/0.704 \\\\ RNN [99] & 29.19/0.931 & MSBDN [17] & 23.36/0.875 \\\\ DeblurGAN-\\(V2\\)[39] & 29.55/0.934 & DuRN [47] & 24.47/0.839 \\\\ _InstructIR_-**5D** & 29.40/0.886 & _InstructIR_-**5D** & 27.10/0.956 \\\\ _InstructIR_-**5D** & **29.73**/**0.892** & _InstructIR_-**3D** & **30.22**/**0.959** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **Deblurring 및 Dehazing 비교. 우리는 벤치마크 데이터 세트에 대한 작업별 고전적 방법과 비교한다.**그림 8: ** 프롬프트를 사용하여 명령어 기반 복원을 위한 InstructPix2Pix[4]와의 비교. _RealSRet_[44, 80]로부터의 이미지. 우리는 7D 변형을 사용합니다. 인스트럭트픽스2픽스[4]는 충실도를 향상시키기 위해 이미지 구성요소의 가중치를 변화시키는 두 가지 구성, 즉 \\(S_{I}=5\\) 및 \\(S_{I}=7\\)(이미지 CFG라고도 함)을 사용하여 실행되며, 이 매개변수는 충실도를 강화하고 환각을 줄이는 데 도움이 된다.**\n' +
      '\n' +
      '도 11: **Image Dehazing Results.** SOTS[41] _outdoor_(0150.jpg)에 대한 다른 방법들과의 비교.\n' +
      '\n' +
      '도 12: **Image Deraining Results** on Rain100L[21] (035.png).\n' +
      '\n' +
      '도 13: **Image Denoising Results** on BSD68[52] (0060.png).\n' +
      '\n' +
      '도 10: **Image Deblurring Results.** GoPro [57] 데이터셋 상의 다른 방법들과의 비교(GOR0854-11-00-00001.png).\n' +
      '\n' +
      '도 9: ** 저조도 이미지 향상 결과.** LOL [83]에 대한 다른 방법과 비교한다(748.png).\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '도 19: **SOTS 실외 데이터세트로부터의 이미지에 대한 올인원 방법에 대한 비교**.\n' +
      '\n' +
      '도 16: **InstructPix2Pix[4]**와의 비교: prompt _“Remove the noise in this photo”_를 이용한 명령어 기반 복원을 위한 도면.\n' +
      '\n' +
      '도 17: **올인원 방법에 대한 노이즈 제거 결과**. BSD68[52]의 영상들과 잡음 레벨 \\(\\sigma=25\\)\n' +
      '\n' +
      '도 18: **Image deraining comparisons** for all-in-one methods on the Rain100L dataset [21].\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]E. 아구스트손과 R. Timofte (2017) NTIRE 2017 challenge on single image super-resolution: dataset and study. CVPR 워크샵에서 인용된 SS1, SS2.1, SS3.1, SS3.2입니다.\n' +
      '*[2]P. 아르벨라에즈 Maire, C. Fowlkes, and J. Malik (2011) Contour detection and hierarchical image segmentation. TPAMI 인용: SS1.\n' +
      '*[3]Y. 배철왕 Xie, C. Dong, C. Yuan, Z. Wang(2023) Textir: 텍스트 기반 편집 가능한 이미지 복원을 위한 간단한 프레임워크. CoRRabs/2302.14736. 인용: SS1.\n' +
      '*[4]T. Brooks, A. Holynski, and A. A. Efros (2023) Instructpix2pix: 이미지 편집 지시를 따르는 것을 배우는 것. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 18392-18402. Cited by: SS1.\n' +
      '*[5]V. 바흐코프스키 Paris, E. Chan, and F. Durand (2011) Learning photographic global tonal adjustment with database of input/output image pairs. The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, Cited by: SS1.\n' +
      '*[6]B. 카이영 서경 Jia, C. Qing, and D. Tao(2016) Dehazenet: end-to-end system for single image haze removal. IEEE Transactions on Image Processing25(11), pp. 5187-5198. Cited by: SS1.\n' +
      '*[7]D. 천민 그, 큐 Fan J. Liao 장동호 Wuan, and G. Hua(2019) Gated context aggregation network for image dehazing and deraining. 2019 IEEE winter conference on applications of computer vision (WACV), pp. 1375-1383. Cited by: SS1.\n' +
      '*[8]H. 천영 왕태 곽철수 덩진 류승 마창수 Gao(2021) Pre-trained image processing transformer. CVPR에서 인용됨: SS1.\n' +
      '*[9]L. 천진 루정장 Chu, and C. Chen(2021) Hinet: 이미지 복원을 위한 하프 인스턴스 정규화 네트워크. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 182-192. Cited by: SS1.\n' +
      '*[10]L. 천진 추승 Zhang 및 J. Sun(2022) 이미지 복원을 위한 단순 기준선. ECCV에서 인용: SS1.\n' +
      '*[11]Y. 천영 왕민 카오와 Y. Chuang(2018) Deep photo enhancer: gans가 있는 사진으로부터 이미지 향상을 위한 짝을 이루지 않은 학습. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6306-6314. Cited by: SS1.\n' +
      '*[12]V. 코닐레, A. 젤루아, W. 이판오 Sorkine-Hornung, and C. Schroers (2019) Blind image super-resolution with spatially variant degradations. ACM Transactions on Graphics (TOG)38 (6), pp. 1-13. Cited by: SS1.\n' +
      '*[13]J. 데블린 장경 이경호 Toutanova (2019) BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1(Long and Short Papers), pp. 4171-4186. Cited by: SS1.\n' +
      '*[14]C. 딩, 지 류승 왕래 쳉과 브이 Naresh Boddeti(2023) 학습 불가능한 프리미티브를 갖는 명시적 태스크 라우팅을 통해 다중 태스크 학습에서 태스크 간섭을 완화한다. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7756-7765. Cited by: SS1.\n' +
      '*[15]C. 동철철 그와 X Tang (2015) Image super-resolution using deep convolutional networks. TPAMI 인용: SS1.\n' +
      '*[16]H. 동종판 상종욱 허진 장, F. Wang, M. Yang(2020) Multi-scale boosted dehazing network with dense feature fusion. CVPR에서 인용됨: SS1.\n' +
      '*[17]H. 동종판 상종욱 허진 장, F. Wang, M. Yang(2020) Multi-scale boosted dehazing network with dense feature fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2157-2167. Cited by: SS1.\n' +
      '*[18]W. 동락 장기시, X Wu(2011) 적응적 희소 도메인 선택 및 적응적 정규화에 의한 이미지 디블러링 및 초해상도. 팁 인용: SS1.\n' +
      '*[19]Y. 동영 류현장 천영 Qiao(2020) Fd-gan: 단일 이미지 해징을 위한 융합-식별기를 갖는 생성적 적대 네트워크. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10729-10736. Cited by: SS1.\n' +
      '*[20]M. Elad and A. Feuer (1997) Restoration of single superresolution image from several blurred, noisy, and undersampled measured images. IEEE transactions on image processing6(12), pp. 1646-1658. Cited by: SS1.\n' +
      '*[21]Q. 판덕천 위안화 Yu, and B. Chen (2019) A general decoupled learning framework for parameterized image operator. IEEE transactions on pattern analysis and machine intelligence43(1), pp.33-47. Cited by: SS1.\n' +
      '*[22]R. Franzen (1999) Kodak 무손실 참색 이미지 제품군. 참고: [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/) Cited by: SS1.\n' +
      '*[23]X. 후덕정 황영 리아오, 엑스 Ding, and J. Paisley (2016) A fusion-based enhance method for weakly illuminated images. 129, pp. 82-96. Cited by: SS1.\n' +
      '*[24]X. 후덕정 황철 장, X Ding (2016) 동시 반사율 및 조도 추정을 위한 가중 분산 모델. CVPR에서 인용됨: SS1.\n' +
      '*[25]H. 가오, X 타오엑스 Shen, and J. Jia(2019) Dynamic scene deblurring with parameter selective sharing and nested skip connections. CVPR, pp. 3848-3856. Cited by: SS1.\n' +
      '*[26]M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand (2019) Deep bilaterallearning for real time image enhancement. _ ACM Transactions on Graphics (TOG)_, 36(4):1-12, 2017.\n' +
      '* [27] Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. _IEEE TIP_, 26(2):982-993, 2016.\n' +
      '* [28] Shijie Hao, Xu Han, Yanrong Guo, Xin Xu, and Meng Wang. Low-light image enhancement with semi-decoupled decomposition. _IEEE TMM_, 22(12):3025-3038, 2020.\n' +
      '* [29] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. _TPAMI_, 2010.\n' +
      '* [30] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [31] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In _ICCV_, 2019.\n' +
      '* [32] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5197-5206, 2015.\n' +
      '* [33] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision. _IEEE TIP_, 30:2340-249, 2021.\n' +
      '* [34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.\n' +
      '* [35] Kwang In Kim and Younghee Kwon. Single-image super-resolution using sparse regression and natural image prior. _TPAMI_, 2010.\n' +
      '* [36] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv:1412.6980_, 2014.\n' +
      '* [37] Johannes Kopf, Boris Neubert, Billy Chen, Michael Cohen, Daniel Cohen-Or, Oliver Deussen, Matt Uyttendaele, and Dani Lischinski. Deep photo: Model-based photograph enhancement and viewing. _ACM TOG_, 2008.\n' +
      '* [38] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. DeblurGAN: Blind motion deblurring using conditional adversarial networks. In _CVPR_, 2018.\n' +
      '* [39] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. DeblurGAN-v2: Deblurring (orders-of-magnitude) faster and better. In _ICCV_, 2019.\n' +
      '* [40] Xiaozhou Lei, Zixiang Fei, Wenju Zhou, Huiyu Zhou, and Minrui Fei. Low-light image enhancement using the cell vibration model. _IEEE TMM_, pages 1-1, 2022.\n' +
      '* [41] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking single-image dehazing and beyond. _IEEE Transactions on Image Processing_, 28(1):492-505, 2018.\n' +
      '* [42] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jianchao Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In _CVPR_, pages 17452-17462, 2022.\n' +
      '* [43] Jiaqian Li, Juncheng Li, Faming Fang, Fang Li, and Guixu Zhang. Luminance-aware pyramid network for low-light image enhancement. _IEEE TMM_, 23:3153-3165, 2020.\n' +
      '* [44] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwiniR: Image restoration using swin transformer. In _ICCV Workshops_, 2021.\n' +
      '* [45] Lin Liu, Lingxi Xie, Xiaopeng Zhang, Shankin Yuan, Xiangyu Chen, Wengang Zhou, Houqiang Li, and Qi Tian. Tape: Task-agnostic prior embedding for image restoration. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVIII_, pages 447-464. Springer, 2022.\n' +
      '* [46] Risheng Liu, Long Ma, Jiao Zhang, Xin Fan, and Zhongxuan Luo. Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. In _CVPR_, 2021.\n' +
      '* [47] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7007-7016, 2019.\n' +
      '* [48] Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan. Learning the degradation distribution for blind image super-resolution. In _CVPR_, pages 6063-6072, 2022.\n' +
      '* [49] Jiaqi Ma, Tianheng Cheng, Guoli Wang, Qian Zhang, Xinggang Wang, and Lefei Zhang. Prores: Exploring degradation-aware visual prompt for universal image restoration. _arXiv preprint arXiv:2306.13653_, 2023.\n' +
      '* [50] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. _TIP_, 2016.\n' +
      '* [51] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image enhancement. In _CVPR_, 2022.\n' +
      '* [52] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _ICCV_, 2001.\n' +
      '* [53] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.\n' +
      '* [54] Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In _ICCV_, 2013.\n' +
      '* [55] Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory Slabaugh. Deepplf: Deep local parametric filters for image enhancement. In _CVPR_, 2020.\n' +
      '* [56] Chong Mou, Qian Wang, and Jian Zhang. Deep generalized unfolding networks for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17399-17410, 2022.\n' +
      '* [57] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _CVPR_, 2017.\n' +
      '* [58] Seungjun Nah, Sanghyun Son, Jaerin Lee, and Kyoung Mu Lee. Clean images are hard to reblur: Exploiting the ill-posed inverse task for dynamic scene deblurring. In _ICLR_, 2022.\n' +
      '* [59] Nhat Nguyen, Peyman Milanfar, and Gene Golub. Efficient generalized cross-validation with applications to parametric image restoration and resolution enhancement. _IEEE Transactions on image processing_, 10(9):1299-1308, 2001.\n' +
      '* [60] Dongwon Park, Byung Hyun Lee, and Se Young Chun. All-in-one image restoration for unknown degradations using adaptive discriminative filters for specific degradations. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5815-5824. IEEE, 2023.\n' +
      '* [61] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. Promptir: Prompting for all-in-one blind image restoration. _arXiv preprint arXiv:2306.13090_, 2023.\n' +
      '* [62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, pages 8748-8763. PMLR, 2021.\n' +
      '* [63] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019_, pages 3980-3990. Association for Computational Linguistics, 2019.\n' +
      '* [64] Chao Ren, Xiaohai He, Chuncheng Wang, and Zhibo Zhao. Adaptive consistency prior based deep network for image denoising. In _CVPR_, 2021.\n' +
      '* [65] Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, and Ming-Hsuan Yang. Gated fusion network for single image dehazing. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3253-3261, 2018.\n' +
      '* [66] Wenqi Ren, Jinshan Pan, Hua Zhang, Xiaochun Cao, and Ming-Hsuan Yang. Single image dehazing via multi-scale convolutional neural networks with holistic edges. _IJCV_, 2020.\n' +
      '* [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE, 2022.\n' +
      '* [68] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.\n' +
      '* [69] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. _arXiv preprint arXiv:1711.01239_, 2017.\n' +
      '* [70] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [71] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring. Many task learning with task routing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1375-1384, 2019.\n' +
      '* [72] Chunwei Tian, Yong Xu, and Wangmeng Zuo. Image de-noising using deep cnn with batch renormalization. _Neural Networks_, 2020.\n' +
      '* [73] Radu Timofte, Vincent De Smet, and Luc Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In _ICCV_, 2013.\n' +
      '* [74] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. MAXIM: Multi-axis MLP for image processing. In _CVPR_, pages 5769-5780, 2022.\n' +
      '* [75] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal M Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions. In _CVPR_, pages 2353-2363, 2022.\n' +
      '* [76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* [77] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6849-6857, 2019.\n' +
      '* [78] Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. _IEEE TIP_, 22(9):3538-3548, 2013.\n' +
      '* [79] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In _CVPR_, 2018.\n' +
      '* [80] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. ESRGAN: enhanced super-resolution generative adversarial networks. In _ECCV Workshops_, 2018.\n' +
      '* [81] Yinglong Wang, Zhen Liu, Jianzhuang Liu, Songcen Xu, and Shuaicheng Liu. Low-light image enhancement with illumination-aware gamma correction and complete image modelling network. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13128-13137, 2023.\n' +
      '* [82] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and Jianzhuang Liu. Uformer: A general u-shaped transformer for image restoration. _arXiv:2106.03106_, 2021.\n' +
      '\n' +
      '* [83] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In _British Machine Vision Conference_, 2018.\n' +
      '* [84] Wenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wenhan Yang, and Jianmin Jiang. Uretinex-net: Retinex-based deep unfolding network for low-light image enhancement. In _CVPR_, 2022.\n' +
      '* [85] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-packaged resources to advance general chinese embedding. _CoRR_, abs/2309.07597, 2023.\n' +
      '* [86] Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau. Learning to restore low-light images via decomposition-and-enhancement. In _CVPR_, 2020.\n' +
      '* [87] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse representation for natural image deblurring. In _CVPR_, 2013.\n' +
      '* [88] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network for image super-resolution. In _CVPR_, 2020.\n' +
      '* [89] Wenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. Band representation-based semi-supervised low-light image enhancement: bridging the gap between signal fidelity and perceptual quality. _IEEE TIP_, 30:3461-3473, 2021.\n' +
      '* [90] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. _IEEE TIP_, 30:2072-2086, 2021.\n' +
      '* [91] Mingde Yao, Ruikang Xu, Yuanshen Guan, Jie Huang, and Zhiwei Xiong. Neural degradation representation learning for all-in-one image restoration. _arXiv preprint arXiv:2310.12848_, 2023.\n' +
      '* [92] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10819-10829, 2022.\n' +
      '* [93] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In _ECCV_, 2020.\n' +
      '* [94] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In _CVPR_, 2021.\n' +
      '* [95] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In _CVPR_, 2022.\n' +
      '* [96] Hui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang. Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(4):2058-2073, 2020.\n' +
      '* [97] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. _arXiv preprint arXiv:2308.03021_, 2023.\n' +
      '* [98] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 2285-2293, 2023.\n' +
      '* [99] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Linchao Bao, Rynson WH Lau, and Ming-Hsuan Yang. Dynamic scene deblurring using spatially variant recurrent neural networks. In _CVPR_, 2018.\n' +
      '* [100] Jinghao Zhang, Jie Huang, Mingde Yao, Zizheng Yang, Hu Yu, Man Zhou, and Feng Zhao. Ingredient-oriented multi-degradation learning for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5825-5835, 2023.\n' +
      '* [101] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _TIP_, 2017.\n' +
      '* [102] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.\n' +
      '* [103] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep CNN denoiser prior for image restoration. In _CVPR_, 2017.\n' +
      '* [104] Kai Zhang, Wangmeng Zuo, and Lei Zhang. FFDNet: Toward a fast and flexible solution for CNN-based image denoising. _TIP_, 2018.\n' +
      '* [105] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realistic blurring. In _CVPR_, pages 2737-2746, 2020.\n' +
      '* [106] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.\n' +
      '* [107] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In _ACM MM_, 2019.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>