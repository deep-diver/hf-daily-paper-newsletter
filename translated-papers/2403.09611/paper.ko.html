<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '위의 발전을 감안할 때 위의 것과 결혼하는 멀티모달 기반 모델의 영역이 우월한 능력을 달성하는 단일 모델로 발전했다. 특히, MLLM(Multimodal Large Language Models)은 이미지 및 텍스트 데이터를 소비하고 텍스트를 생성하는 대규모 기반 모델이다[28, 67, 79, 111]. LLM이 부상한 후, MLLM은 기초 모델의 다음 개척지로 부상하고 있다.\n' +
      '\n' +
      '투명성에 관한 한, 기존의 MLLM은 닫힌 모델[107, 1]과 열린 모델[3, 4, 5, 77, 90]의 두 가지 범주로 분류된다. 전자의 범주에서 모델은 사용할 수 있지만 데이터, 모델 아키텍처 및 훈련 세부 사항에 대해서는 알려져 있는 바가 거의 없다. 후자의 범주에서 모델 매개변수는 데이터, 모델 및 훈련 구성에 대한 자세한 설명과 함께 릴리스되어 커뮤니티가 구축될 수 있다. 그러나 대부분의 작업은 오픈과 클로즈 모두 알고리즘 설계 선택에 도달하기 위해 겪은 프로세스, 특히 멀티모달 사전 훈련에 대해 거의 아무 것도 릴리스되지 않았다.\n' +
      '\n' +
      '이 분야에 대한 추가 연구를 위해서는 구체적인 구성 요소 구현보다 오래 지속될 수 있는 모델을 구축하는 방법에 대한 원칙과 교훈을 증류하는 것이 필수적이라고 믿는다. 따라서 본 논문에서는 MLLM 구축 프로세스를 문서화하고 커뮤니티에 사용되기를 바라는 디자인 수업을 공식화하려고 시도한다.\n' +
      '\n' +
      '특히 저희의 기여도는 다음과 같습니다. 먼저, (1) 모델 아키텍처 결정 및 (2) 사전 훈련 데이터 선택에 걸쳐 소규모로 삭제를 수행한다. 우리는 몇 가지 흥미로운 경향을 식별합니다. 모델링 측면에서는 디자인 측면이 이미지 해상도, 시각적 중요도 순으로 나타남을 알 수 있다.\n' +
      '\n' +
      '그림 1: MM1은 대규모 멀티모달 사전 훈련 덕분에 상황 내 예측을 수행할 수 있다. 이를 통해 MM1은 (a) 객체를 카운트하고 사용자 지정 형식을 따르며, (b) 이미지의 일부를 참조하여 OCR을 수행하고, (c) 일상적인 객체에 대한 상식 및 단어 지식을 보여주고, (d) 기본적인 수학 기능을 수행한다. 이미지는 COCO 2014 검증 세트[72]에서 가져온 것입니다.\n' +
      '\n' +
      '인코더 손실 및 용량, 및 비주얼 인코더 사전 트레이닝 데이터를 포함한다. 그러나 놀랍게도 우리는 시각적 데이터가 LLM 물질에 어떻게 공급되는지에 대한 구조적 결정에 대한 증거를 거의 찾지 못한다.\n' +
      '\n' +
      '또한, 이미지 캡션, 인터리브 이미지 텍스트 및 텍스트 전용 데이터의 세 가지 유형의 사전 학습 데이터를 사용한다. 우리는 소수의 샷과 텍스트 전용의 경우 인터리브와 텍스트 전용의 학습 데이터가 가장 중요한 반면, 제로 샷의 경우 캡션 데이터가 가장 중요하다는 것을 안다. 우리는 이러한 경향이 사전 훈련에 사용된 평가와 추가 벤치마크에서 모두 감독 미세 조정(SFT) 후에 유지된다는 것을 보여준다. 이는 사전 훈련 중에 발견된 기능 및 모델링 결정이 미세 조정 후에도 유지된다는 것을 보여준다.\n' +
      '\n' +
      '마지막으로 3B, 7B, 30B로 더 큰 LLM을 사용하고 64명의 전문가가 있는 3B MoE에서 32명의 전문가가 있는 7B MoE로 혼합 전문가(MoE) 모델을 탐색하여 모델을 확장한다. 이것은 우리가 아는 한 대부분의 관련 작업을 능가하는 수행 모델 가족으로 이어진다. 특히, 사전 훈련된 모델 MM1은 SOTA로, 소형 및 대형 체제 모두에서 수-샷 설정에서 캡션 및 시각적 질문 응답(VQA) 작업에 대해 Emu2[106], 플라밍고[3], IDEFICS[47]보다 우수한 성능을 보인다. 최종 모델은 SFT 이후 12개의 확립된 멀티모달 벤치마크에 걸쳐 경쟁적 성과를 달성한다.\n' +
      '\n' +
      '대규모 멀티모달 사전 훈련 덕분에 그림 1과 2에서 볼 수 있듯이 MM1은 맥락 내 예측, 다중 이미지 및 사상 연쇄 추론과 같은 매력적인 속성을 즐긴다. MM1은 또한 명령어 튜닝 후에 강한 소수 샷 학습 능력을 가능하게 한다. 이러한 강력한 결과는 제안된 MLLM 구축 레시피가 설계 원리를 규모 면에서 경쟁 모델로 변환한다는 것을 보여준다. 우리는 이러한 제시된 통찰력이 특정 모델링 구성 요소 및 데이터 소스가 진화하는 경우에도 관련성이 유지되기를 바란다.\n' +
      '\n' +
      '그림 2: MM1은 이미지 전반에 걸쳐 명령과 이유를 따를 수 있다. VILA[71]의 예제 및 이미지; VILA는 생각 사슬로 프롬프트될 때 올바르게 응답합니다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이 작업에 관련된 MLLM의 유형은 텍스트 및 시각적 토큰을 모두 소비하는 강력한 사전 훈련된 자기회귀 LLM에 기반하며, 후자는 이미지 인코더[5, 17, 28, 45, 64, 76, 90]를 통해 얻는다. 우리의 접근법은 Kosmos-1[45]과 유사한 디코더 전용 아키텍처를 기반으로 한다.\n' +
      '\n' +
      '최근 연구는 사전 훈련된 LLM 위에 시각적 지시 튜닝에 점점 더 초점을 맞추고 있다[63]. 명확한 예로는 LLaVA(-1.5/NeXT)[74, 76, 75, 76], MiniGPT-4[133], mPLUG-Owl(-2/Doc)[124, 124, 124], Otter[60, 61], InstructBLIP[24], Honeybee[12], SPHINX(-X)[36, 73] 등이 있다. 또한 명령어-튜닝 데이터 [15, 37, 66, 114, 131]를 구성하여 참조 및 접지 [14, 125, 57, 90, 116, 57], 이미지 생성 및 편집 [34, 54, 106]을 위한 MLLM을 가능하게 하는 문헌이 풍부하다.\n' +
      '\n' +
      '철저한 절제술에 초점을 맞춘 작업체, 특히 사전 훈련 쪽도 상대적으로 희박하다. VILA[71]은 멀티모달 사전 훈련의 다양한 구성 요소에 대한 연구에 중점을 두고 있지만 최적화 세부 사항이나 상세한 사전 훈련 평가 제공에는 미치지 못한다. 다른 측면에서 Emu2 [106]은 사전 훈련 최적화 매개변수 및 기본 모델 결과에 대한 세부 정보를 제공한다. 그러나, 그들은 다양한 구성 요소 결정을 정당화하는 삭제를 제공하지 않는다. IDEFICS[58]는 대규모 멀티모달 사전 훈련에 관한 세부 사항을 제공하는 또 다른 작업이다. 그러나 그들의 초점은 주로 폐쇄 소스 플라밍고[3] 모델을 밀접하게 복제하는 데 있다.\n' +
      '\n' +
      '이러한 이전 작업과 달리 하이퍼파라미터에서 데이터에 이르기까지 사전 훈련 전략의 모든 구성요소에 대한 세부 정보를 제공하는 것을 목표로 한다. 우리는 또한 멀티모달 사전 훈련 _vs._의 영향을 구별하는 데 도움이 되는 기본 사전 훈련 모델에 대한 결과를 제공한다. 명령어 튜닝. 또한 시각 인코더, 시각 언어 커넥터 및 사전 훈련 데이터 혼합물에 대한 결정의 정확한 영향에 대한 광범위한 삭제를 제공합니다.\n' +
      '\n' +
      '##3 건물 MM1의 레시피\n' +
      '\n' +
      '수행자 MLLM을 구축하는 것은 매우 경험적인 노력입니다. 고위 건축 설계 및 교육 절차는 명확하지만, 그 구체적인 형태와 실행은 명확하지 않다. 이 작업에서는 수행 모델에 도달하기 위해 수행한 삭마의 세부 사항을 제시한다. 우리는 디자인 결정의 세 가지 주요 축을 탐구한다:\n' +
      '\n' +
      '**아키텍처**: 우리는 미리 훈련된 다양한 이미지 인코더를 조사하고 LLM을 이러한 인코더와 연결하는 다양한 방법을 탐구한다.\n' +
      '* **Data**: 우리는 다양한 유형의 데이터와 상대적인 혼합물 가중치를 고려한다.\n' +
      '**트레이닝 절차**: 하이퍼파라미터를 포함하는 MLLM을 트레이닝하는 방법 및 모델의 어떤 부분들이 어떤 단계에서 트레이닝될지를 탐색한다.\n' +
      '\n' +
      '### 어블레이션을 위한 경험적 설정\n' +
      '\n' +
      '위의 각 축을 따라 좋은 선택이 무엇인지 식별하기 위해서는 모델 성능을 평가하는 효율적인 방법이 필요하다. 대규모 MLLM을 훈련하는 것은 상당한 자원을 필요로 하기 때문에, 우리는 삭제를 위해 단순화된 설정을 활용한다.\n' +
      '\n' +
      '보다 구체적으로, 우리는 축소된 모델의 더 작은 기본 구성을 사용한다. 우리는 건축 모듈 또는 데이터 소스 중 하나의 구성요소를 한 번에 수정하고 이러한 각 구성요소에 대한 설계 선택의 영향을 평가합니다. 이를 통해 모델 매개변수와 훈련 시간 측면에서 확장한 최종 모델-데이터 구성에 도달할 수 있다. 어블레이션을 위한 베이스 구성은 다음과 같다:\n' +
      '\n' +
      '**Image Encoder**: DFN-5B[31] 및 VeCap-300M[56]에서 CLIP 손실[91]로 훈련된 ViT-L/14[27] 모델; 336\\(\\times\\)336 크기의 이미지.\n' +
      '**Vision-Language Connector**: 144개의 이미지 토큰을 갖는 C-Abstractor[12].\n' +
      '***사전-트레이닝 데이터**: 캡션 이미지(45%), 인터리빙 이미지-텍스트 문서(45%), 및 텍스트 전용(10%) 데이터의 혼합.\n' +
      '*** 언어 모델**: 1.2B 트랜스포머 디코더 전용 언어 모델.\n' +
      '\n' +
      '다양한 설계 결정들을 평가하기 위해, COCO Captioning[18], NoCaps[2], TextCaps[104], VQAv2[38], TextVQA[105], VizWiz[39], GQA[46], OK-VQA[82]와 같은 다양한 VQA 및 캡션 태스크들에 대해 제로-샷 및 수-샷(4- 및 8-샷) 성능을 사용한다.\n' +
      '\n' +
      '### 모델 아키텍처 제안\n' +
      '\n' +
      '이 작업에서 우리는 LLM이 시각적 데이터를 처리할 수 있는 구성 요소를 분석한다. 구체적으로, 우리는 (1) 시각적 인코더를 가장 잘 사전 훈련하는 방법과 (2) 시각적 특징을 LLM의 공간에 브릿징하는 방법을 조사한다(도 3, 좌측 참조).\n' +
      '\n' +
      '**이미지 인코더 사전-트레이닝.** 대부분의 MLLM은 CLIP 사전-트레이닝된 이미지 인코더[74, 76, 124, 24]를 사용하는 반면, 최근의 작업들은 또한 이미지 인코더로서 DINOv2[109, 73]와 같은 비전-전용 자기-감독 모델들을 탐색하기 시작했다. 이러한 선행 연구와 유사하게, 우리는 사전 훈련된 이미지 인코더의 선택이 멀티모달 사전 훈련 후 및 명령어 튜닝 후 모두에서 다운스트림 결과에 실질적으로 영향을 미칠 수 있음을 발견한다. 여기서는 주로 이미지 해상도와 이미지 인코더 사전 훈련 목표의 중요성을 완화한다. 다른 사람들과는 다르게\n' +
      '\n' +
      '도 3: _Left:_ Model ablations: 어떤 시각적 인코더를 사용할지, 풍부한 시각적 데이터를 공급하는 방법 및 시각적 표현을 LLM에 연결하는 방법. _ 오른쪽:_ 데이터 삭마: 데이터 유형 및 혼합물입니다.\n' +
      '\n' +
      '여기에서 우리는 더 큰 이미지 인코더 중 일부를 사용할 수 있는 충분한 용량이 있는지 확인하기 위해 2.9B LLM(1.2B 대신)을 사용합니다.\n' +
      '\n' +
      '손실.__ 대비 손실. 대용량 이미지-텍스트 데이터세트에 대해 학습했을 때, 결과 모델은 다양한 형태의 이미지 분류 및 검색 작업에 대한 성능으로 입증된 바와 같이 이미지 데이터에 대한 강한 의미론적 이해를 가지고 있다[91]. 이러한 결과는 시각적 인코더에 의미론적 지식을 부여할 수 있는 대규모 이미지 텍스트 데이터의 가용성 때문에 가능했다. 보다 최근에는 자동으로 선별된 대규모 데이터 세트와 합성 캡션이 훨씬 더 강력한 인코더로 이어졌다[56, 31].\n' +
      '\n' +
      '.__ 복원 손실.___ 밀집 예측에 관한 한 CLIP 스타일 모델은 동일한 강력한 성능을 얻기 위해 고군분투한다[94, 95, 113]. 이 속성은 VQA 및 캡션과 같은 많은 작업이 상세한 이미지 이해를 필요로 하기 때문에 MLLM에 문제가 될 수 있다. 따라서 우리는 재구성 손실을 사용하여 학습된 이미지 인코더를 고려하여 이러한 손실이 이미지의 모든 부분을 명시적으로 캡처한다. 특히, 우리는 AIM[30]을 사용하는데, 이는 이미지 데이터에서만 신중하게 설계된 자기회귀 재구성 손실이 잘 확장된다는 것을 보여준다.\n' +
      '\n' +
      '**인코더 레슨: 이미지 해상도가 가장 큰 영향을 미치고, 모델 크기 및 트레이닝 데이터 구성이 그 뒤를 잇는다.** 표 1에서 볼 수 있듯이, 이미지 해상도를 224에서 336으로 증가시키면 대략적인 결과가 나타난다. 모든 아키텍처에 걸쳐 모든 메트릭이 3% 향상되었습니다. 모델 크기를 ViT-L에서 ViT-H로 증가시키면 매개변수가 두 배로 증가하며 일반적으로 1% 미만의 적당한 성능 증가가 발생한다. 마지막으로 합성 캡션 데이터 집합인 VeCap-300M [56]을 추가하면 몇 개의 샷 시나리오에서 1% 이상의 부스트를 얻을 수 있다.\n' +
      '\n' +
      '모형 유형의 경우 결과가 덜 결정적입니다. 대조적 방법은 재구성보다 더 높은 성능을 초래하는 경향이 있다. 특히, 300M 파라미터의 ViT-L에 기반한 인코더는 비슷한 크기(24 AIM 모델 중 20개만)의 AIM\\({}_{600\\text{M}}\\)에 비해 0.3%에서 1.5%의 성능 향상을 가져온다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline  & \\multicolumn{4}{c}{**Setup**} & \\multicolumn{4}{c}{**Results**} \\\\ \\hline \\multicolumn{2}{c}{Model} & \\multicolumn{1}{c}{Arch.} & \\multicolumn{1}{c}{Image Res. Data} & \\multicolumn{3}{c}{0-shot 4-shot 8-shot} \\\\ \\hline \\multirow{4}{*}{\\begin{tabular}{} \\end{tabular} } & AIM\\({}_{600\\text{M}}\\) & ViT/600M & & & 36.6 & 56.6 & 60.7 \\\\  & AIM\\({}_{1\\text{B}}\\) & ViT/1B & 224 & DFN-2B & 37.9 & 59.5 & 63.3 \\\\  & AIM\\({}_{3\\text{B}}\\) & ViT/3B & & & 38.9 & 60.9 & 64.9 \\\\ \\hline \\multirow{4}{*}{\n' +
      '\\begin{tabular}{} \\end{tabular} } & CLIP\\({}_{\\text{DFN}+\\text{VeCap}}\\) & ViT-L & & & DFN-5B+VeCap & 36.9 & 58.7 & 62.2 \\\\  & CLIP\\({}_{\\text{DFN}}\\) & ViT-H & 224 & DFN-5B & 37.5 & 57.0 & 61.4 \\\\  & CLIP\\({}_{\\text{DFN}+\\text{VeCap}}\\) & ViT-H & & & DFN-5B+VeCap & 37.5 & 60.0 & 63.6 \\\\ \\cline{1-1} \\cline{2-7}  & CLIP\\({}_{\\text{DFN}+\\text{VeCap}}\\) & ViT-L & & & 39.9 & 62.4 & 66.0 \\\\  & CLIP\\({}_{\\text{DFN}+\\text{VeCap}}\\) & ViT-H & 336 & DFN-5B+VeCap & 40.5 & **62.6** & 66.3 \\\\  & CLIP\\({}_{\\text{OpenAI}}\\) & ViT-L & & ImageText-400M & 39.3 & 62.2 & 66.1 \\\\ \\cline{1-1} \\cline{2-7}  & CLIP\\({}_{\\text{DFN}}\\) & ViT-H & 378 & DFN-5B & **40.9** & 62.5 & **66.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 상이한 이미지 인코더(2.9B LLM 포함)에 걸친 MM1 사전-트레이닝 절제. 데이터 열의 값들은 MM1. Recon.: Reconstructive loss가 아니라 이미지 인코더 자체의 초기 트레이닝을 위해 사용되었던 데이터에 대응한다는 점에 유의한다. AIM:[30]; DFN-2/5B:[31]; VeCap:VeCap-300M[56]; OpenAI[91].\n' +
      '\n' +
      '레이어는 추론에서 사용된다). 그럼에도 불구하고 이 수업은 데이터의 절반 미만에 대해 훈련되었기 때문에 AIM의 잠재력에 대해 결정적이지 않다. 유사하게, 널리 사용되는 오픈소싱 오픈AI 모델[91]은 비교 가능한 용량의 모델로 동등하게 수행되지만 DFN+VeCap 데이터 믹스에 대해 훈련된다.\n' +
      '\n' +
      '**Vision-Language Connector and Image Resolution.** 이 컴포넌트의 목표는 시각적 표현을 LLM의 공간으로 번역하는 것이다. 이미지 인코더가 ViT이므로, 이들의 출력은 단일 임베딩이거나, 입력 이미지 패치에 대응하는 그리드-배열된 임베딩의 세트이다. 따라서, 이미지 토큰들의 공간적 배열은 LLM 중 순차적인 것으로 변환될 필요가 있다. 동시에, 실제 이미지 토큰 표현들은 단어 임베딩 공간에 매핑된다.\n' +
      '\n' +
      '그렇게 하는 동안 두 가지 상충되는 요구 사항이 있습니다. 한편으로, 우리는 이미지 토큰 임베딩의 수를 증가시킴으로써 충족되는 이미지에서 가능한 한 많은 세부 사항을 캡처하고자 한다. 다른 측면에서, 특히 다중 이미지 입력의 경우, 이미지당 많은 수의 입력 토큰을 갖는 것은 계산적으로 어렵다.\n' +
      '\n' +
      '이미지를 표현하기 위해 64개 또는 144개의 토큰과 224개 및 336개의 서로 다른 이미지 해상도를 사용하는 것을 고려하고 다음 아키텍처 옵션을 고려한다.\n' +
      '\n' +
      '평균 풀링. [106]에 이어서 \\(n\\!\\)을 적용한다. times\\!n\\) average pooling on the output of ViT image encoder, followed the linear projection (\\(n\\in\\{8,12\\}\\))\n' +
      '\n' +
      '_Attention Pooling.__Attention Pooling. 이미지 토큰 표현이 LLM 입력 임베딩과 다른 공간에 있다는 사실에 동기 부여, \\(k\\) 학습 가능한 쿼리를 사용한 주의 풀링은 자연스러운 접근법이다. LLM(k\\(k\\in\\{64,144\\}\\)을 사용하여 단일 영상에서 입력 수를 변화시킬 수 있다.\n' +
      '\n' +
      '합성곱 매핑.___ 보다 최근에 꿀벌 [12]는 위의 문제를 연구하여 C-Abstractor 모듈을 제안하였다. 적응적 풀링을 통해 이미지 토큰의 개수를 변경할 수 있으면서 지역 정보를 보존하는 ResNet[41] 블록으로 구현된다.\n' +
      '\n' +
      '**VL 커넥터 레슨: 시각적 토큰의 수와 이미지 해상도가 가장 중요한 반면, VL 커넥터의 유형은 거의 영향을 미치지 않는다.** 도 4에 도시된 결과는 시각적 토큰의 수 또는/및 이미지 해상도가 증가함에 따라 제로- 및 소수-샷 성능이 모두 증가함을 보여준다. 그러나 문헌 [12]에 보고된 것과 달리 다른 건축 설계는 확실하게 더 강하게 생성하지 않는 것으로 판단된다.\n' +
      '\n' +
      '그림 4: 두 개의 이미지 해상도와 두 개의 이미지 토큰 크기에 대해 서로 다른 시각적 언어 커넥터에 걸쳐 0-샷, 4-샷 및 8-샷 절제.\n' +
      '\n' +
      '모델들 명령어 튜닝 후, 세 가지 아키텍처 모두 336px 및 114 토큰 설정에서 매우 유사한 결과를 달성한다. (미세 조정 결과는 부록 도 10 참조)\n' +
      '\n' +
      '### 사전 훈련 데이터 제거\n' +
      '\n' +
      '대규모 및 작업에 적합한 데이터는 수행 모델 훈련에서 가장 중요하다. 전형적으로, 모델들은 사전 트레이닝 및 명령어 튜닝의 두 단계로 트레이닝된다. 전자의 단계에서는 웹-스케일 데이터를 사용하고, 후자의 단계에서는 과제별 선별된 데이터를 사용한다. 다음에서는 사전 훈련 단계에 초점을 맞추고 데이터 선택을 정교화한다(그림 3, 오른쪽 참조).\n' +
      '\n' +
      'MLLM을 훈련하기 위해 두 가지 유형의 데이터가 일반적으로 사용된다: 쌍을 이루는 텍스트 설명이 있는 이미지로 구성된 캡션 데이터; 및 웹으로부터의 인터리빙된 이미지-텍스트 문서(자세한 내용은 부록 A.1 참조). 캡셔닝 데이터는 이미지와 높은 관련성을 갖는 비교적 짧은 텍스트를 포함하는 경향이 있다는 점에 유의한다. 반대로, 인터리빙된 데이터는 주변 이미지와 평균적으로 덜 관련성을 갖는 실질적으로 더 길고 더 다양한 텍스트를 갖는다. 마지막으로, 우리는 기본 LLM의 언어 이해 능력을 보존하는 데 도움이 되는 텍스트 전용 데이터를 포함한다. 데이터 세트의 전체 목록은 표 2에 요약되어 있다.\n' +
      '\n' +
      '우리는 대규모 데이터 교육을 완전히 활용하기 위해 여기에서 200k 단계를 훈련하는 유일한 예외를 제외하고 섹션 3.1에 설명된 삭제에 대해 동일한 모델 설정을 사용한다. 또한 데이터 혼합물의 효과를 더 잘 평가하기 위해 평가의 일부로 일반적으로 사용되는 텍스트 작업 집합인 TextCore1을 통합한다. 이것들은 다음과 같은 교훈으로 이어진다.\n' +
      '\n' +
      '각주 1: TextCore 태스크들은 ARC[22], PIQA[7], LAMBADA[89], WinoGrande[97], HellaSWAG[128], SciQ[118], TriviaQA[50], 및 WebQS[6]를 포함한다.\n' +
      '\n' +
      '**데이터 레슨 1: 인터리브된 데이터는 소수의 샷 및 텍스트 전용 성능에 중요한 반면 캡션 데이터는 제로 샷 성능을 올린다.** 그림 4(a)에서 인터리브된 데이터와 캡션된 데이터의 다양한 혼합에 걸쳐 결과를 제시한다. 제로샷 성능은 캡션 데이터의 양이 증가함에 따라 25.8%에서 39.3%로 지속적으로 증가한다. 그러나 4-샷 및 8-샷 성능을 위해서는 인터리빙되는 데이터의 최소 50%가 8-샷의 경우 61% 이상 또는 4-샷의 경우 58% 이상을 유지하는 것이 중요하다. 그것이 없으면 성능은 각각 45%와 43.7%로 급격히 떨어진다. 인터리빙된 데이터는 종종 상호 관련되는 다수의 이미지 및 수반되는 텍스트를 자연적으로 포함하기 때문에, 그러한 데이터는 본질적으로 소수의 테스트 입력과 유사하며, 이는 잘 정렬된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Data Type** & **Sources** & **Size** \\\\ \\hline \\multirow{2}{*}{Captioned Images} & CC3M [101], CC12M [13], HQIPT-204M [94], & \\multirow{2}{*}{2B image-text pairs} \\\\  & COYO [11], Web Image-Text-1B (Internal) & \\\\ \\multirow{2}{*}{Captioned Images (Synthetic)} & VeCap [56] & \\multirow{2}{*}{300M image-text pairs} \\\\  & Interleaved Image-Text & OBELICS [58], Web Interleaved (Internal) & 600M documents \\\\ \\multirow{2}{*}{Text-only} & Webpages, Code, Social media, & \\multirow{2}{*}{2T tokens} \\\\  & Books, Encyclopedic, Math & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 멀티모달 대형 언어 모델을 사전 훈련하기 위한 데이터 세트 목록.\n' +
      '\n' +
      '실증적인 결과를 가지고. 그러나 일반적인 평가가 캡션 문제에 크게 맞춰지는 특성(8개의 벤치마크 중 3개는 캡션)으로 인해 캡션 데이터는 특히 제로 샷 성능을 높인다. 흥미롭게도, 인터리브된 데이터의 사용은 소수의 샷 설정에서 이러한 매우 동일한 캡션 벤치마크에 대한 성능을 더욱 향상시킨다. 유사하게, 인터리빙된 데이터의 텍스트 전용 성능 이점은 인터리빙된 데이터에도 긴 형식의 텍스트가 포함될 가능성이 있다.\n' +
      '\n' +
      '**데이터 레슨 2: 텍스트 전용 데이터는 소수의 샷 및 텍스트 전용 성능에 도움이 된다.** 모델의 언어 이해 능력을 유지하기 위한 방법으로 텍스트 전용 데이터를 활용한다. 그림 4(b)에서 볼 수 있듯이 텍스트 전용 데이터와 캡션 데이터를 결합하면 몇 개의 샷 성능이 향상됩니다. 즉, 긴 텍스트는 모델이 더 나은 질문 응답 및 캡셔닝을 수행하기 위해 컨텍스트로서 다수의 이미지 및 텍스트 예제를 활용할 수 있게 한다. 반면에 텍스트만 인터리빙된 데이터와 결합하는 것은 비록 사소한 것이기는 하지만 성능 저하를 초래한다.\n' +
      '\n' +
      '도 5: 데이터 정리. 각 절제에 대해 텍스트코어, 0-샷, 4-샷 및 8-샷의 4가지 다른 메트릭을 제시한다. **(a)** 인터리브된 데이터와 캡션된 데이터 사이의 5가지 다른 혼합 비율을 제시하는 이미지 데이터의 결과. **(b)** 텍스트 전용 데이터가 있거나 없는 결과 텍스트 전용 데이터를 캡션 및 인터리빙 데이터와 별도로 혼합한다. **(c)** 이미지 데이터(캡션 및 인터리빙된)와 텍스트 전용 데이터 사이의 상이한 혼합 비율을 갖는 결과. **(d)** 캡션 데이터의 일부로서 VeCap을 포함하거나 포함하지 않는 결과.\n' +
      '\n' +
      '하나. 두 경우 모두 TextCore 수의 증가와 같이 텍스트 전용 성능이 증가한다.\n' +
      '\n' +
      '**데이터 레슨 3: 이미지와 텍스트 데이터의 신중한 혼합은 최적의 멀티모달 성능을 산출할 수 있고 강력한 텍스트 성능을 유지할 수 있다.** 위의 레슨은 텍스트 전용 데이터를 어떻게 가장 잘 조합하여 강한 이미지와 언어 이해를 모두 달성할 수 있는지에 대한 질문으로 이어진다. 그림 4(c)에서 우리는 이미지(캡션과 인터리빙)와 텍스트 전용 데이터 사이의 여러 혼합 비율을 실험한다. 캡션/인터리브드/텍스트 비율이 5:5:1일 때, 비교 가능한 텍스트 전용 이해 성능을 유지하면서 강력한 멀티모달 성능의 좋은 균형을 달성함을 알 수 있다.\n' +
      '\n' +
      '**데이터 레슨 4: 합성 데이터는 소수의 샷 학습에 도움이 된다.** 마지막으로 합성 캡션 데이터인 VeCap[56]의 중요성을 연구한다. 모든 캡션 데이터에 비해 7%에 불과하여 품질은 높지만 상대적으로 작습니다. 그림 4(d)에서 볼 수 있듯이 2.4%와 4%의 절대적 소수 샷 성능에서 자명하지 않은 증가를 제공한다.\n' +
      '\n' +
      '##4 최종 모델 및 훈련 레시피\n' +
      '\n' +
      'MM1 멀티모달 사전 훈련을 위한 최종 레시피를 결정하기 위해 이전 절제에서 결과를 수집한다:\n' +
      '\n' +
      '**Image Encoder**: 이미지 해상도의 중요성으로 인해, 우리는 DFN-5B에서 CLIP 대물렌즈로 미리 훈련된 378x378px 해상도의 ViT-H[27] 모델을 사용한다[31].\n' +
      '* **Vision-Language Connector**: 비주얼 토큰의 개수가 가장 중요하기 때문에 144개의 토큰이 포함된 VL 커넥터를 사용한다. 실제 아키텍처는 덜 중요한 것 같다. 우리는 C-Abstractor[12]를 선택한다.\n' +
      '* **Data**: 제로 및 소수 샷 성능을 모두 유지하기 위해, 우리는 45% 인터리브 이미지-텍스트 문서, 45% 이미지-텍스트 쌍 문서 및 10% 텍스트 전용 문서의 신중한 혼합을 사용한다.\n' +
      '\n' +
      '모델 성능을 향상시키기 위해 LLM 크기를 3B, 7B 및 30B 매개변수로 확장한다. 기본 LLM은 동일한 텍스트 전용 데이터 세트에서 자체 학습됩니다(Sec. 3.3 참조). LLM과 비주얼 인코더가 모두 사전 훈련됨에 따라 MM1에 대한 초기화로 사용하고 위의 데이터 믹스에 대해 200k 단계(approx. 100B 토큰) 동안 멀티모달 사전 훈련을 수행한다. 모든 모델은 시퀀스 길이 4096으로 완전히 동결되지 않고, 배치 크기가 512개 시퀀스인 378\\(\\times\\)378 해상도에서 시퀀스당 최대 16개의 이미지를 미리 훈련한다. 모든 모델은 AXLearn 프레임워크를 사용하여 훈련됩니다.2\n' +
      '\n' +
      '각주 2: [https://github.com/apple/axlearn](https://github.com/apple/axlearn)\n' +
      '\n' +
      '그림 6: 모델 크기의 함수로서 최적 피크 학습률. 데이터 포인트는 관련 모델 크기에 대해 최적인 8샷 성능을 달성한 실험을 나타낸다.\n' +
      '\n' +
      '**모델 스케일링.** 이 스케일에서 적절한 하이퍼파라미터 검색을 수행하는 것은 불가능합니다. 대신, LLMs [43, 44, 121, 122]의 확립된 스케일링 특성을 사용하여, 우리는 최소 규모인 9M, 85M, 302M 및 1.2B에서 학습률의 그리드 검색을 수행하는 반면, Sec. 3.23에서 식별된 구성 요소를 사용하여 최적의 학습률을 식별하고 더 큰 규모로 외삽한다. 우리는 로그 공간에서 선형 회귀를 사용하여 더 작은 모델에서 더 큰 모델로 외삽하고(그림 6 참조), (비매립) 매개변수 \\(N\\)의 수를 감안할 때 최적의 피크 학습 속도 \\(\\eta\\)를 다음과 같이 예측한다:\n' +
      '\n' +
      '각주 3: 그리드 검색을 위한 계산 비용을 줄이기 위해 \\(\\text{CLIP}_{\\text{DFN}+\\text{VeCap}}\\) ViT-L336px로 축소한 이미지 인코더만 예외이다.\n' +
      '\n' +
      '\\[\\eta=\\exp(-0.4214\\ln(N)-0.5535) \\tag{1}\\]\n' +
      '\n' +
      '[48]과 유사하게 예비 실험에서 검증 손실이 다운스트림 작업 성능과 강한 상관 관계가 없음을 발견했다. 따라서 곡선 적합을 위해 다운스트림 8-샷 평균 성능을 직접 사용한다.\n' +
      '\n' +
      '(N=3e^{10}\\)의 경우, 이 적합치는 최종 MM1-30B에서 사용하는 \\(\\eta=2.2e^{-5}\\)을 예측한다. 우리는 처음에 \\(\\lambda\\)로 표시된 체중 감퇴에 대한 합리적인 값을 결정하기 위해 유사한 절차를 수행했지만 궁극적으로 \\(\\lambda=0.1\\eta\\)과 같은 피크 학습률에 의한 체중 감퇴의 간단한 규칙이 모든 모델에서 잘 작동한다는 것을 발견했다. 모든 추가 훈련 세부 사항은 부록 B에 설명되어 있다.\n' +
      '\n' +
      '**Mixture-of-Experts(MoE)를 통한 스케일링.**MoE는 활성화된 파라미터를 일정하게 유지하면서 모델 파라미터의 총 수를 스케일링한다. 추론 속도를 크게 저하시키지 않고 더 큰 모델 용량을 즐깁니다. 최근 MoE는 언어[23, 29, 32, 49, 135], 멀티모달[70, 87] 및 컴퓨터 비전[16, 25, 55, 96] 작업에서 유망한 결과를 보여주었다.\n' +
      '\n' +
      '실험에서는 언어 모델의 FFN 계층에 더 많은 전문가를 추가하여 조밀한 모델의 스케일링을 추가로 탐구한다. 우리의 MoE 구현은 일반적으로 GShard[59] 및 ST-MoE[135]를 따른다. 구체적으로, Top-2 게이팅, 희소층당 64/32명의 전문가를 사용하고, 밀집층을 매-2/4층의 희소층으로 대체한다. 우리는 더 나은 전문가 부하 균형을 장려하기 위해 0.01 계수의 부하 균형 손실항을 채택한다. 또한 학습을 안정화하기 위해 0.001 계수를 갖는 라우터 z-손실 항을 채택한다. 조밀한 모델을 MoE로 변환하기 위해, 우리는 조밀한 언어 디코더를 MoE 언어 디코더로만 대체한다. 이미지 인코더와 비전 언어 커넥터는 동일하게 유지됩니다. MoE를 훈련하기 위해, 우리는 조밀한 백본4에 대해 발견되는 동일한 훈련 하이퍼-파라미터와 훈련 데이터 및 훈련 토큰을 포함하는 동일한 훈련 설정을 채택한다.\n' +
      '\n' +
      '각주 4: 조밀한 백본은 MoE 모델을 구성하기 위해 사용하는 조밀한 모델로 정의된다.\n' +
      '\n' +
      '**멀티모달 사전 훈련 결과.** 적절한 프롬프트를 통해 캡션 및 VQA 작업에 대한 사전 훈련 모델을 평가한다.5 표 3과 같이 제로 샷 및 소수 샷을 평가하고 퓨 샷 사전 훈련 성능과 비교한다. 우리는 우리의 모델을 더 큰 모델, 예를 들어, 우리의 30B 모델과 두 개의 80B 모델을 비교하는 것만을 비교한다는 점에 유의한다.\n' +
      '\n' +
      'MM1은 적은 수의 샷 성능에서 사전 훈련된 MLLM에 대해 발표된 모든 이전 작업보다 우수하다. 우리는 캡션 벤치마크와 VizWiz-QA 벤치마크에서 30B에서 우수한 성능을 보인다. VQAv2, TextVQA, OKVQA에서 우리는 그 규모에서 Emu2[106]와 비슷하다. 제로 샷 성능 6의 경우, 지침 미세 조정이 없어도, 우리 모델은 모든 모델 크기에 걸쳐 텍스트캡에서 호의적으로 수행되며 대부분의 벤치마크에 대해 작은 규모에서 플라밍고-3B와 비슷하다.\n' +
      '\n' +
      '각주 6: 우리는 연관된 소수의 샷 수에 대한 참조로서 제로 샷 결과를 제공하지만, 사전 트레이닝 혼합물이 연관된 평가 태스크 포맷과 얼마나 잘 일치하는지 대부분 나타내기 때문에 의도적으로 제로 샷 메트릭에 언덕-등반하지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Shot**} & \\multicolumn{3}{c}{**Captioning**} & \\multicolumn{3}{c}{**Visual Question Answering**} \\\\ \\cline{3-8}  & & \\multicolumn{3}{c}{COCO NoCaps TextCaps} & \\multicolumn{3}{c}{VQAv2 TextVQA} & \\multicolumn{1}{c}{VizWiz OKVQA} \\\\ \\hline \\multicolumn{8}{l}{_MM1-3B Model Comparisons_} \\\\ \\hline \\multirow{2}{*}{Flamingo-3B [3]} & 0\\({}^{\\dagger}\\) & 73.0 & – & – & 49.2 & 30.1 & 28.9 & 41.2 \\\\  & 8 & 90.6 & – & – & 55.4 & 32.4 & 38.4 & 44.6 \\\\  & & 73.5 & 55.6 & 63.3 & 46.2 & 29.4 & 15.6 & 26.1 \\\\ MM1-3B & 8 & **114.6** & **104.7** & **88.8** & **63.6** & **44.6** & **46.4** & **48.4** \\\\ \\hline \\multicolumn{8}{l}{_MM1-7B Model Comparisons_} \\\\ \\hline IDEFICS-9B [58] & 0\\({}^{\\dagger}\\) & 46.0* & 36.8 & 25.4 & 50.9 & 25.9 & 35.5 & 38.4 \\\\  & 8 & 97.0* & 86.8 & 63.2 & 56.4 & 27.5 & 40.4 & 47.7 \\\\ Flamingo-9B [3] & 0\\({}^{\\dagger}\\) & 79.4 & – & – & 51.8 & 31.8 & 28.8 & 44.7 \\\\  & 8 & 99.0 & – & – & 58.0 & 33.6 & 39.4 & 50.0 \\\\  & 8 & – & – & – & 52.9 & – & 34.4 & 42.8 \\\\  & 8 & – & – & – & 59.0 & – & 43.9 & – \\\\  & 8 & – & – & – & 59.0 & – & 43.9 & – \\\\  & 0 & 76.3 & 61.0 & 64.2 & 47.8 & 28.8 & 15.6 & 22.6 \\\\ MM1-7B & 8 & **116.3** & **106.6** & **88.2** & **63.6** & **46.3** & **45.3** & **51.4** \\\\ \\hline \\multicolumn{8}{l}{_MM1-30B Model Comparisons_} \\\\ \\hline \\multirow{2}{*}{IDEFICS-80B [58]} & 0\\({}^{\\dagger}\\) & 91.8* & 65.0 & 56.8 & 60.0 & 30.9 & 36.0 & 45.2 \\\\  & 8 & 114.3* & 105.7 & 77.6 & 64.8 & 35.7 & 46.1 & 55.1 \\\\  & 16 & 116.6* & 107.0 & 81.4 & 65.4 & 36.3 & 48.3 & 56.8 \\\\ \\hline \\multirow{2}{*}{Flamingo-80B [3]} & 0\\({}^{\\dagger}\\) & 84.3 & – & – & 56.3 & 35.0 & 31.6 & 50.6 \\\\  & 8 & 108.8 & – & – & 65.6 & 37.3 & 44.8 & 57.5 \\\\  & 16 & 110.5 & – & – & 66.8 & 37.6 & 48.4 & 57.8 \\\\  & 0 & – & – & – & 33.3 & 26.2 & 40.4 & 26.7 \\\\ Emu2-37B [106] & 8 & – & – & – & 67.8 & 49.3 & 54.7 & 54.1 \\\\  & 16 & – & – & – & 68.8 & 50.3 & 57.0 & 57.1 \\\\  & 0 & 70.3 & 54.6 & 64.9 & 48.9 & 28.2 & 14.5 & 24.1 \\\\ MM1-30B & 8 & 123.1 & 111.6 & 92.9 & 70.9 & 49.4 & 49.9 & 58.3 \\\\  & 16 & **125.3** & **116.0** & **97.6** & **71.9** & **50.6** & **57.9** & **59.3** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 멀티모달 사전 훈련 평가. (*) IDEFICS는 그 트레이닝 데이터에 PMD를 포함한다(COCO 포함). (\\(\\dagger\\)) 이러한 모델은 "0" 프롬프트에 두 개의 텍스트 전용 데모를 포함하는 반면 MM1은 그렇지 않다. 전체 표는 부록의 표 6을 참조하십시오.\n' +
      '\n' +
      '##5 감독 미세조정\n' +
      '\n' +
      '이 섹션에서는 이전 섹션에서 설명한 사전 훈련 모델 위에서 훈련된 감독 미세 조정(SFT) 실험에 대해 설명한다.\n' +
      '\n' +
      '**SFT 데이터 혼합물.** LLaVA-1.5[74] 및 LLaVA-NeXT[75]를 따르고, 대략 1M SFT 예를 포함하여 다양한 데이터 세트 세트에서 수집한다.\n' +
      '\n' +
      '* 대화 및 복잡한 추론을 위한 LLaVA-Conv 및 LLaVA-Complex[76], 상세한 이미지 설명을 위한 ShareGPT-4V[15]7을 포함하는 GPT-4 및 GPT-4V에 의해 생성된 명령어-응답 쌍; 각주 7: LVIS-Instruct4V[114]로도 실험했지만 ShareGPT-4V[15]를 사용하는 것보다 더 나은 성능을 관찰하지 못했기 때문에 최종 혼합물에 포함되지 않는다.\n' +
      '* Academic task oriented vision-language (VL) dataset, including (\\(i\\)) VQAv2[38], GQA[46], OKVQA[82], A-OKVQA[98], and COCO Captions[18] for natural images; (\\(ii\\)) OCRVQA[86], and TextCaps[104] for text-rich images; and (\\(iii\\)) DVQA[51], ChartQA[83], AI2D[52], DocVQA[85], InfoVQA[84], and Synthdog-En[53] for document and chart understanding.\n' +
      '* Text-only SFT data: ShareGPT[100]와 유사한 내부 데이터셋을 사용하여 Text-only instruction follow의 기능을 유지한다.\n' +
      '\n' +
      '학술 VL 데이터 세트는 LLaVA-1.5 [74]에 따라 명령어 추적 형식으로 포맷된다. 자세한 내용은 부록 0.A.3에 나와 있습니다. 모든 데이터 세트가 함께 혼합되어 훈련 중 무작위로 샘플링됩니다.8\n' +
      '\n' +
      '각주 8: 일부 다른 데이터 혼합 전략이 탐구되었지만, 이러한 데이터 세트를 단순히 혼합하는 것은 꿀벌 [12]의 관찰과 유사하게 이미 좋은 성능을 달성한다.\n' +
      '\n' +
      'SFT 동안, 우리는 이미지 인코더와 LLM 백본 _unfrozen_를 모두 유지한다; 다른 SFT 트레이닝 세부사항은 부록 0.B.2에 제공된다. 우리는 12개의 벤치마크에 걸쳐 모델을 평가한다(세부사항은 부록 0.C.2 참조).\n' +
      '\n' +
      '**더 높은 결의로 스케일링.** 직관적으로 더 높은 이미지 해상도는 더 나은 성능으로 이어집니다. 고해상도 SFT를 지원하기 위해 두 가지 접근 방식을 사용한다:\n' +
      '\n' +
      '**Qwen-VL[5] 및 BLIP2[65]에서 탐색된 바와 같이, 위치 임베딩 보간**, _예._. 위치 임베딩 보간 후, 비전 트랜스포머 백본은 미세 조정 동안 새로운 해상도에 적응된다. 이 방법을 통해 448\\(\\times\\)448, 560\\(\\times\\)560, 672\\(\\times\\)672의 이미지 해상도를 지원할 수 있는 모델을 성공적으로 미세 조정하였다. 672\\(\\times\\)672의 해상도에 대해 14\\(\\times\\)14의 패치 크기를 갖는 이미지는 2,304\\의 토큰으로 표현된다.\n' +
      '\n' +
      '**서브-이미지 분해**, 최근 SPHINX[73], Monkey[69], 및 LLaVA-NeXT[75]에 의해 소개된다. 2,000\\(2,000\\) 이상의 이미지 토큰 중에서 자기 주의력을 계산하는 것은 계산적으로 어려워 더 높은 이미지 해상도로의 추가 스케일링을 제한한다. SPHINX[73]에 이어 그림 6(a)와 같이 고해상도의 입력 영상인 _e.g._, 1344\\(\\times\\) 1344에 대해 672\\(\\times\\) 672의 5개의 영상을 구성하고 이를 독립적인 영상으로 영상 부호화기에 공급한다. 구체적으로, 먼저 입력 영상을 672\\(\\times\\)672로 하향 샘플링하고, 입력 영상을 1344\\(\\times\\)1344로 리사이징하고, 리사이징된 영상을 672\\(\\times\\)672의 4개의 하위 영상으로 분할하여 보다 상세한 시각적 정보를 보존한다. 각 서브 영상에 대한 위치 임베딩 보간을 이용하여 1792\\(\\times\\)1792의 높은 영상 해상도를 지원할 수 있다.\n' +
      '\n' +
      '### SFT 결과: 절제 및 분석\n' +
      '\n' +
      '**SOTA와의 비교.** 결과는 표 4에 요약되어 있다. 우리는 SFT 후 MM1 모델을 나타내기 위해 "-채팅"을 사용한다. 첫째, 평균적으로 MM1-3B-Chat 및 MM1-7B-Chat은 동일한 크기의 나열된 모든 모델보다 성능이 우수하여 이러한 모델 크기에 대한 새로운 기술의 상태를 설정한다. MM1-3B-Chat 및 MM1-7B-Chat은 VQAv2, TextVQA, ScienceQA, MMBench 및 보다 최근의 벤치마크들(MMMU 및 MathVista)에서 특히 강한 성능을 보인다.\n' +
      '\n' +
      '둘째, 전문가 64명이 있는 3B-MoE, 전문가 32명이 있는 6B-MoE 두 가지 MoE 모델을 탐색한다. 우리의 MoE 모델은 거의 모든 벤치마크에서 밀도가 높은 모델보다 균일하게 더 나은 성능을 달성한다. 이것은 향후 작업으로 남겨진 추가 스케일링을 위한 MoE의 큰 잠재력을 보여준다.\n' +
      '\n' +
      '셋째, 30B 모델 크기의 경우, TextVQA, SEED 및 MMMU에서 MM1-30B-Chat이 Emu2-Chat-37B[106] 및 CogVLM-30B[115]보다 우수하다. 동시 LLaVA-NeXT[75]와 비교하여, 우리는 또한 전반적으로 경쟁력 있는 성과를 달성한다. 그러나 LLaVA-NeXT는 각 이미지가 전송된 2,880개의 토큰으로 표시되기 때문에 다중 이미지 추론이나 몇 개의 샷 프롬프트를 지원하지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c|c c c c c c c} \\hline \\hline Model & VQAv2 & VQAT & SQAI & MMMU & MathV & MMEP & MMEC & MMB & SEED & POPE LLaVA & MM-Vet \\\\ \\hline \\multicolumn{13}{l}{_3B Model Comparison_} \\\\ \\hline MobileVLM [20] & – & 47.5 & 61.0 & –/– & – & 1288.9 & – & 59.6 & –/– & 84.9 & – & – \\\\ LLaVA-Phi [134] & 71.4 & 48.6 & 68.4 & –/– & – & 1335.1 & – & 59.8 & –/– & 85.0 & – & 28.9 \\\\ Imp-v1 [99] & 79.45 & 59.38 & 69.96 & –/– & – & 1434.0 & – & 66.49 & – & 88.02 & – & 33.1 \\\\ TinyLAVA [132] & 79.9 & 50.1 & 69.1 & –/– & – & 1464.9 & – & 66.9 & – & 86.4 & 75.8 & 32.0 \\\\ Buny [42] & 79.8 & – & 70.9 & 38.2/33.0 & – & 1488.8 & 289.3 & 68.6 & 62.5/ & 86.8 & – & – \\\\ Gemini Nano-2 [107] & 67.5 & 65.9 & – & 32.6/– & – & – & – & – & – & – & – \\\\ MMI-3B-Chat & 82.0 & 71.9 & 69.4 & 33.9/33.7 & 32.0 & 1482.5 & 279.3 & 75.9 & 63.0/68.8 & 87.4 & 72.1 & 43.7 \\\\ MM1-3B-MoE-Chat & 82.5 & 72.9 & 76.1 & 38.6/35.7 & 32.6 & 1409.4 & 303.1 & 78.7 & 63.9/69.4 & 87.6 & 76.8 & 42.2 \\\\ \\hline \\multicolumn{13}{l}{_7B Model Comparison_} \\\\ \\hline InstructBLIP-7B [24] & – & 50.1 & 60.5 & –/– & 25.3 & – & 36.0 & 53.4/– & – & 60.9 & 26.2 \\\\ Qwen-VL-Chat-TB [5] & 78.2 & 61.5 & 68.2 & 35.9/32.9 & – & 1487.5 & 360.7 & 60.6 & 58.2/65.4 & – & – & – \\\\ LiLaVA-1.5-7B [74] & 78.5 & 58.2 & 68.8 & –/– & – & 1510.7 & 316.1 & 64.3 & 58.6/66.1 & 85.9 & 63.4 & 31.1 \\\\ SharedGPTV-7B [15] & 80.6 & 60.4 & 68.4 & –/– & – & 1567.4 & 376.4 & 68.8 & –/– & – & 72.6 & – \\\\ LVIS-InvsT-TB [14] & 79.6 & 58.7 & 68.3 & –/– & – & 1528.2 & – & 66.2 & 60.6/– & 86.0 & 67.0 & 31.5 \\\\ VILA-7B [71] & 79.9 & 64.4 & 68.2 & –/– & – & 1531.3 & – & 68.9 & 61.1/– & 85.5 & 69.7 & 34.9 \\\\ SPHIN-Intern2 [36] & 75.5 & – & 70.4 & –/– & 35.5 & 1260.4 & 294.6 & 57.9 & 68.8/– & 86.9 & 57.6 & 36.5 \\\\ LaVa-NeXT-7B [75] & 81.8 & 64.9 & 70.1 & 35.8/– & 34.6 & 1519 & 332 & 67.4 & 7–70.2 & 86.53 & 81.6 & 43.9 \\\\ MMI-7B-Chat & 82.8 & 72.8 & 72.6 & 37.0/35.6 & 35.9 & 1529.3 & 328.9 & 79.0 & 64.0/69.9 & 86.6 & 81.5 & 42.1 \\\\ MM1-7B-MoE-Chat & 83.4 & 72.8 & 75.3 & 40.6/37.7 & 39.1 & 1629.0 & 370.0 & 79.7 & 64.9/70.4 & 87.6 & 82.0 & 47.0 \\\\ \\hline \\multicolumn{13}{l}{_30B Model Comparison_} \\\\ \\hline Emu2-Chat-37B [106] & 84.9 & 66.6 & – & 36.3/34.1 & – & – & – & 62.8/– & – & 48.5 \\\\ CogVLM-30B [115] & SQAI & 83.4 & 68.1 & – & 21.3/30.1 & – & – & – & – & – & – & 56.8 \\\\ LLaVA-NeXT-34B [75] & 83.7 & 69.5 & 81.5 & 11.4/47.4 & 46.5 & 1631 & 397 & 79.3 & 77.5 & 97.3 & 89.6 & 57.4 \\\\ MMI-30B-Chat & 83.7 & 73.5 & 81.0 & 44.7/40.3 & 39.4\\({}^{\\dagger}\\) & 1637.6 & 431.4 & 82.1 & 65.9/72.1 & 87.6 & 89.3 & 48.7 \\\\ \\hline Gemini Pro [107] & 71.2 & 74.6 & – & 47.9/– & 45.2 & – & 436.79 & 73.6 & –/70.7 & – & – & 64.3 \\\\ Gemini Ultra [107] & 77.8 & 82.3 & – & 59.4/– & 53.0 & – & – & – & – & – & – \\\\ GPT4V [1] & 77.2 & 78.0 & – & 56.8/55.7 & 49.9 & – & 517.4 & 75.8 & 67.3/69.1 & – & – & 67.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: MLLM 벤치마크에 대한 SOTA 모델과의 비교. VQAv2[38]; VQAT: TextVQA[105]; SQAI: ScienceQA-IMG[81]; MMMU[127]; MathV: MathVista[80]; MMEP/C: MME[33]의 인지/인지 분할; MMB: MMBench[78]; SEED: SEED-Bench[62]; POPE[68]; LLaVAW: LLaVA-Bench(In-the-Wild)[76]; MM-Vet[126]을 포함한다. MMU에 보고된 두 숫자는 각각 val 및 테스트 분할에 대한 성능을 나타낸다. SEED에 보고된 두 숫자는 SEED-벤치 전체와 이미지 부분에 대한 성능을 각각 나타낸다. (\\(\\dagger\\)) 8-shot prompting: 44.4.\n' +
      '\n' +
      'LLM에, 우리 것은 총 720개에 불과합니다. 이렇게 하면 여러 이미지를 포함하는 특정 응용 프로그램이 제한됩니다.\n' +
      '\n' +
      '**이미지 해상도의 영향.** 도 6(b)는 SFT 평가 메트릭의 평균 성능에 대한 입력 이미지 해상도의 영향을 보여준다(부록 C.3에 메타 평균을 계산하는 방법에 대한 세부 사항을 지연). 영상 해상도가 336 픽셀인 베이스라인 모델과 비교하여, 영상 해상도\\(1344\\!\\times\\!1344\\)을 지원함으로써 15%의 상대적 증가를 달성할 수 있다. 가장 큰 이미지 해상도는 \\(1792\\!\\times\\!1792\\)입니다. 평균 성능이 약간 감소합니다. 이는 많은 평가 이미지들이 이 해상도보다 작고, 크기 조정 아티팩트들이 모델 성능에 영향을 미칠 수 있기 때문이다.\n' +
      '\n' +
      '**사전 훈련의 영향.** 최신 MLLM과 달리 모델에 대한 대규모 사전 훈련을 수행합니다. 사전 훈련이 최종 모델 성능에 미치는 영향을 평가하기 위해 동일한 사전 훈련 실행에서 SFT를 수행하지만 다른 체크포인트 단계에서 수행한다. 이전 체크포인트 단계의 경우, 모델은 이후 체크포인트 단계보다 덜 고유한 데이터 샘플을 보았기 때문에 사전 훈련 데이터 양의 중요성을 측정한 것이다. 그림 6(c)에서, 우리는 모델이 더 많은 사전 훈련 데이터를 볼수록 일관되게 개선된다는 것을 보여준다.\n' +
      '\n' +
      '**SFT 이후의 소수의 샷 체인 추론.** 섹션 3.3에서 볼 수 있듯이 MM1은 인터리브된 데이터 덕분에 샷 기능을 거의 얻지 못한다. 미세 조정 데이터에는 단일 이미지 예제만 포함되어 있지만 MM1-30B-Chat은 여전히 다중 이미지 추론을 나타낸다. 이것은 그림 2와 MathVista [80]에서 정성적으로 보여지며, 여기서 우리는 생각 사슬 프롬프트로 소수의 샷 성능을 평가한다: 4-샷 성능은 0-샷(**39.4**)보다 2.5점 높은 **41.9**이다.\n' +
      '\n' +
      '가장 성능이 좋은 고해상도 SFT 모델은 이미지당 720개의 토큰을 사용합니다. 이것은 문맥 길이로 인해 4개 이상의 문맥 내 예제를 사용할 때 어려운 일이다. 더 많은 예들을 허용하기 위해, 우리는 _mixed resolution in-context examples_ formulation을 탐색하며, 여기서 우리는 더 낮은 해상도로 예시들 중 일부를 공급한다(자세한 내용은 부록 C.5 참조). 8개의 문맥 내 예제와 함께 이 공식을 사용하면 MathVista에 대한 성능이 **44.4**로 증가한다.\n' +
      '\n' +
      '그림 7: 우리는 SFT 성능에 대한 이미지 해상도와 사전 훈련의 영향을 연구한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '* [7] Bisk, Y., Zellers, R., Le bras, R., Gao, J., Choi, Y.: Piqua: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence **34**(05), 7432-7439 (Apr 2020). [https://doi.org/10.1609/aaai.v34i05.6239](https://doi.org/10.1609/aaai.v34i05.6239), [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239)\n' +
      '* [8] Black, K., Janner, M., Du, Y., Kostrikov, I., Levine, S.: Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023)\n' +
      '* [9] Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021)\n' +
      '* [10] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems **33**, 1877-1901 (2020)\n' +
      '* [11] Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: Coyo-700m: Image-text pair dataset. [https://github.com/kakaobrain/coyo-dataset](https://github.com/kakaobrain/coyo-dataset) (2022)\n' +
      '* [12] Cha, J., Kang, W., Mun, J., Roh, B.: Honeybee: Locality-enhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742 (2023)\n' +
      '* [13] Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In: CVPR (2021)\n' +
      '* [14] Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm\'s referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)\n' +
      '* [15] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 (2023)\n' +
      '* [16] Chen, T., Chen, X., Du, X., Rashwan, A., Yang, F., Chen, H., Wang, Z., Li, Y.: Adamv-moe: Adaptive multi-task vision mixture-of-experts. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 17346-17357 (October 2023)\n' +
      '* [17] Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C.R., Goodman, S., Wang, X., Tay, Y., et al.: Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565 (2023)\n' +
      '* [18] Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollar, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)\n' +
      '* [19] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. Journal of Machine Learning Research **24**(240), 1-113 (2023)\n' +
      '* [20] Chu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei, F., Zhang, X., Zhang, B., Wei, X., et al.: Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886 (2023)\n' +
      '* [21] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)\n' +
      '* [22] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv **abs/1803.05457** (2018), [https://api.semanticscholar.org/CorpusID:3922816](https://api.semanticscholar.org/CorpusID:3922816)* [23] Dai, D., Deng, C., Zhao, C., Xu, R.X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y.K., Huang, P., Luo, F., Ruan, C., Sui, Z., Liang, W.: Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models (2024)\n' +
      '* [24] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)\n' +
      '* [25] Daxberger, E., Weers, F., Zhang, B., Gunter, T., Pang, R., Eichner, M., Emmersberger, M., Yang, Y., Toshev, A., Du, X.: Mobile v-moes: Scaling down vision transformers via sparse mixture-of-experts (2023)\n' +
      '* [26] Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)\n' +
      '* [27] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n' +
      '* [28] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 (2023)\n' +
      '* [29] Du, N., Huang, Y., Dai, A.M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.W., Firat, O., Zoph, B., Fedus, L., Bosma, M.P., Zhou, Z., Wang, T., Wang, E., Webster, K., Pellat, M., Robinson, K., Meier-Hellstern, K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y., Chen, Z., Cui, C.: GLaM: Efficient scaling of language models with mixture-of-experts. In: Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., Sabato, S. (eds.) Proceedings of the 39th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 162, pp. 5547-5569. PMLR (17-23 Jul 2022), [https://proceedings.mlr.press/v162/du22c.html](https://proceedings.mlr.press/v162/du22c.html)\n' +
      '* [30] El-Nouby, A., Klein, M., Zhai, S., Bautista, M.A., Shankar, V., Toshev, A., Susskind, J., Joulin, A.: Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541 (2024)\n' +
      '* [31] Fang, A., Jose, A.M., Jain, A., Schmidt, L., Toshev, A., Shankar, V.: Data filtering networks. arXiv preprint arXiv:2309.17425 (2023)\n' +
      '* [32] Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity (2022)\n' +
      '* [33] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)\n' +
      '* [34] Fu, T.J., Hu, W., Du, X., Wang, W.Y., Yang, Y., Gan, Z.: Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102 (2023)\n' +
      '* [35] Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., Zou, A.: A framework for few-shot language model evaluation (12 2023). [https://doi.org/10.5281/zenodo.10256836](https://doi.org/10.5281/zenodo.10256836), [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)\n' +
      '* [36] Gao, P., Zhang, R., Liu, C., Qiu, L., Huang, S., Lin, W., Zhao, S., Geng, S., Lin, Z., Jin, P., et al.: Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935 (2024)* [37] Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W., Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 (2023)\n' +
      '* [38] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6904-6913 (2017)\n' +
      '* [39] Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3608-3617 (2018)\n' +
      '* [40] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16000-16009 (2022)\n' +
      '* [41] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n' +
      '* [42] He, M., Liu, Y., Wu, B., Yuan, J., Wang, Y., Huang, T., Zhao, B.: Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530 (2024)\n' +
      '* [43] Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T.B., Dhariwal, P., Gray, S., et al.: Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701 (2020)\n' +
      '* [44] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L.A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J.W., Vinyals, O., Sifre, L.: Training compute-optimal large language models (2022)\n' +
      '* [45] Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, F.: Language is not all you need: Aligning perception with language models (2023)\n' +
      '* [46] Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and compositional question answering. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6700-6709 (2019)\n' +
      '* [47] IDEFICS: Introducing deficts: An open reproduction of state-of-the-art visual language model. [https://huggingface.co/blog/idefics](https://huggingface.co/blog/idefics) (2023)\n' +
      '* [48] Isik, B., Ponomareva, N., Hazimeh, H., Paparas, D., Vassilvitskii, S., Koyejo, S.: Scaling laws for downstream task performance of large language models (2024)\n' +
      '* [49] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., de las Casas, D., Hanna, E.B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L.R., Saulnier, L., Lachaux, M.A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T.L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., Sayed, W.E.: Miktral of experts (2024)\n' +
      '* [50] Joshi, M., Choi, E., Weld, D.S., Zettlemoyer, L.: Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension (2017)\n' +
      '* [51] Kafle, K., Price, B., Cohen, S., Kanan, C.: Dvqa: Understanding data visualizations via question answering. In: CVPR (2018)\n' +
      '* [52] Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., Farhadi, A.: A diagram is worth a dozen images. In: ECCV (2016)* [53] Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.: Ocr-free document understanding transformer. In: ECCV (2022)\n' +
      '* [54] Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal language models. arXiv preprint arXiv:2305.17216 (2023)\n' +
      '* [55] Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C.R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-experts from dense checkpoints. In: The Eleventh International Conference on Learning Representations (2023), [https://openreview.net/forum?id=T5nUQDrM4u](https://openreview.net/forum?id=T5nUQDrM4u)\n' +
      '* [56] Lai*, J., Zhang*, H., Zhang, B., Wu, W., Bai, F., Timofeev, A., Du, X., Gan, Z., Shan, J., Chuah, C.N., Yang, Y., Cao, M.: Veclip: Improving clip training via visual-enriched captions (2024), [https://arxiv.org/abs/2310.07699](https://arxiv.org/abs/2310.07699)\n' +
      '* [57] Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)\n' +
      '* [58] Laurencon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., Cord, M., Sanh, V.: Obelics: An open web-scale filtered dataset of interleaved image-text documents (2023)\n' +
      '* [59] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., Chen, Z.: {GS}hard: Scaling giant models with conditional computation and automatic sharding. In: International Conference on Learning Representations (2021), [https://openreview.net/forum?id=qrwe7XHTmYb](https://openreview.net/forum?id=qrwe7XHTmYb)\n' +
      '* [60] Li, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C., Liu, Z.: Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425 (2023)\n' +
      '* [61] Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)\n' +
      '* [62] Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking multimodal lms with generative comprehension. arXiv preprint arXiv:2307.16125 (2023)\n' +
      '* [63] Li, C., Gan, Z., Yang, Z., Yang, J., Li, L., Wang, L., Gao, J.: Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 (2023)\n' +
      '* [64] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models (2023)\n' +
      '* [65] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)\n' +
      '* [66] Li, L., Yin, Y., Li, S., Chen, L., Wang, P., Ren, S., Li, M., Yang, Y., Xu, J., Sun, X., et al.: M\\({}^{3}\\)it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387 (2023)\n' +
      '* [67] Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 (2019)\n' +
      '* [68] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)\n' +
      '* [69] Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: Image resolution and text label are important things for large multimodal models. arXiv preprint arXiv:2311.06607 (2023)\n' +
      '* [70] Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Huang, J., Zhang, J., Ning, M., Yuan, L.: Moe-llava: Mixture of experts for large vision-language models (2024)* [71] Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J., Shoeybi, M., Han, S.: Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533 (2023)\n' +
      '* [72] Lin, T., Maire, M., Belongie, S.J., Bourdev, L.D., Girshick, R.B., Hays, J., Perona, P., Ramanan, D., Doll\'a r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. CoRR **abs/1405.0312** (2014), [http://arxiv.org/abs/1405.0312](http://arxiv.org/abs/1405.0312)\n' +
      '* [73] Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al.: Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575 (2023)\n' +
      '* [74] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 (2023)\n' +
      '* [75] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved reasoning, ocr, and world knowledge (January 2024), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n' +
      '* [76] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)\n' +
      '* [77] Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al.: Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437 (2023)\n' +
      '* [78] Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 (2023)\n' +
      '* [79] Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems **32** (2019)\n' +
      '* [80] Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.W., Galley, M., Gao, J.: Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 (2023)\n' +
      '* [81] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.W., Zhu, S.C., Tafjord, O., Clark, P., Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for science question answering. NeurIPS (2022)\n' +
      '* [82] Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question answering benchmark requiring external knowledge. In: Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. pp. 3195-3204 (2019)\n' +
      '* [83] Masry, A., Long, D.X., Tan, J.Q., Joty, S., Hoque, E.: Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244 (2022)\n' +
      '* [84] Mathew, M., Bagal, V., Tito, R., Karatzas, D., Valveny, E., Jawahar, C.: Infographicvqa. In: WACV (2022)\n' +
      '* [85] Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: WACV (2021)\n' +
      '* [86] Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text in images. In: ICDAR (2019)\n' +
      '* [87] Mustafa, B., Ruiz, C.R., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal contrastive learning with LIMoe: the language-image mixture of experts. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in Neural Information Processing Systems (2022), [https://openreview.net/forum?id=Qy1D9JyMBgo](https://openreview.net/forum?id=Qy1D9JyMBgo)\n' +
      '* [88] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)* [89] Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q.N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., Fernandez, R.: The lambda dataset: Word prediction requiring a broad discourse context (2016)\n' +
      '* [90] Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)\n' +
      '* [91] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [92] Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 (2021)\n' +
      '* [93] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research **21**(1), 5485-5551 (2020)\n' +
      '* [94] Ranasinghe, K., McKinzie, B., Ravi, S., Yang, Y., Toshev, A., Shlens, J.: Perceptual grouping in contrastive vision-language models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5571-5584 (2023)\n' +
      '* [95] Rao, Y., Zhao, W., Chen, G., Tang, Y., Zhu, Z., Huang, G., Zhou, J., Lu, J.: Denseclip: Language-guided dense prediction with context-aware prompting. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18082-18091 (2022)\n' +
      '* [96] Ruiz, C.R., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Pinto, A.S., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. In: Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems (2021), [https://openreview.net/forum?id=NGPmH3vbAA_](https://openreview.net/forum?id=NGPmH3vbAA_)\n' +
      '* [97] Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y.: Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM **64**(9), 99-106 (aug 2021). [https://doi.org/10.1145/3474381](https://doi.org/10.1145/3474381), [https://doi.org/10.1145/3474381](https://doi.org/10.1145/3474381)\n' +
      '* [98] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A benchmark for visual question answering using world knowledge. In: ECCV (2022)\n' +
      '* [99] Shao, Z., Ouyang, X., Yu, Z., Yu, J.: Imp: An emprical study of multimodal small language models (2024), [https://huggingface.co/MILVLG/imp-v1-3b](https://huggingface.co/MILVLG/imp-v1-3b)\n' +
      '* [100] ShareGPT: Sharegpt. [https://sharegpt.com/](https://sharegpt.com/) (2023)\n' +
      '* [101] Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2556-2565 (2018)\n' +
      '* [102] Sharma, S., El Asri, L., Schulz, H., Zumer, J.: Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. CoRR **abs/1706.09799** (2017), [http://arxiv.org/abs/1706.09799](http://arxiv.org/abs/1706.09799)\n' +
      '* [103] Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., Catanzaro, B.: Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019)\n' +
      '* [104] Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image captioning with reading comprehension. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16. pp. 742-758. Springer (2020)* [105] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8317-8326 (2019)\n' +
      '* [106] Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., et al.: Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286 (2023)\n' +
      '* [107] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [108] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.T., Jin, A., Bos, T., Baker, L., Du, Y., et al.: Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022)\n' +
      '* [109] Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., Xie, S.: Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024)\n' +
      '* [110] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)\n' +
      '* [111] Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems **34**, 200-212 (2021)\n' +
      '* [112] Vedantam, R., Zitnick, C.L., Parikh, D.: Cider: Consensus-based image description evaluation. CoRR **abs/1411.5726** (2014), [http://arxiv.org/abs/1411.5726](http://arxiv.org/abs/1411.5726)\n' +
      '* [113] Wang, F., Mei, J., Yuille, A.: Sclip: Rethinking self-attention for dense vision-language inference. arXiv preprint arXiv:2312.01597 (2023)\n' +
      '* [114] Wang, J., Meng, L., Weng, Z., He, B., Wu, Z., Jiang, Y.G.: To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574 (2023)\n' +
      '* [115] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)\n' +
      '* [116] Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023)\n' +
      '* [117] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021)\n' +
      '* [118] Welbl, J., Liu, N.F., Gardner, M.: Crowdsourcing multiple choice science questions. In: Derczynski, L., Xu, W., Ritter, A., Baldwin, T. (eds.) Proceedings of the 3rd Workshop on Noisy User-generated Text. pp. 94-106. Association for Computational Linguistics, Copenhagen, Denmark (Sep 2017). [https://doi.org/10.18653/v1/W17-4413](https://doi.org/10.18653/v1/W17-4413), [https://aclanthology.org/W17-4413](https://aclanthology.org/W17-4413)\n' +
      '* [119] Wenzek, G., Lachaux, M.A., Conneau, A., Chaudhary, V., Guzman, F., Joulin, A., Grave, E.: Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359 (2019)\n' +
      '* [120] Wortsman, M., Liu, P.J., Xiao, L., Everett, K., Alemi, A., Adlam, B., Co-Reyes, J.D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-dickstein, J., Xu, K., Lee, J., Gilmer, J., Kornblith, S.: Small-scale proxies for large-scale transformer training instabilities (2023)* [121] Yang, G., Hu, E.J.: Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522 (2020)\n' +
      '* [122] Yang, G., Hu, E.J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., Gao, J.: Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer (2022)\n' +
      '* [123] Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 (2023)\n' +
      '* [124] Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023)\n' +
      '* [125] You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. In: ICLR (2024)\n' +
      '* [126] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023)\n' +
      '* [127] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al.: Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 (2023)\n' +
      '* [128] Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., Choi, Y.: Hellaswag: Can a machine really finish your sentence? (2019)\n' +
      '* [129] Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang, L., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949 (2023)\n' +
      '* [130] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022)\n' +
      '* [131] Zhao, B., Wu, B., Huang, T.: Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087 (2023)\n' +
      '* [132] Zhou, B., Hu, Y., Weng, X., Jia, J., Luo, J., Liu, X., Wu, J., Huang, L.: Tinyllava: A framework of small-scale large multimodal models. arXiv preprint arXiv:2402.14289 (2024)\n' +
      '* [133] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)\n' +
      '* [134] Zhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., Tang, J.: Llava-phi: Efficient multimodal assistant with small language model. arXiv preprint arXiv:2401.02330 (2024)\n' +
      '* [135] Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus, W.: St-moe: Designing stable and transferable sparse expert models (2022)\n' +
      '\n' +
      '## 부록 0. 데이터세트 상세\n' +
      '\n' +
      '###인터리브 이미지-텍스트 데이터\n' +
      '\n' +
      'OBELICS[58]와 유사한 프로세스에 따라 1B 이미지와 500B 텍스트 토큰을 포함하는 500M 인터리빙된 이미지 텍스트 문서 데이터 세트를 구성한다. 이 500M 문서들은 Sec. 0.A.2에 기술된 3B HTML 파일들의 집합으로부터 구축된다. HTML 파일들 각각에서 텍스트 본문 레이어와 모든 <img> 태그들을 추출한다. 이미지가 없거나 30개 이상의 이미지가 있는 문서를 제거합니다. 그런 다음 이미지를 다운로드하고 텍스트의 원래 위치에 삽입합니다. 마지막으로, 저화질 및 반복 영상을 제거하기 위해 **영상 필터링** 및 **영상 중복제거**를 수행한다.\n' +
      '\n' +
      '이미지 필터링 동안 바이트 및/또는 헤더, 종횡비가 1/2 미만 또는 2 초과이거나, 너무 작거나(100px 미만) 너무 크거나(10,000px 초과), 또는 URL에 _logo_, _button_, _icon_, _plugin_ 또는 _widget_가 포함된 이미지를 제거한다. 이미지 중복을 제거하는 동안 데이터 세트에서 URL 또는 MD5 해시가 10회 이상 나타난 이미지를 제거한다. 또한 이미지가 한 페이지에 여러 번 나타날 때 첫 번째 모습만 유지합니다.\n' +
      '\n' +
      '### Text-Only Data\n' +
      '\n' +
      '150B 영문 HTML 파일의 초기 웹 코퍼스에서 주요 내용을 나타내는 HTML에 도달하기 위해 상용구 제거를 수행한다. 그런 다음 GPT-3 [10] 및 CCNet [119]와 유사한 프로세스를 수행하여 너무 짧거나 비속어를 포함하거나 품질이 낮은 문서로 간주되는 문서를 필터링합니다. 정확한 해시 매칭과 LSH 기반 근접 중복 검출을 사용하여 데이터를 중복 해제한다. 이러한 방법을 사용하여 3B HTML 파일에 도달한다.\n' +
      '\n' +
      '###시각적 명령어 튜닝 데이터\n' +
      '\n' +
      '최종 SFT 데이터 혼합물은 다양한 데이터 세트를 포함하며 대부분 LLaVA-1.5[74] 및 LLaVA-NeXT[75]를 따른다. 구체적으로,\n' +
      '\n' +
      '* 모델이 긴 형태의 상세한 응답을 제공하고 대화를 수행하도록 장려하기 위해, 우리는 이전 작업을 따르고, 기존의 GPT-4 생성 데이터(LLaVA-Conv 및 LLaVA-Complex[76]) 및 GPT-4V 생성 데이터(ShareGPT-4V[15])를 모델 훈련에 사용한다. 또한 LAION-GPT4V를 실험했지만 추가 성능 개선을 관찰하지 못하여 최종 혼합물에 포함되지 않았다.\n' +
      '* 더 나은 멀티모달 이해 능력으로 모델을 향상시키기 위해, 우리는 다양한 학문적 과제 지향 멀티모달 데이터 세트를 사용한다. 이 데이터 세트는 이미지 캡셔닝의 형태이거나 짧은 답변의 VQA 형태이다. 구체적으로,\n' +
      '*자연 영상에 대하여: VQAv2[38], GQA[46], OKVQA[82], A-OKVQA[98], COCO Captions[18];\n' +
      '* 텍스트가 풍부한 이미지들에 대해: OCRVQA[86], 및 TextCaps[104];\n' +
      '* 문서 및 차트 이해를 위해: DVQA[51], ChartQA[83], AI2D[52], DocVQA[85], InfoVQA[84], Synthdog-En[53];\n' +
      '* 모델의 텍스트 전용 명령어 후속 기능을 향상시키기 위해 소량의 텍스트 전용 SFT 데이터에도 혼합합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l} \\hline \\hline\n' +
      '**Datasets** & **Size** & **Prompting Strategy** \\\\ \\hline Text-only SFT & 13k & – \\\\ \\hline LLaVA-Conv [76] & 57k & \\\\ LLaVA-Complex [76] & 77k & – \\\\ ShareGPT-4V [15] & 102k & \\\\ \\hline VQAv2 [38] & 83k & \\\\ GQA [46] & 72k & \\\\ OKVQA [82] & 9k & \\\\ OCRVQA [86] & 80k & \\\\ DVQA [51] & 200k & “Answer the question using a single word or phrase.” \\\\ ChartQA [83] & 18k & \\\\ AI2D [52] & 3k & \\\\ DocVQA [85] & 39k & \\\\ InfoVQA [84] & 24k & \\\\ Synthdog-en [53] & 500k & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l|l|l} \\hline \\hline\n' +
      '**Datasets** & **Size** & **Prompting Strategy** \\\\ \\hline Text-only SFT & 13k & – \\\\ \\hline LLaVA-Conv [76] & 57k & \\\\ LLaVA-Complex [76] & 77k & – \\\\ ShareGPT-4V [15] & 102k & \\\\ \\hline VQAv2 [38] & 83k & \\\\ GQA [46] & 72k & \\\\ OKVQA [82] & 9k & \\\\ OCRVQA [86] & 80k & \\\\ DVQA [51] & 200k & phrase.” \\\\ ChartQA [83] & 18k & \\\\ AI2D [52] & 3k & \\\\ DocVQA [85] & 39k & \\\\ InfoVQA [84] & 24k & \\\\ Synthdog-en [53] & 500k & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l|l|l} \\hline \\hline\n' +
      '**Datasets** & **Size** & **Prompting Strategy** \\\\ \\hline Text-only SFT & 13k & – \\\\ \\hline LLaVA-Conv [76] & 57k & \\\\ LLaVA-Complex [76] & 77k & – \\\\ ShareGPT-4V [15] & 102k & \\\\ \\hline VQAv2 [38] & 83k & \\\\ GQA [46] & 72k & \\\\ OKVQA [82] & 9k & \\\\ OCRVQA [86] & 80k & \\\\ DVQA [51] & 200k & phrase.” \\\\ ChartQA [83] & 18k & \\\\ AI2D [52] & 3k & \\\\ DocVQA [85] & 39k & \\\\ InfoVQA [84] & 24k & \\\\ Synthdog-en [53] & 500k & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l|l|l} \\hline \\hline\n' +
      '**Datasets** & **Size** & **Prompting Strategy** \\\\ \\hline Text-only SFT & 13k & – \\\\ \\hline LLaVA-Conv [76] & 57k & \\\\ LLaVA-Complex [76] & 77k & – \\\\ ShareGPT-4V [15] & 102k & \\\\ \\hline VQAv2 [38] & 83k & \\\\ GQA [46] & 72k & \\\\ OKVQA [82] & 9k & \\\\ OCRVQA [86] & 80k & \\\\ DVQA [51] & 200k & phrase.” \\\\ ChartQA [83] & 18k & \\\\ AI2D [52] & 3k & \\\\ DocVQA [85] & 39k & \\\\ InfoVQA [84] & 24k & \\\\ Synthdog-en [53] & 500k & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 감독 미세 조정에 사용되는 데이터 세트의 목록.\n' +
      '\n' +
      '학술 과제 지향 이미지 캡션 및 VQA 데이터 세트는 LLaVA-1.5[74]에 따라 명령어 후속 형식으로 포맷되며 자세한 프롬프트는 표 5에 요약되어 있다.\n' +
      '\n' +
      '## 부록 0.B 훈련 세부사항\n' +
      '\n' +
      '### Pre-training\n' +
      '\n' +
      '**배치 크기 및 조합.** 단순화를 위해 모든 MM1 모델은 512의 동일한 배치 크기 및 4096의 최대 디코더 시퀀스 길이로 사전 트레이닝된다. 입력 시퀀스당 최대 16개의 이미지를 허용하며, 각각의 이미지는 144개의 토큰을 디코더에 입력으로서 생성한다. 따라서 배치당 대략 1M 텍스트 토큰 및 1M 이미지 토큰이 생성됩니다. 각 입력 시퀀스는 (1) 인터리빙, (2) 패킹된 이미지-텍스트 쌍 또는 (3) 텍스트 전용 데이터의 세 가지 유형의 입력 소스 중 하나로부터 샘플링되며, 샘플링 확률은 각각 45%, 45% 및 10%이다. 시퀀스 차원을 따라 이미지-텍스트 쌍 또는 인터리빙된 문서를 패킹할 때, 우리는 토큰이 예제 경계를 넘어 주목받는 것을 방지하기 위해 셀프-어텐션 마스크를 수정한다. 특히, 이미지-텍스트 쌍의 경우, 이것은 강한 소수의 샷 성능을 유지하는 데 중요했다.\n' +
      '\n' +
      '샘플링/혼합 절차는 오프라인에서 한 번 수행되고 사전 훈련 혼합물의 고정 _결정론적_ 스냅샷으로 저장됩니다. 이는 사전 훈련 혼합물 자체에 대한 우리의 삭제를 제외하고, 본 논문의 모든 모델은 동일한 순서로 동일한 예제에 대해 훈련된다는 것을 의미한다. 초기 실험에서 입력 파이프라인의 다른 무작위 종자가 결과 모델에 무시할 수 없는 영향을 미칠 수 있음을 보여주었기 때문에 이것이 결과의 내부 재현성을 보장하는 데 중요하다는 것을 발견했다.\n' +
      '\n' +
      '**학습 속도 스케쥴.** 멀티모달 사전 트레이닝의 경우, MM1은 2000 단계의 초기 선형 워밍업과 함께 표준 코사인 학습 속도 감쇠 스케줄을 채용한다. 그 다음, 학습률은 \\(2e5\\) 훈련 단계 동안 최고치의 10%까지 감쇠된다. 최대 norm 1을 사용하여 기울기 클리핑을 수행하고 학습 속도와 가중치 감소를 분리하는 구현을 통해 AdamW 최적화기를 사용한다. MM1-30B의 경우 [120]과 유사하게 훈련 안정성을 향상시키는 것을 관찰했기 때문에 척도 1e-4를 갖는 z-손실 항도 추가한다.\n' +
      '\n' +
      '연구된 주요 LLM 크기 각각에 대한 예측된 최적(피크) 학습 속도\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline N & Pred. \\(\\eta\\) & Pred. \\(\\lambda\\) \\\\ \\hline\n' +
      '1.2B & 8.6e-5 & 5.0e-6 \\\\\n' +
      '2.9B & 5.9e-5 & 3.5e-6 \\\\\n' +
      '6.4B & 4.2e-5 & 2.5e-6 \\\\\n' +
      '30B & 2.2e-5 & 1.3e-6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: MM1 모델 크기에 대한 최적 피크 학습률 \\(\\eta\\) 및 중량 감소 \\(\\lambda\\) 예측\n' +
      '\n' +
      '그림 8: Sec. 0.B.1에 설명된 그리드 검색을 위한 모델 크기의 함수로서 최적 가중치 감쇠. x축은 (비매립) LLM 매개변수의 수이고 y축은 가중치 감쇠이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Shot**} & \\multicolumn{4}{c}{**Captioning**} & \\multicolumn{4}{c}{**Visual Question Answering**} \\\\ \\cline{3-10}  & & \\multicolumn{2}{c}{COCO NoCaps TextCaps} & \\multicolumn{2}{c}{VQAv2 TextVQA} & \\multicolumn{2}{c}{VizWiz OKVQA} \\\\ \\hline \\multicolumn{10}{l}{_MM1-3B Model Comparisons_} \\\\ \\hline \\multirow{4}{*}{Flamingo-3B [3]} & 0\\({}^{\\dagger}\\) & 73.0 & – & – & 49.2 & 30.1 & 28.9 & 41.2 \\\\  & 4 & 85.0 & – & – & 53.2 & 32.7 & 34.0 & 43.3 \\\\  & 8 & 90.6 & – & – & 55.4 & 32.4 & 38.4 & 44.6 \\\\  & 16 & 95.3 & – & – & 56.7 & 31.8 & 43.3 & 45.6 \\\\  & 0 & 73.5 & 55.6 & 63.3 & 46.2 & 29.4 & 15.6 & 26.1 \\\\ MM1-3B & 4 & 112.3 & 99.7 & 84.1 & 57.9 & 45.3 & 38.0 & 48.6 \\\\  & 8 & 114.6 & 104.7 & 88.8 & 63.6 & 44.6 & 46.4 & 48.4 \\\\  & 16 & 116.8 & 107.6 & 91.6 & 60.9 & 46.1 & 53.8 & 50.5 \\\\ \\hline \\multicolumn{10}{l}{_MM1-7B Model Comparisons_} \\\\ \\hline \\multirow{4}{*}{IDEFICS-9B [58]} & 0\\({}^{\\dagger}\\) & 46.0* & 36.8 & 25.4 & 50.9 & 25.9 & 35.5 & 38.4 \\\\  & 4 & 93.0* & 81.3 & 60.0 & 55.4 & 27.6 & 36.9 & 45.4 \\\\  & 8 & 97.0* & 86.8 & 63.2 & 56.4 & 27.5 & 40.4 & 47.7 \\\\  & 16 & 99.7* & 89.4 & 67.4 & 57.0 & 27.9 & 42.6 & 48.4 \\\\  & 0\\({}^{\\dagger}\\) & 79.4 & – & – & 51.8 & 31.8 & 28.8 & 44.7 \\\\ Flamingo-9B [3] & 4 & 93.1 & – & – & 56.3 & 33.6 & 34.9 & 49.3 \\\\  & 8 & 99.0 & – & – & 58.0 & 33.6 & 39.4 & 50.0 \\\\  & 16 & 102.2 & – & – & 59.4 & 33.5 & 43.0 & 50.8 \\\\  & 0\\({}^{\\dagger}\\) & – & – & – & 52.9 & – & 34.4 & 42.8 \\\\ Emu2-14B [106] & 4 & – & – & – & 58.4 & – & 41.3 & – \\\\  & 8 & – & – & – & 59.0 & – & 43.9 & – \\\\  & 0 & 76.3 & 61.0 & 64.2 & 47.8 & 28.8 & 15.6 & 22.6 \\\\ MM1-7B & 4 & 109.8 & 96.2 & 84.5 & 60.6 & 44.4 & 37.4 & 46.6 \\\\  & 8 & 116.3 & 106.6 & 88.2 & 63.6 & 46.3 & 45.3 & 51.4 \\\\  & 16 & 118.6 & 111.1 & 93.1 & 65.2 & 46.9 & 53.2 & 52.9 \\\\ \\hline \\multicolumn{10}{l}{_MM1-30B Model Comparisons_} \\\\ \\hline \\multirow{4}{*}{IDEFICS-80B [58]} & 0\\({}^{\\dagger}\\) & 91.8* & 65.0 & 56.8 & 60.0 & 30.9 & 36.0 & 45.2 \\\\  & 4 & 110.3* & 99.6 & 72.7 & 63.6 & 34.4 & 40.4 & 52.4 \\\\  & 8 & 114.3* & 105.7 & 77.6 & 64.8 & 35.7 & 46.1 & 55.1 \\\\  & 16 & 116.6* & 107.0 & 81.4 & 65.4 & 36.3 & 48.3 & 56.8 \\\\  & 0\\({}^{\\dagger}\\) & 84.3 & – & – & 56.3 & 35.0 & 31.6 & 50.6 \\\\ Flamingo-80B [3] & 4 & 103.2 & – & – & 63.1 & 36.5 & 39.6 & 57.4 \\\\  & 8 & 108.8 & – & – & 65.6 & 37.3 & 44.8 & 57.5 \\\\  & 16 & 110.5 & – & – & 66.8 & 37.6 & 48.4 & 57.8 \\\\  & 0 & – & – & – & 33.3 & 26.2 & 40.4 & 26.7 \\\\  & 4 & – & – & – & 67.0 & 48.2 & 54.6 & 53.2 \\\\ Emu2-37B [106] & 8 & – & – & – & 67.8 & 49.3 & 54.7 & 54.1 \\\\  & 16 & 110.5 & – & – & 68.8 & 50.3 & 57.0 & 57.1 \\\\ MM1-30B & 8 & 108.8 & – & – & 65.6 & 37.3 & 44.8 & 57.5 \\\\  & 16 & 110.5 & – & – & 66.8 & 37.6 & 48.4 & 57.8 \\\\  & 0 & – & – & – & 33.3 & 26.2 & 40.4 & 26.7 \\\\  & 4 & – & – & – & 67.0 & 48.2 & 54.6 & 53.2 \\\\ Emu2-37B [106] & 8 & – & – & – & 67.8 & 49.3 & 54.7 & 54.1 \\\\  & 16 & 110.5 & – & – & 68.8 & 50.3 & 57.0 & 57.1 \\\\  & 8 & 108.9 & – & – & 65.6 & 37.3 & 44.8 & 57.5 \\\\  & 16 & 110.5 & – & – & 66.8 & 37.6 & 48.4 & 57.8 \\\\  & 0 & – & – & – & 33.3 & 26.2 & 40.4 & 26.7 \\\\  & 4 & – & – & – & 67.0 & 48.2 & 54.6 & 53.2 \\\\ Emu2-37B [106] & 8 & – & – & – & 67.8 & 49.3 & 54.7 & 54.1 \\\\  & 16 & 110.5 & – & – & 68.8 & 50.3 & 57.0 & 57.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 완전 MM1 사전 훈련 소샷 평가 결과. (*) IDEFICS는 그 트레이닝 데이터에 PMD를 포함한다(COCO 포함). (\\({\\dagger}\\)) 이러한 모델에는 "0" 프롬프트에 두 개의 텍스트 전용 시범이 포함된 반면 MM1은 그렇지 않다.\n' +
      '\n' +
      '이 작업에서 표 7에 나와 있다. 단순화를 위해 실제 MM1 3B, 7B 및 30B 모델에 대해 각각 6e-5, 4e-5 및 2e-5와 동일한 \\(\\eta\\)을 사용했다. 마지막으로, 모든 모델 크기에 대해 MM1의 무작위로 초기화된 비전 언어 커넥터의 피크 LR을 \\(\\eta=\\)8e-5로 고정한다. 향후 MM1 버전의 경우 비용이 많이 드는 하이퍼파라미터 검색을 수행할 필요가 없도록 [122]와 유사한 기술을 통합할 계획입니다.\n' +
      '\n' +
      '*학습률 및 가중치 감소 그리드 검색.** 그림 6의 최종 곡선 적합에 해당하는 개별 그리드 검색 결과는 그림 9와 같다. [120]이 결론을 변경하지 않는다는 것을 발견함에 따라 \\(5e^{4}\\) 단계에 대한 그리드 검색 모델을 훈련한다. 그림 8과 같이 가중치 감쇠값을 예측하기 위해 최적의 학습률을 예측하는 데 사용된 것과 동일한 절차를 적용할 수 있다. 파란색 원은 8-샷 평균 성능에 비례하는 샘플링 확률(및 색상의 어두움)을 갖는 그리드 검색의 실제 데이터 포인트에 해당한다. 이 작업의 주요 모델 크기 각각에 대한 해당 예측은 표 7에 나와 있다.\n' +
      '\n' +
      '그림 9: 다양한 LLM 크기에 대한 최대 학습 속도(y축) 및 가중치 감소(x축)에 대한 그리드 검색에 대한 8-샷 평균. 검은색 세포는 우리가 해당 실험을 실행하지 않은 설정에 해당한다.\n' +
      '\n' +
      '###SFT(Supervised Fine-tuning)\n' +
      '\n' +
      '학습률 1e-5와 코사인 감쇠를 0으로 하는 AdaFactor optimizer를 사용하여 학습률을 달리 실험하였으며, 경험적으로 1e-5의 값이 최적임을 확인하였다. SFT 동안, 우리는 이미지 인코더와 LLM _unfrozen_를 모두 유지하고, 경험적으로, 전체 모델을 미세 조정하는 것이 더 나은 성능을 달성한다는 것을 관찰한다.\n' +
      '\n' +
      '## 부록 0.C 평가 상세\n' +
      '\n' +
      '### Pre-training Evaluation\n' +
      '\n' +
      '사용 가능한 경우 훈련 세트에서 데이터 세트당 무작위로 샘플링되는 샷 프롬프트는 거의 없으며, 그렇지 않으면 유효성 검사 세트(쿼리 예제를 확인해도 샷에는 나타나지 않음)가 표시됩니다. 모델이 작업당 기준으로 지정될 수 있는 EOS 토큰 또는 임의의 추가 정지 토큰을 방출할 때까지 그리디 디코딩으로 출력이 생성된다. 캡셔닝 작업에 대한 추가 정지 토큰은 단지 뉴라인 문자이고, VQA 작업에 대해 우리는 또한 유효한 정지 토큰으로서 ".", "질문"을 포함한다. 후처리 VQA 예측을 위해 OpenFlamingo9[4]와 동일한 논리를 사용한다. 캡셔닝 작업의 경우 nlg-eval 패키지[102]를 사용하여 CIDEr 점수[112]를 보고한다. 우리의 모든 멀티모달 사전 훈련 평가는 EleutherAI의 lm-평가-harness[35]의 내부 포크로 구현된다.\n' +
      '\n' +
      '각주 9: 구체적으로, VQAMetric(commit 60a5fd6)의 구현.\n' +
      '\n' +
      '### SFT 평가 벤치마크\n' +
      '\n' +
      '우리는 전통적인 학술 VL 벤치마크와 MLLM을 위해 특별히 설계된 최근 벤치마크 모음에 대해 SFT 모델을 평가한다. 학술적 VL 벤치마크의 경우 VQAv2[38], TextVQA[105] 및 ScienceQA[81]의 이미지 하위 집합을 포함한다. 최근 MLLM 벤치마크들의 경우, POPE[68], MME[33], MMBench[78], SEED-Bench[62], LLAVA-Bench-in-the-Wild[76], MM-Vet[126], MathVista[80], 및 최근 인기 있는 MMMU[127]을 포함한다.\n' +
      '\n' +
      '### SFT 평가 메타-평균\n' +
      '\n' +
      'SFT 절제 과정에서 비교를 단순화하기 위해 모든 벤치마크 결과를 단일 메타 평균 수로 합성한다. 서로 다른 데이터 세트의 평가 메트릭은 서로 다른 범위를 가질 수 있기 때문에 기준선 구성과 관련하여 정규화한다. 이것은 처음에 각 작업에 대한 결과를 표준화함으로써 달성되며, 즉 모든 메트릭을 각각의 베이스라인으로 나눈 다음 모든 메트릭에 걸쳐 평균화하여 조정한다. 를 포함하고,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c} \\hline \\hline Dataset & Evaluation Split \\\\ \\hline COCO & Karpathy test \\\\ NoCaps & val \\\\ TextCaps & val \\\\ VQAv2 & testdev \\\\ TextVQA & val \\\\ VizWiz & testdev \\\\ OKVQA & val \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 사전 훈련 평가에 사용되는 분할. 주요 사전 훈련 결과와 달리 모든 사전 훈련 절제는 VQAv2 및 VizWiz에 대한 검증 분할을 사용한다는 점에 유의해야 한다.\n' +
      '\n' +
      '구체적으로, 우리는 \\(224\\!\\times\\!224\\)으로 훈련된 소형 MM1 모델의 성능 메트릭을 사용하여 기준선을 설정한다. 이미지 해상도 및 64개의 이미지 쿼리로 주의 풀링을 사용합니다.\n' +
      '\n' +
      '### 추가 SFT 어블레이션\n' +
      '\n' +
      '이 섹션에서는 SFT 삭제를 수행합니다. 이 섹션은 섹션 3과 유사하며 여기에서 동일한 체크포인트에서 SFT를 수행하고 사전 훈련 평가 대신 유사한 수업이 SFT 평가에 적용되는지 평가한다. 또한, SFT 동안 영상 인코더를 동결 유지할지 여부를 연구한다. 이러한 모든 삭제에 대해 MM1-3B-채팅을 훈련한다.\n' +
      '\n' +
      '**사전 훈련 데이터 혼합물 제거.** 그림 9(a)에서 사전 훈련 데이터에 대해 서로 다른 가중치로 SFT 성능을 비교한다. 우리는 0-샷 평가에 대해 그림 5와 비교할 때 유사한 경향을 본다. 캡션 전용 데이터로 사전 훈련하면 SFT 평가 메트릭에 걸쳐 최상의 성능을 제공합니다. 이것은 **데이터 레슨 1**을 확증한다: 캡션 데이터는 여전히 SFT 평가에 대한 제로-샷 성능을 상승시킨다. 그러나 SFT 메트릭은 소수의 샷 성능을 측정하지 않으므로 이 테이블에서는 인터리빙된 데이터의 영향이 눈에 띄지 않는다.\n' +
      '\n' +
      '**시각 언어 커넥터 어블레이션.** 도 9(b)에서, 우리는 상이한 시각 언어 커넥터 구성을 평가한다. 이 그림은 그림 4와 유사하지만 해당 SFT 모델을 평가한다. 알 수 있듯이 낮은 수의 이미지 토큰을 사용하면 평균 풀링이 C-Abstractor와 유사한 결과를 제공한다. 이미지 토큰의 수를 늘리면 C-Abstractor 구성이 가장 좋은 결과를 제공한다. 이러한 경향은 그림 4에 보고된 사전 훈련 결과와 완전히 일치하지 않는다. 선택의 영향을 전반적으로\n' +
      '\n' +
      '도 10: **SFT ablations.**(a) 사전 훈련 데이터 혼합물이 SFT 결과에 미치는 영향. 여기서, \\(x/y/z\\)는 데이터의 \\(x\\%\\)이 인터리빙되고, \\(y\\%\\)은 캡션이고, \\(z\\%\\)은 순수 텍스트임을 의미한다. tks: 이미지 토큰의 수. (b) 상이한 비전-언어 커넥터가 SFT 결과에 미치는 영향. (a)와 (b) 모두에 대해 먼저 절제된 설정으로 MM1-3B를 사전 훈련한 다음 사전 훈련된 모델에 대해 SFT를 수행한다. (c) SFT 동안 이미지 인코더를 동결 또는 동결해제하는 단계를 포함한다.\n' +
      '\n' +
      '시각 언어 커넥터의 최종 테스트 성능에 그다지 큰 영향을 미치지 않는 것으로 판단된다. 최종 모델은 C-Abstractor 아키텍처를 사용합니다.\n' +
      '\n' +
      '**이미지 인코더 어블레이션.** 그림 9(c)에서 SFT 동안 이미지 인코더를 냉동 상태로 유지할지 여부를 연구한다. 그 결과, 낮은 영상 해상도에서 동결 영상 인코더가 동결되지 않은 영상 인코더(+2.2 포인트)보다 더 나은 성능을 보였다. 그러나, 더 높은 해상도들(_i.e._, 1344px)에서, 이미지 인코더(+2.9 포인트들)를 부동시키는 것이 유익하다. 이는 사전 훈련이 보간 또는 이미지 하위 분할 없이 기본 해상도로 수행되기 때문일 수 있다.\n' +
      '\n' +
      '퓨샷 MM1-30B-채팅을 위한### 구현 세부사항\n' +
      '\n' +
      '섹션 5.1에서 볼 수 있듯이, 우리의 미세화된 모델은 더 강력한 성능을 달성하기 위해 컨텍스트 내 예를 활용할 수 있다. 흥미롭게도, 예제 수를 늘리면 성능이 올라갑니다. MM1-30B-Chat으로 이를 시연한다.\n' +
      '\n' +
      '서브-이미지 분해의 사용으로 인해 소수의 샷 입력에 대한 하나의 도전이 발생한다. 이 전략은 제로샷 성능을 높이는 반면 이미지당 소비되는 유효 토큰 수를 크게 증가시킨다. MM1-30B-Chat처럼 입력 이미지당 5개의 서브 이미지를 사용하여, 모든 예가 단지 하나의 소스 이미지를 포함하는 4-샷 예를 처리하면 이미 20개의 유효 이미지가 생성된다. 따라서 144개의 토큰으로 모든 이미지를 표현하려면 이미지만 2,880개의 토큰이 필요하므로 제한된 언어 모델 컨텍스트를 빠르게 소진한다. 이러한 한계를 완화하기 위해, 우리는 새로운 _mixed-resolution_ 접근법을 제안한다. 구체적으로, \\(K\\) 문맥 내 예제의 경우, 마지막 \\(N\\) 영상만을 서브 이미지 분해를 통해 높은 해상도로 인코딩하고, 나머지 \\(K-N\\) 문맥 내 예제는 낮은 해상도로 프로세싱한다. 이것은 인-컨텍스트 예제를 인코딩하는데 훨씬 덜 비싸게 하고, 고정된 토큰 버짓 내에서 인-컨텍스트 예제의 수를 증가시킬 수 있게 하여, 성능을 더욱 증가시킨다. 구현에서는 \\(N=3\\)을 설정하였다.\n' +
      '\n' +
      '우리는 Math-Vista 벤치마크를 사용하여 제안된 전략의 효과를 입증한다. Context-of-thought를 사용한 In-Context 학습을 사용하면 MM1-30B-Chat의 성능이 39.4(0-shot)에서 41.9(4-shot, 전체 서브 이미지 분해를 사용한 모든 In-context 예제)로 향상된다. 본 논문에서 제안한 혼합 분해 방법을 적용하면 추가적인 문맥 내 예제를 인코딩할 수 있어 최대 8-shot의 사고 체인을 가능하게 하여 성능을 44.4까지 향상시킬 수 있다. 이는 MM1-Chat 모델이 사전 훈련 요법에서 물려받은 문맥 내 학습 능력을 유지하고, 혼합 분해 전략이 제한된 문맥 길이 내에서 소수의 샷 성능을 더욱 증가시키는 데 효과적임을 보여준다.\n' +
      '\n' +
      '## 부록 0.D 정성적 예\n' +
      '\n' +
      '이 절에서는 MM1 예측의 정성적 예를 공유한다.\n' +
      '\n' +
      '그림 11: MM1 카운팅, OCR 및 과학적 지식 능력을 테스트하는 예제. 이미지와 프롬프트는 COCO 2014 검증 세트 [72] 및 [123]에서 가져온 것입니다.\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**사용자:** 이미지에서 회색 고양이 근처의 빨간색 공이 날개에 밝은 파란색 깃털이 없는 것을 찾을 수 있습니까? 새는 매나 독수리와 같은 큰 먹이의 새로서 전형적인 먹이 깃털을 가지고 있다.\n' +
      '\n' +
      '**User:** 눈금 1부터 10까지, 이미지가 텍스트 프롬프트 "자동차를 운전하는 앵무새"와 얼마나 유사한지를 결정한다. 네 추리를 설명해 봐.\n' +
      '\n' +
      '**User:** 눈금 1부터 10까지, 이미지가 텍스트 프롬프트 "자동차를 운전하는 앵무새"와 얼마나 유사한지를 결정한다. 네 추리를 설명해 봐.\n' +
      '\n' +
      '**User:** 눈금 1부터 10까지, 이미지가 텍스트 프롬프트 "자동차를 운전하는 앵무새"와 얼마나 유사한지를 결정한다. 네 추리를 설명해 봐.\n' +
      '\n' +
      '**User:** 눈금 1부터 10까지, 이미지가 텍스트 프롬프트 "자동차를 운전하는 앵무새"와 얼마나 유사한지를 결정한다. 이미지에는 파란색 차 안에 두 마리의 앵무새가 있습니다. 그 차는 야자수와 노란 하늘을 배경으로 한 녹색 들판에 주차되어 있다.\n' +
      '\n' +
      '**사용자:** 앵무새 중 한 마리가 핸들에 앉아 차를 운전하고 있다는 인상을 줍니다. 다른 앵무새는 차 지붕에 편안하게 앉아 경치를 즐기는 것 같다. 자동차의 파란색이 녹색 들판과 노란색 하늘과 아름답게 대비되는 등 이미지 속 색감이 생동감 넘친다. 야자수는 그 장면에 열대 느낌을 더한다.\n' +
      '\n' +
      '**User:** 이미지는 앵무새의 수 및 그들의 동작을 제외하고 텍스트 프롬프트를 가깝게 표현한 것이다. 프롬프트는 자동차를 운전하는 앵무새 한 마리만 언급하는 반면, 이미지는 두 마리의 앵무새를 보여주는데, 하나는 겉보기에 운전하고 다른 하나는 자동차 지붕에 있다. 이는 이미지에 놀라움과 유머의 요소를 더합니다. 전체적으로 이미지는 텍스트 프롬프트의 긴밀한 표현과 창의적인 해석에 대해 10점 만점에 9점을 받는다.\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '새의 날개에 밝은 파란색 깃털이 보이나요?\n' +
      '\n' +
      '**사용자:** 이미지의 밝은 파란색 깃털이 날개에 밝은 파란색 깃털이 없는 것을 볼 수 있습니까? 새는 매나 독수리와 같은 큰 먹이의 새로서 전형적인 먹이 깃털을 가지고 있다.\n' +
      '\n' +
      '**유저:** 이미지에서 회색 고양이 근처의 빨간 공을 찾을 수 있나요?\n' +
      '\n' +
      '**MN1-30B-Chat (우리들):** 이 이미지를 목격하면 사람들은 경외심, 매혹, 평온과 같은 감정의 혼합을 느낄 수 있다. 비행기의 날개와 그 밑의 광활하고 눈 덮인 산맥을 보여주는 비행기 창문의 숨막히는 풍경은 평온함과 자연의 아름다움을 선사한다. 맑고 푸른 하늘과 대비되는 웅장하고 질서정연한 산과 그들의 눈 봉우리 문양은 자연계에 대한 경이로움과 감상을 진화시킨다. 이 견해는 또한 매일 경험하지 못할 수도 있는 세계의 독특한 관점을 제공하여 호기심과 흥분을 불러일으킨다. 또한, 이미지는 묘사된 풍경의 손대지 않고 깨끗한 자연을 고려할 때 평온함과 평화의 느낌을 가져올 수 있다.\n' +
      '\n' +
      '**User:**<img1><img2> 두 이미지 각각에 대하여 물의 짠맛 여부를 논한다.\n' +
      '\n' +
      '**MN1-30B-Chat(우리들):** 첫 번째 이미지에는 수역 위에 여러 개의 보트가 떠 있습니다. 보트의 존재에 따르면, 이것은 항구일 가능성이 높으며 대부분의 항구는 일반적으로 바다 근처에서 발견되기 때문에 물은 짠 것 같다. 하지만 더 많은 정보가 없으면 물의 염도를 확인할 수 없습니다.\n' +
      '\n' +
      '두 번째 이미지에는 마른 잔디밭의 물웅덩이 주변에 코끼리, 기린 등 다양한 아프리카 동물들이 모여 있다. 이 수역은 동물들이 마실 신선한 물이 필요하기 때문에 담수일 가능성이 가장 높다. 물웅덩이에서 마시는 수많은 동물의 존재는 이 물이 생존에 필수적이며 염수를 마실 수 없을 가능성이 매우 낮다는 것을 확인시켜준다.\n' +
      '\n' +
      '**Fig. 13:** 이미지 미학을 지각하고 다중 이미지를 비교하는 MM1 능력을 테스트하는 예제 이미지와 프롬프트는 COCO 2014 검증 세트 [72] 및 [123]에서 가져온 것입니다.\n' +
      '\n' +
      '**사용자:** 먼저 커피머신 버튼에 대한 작동메뉴를 아래의 내용을 읽어보십시오.\n' +
      '\n' +
      'Menu 1:\n' +
      '\n' +
      'Menu 2:\n' +
      '\n' +
      'Menu 3:\n' +
      '\n' +
      '이제 아래 이미지에서 커피 머신의 작동 패널을 살펴보겠습니다. 6 OZ 커피는 왼쪽, 오른쪽, 위쪽 또는 아래쪽 버튼 중 어느 버튼을 사용해야 하나요?\n' +
      '\n' +
      '**사용자:** 이미지를 설명\n' +
      '\n' +
      '**MM1-30B-Chat(우리들):** 이미지는 자동차의 대시보드 카메라의 관점에서 순간을 포착한다. 그 차는 노란 선으로 나뉘어진 2차선 도로를 주행하고 있는데, 이는 시골이나 인구가 적은 지역을 암시한다. 길 양옆에는 나무들이 우뚝 솟아 있고, 그 위에 회색, 흐린 하늘과 대비되는 푸른 잎들이 있다. 도로 우측에는 차량 허용 최대 속도를 나타내는 \'35\'가 표시된 제한 속도 표지판이 보인다. 전체적인 장면은 담담하고 약간 숲이 우거진 지역을 지나는 장면 드라이브를 암시한다.\n' +
      '\n' +
      '그림 14: [123]에 이어, 우리는 MM1을 작동 기계 및 네비게이팅과 같은 작업 지향 시나리오에서 테스트했습니다. 이미지와 프롬프트는 [123]에서 가져온 것입니다.\n' +
      '\n' +
      '**MM1-30B-Chat(우리들):** 증발과 증발산은 종종 혼동되는 물 순환에서 두 가지 중요한 과정이다. 이미지는 둘 사이의 명확한 구별을 제공한다. 증발은 물이 액체 상태에서 기체 상태로 변하는 과정이다. 이것은 태양의 열로 인해 호수와 바다의 물이 수증기로 변하여 대기 중으로 상승함으로써 이미지에 나타난다.\n' +
      '\n' +
      '반면 증발산은 증발과 식물 증산의 결합이다. 증산은 식물에 흡수된 물이 식물의 잎을 통해 대기 중으로 방출되는 과정이다. 영상에서 증발산량은 나무와 풀에 흡수된 물이 대기 중으로 다시 방출되는 것으로 표현된다.\n' +
      '\n' +
      '요약하면, 증발은 물이 증기로 변하여 대기 중으로 상승하는 과정이며, 증발산량은 증발과 식물의 증산이 결합되어 식물에 흡수되어 대기 중으로 다시 방출되는 물을 나타낸다.\n' +
      '\n' +
      '요약하면, 증발은 물이 증기로 변하여 대기 중으로 상승하는 과정이며, 증발산량은 증발과 식물의 증산이 결합되어 식물에 흡수되어 대기 중으로 다시 방출되는 물을 나타낸다.\n' +
      '\n' +
      '흐름도는 프로세스의 명확하고 구조화된 표현을 제공하여 프로젝트 제안 및 의사 결정 프로세스와 관련된 단계들의 시퀀스를 쉽게 이해할 수 있게 한다. 복잡한 프로세스를 시각화하고 모든 단계를 고려하고 올바른 순서로 따르도록 하는 데 유용한 도구입니다.\n' +
      '\n' +
      '그림 15: 그래픽에서 정보를 추출할 때 MM1 능력을 테스트하는 예제. 오른쪽 부분은 빨간색으로 강조 표시된 혼동의 예를 보여준다. 이미지와 프롬프트는 [123]에서 가져온 것입니다.\n' +
      '\n' +
      '**사용자:** 몇 장의 사진을 보여드리겠습니다. 그리고 당신의 임무는 제 질문에 답하는 것입니다.\n' +
      '\n' +
      '**MM1-30B-Chat (Ours):**\n' +
      '\n' +
      '* 해변에 우산이 있는 이미지: 섭씨 25-30도. 우산은 올라갔고, 사람들은 해변에 있어서 따뜻하고 화창한 날을 암시한다.\n' +
      '* 눈 위를 비행하는 비행기의 이미지: -20도 내지 -30도. 비행기는 눈 덮인 지역 위를 날고 있으며, 지상에는 차량이 있어 추운 겨울날을 알 수 있다.\n' +
      '* 들판에서 풀을 뜯고 있는 동물들의 이미지: 섭씨 15-20도. 동물들은 보통 더 온화한 날씨 조건에서 일어나는 풀을 뜯고 있다. 잔디는 녹색이어서 온화한 기후를 시사한다.\n' +
      '\n' +
      '그림 16: 이미지와 텍스트에 걸쳐 추론에서 MM1 능력을 테스트하는 예제. 이미지는 COCO 2014 검증 세트[72] 및 MMMU dev 세트[127]에서 가져온 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:39]\n' +
      '\n' +
      '작가 기부와 승인\n' +
      '\n' +
      '**First Authors**\n' +
      '\n' +
      '브랜든 맥킨지: 멀티모달 사전 훈련 설계, 구현 및 실험을 이끌었습니다. 멀티모달 평가 인프라의 공동 주도 설계 및 구현 전체 방향을 공동 설정합니다. 종이를 같이 썼어요\n' +
      '\n' +
      'Zhe Gan: 공동 시작된 노력은 SFT 설계, 구현 및 실험을 주도했다. 전체 방향을 공동 설정합니다. 종이를 같이 썼어요\n' +
      '\n' +
      '**Core Authors**\n' +
      '\n' +
      'Jean-Philippe Fauconnier: 공동 주도의 멀티모달 평가 인프라 설계 및 구현, 모델 평가, 모델 구현, 멀티모달 사전 훈련 및 SFT 실험을 지원했다.\n' +
      '\n' +
      '샘 닷지: SFT 실험, 데이터 혼합물 및 멀티모달 평가 인프라를 지원합니다.\n' +
      '\n' +
      '보웬 장: 공동 착수 노력, 훈련된 이미지 인코더, 인프라 지원.\n' +
      '\n' +
      'Philipp Dufter: 모델 구현, 평가 및 실험을 지원합니다.\n' +
      '\n' +
      'Dhruti Shah: 구현된 인터리브 SFT, 실험 보조.\n' +
      '\n' +
      'Xianzhi Du: 멀티모달 사전 훈련, SFT 및 기본 LLM에 대해 MoE를 구현 및 훈련시켰다.\n' +
      '\n' +
      '피터 그래쉬: 실험, 공동 주도 설계 및 멀티모달 평가 인프라의 구현, 공동 작성 논문을 지도 및 분석했다.\n' +
      '\n' +
      '**Further Authors**\n' +
      '\n' +
      '푸탕펭: 데이터 처리 및 조정.\n' +
      '\n' +
      '플로리스 위어: 텍스트 기반 평가 인프라스트럭처 및 멀티모달 평가 인프라스트럭처를 지원합니다.\n' +
      '\n' +
      'Haotian Zhang: MoE 모델을 구현하고 실험했습니다.\n' +
      '\n' +
      '안톤 벨리, 카란지트 싱, 더그 강: 데이터셋 생성 및 필터링.\n' +
      '\n' +
      'Hongyu He: 공동 구현된 V-L 커넥터는 실험을 보조했다.\n' +
      '\n' +
      'Max Schwarzer: 패킹된 이미지-텍스트 쌍들 및 패킹된 인터리빙된 문서들에 대한 사전 트레이닝을 위한 구현된 지원.\n' +
      '\n' +
      'Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee: 기본 LLM을 설계, 구현 및 훈련했습니다.\n' +
      '\n' +
      '루밍 팡, 지루이 왕: 공동 착수 노력, 설계, 구현, 그리고 기초 LLM들을 훈련시켰다\n' +
      '\n' +
      '**Senior Authors**\n' +
      '\n' +
      '알렉산더 토셰프: 전체 방향을 공동 설정하고, 실험을 조언하고 분석하고, 논문을 공동 작성했다.\n' +
      '\n' +
      '인페이 양: 공동 착수 노력, 공동 전체 방향 설정, 실험 조언 및 분석, 공동 논문 작성.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '저자들은 바이샤알 샹카르, 알라 엘누비, 양자오, 샹페이 자이, 러스 웹, 하디 푸란사리, 양하오 리에게 귀중한 안내와 제안, 피드백에 감사하고 싶다. 저자들은 지도 조정에 대한 그의 도움에 대해 첸 첸에게 감사한다. 저자들은 마이트레이 쿤나박캄 빈지무르, 메건 마허 웰시, 바비카 데브나니, 데이비드 코스키가 입력 파이프라인 및 데이터 처리에 도움을 준 것에 대해 감사한다. 저자들은 에스테반 곤잘레스, 이안 클라크, 잭 베일린, 데이비드 코스키, 특히 벤카타 예르네니에게 추적 실험 및 모델 평가를 위한 내부 가중치 및 편향 사례에 대한 지원에 감사한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>