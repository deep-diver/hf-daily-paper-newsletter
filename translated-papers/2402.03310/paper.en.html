<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _V-Irl_: Grounding Virtual Intelligence in Real Life\n' +
      '\n' +
      'Jihan Yang\\({}^{1}\\) Runyu Ding\\({}^{1}\\) Ellis Brown\\({}^{2}\\) Xiaojuan Qi\\({}^{1}\\) Saining Xie\\({}^{2}\\)\n' +
      '\n' +
      '\\({}^{1}\\)The University of Hong Kong \\({}^{2}\\)New York University\n' +
      '\n' +
      'Work conducted during a visit to NYU.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'The advent of large language models (LLMs) has breathed new life into autonomous agent research by offering a universal interface for diverse capabilities, ranging from basic reasoning to complex planning and tool use [72]. While these developments are promising, most of these agents remain confined to text-based environments or simplistic simulations. Visual components in existing agents are either rudimentary--such as simulated tabletop environments [11, 28]--or rely on abstracted representations using ground-truth APIs [27, 67]. Furthermore, the prevalent visual models employed by these agents are trained on photogenic, object-centric Internet images, which fail to capture the unpredictability and diversity of real-world scenes.\n' +
      '\n' +
      'This paper aims to bridge this gap between AI agents and the sensory world by grounding them in rich, real-world environments--a crucial step towards developing autonomous agents that can effectively operate in real-life sce\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'ject thanks to our interactive embodied environment and the sensor-rich visual input. _During the pilot in Hong Kong, RX-399 locates eight trash bins, correctly identifying five but overlooking one. In New York, it accurately detects all five trash bins but mistakenly reports two mailboxes._\n' +
      '\n' +
      'RX-399 can avoid double-counting previously seen objects by using feature matching to check for duplicates among prior detections (see Fig. 5).\n' +
      '\n' +
      '_Imani needs to analyze the distribution of trash bins, fire hydrants, and park benches in New York\'s Central Park for a project with the NYC Parks & Recreation department._\n' +
      '\n' +
      'Imani sets routes spanning Central Park and objects of interest for RX-399, who traverses the routes and records all detected instances. After RX-399 finishes its route, Imani analyzes the collected data at different levels of detail. As depicted in Fig. 3, the coarsest level shows general distributions of trash bins, hydrants, and benches in the park. Imani can also zoom in to specific regions, where lighter colors represent positions with more unique instances identified. The following table presents RX-399\'s counting report:\n' +
      '\n' +
      'By retrieving geotagged sensory-rich data within RX-399, Imani can also inspect the detection results for each object to help her verify the reliability of RX-399\'s reports as illustrated by the bottom level in Fig. 3.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} Category & Trash Bin & Fire Hydrant & Park Bench\\({}^{*}\\) \\\\ \\hline Count & 1059 & 727 & 1015 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: RX-399’s counting report in Central Park, New York City. _(\\({}^{*}\\)Note: contiguous benches counted as one instance)._\n' +
      '\n' +
      'Figure 4: Portions of RX-399’s system records in HK and NYC.\n' +
      '\n' +
      'Figure 5: RX-399 avoids double-counting trash cans by identifying duplicates across different viewpoints using feature matching.\n' +
      '\n' +
      'Figure 3: Imani’s visualization of trash bins, fire hydrants, & park benches in NYC’s Central Park using data collected by RX-399.\n' +
      '\n' +
      '_Hiro is starting a new journey in Hong Kong. He decides to explore without a specific destination in mind, looking for a good local lunch spot with food that\'s not too spicy..._\n' +
      '\n' +
      'As depicted in Fig. 6, starting at \\(\\frac{\\text{\\text{\\text@underline{\\char 10}}}}{\\text{\\text@underline{\\text{ \\char 10}}}}\\), Hiro walks down the street and encounters the first intersection. Thanks to the interactive and sensory-rich environment, he can adjust his pose to fetch street views for each possible path. Using VQA on these views, he decides to turn left:\n' +
      '\n' +
      '_Residential buildings on the left road indicate cozy and family-run local food...A better choice than the others!_\n' +
      '\n' +
      'Then, after exploring for a block, he encounters the second intersection where he looks around and decides to turn right:\n' +
      '\n' +
      '_Looks like there are some local food spots this way..._\n' +
      '\n' +
      'After a few steps, Hiro finds "_A One Chinese Noodles [\\(\\frac{\\text{\\text{\\text@underline{\\char 10}}}}{\\text{\\text@underline{\\text{ \\char 10}}}}\\)]--[\\(\\frac{\\text{\\text{\\text@underline{\\char 10}}}}{\\text{\\text@underline{\\text{ \\char 10}}}}\\)]_" using his open-world detector. He retrieves information, ratings, and reviews for the restaurant using our platform, which _connects street views to places_. Hiro ultimately decides to pass on it and keep exploring because:\n' +
      '\n' +
      '_Most reviews mention the **spicy** pork chop noodles..._\n' +
      '\n' +
      'Finally, at the end of the block \\(\\frac{\\text{\\text{\\text@underline{\\char 10}}}}{\\text{\\text@underline{\\text{ \\char 10}}}}\\), Hiro discovers another lunch spot called "_Xintianfa [\\(\\frac{\\text{\\text{\\text@underline{\\char 10}}}}{\\text{\\text@underline{\\text{ \\char 10}}}}\\)]_". He decides to dine there after reading numerous online reviews praising its authentic cuisine and diverse menu.\n' +
      '\n' +
      '### Collaborative Agents\n' +
      '\n' +
      'Humans often work together to solve complex real-world tasks. This collaboration promotes efficiency and effectiveness by decomposing a complex task into simpler sub-tasks, allowing each to be handled by an expert in its domain. Grounded in the world via our platform, V-_IRL_ agents can leverage geospatial data and street view imagery to collaborate with other agents as well as with human users.\n' +
      '\n' +
      '#### 3.4.1 Agent-Agent Collaboration\n' +
      '\n' +
      'As with previous agents, collaborative agents are designed for specific tasks; however, they can handle objectives beyond their expertise through collaboration with each other.\n' +
      '\n' +
      '_Ling travels to cities around the world. She seeks out authentic experiences and is always undfraid to ask for help from Locals whenever she finds herself lost..._\n' +
      '\n' +
      'After obtaining route descriptions from Locals, Ling starts her journey--as shown in Fig. 7. Grounded in our embodied platform, Ling can adjust her pose and identify visual landmarks along the streets using open-world recognition and her map. Correctly recognizing these landmarks helps GPT-4 to make correct decisions about where to change direction, move forward, and stop, as seen in the top two New York City cases in Fig. 7. The success of these decisions made by GPT-4 relies on the real-sensory input for visual grounding and the interactive environment from V-_IRL_.\n' +
      '\n' +
      'Nevertheless, Ling may occasionally fail to find the destination. In the bottom left San Francisco example in Fig. 7, Ling passes by the Apple Store because only its stainless steel wall is visible from her viewpoint. In the bottom right Hong Kong example, Ling mistakes another restaurant for her destination and stops prematurely. Fortunately, when she makes these mistakes, Ling can ask another Local agent for new directions and start another round of navigation, which eventually leads her to the destination.\n' +
      '\n' +
      'Figure 6: Visualization for Hiro’s lunch exploration in HK.\n' +
      '\n' +
      'Figure 7: Ling and Local collaboration examples. Trajectories in red and green mean Ling’s first and second attempts, respectively.\n' +
      '\n' +
      '#### 3.4.2 Human-Agent Collaboration\n' +
      '\n' +
      'Grounded in the same environment we humans inhabit, V-_IRL_ agents can collaborate with and assist real human users.\n' +
      '\n' +
      'As depicted in Fig. 8, Diego\'s itinerary is tailored to _your_ (the user\'s) needs. Diego not only considers your physical and mental interception status, budget for each activity, but also anticipates your status changes and cost when you follow each event. He is able to take into account _real_ travel times from the V-_IRL_ platform and select suitable destinations by _collaborating_ with another recommendation agent.\n' +
      '\n' +
      'In contrast, Fig. 10 shows that a simpler "ungrounded" LLM-only concierge agent is unable to consider the real distance and travel time between locations without access to V-_IRL_, resulting in an impractical itinerary. For example, lacking _real_ geospatial information, the ungrounded concierge allocates only _30 minutes_ for travel between the "Brooklyn Botanic Garden" and "Wave Hill" in the Bronx, which actually requires _60-100 minutes_*. The hallucinated travel times overlook geospatial realities and result in a plan with excessively distant destinations.\n' +
      '\n' +
      'Footnote *: (per Google Maps [https://maps.app.goo.gl/SW1r5GSx3ZVo7BIr7](https://maps.app.goo.gl/SW1r5GSx3ZVo7BIr7)).\n' +
      '\n' +
      'Also, as shown in Fig. 11, you can intervene in Diego\'s\n' +
      '\n' +
      'Figure 8: _The Perfect Day Itinerary_: Crafted by Diego, our iterative concierge agent, this schedule is meticulously tailored, accounting for your mental and physical well-being and budget variations as your day unfolds.\n' +
      '\n' +
      'Figure 10: An ungrounded LLM-only concierge agent’s itinerary.\n' +
      '\n' +
      'Figure 9: Diego traverses regions of interest to find scenic locations to add to your itinerary.\n' +
      '\n' +
      'planning process by adjusting your interoceptive status or by providing verbal feedback. In response, Diego promptly revises his original plan to accommodate your demands, and re-estimates your state changes after his revision.\n' +
      '\n' +
      'Finally, using V-_IRL_\'s street views and Map, Diego can traverse regions of interest scouting for potential scenic viewpoints for you to visit as shown in Fig. 9. He uses VQA to rate and assess each captured view, and adds the highest-rated locations to your itinerary.\n' +
      '\n' +
      '## 4 System Fundamentals\n' +
      '\n' +
      'This section introduces our system\'s core: a platform designed for perception-driven agents that transforms real-world cities around the world into a vast virtual playground where agents can be constructed to solve practical tasks. At its heart, V-_IRL_ is comprised of a hierarchical architecture (see Fig. 12). The _platform_ lies at the foundation--providing the underlying components and infrastructure for agents to employ. Higher level _capabilities_ of [] Perception, [] Reasoning, [] Action, and [] Collaboration emerge from the platform\'s components. Finally, _agents_ leverage these capabilities and user-defined metadata in task-specific routines to solve tasks.\n' +
      '\n' +
      '### Agent Definition\n' +
      '\n' +
      'In our system, agent behavior is shaped by user-defined metadata, including a background, an intended goal, and an interoceptive state. The _background_ provides the context necessary to instantiate the agent in the real world (location), and to guide its reasoning and decision-making (biography). _Intentions_ outline agents\' purpose within the environment. An agent\'s _interoceptive state_ reflects its internal mental and physical status--varying over time and influencing its behavior. This novel concept is crucial to AI agents for enhancing collaboration with humans (see Sec. 3.4.2).\n' +
      '\n' +
      'Concretely, agents are developed by writing task-specific run() routines that leverage the various components of our platform and the agent\'s metadata to solve tasks.\n' +
      '\n' +
      '### Platform Components\n' +
      '\n' +
      'Next, we delve into the platform components, which provide the infrastructure to instantiate capabilities, execute agent actions, and ground agents in the real world.\n' +
      '\n' +
      '#### 4.2.1 Environment (Action)\n' +
      '\n' +
      'Environment components are responsible for grounding agents in the world around them: providing a navigable representation of real cities (see Sec. 3.1). Geographic coordinates serve as the link between the world and our virtual representation of it. Leveraging the Google Maps Platform (GMP) [24], V-_IRL_ enables agents to access street view imagery, query valid movements, retrieve information about nearby locations, and plan routes. As these coordinates and location information are bound to the real world, they also provide a natural interface with external tools that leverage geolocation--such as real estate APIs (see Sec. 3.2). Technical designs of environment are detailed in Appendix C.\n' +
      '\n' +
      '#### 4.2.2 Vision (Perception)\n' +
      '\n' +
      'Perception components enable agents to process the sensory-rich data provided by the environment, especially street view imagery. Pretrained localization models [37] give agents a precise spatial understanding of their environment. This allows RX-399 to identify and count instances of objects, and Hiro to pick out specific businesses to look up with the GMP (Sec. 3.3). While localization models allow for precise interaction with perceptive input, open-world recognition models [51] are more general, and allow agents to detect a wider range of objects in their field of view (_e.g._, Tourist searches for the Apple Store). Pretrained feature matching models [40] provide an understanding of continuity across views of the same location, and enable agents to identify & deduplicate instances of the same object from different viewpoints (Sec. 3.3). Multimodal models with VQA & Captioning capabilities [36] bridge the perceptual world with natural language, and are essential for integration with reasoning (Sec. 3.3).\n' +
      '\n' +
      'Figure 11: Diego adapts original plan to suit user’s intervention.\n' +
      '\n' +
      'Figure 12: Hierarchical V-_IRL_ architecture described in Sec. 4.\n' +
      '\n' +
      '#### 4.2.3 Language (Reasoning & Collaboration)\n' +
      '\n' +
      'Reasoning components allow decision making based on information from perception and the environment. LLMs such as GPT-4 [2] and Llama 2 [66] interface across various APIs (Sec. 3.2), transforming environmental data and perceptual outputs into actionable insights. They also enable [2]. Collaboration between agents or with humans through natural language (Sec. 3.4) Custom prompts facilitate this interaction (see Sec. 4.4).\n' +
      '\n' +
      '### V-_lr_ Capabilities\n' +
      '\n' +
      'Our platform\'s components can be flexibly combined to exhibit a vast array of capabilities. In Sec. 3, we present agents that exhibit increasingly complex behaviors, each requiring more components of the platform. From simple combinations, like the Route Optimizer (Sec. 3.1), to more complex arrangements, like the Tourist (Sec. 3.4.1), our system showcases the versatility and potential of the V-_IRL_ platform to be applied to various real-world scenarios. Next, we perform a high-level case study of how V-_IRL_\'s components are combined to create our most complex agent; in Appendix D, we delve deeper into the low-level platform details that underpin creating a V-_IRL_ agent.\n' +
      '\n' +
      '### High-Level System Case Study: Interactive Concierge "Diego"\n' +
      '\n' +
      'By studying Diego (Sec. 3.4.2), we illustrate how our platform\'s components are combined to create complex agents.\n' +
      '\n' +
      'Behind Diego\'s proficiency in developing itineraries is his iterative planning pipeline (depicted in Fig. 13). The process begins with Diego creating an initial draft plan for the first activity using GPT-4, taking into account the user\'s biography, requirements, and previous activities in working memory. This draft is then meticulously refined. First, a hierarchical coordination module retrieves real transportation time and asks a recommendation agent for dining recommendations. Subsequently, an interoceptive estimation module evaluates the effect of the proposed activity on the user\'s mental/physical state and budget.\n' +
      '\n' +
      'The crucial final step involves a supervisor module, which reviews ("audits") the incoming activity in light of the current user status, remaining budget, and potential interactions (exemplified in Fig. 11). If the supervisor deems the plan unsuitable, it initiates revisions. The revised plan is then looped back to the hierarchical coordinator and interoceptive estimator for reliability, followed by another review from the supervisor (see the revising loop in Fig. 13). This iterative process between the hierarchical coordinator, the interoceptive estimator, and the supervisor continues until the supervisor approves the activity and adds it to its working memory.\n' +
      '\n' +
      'After finalizing an activity, Diego proceeds to plan the subsequent activity by repeating this process until the day\'s itinerary is complete.\n' +
      '\n' +
      '## 5 V-_lrl_ Benchmarks\n' +
      '\n' +
      'In the previous sections, we illustrate the primary benefit of the V-_IRL_ platform: seamless access to first-person street-view imagery and descriptive information about real-world cities across the globe. This _scalable_ source of _truly open-world_ data can be harnessed to test core component models and agent capabilities. We propose three V-_IRL_ benchmarks: two evaluating vision-language models on open-world vision tasks (Secs. 5.2 and 5.3), and one evaluating end-to-end agent performance (Sec. 5.4). Benchmark details are in Appendix E.\n' +
      '\n' +
      '### Automated Data and Annotation Collection\n' +
      '\n' +
      'To allow our V-_IRL_ benchmarks to scale globally, we develop an automatic data/annotation construction pipeline instead of crawling and manually annotating limited data. This allows models to be conveniently tested worldwide, provided there is access to Google Street Views [24].\n' +
      '\n' +
      '**Region Selection.** Though our benchmark is feasible across all regions covered by the GMP, we select 14 districts across 12 cities from 6 continents to ensure coverage of a diverse\n' +
      '\n' +
      'Figure 13: Architecture overview of interactive concierge agent Diego (Sec. 3.4.2). See pipeline description in Sec. 4.4.\n' +
      '\n' +
      'data distribution while keeping inference costs affordable. The detailed locations of these regions are listed in Tab. 2.\n' +
      '\n' +
      '**Place Types.** We collect place information in each region for all 96 places types annotated by GMP+. Our V-_IRL_ place: localization, recognition and VQA benchmarks are built upon all or part of these place types.\n' +
      '\n' +
      'Footnote †: [https://developers.google.com/maps/documentation/places/web-service/supported_types/#table1](https://developers.google.com/maps/documentation/places/web-service/supported_types/#table1)\n' +
      '\n' +
      '**Vision and Place Data Collection.** Within each region, we collect geolocations with available street views, place information, and place-centric images. **Data Cleaning.** Though scalable, automated data collection can introduce noise due to the absence of human supervision. To this end, we design three automatic data cleaning strategies: \\(i)\\)_distance-based filtering_ to exclude places not easily visible from any street views due to their distance; \\(ii)\\)_human-review filtering_ to remove "zombie" places with no reviews which might no longer be valid or relevant; and \\(iii)\\)_CLIP-based filtering_ to retain only _place-centric images_ with a high CLIP likelihood of being storefronts.\n' +
      '\n' +
      '### V-_lrl_ Place: Localization\n' +
      '\n' +
      'Every day, humans traverse cities, moving between varied places to fulfill a range of goals, like the Intentional Explorer agent (Sec. 3.3). We assess the performance of vision models on the everyday human activity of _localizing places_ using street view imagery and associated place data.\n' +
      '\n' +
      '**Setups.** We modify RX-399 (Sec. 3.3) to traverse polygonal areas while localizing & identifying 20 types of places. We subsample 28 polygonal areas from the 14 districts.\n' +
      '\n' +
      '**Benchmarked Models.** We evaluate three prominent open-world detection models: GroundingDINO [43], GLIP [37] and Owl-ViT [47]. We also implement a straightforward baseline, CLIP (w/ GLIP proposal), which involves reclassifying the categories of GLIP proposals with CLIP [51].\n' +
      '\n' +
      '**Evaluation.** We evaluate the models based on localization recall, which is quantified as \\(\\frac{N_{\\text{tp}}}{N_{\\text{tp}}+N_{\\text{fn}}}\\), where \\(N_{\\text{tp}}\\) and \\(N_{\\text{fn}}\\) represents the number of correctly localized places and missed places, respectively.\n' +
      '\n' +
      '**Matching between Object Proposals and Places.** As mentioned in Sec. 5.1, we do not annotate bounding boxes for places on each potential street view image. Such human annotation diverges from our initial motivation of providing plug-and-play and sensor-rich (V-_IRL_) benchmarks. To assign ground truth for each object proposal in this scenario, we develop a simple matching strategy to assign object proposals from street view object detections to nearby places.\n' +
      '\n' +
      'As illustrated in Fig. 14, we first project the bounding box of each object proposal onto a frustum in the 3D space, subject to a radius. We then determine if any nearby places fall within this frustum and radius. If any nearby place is found, the closest one is assigned as the _ground truth_ for the object proposal. Otherwise, the object proposal is regarded as a _false positive_. When multiple places are inside the frustum, we consider the nearest one as the ground truth since it would likely block the others in the image. _This process is also used in Intentional Explorer agent Hiro to parse object proposals on image to place information._\n' +
      '\n' +
      '**Results.** Tab. 3 shows that open-world detectors like GroundingDINO [43], Owl-ViT [47] and GLIP [37] are biased towards certain place types such as school, cafe, and convenience store, respectively. In contrast, CLIP (w/ GLIP proposal) can identify a broader spectrum of place types. This is mainly caused by the category bias in object detection datasets with a limited vocabulary. Hence, even if detectors like Owl-ViT are initialized with CLIP, their vocabulary space narrows down due to fine-tuning. These results suggest that cascading category-agnostic object proposals to zero-shot recognizers appears promising for "real" open-world localization--especially for less common categories in object detection datasets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Continent** & **City** & **District** \\\\ \\hline Africa & Johannesburg & Rosebank \\\\  & Lagos & Surulere \\\\ \\hline \\multirow{4}{*}{Asia} & Mumbai & Khar \\\\  & New Delhi & Lajpat Nagar \\\\  & Hong Kong & Prince Edward \\\\  & Tokyo & Shinjuku \\\\ \\hline Australia & Melbourne & CBD \\\\  & Melbourne & SouthBank \\\\ \\hline Europe & Milan & Brera \\\\  & London & Oxford St \\\\ \\hline \\multirow{4}{*}{North America} & New York City & Chinatown, Manhattan \\\\  & New York City & SoHo, Manhattan \\\\ \\cline{1-1}  & San Francisco & Union Square \\\\ \\hline South America & Buenos Aires & Monserrat \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Region list for global V-_IRL_ benchmarks.\n' +
      '\n' +
      'Figure 14: Matching between 2D object proposal and street place.\n' +
      '\n' +
      '### V-_lr_ Place: Recognition and VQA\n' +
      '\n' +
      'In contrast to the challenging V-_IRL_ place localization task using street view imagery alone, in real life, humans can recognize businesses by taking a closer, place-centric look. We assess existing vision models in this manner on two perception tasks based on place-centric images: \\(i\\)) recognizing specific place types; \\(ii\\)) identifying human intentions via Vision Question Answering (VQA), dubbed "intention VQA".\n' +
      '\n' +
      '**Setups.** For recognition, we assess 10 open-world recognition models on identifying a place\'s type (from 96 options) using place-centric images (see Tab. 4). For intention VQA, we evaluate 8 multi-modal large language models (MM-LLM) to determine viable human intentions from a four-option multiple-choice. The _V-IRL Place_ VQA process is illustrated in Fig. 15, where the candidate and true choices are generated by GPT-4 [2] given the place types and place names corresponding to the image.\n' +
      '\n' +
      '**Place-centric Images _vs_. Street View Images.** In contrast to the street view imagery utilized in the _V-IRL Place_ localization benchmark, the _V-IRL Place_ recognition and VQA benchmarks use place-centric images. To illustrate the distinction between these image types, we present examples in Fig. 16. The figure shows that street view images, sourced from the Google Street View database5, are taken from the street and encompass a broader view of the surroundings, including multiple buildings and possible occluding object-s/vehicles. In contrast, place-centric images, drawn from the Google Place database6, are taken on foot and focus more closely on the specific place--providing a more concentrated view.\n' +
      '\n' +
      'Footnote 6: [https://developers.google.com/maps/documentation/places/wcb-service/photos](https://developers.google.com/maps/documentation/places/wcb-service/photos)\n' +
      '\n' +
      '**Evaluation.** We adopt mean accuracy (mAcc) to evaluate both place recognition and VQA tasks. For place VQA, we follow MMBench [44] to conduct circular evaluation and GPT-assisted answer parsing.\n' +
      '\n' +
      '**Results.** Tab. 4 shows that CLIP (L/14@336px) outperforms even the biggest version of Eva-02-CLIP and SigLIP in the V-_IRL_ recognition task, highlighting the high-quality data used to train CLIP [51]. The bottom of the table shows that BLIP2 [36], InstructBLIP [17], and LLaVA-1.5 [41] excel at intention VQA, whereas others struggle. We note that\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline  & **Model** & **\\#Param** & **mAcc (\\%)** \\\\ \\hline \\hline \\multicolumn{3}{c}{_V-IRL Place Recognition_} \\\\ CLIP [51] & V-IR/32 & 151M & 18.2 \\\\ CLIP [51] & V-IC/14 & 428M & 37.2 \\\\ CLIP [51] & V-IT-L/14@336px & 428M & 41.3 \\\\ OpenCLIP [16] & V-IR/32 & 151M & 21.2 \\\\ OpenCLIP [16] & V-IR/14 & 428M & 31.0 \\\\ Eva-02-CLIP [64] & V-IR/16 & 150M & 19.5 \\\\ Eva-02-CLIP [64] & V-IT-L/14 & 428M & 34.2 \\\\ Eva-02-CLIP [64] & V-IT-L/14@336px & 428M & 40.7 \\\\ SigLIP [81] & V-IT-B/16 & 203M & 29.5 \\\\ SigLIP [81] & V-IT-L/16@384px & 652M & 37.3 \\\\ \\hline \\hline \\multicolumn{3}{c}{_V-IRL Place VQA_} \\\\ MiniGPT-4 [83] & Vicuna-138-v0 & 14.0B & 3.9 \\\\ mPLUG-Owl [78] & LLaMA-7B & 7.2B & 5.5 \\\\ Shika [15] & Vicuna-7B & 7.2B & 10.9 \\\\ BLIP-2 [36] & FlanT5xx & 12.1B & 69.6 \\\\ InstochIP [17] & FlanT5xx & 12.0B & 68.0 \\\\ LLaVA [42] & Vicuna-13B-v1.3 & 13.4B & 23.5 \\\\ LLaVA-1.5 [41] & Vicuna-7B-v1.5 & 7.2B & 60.1 \\\\ LLaVA-1.5 [41] & Vicuna-13B-v1.5 & 13.4B & 61.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Benchmark results on _V-IRL Place_ recognition and _V-IRL Place_ VQA. Green indicates increased resolution models, while Blue denotes model parameter scaling.\n' +
      '\n' +
      'Figure 16: Top row: examples of street view imagery. Bottom row: corresponding place-centric images.\n' +
      '\n' +
      'Figure 15: Example of _V-IRL Place_ VQA process.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '## 6 Discussion: Ethics & Privacy\n' +
      '\n' +
      'Our platform serves as a tool for AI development and as a crucible for ethical discourse and preparation. As AI is inevitably being integrated into society--_e.g._, via augmented reality wearables, spatial computing platforms, or mobile robots navigating city streets--it is imperative to confront and discuss ethical and privacy concerns now. Unlike these impending _real-time_ systems, the data accessed by V-_IRL_ is "stale" and preprocessed--providing a controlled environment to study these concerns.\n' +
      '\n' +
      'Notably, V-_IRL_ exclusively utilizes preexisting, readily available APIs; it does not capture or make available any previously inaccessible data. Our primary source of street-view imagery, Google Maps [24], is subject to major privacy-protection measures, including blurring faces and license plates [22]. Moreover, V-_IRL_ complies with the Google Maps Platform license4, similarly to notable existing works that also leverage Google\'s street views [1, 14].\n' +
      '\n' +
      'Footnote 4: [https://cloud.google.com/maps-platform/terms](https://cloud.google.com/maps-platform/terms)\n' +
      '\n' +
      'We believe V-_IRL_ is an invaluable tool for researching bias. As discussed in Sec. 5.5, V-_IRL_\'s _global scale_ provides a lens to study linguistic, cultural, and other geographic biases inherent in models. By using V-_IRL_ to study such questions, we aim to preemptively tackle the ethical dilemmas that will arise with deploying real-time systems rather than being blindsided by them. We hope our work helps spur proactive discussion of future challenges throughout the community.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In this work, we introduce V-_IRL_, an open-source platform designed to bridge the sensory gap between the digital and physical worlds, enabling AI agents to interact with the real world in a virtual yet realistic environment. Through V-_IRL_, agents can develop rich sensory grounding and perception, utilizing real geospatial data and street-view imagery. We demonstrate the platform\'s versatility by creating diverse exemplar agents and developing benchmarks measuring the performance of foundational language and vision models on open-world visual data from across the globe.\n' +
      '\n' +
      'This platform opens new avenues for advancing AI capabilities in perception, decision-making, and real-world data interaction. As spatial computing and robotic systems become increasingly prevalent, the demand for and possibilities of AI agents will only grow. From personal assistants to practical applications like urban planning to life-changing tools for the visually impaired, we hope V-_IRL_ helps usher in a new era of perceptually grounded agents.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Image geo-localization based on multiplearest neighbor feature matching usinggeneralized graphs. _TPAMI_, 2014.\n' +
      '* [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Alteschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In _NeurIPS_, 2022.\n' +
      '* [4] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. _arXiv preprint arXiv:1807.06757_, 2018.\n' +
      '* [5] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhari, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. In _CVPR_, 2018.\n' +
      '* [6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [7] Yuki M Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi. PASS: An imagenet replacement for self-supervised pretraining without humans. In _NeurIPS_, 2021.\n' +
      '* [8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.\n' +
      '* [9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.\n' +
      '* [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do As I Can, Not As I Say: Grounding language in robotic affordances. In _CoRL_, 2023.\n' +
      '* [12] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from rgb-d data in indoor environments. In _3DV_, 2017.\n' +
      '* [13] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. In _NeurIPS_, 2020.\n' +
      '* [14] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. TOUCHDOWN: Natural language navigation and spatial reasoning in visual street environments. In _CVPR_, 2019.\n' +
      '* [15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.\n' +
      '* [16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In _CVPR_, 2023.\n' +
      '* [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In _NeurIPS_, 2023.\n' +
      '* [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [19] Danny Driess, Fei Xia, Mehdi S. M. Sajiadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An Embodied Multimodal Language Model. In _ICML_, 2023.\n' +
      '* [20] Y Du, C Li, R Guo, X Yin, W Liu, J Zhou, Y Bai, Z Yu, Y Yang, Q Dang, et al. PP-OCR: A practical ultra lightweight ocr system. arxiv 2020. _arXiv preprint arXiv:2009.09941_, 2020.\n' +
      '* [21] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In _CVPR_, 2021.\n' +
      '* [22] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In _ICCV_, 2009.\n' +
      '* [23] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In _ECCV_, 2022.\n' +
      '* [24] Google Map Team. Google Map Platform. [https://mapslatform.google.com/](https://mapslatform.google.com/).\n' +
      '* [25] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _ICRA_, 2017.\n' +
      '* [26] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In _ICLR_, 2023.\n' +
      '* [27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language Models As Zero-Shot Planners: Extracting actionable knowledge for embodied agents. In _ICML_, 2022.\n' +
      '* [28] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner Monologue: Embodied reasoning through planning with language models. In _CoRL_, 2022.\n' +
      '* [29] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3d value maps for robotic manipulation with language models. _arXiv preprint arXiv:2307.05973_, 2023.\n' +
      '* [30] Heinrich Kuttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktaschel. The nethack learning environment. In _NeurIPS_, 2020.\n' +
      '* [31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.\n' +
      '* [32] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In _NeurIPS_, 2023.\n' +
      '* [33] Alexander C Li, Ellis Brown, Alexei A Efros, and Deepak Pathak. Internet explorer: Targeted representation learning on the open web. In _International Conference on Machine Learning_, pages 19385-19406. PMLR, 2023.\n' +
      '* [34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _ICLR_, 2022.\n' +
      '* [35] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communative agents for" mind" exploration of large language model society. In _NeurIPS_, 2023.\n' +
      '* [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.\n' +
      '* [37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _CVPR_, 2022.\n' +
      '\n' +
      '* [38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language model programs for embodied control. In _ICRA_, 2023.\n' +
      '* [39] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2Motion: From natural language instructions to feasible plans. _arXiv preprint arXiv:2303.12153_, 2023.\n' +
      '* [40] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In _ICCV_, 2023.\n' +
      '* [41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv:2310.03744_, 2023.\n' +
      '* [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.\n' +
      '* [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.\n' +
      '* [44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is Your Multi-modal Model an All-around Player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [45] Zeyi Liu, Arpit Bahety, and Shuran Song. REFLECT: Summarizing robot experiences for failure explanation and correction. In _CoRL_, 2023.\n' +
      '* [46] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac Gym: High performance gpu-based physics simulation for robot learning. In _NeurIPS_, 2021.\n' +
      '* [47] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple Open-Vocabulary Object Detection with Vision Transformers. In _ECCV_, 2022.\n' +
      '* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.\n' +
      '* [49] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.\n' +
      '* [50] Joon Sung Park, Joseph O\'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative Agents: Interactive simulacra of human behavior. In _UIST_, 2023.\n' +
      '* [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [52] Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Bryan Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. GeoDE: a geographically diverse evaluation dataset for object recognition. In _NeurIPS_, 2023.\n' +
      '* [53] William A Gavrika Rojas, Suduya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The Dollar Street Dataset: Images representing the geographic and socioeconomic diversity of the world. In _NeurIPS_, 2022.\n' +
      '* [54] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In _ICCV_, 2019.\n' +
      '* [55] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In _NeurIPS_, 2023.\n' +
      '* [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatas R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.\n' +
      '* [57] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. ChatGPT: Optimizing language models for dialogue. _OpenAI blog_, 2022.\n' +
      '* [58] Raphael Schumann and Stefan Riezler. Generating landmark navigation instructions from maps as a graph-to-text problem. In _ACL_, 2020.\n' +
      '* [59] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang. VELMA: Verbalization embodiment of llm agents for vision and language navigation in street view. _arXiv preprint arXiv:2307.06082_, 2023.\n' +
      '* [60] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu, and Hongsheng Li. LMDrive: Closed-loop end-to-end driving with large language models. _arXiv preprint arXiv:2312.07488_, 2023.\n' +
      '* [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, 2018.\n' +
      '* [62] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. In _NeurIPS_, 2023.\n' +
      '* [63] Significant Gravitas. AutoGPT. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT), 2023.\n' +
      '* [64] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved Training Techniques for CLIP at Scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [65] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. _Communications of the ACM_, 2016.\n' +
      '\n' +
      '* [66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. LLAMA 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [67] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.\n' +
      '* [68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.\n' +
      '* [69] Michael Wooldridge and Nicholas R Jennings. Intelligent Agents: Theory and practice. _The knowledge engineering review_, 1995.\n' +
      '* [70] Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. _arXiv preprint arXiv:2312.14135_, 2023.\n' +
      '* [71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* [72] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.\n' +
      '* [73] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson Env: Real-world perception for embodied agents. In _CVPR_, 2018.\n' +
      '* [74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. SAPIEN: A simulated part-based interactive environment. In _CVPR_, 2020.\n' +
      '* [75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. _arXiv preprint arXiv:2309.16671_, 2023.\n' +
      '* [76] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic segmentation emerges from text supervision. In _CVPR_, 2022.\n' +
      '* [77] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In _ICLR_, 2023.\n' +
      '* [78] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [79] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. HomeRobot: Open-vocabulary mobile manipulation. In _CoRL_, 2023.\n' +
      '* [80] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.\n' +
      '* [81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. _arXiv preprint arXiv:2303.15343_, 2023.\n' +
      '* [82] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _ECCV_, 2022.\n' +
      '* [83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* [84] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. _arXiv preprint arXiv:2305.17144_, 2023.\n' +
      '\n' +
      'A Appendix Outline\n' +
      '\n' +
      'In these supplementary materials, we provide additional details for our V-_IRL_ platform, including:\n' +
      '\n' +
      '* Designs behind V-_IRL_ Agents (Appendix B);\n' +
      '* Technical details and challenges in the V-_IRL_ environment (Appendix C).\n' +
      '* A low-level case study of Intentional Explorer agent Hiro, delving into implementation details of our system such as LLM prompts (Appendix D);\n' +
      '* More detailed setups and results for our V-_IRL_ benchmarks (Appendix E).\n' +
      '\n' +
      '## Appendix B Technical Details of V-_IRL_ Agents\n' +
      '\n' +
      'In Sec. 3, our discussion mainly focuses on the innovative capabilities and behaviors of V-_IRL_ agents empowered by our platform. We avoid in-depth discussions about technical details in the main paper due to the concern of readability. In this section, we go through our main technical designs for each agent. More comprehensive technical implementations are available in our released code.\n' +
      '\n' +
      '### Peng: Route Optimizer\n' +
      '\n' +
      'Peng is designed to showcase the utilization of real geographic coordinates within our platform. By processing a sequence of real addresses, Peng calculates the _shortest path_ for traversing them using various modes of transportation, such as walking, driving, and bicycling, among others. This capability is powered by the mapping module described in Appendix C.3. After that, Peng proceeds to navigate through the destinations along the predetermined path, employing the point navigation procedure outlined in Appendix C.2.2.\n' +
      '\n' +
      '### Aria: Place Recommender\n' +
      '\n' +
      'Aria leverages the realistic place information provided by our Place Info & Search module (see Appendix C.4) to enhance LLMs\' reasoning capability in the geographic aspect. Specifically, Aria evaluates Peng\'s intention to determine the suitable type of place and searches all possible places in the vicinity. For each searched place, Aria considers its reviews and user ratings from Google to summarize a place overview. Subsequently, we customize prompts for Aria to amalgamate Peng\'s biography, intentions, and the summarized place overviews to rate each place between 0 and 10, accompanied by justifications.\n' +
      '\n' +
      'Without such technical designs, LLMs could recommend some places that are either too distant or permanently closed. This issue arises because LLMs struggle to accurately understand geospatial relationships and often depend on outdated databases.\n' +
      '\n' +
      '### Vivek: Estate Agent\n' +
      '\n' +
      'The process employed by Vivek is similar to that of Aria, as both are designed to recommend places. However, Vivek showcases the versatility of our V-_IRL_ platform by demonstrating how it can seamlessly integrate a wide range of realistic information beyond the Google Maps Platform, with a standardized definition of geographic coordinates. This capability enables the creation of even more sophisticated and intriguing agents.\n' +
      '\n' +
      '### RX-399: Urban Assistance Robot\n' +
      '\n' +
      'Different from previous example agents, RX-399 introduces visual perception capabilities such as open-world detection and feature matching. There are two fundamental systems inside it - navigation and perception. In terms of navigation, RX-399 can automatically navigate from the current position to the pre-defined destination step by step. This navigation process is elaborated in Appendix C.2.2, and thus, will not be extensively discussed here.\n' +
      '\n' +
      'When it comes to its perception system, RX-399 is designed to simulate human visual perception by capturing street views within a 90-degree horizontal angle to both its left and right. For each captured view, an open-world detection process is conducted. Leveraging the interactive capabilities of our environment, we further propose an _active detection_ strategy to dynamically adjust the agent\'s egopose and focal length according to the scale and position of potential objects. This significantly improves its performance as illustrated in Tab. 6. In the future, more advanced approaches such as visual search [70] could also be considered. In the subsequent de-duplication procedure, which aims to avoid double-counting objects across different viewpoints, we have tried a few strategies including measuring with multi-view geometry, object tracking, and feature matching. We choose feature matching because of its accuracy and efficiency.\n' +
      '\n' +
      '### Imani: Urban Planner\n' +
      '\n' +
      'The visual perception system of Imani mirrors that of RX-399. The primary distinction between them lies in their navigation systems. Imani possesses the capability to plan a navigation route in the given polygonal region, enabling RX-399 to traverse that region. This functionality is named "region navigation" and elaborated in Appendix C.2.2. Additionally, within the Imani agent, we develop a heatmap visualization tool to visualize and verify the data collected by RX-399 (see Fig. 3).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} City & Hong Kong & New York City \\\\ \\hline w/ active detection & **0.63 / 0.83** & **0.71 / 1.00** \\\\ w/o active detection & 0.10 / 0.33 & 0.30 / 0.60 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: RX-399 detection performance with or without active detection manner. Metrics are accuracy / recall.\n' +
      '\n' +
      '### Hiro: Intentional Explorer\n' +
      '\n' +
      'Hiro is a representative agent equipped with geographical, perceptual, and reasoning abilities, to address a daily human task: randomly exploring to find a suitable restaurant. In this regard, we have dedicated a separate section to offer an in-depth case study, including the detailed methodology and prompts in Appendix D.\n' +
      '\n' +
      '### Ling: Tourist\n' +
      '\n' +
      'Our vision language navigation pipeline of Ling is similar to [59], leveraging _vision models_, the map, and LLMs. At each position, we start by capturing eight street views around the agent, corresponding to front, left front, left, left behind, behind, right and right front. Vision models use these street views to _identify_ landmarks mentioned in route descriptions, which are then verbalized as _landmark observations_. Also, intersection information is retrieved from the _mover_ to formulate an _intersection observation_. LLMs play a crucial role in _processing_ landmark & intersection observations along with the agent\'s previous working history to _determine_ the next action. After each action, current observations and actions are stored into the agent\'s working history. This auto-regressive process continues until the agent decides to stop.\n' +
      '\n' +
      '### Local Agent\n' +
      '\n' +
      'The primary mission of the Local agent is to generate human-like and easily followable navigation instructions on a global scale (refer to 3.4.1). This task is known as navigation instruction generation [58]. Contrary to most existing research, which depends on human-annotated data for limited geographic areas, our "Local" agent automatically selects _suitable landmarks_ taking account into real-world places and _generates human-like route descriptions_ using LLMs across the globe. Remarkably, it achieves this without the need for any training data, relying solely on our tailored prompts and a few in-context examples. The effectiveness of its generated instructions has been verified through collaboration with "Ling". To the best of our knowledge, this is a first in the field. There are massive technical details on selecting easily noticeable landmarks and prompt engineering, which are available in our released code.\n' +
      '\n' +
      '### Diego: Interactive Concierge\n' +
      '\n' +
      'In Sec. 4.4, we have already presented the technical designs of Diego\'s itinerary. Here, we detail how Diego can find scenic locations as shown in Fig. 9. For any given destination, such as "Fort Tryon Park", Diego will sample a rectangle region around it and _traverse_ all navigable positions within it. At each position, he will _capture a photograph_ (_i.e_. street view imagery) using pre-defined headings, pitches, and FOVs. Each photograph will then be evaluated using GPT-4(V) [2], where it receives a rating between 0 and 10 along with _explanatory reasons_.\n' +
      '\n' +
      '## Appendix C Technical Details of Environment\n' +
      '\n' +
      'In Sec. 4.2.1, we provide an overview of our system\'s environment, which grounds agents in real life. Here, we delve into the technical designs beyond mere leveraging Google Map Platform system calls. Concrete implementations can be found in our open-sourced code.\n' +
      '\n' +
      '### Geolocation & Street View Imagery\n' +
      '\n' +
      'At the core of V-_IRL_ lies its innovative use of sensor-rich environment, including street view imagery and geolocations. They enable agents to gather surrounding place and vision information.\n' +
      '\n' +
      '**Geolocation.** Agents in the V-_IRL_ platform inhabit virtual representations of real cities around the globe. At the core of this representation are geographic coordinates (_i.e_. geolocation) corresponding to points on the Earth\'s surface. The initial geolocation of each agent is specified by its "Location" configuration, as illustrated in Fig. 12. Whenever agents require access to surrounding information (_e.g_. street views, places or maps), geolocation serves as a crucial parameter for querying the related Google Map APIs.\n' +
      '\n' +
      '**Street view imagery.** Google Map Platform specifies each street view imagery with multiple key parameters: geolocation, heading (the horizontal angle ranging from 0\\({}^{\\circ}\\) to 360\\({}^{\\circ}\\)), pitch (a vertical angle ranging from -90\\({}^{\\circ}\\) to 90\\({}^{\\circ}\\)), and Field of View (FOV, ranging from \\(20\\sim 120\\)). It\'s noteworthy that adjusting the FOV here is similar to changing the camera\'s focal length, rather than simply zooming in on an image, which ensures that image resolution remains high, even as the FOV decreases to a low value. By modifying the heading, pitch, and FOV, we can simulate the human sensory process of adjusting one\'s pose and concentrating on a specific area.\n' +
      '\n' +
      '**Alignment between street view imagery and geolocation.** Within our sensor-rich platform, a fundamental challenge is to ensure agents are positioned at geolocations where street view imagery is available. To address this issue, we design a custom operation named "_relocate_". Specifically, when an agent is initialized at a location lacking street view imagery, the "relocate" operation will identify and transition the agent to the nearest viable geolocation where street view data is available. Notice that, this operation is indispensable to our platform, as the positions with available street views are relatively sparse in comparison to the vast continuous space of all possible coordinates.\n' +
      '\n' +
      '### Movement\n' +
      '\n' +
      'Enabling agents to move along city streets is a core functionality of our platform, allowing interaction betweenagents and the real world. Whenever an agent needs to move, this module powers all related processes, from route planning and direction selection to the continuous update of the agent\'s geolocation during its moving. Since Google Map Platform does not provide APIs to access nearby navigable positions and directions, the design of this movement module is a significant technical challenge and a substantial contribution from our team. We discuss its low-level implementations in Appendix C.2.1 and the enabled high-level actions in Appendix C.2.2.\n' +
      '\n' +
      '#### c.2.1 Mover\n' +
      '\n' +
      '**Move by controlling the web interface.** A straightforward solution is to let the agent control the web front-end Google Street View to select moving directions and move. Nevertheless, there are three key challenges for this solution:\n' +
      '\n' +
      '\\((i)\\) _How can Python-implemented agents control the movement via the interaction to the webpage?_ We use a Python package Selenium** to locate web elements responsible for movement. After determining a movement direction, the agent uses Selenium to simulate a click action on the web element corresponding to the chosen direction.\n' +
      '\n' +
      '\\((ii)\\) _How can the agent acquire the necessary information to decide moving direction?_ Although agents can access all potential movement directions from web elements, they cannot identify these directions without prior knowledge of what each represents. We find that the "transform" attribute in the web element corresponding to each direction can be leveraged to calculate their represented heading angles. The heading angle also allows us to collect street view imagery for each movement direction. Agent\'s movement decision-making is then based on these heading angles and the visual data from street view imagery.\n' +
      '\n' +
      '\\((iii)\\) _How to track the agent\'s geolocation along its movement?_ To accomplish this, we customize a webpage element to display the geolocation of the current street view panorama. As the agents move and trigger updates to the street view panorama, this customized element concurrently refreshes to reflect the new geolocation. By using Selenium, we can then extract this updated geolocation data, enabling continuous tracking of the agent\'s geolocation changes.\n' +
      '\n' +
      '**Move by grid-based relocating.** In our test of the above web-based mover, a critical limitation emerged: the web-embedded Google Street View panoramas display only a subset of navigable directions. This constraint significantly restricts our agents\' mobility, often preventing them from successfully navigating to their intended destinations due to the incomplete coverage of potential routes.\n' +
      '\n' +
      'To overcome this obstacle, we develop an alternative method: a grid-based relocating mover. This approach involves performing a grid search for geolocations in the vicinity of the agent and employing the "relocate" operation to sift through these locations, identifying those that are navigable. While this method offers a more comprehensive view of navigable positions, it is markedly more time-consuming than the web-based approach due to the extensive number of Google Maps API calls required.\n' +
      '\n' +
      'In our practical applications, we design a heuristic strategy that combines web-based controlling and grid-based relocation. This hybrid approach aims to balance the trade-offs between the speed and the completeness of navigable position data, optimizing our agents\' capabilities and efficiency in real-world scenarios.\n' +
      '\n' +
      '#### c.2.2 Navigator\n' +
      '\n' +
      'Here, we introduce the high-level action of agents powered by the mover - navigation. Unlike the mover, which concentrates on enabling agent mobility in the environment, the focus here shifts to determining the direction of movement. In our platform, we group different navigators according to their usages into four types:\n' +
      '\n' +
      '\\((i)\\) **Point navigator** is designed to tackle navigation tasks that clearly define single or multiple destinations (represented in addresses or geolocations). This navigator employs the route planning function described in Appendix C.3 to obtain a series of key positions for navigation. At each location, the agent utilizes a greedy algorithm to select the most optimal direction towards the next key position that has not yet been reached. Exemplary agents, such as "Peng", "RX-399" and "Local", use this type of navigator in their implementation.\n' +
      '\n' +
      '\\((ii)\\) **Region navigator** is tailored for agents like "Imani" and "Diego", who need to traverse every position within a polygonal region. This navigator first employs a grid search combined with our "relocate" operation to identify all navigable positions within the specified region. Subsequently, it adopts a heuristic algorithm designed to solve the traveling salesman problem, planning an efficient order for visiting these positions. The agents\' task is to simply follow this predetermined route, visiting each navigable position in the planned order.\n' +
      '\n' +
      '\\((iii)\\) **Vision-language navigator** is specifically developed for the tourist agent "Ling", as well as for tasks within the V-_IRL_ vision-language navigation benchmark. Its primary function is to guide the agent in selecting a proper direction based on navigation instructions. The detailed pipeline is presented in Appendix B.7.\n' +
      '\n' +
      '\\((iv)\\) **Intention navigator** is utilized in intentional explorer agent "Hiro" to select the most suitable direction that aligns with the agent\'s specific intentions. The detailed methodology and prompt are detailed in Appendix D.2.\n' +
      '\n' +
      '### Mapping\n' +
      '\n' +
      'The mapping module in our environment is designed to equip agents with functionalities such as route planning and transportation time estimation. It mainly utilizes the "Directions API"17 from the Google Map Platform to facilitate these capabilities. Given the complex nature of this API\'s interface, our principal focus has been on parsing its output and adapting it into various user-friendly interfaces for agents.\n' +
      '\n' +
      'Footnote 7: [https://developers.google.com/maps/documentation/directions](https://developers.google.com/maps/documentation/directions)\n' +
      '\n' +
      '### Place Info & Search\n' +
      '\n' +
      'Place Info & Search module hosts another important information source in our platform beyond the visual street view imagery, enabling agents to interact with real-world "places". It provides various attributes of places, including type, name, location, imagery, reviews, etc. In this module, our technical efforts are primarily focused on understanding, comparing, and integrating the most suitable functions from the vast array of Google Maps Platform APIs related to place information and nearby place searches. Additionally, we devise some post-processing strategies to identify and eliminate invalid or conflicting data sources from the Google Maps Platform.\n' +
      '\n' +
      'Another essential capability enabled by this module is to associate object proposals in street view imagery and their corresponding places in the real city. This function is vital to enhance the reality of our platform by connecting street view and geolocation. It also powers the "Hiro" agent and the evaluation of the _V-IRL Place_ localization benchmark. The implementation is detailed in Sec. 5.2.\n' +
      '\n' +
      '## Appendix D Low-Level System Case Study: Intentional Explorer "Hiro"\n' +
      '\n' +
      'This section delves deeper into the low-level implementation details of the Intentional Explorer agent "Hiro" (Sec. 3.3), focusing on the prompts utilized to interact with various parts of our system. Concretely, we present the prompts in four subparts: _identifying a type of place to search using the user-defined intention_ (Appendix D.1), _selecting appropriate roads_ (Appendix D.2), _summarizing reviews of places_ (Appendix D.3), and _making action decisions_ (Appendix D.4). These four components jointly enable Hiro to explore in our interactive embodied _environment_ driven by his initial intention.\n' +
      '\n' +
      '### Intention to Place Type\n' +
      '\n' +
      'Starting with a user-defined agent intention, Hiro first determines the type of place that could fulfill this intention using GPT-4 and the following prompt:\n' +
      '\n' +
      '```\n' +
      '[Role] YouarePlaceSuggesterGPT,anexpert inrecommendingtypesofplaces basedonuser-specifiedintentions. [TaskDescription] Givenauser-specifiedintention,determinethetypeof"place" oneshouldseektofulfilltheintention.Yourresponseshould beinthefollowingJSONformat: {"place":"DesiredPlaceType"} [Example] Input:"Intention:<buyabbook>" Output:{"place":"bookstore"} [Input] Intention:<(agent_intention)> [Output] Yourrecommendedplacetypebasedontheuser-specifiedintention,intherequiredJSONformat:\n' +
      '```\n' +
      '\n' +
      'Using this prompt with the intention\n' +
      '\n' +
      '_Hiro is hungry and looking for a place where he can try some good local food. He cannot handle spicy food._\n' +
      '\n' +
      'returns the result\n' +
      '\n' +
      '```\n' +
      '{"place":"restaurant"}.\n' +
      '```\n' +
      '\n' +
      'The identified place type (here, restaurant) is extracted and set as the target category for Hiro\'s _open-world detector_ during his exploration.\n' +
      '\n' +
      '### Road Selection\n' +
      '\n' +
      'Whenever Hiro is at a crossroads, he determines the best road to follow using his _multi-modal LLM_ and _GPT-4_. The primary goal of the road selection process is to identify the road most likely to lead to the desired place type that aligns with Hiro\'s intention. First, Hiro _fetches_ the street view towards each potential road using the V-_IRL_ environment. Then he utilizes his _multi-modal LLM_ (such as _Instruct-BLIP_[17] or _LLaVA_[42]) to generate captions for each road using the following prompt:\n' +
      '\n' +
      '```\n' +
      '[load_idx]:{load_description}\n' +
      '```and concatenated to form all_road_descriptions. These road captions, along with Hiro\'s user-defined intention, are then fed into GPT-4 to determine the most promising road to follow using the following prompt:\n' +
      '\n' +
      '[Role] You are PathSelectorGPT, an expert in choosing the optimal road from multiple candidates based on a user-specified intention.\n' +
      '\n' +
      '[Task Description] Given an intention, the road previously traveled, and descriptions of available candidate roads, select the best road from the crossroad. Your response must be in the following JSON format: {"idx": "Selected road index", "reason": "Justification for your selection"}\n' +
      '\n' +
      '[Example] For the intention "find a grocery store", the road previously traveled as "1", and with candidates "2: Leads to residential area, 3: Leads to a shopping district", the output might be: ("idx": "3", "reason": "Road 3 leads to a shopping district which is more likely to have a grocery store.")\n' +
      '\n' +
      '[Input] User Intention: <{agent_intention}> Road Descriptions: {all_road_descriptions} Previously Traveled Road: Road {from_road_idx}\n' +
      '\n' +
      '[Output] Your chosen road index and the reasoning behind your selection, in the required JSON format:\n' +
      '\n' +
      'We design such a two-stage captioning and decision-making pipeline for road selection because Multi-modal LLMs cannot process multiple images simultaneously. However, with the recent advancements of GPT-4V, it may be possible to perform road selection using several road images with a single prompt at once. Empirical findings suggest that GPT-4V yields more reasonable choices with the following prompt:\n' +
      '\n' +
      '[Role] You are PathSelectorGPT, an expert in choosing the optimal road from multiple road images according to a user-specified intention.\n' +
      '\n' +
      '[Task Description] Given a set of road images, select the best road from the crossroad. Your answer must be in the following JSON format: {"idx": "Selected road index (start by 0)", "reason": "Justification for your selection"}\n' +
      '\n' +
      '[Input] User Intention: <{agent_intention}>\n' +
      '\n' +
      '[Output] Please answer with the road index and the reasoning behind your selection, in the required JSON format:\n' +
      '\n' +
      'An example road selection response for the first crossroad selection in Fig. 6 is as follows:\n' +
      '\n' +
      '[] _"idx": "0", "reason": "Choosing road 0 promises a genuine taste of local cuisine in a less commercialized setting. It\'s likely to have family-run eateries where I can request non-spicy dishes and savor authentic flavors. This road offers a tranquil dining atmosphere, allowing for a more engaged and leisurely culinary exploration among the locals."_\n' +
      '\n' +
      '### Summarize Place Reviews\n' +
      '\n' +
      'When Hiro discovers a place in the street view imagery, he retrieves its corresponding name and Google reviews from the V-_IRL_ environment. There is a complex algorithm behind projecting the 2D box on street view imagery to a concrete place in the real world, which is detailed in "matching between object proposal and places" of Sec. 5.2. After Hiro obtains these place reviews, he summarizes them into a place overview (to aid in decision-making) using the following prompt:\n' +
      '\n' +
      '[Role] You are SummarizeGPT, skilled at condensing multiple reviews into concise overview of a location.\n' +
      '\n' +
      '[Task Description] Given multiple reviews with ratings, craft a brief overview of the place. Your response should be in the following JSON format: {"summarization": "Concise description (limited to 80 words)"}\n' +
      '\n' +
      '[Example] For reviews "Great ambiance but average food (Rating: 3)" and "Loved the decor, food could be better (Rating: 3.5)", the output might be: {"summarization": "The place boasts great ambiance and decor, but the food quality receives mixed reviews."}\n' +
      '\n' +
      '[Input] Reviews: {all_reviews}\n' +
      '\n' +
      '[Output] Your concise overview (max 80 words) based on the provided reviews, in the prescribed JSON format:\n' +
      '\n' +
      '### Action Decision\n' +
      '\n' +
      'After obtaining the overview of the identified place, Hiro decides to visit the place or keep exploration using GPI-4 and the following prompt:\n' +
      '\n' +
      '[Role] You are ActionSelectorGPT, proficient in choosing the most appropriate action based on a user\'s background, intention, and an overview of a place.\n' +
      '\n' +
      '[Task Description] Evaluate the provided user background, intention, and place overview to select the most suitable action from the list. Your response should be in the following JSON format: {"action": "Selected Action", "reason": "Justification for your choice"}\n' +
      '\n' +
      'Possible actions: - enter_place(): Enter the designated place. - continue(): Continues searching for another appropriate place.\n' +
      '\n' +
      '[Example] For the background "loves historical sites", intention "discover local history", and place overview "This is a 200-year-old preserved manson", the output might be: "action": "enter_place()", "reason": "The historical manson aligns with the user\'s interest in historical sites."\n' +
      '\n' +
      '[Input] User Background: <{background}> User Intention: <{intention}> Place Overview: <{place_intro}>\n' +
      '\n' +
      '[Output] Yourchosen action and the rationale behind your decision in the prescribed JSON format:\n' +
      '\n' +
      'Hiro\'s exploration will continue if he decides to continue() and will terminate if he opts for enter_place().\n' +
      '\n' +
      '## Appendix E V-_Irl_ Benchmarks: Details\n' +
      '\n' +
      '### V-_Irl_ Places: Localization (Details)\n' +
      '\n' +
      'All category results.Due to the page limit of the main paper, we only present the results of 10 categories in Tab. 3. Here, we present the place recall for all 20 categories in Fig. 18.\n' +
      '\n' +
      'Example illustrations.To facilitate the understanding of _V-IRL Place_ localization benchmark, we present some examples of CLIP (w/ GLIP proposals) in Fig. 21.\n' +
      '\n' +
      '### V-_Irl_ Places: Recognition and VQA (Details)\n' +
      '\n' +
      'Place types performance for recognition.In Figure 19, we present the averaged accuracy for each place type across 10 benchmarked vision models. The size and the x-axis position of each bubble correspond to the number of places within each type. A clear trend emerges: accuracy tends to correlate with the frequency. Common categories such as clothing store, cafe exhibit higher accuracy, whereas vision models often struggle with infrequent place types like bowling alley or mosque.\n' +
      '\n' +
      'Place types performance for VQA.The place types performance of the V-_IRL_ place VQA in Fig. 20 further\n' +
      '\n' +
      'Figure 18: Recalls in _V-IRL Place_ localization\n' +
      '\n' +
      'verifies the correlation between accuracy and frequency from a human intention perspective. The top-10 categories are closely aligned with the most common human activities, purchasing and dining. In contrast, the bottom-10 place types relate to places that are less frequently encountered and serve a more diverse purpose, such as mosque, plumber and embassy.\n' +
      '\n' +
      '### V-_lr_ Vision-Language Navigation (Details)\n' +
      '\n' +
      'Navigation pipeline.As mentioned in Appendix B.7, our VLN pipeline is similar to [59], however, our benchmark offers greater scalability through the worldwide V-_IRL_ platform and an automated data collection pipeline, as opposed to the manual annotation of a specific region. Furthermore, our benchmark emphasizes the analysis of the _vision_ component in the VLN pipeline, as opposed to [59], which aims to enhance performance on existing VLN datasets using LLMs.\n' +
      '\n' +
      'Implementation Details.Here, we introduce the implementation details for LLaVA-1.5 [41] and PP-OCR [20] (+ GPT-3.5). For LLaVA-1.5 [41], we transform the landmark recognition task to a _multiple choice VQA_ problem, asking\n' +
      '\n' +
      'Which of the following landmarks\n' +
      '\n' +
      'can be identified with a high degree of confidence?\n' +
      '\n' +
      'The VQA options include all potential landmarks mentioned in the route description, along with a "None of above" choice. The model\'s response to this question is then parsed as the landmark observation.\n' +
      '\n' +
      'For PP-OCR [20] (+ GPT-3.5), we first extract all recognized text using PP-OCR [20] for each street view image. Then, GPT-3.5 [57] determines the presence of each landmark in this street view image, jointly considering the OCR text and landmark name.\n' +
      '\n' +
      'Full set results.Apart from the mini-set results presented in Sec. 5.4, we also provide the full set results of Oracle and CLIP (L/14@336px) in Tab. 7. The Oracle results, interestingly, do not achieve a 100% success rate, due to incorrect decisions made by the LLM at stop positions. This is evidenced by the high arrival ratio and low reaction accuracy at stop positions. Empirically, we observe that the LLM occasionally decides to keep moving, despite clear destination indications in the observations.\n' +
      '\n' +
      'When we substitute the map in oracle with the CLIP model to gather landmark observations from street view imagery, we observe a significant drop in the success rate, due to the inevitable model prediction errors. To improve the success rate in VLN, we can focus on two important factors: (\\(i\\)) designing better vision models; (\\(ii\\)) developing LLMs and prompt techniques that are robust to vision-related noise. Especially, our empirical findings suggest that sophisticated prompt designs significantly improve the robustness of LLMs to visual observation noise.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Success**} & \\multicolumn{2}{c}{**Start**} & \\multicolumn{2}{c}{**Intersection**} & \\multicolumn{2}{c}{**Stop**} \\\\ \\cline{3-7}  & & **Reac** & **Arr** & **Reac** & **Arr** & **Reac** \\\\ \\hline Oracle (No Vision) & 0.88 & 1.0 & 0.95 & 0.99 & 0.96 & 0.88 \\\\ \\hline CLIP (L/14@336px) & 0.22 & 0.84 & 0.66 & 0.90 & 0.61 & 0.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Results of V-_IRL_ VLN-full.\n' +
      '\n' +
      'Figure 19: Category-wise accuracy and numbers for V-_IRL_ Place Recognition benchmark.\n' +
      '\n' +
      'Figure 20: Top-10 and bottom-10 place types averaged on four vision models of V-_IRL_ Place VQA.\n' +
      '\n' +
      'Figure 21: Samples of _V-IRL Place_ localization using CLIP (w/ GLIP proposals).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>