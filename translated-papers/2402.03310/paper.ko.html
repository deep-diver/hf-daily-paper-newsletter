<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _V-Irl_: 실생활에서의 접지 가상 지능\n' +
      '\n' +
      '지한양({}^{1}\\) 룬유 딩({}^{1}\\) 엘리스 브라운({}^{2}\\) 샤오주안기({}^{1}\\) 염시({}^{2}\\)\n' +
      '\n' +
      '홍콩대학({}^{2}\\)뉴욕대학\n' +
      '\n' +
      '뉴욕대를 방문하는 동안 진행된 일.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '인간이 서식하는 지구와 현대 AI 에이전트가 만들어지는 디지털 영역 사이에는 감각적 만이 있다. 현실 세계 환경에서 인간처럼 감각하고 사고하고 유연하게 행동할 수 있는 AI 에이전트를 개발하기 위해서는 디지털 세계와 물리적 세계 사이의 현실주의 격차를 해소하는 것이 필수적이다. 실제 하드웨어와 제어에 의해 부과되는 제약 없이 우리가 사는 환경만큼 풍부하고 다양한 환경에서 에이전트를 어떻게 구현할 수 있는가? 이를 위해 가상이지만 현실적인 환경에서 에이전트가 실제 세계와 확장 가능하게 상호 작용할 수 있는 플랫폼인 V-IRL을 소개한다. 당사의 플랫폼은 다양한 실용적인 작업을 수행할 수 있는 에이전트를 개발하는 운동장 역할을 하며 전 세계에 걸쳐 인식, 의사 결정 및 실제 데이터와의 상호 작용에 걸친 능력의 진전을 측정하는 방대한 테스트 베드 역할을 합니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)의 출현은 기본 추론에서 복잡한 계획 및 도구 사용에 이르기까지 다양한 기능을 위한 보편적인 인터페이스를 제공함으로써 자율 에이전트 연구에 새로운 생명을 불어넣었다[72]. 이러한 개발은 유망하지만 이러한 에이전트의 대부분은 텍스트 기반 환경 또는 단순 시뮬레이션에만 국한되어 있다. 기존 에이전트의 시각적 구성 요소는 시뮬레이션된 탁상 환경[11, 28]과 같은 초보적이거나 지상-진실 API[27, 67]를 사용하여 추상화된 표현에 의존한다. 또한, 이러한 에이전트에 의해 널리 사용되는 시각적 모델은 실제 장면의 예측 불가능성과 다양성을 포착하지 못하는 사진 생성, 객체 중심 인터넷 이미지에 대해 훈련된다.\n' +
      '\n' +
      '본 논문은 AI 에이전트와 감각 세계의 격차를 해소하는 데 목적이 있으며, AI 에이전트를 풍부하고 실제 환경에서 접지함으로써 현실 세계에서 효과적으로 작동할 수 있는 자율 에이전트를 개발하는 데 중요한 단계입니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '대화형 구현 환경과 센서가 풍부한 시각적 입력 덕분에 프로젝트를 수행할 수 있습니다. _ 홍콩의 조종사 동안 RX-399는 8개의 쓰레기통을 위치시켜 5개를 정확하게 식별하지만 1개는 간과한다. 뉴욕에서는 5개의 쓰레기통을 모두 정확하게 감지하지만 실수로 2개의 우편함을 보고합니다._\n' +
      '\n' +
      'RX-399는 사전 검출들 중 중복을 체크하기 위해 특징 매칭을 사용함으로써 이전에 관찰된 객체들을 이중 카운팅하는 것을 피할 수 있다(도 5 참조).\n' +
      '\n' +
      'NYC 공원 및 레크리에이션 부서와의 프로젝트를 위해 뉴욕 센트럴 파크에 있는 쓰레기통, 소화전, 공원 벤치의 분포를 분석할 필요가 있다.__Imani는 뉴욕 센트럴 파크에 있는 쓰레기통, 소화전, 공원 벤치의 분포를 분석할 필요가 있다.__Imani는 뉴욕 센트럴 파크에 있는 쓰레기통, 소화전, 공원 벤치의 분포를 분석할 필요가 있다.\n' +
      '\n' +
      '이마니는 센트럴 파크에 걸쳐 있는 경로와 경로를 탐색하고 탐지된 모든 인스턴스를 기록하는 RX-399의 관심 객체를 설정한다. RX-399가 경로를 마친 후, 이마니는 수집된 데이터를 다양한 세부 수준에서 분석한다. 도 1에 도시된 바와 같다. 도 3을 참조하면, 가장 조밀한 수준은 공원 내 쓰레기통, 소화전, 벤치의 일반적인 분포를 보여준다. 이마니는 또한 특정 영역을 확대할 수 있으며, 여기서 더 밝은 색상은 더 고유한 인스턴스가 식별된 위치를 나타낸다. 다음 표는 RX-399의 카운팅 보고서를 제시한다:\n' +
      '\n' +
      'RX-399 내에서 지오태그된 감각 풍부 데이터를 검색함으로써, 이마니는 또한 그림 3의 하단 레벨에 의해 예시된 바와 같이 RX-399의 보고서의 신뢰성을 검증하는 데 도움이 되도록 각 객체에 대한 검출 결과를 검사할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} Category & Trash Bin & Fire Hydrant & Park Bench\\({}^{*}\\) \\\\ \\hline Count & 1059 & 727 & 1015 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: RX-399의 뉴욕 센트럴 파크에서의 계수 보고서 _ (\\({}^{*}}\\)Note: 연속 벤치들이 하나의 인스턴스로 카운트됨)._\n' +
      '\n' +
      '그림 4: RX-399의 시스템 레코드 포션들이 HK와 NYC에 있다.\n' +
      '\n' +
      '그림 5: RX-399는 특징 매칭을 사용하여 상이한 시점에 걸쳐 중복을 식별함으로써 쓰레기통을 이중 계수하는 것을 회피한다.\n' +
      '\n' +
      '그림 3: RX-399에 의해 수집된 데이터를 사용하여 뉴욕 센트럴 파크의 쓰레기통, 소화전, 공원 벤치에 대한 이마니의 시각화.\n' +
      '\n' +
      '히로는 홍콩에서 새로운 여행을 시작하고 있다. 그는 특별한 목적지를 염두에 두지 않고, 너무 맵지 않은 음식이 있는 좋은 지역 점심자리를 찾기로 결심한다._\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 6에 도시된 바와 같이, 히로는 (\\frac{\\text{\\text{\\text@underline{\\char 10}}}{\\text{\\text@underline{\\text{char 10}}}}에서 시작하여, 거리를 걷고 첫 번째 교차로를 만난다. 상호 작용적이고 감각적인 풍부한 환경 덕분에, 그는 각각의 가능한 경로에 대한 거리 뷰를 가져오기 위해 그의 포즈를 조정할 수 있다. 이러한 견해에서 VQA를 사용하여 그는 좌회전하기로 결정한다:\n' +
      '\n' +
      '왼쪽 도로에 있는 건물들은 아늑하고 가족이 운영하는 지역 음식을 나타냅니다. 다른 건물들보다 더 나은 선택입니다!__\n' +
      '\n' +
      '그런 다음 한 블록을 탐색한 후 주위를 둘러보고 우회전하기로 결정하는 두 번째 교차로를 만난다.\n' +
      '\n' +
      '이러한 방식으로 약간의 로컬 푸드 스팟이 있는 것 같다...__\n' +
      '\n' +
      '히로는 몇 단계 후에 자신의 오픈 월드 검출기를 사용하여 "A One Chinese Noodles[\\(\\frac{\\text{\\text{\\text@underline{\\text\\char10}}}{\\text{\\text@underline{\\text{\\char10}}}}}{\\text{\\text{\\text@underline{\\text\\char10}}}}}}]]--[\\(\\frac{\\text{\\text@underline{\\text\\char10}}}}}{\\text{\\text{\\text@underline{\\text\\char10}}}}}}}}}}]"을 찾는다. 그는 우리의 플랫폼을 사용하여 레스토랑에 대한 정보, 평점 및 리뷰를 검색하며, 이는 거리 뷰를 장소_로 연결한다. 히로는 결국 그것을 물려주기로 결심하고 계속 탐구한다.\n' +
      '\n' +
      '\'매콤한\' 돼지갈비 국수에 대한 리뷰가 가장 많은 리뷰에 나와 있어요\n' +
      '\n' +
      '마지막으로, 블록(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}{\\text{\\text@underline{\\text{\\char 10}}}})의 끝에서 히로는 "_Xintianfa[\\(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}{\\text{\\text{\\text@underline{\\text{\\char 10}}}}}}}}_Xintianfa[\\(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}}}}}}}_Xintianfa[\\(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}}}}}}}}}}_Xintianfa[\\(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}}}}}}}}}}}}_Xintianfa[\\(\\frac{\\text{\\text{\\text@underline{\\text{\\char 10}}}}}}}}}}}}}}}} 그는 정통 요리와 다양한 메뉴를 칭찬하는 수많은 온라인 리뷰를 읽은 후 그곳에서 식사를 하기로 결심합니다.\n' +
      '\n' +
      '### Collaborative Agents\n' +
      '\n' +
      '인간은 복잡한 현실 세계의 과제를 해결하기 위해 종종 함께 일한다. 이 협업은 복잡한 작업을 보다 단순한 하위 작업으로 분해하여 각 작업을 해당 도메인의 전문가가 처리할 수 있도록 함으로써 효율성과 효율성을 촉진한다. V-_IRL_ 에이전트는 우리의 플랫폼을 통해 전 세계에 기반을 둔 공간 데이터와 거리 뷰 이미지를 활용하여 다른 에이전트뿐만 아니라 인간 사용자와 협력할 수 있다.\n' +
      '\n' +
      '1 : 에이전트-에이전트 협업\n' +
      '\n' +
      '이전 에이전트와 마찬가지로 협업 에이전트는 특정 작업을 위해 설계되지만, 서로 협업을 통해 전문성을 넘어서는 목표를 처리할 수 있습니다.\n' +
      '\n' +
      '링은 전 세계 도시들을 여행한다. 그녀는 진정한 경험을 추구하며 잃어버린 자신을 발견할 때마다 항상 지역 주민들에게 도움을 청하는 것을 두려워하지 않는다.\n' +
      '\n' +
      '로컬에서 경로 설명을 얻은 후 링은 그림 7과 같이 여행을 시작한다. 구현된 플랫폼에 기반을 둔 링은 자세를 조정하고 열린 세계 인식과 지도를 사용하여 거리를 따라 있는 시각적 랜드마크를 식별할 수 있다. 이러한 랜드마크를 올바르게 인식하면 그림 7의 상위 두 뉴욕시 사례에서 볼 수 있듯이 GPT-4가 방향을 바꾸고 앞으로 가고 멈추는 위치에 대한 올바른 결정을 내리는 데 도움이 된다. GPT-4가 내린 이러한 결정의 성공은 시각적 접지를 위한 실제 감각 입력과 V-_IRL_의 대화형 환경에 의존한다.\n' +
      '\n' +
      '그럼에도 불구하고 링은 때때로 목적지를 찾지 못할 수도 있다. 그림의 왼쪽 하단 샌프란시스코 예제. 도 7에 도시된 바와 같이, 링은 그녀의 관점에서 오직 스테인리스 벽만이 보이기 때문에 애플 스토어를 지나간다. 오른쪽 하단의 홍콩 사례에서 링은 다른 레스토랑을 목적지로 착각하고 섣불리 멈춘다. 다행히도, 그녀가 이런 실수를 할 때, 링은 다른 지역 요원에게 새로운 길을 묻고 또 다른 내비게이션을 시작할 수 있고, 이것은 결국 그녀를 목적지로 이끈다.\n' +
      '\n' +
      '그림 6: HK에서 히로의 점심 탐사를 위한 시각화.\n' +
      '\n' +
      '그림 7: 링 및 로컬 협업 사례. 빨강과 초록의 궤적은 링의 첫 번째와 두 번째 시도를 각각 의미한다.\n' +
      '\n' +
      '###### 3.4.2 인간-에이전트 협업\n' +
      '\n' +
      '인간이 서식하는 동일한 환경에서 V-_IRL_ 에이전트는 실제 인간 사용자와 협력하고 지원할 수 있다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 8에 도시된 바와 같이, 디에고의 여행 일정은 _your_(사용자의) 니즈에 맞춰져 있다. 디에고는 신체 및 정신적 차단 상태, 각 활동에 대한 예산을 고려할 뿐만 아니라 각 이벤트를 따를 때 상태 변경 및 비용을 예상합니다. 그는 V-_IRL_ 플랫폼으로부터 _real_ 여행 시간을 고려하고 다른 추천 에이전트와 _collaborating_에 의해 적합한 목적지를 선택할 수 있다.\n' +
      '\n' +
      '이와는 대조적으로, 도. 도 10은 보다 단순한 "어라운드된" LLM-전용 컨시어지 에이전트가 V-_IRL_에 대한 액세스 없이 위치들 사이의 실제 거리 및 이동 시간을 고려할 수 없어 비실용적인 여행 일정을 초래한다는 것을 보여준다. 예를 들어, _real_ 지리공간 정보가 결여된 경우, 비접지 컨시어지는 실제로 _60-100분_*가 필요한 브롱크스에서 "브루클린 식물원"과 "웨이브 힐" 간의 이동을 위해 _30분_만을 할당한다. 환각된 여행 시간은 지리공간적 현실을 간과하고 지나치게 먼 목적지가 있는 계획으로 귀결된다.\n' +
      '\n' +
      '각주*: (구글 맵[https://maps.app.goo.gl/SW1r5GSx3ZVo7BIr7](https://maps.app.goo.gl/SW1r5GSx3ZVo7BIr7])\n' +
      '\n' +
      '또한, 도 1에 도시된 바와 같다. 11, 디에고스에 개입할 수 있다\n' +
      '\n' +
      '그림 8: _The Perfect Day Itinerary_: Diego가 만든, 우리의 반복적인 컨시어지 에이전트, 이 일정은 당신의 날이 전개될 때 당신의 정신적, 육체적 안녕과 예산 변동을 고려하여 꼼꼼하게 조정된다.\n' +
      '\n' +
      '그림 10: 근거 없는 LLM 전용 컨시어지 에이전트의 여행 일정.\n' +
      '\n' +
      '그림 9: 디에고는 여행 일정에 추가할 명승지를 찾기 위해 관심 지역을 횡단합니다.\n' +
      '\n' +
      '감수성 상태를 조정하거나 언어적 피드백을 제공하여 계획을 짜는 과정. 이에 대해 디에고는 당신의 요구를 수용하기 위해 그의 원래 계획을 즉시 수정하고, 그의 수정 이후 당신의 상태 변화를 재추정한다.\n' +
      '\n' +
      '마지막으로 V-_IRL_의 거리 뷰와 지도를 사용하여 디에고는 그림 9와 같이 방문할 수 있는 잠재적인 경관 뷰에 대한 관심 영역을 횡단할 수 있다. 그는 VQA를 사용하여 캡처된 각 뷰의 평가 및 평가를 수행하고 여행 일정에 최고 등급의 위치를 추가한다.\n' +
      '\n' +
      '##4 시스템 기본\n' +
      '\n' +
      '이 섹션에서는 실제 세계의 도시를 실제 작업을 해결하기 위해 에이전트를 구성할 수 있는 광대한 가상 놀이터로 변환하는 인식 기반 에이전트를 위해 설계된 플랫폼인 우리 시스템의 핵심을 소개한다. 그 핵심에서, V-_IRL_은 계층적 아키텍처로 구성된다(도 12 참조). 플랫폼_은 에이전트가 사용할 기본 구성 요소 및 인프라를 제공하는 기반에 있습니다. [] 인식, [] 추론, [] 액션 및 [] 협업의 상위 수준 _capabilities_는 플랫폼의 구성 요소에서 나타납니다. 마지막으로, _agents_는 태스크별 루틴에서 이러한 기능과 사용자 정의 메타데이터를 활용하여 태스크를 해결한다.\n' +
      '\n' +
      '### Agent Definition\n' +
      '\n' +
      '이 시스템에서 에이전트 행동은 배경, 의도한 목표 및 감수성 상태를 포함한 사용자 정의 메타데이터에 의해 형성된다. _background_는 에이전트를 현실 세계(위치)에서 인스턴스화하고, 그 추론 및 의사 결정(전술)을 안내하는데 필요한 컨텍스트를 제공한다. 의도_환경 내에서 에이전트의 목적을 개요화합니다. 에이전트의 _interoceptive state_는 시간이 지남에 따라 변화하고 행동에 영향을 미치는 내부 정신적, 신체적 상태를 반영한다. 이 새로운 개념은 인간과의 협업을 향상시키기 위해 AI 에이전트에게 중요하다(3.4.2절 참조).\n' +
      '\n' +
      '구체적으로, 에이전트는 태스크를 해결하기 위해 플랫폼의 다양한 구성 요소와 에이전트의 메타데이터를 활용하는 태스크별 실행() 루틴을 작성함으로써 개발된다.\n' +
      '\n' +
      '### Platform Components\n' +
      '\n' +
      '다음으로, 실제 환경에서 기능을 인스턴스화하고 에이전트 작업 및 지상 에이전트를 실행할 수 있는 인프라를 제공하는 플랫폼 구성 요소에 대해 조사한다.\n' +
      '\n' +
      '####4.2.1 환경(Action)\n' +
      '\n' +
      '환경 구성 요소는 주변 세계의 접지 에이전트를 담당합니다: 실제 도시를 항해할 수 있는 표현을 제공합니다(3.1절 참조). 지리적 좌표는 세계와 우리의 가상 표현 사이의 연결고리 역할을 한다. 구글 맵 플랫폼(GMP)[24]을 활용하면 V-_IRL_는 에이전트가 거리 뷰 이미지에 액세스하고 유효한 움직임을 쿼리하며 주변 위치에 대한 정보를 검색하고 경로를 계획할 수 있습니다. 이러한 좌표 및 위치 정보가 실제 세계에 바인딩되므로 부동산 API와 같은 지리적 위치를 활용하는 외부 도구와의 자연스러운 인터페이스도 제공합니다. 환경의 기술적 설계는 부록 C에 자세히 설명되어 있다.\n' +
      '\n' +
      '####4.2.2 시각(지각)\n' +
      '\n' +
      '지각 구성 요소는 에이전트가 환경, 특히 거리 뷰 이미지에 의해 제공되는 감각 풍부한 데이터를 처리할 수 있게 한다. 미리 훈련된 위치 추정 모델[37]은 에이전트에게 환경에 대한 정확한 공간 이해를 제공한다. 이를 통해 RX-399는 객체의 인스턴스를 식별하고 카운트할 수 있으며, 히로는 특정 비즈니스를 선택하여 GMP(Sec. 3.3)를 조회할 수 있다. 로컬라이제이션 모델은 지각 입력과의 정확한 상호작용을 허용하지만, 오픈 월드 인식 모델[51]은 보다 일반적이며, 에이전트가 자신의 시야에서 더 넓은 범위의 객체를 검출할 수 있게 한다(_예를 들어, 관광자가 애플 스토어를 검색함). 미리 훈련된 특징 매칭 모델[40]은 동일한 위치의 뷰에 걸쳐 연속성에 대한 이해를 제공하고, 에이전트가 상이한 뷰포인트로부터 동일한 객체의 인스턴스를 식별 및 중복제거할 수 있게 한다(Sec. 3.3). VQA & Captioning 기능을 가진 멀티모달 모델[36]은 지각 세계를 자연어로 연결하고 추론과의 통합에 필수적이다.\n' +
      '\n' +
      '그림 11: 디에고는 사용자의 개입에 맞게 원래 계획을 조정한다.\n' +
      '\n' +
      '도 12: Sec. 4에 기술된 계층적 V-_IRL_ 아키텍처.\n' +
      '\n' +
      '####4.2.3 언어(추론 & 협업)\n' +
      '\n' +
      '추론 구성요소는 인식과 환경의 정보를 기반으로 의사 결정을 허용한다. GPT-4 [2] 및 Llama 2 [66]과 같은LLM은 다양한 API(Sec. 3.2)에 걸쳐 인터페이스하여 환경 데이터와 지각적 출력을 실행 가능한 통찰력으로 변환한다. 또한 [2]를 활성화합니다. 자연어를 통한 에이전트 간 또는 인간과의 협업(Sec. 3.4) 사용자 지정 프롬프트는 이러한 상호 작용을 용이하게 한다(Sec. 4.4 참조).\n' +
      '\n' +
      '### V-_lr_ Capabilities\n' +
      '\n' +
      '플랫폼의 구성 요소를 유연하게 결합하여 방대한 기능을 발휘할 수 있습니다. 섹에서 3, 우리는 플랫폼의 더 많은 구성 요소를 필요로 하는 점점 더 복잡한 행동을 나타내는 에이전트를 제시한다. 루트 옵티마이저(Sec. 3.1)와 같은 단순한 조합에서 관광객(Sec. 3.4.1)과 같은 보다 복잡한 배열에 이르기까지 본 시스템은 V-_IRL_ 플랫폼이 다양한 실제 시나리오에 적용될 수 있는 범용성과 잠재력을 보여준다. 다음으로, V-_IRL_의 컴포넌트들이 어떻게 결합되어 우리의 가장 복잡한 에이전트를 생성하는지에 대한 높은 레벨의 사례 연구를 수행한다; 부록 D에서, 우리는 V-_IRL_ 에이전트를 생성하는 것을 뒷받침하는 낮은 레벨의 플랫폼 세부사항들을 더 깊이 파헤친다.\n' +
      '\n' +
      '### 고수준 시스템 사례연구: 인터랙티브 컨시어지 "디에고"\n' +
      '\n' +
      '디에고(Sec. 3.4.2)를 연구함으로써, 우리는 플랫폼의 구성 요소가 어떻게 결합되어 복잡한 에이전트를 생성하는지 설명한다.\n' +
      '\n' +
      '디에고가 여행 일정 개발에 능숙한 배경에는 그의 반복적인 계획 파이프라인이 있다(도 13에 묘사됨). 프로세스는 디에고가 작업 기억에서 사용자의 전기, 요구 사항 및 이전 활동을 고려하여 GPT-4를 사용하여 첫 번째 활동에 대한 초기 초안 계획을 생성하는 것으로 시작한다. 그런 다음 이 초안은 세심하게 정제됩니다. 먼저, 계층적 조정 모듈은 실제 운송 시간을 검색하고 추천 에이전트에 식사 추천을 요청한다. 이어서, 감수성 추정 모듈은 제안된 활동이 사용자의 정신적/신체적 상태 및 예산에 미치는 영향을 평가한다.\n' +
      '\n' +
      '중요한 최종 단계는 감독자 모듈을 포함하며, 감독자 모듈은 현재 사용자 상태, 잔여 예산 및 잠재적인 상호 작용에 비추어 들어오는 활동을 검토("감사")한다(도 11에서 예시). 관리자가 해당 계획이 부적합하다고 판단하면 수정 작업을 시작합니다. 그런 다음 수정된 계획은 신뢰성을 위해 계층적 코디네이터 및 감수성 추정기로 다시 루프된 다음 감독자의 또 다른 검토가 뒤따른다(도 13의 수정 루프 참조). 계층적 조정자, 감수성 추정자 및 감독자 사이의 이러한 반복 과정은 감독자가 활동을 승인하고 작업 기억에 추가할 때까지 계속된다.\n' +
      '\n' +
      '활동을 종료한 후 디에고는 당일 일정이 완료될 때까지 이 과정을 반복하여 후속 활동을 계획하는 것으로 진행한다.\n' +
      '\n' +
      '##5V-_lrl_Benchmarks\n' +
      '\n' +
      '이전 섹션에서는 V-_IRL_ 플랫폼의 주요 이점인 1인칭 거리뷰 이미지에 대한 원활한 액세스와 전 세계 실제 도시에 대한 설명 정보를 설명한다. 이 _truly open-world_ 데이터의 _scalable_ 소스는 핵심 컴포넌트 모델 및 에이전트 능력을 테스트하기 위해 이용될 수 있다. 본 논문에서는 오픈 월드 비전 태스크에 대한 비젼 언어 모델 평가(Secs. 5.2와 5.3)와 엔드 투 엔드 에이전트 성능 평가(Secs. 5.4)의 3가지 V-_IRL_ 벤치마크를 제안한다. 벤치마크 세부 정보는 부록 E에 있습니다.\n' +
      '\n' +
      '### 자동 데이터 및 주석 수집\n' +
      '\n' +
      'V-_IRL_ 벤치마크를 전 세계적으로 확장할 수 있도록 제한된 데이터를 크롤링하고 수동으로 주석을 달지 않고 자동 데이터/애너테이션 구축 파이프라인을 개발한다. 이를 통해 구글 스트리트 뷰(24)에 액세스할 수 있는 경우 전 세계적으로 모델을 편리하게 테스트할 수 있습니다.\n' +
      '\n' +
      '**지역 선정.** 우리의 벤치마크는 GMP가 다루는 모든 지역에 걸쳐 실현 가능하지만, 우리는 6개 대륙의 12개 도시에 걸쳐 14개 지구를 선택하여 다양한 범위의 보장을 보장합니다.\n' +
      '\n' +
      '도 13: 대화형 컨시어지 에이전트 디에고의 아키텍처 개요(Sec. 3.4.2). Sec. 4.4의 파이프라인 설명을 참조하십시오.\n' +
      '\n' +
      '추론 비용을 저렴하게 유지하면서 데이터 배포 이 지역의 자세한 위치는 탭 2에 나열되어 있습니다.\n' +
      '\n' +
      '**Place Types.** GMP+로 주석이 달린 96개 장소 유형 모두에 대해 각 지역의 장소 정보를 수집한다. 우리의 V-_IRL_ 장소: 현지화, 인식 및 VQA 벤치마크는 이러한 장소 유형의 전부 또는 일부에 기반한다.\n' +
      '\n' +
      '각주 †: [https://developers.google.com/maps/documentation/places/web-service/supported_types/#table1](https://developers.google.com/maps/documentation/places/web-service/supported_types/#table1)\n' +
      '\n' +
      '**비전 및 장소 데이터 수집.** 각 지역 내에서 사용 가능한 거리 뷰, 장소 정보 및 장소 중심 이미지가 있는 지리적 위치를 수집합니다. **데이터 정리** 확장성이 있지만 자동화된 데이터 수집은 사람의 감독 부재로 인해 노이즈를 유발할 수 있다. 이를 위해 거리뷰에서 쉽게 보이지 않는 곳을 제외하기 위한 \\(i)\\)_거리 기반 필터링_와 더 이상 유효하거나 관련성이 없을 수 있는 리뷰가 없는 "좀비" 장소를 제거하기 위한 \\(ii)\\)_인간-재검토 필터링_와 저장 가능성이 높은 CLIP 중심 이미지_만을 유지하기 위한 \\(iii)\\)_CLIP 기반 필터링_의 세 가지 자동 데이터 클리닝 전략을 설계한다.\n' +
      '\n' +
      '### V-_lrl_ Place: Localization\n' +
      '\n' +
      '매일 인간은 의도적인 탐험가 에이전트(Sec. 3.3)처럼 다양한 장소 사이를 이동하면서 도시를 횡단한다. 우리는 거리뷰 이미지 및 관련 장소 데이터를 사용하여 장소 로컬라이제이션의 일상적인 인간 활동에 대한 비전 모델의 성능을 평가한다.\n' +
      '\n' +
      '**Setup.** 우리는 RX-399(Sec. 3.3)를 수정하여 다각형 영역을 횡단하면서 20가지 유형의 장소를 로컬화 및 식별한다. 우리는 14개 구역에서 28개의 다각형 영역을 서브샘플링한다.\n' +
      '\n' +
      '**Benchmarked Models.** 우리는 GroundingDINO[43], GLIP[37] 및 Owl-ViT[47]의 세 가지 두드러진 개방형 탐지 모델을 평가한다. 또한 GLIP 제안의 범주를 CLIP[51]로 재분류하는 간단한 기준선 CLIP(w/GLIP 제안)를 구현한다.\n' +
      '\n' +
      '평가.** 현지화 재현율(localization recall)을 기반으로 모델을 평가하는데, 이 모델은 \\(\\frac{N_{\\text{tp}}}{N_{\\text{tp}}+N_{\\text{fn}}\\)으로 정량화되며, 여기서 \\(N_{\\text{tp}}\\)과 \\(N_{\\text{fn}\\)은 각각 정확하게 현지화된 장소와 누락된 장소의 수를 나타낸다.\n' +
      '\n' +
      '**객체 제안과 장소 간의 일치.** Sec. 5.1에서 언급했듯이 각 잠재적 거리 뷰 이미지의 장소에 대한 경계 상자에 주석을 달지 않습니다. 이러한 인간 주석은 플러그 앤 플레이 및 센서가 풍부한(V-_IRL_) 벤치마크를 제공하는 초기 동기에서 분기한다. 이 시나리오에서 각 객체 제안에 대한 지면 진리를 할당하기 위해 거리 뷰 객체 검출에서 객체 제안을 주변 장소에 할당하는 간단한 매칭 전략을 개발한다.\n' +
      '\n' +
      '도 1에 도시된 바와 같다. 도 14에 도시된 바와 같이, 먼저 각 객체 제안의 바운딩 박스를 반경의 대상이 되는 3D 공간의 절두체에 투영한다. 그런 다음 주변 장소가 이 절두체와 반경 내에 있는지 확인합니다. 근처에 있는 임의의 장소가 발견되면, 가장 가까운 장소가 객체 제안에 대한 _ground truth_로 할당된다. 그렇지 않으면, 객체 제안은 _false positive_로 간주된다. 여러 장소가 절두막 안에 있을 때, 우리는 가장 가까운 곳을 진실로 간주한다. 왜냐하면 그것은 이미지의 다른 곳들을 차단할 가능성이 있기 때문이다. _ 이 과정은 의도적인 익스플로러 에이전트 히로에서도 사용되어 이미지에 대한 객체 제안을 분석하여 정보를 배치한다.__\n' +
      '\n' +
      '**결과**탭 도 3은 GroundingDINO[43], Owl-ViT[47] 및 GLIP[37]과 같은 오픈 월드 디텍터가 각각 학교, 카페 및 편의점과 같은 특정 장소 유형에 편향되어 있음을 보여준다. 대조적으로, CLIP(w/GLIP 제안)는 더 넓은 범위의 장소 유형을 식별할 수 있다. 이는 어휘가 제한된 객체 검출 데이터셋에서의 카테고리 편향에 의해 주로 발생한다. 따라서, Owl-ViT와 같은 검출기가 CLIP로 초기화되더라도, 이들의 어휘 공간은 미세 조정으로 인해 좁아진다. 이러한 결과는 제로 샷 인식기에 대한 캐스케이딩 카테고리 진단 객체 제안이 "실제" 오픈 월드 로컬화에 유망한 것으로 보이며, 특히 객체 검출 데이터 세트에서 덜 일반적인 카테고리에 대해 유망하다는 것을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Continent** & **City** & **District** \\\\ \\hline Africa & Johannesburg & Rosebank \\\\  & Lagos & Surulere \\\\ \\hline \\multirow{4}{*}{Asia} & Mumbai & Khar \\\\  & New Delhi & Lajpat Nagar \\\\  & Hong Kong & Prince Edward \\\\  & Tokyo & Shinjuku \\\\ \\hline Australia & Melbourne & CBD \\\\  & Melbourne & SouthBank \\\\ \\hline Europe & Milan & Brera \\\\  & London & Oxford St \\\\ \\hline \\multirow{4}{*}{North America} & New York City & Chinatown, Manhattan \\\\  & New York City & SoHo, Manhattan \\\\ \\cline{1-1}  & San Francisco & Union Square \\\\ \\hline South America & Buenos Aires & Monserrat \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 글로벌 V-_IRL_ 벤치마크에 대한 영역 리스트.\n' +
      '\n' +
      '도 14: 2D 오브젝트 제안과 거리 장소 간의 매칭.\n' +
      '\n' +
      '### V-_lr_ Place: 인식 및 VQA\n' +
      '\n' +
      '스트리트 뷰 이미지만을 이용한 도전적인 V-_IRL_ 장소 현지화 작업과는 대조적으로, 실제 생활에서 인간은 더 가까이 있는 장소 중심적인 모습을 취함으로써 비즈니스를 인식할 수 있다. 이러한 시각모델은 장소 중심 영상을 기반으로 특정 장소 유형을 인식하는 \\(i\\))와 "의도 VQA"로 명명된 시각 질문 응답(VQA)을 통해 인간의 의도를 식별하는 \\(ii\\))의 두 가지 인식 태스크에 대해 평가한다.\n' +
      '\n' +
      '**설정.** 인식을 위해 장소 중심 이미지를 사용하여 장소의 유형(96개 옵션에서)을 식별하는 데 10개의 오픈 월드 인식 모델을 평가한다(탭 4 참조). 의도 VQA를 위해 8개의 다중 모드 대형 언어 모델(MM-LLM)을 평가하여 4 옵션 객관식으로부터 실행 가능한 인간 의도를 결정한다. _V-IRL Place_ VQA 프로세스가 도에 예시되어 있다. 도 15에 도시된 바와 같이, 후보 및 참 선택들은 이미지에 대응하는 장소 유형들 및 장소 이름들이 주어진 GPT-4 [2]에 의해 생성된다.\n' +
      '\n' +
      '**장소 중심 이미지 _vs_. 스트리트 뷰 이미지.** _V-IRL Place_ 현지화 벤치마크에서 활용되는 스트리트 뷰 이미지와 대조적으로, _V-IRL Place_ 인식 및 VQA 벤치마크는 장소 중심 이미지를 사용한다. 이러한 이미지 유형의 구별을 설명하기 위해 그림 1에 예를 제시한다. 16. 도면은 구글 스트리트 뷰 데이터베이스5로부터 소싱된 스트리트 뷰 이미지들이 스트리트에서 촬영되고 다수의 빌딩들 및 가능한 폐색 객체-s/차량들을 포함하는 주변 환경의 더 넓은 뷰를 포괄하는 것을 보여준다. 대조적으로, 구글 플레이스 데이터베이스6에서 그려진 장소 중심 이미지는 도보로 촬영되고 특정 장소에 더 밀접하게 집중되며, 더 집중된 뷰를 제공한다.\n' +
      '\n' +
      '각주 6: [https://developers.google.com/maps/documentation/place/wcb-service/photos](https://developers.google.com/maps/documentation/place/wcb-service/photos)\n' +
      '\n' +
      '**평가.** 장소 인식과 VQA 작업을 모두 평가하기 위해 평균 정확도(mAcc)를 채택한다. 장소 VQA의 경우 MMBench[44]를 따라 순환 평가와 GPT 지원 답변 파싱을 수행한다.\n' +
      '\n' +
      '**결과**탭 도 4는 V-_IRL_ 인식 태스크에서 CLIP(L/14@336px)가 Eva-02-CLIP 및 SigLIP의 가장 큰 버전조차도 능가하는 것을 보여주며, CLIP를 트레이닝하는데 사용되는 고품질 데이터를 강조한다[51]. 표 하단은 BLIP2[36], InstructBLIP[17], LLaVA-1.5[41]이 의도 VQA에서 우수한 반면 다른 사람들은 어려움을 겪고 있음을 보여준다. 우리는 주목한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline  & **Model** & **\\#Param** & **mAcc (\\%)** \\\\ \\hline \\hline \\multicolumn{3}{c}{_V-IRL Place Recognition_} \\\\ CLIP [51] & V-IR/32 & 151M & 18.2 \\\\ CLIP [51] & V-IC/14 & 428M & 37.2 \\\\ CLIP [51] & V-IT-L/14@336px & 428M & 41.3 \\\\ OpenCLIP [16] & V-IR/32 & 151M & 21.2 \\\\ OpenCLIP [16] & V-IR/14 & 428M & 31.0 \\\\ Eva-02-CLIP [64] & V-IR/16 & 150M & 19.5 \\\\ Eva-02-CLIP [64] & V-IT-L/14 & 428M & 34.2 \\\\ Eva-02-CLIP [64] & V-IT-L/14@336px & 428M & 40.7 \\\\ SigLIP [81] & V-IT-B/16 & 203M & 29.5 \\\\ SigLIP [81] & V-IT-L/16@384px & 652M & 37.3 \\\\ \\hline \\hline \\multicolumn{3}{c}{_V-IRL Place VQA_} \\\\ MiniGPT-4 [83] & Vicuna-138-v0 & 14.0B & 3.9 \\\\ mPLUG-Owl [78] & LLaMA-7B & 7.2B & 5.5 \\\\ Shika [15] & Vicuna-7B & 7.2B & 10.9 \\\\ BLIP-2 [36] & FlanT5xx & 12.1B & 69.6 \\\\ InstochIP [17] & FlanT5xx & 12.0B & 68.0 \\\\ LLaVA [42] & Vicuna-13B-v1.3 & 13.4B & 23.5 \\\\ LLaVA-1.5 [41] & Vicuna-7B-v1.5 & 7.2B & 60.1 \\\\ LLaVA-1.5 [41] & Vicuna-13B-v1.5 & 13.4B & 61.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: _V-IRL Place_ 인식 및 _V-IRL Place_ VQA에 대한 벤치마크 결과. 녹색은 해상도 모델 증가를 나타내는 반면 파란색은 모델 매개변수 축소를 나타냅니다.\n' +
      '\n' +
      '그림 16: 맨 위 행: 거리 뷰 이미지의 예. 맨 아래 행: 해당 장소 중심 이미지입니다.\n' +
      '\n' +
      '도 15: _V-IRL Place_ VQA 프로세스의 예.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '##6 토론 : 윤리 및 프라이버시\n' +
      '\n' +
      '저희 플랫폼은 AI 개발을 위한 도구이자 윤리적 담론과 준비를 위한 도가니 역할을 합니다. AI가 필연적으로 사회에 통합되고 있기 때문에, 예를 들어 증강 현실 웨어러블, 공간 컴퓨팅 플랫폼 또는 도시 거리를 탐색하는 모바일 로봇을 통해 윤리적 및 개인 정보 보호 문제에 직면하고 논의하는 것이 필수적이다. 이러한 임박한 _real-time_ 시스템과 달리 V-_IRL_에 의해 액세스되는 데이터는 "스테일"이며 전처리되어 이러한 우려를 연구하기 위한 제어된 환경을 제공한다.\n' +
      '\n' +
      '특히 V-_IRL_는 기존의 쉽게 사용할 수 있는 API를 독점적으로 활용하며 이전에 액세스할 수 없었던 데이터를 캡처하거나 사용할 수 없게 만든다. 우리의 주요 거리뷰 이미지 소스인 구글 맵[24]은 얼굴과 번호판이 흐려지는 것을 포함한 주요 개인 정보 보호 조치의 대상이 된다[22]. 더욱이, V-_IRL_은 구글 맵 플랫폼 라이센스4를 준수하는데, 이는 구글의 스트리트 뷰들 [1, 14]을 또한 활용하는 주목할만한 기존 작업들과 유사하다.\n' +
      '\n' +
      '각주 4: [https://cloud.google.com/maps-platform/terms](https://cloud.google.com/maps-platform/terms)\n' +
      '\n' +
      '우리는 V-_IRL_가 편향을 연구하는 데 매우 귀중한 도구라고 믿는다. Sec. 5.5에서 논의된 바와 같이 V-_IRL_\'의 _글로벌 스케일_는 모델에 내재된 언어, 문화 및 기타 지리적 편향을 연구하기 위한 렌즈를 제공한다. 이러한 질문을 연구하기 위해 V-_IRL_을 사용함으로써, 우리는 실시간 시스템을 배치하면서 발생할 윤리적 딜레마를 그들에게 뒤처지지 않고 선제적으로 해결하는 것을 목표로 한다. 우리의 작업이 지역 사회 전반에 걸쳐 미래의 도전에 대한 사전적 논의에 박차를 가하기를 바랍니다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '본 연구에서는 디지털 세계와 물리적 세계 사이의 감각적 격차를 해소하기 위해 설계된 오픈 소스 플랫폼인 V-_IRL_를 도입하여 AI 에이전트가 가상적이면서도 현실적인 환경에서 실제 세계와 상호 작용할 수 있도록 한다. V-_IRL_을 통해 에이전트는 실제 공간 데이터와 거리뷰 이미지를 활용하여 풍부한 감각 접지 및 인식을 개발할 수 있다. 다양한 예시 에이전트를 생성하고 전 세계의 오픈 월드 시각 데이터에 대한 기초 언어 및 비전 모델의 성능을 측정하는 벤치마크를 개발함으로써 플랫폼의 범용성을 입증한다.\n' +
      '\n' +
      '이 플랫폼은 인식, 의사 결정 및 실제 데이터 상호 작용에서 AI 기능을 발전시키기 위한 새로운 길을 열어줍니다. 공간 컴퓨팅 및 로봇 시스템이 점점 더 널리 보급됨에 따라 AI 에이전트에 대한 요구와 가능성은 증가할 뿐이다. 개인 비서부터 도시 계획과 같은 실제 응용 프로그램, 시각 장애인을 위한 생활 변화 도구, V-_IRL_가 지각 기반 에이전트의 새로운 시대를 여는 데 도움이 되기를 바란다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Image geo-localization based on multiplearest neighbor feature matching usinggeneralized graphs. _TPAMI_, 2014.\n' +
      '* [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Alteschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. In _NeurIPS_, 2022.\n' +
      '* [4] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. _arXiv preprint arXiv:1807.06757_, 2018.\n' +
      '* [5] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhari, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. In _CVPR_, 2018.\n' +
      '* [6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* [7] Yuki M Asano, Christian Rupprecht, Andrew Zisserman, and Andrea Vedaldi. PASS: An imagenet replacement for self-supervised pretraining without humans. In _NeurIPS_, 2021.\n' +
      '* [8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.\n' +
      '* [9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.\n' +
      '* [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do As I Can, Not As I Say: Grounding language in robotic affordances. In _CoRL_, 2023.\n' +
      '* [12] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from rgb-d data in indoor environments. In _3DV_, 2017.\n' +
      '* [13] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, and Russ R Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. In _NeurIPS_, 2020.\n' +
      '* [14] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. TOUCHDOWN: Natural language navigation and spatial reasoning in visual street environments. In _CVPR_, 2019.\n' +
      '* [15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm\'s referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.\n' +
      '* [16] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In _CVPR_, 2023.\n' +
      '* [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In _NeurIPS_, 2023.\n' +
      '* [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* [19] Danny Driess, Fei Xia, Mehdi S. M. Sajiadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An Embodied Multimodal Language Model. In _ICML_, 2023.\n' +
      '* [20] Y Du, C Li, R Guo, X Yin, W Liu, J Zhou, Y Bai, Z Yu, Y Yang, Q Dang, et al. PP-OCR: A practical ultra lightweight ocr system. arxiv 2020. _arXiv preprint arXiv:2009.09941_, 2020.\n' +
      '* [21] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In _CVPR_, 2021.\n' +
      '* [22] Andrea Frome, German Cheung, Ahmad Abdulkader, Marco Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, Hartmut Neven, and Luc Vincent. Large-scale privacy protection in google street view. In _ICCV_, 2009.\n' +
      '* [23] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In _ECCV_, 2022.\n' +
      '* [24] Google Map Team. Google Map Platform. [https://mapslatform.google.com/](https://mapslatform.google.com/).\n' +
      '* [25] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _ICRA_, 2017.\n' +
      '* [26] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In _ICLR_, 2023.\n' +
      '* [27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language Models As Zero-Shot Planners: Extracting actionable knowledge for embodied agents. In _ICML_, 2022.\n' +
      '* [28] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner Monologue: Embodied reasoning through planning with language models. In _CoRL_, 2022.\n' +
      '* [29] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. VoxPoser: Composable 3d value maps for robotic manipulation with language models. _arXiv preprint arXiv:2307.05973_, 2023.\n' +
      '* [30] Heinrich Kuttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktaschel. The nethack learning environment. In _NeurIPS_, 2020.\n' +
      '* [31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.\n' +
      '* [32] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In _NeurIPS_, 2023.\n' +
      '* [33] Alexander C Li, Ellis Brown, Alexei A Efros, and Deepak Pathak. Internet explorer: Targeted representation learning on the open web. In _International Conference on Machine Learning_, pages 19385-19406. PMLR, 2023.\n' +
      '* [34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _ICLR_, 2022.\n' +
      '* [35] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communative agents for" mind" exploration of large language model society. In _NeurIPS_, 2023.\n' +
      '* [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.\n' +
      '* [37] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _CVPR_, 2022.\n' +
      '\n' +
      '* [38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language model programs for embodied control. In _ICRA_, 2023.\n' +
      '* [39] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2Motion: From natural language instructions to feasible plans. _arXiv preprint arXiv:2303.12153_, 2023.\n' +
      '* [40] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. LightGlue: Local Feature Matching at Light Speed. In _ICCV_, 2023.\n' +
      '* [41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv:2310.03744_, 2023.\n' +
      '* [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.\n' +
      '* [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.\n' +
      '* [44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. MMBench: Is Your Multi-modal Model an All-around Player? _arXiv preprint arXiv:2307.06281_, 2023.\n' +
      '* [45] Zeyi Liu, Arpit Bahety, and Shuran Song. REFLECT: Summarizing robot experiences for failure explanation and correction. In _CoRL_, 2023.\n' +
      '* [46] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac Gym: High performance gpu-based physics simulation for robot learning. In _NeurIPS_, 2021.\n' +
      '* [47] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple Open-Vocabulary Object Detection with Vision Transformers. In _ECCV_, 2022.\n' +
      '* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.\n' +
      '* [49] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. _arXiv preprint arXiv:2112.09332_, 2021.\n' +
      '* [50] Joon Sung Park, Joseph O\'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative Agents: Interactive simulacra of human behavior. In _UIST_, 2023.\n' +
      '* [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [52] Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Bryan Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. GeoDE: a geographically diverse evaluation dataset for object recognition. In _NeurIPS_, 2023.\n' +
      '* [53] William A Gavrika Rojas, Suduya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The Dollar Street Dataset: Images representing the geographic and socioeconomic diversity of the world. In _NeurIPS_, 2022.\n' +
      '* [54] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In _ICCV_, 2019.\n' +
      '* [55] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In _NeurIPS_, 2023.\n' +
      '* [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatas R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.\n' +
      '* [57] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. ChatGPT: Optimizing language models for dialogue. _OpenAI blog_, 2022.\n' +
      '* [58] Raphael Schumann and Stefan Riezler. Generating landmark navigation instructions from maps as a graph-to-text problem. In _ACL_, 2020.\n' +
      '* [59] Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, and William Yang Wang. VELMA: Verbalization embodiment of llm agents for vision and language navigation in street view. _arXiv preprint arXiv:2307.06082_, 2023.\n' +
      '* [60] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander, Yu Liu, and Hongsheng Li. LMDrive: Closed-loop end-to-end driving with large language models. _arXiv preprint arXiv:2312.07488_, 2023.\n' +
      '* [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In _ACL_, 2018.\n' +
      '* [62] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. In _NeurIPS_, 2023.\n' +
      '* [63] Significant Gravitas. AutoGPT. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT), 2023.\n' +
      '* [64] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved Training Techniques for CLIP at Scale. _arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '* [65] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. YFCC100M: The new data in multimedia research. _Communications of the ACM_, 2016.\n' +
      '\n' +
      '* [66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. LLAMA 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [67] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _arXiv preprint arXiv:2305.16291_, 2023.\n' +
      '* [68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In _NeurIPS_, 2022.\n' +
      '* [69] Michael Wooldridge and Nicholas R Jennings. Intelligent Agents: Theory and practice. _The knowledge engineering review_, 1995.\n' +
      '* [70] Penghao Wu and Saining Xie. V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs. _arXiv preprint arXiv:2312.14135_, 2023.\n' +
      '* [71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. AutoGen: Enabling next-gen llm applications via multi-agent conversation framework. _arXiv preprint arXiv:2308.08155_, 2023.\n' +
      '* [72] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.\n' +
      '* [73] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson Env: Real-world perception for embodied agents. In _CVPR_, 2018.\n' +
      '* [74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. SAPIEN: A simulated part-based interactive environment. In _CVPR_, 2020.\n' +
      '* [75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. _arXiv preprint arXiv:2309.16671_, 2023.\n' +
      '* [76] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic segmentation emerges from text supervision. In _CVPR_, 2022.\n' +
      '* [77] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In _ICLR_, 2023.\n' +
      '* [78] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.\n' +
      '* [79] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. HomeRobot: Open-vocabulary mobile manipulation. In _CoRL_, 2023.\n' +
      '* [80] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.\n' +
      '* [81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. _arXiv preprint arXiv:2303.15343_, 2023.\n' +
      '* [82] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In _ECCV_, 2022.\n' +
      '* [83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. _arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '* [84] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the Minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. _arXiv preprint arXiv:2305.17144_, 2023.\n' +
      '\n' +
      '부록 개요\n' +
      '\n' +
      '이러한 보충 자료에서, 우리는 다음을 포함하는 V-_IRL_ 플랫폼에 대한 추가 세부 정보를 제공한다.\n' +
      '\n' +
      '* V-_IRL_에이전트(부록 B) 뒤의 설계;\n' +
      '* V-_IRL_ 환경(부록 C)에서의 기술적 세부사항 및 과제.\n' +
      '* 의도적 익스플로러 에이전트 히로의 저수준 사례 연구, LLM 프롬프트(부록 D)와 같은 우리 시스템의 구현 세부사항을 파고들기;\n' +
      '* V-_IRL_ 벤치마크(부록 E)에 대한 보다 상세한 설정 및 결과.\n' +
      '\n' +
      '## 부록 B V-_IRL_ 에이전트의 기술 상세\n' +
      '\n' +
      '섹에서 3, 우리의 논의는 주로 플랫폼에 의해 권한을 부여받은 V-_IRL_ 에이전트의 혁신적인 역량과 행동에 초점을 맞춘다. 가독성의 우려로 본고에서 기술적 세부 사항에 대한 심도 있는 논의를 피한다. 이 섹션에서는 각 에이전트에 대한 주요 기술 설계를 검토합니다. 공개된 코드에서 보다 포괄적인 기술 구현을 사용할 수 있습니다.\n' +
      '\n' +
      '### 펭: 경로 최적화기\n' +
      '\n' +
      '펭은 플랫폼 내에서 실제 지리적 좌표의 활용을 보여주기 위해 설계되었습니다. 펭은 실제 주소의 시퀀스를 처리함으로써, 무엇보다도 보행, 운전, 자전거 타기와 같은 다양한 교통 수단을 사용하여 그들을 횡단하기 위한 _shortest path_를 계산한다. 이 능력은 부록 C.3에 설명된 매핑 모듈에 의해 구동된다. 그 후, 펭은 부록 C.2.2에 설명된 포인트 내비게이션 절차를 채택하여 미리 결정된 경로를 따라 목적지들을 탐색하는 것으로 진행한다.\n' +
      '\n' +
      '### 아리아 : 장소 추천인\n' +
      '\n' +
      '아리아는 지리적 측면에서 LLM의 추론 능력을 향상시키기 위해 우리의 Place Info & Search 모듈(부록 C.4 참조)에서 제공하는 현실적인 장소 정보를 활용한다. 구체적으로 아리아는 펭의 적합한 장소 유형을 판단하려는 의도를 평가하고 주변의 가능한 모든 장소를 검색한다. 검색된 각 장소에 대해 아리아는 구글의 리뷰와 사용자 평점을 고려하여 장소 개요를 요약한다. 그 후, 우리는 아리아가 펭의 전기, 의도 및 요약된 장소 개요를 병합하기 위한 프롬프트를 맞춤화하고 정당화를 동반하여 각 장소를 0에서 10 사이로 평가한다.\n' +
      '\n' +
      '이러한 기술적 설계가 없다면 LLM은 너무 멀거나 영구적으로 폐쇄된 일부 장소를 추천할 수 있다. 이 문제는 LLM이 지리 공간 관계를 정확하게 이해하는 데 어려움을 겪고 종종 오래된 데이터베이스에 의존하기 때문에 발생한다.\n' +
      '\n' +
      '### Vivek : 부동산 중개업자\n' +
      '\n' +
      '비벡이 사용하는 과정은 둘 다 장소를 추천하도록 설계되었기 때문에 아리아와 유사하다. 그러나 Vivek는 지리적 좌표의 표준화된 정의와 함께 Google Maps Platform을 넘어 광범위한 사실적인 정보를 어떻게 원활하게 통합할 수 있는지 보여줌으로써 V-_IRL_ 플랫폼의 다양성을 보여준다. 이 기능을 통해 훨씬 더 정교하고 흥미로운 에이전트를 만들 수 있습니다.\n' +
      '\n' +
      '### RX-399 : 도시보조로봇\n' +
      '\n' +
      '이전의 예시적인 에이전트들과 달리, RX-399는 오픈-월드 검출 및 특징 매칭과 같은 시각적 지각 능력들을 도입한다. 그 안에는 항해와 지각이라는 두 가지 기본 시스템이 있다. 내비게이션의 관점에서, RX-399는 현재 위치로부터 미리 정의된 목적지까지 단계적으로 자동으로 내비게이션할 수 있다. 이 탐색 과정은 부록 C.2.2에서 자세히 설명되므로 여기에서 광범위하게 논의되지 않는다.\n' +
      '\n' +
      '인식 시스템의 경우 RX-399는 가로 뷰를 좌우로 90도 수평 각도로 캡처하여 인간의 시각 인식을 시뮬레이션하도록 설계되었다. 각각의 캡처된 뷰에 대해, 개방-세계 검출 프로세스가 수행된다. 우리 환경의 상호작용 기능을 활용하여 잠재적인 객체의 규모와 위치에 따라 에이전트의 자아포즈와 초점 거리를 동적으로 조정하는 _active detection_ 전략을 추가로 제안한다. 이것은 탭 6에 예시된 바와 같이 그 성능을 상당히 개선시킨다. 향후, 시각적 검색[70]과 같은 보다 진보된 접근법들이 또한 고려될 수 있다. 서로 다른 시점에 걸쳐 객체를 이중 계수하는 것을 방지하기 위한 후속 중복 제거 절차에서는 다시점 기하학 측정, 객체 추적 및 특징 매칭을 포함한 몇 가지 전략을 시도했다. 우리는 정확성과 효율성 때문에 특징 매칭을 선택한다.\n' +
      '\n' +
      '### Imani: Urban Planner\n' +
      '\n' +
      '이마니의 시지각 시스템은 RX-399의 시지각 시스템을 반영한다. 이 마니의 시지각 시스템은 RX-399의 시지각 시스템을 반영한다. 이 마니의 시지각 시스템은 RX-399의 시지각 시스템을 반영한다. 이마니는 주어진 다각형 영역에서 항로를 계획할 수 있는 능력을 가지고 있어 RX-399가 그 영역을 횡단할 수 있다. 이 기능은 "영역 탐색"으로 명명되고 부록 C.2.2에서 정교화된다. 또한, 이마니 에이전트 내에서 RX-399에 의해 수집된 데이터를 시각화하고 검증하기 위한 히트맵 시각화 도구를 개발한다(도 3 참조).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c} City & Hong Kong & New York City \\\\ \\hline w/ active detection & **0.63 / 0.83** & **0.71 / 1.00** \\\\ w/o active detection & 0.10 / 0.33 & 0.30 / 0.60 \\\\ \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 능동 검출 방식이 있거나 없는 RX-399 검출 성능. 계산은 정확성/재현율이다.\n' +
      '\n' +
      '### 히로: 의도적인 탐험가\n' +
      '\n' +
      '히로는 일상적인 인간의 과제를 해결하기 위해 지리적, 지각적, 추론 능력을 갖춘 대표적인 에이전트로, 적합한 레스토랑을 찾기 위해 무작위로 탐험한다. 이와 관련하여 부록 D의 세부 방법론과 프롬프트를 포함하여 심층 사례 연구를 제공하기 위해 별도의 섹션에 전념했다.\n' +
      '\n' +
      '### Ling: Tourist\n' +
      '\n' +
      '링사의 비전 언어 탐색 파이프라인은 [59]와 유사하며, _vision 모델_, 지도 및 LLM을 활용합니다. 각 위치에서 우리는 앞, 왼쪽 앞, 왼쪽, 왼쪽 뒤, 뒤, 오른쪽 및 오른쪽 정면에 해당하는 에이전트 주변의 8개의 거리 뷰를 캡처하는 것으로 시작한다. 비전 모델은 경로 설명에서 언급된 _identify_ 랜드마크에 이러한 거리 뷰를 사용하며, 그 다음 _랜드마크 관찰_로 언어화된다. 또한, 교차 정보는 _mover_로부터 검색되어 _intersection observation_를 공식화한다. LLM은 에이전트의 이전 작업 이력과 함께 다음 작업을 결정하는데 중요한 역할을 한다. 각 작업 후에 현재 관찰 및 작업이 에이전트의 작업 이력에 저장됩니다. 이 자동 회귀 프로세스는 에이전트가 중단하기로 결정할 때까지 계속됩니다.\n' +
      '\n' +
      '### Local Agent\n' +
      '\n' +
      '로컬 에이전트의 주요 임무는 인간 유사하고 쉽게 따를 수 있는 항법 지침을 글로벌 규모로 생성하는 것이다(3.4.1 참조). 이 작업은 내비게이션 명령어 생성[58]으로 알려져 있다. 제한된 지리적 영역에 대한 인간 주석 데이터에 의존하는 대부분의 기존 연구와 달리, 우리의 "로컬" 에이전트는 실제 장소를 고려하여 _적합한 랜드마크_를 자동으로 선택하고 전 세계 LLM을 사용하여 인간과 유사한 경로 설명을 생성한다. 놀랍게도, 맞춤형 프롬프트와 몇 가지 컨텍스트 예제에만 의존하여 훈련 데이터가 필요 없이 이를 달성합니다. 생성된 명령어들의 유효성은 "링"과의 협업을 통해 검증되었다. 우리가 아는 한, 이것은 이 분야에서 처음입니다. 쉽게 눈에 띄는 랜드마크를 선택하고 신속한 엔지니어링에 대한 방대한 기술 세부 정보가 공개되어 있습니다.\n' +
      '\n' +
      '### Diego : 대화형 컨시어지\n' +
      '\n' +
      '4.4절에서, 우리는 이미 디에고의 여행 일정의 기술적 디자인을 제시했다. 여기서, 우리는 디에고가 도 9에 도시된 바와 같이 명승지를 어떻게 찾을 수 있는지를 상세히 설명한다. "포트 트라이온 공원"과 같은 임의의 주어진 목적지에 대해, 디에고는 그 주위의 직사각형 영역을 샘플링할 것이고 그 내의 모든 탐색가능한 위치들을_traverse_할 것이다. 각 위치에서, 그는 사진_(_i.e_. 거리뷰 이미지)를 캡처할 것이다. 미리 정의된 제목, 피치 및 FOV를 사용합니다. 그런 다음 각 사진은 GPT-4(V) [2]를 사용하여 평가되며, 여기서 _설명 이유_와 함께 0에서 10 사이의 등급을 받는다.\n' +
      '\n' +
      '## 부록 C 환경 기술 세부사항\n' +
      '\n' +
      '섹 4.2.1에서는 실제 에이전트를 기반으로 하는 시스템 환경에 대한 개요를 제공합니다. 여기서는 구글 맵 플랫폼 시스템 호출을 활용하는 것 이상의 기술 설계를 조사합니다. 구체적인 구현은 공개 소스 코드에서 찾을 수 있습니다.\n' +
      '\n' +
      '#지리적 위치 및 거리뷰 이미지\n' +
      '\n' +
      'V-_IRL_의 핵심에는 거리 뷰 이미지 및 지리적 위치를 포함하여 센서가 풍부한 환경의 혁신적인 사용이 있다. 에이전트가 주변 장소와 시각 정보를 수집할 수 있습니다.\n' +
      '\n' +
      '**Geolocation.** V-_IRL_ 플랫폼의 에이전트는 전 세계 실제 도시의 가상 표현에 서식한다. 이 표현의 핵심에는 지리적 좌표(_i.e_. geolocation)가 있다. 지구 표면의 점들에 대응합니다. 각 에이전트의 초기 지리적 위치는 그림 1과 같이 "위치" 구성에 의해 지정된다. 12. 에이전트가 주변 정보(_e.g_. 거리 뷰, 장소 또는 지도)에 대한 액세스를 요구할 때마다 지리적 위치는 관련 구글 맵 API를 쿼리하는 데 중요한 매개 변수 역할을 한다.\n' +
      '\n' +
      '**스트리트 뷰 이미지.** 구글 맵 플랫폼은 지리 위치, 헤딩(0\\({}^{\\circ}\\)에서 360\\({}^{\\circ}\\)까지의 수평 각도), 피치(-90\\({}^{\\circ}\\)에서 90\\({}^{\\circ}\\)까지의 수직 각도), 및 시야(FOV, \\(20\\sim 120\\)의 다양한 주요 매개변수로 각 스트리트 뷰 이미지를 지정한다. 여기서 FOV를 조정하는 것은 단순히 이미지를 줌인하는 것이 아니라 카메라의 초점 거리를 변경하는 것과 유사하여 FOV가 낮은 값으로 감소하더라도 이미지 해상도가 높게 유지되는 것을 보장한다는 점은 주목할 만하다. 헤딩, 피치, FOV를 수정하여 자신의 자세를 조정하고 특정 영역에 집중하는 인간의 감각 과정을 시뮬레이션할 수 있다.\n' +
      '\n' +
      '**스트리트 뷰 이미지와 지리적 위치 간의 정렬.** 센서가 풍부한 플랫폼에서 기본 과제는 에이전트가 스트리트 뷰 이미지를 사용할 수 있는 지리적 위치에 위치하도록 하는 것이다. 이 문제를 해결하기 위해 "_relocate_"라는 사용자 지정 작업을 설계합니다. 구체적으로, 스트리트 뷰 이미지가 결여된 위치에서 에이전트가 초기화될 때, "재배치" 동작은 스트리트 뷰 데이터가 이용가능한 가장 가까운 실행가능한 지리적 위치로 에이전트를 식별하고 전이할 것이다. 이 작업은 가능한 모든 좌표의 방대한 연속 공간에 비해 거리 뷰가 있는 위치가 상대적으로 희박하기 때문에 플랫폼에 필수적입니다.\n' +
      '\n' +
      '### Movement\n' +
      '\n' +
      '에이전트가 도시 거리를 따라 이동할 수 있도록 하는 것은 플랫폼의 핵심 기능이며, 에이전트와 실제 세계 간의 상호 작용을 허용합니다. 에이전트가 이동해야 할 때마다 이 모듈은 경로 계획 및 방향 선택에서 이동 중 에이전트의 지리적 위치의 지속적인 업데이트에 이르기까지 모든 관련 프로세스에 전원을 공급합니다. 구글 맵 플랫폼은 주변 항해 가능한 위치와 방향에 접근할 수 있는 API를 제공하지 않기 때문에 이 이동 모듈의 설계는 중요한 기술적 도전이자 우리 팀의 상당한 기여이다. 우리는 부록 C.2.1에서 낮은 수준의 구현과 부록 C.2.2에서 활성화된 높은 수준의 작업에 대해 논의한다.\n' +
      '\n' +
      '#### c.2.1 Mover\n' +
      '\n' +
      '**웹 인터페이스를 제어하여 이동.** 간단한 해결책은 에이전트가 웹 프론트엔드 구글 스트리트 뷰를 제어하여 이동 방향을 선택하고 이동하도록 하는 것이다. 그럼에도 불구하고, 이 해결책에는 세 가지 핵심 과제가 있다:\n' +
      '\n' +
      '(i)\\)_파이썬 구현 에이전트가 웹페이지에 대한 상호작용을 통해 움직임을 어떻게 제어할 수 있는가?____(i)\\) 우리는 이동을 담당하는 웹 요소를 찾기 위해 파이썬 패키지 셀레늄**를 사용합니다. 이동 방향을 결정한 후, 에이전트는 셀레늄을 사용하여 선택된 방향에 대응하는 웹 요소에 대한 클릭 동작을 시뮬레이션한다.\n' +
      '\n' +
      '에이전트는 이동 방향을 결정하기 위해 필요한 정보를 어떻게 획득할 수 있는가?___\\((ii)\\) 에이전트는 웹 요소로부터 모든 잠재적인 이동 방향에 액세스할 수 있지만 각각이 나타내는 것에 대한 사전 지식 없이는 이러한 방향을 식별할 수 없다. 우리는 각 방향에 해당하는 웹 요소의 "변환" 속성을 활용하여 표현된 헤딩 각도를 계산할 수 있음을 발견했다. 또한 헤딩 각도를 통해 각 이동 방향에 대한 거리 뷰 이미지를 수집할 수 있습니다. 그런 다음 에이전트의 이동 의사 결정은 이러한 헤딩 각도와 거리 뷰 이미지의 시각적 데이터를 기반으로 한다.\n' +
      '\n' +
      '(iii)\\)_이동에 따른 에이전트의 위치추적을 어떻게 추적할 것인가?____________________ 이를 위해 웹 페이지 요소를 사용자 정의하여 현재 거리뷰 파노라마의 지리적 위치를 표시한다. 에이전트가 이동하여 스트리트 뷰 파노라마로 업데이트를 트리거할 때, 이 사용자 지정된 요소는 새로운 지리적 위치를 반영하도록 동시에 새로 고쳐집니다. 셀레늄을 사용하면 이 업데이트된 지리 위치 데이터를 추출할 수 있어 에이전트의 지리 위치 변경 사항을 지속적으로 추적할 수 있다.\n' +
      '\n' +
      '** 그리드 기반 재배치 방식으로 이동합니다.** 위의 웹 기반 이동자에 대한 테스트에서 중요한 한계가 나타났습니다: 웹 내장 구글 스트리트 뷰 파노라마는 탐색 가능한 방향의 하위 집합만 표시합니다. 이 제약은 에이전트의 이동성을 크게 제한하여 잠재적인 경로의 불완전한 적용 범위로 인해 에이전트가 의도한 목적지로 성공적으로 이동하는 것을 방지하는 경우가 많다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 그리드 기반의 이동 이동기를 제안한다. 이 접근법은 에이전트 근처에서 지리적 위치에 대한 그리드 검색을 수행하고 이러한 위치를 면밀히 조사하기 위해 "이전" 작업을 사용하여 탐색 가능한 위치를 식별하는 것을 포함한다. 이 방법은 탐색 가능한 위치에 대한 보다 포괄적인 관점을 제공하지만, 필요한 구글 맵 API 호출의 광범위한 수로 인해 웹 기반 접근법보다 훨씬 더 많은 시간이 소요된다.\n' +
      '\n' +
      '실제 응용에서 우리는 웹 기반 제어와 그리드 기반 재배치를 결합한 휴리스틱 전략을 설계한다. 이 하이브리드 접근법은 실제 시나리오에서 에이전트의 능력과 효율성을 최적화하여 운항 가능한 위치 데이터의 속도와 완전성 사이의 균형을 맞추는 것을 목표로 한다.\n' +
      '\n' +
      '#### c.2.2 Navigator\n' +
      '\n' +
      '여기서는 이동자-항법에 의해 구동되는 에이전트의 높은 수준의 액션을 소개합니다. 환경에서 에이전트 이동성을 가능하게 하는 데 집중하는 이동자와 달리, 여기서의 초점은 이동 방향을 결정하는 것으로 이동한다. 플랫폼에서는 사용 용도에 따라 다른 네비게이터를 네 가지 유형으로 그룹화합니다.\n' +
      '\n' +
      '(i)\\(i)\\) **포인트 네비게이터**는 단일 또는 다중 목적지(주소 또는 지리적 위치에 표시됨)를 명확하게 정의하는 네비게이션 작업을 다루도록 설계되었다. 이 네비게이터는 부록 C.3에 기술된 경로 계획 기능을 채용하여 내비게이션을 위한 일련의 주요 위치를 획득한다. 각 위치에서 에이전트는 그리디 알고리즘을 사용하여 아직 도달하지 않은 다음 키 위치를 향하는 가장 최적의 방향을 선택한다. "Peng", "RX-399" 및 "Local"과 같은 예시적인 에이전트는 이러한 유형의 네비게이터를 구현에 사용한다.\n' +
      '\n' +
      '*영역 네비게이터**는 다각형 영역 내의 모든 위치를 횡단해야 하는 "이마니" 및 "디에고"와 같은 에이전트에 맞게 조정된다. 이 네비게이터는 먼저 "이전" 작업과 결합된 그리드 검색을 사용하여 지정된 영역 내에서 모든 탐색 가능한 위치를 식별합니다. 그 후, 여행 판매원 문제를 해결하기 위해 설계된 휴리스틱 알고리즘을 채택하여 이러한 위치를 방문하기 위한 효율적인 주문을 계획한다. 에이전트의 작업은 계획된 순서대로 각각의 운행 가능한 위치를 방문하는 이 미리 결정된 경로를 단순히 따르는 것이다.\n' +
      '\n' +
      '<((iii)\\)**비전-언어 네비게이터**는 V-_IRL_비전-언어 네비게이터 벤치마크 내의 작업뿐만 아니라 관광 에이전트 "링"을 위해 특별히 개발되었다. 주요 기능은 에이전트가 네비게이션 지침에 따라 적절한 방향을 선택할 수 있도록 안내하는 것입니다. 자세한 파이프라인은 부록 B.7에 나와 있다.\n' +
      '\n' +
      '<(iv)\\)**의도 네비게이터**는 의도적인 탐험가 에이전트 "히로"에서 에이전트의 특정 의도에 부합하는 가장 적합한 방향을 선택하기 위해 사용된다. 자세한 방법론과 프롬프트는 부록 D.2에 자세히 설명되어 있다.\n' +
      '\n' +
      '### Mapping\n' +
      '\n' +
      '우리 환경의 매핑 모듈은 에이전트에게 경로 계획 및 운송 시간 추정과 같은 기능을 갖추도록 설계되었다. 이러한 기능을 용이하게 하기 위해 구글 맵 플랫폼의 \'디렉션 API\'17을 주로 활용한다. 이 API의 인터페이스의 복잡한 특성을 감안할 때, 우리의 주요 초점은 그 출력을 파싱하고 에이전트를 위한 다양한 사용자 친화적인 인터페이스에 적용하는 것이었다.\n' +
      '\n' +
      '각주 7: [https://developers.google.com/maps/documentation/directions](https://developers.google.com/maps/documentation/directions)\n' +
      '\n' +
      '### 장소 정보 및 검색\n' +
      '\n' +
      '장소 정보 및 검색 모듈은 시각적 거리 뷰 이미지를 넘어 플랫폼에 또 다른 중요한 정보 소스를 호스트하여 에이전트가 실제 "장소"와 상호 작용할 수 있도록 합니다. 유형, 이름, 위치, 이미지, 리뷰 등 장소의 다양한 속성을 제공합니다. 이 모듈에서는 주로 장소 정보 및 주변 장소 검색과 관련된 방대한 구글 맵 플랫폼 API에서 가장 적합한 기능을 이해하고 비교 및 통합하는 데 기술적 노력을 집중한다. 또한 구글 맵 플랫폼에서 유효하지 않거나 상충되는 데이터 소스를 식별하고 제거하기 위한 몇 가지 후처리 전략을 고안한다.\n' +
      '\n' +
      '이 모듈에 의해 활성화된 또 다른 필수 기능은 거리 뷰 이미지의 객체 제안과 실제 도시의 해당 장소를 연결하는 것입니다. 이 기능은 거리 뷰와 지리적 위치를 연결하여 플랫폼의 현실을 향상시키는 데 중요합니다. 또한 "히로" 에이전트와 _V-IRL Place_ 현지화 벤치마크의 평가에 전원을 공급한다. 구현은 Sec. 5.2에 자세히 설명되어 있습니다.\n' +
      '\n' +
      '## 부록 D 저수준 시스템 사례연구: 의도적 탐험가 "히로"\n' +
      '\n' +
      '이 섹션에서는 의도적 익스플로러 에이전트 "히로"(Sec. 3.3)의 낮은 수준의 구현 세부 사항을 더 자세히 조사하며, 본 시스템의 다양한 부분과 상호 작용하는 데 사용되는 프롬프트를 중심으로 설명한다. 구체적으로 사용자 정의 의도_(Appendix D.1), 적절한 도로_(Appendix D.2), 장소_(Appendix D.3), 그리고 _making action decisions_(Appendix D.4)의 네 가지 하위 부분으로 프롬프트를 제시한다. 이 네 가지 구성 요소는 히로가 초기 의도에 의해 구동되는 대화형 구현 _environment_에서 공동으로 탐색할 수 있게 한다.\n' +
      '\n' +
      '### 배치유형의 의도\n' +
      '\n' +
      '사용자 정의 에이전트 의도로부터 시작하여, 히로는 먼저 GPT-4 및 다음의 프롬프트를 사용하여 이 의도를 이행할 수 있는 장소의 유형을 결정한다:\n' +
      '\n' +
      '```\n' +
      '[역할] YouarePlaceSuggesterGPT,anexpert inrecommendingtypesofplace basedonuser-specificintions. [TaskDescription] Givenauser-specifiedintention,determinethetypeof "place" oneshouldseektofill.Yourresponseshould beinthefollowingJSONformat: {"place": "DesiredPlaceType"}[예시] Input: "Intention:<buyabbook"" Output:{"place":"bookstore"}[Input] 의도:<(agent_intention)>[Output] Yourrecommendedplacetypebasedontheuser-specendedplacetypebasedontheuser-specendedJSONformat:\n' +
      '```\n' +
      '\n' +
      '이 프롬프트를 고의로 사용\n' +
      '\n' +
      '히로는 배가 고프고 맛있는 현지 음식을 맛볼 수 있는 곳을 찾고 있다. 그는 매운 음식을 잘 먹지 못한다.\n' +
      '\n' +
      '결과를 반환하다.\n' +
      '\n' +
      '```\n' +
      '{"place":"restaurant"}.\n' +
      '```\n' +
      '\n' +
      '식별된 장소 유형(여기서는 레스토랑)을 추출하여 히로의 탐험 중 _open-world detector_에 대한 타겟 카테고리로 설정한다.\n' +
      '\n' +
      '### Road Selection\n' +
      '\n' +
      '히로는 기로에 서 있을 때마다 자신의 _multi-modal LLM_과 _GPT-4_를 사용하여 가장 잘 따라갈 길을 결정한다. 도로 선정 과정의 일차적인 목표는 히로의 의도에 부합하는 원하는 장소 유형으로 이어질 가능성이 가장 높은 도로를 파악하는 것이다. 먼저, Hiro _fetches_ the street view towards each potential road using V-_IRL_ environment. 그런 다음 다음 프롬프트를 사용하여 각 도로에 대한 캡션을 생성하기 위해 그의 _멀티-모달 LLM_(예를 들어 _Instruct-BLIP_[17] 또는 _LLaVA_[42])를 활용한다:\n' +
      '\n' +
      '```\n' +
      '[load_idx]:{load_description}\n' +
      '`및 연접되어 all_road_descriptions을 형성한다. 이러한 도로 캡션은 히로의 사용자 정의 의도와 함께 GPT-4에 공급되어 다음 프롬프트를 사용하여 가장 유망한 도로를 결정한다:\n' +
      '\n' +
      '[역할] 당신은 사용자 지정 의도에 기초하여 다수의 후보들로부터 최적의 도로를 선택하는 전문가인 PathSelectorGPT이다.\n' +
      '\n' +
      '[과제 설명] 의도가 주어지면, 이전에 이동했던 도로와 이용 가능한 후보 도로에 대한 설명이 주어지면, 교차로에서 최상의 도로를 선택한다. 응답은 다음 JSON 형식이어야 합니다. {"idx": "Selected road index", "reason": "Justification for your selection"\n' +
      '\n' +
      '[예시] "식료품점을 찾다"는 의도에 대해, 이전에 "1"로 이동했던 도로, 후보 "2: 주거 지역으로 이끌다, 3: 쇼핑 구역으로 이끌다"에 대해, 산출물은 다음과 같을 수 있다("idx": "3", "이유": "도로 3은 식료품점을 가질 가능성이 더 높은 쇼핑 구역으로 이끌다).\n' +
      '\n' +
      '[입력] 사용자 의도: <{agent_intention}> 도로 설명: {all_road_descriptions} 이전 여행 도로: 도로 {from_road_idx}\n' +
      '\n' +
      '[출력] 필요한 JSON 형식으로 선택한 도로 지수와 선택한 배경의 추론:\n' +
      '\n' +
      '다중 모달 LLM은 여러 영상을 동시에 처리할 수 없기 때문에 도로 선택을 위한 2단계 캡션 및 의사 결정 파이프라인을 설계한다. 그러나, 최근 GPT-4V의 발전으로 한 번에 하나의 프롬프트로 여러 개의 도로 영상을 이용하여 도로 선택을 수행하는 것이 가능할 수 있다. 경험적 결과는 GPT-4V가 다음 프롬프트와 함께 더 합리적인 선택을 산출함을 시사한다.\n' +
      '\n' +
      '[역할] 당신은 사용자가 지정한 의도에 따라 다중 도로 이미지에서 최적의 도로를 선택하는 전문가인 PathSelectorGPT이다.\n' +
      '\n' +
      '[작업 설명] 도로 이미지 세트가 주어지면 교차로에서 최상의 도로를 선택합니다. 답변은 다음의 JSON 형식이어야 한다: {"idx": "Selected road index(start by 0)"", "reason": "Justification for your selection"\n' +
      '\n' +
      '[입력] 사용자 의도: <{agent_intention}>\n' +
      '\n' +
      '[출력] 도로 지수와 선택 이면에 있는 추론과 함께 필요한 JSON 형식으로 답변해 주십시오:\n' +
      '\n' +
      '도 1의 제1 교차로 선택에 대한 예시적인 도로 선택 응답. 도 6은 다음과 같다:\n' +
      '\n' +
      '[]_"idx": "0", "이성": "도로 0 선택"은 덜 상업화된 환경에서 로컬 요리의 진정한 맛을 약속한다. 맵지 않은 요리와 정통한 맛을 요청할 수 있는 가족이 운영하는 식당이 있을 가능성이 높다. 이 길은 고요한 식사 분위기를 제공하여 현지인들 사이에서 보다 몰입적이고 여유로운 요리 탐방을 가능하게 한다."\n' +
      '\n' +
      '장소 리뷰 요약\n' +
      '\n' +
      '히로는 거리뷰 이미지에서 장소를 발견하면 V-_IRL_ 환경에서 해당 이름과 구글 리뷰를 검색한다. 스트리트 뷰 이미지 상의 2D 박스를 현실 세계의 구체적인 장소에 투영하는 복잡한 알고리즘이 있는데, 이는 Sec. 5.2의 "객체 제안과 장소들 사이의 매칭"에서 상세히 설명된다. 히로가 이러한 장소 리뷰들을 획득한 후, 그는 다음의 프롬프트를 사용하여 (의사 결정을 돕기 위해) 장소 개요로 요약한다:\n' +
      '\n' +
      '[역할] 당신은 요약GPT이며, 여러 리뷰를 위치의 간결한 개요로 요약하는 데 능숙합니다.\n' +
      '\n' +
      '[작업 설명] 평점과 함께 여러 리뷰가 주어지면 장소에 대한 간략한 개요를 작성합니다. 당신의 응답은 다음의 JSON 형식이어야 한다: {"요약": "간결한 설명(80 단어로 제한)"\n' +
      '\n' +
      '[실시예] "위대한 분위기는 있지만 평균적인 음식(Rating:3)"과 "장식을 좋아하면 음식이 더 나을 수 있다(Rating:3.5)"의 리뷰에 대해, 출력은 {"요약": "위대한 분위기와 장식을 자랑하지만 음식의 품질은 혼합된 리뷰를 받는다}일 수 있다.\n' +
      '\n' +
      '[입력] 리뷰: {all_reviews}\n' +
      '\n' +
      '[출력] 제공된 리뷰들에 기초하여, 규정된 JSON 포맷으로, 당신의 간결한 개요(최대 80 단어):\n' +
      '\n' +
      '### Action Decision\n' +
      '\n' +
      '식별된 장소의 개요를 얻은 후, 히로는 GPI-4 및 다음의 프롬프트를 사용하여 장소를 방문하거나 탐사를 계속하기로 결정한다:\n' +
      '\n' +
      '[역할] 당신은 ActionSelectorGPT이며, 사용자의 배경, 의도 및 장소의 개요에 기초하여 가장 적절한 액션을 선택하는 데 능숙하다.\n' +
      '\n' +
      '[작업 설명] 제공된 사용자 배경, 의도 및 장소 개요를 평가하여 목록에서 가장 적합한 작업을 선택합니다. 당신의 응답은 다음의 JSON 포맷이어야 한다: {"action": "Selected Action", "reason": "Justification for your choice".\n' +
      '\n' +
      '가능한 동작: - enter_place(): 지정된 장소를 입력한다. - 계속(): 다른 적절한 장소를 검색하는 것을 계속한다.\n' +
      '\n' +
      '[예시] 배경 "역사 유적지를 사랑함", 의도 "지역 역사를 발견함", 및 장소 개요 "이것은 200년 된 보존된 맨슨이다"에 대해, 출력은 다음과 같을 수 있다: "행위": "enter_place()", "이유": "역사 맨슨은 역사 유적지에 대한 사용자의 관심과 정렬된다."\n' +
      '\n' +
      '[입력] 사용자 배경: <{배경}> 사용자 의도: <{의도}> 장소 개요: <{place_intro}\n' +
      '\n' +
      '[출력] 당신의 선택된 행동과 규정된 JSON 형식으로 당신의 결정의 이면에 있는 근거:\n' +
      '\n' +
      '히로의 탐사는 그가 계속하기로 결정한다면 계속될 것이고, 그가 enter_place()를 선택한다면 종료될 것이다.\n' +
      '\n' +
      '## 부록 E V-_Irl_벤치마크: 상세사항\n' +
      '\n' +
      '###V-_Irl_장소 : 위치지정(상세)\n' +
      '\n' +
      '모든 범주 결과.본 논문의 페이지 한계로 인해 탭에서 10개 범주의 결과만 제시한다. 3. 여기서 그림에서 20개 범주 모두에 대한 장소 리콜을 제시한다. 18.\n' +
      '\n' +
      '예시 그림. _V-IRL Place_ 현지화 벤치마크의 이해를 돕기 위해 그림 1에서 CLIP(w/GLIP 제안)의 몇 가지 예를 제시한다. 21.\n' +
      '\n' +
      '###V-_Irl_장소 : 인식 및 VQA(상세)\n' +
      '\n' +
      '인식을 위한 장소 유형 성능.그림 19에서 벤치마킹된 10개의 비전 모델에 걸쳐 각 장소 유형에 대한 평균 정확도를 제시한다. 각 버블의 크기 및 x축 위치는 각 유형 내의 장소의 수에 대응한다. 명확한 경향이 나타난다: 정확성은 빈도와 상관관계가 있는 경향이 있다. 의류 매장, 카페와 같은 일반적인 카테고리는 더 높은 정확도를 나타내는 반면, 비전 모델은 종종 볼링장이나 모스크와 같은 드문 장소 유형과 어려움을 겪는다.\n' +
      '\n' +
      'VQA에 대한 플레이스 유형 성능.그림의 V-_IRL_ 플레이스 VQA의 플레이스 유형 성능. 20\n' +
      '\n' +
      '도 18: _V-IRL Place_ 현지화에서의 리콜\n' +
      '\n' +
      '사람의 의도적 관점에서 정확도와 빈도수의 상관관계를 검증한다. 상위 10개 카테고리는 가장 일반적인 인간 활동, 구매 및 식사와 밀접하게 정렬되어 있습니다. 이에 비해 하위 10개 장소 유형은 접하기 어렵고 모스크, 배관공, 대사관 등 보다 다양한 목적을 수행하는 장소와 관련이 있다.\n' +
      '\n' +
      '### V-_lr_ Vision-Language Navigation (상세)\n' +
      '\n' +
      '내비게이션 파이프라인.부록 B.7에서 언급했듯이 우리의 VLN 파이프라인은 [59]와 유사하지만, 우리의 벤치마크는 특정 지역의 수동 주석과 달리 전 세계 V-_IRL_ 플랫폼과 자동화된 데이터 수집 파이프라인을 통해 더 큰 확장성을 제공한다. 또한, 벤치마크는 LLM을 사용하여 기존 VLN 데이터 세트에 대한 성능을 향상시키는 것을 목표로 하는 [59]와 달리 VLN 파이프라인에서 _vision_ 구성요소의 분석을 강조한다.\n' +
      '\n' +
      '구현 상세.여기서는 LLaVA-1.5[41] 및 PP-OCR[20] (+ GPT-3.5)에 대한 구현 상세를 소개한다. LLaVA-1.5 [41]의 경우, 랜드마크 인식 태스크를 _multiple choice VQA_ 문제로 변환하여 질문한다.\n' +
      '\n' +
      '다음 랜드마크 중 어느 것\n' +
      '\n' +
      '높은 신뢰도로 식별될 수 있는가?\n' +
      '\n' +
      'VQA 옵션에는 "위의 없음" 선택과 함께 경로 설명에서 언급된 모든 잠재적 랜드마크가 포함된다. 그런 다음 이 질문에 대한 모델의 응답을 랜드마크 관찰로 구문 분석합니다.\n' +
      '\n' +
      'PP-OCR[20] (+ GPT-3.5)의 경우, 먼저 각 스트리트 뷰 이미지에 대해 PP-OCR[20]을 사용하여 인식된 모든 텍스트를 추출한다. 그런 다음 GPT-3.5[57]은 OCR 텍스트와 랜드마크 이름을 공동으로 고려하여 이 스트리트 뷰 이미지에서 각 랜드마크의 존재를 결정한다.\n' +
      '\n' +
      'Full set 결과, Sec. 5.4에 제시된 mini-set 결과와 별도로 Tab. 7에 제시된 Oracle과 CLIP(L/14@336px)의 Full set 결과도 제공한다. Oracle 결과는 흥미롭게도 정지 위치에서 LLM이 잘못된 결정을 내렸기 때문에 100% 성공률을 달성하지 못한다. 이는 정지 위치에서 높은 도달 비율과 낮은 반응 정확도로 입증된다. 경험적으로, 우리는 LLM이 관찰에서 명확한 목적지 표시에도 불구하고 때때로 계속 이동하기로 결정한다는 것을 관찰한다.\n' +
      '\n' +
      '거리뷰 영상으로부터 랜드마크 관측치를 수집하기 위해 오라클 내의 지도를 CLIP 모델로 대체할 때, 불가피한 모델 예측 오차로 인해 성공률이 크게 떨어지는 것을 관찰한다. VLN의 성공률을 향상시키기 위해, 우리는 두 가지 중요한 요소, 즉 (\\(i\\)) 더 나은 비전 모델을 설계하는 것; (\\(ii\\)) LLM을 개발하는 것 및 비전 관련 노이즈에 강인한 신속한 기술에 초점을 맞출 수 있다. 특히, 본 연구 결과는 정교한 프롬프트 디자인이 시각적 관찰 노이즈에 대한 LLM의 견고성을 상당히 향상시킨다는 것을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Method**} & \\multirow{2}{*}{**Success**} & \\multicolumn{2}{c}{**Start**} & \\multicolumn{2}{c}{**Intersection**} & \\multicolumn{2}{c}{**Stop**} \\\\ \\cline{3-7}  & & **Reac** & **Arr** & **Reac** & **Arr** & **Reac** \\\\ \\hline Oracle (No Vision) & 0.88 & 1.0 & 0.95 & 0.99 & 0.96 & 0.88 \\\\ \\hline CLIP (L/14@336px) & 0.22 & 0.84 & 0.66 & 0.90 & 0.61 & 0.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: V-_IRL_ VLN-풀의 결과.\n' +
      '\n' +
      '도 19: V-_IRL_장소 인식 벤치마크에 대한 카테고리별 정확도 및 숫자.\n' +
      '\n' +
      '그림 20: Top-10 및 bottom-10 장소 유형은 V-_IRL_ Place VQA의 4가지 비전 모델에 대해 평균을 냈다.\n' +
      '\n' +
      '도 21: CLIP(w/GLIP proposals)를 이용한 _V-IRL Place_ localization의 샘플들.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>