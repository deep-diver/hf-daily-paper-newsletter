<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting\n' +
      '\n' +
      'Jiezhi "Stephen" Yang\\({}^{1}\\) Khushi Desai\\({}^{2}\\) Charles Packer\\({}^{3}\\)\n' +
      '\n' +
      'Harshil Bhatia\\({}^{4}\\) Nicholas Rhinehart\\({}^{3}\\) Rowan McAllister\\({}^{5}\\) Joseph Gonzalez\\({}^{3}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Harvard University \\({}^{2}\\)Columbia University \\({}^{3}\\)UC Berkeley \\({}^{4}\\)Avataar.ai \\({}^{5}\\)Toyota Research Institute\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting, a method for predicting future 3D scenes given past observations, such as 2D ego-centric images. Our method maps an image to a distribution over plausible 3D latent scene configurations using a probabilistic encoder, and predicts the evolution of the hypothesized scenes through time. Our latent scene representation conditions a global Neural Radiance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward downstream applications. This approach extends beyond previous neural rendering work by considering complex scenarios of uncertainty in environmental states and dynamics. We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network. We demonstrate the utility of our method in realistic scenarios using the CARLA driving simulator, where CARFF can be used to enable efficient trajectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions. Our website containing video demonstrations and code is available at: www.carff.website.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Humans often imagine what they cannot see given partial visual context. Consider a scenario where reasoning about the unobserved is critical to safe decision-making: for example, a driver navigating a blind intersection. An expert driver will plan according to what they believe may or may not exist in occluded regions of their vision. The driver\'s belief - defined as the understanding of the world modeled with consideration for inherent uncertainties of real-world environments - is informed by their partial observations (i.e., the presence of other vehicles on the road), as well as their prior knowledge (e.g., past experience navigating this intersection). When reasoning about the unobserved, humans are capable of holding complex beliefs over not just the existence and position of individual objects (e.g., whether or not there is an oncoming car), but also the shapes, colors, and textures composing the entire occluded portion of the scene.\n' +
      '\n' +
      'Traditionally, autonomous systems with high-dimensional sensor observations such as video or LiDAR process the data into low-dimensional state information, such as the position and velocity of tracked objects, which are then used for downstream prediction and planning. This object-centric framework can be extended to reason about partially observed settings by modeling the existence and state of potentially dangerous unobserved objects in addition to tracked fully observed objects. Such systems often plan with respect to worst-case hypotheses, e.g., by placing a "ghost car" traveling at speed on the edge of the visible field of view [41].\n' +
      '\n' +
      'Recent advances in neural rendering have seen tremendous success in learning 3D scene representations directly from posed multi-view images using deep neural networks. Neural Radiance Fields (NeRF) allow for synthesizing novel images in a 3D scene from arbitrary viewing angles, making seeing behind an occlusion as simple as rendering from an unoccluded camera angle. Because NeRF can generate RGB images from novel views, it decouples the dependency of the scene representation on the object detec\n' +
      '\n' +
      'Figure 1: **CARFF 3D planning application for driving**. An input image containing a partially observable view of an intersection is processed by CARFFâ€™s encoder to establish 3D environment state beliefs, i.e. the predicted possible state of the world: whether or not there could be another vehicle approaching the intersection. These beliefs are used to forecast the future in 3D for planning, generating one among two possible actions for the vehicle to merge into the other lane.\n' +
      '\n' +
      'tion and tracking pipeline. For example, images rendered from a NeRF may contain visual information that would be lost by an object detector, but may still be relevant for safe decision-making. Additionally, because NeRF represents explicit geometry via an implicit density map, it can be used directly for motion planning without requiring any rendering [1]. NeRF\'s ability to represent both visual and geometric information makes them a more general and intuitive 3D representation for autonomous systems.\n' +
      '\n' +
      'Despite NeRF\'s advantages, achieving probabilistic predictions in 3D based on reasoning from the occluded is challenging. For example, discriminative models that yield categorical predictions are unable to capture the underlying 3D structure, impeding their ability to model uncertainty. While prior work on 3D representation captures view-invariant structures, their application is primarily confined to simple scenarios [17]. We present CARFF, which to our knowledge, is the first forecasting approach in scenarios with partial observations that uniquely facilitates stochastic predictions within a 3D representation, effectively integrating visual perception and explicit geometry.\n' +
      '\n' +
      'CARFF addresses the aforementioned difficulties by proposing _PC-VAE_: Pose-Conditioned Variational Autoencoder, a framework that trains a convolution and Vision Transformer (ViT) [9] based image encoder. The encoder maps a potentially partially observable ego-centric image to latent scene representations, which hold state beliefs with implicit probabilities. The latents later condition a neural radiance field that functions as a 3D decoder to recover 3D scenes from arbitrary viewpoints. This is trained after PC-VAE in our two-stage training pipeline (see Sec. 3.1). Additionally, we design a mixture density model to predict the evolution of 3D scenes over time stochastically in the encoder belief space (see Sec. 3.2). A potential application of CARFF is illustrated in Fig. 1. Using the CARLA driving simulator, we demonstrate how CARFF can be used to enable contingency planning in real-world driving scenarios that require reasoning into visual occlusions.\n' +
      '\n' +
      '## 2 Related work\n' +
      '\n' +
      '### Dynamic Neural Radiance Fields\n' +
      '\n' +
      '**Neural radiance fields:** Neural Radiance Fields (NeRF) [2, 23, 39] for 3D representations have garnered significant attention due to their ability to generate high-resolution, photorealistic scenes. Instant Neural Graphics Primitive (Instant-NGP) [24] speeds up training and rendering time by introducing a multi-resolution hash encoding. Other works like Plenoxels [11] and DirectVoxGo (DVGO) [38] also provide similar speedups. Given the wide adoption and speedups of Instant-NGP, we use it to model 3D radiance fields in our work. NeRFs have also been extended for several tasks such as modeling large-scale unbounded scenes [2, 40, 46], scene from sparse views [7, 35, 45] and multiple scenes [17, 48]. Tewari et al. [42] presents an in-depth survey on neural representation learning and its applications.\n' +
      '\n' +
      'Generalizable novel view synthesis models such as pixelNeRF and pixelSplat [5, 51] learn a scene prior to render novel views conditioned on sparse existing views. Dynamic NeRF, on the other hand, models scenes with moving objects or objects undergoing deformation. A widely used approach is to construct a canonical space and predict a deformation field [19, 29, 30, 32]. The canonical space is usually a static scene, and the model learns an implicitly represented flow field [29, 32]. A recent line of work also models dynamic scenes via different representations and decomposition [3, 37]. These approaches tend to perform better for spatially bounded and predictable scenes with relatively small variations [3, 20, 29, 51]. Moreover, these methods only solve for changes in the environment but are limited in incorporating stochasticity in the environment.\n' +
      '\n' +
      '**Multi-scene NeRF:** Our approach builds on multi-scene NeRF approaches [17, 44, 48, 49] that learn a global latent scene representation, which conditions the NeRF, allowing a single NeRF to effectively represent various scenes. A similar method, NeRF-VAE, was introduced by Kosiorek _et al._[17] to create a geometrically consistent 3D generative model with generalization to out-of-distribution cameras. However, NeRF-VAE [17] is prone to mode collapse when evaluated on complex, real-world datasets.\n' +
      '\n' +
      '### Scene Forecasting\n' +
      '\n' +
      '**Planning in 2D space:** In general, planning in large and continuous state-action spaces is difficult due to the resulting exponentially large search space [28]. Consequently, several approximation methods have been proposed for tractability [22, 31]. Various model-free [12, 27, 43] and model-based [4] reinforcement learning frameworks emerge as viable approaches, along with other learning-based methods [6, 25]. Several other approaches forecast for downstream control [15], learn behavior models for contingency planning [34], or predict potential existence and intentions of possibly unobserved agents [26]. While these methods are in 2D, we similarly reason under partial observations, and account for these factors in 3D.\n' +
      '\n' +
      '**NeRF in robotics:** Several recent works have explored the application of NeRFs in robotics, like localization [50], navigation [1, 21], dynamics-modeling [10, 19] and robotic grasping [14, 16]. Adamkiewicz et al. [1] proposes a method for quadcopter motion planning in NeRF models via sampling the learned density function, which is a desirable characteristic of NeRF that can be leveraged for forecasting and planning purposes. Additionally, Driess et al. [10] utilize a graph neural network to learn a dynamics model in a multi-object scene represented through NeRF. Li et al. [18] primarily perform pushing tasks in a scene with basic shapes, and approach grasping and planning with NeRF and a separately learned latent dynamics model. Prior work either only performs well on simple and static scenes [1] or has a deterministic dynamics model [18]. CARFF focuses on complicated realistic environments involving both state uncertainty and dynamics uncertainty, which account for the potential existence of an object and unknown object movements respectively.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '3D scene representation has witnessed significant advancements in recent years, allowing for modeling environments in a contextually rich and interactive 3D space. This offers many analytical benefits, such as producing soft occupancy grids for spatial analysis and novel view synthesis for object detection. Given the advantages, our primary objective is to develop a model for probabilistic 3D scene forecasting in dynamic environments. However, direct integration of 3D scene representation via NeRF and probabilistic models like VAE often involves non-convex and inter-dependent optimization, which causes unstable training. For instance, NeRF\'s optimization may rely on the VAE\'s latent space being structured to provide informative gradients.\n' +
      '\n' +
      'To navigate these complexities, our method bifurcates the training process into two stages (see Fig. 3). First, we train the PC-VAE to learn view-invariant scene representations. Next, we replace the decoder with a NeRF to learn a 3D scene from the latent representations. The latent scene representations capture the environmental states and dynamics over possible underlying scenes, while NeRF synthesizes novel views within the belief space, giving us the ability to see the unobserved (see Fig. 2 and Sec. 3.1). During prediction, uncertainties can be modeled by sampling latents auto-regressively from a predicted Gaussian mixture, allowing for effective decision-making. To this extent, we approach scene forecasting as a partially observable Markov decision process (POMDP) over latent distributions, which enables us to capture multi-modal beliefs for planning amidst perceptual uncertainty (see Sec. 3.2).\n' +
      '\n' +
      '### NeRF Pose-Conditional VAE (PC-VAE)\n' +
      '\n' +
      'Architecture:Given a scene \\(S_{t}\\) at timestamp \\(t\\), we render an ego-centric observation image \\(I_{c}^{t}\\) captured from camera pose \\(c\\). The objective is to formulate a 3D representation of the image where we can perform a forecasting step that evolves the scene forward. To achieve this, we utilize a radiance field conditioned on latent variable \\(z\\) sampled from the posterior distribution \\(q_{\\phi}(z|I_{c}^{t})\\). Now, to learn the posterior, we utilize PC-VAE. We construct an encoder using convolutional layers and a pre-trained ViT on ImageNet [9]. The encoder learns a mapping from the image space to a Gaussian distributed latent space \\(q_{\\phi}(z|I_{c}^{t})=\\mathcal{N}(\\mu,\\sigma^{2})\\) parametrized by mean \\(\\mu\\) and variance \\(\\sigma^{2}\\). The decoder, \\(p(I|z,c)\\), conditioned on camera pose \\(c\\), maps the latent \\(z\\sim\\mathcal{N}(\\mu,\\sigma^{2})\\) into the image space \\(I\\). This helps the encoder to generate latents that are invariant to the camera pose \\(c\\).\n' +
      '\n' +
      'To enable 3D scene modeling, we employ Instant-NGP [24], which incorporates a hash grid and an occupancy grid to enhance computation efficiency. Additionally, a smaller multilayer perceptron (MLP), \\(F_{\\theta}(z)\\) can be utilized to model the density and appearance, given by:\n' +
      '\n' +
      '\\[F_{\\theta}(z):(\\mathbf{x},\\mathbf{d},z)\\rightarrow((r,g,b),\\sigma) \\tag{1}\\]\n' +
      '\n' +
      'Here, \\(\\mathbf{x}\\in\\mathbb{R}^{3}\\) and \\(\\mathbf{d}\\in(\\theta,\\phi)\\) represent the location vector and the viewing direction respectively. The MLP is also conditioned on the sampled scene latents \\(z\\sim q_{\\phi}(z|I_{c}^{t})\\) (see Appendix C).\n' +
      '\n' +
      'Training methodology:The architecture alone does not enable us to model complex real-world scenarios, as seen through a similar example in NeRF-VAE [17]. A crucial contribution of our work is our two-stage training framework which stabilizes the training. First, we optimize the convolutional ViT based encoder and pose-conditional convolutional decoder in the pixel space for reconstruction. This enables our method to deal with more complex and realistic scenes as the encoding is learned in a semantically rich 2D space. By conditioning the decoder on camera poses, we achieve disentanglement between camera view angles and scene context, making the representation view-invariant and the encoder 3D-aware. Next, once rich latent representations are learned, we replace the decoder with a latent-conditioned NeRF over the latent space of the frozen encoder. The NeRF reconstructs encoder beliefs in 3D for novel view synthesis.\n' +
      '\n' +
      'Loss:Our PC-VAE is trained using standard VAE loss, with mean square error (MSE) and a Kullback-Leibler (KL) divergence given by evidence lower bound (ELBO):\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\textit{PC-VAE}}=\\mathcal{L}_{\\textit{MSE},\\textit{ PC-VAE}}+\\mathcal{L}_{\\textit{KLD, PC-VAE}}= \\tag{2}\\] \\[||p(I|z,c^{\\prime\\prime})-I_{c^{\\prime\\prime}}^{t}||^{2}+\\mathbb{E }_{q(z|I_{c}^{t})}[\\log p(I|z)]\\] \\[\\qquad\\qquad\\qquad\\qquad\\qquad-w_{\\textit{KL}}D_{KL}(q_{\\phi}(z|I _{c}^{t})\\ ||\\ p(I|z))\\]\n' +
      '\n' +
      'Figure 2: **Novel view planning application**. CARFF allows reasoning behind occluded views from the ego car as simple as moving the camera to see the sampled belief predictions, allowing simple downstream planning using, for example, density probing or 2D segmentation models from arbitrary angles.\n' +
      '\n' +
      'where \\(w_{\\textit{KL}}\\) denotes the KL divergence loss weight and \\(z~{}\\sim~{}q_{\\phi}(z|I_{c}^{t})\\). To make our representation 3D-aware, our posterior is encoded using camera \\(c\\) while the decoder is conditioned on a randomly sampled pose \\(c^{\\prime\\prime}\\).\n' +
      '\n' +
      'KL divergence regularizes the latent space to balance conditioned reconstruction and stochasticity under occlusion. An elevated KL divergence loss weight \\(w_{\\textit{KL}}\\) pushes the latents closer to a standard normal distribution, \\(\\mathcal{N}(0,1)\\), thereby ensuring probabilistic sampling in scenarios under partial observation. However, excessive regularization causes the latents to be less separable, leading to mode collapse. To mitigate this, we adopt delayed linear KL divergence loss weight scheduling to strike a balanced \\(w_{\\textit{KL}}\\).\n' +
      '\n' +
      'Next, we learn a NeRF-based decoder on the posterior of the VAE to model scenes. At any timestamp \\(t\\) we use a standard pixel-wise MSE loss for training the NeRF, given by the following equation:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\textit{MSE, NeRF}}=\\|I_{c}^{t}-\\textit{render}(F_{\\theta}( \\cdot|q_{\\phi}(z|I_{c}^{t})))\\|^{2} \\tag{3}\\]\n' +
      '\n' +
      'We use a standard rendering algorithm as proposed by Muller et al. [24]. Next, we build a forecasting module over the learned latent space of our pose-conditional encoder.\n' +
      '\n' +
      '### Scene Forecasting\n' +
      '\n' +
      'Formulation:The current formulation allows us to model scenes with different configurations across timestamps. In order to forecast future configurations of a scene given an ego-centric view, we need to predict future latent distributions. We formulate the forecasting as a partially observable Markov decision process (POMDP) over the posterior distribution \\(q_{\\phi}(z|I_{c}^{t})\\) in the PC-VAE\'s latent space.\n' +
      '\n' +
      'During inference, we observe stochastic behaviors under occlusion, which motivates us to learn a mixture of several Gaussian distributions that potentially denote different scene possibilities. Therefore, we model the POMDP using a Mixture Density Network (_MDN_), with multi-headed MLPs, that predicts a mixture of \\(K\\) Gaussians. At any timestamp \\(t\\) the distribution is given as:\n' +
      '\n' +
      '\\[q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})=MDN(q_{\\phi}(z_{t-1}|I_{c}^{t-1})) \\tag{4}\\]\n' +
      '\n' +
      'The model is conditioned on the posterior distribution \\(q_{\\phi}(z_{t-1})\\) to learn a predicted posterior distribution \\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})\\) at each timestamp. The predicted posterior distribution is given by the mixture of Gaussian:\n' +
      '\n' +
      '\\[q_{\\phi}^{\\prime}(z_{t})=\\sum_{i=1}^{K}\\pi_{i}~{}\\mathcal{N}(\\mu_{i},\\sigma_{ i}^{2}) \\tag{5}\\]\n' +
      '\n' +
      'here, \\(\\pi_{i}\\), \\(\\mu_{i}\\), and \\(\\sigma_{i}^{2}\\) denote the mixture weight, mean, and variance of the \\(i^{th}\\) Gaussian distribution within the posterior distribution. Here, \\(K\\) is the total number of Gaussians. For brevity we remove their conditioning on the posterior \\(q_{\\phi}(z_{t-1})\\) and sampled latent \\(z_{t-1}\\). We sample \\(z_{t}\\) from the mixture of Gaussians \\(q_{\\phi}^{\\prime}(z_{t})\\), where \\(z_{t}\\) most likely falls within one of the Gaussian modes. The scene configuration corresponding to the mode is reflected in the 3D scene rendered by NeRF.\n' +
      '\n' +
      'Loss:To optimize the MDN, we minimize a negative log-likelihood function, given by:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\textit{MDN}}=-\\sum_{i=1}^{N}log\\left(\\sum_{j=1}^{K}\\pi_{j} \\mathcal{N}(y_{i};\\mu_{j},\\sigma_{j}^{2})\\right) \\tag{6}\\]\n' +
      '\n' +
      'Figure 3: **Visualizing CARFFâ€™s two stage training process. Left:** The convolutional VIT based encoder encodes each of the images \\(I\\) at timestamps \\(t\\), \\(t^{\\prime}\\) and camera poses \\(c\\), \\(c^{\\prime}\\) into Gaussian latent distributions. Assuming only two timestamps and an overparameterized latent, one of the Gaussian distributions will have a smaller \\(\\sigma^{2}\\), and different \\(\\mu\\) across timestamps. **Upper Right:** The pose-conditional decoder stochastically decodes the sampled latent \\(z\\) using the camera pose \\(c^{\\prime\\prime}\\) into images \\(I_{c^{\\prime\\prime}}^{t}\\) and \\(I_{c^{\\prime\\prime}}^{t}\\). The decoded reconstruction and ground truth images are used to take the loss \\(\\mathcal{L}_{\\textit{MSE, PC-VAE}}\\). **Lower Right:** A NeRF is trained by conditioning on the latent variables sampled from the optimized Gaussian parameters. These parameters characterize the distinct timestamp distributions derived from the PC-VAE. An MSE loss is calculated for NeRF as \\(\\mathcal{L}_{\\textit{MSE, NeRF}}\\).\n' +
      '\n' +
      'where \\(y_{i}\\sim q_{\\phi}(z_{t})\\) is sampled from the distribution of latent \\(z_{t}\\), learned by the encoder, and \\(N\\) denotes the total number of samples.\n' +
      '\n' +
      'Inference:We consider an unseen ego-centric image and retrieve its posterior \\(q_{\\phi}(z_{t})\\) through the encoder. Next, we predict the possible future posterior distribution \\(q^{\\prime}_{\\phi}(z_{t+1})\\). From the predicted posterior, we sample a scene latent and perform localization. We achieve this via (a) density probing the NeRF or (b) segmenting the rendered novel views using off-the-shelf methods such as YOLO [33] (see Fig. 2). These allow us to retrieve a corresponding Gaussian distribution \\(q_{\\phi}(z_{t+1})\\) in encoder latent space. This is auto-regressively fed back into the MDN to predict the next timestamp. See Fig. 6 for an overview of the pipeline.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      'Decision-making under perceptual uncertainty is a pervasive challenge faced in robotics and autonomous driving, especially in partially observable environments encountered in driving tasks. In these scenarios, accurate inference regarding the presence of potentially obscured agents is pivotal. We evaluate the effectiveness of CARFF on similar real-world situations with partial observability. We implemented several scenarios in the CARLA driving simulator [8] (see Fig. 4). A single NVIDIA RTX 3090 GPU is used to train PC-VAE, NeRF, and the MDN. All models, trained sequentially, tend to converge within a combined time frame of 24 hours. A detailed experimental setup can be found in Appendix C. We show that, given partially observable 2D inputs, CARFF performs well in predicting latent distributions that represent complete 3D scenes. Using these predictions we design a CARFF-based controller for performing downstream planning tasks.\n' +
      '\n' +
      '### Data Generation\n' +
      '\n' +
      'We generate datasets containing an ego object and varying actor objects in different configurations to test the robustness of our method. We conduct experiments on (a) synthetic blender dataset for simple, controllable simulation and (b) CARLA-based driving datasets for complicated real-world scenarios [8].\n' +
      '\n' +
      'Blender synthetic dataset:This comprises of a stationary blue cube (ego) accompanied by a red cylinder (actor) that may or may not be present (see Fig. 5). If the actor is present, it exhibits lateral movement as depicted in Fig. 5. This simplistic setting provides us with an interpretable framework to evaluate our model.\n' +
      '\n' +
      'CARLA dataset:Each dataset is simulated for \\(N\\) timestamps and uses \\(C=100\\) predefined camera poses to capture images of the environment under full observation, partial observation, and no visibility. These datasets are modeled after common driving scenarios involving state uncertainty that have been proposed in related works such as Active Visual Planning [25].\n' +
      '\n' +
      'Single-Scene Approaching IntersectionThe ego vehicle is positioned at a T-intersection with an actor vehicle traversing the crossing along an evenly spaced, predefined trajectory. We simulate this for \\(N=10\\) timestamps. We mainly use this dataset to predict the evolution of timestamps under full observation.\n' +
      '\n' +
      '_b) Multi-Scene Approaching Intersection:_ We extend the aforementioned scenario to a more complicated setting with state uncertainty, by making the existence of the actor vehicle probabilistic. A similar intersection crossing is simulated for \\(N=3\\) timestamps for both possibilities. The ego vehicle\'s view of the actor may be occluded as it approaches the T-intersection over the \\(N\\) timestamps. The ego vehicle can either move forward or halt at the junction (see Fig. 4).\n' +
      '\n' +
      'Multi-Scene Multi-actor Two Lane MergeTo add more environment dynamics uncertainty, we consider a multi-actor setting at an intersection of two merging lanes. We simulate the scenario at an intersection with partial occlusions, with the second approaching actor having variable speed. Here the ego vehicle can either merge into the left\n' +
      '\n' +
      'Figure 4: **Multi-scene CARLA datasets**. Images illustrating the varying car configurations and scenes for the Multi-Scene Two Lane Merge dataset (**left**) and the Multi-Scene Approaching Intersection dataset (**right**).\n' +
      '\n' +
      'Figure 5: **Blender dataset**. Simple Blender dataset with a stationary blue cube, accompanied by a potential red cylinder exhibiting probabilistic temporal movement. The different camera poses demonstrate how movement needs to be modeled probabilistically based on possible occlusions from different camera angles.\n' +
      '\n' +
      'lane before the second actor or after all the actors pass, (see Fig. 4). Each branch is simulated for \\(N=3\\) timestamps.\n' +
      '\n' +
      '### CARFF Evaluation\n' +
      '\n' +
      'A desirable behavior from our model is that it should predict a complete set of possible scenes consistent with the given ego-centric image, which could be partially observable. This is crucial for autonomous driving in unpredictable environments as it ensures strategic decision-making based on potential hazards. To achieve this we require a rich PC-VAE latent space, high-quality novel view synthesis, and auto-regressive probabilistic predictions of latents at future timestamps. We evaluate CARFF on a simple synthetic blender-based dataset and each CARLA-based dataset.\n' +
      '\n' +
      'Evaluation on blender dataset:In Fig. 5, for both Scene 1a and 1b, our model correctly forecasts the lateral movement of the cylinder to be in either position approximately 50% of the time, considering a left viewing angle. In Scene 2, with the absence of the red cylinder in the input camera angle, the model predicts the potential existence of the red cylinder approximately 50% of the time, and predicts lateral movements with roughly equal probability. This validates PC-VAE\'s ability to predict and infer from the occluded in the latent space, consistent with human intuitions. Similar intuitions, demonstrated within the simple scenes of the Blender dataset, can be transferred to driving scenarios simulated in our CARLA datasets.\n' +
      '\n' +
      'PC-VAE performance and ablations:We evaluate the performance of PC-VAE on CARLA datasets with multiple encoder architectures. We show that PC-VAE effectively reconstructs complex environments involving variable scenes, actor configurations, and environmental noise given potentially partially observable inputs (see Fig. 9). We calculated an average Peak Signal-to-Noise Ratio (PSNR) over the training data, as well as novel view encoder inputs. To evaluate the quality of the latent space generated by the encoder, we utilize t-SNE [47] plots to visualize the distribution of latent samples for each image in a given dataset (see Appendix E). We introduce a Support Vector Machine (SVM) [13] based metric to measure the visualized clustering quantitatively, where a higher value indicates better clustering based on timestamps. Most latent scene samples are separable by timestamps, which indicates that the latents are view-invariant. Samples that are misclassified or lie on the boundary usually represent partially or fully occluded regions. This is desirable for forecasting, as it enables us to model probabilistic behavior over these samples. In this process, balancing KL divergence weight scheduling maintains the quality of the PC-VAE\'s latent space and reconstructions (see Appendix C). The results presented in Tab. 2 substantiate the benefits of our PC-VAE encoder architecture compared to other formulations. Specifically, a non-conditional VAE fails in SVM accuracy as it only reconstructs images and does not capture the underlying 3D structures. Vanilla PC-VAE and PC-VAE without freezing weights require careful fine-tuning of several hyper-parameters and don\'t generalize well to drastic camera movements. Our experiments show that our proposed model is capable of sustaining stochastic characteristics via latent representations in the presence of occlusion, while simultaneously ensuring precise reconstructions.\n' +
      '\n' +
      '3D novel view synthesis:Given an unseen ego-centric view with potentially partial observations, our method maintains all possible current state beliefs in 3D, and faith\n' +
      '\n' +
      'Figure 6: **Auto-regressive inference in scene prediction**. The input image at timestamp \\(t\\), \\(I_{c}^{t}\\), is encoded using the pre-trained encoder from PC-VAE. The corresponding latent distribution is fed into the Mixture Density Network, which predicts a mixture of Gaussians. Each of the \\(K\\) Gaussians is a latent distribution that may correspond to different beliefs at the next timestamp. The mixture of Gaussians is sampled repeatedly for the predicted latent beliefs, visualized as \\(I_{c^{\\prime},scn^{\\prime}}^{t+1}\\), representing potentially the \\(i\\)th possible outcome. This is used to condition the NeRF to generate 3D views of the scene. To accomplish autoregressive predictions, we probe the NeRF for the location of the car and feed this information back to the pre-trained encoder to predict the scene at the next timestamp.\n' +
      '\n' +
      'fully reconstructs novel views from arbitrary camera angles for each belief. Fig. 2 illustrates one of the possible 3D beliefs that CARFF holds. This demonstrates our method\'s capacity for generating 3D beliefs that could be used for novel view synthesis in a view-consistent manner. Our model\'s ability to achieve accurate and complete 3D environmental understanding is important for applications like prediction-based planning.\n' +
      '\n' +
      'Inference under full and partial observations:Under full observation, we use MDN to predict the subsequent car positions in all three datasets. PSNR values are calculated based on bird-eye view NeRF renderings and ground truth bird-eye view images of the scene across different timestamps. In Tab. 1 we report the PSNR values for rendered images over the predicted posterior with the ground truth images at each timestamp. We also evaluate the efficacy of our prediction model using the accuracy curve given in Fig. 8. This represents CARFF\'s ability to generate stable beliefs, without producing incorrect predictions, based on actor(s) localization results. For each number of samples between \\(n=0\\) to \\(n=50\\), we choose a random subset of \\(3\\) fully observable ego images and take an average of the accuracies. In scenarios with partial observable ego-centric images where several plausible scenarios exist, we utilize recall instead of accuracy using a similar setup. This lets us evaluate the encoder\'s ability to avoid false negative predictions of potential danger.\n' +
      '\n' +
      'Fig. 8 shows that our model achieves high accuracy and recall in both datasets, demonstrating the ability to model state uncertainty (Approaching Intersection) and dynamic uncertainty (Two Lane Merge). The results indicate CARFF\'s resilience against randomness in resampling, and completeness in probabilistic modeling of the belief space. Given these observations, we now build a reliable controller to plan and navigate through complex scenarios.\n' +
      '\n' +
      '### Planning\n' +
      '\n' +
      'In all our experiments, the ego vehicle must make decisions to advance under certain observability. The scenarios are designed such that the ego views contain partial occlusion and the state of the actor(s) is uncertain in some scenarios. In order to facilitate decision-making using CARFF, we design a controller that takes ego-centric input images and outputs an action. Decisions are made incorporating sample consistency from the mixture density network. For instance, the controller infers occlusion and promotes the ego car to\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline \\begin{tabular}{c} Ground Truth \\\\ Prediction Pair \\\\ \\end{tabular} & \\begin{tabular}{c} Avg. PSNR \\\\ (Scene 1) \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} Avg. PSNR \\\\ (Scene 2) \\\\ \\end{tabular} \\\\ \\hline \\multicolumn{3}{l}{**Single-Scene Approaching Intersection**} \\\\ \\hline Matching Pairs & **29.06** & N.A \\\\ Un-matching Pairs & 24.01 & N.A \\\\ \\hline \\multicolumn{3}{l}{**Multi-Scene Approaching Intersection**} \\\\ \\hline Matching Pairs & **28.00** & **28.26** \\\\ Un-matching Pairs & 23.27 & 24.56 \\\\ \\hline \\multicolumn{3}{l}{**Multi-Scene Two Lane Merge**} \\\\ \\hline Matching Pairs & **28.14** & **28.17** \\\\ Un-matching Pairs & 22.74 & 23.32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Averaged PSNR for fully observable 3D predictions**. CARFF correctly predicts scene evolution across all timestamps for each dataset. The average PSNR is significantly higher for each prediction \\(\\hat{I}_{t_{i}}\\) and corresponding ground truth, \\(I_{t_{i}}\\). PSNR values for incorrect correspondences, \\(\\hat{I}_{t_{i}},I_{t_{j}}\\), is a result of matching surroundings. The complete table of predictions is in Appendix E.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\begin{tabular}{c} Encoder Architectures \\\\ \\end{tabular} & \\begin{tabular}{c} Train \\\\ PSNR \\\\ \\end{tabular} & \\begin{tabular}{c} SVM \\\\ Accuracy \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} NV \\\\ PSNR \\\\ \\end{tabular} \\\\ \\hline PC-VAE & **26.30** & **75.20** & **25.24** \\\\ PC-VAE w/o CL & 26.24 & 70.60 & 24.80 \\\\ Vanilla PC-VAE & 26.02 & 25.70 & 24.65 \\\\ PC-VAE w/o Freezing & 24.57 & 5.80 & 24.60 \\\\ PC-VAE w/ MobileNet & 17.14 & 19.70 & 17.16 \\\\ \\hline Vanilla VAE & 24.15 & 10.60 & 11.43 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **PC-VAE metrics and ablations**. CARFFâ€™s PC-VAE encoder outperforms other encoder architectures in both image reconstruction and pose-conditioning. We evaluated the following ablations: PC-VAE without Conv Layer, PC-VAE with a vanilla encoder, PC-VAE without freezing weights in ViT, PC-VAE replacing ViT with pre-trained MobileNet, and non pose-conditional Vanilla VAE. The table displays the average training PSNR, novel view (NV) input PSNR, and SVM accuracy for latent timestamp prediction.\n' +
      '\n' +
      'Figure 7: **PC-VAE reconstructions**. The encoder input, \\(I^{t}_{c}\\), among the other ground truth images \\(I_{c}\\) viewed from camera pose \\(c\\) at different timestamps, is reconstructed across a new set of poses \\(c^{\\prime\\prime}\\) respecting timestamp \\(t\\), generating \\(I^{t}_{c^{\\prime\\prime}}\\). A complete grid is in Appendix E.\n' +
      '\n' +
      'pause when scenes alternate between actor presence and absence in the samples. We use the two multi-scene datasets to assess the performance of the CARFF-based controller as they contain actors with potentially unknown behaviors.\n' +
      '\n' +
      'To design an effective controller, we need to find a balance between accuracy and recall (see Fig. 8). A lowered accuracy from excessive sampling means unwanted randomness in the predicted state. However, taking insufficient samples would generate low recall i.e., not recovering all plausible states. This would lead to incorrect predictions as we would be unable to account for the plausible uncertainty present in the environment. To find a balance, we design an open-loop planning controller opting for a sampling strategy that involves generating \\(n=2,10,35\\) samples, where \\(n\\) is a hyperparameter to be tuned for peak performance.\n' +
      '\n' +
      'For sampling values that lie on the borders of the accuracy and recall margin, for example, \\(n=2\\) and \\(35\\), we see that the CARFF-based controller obtains lower success rates, whereas \\(n=10\\) produces the best result. Across the two datasets in Tab. 3, the overconfident controller will inevitably experience collisions in case of a truck approaching, since it does not cautiously account for occlusions. On the other hand, an overly cautious approach results in stasis, inhibiting the controller\'s ability to advance in the scene. This nuanced decision-making using CARFF-based controller is especially crucial in driving scenarios, as it enhances safety and efficiency by adapting to complex and unpredictable road environments, thereby fostering a more reliable and human-like response in autonomous vehicles.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      'Limitations:Like other NeRF-based methods, CARFF currently relies on posed images of specific scenes such as road intersections, limiting its direct applicability to unseen environments. However, we anticipate enhanced generalizability with the increasing deployment of cameras around populated areas, such as traffic cameras at intersections. Additionally, handling very complex dynamics with an extremely large number of actors still poses a challenge for our method, requiring careful fine-tuning to balance comprehensive dynamics modeling against accuracy. Potentially stronger models in the near future may offer a promising avenue for further enhancements in this regard.\n' +
      '\n' +
      'Conclusion:We presented CARFF, a novel method for probabilistic 3D scene forecasting from partial observations. By employing a Pose-Conditional VAE, a NeRF conditioned on the learned posterior, and a mixture density network that forecasts future scenes, we effectively model complex real-world environments with state and dynamics uncertainty in occluded regions critical for planning. We demonstrated the capabilities of our method in realistic autonomous driving scenarios, where, under full observations, we can forecast into the future providing high-fidelity 3D reconstructions of the environment, while we maintain complete recall of potential hazards given incomplete scene information. Overall, CARFF offers an intuitive and unified approach to perceiving, forecasting, and acting under uncertainty that could prove invaluable for vision algorithms in unstructured environments.\n' +
      '\n' +
      'Figure 8: **Multi-Scene dataset accuracy and recall curves from predicted beliefs. We test our framework across \\(n=1\\) and \\(n=50\\) samples from MDNâ€™s predicted latent distributions from ego-centric image input. Across the number of samples \\(n\\), we achieve an ideal margin of belief state coverage generated under partial observation (recall), and the proportion of correct beliefs sampled under full observation (accuracy). As we significantly increase the number of samples, the accuracy starts to decrease due to randomness in latent distribution resampling.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline \\multicolumn{3}{l}{**Multi-Scene Approaching Intersection**} \\\\ Controller Type & Actor Exists & No Actor \\\\ \\hline Underconfident & \\(30/30\\) & \\(0/30\\) \\\\ Overconfident & \\(0/30\\) & \\(30/30\\) \\\\ CARFF (\\(n\\) = 2) & \\(17/30\\) & \\(30/30\\) \\\\\n' +
      '**CARFF (\\(n\\) = 10)** & \\(\\mathbf{30/30}\\) & \\(\\mathbf{30/30}\\) \\\\ CARFF (\\(n\\) = 35) & \\(30/30\\) & \\(19/30\\) \\\\ \\hline \\hline \\multicolumn{3}{l}{**Multi-Scene Two Lane Merge**} \\\\ Controller Type & Fast Actor & Slow Actor \\\\ \\hline Underconfident & \\(30/30\\) & \\(0/30\\) \\\\ Overconfident & \\(0/30\\) & \\(30/30\\) \\\\ CARFF (\\(n\\) = 2) & \\(21/30\\) & \\(30/30\\) \\\\\n' +
      '**CARFF (\\(n\\) = 10)** & \\(\\mathbf{30/30}\\) & \\(\\mathbf{30/30}\\) \\\\ CARFF (\\(n\\) = 35) & \\(30/30\\) & \\(22/30\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: **Planning in 3D with controllers with varying sampling numbers \\(n\\). CARFF-based controllers outperform baselines in success rate over 30 trials. For \\(n\\) = 10, the CARFF-based controller consistently chooses the optimal action in potential collision scenarios. For actor exists and fast-actor scenes, we consider occluded ego-centric inputs to test CARFFâ€™s ability to avoid collisions. For no-actor and slow-actor scenes, we consider state observability and test the controllersâ€™ ability to recognize the optimal action to advance. To maintain consistency, we use one single image input across 30 trials.**\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, and Mac Schwager. Vision-only robot navigation in a neural radiance world. _IEEE Robotics and Automation Letters_, 7(2):4606-4613, 2022.\n' +
      '* [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Int. Conf. Comput. Vis._, pages 5855-5864, 2021.\n' +
      '* [3] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 130-141, 2023.\n' +
      '* [4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. Instance-aware predictive navigation in multi-agent environments. In _IEEE Int. Conf. on Robotics and Automation_, pages 5096-5102. IEEE, 2021.\n' +
      '* [5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction, 2023.\n' +
      '* [6] Felipe Codevilla, Eder Santana, Antonio M Lopez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In _Int. Conf. Comput. Vis._, pages 9329-9338, 2019.\n' +
      '* [7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12882-12891, 2022.\n' +
      '* [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In _Conf. on Robot Learning_, pages 1-16, 2017.\n' +
      '* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _Int. Conf. Learn. Represent._, 2021.\n' +
      '* [10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object dynamics with compositional neural radiance fields. _arXiv preprint arXiv:2202.11855_, 2022.\n' +
      '* [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5501-5510, 2022.\n' +
      '* [12] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In _AAAI_, 2015.\n' +
      '* [13] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. _IEEE Intelligent Systems and their applications_, 13(4):18-28, 1998.\n' +
      '* [14] Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, and Ken Goldberg. Dex-nerf: Using a neural radiance field to grasp transparent objects. _arXiv preprint arXiv:2110.14217_, 2021.\n' +
      '* [15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, and Marco Pavone. Mats: An interpretable trajectory forecasting representation for planning and control. In _Conf. on Robot Learning_, 2021.\n' +
      '* [16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and Ken Goldberg. Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In _Conf. on Robot Learning_, 2022.\n' +
      '* [17] Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Sona Mokra, and Danilo Jimenez Rezende. NeRF-VAE: A geometry aware 3d scene generative model. pages 5742-5752, 2021.\n' +
      '* [18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene representations for visuomotor control. In _Conf. on Robot Learning_, pages 112-123, 2022.\n' +
      '* [19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Devrf: Fast deformable voxel radiance fields for dynamic scenes. In _Adv. Neural Inform. Process. Syst._, pages 36762-36775, 2022.\n' +
      '* [20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, 2023.\n' +
      '* [21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Christian Wolf. Multi-object navigation with dynamically learned neural implicit representations. In _Int. Conf. Comput. Vis._, pages 11004-11015, 2023.\n' +
      '* [22] Rowan McAllister and Carl Edward Rasmussen. Data-efficient reinforcement learning in continuous state-action gaussian-pomdps. In _Adv. Neural Inform. Process. Syst._, 2017.\n' +
      '* [23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _Eur. Conf. Comput. Vis._, 2020.\n' +
      '* [24] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multi-tiresolution hash encoding. _ACM Trans. Graph._, 41(4):1-15, 2022.\n' +
      '* [25] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAllister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learning a planner contingent on perceptual uncertainty. In _Conf. on Robot Learning_, 2022.\n' +
      '* [26] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAllister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learning a planner contingent on perceptual uncertainty. In _Conf. on Robot Learning_, pages 1607-1617, 2023.\n' +
      '* [27] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual to real reinforcement learning for autonomous driving. _arXiv preprint arXiv:1704.03952_, 2017.\n' +
      '* [28] Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes. _Mathematics of operations research_, 12(3):441-450, 1987.\n' +
      '\n' +
      '* [29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In _Int. Conf. Comput. Vis._, pages 5865-5874, 2021.\n' +
      '* [30] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. _ACM Trans. Graph._, 2021.\n' +
      '* [31] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point-based value iteration: An anytime algorithm for pomdps. In _IJCAI_, pages 1025-1032, 2003.\n' +
      '* [32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2020.\n' +
      '* [33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2016.\n' +
      '* [34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A Wright, Rowan McAllister, Joseph E Gonzalez, and Sergey Levine. Contingencies from observations: Tractable contingency planning with learned behavior models. In _IEEE Int. Conf. on Robotics and Automation_, pages 13663-13669, 2021.\n' +
      '* [35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Niessner. Dense depth priors for neural radiance fields from sparse input views. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12892-12901, 2022.\n' +
      '* [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge, 2015.\n' +
      '* [37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 16632-16642, 2023.\n' +
      '* [38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5459-5469, 2022.\n' +
      '* [39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In _Adv. Neural Inform. Process. Syst._, pages 7537-7547, 2020.\n' +
      '* [40] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8248-8258, 2022.\n' +
      '* [41] Omer Sahin Tas and Christoph Stiller. Limited visibility and uncertainty aware motion planning for automated driving. In _IEEE Intelligent Vehicles Symposium (IV)_, 2018.\n' +
      '* [42] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi, T. Simon, C. Theobalt, M. Niessner, J. T. Barron, G. Wetzstein, M. Zollhofer, and V. Golyanik. Advances in Neural Rendering. _Comput. Graph. Forum_, 2022.\n' +
      '* [43] Marin Toromanoff, Emilie Wrobel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 7153-7162, 2020.\n' +
      '* [44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer, Aljaz Bozic, Christoph Lassner, and Christian Theobalt. Scenerflow: Time-consistent reconstruction of general dynamic scenes. In _International Conference on 3D Vision (3DV)_, 2023.\n' +
      '* [45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4190-4200, 2023.\n' +
      '* [46] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale renfs for virtual fly-throughs. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12922-12931, 2022.\n' +
      '* [47] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605, 2008.\n' +
      '* [48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4690-4699, 2021.\n' +
      '* [49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5438-5448, 2022.\n' +
      '* [50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1323-1330. IEEE, 2021.\n' +
      '* [51] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images, 2021.\n' +
      '\n' +
      'A CARLA Datasets\n' +
      '\n' +
      'A complete figure of the actor and ego configurations across scenes and the progression of timestamps for the Single-Scene Approaching Intersection, Multi-Scene Approaching Intersection, and the Multi-Scene Two Lane Merge is visualized in Fig. 14.\n' +
      '\n' +
      '## Appendix B Related Work Comparison\n' +
      '\n' +
      'In this section, we draw a comparison between CARFF and other methods that perform tasks similar to our model. However, these methods do not precisely align with the objectives we aim to achieve. For instance, while some methods integrate 3D reasoning, others may omit this aspect. To establish a fair comparison between our model and current methods, we have conducted an in-depth analysis of their qualitative differences, as delineated in Tab. 4. We primarily compare to other NeRF works based on their ability to model uncertainty (state and dynamics) and perform forecasting in the environment. Our work surpasses all the listed previous works and is on par with 2D-based forecasting approaches in functionality [25]. This comparison highlights that our model comprehensively encompasses the key qualitative factors that should be present to reason from the occluded as humans do.\n' +
      '\n' +
      '## Appendix C Implementation Details\n' +
      '\n' +
      '### Pose-Conditional VAE\n' +
      '\n' +
      'Architecture:We implement PC-VAE on top of a standard PyTorch VAE framework. The encoder with convolutional layers is replaced with a single convolutional layer and a Vision Transformer (ViT) Large 16 [9] pre-trained on ImageNet [36]. We modify fully connected layers to project ViT output of size \\(1000\\) to mean and variances with size of the latent dimension, 8. During training, the data loader returns the pose of the camera angle represented by an integer value. This value is one-hot encoded and concatenated to the re-parameterized encoder outputs, before being passed to the decoder. The decoder input size is increased to add the number of poses to accommodate the additional pose information.\n' +
      '\n' +
      'Optimization:We utilize a single RTX 3090 graphics card for all our experiments. The PC-VAE model takes approximately \\(22\\) hours to converge using this GPU. During this phase, we tune various hyperparameters including the latent size, learning rate and KL divergence loss weight to establish optimal training tailored to our model (see Tab. 5). In order to optimize for the varied actor configurations and scenarios generated within the CARLA [8] simulator, we slightly adjust hyperparameters differently for each dataset.\n' +
      '\n' +
      'The learning rate (LR) and KL divergence (KLD) weight are adjusted to find an appropriate balance between the effective reconstruction of pose conditioning in the latent space, and the regularization of latents. Regularization pushes the latents toward Gaussian distributions and keeps the non-expressive latents in an over-parameterized latent space to be standard normal. This stabilizes the sampling process and ensures stochastic behavior of latent samples in case of occlusion. To achieve this balance, we use a linear KLD weight scheduler, where the weight is initialized at a low value for KLD increment start epoch (see Tab. 5). This allows the model to initially focus on achieving highly accurate conditioned reconstructions. The KLD weight is then steadily increased until KLD increment end epoch is reached, ensuring probabilistic behavior under partial observability.\n' +
      '\n' +
      '### Mixture Density Network\n' +
      '\n' +
      'The mixture density network (MDN) takes in the mean and variances of the latent distributions \\(q_{\\phi}(z_{t-1}|I_{c}^{t-1})\\) and outputs the estimated posterior distribution as a mixture of Gaussian \\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})\\) through a multi-headed MLP.\n' +
      '\n' +
      'Architecture:The shared backbone simply contains \\(2\\) fully connected layers and rectified linear units (ReLU) activation with hidden layer size of \\(512\\). Additional heads with \\(2\\) fully connected layers are used to generate \\(\\mu_{i}\\) and \\(\\sigma_{i}^{2}\\). The mixture weight, \\(\\pi_{i}\\), is generated from a \\(3\\) layer MLP network. We limit the number of Gaussians, \\(K=2\\).\n' +
      '\n' +
      'Optimization:We train our network for \\(30,000\\) epochs using the batch size of \\(128\\) and an initial LR of \\(0.005\\), and apply LR decay to optimize training. This takes approximately \\(30\\) minutes to train utilizing the GPU. During training, the dataloader outputs the means and variances at the current timestamp and indexed view, and the means and variances for the next timestamp, at a randomly sampled neighboring view. This allows the MDN to learn how occluded views advance into all the possible configurations from potentially unoccluded neighboring views, as a mixture of Gaussian.\n' +
      '\n' +
      'At each iteration, the negative log-likelihood loss is computed for \\(1000\\) samples drawn from the predicted mixture of distributions \\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})\\) with respect to the ground truth distribution \\(q_{\\phi}(z_{t}|I_{c}^{t})\\). While the MDN is training, additional Gaussian noise, given by \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})\\), is added to the means and variances of the current timestamp \\(t-1\\), where \\(\\sigma\\in[0.001,0.01]\\). The Gaussian noise and LR decay help prevent overfitting and reduce model sensitivity to environmental artifacts like moving trees, moving water, etc.\n' +
      '\n' +
      '### NeRF\n' +
      '\n' +
      'Architecture:We implement our NeRF decoder utilizing an existing PyTorch implementation of Instant-NGP [24]. We concatenate the latents to the inputs of two parts of the Instant-NGP architecture: the volume density \n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{3D} & \\multicolumn{2}{c}{Realistic} & \\multicolumn{2}{c}{State} & \\multicolumn{2}{c}{Dynamics} & \\multicolumn{1}{c}{Code} \\\\  & & Application & Uncertainty & Uncertainty & Prediction & Planning & Released \\\\ \\hline CARFF & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ \\\\ NeRF-VAE [17] & âœ“ & & âœ“ & & & & \\\\ NeRF for Visuomotor Control [18] & âœ“ & âœ“ & & & âœ“ & âœ“ & \\\\ NeRF Navigation [1] & âœ“ & âœ“ & & & & âœ“ & âœ“ \\\\ \\hline AVP [25] & & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Qualitative comparison of CARFF to related works.** CARFF accomplishes all the highlighted objectives as opposed to the similar works listed. A check mark indicates that the associated method incorporates the qualitative feature in each column, whereas empty spaces indicate that the method does not account for it. Here, 3D refers to methods that reason in a 3D environment and perform novel view synthesis. Realistic application refers to whether the method has been demonstrated in realistic and complex scenarios. State and dynamic uncertainty refer to whether the model predicts probabilistically under these conditions. Prediction refers to forecasting into the future, while planning refers to using model predictions for decision-making.\n' +
      '\n' +
      'Figure 9: **PC-VAE encoder inputs, ground truth timestamps, and reconstructions**. The encoder input, \\(I_{c}^{t}\\), among the other ground truth images \\(I_{c}\\) viewed from camera pose \\(c\\) at different timestamps, is reconstructed across a new set of poses \\(c^{\\prime\\prime}\\) respecting timestamp \\(t\\), generating \\(I_{c^{\\prime\\prime}}^{t}\\). This is a full grid of the reconstructions.\n' +
      '\n' +
      'network, \\(\\sigma(\\mathbf{x})\\), for the density values, and the color network, \\(C(\\mathbf{r})\\), for conditional RGB generation. While the overall architecture is kept constant, the input dimensions of each network are modified to allow additional latent concatenation.\n' +
      '\n' +
      '**Optimization:** Empirically, we observe that it is essential to train the NeRF such that it learns the distribution of scenes within the PC-VAE latent space. Using only pre-defined learned samples to train may run the risk of relying on non-representative samples. On the other hand, direct resampling during each training iteration in Instant-NGP may lead to delayed training progress, due to NeRF\'s sensitive optimization. In our optimization procedure, we use an LR of \\(0.002\\) along with an LR decay and start with pre-defined latent samples. Then we slowly introduce the re-sampled latents. We believe that this strategy progressively diminishes the influence of a single sample, while maintaining efficient training. Based on our observations, this strategy contributes towards Instant-NGP\'s ability to rapidly assimilate fundamental conditioning and environmental reconstruction, while simultaneously pushing the learning process to be less skewed towards a single latent sample.\n' +
      '\n' +
      '## Appendix D GUI Interface\n' +
      '\n' +
      'For ease of interaction with our inference pipeline, our NeRF loads a pre-trained MDN checkpoint, and we build a graphical user interface (GUI) using DearPyGUI for visualization purposes. We implement three features in the GUI: (a) predict, (b) probe and predict, and (c) toggle.\n' +
      '\n' +
      '**Predict:** We implement the function to perform prediction directly from a given image path in the GUI. We use the distribution \\(q_{\\phi}(z_{t-1}|I_{c}^{t-1})\\) from PC-VAE encoder, corresponding to the input image \\(I_{c}^{t-1}\\), to predict the latent distribution for the next timestamp \\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})\\). This process is done on the fly through the MDN. A sample from the predicted distribution is then generated and used to condition the NeRF. This advances the entire scene to the next timestamp.\n' +
      '\n' +
      '**Probe and predict:** The sampled latent from the predicted distribution does not correspond to a singular distribution and hence we can not directly predict the next times\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r} \\hline \\hline\n' +
      '**PC-VAE Hyperparameters** & \\\\ \\hline Latent Size & 8 \\\\ LR & \\(0.004\\) \\\\ KLD Weight Start & \\(0.000001\\) \\\\ KLD Weight End & \\(0.00001-0.00004\\)* \\\\ KLD Increment Start & \\(50\\) epochs \\\\ KLD Increment End & \\(80\\) epochs \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **PC-VAE experimental setup and hyperparameters.** The main hyperparameters in PC-VAE training on the three datasets are latent size, LR, and KLD weight. For KLD scheduling, the KLD increment start refers to the number of epochs at which the KLD weight begins to increase from the initial KLD weight. KLD increment end is the number of epochs at which the KLD weight stops increasing at the maximum KLD weight. The asterisk (*) marks the hyperparameter that is dataset-dependent.\n' +
      '\n' +
      'Figure 10: **NeRF graphical user interface.** The GUI allows us to toggle and predict with an input image path. The probe and predict function probes the current location of the car and predicts the next. The screenshot is sharpened for visual clarity in the paper.\n' +
      '\n' +
      'tamp. To make our model auto-regressive in nature, we perform density probing. We probe the density of the NeRF at the possible location coordinates of the car to obtain the current timestamp and scene. This is then used to match the latent to a corresponding distribution in the PC-VAE space. The new distribution enables auto-regressive predictions using the predict function described above.\n' +
      '\n' +
      'Toggle:The NeRF generates a scene corresponding to the provided input image path using learned latents from PC-VAE. When the input image is a fully observable view, the NeRF renders clear actor and ego configurations respecting the input. This allows us to visualize the scene at different timestamps and in different configurations.\n' +
      '\n' +
      '## Appendix E CARFF Evaluation\n' +
      '\n' +
      '### Pose-Conditional VAE\n' +
      '\n' +
      'Reconstruction Quality:To analyze the reconstruction performance of the model during training, we periodically plot grids of reconstructed images. These grids consist of (a) randomly selected encoder inputs drawn from the dataset, (b) the corresponding ground truth images for those inputs at each timestamp at the same camera pose, and (c) reconstructed outputs at randomly sampled poses respecting\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Encoder Architectures & Train & SVM & NV \\\\  & PSNR & Accuracy & PSNR \\\\ \\hline \\hline \\multicolumn{4}{l}{**Multi-Scene Approaching Intersection**} \\\\ \\hline PC-VAE & **26.47** & **89.17** & **26.37** \\\\ PC-VAE w/o CL & 26.20 & 83.83 & 26.16 \\\\ Vanilla PC-VAE & 25.97 & 29.33 & 25.93 \\\\ PC-VAE w/o Freezing & 24.82 & 29.83 & 24.78 \\\\ PC-VAE w/ MobileNet & 19.37 & 29.50 & 19.43 \\\\ Vanilla VAE & 26.04 & 14.67 & 9.84 \\\\ \\hline \\hline \\multicolumn{4}{l}{**Multi-Scene Two Lane Merge**} \\\\ \\hline PC-VAE & **25.50** & **88.33** & **25.84** \\\\ PC-VAE w/o CL & 24.38 & 29.67 & 24.02 \\\\ Vanilla PC-VAE & 24.75 & 29.67 & 24.96 \\\\ PC-VAE w/o Freezing & 23.97 & 28.33 & 24.04 \\\\ PC-VAE w/ MobileNet & 17.70 & 75.00 & 17.65 \\\\ Vanilla VAE & 25.11 & 28.17 & 8.49 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **PC-VAE metrics and ablations across Multi-Scene datasets.** CARFFâ€™s PC-VAE outperforms other encoder architectures across the Multi-Scene datasets in reconstruction and pose-conditioning.\n' +
      '\n' +
      'Figure 11: **Average train PSNR plot for all CARLA datasets**. The plot shows the increase in average training PSNR of all images for each dataset, over the period of the training process.\n' +
      '\n' +
      'Figure 12: **Latent sample distribution clustering**. The distributions of latent samples for the Multi-Scene Two Lane Merge dataset are separable through t-SNE clustering. In the figure, the clusters for _Scene 0, Timestamp 0_ and _Scene 1, Timestamp 0_ overlap in distribution because they represent the same initial state of the environment under dynamics uncertainty.\n' +
      '\n' +
      'the input scene and timestamp. An example reconstruction grid is provided in Fig. 9. The grid enables visual assessment of whether the model is capable of accurately reconstructing reasonable images using the encoder inputs, conditioned on the poses. This evaluation provides us with visual evidence of improvement in reconstruction quality. We also quantitatively analyze the progressive improvement of reconstruction through the average PSNR calculated over the training data (see Fig. 11).\n' +
      '\n' +
      'Latent Space AnalysisTo assess the quality of the latents generated by PC-VAE, we initially use t-SNE plots to visualize the latent distributions as clusters. Fig. 12 shows that the distributions of the latent samples for the Multi-Scene Two Lane Merge dataset are separable. While t-SNE is good at retaining nearest-neighbor information by preserving local structures, it performs weakly in preserving global structures. Therefore, t-SNE may be insufficient in capturing the differences in distributions for all our datasets.\n' +
      '\n' +
      'Instead, we pivot to Support Vector Machine to perform a quantitative evaluation of the separability of the latents. We utilize a Radial Basis Function (RBF) kernel with the standard regularization parameter (\\(C=1\\)). We perform 10-fold validation on the latents to calculate the accuracy as a metric for clustering. See Tab. 6 for the results.\n' +
      '\n' +
      'Beyond separability, we analyze the recall and accuracy of the learned latents directly from PC-VAE under partial and full observations. This achieves very high accuracy even under a large number of samples while retraining decent recall, enabling downstream MDN training. (See Fig. 13)\n' +
      '\n' +
      '### Fully Observable Predictions\n' +
      '\n' +
      'One of the tasks of the MDN is to forecast the future scene configurations under full observation. We quantitatively evaluate our model\'s ability to forecast future scenes by comparing bird\'s-eye views rendered from the NeRF with chosen ground truth images of the scene for the various timestamps (see Tab. 7). The values are calculated and displayed for all three datasets. In Tab. 7, images are marked as either toggled (\\(\\tilde{I}_{t_{i}}\\)) or predicted (\\(\\hat{I}_{t_{i}}\\)). Toggled images in the table cannot be predicted deterministically due to it being the first timestamp in the dataset, or the state of the previous timestamps across scenes being the same in case of dynamics uncertainty. Due to the same reason, in the Multi-Scene Two Lane Merge Dataset, there are additional bolded PSNR values for the pairs \\((I_{t_{1}},\\tilde{I}_{t_{4}})\\) and \\((I_{t_{4}},\\tilde{I}_{t_{1}})\\).\n' +
      '\n' +
      'Figure 13: **Multi-Scene dataset accuracy and recall curves from learned latents.** We test our framework across \\(n=1\\) and \\(n=50\\) samples from PC-VAEâ€™s latent distributions from egocentric image input. Across the number of samples \\(n\\), we achieve an ideal margin of belief state coverage generated under partial observation (recall), and the proportion of correct beliefs sampled under full observation (accuracy) for the MDN to learn. As we significantly increase the number of samples, the accuracy starts to decrease due to randomness in latent distribution resampling.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>