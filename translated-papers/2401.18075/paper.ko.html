<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# CARFF: 3차원 장면 예측을 위한 조건부 자동 부호화 복사율 필드\n' +
      '\n' +
      'Jiezhi "Stephen" Yang\\({}^{1}\\) Khushi Desai\\({}^{2}\\) Charles Packer\\({}^{3}\\)\n' +
      '\n' +
      'Harshil Bhatia\\({}^{4}\\) Nicholas Rhinehart\\({}^{3}\\) Rowan McAllister\\({}^{5}\\) Joseph Gonzalez\\({}^{3}\\)\n' +
      '\n' +
      '({}^{1}\\)Harvard University \\({}^{2}\\)Columbia University \\({}^{3}\\)UC Berkeley \\({}^{4}\\)Avataar.ai \\({}^{5}\\)Toyota Research Research\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 3차원 장면 예측을 위한 조건부 자동 부호화 복사율 필드(CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting)를 제안한다. 이 필드(CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting)는 2차원 자아 중심 영상과 같은 과거 관측치가 주어진 미래의 3차원 장면을 예측하는 방법이다. 제안하는 방법은 확률적 인코더를 이용하여 영상을 그럴듯한 3차원 잠재 장면 구성들에 대한 분포에 매핑하고, 시간을 통해 가설된 장면들의 진화를 예측한다. 잠재 장면 표현 조건은 3D 장면 모델을 표현하기 위한 전역 신경 복사 필드(NeRF)이며, 이는 설명 가능한 예측과 간단한 다운스트림 응용을 가능하게 한다. 이 접근법은 환경 상태 및 역학에서 불확실성의 복잡한 시나리오를 고려함으로써 이전의 신경망 렌더링 작업을 넘어 확장된다. 우리는 3D 표현을 학습하기 위해 Pose-Conditional-VAE와 NeRF의 2단계 훈련을 사용한다. 또한, 혼합 밀도 네트워크를 이용하여 부분 관측 가능한 마르코프 결정 과정으로 잠재 장면 표현을 자동 회귀적으로 예측한다. 우리는 CARLA 운전 시뮬레이터를 사용하여 실제 시나리오에서 본 방법의 유용성을 입증하며, CARFF는 시각적 폐색을 포함하는 복잡한 다중 에이전트 자율 주행 시나리오에서 효율적인 궤적 및 우발 계획을 가능하게 하는 데 사용될 수 있다. 비디오 시연과 코드가 포함된 당사 웹사이트는 www.carff.website에서 이용할 수 있습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간은 종종 주어진 부분적 시각적 맥락을 볼 수 없는 것을 상상한다. 관측되지 않은 것에 대한 추론이 안전한 의사 결정에 중요한 시나리오, 예를 들어 맹목적인 교차로를 탐색하는 운전자를 고려한다. 전문 운전자는 시야가 가려진 영역에 존재할 수 있거나 존재하지 않을 수 있다고 믿는 것에 따라 계획을 세울 것이다. 실제 환경의 고유한 불확실성에 대한 고려로 모델링된 세계에 대한 이해로 정의되는 운전자의 믿음은 그들의 사전 지식(예: 이 교차로를 탐색하는 과거의 경험)뿐만 아니라 부분적인 관찰(예: 도로 상의 다른 차량의 존재)에 의해 알려진다. 관찰되지 않은 것에 대해 추론할 때, 인간은 개별 물체의 존재와 위치(예를 들어, 다가오는 자동차가 있는지 여부)뿐만 아니라 장면의 전체 가려진 부분을 구성하는 모양, 색상 및 질감에 대해 복잡한 믿음을 가질 수 있다.\n' +
      '\n' +
      '전통적으로, 비디오 또는 LiDAR와 같은 고차원 센서 관측을 갖는 자율 시스템은 데이터를 추적된 물체의 위치 및 속도와 같은 저차원 상태 정보로 처리하고, 이를 다운스트림 예측 및 계획에 사용한다. 이 객체 중심 프레임워크는 추적된 완전히 관찰된 객체 외에 잠재적으로 위험하지 않은 객체의 존재 및 상태를 모델링함으로써 부분적으로 관찰된 설정에 대한 추론으로 확장될 수 있다. 이러한 시스템은 종종 최악의 가설과 관련하여, 예를 들어 가시 시야의 에지에 속도로 이동하는 "유령 자동차"를 배치함으로써 계획한다[41].\n' +
      '\n' +
      '최근 신경망 렌더링의 발전은 심층 신경망을 사용하여 포즈된 다시점 이미지로부터 직접 3차원 장면 표현을 학습하는 데 엄청난 성공을 거두었다. 신경 복사 필드(Neural Radiance Fields, NeRF)는 임의의 시야각으로부터 3D 장면에서 새로운 이미지를 합성할 수 있게 하여 폐색 뒤쪽을 폐색되지 않은 카메라 각도에서 렌더링하는 것만큼 간단하게 볼 수 있게 한다. NeRF는 새로운 뷰로부터 RGB 이미지를 생성할 수 있기 때문에, 객체 디텍에 대한 장면 표현의 의존성을 분리한다.\n' +
      '\n' +
      '도 1: **CARFF 3D planning application for driving**. 교차로의 부분적으로 관찰 가능한 뷰를 포함하는 입력 이미지는 CARFF의 인코더에 의해 처리되어 3D 환경 상태 신념, 즉 세계의 예측된 가능한 상태를 확립한다: 교차로에 접근하는 다른 차량이 있을 수 있는지 여부. 이러한 믿음은 3D 계획에서 미래를 예측하는 데 사용되며, 차량이 다른 차선으로 병합될 수 있는 두 가지 가능한 행동 중 하나를 생성한다.\n' +
      '\n' +
      '옵션 및 추적 파이프라인입니다. 예를 들어, NeRF로부터 렌더링된 이미지들은 객체 검출기에 의해 손실될 시각적 정보를 포함할 수 있지만, 여전히 안전한 의사 결정과 관련될 수 있다. 또한, NeRF는 암시적 밀도 맵을 통해 명시적 기하학을 나타내기 때문에, 임의의 렌더링[1]을 요구하지 않고 모션 플래닝에 직접 사용될 수 있다. 시각적 정보와 기하학적 정보를 모두 표현하는 NeRF의 능력은 자율 시스템에 대해 보다 일반적이고 직관적인 3D 표현을 가능하게 한다.\n' +
      '\n' +
      'NeRF의 장점에도 불구하고, 폐색된 추론으로부터 추론에 기초하여 3D에서 확률론적 예측을 달성하는 것은 어렵다. 예를 들어, 범주형 예측을 산출하는 판별 모델은 기본 3D 구조를 포착할 수 없어 불확실성을 모델링하는 능력을 저해한다. 3D 표현에 대한 이전 작업은 뷰 불변 구조를 캡처하지만, 이들의 적용은 주로 간단한 시나리오[17]로 제한된다. 우리가 아는 한 CARFF는 3D 표현 내에서 확률적 예측을 독특하게 용이하게 하여 시각적 지각과 명시적 기하학을 효과적으로 통합하는 부분 관찰이 있는 시나리오의 첫 번째 예측 접근법이다.\n' +
      '\n' +
      'CARFF는 컨볼루션 및 비전 트랜스포머(ViT) [9] 기반 이미지 인코더를 트레이닝하는 프레임워크인 _PC-VAE_: Pose-Conditioned Variational Autoencoder를 제안함으로써 전술한 어려움을 해결한다. 인코더는 잠재적으로 부분적으로 관찰가능한 자아 중심 이미지를 잠재 장면 표현들에 맵핑하며, 잠재 장면 표현들은 암시적 확률들을 갖는 상태 신념들을 보유한다. 레이턴트들은 나중에 임의의 시점들로부터 3D 장면들을 복구하기 위해 3D 디코더로서 기능하는 신경 복사 필드를 컨디셔닝한다. 이것은 우리의 2단계 훈련 파이프라인에서 PC-VAE 후에 훈련된다(Sec. 3.1 참조). 또한, 인코더 신뢰 공간에서 시간에 따른 3D 장면의 진화를 확률적으로 예측하기 위한 혼합 밀도 모델을 설계한다(Sec. 3.2 참조). CARFF의 잠재적 적용은 그림 1에 나와 있다. CARLA 운전 시뮬레이터를 사용하여 시각적 폐색으로의 추론이 필요한 실제 운전 시나리오에서 CARFF가 어떻게 우발 계획을 가능하게 하는 데 사용될 수 있는지 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '# 동적 신경 복사 필드\n' +
      '\n' +
      '**신경 복사 필드:**신경 복사 필드(NeRF) [2, 23, 39]는 3D 표현을 위해 고해상도, 사실적 장면을 생성하는 능력으로 인해 상당한 관심을 받았다. 인스턴트 뉴럴 그래픽스 프리미티브(Instant Neural Graphics Primitive; Instant-NGP) [24]는 다중 해상도 해시 인코딩을 도입함으로써 트레이닝 및 렌더링 시간을 빠르게 한다. 플레녹셀[11]과 다이렉트박스고(DVGO)[38] 같은 다른 작품들도 비슷한 속도 향상을 제공한다. 인스턴트-NGP의 광범위한 채택과 속도를 감안할 때, 우리는 작업에서 3D 복사 필드를 모델링하는 데 사용합니다. NeRF는 또한 대규모 무제한 장면[2, 40, 46], 희소 뷰로부터의 장면[7, 35, 45] 및 다중 장면[17, 48]을 모델링하는 것과 같은 여러 작업에 대해 확장되었다. Tewari et al. [42]는 신경 표현 학습과 그 응용에 대한 심층 조사를 제시한다.\n' +
      '\n' +
      '픽셀NeRF 및 픽셀Splat[5, 51]과 같은 일반화할 수 있는 신규 뷰 합성 모델들은 희박한 기존 뷰들에 컨디셔닝된 신규 뷰들을 렌더링하기 전에 장면을 학습한다. 반면에 동적 NeRF는 움직이는 물체나 변형을 겪는 물체가 있는 장면을 모델링한다. 널리 사용되는 접근법은 표준 공간을 구성하고 변형장을 예측하는 것이다[19, 29, 30, 32]. 정준 공간은 보통 정적인 장면이며, 모델은 암묵적으로 표현된 흐름 필드를 학습한다[29, 32]. 최근의 작업은 또한 다른 표현과 분해를 통해 역동적인 장면을 모델링한다[3, 37]. 이러한 접근법들은 비교적 작은 변형들[3, 20, 29, 51]을 갖는 공간적으로 경계되고 예측가능한 장면들에 대해 더 나은 성능을 발휘하는 경향이 있다. 더욱이, 이러한 방법들은 단지 환경의 변화만을 해결하지만, 확률성을 환경에 통합하는 데 한계가 있다.\n' +
      '\n' +
      '**다중 장면 NeRF:** 우리의 접근법은 전역 잠재 장면 표현을 학습하는 다중 장면 NeRF 접근법[17, 44, 48, 49]을 기반으로 하며, 이는 NeRF를 조건화하여 단일 NeRF가 다양한 장면을 효과적으로 표현할 수 있게 한다. 유사한 방법인 NeRF-VAE는 Kosiorek _et al._[17]에 의해 도입되어 분포 외 카메라로 일반화되는 기하학적으로 일관된 3D 생성 모델을 생성했다. 그러나 NeRF-VAE[17]은 복잡한 실제 데이터 세트에서 평가될 때 모드 붕괴가 발생하기 쉽다.\n' +
      '\n' +
      '### Scene Forecasting\n' +
      '\n' +
      '**2D 공간에서의 계획:** 일반적으로, 크고 연속적인 상태-행동 공간에서의 계획은 결과적으로 기하급수적으로 큰 탐색 공간으로 인해 어렵다[28]. 결과적으로, 다루기 쉽도록 몇 가지 근사 방법들이 제안되었다[22, 31]. 다양한 모델 없는 [12, 27, 43] 및 모델 기반 [4] 강화 학습 프레임워크는 다른 학습 기반 방법[6, 25]과 함께 실행 가능한 접근법으로 등장한다. 다운스트림 제어에 대해 예측하는 몇 가지 다른 접근법[15], 우발 계획을 위한 행동 모델을 학습[34]하거나 관측되지 않을 가능성이 있는 에이전트의 잠재적 존재 및 의도를 예측[26]한다. 이러한 방법은 2D이지만 부분 관찰에서도 유사하게 추론하고 이러한 요인을 3D에서 설명한다.\n' +
      '\n' +
      '로봇 공학에서**NeRF:** 최근 몇 가지 작업에서 현지화[50], 항법[1, 21], 역학 모델링[10, 19] 및 로봇 파지[14, 16]와 같은 로봇 공학에서 NeRF의 적용을 탐구했다. Adamkiewicz et al. [1]은 예측 및 계획 목적으로 활용될 수 있는 NeRF의 바람직한 특성인 학습된 밀도 함수를 샘플링하여 NeRF 모델에서 쿼드콥터 모션 플래닝을 위한 방법을 제안한다. 추가적으로 Driess et al. [10]은 그래프 신경망을 활용하여 NeRF를 통해 표현되는 다중 객체 장면에서의 동역학 모델을 학습한다. Li et al. [18]은 주로 기본 형상을 가진 장면에서 푸싱 작업을 수행하고, NeRF와 별도로 학습된 잠재 역학 모델로 파지 및 계획을 접근한다. 이전 작업은 단순하고 정적인 장면[1]에서만 잘 수행되거나 결정론적 역학 모델을 가지고 있다[18]. CARFF는 물체의 잠재적 존재와 알려지지 않은 물체의 움직임을 각각 설명하는 상태 불확실성과 동적 불확실성을 모두 포함하는 복잡한 현실 환경에 초점을 맞춘다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '3D 장면 표현은 문맥적으로 풍부하고 상호작용적인 3D 공간에서 환경을 모델링할 수 있게 하는 중요한 발전을 최근 몇 년 동안 목격했다. 이는 공간 분석을 위한 소프트 점유 그리드 생성 및 객체 검출을 위한 새로운 뷰 합성 등 많은 분석적 이점을 제공한다. 이러한 장점을 고려할 때, 본 연구의 주요 목적은 동적 환경에서 확률적 3차원 장면 예측 모델을 개발하는 것이다. 그러나, NeRF와 VAE와 같은 확률적 모델을 통한 3D 장면 표현의 직접 통합은 종종 비볼록 및 상호 의존적 최적화를 수반하며, 이는 불안정한 트레이닝을 야기한다. 예를 들어, NeRF의 최적화는 유익한 기울기를 제공하도록 구조화되는 VAE의 잠재 공간에 의존할 수 있다.\n' +
      '\n' +
      '이러한 복잡성을 탐색하기 위해 우리의 방법은 훈련 과정을 두 단계로 나눈다(도 3 참조). 먼저, PC-VAE를 학습하여 뷰 불변 장면 표현을 학습한다. 다음으로, 잠재 표현으로부터 3차원 장면을 학습하기 위해 디코더를 NeRF로 교체한다. 잠재 장면 표상은 가능한 기본 장면에 대한 환경 상태와 역학을 포착하는 반면 NeRF는 믿음의 공간 내에서 새로운 관점을 합성하여 관찰되지 않은 장면을 볼 수 있는 능력을 제공한다(그림 2 및 3.1 참조). 예측하는 동안 불확실성은 예측된 가우시안 혼합물로부터 레이턴트를 자동 회귀적으로 샘플링함으로써 모델링될 수 있어 효과적인 의사 결정이 가능하다. 이 범위 내에서, 우리는 장면 예측을 잠재 분포에 대한 부분 관측 가능한 마르코프 결정 과정(POMDP)으로 접근하며, 이는 지각적 불확실성 속에서 계획을 위한 다중 모달 신념을 포착할 수 있게 한다(Sec. 3.2 참조).\n' +
      '\n' +
      '### NeRF Pose-Conditional VAE(PC-VAE)\n' +
      '\n' +
      '아키텍처: 타임스탬프에서의 장면\\(S_{t}\\)이 주어지면, 카메라 포즈\\(c\\)에서 캡처된 자아 중심 관찰 이미지\\(I_{c}^{t}\\)을 렌더링한다. 목적은 장면을 앞으로 진화하는 예측 단계를 수행할 수 있는 이미지의 3D 표현을 공식화하는 것이다. 이를 위해 사후 분포(q_{\\phi}(z|I_{c}^{t})에서 샘플링된 잠재 변수 \\(z\\)에 조건화된 복사 필드를 활용한다. 이제 후방을 학습하기 위해 PC-VAE를 활용한다. 우리는 합성곱 계층과 이미지넷에서 미리 훈련된 ViT를 사용하여 인코더를 구성한다[9]. 인코더는 영상공간에서 평균(\\mu\\)과 분산(\\sigma^{2}\\)에 의해 매개변수화된 가우시안 분포 잠재공간(q_{\\phi}(z|I_{c}^{t})=\\mathcal{N}(\\mu,\\sigma^{2})으로 매핑을 학습한다. 디코더(p(I|z,c)\\)는 카메라 포즈에 맞춰져 있으며, 잠재(z\\sim\\mathcal{N}(\\mu,\\sigma^{2})\\)를 이미지 공간(I\\)에 매핑한다. 이는 인코더가 카메라 포즈 \\(c\\)에 불변인 레이턴트를 생성하는 것을 돕는다.\n' +
      '\n' +
      '3차원 장면 모델링을 가능하게 하기 위해 해시 그리드와 점유 그리드를 결합한 Instant-NGP[24]를 사용한다. 추가로, 더 작은 다층 퍼셉트론(MLP), \\(F_{\\theta}(z)\\)은 다음과 같이 주어진 밀도 및 외관을 모델링하는데 활용될 수 있다:\n' +
      '\n' +
      '\\[F_{\\theta}(z):(\\mathbf{x},\\mathbf{d},z)\\rightarrow((r,g,b),\\sigma) \\tag{1}\\]\n' +
      '\n' +
      '여기서 \\(\\mathbf{x}\\in\\mathbb{R}^{3}\\)과 \\(\\mathbf{d}\\in(\\theta,\\phi)\\)은 각각 위치벡터와 시청방향을 나타낸다. MLP는 또한 샘플링된 장면 잠복기 \\(z\\sim q_{\\phi}(z|I_{c}^{t})\\)(부록 C 참조)에서 조정된다.\n' +
      '\n' +
      '훈련 방법론: 아키텍처만으로는 NeRF-VAE[17]의 유사한 예를 통해 볼 수 있듯이 복잡한 실제 시나리오를 모델링할 수 없다. 우리 작업의 중요한 기여는 훈련을 안정화하는 2단계 훈련 프레임워크이다. 먼저, 재구성을 위해 픽셀 공간에서 컨볼루션 ViT 기반 인코더와 포즈 조건 컨볼루션 디코더를 최적화한다. 이를 통해 의미적으로 풍부한 2D 공간에서 인코딩을 학습함에 따라 보다 복잡하고 사실적인 장면을 다룰 수 있다. 카메라 포즈에서 디코더를 컨디셔닝함으로써, 카메라 뷰 앵글과 장면 컨텍스트 사이의 디엔탠먼트를 달성하여 표현 뷰 불변과 인코더 3D를 인식하게 한다. 다음으로, 풍부한 잠재 표현들이 학습되면, 우리는 냉동 인코더의 잠재 공간에 걸쳐 디코더를 잠재-조건화된 NeRF로 대체한다. NeRF는 새로운 뷰 합성을 위해 인코더 믿음을 3D로 재구성한다.\n' +
      '\n' +
      '손실: 우리의 PC-VAE는 표준 VAE 손실, 평균 제곱 오차(MSE) 및 증거 하한(ELBO)에 의해 주어지는 쿨백 라이블러(KL) 발산을 사용하여 훈련된다:\n' +
      '\n' +
      '\\mathcal{L}_{\\textit{PC-VAE}=\\mathcal{L}_{\\textit{MSE},\\textit{PC-VAE}+\\mathcal{L}_{\\textit{KLD,PC-VAE}= \\tag{2}\\[||p(I|z,c^{\\prime\\prime})-I_{c^{\\prime\\prime}}^{t}||^{2}+\\mathbb{E}_{q(z|I_{c}^{t}}[\\log p(I|z)]\\\\qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq\n' +
      '\n' +
      '도 2: **신규 뷰 기획 애플리케이션**. CARFF는 샘플링된 믿음 예측을 보기 위해 카메라를 움직이는 것만큼 간단한 에고 자동차에서 가려진 뷰 뒤에 추론을 허용하여, 예를 들어 밀도 프로빙 또는 임의의 각도에서 2D 분할 모델을 사용하여 간단한 다운스트림 계획을 허용한다.\n' +
      '\n' +
      '여기서 \\(w_{\\textit{KL}}\\)는 KL 발산 손실 중량을 나타내고 \\(z~{}\\sim~{}q_{\\phi}(z|I_{c}^{t})\\는 본 논문에서 제안한 3D 인식 방법을 구현하기 위해 카메라(c\\)를 이용하여 후방을 부호화하고, 랜덤하게 샘플링된 포즈(c^{\\prime\\prime}\\)에서 디코더를 조정한다.\n' +
      '\n' +
      'KL 발산은 교합 하에서 조건화된 재구성과 확률성의 균형을 맞추기 위해 잠재 공간을 정규화한다. 상승된 KL 발산 손실 중량\\(w_{\\textit{KL}}\\)은 잠복기를 표준 정규 분포인 \\(\\mathcal{N}(0,1)\\에 가깝게 밀어내어 부분 관측 시나리오에서 확률적 샘플링을 보장한다. 그러나 과도한 정규화는 잠복기를 덜 분리 가능하게 하여 모드 붕괴를 초래한다. 이를 완화하기 위해 지연 선형 KL 발산 손실 가중치 스케쥴링을 적용하여 균형적인 \\(w_{\\textit{KL}}\\)을 타격한다.\n' +
      '\n' +
      '다음으로 VAE의 뒤쪽에 있는 NeRF 기반 디코더를 학습하여 장면을 모델링한다. 임의의 타임스탬프\\(t\\)에서, 우리는 다음 식에 의해 주어진 NeRF를 트레이닝하기 위해 표준 픽셀-별 MSE 손실을 사용한다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\textit{MSE, NeRF}}=\\|I_{c}^{t}-\\textit{render}(F_{\\theta}(\\cdot|q_{\\phi}(z|I_{c}^{t}))\\|^{2} \\tag{3}\\]\n' +
      '\n' +
      '우리는 Muller 등이 제안한 표준 렌더링 알고리즘을 사용한다[24]. 다음으로, 포즈 조건 인코더의 학습된 잠재 공간에 대한 예측 모듈을 구축한다.\n' +
      '\n' +
      '### Scene Forecasting\n' +
      '\n' +
      '제형: 현재의 제형은 타임스탬프에 걸쳐 상이한 구성으로 장면을 모델링할 수 있게 한다. 자아 중심 뷰가 주어진 장면의 미래 구성을 예측하기 위해서는 미래 잠재 분포를 예측할 필요가 있다. 예측은 PC-VAE의 잠재공간에서 사후분포\\(q_{\\phi}(z|I_{c}^{t})\\(q_{\\phi}(z|I_{c}^{t})에 대한 부분 관측 가능한 마르코프 결정 과정(POMDP)으로 공식화된다.\n' +
      '\n' +
      '추론하는 동안 우리는 폐색 상태에서 확률적 행동을 관찰하며, 이는 잠재적으로 다른 장면 가능성을 나타내는 여러 가우스 분포의 혼합물을 학습하도록 동기를 부여한다. 따라서 본 논문에서는 다중 헤드 MLP를 갖는 Mixture Density Network (_MDN_)을 사용하여 \\(K\\) Gaussians의 혼합을 예측하는 POMDP를 모델링한다. 임의의 타임스탬프 \\(t\\)에서 분포는 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})=MDN(q_{\\phi}(z_{t-1}|I_{c}^{t-1})) \\tag{4}\\]\n' +
      '\n' +
      '모델은 각 타임스탬프에서 예측된 사후 분포(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})를 학습하기 위해 사후 분포(q_{\\phi}(z_{t-1})에 조건화된다. 예측된 사후 분포는 가우시안 혼합에 의해 주어진다:\n' +
      '\n' +
      '\\[q_{\\phi}^{\\prime}(z_{t})=\\sum_{i=1}^{K}\\pi_{i}~{}\\mathcal{N}(\\mu_{i},\\sigma_{i}^{2}} \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(\\pi_{i}\\), \\(\\mu_{i}\\) 및 \\(\\sigma_{i}^{2}\\)는 사후 분포 내에서 \\(i^{th}\\) 가우시안 분포의 혼합물 중량, 평균 및 분산을 나타낸다. 여기서, \\(K\\)는 가우시안들의 총 수이다. 간결함을 위해 후방(q_{\\phi}(z_{t-1})\\) 및 표본 잠재(z_{t-1}\\)에서 컨디셔닝을 제거한다. 우리는 가우시안(q_{\\phi}^{\\prime}(z_{t})\\)의 혼합물로부터 \\(z_{t}\\)을 샘플링하고, 여기서 \\(z_{t}\\)은 가우시안 모드들 중 하나에 속할 가능성이 가장 높다. 모드에 대응하는 장면 구성은 NeRF에 의해 렌더링되는 3D 장면에 반영된다.\n' +
      '\n' +
      '손실: MDN을 최적화하기 위해 다음과 같이 주어진 음의 로그 우도 함수를 최소화한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\textit{MDN}=-\\sum_{i=1}^{N}log\\left(\\sum_{j=1}^{K}\\pi_{j}\\mathcal{N}(y_{i};\\mu_{j},\\sigma_{j}^{2})\\right) \\tag{6}\\right)\n' +
      '\n' +
      '그림 3: **CARFF의 2단계 훈련 과정을 시각화한다. Left:** 컨볼루션 VIT 기반 인코더는 타임스탬프(t\\), (t^{\\prime}\\) 및 카메라 포즈(c\\), (c^{\\prime}\\)에서 각각의 이미지(I\\)를 가우시안 잠재 분포로 인코딩한다. 두 개의 타임스탬프와 과대모수화된 잠재성을 가정하면, 가우시안 분포 중 하나는 타임스탬프에 걸쳐 더 작은 \\(\\sigma^{2}\\)과 다른 \\(\\mu\\)을 가질 것이다. 상단 오른쪽:** 포즈 조건 디코더는 카메라 포즈 \\(c^{\\prime\\prime}\\)을 이용하여 샘플링된 잠재 \\(z\\)을 확률적으로 이미지 \\(I_{c^{\\prime\\prime}}^{t}\\)과 \\(I_{c^{\\prime\\prime}}^{t}\\)으로 디코딩한다. 디코딩된 복원 영상과 지상진실 영상은 손실(\\mathcal{L}_{\\textit{MSE, PC-VAE}})을 얻기 위해 사용된다. **Lower Right:** NeRF는 최적화된 가우시안 파라미터들로부터 샘플링된 잠재 변수들에 대한 컨디셔닝에 의해 트레이닝된다. 이러한 매개변수는 PC-VAE에서 파생된 별개의 타임스탬프 분포를 특성화한다. NeRF에 대한 MSE 손실은 \\(\\mathcal{L}_{\\textit{MSE, NeRF}}\\)으로 계산된다.\n' +
      '\n' +
      '여기서, \\(y_{i}\\sim q_{\\phi}(z_{t})\\)는 인코더에 의해 학습된 잠재 \\(z_{t}\\)의 분포로부터 샘플링되며, \\(N\\)은 총 샘플 수를 나타낸다.\n' +
      '\n' +
      '추론: 우리는 보이지 않는 자아 중심 이미지를 고려하고 인코더를 통해 그 뒤의 \\(q_{\\phi}(z_{t})\\을 검색한다. 다음으로, 우리는 가능한 미래 후방 분포\\(q^{\\prime}_{\\phi}(z_{t+1})\\을 예측한다. 예측된 후방으로부터, 우리는 잠재된 장면을 샘플링하고 로컬리제이션을 수행한다. 우리는 (a) NeRF를 조사하는 밀도 또는 (b) YOLO[33]과 같은 기성 방법을 사용하여 렌더링된 새로운 뷰를 세그먼트화하여 이를 달성한다(도 2 참조). 이를 통해 인코더 잠재공간에서 가우시안 분포\\(q_{\\phi}(z_{t+1})\\)를 검색할 수 있다. 이것은 다음 타임스탬프를 예측하기 위해 MDN에 자동 회귀식으로 피드백된다. Fig.를 참조한다. 파이프라인의 개요를 위한 도 6.\n' +
      '\n' +
      '## 4 Results\n' +
      '\n' +
      '지각적 불확실성 하에서 의사 결정은 로봇 공학 및 자율 주행, 특히 주행 작업에서 마주치는 부분적으로 관찰 가능한 환경에서 직면하는 만연한 도전이다. 이러한 시나리오에서 잠재적으로 모호한 에이전트의 존재에 대한 정확한 추론이 중요하다. 부분 관찰 가능성이 있는 유사한 실제 상황에 대한 CARFF의 효과를 평가한다. 우리는 CARLA 운전 시뮬레이터 [8]에서 여러 시나리오를 구현했다(도 4 참조). 단일 NVIDIA RTX 3090 GPU는 PC-VAE, NeRF 및 MDN을 훈련하는 데 사용된다. 순차적으로 훈련된 모든 모델은 24시간의 결합된 시간 프레임 내에서 수렴하는 경향이 있다. 자세한 실험 설정은 부록 C에서 찾을 수 있다. 우리는 부분적으로 관찰 가능한 2D 입력이 주어진 CARFF가 완전한 3D 장면을 나타내는 잠재 분포를 예측하는 데 잘 수행함을 보여준다. 이러한 예측을 사용하여 다운스트림 계획 작업을 수행하기 위한 CARFF 기반 컨트롤러를 설계한다.\n' +
      '\n' +
      '### Data Generation\n' +
      '\n' +
      '본 논문에서 제안한 방법의 강인성을 검증하기 위해 에고 객체와 다양한 액터 객체를 포함하는 데이터셋을 서로 다른 구성으로 생성한다. 우리는 (a) 단순하고 제어 가능한 시뮬레이션을 위한 합성 블렌더 데이터세트와 (b) 복잡한 실제 시나리오에 대한 CARLA 기반 주행 데이터세트에 대한 실험을 수행한다[8].\n' +
      '\n' +
      '블렌더 합성 데이터세트: 이것은 존재할 수 있거나 존재하지 않을 수 있는 적색 실린더(액터)를 수반하는 정지된 청색 큐브(예를 들어)로 구성된다(도 5 참조). 행위자가 존재하는 경우 그림 5와 같이 측면 움직임을 나타낸다. 이 단순한 설정은 모델을 평가할 수 있는 해석 가능한 프레임워크를 제공한다.\n' +
      '\n' +
      'CARLA 데이터세트: 각 데이터세트는 \\(N\\) 타임스탬프에 대해 시뮬레이션되고 \\(C=100\\) 미리 정의된 카메라 포즈를 사용하여 전체 관찰, 부분 관찰 및 가시성이 없는 환경에서 환경의 이미지를 캡처한다. 이러한 데이터 세트는 Active Visual Planning[25]과 같은 관련 작업에서 제안된 상태 불확실성을 포함하는 일반적인 주행 시나리오를 기반으로 모델링된다.\n' +
      '\n' +
      '단일 장면 접근 교차점 에고 차량은 균등하게 이격된 미리 정의된 궤적을 따라 교차로를 횡단하는 행위자 차량과 T 교차점에 위치한다. 우리는 이것을 \\(N=10\\) 타임스탬프에 대해 시뮬레이션한다. 우리는 주로 이 데이터 세트를 사용하여 전체 관찰에서 타임스탬프의 진화를 예측한다.\n' +
      '\n' +
      '_b) Multi-Scene Approaching Intersection:_ 우리는 액터 차량의 존재를 확률적으로 함으로써, 전술한 시나리오를 상태 불확실성을 갖는 보다 복잡한 설정으로 확장한다. 두 가지 가능성 모두에 대해 유사한 교차 교차가 \\(N=3\\) 타임스탬프에 대해 시뮬레이션된다. 자아차량의 행위자 관점은 시간스탬프 상에서 T-교차점에 접근함에 따라 가려질 수 있다. 자아 차량은 분기점에서 전진하거나 정지할 수 있다(도 4 참조).\n' +
      '\n' +
      '다중-장면 다중-액터 2차선 병합을 통해 환경 동역학 불확실성을 더하기 위해, 병합된 두 차선의 교차점에서의 다중-액터 설정을 고려한다. 부분 폐색이 있는 교차점에서 두 번째로 접근하는 액터가 가변 속도를 갖는 시나리오를 시뮬레이션한다. 여기서, 상기 자차량은 좌측으로 병합될 수 있다\n' +
      '\n' +
      '그림 4: **다중 장면 CARLA 데이터 세트**. 다중 장면 2차선 병합 데이터 세트(**왼쪽**) 및 다중 장면 접근 교차로 데이터 세트(**오른쪽**)에 대한 다양한 자동차 구성 및 장면을 나타내는 이미지.\n' +
      '\n' +
      '도 5: **Blender dataset**. 확률론적 시간적 움직임을 나타내는 잠재적인 빨간색 실린더와 함께 고정된 파란색 큐브가 있는 단순 블렌더 데이터 세트이다. 상이한 카메라 포즈들은 움직임이 상이한 카메라 각도들로부터의 가능한 폐색들에 기초하여 어떻게 확률적으로 모델링될 필요가 있는지를 보여준다.\n' +
      '\n' +
      '두 번째 배우가 지나기 전 또는 모든 배우가 지나간 후, (도 4 참조) 각 브랜치는 \\(N=3\\) 타임스탬프에 대해 시뮬레이션된다.\n' +
      '\n' +
      '### CARFF Evaluation\n' +
      '\n' +
      '우리의 모델로부터 바람직한 행동은 부분적으로 관찰될 수 있는 주어진 자아 중심 이미지와 일치하는 가능한 장면들의 완전한 세트를 예측해야 한다는 것이다. 이는 잠재적인 위험에 기반한 전략적 의사 결정을 보장하기 때문에 예측할 수 없는 환경에서 자율 주행에 중요하다. 이를 위해서는 풍부한 PC-VAE 잠재 공간, 고품질 신규 뷰 합성 및 미래 타임스탬프에서 잠복기의 자동 회귀 확률 예측이 필요하다. 우리는 간단한 합성 블렌더 기반 데이터 세트와 각 CARLA 기반 데이터 세트에 대해 CARFF를 평가한다.\n' +
      '\n' +
      '블렌더 데이터 세트에 대한 평가: 그림 1에서. 도 5를 참조하면, 장면 1a 및 1b 모두에 대해, 본 모델은 왼쪽 시야각을 고려하여 실린더의 측면 이동이 시간의 약 50%에 위치하도록 정확하게 예측한다. 장면 2에서 입력 카메라 각도에 빨간색 실린더가 없는 경우 모델은 빨간색 실린더의 잠재적 존재를 시간의 약 50%로 예측하고 대략 동일한 확률로 측면 움직임을 예측한다. 이것은 인간의 직관과 일치하는 잠재 공간에 가려진 것을 예측하고 추론하는 PC-VAE의 능력을 검증한다. 블렌더 데이터 세트의 간단한 장면 내에서 입증된 유사한 직관은 CARLA 데이터 세트에서 시뮬레이션된 주행 시나리오로 옮겨질 수 있다.\n' +
      '\n' +
      'PC-VAE 성능 및 개선: 다양한 인코더 구조를 가진 CARLA 데이터 세트에서 PC-VAE의 성능을 평가한다. PC-VAE가 잠재적으로 부분적으로 관찰 가능한 입력이 주어진 가변 장면, 액터 구성 및 환경 노이즈를 포함하는 복잡한 환경을 효과적으로 재구성한다는 것을 보여준다(도 9 참조). 학습 데이터와 새로운 뷰 인코더 입력에 대한 평균 Peak Signal-to-Noise Ratio (PSNR)를 계산하였다. 인코더에서 생성된 잠재 공간의 품질을 평가하기 위해 t-SNE[47] 플롯을 활용하여 주어진 데이터셋에서 각 이미지에 대한 잠재 샘플의 분포를 시각화한다(부록 E 참조). 시각화된 클러스터링을 정량적으로 측정하기 위해 Support Vector Machine (SVM) [13] 기반 메트릭을 도입하였으며, 여기서 더 높은 값은 타임스탬프를 기반으로 더 나은 클러스터링을 나타낸다. 대부분의 잠재 장면 샘플은 타임스탬프에 의해 분리될 수 있으며, 이는 잠복기가 뷰 불변임을 나타낸다. 오분류되거나 경계에 있는 샘플은 일반적으로 부분적으로 또는 완전히 폐색된 영역을 나타낸다. 이것은 우리가 이러한 표본에 대한 확률적 행동을 모델링할 수 있기 때문에 예측에 바람직하다. 이 과정에서 KL 발산 가중치 스케줄링의 밸런싱은 PC-VAE의 잠재 공간 및 재구성의 품질을 유지한다(부록 C 참조). 결과는 탭에 나와 있습니다. 2는 다른 제형에 비해 PC-VAE 인코더 아키텍처의 이점을 입증한다. 구체적으로, 비조건적 VAE는 이미지를 재구성하고 기본 3D 구조를 캡처하지 않기 때문에 SVM 정확도에서 실패한다. 냉동 중량이 없는 바닐라 PC-VAE 및 PC-VAE는 여러 하이퍼 파라미터의 세심한 미세 조정이 필요하며 급격한 카메라 움직임에 잘 일반화되지 않는다. 실험을 통해 제안된 모델이 폐색 상태에서 잠재 표상을 통해 확률적 특성을 유지하면서 동시에 정확한 재구성을 보장할 수 있음을 보여준다.\n' +
      '\n' +
      '3D 새로운 관점 합성: 잠재적으로 부분적인 관찰을 가진 보이지 않는 자아 중심 관점을 감안할 때, 우리의 방법은 3D에서 가능한 모든 현재 상태 신념과 믿음을 유지한다.\n' +
      '\n' +
      '도 6: ** 장면 예측**에서의 자동-회귀 추론. Timestamp \\(t\\), \\(I_{c}^{t}\\)에서의 입력 영상은 PC-VAE로부터 미리 학습된 인코더를 사용하여 인코딩된다. 해당 잠재 분포는 가우시안들의 혼합을 예측하는 혼합 밀도 네트워크에 공급된다. 각각의 \\(K\\) 가우시안들은 다음 타임스탬프에서 서로 다른 믿음에 대응할 수 있는 잠재 분포이다. 예측된 잠재신념에 대해 가우시안 혼합을 반복적으로 샘플링하여 \\(I_{c^{\\prime},scn^{\\prime}^{t+1}\\)으로 시각화하여 잠재적으로 \\(i\\)번째 가능한 결과를 나타낸다. 이것은 NeRF를 컨디셔닝하여 장면의 3D 뷰들을 생성하는 데 사용된다. 자기회귀 예측을 수행하기 위해 차량의 위치에 대한 NeRF를 조사하고 이 정보를 미리 훈련된 인코더에 피드백하여 다음 타임스탬프에서 장면을 예측한다.\n' +
      '\n' +
      '각각의 믿음에 대해 임의의 카메라 각도에서 새로운 뷰를 완벽하게 재구성합니다. 도. 도 2는 CARFF가 보유하는 가능한 3D 믿음들 중 하나를 예시한다. 이는 새로운 뷰 합성을 위해 뷰 일관성 있는 방식으로 사용될 수 있는 3D 믿음을 생성하는 우리의 방법의 능력을 보여준다. 예측 기반 계획과 같은 응용 프로그램에는 정확하고 완전한 3D 환경 이해 능력을 달성하는 모델의 능력이 중요하다.\n' +
      '\n' +
      '전체 및 부분 관찰에서 추론:전체 관찰에서 MDN을 사용하여 세 데이터 세트 모두에서 후속 자동차 위치를 예측한다. PSNR 값들은 상이한 타임스탬프들에 걸친 장면의 조류-눈 뷰 NeRF 렌더링들 및 그라운드 진실 조류-눈 뷰 이미지들에 기초하여 계산된다. 탭에서 1은 각 타임스탬프에서 지상진실 영상을 사용하여 예측된 후방에서 렌더링된 영상에 대한 PSNR 값을 보고한다. 우리는 또한 그림 8에 주어진 정확도 곡선을 사용하여 예측 모델의 효능을 평가한다. 이는 배우(들) 현지화 결과를 기반으로 잘못된 예측을 생성하지 않고 안정적인 믿음을 생성하는 CARFF의 능력을 나타낸다. 각 표본수(n=0\\)에서 \\(n=50\\) 사이의 표본수(n=50\\)에 대해, 우리는 완전히 관찰할 수 있는 \\(3\\)의 에고 이미지의 랜덤 서브세트를 선택하고 정확도의 평균을 취한다. 몇 가지 그럴듯한 시나리오가 존재하는 부분 관찰 가능한 자아 중심 이미지가 있는 시나리오에서는 유사한 설정을 사용하여 정확성 대신 리콜을 활용한다. 이를 통해 잠재적 위험에 대한 잘못된 부정적인 예측을 피할 수 있는 인코더의 능력을 평가할 수 있다.\n' +
      '\n' +
      '도. 도 8은 우리의 모델이 두 데이터 세트 모두에서 높은 정확도와 재현율을 달성하여 상태 불확실성(교차로의 접근) 및 동적 불확실성(2차선 병합)을 모델링하는 능력을 보여준다. 결과는 재샘플링의 무작위성에 대한 CARFF의 회복력과 신뢰 공간의 확률적 모델링의 완전성을 나타낸다. 이러한 관찰을 감안할 때 이제 복잡한 시나리오를 계획하고 탐색할 수 있는 신뢰할 수 있는 컨트롤러를 구축한다.\n' +
      '\n' +
      '### Planning\n' +
      '\n' +
      '우리의 모든 실험에서, 자아 매개체는 특정 관찰 가능성 하에서 전진하기 위한 결정을 내려야 한다. 시나리오는 에고 뷰가 부분 폐색을 포함하고 일부 시나리오에서 행위자의 상태가 불확실하도록 설계된다. CARFF를 이용한 의사결정을 용이하게 하기 위해, 본 논문에서는 자아 중심 입력 영상을 취하여 액션을 출력하는 제어기를 설계한다. 혼합물 밀도 네트워크에서 샘플 일관성을 통합하는 결정이 이루어집니다. 예를 들어, 컨트롤러는 폐색을 추론하고 에고카를 에고카에 홍보합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline \\begin{tabular}{c} Ground Truth \\\\ Prediction Pair \\\\ \\end{tabular} & \\begin{tabular}{c} Avg. PSNR \\\\ (Scene 1) \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} Avg. PSNR \\\\ (Scene 2) \\\\ \\end{tabular} \\\\ \\hline \\multicolumn{3}{l}{**Single-Scene Approaching Intersection**} \\\\ \\hline Matching Pairs & **29.06** & N.A \\\\ Un-matching Pairs & 24.01 & N.A \\\\ \\hline \\multicolumn{3}{l}{**Multi-Scene Approaching Intersection**} \\\\ \\hline Matching Pairs & **28.00** & **28.26** \\\\ Un-matching Pairs & 23.27 & 24.56 \\\\ \\hline \\multicolumn{3}{l}{**Multi-Scene Two Lane Merge**} \\\\ \\hline Matching Pairs & **28.14** & **28.17** \\\\ Un-matching Pairs & 22.74 & 23.32 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 완전히 관찰 가능한 3D 예측에 대한 **평균 PSNR**. CARFF는 각 데이터 세트에 대한 모든 타임스탬프에 걸쳐 장면 진화를 정확하게 예측한다. 평균 PSNR은 예측치\\(\\hat{I}_{t_{i}}\\)과 대응접지진리 \\(I_{t_{i}}\\)에 대해 상당히 높다. 잘못된 대응 관계인 \\(\\hat{I}_{t_{i}},I_{t_{j}}\\)에 대한 PSNR 값은 주변 환경과 일치하는 결과이다. 예측의 전체 표는 부록 E에 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline \\begin{tabular}{c} Encoder Architectures \\\\ \\end{tabular} & \\begin{tabular}{c} Train \\\\ PSNR \\\\ \\end{tabular} & \\begin{tabular}{c} SVM \\\\ Accuracy \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} NV \\\\ PSNR \\\\ \\end{tabular} \\\\ \\hline PC-VAE & **26.30** & **75.20** & **25.24** \\\\ PC-VAE w/o CL & 26.24 & 70.60 & 24.80 \\\\ Vanilla PC-VAE & 26.02 & 25.70 & 24.65 \\\\ PC-VAE w/o Freezing & 24.57 & 5.80 & 24.60 \\\\ PC-VAE w/ MobileNet & 17.14 & 19.70 & 17.16 \\\\ \\hline Vanilla VAE & 24.15 & 10.60 & 11.43 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **PC-VAE 메트릭 및 삭마**. CARFF의 PC-VAE 인코더는 이미지 재구성 및 포즈 조절 모두에서 다른 인코더 아키텍처를 능가한다. 우리는 Conv Layer가 없는 PC-VAE, 바닐라 인코더가 있는 PC-VAE, ViT에서 동결 가중치가 없는 PC-VAE, ViT를 사전 훈련된 모바일넷으로 대체하는 PC-VAE 및 비포즈 조건 바닐라 VAE의 삭제를 평가했다. 표에는 잠재 타임스탬프 예측을 위한 평균 학습 PSNR, 신규 뷰(NV) 입력 PSNR 및 SVM 정확도가 표시된다.\n' +
      '\n' +
      '도 7: **PC-VAE 재구성**. 다른 타임스탬프에서 카메라 포즈(c\\)에서 바라본 다른 지상진리 영상들 중에서, 인코더 입력인 \\(I^{t}_{c}\\)은 타임스탬프를 존중하는 새로운 포즈(c^{\\prime\\prime}\\)에 걸쳐 재구성되어 \\(I^{t}_{c^{\\prime\\prime}\\)을 생성한다. 전체 그리드는 부록 E에 있습니다.\n' +
      '\n' +
      '샘플에서 배우의 존재와 부재 사이에서 장면이 번갈아 나올 때 일시 중지합니다. 두 개의 다중 장면 데이터 세트를 사용하여 CARFF 기반 컨트롤러의 성능을 평가하는데 잠재적으로 알려지지 않은 행동을 가진 행위자가 포함되어 있기 때문이다.\n' +
      '\n' +
      '효과적인 제어기를 설계하기 위해서는 정확도와 재현율 사이의 균형을 찾아야 한다(도 8 참조). 과도한 샘플링에서 낮은 정확도는 예측된 상태에서 원하지 않는 무작위성을 의미한다. 그러나 불충분한 샘플을 채취하면 낮은 회상이 생성되며, 즉 모든 그럴듯한 상태를 복구하지 못한다. 이것은 환경에 존재하는 그럴듯한 불확실성을 설명할 수 없기 때문에 잘못된 예측으로 이어질 것이다. 균형을 찾기 위해, 우리는 \\(n=2,10,35\\) 샘플을 생성하는 샘플링 전략을 선택하는 개방 루프 계획 제어기를 설계하며, 여기서 \\(n\\)은 피크 성능을 조정하기 위한 하이퍼파라미터이다.\n' +
      '\n' +
      '정확도(n=2\\)와 재현율(35\\)의 경계에 있는 샘플링 값들에 대해, CARFF 기반 제어기는 더 낮은 성공률을 얻는 반면, \\(n=10\\)은 가장 좋은 결과를 얻는다는 것을 알 수 있다. 탭의 두 데이터 집합에 걸쳐 있습니다. 3, 과신성 제어기는 트럭이 접근하는 경우 폐색을 조심스럽게 설명하지 않기 때문에 필연적으로 충돌을 경험할 것이다. 반면에 지나치게 신중한 접근은 정체를 초래하여 컨트롤러가 현장에서 전진하는 능력을 억제한다. CARFF 기반 컨트롤러를 사용한 미묘한 의사 결정은 복잡하고 예측할 수 없는 도로 환경에 적응하여 안전성과 효율성을 향상시켜 자율 주행 차량에서 보다 신뢰할 수 있고 인간과 유사한 반응을 육성하기 때문에 주행 시나리오에서 특히 중요하다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '제한점: 다른 NeRF 기반 방법과 마찬가지로 CARFF는 현재 도로 교차점과 같은 특정 장면의 포즈 이미지에 의존하여 보이지 않는 환경에 대한 직접적인 적용 가능성을 제한한다. 그러나 교차로의 교통 카메라와 같은 인구 밀집 지역 주변의 카메라 배치가 증가함에 따라 일반화 가능성이 향상될 것으로 예상한다. 또한, 매우 많은 수의 배우와 함께 매우 복잡한 역학을 처리하는 것은 여전히 우리의 방법에 대한 도전을 제기하며, 정확도에 대한 포괄적인 역학 모델링의 균형을 맞추기 위해 세심한 미세 조정이 필요하다. 가까운 미래에 잠재적으로 더 강력한 모델은 이와 관련하여 추가 개선을 위한 유망한 방법을 제공할 수 있다.\n' +
      '\n' +
      '결론: 부분 관측으로부터 확률적 3차원 장면 예측을 위한 새로운 방법인 CARFF를 제시하였다. 포즈 조건 VAE(Pose-Conditional VAE), 학습된 후방의 NeRF(NeRF) 및 미래 장면을 예측하는 혼합 밀도 네트워크를 사용하여, 계획 수립에 중요한 폐색 영역에서 상태 및 역학 불확실성이 있는 복잡한 실제 환경을 효과적으로 모델링한다. 우리는 실제 자율주행 시나리오에서 본 방법의 성능을 입증했으며, 완전한 관찰 하에 환경의 고충실도 3D 재구성을 제공하는 미래로 예측할 수 있는 반면 불완전한 장면 정보가 주어진 잠재적 위험에 대한 완전한 리콜을 유지한다. 전반적으로 CARFF는 구조화되지 않은 환경에서 비전 알고리즘에 매우 유용할 수 있는 불확실성 하에서 인식, 예측 및 행동에 대한 직관적이고 통합된 접근법을 제공한다.\n' +
      '\n' +
      '그림 8: **다중 장면 데이터 세트의 정확도와 예측된 믿음으로부터의 회상 곡선. 자아 중심 이미지 입력에서 MDN의 예측 잠재 분포로부터 \\(n=1\\) 및 \\(n=50\\) 표본에 걸쳐 프레임워크를 테스트한다. 샘플 수\\(n\\)에 걸쳐 부분 관찰(리콜)에서 생성된 이상적인 믿음의 상태 범위와 전체 관찰(정확도)에서 샘플링된 올바른 믿음의 비율(정확도)을 달성한다. 샘플 수를 크게 늘리면 잠재 분포 리샘플링의 랜덤성으로 인해 정확도가 감소하기 시작한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r} \\hline \\hline \\multicolumn{3}{l}{**Multi-Scene Approaching Intersection**} \\\\ Controller Type & Actor Exists & No Actor \\\\ \\hline Underconfident & \\(30/30\\) & \\(0/30\\) \\\\ Overconfident & \\(0/30\\) & \\(30/30\\) \\\\ CARFF (\\(n\\) = 2) & \\(17/30\\) & \\(30/30\\) \\\\\n' +
      '**CARFF (\\(n\\) = 10)** & \\(\\mathbf{30/30}\\) & \\(\\mathbf{30/30}\\) \\\\ CARFF (\\(n\\) = 35) & \\(30/30\\) & \\(19/30\\) \\\\ \\hline \\hline \\multicolumn{3}{l}{**Multi-Scene Two Lane Merge**} \\\\ Controller Type & Fast Actor & Slow Actor \\\\ \\hline Underconfident & \\(30/30\\) & \\(0/30\\) \\\\ Overconfident & \\(0/30\\) & \\(30/30\\) \\\\ CARFF (\\(n\\) = 2) & \\(21/30\\) & \\(30/30\\) \\\\\n' +
      '**CARFF (\\(n\\) = 10)** & \\(\\mathbf{30/30}\\) & \\(\\mathbf{30/30}\\) \\\\ CARFF (\\(n\\) = 35) & \\(30/30\\) & \\(22/30\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 샘플링 수가 다양한 컨트롤러를 사용하여 3D로 **계획. CARFF 기반 컨트롤러는 30번의 시도에서 성공률에서 기준선을 능가합니다. \\(n\\)=10의 경우, CARFF 기반 제어기는 잠재적인 충돌 시나리오에서 최적의 동작을 일관되게 선택한다. 배우가 존재하고 빠른 연기 장면을 위해, 우리는 충돌을 피하는 CARFF의 능력을 테스트하기 위해 폐쇄된 자아 중심 입력을 고려한다. 무액터 장면과 느린 액터 장면에 대해 상태 관찰 가능성을 고려하고 발전하기 위한 최적의 동작을 인식하는 컨트롤러의 능력을 테스트한다. 일관성을 유지하기 위해 30번의 시행에 걸쳐 하나의 단일 이미지 입력을 사용한다.**\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, and Mac Schwager. Vision-only robot navigation in a neural radiance world. _IEEE Robotics and Automation Letters_, 7(2):4606-4613, 2022.\n' +
      '* [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _Int. Conf. Comput. Vis._, pages 5855-5864, 2021.\n' +
      '* [3] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 130-141, 2023.\n' +
      '* [4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. Instance-aware predictive navigation in multi-agent environments. In _IEEE Int. Conf. on Robotics and Automation_, pages 5096-5102. IEEE, 2021.\n' +
      '* [5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction, 2023.\n' +
      '* [6] Felipe Codevilla, Eder Santana, Antonio M Lopez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In _Int. Conf. Comput. Vis._, pages 9329-9338, 2019.\n' +
      '* [7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12882-12891, 2022.\n' +
      '* [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In _Conf. on Robot Learning_, pages 1-16, 2017.\n' +
      '* [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _Int. Conf. Learn. Represent._, 2021.\n' +
      '* [10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object dynamics with compositional neural radiance fields. _arXiv preprint arXiv:2202.11855_, 2022.\n' +
      '* [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5501-5510, 2022.\n' +
      '* [12] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In _AAAI_, 2015.\n' +
      '* [13] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. _IEEE Intelligent Systems and their applications_, 13(4):18-28, 1998.\n' +
      '* [14] Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, and Ken Goldberg. Dex-nerf: Using a neural radiance field to grasp transparent objects. _arXiv preprint arXiv:2110.14217_, 2021.\n' +
      '* [15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, and Marco Pavone. Mats: An interpretable trajectory forecasting representation for planning and control. In _Conf. on Robot Learning_, 2021.\n' +
      '* [16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and Ken Goldberg. Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In _Conf. on Robot Learning_, 2022.\n' +
      '* [17] Adam R Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Sona Mokra, and Danilo Jimenez Rezende. NeRF-VAE: A geometry aware 3d scene generative model. pages 5742-5752, 2021.\n' +
      '* [18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene representations for visuomotor control. In _Conf. on Robot Learning_, pages 112-123, 2022.\n' +
      '* [19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Devrf: Fast deformable voxel radiance fields for dynamic scenes. In _Adv. Neural Inform. Process. Syst._, pages 36762-36775, 2022.\n' +
      '* [20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, 2023.\n' +
      '* [21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Christian Wolf. Multi-object navigation with dynamically learned neural implicit representations. In _Int. Conf. Comput. Vis._, pages 11004-11015, 2023.\n' +
      '* [22] Rowan McAllister and Carl Edward Rasmussen. Data-efficient reinforcement learning in continuous state-action gaussian-pomdps. In _Adv. Neural Inform. Process. Syst._, 2017.\n' +
      '* [23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _Eur. Conf. Comput. Vis._, 2020.\n' +
      '* [24] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multi-tiresolution hash encoding. _ACM Trans. Graph._, 41(4):1-15, 2022.\n' +
      '* [25] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAllister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learning a planner contingent on perceptual uncertainty. In _Conf. on Robot Learning_, 2022.\n' +
      '* [26] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAllister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learning a planner contingent on perceptual uncertainty. In _Conf. on Robot Learning_, pages 1607-1617, 2023.\n' +
      '* [27] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual to real reinforcement learning for autonomous driving. _arXiv preprint arXiv:1704.03952_, 2017.\n' +
      '* [28] Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes. _Mathematics of operations research_, 12(3):441-450, 1987.\n' +
      '\n' +
      '* [29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In _Int. Conf. Comput. Vis._, pages 5865-5874, 2021.\n' +
      '* [30] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. _ACM Trans. Graph._, 2021.\n' +
      '* [31] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point-based value iteration: An anytime algorithm for pomdps. In _IJCAI_, pages 1025-1032, 2003.\n' +
      '* [32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2020.\n' +
      '* [33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2016.\n' +
      '* [34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A Wright, Rowan McAllister, Joseph E Gonzalez, and Sergey Levine. Contingencies from observations: Tractable contingency planning with learned behavior models. In _IEEE Int. Conf. on Robotics and Automation_, pages 13663-13669, 2021.\n' +
      '* [35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Niessner. Dense depth priors for neural radiance fields from sparse input views. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12892-12901, 2022.\n' +
      '* [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge, 2015.\n' +
      '* [37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 16632-16642, 2023.\n' +
      '* [38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5459-5469, 2022.\n' +
      '* [39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In _Adv. Neural Inform. Process. Syst._, pages 7537-7547, 2020.\n' +
      '* [40] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8248-8258, 2022.\n' +
      '* [41] Omer Sahin Tas and Christoph Stiller. Limited visibility and uncertainty aware motion planning for automated driving. In _IEEE Intelligent Vehicles Symposium (IV)_, 2018.\n' +
      '* [42] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi, T. Simon, C. Theobalt, M. Niessner, J. T. Barron, G. Wetzstein, M. Zollhofer, and V. Golyanik. Advances in Neural Rendering. _Comput. Graph. Forum_, 2022.\n' +
      '* [43] Marin Toromanoff, Emilie Wrobel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 7153-7162, 2020.\n' +
      '* [44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer, Aljaz Bozic, Christoph Lassner, and Christian Theobalt. Scenerflow: Time-consistent reconstruction of general dynamic scenes. In _International Conference on 3D Vision (3DV)_, 2023.\n' +
      '* [45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4190-4200, 2023.\n' +
      '* [46] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale renfs for virtual fly-throughs. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12922-12931, 2022.\n' +
      '* [47] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. _Journal of Machine Learning Research_, 9:2579-2605, 2008.\n' +
      '* [48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4690-4699, 2021.\n' +
      '* [49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5438-5448, 2022.\n' +
      '* [50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In _IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1323-1330. IEEE, 2021.\n' +
      '* [51] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images, 2021.\n' +
      '\n' +
      '칼라 데이터세트\n' +
      '\n' +
      '단일 장면 접근 교차로, 다중 장면 접근 교차로 및 다중 장면 2차선 병합에 대한 장면 간의 배우 및 자아 구성의 완전한 그림과 타임스탬프의 진행이 그림에서 시각화된다. 14.\n' +
      '\n' +
      '## 부록 B 관련 업무 비교\n' +
      '\n' +
      '이 섹션에서는 CARFF와 모델과 유사한 작업을 수행하는 다른 방법 간의 비교를 그린다. 그러나 이러한 방법은 우리가 달성하려는 목표와 정확하게 일치하지 않는다. 예를 들어, 일부 방법은 3D 추론을 통합하는 반면, 다른 방법은 이러한 측면을 생략할 수 있다. 본 연구에서 제시한 모형과 기존의 모형과의 타당한 비교를 위해, Tab. 4에 기술되어 있는 정성적인 차이에 대한 심층적인 분석을 수행하였고, 주로 환경에서의 불확실성(상태 및 역학)을 모델링하고 예측을 수행하는 능력을 바탕으로 다른 NeRF 모형과 비교하였다. 우리의 작업은 나열된 모든 이전 작업을 능가하며 기능 면에서 2D 기반 예측 접근법과 동등하다[25]. 이 비교는 우리의 모델이 인간처럼 폐색된 것에서 추론에 존재해야 하는 주요 질적 요인을 포괄적으로 포함한다는 것을 강조한다.\n' +
      '\n' +
      '## 부록 C 구현 상세\n' +
      '\n' +
      '### Pose-Conditional VAE\n' +
      '\n' +
      '구조: 우리는 표준 PyTorch VAE 프레임워크 위에 PC-VAE를 구현한다. 컨벌루션 레이어가 있는 인코더는 단일 컨벌루션 레이어와 ImageNet[36]에서 미리 학습된 Vision Transformer(ViT) Large 16[9]로 대체된다. ViT 출력의 크기\\(1000\\)을 잠재 차원의 크기 8에 대한 평균과 분산으로 투영하기 위해 완전히 연결된 레이어를 수정하고, 훈련 중에 데이터 로더는 정수 값으로 표현되는 카메라 앵글의 포즈를 반환한다. 이 값은 디코더로 전달되기 전에, 원-핫 인코딩되고 재-파라미터화된 인코더 출력들에 연결된다. 디코더 입력 크기는 추가 포즈 정보를 수용하기 위해 포즈들의 수를 추가하기 위해 증가된다.\n' +
      '\n' +
      '최적화: 모든 실험을 위해 단일 RTX 3090 그래픽 카드를 사용합니다. PC-VAE 모델은 이 GPU를 사용하여 수렴하는 데 약 22시간 걸린다. 이 단계에서 잠재 크기, 학습 속도 및 KL 발산 손실 가중치를 포함한 다양한 하이퍼파라미터를 조정하여 모델에 맞춘 최적의 학습을 설정한다(탭 5 참조). CARLA [8] 시뮬레이터 내에서 생성된 다양한 액터 구성 및 시나리오에 최적화하기 위해 각 데이터 세트에 대해 하이퍼파라미터를 약간 다르게 조정한다.\n' +
      '\n' +
      '잠재 공간에서의 포즈 컨디셔닝의 효과적인 재구성과 래턴트의 정규화 사이의 적절한 균형을 찾기 위해 학습률(LR) 및 KL 발산(KLD) 가중치를 조정한다. 정규화는 위성을 가우시안 분포로 밀어내고, 과대모수화된 잠재공간에서 비표현위성을 표준정규로 유지한다. 이는 샘플링 과정을 안정화하고 폐색 시 잠재 샘플의 확률적 거동을 보장한다. 이 균형을 달성하기 위해 선형 KLD 가중치 스케줄러를 사용하며, 여기서 가중치는 KLD 증분 시작 에폭에 대해 낮은 값으로 초기화된다(탭 5 참조). 이를 통해 모델은 초기에 매우 정확한 조건부 재구성을 달성하는 데 집중할 수 있다. 그런 다음 KLD 가중치는 KLD 증가 종료 에폭에 도달할 때까지 꾸준히 증가하여 부분 관찰 가능성 하에서 확률적 거동을 보장한다.\n' +
      '\n' +
      '### 혼합 밀도 네트워크\n' +
      '\n' +
      '혼합 밀도 네트워크(MDN)는 잠재 분포(q_{\\phi}(z_{t-1}|I_{c}^{t-1})의 평균과 분산을 취하여 추정된 사후 분포를 다중 헤드 MLP를 통해 가우시안 \\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})의 혼합으로 출력한다.\n' +
      '\n' +
      '구조: 공유 백본은 단순히 \\(2\\) 완전 연결 레이어와 \\(512\\)의 은닉 레이어 크기를 갖는 ReLU(Rerectified Linear Unit) 활성화를 포함한다. 2\\(2\\) 층이 완전히 연결된 추가 헤드를 사용하여 \\(\\mu_{i}\\) 및 \\(\\sigma_{i}^{2}\\)을 생성했다. 혼합물 중량인 \\(\\pi_{i}\\)는 \\(3\\) 층 MLP 네트워크에서 생성된다. 우리는 가우시안 수를 \\(K=2\\)으로 제한한다.\n' +
      '\n' +
      '최적화: 배치 크기(128\\)와 초기 LR(0.005\\)을 사용하여 30,000\\ 에폭에 대한 네트워크를 학습하고 LR 감쇠를 적용하여 학습을 최적화한다. GPU를 사용하여 훈련하는 데 약 30분이 걸립니다. 트레이닝 동안, 데이터 로더는 현재 타임스탬프 및 인덱싱된 뷰에서의 평균 및 분산들, 및 다음 타임스탬프에 대한 평균 및 분산들을 랜덤하게 샘플링된 이웃 뷰에서 출력한다. 이것은 MDN이 가우시안들의 혼합물로서, 폐색된 뷰들이 잠재적으로 폐색되지 않은 이웃 뷰들로부터 가능한 모든 구성들로 어떻게 전진하는지를 학습할 수 있게 한다.\n' +
      '\n' +
      '각 반복에서, 지면진리분포에 대한 예측분포(q_{\\phi}(z_{t}|I_{c}^{t-1})\\(q_{\\phi}(z_{t}|I_{c}^{t})\\)의 혼합물로부터 추출한 표본에 대해 음의 로그우도손실을 계산한다. MDN이 훈련되는 동안, 추가적인 가우시안 잡음은 \\(\\epsilon\\sim\\mathcal{N}(0,\\sigma^{2})\\)으로 주어지며, 여기서 \\(\\sigma\\in[0.001,0.01]\\)은 현재 타임스탬프 \\(t-1\\)의 평균과 분산에 추가된다. 가우스 잡음과 LR 감쇠는 과적합을 방지하고 움직이는 나무, 움직이는 물 등과 같은 환경 인공물에 대한 모델 민감도를 줄이는 데 도움이 된다.\n' +
      '\n' +
      '### NeRF\n' +
      '\n' +
      'Architecture: Instant-NGP의 기존 PyTorch 구현을 활용하여 NeRF 디코더를 구현한다[24]. 우리는 Instant-NGP 아키텍처의 두 부분의 입력에 잠복기를 연결한다: 부피 밀도\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{Method} & \\multirow{2}{*}{3D} & \\multicolumn{2}{c}{Realistic} & \\multicolumn{2}{c}{State} & \\multicolumn{2}{c}{Dynamics} & \\multicolumn{1}{c}{Code} \\\\  & & Application & Uncertainty & Uncertainty & Prediction & Planning & Released \\\\ \\hline CARFF & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ NeRF-VAE [17] & ✓ & & ✓ & & & & \\\\ NeRF for Visuomotor Control [18] & ✓ & ✓ & & & ✓ & ✓ & \\\\ NeRF Navigation [1] & ✓ & ✓ & & & & ✓ & ✓ \\\\ \\hline AVP [25] & & ✓ & ✓ & ✓ & ✓ & ✓ & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **관련 저작물에 대한 CARFF의 질적 비교.** CARFF는 나열된 유사 저작물과 대조적으로 강조된 모든 목적을 달성한다. 체크 마크는 관련 방법이 각 열에 정성적 특징을 통합한다는 것을 나타내는 반면, 빈 공간은 방법이 이를 설명하지 않는다는 것을 나타낸다. 여기서, 3D는 3D 환경에서 추론하고 새로운 뷰 합성을 수행하는 방법들을 의미한다. 사실적 적용은 그 방법이 현실적이고 복잡한 시나리오에서 입증되었는지 여부를 의미한다. 상태 및 동적 불확실성은 이러한 조건에서 모델이 확률적으로 예측하는지 여부를 나타낸다. 예측은 미래에 대한 예측을 의미하며, 계획이란 의사결정에 모형 예측을 사용하는 것을 의미한다.\n' +
      '\n' +
      '도 9: **PC-VAE 인코더 입력, 그라운드 트루스 타임스탬프, 및 재구성**. 다른 타임스탬프에서 카메라 포즈(c\\)에서 본 다른 지상진리 영상들 중, 타임스탬프(t\\)를 고려한 새로운 포즈(c^{\\prime\\prime}\\)에 걸쳐 인코더 입력인 \\(I_{c}^{t}\\)을 재구성하여 \\(I_{c^{\\prime\\prime}}^{t}\\)을 생성한다. 이것은 재구성의 전체 그리드입니다.\n' +
      '\n' +
      'network, \\(\\sigma(\\mathbf{x})\\), for density values and color network, \\(C(\\mathbf{r})\\), for conditional RGB generation. 전체 아키텍처가 일정하게 유지되는 동안, 각 네트워크의 입력 차원은 추가적인 잠재 연계를 허용하도록 수정된다.\n' +
      '\n' +
      '**최적화:** 경험적으로, 우리는 NeRF가 PC-VAE 잠재 공간 내에서 장면의 분포를 학습하도록 훈련하는 것이 필수적이라는 것을 관찰한다. 훈련하기 위해 미리 정의된 학습된 샘플만을 사용하는 것은 비대표 샘플에 의존하는 위험을 초래할 수 있다. 반면에, Instant-NGP에서 각각의 트레이닝 반복 동안 직접 리샘플링은 NeRF의 민감한 최적화로 인해 트레이닝 진행이 지연될 수 있다. 최적화 과정에서 LR 붕괴와 함께 LR(0.002\\)을 사용하고 미리 정의된 잠재 샘플로 시작한다. 그런 다음 재샘플링된 래턴트를 천천히 소개합니다. 우리는 이 전략이 효율적인 훈련을 유지하면서 단일 표본의 영향을 점진적으로 감소시킨다고 믿는다. 우리의 관찰에 기초하여, 이 전략은 기본 컨디셔닝과 환경 재구성을 빠르게 동화시키는 Instant-NGP의 능력에 기여하는 동시에 학습 프로세스가 단일 잠재 샘플로 덜 편향되도록 밀어붙인다.\n' +
      '\n' +
      '## 부록 D GUI 인터페이스\n' +
      '\n' +
      '추론 파이프라인과의 상호 작용을 용이하게 하기 위해, NeRF는 미리 훈련된 MDN 체크포인트를 로드하고, 시각화 목적으로 DearPyGUI를 사용하여 그래픽 사용자 인터페이스(GUI)를 구축한다. 우리는 GUI에서 (a) 예측, (b) 프로브 및 예측, (c) 토글의 세 가지 기능을 구현한다.\n' +
      '\n' +
      '**Predict:** GUI 내의 주어진 이미지 경로로부터 직접 예측을 수행하는 기능을 구현한다. 입력영상에 대응하는 PC-VAE 인코더의 분포\\(q_{\\phi}(z_{t-1}|I_{c}^{t-1})\\)을 이용하여 다음 타임스탬프\\(q_{\\phi}^{\\prime}(z_{t}|I_{c}^{t-1})에 대한 잠재분포를 예측한다. 이 과정은 MDN을 통해 즉석에서 수행된다. 예측된 분포로부터의 샘플이 생성되고 NeRF를 컨디셔닝하는 데 사용된다. 이것은 전체 장면을 다음 타임스탬프로 진행시킨다.\n' +
      '\n' +
      '**프로브 및 예측:** 예측된 분포로부터 샘플링된 잠재는 특이 분포에 해당하지 않으므로 다음 시간을 직접 예측할 수 없음\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r} \\hline \\hline\n' +
      '**PC-VAE Hyperparameters** & \\\\ \\hline Latent Size & 8 \\\\ LR & \\(0.004\\) \\\\ KLD Weight Start & \\(0.000001\\) \\\\ KLD Weight End & \\(0.00001-0.00004\\)* \\\\ KLD Increment Start & \\(50\\) epochs \\\\ KLD Increment End & \\(80\\) epochs \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **PC-VAE 실험 설정 및 하이퍼파라미터.** 세 데이터 세트에 대한 PC-VAE 훈련의 주요 하이퍼파라미터는 잠재 크기, LR 및 KLD 가중치이다. KLD 스케줄링을 위해, KLD 증가 시작은 초기 KLD 가중치로부터 KLD 가중치가 증가하기 시작하는 에폭의 수를 지칭한다. KLD 증가 끝은 KLD 가중치가 최대 KLD 가중치에서 증가하는 것을 멈추는 에폭의 수이다. 별표(*)는 데이터 집합에 종속적인 하이퍼모수를 표시합니다.\n' +
      '\n' +
      '도 10: **NeRF 그래픽 사용자 인터페이스.** GUI를 통해 입력 이미지 경로로 토글하고 예측할 수 있다. 프로브 및 예측 기능은 자동차의 현재 위치를 탐색하고 다음을 예측한다. 스크린샷은 종이의 시각적 선명도를 위해 선명하게 표시됩니다.\n' +
      '\n' +
      'tamp. 모델이 자연에서 자동 회귀되도록 하기 위해 밀도 탐사를 수행한다. 우리는 현재 타임스탬프와 장면을 얻기 위해 자동차의 가능한 위치 좌표에서 NeRF의 밀도를 조사한다. 그런 다음 PC-VAE 공간의 해당 분포에 잠재된 것을 일치시키는 데 사용됩니다. 새로운 분포는 위에서 설명한 예측 함수를 사용하여 자동 회귀 예측을 가능하게 한다.\n' +
      '\n' +
      'Toggle:NeRF는 PC-VAE로부터 학습된 래턴트를 이용하여 제공된 입력 이미지 경로에 대응하는 장면을 생성한다. 입력 이미지가 완전히 관찰 가능한 뷰일 때, NeRF는 입력을 존중하는 명확한 액터 및 에고 구성을 렌더링한다. 이를 통해 서로 다른 타임스탬프와 서로 다른 구성으로 장면을 시각화할 수 있다.\n' +
      '\n' +
      '## 부록 E CARFF 평가\n' +
      '\n' +
      '### Pose-Conditional VAE\n' +
      '\n' +
      '재구성 품질: 훈련 중 모델의 재구성 성능을 분석하기 위해 재구성된 이미지의 그리드를 주기적으로 표시한다. 이들 그리드는 (a) 데이터세트로부터 드로잉된 랜덤하게 선택된 인코더 입력, (b) 동일한 카메라 포즈에서의 각각의 타임스탬프에서의 이들 입력에 대한 대응하는 그라운드 진실 이미지, 및 (c) 랜덤하게 샘플링된 포즈에서의 출력을 재구성하는 것으로 구성된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Encoder Architectures & Train & SVM & NV \\\\  & PSNR & Accuracy & PSNR \\\\ \\hline \\hline \\multicolumn{4}{l}{**Multi-Scene Approaching Intersection**} \\\\ \\hline PC-VAE & **26.47** & **89.17** & **26.37** \\\\ PC-VAE w/o CL & 26.20 & 83.83 & 26.16 \\\\ Vanilla PC-VAE & 25.97 & 29.33 & 25.93 \\\\ PC-VAE w/o Freezing & 24.82 & 29.83 & 24.78 \\\\ PC-VAE w/ MobileNet & 19.37 & 29.50 & 19.43 \\\\ Vanilla VAE & 26.04 & 14.67 & 9.84 \\\\ \\hline \\hline \\multicolumn{4}{l}{**Multi-Scene Two Lane Merge**} \\\\ \\hline PC-VAE & **25.50** & **88.33** & **25.84** \\\\ PC-VAE w/o CL & 24.38 & 29.67 & 24.02 \\\\ Vanilla PC-VAE & 24.75 & 29.67 & 24.96 \\\\ PC-VAE w/o Freezing & 23.97 & 28.33 & 24.04 \\\\ PC-VAE w/ MobileNet & 17.70 & 75.00 & 17.65 \\\\ Vanilla VAE & 25.11 & 28.17 & 8.49 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 멀티-씬 데이터 세트에 걸친 **PC-VAE 메트릭 및 절제.** CARFF의 PC-VAE는 재구성 및 포즈-컨디셔닝에서 멀티-씬 데이터 세트에 걸친 다른 인코더 아키텍처를 능가한다.\n' +
      '\n' +
      '그림 11: **모든 CARLA 데이터 세트에 대한 평균 열차 PSNR 도표**. 그림은 훈련 과정의 기간 동안 각 데이터 세트에 대한 모든 이미지의 평균 훈련 PSNR의 증가를 보여준다.\n' +
      '\n' +
      '도 12: **잠재 샘플 분포 클러스터링**. 다중 장면 2차선 병합 데이터 세트에 대한 잠재 샘플의 분포는 t-SNE 클러스터링을 통해 분리할 수 있다. 도면에서, _Scene 0, Timestamp 0_ 및 _Scene 1, Timestamp 0_에 대한 클러스터는 동역학 불확실성 하에서 환경의 동일한 초기 상태를 나타내기 때문에 분포에서 중첩된다.\n' +
      '\n' +
      '입력 장면과 타임스탬프 예시적 재구성 그리드가 도 9에 제공된다. 그리드는 모델이 포즈들에 컨디셔닝된 인코더 입력들을 사용하여 합리적인 이미지들을 정확하게 재구성할 수 있는지 여부의 시각적 평가를 가능하게 한다. 이 평가는 재구성 품질의 개선에 대한 시각적 증거를 제공한다. 우리는 또한 훈련 데이터에 대해 계산된 평균 PSNR을 통해 재구성의 점진적인 개선을 정량적으로 분석한다(도 11 참조).\n' +
      '\n' +
      '잠재 공간 분석은 PC-VAE에 의해 생성된 잠복기의 품질을 평가하기 위해 처음에 t-SNE 도표를 사용하여 잠재 분포를 군집으로 시각화한다. 도. 도 12는 Multi-Scene Two Lane Merge 데이터셋에 대한 잠재 샘플의 분포가 분리 가능함을 보여준다. t-SNE는 지역 구조를 보존함으로써 가장 가까운 이웃 정보를 유지하는 데 능숙하지만 전역 구조를 보존하는 데는 약한 성능을 보인다. 따라서 t-SNE는 모든 데이터 세트에 대한 분포의 차이를 포착하는 데 충분하지 않을 수 있다.\n' +
      '\n' +
      '대신 서포트 벡터 머신으로 피벗하여 래턴트의 분리성에 대한 정량적 평가를 수행한다. 표준 정규화 파라미터(\\(C=1\\))를 갖는 RBF 커널을 이용한다. 클러스터링을 위한 메트릭으로 정확도를 계산하기 위해 잠복기에 대해 10배 검증을 수행한다. 탭을 참조하십시오. 6.\n' +
      '\n' +
      '분리성을 넘어 부분 관측과 완전 관측에서 PC-VAE로부터 직접 학습된 위성의 재현율과 정확도를 분석한다. 이는 적절한 리콜을 재훈련하는 동안 많은 수의 샘플에서도 매우 높은 정확도를 달성하여 다운스트림 MDN 훈련을 가능하게 한다. (도 13 참조)\n' +
      '\n' +
      '완전히 관찰 가능한 예측\n' +
      '\n' +
      'MDN의 작업 중 하나는 완전한 관찰 하에 미래의 장면 구성을 예측하는 것이다. 우리는 NeRF로부터 렌더링된 조감 뷰와 다양한 타임스탬프에 대한 장면의 선택된 지상 진리 이미지를 비교하여 미래 장면을 예측하는 모델의 능력을 정량적으로 평가한다(탭 7 참조). 값은 세 가지 데이터 세트 모두에 대해 계산되고 표시된다. 탭에서 도 7을 참조하면, 이미지는 토글링(\\(\\tilde{I}_{t_{i}}\\)) 또는 예측(\\(\\hat{I}_{t_{i}}\\))으로 표시된다. 테이블에서 전환된 이미지는 데이터 세트의 첫 번째 타임스탬프이거나 역학 불확실성의 경우 장면에 걸친 이전 타임스탬프의 상태가 동일하기 때문에 결정적으로 예측할 수 없다. 같은 이유로 Multi-Scene Two Lane Merge Dataset에서는 쌍 \\((I_{t_{1}},\\tilde{I}_{t_{4}})\\) 및 \\((I_{t_{4}},\\tilde{I}_{t_{1}})\\)에 대해 추가적인 Bolded PSNR 값이 존재한다.\n' +
      '\n' +
      'Figure 13: **Multi-Scene dataset accuracy and recall curves from learned Latents.** we test our framework across \\(n=1\\) and \\(n=50\\) samples from PC-VAE의 잠재분포로부터 자기중심적 이미지 입력으로부터 얻은 표본에 걸쳐 실험한다. 샘플 수\\(n\\)에 걸쳐 부분 관찰(리콜)에서 생성된 이상적인 믿음의 상태 범위와 MDN이 학습하기 위해 전체 관찰(정확도)에서 샘플링된 올바른 믿음의 비율을 달성한다. 표본 수가 크게 증가함에 따라 잠재 분포 재표본의 무작위성으로 인해 정확도가 감소하기 시작한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>