<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization\n' +
      '\n' +
      ' Yuhang Zang\n' +
      '\n' +
      'Work done while interning at Apple.\n' +
      '\n' +
      'Hanlin Goh\n' +
      '\n' +
      '1Nanyang Technological University 2Apple Inc.\n' +
      '\n' +
      '{zang0012}@ntu.edu.sg {hanlin,jsusskind,chen-huang}@apple.com\n' +
      '\n' +
      'Josh Susskind\n' +
      '\n' +
      '2Apple Inc.\n' +
      '\n' +
      '{zang0012}@ntu.edu.sg {hanlin,jsusskind,chen-huang}@apple.com\n' +
      '\n' +
      'Chen Huang\n' +
      '\n' +
      '22\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution (ID) and out-of-distribution (OOD) samples, but also show some improvements in both ID and OOD accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach OGEN to address this pitfall, with the main focus on improving the OOD GENeralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize OOD features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between ID and OOD data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, _i.e._, adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in OOD generalization performance in different settings.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large-scale pre-trained vision-language models like CLIP (Radford et al., 2021) demonstrate promising generalizability on various visual domains and tasks in the real world. However, their zero-shot in-distribution (ID) performance can be limited for some downstream datasets. Also due to their zero-shot evaluation in a closed-set manner (_i.e._, to match input image to a predefined set of classes), vision-language models often struggle to handle the out-of-distribution (OOD) samples from novel classes. Such shortcomings create major safety risks in the open domain that often require capabilities of OOD detection and/or accurate identification of both novel and seen classes.\n' +
      '\n' +
      'Some recent works attempt to improve the _zero-shot_ OOD detection performance of existing vision-language models, either by simple softmax scaling (Ming et al., 2022) or training an extra text generator (Esmaeilpour et al., 2022). Alternatively, Fort et al. (2021) first show the promise of CLIP models _finetuned_ on an ID dataset. Encouragingly both ID and OOD accuracies are improved after finetuning. Parameter-efficient finetuning methods, such as prompt learning (Zhou et al., 2022; 20) or adaptor tuning (Zhang et al., 2022), illustrate similar benefits without heavy training.\n' +
      '\n' +
      'Despite the success of prior finetuning methods, we found from our extensive benchmarking that finetuning on ID datasets is prone to overfitting (Fig. 1(b)). More specifically, we observed that models after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with inferior generalization on unknown classes. Unfortunately, an explicit regularization mechanism has not been explored in literature to address this pitfall, and simpleregularization strategies like early stopping seem insufficient. E.g. in Fig. 1(b), it is difficult to find an early model checkpoint with good trade-off between the known and unknown class performance.\n' +
      '\n' +
      'One main challenge of effective model regularization is the missing knowledge about unknowns. Such knowledge could actually offer useful supervision signals to avoid overconfident predictions on OOD data. In this paper, we propose a novel method that features 1) image feature synthesis for unknown classes and 2) an unknown-aware finetuning algorithm with effective model regularization. The goal is to improve OOD generalization without hurting the ID performance of finetuned models. To synthesize unknown features, we introduce a class-conditional feature generator. _i.e._, generating image features just given the name of an unknown class. This is made possible by CLIP\'s well-aligned image-text feature spaces. Our feature generator is implemented by a lightweight attention module, with an "extrapolating bias" on the unknown classes. It generalizes well to "unknown unknowns" and hence can model the complex distributions of visual classes in the open domain. Then we use both the ID and synthesized OOD data for joint optimization, leading to a better regularized decision boundary. Another contribution is an adaptive self-distillation mechanism that regularizes our feature generator to further reduce overfitting during joint optimization. The idea is to find an adaptive teacher model of the feature generator from historical training epochs (with less overfitting) to guide optimization at the current epoch (student model, often with more overfitting).\n' +
      '\n' +
      'Our overall approach OGEN is applicable to different finetuning methods _e.g._, (Zhou et al., 2022; Jia et al., 2022; Jia et al., 2022) for CLIP-like models. OGEN is shown to consistently improve their OOD generalization performance (by up to absolute 18.77%) under two settings: within-dataset (base-to-new class) generalization and cross-dataset generalization. Summarizing, our **main contributions** are:\n' +
      '\n' +
      '* Provide the first _comprehensive_ study on OOD generalization that unveils the pitfalls of finetuning methods (based on prompt learning) for vision-language models.\n' +
      '* A class-conditional feature generator to synthesize OOD data for effective regularization.\n' +
      '* Adaptive self-distillation on our feature generator to further reduce overfitting.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '**Vision-Language Models.** Recent large-scale vision-language models like ViLT (Kim et al., 2021) and PaLI (Chen et al., 2023) simply consume image-and-text features for multimodal downstream tasks with remarkable performance. Another popular paradigm used in CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) contrastively aligns image and text encoders. These contrastive models are trained on massive web-scale image-text pairs, also showing strong adaptability to a range of\n' +
      '\n' +
      'Figure 1: **(a) We study OOD generalization when finetuning the vision-language model CLIP on various downstream tasks. We consider both within-dataset generalization where one dataset has ID vs. OOD (or known vs. unknown) class splits for finetuning and evaluation respectively, and the more challenging cross-dataset generalization setting. More clarifications on the problem definition in Appendix A. (b) Examples of within-dataset generalization: we show learning curves of the prompt learning method CoOp (Zhou et al., 2022) that finetunes CLIP for long enough (200 epochs) on three datasets (more in Appendix B). Apparently, CoOp overfits the known classes of each dataset with notable accuracy drop on the unknowns. Our proposed method OGEN largely reduces such overfitting through effective regularization.**\n' +
      '\n' +
      'downstream tasks, such as semantic segmentation (Zang et al., 2022; Ghiasi et al., 2021) and video classification (Qian et al., 2022). Numerous follow-up works (Li et al., 2022; Zhou et al., 2022) aim to improve CLIP-like models in data efficiency or generalization. However, the zero-shot performance on some tasks can still be limited for existing vision-language models. Hu et al. (2023) found that they make different kinds of errors, _e.g._, PaLI is erroneous at tail visual concepts while CLIP may fail for common ones. This paper mainly studies and improves the generalization of fine-tuned CLIP models, but our approach is model-agnostic and thus applicable to other vision-language models as well.\n' +
      '\n' +
      '**Finetuning methods** have been studied to improve the downstream performance of vision-language models over their zero-shot counterparts. Fort et al. (2021) showed that after finetuning the CLIP model on datasets of interest, both the ID and OOD generalization performance will be improved. More parameter-efficient finetuning methods are popularized in recent years. In particular, prompt learning focuses on learning visual (Jia et al., 2022), textual (Zhou et al., 2022; Yao et al., 2023; Wang et al., 2023; Shu et al., 2023; Khattak et al., 2023) or multi-modal Zang et al. (2022); Khattak et al. (2023) prompts, while adaptor tuning (Zhang et al., 2022) optimizes feature representations with the model backbone kept frozen. In this paper, we first unveil the overfitting issue of recent finetuning methods, and then propose a new regularization method to prevent overfitting. Our approach is orthogonal to the finetuning research, and shows consistent gains over various finetuning baselines.\n' +
      '\n' +
      '**Outlier synthesis** proves effective for model regularization in the absence of OOD data. Previous methods rely on GANs (Lee et al., 2018) to synthesize outlier images. More recent methods like VOS (Du et al., 2022) directly synthesize virtual features which allows greater flexibility. Tao et al. (2023) propose non-parametric outlier synthesis, without the restrictive Gaussian assumption on feature distributions in VOS. Here we present a new feature synthesis method that has the same format as the CLIP framework and hence facilitates multimodal regularization. Specifically, given the name of an unknown class, we synthesize its example features in a generalizable way.\n' +
      '\n' +
      '**Model distillation** techniques transfer knowledge from a teacher model to student models, _e.g._, from a large model to its efficient counterparts (Hinton et al., 2015) or from a weakly augmented model to the strongly augmented (Sohn et al., 2020). Here we aim to reduce overfitting for unseen classes and propose to distill knowledge from early to current epochs (_i.e._, self-distillation). Specifically, we extend Mean teacher (Tarvainen and Valpola, 2017) to an adaptive localized one with suitable teacher curriculum. In the vision-language domain, our approach differs from distillation into smaller models (Li et al., 2023) or towards various downstream tasks (Gu et al., 2022; Dai et al., 2022; Mal et al., 2022). Our approach is also orthogonal (and applicable) to recent distillation frameworks for improved multimodal _pretraining_(Dong et al., 2023; Li et al., 2021; Zhong et al., 2022).\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**CLIP**(Radford et al., 2021) is the vision-language model that we mainly study in this paper, although our study is applicable to other popular models. CLIP consists of an image encoder \\(\\phi\\) and a text encoder \\(\\psi\\), which map the image and text inputs into a joint feature space. The CLIP training aims at aligning the image and text modalities by maximizing their feature similarity. Given an input image \\(\\mathbf{x}\\) that belongs to one of the classes \\(\\mathbf{Y}=\\{\\mathbf{y}_{1},\\mathbf{y}_{2},...,\\mathbf{y}_{C}\\}\\), the image encoder \\(\\phi\\) first extracts image features \\(\\mathbf{z}=f_{\\phi}(\\mathbf{x})\\in\\mathbb{R}^{d}\\). To obtain the corresponding text features \\(\\mathbf{w}_{c\\in\\{1,...,C\\}}\\), all the given class names can be fed into a fixed prompt template {a photo of a [CLASS]}, leading to text descriptions \\(\\mathbf{A}\\) which are further encoded by \\(\\psi\\) into the text embeddings \\(\\mathbf{W}=f_{\\psi}(\\mathbf{A})\\in\\mathbb{R}^{d\\times C}\\) (hence \\(\\mathbf{w}_{c}=\\mathbf{W}_{c,c}\\)). The image-text alignment is optimized based on the cosine feature similarity:\n' +
      '\n' +
      '\\[p(y=c\\mid\\mathbf{x})=\\frac{\\exp\\left(\\cos\\left(\\mathbf{w}_{c},\\mathbf{z}\\right)/\\tau\\right) }{\\sum_{i=1}^{C}\\exp\\left(\\cos\\left(\\mathbf{w}_{i},\\mathbf{z}\\right)/\\tau\\right)}, \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\tau\\) is the temperature. A larger cosine score often indicates stronger image-text alignment in their underlying semantics.\n' +
      '\n' +
      '**Prompt Learning.** For efficient model finetuning on downstream tasks, recent prompt learning approaches like CoOp (Zhou et al., 2022) replace the aforementioned fixed prompts with learnable ones \\(\\mathbf{V}=[\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{L}]\\in\\mathbb{R}^{d\\times L}\\) where \\(L\\) is the prompt length. Then the text encoder \\(\\psi\\) of CLIP will be able to convert the learned prompts \\(\\mathbf{V}\\) (together with \\(\\mathbf{Y}\\)) into adapted text embeddings \\(\\hat{\\mathbf{W}}=f_{\\psi}([\\mathbf{V},\\mathbf{Y}])\\in\\mathbb{R}^{d\\times C}\\). Note \\(\\mathbf{V}\\) is learned on each downstream task using the task-specific loss. The image encoder \\(\\phi\\) and text encoder \\(\\psi\\) of CLIP are kept frozen during prompt learning.\n' +
      '\n' +
      '### Class-Conditional Feature Generator\n' +
      '\n' +
      'As shown in Fig. 1(b), the "prompt-tuned" CLIP model tends to overfit the known classes (_aka_ base classes \\(\\mathbf{Y}^{b}=\\{\\mathbf{y}_{1},\\mathbf{y}_{2},...,\\mathbf{y}_{C_{b}}\\}\\)) from the downstream task, while OOD generalization on unknown classes (_aka_ new classes \\(\\mathbf{Y}^{n}\\) with \\(|\\mathbf{Y}^{n}|=C_{n}\\)) will deteriorate. To reduce overfitting, one might choose model regularization strategies, which will inevitably suffer from the missing knowledge about unknowns. Moreover, the potential number of unknown classes \\(C_{n}\\) is huge and \\(C_{n}\\gg C_{b}\\). Hence it is very challenging to model their complex distributions for effective regularization.\n' +
      '\n' +
      'Here we make one step towards gaining knowledge of unknowns in a class-conditional manner, in order to provide supervision signals for the vast space of unknown data. Given a textual description or simply the class name of _any_ unknown class, we aim to synthesize the class example _features_ without seeing labeled instances (Fig. 2(a)), leveraging the well-aligned image-text feature spaces of CLIP. Such synthesized image features will then facilitate learning a regularized decision boundary between known and unknown classes, leading to improved OOD generalization capabilities.\n' +
      '\n' +
      'In early experiments, we found that directly generating OOD image features out of class names is hard due to the highly non-linear and high-dimensional nature of the former. This is similarly observed in those strong cases of OOD generalization in (Abbe et al., 2023), where the manifold embeddings are typically nonlinear and, more critically, part of the distribution domain is entirely unseen at training. It is proved that successful learning under such extreme distribution shift leads to extrapolating solutions since memorization is voided on the unseen domain. Following the "extrapolating bias" on the unknown, we reframe our feature synthesis problem as an easier one -- extrapolating from the most similar classes of the seen data, _e.g._, to generate features of the unknown class _raccoson_ by extrapolating features of the similar training classes like _cat_ and _bear_.\n' +
      '\n' +
      'More specifically, for prompt learning, given the learned prompts and one unknown [CLASS] from the open set \\(\\mathbf{Y}^{n}\\), we first obtain the corresponding text features \\(\\hat{\\mathbf{w}}^{n}\\in\\mathbb{R}^{d}\\) through the text encoder \\(\\psi\\) of CLIP. Then we find for \\(\\hat{\\mathbf{w}}^{n}\\) its kNN classes from the entire set of text features of known classes \\(\\hat{\\mathbf{W}}^{b}\\in\\mathbb{R}^{d\\times C_{b}}\\), resulting in \\(\\hat{\\mathbf{W}}^{b}_{R}\\in\\mathbb{R}^{d\\times K}\\) where \\(R\\) is the neighbor set with \\(|R|=K\\). From each of the kNN classes, we randomly sample only one class example and obtain its text-aligned image features from the image encoder \\(\\phi\\), leading to the same number of \\(K\\) image feature vectors \\(\\mathbf{Z}^{b}_{R}\\in\\mathbb{R}^{d\\times K}\\). Our goal is to train a class-conditional feature generator \\(f_{\\theta}(\\hat{\\mathbf{w}}^{n},\\hat{\\mathbf{W}}^{b}_{R},\\mathbf{Z}^{b}_{R})\\) that can synthesize unknown image features conditioned on the text features \\(\\hat{\\mathbf{w}}^{n}\\) of an unknown class and auxiliary text/image features \\((\\hat{\\mathbf{W}}^{b}_{R},\\mathbf{Z}^{b}_{R})\\) of kNN known classes, see Fig. 2 (b).\n' +
      '\n' +
      '**Remarks.** To retrieve semantically similar kNN classes \\(\\hat{\\mathbf{W}}^{b}_{R}\\) from \\(\\hat{\\mathbf{W}}^{b}\\), we choose to use the cosine similarity score between the text features (not image features) of class pairs. Then the kNN retrieval\n' +
      '\n' +
      'Figure 2: **(a) To improve OOD generalization, we propose to gain knowledge of unknown classes by directly synthesizing their image features. This helps to learn a more reliable decision boundary between known and unknown classes in the feature space. (b) Prompt learning based on discriminating both the known and synthesized unknown features (from our class-conditional feature generator \\(\\theta\\), see details in text). (c) Implementation of \\(\\theta\\) using a lightweight attention module.**\n' +
      '\n' +
      'process can be formally defined as:\n' +
      '\n' +
      '\\[\\underset{R\\subset\\{1,\\dots,C_{b}\\};|R|=K}{\\arg\\max}\\ \\ \\sum_{i\\in R}\\cos\\left(\\hat{ \\mathbf{w}}^{n},\\hat{\\mathbf{w}}_{i}^{b}\\right),\\ where\\ \\ \\hat{\\mathbf{w}}_{i}^{b}=\\hat{\\mathbf{W}}_{:,i}^{b}. \\tag{2}\\]\n' +
      '\n' +
      'On another note, our empirical study shows that the one random example sampled from each kNN class is enough for assisting new feature generation. Such randomness encourages the diversity of the synthesized features for new classes.\n' +
      '\n' +
      '**Extrapolating per class.** Recall the tuple \\((\\hat{\\mathbf{W}}_{R}^{b},\\mathbf{Z}_{R}^{b})\\) consists of \\(K\\) text and image feature vectors respectively (one for each similar known class). One straightforward feature synthesis method for an unknown class (with text features \\(\\hat{\\mathbf{w}}^{n}\\)) is to extrapolate each image feature vector in \\(\\mathbf{Z}_{R}^{b}\\) based on some notion of similarity with \\(\\hat{\\mathbf{w}}^{n}\\), leading to a total of \\(K\\) extrapolated image features from \\(K\\) known classes (_e.g., cat_-_raccoon, bear_-_raccoon,...). The similarity notion can be well learned by Multi-Head Cross-Attention (MHCA) that operates on the triplets of queries, keys and values \\((\\hat{\\mathbf{w}}^{n},\\hat{\\mathbf{W}}_{R}^{b},\\mathbf{Z}_{R}^{b})\\). This way, we can effectively take into account the similarity between the unknown class and each known class in \\(R\\) as well as all other between-class similarities. Summarizing, the matrix form of our "extrapolating-per-class" scheme is given as:\n' +
      '\n' +
      '\\[\\mathbf{Z}^{n}=\\texttt{LN}(\\mathbf{Z}_{R}^{b}+\\hat{\\mathbf{Z}}^{n})\\in\\mathbb{R}^{d\\times K },\\ \\ \\hat{\\mathbf{Z}}^{n}=\\texttt{MHCA}(\\hat{\\mathbf{w}}^{n}\\cdot\\mathbf{1}_{K}^{\\top},\\hat{\\mathbf{ W}}_{R}^{b},\\mathbf{Z}_{R}^{b})\\in\\mathbb{R}^{d\\times K}, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\hat{\\mathbf{Z}}^{n}\\) are the learned feature residuals when extrapolating each of the \\(K\\) known classes. LN denotes layer normalization. Obviously, our feature generator \\(\\hat{\\theta}\\) is lightweight with only one MHCA layer and one LN layer. The simplicity benefits from the "extrapolating bias" in our generator design.\n' +
      '\n' +
      'Finally, we use the synthesized features \\(\\mathbf{Z}^{n}\\) to regularize prompt learning and perform joint discrimination of \\(C_{b}\\) known and \\(C_{n}\\) unknown class features. The objective of maximizing the image-text alignment in Eq. (1) now becomes:\n' +
      '\n' +
      '\\[p(y=c\\mid\\mathbf{Z}^{n})=\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\cos\\left(\\hat{ \\mathbf{w}}_{c},\\mathbf{z}_{k}^{n}\\right)/\\tau\\right)}{\\sum_{i=1}^{C_{b}+C_{n}}\\exp \\left(\\cos\\left(\\hat{\\mathbf{w}}_{i},\\mathbf{z}_{k}^{n}\\right)/\\tau\\right)},\\forall c \\in\\{1,\\dots,C_{b}+C_{n}\\}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(\\hat{\\mathbf{w}}_{c}=[\\hat{\\mathbf{W}}^{b},\\hat{\\mathbf{W}}^{n}]_{:,c}\\) and \\(\\mathbf{z}_{k}^{n}=\\mathbf{Z}_{:,k}^{n}\\). Note under the "extrapolating-per-class" scheme, we have synthesized \\(K\\) image features for the same unknown class. We simply aggregate them at the score level when computing the cosine feature similarity score in Eq. (4).\n' +
      '\n' +
      '**Extrapolating jointly** is a more collaborative approach for new feature synthesis. As the name hints, we extrapolate a _single_ image feature vector \\(\\mathbf{z}^{n}\\) from all the kNN known class features \\((\\hat{\\mathbf{W}}_{R}^{b},\\mathbf{Z}_{R}^{b})\\), based on the cross attention against \\(\\hat{\\mathbf{w}}^{n}\\):\n' +
      '\n' +
      '\\[\\mathbf{z}^{n}=\\texttt{LN}(\\texttt{FFN}(\\hat{\\mathbf{w}}^{n})+\\hat{\\mathbf{z}}^{n})\\in \\mathbb{R}^{d},\\ \\ \\hat{\\mathbf{z}}^{n}=\\texttt{MHCA}(\\hat{\\mathbf{w}}^{n},\\hat{\\mathbf{W}}_{R}^{b},\\mathbf{Z}_{R}^{ b})\\in\\mathbb{R}^{d}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(\\hat{\\mathbf{z}}^{n}\\) is the residual image feature vector, while text features \\(\\hat{\\mathbf{w}}^{n}\\) are projected into the image feature space via a two-layer fully connected feed-forward network FFN. Note \\(\\texttt{FFN}(\\hat{\\mathbf{w}}^{n})\\) could be replaced by some anchor point directly searched in the image feature space, _e.g._, a weighted average of kNN image features from \\(\\mathbf{Z}_{R}^{b}\\). However, searching is a hard problem itself and learning an explicit text-to-image feature mapping works consistently better in our experiments. Fig. 2 (c) summarizes the overall network architecture, and the objective function in Eq. (4) could be updated as:\n' +
      '\n' +
      '\\[p(y=c\\mid\\mathbf{z}^{n})=\\frac{\\exp\\left(\\cos\\left(\\hat{\\mathbf{w}}_{c},\\mathbf{z}^{n} \\right)/\\tau\\right)}{\\sum_{i=1}^{C_{b}+C_{n}}\\exp\\left(\\cos\\left(\\hat{\\mathbf{w}}_ {i},\\mathbf{z}^{n}\\right)/\\tau\\right)},\\forall c\\in\\{1,\\dots,C_{b}+C_{n}\\}. \\tag{6}\\]\n' +
      '\n' +
      '**Remarks.** Our ablation study (Table 4) shows that "extrapolating jointly" (**our default approach**) is better than "extrapolating per class" at synthesizing useful unknown features for joint optimization. We train our class-conditional feature generator using the "known" and "unknown" class splits from the training set of downstream tasks. Fig. 3 demonstrates the ability of our feature generator to generalize to "unknown unknowns" during testing, with faithful image feature synthesis.\n' +
      '\n' +
      '### Adaptive Self-Distillation\n' +
      '\n' +
      'Optimizing both known and synthesized unknown features generally improves OOD generalization and oftentimes the ID performance too. However, that does not take into account the optimization dynamics that could also impact the ID-OOD performance tradeoff, especially with long finetuning runs. Take Fig. 1(b) for example. Without proper regularization, the CoOp baseline achieves either suboptimal ID performance at early epochs, or saturated ID performance but decreasing OOD performance (_i.e._, overfitting) later on. To address this issue, we introduce an adaptive self-distillation method that regularizes optimization dynamics to further reduce overfitting.\n' +
      '\n' +
      'More specifically, we use the model checkpoints from earlier epochs (_i.e._, teacher model often with less overfitting) to guide optimization at the current epoch (_i.e._, student model often with more overfitting). Since the CLIP model is frozen during prompt learning, the "model" we consider here is our feature generator \\(\\theta\\) whose synthesized OOD features will impact the joint ID-OOD optimization. Hence we enforce the consistency between the final prediction probabilities (Eq. (4) or (6)) induced by the teacher model \\(p^{T}\\) and student model \\(p^{S}\\) using the mean squared error MSE\\((p^{T},p^{S})\\). Ideally, this will help us to avoid OOD performance drop while preserving the ID performance.\n' +
      '\n' +
      'The key to our self-distillation method is the choice of teacher model \\(\\theta^{T}\\). Obviously, selecting \\(\\theta^{T}\\) as one single model checkpoint at a historical epoch time is unlikely to strike a good trade-off between the ID and OOD performance. Mean Teacher (MT) (Tarvainen and Valpola, 2017) is a better alternative, which calculates an Exponential Moving Average (EMA) over the past checkpoints up until the current time \\(t\\) (Eq. (7)). Here we propose Adaptive Local Mean Teacher (ALMT) that extends MT in two ways: 1) calculating EMA only within a local time window \\([t-m_{t},t]\\) using the last \\(m_{t}\\) checkpoints. This avoids the negative impact on the teacher\'s ID performance from those underfit early checkpoints. 2) the window size \\(m_{t}\\) is time-adaptive such that \\(m_{t}\\) is small in the early stage of finetuning (for the same purpose of ruling out underfit checkpoints), and then \\(m_{t}\\) gradually increases in order to cover older checkpoints with improved ID performance but less overfitting. Such curriculum is summarized in Eq. (8) as below:\n' +
      '\n' +
      '\\[\\textbf{MT}_{[1,t]}:~{}\\theta^{T}_{i}=\\alpha\\theta^{T}_{i-1}+(1- \\alpha)\\theta_{i},~{}~{}for~{}~{}i=\\{1,\\ldots,t\\}, \\tag{7}\\] \\[\\textbf{ALMT}_{t}:~{}\\textbf{MT}_{[t-m_{t},t]},~{}~{}m_{t}=\\left| \\left(1+\\cos\\left(\\frac{t_{\\max}+t}{t_{\\max}}\\pi\\right)\\right)\\cdot\\frac{1}{2 }(m_{\\max}-m_{\\min})+m_{\\min}\\right|, \\tag{8}\\]\n' +
      '\n' +
      'where \\(m_{\\max}=9,m_{\\min}=2\\), \\(t_{\\max}\\) is the maximum number of finetuning epochs, and the window size \\(m_{t}\\) is increased following a cosine schedule. Note our ALMT method requires maintaining a queue of past \\(m_{t}\\) checkpoints and re-calculating EMA for each time \\(t\\), both of which are cheap thanks to our compact model size of \\(\\theta_{i}\\) and the small window size \\(m_{t}\\in\\{2,\\ldots,9\\}\\).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'We evaluate OOD generalization under the two settings introduced in (Zhou et al., 2022) (more details in Appendix A): 1) generalization from ID (base) to OOD (new) classes within one dataset. The base and new class splits are used for finetuning and evaluation respectively. 2) cross-dataset generalization with one ID dataset for finetuning and other datasets for OOD evaluation. The cross\n' +
      '\n' +
      'Figure 3: Visualizing image feature synthesis based on the **joint extrapolation** scheme (Eq. (5)) on Flowers102 dataset. Note our feature generator is not trained on the unknown classes, but can still synthesize faithful image features (red triangle) lying close to the real ones (gray cross). This is achieved by extrapolating an unseen instance from the kNN class examples (only a random one per kNN class is used), effectively combining their related patterns like the shape and texture of flowers.\n' +
      '\n' +
      'dataset setting is more challenging since there will be both domain- and class-incremental distribution shift, _e.g._, from generic object classification on ImageNet (Deng et al., 2009) to satellite imagery recognition on EuroSAT (Helber et al., 2019).\n' +
      '\n' +
      '**Datasets.** For both settings we use 11 datasets: ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei et al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), FGVC-Aircraft (Maji et al., 2013), SUN397 (Xiao et al., 2010), UCF101 (Soomro et al., 2012), DTD (Cimpoi et al., 2014) and EuroSAT (Helber et al., 2019).\n' +
      '\n' +
      '**Baselines.** For finetuning, we consider prompt learning approaches CoOp (Zhou et al., 2022), CoCoOp (Zhou et al., 2022), VPT (Jia et al., 2022), and the state-of-the-art methods SHIP (Wang et al., 2023), KgCoOp (Yao et al., 2023), MaPLe (Khattak et al., 2023) and PromptSRC (Khattak et al., 2023). For each baseline, we apply our method (dubbed OGEN) to obtain an OOD GENer-alization improved version. For fairness, we use the same implementation details of each baseline, including the prompt length, vision backbone in CLIP (Radford et al., 2021) (_i.e._, ViT-B/16) and train/test data splitting. The reported results are an average over three random seeds.\n' +
      '\n' +
      '### Generalization from Base to New Classes\n' +
      '\n' +
      'The base-to-new generalization setting creates a strictly class-incremental distribution shift since the base and new class splits in one dataset are disjoint. All prompt learners are trained on the base classes, and tested on the base and new classes separately to evaluate the trade-off between ID and OOD performance. Here we follow (Xian et al., 2017) to report the harmonic mean of base and new class accuracies to quantify such trade-off.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c|c c|c c c|c c c|c c c} \\hline \\hline  & & \\multicolumn{2}{c|}{CoOp} & \\multicolumn{2}{c|}{CoCoOp} & \\multicolumn{2}{c|}{VPT} & \\multicolumn{2}{c|}{SHIP} & \\multicolumn{2}{c|}{KgCoOp} & \\multicolumn{2}{c|}{MoPLe} & \\multicolumn{2}{c}{PromptSRC} \\\\  & & & & & & & & & & & & & & & & & & \\\\ \\hline \\multirow{2}{*}{Avg across 11 datasets} & Base & 82.69 & **83.47** & **80.47** & 79.86 & 82.51 & **82.52** & 80.03 & **80.79** & **80.73** & **81.34** & 82.8 & **82.40** & **84.26** & 84.17 \\\\  & New & 63.22 & **69.54** & 11.69 & **73.35** & 69.01 & **70.61** & 73.69 & **76.14** & 73.60 & **75.68** & 75.14 & **76.37** & 76.10 & **76.56** \\\\  & \\(\\Delta\\) & +63.2 & +63.2 & +1.66 & +1.60 & +2.45 & +2.45 & +2.08 & +1.23 & +4.76 & +7.66 \\\\  & \\(\\Pi\\) & 71.66 & **75.87** & 75.83 & **76.47** & 75.16 & 76.10 & 76.73 & **78.40** & 77.00 & **78.40** & 78.55 & **79.27** & 79.99 & **80.34** \\\\ \\hline \\multirow{2}{*}{ImageNet} & Base & 76.47 & 76.40 & 75.98 & 76.50 & 75.96 & 75.50 & 75.87 & 76.14 & 75.83 & 75.88 & 76.66 & 77.02 & 77.60 & 77.50 \\\\  & New & 67.88 & 68.50 & 70.43 & 76.32 & 67.52 & 66.95 & 71.18 & 69.76 & 79.03 & 70.54 & 70.73 & 70.73 & 70.79 \\\\  & \\(\\Pi\\) & 71.92 & 72.70 & 73.10 & 73.23 & 71.38 & 71.18 & 72.79 & 73.58 & 72.78 & 73.32 & 74.74 & 73.74 & 74.01 & 74.09 \\\\ \\hline \\multirow{2}{*}{Caltech101} & Base & 98.00 & 96.67 & 97.96 & 96.67 & 97.50 & 96.63 & 97.55 & 98.09 & 97.72 & 98.52 & 97.74 & 98.37 & 98.10 & 98.32 \\\\  & New & 89.81 & 92.61 & 93.81 & 94.79 & 94.10 & 92.46 & 95.20 & 92.56 & 94.93 & 94.92 & 94.36 & 94.54 & 94.03 & 94.76 \\\\  & \\(\\Pi\\) & 93.73 & 94.95 & 95.84 & 97.52 & 97.57 & 94.30 & 96.36 & 96.65 & 96.03 & 96.27 & 96.02 & 96.42 & 96.02 & 96.50 \\\\ \\hline \\multirow{2}{*}{OxfordPets} & Base & 93.67 & 95.18 & 95.20 & 96.49 & 96.05 & 96.05 & 95.37 & 96.93 & 94.65 & 95.91 & 95.43 & 95.11 & 95.33 & 95.96 \\\\  & New & 93.52 & 96.45 & 97.69 & 97.68 & 95.64 & 98.64 & 97.87 & 97.33 & 97.16 & 97.65 & 97.76 & 97.39 & 97.30 & 97.48 \\\\  & \\(\\Pi\\) & 94.47 & 95.81 & 94.37 & 97.19 & 95.44 & 96.11 & 97.14 & 96.18 & 96.77 & 96.58 & 96.47 & 96.50 & 96.71 \\\\ \\hline \\multirow{2}{*}{Stanford} & Base & 78.12 & 78.65 & 70.49 & 89.66 & 75.00 & 72.43 & 68.57 & 68.63 & 71.76 & 78.16 & 72.94 & 73.63 & 78.22 & 77.59 \\\\  & New & 60.60 & 65.23 & 73.92 & 64.23 & 63.65 & 67.57 & 73.90 & 73.54 & 75.59 & 74.00 & 74.30 & 74.97 & 73.17 \\\\  & \\(\\Pi\\) & 68.13 & 71.35 & 72.01 & 71.50 & 68.74 & 79.96 & 71.14 & 71.88 & 73.36 & 73.84 & 73.47 & 73.96 & 76.58 & 76.38 \\\\ \\hline \\multirow{2}{*}{Flowers102} & Base & 97.60 & 97.38 & 94.87 & 93.95 & 96.89 & 89.03 & 94.02 & 94.67 & 95.00 & 98.33 & 95.92 & 96.52 & 98.07 & 97.34 \\\\  & New & 95.67 & 67.70 & 71.75 & 20.08 & 90.15 & 74.40 & 76.49 & 74.73 & 74.75 & 72.66 & 74.66 & 75.07 & 77.67 \\\\  & \\(\\Pi\\) & 74.06 & 79.87 & 81.71 & 81.57 & 81.29 & 81.09 & 83.06 & 84.61 & 83.65 & 83.98 & 82.56 & 84.06 & 85.95 & 86.39 \\\\ \\hline \\multirow{2}{*}{Food101} & Base & 83.83 & 89.91 & 90.70 & 91.17 & 88.89 & 95.01 & 90.54 & 91.07 & 90.50 & 90.80 & 90.71 & 91.02 & 90.67 & 90.69 \\\\  & New & 82.86 & 72.62 & 91.29 & 91.67 & 88.95 & 88.53 & 91.03 & 92.79 & 91.02 & 92.01 & 92.05 & 92.02 & 91.53 & 91.68 \\\\  & H & 85.19 & 88.21 & 90.99 & 91.42 & 88.91 & 89.99 & 90.78 & 91.92 & 91.09 & 91.38Table 1 summarizes the results on 11 datasets. On average, our OGEN method consistently improves the new class accuracy for all the prompt learning baselines. CoOp is particularly interesting since its default learning schedule (200 epochs) is much longer than that of CoCoOp and VPT (10 epochs). Without proper regularization, CoOp inevitably shows more serious overfitting to the base classes (82.69% on average) with low performance on new classes (63.22%) after long training runs. Our OGEN is especially useful in this case, significantly improving the average new class accuracy of CoOp from 63.22% to 69.54%. As also visualized in Appendix C - Fig. 6(a), the new class generalization sees notable gains on 3 datasets -- DTD for texture classification, EuroSAT for satellite image recognition and UCF101 for action recognition, which all demonstrate large inter-class variations. This validates the superior generalizability of OGEN, thanks to its capability of OOD feature synthesis and regularization. OGEN also improves the average base class accuracy of CoOp from 82.69% to 83.47%. Specifically, OGEN improves on 6 datasets with negligible performance drop on other 5, see Fig. 6(b). The gains on base classes can be attributed to 1) the joint discrimination of known and unknown classes and 2) our adaptive self-distillation method that strikes a good ID-OOD performance tradeoff.\n' +
      '\n' +
      'For CoCoOp and VPT with a significantly shorter training schedule, they suffer from much less overfitting with higher new but lower base accuracies than CoOp. This makes our OGEN unable to unleash its full potential to address overfitting. That said, we find both OGEN-CoCoOp and OGEN-VPT can still improve the average new class accuracy while achieving a similar base class accuracy. We are likely to further improve the base accuracy when given a longer optimization schedule that allows more ID-OOD performance balancing.\n' +
      '\n' +
      'Among the state-of-the-art methods, SHIP (+CoOp) and PromptSRC are related to our OGEN approach in the use of similar techniques of feature synthesis and self-regularization respectively. Table 1 shows that OGEN can improve the new class generalization of both SHIP and PromptSRC by exploring the synergy between regularization and OOD feature synthesis. OGEN also consistently improves the average base and new class accuracies for KgCoOp and MaPLe. Fig. 5 uses KgCoOp to exemplify how these methods still suffer from overfitting (although reduced to some extent by various techniques), and how our OGEN improves the learning curves of both base and new classes. It is worth noting that different methods are trained for different numbers of epochs, thus again, they have different levels of overfitting. OGEN improves generalization more over SHIP (200 epochs) and KgCoOp (100 epochs) with long learning schedules (more serious overfitting). Our gains are smaller over MaPLe (5 epochs) and PromptSRC (20 epochs) with short training runs, but larger gains are expected when trained for longer runs.\n' +
      '\n' +
      '### Cross-Dataset Generalization\n' +
      '\n' +
      'Table 2 shows the generalization performance from ImageNet to 10 target datasets. We consider the representative CoOp and CoCoOp baselines with long and short training runs respectively. As shown in the table, our OGEN uniformly improves the generalization performance (across baselines and target datasets) with competitive source dataset performance. The improvements are especially large on those low performing datasets DTD, EuroSAT, UCF101 with large distribution shift from ImageNet. This highlights the benefits of our OOD feature generation module. OGEN also obtains reasonable gains on the high performing datasets like OxfordPets that contains similar classes (_e.g._, different dog breeds) with ImageNet, demonstrating the universality of our approach.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{Source} & \\multicolumn{6}{c}{Target} \\\\ \\cline{3-13}  & \\multicolumn{1}{c}{\\multirow{-2}{*}{\\(\\mathbf\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abbe et al. (2023) Emmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic reasoning and degree curriculum. In _ICML_, 2023.\n' +
      '* Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _ECCV_, 2014.\n' +
      '* Chen et al. (2023) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigecerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-scaled multilingual language-image model. In _ICLR_, 2023.\n' +
      '* Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _CVPR_, 2014.\n' +
      '* Dai et al. (2022) Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Enabling multimodal generation on CLIP via vision-language knowledge distillation. In _ACL_, 2022.\n' +
      '* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _CVPR_, 2009.\n' +
      '* Dong et al. (2023) Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. MaskCLIP: Masked self-distillation advances contrastive language-image pretraining. In _CVPR_, 2023.\n' +
      '* Du et al. (2022) Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\'t know by virtual outlier synthesis. In _ICLR_, 2022.\n' +
      '* Esmaeilpour et al. (2022) Sepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detection based on the pretrained model clip. In _AAAI_, 2022.\n' +
      '* Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _CVPR workshop_, 2004.\n' +
      '* Fort et al. (2021) Stanislav Fort, Jie Jessie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. In _NeurIPS_, 2021.\n' +
      '* Ghiasi et al. (2021) Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-vocabulary image segmentation. _arXiv preprint arXiv:2112.12143_, 2021.\n' +
      '* Gu et al. (2022) Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. In _ICLR_, 2022.\n' +
      '* Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens._, 2019.\n' +
      '* Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.\n' +
      '* Hu et al. (2023) Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina Toutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. _arXiv preprint arXiv:2302.11154_, 2023.\n' +
      '* Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* Jia et al. (2022) Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In _ECCV_, 2022.\n' +
      '\n' +
      '* Khattak et al. (2023a) Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. MaPLe: Multi-modal prompt learning. In _CVPR_, 2023a.\n' +
      '* Khattak et al. (2023b) Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In _ICCV_, 2023b.\n' +
      '* Kim et al. (2021) Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In _ICML_, 2021.\n' +
      '* Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _ICCV workshops_, 2013.\n' +
      '* Lee et al. (2018) Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In _ICLR_, 2018.\n' +
      '* Li et al. (2021) Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In _NeurIPS_, 2021.\n' +
      '* Li et al. (2023) Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, and Hao Su. Distilling large vision-language model with out-of-distribution generalizability. In _ICCV_, 2023.\n' +
      '* Li et al. (2022) Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. In _ICLR_, 2022.\n' +
      '* Maji et al. (2013) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. _arXiv preprint arXiv:1306.5151_, 2013.\n' +
      '* Mal et al. (2022) Z. Mal, G. Luo, J. Gao, L. Li, Y. Chen, S. Wang, C. Zhang, and W. Hu. Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation. In _CVPR_, 2022.\n' +
      '* Ming et al. (2022) Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distribution detection with vision-language representations. In _NeurIPS_, 2022.\n' +
      '* Nilsback & Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _ICVGIP_, 2008.\n' +
      '* Parkhi et al. (2012) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _CVPR_, 2012.\n' +
      '* Qian et al. (2022) Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, and Yin Cui. Multimodal open-vocabulary video classification via pre-trained vision and language models. _arXiv preprint arXiv:2207.07646_, 2022.\n' +
      '* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatuszaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* Shu et al. (2023) Yang Shu, Xingzhuo Guo, Jialong Wu, Ximei Wang, Jianmin Wang, and Mingsheng Long. CLIPood: Generalizing clip to out-of-distributions. In _ICML_, 2023.\n' +
      '* Sohn et al. (2020) Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, 2020.\n' +
      '* Soomro et al. (2012) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.\n' +
      '* Szegedy et al. (2015)* Tao et al. (2023) Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In _ICLR_, 2023.\n' +
      '* Tarvainen & Valpola (2017) Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In _NeurIPS_, 2017.\n' +
      '* Wang et al. (2023) Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, and Tieniu Tan. Improving zero-shot generalization for clip with synthesized prompts. In _ICCV_, 2023.\n' +
      '* Xian et al. (2017) Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly. In _CVPR_, 2017.\n' +
      '* Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _CVPR_, 2010.\n' +
      '* Yao et al. (2023) Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-guided context optimization. In _CVPR_, 2023.\n' +
      '* Zang et al. (2022a) Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional matching. In _ECCV_, 2022a.\n' +
      '* Zang et al. (2022b) Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and language prompt learning. _arXiv preprint arXiv:2210.07225_, 2022b.\n' +
      '* Zhang et al. (2022) Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Jiao Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In _ECCV_, 2022.\n' +
      '* Zhong et al. (2022) Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In _CVPR_, 2022.\n' +
      '* Zhou et al. (2022a) Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _IJCV_, 2022a.\n' +
      '* Zhou et al. (2022b) Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _CVPR_, 2022b.\n' +
      '\n' +
      '## Appendix A Remarks on Problem Definition\n' +
      '\n' +
      'The focus of this paper is to understand the behaviors of and improve OOD generalization of CLIP finetuning. As shown in Fig. 1(a) of the main paper, the CLIP model can be finetuned on various downstream tasks before evaluating OOD generalization. Two settings are considered: 1) within-dataset generalization where one dataset has ID (base) vs. OOD (new) class splits for finetuning and evaluation respectively; 2) cross-dataset generalization with one ID dataset for finetuning and other OOD datasets for evaluation.\n' +
      '\n' +
      'Note CLIP is pretrained on enormous volumes of data, which inevitably could have class overlap with some "OOD" data for evaluation. Also, there could be potential class overlap between the curated ID and OOD data themselves, _e.g._, under the cross-dataset generalization setting where the dataset pair may include similar categories in their class taxonomies. Therefore, we consider more of a generalized OOD generalization test for the large-scale pretrained CLIP model. That is, whenever the class overlap happens, the domain-incremental distribution shift is considered in our evaluation, otherwise we evaluate under the strictly class-incremental distribution shift.\n' +
      '\n' +
      'Figure 4: More example learning curves of the long finetuning runs (200 epochs) with CoOp (Zhou et al., 2022) method. Under the within-dataset generalization setting, CoOp typically overfits the known classes and achieves decreasing accuracy for the unknown classes. The class-conditional feature generator plays a key role in our full method OGEN, which reduces overfitting by generating OOD features for the unknown-aware optimization. Our adaptive self-distillation method further reduces overfitting via regularizing the optimization dynamics.\n' +
      '\n' +
      'Figure 5: Learning curves of the long finetuning runs (100 epochs) with KgCoOp (Yao et al., 2023) vs. OGEN-KgCoOp methods (within-dataset generalization setting). Despite the overfitting reducing technique used in KgCoOp, it still suffers from some extent of overfitting. See how OGEN often improves the learning curves of both base and new classes.\n' +
      '\n' +
      '## Appendix B Overfitting with CLIP Finetuning: More Examples\n' +
      '\n' +
      'Fig. 4 shows more learning curves of the prompt learning method CoOp (Zhou et al., 2022) that finetunes CLIP on a long schedule. Clearly, CoOp overfits on all the considered datasets with decreasing generalization performance on the unknown classes. While both components of our proposed OGEN method - class-conditional feature generator and adaptive self-distillation - are found useful to address overfitting. Our feature generator is observed to play a key role by generating OOD features for the following unknown-aware optimization and regularization.\n' +
      '\n' +
      'Fig. 5 shows example learning curves of one of the state-of-the-art methods KgCoOp (Yao et al., 2023). This method has relatively long finetuning runs by default (100 epochs). We can see that KgCoOp still suffers from some extent of overfitting, although there is an overfitting reducing component in it. Our OGEN can further alleviate overfitting with KgCoOp, improving its learning curves of both base and new classes.\n' +
      '\n' +
      '## Appendix C Visualizing Per-Dataset Results\n' +
      '\n' +
      'Fig. 6 breaks down the performance gap between CoOp and our OGEN-CoOp on both the base and new classes for 11 datasets. The base-to-new class generalization setting is considered on each dataset.\n' +
      '\n' +
      '## Appendix D Feature Synthesis vs. Replay-based Method\n' +
      '\n' +
      'Recall the goal of our class-conditional feature generator is to model the vast space and complex distribution of unknown class data in OOD domains. Here we investigate how well our synthesized OOD features can represent the real world of unknown data. Specifically, we explore the use of replay methods by sampling real OOD data from the large-scale LAION-400M dataset (Schuhmann et al., 2021) and using the sampled data for replay. We compare our synthesized OOD features against those real OOD data in terms of their contribution to training regularization on downstream tasks.\n' +
      '\n' +
      '**Experimental details.** We experiment under the base-to-new class generalization setting where CoOp (Zhou et al., 2022) is the CLIP finetuning baseline on each of the 11 downstream datasets. For fair comparison between the OGEN- and replay-boosted CoOp variants, we always use ALMT with the only difference in the OOD data source (synthesis vs. replay).\n' +
      '\n' +
      '**Sampling of replay data.**\n' +
      '\n' +
      '* Class filtering: to ensure the replay data serve as a good proxy of OOD data for each downstream task, we need to perform class filtering when sampling image-text-pairs from LAION-400M, _i.e._, we filter image-text-pairs that are semantically close to those "known" classes on the considered dataset. We do so by using CLIP to calculate the cosine similarity\n' +
      '\n' +
      'Figure 6: **Base-to-new class generalization** when our OGEN approach is applied to the CoOp baseline that suffers from overfitting due to a long learning schedule (200 epochs). OGEN largely overcomes overfitting and **(a)** improves OOD generalization on new classes for all 11 datasets, sometimes by a large margin. **(b)** At the same time, OGEN is able to improve the base class accuracies on most datasets, with only minor accuracy drop on a few others.\n' +
      '\n' +
      'between the text features of the query text and known class names, and then dropping query texts with the maximum similarity score higher than 0.82. Note our synthesized features are guaranteed to be OOD since they are generated in a class-conditional manner with the query classes disjoint from known classes.\n' +
      '* Sampling strategy: with class filtering in place, we consider both random data sampling and hard sample mining to retrieve replay data from LAION-400M. For hard sample mining, we first rank the query texts from a sufficiently large pool (randomly sampled) in terms of the maximum similarity score mentioned above. Then we simply select the top similar query texts and use the corresponding images as hard replay data (similar to hard negatives from hard negative sampling).\n' +
      '* Sampling data size: given the large amount of image-text-pairs in LAION-400M, we increase the data size of the replay data (either random or hard) by 1-to-12x more than the size of our synthesized OOD data on each downstream dataset. Note our feature generator synthesizes the same amount of OOD features as the "unknown" class splits of the training set of considered dataset.\n' +
      '\n' +
      '**Observations from Fig. 7.**\n' +
      '\n' +
      '* Better performance is generally obtained with the use of more real OOD data for replay, and performance grows faster with hard mined replay data. When sufficient replay data are used, the performance can surpass that of our synthesized OOD features. This demonstrates the benefits of sampling big and diverse OOD data for training regularization purposes.\n' +
      '* However, our feature synthesis approach is much more data efficient than replay-based methods. The key reason behind such advantage is that our feature generator is more likely to generate hard OOD samples to better regularize decision boundaries, in comparison to real-world samples. In our case, 1) the feature generator itself is trained on the downstream dataset, thus can synthesize dataset-specific OOD features that adapt better to the task at hand. 2) Recall that we extrapolate OOD features from kNN known class features. This suggests there is inevitable shared information between the known class and synthesized features, further increasing the hardness of the latter. On the other hand, both of the aforementioned factors are missing for the real OOD data sampled from a separate domain, which contributes to their data inefficiency in training regularization. Real OOD data are most useful when they are distributionally similar to the downstream dataset (_e.g._, Caltech101) and hence can act as semi-hard negatives. Otherwise, data efficiency will see a significant drop when replaying OOD data on distributionally distant dataset like DTD. On\n' +
      '\n' +
      'Figure 7: **OOD feature synthesis is much more data efficient than replaying real OOD data. Real OOD data are sampled from LAION-400M dataset with varying data scale, _i.e._, multiple times more than the synthesized data. Here we use the CoOp baseline for base-to-new class generalization, and measure the Harmonic mean of base and new accuracies. (a) We can see that OGEN with OOD feature synthesis creates about 3.5x and 7.5x gain in data efficiency on average across 11 datasets, when compared to the replay method with hard sample mining and random sampling strategy respectively (see text for details). (b-c) The replayed OOD samples, despite being data inefficient, are more helpful when they have a closer semantic or data distribution with the downstream dataset (_e.g._, Caltech101) so to act as semi-hard negatives. They are far less useful for distributionally different DTD, the texture database. On the other hand, OGEN benefits from dataset-specific OOD feature synthesis, which often offers hard negatives to boost data efficiency for training.**\n' +
      '\n' +
      'average (across 11 datasets), our feature synthesis method is about 3.5x and 7.5x more data efficient than the replay method with hard mining and random sampling respectively.\n' +
      '\n' +
      '**Summary.** Replay-based methods perform well with large data size, but suffer from low data efficiency as well as large memory cost (to maintain replay data). Our class-conditional feature generator avoids these issues by synthesizing hard OOD features on the fly. Note our feature generator is lightweight and only incurs small computational cost. Its runtime on GPU is 0.019 seconds, which is significantly smaller than that of the feature extraction step of CLIP (text encoder: 0.046 seconds, image encoder: 1.016 seconds). One promising future direction is the combination of our feature synthesis method and replay methods, aiming to take advantage of their respective benefits of data efficiency and diversity.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>