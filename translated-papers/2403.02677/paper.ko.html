<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Finetuned Multimodal Language Models Are High quality Image-Text Data Filter\n' +
      '\n' +
      ' Weizhi Wang\\({}^{1}\\) Khalil Mirini\\({}^{2}\\) Linjie Yang\\({}^{2}\\) Sateesh Kumar\\({}^{2}\\)\n' +
      '\n' +
      '**Yu Tian\\({}^{2}\\) Xifeng Yan\\({}^{1}\\) Heng Wang\\({}^{2}\\)**\n' +
      '\n' +
      'University of California, Santa Barbara \\({}^{2}\\)Bytedance, US\n' +
      '\n' +
      '[https://mlm-filter.github.io](https://mlm-filter.github.io)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 MLM(fine-tuned Multimodal Language Models)을 이용하여 이미지 텍스트 데이터를 필터링하는 새로운 프레임워크를 제안한다. 본 논문에서 제안한 방법은 MLM의 최근 발전을 통합하여 우세한 필터링 방법(예:_CLIPScore)을 능가한다. 우리는 이미지 텍스트 데이터의 품질을 총체적으로 측정하기 위해 4가지 구별되지만 보완적인 메트릭을 설계한다. MLM을 미세 조정하기 위한 고품질 명령어 데이터를 데이터 필터로 구성하기 위해 새로운 파이프라인을 구축한다. CLIPS 코어와 비교하여 MLM 필터는 필터링된 데이터의 품질을 직접 개선하고 사전 훈련된 모델의 성능을 향상시키는 보다 정확하고 포괄적인 점수를 생성한다. 우리는 인기 있는 기초 모델(_즉,_CLIP 및 BLIP2) 및 다양한 다운스트림 작업에서 CLIPS코어보다 상당한 개선을 달성한다. MLM 필터는 다양한 모델과 작업으로 일반화할 수 있으며 CLIPS 코어의 드롭인 대체물로 사용할 수 있다. MLM 필터에 대한 설계 선택을 확인하기 위해 추가 절제 연구가 제공된다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 이미지 텍스트 데이터 세트[20, 21, 22, 23]는 최근 비전 언어 모델(VLMs) 및 텍스트 대 이미지 생성 모델의 획기적인 원동력이 되었다. 이러한 데이터 세트의 계속 증가하는 크기를 통해 연구자들은 수십억 또는 수조 개의 매개변수로 모델을 전례 없는 용량으로 확장할 수 있다. 이러한 거대한 기반 모델은 이미지 분류, 텍스트 대 이미지 검색, 이미지 캡션, 시각적 질문 응답, 이미지 생성 및 편집, _etc._ 한 가지 훌륭한 예는 400M 웹 크롤링된 이미지-텍스트 쌍으로 훈련된 OpenAI CLIP[24] 모델이다. CLIP 모델은 다양한 작업에 걸쳐 인상적인 제로샷 학습 능력을 보여줍니다.\n' +
      '\n' +
      '이미지-텍스트 데이터의 품질은 기초 모델의 최종 성능에 결정적인 역할을 한다. 그러나 웹 크롤링된 이미지-텍스트 데이터는 종종 매우 시끄럽고, 예를 들어, 대응하는 텍스트 데이터는 품질이 낮거나 이미지의 내용과 일치하지 않는다. 고품질 이미지 텍스트 데이터 세트를 구축하는 방법은 최근 많은 관심을 끄는 어려운 연구 문제이다. [25] CLIP에서 데이터 큐레이션 프로세스를 다시 만들려고 합니다. [26] 모델 견고성을 위해 데이터 품질이 양보다 더 중요하다고 주장합니다. DataComp 챌린지[19]는 서로 다른 데이터 필터링 기술을 체계적으로 평가하기 위해 도입된다.\n' +
      '\n' +
      '성공적인 각 기초 모델에는 데이터 필터링을 위한 고유한 비밀 레시피가 있습니다. CLIP의 발명 이전에, 대부분의 기술은 손으로 설계되거나 규칙 기반이다. 예를 들어, CC3M 및 CC12M은 이미지 기반, 텍스트 기반 및 이미지&텍스트 기반 필터링을 위한 일련의 휴리스틱을 설계한다. 모델 기반 필터링은 CLIPScore[18]의 도입 이후 널리 보급되고 있으며, 이는 CLIP 모델을 활용하여 이미지와 텍스트 간의 코사인 유사도를 계산하여 정렬을 측정한다.\n' +
      '\n' +
      'CLIPScore는 이미지-텍스트 데이터를 필터링하는 주된 방법이 되었다. 그러나, 최근의 연구 [17, 24]는 CLIP로부터의 시각적 특징들이 이미지의 미묘한 차이들, 예를 들어,_객체 번호, 형상 및 위치에 대해 맹목적이라는 것을 발견한다. 대조적 손실이 전체 이미지에 적용되기 때문에 CLIPS코어는 그림 1과 같이 세밀한 객체 수준 정렬 정보를 캡처하는 데 덜 민감하다. 또한 CLIP의 텍스트 인코더는 최대 77개의 토큰만 처리할 수 있다. 텍스트 인코더로부터의 정보 손실은 CLIPS코어를 제한하여 긴 캡션을 갖는 데이터를 처리할 수 있다. 이러한 제한은 길고 고도로 묘사적인 캡션에 의존하는 텍스트-이미지 생성 모델[23]에 대해 심각할 수 있다.\n' +
      '\n' +
      '비교적 학습된 CLIP 모델과 비교하여, 멀티모달 언어 모델은 생성된 이미지 또는 텍스트의 품질을 예측하고 인간의 선호도와 잘 정렬하는 데 유망한 능력을 보여주었다. 보다 구체적으로, GPT-4Vision [14]에 의해 생성된 이미지-텍스트 매칭 스코어들은 최근의 MLM 기반 평가 [13, 24]에서 CLIPScore에 비해 인간 전문가들과 더 일치한다. 이것은 우리가 고품질 데이터 필터링을 위해 MLM의 최근 발전을 통합하도록 동기를 부여한다:\n' +
      '\n' +
      '"강한 MLM을 적용하여 이미지 텍스트 데이터 품질을 평가하기 위한 점수를 생성하고 이미지 텍스트 데이터 필터링을 위해 CLIPScore를 능가할 수 있습니까?"\n' +
      '\n' +
      'GPT-4V는 이미지 텍스트 정렬을 측정하는 데 더 우수하지만 수십억 개의 이미지 텍스트 데이터를 필터링하는 데 GPT-4V 스케일 MLM을 직접 적용하는 것은 계산 비용이 너무 많이 든다. 좋은 필터링 방법은 우리가 처리해야 하는 데이터의 양이 적기 때문에 효과적이고 효율적이여야 한다. 더 작은 MLM(예를 들어,_LLaVA[12], MiniGPT-4[25] 등)이 있는데, 이는 더 효율적이지만 주로 태스크 완료 데이터에 대해 명령-튜닝되기 때문에 이미지-텍스트 데이터의 미묘한 변화를 반영할 수 있는 입도에서 점수를 생성하지 못한다. 본 논문에서는 고품질 명령어 튜닝을 구성하기 위해 독점 LLM 또는 MLM을 활용하는 두 세계의 장점을 결합하는 것을 제안한다.\n' +
      '\n' +
      '그림 1: CLIPS코어는 세밀한 객체 수준의 이미지-텍스트 정렬을 구별하는 데 실패하는 반면 MLM 필터에 의해 생성된 이미지-텍스트 매칭 점수는 이러한 정렬을 상당히 캡처한다.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      '제안된 필터링 방법의 효율성을 입증하기 위해 다운스트림 태스크에서 미리 훈련된 VLM을 데이터 셋하고 평가한다. 세 단계에 대한 세부 파이프라인은 그림 2와 같다.\n' +
      '\n' +
      '점수 매기기 작업을 위한 멀티모달 명령어 튜닝 데이터 구축###\n' +
      '\n' +
      '효과적인 데이터 필터로서 동작하기 위해, MLM은 데이터 선택 및 필터링을 위해 각각의 단일 이미지-텍스트 쌍에 대한 품질 스코어들을 생성해야 한다. LLAVA와 같은 MLM이 품질 점수에 대해 정확하게 추론할 수 있도록, 우리는 이러한 MLM이 채점 능력을 향상시키기 위해 일련의 채점 작업에서 미세 조정할 것을 제안한다. 채점 작업에 필요한 멀티모달 명령어 튜닝 데이터는 인간 레이블링을 통해 수집하기 어렵고 비용이 많이 들기 때문에 독점 모델 GPT-4 또는 GPT-4V를 사용하여 채점 작업에 이러한 멀티모달 명령어 데이터를 구성한다.\n' +
      '\n' +
      '**이미지-텍스트 품질 평가를 위한 메트릭 정의** CLIPScore와 같은 기존 데이터 필터는 이미지와 텍스트의 숨겨진 특징 간의 코사인 유사성을 계산하여 이미지와 텍스트의 전체적 매칭에 중점을 둔다. 그러나 이러한 암시적 채점은 딱딱하거나 모호한 샘플을 구별하는 데 좋지 않아 그림 1에 표시된 잘못된 음성 점수 예측으로 이어진다. 우리는 이미지-텍스트 쌍에 대한 품질 점수를 예측하기 위해 강력한 멀티모달 언어 모델을 활용할 것을 제안한다. 전체 이미지-텍스트 정렬 평가 외에도 미세 조정된 MLM 필터는 여러 관점에서 이미지-텍스트 쌍의 품질을 평가할 수 있다. 데이터 품질을 종합적으로 평가하기 위해 4가지 품질 평가 메트릭을 제안한다.\n' +
      '\n' +
      '* 이미지-텍스트 매칭(Image-Text Matching: ITM): ITM 메트릭은 이미지 캡션이 이미지의 주요 특징 및 객체를 정확하게 나타내고 그 주요 주제를 캡처하는지 여부를 평가하는 데 중점을 둔다. 미세 조정된 MLM 데이터 필터는 100의 척도로 ITM 점수를 명시적으로 생성할 수 있다.\n' +
      '* 객체 상세 설명(ODF: Object Detail Fulfillment) : ODF 메트릭은 이미지 캡션이 이미지와 정렬되는 객체에 대한 상세한 설명을 제공하는지 여부를 평가하는 것에 초점을 맞춘다. 구체적으로, ODF는 캡션이 이미지 내의 객체들의 속성들, 예를 들어,_수, 색상, 크기, 위치, 형상 등을 충분히 기술하는지 평가한다. ITM 메트릭과 비교하여, ODF 메트릭은 이미지 내의 상세한 객체 속성들과 대응하는 캡션에 기술된 것들 사이의 세밀한 정렬에 더 초점을 맞춘다.\n' +
      '* 캡션 텍스트 품질(CTQ: Caption Text Quality): CTQ 메트릭은 문법적 정확성, 어휘의 다양성(_e.g.,_ 단어의 범위 및 유일성), 유창성(_e.g.,_ 문장의 평활성 및 자연스러운 흐름), 가독성, 길이 및 구조에 기초하여 이미지 캡션의 텍스트 품질을 평가하는 것에 중점을 둔다. 이전의 데이터 중심 연구[17]는 웹 크롤링된 데이터가 반복된 단어 또는 텍스트 노이즈와 같은 다양한 나쁜 텍스트 패턴을 포함하고 있기 때문에 텍스트 품질이 좋지 않다는 것을 발견한다. 따라서 본 논문에서는 데이터 필터링을 위한 이미지 캡션의 텍스트 품질을 평가하기 위해 MLM을 미세 조정할 것을 제안한다.\n' +
      '* 시맨틱 이해(SU): SU 메트릭은 이미지 캡션이 이미지 자체만으로 쉽게 명백하지 않은 추가적인 시맨틱 정보를 제공하는지를 결정하는 것에 초점을 맞춘다. 그러한 보조 시맨틱 정보는 1) 이미지 내의 사람들의 직업일 수 있다; 2) 상기 위치들은,\n' +
      '\n' +
      '그림 2: 미세 조정 MLM 필터의 파이프라인 그림 및 데이터 필터링에 사용한다.\n' +
      '\n' +
      '주소, 축제, 국가명, 도시명; 3) 이미지 내의 건물, 사람, 조류종, 동물종, 자동차 모델, 엔진의 이름 또는 개체; 4) 이미지 내의 사람들 사이의 사회적 관계, _즉,_낮음, 부모 또는 자식. 본 논문에서 제안하는 SU 메트릭은 데이터 필터링을 위한 보조 시맨틱스를 갖는 이미지-텍스트 쌍을 선택할 수 있으며, 이는 사전 학습된 VLM의 상식 추론 능력을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '**교사 모델을 프롬프트.** 우리는 품질 채점 작업을 위한 멀티모달 수업 데이터를 구성하기 위해 두 가지 최첨단 교사 모델인 GPT-4와 GPT-4V를 선택한다. GPT-4V로 멀티모달 명령어 데이터를 구성하는 것은 GPT-4V가 직접 시각적 입력을 취할 수 있기 때문에 훨씬 쉽다. GPT-4는 텍스트 전용 LLM이므로, 텍스트 전용 GPT-4를 프롬프트하기 위해 이미지를 상세한 텍스트 디스크립션으로 변환한다. 이러한 조밀한 캡션 처리를 위한 프롬프트는 _Please generate a dense caption in 4-6 sentence for describe the image to detail with you can_. 이러한 포괄적인 이미지 설명은 LLaVA 또는 ShareGPT4V [1]과 같은 SOTA 이미지 캡션 모델을 사용하여 생성된다. 교사 모델에 대한 프롬프트 및 생성된 출력으로, 시각적 지시 데이터는 _User:[Prompt] Assistant:[Output]_로 간단히 포맷될 수 있다.\n' +
      '\n' +
      '**Prompting Strategies.** 채점 작업에는 이미지-텍스트 쌍에 대한 최종 정확한 품질 메트릭을 예측하기 위한 추론 프로세스가 포함됨에 따라, 우리는 미세 조정된 멀티모달 언어 모델의 추론 정확도를 보장하기 위한 두 가지 프롬프트 전략인 CoT(Chain-of-Thought) 추론[22] 및 합리화 추론[1]을 고려한다. 두 가지 프롬프트 전략의 주요 차이점은 점수의 생성 순서와 생성된 추론 단계이다. 두 가지 프롬프트 전략에 대한 예시 프롬프트는 부록 B 표 7에 제시되어 있으며, 이 두 프롬프트 전략 중 가장 효율적이고 정확한 추론으로 합리화 추론을 선택한다. 점수화 MLM은 수십억 개의 이미지-텍스트 쌍을 점수화할 수 있어야 하기 때문에 계산 효율이 우려된다. MLM을 미세 조정하여 스코어 값을 먼저 출력한다면 필터링에 스코어 값만 있으면 되므로 추론 단계에서 모델의 텍스트 생성 과정을 조기에 중단할 수 있다. 둘째, LLaVA의 실험 결과는 합리화 추론에 의한 명령어 튜닝이 CoT 추론보다 ScienceQA 벤치마크 [16]에서 더 나은 성능을 가져온다는 것을 보여준다. 다른 채점 메트릭에 대한 4개의 최종 프롬프트가 부록 A에 제시되어 있다.\n' +
      '\n' +
      '**데이터 수집을 위한 이미지-텍스트 쌍을 선택.** 미세 조정에 사용되는 멀티모달 명령어 데이터는 다양한 품질의 이미지-텍스트 쌍을 포함해야 한다. 따라서 미세 조정된 MLM 필터를 향상시키기 위해서는 데이터 다양성이 필수적이며, 모든 품질 수준에 걸쳐 이미지 텍스트 데이터를 효과적으로 점수화할 수 있다. 우리는 명령어 튜닝 데이터를 구성하기 위한 데이터 풀로서 개념 캡션 12M(CC12m) [17]과 DataComp Medium 128M 데이터 세트 [15]의 두 가지 이미지 텍스트 데이터 세트를 선택한다. 본 논문에서는 명령어 집합의 다양성을 높이기 위해 각 자막 텍스트의 문장 임베딩에 대해 클러스터링과 균일 샘플링을 수행한다. 우리가 사용하는 문장 임베딩 모델은 사전 학습된 MPNet[22] 인코더 모델로, 검색과 자연어 추론 데이터셋의 혼합에서 대비적으로 사전 학습된다. 우리는 Sentence Transformers[14]에서 제공하는 사전 훈련된 MPNet을 사용하여 각 이미지 캡션을 향한 문장 임베딩을 생성한다. CC12M과 Datacomp-Medium의 클러스터 수를 각각 \\(10k\\)과 \\(20k\\)으로 설정하였다. 상기 제어부는\n' +
      '\n' +
      '도 3: (a) CC12M 상의 GPT-4V를 사용하는 초기 10k 명령어들의 이미지 텍스트 매칭 스코어 분포; (b) 10 버킷들로부터 균일하게 샘플링된 최종 1k 명령어들의 이미지 텍스트 매칭 스코어 분포.\n' +
      '\n' +
      '명령어 튜닝 데이터를 구성하기 위한 이미지-텍스트 쌍들은 클러스터 중심과 가장 가까운 하나의 데이터 포인트만이 선택되는 각각의 클러스터로부터 균일하게 샘플링된다.\n' +
      '\n' +
      '**Sampling Final Instructions for Scoring Task.** 교사 모델에 의해 생성된 초기 \\(10k\\) 명령어 데이터가 그림 3(a)의 \\(100\\)의 점수 척도에 균일하게 분포되지 않는다는 것을 발견함에 따라, 학습 편향을 피하기 위해 초기 명령어 데이터를 균형 있는 명령어 세트로 샘플링해야 한다. 다중 작업 명령어 튜닝 데이터세트의 이상적인 크기가 \\(50k\\) 명령어 [12, 13, 14]임을 고려하여 각 채점 작업에 대해 \\(10k\\)의 초기 생성 명령어 데이터에서 \\(1k\\) 명령어를 샘플링하여 명령어 튜닝 MLM의 일반화 능력을 보장한다. 따라서, 전체 \\(50k\\) 명령어 데이터 세트에 포함될 품질 스코어링 태스크의 \\(4k\\) 명령어 데이터가 있으며, 이는 각각의 제안된 품질 메트릭에 대해 1k 명령어 데이터가 있다. 각 버킷에서 모든 데이터를 \\(10\\)의 버킷으로 그룹화하고 각 버킷에서 \\(100\\)의 명령을 균일하게 샘플링하는 방법과, 모든 데이터를 \\(100\\)의 버킷으로 그룹화하고 각 버킷에서 \\(10\\)의 명령을 균일하게 샘플링하는 두 가지 샘플링 방법을 실험한다. 그림 3(b)의 샘플링된 10k 수업의 점수 분포는 그림 3(a)의 원래 점수 분포보다 더 다양하고 균일하다. 최종 \\(4k\\) 명령어를 샘플링하기 위한 코드는 부록 C에 제시되어 있다.\n' +
      '\n' +
      '**멀티-태스크의 명령어 데이터와 혼합.** 멀티모달 명령어 튜닝 프로세스는 미세-튜닝된 MLM의 제로-샷 추론 능력을 향상시키기 위해 다양한 태스크 세트[12, 13]를 수반해야 한다. 제안된 데이터 품질 채점 작업의 4k 멀티모달 명령어 데이터 외에도 LLaVA-665k 명령어 데이터 세트에서 또 다른 46k 멀티모달 명령어를 샘플링한다. 우리는 추론 능력 향상이 미세 조정된 MLM의 채점 능력을 향상시킬 것이라고 생각하여 복잡한 추론[13] 및 GQA[14]와 같은 추론 작업에 데이터 혼합물의 더 많은 부분을 할당한다. 데이터 혼합물에 대해 샘플링된 각 데이터 세트의 크기에 대한 자세한 통계는 부록 D 표 8에 나와 있다.\n' +
      '\n' +
      '멀티모달 언어 모델에 대한### 명령어-튜닝\n' +
      '\n' +
      '본 논문에서는 Vicuna-13B LLM[12, 13] 기반의 LLaVA-1.5를 데이터 품질 채점 태스크와 다른 멀티모달 태스크의 혼합 명령어 튜닝을 위한 멀티모달 언어 모델 아키텍처로 채택한다. LLaVA-1.5의 훈련 과정은 이미지-텍스트 쌍에 대한 사전 훈련과 멀티모달 명령에 대한 명령어 튜닝을 포함한다. 우리는 미리 훈련된 체크포인트를 직접 취하고 혼합 명령어 세트로 명령어 튜닝 단계만 다시 구현한다.\n' +
      '\n' +
      '### 최적 MLM 데이터 필터 생성\n' +
      '\n' +
      '3.2절에서 데이터 품질 채점 작업을 위한 명령어 데이터를 구성하기 위한 다양한 설계 선택을 제안하며, 이러한 설계 선택은 명령어 튜닝의 효율성에 상당한 차이를 가져올 수 있다. 최적의 미세 조정 MLM 데이터 필터를 생성하기 위해 다양한 설계 선택이 필터링 성능에 미치는 영향을 조사하기 위해 포괄적인 절제 연구를 수행한다. 채점 작업을 위한 지시 데이터를 구성하기 위한 4가지 주요 설계 선택 사항을 조사한다: 1) LLaVA 및 ShareGPT4V[12]를 포함하는 GPT-4를 프롬프트하기 위한 텍스트 기반 상세 설명으로 이미지를 변환하기 위한 2개의 캡션 모델을 실험하고, 2) CC12M 및 DataComp Medium 128M을 포함하는 시각적 지시 데이터를 구성하기 위한 2개의 다른 이미지 텍스트 데이터 세트를 실험하고, 3) 그룹화 버킷 10 및 100의 2개의 다른 수를 실험한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l} \\hline \\hline\n' +
      '**Captioner** & **Data Resource** & \\begin{tabular}{c} **\\#Sampling** \\\\ **Buckets** \\\\ \\end{tabular} & \\begin{tabular}{c} **Teacher** \\\\ **Model** \\\\ \\end{tabular} & \\begin{tabular}{c} **ImageNet-1k** \\\\ **Mist. shifts** \\\\ \\end{tabular} & \\begin{tabular}{c} **ImageNet** \\\\ **Mist. shifts** \\\\ \\end{tabular} & \\begin{tabular}{c} **VTAB** \\\\ \\end{tabular} & \\begin{tabular}{c} **Retrieval** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Average over** \\\\ **38 datasets** \\\\ \\end{tabular} \\\\ \\hline\n' +
      '**LLaVA** & CC12M & 10 & GPT-4 & 29.0 & 24.5 & 35.0 & 29.3 & 34.2\\\\\n' +
      '**ShareGPT4V** & CC12M & 10 & GPT-4 & 28.4 & 24.9 & 35.3 & 28.2 & 33.7 \\\\ \\hline N/A & **DataComp** & 10 & GPT-4V & 29.6 & 24.8 & 34.2 & 26.7 & 33.2 \\\\ N/A & **CC12M** & 10 & GPT-4V & 30.5 & 25.3 & 33.4 & 28.0 & 33.7 \\\\ \\hline ShareGPT4V & CC12M & **10** & GPT-4 & 28.4 & 24.9 & 35.3 & 28.2 & 33.7 \\\\ ShareGPT4V & CC12M & **100** & GPT-4 & 27.5 & 23.0 & 34.6 & 28.8 & 33.2 \\\\ \\hline LLaVA & CC12M & 10 & **GPT-4** & 29.0 & 24.5 & 35.0 & 29.3 & 34.2 \\\\ N/A & CC12M & 10 & **GPT-4V** & 30.5 & 25.3 & 33.4 & 28.0 & 33.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 품질 채점 작업에 대한 멀티모달 명령어 데이터를 구성하기 위한 상이한 설계 선택에 대한 정리.\n' +
      '\n' +
      '최종 4k 명령어를 샘플링하는 단계; 4) GPT-4 및 GPT-4 Vision을 포함한 멀티모달 명령어를 얻기 위해 다양한 교사 모델을 실험한다. 또한 데이터컴 벤치마크를 사용하여 다양한 데이터 필터링 하이퍼파라미터의 효과를 평가한다.\n' +
      '\n' +
      '**DataComp Benchmark.** DataComp 벤치마크 [14]는 서로 다른 데이터 필터링 방법의 성능을 체계적으로 비교하기 위해 도입되었다. 이 벤치마크에서 훈련 코드와 계산 예산은 모든 경쟁 방법에 걸쳐 고정되어 방법 간의 직접적인 비교를 용이하게 한다. DataComp는 공정한 비교를 보장하기 위해 서로 다른 필터링 방법을 위한 고정된 원본 이미지-텍스트 데이터 풀을 제공한다. 성능은 필터링된 데이터 세트에서 CLIP 모델을 훈련시킨 다음 38개의 분류 및 검색 작업 세트에서 이 CLIP 모델의 제로 샷 기능을 테스트하여 측정된다. 다양한 MLM 데이터 필터 구성으로 인한 데이터 세트에서 ViT-B/32 CLIP 모델을 훈련하기 위해 중간 규모 훈련 설정을 선택한다.\n' +
      '\n' +
      '절제 결과.각 디자인 선택의 효과를 조사하기 위해 다른 세 가지 디자인 선택의 선택을 동일하게 유지하고 각 실험 그룹에 대해 한 가지 디자인 선택만 변경한다. 데이터 품질을 평가하기 위해 4가지 다른 메트릭을 제안함에 따라 128M 중간 규모 데이터 풀에서 고품질 하위 집합을 선택하기 위해 필터링 메트릭으로 _Object Detail Fulfillment_의 메트릭만 채택한다. 4가지 설계 선택 모두에 대한 절제 결과는 표 1에 나와 있다.\n' +
      '\n' +
      '표 1의 처음 두 라인은 명령어 데이터 구성을 위해 이미지를 상세한 설명으로 변환하기 위해 캡션 모델로 LLaVA를 채택하는 것이 더 나은 필터링 성능으로 이어진다는 것을 보여준다. 다음으로, 데이터 구축을 위해 이미지-텍스트 쌍을 샘플링하기 위해 CC12M을 채택하는 것은 DataComp-Medium 데이터 세트를 사용하는 설계 선택보다 우수하다. 우리는 CC12M의 화질이 DataComp의 화질보다 훨씬 우수하여 명령어 튜닝 과정이 지식 집약적으로 가능하기 때문이라고 가정한다. 셋째, 샘플링을 위해 초기 명령어를 10개의 버킷으로 그룹화하는 것은 100개의 버킷을 사용하는 것보다 우선 순위를 나타낸다. 교사 모델의 선택 측면에서 서로 다른 교사 모델에서 학습한 MLM 필터는 서로 다른 과제에 걸쳐 뚜렷한 강점을 보인다. GPT-4로부터 학습된 MLM 필터는 VTAB[14] 분류 및 검색 데이터 세트에서 더 나은 성능을 보이는 반면, GPT-4V로부터 학습된 MLM 필터는 ImageNet[13] 관련 데이터 세트에서 더 높은 점수를 얻는다. 마지막으로 LLaVA 캡션, CC12M 데이터 리소스 및 10개의 샘플링 버킷으로 다른 세 가지 선택을 수정하기로 결정했다. 향후 실험을 위해 서로 다른 교사 모델 GPT4 및 GPT-4V를 사용하는 MLM 기반 필터의 두 가지 버전을 보고하며, 각각 MLM-Filter-GPT4 및 MLM-Filter-GPT4V로 표시된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 미세 조정된 MLM을 고품질 이미지 텍스트 데이터 필터로 채택하는 것의 효과를 평가한다. 베이스라인 필터를 사용하여 필터링된 데이터셋에서 미리 학습된 비전 언어 모델의 성능과 MLM 필터를 사용한 성능을 비교한다. 두 개의 서로 다른 VLM\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline\n' +
      '**Filter** & **Metrics** & \\begin{tabular}{c} **Teacher** \\\\ **Model** \\\\ \\end{tabular} & **ImageNet-1k** & \\begin{tabular}{c} **ImageNet** \\\\ **dist. shifts** \\\\ \\end{tabular} & **VTAB** & **Retrieval** &\n' +
      '\\begin{tabular}{c} **Average over** \\\\ **38 datasets** \\\\ \\end{tabular} \\\\ \\hline No Filtering & - & - & 17.6 & 15.2 & 25.9 & 21.9 & 25.8 \\\\ Basic Filtering & Rules & - & 22.6 & 19.3 & 28.4 & 25.1 & 28.5 \\\\ LAION Filtering & CLIPScore+Rules & - & 23.0 & 19.8 & 30.7 & 23.3 & 29.2 \\\\ CLIPScore & CLIPScore & - & 27.3 & 23.0 & 33.8 & 25.1 & 32.8 \\\\ \\hline MLM-Filter & Image-Text Matching & GPT-4 & 28.6 & 23.7 & 34.4 & **30.0** & 33.4 \\\\ MLM-Filter & Object Detail Fulfillment & GPT-4 & 29.0 & 24.5 & 35.0 & 29.3 & 34.2 \\\\ MLM-Filter & Caption Text Quality & GPT-4 & 25.2 & 20.9 & 32.1 & 26.4 & 30.9 \\\\ MLM-Filter & Semantic Understanding & GPT-4 & 20.3 & 16.1 & 28.4 & 20.2 & 27.0 \\\\ \\hline MLM-Filter & Image-Text Matching & GPT-4V & 29.4 & 24.4 & **36.1** & 29.7 & 34.2 \\\\ MLM-Filter & Object Detail Fulfillment & GPT-4V & **30.5** & 25.3 & 33.4 & 28.0 & 33.7 \\\\ MLM-Filter & Caption Text Quality & GPT-4V & 24.3 & 20.4 & 32.3 & 24.5 & 30.9 \\\\ MLM-Filter & Semantic Understanding & GPT-4V & 16.2 & 13.9 & 23.3 & 18.7 & 24.0 \\\\ \\hline MLM-Filter & ITM AND ODF & GPT-4V & 30.3 & **25.6** & 36.0 & 29.0 & **34.5** \\\\ MLM-Filter & ITM OR ODF & GPT-4V & 28.9 & 24.5 & 35.2 & 29.0 & 33.9 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 베이스라인 필터링 방법을 사용하여 사전 훈련된 CLIP 모델의 제로샷 성능 및 데이터콤 벤치마크의 _Medium_ 스케일 풀에서 제안된 MLM-Filter. AND는 AND 연산을 사용하는 ITM 및 ODF 메트릭의 조합을 나타낸다.\n' +
      '\n' +
      '종합적인 평가를 위한 아키텍처: CLIP 사전 훈련 및 BLIP-2 사전 훈련. 또한, 제안된 MLM 필터 모델에 의해 생성된 스코어링과 베이스라인 CLIP 모델 사이의 상관 관계를 계산하기 위해 인간 평가를 수행한다.\n' +
      '\n' +
      '데이터컴퍼니 중·대축척에 대한### CLIP 사전교육\n' +
      '\n' +
      '**평가 설정.** 미세 조정된 MLM을 데이터 필터로 채택하는 효과를 평가하기 위해 데이터Comp 벤치마크를 선택한다. 평가 과정은 자료 필터링 단계와 평가 단계를 포함하며, 자료 필터링 단계에서는 MLM-Filter를 채택하여 128M 중규모 자료와 1.28B 대규모 자료에 대한 품질 점수를 생성한다. 그 후, 전체 데이터 풀의 30%, Medium의 경우 38.4M, Large의 경우 384M을 유지하는 가장 가까운 값을 기반으로 정수 필터링 임계값을 계산한다. 이러한 임계치는 품질 점수가 임계값보다 크거나 같은 모든 이미지-텍스트 쌍을 선택하도록 설정된다. 각 정의된 메트릭을 사용하여 데이터를 개별적으로 필터링한 결과를 보고하고 서로 다른 교사 모델에서 학습하는 두 개의 MLM 필터를 고려한다. 또한, 데이터 필터링을 위한 두 가지 메트릭을 조합하여 실험한 결과도 보고한다. 마지막으로, 제안된 다양한 품질 메트릭을 기반으로 중대형 이미지 텍스트 데이터 풀에서 고품질 하위 집합을 선택한다. 평가 단계에서 CLIP 모델을 사전 훈련하기 위해 선택된 고품질 데이터 하위 집합을 채택하고 다른 방법으로 필터링된 데이터 세트에 대해 사전 훈련된 CLIP 모델과 CLIP 모델의 성능을 비교한다.\n' +
      '\n' +
      '**베이스라인.** 우리는 제안된 MLM 필터를 필터링 없음, 기본 필터링, LAION 필터링 및 CLIPScore 필터링을 적용하는 것을 포함하여 DataComp의 다른 베이스라인 필터링 방법과 비교한다. 기본 필터링 방법은 세 가지 규칙 기반 필터, 영어만 필터링, 자막 길이별 필터링, 이미지 크기별 필터링을 채택한다. LAION 필터링은 ViT-B/32 CLIP 모델을 사용한 CLIPScore 필터링과 영어 필터링을 모두 채택한다. CLIPScore 필터링은 점수 생성 및 데이터 필터링을 위해 더 큰 ViT-L/14 CLIP 모델을 활용한다.\n' +
      '\n' +
      '**훈련 세부사항** DataComp에서 제공하는 훈련 설정을 엄격히 따릅니다. 계산 예산과 하이퍼파라미터는 서로 다른 필터를 사용하여 CLIP를 사전 훈련하기 위해 고정된다. CLIP 모델 아키텍처는 데이터 척도에 의해 결정되며, ViT-B/32 모델은 중간 규모 설정에서 사전 훈련되고 ViT-B/16 모델은 대규모 설정에서 사전 훈련된다. 우리는 모델을 훈련시키기 위해 \\(32\\) Nvidia A100 GPU를 사용한다.\n' +
      '\n' +
      '**DataComp Medium and Large Scale.** 제안된 MLM 필터와 다른 기준선 사이의 DataComp 결과는 각각 Medium and Large scale에 대해 표 2와 표 3에 제시되어 있다. 중간 규모의 DataComp 벤치마크에서 제안된 MLM 필터는 서로 다른 태스크 하위 그룹에서 CLIPScore 기준선보다 크게 향상되어 ImageNet-1k에서 +3.2 정확도, ImageNet 이동 데이터 세트 6개에서 +2.6 평균 정확도, VTAB 데이터 세트 13개에서 +2.3 평균 정확도, 검색 데이터 세트 3개에서 +4.9 평균 점수의 눈에 띄는 개선을 달성했다. 또한, 제안된 MLM 필터는 데이터콤 중대형 벤치마크에서 38개의 데이터 세트에 대한 평균 점수에서 +1.7 및 +1.3 개선으로 CLIPScore 기준선을 능가하며, 이는 제안된 MLM 필터가 CLIPScore 필터보다 더 효과적인 필터링 방법으로 작용할 수 있음을 보여준다. 추가적으로, 우리는 그 결과로부터 다음과 같은 보조 결론을 도출할 수 있다:\n' +
      '\n' +
      '**GPT-4V로부터 학습된 MLM 필터는 GPT-4.**로부터 학습된 MLM 필터보다 ImageNet 관련 데이터셋에서 더 좋은 성능을 보인다. *MLM-Filter-GPT4V는 ImageNet-1k 및 6 ImageNet Shifted 데이터셋 모두에서 가장 좋은 성능을 보인다. MLM-Filter-GPT4V에 의해 생성된 Image Text Matching과 Object Detail Fulfillment의 필터링 메트릭은 모두 MLM-Filter-GPT4의 가장 우수한 ImageNet-1k 정확도를 능가하여 +1.1 정확도의 주목할만한 개선을 달성했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline\n' +
      '**Filter** & **Metrics** & \\begin{tabular}{c} **Teacher** \\\\ **Model** \\\\ \\end{tabular} & **ImageNet-1k** & \\begin{tabular}{c} **ImageNet** \\\\ **dist. shifts** \\\\ \\end{tabular} & **VLAB** & **Retrieval** &\n' +
      '\\begin{tabular}{c} **Average over** \\\\ **38 datasets** \\\\ \\end{tabular} \\\\ \\hline No Filtering & - & - & 45.9 & 37.8 & 42.6 & 41.9 & 43.7 \\\\ Basic Filtering & Rules & - & 51.6 & 42.3 & 44.6 & 48.0 & 45.8 \\\\ LAION Filtering & CLIPScore+Rules & - & 55.3 & 45.3 & 51.0 & 49.5 & 50.1 \\\\ CLIPScore & CLIPScore & - & 57.8 & 47.4 & 53.8 & 46.6 & 52.9 \\\\ \\hline MLM-Filter & Object Detail Fulfillment & GPT-4 & 58.9 & 48.9 & 57.4 & 52.5 & 54.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 베이스라인 필터링 방법을 사용하여 사전 훈련된 CLIP 모델의 제로샷 성능 및 데이터콤 벤치마크의 _Large_ 스케일 풀에서 제안된 MLM-Filter.\n' +
      '\n' +
      '**서로 다른 교사 모델로부터 학습된 미세 조정된 MLM 필터에 대해 최적의 필터링 메트릭이 달라진다.** 서로 다른 교사 모델로부터 학습된 제안된 MLM 필터에 대해 단일 메트릭 필터링 설정 하에서 최적의 필터링 메트릭이 다르다. Image-Text Matching은 MLM-Filter-GPT4V에 대한 최적의 필터링 메트릭이고, Object Detail Fulfillment 메트릭은 MLM-Filter-GPT4에 가장 도움이 된다. 캡션 텍스트 품질과 의미 이해의 다른 두 가지 메트릭은 데이터콤 벤치마크에서 효과적인 필터링 품질 메트릭으로 작동하지 않아 CLIPS코어 기준보다 성능이 좋지 않다. 우리는 대부분의 DataComp 평가 데이터셋이 CTQ 및 SU 메트릭의 필터링 방향과 목표에 맞지 않는 이미지 분류 데이터셋이기 때문이라고 생각한다.\n' +
      '\n' +
      '**Image-Text Matching은 검색 작업에 가장 적합한 필터링 메트릭이다.** 본 논문에서 제안하는 MLM 필터는 DataComp Medium 설정 하에서 3가지 이미지 대 텍스트 및 텍스트 대 이미지 데이터 세트에 대해 SOTA 성능을 달성한다. 두 가지 유형의 MLM 필터는 ITM 필터링 메트릭을 사용하여 세 가지 검색 작업에서 평균 30.0 및 29.7의 성능을 달성하여 CLIPScore 기준선을 4.9의 평균 점수로 능가한다. 또한 두 MLM 필터 변형의 결과에서 이미지-텍스트 매칭 메트릭이 다른 세 가지 필터링 메트릭에 비해 검색 작업에서 더 나은 성능을 가져온다는 것을 관찰할 수 있다.\n' +
      '\n' +
      '**서로 다른 품질 메트릭을 결합하는 것은 더 나은 품질의 이미지-텍스트 쌍을 효과적으로 필터링하고 식별한다.** ITM 및 ODF 품질 메트릭을 결합하는 AND 동작은 선택된 데이터포인트의 ITM 및 ODF 점수가 두 메트릭의 필터링 임계값을 초과해야 함을 의미하는 반면, 두 메트릭을 결합하는 OR 동작은 선택된 데이터포인트가 ITM 메트릭에 대한 임계값을 초과하거나 ODF 메트릭에 대한 임계값을 초과해야 함을 의미한다. AND 연산을 사용한 ITM 및 ODF 메트릭의 조합은 모든 베이스라인 필터링 방법 및 MLM 필터의 다른 변형보다 우수하여 38개의 데이터 세트에서 34.5의 최상의 평균 성능을 달성한다.\n' +
      '\n' +
      '** 디지트 분류 작업에서 더 나쁜 성능은 MLM-Filter-GPT4V가 23 ImageNet, VTAB 및 검색 데이터 세트에서 MLM-Filter-GPT4를 현저하게 능가하는 것을 방지한다.** MLM-Filter-GPT4V가 23 ImageNet, VTAB 및 검색 데이터 세트에서 MLM-Filter-GPT4를 능가하더라도 MLM-Filter-GPT4와 동일한 평균 성능만을 달성한다. 이는 두 디지트 분류 데이터 세트에서 MLM-Filter-GPT4의 성능이 표 4에 표시된 MLM-Filter-GPT4보다 5.1 평균 점수만큼 크게 지연되어 38 데이터 세트에서 0.27 평균 점수 차이로 이어지기 때문이다. 두 품질 메트릭의 조합은 MLM-Filter-GPT4V의 디지트 분류 성능을 촉진하지만 해결하지는 못한다.\n' +
      '\n' +
      '### BLIP2 Pre-Training\n' +
      '\n' +
      '다양한 VLM 모델 아키텍처에 걸쳐 제안된 MLM 필터의 효율성을 입증하기 위해 필터링된 데이터셋에서 BLIP-2 VLM을 사전 훈련하고 VQA 데이터셋에서 이러한 BLIP-2 모델의 제로샷 성능을 평가하여 고수준 비전 언어 작업에 대한 필터링 방법의 효율성을 비교한다.\n' +
      '\n' +
      '**Training setup.** CLIPScore filtering과 제안된 MLM Filtering을 이용하여 DataComp Large 1.28B 데이터 풀에서 필터링된 데이터셋을 직접 사용한다. CLIPScore 필터링된 데이터셋과 MLM 필터링된 데이터셋 모두에 대해 배치 크기 및 사전 학습 단계의 수는 원본 구현[11]과 동일하게 유지되며, 여기서 BLIP-2 모델은 모두 사전 학습 단계 1의 경우 420M 이미지, 2의 경우 154M 이미지 상에서 반복 학습되며, 사전 학습 단계는 동일한 하이퍼파라미터 및 GPU 수를 사용한다. BLIP-2 아키텍처에 사용된 비주얼 인코더와 LLM은 Eva-CLIP이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline\n' +
      '**Filter** & **Metrics** & **SVIN** & **MNIST** & **Avg.** \\\\ \\hline MLM-Filter-GPT4 & ITM & 8.2 & 10.3 & 9.2 \\\\ MLM-Filter-GPT4 & ODF & 14.6 & 19.3 & 16.9 \\\\ \\hline MLM-Filter-GPT4V & ITM & 15.4 & 8.3 & 11.8 \\\\ MLM-Filter-GPT4V & ODF & 9.0 & 6.8 & 7.9 \\\\ \\hline MLM-Filter-GPT4V & AND & 12.9 & 11.6 & 12.3 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: SVHN 및 MNIST 디지트 분류 데이터셋에 대한 사전 학습된 CLIP의 제로샷 성능. 에이브 는 두 개의 디지트 데이터 세트의 평균 성능을 나타낸다. AND는 AND 연산을 사용하는 ITM 및 ODF 메트릭의 조합을 나타낸다.\n' +
      '\n' +
      'ViT-g/14 [SFW\\({}^{+}\\)23]와 Vicuna-7b [CLL\\({}^{+}\\)23]을 각각 나타내었다. 더 많은 교육 세부 사항은 부록 E 표 9에서 확인할 수 있다.\n' +
      '\n' +
      'VQAv2 [GKSS\\({}^{+}\\)17]와 GQA [HM19] 데이터셋에 대해 0-shot 방식으로 사전 학습된 두 BLIP-2 모델을 평가하였고, 0-shot VQA 성능 결과를 표 5에 나타내었다. MLM-Filter-GPT4 필터링 영상 텍스트 데이터로 사전 학습된 BLIP-2는 CLIPSCore 필터링 데이터셋에 사전 학습된 BLIP-2보다 VQAv2 및 GQA 데이터셋에 대해 +1.7 및 +1.4 개선을 달성하였다.\n' +
      '\n' +
      '### Human Scoring과의 상관관계\n' +
      '\n' +
      '우리는 [ZLW\\({}^{+}\\)23]을 따라 인간 채점과 모델 채점 사이의 상관 관계를 계산하여 인간과 필터링 모델 간의 정렬을 평가한다. 100개의 이미지-텍스트 쌍을 CC12M과 MSCOCO[LMB\\({}^{+}\\)14]로부터 샘플링하고 이미지-텍스트 매칭의 관점에서 인간 점수로 라벨링한다. CLIPSCore 및 미세 조정된 MLM 필터들은 선택된 이미지-텍스트 쌍들에 대한 이미지-텍스트 매칭 스코어들을 생성하기 위해 사용된다. 그런 다음 표 6에 제시된 바와 같이 인간 점수와 모델 점수 사이에 피어슨 및 스피어만 점수가 보고된다. 제안된 MLM-필터 점수는 인간 품질 점수와 유의하게 정렬되고 상관 관계가 있는 반면 CLIPSCore는 그러한 상관 관계를 입증하지 않는다. 두 가지 품질 메트릭 이미지-텍스트 매칭 및 객체 상세 채우기는 모두 유사한 수준에서 유의한 상관 관계를 보여준다.\n' +
      '\n' +
      '### Analysis\n' +
      '\n' +
      '** 필터링 분획의 효과** CLIP 사전 훈련을 위해 선택된 샘플의 분획이 DataComp Medium 벤치마크 성능에 미치는 영향을 조사하기 위해 절제 연구를 수행한다. DataComp medium pool의 총 128M 영상 중 5개의 분획(\\{0.2,0.25,0.3,0.35,0.4\\}\\)을 선택하였다. 결과는 표 4에 제시되어 있다. CLIP 트레이닝을 위해 선택된 이미지의 상위 30%가 가장 좋은 성능을 달성하며, 이는 [GIF\\({}^{+}\\)23]에서도 관찰된다. 5% 포이즌 데이터를 추가하더라도 이미지넷과 38개 데이터 세트에서 평균 성능이 크게 떨어진다.\n' +
      '\n' +
      '**MLM 필터의 효율성.** 품질 점수 생성에 사용되는 MLM 필터는 14B 모델 파라미터를 갖는 LLaVA-1.5이고, CLIPSCore는 총 492M 파라미터를 갖는 CLIP ViT-L/14 모델을 채택한다. 제안된 MLM 필터의 모델 크기가 CLIPSCore의 모델 크기보다 훨씬 크더라도, CLIP의 듀얼 인코더 아키텍처의 계산 중복성으로 인해, 10k 이미지-텍스트 쌍에 대한 스코어를 생성하는 시간 비용은 하나의 A100 GPU를 사용하여 MLM 필터의 경우 평균 24.3분, CLIPSCore-ViT/L의 경우 11.2분이다. 또한 언어 모델 추론 가속의 최신 기술인 텐서RT-LLM 툴킷1의 도움으로 MLM의 점수 생성을 가속화한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Filter** & **Metric** & **Pearson** & **Spearman** \\\\ \\hline CLIPSCore & - & 0.164 & 0.072 \\\\ \\hline MLM-Filter-GPT4 & ITM & **0.452\\({}^{*}\\)** & **0.430\\({}^{*}\\)** \\\\ MLM-Filter-GPT4 & ODF & 0.410\\({}^{*}\\) & 0.384\\({}^{*}\\) \\\\ MLM-Filter-GPT4V & ITM & 0.328\\({}^{*}\\) & 0.331\\({}^{*}\\) \\\\ MLM-Filter-GPT4V & ODF & 0.368\\({}^{*}\\) & 0.374\\({}^{*}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 인간 라벨 품질 점수와 MLM-Filter 및 CLIP에 의해 생성된 점수 사이의 피어슨 및 스피어먼 상관 관계. 영상은 MLMFilter에 대해 100의 척도로 채점되고 CLIPSCore는 100의 척도로 정규화되어 있으며, \\({}^{*}\\)은 \\(p<0.05\\)에서 유의한 상관관계를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Filter** & **Metric** & **VQA** & **GQA** \\\\ \\hline CLIPSCore & CLIPScore & 55.1 & 34.8 \\\\ \\hline MLM-Filter-GPT4 & ODF & 56.8 & 36.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 상이한 필터링 방법에 의해 필터링된 데이터세트에 대해 미리 훈련된 BLIP-2 모델의 제로 샷 VQA 성능.\n' +
      '\n' +
      '4회 여과하면 10k 샘플의 평균 6.1분이 발생한다. 따라서 제안된 MLM 필터는 CLIPScore보다 훨씬 우수한 효율을 얻을 수 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 품질 채점 작업에 대한 멀티모달 언어 모델을 명령어 조정하고, 이러한 미세 조정된 MLM을 효과적인 데이터 필터로 활용하여 대규모 웹 크롤링된 데이터 세트에서 고품질 이미지 텍스트 쌍을 선택할 것을 제안한다. 우리는 CLIP 및 BLIP-2 모델에서 제안된 MLM 필터로 필터링된 데이터셋에 대한 사전 트레이닝이 CLIPS코어 필터링된 데이터셋에 대한 사전 트레이닝보다 훨씬 우수하다는 것을 발견하여 CLIPS코어 필터링에 비해 제안된 MLM 필터의 우수성을 입증한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[ADL\\({}^{+}\\)22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736, 2022에서의 발전.\n' +
      '*[BGJ\\({}^{+}\\)23] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo 등 보다 우수한 캡션을 갖는 이미지 생성 개선. _ 컴퓨터 과학 https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3), 2023.\n' +
      '*[BPK\\({}^{+}\\)22] 민우변, 박범희, 해천김, 이성준, 백운혁, 김새훈. Coyo-700m: Image-Text pair dataset. [https://github.com/kakabrain/coyo-dataset] (https://github.com/kakabrain/coyo-dataset), 2022.\n' +
      '*[CLD\\({}^{+}\\)23] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin. Sharegpt4v: 캡션이 더 좋은 대형 멀티모달 모델 개선. _ arXiv preprint arXiv:2311.12793_, 2023.\n' +
      '*[CLL\\({}^{+}\\)23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, 및 Eric P. Xing. 비쿠나: 2023년 3월, 90%의 채팅 품질을 가진 gpt-4를 인상하는 오픈 소스 챗봇.\n' +
      '*[CLY\\({}^{+}\\)23] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: 더 적은 데이터로 더 나은 알파카를 훈련한다. _ arXiv preprint arXiv:2307.08701_, 2023.\n' +
      '* [CRLB18] Oana-Maria Camburu, Tim Rocktaschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: 자연어 설명과 자연어 추론_ 신경 정보 처리 시스템_, 31, 2018의 발전.\n' +
      '*[DDS\\({}^{+}\\)09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. Imagenet: 대규모 계층 이미지 데이터베이스. 2009년 IEEE Conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.\n' +
      '*[DLL\\({}^{+}\\)23] 원량 다이, 준난 리, 동수 리, 앤서니 멍 화트 티온그, 준치 자오, 위성 왕, 보양 리, 파스칼레 펑, 스티븐 호이. 인스트럭션 블립: 명령어 튜닝이 있는 범용 비전 언어 모델에 대해, 2023.\n' +
      '*[FJJ\\({}^{+}\\)23] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, Vaishaal Shankar. 데이터 필터링 네트워크. _ arXiv preprint arXiv:2309.17425_, 2023.\n' +
      '\n' +
      '그림 4: CLIP 트레이닝을 위해 선택된 이미지의 분율의 효과.\n' +
      '\n' +
      '[GIF\\({}^{+}\\)23]Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: 차세대 멀티모달 데이터셋을 검색한다. _ arXiv preprint arXiv:2304.14108_, 2023.\n' +
      '*[GKSS\\({}^{+}\\)17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. vqa 문제에서 v를 만드는 것: 시각적 질문 응답에서 이미지 이해의 역할을 높입니다. IEEE Conference on computer vision and pattern recognition_의 _Proceedings, pages 6904-6913, 2017.\n' +
      '*[HDW\\({}^{+}\\)24] Shaohan Huang, Li Dong, Wenhui Wang, Yuru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra 등 언어는 당신이 필요로 하는 전부가 아니다: 언어 모델과 인식을 정렬하는 것; _ 신경 정보 처리 시스템_, 36, 2024의 발전.\n' +
      '*[HHF\\({}^{+}\\)21] 잭 헤셀, 아리 홀츠만, 맥스웰 포브스, 로난 르 브라스, 그리고 최예진. Clipscore: 이미지 캡셔닝을 위한 참조없는 평가 메트릭. _ arXiv preprint arXiv:2104.08718_, 2021.\n' +
      '[HM19] 드류 A 허드슨과 크리스토퍼 D 매닝. Gqa: 실제 시각 추론 및 구성 질문 답변을 위한 새로운 데이터 세트. 2019년 _CVPR_에서.\n' +
      '*[JYX\\({}^{+}\\)21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig. 시끄러운 텍스트 감독으로 시각 및 시각 언어 표현 학습을 확장합니다. _International conference on machine learning_, pages 4904-4916. PMLR, 2021.\n' +
      '* [LLLL23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 시각적 지시 조정을 통해 개선된 기준선입니다. _ arXiv preprint arXiv:2310.03744_, 2023.\n' +
      '* [LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_, 2023.\n' +
      '* [LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 시각적 지시 조율 arXiv preprint arXiv:2304.08485_, 2023.\n' +
      '* [LMB\\({}^{+}\\)14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, 및 C Lawrence Zitnick. 마이크로소프트 코코: 맥락상 흔한 물건들. _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, Proceedings, Part V 13_, pages 740-755. Springer, 2014.\n' +
      '*[MGL\\({}^{+}\\)23] Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-mars: 텍스트 특징 학습을 우회하여 시각적 표현을 개선한다. _ arXiv preprint arXiv:2307.03132_, 2023.\n' +
      '* [MKBH21] 스와룹 미쉬라, 다니엘 카샤비, 치타 바랄, 한나네 하지시르지. 자연어 크라우드소싱 지침을 통한 교차 작업 일반화 arXiv preprint arXiv:2104.08773_, 2021.\n' +
      '[MRFM19] 케네스 마리노, 모하마드 라스테가리, 알리 파하디, 루즈베 모타기. Ok-vqa: 외부 지식이 필요한 시각적 질문 응답 벤치마크. _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [MSSC19] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: 이미지의 텍스트를 판독하여 시각적 질문 응답. _2019 국제학술대회에서는 문서 분석 및 인식(ICDAR)_, 페이지 947-952. IEEE, 2019에 개시되어 있다.\n' +
      '*[INW\\({}^{+}\\)22] Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, O Se웅, 그리고 Ludwig Schmidt. 질적인 양이 아닌 양: 데이터세트 설계와 클립의 견고성 사이의 상호 작용에 대해. _ 신경 정보 처리 시스템_, 35:21455-21469, 2022에서의 발전.\n' +
      '*[Ope23] OpenAI. Gpt-4v(비전) 기술 작업 및 저자. 2023년\n' +
      '*[OWJ\\({}^{+}\\)22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744, 2022에서의 발전.\n' +
      '* [RG20] Nils Reimers and Iryna Gurevych. 지식 증류를 사용하여 다국어를 포함하는 단일 언어 문장 만들기 In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_. 2020년 11월, 컴퓨팅 언어학 협회\n' +
      '\n' +
      '* [RKH\\({}^{+}\\)21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 자연어 감독에서 전이 가능한 시각적 모델을 학습합니다. 2021년 _ICML_에서.\n' +
      '*[SBV\\({}^{+}\\)22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: 차세대 이미지-텍스트 모델을 훈련시키기 위한 개방형 대규모 데이터세트. _ 신경 정보 처리 시스템_, 35:25278-25294, 2022에서의 발전.\n' +
      '* [SDGS18a] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 개념 캡션: 자동 이미지 캡션을 위한 세척, 하이퍼니밍, 이미지 알트 텍스트 데이터 세트. _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.\n' +
      '* [SDGS18b] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 개념 캡션: 자동 이미지 캡션을 위한 세척, 하이퍼니밍, 이미지 알트 텍스트 데이터 세트. _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, 2018.\n' +
      '*[SFW\\({}^{+}\\)23] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: 스케일에서 클립을 위한 개선된 훈련 기법. _ arXiv preprint arXiv:2303.15389_, 2023.\n' +
      '*[SGM\\({}^{+}\\)22] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, Pushpak Bhattacharyya. Scienceqa: 학술 논문에 대한 질의응답을 위한 참신한 자료. _ International Journal on Digital Libraries_, 23(3):289-301, 2022.\n' +
      '*[Sha23] ShareGPT. [https://sharegpt.com/] (https://sharegpt.com/), 2023.\n' +
      '* [SHRS20] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 텍스트캡: 읽기 이해력이 있는 이미지 캡션을 위한 데이터 세트입니다. _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, Proceedings, Part II 16_, pages 742-758. Springer, 2020.\n' +
      '*[STQ\\({}^{+}\\)20] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. Mpnet: 언어 이해를 위한 마스킹 및 퍼뮤테이션 프리 트레이닝. _ 신경 정보 처리 시스템_, 33:16857-16867, 2020에서의 발전.\n' +
      '*[SVB\\({}^{+}\\)21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: 클립 필터링된 4억 개의 이미지-텍스트 쌍들의 오픈 데이터세트 _ arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [TGZ\\({}^{+}\\)23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '* [TJS23] 성방통, 에릭 존스, 제이콥 스타인하르트. 언어 모델을 사용한 멀티모달 시스템의 대량 생산 실패 arXiv preprint arXiv:2306.12105_, 2023.\n' +
      '* [TLZ\\({}^{+}\\)24] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie. 눈을 크게 감고? 멀티모달 llms의 시각적 단점을 탐구합니다. _ arXiv preprint arXiv:2401.06209_, 2024.\n' +
      '*[TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '*[WBZ\\({}^{+}\\)21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 최적화된 언어 모델은 제로샷 학습자입니다. _ arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '*[WDC\\({}^{+}\\)22] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei. 시각적으로 증강된 언어 모델링 arXiv preprint arXiv:2205.10178_, 2022.\n' +
      '* [WJHS23] Lai Wei, Zihao Jiang, Weiran Huang 및 Lichao Sun. Instructiongpt-4: 200-instruction paradigm for fine-tuning minigpt-4. _arXiv preprint arXiv:2308.12067_, 2023.\n' +
      '*[WLY\\({}^{+}\\)23] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang. Cogvlm: 사전 훈련된 언어 모델에 대한 시각적 전문가, 2023.\n' +
      '\n' +
      '*[WWS\\({}^{+}\\)22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou. 사고 유발의 사슬은 큰 언어 모델에서 추론을 이끌어낸다. _ arXiv preprint arXiv:2201.11903_, 2022.\n' +
      '* [XXT\\({}^{+}\\)23] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. 클립 데이터의 디시스터마이징 arXiv preprint arXiv:2309.16671_, 2023.\n' +
      '*[YLL\\({}^{+}\\)23] Zhen규안 Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, 및 Lijuan Wang. lmms의 새벽: gpt-4v(ision)를 이용한 예비 탐사 arXiv preprint arXiv:2309.17421_, 9(1):1, 2023.\n' +
      '*[YTK\\({}^{+}\\)23] Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, Heng Wang. 악마는 세부 사항에 있습니다: 데이터 필터링의 토끼 구멍으로 깊이 파고듭니다. _ arXiv preprint arXiv:2309.15954_, 2023.\n' +
      '*[ZCS\\({}^{+}\\)23] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: 고급 대형 언어 모델로 비전 언어 이해력 향상. _ arXiv preprint arXiv:2304.10592_, 2023.\n' +
      '*[ZLW\\({}^{+}\\)23] 신루 장, 유제 루, 웨이지 왕, 안 얀, 준 얀, 리안케 진, 헝 왕, 시펑 얀, 윌리엄 양 왕, 린다 루스 페츤드. 비전 언어 작업에 대한 일반주의 평가자로서의 Gpt-4v(ision). _ arXiv preprint arXiv:2311.01361_, 2023.\n' +
      '*[ZPK\\({}^{+}\\)19] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019년\n' +
      '\n' +
      'Prompt Construction\n' +
      '\n' +
      '첫 번째 버전의 프롬프트를 수동으로 작성한 후 GPT-4를 활용하여 사람이 작성한 프롬프트를 정제합니다. 4개의 품질 채점 작업에 대한 최종 프롬프트는 다음과 같다:\n' +
      '\n' +
      '**Image Text Matching**\n' +
      '\n' +
      '제공된 텍스트 캡션이 이미지의 주요 특징 및 객체를 정확하게 나타내는지 평가하십시오. 캡션은 이미지의 모든 측면을 자세히 설명할 필요는 없지만 주요 주제를 캡처해야 합니다. 언급된 기준을 고려하여 텍스트 캡션의 이미지와 일치되는 전체 품질을 1-100의 척도로 평가합니다.\n' +
      '\n' +
      '**Object Detail Fulfillment**\n' +
      '\n' +
      '텍스트 캡션이 이미지 설명과 일치하는 객체에 대한 자세한 설명을 제공하는지 확인하려면 텍스트 캡션을 평가하십시오. 구체적으로, 캡션이 객체들의 색상, 크기, 위치, 형상, 재료 등을 충분히 기술하는지 평가한다. 이후 제공된 기준에 따라 이미지에서 객체 세부 정보를 캡처하는 데 캡션의 전체 정확도를 1-100의 척도로 평가한다.\n' +
      '\n' +
      '**Caption Text Quality**\n' +
      '\n' +
      '텍스트 캡션은 문법적 정확성, 어휘의 다양성(예: 사용된 단어의 범위와 고유성), 유창성(예: 문장의 매끄러움과 자연스러운 흐름), 가독성, 길이, 구조 등의 기준에 따라 평가해 주시기 바랍니다. 전체 품질 점수를 1-100의 척도로 지정합니다.\n' +
      '\n' +
      '**Semantic Understanding**\n' +
      '\n' +
      '해당 이미지 설명과 관련하여 지정된 텍스트 캡션을 평가하십시오. 텍스트 캡션이 이미지 자체만으로 쉽게 알 수 없는 추가 의미 정보를 제공하는지 확인하는 것이 목표입니다.\n' +
      '\n' +
      'For example:\n' +
      '\n' +
      '1. 이미지 기술이 "남자"를 언급하지만 캡션이 그를 "노숙자" 또는 "사업가"라고 정교하게 만든다면, 캡션은 의미적 맥락을 풍부하게 하고 있다.\n' +
      '\n' +
      '2. 자막이 수학적 접함수와 같은 개념을 도입하여 추론하기 위해서는 깊이 있는 지식을 필요로 하는 경우, 외부 의미론을 부여하고 있다.\n' +
      '\n' +
      '3. 특정 위치 주소, 축제 세부사항, 또는 이미지로부터 추론하기 쉽지 않은 다른 미묘한 데이터를 드러내는 캡션은 또한 외부 의미 정보를 제공한다.\n' +
      '\n' +
      '4. 캡션에서 건물, 사람, 조류 종, 동물 품종, 자동차 모델, 엔진 등과 같은 이미지 내의 특정 엔티티들을 직접 식별하는 것은 추가적인 통찰을 도입한다.\n' +
      '\n' +
      '5. 이미지가 맥락적 배경으로서 작용하고 캡션이 이미지에 명시적으로 나타나지 않는 요소를 기술해야 하며, 이는 의미론적 깊이를 갖는다.\n' +
      '\n' +
      '6. 마지막으로, 캡션이 이해하는데 상식적인 지식이 필요한 이미지 내의 피사체들 간의 관계를 묘사한다면, 의미적으로 풍부하다고 간주되어야 한다.\n' +
      '\n' +
      '캡션이 이미지 설명을 통해 제공하는 의미 강화 정도를 평가하고 결정하십시오. 텍스트 캡션의 의미 깊이를 1에서 100까지 척도로 평가합니다.\n' +
      '\n' +
      '두 가지 프롬프트 전략의 예제\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l} \\hline \\hline Example for **Chain-of-Thought Reasoning** & Example for **Rationalization** \\\\ \\hline Please evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn’t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption\'s match to the image on a scale of 1-100, considering the criteria mentioned. & Please evaluate if the provided text caption accurately represents the main features and objects of the image. The caption doesn’t need to detail every aspect of the image, but it should capture its primary theme. Rate the overall quality of the text caption’s match to the image on a scale of 1-100, considering the criteria mentioned. & Please first output a single line containing the value indicating the scores. **In the subsequent line, please output a single line containing the value indicating the scores.** & **line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias.** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 이미지-텍스트 매칭 점수를 평가하기 위한 제로-샷 체인-사고 추론 및 합리화 추론에 대한 프롬프트.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l} \\hline \\hline Data & Size & Task \\\\ \\hline Visual Conversation [11] & 5K & Conversation \\\\ Complex Reasoning [11] & 16k & Visual Reasoning \\\\ Detail Description [11] & 5k & Captioning \\\\ \\hline ShareGPT [14] & 10K & Language-Only Instructions \\\\ \\hline VQAv2 [15] & 2K & VQA \\\\ GQA [16] & 3K & Visual Reasoning \\\\ OKVQA [17] & 2K & Knowledge Grounded VQA \\\\ OCRVQA [18] & 1K & OCR \\\\ TextCaption [19] & 2K & Captioning \\\\ \\hline ITM Scoring & 1k & \\\\ ODF Scoring & 1k & \\\\ CTQ Scoring & 1k & \\\\ SU Scoring & 1k & \\\\ \\hline \\hline Total & 50k & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 데이터 품질 스코어링 태스크들과 다른 멀티모달 태스크들의 멀티모달 명령어 데이터 혼합물.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r} \\hline \\hline\n' +
      '**Hyperparameter** & BLIP-2 \\\\ \\hline \\multicolumn{2}{c}{**Stage-1 Pre-training**} \\\\ \\# Trainable Parameters & 188M \\\\ Precision & float16 \\\\ Global Batch Size & 1680 \\\\ \\# Training Steps & 250k \\\\ \\# GPUs & 16 \\\\ \\# Gradient Accumulation Steps & 1 \\\\ Min LR & 1e-5 \\\\ Peak LR & 1e-4 \\\\ \\# Warmup Steps & 2000 \\\\ LR Scheduler & Cosine LR Decay \\\\ Weight Decay & 0.05 \\\\ Adam \\((\\beta_{1},\\beta_{2})\\) & (0.9, 0.98) \\\\ \\hline \\multicolumn{2}{c}{**Stage-2 Pre-training**} \\\\ \\# Trainable Parameters & 188M \\\\ Precision & float16 \\\\ Global Batch Size & 1920 \\\\ \\# Training Steps & 80k \\\\ \\# GPUs & 16 \\\\ \\# Gradient Accumulation Steps & 4 \\\\ Min LR & 5e-5 \\\\ Peak LR & 1e-4 \\\\ \\# Warmup Steps & 2000 \\\\ LR Scheduler & Cosine LR Decay \\\\ Weight Decay & 0.05 \\\\ Adam \\((\\beta_{1},\\beta_{2})\\) & (0.9, 0.98) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: BLIP-2 사전 훈련 단계 1 및 단계 2에 대한 훈련 세부사항.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>