<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MusicRL: 음악 생성을 인간 선호도에 정렬\n' +
      '\n' +
      'Geoffrey Cideron\n' +
      '\n' +
      'Sertan Girgin\n' +
      '\n' +
      'Mauro Verzetti\n' +
      '\n' +
      'Damien Vincent\n' +
      '\n' +
      'Matej Kastelic\n' +
      '\n' +
      'Zalan Borsos\n' +
      '\n' +
      'Brian McWilliams\n' +
      '\n' +
      'Victor Ungureanu\n' +
      '\n' +
      'Olivier Bachem\n' +
      '\n' +
      'Olivier Pietquin\n' +
      '\n' +
      'Matthieu Geist\n' +
      '\n' +
      'Leonard Hussenot\n' +
      '\n' +
      'Neil Zeghidour\n' +
      '\n' +
      'Sedideron@google.com, agostinelli@google.com\n' +
      '\n' +
      'Andrea Agostinelli\n' +
      '\n' +
      'Google DeepMind\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 인간의 피드백에 의한 최초의 음악 생성 시스템인 MusicRL을 제안한다. 텍스트 대 음악 모델의 감상은 음악성의 개념뿐만 아니라 자막 이면의 특정 의도가 사용자 의존적이기 때문에 특히 주관적이다(예를 들어, "업비트 운동 음악"과 같은 자막은 레트로 기타 솔로 또는 테크노팝 박자에 매핑될 수 있다). 이는 이러한 모델의 감독 훈련을 어렵게 만들 뿐만 아니라 배치 후 미세 조정에 지속적인 인간 피드백을 통합해야 한다. MusicRL은 시퀀스-레벨 보상을 최대화하기 위해 강화 학습으로 미세 조정된 이산 오디오 토큰들의 사전 훈련된 자기회귀 뮤직LM(Agostinelli et al., 2023) 모델이다. 우리는 선택된 평가자의 도움을 받아 텍스트 준수 및 오디오 품질과 관련된 보상 기능을 설계하고 이를 사용하여 MusicLM을 MusicRL-R로 미세 조정한다. 우리는 MusicLM을 사용자에게 배포하고 300,000개의 쌍별 선호도를 포함하는 실질적인 데이터 세트를 수집한다. 인간 피드백으로부터 강화 학습(Reinforcement Learning from Human Feedback;RLHF)을 사용하여, 우리는 인간의 피드백을 스케일에서 통합하는 최초의 텍스트-음악 모델인 MusicRL-U를 훈련한다. 인간 평가는 MusicRL-R과 MusicRL-U가 모두 기준선보다 선호된다는 것을 보여준다. 궁극적으로 MusicRL-RU는 두 가지 접근법을 결합하여 인간 평가자에 따라 최상의 모델을 만든다. 절제 연구는 인간의 선호도에 영향을 미치는 음악적 속성을 조명하여 텍스트 준수 및 품질이 그 일부만을 설명함을 나타낸다. 이것은 음악 감상에 대한 주관성의 만연을 강조하고 음악 생성 모델의 미세 조정에 인간 청취자의 추가 참여를 요구한다. 샘플은 Google-research.github.io/seanet/musiclm/rhlf/에서 찾을 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '음악의 생성 모델링은 최근까지 개별 악기의 미세 모델링(Defossez et al., 2018; Engel et al., 2017, 2020) 또는 다성 음악의 거친 생성(Dhariwal et al., 2020)으로 제한되었지만, 모델은 이제 개방형, 고충실도 텍스트 제어 음악 생성을 처리할 수 있다(Agostinelli et al., 2023; Copet et al., 2023; Forsgren and Martiros, 2022; Liu et al., 2023). 특히, MusicLM(Agostinelli et al., 2023) 및 MusicGen(Copet et al., 2023)과 같은 텍스트-투-음악 시스템들은 생성 프로세스를 신경 오디오 코덱(Defossez et al., 2022; Zeghidour et al., 2022)의 이산 표현 공간에서 자기회귀 예측 태스크로서 캐스팅함에 따라, 오디오 언어 모델들을 기반으로 구축된다. 이 접근법은 현실적인 스피치(Borsos et al., 2023, 2023; Wang et al., 2023), 사운드 이벤트(Kreuk et al., 2022) 및 음악을 생성하는 능력을 입증했지만, 몇 가지 단점을 겪는다. 첫째, 이러한 시스템을 훈련시키는 데 사용되는 다음 토큰 예측 작업은 임의의 오디오 신호를 모델링하기에 충분히 일반적이지만 청취자에게 더 매력적인 음악을 생성하는 쪽으로 편향시킬 수 있는 음악성에 대한 사전 지식이 부족하다. 둘째, 추론에 사용되는 온도 샘플링은 단일 텍스트 캡션으로부터 다양한 오디오를 생성할 수 있지만, 이러한 다양성은 멜로디 또는 성능과 같은 특정 축을 따라만 바람직한 반면, 음악성과 프롬프트에 대한 순응도는 일관되게 높게 유지되어야 한다.\n' +
      '\n' +
      '자기회귀 생성 모델의 이러한 근본적인 문제는 언어 모델링의 맥락에서 광범위하게 관찰되고 해결되었다. 예를 들어, 여러 작업에서 BLEU 점수를 최대화하기 위해 기계 번역 모델을 미세 조정(Ranzato et al., 2016; Wu et al., 2016)하거나 관련 ROUGE 메트릭을 개선하기 위해 요약 모델을 탐색했다(Ranzato et al., 2016; Roit et al., 2023; Wu and Hu, 2018). 이러한 메트릭들은 전형적으로 시퀀스-레벨이고, 미분가능하지 않은 샘플링 프로세스(예를 들어, 그리디 디코딩, 온도 샘플링)의 출력을 평가한다. 이는 일반적으로 보상 함수의 관심 메트릭과 생성 모델을 정책으로 모델링하는 강화 학습 방법을 사용하여 우회한다. 이러한 텍스트 생성 시스템과 자기회귀 음악 모델 사이의 근본적인 알고리즘 유사성은 적절한 보상 함수를 고려할 때 음악 생성을 개선하기 위해 강화 학습을 사용할 수 있음을 시사한다.\n' +
      '\n' +
      '프롬프트가 주어졌을 때 생성된 음악은 입력 텍스트에 대한 순응성, 높은 음향 품질(인공물의 부재), "음악성" 또는 일반적인 쾌적성의 세 가지 특성을 나타내야 한다. 자동 메트릭은 분류기 KLD(Yang et al., 2022) 또는 MuLan Cycle Consistency(Agostinelli et al., 2023)와 같은 텍스트 준수 및 Frechet Audio Distance(Kilgour et al., 2019)를 갖는 음향 품질을 정량화하기 위해 제안되었다. 이러한 메트릭은 보상 함수로서 사용될 수 있다. 그러나 음악성을 측정하기 위해 자동 프록시를 설계하는 것은 어렵다. 이전의 접근법들(Guimaraes et al., 2017; Jaques et al., 2017; Kotecha, 2018; Latif et al., 2023)의 대부분은 복잡한 음악 이론 규칙에 의존하며, 특정 음악 영역(예를 들어, 클래식 피아노)으로 제한되고 단지 인간의 선호도와 부분적으로 정렬된다. 자동 메트릭과 인간 선호도 사이의 이러한 격차는 언어 모델링에서 다시 광범위하게 연구되었으며, RLHF(Reinforcement Learning from Human Preferences)는 대화 모델을 인간 피드백과 정렬하는 _de facto_ 방식이 되었다(Achiam et al., 2023; Team et al., 2023).\n' +
      '\n' +
      '선행 작업(Ouyang et al., 2022; Stiennon et al., 2020)에서 언급하는 인간 선호도는 주로 평가자의 선호도를 의미한다. 평가자들은 모델과 상호작용하는 모집단을 대표하지 않을 수 있다(예를 들어, 아마존 머신 투르크3과 같은 등급 서비스는 글로벌 인력을 사용한다). 특히 음악의 맥락에서 이러한 인구 격차는 선호도에 상당한 영향을 미칠 수 있다(Trehub et al., 2015). 대규모 사용자 선호도 데이터를 수집하면 인구 격차를 줄이는 데 도움이 될 수 있습니다.\n' +
      '\n' +
      '그림 1: RLHF 미세화 모델에 대한 정성적인 측면 평가 결과. 각 X 대 X에서. Y 비교에서 녹색 막대는 모델 X가 선호된 시간의 백분율에 해당하고 노란색 막대는 넥타이의 백분율에, 빨간색 막대는 모델 Y가 선호되었다. MusicRL-R은 품질 및 텍스트 준수 보상에 대해 미세 조정된 MusicLM 모델이다. 음악RL-U는 사용자 선호도의 보상 모델에 대해 미세 조정된다. 뮤직RL-RU는 품질과 텍스트에 대한 준수 및 사용자 선호도의 보상 모델에 대해 순차적으로 미세 조정된다. 모든 RLHF 미세 버전의 MusicLM이 MusicLM보다 크게 우수한 반면, MusicRL-R 및 MusicRL-U는 비교 가능한 성능을 달성하는 반면, MusicRL-RU는 전반적으로 선호되는 모델이다.\n' +
      '\n' +
      ' 평가자와 대조적으로 훨씬 더 많은 상호 작용을 수집합니다.\n' +
      '\n' +
      '본 논문에서는 강화학습을 이용한 텍스트-음악 생성 모델인 MusicRL을 소개한다. MusicLM 기준선에서 시작하여 RL 미세 조정을 수행하기 위해 보상 함수로 텍스트 준수 자동 측정과 새로운 음향 충실도 메트릭을 사용한다. 인간 평가는 \\(win/(win+loss)\\)으로 측정한 결과 결과 MusicRL-R의 세대가 MusicLM의 세대보다 83% 더 선호됨을 나타낸다. 그런 다음 모델을 인간의 판단과 명시적으로 정렬하기 위해 보상 모델에 적합하도록 MusicLM과 상호작용하는 사용자의 쌍별 선호도 데이터 세트를 수집한다. 사용자 상호작용 데이터에 대해 훈련된 보상 모델에 대한 절제 연구는 사용자 선호도가 음악성과 강한 상관관계가 있음을 보여준다. 광범위한 인간 평가는 결과 뮤직RL-U에서 나오는 음악 세대가 기본 모델보다 74% 더 선호된다는 것을 보여준다. 마지막으로, 자동 보상과 인간 피드백을 결합하여 MusicRL-R을 MusicRL-RU로 세분화하고 이 모델이 62% 이상의 모든 대안보다 우수함을 보여준다. 우리가 아는 한, 이 작업은 오디오 생성 모델을 개선하기 위해 규모의 인간 피드백을 활용하는 첫 번째 시도이다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**음악 생성.** 음악 오디오 생성에 대한 이전의 승인들은 높은 품질의 출력들(Dhariwal et al., 2020) 또는 의미적으로 일관된 긴 오디오들(Hawthorne et al., 2022)을 생성하는 측면에서 제한되었지만, 최근의 연구는 즐거운 청취 경험을 허용하는 수준의 품질을 달성하였다. 첫 번째 작업 라인은 신경 오디오 코덱(Defossez et al., 2022; Zeghidour et al., 2022)에 의해 제공되는 이산 토큰 공간에서 음악 생성의 작업을 범주형 예측으로서 캐스팅하고, 다음 토큰 예측을 위한 Transformer-based(Vaswani et al., 2017) 모델(Borsos et al., 2023a) 또는 병렬 토큰 디코딩(Borsos et al., 2023b; Garcia et al., 2023; Parker et al., 2024)을 트레이닝한다. 이 생성 백본을 텍스트 인코더 또는 텍스트-오디오 임베딩을 통해 텍스트-컨디셔닝과 결합한다(Elizalde et al., 2022; Huang et al., 2022)는 고품질의 텍스트-투-음악 모델을 제공한다(Agostinelli et al., 2023; Copet et al., 2023). 평행 작업 라인은 확산 모델에 의존하며 음악 생성의 작업을 오디오 파형 및 스펙트로그램의 잡음 제거(Huang et al., 2023) 또는 학습된 잠재 표현(Liu et al., 2023; Schneider et al., 2023)으로서 캐스팅한다. 두 경우 모두, 모델은 기존의 음악 녹음 모음에서 오프라인으로 훈련되고 추론은 확률적 방식(예: 확산 또는 온도 샘플링)으로 실행되며, 이는 다양성을 제공하지만 출력에 대한 불확실성(예: 텍스트 준수 또는 품질 측면에서)을 제공한다. 이전 작업(Kharitonov et al., 2023)은 많은 시퀀스를 샘플링하여 점수 함수(예: 참조 없는 오디오 품질 추정기)로 순위를 매기고 최상의 후보를 반환함으로써 이 문제를 회피했다. 이는 추론 비용을 상당히 증가시키고 잘 정의된 점수 함수를 필요로 한다.\n' +
      '\n' +
      'MusicRL은 뮤직LM(Agostinelli et al., 2023) 모델을 강화 학습으로 미세 조정함으로써, 자동 메트릭, 소규모 고품질 인간 등급 및 대규모 사용자 피드백으로부터 유도된 보상 함수를 사용하여 이러한 한계를 해결한다. 우리가 아는 한, MusicRL은 수십만 명의 사용자의 피드백을 통합함으로써 얻을 수 있는 이점을 보여주는 최초의 음악 생성 시스템이다.\n' +
      '\n' +
      '**RL-finetuning of music generation models.** RL-finetuning music generation models 내 대부분의 이전 작품들은 음악 이론의 원리들(Guimaraes et al., 2017; Jaques et al., 2017; Kotecha, 2018; Latif et al., 2023) 또는 반복과 같은 간단한 패턴들을 설계하는 것을 포함한다(Karbasi et al., 2021). Jaques et al. (2017)은 KL 정규화 용어와 조합하여 선율 작곡 이론(Gauldin, 1988)에서 영감을 얻은 일련의 규칙(예를 들어, 키에 머무르고, 모티브를 재생하고, 그것들을 반복하고, 과도하게 반복되는 음을 피함)을 사용한다. 이러한 접근 방식은 몇 가지 한계를 가지고 있는데, 규칙 집합은 불완전하거나 모순될 수 있고, 실천가들은 서로 다른 보상 사이의 올바른 균형을 찾아야 하며, 규칙 자체는 인간의 음악적 선호의 불완전한 근사치인 음악 이론에서 파생된다. Jiang et al. (2020)은 음표가 과도하게 반복될 때 -1을 할당하는 규칙 기반 리워드와 데이터로부터 학습된 4개의 리워드 모델로 온라인 음악 반주 생성 모델을 미세화한다. 각각의 보상 모델은 컨텍스트(예측할 컨텍스트 및 청크가 각각의 보상마다 상이함)가 주어진 세대의 청크의 확률에 대응한다. 이러한 보상은 음악 데이터세트 상의 마스킹된 언어 모델(Devlin et al., 2019) 손실로 학습된다. 그러나 이러한 방법은 제한된 음악 영역(예: 모노포닉 피아노) 또는 상징 생성에만 적용된다. 이전 작업과 대조적으로 MusicRL은 자체 원시 오디오 세대로부터 인간의 선호도를 학습한다. 이를 통해 로파이 힙합부터 오케스트라 교향곡, 모달 재즈에 이르기까지 음악 장르와 스타일의 전체 스펙트럼에 걸쳐 음악 생성을 개선할 수 있다.\n' +
      '\n' +
      '인간 피드백의**RL.**RLHF는 최근 바드(Gemini Team, 2023) 또는 GPT-4(OpenAI, 2023)와 같은 애플리케이션에서 사용되는 대화 모델 훈련에서 중요한 단계가 되었다. RLHF는 널리 사용되기 전에 아타리 게임(Christiano et al., 2017)을 해결하기 위해 먼저 적용되었으며, 예를 들어 자연어 작업(Bai et al., 2022; Jaques et al., 2019; Ouyang et al., 2022; Stiennon et al., 2020; Ziegler et al., 2019) 또는 이미지 생성(Lee et al., 2023; Wallace et al., 2023)에서 사용된다. 월러스 등(2023)은 직접 최적화 알고리즘(Direct Optimization Algorithm; DPO)(Rafailov 등, 2023)을 사용하여 인간 선호 데이터에 대한 확산 모델을 미세화한다. 우리가 아는 한, 우리는 RLHF를 음악 생성 모델에 최초로 적용했다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### MusicLM\n' +
      '\n' +
      'MusicLM(Agostinelli et al., 2023)은 텍스트 기술로부터 음악을 생성하기 위한 자기회귀 모델이다. AudioLM(Borsos et al., 2023)의 설계에 이어서, MusicLM은 생성을 위한 두 가지 상이한 유형의 오디오 표현들: w2v-BERT(Chung et al., 2021)와 같은 마스킹된 오디오 언어 모델들의 양자화된 표현들인 _semantic_tokens 및 _acoustic_tokens, SoundStream(Zeghidour et al., 2022)과 같은 신경 오디오 코덱들에 의해 생성된 이산 표현들이다. 시맨틱 토큰은 생성 프로세스의 장기적인 구조적 일관성을 보장하는 반면, 음향 토큰은 고품질 합성을 허용한다. 고-비트레이트 재구성을 보장하기 위해, 사운드스트림은 잔여 벡터 양자화기(Residual Vector Quantization; RVQ)를 사용하는데, 여기서 각각의 양자화기가 이전 양자화기에 의해 생성된 잔차에 대해 동작하는 벡터 양자화기의 스택은 연속적인 오디오 표현들을 이산화하기 위해 음향 토큰들에 계층적 구조를 부과한다. 추가적으로, MusicLM은 기술 텍스트 상에서 오디오 생성 태스크를 컨디셔닝하기 위해 공동 음악-텍스트 대조 모델인 Mulan(Huang et al., 2022)에 의존한다.\n' +
      '\n' +
      'MusicLM은 처음에 3단계 트랜스포머 기반 자기회귀 모델로 도입되었다. 첫 번째 단계는 뮬란과 시맨틱 토큰 간의 매핑을 학습한다. 두 번째 단계는 첫 번째 단계를 예측합니다.\n' +
      '\n' +
      '도 2: 음악 캡션들의 데이터세트가 주어지면, MusicLM은 보상 함수로 스코어링되는 오디오 샘플들을 생성한다. RL 알고리즘은 수신된 보상을 최대화하기 위해 모델을 미세 조정한다.\n' +
      '\n' +
      'levels from the output of the SoundStream RVQ(coarse acoustic tokens) from MuLan and semantic tokens. 마지막 단계는 거친 음향 토큰들로부터 나머지 사운드스트림 RVQ 레벨들(미세 음향 토큰들)을 예측한다.\n' +
      '\n' +
      'RL finetuning을 위해, 우리는 음향 품질, 텍스트 준수 및 생성된 음악의 전반적인 매력에 가장 중요한 기여자인 의미론적 및 거친 음향 모델링 단계를 최적화하는 것을 선택한다. 우리는 프레임 인터리빙된 시맨틱 및 음향 토큰에서 작동하는 단일 자기회귀 스테이지를 사용하여 시맨틱 및 거친 음향 모델링을 공동으로 최적화하는 문제를 해결한다. RL 설정 및 문제 공식화를 단순화하면서, 이 접근법은 모델링된 토큰 시퀀스 길이를 증가시킨다. 이 문제를 계층적 변압기로 해결한다. Lee et al.(2022); Yang et al.(2023); Yu et al.(2023)과 유사하게. 마지막으로, MusicLM의 원래 자기회귀 미세 음향 모델링 단계 대신에, 효율적인 병렬 생성을 달성하기 위해 사운드스톰(Borsos et al., 2023b)을 사용한다.\n' +
      '\n' +
      '단순화를 위해 본 연구에서는 MusicLM(MusicLM)을 참조하여 RL로 미세 조정이 가능한 텍스트 조건 모델링 단계인 인터리브 시맨틱 및 거친 음향 토큰의 자기회귀 모델링 단계만을 언급한다.\n' +
      '\n' +
      '### RL finetuning 절차\n' +
      '\n' +
      '우리는 이전 작업에서 수행된 대로 대규모 언어 모델을 미세 조정하는 맥락에서 RL의 표준 공식을 사용한다(Ziegler et al., 2019). 도 2는 RL 트레이닝 루프를 예시한다. 에이전트는 정책을 매개변수화하는 가중치\\(\\theta\\)와 함께 정책\\(\\pi_{\\theta}\\)에 따라 동작한다. 정책은 입력\\(a_{0},\\ldots,a_{t-1}\\)을 자기회귀모델로 하고, 이전에 생성된 토큰들의 시퀀스를 다음 액션, 즉 다음 선택 토큰에 대한 확률분포를 출력한다. \\(a_{t}\\sim\\pi_{\\theta}(.|a_{0}\\ldots a_{t-1})\\. RL 미세화 단계는 주어진 보상함수로 \\(r\\)\\(\\mathbb{E}_{\\pi_{\\theta}[\\sum_{t}r(a_{0}\\ldots a_{t})]\\을 최대화하는 것을 목표로 한다. 우리는 정책 가중치를 업데이트하기 위해 REINFORCE 알고리즘의 KL 정규화된 버전을 사용한다(Jaques et al., 2017; Williams, 1992). 궤적\\((a_{t})_{t=0}^{T}\\)과 표현\\(s_{t}=(a_{0}...a_{t-1})\\)이 주어지면, 최대화를 위한 해당 정책 기울기 목표는\n' +
      '\n' +
      '\\theta)=(1-\\alpha)[\\sum_{t=0}^{T}\\log\\pi_{\\theta}(a_{t}|s_{t})(\\sum_{t=t}^{T}r(s_{i})-V_{\\phi}(s_{t}))]\\\\[-\\alpha\\sum_{t=0}^{T}\\sum_{a\\in A}[\\log(\\pi_{\\theta}(a|s_{t})/\\pi_{\\theta_{0}}(a|s_{t}))],\\\n' +
      '\n' +
      '여기서 코드북에 해당하는 동작공간은 \\(A\\)이고, KL 정규화 강도는 \\(α\\)이고, 기준선은 \\(V_{\\phi}\\)이다. 기준 값 함수 \\(V_{\\phi}\\)는 정책 기울기 목표(Sutton and Barto, 2018)의 분산을 줄이기 위해 사용되며, 현재 정책의 평균 수익률을 추정하도록 훈련된다. 베이스라인은 다음과 같이 학습된다:\n' +
      '\n' +
      '\\[\\min_{\\phi}\\mathbb{E}_{\\pi_{\\theta}}\\sum_{t}(\\sum_{k=t}^{T}r(s_{k})-V_{\\phi}(s _{t}))^{2}.\\]\n' +
      '\n' +
      '정책 및 값 함수는 모두 가중치\\(\\theta_{0}\\)를 갖는 초기 MusicLM 체크포인트로부터 초기화된다.\n' +
      '\n' +
      '### Reward Signals\n' +
      '\n' +
      '**Text adherence.** 사전 훈련된 MuLan(Huang et al., 2022) 임베딩으로부터 Text adherence에 대한 보상 모델을 도출한다. MuLan은 음악 클립과 약하게 연관된 자유 형식의 텍스트 주석에 대해 훈련된 대조적인 오디오 텍스트 임베딩 모델이다. 입력 프롬프트의 텍스트 임베딩과 생성된 음악의 오디오 임베딩 사이의 코사인 유사도를 계산하여 \\([-1;1]\\)의 보상값을 얻는다. 우리는 이 메트릭을 멀란 점수라고 부른다. 모델은 30초 오디오를 생성하는 반면, MulLan은 10초 오디오 클립으로 훈련되기 때문에 각 오디오를 3개의 세그먼트로 나누고 각 세그먼트에 대한 MulLan 점수를 계산하고 결과를 평균화한다.\n' +
      '\n' +
      '**음향 품질.** 음악 생성의 또 다른 주요 속성은 음향 품질, 예를 들어 클립이 전문적인 녹음처럼 들리는지 또는 인공물에 오염된지 여부이다. 우리는 20초 음악 클립의 인간 평균 의견 점수(MOS - 1에서 5 사이)를 예측하도록 훈련된 참조 없는 품질 추정기에 의존한다. 우리는 인간이 만든 뮤직클립과 뮤직LM이 만든 뮤직클립을 혼합하여 모델을 훈련하는데, 여기서 각 클립은 3명의 평가자가 평가했다. 평가자들은 음악성과 같은 교란 요인을 피하기 위해 음향 품질만 판단하는 임무를 받았다. 우리는 이 메트릭을 품질 점수로 언급합니다. 모델은 30초 클립을 생성하기 때문에 처음 20초와 마지막 20초에 품질 점수를 계산하고 두 점수를 평균화한다.\n' +
      '\n' +
      '**사용자 선호도.** AITK 웹 기반 인터페이스 4를 통해 사전 훈련된 텍스트 대 음악 MusicLM 모델을 대규모 사용자 기반으로 배포한다. 우리는 쌍대 비교를 통해 피드백을 수집하기로 선택한다(Christiano et al., 2017): 사용자가 프롬프트를 포착할 때, 우리는 2개의 20대 후보 클립을 생성하고 사용자가 선택적으로 그들 중 하나에 트로피를 할당하게 한다. 이 과정이 암시하는 중요한 디자인 선택은 특정 지시가 없다는 것인데, 이는 사용자가 정확한 음악적 속성에 편향되지 않고 오히려 전반적인 주관적 취향을 전달하기 위한 것이다. 우리는 두 세대를 모두 듣는 사용자의 선호도만 고려합니다. 필터링 후, 300,000 크기의 쌍별 사용자 데이터의 데이터 세트를 획득하는데, 이 데이터 세트는 (부록 C에 상세히 설명된 바와 같이) 인간 평가자로부터 종종 발생하는 바이어스를 최소화한다.\n' +
      '\n' +
      '각주 4: [https://aitestkitchen.withgoogle.com/](https://aitestkitchen.withgoogle.com/)\n' +
      '\n' +
      '보상 모델은 캡션의 텍스트와 해당 오디오 토큰을 입력으로 하고 스칼라 점수를 출력한다. 이 모델은 다음과 같이 브래들리-테리 모델(브래들리 및 테리, 1952)로 훈련된다.\n' +
      '\n' +
      '그림 3: AI Test Kitchen MusicLM 인터페이스. 사용자는 프롬프트를 작성하거나 제안 중에서 선택할 수 있다. 각각의 프롬프트는 2개의 20대 클립을 생성하고, 사용자는 2개 중 자신이 좋아하는 클립에 트로피를 라벨링할 수 있다.\n' +
      '\n' +
      'Christiano et al.(2017)은 쌍별 선호도로부터 점별 ELO 점수를 학습할 수 있도록 한다. 그것은 MusicLM 체크포인트로 초기화되는데, 첫 번째 결과는 처음부터 보상 모델이 인간의 선호도를 예측하는 데 우연보다 더 잘 할 수 없다는 것을 보여주었기 때문이다. 사용자 선호도 데이터셋을 285,000 크기의 열차 분할과 15,000 크기의 평가 분할로 나누고, 32쌍의 배치에서 10,000 단계를 훈련한 후, 보상 모델은 평가 세트에서 60%의 정확도를 달성한다(도 6 참조).\n' +
      '\n' +
      '보상 모델의 성능을 사전 평가하기 위해 사용자 선호도 데이터 세트에서 156개의 오디오 비교에 대한 내부 소규모 인간 평가를 수행한다. 60%의 경우, 우리 팀의 선호도는 데이터 세트의 설정된 선호도와 일치합니다. 이 결과는 보상 모형의 성과와 견줄 만하다. 또한, 이러한 낮은 일치율은 Stiennon et al.(2020)이 OpenAI 인간 선호도 데이터 세트에 대한 일치율을 73-77%로 추정한 요약과 같은 도메인에 비해 음악 선호도를 판단하는 데 내재된 주관성을 강조한다. 사용자 선호도 보상 모델에서 MusicLM을 미세 조정할 때, 우리의 모델은 30초의 오디오를 생성하기 때문에, 우리는 오디오의 처음과 마지막 20초로부터 계산된 스코어들의 평균을 구한다.\n' +
      '\n' +
      '##4 실험 설정\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '섹션 3.3에 설명된 대로 사전 훈련된 보상 신호가 주어지면 RL 미세 조정 단계는 모든 MusicLM 기반 모델을 프롬프트하는 데 사용되는 캡션으로만 구성된 데이터 세트를 사용한다. 결과적으로, 어떠한 지상-진실 오디오도 미세조정 프로세스에 관여하지 않는다. 우리는 3개의 소스로부터 캡션을 합성적으로 생성하기 위해 Huang et al.(2023)과 동일한 절차를 따른다. 우리는 LaMDA 모델(Thoppilan et al., 2022)을 사용하여 150,000개의 인기 노래에 대한 설명을 생성한다. 노래 제목과 아티스트를 제공한 후 LaMDA의 응답은 음악에 대한 400만 개의 설명 문장으로 처리된다. 우리는 MusicCaps(Agostinelli et al., 2023)로부터 10,028개의 캡션을 음악을 기술하는 35,333개의 단일 문장으로 분할한다. 또한, MusicCaps에서 23,906개의 짧은 형식의 음악 태그를 수집한다. 또한, 3.3절에 기술된 바와 같이 사용자로부터 수집된 300,000개의 프롬프트로 이전 캡션을 확장하고, 90%의 훈련과 10%의 평가를 사용하여 데이터를 무작위로 분할한다.\n' +
      '\n' +
      '### Training procedure\n' +
      '\n' +
      '다음 실험에서 우리는 동일한 RL 알고리즘과 동일한 하이퍼파라미터를 가진 MusicLM 모델을 RL-finetune한다. 공통 디코딩 방식은 온도\\(T=0.99\\)을 갖는 온도 샘플링이다. 온도는 MusicLM 세대에 대해 좋은 품질-다양성 균형을 갖기 위해 주관적인 검사와 함께 선택되었다. RL-finetuned 모델은 훈련 과정에서 사용되는 보상 함수만 다르다.\n' +
      '\n' +
      '음악 R-R MuLan 보상, 품질 보상, 그리고 MuLan 보상과 품질 보상의 선형 조합으로 구성된 20,000개의 훈련 단계를 위한 RL-finetune MusicLM의 결과 모델은 각각 MusicRL-MuLan, MusicRL-Quality, MusicRL-R로 불린다. 실험 결과, 두 보상 모두 동일한 규모일 때 MuLan과 품질 보상의 조합이 가장 좋은 결과를 나타냄을 알 수 있었다. 우리는 여전히 정상화되지 않은 점수를 수치로 표시한다.\n' +
      '\n' +
      '뮤직RL-U 우리는 뮤직RL-U라고 부르는 모델을 얻기 위해 사용자 선호도 보상 모델을 사용하여 5000개의 훈련 단계를 위한 RL-finetune MusicLM이다.\n' +
      '\n' +
      '뮤직RL-RU 모든 보상 신호를 결합하기 위해 사용자 선호도 보상 모델에 대한 1000개의 훈련 단계를 위한 RL-finetune MusicRL-R을 사용한다. 이 실험을 위해, KL 정규화는 미세 조정되는 모델과 MusicRL-R 사이에서 계산된다. 결과 모델을 MusicRL-RU라고 합니다. MulLan에 대한 첫 번째 피니튜닝과 품질에 대한 순차적 접근과 사용자 선호 보상에 대한 피니튜닝이 세 가지 보상에서 동시에 학습보다 우수하다는 것을 알 수 있다. 이는 사용자 선호 보상에 최적화하기 전에 적은 수의 그래디언트 단계(2000 미만)를 거치는 반면 다른 보상을 최적화하기 위해서는 약 10,000 단계가 걸린다는 사실에서 비롯된다고 가정한다. 더욱이, 이 문제에서 최종 단계에서 사용자 선호도 보상 모델을 사용하면 모델이 인간 선호도에 더 잘 정렬되도록 할 수 있다.\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '실험에서 보고하는 주요 지표는 품질 보상, MuLan 보상 및 사용자 선호도 보상 모델이다. 우리는 훈련에 따른 진행을 보여주기 위한 훈련 단계에 대해 또는 기본 모델에 대한 KL 발산에 대해 메트릭을 보고한다. 이는 통상적으로 베이스 체크포인트까지의 거리 및 따라서 모델의 원래 능력들의 보유를 측정하기 위한 프록시로서 사용된다(Christiano et al., 2017; Roit et al., 2023).\n' +
      '\n' +
      '정성적 평가를 위해 음악 장르의 균형 있는 범위를 나타내는 101개의 다양한 내부 수집 프롬프트를 사용한다(전체 목록은 부록 A를 참조). 이러한 프롬프트를 사용하여 평가된 각 모델에서 오디오 샘플을 생성한다. 다양한 음악 스타일(>6년)을 청취한 경험과 서면 영어 유창함으로 평가자를 선정합니다. 정성적 평가 동안, 평가자들은 동일한 텍스트 프롬프트를 사용하여 상이한 모델들에 의해 생성된 두 개의 오디오 클립들로 제시된다. 평가자들은 텍스트 프롬프트에 대한 준수, 음향 품질 및 오디오 클립에 대한 전반적인 매력을 고려하여 각 클립을 1~5의 척도로 평가할 것을 요청한다. 각 비교는 3개의 다른 평가자에 의해 수행되며, 모델 비교당 총 303개의 평점이 있다. 이러한 등급으로부터 \\(win/(win+loss)\\)으로 정의되는 승률 메트릭을 계산한다.\n' +
      '\n' +
      '### Checkpoint selection\n' +
      '\n' +
      '모든 RL-finetuned 모델에 대해 정량적 결과를 검사하고 음악 세대를 청취하여 최상의 체크포인트를 선택한다. MusicRL-R, MusicRL-U 및 MusicRL-RU의 경우 각각 10,000개의 훈련 단계, 2000개의 훈련 단계 및 1000개의 훈련 단계 후에 체크포인트를 선택한다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '우리는 다음과 같은 질문에 답하는 것을 목표로 한다. (1) MuLan에 대한 RL-finetuning과 품질 보상은 MusicLM과 같은 텍스트-음악 모델의 생성 품질을 향상시킬 수 있는가? (2) RLHF는 생성된 음악의 사용자로부터의 일반 선호도에 대한 정렬을 개선할 수 있는가? (3) 모든 보상 신호를 조합하여 성능을 더욱 향상시킬 수 있는가?\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '모든 정량적 평가에서 초기 모델로부터 KL 발산에 대한 보상 점수를 추적하여 RL 미세 조정 동안 모델 진행을 분석한다. 뮤직RL-R에서와 같이 단일 보상 모델 또는 둘 모두의 조합으로 훈련하는지 여부에 관계없이 모든 보상 신호에 대한 모델 성능을 평가한다.\n' +
      '\n' +
      '그림 4는 RL-피네튜닝이 품질과 MuLan 점수 모두를 성공적으로 최적화하는 것을 보여준다. 구체적으로, 품질 보상에 대한 미세 조정만으로는 품질 점수의 가장 큰 증가(3.5 MOS에서 4.6 MOS로), MuLan 점수의 더 작은 증가(0.58에서 0.61로)로 이어진다. 반대로, MuLan 보상에만 대한 미세 조정은 MuLan 점수(0.58에서 0.71)를 최대화하고 덜 뚜렷하다.\n' +
      '\n' +
      '품질 점수 향상(3.5 MOS에서 4.1 MOS로) 품질과 MuLan 보상을 모두 활용하면 두 점수(품질: 3.5 MOS ~ 4.4 MOS; MuLan: 0.58 ~ 0.71)가 크게 향상되지만 KL 발산은 약간 증가한다. MuLan과 품질 점수를 동시에 최적화하는 데 있어 유망하고 안정적인 성능을 감안할 때 MusicRL-R에서만 정성적 평가를 수행한다.\n' +
      '\n' +
      '도 8(부록 B에서)은 품질 보상에 대한 10,000개의 미세조정 단계 후에, 사용자 선호도에 트레이닝된 보상 모델이 음악 샘플들에 더 낮은 스코어들을 할당하기 시작함을 보여준다. 이는 품질 보상에만 초점을 맞추는 것이 보상 과잉 최적화(Coste et al., 2023; Jiang et al., 2020; Rame et al., 2024)에 취약하다는 것을 시사한다.\n' +
      '\n' +
      '그림 4: RL-finetuned 모델에 대한 품질(왼쪽) 또는 MuLan 점수(오른쪽) 대 KL 발산. KL 발산은 KL 발산이 MusicRL-R에 대해 계산되는 MusicRL-RU를 제외하고 RL-finetuned 모델과 MusicLM 사이에서 계산된다. 검정색 십자가는 MusicRL-RU의 트레이닝을 시작하기 위해 사용되는 체크포인트에 대응한다. RL-피네튜닝은 품질과 MuLan 점수(MusicRL-R)를 성공적으로 최적화한다. 또한, 사용자 선호도 보상(MusicRL-RU, MusicRL-RU)을 최적화하는 것은 MuLan 점수를 약간 감소시키면서 품질 점수를 향상시킨다.\n' +
      '\n' +
      '그림 5: 서로 다른 RL-finetuned 모델에 대한 사용자 선호 보상 모델 점수. KL 발산은 KL 발산이 MusicRL-R에 대해 계산되는 MusicRL-RU를 제외하고 RL-finetuned 모델과 MusicLM 사이에서 계산된다. 검정색 십자가는 MusicRL-RU의 트레이닝을 시작하기 위해 사용되는 체크포인트에 대응한다. RL-피네튜닝은 세대들의 사용자 선호도 보상 모델 점수를 성공적으로 개선한다(MusicRL-U 및 MusicRL-RU 곡선 참조). 다른 보상(MuLan 및/또는 품질)에 대해 트레이닝될 때, 사용자 선호도 보상 모델 점수는 약간 개선된다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '##6 보상 모델의 렌즈를 통한 인간 피드백 이해\n' +
      '\n' +
      '이 섹션에서는 사용자 선호도에 영향을 미치는 특정 음악 요소를 밝히기 위해 보상 모델 정확도를 분석한다. 이 분석은 우리의 연구 문제를 직접적으로 다룬다: 사용자가 오디오를 평가할 때 주목하는 것은 무엇인가?\n' +
      '\n' +
      '우리는 생성된 음악을 (1) 텍스트 준수, (2) 오디오 품질 및 (3) 음악성의 세 가지 구성 요소로 분류하여 사용자의 오디오 선호도에 대한 선택을 유도할 수 있다. 특히 음악성을 정의하고 모델링하는 것은 복잡한 작업으로, 인간의 피드백에 대한 우리의 초점을 해결책으로 강조하며 규칙 기반 한계를 넘어 나아가고 있다.\n' +
      '\n' +
      '### 텍스트 입력의 중요도\n' +
      '\n' +
      '텍스트가 쌍별 선호도 평가에 미치는 영향을 분리하기 위해 보상 모델을 훈련하는 동안 텍스트 토큰을 삭제한다. 정확도는 그림 6과 같이 안정적으로 유지되며, 또한 MuLan 점수가 가장 높은 뮤직 클립이 선호하는 뮤직 클립에 얼마나 자주 해당하는지를 측정한다. 평가 세트에서 이러한 지표는 시간의 51.6%에 불과하여 무작위 정확도에 매우 가깝다. 전반적으로, 이러한 발견은 텍스트 프롬프트에 대한 순응이 우리 실험에서 인간 선호도의 주요 동인이 아님을 나타낸다. 이것은 MusicRL-U를 훈련할 때 MuLan에 의해 측정된 텍스트 준수의 유의미한 개선을 보이지 않는 섹션 5.1의 정량적 결과와 일치한다.\n' +
      '\n' +
      '###음질의 중요도\n' +
      '\n' +
      '오디오 품질은 생성된 시퀀스 내에서 비교적 일관되게 유지되기 때문에, 몇 초의 오디오는 이러한 측면을 평가하기에 충분한 정보를 제공해야 한다. 우리는 10초, 5초, 3초에 해당하는 서로 다른 입력 오디오 토큰 길이에 대한 보상 모델을 훈련한다. 그림 6에서 볼 수 있듯이 쌍별 선호도에 대한 평가 정확도는 입력 토큰의 길이를 줄임으로써 3-5초의 입력 오디오를 사용할 때 60%에서 56%로 감소한다. 유의한 정확도 감소는 다른 음악적 구성 요소가 사용자 선호도에 보완적인 역할을 한다는 것을 시사한다. 또한, 우리는 복제\n' +
      '\n' +
      '도 6: 사용자 선호도 보상 모델에 대한 블리테이션들. 보상 모델은 텍스트 토큰 없음(Text 없음) 또는 입력 오디오의 크롭된 버전(즉, 10, 5, 3)으로 학습된다. 텍스트 토큰들을 드롭하는 것은 보상 모델의 정확도에 크게 영향을 미치지 않지만, 오디오를 크롭하는 것은 실질적으로 성능을 저하시킨다. 이는 텍스트 준수 기반(MuLan) 또는 사용자 선호도에 대한 오디오 품질 기반 예측 변수를 사용할 때 낮은 정확도로 추가로 보여지는 바와 같이 텍스트 준수 및 오디오 품질이 사용자 오디오 선호도에 영향을 미치는 주요 요인이 아님을 시사한다.\n' +
      '\n' +
      '분석은 6.1에서 수행되었으며 가장 높은 품질 점수를 가진 뮤직 클립이 얼마나 자주 선호되는지 측정한다. 그림 6과 같이 품질 예측기는 평가 데이터 세트에서 53.3%의 정확도를 달성한다. 이러한 발견은 오디오 품질이 텍스트 준수보다 더 나은 신호이면서 인간 선호의 유일한 동인이 아님을 나타낸다. 이것은 MusicRL-U 훈련이 품질 점수에서 약간 개선되는 섹션 5.1의 정량적 결과와 일치한다. 전반적으로, 이 분석은 사용자 선호도가 텍스트 준수 및 오디오 품질을 넘어서는 음악 요소에 의해 영향을 받는다는 것을 보여준다.\n' +
      '\n' +
      '##7 한계와 향후 작업\n' +
      '\n' +
      '**피드백 정렬 및 평가.** 사용자 선호도 데이터에 대한 트레이닝 시, 우리의 현재 설정의 한계는 모델을 개선하기 위한 피드백을 제공하는 사람들(일반 사용자들)과 결과를 평가하는 사람들(선택된 평가자들) 사이의 _population gap_이다. 향후 작업의 방향은 사용자의 관점에서 인지된 개선 사항을 직접 측정하는 것이다.\n' +
      '\n' +
      '** on-policy data.** 섹션 3.1에 설명된 이유로 이 작업에서 RL 미세 조정에 사용된 것과 비교하여 다른 버전의 MusicLM에서 사용자 선호도를 수집했다. 개선할 수 있는 명확한 경로는 온 정책 데이터(미세 조정 중인 모델에 의해 생성된 데이터)를 반복적으로 수집하여 모델을 업데이트하는 데 사용하는 것이다. 결국, 이는 사용자 경험을 향상시키면서 새로운 피드백을 수집하기 위해 미세화된 모델들이 연속적으로 배치되는 실제 통합 피드백을 허용할 것이다.\n' +
      '\n' +
      '**사용자 선호도 데이터세트를 정제하는 것.** 몇 가지 흥미로운 연구 방향은 큰 사용자 상호작용 데이터세트를 정제하는 것을 포함한다. 예를 들어, 사용자가 자신 있고 명확한 선호도를 표현하는 예를 식별하고 유지하는 것은 노이즈를 줄이고 전체 데이터 세트 품질을 향상시킬 수 있다. 또한, 보다 작지만 관련성이 높은 데이터 세트에서 강력한 보상 모델을 훈련하는 기술에 초점을 맞추면 특정 사용자를 위한 모델 개인화 등의 연구 방향을 용이하게 할 수 있다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '본 연구에서는 인간의 기호에 부합하는 최초의 텍스트-음악 생성 모델인 MusicRL을 소개한다. 첫 번째 실험에서, 우리는 음향 품질뿐만 아니라 캡션의 준수를 알려주는 시퀀스 레벨 보상 함수를 유도한다. 사전 훈련된 MusicLM 모델을 미세 조정하여 RL로 이러한 보상을 최적화할 때 정량적 및 질적 결과는 사전 훈련된 기준선에 대해 일관된 개선을 보여준다. 그런 다음 사용자의 일반적인 선호도에 따라 음악 생성을 정렬할 수 있음을 처음으로 보여줍니다. 이를 위해 웹 인터페이스를 통해 300,000개의 사용자 생성 캡션 및 오디오 쌍을 쌍별 선호도와 함께 수집한다. 우리는 이 대규모 피드백을 활용하여 보상 모델을 훈련하고 RLHF를 통해 모델을 개선하여 다시 일관되게 기준선을 능가한다. 마지막으로 모든 보상 신호를 결합하여 가장 성능이 높은 모델을 생성합니다. 추가 분석은 사용자 선호도에서 추출된 신호가 텍스트 준수 및 오디오 품질을 넘어서는 정보를 포함한다는 것을 나타낸다. 이는 음악 생성 모델을 개선할 때 사용자 피드백을 통합하는 가치를 강조하면서 음악적 매력의 주관적이고 복합적인 성격을 부각시킨다.\n' +
      '\n' +
      '구글의 AI 테스트 키친 팀이 뮤직LM 경험을 스케일로 사용자에게 설계하고 배포하는 데 기여한 것에 대해 감사드립니다: 사이 키란 고시, 크리스틴 임, 필립 메이어, 세잘 카트리, 셜리 렝, 이룬 류, 이야오 및 엘리아스 로만. 우리는 뮤직엘엠의 다른 멤버들과 기고자들에게 감사한다: 티모 덴크, 마우리시오 줄루아가, 마르코 타글리아사치, 맷 샤리피, 마이클 둘리, 크리스천 프랭크와 헤마 마니카바사감. 우리는 또한 이전 논문 초안에 대한 귀중한 피드백에 대해 로버트 다다시, 난도 드 프라이타스, 더그 에크에게 감사드린다. 마지막으로 본 논문에서 사용된 RL 훈련 인프라를 설계하고 구축한 개인에게 감사드린다: 요한 페렛, 니노 비야르드, 알렉시스 자크, 사벨라 라모스, 피오트르 스탠치크, 다닐라 시노팔니코프, 아멜리 헬리오우, 니콜라 맘체프. 우리는 또한 MusicLM의 모든 사용자들이 그들의 소중한 피드백과 MusicRL을 만드는데 기여한 것에 감사한다.\n' +
      '\n' +
      '** 기여도.** 제프리 시데론과 안드레아 아고스티넬리(프로젝트의 주요 조사자), 닐 제기두르(초기 설계, 감독, 품질 보상 모델링, 종이 작성에 대한 핵심 기여), 세르탄 기진(핵심 인프라), 마우로 베르제티(인프라, 핵심 품질 보상 모델링), 레오나르 후세노(감독, 핵심 종이 작문), 빅토르 웅구레아누(핵심 품질 보상 모델링), 마테지 카스텔릭(사용자 데이터 인프라), 잘란 보로스, 브라이언 맥윌리엄스와 데미안 빈센트(코어 뮤직LM 모델링), 올리비에 바켐과 올리비에 피에틴(감독, 초기 설계), 마티에 가이스트(감독) 등이 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al.(2023) J. Achiam, S. S. 애들러 L. 애거월 Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. 알트만 Anadkat, et al. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Agostinelli et al. (2023) A. Agostinelli, T. I. Denk, Z. 보로스, J.엥겔 버제티, A. 케이용, Q. 황 A. 잰슨 A. 로버츠 M. 타글리아사치 N. 샤리피 제기두르와 C. 프랭크 Musiclm: 텍스트로부터 음악을 생성하는 단계, 2023.\n' +
      '*Bai et al.(2022) Y. 배아존스 Ndousse, A. Askell, A. Chen, N. D. D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. training helpful and harmless assistant with reinforcement learning from human feedback. _ ArXiv:2204.05862_, 2022.\n' +
      '* Boros et al.(2023a) Z. 보로스, R 마리니에, D. 빈센트, E. 하리토노프, O. 피에틴 샤리피, D. 로블렉, O. 테불, D. 그랑지에, M. Tagliasacchi, et al. Audiolm: language modeling approach to audio generation. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2023a.\n' +
      '* Boros et al.(2023b) Z. 보로스 샤리피, D. 빈센트, E. 하리토노프, N. 제기두르와 M. 타글리아삭키 Soundstorm: 효율적인 병렬 오디오 생성, 2023b.\n' +
      '* Bradley and Terry (1952) R. A. Bradley and M. E. Terry. 불완전 블록 설계의 순위 분석: I. 쌍을 이루는 비교 방법. _ Biometrika_, 39, 1952\n' +
      '* Christiano et al.(2017) P. F. Christiano, J. Leike, T. 브라운 마틱 레그랑 아모디 인간의 선호로부터 심층 강화 학습. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Chung et al.(2021) Y. 정용 장원 한창주 팡과 Y 우 W2v-bert: 자기 지도 음성 사전 훈련을 위한 대비 학습과 마스킹 언어 모델링을 결합한다. _ arXiv:2108.06209_, 2021.\n' +
      '* Copet et al. (2023) J. Copet, F. Kreuk, I. Gat, T. 레메즈, D. 칸트, G. 신내브, Y. 아디와 A. 데포세즈 간단하고 제어 가능한 음악 생성, 2023년.\n' +
      '* Coste et al.(2023) T. 코스트 안와르 커크, 그리고 D. 크루거 보상 모델 앙상블은 과잉 최적화를 완화하는 데 도움이 됩니다. _ arXiv preprint_, 2023.\n' +
      '* Defossez et al. (2021) A. Defossez, N. 제기두르 유수니에 L. 보투와 F. R. 바흐 SING: 심벌-대-기기 신경 발생기. 인석 Bengio, H. M. Wallach, H. Larochelle, K 그라우만 세사-비안치, R. Garnett, Editors, _Advances in Neural Information Processing Systems 31: Annual Conferenceon Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 9055-9065, 2018. URL[https://proceedings.neurips.cc/paper/2018/hash/56dc0997d871e9177069bb472574eb29-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/56dc0997d871e9177069bb472574eb29-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/56dc0997d871e9177069eb29-Abstract.html)\n' +
      '* Devlin et al. (2019) J. Devlin, M. - W. 장경 이경호 투타노바 BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. J. Burstein, C. Doran, T. 솔로리오, 편집자, _Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Computational Linguistics Association. doi: 10.18653/v1/N19-1423. URL[https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).\n' +
      '* Dhariwal et al. (2020) P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. 음악을 위한 생성 모델 arXiv:2005.00341_, 2020.\n' +
      '* Defossez et al. (2022) A. Defossez, J. Copet, G. Synnaeve, and Y. 아디 고충실도 신경 오디오 압축 2210.13438_, 2022년\n' +
      '* Elizalde et al. (2022) B. Elizalde, S. Deshmukh, M. A. Ismail, H. Wang. 박수: 2022년 자연어 감독에서 오디오 개념을 학습합니다.\n' +
      '* Engel et al. (2017) J. H. Engel, C. Resnick, A. Roberts, S. 다일만 노루지, D. 엑, K. 사이먼 웨이베넷 오토인코더를 사용한 음표의 신경 오디오 합성 D. Precup and Y. W. Teh, Editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1068-1077. PMLR, 2017. URL[http://proceedings.mlr.press/v70/engel17a.html](http://proceedings.mlr.press/v70/engel17a.html).\n' +
      '* Engel 등(2020) J. H. Engel, L. 한트라쿨, C구, A 로버츠 DDSP: 미분 가능한 디지털 신호 처리. In _International Conference on Learning Representations (ICLR)_, 2020.\n' +
      '* 실시간 음악 생성을 위한 안정적인 확산, 2022. URL[https://riffusion.com/about](https://riffusion.com/about)\n' +
      '* Garcia et al. (2023) H. F. Garcia, P. Seetharaman, R. 쿠마르와 B 파르도 Vampnet: 마스킹된 음향 토큰 모델링을 통한 음악 생성, 2023.\n' +
      '* Gauldin(1988) R. 골딘 18세기 대위법에 대한 실용적인 접근법. 1988년 프렌티스 홀\n' +
      '* 쌍둥이자리 팀 (2023) G. 쌍둥이자리 팀. 쌍둥이자리: 매우 유능한 멀티모달 모델들의 가족. 2023년\n' +
      '* Guimaraes et al. (2017) G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. 시퀀스 생성 모델을 위한 객체 강화 생성적 적대 네트워크(organ. _ arXiv preprint arXiv:1705.10843_, 2017.\n' +
      '* Hawthorne et al. (2022) C. Hawthorne, A. Jaegle, C. Cangea, S. 보르헤오, C. 내쉬, M. 말리노스키 오델만 Vinyals, M. M. Botvinick, I. Simon, H. Sheahan, N. J. Alayrac, J. Carreira, J. H. Engel. 수신자 AR을 사용한 범용, 긴 컨텍스트 자기회귀 모델링. 인규 차우두리 제겔카 L. 송세페스바리, G. 니우, S. Sabato, editorators, _International Conference on Machine Learning(ICML)_, 2022.\n' +
      '* Huang et al.(2022) Q. 황아잔센 간티, J. Y. 리, 그리고 D. P. W. 엘리스. Mulan: 음악 오디오와 자연어의 공동 임베딩. _International Society for Music Information Retrieval Conference (ISMIR)_, 2022.\n' +
      '\n' +
      '* Huang et al.(2023) Q. 황동석 왕태일 덴크 천진 장장 Zhang, J. Yu, C. Frank, et al. Noise2music: Text-conditioned music generation with diffusion models. _ arXiv preprint arXiv:2302.03917_, 2023.\n' +
      '* Jaques et al.(2017) N. 재퀸스 구, D. 바다나우, J. M. 에르난데스-로바토, R. E. 터너, D. 엑. 서열 튜터: kl-컨트롤을 사용하여 서열 생성 모델의 보존적 미세 조정. _International Conference on Machine Learning_, pages 1645-1654. PMLR, 2017.\n' +
      '* Jaques et al.(2019) N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. 존스 구, R 피카드 대화 상자에서 암시적 인간 선호도에 대한 잘못된 정책 배치 심층 강화 학습 ArXiv:1907.00456_, 2019.\n' +
      '* Jiang et al.(2020) N. 장승 진진 두안, 장씨 Rl-duet: 심층 강화 학습을 이용한 온라인 음악 반주 생성. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 710-718, 2020.\n' +
      '* Karbasi et al. (2021) S. M. Karbasi, H. S. Haug, M. - K. Kvalsund, M. J. Krzyzaniak 및 J. Torresen. 심층 강화 학습으로 음악적 리듬을 만드는 생성 모델입니다. 2021년 제2회 AI 음악 창의성 컨퍼런스.\n' +
      '* Kharitonov et al. (2023) E. Kharitonov, D. Vincent, Z. 보르소 매리니에, S 처진오 피에틴 샤리피 Tagliasacchi, N. 제기두르 말하기, 읽기 및 프롬프트: 최소한의 감독으로 고충실도 텍스트 투 스피치 _ Transactions of the Association for Computational Linguistics_, 11:1703-1718, 2023. URL[https://api.semanticscholar.org/CorpusID:256627687](https://api.semanticscholar.org/CorpusID:256627687).\n' +
      '* Kilgour et al.(2019) K. 길구르 줄루가, D. 로블렉, M. 샤리피 사전 오디오 거리: 음악 향상 알고리즘을 평가하기 위한 참조 없는 메트릭. 2019년 _INTERSPEECH_에서\n' +
      '* Kotecha(2018) N. 코테차 바흐2바흐: 심층 강화 학습 접근법을 사용하여 음악을 생성하는 단계. _ arXiv preprint arXiv:1812.01060_, 2018.\n' +
      '* Kreuk et al. (2022) F. Kreuk, G. Synnaeve, A. Polyak, U. 가수 A. Defossez, J. Copet, D. Parikh, Y. 타이그먼과 Y 아디 Audiogen: Textually guided audio generation, 2022.\n' +
      '* Latif et al. (2023) S. Latif, H. Cuayahuitl, F. Pervez, F. Shamshad, H. S. Ali, and E. Cambria. 오디오 기반 응용 프로그램의 심층 강화 학습에 대한 조사. _ Artificial Intelligence Review_, 56(3):2193-2240, 2023.\n' +
      '* Lee et al.(2022) D. Lee, C. Kim, S. 김민 조완 - S. 한 잔차 양자화를 이용한 자기회귀 영상 생성 방법. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11523-11532, 2022.\n' +
      '* Lee et al.(2023) K. 이현우 류오 왓킨스 Du, C. Boutilier, P. Abbeel, M. 가함자데, S. S. 구. 사람 피드백을 사용하여 텍스트-이미지 모델을 정렬합니다. _ arXiv preprint arXiv:2302.12192_, 2023.\n' +
      '* Lewkowicz (2001) D. J. Lewkowicz. 생태적 타당성의 개념: 그것의 한계는 무엇이며 무효가 되는 것은 나쁜가? _ Infancy_, 2(4):437-450, 2001.\n' +
      '* Liu et al.(2023) H. Liu, Z. 천영 위안, 엑스 메익 류동필만딕 왕과 M.D. 플럼블리 Audioldm: 잠재 확산 모델을 가진 텍스트 대 오디오 생성. A. 크라우스, E. 브런스킬, K. 조병하르트 Sabato, and J. Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 21450-21474. PMLR, 2023. URL[https://proceedings.mlr.press/v202/liu23f.html](https://proceedings.mlr.press/v202/liu23f.html).\n' +
      '\n' +
      'OpenAI(2023) OpenAI. Gpt-4 기술 보고서입니다 2023년\n' +
      '\n' +
      '* Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* Parker et al. [2024] J. D. Parker, J. Spijkervet, K. Kosta, F. Yesiler, B. Kuznetsov, J.-C. Wang, M. Avent, J. Chen, and D. Le. Stemgen: A music generation model that listens, 2024.\n' +
      '* Rafailov et al. [2023] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.\n' +
      '* Rame et al. [2024] A. Rame, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. Warm: On the benefits of weight averaged reward models, 2024.\n' +
      '* Ranzato et al. [2016] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. In Y. Bengio and Y. LeCun, editors, _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL [http://arxiv.org/abs/1511.06732](http://arxiv.org/abs/1511.06732).\n' +
      '*Rey and Neuhauser[2011] D. Rey and M. 뉴하우저 윌콕슨 서명 순위 테스트 인민 Lovric, editor, _International Encyclopedia of Statistical Science_, pages 1658-1659. Springer, 2011. Doi: 10.1007/978-3-642-04898-2_616. URL[https://doi.org/10.1007/978-3-642-04898-2_616](https://doi.org/10.1007/978-3-642-04898-2_616](https://doi.org/10.1007/978-3-642-04898-2_616)\n' +
      '* Roit et al. [2023] P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi, M. Geist, S. Girgin, L. Hussenot, O. Keller, N. Momchev, S. R. Garea, P. Stanczyk, N. Vieillard, O. Bachem, G. Elidan, A. Hassidim, O. Pietquin, and I. Szpektor. Factually consistent summarization via reinforcement learning with textual entailment feedback. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 6252-6272. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.344. URL [https://doi.org/10.18653/v1/2023.acl-long.344](https://doi.org/10.18653/v1/2023.acl-long.344).\n' +
      '* Schneider et al. [2023] F. Schneider, O. Kamal, Z. Jin, and B. Scholkopf. Mousai: Text-to-music generation with long-context latent diffusion, 2023.\n' +
      '* Stiennon et al. [2020] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.\n' +
      '* Sutton and Barto[2018] R. S. Sutton and A. G. Barto. _ 강화 학습: 도입_. 2018년 MIT 기자\n' +
      '* Team et al. [2023] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Tervaniemi[2023] M. 테르바니에미 음악의 신경과학은 생태학적 타당성에 달려있다. __ 2023년 뉴로사이언스 동향\n' +
      '* Thomas and Kellogg[1989] J. C. Thomas and W. A. Kellogg. 인터페이스 설계의 생태학적 격차를 최소화합니다. _ IEEE Software_, 6(1):78-86, 1989.\n' +
      '* Thoppilan et al. [2022] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.\n' +
      '\n' +
      '* Trehub et al.(2015) S. E. Trehub, J. Becker, and I. Morley. 음악과 음악성에 대한 상호문화적 관점 왕립 학회의 철학 거래 B: 생물 과학_, 370(1664):20140096, 2015.\n' +
      '* Vaswani et al. (2017) A. Vaswani, N. N. 쉐이저 파마르, J. 우즈코리트, L. 존스, A. N. 고메즈, L. 카이저, 나 폴로수킨 주목해 주세요 신경 정보 처리 시스템(NeurIPS)_, 2017에서의 발전.\n' +
      '* Wallace et al. (2023) B. Wallace, M. 당룡 라파일로프 주아루 푸루시워캄 Ermon, C. Xiong, S 조티와 N 나이크 직접 선호도 최적화를 사용한 확산 모델 정렬. _ arXiv preprint arXiv:2311.12908_, 2023.\n' +
      '* Wang et al. (2023) C. Wang, S. 천영 우진 장룡 저우성 류종 천영 류현왕 허승 자오와 F. 웨이 신경 코덱 언어 모델은 음성 합성기에 제로 샷 텍스트이다. _ CoRR_, abs/2301.02111, 2023. doi: 10.48550/ARXIV.2301.02111. URL[https://doi.org/10.48550/arXiv.2301.02111](https://doi.org/10.48550/arXiv.2301.02111).\n' +
      '* Williams (1992) R. J. Williams. 연결주의 강화 학습을 위한 간단한 통계적 기울기 추종 알고리즘. _ Machine learning_, 8(3-4):229-256, 1992.\n' +
      '* Wu and Hu(2018) Y. 우랑 B. 후 딥 강화 학습을 통해 일관된 요약을 추출하는 학습. S. A. McIlraith and K. Q. Weinberger, editoritors, _Proceedings of Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence(IAAI-18), and 8th AAAI Symposium on Educational Advances in Artificial Intelligence(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pages 5602-5609. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11987. URL[https://doi.org/10.1609/aaai.v32i1.11987](https://doi.org/10.1609/aaai.v32i1.11987)\n' +
      '* Wu et al.(2016) Y. 우민 슈스터, 지 진규배 노루지 매케리 권윤 조규 가오경 매케리, J. 클링너, A. 샤, M. 존슨, 엑스 류룡 카이저 우영 가토태 구도, H. 카자와, K. 스티븐스, G. 쿠리안, N. 파틸원 왕찬영 J. 스미스 J. 리사 A. 러드닉 O. 비닐스, G. 코라도, M. 휴즈, 그리고 J 딘 구글의 신경망 기계 번역 시스템: 인간과 기계 번역의 격차를 해소한다. _ CoRR_, abs/1609.08144, 2016. URL[http://arxiv.org/abs/1609.08144](http://arxiv.org/abs/1609.08144).\n' +
      '* Yang et al. (2022) D. Yang, J. Yu, H. Wang, W. 왕창웅 주, 유동 diffsound: text-to-sound 생성을 위한 이산 확산 모델. _ arXiv:2207.09983_, 2022.\n' +
      '* Yang et al. (2023) D. Yang, J. Tian, X. 황룡 황승 류진 장재시 조준환 우진 조승 와타나베와 멍 Uniaudio: A audio foundation model toward universal audio generation, 2023.\n' +
      '* Yu et al.(2023) L. 유대미그, C. 플래허티, A. 아하잔얀, L. 제틀모이어와 M 루이스 메가바이트: 멀티스케일 변압기로 백만 바이트의 시퀀스를 예측하는 것 _ arXiv preprint arXiv:2305.07185_, 2023.\n' +
      '* Zeghidour et al. (2022) N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, M. 타글리아삭키 사운드스트림: End-to-End 신경 오디오 코덱. _ IEEE ACM Trans. Audio Speech Lang. 처리._ 2022년, 30년\n' +
      '* Ziegler et al.(2019) D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. 인간의 기호에 따라 언어 모델을 미세 조정합니다. _ ArXiv preprint arXiv:1909.08593_, 2019.\n' +
      '\n' +
      '## 부록 정성적 평가.\n' +
      '\n' +
      '질적 평가를 위해 101개의 다양한 프롬프트 목록은 다음과 같다.\n' +
      '\n' +
      '[1em] 중간 템포 컨트리 코러스, \'북앤베이스 박자와 함께 그리프\', \'강한 베이스와 깊은 남성 목소리를 가진 으스스한 느린 메탈 기타 리프\', \'깊은 베이스와 랩하는 목소리를 가진 레게톤\', \'재즈 트리오가 연주하는 모던하고 낭만적인 느린 왈츠\', \'피아노와 현악기 반주가 연주되는 슬픈 팝 멜로디\', \'간단한 실로폰이 연주되는 빠른 속도의 디스코 연주\', \'감미로운 음악, 빠른 속도로 연주되는 클래식 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음절과 드럼을 연주하는 낭만적인 음악\', \'단음\n' +
      '\n' +
      '\'복음 합창단이 금속 기타 받침 위에서 노래한다\'\n' +
      '\n' +
      '강렬한 여성 목소리는 드럼 박자에 맞춰 영혼과 에너지를 담아 노래합니다\n' +
      '\n' +
      '\'캐릴론 뒤에 여자 목소리가 부르는 반복적인 자장가\'\n' +
      '\n' +
      '\'아코디언 백킹으로 남성 목소리가 연주하는 전통 속곡\'\n' +
      '\n' +
      '\'남자의 목소리가 깊고 피아노가 화음을 치는 업비트 레게\'\n' +
      '\n' +
      '시타와 현악이 뒷받침하는 느리고 선율적인 음악\n' +
      '\n' +
      '\'강하고 춤출 수 있는 박자와 두드러진 베이스라인, 키보드 멜로디를 가진 펑키한 작품\'\n' +
      '\n' +
      '\'50년대 춤출 수 있고 빠르고 경쾌한 스윙 곡조\'\n' +
      '\n' +
      '\'셀로에서 솔로 첼로를 위한 슬픈 멜로디를 연주하는 프로 솔로 첼로, 고품질 녹음\'\n' +
      '\n' +
      '록 기타 리프, 슬라이드 기타 솔로, 플루트 멜로디가 활기차고 경쾌한 사운드를 연출합니다.\n' +
      '\n' +
      '크리스마스 노래를 부르는 카펠라 합창단\n' +
      '\n' +
      '\'멋진 래그타임 기타 코드 진행\'\n' +
      '\n' +
      '경쾌한 R\'n\'B곡은 트럼펫 선율로 두 가수가 연주하고,\n' +
      '\n' +
      '\'샘플링된 음성에서 빠른 선율을 취한 댄스곡으로, 진통제 같은 느낌을 준다\'\n' +
      '\n' +
      '여자 리드 싱어가 나오는 복음가\n' +
      '\n' +
      '아코디언 밴드가 연주하는 향수적인 곡\n' +
      '\n' +
      '서사시적 반전과 교향악단의 후원이 있는 마라이치 노래\n' +
      '\n' +
      '\'진탕과 플루트를 곁들인 중부곡\'\n' +
      '\n' +
      '\'피아노와 트럼펫을 위한 재즈 작곡\'\n' +
      '\n' +
      '\'느린 블루스 인트로 하모니카와 최소한의 지원\'\n' +
      '\n' +
      '\'실험 모듈식 신디사이저는 물의 소리와 전자음악을 결합해 독특한 사운드스케이프를 만들었다\'\n' +
      '\n' +
      '기타가 달린 쾌활한 래그타임\n' +
      '\n' +
      '\'산업 테크노 사운드와 최면 리듬 반복 멜로디를 연주하는 스트링은 불안한 분위기를 자아낸다\'\n' +
      '\n' +
      '\'마이크는 노래의 절정에 이르자\' \'리드 싱어의 소울풀하고 펑키한 비명을 들었다\'\n' +
      '\n' +
      '\'사레드럼과 루트는 생동감 넘치는 듀엣을 연주했으며, 올가미드럼은 꾸준한 비트를 제공하고 루트는 위에서 멜로디를 연주했다\'\n' +
      '\n' +
      '두 래퍼는 박동하는 신디사이저 비트에 맞춰 구절을 주고받으며 에너지가 넘치고 전염성이 강한 사운드를 만들어냈다.\n' +
      '\n' +
      '\'배핑은 펑크 백으로 공격적인 곡을 연주하고\'\n' +
      '\n' +
      '현악 4중주는 활기찬 곡조를 연주한다.\n' +
      '\n' +
      '\'매우 빠른 피아노 카덴차\'\n' +
      '\n' +
      '\'고독한 하모니카는 사막에서 불어오는 바람의 소리 위에서 출몰하는 멜로디를 연주한다\'\n' +
      '\n' +
      '느린 기타 멜로디와 진한 베이스 라인이 돋보이는 공격적이지만 슬픈 펑크 구절입니다.\n' +
      '\n' +
      '콜롬비아의 마그달레나 강을 따라 배에서 큐미비아를 연주하는 밴드\n' +
      '\n' +
      '\'오르간 받침이 달린 느린 자메이카 스카 노래\'\n' +
      '\n' +
      '축음기 바늘이 비닐 레코드를 넘나들며 쾅쾅 소리를 내며 따뜻하고 향수를 불러일으키는 소리로 방을 가득 채웠다.\n' +
      '\n' +
      '\'패스트 피아노 토카타\'\n' +
      '\n' +
      '따뜻한 여성 목소리가 담긴 \'로맨틱 R\'n\'B 노래\'\n' +
      '\n' +
      '\'경쾌한 발리우드 스타일의 그룹 댄스\'\n' +
      '\n' +
      '\'선율적인 신스 라인과 아르페지이션을 가진 댄스 음악\'\n' +
      '\n' +
      '나무 봉고 드럼은 춤꾼들이 음악에 맞춰 몸을 움직이면서 깊은 저음을 울렸다.\n' +
      '\n' +
      '배킹 기타로 노래하는 테너\n' +
      '\n' +
      '\'느린 함정 노래\' \'리베브와 오토튠\'\n' +
      '\n' +
      '\'색소폰을 이용한 동기화된 진행성 록곡\'\n' +
      '\n' +
      '\'싱코프드 드럼이 하드 록 기타 리프를 치며\'\n' +
      '\n' +
      '\'그레고리언 성가\'\n' +
      '\n' +
      '\'춤추는 민속 왈츠는 아코디언에 의해 연주된다\'\n' +
      '\n' +
      '\'바핑은 춤을 추는 노래에서\' \'빠르게 귀에 쏙 들어오는 곡을 연주한다\'\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '## 부록 C 사용자 데이터의 장점\n' +
      '\n' +
      '섹션 5에서 우리는 RL을 사용하여 음악 생성 모델을 개선하기 위해 인간 평가자 데이터로 훈련된 모델을 활용할 수 있음을 보여준다. 그러나 평가자 데이터에는 몇 가지 제한 사항과 편향이 있습니다.\n' +
      '\n' +
      '행동 과학에서, 실험실 연구의 생태학적 타당성_5는 일반화의 잠재력을 현실 세계에 언급한다(Lewkowicz, 2001). 음악의 맥락에서 실제 환경에서 실험하는 것이 중요하다(Tervaniemi, 2023). 토마스와 켈로그(1989)는 인터페이스 설계의 맥락에서 _생태학적 타당성_ 개념을 탐색하고 "사용자 관련 생태적 격차는 사용자의 특성 - 동기를 부여하는 것, 사용자의 인지 능력, 선호도 및 습관과 같은 - 연구실과 대상 환경 간에 다를 수 있음 -에 의해 발생한다."라고 말한다. 사용자 관련 생태적 격차의 개념은 특히 평가자와 사용자가 종종 다르므로 대규모 언어 모델의 정밀화 및 평가와 관련이 있다.\n' +
      '\n' +
      '각주 5: [https://en.wikipedia.org/wiki/Ecological_validity](https://en.wikipedia.org/wiki/Ecological_validity)\n' +
      '\n' +
      '**인구 격차**레이터는 특히 등급 작업이 모델이 배치된 것과 다른 국가에 있는 사람들을 고용하는 크라우드소싱 서비스에 아웃소싱되는 경우가 많기 때문에 사용자 모집단을 대표하지 않는 경우가 많다. 이러한 인구 차이는 음악 선호도에 영향을 줄 수 있는 문화적 편견과 같은 편향을 발생시킨다(Trehub et al., 2015).\n' +
      '\n' +
      '각주 6: [https://www.mturk.com/](https://www.mturk.com/)\n' +
      '\n' +
      '**동기 갭.** Thomas and Kellogg(1989)에서 언급한 바와 같이, 상이한 사용자들 간의 동기들의 차이에 대응하는 _동기 갭_은 결과에 상당한 영향을 미칠 수 있다. 우리의 맥락에서, 음악 생성 모델의 사용자는 모델과 함께 연주하는 데 진정한 관심을 가지고 있지만, 평가자의 인센티브는 매우 다르다. 따라서 평가 작업의 경우 평가자가 편향의 원인이 될 수 있는 평가 작업의 생성자가 기대하는 것과 일치하는 결정을 내릴 수 있도록 특정 지시 세트를 제공하는 것이 중요하다. 반면에 사용자의 경우 지침이 제공되지 않는 일반적인 상호 작용에 관심이 있습니다.\n' +
      '\n' +
      '**Dataset Size.** 평가자 데이터의 비용으로 인해 수집된 인간 선호의 수는 종종 100,000 이하(Lee et al., 2023; Stiennon et al., 2020; Ziegler et al., 2019)이다. 한편, 사용자 상호작용의 수는 일단 모델이 전개되면 그보다 훨씬 더 많을 수 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>