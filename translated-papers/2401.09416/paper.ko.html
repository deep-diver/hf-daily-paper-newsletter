<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '이미지 인식 세확산 차단제: 이미지 유도 패턴 합성\n' +
      '\n' +
      '유징 예하\\({}^{13}\\,{}^{23}\\) 창이 김\\({}^{3}\\,{}^{3}\\) 루 응우옌-Phuoc\\({}^{3}\\).\n' +
      '\n' +
      '정장\\({}^{3}\\)는 카를 S 마셜\\({}^{3}\\,{}^{3}\\) 자하닌 Li\\({}^{3}\\)\n' +
      '\n' +
      '캘리포니아 대학교의 샌디에이고\\({}^{1}\\,{}^{2}\\,{}^{3}\\)입니다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '소수의 입력 이미지(3~5)에서 임의의 카테고리에 걸쳐 3D 형상을 표적으로 하기 위해 재발 가능한 질감을 전달하기 위한 새로운 이미지 유도 텍스처 합성 방법인 TextureDreamer를 제시한다. 텍스트 생성은 비전과 그래픽의 중추적인 도전입니다. 산업 기업들은 경험 있는 아티스트를 고용하여 3D 자산을 위한 텍스처를 수동으로 제작합니다. 고전적인 방법은 조밀하게 샘플링된 뷰와 정확하게 정렬된 기하학을 필요로 하는 반면, 학습 기반 방법은 데이터세트 내의 카테고리 특이적 형상에 국한된다. 대조적으로, TextureDreamer는 매우 상세하고 복잡한 텍스처를 실제 환경에서 소수의 카지노 캡처된 이미지만으로 임의의 객체들로 전달할 수 있으며, 잠재적으로 유의하게 민주화된 텍스처 생성을 촉진할 수 있다. 우리의 핵심 아이디어, 개인화된 기하학 인식 점수 증류(PGSD)은 질감 정보 추출을 위한 개인화된 모델링, 세부 외관 합성을 위한 가변 점수 증류, 제어넷을 사용한 명시적인 기하학 안내 등 확산 모델의 최근 발전에서 영감을 얻는다. 우리의 통합과 몇 가지 필수 변형은 질감의 품질을 실질적으로 향상시킵니다. 상이한 카테고리에 걸쳐 있는 실제 이미지에 대한 실험은 TextureDreamer가 매우 현실적이고 의미 있는 의미 있는 질감을 임의의 물체에 성공적으로 전달할 수 있어 이전 최첨단 기술의 시각적 품질을 능가할 수 있음을 보여준다. 프로젝트 페이지: [https://://edreamer.githubio](https://textreamer.githubio) (https://://ulsereamer.githubio)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '고품질 3D 콘텐츠는 AR/VR, 로봇 공학, 필름 및 게임을 포함한 광범위한 중요한 응용 분야에 필수불가결한 것이다. 최근 3D 재구성 [40, 42] 및 생성 모델[18, 59]의 발전으로 촉진된 3D 콘텐츠 생성 파이프라인을 민주화하는 데 괄목할만한 진전이 있었다. NRF[40]과 같은 _ge측정 성분_[8, 12, 64] 및 신경 암묵적 표현[44]을 탐색하는 데 상당한 관심이 집중되었지만 고품질 _텍스트_의 생성은 상대적으로 과소 설명된다. 텍스트는 현실적인, 매우 상세한 모습을 만드는 데 중추적이며 전통적으로 산업이 전문적이고 경험 있는 아티스트들이 텍스처를 만들기 위해 텍스처를 만들기 위해 다양한 그래픽 파이프라인에 필수적입니다. 이 과정은 일반적으로 수동으로 절차 그래프[1]와 UV 지도를 저작하여 비싸고 비효율적인 것을 포함한다. 따라서 주변 대상의 다양한 시각적 외관을 목표 기하학의 질감으로 자동으로 이전하는 것은 매우 유익할 것이다.\n' +
      '\n' +
      '우리는 희박한 이미지로부터 고품질의 재발 가능한 텍스처를 생성하기 위한 새로운 프레임워크인 _TextureDreamer_를 제시한다. 객체에 대한 무작위로 샘플링된 3~5개의 뷰를 감안할 때, 우리는 그 텍스처를 다른 카테고리에서 발생할 수 있는 표적 기하학으로 전달할 수 있다. 이전 질감 생성 방법은 보통 정렬된 기하학[3, 32, 68]으로 조밀하게 샘플링된 뷰가 필요하거나 카테고리 특이적 형상[4, 21, 46, 58]에서만 작동할 수 있기 때문에 매우 어려운 문제이다. 우리의 프레임워크는 확산 기반 생성 모델[23, 59, 60]의 최근 발전에서 영감을 얻습니다. 수십억 개의 텍스트 이미지 쌍으로 훈련된 이러한 확산 모델은 특별한 시각적 품질과 다양성[52]로 텍스트 유도 이미지 생성을 가능하게 한다. 이 사전 훈련된 2D 확산 모델을 텍스트 유도 3D 콘텐츠 생성 [34, 47, 63]에 적용한 작품들이다. 그러나 이러한 방법들 사이의 공통 한계는 그림 2에서 보여지는 바와 같이, _텍스트 전용 입력_가 복잡한 세부 패턴을 설명하는 데 충분히 발현되지 않을 수 있다는 것이다. 텍스트 유도 방법과 대조적으로, 고유한 텍스트 토큰[16, 54]으로 미리 학습된 확산 모델을 미세 조정함으로써 입력 이미지들의 작은 세트로부터 텍스처 정보를 효과적으로 추출한다. 따라서 우리의 프레임워크는 복잡한 질감을 정확하게 설명하는 문제를 다룬다.\n' +
      '\n' +
      '스카어 교차 샘플링(SDS) [47, 62]은 3D 콘텐츠 생성으로 사전 훈련된 2D 확산 모델을 교량의 하나의 핵심 요소이다. 이는 렌더링된 이미지의 분포와 미리 학습된 확산 모델[34, 37]에 의해 정의된 분포의 불일치를 최소화함으로써 3D 콘텐츠를 생성 및 편집하는 데 널리 사용된다. 인기에도 불구하고 잘 알려진 두 가지 한계는 고품질 질감을 생성하는 능력을 방해한다. 먼저, 방법이 수렴되는 데 필요한 이례적으로 높은 분류기가 없는 지침으로 인해 과부팅되고 포화되는 모습을 생성하는 경향이 있다. 둘째, 3D-지속적 외관을 생성하는 지식이 부족하며, 종종 질감과 기하학 사이의 다면적 유물 및 불일치를 초래한다.\n' +
      '\n' +
      '이러한 과제를 해결하기 위해 두 가지 주요 디자인 선택을 제안합니다. SDS를 사용하는 대신 최적화 접근법에서 변색 스코어 교차(VSD)를 구축하여 훨씬 더 광학적 및 다양한 질감을 생성할 수 있다. 처음에 ProlificDreamer[63]에 도입된 VSD는 전체 3D 표현을 무작위 변수로 취급하고 그 분포를 사전 훈련된 확산 모델과 정렬한다. 현실적이고 다양한 외관을 만드는 데 필수적인 수렴을 위해서는 큰 분류기가 없는 지침 중량이 필요하지 않다. 그러나 VSD 업데이트를 순전히 적용하면 응용 프로그램에서 고품질 질감을 생성하는 데 충분하지 않습니다. 우리는 계산 비용을 약간 줄이면서 질감의 질을 향상시킬 수 있는 간단한 수정을 식별한다. 또한 VSD 손실만으로는 3D 일관성 문제를 완전히 해결할 수 없다. 거친 투입물에 대해 미세 조정은 이전 작업[51]에서 관찰된 바와 같이 수렴을 더 어렵게 만든다. 따라서 컨트롤넷[67] 아키텍처를 통해 미세 조정 확산 모델에 렌더링된 정상 맵을 주입하여 주어진 메쉬에서 추출한 기하학 정보에 대한 질감 생성 과정을 명시적으로 조건화한다. 개인화된 기하학적으로 인식되는 점수 증류(PGSD)으로 지정된 우리의 프레임워크는 의미 있고 시각적으로 매력적인 방식으로 매우 상세한 질감을 다양한 기하학에 효과적으로 전달할 수 있다. 광범위한 정성적 및 정량적 실험은 우리의 틀이 실질적으로 최첨단 텍스처 전달 방법을 능가한다는 것을 보여준다.\n' +
      '\n' +
      '2번으로 작업했습니다.\n' +
      '\n' +
      '**exture 합성 및 재구성** 클래식 텍스처 생성 방법은 동네 [13, 28], 타일링 반복 패턴[29]에서 파생된 분포로부터 샘플링하거나 대상체에 멀티뷰 이미지를 융합하는 것을 포함한다.\n' +
      '\n' +
      '그림 2: ** 텍스트 유도 텍스트링의 제한***는 이미지의 모든 세부 사항을 표현할 수 없는 텍스트 프롬프트를 생성하기 위해 자막 방법을 필요로 하는 텍스트 유도 텍스트링 방법에 따라 이미지 기반 가이드 텍스트링은 더 효과적이고 표현적일 수 있다. BLIP[33]에 의해 이미지 캡션이 예측되며, TEXTure[53]을 통해 텍스트 유도 텍스트링이 생성되며 이미지 유도 결과는 우리의 방법에서 비롯된다.\n' +
      '\n' +
      '표면 [3, 32, 68]. 전자는 의미론적 의미 있는 질감을 만드는 데 짧지만 후자는 매우 정확한 기하학 재구성이 필요하다. 대규모 3D 데이터세트[4, 11, 21, 46, 58]에서 텍스처 생성을 학습하되 데이터세트 내에서 특정 카테고리에 국한되도록 수많은 학습 기반 방법을 제안하였다. 최근의 작품들은 또한 임의의 객체들의 텍스트 유도 텍스처 생성(31, 36, 39, 41)에 CLIP 모델[50]을 사용하지만, 그 질감 특성은 보통 낮다. 대조적으로, Texture-Dreamer는 미관련 희소 이미지를 사용하여 임의의 객체에 대한 의미 있는 고품질 텍스처를 생성할 수 있다. 전통적으로 질감은 2D 이미지로 표현되며 UV 매핑을 통해 표면을 객체화할 것으로 예상된다. 신경 암묵적 표현의 최근 진행 상황을 추적해보면, 우리의 방법은 역 렌더링 [5, 7, 17, 61] 및 3D 생성[7, 17]의 최근 발전과 함께 신경 암묵적 텍스처 필드로 질감을 나타낸다.\n' +
      '\n' +
      '**D확산 모델**디퓨전 모델은 최첨단 생성 모델[23, 60]으로 등장하여 탁월한 시각적 품질[52]를 보여준다. 그것의 훈련과 추론은 서로 다른 분산을 가진 노이즈를 반복적으로 추가하고 데이터를 변성시키는 것을 포함한다. 인터넷 규모의 이미지-텍스트 쌍 데이터 세트[52]에서 훈련된 이러한 사전 훈련된 모델은 텍스트 유도 이미지 합성에서 전례 없는 능력을 나타내며 다양한 이미지 편집 작업에서 성공적인 것으로 입증되었다. 최근 작품들은 또한 훨씬 더 작은 데이터셋들 또는 심지어 몇 개의 이미지들에 대해 미세 조정된 사전 훈련된 확산 모델을 관리하여 맞춤형/개인화된 이미지 합성 [54] 및 정상 및 의미적 맵과 같은 멀티 모달 데이터 [67] 상에서 조건화된 이미지 생성을 용이하게 한다. 이러한 진행 상황에 따라, TextureDreamer는 희소 뷰로부터 질감 정보를 효과적으로 추출하여 기하학 인식 방식으로 새로운 대상 객체로 전달할 수 있다.\n' +
      '\n' +
      '2D 확산제*** 디확산 기반 3D 콘텐츠 생성으로**3D 생성은 최근에 상당한 관심을 얻었습니다. 포인트 클라우드[35]를 포함한 다양한 표현에서 3D 콘텐츠를 생성하기 위해 3D 확산 모델을 직접 훈련시키는 여러 방법[35], 신경망 방사도는 [26], 하이퍼네트워크[14] 및 텍스처[66]를 제공했다. 다른 사람들은 다양한 뷰[2, 6, 9, 53]에서 생성된 이미지를 점진적으로 융합하거나 점수 증류 샘플링[34, 37, 47] 및 개선된 변화[27, 63]를 통해 3D 표현을 최적화하여 사전 훈련된 2D 확산 모델을 사용한다. 많은 방법이 텍스트 유도 3D 생성에 초점을 맞추는 반면, 이미지로부터 3D 콘텐츠를 생성하기 위해 확산 모델을 레버리지하려는 시도는 더 적다. 대략적인 뷰 재구성을 위한 대규모 3D 데이터셋(48, 57)에 대한 많은 동시 작업 2D 확산 모델들이 주로 전체 3D 객체 재구성에 초점을 맞추고 있다. 대조적으로, Texture-Dreamer는 일치하지 않는 기하학을 가진 소수의 이미지로부터 타겟 3D 형태로 텍스처를 전달하는 것을 목표로 한다. 드림보스3D[51]과 TEXTure[53]은 희박한 뷰로부터 새로운 텍스트 토큰과 미세 도금 확산 모델 가중치로 정보를 추출하는데, 이는 개인화된 3D 객체 또는 질감 없는 객체를 생성하는 데 사용될 수 있다. 텍스트-드림러는 희박한 이미지로부터 정보를 추출하는 것과 유사한 방법을 사용한다. 다만, 추출된 정보를 질감 생성을 위해 활용하는 선행 작품과는 차이가 있어 일관성과 광학적주의의 개선으로 이어진다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '우리는 물체의 3-5 입력 이미지와 유사한 외관을 가진 주어진 메쉬에 대해 기하학 인식 질감을 합성하는 프레임워크인 TextureDreamer를 제안한다. 3.1절에서 우리는 먼저 드림보트[54], 컨트롤넷[67]에 대한 예선을 소개하고 점수 증류 샘플링[47, 62, 63]을 소개한다. 3.2절에서는 희박한 이미지에서 임의의 기하학으로의 고품질의 이미지 유도 질감 전달을 가능하게 하는 핵심 기술 기여인 개인화된 기하학 인식 점수 증류(PGSD)을 제안한다.\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '**Dreambooth***[54]는 개인화된 텍스트 유도 이미지 생성을 위한 소수의 입력 이미지 상에서 미리 학습된 텍스트 대 이미지 확산 모델을 미세 조정하기 위한 간단하고 효과적인 방법이다. 특정 텍스트-토큰 _"[V]"_로 확산 모델 가중치에 피험자의 모습을 저장한다. 드림보드는 두 개의 손실 기능으로 미세 조정됩니다. 재건축 손실은 입력 이미지에 대한 표준 확산 변성 감독이다. 언어 드리프트와 미세 조정으로 인한 다양성 손실을 피하기 위해 수업별 사전 보존 손실이 제안된다. 자체 생성 사례가 많은 사전 학습 모델을 더 감독한다. 텍스트링드림어는 드림보드를 사용하여 입력 이미지로부터 텍스처 정보를 증류합니다. 이미지 합성 대신 기하학이 다른 3D 객체에 증류 정보를 적용한다.\n' +
      '\n' +
      '** 대조군Net***[67]은 사전 훈련된 확산 모델에 공간 조절 제어를 추가하는 새로운 아키텍처를 제안한다. 주요 통찰력은 수십억 개의 이미지에 대해 훈련된 많은 수의 확산 모델 파라미터를 재사용하고 윈도우 크기 1 및 제로-시알화된 가중치로 작은 컨볼루션 네트워크를 모델에 삽입하는 것이다. 깊이, 정상, 에지 맵과 같은 다양한 유형의 2D 조건을 가진 소형 데이터셋에 대한 강력한 미세 조정 성능을 가능하게 합니다. 우리는 제어넷 모델을 사용하여 생성된 질감이 주어진 기하학과 정렬되도록 합니다.\n' +
      '\n' +
      '**Score Distillation 샘플링**[47, 62]는 3D 콘텐츠 생성[10, 34, 47]을 위해 미리 훈련된 2D 확산 모델을 사용하는 수많은 방법의 핵심 구성 요소이다. 이는 렌더링된 이미지를 미리 학습된 확산 모델에 의해 모델링된 고차원 매니폴드에 밀어 넣음으로써 3D 표현을 최적화한다. \\(\\theta\\)는 3D 표현이고 \\(\\epsilon_{\\psi}\\)는 미리 훈련된 확산 모델이다. 파라미터 \\(이하ta\\)에 역전파된 기울기는 파라미터 \\(\\theta\\)에 있다.\n' +
      '\n' +
      '}(\\mathbb{t)\\ll}(\\mathbb{E})\\epsilon}(\\mathbf{x}_{t},y,\\med)\\\n' +
      '\n' +
      '\\(c\\)는 문자 입력,\\(c\\)는 시간 계수로,\\(c\\)는 카메라 포즈, \\(g(\\cdot)는 서로 다른 렌더링자, \\(\\mathbf{x}_{t}\\)는 시간 \\(t\\)에 의존하는 분산을 갖는 렌더링된 이미지(\\mathbf{x}=g(\\mathbf{x})에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(I\\)는 시간(t\\)인 경우,\\(Hf{x}_{t)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\(\\)에 대한 분산을 사용하여 계산되는 이미지 \\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지)인 경우,\\(\\)는 시간 \\)인 경우,\\(\\)에 노이즈를 추가하여 계산된 시끄러운 이미지이다. 광범위한 사용에도 불구하고 SDS는 정상 분류기가 없는 지침[22]보다 훨씬 더 높은 중량을 요구하여 수렴, 과잉 흡입 및 과포화 외관을 필요로 한다. 이 문제를 극복하기 위해 왕 등은 표준 분류기가 없는 지침으로 수렴할 수 있는 가변 점수 증류(VSD)이라는 개선된 버전을 제안한다. VSD는 전체 3D 표현 \\(\\theta\\)를 무작위 변수로 취급하고 \\(\\theta\\)와 사전 훈련된 확산 모델에 의해 정의된 분포 사이의 KL 분산을 최소화한다. 그것은 3D 표현 \\(\\)에서 생성된 시끄러운 이미지를 변성시키기 위해 카메라를 임베딩하는 LoRA[24] 네트워크 \\(\\epsilon_{\\phi}\\)와 카메라 인코더 \\(\\rho\\)를 포함한다.\n' +
      '\n' +
      '\\[\\min_{\\phi}\\mathbb{E}_{t,\\epsilon,c}\\big{[}||\\epsilon_{\\phi}(\\mathbf{x}_{t},y,t,c)-\\epsilon||_{2}^{2}\\big{]} \\tag{1}\\]\n' +
      '\n' +
      '그런 다음 3D 표현 \\(\\theta\\)에 대한 기울기는 3D 표현 \\(\\theta\\)에 대한 기울기로 계산된다.\n' +
      '\n' +
      '종종[\\mathbb{E}{t,\\mathbb{\\psi}\\]{mathbf{x}\\left[t)(\\mathbf{x}_{t}_{t},y,t \\right)-\\epsilon_{\\phi}(\\mathbf{x}_{t)-\\epsilon_{t)(\\mathbf{x}_{t,y,t \\right)-\\left)(\\mathbf{x}_{t,y,t)\\left(\\mathbf{x}_{t,y,t)\\left,y,t)\\left(\\mathbf{t)\\left,y,t-{t)-\\compon_{t)-\\compon_{t)-\\compon_{t)-\\compon_{t-{t)-\\compon_{t－{t-{t-{t)}(\\:\\left,y,y,t-{t)-\\compon_{t)-\n' +
      '\n' +
      'VSD는 생성된 3D 콘텐츠의 시각적 품질과 다양성을 모두 유의하게 향상시키지만 3D 지식의 고유한 부족으로 인해 3D 일관성 문제를 해결할 수 없어 기하학과 질감의 다면 오류 및 불일치로 이어진다. 우리는 확산 모델 기하학을 인식하도록 기하학 정보를 명시적으로 주입함으로써 이 문제를 해결한다.\n' +
      '\n' +
      '## 개인화 지질 측정-인식 점수 교차(PGSD)이다.\n' +
      '\n' +
      '**문제 설정*** 우리의 방법을 그림 3에 보여준다. 프레임워크에 대한 입력에는 다양한 뷰(\\{I\\}_{k =1}^{K}\\)와 타겟 3D 메쉬 \\(\\mathcal{M}\\)에서 캐소하게 캡처된 작은 이미지 세트(3~5)가 포함된다. 우리의 프레임워크의 출력은 이미지 세트(\\{I\\}_{k=1}^{K}\\)에서 \\(\\mathcal{M}\\)로 의미 있고 시각적으로 즐거운 방식으로 전달되는 재발 가능한 질감이다. 우리의 재발 가능한 질감은 3개의 매개변수, 확산 알비도 \\(\\), 거칠기 \\(r\\), 금속성 \\(m\\)로 구성된 표준 미세 표면 양방향 반사율 분포(BRDF) 모델[25]로 매개변수가 된다. 우리는 파이프라인을 메쉬 \\(\\mathcal{M}\\)와 일치하지 않는 가짜 세부 정보를 장려하기 때문에 의도적으로 정상 지도를 최적화할 수 없다. 최적화 동안 신경 암묵적 표현 [20, 42, 43]의 최근 흐름에 따라 우리는 질감을 신경 BRDF 필드 \\(f_{\\theta}(v):v\\in\\mathbb}}^{R}^{3}\\rightarrow,a,r,m\\in\\mathb{R}^{5}\\)로 표현하는데, 여기서 (v\\)는 \\의 표면에 샘플링된 임의의 지점이다.\n' +
      '\n' +
      '그림 3: ** 섬유꿈치**는 물체의 3-5 입력 이미지와 유사한 외관으로 주어진 메쉬에 대한 질감을 합성하는 프레임워크이다. 우리는 먼저 입력 이미지에 대한 드림보트[54] 핀셋을 사용하여 개인화된 확산 모델 \\(\\psi\\)을 얻는다. 3D 메쉬 \\(\\mathcal{M}\\)에 대한 공간적으로 횡방향 반사율 분포(BRDF) 필드 \\(f_{\\theta}\\)가 개인화된 기하학적 인식 점수 증류(PGSD)(3.2절에서 상세화)을 통해 최적화된다. 최적화가 끝난 후 최적화된 BRDF 필드에서 알베도, 금속성, 거칠기에 해당하는 고해상도 텍스처 맵을 추출할 수 있다.\n' +
      '\n' +
      '및 \\(f_{\\theta}\\)는 다중 규모의 해시 인코딩과 작은 MLP로 구성된다. 이러한 암묵적 표현은 최적화 과정을 더 잘 규칙화할 수 있어 보다 원활한 질감을 이끌어낼 수 있다는 것을 알게 된다. 그러나 \\(\\mathcal{M}\\)의 UV 매핑을 감안할 때, 우리의 표현은 그림 3의 오른쪽 측면에서 볼 수 있듯이 각 텍셀에 해당하는 3D 포인트마다 질의함으로써 표준 그래픽 파이프라인과 호환되는 표준 2D 텍스처 맵으로 전환될 수도 있다.\n' +
      '\n' +
      '** 개인화된 텍스처 정보 추출*** 우리는 희박한 이미지로부터 텍스처 정보를 추출하기 위해 드림보스[54]를 따른다. 구체적으로, 우리는 텍스트 프롬프트 \\(y\\), [V] 객체"___\'A 사진이 있는 입력 영상에 대한 개인화된 확산 모델을 미세 조정했으며, 여기서 _"[V]"_은 입력 객체를 설명하기 위한 고유 식별자이다. 대체 텍스트 반전 방법[16]에 비해 드림보드가 더 빠르게 수렴하여 더 큰 용량 때문에 복잡한 질감 패턴을 더 잘 보존할 수 있음을 관찰한다. 우리는 먼저 흰색으로 타겟 오브젝트의 배경을 가린다. 재건축 손실을 위해 입력 이미지의 더 짧은 가장자리를 512로 재구성하고 훈련용 무작위로 작물 512x512 패치를 재구성한다. 우리의 꿈보트 핀셋링 모델이 다른 범주에 일반화할 수 있기를 바라며, 우리는 계급별 사전 보존 손실을 적용하지 않는다. 또한 텍스트 인코더를 공동으로 미세 조정하고 확산 변성 네트워크를 사전 훈련된 제어넷으로 대체하는 것을 포함하여 다양한 변화를 실험하지만 개선점을 관찰하지 못한다.\n' +
      '\n' +
      '**Ge측정 인식 점수 증류** Once는 드림보드로 질감 정보를 추출하고, 점수 증류 샘플링을 위한 변성 네트워크 \\(\\epsilon_{\\psi}\\)로 미세 박리된 드림보드를 채택하여 메쉬 \\(\\mathcal{M}\\)로 정보를 전달합니다. 구체적으로, 우리는 매우 현실적이고 다양한 모습을 생성하는 우수한 능력 때문에 원래 SDS 대신 VSD를 선택한다. VSD 구배 계산을 위한 이미지 \\(\\mathbf{x}\\)를 렌더링하기 위해 고정된 HDR 환경 맵 \\(E\\)을 조명으로 사전 선택하기 위해 판타지아3D[10]를 따르고 Nvdiffuls [30]을 다양한 렌더링기로 사용한다. 우리는 꿈보트 교육을 위한 입력 이미지와 일치하도록 객체 배경을 일정한 화이트 컬러로 설정했다. 우리는 이것이 무작위 색상이나 중립 배경에 비해 더 나은 색상 충실도를 달성하는 데 도움이 될 수 있음을 관찰한다.\n' +
      '\n' +
      '그러나 단순히 SDS를 VSD로 대체하면 2D 확산 모델에서 3D 지식이 부족한 한계를 해결할 수 없다. 따라서 우리는 메쉬 \\(\\mathcal{M}\\)에서 추출한 기하학 정보를 \\(\\mathcal{M}\\)에서 렌더링된 정상 맵(k\\)에 미리 학습된 제어넷을 통해 개인화된 확산 모델 \\(\\epsilon_{\\psi}\\)에 주입하는 기하학 인식 점수 증류를 제안한다. 이 증강은 생성된 질감의 3D 일관성을 크게 증가시킨다(그림 10 참조). 제어넷 컨디셔닝으로, 형상 불일치에도 불구하고 입력 이미지로부터의 베개 텍스처가 목표 형상에 정확하게 매칭될 수 있다. 우리는 다른 제어넷 조건을 실험하고 정상적인 조건이 질감 측정 불일치를 가장 잘 예방할 수 있음을 보여준다.\n' +
      '\n' +
      '\\(\\mathbf{x}=g(\\theta,c)\\)는 추출된 BRDF 맵 \\(a_{\\theta},r_{\\theta},m_{\\theta}\\)로 카메라 포즈 \\(c\\)에서 고정된 환경 맵 아래 렌더링된 이미지이다. BRDF 분야의 MLP 매개변수 \\(\\theta\\)를 최적화하기 위해 제안된 개인화된 지질 분석 인식 점수 교차(PGSD)의 기울기가 있다.\n' +
      '\n' +
      '(\\mathbf{x})\\mathbf{t}(\\mathbf{t})\\.\n' +
      '\n' +
      '시간 \\(t\\), \\(c_{\\rho}\\)에서 카메라 외재성 \\(c\\bf{I}(\\math{f{I})의 임베딩(\\math{f{I})은 학습 가능한 카메라 인코더 \\(\\rho\\)에 의해 암호화된 카메라 외재성 \\(c\\)의 임베딩(\\math{I}(\\math{f{I}(\\b\\,\\b\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\,\\b\\(\\b\\)은 각각\\(\\b\\(\\b\\(\\b\\(\\b\\(\\b\\)\\(\\b\\(\\b\\:\\(\\b\\)의 임베딩)이고,\\(c\\(\\b\\(\\b\\(\\b\\,\\b\\) 입니다.{t{t{t{t{t{t{t{f{f{f{f{f{f{f{f{f{f{f{f{f{f{I} 두 모델 모두 그림 3의 확산 모델 아래의 노란색 부분에 나타난 바와 같이 정상 지도 \\(k\\)에 조절망을 조건화하여 증강된다.\n' +
      '\n' +
      '우리는 이 방법이 분류기가 없는 안내(CFG) [22]에서 혜택을 받지 않는다는 것을 발견했는데, 이는 개인화된 모델 \\(에피실론_{\\psi}\\)가 소수의 이미지에 미세 조정되었기 때문일 것이다. 우리의 목표는 입력 외관을 목표 형태로 충실히 전달하는 것이기 때문에 다양성을 높이기 위해 CFG를 가질 필요는 없다. 최근의 문헌[55]에서도 유사한 관찰이 발견될 수 있다.\n' +
      '\n' +
      '우리는 또한 광범위한 실험을 통해 몇 가지 중요한 디자인 선택을 식별한다. 먼저 Eq에서 \\(\\epsilon_{\\파이}\\)를 초기화하는 것이 중요하다. 원래 사전 훈련된 확산 모델 가중치가 있는 1과 드림보드의 무게는 질감 디테일을 제거할 것입니다. 이는 드림보드의 미세 조정 과정이 이전 작업[51]에서 지적한 바와 같이 확산 모델이 작은 훈련 세트에 과적합하게 만들기 때문일 것이다. 또한 LoRA 가중치를 제거하는 것이 질감 충실도를 실질적으로 향상시킬 수 있다는 것을 발견했다. LoRA 훈련의 유사한 어려움도 [56]에서 보고되었다. 따라서 우리는 개인화된 기하학 인식 점수 증류 손실 \\(\\mathcal{L}_{PGSD}\\)를 구현하여 \\(\\epsilon_{\\파이}\\)에서 LoRA 구조를 제거하고 카메라 임베딩만 유지하면서 최고의 품질을 달성했다. 우리는 그림 10에서 더 많은 비교를 보여준다.\n' +
      '\n' +
      '## 4 Experiment\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '우리는 소파, 침대, 머그/보울 및 플러시 장난감 4가지 범주에서 실험을 수행한다. 각 카테고리에 대해 8개의 객체 인스턴스를 선택하고, 객체를 둘러싼 3~5개의 뷰를 자유로이 샘플링하여 작은 이미지를 생성하여 총 32개의 이미지 세트를 생성한다. 32개의 이미지 세트 내의 모든 이미지에 대해 U2-Net[49]을 적용하여 전경 마스크를 자동으로 획득하거나 반자동 배경 제거 애플리케이션1을 사용하여 보다 정확한 마스크를 얻는다. 우리는 동일한 카테고리 형상, 상이한 카테고리 형상 또는 다른 속 수를 갖는 기하학을 포함하지만 제한되지 않는 다양한 메쉬로 설정된 각 이미지 세트에 대한 텍스처 전달을 수행한다. 질감 전달 프레임워크를 테스트하기 위해 캡처된 이미지 세트와 다른 4개 카테고리 각각에 대해 3개의 메쉬를 선택합니다. 우리는 3D-FUTURE [15] 및 온라인 레포스터.23에서 이러한 3D 메서를 획득한다. 4가지 범주의 모든 객체에 대해 클래스 내 질감 전달을 실행하고 침대와 의자 사이의 클래스 간 질감 전달을 실행하여 방법의 일반화 능력을 테스트합니다.\n' +
      '\n' +
      '발주 2: [주행://www.cgtrader.com/] (https://www.cgtrader.com/) (https://www.cgtrader.com/)\n' +
      '\n' +
      '부츠 3: [https://sketchfah.com/] (https://sketchfah.com/)\n' +
      '\n' +
      '** 구현 세부 정보** 우리는 PyTorch [45] 및 트라리오 [19]를 기반으로 프레임워크를 구현한다. 잠재 확산 및 제어넷 v1.1을 각각 사전 훈련된 확산 모델 및 제어넷으로 사용한다. 모든 실험에서 우리는 \\(\\mathcal{L}_{PGSD}\\)의 분류기가 없는 지침 중량을 원래 CFG 제형에서 \\(\\omega=0\\) 설정과 동등하다고 설정했다. 드림퓨전[47]에 이어 입력 텍스트 프롬프트에도 뷰 의존적 컨디셔닝을 적용합니다. BRDF 필드는 사전 작업 [10, 63]에 따라 해시-그리드 위치 인코딩[42]을 사용하여 MLP로 매개변수가 된다. 당사의 카메라 인코더는 카메라 외재성을 U-Net에서 시간과 텍스트 임베딩과 융합될 \\(1,280\\) 치수의 잠재 벡터에 투영하는 두 개의 선형 층으로 구성된다. 모든 실험에 대한 카메라 인코더용 \\(0.001\\) 및 \\(0.0001\\)를 인코딩하기 위한 학습률을 \\(0.01\\)로 실증적으로 설정하였다.\n' +
      '\n' +
      '### Baseline methods\n' +
      '\n' +
      'Latent-paint [37]과 TEXTure[53]은 이전에 2D 확산을 가진 최근 두 가지 텍스트 유도 텍스트링 방법이다. 그들은 또한 이미지로부터 메서를 텍스트화하는 능력을 보여준다. 표현식 [37]은 이미지 정보를 텍스트 임베딩으로 추출하고 텍스쳐를 SDS로 증류하기 위해 텍스트 변환 [16]을 기록한다. TEXTure[53] 첫 번째 핀셋은 텍스트 전환과 드림보스[54]를 결합하여 사전 훈련된 확산 모델을 융합하고 이 미세 조정 모델을 사용하여 반복적인 메쉬 페인팅 알고리즘으로 텍스쳐를 합성한다. 이전 방법[53]에 의해 선호되는 바와 같이, 우리는 랜덤 컬러 바탕으로 입력 이미지를 증가시킨다. 우리는 실험을 실행하기 위해 기준선 방법의 원래 구현을 밀접하게 따른다.\n' +
      '\n' +
      '임기 유도 질감 전달.\n' +
      '\n' +
      '*** 정성적 평가** 우리 방법은 _same_ 범주의 기하학 또는 다양한 범주에서 기하학을 포함하여 다양한 객체 기하학으로의 질감 전달을 수행할 수 있다. 그림 4는 4가지 범주의 객체에 대한 우리의 질감 전달 결과를 보여준다. 우리의 방법은 입력과 유사한 패턴과 스타일을 갖는 기하학 인식 및 심리스 질감을 합성할 수 있다. 우리는 또한 우리의 방법이 텍스처 _가닥 다른 카테고리_를 전달할 수 있음을 보여준다. 그림 1에서 소파 이미지에서 침상 형상으로 질감 전달 결과를 보여주고 그 반대의 경우도 마찬가지이다. 우리의 방법은 또한 광범위한 다양한 범주에서 질감 전달을 수행할 수 있다. 그림 5와 같이 의자와 머그, 플러시 장난감 범주에 걸쳐 고품질 및 실감형 질감을 합성할 수 있다. 합성된 질감은 알베도, 금속성 및 거칠기 지도를 포함하므로 그림 6과 같이 합성된 외관이 있는 표적 물체를 재배치할 수 있으며, 다른 무작위 종자를 사용하면 그림 8과 같이 다양한 질감을 생성할 수 있다.\n' +
      '\n' +
      '그림 7에서 우리는 방법을 기준 방법과 정성적으로 비교한다. 각 예에서 두 가지 견해가 표시된다. Latent-Paint은 입력 이미지와 다른 색상 및 패턴으로 텍스처를 생성하는 경향이 있다. TEXTure는 라텐트-Paint보다 색깔과 질감을 더 잘 보존할 수 있지만 식감은 가시적인 솔기를 포함하고 있다(아마도 반복적인 그림으로 인한 것). 우리의 결과 방법은 기하학(_e.g__눈의 위치)의 의미론적 이유를 설명할 수 있다. 입력 이미지로부터 더 높은 충실도로 더 높은 품질, 심리스 및 기하학 인식 텍스트링 결과를 보여준다.\n' +
      '\n' +
      '** 정량적 평가** 기하학과 사진의 형상 차이로 인한 질감 이동에 대한 정량적 비교를 수행하는 것은 비침습적이다. 사용자 연구를 수행하여 사용자들에게 다음과 같은 질문을 함으로써 기저부에 걸친 충실도, 질감 광학적, 질감 측정 호환성을 평가하는데, 1) 입력 이미지와 더 유사하게 보이는 질감을 가지고 있는가? 2) 진짜 물건처럼 보이는 질감이 있어요? 3) 메세지와 더 호환되는 질감이 있습니까? (기하학에 더 맞춰진 텍스처가 어느 정도인가?) 세 가지 별도 작업으로 아마존 투르크와 함께 사용자 연구를 진행한다. 각 작업에 대해 각 사용자 24 질문에 문의합니다. 각 질문은 동일한 \\(4\\) 샘플링된 뷰의 렌더링된 이미지를 가진 우리의 두 가지 옵션과 하나의 기준 결과를 가진 강제 단일 선택 선택이며 20명의 다른 사용자에 의해 평가된다. 우리는 첫 번째 유사도 질문에 대한 입력 사진만 보여주고, 다른 두 질문에 대한 입력 사진을 숨겨 사용자가 질감 품질에 초점을 맞출 수 있도록 한다. 우리는 표 1의 결과를 요약하면 결과를 요약할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline  & \\multicolumn{2}{c}{Ours preferred over} \\\\ \\cline{2-3}  & Latent-Paint & TEXTure \\\\ \\hline Image Fidelity & 71.82\\% & 69.43\\% \\\\ Texture Photorealism & 77.03\\% & 85.52\\% \\\\ Shape-Texture Consistency & 78.49\\% & 85.16\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스처 전달에 대한 표 1: ** 사용자 연구***.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline Latent-Paint [37] & 0.7969 \\\\ TEXTure [53] & 0.7988 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '영상 유도 텍스트화에 대한 표 2: ** 정량적 평가***.\n' +
      '\n' +
      '이미지 충실도, 텍스처 광자주의, 형상-텍스트 일관성 측면에서 사용자가 선호하는 것이다.\n' +
      '\n' +
      '우리는 또한 참조와 렌더링된 사이 이미지 기반 CLIP 특징 [41]을 통한 유사성을 평가할 것을 제안한다.\n' +
      '\n' +
      '그림 4: ** 이미지 유도 전달 결과**는 이미지 세트의 4개 범주(베드, 소파, 플러시 장난감 및 머그컵)에서 다양한 객체에 이르기까지 다양하다. 우리의 방법은 광범위한 객체 유형에 적용될 수 있고 텍스처를 다양한 객체 형태로 전달할 수 있다.\n' +
      '\n' +
      '합성된 질감의 장점. CLIP 유사성은 재료 매칭 [65] 및 양식화 [38]에 적용되었다. 좋은 전달은 이미지로부터 질감만을 전달해야 하며 목표 형상 기하학을 고려하여 정적으로 질감을 전달해야 한다. 예를 들어, 이체는 형상의 각 부분에 대해 도색되어야 한다. 우리는 비교를 계산하기 위해 평가 세트를 사용합니다. 각 이미지 세트 및 타겟 3D 메쉬 쌍에 대해 각 기준 이미지 중 메트릭의 평균과 샘플링된 \\(4\\)의 렌더링된 이미지 각각을 계산(좌측 전면, 우전, 좌뒤 및 우후면)한다. 우리는 모든(이미지 세트, 메쉬) 쌍에 걸쳐 CLIP 유사성을 평균화한다. 표 2는 우리의 방법이 CLIP 유사도가 가장 높다.\n' +
      '\n' +
      '### Ablation Studies\n' +
      '\n' +
      '우리는 먼저 기하학 인식 제어망의 중요성에 대한 절제 연구를 질적으로 수행한다. 그림 10에서 보는 바와 같이, 그 결과는 제어넷이나 깊이 기반 제어넷이 없는 기하학-텍스트 오정렬에 시달린다. 정상 기반 제어넷만이 합성된 텍스처를 입력 메시 기하학과 일치하도록 정확하게 제어할 수 있다. 다음으로 점수 증류 손실의 중요성을 검증한다. 우리의 프레임워크에서 SDS 손실을 사용하는 것만이 충분한 입력 충실도를 달성할 수 없으며 결과는 더 흐릿해지는 경향이 있다. LoRA가 제거되지 않고(일반적으로 바닐라 VSD 손실로 최적화되어 있는 경우, 최적화는 드림보트-피네이션 분포에서 분포를 발산시키는 경향이 있다. 이것은 원래 질감이 적지만 입력과는 더 무관한 패턴을 포함하는 출력을 초래한다. 이는 희귀 식별자가 포함된 텍스트 조건으로 LoRA 가중치를 최적화하는 것이 희소한 외관을 가지도록 렌더링된 이미지의 분포를 유도하는 경향이 있기 때문이라고 가정한다.\n' +
      '\n' +
      '일반 확산 모델 \\(\\epsilon_{\\파이}\\)을 개인화된 확산 모델 \\(\\epsilon_{\\psi}\\)로 대체하거나 분류기 자유 안내 가중치 \\(7.5\\)를 적용하면, 그 결과는 입력 이미지에 존재하지 않는 랜덤 패턴을 도입하는 경향이 있다. 카메라 인코더 가중치(\\rho\\)를 동결하는 것을 선택하면 결과는 전체 방법보다 더 심하거나 시끄러워진다.\n' +
      '\n' +
      '우리는 또한 시스템에서 각 구성요소의 중요성을 정량적으로 평가한다. 우리는 참조 이미지 간의 유사성을 측정하기 위해 이미지 기반 CLIP 특징을 사용한다.\n' +
      '\n' +
      '그림 5: ** 범주의 질감 전달 결과의 예.** 첫 번째 행에서는 플러시 장난감에서 컵과 의자로 외모를 전달한다. 두 번째 행에서는 머그잔의 특별한 무늬가 곰과 의자로 전달된다. 세 번째 행에서는 입력 소파로부터의 질감이 컵과 곰으로 전달된다.\n' +
      '\n' +
      '그림 6: ** 이전 결과의 예** 질감은 원래 HDR 환경 지도(1열)와 신규 지도(2열, 3열)에 의해 재발한다.\n' +
      '\n' +
      '그랑 렌더링된 이미지. 공정한 평가를 보장하기 위해 참조 이미지와 렌더링된 이미지의 배경은 흰색 색상으로 마스킹된다.\n' +
      '\n' +
      '표 3에서 보는 바와 같이, 우리의 전체 방법은 _w/o ControlNet_ 및 _w/ ControlNet (Depth)_을 제외한 일반 기저부 중에서 가장 높은 유사성 점수를 달성한다. 그림 10에서 보는 바와 같이, 이 두 방법은 목표 형상을 무시하고 기하학에 적응하지 않고 질감을 직접 칠하는 경향이 있다. 따라서 모양에 관계없이 원래 질감을 그려 더 높은 점수에 도달할 수 있습니다. 또한 SDS 결과가 포화되거나 흐릿한 경향이 있으며 입력에서 질감을 회복할 수 없다는 것을 관찰했다. 일반 확산 모델(\\epsilon_{\\파이}\\)에서 LoRA를 잡는 것은 합성된 질감에 무작위 패턴을 도입할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline  & CLIP similarity \\(\\uparrow\\) \\\\ \\hline w/o ControlNet & _0.8394_ \\\\ w/ ControlNet (Depth) & _0.8320_ \\\\ SDS, w/o CFG & 0.8101 \\\\ SDS, CFG \\(100\\) & 0.7983 \\\\ w/o LoRA removed & 0.8110 \\\\ Personalized model as \\(\\phi\\) & 0.8218 \\\\ CFG weight as \\(7.5\\) & 0.8218 \\\\ w/o camera encoder \\(\\rho\\) updated & 0.8267 \\\\ Ours & **0.8296** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '이미지 기반 텍스트링 w.r.t. CLIP 이미지 기반 특징에 대한 표 3: **Ablation 연구**. 비록 _w/o ControlNet_ 및 _w/ ControlNet (Depth)__는 더 높은 유사성 점수를 달성하지만, 전달 결과는 목표 형상을 무시하고 기하학을 추론하지 않고 질감을 직접 페인트리는 경향이 있다. 나머지 절제 방법 중 우리의 전체 방법은 가장 높은 CLIP 유사성 w.r.t 참조 이미지를 달성한다.\n' +
      '\n' +
      '합성된 질감의 그림 8: **다양성****.\n' +
      '\n' +
      '그림 7: ** 기준 방법 간의 비교 a 라텐트-퍼트 [37] 및 TEXTure[53]은 목표 메쉬 기하학과 호환되는 심리스 및 기하학 인식 질감을 합성할 수 있다.\n' +
      '\n' +
      '그림 9: ** 한계** 우리 방법은 텍스쳐에 조명을 굽고, 충분한 입력 관점이 없을 때 야누스 문제를 갖고, 입력으로부터 특수 및 비반복 패턴을 무시할 수 있다.\n' +
      '\n' +
      '## 5 Discussions\n' +
      '\n' +
      '입력 이미지로부터 임의의 형상으로 질감을 전달하는 프레임워크를 제안했다. 우리의 방법은 대부분의 경우 고품질 질감을 전달할 수 있지만 몇 가지 제한 사항이 있습니다. 그림 9는 우리의 방법이 특별한 질감과 불균일한 질감을 타겟 모양으로 전달할 수 없음을 보여준다. 또한, 우리의 방법은 입력 영상에 강한 사각형 하이라이트가 있을 때 조명으로 질감으로 굽는 경향이 있다. 입력 이미지의 관점이 객체 전체를 커버하지 않을 때 얀누스 문제가 나타날 수 있다. 그럼에도 불구하고, 우리는 우리의 방법이 이 도전적인 문제를 해결하는 첫 번째 단계가 될 수 있으며 3D 콘텐츠 생성 커뮤니티에 영향을 미칠 것이라고 믿는다.\n' +
      '\n' +
      '그림 10: ** 삭제 연구***(첫 번째 행)는 정상 지도에 조절망과 함께 최상의 질감 측정 일관성을 가지고 있다. 제어넷이나 깊이 기반 제어넷이 없으면 결과는 질감 측정 오정렬에 시달린다. SDS 손실을 사용하면 흐릿한 질감이 발생합니다. LoRA 모듈이 제거되지 않으면, 결과는 개인화된 확산 모델에서 기존의 텍스처를 제거하는 경향이 있다. 우리의 전체 방법은 입력 외관과 유사한 정확한 질감을 합성할 수 있습니다. (2열) 일반 확산 모델 \\(\\파이\\)을 개인화된 모델로 대체하거나 분류자 안내 척도 \\(7.5\\)를 적용하면 합성된 질감에 일부 무작위 패턴이 나타날 수 있다. 카메라 인코더 \\(\\rho\\)를 동결하면 결과는 전체 방법보다 더 나쁘거나 시끄러울 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Adobe substance 3d. [https://docs.substance3d.com/sat](https://docs.substance3d.com/sat).\n' +
      '* [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d human digitization with shape-guided diffusion. In _SIGGRAPH Asia_, 2023.\n' +
      '* [3] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi. Patch-based optimization for image-based texture mapping. _ACM Trans. Graph._, 36(4):106-1, 2017.\n' +
      '* [4] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. _arXiv preprint arXiv:2304.05868_, 2023.\n' +
      '* [5] G. Cai, K. Yan, Z. Dong, I. Gkioulekas, and S. Zhao. Physics-based inverse rendering using combined implicit and explicit geometries. _Computer Graphics Forum_, 41(4):129-138, 2022.\n' +
      '* [6] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, and Kangxue Yin. Textfusion: Synthesizing 3d textures with text-guided image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4169-4181, 2023.\n' +
      '* [7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16123-16133, 2022.\n' +
      '* [8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. _arXiv preprint arXiv:1512.03012_, 2015.\n' +
      '* [9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niessner. Text2tex: Text-driven texture synthesis via diffusion models. _arXiv preprint arXiv:2303.11396_, 2023.\n' +
      '* [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. _arXiv preprint arXiv:2303.13873_, 2023.\n' +
      '* [11] Zhiqin Chen, Kangxue Yin, and Sanja Fidler. Auv-net: Learning aligned uv maps for texture transfer and synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1465-1474, 2022.\n' +
      '* [12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-?2n2: A unified approach for single and multi-view 3d object reconstruction. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14_, pages 628-644. Springer, 2016.\n' +
      '* [13] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In _Proceedings of the seventh IEEE international conference on computer vision_, pages 1033-1038. IEEE, 1999.\n' +
      '* [14] Ziya Erkoc, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. _arXiv preprint arXiv:2303.17015_, 2023.\n' +
      '* [15] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. _International Journal of Computer Vision_, 129:3313-3337, 2021.\n' +
      '* [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. _Advances In Neural Information Processing Systems_, 35:31841-31854, 2022.\n' +
      '* [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.\n' +
      '* [19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zixin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified framework for 3d content generation. [https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio), 2023.\n' +
      '* [20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from images using monte carlo rendering and denoising. _Advances in Neural Information Processing Systems_, 35:22856-22869, 2022.\n' +
      '* [21] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert. Leveraging 2d data to learn textured 3d mesh generation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 7498-7507, 2020.\n' +
      '* [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.\n' +
      '* [25] Brian Karis and Epic Games. Real shading in unreal engine 4. _Proc. Physically Based Shading Theory Practice_, 4(3):1, 2013.\n' +
      '* [26] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a 3d diffusion model using 2d images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18423-18433, 2023.\n' +
      '* [27] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. _arXiv preprint arXiv:2310.17590_, 2023.\n' +
      '* [28] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid texture synthesis from 2d exemplars. In _ACM SIGGRAPH 2007 papers_, pages 2-es. 2007.\n' +
      '* [29] Vivek Kwatra, Arno Schodl, Irfan Essa, Greg Turk, and Aaron Bobick. Graphcut textures: Image and video synthesis using graph cuts. _Acm transactions on graphics (tog)_, 22(3):277-286, 2003.\n' +
      '* [30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. _ACM Transactions on Graphics_, 39(6), 2020.\n' +
      '* [31] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition. _Advances in Neural Information Processing Systems_, 35:30923-30936, 2022.\n' +
      '* [32] Marc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, et al. The digital michelangelo project: 3d scanning of large statutes. In _Proceedings of the 27th annual conference on Computer graphics and interactive techniques_, pages 131-144, 2000.\n' +
      '* [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.\n' +
      '* [34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 300-309, 2023.\n' +
      '* [35] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2837-2845, 2021.\n' +
      '* [36] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji. X-mesh: Towards fast and accurate text-driven 3d stylization via dynamic textual guidance. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2749-2760, 2023.\n' +
      '* [37] Gal Metzler, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12663-12673, 2023.\n' +
      '* [38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. _arXiv preprint arXiv:2112.03221_, 2021.\n' +
      '* [39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization for meshes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13492-13502, 2022.\n' +
      '* [40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.\n' +
      '* [41] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating textured meshes from text using pretrained image-text models. In _SIGGRAPH Asia 2022 conference papers_, pages 1-8, 2022.\n' +
      '* [42] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Transactions on Graphics (ToG)_, 41(4):1-15, 2022.\n' +
      '* [43] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting triangular 3d models, materials, and lighting from images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8280-8290, 2022.\n' +
      '* [44] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.\n' +
      '* [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Braddury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [46] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien Lucchi. Learning generative models of textured 3d meshes from real-world images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13879-13889, 2021.\n' +
      '* [47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.\n' +
      '* [48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandar Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. _arXiv preprint arXiv:2306.17843_, 2023.\n' +
      '* [49] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. page 107404, 2020.\n' +
      '* [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [51] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '\n' +
      '* [53] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In _ACM SIGGRAPH 2023 Conference Proceedings_, New York, NY, USA, 2023. Association for Computing Machinery.\n' +
      '* [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [55] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T Freeman, and Mark Matthews. Alchemist: Parametric control of material properties with diffusion models. _arXiv preprint arXiv:2312.02970_, 2023.\n' +
      '* [56] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model, 2023.\n' +
      '* [57] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. _arXiv preprint arXiv:2308.16512_, 2023.\n' +
      '* [58] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias Niessner, and Angela Dai. Texturify: Generating textures on 3d shape surfaces. In _European Conference on Computer Vision_, pages 72-88. Springer, 2022.\n' +
      '* [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [61] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and Zhao Dong. Neural-pbir reconstruction of shape, material, and illumination. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023.\n' +
      '* [62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12619-12629, 2023.\n' +
      '* [63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [64] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. _Advances in neural information processing systems_, 29, 2016.\n' +
      '* [65] K. Yan, F. Luan, M. Hasan, T. Groueix, V. Deschaintre, and S. Zhao. Psdr-room: Single photo to scene using differentiable rendering. In _ACM SIGGRAPH Asia 2023 Conference Proceedings_, 2023.\n' +
      '* [66] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. Texture generation on 3d meshes with point-uv diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4206-4216, 2023.\n' +
      '* [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.\n' +
      '* [68] Qian-Yi Zhou and Vladlen Koltun. Color map optimization for 3d reconstruction with consumer depth cameras. _ACM Transactions on Graphics (ToG)_, 33(4):1-10, 2014.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>