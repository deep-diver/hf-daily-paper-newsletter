<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '안정적인 전송 장치(Hourglusion Transformers)와 함께 고액화 고액화 Pixel-Space Image)이 결합됩니다.\n' +
      '\n' +
      'Katherine Crowson\n' +
      '\n' +
      '안드레아스.\n' +
      '\n' +
      'Alex Birch\n' +
      '\n' +
      '펜크 마테.\n' +
      '\n' +
      '다닐 Z. Ka.\n' +
      '\n' +
      'Enrico Shippole\n' +
      '\n' +
      '유사성 AI({}^{1}\\).LMU 뮌헨({}^{2}\\).\n' +
      '\n' +
      '\\({}^{4}\\) 독립 연구원 \\({}^{5}\\) 독립 연구원이다. Katherine Crowson \\(<\\)crowsonkb@gmail.com\\), Stefan Baumann \\(<\\)stefanban@lmu(>\\), 알렉스 비르치 irch \\(<\\)alex@birchlabs.co.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델은 스테이블 디퓨전(Rombach et al., 2022), 임젠(Saharia et al., 2022), eDiff-I(Balaji et al., 2023), 또는 Dall-E 2(Ramesh et al., 2022)와 같은 최첨단 접근법으로 입증된 바와 같이 이미지 생성의 주요 방법으로 등장했다. 그들의 성공은 정적 이미지를 넘어 비디오 및 오디오(Blattmann et al., 2023; Kong et al., 2021)와 같은 다양한 양식으로 확장되며 확산 아키텍처의 다양성을 보여준다. 이러한 최근의 성공은 확장성, 훈련의 안정성 및 생성된 샘플의 다양성에 기인할 수 있다.\n' +
      '\n' +
      '확산 모델의 공간 내에서 CNN 기반(호 et al., 2020), 변압기 기반(피블 및 Xie, 2023; 비오 et al., 2023), CNN 변환기 하이브리드(후게보톰 et al., 2023), 또는 심지어 상태 공간 모델(Yan et al., 2023)에 걸쳐 사용되는 백본 아키텍처에 많은 양의 변화가 있다. 고해상도 이미지 합성을 지원하기 위해 이러한 모델을 스케일링하는 데 사용되는 접근법에 마찬가지로 변화가 있다. 현재의 접근법은 교육에 복잡성을 추가하고 추가 모델 또는 희생 품질을 필요로 한다.\n' +
      '\n' +
      '고해상도 이미지 합성을 달성하기 위한 지배적인 방법으로 라이센트 확산(Rombach et al., 2022)이 집권한다. 실제로는 미세한 디테일(Dai et al, 2023)을 나타내지 못하고 표본 품질에 영향을 미치고 이미지 편집과 같은 응용 분야에서 효용성을 제한한다. 고해상도 합성에 대한 다른 접근법에는 캐스케이드 초해상도(사아리아 et al, 2022), 다중 규모 손실(후게보럼 et al, 2023), 다중 결의에서 투입 및 산출물의 추가(구 등, 2023), 자체 조건 활용 및 근본적으로 새로운 아키텍처 제도의 적응(자브리 et al., 2023)이 포함된다.\n' +
      '\n' +
      '우리의 작업은 백본 개선을 통해 고해상도 합성을 해결합니다. Hourglass Diffusion Transformer(\\(\\overline{\\pi}\\) HDiT라고 부르는 (Nawrot et al., 2022)에 도입된 계층적 구조에 영감을 받은 순수 변압기 구조를 소개합니다. 다양한 건축 개선을 도입함으로써 표준 확산 세트에서 메가픽셀 규모로 고품질의 이미지 생성이 가능한 백본을 얻는다. 이 아키텍처는 \\(128\\·128\\)와 같은 낮은 공간 해상도에서도 디T(Peeuns and Xie, 2023)와 같은 일반적인 확산 변압기 백본(그림 2 참조)보다 세대 품질 경쟁력이 있지만 실질적으로 더 효율적이다. 모델 아키텍처를 다른 표적 해상도에 적응시키는 방법을 사용하여 우리는 정상 확산 변압기 아키텍처의 \\(\\mathcal{O}(n^{2}) 스케일링 대신 이미지 토큰(n\\)의 타겟 수로 계산 복잡도 스케일링을 획득하여 픽셀 공간 고해상도 이미지 합성을 위한 컨볼루션 U-Nets와 계산 복잡도에서 경쟁적인 제1 변압기 기반 확산 백본 구조를 생성한다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 우리는 효율적인 고품질 픽셀 공간 이미지 생성을 위해 변압기 기반 확산 백본 적응 방법을 조사한다.\n' +
      '** Hourglass Diffusion Transformer(\\(\\overline{\\pi}\\)와 함께 압축 비용의 하위 측정값을 갖는 고해상도 픽셀 공간 이미지 생성을 위한 아키텍처(HDiT)를 소개한다.\n' +
      '*는 이 아키텍처가 여전히 더 낮은 해상도에서 고해상도 직접 픽셀 공간 생성과 경쟁적이면서도 진행성 성장 또는 다규모 손실과 같은 고해상도 직접 픽셀 기반 아키텍처와 경쟁하고 있음을 보여준다.\n' +
      '\n' +
      '2개는 회사 관련.\n' +
      '\n' +
      '### Transformers\n' +
      '\n' +
      '변혁자(Vaswani et al, 2017)는 다양한 영역에서 최첨단 아키텍처(오픈AI, 2023, Zong et al, 2022, Zhang et al, 2022, 유 등, 2022, Piergiovanni 등)로 집권한다. 특히, 이들은 비전 공간에서 최대 수십억 개의 매개변수(데히야 등 2023), 그리고 자연어 처리(초헤디 et al., 2023; 페투스 et al., 2022)와 같은 다른 영역에서 그 이상으로 큰 확장성을 제공한다. 형질전환자는 주의 메커니즘을 통해 서열의 모든 요소 간의 상호 작용을 고려한다. 이를 통해 장거리 상호 작용을 효율적으로 학습할 수 있지만 입력 시퀀스의 길이와 2차적으로 계산 복잡성을 스케일링하는 단면을 갖는다.\n' +
      '\n' +
      '확산 이전(Ramesh et al., 2022)의 일부로 저차원 임베딩을 생성하고 압축 이미지 래치(Peeuns and Xie, 2023; Bao et al, 2023; Zao et al, 2023; Gao et al, 2023; Bao et al, 2023; Chen et al, 2023;b)를 생성하기 위해** 트랜스포머 기반 디퓨전모델** 레센트는 확산 모델에 변압기를 적용하여 잠재 확산 설정(Ramesach et al., 2023; Bao et al., 2023; Bao et al, 2023; Bao et al, 2023; Chen et al., 2023; Chen et al, 2023; Chen et al, 2023)에서 압축된 이미지 래치들)을 생성하기 위해 확산 모델에 형질전환체를 적용했으며, Bao et al, 2023, Bao et al, 2023; Chen et al, 2023; Chen et al, 2023)에서 저차원 임베딩을 적용하여 최첨단 성능을 이끌어냈다. 다른 작품들(후게보롬 등, 2023; 징 등은 또한 U-Net(론네버거 등, 2015년)의 최저 수준에서 변압기 기반 아키텍처를 적용하거나 두 건축가(카오 등 2022년)를 혼성화하여 저수준 확산 U-Nets(호 et al., 2020)에 자구 블록을 투입하는 공통된 관행을 넘어서게 된다. 그러나 확산 모델에 대한 대부분의 변압기 아키텍처는 주의 기전의 이차 계산 복잡성으로 인해 (양 등, 2022)에서 볼 수 있듯이 고해상도 픽셀 공간 이미지 합성을 위한 확산 변압기를 적용하기 어렵기 때문에 잠재 확산과 픽셀 공간에서 직접 적용되지 않는다.\n' +
      '\n' +
      '디퓨전트랜스포머(DiT) 아키텍처(Peeuns and Xie, 2023)를 기반으로 두 작품(Gao et al., 2023; 정 등은 2023)도 확산 훈련 과정을 변화시키고 이를 위해 마스킹 작업을 추가하여 객체 부품 간의 더 나은 관계를 학습하도록 장려하였다. 우리는 이러한 추가적인 변화가 이 작업에서 추구하는 목표와 직교하다고 생각한다.\n' +
      '\n' +
      '** 트랜스포머 개선*** 자체 의도 계산 복잡도 척도 서열 길이와 2차적으로 많은 작품들(Liu et al, 2021, 2022, Hassani et al, 2023)은 비전 변압기의 로컬 토큰 세트에 주목만 적용하여 수용 분야를 줄이는 비용에서 이러한 지역 주의 메커니즘의 토큰 수에 대한 선형 계산 복잡성으로 이어졌다.\n' +
      '\n' +
      '최근 전형적인 절대 첨가제, 주파수 기반 위치 임베딩도 조사 중이며 절대 위치 대신 상대적인 위치를 효과적으로 인코딩하는 개선이 제안되었다. 핵 위치 임베딩(Su et al., 2022)은 그러한 예 중 하나로 변압기가 다양한 시퀀스 길이에 유연하게 적응하고 성능을 향상시킬 수 있다.\n' +
      '\n' +
      '변압기 아키텍처, 특히 ViT를 개선하는 이러한 발전에도 불구하고, 이러한 변형은 확산 변압기에 대해 최소한으로 탐구되었다.\n' +
      '\n' +
      '**Hourglass Transformers** Hourglass 아키텍처(Nawrot et al., 2022)는 훈련과 추론을 위한 표준 변환기 모델보다 언어 모델링에 훨씬 더 효율적인 것으로 입증된 변압기의 계층적 구현이다. 이는 트랜스포머의 레이어를 적용하는 과정에 걸쳐 반복적으로 단축된 다음 시퀀스를 반복적으로 재팽창하는 방식으로 수행된다. 추가적으로, 일부 스킵 연결들은 확장 단계들 근처에서 고해상도 정보를 재도입한다. 일반적으로 이 아키텍처는 U-Net(Ronneberger et al., 2015)과 유사하지만 어떤 컨볼루션 레이어도 사용하지 않는다. 이와 관련 (왕 등은 2022년) 영상 복원 작업에 대해서도 유사한 구조의 큰 성과를 보여 탈의 확산 목표와 밀접한 관련이 있다고 볼 수 있다.\n' +
      '\n' +
      '디퓨전 모델과 하이솔루션 이미지 합성 제품입니다.\n' +
      '\n' +
      '확산 모델로 고해상도 이미지 합성을 가능하게 하는 데 대한 광범위한 조사가 있었고, 이는 일반적으로 상자 밖으로 투쟁하는 과제이다. 확산 모델이 다운샘플링된 이미지(호 et al, 2021) 또는 공간적으로 다운샘플링된 "로바흐(Rombach et al., 2022) 표현(Rombach et al., 2022), 다른 확산 모델(호 et al, 2021, Li et al, 2023; Fischer et al., 2023)에 의해 고해상도 이미지가 생성된 고해상도 이미지(Rombach et al., 2022) 또는 기타 생성 모델(Betker et al. Fischer et al., Fischer et al. Fischer et al. Fischer et al. Fischer et al., 2023)에 의해 생성된 고해상도 이미지가 합성곱(Rombach et al., 2022), 다른 생성 모델(Betker et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al. Fischer et al., 2023)에 의해 생성 모델)에 의해 생성 모델)를 학습함으로써 생성 과정을 여러 단계로 이 접근법은 또한 대부분의 변압기 기반 확산 모델(제2.1절)에 의해 사용된다. 최근 작품들은 전반적인 아키텍처를 단순화하기 위해 픽셀 공간에서 고해상도 이미지 합성을 탐색하여 공간 차원을 줄이기 위해 이산 웨이브렛 변환(Jabri et al, 2023), 샘플링 단계(Jabri et al, 2023), 다중 해결 훈련(Gu et al., 2023), 다중 해결 손실(Hoogeboom et al., 2023)을 통해 자체 조건을 포함하는 확산(트레이닝) 과정에 대한 다양한 수정(트레이닝) 과정으로의 다양한 수정(트레이닝)을 사용하여 이미지 데이터를 변환하고, 전체 아키텍처(Hoogeboom et al.,Hoogeboom et al.,Hoogeboom et al.,Hoogeboom et al. 다중 단계 접근법이나 확산 설정의 앞서 언급한 적응(송 등 2021)을 사용하지 않는 심플러 접근법은 보통 사용 가능한 해상도를 완전히 사용하고 전 세계적으로 일관된 샘플 생산으로 투쟁한다.\n' +
      '\n' +
      '그림 3:\\(\\frac{\\pi}{2}\\) HDiT 아키텍처의 하이 레벨 개요, 특히 패치 크기 \\(p=4\\)에서 \\(256^{2}\\)의 입력 해상도에서 이미지넷에 대한 버전. 목표 해상도가 2배 이상 증가할 경우 다른 지역 주의블록이 추가된다. "러프"는 학습 가능한 보간 중량을 갖는 선형 보간을 나타낸다. 모든 \\(\\frac{\\pi}{2}\\) HDiT 블록은 노이즈 레벨과 컨디셔닝(맵핑 네트워크를 사용하여 공동으로 내장)을 추가 입력으로 갖는다.\n' +
      '\n' +
      '## 3 Preliminaries\n' +
      '\n' +
      '### Diffusion Models\n' +
      '\n' +
      '확산 모델은 확산 과정을 역전시키기 위해 학습하여 데이터를 생성한다. 이 확산 과정은 가우시안 노징 과정으로 가장 일반적으로 정의된다. 데이터 분포 \\(p_{\\text{data}}(\\mathbf{x})\\를 감안할 때, 우리는 사전 정의된 단일 광학적으로 증가하는 표준 편차 \\(p(\\mathbf{x}_{\\sigma_{t}};\\sigma_{t})에 의해 제공되는 \\(\\sigma_{t})의 계열과 함께 _forward_ 노징 프로세스를 정의한다. 따라서, \\(\\mathbf{x}_{\\sigma_{t}=\\mathbf{x}_{0}_{0}+\\sigma_{t}}:\\epsilon}(\\mathbf{0},\\mathbf{I})\\)를 사용한다. 디데노징 신경망 \\(D_{\\theta}(D_{\\bf{x}_{\\sigma_{t}},\\sigma_{t})는 \\(\\mathbf{x}_{\\sigma_{t}_{\\})를 통해 \\(\\mathbf{x}_{\\)를 예측하기 위해 훈련된다. 샘플링은 \\(\\mathbf{x}_{T}\\sim\\mathcal{N}\\ason\\mathcal{N}\\mathbf{0}}}\\sigma_{\\text{max}}^{2} \\mathbf{I}\\big{x}\\)에서 시작하여 각 노이즈 수준에서 순차적으로 변성되어 샘플 \\(\\mathbf{x}\\)을 생성함으로써 수행된다. 데오이저 신경망은 평균 제곱 오차 손실로 훈련된다.\n' +
      '\n' +
      '}(\\mathd{t},\\mathobacterium_{f})}}\\mathobacterium_{{t}}.\n' +
      '\n' +
      '아이티(\\lambda_{\\sigma_{t}}\\)는 가중 함수이다. 종종 데오이저는 노이즈 예측기로 매개변수가 된다.\n' +
      '\n' +
      '\\{mathbf{x}}(\\mathbf{sigma_{\\sigma_{\\sigma_{t}},\\sigma_{\\sigma_{\\sigma_{t})=\\frac{\\mathbf{x}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{\\sigma_{t}_{\\sigma_{\\sigma_{\\sigma_{\\sigma_{\\sigma_{\\sigma_{t},\\sigma_{\\sigma_{\\sigma_{t})=\\fathbff{t}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{t}_{\\sigma_{t}_{\\mathbf{t}_{t}_{\\sigma_{t}_{ff{t}_{\n' +
      '\n' +
      '이는 \\(\\epsilon\\)를 예측하는 손실의 제형을 가능하게 한다.\n' +
      '\n' +
      '}\\mathbb{t}}\\mathbb{t}}\\mathb{t}}\\lathda_{\\text{t}}\\lathda_{\\sigma_{\\sigma_{\\sigma{t}}}\\lathda_{\\sigma_{\\sigma_{\\f{t}} <\\mathb{t}} <\\mathb{t}} <\\mathb{t}} <\\mathb{t}} <\\mathb{t{sigma_{f{t}}}\\lathd{sigma_{\\math{sigma_{\\math{t}} <\\math{sigma_{\\math{sigma_{\\math{sigma_{f{f{t}} <\\math{sigma_{\\math{sigma_{f{sigma_{\\math{sigma_{\\math{sigma_{f{f{sigma_{f{f{{t}}}}} <{\\\n' +
      '\n' +
      '이전 작업은 노이즈 예측 목적이 데노징 점수 매칭을 통해 점수를 배우는 것과 밀접한 관련이 있음을 관찰함으로써 확산 모델 제형을 점수 기반 생성 모델로 연결했다.\n' +
      '\n' +
      '**D확산 개선*** 우리의 모델에 의해 채택된 확산 관행에 대한 눈에 띄는 최근의 개선을 설명한다. EDM(카라스 등 2022년)에서 확산 프레임워크에 대한 몇 가지 변형이 성능을 향상시키는 것으로 나타났다. 가장 중요한 것은 입력 및 출력 크기가 노이즈 레벨보다 일정하게 유지되도록 데오저 신경망의 입력 및 출력에 전제 조건을 적용한다. 구체적으로 데오이저 신경망을 그대로 다시 작성합니다.\n' +
      '\n' +
      'F_{\\text{in}}(c_{\\text{t})=c_{\\text{t}}(\\sigma_{t})\\mathbf{t}}(\\sigma_{\\sigma_{\\sigma_{t})\\mathbf{t}}(\\sigma_{\\sigma_{\\sigma_{t}})\\mathbf{t}},\\sigma_{\\sigma_{t}},\\sigma_{f{t}},\\sigma_{\\sigma_{f{t}},\\sigma_{\\sigma_{f{t}},\\sigma_{\\sigma_{f{t}},\\sigma_{\\sigma_{f{t}}_{\\sigma_{f{t}}}}_{\\sigma_{\\sigma_{\\sigma_{f{t}}}}_{\\sigma_{\\sigma_{\\sigma_{f{t}}}}}\n' +
      '\n' +
      '변조 기능은 (Karras et al., 2022)에 나와 있습니다.\n' +
      '\n' +
      '(Hang et al., 2023)에서 입증된 또 다른 최근의 접근법은 모델 융합을 개선하기 위해 클램핑된 신호 대 잡음 비율(SNR)을 기반으로 다양한 소음 수준에서 손실 가중치를 적응시킨다. EDM 제형에서는 사용된 손실 가중치를 사용한다.\n' +
      '\n' +
      '모셸{SNR}}.\n' +
      '\n' +
      '민-SNR 손실 가중치가 \\(\\mathbf{x}_{0}\\)-파라미터화에 적용되기 때문에, EDM 사전 조건 파라미터화를 설명하기 위해 \\(c_{\\text{out}}^{-2}(\\sigma) 인자가 통합된다.\n' +
      '\n' +
      '또 다른 개선은 높은 해상도를 위한 소음 스케줄의 적응이었다. 원래 낮은 해상도(32x32 또는 64x64)에 대해 설계된 일반적으로 사용되는 소음 스케줄이 고해상도로 소음을 충분히 첨가하지 못하는 것은 이전에 관찰되었다(Hoogeboom et al, 2023). 따라서, 더 높은 해상도로 적절한 노이즈를 추가하기 위해 기준 저해상도 노이즈 스케쥴에서 소음 일정을 이동 및 보간할 수 있다.\n' +
      '\n' +
      '증류 전이물질 4개.\n' +
      '\n' +
      '확산트랜스포머(피블 및 Xie, 2023) 및 기타 유사한 작품(제2.1절 참조)은 생성 품질(Gao et al, 2023; 정 등 2023) 측면에서 사전 작업을 능가하는 잠재 확산(Rombach et al., 2022) 설정에서 변성 확산 오토인코더로서의 인상적인 성능을 보여주었다. 그러나 계산 복잡성이 \\(n=w\\cot h\\) 모양의 이미지에 대해 2차적으로 증가(H\\mathcal{O}(n^{2}) 채널로 확장성을 제한하여 고해상도 입력에서 훈련 및 실행 모두에 매우 비싸게 만들고, 매우 큰 패치 크기가 생성된 샘플(Peeuns 및 Xie, 2023) 품질에 해로운 것으로 밝혀진 한, 변형이 충분히 작은 차원으로 공간적으로 압축된 래치들로 효과적으로 제한됨)에 의해 변환기를 효율적으로 제한한다는 사실(Cao et al.\n' +
      '\n' +
      '우리는 디퓨전트랜스포머(Pee Travel and Xie, 2023), Hourglass et al., Hourglass Diffusion Transformers) - Hourglass Diffusion Transformers(\\(\\mathfrak{T}\\) HDiT) - 고품질 픽셀 공간 이미지 생성을 가능하게 하고 \\(\\mathcal{O}(n^{2})\\ 대신 \\(\\mathcal{O}(n^{2})\\의 계산 복잡도 스케일링(\\mathcal{O}(n^{O}(n^{2})\\)의 계산 복잡도 스케일링으로 더 높은 해상도 스케일링(\\mathcal{O}(n^{O})\\)의 계산 복잡도 스케일링(\\mathcal{O}(n)\\)의 계산 복잡도 스케일링으로 보다 높은 해상도에 효율적으로 적응될 수 있는 계산 복잡도 스케일링으로 더 높은 결의에 효율적으로 적응할 수 있는 고품질 픽셀 공간(\\mathcal{O}(n)\\로 더 높은 해상도로 더 높은 해상도에 효율적으로 적응할 수 있도록 하는 고품질 픽셀 공간 이미지 생성을 가능하게 하는 보다 높은 해상도에 효율적으로 이것은 이 모델을 메가픽셀 해상도에서 픽셀 공간 생성을 지시하기 위해 스케일링하는 것조차 실행 가능해지고, 이는 5절에서 최대 \\(1024\\ 1024\\)의 해상도에서 모델에 대해 입증한다는 것을 의미한다.\n' +
      '\n' +
      '###는 중요성의 계층적 자연을 조사한다.\n' +
      '\n' +
      '자연 영상은 계층(Saremi and Sejnowski, 2013)을 나타낸다. 이는 이미지 생성 과정을 계층적 모델로 매핑하게 되는데, 이는 앞서 확산 모델에서 일반적으로 사용되는 U-Net 아키텍처(론네버거 등, 2015)에서 성공적으로 적용되었지만 확산 변압기(피블 및 Xie, 2023; Bao et al., 2023)에서 일반적으로 사용되지 않는다. 변압기 백본에 대한 이미지의 이러한 위계적 특성을 레버리지하기 위해 우리는 변압기 백본의 고수준의 구조에 이미지를 포함한 다양한 유형에 효과적인 것으로 밝혀진 시간 유리 구조(노트로트 등 2022)를 적용한다. 모델의 1차 해상도에 기초하여, 우리는 가장 안쪽 레벨이 \\(16\\t 16\\) 토큰을 가지도록 계층에서 수준의 수를 선택한다. 저해상도 수준은 고해상도 수준을 따르는 것과 관련된 저해상도 정보와 정보를 모두 처리해야 하므로 이에 대한 더 큰 숨겨진 차원을 선택한다. 인코더 측의 모든 레벨에 대해, 우리는 \\(2\\t 생후 2\\) 토큰을 PixelUnShuffle(Shi et al 2016)를 사용하여 공간적으로 하나로 병합하고 디코더 측에서 역을 수행한다.\n' +
      '\n' +
      '**Skip Merging Mechanism**와 같은 아키텍처의 중요한 고려 사항은 최종 성과에 유의한 영향을 미칠 수 있기 때문에 스킵 연결의 병합 메커니즘이다(Bao et al., 2023). 이전 비계층적 U-ViT(Bao et al., 2023)는 표준 U-Net(Ronneberger et al., 2015)과 유사하게 연결된 기반 스킵 구현을 사용하고 있으며, 이는 다른 옵션보다 훨씬 더 나은 것으로 밝혀졌지만, 이 계층적 아키텍처에 더 나은 성능을 발휘하기 위한 추가 스킵을 발견했다. 스킵마다 둘 사이에 제공되는 정보의 유용성이 크게 다를 수 있으므로 특히 매우 깊은 계층에서 선형 보간(lerp) 계수 \\(f\\)를 학습하여 스킵과 업샘플링된 분기의 상대적 중요도를 학습하는 모델을 가능하게 한다.\n' +
      '\n' +
      '\\[\\mathbf{x}_{\\mathrm{merged}}}=f\\cdot\\mathbf{x}(\\mathbf{x.\\;lerp}:\\cdot\\mathbf{x}_{\\mathrm{x}_{\\mathrm{x}_{\\mathrm{x}_{\\mathrm{x}_{\\mathbf{x}_{\\mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}_{mathrm{x}}_{mathrm{x}}_{mathrm{x}}_{mathrm{x}}_{mathrm{x}}}}}}}\n' +
      '\n' +
      '헬럭슨 전달자.\n' +
      '\n' +
      '우리의 기본 변압기 블록 설계(그림 4의 DiT와 비교하여 다운로드)는 일반적으로 LLaMA(Touvron et al., 2023)가 사용하는 블록에서 영감을 받아 최근 고품질의 언어 생성이 매우 가능한 것으로 나타난 변압기 아키텍처이다. 조절을 가능하게 하기 위해 RMS 정규 연산이 사용하는 출력 스케일을 적응적으로 만들고 클래스 및 확산 시간 단계에서 조건화되는 매핑 네트워크를 가지고 있어 예측할 수 있다. 디T와 달리 (적응) 출력 게이트를 사용하지 않고 자기 의도 및 FFN 블록 모두의 출력 예측을 제로스로 초기화한다. 변압기 모델에 위치 정보를 액세스할 수 있도록 DiT 및 U-ViT와 같은 공통 확산 변압기 아키텍처는 학습 가능한 부가 위치 인코딩을 사용한다. (피블 및 Xie, 2023; Bao et al., 2023) 새로운 서열 길이에 외삽하는 모델의 일반화 및 능력을 향상시키는 것으로 알려져 있으며, 2D 이미지 데이터에 대해 회전 위치 임베딩(RoPE)(Su et al., 2022)의 적응으로 이를 대체하며(호 et al., 2019)과 유사한 접근법을 따르고 각 축에서 작동하도록 인코딩을 분할하여 각각의 공간 축에 대한 RoPE를 각각 쿼리 및 키의 별개의 부분에 적용하여 각 축에 동작한다. 우리는 또한 이 인코딩 방식을 쿼리 및 키 벡터의 절반에만 적용하고 나머지는 성능에 유익하도록 수정하지 않는다는 것을 발견했다. 전반적으로, 우리는 정상적인 첨가제 위치 임베딩을 적응된 RoPE로 대체하면 융합이 향상되고 패치 아티팩트를 제거하는 데 도움이 된다는 것을 경험적으로 발견했다. RoPE를 적용하는 것 외에도 이전에 (Liu et al., 2022)1에서 사용된 코사인 유사도 기반 주의 메커니즘을 사용하고 있으며, 비전 변압기(Dehghani et al., 2023)에 대해 100억 매개변수 척도로 유사한 접근법이 입증되었다는 점에 주목한다.\n' +
      '\n' +
      '부츠 1: 대수 공간에서 헤드당 척도를 매개변수화하지 않고 선형 공간에서 학습하여 안정성을 향상시킨다는 것을 알 수 있다. 자세한 내용은 부록 C를 참조하세요.\n' +
      '\n' +
      '피드포워드 블록(DT와의 비교를 위해 그림 5 참조)은 DiT와 같은 출력 게이트가 아닌 GEGLU(Shazeer, 2020)를 사용하여 조절 신호가 컨디셔닝 대신 데이터 자체에서 나와 FFN의 두 번째 레이어 대신 첫 번째 레이어에 적용된다.\n' +
      '\n' +
      '그림 4: 변압기 블록 아키텍처의 비교 및 DiT(피블즈와 Xie, 2023)에서 사용하는 것이다.\n' +
      '\n' +
      '그림 5: 우리의 점별 피드포워드 블록 아키텍처의 비교 및 DiT(피블즈와 Xie, 2023)가 사용하는 구조이다.\n' +
      '\n' +
      '고위험에 대한 마찰을 효율적으로 할 수 있다.\n' +
      '\n' +
      '시간 글라스 구조는 우리가 다양한 해상도로 이미지를 처리할 수 있게 한다. 우리는 저결의에서 글로벌 자치를 사용하여 일관성을 달성하며, 세부 사항을 향상시키기 위해 모든 더 높은 결의에서 지역 자기의도(Liu et al, 2021, 2022a, Hassani et al, 2023)를 사용한다. 이것은 관리 가능한 양으로 2차 복잡성 글로벌 주의 필요성을 제한하고 분해능의 추가 증가에 대한 선형 보완 스케일링을 즐긴다. 공교롭게도 복잡성은 \\(\\mathcal{O}(n)\\)로(부록 A) w.r.t 픽셀 수 \\(n\\)이다.\n' +
      '\n' +
      '국소적인 자기 의사를 위한 전형적인 선택은 이전 확산 모델(Cao et al, 2022, Li et al, 2022)에서 사용하는 바와 같이 윈도우 주의(Liu et al, 2021, 2022a)이다. 그러나 우리는 근린 주의(Hassani et al., 2023)가 실제에서 훨씬 더 잘 수행된다는 것을 발견했다.\n' +
      '\n' +
      '글로벌 자기의도2를 적용할 최대 해상도는 데이터셋(장거리 정합성이 필요한 작은 특징이 이성에 주목할 만큼 충분히 커지게 되는 크기)과 과제(장거리 관계가 수용되기 위해서는 보존될 필요가 있는 가장 작은 특징)에 의해 결정되는 선택이다. 특히 낮은 해상도(예: \\(256^{2}\\)에서 일부 데이터 세트는 더 적은 수준의 글로벌 주목을 받아 일관된 생성을 허용한다.\n' +
      '\n' +
      '뿌리 2: FFHQ-\\(1024^{2}\\) 실험을 위해 \\(16^{2}\\)에서 1개, \\(32^{2}\\)에서 1개 수준의 전 세계적인 주의를 적용한다. 임니지넷-\\(128^{2}\\) 및 \\(256^{2}\\)의 경우, 우리는 이미지 생성의 낮은 해상도로 인해 단일 수준의 \\(16^{2}\\)가 글로벌 주의를 요구하는 사전 작품(호 et al., 2020; Hoogeboom et al, 2023; Nichol 및 Dhariwal, 2021)과 같은 것을 발견했다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '제안된 \\(\\frac{\\pi}{2}\\) HDiT 아키텍처를 조건부 및 무조건적인 이미지 생성, 건축적 선택(섹션 5.2)에 걸쳐 평가하며, 메가픽셀 픽셀 공간 이미지 생성(섹션 5.3) 및 대규모 픽셀 공간 이미지 생성(섹션 5.4)을 모두 평가한다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '달리 언급되지 않은** 훈련***는 어떤 종류의 잠재 표현 없이 RGB 픽셀에 직접 \\(128\\t 128\\)의 해상도로 이미지넷(Deng et al., 2009)에서 계급적 모델을 훈련시킨다. 우리는 \\(5\\ 10^{-4}\\)의 일정한 학습률과 \\(\\lambda=0.01\\)의 체중 붕괴를 이용하여 모든 모델을 AdamW(Loshchilov and Hutter, 2019)로 훈련시킨다. 우리는 일반적으로 층화된 확산 타임스팟 샘플링과 함께 400k 단계(피블링(피블 및 시, 2023a)에 대해 \\(256\\)의 배치 크기로 훈련하고 달리 명시되지 않는 한 다이프아웃을 사용하지 않는다. I\\(128\\·128\\)에서 소규모 이미지넷 트레이싱의 경우, 우리는 어떤 증강도 적용하지 않는다. 작은 데이터 세트의 실행을 위해 우리는 (카라스 등, 2020)와 유사한 비봉쇄 증강 방식을 적용한다. 공통 확산 모델 훈련 실습 및 (피블 및 Xie, 2023a) 후, 우리는 또한 \\(0.9999\\)의 붕괴로 모델 가중치의 지수 이동 평균(EMA)을 계산한다. 우리는 모든 평가 및 생성된 샘플에 이 EMA 버전의 모델을 사용하고 50단계 DPM++(3M)(Lu et al., 2023; Crowson, 2023) SDE 샘플링을 사용하여 샘플링을 수행한다. 자세한 내용은 표 6을 참조하십시오.\n' +
      '\n' +
      '**Diffusion*** 우리는 사전 조건을 포함하여 (Karras et al., 2022)의 일반적인 훈련 설정을 조정하고 지속적인 확산 제형을 사용한다. 추론 중 분류기가 없는 안내(호·살리만스, 2021)를 가능하게 하기 위해 수업조건 데이터셋에 대한 훈련 중 시간의 수업조절정보 \\(10\\%\\)를 떨어뜨린다.\n' +
      '\n' +
      '생성 이미지 모델에 대한*** 평가** 공통 관행에 따라 50k 샘플에 대해 계산된 프로체트 인셉션 거리(FID)(허셀 등 2017)를 보고한다. FID를 계산하기 위해 우리는 일반적으로 사용되는 구현(2021년 Dhariwal and Nichol)을 사용한다. 또한 건축의 고해상도 버전에 대한 FLOP를 포함하여 주요 절제 연구에 대한 절대 및 점근 계산 복잡성을 모두 보고한다.\n' +
      '\n' +
      '### 건축의 효과.\n' +
      '\n' +
      '건축 선택의 효과를 평가하기 위해 확산을 위한 시간 유리 구조의 기본 구현으로 시작하여 최종 아키텍처가 고품질 메가픽셀 이미지 합성을 효율적으로 수행할 수 있도록 하는 변화를 반복적으로 추가하는 절제 연구를 수행한다. 우리는 **A**, **B1**,....., **E**로 절제 단계를 표시하고 표 1의 특징 구성과 실험 결과를 보여준다. 우리는 또한 다양한 환경에서 DiT(피블레스 및 Xie, 2023a) 모델을 훈련하여 공정한 비교를 가능하게 하는 바젤린 **R1-R4** 세트를 제공한다.\n' +
      '\n' +
      '우리는 보통 이 비교(승인)에 DiT-B 규모 모델을 사용한다. dT에 대한 130M 매개변수, 상대적으로 낮은 훈련 비용으로 인해 \\(\\frac{\\pi}{2}\\) HDiT를 위한 승인 105M~120M, 절제 단계에 따라 HDiT를 120M으로 훈련하고 \\(128^{2}\\)의 해상도로 픽셀 공간 이미지넷(Deng et al, 2009)과 패치 크기 4로 훈련시킨다.\n' +
      '\n' +
      '** 기본*** 서로 다른 세트에서 DiT의 4 버전을 훈련하여 표 1 **R1***의 기저부로 공정한 비교를 제공하지만 공식 디T 구현(피블 및 Xie, 2023b)을 직접 사용하지만 VAE 잠재 계산 단계를 생략하고 스케일링 및 분산을 데이터에 맞게 조정한다. 디티는 픽셀 공간(피블즈와 Xie, 2023a)에 직접 적용될 수 있기 때문에 다른 변화는 이루어지지 않았다. 트레이너 및 손실 가중 방식의 영향을 평가하기 위해 원래 DiT 모델을 직접 포장하고 트레이너3으로 개질하는 포장지를 구현하며 이 실험의 결과는 **R2**로 표시된다. **R3***는 포장된 DiT 모델을 하이퍼파라미터 일치 단일 수준의 절제 단계 **A***로 대체하고 원래 코드베이스로 학습된 원래 DiT의 성능과 일치시킨다. 이 설정에서 또한 절제 단계 **E***와 같이 **R4**에 소프트-민-snr 손실 가중치를 추가하여 최종 모델과 공정한 비교가 가능하다. \\(256\\) 및 \\(512\\ 55\\)의 결의에서 동일한 아키텍처에 대한 계산 비용도 보고된다. 우리의 모델의 경우 해상도의 매 두 배는 섹션 4.1에 따라 하나의 로컬 주의 블록(촉매 단계 **A**를 제외하고, 전 세계적으로)을 추가하는 것을 포함한다.\n' +
      '\n' +
      'Footnote 3: 픽셀 공간 DiT **R2**는 최적화기 매개변수를 제외하고 나머지 연마물과 동일한 설정으로 훈련되었으며, 처음에 최적화기를 사용하여 이 모델을 훈련했지만 원래 매개변수보다 불안정하고 더 나쁘다는 것을 발견했기 때문에 비교를 위해 (Peeuns and Xie, 2023)의 원래 매개변수를 사용했다.\n' +
      '\n' +
      '***Base Hourglass St구조***세부 **A**는 고해상도 수준과 선형 스킵 인터폴이 낮은 간단한 시간 유리 구조이며 RMS 정규이지만 GEGLU가 없는 블록과 모든 수준에서 완전한 글로벌 자기 의사를 가진 블록의 기본 구현이다. 여기에 간단한 부가 위치 인코딩이 사용된다. 이 간단한 아키텍처라도 추가 변경 없이 시간 유리 구조로 인해 픽셀 공간에서 작동하는 유사하게 크기의 디T(피블 및 Xie, 2023) 모델보다 이미 훨씬 더 저렴한(전방 패스당 FLOP의 30%)이다. 이것은 절제 단계에서 DiT 기저부에 비해 증가된 FID의 비용으로 나온다.\n' +
      '\n' +
      '** 지역의도결정론**Next, 최저해상도를 제외한 모든 수준에 현지 관심을 더합니다. 우리는 시파트-윈도(SWin)(Liu et al., 2021; 2022)의 두 가지 옵션(**B1***)의 주의를 평가하고 이전에 확산 모델(Cao et al, 2022; Li et al., 2022)) 및 이웃(Hassani et al., 2023)의 관심(**B2***)에 사용되었다. 두 가지 모두 저해상도 규모(128\\)에서도 FLOP의 작은 감소를 초래하지만 가장 중요한 것은 \\(\\mathcal{O}(n^{2})\\에서 \\(\\mathcal{O}(n)\\)로 염기 분해능을 감소시켜 실질적인 스케일링이 훨씬 더 높은 해상도를 가능하게 한다. 두 변이체 모두 이 감소된 국소 주의 표현성으로 인해 증가된 FID로 고통받는다. 그럼에도 불구하고 이러한 변화는 근린 주의에서 상당히 덜 뚜렷하여 SWin 주의의 공통 선택에 비해 이 경우 명확하게 우수한 선택이다.\n' +
      '\n' +
      '*** 피드포워드 활성화** 3단계에서는 데이터 자체가 피드포워드 네트워크의 표준 GeLU와 비교하여 피드포워드 블록의 출력 조절에 영향을 미치는 GEGLU(Shazeer, 2020)를 사용하여 절제한다. 이전 작업(Touvron et al., 2023)과 유사하게 GEGLU 작업으로 인해 숨겨진 크기의 효과적인 변화를 설명하기 위해 \\(4\\cdot d_{\\mathrm{model}}\\)에서 \\(3\\cdot d_{\\mathrm{model}}\\)로 숨겨진 차원을 감소시킨다. 우리는 이러한 변화가 출력 폭의 반감기를 설명하기 위해 피드포워드 블록의 선형 투영의 폭을 증가시켜야 하기 때문에 계산 비용의 약간의 증가 비용에서 FID를 크게 향상시킨다는 것을 발견했다.\n' +
      '\n' +
      '***위치 코딩**Next, **D***에서 RoPE(Su et al., 2022)의 2d 축 적응으로 표준 부가 위치 임베딩을 대체하여 호르글라스 DiT 백본 구조를 완료한다. 이것은 FID를 더욱 향상시킵니다. 추가적인 이점으로서 RoPE는 절제 연구가 그것에 대해 테스트하지 않지만 추가 위치 임베딩 이외의 다른 해상도로 훨씬 더 나은 외삽을 가능하게 해야 한다.\n' +
      '\n' +
      '** Loss Weeria***에서는 표준 \\(\\frac{1}{\\sigma^{2}}\\) 손실 가중치(호 et al., 2020; 송 et al., 2021)를 저소음 수준에 대한 SNR 가중치와 비교하여 손실 가중치를 감소시키는 적응된 min-snr(Hang et al., 2023) 손실 가중치 방법으로 대체한다. 이것은 실질적으로 FID를 개선하여 픽셀 공간 확산을 위한 적절한 훈련 설정과 결합될 때 \\(팔버라인{\\mathbf{\\Xi}}\\) HDiT의 효과를 보여준다.\n' +
      '\n' +
      '**Skip 구현***이 주요 절제 연구에 이어서 절제 단계 **E**에 기초한 상이한 스킵 구현보다 절제된다. 우리는 깊은 위계들을 학습할 때 특히 도움이 되는 것으로 실증적으로 발견된 학습 가능한 선형 보간(lerp)과 업샘플링 및 스킵 데이터가 직접 추가된 표준 첨가제 스킵, 그리고 데이터가 먼저 연결된 후 점적 컨볼루션을 사용하여 원래 채널 카운트에 투영되는 연결 버전을 비교한다. 이 절제 결과는 표 2에 나와 있다.\n' +
      '\n' +
      '고진화 서브셀-스페이스 이미지 합성.\n' +
      '\n' +
      '이 섹션에서는 고해상도 픽셀 공간 이미지 합성을 위한 모델을 훈련합니다. 이전 작업에 이어 이러한 높은 해상도로 이미지 생성을 위한 표준 벤치마크 데이터세트인 FFHQ-\\(1024^{2}\\)(Karras et al., 2021)에서 훈련한다. 이전 작업에는 셀프 컨디셔닝과 같은 트레이드가 필요합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Skip Implementation** & **FID\\(\\downarrow\\)** \\\\ \\hline Concatenation (U-Net (Ronneberger et al., 2015)) & 33.75 \\\\ Addition (Original Hourglass (Nawrott et al., 2022)) & 28.37 \\\\ Learnable Linear Interpolation (**Ours**) & **27.74** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>는 스미프 정보 추신 양자화(Jabri et al., 2023), 다중 규모 모델 아키텍처(Gu et al., 2023), 또는 다중 규모 손실(Hoogeboom et al., 2023)에서 이러한 고해상도에서 고품질 생성을 가능하게 하는 것이다. 우리는 우리의 모델이 고품질 생성을 가능하게 하기 위해 이러한 트릭을 필요로 하지 않는다는 것을 발견하므로(생성된 샘플의 품질을 더욱 증가시킬 것으로 예상한다)하고, 따라서 이미지의 중복성(Hoogeboom et al, 2023) 증가에 따라 각 단계에서 SNR을 적응시키는 것을 제외하고, 그것들을 포함하지 않고 모델을 훈련시킨다. 그림 6의 모델 샘플에서 볼 수 있듯이, 우리의 모델은 분류기가 없는 지침 없이 정밀한 디테일로 날카로운 사진을 생성하기 위해 사용 가능한 해상도를 적절하게 활용하는 고품질 전 세계적으로 일관된 샘플을 생성할 수 있다.\n' +
      '\n' +
      '우리는 정량적 비교를 위해 표 3의 상태 대응물에 대한 모델을 벤치마킹한다. 특히 NCSN++(송 등 2021) 베이스라인에 대한 사전 계산 메트릭을 사용할 수 없기 때문에 제공된 체크포인트4를 사용하여 독립적으로 계산합니다. 우리는 우리의 모델이 이 기준선을 정량적으로 질적으로 능가한다는 것을 발견했다(그림 10 및 그림 11 참조 및 NCSN++ 기준치 모두에서 미확인 샘플에 대해 그림 11 참조). 특히, 우리의 모델은 대칭적 특징을 가진 얼굴을 생성하는 데 탁월하고 NCSN++는 눈에 띄는 비대칭성을 나타낸다. 더욱이, \\(\\mathbf{\\underline{\\pi}}\\) HDiT는 가용 해상도를 효과적으로 침출하여 날카롭고 미세한 상세 이미지를 생성하며, 이는 종종 흐릿한 샘플을 생성하는 NCSN++ 모델에 대한 주목할만한 개선이다. 우리의 모델이 HiT(Zhao et al., 2021), 또는 StyleSwin(Zhang et al., 2022)와 같은 고해상도 변압기 GAN과 FID에 대해 경쟁력이 있지만 StyleGAN-XL(Sauer et al., 2022)과 같은 최첨단 GAN과 동일한 FID에 도달하지 못한다는 것을 발견했다. (Stein et al., 2023)에서 강조된 확산 모델의 샘플보다 GAN에 의해 생성된 샘플에 대한 편향으로 알려져 있는 FID 메트릭이 우리의 모델의 인상적인 성능을 강조하여 달성된 친밀도가 확산 모델에 대한 이 특정 메트릭의 하한선에 접근할 수 있음을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|c c c||c c} \\hline \\hline\n' +
      '**Configuration** & **FID\\(\\downarrow\\)** & **GFLOP@\\(128^{2}\\)\\(\\downarrow\\)** & **Complexity\\(\\downarrow\\)** & **GFLOP@\\(256^{2}\\)** & **GFLOP@\\(512^{2}\\)** \\\\ \\hline \\multicolumn{6}{l}{**Baselines (R1 uses 250 DDPM sampling steps with learned \\(\\sigma(t)\\) as in the original publication instead of 50-step DPM++ sampling)**} \\\\\n' +
      '앤앤앤티앤앤비(애스칼{O}(n^{2}) & 657&6,341 \\\\(\\mathcal{O}) & 6,341 \\\\-B/4(Pee beon \\&Xie, 2023) & 42.03 & 106 & \\\\-B/4(Peeuns \\－R1** & DiT-B/4) & 42.03 & 6,341 \\-B/4(Peeied \\ & DiT-B****R1** & DiT-B/4(Peeied \\&Xie, 2023) & 0.03*R1**R1* & DiT-B/4(P^ \\ & DiT-B·디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디앤디\n' +
      '앤657 & 6,341 \\\\(\\mathcal{O}) & 6,341 \\\\(n^{O},n^{2* & **R1**) + 트레이너(소프트-min-snr)\n' +
      '**R3** & **R2** + our basic blocks \\& mapping network & 42.49 & 106 & \\(\\mathcal{O}(n^{2})\\) & 657 & 6,341 \\\\ \\hline\n' +
      '**R4** & **R3** + soft-min-snr & 30.71 & 106 & \\(\\mathcal{O}(n^{2})\\) & 657 & 6,341 \\\\ \\hline \\hline \\multicolumn{6}{l}{**Ablation Steps**} \\\\\n' +
      '앤114 & 50.76&32 &\\(Imathcal{O}(n^{2}) & 114 & 114 & 1,060 \\\\ Diffusion Hourglass(섹션 4.1:n^{2})*****\n' +
      '**B1** & **A** + Swin Attn. (Ku et al., 2021) & 55.93 & **29** & \\(\\mathcal{O}(\\mathcal{O},\\mathbf{n})\n' +
      '**B2** & **A** + 근린 Attn. (Hassani et al., 2023) & 60 & 184 \\\\*29** &\\(\\mathcal{O}(\\mathcal{O}:\\mathbf{n})\n' +
      '*** + **B2** + GeGLU(Ihazeer, 2020) & 44.36 &\\(\\mathcal{31}) & \\(\\mathcal{O}(\\mathbf{n}) & 65& 198 \\\\]\n' +
      '**D** & **C** + Axial RoPE (Section 4.2) & 41.41 & \\(\\underline{31}\\) & \\(\\mathcal{O}(\\mathbf{n})\\) & 65 & 198 \\\\ \\hline\n' +
      '**E** & **D** + soft-min-snr (Appendix B) & **27.74** & \\(\\underline{31}\\) & \\(\\mathcal{O}(\\mathbf{n})\\) & 65 & 198 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '디T-B/4(Peeied \\& Xie, 2023)와 유사한 시간 유리 확산 변압기의 줄무늬다운 구현에서 시작하여 건축 선택의 표 1: Ablation. 또한 전체 모델을 훈련하는 데 사용하지만 건축의 일부를 고려하지 않는 부드러운 분-snr 손실 가중치를 사용하는 추가 선택을 준수합니다. 또한 다양한 DiT-B/4 기반 모델이 기저부로 작용하기 위한 결과를 제시한다. 훈련 결과 외에도 표준 해상도 의존성 모델 적응을 포함하여 여러 해상도로 순방향 패스당 계산 비용을 보고한다.\n' +
      '\n' +
      '그림 6: 85M 매개변수 FFHQ-\\(1024^{2}\\) 모델의 샘플이다. 가장 크게 줌으로 보았어요.\n' +
      '\n' +
      '대규모 이미지넷 제품입니다.\n' +
      '\n' +
      '앞서 실험(5.3절 참조)에서 볼 수 있듯이 \\(\\frac{\\pi}{2}\\) HDiT는 고 충실도 고해상도 샘플을 생성하는 데 좋은 성능을 보인다. 또한 대규모 생성 능력을 평가하기 위해 클래스조건 픽셀 공간 이미지넷-\\(256^{2}\\) 모델도 훈련합니다. 우리는 이 작업에 대한 하이퍼파라미터 튜닝을 수행하지 않았으며 557M 매개변수에서 이 모델이 많은 최첨단 모델보다 훨씬 작다는 점에 주목한다. 고해상도 실험에서 우리의 방법론과 정렬하여 비표준 훈련 트릭 또는 확산 변형을 적용하는 것을 자제하고(후게보럼 등 2023년)와 일치하여 분류기가 없는 지침을 적용하지 않고 결과를 비교하여 상자 외 비교를 강조했다.\n' +
      '\n' +
      '우리는 그림 7의 샘플을 보여주고 표 4의 최첨단 확산 모델과 정량적으로 비교하며, 질적으로 우리의 모델이 이 작업에 대한 높은 충실도 샘플을 쉽게 생성할 수 있음을 발견했다. 기준 모델 DiT에 비해, 우리의 모델은 고해상도 래치 대신 픽셀 공간에서 동작했음에도 불구하고 실질적으로 더 낮은 FID 및 더 높은 IS를 달성한다. 우리의 모델은 다른 단일 단계 픽셀 공간 확산 모델에 비해 ADM과 같은 단순한 U-Net 기반 모델을 능가하지만 샘플링(RIN) 동안 자가 조건을 사용하거나 실질적으로 더 큰 모델(단순 확산, VDM++)에 의해 능가된다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 작업은 이전 변압기 기반 백본보다 더 효율적으로 고해상도로 스케일링되는 확산 모델을 가진 이미지 생성을 위한 계층적 순수 변압기 백본인 \\(\\frac{\\pi}{2}\\) HDiT를 제시한다. 이 아키텍처는 해상도에 관계없이 이미지를 동일하게 처리하는 대신 목표 해상도에 적응하고, 고해상도에서 로컬 현상을 국소적으로 처리하고, 위계성의 저해상도 부분에서 글로벌 현상을 별도로 처리한다. 이는 \\(\\mathcal{O}(n^{2})\\ 대신 더 높은 해상도로 사용될 때 \\(\\mathcal{O}(n)\\)으로 계산 복잡도 척도(\\mathcal{O})를 갖는 아키텍처를 산출하여 변압기 모델의 우수한 스케일링 특성과 U-Nets의 효율 사이의 격차를 해소한다. 이 아키텍처가 자기 조건이나 다중 해결 아키텍처와 같은 트릭을 필요로 하지 않고 메가픽셀 규모의 픽셀 공간 확산 모델을 가능하게 하고 상당히 더 효율적인 픽셀 공간 설정과 잠재 확산 세트의 변압기와 비교할 때 작은 해상도에서도 다른 변압기 확산 백본과 경쟁적이라는 것을 보여준다.\n' +
      '\n' +
      '본 논문에서 유망한 결과를 감안할 때, 우리는 \\(\\frac{\\pi}{2}\\) HDiT가 효율적인 고해상도 이미지 합성에 대한 추가 연구의 기초를 제공할 수 있다고 믿는다. 우리는 무조건적이고 계급적인 이미지 합성에만 초점을 맞추고 있지만 \\(\\frac{\\pi}{2}\\) HDiT는 특히 아키텍처 스케일링을 사용하여 초해상도, 텍스트-이미지 생성 및 오디오 및 비디오와 같은 다른 양식 합성과 같은 다른 생성 작업에서 효율성과 성능 이득을 제공하는 데 잘 도움이 될 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Method** & **Params** & **L\\(\\times\\)RS** & **Steps** & **FID\\(\\downarrow\\)** & **IS\\(\\uparrow\\)** \\\\ \\hline _Latent Diffusion Models_ & & & & & \\\\   LDM-4 (Rombach et al., 2022) & 400M & 214M & 250 & 10.56 & 209.5 \\\\   DIT-XL(2 Peebles \\& Xie, 2023a) & 675M & 1.8B & 250 & 9.62 & 121.5 \\\\   U-VI-H2 (Bao et al., 2023a) & 501M & 512M & 50.2 & 6.58 & - \\\\   MDiT-XL(2 Gao et al., 2023) & 676M & 1.7B & 250 & 6.23 & 143.0 \\\\   MaskDHY(Zheng et al., 2023) & 736M & 28 & 40.2 & 5.69 & 178.0 \\\\ \\hline _Single-Stage Pixel-Space Diffusion Models_ & & & & & \\\\   iDDM (Nichol and Buraiwal, 2021) & - & - & 250 & 32.50 & - \\\\   ADM (Dharival and Nichol, 2021) & 554M & 507M & 1000 & 10.94 & 101.0 \\\\   RN (Jabri et al., 2023) & 410M & 614M & 1000 & 4.51 & 161.0 \\\\   simple diffusion (Hoogeboom et al., 2023) & 2B & 1B & 512 & 2.77 & 211.8 \\\\   VDM++ (Kingman and Gao, 2023) & 2B & - & 256:2 & 2.40 & 225.3 \\\\  **2** HDiT (**Ours**) & 557M & 742M & 50:2 & 6.92 & 135.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 이미지넷-\\(256^{2}\\)에 대한 우리의 결과를 문헌의 다른 모델과 비교한 것이다. (Hoogeboom et al, 2023)에 이어 분류기가 없는 지침 없이 결과를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Method** & **FID\\(\\downarrow\\)** \\\\ \\hline _Diffusion Models_ & \\\\   NCSN++ (Song et al., 2021) (5k samples) & 53.52 \\\\   HDiT-85M (**Ours**, 5k samples) & 8.48 \\\\  **2** HDiT-85M (**Ours**) & 5.23 \\\\ \\hline _Generative Adversarial Networks_ & \\\\   HiT-B (Zhao et al., 2021) & 6.37 \\\\   StyleSwin (Zhang et al., 2022a) & 5.07 \\\\   StyleGAN2 (Karras et al., 2020b) & 2.70 \\\\   StyleGAN-XL (Sauer et al., 2022) & 2.02 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: FFHQ 1024 \\(표본) 1024에 대한 우리의 결과를 문헌에서 다른 모델과 비교했다. 달리 명시되지 않는 한 50k 샘플은 FID 계산에 사용된다.\n' +
      '\n' +
      '그림 7: 그룹-조건 557M-파라미터 이미지넷-\\(256^{2}\\) 모델의 샘플에는 분류기가 없는 지침이 없다.\n' +
      '\n' +
      '7개의 미래 작업을 합니다.\n' +
      '\n' +
      'HDiT는 픽셀 공간 확산 모델의 맥락에서 연구되었지만 미래의 작업은 효율성을 더욱 높이고 다메가픽셀 이미지 해상도를 달성하기 위해 잠재 확산 설정에서 \\(\\mathbbm{x}\\) HDiT를 적용하거나 자체 조건(Jabri et al, 2023) 또는 진행성 훈련(Sauer et al., 2022)과 같은 직교 트릭을 적용하여 생성된 샘플의 품질을 더욱 향상시킬 수 있다.\n' +
      '\n' +
      '5.4절에서 제시된 대규모 이미지넷 교육에 대한 결과는 유망하고 많은 최첨단 아키텍처에 경쟁적으로 수행되지만 하이퍼파라미터 튜닝 및 아키텍처 스케일링으로 실질적인 추가 개선이 가능할 것으로 기대한다. 향후 작업은 이 건축의 가능성을 완전히 실현하는 방법을 탐색할 수 있다.\n' +
      '\n' +
      '지역 주의 블록이 있는 아키텍처는 또한 효율적인 확산 슈퍼해상도 및 확산 VAE 특징 디코딩 모델에 유용할 수 있으며 모든 수준이 지역 주의만을 수행하기 위해 설정된다(이러한 응용 프로그램을 위한 샘플에 이미 글로벌 구조가 존재하기 때문에 글로벌 주의 블록이 필요하지 않아야 한다), 임의의 해상도로 스케일링할 수 있는 효율적인 변압기 기반 모델을 훈련시킬 수 있다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '우리는 종이 쓰기 과정에서 광범위한 입력에 대해 _uptightmoose_와 Tao Hu에 감사를 표합니다. AB는 슈퍼컴퓨터에 대한 가우스 센터(Gauss 센터)와 줄리치 슈퍼컴퓨터센터(JSC)에서 슈퍼컴퓨터 JUWELS 부스터 및 JURECA에 대한 존 폰 니만 컴퓨터 연구소(NIC)가 부여하는 예산을 계산할 수 있도록 LAION eV를 완전히 인정한다. ES는 실험을 수행하기 위해 리소스에 대한 안정성 AI를 격렬하게 인정한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* 살지 등은 (2023) 발지, Y, 나, S, 황, X, Vahdat, A, 송, 장, Q, 크리스, K, 아칸타, M, 아필라, 라인, 스, 카탄자로, B, 카라스, T, 류, M. >야. eDiff-I: 2023년 전문가 데오이소머의 앙상블이 있는 이미지 대 이미지 융합 모델이다.\n' +
      '*바오 등 (2023a) 비오, F, Nie, S, Xue, K, Cao, Y, Li, C, Su, H 및 Zhu, J. 모두 디퓨전 모텔의 A ViT 백본이다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2023a에 대한 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 바오(2023b) Bao, F, Nie, S, Xue, K, Li, C, 푸, S, 왕, Y, 쿠, Y, Cao, Y, Su, H 및 Zhu, J. 원트랜스포머 피브레이션은 스코일에서 멀티-모델 디퓨젼의 모든 취약점을 포함한다. _국제기계학습회의(ICML)_. JMLR.org, 2023b.\n' +
      '* 베커 등 (2023) 베커, J, 고, G, 징, 브루크, L, 브로크, 왕, 리, 루, 오양, 주앙, J, 이, 주, Y, 마나스라, W, Dhariwal, P, 추, C, 자오, Y, 라메쉬, A. 베터캡션으로 이미지 세대를 개선한다. 기술 보고서 2023.\n' +
      '* 블라트만 등은 (2023) 블라타만, A, 롬바흐, R, Ling, H., Dockhorn, T., 김, S. W, 피들러, S. 및 Kreis, K. 라텐스: 하이-솔루션 비디오 합성물과 라텐트 디퓨전 모델을 정렬하세요. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2023년에 _IEEE/CVF 콘퍼런스에서.\n' +
      '*카오 등은 (2022)호, 왕, 자, 르, 테, 키, X, 첸, 야, 야, Y, 장, L. 비전 전송기를 2022년 디퓨전 학습자로 탐색한다.\n' +
      '* 첸 등은 (2023a) 천, J, 유, J, Ge, C, 야오, L, Xie, E, 우, Y, 왕, Z, 쿠옥, J, 루노, P., H. 및 리, Z. 픽스아트-\\(\\alpha\\): 2023a 사진학적 텍스트-영상 합성용 디퓨전트랜스포머의 마지막 훈련.\n' +
      '* 첸 등은 S, S, Xu, M, Ren, J, Cong, Y, He, S, Xie, Y, 신하, A, 루노, P, 샤랑, T 및 페레스-루아, J-M. 제트론: 딥을 이미지 및 비디오 세대 2023b를 위한 디퓨전 트랜스포머로 전환한다.\n' +
      '오우, 아이즈, 스웨이트, 오에니, 베드, 시르네, 아, 다에, 스리, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아, 아. PaLM: path 고속도로와 함께 모델링하는 스칼링 언어 모델. __athing 언어 모델. 기계학습연구 저널(JMLR)_ 2023.\n' +
      '* 크로우손(2023) 크리슨 K. 19 2023년 URL[https://github.com/crowsonkb/kN-확산/cc49cf61822844343f8e2/k_확산/샘플링py#L656] (https://github.com/crowsonkb/k-확산/k_확산/샘플링.py#L656), (https://github.com/k-확산/cc49cf6182222864343f8e3M) SDE.com/k-적응/k-확산/cc49cf8e89434343f8e2/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/k_적응/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/k_확산/샘플링.py#L656) SDE.com/cc49cac38384343f8e2/k_확산/k_확산/샘플링.py#L656.py#L656\n' +
      '4, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 3, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 4, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n' +
      '\n' +
      '마이스트릭, 아이, 베이어, 베이어, 베이어, 베이어, 베네, 베이어, 알라베, 베이어, 아, 스미스터, 에비네, 헤센, 유, 푸네, 미라, 라이스, 스물스, 스물리, 우, 엘에니(2023) 알라. 비전 트랜스포머를 2,200만 파라미터로 스칼링합니다. _국제기계학습회의(ICML)_. JMLR.org, 2023.\n' +
      '* 덩 등은 (2009) 덩, J, 동, W, 소셔, R., Li, L. -J, Li, K 및 Fei-Fei, L. 이미지넷: 대규모 계층적 이미지 데이터베이스입니다. 2009년 컴퓨터 비전 및 패턴 인식(CVPR)__IEEE 콘퍼런스에서.\n' +
      '* Dhariwal and Nichol(2021) Dhariwal, P. 및 Nichol, A. Q. Diffusion Models Beat GAN은 이미지 합성에서였다. 2021년 신경정보처리시스템(NeurIPS)__Conference에서.\n' +
      '* 페투스 등은 (2022) 페투스, W, 조프, B 및 셰저, N. 스위치 트랜스퍼러: 심플하고 효율적인 믹스를 가진 3,000만 개의 파라파라미터 모달로 스칼링. 기계학습연구 저널(JMLR)_ 2022.\n' +
      '* 피셔 등 (2023) 피셔, J S, 구이, M, 마, P, 스트라케, N, 바만, S. A 및 오머, B. 보스토링 라텐트 디퓨전스는 2023년 플로우 매칭으로 나타났다.\n' +
      '* 가오 등은 (2023) 가오, S, 주, P, 청, M. -M과 옌, S. 마스크 디퓨전 트랜스포머는 스트롱 이미지 합성기입니다. 1998년 10월 컴퓨터 비전(ICCV)__IEEE/CVF 국제 회의.\n' +
      '* 구는 (2023)구, J, Z하이, S, 장, Y, 솔리드, J 및 나, N. 마리오쉬카 디퓨전 모델 2023.\n' +
      '* 항(2023) 항, T, 구, S, Li, C, Bao, J, Chen, D, Hu, H, Geng, X 및 Guo, B. B. 민-SNR 위팅 전략을 통한 효율적인 디퓨전 트레이닝입니다. 1998년 10월 컴퓨터 비전(ICCV)__IEEE/CVF 국제 회의.\n' +
      '* 하사니 등은 A(2023) 하사니, A, 월턴, S, Li, J, Li, S, Shi, H. 근린 고의 전환기이다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2023년 6월 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 헨리 등은 A(2020) 헨리, A, 다멸종, P. R, 파와르, S. S 및 Chen, Y. 변압기에 대한 쿼리키 정규화입니다. 코손, T, He, Y, Liu, Y에서. 컴퓨팅 언어학 협회의 _ 찾음: EMNLP 2020_ 2020년 11월이다.\n' +
      '* Heusel et al. (2017) Heusel, M., 람소어, H., Unterthiner, T., Nessler, B 및 Hochreiter, S. GAN은 두 시간-스케일 업데이트 룰이 로컬 나시 평형에 연결한다. 가이온, 아이, 럭스부르크, U V, 벤지오, S, 월차, H, 페르거스, R, 비슈완아탄, S 및 가넷, R. (종), 2017년 신경정보처리시스템(NeurIPS)__Conference.\n' +
      '*호앤살리만스(2021) 호, J, 살리만스, T. 클래스 프리 디퓨전 가이드입니다. 2021년 딥 유전체 모델 및 다운스트림 애플리케이션_의 _NeurIPS 2021 워크숍에서입니다.\n' +
      '*호 등은 (2020)호, J, 칼치브렌너, N, 바이세네널, D, 살리만, T. 2019년 다차원 전환기에 대한 비판입니다.\n' +
      '*호 등은 (2020)호, J, Jain, A 및 Abbeel, P. 덴오징 세확산 안정화 모델이다. 신경정보처리시스템(NeurIPS)_ 2020년 _Conference에서.\n' +
      '*호 등은 (2021)호, J, 사하라리아, C, 찬, W, 플렛, D. J, 노루지, M 및 살리만스, T. 2021년 하이 충실도 이미지 세대에 대한 카시드의 디퓨전 모델입니다.\n' +
      '* Hoogeboom 등은 (2023) Hoogeboom, E,희크, J 및 Salimans T. 심플 디퓨전: 고등 해결 이미지를 위한 엔드 투 엔드 디퓨전. _국제기계학습회의(ICML)_. JMLR.org, 2023.\n' +
      '* 자브리 등은 (2023) 자브리, A, 플렛, D 및 첸, T. 이터티브 세대인 2023년에 사용할 수 있는 적응 컴퓨팅입니다.\n' +
      '* 징 등은 (2023) 징, X, 창, Y, 양, Z, Xie, J, 트리판타필로폴로스, A 및 Schuller, B W U-디퓨전 비전 트랜스포머, 2023. TTS.\n' +
      '* 카라스 등은 (2020a) 카라스, T, 아피카, M, 헬스트렌, J, 메인, S, 리히텐, J 및 아라, T. 리미티드 데이터와의 훈련 유전자 역학 네트워크입니다. 신경정보처리시스템(NeurIPS)_, 2020a에 대한 _Conference.\n' +
      '* 카라스 등은 (2020b) 카라스, T, Laine, S, Aittala, M, Hellsten, J, Lehtinen, J 및 Aila, T. 스타일GAN의 이미지 품질을 분석하고 개선합니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2020b에 관한 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 카라스 등은 (2021) 카라스, T, 루인, S 및 아라, T. 진보적 적대적네트웍스를 위한 스타일 기반 유전체 건축법 __. IEEE 전환은 2021년 패턴 분석 및 기계 정보(TPAMI)_ 디코딩에 관한 것이다.\n' +
      '* 카라스 등은 (2022) 카라스, T, 아칸타, M, 아릴라, T 및 메인, S. 디퓨전 기반 유전체 모델의 디자인 공간을 설명하십시오. 2022년 신경정보처리시스템(NeurIPS)__Conference에서.\n' +
      '* 킹마 & 가오(2023) 킹마, D. P. 및 Gao, R. 확산 대상물을 단순 데이터 발효, 2023을 사용하여 ELBO로 이해한다.\n' +
      '* 킹마 등 (2014)* 콩 등 (2021) 홍콩, Z, Ping, W, 황, J, Zhao, K) 및 카탄자로, B. 디프웨이브: 오디오 합성용 버라이어티 디퓨전 모델. 2021년 _국제 학습 발표회의(ICLR)__.\n' +
      '* 리는 (2022) 리, R, Li, W, 양, Y, Wei, H., 장, J 및 Bai, Q. Swinv2-Imagen: 2022년 텍스트 대 이미지 세대에 대한 계층 비전 트랜스포머 디퓨전 모델입니다.\n' +
      '* 류 등은 (2021) 류, Z, Lin, Y, Cao, Y, Hu, H, Wei, Y, 장, Z, Lin, S, Guo, B. Swin Transformer: 시프티드 윈도를 이용한 계층적 비전 트랜스포머이다. 2021년 컴퓨터 비전(ICCV)__IEEE/CVF 국제 회의.\n' +
      '* 류 등은 (2022a) 류, Z, Hu, H, Lin, Y, Y, Xie, Z, Xie, Z, Ning, Y, Cao, Y, Z, 동, L, Wei, F 및 구, B. Swin Transformer V2: 스칼링 업 역량 및 해결이다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2022a에 대한 _IEEE/CVF 콘퍼런스에서.\n' +
      '* Liu 등은 (2022b) Liu, Z, Hu, H, Lin, Y, Y, Y, Xie, Z, Wei, Y, Ning, J, Cao, Z, Z, 동, L, Wei, F 및 Guo, B. Swin Transformer v2, 2022b. URL[https://github.com/bla45bc2f2del45bb9fc24/modin_swin_swin_swin_swin#L156][https://github.com/마이크로소프트/마이크로소프트/마이크로소프트/Swin-Transformer/2c103f2del45bc2f2f2f2.py#L156][https://github.com/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/Swin-Transin_swin_swin_swin_swin_swin_swin_swin/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/마이크로소프트/Swin-형질전환기/Swin-형질전환체_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin_swin\n' +
      '* Loshchilov & Hutter(2019) Loshchilov, I. 및 Hutter, F. 105all Weight Decay Regularization. 2019학년도 _국제학습설명회(ICLR)_.\n' +
      '* 루 등 (2023) 루, C, 저우, Y, 바오, F, 첸, J, 리, C 및 주, J DPM-솔버++: 2023 Diffusion Probabilistic Models의 유도 샘플링을 위한 패스트솔버이다.\n' +
      '* 나와롯 et al.(2022) 나와롯, P., 테워프스키, S., 티롤스키, M., 카이저, L., 우, Y, 스제그리디, C. 및 미칼레세키, H. 계층적 트랜스포머는 효율적인 언어 모델을 사용한다. 컴퓨팅 언어학 협회의 _ 찾음: 2022년 7월 NAACL 2022_.\n' +
      '* 니콜 & 다라리왈(2021) 니콜, A. Q. 및 Dhariwal, P. 임프로듀싱 변성 확산 확률 모델. _국제기계학습회의(ICML)_. PMLR, 2021.\n' +
      '* 오픈AI(2023) 오픈AI입니다. GPT-4 기술 보고서. 기술 보고서 2023.\n' +
      '* 피글 & 제이(2023) 피블, W. S, S. 트랜스포머가 있는 충전 가능한 디퓨전 모델입니다. 2006년 10월 컴퓨터 비전(ICCV)__IEEE/CVF 국제 회의.\n' +
      '* 피글 & 제이(2023) 피블, W. S, S. 서류 검색/도표, 2023b. URL[https://github.com/facebookres 연구이다/DiT/tree/ed81ce2229091fd4ecc9a22364] (https://github.com/facebookres 연구이다/DiT/tree/ed81ce2229091fd4ecc9a22364)\n' +
      '* Piergiovanni 등 (2023) Piergiovanni, A, 쿠오, W 및 앙헬로바, A. Rethinking Video ViTs: 공동 이미지 및 비디오 학습을 위한 Sparse Video T튜브입니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2023년에 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 라메쉬 등은 (2022) 라메쉬, A, 다라리왈, P, 니콜, A, 추, C 및 첸, M. CLIP 라테트가 있는 계층적 텍스트-조건적 이미지 세대는 2022년이다.\n' +
      '* 람바흐 et al. (2022) 롬바흐, R., 블라트만, A, 로렌츠, D, 에서, P 및 Ommer, B. 고진화 이미지 합성 a 라텐트 디퓨전 모텔이다. 컴퓨터 비전 및 패턴 인식(CVPR)_ 2022년 6월 _IEEE/CVF 콘퍼런스에서.\n' +
      '* Ronneberger et al. (2015) Ronneberger, O, Fischer, P 및 Brox, T. U-Net: 바이오메디컬 이미지 세그먼트화를 위한 콘볼루션네트웍스입니다. 2015년 _ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입(MICCAI)_에서.\n' +
      '*사아리아 등은 (2022)사아리아, C, 찬, W, Saxena, S, Li, L, Whang, J, Denton, E, Ghasemipour, R, Gonti조 로프, R, Ayan, T, Ho, J 및 노루지, M. 딥 언어 이해가 있는 사진학적 텍스트 대 이미지 디확산 모델입니다. 오, A, A, A, 벨그레브, D, 조, K. 신경정보처리시스템(NeurIPS)_ 2022년 _Conference.\n' +
      '세레미 & 세존스키(2013) 사레미, S. 자연 이미지의 계층적 모델과 규모 불변의 출처인 T. J. 계층적 모델. __ 자연 이미지의 근원적 모델. 국립과학원, 110(8):3071-3076, 2013년 2월 ISSN 1091-6490. 도이: 10.1073/pnas.1222618110 URL[http://dx.org/10.1073/pnas.1222618110](http://dx.doi.org/10.73/pnas.1222618110])을 모집한다.\n' +
      '* Sauer et al. (2022) Sauer, A, 슈바르츠, K 및 거저리, A. StyleGAN-XL: 스칼링 StyleGAN을 대형 디버스 Datasets에 할당한다. _ACM SIGGRAPH 2022 콘퍼런스 결정___ACM SIGGRAPH 2022 콘퍼런스 식별. 컴퓨터 기계 연합, 2022년.\n' +
      '* 셰제러(2020) 샤제러 N. GLU 변종은 2020년 트랜스포머를 개선합니다.\n' +
      '* Shi 등은 (2016) Shi, W, 카바렌로, J, Huszar, F, Totz, J, Aitken, A, Bishop, R., R., Rueckert, D 및 왕, Z. 효율적인 서브 픽셀 컨솔루션 신경망을 이용한 실시간 싱글 이미지 및 비디오 슈퍼 해결입니다. 컴퓨터 비전 및 패턴 인식(CVPR)_, 2016년에 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 송(2021) 송, 예, 소l-디키슈타인, J, 킹마, D. P, 쿠마르, A, 에르몬, S 및 포올, 스토코스틱 차이식을 통한 B. 스코어 기반 유전 모델링입니다. 2021년 _국제 학습 발표회의(ICLR)__.\n' +
      '(2023) 스테인, G, Cresswell, J C, Hosseinzadeh, R, Hosseinzadeh, R, Ross, B L, Villecroze, V, Liu, Z, Caterini, A. L, Taylor, J. T. 및 Loaiza-Ganem, G.는 생성 모델 평가 메트릭의 결함 및 확산 모델의 불공정한 처리를 2023.\n' +
      '* Soder 등은 (2021)Su, J, 루, Y, 판, S, 무라다하, A, 원, B, Y, 류, Y. 전: 2022년 로타리 위치 커버가 있는 강화 트랜스포머입니다.\n' +
      '* 투브론 등은 (2022) 투브론, H., 라브릴, T., 아이자카드, G., 마르티넷, X, 라차오, M. A, 라크로닉스, T, 로지레, B, 고달, N, 함크로, E, 아즈하, F, 로드리게스, A, 자울린, A, 그레이브, E, Lample, G. LLaMA: 오픈 및 효율적인 재단 언어 모델입니다. 기술 보고서 2023.\n' +
      '* 바소와이 등은 (2017) 바소와이, A, 샤제르, N, 파마르, N, 우즈코레이트, J, 존스, L, Gomez, A. N, 카이저, L. u, 폴로신, I. 신경정보처리시스템(NeurIPS)_ 2017년 _Conference에서.\n' +
      'U. URL[https://github.com/sucidrains/flash-cosine-sim- shifting/tree/6f17f29a979a8bcab247979c65b7740523] 왕, P. 플래시 코신 유사성표시. (https://github.com/sucidrains/flash-cosine-sim-attention/tree/6f17f29a979a8bcab2479c65b7740523)\n' +
      '* 왕 등은 왕(2022) 왕, Z, Cun, X, Bao, J, 저우, W, 류, J, Li, H. Uformer: 이미지 복구를 위한 U-Shaped Transformer이다. 컴퓨터 비전 및 패턴 인식(CVPR)_ 2022년에 _IEEE/CVF 콘퍼런스에서.\n' +
      '* 옌 et al.(2022) 옌, J N, 구, J, Rush, A. M. Diffusion Models, 2023.\n' +
      '* 양 등은 (2022) 양, X, 시, S. M, Fu, Y, Zhao, X, 지, S. 귀하의 ViT는 2022년 하이브리도 판별 생성 디퓨전 모델입니다.\n' +
      '*유 등은 (2022)유, 자, 왕, Z, 바슈다반, V, 예웅, L, 세이다호시니, M, 우, Y. CoCa: 콘트롤티브 케이터링은 이미지-텍스트 재단의 모델. __ 프로세싱티브 에디터는 이미지-텍스트 재단 모델이다. 기계학습연구(TMLR)_ 2022년 거래.\n' +
      '* 장(2022a) 장, B, 구, S, 장, B, 비오, J, 첸, D, 위, F, 왕, Y, 구, 보스타일스윈은 고진화 이미지 세대에 대한 트랜스포머 기반 GAN이다. 컴퓨터 비전 및 패턴 인식(CVPR)_, pp. 11304-11314, 2022년 6월에 IEEE/CVF 회의의 _검토에서.\n' +
      '*장 등은 장(2022b) 장, Y, 진, J, 박, D S, 한, W, 치우, C-C, 판, 르, Q. V, 우, Y. 자동 스피치 인지, 2022b를 위한 반지도학습의 임수를 작성하세요.\n' +
      '* Zhao et al.(2021) Zhao, L., 장, Z., Chen, T., 메타카스, D 및 장, H. 임프로듀싱 GAN을 위한 트랜스포머이다. 2021년 신경정보처리시스템(NeurIPS)__Conference에서.\n' +
      '* 정(2023) 정, H, Nie, W, Vahdat, A 및 아나난드쿠마르는 2023년 탈의 트랜스포머와 함께 디퓨전 모텔의 마지막 훈련이다.\n' +
      '*동 등은 Z, Z, 송, G, Y, Liu 등이 있다. 협업 하이브리드 조정 훈련이 있는 DETR입니다. 2022년 컴퓨터 비전(ICCV)__IEEE/CVF 국제 회의.\n' +
      '\n' +
      'HDiT 적용\n' +
      '\n' +
      '확산 모델(피블과 Xie, 2023, Bao et al., 2023)을 포함한 전통적인 비전 변압기에서 이미지 크기와 관련하여 점근적 계산 복잡성은 토큰/픽셀 수 \\(n\\) 및 임베딩 차원 \\(d\\)를 갖는 \\(\\mathcal{O}(n^{2}d)로 스케일링되는 자기 의도 메커니즘에 의해 지배된다. 피드포워드 블록과 주의 투영 헤드는 차례로 \\(\\mathcal{O}(nd^{2})로서 스케일이다.\n' +
      '\n' +
      'Hourglass Diffusion Transformer 아키텍처의 경우 U-Nets(Ronneberger et al., 2015)과 함께 사용된 이전 접근법과 유사하게 다양한 표적 결의에 대한 아키텍처를 조정한다. 우리의 아키텍처는 일반적으로 전체 패치 해상도에서 최외곽 레벨이 작동하는 다수의 계층적 레벨로 나뉘며, 각 추가 레벨은 축당 공간 해상도의 절반으로 동작한다. 단순화를 위해 먼저 2개의 권한의 제곱 해상도로 비용을 충당할 것입니다.\n' +
      '\n' +
      '특정 해상도를 위한 아키텍처를 설계할 때, 우리는 자연 이미지에 대해 일반적으로 \\(16^{2}\\) 또는 \\(16^{2}\\) 및 \\(32^{2}\\)에서 작동하는 하나 또는 두 개의 글로벌 선택 계층 수준을 포함하는 데이터세트 의존성 _core_ 아키텍처에서 시작한다. 주변에 많은 지역 주의 수준이 있습니다. 이 핵심은 고정된 해상도에서만 작동하기 때문에 전체 모델의 점근적 계산 복잡성에 영향을 미치지 않는다.\n' +
      '\n' +
      '**Asymptotic 복합체 스칼링*** 이 아키텍처가 더 높은 해상도에 적응되면 공유 파라미터가 있는 추가 지역 주의 수준이 추가되어 \\(16^{2}\\)에서 작동하는 가장 안쪽 수준을 유지한다. 이는 이미지 토큰의 수를 \\(\\mathcal{O},\\log(n))\\로 하는 계층 척도에서의 수준의 수가 있음을 의미한다. 공간적 하향 샘플링으로 인해 토큰 카운트가 선형적으로 스케일링되지 않는 우리의 모델의 유일한 부분은 토큰 카운트가 \\(\\mathcal{O}(n\\log(nd)인 반면, 지역 주의 레이어의 복잡성은 \\(\\mathcal{O})로 인해 계층에서 각 수준의 해상도 감소로 인해 직관적으로 4개의 요인이 될 수 있다.\n' +
      '\n' +
      '\\[\\sum_{l=1}^{\\log_{4}(n)-\\log_{4}(\\mathrm{res}_{\\mathrm{core}})}\\frac{nd}{4^{l -1}}.\\]\n' +
      '\n' +
      '수익률 \\(n\\)를 측정하고 \\(m=l-1\\)를 정의하면 된다(m=l-1\\)\n' +
      '\n' +
      '\\[n\\cdot\\sum_{m=0}^{\\log_{4}(n)-\\log_{4}(\\mathrm{res}_{\\mathrm{core}})-1}d\\cdot \\bigg{(}\\frac{1}{4}\\bigg{)}^{m},\\]\n' +
      '\n' +
      '공통 비율이 1보다 작은 (컷오프) 기하계열은 기하학적 계열이 수렴함에 따라 점근적 복잡성에 영향을 미치지 않아 추가 수준 \\(\\mathcal{O}(n)의 국소적 자기고의 누적 복잡성을 초래한다는 것을 의미한다. 따라서 \\(\\mathcal{O}(n)\\보다 더 나쁜 규모의 다른 부분이 없기 때문에 표적 분해능이 증가함에 따라 호르글래스 디퓨전 트랜스포머 구조의 전반적인 복잡성은 \\(\\mathcal{O}(n)\\이다.\n' +
      '\n' +
      '목표 해상도가 축당 2개의 전력보다 작은 요인에 의해 증가되면 아키텍처가 적응되지 않는다. 이는 이러한 중간 해상도의 경우 서로 다른 스케일링 행동이 우세함을 의미한다. 여기서 이 경우 수가 변하지 않는 지역 주의 수준의 비용은 이전과 같이 \\(\\mathcal{O}(n)\\를 갖는 규모이지만, 전 지구적 주의 수준은 결의에 따라 2차적인 비용 증가를 야기한다. 그러나 해상도가 더 증가함에 따라 새로운 수준이 추가되어 전 세계 주의 블록이 원래 값으로 작동하고 \\(\\mathcal{O}(n)\\의 전체 점근 스케일링 행동을 유지한다.\n' +
      '\n' +
      '부록 B Soft-Min-SNR Loss.\n' +
      '\n' +
      '민-SNR 손실 가중치(Hang et al., 2023)는 확산 모델 교육을 개선하는 최근 도입된 훈련 손실 가중치 방식이다. SNR 가중 체계(IH(\\mathbf{x}\\in[-1,1]^{h\\ross c}\\))에 적응한다.\n' +
      '\n' +
      '\\[w_{\\text{SNR}}(\\sigma)=\\frac{1}{\\sigma^{2}} \\tag{7}\\]\n' +
      '\n' +
      '\\의 SNR(\\gamma=5\\)에 닫아라.\n' +
      '\n' +
      '\\[w_{\\text{Min-SNR}}(\\sigma)=\\min\\bigg{\\{}\\frac{1}{\\sigma^{2}},\\gamma\\bigg{\\}}. \\tag{8}\\]\n' +
      '\n' +
      '우리는 정상 SNR 가중치와 절편 구간 간의 전환을 매끄럽게 하는 약간 변형된 버전을 사용한다.\n' +
      '\n' +
      '\\[w_{\\text{Soft-Min-SNR}}(\\sigma)=\\frac{1}{\\sigma^{2}+\\gamma^{-1}}. \\tag{9}\\]\n' +
      '\n' +
      '아이티(토끼 사마귀마미)와 \\(팔새마미금)의 경우, 이는 Min-SNR과 일치하는 반면, 두 섹션 사이의 원활한 전환을 제공한다.\n' +
      '\n' +
      '실제로 우리는 또한 하이퍼파라미터 \\(\\gamma=5\\)를 \\(\\gamma=4\\)에서 \\(\\gamma=4\\)로 변경한다.\n' +
      '\n' +
      '그림 8과 같이 min-snr과 부드러운-min-snr 모두에 대한 결과 손실 가중치를 샘플링하면 손실 가중치가 상당히 매끄러운 전환을 제외하고 min-snr과 동일함을 알 수 있다. 분-snr에 비해 부드러운 분-snr의 절제는 또한 우리의 손실 가중 방식이 모델에 대한 개선된 FID 점수(표 5 참조)로 이어짐을 보여준다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:15]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '\'## 부록 F Our FFHQ-\\(1024^{2}\\)\'입니다.\n' +
      '\n' +
      '그림 10: 85M \\(터버라인{\\pi}\\) HDiT FFHQ-\\(1024^{2}\\) 모델의 정제되지 않은 샘플을 보여준다.\n' +
      '\n' +
      '그림 11: NCSN++(송 et al., 2021) FFHQ-\\(1024^{2}\\) 기준선 모델의 수정된 참조 샘플을 참조했다.\n' +
      '\n' +
      '삼각형## 부록 H Our ImageNet-\\(256^{2}\\)\n' +
      '\n' +
      '그림 12: 557M \\(\\frac{\\pi}{2}\\) HDiT ImageNet-\\(256^{2}\\) 모델의 무작위로 분류된 무작위 샘플이다.\n' +
      '\n' +
      '그림 13: \\(\\overline{\\pi}\\) HDiT-557M 이미지넷-\\(256^{2}\\) 모델의 수정되지 않은 무작위 그룹 조건 샘플을 보여준다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>