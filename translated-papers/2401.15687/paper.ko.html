<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Media2Face: Multi-Modality Guidance를 이용한 Co-speech Facial Animation 생성\n' +
      '\n' +
      '청청 Zhao\\({}^{1,2}\\) Pengyu Long\\({}^{1,2}\\) Qixuan Zhang\\({}^{1,2}\\) Dafei Qin\\({}^{2,3}\\) Han Liang\\({}^{1}\\)\n' +
      '\n' +
      'Longwen Zhang\\({}^{1,2}\\) Yingliang Zhang\\({}^{4}\\) Jingyi Yu\\({}^{1\\ddagger}\\) Lan Xu\\({}^{1\\ddagger}\\)\n' +
      '\n' +
      '홍콩대학\n' +
      '\n' +
      '(주)디지엔디지털테크놀로지\n' +
      '\n' +
      '{zhaoqch1, longpy, zhangqx1, lianghan, zhanglw2, yujingyi, xulanl}@shanghaitech.edu.cn qindafei@connect.hku.hk yingliang.zhang@dgene.com\n' +
      '\n' +
      '[https://sites.google.com/view/media2face](https://sites.google.com/view/media2face)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '음성으로부터 3D 얼굴 애니메이션의 합성은 상당한 관심을 끌었다. 고품질 4D 얼굴 데이터와 주석이 잘 달린 풍부한 멀티모달리티 라벨의 부족으로 인해 이전 방법은 종종 제한된 현실감과 유연한 컨디셔닝 부족으로 고통받는다. 우리는 3부작을 통해 이 도전을 다룬다. 먼저, 얼굴 기하와 이미지를 고도로 일반화된 표현 잠재 공간, 디커플링 표현과 아이덴티티에 매핑하는 효율적인 가변 자동 인코더인 Generalized Neural Parametric Facial Asset(GNPFA)을 소개한다. 그런 다음 GNPFA를 사용하여 대규모 비디오 배열에서 고품질 표현과 정확한 머리 포즈를 추출합니다. 이것은 주석이 잘 달린 감정 및 스타일 레이블이 있는 크고 다양하며 스캔 수준의 공동 음성 3D 얼굴 애니메이션 데이터 세트인 M2F-D 데이터 세트를 제시한다. 마지막으로 GNPFA 잠재공간의 확산 모델인 Media2Face를 제안한다. Media2Face는 오디오, 텍스트, 이미지의 풍부한 멀티모달리티 지도를 수용하여 코스피치 얼굴 애니메이션 생성을 위한 것이다. 광범위한 실험을 통해 얼굴 애니메이션 합성에서 높은 충실도를 얻을 수 있을 뿐만 아니라 3D 얼굴 애니메이션에서 표현성과 스타일 적응성의 범위를 넓힐 수 있음을 보인다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델로 구동되는 생성 AI의 발전은 영화 "Her"를 연상시키는 가상 컴패니언 AI 시스템에 생명을 불어넣었다. 이러한 시스템의 핵심은 현실적이고 몰입적인 경험, 특히 사용자와의 지속적인 정서적 연결을 제공하는 것이다. 이러한 목적을 달성하기 위해서는 풍부한 음성 내용, 미묘한 음성 톤 및 복잡한 기저 감정과 일치하는 자연스러운 얼굴 애니메이션을 생성하는 것이 중요하다.\n' +
      '\n' +
      '그래픽 커뮤니티가 사실적인 코스피치 페이셜 애니메이션을 생성하는 것은 긴 여정이었습니다. 초기 결정론적 방법[16, 18, 19]은 오디오로부터 제한된 애니메이션 변형만을 생성할 수 있었다. 최근, 생성 모델, 특히 확산 모델은 2D 이미지[52, 55, 26]에서 3D 인간 동작[39, 1, 59, 3]까지 비결정적 생성으로 가는 길을 발견했다. 이들로부터 영감을 받아 확산 기반 얼굴 애니메이션 생성[2, 10, 56, 57, 60]은 유망한 결과를 달성하여 상당한 관심을 받았다.\n' +
      '\n' +
      '이러한 얼굴 애니메이션을 위한 확산 기반 생성 기법을 탐구하기 위해 두 가지 주요 요소를 관찰한다. 첫째, 확산 모델의 능력은 대규모 및 고품질 훈련 데이터에 크게 의존한다. 그러나, 대부분의 기존 방법[20, 56, 70]은 VOCASET[14] 또는 BIWI[22]와 같은 소규모 데이터 세트에 대해 훈련된다. 이러한 데이터 세트는 제한된 음성 상태만을 다루고 정서적 변이와 성격 특성의 다양성이 부족하다. 일부 최근 방법[15, 40, 46, 57, 73]은 2D 데이터 세트로 말하기 스타일을 풍부하게 하려고 시도한다. 그러나, 채택된 비디오 기반 얼굴 추적기는 종종 차선책 표정을 생성하거나 머리 동작을 무시하기 때문에 자연 표현을 실제로 복제하는 데 부족한다. 둘째, 언어, 스타일 또는 감정과 같은 다양한 양식에서 유연한 컨디셔닝 및 얽힘 없는 제어를 가능하게 하는 것이 중요하다. 일부 동시 메서드는 키프레임[60](3DiFace), 암시적 스타일[46, 57] 또는 이모지 기반[15] 컨트롤을 제공합니다. 그러나 텍스트 및 이미지 입력과 같은 보다 다양한 멀티 모달리티로부터의 충실한 컨디셔닝은 여전히 열린 과제로 남아 있다.\n' +
      '\n' +
      '본 논문에서는 위의 과제들을 3부작을 통해 접근한다. 먼저, 잠재 공간에서의 세립 얼굴 표정과 머리 포즈의 신경 표현인 GNPFA(General Neural Parametric Facial Asset)를 소개한다. 우리는 고해상도 이미지와 아티스트의 정제된 얼굴 기하학, RoM(Range of Motion) 데이터를 포함한 다양한 다중 ID 4D 얼굴 스캐닝 데이터에 대해 GNPFA를 훈련한다. 따라서 우리는 다양한 대화 스타일로 일반화할 수 있는 잠재된 표현에서 미묘한 얼굴 표정을 정체성에서 분리한다. 그런 다음 GNPFA를 사용하여 다양한 콘텐츠, 스타일, 감정 및 언어를 포함한 다양한 비디오에서 고품질 얼굴 표정과 머리 포즈를 추출합니다. 이는 다양한 감정과 스타일로 주석이 달린 다양한 4D 데이터셋인 미디어2페이스 데이터셋(Media2Face Dataset, M2F-D)을 얼굴 스캔에 필적하는 품질로 만드는 결과를 낳는다.\n' +
      '\n' +
      '마지막으로, M2F-D 데이터셋을 이용하여 코스피치 얼굴 애니메이션 생성을 위한 잠재 확산 모델인 _Media2Face_를 제안한다. 음성과 함께 고품질 립 싱크를 생성하고 텍스트, 이미지, 심지어 음악에 포함된 미묘한 인간의 감정을 표현합니다. 구체적으로, GNPFA의 잠재공간에서 Media2Face를 학습하여 세립 얼굴 애니메이션을 복원한다. Wav2Vec2[4]에 의해 추출된 오디오 특징과 CLIP[49]에 의해 인코딩된 텍스트/이미지 프롬프트를 모두 조건으로 취하고 다중 분류기 없는 안내 방식으로 순차적인 얼굴 표정 및 머리 포즈를 생성한다. 우리는 Media2Face의 효과를 입증하기 위해 광범위한 실험과 사용자 연구를 수행한다. 또한 다양한 텍스트/이미지 기반 컨디셔닝 및 편집뿐만 아니라 대화, 음악 및 연설과 같은 다양한 오디오 소스에서 생생하고 사실적인 얼굴 애니메이션을 생성하는 등 다양한 응용 프로그램을 보여준다. 요약하자면, 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 다양한 미디어 입력(오디오, 이미지, 텍스트)을 통합하여 머리 포즈를 포함한 생생한 얼굴 애니메이션을 구동하는 확산 기반 생성기인 Media2Face를 제시한다.\n' +
      '* Media2Face를 훈련하기 위해, 우리는 미묘한 얼굴 움직임 세부 사항을 캡처하기 위한 신경 잠재 표현인 GNPFA를 제안하여 주석이 달린 표현과 스타일로 다양한 공동 음성 4D 얼굴 애니메이션 데이터 세트를 수집할 수 있다.\n' +
      '* 우리는 광범위한 실험과 사용자 연구를 수행하고 다중 촬영 지침으로 얼굴 애니메이션을 생성하기 위한 흥미로운 응용 프로그램을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '##3D 얼굴 애니메이션 표현\n' +
      '\n' +
      '생성된 3차원 얼굴 표정의 품질은 3차원 얼굴 애니메이션을 위한 선택된 표현 방법에 크게 의존한다. 수년에 걸쳐, 다양한 표현 방법들이 그 분야에서 제안되었다[5, 18, 35]. 널리 채택된 접근법 중 하나는 안면 동작 코딩 시스템(FACS; Facial Action Coding System) [18]이며, 안면 동작은 인체 해부학에 대한 전문가 지식을 기반으로 근육 활성화의 조합으로 정의된다. 전통적으로, 이러한 활성화는 블렌드 쉐이프 디포머를 사용하여 구현되었다[35]. 과정을 자동화하고 더 넓은 범위의 표정을 포괄하기 위해 [5]는 3D Morphable Models (3DMM)을 제안한다. 이러한 모델은 다양한 정체성과 표현식 [6, 29, 2]를 포함하는 얼굴 스캔에서 직접 선형 변형 공간을 캡처한다. 3DMM 관련 방법에 대한 포괄적인 조사를 위해 우리는 독자들에게 [17]을 참조한다. 그 유용성에도 불구하고 선형 방법은 얼굴 표정의 미묘한 뉘앙스를 포착하는 데 한계가 있다. 이를 해결하기 위해 FLAME[38]은 포즈 의존적 교정 블렌드쉐이프와 관절형 턱, 목, 안구를 도입하여 얼굴 애니메이션 모델링의 충실도를 향상시킨다. 비선형 공간에서 얼굴 표정을 모델링하기 위한 추가 접근법이 개발되었다[34, 54]. [34] 전통적인 3DMM을 확장하기 위해 가우시안 혼합 모델을 사용하고, [54]는 표현 주름을 고해상도 예제 얼굴 모델에서 목표 모델로 전달하기 위해 다중 스케일 맵의 사용을 도입한다.\n' +
      '\n' +
      '최근 작업[7, 11, 63, 64, 65, 48, 63, 65]은 심층 신경망을 활용하여 데이터로부터 잠재된 표현 공간을 구축하고, 얼굴 애니메이션 작업에 대한 최첨단 성능을 달성한다[7]. 3D 메쉬를 위해 설계된 특수 콘볼루션 연산자를 통합하고, 변하지 않는 토폴로지를 갖는 변형 가능한 형상 고유의 일관된 그래프 구조를 자본화한다. [48] 고충실도 얼굴 변형을 통해 해석 가능하고 편집 가능한 잠재 코드를 학습하여 다양한 메쉬 토폴로지로 응용 범위를 확장한다. 그러나 이러한 방법은 사용 가능한 데이터 세트의 범위에 의해 제한되므로 종종 품질 향상과 다양성 유지 사이의 절충이 필요하다. 암묵적 표현이 발전함에 따라, 몇 가지 작업[24, 72, 80]은 사람의 머리 형상을 모델링하기 위해 Signed Distance Fields(SDFs)를 사용하려는 노력에 박차를 가했고, 얼굴 표정을 아티큘레이션하기 위해 전방 변형 필드와 결합했다. 그러나 고정된 토폴로지를 가진 헤드 메쉬로부터 잠재 공간을 구성하는 우리의 접근법에 비해 부족한다. 우리의 기술은 실제 응용 프로그램과 더 잘 일치할 뿐만 아니라 전통적인 컴퓨터 그래픽(CG) 파이프라인과의 호환성을 보장한다.\n' +
      '\n' +
      '조건부 얼굴 애니메이션 합성\n' +
      '\n' +
      'Driven Facial Animation.__Audio-Driven Facial Animation.__ 오디오 기반 3D 얼굴 애니메이션의 최근 연구는 절차적 방법[19, 31]과 학습 기반 방법[28, 20, 21, 9, 30, 50, 62, 81, 9, 81]을 활용하여 음성을 얼굴 움직임에 매핑했다. 절차적 기법들은 예술가들에게 입술 모양에 대한 통제권을 주지만, 그들은 개별 스피치 스타일을 포착하는데 유연성이 부족하고 집중적인 수동 튜닝을 요구한다[16]. 블렌드쉐이프[46, 47, 58, 67, 75] 또는 3D 메쉬[14, 20, 45]를 사용하는 학습 기반 접근법은 음성 뉘앙스를 더 잘 포착하지만 지나치게 부드러운 입술 모션과 제한된 상부 얼굴 표현성을 초래할 수 있다. 복잡한 음성-얼굴-표정 매핑을 해결하기 위해, 벡터 양자화된 변량 자동 인코더(VQ-VAE) [66]과 같은 확률 모델이 소개되어, 음성으로부터 얼굴 표정 분포를 예측한다[74, 44, 70]. 이러한 모델은 장점에도 불구하고 얼굴 표정의 확률적 특성을 완전히 나타내지 못하는 경우가 많다. 확산 모델[26, 55]은 복잡한 생성 작업[23, 51, 52, 71]을 처리하는 것으로 인정되며 오디오 기반 얼굴 애니메이션[2, 10, 56, 57, 60]에서 가능성을 보여 멀티 모달 생성을 용이하게 한다. 그러나 말을 머리 움직임과 통합하는 것은 여전히 도전으로 남아 있으며 종종 설득력이 떨어지는 표현을 산출한다. 본 논문에서 제안하는 방법은 빠른 확산 모델 내에서 새로운 헤드 모션 성분을 도입하여 생성된 얼굴 애니메이션의 응집성과 표현력을 향상시켜 스피치 기반 3D 얼굴 애니메이션에서 최첨단 기술을 발전시킨다.\n' +
      '\n' +
      'tyle Control.____ 현재 접근법은 스타일 변조를 위해 레이블 기반 및 상황 기반 제어의 두 가지 방법을 주로 사용한다. 레이블 기반 제어는 감정[15, 32], 화자 스타일[14, 20] 및 감정 강도[46]와 같은 레이블을 구성하기 위해 데이터 내의 고정된 카테고리 세트를 사용한다. 그러나 미리 정의된 범주는 모델이 복잡하고 미묘한 감정을 포착하도록 제한한다. 맥락 기반 제어는 원샷 접근법을 통해 주어진 세그먼트로부터 스타일을 모방하거나[57] 주어진 동작으로부터 새로운 말하기 스타일을 학습할 수 있게 한다[61]. 3DiFace[60]과 같은 기술은 제공된 키프레임의 컨텍스트에 적응하는 반면, DiffPoseTalk[57] 및 Imitator[61]은 각각 주어진 얼굴 애니메이션 및 스타일 적응으로부터 스타일 임베딩을 추출하거나 학습한다. 그러나 이러한 방법은 품질 및 명시적 컨트롤에 미치지 못하여 종종 진정한 인간 표현의 깊이와 복잡성이 결여된 애니메이션을 초래한다.\n' +
      '\n' +
      '##3 재구성 얼굴 애니메이션 데이터\n' +
      '\n' +
      '3D 얼굴 애니메이션의 사실적인 합성은 일반적으로 멀티뷰 카메라 설정에 의존하는 4D 동적 얼굴 성능 캡처를 필요로 한다[69]. 이 요구 사항은 이러한 데이터를 캡처하고 처리하는 노동 집약적 특성으로 인해 데이터 획득의 다양성과 확장성을 크게 제한한다.\n' +
      '\n' +
      '본 논문에서는 이러한 제약 조건을 해결하기 위해 얼굴 기하학과 비디오 발자국을 동일한 잠재 공간에 매핑하는 Variational Auto Encoder인 Generalized Neural Parametric Facial Asset (GNPFA)를 제안한다. 우리는 고해상도 이미지와 아티스트의 세련된 기하학을 포함한 대규모 4D 얼굴 스캐닝에 대한 GNPFA를 훈련하여 다양한 정체성, 언어, 감정 및 머리 포즈를 가진 비디오에서 미묘한 얼굴 애니메이션을 생성할 수 있다.\n' +
      '\n' +
      '### 표현 잠재 공간 학습\n' +
      '\n' +
      '트레이닝 데이터 우리는 먼저 방대한 다중 ID 4D 스캔으로 데이터 세트를 캡처한다. 우리는 이것을 RoM(Range of Motion) 데이터라고 부른다. RoM은 43652개의 등록된 메쉬와 다양한 성별, 연령 및 민족에 걸쳐 300개의 신원에서 698432개의 이미지로 구성된다. 또한 표현-정체성 분리의 견고성을 높이기 위해 [36]에 따른 FACS 표준에 따라 200개의 정체성에 대한 개인화된 블렌드쉐이프들을 생성하고, 증강을 위한 훈련 동안 무작위 인공 표현을 생성한다.\n' +
      '\n' +
      '기하학적 VAE는 아이덴티티로부터 단절된 표현 잠재 공간을 학습하기 위해 기하 인코더\\(\\mathcal{E}_{\\text{geo}\\)와 기하 생성기\\(\\mathcal{G}_{\\text{geo}\\)으로 구성된 기하학적 VAE를 설계하며, 여기서 \\(\\mathcal{G}_{\\text{geo}\\)은 중립 기하상에서 조건화되고 UNet 아키텍처를 활용한다. 전통적인 블렌드쉐이프 애니메이션을 지원하기 위해 두 개의 매핑 네트워크(\\(\\mathcal{M}\\)와 \\(\\mathcal{M}^{\\prime}\\)를 학습하는데, 여기서 전자는 블렌드쉐이프의 가중치(\\(w\\)를 잠재공간에 매핑하고 후자는 역행렬을 수행한다. 전진 과정은 그림 2에 나와 있다.\n' +
      '\n' +
      '입력기하, \\(\\mathbf{G}_{\\text{R}\\) 및 그 쌍을 이루는 중성기하\\(\\mathbf{\\bar{G}_{\\text{R}\\)이 주어지면, 본 기하구조 인코더는 VAE 샘플링을 통해 표현 잠재 코드로 인코딩한다. \\(z_{\\text{R}=\\mathcal{E}_{\\text{geo}(\\mathbf{G}_{\\text{R}})\\. 그 후, 기하 디코더는 얼굴 기하학을 복원한다:\\(\\mathbf{\\tilde{G}}_{\\text{R}=\\mathcal{G}_{\\text{geo}(\\mathbf{\\tilde{G}_{\\text{R}},z_{\\text{R}}). 훈련 목표는 단순히 재구성 손실이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{recon,R}}=\\|\\mathbf{\\tilde{G}_{\\text{R}-\\mathbf{G}_{\\text{R}}\\|_{2}^{2}. \\tag{1}\\\n' +
      '\n' +
      '무작위로 샘플링된 블렌드쉐이프\\(w_{\\text{B}\\)와 중성면\\(\\mathbf{\\tilde{G}_{\\text{B}\\)이 주어지면, 개인화된 블렌드쉐이프들을 이용하여 변형된 표현\\(\\mathbf{G}_{\\text{B}\\)을 얻는다. 실제 데이터 시나리오와 유사하게 표현 잠재 코드 \\(\\tilde{z}_{\\text{B}=\\mathcal{M}_{\\text{geo}(w_{\\text{B}))를 추출하고 기하 디코더 \\(\\mathbf{\\tilde{G}_{\\text{B}}=\\mathcal{G}_{\\text{geo}(\\mathbf{\\tilde{G}}_{text{B}},z_{\\text{B}})를 통해 기하를 재구성한다. 우리는 식을 \\(\\tilde{w}_{\\text{B}}=\\mathcal{M}^{\\prime}(\\tilde{z}_{\\text{B}})으로 다시 매핑한다. 상기 훈련 목표는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{recon,B}}=\\|\\mathbf{G}_{\\text{B}}-\\mathbf{G}_{\\text{B}}\\|_{2}^{2}+\\|\\tilde{z}_{\\text{B}}-z_{\\text{B}}\\|_{2}^{2}+\\|\\tilde{w}_{\\text{B}}-w_{2}^{2}}.\\tag{2}\\text{2}}\n' +
      '\n' +
      '우리는 좌표 맵[77]을 사용하여 UV 공간에서 2D 지오메트리 맵 상의 각 꼭지점의 3D 위치를 저장하는 지오메트리를 표현한다. 이 표현은 고정된 토폴로지를 사용하여 메쉬 표현으로 및 메쉬 표현으로부터 변환될 수 있다. CG 호환성 외에도 비선형성과 정점 수준의 입상성으로 인해 기존의 파라메트릭 얼굴 모델보다 더 사실적이고 믿을 수 있는 애니메이션 공간을 생성한다.\n' +
      '\n' +
      '### 이미지 얼굴 표정 추출\n' +
      '\n' +
      '기하학적 VAE 외에도 RGB 영상으로부터 통합된 표정 잠재 코드와 머리 자세를 추출하기 위해 두 개의 비전 인코더인 \\(\\mathcal{E}_{\\text{exp}\\)와 \\(\\mathcal{E}_{\\text{pose}\\)를 학습한다. 우리는 기하학 VAE를 동결하고 RoM 데이터의 실제 이미지와 개인화된 블렌드쉐이프에 의해 무작위로 생성된 기하학에서 렌더링된 이미지의 감독하에 비전 인코더를 훈련한다.\n' +
      '\n' +
      '구체적으로, 지면-진리 기하학(\\mathbf{G}_{\\text{R}\\) 및 중성(\\mathbf{\\bar{G}_{\\text{R}\\)을 갖는 RoM 데이터 내의 이미지\\(\\mathbf{I}_{\\text{R}\\)이 주어지면, 우리는 표현 잠재 코드\\(\\hat{z}_{R}=\\mathcal{E}_{\\text{exp}(\\mathbf{I}_{\\text{R}}) 및 머리 포즈\\(\\hat{p}_{\\text{R}=\\mathcal{E}_{\\text{pose}(\\mathbf{I}_{\\text{R}})를 추출한다. 그런 다음, 미리 훈련된 디코더를 사용하여 얼굴 기하학을 재구성한다. 우리는 얼굴의 렌더링 이미지를 얻기 위해 미분 가능한 렌더러\\(\\mathcal{R}\\)을 사용한다. \\(\\mathbf{\\hat{I}}_{\\text{R}=\\mathcal{R}(\\mathbf{\\hat{G}}_{\\text{R}},\\hat{p}_{\\text{R}})\\. 학습 손실, \\(\\mathcal{L}_{\\text{exp}}\\)을 기하 손실과 이미지 손실의 조합으로 정의한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{exp, R}}=\\\\|\\mathbff{\\hat{G}_{\\text{R}-\\mathbf{G}_{\\text{R}}\\|_{2}^{2}++\\|\\mathbf{\\hat{I}_{\\text{R}-\\mathbf{I}_{\\text{R}}\\|_{2}^{2}.\\tag{3}\\tag{2}\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Hours & Emotion & Head Pose & Language \\\\ \\hline VOCASET & 0.5 & ✗ & ✗ & EN \\\\ BIWI & 1.7 & ✓ & ✓ & EN \\\\ MultiFace & 2.8 & ✓ & ✓ & EN \\\\ UUDaMM & 9.6 & ✗ & ✗ & EN \\\\ DiffposeTalk & 26.5 & ✗ & ✓ & EN \\\\ EMOTE & 25.3 & ✓ & ✗ & EN \\\\\n' +
      '**M2F-D (Ours)** & 60.6 & ✓ & ✓ & 6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **4D 데이터셋 비교**. 디포즈톡[57]은 재구성된 TFHP[57]와 HDTF의 조합임을 주목하라. EMOTE[15]는 재구성된 MEAD에 대해 훈련된다.\n' +
      '\n' +
      '도 2: **GNPFA 파이프라인**. (_Left:_) 우리는 기하학 VAE를 학습하여 표현과 머리 포즈의 잠재 공간을 학습하고, 정체성을 가진 표현을 분리한다. (_Right:_) 두 개의 비전 인코더는 RGB 이미지에서 표현 잠재 코드와 머리 포즈를 추출하도록 훈련되어 4D 데이터의 광범위한 배열을 캡처할 수 있다.\n' +
      '\n' +
      '유사하게, 랜덤하게 샘플링된 기하학\\(\\mathbf{G}_{\\text{B}\\)으로, 우리는 머리 포즈\\(\\hat{p}_{\\text{B}=\\mathcal{E}_{\\text{pose}(\\mathcal{R}(\\mathbf{G}_{\\text{B}},p_{\\text{B})) 및 표현 코드\\(\\hat{z}_{\\text{B}=\\mathcal{E}_{\\text{exp}(\\mathcal{R}(\\mathbf{G}_{\\text{B},p_{\\text{B}))을 추출할 수 있다. 우리는 동일한 미분 가능한 렌더러\\(\\mathbf{\\hat{I}}_{\\text{B}=\\mathcal{R}(\\mathbf{\\hat{G}}_{\\text{B}},\\hat{p}_{\\text{B}})에 의해 이미지를 렌더링한다. 우리의 훈련 목표는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{exp, B}}=\\|\\mathbf{\\hat{G}_{\\text{B}}-\\mathbf{G}_{\\text{B}}\\|_{2}^{2}+\\|\\mathcal{R}(\\mathbf{\\hat{G}_{\\text{B}},\\hat{p}_{\\text{B}})-\\mathcal{R}(\\mathbf{G}_{\\text{B}}, p_{\\text{B}}\\|_{2}^{2}}.\\|\\mathcal{R}(\\mathbf{G}_{\\text{B}}, p_{\\text{B}}}}\\|_{2}^{2}}.\\|\\mathcal{R}(\\mathbf{\\hat{G}}_{\\text{B}}},\\hat{p}_{\\text{B}}}}\\mathbf{G}_{\\text{B}}},\\hat{\n' +
      '\n' +
      '훈련 후, \\(\\mathcal{E}_{\\text{exp}\\)와 \\(\\mathcal{E}_{\\text{pose}\\)은 야생 동영상에서 세립화된 표정과 머리 포즈를 포착하고, 표정 잠재 공간에 표현되며, \\(\\mathcal{G}_{\\text{geo}\\)에 의해 개인화된 표정으로 매핑될 수 있다. GNPFA의 빠른 추론 속도로 인해, 우리는 야생 비디오에서 고품질이고 다양한 표현과 머리 포즈를 효율적으로 추출할 수 있다.\n' +
      '\n' +
      '### 잠재성 기반 얼굴 애니메이션 데이터세트\n' +
      '\n' +
      '우리는 풍부한 오디오 및 텍스트 라벨이 있는 대규모 온라인 비디오 얼굴 데이터 모음을 활용하고 GNPFA를 사용하여 정확한 얼굴 표정과 정확한 머리 포즈를 추출한다. 이를 통해 지루한 주석을 피하고 제한된 4D 얼굴 애니메이션 데이터셋을 쉽게 증강할 수 있으며, 이는 Media2Face Dataset(M2F-D)을 제시한다.\n' +
      '\n' +
      'MEAD[68], CREMA-D[8], RAVDESS[41], HDTF[79] 및 아카펠라[43]에서 잠재 코드와 머리 포즈 표현을 모두 검색한다. MEAD 데이터 세트는 3가지 다양한 강도 수준에서 8가지 다른 감정을 표현하는 60명의 배우 및 여배우의 대화 얼굴 비디오로 구성된다. CREMA-D 데이터 세트는 4개의 다른 강도 수준에서 6개의 다른 감정을 표현하는 12개의 문장을 전달한 91명의 배우를 특징으로 하는 7,442개의 별개의 클립으로 구성된다. RAVDESS 데이터셋은 연설과 노래를 발성하는 24명의 전문 배우의 동영상으로 구성되어 있으며, 각각 총 7개와 5개의 감정이 있다. HDTF 데이터 세트는 고품질 비디오의 집합이며 아카펠라 데이터 세트는 솔로 노래 비디오를 포함한다.\n' +
      '\n' +
      '대화 언어의 다양성을 더욱 높이기 위해 중국어, 프랑스어, 독일어, 일본어, 러시아어, 스페인어의 6가지 언어를 포함하는 야생 비디오에서 2시간 데이터 세트를 수집한다. 다양한 시나리오에서 명시적인 머리 포즈 제어를 허용하기 위해, 우리는 말하기, 노래하기, 끄덕이기, 흔들기, 찡그리기, 윙크 등을 포함한 14개의 다른 머리 움직임을 수행하는 14명의 화자로 구성된 1.6시간 데이터 세트를 캡처한다.\n' +
      '\n' +
      '우리의 M2F-D 데이터 세트는 30 fps에서 총 60시간 이상의 지속 시간을 가지고 있다. 표 1에서 볼 수 있듯이 지속 시간과 다양성 모두에서 기존 시청각 데이터 세트를 능가한다.\n' +
      '\n' +
      '## 4 미디어2페이스 방법\n' +
      '\n' +
      '_Media2Face_는 다중 모드 구동 신호에 대한 트랜스포머 기반 잠재 확산 모델 컨디셔닝이다. 순차적인 머리 포즈 및 얼굴 표정, 즉 풀 페이셜 애니메이션의 공동 분포를 모델링하여 포즈 및 표현의 자연스러운 시너지를 촉진한다. 또한 멀티 컨디셔닝 가이드를 사용하여 CLIP 유도 스타일화 및 이미지 기반 키프레임 편집으로 일관성이 높은 코스피치 페이셜 애니메이션 합성을 가능하게 한다.\n' +
      '\n' +
      '### 얼굴 애니메이션 잠재 확산 모델\n' +
      '\n' +
      '도 1에서 설명한 바와 같다. 도 3을 참조하면, 각 비디오 프레임으로부터 표현 잠재 코드\\(\\mathbf{z}_{e}^{i}\\)와 머리 포즈\\(\\theta^{i}\\)을 추출한다. 그런 다음, 이들을 연결하여 \\(\\mathbf{x}^{i}=[\\mathbf{z}_{e}^{i},\\theta^{i}]]으로 표시된 단일 프레임 얼굴 애니메이션 상태를 형성한다. 따라서 얼굴 애니메이션은 일련의 상태(\\(\\mathbf{X}^{1:N}=[x^{i}]_{i=1}^{N}\\)에 의해 형성된다.\n' +
      '\n' +
      '확산 모델에서는 생성을 마르코프 잡음 제거 과정으로 모델링한다. 여기서, \\(\\mathbf{X}_{t}^{1:N}\\)는 \\(t\\) 단계에 걸쳐 그라운드 트루스 헤드 모션 코드 \\(\\mathbf{X}_{0}^{1:N}\\)에 노이즈를 추가하여 구한다. 이 방법은 단계적 디노이징 과정을 용이하게 하기 위해 분포\\(p\\left(\\mathbf{X}_{0}^{1:N}|\\mathbf{X}_{t}^{1:N}\\right)를 모델링한다. MDM[59]의 접근법과 유사하게 \\(\\mathbf{X}_{0}^{1:N}\\)을 직접 예측한다. 이 예측 방법을 사용하면 액션 일관성과 평활성을 개선하기 위해 추가 정규화 용어를 도입할 수 있다.\n' +
      '\n' +
      '다중 모드 조건을 통합하기 위해 대규모 사전 훈련 인코더를 사용합니다. 원시 음성 오디오는 사전 훈련된 Wav2Vec2[4]에 의해 인코딩되고 선형 보간법에 의해 얼굴 애니메이션 시퀀스의 길이에 정렬되어 오디오 특징 \\(\\mathbf{A}^{1:N}\\)이 생성된다. 또한, 대화 스타일의 프롬프트 역할을 하는 텍스트 또는 이미지는 사전 훈련된 CLIP 모델에 의해 CLIP 잠재 코드\\(\\mathbf{P}\\)로 인코딩된다[49]. 트랜스포머 기반 디노이저는 일반적인 스타일 인식 교차 주의 레이어를 통해 이러한 다중 모달 임베딩의 연결에서 얼굴 애니메이션 \\(\\mathbf{X}_{0}^{1:N}\\) 컨디셔닝을 예측하도록 학습한다. 각각의 시간 단계에서, 잡음 제거 공정은 다음과 같이 공식화될 수 있다:\n' +
      '\n' +
      '\\mathcal{G}\\left(\\mathbf{X}_{t}^{1:N},t,\\mathbf{A}^{1:N},\\mathbf{P}\\right)\\tag{4}\\right)\n' +
      '\n' +
      '말의 풀림 및 신속한 제어를 가능하게 하기 위해, 트레이닝 동안, 두 개의 랜덤 마스크가 다중-조건 분류기-없는 안내를 위해 도입된다[27]. 처음에 CLIP 잠재 코드\\(\\mathbf{P}\\)는 첫 번째 랜덤 마스크를 거치며, 이는 스타일화된 코-스피치 디누아와 스타일화되지 않은 코-스피치 디누아 모두를 가져오고 음성 신호와 단절된 스타일 제어를 가능하게 한다. 그런 다음 이 마스킹된 코드를 오디오 특징 \\(\\mathbf{A}^{1:N}\\)과 연결한다. 두 번째 단계의 랜덤 마스킹이 최종 연접 코드에 적용되며, 이는 유사하게 스피치-구동 및 비-스피치-구동 디누아들을 모두 가져오고 스피치 콘텐츠 일관성 강도를 조정하는 것을 용이하게 한다.\n' +
      '\n' +
      '훈련은 모델을 훈련하기 위한 주요 목표로 단순 손실[26]을 사용하며, 이는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{simple}}=\\|\\mathbf{X}_{0}^{1:N}-\\hat{\\mathbf{X}}_{0}^{1:N}\\|_{2}^{2}. \\tag{5}\\]\n' +
      '\n' +
      '또한, 인접한 프레임 사이의 자연스러운 전이를 생성하기 위해 모델을 강제하기 위해 속도 손실[14]을 도입하며, 이는 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\left\\|\\left(\\mathcal{L}_{\\text{velocity}}=\\mathbf{X}_{0}^{2:N}-\\mathbf{X}_{0}^{1:N-1}\\right)-\\left(\\mathbf{\\hat{X}_{0}^{2:N}-\\mathbf{0}^{1:N-1}\\right)\\right\\|_{2}^{2}.\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\tag{6}\\t\n' +
      '\n' +
      '또한, 평활성을 강화하고 급격한 전이 및 불연속을 감소시키기 위해 평활 손실[57]이 사용된다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{smooth}=\\left\\|\\mathbf{\\hat{X}}_{0}^{3:N}+\\mathbf{\\hat{X}}_{0}^{1:N-2}-\\mathbf{\\hat{X}}_{0}^{1:N-2}-\\mathbf{\\hat{X}}_{0}^{2:N-1}\\right\\|_{2}^{2}. \\tag{7}\\\\tag{7}\\\n' +
      '\n' +
      '전반적으로, 데노이저는 다음과 같은 목적으로 훈련된다:\n' +
      '\n' +
      '\\lambda_{\\text{simple}\\mathcal{L}_{\\text{simple}}+\\lambda_{\\text{velocity}\\mathcal{L}_{\\text{velocity}+\\lambda_{\\text{smooth}\\mathcal{L}_{\\text{smooth}, \\tag{8}\\mathcal}\n' +
      '\n' +
      '여기서 \\(\\lambda_{\\text{simple}}\\), \\(\\lambda_{\\text{velocity}}\\) 및 \\(\\lambda_{\\text{smooth}}\\)은 이러한 항들로부터 기여들의 균형을 맞추기 위한 손실 가중치로서 작용하는 하이퍼-파라미터들이다.\n' +
      '\n' +
      '디노이징 프로세스 동안, 우리의 모델은 두 가지 유형의 안내, 즉 메인 스피치 오디오 및 추가 텍스트/이미지 스타일 안내를 분류기 없는 안내 기술과 결합한다[27].\n' +
      '\n' +
      '\\cdot\\mathcal{G}\\left(\\mathbf{s}_{A}-\\mathbf{s}_{0}^{1:N},t\\right)\\cdot\\mathcal{G}\\left(\\mathbf{s}_{t}^{1:N},t,\\mathbf{A}^{1:N},\\mathbf{s}^{P}\\cdot\\mathcal{G}\\left(\\mathbf{X}_{t}^{1:N},t,\\mathbf{A}^{1:N},\\mathbf{P}\\mathcal{G}\\left(\\mathbf{x}_{t}^{1:N},\\mathbf{s}^{P}\\mathcal{G}\\left(\\mathbf{x}_{t}^{1:N},\\mathbf{P}\\mathcal{G}\\left(\\mathbf{x}_{t}^{1:N},\\\n' +
      '\n' +
      '여기서 \\(\\mathbf{s}_{A}\\) 및 \\(\\mathbf{s}_{P}\\)은 각각 음성 및 스타일 안내 강도를 조정하기 위한 두 가지 강도 인자이다. 마지막 두 용어는 동일한 스피치 입력 내에서 비스타일화 및 스타일화된 예측을 모두 제공하며, 이는 스피치 콘텐츠를 넘어 얽힌 스타일 제어를 의미한다.\n' +
      '\n' +
      '중첩된 배칭 디노이징은 실시간 응용에 대한 추론 시간을 줄이기 위해 StreamDiffusion[33]에서 소개된 배칭 디노이징 단계와 유사한 기법인 _batching denoising_을 사용하고, 오디오가 중첩된 윈도우로 분할되어 하나의 디노이징 패스에서 매우 긴 오디오를 처리하기 위해 _overlapped batching denoising_로 확장한다. 중첩 배치 잡음 제거 접근법은 전통적으로 다중 자기회귀 시퀀스 생성 작업을 병렬화 가능한 노력으로 변환한다. VRAM 용량의 범위 내에서, 그 처리 시간은 오디오의 길이에 따라 선형적으로 증가하지 않으며, 따라서 헤드 모션 생성의 속도를 상당히 향상시킨다.\n' +
      '\n' +
      '조건부 얼굴 애니메이션 편집\n' +
      '\n' +
      'Media2Face는 키프레임 편집 및 텍스트/이미지 안내를 통해 세밀한 생성 제어를 달성한다. 도 1에 도시된 바와 같다. 4, 우리는 GNPFA와 CLIP를 사용하여 얼굴 이미지와 텍스트/이미지 프롬프트에서 조건을 추출하고 분류기가 없는 지침을 활용하여 확산 프로세스를 제어한다.\n' +
      '\n' +
      '도 3: **Media2Face의 아키텍처.** 우리의 모델은 오디오 피처 및 CLIP 잠재 코드를 조건으로 취하고 머리 포즈, 즉 머리 모션 코드와 함께 발현 잠재 코드의 잡음화된 시퀀스를 잡음 제거한다. 조건들은 랜덤하게 마스킹되고 노이즈 헤드 모션 코드와 교차-어텐션된다. 추론에서, 우리는 DDIM에 의해 헤드 모션 코드들을 샘플링한다. 표정 잠재 코드를 GNPFA 디코더에 전달하여 표정의 기하학을 추출하고, 모델 템플릿과 결합하여 머리 포즈 파라미터에 의해 향상된 얼굴 애니메이션을 생성한다.\n' +
      '\n' +
      '키프레임 편집은 생성된 애니메이션의 키프레임을 수정하고 시간 영역에서 확산 인페인팅 기법[42]을 사용하여 해당 입술 움직임과 부드럽게 통합할 수 있다. 도. 도 4는 GNPFA를 이용하여 이미지에서 검색된 키프레임을 수정하는 예를 나타낸다. 유사하게, 이 능력은 상이한 소스들로부터의 애니메이션들을 함께 확산시키기 위해 [53]에서 _sequential composition_로서 일반화될 수 있다. 저희 보충 영상을 참고해 주세요.\n' +
      '\n' +
      'CLIP 유도 스타일 편집은 [3, 59]에서 중간 기법을 사용하여 오디오 세그먼트 내에서 서로 다른 프레임에 걸쳐 다양한 스타일 컨트롤을 적용할 수 있다. 개별 프레임에 고유한 스타일 프롬프트를 할당하고 각 확산 단계에서 그라디언트 마스크를 사용하여 다양한 프롬프트의 샘플링 결과를 매끄럽고 자연스럽게 통합한다. 이 방법론은 오디오 시퀀스 전체에 걸쳐 스타일 영향의 일관된 전환을 보장한다.\n' +
      '\n' +
      '## 5 Results\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 8에서는 텍스트 및 이미지 프롬프트를 기반으로 한 스타일 기반 생성 결과뿐만 아니라 다중 오디오 기반 애니메이션을 보여준다. 자세한 결과는 보충 영상을 참고하시기 바랍니다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'GNPFA의 경우 드림페이스[76]를 따라 기하학 VAE를 설계하며, 비전 인코더는 기하학 인코더와 동일한 아키텍처를 공유한다. 기하학 VAE 및 비전 인코더의 훈련은 AdaBelief 최적화기를 사용하여 Nvidia A6000 GPU에서 각각 수렴하는 데 10일 및 96시간이 소요된다. GNPFA는 약 500 fps에서 Nvidia RTX 3090 GPU를 추론할 수 있다. Media2Face의 경우, 4개의 주의 헤드를 사용하여 8-계층 변압기 디코더를 디노이저에 사용한다. 특징치수는 512이고, 윈도우 크기는 30 fps에서 \\(N=200\\)이다. 훈련하는 동안, 우리 모델은 500개의 소음 단계와 함께 코사인 소음 일정을 따릅니다. Media2Face는 AdamW 최적화기를 사용하여 36시간 동안 Nvidia RTX 3090 GPU에서 훈련된다. \\(\\lambda_{\\text{smooth}}=0.01,\\lambda_{\\text{velocity}}=1,\\lambda_{\\text{simple}}=1\\)로 설정하였다. 추론 시간 동안 \\(\\textbf{s}_{A}=2.5\\), \\(\\textbf{s}_{P}=1.5\\)을 설정하였다. 우리는 Nvidia RTX 3090 GPU에서 오프라인에서 300fps 이상, 실시간으로 30fps 이상을 달성했다.\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '우리는 Media2Face를 여러 최신 얼굴 애니메이션 방법과 비교한다. 우리는 테스트 세트로 M2F-D에서 2시간 세그먼트를 분리하고 훈련 세트와 유사한 데이터 구조로 유지한다. 3D 방법의 경우 FaceFormer[20], CodeTalker[70], FaceDiffuser[56] 및 EmoTalk[46]과 비교하여 미리 학습된 모델을 베이스라인으로 사용한다. 우리는 공정한 비교를 위해 모든 결과를 동일한 FLAME 토폴로지로 통합한다. 또한 생성된 머리 포즈의 품질을 머리 움직임을 통합하는 2D 대화 얼굴 생성 방법인 SadTalker[78]와 비교했다.\n' +
      '\n' +
      '###### 5.2.1 정량적 비교\n' +
      '\n' +
      '입술 동기화를 측정하기 위해 각 프레임에 대해 모든 입술 정점에 걸쳐 최대 L2 오차를 계산하는 LVE(Lip Vertex Error) [50]을 사용한다. 상면 동역학 편차(FDD; Upper Face Dynamics Deviation) [70]은 합성한 것과 지상 진리 사이의 시간에 따른 각 상면 동작의 표준편차를 비교하여 표현의 다양성을 측정한다. 오디오와 생성된 머리 포즈 사이의 동기화를 평가하기 위해, 생성된 머리와 그라운드 트루스 사이의 머리 포즈 비트의 동기화를 계산하는 비트 정렬(BA) [57, 78]을 활용한다. 표 2에 나타난 바와 같이, 우리의 방법은 입술 정확도, 표정 스타일화, 리드미컬한 머리 움직임의 합성 측면에서 기존의 방법을 능가한다.\n' +
      '\n' +
      '###### 5.2.2 질적 비교\n' +
      '\n' +
      '우리는 도 7에서 우리의 정성적 비교를 보여준다. 감정-맹검 방법과 비교하여, (FaceFormer, CodeTalker),\n' +
      '\n' +
      '도 4: **Application show case.** 생성된 얼굴 애니메이션(_Row 2_)을 1로 미세 조정할 수 있다. 우리의 표정 인코더(_Row 3_)를 통해 키-프레임 표정 잠재 코드를 추출하고, 2. CLIP(_Row 4_, _Left_: happy, _Right_: Sad)를 통해 프레임별 스타일 프롬프트를 제공한다. 제어의 강도와 범위는 확산 사이 기술을 사용하여 조정될 수 있다.\n' +
      '\n' +
      'FaceDiffuser, 본 논문에서 제안하는 방법은 보다 정확한 입술 움직임뿐만 아니라 중립적인 조건(눈 깜박임, 눈썹 제스처)에서도 미세 표정을 생성할 수 있다. 이 방법은 감정 인식 방법(EMOTE, EmoTalk)과 비교하여 입술 모양 정확도를 유지하면서 감정과 얼굴 디테일의 보다 생동감 있고 자연스러운 표현을 보여준다. 이 방법은 또한 주어진 조건에 매우 동기화된 머리 포즈를 생성한다는 것을 주목하라(깜짝 놀라 머리를 올리고 슬픔으로 낮추기).\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      '(1) GNPFA 매핑 네트워크(\\(\\mathcal{M}^{\\prime}\\)에서 얻은 선형 블렌드쉐이프에서 Media2Face를 학습하여 주요 구성 요소를 평가하기 위해 다음과 같은 절제 실험을 수행한다. (2) _Ours w/o CFG_: 분류기 없는 안내 없이 Media2Face를 추론한다. 표 2에 나타난 바와 같이, GNPFA의 제거는 LVE에서 상당한 저하를 초래하여 정확한 입술 모양을 모델링하는 데 GNPFA의 효과를 검증한다. CFG가 없는 추론은 모델이 양식화된 머리 움직임을 생성하지 못하기 때문에 FDD에서 성능이 좋지 않다. 또한 M2F-D의 10%, 40%, 70%에서 Media2Face를 훈련한다. 표 2에 나타난 바와 같이, 데이터 세트 스케일링 업 동안 FDD 및 BA에 대한 모델 성능은 증가하는 반면 LVE에 대한 모델 성능은 일정하게 유지된다. 이것은 모델이 작은 데이터 세트에서 정밀한 립싱크 애니메이션을 학습할 수 있지만, 사실적인 표현, 다양한 감정, 적절한 머리 움직임으로 애니메이션을 생성하기 위해 풍부한 조건의 많은 데이터에서 학습이 필요하다는 가설을 입증한다.\n' +
      '\n' +
      '### User Study\n' +
      '\n' +
      '대화, 연설, 노래 등 30개의 다양한 오디오 샘플을 진행하고 100명의 참가자를 반전시킵니다. 생성된 모든 지오메트리에 동일한 쉐이더와 템플릿을 사용하여 공정한 비교를 보장합니다. 참가자들은 미디어2페이스를 각 오디오에 대해 특정 스타일 프롬프트로, 중립 프롬프트로, 프롬프트 및 머리 포즈 애니메이션 없이 세 가지 조건으로 평가하는 다른 방법으로 나란히 애니메이션을 보장한다. 모델은 일반 사례의 경우 90% 이상, 특정 스타일 프롬프트가 없는 경우 80%, 특정 스타일 프롬프트와 머리 포즈가 없는 경우 70%의 우수한 선호 등급을 보여 머리 포즈 생성 및 스타일 프롬프트의 효과를 강조한다.\n' +
      '\n' +
      '## 6 Conclusions\n' +
      '\n' +
      '본 논문에서는 풍부한 멀티모달 컨디셔닝을 가진 실감형 코스피치 얼굴 애니메이션 합성을 위한 확산 모델의 경계를 미는 Media2Face를 제시한다.\n' +
      '\n' +
      '고품질의 얼굴 애니메이션 데이터로 확산 모델을 향상시키기 위해, 얼굴 표정과 머리 포즈에 대한 잠재 신경 표현이 있는 얼굴 VAE인 GNPFA를 광범위한 얼굴 스캐닝 데이터에 대해 사전 훈련한다. 그런 다음 GNPFA는 다양한 리소스에서 접근 가능한 얼굴 비디오의 덩어리에서 고품질 표현과 머리 포즈를 추출하는 데 사용된다. 60시간 이상의 풍부한 음성, 감정 및 스타일 주석이 있는 크고 다양한 스캔 수준의 3D 얼굴 애니메이션 데이터 세트인 M2F-D를 제공한다. 마지막으로, M2F-D 데이터셋을 이용하여 GNPFA 잠재공간에서 Media2Face 모델을 학습한다. Media2Face는 오디오, 텍스트, 이미지를 포함한 다양한 미디어 입력을 통합하여 음성과 고품질의 립 싱크를 유지하면서 얼굴의 감정과 스타일을 유연하게 제어합니다. 실험 결과는 Media2Face의 효과를 입증하고 대화 상황 재구성, 멀티모달리티 조건 편집 등 다양한 관련 애플리케이션을 선보인다. 우리는 Media2Face가 우리 인간과의 강한 정서적 연결과 공명을 가진 현실적인 인간 중심 AI 가상 동반자를 실현하기 위한 중요한 단계라고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline \\hline Methods & LVE(mm)\\(\\downarrow\\) & FDD(\\(\\times 10^{-5}\\)m)\\(\\downarrow\\) & BA \\(\\uparrow\\) \\\\ \\hline FaceFormer & 18.19 & 21.37 & N/A \\\\ CodeTalker & 16.74 & 21.95 & N/A \\\\ FaceDiffuser & 16.33 & 22.38 & N/A \\\\ EmoTalk & 14.61 & 17.84 & N/A \\\\ SadTalker & — & — & 0.219 \\\\ \\hline Ours w/o CFG & 10.67 & 16.69 & 0.166 \\\\ Ours w/o GNPFA & 14.89 & 12.81 & 0.198 \\\\ \\hline Ours w/ 10\\% data & 10.75 & 20.65 & 0.170 \\\\ Ours w/ 40\\% data & 10.55 & 18.32 & 0.208 \\\\ Ours w/ 70\\% data & **10.43** & 14.98 & 0.221 \\\\ \\hline\n' +
      '**Ours** & 10.44 & **12.21** & **0.254** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **정량 비교 및 평가**. BA 메트릭은 머리 포즈를 생성하지 않기 때문에 페이스포머, 코드토커, 페이스디퓨저 및 EmoTalk에 사용되지 않는다는 점에 유의하십시오. 또한, 정점과 관련된 메트릭은 서로 다른 얼굴 토폴로지로 인해 SadTalker에 활용되지 않는다.\n' +
      '\n' +
      '그림 5: **사용자 연구 결과.** 우리의 방법이 노래 사례에서 압도적인 우월성을 어떻게 입증했는지에 주목하여, 풍부한 감정과 리드미컬한 머리 움직임을 생성하는 모델의 능력을 보여준다.\n' +
      '\n' +
      '그림 6: **다양한 정체성에 대한 리타겟팅.** GNPFA 덕분에 우리는 다양한 성별, 연령 및 민족에 걸쳐 다양한 정체성에 맞는 개인화된 미묘한 얼굴 메쉬를 추가로 생성할 수 있다. 서로 다른 정체성, 특히 서로 다른 주름 사이의 얼굴 세부 사항의 차이를 주목하세요.\n' +
      '\n' +
      '도 7: **Qualitative 비교.**_top:_ 감정-맹검 방법과 비교, 미디어2Face로 전달하기 위해 중성 프롬프트를 사용한다. _ Bottom:_ 감정 인식 방법과 비교. Media2Face에 대한 텍스트 프롬프트를 활용하고 EMOTE에 해당 레이블을 할당한다. EmoTalk은 수동으로 할당할 수 없는 오디오에서 감정 특징을 추출합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. _ACM Transactions on Graphics_, 42(4):1-20, July 2023.\n' +
      '\n' +
      '도 8: **결과 갤러리.** 스크립트화된 텍스트 설명을 통해 생생한 대화 장면(_Row 1,2_)을 생성한다. 이미지 프롬프트를 통해 스타일화된 얼굴 애니메이션(_Row 3,4_)을 합성하는데, 이는 이모티콘이나 더 추상적인 이미지일 수 있다. 또한 프랑스, 영어, 일본어(_Row 5-7_)에서 감성적인 노래를 연주합니다. 자세한 결과는 보충 영상을 참고하시기 바랍니다.\n' +
      '\n' +
      '* [2] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias Niessner. Facetalk: Audio-driven motion diffusion for neural parametric head models, 2023.\n' +
      '* [3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturedift-fulclip: Gesture diffusion model with clip latents. _ACM Trans. Graph._, 2023.\n' +
      '* [4] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: a framework for self-supervised learning of speech representations. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS\'20, Red Hook, NY, USA, 2020. Curran Associates Inc. 2, 5\n' +
      '* [5] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In _Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques_, SIGGRAPH \'99, page 187-194, USA, 1999. ACM Press/Addison-Wesley Publishing Co.\n' +
      '* [6] James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, and Stefanos Zafeiriou. 3d face morphable models" in-the-wild". In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 48-57, 2017.\n' +
      '* [7] Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Stefanos Zafeiriou, and Michael Bronstein. Neural 3d morphable models: Spiral convolutional networks for 3d shape representation learning and generation. In _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 7212-7221, 2019.\n' +
      '* [8] Houwei Cao, David G Cooper, Michael K Kautmann, Ruben C Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. _IEEE transactions on affective computing_, 5(4):377-390, 2014.\n' +
      '* [9] Yong Cao, Wen C. Tien, Petros Faloutsos, and Frederic Pighin. Expressive speech-driven facial animation. _ACM Trans. Graph._, 24(4):1283-1302, oct 2005.\n' +
      '* [10] Peng Chen, Xiaobao Wei, Ming Lu, Yitong Zhu, Naiming Yao, Xingyu Xiao, and Hui Chen. Diffusiontalker: Personalization and acceleration for speech-driven 3d face diffuser, 2023.\n' +
      '* [11] Zhixiang Chen and Tae-Kyun Kim. Learning feature aggregation for deep 3d morphable models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 13164-13173, 2021.\n' +
      '* [12] Byoungwon Choe and Hyeong-Seok Ko. Analysis and synthesis of facial expressions with hand-generated muscle actuation basis. In _ACM SIGGRAPH 2006 Courses_, pages 21-es. 2006.\n' +
      '* [13] Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan, Bolun Cai, and Xiangmin Xu. Corttalk: Correlation between hierarchical speech and facial activity variances for 3d animation, 2023.\n' +
      '* [14] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, and Michael J. Black. Capture, learning, and synthesis of 3d speaking styles. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.\n' +
      '* [15] Radek Danecek, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael Black, and Timo Bolkart. Emotional speech-driven animation with content-emotion disentanglement. In _SIGGRAPH Asia 2023 Conference Papers_, SA \'23, New York, NY, USA, 2023. Association for Computing Machinery.\n' +
      '* [16] Pif Edwards, Chris Landreth, Eugene Fiume, and Karan Singh. Jali: an animator-centric viseme model for expressive lip synchronization. _ACM Transactions on graphics (TOG)_, 35(4):1-11, 2016.\n' +
      '* [17] Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter. 3d morphable face models--past, present, and future. _ACM Trans. Graph._, 39(5), jun 2020.\n' +
      '* [18] Paul Ekman and Wallace V Friesen. Facial action coding system. _Environmental Psychology & Nonverbal Behavior_, 1978.\n' +
      '* [19] Tony Ezzat and Tomaso Poggio. Miketalk: A talking facial display based on morphing visemes. In _Proceedings Computer Animation\'98 (Cat. No. 98EX169)_, pages 96-102. IEEE, 1998.\n' +
      '* [20] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Faceformer: Speech-driven 3d facial animation with transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.\n' +
      '* [21] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. Joint audio-text model for expressive speech-driven 3d facial animation. 5(1), may 2022.\n' +
      '* 598, 2010년 10월\n' +
      '* [23] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022.\n' +
      '* [24] Simon Giebenhain, Tobias Kirschstein, Markos Georgopoulos, Martin Runz, Lourdes Agapito, and Matthias Niessner. Learning neural parametric head models. In _Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [25] Kazi Injamamul Haque and Zerrin Yumak. Facex-hubert: Text-less speech-driven e(x)pressive 3d facial animation synthesis using self-supervised speech representation learning, 2023.\n' +
      '* [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020.\n' +
      '* [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In _NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications_, 2021.\n' +
      '* [28] Dong-Yan Huang, Ellensi Chandra, Xiangting Yang, Ying Zhou, Huaiping Ming, Weisi Lin, Minghui Dong, and Haizhou Li. Visual speech emotion conversion using deep learning for 3d talking head. In _Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data_, ASMMC-MMAC\'18, page 7-13, New York, NY, USA, 2018. Association for Computing Machinery.\n' +
      '* [29] Patrik Huber, Guosheng Hu, Rafael Tena, Pouria Mortazavian, Willem P Koppen, William J Christmas, Matthias Ratsch, and Josef Kittler. A multiresolution 3d morphable face model and fitting framework. In _International conference on computer vision theory and applications_, volume 5, pages 79-86. SciTePress, 2016.\n' +
      '* [30] Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, and Jonas Beskow. Let\'s face it: Probabilistic multimodal interlocutor-aware generation of facial gestures in dyadic settings. In _Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents_, IVA \'20, New York, NY, USA, 2020. Association for Computing Machinery.\n' +
      '* [31] G.A. Kalberer and L. Van Gool. Face animation based on observed 3d speech dynamics. In _Proceedings Computer Animation 2001. Fourteenth Conference on Computer Animation (Cat. No.01TH8596)_, pages 20-251, 2001.\n' +
      '* [32] Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen. Audio-driven facial animation by joint end-to-end learning of pose and emotion. _ACM Trans. Graph._, 36(4), jul 2017.\n' +
      '* [33] Akio Kodaira, Chenfeng Xu, Toshiki Hazama, Takanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori, Soichi Sugano, Hanying Cho, Zhijian Liu, and Kurt Keutzer. Streamdiffusion: A pipeline-level solution for real-time interactive generation. 2023.\n' +
      '* [34] Paul Koppen, Zhen-Hua Feng, Josef Kittler, Muhammad Awais, William Christmas, Xiao-Jun Wu, and He-Feng Yin. Gaussian mixture 3d morphable face model. _Pattern Recogn._, 74(C):617-628, feb 2018.\n' +
      '* [35] John P Lewis, Ken Anjyo, Taehyun Rhee, Mengjie Zhang, Frederic H Pighin, and Zhigang Deng. Practice and theory of blendshape facial models. _Eurographics (State of the Art Reports)_, 1(8):2, 2014.\n' +
      '* [36] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin, and Hao Li. Dynamic facial asset and rig generation from a single scan. _ACM Trans. Graph._, 39(6):215-1, 2020.\n' +
      '* [37] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chinara, Owen Ingraham, Pengda Xiang, Xinglei Ren, Pratusha Prasad, Bipin Kishore, Jun Xing, and Hao Li. Learning formation of physically-based face attributes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.\n' +
      '* [38] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4d scans. _ACM Trans. Graph._, 36(6), nov 2017.\n' +
      '* [39] Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, and Lan Xu. Omg: Towards open-vocabulary motion generation via mixture of controllers. _arXiv preprint arXiv:2312.08985_, 2023.\n' +
      '* ECCV 2022_, 페이지 612-630, Cham, 2022. Springer Nature Switzerland.\n' +
      '* [41] Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english. _PloS one_, 13(5):e0196391, 2018.\n' +
      '* [42] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11461-11471, June 2022.\n' +
      '* [43] Juan F Montesinos, Venkatesh S Kadandale, and Gloria Haro. A cappella: Audio-visual singing voice separation. In _32nd British Machine Vision Conference, BMVC 2021_, 2021.\n' +
      '* [44] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Modeling non-deterministic dyadic facial motion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 20395-20405, June 2022.\n' +
      '* [45] Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Self-talk: A self-supervised commutative training diagram to comprehend 3d talking faces, 2023.\n' +
      '* [46] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Emotalk: Speech-driven emotional disentanglement for 3d face animation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 20687-20697, 2023.\n' +
      '* [47] Hai Xuan Pham, Yuting Wang, and Vladimir Pavlovic. End-to-end learning for 3d facial animation from speech. In _Proceedings of the 20th ACM International Conference on Multimodal Interaction_, ICMI \'18, page 361-365, New York, NY, USA, 2018. Association for Computing Machinery.\n' +
      '* [48] Dafei Qin, Jun Saito, Noam Aigerman, Thibault Groueix, and Taku Komura. Neural face rigging for animating and retargeting facial meshes in the wild. _arXiv preprint arXiv:2305.08296_, 2023.\n' +
      '* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [50] Alexander Richard, Michael Zollhofer, Yandong Wen, Fernando de la Torre, and Yaser Sheikh. Meshtalk: 3d face animation from speech using cross-modality disentanglement. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 1173-1182, October 2021.\n' +
      '* [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.\n' +
      '* [52] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022.\n' +
      '* [53] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. _arXiv preprint arXiv:2303.01418_, 2023.\n' +
      '* [54] Il-Kyu Shin, A. Cengiz Oztireli, Hyeon-Joong Kim, Thabo Beeler, Markus Gross, and Soo-Mi Choi. Extraction and Transfer of Facial Expression Wrinkles for Facial Performance Enhancement. In John Keyser, Young J. Kim, and Peter Wonka, editors, _Pacific Graphics Short Papers_. The Eurographics Association, 2014.\n' +
      '* [55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265, Lille, France, 07-09 Jul 2015. PMLR.\n' +
      '* [56] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yumak. Facediffuser: Speech-driven 3d facial animation synthesis using diffusion. In _ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG \'23), November 15-17, 2023, Rennes, France_, New York, NY, USA, 2023. ACM.\n' +
      '* [57] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, and Yong Jin Liu. Diffposetalk: Speech-driven stylistic 3d facial animation and head pose generation via diffusion models, 2023.\n' +
      '* [58] Sarah Taylor, Taehwan Kim, Yisong Yue, Moshe Mahler, James Krahe, Anastasio Garcia Rodriguez, Jessica Hodgins, and Iain Matthews. A deep learning approach for generalized speech animation. _ACM Trans. Graph._, 36(4), jul 2017.\n' +
      '* [59] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. 2022.\n' +
      '* [60] Balamurugan Thambiraja, Sadegh Aliakbarian, Darren Cosker, and Justus Thies. 3diface: Diffusion-based speech-driven 3d facial animation and editing, 2023.\n' +
      '* [61] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh Aliakbarian, Darren Cosker, Christian Theobalt, and Justus Thies. Imitator: Personalized speech-driven 3d facial animation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 20621-20631, October 2023.\n' +
      '\n' +
      '- ECCV 2020_, pages 716-731, Cham, 2020. Springer International Publishing.\n' +
      '* [63] Luan Tran, Feng Liu, and Xiaoming Liu. Towards high-fidelity nonlinear 3d face morphable model. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1126-1135, 2019.\n' +
      '* [64] Luan Tran and Xiaoming Liu. Nonlinear 3d face morphable model. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7346-7355, 2018.\n' +
      '* [65] Luan Tran and Xiaoming Liu. On learning 3d face morphable model from in-the-wild images. _IEEE transactions on pattern analysis and machine intelligence_, 43(1):157-171, 2019.\n' +
      '* [66] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [67] Monica Villanueva Aylagas, Hector Anadon Leon, Mattias Teye, and Konrad Tollmar. Voice2face: Audio-driven facial and tongue rig animations with cvaes. _Computer Graphics Forum_, 41(8):255-265, 2022.\n' +
      '* ECCV 2020_, pages 700-717, Cham, 2020. Springer International Publishing.\n' +
      '* [69] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan Bali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timothy Godisart, Hyowon Ha, Xuhua Huang, et al. Multiface: A dataset for neural face rendering. _arXiv preprint arXiv:2207.11243_, 2022.\n' +
      '* [70] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven 3d facial animation with discrete motion prior. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12780-12790, 2023.\n' +
      '* [71] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Comput. Surv._, 56(4), nov 2023.\n' +
      '* [72] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, and Christian Theobalt. i3dmm: Deep implicit 3d morphable model of human heads. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12803-12813, 2021.\n' +
      '* [73] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J. Black. Generating holistic 3d human motion from speech, 2023.\n' +
      '* [74] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J. Black. Generating holistic 3d human motion from speech. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 469-480, June 2023.\n' +
      '* [75] Chenxu Zhang, Saifeng Ni, Zhipeng Fan, Hongbo Li, Ming Zeng, Madhukar Budagavi, and Xiaohu Guo. 3d talking face with personalized pose dynamics. _IEEE Transactions on Visualization and Computer Graphics_, 29(2):1438-1449, 2023.\n' +
      '* [76] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. Dreamface: Progressive generation of animatable 3d faces under text guidance. _arXiv preprint arXiv:2304.03117_, 2023.\n' +
      '* [77] Longwen Zhang, Chuxiao Zeng, Qixuan Zhang, Hongyang Lin, Ruixiang Cao, Wei Yang, Lan Xu, and Jingyi Yu. Video-driven neural physically-based facial asset for production. volume 41, pages 1-16. ACM New York, NY, USA, 2022.\n' +
      '* [78] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation, 2022.\n' +
      '* [79] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.\n' +
      '* [80] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen. Imface: A nonlinear 3d morphable face model with implicit neural representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20343-20352, 2022.\n' +
      '* [81] Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, and Karan Singh. Visemenet: Audio-driven animator-centric speech animation. 37(4), jul 2018.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>