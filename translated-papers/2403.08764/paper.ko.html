<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VLOGGER: 구현된 아바타 합성을 위한 멀티모달 확산\n' +
      '\n' +
      'Enric Corona\n' +
      '\n' +
      'Nikos Kolotouros\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '[https://enriccorona.github.io/vlogger/](https://enriccorona.github.io/vlogger/)\n' +
      '\n' +
      'Andrei Zanfir\n' +
      '\n' +
      'Hiemo Alldieck\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '[https://enriccorona.github.io/vlogger/](https://enriccorona.github.io/vlogger/)\n' +
      '\n' +
      '에다드 가브리엘 바자반\n' +
      '\n' +
      'Hiemo Alldieck\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '[https://enriccorona.github.io/vlogger/](https://enriccorona.github.io/vlogger/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 논문에서는 최근 생성 확산 모델의 성공을 기반으로 한 사람의 단일 입력 영상으로부터 오디오 기반 인간 영상 생성 방법인 VLOGGER을 제안한다. 제안하는 방법은 1) 확률적 인간-3d-움직임 확산 모델, 2) 공간 및 시간 제어로 텍스트-이미지 모델을 증강하는 새로운 확산 기반 구조로 구성된다. 이는 인간의 얼굴과 신체의 높은 수준의 표현을 통해 쉽게 제어할 수 있는 가변 길이의 고품질 비디오 생성을 지원한다. 이전 작업과 달리,\n' +
      '\n' +
      '그림 1: VLOGGER는 오디오로부터 인간을 합성하는 새로운 프레임워크이다. 첫 번째 열에 표시된 것과 같은 단일 입력 이미지와 샘플 오디오 입력이 주어지면, 본 방법은 대화하고 생생하게 움직이는 사람의 사실적이고 시간적으로 일관된 비디오를 생성한다. 오른쪽 열의 합성 이미지에서 볼 수 있듯이 머리 움직임, 시선, 깜박임, 입술 움직임을 생성하고 이전 방법, 상반신 및 손 제스처와 달리 오디오 기반 합성을 한 단계 더 수행한다.\n' +
      '\n' +
      '우리의 방법은 각 사람에 대한 훈련을 필요로 하지 않고, 얼굴 검출 및 크롭에 의존하지 않고, 완전한 이미지(얼굴 또는 입술뿐만 아니라)를 생성하고, 통신하는 인간을 올바르게 합성하는데 중요한 광범위한 시나리오(_예: 가시 몸통 또는 다양한 피사체 아이덴티티)를 고려한다. 또한 3D 포즈와 표현 주석이 있는 새롭고 다양한 데이터 세트인 MENTOR을 큐레이션하며, 이전(800,000개의 아이덴티티)보다 10배 더 크고 동적 제스처로 주요 기술적 기여를 훈련하고 제거한다.\n' +
      '\n' +
      'VLOGGER은 이미지 품질, 신원 보존 및 시간적 일관성을 고려하면서 상체 제스처도 생성하면서 세 가지 공개 벤치마크에서 최첨단 방법을 능가한다. 다중 다양성 메트릭에 대한 VLOGGER의 성능을 분석하여, 우리의 아키텍처 선택과 MENTOR의 사용이 규모에서 공정하고 편향되지 않은 모델임을 보여준다. 마지막으로 비디오 편집 및 개인화에서 응용 프로그램을 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트나 오디오를 기반으로 대화와 움직이는 사람의 동영상을 자동으로 생성하고 그 사람의 단일 이미지만 제공하는 방법인 VLOGGER을 제시한다. 콘텐츠 제작, 엔터테인먼트 또는 게임과 같은 산업은 모두 인간 합성에 대한 수요가 높습니다. 그러나, 인간의 사실적인 비디오의 생성은 여전히 복잡하고 유물로 무르익었다. 이를 위해서는 현실적인 결과를 위한 상당한 수동 개입이 필요하다. 그러나 완전한 자동화는 창의적인 프로세스를 용이하게 할 뿐만 아니라 향상된 온라인 통신, 교육 또는 개인화된 가상 비서와 같은 완전히 새로운 사용 사례를 가능하게 한다. 최근 채팅 에이전트의 성공을 고려할 때 후자는 특히 관련이 있다[43, 50]. 연구는 그러한 해결책들이 공감을 발전시킬 만큼 충분히 자연스럽다고 인식되지 않는다는 것을 보여주었고[103], 몇몇 저자들[37]은 의인화와 행동적 사실주의(_예를 들어, 시선, 표정, 전신 움직임, _etc._)를 주장한다. 사회적 존재감을 창출하고 사용자로부터 공감적 반응을 이끌어내는 데 중요하다. 이러한 기능은 고객 서비스[1, 53], 원격 진료[62], 교육[61] 또는 인간-로봇 상호 작용[58]과 같은 영역에서 에이전트를 광범위하게 채택하는 결과를 초래할 것이다. 이 작업에서 우리가 목표로 하는 것은 정확히 자동화와 행동 현실주의이다: VLOGGER은 복잡한 얼굴 표정과 증가하는 신체 움직임 수준을 특징으로 하는 오디오 및 애니메이션 시각적 표현이 장착된 _embodied 대화 에이전트_[74]에 대한 멀티모달 인터페이스이며, 인간 사용자와의 자연스러운 대화를 지원하도록 설계되었다. VLOGGER은 프레젠테이션, 교육, 내레이션, 낮은 대역폭의 온라인 통신을 위한 독립형 솔루션으로, 텍스트 전용 HCI를 위한 인터페이스로 사용될 수 있다[3, 100]. 본 논문에서는 동영상 편집 작업에서 그 가능성을 추가적으로 설명한다.\n' +
      '\n' +
      '멀티모달, 사실적 인간 합성은 데이터 획득, 자연스러운 방식으로 얼굴 표정 제정, 오디오 동기화, 폐색 또는 전신 움직임 표현과 같은 어려움으로 인해 복잡하다. 많은 시도들은 운전 비디오의 입 영역을 편집함으로써 립싱크[75, 54, 82]에만 집중되었다. 최근, [93, 95]는 얼굴 재연[9, 19, 29, 49, 69, 87, 96]의 광범위한 발전에 의존하여 오디오로부터 얼굴 동작을 예측함으로써 단일 이미지로부터 대화 헤드 비디오를 생성한다. 시간적 일관성은 일반적으로 얼굴 키포인트로부터의 부드러운 안내 모션에 의존함으로써 프레임당 이미지 생성 네트워크에 의해 달성된다. 그러나 이는 흐릿함을 유발할 수 있으며 얼굴에서 더 멀리 떨어진 영역에서 시간적 일관성을 보장하지 않는다. 결과적으로 대부분의 방법은 신체의 상당 부분이 보일 때마다 머리를 감지하고 잘라야 한다. 본 논문에서는 사람이 몸짓, 시선, 눈 깜박임 또는 포즈를 통해 자신의 몸을 사용하여 의사소통하는 것은 입술과 얼굴 동작이 결합된 "단지" 오디오 이상의 의사소통이라고 주장한다. MODA[40]은 최근 얼굴과 신체 모두의 애니메이션을 제한된 시나리오에서 그리고 새로운 아이덴티티로 일반화하지 않고 탐구하기 시작했다. 대조적으로, 우리는 머리 및 손 제스처를 모두 포함하는 동작의 사실성과 다양성에 초점을 맞춘 _일반, 사람 불가지론 합성 솔루션을 목표로 한다. 우리의 목표는 아이덴티티나 포즈에 대한 제어 없이 동적 비디오를 생성할 수 있는 최근의 비디오 합성 노력[2, 6, 36, 64]과 제어 가능한 이미지 생성 방법[9, 19, 59] 사이의 격차를 해소하는 것이다.\n' +
      '\n' +
      '이를 위해 먼저 생성된 확산 기반 네트워크가 입력 오디오 신호에 따라 신체 움직임과 표정을 예측하는 2단계 접근법을 제안한다. 이러한 확률론적 접근은 말과 포즈, 시선, 표현 사이의 미묘한(일대다) 매핑을 모델링하기 위해 필요하다. 둘째, 최근 이미지 확산 모델을 기반으로 한 새로운 아키텍처를 제안하며, 이 아키텍처는 시간적, 공간적 영역에서 제어를 제공한다. 사전 훈련 중에 획득한 생성적 인간 전적에 추가로 의존함으로써, 우리는 이 결합된 아키텍처가 일관된 인간 이미지(예: 눈)를 생성하기 위해 종종 어려움을 겪는 이미지 확산 모델의 용량을 향상시키는 방법을 보여준다. VLOGGER은 고품질 비디오를 얻기 위해 기본 모델과 초해상도 확산 모델로 구성된다. 이전 작업과 같이 얼굴 표정뿐만 아니라 몸과 손까지 포함한 전신을 나타내는 2d 컨트롤에 영상 생성 과정을 조정한다. 임의의 길이의 비디오들을 생성하기 위해, 우리는 이전 프레임들에 기초하여 새로운 비디오 클립들을 컨디셔닝하기 위한 시간적 아웃페인팅 접근법을 따른다. 마지막으로, VLOGGER의 유연성은 입술 또는 얼굴 영역과 같은 입력 비디오의 특정 부분을 편집할 수 있게 한다.\n' +
      '\n' +
      '견고성과 일반화를 위해 피부톤, 신체 포즈, 시점, 음성 및 신체 가시성 측면에서 이전에 사용 가능한 데이터보다 훨씬 더 큰 다양성을 특징으로 하는 대규모 데이터 세트를 큐레이션한다. 이전 시도들과 대조적으로, 데이터 세트는 또한 인간 통신의 복잡성을 학습하는 데 중요한 동적 손 제스처를 갖는 비디오들을 포함한다. VLOGGER은 다양한 다양성 메트릭에서 이전 작업을 능가하고 이전 HDTF[97] 및 TalkingHead-1KH[79] 데이터 세트에서 최첨단 이미지 품질 및 다양성 결과를 얻는다. 또한, 머리 움직임과 상체 움직임의 고해상도 영상을 생성하고, 상당히 다양한 표정과 제스처를 특징으로 함으로써, 기준선보다 더 큰 범위의 시나리오를 고려한다. 마지막으로 실험 섹션에서는 다양한 시나리오에 적응할 수 있는 VLOGGER의 유연성과 능력을 입증하기 위해 다운스트림 응용 프로그램을 탐구한다. 예를 들어, VLOGGER는 인페인트에 의한 비디오 편집에 사용될 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 오디오 입력 장치. 후속 작업은 중간 2d, 3d 랜드마크 또는 흐름 기반 표현을 사용하여 머리 움직임, 시선 및 깜박임[32, 41, 56, 67, 98, 102]과 같은 확장된 특징을 추가했다. 실사 수준을 높이기 위해 많은 작업에서 손실[8, 9, 17, 55, 80, 92]의 일부로 판별기를 광범위하게 사용했으며 최근 일부 방법에서는 확산 모델의 사용을 제안했다[65, 66, 93]. 그러나 GAN[20, 34] 또는 일반 확산 모델의 잠재 공간에서 작동할 때 신체, 머리 움직임, 깜박임, 시선 및 표정 간의 적절한 엉킴을 보장하기 어렵다. 우리의 방법은 사용자 지정 지각, 시선, 신원 보존 또는 립싱크 손실을 사용할 필요가 없다. 신체 움직임과 제스처는 데이터의 부족과 일관된 비디오 생성의 어려움으로 인해 고려되지 않았다. 우리는 대규모 데이터 세트를 큐레이션하고 이 문제를 위한 완전한 파이프라인을 제안한다. VLOGGER은 다양한 표정, 머리와 몸의 움직임, 시선, 눈 깜박임과 정확한 입술 움직임으로 일관된 얼굴과 상체 움직임을 생성할 수 있다. 또한, 본 논문에서 제안하는 방법은 다양한 다양성 축에 걸쳐 더욱 표현력이 뛰어나고 강인함을 보인다.\n' +
      '\n' +
      '**Face Reenactment.** Video-based talking face generation은 소스 비디오의 동작을 타겟 사람에게 전달하는 것을 목표로 하며, 과거에는 널리 탐색되어 왔다[9, 23, 28, 29, 49, 69, 81, 87, 96, 101]. 대부분의 방법들은 중간 표현, 예컨대 희박하거나 조밀한 랜드마크들, 시맨틱 마스크들, 3d 조밀한 표현들 또는 뒤틀린 특징들에 의존한다. 3d 도메인에서, 여러 작업들이 NeRF[4, 44] 기반 솔루션[22, 39, 88, 89]을 이용했다. 그러나, 이것은 그들을 재훈련하고 애니메이션화하기 위해, 대화하는 타겟 사람의 상당한 양의 프레임을 필요로 한다. 이 작업은 우리와 밀접한 관련이 있으며 일부 이전 작업은 오디오를 입력으로 고려할 때 이러한 중간 표현을 적용한다. 그러나 우리의 경우 얼굴 전용 비디오에서 앞으로 나아가고 더 다양한 입력 샘플인 _e.g_를 고려하는 것을 목표로 한다. 몸과 모발 움직임이 포함되어 있습니다.\n' +
      '\n' +
      '**비디오 생성.** 또한 우리의 작업과 관련된 것이 비디오 생성의 주제이다. 이는 지역사회에서 광범위하게 탐구되어 온 과제이므로 가장 관련된 방향에만 초점을 맞춘다. 텍스트-이미지 확산 모델[16]의 성공으로 많은 작품들도 비디오 도메인[2, 6, 24, 26, 35, 36, 64, 72, 83]으로의 확장을 탐구했지만 대부분은 초 수나 해상도에 제한이 있다. 더욱이, 대부분의 이전 작업들은 이용 가능한 데이터의 양에도 불구하고 인간을 명시적으로 다루지 않는다. 이 경우, 시공간 제어를 추가하여 현재 최신 이미지 확산 모델을 시간 영역으로 확장하고 가변 길이의 비디오를 생성하기 위한 반복적인 아웃페인팅 절차를 제안한다. 동시 작업은 보다 일반적인 시나리오를 위해 유사한 네트워크 아키텍처[2, 64]를 탐색하지만, 우리의 목표는 1) 포즈된 3D 바디 모델의 밀집 렌더 및 2) 뒤틀린 참조 이미지로 각 프레임을 매개변수화하여 대화하는 사람을 애니메이션화하는 것이다. 이러한 제어는 실험 섹션에서 절제된 바와 같이 생성 과정을 더 안정적으로 만든다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '본 연구의 목적은 현실감 있는 머리동작과 몸짓으로 사람의 말을 합성하는 가변길이의 실사영상(\\(\\mathbf{V}\\)을 생성하는 것이다. VLOGGER이라고 하는 우리의 프레임워크는 그림 2에 나와 있다. VLOGGER은 음성에서 비디오로의 일대다 매핑을 나타내기 위해 확률적 확산 모델에 기반한 2단계 파이프라인이다. 첫 번째 네트워크는 샘플 속도\\(S\\)에서 오디오 파형\\(\\mathbf{a}\\in\\mathbb{R}^{NS}\\)을 입력으로 하여 목표 비디오 길이\\(N\\)에 걸쳐 시선, 표정 및 3D 포즈를 담당하는 중간 신체 모션 컨트롤\\(\\mathbf{C}\\)을 생성한다. 제2 네트워크는 큰 이미지 확산 모델들을 확장하는 시간적 이미지-이미지 변환 모델로서, 예측된 바디 컨트롤들을 취하여 대응하는 프레임들을 생성한다. 프로세스를 특정 아이덴티티로 컨디셔닝하기 위해, 네트워크는 또한 사람의 참조 이미지를 취한다. 우리는 새로 도입된 MENTOR 데이터셋(SS3.3)에서 VLOGGER을 훈련한다. 다음으로 두 네트워크를 설명한다.\n' +
      '\n' +
      '### 오디오 기반 모션 생성\n' +
      '\n' +
      '**아키텍처.** 우리의 파이프라인 \\(M\\)의 첫 번째 네트워크는 입력 음성을 기반으로 주행 동작을 예측하도록 설계되었다. 또한 텍스트-음성 변환 모델을 통한 입력 텍스트를 고려하여 입력을 파형으로 변환하고[70], 결과 오디오를 표준 멜-스펙트로그램으로 나타낸다. \\ (M\\)은 시간차원에서 4개의 멀티헤드 어텐션 레이어를 갖는 트랜스포머 아키텍처[71]를 기반으로 한다. 프레임 수 및 확산 단계에 대한 위치 인코딩을 포함하고, 그리고\n' +
      '\n' +
      '도 2: **High-level overview.** VLOGGER은 통계적 3D 바디 모델을 사용하여 비디오 생성 프로세스를 조건화한다. 입력 영상\\(\\mathbf{I_{ref}}\\)(왼쪽)이 주어지면, 예측된 형상 파라미터들은 타겟 아이덴티티의 기하학적 특성들을 인코딩한다. 먼저, 네트워크(M\\)는 입력 음성의 Mel-Spectrogram\\(\\mathbf{a}\\)을 취하여 3차원 얼굴 표정\\(\\left\\theta_{i}^{e}\\right\\}_{1\\leq i\\leq N}\\)과 신체 포즈\\(\\left\\theta_{i}^{b}\\right\\\\leq i\\leq N}\\)의 시퀀스를 생성한다. 움직이는 3차원 물체의 조밀한 표현을 렌더링하여 영상 생성 단계에서 2차원 컨트롤(\\(\\left\\{\\mathbf{C}_{i}\\right\\}_{1\\leq i\\leq N}\\) 역할을 한다. 대상체의 참조 영상과 함께 시간 확산 모델 및 초해상도 모듈에 입력으로 주어지며, 대상체의 실사 재연(\\(\\left\\{\\mathbf{G}_{i}\\right\\}_{1\\leq i\\leq N}\\) 시퀀스를 생성하도록 학습된다. Sup의 구현 세부 정보입니다. 매트\n' +
      '\n' +
      '상기 입력 오디오 및 상기 확산 단계에 대한 임베딩 MLP를 포함하는, 방법. 각 프레임에서 인과 마스크를 사용하여 모델이 이전 프레임에만 참석하도록 한다. 모델은 _e.g_와 같이 매우 긴 시퀀스의 생성을 가능하게 하기 위해 가변 길이 비디오를 사용하여 트레이닝된다. In the TalkingHead-1KH Dataset [79] (SS4 참조).\n' +
      '\n' +
      '우리는 합성 비디오에 대한 중간 제어 표현을 생성하기 위해 통계적 및 표현적 3D 바디 모델[33, 51, 63, 85]의 추정된 파라미터에 의존한다. 이러한 모델은 얼굴 표정과 신체 동작을 모두 고려하여 보다 표현적이고 역동적인 제스처로 인간 합성의 문을 열어준다. 움직임 생성 네트워크는 프레임 내 입력 오디오\\(\\mathbfa}_{i}\\)을 기반으로 얼굴 및 신체 파라미터\\(M(\\mathbfa}_{i})=\\{\\theta_{i}^{e},\\Delta\\theta_{i}^{b}\\})를 예측한다. 특히, 모델은 표현식\\(\\theta_{i}^{e}\\)과 신체 포즈에 대한 잔차를 생성한다. 변위를 예측하여 _i.e_\\ (\\Delta\\theta_{i}^{b}\\) 모델은 대상 피사체에 대해 기준 포즈(\\theta_{\\text{ref}}^{b}\\)로 입력 영상을 촬영하고, 프레임(1\\leq i\\leq N\\)에 대해 상대적으로 사람(\\theta_{i}^{b}=\\theta_{\\text{ref}}^{b}+\\Delta\\theta_{i}^{b}\\)을 애니메이트할 수 있다. 기하학적 영역에서 사람의 정체성은 체형 코드에 의해 모델링된다. 훈련과 테스트 모두 입력 영상에 파라메트릭 신체 모델을 피팅하여 얻은 추정된 3차원 형상 파라미터를 사용한다. CNN 기반 구조로 2D/3D 예측을 활용하기 위해 예측된 표현과 포즈 파라미터를 이용하여 모델을 포즈하고 포즈된 몸체의 템플릿 정점 위치를 조밀한 표현으로 래스터화하여 조밀한 마스크\\(\\big{\\{}\\mathbfC}_{i}^{d}\\big{\\}_{1\\leq i\\leq N}\\in\\mathbb{R}^{H\\times W\\times 3}\\)을 얻는다. 또한, 서로 다른 의미 클래스에 대해 본문의 의미영역인 \\(\\{\\mathbf{C}_{i}^{m}\\}_{1\\leq i\\leq N}\\in\\{0,1\\}^{H\\times W\\times N_{c}\\)을 래스터화한다.\n' +
      '\n' +
      '더욱이, 이전의 얼굴 재연 작업은 종종 뒤틀린 이미지[19, 76, 95, 99]에 의존하지만, 인간 애니메이션을 위한 확산 기반 아키텍처에서 간과되어 왔다[10, 30, 78]. 이 두 표현 사이의 간격을 좁히는 방법을 제안하고 와핑된 이미지를 사용하여 생성 프로세스를 안내하며, 이는 네트워크의 작업을 용이하게 하고 주제 ID를 보존하는 데 도움이 된다(탭. 3 참조). 참조영상에서 볼 수 있는 각 바디 정점에 픽셀 컬러를 할당하고, 각 새로운 프레임에서 바디를 렌더링하여 부분 휨(\\{\\mathbf{C}_{i}^{w}\\}_{1\\leq i\\leq N}\\in\\mathbb{R}^{H\\times W\\times 3}\\)을 얻는다. 모든 렌더들에 대해, 래스터화 프로세스는 트레이닝 비디오 또는 참조 이미지 중 하나로부터 추론된 대각선 시야를 갖는 전체 투시도 카메라를 가정한다. 그림은 그림 2를 참조하십시오. 다음 섹션과 Sup에서 시간 이미지 확산 모델을 설명합니다. 매트 또한 실험 섹션에서 조밀한 표현과 왜곡된 이미지의 사용을 줄인다.\n' +
      '\n' +
      '####3.2.1 손실함수.\n' +
      '\n' +
      '이 모델은 가우스 잡음(\\epsilon\\sim\\mathcal{N}(0,1)\\)을 접지-진리 샘플(x_{0}=\\{\\big\\theta_{i}^{e},\\Delta\\theta_{i}^{big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\big\\b 본 연구의 목적은 잡음입력(x_{t}\\)으로부터 부가된 잡음을 예측하는 잡음제거 네트워크(\\(\\epsilon_{\\phi}\\)를 학습하여 실제 머리와 몸체의 움직임 분포, \\(x_{0}\\sim q(x_{0}|\\mathbf{a})\\을 모델링하는 것이다. 여기서 \\(t\\)은 임의의 확산단계이다. 우리의 경우, 지상-진리 분포를 직접 예측하여 더 나은 성능을 얻었다.\n' +
      '\n' +
      '[\\mathcal{L}_{\\text{diff}=\\mathbb{E}_{x_{0},t,\\mathbf{a},\\epsilon\\sim\\mathcal{N}(0,1}\\Big{[}\\left\\lVert x_{0}-\\epsilon_{\\phi}(x_{t},t,\\mathbf{a})\\right\\rVert_{2}^{2}\\Big{}}. \\tag{1}\\maccal{N}(0,1}\\Big{[}\\left\\lVert x_{0}-\\epsilon_{\\phi}(x_{t},t,\\mathbf{a}}\\right\\rVert_{2}^{2}\\Big{}}\\tag{1}\\maccal{N}(0,1}}\\Big{[}\\left\\lVert x_{0}-\\epsilon_{\\phi}(x_{t},t,\\mathbf 또한 연속 프레임에서의 예측 차이를 벌하기 위한 추가적인 시간적 손실을 포함한다. \\(\\mathcal{L}_{\\text{temp}=\\left\\|\\epsilon_{\\phi}(x_{t},t,\\mathbf{a})_{i+1}-\\epsilon_{\\phi}(x_{t},t,\\mathbf{a})_{i}\\right\\|_{2}^{2}\\), 임의의 주어진 프레임(i\\in N\\)에 대해, {i.e._\\(\\mathcal{L}_{\\text{diff}+\\lambda_{\\text{temp}}\\mathcal{L}_{\\text{temp}}). 실제로 우리는 표정과 신체 포즈에 대해 서로 다른 시간적 손실 가중치를 사용하여 얼굴 표정에 대해 더 큰 역동성을 허용하면서 머리와 손에 대해 더 부드러운 동작을 보장한다.\n' +
      '\n' +
      '시각적 말과 움직이는 인간의 생성\n' +
      '\n' +
      '#### 3.2.1 Architecture.\n' +
      '\n' +
      '우리의 다음 목표는 사람의 입력 이미지\\(\\mathbf{I_{ref}}\\)를 애니메이션하여 이전에 예측된 신체 및 얼굴 동작을 따르도록 하는 것이며, 이는 의미적, 희소하고 조밀한 마스크\\(\\mathbf{C}\\)로 표현된다. 이러한 이미지 기반 제어를 기반으로, 우리는 시간적으로 인식 가능한 최첨단 확산 모델 확장을 제안한다[60]. ControlNet[94]에서 영감을 얻어, 초기 학습 모델을 동결하고 입력 시간 제어 \\(\\mathbf{C}\\)를 취하는 인코딩 레이어의 제로 초기화 학습 가능 복사본을 만든다. 우리는 그림 2와 같이 각 다운샘플링 블록의 첫 번째 레이어 이후와 두 번째 그룹Norm 활성화 전에 시간 도메인에서 1d 컨볼루션 레이어를 인터리빙한다. 네트워크는 \\(N\\)개의 연속적인 프레임과 컨트롤을 취하여 훈련되고, 입력 컨트롤에 따라 애니메이션된 참조인의 짧은 클립을 생성하도록 작업된다.\n' +
      '\n' +
      '#### 3.2.2 Training.\n' +
      '\n' +
      '우리는 고유한 인간 피험자의 전장 비디오로 구성된 MENTOR 데이터 세트에서 방법을 훈련한다. 학습 중에 네트워크는 연속된 프레임들의 시퀀스와 사람의 임의의 참조 영상\\(\\mathbf{I_{ref}}\\)을 취하기 때문에 이론적으로 임의의 비디오 프레임을 참조로서 할당할 수 있다. 실제로, 우리는 더 가까운 예가 훈련을 하찮게 하고 덜 일반화 가능성을 제공하기 때문에 목표 클립에서 더 멀리(시간적으로) 떨어져 있는 참조를 샘플링한다. 네트워크는 단일 프레임에서 새로운 제어 계층[94]을 먼저 학습하고, 시간 성분을 추가하여 비디오에 대해 나중에 학습함으로써 두 단계로 학습된다. 이를 통해 첫 번째 단계에서 큰 배치 크기를 사용하고 헤드 재연 작업을 더 빨리 학습할 수 있습니다. 학습률 5e-5를 갖는 이미지 모델들을 배치 크기 128의 \\(400k\\) 단계에 대해 두 단계로 학습한다. 우리는 표 3의 이 훈련 일정의 효과를 제거하며 훈련 절차에 대한 자세한 내용은 Sup에 제공된다. 매트\n' +
      '\n' +
      '#### 3.2.3 손실 기능.\n' +
      '\n' +
      '이전 섹션 및 Eq에 설명된 손실과 유사하다. (1) 지상진실영상(\\(\\mathbf{I}\\)에 잡음(\\epsilon^{I}\\)을 더하는 확산과정을 따른다. 우리는 내부 데이터 소스에 대해 훈련된 Imagen [60] 버전을 기반으로 추가 노이즈\\(\\epsilon^{I}\\)을 예측한다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{diff}}^{I}=\\mathbb{E}_{x_{0}^{I},t,\\mathbf{C},\\epsilon^{I}\\sim\\mathcal{N}(0,1)}\\Big{[}\\left\\|\\epsilon^{I}-\\epsilon_{\\phi}^{I}(x_{t}^{I},t,\\mathbf{C}\\right\\|_{2}^{2}\\Big{}}. \\tag{2}\\sim\\mathcal{N}(0,1)}\\Big{[}\\left\\|\\epsilon^{I}-\\epsilon_{\\phi}^{I}(x_{t}^{I},t,\\mathbf{C}\\right\\|_{2}^{2}\\Big{}}\\tag{2}\\sim\\mathcal{N}(0,1)}\\Big{[}\\left\\|\\ep\n' +
      '\n' +
      '####3.2.4 Super Resolution.\n' +
      '\n' +
      '기존의 방법은 해상도 독립적이지만, 기본 비디오는 128(128\\times 128\\) 해상도로 생성하고, 더 높은 품질의 비디오는 256(256\\times 256\\) 또는 512(512\\times 512\\)에서 두 개의 초해상도 변형에서 시간적 컨디셔닝을 확장하기 위해 캐스케이드 확산 접근법을 사용한다. 생성된 이미지는 \\(\\left\\{\\mathbf{G}_{i}\\right\\}_{1\\leq i\\leq N}\\)으로 표시된다. 고해상도 예가 도 1에 도시되어 있다. 도 1 및 도 4에 도시된 바와 같다.\n' +
      '\n' +
      '추론 과정에서 시간적 아웃페인팅(Temporal Outpainting)은 고정된 프레임 수\\(N\\)만을 생성하도록 학습되므로 가변 길이 비디오로 확장하는 방법은 명확하지 않다. 대부분의 이전의 확산-기반 비디오 생성 방법들은 짧은 클립들[27, 35, 83]로 제한되거나 평활하게 생성된 중간 토큰 표현들[72]에 의존하지만, 픽셀 도메인의 평활한 변화들의 보장 없이. 여기서는 시간적인 아웃페인팅의 개념을 탐색한다. 먼저 \\(N\\) 프레임을 생성한 다음, 이전의 \\(N-N^{\\prime}<N\\) 프레임을 기반으로 반복적으로 아웃페인팅한다. 연속된 두 개의 클립(N-N^{\\prime}\\) 사이의 중첩량은 품질과 실행 시간 사이의 절충으로 선택된다. 우리는 각 비디오 클립을 생성하기 위해 DDPM을 사용하며, 이러한 접근법이 수천 개의 프레임으로 확장될 수 있음을 보여준다. 자세한 내용은 탭의 절제를 참조하십시오. 2에서 우리는 주요 디자인 선택을 검증하고 우리의 최종 네트워크가 인간의 사실적이고 시간적으로 일관된 비디오를 생성할 수 있음을 보여준다.\n' +
      '\n' +
      '### MENTOR Dataset\n' +
      '\n' +
      '우리는 주로 카메라를 향하는 단일 스피커가 포함된 내부 비디오의 대규모 저장소에서 MENTOR 데이터셋을 몸통 위에서 영어로 의사소통하는 것으로 큐레이션합니다. 비디오에는 24 fps(10초 클립)에서 240프레임, 16kHz에서 오디오가 포함되어 있습니다.\n' +
      '\n' +
      '전신 커뮤니케이션 인간 모델링을 목표로 3차원 신체 관절과 손을 추정하고 연속된 프레임 간의 투영 오차와 시간적 차이를 최소화하여 통계적 관절형 3차원 신체 모델에 부합한다. 우리는 배경이 의미 있게 변하고 얼굴이나 신체가 부분적으로만 감지되거나 손은 완전히 감지되지 않는 _jittery_인 비디오를 필터링한다(예를 들어, 사람이 물체를 잡고 조작하는 경우). 또는 오디오의 품질이 낮습니다. 이 과정을 통해 8M 초(2.2K 시간)와 800K 아이덴티티의 학습 세트가 생성되었고, 120시간과 \\(\\sim\\)4K 아이덴티티의 테스트 세트가 생성되어 더 높은 해상도에서 아이덴티티 및 길이 측면에서 현재까지 사용된 가장 큰 데이터 세트가 되었다. 더욱이, MENTOR 데이터세트는 광범위한 피험자(_e.g_. 피부톤, 나이)를 포함한다. , 시점 또는 신체 가시성 중 적어도 하나를 포함하는 것을 특징으로 하는 방법. 기존 데이터 세트에 대한 통계 및 광범위한 비교가 Sup에 제공된다. 매트 우리는 선별된 비디오 ID, 얼굴 맞춤 및 추정된 신체 포즈를 광범위한 연구 커뮤니티에 공개하는 것을 목표로 한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      'Data and Training.We trained VLOGGER on the MENTOR dataset as Sec. 3.3, the base resolution of \\(128\\times 128\\) and cascade resolution at \\(256\\times 256\\) and \\(512\\times 512\\)에서 훈련한다. 평가는 HDTF[97], TalkingHead-1KH[79] 및 MENTOR의 테스트 세트에 대해 수행된다. 또한 MENTOR 데이터 세트의 다양한 시나리오에서 본 방법의 성능을 축소하고 연령, 인식된 성별 또는 피부 톤과 같은 여러 다양성 메트릭에 걸쳐 기준선에 대한 성능을 보고한다.\n' +
      '\n' +
      '기준.우리는 여러 최첨단 방법, 즉 [42, 76, 77, 95, 104]와 비교한다. 우리의 방법과 달리 모든 기준선은 머리만 감지하고 애니메이션할 수 있기 때문에 얼굴 영역을 잘라야 한다.\n' +
      '\n' +
      '**메트릭.** 생성된 비디오의 이미지 품질, 립싱크, 시간적 일관성 및 신원 보존을 평가하기 위해 메트릭의 조합에 의존한다. 화질의 경우, FID 점수[25]는 지상-진실과 생성된 이미지 분포 사이의 거리를 측정하는 반면, CPUBD(Cumulative Probability of Blur Detection) [47, 48] 및 NIQE(Natural Image Quality Evaluator) [45]는 생성된 이미지의 품질을 검증한다. 말하는 얼굴 생성의 문헌에 이어, 다음으로 얼굴 랜드마크 좌표를 추정하고 입 정점 위치(LME)의 차이를 보고하여 립싱크 품질을 측정한다. 우리는 또한 LSE-D[12] 점수를 보고한다. 마찬가지로, 생성된 비디오에서 시간적 평활도를 측정하기 위해 [91] 이후의 지터(또는 _jerk_) 오류를 보고한다. 또한, 음성-대-비디오가 항상 일대일 매핑이 아니며 사실적인 비디오의 분포를 생성하는 것이 중요하다는 점에서, 표정 및 시선 측면에서 다양성을 평가하기 위해 생성된 비디오로부터 예측된 표정 파라미터의 표준 편차를 제공한다. 신체 및 손 동작의 다양성과 관련하여 VLOGGER은 제스처를 고려한 첫 번째 모델이며 이를 정성적으로 평가한다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '표 2 및 표 3. 탭에서 주요 설계 선택을 광범위하게 삭제합니다. 도 2는 전체 방법(마지막 행)에 대한 가장 대표적인 메트릭을 요약하고, 각 행은 하나의 특징(_e.g_. 모션 예측기를 트레이닝할 때 시간적 손실을 사용하지 않음)을 변경하는 효과를 나타낸다. 탭 도 3은 비디오를 생성하는 데 사용되는 2d 컨트롤의 중요성을 검증한다. 우리는 그 결과에 대해 다음에 논의합니다.\n' +
      '\n' +
      '**동작 생성** 탭의 상위 부분에 있습니다. 도 2는 시간적 손실을 사용하지 않거나 \\(\\Delta\\)을 예측하지 않을 때 시간적 일관성의 저하를 보여준다(Sec 3.1 참조). 네트워크는 신체 움직임 이상의 잔차를 예측할 때 평활성과 안정성이 향상되어 전체적으로 더 높은 화질을 가져온다. 또한 분류기 없는 안내( Sup. Mat에서 논의됨)의 긍정적인 사용을 보여준다. LME 및 FID[25]에 관한 것이다.\n' +
      '\n' +
      '**비디오 생성** 탭의 하단 부분 2는 시간적 비디오 생성 모델 상의 설계 선택들을 제거한다. 첫째, 가변 길이 비디오 생성을 지원할 뿐만 아니라 매끄러움과 낮은 지터를 보장하는 제안된 아웃페인팅 절차의 유효성을 검증한다. 최종 모델은 생성된 프레임과 주어진 프레임 사이에 50%의 중첩을 가지며 더 큰 값에서 안정되지만 더 작은 중첩(25%) 또는 아웃페인팅이 없는 것과 관련하여 눈에 띄는 개선을 얻는다. 모델은 또한 신체 포즈 제어로 더 나은 성능을 발휘합니다.\n' +
      '\n' +
      '**비디오 생성에서 2d 컨트롤의 효과.** 마침내 탭에서 비디오 생성 프로세스를 안내하는 데 사용되는 다양한 표현의 중요성을 완화한다. 3, 테스트 세트 샘플을 그들의 그라운드트루스 모션으로 재연하고 이미지 재구성 메트릭을 보고함으로써. 우리는 기준 입력 영상으로부터 왜곡된 조밀한 신체 표현과 기준 부분 뷰를 결합한 2차원 랜드마크, 조밀한 표현 및 최종 제안된 컨트롤을 탐색한다. 후자는 네트워크의 작업을 상당히 완화하고 최상의 결과로 이끈다.\n' +
      '\n' +
      '또한 섹션 3(및 Sup. Mat)에 설명된 훈련 일정으로 추가 성능 향상을 얻습니다. 단일 이미지에서 첫 번째 훈련을 하고 나중에 비디오에서 시간 계층을 미세 조정합니다.\n' +
      '\n' +
      '### Quantitative Results\n' +
      '\n' +
      '\'헤드 세대\' \'탭\' 도 4는 오디오-구동 비디오 생성의 태스크에 대한 이전의 최신 방식들에 대한 VLOG-GER의 성능을 요약한다. 우리는 대규모 데이터세트인 HDTF 데이터세트[97], 그러나 낮은 수의 아이덴티티(300) 피험자와 다소 제한된 시점 가변성을 가진 결과, 그리고 TalkingHead-1KH 데이터세트[79]에 대해 보고한다. 토킹 헤드 생성은 다양한 메트릭에 의해 평가되는 몇 가지 바람직한 특성을 가진 도전적인 작업이다. 특히, 이미지 품질, 다양성 사이에는 상충 관계가 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\multicolumn{2}{c}{FID [25] \\(\\downarrow\\) LME [mm] \\(\\downarrow\\) Jitter [mm/s\\({}^{3}\\)] \\(\\downarrow\\)} \\\\ \\hline \\multicolumn{3}{c}{Motion Generation} \\\\ \\hline Not predicting \\(\\Delta\\) over body pose & 52.27 & 4.22 & 6.56 \\\\ Not training with temporal loss & 16.56 & 3.18 & 4.64 \\\\ Not using classifier-free guidance & 16.54 & 3.32 & **3.49** \\\\ \\hline \\multicolumn{3}{c}{Temporal Diffusion Model} \\\\ \\hline No body controls (Only renders of head area) & 16.95 & 3.10 & 4.45 \\\\ No temporal outpainting during inference & 15.88 & 3.25 & 3.70 \\\\\n' +
      '25\\% outpainting overlap during inference & 15.90 & 3.23 & 3.61 \\\\ \\hline Full model & **15.36** & **3.06** & 3.58 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MENTOR Dataset에서 평가된 VLOGGER**의 주요 디자인 선택에 대한 **절제 연구, FID[25] 점수를 통해 이미지 품질을 검증하기 위한 가장 대표적인 메트릭, 랜드마크 오류(LME)를 통한 표현성 및 립싱크 품질, 얼굴 정점 지터를 기반으로 한 시간적 일관성을 보고한다. 첫 번째 부분은 시간적 손실과 분류기 없는 지침이 화질과 LME(비교를 위한 마지막 행의 전체 모델)에서 최상의 성능을 이끈다는 것을 보여준다. 두 번째 부분은 시간 확산 모델에서 설계 선택에 대한 개선 사항을 요약한다. 최종 파이프라인은 바디 컨트롤을 취함으로써 이득을 얻으며, 제안된 시간 아웃페인팅(전체 모델에서 50% 중첩)은 최상의 시간 일관성을 초래한다. 우리는 더 많은 겹침을 가진 모델 고원을 발견했다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c} \\multicolumn{2}{c}{Face} & \\multicolumn{2}{c}{Body} & \\multicolumn{2}{c}{Hands} & \\multicolumn{2}{c}{Full Image} \\\\ \\cline{2-11} \\multicolumn{2}{c}{PSNR \\(\\uparrow\\) L1 \\(\\downarrow\\)} & \\multicolumn{2}{c}{PSNR \\(\\uparrow\\) L1 \\(\\downarrow\\)} & \\multicolumn{2}{c}{PSNR \\(\\uparrow\\) L1 \\(\\downarrow\\)} & \\multicolumn{2}{c}{PSNR \\(\\uparrow\\) L1 \\(\\downarrow\\)} & \\multicolumn{2}{c}{PSNR \\(\\uparrow\\) SSIM \\(\\uparrow\\) LPIPS \\(\\downarrow\\) L1 \\(\\downarrow\\)} \\\\ \\hline Using 2D body keypoints & 20.5 &.0591 & 17.9 &.0778 & 17.8 &.0763 & 19.8 &.702 & 0.138 &.0564 \\\\ Using Dense Body Representation & 20.4 &.0604 & 18.3 &.0750 & 18.2 &.0744 & 20.1 &.719 & 0.128 &.0548 \\\\ + Warped Image Based on Body Model & 21.6 &.0517 & 19.3 &.0668 & 19.1 &.0680 & 20.7 &.722 & 0.113 &.0496 \\\\ + Training Schedule (Full model) & **22.2** & **.0468** & **20.2** & **.0594** & **20.0** & **.058** & **21.6** & **.76** & **.095** & **.0447** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MENTOR 데이터셋에서 비디오 생성에서 2d 컨트롤의 **절제. 우리는 2d 골격[30, 78], 조밀한 신체 표현[86] 또는 조밀한 표현과 왜곡된 이미지를 포함하는 제안된 제어와 같은 동시 작업에서 고려되는 서로 다른 2d 제어를 제거한다. 이 실험에서, 우리는 첫 번째 이미지를 취하고 원래 움직임에 따라 나머지 비디오를 애니메이션화하여 평균 이미지 유사성 메트릭을 평균 및 신체 부위별로 보고한다. 모든 변형은 동일한 데이터에 대해 훈련된다.** 및 신원 보존. VLOGGER은 얼굴 모션이 거의 도입되지 않는 StyleTalk[42]에 이어 두 번째로 낮은 모션 지터로 가장 높은 화질과 아이덴티티 보존을 달성하면서 실제 비디오에 존재하는 표현 다양성의 양에 근접한다(도 4 참조). 시간적 일관성은 우리의 시간적 계층과 아웃페인팅 절차의 기여도를 검증하며, 여전히 최첨단 확산 모델의 고품질 이미지 생성 능력을 활용한다. 모든 방법은 비교 가능한 Lip Sync 점수를 얻으며 결과는 평가된 두 데이터 세트의 모든 메트릭에 대해 일관적이다. 또한 피험자당 가장 성능이 좋은 비디오를 선택하여 생성된 샘플 수(3개, 5개 또는 8개)로 방법을 평가하여 샘플 수가 증가함에 따라 성능이 크게 향상되었다. 이는 VLOGGER의 생성 특성을 지원하여 피험자마다 다른 샘플을 생성하는 능력을 보여준다. 또한, 이러한 것들은 얼굴의 이미지만을 고려하는 반면, 우리의 목표는 손을 포함한 가시적인 신체 부위를 모델링하는 것이다. 신체나 몸짓을 고려한 기준선은 없지만, 우리는 이와 관련하여 표 2와 표 3에서 디자인 선택을 축소한다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 3, 우리는 (부분적으로 우리 훈련 세트의 규모와 다양성으로 인한) 비교를 실행하여 공정성과 일반화 능력을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c} \\multicolumn{8}{c}{HDTF Dataset [97]} \\\\ \\cline{2-10}  & \\multicolumn{3}{c}{Photorealism} & \\multicolumn{3}{c}{Lip Sync} & \\multicolumn{3}{c}{Diversity} & Identity Preserv. & Temp. Consist. \\\\ \\cline{2-10}  & FID [29] \\(\\downarrow\\) & CPBD [48] \\(\\uparrow\\) & NQE [48] \\(\\downarrow\\) & LSE-D [12] \\(\\downarrow\\) & LME [mm] \\(\\downarrow\\) & Expression \\(\\uparrow\\) & Head Err. \\(\\downarrow\\) & ArcFace [13] \\(\\downarrow\\) & Jitter [mm/s\\({}^{3}\\)] \\(\\downarrow\\) \\\\ \\multicolumn{8}{c}{} \\\\ Groundtruth & 0.00 & 0.562 & 6.31 & 7.79 & 0.0 & 0.401 & 0.00 & 0.00 & 5.19 \\\\ MahletTalk [10] & 22.63 & 0.428 & 6.65 & 8.30 & 3.26 & 0.364 & 0.911 & 0.828 & 6.21 \\\\ Audio2Head [79] & 19.58 & 0.512 & 6.41 & **7.55** & 3.08 & 0.415 & 0.896 & 1.92 & 6.15 \\\\ Wang et al. [77] & 21.23 & 0.428 & 7.71 & 8.04 & 4.48 & 0.365 & 1.37 & 2.52 & 6.46 \\\\ SaffTalk [95] & 29.44 & 0.520 & 6.48 & 7.73 & **3.01** & 0.287 & 0.880 & 0.874 & 5.51 \\\\ StyleTalk [24] & 31.16 & 0.472 & 6.47 & 7.87 & 3.79 & **0.416** & 1.14 & **0.692** & **4.34** \\\\ Ours & **18.98** & **0.621** & **5.92** & 8.10 & 3.05 & 0.397 & **0.877** & 0.759 & 0.05 \\\\ \\hline Ours (Bost of 3) & - & 0.628 & 5.64 & 7.43 & 2.95 & 0.425 & 0.829 & 0.706 & 4.75 \\\\ Ours (Bost of 5) & - & 0.631 & 5.53 & 7.22 & 2.91 & 0.436 & 0.814 & 0.687 & 4.67 \\\\ Ours (Bost of 8) & - & **0.634** & **5.44** & **7.04** & **2.84** & **0.448** & **0.800** & **0.677** & 4.55 \\\\ \\multicolumn{8}{c}{} \\\\ \\multicolumn{8other methods across several perceived attributes. Previous works exhibit a clear performance degradation for different classes (_e.g_. light vs dark skin, young vs old, _etc_.), and do not generalize to videos with visible torsos or hands. In contrast, VLOGGER exhibits fairly low bias on all the evaluated axes. We hope that the release of MENTOR will enable the community to address critical fairness issues and further advance the state-of-the-art.\n' +
      '\n' +
      '### Qualitative Results\n' +
      '\n' +
      '그림 1에서 정성적 결과를 보여준다. 4는 야생에서 이미지에 대한 가장 최근 및 고성능 기준선에 대한 것입니다. 대부분의 이전 작업들은 제한된 생성 용량을 가지며, 이는 기준 이미지 내에 폐색된 부분들을 생성하는 것을 어렵게 한다(_e.g_. 치아가 입 내부를 가리고 있었다면, 이들은 생성된 비디오에 걸쳐 지속될 것이다). 대조적으로, 본 모델은 보다 다양한 표현을 생성할 수 있고 움직이는 헤드의 폐색 영역을 올바르게 칠할 수 있다.\n' +
      '\n' +
      '**샘플 다양성.** VLOGGER은 확률적이기 때문에, 그림 5에 예시된 바와 같이, 동일한 입력 오디오/텍스트가 주어진 다수의 모션 및 비디오를 생성할 수 있다. 첫 번째 행으로부터, 배경은 거의 정적인 반면, 얼굴, 모발, 시선 및 신체 모션은 비디오가 시간적으로 전개됨에 따라 증가하는 변화량을 특징으로 하는 것을 알 수 있다.\n' +
      '\n' +
      '**비디오 편집.** 유사하게, 우리의 확산 접근법은 비디오 편집에서 기능을 나타낸다. 도. 도 6은 입(두 번째 행), 눈(세 번째 행)을 닫거나 피험자의 눈을 뜨게 하여 입력 비디오(상단 행)가 주어진 편집 예, _e.g_를 나타낸다. 시간적으로 일관된 방식으로 깜박이지 않음(세 번째 행) 이 경우, 얼굴 편집 후, 지면진실 영상과 다르게 투사되는 신체 좌표를 기반으로 인페인팅 마스크를 자동으로 생성한다.\n' +
      '\n' +
      '그림 3: MENTOR 데이터 세트의 테스트 세트에서 피부톤, 성별 및 연령과 같은 **다른 인지된 속성**에 걸쳐 우리의 모델과 가장 가까운 경쟁자. 우리의 모델은 사전 훈련된 대규모 확산 모델과 제안된 대규모 데이터 세트의 이전 데이터를 활용한다. 따라서 다른 방법과 달리 모든 범주에서 일관되게 수행하므로 편향이 거의 또는 전혀 나타나지 않는다. 우리는 또한 우리의 모델이 얼굴 주위에 빽빽한 경계 상자를 자르는 대신 광범위한 시점의 이미지에서 인간을 애니메이션할 수 있음을 보여준다.\n' +
      '\n' +
      '도 4: 입력 영상(좌측) 및 생성된 프레임을 보여주는 **정성적 비교** 기준선은 일반적으로 전체 서열을 따라 발현을 유지하고 머리[42, 77, 95]를 자르는 것을 필요로 한다. 대조적으로, VLOGGER은 면(세 번째 행)뿐만 아니라 가시 상체(다섯 번째 행)를 고려할 때 가시 영역의 변화를 생성한다. 이 그림은 애니메이션된 얼굴을 보여주지만 제스처가 있는 예가 그림에 나와 있다. 1과 Sup. 매트\n' +
      '\n' +
      '도 5: **쇼케이싱 모델 다양성**. VLOGGER은 확률적이며 동일한 주제에 대해 다양한 비디오를 생성할 수 있다. 피사체 이미지들 및 입력 스피치가 주어지면, 열 2 내지 5는 24개의 생성된 비디오들로부터 획득된, 각각 1 내지 4초 후에 픽셀 컬러의 편차를 보여준다. 단 1초(초콜) 후에, 모델은 이미 좋은 시각적 품질의 모든 비디오와 함께 손 포즈 및 얼굴 표정에서 큰 다양성을 보여준다.\n' +
      '\n' +
      '표현하고, 이 시간적 마스크를 사용하여 새로운 타겟 컨트롤에 따라 픽셀을 재생성한다. 이 과정은 비디오의 길이, 카메라까지의 거리 또는 피사체 동일성과 무관하며, 이러한 결과가 창의적인 비디오 편집에 대한 새로운 응용으로 이어질 수 있기를 바란다. Sup에서 동영상을 봅니다. 매트\n' +
      '\n' +
      '**개인화.** 확산 모델의 맥락에서 개인화는 최근 주제 중심 세대에 대해 광범위하게 탐구되었다[59]. 우리의 경우, VLOGGER은 합성을 위한 소스로서 단안 입력 이미지만을 취하고, 그럴듯한 합성을 생성할 수 있지만, 폐색된 부분에 대한 액세스가 없고,\n' +
      '\n' +
      '도 6: **비디오 편집 결과**. 입력 비디오(첫 번째 행)가 주어지면, 우리는 입(두 번째 행), 눈(세 번째 행)을 변경하거나 전체 비디오(네 번째 행) 동안 눈을 뜨도록 새로운 얼굴 표현을 정의한다. 시간적 인페인팅 마스크는 신체의 변화하는 부분으로부터 자동으로 정의된다. \'섭\'에서 제일 잘 봤어 매트\n' +
      '\n' +
      '그림 7: 모델 개인화에 대한 질적 결과. 사용자의 단일 비디오에서 모델 [59]를 미세 조정하면 광범위한 표현에서 보다 확실한 합성이 가능합니다.\n' +
      '\n' +
      '그 결과 나온 비디오는 그 사람에 대한 미세한 분석으로는 정당하지 않을 수 있다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 7을 참조하면, 확산 모델을 더 많은 데이터로 미세 조정함으로써, 피험자의 단안 비디오에서 VLOGGER는 기준 이미지가 눈을 감은 것으로 표시할 때 아이덴티티를 더 잘 캡처하는 것을 학습할 수 있음을 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 오디오 또는 텍스트로 조절된 단일 입력 이미지에서 얼굴과 몸을 모두 포함하는 인간 비디오 합성을 위한 방법론인 VLOGGER을 제시했다. VLOGGER은 3차원 인체 머리와 신체 포즈 표현을 기반으로 한 기본 스캐폴딩과 함께 제어 기반 확산 모델의 시간적 확장으로 구축되어 가변 길이의 고품질 애니메이션을 생성한다. 본 논문에서는 다양한 대규모 데이터 셋(이전 데이터 셋보다 1배 큰 크기)을 소개하고, 이 리포지토리와 여러 다른 리포지토리에서 VLOGGER의 성능을 검증하여 대화 얼굴 생성 작업에서 이전 최신 기술보다 우수함을 보이고, 다양한 다양성 축에서 더 강건함을 보인다. 안녕 매트 제한 사항과 사회적 영향을 논의합니다.\n' +
      '\n' +
      '알론소 마르티네스, 안야 하우트, 세르기 카엘레스, 에르난 모랄도, 에릭 프레이, 크리슈나 소만데팔리, 브렌단 주우 등이 MENTOR를 선별한 크고 다양한 비디오 저장소에 대한 세심한 수집과 분석에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Adam, M., Wessel, M., Benlian, A.: Ai-based chatbots in customer service and their effects on user compliance. Electronic Markets **31**(2), 427-445 (2021)\n' +
      '* [2] Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Li, Y., Michaeli, T., et al.: Lumiere: A space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945 (2024)\n' +
      '* [3] Bard: Bard: A large language model from google ai (2023), [https://blog.google/technology/ai/bard-google-ai-search-updates/](https://blog.google/technology/ai/bard-google-ai-search-updates/)\n' +
      '* [4] Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR (2022)\n' +
      '* [5] Bazavan, E.G., Zanfir, A., Szente, T.A., Zanfir, M., Alldieck, T., Sminchisescu, C.: Sphear: Spherical head registration for complete statistical 3d modeling. 3DV (2024)\n' +
      '* [6] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023)\n' +
      '* [7] Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep it SMPL: Automatic estimation of 3d human pose and shape from a single image. In: ECCV (2016)\n' +
      '* [8] Bounareli, S., Argyriou, V., Tzimiropoulos, G.: Finding directions in gan\'s latent space for neural face reenactment. BMVC (2022)* [9] Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I., Tzimiropoulos, G.: Hyperreenact: one-shot reenactment via jointly learning to refine and retarget faces. In: ICCV. pp. 7149-7159 (2023)\n' +
      '* [10] Chang, D., Shi, Y., Gao, Q., Fu, J., Xu, H., Song, G., Yan, Q., Yang, X., Soleymani, M.: Magicdance: Realistic human dance video generation with motions & facial expressions transfer. arXiv preprint arXiv:2311.12052 (2023)\n' +
      '* [11] Chen, L., Maddox, R.K., Duan, Z., Xu, C.: Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In: CVPR. pp. 7832-7841 (2019)\n' +
      '* [12] Chung, J.S., Zisserman, A.: Out of time: automated lip sync in the wild. In: ACCV-W (2016)\n' +
      '* [13] Chung, J.S., Jamaludin, A., Zisserman, A.: You said that? (2017)\n' +
      '* [14] Cudeiro, D., Bolkart, T., Laidlaw, C., Ranjan, A., Black, M.J.: Capture, learning, and synthesis of 3d speaking styles. In: CVPR (2019)\n' +
      '* [15] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face recognition. In: CVPR. pp. 4690-4699 (2019)\n' +
      '* [16] Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS **34**, 8780-8794 (2021)\n' +
      '* [17] Doukas, M.C., Zafeiriou, S., Sharmanska, V.: Headgan: One-shot neural head synthesis and editing. In: ICCV. pp. 14398-14407 (October 2021)\n' +
      '* [18] Fan, Y., Lin, Z., Saito, J., Wang, W., Komura, T.: Faceformer: Speech-driven 3d facial animation with transformers. In: CVPR. pp. 18770-18780 (2022)\n' +
      '* [19] Gao, Y., Zhou, Y., Wang, J., Li, X., Ming, X., Lu, Y.: High-fidelity and freely controllable talking head video generation. In: CVPR. pp. 5609-5619 (2023)\n' +
      '* [20] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)\n' +
      '* [21] Guan, J., Zhang, Z., Zhou, H., Hu, T., Wang, K., He, D., Feng, H., Liu, J., Ding, E., Liu, Z., et al.: Stylesync: High-fidelity generalized and personalized lip sync in style-based generator. In: CVPR. pp. 1505-1515 (2023)\n' +
      '* [22] Guo, Y., Chen, K., Liang, S., Liu, Y.J., Bao, H., Zhang, J.: Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In: ICCV. pp. 5784-5794 (2021)\n' +
      '* [23] Ha, S., Kersner, M., Kim, B., Seo, S., Kim, D.: Marionette: Few-shot face reenactment preserving identity of unseen targets. In: AAAI (2020)\n' +
      '* [24] He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221 (2022)\n' +
      '* [25] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS **30** (2017)\n' +
      '* [26] Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P., Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022)\n' +
      '* [27] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video diffusion models. arXiv:2204.03458 (2022)\n' +
      '* [28] Hong, F.T., Zhang, L., Shen, L., Xu, D.: Depth-aware generative adversarial network for talking head video generation. In: CVPR. pp. 3397-3406 (2022)\n' +
      '* [29] Hsu, G.S., Tsai, C.H., Wu, H.Y.: Dual-generator face reenactment. In: CVPR. pp. 642-650 (2022)* [30] Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., Bo, L.: Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117 (2023)\n' +
      '* [31] Jamaludin, A., Chung, J.S., Zisserman, A.: You said that?: Synthesising talking faces from audio. IJCV **127**, 1767-1779 (2019)\n' +
      '* [32] Ji, X., Zhou, H., Wang, K., Wu, Q., Wu, W., Xu, F., Cao, X.: Eamm: One-shot emotional talking face via audio-based emotion-aware motion model. In: SIGGRAPH \'22 (2022). [https://doi.org/10.1145/3528233.3530745](https://doi.org/10.1145/3528233.3530745), [https://doi.org/10.1145/3528233.3530745](https://doi.org/10.1145/3528233.3530745)\n' +
      '* [33] Joo, H., Simon, T., Sheikh, Y.: Total capture: A 3d deformation model for tracking faces, hands, and bodies. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8320-8329 (2018)\n' +
      '* [34] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: CVPR. pp. 8110-8119 (2020)\n' +
      '* [35] Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439 (2023)\n' +
      '* [36] Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., et al.: Videopoet: A large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125 (2023)\n' +
      '* [37] Kyrlitsias, C., Michael-Grigoriou, D.: Social interaction with agents and avatars in immersive virtual environments: A survey. Frontiers in Virtual Reality **2**, 786665 (2022)\n' +
      '* [38] Li, T., Bolkart, T., Black, M.J., Li, H., Romero, J.: Learning a model of facial shape and expression from 4d scans. TOG **36**(6), 194-1 (2017)\n' +
      '* [39] Liu, X., Xu, Y., Wu, Q., Zhou, H., Wu, W., Zhou, B.: Semantic-aware implicit neural audio-driven video portrait generation. In: ECCV. pp. 106-125. Springer (2022)\n' +
      '* [40] Liu, Y., Lin, L., Yu, F., Zhou, C., Li, Y.: Moda: Mapping-once audio-driven portrait animation with dual attentions. In: ICCV (2023)\n' +
      '* [41] Lu, Y., Chai, J., Cao, X.: Live Speech Portraits: Real-time photorealistic talking-head animation. ACM Transactions on Graphics **40**(6) (December 2021). [https://doi.org/10.1145/3478513.3480484](https://doi.org/10.1145/3478513.3480484)\n' +
      '* [42] Ma, Y., Wang, S., Hu, Z., Fan, C., Lv, T., Ding, Y., Deng, Z., Yu, X.: Styletalk: One-shot talking head generation with controllable speaking styles. arXiv preprint arXiv:2301.01081 (2023)\n' +
      '* [43] Manyika, J.: An overview of bard: an early experiment with generative ai. AI. Google Static Documents (2023)\n' +
      '* [44] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n' +
      '* [45] Mittal, A., Soundararajan, R., Bovik, A.C.: Making a "completely blind" image quality analyzer. Signal Processing Letters **20**(3), 209-212 (2012)\n' +
      '* [46] Moussawi, S., Koufaris, M., Benbunan-Fich, R.: How perceptions of intelligence and anthropomorphism affect adoption of personal intelligent agents. Electronic Markets **31**, 343-364 (2021)\n' +
      '* [47] Narvekar, N.D., Karam, L.J.: A no-reference perceptual image sharpness metric based on a cumulative probability of blur detection. In: International Workshop on Quality of Multimedia Experience. pp. 87-91. IEEE (2009)* [48] Narvekar, N.D., Karam, L.J.: A no-reference image blur metric based on the cumulative probability of blur detection (cpbd). Image Processing **20**(9), 2678-2683 (2011) 10, 12\n' +
      '* [49] Nirkin, Y., Keller, Y., Hassner, T.: Fsgan: Subject agnostic face swapping and reenactment. In: ICCV. pp. 7184-7193 (2019) 3, 4, 5\n' +
      '* [50] OpenAI: Gpt-4 technical report (2023) 2\n' +
      '* [51] Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A.A., Tzionas, D., Black, M.J.: Expressive body capture: 3d hands, face, and body from a single image. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10975-10985 (2019) 7\n' +
      '* [52] Paysan, P., Knothe, R., Amberg, B., Romdhani, S., Vetter, T.: A 3d face model for pose and illumination invariant face recognition. In: International Conference on Advanced Video and Signal Based Surveillance. pp. 296-301. Ieee (2009) 4\n' +
      '* [53] Pizzi, G., Vannucci, V., Mazzoli, V., Donvito, R.: I, chatbot! the impact of anthropomorphism and gaze direction on willingness to disclose personal information and behavioral intentions. Psychology & Marketing **40**(7), 1372-1387 (2023) 2\n' +
      '* [54] Prajwal, K., Mukhopadhyay, R., Namboodiri, V.P., Jawahar, C.: A lip sync expert is all you need for speech to lip generation in the wild. In: ACM International Conference on Multimedia. pp. 484-492 (2020) 2\n' +
      '* [55] Pumarola, A., Agudo, A., Martinez, A., Sanfeliu, A., Moreno-Noguer, F.: Ganimation: One-shot anatomically consistent facial animation. In: IJCV (2019) 5\n' +
      '* [56] Ren, Y., Li, G., Chen, Y., Li, T.H., Liu, S.: Pirenderer: Controllable portrait image generation via semantic neural rendering. In: ICCV. pp. 13759-13768 (2021) 5\n' +
      '* [57] Richard, A., Zollhofer, M., Wen, Y., De la Torre, F., Sheikh, Y.: Meshtalk: 3d face animation from speech using cross-modality disentanglement. In: ICCV. pp. 1173-1182 (2021) 4\n' +
      '* [58] Roesler, E., Manzey, D., Onnasch, L.: A meta-analysis on the effectiveness of anthropomorphism in human-robot interaction. Science Robotics **6**(58), eabj5425 (2021) 2\n' +
      '* [59] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arxiv:2208.12242 (2022) 3, 15\n' +
      '* [60] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS **35**, 36479-36494 (2022) 8\n' +
      '* [61] Seeger, A.M., Pfeiffer, J., Heinzl, A.: Texting with humanlike conversational agents: Designing for anthropomorphism. Journal of the Association for Information Systems **22**(4), 8 (2021) 2\n' +
      '* [62] Seitz, L., Bekmeier-Feuerhahn, S., Gohil, K.: Can we trust a chatbot like a physician? a qualitative study on understanding the emergence of trust toward diagnostic chatbots. International Journal of Human-Computer Studies **165**, 102848 (2022) 2\n' +
      '* [63] Shen, K., Guo, C., Kaufmann, M., Zarate, J.J., Valentin, J., Song, J., Hilliges, O.: X-avatar: Expressive human avatars. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16911-16921 (2023) 7\n' +
      '* [64] Sora. [https://openai.com/sora](https://openai.com/sora) (2024) 3, 5\n' +
      '* [65] Stan, S., Haque, K.I., Yumak, Z.: Facediffuser: Speech-driven 3d facial animation synthesis using diffusion. arXiv preprint arXiv:2309.11306 (2023)* [66] Stypulkowski, M., Vougioukas, K., He, S., Zieba, M., Petridis, S., Pantic, M.: Diffused heads: Diffusion models beat gans on talking-face generation. arXiv preprint arXiv:2301.03396 (2023)\n' +
      '* [67] Suzhen, W., Lincheng, L., Yu, D., Changjie, F., Xin, Y.: Audio2head: Audio-driven one-shot talking-head generation with natural head motion. In: IJCAI (2021)\n' +
      '* [68] Thambiraja, B., Habibie, I., Aliakbarian, S., Cosker, D., Theobalt, C., Thies, J.: Imitator: Personalized speech-driven 3d facial animation. In: ICCV. pp. 20621-20631 (2023)\n' +
      '* [69] Thies, J., Zollhofer, M., Stamminger, M., Theobalt, C., Niessner, M.: Face2face: Real-time face capture and reenactment of rgb videos. In: CVPR. pp. 2387-2395 (2016)\n' +
      '*Google cloud. [https://cloud.google.com/text-to-speech] (https://cloud.google.com/text-to-speech)(2019)\n' +
      '* [71] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. NeurIPS (2017)\n' +
      '* [72] Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar, M.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399 (2022)\n' +
      '* [73] Vougioukas, K., Petridis, S., Pantic, M.: Realistic speech-driven facial animation with gans. IJCV **128**, 1398-1413 (2020)\n' +
      '* [74] Wahde, M., Virgolin, M.: Conversational agents: Theory and applications. In: HANDBOOK ON COMPUTER LEARNING AND INTELLIGENCE: Volume 2: Deep Learning, Intelligent Control and Evolutionary Computation, pp. 497-544. World Scientific (2022)\n' +
      '* [75] Wang, J., Qian, X., Zhang, M., Tan, R.T., Li, H.: Seeing what you said: Talking face generation guided by a lip reading expert. In: CVPR. pp. 14653-14662 (2023)\n' +
      '* [76] Wang, S., Li, L., Ding, Y., Fan, C., Yu, X.: Audio2head: Audio-driven one-shot talking-head generation with natural head motion. arXiv preprint arXiv:2107.09293 (2021)\n' +
      '* [77] Wang, S., Li, L., Ding, Y., Yu, X.: One-shot talking face generation from single-speaker audio-visual correlation learning. In: AAAI (2022)\n' +
      '* [78] Wang, T., Li, L., Lin, K., Lin, C.C., Yang, Z., Zhang, H., Liu, Z., Wang, L.: Disco: Disentangled control for referring human dance generation in real world. arXiv e-prints pp. arXiv-2307 (2023)\n' +
      '* [79] Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: CVPR (2021)\n' +
      '* [80] Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: CVPR. pp. 10039-10049 (2021)\n' +
      '* [81] Wiles, O., Koepke, A., Zisserman, A.: X2face: A network for controlling face generation using images, audio, and pose codes. In: ECCV. pp. 670-686 (2018)\n' +
      '* [82] Wu, X., Hu, P., Wu, Y., Lyu, X., Cao, Y.P., Shan, Y., Yang, W., Sun, Z., Qi, X.: Speech2lip: High-fidelity speech to lip generation by learning from a short video. In: ICCV. pp. 22168-22177 (2023)\n' +
      '* [83] Xing, J., Xia, M., Liu, Y., Zhang, Y., Zhang, Y., He, Y., Liu, H., Chen, H., Cun, X., Wang, X., et al.: Make-your-video: Customized video generation using textual and structural guidance. arXiv preprint arXiv:2306.00943 (2023)* [84] Xing, J., Xia, M., Zhang, Y., Cun, X., Wang, J., Wong, T.T.: Codotelaker: Speech-driven 3d facial animation with discrete motion prior. In: CVPR. pp. 12780-12790 (2023)\n' +
      '* [85] Xu, H., Bazavan, E.G., Zanfir, A., Freeman, B., Sukthankar, R., Sminchisescu, C.: GHUM & GHUML: Generative 3D human shape and articulated pose models. CVPR (2020)\n' +
      '* [86] Xu, Z., Zhang, J., Liew, J.H., Yan, H., Liu, J.W., Zhang, C., Feng, J., Shou, M.Z.: Magicanimate: Temporally consistent human image animation using diffusion model. arXiv preprint arXiv:2311.16498 (2023)\n' +
      '* [87] Yang, K., Chen, K., Guo, D., Zhang, S.H., Guo, Y.C., Zhang, W.: Face2face \\(\\rho\\): Real-time high-resolution one-shot face reenactment. In: ECCV. pp. 55-71. Springer (2022)\n' +
      '* [88] Yao, S., Zhong, R., Yan, Y., Zhai, G., Yang, X.: Dfa-nerf: Personalized talking head generation via disentangled face attributes neural rendering. arXiv preprint arXiv:2201.00791 (2022)\n' +
      '* [89] Ye, Z., Jiang, Z., Ren, Y., Liu, J., He, J., Zhao, Z.: Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis. arXiv preprint arXiv:2301.13430 (2023)\n' +
      '* [90] Yi, H., Liang, H., Liu, Y., Cao, Q., Wen, Y., Bolkart, T., Tao, D., Black, M.J.: Generating holistic 3d human motion from speech. In: CVPR. pp. 469-480 (June 2023)\n' +
      '* [91] Yi, X., Zhou, Y., Xu, F.: Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. ACM Transactions on Graphics (TOG) **40**(4), 1-13 (2021)\n' +
      '* [92] Yin, F., Zhang, Y., Cun, X., Cao, M., Fan, Y., Wang, X., Bai, Q., Wu, B., Wang, J., Yang, Y.: Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In: ECCV. pp. 85-101. Springer (2022)\n' +
      '* [93] Yu, Z., Yin, Z., Zhou, D., Wang, D., Wong, F., Wang, B.: Talking head generation with probabilistic audio-to-visual diffusion priors. In: ICCV. pp. 7645-7655 (2023)\n' +
      '* [94] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models (2023)\n' +
      '* [95] Zhang, W., Cun, X., Wang, X., Zhang, Y., Shen, X., Guo, Y., Shan, Y., Wang, F.: Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In: CVPR. pp. 8652-8661 (2023)\n' +
      '* [96] Zhang, Y., Zhang, S., He, Y., Li, C., Loy, C.C., Liu, Z.: One-shot face reenactment. arXiv preprint arXiv:1908.03251 (2019)\n' +
      '* [97] Zhang, Z., Li, L., Ding, Y., Fan, C.: Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In: CVPR. pp. 3661-3670 (2021)\n' +
      '* [98] Zhang, Z., Li, L., Ding, Y., Fan, C.: Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In: CVPR. pp. 3661-3670 (2021)\n' +
      '* [99] Zhao, J., Zhang, H.: Thin-plate spline motion model for image animation. In: CVPR. pp. 3657-3666 (2022)\n' +
      '* [100] Zhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., et al.: A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419 (2023)\n' +
      '* [101] Zhou, H., Liu, Y., Liu, Z., Luo, P., Wang, X.: Talking face generation by adversarially disentangled audio-visual representation. In: AAAI (2019)\n' +
      '*[*[102] Zhou, H., Sun, Y., Wu, W., Loy, C.C., Wang, X., Liu, Z.: 묵시적으로 모듈화된 시청각 표현에 의한 포즈 제어 가능한 대화 얼굴 생성. In: CVPR. pp. 4176-4186 (2021)\n' +
      '* [103] Zhou, Q., Li, B., Han, L., Jou, M.: Talking to a bot or a wall? how chatbots vs. human agents affect anticipated communication quality. Computers in Human Behavior **143**, 107674 (2023)\n' +
      '* [104] Zhou, Y., Han, X., Shechtman, E., Echevarria, J., Kalogerakis, E., Li, D.: Makelttalk: speaker-aware talking-head animation. ACM Transactions On Graphics (TOG) **39**(6), 1-15 (2020)\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>