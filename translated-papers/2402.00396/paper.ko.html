<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '언어 모델을 _수동 탐색_로 사용하는 응답 쌍. 우리는 수동 탐사의 성능을 여러 능동 탐사 알고리즘과 비교한다. 하나는 볼츠만 탐사로 예측 보상이 높은 반응을 선택하는 경향이 있다. 또한 인식 신경망(ENN)이 제공하는 불확실성 추정치를 활용하는 두 가지 접근법을 시도했다. 우리가 _infomax_라고 부르는 첫 번째는 피드백에 의해 드러나는 정보를 최대화하기 위한 목적으로 한 쌍의 응답을 선택한다. 이는 정보 이득을 최대화하는 것을 목표로 하는 알고리즘들의 널리 사용되는 컬렉션 내에 속한다(예를 들어, (Houthooft et al., 2016; MacKay, 1992; Sadigh et al., 2018; Sun et al., 2011 참조). 둘째, _double Thompson sampling_(Wu and Liu, 2016)로 불리는 응답은 최적 확률에 따라 표본을 추출합니다. 이러한 탐사 알고리즘에 대해서는 4절에서 보다 정밀하게 설명한다.\n' +
      '\n' +
      '그림 1은 서로 다른 탐사 알고리즘을 이용하여 산출된 경험적 결과를 비교한 것이다. 이러한 결과를 생성한 실험은 섹션 5에 설명되어 있으며 각 플롯된 점은 달성된 성능 수준에 해당한다. 수평 좌표는 그 성능 레벨에 도달하기 위해 이중 TS에 의해 요구되는 질의의 수를 식별하는 반면, 수직 좌표는 대안에서 요구되는 질의를 식별한다. 수동 탐사를 위한 플롯은 이중 TS를 사용하는 ** 능동 탐사가 높은 수준의 성능에 도달하는 데 필요한 쿼리 수를 크게 감소시킨다는 것을 분명히 보여준다. 불확실성 추정 없이 점 추정 보상 모델만 사용한 알고리즘 중에서 볼츠만 탐사가 가장 잘 수행되었다. 볼츠만에 대한 도표는 이중 TS가 사용하는 **불확실성 추정치가 극적인 개선을 가능하게 함을 보여준다. 마지막으로, 인포맥스에 대한 그림은 불확실성 추정치를 활용하는 시도되고 테스트된 알고리즘 중에서도 탐색 알고리즘의 선택이 큰 성능 차이를 유발할 수 있는 방법을 보여준다.\n' +
      '\n' +
      '이러한 결과는 대규모 언어 모델을 조정함에 있어 적극적인 탐사의 실질적인 이점을 보여주는 첫 번째 결과이지만, 탐사 알고리즘(Lattimore and Szepesvari, 2020)과 관련된 오랜 작업 역사를 기반으로 한다. 특히, 우리의 문제는 문맥적 결투 강도(Dudik et al., 2015; Saha, 2021; Yue et al., 2012)의 경우이며, 우리의 알고리즘은 정보 추구 방식(Hennig and Schuler, 2012; Houthooft et al., 2016; MacKay, 1992; Russo and Van Roy, 2014; Ryzhov et al., 2012; Sadigh et al., 2018; Sun et al., 2011) 및 Thompson 샘플링(Russo et al., 2018; Thompson, 1933; Wu and Liu, 2016)을 기반으로 한다. 또한, 우리의 노력은 신경망을 사용하여 점점 더 복잡한 환경에 대한 효율적인 탐색 알고리즘을 스케일링한 작업 라인을 계속한다(Badia et al., 2020; Bellemare et al., 2016; Burda et al., 2018; Dwaracherla et al., 2020; Lu and Van Roy, 2017; Osband et al., 2016, 2019, 2023; Ostrovski et al., 2017; Riquelme et al., 2018; Zhang et al., 2020; Zhou et al., 2020).\n' +
      '\n' +
      '도 1 | 다양한 수준의 성능을 달성하기 위해 이중 TS 대 대안들에 의해 요구되는 질의들.\n' +
      '\n' +
      ' \n' +
      '\n' +
      '##2 실험관로\n' +
      '\n' +
      '우리는 탐색 알고리즘을 연구하기 위해 사용하는 실험 파이프라인을 제시하는 것으로 시작한다. 이 파이프라인은 Anthropic 데이터 세트(Bai et al., 2022) 및 Gemini Nano 및 Gemini Pro 사전 훈련 언어 모델(Team et al., 2023)을 포함한 기존 도구를 기반으로 한다. 이것은 인간 피드백 시뮬레이터를 사용하여 각 질의에 대한 응답으로 응답 간의 선호도의 이진 표현을 생성한다. 파이프라인은 학습 파이프라인과 평가 파이프라인의 두 부분으로 구성된다. 전자는 순차적 질의 및 학습 과정에서 에이전트와 인간 피드백 시뮬레이터 사이의 인터페이스를 제어한다. 후자는 상대 성능을 평가하는 과정에서 사전 훈련된 언어 모델, 새로운 응답 생성 모델 및 인간 피드백 시뮬레이터 간의 인터페이스를 지배한다.\n' +
      '\n' +
      '에이전트는 피드백에서 쿼리에 이르기까지 순차적으로 학습하며, 각각은 프롬프트와 두 개의 대체 응답으로 구성된다. 그림 2에 예시된 바와 같이, 각각의 쿼리는 에이전트에 의해 조작되고 인간 선호도 시뮬레이터에 제시되며, 이는 둘 사이의 이진 선호도를 나타낸다. 상호작용의 각 에폭에 걸쳐 에이전트는 \\(B\\) 질의의 배치를 전송하고 \\(B\\) 비트의 피드백을 수신한다. 인체 도움말 기반 열차 데이터 세트에서 각 프롬프트가 균일하게 샘플링됩니다. 각 에이전트는 프롬프트와 함께 제시될 때 먼저 제미니 나노 모델을 사용하여 \\(N\\) 후보를 생성한 다음 이 중에서 두 개를 선택하는 탐색 알고리즘을 적용하여 한 쌍의 응답을 설계한다. 탐색 기법은 지금까지 관찰된 질의와 피드백에 대해 학습된 보상 모델에 접근한다. 우리가 고려하는 각 에이전트는 탐색 알고리즘과 보상 모델을 생성하는 아키텍처 및 훈련 알고리즘으로 구별된다. 우리가 고려하는 에이전트 중 일부에서 보상 모델은 인식 신경망의 형태를 취하며, 이는 보상의 포인트 추정 외에도 불확실성 추정치에 대한 탐색 알고리즘 액세스를 제공한다. 각각의 보상 모델은 제미니 나노 모델의 몸통을 기반으로 한다. 이는 보상 모델이 먼저 미리 훈련된 트랜스포머 모델의 마지막 레이어 임베딩을 계산한 후 다층 퍼셉트론(MLP) 헤드를 적용한다는 것을 의미한다. 섹션 3의 아키텍처 및 훈련 알고리즘에 대해 자세히 설명합니다.\n' +
      '\n' +
      '인간이 응답 사이에서 선택하는 방법을 시뮬레이션하기 위해 각 프롬프트-응답 쌍을 점수화하는 보상 모델을 사용한다. 각 쿼리에 대해 두 가지 프롬프트-응답 쌍에 할당된 점수를 기반으로 브래들리-테리 선택 모델에 따라 선호도를 샘플링한다. 이 시뮬레이터가 사용하는 보상 모델은 제미니 프로 언어 모델의 몸통을 재사용하는 아키텍처와 함께 인류학 데이터 세트에 적합하다. 추가 세부 사항은 부록 A에 제공된다. 제미니 프로는 제미니 나노보다 훨씬 크기 때문에 에이전트가 사용할 수 있는 것보다 훨씬 복잡한 모델에 의해 선택된다는 점에 유의해야 한다. 이러한 규모의 차이는 인간이 에이전트에 의해 모델링된 것보다 더 복잡한 행동을 보일 수 있다는 사실을 반영하기 위한 것이다. 알고리즘 1은 우리의 학습 파이프라인에서 에이전트 및 시뮬레이터와 송수신되는 상호 작용의 간결한 프레젠테이션을 제공한다.\n' +
      '\n' +
      '```\n' +
      '0 : prompt_set, agent, feedback_simulator\n' +
      '1:for\\(t\\ in \\(1,\\dots,T\\)do\n' +
      '2 : 에이전트로 전송 : \\(B\\) 프롬프트\n' +
      '3: 에이전트로부터 수신: 프롬프트당 두 개의 응답\n' +
      '4 : 시뮬레이터로 전송 : \\(B\\) 질의\n' +
      '5: 시뮬레이터로부터 수신: \\(B\\) 비트의 피드백\n' +
      '6 : 에이전트로 전송 : \\(B\\) 비트의 피드백\n' +
      '7:endfor\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 학습 인터페이스\n' +
      '\n' +
      '그림 3은 에이전트 성능을 평가하기 위한 파이프라인을 보여준다. 성능은 제미니 나노 모델에 대해 측정된다. 인체 도움말 기반 평가 데이터 세트에서 일련의 프롬프트가 샘플링됩니다. 각각에 대해 두 개의 반응이 샘플링됩니다. 하나는 제미니 나노이고 다른 하나는 학습된 보상 모델을 사용하는 새로운 반응 생성 모델에 의한 것이다. 이 새로운 모델은 제미니 나노를 사용하여 \\(N\\) 응답을 샘플링한 다음 에이전트의 보상 모델에 따라 가장 높은 점수를 받는 모델을 선택하여 작동한다. 인간 선호도 시뮬레이터는 제미니 나노에 의해 생성된 대안보다 에이전트의 반응을 선택할 확률을 출력한다. 이러한 확률은 프롬프트에 대해 평균화되고, 이 평균은 에이전트의 응답이 선호되는 시간의 분율을 나타내기 때문에 에이전트의 _win rate_로 지칭된다. 이러한 방식으로 생성된 추정치가 수렴하기 위해서는 더 많은 수의 쿼리가 요구되지만, 승률은 시뮬레이션된 선택의 이진 표시를 평균함으로써 또한 추정될 수 있다는 점에 유의한다. 알고리즘 2는 평가 단계에서 상호 작용의 간결한 표현을 제공한다.\n' +
      '\n' +
      '우리의 실험 파이프라인은 보상을 최적화하기 위해 일반적으로 사용되는 정책 기울기 방법의 종류를 피한다는 점에 유의한다. 대신에, 우리의 에이전트는 기본 언어 모델(Gemini)에서 \\(N\\) 응답을 샘플링한다.\n' +
      '\n' +
      '그림 3: 성능 평가 파이프라인.\n' +
      '\n' +
      '나노) 및 보상을 최대화하는 것 중 하나를 선택한다. 이 최적-\\(N\\) 절차는 정책-구배 기반 최적화를 근사화하는 역할을 하지만 번거로운 계산 요구 사항이 없다. 또한 정책 구배 방법에서 합리적인 결과를 얻기 위해 종종 필요한 하이퍼모수 손질에 대한 잘 이해되지 않는 의존성을 피하기 때문에 가장 좋은\\(N\\) 절차는 보다 투명한 분석을 배양한다. 원형의 정책 기울기 접근법은 기본 언어 모델과의 유사성 및 보상과의 정렬이라는 두 목적 사이에서 균형을 이루는 손실 함수를 최소화한다. 스칼라 하이퍼모수는 유사성 측정을 곱하여 이러한 목표 간의 균형을 이룬다. 파라미터 \\(N\\)은 최적-of-\\(N\\) 접근법에서 유사한 역할을 한다. \\(N\\)이 증가함에 따라 반응에 비해 최대화하면 에이전트가 보상에 더 밀접하게 정렬됩니다. Moderating \\(N\\)은 기본 언어 모델과 더 유사한 에이전트 행동을 장려한다.\n' +
      '\n' +
      '##3 보상모델 구조 및 훈련\n' +
      '\n' +
      '보상 모델은 실험 파이프라인의 학습 및 평가 단계 모두에서 응답 선택을 안내한다. 우리는 관찰된 선호도 데이터에 각각 맞는 두 가지 유형의 보상 모델을 고려한다. 첫 번째는 각 프롬프트-응답 쌍에 보상을 할당하는 포인트 추정치입니다. 두 번째는 인식 지수에 추가로 의존한다. 기준 분포로부터 인식 지수를 샘플링하는 것은 보상에 대한 인식 불확실성을 모델링하는 보상의 무작위성을 유도한다. 이 섹션에서는 실험에 사용된 신경망 아키텍처 및 훈련 알고리즘에 대해 설명한다.\n' +
      '\n' +
      '우리는 각각 제미니 나노 언어 모델의 마지막 레이어 임베딩을 입력으로 하는 보상 모델을 훈련한다. 그림 4에 예시된 바와 같이, 언어 모델 몸통을 먼저 통과시킨 후 보상 모델을 통과시킴으로써 프롬프트-응답 쌍에 보상이 할당된다.\n' +
      '\n' +
      '그림 4\\(|\\) 우리의 보상 모델은 제미니 나노 언어 모델의 마지막 레이어 임베딩을 입력으로 한다. 정지 그래디언트는 몸통 무게의 몸통 업데이트를 방지합니다.\n' +
      '\n' +
      '### Point Estimate\n' +
      '\n' +
      '우리의 아키텍처에서 포인트 추정 보상 모델은 피드포워드 다층 퍼셉트론(MLP)의 형태를 취한다. 이 보상 모델은 Gemini Nano 언어 모델의 마지막 레이어 임베딩을 입력으로 하고, 그 자체는 즉각적인 응답 쌍 \\((x,y)\\)를 입력으로 한다. 보상 모델은 스칼라 보상\\(\\widehat{r}_{\\theta}(x,y)\\)을 출력한다. 여기서 \\(\\theta\\)는 MLP 파라미터의 벡터이다.\n' +
      '\n' +
      '선호 데이터에 대한 보상 모델을 훈련합니다. 각 데이터 포인트는 질의로 구성되며, 응답의 프롬프트와 쌍으로 구성되며, 응답 간의 선호도에 대한 이진 표시로 구성됩니다. 이러한 데이터 포인트의 집합 \\(\\mathcal{D}\\)이 주어지면, MLP 파라미터를 계산하기 위해 손실 함수를 최적화한다.\n' +
      '\n' +
      '\\thetacal{L}_{\\text{point}(\\theta|\\mathcal{D})=\\sum_{(x,y,y^{\\prime},c)\\mathcal{D}in\\mathcal{D}\\text{ce}(r_{\\theta}(x,y),r_{\\theta}(x,y^{\\prime}),c)+\\lambda\\|\\theta\\|_{2}^{2}, \\tag{1}\\text{1}}\n' +
      '\n' +
      '여기서 \\(\\lambda\\)는 정규화 강도, \\(c\\)는 선택 또는 선호도를 나타내며, \\(\\text{ce}(\\cdot,\\cdot,\\cdot)\\)는 교차 엔트로피 손실을 나타낸다:\n' +
      '\n' +
      '\\[\\text{ce}(R,R^{\\prime},c)=-(1-c)R-cR^{\\prime}+\\ln(e^{R}+e^{R^{\\prime}}). \\tag{2}\\ 반응 \\(y\\)이 \\(y^{\\prime}\\)보다 선호될 때, 선호 지표 \\(c\\)는 0이고 그 반대도 마찬가지임을 유의한다.\n' +
      '\n' +
      '### 인식 신경망\n' +
      '\n' +
      '우리는 보상에 대한 인식적 불확실성을 모델링하기 위해 인식적 신경망(ENNs)을 사용한다(Osband et al., 2023a). 데이터세트\\(\\mathcal{D}\\)이 주어지면 손실함수를 최소화하여 ENN 파라미터를 구한다.\n' +
      '\n' +
      '\\theta|\\mathcal{L}_{\\text{ENN}(\\theta|\\mathcal{D})=\\lambda\\|\\theta-\\bar{\\theta}\\|_{2}+\\int_{z\\in\\mathcal{Z}p_{z}(dz)\\mathcal{L}(\\theta|\\mathcal{D},z), \\tag{3}\\tag{3}}\n' +
      '\n' +
      '여기서 \\(p_{z}\\)는 인식 지수 기준 분포이고, \\(\\bar{\\theta}\\)는 초기 파라미터 벡터이고, 그리고\n' +
      '\n' +
      '\\[\\mathcal{L}(\\theta|\\mathcal{D},z)=\\sum_{(x,y,y^{\\prime},c)\\in\\mathcal{D}} \\text{ce}(r_{\\theta}(x,y|z),r_{\\theta}(x,y^{\\prime}|z),c).\\]\n' +
      '\n' +
      '이러한 객체들을 해석하기 위해, \\(p_{z}\\)에서 샘플링된 \\(z\\)의 보상 함수 \\(r_{\\bar{\\theta}}(\\cdot|z)\\)는 보상 함수에 대한 이전의 분포에서 샘플을 나타낸다. 손실함수 \\(\\mathcal{L}_{\\text{ENN}\\)에서 \\(\\bar{\\theta}\\)으로 정규화하는 것은 훈련 후 인식지수에 걸쳐 적절한 다양성을 유지하는 역할을 한다.\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '각 보상 모델을 훈련하기 위해 리플레이 버퍼를 유지하고 섹션 3.1 및 섹션 3.2에 설명된 손실 함수에 대한 확률적 경사 하강(SGD) 알고리즘을 적용한다. 특히, 에이전트가 \\(B\\) 쿼리를 전송하고 \\(B\\) 비트의 피드백을 받는 상호작용의 각 에포크가 끝나면 에이전트는 생성된 \\(B\\) 데이터 포인트를 용량의 FIFO 리플레이 버퍼에 삽입한다. 그런 다음, SGD 단계들은 ADAM에 의해 적응된 단계들과 함께 리플레이 버퍼로부터의 랜덤 미니배치들을 적용한다. 트레이닝된 보상 모델은 후속 에포크에서 공식화된 쿼리들을 결정하기 위해 사용된다.\n' +
      '\n' +
      '##4 탐사 알고리즘\n' +
      '\n' +
      '이제 경험적 연구에 사용된 탐색 알고리즘 세트를 설명한다.\n' +
      '\n' +
      '### Passive Exploration\n' +
      '\n' +
      '현재 RLHF 시스템은 일반적으로 알고리즘 3에 따라 응답 쌍을 선택하여 수동적으로 탐색한다. 이 알고리즘은 입력으로서 프롬프트 \\(x\\)와 언어 모델 \\(\\pi\\)을 취한다. 언어 모델은 응답을 샘플링하는 분포\\(\\pi(\\cdot|x)\\)를 인코딩한다. 알고리즘은 언어 모델에 의해 샘플링된 두 개의 응답을 반환한다.\n' +
      '\n' +
      '```\n' +
      '1:\\(x\\), \\(\\pi\\)\n' +
      '2:시료반응 \\(y\\sim\\pi(\\cdot|x)\\)\n' +
      '3:repeat\n' +
      '4:시료반응 \\(y^{\\prime}\\sim\\pi(\\cdot|x)\\)\n' +
      '5:until\\(y^{\\prime}\\neq y\\)return\\(y,y^{\\prime}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 3** 수동탐색\n' +
      '\n' +
      '점 추정치를 이용한 능동 탐사\n' +
      '\n' +
      '응답 쌍을 선택할 때, 에이전트는 모든 또는 일부 과거 질의에 대한 피드백에 대해 트레이닝된 보상 모델을 사용할 수 있다. 수동적 탐사는 이 기회를 포기한다. 우리는 현재 볼츠만 탐사를 고려하는데, 점 추정 보상 모델을 사용하여 각 프롬프트-응답 쌍에 보상 \\(r(x,y)\\)을 할당한다. 이것은 능동적 탐구의 한 형태를 구성하는데, 응답은 수동적 탐색보다 더 유용한 미래 피드백을 모으기 위한 목적으로 과거의 피드백에 기초하여 맞춤화된다.\n' +
      '\n' +
      '알고리즘 4에 제시된 바와 같이 수동 탐사에 사용되는 입력 \\(x\\) 및 \\(\\pi\\) 외에 볼츠만 탐사는 점 추정 보상 모델 \\(r\\)이 필요하다. 또한, 온도\\(\\tau\\)와 반응집합 카디널리티\\(N\\)의 두 가지 하이퍼파라미터가 존재한다. 언어 모델은 \\(N\\) 응답을 생성하고, 두 개의 표본은 각각 \\(n\\)번째 응답 \\(\\tilde{y}_{n}\\)에 할당된 지수 \\(r(x,\\tilde{y}_{n})/\\tau\\)를 갖는 볼츠만 분포로부터 추출된다.\n' +
      '\n' +
      '```\n' +
      '0:\\(x\\), \\(\\pi\\), \\(r\\)\n' +
      '0:\\(\\tau\\), \\(N\\)\n' +
      '1 : 시료반응 \\(\\tilde{y}_{1},\\ldots,\\tilde{y}_{N}\\sim\\pi(\\cdot|x)\\)\n' +
      '2:probs \\(q_{n}=\\frac{\\exp(r(x,\\tilde{y}_{n})/\\tau)}{\\sum_{\\pi^{\\prime}=1}^{N}\\exp(r(x,\\tilde{y}_{n})/\\tau)}\\), \\(\\forall n\\)\n' +
      '3: 치환이 없는 시료\\(i,i^{\\prime}\\sim q\\) return\\(y_{i},y_{i^{\\prime}}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 4** 볼츠만\n' +
      '\n' +
      '이 알고리즘은 온도\\(\\tau\\)가 증가함에 따라 수동 탐사를 회복한다는 점에 유의한다. 반면에 볼츠만 탐사는 \\(\\tau\\)이 사라짐에 따라 최적 또는 거의 유사한 응답을 선택하는 경향이 있다. 또한 두 응답을 선택하기 위해 서로 다른 온도\\(\\tau_{1}\\)와 \\(\\tau_{2}\\)을 사용하는 알고리즘의 일반화를 고려할 수 있다. 그런 다음, 예를 들어 \\(\\tau_{1}\\)이 사라지고 \\(\\tau_{2}\\)이 성장함에 따라 첫 번째 반응이 최적이 되는 반면 두 번째 반응은 균일하게 샘플링된다. 실험 작업에서 성능을 향상시키기 위해 별도의 온도를 사용하지 않았습니다. 또한, 동일한 입력을 취하는 많은 대안 중에서 가장 좋은 성능을 제공하는 알고리즘 4를 발견했다. 이는 볼츠만 탐사가 점 추정 보상 모델을 기반으로 희망할 수 있는 것뿐만 아니라 그에 대한 반응을 선택한다는 것을 시사한다.\n' +
      '\n' +
      'NN을 이용한### 능동 탐사\n' +
      '\n' +
      '다음으로, 각 프롬프트-응답 쌍에 할당된 보상 \\(r(x,y|z)\\)이 인식지수에 추가적으로 의존하는 ENN 보상 모델을 사용하는 알고리즘을 고려한다. 섹션 3.2에서 논의된 바와 같이, ENN은 보상 모델 \\(r\\) 및 기준 분포 \\(p\\)으로 특징지어진다. 고정된 \\(x\\)과 \\(y\\)의 경우, \\(p\\)에서 여러 인식 지수를 샘플링하여 이들 샘플 간의 분산으로부터 보상 불확실성을 확인할 수 있다.\n' +
      '\n' +
      '인포맥스(알고리즘 5)는 ENN 보상 모델을 입력으로 한다. 볼츠만 탐사(알고리즘 4)처럼, 인포맥스는 언어 모델이 \\(N\\) 응답을 생성하는 것으로 시작한다. 그런 다음 \\(p\\)에서 \\(M\\) 인식 지수를 샘플링한다. 각 응답 쌍과 각 인식 지표에 대해 ENN은 무작위 인간 희귀자가 두 번째 응답보다 첫 번째 응답을 선호하는 이벤트에 확률을 할당한다. 인포맥스는 이 확률에 대한 불확실성을 평가하여 \\(M\\) 인식 지수 전반에 걸친 표본 분산을 계산한다. 그런 다음, 알고리즘은 불확실성을 최대화하기 위해 응답 쌍을 선택한다. 직관적으로 이것은 피드백 정보성의 척도를 극대화하는 것으로 생각할 수 있다.\n' +
      '\n' +
      '인포맥스의 가능한 한계는 알고리즘이 최상의 응답을 선택하는 데 유용한 정보인지 여부에 관계없이 보상에 대한 정보를 찾는 데 투자한다는 것이다. 예를 들어, 인포맥스는 이전 피드백에 기초하여 이미 결정된 응답에 할당된 보상 추정치를 불량한 선택으로 정제하는 데 투자할 수 있다. 반면에 이중 톰슨 샘플링(Wu and Liu, 2016)은 최상의 응답을 식별하는 데 도움이 되는 쿼리에 더 중점을 두는 경향이 있다. 섹션 5에서 볼 수 있듯이 이중 TS는 볼츠만 탐사뿐만 아니라 인포맥스의 성능도 향상한다.\n' +
      '\n' +
      '직관적으로 이중 TS(알고리즘 6)는 각각이 최적일 가능성이 어느 정도 있는 두 개의 응답을 선택하는 것을 목표로 한다. 알고리즘 4와 5와 마찬가지로 우리는 \\(N\\) 응답을 샘플링하는 것으로 시작한다. 이 중 2개의 응답은 \\(p\\)에서 두 개의 인식 지수를 샘플링하고 각각에 의해 규정된 보상 전반에 걸쳐 최대화함으로써 선택된다. 샘플이 동일한 경우 두 번째 반응이 다를 때까지 다시 샘플링됩니다. 만약 \\(K\\) 반복 후에 차이가 없다면, 두 번째 응답은 대신 균일하게 샘플링된다.\n' +
      '\n' +
      '```\n' +
      '1:\\(x\\), \\(\\pi\\), \\((r,p)\\)\n' +
      '2: 샘플 응답 \\(\\tilde{y}_{1},\\ldots,\\tilde{y}_{N}\\sim\\pi(\\cdot|x)\\)\n' +
      '3: 응답 \\(i\\in\\arg\\max_{n}r(x,\\tilde{y}_{n}|z)\\) 선택\n' +
      '4:repeat\n' +
      '5: 샘플 인덱스 \\(z^{\\prime}\\sim p\\)\n' +
      '6: 응답 \\(i^{\\prime}\\in\\arg\\max_{n}r(x,\\tilde{y}_{n}|z^{\\prime})\\) 선택\n' +
      '7: \\(K\\)Try 후, 대신 샘플 \\(i^{\\prime}\\sim\\text{unif}(1,\\ldots,N)\\)\n' +
      '8:until\\(i^{\\prime}\\neq i\\)return\\(y_{i},y_{i^{\\prime}}\\)\n' +
      '```\n' +
      '\n' +
      '**알고리즘 6** 이중 톰슨 샘플링\n' +
      '\n' +
      '## 5 실증 결과\n' +
      '\n' +
      '실험에서 각 에이전트는 상호 작용의 시작 시점에 \\(B=32\\) 프롬프트의 배치를 받은 다음 각 프롬프트에 대해 한 쌍의 응답을 생성하여 쿼리를 형성한다. 각 에이전트의 \\(B=32\\) 질의는 선호도 시뮬레이터에 제출되어 \\(B=32\\) 비트의 피드백이 생성된다. 각 에이전트는 \\(B=32\\) 데이터 포인트의 배치를 리플레이 버퍼에 삽입한다. 리플레이 버퍼는 FIFO(first-in-first-out) 버퍼이며, 최대 용량은 \\(C=3200\\) 데이터 포인트이다. 즉, 재생 버퍼는 최대 100개의 가장 최근의 에폭으로부터 선호도 데이터를 보유한다. 각 에폭의 끝에서 각 에이전트는 섹션 3에서 논의된 바와 같이 보상 모델을 업데이트한다.\n' +
      '\n' +
      '각 탐사 알고리즘은 제미니 나노에서 샘플링된 \\(N\\) 후보에서 각 응답 쌍을 선택한다는 것을 기억하라. 실험에서는 \\(N=100\\)을 설정하였다. 제2절에서 설명한 바와 같이 2048개의 표본 외 만능 기초 평가 프롬프트에서 제미니 나노에 대한 승률 측면에서 성능을 평가하며, 본 평가에서 선택된 각 응답은 에이전트의 보상 모델에 따라 제미니 나노에 의해 샘플링된 \\(N=100\\) 후보 중 가장 높은 점수를 받도록 선택된다. 우리는 훈련 및 평가 파이프라인 모두에서 \\(N=100\\) 응답을 사용한다는 점에 유의한다.\n' +
      '\n' +
      '특이점 추정을 위해 각 층에 128개의 은닉 단위가 있는 두 개의 은닉층으로 구성된 피드포워드 다층 퍼셉트론(MLP)을 사용한다. ENN 아키텍처로서, 우리는 각각의 개별 MLP를 입자로 지칭하는 \\(S=10\\) MLP의 컬렉션을 활용한다. 앙상블의 각 입자는 2개의 128개의 단위 은닉층으로 구성된다. 기준분포 \\(p_{z}\\)는 \\(\\{1,2,\\ldots,S\\}\\)상의 균일분포로 정의된다. \\(\\text{Unif}(1,2,\\ldots,S)\\)에서 샘플링된 인식 지수\\(z\\)를 선택할 때 입자\\(z\\)을 사용하여 특정 지수\\(z\\)의 출력을 생성한다. 섹션 3.2에 제시된 ENN 손실 함수는 각각을 초기 매개변수로 정규화하여 입자 전반에 걸친 다양성을 유지한다.\n' +
      '\n' +
      '볼츠만 탐사 계획을 위해 우리는 여러 온도를 휩쓸었고 작은 온도가 가장 좋은 결과를 낳는다는 것을 발견했다. 유사한 수준의 성능이 볼츠만을 사용하여 탐욕스럽게 응답과 두 번째 응답 중 하나를 선택하는 볼츠만 스킴의 변형에 의해 달성되었다. 더 자세한 내용은 부록 C에서 확인할 수 있다.\n' +
      '\n' +
      '인포맥스의 경우 30개의 인식 지수를 사용하여 평균과 분산을 계산했다. 이중 TS 에이전트의 경우, \\(K=30\\)에 대해 뚜렷한 두 번째 반응을 생성하기 위한 최대 시도 횟수를 설정했다.\n' +
      '\n' +
      '부록 B는 하이퍼파라미터 선택 프로세스에 대한 자세한 내용을 제공합니다.\n' +
      '\n' +
      '탐험 알고리즘의### 평가\n' +
      '\n' +
      '그림 5 도표는 상호 작용의 서로 다른 수의 시대에 걸쳐 각 에이전트의 비율을 얻는다. 5개의 무작위 씨앗에 걸쳐 평균을 내어 얻은 결과는 적극적인 탐구가 학습을 가속화하고 더 높은 승률을 초래한다는 것을 분명히 보여준다. 특히 이중 TS 에이전트가 최고 수행자로 등장한다.\n' +
      '\n' +
      '우리는 인포맥스가 초기 시대에 비해 매우 잘 수행되지만 나중에 이중 TS에 크게 미치지 못한다는 것을 관찰한다. 이러한 차이는 해당 정보가 바람직한 반응에 도움이 되는지 여부에 관계없이 정보를 찾는 인포맥스의 성향 때문일 수 있다.\n' +
      '\n' +
      '그림 5의 각 성능 곡선은 수렴하는 것처럼 보이지만 인간 상호 작용의 양이 증가함에 따라 지속적인 개선을 희망할 것이다. 피드백을 통해 학습된 유효 매개변수 수로 느슨하게 생각할 수 있는 보상 모델 용량 - 개선 정도를 측정합니다. 모든 용량에 대해 쿼리의 수가 증가함에 따라 수렴을 기대할 수 있습니다. 용량의 증가는 증가된 계산의 비용으로 추가적인 개선을 가능하게 한다. 이는 아루무감(Arumugam)과 반로이(Van Roy, 2021)가 에이전트가 탐색하기를 기대하는 기간에 따라 학습 대상의 복잡성을 조절하는 것이 유익하다고 설명한 개념과 관련이 있다.\n' +
      '\n' +
      '피드백의 크기에 따른### 스케일링\n' +
      '\n' +
      '편의를 위해 섹션 1에서 재현된 그림 1은 이중 TS의 성능에 맞추기 위해 대안에서 필요한 쿼리 수를 표시하며, 이는 우리가 고려한 탐색 알고리즘 중 가장 효율적인 것으로 나타났다. 줄거리는 결정적이지 않지만, 우리는 그것들이 오목하다는 것을 식별한다. 주어진 수준의 성능을 달성하는 데 필요한 데이터의 백분율 감소 측면에서 효율적인 탐색의 이점을 측정한다고 가정하자. 그림 1의 플롯의 공동은 인간 피드백 데이터의 규모가 증가함에 따라 효율적인 탐사에 의해 제공되는 이점도 증가한다는 것을 의미한다. \\(30,000\\) 수동 쿼리에 의해 달성되는 성능 수준에 대해 이중 TS\n' +
      '\n' +
      '데이터 요구량을 크기만큼 줄입니다. 매혹적인 가능성은 상호 작용의 수가 수십억으로 증가함에 따라 효율적인 탐사가 몇 가지 규모에 이르는 승수 효과를 제공할 수 있다는 것이다. 이것은 초인적인 창의성에 도달하는 데 수십 년이 걸릴 가능성이 있다.\n' +
      '\n' +
      '불확실성 추정의 품질\n' +
      '\n' +
      '볼츠만 탐사는 포인트 추정 보상 모델을 기반으로 쿼리를 선택하는 알고리즘 중에서 가장 잘 수행되었다. 이중 TS에 의해 입증된 큰 개선은 ENN 보상 모델이 제공하는 불확실성 추정에 의해 활성화된다.\n' +
      '\n' +
      '불확실성 추정의 품질은 쌍방향 공동 마이너스 로그 손실(NLL)(Osband et al., 2022)의 관점에서 평가될 수 있다. 그림 6과 7은 각각 \\(40,000\\) 쿼리에 대해 훈련된 점 추정 및 ENN 보상 모델에 대한 한계 및 이분산 조인트 NLL을 보여준다. 이 그림은 두 가지 모두\n' +
      '\n' +
      '그림 5: 수동, 볼츠만, 인포맥스 및 이중 TS 탐사 알고리즘을 사용한 성능. 우리는 적극적인 탐구가 동일한 양의 데이터로 훨씬 더 나은 수준의 성능으로 이어진다는 것을 알 수 있다. 이중 TS 탐색 방식은 최고의 성능 수준으로 이어집니다.\n' +
      '\n' +
      '보상 모델은 유사한 한계 NLL을 렌더링하며, ENN 보상 모델은 매우 유리한 쌍방향 조인트 NLL을 제공한다. 이것은 우리의 ENN 보상 모델이 실제로 의미 있는 불확실성 추정치를 생성한다는 정상 확인 역할을 한다.\n' +
      '\n' +
      '우리는 또한 점 추정을 위한 하이퍼파라미터 선택과 탐색 알고리즘이 사용하는 ENN 보상 모델을 안내하기 위해 쌍방향 조인트 NLL을 사용했다. 특히, 학습률을 위한 후보들을 훑어보고, 다중 에폭시에 걸쳐 에이전트를 훈련시켜 학습률을 식별함으로써 쌍관절 NLL을 최소화하였다.\n' +
      '\n' +
      '프롬프트의 삶\n' +
      '\n' +
      '우리의 결과는 이중 TS가 대안보다 더 나은 반응에 수렴하는 경향이 있음을 나타낸다. 이것이 어떻게 발생하는지 보다 구체적으로 이해하기 위해 모델이 특정 프롬프트에 대한 반응에 할당하는 보상의 진화를 연구합시다. 이 조사를 단순화하기 위해 우리는 이중 TS와 볼츠만 탐사를 비교할 것이다.\n' +
      '\n' +
      '포인트 추정 보상 모델에 대한 결정을 기반으로 하는 알고리즘 중 볼츠만 탐사가 최고의 수행자임을 발견했음을 상기하십시오. 반면에 이중 TS는 ENN 보상 모델에서 제공하는 불확실성 추정치를 사용한다. 평가된 데이터 세트에서 선택된 단일 프롬프트 및 두 응답과 관련된 추정치를 조사할 것이다. 첫 번째는 더블 TS가 도달하는 응답이고, 두 번째는 볼츠만 탐사가 도달하는 응답이다. 인간 피드백 시뮬레이터는 시간의 57.5%의 첫 번째 프롬프트에 대한 선호도를 나타낸다.\n' +
      '\n' +
      '그림 8은 첫 번째 반응이 선호될 확률의 각 보상 모델이 제공하는 예측을 보여준다. 수평 점선은 피드백 시뮬레이터가 제1 응답에 대한 선호도를 표현하는 0.575의 확률을 나타낸다. 예측은 보상 모델이 쿼리에서 학습함에 따라 진화한다. 40,000개의 질의 후, 이중 TS는 1/2보다 큰 예측에 도달하여 첫 번째 응답에 대한 선호도를 표현한다. 반면 볼츠만 탐사는 2분의 1도 안 되는 예측으로 2분의 1에 대한 선호도를 표현한다.\n' +
      '\n' +
      '또한 그림에는 ENN 보상 모델에 의해 표현된 불확실성에 기초한 2-표준 편차 신뢰 구간이 표시된다. 일부 지점에서 이중 TS가 1/2 미만으로 예측되지만 신뢰 구간의 상한은 1/2 이상으로 유지된다. 따라서 어떤 것이 더 나은 반응인지에 대해서는 아직 불확실하다. 이러한 불확실성을 해결함에 있어서, 그것은 회복되어 1/2 이상의 예측에 도달한다. 반면에 볼츠만 탐사는 불확실성 추정치에 의해 유도되지 않으므로 잘못된 예측으로부터 회복되지 않는다.\n' +
      '\n' +
      '##6 클로징 리마크\n' +
      '\n' +
      '우리가 제시한 결과는 대규모 언어 모델을 조정하는 데 적극적인 탐구의 실질적인 이점을 입증하는 첫 번째 결과를 나타낸다. 말하자면, 이 지역에서 더 많은 일을 할 여지가 있다. 이 논문을 마무리하기 위해 몇 가지 중요한 연구 방향에 대해 논의한다.\n' +
      '\n' +
      '우리의 실험은 MLP의 앙상블로 구성된 특히 간단한 ENN 아키텍처를 사용했다. (Osband et al., 2023)에서 입증된 바와 같이, 대안 아키텍처는 계산 요구 사항과 불확실성 추정의 품질 사이에서 보다 효과적인 트레이드오프를 겪는다. 또한, MLP에 기초하여 ENN을 설계하는 대신에, 특히 인간 피드백 데이터의 양이 증가함에 따라, ENN 설계를 변압기 아키텍처에 기초함으로써 성능을 향상시키는 것이 가능할 수 있다.\n' +
      '\n' +
      '우리의 보상 모델 아키텍처의 또 다른 한계는 각각이 LLM의 마지막 레이어 임베딩을 입력으로 하는 "헤드"에 불과하다는 것이다. LLM 몸통도 튜닝하여 성능을 향상시킬 수 있습니다. 효율적인 탐사에 의해 제공되는 이점은 확장되어야 하지만 LLM을 더 조정하면서 탐사를 위한 가장 효과적인 아키텍처 및 알고리즘을 식별하는 것은 향후 작업을 위해 남아 있다.\n' +
      '\n' +
      '마지막으로, 다중항 대화의 효율적인 탐사는 향후 연구에 흥미롭고 중요한 방향을 제시한다. 본 논문에서는 탐구를 고립된 상태에서 바람직하다고 판단되는 대응을 신속하게 파악하기 위한 수단으로 보았다. 다중턴 대화 상자에서 응답은 후속 상호 작용을 형성하는 방식으로 인해 대신 선택될 수 있습니다. 심층 탐사의 주제는 에이전트가 순차적인 상호 작용을 구성하는 효과적인 응답을 어떻게 효율적으로 식별할 수 있는지를 다룬다(Osband et al., 2016, 2019). 대화를 개선하기 위해 심층 탐색 알고리즘을 활용하는 것은 여전히 과제로 남아 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al. (2016) Anil, R., Dai, A. M., Firat, O., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Shafey, L., Chu, E., Mishra, E., Moreira, K., Opernick, S., Tay, Y., Xiao, Y., Zhang, Y., Abrego, J., Brahma, S., Brooks, K., Catasta, Y., Diaz, M., Deng, S., Dyer, J., Gerzalez, L., Gur-Ari, G., H., Hu, L., H., Hurwitz, J.,\n' +
      '\n' +
      '도 8: 특정 프롬프트에 대해, 점선은 시뮬레이터가 다른 응답보다 한 응답에 대한 선호도를 표현할 확률을 나타낸다. 불확실성 추정은 이중 TS가 볼츠만 탐사가 하지 않는 부정확한 예측으로부터 복구할 수 있게 한다.\n' +
      '\n' +
      'J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Lan, C., Lee, B., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., R., Ros, A., Roy, D., Saeta, B., Samuel, R. 팜 2 기술 보고서, 2023년\n' +
      '* Arumugam and Van Roy (2021) Arumugam, D. and Van Roy, B. 결정: 율-왜곡 접근법. In _Proceedings of the 38th International Conference on Machine Learning_, pp. 373-382, 2021.\n' +
      '* Badia et al. (2020) Badia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., et al. Never 포기: Learning directed exploration strategies. _ arXiv preprint arXiv:2002.06038_, 2020.\n' +
      '* Bai et al. (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. training helpful and harmless assistant with reinforcement learning from human feedback. _ ArXiv:2204.05862_, 2022.\n' +
      '* Bellemare et al. (2016) Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. 카운트 기반 탐색과 내재적 동기를 통합합니다. _ 신경 정보 처리 시스템_, 29, 2016의 발전.\n' +
      '* Bradley and Terry (1952) Bradley, R. A. and Terry, M. E. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. _ Biometrika_, 39(3/4):324-345, 1952.\n' +
      '* Burda et al. (2018) Burda, Y., Edwards, H., Storkey, A., and Klimov, O. 랜덤 네트워크 증류에 의한 탐사. _ arXiv preprint arXiv:1810.12894_, 2018.\n' +
      '* Dudik et al. (2015) Dudik, M., Hofmann, K., Schapire, R. E., Slivkins, A., and Zoghi, M. 맥락적 결투 강도들 In _Conference on Learning Theory_, pp. 563-587. PMLR, 2015.\n' +
      '* Dwaracherla et al. (2020) Dwaracherla, V., Lu, X., Ibrahimi, M., Osband, I., Wen, Z., and Van Roy, B. Hypermodels for exploration. _International Conference on Learning Representations_, 2020.\n' +
      '* Glaese et al. (2022) Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., et al. ArXiv:2209.14375_, 2022.\n' +
      '* Glorot and Bengio(2010) Glorot, X. 및 벵지오, Y. 딥 피드포워드 신경망을 훈련하는 것의 어려움을 이해한다. In _Proceedings of the 13th international conference on artificial intelligence and statistics_, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.\n' +
      '* Hennig and Schuler(2012) Hennig, P. and Schuler, C. J. Entropy search for information-efficient global optimization. _ Journal of Machine Learning Research_, 13(6), 2012.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J., and Sifre, L. 계산 최적 대형 언어 모델 훈련에 대한 경험적 분석. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 30016-30030. Curran Associates, Inc., 2022.\n' +
      '\n' +
      '* Houthooft et al. (2016) Houthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. Vime: Variational information maximizing exploration. Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.\n' +
      '* Lattimore & Szepesvari (2020) Lattimore, T. 및 Szepesvari, C. _Bandit Algorithms_. 2020년 케임브리지 대학 출판사\n' +
      '* Lu & Van Roy (2017) Lu, X. 그리고 밴 로이, B. 앙상블 샘플링. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, pp. 3260-3268, 2017.\n' +
      '* MacKay(1992) MacKay, D. J. Information-based objective functions for active data selection. _ Neural computation_, 4(4):590-604, 1992.\n' +
      '* OpenAI(2022) OpenAI. ChatGPT: Optimizing Language Models for Dialogue, 2022. URL[https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).\n' +
      '* OpenAI(2023) OpenAI. GPT-4 기술 보고서 기술 보고서 오픈아이, 2023\n' +
      '* Osband et al. (2016) Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN. Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.\n' +
      '* Osband et al. (2019) Osband, I., Van Roy, B., Russo, D. J., and Wen, Z. 무작위 값 함수를 통한 심층 탐색 Journal of Machine Learning Research_, 20(124):1-62, 2019.\n' +
      '* Osband et al. (2022) Osband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Hao, B., Ibrahimi, M., Lawson, D., Lu, X., O\'Donoghue, B., and Van Roy, B. The neural testbed: Evaluation joint predictions. In _Advances in Neural Information Processing Systems_, volume 35. Curran Associates, Inc., 2022.\n' +
      '* Osband et al. (2023a) Osband, I., Wen, Z., Asghari, M., Dwaracherla, V., Ibrahimi, M., Lu, X., and Van Roy, B. Epistemic neural networks. _ 신경 정보 처리 시스템_, 34, 2023a의 발전.\n' +
      '* Osband et al. (2023b) Osband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Ibrahimi, M., Lu, X., and Van Roy, B. Approximate Thompson sampling via epistemic neural networks. In Evans, R. J. and Shpitser, I. (eds.), _Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence_, volume 216 of _Proceedings of Machine Learning Research_, pp. 1586-1595. PMLR, 31 Jul-04 Aug 2023b.\n' +
      '* Ostrovski et al. (2017) Ostrovski, G., Bellemare, M. G., Oord, A., and Munos, R. 신경 밀도 모델을 사용한 카운트 기반 탐색. In _International conference on machine learning_, pp. 2721-2730. PMLR, 2017.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. 인간의 피드백으로 지침을 따르도록 언어 모델을 훈련합니다. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), _Advances in Neural Information Processing Systems_, volume 35, pp. 27730-27744. Curran Associates, Inc., 2022.\n' +
      '* Riquelme et al. (2018) Riquelme, C., Tucker, G., and Snoek, J. Deep Bayesian bandits showdown: empirical comparison of Bayesian deep networks for Thompson sampling. _ arXiv preprint arXiv:1802.09127_, 2018.\n' +
      '* Russo & Van Roy (2014) Russo, D and Van Roy, B. Learning to optimize by information-directed sampling. _ 신경 정보 처리 시스템_, 27:1583-1591, 2014에서의 발전.\n' +
      '* Russo et al. (2018) Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. 톰슨 샘플링에 대한 자습서 Foundations and Trends(r) in Machine Learning_, 11(1):1-96, 2018.\n' +
      '\n' +
      '* Ryzhov et al. (2012) Ryzhov, I. O., Powell, W. B., and Frazier, P. I. The knowledge gradient algorithm for general class of online learning problems. _ Operations Research_, 60(1):180-195, 2012.\n' +
      '* Sadigh et al. (2018) Sadigh, D., Landolfi, N., Sastry, S. S., Seshia, S. A., and Dragan, A. D. Planning for the human actions for planning and active information gathering over human internal state. _ Autonomous Robots (AURO)_, 42(7):1405-1426, October 2018. ISSN 1573-7527. doi: 10.1007/s10514-018-9746-1.\n' +
      '* Saha(2021) Saha, A. 최적 알고리즘 for stochastic context preference bandits. _ 신경 정보 처리 시스템_, 34:30050-30062, 2021에서의 발전.\n' +
      '* Stiennon et al. (2020) Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. 휴먼 피드백으로 요약하는 학습. _ 2020년, 신경망 정보 처리 시스템_, 33:3008-3021의 발전.\n' +
      '* Sun et al. (2011) Sun, Y., Gomez, F., and Schmidhuber, J. Planning to surprise: Optimal Bayesian exploration in dynamic environment. Shmidhuber, J., Thorisson, K. R., and Looks, M. (eds.), _Artificial General Intelligence_, pp. 41-51, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.\n' +
      '* Team et al. (2023) Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models, 2023.\n' +
      '* Thompson(1933) Thompson, W. R. on the likelihood of one unknown probability exceeds another of the evidence of two samples. _ Biometrika_, 25(3/4):285-294, 1933.\n' +
      '* Wu and Liu(2016) Wu, H. and Liu, X. 결투하는 도적들을 위한 더블 톰슨 샘플링. Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016.\n' +
      '* Yue et al. (2012) Yue, Y., Broder, J., Kleinberg, R., and Joachims, T. 무장한 도적들 문제. _ Journal of Computer and System Sciences_, 78(5):1538-1556, 2012.\n' +
      '* Zhang et al. (2020) Zhang, W., Zhou, D., Li, L., and Gu, Q. 신경 탐슨 샘플 채취 arXiv preprint arXiv:2010.00827_, 2020.\n' +
      '* Zhou et al. (2020) Zhou, D., Li, L., and Gu, Q. ucb 기반 탐사가 있는 신경 맥락적 도적. In _International Conference on Machine Learning_, pp. 11492-11502. PMLR, 2020.\n' +
      '\n' +
      '## 부록 : 인간 선호 시뮬레이터\n' +
      '\n' +
      '우리는 선호 시뮬레이터를 구성하기 위해 오라클 보상 모델을 사용한다. 프롬프트 및 2개의 잠재적인 응답들을 포함하는 쿼리가 주어지면, 선호도 시뮬레이터는 2개의 응답들 각각에 대한 제1 컴퓨팅 스코어들에 의해 2개의 응답들 사이의 선호도를 나타내는 이진 피드백을 생성한다. 이러한 점수로부터 선호 확률을 시뮬레이션하기 위해, 우리는 결정 분석에서 잘 확립된 방법론인 브래들리-테리 모델 브래들리&테리(1952)를 사용한다. 학습 파이프라인에서 선호 확률로부터 추출된 이진 피드백을 사용하는 반면, 평가 파이프라인에서 선호 확률을 직접 사용한다. 평가 파이프라인에서 선호 확률을 직접 사용하는 것은 평가의 확률성을 줄이는 것이다.\n' +
      '\n' +
      '오라클 보상 모델 자체는 임베딩 생성을 담당하는 몸통 및 스칼라 추정치를 생성하는 선형 레이어 헤드를 특징으로 하는 2-부분 아키텍처로 구성된다. 몸통은 미리 훈련된 제미니 프로 트랜스포머 몸통에 초기화되며, 선형 헤드는 Xavier 초기화(Glorot&Bengio, 2010)를 사용한다. 훈련 과정은 Anthropic Helpfulness 및 Harmlessness 데이터셋에 적용된 교차 엔트로피 손실 함수를 통해 몸통 및 보상 헤드를 모두 최적화하는 과정을 포함한다(Bai et al., 2022).\n' +
      '\n' +
      '우리의 오라클 보상 모델은 인체 유해성 평가 데이터 세트에서 0.755, 인체 유해성 평가 데이터 세트에서 0.748의 정확도를 달성했다. 특히, 이러한 성능 수준은 (Bai et al., 2022)에 보고된 가장 큰 모델의 성능보다 높다.\n' +
      '\n' +
      '제미니 프로는 제미니 나노보다 훨씬 크기 때문에 에이전트가 사용할 수 있는 것보다 훨씬 복잡한 모델에 의해 선택된다는 점에 유의해야 한다. 이러한 규모의 차이는 인간이 에이전트에 의해 모델링된 것보다 더 복잡한 행동을 보일 수 있다는 사실을 반영하기 위한 것이다.\n' +
      '\n' +
      '## 부록 B 실험용 하이퍼파라미터 선택\n' +
      '\n' +
      '우리는 성능을 최적화하기 위해 에이전트의 하이퍼 파라미터를 조정합니다. 이러한 하이퍼파라미터는 상호작용의 각 에포크 후에 실행되는 l2 정규화 강도, 학습 속도 및 확률적 경사 하강(SGD) 단계의 수를 포함한다. 훈련 과정의 모든 확률적 경사 하강(SGD) 단계는 에이전트의 재생 버퍼에서 무작위로 샘플링된 데이터 배치에서 실행된다.\n' +
      '\n' +
      '점 추정 모델과 ENN 보상 모델 모두 학습률을 \\(\\{1e-6,1e-5,1e-4\\}\\) 이상으로 스윕하여 최적의 학습률을 선택한다. 섹션 5.3에 설명된 공동 nll 실험과 능동 학습 실험 모두에서 최상의 학습률이 일치한다는 것을 발견했다.\n' +
      '\n' +
      '데이터 수집 프로세스의 진화하는 특성에 적응하기 위해 정규화 강도에 대한 붕괴 전략을 구현한다. 식 1과 식 3에서 \\(\\lambda\\)로 표시된 정규화 강도는 에이전트에 의해 축적된 데이터의 부피에 따라 조정된다. 구체적으로, 리플레이 버퍼로부터 \\(B=32\\)의 선호 데이터 포인트들을 포함하는 배치에서 매 에폭에서 수행되는 각각의 확률적 경사 하강(SGD) 단계에 대해, \\(\\lambda=\\frac{32\\times\\lambda^{\\prime}}{|\\mathcal{D}|}\\)으로 주어지는 감쇠 정규화 강도를 통합한다. 여기서 \\(\\mathcal{D}\\)은 에이전트에 의해 축적되는 피드백 데이터 포인트들의 총 수를 나타내고, 모든 에이전트에 대해 \\(\\{0.1,1,10,100,1000\\}\\)에 걸쳐 스위핑함으로써 \\(\\lambda^{\\prime}\\)을 조정한다.\n' +
      '\n' +
      '또한 상호 작용의 각 시기 후에 수행된 sgd 단계의 수를 \\(\\{1,10,100\\}\\)에서 휩쓸었다. 우리는 인포맥스 에이전트가 더 많은 sgd 단계를 실행함으로써 이점이 있는 반면 다른 에이전트의 성능은 과적합으로 인해 점 이상으로 악화된다는 것을 관찰했다.\n' +
      '\n' +
      'ENN 모델의 경우 앙상블 입자의 다양성을 조절하는 역할을 하는 출력 스케일 파라미터도 조정한다. 특히, 에이전트당 가장 좋은 성능을 얻을 수 있는 값을 \\(\\{0.1,1,10\\}\\)으로 스윕하여 선택한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '그림 10. 특히 온도 매개변수를 미세 조정한 결과 알고리즘 4에 제시된 바와 같이 Greedy-Boltzmann이 표준 볼츠만 에이전트에 의해 달성된 성능보다 향상되지 않음을 나타낸다.\n' +
      '\n' +
      '도 10 | 볼츠만에 대한 다른 온도에 걸친 그리디-볼츠만의 성능. 우리는 최고의 그리디 볼츠만과 최고의 볼츠만 에이전트가 매우 유사하게 공연하는 것을 볼 수 있는데, 그리디 볼츠만은 볼츠만보다 이점을 제공하지 않는다.\n' +
      '\n' +
      'Boltzmann 및 Greedy-Boltzmann 에이전트는 알고리즘 4와 7에 사용된 온도에 의해 결정되는 다양한 탐사 전략을 근사화하는 것으로 개념화될 수 있다. 이는 가장 좋은 두 응답의 선택을 포함하는 "탐욕스러운" 탐사, 첫 번째 응답이 탐욕스럽게 선택되고 두 번째 응답이 무작위로 선택되는 "탐욕스러운" 탐사, 두 응답이 무작위로 샘플링되는 "수동적인" 탐사와 같은 예를 포함한다. 따라서 볼츠만과 그리디 볼츠만의 성능을 평가할 때, 우리는 포인트 추정 보상 모델에서 파생된 광범위한 탐사 계획을 본질적으로 평가하고 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>