<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# RT-Sketch: Hand-Drawn 스케치로부터 목표조건 모방학습\n' +
      '\n' +
      ' Priya Sundaresan\\({}^{1,3}\\), Quan Vuong\\({}^{2}\\), Jiayuan Gu\\({}^{2}\\), Peng Xu\\({}^{2}\\), Ted Xiao\\({}^{2}\\), Sean Kirmani\\({}^{2}\\), Tianhe Yu\\({}^{2}\\)\n' +
      '\n' +
      'Michael Stark\\({}^{3}\\), Ajinkya Jain\\({}^{3}\\), Karol Hausman\\({}^{1,2}\\), Dorsa Sadigh\\({}^{*1,2}\\), Jeannette Bohg\\({}^{*2}\\), Stefan Schaal\\({}^{*3}\\)\n' +
      '\n' +
      '({}^{*}\\)equal advising, alphabetical order order\n' +
      '\n' +
      'Stanford University, \\({}^{2}\\)Google DeepMind, \\({}^{3}\\)Google Intinsic\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '자연어와 이미지는 일반적으로 목표 조건 모방 학습(IL)에서 목표 표상으로 사용된다. 그러나 자연어는 모호할 수 있고 이미지는 지나치게 특정될 수 있다. 이 연구에서는 목표 명세를 위한 촬영장비로서 손으로 그린 스케치를 연구한다. 스케치는 사용자가 언어처럼 즉석에서 제공하기 쉽지만 이미지와 유사하게 다운스트림 정책이 공간적으로 인식되도록 돕고 심지어 이미지를 넘어 작업 관련 객체로부터 작업 관련 객체를 명확하게 하는 데 도움이 될 수 있다. 우리는 원하는 장면을 손으로 그린 스케치를 입력으로 하고 동작을 출력하는 조작을 위한 목표 조건 정책인 RT-Sketch를 제시한다. 우리는 합성적으로 생성된 목표 스케치와 쌍을 이루는 궤적 데이터 세트에 대한 RT-스케치를 훈련한다. 본 논문에서는 관절형 조리대에서 테이블탑 객체 재배열과 관련된 6가지 조작 기술에 대한 접근 방법을 평가한다. 실험적으로 우리는 RT-스케치가 언어 목표가 모호하거나 시각적 산만자가 있을 때 더 큰 견고성을 달성하는 동시에 간단한 설정에서 이미지 또는 언어 조건 에이전트와 유사한 수준에서 수행할 수 있음을 발견했다. 또한 RT-스케치가 최소 선 도면부터 상세 색상 도면에 이르기까지 다양한 수준의 특이성을 가진 스케치를 해석하고 수행할 수 있음을 보여준다. 보충 자료 및 동영상은 저희 홈페이지를 참고하시기 바랍니다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '가정, 직장 또는 산업 환경에서 인간과 함께 작동하는 로봇은 지원 및 자율성에 대한 엄청난 잠재력을 가지고 있지만 인간_로봇에게 전달하는 가장 쉬운 목표 표현과 해석 및 행동하는 로봇_에 대한 신중한 고려가 필요하다.\n' +
      '\n' +
      '명령어 추적 로봇은 자연어 명령의 직관적인 인터페이스를 언어 조건 모방 학습 정책[8, 9, 23, 28, 29]의 입력으로 사용하여 이 문제를 해결하려고 시도한다. 예를 들어, 가정용 로봇에게 저녁 식탁을 차리라고 부탁하는 것을 상상해 보세요. "_" 테이블 위에 식기, 냅킨, 판을 놓는 것과 같은 언어 설명은 과소 지정되거나 모호하다. 식기가 접시나 냅킨에 대해 정확히 어떻게 배치되어야 하는지, 또는 서로 간의 거리가 문제인지 여부는 불분명하다. 이러한 더 높은 수준의 정밀도를 달성하기 위해, 사용자는 _"포크를 플레이트의 우측에 2cm, 및 테이블의 최좌측에 5cm"와 같은 더 긴 설명을 제공할 필요가 있을 수 있다._, 또는 심지어 온라인 수정(_"아니오, 당신은 너무 멀리 오른쪽으로 이동했고, 비트를 뒤로 이동했습니다."_)[15, 29] 언어는 목표를 지정하는 직관적인 방법이지만, 그 질적 특성과 모호성은 인간이 긴 지시나 수정 없이 제공하는 것과 로봇 정책이 다운스트림 정밀 조작을 위해 해석하는 것을 불편하게 할 수 있다.\n' +
      '\n' +
      '목표 이미지를 사용하여 목표를 지정하고 언어 지침이 있거나 없는 목표 조건 모방 학습 정책을 훈련하는 것은 최근 몇 년 동안 상당히 성공적인 것으로 나타났다[21, 22, 35]. 이 설정에서 이미지의 경우\n' +
      '\n' +
      '도. 1: (왼쪽) RT-스케치, RT-1 및 RT-목표-이미지를 비교하는 정성적 롤아웃, (오른쪽) (위) 모호한 언어 및 (아래) 시각적 산만자에 대한 RT-스케치의 견고성을 강조한다.\n' +
      '\n' +
      '원하는 최종 상태의 장면은 의도된 목표를 완전히 특정할 수 있다. 그러나 목표 이미지에 대한 접근은 강력한 사전 가정이며, 미리 기록된 목표 이미지는 특정 환경에 묶여 일반화를 위한 재사용이 어려울 수 있다는 단점이 있다.\n' +
      '\n' +
      '요약하자면, 자연어는 매우 유연하지만, 그것은 또한 매우 모호하거나 긴 설명을 요구할 수 있다. 이는 긴 지평선 작업이나 공간 인식이 필요한 작업에서는 빠르게 어려워진다. 한편, 목표 이미지는 불필요한 세부 사항으로 목표를 과도하게 지정하므로 일반화를 위한 인터넷 규모의 데이터가 필요하다.\n' +
      '\n' +
      '이러한 문제를 해결하기 위해 우리는 시각적 모방 학습에서 목표 명세를 위한 편리하면서도 표현적인 양식으로 _hand-drawn 스케치를 연구한다. 최소한으로 인해 스케치는 여전히 사용자가 언어처럼 즉석에서 제공하기 쉽지만 공간적으로 더 인식 가능한 작업 사양을 허용한다. 목표 이미지와 마찬가지로 스케치는 시각적 입력을 취하지만 불필요한 픽셀 수준의 세부 사항을 무시하는 추가 수준의 목표 추상화를 제공하는 기성 정책 아키텍처와 쉽게 통합됩니다. 마지막으로 스케치에서 세부 사항의 품질과 선택적 포함/배제는 다운스트림 정책이 관련 사항과 관련 없는 세부 사항을 구별하는 데 도움이 될 수 있다.\n' +
      '\n' +
      '본 논문에서는 사용자가 원하는 장면을 손으로 그린 스케치를 입력으로 하여 동작을 출력하는 조작의 목표 조건 정책인 RT-Sketch를 제시한다. RT-Sketch의 새로운 아키텍처는 원래 RT-1 언어 대 액션 트랜스포머 아키텍처[9]를 수정하여 언어가 아닌 시각적 목표를 소비하여 스케치, 이미지 또는 시각적으로 나타낼 수 있는 다른 목표에 유연한 컨디셔닝을 허용한다. 이를 위해 우리는 언어를 생략하면서 토큰화 전에 입력으로 목표 스케치 및 관찰 기록을 연결한다. 우리는 수백 개의 이미지-스케치 쌍으로부터 훈련된 이미지-투-스케치 스타일화 네트워크에 의해 생성된 합성 목표 스케치와 쌍을 이루는 \\(80\\)K 궤적의 데이터 세트에서 RT-스케치를 훈련한다.\n' +
      '\n' +
      '우리는 다양한 장면 변화에 따라 서랍이 있는 조리대에서 테이블탑 객체 재배열을 포함하는 실제 로봇에 대한 6가지 조작 기술에 걸쳐 RT-스케치를 평가한다. 이러한 기술에는 서로 가까이 있는 물체를 움직이는 것, 캔을 옆으로 두드리는 것, 캔을 똑바로 놓는 것, 서랍을 닫는 것, 서랍을 여는 것이 포함됩니다. 실험적으로, 우리는 RT-스케치가 간단한 설정에서 이미지 또는 언어 조건 에이전트와 유사한 수준에서 수행된다는 것을 발견했다. 언어 지침이 모호하거나 시각적 산만자가 있는 경우 RT-스케치가 인간 라벨러에 의해 평가된 바와 같이 언어 또는 목표 이미지 조절 정책에 비해 \\(\\sim 2\\)X 더 많은 공간 정밀도 및 정렬 점수를 달성한다는 것을 발견했다(그림 참조). 1(우측) 또한 RT-스케치가 거친 스케치에서 더 많은 장면 보존, 유색 도면에 이르기까지 다양한 수준의 입력 특이성을 처리할 수 있음을 보여준다(그림 참조). 1(좌측).\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '이 절에서는 목표 조건 모방 학습을 위한 사전 방법에 대해 논의한다. 또한 로봇 공학에서 덜 탐구된 목표 조절 양식에 대한 새로운 가능성을 열어주는 이미지 스케치 변환을 향한 지속적인 노력을 강조한다.\n' +
      '\n' +
      '목표조건 모방학습은 명목상의 유사성에도 불구하고, 원하는 장면의 손으로 그린 스케치에 조건화된 조작 정책에 대한 학습은 정책 스케치의 개념[1]과 다르며, 그 하위 구성 요소를 설명하는 과제 구조의 상징적 표현이다. 강화 학습(RL)은 주어진 장면 스케치와 훈련 중에 에이전트가 방문한 상태 사이의 정렬을 정확하게 정량화하는 보상 목표를 정의하는 것은 간단하지 않기 때문에 우리의 시나리오에서 쉽게 적용할 수 없다. 대신 우리는 모방 학습(IL) 기법, 특히 목표 조건 설정에 초점을 맞춘다[17].\n' +
      '\n' +
      '목표 조건 IL은 정책이 동일한 작업에 대한 공간적 또는 의미적 변화를 처리할 수 있어야 하는 설정에서 유용한 것으로 입증되었다[3]. 이러한 설정들은 다수의 객체들[8, 9, 29, 30, 35], 키팅[46], 변형 가능한 객체들을 상이한 구성들로 폴딩[18], 및 클러터에서 상이한 타겟 객체들을 검색[16]을 포함한다. 그러나, 이러한 접근법들은 변형을 특정하기 위해 언어[9, 23, 28, 29, 39] 또는 목표 이미지[16]에 의존하는 경향이 있다. 후속 작업은 목표 이미지와 언어[21], 즉흥 이미지[22] 또는 이미지 임베딩[18, 30, 46] 중 하나에 대한 멀티모달 컨디셔닝을 가능하게 했다. 그러나 이러한 모든 표상은 궁극적으로 어떤 식으로든 원시 이미지 또는 언어에서 파생되며, 이는 명시하기 쉽지만 스케치와 같은 공간 인식을 보존하는 더 추상적인 목표 표상의 가능성을 간과한다.\n' +
      '\n' +
      '목표 표현 측면에서 비유동성 외에도 목표 조건 IL은 실증 데이터에 과도하게 적합되는 경향이 있으며 새로운 시나리오에서 약간의 분포 변화도 처리하지 못한다[37]. 언어-조건화를 위해, 분배 이동은 의미적 또는 공간적 모호성, 신규한 명령 또는 표현, 또는 보이지 않는 객체를 포함할 수 있다[9, 21]. 목표-이미지 컨디셔닝은 조명 또는 객체 외관의 변화, 또는 보이지 않는 배경 텍스처(5, 11)와 같은 아웃-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오 대신 시각적 산만함을 퇴치할 수 있을 만큼 작지만 명확한 목표를 제공할 만큼 표현력이 있는 스케치를 선택합니다. [4] 및 [32]를 포함한 이전 작업은 탐색 및 제한된 조작 설정을 위한 순수 언어에 대한 스케치의 유용성을 보여주었다. 그러나, 이러한 작업들에서 탐색된 스케치는 주로 조작을 위해 관절-레벨에서 낮은 레벨의 동작을 안내하거나, 내비게이션을 위한 명시적인 방향 큐들을 제공하기 위한 것이다. [14] 다른 양식 중에서 스케치를 목표 조건 조작에 대한 입력으로 고려하지만 스케치에 조건화된 정책을 명시적으로 교육하지는 않습니다. 따라서 목표 명세에서 장면 이미지가 스케치 이미지보다 더 낫다는 결론에 도달했다. 우리의 결과는 시각적 산만자의 대상이 되는 인지된 공간 및 의미 정렬에 대한 리커트 등급 측면에서 1.63x 및 1.5x만큼 스케치를 입력으로 하도록 훈련된 정책이 장면 이미지 조건 정책을 능가한다는 점에서 다르고 보완적이다. 가장 최근에, Gu et al. [19]는 _hindsight-trajectory_schedches를 통한 목표-조건 조작의 접근법을 제안한다. 여기서, 스케치는 로봇이 추종하기 위한 원하는 궤적_을 나타내기 위해 이미지 위에 그려진 2D 경로를 나타낸다. 이 작업은 스케치를 의도된 로봇 궤적을 지정하기 위한 _motion-centric_ 표현으로 취급하지만, 우리의 작업에서 스케치는 로봇이 명시적으로 취해야 하는 동작보다는 원하는 시각적 목표 상태를 나타내는 _scene-centric_이다.\n' +
      '\n' +
      '이미지-스케치 변환(Image-Sketch Conversion) 최근, 스케치는 객체 검출[6, 7, 12], 시각적 질문 응답[25, 33], 및 장면 이해[13]과 같은 애플리케이션들에 대해 컴퓨터 비전 커뮤니티 내에서 텍스트 및 이미지 외에 격리된 상태로 또는 추가로 증가하는 인기를 얻고 있다. IL에서 스케치를 가장 잘 통합하는 방법을 고려할 때, 중요한 설계 선택은 (1) 테스트 시간(즉, 스케치를 미리 훈련된 정책과 호환되는 또 다른 목표 모달리티로 변환하는 것), 또는 (2) 트레이닝 시간(즉, 스케치에 컨디셔닝된 IL 정책을 명시적으로 훈련시키는 것)에서 스케치를 고려할지 여부이다. (1)의 경우 주어진 스케치를 먼저 목표 이미지로 변환한 다음 바닐라 목표 이미지 조건 정책을 펼칠 수 있다. 이는 ControlNet[47], GAN 스타일 접근법[24], 또는 Instruct-Pix2Pix[10] 또는 Stable Diffusion[36]과 같은 텍스트-이미지 합성(text-to-image synthesis)과 같은 스케치-이미지 변환을 위한 기존의 프레임워크에 기초할 수 있다. 이러한 모델은 최적의 조건에서 사실적인 결과를 생성하지만 이미지 생성과 스타일 전달을 공동으로 처리하지 않으므로 생성된 이미지가 에이전트 관찰의 스타일과 일치하지 않는다. 동시에 이러한 접근법은 환각 인공물을 생산하여 분포 변화를 도입하기 쉽다[47].\n' +
      '\n' +
      '이러한 문제를 기반으로 대신 (2)를 선택하고 미리 기록된 시연 궤적에서 단말 이미지의 후각 재표지를 위한 이미지 대 스케치 변환 기술을 고려한다. 최근, Vinker 등은 입력 이미지 객체들 또는 장면들의 베지어 곡선 기반 스케치를 예측하기 위한 네트워크들을 제안한다[44, 45]. 스케치 품질은 CLIP 기반 정렬 메트릭에 의해 감독됩니다. 이러한 접근 방식은 높은 시각적 충실도의 스케치를 생성하지만 테스트 시간 최적화는 몇 분 정도의 시간이 소요되며, 이는 로봇 학습 데이터 세트의 일반적인 크기(수백에서 수천 개의 시연 궤적)로 확장되지 않는다. 한편, Pix2Pix[20]와 같은 조건부 생성적 적대 네트워크(cGAN)는 확장 가능한 이미지 대 이미지 변환에 유용한 것으로 입증되었다. 우리의 작업과 관련된 대부분은 Li et al. [26]의 작업으로, \\(5K\\) 쌍을 이루는 이미지와 라인 도면의 대규모 크라우드소싱 데이터셋에 주어진 이미지에서 스케치를 생성하기 위해 Pix2Pix 모델을 훈련한다. 본 논문에서는 로봇 궤적 데이터에 대한 이미지-투-스케치 모델을 미세 조정하기 위해 이 작업을 구축하고, 스케치로부터 다운스트림 조작을 가능하게 하는 유용성을 보인다.\n' +
      '\n' +
      '## III 스케치 조건 모방 학습\n' +
      '\n' +
      '이 절에서는 먼저 스케치 조건 정책을 배우는 문제를 소개하겠습니다. 그런 다음 엔드 투 엔드 스케치 투 액션 IL 에이전트를 훈련하기 위한 접근법에 대해 논의할 것이다. 먼저, 섹션 III-A에서 참조 이미지로부터 스케치를 자동으로 생성하는 보조 이미지 대 스케치 번역 네트워크의 인스턴스화에 대해 논의한다. 섹션 III-B에서는 이러한 모델을 사용하여 합성적으로 생성된 목표 스케치로 기존 시연 데이터 세트에 자동으로 다시 레이블을 지정하고 이 데이터 세트에 대해 스케치 조건 정책을 훈련하는 방법에 대해 논의한다.\n' +
      '\n' +
      '문제 진술 우리의 목표는 원하는 장면 상태의 목표 _sketch_와 상호 작용의 이력을 조건으로 하는 조작 정책을 배우는 것이다. 형식적으로, 우리는 이러한 정책을 \\(\\pi_{\\mathrm{sketch}(a_{t}|g,\\{o_{j}\\}_{j=1}^{t})\\)으로 표시하는데, 여기서 \\(a_{t}\\)은 타임스텝 \\(t\\), \\(g\\in\\mathbb{R}^{W\\times H\\times 3}\\)은 폭 \\(W\\)과 높이 \\(H\\\\), \\(o_{t}\\in\\mathbb{R}^{W\\times H\\times 3}\\)은 시간 \\(t\\)에서의 관찰이다. 추론 시간에 정책은 실행할 작업을 추론하기 위해 RGB 이미지 관찰의 이력과 함께 주어진 목표 스케치를 취한다. 실제로, 우리는 \\(t=1\\)에서 초기 상태로부터의 모든 관측치보다 \\(D\\) 이전의 관측치들의 이력에 \\(\\pi_{\\mathrm{sketch}}\\)을 조건화한다. 이러한 정책을 훈련하기 위해 데이터세트\\(\\mathcal{D}_{\\mathrm{sketch}}=\\{g^{n},\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{(n)}}}_{n=1}^{N}\\)에 대한 접근을 가정한다. 여기서 \\(T^{(n)}\\)은 타임스텝에서 \\(n^{th}\\) 궤적의 길이를 의미한다. 데이터세트의 각 에피소드는 주어진 목표 스케치 및 해당 시연 궤적으로 구성되며 각 타임스텝에서 이미지 관찰이 기록된다. 따라서 본 연구의 목적은 스케치 조건 모방 정책\\(\\pi_{\\mathrm{sketch}}(a_{t}|g,\\{o_{j}\\}_{j=1}^{t})\\)을 학습하기 위한 것이다.\n' +
      '\n' +
      '### _Image-to-Sketch Translation_\n' +
      '\n' +
      '스케치 조건 정책을 훈련하려면 로봇이 달성한 목표 상태의 스케치와 각각 쌍을 이루는 로봇 궤적의 데이터 세트가 필요합니다. 궤적 자체 및 수동으로 그려진 스케치를 포함하여 스케일에서 그러한 데이터 세트를 처음부터 수집하는 것은 쉽게 비실용적일 수 있다. 따라서 본 논문에서는 영상 관측을 취하여 해당 목표 스케치를 출력하는 영상-스케치 변환 네트워크 \\(\\mathcal{T}(g|o)\\)를 학습하는 것을 목표로 한다. 이 네트워크는 기존의 시연 데이터세트\\(\\mathcal{D}=\\{\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{(n)}}\\}_{n=1}^{N}\\)를 후처리하는데 사용될 수 있다. 이는 스케치 기반 IL을 위한 데이터셋을 생성한다: \\(\\mathcal{D}_{\\mathrm{sketch}}=\\{g^{n},\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{(n)}}_{n=1}^{N}\\.\n' +
      '\n' +
      'RT-1 데이터세트 이 작업에서는 사전 작업 [9]에 의해 수집된 기존의 시각적 시연 데이터세트에 의존한다. RT-1은 VR 원격 조작 시위의 대규모 데이터세트(\\(80K\\) 궤적)에서 훈련된 사전 언어 대 행동 모방 학습 에이전트로서, 서로 가까운 곳에 움직이는 물체, 깡통 및 병을 직립 또는 옆으로 배치, 캐비닛 개폐, 조리대 및 서랍에서 픽 앤 플레이스 수행 등의 기술을 포함한다[9]. 여기서는 RT-1 데이터 세트의 용도를 변경하고 섹션 III-B에 자세히 설명된 스케치를 수용하기 위해 RT-1 정책 아키텍처를 추가로 조정한다.\n' +
      '\n' +
      '스케치에 대한 가정은 인간이 장면의 주어진 이미지에 대응하는 스케치를 제공하는 수많은 방법이 있음을 인정한다. 본 연구에서는 제어된 실험 검증 절차를 위해 입력 스케치에 대해 다음과 같은 가정을 한다. 특히, 주어진 스케치는 테이블탑 에지, 서랍 핸들 및 태스크 관련 객체가 스케치에 포함되고 식별될 수 있도록 관련 이미지의 태스크 관련 윤곽을 존중한다고 먼저 가정한다. 스케치의 등고선을 이미지의 등고선과 모서리 정렬 또는 픽셀 정렬이라고 가정하지 않습니다. 우리는 입력 스케치가 최소한 검은색 윤곽으로 구성되어 있으며 색상 음영은 선택 사항이라고 가정한다. 우리는 또한 스케치가 환각 객체, 낙서 또는 텍스트 주석과 같은 관련 이미지에 존재하지 않는 정보를 포함하지 않지만 원본 이미지에 나타나는 작업 관련 세부 정보를 생략할 수 있다고 가정한다.\n' +
      '\n' +
      '이미지-투-스케치 변환 네트워크(\\(\\mathcal{T}\\)를 학습하기 위해 새로운 데이터세트(\\(\\mathcal{D_{\\mathcal{T}}}=\\{(o_{i},g_{i}^{1},\\dots,g_{i}^{L^{(i)}})\\(M\\) 이미지 관측치(o_{i}\\)와 목표 스케치(g_{i}^{1},\\dots,g_{i}^{L^{(i)}})로 구성된 새로운 데이터세트(\\(o_{i}}})를 수집한다. 그들은 동일한 장면을 스케치하는 여러 가지 유효한 방법이 있다는 사실을 설명하기 위해 동일한 이미지의 \\(L^{(i)}\\)_different_ 표현을 나타낸다. 이를 위해 RT-1 데이터세트의 시연궤적에서 무작위로 샘플링된 500장의 단말기 영상을 수집하고, 조작면에 보이는 탁상, 서랍, 관련 객체를 촬영한 흰색 배경에 검은색 선으로 그림을 수동으로 그렸다. 우리는 각 로봇 관찰에 단일 스케치만으로 개인적으로 주석을 달지만, 이 데이터를 훨씬 더 큰 기존 비로봇 데이터 세트에 추가한다[26]. 이 데이터 집합은 이미지당 여러 크라우드소싱 스케치를 통해 스케치 간 변동을 캡처합니다. 우리는 최소한의 표현이 가장 자연스럽다는 것을 발견하기 때문에 수동 스케치에 로봇 암을 포함하지 않습니다. 경험적으로, 우리는 우리의 정책이 암을 볼 수 있는 실제 목표 구성에도 불구하고 그러한 스케치를 처리할 수 있음을 발견한다. 사용자가 원본 이미지 위에 에지 정렬 스케치를 그리는 맞춤형 디지털 스타일러스 드로잉 인터페이스를 사용하여 이러한 도면을 수집한다(부록 도 16). 최종적으로 기록된 스케치는 원본 이미지 치수를 갖는 흰색 캔버스에 블랙으로 사용자의 스트로크를 포함한다.\n' +
      '\n' +
      '이미지-투-스케치 트레이닝은 Pix2Pix 조건부 생성적 적대 네트워크(cGAN) 구조를 갖는 이미지-투-스케치 변환 네트워크(\\(\\mathcal{T}\\)를 구현하는데, 이는 생성기\\(G_{\\mathcal{T}}\\)와 판별기\\(D_{\\mathcal{T}}\\)[20]로 구성된다. 생성기 \\(G_{\\mathcal{T}}\\)는 입력 이미지 \\(o\\), 랜덤 노이즈 벡터 \\(z\\)을 취하여 목표 스케치 \\(g\\)을 출력한다. 판별기\\(D_{\\mathcal{T}}\\)는 인공적으로 생성된 스케치와 지상진실 목표 스케치를 구별하도록 훈련된다. 표준 cGAN 감독 손실을 활용하여 [20, 26]을 모두 훈련한다.\n' +
      '\n' +
      '\\mmathcal{L}_{\\text{cGAN}=\\min_{G_{\\mathcal{T}}\\max_{D_{\\mathcal{T}}}\\mmathbb{E}_{o,g}[\\log D_{\\mathcal{T}}(o,g)]\\\\\\mmathbb{E}_{o,g}[\\log(1-D_{\\mathcal{T}}(o,G_{\\mathcal{T}}(o,g))]\\tag{1}\\mmathbb{E}_{o,g}[\\log(1-D_{\\mathcal{T}}(o,g))]\n' +
      '\n' +
      '또한 [26]과 같이 생성된 스케치가 지상진실 스케치와 일치하도록 유도하기 위해 \\(\\mathcal{L}_{1}\\) 손실을 추가한다. 주어진 이미지에 대해 다수의 유효한 스케치가 존재할 수 있다는 사실을 설명하기 위해, Li 등 [26]과 같이 주어진 이미지에 대해 제공된 모든 \\(L^{(i)}\\) 스케치에 걸쳐 발생하는 최소 \\(\\mathcal{L}_{1}\\) 손실만을 벌한다. 이는 사실적 스케치의 양식적 차이로 인해 하나의 예제와 잘 일치하지만 다른 예제와는 일치하지 않는 타당한 스케치를 생성하기 위해 \\(\\mathcal{T}\\)의 잘못된 벌점을 방지하기 위한 것이다. 최종 목표는 평균 cGAN 손실과 최소 정렬 손실의 \\(\\lambda\\) 가중 조합이다:\n' +
      '\n' +
      '\\frac{\\lambda}{L^{(i)}\\sum_{k=1}^{L^{(i)}\\mathcal{L}_{\\text{cGAN}(o_{i},g_{i}^{(k)})+\\min_{k\\in\\{1,\\dots,L^{(i)}\\mathcal{L}_{1}(o_{i},g_{i}^{(k)}}}\\tag{2}\\tag{2}}\n' +
      '\n' +
      '실제로는 기존의 대규모 등고선도 데이터세트[26]를 활용하여 \\(\\mathcal{D_{\\mathcal{T}}}\\)에서 수작업으로 그린 스케치 500개를 보완한다. 이 데이터셋은 아마존메카니컬 투르크에서 수집한 이미지당 사물, 사람, 동물 등이 포함된 인터넷 스크래핑 이미지의 예를 \\(1000\\) 포함하는 \\(\\mathcal{D}_{\\text{CD}\\)이라고 한다. 이 데이터세트의 시각화는 부록 그림 5에 나와 있다. 먼저 미리 훈련된 이미지-스케치 변환 네트워크\\(\\mathcal{T}_{\\text{CD}}\\)[26]를 이미지당 \\(L^{(i)}=5\\) 스케치와 함께 \\(\\mathcal{D}_{\\text{CD}}\\)으로 훈련한다. 그런 다음, 로봇 관찰당 수동으로 그린 스케치(L^{(i)}=1\\)만으로 \\(\\mathcal{T}_{\\text{CD}\\)을 \\(\\mathcal{D}_{\\text{T}}\\)으로 미세조정하여 최종 영상-스케치 네트워크 \\(\\mathcal{T}\\)을 얻는다. 다른 로봇 관찰에 대해 \\(\\mathcal{T}\\)에 의해 생성된 스케치의 시각화는 그림 6에 나와 있다.\n' +
      '\n' +
      '### _RT-Sketch_\n' +
      '\n' +
      '이미지 관찰을 \\(\\mathcal{T}\\)을 통해 흑백 스케치로 변환하는 수단(섹션 III-A)을 사용하여 기존 RT-1 데이터 세트를 목표 스케치로 자동으로 추가할 수 있다. 이거.\n' +
      '\n' +
      '도. 2: 서로 다른 종류의 시각적 입력을 허용하는 RT-스케치의 아키텍처. RT-Sketch는 EfficientNet[41] 토큰화를 입력으로 하는 Transformer[43] 아키텍처를 채택하고, 버킷화된 액션을 출력한다.\n' +
      '\n' +
      '결과는 RT-Sketch 알고리즘을 훈련하기 위해 사용할 수 있는 \\(\\mathcal{D}_{\\mathrm{sketch}\\)이라고 하는 데이터 세트에서 나타난다.\n' +
      '\n' +
      'RT-스케치 데이터세트 원래의 RT-1 데이터세트 \\(\\mathcal{D}_{\\mathrm{lang}}=\\{i^{n},\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{(n)}}}_{n =1}^{N}\\)은 짝을 이루는 자연어 지시 \\(i\\)과 시연궤적 \\(\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{n}}\\)으로 구성된다. 우리는 언어 목표 [2] 대신에 목표 이미지로 그러한 데이터세트를 자동으로 뒤돌아 볼 수 있다. 궤적 \\(n\\)의 마지막 단계를 \\(T^{(n)}\\)으로 나타내자. 그리고 언어 목표 대신 이미지 목표를 갖는 새로운 데이터셋은 \\(\\mathcal{D}_{\\mathrm{img}}=\\{o_{T^{(n)}}^{n},\\{(o_{t}^{n},a_{t}^{n})\\}_{t=1}^{T^{(n)}}}_{n=1}^{N}\\이며, 여기서 궤적 \\(o_{T^{(n)}}^{n}\\)의 마지막 관측을 목표 \\(g^{n}\\)으로 처리한다. \\(\\pi_{\\mathrm{sketch}\\)에 대한 데이터 세트를 생성하기 위해, 우리는 \\(\\hat{g}^{n}=\\mathcal{T}(o_{T^{(n)}}^{n})\\)을 \\(\\hat{g}^{n}=\\mathcal{T}(o_{T^{(n)}}^{n})\\)으로 간단히 대체할 수 있다.\n' +
      '\n' +
      '다양한 수준의 입력 스케치 특이성을 제공하는 정책을 장려하기 위해 우리는 실제로 \\(\\hat{g}^{n}=\\mathcal{A}(o_{T^{(n)}}^{n})\\)에 의한 목표를 생산하며, 여기서 \\(\\mathcal{A}\\)은 무작위 확대 함수이다. \\\\at{g}^{n}=\\mathcal{A}(o_{T^{(n)}}^{n})\\. (\\mathcal{A}\\)는 단순히 \\(\\mathcal{T}\\), \\(\\mathcal{T}\\)을 적용하는 것, (예를 들어, 이진 스케치 위에 흐릿한 버전의 Ground truth RGB 이미지를 중첩하여), 에지 검출을 위한 Sobel 연산자[40]를 적용하는 것, 또는 원래의 Ground truth 목표 이미지를 보존하는 연산자를 적용하지 않는 것 중 하나를 선택한다(도 2). 모든 표현에 대해 공동 훈련함으로써, 우리는 RT-스케치가 이진 스케치, 컬러화된 스케치, 에지 검출 이미지 및 목표 이미지에서 이어지는 특이성의 스펙트럼을 처리하도록 의도한다(부록 도 6).\n' +
      '\n' +
      'RT-Sketch Model Architecture는 RT-1과 같이 언어 지시가 아닌 스케치로 제공되는 목표를 고려하며, 이러한 입력 표현의 변화는 모델 아키텍처의 변화를 필요로 한다. 원래 RT-1 정책은 트랜스포머 아키텍처 백본[43]에 의존합니다. RT-1은 먼저 토큰화된 이미지 임베딩을 생성하는 EfficientNet-B3 모델[41]을 통해 \\(D=6\\) 이미지의 히스토리를 전달하고, FiLM[31]과 Token Learner[38]를 통해 텍스트 임베딩과 토큰을 별도로 추출한다. 그런 다음 토큰이 버킷화된 작업을 출력하는 트랜스포머에 공급됩니다. 출력 액션 차원은 엔드 이펙터(x, y, z, 롤, 피치, 요, 그리퍼 폭)의 경우 7, 모바일 베이스의 경우 3, 베이스 모션, 팔 모션 및 에피소드 종료 중 선택할 수 있는 플래그의 경우 1이다. RT-1 아키텍처를 재교육하지만 입력 표현의 변화를 수용하기 위해 FiLM 언어 토큰화를 완전히 생략했다. 대신, 주어진 목표 이미지 또는 스케치를 EfficientNet에 입력으로 이미지의 히스토리와 연결하고 그 출력에서 토큰을 추출하여 나머지 정책 아키텍처를 변경하지 않는다. 우리는 그림 2의 RT-스케치 훈련 입력 및 정책 아키텍처를 시각화한다. 우리는 이 아키텍처를 RT-목표-이미지로 이미지(즉, 이미지 목표-조건 RT1 정책)에서만 훈련될 때 참조하고 이 섹션에서 논의된 바와 같이 스케치에서 훈련될 때 RT-스케치로 참조한다.\n' +
      '\n' +
      'Training RT-Sketch 이제 우리는 RT-1[9]을 훈련하는 데 사용된 것과 동일한 절차를 사용하여 \\(\\pi_{\\mathrm{sketch}\\)을 \\(\\pi_{\\mathrm{sketch}}\\)으로 훈련할 수 있다. 우리는 행동 복제 목적 함수를 사용하여 \\(\\pi_{\\mathrm{sketch}}\\)을 맞추었다. 이것은 관찰의 이력 및 주어진 스케치 목표[42]를 제공하는 동작의 음의 로그-우도를 최소화하는 것을 목표로 한다:\n' +
      '\n' +
      '\\[J(\\pi_{\\mathrm{sketch}})=\\sum_{n=1}^{N}\\sum_{t=1}^{T^{(n)}}\\log\\pi_{\\mathrm{ sketch}}(a_{t}^{n}|g^{n},\\{o_{j}\\}_{j=1}^{t})\\]\n' +
      '\n' +
      '## IV Experiments\n' +
      '\n' +
      '우리는 언어와 같은 상위 수준의 목표 추상화 또는 목표 이미지와 같은 더 과도하게 지정된 양식에서 작동하는 정책과 비교하여 목표 조건 조작을 수행하는 RT-스케치의 능력을 이해하려고 한다. 이를 위해 다음과 같은 네 가지 가설을 검정한다.\n' +
      '\n' +
      '**H1: RT-스케치는 목표 조건 IL에서 성공적이다.** 스케치는 실제 이미지의 추상화이지만, 우리의 가설은 그들이 정책에 조작 목표를 제공할 만큼 충분히 구체적이라는 것이다. 따라서, 간단한 조작 설정에서 RT-스케치가 언어 목표(RT-1) 또는 목표 이미지(RT-Goal-Image)와 유사한 수준에서 수행될 것으로 기대한다.\n' +
      '\n' +
      '**H2: RT-Sketch는 다양한 수준의 특수성을 다룰 수 있다.** 사람들이 있는 만큼 장면을 스케치하는 방법이 많다. 우리는 다양한 수준의 특수성의 스케치에 대한 RT-스케치를 훈련했기 때문에 동일한 장면에 대한 입력 스케치의 변화에 대해 견고할 것으로 기대한다.\n' +
      '\n' +
      '**H3: 스케치는 목표 이미지보다 산만자에게 더 나은 견고성을 가능하게 한다.** 스케치는 장면의 작업 관련 세부 사항에 초점을 맞춘다. 따라서, RT-Sketch는 상세한 이미지 목표에서 동작하는 RT-Goal-Image에 비해 스케치에 포함되지 않는 환경에서 산만자에 대한 강건성을 제공할 것으로 기대한다.\n' +
      '\n' +
      '**H4: 언어가 모호할 때 스케치가 유리하다.** RT-1을 사용할 때 모호한 언어 입력에 비해 RT-스케치가 더 높은 성공률을 제공할 것으로 기대한다.\n' +
      '\n' +
      '### _Experimental Setup_\n' +
      '\n' +
      '정책은 RT-스케치를 원래 언어 조건 에이전트 RT-1[9]과 비교하고, RT-스케치와 아키텍처가 동일하지만 스케치보다는 목표 이미지를 입력으로 하는 정책인 RT-Goal-Image를 비교한다. 모든 정책은 Brohan et al. [9]의 설정을 사용하여 VR 텔레오퍼레이션을 통해 수동으로 수집된 \\(\\sim 80\\)K 실세계 궤적의 다중 작업 데이터 세트에 대해 훈련된다. 이러한 궤적은 물건을 따고 배치하는 것, 컵과 병을 직립 또는 옆으로 재배치하는 것, 서랍을 열고 닫는 것, 서랍이나 조리대 사이에 물건을 재배치하는 것과 같은 일반적인 사무실 및 주방 작업 세트에 걸쳐 있다.\n' +
      '\n' +
      '평가 프로토콜은 공정한 비교를 보장하기 위해 인간 로봇 운영자를 위한 참조 역할을 하는 잘 정의된 평가 시나리오 카탈로그를 통해 다양한 정책 롤아웃에 걸쳐 환경의 동일한 초기 및 목표 상태를 제어한다. 각 시나리오에 대해 장면의 초기 이미지(RGB 관찰), 목표 이미지(원하는 대로 수동으로 객체를 재배열함), 목표를 달성하기 위해 원하는 에이전트 동작을 설명하는 자연어 작업 문자열 및 기록된 목표 이미지에 해당하는 손으로 그린 스케치 세트를 기록한다. 테스트 시간에, 인간 운영자는 카탈로그로부터 특정 평가 시나리오를 검색하고, 커스텀 시각화 유틸리티를 사용하여 기준 이미지에 따라 물리적 로봇 및 장면을 정렬하고, 관련 객체를 각자의 위치에 배치한다. 마지막으로 로봇은 시나리오에 대한 목표 표현(언어, 이미지, 스케치 등) 중 하나를 정책에 대한 입력으로 선택한다. 우리는 다운스트림 평가를 위한 정책 롤아웃의 비디오를 녹화한다(IV-B 절 참조). 모든 실험은 모바일 베이스, 오버헤드 카메라, 평행 턱 그리퍼가 있는 7DoF 조작기 암이 포함된 에브리데이 로봇을 사용하여 수행한다. 평가를 위한 모든 스케치는 디지털 스타일러스가 있는 태블릿의 단일 인간 주석자에 의한 맞춤형 수동 드로잉 인터페이스로 수집된다.\n' +
      '\n' +
      '목표 정렬을 위한 표준화된 자동화된 평가 프로토콜을 정의하는 성능 메트릭은 간단하지 않습니다. 이진 태스크 성공은 프레임 차분 또는 CLIP[34]와 같은 너무 거친 입도와 이미지 유사성 메트릭이 부서지기 쉬운 경향이 있기 때문에 두 가지 더 표적화된 메트릭으로 성능을 측정한다. 먼저, 수동 키포인트 주석을 사용하여 달성된 객체 중심과 지상 진리 목표 상태 사이의 거리(픽셀 내)로 정책 정밀도를 정량화한다. 객체 중심 검출을 위해 박스 외 객체 검출기를 활용하는 것이 가능하지만, 정책 자체의 조작 오류로부터 객체 검출의 오류(불정확 바운딩 박스, 잘못된 객체 등)를 혼동하는 것을 방지하고자 한다. 둘째, 일반적으로 사용되는 리커트[27] 등급 체계에 따라 1(강력하게 동의하지 않음)에서 7(강력하게 동의함)까지 인식된 목표 정렬에 대한 인간 제공 평가를 수집한다.\n' +
      '\n' +
      '****(Q1)**_로봇은 롤아웃 동안 주어진 목표와 **의미 정렬**을 달성한다._\n' +
      '****(Q2)**_로봇은 롤아웃 동안 주어진 목표와 **공간 정렬**을 달성한다._\n' +
      '\n' +
      '**Q1**의 경우 라벨러에게 주어진 지상 진실 언어 작업 설명과 함께 정책 롤아웃 비디오를 제공합니다. 우리는 간단한 조작 시나리오(**H1**)에 대해 모든 방법에 걸쳐 상당히 높은 평가를 기대한다. 스케치 조건 정책은 작업 문자열이 모호할 때 언어 조건 정책보다 높은 점수를 얻어야 합니다. **Q2**는 대신 정책이 원하는 대로 객체를 공간적으로 배치할 수 있는 정도를 측정하는 데 맞춰진다. 예를 들어, 정책은 명령 _place가 우측 배향으로 끝나는 한 upright_에 대한 의미 정렬을 달성할 수 있다. **Q2**의 경우, 인지된 공간 정렬을 평가하기 위해 주어진 시각적 목표(지상 진실 이미지, 스케치 등)로 정책 롤아웃을 나란히 시각화한다. 우리는 모든 정책이 단순 시나리오(**H1**)에 대해 높은 등급을 받아야 하며, 암시적으로 목표에 인코딩된 더 강한 공간 이전을 갖는 시각적 조건 정책에 대해 약간의 가장자리가 있다고 가정한다. 우리는 또한 장면의 시각적 복잡성이 증가함에 따라 스케치가 다양한 수준의 스케치 특이성(**H4**)에 대해서도 목표의 적절한 측면에 더 잘 참석하고 이미지 조절 에이전트(**H3**)보다 더 나은 공간 정렬을 달성할 수 있을 것으로 기대한다. 부록 그림에서 **Q1** 및 **Q2**에 대한 평가 인터페이스의 시각화를 제공한다. 17. 우리는 62명의 개인(비전문가, 우리 시스템에 익숙하지 않음)에 걸쳐 이러한 인체 평가 조사를 수행한다는 점에 주목하며, 여기서 6개의 상이한 각각을 평가하기 위해 8명에서 12명 사이를 할당한다.\n' +
      '\n' +
      '도. 3: **목표 정렬 결과:**지각된 의미 정렬(**Q1**) 및 공간 정렬(**Q2**)을 제공된 목표에 대한 상이한 정책 등급에 대한 평균 리커트 점수. 간단한 벤치마크 조작 작업의 경우, RT-스케치는 6개 기술 중 5개 기술(**H1**)에 대해 두 메트릭 측면에서 RT-1 및 RT-목표-이미지보다 비교 및 경우에 더 나은 성능을 수행한다. RT-스케치는 시각적 장면이 산만하거나(**H3**) 언어가 모호할 때 기준선보다 더 나은 목표 정렬을 달성하면서, 상이한 레벨의 디테일(**H2**)의 스케치를 처리하는 능력을 추가로 나타낸다(**H4**). 오차 막대는 레이블러 등급에 걸친 표준 오차를 나타냅니다.\n' +
      '\n' +
      '아래에 고려된 기술입니다.\n' +
      '\n' +
      '### _Experimental Results_\n' +
      '\n' +
      '이 섹션에서는 섹션 IV의 가설과 관련된 연구 결과를 제시한다. 표 I 및 II는 픽셀별 거리 측면에서 정책에 의해 달성된 공간 정밀도를 측정하는 반면, 그림 3은 7점 리커트 척도 등급을 기반으로 인간이 인식한 의미 및 공간 정렬의 결과를 보여준다.\n' +
      '\n' +
      '**H1**: RT-1 벤치마크 [9]로부터 6개의 기술을 평가한다: Y_ 근처의 X 이동, _place X upright_, _knock X over_, X drawer_ 열기, X drawer_ 닫기, Y_로부터의 _pick X. 각 스킬에 대해 15가지 다른 카탈로그 시나리오를 기록하며 개체(총 16개)와 개체 배치를 모두 변경합니다.\n' +
      '\n' +
      '일반적으로, 우리는 RT-스케치가 의미(**Q1**) 및 공간 정렬(**Q2**) 모두에 대해 RT-1 및 RT-Goal-Image와 비교 가능한 수준에서 수행한다는 것을 발견했으며, 거의 모든 기술에 대해 평균적으로 \'동의\'에서 \'강력한 동의\' 범위의 등급을 달성했다(그림 3(상단). 주목할 만한 예외는 _upright_이며, 여기서 RT-Sketch는 비록 어느 정도의 공간 정렬(**Q2**)이 있더라도 본질적으로 목표를 의미론적으로 달성하지 못한다(**Q1**). RT-Sketch와 RT-Goal-Image는 모두 캔이나 병을 적절하게 배치한 다음 다시 방향을 바꿀 필요 없이 종료하는 경향이 있다(부록 그림 9). 이 동작은 목표까지의 낮은 중심 거리(표 I(왼쪽)의 어두운 회색)를 초래한다. 반면에 RT-1은 캔과 병을 성공적으로 복구했지만 더 높은 오류를 희생시키면서(부록 그림 9, 표 I(왼쪽)의 밝은 색상). 실험에서는 또한 정책이 현재 장면을 주어진 목표와 일치시키려고 시도하는 과도한 재시도 행위_의 발생을 관찰하며, 이는 파지 및 배치와 같은 재시도 행위이다. 그러나, 이러한 낮은 레벨의 액션들을 높은 정밀도로 수행하는 것은 도전적이며, 따라서 과도한 재시도들은 실제로 테이블에서 오브젝트들을 노크하거나 태스크 진행을 취소하는 것으로 이어지는 장면을 방해할 수 있다. 표 I에서 우리는 모든 정책에서 이러한 행동을 관찰하는 롤아웃의 비율을 보고한다. 우리는 RT-Goal-Image가 픽셀 수준의 세부 사항에 과도하게 집중하고 주어진 목표를 정확하게 맞추기 위해 과도하게 시도하는 결과로 이 실패 모드에 가장 취약하다는 점에 주목한다. 한편, RT-Sketch와 RT-1은 스케치와 언어 모두 더 높은 수준의 목표 추상화를 제공하기 때문에 훨씬 덜 취약하다.\n' +
      '\n' +
      '**H2**: 다음으로 다양한 수준의 디테일의 입력 스케치(자유손, 에지 정렬 라인 스케치, 컬러화된 라인 스케치 및 소벨 에지 검출 이미지)를 상한으로 처리하는 RT-스케치의 능력을 평가한다. 자유손 스케치는 빈 캔버스 옆에 참조 이미지로 그려지는 반면, 선 스케치는 이미지 위에 겹쳐진 반투명 캔버스에 그려진다(부록 그림 16 참조). 에이전트의 현재 관찰이 일반적으로 사용 가능하고 선과 모서리를 스케치하는 데 유용한 가이드를 제공하기 때문에 이러한 UI가 편리하고 실용적이라는 것을 발견했다. _move near_ 및 _open drawer_ 스킬 각각 5번의 시도 동안 표II에서 모든 유형의 스케치가 합리적인 수준의 공간 정밀도를 생성한다는 것을 알 수 있다. 예상대로 소벨 에지는 오차가 가장 적게 발생하지만 원근사영을 반드시 보존할 필요는 없는 자유손 스케치와 훨씬 더 세밀한 선 스케치가 크게 뒤쳐지지 않는다. 이는 해당 Likert 등급(도 3(좌측, 하단))에도 반영된다. 자유손 스케치는 이미 인지된 공간 및 의미 정렬의 중간 등급(4\\(4\\))을 획득하지만, 라인 스케치는 에지 검출 목표 이미지를 제공하는 상한과 동등하게 거의 7로 현저한 성능 향상을 가져온다. 색채를 추가하는 것은 성능을 더 향상시키지 않지만, 행동의 흥미로운 질적 차이로 이어진다(부록 그림 10 참조).\n' +
      '\n' +
      '또한 RT-스케치가 다른 개인이 그린 스케치로 일반화하고 스타일 변형을 처리할 수 있는지 여부를 평가한다. 먼저 6명의 다른 주석자가 그린 30개의 스케치를 _move near_ 평가 시나리오에서 5개의 목표 이미지에 대한 라인 스케치(추적)를 사용하여 수집한다. 이러한 스케치를 입력으로 하여 RT-스케치에 의해 생성된 롤아웃을 얻는다. 리커트 등급을 통해 인지된 공간 정렬을 보고하는 22명의 인간 평가자 전체에서 RT-스케치가 다른 주석자가 그린 스케치에서 높은 공간 정렬을 달성한다는 것을 발견했다. 특히, 우리의 원래 스케치가 입력인 경우와 비교하여 다른 주석자가 그린 스케치 간의 성능 또는 정책 성능에서 유의미한 드롭오프가 없다(그림 4).\n' +
      '\n' +
      '**H3**: 다음으로, 우리는 RT-Sketch 및 RT-Goal-Image의 견고성을 시각적 산만체의 존재와 비교한다. 카탈로그에서 Y_시행 근처에 15 _move X를 다시 사용하되, 정렬 후 초기 시각 장면에 \\(5-9\\)의 산만 물체를 도입한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c||c c c}  & \\multicolumn{3}{c||}{**Spatial Precision (RMSE in px.)**} & \\multicolumn{2}{c}{**Failure Occurrence (Excessive Retrying)**} \\\\\n' +
      '**Skill** & RT-1 & RT-Sketch & RT-Goal-Image & RT-1 & RT-Sketch & RT-Goal-Image \\\\ \\hline Move Near & \\(5.43\\pm 2.15\\) & \\(\\mathbf{3.49\\pm 1.38}\\) & \\(3.89\\pm 1.16\\) & \\(\\mathbf{0.00}\\) & \\(0.06\\) & \\(0.33\\) \\\\ Pick Drawer & \\(5.69\\pm 2.90\\) & \\(4.77\\pm 2.78\\) & \\(\\mathbf{4.74\\pm 2.01}\\) & \\(\\mathbf{0.00}\\) & \\(0.13\\) & \\(0.20\\) \\\\ Drawer Open & \\(4.51\\pm 1.55\\) & \\(\\mathbf{3.34\\pm 1.08}\\) & \\(4.98\\pm 1.16\\) & \\(\\mathbf{0.00}\\) & \\(\\mathbf{0.00}\\) & \\(0.07\\) \\\\ Drawer Close & \\(\\mathbf{2.69\\pm 0.93}\\) & \\(3.02\\pm 1.35\\) & \\(3.71\\pm 1.67\\) & \\(\\mathbf{0.00}\\) & \\(\\mathbf{0.00}\\) & \\(0.07\\) \\\\ Knock & \\(7.39\\pm 1.77\\) & \\(\\mathbf{5.36\\pm 2.74}\\) & \\(5.63\\pm 2.60\\) & \\(\\mathbf{0.00}\\) & \\(0.13\\) & \\(0.40\\) \\\\ Upright & \\(7.84\\pm 2.37\\) & \\(5.08\\pm 2.08\\) & \\(\\mathbf{4.18\\pm 1.54}\\) & \\(0.06\\) & \\(\\mathbf{0.00}\\) & \\(0.27\\) \\\\ \\hline Visual Distractors & - & \\(\\mathbf{4.78\\pm 2.17}\\) & \\(7.95\\pm 2.86\\) & - & \\(\\mathbf{0.13}\\) & \\(0.67\\) \\\\ Language Ambiguity & \\(8.03\\pm 2.52\\) & \\(\\mathbf{4.45\\pm 1.54}\\) & - & \\(0.40\\) & \\(\\mathbf{0.13}\\) & - \\\\ \\end{tabular}\n' +
      '\\end{table} TABLE I: **Spatial Precision and Failure Occurrence** : Left: We report the level of spatial precision achieved across policies, measured in terms of RMSE of the centroids of manipulated objects in achieved vs. given reference goal images. Darker shading indicates higher precision (lower centroid distance). Fig.8 contains visualizations illustrating the degree of visual alignment that different RMSE values correspond to. Right: We report the proportion of rollouts in which different policies exhibit _excessive retrying_ behavior. Bolded numbers indicate the most precise and least failure-prone policy for each skill.\n' +
      '\n' +
      '이 테스트 절차는 _medium-high_ difficulty[9]로 지칭되는 RT-1 일반화 실험으로부터 적응된다. 표 I(왼쪽, 아래쪽)에서 RT-스케치가 평균적으로 훨씬 낮은 공간 오류를 나타내는 반면 RT-목표-이미지에 비해 더 높은 의미 및 공간 정렬 점수를 생성한다는 것을 알 수 있다(그림). 3(중간, 바닥)) RT-Goal-Image는 산만 물체에 의해 도입되는 분포 이동으로 쉽게 혼동되며, 종종 잘못된 물체를 집어 올리는 것과 내려놓는 것 사이에서 순환한다. 반면, RT-Sketch는 스케치에서 캡처되지 않은 작업 관련 객체를 무시하고 대부분의 경우 작업을 완료한다(부록 도 11 참조).\n' +
      '\n' +
      '**H4**: 마지막으로 언어 목표만으로는 모호할 때 표현으로서의 스케치가 유리한지 평가한다. 언어 명령어에서 3가지 유형의 모호성을 포괄하는 15가지 시나리오를 수집한다: 인스턴스 모호성(**T1**)(예를 들어, 다수의 오렌지 인스턴스가 존재할 때 오렌지 근처로 _move apple near orange_), 다소 유통 외(OOD) 언어(**T2**)(예를 들어, 오렌지 근처로 _move left apple near orange_), 및 고도로 OOD 언어(**T3**)(예를 들어, 무지개__complete the rainbow_)(부록 도 12 참조). 후자의 두 자격은 모호성을 직관적으로 해결하는 데 도움이 되어야 하지만 원래 RT-1 훈련[9]의 일부로 명시적으로 만들어지지 않았으므로 제한된 유용성만 제공한다. 표 I(왼쪽, 아래쪽)에서 RT-스케치는 RT-1의 오차의 거의 절반을 달성하고 의미 및 공간 정렬에 대해 각각 \\(2.39\\)배 및 \\(2.79\\)배 점수를 증가시킨다. 3(우측, 하측) **T1** 및 **T2** 시나리오에 대해, RT-1은 종종 태스크 스트링에서 언급된 임의의 객체의 인스턴스를 픽업하려고 시도하지만, 그 이상의 진행을 하지 못한다(부록 도 13). 이것은 언어가 배포 내 어휘만으로 불투명하거나 표현하기 어려울 수 있는 경우 최소한의 오버헤드로 새로운 보이지 않는 목표를 표현하는 스케치의 유용성을 추가로 시사한다(부록 도 14).\n' +
      '\n' +
      '한계 및 장애 모드 첫째, 이 작업에 사용된 이미지-스케치 생성 네트워크는 단일 인간 주석자가 제공하는 스케치 데이터 세트에서 미세 조정된다. 그럼에도 불구하고 RT-스케치가 다른 주석자에 의해 그려진 스케치를 처리할 수 있음을 경험적으로 보여주었지만, 우리는 아직 다른 사람들이 제작한 스케치로 RT-스케치를 규모별로 훈련한 효과를 조사하지 못했다. 둘째, RT-Sketch는 훈련된 특정 기술을 수행하는 데 내재된 일부 편향을 나타내며 때때로 잘못된 기술을 수행한다는 점에 주목한다. RT-스케치의 제한 및 고장 모드에 대한 자세한 내용은 부록 A를 참조하십시오.\n' +
      '\n' +
      '## V Conclusion\n' +
      '\n' +
      '본 논문에서는 원하는 장면을 손으로 그린 스케치를 입력으로 하여 동작을 출력하는 조작의 목표 조건 정책인 RT-Sketch를 제안한다. 이러한 정책을 구현하기 위해 먼저 이미지-투-스케치 변환 네트워크를 통해 쌍을 이루는 스케치-궤적 학습 데이터를 생성하는 확장 가능한 방법을 개발하고 기존의 RT-1 아키텍처를 수정하여 시각적 정보를 입력으로 한다. 경험적으로 RT-Sketch는 다양한 조작 기술에 대해 기존의 언어 또는 목표 이미지 컨디셔닝 정책과 비교할 수 있는 수준에서 수행할 뿐만 아니라 다양한 수준의 스케치 충실도에 적합하고 시각적 산만자 또는 모호성에 더 강력하다는 것을 보여준다. 향후 작업은 조립 작업에 대한 도식이나 다이어그램과 같이 손으로 그린 스케치를 보다 구조화된 표현으로 확장하는 데 중점을 둘 것이다. 강력하지만 스케치는 자체적인 제한, 즉 생략된 세부사항으로 인한 모호성 또는 품질이 좋지 않은 스케치가 없는 것은 아닙니다. 미래에는 언어, 스케치 및 기타 모달리티의 이점을 활용하여 단일 모달리티만으로 모호성을 공동으로 해결할 수 있는 멀티모달 목표 사양에 대한 방안이 기대됩니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 166-175. PMLR, 06-11 Aug 2017. URL [https://proceedings.mlr.press/v70/andreas17a.html](https://proceedings.mlr.press/v70/andreas17a.html).\n' +
      '* [2] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In _31st Conference on Neural Information Processing Systems (NIPS 2017)_, 2017.\n' +
      '* [3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. _Robotics and autonomous systems_, 57 (5):469-483, 2009.\n' +
      '\n' +
      '도. 4: **Perceived Spatial Alignment for Sketches Drawn by Other Annotator(H2):** RT-Sketch를 위한 훈련 데이터세트에 표현되지 않은 6명의 주석이 그린 라인 스케치 전반에 걸쳐, 우리는 _move near_ 스킬을 위한 입력으로 이러한 스케치를 사용하여 정책 롤아웃을 기록한다. 우리는 달성된 목표 상태와 주어진 스케치 사이의 공간 정렬을 측정하는 리커트 등급을 제공하는 22명의 인간 평가자에 걸쳐 결과 롤아웃을 평가한다. 이 새로운 입력 스케치에 대한 RT-스케치의 성능은 원래 스케치(OURS)의 정책 성능과 동등하며 다른 주석자가 그린 아티팩트 간에 유의미한 드롭오프가 없습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c}  & \\multicol* [4] Christine M Barber, Robin J Shucksmith, Bruce MacDonald, and Burkhard C Wunsche. Sketch-based robot programming. In _2010 25th International Conference of Image and Vision Computing New Zealand_, pages 1-8. IEEE, 2010.\n' +
      '* [5] Suneel Belkhale, Yuchen Cui, and Dorsa Sadigh. Data quality in imitation learning. _arXiv preprint arXiv:2306.02437_, 2023.\n' +
      '* [6] Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Subhadeep Koley, Rohit Kundu, Aneeshan Sain, Tao Xiang, and Yi-Zhe Song. Doodle it yourself: Class incremental learning by drawing a few sketches. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2293-2302, 2022.\n' +
      '* [7] Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Kumar, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Sketch2saliency: Learning to detect salient objects from human drawings. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2733-2743, 2023.\n' +
      '* [8] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In _arXiv preprint arXiv:2307.15818_, 2023.\n' +
      '* [9] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S Ryoo, Grecia Salazar, Pannag R Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan H Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics Transformer for Real-World Control at Scale. In _Proceedings of Robotics: Science and Systems_, Daegu, Republic of Korea, July 2023, doi: 10.15607/RSS.2023.XIX.025.\n' +
      '* [10] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.\n' +
      '* [11] Kaylee Burns, Tianhe Yu, Chelsea Finn, and Karol Hausman. Robust manipulation with spatial features. In _CoRL 2022 Workshop on Pre-training Robot Learning_, 2022.\n' +
      '* [12] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. What can human sketches do for object detection? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15083-15094, 2023.\n' +
      '* [13] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. Scenetrilogy: On human scene-sketch and its complementarity with photo and text. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10972-10983, 2023.\n' +
      '* [14] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran. Can foundation models perform zero-shot task specification for robot manipulation? In _Learning for Dynamics and Control Conference_, pages 893-905. PMLR, 2022.\n' +
      '* [15] Yuchen Cui, Siddharth Karamcheti, Raj Palleti, Nidhya Shivakumar, Percy Liang, and Dorsa Sadigh. No, to the right: Online language corrections for robotic manipulation via shared autonomy. In _Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction_, pages 93-101, 2023.\n' +
      '* [16] Michael Danielczuk, Andrey Kurenkov, Ashwin Balakrishna, Matthew Matl, David Wang, Roberto Martin-Martin, Animesh Garg, Silvio Savarese, and Ken Goldberg. Mechanical search: Multi-step retrieval of a target object occluded by clutter. In _2019 International Conference on Robotics and Automation (ICRA)_, pages 1614-1621. IEEE, 2019.\n' +
      '* [17] Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation learning. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [18] Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan, Ashwin Balakrishna, Daniel Seita, Jennifer Grannen, Minho Hwang, Ryan Hoque, Joseph E Gonzalez, Nawid Jamali, et al. Learning dense visual correspondences in simulation to smooth and fold real fabrics. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 11515-11522. IEEE, 2021.\n' +
      '* [19] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, et al. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. _arXiv preprint arXiv:2311.01977_, 2023.\n' +
      '\n' +
      '* [20] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1125-1134, 2017.\n' +
      '* [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In _Conference on Robot Learning_, pages 991-1002. PMLR, 2022.\n' +
      '* [22] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. _arXiv preprint arXiv:2210.03094_, 2022.\n' +
      '* [23] Siddharth Karamcheti, Suraj Nair, Annie S Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-driven representation learning for robotics. _arXiv preprint arXiv:2302.12766_, 2023.\n' +
      '* [24] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Picture that sketch: Photorealistic image generation from abstract sketches. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6850-6861, 2023.\n' +
      '* [25] Zixing Lei, Yiming Zhang, Yuxin Xiong, and Siheng Chen. Emergent communication in interactive sketch question answering. _arXiv preprint arXiv:2310.15597_, 2023.\n' +
      '* [26] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and Deva Ramanan. Photo-sketching: Inferring contour drawings from images. In _2019 IEEE Winter Conference on Applications of Computer Vision (WACV)_, pages 1403-1412. IEEE, 2019.\n' +
      '* [27] Rensis Likert. A technique for the measurement of attitudes. _Archives of Psychology_, 1932.\n' +
      '* [28] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. _arXiv preprint arXiv:2005.07648_, 2020.\n' +
      '* [29] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. _IEEE Robotics and Automation Letters_, 2023.\n' +
      '* [30] Lucas Manuelli, Wei Gao, Peter Florence, and Russ Tedrake. kpam: Keypoint affordances for category-level robotic manipulation. In _The International Symposium of Robotics Research_, pages 132-157. Springer, 2019.\n' +
      '* [31] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.\n' +
      '* [32] David Porfirio, Laura Stegner, Maya Cakmak, Allison Sauppe, Aws Albarghouthi, and Bilge Mutlu. Sketching robot programs on the fly. In _Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction_, HRI \'23, page 584-593, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450399647. doi: 10.1145/3568162.3576991. URL [https://doi.org/10.1145/3568162.3576991](https://doi.org/10.1145/3568162.3576991).\n' +
      '* [33] Shuwen Qiu, Sirui Xie, Lifeng Fan, Tao Gao, Jungseock Joo, Song-Chun Zhu, and Yixin Zhu. Emergent graphical conventions in a visual communication game. _Advances in Neural Information Processing Systems_, 35:13119-13131, 2022.\n' +
      '* [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [35] Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation learning using score-based diffusion policies. _Robotics: Science and Systems (RSS)_, 2023.\n' +
      '* [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10684-10695, June 2022.\n' +
      '* [37] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.\n' +
      '* [38] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. _Advances in Neural Information Processing Systems_, 34:12786-12797, 2021.\n' +
      '* [39] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. In _Proceedings of Robotics: Science and Systems (RSS)_, 2020.\n' +
      '* [40] Irwin Sobel. An isotropic 3x3 image gradient operator. _Presentation at Stanford A.I. Project 1968_, 1968.\n' +
      '* [41] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.\n' +
      '* [42] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. _arXiv preprint arXiv:1805.01954_, 2018.\n' +
      '* [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [44] Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel Shamir. Clipascene: Scene sketching with differ ent types and levels of abstraction. _arXiv preprint arXiv:2211.17256_, 2022.\n' +
      '* [45] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching. _ACM Transactions on Graphics (TOG)_, 41(4):1-11, 2022.\n' +
      '* [46] Kevin Zakka, Andy Zeng, Johnny Lee, and Shuran Song. Form2fit: Learning shape priors for generalizable assembly from disassembly. In _2020 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9404-9410. IEEE, 2020.\n' +
      '* [47] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.\n' +
      '\n' +
      '## 부록, 부록\n' +
      '\n' +
      '이 섹션에서는 RT-스케치가 기차 및 테스트 시간에서 보는 시각적 목표 표현(부록 A), 실험적 롤아웃의 정성적 시각화(부록 A), RT-스케치의 제한(부록 A) 및 데이터 주석, 평가 및 인간 평가에 사용되는 인터페이스(부록 A)에 대한 추가 세부 정보를 제공한다.\n' +
      '\n' +
      'RT-Sketch와 같은 스케치-대-액션 정책을 훈련하기 위한 주요 병목점은 페어링된 궤적과 목표 스케치의 데이터 세트를 수집하기 때문에 먼저 섹션 III에서 논의된 스케치 표현 \\(g_{i}\\)에 이미지-대-스케치 변환 네트워크 \\(\\mathcal{T}\\) 매핑 이미지 관찰 \\(o_{i}\\)을 매핑하는 이미지-대-스케치 변환 네트워크를 훈련한다. \\(\\mathcal{T}\\)을 학습하기 위해 먼저 쌍을 이루는 이미지와 에지 정렬 스케치의 ContourDrawing 데이터셋에서 학습된 스케치 대 이미지 번역[26]을 위해 미리 학습된 네트워크를 취한다(도 5). 이 데이터세트에는 \\(1000\\) 이미지에 대한 이미지당 \\(L^{(i)}=5\\) 크라우드소싱 스케치가 포함되어 있다. 이 데이터셋에 대한 사전 학습을 통해 \\(\\mathcal{T}\\)에 강력한 선행을 임베딩하고 훨씬 더 작은 데이터셋에 대한 학습을 가속화할 수 있기를 기대한다. 다음으로, RT-1 로봇 영상을 위한 500개의 수작업으로 그려진 선 스케치의 데이터셋에 대해 \\(\\mathcal{T}\\)을 세절한다. 그림 1에서 수동으로 스케치한 목표의 몇 가지 예를 시각화한다. 6 under \'Line Drawings\'\n' +
      '\n' +
      '특히, 흑백 선 스케치에 이미지를 매핑하기 위해 \\(\\mathcal{T}\\)만 훈련하는 반면, 생성된 목표 위에 다양한 증강 \\(\\mathcal{A}\\)을 고려하여 다양한 색상, 아핀 및 원근 왜곡, 상세 수준을 시뮬레이션한다. 도. 도 6은 원본 RGB 이미지의 블러링된 버전을 중첩하여 흑백 스케치를 자동으로 컬러화하고, 에지 검출된 원본 이미지의 버전을 생성된 스케치로 처리하여 많은 세부 사항을 갖는 스케치를 시뮬레이션하는 것과 같은 이러한 증강들 중 일부를 시각화한다. 우리는 \\(\\mathcal{T}\\)와 \\(\\mathcal{A}\\)을 통해 \'후시 재레이블된 목표 이미지를 스케치하여 RT-스케치를 훈련하기 위한 데이터 세트를 생성한다.\n' +
      '\n' +
      'RT-스케치는 생성된 라인 스케치, 컬러화된 라인 스케치, 에지 검출 이미지 및 목표 이미지에 대해서만 학습되지만, 훨씬 더 큰 다양성의 스케치를 처리할 수 있음을 발견했다. 여기에는 그림 6과 같이 비에지 정렬 자유 손 스케치 및 색상 채우기가 있는 스케치가 포함된다.\n' +
      '\n' +
      '### _Alternate Image-to-Sketch Techniques__\n' +
      '\n' +
      '우리가 사용하는 이미지-투-스케치 기법의 선택은 RT-스케치 파이프라인의 전반적인 성공에 중요하다. 우리는 위의 접근법에 수렴하기 전에 다양한 다른 기술로 실험한다.\n' +
      '\n' +
      '최근 CLIPAsso[45]와 CLI-PASCene[44]는 영상에서 스케치를 자동으로 생성하는 방법을 탐색한다. 이러한 작업은 주어진 입력 영상에 대해 CLIP-유사도가 최대인 생성된 스케치를 생성하기 위해 "스트로크"를 나타내는 베지어 곡선의 매개변수를 추론하는 것으로 스케치 생성을 포즈한다. 이러한 방법은 많은 이미지에 걸쳐 전역 배치 작업이 아닌 그럴듯한 스케치를 생성하기 위해 이미지당 최적화를 수행하여 확장성을 제한한다. 또한, 그들은 근본적으로 많은 외부 세부 사항을 캡처하는 고품질, 미학적으로 만족스러운 스케치를 생산하는 데 더 관심이 있습니다.\n' +
      '\n' +
      '반면에 우리는 최소이지만 합리적인 품질의 스케치를 만드는 데 신경을 씁니다. 우리가 탐구하는 두 번째 기술은 페어링된 이미지와 스케치의 인터넷 데이터에 대해 사전 훈련된 포토스케치 GAN[26]을 시도하는 것이다. 그러나 이 모델 출력은 로봇 관찰에 대해 훈련되지 않았기 때문에 객체 세부 정보를 잘 캡처하지 못하고 관련 없는 스케치 세부 정보를 포함한다. 마지막으로, 이 포토스케치 GAN을 자체 데이터에서 미세 조정함으로써 출력은 가능한 한 최소한으로 두드러진 객체 세부 사항을 캡처하는 실제 손으로 그린 인간 스케치에 훨씬 더 가깝다. 그림 7에서 이러한 차이를 시각화한다.\n' +
      '\n' +
      'RT-Sketch의 성능을 더 해석하기 위해 정밀 메트릭의 시각화와 실험적 롤아웃을 제공한다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 8에서, 우리는 달성 대 달성에서 객체 중심부의 픽셀별 거리로 정량화된 바와 같이 정렬 RT-스케치 달성 정도를 시각화한다. 목표 이미지가 주어집니다. 인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 9를 참조하면, 도 10을 참조하면, 도 11 및 도 1을 참조하여 설명한다. 도 13에서는 각각 **H1, H2, H3** 및 **H4**에 대한 각 정책의 행동을 시각화한다. 도. 도 12는 **H4**에 대해 분석하는 언어 모호성의 난이도 4단계를 시각화한다.\n' +
      '\n' +
      'RT-스케치는 다양한 수준의 스케치 디테일을 처리할 수 있고 시각적 산만자에 강력하며 모호한 언어의 영향을 받지 않는 여러 조작 벤치마크 기술에서 수행되지만 실패와 제한이 없는 것은 아니다.\n' +
      '\n' +
      '인 것을 특징으로 하는 반도체 소자의 제조 방법. 도 15에서는 RT-Sketch의 고장모드를 시각화하였다. RT-Sketch에서 볼 수 있는 한 가지 실패 모드는 장면을 최대한 가깝게 정렬하려고 시도한 결과 때때로 과도하게 재시도하는 것입니다. 예를 들어, 상단 행인 롤아웃 이미지 3에서 장면은 이미 잘 정렬되어 있지만 RT-스케치는 칩 백 방향 측면에서 일부 정렬 불량을 유발하는 칩 백을 계속 이동시킨다. 여전히 이러한 종류의 실패는 RT-Goal-Image(표 I)에서 가장 흔하며, RT-Sketch에서는 거의 빈번하지 않다. 우리는 스케치가 픽셀 수준의 세부 사항에 과도하게 주의를 기울이지 않고 높은 수준의 공간 추론을 가능하게 한다는 사실 때문일 수 있다고 가정한다.\n' +
      '\n' +
      '도. 5: **ContourDrawing Dataset**: [26]의 ContourDrawing Dataset에서 6개의 샘플을 시각화한다. 각 이미지에 대해 5개의 개별 주석이 원본 이미지 상단에 윤곽을 지정하여 장면의 모서리 정렬 스케치를 제공합니다. 묘사된 바와 같이, 주석자는 장면의 주요 윤곽을 보존하도록 장려되지만, 배경 상세 또는 세립 기하학적 상세는 종종 생략된다. Li 등[26]은 이어서 주어진 참조 스케치들(Eq) 중 적어도 하나와 정렬하는 것을 장려하는 손실을 갖는 이미지-투-스케치 변환 네트워크 \\(\\mathcal{T}\\)를 트레이닝한다. (2)).\n' +
      '\n' +
      '그러나 그러한 높은 수준에서 공간 추론의 한 가지 결과는 때때로 정밀도의 부족이다. 이것은 RT-스케치가 항목을 잘못 정렬하거나(두 번째 행) 약간 벗어났을 때 두드러지며, 이는 장면의 다른 항목(세 번째 행)을 방해할 수 있다. 이는 스케치가 본질적으로 불완전하기 때문에 이러한 높은 정밀도로 추론하기가 어렵기 때문일 수 있다.\n' +
      '\n' +
      '마지막으로, RT-스케치가 때때로 잘못된 객체(행 4 및 5)를 조작한다는 것을 알 수 있다. 흥미롭게도, 우리는 상당히 빈번한 행동 패턴이 잘못된 객체(4행 주황색)를 올바른 목표 위치(4행 녹색 캔 근처)로 조작하는 것임을 안다. 이는 스케치 생성 GAN이 때때로 실제 객체에서 누락된 환각 인공물 또는 기하학적 세부 사항을 가지고 있기 때문일 수 있다. 이와 같은 몇 가지 예에 대해 교육을 받은 RT-스케치는 잘못된 객체가 스케치에서 그려진 객체와 정렬되는 것을 잘못 인식할 수 있다. 그러나 스케치는 여전히 장면 내의 객체들의 상대적인 원하는 공간 포지셔닝을 나타내므로, 이 경우 RT-Sketch는 여전히 부정확한 객체를 적절한 장소에 정렬하려고 시도한다.\n' +
      '\n' +
      '마지막으로, 가장 빈번한 고장 모드는 잘못된 물체를 잘못된 목표 위치로 조작하는 것이다(즉, 잘못된 서랍 핸들을 여는 것). 이는 입력이 자유 손 스케치일 때 가장 빈번하며, 스케치 세부 정보를 증가시켜 생략할 수 있다(표 II).\n' +
      '\n' +
      '도. 6: **Visual Goal Diversity**: RT-Sketch는 열차 및 테스트 시간 모두에서 다양한 시각적 목표를 처리할 수 있다. RT-스케치는 \'생성 목표\' 아래 오른쪽에 표시된 것과 같은 생성 및 증강된 이미지에 대해 훈련되지만, \'수동 스케치 목표\' 아래 왼쪽에 표시된 것과 같은 테스트 시간에 자유손, 선 스케치 및 유색 스케치를 해석할 수도 있다.\n' +
      '\n' +
      '도. 7 : **대체 이미지-투-스케치 기법**\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '도. 8: **공간 정밀 시각화**: RMSE 측면에서 측정된 공간 정밀도와 함께 Move Near 스킬에 대한 RT-Sketch의 네 가지 시도를 시각화한다. 공간 정밀도를 평가하기 위해 인간의 주석이 시각적으로 가장 정렬된 프레임에 주석을 달고 이 프레임과 제공된 기준 목표 이미지에서 이동된 객체에 대한 키포인트를 가지고 있다. 4개의 시도 각각에 대해 레이블이 지정된 객체 중심 및 달성 대 간격띄우기와 함께 정렬이 달성될 때까지 롤아웃 프레임을 시각화한다. 원하는 자리 오른쪽 상단 예는 클립 백 대신 사과를 이동하여 높은 RMSE를 발생하는 RT-스케치의 실패를 보여준다. 이러한 시각화는 표 I의 숫자를 더 잘 맥락화하기 위한 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '도. 10: **H2 롤아웃 시각화**: _open drawer_ 스킬을 위해 서로 다른 입력 유형에서 작동하는 RT-Sketch의 4개의 개별 롤아웃을 시각화합니다. 자유손 스케치는 원근법 차이, 부분적으로 가려진 객체(서랍 손잡이) 및 대략적으로 그려진 객체 윤곽을 포함할 수 있도록 원래 이미지 위에 윤곽을 그리지 않고 그려진다. 선 스케치는 부록 그림에 제시된 스케치 인터페이스를 사용하여 원본 이미지 위에 그려진다. 16. 컬러 스케치는 단순히 이전 모달리티에 컬러 필트를 추가하고, 소벨 에지는 비현실적인 스케치 디테일의 관점에서 상한을 나타낸다. 우리는 RT-Sketch가 눈에 띄는 성능 증가나 감소 없이 자유손 스케치를 제외한 모든 스케치 입력에 대해 올바른 드로어를 성공적으로 열 수 있음을 알 수 있다. 자유손 스케치의 경우 RT-스케치는 여전히 드로어를 열 필요가 있음을 인식하지만 스케치 원근법과 축척의 차이로 인해 표시된 대로 정책이 잘못된 드로어를 처리할 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "A new approach to image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [2] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [3] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [4] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [5] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [6] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [7] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [8] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [9] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [10] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [11] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [12] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [13] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [14] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [15] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [16] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [17] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [18] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [19] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [20] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [21] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [22] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [23] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [24] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [25] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [26] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [27] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [28] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [29] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [30] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [31] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [32] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [33] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [34] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [35] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [36] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [37] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [38] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [39] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [40] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [41] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [42] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [43] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [44] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [45] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1-10.\n' +
      '* [46] S. A. Abadi, M. A. Abadi, and M. A. Abadi, "Image segmentation in multi-modal image segmentation," in _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2011, pp. 1\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:19]\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n' +
      '## 부록 실험 결과\n' +
      '\n' +
      '도. 16: **스케치 UI: 짝을 이루는 로봇 이미지 및 스케치를 수작업으로 수집하고, 학습시킬 수 있는 \\(\\mathcal{T}\\)의 스케치와 평가를 위한 목표 스케치를 위한 맞춤형 스케치 인터페이스를 설계한다. 인터페이스는 현재 로봇 관찰을 시각화하고 스타일러스를 사용하여 디지털 화면에 그리는 기능을 제공한다. 인터페이스는 다양한 색상과 지우기를 지원합니다. 예를 들어 현재 에이전트 관찰이 목표 이미지보다 훨씬 더 쉽게 사용할 수 있기 때문에 직관적으로 이미지 위에 그리는 것은 무리한 가정이 아니라는 점에 주목한다. 추가적으로, 오버레이는 서랍들 또는 핸들들에 대해 맹목적으로 에지들을 볼 필요 없이, 사용자가 제공하기 쉬운 스케치 인터페이스를 만들기 위한 것이다.**\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:23]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>