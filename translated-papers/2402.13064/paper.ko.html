<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 스크래치로부터의 합성 데이터(Almost):\n' +
      '\n' +
      '언어 모델을 위한 일반화된 명령어 튜닝\n' +
      '\n' +
      '하오란 리, 칭시우 동, 정양 탕, 차오준 왕, 칭싱 장, 하오양 황원, 소한 황, 샤오롱 황, 제창 황, 동동 장, 유천 구, 신청순 왕, 시칭 천, 리동, 웨이루, 지팡 수이, 벤유 왕, 와이람, 후루 웨이\n' +
      '\n' +
      '[https://aka.ms/GeneralAI](https://aka.ms/GeneralAI)\n' +
      '\n' +
      '동등한 기여. X. Zhang(xingxing.zhang@microsoft.com), H. Huang, S. 황철 황진 황동장 왕승 천락 Dong and F. Wei는 Microsoft. H. Li and W와 함께 있다. Lu는 싱가포르 기술 디자인 대학 Q. Dong, X. 쳉과 지 수이는 북경대학, Z. Tang과 B. Wang은 중국 홍콩대학, 선전. C. Wang과 W. 람은 중국 홍콩대, Y. 구는 칭화대 소속이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:2]\n' +
      '\n' +
      '각 학문에 대한 포괄적인 과목 목록을 작성하고 LLM을 사용하여 각 과목에 맞는 강의 계획서를 다시 설계합니다. 강의 계획서의 모든 수업 세션에 자세히 설명된 세밀한 핵심 개념을 사용하여 먼저 이를 샘플링한 다음 인간의 지식과 기술의 전체 스펙트럼에 걸쳐 광범위한 범위로 다양한 지침을 생성할 수 있다. 위에서 설명한 과정은 각 학문의 교육자들이 학생 학습을 위한 일련의 교과를 제작하는 인간 교육 시스템을 반영한다. 그런 다음 교수자는 각 과목에 대한 강의 계획서를 개발하여 내용을 특정 수업 세션으로 세분화한다. 그런 다음 이러한 세션은 학생들이 이해하고 내면화해야 하는 핵심 개념으로 다시 나뉜다. 강의 계획서에 요약된 이러한 세부 핵심 개념을 기반으로 교재 및 연습이 후속적으로 생성되며, 이는 우리의 수업 조율 데이터이다.\n' +
      '\n' +
      'GLAN은 일반적이고 확장 가능하며 사용자 정의할 수 있습니다. GLAN은 일반적인 방법으로 작업 진단적이며 광범위한 영역을 포괄할 수 있다. GLAN은 확장 가능합니다. [23, 24]와 유사하게 GLAN은 LLM을 사용하여 지침을 생성하며, 이는 대규모로 지침을 생성할 수 있다. 또한 GLAN의 입력은 LLM 및 인간 검증을 촉발하여 생성되는 분류로 최소한의 인간 노력이 필요하다. GLAN을 사용하면 쉽게 사용자 정의할 수 있습니다. 새로운 분야나 스킬은 단순히 새로운 노드를 우리의 분류법에 통합함으로써 추가될 수 있다. 분류학의 각 노드는 독립적으로 확장될 수 있으며, 이는 전체 데이터 세트를 다시 생성하지 않고 새로 추가된 노드에만 방법을 적용하면 된다는 것을 의미한다. 대형 언어 모델(예를 들어, 미스트랄)에 대한 광범위한 실험은 GLAN이 이러한 과제의 과제별 훈련 데이터를 사용하지 않고 수학적 추론, 코딩, 학업 시험, 논리적 추론에서 후속 일반 수업에 이르기까지 다차원적으로 탁월하다는 것을 보여준다.\n' +
      '\n' +
      '##2 GLAN : Generalized Instruction-Tuned Language Models\n' +
      '\n' +
      'GLAN은 인간의 지식과 능력의 다양한 영역을 포괄하는 합성 수업 데이터를 대규모로 생성하는 것을 목표로 한다. 알고리즘 1에 도시된 바와 같이, 먼저 프론티어 LLMs(즉, GPT-4) 및 인간 검증을 사용하여 인간 지식 및 능력의 분류를 구축한다. 분류법은 자연스럽게 인간의 지식과 능력을 _fields_, _sub-fields_ 및 궁극적으로 다른 _discilines_로 분해한다(2.1절 참조). 다음 단계는 GPT-4(또는 GPT-3.5)에 의해 완전히 자율적으로 촉진된다. 그런 다음 각 분야에 대해 GPT-4에 이 분야 내의 주제 목록으로 추가 분해하도록 다시 지시한다(섹션 2.2). GPT-4는 교수자와 유사하게 교과별 강의계획서를 지속적으로 설계하는데, 이는 본질적으로 학생들이 숙달해야 할 핵심 개념들로 다양한 수업 세션의 교과목을 단절시킨다(2.3절). 획득한 수업 세션과 핵심 개념을 통해 합성 명령어를 구성할 준비가 되었습니다. 우리는 GPT-4가 강의 계획서뿐만 아니라 무작위로 샘플링된 수업 세션과 주요 개념을 기반으로 숙제 질문을 생성하도록 촉구한다(2.4절). 우리는 원자 수준의 구성 요소(클래스 세션 및 핵심 개념)까지 인간의 지식과 능력을 더 작은 단위로 재귀적으로 분해한다. 우리는 이러한 수업 세션과 핵심 개념을 무작위로 결합하여 합성 지시의 적용 범위와 다양성을 보장할 것으로 기대한다.\n' +
      '\n' +
      '```\n' +
      'd\\in\\mathbbb\\texttt{build\\taxonomy()\\\\triangleright\\)do\\(2.1)\\(\\mathbb\\texttt{generate\\_subjects}(d)\\(2.3)\\(\\mathbb\\texttt{generate\\_syllabus}(s,d)\\)\\(2.4)\\(\\mathbb\\texttt{generate\\\\cup\\mathbb\\texttt{generate\\cup\\mathbb\\texttt{extract\\\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.4)\\(2.\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** GLAN 명령어 생성\n' +
      '\n' +
      '### 인간의 지식과 능력에 대한 분류\n' +
      '\n' +
      '우리는 합성 지시의 생성을 안내하기 위해 인간의 지식과 능력의 분류법을 구축한다. 따라서 커버리지가 중요합니다. 한편, LLM의 선호 기능이 시간이 지남에 따라 변경될 수 있기 때문에 분류법을 고도로 확장 가능하게 만드는 것도 필수적이다. 첫 번째 단계에서는 GPT-4를 서로 다른 명령어(예: 인간 지식과 능력의 모든 분야를 나열)로 프롬프트하여 분류법을 생성할 것을 제안한다. 그런 다음, 정확성과 완전성을 보장하기 위해 인간 사후 편집을 수행합니다. 우리 분류학의 분야, 하위 분야, 학문의 수가 제한되어 있어 인적 검증에 소요되는 비용이 상대적으로 낮다. 인간 사후 편집의 또 다른 장점은 필요에 따라 분류학에 새로운 분야나 학문을 쉽게 추가할 수 있다는 것이다.\n' +
      '\n' +
      '우리의 분류법은 현재 학문 교육과 직업 훈련 모두에서 다양한 지식과 역량을 다루고 있다. 분류법의 최상위 레벨은 _Natural Sciences_, _Humanities_ 또는 _Services_(직업 훈련)와 같은 _fields_를 포함한다. 이들 필드는 _Chemistry_, _Sociology_ 또는 _Retailing_와 같은 다양한 _sub-fields_ 및/또는 _disciplines_로 분기된다. 우리는 분류학의 노드를 _disciplines_까지 계속 분해하고, 분류의 분해는 다음 섹션에 설명된 자동 방법에 맡긴다. 분류학의 잎 마디들을 모아 \\(\\mathbb{D}=\\{d_{1},d_{2},\\ldots,d_{M}\\}\\)의 분류 목록을 얻는다.\n' +
      '\n' +
      '### Subject Generator\n' +
      '\n' +
      '알고리즘 1에서와 같이, 각 분야 \\(d\\)에 대해, 우리는 신속한 공학을 통해 그 안에 있는 과목들의 목록을 추출하는 것을 목표로 한다. 구체적으로, GPT-4는 학문의 교육 전문가 역할을 하고 학생이 배워야 할 과목 목록을 설계하도록 지시한다. GPT-4의 완료는 비구조화된 텍스트 형식의 주제 및 그 메타 데이터(예를 들어, 주제의 레벨, 소개 및 하위 주제)의 포괄적인 목록을 포함하며, 이는 후속 단계에서 직접 사용될 수 없다. 따라서 우리는 완료를 jsonl 형식으로 변환하기 위해 프롬프트의 또 다른 라운드를 사용했다.\n' +
      '\n' +
      '멋지다! 위의 내용을 jsonl 형식으로 변환하면 컴퓨터가 쉽게 이해할 수 있습니다. "\\(\\hookrightarrow\\)\\(\\char120\\)" "\\(\\char120\\)" 태그 사이의 jsonl 출력\n' +
      '\n' +
      '각 라인에 대해, "subject_name", "level" 및 "subtopics" 키를 사용한다.\n' +
      '\n' +
      '단일 프롬프트를 사용하여 jsonl 형식의 주제 목록을 생성하는 것이 가능하다는 점에 주목할 필요가 있다. 그러나 추가 형식 지정 지침을 프롬프트에 직접 통합하면 결과 제목 목록의 품질이 저하될 수 있으므로 이를 자제합니다. 추출된 피험자(메타 데이터뿐만 아니라)\\(\\mathbb{S}=\\{s_{1},s_{2},\\ldots,s_{N}\\}\\)은 다음 단계에서 후속적으로 사용될 수 있다. 각 \\(s\\in\\mathbb{S}\\)에 대해 s.name, s.level 및 s.subtopics는 각각 과목의 이름, 학년 및 하위 주제를 나타낸다. 우리는 위의 프롬프트를 여러 번 적용하여 이 분야 내에서 피험자의 더 나은 범위를 보장할 수 있다.\n' +
      '\n' +
      '### Syllabus Generator\n' +
      '\n' +
      '각 주제 \\(s\\)에 대해 이미 구조화된 형식으로 이름(s.name), 학년(s.level) 및 포함된 하위 주제(s.subtopics)의 작은 집합을 추출했다. 이 섹션에서는 각 과목을 더 작은 단위로 세분화하여 과제 작성에 더 적합하도록 하는 것을 목표로 한다. 우리는 이 과목에 대한 강의계획서를 설계하기 위해 GPT-4를 상담한다. 다음과 같은 이유로 강의계획서 작성을 선택합니다.\n' +
      '\n' +
      '* 강의계획서는 본질적으로 한 과목의 주요 주제를 계층적인 방식으로 더 작은 부분으로 분해한다. 구체적으로, 각 과목은 여러 개의 수업 세션으로 구성되며, 각 세션은 다양한 하위 주제 및 핵심 개념을 다룬다.\n' +
      '* 강의 계획서는 과목의 소개, 목표 및 예상 결과를 제공하며, 이는 본질적으로 숙제 질문을 공식화하는 데 유용하다.\n' +
      '\n' +
      '우리는 GPT-4에서 1) 메타 데이터(s.level, s.name, s.subtopics)를 기반으로 강의 계획서를 설계한다; 2) 주제를 다른 수업 세션으로 분리한다; 3) 각 수업 세션에 대한 세부 정보를 제공하고 학생들이 숙달해야 하는 세부 핵심 개념을 제공한다. [\\(\\mathcal{A}\\)는 생성된 강의계획서를 나타낸다.\n' +
      '\n' +
      '결과적인 교수요목 \\(\\mathcal{A}\\)은 비구조화된 텍스트 형식이다. 그러나, 명령어 생성 단계에서 각 클래스의 클래스 세션 이름 및 주요 개념이 필요하다(알고리즘 1 참조). 2.2절에서의 주제 목록 추출 과정과 유사하게, GPT-4를 프롬프트하여 각 클래스 세션의 메타 데이터를 다시 추출한다. 그 결과, 클래스 세션의 목록 \\(\\mathbb{C}=\\{c_{1},c_{2},\\ldots,c_{|\\mathbb{C}|}\\})과 그에 상응하는 핵심 개념 \\(\\mathbb{K}=\\{\\mathbf{k}_{1},\\mathbf{k}_{2},\\ldots,\\mathbf{k}{|\\mathbb{C}|}\\}\\})을 얻는다.\n' +
      '\n' +
      '### Instruction Generator\n' +
      '\n' +
      '강의계획서\\(\\mathcal{A}\\)와 수업시간\\(\\mathbb{C}\\) 및 그와 관련된 주요 개념\\(\\mathbb{K}\\)의 목록을 주어, 우리는 숙제 질문과 그에 대한 답을 생성할 준비가 되어 있다. 다양한 숙제 질문을 생성하기 위해, 우리는 먼저 이 선택된 수업 세션에서 \\(\\mathbb{C}\\)과 1~5개의 핵심 개념으로부터 하나 또는 두 개의 수업 세션 이름을 샘플링한다. [\\hat{\\mathbb{C}\\]은 선택된 수업 세션 이름을 나타내고 [\\hat{\\mathbb{K}\\]은 선택된 핵심 개념을 나타낸다. 그리고 선택된 수업시간\\(\\hat{\\mathbb{C}\\)과 주요 개념\\(\\hat{\\mathbb{K}\\)과 강의계획서\\(\\mathcal{A}\\)을 주어 GPT-4(또는 GPT-3.5)에게 숙제문제를 생성하도록 유도한다. 과제를 만들 때 GPT-4/3.5 컨텍스트(예: 이전 세션에서 학생들이 이미 학습한 내용)를 더 제공하고자 한다. 따라서 본 연구에서는 GPT에게 과제를 만들 때 수업시간까지 학습한 것을 고려하도록 지시하고, 다양한 수업시간에 걸쳐 여러 가지 핵심개념을 활용하도록 한다.\n' +
      '\n' +
      '표본 수업 세션과 핵심 개념 단일 강의 계획서에는 수많은 수업 세션과 핵심 개념이 있다. 그들로부터 샘플링할 두 가지 전략이 있습니다. 첫 번째 전략에서는 단일 클래스 세션에서 할당을 생성합니다. 따라서 클래스 세션 이름은 하나만 있습니다. 이 세션에서 우리가 총체적으로 \\(m\\)의 핵심 개념을 가지고 있다고 가정하자. 우리는 \\(m\\)의 핵심 개념에서 무작위로 1~5개의 핵심 개념을 추출하는데, 이것은 우리가 완전히 \\(\\sum_{i=1}^{5}\\binom{m}{i}\\)의 조합을 가지고 있다는 것을 의미한다. 이 전략에서는 기본 숙제 문제를 만드는 데 중점을 둡니다. 결과적인 질문을 더 도전적으로 만들기 위해(여러 수업 세션의 지식을 결합) 두 번째 전략에서 두 수업 세션의 핵심 개념을 결합하는 두 번째 전략을 제안한다. 우리는 두 개의 다른 수업 세션에서 지식을 활용하는 질문을 생성하려고 한다. 첫 번째와 두 번째 수업 시간에 각각 \\(m_{1}\\)과 \\(m_{2}\\)의 핵심 개념을 가지고 있다고 가정하자. 우리는 \\(\\sum_{i=2}^{5}\\binom{m_{1}+m_{2}}{i}-\\sum_{i=2}^{5}\\binom{m_{1}}{i}-\\sum_{i=2}}^{5}\\binom{m_{2}}{i}\\)의 다른 조합을 가질 수 있으며, 이는 첫 번째 전략보다 훨씬 더 크다. 우리는 생성된 질문이 난이도가 다양하도록 두 가지 전략을 모두 사용합니다.\n' +
      '\n' +
      '답변 생성 이전 단계에서 질문을 생성한 후 이 질문을 GPT-3.5로 보내고 답변을 수집한다. 우리는 GPT-3.5에서 생성된 답변의 품질이 충분하고 GPT-3.5를 사용하는 것이 GPT-4보다 훨씬 빠르기 때문에 응답 생성을 위해 GPT-3.5를 사용한다. 다양한 난이도를 가진 다양한 분야의 질문-답변 쌍으로 인해, 우리는 결과적인 LLM이 광범위한 작업에서 탁월할 수 있을 것으로 기대한다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '### Data Generation\n' +
      '\n' +
      'GPT-4에게 인간 지식과 능력의 분류법을 만들도록 요청함으로써, 우리는 인간 지식과 능력의 광범위한 영역을 포괄하는 일련의 분야, 하위 분야 및 학문으로 귀결된다. 다음으로, 인간의 주석이 분류의 정확성을 유지하면서 분류의 중복성을 줄이기 위해 분류에서 이러한 요소를 유지해야 하는지 여부를 결정할 것을 요청한다. 필드 또는 하위 필드가 _remove_로 표시된 경우 자손도 제거합니다. 우리는 다수결 투표 후에 126개의 규율을 유지했다. 필요할 때마다 추가 분야, 하위 분야 또는 필드를 수동으로 추가하는 것이 가능하다는 점에 유의하십시오.\n' +
      '\n' +
      '피험자 목록과 강의계획서 생성 동안 GPT-4를 자극하고 온도\\(T=1.0\\)와 상위\\(p=0.95\\)의 핵 샘플링[1]을 사용하여 다양성을 장려한다. 일부 피험자는 GPT-3.5-터보에 의해 효과적으로 모델링되지 않을 수 있는 롱테일 분포에 속하기 때문에 GPT-3.5-터보를 사용하지 않는다. 생성된 과목의 다양성과 완전성을 보장하기 위해 각 분야별로 GPT-4를 10번 질의한다(2.2절). 각 학문별로 평균적으로 100~200개의 과목이 있다. 동일한 과목이 다른 학문에서 나타날 수 있다는 점에 주목할 필요가 있다. 예를 들어, 과목 _calculus_는 물리학과 수학 모두에 있다. 인간 지식에서 그들의 중요성을 반영할 수 있기 때문에 우리는 그러한 주제를 복제하지 않는다.\n' +
      '\n' +
      '지정된 분야의 과목이 주어지면, 우리는 단 한 번만 GPT-4에 질의하여 강의계획서를 설계한다(섹션 2.3의 세부사항을 참조). 온도 및 top-\\(p\\)는 여전히 각각 1.0 및 0.95로 설정된다. 각 강의 계획서에 포함된 수업 세션의 수는 10개에서 30개까지 다양하며, 각 수업 세션에는 약 5개의 핵심 개념이 포함되어 있다.\n' +
      '\n' +
      '명령어 생성 각 명령어 데이터는 질문과 그 답변으로 구성된다. 우리는 별도의 세대가 더 나은 품질로 이어진다는 것을 관찰했기 때문에 질문과 답변을 별도로 생성하기로 선택한다. 그리고 GPT-4로 문항을 생성한 후 각 문항을 온도\\(T=0.7\\), top\\(p=0.95\\)으로 GPT-3.5-turbo로 답하였다. 답변 생성을 위해 GPT-4 대신 GPT-3.5-터보를 사용하는데, GPT-3.5-터보가 상당히 빠르고 비교적 좋은 결과를 얻기 때문이다. 우리는 총 1,000만 개의 지시-응답 쌍을 생성한 다음 훈련 데이터 오염 제거를 수행한다. 구체적으로, 훈련 지시-응답 쌍은 우리가 평가하는 벤치마크 세트의 테스트 및 훈련(있는 경우)에서 질문 또는 입력 프롬프트를 포함하는 쌍을 제거함으로써 오염 제거된다. 우리는 합성 데이터의 일반화 능력을 검증하기 위해 평가되는 벤치마크 훈련 세트를 제외한다.\n' +
      '\n' +
      '### Model Training\n' +
      '\n' +
      '우리는 기본 모델로 미스트랄 7B[12]를 사용한다. 트레이닝 동안, 우리는 각각의 명령 및 응답 쌍을 단일 시퀀스에 연결하고 응답 토큰에 대한 손실만을 계산한다. 학습률이 \\(3e-6\\)인 세 시대에 대한 모델을 학습한다. 배치 크기는 512개의 명령-응답 쌍으로 설정된다. 코사인 학습률 스케줄을 사용하고 1000단계의 선형 웜업으로 시작하여 최종 학습률이 0으로 감소한다.\n' +
      '\n' +
      '### Benchmark Evaluation\n' +
      '\n' +
      '생성된 지시 데이터 GLAN은 광범위한 주제에 걸쳐 있다. 우리는 수학적 추론, 코딩, 논리적 추론 및 학업 시험에서 그 효과를 평가한다.\n' +
      '\n' +
      '수학적 추론 수학은 많은 다른 학문에서 공통적인 주제이다. 따라서 GLAN의 수학 추론 능력을 테스트할 필요가 있다. 우리는 평가를 위해 두 가지 인기 벤치마크(즉, GSM8K[13] 및 MATH[14])를 선택한다. 초등학교 수학 문제(GSM8K[13])는 기본적인 다단계 수학적 추론 능력을 측정하는 양질의 수학 문제 데이터셋이다. 훈련용 7k 내외의 문제와 평가용 1K 테스트 문제를 담고 있다. 수학 적성 검사 휴리스틱 데이터세트(MATH[14])는 AMC 10, AMC 12, AIME 등의 수학 경쟁 문제를 포함하는 도전적인 수학 데이터세트이다. 7.5k 훈련과 5K 시험 문제는 7개의 수학 과목, 즉 프리대수, 프리칼쿨루스를 포함한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline\n' +
      '**Model** & \\(|\\theta|\\) & **HumanE** & **MBPP** & **GSM8K** & **MATH** & **BBH** & **ARC-E** & **ARC-C** & **MMLU** \\\\ \\hline GPT-4 & – & 88.4 & 80.0 & 92.0 & 52.9 & 86.7 & 95.4 & 93.6 & 86.4 \\\\ GPT-3.5-turbo & – & 72.6 & 70.8 & 74.1 & 37.8 & 70.1 & 88.9 & 83.7 & 70.0 \\\\ \\hline LLaMA2 & 7B & 12.8 & 36.2 & 15.4 & 4.2 & 39.6 & 74.6 & 46.3 & 45.9 \\\\ Orca 2 & 7B & 17.1 & 28.4 & 55.7 & 10.1 & 42.8 & 87.8 & 78.4 & 53.9 \\\\ WizardLM v1.2 & 13B & 31.7 & 47.9 & 46.8 & 9.0 & 48.4 & 74.2 & 50.2 & 52.7 \\\\ Mistral & 7B & 28.0 & 50.2 & 43.4 & 10.0 & 56.1 & 79.5 & 53.9 & 62.3 \\\\ Mistral Instruct & 7B & 46.7 & 31.7 & 24.4 & 8.2 & 46.0 & 76.9 & 52.0 & 53.7 \\\\ MetaMath Mistral & 7B & 35.4 & 48.6 & 77.7 & 28.2 & 55.7 & 77.3 & 51.0 & 61.0 \\\\ WizardMath v1.1 & 7B & **51.2** & 54.1 & **83.2** & **33.0** & 58.2 & 79.8 & 53.2 & 60.3 \\\\ Mistral CodeAlpaca & 7B & 35.4 & 50.2 & 34.6 & 8.3 & 56.1 & 79.1 & 54.2 & 60.9 \\\\ \\hline GLAN & 7B & 48.8 & **57.6** & 80.8 & 32.7 & **60.7** & **90.7** & **81.1** & **62.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 수학 추론, 코딩, 논리 추론 및 학업 시험 벤치마크에 대한 주요 결과. 가장 좋은 결과는 볼드체로 표시되고 두 번째로 좋은 결과는 밑줄이 그어져 있습니다.\n' +
      '\n' +
      '대수학, 중급수학, 수이론, 계수, 확률과 기하학. GLAN은 GSM8K 또는 MATH의 트레이닝 세트에서 어떠한 예도 사용하지 않는다는 점에 유의한다. [13]에 이어 GLAN에 대한 0-샷 설정 결과를 보고한다.\n' +
      '\n' +
      'GLAN의 코딩 능력을 평가하기 위해 우리는 HumanEval[21]과 MBPP[3]의 두 가지 코딩 벤치마크를 선택한다. 우리는 HumanEval을 위한 0-shot 설정과 종래 기술에 따른 MBPP를 위한 3-shot 설정을 채용한다[21, 22].\n' +
      '\n' +
      'BIG-Bench Hard 우리가 생성한 명령어 데이터 세트는 GLAN의 추론 능력을 잠재적으로 향상시킬 수 있는 많은 분야를 포함한다. 따라서 우리는 LLM의 일반적인 추론 능력을 평가하기 위해 Big-Bench[24]에서 23개의 도전 과제를 포함하는 BIG-Bench Hard 데이터셋(BBH[23])에서 GLAN을 평가한다. 우리는 생각 사슬 시연을 통해 표준 3발 설정을 사용합니다.\n' +
      '\n' +
      '학업 시험 우리는 또한 GLAN이 시험 문제를 해결할 수 있는지 확인하기 위해 다양한 학문적 벤치마크에서 GLAN을 평가한다. 우리는 두 개의 벤치마크(즉, ARC[21]와 MMLU[30])를 선택한다. 두 벤치마크 모두 객관식 문항으로 구성되어 있다. AI2 추론 도전(ARC[21])에는 학년 수준, 다중 선택 과학 질문이 포함되어 있다. 이를 정확하게 답하기 위해 모델은 기초 지식을 파악할 뿐만 아니라 일정 수준의 추론 능력을 가질 것으로 예상된다. 여기에는 ARC-Challenge(ARC-C)와 ARC-Easy(ARC-E)의 두 가지 하위 집합이 포함되어 있다. 대용량 멀티태스크 언어 이해(MMLU[30])는 초등 수준부터 전문 수준까지 난이도에 해당하는 57개 과목에 대한 객관식 질문 세트로 구성된다. 인문, STEM, 사회과학 등 다양한 지식 영역을 다루고 있다. ARC에 대한 교육 세트가 있습니다. 그러나 섹션 3.1에 설명된 오염 제거 과정에서 훈련 세트에서 제외했다. 이전 모델은 대부분 ARC 및 MMLU에 기반한 확률 기반 방법을 활용하며, 이는 해당 다중 선택 질문에 조건화된 4가지 옵션의 확률에 기반한 최상의 옵션을 반환한다. 우리는 실험에서 1천만 개의 숙제 질문에 대한 교육을 받은 후 GLAN이 GPT-3.5-터보처럼 일반 텍스트에서 예측 옵션을 생성하고 다중 선택 질문을 분석할 수 있음을 관찰한다. 따라서 우리는 GLAN에 대한 0-샷 설정을 선택하고 [23]에서와 같이 그 완성도를 기반으로 규칙을 사용하여 예측을 추출한다.\n' +
      '\n' +
      '주요 결과는 표 1에 나와 있다. GLAN을 일반 도메인 모델(Orca 2[23], Mistral Instruct[24] 및 WizardLM[24])과 수학 최적화 모델(MetaMath[24] 및 WizardMath[13]) 및 코딩 최적화 모델(CodeAlpaca[25])과 비교한다. 우리는 또한 기준 LLM(즉, LLaMA2[24] 및 미스트랄[24])의 결과를 보고한다. GLAN은 모든 벤치마크에서 최상의 결과를 얻거나 최상의 결과에 가까운 결과를 얻습니다. 우리는 수학 또는 코딩 최적화 모델의 능력이 수학 또는 코딩 벤치마크에서 증가하는 반면 일반적으로 다른 모델에서는 증가하지 않는다는 것을 관찰한다. 수업 튜닝 후, GLAN은 체계적인 데이터 생성 접근법으로 수학적 추론, 코딩, 추론 및 학업 시험으로부터 다차원에서 탁월하다. 또한 본 방법은 GLAN의 일반적인 적용 가능성을 나타내는 Orca 2, MetaMath 및 WizardMath에서와 같이 GSM8K, MATH 또는 ARC의 훈련 세트와 같은 작업 특정 훈련 데이터를 사용하지 않는다는 점에 유의한다.\n' +
      '\n' +
      '학업 시험ARC와 MMLU를 자세히 살펴보면 모두 학력 시험의 다중 선택 기반 벤치마크입니다. 그러나 ARC의 미스트랄에 대한 GLAN의 개선이 MMLU에서보다 훨씬 더 크다는 것을 관찰한다(표 1 참조). MLU의 57개 과목을 네 가지 범주(즉, STEM, 인문, 사회 과학 및 기타(비즈니스, 건강, 혼돈)로 그룹화함으로써, 우리는 GLAN이 MMLU의 STEM에서 크게 개선되는 반면 다른 범주는 개선되지 않는 것을 관찰한다(표 2). 또한 유의하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**ARC-E**} & \\multirow{2}{*}{**ARC-C**} & \\multicolumn{4}{c}{**MMLU**} \\\\  & & & **STEM** & **Humanities** & **Social Sciences** & **Other** \\\\ \\hline Mistral & 79.5 & 53.9 & 52.0 & 56.5 & 73.3 & 70.1 \\\\ GLAN & **90.7** & **81.1** & **60.1** & 54.9 & 71.8 & 68.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 학업 시험 벤치마크에 대한 세부 결과.\n' +
      '\n' +
      'ARC는 STEM 질문이기도 한 고등학교 과학 문제로 구성되어 있다. GLAN은 STEM 피험자를 잘 다룰 수 있는데, 이는 데이터 세트의 응답이 GPT-3.5-터보에서 나왔기 때문일 수 있으며, 이는 기본적으로 CoT(Chain-of-Thoughts) 추론으로 응답을 생성한다. 실제로, 우리는 GLAN이 다중 선택 질문에 대해 CoT로 솔루션을 생성한다는 것을 관찰한다. CoT는 STEM 다중 선택 질문 [\\(\\mathrm{WWS}^{+}\\)22]에서 다단계 추론을 도울 수 있는 반면, 인문 및 사회 과학 질문은 암기와 단일 단계 추론을 더 포함하며, 여기서 CoT는 추가 오류를 도입할 수 있다.\n' +
      '\n' +
      '### 과제별 훈련 데이터\n' +
      '\n' +
      'GLAN은 명령어 튜닝을 위한 합성 데이터를 생성하기 위한 일반화된 방법이다. 이 합성 데이터의 일반화 능력을 평가하기 위해 우리는 의도적으로 평가를 수행하는 모든 벤치마크에서 작업별 훈련 세트를 제외한다. [\\(\\mathrm{WZZ}^{+}\\)23]과 유사하게, 우리는 모델들이 태스크 특정 도메인 데이터에 대해 트레이닝되었는지 여부를 탐색한다. GLAN과 다른 모델에 대해 ARC Challenge (ARC-C), ARC Easy (ARC-E), GSM8K 및 MATH에 대한 훈련 손실\\(L_{train}\\) 및 테스트 손실\\(L_{test}\\)을 비교 계산한다. 섹션 3.3에서 평가된 모든 벤치마크 중 이러한 벤치마크에는 훈련 세트가 포함되어 있기 때문에 이 네 가지 데이터 세트를 선택한다. 직관적으로 \\(\\Delta=L_{test}-L_{train}\\)이 클수록 훈련 집합이 노출될 가능성이 높다. \\(\\Delta\\)를 더 쉽게 해석할 수 있도록 상대차 \\(\\Delta(\\%)=(L_{test}-L_{train})/L_{test}\\을 추가로 계산한다. 표 3은 GLAN에 대한 훈련 및 테스트 분할의 손실이 거의 동일함을 보여준다(또는 \\(\\Delta\\)은 음성이다). 이는 GLAN이 훈련 및 조정 절차 동안 도메인 내 데이터에 노출되지 않았음을 시사한다. 또한, GLAN이 다른 모델에 비해 GSM8K, MATH 및 ARC에서 테스트 및 훈련 분할 모두에서 더 높은 손실을 얻는 반면, 이 네 가지 데이터 세트에 대한 GLAN의 결과는 높다는 것을 관찰한다(표 1 참조). 이는 GLAN에 의해 생성된 합성 데이터가 다양하고 결과 모델이 기존 벤치마크에 존재하는 특정 도메인 또는 스타일로 수렴하는 것을 피할 수 있음을 의미할 수 있다.\n' +
      '\n' +
      '### 수업 후 평가\n' +
      '\n' +
      'IFEvalWe assess the instruction following evaluation dataset (IFEval [\\(\\mathrm{ZLM}^{+}\\)23])을 이용하여 GLAN의 instruction-following capability를 평가한다. IFEval은 25개의 별개의 유형의 명령어(총 500개의 프롬프트 약)를 포함하는 "검증 가능한 명령어"의 집합으로 구성된다. 각각의 프롬프트는 하나 이상의 검증가능한 명령어들을 포함한다. 평가에는 신속 수준과 지시 수준 모두에서 4가지 유형의 메트릭이 포함되어 엄격하고 느슨한 정확도를 평가한다.\n' +
      '\n' +
      '표 4에 나타난 바와 같이, GLAN은 프롬프트-레벨 및 명령어-레벨 평가 모두에서 우수한 명령어-추종 능력을 보여준다. 다만 GPT-3.5-터보, GPT-4에 비해 여전히 상당한 격차가 존재한다.\n' +
      '\n' +
      'Evol-Instruct TestEvol-Instruct Testset [\\(\\mathrm{XSZ}^{+}\\)23]은 다양한 소스로부터 실제 인간의 지시를 포함하고 있으며, 29개의 별개의 기술을 가진 218개의 인스턴스로 구성된다. 각 지침은 1에서 10까지의 난이도와 관련이 있으며, 응답은 종종 무제한 설명이며 이 벤치마크는 IFEval(지침에 대한 답변이 "검증 가능"함)에 필요한 보완이라고 믿는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c c} \\hline \\hline\n' +
      '**Benchmark/Loss** & **LLaMA2-7B** & **Orca2-7B** & **Mistral-7B-Instruct** & **WizardLM-13B-V1.2** & **GLAN-7B** \\\\ \\hline \\multirow{3}{*}{**ARC-C**} & \\(L_{test}\\) & 2.02 & 2.39 & 2.32 & 2.11 & 4.03 \\\\  & \\(L_{train}\\) & 2.03 & 2.34 & 2.33 & 2.12 & 4.06 \\\\  & \\(\\Delta\\) & -0.01 & 0.05 & -0.01 & -0.01 & -0.03 \\\\  & \\(\\Delta\\) (\\%) & **-0.5\\%** & 2.10\\% & **-0.43\\%** & **-0.47\\%** & **-0.74\\%** \\\\ \\hline \\multirow{3}{*}{**ARC-E**} & \\(L_{test}\\) & 2.10 & 2.47 & 2.51 & 2.18 & 4.31 \\\\  & \\(L_{train}\\) & 2.12 & 2.43 & 2.54 & 2.20 & 4.32 \\\\  & \\(\\Delta\\) & -0.02 & 0.04 & -0.03 & -0.02 & -0.01 \\\\  & \\(\\Delta\\) (\\%) & **-0.95\\%** & 1.61\\% & **-1.19\\%** & **-0.91\\%** & **-0.23\\%** \\\\ \\hline \\multirow{3}{*}{**GSM8K**} & \\(L_{test}\\) & 1.38 & 1.14 & 1.26 & 1.14 & 2.17 \\\\  & \\(L_{train}\\) & 1.38 & 1.01 & 1.26 & 1.09 & 2.15 \\\\  & \\(\\Delta\\) & 0 & 0.13 & 0 & 0.05 & 0.02 \\\\  & \\(\\Delta\\) (\\%) & **0\\%** & 11.4\\% & **0\\%** & 4.39\\% & **0.92\\%** \\\\ \\hline \\multirow{3}{*}{**MATH**} & \\(L_{test}\\) & 1.11 & 1.18 & 1.12 & 1.22 & 1.67 \\\\  & \\(L_{train}\\) & 1.14 & 1.15 & 1.15 & 1.24 & 1.70 \\\\ \\cline{1-1}  & \\(\\Delta\\) & -0.03 & 0.03 & -0.03 & -0.02 & -0.03 \\\\ \\cline{1-1}  & \\(\\Delta\\) (\\%) & **-2.70\\%** & 2.54\\% & **-2.67\\%** & **-1.63\\%** & **-1.79\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 테스트 데이터와 트레이닝 데이터 간의 손실값 평가. 큰 양의 \\(\\Delta\\)(또는 \\(\\Delta(\\%)\\))는 훈련 중에 도메인 내 훈련 데이터가 모델에 노출됨을 나타낼 수 있다.\n' +
      '\n' +
      '[23] 및 [14]에 이어 GPT-4 기반 자동 평가 방법을 채택하여 GLAN과 다른 모델 간의 쌍별 비교를 수행한다. 구체적으로, GPT-4는 주어진 입력 질문에 대해 두 개의 다른 모델에 의해 생성된 응답의 유용성, 관련성, 정확도 및 세부 사항 수준을 1에서 10 사이의 총 점수를 할당하도록 지시받는다. 점수가 높을수록 전반적인 성능이 향상됨을 나타냅니다. 잠재적인 순서 편향을 완화하기 위해 각 응답 쌍에 대해 양방향 비교를 수행하고 평균 점수를 결정한다. GLAN에 대한 평균 점수 차이(즉, avg_score(GLAN) \\(-\\) avg_score(\\(x\\))가 최종 메트릭 역할을 한다. <표 5>는 다양한 수준의 수업 난이도에 따른 쌍대비교 결과를 제시하고 있다. GLAN은 대부분의 난이도와 전체 점수에서 LLaMA-2, Orca 2, Mistral Instruct, 심지어 WizardLM-13B(GLAN에는 7B 매개 변수만 포함되어 있음)에 비해 우수한 성능을 보여준다. 이는 GLAN이 어려움이나 복잡성에 관계없이 다양한 명령어를 처리하는 능력이 향상되었음을 시사한다. 또한 GLAN은 비교에서 다른 모델로서 GPT-3.5-터보에 뒤처진다는 점에 유의한다. 또한, 우리는 29개의 기술에 따라 Evol-Instruct 테스트를 그룹화하고 동일한 경향을 관찰한다. 자세한 결과는 부록(표 7)에 나와 있다. GLAN은 특히 수학, 코딩 및 추론에서 대부분의 기술에서 강력한 성능을 보여줍니다. 그러나 상식적인 관련 업무에는 다소 부족합니다.\n' +
      '\n' +
      'GLAN-Test In IFEval 및 Evol-Instruct Test에는 수백 가지 지침만 있으며 우리는 그들이 다룰 수 있는 도메인이나 기술이 다소 제한적이라고 믿는다. 따라서 GLAN 데이터를 사용하여 보류 테스트 세트를 제안하고 이를 GLAN 테스트라고 한다. 126개 학문에 대한 6,300개의 지침(각 학문에 대한 50개 지침)이 포함되어 있습니다. 또한 126개 학문을 8개의 별개의 _fields_(즉, 학술-인문학, 학술-사회과학, 학술-자연과학, 학술-응용과학, 학술-형식과학, 산업-제조, 산업-서비스 및 산업-농업)로 분류한다. GLAN-Test의 광범위한 도메인 적용 범위가 LLM의 일반화 능력 평가를 위한 효과적인 테스트 베드로 만든다고 믿는다. 우리는 Evol-Instruct Test(이전 단락)에서와 동일한 GPT-4 기반 평가 프로토콜을 채택한다. GPT-4가 GLAN과 다른 모델의 쌍별 순위를 비교하도록 촉구한다. 8개 필드에 걸친 전체 결과와 결과는 표 6에 나와 있으며, 여기서 GLAN은 7B 매개변수만 사용함에도 불구하고 Orca2-7B, Mistral-7B 지시 및 WizardLM-13B보다 더 높은 GPT-4 점수를 얻는다. GLAN은 여전히 GPT-4에 뒤쳐져 있다. 126개의 세립 학문에 대한 자세한 결과는 부록 A.2에서 찾을 수 있다(자세한 내용은 표 8 참조). GLAN은 다중 도메인(또는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline\n' +
      '**Model** & \\begin{tabular}{l} **Prompt-level** \\\\ **strict-accuracy** \\\\ \\end{tabular} & \\begin{tabular}{l} **Instruction-level** \\\\ **strict-accuracy** \\\\ \\end{tabular} & \\begin{tabular}{l} **Prompt-level** \\\\ **strict-accuracy** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{l} **Instruction-level** \\\\ **toxic-accuracy** \\\\ \\end{tabular} \\\\ \\hline GPT-3.5-turbo & 53.8 & 64.7 & 56.6 & 67.5 \\\\ GPT-4 & 77.1 & 83.7 & 79.7 & 85.6 \\\\ \\hline LLaMA2-7B & 14.8 & 27.1 & 16.6 & 29.4 \\\\ Orca2-7B & 19.4 & 28.9 & 26.1 & 34.7 \\\\ Mistral-7B-Instruct-v0.1 & 32.0 & 42.8 & 37.7 & 48.0 \\\\ WizardLM-13B-V1.2 & 23.1 & 33.5 & 26.6 & 37.6 \\\\ GLAN-7B & **34.0** & **44.8** & **41.2** & **51.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: IFEval에 대한 역량 평가 후 지도.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c|c c c c c} \\hline \\hline\n' +
      '**Diffulty** & **Ratio** & **LLaMA2-7B** & **Orca2-7B** & **Mistral-7B-Instruct** & **Wizard-13B-V1.2** & **GPT-3.5-turbo** \\\\ \\hline\n' +
      '1 & 5.1\\% & 5.41 & 2.23 & -0.37 & -0.21 & -2.41\\\\%\n' +
      '2 & 8.7\\% & 5.87 & 1.74 & 1.06 & 1.41 & -1.18\\\\\n' +
      '3 & 12.4\\% & 5.72 & 2.35 & 1.04 & 1.37 & -1.14\\\\\n' +
      '4 & 10.5\\% & 5.61 & 1.34 & 1.52 & 1.54 & -0.92 \\\\\n' +
      '5 & 4.1\\% & 4.67 & 3.31 & 2.39 & 2.5 & -0.45\\\\\n' +
      '6 & 19.3\\% & 4.43 & 2.42 & 0.74 & 1.54 & -1.36\\\\\n' +
      '7 & 11.0\\% & 4.97 & 1.26 & 1.62 & 1.36 & -0.41\\\\\n' +
      '8 & 17.9\\% & 6.02 & 3.58 & 3.17 & 1.7 & 0.15\\\\\n' +
      '9 & 6.0\\% & 6.35 & 4.2 & 1.36 & 0.9 & -0.92\\\\\n' +
      '10 & 5.1\\% & 5.14 & -0.05 & 1.53 & -0.54 & -0.85 \\\\ \\hline (1-5) Easy & 41.00\\% & **5.46** & **2.19** & **1.13** & **1.32** & -1.22 \\\\\n' +
      '6-10 Hard & 59.00\\% & **5.38** & **2.28** & **1.68** & **0.99** & -0.68 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: GLAN과 Evol-Instruct 테스트 세트의 다른 모델 간의 다양한 난이도에 대한 쌍별 비교. 점수는 GPT-4가 할당한 점수의 평균 격차로 avg_score(GLAN) \\(-\\) avg_score(\\(x\\))로 계산된다.\n' +
      '\n' +
      'discilines)와 같은 수학, 물리학, 화학, 컴퓨터 과학, 전기, 기계 등은 더 작은 모델들이 전략적 미세 조정을 통해 다양한 도메인들에 대한 일반적인 개선들을 산출할 수 있음을 나타낸다. 또한 GLAN은 미국 역사, 신성 또는 방사선과 같은 별개의 분야에서 이상보다 낮은 성능을 보여준다. 이 관찰은 이러한 도메인 내에서 방법론의 추가 정제 및 개발 가능성을 강조한다.\n' +
      '\n' +
      '##4 관련 업무\n' +
      '\n' +
      '최근 문헌은 수업 조정을 위해 다양한 인간이 만든 자원의 수집을 광범위하게 탐구했다. 직관적인 방향은 기존의 NLP 데이터세트와 대응하는 태스크 설명[14, 15, 16]을 수집하는 것이며, BLOOMZ[17] 및 FLAN[15]과 같은 전형적인 LLM은 이러한 유형의 명령어 튜닝 데이터에 대해 트레이닝된다. 그러나, 기존의 데이터 셋이 수만 개에서 수천 개에 불과하여, 명령어 튜닝의 범위와 다양성은 제한될 수밖에 없다. 또 다른 일반적인 관행은 실제 인간 사용자 프롬프트로 명령어 튜닝을 구현하는 것이다. 예를 들어, InstructGPT[15]는 실제 사용자가 오픈AI GPT API에 제출한 고품질 인간 프롬프트에 대해 훈련되었다. Vicuna[13]은 명령어 조정을 위해 ChatGPT 응답과 함께 사용자 공유 프롬프트를 활용하며, Dolly[16]은 5k 이상의 직원이 작성한 시뮬레이션된 인간 사용자 상호 작용에 대해 훈련되었다. 그럼에도 불구하고, 인간 사용자들로부터 교육 데이터를 획득하는 것은 전형적으로 높은 비용을 수반하고 프라이버시 염려를 수반한다.\n' +
      '\n' +
      'LLM 기능이 개선됨에 따라 LLM 생성 데이터를 사용한 명령어 조정은 초정렬 문제를 해결하는 데 더 나은 확장성과 잠재력을 나타낸다[17]. LLM의 언컨텍스트 학습 능력을 활용하여, 새로운 명령어를 생성하기 위해 LLM을 유도하기 위해 부자연스러운 명령어 [18] 및 자체 명령어 [14] 샘플 시드 명령어를 예로 들 수 있다. LLM의 리프레이징 능력을 활용하여 WizardLM[19]과 WizardMath[19]를 Evol-Instruct를 이용하여 훈련하였다. Evol-Instruct는 ChatGPT를 반복적으로 사용하여 시드 명령어를 점점 더 복잡한 명령어로 재작성한다. 시드 명령어로부터의 생성과 유사하게, 엄선된 시드 토픽들은 교과서-유사 합성 데이터(20) 또는 명령어 튜닝을 위한 자가 채팅 멀티턴 다이얼로그들(21, 22)을 생성하기 위해 사용된다. 그러나 이러한 LLM 생성 데이터에 대해 학습된 모델은 수학 [19], YJS\\({}^{+}\\)23], 대화 [21, 22] 또는 개방형 질문 응답 [23, 24]와 같은 특정 도메인에서만 잘 작동한다. 이러한 방법은 데이터 다양성이 종자 지시 또는 종자 주제에 의해 제한되기 때문에 일반화에 어려움을 겪는다[15].\n' +
      '\n' +
      '## 5 Conclusions\n' +
      '\n' +
      '우리는 지시 데이터를 합성하기 위한 일반적이고 확장 가능한 방법인 GLAN을 제안한다. 실험 결과 GLAN은 대규모 언어 모델이 수학적 추론, 코딩, 학업 시험, 논리적 추론에서 일반 수업에 이르기까지 다차원적으로 능력을 향상시키는 데 도움이 될 수 있음을 보여준다. 현재 우리의 합성 데이터는 인간의 지식과 능력의 분류에 기초하고 있으며 다루지 않은 다른 유형의 유용한 데이터가 있다. 우리는 보더 커버리지를 갖춘 방법을 설계하는 데 관심이 있습니다. 현재 우리의 지시 데이터는 대부분 질문 응답 쌍이며 다음 단계에서는 멀티턴 대화와 긴 문서의 합성 데이터를 생성할 계획이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Field (Ratio)** & **Orca2-7B** & **Mistral-7B-Instruct** & **WizardLM-13B-V1.2** & **GPT-4** \\\\ \\hline Academic-Hummities (15.9\\%) & 0.79 & 0.25 & 0.02 & -0.62 \\\\ Academic-Social Science (7.9\\%) & 1.22 & 0.21 & 0.09 & -0.63 \\\\ Academic-Natural Science (4.0\\%) & 1.73 & 1.23 & 0.53 & -0.5 \\\\ Academic-Applied Science (42.1\\%) & 1.58 & 0.32 & 0.08 & -0.58 \\\\ Academic-Formal Science (3.2\\%) & 3.87 & 2.48 & 2.32 & -0.55 \\\\ Industry-Manufacturing (12.7\\%) & 2.26 & 0.56 & 0.33 & -0.43 \\\\ Industry-Services (11.9\\%) & 1.82 & 0.23 & 0.09 & -0.5 \\\\ Industry-Agriculture (2.4\\%) & 1.2 & 0.46 & 0.13 & -0.33 \\\\ \\hline Overall (100.0\\%) & **1.61** & **0.43** & **0.19** & -0.55 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: GLAN-테스트에서 GLAN과 다른 모델 간의 쌍별 비교(126개 분야는 그림의 명확성을 위해 8개 분야로 분류된다. 점수는 GPT-4에 의해 부여된 점수의 평균 격차로 \\(\\text{avg\\_score}(\\text{GLAN})-\\text{avg\\_score}(x)\\으로 계산된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[AON\\({}^{+}\\)201] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _ arXiv preprint arXiv:2108.07732_, 2021.\n' +
      '*[BMR\\({}^{+}\\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등의 언어 모델들은 소수의 학습자들이다. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '*[CCE\\({}^{+}\\)18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 답하는 걸 해결했다고 생각해? try arc, the ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_, 2018.\n' +
      '*[Cha23] 사힐 차우드해리. 코드 알파카: 코드 생성을 위한 명령어-후속 라마 모델. [https://github.com/sahil280114/codealpaca] (https://github.com/sahil280114/codealpaca), 2023.\n' +
      '*[CHM\\({}^{+}\\)23] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 무료 돌리: 세계 최초로 진정으로 열린 지침 조정 llvm, 2023을 소개합니다.\n' +
      '* [CKB\\({}^{+}\\)21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 수학 단어 문제를 해결하기 위한 검증자 훈련 arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '*[CLL\\({}^{+}\\)23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, 및 Eric P. Xing. 비쿠나: 2023년 3월, 90%의 채팅 품질을 가진 gpt-4를 인상하는 오픈 소스 챗봇.\n' +
      '*[CTJ\\({}^{+}\\)21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman 등 코드로 훈련된 대형 언어 모델을 평가한다. _ arXiv preprint arXiv:2107.03374_, 2021.\n' +
      '* [DCX\\({}^{+}\\)23] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 고품질 수업 대화를 확장하여 채팅 언어 모델을 향상시킵니다. _ arXiv preprint arXiv:2305.14233_, 2023.\n' +
      '* [GWS\\({}^{+}\\)23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song. 전매특허인 llvms를 모방한다는 잘못된 약속 arXiv preprint arXiv:2305.15717_, 2023.\n' +
      '*[HBB\\({}^{+}\\)20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 대규모 멀티태스크 언어 이해도 측정 arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '*[HBD\\({}^{+}\\)19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 신경 텍스트 변성의 특이한 경우. _ ArXiv preprint arXiv:1904.09751_, 2019.\n' +
      '*[HBK\\({}^{+}\\)21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt. 상기 수학 데이터셋으로 수학 문제 풀이를 측정하는 단계; _ NeurIPS_, 2021.\n' +
      '*[HBM\\({}^{+}\\)22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* [HSLS22] or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 부자연스러운 명령: (거의) 인간의 노동력이 없는 언어 모델을 조정합니다. _ ArXiv_, abs/2212.09689, 2022.\n' +
      '\n' +
      '*[JSM\\({}^{+}\\)23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier 등 Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '*[KMH\\({}^{+}\\)20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei. 신경 언어 모델의 법칙을 확장합니다. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '*[LBE\\({}^{+}\\)23] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 교과서는 ii: phi-1.5 기술 보고서가 필요한 전부입니다. _ arXiv preprint arXiv:2309.05463_, 2023.\n' +
      '*[LHV\\({}^{+}\\)23] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The final collection: Designing data and methods for effective instruction tuning. _ arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '*[LSX\\({}^{+}\\)23] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang. Wizardmath: 강화된 evol-instruct를 통해 큰 언어 모델에 대한 수학적 추론력을 강화한다. _ arXiv preprint arXiv:2308.09583_, 2023.\n' +
      '*[LXZ\\({}^{+}\\)23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder:evol-instruct로 코드 대언어 모델을 Empowering하는 단계; _ arXiv preprint arXiv:2306.08568_, 2023.\n' +
      '*[MDCM\\({}^{+}\\)23] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. Orca 2: 소언어 모델들에게 추론하는 방법을 가르친다. _ arXiv preprint arXiv:2311.11045_, 2023.\n' +
      '*[MMJ\\({}^{+}\\)23] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: gpt-4의 복잡한 설명 흔적으로부터 점진적 학습. _arXiv preprint arXiv:2306.02707_, 2023.\n' +
      '*[MWS\\({}^{+}\\)22] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. ArXiv:2211.01786_, 2022.\n' +
      '*[OWJ\\({}^{+}\\)22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744, 2022에서의 발전.\n' +
      '*[SJH\\({}^{+}\\)23] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong. 대언어 모델 정렬: 설문조사 _ arXiv preprint arXiv:2309.15025_, 2023.\n' +
      '*[SRR\\({}^{+}\\)23] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. 브라운, 아담 산토로, 아디티야 굽타, 아드리아 가리가-알론소, 아그네츠카 클루스카, 아토르 루코위츠, 악샤트 아가왈, 알레티아 파워, 알렉스 레이, 알렉스 워스타트, 알렉산더 W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Allicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrew Madotto, Andrew La, Andrew Lampinen, Arash Golamidayi, Animesh Gottardi, Arfa Tabassum, Arul Menezes, Arh Mullokandov, Ashish Sabharwal, Austin Herrick, Abia Efrat, Aykut Erdem, Aykut Ekmekci, Ryan Roberts, Bao Sheng Loe, Benno Stour, Catherine Rathkopf, Cedrick Argueta, Cesar Perri Ramirez, Chantan Singh, Citromiej Bojanowski, Bitan Ozyurt, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairsatnia, Bhenan Hairs Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. 룰, 조이스 추아, 카밀 칸클레르츠, 카렌 리브스쿠, 카텔 고팔라크리슈난, 카테리나 이그나티바, 카쟈 마커트, 코리 매튜슨, 크리스틴 치아풀로, 세니아 슈카루타, 쿠마르 슈리드하르, 카일 맥도넬, 카일 리처드슨, 라리아 레이놀즈, 레오 가오, 루헴 허, 루이스 올리베로스 콜론, 루카 메츠, 루크 노블, 루드비히 슈미트, 루이히 오찬도, 루이 필리페 모레시, 루카 모레마, 마르코 루이스, 마르타 루이스, 마틴 포타스트, 매튜 L. 리빗, 마티아스 하겐, 마티아스 슈베르트, 메디나 오르두나 바이테미로바, 멜로디 아르나, 마이클 구, 마이클 이바니츠키, 마이클 스타릿트, 마이클 스트루브, 미칼 스웨드로스키, 미셸 베빌락쿠아, 미히르 야수나가, 미히르 케일, 마이크 케인, 미메에 슈, 미히르 수즈건, 미치 워커, 모티와리, 모히트 반살, 모히트 반살, 모힌 아민나세리, 모히트 반살, 모히트 반살, 모히트 반살, 모히트 반살, 모힌 바르타 T, 나연 펭, 나단 A, 니클라스 데커스, 니클라스 무엔히코프, 니티시 시리쉬 케스카, 니니티타 S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Hutt, Pinyu Hwang, Piotr Mikkowski, Piyush Patil, Piaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Rabacker, Ramon Risco, Rylan Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Reean Ree Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. 새뮤얼 S. 보먼 Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, ShimaAsaadi, Shixiang Shane Gu, Shubh Shachchigar, Shubham Toshniwal, Shhyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Su-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephen Prasad, Steven T. 스튜어트 피안타도시 시베르, 서머 미셔기, 스베틀라나 키리첸코, 스와루프 미쉬라, 탈 린젠, 탈 슈스터, 타오 리, 타오 유, 타타리크 알리, 테오 린 우, 테오 데스보르드 왕, 티베리우스 니키닐리, 티모 쉬크, 티모페이 코르네프, 토바이어스 게르스텐베르그, 트렌턴 장, 트리샤르 니에르, 타일러 슈나예르, 타일러 슈나예르, 타일러 슈나예르, 비카스 라흐마르, 비카스 라흐마르, 비카스 라흐마르, 비카스 라흐마르, 비카스 파드마흐마르, 비카스 세이드, 주오예 자오, 지제 왕, 지이 왕, 지이 우. 모방 게임을 넘어: 언어 모델의 능력을 정량화하고 추론하는 것, 2023.\n' +
      '*[SSS\\({}^{+}\\)22] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, 및 Jason Wei. 큰 벤치의 도전과 생각의 사슬이 그것을 해결할 수 있는지 여부. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '*[SWR\\({}^{+}\\)22] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. 나약, 데바요티 다타, 조나단 장, 마이크 톈-지안 장, 한 왕, 마테오 마니카, 정셴, 정신 용, 허름한 팬디, 레이첼 바덴, 토마스 왕, 트리샨 니라즈, 조스 로젠, 아비슈트 샤르마, 안드레아 산틸리, 티볼트 페브리, 제이슨 앨런 프라이스, 라이언 티한, 테븐 르 스카오, 스텔라 바이더만, 레오 가오, 토마스 울프, 알렉산더 M. 러쉬 멀티태스크 프롬프트 트레이닝은 제로 샷 태스크 일반화를 가능하게 한다. _The Tenth International Conference on Learning Representations_, 2022.\n' +
      '* [TGZ\\({}^{+}\\)23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto. 스탠포드 알파카: 지시를 따르는 라마 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.\n' +
      '*[TMS\\({}^{+}\\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and finetuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '*[WBZ\\({}^{+}\\)21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 최적화된 언어 모델은 제로샷 학습자입니다. _ arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* [wik23] 교육, 2023. 2023년 3월 24일에 마지막으로 편집되었습니다.\n' +
      '* [WKM\\({}^{+}\\)22] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi. 자가 명령어: 언어 모델을 자가 생성된 명령어와 정렬합니다. _ ARXiv 프리프린트 arXiv:2212.10560_, 2022.\n' +
      '*[WMA\\({}^{+}\\)22] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-natural instructions: 1600+ nlp 태스크에 대한 선언적 지시를 통한 일반화 ArXiv:2204.07705_, 2022.\n' +
      '*[WWS\\({}^{+}\\)22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting reasoning in large language models. _ 신경 정보 처리 시스템_, 35:24824-24837, 2022에서의 발전.\n' +
      '\n' +
      '*[WZZ\\({}^{+}\\)23] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Chheng Chang Hu, Weiwei Lu, Rui Hu, Chhenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou. 스카이워크: 좀 더 개방적인 이중언어 기반 모델, 2023.\n' +
      '* [XGDM23] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: self-chat 데이터에 대한 파라미터-효율적인 튜닝을 갖는 오픈 소스 채팅 모델 _ arXiv preprint arXiv:2304.01196_, 2023.\n' +
      '*[XSZ\\({}^{+}\\)23] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang. Wizardlm: 복잡한 명령어를 따르기 위해 대규모 언어 모델의 권한을 부여합니다. _ arXiv preprint arXiv:2304.12244_, 2023.\n' +
      '*[YJS\\({}^{+}\\)23] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 메타매스: 큰 언어 모델에 대해 자신의 수학적 질문을 부트스트랩합니다. _ arXiv preprint arXiv:2309.12284_, 2023.\n' +
      '*[ZLM\\({}^{+}\\)23] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 대형 언어 모델에 대한 명령어 후속 평가. _ arXiv preprint arXiv:2311.07911_, 2023.\n' +
      '*[ZLX\\({}^{+}\\)23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less는 정렬에 더 가깝다. _ arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '\\begin{tabular}{l r r r r} \\hline\n' +
      '**Discipline** & **Orca-2.7b** & **Mistral-7B-Instruct-v0.1** & **WizardLM-13B-V1.2** & **GPT-4** \\\\ \\hline Avg. & 1.61 & 0.43 & 0.19 & -0.55 \\\\ \\hline Advertising & 1.92 & 0.46 & 0.21 & -0.04 \\\\ Aerospace industry & 3.24 & 1.24 & 0.6 & -0.42 \\\\ Agriculture & 2.44 & 0.04 & -0.05 & -0.48 \\\\ American history & -0.49 & -0.27 & -0.76 & -0.83 \\\\ American politics & 1.23 & -0.3 & -0.4 & -0.87 \\\\ Anthropology & 0.59 & 0.17 & 0.06 & -0.27 \\\\ Applied mathematics & 3.75 & 2.6 & 2.74 & -0.47 \\\\ Archaeology & 2.59 & -0.11 & 0.1 & -0.56 \\\\ Architecture and design & 2.63 & 0.34 & 0.4 & -0.37 \\\\ Astronomy & 1.01 & 0.83 & 0.03 & -0.44 \\\\ Automotive industry & 1.27 & 0.71 & 0.46 & -0.06 \\\\ Bibilical studies & -0.05 & 0.33 & -0.47 & -0.65 \\\\ Biology & 1.09 & 0.22 & -0.09 & -0.17 \\\\ Business & 3.61 & 1.14 & 0.88 & -0.26 \\\\ Chemical Engineering & 3.15 & 1.6 & 1.18 & -0.77 \\\\ Chemistry & 3.06 & 2.09 & 0.8 & -0.87 \\\\ Civil Engineering & 1.94 & 0.74 & 0.75 & -0.25 \\\\ Clinical laboratory sciences & 1.32 & 0.94 & -0.11 & -0.47 \\\\ Clinical neurophysiology & 2.15 & 0.29 & 0.25 & -0.4 \\\\ Clinical physiology & 2.07 & 0.41 & 0.51 & -0.08 \\\\ Communication studies & 0.3 & 0.26 & -0.15 & -0.3 \\\\ Computer science & 4.29 & 1.45 & 1.9 & -0.33 \\\\ Cultural industry & 3.15 & 0.44 & 0.05 & -0.36 \\\\ Dance & 2.11 & 0.21 & 0.4 & -0.47 \\\\ Dentistry & 1.67 & 0.66 & 0.48 & 0.01 \\\\ Dermatology & 2.12 & 0.55 & -0.05 & -0.65 \\\\ Divinity & -0.34 & -0.17 & -0.48 & -0.89 \\\\ Earth science & 0.39 & 0.44 & -0.08 & -0.33 \\\\ Economics & 2.62 & 0.96 & 0.62 & -0.4 \\\\ Education & 2.67 & 0.42 & 0.2 & -0.84 \\\\ Education industry & 2.19 & 0.4 & 0.56 & -1.33 \\\\ Electric power industry & 3.23 & 1.31 & 0.39 & -0.79 \\\\ Electrical Engineering & 3.81 & 1.26 & 1.41 & -0.34 \\\\ Emergency medicine & 2.04 & 0.44 & -0.18 & -0.86 \\\\ Energy industry & 3.59 & 0.98 & 0.54 & -0.22 \\\\ Environmental studies and forestry & 0.12 & 0.41 & 0.1 & -0.45 \\\\ Epidemiology & 3.02 & 0.52 & 0.33 & -0.46 \\\\ European history & 0.14 & 0.62 & 0.15 & -0.18 \\\\ Fashion & 2.5 & 0.66 & 0.47 & -0.53 \\\\ Film & 0.76 & 0.45 & -0.16 & -0.78 \\\\ Film industry & 1.58 & 0.46 & 0.25 & -0.59 \\\\ Fishing industry & 1.67 & 1 & 0.57 & -0.09 \\\\ Floral & 1.92 & 0.89 & 0.58 & -0.09 \\\\ Food industry & 3.64 & 0.12 & 0.14 & -0.42 \\\\ Foreign policy & 2.4 & 0.49 & 0.16 & -0.46 \\\\ Geography & 0.88 & 0.6 & 0.28 & -0.66 \\\\ Geriatrics & 2.19 & -0.32 & -0.56 & -0.71 \\\\ Gynaecology & 1.05 & -0.27 & -0.26 & -0.67 \\\\ Healthcare industry & 1.62 & -0.25 & 0.14 & -0.5 \\\\ Hematology & 0.35 & 0.32 & -0.05 & -0.72 \\\\ History & 0.75 & 0.54 & -0.04 & -0.38 \\\\ Holistic medicine & 0.85 & 0.48 & 0.26 & -0.27 \\\\ Hospitality industry & 2.36 & 0.48 & 0.28 & -0.07 \\\\ Housing & 4.04 & 0.15 & -0.22 & -0.62 \\\\ Industrial robot industry & 3.84 & 1.22 & 0.84 & -0.71 \\\\ Infectious disease & 1.76 & 0.14 & 0.18 & -0.56 \\\\ Insurance industry & 2.67 & 0.42 & 0.61 & -0.4 \\\\ Intensive care medicine & 1.11 & 0.56 & 0.08 & -0.33 \\\\ Internal medicine & 1.02 & 0.45 & -0.01 & -0.42 \\\\ Journalism & 2.77 & -0.13 & -0.21 & -0.69 \\\\ Languages and literature & 0.45 & 0.05 & -0.39 & -0.84 \\\\ Law & 0.42 & 0.39 & 0.04 & -0.49 \\\\ Leisure industry & 1.49 & 0.12 & -0.09 & -0.49 \\\\ Library and museum studies & 1.52 & 0.5 & 0.33 & -0.32 \\\\ \\hline \\end{tabular}\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r} \\hline \\hline\n' +
      '**Discipline** & **Orca-2-7b** & **Mistral-7B-Instruct-v0.1** & **WizardLM-13B-V1.2** & **GPT-4** \\\\ \\hline \\hline Linguistics & 0.39 & 0.38 & -0.12 & -0.96 \\\\ Logic & 2.95 & 1.56 & 1.62 & -0.79 \\\\ Materials Science and Engineering & 1.71 & 0.97 & 0.54 & -0.91 \\\\ Mathematics & 4.69 & 3.81 & 2.73 & -0.61 \\\\ Mechanical Engineering & 2.25 & 1.71 & 1.15 & -0.95 \\\\ Medical toxicology & 0.62 & 0 & 0.11 & -1.01 \\\\ Medicine & 1.49 & 0.93 & 0.36 & -0.37 \\\\ Military sciences & 0.42 & 0.53 & 0.17 & -0.45 \\\\ Mining & 3.17 & 0.32 & 0.41 & -0.61 \\\\ Music & 2.85 & 0.38 & 1.07 & -0.05 \\\\ Music industry & 2.05 & -0.03 & -0.08 & -0.8 \\\\ Nursing & 1.49 & 0.14 & -0.12 & -0.59 \\\\ Nutrition & 1.15 & -0.2 & -0.13 & -0.65 \\\\ Obstetrics & 1.49 & 0.08 & -0.43 & -0.53 \\\\ Ophthalmology & 0.97 & 0.01 & -0.47 & -0.97 \\\\ Otolaryngology & 1.51 & -0.44 & -0.29 & -1.11 \\\\ Pathology & 0.23 & 0.35 & 0.19 & -0.72 \\\\ Pediatrics & 1.62 & 0.55 & -0.34 & -0.47 \\\\ Performing arts & 0.38 & 0.09 & -0.36 & -1.06 \\\\ Petroleum industry & 3.12 & 0.44 & 0.08 & -0.54 \\\\ Pharmaceutical industry & 2.75 & 0.41 & 0.4 & -0.46 \\\\ Pharmaceutical sciences & 0.77 & 0.19 & 0.16 & -0.8 \\\\ Philosophy & 0.51 & 0.25 & 0.49 & -0.64 \\\\ Physics & 3.15 & 2.67 & 2.05 & -0.73 \\\\ Political science & 0.04 & -0.05 & -0.31 & -0.91 \\\\ Prehistory & 0.35 & 0.19 & 0.05 & -0.41 \\\\ Preventive medicine & 2.69 & 0.57 & 0.09 & -0.36 \\\\ Psychiatry & 2.93 & 0.27 & -0.07 & -0.32 \\\\ Psychology & 0.53 & -0.02 & -0.3 & -0.96 \\\\ Public administration & 0.94 & -0.27 & 0.1 & -1.2 \\\\ Public health & 1.21 & 0.07 & 0.22 & -0.56 \\\\ Public policy & 0.78 & -0.06 & -0.28 & -0.92 \\\\ Pulp and paper industry & 1.13 & 0.63 & 0.57 & -0.25 \\\\ Radiology & -0.17 & -0.19 & -0.82 & -0.62 \\\\ Real estate industry & 1.01 & 0.02 & -0.12 & -0.5 \\\\ Religious Studies & 0.38 & 0 & -0.32 & -0.63 \\\\ Retail industry & 1.1 & -0.25 & -0.37 & -0.6 \\\\ Semiconductor industry & 1.49 & 0.64 & 0.71 & -0.42 \\\\ Sexology & 1.81 & -0.44 & -0.37 & -0.96 \\\\ Shipbuilding industry & 1.54 & 0.37 & 0.42 & -0.32 \\\\ Social work & 0.93 & -0.42 & -0.53 & -0.77 \\\\ Sociology & 1.49 & 0.21 & 0.76 & -0.3 \\\\ Steel industry & 0.88 & 0.45 & 0.09 & -0.34 \\\\ Surgery & 0.86 & -0.02 & -0.35 & -0.73 \\\\ Systems science & 1.9 & 0.56 & 0.41 & -0.45 \\\\ Telecommunications industry & 1.81 & 0.4 & 0.39 & -0.27 \\\\ Television & 0.37 & -0.33 & -0.69 & -1 \\\\ Textile industry & 0.82 & -0.26 & -0.68 & -0.59 \\\\ Theatre & 0.31 & -0.27 & -0.34 & -1.07 \\\\ Theology & -0.38 & 0.37 & -0.45 & -0.54 \\\\ Tobacco industry & 0.59 & -0.13 & -0.48 & -0.67 \\\\ Transport industry & 1.19 & -0.33 & -0.36 & -0.56 \\\\ Transportation & 1.74 & 0.26 & 0.17 & -0.74 \\\\ Urology & 0.05 & -0.29 & -0.36 & -0.64 \\\\ Veterinary medicine & -0.14 & 0.36 & -0.31 & -0.62 \\\\ Video game industry & 1.67 & 0.2 & -0.24 & -0.62 \\\\ Visual arts & 0.98 & 0.22 & 0.26 & -0.56 \\\\ Water industry & 0.9 & -0.11 & -0.09 & -0.51 \\\\ Wood industry & 1.36 & 0.5 & 0.31 & -0.25 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: _GLAN-Test_ 상의 126개 분야(또는 도메인)에 걸친 쌍별 비교. 점수는 GPT-4에 의해 할당된 평가 점수에서 GLAN과 다른 모델 \\(x\\) 사이의 평균 갭으로부터 생성되며, avg_score(GLAN) \\(-\\)avg_score(\\(x\\))로 계산된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>