<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#드림매처: 외모매칭 셀프-어텐션\n' +
      '\n' +
      '시맨틱 일관성 있는 텍스트 대 이미지 개인화를 위해\n' +
      '\n' +
      'Jisu Nam\n' +
      '\n' +
      ' 고려대학교 컴퓨터학과\n' +
      '\n' +
      'Heesu Kim\n' +
      '\n' +
      ' 네이버 클라우드\n' +
      '\n' +
      ' 이동재\n' +
      '\n' +
      ' 네이버 클라우드\n' +
      '\n' +
      ' 진세윤\n' +
      '\n' +
      ' 고려대학교 컴퓨터학과\n' +
      '\n' +
      'Seungryong Kim\n' +
      '\n' +
      ' 고려대학교 컴퓨터학과\n' +
      '\n' +
      'Seunggyu Chang\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '텍스트-이미지(Text-to-Image; T2I) 개인화의 목적은 확산 모델을 사용자 제공 참조 개념으로 커스터마이징하여 타겟 프롬프트에 정렬된 개념의 다양한 이미지를 생성하는 것이다. 고유한 텍스트 임베딩을 사용하여 참조 개념을 표현하는 종래의 방법들은 종종 참조의 외관을 정확하게 모방하는데 실패한다. 이를 해결하기 위해, 하나의 솔루션은 키-값 대체(key-value replacement)로 알려진 타겟 잡음 제거 프로세스로 참조 이미지들을 명시적으로 컨디셔닝할 수 있다. 그러나 선행 작업은 사전 학습된 T2I 모델의 구조 경로를 방해하기 때문에 로컬 편집에 제약이 있다. 이를 극복하기 위해 T2I 개인화를 시맨틱 매칭으로 재구성하는 새로운 플러그인 방법인 드림매처(DreamMatcher)를 제안한다. 구체적으로, 드림매처는 다양한 구조를 생성하기 위해 미리 훈련된 T2I 모델의 다용도 능력을 보존하기 위해 구조 경로를 변경하지 않고 시맨틱 매칭에 의해 정렬된 기준 값으로 목표 값을 대체한다. 또한 목표 프롬프트에 의해 소개된 관련 없는 영역에서 개인화된 개념을 분리하기 위한 의미 일관성 마스킹 전략을 소개한다. 기존 T2I 모델과 호환되는 드림매처는 복잡한 시나리오에서 상당한 개선을 보여줍니다. 집중적인 분석은 우리의 접근법의 효과를 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'T2I(Text-to-Image) 개인화[17, 32, 44]의 목적은 사용자가 제공한 피사체 이미지를 기반으로 T2I 확산 모델을 맞춤화하는 것이다. 몇 개의 참조 이미지가 주어지면 대상 프롬프트에 의해 안내되는 다양한 장면, 포즈 및 시점에 걸쳐 피사체의 새로운 렌디션을 생성할 수 있다.\n' +
      '\n' +
      'T2I 개인화를 위한 종래의 접근법들[14, 17, 21, 32, 44, 62]은 종종 텍스트 임베딩 자체 또는 확산 모델의 파라미터들 중 하나를 최적화함으로써, 고유한 텍스트 임베딩들[42]을 사용하여 주제들을 나타낸다. 그러나 그림 1과 같이 색상, 질감, 모양 등 피사체의 모습을 정확하게 모방하지 못하는 경우가 많다. 이는 텍스트 임베딩이 피사체의 시각적 외형을 표현하기에 충분한 공간적 표현성이 부족하기 때문이다[22, 42]. 이를 극복하기 위해 최근 연구[8, 10, 18, 29, 34, 48, 53, 63, 65]는 대규모 데이터셋으로 T2I 모델을 학습함으로써 표현력을 향상시키지만, 학습을 위해서는 방대한 텍스트 이미지 쌍이 필요하다.\n' +
      '\n' +
      '전술한 과제들을 해결하기 위해, 하나의 솔루션은 기준 이미지들을 타겟 잡음 제거 프로세스 내로 명시적으로 컨디셔닝할 수 있다. 최근의 주제 중심 이미지 편집 기술[4, 9, 11, 28, 31, 37]은 종종 키-값 대체라고 불리는 잡음 제거 U-Net의 자기-어텐션 모듈을 통해 참조 이미지를 컨디셔닝하는 것을 제안한다. 자기 주목 모듈[25]에서, 선행 층으로부터의 이미지 특징들은 쿼리들, 키들, 및 값들로 투영된다. 그런 다음 주의 작업에 의해 자체 집계됩니다[61]. 이 메커니즘을 활용하여, 이전의 이미지 편집 방법들[4, 37]은 기준 이미지를 타겟 합성 프로세스로 조정하기 위해 타겟으로부터의 키들 및 값들을 기준으로부터의 키들 및 값들로 대체한다. [1, 24, 55, 60]에서 언급한 바와 같이, 우리는 자기 주의 모듈을 T2I 개인화를 위한 서로 다른 역할을 갖는 두 개의 별개의 경로로 분석한다: 질의-키 유사성은 _structure_ 경로를 형성하고, 생성된 이미지의 레이아웃을 결정하는 반면, 값들은 _appearance_ 경로를 형성하고, 이미지 레이아웃에 공간적인 외관을 주입한다.\n' +
      '\n' +
      '그림 2에서 알 수 있듯이, 우리의 키 관찰은 자기 주의 모듈에서 목표 키를 참조 키로 교체하는 것이 미리 훈련된 T2I 모델의 구조 경로를 방해한다는 것이다. 구체적으로, 쿼리 포인트에 대한 최적의 키 포인트는 교체된 참조 키들에서 이용가능하지 않을 수 있으며, 이는 구조 경로 상의 타겟 쿼리들과 참조 키들 간의 차선의 매칭으로 이어진다. 결과적으로, 참조 외관은 이 불완전한 대응관계에 기초하여 적용된다. 이러한 이유로, 키 및 값 대체를 통합하는 이전의 방법들은 종종 큰 구조적 차이들을 갖는 개인화된 이미지들을 생성하는 데 실패하여, 로컬 편집에 제한된다. 이를 해결하기 위해 ViCo[22]는 키 및 값 대체와 결합된 모델 가중치의 하위 집합의 튜닝을 통합한다. 그러나, 이 접근법은 실제 사용 전에 뚜렷한 튜닝 프로세스를 필요로 한다.\n' +
      '\n' +
      '본 논문에서는 다양한 구조를 생성하면서 참조 외형을 효과적으로 전달하는 플러그인 기법인 드림매처를 제안한다. 드림매처는 개인화를 위해 자기 주의 모듈 내의 출현 경로에 집중하는 반면, 구조 경로는 변경되지 않는다. 그러나, 타겟으로부터의 값들을 참조로부터의 값들로 간단히 교체하는 것은 구조-외관 오정렬을 초래할 수 있다. 이를 해결하기 위해, 우리는 기준 외형을 목표 구조 쪽으로 정렬하기 위해 의미적 대응을 활용하는 매칭 인식 값 주입을 제안한다. 또한, 객체의 폐색 또는 배경 변동과 같은 타겟의 다른 구조적 요소를 보존하기 위해 매칭된 참조 외관만을 격리하는 것이 필수적이다. 이를 위해 의미적으로 일관된 참조 모양을 대상 구조에 선택적으로 통합할 수 있도록 하는 의미 일관성 마스킹 전략을 소개한다. 결합하면, 각각의 시간 단계에서 자기 주의 모듈을 통해 정확하게 정렬된 기준 외관만이 타겟 구조물에 통합된다. 그러나 초기 확산 시간 단계에서 추정된 참조 외관은 세립 피사체 세부 정보가 부족할 수 있다. 이를 극복하기 위해, 우리는 타겟 잡음 제거 프로세스 중간에 풍부한 참조 모양을 제공하기 위해 시맨틱 매칭 안내라고 명명된 샘플링 안내 기법을 소개한다.\n' +
      '\n' +
      '드림매처는 훈련이나 미세 조정 없이 기존 T2I 개인화 모델과 호환됩니다. 우리는 세 가지 다른 기준선에서 우리의 방법의 효과를 보여준다[17, 32, 44]. 드림매처는 기존의 튜닝 프리 플러그인 방식[4, 49, 68]과 학습 가능한 방식[22]에 비해 최첨단 성능을 달성한다. 그림 1과 같이 드림매처는 극단적인 비강성 개인화 시나리오에서도 효과적이다. 우리는 개인화에 도전하는 시나리오에서 본 방법의 견고성을 추가로 검증한다. 절제 연구는 설계 선택을 확인하고 각 구성 요소의 효과를 강조한다.\n' +
      '\n' +
      '도 2: **DreamMatcher의 직관:**(a) 참조 이미지, (b) 키-값 대체에 의한 목표 구조 경로 파괴 [4, 9, 11, 28, 31, 37], (c) (b)에 의한 생성 이미지, (d) 미리 훈련된 T2I 모델에서의 목표 구조 경로 [44], 및 (e) DreamMatcher에 의한 생성 이미지. 시각화를 위해 구조 경로에 주성분 분석(PCA)[41]을 적용한다. 키값 교체는 목표 구조를 방해하여 차선의 개인화된 결과를 가져오는 반면, 드림매처는 목표 구조를 보존하여 목표 프롬프트와 정렬된 고충실도 피사체 이미지를 생성한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**최적화 기반 T2I 개인화.** 소수의 이미지가 주어지면, T2I 개인화는 목표 프롬프트와 일치하는 주어진 개념의 새로운 이미지 변형을 생성하는 것을 목표로 한다. 초기 확산 기반 기술[14, 17, 21, 32, 44, 62]은 일반적으로 특정 토큰으로 표현되는 텍스트 도메인 내에서 주어진 개념을 캡슐화한다. 텍스트 반전[17]은 텍스트 임베딩을 최적화하고 토큰을 타겟 프롬프트와 통합하여 개인화된 이미지를 합성한다. DreamBooth[44]는 특정 토큰과 주제의 클래스 카테고리에 기초하여 디노이징 U-Net의 모든 파라미터를 최적화하는 것을 제안한다. 여러 작업[7, 21, 22, 32, 35, 45, 55, 64]은 효율적인 최적화 및 더 나은 컨디셔닝을 위해 가중치 하위 집합 또는 추가 어댑터를 최적화하는 데 중점을 둔다. 예를 들어, CustomDiffusion[32]는 U-Net 내의 크로스 어텐션 계층만을 미세 조정하는 반면, ViCo[22]는 추가적인 이미지 인코더를 최적화한다. 유망한 결과에도 불구하고, 앞서 언급한 접근법들은 종종 피험자의 외관을 정확하게 모방하지 못한다.\n' +
      '\n' +
      '**훈련 기반 T2I 개인화** 여러 연구[8, 10, 18, 29, 34, 48, 53, 63, 65]는 텍스트-이미지 쌍이 큰 T2I 개인 맞춤형 모델을 훈련하는 쪽으로 초점을 옮겼다. 예를 들어, Taming Encoder[29], Instant-Booth[48], FastComposer[65]는 영상 인코더를 학습시키고, SuTI[10]은 별도의 네트워크를 학습시킨다. 이러한 접근 방식은 미세 조정 문제를 우회하지만 대규모 데이터 세트를 사용하여 광범위한 사전 교육을 필요로 한다.\n' +
      '\n' +
      '**Plug-in Subject-driven T2I Synthesis.** 최근 연구 [4, 20, 37, 47, 49, 68]은 추가적인 미세 조정 또는 훈련이 필요 없이 주제 중심 T2I 개인화 또는 비강성 편집을 달성하는 것을 목표로 한다. 구체적으로, MasaCtrl[4]는 이중-분기 사전 훈련된 확산 모델들을 평균하여 기준 분기로부터의 이미지 특징들을 타겟 분기에 통합한다. FreeU[49]는 주파수 분석에 기초하여 미리 훈련된 개인화된 모델[44]로부터 중간 특징 맵들을 재가중화하는 것을 제안한다. 매직퓨전[68]은 사전 학습된 확산 모델과 T2I 개인화 모델 간의 노이즈 블렌딩 방법을 소개한다[44]. 드림매처는 이러한 방법과 정렬되어 있으며, 기성품 T2I 개인화 모델과 호환되도록 설계되어 추가 미세 조정 또는 훈련을 제거합니다.\n' +
      '\n' +
      '## 3 Preliminary\n' +
      '\n' +
      '### 잠재 확산 모델\n' +
      '\n' +
      '확산 모델 [25, 50]은 점진적 잡음 제거 과정을 통해 가우시안 잡음으로부터 원하는 데이터 샘플을 생성한다. 잠재 확산 모델[43]은 RGB 공간 대신에 오토인코더에 의해 투영된 잠재 공간에서 이러한 프로세스를 수행한다. 구체적으로, 인코더는 RGB 이미지 \\(x_{0}\\)을 잠재 변수 \\(z_{0}\\)으로 매핑한 후 다시 \\(x_{0}\\)으로 재구성한다. 정방향 확산 과정에서 각 시간 단계 \\(t\\)에서 잠재 \\(z_{t}\\)에 가우시안 잡음을 점진적으로 추가하여 잡음 잠재 \\(z_{t+1}\\)을 생성한다. 역확산 과정에서 신경망 \\(\\epsilon_{\\theta}(z_{t},t)\\)은 \\(z_{t}\\)을 데노이즈하여 \\(z_{t-1}\\)을 생성한다. 반복적으로 \\(z_{t-1}\\)을 샘플링함으로써, 가우시안 잡음 \\(z_{T}\\)은 잠재 \\(z_{0}\\)으로 변환된다. 디코더를 이용하여 잡음 제거된 \\(z_{0}\\)을 \\(x_{0}\\)으로 다시 변환한다. 조건(예: 텍스트 프롬프트 \\(P\\)이 추가되면 \\(\\epsilon_{\\theta}(z_{t},t,P)\\)은 텍스트 설명과 일치하는 래턴트를 생성한다.\n' +
      '\n' +
      '확산모델에서### 자기주목\n' +
      '\n' +
      '확산 모델은 종종 잔차 블록, 교차 주의 모듈 및 자기 주의 모듈을 포함하는 U-Net 아키텍처에 기초한다[25, 43, 50]. 잔차 블록은 선행 층들로부터의 특징들을 처리하고, 교차-어텐션 모듈은 이러한 특징들을 조건, 예를 들어 텍스트 프롬프트와 통합하며, 자기-어텐션 모듈은 어텐션 동작을 통해 이미지 특징들 자체를 집계한다.\n' +
      '\n' +
      '구체적으로, 셀프 어텐션 모듈은 시간 단계 \\(t\\)의 이미지 특징을 쿼리 \\(Q_{t}\\), 키 \\(K_{t}\\) 및 값으로 투영한다.\n' +
      '\n' +
      '그림 3: **전체 아키텍처: 기준 이미지\\(I^{X}\\)이 주어지면, 외관 매칭 셀프 어텐션(AMA)은 미리 훈련된 개인화된 모델 \\(\\epsilon_{\\theta}\\)의 셀프 어텐션 모듈에서 기준 외관을 고정 타겟 구조에 정렬한다. 이는 참조에서 목표까지의 신뢰할 수 있는 시맨틱 매칭을 설명적으로 활용함으로써 달성된다. 또한, 의미적 매칭 안내는 생성된 영상에서 피사체의 세부 정보를 향상시킨다.**\\(V_{t}\\). 이 모듈로부터의 결과적인 출력은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\mathrm{SA}(Q_{t},K_{t},V_{t})=\\mathrm{Softmax}\\left(\\frac{Q_{t}K_{t}^{T}{\\sqrt{d}}\\right)V_{t}.\\tag{1}\\t}\n' +
      '\n' +
      '여기서, 각 질의에 대한 키 위에 \\(\\mathrm{Softmax}(\\cdot)\\)을 적용한다. \\ (Q_{t}\\in\\mathbb{R}^{h\\times w\\times d}\\), \\(K_{t}\\in\\mathbb{R}^{h\\times w\\times d}\\) 및 \\(V_{t}\\in\\mathbb{R}^{h\\times w\\times d}\\)은 투영된 행렬이며, 여기서 \\(h\\), \\(w\\) 및 \\(d\\)은 각각 높이, 너비 및 채널 차원을 나타낸다. [1, 24, 55, 60]에서 분석한 바와 같이, 우리는 자기 주의 모듈을 _structure_ 경로와 _appearance_ 경로의 두 가지 별개의 경로로 본다. 보다 구체적으로, 구조 경로는 이미지 요소의 공간 배치를 제어하는 유사성 \\(\\mathrm{Softmax}(Q_{t}K_{t}^{T}/\\sqrt{d})\\에 의해 정의된다. 값 \\(V_{t}\\)은 색상, 질감, 모양과 같은 시각적 속성을 이미지 내의 각 해당 요소에 주입하여 출현 경로를 구성한다.\n' +
      '\n' +
      '## 4 Method\n' +
      '\n' +
      '기존의 방법[17, 32, 44]은 대상체(\\(\\langle S^{*}\\rangle\\)에 대한 특정 텍스트 프롬프트와 함께 T2I 모델\\(\\epsilon_{\\theta}(\\cdot)\\)을 개인화 한다. 추론적으로, \\(\\epsilon_{\\theta}(\\cdot)\\)는 정글에서 목표 프롬프트(예를 들어, \\(A\\)\\(\\langle S^{*}\\rangle\\)_)에 의해 정렬된 피사체와 반복적 잡음 제거 과정을 통해 임의의 잡음으로부터 새로운 장면을 생성할 수 있다. 그러나, 텍스트 임베딩은 피사체의 시각적 속성을 표현하기 위한 공간 표현성이 부족하기 때문에 피사체 외관을 정확하게 모방하지 못하는 경우가 많다[22, 42]. 본 논문에서는 기준 이미지(\\(\\mathcal{X}\\)와 목표 텍스트 프롬프트(P\\)를 이용하여 개인화된 이미지(I^{Y}\\)에서 피사체 외형을 향상시키는 동시에 프롬프트(P\\)에 의해 지시되는 세부 목표 구조를 보존하는 것을 목표로 한다. DreamMatcher는 참조-타겟 듀얼-브랜치 프레임워크를 포함한다. \\ (I^{X}\\)는 DDIM 반전[50]을 통해 \\(z_{T}^{X}\\)으로 반전된 후 \\(\\hat{I}^{X}\\)으로 복원되는 반면, \\(P\\)에 의해 유도된 랜덤 가우시안 잡음 \\(z_{T}^{Y}\\)으로부터 \\(I^{Y}\\)이 생성된다. 각 시간 단계에서 기준 분기로부터의 self-attention 모듈은 이미지 특징을 질의(Q_{t}^{X}\\), 질의(K_{t}^{X}\\), 질의(V_{t}^{X}\\)로 투영하고, 대상 분기에서는 질의(Q_{t}^{Y}\\), 질의(K_{t}^{Y}\\), 질의(V_{t}^{Y}\\)로 투영한다. 그리고 레퍼런스 외관\\(V_{t}^{X}\\)을 자기 주의 모듈을 통해 타겟 노이즈 제거 U-Net으로 전송한다. 드림매처의 전체적인 아키텍처는 그림 3에 예시되어 있다.\n' +
      '\n' +
      '# 외모매칭 셀프-어텐션\n' +
      '\n' +
      '도 4에 예시된 바와 같이, 우리는 타겟 프롬프트-지향 레이아웃을 보존하면서 피사체 표현성을 향상시키기 위해, 미리 트레이닝된 타겟 구조 경로를 유지하면서 외관 경로만을 조작하는 외관 매칭 자기-집중(AMA)을 제안한다.\n' +
      '\n' +
      '그러나, 타겟 외관 \\(V_{t}^{Y}\\)을 기준 \\(V_{t}^{X}\\)과 순순히 스와핑하는 것은 구조-외관 오정렬을 초래하며, 이는 수학식 1을 다음과 같이 재구성한다:\n' +
      '\n' +
      '\\[\\mathrm{SA}(Q_{t}^{Y},K_{t}^{Y},V_{t}^{X})=\\mathrm{Softmax}\\left(\\frac{Q_{t}^{Y}(K_{t}^{Y}}(K_{t}^{Y}}}}\\sqrt{d}}\\right)V_{t}^{X}.\\tag{2}\\t}\n' +
      '\n' +
      '이를 해결하기 위해 본 논문에서는 기준 외형\\(V_{t}^{X}\\)을 고정 목표 구조\\(\\mathrm{Softmax}(Q_{t}^{Y}(K_{t}^{Y})^{T}/\\sqrt{d})\\과 정확하게 정렬하기 위해 의미적 매칭을 활용하는 매칭 인식 값 주입 방법을 제안한다. 구체적으로, AMA는 두 영상에서 의미적으로 동일한 위치 사이의 밀집 변위 필드[12, 38, 56, 57, 59]인 기준으로부터 목표까지의 추정된 의미 대응성 \\(F_{t}^{X}\\)에 의해 기준 값 \\(V_{t}^{X}\\)을 왜곡한다. 휨된 기준 값 \\(V_{t}^{X\\to Y}\\)은 다음과 같이 공식화된다.\n' +
      '\n' +
      '\\[V_{t}^{X\\to Y}=\\mathcal{W}(V_{t}^{X};F_{t}^{X\\to Y}), \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\mathcal{W}\\)는 워핑 동작[58]을 나타낸다.\n' +
      '\n' +
      '또한 일치하는 참조 모양만 분리하고 특이치를 필터링하는 것이 중요하다. 이는 전형적인 개인화 시나리오들이 도 1에 도시된 바와 같이, 참조 이미지들에 존재하지 않는 폐색들, 상이한 시점들 또는 배경 변화들을 종종 수반하기 때문이다. 이를 달성하기 위해, 이전의 방법들 [4, 22]은 전경 마스크 \\(M_{t}\\)를 사용하여 단지 피사체 전경에 초점을 맞추고 배경 변화들을 처리한다. \\ (M_{t}\\)는 피험자 텍스트 프롬프트(예를 들어, \\(\\langle S^{*}\\rangle\\))에 대한 평균 교차-어텐션 맵으로부터 얻어진다. 이러한 고려사항들을 가지고, 수학식 3은 다음과 같이 재구성될 수 있다:\n' +
      '\n' +
      '\\[V_{t}^{W}=V_{t}^{X\\to Y}\\odot M_{t}+V_{t}^{Y}\\odot(1-M_{t}), \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(\\odot\\)은 하다마드 생성물[27]을 나타낸다.\n' +
      '\n' +
      '그런 다음 AMA는 자기 주의 모듈을 통해 고정된 목표 구조물 경로에 \\(V_{t}^{W}\\)을 삽입한다. 수학식 2는 다음과 같이 재구성된다:\n' +
      '\n' +
      '\\[\\mathrm{AMA}(Q_{t}^{Y},K_{t}^{Y},V_{t}^{W})=\\mathrm{Softmax}\\left(\\frac{Q_{t}^{Y}(K_{t}^{Y}}}(K_{t}^{Y}}}\\right)V_{t}^{W}.\\tag{5}\\w}\n' +
      '\n' +
      '프레임워크에서 기준 및 목표 간의 의미 일치성을 찾아 표준 의미 일치와 일치시킨다.\n' +
      '\n' +
      '도 4: ** (a) 키-값 대체 및 (b) 외관 매칭 셀프-어텐션(AMA):** AMA는 명시적 의미 매칭 및 일관성 모델링을 통해 고정된 타겟 구조 경로를 향해 기준 외관 경로를 정렬한다.\n' +
      '\n' +
      ' 일치 워크플로우[12, 13, 26, 38, 56, 57]입니다. 그림 5는 제안된 매칭 프로세스의 상세한 개략도를 제공한다. 아래에서, 우리는 그 과정을 상세하게 설명할 것이다.\n' +
      '\n' +
      '**Feature Extraction.** 클래식 매칭 파이프라인 [12, 13, 26, 38, 56, 57]은 이미지 쌍 \\(I^{X}\\) 및 \\(I^{Y}\\)으로부터 특징 기술자 \\(\\psi^{X}\\) 및 \\(I^{Y}\\)을 얻기 위해 미리 훈련된 특징 추출기 [6, 23, 40]를 포함한다. 그러나 역확산 과정에서 추정된 표적 영상의 잡음 특성으로 인해 T2I 개인화에 맞는 좋은 특징을 찾는 것은 사소한 일이 아니며, 기존의 특징 추출기의 추가적인 미세 조정이 필요하다. 이를 해결하기 위해 사전 학습된 T2I 모델 자체에 확산 특징 공간[54, 67]을 집중하여 T2I 개인화에 맞춘 시맨틱 매칭을 찾는다.\n' +
      '\n' +
      '\\(\\epsilon_{\\theta,l}(\\cdot,t+1)\\)은 시간 단계 \\(t+1\\)에서 디노이징 U-Net [25]\\(\\epsilon_{\\theta}\\)의 \\(l\\)번째 디코더 층의 출력을 나타낸다. 시간 스텝 \\(t+1\\)과 텍스트 프롬프트 \\(P\\)을 입력으로 하는 잠재 \\(z_{t+1}\\)을 주어 U-Net 디코더의 \\(l\\)번째 계층에서 특징 기술자 \\(\\psi_{t+1,l}\\)을 추출한다. 프로세스는 다음과 같이 주어진다:\n' +
      '\n' +
      '\\[\\psi_{t+1,l}=\\epsilon_{\\theta,l}(z_{t+1},\\,t+1,\\,P), \\tag{6}\\]\n' +
      '\n' +
      '여기서 \\(z^{X}_{t+1,l}\\)과 \\(z^{Y}_{t+1,l}\\)으로부터 \\(\\psi^{X}_{t+1,l}\\)과 \\(z^{Y}_{t+1,l}\\)을 각각 구한다. 간결함을 위해, 우리는 다음 논의에서 \\(l\\)을 생략할 것이다.\n' +
      '\n' +
      '도 6은 참조와 표적 사이의 확산 특징 공간 내의 의미적 관계를 탐색하기 위해 주성분 분석(PCA)을 사용하여 서로 다른 시간 단계에서 \\(\\psi^{X}_{t+1}\\)과 \\(\\psi^{Y}_{t+1}\\)의 관계를 시각화한다[41]. 전경 피사체는 미리 훈련된 개인화된 모델의 대상 이미지가 피사체 표현력이 부족한 경우가 많기 때문에 서로 다른 외형을 가지더라도 유사한 의미론을 공유한다는 것을 관찰한다. 이 관찰은 샘플링 단계의 각 시간 단계에서 추정된 참조와 표적 사이의 의미적 일치를 설정하기 위해 내부 확산 특징을 활용하도록 영감을 준다.\n' +
      '\n' +
      '이를 바탕으로 채널 연접을 이용하여 서로 다른 계층에서 PCA 특징을 결합하여 \\(\\psi_{t+1}\\in\\mathbb{R}^{H\\times W\\times D}\\)을 유도하는데, 여기서 \\(D\\)은 연접 채널 차원이다. 특징 추출에 대한 상세한 분석 및 구현은 E.1에 제공된다.\n' +
      '\n' +
      '**Flow Computation.** 종래의 방법 [12, 26, 56, 57, 59]에 이어서, 참조 이미지와 타겟 이미지 모두에 대한 특징 기술자들 간의 쌍별 코사인 유사도를 계산함으로써 매칭 비용을 구축한다. 시간 단계 \\(t+1\\)에서 주어진 \\(\\psi^{X}_{t+1}\\)과 \\(\\psi^{Y}_{t+1}\\)에 대해, 특징 기술자의 모든 위치 사이의 내적을 취하여 정합 비용 \\(C_{t+1}\\)을 계산한다. 이것은 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\frac{\\psi^{X}_{t+1}(i,j)=\\cdot\\psi^{Y}_{t+1}(j)}{\\|\\psi^{X}_{t+1}(i)\\|\\|\\psi^{Y}_{t+1}(j)\\|}, \\tag{7}\\|}\n' +
      '\n' +
      '여기서 \\(i,j\\in[0,H)\\times[0,W)\\), \\(\\\\cdot\\|\\)는 \\(l\\)-2 정규화를 나타낸다.\n' +
      '\n' +
      '이어서, 정합비용 \\(C_{t+1}\\)에 대한 argmax 연산 [12]를 이용하여 시간단계 \\(t\\)에서 목표물에 대한 조밀한 변위장을 유도하였다. 그림 7(c)는 생성 과정 중간에 \\(\\psi^{X}_{t+1}\\)와 \\(\\psi^{Y}_{t+1}\\) 사이의 예측 대응관계 \\(F^{X\\to Y}_{t}\\)을 이용하여 얻어진 와핑된 참조 영상을 보여준다. 이는 큰 변위, 폐색 및 새로운 시점 합성을 포함하는 복잡한 비강성 표적 컨텍스트에서도 역확산 과정에서 대응 관계가 안정적으로 확립됨을 보여준다.\n' +
      '\n' +
      '### Consistency Modeling\n' +
      '\n' +
      '도 7(d)에 도시된 바와 같이, 포그라운드 마스크 \\(M_{t}\\)는 교차-어텐션 모듈 내에서 구별하기 어렵기 때문에 폐색 및 배경 클러터(예: 요리사 의상_ 또는 꽃다발_)를 처리하기에 불충분하다.\n' +
      '\n' +
      '이를 보완하기 위해 신뢰 마스크 \\(U_{t}\\)를 도입하여 잘못된 대응 관계를 폐기함으로써 세부적인 목표 구조를 보존한다. 구체적으로, 우리는 주기 일관성 제약[30]을 시행하며, 단순히 우리가 설정한 임계값보다 큰 대응 관계를 거부한다. 즉,\n' +
      '\n' +
      '그림 5: **의미적 매칭 및 일관성 모델링:** 각 시간 단계에서 내부 확산 특징을 활용하여 참조와 대상 간의 의미적 매칭 \\(F^{X\\to Y}_{t}\\)을 찾는다. 또한, 사이클-일관성을 통해 예측된 일치도(U_{t}\\)의 신뢰도 맵을 계산한다.\n' +
      '\n' +
      '그림 6: **확산 특징 시각화:** 상단은 기준 및 표적의 중간 추정 이미지를 표시하며, 드림부스[44]에서 생성된 표적은 해변_에 프롬프트 _A\\(\\langle S^{*}\\rangle\\)을 사용하여 프롬프트 _A\\(\\langle S^{*}\\rangle\\)을 사용한다. 중간 확산 피쳐의 세 가지 주요 구성 요소를 더 낮게 시각화합니다. 유사한 의미론은 유사한 색상을 공유합니다.\n' +
      '\n' +
      '우리는 \\(F_{t}^{Y\\to X}\\)에 의해 얻어진 일치된 기준 위치가 \\(F_{t}^{X\\to Y}\\)을 사용하여 재왜곡될 때 목표 위치 \\(x\\)이 일관성을 유지하는 대응관계만을 수용한다. 우리는 목표 전경 영역에 비례하는 임계값을 경험적으로 설정했다. 이것은 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\begin{cases}1,&\\text{if}\\left\\|\\mathcal{W}\\left(F_{t}^{Y\\to X};F_{t}^{X\\to Y}\\right)(x)\\right\\|<\\gamma\\lambda_{c},\\\\0,&\\text{otherwise}.\\end{cases}\\tag{8}\\text{if}\\left(F_{t}^{Y\\to X};F_{t}^{X\\to Y}\\right)(x)\\right\\|<\\gamma\\lambda_{c},\\\\0,&\\text{otherwise}.\\end{cases}\\tag{8}\\text{if}\\left(F_{t}^{Y\\to X};F_{t}^{X\\to Y}\\right)(x)\\right\\|<\\gamma\\lambda_{c},\\\\0,&\\text{otherwise}.\n' +
      '\n' +
      '여기서 \\(\\left\\|\\cdot\\right\\|\\)은 \\(l\\)-2 norm을 나타내고, \\(\\mathcal{W}\\)은 워핑 동작을 나타낸다[58]. (F_{t}^{Y\\to X}\\)는 정방향 대응물의 역방향 유동장을 나타낸다. \\(F_{t}^{X\\to Y}\\) (\\gamma\\)는 전경 영역에 비례하도록 설계된 스케일링 팩터이고, \\(\\lambda_{c}\\)는 하이퍼파라미터이다. 자세한 내용은 부록 E.2에서 확인할 수 있습니다.\n' +
      '\n' +
      '마지막으로 Hadamard 곱[27]을 이용하여 \\(M_{t}\\)과 \\(U_{t}\\을 결합하여 의미 일치 마스크 \\(M_{t}^{\\prime}\\)을 정의하여, \\(M_{t}\\)이 전경 피사체를 거칠게 포착하는 반면, \\(U_{t}\\)은 신뢰할 수 없는 정합을 세밀하게 걸러내고 세밀한 타겟 컨텍스트를 보존한다. 그림 7(e)에서 볼 수 있듯이 우리 네트워크는 복잡한 비강성 시나리오를 효과적으로 해결하면서 자신 있는 일치만 선택적으로 통합한다.\n' +
      '\n' +
      '우리는 이제 식 4에서 자기 주의와 일치하는 외모에 대해 \\(M_{t}\\)을 \\(M_{t}^{\\prime}\\)으로 대체함으로써 신뢰 인식 수정을 적용한다.\n' +
      '\n' +
      '### 시맨틱 매칭 안내\n' +
      '\n' +
      '이 방법은 각 시간 단계에서 중간 기준 값 \\(V_{t}^{X}\\)을 사용한다. 그러나 초기 시간 단계에서 이러한 노이즈 값은 세밀한 주제 세부 정보가 부족하여 최적이 아닌 결과를 초래할 수 있음을 관찰한다. 이를 극복하기 위해, 우리는 타겟 잡음 제거 프로세스 중간에 풍부한 참조 의미론을 제공하기 위해 시맨틱 매칭 안내라고 명명된 샘플링 안내 기법을 추가로 소개한다.\n' +
      '\n' +
      '점수 기반 생성 모델 [51, 52]의 관점에서, 유도 함수 \\(g\\)는 목표 이미지를 더 높은 우도로 향하게 한다. 시간 단계 \\(t\\)에서 업데이트된 방향 \\(\\hat{\\epsilon}_{t}\\)은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\hat{\\epsilon}_{t}=\\epsilon_{\\theta}(z_{t},t,P)-\\lambda_{g}\\sigma_{t}\\nabla_{z_{t}}g(z_{t},t,P), \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(\\lambda_{g}\\)는 유도 강도를 변조하는 하이퍼파라미터이고, \\(\\sigma_{t}\\)는 시간 스텝 \\(t\\)에서의 잡음 스케줄 파라미터를 나타낸다.\n' +
      '\n' +
      '본 논문에서는 DDIM 역산(50)의 \\(z_{0}^{X}\\)을 이용하여 유도함수 \\(g\\)를 설계하였으며, 이는 최종 역산 단계 \\(t=0\\)에서 세부 주제 표현을 캡슐화한다. 각 시간 단계 \\(t\\)에서, \\(z_{0}^{X}\\)는 다음과 같이 \\(F_{t}^{X\\to Y}\\)을 통해 타겟 구조와 정렬되도록 변환된다:\n' +
      '\n' +
      '\\[z_{0,t}^{X\\to Y}=\\mathcal{W}(z_{0}^{X};F_{t}^{X\\to Y}). \\tag{10}\\}\n' +
      '\n' +
      '그 다음 시간단계에서의 유도함수 \\(g_{t}\\)는 의미일치 마스크 \\(M_{t}^{\\prime}\\)을 고려하여 수정된 \\(z_{0,t}^{X\\to Y}\\)와 목표 잠재 \\(\\hat{z}_{0,t}^{Y}\\)의 픽셀별 차이로 정의된다.\n' +
      '\n' +
      '\\[g_{t}=\\frac{1}{|M_{t}^{\\prime}|}\\sum_{i\\in M_{t}^{\\prime}}\\left\\|z_{0,t}^{X\\to Y}(i)-\\hat{z}_{0,t}^{Y}(i)\\right\\|, \\tag{11}\\\\t.\n' +
      '\n' +
      '여기서 \\(\\|\\cdot\\|\\)은 \\(l\\)-2 규범을 나타낸다.\n' +
      '\n' +
      '우리의 접근법은 전경들 사이의 평균 특징 차이를 계산함으로써 거친 외관 안내를 제공하는 기존의 방법들[2, 16, 37]과 다르다는 점에 유의한다. 대신에, 우리는 더 정확하고 픽셀 단위의 제어를 제공하기 위해 신뢰 인식 시맨틱 대응을 활용한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '### Experimental Settings\n' +
      '\n' +
      '**Dataset.** ViCo[22]는 16개의 개념과 31개의 프롬프트를 포함하는 이전 작업 [17, 32, 44]에서 이미지 프롬프트 데이터 세트를 수집했다. 우리는 ViCo 데이터 세트 및 평가 설정을 준수하여 총 3,969개의 이미지에 대해 개념 및 프롬프트당 8개의 샘플을 테스트했다. 복잡한 비강성 개인화 시나리오에서 방법의 견고성을 추가로 평가하기 위해 큰 변위, 폐색 및 새로운 뷰 합성의 세 가지 범주로 분할된 프롬프트 데이터 세트를 생성했다. 이 데이터 세트에는 큰 변위 및 폐색에 대한 10개의 프롬프트와 새로운 뷰 합성에 대한 4개가 포함되어 있으며 모두 ChatGPT[39]를 사용하여 생성된다. 자세한 절차와 프롬프트 목록은 부록 B에 나와 있습니다.\n' +
      '\n' +
      '**기준 및 비교.** 드림매처는 모든 T2I 개인 맞춤형 모델과 호환되도록 설계되었습니다. 제안된 방법은 Textual Inversion[17], DreamBooth[44], CustomDiffusion[32]의 세 가지 기준선을 사용하여 구현하였다. 우리는 이전의 튜닝 프리 플러그인 모델인 FreeU[49] 및 MagicFusion[68]과 최적화 기반 모델인 ViCo[22]에 대해 드림매처를 벤치마킹했다. 안정성에 드림매처를 포함한 추가 실험에 유의하십시오.\n' +
      '\n' +
      '도 7: **대응 시각화: (a) 참조 이미지. (b) 역방향 확산 프로세스의 50%에서 DreamBooth[44]로부터 추정된 타겟 이미지를 추정하는 단계. (c) 예측된 대응 관계 \\(F_{t}^{X\\to Y}\\)에 기초한 워핑된 참조 이미지. (d) 전경 마스크 \\(M_{t}\\)와 결합된 워핑된 참조 이미지. (e) \\(M_{t}\\) 및 신뢰 마스크 \\(U_{t}\\) 모두 조합된 워핑된 참조 이미지.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '복잡한 현실 세계 개인화 응용 프로그램\n' +
      '\n' +
      '**최적화 기반 모델과 비교.** 51.3M 매개변수로 이미지 어댑터를 미세 조정하는 최적화 기반 모델 ViCo[22]에 대해 드림매처를 추가로 평가했다. 균형 잡힌 비교를 위해 ViCo와 CustomDiffusion이 결합된 DreamMatcher[32]를 비교했으며, 유사한 수의 훈련 가능한 매개변수(57.1M)로 구성되었다. 표 4는 드림매처가 추가 미세 조정을 요구하지 않으면서 모든 주제 충실도 메트릭에서 ViCo를 현저하게 능가함을 보여준다. 그림 9는 질적 비교를 제공한다. 부록 F.2에 더 많은 결과가 나와 있다.\n' +
      '\n' +
      '**사용자 연구.** 또한 그림 10에서 사용자 연구 결과를 제시하며, 여기서 드림매처는 주제 및 신속한 충실도 모두에서 다른 모든 방법을 상당히 능가한다. 자세한 내용은 부록 D.2에 나와 있다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '이 절제 연구에서 우리는 프레임워크에서 각 구성요소의 효과를 입증하는 것을 목표로 한다. 그림 11과 표 5는 그 결과를 요약한 것이다. (b) 및 (I)는 기준선의 결과를 제시하는 반면, (II)는 목표 구조를 보존하지 못하고 정적 피사체 이미지를 생성하는 키-값 대체의 결과를 나타낸다. (c) 및 (III)는 예측된 대응관계를 사용하여 AMA를 디스플레이하여, (b) 및 (I)에 비해 피사체 충실도를 향상시키지만, 신뢰할 수 없는 매치들을 걸러낼 수 없기 때문에 신속한 충실도를 획기적으로 감소시킨다. 이것은 기준선(I)까지 프롬프트 충실도를 상당히 향상시키는 의미 일치 마스크의 효과를 강조하는 (d) 및 (IV)에서 해결된다. 마지막으로, (d)와 (e)의 비교를 통해 의미 일치 지침이 목표 구조에서 최소한의 희생으로 주제 표현성을 향상시킨다는 것을 입증했으며, 이는 (V)에 의해 추가로 입증된다. 드림매처와 MasaCtrl을 비교하는 사용자 연구를 포함한 더 많은 분석 및 절제 연구가 부록 E에 제공된다.\n' +
      '\n' +
      '도 11: **성분 분석:**(a) 참조 이미지, (b) DreamBooth[44]에 의해 생성된 이미지, (c) 제안된 의미 매칭, (d) 의미 일치 마스크와 더 결합, (e) 의미 매칭 안내와 더 결합.\n' +
      '\n' +
      '도 10: **사용자 연구.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c} \\hline \\hline  & Component & \\(I_{\\text{DISO}}\\uparrow\\) & \\(I_{\\text{CLIP}}\\uparrow\\) & \\(T_{\\text{CLIP}}\\uparrow\\) \\\\ \\hline (I) & Baseline (DreamBooth [44]) & 0.638 & 0.808 & 0.237 \\\\ \\hline (II) & (I) + Key-Value Replacement (MasaCtrl [4]) & 0.728 & 0.854 & 0.201 \\\\ \\hline (III) & (I) + Semantic Matching & 0.683 & 0.830 & 0.201 \\\\ (IV) & (III) + Semantic-Consistent Mask (AMA) & 0.676 & 0.818 & 0.232 \\\\ (V) & (IV) + Semantic Matching Guid. (Ours) & 0.680 & 0.821 & 0.231 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: **성분 분석.** 이 분석을 위해, 기준선에 DreamBooth[44]를 사용하였다.\n' +
      '\n' +
      '그림 9: **이전 작품과의 질적 비교 [4, 22, 44, 49, 68]:** 이 비교를 위해 DreamBooth [44]를 MasaCtrl, FreeU, MagicFusion, DreamMatcher의 베이스라인으로 사용하였다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '텍스트 투 이미지(T2I) 개인화를 위한 튜닝 프리 플러그인 드림매처를 소개합니다. 드림매처는 미리 훈련된 T2I 개인화 모델 내에서 자기 주의 모듈의 생성 능력을 활용하여 의미적으로 정렬된 시각적 조건을 제공함으로써 개인화된 이미지에서 외관 유사성을 향상시킨다. 드림매처는 개인화에서 의미적으로 정렬된 시각적 컨디셔닝의 중요성을 개척하고 주의 프레임워크 내에서 효과적인 솔루션을 제공한다. 실험 결과 드림매처는 기존 T2I 모델의 개인화 기능을 향상시키고 복잡한 시나리오에서도 다른 플러그인 및 튜닝 기반 베이스라인보다 성능이 우수함을 알 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.\n' +
      '* [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 843-852, 2023.\n' +
      '* [3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.\n' +
      '* [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. _arXiv preprint arXiv:2304.08465_, 2023.\n' +
      '* [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.\n' +
      '* [6] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. _arXiv preprint arXiv:1405.3531_, 2014.\n' +
      '* [7] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. _arXiv preprint arXiv:2305.03374_, 2023.\n' +
      '* [8] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. _arXiv preprint arXiv:2309.05793_, 2023.\n' +
      '* [9] Songyan Chen and Jiancheng Huang. Fec: Three finetuning-free methods to enhance consistency for real image editing. _arXiv preprint arXiv:2309.14934_, 2023.\n' +
      '* [10] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. _arXiv preprint arXiv:2304.00186_, 2023.\n' +
      '* [11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. _arXiv preprint arXiv:2307.09481_, 2023.\n' +
      '* [12] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. _Advances in Neural Information Processing Systems_, 34:9011-9023, 2021.\n' +
      '* [13] Seokju Cho, Sunghwan Hong, and Seungryong Kim. Cats++: Boosting cost aggregation with convolutions and transformers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.\n' +
      '* [14] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via contrastive prompt-tuning. _arXiv preprint arXiv:2211.11337_, 2022.\n' +
      '* [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [16] Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. _arXiv preprint arXiv:2306.00986_, 2023.\n' +
      '* [17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.\n' +
      '* [18] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Designing an encoder for fast personalization of text-to-image models. _arXiv preprint arXiv:2302.12228_, 2023.\n' +
      '* [19] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. _arXiv preprint arXiv:2310.11513_, 2023.\n' +
      '* [20] Jing Gu, Yilin Wang, Nankuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, Hyun-Joon Jung, et al. Photoswap: Personalized subject swapping in images. _arXiv preprint arXiv:2305.18286_, 2023.\n' +
      '* [21] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. _arXiv preprint arXiv:2303.11305_, 2023.\n' +
      '* [22] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K Wong. Vico: Detail-preserving visual condition for personalized text-to-image generation. _arXiv preprintarXiv:2306.00971_, 2023.\n' +
      '* [23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.\n' +
      '* [25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [26] Sunghwan Hong, Jisu Nam, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong Kim. Neural matching fields: Implicit representation of matching fields for visual correspondence. _Advances in Neural Information Processing Systems_, 35:13512-13526, 2022.\n' +
      '* [27] Roger A Horn. The hadamard product. In _Proc. Symp. Appl. Math_, pages 87-169, 1990.\n' +
      '* [28] Jiancheng Huang, Yifan Liu, Jin Qin, and Shifeng Chen. Kv inversion: Kv embeddings learning for text-conditioned real image action editing. _arXiv preprint arXiv:2309.16608_, 2023.\n' +
      '* [29] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. _arXiv preprint arXiv:2304.02642_, 2023.\n' +
      '* [30] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. Cotr: Correspondence transformer for matching across images. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6207-6217, 2021.\n' +
      '* [31] Anant Khandelwal. Infusion: Inject and attention fusion for multi concept zero-shot text-based video editing. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3017-3026, 2023.\n' +
      '* [32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.\n' +
      '* [33] Jason Lee, Kyunghyun Cho, and Douwe Kiela. Countering language drift via visual grounding. _arXiv preprint arXiv:1909.04499_, 2019.\n' +
      '* [34] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. _arXiv preprint arXiv:2305.14720_, 2023.\n' +
      '* [35] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. _arXiv preprint arXiv:2305.19327_, 2023.\n' +
      '* [36] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In _International Conference on Machine Learning_, pages 6437-6447. PMLR, 2020.\n' +
      '* [37] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. _arXiv preprint arXiv:2307.02421_, 2023.\n' +
      '* [38] Jisu Nam, Gyuseong Lee, Sunwoo Kim, Hyeonsu Kim, Hyeonwuon Cho, Seyeon Kim, and Seungryong Kim. Diffmatch: Diffusion model for dense matching. _arXiv preprint arXiv:2305.19094_, 2023.\n' +
      '* [39] OpenAI. Gpt-4 technical report, 2023.\n' +
      '* [40] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* [41] Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559-572, 1901.\n' +
      '* [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aherman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.\n' +
      '* [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aherman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. _arXiv preprint arXiv:2307.06949_, 2023.\n' +
      '* [46] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.\n' +
      '* [47] Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, and Seungryong Kim. Midms: Matching interleaved diffusion models for exemplar-based image translation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 2191-2199, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* [49] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. _arXiv preprint arXiv:2309.11497_, 2023.\n' +
      '* [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.\n' +
      '* [51] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in neural information processing systems_, 32, 2019.\n' +
      '* [52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [53] Yu-Chuan Su, Kelvin CK Chan, Yandong Li, Yang Zhao, Han Zhang, Boqing Gong, Huisheng Wang, and Xuhui Jia. Identity encoder for personalized diffusion. _arXiv preprint arXiv:2304.07429_, 2023.\n' +
      '* [54] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. _arXiv preprint arXiv:2306.03881_, 2023.\n' +
      '* [55] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In _ACM SIGGRAPH 2023 Conference Proceedings_, pages 1-11, 2023.\n' +
      '* [56] Prune Truong, Martin Danelljan, Luc V Gool, and Radu Timofte. Gocor: Bringing globally optimized correspondence volumes into your neural network. _Advances in Neural Information Processing Systems_, 33:14278-14290, 2020.\n' +
      '* [57] Prune Truong, Martin Danelljan, and Radu Timofte. Glunet: Global-local universal network for dense flow and correspondences. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6258-6268, 2020.\n' +
      '* [58] Prune Truong, Martin Danelljan, Fisher Yu, and Luc Van Gool. Warp consistency for unsupervised learning of dense correspondences. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10346-10356, 2021.\n' +
      '* [59] Prune Truong, Martin Danelljan, Radu Timofte, and Luc Van Gool. Pdc-net+: Enhanced probabilistic dense correspondence network. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [60] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1921-1930, 2023.\n' +
      '* [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [62] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aherman. \\(p+\\): Extended textual conditioning in text-to-image generation. _arXiv preprint arXiv:2303.09522_, 2023.\n' +
      '* [63] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. _arXiv preprint arXiv:2302.13848_, 2023.\n' +
      '* [64] Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. A closer look at parameter-efficient tuning in diffusion models. _arXiv preprint arXiv:2303.18181_, 2023.\n' +
      '* [65] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. _arXiv preprint arXiv:2305.10431_, 2023.\n' +
      '* [66] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. _arXiv preprint arXiv:2305.10400_, 2023.\n' +
      '* [67] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. _arXiv preprint arXiv:2305.15347_, 2023.\n' +
      '* [68] Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, and Wenjing Yang. Magicfusion: Boosting text-to-image generation performance by fusing diffusion models. _arXiv preprint arXiv:2303.13126_, 2023.\n' +
      '\n' +
      '## 부록 구현 세부사항\n' +
      '\n' +
      '모든 실험을 위해 NVIDIA GeForce RTX 3090 GPU와 DDIM 샘플러 [50]을 사용하여 총 샘플링 시간 단계를 \\(T=50\\)으로 설정했다. 외모 일치 자기 주의와 의미 일치 안내를 모두 수행하기 위해 시간 단계를 \\(t\\in[4,50)\\)으로 경험적으로 설정했다. 모든 디코더 계층의 모든 self-attention 모듈을 self-attention과 일치하는 appearance으로 변환하였다. ViCo [22] 데이터 집합에 대한 평가는 \\(\\lambda_{c}=0.4\\)과 \\(\\lambda_{g}=75\\)을, 제안된 도전 프롬프트 리스트의 평가는 \\(\\lambda_{c}=0.4\\)과 \\(\\lambda_{g}=50\\)을 선택하였다.\n' +
      '\n' +
      '## 부록 B 데이터세트\n' +
      '\n' +
      'T2I(Text-to-Image) 개인화의 이전 작업[17, 32, 44]은 평가를 위해 서로 다른 데이터 세트를 사용했다. 공정하고 편견 없는 평가를 보장하기 위해 ViCo[22]는 장난감 6개, 살아있는 동물 5개, 액세서리 2개, 용기 2개, 건물 1개를 포함하는 16개의 고유한 개념으로 구성된 이러한 작업 [17, 32, 44]에서 이미지 데이터 세트를 수집했다. 프롬프트의 경우 ViCo는 11개의 비라이브 객체에 대해 31개의 프롬프트를 수집하고 5개의 라이브 객체에 대해 또 다른 31개의 프롬프트를 수집했다. 이는 보다 복잡한 텍스트 맥락에서 객체의 표현성을 평가하기 위해 원래 드림부스[44] 프롬프트에서 수정되었다. 공정한 비교를 위해 본 논문에서는 ViCo 데이터셋과 평가 설정을 준수하여 각 객체에 대해 8개의 샘플을 생성하고 프롬프트(prompt)를 생성했으며 총 3,969개의 이미지를 생성했다.\n' +
      '\n' +
      '우리의 목표는 복잡한 비강성 시나리오에서 의미적으로 일관된 T2I 개인화를 달성하는 것이다. 복잡한 설정에서 방법의 견고성을 평가하기 위해 ChatGPT[39]를 사용하여 신속한 데이터 세트를 만들었으며, 이는 큰 변위, 폐색 및 새로운 뷰 합성의 세 부분으로 분류된다. 데이터 세트는 큰 변위 및 폐색에 대해 각각 10개의 프롬프트, 라이브 및 비라이브 객체에 대해 별도로 새로운 뷰 합성에 대해 4개로 구성된다. 구체적으로 텍스트-이미지 확산 개인화 과제를 정의하고, ViCo의 예시 프롬프트 리스트를 제공하며, 본 연구의 목적에 부합하는 도전적인 프롬프트 리스트의 필요성을 강조한다. 그런 다음 ChatGPT에 각 범주에 대해 별개의 프롬프트 목록을 만들도록 요청했다. 복잡한 비강성 개인화 시나리오에 맞게 조정된 결과 프롬프트 목록은 그림 A.12에 자세히 설명되어 있다.\n' +
      '\n' +
      '## 부록 C기준 및 비교\n' +
      '\n' +
      '### Baseline\n' +
      '\n' +
      '드림매처는 모든 T2I 개인화 모델과 호환되도록 설계되었습니다. 제안된 방법은 Textual Inversion[17], DreamBooth[44], CustomDiffusion[32]의 세 가지 기준선을 사용하여 구현하였다.\n' +
      '\n' +
      '텍스트 역산[17]은 주어진 주제를 특수 토큰 \\(\\langle S^{*}\\rangle\\)에서 파생된 768차원 텍스트 임베딩으로 캡슐화한다. 몇 개의 참조 이미지를 사용하여, 이것은 T2I 확산 모델을 동결 상태로 유지하면서 텍스트 임베딩을 트레이닝함으로써 달성된다. 추론하는 동안, 모델은 타겟 프롬프트를 \\(\\langle S^{*}\\rangle\\)으로 조작하여 피사체의 새로운 렌더링을 생성할 수 있다. DreamBooth[44]는 T2I 확산 모델을 고유 식별자 및 피험자의 클래스 이름(예: \\(A\\)[V] _cat_)으로 더 미세 조정함으로써 이러한 접근법을 확장한다. 그러나 모든 파라미터를 미세 조정하면 언어 이동 문제가 발생할 수 있다[33, 36]. 이를 해결하기 위해 DreamBooth는 클래스별 사전 보존 손실을 제안하는데, 클래스별 사전 보존 손실은 카테고리 이름을 프롬프트(예: _A cat_)로 사용하여 미리 훈련된 T2I 모델에 의해 생성된 다양한 샘플로 모델을 훈련시킨다. 마지막으로 CustomDiffusion[32]은 매개 변수의 부분 집합, 특히 교차 주의 투영 계층만 미세 조정하는 것이 새로운 개념을 학습하는 데 효율적임을 보여준다. 이것은 DreamBooth와 유사하게, 고유 인스턴스를 일반 카테고리와 결합하는 텍스트 프롬프트를 사용하여 구현되며, 또한 대규모 오픈 이미지-텍스트 데이터세트로부터의 정규화 데이터세트를 포함한다[46]. 유망한 결과에도 불구하고 앞서 언급한 접근법은 색상, 질감 및 모양을 포함하여 피사체의 외관을 정확하게 모방하기 위해 자주 어려움을 겪는다. 이를 해결하기 위해 본 논문에서는 타겟 프롬프트로부터 다양한 구조를 유지하면서 기준 외형을 크게 향상시키는 튜닝 프리 플러그인 방법을 제안한다.\n' +
      '\n' +
      '### Comparision\n' +
      '\n' +
      '우리는 이전의 튜닝 프리 플러그인 모델인 FreeU[49] 및 MagicFusion[68]과 최적화 기반 모델인 ViCo[22]에 대해 드림매처를 벤치마킹했다.\n' +
      '\n' +
      'FreeU[49]의 핵심 통찰력은 노이즈 제거 U-Net의 주요 백본이 저주파 의미론에 기여하는 반면 스킵 연결은 고주파 세부 사항에 초점을 맞춘다는 것이다. 이 관찰을 활용하여 FreeU는 이 두 가지 고유한 기능에 대해 주파수 인식 재가중 기법을 제안하고 드림부스에 통합될 때 향상된 생성 품질을 보여준다[44]. 매직퓨전[68]은 두 개의 미리 훈련된 확산 모델로부터 예측된 잡음을 결합하는 현저성 인식 잡음 혼합 방법을 소개한다. 매직퓨전은 개인화된 모델인 드림부스[44]를 일반적인 T2I 확산 모델과 통합할 때 T2I 개인화에서 그 효과를 입증한다. ViCo[22]는 키-값 대체 개념으로 설계된 추가 이미지 어댑터를 최적화합니다.\n' +
      '\n' +
      '## 부록 D 평가\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '평가를 위해 주제 충실도와 신속 충실도의 두 가지 주요 측면에 초점을 맞추었다. 피험자 충실도를 위해 선행 연구 [17, 22, 32, 44]에 이어 각각 \\(I_{\\mathrm{CLIP}}\\) 및 \\(I_{\\mathrm{DINO}}\\)으로 표시된 CLIP [42] 및 DINO [5] 이미지 유사성 메트릭을 채택했다. (I_{\\mathrm{DINO}}\\)는 주제 표현성을 평가하는 데 우리가 선호하는 메트릭이다. [22, 44]에서 언급된 바와 같이, DINO는 동일한 카테고리 내의 객체들을 구별하기 위해 자체 감독 방식으로 트레이닝되어, 동일한 주제의 시각적 속성들을 모방하는 것을 목표로 하는 상이한 방법들을 평가하는데 더 적합하다. 즉시 충실도를 위해 [17, 22, 32, 44]에 이어 이미지-텍스트 유사성 메트릭 \\(T_{\\text{CLIP}}\\)을 채택하여 생성된 이미지의 CLIP 시각적 특징을 플레이스홀더를 제외한 해당 텍스트 프롬프트의 CLIP 텍스트 특징과 비교한다. 이전 작업 [17, 22, 32, 44]에 이어 CLIP 및 DINO에 각각 ViT-B/32 [15] 및 ViT-S/16 [15]를 사용했다.\n' +
      '\n' +
      '### User study\n' +
      '\n' +
      '사용자 연구의 예제 질문은 그림 A.13에 나와 있다. 우리는 DreamMatcher를 이전 작품[22, 49, 68]과 비교하여 피험자와 신속한 충실도에 대한 짝을 이룬 인간 선호도 연구를 수행했다. 그 결과는 본 논문에서 그림 10에 요약되어 있다. 피험자 충실도를 위해 참가자에게 참조 이미지와 다른 방법의 생성된 이미지를 제시하고 참조에서 피험자를 더 잘 나타내는지 질문했다. 신속한 충실도를 위해 해당 텍스트 프롬프트와 함께 다른 작업에서 생성된 이미지를 보여주고 주어진 프롬프트와 더 일치하는지를 물었다. 45명의 사용자가 32개의 비교 질문에 응답해 총 1440개의 응답을 얻었다. 우리는 두 가지 다른 설문지를 배포했으며 23명의 사용자가 한 명에게 응답하고 22명의 사용자가 다른 사용자에게 응답했다. 샘플은 편향되지 않은 큰 풀에서 무작위로 선택되었습니다.\n' +
      '\n' +
      '## 부록 E 분석\n' +
      '\n' +
      '# 외모매칭 셀프-어텐션\n' +
      '\n' +
      '**특징 추출.**도 A.1은 상이한 디코더 계층으로부터 추출된 특징 기술자에 대한 PCA[41] 결과를 시각화한다. 이 분석을 위해 우리는 제안된 기술을 적용하지 않는다. PCA는 역방향 확산 과정의 50%에서, 드림부스[44]로부터 추정된 참조 이미지 및 타겟 이미지의 중간 특징 기술자들에 적용된다. 우리의 주요 통찰력은 초기 레이어가 높은 수준의 의미론을 포착하는 반면, 이후 레이어는 생성된 이미지의 더 미세한 세부 사항에 초점을 맞춘다는 것이다. 특히, \\(l=1\\)은 지나치게 높은 수준의 저해상도 의미들을 포착하여, 대응 관계를 찾기 위한 충분한 의미들을 제공하지 못한다. 반대로 \\(l=4\\)은 너무 세밀한 세부 사항에 초점을 맞추고 있어 특징 간의 의미적으로 일치하는 영역을 찾기 어렵다. 대조적으로, \\(l=2\\)과 \\(l=3\\)은 의미적 매칭을 용이하게 하기 위한 충분한 의미적 및 구조적 정보에 초점을 맞추어 균형을 이룬다. 이 분석을 바탕으로 디코더 계층에서 연접된 특징 기술자를 사용하여 \\(l\\in[2,3]\\)\\(\\psi_{t}\\in\\mathbb{R}^{H\\times W\\times 1920}\\)을 얻는다. 그리고 이러한 특징 기술자에 PCA를 적용하여 정합 정확도를 높이고 메모리 소모를 줄이기 위해 \\(\\psi_{t}\\in\\mathbb{R}^{H\\times W\\times 256}\\)의 결과를 얻는다. 다양한 시간 단계에 걸친 확산 특징 시각화는 본 논문의 그림 6에 나와 있다.\n' +
      '\n' +
      '우리의 접근 방식은 특정 시간 단계를 선택하고 미리 훈련된 확산 모델을 통과하기 전에 깨끗한 RGB 이미지에 해당 노이즈를 주입하는 이전 작업[37, 54, 67]과 다르다는 점에 유의한다. 이와는 대조적으로, 우리는 개인화 절차의 각 단계 동안 의미적 매칭을 찾기 위해 역확산 프로세스의 각 시간 단계로부터의 확산 특징을 활용한다.\n' +
      '\n' +
      '**A on different time steps and layers.** We ablate start time steps and decoder layer in related to the proposed appearance matching self-attention (AMA) module. 그림 A.2는 결과를 요약한다. 흥미롭게도, 우리는 초기 시간 단계와 디코더 레이어에서 AMA를 적용하면 모양, 질감 및 색상을 포함한 피사체의 전반적인 모양을 효과적으로 보정한다는 것을 관찰한다. 대조적으로, 후기 시간 단계 및 층에서 적용된 AMA는 기준선에서와 같이 피험자의 외관을 더 밀접하게 보존하는 경향이 있다. 매 시간 단계에서 AMA를 주입하면 차선책이 생성된다는 점에 유의하십시오.\n' +
      '\n' +
      '그림 A.1: 상이한 디코더 층에서의 **확산 특징 시각화: 좌측은 역확산 프로세스의 50%에서 중간 추정 기준 및 타겟 이미지를 디스플레이한다. 목표물은 드림부스[44]에서 beach_ 상의 prompt _A\\(\\langle S^{*}\\rangle\\)를 이용하여 생성하였다. 우측은 서로 다른 디코더 계층으로부터 확산 특징 기술자의 상위 세 가지 주성분을 시각화한다. 의미적으로 유사한 영역은 유사한 색상을 공유한다.**\n' +
      '\n' +
      '도 2: ** 상이한 시간 단계들 및 층들 상에서 AMA를 삭마하는 것: 좌측 섹션은 기준 이미지 및 베이스라인에 의해 생성된 타겟 이미지를 도시한다[44]. 우측 섹션은 (a) 상이한 시간 단계들 및 (b) 상이한 디코더 계층들 상에 자기-어텐션 매칭에 의해 생성된 개선된 타겟 이미지를 디스플레이한다. 이 절제 연구를 위해 우리는 의미 일치 지침을 사용하지 않는다.**\n' +
      '\n' +
      '결과는 시간 단계 4 이전의 기준선이 아직 대상 이미지 레이아웃을 구성하지 않았기 때문이다. 이러한 분석을 바탕으로 사전 학습된 U-Net의 자기 주의 모듈을 모든 평가에서 \\(t\\in[4,50)\\)과 \\(l\\in[1,4)\\)에 대한 자기 주의와 일치하는 외관으로 변환하였다.\n' +
      '\n' +
      '### Consistency Modeling\n' +
      '\n' +
      '그림 A.3에서는 주기-일관성 하이퍼파라미터 \\(\\lambda_{c}\\)와 개인화 충실도 사이의 정량적 관계를 보여준다. 먼저 \\(\\lambda_{c}\\)를 도입함에 따라 신속한 충실도 \\(T_{\\mathrm{CLIP}\\)이 급격히 향상되어 신뢰 마스크가 잘못된 정합을 효과적으로 필터링하여 모델이 세부 목표 구조를 보존할 수 있음을 보여준다. 그 후, 높은 \\(\\lambda_{c}\\) 값은 목표 구조물에 더 많은 참조 모양을 주입하고, \\(I_{\\mathrm{DINO}}\\) 및 \\(I_{\\mathrm{CLIP}}\\)을 증가시키지만, 즉각적인 충실도 \\(T_{\\mathrm{CLIP}}\\)을 약간 희생시킨다. 이는 사용자가 \\(\\lambda_{c}\\)를 조정함으로써 기준 외관의 범위와 목표 구조물 보존을 제어할 수 있음을 나타낸다. 전체 AMA에 대한 의사 코드는 알고리즘 1에서 사용할 수 있다.\n' +
      '\n' +
      '### 시맨틱 매칭 안내\n' +
      '\n' +
      '그림 A.4에서 우리는 의미 일치 안내 \\(\\lambda_{g}\\)와 개인화 충실도 사이의 정량적 관계를 보여준다. 증가하는 \\(\\lambda_{g}\\)는 생성된 목표 \\(\\hat{z}_{0,t}^{\\mathrm{Y}\\)을 깨끗한 기준 잠재 \\(z_{0}^{\\mathrm{X}\\)에 더 가깝게 지향함으로써 주어 충실도 \\(I_{\\mathrm{DINO}\\) 및 \\(I_{\\mathrm{CLIP}\\)을 향상시킨다. 그러나, 지나치게 높은 \\(\\lambda_{g}\\)은 초기 단계에서 기준 및 목표 잠복기 사이의 불일치로 인해 피사체 충실도를 감소시킬 수 있다. ViCo 데이터셋은 매개변수 \\(\\lambda_{g}\\)를 세심하게 제거하고, 제안한 도전 데이터셋은 \\(\\lambda_{g}=75\\)과 \\(\\lambda_{g}=50\\)을 선택하였다. 전체 시맨틱 매칭 안내를 위한 의사 코드는 알고리즘 2에서 이용 가능하다.\n' +
      '\n' +
      '### 키-값 교체 vs. 드림매처\n' +
      '\n' +
      'MasaCtrl[4]은 로컬 편집 작업에 대한 키-값 대체 기법을 도입하였다. 여러 후속 작업[4, 9, 11, 28, 31, 37]이 이 프레임워크를 채택하고 추가로 개발했다. 드림매처와 MasaCtrl의 정성적 비교를 제공하는 그림 A.5에 도시된 바와 같이, 키-값 대체는 종종 참조 이미지 내의 피사체의 포즈들과 유사한 포즈들을 갖는 피사체 중심 이미지들을 생성하기 쉽다. 이러한 경향은 키-값 교체가 미리 훈련된 자기 주의 모듈로부터의 타겟 구조를 방해하고 타겟 키들과 참조 쿼리들 사이의 차선의 매칭에 의존하기 때문에 발생한다. 또한, 이 기법은 예측된 매칭의 불확실성을 고려하지 않으며, 이는 참조 이미지로부터 변경된 배경으로의 또는 타겟 프롬프트에 의해 생성되는 새롭게 출현하는 객체로의 무관한 요소의 주입으로 이어진다.\n' +
      '\n' +
      '대조적으로, 드림매처는 고정된 타겟 구조를 보존하고 시맨틱 매칭을 명시적으로 활용하여 기준 외관을 정확하게 정렬한다. 또한 제안된 방법은 예측된 매칭의 불확실성을 고려함으로써, 잘못된 매칭을 필터링하고 타겟 프롬프트에 의해 새롭게 도입된 이미지 요소를 유지한다. 이미지 유사성 메트릭 \\(I_{\\mathrm{DINO}}\\)과 \\(I_{\\mathrm{CLIP}}\\)은 표적 구조의 보존과 참조 외관의 반영을 동시에 고려하지 않는다는 점에 유의한다. 그들은 기준 이미지와 생성된 이미지의 전체 픽셀 간의 유사성만 계산합니다. 그 결과, 주제 중심 이미지를 생성하고 참조 이미지에서 관련 없는 요소를 목표 컨텍스트에 주입하는 키-값 대체는 본 논문의 표 5에서 볼 수 있듯이 드림매처보다 더 나은 이미지 유사성을 달성한다. 그러나, 도 A.5에 도시된 바와 같이, 드림매처는 더 ac\n' +
      '\n' +
      '그림 A.3: \\(\\lambda_{c}\\)와 개인화 충실도 사이의 **관계.** 이 절제 연구에서 제안된 도전 프롬프트 목록을 사용하여 방법을 평가한다.\n' +
      '\n' +
      '그림 A.6: **사용자 연구:** 이 연구에서 드림부스[44]는 MasaCtrl 및 드림매처 모두에 대한 기준선으로 사용된다.\n' +
      '\n' +
      '그림 A.4: \\(\\lambda_{g}\\)와 개인화 충실도 사이의 **관계.** 이 절제 연구에서 제안된 도전 프롬프트 목록을 사용하여 방법을 평가한다.\n' +
      '\n' +
      '큰 구조 변위에서도 참조 모양을 대상 컨텍스트에 적절하게 정렬합니다. 더 많은 질적 비교가 그림 A.17과 A.18에 제공된다.\n' +
      '\n' +
      '이는 그림 A.6에 요약된 MasaCtrl [4]와 DreamMatcher를 비교한 사용자 연구에서 추가로 입증되며, 총 39명의 사용자가 32개의 비교 질문에 응답하여 1248개의 응답을 얻었다. 이 응답은 두 개의 다른 설문지로 나뉘었으며 20명의 사용자가 하나에 응답하고 다른 하나에 19명이 응답했다. 샘플은 편향되지 않은 큰 풀에서 무작위로 선택되었다. 이 사용자 연구의 예는 그림 A.14에 나와 있다. 드림매처는 두 충실도 모두에서 MasaCtrl을 큰 마진으로 크게 능가하여 제안된 매칭 인식 값 주입 방법의 유효성을 입증한다.\n' +
      '\n' +
      '###Key Retention의 정당화\n' +
      '\n' +
      'DreamMatcher는 시맨틱 매칭을 통해 왜곡된 기준 값들을 타겟 구조에 가져온다. 이 설계 선택은 동일한 구조를 공유하는 쿼리 및 키 쌍으로 훈련된 사전 훈련된 U-Net을 활용하기 때문에 합리적이다. 이를 통해 타겟 키 및 쿼리를 변경하지 않고 유지함으로써 미리 훈련된 타겟 구조 경로를 보존할 수 있다. 이를 검증하기 위해 표 A.1은 참조 값만 워핑하고 참조 키와 값을 모두 워핑하는 정량적 비교를 보여주며, 이는 참조 모양만 워핑하면서 목표 구조 경로를 고정하는 것이 전체 성능에 중요하다는 것을 나타낸다. 대상 키와 뒤틀린 기준 값 간의 정렬 오류에 대한 우려가 발생할 수 있습니다. 그러나 자기 주의와 일치하는 외관은 기준 값을 목표 구조와 정확하게 정렬하여 목표 키와 뒤틀린 기준 값이 미리 훈련된 대로 기하학적으로 정렬되도록 함을 강조한다.\n' +
      '\n' +
      '### Reference Selection\n' +
      '\n' +
      '우리는 무작위로 선택된 5개의 참조 이미지 세트에 걸쳐 모든 메트릭의 분산을 측정하여 참조 이미지 선택의 변화에 대한 드림매처의 안정성을 평가한다. 그림 A.7은 모든 메트릭이 평균을 중심으로 밀접하게 분포되어 있음을 나타낸다. 구체적으로, 평균 \\(I_{\\mathrm{DINO}}\\)은 분산이 \\(6e{-6}\\)인 0.683이고, 평균 \\(T_{\\mathrm{CLIP}}\\)은 분산이 \\(3e{-5}\\)인 0.225이다. 이것은 우리의 방법이 참조 선택에 견고하고 신뢰할 수 있는 결과를 일관되게 생성한다는 것을 강조한다. 우리는 G절의 다른 참조 이미지와 함께 질적 비교에 대해 더 논의한다.\n' +
      '\n' +
      '안정적인 확산을 위한 드림매처\n' +
      '\n' +
      '드림매처는 기준선에 의존하는 플러그인 방식이어서 본 논문에서 미리 훈련된 개인화된 모델[17, 32, 44]을 대상으로 드림매처를 평가하였다. 이 절에서는 또한 안정확산을 기준으로 드림매처를 평가했다. 표 A.2 및 그림 A.8은 드림매처가 769차원 텍스트 임베딩을 최적화하는 Textual Inversion의 \\(I_{\\mathrm{DINO}}\\) 및 \\(I_{\\mathrm{CLIP}}\\)을 능가하는 기성 사전 훈련된 모델 없이 주제 충실도를 향상시킨다는 것을 보여준다.\n' +
      '\n' +
      '### 다중 주제 개인화\n' +
      '\n' +
      '그림 A.9에서 볼 수 있듯이, 우리는 여러 주제에 대해 드림매처를 확장한다. 이 실험을 위해 CustomDiffusion[32]를 기준선으로 사용했다. 입력으로서 두 개의 상이한 주제를 배치시키는 것을 포함하는 간단한 수정이 이러한 기능을 가능하게 한다는 점에 유의한다.\n' +
      '\n' +
      '### Computational Complexity\n' +
      '\n' +
      '표 A.3에 요약된 바와 같이, 본 프레임워크의 상이한 구성에 대한 시간 및 기억 소비를 조사한다. 본 바와 같이, 드림매처는 베이스라인 드림부스에 비해 시간 및 기억의 합리적인 증가로 피험자 외모를 상당히 개선한다[44]. 또한, 비용 볼륨을 구축하기 전에 특징 기술자의 PCA[41] 차원을 줄이는 것이 전체 성능에 영향을 미치지 않는 반면, 시간 소비를 극적으로 감소시킨다는 것을 관찰한다. 우리의 방법은 이전의 트레이닝 기반 [8, 10, 18, 29, 34, 48, 53, 63, 65]와 달리, 또는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline Method & \\(I_{\\mathrm{DINO}}\\uparrow\\) & \\(I_{\\mathrm{CLIP}}\\uparrow\\) & \\(T_{\\mathrm{CLIP}}\\uparrow\\) \\\\ \\hline \\hline Textual Inversion [17] & 0.529 & 0.762 & **0.220** \\\\ \\hline Stable Diffusion (SD) & 0.770 & **0.778** & 0.220 \\\\ DreamMatcher & **0.571** (+10.74\\%) & **0.785** (+1.95\\%) & 0.214 (-0.50\\%) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 A.2: **안정확산에 대한 드림매처의 정량적 결과.**\n' +
      '\n' +
      '그림 A.7: ** 무작위로 선택된 참조 이미지 5세트의 통계 결과.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline Method & \\(I_{\\mathrm{DINO}}\\uparrow\\) & \\(I_{\\mathrm{CLIP}}\\uparrow\\) & \\(T_{\\mathrm{CLIP}}\\uparrow\\) \\\\ \\hline \\hline Warping only reference values (DreamMatcher) & **0.680** & **0.821** & 0.231 \\\\ Warping both reference keys and values & 0.654 & 0.809 & **0.235** \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 A.1: **키 보유에 관한 절제 연구.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:16]\n' +
      '\n' +
      '자신 있게 일치하는 외모의 부족으로 인한 대상 이미지 이는 본 논문에서 제안한 방법이 참조 선택에 강인하더라도, 피사체의 풍부한 시각적 속성을 포함하는 더 나은 참조 이미지가 성능에 도움이 될 것임을 나타낸다. 우리의 향후 연구는 참조 이미지의 선택을 자동화하거나 여러 참조 이미지를 공동으로 통합하는 데 중점을 둘 것이다.\n' +
      '\n' +
      '그림 A.12: **도전 텍스트 프롬프트 목록:** 비라이브 및 라이브 주제 모두에 대한 복잡하고 비강성 시나리오에서의 평가 프롬프트. ‘{}’는 Textual Inversion[17]의 \\(\\langle S^{*}\\rangle\\)과 DreamBooth[44]의 [[V] class]와 CustomDiffusion[32]의 \\(\\langle S^{*}\\rangle\\)을 나타낸다.\n' +
      '\n' +
      '그림 A.13: **DreamMacher와 이전 방법을 비교한 사용자 연구의 예:** 피험자 충실도에 대해, 우리는 참조 이미지와 상이한 방법, ViCo[22], FreeU[49], MagicFusion[68] 및 DreamMatcher로부터 생성된 이미지를 제공한다. 신속한 충실도를 위해 대상 프롬프트와 생성된 이미지를 이러한 방법으로 제공한다. 공정한 비교를 위해 우리는 편향되지 않은 큰 풀에서 이미지 샘플을 무작위로 선택한다.\n' +
      '\n' +
      '그림 A.14: **DreamMatcher와 MasaCtrl[4]:**를 비교하는 사용자 연구의 예. 피험자 충실도에 대해, 우리는 MasaCtrl 및 DreamMatcher의 두 가지 다른 방법으로부터 생성된 참조 이미지 및 이미지를 제공한다. 프롬프트 충실도를 위해, 타겟 프롬프트 및 이 두 가지 방법으로부터의 생성된 이미지가 제공된다. 공정한 비교를 위해 이미지 샘플은 편향되지 않은 큰 풀에서 무작위로 선택된다.\n' +
      '\n' +
      '```\n' +
      'defAMA(self,pca_feats,q_tgt,q_ref,k_tgt,k_ref,v_tgt,v_ref,mask_tgt,cc_thres,num_heads,**kwargs):  #Initializedimensionsandrearrangeinputs. B,H,W=init_dimensions(q_tgt,num_heads)  q_tgt,q_ref,K_tgt,k_ref,v_tgt,v_ref=rearrange_inputs(q_tgt,q_ref,k_tgt,k_ref,v_tgt,v_ref,num_heads,H,W)  #Performfeatureinterpolationandrearrangement. src_feat,trg_feat=interpolate_and_rearrangerange(pca_feats,H) src_feat,trg_feat=12_norm(src_feat,trg_feat) #Computesimilarity. sim=compute_similarity(trg_feat,src_feat)  #Calculateforward andbackwardsimilarities andflows. sim_backward=rearrange(sim,"b(HtWt)(HWsWs)->b(HWsWs)HtWt")  sim_forward=rearrange(sim,"b(HtWt)(HWsWs)->b(HtWt)HsWs\')  flow_tgt_to_ref,flow_ref_to_tgt=compute_flows_with_argmax(sim_backward,sim_forward)  #Computecycleconsistencyerrorandconfidence. cc_error=compute_cycle_consistency(flow_tgt_to_ref,flow_ref_to_tgt)  f_g_ratio=mask_tgt.sum()/(H*W) confidence=(cc_error<cc_thres*H*fg_ratio) #Warpvalueandapplysemantic-consistentmask. warped_v=warp_verf,flow_tgt_to_ref warped_v=warped_v*confidence+v_tgt*(1-confidence) warped_v=warped_v*mask_tgt+v_tgt*(1-mask_tgt) #self-attention을 수행한다. aff=compute_affinity(q_tgt,k_tgt)  attn=aff_softmax(-1)  out=compute_output(attn,warped_v)  returnout\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 외모매칭 자기주장을 위한 의사코드, PyTorch-like``\n' +
      '``` fori,tininenumerate(tqdm(self.scheduler.timesteps)):\n' +
      '#Definemodelinputs. latents=combine_latents(latents_ref,latents_tgt)\n' +
      '#Enablegradientcomputationformatchingguidance. enable_gradients(latents)\n' +
      '#SamplingandfeatureextractionfromtheU-Net. noise_pred,feats=self.unet(latents,t,text_embeddings)\n' +
      '#Interpolationandconcatenatingfeaturesfromdifferentlayers. src_feat_uncond,tgt_feat_uncond,src_feat_cond,tgt_feat_cond=interpolate_and_concat(feats)\n' +
      '#PerformPCAandnormalizethefeatures. pca_feats=perform_pca_and_normalize(src_feat_uncond,tgt_feat_uncond,src_feat_cond,tgt_feat_cond)\n' +
      '#Applysemanticmatchingguidanceifrequired. ifmatching_guidanceand(iinself.mg_step_idx): _pred_z0_tgt=self.step(noise_pred,t,latents) pred_z0_src=image_to_latent(src_img uncond_grad,cond_grad=compute_gradients(pred_z0_tgt,pred_z0_src,t,pca_feats) alpha_prod_t=self.scheduler.aligns_cumprod[t]β_weight*beta_prod_t=1-alpha_prod_t noise_pred[1]-grad_weight*beta_prod_t**0.5*uncond_grad\n' +
      '#Applyclassifier-freeguidanceforenhancedgeneration. ifguidance_scale>1.0: noise_pred=classifier_free_guidance(noise_pred,guidance_scale)\n' +
      '#Stepfromz_ttoz_t-1. latents=self.step(noise_pred,t,latents)\n' +
      '```\n' +
      '\n' +
      '**시맨틱 매칭 유도, PyTorch-like를 위한 알고리즘 2** 의사 코드\n' +
      '그림 A.15: **실제 사물에 대한 기준선과 질적 비교:** 우리는 드림매처를 세 가지 다른 기준선, Textual Inversion[17], DreamBooth[44], CustomDiffusion[32]과 비교한다.\n' +
      '\n' +
      '그림 A.16: **생물이 아닌 물체에 대한 기준선과 정성적 비교:** 우리는 드림매처를 세 가지 다른 기준선, Textual Inversion[17], DreamBooth[44], CustomDiffusion[32]과 비교한다.\n' +
      '\n' +
      '그림 A.17: 살아있는 객체에 대한 이전 작업 [4, 22, 44, 49, 68]과의 **정성적 비교:** 이 비교를 위해 DreamBooth[44]는 MasaCtrl[4], FreeU[49], MagicFusion[68], DreamMatcher의 베이스라인으로 사용된다.\n' +
      '\n' +
      '그림 A.18: 생물이 아닌 물체에 대한 이전 작업 [4, 22, 44, 49, 68]과의 **정성적 비교:** 이 비교를 위해 Dream-Booth[44]는 MasaCtrl[4], FreeU[49], MagicFusion[68], DreamMatcher의 베이스라인으로 사용된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>