<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners\n' +
      '\n' +
      'Yazhou Xing\\({}^{1}\\) Yingqing He\\({}^{1}\\) Zeyue Tian\\({}^{1}\\) Xintao Wang\\({}^{2}\\) Qifeng Chen\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)HKUST \\({}^{2}\\)ARC Lab, Tencent PCG\n' +
      '\n' +
      'equal contribution\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Video and audio content creation serves as the core technique for the movie industry and professional users. Recently, existing diffusion-based methods tackle video and audio generation separately, which hinders the technique transfer from academia to industry. In this work, we aim at filling the gap, with a carefully designed optimization-based framework for cross-visual-audio and joint-visual-audio generation. We observe the powerful generation ability of off-the-shelf video or audio generation models. Thus, instead of training the giant models from scratch, we propose to bridge the existing strong models with a shared latent representation space. Specifically, we propose a multimodality latent aligner with the pre-trained ImageBind model. Our latent aligner shares a similar core as the classifier guidance that guides the diffusion denoising process during inference time. Through carefully designed optimization strategy and loss functions, we show the superior performance of our method on joint video-audio generation, visual-steered audio generation, and audio-steered visual generation tasks. The project website can be found at [https://yzxing87.github.io/Seeing-and-Hearing/](https://yzxing87.github.io/Seeing-and-Hearing/).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Recently, AI-generated content has made significant advances in creating diverse and high-realistic images [4, 9, 22, 32, 34], videos [4, 7, 15, 19, 20, 22, 38], or sound [25, 28, 29, 30, 44], based on the input descriptions from users. However, existing works primarily concentrate on generating content within a single modality, disregarding the multimodal nature of the real world. Consequently, the generated videos lack accompanying audio, and the generated audio lacks synchronized visual effects. This research gap restricts users from creating content with greater impact, such as producing films that necessitate the simultaneous creation of both visual and audio modalities. In this work, we study the visual-audio generation task for crafting both video and audio content.\n' +
      '\n' +
      'One potential solution to this problem is to generate visual and audio content in two stages. For example, users can first generate the video based on the input text prompt utilizing existing text-to-video (T2V) models [7, 18]. Then, a video-to-audio (V2A) model can be employed to generate aligned audio. Alternatively, a combination of text-to-audio (T2A) and audio-to-video (A2V) models can be used to generate paired visual-audio content. However, existing V2A and A2V generation methods [26, 46] either have limited capability to specific downstream domains or exhibit poor generation performance. Moreover, the task of joint video-audio generation (Joint-VA) has received limited attention, and existing work [36] shows limited generation performance even within a small domain and also lacks semantic control.\n' +
      '\n' +
      'In this work, we propose a new generation paradigm for open-domain visual-audio generation. We observe that: (1) There are well-trained single-modality text-conditioned generation models that demonstrate excellent performance. Leveraging these pre-trained models can avoid expensive training for synthesizing each modality. (2) We have noticed that the pre-trained model ImageBind [17] possesses remarkable capability in establishing effective connections between different data modalities within a shared semantic space. Our objective is to explore how we can leverage ImageBind as a bridge to connect and integrate various modalities effectively.\n' +
      '\n' +
      'Leveraging these observations, we propose to utilize ImageBind as an aligner in the diffusion latent space of different modalities. During the generation of one modality, we input the noisy latent and the guided condition of another modality to our aligner to produce a guidance signal that influences the generation process. By gradually injecting the guidance into the denoising process, we bridge the generated content closer to the input condition in the ImageBind embedding space. For Joint-VA generation, we make the guidance bidirectional to impact the generation processes of both modalities.\n' +
      '\n' +
      'With our design, we successfully bridge the pre-trained single-modality generation models into an organic systemand achieve a versatile and flexible visual-audio generation. In addition, our approach does not require training on large-scale datasets, making our approach very resource-friendly. Besides the generality and low cost of our approach, we validate our performance on four tasks and show the superiority over baseline approaches.\n' +
      '\n' +
      'In summary, our key contributions are as follows:\n' +
      '\n' +
      '* We propose a novel paradigm that _bridges_ pre-trained diffusion models of single modality together to achieve audio-visual generation.\n' +
      '* We introduce _diffusion latent aligner_ to gradually align diffusion latent of visual and audio modalities in a multimodal embedding space.\n' +
      '* We conduct extensive experiments on four tasks including V2A, I2A, A2V, and Joint-VA, demonstrating the superiority and generality of our approach.\n' +
      '* To the best of our knowledge, we present the first work for text-guided joint video-audio generation.\n' +
      '\n' +
      '## 2 Related Work\n' +
      '\n' +
      '### Conditional Audio Generation\n' +
      '\n' +
      'Audio generation is an emerging field that focuses on modeling the creation of diverse audio content. This includes tasks such as generating audio conditioned on various inputs like text [11, 16, 24, 25, 28, 44], images [37], and videos [12, 26, 31, 39].\n' +
      '\n' +
      'In the field of text-to-audio research, AudioGen [28] proposes an auto-regressive generative model that operates on discrete audio representations, DiffSound [44] utilizes non-autoregressive token-decoder to address the limitations of unidirectional generation in auto-regressive models. While some other works like Make-An-Audio [25], AudioLDM [29], employ latent diffusion methods for audio generation. Some recent studies, such as Make-an-Audio2 [24], AudioLDM2 [30], TANGO [16], have leveraged Large Language Models (LLMs) to enhance the performance of audio generation models.\n' +
      '\n' +
      'Research focusing on audio generation that is conditioned on images and videos, exemplified by works like Im2Wav [37] and SpecVQGAN [26], has also captured significant interest within the scholarly community. Utilizing the semantics of a pre-trained CLIP model for visual representation (Contrastive Language-Image Pre-training) [33], Im2Wav [37] first crafts a foundational audio representation via a language model, then employs an additional language model to upsample these audio tokens into high-fidelity sound samples. SpecVQGAN [26] utilizes a transformer to generate new spectrograms from a pre-trained codebook based on input video features. It then reconstructs the waveform from these spectrograms using a pre-trained vocoder.\n' +
      '\n' +
      '### Conditional Visual Generation\n' +
      '\n' +
      'The task of text-to-image generation has seen significant development and achievements in recent years [2, 35, 40]. This progress has sparked interest in a new research domain focusing on audio-to-image generation. In 2019, [42] proposed a method to generate images from audio recordings, employing Generative Adversarial Networks (GANs). [47] focused narrowly on generating images of MNIST digits using audio inputs and did not extend to image generation from general audio sounds. In contrast, the approach by [42], was capable of generating images from a broader range of audio signals. Wav2CLIP [43] adopts a CLIP-inspired approach to learn joint representations for audio-image pairs, which can subsequently facilitate image generation using VQ-GAN [13]. Text-to-video has also achieved remarkable progress recently [1, 4, 7, 15, 19,\n' +
      '\n' +
      'Figure 1: **Overview.** Our approach is versatile and can tackle four tasks: joint video-audio generation (Joint-VA), video-to-audio (V2A), audio-to-video (A2V), and image-to-audio (I2A). By leveraging a multimodal binder, e.g., pretrained ImageBind, we establish a connection between isolated generative models that are designed for generating a single modality. This enables us to achieve both bidirectional conditional and joint video/audio generation.\n' +
      '\n' +
      ' 22, 23, 25, 48, 49] empowered by video diffusion models [23]. The mainstream idea is to incorporate temporal modeling modules in the U-Net architecture to learn the temporal dynamics [1, 19, 23, 38, 49] in the video pixel space [22, 23] or in the latent space [4, 19]. In this work, we leverage the open-source latent-based text-to-video model as our base model for the video generation counterpart. There\'re also some Audio-to-video works that have been done, such as Sound2sight [5], TATS [14], and Tempotokens [45]. While [5] focuses on extending videos in a way that aligns with the audio, Tempotokens [45] takes a different approach by exclusively generating videos from the audio input. TATS [14] introduced a technique for creating videos synchronized with audio, but despite its remarkable aspects, the variety in the videos it produces is significantly constrained.\n' +
      '\n' +
      '### Multimodal Joint Generation\n' +
      '\n' +
      'Some research has already begun exploring the area of Multimodal Joint Generation [36, 50]. MM-Diffusion [36] introduces the first framework for simultaneous audio-video generation, designed to synergistically enhance both visual and auditory experiences cohesively and engagingly. However, it\'s unconditional and can only generate results in the training set domain, which would limit generation diversity. MovieFactory [50] employs ChatGPT to elaborately expand user-input text into detailed sequential scripts for generating movies, which are then vividly actualized both visually and acoustically through vision generation and audio retrieval techniques. However, a notable constraint of MovieFactory lies in its reliance on audio retrieval, limiting its capacity to generate audio that is more intricately tailored to the specific scenes.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '#### 3.1.1 Latent diffusion models\n' +
      '\n' +
      'We adopt latent-based diffusion models (LDM) for our generation model. The diffusion process follows the standard formulation in DDPM [21] that consists of a forward diffusion and a backward denoising process. Given a data sample \\(\\mathbf{x}\\sim p(\\mathbf{x})\\), an autoencoder consisting an encoder \\(\\mathcal{E}\\) and a decoder \\(\\mathcal{D}\\) first project the \\(\\mathbf{x}\\) into latent \\(\\mathbf{z}\\) via \\(\\mathbf{z}=\\mathcal{E}(\\mathbf{x})\\). Then, the diffusion and denoising process are conducted in the latent space. Once the denoising is completed at timestep 0, the sample will be decoded via \\(\\mathbf{x}=\\mathcal{D}(\\tilde{\\mathbf{z}_{0}})\\). The forward diffusion is a fixed Markov process of \\(T\\) timesteps that yields latent variables \\(\\mathbf{z}_{t}\\) based on the latent variable at previous timestep \\(\\mathbf{z}_{t-1}\\) via\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{1-\\beta_ {t}}\\mathbf{z}_{t-1},\\beta_{t}\\mathbf{I}), \\tag{1}\\]\n' +
      '\n' +
      'where \\(\\beta_{t}\\) is a predefined variance at each step \\(t\\). Finally, the clean data \\(\\mathbf{z}_{0}\\) becomes \\(\\mathbf{z}_{T}\\), which is indistinguishable from a Gaussian noise. The \\(\\mathbf{z}_{t}\\) can be directly derived from \\(\\mathbf{z}_{0}\\) in a closed form:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{\\bar{ \\alpha}_{t}}\\mathbf{z}_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}), \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}\\), and \\(\\alpha_{t}=1-\\beta_{t}\\). Leveraging the reparameterization trick, the \\(\\mathbf{z}_{t}\\) can be computed via\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0}+(1-\\bar{\\alpha}_{t})\\epsilon, \\tag{3}\\]\n' +
      '\n' +
      'where \\(\\epsilon\\) is a random Gaussian noise. The backward denoising process leverages a trained denoiser \\(\\theta\\) to obtain less noisy data \\(\\mathbf{z}_{t-1}\\) from the noisy input \\(\\mathbf{z}_{t}\\) at each timestep:\n' +
      '\n' +
      '\\[p_{\\theta}\\left(\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t}\\right)=\\mathcal{N}\\left( \\mathbf{z}_{t-1};\\mu_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right),\\boldsymbol{ \\Sigma}_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\right). \\tag{4}\\]\n' +
      '\n' +
      'Here \\(\\mu_{\\theta}\\) and \\(\\boldsymbol{\\Sigma}_{\\theta}\\) are determined through a denoiser network \\(\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\), where \\(p\\) represents input prompt. The training objective of \\(\\theta\\) is a noise estimation loss, formulated as\n' +
      '\n' +
      '\\[\\min_{\\theta}\\mathbb{E}_{t,\\mathbf{z}_{t},\\epsilon}\\left\\|\\epsilon-\\epsilon_{ \\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\right\\|_{2}^{2}. \\tag{5}\\]\n' +
      '\n' +
      '#### 3.1.2 Classifier guidance\n' +
      '\n' +
      'Classifier guidance [10] is a conditional generation mechanism that leverages the unconditional diffusion model to generate samples with the desired category. Given an unconditional diffusion model \\(p_{\\theta}(\\mathbf{z}_{t}|\\mathbf{z}_{t+1})\\), in order to condition it on a class label \\(y\\), it can be approximated via\n' +
      '\n' +
      '\\[p_{\\theta,\\phi}(\\mathbf{z}_{t}|\\mathbf{z}_{t+1},y)=\\mathcal{Z}p_{\\theta}( \\mathbf{z}_{t}|\\mathbf{z}_{t+1})p_{\\phi}(y|\\mathbf{z}_{t},t), \\tag{6}\\]\n' +
      '\n' +
      'where \\(\\mathcal{Z}\\) is a constant coefficient for normalization, \\(\\phi\\) is a trained time-aware noisy classifier for the approximation of label distribution of each sample of \\(\\mathbf{z}_{t}\\). The guidance from the classifier \\(\\phi\\) is the gradient of \\(\\mathbf{z}_{t}\\) with respect to y and is applied to the original \\(\\mathbf{z}_{t}\\) predicted from \\(\\epsilon_{\\theta}\\):\n' +
      '\n' +
      '\\[\\hat{\\epsilon}(\\mathbf{z}_{t})=\\epsilon_{\\theta}(\\mathbf{z}_{t})-\\sqrt{1-\\bar{ \\alpha}_{t}}\\triangledown_{\\mathbf{z}_{t}}\\log p_{\\phi}(y|\\mathbf{z}_{t}). \\tag{7}\\]\n' +
      '\n' +
      '#### 3.1.3 Linking multiple modalities\n' +
      '\n' +
      'We aim to force the generated samples in different modalities to become closer in a joint semantic space. To achieve this goal, we choose ImageBind [17] as the aligner since it learns an effective joint embedding space for multiple modalities. ImageBind learns a joint semantic embedding space that binds multiple different modalities including image, text, video, audio, depth, and thermal. Given a pair of data with different modalities (\\(M_{1},M_{2}\\)), e.g., (video, audio), the encoder of the corresponding modality \\(\\mathbf{E}_{i}\\) takes the data as input and predicts its embedding \\(\\mathbf{e}_{i}\\). The ImageBindis trained with a contrastive learning objective formulated as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{M_{1},M_{2}}=-\\log\\frac{\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{i} /\\tau)}{\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{i}/\\tau)+\\sum_{j\\neq i}\\exp( \\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{j}/\\tau)}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\tau\\) is a temperature factor to control the smoothness of the Softmax distribution, and \\(j\\) represents the negative sample, which is the data from another pair. By projecting samples of different modalities into embeddings in a shared space, minimizing the distance of the embeddings from the same data pair, and maximizing the distance of the embeddings from different data pairs, the ImageBind model achieves semantic alignment capability and thus can be served as a desired tool for multimodal alignment.\n' +
      '\n' +
      '### Diffusion Latent Aligner\n' +
      '\n' +
      '#### 3.2.1 Problem formulation\n' +
      '\n' +
      'Consider two modalities \\(M_{1},M_{2}\\), where \\(M_{2}\\) is the conditional modality and \\(M_{1}\\) is the generative modality. Given a latent diffusion model (LDM) \\(\\theta\\) that produces data of \\(M_{1}\\), our objective is to leverage the information from the condition \\(\\mathbf{x}^{M_{2}}\\sim p(\\mathbf{x}^{M_{2}})\\) to steer the generation process to a desired content, i.e., aligned the intermediate generative content with the input condition. To achieve this goal, we devise a diffusion latent aligner that guides the intermediate noisy latent towards a target direction to the content that the condition depicted during the denoising process. Formally, given a sequence of latent variables \\(\\mathbf{z}_{t},\\mathbf{z}_{t-1},...,\\mathbf{z}_{0}\\) from an LDM, the diffusion latent aligner \\(\\mathcal{A}\\) takes the corresponding latent \\(\\mathbf{z}_{t}\\) at arbitrary timestep \\(t\\) alongside the guided condition \\(\\mathbf{x}^{M_{2}}\\), and produce a modified latent \\(\\hat{\\mathbf{z}}_{t}\\) which has better alignment with the condition.\n' +
      '\n' +
      '\\[\\hat{\\mathbf{z}}_{t}^{M_{1}}=\\mathcal{A}(\\mathbf{z}_{t}^{M1},\\mathbf{x}^{M_{2 }}). \\tag{9}\\]\n' +
      '\n' +
      'For joint visual-audio generation, the aligner should simultaneously obtain information from the two modalities and provide guidance signals to these latents:\n' +
      '\n' +
      '\\[(\\hat{\\mathbf{z}}_{t}^{M_{1}},\\hat{\\mathbf{z}}_{t}^{M_{2}})=\\mathcal{A}( \\mathbf{z}_{t}^{M_{1}},\\mathbf{z}_{t}^{M_{2}}). \\tag{10}\\]\n' +
      '\n' +
      'After the sequential denoising process, the goal of our aligner is to minimize the \\(\\mathcal{F}(\\mathcal{D}(\\mathbf{z}_{0}^{M_{1}}),\\mathbf{x}^{M_{2}})\\), for unidirectional guidance, and \\(\\mathcal{F}(\\mathcal{D}(\\mathbf{z}_{0}^{M_{1}}),\\mathcal{D}(\\mathbf{z}_{0}^{ M_{2}}))\\) for synchronized bidirectional guidance, where \\(\\mathcal{F}\\) indicates a distance function to measure the degree of alignment between samples with two modalities. The updatable parameters in this process can be latent variables, embedding vectors, or neural network parameters.\n' +
      '\n' +
      '#### 3.2.2 Multimodal guidance\n' +
      '\n' +
      'To design such a latent aligner stated in Section 3.2.1, we propose a training-free solution that leverages the great capability of a multimodal model trained for representation learning, i.e., ImageBind [17] to provide rational guidance on the denoising process. Specifically, given latent variables \\(\\mathbf{z}_{t}\\) at each timestep \\(t\\), the predicted \\(\\mathbf{z}_{0}\\) can be derived from \\(\\mathbf{z}_{t}\\) and the predicted noise \\(\\hat{e}\\) via\n' +
      '\n' +
      '\\[\\tilde{\\mathbf{z}}_{0}=\\mathcal{G}(\\mathbf{z}_{t})=\\frac{1}{\\sqrt{\\hat{ \\alpha}_{t}}}\\mathbf{z}_{t}-\\sqrt{\\frac{1-\\tilde{\\alpha}_{t}}{\\tilde{\\alpha}_ {t}}}\\hat{\\epsilon}, \\tag{11}\\]\n' +
      '\n' +
      'where \\(\\hat{\\epsilon}=\\epsilon_{\\theta}(\\mathbf{z}_{t},t)\\). With such a clean prediction, we can leverage the external models that are trained on normal data without retraining them on noisy data like the classifier guidance is needed. We feed the \\(\\mathbf{z}_{0}\\) and the guiding condition to the ImageBind model to compute their distance in\n' +
      '\n' +
      'Figure 2: **The proposed diffusion latent aligner.** During the denoising process of generating one specific modality (visual/audio), we adopt the condition information (audio/video) to guide the denoising process. By leveraging the pretrained ImageBind model, we calculate the distance of the generative latent \\(\\mathbf{z}_{t}^{M_{1}}\\) with the condition \\(\\mathbf{z}_{0}^{M_{2}}\\) in the shared embedding space of ImageBind. Then we backpropagate the distance value to obtain the gradient of \\(\\mathbf{z}_{t}^{M_{1}}\\) with respect to the distance.\n' +
      '\n' +
      'the ImageBind embedding space. The obtained distance can then serve as a penalty, which can be used to backpropagate the computation graph and obtain a gradient on the latent variable \\(\\mathbf{z}_{t}\\):\n' +
      '\n' +
      '\\[\\mathcal{L}(\\tilde{\\mathbf{z}}_{0},\\mathbf{x}^{M_{2}})=1-\\mathcal{F}(\\mathbf{E}^ {M_{1}}(\\tilde{\\mathbf{z}}_{0}),\\mathbf{E}^{M_{2}}(\\mathbf{x}^{M_{2}})). \\tag{12}\\]\n' +
      '\n' +
      'Then we update the \\(\\mathbf{z}_{t}\\) via\n' +
      '\n' +
      '\\[\\hat{\\mathbf{z}}_{t}=\\mathbf{z}_{t}-\\lambda_{1}\\nabla_{\\mathbf{z}_{t}} \\mathcal{L}(\\mathcal{D}(\\tilde{\\mathbf{z}}_{0}),\\mathbf{x}^{M_{2}}), \\tag{13}\\]\n' +
      '\n' +
      'where \\(\\lambda_{1}\\) serves as the learning rate of each optimization step. In this way, we alter the sampling trajectory at each timestep through our multimodal guidance signal to achieve both audio-to-visual and visual-to-audio. This procedure only costs a small amount of extra sampling time, without any additional datasets and expensive network training.\n' +
      '\n' +
      '#### 3.2.3 Dual/Triangle loss function\n' +
      '\n' +
      'We observed that audio often lacks enough semantic information such as some audio is pure background music, while the paired video contains rich semantic information such as multiple objects and environment sound. Using this type of condition to guide visual generation is not enough and may provide useless guidance information. To solve this, we incorporate another modality, e,g., text, to provide a comprehensive measurement as\n' +
      '\n' +
      '\\[\\mathcal{L}_{a2v}=\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{\\mathbf{a}})+ \\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{\\mathbf{p}}). \\tag{14}\\]\n' +
      '\n' +
      'The \\(\\mathbf{e}_{v}\\), \\(\\mathbf{e}_{a}\\) and \\(\\mathbf{e}_{p}\\) are the corresponding embeddings in the multimodal space of ImageBind. The \\(\\mathcal{F}\\) represents the distance function between two embedding vectors which is one minus cosine similarity between them. Similarly, the loss for V2A can be written as\n' +
      '\n' +
      '\\[\\mathcal{L}_{v2a}=\\mathcal{F}(\\mathbf{e}_{a},\\mathbf{e}_{\\mathbf{v}})+ \\mathcal{F}(\\mathbf{e}_{a},\\mathbf{e}_{\\mathbf{p}}). \\tag{15}\\]\n' +
      '\n' +
      'For visual-audio joint generation, the loss turns into a triangle:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{joint-va}}=\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{p})+ \\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{a})+\\mathcal{F}(\\mathbf{e}_{a},\\mathbf{ e}_{p}). \\tag{16}\\]\n' +
      '\n' +
      'The text can be input by the user to provide a user-guided interactive system or can be extracted via audio captioning models. As stated before, the audio tends to present incomplete semantic information. Thus, the extracted caption should be worse than that. However, we empirically find that our approach helps to correct these semantic errors, and improves the semantic alignment.\n' +
      '\n' +
      '#### 3.2.4 Guided prompt tuning\n' +
      '\n' +
      'Using the aforementioned multimodal latent guidance, we successfully achieved good generation quality and better content alignment on visual-to-audio generation. However, we observed that when applying this approach to audio-to-visual generation, the guidance has a neglectable effect. Meanwhile, when leveraging the audio to generate corresponding audios, the generated video becomes less temporal consistent due to the gradient of each frame having no ensure of temporal coherence. Therefore, to overcome this issue, we further propose guided prompt tuning by optimizing the input text embedding vector of the generative model, which is formulated as\n' +
      '\n' +
      '\\[\\hat{\\mathbf{y}}=\\mathbf{y}-\\lambda_{2}\\nabla_{\\mathbf{y}}\\mathcal{L}. \\tag{17}\\]\n' +
      '\n' +
      'The \\(\\lambda_{2}\\) indicates the learning rate for the prompt embedding. Specifically, we detach the prompt text embedding at the beginning of predicting the noise and retain a computational graph from the text embedding to the calculation of multimodal loss. Then we backpropagate the computational graph to obtain the gradient of the prompt embedding _w.r.t._ the multimodal loss. The updated embedding is shared across all timesteps to provide consistent semantic guidance information.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Dataset** We utilize the VGGSound dataset [6] and Landscape dataset [36] for evaluation on video-to-audio, audio-to-video, and audio-video joint generation task. Since our method is an optimization-based solution, there is no need to utilize the entire dataset for evaluation. Instead, we randomly sample 3k video-audio pairs from the VGGSounddataset for video-to-audio generation, 3k pairs for audio-to-video generation, and 3k pairs for image-to-audio generation respectively. We extract the key frame from each video for the image-to-audio generation task. We also randomly sample 200 video-audio pairs from the Landscape dataset for video-audio joint generation.\n' +
      '\n' +
      '**Implementation details** We utilize the pretrained AudioLDM [29] for video-to-audio and image-to-audio generation, the AnimateDiff [18] for audio-to-video generation. We use both the pre-trained AudioLDM and AnimateDiff for the joint audio-video generation. We set the denoising step to 30 for video-to-audio generation, 25 for audio-to-video generation, and 25 for audio-video joint generation, respectively. We use the learning rate 0.1 for guiding the AudioLDM denoising and 0.01 for guiding the AnimateDiff denoising, which applies to all the tasks. We fixed the random seed of the optimization process for fair comparisons. All the experiments are conducted on NVIDIA Geforce RTX 3090 GPUs.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**Video-to-Audio** We choose SpecVQGAN [26] as the baseline of Video-to-Audio generation task. We used the pre-trained model, which was trained using ResNet-50 with 5 features on VGGSound [26] as our inference model and compared our method with SpecVQGAN on 3k VGGSound sample datasets.\n' +
      '\n' +
      '**Image-to-Audio** We choose Im2Wav as the baseline of the Image-to-Audio generation task and used the pre-trained model provided by the authors [37], test on 3k Paprika style transferred VGGSound samples transferred by AnimeGANv2 [8].\n' +
      '\n' +
      '**Audio-to-Video** We choose TempoTokens as the baseline of the Audio-to-Video generation task and used the pre\n' +
      '\n' +
      'Figure 3: Compared with baseline on the video-to-audio generation task. SpecVQGAN fails to generate realistic and aligned audio with the input video. Our method can produce aligned audio with the input video rhythm.\n' +
      '\n' +
      'trained model provided by the authors [46], test on 3k VG-GSound samples.\n' +
      '\n' +
      '**Joint video and audio generation** As MM-Diffusion [36] is the state-of-the-art of unconditional video and audio joint generation task, We choose it as the baseline of unconditional video and audio joint generation task in the limit Landscape domain with 200 Landscape samples using the model pre-trained on Landscape datasets. On the open domain, we compare our Ours-with-guidance model with the Ours-vanilla model, as, to the best of our knowledge, there is no established baseline for this task.\n' +
      '\n' +
      '**Ours-Vanilla** We design several vanilla models of our tasks with the combination of existing tools. For the video-to-audio task, we extract the key frame [27] and use a pre\n' +
      '\n' +
      'Figure 4: Compared with baseline on the joint video-and-audio generation task. Our method can produce better text-aligned visual content than the vanilla model. Besides, our generated audio is also of better quality and better alignment with the generated videos.\n' +
      '\n' +
      'Figure 5: Compared with baseline on the audio-to-video task. Given the input audio, the generated videos by TempoToken are not aligned with the input audio and the generation with poor visual quality. Our method can produce visually much better and semantically aligned content with the input condition.\n' +
      '\n' +
      'trained image caption model [3] to obtain the caption for the video. We then use the extracted caption to generate audio with the AudioLDM model. For the audio-to-video task, we use an audio caption model and feed the extracted caption to the AnimateDiff to generate the videos for the input audio. For the joint audio and video generation task, we directly take the test prompt as input to the AudioLDM model and AnimateDiff model to compose the joint generation results.\n' +
      '\n' +
      '### Visual-to-Audio Generation\n' +
      '\n' +
      'Visual-to-audio generation includes video-to-audio generation and image-to-audio generation tasks. The image-to-audio generation requires audio-visual alignment from the semantic level, whereas temporal alignment is additionally needed for video-to-audio generation. Moreover, the generated audio also needs to be high-fidelity. To quantitatively evaluate our performance on these aspects, we utilize the MKL metric [26] for audio-video relevance, Inception score (ISc), Frechet distance (FD), and Frechet audio distance (FAD) for audio fidelity evaluation. From Tab. 1, we can see that even though our method is training-free, we can still outperform the baseline which requires large-scale training on audio-video pairs. When compared with the text-to-audio baseline, we could see that our method consistently improves the audio-video relevance and the audio generation quality. When compared with our vanilla baseline, we find our method can significantly improve the audio quality, especially by reducing irrelevant sound and background noise, as shown in Fig. 6.\n' +
      '\n' +
      '### Audio-to-Video Generation\n' +
      '\n' +
      'Audio-to-video generation requires the generated videos to be high-quality, as well as semantically and temporally aligned with the input audio. To quantitatively evaluate the visual quality of the generated videos, we adopt the Frechet Video Distance (FVD) and Kernel Video Distance (KVD) [41] as the metrics. We also use the audio-video alignment (AV-align) [46] metric to measure the alignment of the generated video and the input audio. We show our quantitative results in Tab. 1. We observe that our training-free method can outperform the training-based baseline in terms of both semantic alignment and video quality. Besides, compared with the text-to-video method, our method can achieve better audio-video alignment while maintaining a comparable visual quality performance. We show our qualitative results in Fig. 5. We observe that TempoToken struggles with visual quality and audio-visual alignment, and thus the generated videos are not relevant to the input audio and the generated quality is relatively poor. Although the text-to-video method can achieve good performance on the visual quality of the generated videos, it struggles to accurately align with the input audio content. Our training-free method, utilizing a shared audio-visual representation space, can achieve a good tradeoff between visual quality and audio-visual alignment.\n' +
      '\n' +
      '### Joint Video and Audio Generation\n' +
      '\n' +
      'The practical joint video and audio generation task should take the text as the input, produce high-fidelity videos and audio, maintain the audio-video alignment, and maintain the text-audio and text-video relevance. Specifically, we adopt the FVD for video quality, FAD for audio quality, AV-align [46] for audio-video relevance, TA-align for text-audio alignment, and the TV-align for text-video alignment. Our quantitative evaluation is shown in Tab. 1. Our latent aligner can be plugged into existing unconditional audio-video joint generation framework MM-Diffusion [36]. The results show that compared with the original MM-Diffusion, our latent aligner can boost the audio generation quality when maintaining the video generation performance. We also verify our method of text-conditioned joint video and audio generation. We bridge the video diffusion model AnimateDiff [18] and audio diffusion model AudioLDM [29] with our diffusion latent aligner. We randomly collect 100 prompts from the web to condition our generation. Compared with separate text-to-video and text-to-audio models, our aligner can improve text-video alignment, text-audio alignment, and video-audio alignment. We show the qualitative comparison in Fig. 4. More qualitative results can be found in the Supplementary.\n' +
      '\n' +
      'Figure 6: Compared with our vanilla model on the video-to-audio generation task. Our method can significantly reduce the background and irrelevant sound and thus achieve better audio quality, which is also reflected in Tab. 1.\n' +
      '\n' +
      'Figure 7: We visualize the effect of our guided prompt tuning. The automatic caption generated is “frozen 2 - screenshot”, which fails to capture the meaningful visual content, and thus, the text-to-audio method fails to produce meaningful sounds. Our prompt tuning can inspect the visual information to complement the semantic information to generate meaningful sounds.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '* [17] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15180-15190, 2023.\n' +
      '* [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [20] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. _arXiv preprint arXiv:2307.06940_, 2023.\n' +
      '* [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.\n' +
      '* [24] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation. _arXiv preprint arXiv:2305.18474_, 2023.\n' +
      '* [25] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. _arXiv preprint arXiv:2301.12661_, 2023.\n' +
      '* [26] Vladimir Isahin and Esa Rahtu. Taming visually guided sound generation. _arXiv preprint arXiv:2110.08791_, 2021.\n' +
      '* [27] KeplerLab. Tool for automating common video key-frame extraction, video compression and image auto-crop/image-resizee tasks. [https://github.com/keplerlab/katna](https://github.com/keplerlab/katna), 2021.\n' +
      '* [28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. _arXiv preprint arXiv:2209.15352_, 2022.\n' +
      '* [29] Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. _arXiv preprint arXiv:2301.12503_, 2023.\n' +
      '* [30] Haobe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. _arXiv preprint arXiv:2308.05734_, 2023.\n' +
      '* [31] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. _arXiv preprint arXiv:2306.17203_, 2023.\n' +
      '* [32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '* [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE, 2022.\n' +
      '* [36] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10219-10228, 2023.\n' +
      '* [37] Roy Sheffer and Yossi Adi. I hear your true colors: Image guided audio generation. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.\n' +
      '* [38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [39] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, and Chuang Gan. Physics-driven diffusion models for impact sound synthesis from videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9749-9759, 2023.\n' +
      '* [40] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14214-14223, 2023.\n' +
      '* [41] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '** [42] Chia-Hung Wan, Shun-Po Chuang, and Hung-Yi Lee. Towards audio to scene image synthesis using generative adversarial network. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 496-500. IEEE, 2019.\n' +
      '* [43] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning robust audio representations from clip. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4563-4567. IEEE, 2022.\n' +
      '* [44] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2023.\n' +
      '* [45] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. _arXiv preprint arXiv:2309.16429_, 2023.\n' +
      '* [46] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. _arXiv preprint arXiv:2309.16429_, 2023.\n' +
      '* [47] Maciej Zelaszczyk and Jacek Mandziuk. Audio-to-image cross-modal generation. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2022.\n' +
      '* [48] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [49] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [50] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and Jianlong Fu. Moviefactory: Automatic movie creation from text using large generative models for language and images. _arXiv preprint arXiv:2306.07257_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>