<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 보기 및 듣기: 확산 잠재 정렬기를 이용한 Open-domain Visual-Audio 생성\n' +
      '\n' +
      'Yazhou Xing\\({}^{1}\\) Yingqing He\\({}^{1}\\) Zeyue Tian\\({}^{1}\\) Xintao Wang\\({}^{2}\\) Qifeng Chen\\({}^{1}\\)\n' +
      '\n' +
      'HKUST \\({}^{2}\\)ARC Lab, Tencent PCG\n' +
      '\n' +
      'equal contribution\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '비디오 및 오디오 콘텐츠 생성은 영화 산업 및 전문 사용자의 핵심 기술 역할을 한다. 최근, 기존의 확산 기반 방법은 비디오와 오디오 생성을 별도로 다루기 때문에 학계에서 산업으로의 기술 이전을 방해한다. 이 작업에서는 교차-시각-음성 및 공동-시각-음성 생성을 위해 신중하게 설계된 최적화 기반 프레임워크를 사용하여 격차를 채우는 것을 목표로 한다. 우리는 기성 비디오 또는 오디오 생성 모델의 강력한 생성 능력을 관찰합니다. 따라서, 우리는 거대 모델들을 처음부터 훈련시키는 대신에, 기존의 강한 모델들을 공유된 잠재 표현 공간으로 브릿지하는 것을 제안한다. 구체적으로, 우리는 미리 훈련된 ImageBind 모델을 갖는 다중 모드 잠재 정렬기를 제안한다. 잠재 정렬기는 추론 시간 동안 확산 잡음 제거 프로세스를 안내하는 분류기 안내와 유사한 코어를 공유한다. 세심하게 설계된 최적화 전략과 손실 함수를 통해 공동 비디오-오디오 생성, 비주얼-스티어링 오디오 생성 및 오디오-스티어링 비주얼 생성 작업에서 제안된 방법의 우수한 성능을 보여준다. 프로젝트 웹사이트는 [https://yzxing87.github.io/Seeing-and-Hearing/](https://yzxing87.github.io/Seeing-and-Hearing/)에서 확인할 수 있다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 인공지능에 의해 생성된 콘텐츠는 사용자로부터의 입력 설명에 기초하여 다양하고 사실적인 이미지[4, 9, 22, 32, 34], 비디오[4, 7, 15, 19, 20, 22, 38], 또는 사운드[25, 28, 29, 30, 44]를 생성하는 데 상당한 발전을 이루었다. 그러나 기존의 연구는 주로 현실 세계의 복합적 성격을 무시하고 단일 모달리티 내에서 콘텐츠를 생성하는 데 초점을 맞추고 있다. 결과적으로, 생성된 비디오들은 수반되는 오디오가 부족하고, 생성된 오디오는 동기화된 시각적 효과들이 부족하다. 이러한 연구 격차는 시각적, 청각적 양식의 동시 창작이 필요한 영화를 제작하는 것과 같이 사용자가 더 큰 영향을 미치는 콘텐츠를 제작하는 것을 제한한다. 본 연구에서는 비디오와 오디오 콘텐츠 제작을 위한 비주얼 오디오 생성 과제를 연구한다.\n' +
      '\n' +
      '이 문제에 대한 한 가지 잠재적인 해결책은 시각적 및 오디오 콘텐츠를 두 단계로 생성하는 것이다. 예를 들어, 사용자들은 먼저 기존의 텍스트-투-비디오(T2V) 모델들을 활용하는 입력 텍스트 프롬프트에 기초하여 비디오를 생성할 수 있다[7, 18]. 그런 다음, V2A(video-to-audio) 모델이 채용되어 정렬된 오디오를 생성할 수 있다. 대안적으로, 텍스트-대-오디오(T2A) 및 오디오-대-비디오(A2V) 모델들의 조합이 페어링된 시각적-오디오 콘텐츠를 생성하기 위해 사용될 수 있다. 그러나, 기존의 V2A 및 A2V 생성 방법[26, 46]은 특정 다운스트림 도메인에 대한 제한된 능력을 갖거나 열악한 생성 성능을 나타낸다. 더욱이, 공동 비디오-오디오 생성(Joint-VA)의 작업은 제한된 주목을 받았으며, 기존의 작업 [36]은 작은 도메인 내에서도 제한된 생성 성능을 보이며 의미 제어도 부족하다.\n' +
      '\n' +
      '본 논문에서는 오픈 도메인 비주얼 오디오 생성을 위한 새로운 생성 패러다임을 제안한다. 우리는 (1) 우수한 성능을 보여주는 잘 훈련된 단일 양식 텍스트 조건 생성 모델이 있다는 것을 관찰한다. 이러한 사전 훈련된 모델을 활용하면 각 촬영장비를 합성하기 위한 값비싼 훈련을 피할 수 있다. (2) 사전 학습된 모델 ImageBind[17]은 공유된 의미 공간 내에서 서로 다른 데이터 모달리티 간의 효과적인 연결을 확립하는 데 현저한 능력을 가지고 있음을 알 수 있었다. 우리의 목표는 이미지바인을 브리지로 활용하여 다양한 양식을 효과적으로 연결하고 통합할 수 있는 방법을 탐색하는 것입니다.\n' +
      '\n' +
      '이러한 관찰을 활용하여 서로 다른 양식의 확산 잠재 공간에서 ImageBind를 얼라이너로 활용할 것을 제안한다. 하나의 모달리티를 생성하는 동안, 우리는 생성 과정에 영향을 미치는 유도 신호를 생성하기 위해 다른 모달리티의 잡음 잠재 및 유도 조건을 얼라이너에 입력한다. 디노이징 프로세스에 가이드를 점진적으로 주입함으로써, 생성된 콘텐츠를 ImageBind 임베딩 공간에서 입력 조건에 더 가깝게 브리지한다. Joint-VA 생성을 위해 두 양식의 생성 과정에 영향을 미치는 지침을 양방향으로 만든다.\n' +
      '\n' +
      '우리의 설계로, 우리는 미리 훈련된 단일 모달리티 생성 모델을 유기적인 시스템으로 성공적으로 연결하고 다재다능하고 유연한 시각적 오디오 생성을 달성했다. 또한, 우리의 접근 방식은 대규모 데이터 세트에 대한 교육을 필요로 하지 않으므로 우리의 접근 방식을 매우 자원 친화적으로 만든다. 우리의 접근법의 일반성과 저렴한 비용 외에도, 우리는 네 가지 작업에서 우리의 성능을 검증하고 기본 접근법에 비해 우월함을 보여준다.\n' +
      '\n' +
      '요약하면, 우리의 주요 기여는 다음과 같다:\n' +
      '\n' +
      '* 우리는 시청각 생성을 달성하기 위해 단일 모달리티의 _bridges_ 사전 훈련된 확산 모델을 함께 제공하는 새로운 패러다임을 제안한다.\n' +
      '* 멀티모달 임베딩 공간에서 시각 및 오디오 모달리티의 확산 잠재성을 점진적으로 정렬하기 위해 _diffusion latent aligner_를 소개한다.\n' +
      '* 우리는 V2A, I2A, A2V 및 Joint-VA를 포함한 네 가지 작업에 대해 광범위한 실험을 수행하여 접근법의 우수성과 일반성을 입증한다.\n' +
      '* 우리가 아는 한, 우리는 텍스트 유도 공동 비디오-오디오 생성을 위한 첫 번째 작업을 제시한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '조건부 오디오 생성\n' +
      '\n' +
      '오디오 생성은 다양한 오디오 콘텐츠의 생성을 모델링하는 데 중점을 둔 새로운 분야이다. 이것은 텍스트[11, 16, 24, 25, 28, 44], 이미지[37], 및 비디오[12, 26, 31, 39]와 같은 다양한 입력들에 컨디셔닝된 오디오를 생성하는 것과 같은 태스크들을 포함한다.\n' +
      '\n' +
      '텍스트-오디오 연구 분야에서 AudioGen[28]은 이산 오디오 표현에서 작동하는 자동 회귀 생성 모델을 제안하고, DiffSound[44]는 자동 회귀 모델에서 단방향 생성의 한계를 해결하기 위해 비자동 회귀 토큰-디코더를 사용한다. Make-An-Audio[25], AudioLDM[29]와 같은 일부 다른 작업은 오디오 생성을 위해 잠재 확산 방법을 사용한다. Make-an-Audio2[24], AudioLDM2[30], TANGO[16]와 같은 최근의 몇몇 연구들은 오디오 생성 모델의 성능을 향상시키기 위해 LLM(Large Language Models)을 활용하고 있다.\n' +
      '\n' +
      'Im2Wav[37] 및 SpecVQGAN[26]과 같은 작품으로 예시되는 이미지와 비디오를 조건으로 하는 오디오 생성에 초점을 맞춘 연구도 학계 내에서 상당한 관심을 끌었다. 시각적 표현(Contrastive Language-Image Pre-training)을 위해 미리 훈련된 CLIP 모델의 시맨틱스를 이용하는 [33], Im2Wav[37]은 먼저 언어 모델을 통해 기초적인 오디오 표현을 공예한 다음, 이러한 오디오 토큰들을 고충실도 사운드 샘플들로 업샘플링하기 위해 추가적인 언어 모델을 채용한다. SpecVQGAN[26]은 입력 비디오 특징들에 기초하여 미리 트레이닝된 코드북으로부터 새로운 스펙트로그램들을 생성하기 위해 트랜스포머를 이용한다. 그런 다음 미리 훈련된 보코더를 사용하여 이러한 스펙트로그램에서 파형을 재구성한다.\n' +
      '\n' +
      '조건부 시각 생성\n' +
      '\n' +
      '텍스트-이미지 생성의 과제는 최근 몇 년 동안 상당한 발전과 성과를 보았다[2, 35, 40]. 이러한 진보는 오디오-투-이미지 생성에 초점을 맞춘 새로운 연구 영역에 대한 관심을 불러일으켰다. 2019년 [42]는 Generative Adversarial Networks (GANs)를 이용하여 오디오 레코딩으로부터 이미지를 생성하는 방법을 제안하였다. [47] 오디오 입력을 사용하여 MNIST 자리의 이미지를 생성하는 데 좁게 초점을 맞추고 일반적인 오디오 사운드에서 이미지 생성으로 확장되지 않았다. 대조적으로, [42]에 의한 접근법은 더 넓은 범위의 오디오 신호로부터 이미지를 생성할 수 있었다. Wav2CLIP[43]은 오디오-이미지 쌍들에 대한 조인트 표현들을 학습하기 위해 CLIP에서 영감을 받은 접근법을 채택하며, 이는 후속적으로 VQ-GAN[13]을 사용하여 이미지 생성을 용이하게 할 수 있다. 텍스트 투 비디오는 또한 최근 주목할 만한 진전을 이루었다[1, 4, 7, 15, 19].\n' +
      '\n' +
      '도 1: **Overview.** 우리의 접근법은 다재다능하며, 공동 비디오-오디오 생성(Joint-VA), 비디오-투-오디오(V2A), 오디오-투-비디오(A2V) 및 이미지-투-오디오(I2A)의 네 가지 과제를 해결할 수 있다. 사전 훈련된 이미지바인더와 같은 멀티모달 바인더를 활용하여 단일 모달리티를 생성하기 위해 설계된 고립된 생성 모델 간의 연결을 설정한다. 이를 통해 양방향 조건부 및 공동 비디오/오디오 생성을 모두 달성할 수 있다.\n' +
      '\n' +
      ' 22, 23, 25, 48, 49]는 비디오 확산 모델들에 의해 권한 부여되었다[23]. 주류 아이디어는 U-Net 아키텍처에 시간 모델링 모듈을 통합하여 비디오 픽셀 공간[22, 23] 또는 잠재 공간[4, 19]에서 시간 역학[1, 19, 23, 38, 49]을 학습하는 것이다. 본 연구에서는 오픈 소스 잠재 기반 텍스트 대 비디오 모델을 비디오 생성 상대에 대한 기본 모델로 활용한다. Sound2sight[5], TATS[14], Tempotokens[45]와 같은 오디오 투 비디오 작업도 있습니다. [5]는 오디오와 정렬되는 방식으로 비디오를 확장하는 것에 초점을 맞추지만, Tempotokens[45]는 오디오 입력으로부터 비디오들을 배타적으로 생성함으로써 상이한 접근법을 취한다. TATS[14]는 오디오와 동기화된 비디오를 생성하는 기술을 도입했지만, 그 놀라운 측면에도 불구하고, 그것이 생산하는 비디오의 다양성은 상당히 제한된다.\n' +
      '\n' +
      '### 멀티모달 조인트 생성\n' +
      '\n' +
      '일부 연구는 이미 멀티모달 조인트 세대의 영역을 탐구하기 시작했다[36, 50]. MM-Diffusion[36]은 시각적 및 청각적 경험 모두를 응집적이고 매력적으로 상승적으로 향상시키도록 설계된 동시 오디오-비디오 생성을 위한 첫 번째 프레임워크를 도입한다. 그러나 이는 무조건적이며 학습 집합 도메인에서만 결과를 생성할 수 있으므로 생성 다양성을 제한할 수 있다. 영화공장[50]은 ChatGPT를 사용하여 사용자 입력 텍스트를 영화 생성을 위한 세부 순차 스크립트로 정교하게 확장한 다음 비전 생성 및 오디오 검색 기술을 통해 시각적 및 청각적으로 생생하게 구현한다. 그러나 무비팩토리의 주목할 만한 제약은 오디오 검색에 의존하여 특정 장면에 더 복잡하게 맞춤화된 오디오를 생성하는 능력을 제한한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Preliminaries\n' +
      '\n' +
      '잠재 확산 모델 3.1.1\n' +
      '\n' +
      '우리는 잠재 기반 확산 모델(LDM)을 생성 모델에 채택한다. 확산 과정은 순방향 확산과 역방향 잡음 제거 과정으로 구성된 DDPM[21]의 표준 공식을 따른다. 데이터 샘플 \\(\\mathbf{x}\\sim p(\\mathbf{x})\\), 인코더 \\(\\mathcal{E}\\) 및 디코더 \\(\\mathcal{D}\\)으로 구성된 오토인코더가 \\(\\mathbf{x}\\)을 \\(\\mathbf{z}=\\mathcal{E}(\\mathbf{x})\\)을 통해 잠재 \\(\\mathbf{z}\\)으로 먼저 투영한다. 그런 다음 잠재 공간에서 확산 및 잡음 제거 과정을 수행한다. 일단 타임스텝 0에서 잡음제거가 완료되면, 샘플은 \\(\\mathbf{x}=\\mathcal{D}(\\tilde{\\mathbf{z}_{0})\\을 통해 디코딩될 것이다. 정방향 확산은 이전 타임스텝 (\\(\\mathbf{z}_{t-1}\\)에서 잠재변수에 기초하여 잠재변수 \\(\\mathbf{z}_{t}\\)를 산출하는 고정 마르코프 과정이다.\n' +
      '\n' +
      '\\mathbf{z}_{t}|\\mathbf{z}_{t-1})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{z}_{t-1},\\beta_{t}\\mathbf{I}), \\tag{1}\\\n' +
      '\n' +
      '여기서 \\(\\beta_{t}\\)는 각 단계 \\(t\\)에서 미리 정의된 분산이다. 마지막으로, 깨끗한 데이터\\(\\mathbf{z}_{0}\\)는 가우스 잡음과 구별할 수 없는 \\(\\mathbf{z}_{T}\\)이 된다. \\(\\mathbf{z}_{t}\\)는 닫힌 형태로 \\(\\mathbf{z}_{0}\\)으로부터 직접 유도될 수 있다:\n' +
      '\n' +
      '\\mathbf{z}_{t}|\\mathbf{z}_{0})=\\mathcal{N}(\\mathbf{z}_{t};\\sqrt{\\bar{\\alpha}_{t}\\mathbf{z}_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}), \\tag{2}\\tag{2}}\n' +
      '\n' +
      '여기서 \\(\\bar{\\alpha}_{t}=\\prod_{i=1}^{t}\\alpha_{i}\\), \\(\\alpha_{t}=1-\\beta_{t}\\). 재매개변수화 트릭을 활용하여 \\(\\mathbf{z}_{t}\\)를 통해 계산할 수 있다.\n' +
      '\n' +
      '\\[\\mathbf{z}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{z}_{0}+(1-\\bar{\\alpha}_{t})\\epsilon, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\epsilon\\)은 랜덤 가우시안 잡음이다. 백워드 디노이징 과정은 훈련된 디노이저\\(\\theta\\)을 평균하여 각 타임스텝에서 노이즈 입력\\(\\mathbf{z}_{t}\\)으로부터 더 적은 노이즈 데이터\\(\\mathbf{z}_{t-1}\\)를 얻는다:\n' +
      '\n' +
      '\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1};\\mu_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right),\\boldsymbol{\\Sigma}_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\mathcal{N}\\left(\\mathbf{z}_{t},t,p\\right)\\mathccal{N}\\left(\\mathbf{z}_{t-1}\\mid\\mathbf{z}_{t-1};\\mu_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right),\\boldsymbol{\\Sigma}_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\tag{4}\\\n' +
      '\n' +
      '여기서 \\(\\mu_{\\theta}\\)와 \\(\\boldsymbol{\\Sigma}_{\\theta}\\)은 디노이저 네트워크 \\(\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\)를 통해 결정되며, 여기서 \\(p\\)은 입력 프롬프트를 나타낸다. \\(\\theta\\)의 훈련 목적은 잡음 추정 손실로 공식화된다.\n' +
      '\n' +
      '[\\min_{\\theta}\\mathbb{E}_{t,\\mathbf{z}_{t},\\epsilon}\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\right\\|_{2}^{2}.\\tag{5}\\tag{5}\\epsilon-\\epsilon-\\epsilon-\\theta}\\left(\\mathbf{z}_{t},t,p\\right)\\right\\|_{2}^{2}.\n' +
      '\n' +
      '###### 3.1.2 분류기 안내\n' +
      '\n' +
      '분류기 안내 [10]은 무조건 확산 모델을 활용하여 원하는 범주로 샘플을 생성하는 조건부 생성 메커니즘이다. 무조건 확산 모델\\(p_{\\theta}(\\mathbf{z}_{t}|\\mathbf{z}_{t+1})\\)이 주어지면 클래스 레이블\\(y\\)에 조건화하기 위해 이를 통해 근사화할 수 있다.\n' +
      '\n' +
      '\\[p_{\\theta,\\phi}(\\mathbf{z}_{t}|\\mathbf{z}_{t+1},y)=\\mathcal{Z}p_{\\theta}(\\mathbf{z}_{t}|\\mathbf{z}_{t+1})p_{\\phi}(y|\\mathbf{z}_{t},t),\\tag{6}\\mathcal{Z}p_{\\theta}(\\mathbf{z}_{t},y)=\\mathcal{Z}p_{\\theta}(\\mathbf{z}_{t}}|\\mathbf{z}_{t+1})p_{\\phi}(y|\\mathbf{z}_{t},t),\\tag{6}\\mathcal{Z}p_{\\theta}(\\mathbff{z}_{t},y)=\\mathcal{Z}p_{\\theta}(\\mathbff{z}_{t}\n' +
      '\n' +
      '여기서 \\(\\mathcal{Z}\\)은 정규화를 위한 상수 계수이고, \\(\\phi\\)은 \\(\\mathbf{z}_{t}\\)의 각 샘플의 라벨 분포의 근사화에 대해 훈련된 시간 인식 잡음 분류기이다. 분류기 \\(\\phi\\)의 안내는 y에 대한 \\(\\mathbf{z}_{t}\\)의 기울기이며 \\(\\epsilon_{\\theta}\\)으로부터 예측된 원래의 \\(\\mathbf{z}_{t}\\)에 적용된다:\n' +
      '\n' +
      '\\[\\hat{\\epsilon}(\\mathbf{z}_{t})=\\epsilon_{\\theta}(\\mathbf{z}_{t})-\\sqrt{1-\\bar{\\alpha}_{t}\\triangledown_{\\mathbf{z}_{t}}\\log p_{\\phi}(y|\\mathbf{z}_{t}}}\\tag{7}\\t}\n' +
      '\n' +
      '다중 모달리티 연결\n' +
      '\n' +
      '우리는 생성된 샘플을 서로 다른 양식으로 강제하여 결합 의미 공간에서 더 가까워지도록 하는 것을 목표로 한다. 이 목적을 달성하기 위해 ImageBind[17]을 얼라이너로 선택하는데, 이는 여러 모달리티에 대한 효과적인 조인트 임베딩 공간을 학습하기 때문이다. ImageBind는 이미지, 텍스트, 비디오, 오디오, 깊이 및 열(thermal)을 포함하는 다수의 상이한 모달리티들을 바인딩하는 조인트 시맨틱 임베딩 공간을 학습한다. 서로 다른 모달리티(\\(M_{1},M_{2}\\)), 예를 들어 비디오, 오디오)를 갖는 한 쌍의 데이터가 주어지면, 해당 모달리티의 인코더(\\(\\mathbf{E}_{i}\\)는 데이터를 입력으로 하고 그 임베딩 \\(\\mathbf{e}_{i}\\)을 예측한다. 이미지빈디스는 다음과 같이 공식화된 대조적 학습 목표로 훈련된다:\n' +
      '\n' +
      '[\\mathcal{L}_{M}{1},M_{2}=-\\log\\frac{\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{i}/\\tau}{\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{i}/\\tau)+\\sum_{j\\neqi}\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}_{j}/\\tau}, \\tag{8}\\mathbf{k}_{i}^{\\intercal}\\tau},\\tag{8}\\mathbf{k}_{i}^{\\intercal}\\tau}{\\exp(\\mathbf{q}_{i}}^{\\intercal}\\mathbf{k}_{i}/\\tau}{\\exp(\\mathbf{q}_{i}^{\\intercal}\\mathbf{k}\n' +
      '\n' +
      '여기서 \\(\\tau\\)은 Softmax 분포의 평활도를 제어하기 위한 온도 인자이고, \\(j\\)은 다른 쌍의 데이터인 음성 샘플을 나타낸다. 상이한 모달리티들의 샘플들을 공유 공간 내의 임베딩들로 투영하고, 동일한 데이터 쌍으로부터의 임베딩들의 거리를 최소화하고, 상이한 데이터 쌍으로부터의 임베딩들의 거리를 최대화함으로써, ImageBind 모델은 의미 정렬 능력을 달성하고, 따라서 멀티모달 정렬을 위한 원하는 툴로서 제공될 수 있다.\n' +
      '\n' +
      '### 확산 잠재 정렬기\n' +
      '\n' +
      '###### 3.2.1 문제 공식화\n' +
      '\n' +
      '두 가지 모달리티 \\(M_{1},M_{2}\\)를 고려하며, 여기서 \\(M_{2}\\)은 조건부 모달리티이고 \\(M_{1}\\)은 생성 모달리티이다. 잠재 확산 모델(LDM)이 주어졌을 때, 본 연구의 목적은 생성 과정을 원하는 내용으로 이끌기 위해 조건(\\mathbf{x}^{M_{2}}\\sim p(\\mathbf{x}^{M_{2}})에서 정보를 활용하여 중간 생성 내용을 입력 조건과 정렬하는 것이다. 이 목적을 달성하기 위해, 우리는 잡음 제거 프로세스 동안 조건이 묘사된 콘텐츠로 목표 방향을 향해 잠재된 중간 잡음을 안내하는 확산 잠재 정렬기를 고안한다. 형식적으로, LDM으로부터 잠재변수 \\(\\mathbf{z}_{t},\\mathbf{z}_{t-1},...,\\mathbf{z}_{0}\\)의 시퀀스가 주어지면, 확산 잠재 정렬기 \\(\\mathcal{A}\\)는 유도조건 \\(\\mathbf{x}^{M_{2}}\\)과 함께 임의의 타임스텝 \\(t\\)에서 대응하는 잠재 \\(\\mathbf{z}_{t}\\)을 취하고, 조건과의 정렬이 더 우수한 수정된 잠재 \\(\\hat{\\mathbf{z}_{t}\\)을 생성한다.\n' +
      '\n' +
      '\\[\\hat{\\mathbf{z}}_{t}^{M_{1}}=\\mathcal{A}(\\mathbf{z}_{t}^{M1},\\mathbf{x}^{M_{2}}}}}\\tag{9}\\}\n' +
      '\n' +
      '관절 시각-음성 생성을 위해, 얼라이너는 두 모달리티로부터 정보를 동시에 획득하고 이러한 래턴트에 안내 신호를 제공해야 한다:\n' +
      '\n' +
      '\\[(\\hat{\\mathbf{z}}_{t}^{M_{1}},\\hat{\\mathbf{z}}_{t}^{M_{2}})=\\mathcal{A}(\\mathbf{z}_{t}^{M_{1}},\\mathbf{z}_{t}^{M_{2}}}}}\\tag{10}\\t}}\n' +
      '\n' +
      '순차적 잡음 제거 과정을 거친 후, 정렬기의 목표는 단방향 안내를 위한 \\(\\mathcal{F}(\\mathbf{z}_{0}^{M_{1}}),\\mathbf{x}^{M_{2}})\\, 동기화된 양방향 안내를 위한 \\(\\mathcal{F}(\\mathbf{z}_{0}^{M_{1}}),\\mathbf{z}_{0}^{M_{2}}))을 최소화하는 것이며, 여기서 \\(\\mathcal{F}\\)는 두 개의 모달리티를 갖는 샘플 간의 정렬 정도를 측정하기 위한 거리 함수를 나타낸다. 이 과정에서 갱신 가능한 파라미터는 잠재 변수, 임베딩 벡터 또는 신경망 파라미터일 수 있다.\n' +
      '\n' +
      '###### 3.2.2 멀티모달 안내\n' +
      '\n' +
      '섹션 3.2.1에 명시된 이러한 잠재 정렬기를 설계하기 위해 표현 학습을 위해 훈련된 멀티모달 모델, 즉 이미지 바인드[17]의 큰 능력을 활용하여 노이즈 제거 프로세스에 대한 합리적인 지침을 제공하는 훈련 없는 솔루션을 제안한다. 구체적으로, 각 timestep \\(t\\)에서 잠재변수 \\(\\mathbf{z}_{t}\\)이 주어지면, 예측된 \\(\\mathbf{z}_{0}\\)은 \\(\\mathbf{z}_{t}\\)과 예측된 잡음 \\(\\hat{e}\\)을 통해 도출될 수 있다.\n' +
      '\n' +
      '\\mathcal{G}(\\tilde{\\mathbff{z}}_{0}=\\frac{1}{\\sqrt{\\hat{\\alpha}_{t}}\\mathbffff{z}_{t}-\\sqrt{\\frac{1-\\tilde{\\alpha}_{t}{\\tilde{\\alpha}_{t}}\\hat{\\epsilon}, \\tag{11}\\tqrt{\\sqrt{\\frac{1-\\tilde{\\alpha}_{t}}\\hat{\\epsilon}, \\tag{11}\\tqrt{\\sqrt{\\sqrt{\\frac{1-\\tilde{\\alpha}_{t}}\\hat{\\epsilon}, \\tag{11}}\\tqrt{\\sqrt{\\sqrt{\\frac{1-\\tilde{\\alpha}_{t}}\\hat{\\epsilon}, \\tag{\n' +
      '\n' +
      '여기서 \\(\\hat{\\epsilon}=\\epsilon_{\\theta}(\\mathbf{z}_{t},t)\\). 이러한 깨끗한 예측을 통해, 분류기 지침이 필요한 것처럼 잡음이 많은 데이터에 대해 재교육하지 않고 정상 데이터에 대해 훈련된 외부 모델을 활용할 수 있다. 우리는 이미지 바인드 모델에 \\(\\mathbf{z}_{0}\\)과 안내 조건을 공급하여 그들의 거리를 계산한다.\n' +
      '\n' +
      '그림 2: **제안된 확산 잠재 정렬기.** 하나의 특정 모달리티(시각/오디오)를 생성하는 잡음 제거 프로세스 동안, 우리는 잡음 제거 프로세스를 안내하기 위해 조건 정보(오디오/비디오)를 채택한다. 사전 학습된 ImageBind 모델을 이용하여 ImageBind의 공유 임베딩 공간에서 생성 잠재 \\(\\mathbf{z}_{t}^{M_{1}}\\)과 조건 \\(\\mathbf{z}_{0}^{M_{2}}}\\)의 거리를 계산한다. 그리고 거리 값을 역전파하여 거리에 대한 \\(\\mathbf{z}_{t}^{M_{1}}\\)의 기울기를 구한다.\n' +
      '\n' +
      '이미지 바인드 임베딩 공간입니다. 그 후, 획득된 거리는 페널티의 역할을 할 수 있으며, 이는 계산 그래프를 역전파하고 잠재 변수 \\(\\mathbf{z}_{t}\\) 상의 구배를 얻는 데 사용될 수 있다:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\tilde{\\mathbf{z}}_{0},\\mathbf{x}^{M_{2}})=1-\\mathcal{F}(\\mathbf{E}^{M_{1}}(\\tilde{\\mathbf{z}}_{0}),\\mathbf{E}^{M_{2}}(\\mathbf{x}^{M_{2}}}}}}}{tag{12}}}}\n' +
      '\n' +
      '그런 다음 \\(\\mathbf{z}_{t}\\)를 통해 업데이트한다.\n' +
      '\n' +
      '\\mathbf{z}_{t}=\\mathbf{z}_{t}-\\lambda_{1}\\nabla_{\\mathbf{z}_{t}\\mathcal{L}(\\mathcal{D}(\\tilde{\\mathbf{z}}_{0}),\\mathbf{x}^{M_{2}), \\tag{13}\\t}\n' +
      '\n' +
      '여기서 \\(\\lambda_{1}\\)는 각 최적화 단계의 학습률 역할을 한다. 이러한 방식으로, 우리는 오디오-투-시각 및 비주얼-투-오디오를 모두 달성하기 위해 멀티모달 안내 신호를 통해 각 타임스텝에서 샘플링 궤적을 변경한다. 이 절차는 추가 데이터 세트 및 값비싼 네트워크 교육 없이 소량의 추가 샘플링 시간만 소요된다.\n' +
      '\n' +
      '###### 3.2.3 이중/삼각 손실 기능\n' +
      '\n' +
      '우리는 오디오가 순수 배경음악과 같은 충분한 의미 정보가 부족한 경우가 많은 반면, 페어링된 비디오에는 여러 객체 및 환경 사운드와 같은 풍부한 의미 정보가 포함되어 있음을 관찰했다. 시각적 생성을 유도하기 위해 이러한 유형의 조건을 사용하는 것은 충분하지 않으며 쓸모없는 안내 정보를 제공할 수 있다. 이를 해결하기 위해, 우리는 포괄적인 측정을 제공하기 위해 다른 모달리티, 예를 들어 텍스트를 통합한다.\n' +
      '\n' +
      '\\mathcal{L}_{a2v}=\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{\\mathbf{a}})+\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{\\mathbf{p}}). \\tag{14}\\\n' +
      '\n' +
      'ImageBind의 멀티모달 공간에서의 임베딩은 \\(\\mathbf{e}_{v}\\), \\(\\mathbf{e}_{a}\\) 및 \\(\\mathbf{e}_{p}\\)이다. (\\(\\mathcal{F}\\)는 두 임베딩 벡터 사이의 거리 함수를 나타내며, 이들 사이의 코사인 유사도는 1 빼기이다. 유사하게, V2A에 대한 손실은 다음과 같이 기록될 수 있다.\n' +
      '\n' +
      '\\mathcal{L}_{v2a}=\\mathcal{F}(\\mathbf{e}_{a},\\mathbf{e}_{\\mathbf{v}})+\\mathcal{F}(\\mathbf{e}_{a},\\mathbf{e}_{\\mathbf{p}}). \\tag{15}\\\n' +
      '\n' +
      '시각-오디오 조인트 생성을 위해, 손실은 삼각형으로 변한다:\n' +
      '\n' +
      '\\mathcal{L}_{\\text{joint-va}=\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{p})+\\mathcal{F}(\\mathbf{e}_{v},\\mathbf{e}_{a})+\\mathcal{F}(\\mathbf{e}_{a},\\mathbf{e}_{p}}}. \\tag{16}\\mathbf{e}_{p}\n' +
      '\n' +
      '텍스트는 사용자에 의해 입력되어 사용자-유도 대화형 시스템을 제공할 수 있거나 오디오 캡션 모델들을 통해 추출될 수 있다. 앞서 언급한 바와 같이, 오디오는 불완전한 의미 정보를 제시하는 경향이 있다. 따라서, 추출된 캡션은 그것보다 더 나빠야 한다. 그러나 이러한 의미적 오류를 수정하고 의미 정렬을 개선하는 데 도움이 된다는 것을 경험적으로 발견한다.\n' +
      '\n' +
      '3.2.4 안내 프롬프트 튜닝\n' +
      '\n' +
      '앞서 언급한 다중 모드 잠재 지침을 사용하여 우리는 시각적-음성 생성에서 좋은 생성 품질과 더 나은 콘텐츠 정렬을 성공적으로 달성했다. 그러나 이러한 접근 방식을 시청각 생성에 적용할 때 지침이 무시될 수 있는 효과가 있음을 관찰했다. 한편, 대응하는 오디오들을 생성하기 위해 오디오를 레버리지할 때, 생성된 비디오는 시간적 일관성의 보장이 없는 각 프레임의 그래디언트로 인해 덜 시간적 일관성이 된다. 따라서 본 논문에서는 이러한 문제를 극복하기 위해 생성 모델의 입력 텍스트 임베딩 벡터를 최적화하여 유도 프롬프트 튜닝을 제안한다.\n' +
      '\n' +
      '\\[\\hat{\\mathbf{y}}=\\mathbf{y}-\\lambda_{2}\\nabla_{\\mathbf{y}}\\mathcal{L}. \\tag{17}\\]\n' +
      '\n' +
      '\\(\\lambda_{2}\\)는 빠른 임베딩에 대한 학습률을 나타낸다. 구체적으로, 잡음 예측 초기에 즉각적인 텍스트 임베딩을 분리하고 텍스트 임베딩에서 멀티모달 손실 계산까지 계산 그래프를 유지한다. 그런 다음 계산 그래프를 역전파하여 프롬프트 임베딩 _w.r.t._ 멀티모달 손실의 기울기를 구한다. 업데이트된 임베딩은 일관된 의미 안내 정보를 제공하기 위해 모든 타임스텝에 걸쳐 공유된다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '**Dataset** 비디오-오디오, 오디오-비디오 및 오디오-비디오 공동 생성 태스크에 대한 평가를 위해 VGGSound 데이터셋 [6] 및 Landscape 데이터셋 [36]을 활용한다. 우리의 방법은 최적화 기반 솔루션이기 때문에 전체 데이터 세트를 평가에 활용할 필요가 없다. 대신, 우리는 비디오 대 오디오 생성을 위해 VGGSounddataset에서 3k개의 비디오 대 오디오 쌍을 무작위로 샘플링하고, 오디오 대 비디오 생성을 위해 3k 쌍, 이미지 대 오디오 생성을 위해 3k 쌍을 각각 샘플링한다. 영상 대 음성 생성 작업을 위해 각 비디오에서 키 프레임을 추출한다. 또한 비디오 오디오 조인트 생성을 위해 랜드스케이프 데이터 세트에서 200개의 비디오 오디오 쌍을 무작위로 샘플링한다.\n' +
      '\n' +
      '**구현 상세사항** 비디오-투-오디오 및 이미지-투-오디오 생성을 위해 사전 훈련된 AudioLDM[29], 오디오-투-비디오 생성을 위해 AnimateDiff[18]을 활용한다. 공동 오디오 비디오 생성을 위해 사전 훈련된 AudioLDM과 AnimateDiff를 모두 사용한다. 우리는 잡음 제거 단계를 비디오-오디오 생성의 경우 30, 오디오-비디오 생성의 경우 25, 오디오-비디오 조인트 생성의 경우 25로 각각 설정했다. 모든 작업에 적용되는 AudioLDM 디노이징을 안내하는 학습률 0.1과 AnimateDiff 디노이징을 안내하는 학습률 0.01을 사용한다. 우리는 공정한 비교를 위해 최적화 프로세스의 무작위 시드를 고정했다. 모든 실험은 NVIDIA Geforce RTX 3090 GPU에서 수행된다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '**Video-to-Audio** Video-to-Audio 생성 태스크의 베이스라인으로 SpecVQGAN[26]을 선택한다. VGGSound [26]에서 5개의 특징을 가진 ResNet-50을 사용하여 훈련된 사전 훈련된 모델을 추론 모델로 사용하고 3k VGGSound 샘플 데이터셋에서 SpecVQGAN과 방법을 비교했다.\n' +
      '\n' +
      '**Image-to-Audio** Im2Wav를 Image-to-Audio 생성 태스크의 베이스라인으로 선택하고 저자가 제공한 사전 훈련 모델을 사용하여 [37], AnimeGANv2[8]에 의해 전송된 3k 파프리카 스타일에 대해 테스트한다.\n' +
      '\n' +
      '**오디오-투-비디오** 오디오-투-비디오 생성 태스크의 베이스라인으로 TempoTokens를 선택하고 사전 사용\n' +
      '\n' +
      '그림 3: 비디오-오디오 생성 작업에 대한 기준선과 비교. SpecVQGAN은 입력 비디오와 사실적이고 정렬된 오디오를 생성하지 못한다. 제안된 방법은 입력 비디오 리듬에 맞춰 정렬된 오디오를 생성할 수 있다.\n' +
      '\n' +
      '저자가 제공한 훈련된 모델[46], 3k VG-GSound 샘플에 대해 테스트한다.\n' +
      '\n' +
      '**조인트 비디오 및 오디오 생성** MM-Diffusion[36]은 무조건 비디오 및 오디오 조인트 생성 태스크의 최첨단이므로, 랜드스케이프 데이터셋에 미리 학습된 모델을 이용하여 200개의 랜드스케이프 샘플로 제한 랜드스케이프 도메인에서 무조건 비디오 및 오디오 조인트 생성 태스크의 베이스라인으로 선택한다. 오픈 도메인에서 우리는 Ours-with-Guidance 모델과 Ours-바닐라 모델을 비교하는데, 우리가 아는 한 이 작업에 대한 확립된 기준선이 없기 때문이다.\n' +
      '\n' +
      '**s-Vanilla** 기존 도구의 조합으로 작업의 여러 바닐라 모델을 설계합니다. 비디오-투-오디오 태스크를 위해, 우리는 키 프레임[27]을 추출하고 사전을 사용한다.\n' +
      '\n' +
      '그림 4: 공동 비디오-오디오 생성 작업에 대한 기준선과 비교. 제안된 방법은 바닐라 모델보다 더 나은 텍스트 정렬 시각적 콘텐츠를 생성할 수 있다. 게다가, 우리의 생성된 오디오는 또한 더 나은 품질과 생성된 비디오와의 더 나은 정렬입니다.\n' +
      '\n' +
      '도 5: 오디오-투-비디오 태스크에 대한 기준선과 비교. 입력 오디오가 주어지면, TempoToken에 의해 생성된 비디오들은 입력 오디오 및 열악한 시각적 품질을 갖는 생성과 정렬되지 않는다. 제안된 방법은 입력 조건에 따라 시각적으로 훨씬 더 우수하고 의미적으로 정렬된 콘텐츠를 생성할 수 있다.\n' +
      '\n' +
      '학습된 이미지 캡션 모델 [3]은 동영상에 대한 캡션을 획득한다. 그런 다음 추출된 캡션을 사용하여 AudioLDM 모델로 오디오를 생성한다. 오디오-투-비디오 태스크를 위해 오디오 캡션 모델을 사용하고, 추출된 캡션을 AnimateDiff에 공급하여 입력된 오디오에 대한 비디오를 생성한다. 조인트 오디오 및 비디오 생성 태스크는 AudioLDM 모델과 AnimateDiff 모델에 대한 입력으로 테스트 프롬프트를 직접 가져와서 조인트 생성 결과를 구성한다.\n' +
      '\n' +
      '### Visual-to-Audio Generation\n' +
      '\n' +
      '시각-대-오디오 생성은 비디오-대-오디오 생성 및 이미지-대-오디오 생성 태스크를 포함한다. 영상-음성 생성은 의미적 수준에서 시청각적 정렬을 필요로 하는 반면, 영상-음성 생성을 위해서는 시간적 정렬이 추가로 필요하다. 더욱이, 생성된 오디오는 또한 높은 충실도일 필요가 있다. 이러한 측면에서 성능을 정량적으로 평가하기 위해 오디오-비디오 관련성에 대한 MKL 메트릭[26], 오디오 충실도 평가에 대한 인셉션 점수(ISc), 프레쳇 거리(FD) 및 프레쳇 오디오 거리(FAD)를 활용한다. 탭에서요 1, 우리는 우리의 방법이 훈련이 없어도 오디오-비디오 쌍에 대한 대규모 훈련이 필요한 기준선을 여전히 능가할 수 있음을 알 수 있다. 텍스트-오디오 기준선과 비교할 때, 우리의 방법이 오디오-비디오 관련성과 오디오 생성 품질을 일관되게 향상시킨다는 것을 알 수 있었다. 바닐라 베이스라인과 비교할 때, 우리는 우리의 방법이 특히 그림 6과 같이 관련 없는 소리와 배경 소음을 줄임으로써 오디오 품질을 크게 향상시킬 수 있음을 발견했다.\n' +
      '\n' +
      '### Audio-to-Video Generation\n' +
      '\n' +
      '오디오 대 비디오 생성은 생성된 비디오가 입력 오디오와 의미적 및 시간적으로 정렬될 뿐만 아니라 고품질이어야 한다. 생성된 비디오의 시각적 품질을 정량적으로 평가하기 위해 FVD(Frechet Video Distance)와 KVD(Kernel Video Distance)[41]을 메트릭으로 채택한다. 또한 생성된 비디오와 입력 오디오의 정렬을 측정하기 위해 오디오-비디오 정렬(AV-align) [46] 메트릭을 사용한다. 본 논문의 정량적 결과를 Tab. 1에 나타내었다. 본 논문에서 제안한 방법은 의미 정렬과 비디오 품질 측면에서 훈련 기반 기준선을 능가할 수 있음을 보인다. 또한, 텍스트-비디오 방법과 비교하여, 본 방법은 유사한 시각적 품질 성능을 유지하면서 더 나은 오디오-비디오 정렬을 달성할 수 있다. 그림 5에서 정성적 결과를 보여주는데, 템포토큰은 시각적 품질과 시청각 정렬에 어려움을 겪으며, 따라서 생성된 비디오는 입력 오디오와 관련이 없고 생성된 품질은 상대적으로 열악하다는 것을 관찰한다. 텍스트 투 비디오 방법은 생성된 비디오의 시각적 품질에 대해 좋은 성능을 달성할 수 있지만, 입력 오디오 콘텐츠와 정확하게 정렬하는 데 어려움을 겪는다. 공유된 시청각 표현 공간을 활용하는 훈련 없는 방법은 시각적 품질과 시청각 정렬 사이의 좋은 균형을 달성할 수 있다.\n' +
      '\n' +
      '### 합동 비디오 및 오디오 생성\n' +
      '\n' +
      '실용적인 공동 비디오 및 오디오 생성 작업은 텍스트를 입력으로 하고, 고충실도 비디오 및 오디오를 생성하고, 오디오-비디오 정렬을 유지하고, 텍스트-오디오 및 텍스트-비디오 관련성을 유지해야 한다. 구체적으로, 비디오 품질은 FVD, 오디오 품질은 FAD, 오디오-비디오 관련성은 AV-정렬[46], 텍스트-오디오 정렬은 TA-정렬, 텍스트-비디오 정렬은 TV-정렬을 채택한다. 우리의 정량적 평가는 Tab. 1에 나와 있다. 우리의 잠재 얼라이너는 기존의 무조건적인 오디오-비디오 조인트 생성 프레임워크 MM-Diffusion[36]에 플러그인될 수 있다. 그 결과, 기존의 MM-Diffusion과 비교했을 때, 잠재 정렬기는 비디오 생성 성능을 유지할 때 오디오 생성 품질을 높일 수 있음을 보여준다. 또한 텍스트-조건 조인트 비디오 및 오디오 생성 방법을 검증한다. 비디오 확산 모델 AnimateDiff[18] 및 오디오 확산 모델 AudioLDM[29]을 우리의 확산 잠재 정렬기와 브릿지한다. 웹에서 100개의 프롬프트를 무작위로 수집하여 세대를 조정합니다. 별도의 텍스트-비디오 및 텍스트-오디오 모델과 비교하여 얼라이너는 텍스트-비디오 정렬, 텍스트-오디오 정렬 및 비디오-오디오 정렬을 개선할 수 있습니다. 우리는 그림 4의 질적 비교를 보여준다. 더 많은 질적 결과는 보충에서 찾을 수 있다.\n' +
      '\n' +
      '그림 6: 비디오-오디오 생성 작업에 대한 우리의 바닐라 모델과 비교된다. 제안된 방법은 배경 및 관련 없는 사운드를 상당히 줄일 수 있고, 따라서 더 나은 오디오 품질을 달성할 수 있으며, 이는 탭 1에도 반영된다.\n' +
      '\n' +
      '그림 7: 안내된 프롬프트 튜닝의 효과를 시각화합니다. 생성된 자동 자막은 “겨울왕국 2 - 스크린샷”으로 의미 있는 시각적 콘텐츠를 캡처하지 못하며, 따라서 텍스트 투 오디오 방식은 의미 있는 사운드를 생성하지 못한다. 우리의 신속한 튜닝은 의미 있는 소리를 생성하기 위해 의미 정보를 보완하기 위해 시각적 정보를 검사할 수 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '* [17] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15180-15190, 2023.\n' +
      '* [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. _arXiv preprint arXiv:2211.13221_, 2022.\n' +
      '* [20] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. _arXiv preprint arXiv:2307.06940_, 2023.\n' +
      '* [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.\n' +
      '* [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.\n' +
      '* [24] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation. _arXiv preprint arXiv:2305.18474_, 2023.\n' +
      '* [25] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. _arXiv preprint arXiv:2301.12661_, 2023.\n' +
      '* [26] Vladimir Isahin and Esa Rahtu. Taming visually guided sound generation. _arXiv preprint arXiv:2110.08791_, 2021.\n' +
      '* [27] KeplerLab. Tool for automating common video key-frame extraction, video compression and image auto-crop/image-resizee tasks. [https://github.com/keplerlab/katna](https://github.com/keplerlab/katna), 2021.\n' +
      '* [28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. _arXiv preprint arXiv:2209.15352_, 2022.\n' +
      '* [29] Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. _arXiv preprint arXiv:2301.12503_, 2023.\n' +
      '* [30] Haobe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-uqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. _arXiv preprint arXiv:2308.05734_, 2023.\n' +
      '* [31] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. _arXiv preprint arXiv:2306.17203_, 2023.\n' +
      '* [32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.\n' +
      '* [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1 (2):3, 2022.\n' +
      '* [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE, 2022.\n' +
      '* [36] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10219-10228, 2023.\n' +
      '* [37] Roy Sheffer and Yossi Adi. I hear your true colors: Image guided audio generation. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.\n' +
      '* [38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [39] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, and Chuang Gan. Physics-driven diffusion models for impact sound synthesis from videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9749-9759, 2023.\n' +
      '* [40] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clips for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14214-14223, 2023.\n' +
      '* [41] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. _arXiv preprint arXiv:1812.01717_, 2018.\n' +
      '**[42] Chia-Hung Wan, Shun-Po Chuang, and Hung-Yi. 생성적 적대 네트워크를 사용하여 장면 이미지 합성에 오디오를 향합니다. _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 496-500. IEEE, 2019.\n' +
      '* [43] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning robust audio representations from clip. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4563-4567. IEEE, 2022.\n' +
      '* [44] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 2023.\n' +
      '* [45] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. _arXiv preprint arXiv:2309.16429_, 2023.\n' +
      '* [46] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. _arXiv preprint arXiv:2309.16429_, 2023.\n' +
      '* [47] Maciej Zelaszczyk and Jacek Mandziuk. Audio-to-image cross-modal generation. In _2022 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2022.\n' +
      '* [48] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. _arXiv preprint arXiv:2309.15818_, 2023.\n' +
      '* [49] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [50] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and Jianlong Fu. Moviefactory: Automatic movie creation from text using large generative models for language and images. _arXiv preprint arXiv:2306.07257_, 2023.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>