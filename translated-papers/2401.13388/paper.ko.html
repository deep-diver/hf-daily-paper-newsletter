<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '멀티모달의 조건인 UNIMO-G를 통한 통합 이미지 세대입니다.\n' +
      '\n' +
      '위 Li\\({}^{**}\\), Xue Xu\\({}^{**}\\), 지아셴 류({}^{**}\\), 자니아 샤오셴 류({}^{*}\\), 지아셴 류({}^{*}\\), 자니아 샤오셴 류({}^{*}\\), 지아셴 류({}^{*}\\)\n' +
      '\n' +
      '베이징, 베이징, 중국 중국, 중국, 중국, 중국 보루루사.\n' +
      '\n' +
      '{liwei85.2023}@gmail.com\n' +
      '\n' +
      '{xuxue,xiaoxinyan,liujiachen}@baidu.com\n' +
      '\n' +
      '이 저자는 이 작업에 동등하게 기여했다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '기존의 텍스트 대 이미지 확산 모델은 주로 텍스트 프롬프트로부터 이미지를 생성한다. 그러나 텍스트 설명의 고유한 구체성은 특정 엔티티 또는 장면과 같은 복잡한 세부사항으로 이미지를 충실히 합성하는 데 어려움을 제기한다. 이 논문은 인터리빙된 텍스트 및 시각적 입력과 함께 다중 모드 프롬프트에서 작동하는 간단한 다중 모드 조건부 확산 프레임워크인 **UNIMO-G**를 제시하며, 이는 텍스트 구동 및 주제 구동 이미지 생성 모두에 대해 통일된 능력을 보여준다. UNIMO-G는 다중 모드 프롬프트를 인코딩하기 위한 멀티모달 대형 언어 모델(MLLM)과 인코딩된 멀티모달 입력을 기반으로 이미지를 생성하기 위한 조건부 데노징 확산 네트워크의 두 가지 핵심 구성 요소를 포함한다. 우리는 틀을 효과적으로 훈련시키기 위해 2단계 훈련 전략을 활용하는데, 첫째, 대규모 텍스트 이미지 쌍에서 사전 훈련하여 조건부 이미지 생성 능력을 개발한 다음 다중 모드와의 명령어 튜닝은 통일된 이미지 생성 능력을 달성하는 것을 의미한다. 언어 접지 및 이미지 분할을 포함하는 잘 설계된 데이터 처리 파이프라인을 사용하여 다중 모달 프롬프트를 구성한다. UNIMO-G는 텍스트 대 이미지 생성 및 제로 샷 주제 구동 합성 모두에서 탁월하며, 다중 이미지 엔티티들을 포함하는 복잡한 다중 모드 프롬프트로부터 고 충실도 이미지를 생성하는 데 특히 효과적이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트 대 이미지(T2I) 확산 모델의 최근 발전은 텍스트 설명에서 고 충실도 이미지를 생성하는 데 인상적인 결과를 얻었다. DALLE-2 Ramesh et al.(2022), 임젠 사아리아 et al.(2022), 스테이블 디확산 라이브리치 et al.(2022), SD-XL Podell et al.(2023) 등 다양한 방법이 텍스트 프롬프트를 기반으로 사진 실현적이고 맥락적으로 관련된 이미지를 생산하는 데 성공하였다. 그럼에도 불구하고, 특히 복잡한 세부 사항, 특정 엔티티 또는 누드 장면들이 관련될 때 텍스트 설명의 고유한 불균형으로 인해 근본적인 도전은 지속된다. 따라서 영상 생성의 제어성을 향상시키기 위해서는 일반적인 비전 언어(VL) 입력으로부터 영상을 충실히 생성하는 것이 필수적이다.\n' +
      '\n' +
      '많은 연구에서 VL 대 이미지 생성 기술을 조사했다. 드림보스 루즈(2023), 임작용 카와르(2023), SuTI Chen et al.(2023), BLIP-Diffusion Li et al.(2023)와 같은 방법은 피험자 주도 생성을 강조하며, 이는 피험자 이미지와 텍스트 설명을 모두 투입물로 사용하여 새로 기술된 환경에서 주제를 재구성한다. 그들은 주어진 주제에 대해 미세 조정 특정 모델을 사용하거나 미리 훈련된 주제 표현을 사용한다. 그러나 그들의 구체적인 훈련 설계 및 입력 템플릿은 특히 여러 개체를 가진 복잡한 시나리오에서 확장성을 방해한다. 또한, 패스트컴포저 샤오(2023) 및 서브객체-디퓨전 마 등(2023)과 같은 연구는 이미지 인코더로부터의 이미지 임베딩과 사전 훈련된 확산 모델에서 표준 텍스트 컨디셔닝을 통합하는 다중-엔티티 이미지 생성에 초점을 맞추고 있다. 그럼에도 불구하고, 이러한 접근법은 자유 형태의 텍스트 및 시각적 정보의 혼합을 구성하는 일반화된 비전-언어 입력들을 효율적으로 처리할 수 있는 능력이 부족하다.\n' +
      '\n' +
      '본 논문에서는 자유형 인터리빙된 비전-언어 입력들을 포함하는 멀티모달 프롬프트 상에서 동작하는 간단한 멀티모달 조건부 확산 프레임워크인 **UNIMO-G**를 제안한다. 전통적인 텍스트 전용 프롬프트와 달리 그림 1. UNIMO-G에서 입증된 바와 같이 다중 모드 프롬프트는 모든 이미지 엔티티들을 충실히 재생산하고 텍스트 콘텐츠를 렌더링하고 다중 모드 프롬프트에서 지침을 따르도록 설계된다. 구체적으로, 우리는 텍스트 구동 및 제로 샷 주제 구동 생성 모두에 대해 UNIMO-G의 그림 1:예의 인식 능력을 레버리지한다. UNIMO-G는 자유형 인터리빙된 시각적 언어 입력을 인식하고 이미지를 충실히 생성할 수 있다. 특히, 다중 이미지 엔티티들로 멀티 모달 프롬프트로부터 이미지를 생성할 수 있다.\n' +
      '\n' +
      '다중 모드 대어 모델(MLLM)을 인코딩하여 멀티모달 프롬프트를 통일된 비전 언어 의미 공간으로 인코딩한다. 이어서, 조건부 확산 네트워크는 이러한 인코딩된 표현들로부터 이미지를 생성한다.\n' +
      '\n' +
      'UNIMO-G를 효율적으로 훈련시키기 위해 2단계 전략을 구현합니다. 처음에 모델은 텍스트 이미지 쌍의 대규모 데이터 세트를 사전 학습하여 조건부 이미지 생성의 능력을 향상시킨다. 그 다음 다중 모드 프롬프트와 명령어 튜닝의 단계가 뒤따르고, 이러한 프롬프트에서 제공되는 세부 사양과 일치하는 이미지를 생성하도록 학습한다. 언어 접지 및 이미지 분할을 통합하는 신중하게 설계된 데이터 처리 파이프라인을 사용하여 이러한 다중 모드 프롬프트를 구성한다. 이 접근법은 UNIMO-G가 MLLM 인코더로부터 풍부한 특징을 이용하여 다양한 맥락에서 콘텐츠를 충실히 재생하는 이미지를 생성할 수 있게 한다.\n' +
      '\n' +
      'UNIMO-G는 텍스트 대 이미지 합성뿐만 아니라 제로 샷 주제 구동 생성에서도 탁월한 제어 가능한 이미지 생성을 위한 포괄적인 능력을 나타낸다. 다중 이미지 엔티티도 포함하는 다중 모드 프롬프트로부터 충실도 높은 이미지를 확실하게 생성한다. 성능을 평가하기 위해 MS-COCO 및 드림벤치 데이터 세트를 사용하여 텍스트 대 이미지 및 주제 주도 생성 컨텍스트 모두에서 평가를 수행했다. 결과는 이러한 시나리오에서 UNIMO-G의 우수한 성능을 일관되게 강조한다. 또한 드림벤치가 단일 주체 세대에 초점을 맞추는 것을 인식하여 여러 개체가 있는 이미지를 특징으로 하는 새로운 벤치마크인 멀티벤치를 소개한다. 멀티벤치에 대한 평가는 제로샷 다중중심주제 주도세대에서 UNIMO-G의 효과를 확인한다.\n' +
      '\n' +
      '요약하면, 이 작업에 대한 우리의 기여는 다음과 같이 요약될 수 있다.\n' +
      '\n' +
      '* 우리는 인터리빙된 이미지 및 텍스트 입력으로 다중 모드 프롬프트를 지원함으로써 이미지 생성의 제어 가능성을 크게 향상시키는 간단한 멀티 모달 조건부 확산 프레임워크를 제안한다.\n' +
      '* 우리는 멀티모달 명령어 튜닝을 통해 제로샷 멀티센트리티 주제어 생성의 힘을 발휘하여 효과적인 2단계 훈련 전략을 소개한다.\n' +
      '* UNIMO-G는 단일 및 다중 중심 주제 주도 생성 과제, 특히 다음 다중 모드 수업 역량 모두에서 기존 VL 대 이미지 모델을 능가한다.\n' +
      '\n' +
      '## 2 Method\n' +
      '\n' +
      '그림 2에 묘사된 UNIMO-G의 아키텍처는 주로 다중 모드 프롬프트 인코딩을 담당하는 멀티모달 대형 언어 모델(MLLM)과 인코딩된 표현을 기반으로 이미지 생성을 위한 조건부 데노징 확산 네트워크의 두 가지 핵심 구성 요소를 포함한다. 우리 연구에서 우리는 구조적으로 미니GPT-4 Zhu et al.(2023)와 유사한 사내 중국 MLLM을 사용했다. 총 70억 개의 파라미터를 포함하고 있으며 수십억 개의 이미지-텍스트 쌍을 포함하는 방대한 데이터 세트를 사전 훈련했다. 이 광범위한 훈련 과정은 모델을 복잡한 다중 모드 데이터를 처리하고 해석하는 강력한 능력과 동일시한다. UNIMO-G의 교육은 2단계 과정으로 진행된다.\n' +
      '\n' +
      '****-이미지 사전 학습**: 대규모 중국 텍스트 이미지 쌍에서 처음부터 조건부 변성 확산 네트워크를 사전 훈련한다. 우리는 Rombach et al.(2022)에서 동일한 U-Net 네트워크 아키텍처를 사용하고 교차 의도 메커니즘을 사용하여 텍스트에 조건화한다.\n' +
      '** ** 멀티 모달 명령어 튀는**: 수백만 쌍의 멀티모달 프롬프트와 이미지에 대한 추가 미세 조정 UNIMO-G는 다중 모드 입력에서 이미지를 충실히 생성하는 능력을 향상시킨다.\n' +
      '\n' +
      '두 훈련 단계에서 U-Net 성분만 적극적으로 훈련된다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '그림 2: UNIMO-G는 다중 모드 인식을 위한 MLLM과 이미지 생성을 위한 조건부 데노징 UNet로 구성된다. 인터리빙된 이미지 및 텍스트로 멀티모달 프롬프트를 수용하고, 이미지 엔티티들과 일치하는 이미지를 생성한다. 오렌지란 훈련 가능한 모듈을 나타내며, 블루는 냉동된 모듈을 나타낸다.\n' +
      '\n' +
      'MLLM 매개변수와 함께 동결되었다. 이 전략은 UNIMO-G가 미리 훈련된 MLLM에 인코딩된 인식 지식을 유지하면서 이미지를 생성하는 것을 효과적으로 학습하도록 한다.\n' +
      '\n' +
      '### Text-to-Image Pre-training\n' +
      '\n' +
      '우리는 잠재 확산 모델(Rombach et al, 2022)을 따라 이미지 인코더 \\(\\mathcal{E}\\) 및 디코더 \\(\\mathcal{D}\\)로 구성된 지각 압축 모델(즉, VQ-VAE)을 사용하여 픽셀 데이터를 잠재 공간과 역전(\\mathcal{D})으로 인코딩하여 \\(\\mathcal{D}(x))\\(\\mathcal{E}(\\mathcal{D}(x))를 사용하여 잠재 공간 및 역전(\\mathcal{D}(\\mathcal{D}(\\mathcal{D}(\\mathcal{D}(\\mathcal{D}) 픽셀 데이터를 잠재 공간으로 인코딩하여\\(\\mathcal{D}(\\mathcal{D}(\\mathcal{D}(\\mathcal{E}(x))를 사용하여\\(\\mathcal{E}(\\mathcal{E}) 픽셀}(\\mathcal{E}(x))를 사용하여\\(\\mathcal{E}(x),\\) 픽셀}(\\mathcal{E 그런 다음 확산 과정은 잠재 공간에 대해 수행되며, 이는 C\\(T\\) 단계보다 초기 잠재 표현 \\(z_{0}=\\mathcal{E}(x)\\)에 가우시안 노이즈를 점진적으로 추가하여 순방향 확산 프로세스 \\(q\\)의 마르코프 사슬을 정의한다. 각 시간 단계 \\(t\\)에서 순방향 과정 \\(z_{t}|z_{t}|z_{t-1})은 다음과 같이 나타낼 수 있다.\n' +
      '\n' +
      '\\[q(z_{t}|z_{t-1})=\\mathcal{N}(z_{t};\\sqrt{1-\\beta_{t}}z_{t-1},\\beta_{t}I)\\]\n' +
      '\n' +
      '\\(\\{\\beta_{t}\\}\\)는 일련의 초모수이다. 확산 모델은 조건부 U-Net(Ronneberger et al., 2015) 데노저 \\(\\epsilon_{\\theta}\\)를 학습하여 확산 마코브 사슬을 역전시켜 현재 타임스페 \\(t\\), 시끄러운 잠복 \\(z_{t}\\) 및 생성 조건 \\(c\\)으로 소음을 예측하도록 훈련된다. 훈련 손실은 예측된 노이즈 \\(\\epsilon_{\\theta}(z_{t},t,c)와 \\(z_{t}\\)의 실제 노이즈 \\(\\epsilon\\) 사이의 평균 제곱 오차(MSE)이다.\n' +
      '\n' +
      '\\[\\mathcal{L}=\\mathbb{E}_{z_{0},c,\\epsilon\\sim\\mathcal{N}(0,1),t}[\\|\\epsilon- \\epsilon_{\\theta}(z_{t},t,c)]\\|^{2}\\]\n' +
      '\n' +
      '대규모 중국 텍스트 이미지 쌍은 위의 데노징 목적을 훈련시키기 위해 사용된다. 조건 정보 \\(c\\)는 UNet 모델의 각 교차 주의 블록에 그대로 공급된다.\n' +
      '\n' +
      '\\[z_{t}]=softmax(\\frac{Q(z_{t})\\cdot K(c)^{T}}{\\sqrt{d\\})\n' +
      '\n' +
      'V\\(Q\\), \\(K\\) 및 \\(V\\)는 각각 질의, 키 및 값 예측을 나타낸다. (d\\)는 특징들의 출력 차원을 나타낸다. 우리의 모델에서 조건 \\(c\\)는 미리 훈련된 MLLM에 의해 인코딩된다.\n' +
      '\n' +
      '처음부터 텍스트 대 이미지 확산 모델을 훈련하는 사전 훈련 전략들은 복잡성과 자원 지출 측면에서 중요한 과제를 제시한다. 이를 해결하기 위해 모델 훈련의 효율성과 성능을 높이기 위한 효과적인 훈련 일정을 소개합니다. 이 일정은 발견된 시각적 분포 지식을 확립하기 위해 작은 이미지 코퍼스에 대한 (1) 초기 훈련, (2) 텍스트-시각적 대응에 초점을 맞춘 대규모 텍스트-이미지 쌍 훈련으로의 후속 확장, (3) 시각 미학과 정밀한 텍스트-이미지 정렬이 특징인 작은 정제된 코퍼스와의 최종 훈련 단계에서 절정하는 세 단계를 포함한다. 우리의 실험에서 UNet 모델의 교육은 처음에 CC3M 데이터세트(샤마 등 2018)를 사용하여 수행된다. 이 데이터 세트는 간단한 이미지 설명과 결합된 다양한 시각적 개념에 대해 선택되어 처음부터 훈련을 시작하기 위한 효과적인 도구이다. 이어서, 모델은 개념적 이해를 넓히고 텍스트 설명으로의 정렬을 개선하기 위한 300M 텍스트 이미지 쌍의 광범위한 수집으로 추가 훈련을 거치게 된다. 최종 훈련 단계는 우수한 품질을 위해 신중하게 선택된 수만 개의 고품질 이미지 텍스트 쌍으로 구성된 꼼꼼하게 큐레이트된 코퍼스를 사용하여 모델을 미세 조정하는 것을 포함한다. 위의 전략과 아키텍처 설계를 바탕으로 스테이블 디확산 및 그 고급 버전 SD-XL(Podell et al, 2023)과 같은 오픈 소스 모델을 능가하는 강력한 중국 텍스트 대 이미지 생성 모델을 얻는다. 우리는 4.2절에서의 결과에 대한 자세한 평가와 부록 A의 구현 세부 정보를 제공한다.\n' +
      '\n' +
      '멀티모달은 튀는 것을 보장합니다.\n' +
      '\n' +
      '텍스트 대 이미지 사전 학습 후, UNIMO-G는 MLLM의 인식 능력에 의존하여 인터리빙된 이미지와 텍스트로부터 이미지를 생성할 수 있는 것이 사실이다. 그러나 사전 훈련 단계는 주로 입력 표현과 의미적으로 일치하는 이미지를 생성하는 데 초점을 맞추고 있다는 점에 유의하는 것이 중요하다. 그 결과 UNIMO-G는 이미지 조건에 명시된 내용을 충실히 재현하기 위해 입력의 시각적 특징을 활용하는 데 여전히 어려움을 겪고 있다. 이러한 한계를 해결하기 위해 다양한 맥락에서 이미지 콘텐츠를 충실히 재생할 수 있는 UNIMO-G의 능력을 향상시키기 위해 다중 모드 명령어 튜닝을 추가로 수행한다.\n' +
      '\n' +
      '텍스트 프롬프트의 대표성을 높이기 위해 인터리빙된 이미지와 텍스트로 구성된 멀티모달 프롬프트에 대한 포맷을 소개합니다. 구체적으로 텍스트 캡션에서 언급된 엔티티들은 "그림 2"와 같이 "그림 2"와 같이 "엘론 머스크</임그>의 <임그>의 이미지</임그>가 거리에서 셀카를 가져간다"와 같이 해당 이미지로 치환될 수 있으며, 다중 모드 프롬프트 쌍과 이미지를 생성하는데, 파이프라인은 그림 3과 같이 데이터 처리 파이프라인을 만들고 MLLM에 의한 캡션으로부터 엔티티들을 추출한다. 이어서, 그로그라운드 DINO Liu et al.(2023)에 의한 언어 접지 및 SAM Kirillov et al.(2023)에 의한 이미지 분할의 조합을 사용하여 각 엔티티에 대한 해당 이미지 세그먼트를 획득한다. 데이터 구축 과정에 대한 추가 도입은 A절에서 제공되며 다중 모드 프롬프트 쌍과 이미지 수집과 함께 UNIMO-G는 이러한 다중 모드 지침에 따라 이미지를 생성하기 위해 훈련된다.\n' +
      '\n' +
      '멀티 모달 입력의 시각적 특징을 더 잘 활용하기 위해 생성된 객체와 입력 이미지 엔티티 간의 교차 의도 메커니즘에 대한 향상을 소개한다. 이러한 개선은 생성된 콘텐츠와 입력 이미지 내의 시각적 요소 간의 관계에 대한 보다 강력하고 맥락 인식적인 이해를 함양하는 것을 목표로 한다. 프로프트별 Hertz 등(2022)에 의해 명시된 바와 같이 텍스트 대 이미지 확산 모델에서 교차 표현은 해당 텍스트 토큰에 의해 지정된 각각의 생성된 객체의 위치를 반영할 수 있다. 마찬가지로, 이미지 엔티티들의 시각적 특징들도 시각적 토큰으로 취급될 수 있다. 시끄러운 잠재 \\(z_{t}\\)의 중간 특징과 시각적 토큰 \\(v\\) 사이의 교차 의도 지도를 계산할 수 있다.\n' +
      '\n' +
      '\\[CA(z_{t},v)=Softmax(Q(z_{t})\\cdot K(v)^{T})\\]\n' +
      '\n' +
      'i\\(Q\\)와 \\(K\\)는 각각 질의 및 키 투영을 나타낸다. 각 시각적 토큰에 대해 \\(h\\)와 \\(w\\)가 잠재 피처 \\(z_{t}\\)의 공간 차원인 \\(h\\t w\\)의 주의 지도를 얻을 수 있었다. 교차지도의 점수는 시각적 토큰에서 잠재 화소로 흐르는 정보의 양을 나타낸다. 따라서 각 시각적 토큰이 해당 오브젝트가 차지하는 이미지 영역에 주로 참여할 수 있도록 모델을 장려하는 추가 손실 항을 소개합니다. 구체적으로 \\(z_{t}\\)를 목적물의 원하는 영역이 주의도와 기업의 해당 세분화 지도 간의 \\(L1\\) 편차를 형벌하여 큰 값을 갖는다는 대상에 최적화한다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{attn}=\\frac{1}{N}\\sum_{k=1}^{N}\\mid CA(z_{t},v_{k})-M_{k}\\mid\\]\n' +
      '\n' +
      'HH(M_{k}\\)는 시각적 토큰 \\(v_{k}\\)에 해당하는 \\(k_{th}\\) 객체의 분할 마스크이다. 이 훈련 과정을 통해 UNIMO-G는 입력된 이미지의 시각적 특징을 효과적으로 이용하여 해당 콘텐츠를 충실히 재생산하도록 학습한다.\n' +
      '\n' +
      '3개의 관련 작업.\n' +
      '\n' +
      '텍스트 대 이미지 합성에 확산 모델을 통합하는 것은 계산 창의성 호(2020) 알(2020), 송(2020), 리 등(2022)에서 주목할 만한 발전을 나타낸다. CLIP 이미지 임베딩을 활용하는 GLIDE 니콜 et al.(2021) 및 DALL-E 2 Ramesh et al.(2022)와 같은 모델은 텍스트 입력과 다양하고 의미적으로 일치하는 이미지를 생성하는 데 실질적으로 진행되었다. 이미젠 사하라리아(2022) 등은 언어 이해력의 중요성을 강조하여 의미론적 표현을 향상시키기 위해 큰 T5 언어 모델의 통합을 제안한다. 라텐트 디확산 모델 라이바흐(2022)는 텍스트 조건, 저차원 잠재 공간으로부터 이미지를 생성하여 계산 제약을 다룬다. 제안된 프레임워크는 라트 디확산 모델의 원리에 따라 구축되어 계산 효율성과 확장성을 활용합니다.\n' +
      '\n' +
      '텍스트 설명에서 고품질 이미지를 생성하는 성공 이후 대상-학생 이미지 세대는 최근 연구들에서 교과 중심의 생성 기법을 탐색해 왔다. 드림보스 루즈(2023), 텍스트-전환 Gal et al.(2022), 커스텀-확산 구마리 등 모델은 최적화 기반 방법을 사용하여 피험자를 확산 모델에 구현했다. 이는 모델 가중치 루즈(2023)를 미세 조정함으로써 달성되며, 구마리 등은 알(2023) 또는 피사체 이미지를 대상 동일성 Gal et al.(2022)을 인코딩하는 텍스트 토큰으로 반전시킨다. 일부 작품은 튜닝이 없습니다.\n' +
      '\n' +
      '그림 3: 다중 모달 명령어 튜닝을 위한 데이터 구축 파이프라인의 여기.\n' +
      '\n' +
      '방법. ELITE(Wei et al., 2023)와 인판트 보트(Shi et al., 2023)는 단어 임베딩에 참조 이미지를 프로젝트하고 참조 이미지 패치 특징을 교차 의도 계층에 주입하여 로컬 디테일을 향상시킨다. 사진메이커(Li et al., 2023)는 복수의 ID 이미지로부터 적층된 ID 임베딩을 추출하여 인간 초상화의 생성에 초점을 맞추고 있다. 단일 객체 맞춤화에 대한 인상적인 결과에도 불구하고, 그들의 아키텍처 디자인은 확장성을 여러 교과 설정으로 제한한다. 서브객체-디퓨전(Ma et al., 2023), 패스트컴포저(Xiao et al., 2023)와 같은 모델은 다중 중심 주체 주도 생성을 위해 설계되었습니다. 그들은 이미지 인코더로부터 추출된 주제 임베딩을 통합함으로써 확산 모델 내의 텍스트 컨디셔닝을 향상시키지만, 이러한 접근법의 널리 퍼진 한계는 텍스트 및 시각적 지침을 분리하기 위한 성향이며, 따라서 관절 양식 통합의 효능을 제약한다. 이 문제는 특히 여러 개체가 있는 시나리오로 확장할 때 두드러진다.\n' +
      '\n' +
      '멀티모달 언어 모델*** 멀티모달 대형 언어 모델(MLLM)으로 생성된**는 이미지를 포함한 다양한 방식을 처리하기 위해 언어 모델의 능력을 크게 넓혔다. 이 모델은 본질적으로 인터리빙된 비전-언어 입력을 용이하게 하며, 다수의 이미지를 효과적으로 처리합니다. 보이지 않는 작품에는 이미지-선택 쌍 훈련을 통해 MLLM 의미론과 확산 이미지 디코더를 정렬하는 M-VADER(Weinbach et al, 2022)이 포함된다. 또한 GILL(Koh et al, 2023), Emu(Sun et al, 2023), Emu2(Sun et al, 2023), 드림LLM(동 et al., 2023)과 같은 모델은 인터리빙된 비전-언어 생성을 전문으로 하며 CLIP 감독을 활용하거나 다중 모드 데이터 세트를 사전 학습하여 MLLM의 출력 공간을 확산 이미지 디코더와 정렬한다. 그러나 이러한 방법은 주로 의미 수준에서 정렬되며 상세하고 주제 중심적인 이미지 생성으로 투쟁할 수 있다. BLIP-확산(Li et al., 2023)은 무작위로 대상을 구성하여 이미지를 합성하여 영샷, 주제 중심 텍스트 대 이미지 생성 능력으로 종료한다. 그러나 그 구체적인 입력 템플릿과 훈련 과정은 여러 개체에 대한 확장성을 제한한다. 우리의 작업과 밀접한 관련이 있는 모델인 KOSMOS-G(Pan et al., 2023)는 인터리빙된 텍스트-시각적 입력을 인코딩하기 위해 MLLM을 침출한다. 훈련 과정 전반에 걸쳐 동결된 상태로 유지되는 이미지 디코더로서 스테이블 디퓨전(SD) v1.5의 U-Net을 활용한다. KOSMOS-G의 핵심 구성 요소는 냉동 SDv1.5 U-Net의 출력 임베딩 공간과 MLLM을 정렬하기 위해 텍스트 데이터에서만 학습된 AlignerNet이다. 대조적으로, 우리의 접근법은 다중 모드 확산을 위해 특별히 U-Net 모델 종말단을 훈련하는 데 기초하며 다중 모드 맥락에서 생성된 이미지의 충실도와 관련성을 크게 향상시킨다. 정렬 기반 방법론을 임베딩하는 것을 통해, 우리의 2단계 훈련 전략은 다중 모드 지침에 따라 모델의 능력을 현저하게 향상시키며, 이는 텍스트 중심 및 주제 주도 생성 작업, 특히 복잡한 다중 중심 시나리오 모두에서 우수한 성능을 초래한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '본 절에서는 먼저 실험내용과 설정을 소개한다. 그런 다음 텍스트 중심 시나리오와 주제 중심 시나리오 모두에서 평가 결과를 제시한다. 마지막으로 정량적 절제 연구로 결과를 추가로 분석한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      'UNIMO-G는 미니GPT-4 아키텍처(Zhu et al, 2023)에 이어 7B-파라미터 MLLM 인코더와 4B-파라미터 변성 U-Net으로 구성되어 대략 11B 매개변수를 총화한다. MLLM은 텍스트, 이미지-선택 쌍 및 인터리빙된 이미지-텍스트 데이터를 포함하는 대규모 중국 다중 모드 코퍼스에서 전처리된다. U-Net 아키텍처는 채널 크기[640, 1280, 2560, 2560]가 있는 5개의 다운샘플링 및 5개의 업샘플링 블록과 4096 치수와 16개의 헤드를 갖는 교차 의도 메커니즘을 포함한다. LDM 프레임워크를 기반으로 하는 이미지 자동 인코더는 우리의 특정 이미지 코퍼스에 최적화되었다. 세부 훈련 과정 및 데이터 구성은 부록 A에 추가로 도입된다.\n' +
      '\n' +
      '### Evaluation Results\n' +
      '\n' +
      'UNIMO-G는 그림 1과 같이 텍스트 중심 이미지 생성과 주제 주도 이미지 생성 모두에 대한 통일된 이미지 생성 능력을 보여주고 있으며, 다음에서는 서로 다른 측면에서 UNIMO-G의 성능을 평가할 것이다.\n' +
      '\n' +
      '텍스트 대 이미지 생성을 위해 MS-COCO(Lin et al., 2014) 검증 세트에서 무작위로 샘플링된 3만 개의 캡션을 사용하여 이를 중국어로 번역하여 UNIMO-G의 입력 요구 사항과 정렬했다. 이미지는 512x512 픽셀에서 생성되었고 현장의 표준인 FID-30k 메트릭을 사용하여 평가를 위해 256x256으로 재구성되었다. 우리의 모델은 확산 샘플링을 위한 5.0 및 50 DDIM 추론 단계의 분류기가 없는 안내 척도를 사용한다. 표 1에서 보는 바와 같이 UNIMO-G는 성능에서 이미지(VL2I) 모델에 대한 다른 비전 언어를 크게 능가한다.\n' +
      '\n' +
      '우리의 모델을 추가로 검증하기 위해 UNIMO-G와 선도 오픈 소스 모델인 SD-XL(Podell et al., 2023)을 비교한 인간 평가를 수행했다. 312개의 프롬프트( DrawBench에서162개, 온라인 * 플랫폼에서 무작위로 샘플링된 150개의 사용자 질의)를 포함하는 포괄적인 이중언어 벤치마크를 확립했다. DrawBench 프롬프트는 언어 특이적 프롬프트를 배제하기 위해 필터링되었다. 모든 프롬프트는 최종 평행 중국어와 영어 세트를 달성하기 위해 수동으로 번역되고 신중하게 증명됩니다. 3명의 독립적인 평가자는 이미지 미학의 측면, 이미지-텍스트 관련성 및 전반적인 품질에 초점을 맞추어 자신이 선호하는 모델을 선택하여 UNIMO-G 및 SD-XL의 이미지를 각각 평가했다. 결과는 모든 측면에서 UNIMO-G의 실질적인 우월성을 보여준다.\n' +
      '\n' +
      '부타주 *: [부츠레티게.바이두.com/](부스트레티게.바이두.com/)\n' +
      '\n' +
      '단일 주체 주체 주도 세대를 위해, 우리는 드림벤치(Ruiz et al., 2023)에서 UNIMO-G를 평가한다. 드림벤치는 25개의 프롬프트 템플릿이 있는 30개의 피험자로 구성되어 있으며, 재맥락화, 수정 및 접근과 같은 테스트 능력이 있는 750개의 고유한 프롬프트를 생성한다. 우리는 각 프롬프트에 대해 4개의 이미지를 생성하기 위해 사전 작업을 따라 종합 평가를 위해 총 3,000개의 이미지를 생성한다. 우리는 피험자 충실도 평가를 위해 DINO 및 CLIP-I 메트릭과 이미지 텍스트 관련성 평가를 위해 CLIP-T를 사용했다. 샘플링에 5.0 및 50 DDIM 추론 단계의 분류기가 없는 안내 척도를 사용했다. 단일 이미지 입력을 수용하는UNIMO-G는 일관된 비교를 위해 KOSMOS-G(Pan et al, 2023)와 동일한 이미지를 사용하였다. 표 2에 표시된 바와 같이 제로샷 설정의 UNIMO-G는 텍스트 전환(Gal et al, 2022), 드림보스(Ruiz et al, 2023), BLIP-확산(Li et al., 2023), Re-이미젠(Chen et al., 2022)과 같은 다른 모델을 능가하고 KOSMOS-G를 약간 능가한다. 특히, 우리의 모델은 밀접하게 관련된 KOSMOS-G와 비교하여 이미지-텍스트 관련성 및 이미지 충실도의 균형을 맞추는 데 상당한 개선을 보여준다. 우리는 기존의 방법이 텍스트 입력에 비해 이미지 정보를 우선시하는 경향이 있음을 관찰했다. 이러한 경향은 간혹 의미론적 내용에 대한 초점이 감소되어 주제 재구성을 선호한다. 우리의 2단계 종단 간 학습 프레임워크 덕분에 UNIMO-G는 높은 이미지 충실도를 유지하고 이미지-텍스트 관련성에 대해 가장 높은 CLIP-T 점수를 달성했으며, 이는 다중 모달 지침에 따라 강력한 능력을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Methods** & **FID** \\\\ \\hline \\multicolumn{3}{c}{_T2I Models_} \\\\ \\hline GLIDE (Nichol et al., 2021) & 12.24 \\\\ DALL-E 2 (Ramesh et al., 2022) & 10.39 \\\\ SDv1.5 (Rombach et al., 2022) & 9.34 \\\\ Imagen (Saharia et al., 2022) & 7.27 \\\\ SDXL (Podell et al., 2023) & 11.93 \\\\ \\hline \\multicolumn{3}{c}{_VL2I Models_} \\\\ \\hline GILL (Koh et al., 2023) & 12.20 \\\\ Emu (Sun et al., 2023) & 11.66 \\\\ KOSMOS-G (Pan et al., 2023) & 10.99 \\\\ UNIMO-G & 8.36 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1:제로 샷 FID-30K 비교는 MSCOCO 256x256에 대한 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Methods** & **DINO** & **CLIP-I** & **CLIP-T** & **Avg** \\\\ \\hline \\multicolumn{5}{c}{_Fine-Tuning Methods_} \\\\ \\hline Textual Inversion & 0.569 & 0.780 & 0.255 & 0.535 \\\\ DreamBooth & 0.668 & 0.803 & 0.305 & 0.592 \\\\ BLIP-Diffusion & 0.670 & 0.805 & 0.302 & 0.592 \\\\ \\hline \\multicolumn{5}{c}{_Zero-Shot Methods_} \\\\ \\hline Re-Imagen & 0.600 & 0.740 & 0.270 & 0.537 \\\\ BLIP-Diffusion & 0.594 & 0.779 & 0.300 & 0.558 \\\\ KOSMOS-G & **0.694** & **0.847** & 0.287 & 0.609 \\\\ UNIMO-G & 0.668 & 0.841 & **0.329** & **0.613** \\\\ w/o Tuning & 0.371 & 0.717 & 0.306 & 0.465 \\\\ w/o VisualEnh & 0.617 & 0.815 & 0.329 & 0.587 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 2>는 드림벤치에 대한 단일 중심 주체 주도 이미지 생성의 비교 __드림벤치에 대한 비교이다. Avg_는 DINO, CLIP-I 및 CLIP-T의 평균 점수를 나타낸다.\n' +
      '\n' +
      '그림 4: 인간 평가에 의한 UNIMO-G 및 SD-XL의 비교이다. 평균 및 표준편차는 그림에 나와 있다.\n' +
      '\n' +
      '다세대\n' +
      '\n' +
      'UNIMO-G는 제로샷 다중중심주제 주도세대에서 탁월한 성능을 보여주고 있다. 이 능력을 평가하기 위해 다중 중심 주체 주도 생성 평가를 위해 특별히 설계된 새로운 벤치마크인 _Multi-Bench_를 구축했다. 멀티벤치는 각각 10개의 다른 물체를 포함하는 살아있는 물체(인간 및 동물), 음식, 웨어러블 아이템 및 장난감 등 4개의 객체 카테고리를 포함한다. 2개 및 3개의 객체로 시나리오를 구성하기 위한 5개의 신속한 템플릿을 개발하여 총 2,300개의 별개의 프롬프트가 생성되었다. 상세 내용은 부록 B에 소개되어 있으며, 각 프롬프트마다 4개의 이미지가 생성되어 배기 평가를 위해 9,200개의 이미지에서 절정에 도달했다. CLIP-T를 사용한 텍스트 관련성 평가와 함께 DINO 및 CLIP-I 메트릭을 사용하여 이미지 유사성 분석을 수행했다. 이미지 유사성은 생성된 이미지와 둘 또는 세 피험자 각각의 유사성을 평균화하여 결정되었다. 표 3과 같이 결과는 UNIMO-G가 이미지 유사성 및 텍스트 관련성 측면에서 BLIP-D확산 및 KOSMOS-G를 능가함을 나타낸다. 일부 비교 예는 그림 5에 나와 있으며, 이는 입력 이미지로부터 피험자 정보를 정확하게 캡처하고 멀티 모달 명령어를 효과적으로 따르는 UNIMO-G의 우수한 능력을 보여준다. 더 많은 예가 그림 5에 나와 있다.\n' +
      '\n' +
      '그림 5: 다중 중심 주제 중심 이미지 생성을 위한 기저부와 비교한다.\n' +
      '\n' +
      '그림 8과 9에 나와 있습니다.\n' +
      '\n' +
      '우리의 모델을 추가로 검증하기 위해 멀티벤치에서 200개의 프롬프트를 샘플링하여 UNIMO-G와 KSOMOS-G를 비교하는 인간 평가를 수행했다. 3명의 래더는 UNIMO-G에 의해 생성된 2개의 이미지 세트와 비교 모델을 제시한다. 그들은 의미론적 관련성, 시각적 충실성 및 이미지 충실도의 세 가지 차원으로부터 이러한 이미지를 비교하는 다음 선호하는 모델 또는 무관심을 선택하도록 요청된다. 공정 전반에 걸쳐, 비준자들은 어떤 모델로부터 이미지가 생성되는지 알지 못한다. 그림 6의 결과는 인간 비준자들이 모든 측면에서 KOSMOS-G보다 UNIMO-G를 크게 선호한다는 것을 보여주며, 이는 자유 형태 다중 모드 프롬프트에서 고품질 개인화된 이미지를 생성하는 데 우리의 접근법의 효과를 추가로 검증한다.\n' +
      '\n' +
      '### Analysis\n' +
      '\n' +
      '멀티 모달 지시 튜닝(w/o 튀기) 전에 멀티 모달 명령 튀기** UNIMO-G의*** 효과성(w/o 튀기)은 또한 MLLM을 기반으로 다중 모달 프롬프트로부터 이미지를 생성하는 능력을 보여준다. 그럼에도 불구하고, 처음에 입력 이미지를 정확하게 재생하는 데 짧습니다. 멀티 모달 지도 튜닝의 효과를 평가하기 위해 단일-엔티티 및 다중-엔티 피사체 주도 생성 작업에서 이러한 튜닝이 있거나 없는 UNIMO-G의 성능을 비교했다. 표 2와 표 3의 비교 결과는 다중 모달 명령 튜닝이 단일 시나리오와 다중 위치 시나리오 모두에서 이미지 유사성 메트릭(DINO 및 CLIP-I)을 실질적으로 향상시킨다는 것을 보여준다. 이러한 개선은 튜닝 후 UNIMO-G가 입력들로부터 시각적 특징을 보다 효과적으로 침출하여 이미지 조건에서 정의된 내용을 정확하게 복제함을 나타낸다. 또한, 명령어 튜닝이 있는 UNIMO-G는 CLIP-T 메트릭에 의해 평가된 바와 같이 텍스트 관련성에서도 명백한 발전을 보여준다. 이는 튜닝 프로세스가 이미지 충실도를 강화할 뿐만 아니라 다중 모드 지침을 따르는 모델의 능력을 증폭시킨다는 것을 나타낸다.\n' +
      '\n' +
      '비주얼 분산 학습*** 다중 모드 수업 튜닝 동안 시각적 강화 학습 전략의 통합은 입력 및 출력 이미지 간의 시각적 정렬을 크게 향상시킨다. 이러한 효과를 정량화하기 위해 복합 튜닝(w/o VisualEnh) 동안 시각적 향상 구성요소를 생략하여 절제 연구를 수행하고 단일-엔티 및 다중-엔티 생성 작업 모두에 미치는 영향을 평가했다. 표 2와 표 3에 자세히 설명된 바와 같이 결과는 시각적 강화 학습 전략이 단일 시나리오와 다중 위치 시나리오 모두에서 이미지 유사성 메트릭(DINO 및 CLIP-I)의 성능을 현저하게 증가시킨다는 것을 보여준다. 특히, 그것은 또한 개체 블렌딩 또는 누락을 감소시켜 다중-엔티 시나리오에서 이미지-텍스트 정렬을 향상시킨다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문은 텍스트와 시각적 입력을 인터리빙하는 멀티모달 프롬프트를 처리하도록 설계된 간단한 멀티모달 조건부 확산 프레임워크인 UNIMO-G를 제시한다. 텍스트 대 이미지 생성 및 제로 샷 주제 구동 합성에서 탁월한 숙련도를 보여주며, 특히 여러 이미지 엔티티와 복잡한 멀티 모달 프롬프트로부터 고 충실도 이미지를 생성하는 데 능숙하다. 표준 텍스트-조건 확산 모델과 비교하여 UNIMO-G는 이미지 생성에서 시각적 제어 가능성을 크게 향상시킨다. 우리의 2단계 훈련 전략 덕분에 UNIMO-G는 특히 복잡한 다중 모드 지침을 따르는 능력에 대한 기존 VL 대 이미지 모델도 능가한다. 전반적으로 UNIMO-G는 보다 정교하고 통제된 이미지 생성 과정의 가능성을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l} \\hline \\hline\n' +
      '**Methods** & **DINO** & **CLIP-I** & **CLIP-T** & **Avg** \\\\ \\hline BLIP-Diffusion & 0.410 & 0.648 & 0.249 & 0.436 \\\\ KOSMOS-G & 0.419 & **0.671** & 0.283 & 0.458 \\\\ UNIMO-G & **0.436** & 0.665 & **0.298** & **0.466** \\\\ w/o Tuning & 0.235 & 0.583 & 0.240 & 0.353 \\\\ w/o VisualEnh & 0.399 & 0.631 & 0.276 & 0.435 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '멀티벤치에 대한 다중 중심 주체 주도 이미지 생성의 비교 표 3과 같다.\n' +
      '\n' +
      '그림 6: 인간 평가에 의해 멀티벤치에 대한 UNIMO-G 및 KOSMOS-G의 비교이다. 평균 및 표준편차는 그림에 나와 있다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '시첸 판, 리동, 샤오한 황, 지리랑 펑, 원후 첸, 푸루 웨이가 있다. 다중 모드 대형 언어 모델과의 맥락에서 이미지를 설명하는 __2023. 코스모스-g: 다중 모드 대형 언어 모델과의 맥락에서 이미지를 촬영한다. arXiv 프리프린트 arXiv:2310.02992_입니다.\n' +
      '* 포델 등은 (2023) 더스틴 포델, 지온 영어, 카일 레이시, 안드레아스 블라트만, 팀 도코렌, 조 뮬러, 조펜나, 로빈 람바흐 등을 들 수 있다. 고해상도 이미지 합성을 위한 잠재 확산 모델 개선:2023. Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. arXiv 프리프린트 arXiv:2307.01952_입니다.\n' +
      '* 라즈(2022) 아디야 레즈, 프라풀라 다라리왈, 알렉스 니콜, 케이시 추, 마크 첸 등이 있다. 클립 래치들이 있는 계층적 텍스트-조건적 이미지 생성 __ 클립 래치들이 있는 계층적 텍스트-조건적 이미지 생성. arXiv 프리프린트 arXiv:2204.06125_, 1(2):3이다.\n' +
      '* 람바흐 등(2022) 로빈 라이바흐, 안드레아스 블라트만, 도미니크 로렌츠, 패트릭 에저, 비콘 오머 등이 있다. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 10684-10695에 대한 IEEE/CVF 회의의 _프로젝션에서.\n' +
      '* Ronneberger et al. (2015) Olaf Ronneberger, 필리필 피셔 및 토머스 Brox. 2015. U-net: 생물의학적 이미지 세분화를 위한 컨솔루션 네트워크. E_ 의료 이미지 컴퓨팅 및 컴퓨터 보조 개입-MICCAI 2015: 제18차 국제 회의, 뮌헨, 독일, 2015년 10월 5-9일, 제작, 파트 III 18_, 페이지 234-241. 스프링거.\n' +
      '* 루즈 등은 (2023) 나타니엘 루즈, 원안전 리, 바룬 탬파니, 야엘 프릿치, 마이클 루비틴, 카피르 애버만 등이 있다. 2023. 드림보스: 주제 중심 생성을 위한 텍스트 대 이미지 확산 모델입니다. 컴퓨터 비전 및 패턴 인식_ 페이지 22500-22510에 대한 IEEE/CVF 회의의 _발표에서.\n' +
      '*사아리아 등은 윌리엄 샤아리아, 윌리엄 찬, 소라바 시세나, 라라 리, 자 휘앙, 에밀릴 리 디펜턴, 카파파 게르미포, 라파엘 고몬티조 로프, 버쿠 카가골 아얀, 팀 살림산 등 2022년 언어 이해가 깊은 사진 현실론적 텍스트 대 이미지 확산 모델이다. 신경정보처리시스템_, 35:36479-36494의 효과.\n' +
      '* 슈하만(2022) 크리스토프 슈하만(2022) 크리스틴 슈하만, 로메인 베어먼, 리처드 버몬, 케이드 고든, 로즈 웨이트만, 메흐디 커티, 테오 코퍼티, 아루시 카타, 클레이튼 풀리스, 미첼 위츠만 등 2022년 라온-5b는 차세대 이미지-텍스트 모델을 훈련하기 위한 대규모 데이터세트를 오픈한다. 신경정보처리시스템_, 35:25278-25294의 효과.\n' +
      '* 샤마 등은 (2018) Piyush Sharma, Nan Ding, Sebastian 굿만 및 Radu Soricut. 2018년 개념 캡션: 자동 이미지 캡싱을 위한 세척, 과공, 이미지 제트-텍스트 데이터셋. 제56회 컴퓨터 통계협회 연차 회의(1: 롱 파이어)_, 2556-2565쪽)의 _검토에서.\n' +
      '*시 등은 (2023) 징시, 위시온그, 저장린, 현준 등이 있다. 테스트 타임 핀셋링 없이 개인화된 텍스트 대 이미지 생성: __ Instantbooth: 개인화된 텍스트 대 이미지 생성. arXiv 프리프린트 arXiv:2304.03411_입니다.\n' +
      '* 송 등은 송(2020)의 지밍 송, 첸린 멍, 스테파노 에르몬 등이 있다. M____. 덴오징 확산 암묵 모델. __2020. 덴오징 확산 암묵 모델. __. 덴오징 확산 함축 모델. arXiv 프리프린트 arXiv:2010.02502_입니다.\n' +
      '*선(2023) 취안선, 기잉유, 유펑쿠이, 판장, 샤오송장, 유제왕, 홍청가오, 징징류, 티준황, 신장왕 등이 있다. 다중성 전치. __ 다중성 전치.2023. __ 다중성 전치. arXiv 프리프린트 arXiv:2307.05222_.\n' +
      '*웨이 등은 (2023) 유시앙위, 야보장, 자룽지, 진펑바이, 르장, 왕멍주오. 2023. 엘라이트: 시각적 개념을 맞춤형 텍스트 대 이미지 생성을 위한 텍스트 임베딩으로 텍스트 임베딩으로 코딩한다. arXiv 프리프린트 arXiv:2302.13848_입니다.\n' +
      '* 위인바흐(2022) 사마엘 위인바흐, 마르코 벨라젠테, 콘스탄틴 에셴버그, 앤드루 다이, 로버트 발닥, 스롤데프 나다, 보렌 데세오세르, 코엔 오스테네제, 한나 테우펠, 안드레스 펠리페 크루즈-살린나스 등이다. 다중 모드 맥락으로 확산하기 위한 모델 __M-바더: M-바더: 다중 모드 컨텍스트로 확산하기 위한 모델. arXiv 프리프린트 arXiv:2212.02936_.\n' +
      '* 샤오 등은 (2023) 광수안 샤오, 톈웨이 진, 윌리엄 티 프로만, 프레도 듀란드, 송한 등이 있다. 라스트코포저: __astcomposer: 국소적인 관심을 가진 튀김이 없는 다중 대상 이미지 생성. arXiv 프리프린트 arXiv:2305.10431_.\n' +
      '* 주 등은 (2023) 데야오 주, 준 첸, 샤오치안 선, 샤앙 리, 모하메드 엘호세니 등이 있다. 2023. 미니pt-4: 고급 대형 언어 모델을 사용한 비전-언어 이해 향상. __Chigpt-4: 첨단 대형 언어 모델을 사용하여 시력-언어 이해도를 향상시킵니다. arXiv 프리프린트 arXiv:2304.10592_.\n' +
      '\n' +
      '신청자 A\n' +
      '\n' +
      '훈련 과정에는 텍스트 대 이미지 사전 훈련 및 멀티 모달 명령어 튜닝이 포함됩니다.\n' +
      '\n' +
      '우리의 모델의 사전 훈련은 각각의 별개의 데이터 세트를 사용하여 세 단계를 포함한다.\n' +
      '\n' +
      '1. CC3M Dataset을 사용한 사전 훈련: CC3M 데이터셋은 약 3.3M 이미지 설명 쌍으로 구성되며 바이두 트랜스레이트 API *를 사용하여 중국어로 번역되었다. 모델은 체중 붕괴가 0.01인 AdamW 최적화기를 사용하여 256x256에서 처음부터 훈련되며, 학습 속도는 5e-5이고, 배치 크기는 100K 단계에 대해 40x256이다. 부타주 *: [https://fanyi.baidu.com](https://fanyi.baidu.com)\n' +
      '2의 대규모 중국 데이터 확장: LAION-2B(Schuhmann et al., 2022), COYO-700M(B연 et al., 2022), 개념적 랩션(창피요 et al., 2021) 및 일련의 중국 내부 데이터 세트를 포함한 여러 데이터 세트의 약 300M 내부 중국 텍스트 이미지 쌍을 통합한다. 영어 캡션은 중국어로 번역됩니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '그림 8: UNIMO-G에 의한 멀티 벤치에 대한 다중 중심 주제 주도 이미지 생성의 실시예이다.\n' +
      '\n' +
      '그림 9: UNIMO-G에 의한 멀티 벤치에 대한 다중 중심 주제 주도 이미지 생성의 실시예이다.\n' +
      '\n' +
      '그림 10: UNIMO-G에 의한 텍스트 대 이미지 생성의 예.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>