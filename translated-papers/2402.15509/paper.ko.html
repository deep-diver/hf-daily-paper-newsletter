<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '혼합 위치 엔코딩을 이용한 이음매 없는 인체 동작 구성\n' +
      '\n' +
      '독일 바케로 세르히오 에스칼레라 크리스티나 팔메로\n' +
      '\n' +
      '스페인 바르셀로나대학교 컴퓨터비전센터\n' +
      '\n' +
      '{germanbarquero, sescalera}@ub.edu, crpalmec7@umnes.ub.edu\n' +
      '\n' +
      '[https://barquerogerman.github.io/FlowMDM/](https://barquerogerman.github.io/FlowMDM/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '조건부 인간의 동작 생성은 가상 현실, 게임 및 로봇 공학에서 많은 응용 분야에서 중요한 주제이다. 이전 작업은 텍스트, 음악 또는 장면에 의해 안내되는 동작을 생성하는 데 중점을 두었지만 일반적으로 짧은 기간에 국한된 고립된 동작을 초래한다. 대신, 일련의 다양한 텍스트 설명에 의해 안내되는 길고 연속적인 시퀀스의 생성을 다룬다. 이러한 맥락에서, 우리는 후처리나 중복 잡음 제거 단계 없이 끊김 없는 인간 움직임 구성(HMC)을 생성하는 첫 번째 확산 기반 모델인 FlowMDM을 소개한다. 이를 위해 우리는 잡음 제거 체인에서 절대 및 상대 위치 인코딩을 모두 활용하는 기법인 혼합 위치 인코딩을 소개한다. 보다 구체적으로, 글로벌 모션 코히어런스는 절대 단계에서 회복되는 반면, 매끄럽고 현실적인 전환은 상대 단계에서 구축된다. 그 결과, 바벨과 휴먼ML3D 데이터셋에 대한 정확성, 사실성, 매끄러움 측면에서 최신의 결과를 얻을 수 있었다. FlowMDM은 Pose-Centric Cross-ATtention 덕분에 모션 시퀀스당 단일 설명만으로 훈련될 때 탁월하며, 이는 추론 시간에 다양한 텍스트 설명에 대해 견고하다. 마지막으로, 기존 HMC 메트릭의 한계를 해결하기 위해 피크 저크와 저크 아래의 영역이라는 두 가지 새로운 메트릭을 제안하여 급격한 전환을 감지한다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '컴퓨터 비전 분야에서는 최근 가상현실, 게임, 로봇공학 등의 응용을 위한 실사 아바타 개발[54]이 진행되고 있다[62, 79]. 시각적으로 사실적으로 보이는 것 외에도 이러한 아바타는 설득력 있는 방식으로 움직여야 합니다. 이는 환경, 상호 작용, 신체 접촉 등 다양한 요인에 의해 강하게 영향을 받는 인간 동작의 복잡한 특성으로 인해 어렵다[14]. 또한 이러한 동작을 제어하려고 할 때 복잡성이 증가한다. 최근의 진보들은 텍스트 설명들 또는 액션들(109)과 같은 _제어 신호들로부터의 모션 시퀀스들의 생성을 포함하지만, 그러한 방법들은 고립된 독립형 모션만을 생성한다. 따라서, 이러한 접근법들은 긴 모션이 상이한 시간 슬라이스들 상의 별개의 제어 신호들에 의해 구동되는 시나리오들을 처리하는데 실패한다. 이러한 능력은 원하는 동작의 순서 및 그 기간에 대한 완전한 제어를 제공하기 위해 필요하다. 이러한 시나리오에서, 생성된 모션은 액션들 사이의 끊김 없고 현실적인 전환을 특징으로 할 필요가 있다. 이 작업에서 우리는 생성적 인간 동작 구성(HMC)이라고 하는 이 문제를 해결한다. 특히, 우리는 그림 1에 표시된 텍스트에서 단일 인간 동작을 생성하는 데 중점을 둔다.\n' +
      '\n' +
      'HMC의 주요 장애물 중 하나는 다양한 텍스트 주석이 있는 긴 모션 시퀀스를 제공하는 데이터 세트의 부족이다. 기존 데이터 세트는 일반적으로 제한된 지속 시간의 시퀀스를 특징으로 하며, 종종 최대 10초까지만 지속되며, 전체 시퀀스를 제어하는 단일 제어 신호[26, 64]를 갖는다. 이러한 한계는 과제의 내재된 복잡성을 해결하기 위한 혁신적인 해결책을 요구한다. 이전 작업은 대부분 자기회귀 접근법[4, 45, 48, 66, 104]으로 이 문제를 해결했다. 이러한 방법들은 현재 모션을 기초로 하여 후속 모션을 생성함으로써 반복적으로 구도를 생성한다. 그러나, 이들은 다수의 연속적인 주석이 달린 모션들을 갖는 데이터세트들을 필요로 하고, 에러 누적으로 인해 매우 긴 HMC 시나리오들에서 퇴화하는 경향이 있다[107]. 다른 최근의 작업들은 모션 조성물들을 생성하기 위해 모션 확산 모델들의 주입 능력들을 활용했다[73, 103]. 그러나, 이것들에 대해, 각각의 모션 시퀀스의 상당 부분은 인접한 모션들로부터 독립적으로 생성되고, 트랜지션들을 생성하는 것은 여분의 잡음 제거 단계들을 컴퓨팅하는 것을 필요로 한다. 이 작업에서는 이러한 특정 문제를 해결하기 위해 설계된 새로운 아키텍처를 제안한다. 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 후처리나 추가적인 잡음 제거 단계 없이 끊김 없는 인간 모션 조성물을 생성하는 최초의 확산 기반 모델인 FlowMDM을 제안한다. 이를 위해 샘플링 시 절대 및 상대 위치 인코딩의 장점을 결합한 새로운 확산 트랜스포머 기법인 BPE(Blended Positional Encodings)를 소개한다. 특히, 잡음 제거는 먼저 절대 정보를 이용하여 전역 움직임 일관성을 회복한 후, 상대적 위치를 활용하여 행동 간의 원활하고 현실적인 전환을 구축한다. 그 결과 FlowMDM은 HumanML3D[26]와 Babel[65] 데이터셋에서 정확성, 사실성, 평활성 측면에서 최첨단 결과를 달성하였다.\n' +
      '* HMC에 맞춘 새로운 어텐션 기법인 PCCAT(Pose-Centric Cross-ATtention)을 소개한다. 이 계층은 각각의 포즈가 그 자신의 상태 및 그 이웃 포즈들에 기초하여 잡음제거되는 것을 보장한다. 결과적으로, FlowMDM은 모션 시퀀스당 이용 가능한 단일 조건만을 갖는 데이터세트 상에서 트레이닝될 수 있고, 추론 시간에 다수의 조건들을 사용할 때 여전히 현실적인 전이들을 생성할 수 있다.\n' +
      '* 불연속적이거나 날카로운 전이를 식별하기 위해 현재 HMC 메트릭의 감도 부족을 밝히고, 이를 감지하는 데 도움이 되는 두 가지 새로운 메트릭인 피크 저크(PJ)와 에어리어 언더 저크(AUJ)를 소개한다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**조건부 인간 동작 생성.** 모션 생성의 최근 연구들은 텍스트[21, 26, 27, 35, 40, 63, 81, 100, 101, 102], 음악[2, 17, 47, 77, 84, 96, 110], 장면[15, 87, 88, 89, 97], 상호작용 객체[1, 18, 42, 92] 및 심지어 다른 인간의 행동[93, 10, 28, 80, 9]과 같은 다양한 모달리티들에 조건부 동작들을 합성하는데 주목할 만한 진전을 보여주었다. 전통적으로, 이러한 접근법들은 단일 조건에 매칭되는 모션 시퀀스들을 생성하도록 설계되어 왔다. 이 도메인의 진행은 다양한 양식 또는 수동 주석[12, 26, 28, 47, 51, 60, 64, 65]을 포함한 빅 데이터 세트의 릴리스에 의해 촉진되었다. 연구는 또한 인간의 모션 예측[3, 53, 57, 72, 78, 83, 86, 99] 및 모션 채우기[29, 36, 39, 49, 50, 59, 67, 75, 108]와 같은 문제에 초점을 맞추었으며, 이는 광범위한 수동 주석에 의존하지 않고 모션 자체에 의존한다. 두 작업 모두 HMC와 공통 과제를 공유하는데, 합성 운동은 그럴듯할 뿐만 아니라 이웃 행동과 매끄럽게 통합되어 유동성과 연속성을 보장해야 한다. 이러한 맥락에서, 인간 동작 사전의 활용은 임의의 생성된 동작이 자연적 전이를 포함하는 것을 보장하는 성공적인 기술임이 입증되었다[8, 46, 91]. 이러한 접근법에 따라, 본 방법은 HMC에 특별히 맞춤화된 동작을 미리 학습한다.\n' +
      '\n' +
      '**자기회귀적 인간 움직임 구성.** 다른 많은 서열 모델링 작업에서와 같이 HMC도 자기회귀적 방법으로 처음 해결되었다. 금본위제는 변분 오토인코더를 순환 신경망[104] 또는 트랜스포머[4, 45, 66, 4]와 같은 자기회귀 디코더와 페어링해 왔다. 대안적인 접근법들은 전문화된 강화 학습 프레임워크들을 도입했다[52, 95, 105]. 자기회귀 모델은 주석이 달린 모션 전환의 가용성에 의존하며, 이는 이러한 데이터의 부족으로 인해 모델의 견고성을 제한하는 요구 사항이다. 이 문제를 완화하기 위해 일부 방법에는 선형 보간[4] 또는 아핀 변환[45]과 같은 추가 후처리 단계가 포함된다. 그러나, 이들은 인간의 운동 역학을 왜곡시킬 수 있고 전이 지속기간의 미리 결정된 추정을 필요로 한다. 또한, 자기회귀 접근법은 선행 모션만을 기반으로 모션을 생성한다. 우리는 정확한 모델이 인간의 타고난 능력을 모방하여 다음 행동을 예측하고 그에 따라 현재의 행동을 적응시켜야 한다고 주장한다[43, 24].\n' +
      '\n' +
      '**확산 기반 인간 움직임 구성**확산 모델은 조건부 생성[74, 32, 20]에서 우수했다. 또한 이미지 인페인팅(70)에 대한 뛰어난 제로샷 기능과 모션의 동등성, 모션 주입 기능을 갖추고 있습니다. DiffCollage[103], MultiDiffusion[7], DoubleTake[73]은 확산 샘플링 과정을 수정하여 시간적으로 중첩된 움직임 시퀀스를 동시에 생성하고, 중첩된 영역에서 추정된 잡음을 결합하여 채워진 전이가 나타나도록 제안하였다. 이중 테이크는 나타난 전환이 추가 무조건 잡음 제거 단계를 겪는 정제 단계로 이러한 중첩 샘플링을 보완했다. 이 모든 방법은 두 가지 주요 한계를 공유한다. 먼저, 이웃한 모션 시퀀스들 간의 종속성을 모델링하는 것에 제약을 받는다. 이는 연속되는 3개 이상의 액션이 의미론을 공유하고 집합적으로 보다 포괄적인 액션을 대표할 때 한계가 된다. 이 경우, 모션 종속성은 연속적인 동작을 넘어 확장될 수 있다. 둘째, 추가 계산이 통합되는 연속 작업 간에 취하는 각 전환의 프레임 수를 설정해야 합니다. 우리의 작업은 추가 계산 부담이나 미리 정의된 전환 기간을 부과하지 않고 더 긴 서열 간 역학을 모델링할 수 있는 솔루션을 제공하여 이러한 제약을 해결하려고 한다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '문제 정의.** 우리의 목표는 \\(N\\) 프레임들의 모션 시퀀스를 생성하는데 있다. \\(0,\\tau_{1}), [\\tau_{1},\\tau_{2}),...,[\\tau_{j},N)\\), \\(0{<}\\tau_{1}{<}\\tau_{j}{<}N\\). 우리는 이 구간들 내부의 움직임을 _motion 서브시퀀스_, 또는 \\(\\mathcal{S}_{i}=\\{x_{\\tau_{i}},...,x_{\\tau_{i+1}-1}\\})라고 부르며, 각각은 해당 조건 \\(c_{i}\\)에 의해 구동되며, 최대 길이는 \\(L\\)이다. 연속 서브시퀀스는 서로 다른 제어 신호에 의해 영향을 받고 매끄럽고 현실적으로 전환되는 것이 필수적이다. 특히, 우리는 몇 쌍의 \\((S_{i},c_{i})\\)을 포함하는 모션 시퀀스가 데이터 세트에서 반드시 이용 가능하지 않은 훨씬 더 어려운 경우를 목표로 한다.\n' +
      '\n' +
      '이 섹션에서는 **강력한 번역 불변 운동의 출현을 촉진하는 강력한 귀납적 편향을 가진 아키텍처인 FlowMDM을 제시한다. 이러한 _motion prior_는 선행 작업 [73, 82]와 유사하게 양방향(즉, 인코더 전용) 트랜스포머가 장착된 확산 모델로 학습된다. 이를 통해 자기회귀법의 주요 한계점(Sec. 3.1)을 극복하였다. 그러나, 이전 연구들은 움직임의 지속시간 측면에서 제약을 받는다. 우리는 틀림없이 절대 위치 인코딩을 상대적 대안으로 대체하여 각 포즈 _번역 불변_의 잡음 제거를 함으로써 확산 모델에 외삽 기능을 제공할 수 있다. 그러나, 이 기술은 각 서브시퀀스의 시작과 끝에 대한 지식을 필요로 하는 복잡한 구성 의미론을 구축하지 못할 것이다. 예를 들어, \\(c_{i}{=}\\)_walking_\'와 \\(c_{i+1}{=}\\)_walk and sit down_\'를 갖는 동작구성 \\(S_{i}{\\rightarrow}S_{i+1}\\)을 생성할 때, \\(S_{i+1}\\)은 단지 상대적인 위치 정보만을 가지고 있어서, 트랜스포머는 \\(S_{i+1}\\)의 시작에 선행하는 부분적으로 잡음 제거된 \'_walking_\' 동작이 \\(S_{i}\\) 또는 \\(S_{i+1}\\)에 속하는지 알 수 없기 때문에 동작 \'_sit down_\'만을 특징짓을 수 있다. 상대적 위치 부호화와 절대적 위치 부호화의 장점을 결합하기 위해 BPE(Sec. 3.2)를 소개한다. 이 새로운 기술은 확산 모델의 반복적인 특성을 활용하여 초기 잡음 제거 단계에서 후속 잡음 제거 단계 번역을 불변으로 만드는 동시에 하위 시퀀스 간에 사실적이고 그럴듯한 전이가 자연스럽게 나타나도록 한다. 여전히, 훈련 동안, 상태는 모든 지상 진실 동작 시퀀스들 전체에 걸쳐 변하지 않고 유지된다. 추론 시 시퀀스당 여러 조건을 갖는 데노이징 모델 _robust_을 만들기 위해 PCCAT(Sec. 3.3)라는 새로운 주의 패러다임을 소개한다. 결과적으로, FlowMDM은 트랜지션 생성에 대한 명시적인 감독 없이, 인간 모션 서브시퀀스들의 매우 긴 구성들을 모두 조화시키고 그들 사이의 그럴듯한 트랜지션들을 육성하는 동시에 생성할 수 있다.\n' +
      '\n' +
      '### Bidirectional diffusion\n' +
      '\n' +
      '자기 회귀 모델에서 오류의 누적 특성은 종종 긴 시퀀스를 생성할 때 성능 저하를 초래한다[107]. 이것은 훈련 코퍼스에서 전이가 부족하거나 심지어 누락된 HMC에서 악화되며, 모델은 추론 시 도메인 이동을 다룰 필요가 있다. 자기회귀법의 또 다른 한계는 생성된 \\(\\{\\mathcal{S}_{i}\\)는 \\(\\{\\mathcal{S}_{j}\\}_{j<i}\\)에만 의존한다는 것이다. 우리는 Sec에서 논의했다. 2 이것이 HMC에 대한 차선책 솔루션인 이유. 따라서, HMC에 적합한 모델은 다음 동작인 \\(\\mathcal{S}_{i+1}\\)을 예측할 수 있어야 하며, 변환이 가능하도록 \\(\\mathcal{S}_{i}\\)을 적응시킬 수 있어야 한다. 확산 모델의 반복적 패러다임은 이러한 능력을 자연스럽게 모방할 수 있는 매우 적절한 귀납적 편향을 제공한다고 주장한다. 즉, 부분적으로 잡음 제거된 \\(\\mathcal{S}_{i}\\)과 \\(\\mathcal{S}_{i+1}\\)은 연속적인 잡음 제거 단계에서 나중에 정제된다. 양방향 트랜스포머를 노이즈 제거 함수로 선택함으로써 [38] 과거 종속성과 미래 종속성을 모두 모델링할 수 있다. 따라서, 우리는 MDM[82]과 유사한 양방향 모션 확산 모델로 프레임워크를 설계한다. 확산 모델의 이론적 측면에 대한 자세한 내용은 독자에게 [94]를 참조한다.\n' +
      '\n' +
      '혼합 위치 인코딩\n' +
      '\n' +
      '확산 모델은 생성된 모션이 사실적이고 그럴듯하다는 것을 보장하는 강력한 모션 전조를 학습할 수 있다[73]. 사실, 그들은 또한 서브시퀀스들[7, 73, 103] 사이의 부드러운 전이들을 생성할 수 있다. 그러나 이러한 기능은 추론 시간 모션 주입 기술에서 비롯되며, 이는 인간의 모션 전위의 잠재력을 완전히 활용하지 않는다고 주장한다. 사실, 훈련 중에 관찰된 것보다 더 긴 서열로 잘 외삽하는 이전을 구축하는 것은 매우 어렵다. 자연어 처리 분야는 특히 절대 위치 인코딩(APE)을 상대(RPE) 대응물로 대체함으로써 시퀀스 외삽 기술에서 진전을 이루었다[37]. 토큰 사이에 얼마나 멀리 있는지에 대한 정보만 제공함으로써 시퀀스별 번역 불변성을 달성하므로 모델링 기능을 더 긴 시퀀스로 추론할 수 있다. 그러나, 동작의 시작 및 종료까지의 거리를 포함하는 동작 내의 포즈의 절대 위치는 이 섹션의 시작에서 예시된 바와 같이 동작의 글로벌 의미론을 구축하기 위해 필요하다.\n' +
      '\n' +
      '본 논문에서는 전역 움직임의 의미를 유지하면서 움직임 외삽을 가능하게 하는 확산 모델을 위해 설계된 새로운 위치 부호화 기법인 BPE를 제안한다. 우리의 BPE는 움직임에서 높은 주파수가 국부적인 미세한 세부 사항을 포괄하는 반면 낮은 주파수는 전역 구조를 포착한다는 관찰에서 영감을 받았다. 이미지에 대해서도 유사한 통찰이 도출되었다[61]. 확산 모델은 생성 과정을 분해하여 더 낮은 주파수를 회복하고 점점 더 높은 주파수로 전환하는 데 있다. 도. 도 2는 초기 잡음 제거 단계에서 모션 확산 모델이 프로세스가 전개될 때 전역 프레임 간 종속성을 우선시하여 로컬 상대적 종속성으로 이동하는 방법을 보여준다. 제안된 BPE는 추론 동안 이러한 역학을 조화시킨다: 초기 잡음 제거 단계에서 우리의 잡음 제거 모델은 APE를 공급받고 결론을 위해 RPE를 공급받는다. 스케줄러가 이 전환을 안내합니다. 그 결과, 잡음 제거 초기에 인트라-서브시퀀스 전역 의존성이 회복되고, 이후에 인트라- 및 인터-서브시퀀스 모션 평활성 및 현실성이 촉진된다. 추론 시 모델이 APE와 RPE를 이해하도록 하기 위해 훈련 중에 무작위로 번갈아 가면서 두 인코딩에 노출시킨다. 결과적으로, BPE 스케줄은 추론 시간에 튜닝되어 인트라-서브시퀀스 코히어런스와 인터-서브시퀀스 리얼리즘 트레이드-오프의 균형을 맞출 수 있다.\n' +
      '\n' +
      '** 회전 위치 인코딩(RoPE).** RPE에 대한 우리의 선택은 회전 임베딩[76]이다. RoPE는 위치 임베딩을 쿼리와 키에 통합하여 도트 곱 곱셈 후 주의 점수의 위치 정보가 쿼리와 키 사이의 상대적인 쌍별 거리만을 반영하도록 한다. 구체적으로 \\(W_{q}\\)과 \\(W_{k}\\)을 질의와 키의 \\(d\\)차원 공간으로의 투영행렬이라고 하자. 그런 다음 RoPE는 투영된 포즈(x_{m},x_{n}\\(q_{m}{=}W_{q}x_{m}\\)와 키(\\(k_{n}{=}W_{k}x_{n}\\))의 절대 위치(\\(m\\)와 \\(n\\)를 각각 \\(d\\)차원 회전(R_{m}^{d},R_{n}^{d}\\)으로 인코딩한다. 회전각은 주의력 공식이 되도록 \\(m\\) 및 \\(n\\)에 의해 매개변수화된다:\n' +
      '\n' +
      '[q_{m}^{T}k_{n}=(R_{m}^{d}W_{q}x_{m})^{T}(R_{n}^{d}W_{k}x_{n})=x_{m}^{T}W_{q}R_{n-m}^{T}W_{k}x_{n}. \\tag{1}\\}\n' +
      '\n' +
      '결과적으로 회전\\(R_{n-m}^{d}\\)은 \\(n\\)과 \\(m\\) 사이의 거리에만 의존하며 \\(n\\) 또는 \\(m\\)에 대한 절대 정보는 제거된다는 점에 유의한다. RoPE는 주의가 집중되기 전에 단순하고 편리한 주입으로 인해 우리의 RPE에게 자연스러운 선택이다. 결과적으로 RoPE는 플래시 어텐션[22, 23]과 같은 더 빠른 어텐션 기술과 호환된다.\n' +
      '\n' +
      '**정현파 위치 부호화.** 우리의 APE는 고전적인 정현파 위치 부호화[85]로서, 사인 및 코사인 함수를 이용하여 위치 정보를 주입한다. 어텐션 레이어의 쿼리, 키 및 값에 추가됩니다.\n' +
      '\n' +
      'APE의 경우, 주의는 각각의 서브시퀀스에 제한되는 반면, RPE의 경우, 주의는 주의 수평선 \\(H{<}L{<}N\\)까지 모든 프레임에 걸쳐 있다. RPE 훈련 시 학습되는 운동역학의 최대 범위를 \\(L\\)으로 정의하기 때문에, \\(H{\\geq}L\\)(Tabs. D/E in supp. material)의 설정에는 이점이 없다. APE 및 RPE 제약 조건을 모두 활용하면 메모리와 계산 모두에서 최대 서브시퀀스 길이 \\(L\\)에 걸쳐 2차 복잡성이 보장된다[11]. 그 결과, FlowMDM의 복잡도는 다른 Transformer 기반 움직임 확산 모델[73, 103]과 동등하다.\n' +
      '\n' +
      '### Pose-centric cross-attention\n' +
      '\n' +
      '확산 모델을 이용한 움직임 생성을 효율적으로 하기 위해, 우리는 매우 긴 시퀀스를 생성하고자 한다. 모션 트랜스포머에서, 생성은 토큰으로서 상기 조건을 주입함으로써 시퀀스 레벨에서 컨디셔닝되거나[82], 또는 중간 계층들에서 시퀀스-별 변환으로서 컨디셔닝된다[102]. 그러므로, 이들은 상이한 서브시퀀스에서 다수의 신호들에 대해 컨디셔닝될 수 없다. 이러한 이유로 HMC를 위한 확산 기반 방법은 개별적으로 시퀀스를 생성한 다음 병합하는 것을 선택했다[73, 103]. 추가적인 후처리 없이 이러한 동시 이질적인 컨디셔닝을 가능하게 하기 위해, 우리는 매 프레임마다 컨디션을 주입하는 것을 제안한다. 그러나, 우리는 여전히 도전을 처리해야 한다: 훈련 시간에 그 상태는 결코 변하지 않는다. 따라서 추론 시 주의점수는 포즈 조건 쌍(\\(x_{m}\\), \\(c_{m}\\)) 및 (\\(x_{n}\\), \\(c_{n}\\))의 임베딩(E_{x_{m},c_{m}}\\)과 \\(x_{n}\\), \\(c_{n}\\))으로 계산된다.\n' +
      '\n' +
      '[q_{m}^{T}k_{n}=(W_{q}E_{x_{m},c_{m}})^{T}(W_{k}E_{x_{n},c_{n}})=E_{x_{m},c_{m}}^{T}W_{q}^{T}W_{k}E_{x_{n},c_{n}}}. \\tag{2}\\}\n' +
      '\n' +
      '(c_{m}{\\neq}c_{n}\\)일 때, \\(q_{m}^{T}k_{n}\\)은 훈련 중에 결코 마주치지 않았다. 모든 프레임에서 조건을 주입하는 대신 교차 주의 레이어를 사용하면 뚜렷한 조건도 시간적으로 혼합되어 동일한 문제에 직면할 수 있다. 이러한 훈련 추론 오정렬의 존재와 영향을 줄이기 위해 PCCAT를 도입한다. 3은 조건과 시끄러운 포즈 사이의 얽힘을 최소화하는 것을 목표로 한다. 구체적으로, PCCAT는 모든 프레임의 노이즈 포즈 및 상태를 쿼리로 결합하면서, 노이즈 포즈만을 키 및 값으로 사용한다. 따라서, Eq. 도 2는,\n' +
      '\n' +
      '[q_{m}^{T}k_{n}=(W_{q}E_{x_{m},c_{m}})^{T}(W_{k}E_{m},c_{m}}}^{T}W_{q}^{T}W_{k}E_{x_{n}}. \\tag{3}\\}\n' +
      '\n' +
      'PCCAT를 사용하면 포즈에 대한 주의 출력은 이웃한 잡음 포즈의 값 투영의 가중 평균이 된다. 잔여 연결은 PCCAT 출력을 노이즈 포즈에 추가합니다. 트레이닝 데이터세트 내의 모션 스펙트럼의 포괄적인 커버리지를 가지고, 네트워크는 특히 그의 로컬 이웃 내에서 각각의 포즈에 선행하고 후속하는 다양한 포즈들을 관찰한다. 그러므로, 지역 관계들은 보이지 않는 중간 표현들로 고통받지 않는다. 여전히, 해결해야 할 장애물이 있다: 장거리 de\n' +
      '\n' +
      '도 2: 정현파 절대 위치 인코딩을 갖는 확산-기반 모션 생성 모델에서 (x-축)에 참석한 포즈의 함수로서 단일 쿼리 포즈(현재 프레임)의 주의 점수. 곡선은 각 잡음 제거 단계에서 점수를 표시합니다. 우리는 초기 단계가 강력한 글로벌 종속성(파란색)을 나타내는 반면, 나중에 잡음 제거 단계가 명확한 로컬 행동(빨간색)을 나타낸다는 것을 관찰한다.\n' +
      '\n' +
      '급료. 그러나 Sec. 3.2에서 논의된 바와 같이, 그들의 중요성은 대부분 잡음 제거의 초기 단계에 국한된다. 거기에서, 네트워크는 매우 시끄러운 모션 데이터에 노출되고, 따라서 그러한 보이지 않는 포즈들의 조합들에 견고해진다. 최신 잡음 제거 단계에서 네트워크가 거의 깨끗한 입력 시퀀스를 다룰 때, 글로벌 종속성은 이미 개발되어 관심이 짧다(도 2).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '**Datasets.** 우리의 실험은 Babel[65] 및 HumanML3D[26] 데이터세트에 대해 열차와 테스트 분할을 사용하여 수행된다. HumanML3D는 각 모션 시퀀스에 대한 여러 텍스트 설명을 특징으로 하지만 명시적인 전환 주석이 부족하여 지도 학습이 전환 생성에 불가능한다. 반면에 바벨은 전이를 포함하여 원자 수준에서 미세하게 세분화된 텍스트 설명을 제공하며, 이는 보다 정확하고 동적 모션 제어를 용이하게 하지만 빠르고 짧은 전이로 인해 더 큰 도전을 제공한다. FlowMDM의 유연성을 입증하기 위해 각 데이터 세트와 함께 제공되는 표준 모션 표현을 사용한다. HumanML3D는 관절 좌표, 각도, 속도 및 발 접촉을 포함하는 263D 포즈 벡터를 이용한다. 대조적으로, 바벨은 [63]에서와 같이 SMPL 모델 조인트[13]의 전역 위치 및 배향 및 6D 회전 표현[106]을 사용한다.\n' +
      '\n' +
      '**평가.** 우리의 평가는 [26]에 의해 확립된 메트릭을 사용하고, 나중에 [48, 73, 95]에서 이 작업에 대해 정제된다. 보다 구체적으로, 모션 시퀀스는 32쌍의 텍스트 설명 및 이들의 지속시간의 조성물로서 합성된다. 32개의 서브시퀀스와 \\(S_{i-1}\\) 쌍과 \\(S_{i}\\) 쌍 사이의 31개의 전이를 독립적으로 평가한다. 특히, 각 변환은 연속적인 포즈 집합 \\(\\{x_{\\tau_{i}-L_{tr}/2},\\ldots,x_{\\tau_{i}+L_{tr}/2-1}\\}), \\(\\frac{L_{tr}}{2}\\) 프레임과 \\(S_{i-1}\\) 및 \\(S_{i}\\)으로 정의된다. 전이 지속 시간\\(L_{tr}\\)은 바벨과 휴먼ML3D(1초와 3초)에 대해 각각 30 프레임과 60 프레임으로 설정된다. 상위-3 R-정밀도(R-prec) 및 멀티모달 거리(MM-Dist)는 서브시퀀스의 모션이 그들의 텍스트 설명과 얼마나 잘 일치하는지 평가하는 데 사용된다[26]. 모든 모션 임베딩들(다양성) 사이의 FID 점수 및 평균 쌍별 거리는 서브시퀀스들 및 전이들 모두의 품질 및 다양성을 각각 평가한다[26, 31]. 모든 메트릭은 95% 신뢰 구간이 보고된 10개의 런에 걸쳐 평균됩니다.\n' +
      '\n' +
      '**간격 닫기: Jerk.** 생성 모델은 [19, 71, 94]를 평가하기 어렵다. FID 점수[31]는 모션 아티팩트 또는 노이즈에 민감하면서도 생성된 모션 데이터와 실제 모션 데이터의 분포 사이의 유사성을 정량화하는 데 매우 신뢰할 수 있는 메트릭임이 입증되었다[55]. 그럼에도 불구하고, 전환 품질을 평가하기 위해 FID와 같은 지각 메트릭에만 의존하는 것은 갑작스러운 가속도[8] 또는 풋 스케이팅[56]과 같은 움직임 이상에 대한 둔감함으로 인해 오판의 소지가 있다. FID를 보완하기 위해, 본 연구는 운동 평활성을 나타내며 운동 불규칙성에 민감한 것으로 알려진 _jerk_(즉, 가속도의 시간 미분) 개념을 기반으로 하는 두 가지 새로운 메트릭을 소개한다[5, 6, 16, 25, 34, 44, 98]. 자연 인간의 움직임이 비교적 일관된 가속 패턴[25, 44]으로 인해 일반적으로 제한된 저크를 나타낸다는 점을 감안할 때, 우리의 메트릭은 생성된 전환에서 이 규범으로부터의 _영구적 편차를 강조하도록 조정된다. 먼저, 모든 관절에서 전이 운동 전반에 걸쳐 발견되는 최대값을 취하여 Peak Jerk(PJ)를 계산한다. 이 측정은 극단적인 변동을 포착하지만 여러 더 넓은 저크 피크에 걸쳐 비정상적으로 부드러운 전환을 하는 모델을 선호할 수 있다. 이러한 바람직하지 않은 영향을 측정하기 위해 우리는 방법의 순간 저크와 데이터 세트의 평균 저크 값 사이의 L1-norm 차이의 합으로 계산되는 AUJ(area Under the Jerk)를 소개한다. 이 척도는 전체 전환에 걸쳐 자연스러운 인간 움직임으로부터의 누적 편차를 정량화하는 동작 평활성의 집계 지표 역할을 한다. 전환의 PJ 및 AUJ는 다음과 같이 형식적으로 정의된다:\n' +
      '\n' +
      '\\text{PJ}=\\max_{\\begin{subarray}{c}1\\leq i\\leq K\\\\leq\\tau\\leq L_{tr}\\end{subarray}|j_{i}(\\tau),\\quad\\text{AUJ}=\\sum_{\\tau=1}^{L_{tr}}\\max_{1\\leq i\\leq K}|j_{i}(\\tau)-j_{avg}|_{1}, \\tag{4}\\text{AUJ}=\\sum_{ \\tau=1}^{L_{tr}}\\max_{1\\leq i\\leq K}|j_{i}(\\tau)-j_{avg}|_{1}, \\tag{4}\\text{AUJ}=\\sum_{ \\tau=1}^{L_{tr}}\\max_{1\\leq i\\leq K}|j_{i}(\\tau)-\n' +
      '\n' +
      '여기서 \\(j_{i}(\\tau)\\)는 조인트 \\(i\\)에 대한 시간 \\(\\tau\\)에서의 저크이고, \\(K\\)은 조인트 수이며, \\(j_{avg}\\)은 데이터 세트에 걸친 평균 조인트별 최대 저크이다.\n' +
      '\n' +
      '**기준.** 텍스트로부터 순차적인 움직임을 생성할 수 있는 공개 관련 작업, 즉 자기회귀 TEACH[4], 확산 샘플링 기술 DoubleTake[73], DiffCollage[103], MultiDiffusion[7]과 우리의 방법을 비교한다. 샘플링 기술은 더 공정한 비교를 위해 PCCAT 및 APE로 평가된다. 추가적으로, 구면 선형 보간법을 이용한 TEACH(TEACH_B)와 DoubleTake with를 이용한 TEACH를 평가한다.\n' +
      '\n' +
      '도 3: **Pose-centric cross-attention.** 우리의 주의는 전자만을 쿼리에 공급함으로써 제어 신호(예를 들어, 텍스트, 객체)와 노이즈 모션 사이의 얽힘을 최소화한다. 결과적으로, 우리의 모델은 각 프레임의 노이즈 포즈를 자체 상태와 이웃 노이즈 포즈만을 이용하여 잡음 제거한다.\n' +
      '\n' +
      'MDM, 원래 제안된 바와 같다(DoubleTake*). TEACH 및 TEACH_B는 연속적인 액션들의 쌍들 및 텍스트 설명들의 부족으로 인해 HumanML3D에 대해 트레이닝될 수 없다.\n' +
      '\n' +
      '**구현 상세.** 그리드 검색으로 모든 모델의 하이퍼파라미터를 조정합니다. RPE의 주의 지평선인 \\(H\\)은 바벨/HumanML3D의 경우 100/150으로 설정되었다. 확산 단계의 수는 모든 실험에 대해 1K이다. 제안된 모델은 \\(x_{0}\\) 파라미터화[90]로 학습되며, L2 복원 손실을 최소화한다. 학습 중, RPE와 APE는 0.5의 빈도로 랜덤하게 교체되며, 가중치 1.5/2.5[33]를 갖는 분류기 없는 안내를 사용한다. 우리는 BPE 샘플링을 유도하기 위해 이진 단계 함수를 사용하여 125/60 초기 APE 단계를 산출한다. 트레이닝 서브시퀀스에 대한 최소/최대 길이는 30/200 및 70/200 프레임(즉, 1/6.7s 및 3.5/10s)으로 설정된다. 바벨의 경우, 훈련 서브시퀀스는 움직임의 가변성을 높이기 위해 텍스트 설명이 뚜렷한 연속적인 지상진실 움직임을 포함하고, 네트워크가 여러 조건에 명시적으로 강건하도록 한다. 절제 연구에는 1) 각 프레임의 상태와 노이즈 포즈를 연결하고 PCCAT를 바닐라 자기 주의(SAT)로 교체하고 2) 조건을 교차 주의 레이어(CAT)로 주입하는 두 가지 조절 기준이 포함된다. 자세한 내용은 저녁에 확인하세요. 물질 A Sec.\n' +
      '\n' +
      '### Quantitative analysis\n' +
      '\n' +
      '**HMC에 대한 최첨단과의 비교.** 표 1 및 표 2는 각각 바벨 및 HumanML3D 데이터 세트의 현재 최첨단 모델과 FlowMDM의 비교를 보여준다. HumanML3D에서 본 모델은 서브시퀀스 정확도 메트릭(R-prec, MM-Dist)과 FID 측면에서 다른 방법들보다 우수한 성능을 보였다. 바벨에서는 정확도에서 최신 기술과 일치하고 FID 점수에서 탁월합니다. FlowMDM은 FID, PJ 및 AUJ 메트릭으로 표시된 대로 두 데이터 세트에서 더 높은 품질과 부드러움의 전환을 생성한다. FID 점수와 AUJ 사이의 상관 관계가 없다는 것은 부드러움을 평가하기 위한 보완 척도로서 후자의 중요성을 강조한다. 도. 4-left는 생성된 전이에 걸친 평균 저크 값을 나타낸다. 우리는 그 상태를 관찰한다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{Subsequence} & \\multicolumn{4}{c}{Transition} \\\\  & R-prec \\(\\uparrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & MM-Dist \\(\\downarrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & PJ \\(\\rightarrow\\) & AUJ \\(\\downarrow\\) \\\\ \\hline GT & \\(0.715^{\\pm 0.003}\\) & \\(0.00\\pm 0.00\\) & \\(8.42\\pm 0.15\\) & \\(3.36\\pm 0.00\\) & \\(0.00\\pm 0.00\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02\\pm 0.00\\) & \\(0.00\\pm 0.00\\) \\\\ \\hline TEACH,B & \\(\\textbf{0.703}^{\\pm 0.002}\\) & \\(1.71\\pm 0.03\\) & \\(8.18\\pm 0.14\\) & \\(\\textbf{3.43}^{\\pm 0.01}\\) & \\(3.01\\pm 0.04\\) & \\(\\textbf{6.23}^{\\pm 0.05}\\) & \\(1.09\\pm 0.00\\) & \\(2.35^{\\pm 0.01}\\) \\\\ TEACH & \\(0.655^{\\pm 0.002}\\) & \\(1.82\\pm 0.02\\) & \\(7.96^{\\pm 0.11}\\) & \\(3.72^{\\pm 0.01}\\) & \\(3.27^{\\pm 0.04}\\) & \\(6.14\\pm 0.06\\) & \\(0.07^{\\pm 0.00}\\) & \\(0.44^{\\pm 0.00}\\) \\\\ DoubleTake* & \\(0.596^{\\pm 0.005}\\) & \\(3.16\\pm 0.06\\) & \\(7.53^{\\pm 0.11}\\) & \\(4.17^{\\pm 0.02}\\) & \\(3.33^{\\pm 0.06}\\) & \\(6.16^{\\pm 0.05}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.04^{\\pm 0.01}\\) \\\\ DoubleTake & \\(0.668^{\\pm 0.005}\\) & \\(1.33\\pm 0.04\\) & \\(7.98\\pm 0.12\\) & \\(3.67^{\\pm 0.03}\\) & \\(3.15^{\\pm 0.05}\\) & \\(6.14^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.64^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(0.702^{\\pm 0.005}\\) & \\(1.74\\pm 0.04\\) & \\(8.87\\pm 0.13\\) & \\(\\textbf{3.43}^{\\pm 0.02}\\) & \\(6.56^{\\pm 0.12}\\) & \\(5.72^{\\pm 0.07}\\) & \\(0.18^{\\pm 0.00}\\) & \\(0.68^{\\pm 0.00}\\) \\\\ DiffCollage & \\(0.671^{\\pm 0.003}\\) & \\(1.45^{\\pm 0.05}\\) & \\(7.93^{\\pm 0.09}\\) & \\(3.71^{\\pm 0.01}\\) & \\(4.36^{\\pm 0.09}\\) & \\(6.09^{\\pm 0.08}\\) & \\(0.19^{\\pm 0.00}\\) & \\(0.84^{\\pm 0.01}\\) \\\\ \\hline FlowMDM & \\(0.702^{\\pm 0.004}\\) & \\(\\textbf{0.99}^{\\pm 0.04}\\) & \\(\\textbf{8.36}^{\\pm 0.13}\\) & \\(3.45^{\\pm 0.02}\\) & \\(\\textbf{2.61}^{\\pm 0.06}\\) & \\(6.47^{\\pm 0.05}\\) & \\(\\textbf{0.06}^{\\pm 0.00}\\) & \\(\\textbf{0.13}^{\\pm 0.00}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: FlowMDM과 바벨의 최신 기술의 비교. 기호 \\(\\uparrow\\), \\(\\downarrow\\) 및 \\(\\rightarrow\\)은 각각 지상 진실에 가까운 값, 낮은 값 또는 더 나은 값을 나타낸다. 평가는 10회 실행되며 \\(\\pm\\)은 95% 신뢰 구간을 지정합니다.\n' +
      '\n' +
      '도 4: **전이 평활도.** 움직임 구성(좌측) 및 외삽(우측) 작업 모두에 대한 전이의 각 프레임에서 관절에 대한 평균 최대 저크. 다른 방법은 전이 정제 과정의 시작과 끝에서 심각한 평활도 인공물을 보여주지만 FlowMDM의 저크 곡선은 구성에 대한 피크가 가장 짧고 외삽에 대한 피크가 없다.\n' +
      '\n' +
      '최첨단 방법은 심각한 평활도 인공물을 나타낸다. TEACH의 구형 선형 보간 동안 저크는 빠르게 0에 가까운 값에 도달한다. 대조적으로, DiffCollage는 평균보다 높은 저크 값으로 기울어지는 반면 MultiDiffusion은 반대 경향을 나타낸다. 더블 테이크는 2단계 잡음 추정 과정으로 인해 세 개의 피크를 보여준다. 이에 비해 FlowMDM은 피크 저크 값을 성공적으로 최소화하여 서브시퀀스 간의 가장 부드러운 전환을 생성한다. 저녁을 봐 물질 Sec. 심층 분석을 위한 C.\n' +
      '\n' +
      '**인간 모션 외삽.** 단일 텍스트-투-모션에서, 생성된 모션의 지속기간은 트레이닝 세트에서 이용가능한 최장 서브시퀀스 길이 \\(L\\)로 제한된다. 기본 진리에 있는 것보다 더 긴 시퀀스로 주기적 동작을 외삽하는 것은 주목할만한 도전을 제시한다. HMC를 통해 이를 달성하는 것은 인접한 서브시퀀스에 걸친 주기성의 조화를 필요로 한다. 그러나, 독립적으로 생성된 서브시퀀스들을 결합하는 공통 전략들은 종종 모션의 주기성을 방해한다. 이 문제를 해결하는 데 있어 모델의 기능을 평가하기 위해 바벨 및 HumanML3D 테스트 세트에서 추출한 \'앞으로 걷기\', \'점프하기\', \'기타 연주하기\'와 같은 32개의 다른 외삽 가능한 동작을 32회 연속 반복하는 평가 세트를 구성한다. 도. 4-우측에는 이 작업의 모든 모델에 대한 전환에 걸쳐 모션 저크가 표시됩니다. 우리는 다른 모델이 HMC 평가에 표시된 것과 유사한 평활도 이상을 나타내는 반면 FlowMDM은 지상 진리 저크를 밀접하게 반영한다는 것을 관찰한다. 이 관찰은 구성 작업에 대해 FlowMDM에서 언급된 저크 피크가 더 복잡한 전이의 평활도 불규칙성에 기인할 가능성이 있음을 나타낸다.\n' +
      '\n' +
      '**절제 연구.** BPE 및 PCCAT의 효과는 표 3 및 표 4에 제시되어 있다. 추론적으로, APE로만 훈련된 기준선 모델은 부드러운 전환을 생성하지 못한다. 반대로, 가장 부드러운 전환을 생성함에도 불구하고, RPE로만 훈련된 모델은 전역 모션 의존성을 모델링하고 대응하는 텍스트 설명을 정확하게 반영하기 위해 고군분투한다. 흥미롭게도 BPE를 사용한 훈련은 APE 및 RPE 전용 샘플링의 성능을 향상시킨다. BPE를 사용한 샘플링은 RPE 모델의 우수한 AUJ 값을 보존하고 최첨단 정확도에 도달하여 두 세계의 최고를 결합한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c c|c c c} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Cond.}} & \\multicolumn{3}{c}{\\multicolumn{3}{c}{Subsequence}} & \\multicolumn{5}{c}{Transition} \\\\  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline GT & - & - & \\(0.796^{\\pm 0.004}\\) & \\(0.00^{\\pm 0.00}\\) & \\(9.34^{\\pm 0.08}\\) & \\(2.97^{\\pm 0.01}\\) & \\(0.00^{\\pm 0.00}\\) & \\(9.54^{\\pm 0.15}\\) & \\(0.04^{\\pm 0.00}\\) & \\(0.07^{\\pm 0.00}\\) \\\\ \\hline PCCAT & A & A & \\(0.689^{\\pm 0.005}\\) & \\(0.66^{\\pm 0.02}\\) & \\(9.73^{\\pm 0.12}\\) & \\(3.63^{\\pm 0.02}\\) & \\(3.90^{\\pm 0.12}\\) & \\(8.29^{\\pm 0.08}\\) & \\(1.50^{\\pm 0.01}\\) & \\(3.40^{\\pm 0.02}\\) \\\\ PCCAT & R & R & \\(0.531^{\\pm 0.005}\\) & \\(1.75^{\\pm 0.07}\\) & \\(8.71^{\\pm 0.10}\\) & \\(4.80^{\\pm 0.03}\\) & \\(2.53^{\\pm 0.12}\\) & \\(8.62^{\\pm 0.08}\\) & \\(\\textbf{0.03}^{\\pm 0.00}\\) & \\(0.58^{\\pm 0.01}\\) \\\\ \\hline PCCAT & B & A & \\(\\textbf{0.699}^{\\pm 0.005}\\) & \\(0.61^{\\pm 0.02}\\) & \\(9.76^{\\pm 0.10}\\) & \\(3.54^{\\pm 0.02}\\) & \\(2.42^{\\pm 0.09}\\) & \\(8.39^{\\pm 0.09}\\) & \\(1.40^{\\pm 0.01}\\) & \\(3.29^{\\pm 0.02}\\) \\\\ PCCAT & B & R & \\(0.554^{\\pm 0.007}\\) & \\(1.06^{\\pm 0.06}\\) & \\(9.02^{\\pm 0.11}\\) & \\(4.54^{\\pm 0.02}\\) & **1.12\\({}^{\\pm 0.04}\\)** & **9.00\\({}^{\\pm 0.10}\\)** & \\(0.05^{\\pm 0.00}\\) & \\(0.53^{\\pm 0.01}\\) \\\\ \\hline SAT & B & B & \\(0.692^{\\pm 0.004}\\) & \\(0.49^{\\pm 0.02}\\) & \\(9.08^{\\pm 0.09}\\) & **3.51\\({}^{\\pm 0.01}\\)** & \\(3.19^{\\pm 0.08}\\) & \\(8.09^{\\pm 0.11}\\) & \\(0.04^{\\pm 0.00}\\) & **0.36\\({}^{\\pm 0.02}\\)** \\\\ CAT & B & B & \\(0.622^{\\pm 0.005}\\) & \\(1.27^{\\pm 0.04}\\) & \\(8.86^{\\pm 0.15}\\) & \\(4.10^{\\pm 0.01}\\) & \\(3.93^{\\pm 0.14}\\) & \\(8.23^{\\pm 0.10}\\) & \\(\\underline{0.04}^{\\pm 0.00}\\) & \\(\\underline{0.49}^{\\pm 0.02}\\) \\\\ \\hline PCCAT & B & B & \\(0.685^{\\pm 0.004}\\) & \\(\\textbf{0.29}^{\\pm 0.01}\\) & \\(\\textbf{9.58}^{\\pm 0.12}\\) & \\(3.61^{\\pm 0.01}\\) & \\(1.38^{\\pm 0.05}\\) & \\(\\underline{8.79}^{\\pm 0.09}\\) & \\(0.06^{\\pm 0.00}\\) & \\(0.51^{\\pm 0.01}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: HumanML3D에서의 절제 연구.\n' +
      '\n' +
      '도 5: **BPE 트레이드-오프.** BPE 샘플링 동안 진행된 APE 단계들의 수를 증가시키는 것은 모션과 텍스트 기술(R-prec) 사이의 대응관계를 개선하지만, 전이 사실성 및 평활성(FID 및 AUJ)을 감소시킨다. 최상의 균형은 APE 노이즈 제거 단계의 약 10%에 도달합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c|c c c|c c c} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Cond.}} & \\multicolumn{3}{c}{\\multicolumn{3}{c}{Subsequence}} & \\multicolumn{5}{c}{Transition} \\\\  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline GT & - & - & \\(0.715^{\\pm 0.003}\\) & \\(0.00^{\\pm 0.00}\\) & \\(8.42^{\\pm 0.15}\\) & \\(3.36^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) \\\\ \\hline PCCAT & A & A & \\(0.699^{\\pm 0.004}\\) & \\(1.34^{\\pm 0.04}\\) & \\(\\textbf{8.36}^{\\pm 0.12}\\) & \\(3.40^{\\pm 0.02}\\) & \\(4.\n' +
      '\n' +
      'APE 모델의 점수입니다. 도. 도 5는 이러한 밸런스를 도시한 것이다. 특히, APE 단계들의 수를 증가시키는 것은 트랜지션들의 평활성 및 사실성을 감소시키는 비용으로, 텍스트 기술과의 모션의 일치를 향상시킨다. HumanML3D에서 SAT 및 CAT 컨디셔닝 방식은 FID 및 다양성 측면에서 더 나쁜 전환을 초래한다. 이는 훈련 중 절대 일어나지 않는 추론 시 전환의 지역 동네에 서로 다른 조건이 공존하는 데서 비롯된다. 우리의 PCCAT 컨디셔닝 기술은 이 문제를 효과적으로 해결한다. 바벨에서, 트레이닝 모션 시퀀스들이 여러 서브시퀀스들을 포함하기 때문에 그러한 효과는 존재하지 않으며, 따라서 다양한 조건들에 따른 천이에 대한 모델의 견고성을 증가시킨다.\n' +
      '\n' +
      '**FlowMDM.**의 효율성에 대해, MultiDiffusion 및 DiffCollage denoise 포즈와 같은 Diffusion 기반 최첨단 방법은 인접한 모션과 조화를 이루기 위해 천이로부터 두 번 이상 포즈한다. 더블 테이크의 전환은 추가적인 잡음 제거 과정을 거치게 되는데, 이는 계산 부담을 가중시키고 병렬화할 수 없다. 반대로, FlowMDM은 임의의 포즈에 중복 잡음 제거 단계를 적용하지 않는다. 특히, 본 모델은 DoubleTake, DiffCollage, MultiDiffusion에 비해 각각 47.1%, 28.4%, 16.5% 적은 포즈별 노이즈 제거 단계를 거친다.\n' +
      '\n' +
      '### Qualitative results\n' +
      '\n' +
      '도. 도 6은 우리의 정량적 발견이 인간의 움직임 구성 및 외삽 작업에 대한 시각적 결과로 어떻게 변환되는지 보여준다. 먼저, Fig.에 의해 예상된 바와 같다. 도 4를 참조하면, 최첨단 방법은 전이 주변에서 짧은 간격의 저크 피크를 생성한다는 것을 확인할 수 있다. 이러한 것들은 일반적으로 이러한 얼간이들이 문맥적으로 적절할 수 있는 장거리 모션 시나리오와 일치하지 않는다. 반대로 FlowMDM은 사실적이고 정확하며 부드러운 움직임을 생성합니다. 특히, 전이 주변에서 지속적으로 높은 저크 값을 생성하는 데 대한 DiffCollage의 편향이 전반적인 혼돈 운동으로 인식된다는 것을 알 수 있다. 서브시퀀스의 독립적인 생성으로 인해 DoubleTake, DiffCollage 및 MultiDiffusion은 액션을 외삽할 때 액션의 정적 또는 주기적 특성을 유지할 수 없다. TEACH와 FlowMDM만이 정적 \'t-포즈\'를 성공적으로 외삽할 수 있으며, 우리의 것은 a\'tep to the right\' 시퀀스를 현실적으로 외삽할 수 있는 유일한 것이다. 마지막으로 FlowMDM은 그림과 같이 모션 확산 모델의 궤적 제어 능력도 계승한다. 1-right.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 후처리 또는 중복 잡음 제거 확산 단계를 거치지 않고 동시에 인간의 움직임 구성을 생성하는 첫 번째 접근 방법인 FlowMDM을 제시했다. 또한 잡음 제거 체인 동안 절대 위치 인코딩과 상대 위치 인코딩의 이점을 결합하기 위해 혼합 위치 인코딩을 도입했다. 마지막으로, 모션 시퀀스 당 단일 조건만으로 훈련할 때 전이 생성을 개선하는 기법인 포즈 중심 교차 주의를 제시하였다.\n' +
      '\n' +
      '**제한사항 및 향후 작업.** BPE의 절대 단계는 서브시퀀스 간의 관계를 모델링하지 않는다.\n' +
      '\n' +
      '도 6: **정성적 분석(Babel.**A) 및 B)은 3개의 동작(\'직진\'\\(\\,\\우측 행\\,\\) \'후진\' 및 \'보행\'\\(\\,\\우측 행\\,\\) \'회전\'\\(\\,\\우측 행\\,\\) \'벤치에 앉는다)의 구성을 나타내고, C) 및 D)는 각각 정적(t-포즈) 및 동적(오른쪽으로 가는 단계) 동작을 6회 반복하는 외삽을 나타낸다. 실선 곡선은 전역 위치(파란색) 및 왼쪽/오른쪽 손(보라색/녹색)의 궤적과 일치합니다. 더 어두운 색상은 데이터 세트(검은색 세그먼트)에서 저크의 표준 편차의 두 배로 포화되는 중앙값에서 순간 저크 편차를 나타낸다. 부드러운 전환은 더 가벼운 것 가운데 검은색 부분으로 나타난다. FlowMDM은 자발적인 높은 저크 값을 보여주고 외삽에서 운동 일관성을 유지하지 못하는 다른 방법과 달리 가장 유동적인 운동을 나타내며 외삽된 동작의 정적 또는 주기성을 보존한다.\n' +
      '\n' +
      '결과적으로, 이들의 저주파 스펙트럼은 독립적으로 생성된다. 이러한 한계는 의도 계획 모듈을 통합하여 향후 작업에서 해결할 수 있다. 마지막으로, 본 논문에서 제안하는 방법은 훈련 시간에 보이지 않는 동작들의 조합들 사이의 전이를 생성하는 강한 동작을 학습한다. 이러한 능력은 이론적으로 서로 다른 제어 신호를 활용하는 서로 다른 모델과 함께 사용될 수 있으며, 이는 모두 동일한 프레임워크에서 훈련된다고 가정한다. 향후 작업은 이 가설을 실험적으로 검증할 것이다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Carlos Niebles, Silvio Savarese, Ehsan Adeli, and Hamid Rezatofighi. Tripod: Human trajectory and pose dynamics forecasting in the wild. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13390-13400, 2021.\n' +
      '* [2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-20, 2023.\n' +
      '* [3] Sadegh Aliakbarian, Microsoft Fatemeh Saleh ACRV, Stephen Gould ACRV, and Anu Mathieu Salzmann CVLab. Contextually plausible and diverse 3d human motion prediction. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [4] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and Gul Varol. Teach: Temporal action composition for 3d humans. In _2022 International Conference on 3D Vision (3DV)_, pages 414-423. IEEE, 2022.\n' +
      '* [5] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, and Etienne Burdet. A robust and sensitive metric for quantifying movement smoothness. _IEEE transactions on biomedical engineering_, 59(8):2126-2136, 2011.\n' +
      '* [6] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, Agnes Roby-Brami, and Etienne Burdet. On the analysis of movement smoothness. _Journal of neuroengineering and rehabilitation_, 12(1):1-11, 2015.\n' +
      '* [7] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. _arXiv preprint arXiv:2302.08113_, 2023.\n' +
      '* [8] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for behavior-driven human motion prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2317-2327, 2023.\n' +
      '* [9] German Barquero, Johnny Nunez, Sergio Escalera, Zhen Xu, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero. Didn\'t see that coming: a survey on non-verbal social human behavior forecasting. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 139-178. PMLR, 2022.\n' +
      '* [10] German Barquero, Johnny Nunez, Zhen Xu, Sergio Escalera, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero. Comparison of spatio-temporal models for human motion and pose forecasting in face-to-face interaction scenarios. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 107-138. PMLR, 2022.\n' +
      '* [11] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* [12] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. Behave: Dataset and method for tracking human object interactions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15935-15946, 2022.\n' +
      '* [13] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14_, pages 561-578. Springer, 2016.\n' +
      '* [14] Paulo Vinicius Koerich Borges, Nicola Conci, and Andrea Cavallaro. Video-based human behavior understanding: A survey. _IEEE transactions on circuits and systems for video technology_, 23(11):1993-2008, 2013.\n' +
      '* [15] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion prediction with scene context. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16_, pages 387-404. Springer, 2020.\n' +
      '* [16] Angela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbelaez, Ali Thabet, and Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion synthesis. _arXiv preprint arXiv:2304.11118_, 2023.\n' +
      '* [17] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster: choreography-oriented music-driven dance synthesis. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [18] Enric Corona, Albert Pumarola, Guillem Alenya, and Francesc Moreno-Noguer. Context-aware human motion prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6992-7001, 2020.\n' +
      '* [19] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. _IEEE signal processing magazine_, 35(1):53-65, 2018.\n' +
      '* [20] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [21] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9760-9770, 2023.\n' +
      '* [22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_, 2023.\n' +
      '**[23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 플래시어텐션: io-awareness와 함께 빠르고 메모리 효율적인 정확한 주의력. _ 신경 정보 처리 시스템_, 35:16344-16359, 2022에서의 발전.\n' +
      '* [24] David A Engstrom, JA Scott Kelso, and Tom Holroyd. Reaction-anticipation transitions in human perception-action patterns. _Human movement science_, 15(6):809-832, 1996.\n' +
      '* [25] Philipp Gulde and Joachim Hermsdorfer. Smoothness metrics in complex movement tasks. _Frontiers in neurology_, 9:615, 2018.\n' +
      '* [26] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5152-5161, 2022.\n' +
      '* [27] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In _European Conference on Computer Vision_, pages 580-597. Springer, 2022.\n' +
      '* [28] Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Multi-person extreme motion prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13053-13064, 2022.\n' +
      '* [29] Felix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. _ACM Transactions on Graphics (TOG)_, 39(4):60-1, 2020.\n' +
      '* [30] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11374-11384, 2021.\n' +
      '* [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [33] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [34] Neville Hogan and Dagmar Sternad. Sensitivity of smoothness measures to movement duration, amplitude, and arrests. _Journal of motor behavior_, 41(6):529-534, 2009.\n' +
      '* [35] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. _arXiv preprint arXiv:2306.14795_, 2023.\n' +
      '* [36] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler, and Otmar Hilliges. Convolutional autoencoders for human motion infilling. In _2020 International Conference on 3D Vision (3DV)_, pages 918-927. IEEE, 2020.\n' +
      '* [37] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. _arXiv preprint arXiv:2305.19466_, 2023.\n' +
      '* [38] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.\n' +
      '* [39] Jihoon Kim, Taehyun Byun, Seungyoun Shin, Jungdarn Won, and Sungjoon Choi. Conditional motion in-betweening. _Pattern Recognition_, 132:108894, 2022.\n' +
      '* [40] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis & editing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 8255-8263, 2023.\n' +
      '* [41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [42] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty: Neural object interaction fields for guided human motion synthesis. _arXiv preprint arXiv:2307.07511_, 2023.\n' +
      '* [43] Wilfried Kunde, Katrin Elsner, and Andrea Kiesel. No anticipation-no action: the role of anticipation in action and perception. _Cognitive Processing_, 8:71-78, 2007.\n' +
      '* [44] Caroline Larboulete and Sylvie Gibet. A review of computable expressive descriptors of human motion. In _Proceedings of the 2nd International Workshop on Movement and Computing_, pages 21-28, 2015.\n' +
      '* [45] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Multiact: Long-term 3d human motion generation from multiple action labels. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 1231-1239, 2023.\n' +
      '* [46] Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang, Hao Li, and Yajie Zhao. Task-generic hierarchical human motion prior using vaes. In _2021 International Conference on 3D Vision (3DV)_, pages 771-781. IEEE, 2021.\n' +
      '* [47] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13401-13412, 2021.\n' +
      '* [48] Shuai Li, Sisi Zhuang, Wenfeng Song, Xinyu Zhang, Hejia Chen, and Aimin Hao. Sequential texts driven cohesive motions synthesis with natural transitions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9498-9508, 2023.\n' +
      '* [49] Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen. Example-based motion synthesis via generative motion matching. _arXiv preprint arXiv:2306.00378_, 2023.\n' +
      '* [50] Yunhao Li, Zhenbo Yu, Yucheng Zhu, Bingbing Ni, Guangdao Zhai, and Wei Shen. Skeleton2humanoid: Animating simulated characters for physically-plausible motion in-betweening. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 1493-1502, 2022.\n' +
      '* [51] Jing Lin, Ailing Zeng, Shuulin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: A large-scale 3d expressive whole-body human motion dataset. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.\n' +
      '* [52] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10895-10904, 2023.\n' +
      '* [53] Hengbo Ma, Jiachen Li, Rantin Hosseini, Masayoshi Tomizuka, and Chiho Choi. Multi-objective diverse human motion prediction with knowledge distillation. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* [54] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel codec avatars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 64-73, 2021.\n' +
      '* [55] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit. Evaluating the quality of a synthesized motion with the frechet motion distance. In _ACM SIGGRAPH 2022 Posters_, pages 1-2, 2022.\n' +
      '* [56] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit. Validating objective evaluation metric: Is frechet motion distance able to capture foot skating artifacts? In _Proceedings of the 2023 ACM International Conference on Interactive Media Experiences_, pages 242-247, 2023.\n' +
      '* [57] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Generating smooth pose sequences for diverse human motion prediction. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [58] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.\n' +
      '* [59] Boris N Oreshkin, Antonios Valkanas, Felix G Harvey, Louis-Simon Menard, Florent Bocquelet, and Mark J Coates. Motion in-betweening via deep delta-interpolator. _IEEE Transactions on Visualization and Computer Graphics_, 2023.\n' +
      '* [60] Cristina Palmero, German Barquero, Julio CS Jacques Junior, Albert Clapes, Johnny Nunez, David Curto, Sorina Smeureanu, Javier Selva, Zejian Zhang, David Saeteros, et al. Chalearn lap challenges on self-reported personality recognition and non-verbal behavior forecasting during social dyadic interactions: Dataset, design, and results. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 4-52. PMLR, 2022.\n' +
      '* [61] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hide attention. _Advances in Neural Information Processing Systems_, 35:14541-14554, 2022.\n' +
      '* [62] Sang-Min Park and Young-Gab Kim. A metaverse: Taxonomy, components, applications, and open challenges. _IEEE access_, 10:4209-4251, 2022.\n' +
      '* [63] Mathis Petrovich, Michael J Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In _European Conference on Computer Vision_, pages 480-497. Springer, 2022.\n' +
      '* [64] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. _Big data_, 4(4):236-252, 2016.\n' +
      '* [65] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J Black. Babel: Bodies, action and behavior with english labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 722-731, 2021.\n' +
      '* [66] Yijun Qian, Jack Urbanek, Alexander G Hauptmann, and Jungdam Won. Breaking the limits of text-conditioned 3d motion synthesis with elaborative descriptions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2306-2316, 2023.\n' +
      '* [67] Jia Qin, Youyi Zheng, and Kun Zhou. Motion in-betweening via two-stage transformers. _ACM Transactions on Graphics (TOG)_, 41(6):1-16, 2022.\n' +
      '* [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [69] Tianxiang Ren, Jubo Yu, Shihui Guo, Ying Ma, Yutao Ouyang, Zijiao Zeng, Yazhan Zhang, and Yipeng Qin. Diverse motion in-betweening with dual posture stitching. _arXiv preprint arXiv:2303.14457_, 2023.\n' +
      '* [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [71] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. _Advances in neural information processing systems_, 31, 2018.\n' +
      '* [72] Tim Salzmann, Marco Pavone, and Markus Ryll. Motron: Multimodal probabilistic human motion forecasting. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* [73] Yonatan Shafir, Guy Tveet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. _arXiv preprint arXiv:2303.01418_, 2023.\n' +
      '* [74] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [75] Paul Starke, Sebastian Starke, Taku Komura, and Frank Steinicke. Motion in-betweening with phase manifolds. _Proceedings of the ACM on Computer Graphics and Interactive Techniques_, 6(3):1-17, 2023.\n' +
      '* [76] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* [77] Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan S Kankankanhalli, Weidong Geng, and Xiangdong Li. Deep-dance: music-to-dance motion choreography with adversarial learning. _IEEE Transactions on Multimedia_, 23:497-509, 2020.\n' +
      '* [78] Jiarui Sun and Girish Chowdhary. Towards globally consistent stochastic human motion prediction via motion diffusion. _arXiv preprint arXiv:2305.12554_, 2023.\n' +
      '* [79] Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati, and Nicolai Marquardt. Augmented reality and robotics: A survey and taxonomy for ar-enhanced human-robot interaction and robotic interfaces. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-33, 2022.\n' +
      '* [80] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng Tang, Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall, and Cem Keskin. Social diffusion: Long-term multiple human motion anticipation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9601-9611, 2023.\n' +
      '* [81] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In _European Conference on Computer Vision_, pages 358-374. Springer, 2022.\n' +
      '* [82] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [83] Sibo Tian, Minghui Zheng, and Xiao Liang. Transfusion: A practical and effective transformer-based diffusion model for 3d human motion prediction. _arXiv preprint arXiv:2307.16106_, 2023.\n' +
      '* [84] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 448-458, 2023.\n' +
      '* [85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [86] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video forecasting by generating pose futures. _Proceedings of the IEEE international conference on computer vision_, 2017.\n' +
      '* [87] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20460-20469, 2022.\n' +
      '* [88] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9401-9411, 2021.\n' +
      '* [89] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-aware generative network for human motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12206-12215, 2021.\n' +
      '* [90] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [91] Jiachen Xu, Min Wang, Jingyu Gong, Wentao Liu, Chen Qian, Yuan Xie, and Lizhuang Ma. Exploring versatile prior for human motion via motion frequency guidance. In _2021 International Conference on 3D Vision (3DV)_, pages 606-616. IEEE, 2021.\n' +
      '* [92] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14928-14940, 2023.\n' +
      '* [93] Sirui Xu, Yu-Xiong Wang, and Liangyan Gui. Stochastic multi-person 3d motion forecasting. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [94] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Computing Surveys_, 2022.\n' +
      '* [95] Zhao Yang, Bing Su, and Ji-Rong Wen. Synthesizing long-term human motions with diffusion models via coherent sampling. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 3954-3964, 2023.\n' +
      '* [96] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang. Choreonet: Towards music to dance synthesis with choreographic action unit. In _Proceedings of the 28th ACM International Conference on Multimedia_, pages 744-752, 2020.\n' +
      '* [97] Hongwei Yi, Chun-Hao P Huang, Shashank Tripathi, Lea Hering, Justus Thies, and Michael J Black. Mime: Human-aware 3d scene generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12965-12976, 2023.\n' +
      '* [98] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [99] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pages 346-364. Springer, 2020.\n' +
      '* [100] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16010-16021, 2023.\n' +
      '* [101] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14730-14740, 2023.\n' +
      '* [102] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. _arXiv preprint arXiv:2208.15001_, 2022.\n' +
      '* [103] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. DiffCollage: Parallel generation of large content with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10188-10198, 2023.\n' +
      '* [104] Yan Zhang, Michael J Black, and Siyu Tang. Perpetual motion: Generating unbounded human motion. _arXiv preprint _arXiv:2007.13886_, 2020.\n' +
      '* [105] Yan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20481-20491, 2022.\n' +
      '* [106] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5745-5753, 2019.\n' +
      '* [107] Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. Auto-conditioned recurrent networks for extended complex human motion synthesis. In _International Conference on Learning Representations_, 2018.\n' +
      '* [108] Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, et al. Generative tweening: Long-term inbetweening of 3d human motions. _arXiv preprint arXiv:2005.08891_, 2020.\n' +
      '* [109] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. _arXiv preprint arXiv:2307.10894_, 2023.\n' +
      '* [110] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet for music-driven dance generation. _ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)_, 18(2):1-21, 2022.\n' +
      '\n' +
      '## Supplementary Material\n' +
      '\n' +
      '### 추가 구현 세부사항\n' +
      '\n' +
      '모든 값은 바벨/HumanML3D의 경우 X/Y로 보고되거나 두 값에 대해 동일한 경우 Z로 보고된다. 모션 시퀀스는 30/20 fps로 다운샘플링된다는 점에 유의한다.\n' +
      '\n' +
      '**최신 모델.** TEACH는 원래 제안된 정렬 및 구형 선형 보간과 함께, 그리고 그들 없이 (TEACH_B) 기성품 1에서 사용된다. DoubleTake는 원래 저장소에서 Off-the-shelf 2에 사용되며, 매개변수 _handshake size_ 및 _blending length_는 각각 10/20f(프레임) 및 10/5f로 설정된다. 이들의 방법의 제약조건을 만족시키기 위해, 악수 크기는 우리가 생성하고자 하는 가장 짧은 시퀀스의 절반보다 짧아야 하며, 이는 바벨의 경우 30f(1s)이다. DoubleTake는 트레이닝이 매우 짧은 시퀀스를 버린 원래의 모션 확산 모델[82]을 사용하기 때문에, 보다 포괄적인 평가 프로토콜(Sec. B 참조)에서 성능이 떨어진다. 더 공정한 비교를 위해 우리는 또한 절대 위치 인코딩(APE)이 있는 확산 모델을 사용하여 평가하고 DoubleTake*라고 한다. 더블 테이크*는 더블 테이크와 동일한 핸드셰이크 크기 및 블렌딩 길이를 사용한다. DiffCollage와 MultiDiffusion은 수동으로 구현되었으며 앞서 언급한 것과 동일한 이유로 우리의 모델도 활용했다. 샘플링 매개변수 _전이 길이_를 10/20f로 설정했다. DoubleTake, DiffCollage 및 MultiDiffusion의 경우 샘플링 동안 가중치 1.5/2.5인 분류기 없는 안내를 사용한다.\n' +
      '\n' +
      '각주 1: [https://github.com/athn-nik/teach/commit/f4285affOfd556a5b46518a751fc90825d91e68b](https://github.com/athn-nik/teach/commit/f4285affOfd556a5b46518a751fc90825d91e68b)\n' +
      '\n' +
      '**FlowMDM.** 우리의 확산 모델은 1k 단계와 코사인 잡음 스케줄을 사용한다[58]. FlowMDM은 \\(x_{0}\\) 파라미터화[90]와 L2 복원 손실로 학습된다. 노이즈 타임스텝은 512D 벡터로 두 개의 조밀한 레이어를 통과하는 정현파 위치 인코딩으로 인코딩된다. 텍스트 설명은 토큰화되고 CLIP[68]과 함께 512D 벡터로 임베딩된다. 135/263D의 포즈는 조밀한 층에 의해 512D 벡터의 시퀀스로 인코딩된다. APE가 활성인 경우, 이 단계에서 임베딩된 포즈에 정현파 인코딩이 추가된다. 그런 다음, 내장된 포즈를 트랜스포머의 _key_ 및 _values_로 취한다. 임베디드 포즈는 타임스텝과 텍스트 임베딩의 합에 연결되고, 조밀한 레이어에 공급된다. 결과 512D 벡터는 _queries_이다. 상대적 위치 인코딩(RPE)이 활성인 경우, 회전 임베딩들[76]은 이 단계에서 질의들 및 키들에 주입된다. 트랜스포머의 출력은 잔여 연결과 함께 내장된 포즈에 추가됩니다. 8개의 트랜스포머가 함께 쌓여 있습니다. 최종 조밀한 계층은 포즈 임베딩들을 다시 잡음 제거된 포즈들인 135/263D의 벡터로 변환한다. APE와 트랜스포머의 입력에 0.1의 드롭아웃이 적용됩니다. 트랜스포머의 주의 범위는 APE 단계에서 각 서브시퀀스 내에서, 그리고 RPE 단계에서 주의 범위 H=100/150f 내에서 캡핑된다. 혼합 위치 부호화(BPE, Blended Position Encodings)를 이용하여 RPE와 APE를 0.5의 빈도로 랜덤하게 교대로 훈련하고, 학습률 0.0001인 Adam[41]을 최적기로 사용하며, RTX 3090(약 4/2일)에서 1.3M/500k 단계로 훈련한다. BPE 샘플링 동안, 이진 단계 스케줄은 125/60 디노이징 단계(1k 단계 중) 후에 절대 모드에서 상대 모드로 전환된다. 샘플링 중에 가중치 1.5/2.5가 포함된 분류기 없는 지침이 사용된다.\n' +
      '\n' +
      '####) B 평가 세부사항\n' +
      '\n' +
      '생성 모델은 메트릭의 한계(Sec. 4.1에서 논의됨)와 샘플링 중에 존재하는 확률성으로 인해 평가 및 비교가 어렵다. 후자를 완화하기 위해 모든 평가를 10회 실행하고 95% 신뢰 구간을 제공한다. 그러나 우리는 여전히 과제에서 또 다른 문제에 직면해 있다: 텍스트 설명 조합의 무작위성. \'it down\'\\(\\rightarrow\\)\'stand up\'\\(\\rightarrow\\)\'run\' 조합의 세대 난이도는 \'it down\'\\(\\rightarrow\\\'run\'\\(\\rightarrow\\\'stand up\'과 같지 않다. [73]으로부터의 평가 프로토콜은 테스트 세트로부터 무작위로 샘플링된 32개의 텍스트 설명들의 32개의 평가 시퀀스들을 포함한다. 생성된 동작은 각각의 평가 시퀀스로부터 32개의 동작을 순차적으로 수행할 필요가 있다. 그러나 이러한 설명은 각 평가 실행에서 다르게 샘플링되어 재현성을 방해한다. 향후 작업에서 적절한 복제 및 공정한 비교를 보장하기 위해 _scenarios_(Sec. C.1에 제공된 분석)를 기반으로 보다 세밀한 분석을 가능하게 하는 보다 철저하고 완전히 재현 가능한 평가 프로토콜을 제안한다.\n' +
      '\n' +
      '**바벨.** 배포 내(50%) 및 배포 외(50%) 조합으로 두 가지 시나리오를 구축했습니다. 배포 내 시나리오의 경우, 먼저 총 지속 시간이 1.5s인 최소 3개의 연속적인 동작(즉, 텍스트 설명)을 보여주는 테스트 모션 시퀀스를 선택했다. 그런 다음 텍스트 설명의 32개 조합으로 구성된 32개의 세트를 구축하기 위해 무작위로 샘플링했다. 배포 외 시나리오의 경우 32개의 텍스트 설명을 자동으로 샘플링하여 32개의 세트를 구축하여 훈련이나 테스트 세트에서 연속 작업이 함께 나타나지 않도록 했다.\n' +
      '\n' +
      '**HumanML3D.** HumanML3D의 주석에는 연속적인 액션이 포함되어 있지 않기 때문에, 배포 내 및 배포 외 시나리오를 구축할 수 없다. 그러나 이 데이터 세트에는 서열 길이(3-10s)의 큰 가변성이 포함되어 있다. 따라서, 우리는 포함된 서브시퀀스의 길이를 변경하여 네 가지 시나리오를 구축하기로 결정했다. 보다 구체적으로, 우리는 각각 32개의 짧은(3-5초), 중간(5-8초), 긴(8-10초) 테스트 동작을 샘플링하여 6, 8, 18개 조합(9.4, 12.5, 28.1%)의 세 세트를 생성했다. 비율은 원래 테스트 세트에서 모두 짧은, 중간 및 긴 서브시퀀스의 비율을 보존하도록 설정되었다. 이는 FID와 같은 통계적 측정의 타당성을 유지하는 데 중요하다. 또한 테스트 세트에서 32개의 무작위 모션 시퀀스 중 32개 세트(50%)가 있는 또 다른 시나리오를 포함했다.\n' +
      '\n' +
      '공용 코드 저장소 3에서 인간의 움직임 구성과 외삽 작업에 대한 평가 조합 목록을 공유한다. 조합은 텍스트 설명 목록과 관련 기간으로 구성된다는 점에 유의한다. Sec의 외삽 실험에 사용된 32개의 텍스트 설명이다. 4는 탭 A에 열거되어 있다.\n' +
      '\n' +
      '각주 3: [https://barquerogerman.github.io/FlowMDM/](https://barquerogerman.github.io/FlowMDM/)\n' +
      '\n' +
      '## 부록 C 더 실험 결과\n' +
      '\n' +
      '### Fine-grained comparison\n' +
      '\n' +
      '탭 B는 유통 내 및 유통 외 시나리오 모두에서 FlowMDM과 기술의 상태를 비교한 것을 보여준다. 우리는 모든 방법이 서브시퀀스 생성에 대해 두 시나리오 모두에서 유사한 성능을 유지하지만, 아웃-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브-오브- FlowMDM은 전환 평활성에 관한 이전 기술과 관련하여 중요한 갭을 갖는, 두 시나리오 모두에서 대부분의 메트릭에서 최상의 성능을 수행한다. 탭 C는 HumanML3D에 대한 시나리오별 결과를 보여주며, 여기서 FlowMDM은 또한 대부분의 메트릭 및 시나리오에서 최고의 성능을 수행한다. 흥미롭게도 MultiDiffusion은 시나리오(PJ 및 AUJ)에 걸친 전이 평활도 측면에서 가장 안정적인 방법인 반면, DiffCollage와 DoubleTake는 긴 시퀀스의 조합에서 심각한 전이 변성을 나타낸다. 이러한 변성은 대부분 샘플링 동안 모션 시퀀스를 패딩해야 하는 방법론적 필요성 때문이다. 긴 시퀀스를 다룰 때, 시퀀스는 트레이닝 시간에서 최대 시퀀스 길이 이상으로 확장될 수 있다. 따라서 APE가 잘 외삽하지 않는다는 점을 감안할 때 패딩된 운동에서의 생성 또는 전이는 퇴화하는 경향이 있다. 우리의 방법은 자연스럽게 이러한 제한을 피한다.\n' +
      '\n' +
      '주의 지평선 위에\n' +
      '\n' +
      '탭스에서요 D와 E, 우리는 순전히 상대적인 추론 스케줄 또는 제안된 BPE 추론 스케줄에 대해 RPE를 사용할 때 주의 지평의 효과를 보여준다. 이를 위해 데이터 셋(FID, AUJ)과 HumanML3D(R-prec, MM-Dist)에 대한 서브시퀀스 생성에서 전이생성(transition generation)을 수행하는데 있어서 H=200이 너무 많아질수록 네트워크의 성능이 저하되는 것을 관찰하였다. 반대로, 그것을 너무 많이 감소시킬 때(H=50), 장거리 다이내믹을 모델링하는 용량은 제한되고, 따라서 생성된 서브시퀀스들(R-prec 및 MM-Dist)의 정확도를 감소시킨다. H가 100과 150인 성능은 두 데이터 세트에서 유사하기 때문에 Ba-bel/HumanML3D에 대해 각 데이터 세트의 평균 서열 길이에 가장 가까운 값, 즉 100/150f를 선택했다.\n' +
      '\n' +
      '확산 스케줄에 있어\n' +
      '\n' +
      'Sec. 3.2의 논의와 BPE 설계는 확산 모델의 잡음 제거 단계에서 저주파에서 고주파로의 분해에 의해 동기화된다. 그러나 잡음 제거 과정은 잡음이 주입되는 방식 또는 _noise schedule_에 따라 달라진다. 선형 및 코사인(우리의 선택) 노이즈 스케줄이 가장 일반적인 스케줄이다. 선형 스케줄은 확산 단계의 75%를 거친 후 인식할 수 없는 상태에 도달하여 매우 빠르게 움직임을 파괴한다[58]. 대신 코사인 스케줄은 모션 신호를 더 느리고 더 고르게 분배하는 방식으로 파괴한다. 도. A는 두 스케줄과 함께 BPE 샘플링 동안 FlowMDM의 성능을 보여준다. 첫째, FlowMDM은 코사인 스케줄의 안정적인 노이즈 주입으로 인해 모든 사실성 및 정확도 메트릭(R-prec 및 FID)에서 더 나은 성능을 달성한다는 것을 관찰한다. 둘째, 정확도(R-prec)와 평활도(AUJ) 곡선의 변위를 식별한다(검은색 화살표 참조). 선형 스케줄 글로벌 종속성이 나중에 복구되기 시작한다는 점을 감안할 때 정확도를 달성하기 위해 더 많은 APE 단계가 필요하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multicolumn{1}{l}{Babel} & HumanML3D \\\\ \\hline walk forward & a person walks in a curved path to the left. \\\\ swim movement & a person stands still and does not move. \\\\ stretch arms & a person walks straight forward. \\\\ walk & a person does jumping jacks. \\\\ stand & a person start to dance with legs. \\\\ step backwards & person walking in an s shape. \\\\ t-pose & a person walks to his right. \\\\ throw the ball & a person slowly walked forward. \\\\ run & the person is standing still doing body stretches. \\\\ circle right arm backwards & the person is dancing the waltz. \\\\ wave right & the person is clapping. \\\\ ginga dance & walking side to side. \\\\ forward kick & a person stayed on the place. \\\\ look around & person is jogging in place. \\\\ steps to the right & a person walks backward for 3 steps. \\\\ side steps & person is running in a circle. \\\\ hop forward & the person is waving hi. \\\\ dance with arms & a person walks in a circular path. \\\\ jog & swinging arms up and down. \\\\ walk slowly & a man walks counterclockwise in a circle. \\\\ jump jacks series & the person is walking towards the left. \\\\ run in half a circle & the person is walking on the treadmill. \\\\ walk a few steps ahead & the man is moving his left arm. \\\\ move head up and down & the person is doing basketball signals. \\\\ rotate right ankle & a person remained sitting down. \\\\ play guitar & a person hits his drums. \\\\ jump forward & person is doing a dance. \\\\ move both hands around chest & a person takes some steps forward. \\\\ swing back and forth & a person slowly walks forward five steps. \\\\ wave & a person jumps in place. \\\\ shake it & this person appears to be painting. \\\\ walk in circle & a person wiping a surface with something. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 A: 바벨 및 HumanML3D에 대한 외삽 운동.\n' +
      '\n' +
      '코사인 스케줄에 따라 부드러움에 도달했습니다.\n' +
      '\n' +
      '분류기 없는 안내에서###\n' +
      '\n' +
      '분류기 없는 안내는 컨디셔닝 신호를 강화시키는 확산 샘플링을 위한 중요한 부가 기능이며, 따라서 생성된 샘플들의 품질 및 정확도를 향상시킨다[33]. 먼저 조건부 잡음제거 동작\\(x_{c}\\)과 조건부 잡음제거 동작\\(x\\)을 계산하여 구현하였다. 그런 다음, 잡음 제거된 샘플은 \\(x+w(x_{c}-x)\\)로 계산된다. \\(w{=}1\\)이면 분류기 없는 안내는 비활성화된다. 분류기 없는 안내로 단일 텍스트 설명으로부터 모션을 생성할 때, 우리는 텍스트 설명에 더 잘 일치하는 모션으로 노이즈 제거를 계속 조향한다. 그러나, 사람의 움직임을 구축할 때\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c} \\hline \\hline  & \\multicolumn{6}{c}{Subsequence} & \\multicolumn{6}{c}{Transition} \\\\  & R-prec \\(\\uparrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & MM-Dist \\(\\downarrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & PJ \\(\\rightarrow\\) & AUJ \\(\\downarrow\\) \\\\ \\hline GT & \\(0.715^{\\pm 0.003}\\) & \\(0.00^{\\pm 0.00}\\) & \\(8.42^{\\pm 0.15}\\) & \\(3.36^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) \\\\ \\hline In-distribution & & & & & & & \\\\ \\hline TEACH\\_B & \\(\\mathbf{0.727}^{\\pm 0.004}\\) & \\(2.26^{\\pm 0.03}\\) & \\(8.20^{\\pm 0.12}\\) & \\(\\mathbf{3.35}^{\\pm 0.01}\\) & \\(2.77^{\\pm 0.05}\\) & \\(6.32^{\\pm 0.07}\\) & \\(1.03^{\\pm 0.00}\\) & \\(2.20^{\\pm 0.01}\\) \\\\ TEACH & \\(0.665^{+0.003}\\) & \\(2.09^{\\pm 0.03}\\) & \\(8.06^{\\pm 0.09}\\) & \\(3.73^{\\pm 0.02}\\) & \\(2.78^{\\pm 0.06}\\) & \\(6.31^{\\pm 0.07}\\) & \\(0.07^{\\pm 0.00}\\) & \\(0.42^{\\pm 0.01}\\) \\\\ DoubleTake* & \\(0.620^{\\pm 0.006}\\) & \\(3.04^{\\pm 0.06}\\) & \\(7.49^{\\pm 0.07}\\) & \\(4.19^{\\pm 0.02}\\) & \\(3.04^{\\pm 0.12}\\) & \\(6.21^{\\pm 0.06}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.01^{\\pm 0.01}\\) \\\\ DoubleTake & \\(0.682^{\\pm 0.008}\\) & \\(1.52^{\\pm 0.03}\\) & \\(7.90^{\\pm 0.07}\\) & \\(3.67^{\\pm 0.04}\\) & \\(3.47^{\\pm 0.08}\\) & \\(6.16^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.62^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(0.724^{\\pm 0.008}\\) & \\(2.00^{\\pm 0.05}\\) & \\(8.36^{\\pm 0.10}\\) & \\(3.38^{\\pm 0.02}\\) & \\(6.33^{\\pm 0.13}\\) & \\(5.91^{\\pm 0.06}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.65^{\\pm 0.01}\\) \\\\ DiffCollage & \\(0.690^{\\pm 0.006}\\) & \\(1.92^{\\pm 0.07}\\) & \\(7.92^{\\pm 0.09}\\) & \\(3.67^{\\pm 0.02}\\) & \\(4.25^{\\pm 0.15}\\) & \\(\\mathbf{6.19}^{\\pm 0.07}\\) & \\(0.19^{\\pm 0.01}\\) & \\(0.82^{\\pm 0.02}\\) \\\\ FlowMDM (Ours) & \\(0.726^{\\pm 0.006}\\) & \\(\\mathbf{1.36}^{\\pm 0.05}\\) & \\(\\mathbf{8.47}^{\\pm 0.10}\\) & \\(3.40^{\\pm 0.03}\\) & \\(\\mathbf{2.26}^{\\pm 0.08}\\) & \\(6.60^{\\pm 0.08}\\) & \\(\\mathbf{0.05}^{\\pm 0.00}\\) & \\(\\mathbf{0.11}^{\\pm 0.00}\\) \\\\ \\hline Out-of-distribution & & & & & & & & \\\\ \\hline TEACH\\_B & \\(0.680^{\\pm 0.006}\\) & \\(1.75^{\\pm 0.04}\\) & \\(8.15^{\\pm 0.11}\\) & \\(3.51^{\\pm 0.01}\\) & \\(3.53^{\\pm 0.06}\\) & \\(6.04^{\\pm 0.10}\\) & \\(1.14^{\\pm 0.01}\\) & \\(2.49^{\\pm 0.01}\\) \\\\ TEACH & \\(0.644^{\\pm 0.004}\\) & \\(2.06^{\\pm 0.03}\\) & \\(7.94^{\\pm 0.12}\\) & \\(3.70^{\\pm 0.01}\\) & \\(4.08^{\\pm 0.08}\\) & \\(6.00^{\\pm 0.09}\\) & \\(\\mathbf{0.07}^{\\pm 0.00}\\) & \\(0.46^{\\pm 0.00}\\) \\\\ DoubleTake* & \\(0.572^{\\pm 0.007}\\) & \\(3.78^{\\pm 0.07}\\) & \\(7.53^{\\pm 0.12}\\) & \\(4.15^{\\pm 0.02}\\) & \\(3.83^{\\pm 0.09}\\) & \\(\\mathbf{6.12}^{\\pm 0.07}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.07^{\\pm 0.02}\\) \\\\ DoubleTake & \\(0.654^{\\pm 0.009}\\) & \\(1.65^{\\pm 0.07}\\) & \\(8.06^{\\pm 0.08}\\) & \\(3.66^{\\pm 0.02}\\) & \\(\\mathbf{2.98}^{\\pm 0.06}\\) & \\(6.03^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.66^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(\\mathbf{0.681}^{\\pm 0.009}\\) & \\(2.11^{\\pm 0.06}\\) & \\(\\mathbf{8.35}^{\\pm 0.08}\\) & \\(\\mathbf{3.47}^{\\pm 0.03}\\) & \\(6.97^{\\pm 0.12}\\) & \\(5.67^{\\pm 0.05}\\) & \\(0.19^{\\pm 0.00}\\) & \\(0.71^{\\pm 0.01}\\) \\\\ DiffCollage & \\(0.652^{\\pm 0.004}\\) & \\(1.60^{\\pm 0.07}\\) & \\(7.91^{\\pm 0.09}\\) & \\(3.74^{\\pm 0.01}\\) & \\(4.65^{\\pm 0.19}\\) & \\(6.00^{\\pm 0.09}\\) & \\(0.20^{\\pm 0.00}\\) & \\(0.86^{\\pm 0.01}\\) \\\\ FlowMDM (Ours) & \\(0.679^{\\pm 0.004}\\) & \\(\\mathbf{1.26}^{\\pm 0.06}\\) & \\(8.16^{\\pm 0.08}\\) & \\(3.50^{\\pm 0.03}\\) & \\(3.17^{\\pm 0.12}\\) & \\(6.44^{\\pm 0.09}\\) & \\(\\mathbf{0.07}^{\\pm 0.00}\\) & \\(\\mathbf{0.17}^{\\pm 0.00}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: HumanML3D에서의 시나리오별 비교.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '## 부록 D 질적 결과\n' +
      '\n' +
      '무화과 C 및 D는 각각 바벨 및 HumanML3D에 대한 6개의 인간 동작 구성(A 내지 F) 및 2개의 외삽(G 및 H)을 나타낸다. 구성들은 32개의 액션들로 구성된 평가 조합들의 서브세트들이기 때문에, 이것들의 시작과 끝은 다른 액션들로의 부분적인 전이를 포함할 수 있다. 모션 비디오도 보충 자료의 일부로 포함되어 있습니다. SMPL 매개 변수를 포함한 움직임 표현 덕분에 바벨의 움직임을 SMPL 바디 메쉬로 표현할 수 있다는 점에 유의하십시오.\n' +
      '\n' +
      '도 C: **정성적 예(바벨).** A-F는 6개의 인간 동작 구성, 및 G-H 2개의 인간 동작 외삽을 특징으로 한다. Sec에 정의된 시나리오에 따라. B, A, B, C는 유통 내 조합에 속하며, D, E, F는 유통 외 조합에 속한다. 모든 샘플의 비디오도 이 보충 자료의 일부로 포함되어 있습니다. 실선 곡선은 전역 위치(파란색) 및 왼쪽/오른쪽 손(보라색/녹색)의 궤적과 일치합니다. 더 어두운 색상은 데이터 세트(검은색 세그먼트)에서 저크의 표준 편차의 두 배로 포화되는 중앙값에서 순간 저크 편차를 나타낸다. 부드러운 전환은 더 가벼운 것 가운데 검은색 부분으로 나타난다.\n' +
      '\n' +
      '도 D. **정성적 예(HumanML3D).** A-F는 6개의 인간 동작 조성물, 및 G-H 2개의 인간 동작 외삽을 특징으로 한다. Sec에 정의된 시나리오에 따라. B, A, B, C는 각각 짧은 시나리오, 중간 시나리오 및 긴 시나리오의 샘플이고 D, E, F는 혼합 시나리오의 샘플이다. 모든 샘플의 비디오도 이 보충 자료의 일부로 포함되어 있습니다.\n' +
      '\n' +
      '[13] HumanML3D의 경우, 골격은 관절의 3차원 좌표만을 포함하기 때문에 사용한다.\n' +
      '\n' +
      '**토론.** 손의 궤적 및 저크 색상 표시기. C 및 D와 비디오는 FlowMDM이 서브시퀀스 간의 가장 부드러운 전환을 생성한다는 것을 강조한다. 특히 최첨단 방법은 전이의 경계에서 빈번한 평활도 아티팩트(검은색 세그먼트)를 나타낸다. 우리는 TEACH에 의해 생성된 구성이 순진한 구형 선형 보간법의 사용으로 인해 사실성이 부족하여 운동 역학을 방해한다는 것을 알 수 있다. 이것은 움직임의 주기성이 명확하게 손상된 두 데이터 세트의 외삽 G 및 H에서 더 분명해진다. 반면, DoubleTake, DiffCollage, MultiDiffusion은 두 가지 중요한 한계를 공유한다. 첫째, 그들은 모든 상황에 맞지 않을 수 있는 미리 결정된 전이 길이를 고수한다. 예를 들어, 바벨-A에서 \'피킹\' 동작은 자연스러운 전환을 생성하기 위한 길이가 부족하기 때문에 매우 빠르게 발생한다. 대조적으로, 우리의 접근법은 인위적인 제약 없이 필요한 경우 두 전환 측면에서 더 많은 전환 시간을 활용할 수 있다. 둘째, 이러한 방법들에서 잡음 제거 과정은 이웃 서브시퀀스들의 작은 부분만을 고려함으로써, 동적 움직임 외삽에서의 성능 저하를 초래한다. 예를 들어, HumanML3D-G에서는 모두 불규칙한 점핑 잭을 생성한다. 제안된 방법은 저주파 움직임 스펙트럼을 독립적으로 생성하지만, 이후 단계에서 발생하는 불일치를 효과적으로 보정하여 사실적이고 주기적인 움직임을 생성한다. \'홉 포워드\' 동작을 성공적으로 외삽하는 바벨-H의 경우, 각 서브시퀀스를 이웃 동작 전체와 동기화해야 하는 경우, 본 모델은 부드럽고 일관성 있으며 현실적인 외삽을 생성할 수 있는 유일한 모델이다.\n' +
      '\n' +
      '**제한.** 그러나 FlowMDM은 그것의 불완전함이 없는 것은 아니다. 우리는 우리의 방법이 인간ML3D-B의 첫 번째 설명과 같이 매우 복잡한 설명에 어려움을 겪는다는 것을 발견했다. \'뒤로 걷다, 앉다, 서다, 앞으로 다시 걷다\'를 포함하는 복잡한 설명을 실행하는 대신 뒤로만 걷는다. 동작의 부분 실행이 다른 방법에서도 관찰된다는 점을 감안할 때, 우리는 더 넓은 텍스트 대 동작 작업과 관련된 도전이라고 생각한다. 실제로, 우리의 모델은 이론적으로 더 나은 텍스트 임베딩을 사용하는 것과 같은 개선된 컨디셔닝 스킴으로부터도 이익을 얻을 수 있다. Sec에서 논의된 우리 모델의 또 다른 인정된 한계. 도 5는 저주파 성분의 독립적인 생성이다. 예를 들어, 바벨-B에서는 앉은 자세와 서 있는 자세 사이의 약간의 불일치가 관찰된다. 그럼에도 불구하고 이러한 효과를 나타내는 DiffCollage, MultiDiffusion 및 DoubleTake와 달리 FlowMDM은 더 부드러운 결과를 생성한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>