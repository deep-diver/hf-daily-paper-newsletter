<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Seamless Human Motion Composition with Blended Positional Encodings\n' +
      '\n' +
      'German Barquero  Sergio Escalera  Cristina Palmero\n' +
      '\n' +
      'Universitat de Barcelona and Computer Vision Center, Spain\n' +
      '\n' +
      '{germanbarquero, sescalera}@ub.edu, crpalmec7@alumnes.ub.edu\n' +
      '\n' +
      '[https://barquerogerman.github.io/FlowMDM/](https://barquerogerman.github.io/FlowMDM/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'In the field of computer vision, recent progress has been made in developing photorealistic avatars [54] for applications like virtual reality, gaming, and robotics [62, 79]. Aside from looking visually realistic, these avatars must also move in a convincing manner. This is challenging due to the intricate nature of human motion, strongly influenced by various factors such as the environment, interactions, and physical contact [14]. Furthermore, complexity increases when attempting to control these motions. Recent advances include the generation of motion sequences from _control signals_ like textual descriptions or actions [109]; however, such methods only produce isolated, standalone motion. Therefore, these approaches fail to handle scenarios where a long motion is driven by distinct control signals on different time slices. Such capability is needed to provide full control over the sequence of desired actions and their duration. In these scenarios, the generated motion needs to feature seamless and realistic transitions between actions. In this work, we tackle this problem, which we refer to as generative Human Motion Composition (HMC). In particular, we focus on generating single-human motion from text, illustrated in Fig. 1.\n' +
      '\n' +
      'One of the primary obstacles in HMC is the lack of datasets that offer long motion sequences with diverse textual annotations. Existing datasets typically feature sequences of limited duration, often lasting only up to 10 seconds, and with just a single control signal governing the entire sequence [26, 64]. This limitation calls for innovative solutions to address the inherent complexities of the task. Prior works have tackled this problem mostly with autoregressive approaches [4, 45, 48, 66, 104]. These methods iteratively create compositions by using the current motion as a basis to generate subsequent motions. However, they require datasets with multiple consecutive annotated motions, and tend to degenerate in very long HMC scenarios due to error accumulation [107]. Other recent works have leveraged the infilling capabilities of motion diffusion models to generate motion compositions [73, 103]. However, for these, a substantial portion of each motion sequence is generated independently from adjacent motions, and generating transitions requires computing redundant denoising steps. In this work, we propose a novel architecture designed to address these specific challenges. Our main contributions are:\n' +
      '\n' +
      '* We propose FlowMDM, the first diffusion-based model that generates seamless human motion compositions without any postprocessing or extra denoising steps. To accomplish it, we introduce Blended Positional Encodings (BPE), a new technique for diffusion Transformers that combines the benefits of both absolute and relative positional encodings during sampling. In particular, the denoising first exploits absolute information to recover the global motion coherence, and then leverages relative positions to build smooth and realistic transitions between actions. As a result, FlowMDM achieves state-of-the-art results in terms of accuracy, realism, and smoothness in the HumanML3D [26] and Babel [65] datasets.\n' +
      '* We introduce a new attention technique tailored for HMC: the Pose-Centric Cross-ATtention (PCCAT). This layer ensures each pose is denoised based on its own condition and its neighboring poses. Consequently, FlowMDM can be trained on a dataset with only a single condition available per motion sequence and still generate realistic transitions when using multiple conditions at inference time.\n' +
      '* We reveal the lack of sensitivity of current HMC metrics to identify discontinuous or sharp transitions, and introduce two new metrics that help to detect them: the Peak Jerk (PJ) and the Area Under the Jerk (AUJ).\n' +
      '\n' +
      '## 2 Related work\n' +
      '\n' +
      '**Conditional human motion generation.** Recent studies in motion generation have shown notable progress in synthesizing movements conditioned on diverse modalities such as text [21, 26, 27, 35, 40, 63, 81, 100, 101, 102], music [2, 17, 47, 77, 84, 96, 110], scenes [15, 87, 88, 89, 97], interactive objects [1, 18, 42, 92], and even other humans\' behavior [93, 10, 28, 80, 9]. Traditionally, these approaches have been designed to generate motion sequences matching a single condition. The progress of this domain has been boosted by the release of big datasets including diverse modalities or manual annotations [12, 26, 28, 47, 51, 60, 64, 65]. Research has also focused on problems like human motion prediction [3, 53, 57, 72, 78, 83, 86, 99] and motion infilling [29, 36, 39, 49, 50, 59, 67, 75, 108], which do not rely on extensive manual annotations but rather on motion itself. Both tasks share a common challenge with HMC: the synthesized motion must not only be plausible but also integrate seamlessly with the neighboring behavior, ensuring fluidity and continuity. In this context, the utilization of human motion priors has been proven to be a successful technique to ensure any generated motion includes natural transitions [8, 46, 91]. In line with these approaches, our method learns a motion prior specifically tailored for HMC.\n' +
      '\n' +
      '**Autoregressive human motion composition.** As in many other sequence modeling tasks, HMC was also first tackled with autoregressive methods. The gold standard has been pairing variational autoencoders with autoregressive decoders such as recurrent neural networks [104] or Transformers [4, 45, 66, 4]. Alternative approaches have introduced specialized reinforcement learning frameworks [52, 95, 105]. Autoregressive models rely on the availability of annotated motion transitions, a requirement that constrains the robustness of the models due to the scarcity of such data. To mitigate this issue, some methods include additional postprocessing steps like linear interpolations [4], or affine transformations [45]. However, these can distort the human motion dynamics and require a predetermined estimation of the transitions duration. Furthermore, autoregressive approaches generate motion solely based on the preceding motion. We argue that an accurate model should mimic the humans innate capacity to anticipate their next action and adapt their current behavior accordingly [43, 24].\n' +
      '\n' +
      '**Diffusion-based human motion composition.** Diffusion models have excelled at conditional generation [74, 32, 20]. They also possess great zero-shot capabilities for image inpainting [70], and its equivalence in motion: motion infilling. DiffCollage [103], MultiDiffusion [7], and DoubleTake [73] proposed to modify the diffusion sampling process to simultaneously generate temporally superimposed motion sequences, and combine the estimated noise in the overlapped regions so that an infilled transition emerges. DoubleTake complemented such overlapped sampling with a refinement step in which the emerged transition undergoes further unconditional denoising steps. All these methods share two main limitations. First, they are constrained to modeling dependencies among neighboring motion sequences. This becomes a limitation when three or more consecutive actions share semantics and collectively represent a more comprehensive action. In this case, the motion dependencies may extend beyond contiguous actions. Second, they need to set the number of framesthat each transition takes between consecutive actions, for which extra computations are incorporated. Our work seeks to address these constraints by offering a solution able to model longer inter-sequence dynamics without imposing extra computational burdens or predefined transition durations.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '**Problem definition.** Our goal consists in generating a motion sequence of \\(N\\) frames, with the capability of conditioning the generated motion inside non-overlapping intervals \\([0,\\tau_{1}),[\\tau_{1},\\tau_{2}),...,[\\tau_{j},N)\\), with \\(0{<}\\tau_{1}{<}\\cdots{<}\\tau_{j}{<}N\\). We will refer to the motion inside these intervals as _motion subsequences_, or \\(\\mathcal{S}_{i}=\\{x_{\\tau_{i}},...,x_{\\tau_{i+1}-1}\\}\\), each driven by its corresponding condition \\(c_{i}\\), and with a maximum length of \\(L\\). It is essential that consecutive subsequences, influenced by different control signals, transition seamlessly and realistically. In particular, we aim at the even more challenging case where motion sequences containing several pairs of \\((S_{i},c_{i})\\) are not necessarily available in our dataset.\n' +
      '\n' +
      'In this section, we present FlowMDM, an architecture with strong inductive biases that promote the emergence of a **robust translation-invariant motion prior**. Such _motion prior_ is learned with a diffusion model equipped with a bidirectional (i.e., encoder-only) Transformer, similar to prior works [73, 82]. With it, we overcome the main limitations of autoregressive methods (Sec. 3.1). However, previous works are constrained in terms of motion duration. We could arguably provide extrapolation capabilities to the diffusion model by replacing the absolute positional encoding with a relative alternative, thus making the denoising of each pose _translation invariant_. However, this technique would fail to build complex compositional semantics that require knowledge about the start and end of each subsequence. For example, when generating the motion composition \\(S_{i}{\\rightarrow}S_{i+1}\\) with \\(c_{i}{=}\\)_walking_\' and \\(c_{i+1}{=}\\)_walk and sit down_\', \\(S_{i+1}\\) might only feature the action \'_sit down_\' because, with only relative positional information, the Transformer cannot know if the partially denoised \'_walking_\' motion preceding the beginning of \\(S_{i+1}\\) belongs to \\(S_{i}\\) or \\(S_{i+1}\\). To combine the benefits of both relative and absolute positional encodings, we introduce BPE (Sec. 3.2). This novel technique exploits the iterative nature of diffusion models to promote intra-subsequence global coherence in earlier denoising stages, while making later denoising stages translation invariant, ensuring that realistic and plausible transitions naturally emerge between subsequences. Still, during training, the condition remains unchanged throughout all ground truth motion sequences. In order to make our denoising model _robust_ to having multiple conditions per sequence at inference, we introduce a new attention paradigm called PCCAT (Sec. 3.3). As a result, FlowMDM is able to simultaneously generate very long compositions of human motion subsequences, all in harmony and fostering plausible transitions between them, without explicit supervision on transitions generation.\n' +
      '\n' +
      '### Bidirectional diffusion\n' +
      '\n' +
      'The cumulative nature of errors in autoregressive models often results in a decline in performance when generating long sequences [107]. This is exacerbated in HMC, where transitions are scarce or even missing in the training corpus, and the model needs to deal with domain shifts at inference. Another limitation of autoregressive methods is that the generated \\(\\mathcal{S}_{i}\\) only depends on \\(\\{\\mathcal{S}_{j}\\}_{j<i}\\). We discussed in Sec. 2 why this is a suboptimal solution for HMC. Thus, an appropriate model for HMC should also be able to anticipate the following motion, \\(\\mathcal{S}_{i+1}\\), and possibly adapt \\(\\mathcal{S}_{i}\\) so that the transition is feasible. We argue that the iterative paradigm of diffusion models provides very appropriate inductive biases for naturally mimicking such ability: the partially denoised \\(\\mathcal{S}_{i}\\) and \\(\\mathcal{S}_{i+1}\\) are refined later in successive denoising steps. By choosing a bidirectional Transformer as our denoising function [38], we enable the modeling of both past and future dependencies. Therefore, we design our framework as a bidirectional motion diffusion model, similar to MDM [82]. We refer the reader to [94] for more details on the theoretical aspects of diffusion models.\n' +
      '\n' +
      '### Blended positional encodings\n' +
      '\n' +
      'Diffusion models can learn strong motion priors that ensure any motion generated is realistic and plausible [73]. In fact, they can also generate smooth transitions between subsequences [7, 73, 103]. However, these capabilities stem from inference-time motion infilling techniques, which we argue do not exploit the full potential of human motion priors. In fact, building a prior that extrapolates well to sequences longer than those observed during training is very challenging. The field of natural language processing has made progress in sequence extrapolation techniques, notably by substituting absolute positional encoding (APE) with a relative (RPE) counterpart [37]. By only providing information regarding how far tokens are between them, they achieve sequence-wise translation invariance and, therefore, can extrapolate their modeling capabilities to longer sequences. Yet, the absolute positions of poses within a motion, including their distances to the start and end of the action, are necessary to build the global semantics of the motion, as exemplified at the beginning of this section.\n' +
      '\n' +
      'Here, we propose BPE, a novel positional encoding scheme designed for diffusion models that enables motion extrapolation while preserving the global motion semantics. Our BPE is inspired by the observation that in motion, high frequencies encompass local fine details, whereas low frequencies capture global structures. Similar insights have been drawn for images [61]. Diffusion models ex cel at decomposing the generation process into recovering lower frequencies, and gradually transitioning to higher frequencies. Fig. 2 shows how at early denoising phases, motion diffusion models prioritize global inter-frame dependencies, shifting towards local relative dependencies as the process unfolds. The proposed BPE harmonizes these dynamics _during inference_: at early denoising stages, our denoising model is fed with an APE and, towards the conclusion, with an RPE. A scheduler guides this transition. As a result, intra-subsequence global dependencies are recovered at the beginning of the denoising, and intra- and inter-subsequences motion smoothness and realism are promoted later. To make the model understand APE and RPE at inference, we expose it to both encodings by randomly alternating them during training. As a result, the BPE schedule can be tuned at inference time to balance the intra-subsequence coherence and the inter-subsequence realism trade-off.\n' +
      '\n' +
      '**Rotary Position Encoding (RoPE).** Our choice for RPE is rotary embeddings [76]. RoPE integrates a position embedding into the queries and keys, ensuring that after dot-product multiplication, the attention scores\' positional information reflects only the relative pairwise distance between queries and keys. Specifically, let \\(W_{q}\\) and \\(W_{k}\\) be the projection matrices into the \\(d\\)-dimensional spaces of queries and keys. Then, RoPE encodes the absolute positions \\(m\\) and \\(n\\) of a pair of query (\\(q_{m}{=}W_{q}x_{m}\\)) and key (\\(k_{n}{=}W_{k}x_{n}\\)), respectively, as \\(d\\)-dimensional rotations \\(R_{m}^{d},R_{n}^{d}\\) over the projected poses \\(x_{m},x_{n}\\). The rotation angles are parameterized by \\(m\\) and \\(n\\) so that the attention formulation becomes:\n' +
      '\n' +
      '\\[q_{m}^{T}k_{n}=(R_{m}^{d}W_{q}x_{m})^{T}(R_{n}^{d}W_{k}x_{n})=x_{m}^{T}W_{q}R_ {n-m}^{T}W_{k}x_{n}. \\tag{1}\\]\n' +
      '\n' +
      'Note that the resulting rotation \\(R_{n-m}^{d}\\) only depends on the distance between \\(n\\) and \\(m\\), and any absolute information about \\(n\\) or \\(m\\) is removed. RoPE is a natural choice for our RPE due to its simplicity and convenient injection before the attention takes place. As a result, RoPE is compatible with faster attention techniques like FlashAttention [22, 23].\n' +
      '\n' +
      '**Sinusoidal Position Encoding.** Our APE is the classic sinusoidal position encoding [85], which leverages sine and cosine functions to inject positional information. It is added to the queries, keys, and values of the attention layers.\n' +
      '\n' +
      'Note that for APE, attention is limited to each subsequence, while for RPE, attention spans all frames up to the attention horizon \\(H{<}L{<}N\\). Since \\(L\\) defines the maximum range of motion dynamics learned during RPE training, there is no advantage in setting \\(H{\\geq}L\\) (Tabs. D/E in supp. material). Leveraging both APE and RPE constraints ensures quadratic complexity over the maximum subsequence length \\(L\\) in both memory and computation [11]. As a result, FlowMDM\'s complexity is equivalent to that of other Transformer-based motion diffusion models [73, 103].\n' +
      '\n' +
      '### Pose-centric cross-attention\n' +
      '\n' +
      'In order to make motion generation with diffusion models efficient, we would like to _simultaneously_ generate very long sequences. In motion Transformers, the generation is conditioned at a sequence level by injecting the condition as a token [82], or as a sequence-wise transformation in intermediate layers [102]. Therefore, they cannot be conditioned on multiple signals in different subsequences. For this reason, diffusion-based methods for HMC opted for individually generating sequences and then merging them [73, 103]. To enable such simultaneous heterogeneous conditioning without any extra postprocessing, we propose to inject the condition at every frame. However, we still need to deal with a challenge: the condition never varies at training time. Therefore, at inference time, attention scores are computed with the embeddings \\(E_{x_{m},c_{m}}\\) and \\(E_{x_{n},c_{n}}\\) of the pose-condition pairs (\\(x_{m}\\), \\(c_{m}\\)) and (\\(x_{n}\\), \\(c_{n}\\)) as:\n' +
      '\n' +
      '\\[q_{m}^{T}k_{n}=(W_{q}E_{x_{m},c_{m}})^{T}(W_{k}E_{x_{n},c_{n}})=E_{x_{m},c_{m} }^{T}W_{q}^{T}W_{k}E_{x_{n},c_{n}}. \\tag{2}\\]\n' +
      '\n' +
      'When \\(c_{m}{\\neq}c_{n}\\), \\(q_{m}^{T}k_{n}\\) was never encountered during training. If instead of injecting the condition at every frame, we used cross-attention layers, distinct conditions would also be temporally mixed, and we would face the same problem. To reduce the presence and impact of such training-inference misalignment, we introduce PCCAT, see Fig. 3, which aims at minimizing the entanglement between conditions and noisy poses. Specifically, PCCAT combines every frame\'s noisy pose and condition into queries, while using only noisy poses as keys and values. Thus, Eq. 2 becomes:\n' +
      '\n' +
      '\\[q_{m}^{T}k_{n}=(W_{q}E_{x_{m},c_{m}})^{T}(W_{k}E_{x_{n}})=E_{x_{m},c_{m}}^{T}W _{q}^{T}W_{k}E_{x_{n}}. \\tag{3}\\]\n' +
      '\n' +
      'With PCCAT, the attention output for pose \\(m\\) becomes a weighted average of the value projections of its neighboring noisy poses. A residual connection adds the PCCAT output to the noisy poses. With comprehensive coverage of the motion spectrum in the training dataset, the network observes various poses preceding and following each pose, particularly within its local neighborhood. Therefore, local relationships do not suffer from unseen intermediate representations. Still, there is an obstacle to address: long-range de\n' +
      '\n' +
      'Figure 2: Attention scores of a single query pose (current frame) as a function of the pose attended to (x-axis) in a diffusion-based motion generation model with a sinusoidal absolute positional encoding. Curves show the scores at each denoising step. We observe that, whereas early steps show strong global dependencies (blue), later denoising stages exhibit a clearly local behavior (red).\n' +
      '\n' +
      'pendencies. However, as discussed in Sec. 3.2, their importance is mostly confined to the initial stages of denoising. There, the network is exposed to very noisy motion data, thus becoming robust to such unseen combinations of poses. In the latest denoising stages, when the network deals with almost clean input sequences, global dependencies have already been developed and attention is short-ranged (Fig. 2).\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Experimental setup\n' +
      '\n' +
      '**Datasets.** Our experiments are conducted on the Babel [65] and HumanML3D [26] datasets, with their train and test splits. HumanML3D features multiple textual descriptions of each motion sequence, but lacks explicit transition annotations, making supervised learning infeasible for transition generation. Babel, on the other hand, provides finely-grained textual descriptions at an atomic level, including transitions, which facilitates more precise and dynamic motion control but also presents a greater challenge due to fast and short transitions. To demonstrate the flexibility of FlowMDM, we employ the standard motion representations provided with each dataset. HumanML3D utilizes a 263D pose vector that includes joint coordinates, angles, velocities, and feet contact. By contrast, Babel uses the global position and orientation and a 6D rotation representation [106] of the SMPL model joints [13], as in [63].\n' +
      '\n' +
      '**Evaluation.** Our evaluation uses the metrics established by [26], and later refined for this task in [48, 73, 95]. More specifically, motion sequences are synthesized as compositions of 32 pairs of textual descriptions and their durations. The 32 subsequences and the 31 transitions between \\(S_{i-1}\\) and \\(S_{i}\\) pairs are evaluated independently. In particular, each transition is defined as the set of consecutive poses \\(\\{x_{\\tau_{i}-L_{tr}/2},\\ldots,x_{\\tau_{i}+L_{tr}/2-1}\\}\\), sharing \\(\\frac{L_{tr}}{2}\\) frames with \\(S_{i-1}\\) and \\(S_{i}\\). The transition duration \\(L_{tr}\\) is set to 30 and 60 frames for Babel and HumanML3D (1 and 3 seconds), respectively. The top-3 R-precision (R-prec), and the multimodal distance (MM-Dist) are used to evaluate how well the subsequences\' motion matches their textual description [26]. The FID score and the average pairwise distance among all motion embeddings (diversity) assess the quality and variety of both subsequences and transitions, respectively [26, 31]. All metrics are averaged over 10 runs with 95% confidence intervals reported.\n' +
      '\n' +
      '**Closing the gap: the Jerk.** Generative models are hard to evaluate [19, 71, 94]. The FID score [31] has proven to be a very reliable metric in quantifying the similarity between distributions of generated and real motion data while being sensitive to motion artifacts or noise [55]. Nevertheless, exclusively relying on perceptual metrics like FID for assessing transition quality can be misleading due to their insensitivity to motion anomalies such as abrupt accelerations [8], or foot skating [56]. To complement the FID, our work introduces two novel metrics built upon the concept of _jerk_ (i.e., the time derivative of acceleration), which is indicative of motion smoothness and known to be sensitive to kinetic irregularities [5, 6, 16, 25, 34, 44, 98]. Given that natural human motion typically exhibits constrained jerk due to relatively consistent acceleration patterns [25, 44], our metrics are tailored to highlight _persistent deviations_ from this norm in generated transitions. Firstly, we compute the Peak Jerk (PJ), taking the maximum value found throughout the transition motion over all joints. While this measure captures extreme fluctuations, it may favor models that unnaturally smooth transitions across several wider peaks of jerk. To measure this undesirable effect, we introduce the Area Under the Jerk (AUJ), calculated as the sum of L1-norm differences between a method\'s instantaneous jerk and the dataset\'s average jerk value. This measure serves as an aggregate indicator of motion smoothness, quantifying the cumulative deviation from natural human movement across the entire transition. The PJ and AUJ of a transition are formally defined as follows:\n' +
      '\n' +
      '\\[\\text{PJ}=\\max_{\\begin{subarray}{c}1\\leq i\\leq K\\\\ 1\\leq\\tau\\leq L_{tr}\\end{subarray}}|j_{i}(\\tau)|_{1},\\quad\\text{AUJ}=\\sum_{ \\tau=1}^{L_{tr}}\\max_{1\\leq i\\leq K}|j_{i}(\\tau)-j_{avg}|_{1}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(j_{i}(\\tau)\\) is the jerk at time \\(\\tau\\) for joint \\(i\\), \\(K\\) is the number of joints, and \\(j_{avg}\\) is the average joints-wise maximum jerk across the dataset.\n' +
      '\n' +
      '**Baselines.** We compare our method to publicly released related works that can generate sequential motions from text: the autoregressive TEACH [4], and the diffusion sampling techniques DoubleTake [73], DiffCollage [103], and MultiDiffusion [7]. Sampling techniques are evaluated with PCCAT and APE for a fairer comparison. Additionally, we evaluate TEACH with its spherical linear interpolation over transitions turned off (TEACH_B), and DoubleTake with\n' +
      '\n' +
      'Figure 3: **Pose-centric cross-attention.** Our attention minimizes the entanglement between the control signal (e.g., text, objects) and the noisy motion by feeding the former only to the query. Consequently, our model denoises each frame’s noisy pose only leveraging its own condition, and the neighboring noisy poses.\n' +
      '\n' +
      'MDM, as originally proposed (DoubleTake*). TEACH and TEACH_B cannot be trained for HumanML3D due to the lack of pairs of consecutive actions and textual descriptions.\n' +
      '\n' +
      '**Implementation details.** We tune the hyperparameters of all models with grid search. The attention horizon for RPE, \\(H\\), is set to 100/150 for Babel/HumanML3D. The number of diffusion steps is 1K for all experiments. Our model is trained with the \\(x_{0}\\) parameterization [90], and minimizes the L2 reconstruction loss. During training, RPE and APE are alternated randomly at a frequency of 0.5. We use classifier-free guidance with weights 1.5/2.5 [33]. We use a binary step function to guide the BPE sampling, yielding 125/60 initial APE steps. The minimum/maximum lengths for training subsequences are set to 30/200 and 70/200 frames (i.e., 1/6.7s and 3.5/10s). For Babel, training subsequences include consecutive ground truth motions with distinct textual descriptions in order to increase the motions variability, and make the network explicitly robust to multiple conditions. The ablation study includes two conditioning baselines: 1) concatenating each frame\'s condition and noisy pose, and replacing the PCCAT with vanilla self-attention (SAT), and 2) injecting the condition with cross-attention layers (CAT). See more details in supp. material Sec. A.\n' +
      '\n' +
      '### Quantitative analysis\n' +
      '\n' +
      '**Comparison with the state of the art on HMC.** Tables 1 and 2 show the comparison of FlowMDM with current state-of-the-art models in Babel and HumanML3D datasets, respectively. In HumanML3D, our model outperforms by a fair margin the other methods in terms of subsequence accuracy-wise metrics (R-prec and MM-Dist), and FID. In Babel, it matches the state of the art in accuracy and excels in FID score. FlowMDM produces transitions of higher quality and smoothness on both datasets, as indicated by FID, PJ, and AUJ metrics. The lack of correlation between the FID score and the AUJ underscores the importance of the latter as a complementary metric for assessing smoothness. Fig. 4-left shows the average jerk values across the generated transitions. We observe that state-of\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c}{Subsequence} & \\multicolumn{4}{c}{Transition} \\\\  & R-prec \\(\\uparrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & MM-Dist \\(\\downarrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & PJ \\(\\rightarrow\\) & AUJ \\(\\downarrow\\) \\\\ \\hline GT & \\(0.715^{\\pm 0.003}\\) & \\(0.00\\pm 0.00\\) & \\(8.42\\pm 0.15\\) & \\(3.36\\pm 0.00\\) & \\(0.00\\pm 0.00\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02\\pm 0.00\\) & \\(0.00\\pm 0.00\\) \\\\ \\hline TEACH,B & \\(\\textbf{0.703}^{\\pm 0.002}\\) & \\(1.71\\pm 0.03\\) & \\(8.18\\pm 0.14\\) & \\(\\textbf{3.43}^{\\pm 0.01}\\) & \\(3.01\\pm 0.04\\) & \\(\\textbf{6.23}^{\\pm 0.05}\\) & \\(1.09\\pm 0.00\\) & \\(2.35^{\\pm 0.01}\\) \\\\ TEACH & \\(0.655^{\\pm 0.002}\\) & \\(1.82\\pm 0.02\\) & \\(7.96^{\\pm 0.11}\\) & \\(3.72^{\\pm 0.01}\\) & \\(3.27^{\\pm 0.04}\\) & \\(6.14\\pm 0.06\\) & \\(0.07^{\\pm 0.00}\\) & \\(0.44^{\\pm 0.00}\\) \\\\ DoubleTake* & \\(0.596^{\\pm 0.005}\\) & \\(3.16\\pm 0.06\\) & \\(7.53^{\\pm 0.11}\\) & \\(4.17^{\\pm 0.02}\\) & \\(3.33^{\\pm 0.06}\\) & \\(6.16^{\\pm 0.05}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.04^{\\pm 0.01}\\) \\\\ DoubleTake & \\(0.668^{\\pm 0.005}\\) & \\(1.33\\pm 0.04\\) & \\(7.98\\pm 0.12\\) & \\(3.67^{\\pm 0.03}\\) & \\(3.15^{\\pm 0.05}\\) & \\(6.14^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.64^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(0.702^{\\pm 0.005}\\) & \\(1.74\\pm 0.04\\) & \\(8.87\\pm 0.13\\) & \\(\\textbf{3.43}^{\\pm 0.02}\\) & \\(6.56^{\\pm 0.12}\\) & \\(5.72^{\\pm 0.07}\\) & \\(0.18^{\\pm 0.00}\\) & \\(0.68^{\\pm 0.00}\\) \\\\ DiffCollage & \\(0.671^{\\pm 0.003}\\) & \\(1.45^{\\pm 0.05}\\) & \\(7.93^{\\pm 0.09}\\) & \\(3.71^{\\pm 0.01}\\) & \\(4.36^{\\pm 0.09}\\) & \\(6.09^{\\pm 0.08}\\) & \\(0.19^{\\pm 0.00}\\) & \\(0.84^{\\pm 0.01}\\) \\\\ \\hline FlowMDM & \\(0.702^{\\pm 0.004}\\) & \\(\\textbf{0.99}^{\\pm 0.04}\\) & \\(\\textbf{8.36}^{\\pm 0.13}\\) & \\(3.45^{\\pm 0.02}\\) & \\(\\textbf{2.61}^{\\pm 0.06}\\) & \\(6.47^{\\pm 0.05}\\) & \\(\\textbf{0.06}^{\\pm 0.00}\\) & \\(\\textbf{0.13}^{\\pm 0.00}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Comparison of FlowMDM with the state of the art in Babel. Symbols \\(\\uparrow\\), \\(\\downarrow\\), and \\(\\rightarrow\\) indicate that higher, lower, or values closer to the ground truth (GT) are better, respectively. Evaluation is run 10 times and \\(\\pm\\) specifies the 95% confidence intervals.\n' +
      '\n' +
      'Figure 4: **Transitions smoothness.** Average maximum jerk over joints at each frame of the transitions for both motion composition (left) and extrapolation (right) tasks. While other methods show severe smoothness artifacts in the beginning and end of their transition refinement processes, FlowMDM’s jerk curve has the shortest peak for composition, and an absence of peaks for extrapolation.\n' +
      '\n' +
      'the-art methods exhibit severe smoothness artifacts. During TEACH\'s spherical linear interpolation, the jerk quickly reaches values near zero. By contrast, DiffCollage leans toward higher-than-average jerk values, while MultiDiffusion exhibits the opposite trend. DoubleTake shows three peaks, caused by their two-stage noise estimation process. In comparison, FlowMDM successfully minimizes peak jerk values, producing the smoothest transitions between subsequences. See supp. material Sec. C for in-depth analyses.\n' +
      '\n' +
      '**Human motion extrapolation.** In single text-to-motion, the duration of the generated motion is limited to the longest subsequence length \\(L\\) available in the training set. Extrapolating periodic actions into sequences longer than those in the ground truth presents a notable challenge. Achieving this through HMC requires the harmonization of periodicity across adjacent subsequences. However, common strategies that combine independently generated subsequences often disrupt the periodicity of the motion. To assess our model\'s capabilities in addressing this issue, we construct an evaluation set comprising 32 consecutive repetitions of 32 different extrapolatable actions such as \'walk forward\', \'jumping\', or \'playing the guitar\', extracted from the Babel and HumanML3D test sets (more details in supp. material Sec. B). Fig. 4-right displays the motion jerk across transitions for all models on this task. We observe that, while other models exhibit smoothness anomalies similar to those shown in the HMC evaluation, FlowMDM closely mirrors the ground truth jerk. This observation indicates that the jerk peak noted in FlowMDM for the composition task is likely attributed to smoothness irregularities in more complex transitions.\n' +
      '\n' +
      '**Ablation study.** The effectiveness of BPE and PCCAT is presented in Tables 3 and 4. Reasonably, the baseline model trained solely with APE fails to generate smooth transitions. Conversely, a model trained only with RPE, despite producing the smoothest transitions, struggles to model global motion dependencies and accurately reflect the corresponding textual descriptions. Interestingly, training with BPE improves the performance of both APE- and RPE-only samplings. Sampling with BPE combines the best of both worlds by preserving the excellent AUJ values of the RPE models and reaching the state-of-the-art accuracy and FID\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c c c|c c c} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Cond.}} & \\multicolumn{3}{c}{\\multicolumn{3}{c}{Subsequence}} & \\multicolumn{5}{c}{Transition} \\\\  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline GT & - & - & \\(0.796^{\\pm 0.004}\\) & \\(0.00^{\\pm 0.00}\\) & \\(9.34^{\\pm 0.08}\\) & \\(2.97^{\\pm 0.01}\\) & \\(0.00^{\\pm 0.00}\\) & \\(9.54^{\\pm 0.15}\\) & \\(0.04^{\\pm 0.00}\\) & \\(0.07^{\\pm 0.00}\\) \\\\ \\hline PCCAT & A & A & \\(0.689^{\\pm 0.005}\\) & \\(0.66^{\\pm 0.02}\\) & \\(9.73^{\\pm 0.12}\\) & \\(3.63^{\\pm 0.02}\\) & \\(3.90^{\\pm 0.12}\\) & \\(8.29^{\\pm 0.08}\\) & \\(1.50^{\\pm 0.01}\\) & \\(3.40^{\\pm 0.02}\\) \\\\ PCCAT & R & R & \\(0.531^{\\pm 0.005}\\) & \\(1.75^{\\pm 0.07}\\) & \\(8.71^{\\pm 0.10}\\) & \\(4.80^{\\pm 0.03}\\) & \\(2.53^{\\pm 0.12}\\) & \\(8.62^{\\pm 0.08}\\) & \\(\\textbf{0.03}^{\\pm 0.00}\\) & \\(0.58^{\\pm 0.01}\\) \\\\ \\hline PCCAT & B & A & \\(\\textbf{0.699}^{\\pm 0.005}\\) & \\(0.61^{\\pm 0.02}\\) & \\(9.76^{\\pm 0.10}\\) & \\(3.54^{\\pm 0.02}\\) & \\(2.42^{\\pm 0.09}\\) & \\(8.39^{\\pm 0.09}\\) & \\(1.40^{\\pm 0.01}\\) & \\(3.29^{\\pm 0.02}\\) \\\\ PCCAT & B & R & \\(0.554^{\\pm 0.007}\\) & \\(1.06^{\\pm 0.06}\\) & \\(9.02^{\\pm 0.11}\\) & \\(4.54^{\\pm 0.02}\\) & **1.12\\({}^{\\pm 0.04}\\)** & **9.00\\({}^{\\pm 0.10}\\)** & \\(0.05^{\\pm 0.00}\\) & \\(0.53^{\\pm 0.01}\\) \\\\ \\hline SAT & B & B & \\(0.692^{\\pm 0.004}\\) & \\(0.49^{\\pm 0.02}\\) & \\(9.08^{\\pm 0.09}\\) & **3.51\\({}^{\\pm 0.01}\\)** & \\(3.19^{\\pm 0.08}\\) & \\(8.09^{\\pm 0.11}\\) & \\(0.04^{\\pm 0.00}\\) & **0.36\\({}^{\\pm 0.02}\\)** \\\\ CAT & B & B & \\(0.622^{\\pm 0.005}\\) & \\(1.27^{\\pm 0.04}\\) & \\(8.86^{\\pm 0.15}\\) & \\(4.10^{\\pm 0.01}\\) & \\(3.93^{\\pm 0.14}\\) & \\(8.23^{\\pm 0.10}\\) & \\(\\underline{0.04}^{\\pm 0.00}\\) & \\(\\underline{0.49}^{\\pm 0.02}\\) \\\\ \\hline PCCAT & B & B & \\(0.685^{\\pm 0.004}\\) & \\(\\textbf{0.29}^{\\pm 0.01}\\) & \\(\\textbf{9.58}^{\\pm 0.12}\\) & \\(3.61^{\\pm 0.01}\\) & \\(1.38^{\\pm 0.05}\\) & \\(\\underline{8.79}^{\\pm 0.09}\\) & \\(0.06^{\\pm 0.00}\\) & \\(0.51^{\\pm 0.01}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Ablation study in HumanML3D.\n' +
      '\n' +
      'Figure 5: **BPE trade-offs.** Increasing the number of APE steps undergone during BPE sampling improves the correspondence between motion and textual description (R-prec), but reduces the transition realism and smoothness (FID and AUJ). The best balance is reached around 10% of APE denoising steps.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c|c c|c c c|c c c} \\hline \\hline \\multicolumn{1}{c}{\\multirow{2}{*}{Cond.}} & \\multicolumn{3}{c}{\\multicolumn{3}{c}{Subsequence}} & \\multicolumn{5}{c}{Transition} \\\\  & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} \\\\ \\hline GT & - & - & \\(0.715^{\\pm 0.003}\\) & \\(0.00^{\\pm 0.00}\\) & \\(8.42^{\\pm 0.15}\\) & \\(3.36^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) \\\\ \\hline PCCAT & A & A & \\(0.699^{\\pm 0.004}\\) & \\(1.34^{\\pm 0.04}\\) & \\(\\textbf{8.36}^{\\pm 0.12}\\) & \\(3.40^{\\pm 0.02}\\) & \\(4.\n' +
      '\n' +
      'scores of the APE models. Fig. 5 illustrates this balance. Specifically, increasing the number of APE steps enhances the motion\'s congruence with the textual description, at the cost of reducing the smoothness and realism of the transitions. In HumanML3D, the SAT and CAT conditioning schemes lead to worse transitions in terms of FID and diversity. This is caused by the coexistence of different conditions in the local neighborhood of the transition at inference, which never happens during training. Our PCCAT conditioning technique effectively solves this problem. In Babel, such effect is not present because the training motion sequences include several subsequences, thus increasing the model\'s robustness to transitions with varying conditions.\n' +
      '\n' +
      '**On the efficiency of FlowMDM.** Diffusion-based state-of-the-art methods such as MultiDiffusion and DiffCollage denoise poses from the transition more than once in order to harmonize it with the adjacent motions. DoubleTake\'s transitions undergo an additional denoising process, which adds computational burden and can not be parallelized. Oppositely, FlowMDM does not apply redundant denoising steps to any pose. In particular, our model goes through 47.1%, 28.4%, and 16.5% less pose-wise denoising steps than DoubleTake, DiffCollage, and MultiDiffusion, respectively.\n' +
      '\n' +
      '### Qualitative results\n' +
      '\n' +
      'Fig. 6 illustrates how our quantitative findings translate into visual outcomes on the human motion composition and extrapolation tasks. First, as anticipated by Fig. 4, we confirm that state-of-the-art methods produce short intervals of jerk peaks around transitions. These do not typically match long-range motion scenarios, where such jerks might be contextually appropriate. Contrarily, FlowMDM produces motion that is realistic, accurate, and smooth. Particularly, we notice that DiffCollage\'s bias toward producing constantly high jerk values around transitions is perceived as an overall chaotic motion. Due to the independent generation of their subsequences, DoubleTake, DiffCollage, and MultiDiffusion are unable to maintain the static or periodic nature of actions when extrapolating them. Only TEACH and FlowMDM are able to successfully extrapolate a static \'t-pose\', and ours is the only one capable of extrapolating a\'step to the right\' sequence realistically. Finally, FlowMDM also inherits the trajectory control capabilities of motion diffusion models as shown in Fig. 1-right.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We presented FlowMDM, the first approach that generates human motion compositions simultaneously, without undergoing postprocessing or redundant denoising diffusion steps. We also introduced the blended positional encodings to combine the benefits of absolute and relative positional encodings during the denoising chain. Finally, we presented the pose-centric cross-attention, a technique that improves the generation of transitions when training with only a single condition per motion sequence.\n' +
      '\n' +
      '**Limitations and future work.** The absolute stage of BPE does not model relationships between subsequences.\n' +
      '\n' +
      'Figure 6: **Qualitative analysis (Babel).** A) and B) show compositions of 3 motions (‘walk straight’\\(\\,\\rightarrow\\,\\)‘side steps’\\(\\,\\rightarrow\\,\\)‘walk backward’, and ‘walk’\\(\\,\\rightarrow\\,\\)‘turn around’\\(\\,\\rightarrow\\,\\)‘sit on the bench’, respectively), and C) and D) illustrate extrapolations that repeat 6 times a static (‘t-pose’) and a dynamic (‘step to the right’) action, respectively. Solid curves match the trajectories of the global position (blue) and left/right hands (purple/green). Darker colors indicate instantaneous jerk deviations from the median value, saturating at twice the jerk’s standard deviation in the dataset (black segments). Abrupt transitions manifest as black segments amidst lighter ones. FlowMDM exhibits the most fluid motion and preserves the staticity or periodicity of extrapolated actions, in contrast to other methods that show spontaneous high jerk values and fail to keep the motion coherence in extrapolations.\n' +
      '\n' +
      'Consequently, their low-frequency spectrum is generated independently. This limitation could be addressed in future work by incorporating an intention planning module. Finally, our method learns a strong motion prior that generates transitions between combinations of actions never seen at training time. Such capability could theoretically be used with different models leveraging different control signals, assuming they all are trained under the same framework. Future work will experimentally validate this hypothesis.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Vida Adeli, Mahsa Ehsanpour, Ian Reid, Juan Carlos Niebles, Silvio Savarese, Ehsan Adeli, and Hamid Rezatofighi. Tripod: Human trajectory and pose dynamics forecasting in the wild. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13390-13400, 2021.\n' +
      '* [2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. _ACM Transactions on Graphics (TOG)_, 42(4):1-20, 2023.\n' +
      '* [3] Sadegh Aliakbarian, Microsoft Fatemeh Saleh ACRV, Stephen Gould ACRV, and Anu Mathieu Salzmann CVLab. Contextually plausible and diverse 3d human motion prediction. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [4] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and Gul Varol. Teach: Temporal action composition for 3d humans. In _2022 International Conference on 3D Vision (3DV)_, pages 414-423. IEEE, 2022.\n' +
      '* [5] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, and Etienne Burdet. A robust and sensitive metric for quantifying movement smoothness. _IEEE transactions on biomedical engineering_, 59(8):2126-2136, 2011.\n' +
      '* [6] Sivakumar Balasubramanian, Alejandro Melendez-Calderon, Agnes Roby-Brami, and Etienne Burdet. On the analysis of movement smoothness. _Journal of neuroengineering and rehabilitation_, 12(1):1-11, 2015.\n' +
      '* [7] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. _arXiv preprint arXiv:2302.08113_, 2023.\n' +
      '* [8] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for behavior-driven human motion prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2317-2327, 2023.\n' +
      '* [9] German Barquero, Johnny Nunez, Sergio Escalera, Zhen Xu, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero. Didn\'t see that coming: a survey on non-verbal social human behavior forecasting. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 139-178. PMLR, 2022.\n' +
      '* [10] German Barquero, Johnny Nunez, Zhen Xu, Sergio Escalera, Wei-Wei Tu, Isabelle Guyon, and Cristina Palmero. Comparison of spatio-temporal models for human motion and pose forecasting in face-to-face interaction scenarios. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 107-138. PMLR, 2022.\n' +
      '* [11] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.\n' +
      '* [12] Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. Behave: Dataset and method for tracking human object interactions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15935-15946, 2022.\n' +
      '* [13] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14_, pages 561-578. Springer, 2016.\n' +
      '* [14] Paulo Vinicius Koerich Borges, Nicola Conci, and Andrea Cavallaro. Video-based human behavior understanding: A survey. _IEEE transactions on circuits and systems for video technology_, 23(11):1993-2008, 2013.\n' +
      '* [15] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion prediction with scene context. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16_, pages 387-404. Springer, 2020.\n' +
      '* [16] Angela Castillo, Maria Escobar, Guillaume Jeanneret, Albert Pumarola, Pablo Arbelaez, Ali Thabet, and Artsiom Sanakoyeu. Bodiffusion: Diffusing sparse observations for full-body human motion synthesis. _arXiv preprint arXiv:2304.11118_, 2023.\n' +
      '* [17] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster: choreography-oriented music-driven dance synthesis. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [18] Enric Corona, Albert Pumarola, Guillem Alenya, and Francesc Moreno-Noguer. Context-aware human motion prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6992-7001, 2020.\n' +
      '* [19] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. _IEEE signal processing magazine_, 35(1):53-65, 2018.\n' +
      '* [20] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.\n' +
      '* [21] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9760-9770, 2023.\n' +
      '* [22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_, 2023.\n' +
      '** [23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.\n' +
      '* [24] David A Engstrom, JA Scott Kelso, and Tom Holroyd. Reaction-anticipation transitions in human perception-action patterns. _Human movement science_, 15(6):809-832, 1996.\n' +
      '* [25] Philipp Gulde and Joachim Hermsdorfer. Smoothness metrics in complex movement tasks. _Frontiers in neurology_, 9:615, 2018.\n' +
      '* [26] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5152-5161, 2022.\n' +
      '* [27] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In _European Conference on Computer Vision_, pages 580-597. Springer, 2022.\n' +
      '* [28] Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Multi-person extreme motion prediction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13053-13064, 2022.\n' +
      '* [29] Felix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion in-betweening. _ACM Transactions on Graphics (TOG)_, 39(4):60-1, 2020.\n' +
      '* [30] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 11374-11384, 2021.\n' +
      '* [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.\n' +
      '* [33] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [34] Neville Hogan and Dagmar Sternad. Sensitivity of smoothness measures to movement duration, amplitude, and arrests. _Journal of motor behavior_, 41(6):529-534, 2009.\n' +
      '* [35] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a foreign language. _arXiv preprint arXiv:2306.14795_, 2023.\n' +
      '* [36] Manuel Kaufmann, Emre Aksan, Jie Song, Fabrizio Pece, Remo Ziegler, and Otmar Hilliges. Convolutional autoencoders for human motion infilling. In _2020 International Conference on 3D Vision (3DV)_, pages 918-927. IEEE, 2020.\n' +
      '* [37] Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. _arXiv preprint arXiv:2305.19466_, 2023.\n' +
      '* [38] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.\n' +
      '* [39] Jihoon Kim, Taehyun Byun, Seungyoun Shin, Jungdarn Won, and Sungjoon Choi. Conditional motion in-betweening. _Pattern Recognition_, 132:108894, 2022.\n' +
      '* [40] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis & editing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 8255-8263, 2023.\n' +
      '* [41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.\n' +
      '* [42] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty: Neural object interaction fields for guided human motion synthesis. _arXiv preprint arXiv:2307.07511_, 2023.\n' +
      '* [43] Wilfried Kunde, Katrin Elsner, and Andrea Kiesel. No anticipation-no action: the role of anticipation in action and perception. _Cognitive Processing_, 8:71-78, 2007.\n' +
      '* [44] Caroline Larboulete and Sylvie Gibet. A review of computable expressive descriptors of human motion. In _Proceedings of the 2nd International Workshop on Movement and Computing_, pages 21-28, 2015.\n' +
      '* [45] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Multiact: Long-term 3d human motion generation from multiple action labels. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 1231-1239, 2023.\n' +
      '* [46] Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang, Hao Li, and Yajie Zhao. Task-generic hierarchical human motion prior using vaes. In _2021 International Conference on 3D Vision (3DV)_, pages 771-781. IEEE, 2021.\n' +
      '* [47] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13401-13412, 2021.\n' +
      '* [48] Shuai Li, Sisi Zhuang, Wenfeng Song, Xinyu Zhang, Hejia Chen, and Aimin Hao. Sequential texts driven cohesive motions synthesis with natural transitions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9498-9508, 2023.\n' +
      '* [49] Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen. Example-based motion synthesis via generative motion matching. _arXiv preprint arXiv:2306.00378_, 2023.\n' +
      '* [50] Yunhao Li, Zhenbo Yu, Yucheng Zhu, Bingbing Ni, Guangdao Zhai, and Wei Shen. Skeleton2humanoid: Animating simulated characters for physically-plausible motion in-betweening. In _Proceedings of the 30th ACM International Conference on Multimedia_, pages 1493-1502, 2022.\n' +
      '* [51] Jing Lin, Ailing Zeng, Shuulin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: A large-scale 3d expressive whole-body human motion dataset. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.\n' +
      '* [52] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10895-10904, 2023.\n' +
      '* [53] Hengbo Ma, Jiachen Li, Rantin Hosseini, Masayoshi Tomizuka, and Chiho Choi. Multi-objective diverse human motion prediction with knowledge distillation. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* [54] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel codec avatars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 64-73, 2021.\n' +
      '* [55] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit. Evaluating the quality of a synthesized motion with the frechet motion distance. In _ACM SIGGRAPH 2022 Posters_, pages 1-2, 2022.\n' +
      '* [56] Antoine Maiorca, Youngwoo Yoon, and Thierry Dutoit. Validating objective evaluation metric: Is frechet motion distance able to capture foot skating artifacts? In _Proceedings of the 2023 ACM International Conference on Interactive Media Experiences_, pages 242-247, 2023.\n' +
      '* [57] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Generating smooth pose sequences for diverse human motion prediction. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.\n' +
      '* [58] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.\n' +
      '* [59] Boris N Oreshkin, Antonios Valkanas, Felix G Harvey, Louis-Simon Menard, Florent Bocquelet, and Mark J Coates. Motion in-betweening via deep delta-interpolator. _IEEE Transactions on Visualization and Computer Graphics_, 2023.\n' +
      '* [60] Cristina Palmero, German Barquero, Julio CS Jacques Junior, Albert Clapes, Johnny Nunez, David Curto, Sorina Smeureanu, Javier Selva, Zejian Zhang, David Saeteros, et al. Chalearn lap challenges on self-reported personality recognition and non-verbal behavior forecasting during social dyadic interactions: Dataset, design, and results. In _Understanding Social Behavior in Dyadic and Small Group Interactions_, pages 4-52. PMLR, 2022.\n' +
      '* [61] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. Fast vision transformers with hide attention. _Advances in Neural Information Processing Systems_, 35:14541-14554, 2022.\n' +
      '* [62] Sang-Min Park and Young-Gab Kim. A metaverse: Taxonomy, components, applications, and open challenges. _IEEE access_, 10:4209-4251, 2022.\n' +
      '* [63] Mathis Petrovich, Michael J Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In _European Conference on Computer Vision_, pages 480-497. Springer, 2022.\n' +
      '* [64] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. _Big data_, 4(4):236-252, 2016.\n' +
      '* [65] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J Black. Babel: Bodies, action and behavior with english labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 722-731, 2021.\n' +
      '* [66] Yijun Qian, Jack Urbanek, Alexander G Hauptmann, and Jungdam Won. Breaking the limits of text-conditioned 3d motion synthesis with elaborative descriptions. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2306-2316, 2023.\n' +
      '* [67] Jia Qin, Youyi Zheng, and Kun Zhou. Motion in-betweening via two-stage transformers. _ACM Transactions on Graphics (TOG)_, 41(6):1-16, 2022.\n' +
      '* [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.\n' +
      '* [69] Tianxiang Ren, Jubo Yu, Shihui Guo, Ying Ma, Yutao Ouyang, Zijiao Zeng, Yazhan Zhang, and Yipeng Qin. Diverse motion in-betweening with dual posture stitching. _arXiv preprint arXiv:2303.14457_, 2023.\n' +
      '* [70] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.\n' +
      '* [71] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. _Advances in neural information processing systems_, 31, 2018.\n' +
      '* [72] Tim Salzmann, Marco Pavone, and Markus Ryll. Motron: Multimodal probabilistic human motion forecasting. _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.\n' +
      '* [73] Yonatan Shafir, Guy Tveet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. _arXiv preprint arXiv:2303.01418_, 2023.\n' +
      '* [74] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.\n' +
      '* [75] Paul Starke, Sebastian Starke, Taku Komura, and Frank Steinicke. Motion in-betweening with phase manifolds. _Proceedings of the ACM on Computer Graphics and Interactive Techniques_, 6(3):1-17, 2023.\n' +
      '* [76] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.\n' +
      '* [77] Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan S Kankankanhalli, Weidong Geng, and Xiangdong Li. Deep-dance: music-to-dance motion choreography with adversarial learning. _IEEE Transactions on Multimedia_, 23:497-509, 2020.\n' +
      '* [78] Jiarui Sun and Girish Chowdhary. Towards globally consistent stochastic human motion prediction via motion diffusion. _arXiv preprint arXiv:2305.12554_, 2023.\n' +
      '* [79] Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati, and Nicolai Marquardt. Augmented reality and robotics: A survey and taxonomy for ar-enhanced human-robot interaction and robotic interfaces. In _Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems_, pages 1-33, 2022.\n' +
      '* [80] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng Tang, Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall, and Cem Keskin. Social diffusion: Long-term multiple human motion anticipation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9601-9611, 2023.\n' +
      '* [81] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In _European Conference on Computer Vision_, pages 358-374. Springer, 2022.\n' +
      '* [82] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [83] Sibo Tian, Minghui Zheng, and Xiao Liang. Transfusion: A practical and effective transformer-based diffusion model for 3d human motion prediction. _arXiv preprint arXiv:2307.16106_, 2023.\n' +
      '* [84] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 448-458, 2023.\n' +
      '* [85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.\n' +
      '* [86] Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video forecasting by generating pose futures. _Proceedings of the IEEE international conference on computer vision_, 2017.\n' +
      '* [87] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20460-20469, 2022.\n' +
      '* [88] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9401-9411, 2021.\n' +
      '* [89] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-aware generative network for human motion synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12206-12215, 2021.\n' +
      '* [90] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In _International Conference on Learning Representations (ICLR)_, 2022.\n' +
      '* [91] Jiachen Xu, Min Wang, Jingyu Gong, Wentao Liu, Chen Qian, Yuan Xie, and Lizhuang Ma. Exploring versatile prior for human motion via motion frequency guidance. In _2021 International Conference on 3D Vision (3DV)_, pages 606-616. IEEE, 2021.\n' +
      '* [92] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14928-14940, 2023.\n' +
      '* [93] Sirui Xu, Yu-Xiong Wang, and Liangyan Gui. Stochastic multi-person 3d motion forecasting. In _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [94] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. _ACM Computing Surveys_, 2022.\n' +
      '* [95] Zhao Yang, Bing Su, and Ji-Rong Wen. Synthesizing long-term human motions with diffusion models via coherent sampling. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 3954-3964, 2023.\n' +
      '* [96] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang. Choreonet: Towards music to dance synthesis with choreographic action unit. In _Proceedings of the 28th ACM International Conference on Multimedia_, pages 744-752, 2020.\n' +
      '* [97] Hongwei Yi, Chun-Hao P Huang, Shashank Tripathi, Lea Hering, Justus Thies, and Michael J Black. Mime: Human-aware 3d scene generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12965-12976, 2023.\n' +
      '* [98] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time 3d human translation and pose estimation with six inertial sensors. _ACM Transactions on Graphics (TOG)_, 40(4):1-13, 2021.\n' +
      '* [99] Ye Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pages 346-364. Springer, 2020.\n' +
      '* [100] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16010-16021, 2023.\n' +
      '* [101] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14730-14740, 2023.\n' +
      '* [102] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. _arXiv preprint arXiv:2208.15001_, 2022.\n' +
      '* [103] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. DiffCollage: Parallel generation of large content with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10188-10198, 2023.\n' +
      '* [104] Yan Zhang, Michael J Black, and Siyu Tang. Perpetual motion: Generating unbounded human motion. _arXiv preprint _arXiv:2007.13886_, 2020.\n' +
      '* [105] Yan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20481-20491, 2022.\n' +
      '* [106] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5745-5753, 2019.\n' +
      '* [107] Yi Zhou, Zimo Li, Shuangjiu Xiao, Chong He, Zeng Huang, and Hao Li. Auto-conditioned recurrent networks for extended complex human motion synthesis. In _International Conference on Learning Representations_, 2018.\n' +
      '* [108] Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, et al. Generative tweening: Long-term inbetweening of 3d human motions. _arXiv preprint arXiv:2005.08891_, 2020.\n' +
      '* [109] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. _arXiv preprint arXiv:2307.10894_, 2023.\n' +
      '* [110] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet for music-driven dance generation. _ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)_, 18(2):1-21, 2022.\n' +
      '\n' +
      '## Supplementary Material\n' +
      '\n' +
      '### A Further implementation details\n' +
      '\n' +
      'All values are reported as X/Y for Babel/HumanML3D, or as Z if values are equal for both. Note that motion sequences are downsampled to 30/20 fps.\n' +
      '\n' +
      '**State-of-the-art models.** TEACH is used off-the-shelf 1 with the originally proposed alignment and spherical linear interpolation, and without them (TEACH_B). DoubleTake is used off-the-shelf 2 from their original repository, with the parameters _handshake size_ and _blending length_ set to 10/20f (frames), and 10/5f, respectively. To fulfill the constraints of their method, the handshake size needs to be shorter than half the shortest sequence we want to generate, which is 30f (1s) for Babel. Since DoubleTake uses the original Motion Diffusion Model [82], whose training discarded very short sequences, it underperforms in our more comprehensive evaluation protocol (see Sec. B). For a fairer comparison, we also evaluate it using our diffusion model with absolute positional encodings (APE), and call it DoubleTake*. DoubleTake* uses the same handshake size and blending length as DoubleTake. DiffCollage and MultiDiffusion were implemented manually, and utilize our model as well for the same reasons mentioned earlier. We set their sampling parameter _transition length_ to 10/20f. For DoubleTake, DiffCollage, and MultiDiffusion, we use classifier-free guidance with weights 1.5/2.5 during sampling.\n' +
      '\n' +
      'Footnote 1: [https://github.com/athn-nik/teach/commit/f4285affOfd556a5b46518a751fc90825d91e68b](https://github.com/athn-nik/teach/commit/f4285affOfd556a5b46518a751fc90825d91e68b)\n' +
      '\n' +
      '**FlowMDM.** Our diffusion model uses 1k steps and a cosine noise schedule [58]. FlowMDM is trained with the \\(x_{0}\\) parameterization [90], and an L2 reconstruction loss. Denoising timesteps are encoded as a sinusoidal positional encoding that goes through two dense layers into a 512D vector. Textual descriptions are tokenized and embedded with CLIP [68] into 512D vectors. Poses of 135/263D are encoded by a dense layer into a sequence of 512D vectors. If the APE is active, a sinusoidal encoding is added to the embedded poses at this stage. Then, the embedded poses are taken as the _keys_ and _values_ of a Transformer. Embedded poses are concatenated to the sum of the timesteps and text embeddings, and fed to a dense layer. The resulting 512D vectors are the _queries_. If the relative positional encoding (RPE) is active, rotary embeddings [76] are injected to the queries and keys at this stage. The output of the Transformer is added to the embedded poses with a residual connection. 8 Transformers are stacked together. A final dense layer converts the pose embeddings back to a vector of 135/263D, which are the denoised poses. A dropout of 0.1 is applied to the APE, and to the inputs of the Transformers. The attention span of the Transformers is capped within each subsequence during the APE stage, and within the attention horizon H=100/150f during the RPE stage. We train with blended positional encodings (BPE), i.e., RPE and APE are alternated randomly at a frequency of 0.5. We use Adam [41] with learning rate of 0.0001 as our optimizer, and train for 1.3M/500k steps in a single RTX 3090 (about 4/2 days). During BPE sampling, the binary step schedule transitions from absolute to relative mode after 125/60 denoising steps (out of 1k steps). Classifier-free guidance with weights 1.5/2.5 is used during sampling.\n' +
      '\n' +
      '### B Evaluation details\n' +
      '\n' +
      'Generative models are difficult to evaluate and compare due to the limitations of the metrics (discussed in Sec. 4.1) and the stochasticity present during sampling. To alleviate the latter, we run all our evaluation 10 times and provide the 95% confidence intervals. However, we still face another issue in our task: the randomness in the combinations of textual descriptions. The generation difficulty for the combination\'sit down\'\\(\\rightarrow\\)\'stand up\'\\(\\rightarrow\\)\'run\' is not the same as for\'sit down\'\\(\\rightarrow\\)\'run\'\\(\\rightarrow\\)\'stand up\'. The evaluation protocol from [73] includes 32 evaluation sequences of 32 randomly sampled textual descriptions from the test set. The generated motion needs to perform sequentially the 32 actions from each evaluation sequence. However, these descriptions are sampled differently in each evaluation run, which hinders reproducibility. In order to ensure proper replication and a fair comparison in future works, we propose a more thorough and fully reproducible evaluation protocol that enables a more fine-grained analysis based on _scenarios_ (analysis provided in Sec. C.1):\n' +
      '\n' +
      '**Babel.** We built two scenarios with in-distribution (50%) and out-of-distribution (50%) combinations. For the in-distribution scenario, we first selected test motion sequences showcasing at least three consecutive actions (i.e., textual descriptions) with a total duration of 1.5s. Then, we randomly sampled from them to build 32 sets of 32 combinations of textual descriptions. For the out-of-distribution scenario, 32 sets were built by autoregressively sampling 32 textual descriptions so that consecutive actions did not appear together neither in the training nor in the test set.\n' +
      '\n' +
      '**HumanML3D.** Since annotations in HumanML3D do not include consecutive actions, we cannot build in- and out-of-distribution scenarios. However, this dataset contains a great variability of sequence lengths (3-10s). Therefore, we decided to build four scenarios by varying the length of the subsequences included. More specifically, we created three sets of 6, 8, and 18 combinations (9.4, 12.5, 28.1%) by sampling 32 short (3-5s), medium (5-8s), and long (8-10s) test motions, respectively. Ratios were set so that all together preserved the proportion of short, medium, and long subsequences in the original test set. This is important to keep the validity of statistical measures like FID. Additionally, we included another scenario with 32 sets (50%) of 32 random motion sequences from the test set.\n' +
      '\n' +
      'We share the list of evaluation combinations for both the human motion composition and extrapolation tasks in our public code repository3. Note that a combination consists of a list of textual descriptions and their associated durations. The 32 textual descriptions used for the extrapolation experiments from Sec. 4 are enumerated in Tab. A.\n' +
      '\n' +
      'Footnote 3: [https://barquerogerman.github.io/FlowMDM/](https://barquerogerman.github.io/FlowMDM/)\n' +
      '\n' +
      '## Appendix C More experimental results\n' +
      '\n' +
      '### Fine-grained comparison\n' +
      '\n' +
      'Tab. B shows the comparison of FlowMDM with the state of the art in both in-distribution and out-of-distribution scenarios. We observe that, while all methods maintain similar performance in both scenarios for the subsequence generation, they generate less realistic and more abrupt transitions in the out-of-distribution case. FlowMDM performs the best at most metrics in both scenarios, with an important gap with respect to the previous state of the art regarding transition smoothness. Tab. C shows the scenario-wise results for HumanML3D, where FlowMDM also performs the best in most metrics and scenarios. Interestingly, MultiDiffusion is, after ours, the most stable method in terms of transition smoothness across scenarios (PJ and AUJ), whereas DiffCollage and DoubleTake show severe transition degeneration in combinations of long sequences. Such degeneration is mostly due to their methodological need to pad the motion sequence during sampling. When dealing with long sequences, sequences might be extended beyond the maximum sequence length at training time. Therefore, given that the APE does not extrapolate well, the generation in the padded motion, or transition, tends to degenerate. Our method naturally avoids this limitation.\n' +
      '\n' +
      '### On the attention horizon\n' +
      '\n' +
      'In Tabs. D and E, we show the effect of the attention horizon when using RPE for either a purely relative inference schedule, or our proposed BPE inference schedule. We observe how increasing it too much (H=200) makes the network perform worse at transition generation in both datasets (FID and AUJ), and also in subsequence generation for HumanML3D (R-prec and MM-Dist). Conversely, when decreasing it too much (H=50), the capacity to model long-range dynamics becomes limited, thus reducing the accuracy of the generated subsequences (R-prec and MM-Dist). As the performance with H of 100 and 150 is similar in both datasets, we chose values that are closest to the average sequence length in each dataset, i.e., 100/150f for Ba-bel/HumanML3D.\n' +
      '\n' +
      '### On the diffusion schedule\n' +
      '\n' +
      'The discussion and the BPE design in Sec. 3.2 are motivated by the low-to-high frequencies decomposition during the denoising stage of diffusion models. However, the denoising process depends on how the noise is injected, or the _noise schedule_. The linear and the cosine (our choice) noise schedules are the most common schedules. The linear schedule destroys the motion very fast, reaching a non-recognizable state after going through the 75% of the diffusion steps [58]. Instead, the cosine schedule destroys the motion signal slower and in a more evenly distributed way. Fig. A shows the performance of FlowMDM during BPE sampling with both schedules. First, we observe that FlowMDM benefits from the steadier noise injection of the cosine schedule, achieving better performance in all realism and accuracy metrics (R-prec and FID). Second, we identify a displacement in the accuracy (R-prec) and smoothness (AUJ) curves (see black arrows). Given that with the linear schedule global dependencies start being recovered later, more APE steps are needed to achieve the accuracy\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline \\multicolumn{1}{l}{Babel} & HumanML3D \\\\ \\hline walk forward & a person walks in a curved path to the left. \\\\ swim movement & a person stands still and does not move. \\\\ stretch arms & a person walks straight forward. \\\\ walk & a person does jumping jacks. \\\\ stand & a person start to dance with legs. \\\\ step backwards & person walking in an s shape. \\\\ t-pose & a person walks to his right. \\\\ throw the ball & a person slowly walked forward. \\\\ run & the person is standing still doing body stretches. \\\\ circle right arm backwards & the person is dancing the waltz. \\\\ wave right & the person is clapping. \\\\ ginga dance & walking side to side. \\\\ forward kick & a person stayed on the place. \\\\ look around & person is jogging in place. \\\\ steps to the right & a person walks backward for 3 steps. \\\\ side steps & person is running in a circle. \\\\ hop forward & the person is waving hi. \\\\ dance with arms & a person walks in a circular path. \\\\ jog & swinging arms up and down. \\\\ walk slowly & a man walks counterclockwise in a circle. \\\\ jump jacks series & the person is walking towards the left. \\\\ run in half a circle & the person is walking on the treadmill. \\\\ walk a few steps ahead & the man is moving his left arm. \\\\ move head up and down & the person is doing basketball signals. \\\\ rotate right ankle & a person remained sitting down. \\\\ play guitar & a person hits his drums. \\\\ jump forward & person is doing a dance. \\\\ move both hands around chest & a person takes some steps forward. \\\\ swing back and forth & a person slowly walks forward five steps. \\\\ wave & a person jumps in place. \\\\ shake it & this person appears to be painting. \\\\ walk in circle & a person wiping a surface with something. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table A: Extrapolated motions for Babel and HumanML3D.\n' +
      '\n' +
      'and smoothness reached with the cosine schedule.\n' +
      '\n' +
      '### On the classifier-free guidance\n' +
      '\n' +
      'The classifier-free guidance is an important add-on for diffusion sampling that intensifies the conditioning signal, thus improving the quality and accuracy of the generated samples [33]. It is implemented by first computing the conditionally denoised motion \\(x_{c}\\), and the unconditionally denoised motion \\(x\\). Then, the denoised sample is computed as \\(x+w(x_{c}-x)\\). If \\(w{=}1\\), the classifier-free guidance is deactivated. When generating motion from single textual descriptions with classifier-free guidance, we keep steering the denoising toward motions matching better the textual description. However, when building human motion\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c c c c} \\hline \\hline  & \\multicolumn{6}{c}{Subsequence} & \\multicolumn{6}{c}{Transition} \\\\  & R-prec \\(\\uparrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & MM-Dist \\(\\downarrow\\) & FID \\(\\downarrow\\) & Div \\(\\rightarrow\\) & PJ \\(\\rightarrow\\) & AUJ \\(\\downarrow\\) \\\\ \\hline GT & \\(0.715^{\\pm 0.003}\\) & \\(0.00^{\\pm 0.00}\\) & \\(8.42^{\\pm 0.15}\\) & \\(3.36^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) & \\(6.20^{\\pm 0.06}\\) & \\(0.02^{\\pm 0.00}\\) & \\(0.00^{\\pm 0.00}\\) \\\\ \\hline In-distribution & & & & & & & \\\\ \\hline TEACH\\_B & \\(\\mathbf{0.727}^{\\pm 0.004}\\) & \\(2.26^{\\pm 0.03}\\) & \\(8.20^{\\pm 0.12}\\) & \\(\\mathbf{3.35}^{\\pm 0.01}\\) & \\(2.77^{\\pm 0.05}\\) & \\(6.32^{\\pm 0.07}\\) & \\(1.03^{\\pm 0.00}\\) & \\(2.20^{\\pm 0.01}\\) \\\\ TEACH & \\(0.665^{+0.003}\\) & \\(2.09^{\\pm 0.03}\\) & \\(8.06^{\\pm 0.09}\\) & \\(3.73^{\\pm 0.02}\\) & \\(2.78^{\\pm 0.06}\\) & \\(6.31^{\\pm 0.07}\\) & \\(0.07^{\\pm 0.00}\\) & \\(0.42^{\\pm 0.01}\\) \\\\ DoubleTake* & \\(0.620^{\\pm 0.006}\\) & \\(3.04^{\\pm 0.06}\\) & \\(7.49^{\\pm 0.07}\\) & \\(4.19^{\\pm 0.02}\\) & \\(3.04^{\\pm 0.12}\\) & \\(6.21^{\\pm 0.06}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.01^{\\pm 0.01}\\) \\\\ DoubleTake & \\(0.682^{\\pm 0.008}\\) & \\(1.52^{\\pm 0.03}\\) & \\(7.90^{\\pm 0.07}\\) & \\(3.67^{\\pm 0.04}\\) & \\(3.47^{\\pm 0.08}\\) & \\(6.16^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.62^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(0.724^{\\pm 0.008}\\) & \\(2.00^{\\pm 0.05}\\) & \\(8.36^{\\pm 0.10}\\) & \\(3.38^{\\pm 0.02}\\) & \\(6.33^{\\pm 0.13}\\) & \\(5.91^{\\pm 0.06}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.65^{\\pm 0.01}\\) \\\\ DiffCollage & \\(0.690^{\\pm 0.006}\\) & \\(1.92^{\\pm 0.07}\\) & \\(7.92^{\\pm 0.09}\\) & \\(3.67^{\\pm 0.02}\\) & \\(4.25^{\\pm 0.15}\\) & \\(\\mathbf{6.19}^{\\pm 0.07}\\) & \\(0.19^{\\pm 0.01}\\) & \\(0.82^{\\pm 0.02}\\) \\\\ FlowMDM (Ours) & \\(0.726^{\\pm 0.006}\\) & \\(\\mathbf{1.36}^{\\pm 0.05}\\) & \\(\\mathbf{8.47}^{\\pm 0.10}\\) & \\(3.40^{\\pm 0.03}\\) & \\(\\mathbf{2.26}^{\\pm 0.08}\\) & \\(6.60^{\\pm 0.08}\\) & \\(\\mathbf{0.05}^{\\pm 0.00}\\) & \\(\\mathbf{0.11}^{\\pm 0.00}\\) \\\\ \\hline Out-of-distribution & & & & & & & & \\\\ \\hline TEACH\\_B & \\(0.680^{\\pm 0.006}\\) & \\(1.75^{\\pm 0.04}\\) & \\(8.15^{\\pm 0.11}\\) & \\(3.51^{\\pm 0.01}\\) & \\(3.53^{\\pm 0.06}\\) & \\(6.04^{\\pm 0.10}\\) & \\(1.14^{\\pm 0.01}\\) & \\(2.49^{\\pm 0.01}\\) \\\\ TEACH & \\(0.644^{\\pm 0.004}\\) & \\(2.06^{\\pm 0.03}\\) & \\(7.94^{\\pm 0.12}\\) & \\(3.70^{\\pm 0.01}\\) & \\(4.08^{\\pm 0.08}\\) & \\(6.00^{\\pm 0.09}\\) & \\(\\mathbf{0.07}^{\\pm 0.00}\\) & \\(0.46^{\\pm 0.00}\\) \\\\ DoubleTake* & \\(0.572^{\\pm 0.007}\\) & \\(3.78^{\\pm 0.07}\\) & \\(7.53^{\\pm 0.12}\\) & \\(4.15^{\\pm 0.02}\\) & \\(3.83^{\\pm 0.09}\\) & \\(\\mathbf{6.12}^{\\pm 0.07}\\) & \\(0.28^{\\pm 0.00}\\) & \\(1.07^{\\pm 0.02}\\) \\\\ DoubleTake & \\(0.654^{\\pm 0.009}\\) & \\(1.65^{\\pm 0.07}\\) & \\(8.06^{\\pm 0.08}\\) & \\(3.66^{\\pm 0.02}\\) & \\(\\mathbf{2.98}^{\\pm 0.06}\\) & \\(6.03^{\\pm 0.07}\\) & \\(0.17^{\\pm 0.00}\\) & \\(0.66^{\\pm 0.01}\\) \\\\ MultiDiffusion & \\(\\mathbf{0.681}^{\\pm 0.009}\\) & \\(2.11^{\\pm 0.06}\\) & \\(\\mathbf{8.35}^{\\pm 0.08}\\) & \\(\\mathbf{3.47}^{\\pm 0.03}\\) & \\(6.97^{\\pm 0.12}\\) & \\(5.67^{\\pm 0.05}\\) & \\(0.19^{\\pm 0.00}\\) & \\(0.71^{\\pm 0.01}\\) \\\\ DiffCollage & \\(0.652^{\\pm 0.004}\\) & \\(1.60^{\\pm 0.07}\\) & \\(7.91^{\\pm 0.09}\\) & \\(3.74^{\\pm 0.01}\\) & \\(4.65^{\\pm 0.19}\\) & \\(6.00^{\\pm 0.09}\\) & \\(0.20^{\\pm 0.00}\\) & \\(0.86^{\\pm 0.01}\\) \\\\ FlowMDM (Ours) & \\(0.679^{\\pm 0.004}\\) & \\(\\mathbf{1.26}^{\\pm 0.06}\\) & \\(8.16^{\\pm 0.08}\\) & \\(3.50^{\\pm 0.03}\\) & \\(3.17^{\\pm 0.12}\\) & \\(6.44^{\\pm 0.09}\\) & \\(\\mathbf{0.07}^{\\pm 0.00}\\) & \\(\\mathbf{0.17}^{\\pm 0.00}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Scenario-wise comparison in HumanML3D.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:17]\n' +
      '\n' +
      '## Appendix D Qualitative results\n' +
      '\n' +
      'Figs. C and D show six human motion compositions (A to F), and two extrapolations (G and H) for Babel and HumanML3D, respectively. The compositions are subsets of the evaluation combinations composed of 32 actions, so the beginning and end of these can contain partial transitions toward other actions. Motion videos are also included as part of the supplementary material. Note that we can represent the motions from Babel with SMPL body meshes thanks to its motion representation including the SMPL parame\n' +
      '\n' +
      'Figure C: **Qualitative examples (Babel).** A-F feature six human motion compositions, and G-H two human motion extrapolations. According to the scenarios defined in Sec. B, A, B, C belong to in-distribution combinations, and D, E, F to out-of-distribution combinations. Videos of all samples are also included as part of this supplementary material. Solid curves match the trajectories of the global position (blue) and left/right hands (purple/green). Darker colors indicate instantaneous jerk deviations from the median value, saturating at twice the jerk’s standard deviation in the dataset (black segments). Abrupt transitions manifest as black segments amidst lighter ones.\n' +
      '\n' +
      'Figure D. **Qualitative examples (HumanML3D).** A-F feature six human motion compositions, and G-H two human motion extrapolations. According to the scenarios defined in Sec. B, A, B, C are samples from the short, medium, and long scenarios, respectively, and D, E, F from the mixed scenario. Videos of all samples are also included as part of this supplementary material.\n' +
      '\n' +
      'ters [13]. For HumanML3D, we use skeletons, as its motion representation only includes the 3D coordinates of the joints.\n' +
      '\n' +
      '**Discussion.** The hands trajectories and the jerk color indicators in Figs. C and D and the videos highlight that FlowMDM generates the smoothest transitions between subsequences. Notably, state-of-the-art methods exhibit frequent smoothness artifacts (black segments) in the boundaries of their transitions. We notice that the compositions produced by TEACH lack realism due to the use of a naive spherical linear interpolation, disrupting the motion dynamics. This becomes more apparent in extrapolations G and H of both datasets, where the periodicity of the movement is clearly compromised. On the other side, DoubleTake, DiffCollage, and MultiDiffusion share two significant limitations. Firstly, they adhere to a predetermined transition length, which may not fit all situations. For example, in Babel-A, the \'picking\' actions occur very rapidly due to the insufficient length for generating a natural transition. By contrast, our approach is able to leverage more transitioning time from either transition side if needed, without artificial constraints. Secondly, the denoising process in these methods only considers a small portion of the neighboring subsequences, leading to poor performance in dynamic motion extrapolations. For example, in HumanML3D-G, they all generate erratic jumping jacks. While our method also independently generates the low-frequency motion spectrum, it effectively rectifies inconsistencies in later stages, yielding realistic and periodic motion. In the case of Babel-H, where successfully extrapolating the \'hop forward\' action requires synchronizing each subsequence with the whole neighboring motion, our model is the only one able to generate a smooth, coherent, and realistic extrapolation.\n' +
      '\n' +
      '**Limitations.** However, FlowMDM is not without its imperfections. We noticed that our method struggles with very complex descriptions, such as the first one in HumanML3D-B. Instead of executing the intricate description that includes \'walk backwards, sit, stand, and walk forward again\', it only walks backwards. Given that the partial execution of actions is also observed in other methods, we consider it a challenge associated with the broader text-to-motion task. Indeed, our model could theoretically also benefit from improved conditioning schemes such as using better text embeddings. Another acknowledged limitation of our model, discussed in Sec. 5, is the independent generation of low-frequency components. In Babel-B, for example, a slight mismatch between the sitting and standing positions is observed. Nonetheless, in contrast to DiffCollage, MultiDiffusion, and DoubleTake which also exhibit this effect, FlowMDM produces a smoother result.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>