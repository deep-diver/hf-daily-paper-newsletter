<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# _VideoAgent_: 롱폼 비디오 이해\n' +
      '\n' +
      '에이전트가 있는 대규모 언어 모델\n' +
      '\n' +
      'Xiaohan Wang\n' +
      '\n' +
      '동등한 기부금 프로젝트 페이지: [https://vxh1996.github.io/VideoAgent-Website/](https://vxh1996.github.io/VideoAgent-Website/) 스탠포드 대학교\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 장유희\n' +
      '\n' +
      '동등한 기부금 프로젝트 페이지: [https://vxh1996.github.io/VideoAgent-Website/](https://vxh1996.github.io/VideoAgent-Website/) 스탠포드 대학교\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 오르조하르\n' +
      '\n' +
      ' 스탠포드 대학교\n' +
      '\n' +
      '1\n' +
      '\n' +
      ' 세레나 영레비\n' +
      '\n' +
      ' 스탠포드 대학교\n' +
      '\n' +
      '1\n' +
      '\n' +
      '각주 1: 이메일: {xhanwang,yuhuiz,orrzohar,syyeung}@stanford.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '긴 형태의 비디오 이해는 긴 멀티모달 시퀀스에 대해 추론할 수 있는 모델을 요구하면서 컴퓨터 비전 내에서 중요한 도전을 나타낸다. 긴 형태의 비디오 이해를 위한 인간의 인지 과정에 동기 부여되어, 우리는 긴 시각적 입력을 처리하는 능력보다 상호작용적 추론과 계획을 강조한다. 본 논문에서는 대용량 언어 모델을 중심 에이전트로 사용하는 새로운 에이전트 기반 시스템인 _VideoAgent_를 소개한다. _VideoAgent_는 시각적 정보를 번역하고 검색하기 위한 도구로서 시각 언어 기반 모델을 사용하여 질문에 대한 중요한 정보를 반복적으로 식별하고 컴파일한다. 도전적인 EgoSchema 및 NExT-QA 벤치마크를 평가한 결과, _VideoAgent_는 평균 8.4 및 8.2 프레임만 사용하여 54.1% 및 71.3%의 제로 샷 정확도를 달성했다. 이러한 결과는 기존 최신 방법에 비해 제안된 방법의 우수한 효과와 효율성을 보여주며, 긴 형식의 비디오 이해도를 향상시키는데 있어 에이전트 기반 접근법의 가능성을 강조한다.\n' +
      '\n' +
      '키워드: 대용량 언어 모델 에이전트 비전 언어 기반 모델의 장기 동영상 이해\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '분부터 시간까지 다양한 긴 형식의 비디오를 이해하는 것은 컴퓨터 비전 분야에서 중요한 과제를 제기한다. 이 작업은 다중 모드 정보를 처리하고, 매우 긴 시퀀스를 처리하고, 이러한 시퀀스에 대한 추론을 효과적으로 수행할 수 있는 모델을 요구한다.\n' +
      '\n' +
      '이러한 능력을 향상시켜 이 문제를 해결하려는 수많은 시도[10, 12, 14, 27, 30, 42, 50, 53, 59]에도 불구하고 기존 모델은 세 가지 영역 모두에서 동시에 뛰어나기 위해 고군분투한다. 현재의 대규모 언어 모델(LLM)은 긴 컨텍스트[13, 48, 52, 62]를 추론하고 처리하는 데 탁월하지만 시각적 정보를 처리하는 능력이 부족하다. 반대로, 시각적 언어 모델들(VLMs)은 긴 시각적 입력들[9, 17, 21, 23, 18]을 모델링하는 데 어려움을 겪는다. VLM의 긴 컨텍스트 모델링 능력을 가능하게 하기 위한 초기 노력이 있었지만, 이러한 적응은 비디오 이해 벤치마크에서 성능이 떨어지고 긴 형식의 비디오 콘텐츠를 처리하는 데 비효율적이다[22].\n' +
      '\n' +
      '긴 형식의 비디오 전체를 모델에 직접 공급해야 합니까? 이는 인간이 긴 형태의 영상 이해 과제를 어떻게 성취하는지와는 크게 차이가 있다. 긴 비디오를 이해하는 작업을 수행할 때, 인간은 일반적으로 답변을 공식화하기 위해 다음과 같은 대화형 프로세스에 의존한다: 프로세스는 비디오의 맥락을 이해하기 위해 비디오의 빠른 개요로 시작한다. 이어서, 당면한 특정 질문에 의해 안내되고, 인간은 관련 정보를 수집하기 위해 새로운 프레임을 반복적으로 선택한다. 질문에 응답하기에 충분한 정보를 획득하면, 반복 프로세스가 종결되고, 답변이 제공된다. 이 프로세스 전반에 걸쳐, 이 반복 프로세스를 제어하는 추론 능력은 긴 시각적 입력을 직접 처리하는 능력보다 더 중요하다.\n' +
      '\n' +
      '인간이 긴 형식의 비디오를 어떻게 이해하는지에 영감을 얻어 에이전트 기반 시스템을 통해 이 과정을 시뮬레이션하는 시스템인 VideoAgent를 제시한다. 우리는 비디오 이해 과정을 상태, 행동 및 관찰의 시퀀스로 공식화하고 LLM이 이 과정을 제어하는 에이전트 역할을 한다(그림 1). 처음에, LLM은 비디오로부터 균일하게 샘플링된 프레임들의 세트를 봄으로써 비디오 컨텍스트에 익숙해진다. 각 반복 동안 LLM은 현재 정보(상태)가 질문에 답하기에 충분한지 여부를 평가하고, 그렇지 않은 경우 추가 정보가 필요한지 식별한다(액션). 이어서, CLIP[36]을 활용하여 이러한 정보(관찰)를 포함하는 새로운 프레임을 검색하고 VLM을 사용하여 이러한 새로운 프레임을 텍스트 설명으로 캡션하여 현재 상태를 업데이트한다. 이 설계는 VLM과 CLIP가 LLM이 시각적 이해와 긴 맥락 검색 기능을 가질 수 있도록 도구 도구 역할을 하는 긴 시각적 입력의 직접적인 처리에 대한 추론 능력과 반복적인 프로세스를 강조한다.\n' +
      '\n' +
      '우리의 작업은 두 가지 측면에서 이전 작업과 다르다. 단일 반복[16, 56, 66]에서 프레임을 균일하게 샘플링하거나 프레임을 선택하는 작업과 비교하여, 본 방법은 다중 라운드 방식으로 프레임을 선택하여 정보를 보장한다.\n' +
      '\n' +
      '도 1:_VideoAgent의 개요._ 긴 형식의 비디오가 주어지면 _VideoAgent_는 질문에 답하기 위해 핵심 정보를 반복적으로 검색하고 집계한다. 프로세스는 에이전트로서 큰 언어 모델(LLM)에 의해 제어되며, 시각적 언어 모델(VLM) 및 대조적 언어-이미지 모델(CLIP)이 도구 역할을 한다.\n' +
      '\n' +
      '현재의 필요에 따라 더 정확하도록 모였다. 원래의 질문을 질의로 사용하여 프레임을 검색하는 작업[56, 66]에 비해, 보다 정확하고 세밀한 프레임 검색이 가능하도록 질의를 다시 작성한다.\n' +
      '\n' +
      '잘 정립된 두 개의 롱폼 비디오 이해 벤치마크인 EgoSchema[28]와 NExT-QA[55]에 대한 우리의 엄격한 평가는 _VideoAgent_의 탁월한 효과와 기존 방법에 비해 효율성을 보여준다. ___ VideoAgent_는 이 두 벤치마크에서 각각 54.1%와 71.3%의 정확도를 달성하여 동시 최첨단 방법 LLoVi[67]를 3.8%와 3.6% 능가한다. 특히, _VideoAgent_는 LLoVi에 비해 20배 적은 이러한 성능을 달성하기 위해 평균 8.4 프레임만을 이용한다. 본 논문에서는 비디오의 복잡도를 기반으로 관련 정보를 적응적으로 검색하고 집계하는 반복 프레임 선택 과정의 중요성을 강조한다. 또한, 우리의 사례 연구는 _VideoAgent_가 1시간 이상으로 연장되는 비디오를 포함하여 임의로 긴 비디오로 일반화되는 것을 보여준다.\n' +
      '\n' +
      '요약하면, _VideoAgent_는 인간의 인지 과정을 모방하기 위해 에이전트 기반 시스템을 수용하고 긴 컨텍스트 시각적 정보 모델링보다 추론의 우선성을 강조하는 긴 형식의 비디오 이해에 중요한 보폭을 나타낸다. 우리는 우리의 작업이 긴 형식의 비디오 이해에서 새로운 벤치마크를 설정할 뿐만 아니라 이러한 방향으로 향후 연구를 조명하기를 바란다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 긴 형식의 비디오 이해\n' +
      '\n' +
      '긴 형태의 비디오 이해는 시공간 입력의 고유한 복잡성과 높은 차원성으로 인해 컴퓨터 비전에서 특히 어려운 영역이며, 이는 상당한 계산 요구로 이어진다. 긴 형태의 비디오 이해 방법은 계산 효율과 성능의 균형을 유지해야 하며, 크게 선택적 또는 압축 희소성 전략으로 분류될 수 있다.\n' +
      '\n' +
      '압축 희소성 방법[10, 12, 14, 30, 42, 50, 53, 59, 65]은 가능한 최소 차원으로 비디오를 의미 있는 임베딩/표현으로 압축하려고 시도한다. 예를 들어, MovieChat[42]는 코사인 유사도에 기초하여 유사한 인접 프레임 토큰을 병합하는 메모리 병합 메커니즘을 채용하여 긴 비디오 시퀀스에서 토큰 중복성을 효과적으로 감소시킨다. Chat-UniVi[14]는 비디오 토큰들을 시공간적으로 압축하기 위해 kNN 클러스터링을 활용하였다. 그러나, 압축은 임베딩 자체 상에서 일어날 필요는 없으며, 시공간 그래프[10, 50, 59] 또는 심지어 텍스트[19, 38, 67]로 압축될 수 있다. 예를 들어, Zhang et. [67]은 LLoVi를 도입했으며, 단순히 이전에 비디오를 캡션하고 이러한 캡션으로 LLM을 프롬프트하는 것이 강력한 베이스라인으로서 작용할 수 있음을 보여주었다.\n' +
      '\n' +
      '한편, 선택적 압축 방법론들은 입력된 질문/텍스트를 가이드로서 사용하여 비디오를 보다 의미 있는 프레임들로 서브샘플링하려고 시도하고, 사실상 당면한 질문과 관련된 프레임들만을 샘플링하려고 시도한다[7, 20, 37, 56, 66]. 예를 들어, R-VLM 및 R2A[8, 33, 56]과 같은 방법들은 텍스트 프롬프트가 주어진 관련 프레임들을 검색하기 위해 CLIP 모델을 활용하는 반면, Q-ViD[38]은 비디오를 선택적으로 캡션하기 위해 질문을 활용한다. 이전 작업과 달리, 우리는 LLM이 비디오 프레임을 캡셔너에 의해 샘플링하도록 지시할 수 있다.\n' +
      '\n' +
      '### LLM Agents\n' +
      '\n' +
      '에이전트는 어떤 특정한 목표를 달성하기 위해 역동적이고 실시간적인 환경에서 결정을 내리고 행동을 취하는 개체로 정의된다. 대형 언어 모델(LLM)의 발전, 특히 그들의 새로운 추론 및 계획 능력[52, 62, 70]은 자연 언어 처리의 최근 연구에 영감을 주어 실제 시나리오[35, 63]에서 에이전트로 활용했다. 이러한 모델은 온라인 검색, 카드 게임 플레이 및 데이터베이스 관리[25, 26, 61]와 같은 다양한 시나리오에서 큰 성공을 보여주었다. 그들의 효과는 생각 사슬 추론이나 자기 성찰과 같은 방법으로 더욱 증폭된다[41, 52].\n' +
      '\n' +
      '동시에 컴퓨터 비전 커뮤니티는 GUI 이해 및 로봇 내비게이션[3, 5, 9, 45]과 같은 다양한 시각적 맥락에서 LLM-as-agent 기반 접근법을 탐구하기 시작했다. 긴 형식의 비디오 이해 영역에서 여러 연구에서 외부 도구와 상호 작용하거나 추가 기능을 통합하기 위해 LLM을 사용하는 에이전트 유사 접근법으로 초기 시도를 했다[6, 45, 60]. 이러한 접근법과 달리 우리의 작업은 비디오 이해도를 의사 결정 과정으로 재구성하는데, 이는 인간이 비디오 해석 방법을 해결하는 방법에 영감을 받는다. 우리는 비디오를 더 많은 정보를 찾거나 상호 작용을 마무리하는 결정을 포함하는 환경으로 본다. 이러한 관점은 비디오 이해에 내재된 의사 결정 측면을 강조함으로써 기존의 방법론과 크게 다른 새로운 프레임워크인 _VideoAgent_의 생성을 안내하였다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '이 섹션에서는 _VideoAgent_의 방법을 소개한다. _ VideoAgent_는 긴 형태의 비디오를 이해하는 인간의 인지 과정에서 영감을 받는다. 비디오가 주어졌을 때\n' +
      '\n' +
      '그림 2:_VideoAgent의 반복 과정에 대한 상세 보기. 각각의 라운드는 이전에 시청된 비디오 프레임들을 포함하는 상태로 시작한다. 그런 다음 큰 언어 모델은 예측과 자기 성찰에 답하여 후속 행동을 결정한다. 추가 정보가 필요한 경우, 비디오 프레임 형태로 새로운 관찰이 획득된다._\n' +
      '\n' +
      '질문과 함께, 인간은 먼저 그 맥락을 이해하기 위해 여러 프레임을 힐끗 본 다음, 질문에 답하기에 충분한 정보를 얻기 위해 추가 프레임을 반복적으로 검색하고, 최종적으로 모든 정보를 집계하고 예측을 할 것이다.\n' +
      '\n' +
      '이 과정을 상태, 동작, 관찰의 순서로 공식화한다. 여기서 상태는 모든 볼 수 있는 프레임의 기존 정보이고, 동작은 질문에 답할 것인지 새로운 프레임을 계속 검색할 것인지, 관찰은 현재 반복에서 볼 수 있는 새로운 프레임이며, \\(s_{t},a_{t},o_{t})|1\\leq t\\leq T\\}\\.\n' +
      '\n' +
      '위 과정(그림 1)을 수행하기 위해 에이전트로 LLM(Large Language Model) GPT-4[31]을 활용한다. LLM은 메모리, 추론 및 계획, 도구 사용 능력[39, 46, 52, 70]을 갖는 것으로 입증되었으며, 이는 각각 상태, 동작 및 관찰을 모델링하는 데 사용할 수 있다.\n' +
      '\n' +
      '### 초기 상태 획득\n' +
      '\n' +
      '반복적인 과정을 시작하기 위해 먼저 비디오에서 균일하게 샘플링된 \\(N\\) 프레임을 힐끗 봄으로써 얻을 수 있는 비디오의 컨텍스트를 LLM에 익숙하게 한다. LLM은 시각적 이해 능력이 없기 때문에 시각적 콘텐츠를 언어 설명으로 변환하기 위해 비전 언어 모델(VLM)을 활용한다. 구체적으로, 우리는 이 \\(N\\) 프레임을 "이미지를 자세히 설명"하라는 프롬프트로 캡션하고 캡션을 LLM에 공급한다. 이 초기 상태 \\(s_{1}\\)는 비디오의 내용 및 의미들의 스케치를 기록한다.\n' +
      '\n' +
      '### 다음 행동 결정\n' +
      '\n' +
      '보이는 모든 프레임의 정보를 저장하는 현재 상태 \\(s_{t}\\)이 주어지면, 다음 동작 \\(a_{t}\\)에 대한 두 가지 가능한 옵션이 있다:\n' +
      '\n' +
      '**Action 1: 질문에 답하라.** 상태 \\(s_{t}\\)의 정보가 질문에 답하기에 충분하다면, 우리는 질문에 답하고 반복적인 과정을 끝내야 한다.\n' +
      '**Action 2: 새로운 정보를 검색.** \\(s_{t}\\)의 현재 정보가 부족할 경우 질문에 답하기 위해 어떤 추가 정보가 필요한지 결정하여 계속 검색해야 한다.\n' +
      '\n' +
      '행동 1과 행동 2 사이에서 결정하려면 질문과 기존 정보를 추론할 LLM이 필요하다. 이것은 3단계 프로세스에 의해 달성된다. 먼저, 우리는 LLM이 사고 연쇄 프롬프트를 통해 현재 상태와 질문을 기반으로 예측을 하도록 강제한다. 둘째, LLM에게 단계 1에 의해 생성된 상태, 질문, 예측 및 추론 과정을 기반으로 자기 성찰하고 신뢰 점수를 생성하도록 요청하며, 신뢰 점수는 1(부족한 정보), 2(부분 정보), 3(충분한 정보)의 세 가지 수준을 갖는다. 마지막으로, 우리는 신뢰 점수에 기초하여 액션 1 또는 2를 선택한다. 이 프로세스는 그림 2에 설명되어 있다. 우리는 직접 예측이 항상 새로운 정보를 검색하기로 결정함에 따라 행동을 직접 선택하는 단일 단계 프로세스보다 3단계 프로세스를 사용할 것을 제안한다(Action 2). 이러한 자기 성찰 과정은 [41]에 의해 동기가 부여되며, 이는 자연어 처리에서 월등한 효과를 입증하였다.\n' +
      '\n' +
      '새로운 관찰을 모으는 것\n' +
      '\n' +
      'LLM이 질문에 답할 불충분한 정보를 결정하고 새로운 정보 검색을 선택한다고 가정하자. 이 경우, 우리는 추가로 LLM에 어떤 추가 정보가 필요한지 결정하여 도구를 활용하여 도구를 검색할 수 있도록 요청합니다(그림 2). 비디오 내에서 여러 번 정보가 발생할 수 있기 때문에 시간적 추론 능력을 향상시키기 위해 비디오 레벨 검색 대신 세그먼트 레벨 검색을 수행한다. 예를 들어, "소년이 방을 떠난 후에 소파에 남아 있는 장난감은 무엇인가?"라는 질문과 우리가 소년이 방을 떠난 것을 액자 \\(i\\)에서 보았다고 가정하자. 우리가 "소파 위의 장난감을 보여주는 프레임"이라는 질문으로 검색하면 "소파 위의 장난감"이 포함된 프레임 \\(i\\) 앞에 프레임이 있을 수 있지만 질문에 답하는 것과 무관하다.\n' +
      '\n' +
      '세그먼트 레벨 검색을 수행하기 위해 먼저 볼 수 있는 프레임 인덱스에 따라 비디오를 서로 다른 세그먼트로 분할하고 LLM에 쿼리 텍스트로 검색할 세그먼트를 예측하도록 요청한다. 예를 들어, 비디오의 프레임 \\(i\\), \\(j\\), \\(k\\)을 보았다면, 하나의 유효한 예측은 세그먼트 2(프레임 \\(i\\)에서 \\(j\\))이며, "소파에 장난감을 보여주는 프레임"이라는 쿼리가 있다.\n' +
      '\n' +
      '우리는 CLIP[36]을 활용하여 LLM에 의한 출력이 주어진 이 추가 정보를 얻는다. 구체적으로, 각 질의와 세그먼트가 주어졌을 때, 우리는 그 세그먼트에서 텍스트 질의와 코사인 유사도가 가장 높은 이미지 프레임을 반환한다. 이러한 검색된 프레임은 상태를 업데이트하기 위한 관찰로 사용됩니다.\n' +
      '\n' +
      '검색 단계에서 CLIP의 사용은 여러 가지 이유로 LLM 또는 VLM을 사용하는 것에 비해 계산적으로 효율적이고 무시할 수 있다. 첫째, CLIP의 특징 계산은 단지 하나의 피드포워드 과정을 포함한다. 둘째, CLIP는 이미지-텍스트 후기 상호작용 아키텍처를 채택하여, 서로 다른 텍스트 쿼리에 걸쳐 이미지 프레임 특징을 캐싱하고 재사용할 수 있게 한다. 마지막으로, 세그먼트 수준 검색 설계는 특정 세그먼트 내의 컴퓨팅 기능만 요구하여 효율성을 더욱 향상시킵니다. 경험적으로, 우리의 실험은 CLIP 계산이 VLM 및 LLM 계산의 1% 미만임을 보여준다.\n' +
      '\n' +
      '###현재 상태 업데이트\n' +
      '\n' +
      '마지막으로, 새로운 관찰(즉, 검색된 프레임)이 주어지면 VLM을 활용하여 각 프레임에 대한 캡션을 생성한 다음 프레임 인덱스에 따라 새로운 캡션을 이전 프레임 캡션과 정렬 및 연결하고 LLM에 다음 라운드 예측을 생성하도록 요청한다.\n' +
      '\n' +
      '몇 가지 기존 작업이 단일 단계 [16, 67]에서 전체 또는 균일하게 샘플링된 프레임을 상태로 사용하기 때문에 다중 라운드 프로세스를 활용하는 이유가 질문일 수 있다. 이러한 기준선에 비해 우리의 접근법에는 많은 이점이 있다. 먼저, 너무 많은 프레임들을 사용하는 것은 광범위한 정보 및 잡음을 도입하며, 이는 LLM들이 긴 컨텍스트들에 시달리고 쉽게 산만해질 수 있기 때문에 성능 저하를 초래한다[24, 40]. 또한, LLM 컨텍스트 길이 제한으로 인해 최대 1시간 길이의 비디오를 확장하는 것은 계산적으로 비효율적이며 어렵다[31]. 반대로, 너무 적은 프레임들을 사용하는 것은 관련 정보를 캡처하지 않을 수 있다. 우리의 적응형 선택 전략은 가장 관련성이 높은 정보를 찾고 다양한 난이도에서 질문에 답하는 데 가장 낮은 비용이 필요하다.\n' +
      '\n' +
      '우리는 _VideoAgent_를 알고리즘 1로 요약한다.\n' +
      '\n' +
      '```\n' +
      '0: Video \\(v\\), question \\(q\\), LLM \\(F_{l}\\), VLM \\(F_{v}\\), CLIP \\(F_{c}\\), maxiteration \\(T\\), confidence threshold \\(C\\)\n' +
      '0: 예측 \\(\\hat{y}\\), 상태-작용-관찰 시퀀스 \\(\\{s_{t},a_{t},o_{t}|1\\leq t\\leq T\\}\\)\n' +
      '1:\\(s_{1}\\leftarrow\\texttt{GenerateCaptions}(F_{v},\\texttt{UniformSample}(v))\\)\n' +
      '2:for\\(t=1\\)to\\(T\\)do\n' +
      '3:\\(\\hat{y}\\leftarrow\\texttt{PredictAnswer}(F_{l},s_{t},q)\\)\n' +
      '4:\\(c\\leftarrow\\texttt{SelfReflect}(F_{l},s_{t},q,\\hat{y})\\)\n' +
      '5:if\\(a_{t}\\leftarrow\\mathbb{1}_{[c\\geq C]}\\)then\n' +
      '6:break\n' +
      '7:else\n' +
      '8:\\(h\\leftarrow\\texttt{FindMissingInfo}(F_{l},s_{t},q)\\)\n' +
      '9:\\(o_{t}\\leftarrow\\texttt{RetrieveFrames}(F_{c},v,h)\\)\n' +
      '10:\\(s_{t+1}\\leftarrow\\texttt{Merge}(s_{t},\\texttt{GenerateCaptions}(F_{v},o_{t}))\\)\n' +
      '11:endif\n' +
      '12:endfor\n' +
      '13:return\\(\\hat{y}\\), \\(\\{s_{t},a_{t},o_{t}|1\\leq t\\leq T\\}\\)\n' +
      '```\n' +
      '\n' +
      '**Algorithm 1**_VideoAgent_\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '이 섹션에서는 먼저 데이터 세트와 구현 세부 사항을 소개하고 _VideoAgent_의 결과, 분석, 절제 및 사례 연구를 제시한다.\n' +
      '\n' +
      '데이터셋과 메트릭스\n' +
      '\n' +
      '실험에서 우리는 제로 샷 이해 능력에 특히 중점을 두고 모델의 성능을 벤치마킹하기 위해 잘 확립된 두 가지 데이터 세트를 사용한다.\n' +
      '\n' +
      'EgoSchema[28].EgoSchema는 5,000개의 자기중심적 비디오에서 파생된 5,000개의 객관식 질문을 특징으로 하는 긴 형식의 비디오 이해의 벤치마크이다. 이 비디오들은 광범위한 활동에 참여하는 인간에 대한 자기중심적인 관점을 제공한다. 이 데이터 세트의 독특한 특징은 각각 3분 동안 지속되는 비디오의 길이이다. EgoSchema는 공개적으로 이용 가능한 라벨들을 갖는 500개의 질문들의 서브세트와 함께 테스트 세트만을 포함한다. 전체 질문 세트는 공식 리더보드에서만 평가됩니다.\n' +
      '\n' +
      'NExT-QA[55]. NExT-QA 데이터셋에는 일상 생활에서 객체 상호작용을 특징으로 하는 5,440개의 자연 동영상과 48,000개의 객관식 질문이 포함되어 있다. 비디오의 평균 길이는 44초입니다. 이러한 질문은 시간, 인과, 서술의 세 가지 범주로 분류되어 비디오 이해 모델에 대한 포괄적인 평가를 제공한다. 표준 관행에 따라 제로 샷 평가는 570개의 비디오와 5,000개의 객관식 질문을 포함하는 유효성 검사 세트에 초점을 맞췄다. 우리는 NExT-QA 검증 세트의 ATP-하드 하위 집합에 대한 성능을 보고하기 위해 [4]를 추가로 따른다. 이 부분 집합은 한 프레임으로 해결할 수 없는 가장 어려운 QA 쌍을 유지하여 장기적인 시간적 추론에 더 중점을 둔다.\n' +
      '\n' +
      '각 데이터 세트는 객관식 질문을 특징으로 하기 때문에 정확도를 평가 메트릭으로 활용했다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '1 fps에서 모든 비디오를 디코딩하고 EVA-CLIP-8B+[43]을 사용하여 생성된 시각적 설명과 프레임 특징 사이의 코사인 유사성을 기반으로 가장 관련성이 높은 프레임을 검색한다. EgoSchema에 대한 실험을 위해 클립 기반 자막 모델인 LaViLa[68]을 자막기로 사용한다. [67]에 이어, 제로샷 평가를 보장하기 위해, 우리는 에고스키마와 중첩된 비디오를 필터링하면서 에고4D 데이터에서 재훈련된 LaViLa 모델을 활용한다. CLIP 검색 모듈에 의해 반환된 프레임 색인에 기초하여 캡션용 비디오 클립을 샘플링한다. NExT-QA의 경우 CogAgent[9]를 캡셔너로 사용한다. 모든 실험에 대해 LLM으로 GPT-4[31]을 사용하며, GPT의 버전은 재현성을 보장하기 위해 _gpt-4-1106-preview_로 고정된다.\n' +
      '\n' +
      '### 최신기술과의 비교\n' +
      '\n' +
      '_VideoAgent_는 새로운 벤치마크를 설정하여 EgoSchema 및 NExT-QA 데이터셋에 대한 최신(SOTA) 결과를 달성하며, 분석을 위해 최소한의 프레임만 요구하면서 이전 방법을 크게 능가한다.\n' +
      '\n' +
      'EgoSchema.표 1 및 표 2에 나타낸 바와 같이, _VideoAgent_는 EgoSchema 전체 데이터세트에서 54.1%, 500-질문 서브세트에서 60.2%의 정확도를 달성한다. 지상 진실 레이블을 공개적으로 사용할 수 없기 때문에 모델의 예측을 공식 리더보드에 업로드하여 전체 데이터 세트의 정확도를 확인했다. 이러한 결과는 기존 SOTA 방법 LLoVi[67]보다 3.8% 크게 능가할 뿐만 아니라 제미니-1.0[47]과 같은 고급 독점 모델과 유사한 성능을 달성한다. 특히, 우리의 방법은 비디오당 평균 8.4 프레임만을 필요로 하며, 이는 기존 접근법에 비해 2배에서 30배까지 훨씬 적다.\n' +
      '\n' +
      'NExT-QA. 표 3에서 _VideoAgent_가 NExT-QA 전체 검증 세트에서 71.3%의 정확도를 달성하여 이전 SOTA인 LLoVi[67]를 3.6% 능가함을 보여준다. 0-shot 평가를 위해 비디오당 평균 8.2 프레임만 사용된 경우, _VideoAgent_는 모델의 인과적, 시간적, 기술적 능력을 테스트하는 것을 포함하여 모든 하위 집합에서 이전 감독 및 제로-shot 방법을 큰 마진으로 일관되게 능가한다. 중요하게도, _VideoAgent_는 보다 도전적인 부분 집합인 ATP-hard[4]에서 현저한 성능 향상을 달성하여 복잡한 긴 형식의 비디오 쿼리를 다루는 데 능숙함을 보여준다.\n' +
      '\n' +
      '이러한 결과는 긴 형식의 비디오에서 복잡한 질문을 처리하고 이해하는 데 있어 _VideoAgent_의 탁월한 효과와 효율성을 강조한다.\n' +
      '\n' +
      '반복골조선택의### 분석\n' +
      '\n' +
      '_VideoAgent_의 주요 구성 요소 중 하나는 비디오를 이해하는 인간의 과정을 모방하여 질문에 답하기에 충분할 때까지 더 많은 정보를 동적으로 검색하고 집계하는 반복 프레임 선택이다. 이 과정을 더 잘 이해하기 위해 포괄적인 분석과 절제 연구를 수행했다.\n' +
      '\n' +
      '프레임 효율성.우리의 첫 번째 분석은 프레임 선택이 질문에 답하는 데 필요한 유익한 프레임을 효과적으로 식별하는지에 초점을 맞췄다. 이것은 프레임 효율에 의해 측정될 수 있다: 고정된 수의 프레임이 주어지면, 어떤 모델 정확도를 달성할 수 있는가? 가설은 정보가 많은 프레임을 식별할수록 프레임 효율성이 높아야 한다는 것이다. 그림 3(왼쪽)에서 우리는 균일한 샘플링 기준선 및 기타와 비교하여 방법의 정확도를 표시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c c|c c c} \\hline \\hline \\multicolumn{2}{c|}{Methods} & \\multicolumn{4}{c|}{Val} & \\multicolumn{4}{c}{ATP-hard subset} \\\\  & Acc@C & Acc@T & Acc@D & Acc@All & Acc@C & Acc@T & Acc@All \\\\ \\hline \\hline \\multicolumn{2}{c|}{_Supervised_} & & & & & & & & \\\\ VFC [57] & [ICCV2021] & 49.6 & 51.5 & 63.2 & 52.3 & - & - & - \\\\ ATP [4] & [CVPR2022] & 53.1 & 50.2 & 66.8 & 54.3 & 38.4 & 36.5 & 38.8 \\\\ MIST [7] & [CVPR2023] & 54.6 & 56.6 & 66.9 & 57.2 & - & - & - \\\\ GF [1] & [NeurIPS2023] & 56.9 & 57.1 & 70.5 & 58.8 & 48.7 & 50.3 & 49.3 \\\\ CoVGT [54] & [TPAMI2023] & 59.7 & 58.0 & 69.9 & 60.7 & - & - & - \\\\ SeViT [15] & [arXiv2023.1] & 54.0 & 54.1 & 71.3 & 56.7 & 43.3 & 46.5 & - \\\\ HiTeA [64] & [ICCV2023] & 62.4 & 58.3 & 75.6 & 63.1 & 47.8 & 48.6 & - \\\\ \\hline \\multicolumn{2}{c|}{_Zero-shot_} & & & & & & & \\\\ VFC [29] & [ICCV2023] & 51.6 & 45.4 & 64.1 & 51.5 & 32.2 & 30.0 & 31.4 \\\\ InterunVideo [51] & [arXiv2023.12] & 43.4 & 48.0 & 65.1 & 49.1 & - & - & - \\\\ AssistGPT [6] & [arXiv2023.0] & 60.0 & 51.4 & 67.3 & 58.4 & - & - & - \\\\ ViperGPT [45] & [ICCV2023] & - & - & - & 60.0 & - & - & - \\\\ SeViLA [66] & [NeurIPS2023] & 61.3 & 61.5 & 75.6 & 63.6 & - & - & - \\\\ LLoVi [67] & [arXiv2024.2] & 69.5 & 61.0 & 75.6 & 67.7 & - & - & - \\\\ \\hline VideoAgent & (ours) & **72.7** & **64.5** & **81.1** & **71.3** & **57.8** & **58.8** & **58.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 최첨단 대비 NExT-QA에 대한 결과._ C, T 및 D는 각각 인과적, 시간적 및 설명적 하위 집합이다.\n' +
      '\n' +
      'EgoSchema 500-question subset에 대한 이전 방법들. 제안된 방법은 동일한 수의 프레임들에서 균일한 선택 및 다른 기준선들을 상당히 능가하여, 프레임 효율에서 그 우월성을 입증한다. 특히, 60.2%의 정확도를 달성하기 위해 8.4프레임만을 사용하는 우리의 방법은 59.6%의 정확도를 달성하기 위해 180프레임을 균일하게 샘플링하는 기준선을 능가한다. 이것은 유익한 프레임을 찾는 데 있어 우리의 방법의 효과를 강조하고 관련이 없고 잡음이 많은 정보가 긴 컨텍스트와 산만함으로 언어 모델을 압도할 수 있기 때문에 더 많은 프레임이 항상 더 나은 성능으로 이어지는 것은 아님을 보여준다[24, 40].\n' +
      '\n' +
      '라운드 수.반복 라운드 수가 모델 성능에 어떤 영향을 미치는지 분석하였다. 동일한 그림 3(왼쪽)에서 우리는 1-4 라운드에 걸친 성능과 선택된 프레임 수를 도표하여 각각 5, 7.5, 8.4 및 9.9 프레임으로 53.8%, 58.6%, 60.2% 및 59.8%의 정확도를 달성했다. 성능은 추가 라운드로 향상되지만 EgoSchema 500-질문 서브세트에서 3라운드로 포화된다. 이는 우리의 접근법이 질문에 답하는 데 필요한 정보를 효율적으로 찾을 수 있음을 나타내며, 특정 지점 이상에서는 추가 정보가 질문에 답하는 데 더 이상 도움이 되지 않는다.\n' +
      '\n' +
      '다른 질문 유형.우리의 프레임 선택 프로세스가 동적이라는 점을 고려하여, 언어 모델 에이전트가 정보가 충분한지 여부를 결정함으로써, 다른 질문 유형이 난이도가 다르기 때문에 다양한 양의 정보를 필요로 할 수 있다는 가설을 세웠다. 우리는 기술 작업, 인과 추론 또는 시간적 추론과 같은 각 질문 유형에 대한 주석을 제공하는 NExT-QA 데이터 세트에 대해 이 가설을 테스트했다. 그림 3(오른쪽)에서는 각 문항 유형에 대한 프레임 수의 분포를 도식화한다. 사용된 평균 프레임 수는 서술형(5.9 프레임), 인과형(7.1 프레임), 시간형(7.8 프레임) 질문과 같이 순위가 매겨지는 것을 관찰했다. 이것은 기술 작업이 초기처럼 종종 더 적은 프레임을 필요로 한다는 인간의 직관과 일치한다.\n' +
      '\n' +
      '그림 3: (왼쪽) 균일한 샘플링 및 이전 방법과 비교한 프레임 효율성. X축은 로그 축척입니다. 본 논문에서 제안하는 방법은 긴 형태의 비디오 이해에 탁월한 프레임 효율성을 제공한다. (오른쪽) 다양한 유형의 NExT-QA 질문에 대한 프레임 수입니다. Min, 평균, 최대, 분포가 표시됩니다. 비디오에이전트는 인과적 추론과 서술적 질문보다 시간적 추론과 관련된 질문에 더 많은 프레임을 선택한다.\n' +
      '\n' +
      '균일 샘플링은 보통 충분한 반면, 추론 작업, 특히 시간적 추론은 질문에 정확하게 답하기 위해 더 많은 프레임을 보아야 한다.\n' +
      '\n' +
      '초기 프레임 수.반복 프레임 선택 과정을 시작하기 전에 언어 모델을 비디오 컨텍스트에 익숙하게 하기 위해 \\(N\\) 프레임을 균일하게 샘플링한다. 초기 샘플링된 프레임의 수가 모델 성능과 평균 사용되는 프레임 수에 어떻게 영향을 미치는지 조사하기 위해 절제 연구를 수행한다. 구체적으로, 처음에 EgoSchema 500-question subset에서 3, 5, 8개의 프레임을 샘플링하고 표 4의 결과를 보고한다. 결과는 각각 평균 6.4, 8.4, 11.0 프레임으로 58.4%, 60.2%, 57.4%의 정확도를 나타낸다. 5프레임으로 시작하여 가장 높은 성능으로 이어집니다. 또한, 유사하거나 약간 더 많은 수의 프레임을 사용하여 균일한 샘플링과 비교할 때 7, 9 및 11 프레임에 대해 각각 54.6%, 54.8% 및 55.8%의 정확도를 관찰했다. 이 비교는 우리의 프레임 선택 방법의 우수한 효율성을 다시 강조한다.\n' +
      '\n' +
      '자기 평가.반복 선택 과정 동안, 우리는 이용 가능한 정보가 질의에 응답하기에 충분한지를 확인하기 위해 자기 평가를 수행한다. 충분하다면, 반복은 이 단계에서 종료된다. 우리는 이것을 세 번의 반복을 통해 모든 질문이 처리되는 자체 평가가 없는 기준선 방법과 벤치마킹한다. 표 5에 자세히 설명된 대로 평균 프레임 수가 8.4개에서 11.8개로 증가하고 정확도가 60.2%에서 59.6%로 감소하는 것을 관찰한다. 이러한 결과는 정보의 적절성을 결정하는 데 있어 자체 평가의 효율성을 강조하여 불필요한 반복을 줄인다. 특히, 추가 라운드를 통해 더 많은 정보를 수집하는 것은 성능 향상으로 이어지지 않고 오히려 한계 하락으로 귀결된다.\n' +
      '\n' +
      '세그먼트 선택.추가 정보가 필요한 것으로 결정될 때, 입력 비디오들은 세그먼트들로 분할된다. 그런 다음 언어 모델은 해당 세그먼트 내에서 정보를 검색하도록 특별히 맞춤화된 쿼리를 생성한다. 이 접근법은 세그먼트를 지정하지 않고 쿼리를 생성하는 것과 관련된 대체 전략과 대조된다. 표 5에서 세그먼트 선택이 비활성화되었을 때 3.6%의 정확도 저하를 관찰한다. 세그먼트 선택은 모델의 시간적 추론 능력을 향상시키고 이질적인 세그먼트로부터 정보를 융합할 위험을 완화한다. 이것은 특히 "후에 일어나는 일...?"과 같은 질의에 유익하며, 여기서 검색은 단지 후속 세그먼트로부터만 요망된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c} \\hline \\hline \\multirow{2}{*}{Uniform} & Uni-7 & Uni-9 & Uni-11 \\\\  & 54.6 & 54.8 & 55.8 \\\\ \\hline \\multirow{2}{*}{Ours} & 3\\(\\rightarrow\\)6.4 & 5\\(\\rightarrow\\)8.4 & 8\\(\\rightarrow\\)11.0 \\\\  & **58.4** & **60.2** & **57.4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{c c c} \\hline \\hline Method & Frames & Acc \\\\ \\hline Ours w/o Seg. Selection & 7.5 & 56.6 \\\\ Ours w/o Self-Evaluation & 11.8 & 59.6 \\\\ Ours & 8.4 & **60.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 균일하게 샘플링된 프레임의 초기 수의 절제.\n' +
      '\n' +
      '### 기초모형의 절제에 관한 연구\n' +
      '\n' +
      '_VideoAgent_는 대규모 언어 모델(LLM), 시각적 언어 모델(VLM) 및 대조적 언어 이미지 모델(CLIP)의 세 가지 기본 모델 유형을 통합한다는 점을 감안할 때 각 구성 요소의 설계가 모델의 전체 성능에 미치는 영향을 평가하기 위해 일련의 절제 연구를 수행한다.\n' +
      '\n' +
      'Llm. 우리는 LLM이 전체 프로세스를 조정하는 에이전트로 기능하는 방법론에서 LLM의 중추적인 역할을 감안할 때 다른 LLM이 모델의 성능에 어떻게 영향을 미치는지 평가하여 연구를 시작했다. 표 6에서 LLaMA-2-70B[48], Mixtral-8x7B[13], GPT-3.5[32] 및 GPT-4[31]를 포함한 여러 최신 공개 및 독점 LLM을 비교한다. 우리의 연구 결과는 GPT-4가 대응물보다 훨씬 우수하다는 것을 나타낸다. 그러나, 그것은 주로 구조화된 예측에서의 능력 때문이다. 반복 프로세스는 출력을 위해 JSON을 사용하며, 여기서 정확한 JSON 파싱이 중요하다. GPT-4는 올바른 JSON 형식을 생성하는 데 강력한 성능을 보여주며, 이는 LLM[69]에서 활발한 연구 영역으로 남아 있는 다른 모델에 의해 일관되게 달성되지 않은 업적이다.\n' +
      '\n' +
      'Vlm.Leveraging GPT-4는 시각적 기능이 없는 텍스트 전용 모델로서, 이미지 프레임을 VLM을 통해 설명적 캡션으로 변환한 후, 이러한 캡션을 GPT-4에 공급한다. 다양한 VLM에 의해 생성된 캡션 품질이 성능에 미치는 영향을 평가하기 위해, 표 7에 제시된 클립 기반 LaViLa[68]과 함께 프레임 기반 BLIP-2[18] 및 CogAgent[9]의 세 가지 최신 VLM을 검사했다. 분석 결과, CogAgent와 LaViLa로부터의 캡션은 길이가 상당히 다른 반면 BLIP-2 생성 캡션은 훨씬 더 나쁜 성능을 나타낸다.\n' +
      '\n' +
      'Clip.CLIP은 이미지 및 텍스트 특징에 대한 늦은 상호 작용 설계로 인해 검색 작업에 탁월하여 다양한 쿼리에 대한 이미지 임베딩을 재컴퓨팅할 필요가 없다. CLIP의 세 가지 버전인 OpenCLIP ViT-G[11], EVA-CLIP-8B[43], EVA-CLIP-8B+[43]을 표 8과 같이 평가한 결과, 서로 다른 CLIP 모델 간에 유사한 성능을 나타내어 검색 방법이 병목 현상을 구성하지 않음을 알 수 있다.\n' +
      '\n' +
      '우리 연구의 주요 기여는 특정 모델의 사용보다는 긴 형식의 비디오를 이해하는 인간의 과정을 모방하는 프레임워크의 도입이라는 점에 유의할 필요가 있다. LLM, VLM 및 CLIP와 같은 기반 모델의 급속한 발전으로 더 나은 모델의 통합으로 또는 채택함으로써 우리의 접근 방식을 더욱 개선할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline LLM & Model Sim & Acc. (\\%) \\\\ \\hline \\hline Mixtral-8x7B & TUB & 37.8 \\\\ Llam-70B & TUB & 45.4 \\\\ GPT-3.5 & N/A & 48.8 \\\\ GPT-4 & N/A & **60.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: \\(LLM\\)_ablation._\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline CLIP & Model Sim & Resolution & Acc. (\\%) \\\\ \\hline \\hline Object-VIT-G & 11 & 22 & 20.2 \\\\ HX-CLIP-8B & 51 & 22 & 20.4 \\\\ HX-CLIP-8B-plus & 51 & 22 & 20.4 \\\\ HX-CLIP-8B-plus & 51 & 40B & **60.2** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: _CLIP ablation._ GPT-4를 GPT-4V로 대체하여 캡션이 없는 방법론. 우리는 우리의 작업이 이 방향에서 미래의 작업에 빛을 발하기를 바란다.\n' +
      '\n' +
      '### Case Studies\n' +
      '\n' +
      '본 논문에서는 긴 형태의 비디오를 이해하는 데 있어 _VideoAgent_의 기능을 입증하기 위한 몇 가지 사례 연구를 제시한다.\n' +
      '\n' +
      'NExT-QA로부터의 질문 [55].도 4에서, 우리는 두 번의 반복으로 풀린 NExT-QA로부터의 인스턴스를 예시한다. 문제는 그 남자가 왜 친구들과 이야기할 때 물 한 컵을 들고 있느냐는 것이다. VideoAgent_는 누락된 정보를 정확하게 식별한다(비록 컵이 프레임 69에서 보이더라도, 그것은 남자가 그것을 들고 있는 것을 드러내지 않는다). 그런 다음 어떤 추가 정보가 필요한지 결정한다(검은 스웨터를 입은 남자가 물컵을 들고 있는 모습). 마지막으로 CLIP를 활용하여 이 세부 사항(남자가 잔을 들고 있고 그로부터 술을 마시는 행위)을 검색하고 그 답변에 대해 자신감을 느낀다.\n' +
      '\n' +
      '그림 4: _NExT-QA에 대한 사례 연구 VideoAgent_는 제1 라운드에서 누락된 정보를 정확하게 식별하고, 제2 라운드에서 정보 갭을 브릿지하며, 이에 의해 정확한 예측을 한다.\n' +
      '\n' +
      '1시간짜리 동영상 NExT-QA와 EgoSchema 비디오가 모두 몇 분에 불과하다는 점을 감안할 때, 그림 5는 _VideoAgent_가 YouTube+의 1시간 길이의 비디오를 정확하게 해결할 수 있는 방법을 보여준다. 문제는 비디오의 작은 부분만 차지하는 녹색 식물이 둘러싸고 있는 계단의 색상을 파악하는 것이다. _ VideoAgent_는 필요한 정보를 효율적으로 식별하고 GPT-4V와 같은 최첨단 모델을 능가하는 2개의 반복 및 7개의 프레임 내에서 질문에 응답한다. 특히, GPT-4V는 48개의 이미지의 최대 컨텍스트 길이에 걸쳐 균일한 샘플링에 어려움을 겪는다. 그러나, GPT-4V가 _VideoAgent_에 의해 핀포인팅된 프레임과 함께 제공될 때, 그것은 질문에 성공적으로 답변할 수 있다. 이것은 우리의 접근법을 통합함으로써 비디오 이해에서 GPT-4V의 능력을 향상시킬 가능성을 강조한다.\n' +
      '\n' +
      '각주 †: [https://www.youtube.com/watch?v=H9Y5_X1sEEA](https://www.youtube.com/watch?v=H9Y5_X1sEEA)\n' +
      '\n' +
      '결론적으로, _VideoAgent_는 한 라운드 희소 또는 밀집 샘플링에 의존하는 전통적인 방법을 능가하는 실제 비디오 이해 문제를 해결할 준비가 되어 있다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 연구에서는 긴 동영상의 이해를 위해 인간의 인지 과정을 반영하기 위해 큰 언어 모델을 에이전트로 사용하는 시스템인 _VideoAgent_를 소개한다. _VideoAgent_ VideoAgent_는 다중 라운드 반복 과정을 통해 효과적으로 정보를 검색하고 집계한다. 다양한 데이터 세트에 대한 양적 및 질적 연구 모두에서 입증된 바와 같이 긴 형식의 비디오 이해에서 탁월한 효과와 효율성을 보여준다.\n' +
      '\n' +
      '그림 5: _1시간짜리 동영상에 대한 사례 연구. VideoAgent_는 제2 반복 동안 키 프레임을 정확하게 식별하여, 후속적으로 정확한 예측을 한다. 반대로 GPT-4V는 최대 컨텍스트 길이까지 균일하게 샘플링된 48개의 프레임에 의존할 때 성공적인 예측을 얻지 못한다. 그러나, _VideoAgent_에 의해 핀포인팅된 프레임을 통합함으로써, GPT-4V는 질문에 정확하게 답할 수 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Bai, Z., Wang, R., Chen, X.: Glance and focus: Memory prompting for multi-event video question answering. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [2] Balazevic, I., Shi, Y., Papalampidi, P., Chaabouni, R., Koppula, S., Henaff, O.J.: Memory consolidation enables long-context video understanding. arXiv preprint arXiv:2402.05861 (2024)\n' +
      '* [3] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al.: Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818 (2023)\n' +
      '* [4] Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., Niebles, J.C.: Revisiting the" video" in video-language understanding. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2917-2927 (2022)\n' +
      '* [5] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 (2023)\n' +
      '* [6] Gao, D., Ji, L., Zhou, L., Lin, K.Q., Chen, J., Fan, Z., Shou, M.Z.: Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640 (2023)\n' +
      '* [7] Gao, D., Zhou, L., Ji, L., Zhu, L., Yang, Y., Shou, M.Z.: Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14773-14783 (2023)\n' +
      '* [8] Han, W., Chen, H., Kan, M.Y., Poria, S.: Sas video-qa: Self-adaptive sampling for efficient video question-answering (2023)\n' +
      '* [9] Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al.: Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914 (2023)\n' +
      '* [10] Hussein, N., Gavves, E., Smeulders, A.W.: Videograph: Recognizing minutes-long human activities in videos. arXiv preprint arXiv:1905.05143 (2019)\n' +
      '* [11] Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt, L.: Openclip (Jul 2021). [https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773), [https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773), if you use this software, please cite it as below.\n' +
      '* [12] Islam, M.M., Bertasius, G.: Long movie clip classification with state-space video models. In: European Conference on Computer Vision. pp. 87-104. Springer (2022)\n' +
      '* [13] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024)\n' +
      '* [14] Jin, P., Takanobu, R., Zhang, C., Cao, X., Yuan, L.: Chat-univi: Unified visual representation empowers large language models with image and video understanding (2023)\n' +
      '* [15] Kim, S., Kim, J.H., Lee, J., Seo, M.: Semi-parametric video-grounded text generation. arXiv preprint arXiv:2301.11507 (2023)\n' +
      '* [16] Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T.L., Bansal, M., Liu, J.: Less is more: Clipbert for video-and-language learningvia sparse sampling. In: CVPR (2021)\n' +
      '* [17] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In: ICML (2023)* [18] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In: ICML (2023)\n' +
      '* [19] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., Qiao, Y.: Videochat: Chat-centric video understanding (2023)\n' +
      '* [20] Li, Y., Chen, X., Hu, B., Zhang, M.: Llms meet long video: Advancing long video comprehension with an interactive visual adapter in llms (2024)\n' +
      '* [21] Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122 (2023)\n' +
      '* [22] Liu, H., Yan, W., Zaharia, M., Abbeel, P.: World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268 (2024)\n' +
      '* [23] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023)\n' +
      '* [24] Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang, P.: Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics **12**, 157-173 (2024)\n' +
      '* [25] Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al.: Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688 (2023)\n' +
      '* [26] Ma, C., Zhang, J., Zhu, Z., Yang, C., Yang, Y., Jin, Y., Lan, Z., Kong, L., He, J.: Agentboard: An analytical evaluation board of multi-turn llm agents. arXiv preprint arXiv:2401.13178 (2024)\n' +
      '* [27] Ma, F., Jin, X., Wang, H., Xian, Y., Feng, J., Yang, Y.: Vista-llama: Reliable video narrator via equal distance to visual tokens. arXiv preprint arXiv:2312.08870 (2023)\n' +
      '* [28] Mangalam, K., Akshulakov, R., Malik, J.: Egoschema: A diagnostic benchmark for very long-form video language understanding. arXiv preprint arXiv:2308.09126 (2023)\n' +
      '* [29] Momeni, L., Caron, M., Nagrani, A., Zisserman, A., Schmid, C.: Verbs in action: Improving verb understanding in video-language models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15579-15591 (2023)\n' +
      '* [30] Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., Re, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems **35**, 2846-2861 (2022)\n' +
      '* [31] OpenAI: Gpt-4 technical report (2023)\n' +
      '* [32] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems **35**, 27730-27744 (2022)\n' +
      '* [33] Pan, J., Lin, Z., Ge, Y., Zhu, X., Zhang, R., Wang, Y., Qiao, Y., Li, H.: Retrieving-to-answer: Zero-shot video question answering with frozen large language models (2023)\n' +
      '* [34] Papalampidi, P., Koppula, S., Pathak, S., Chiu, J., Heyward, J., Patraucean, V., Shen, J., Miech, A., Zisserman, A., Nematzdeh, A.: A simple recipe for contrastively pre-training video-first encoders beyond 16 frames. arXiv preprint arXiv:2312.07395 (2023)\n' +
      '* [35] Park, J.S., O\'Brien, J., Cai, C.J., Morris, M.R., Liang, P., Bernstein, M.S.: Generative agents: Interactive simulacra of human behavior. In: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. pp. 1-22 (2023)* [36] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748-8763. PMLR (2021)\n' +
      '* [37] Ren, S., Yao, L., Li, S., Sun, X., Hou, L.: Timechat: A time-sensitive multimodal large language model for long video understanding (2023)\n' +
      '* [38] Romero, D., Solorio, T.: Question-instructed visual descriptions for zero-shot video question answering (2024)\n' +
      '* [39] Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [40] Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.H., Scharli, N., Zhou, D.: Large language models can be easily distracted by irrelevant context. In: International Conference on Machine Learning. pp. 31210-31227. PMLR (2023)\n' +
      '* [41] Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [42] Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Guo, X., Ye, T., Lu, Y., Hwang, J.N., et al.: Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449 (2023)\n' +
      '* [43] Sun, Q., Wang, J., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, X.: Eva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252 (2024)\n' +
      '* [44] Sun, Y., Xue, H., Song, R., Liu, B., Yang, H., Fu, J.: Long-form video-language pre-training with multimodal temporal contrastive learning. Advances in neural information processing systems **35**, 38032-38045 (2022)\n' +
      '* [45] Suris, D., Menon, S., Vondrick, C.: Vipergpt: Visual inference via python execution for reasoning. Proceedings of IEEE International Conference on Computer Vision (ICCV) (2023)\n' +
      '* [46] Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., Metzler, D.: Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006 (2020)\n' +
      '* [47] Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023)\n' +
      '* [48] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n' +
      '* [49] Wang, S., Zhao, Q., Do, M.Q., Agarwal, N., Lee, K., Sun, C.: Vamos: Versatile action models for video understanding (2023)\n' +
      '* [50] Wang, Y., Bertasius, G., Oh, T.H., Gupta, A., Hoai, M., Torresani, L.: Supervoxel attention graphs for long-range video modeling. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 155-166 (2021)\n' +
      '* [51] Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al.: Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191 (2022)\n' +
      '* [52] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems **35**, 24824-24837 (2022)* [53] Wu, C.Y., Li, Y., Mangalam, K., Fan, H., Xiong, B., Malik, J., Feichtenhofer, C.: Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13587-13597 (2022)\n' +
      '* [54] Xiao, J., Zhou, P., Yao, A., Li, Y., Hong, R., Yan, S., Chua, T.: Contrastive video question answering via video graph transformer. IEEE Transactions on Pattern Analysis; Machine Intelligence **45**(11), 13265-13280 (nov 2023)\n' +
      '* [55] Xiao, J., Shang, X., Yao, A., Chua, T.S.: Next-qa: Next phase of question-answering to explaining temporal actions. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9777-9786 (2021)\n' +
      '* [56] Xu, J., Lan, C., Xie, W., Chen, X., Lu, Y.: Retrieval-based video language model for efficient long video question answering (2023)\n' +
      '* [57] Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Just ask: Learning to answer questions from millions of narrated videos. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1686-1697 (2021)\n' +
      '* [58] Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Zero-shot video question answering via frozen bidirectional language models. In: NeurIPS (2022)\n' +
      '* [59] Yang, J., Zhu, Y., Wang, Y., Yi, R., Zadeh, A., Morency, L.P.: What gives the answer away? question answering bias analysis on video qa datasets. arXiv preprint arXiv:2007.03626 (2020)\n' +
      '* [60] Yang, Z., Chen, G., Li, X., Wang, W., Yang, Y.: Doraemongpt: Toward understanding dynamic scenes with large language models. arXiv preprint arXiv:2401.08392 (2024)\n' +
      '* [61] Yao, S., Chen, H., Yang, J., Narasimhan, K.: Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems **35**, 20744-20757 (2022)\n' +
      '* [62] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., Narasimhan, K.: Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems **36** (2024)\n' +
      '* [63] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., Cao, Y.: React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022)\n' +
      '* [64] Ye, Q., Xu, G., Yan, M., Xu, H., Qian, Q., Zhang, J., Huang, F.: Hitea: Hierarchical temporal-aware video-language pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15405-15416 (2023)\n' +
      '* [65] Yeung, S., Russakovsky, O., Mori, G., Fei-Fei, L.: End-to-end learning of action detection from frame glimpses in videos. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2678-2687 (2016)\n' +
      '* [66] Yu, S., Cho, J., Yadav, P., Bansal, M.: Self-chained image-language model for video localization and question answering. NeurIPS (2023)\n' +
      '* [67] Zhang, C., Lu, T., Islam, M.M., Wang, Z., Yu, S., Bansal, M., Bertasius, G.: A simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235 (2023)\n' +
      '* [68] Zhao, Y., Misra, I., Krahenbuhl, P., Girdhar, R.: Learning video representations from large language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6586-6597 (2023)\n' +
      '* [69] Zheng, L., Yin, L., Xie, Z., Huang, J., Sun, C., Yu, C.H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J.E., Barrett, C., Sheng, Y.: Efficiently programming large language models using sglang (2023)* [70] Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al.: Least-to-most prompting enables complex reasoning in large language models. In: ICLR (2023)_VideoAgent_**: Long-form Video Understanding**\n' +
      '\n' +
      '**with large language model with agent**\n' +
      '\n' +
      '샤오한 왕({}^{\\star}\\), 유후이 장({}^{\\star}\\), 오르 조하르, 세레나 영레비\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      '{xhanwang,yuhuiz,orrzohar,syyeung}@stanford.edu\n' +
      '\n' +
      '이 문서는 다음과 같이 구성된 접근법과 추가 실험 결과에 대한 자세한 내용을 제공한다.\n' +
      '\n' +
      'CLIP의 실행시간 분석\n' +
      '* \\(\\lx@sectionsign\\) B 추가 구현 세부사항\n' +
      'GPT-4에 대한 \\(\\lx@sectionsign\\)C Prompts\n' +
      '\n' +
      '## 부록 0.CLIP의 런타임 분석\n' +
      '\n' +
      '_VideoAgent_의 CLIP는 비디오 내의 모든 프레임까지 볼 수 있고 VLM은 단지 몇 개의 프레임만을 캡션하는 반면, CLIP를 사용하는 것은 여러 가지 이유로 LLM 또는 VLM을 사용하는 것에 비해 계산적으로 효율적이고 무시할 수 있다. 첫째, CLIP의 특징 계산은 단지 하나의 피드포워드 과정을 포함한다. 둘째, CLIP는 이미지-텍스트 후기 상호작용 아키텍처를 채택하여, 서로 다른 텍스트 쿼리에 걸쳐 이미지 프레임 특징을 캐싱하고 재사용할 수 있게 한다.\n' +
      '\n' +
      'CLIP 특징의 계산은 이미지와 텍스트당 \\(x\\)초를 필요로 하는 반면 VLM 캡셔닝은 이미지당 \\(y\\)초를 필요로 하고 LLM 계산은 라운드당 \\(z\\)초를 필요로 하는 시나리오를 고려한다. \\(N\\) 프레임을 포함하는 비디오의 경우, _VideoAgent_가 \\(n\\) 프레임 중 \\(n\\) 프레임을 \\(t\\) 라운드에 걸쳐 선택적으로 처리한다고 가정한다. 이러한 맥락에서, 이미지 및 텍스트에 대한 CLIP 특징을 계산하는 시간은 각각 \\(Nx\\) 및 \\(nx\\) 초에 달한다. VLM 자막의 생성은 \\(ny\\)초, LLM 연산은 총 \\(tz\\)초가 필요하다. 결과적으로, 전체 계산 시간에 대한 CLIP 특징 계산 전용 시간의 비율은 \\(\\frac{Nx+nx}{Nx+nx+ny+tz}\\)으로 근사화된다.\n' +
      '\n' +
      '실제로 OpenCLIP ViT-G를 CLIP로, CogAgent를 VLM으로, GPT-4를 LLM으로 A6000 GPU와 EgoSchema 데이터셋을 사용하여 \\(N=180\\), \\(n=8.4\\), \\(x=0.02\\), \\(y=20\\), \\(z=10\\), \\(t=3\\)을 찾았다. 따라서 공식은 \\(\\frac{180\\times 0.02+8.4\\times 0.02}{180\\times 0.02+8.4\\times 0.02+8.4\\times 20+3\\times 10}\\)으로 단순화되며, 이는 \\(1.9\\%\\)으로 평가된다. 이는 이러한 조건에서 CLIP 특징의 계산이 전체 계산 노력의 작은 부분, 특히 \\(1.9\\%\\)을 나타냄을 보여준다.\n' +
      '\n' +
      '또한, 상기 추정은 상한을 나타낸다는 점에 유의해야 한다. 실제로, 세그먼트 레벨 검색 방법은 모든 프레임(N\\)이 아닌 지정된 세그먼트 내에서만 컴퓨팅 기능을 필요로 하며, 이는 효율성을 더욱 향상시킨다.\n' +
      '\n' +
      '## 부록 0.B 추가 구현 상세사항\n' +
      '\n' +
      'CogAgent에 대한 자세한 내용.NExT-QA[55]에 대한 실험을 위해 프레임 기반 캡션 모델인 CogAgent[68]을 캡셔너로 사용한다. CogAgent는 입력 영상 해상도 1120 \\(\\times\\)1120을 갖는 18B 파라미터를 갖는다.\n' +
      '\n' +
      'LaViLa에 대한 세부사항.EgoSchema[28]에 대한 실험을 위해 클립 기반 캡션 모델인 캡셔너로서 LaViLa[68]을 활용한다. LaViLA는 4\\(\\times\\) 336\\(\\times\\) 336에서 해상도를 갖는 입력 클립을 취하며, [67]에 이어 제로샷 평가를 보장하기 위해 에고 스키마로 중첩된 영상을 필터링하여 에고4D 데이터에 재학습한 LaViLa 모델을 활용한다.\n' +
      '\n' +
      'CLIP에 대한 자세한 내용은 프레임 검색을 위해 EVA-CLIP-8B-plus[43] 모델, 75억 매개 변수를 갖는 비전 인코더 및 77억 매개 변수를 갖는 텍스트 인코더를 포함하는 최첨단 CLIP 모델을 사용한다. 이 모델은 448\\(\\times\\) 448의 해상도로 이미지를 처리하고 1280의 차원을 갖는 출력 특징을 생성한다.\n' +
      '\n' +
      '## 부록 0.C GPT-4에 대한 프롬프트\n' +
      '\n' +
      'GPT-4가 답변, 자기 성찰, 누락된 정보를 찾기 위해 활용하는 구체적인 프롬프트는 각각 그림 1, 2, 3과 같다.\n' +
      '\n' +
      '도 1: ** 답변을 예측하기 위한 프롬프트.** 실제 비디오 및 질문(오렌지색으로 강조됨)으로 프롬프트를 인스턴스화한다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:22]\n' +
      '\n' +
      '도 3: ** 누락된 정보를 찾기 위한 프롬프트.** 실제 비디오 및 질문(오렌지색으로 강조됨)으로 프롬프트를 인스턴스화한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>