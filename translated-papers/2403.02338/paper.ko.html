<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '두 손으로 뚜껑을 비틀고\n' +
      '\n' +
      '토루린1, 자오행인1, 하오지치, 피에터 아벨, 지텐드라 말릭\n' +
      '\n' +
      'UC Berkeley\n' +
      '\n' +
      '[https://toruowo.github.io/bimanual-twist](https://toruowo.github.io/bimanual-twist)\n' +
      '\n' +
      '각주 1: 균등 기여.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '두 개의 다손가락 손으로 물체를 조작하는 것은 많은 조작 작업의 접촉이 풍부한 특성과 고차원 양손 시스템을 조정하는 데 내재된 복잡성에 기인하여 로봇 공학에서 오랜 도전이었다. 본 연구에서는 다양한 병 모양의 물체의 뚜껑을 두 손으로 꼬는 문제를 고려하고, 심층 강화 학습을 사용하여 시뮬레이션에서 훈련된 정책이 실제 세계로 효과적으로 전달될 수 있음을 보여준다. 물리적 모델링, 실시간 인식 및 보상 설계에 대한 새로운 엔지니어링 통찰력을 통해 정책은 보이지 않는 다양한 객체 세트에 걸쳐 일반화 기능을 보여주며 역동적이고 능숙한 행동을 보여준다. 우리의 연구 결과는 심 투 리얼 이전과 결합된 심층 강화 학습이 전례 없는 복잡성의 조작 문제를 해결하기 위한 유망한 접근법으로 남아 있다는 강력한 증거 역할을 한다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '우리는 두 개의 다손가락 로봇 손으로 뚜껑을 비틀거나 제거하는 작업에 대해 연구합니다. 이 일은 실질적으로 중요하고 대단히 흥미롭다. 첫째, 용기의 뚜껑을 비틀거나 제거하는 능력은 유아들이 초기 발달 단계에서 습득하는 중요한 운동 기술이다[36, 54]. 다른 것에 대해, 다중-부분 객체를 조작하기 위한 손가락들의 조정과 같은 이 작업에 요구되는 조작 기술들은 일반적으로 실용적인 태스크들의 대규모 컬렉션에 걸쳐 유용할 수 있다.\n' +
      '\n' +
      '우리의 연구는 순수하게 시뮬레이션과 제로샷에서 능숙하고 역동적인 쌍방향 조작 정책을 학습하고 실제 세계로 전달하는 가능성을 보여준다. 다관절 객체 클래스를 무작위화된 물리적 특성을 가진 단순한 2-부분 객체 모델로 추상화함으로써, 우리는 다양한 모양, 크기, 시각적 외관 및 기타 물리적 특성을 가진 다양한 새로운 객체 집합으로 일반화할 수 있는 정책을 성공적으로 훈련했다(그림 1). 제안하는 방법은 개별 객체의 정확한 모델링이나 객체 속성에 대한 사전 지식을 하드코딩할 필요가 없다. 대신 대규모 강화학습(RL) 훈련을 통해 안정적이고 자연스러운 비틀림 행동이 나타날 수 있도록 한다.\n' +
      '\n' +
      '이러한 기능을 갖춘 시스템을 개발할 때, 우리는 아래에서 개요를 설명하는 많은 새로운 공학 통찰력을 발견합니다.\n' +
      '\n' +
      '**물리적 모델링** 우리의 작업은 손내 조작을 위한 새로운 종류의 물체를 특징으로 한다: 관절형 물체는 나사 구조를 가진 회전 조인트를 통해 연결된 두 개의 강체로 정의된다. 회전 조인트 및 나사산 구조물과의 마찰 및 접촉을 정확하게 모델링하는 것은 전통적으로 사용되어 왔다.\n' +
      '\n' +
      '그림 1: 우리는 두 개의 의인화된 로봇 손이 다양한 관절 물체의 뚜껑을 비틀도록 훈련한다. 제어 정책은 먼저 심층 강화 학습으로 시뮬레이션에서 훈련된 다음 실제 설정으로 제로샷을 전송한다. 단순하고 시뮬레이션된 병 모양의 객체로 훈련된 단일 정책은 크게 다른 물리적 특성(예: 모양, 크기, 색상, 재료, 질량)을 가진 실제 객체로 일반화된다. 각 객체의 길이, 지름(또는 대각선 길이) 및 질량은 하단에 주석이 달려 있습니다. 자세한 시각화는 동영상을 참조하십시오.\n' +
      '\n' +
      '로봇 공학 시뮬레이션[33]에서 어려운 도전. 이를 해결하기 위해, 우리는 뚜껑과 병 모양의 물체 사이의 상호 작용을 모델링하기 위한 브레이크 기반 디자인을 소개한다. 이 설계는 실제 물리적 역학에 대한 높은 충실도를 유지하면서 시뮬레이션하는 것이 빨라 효율적인 정책 학습과 성공적인 심투리얼 전이가 가능하다.\n' +
      '\n' +
      '**지각** 우리는 처음에 뚜껑 비틀기와 같은 미세한 접촉 풍부 조작 작업이 물체 상태에 대한 정확한 지각 정보를 요구해야 한다고 가정한다. 놀랍게도, 기성품 객체 분할 및 추적 도구에서 추출된 2점 희소 객체 표현은 인식 문제를 해결하기에 충분하다. 간단한 도메인 랜덤화 기법을 이용하여 폐색 및 카메라 잡음에 강인한 정책을 학습한다. 이 발견은 복잡한 양손 조작 작업에 최소한의 객체 인식 정보가 적절할 수 있음을 시사한다.\n' +
      '\n' +
      '**Reward Design.** 이전에 In-Hand Reorientation과 같은 작업에 대한 수행적 보상 디자인은 우리의 작업에 쉽게 적용될 수 없는데, 그 이유는 이러한 작업이 두 손으로 다관절체가 아닌 한 손으로 단일 부분 강체를 조작하는 데 중점을 두고 있기 때문이다. 이 작업을 해결하는 것은 더 복잡하고 정확한 접촉(예를 들어, 뚜껑을 잡기 위해 두 손을 사용하는 것)을 포함하기 때문에 더 어렵다. 이 문제를 해결하기 위해 우리는 로봇 손가락에 자연스러운 뚜껑 비틀림 행동을 산출하는 간단한 키포인트 기반 접촉 보상을 발견한다.\n' +
      '\n' +
      '시뮬레이션과 실제 세계 모두에서 여러 가지 제어 실험을 수행한다. 실증 분석을 통해 시뮬레이션 모델링, 인지 모듈 및 보상 설계가 뚜껑 비틀림의 원하는 행동으로 안정적으로 이어질 수 있음을 검증한다. 마지막으로 성공적인 정책은 시뮬레이션에서 모양, 크기 및 질량과 같은 다양한 물리적 특성을 가진 테스트 대상 전반에 걸쳐 자연스러운 행동을 나타낸다. 더욱이, 학습된 정책은 뚜껑이 제거될 수 있는 광범위한 새로운 가정용 객체에 제로-샷될 수 있고(그림 1), 섭동에 대해 견고하다.\n' +
      '\n' +
      '## II Background\n' +
      '\n' +
      '###_Bimanual Manipulation__을 위한 고전적 접근법\n' +
      '\n' +
      '수십 년 동안, 양손 조작의 문제는 로봇 공학에서 매혹적인 미해결 과제로 남아 있었다[53, 48, 49, 6, 23, 5]. 다손 로봇 핸드는 이론상 양손 로봇 시스템에 대한 자연스러운 선택인 것처럼 보이지만, 고차원 작동 공간을 위한 제어기 설계는 기계적 설계 및 신뢰할 수 있는 작동의 어려움으로 인해 여전히 열린 문제로 남아 있다. 따라서 양손 조작의 대부분의 이전 작업은 간단하고 내구성이 뛰어난 평행 턱 그리퍼를 엔드 이펙터로 사용한다. 예를 들어, Caccavale et al. [5]는 임피던스 제어와 함께 두 개의 암을 사용하여 운반 및 움직이는 물체를 시연한다. Sarkar et al. [45]는 롤링 컨택트를 이용하여 오브젝트 상태를 제어하는 것을 나타낸다. 우리의 작업은 대부분의 이전 작업과 달리 두 개의 다 손가락 손으로 쌍방향 조작에 중점을 둡니다.\n' +
      '\n' +
      '조작을 위해 두 개의 다손가락 손을 사용하는 이전 작업이 있지만 접근 및 결과 측면에서 우리의 작업과 크게 다르다. Vahrenkamp et al. [53]은 다중 손가락 손들을 사용하여 큰 물체들에 대한 파지들을 구축하는 데 사용될 수 있지만, 어떠한 실제-세계 결과들을 입증하지 않는 쌍방향 파지 플래너를 제시한다. Platt et al. [39]는 두 개의 3-손가락 로봇 핸들을 사용하여 객체 재구성을 달성하지만, 로우-레벨 제어기들의 시퀀스를 계획함으로써 그렇게 한다. Steffen et al. [50]은 우리의 관심 태스크와 유사한 나사 체결 태스크를 연구하지만, 제어 모션 시퀀스를 데이터 장갑으로 캡처된 인간 모션 데이터에 적용되는 커널 접근법과 합성한다. 우리의 작업은 설계자, 정확한 객체 모델 또는 인간 데이터에 의존하지 않기 때문에 쌍방향 다중 손가락 조작 문헌에서 누락된 공간을 채운다. 대신, 우리는 무작위 환경에서 RL을 사용하여 고차원 액션 공간에서 직접 작업한다.\n' +
      '\n' +
      '###_Bimanual Manipulation을 위한 학습 접근법_\n' +
      '\n' +
      '최근에는 학습 알고리즘 및 컴퓨팅 인프라의 발전으로 인해 학습 기반 방법으로 쌍방향 조작이 보다 활발하게 연구되고 있다. 이러한 학습 기반 접근법은 일반적으로 두 가지 유형으로 분류될 수 있다: 1) 실세계 데이터로부터 학습하는 것; 2) 시뮬레이션에서 학습한 다음, 실세계로 이전하는 것(심-투-리얼)이다.\n' +
      '\n' +
      '**Real-World Data.** 현실 세계의 강화 학습(RL)에서 빠른 진전이 이루어졌다. Zhang et al. [58]은 비교적 모션 프리미티브들을 갖는 야채 절단을 위한 운동 프리미티브들을 체인하는 것을 학습한다; 많은 작업 난이도는 전문화된 엔드-이펙터들 [1, 14]의 사용을 통해 바이패스된다. Chiu et al. [12]는 샘플링 기반 플래너와 RL을 통합하여 두 개의 그리퍼로 정밀한 바늘 조작을 학습한다. 그러나 이러한 작업은 표본 비효율성 또는 휴리스틱 기반 액션 프리미티브를 정의해야 하는 필요성으로 인해 고차원 액션 공간으로 쉽게 확장할 수 없다.\n' +
      '\n' +
      '양손 조작의 가장 최근의 성공은 시연으로부터 학습함으로써 달성된다[59, 47, 51]. 그러나 지금까지의 성공은 평행 턱 그리퍼와 같은 단순한 엔드 이펙터로 크게 제한된다. 한 가지 이유는 다손가락 로봇 핸드의 고품질 시연 데이터가 부족하기 때문이다[24]. 이러한 손 하드웨어의 가용성은 정교한 데이터 수집 기반 시설은 말할 것도 없고 그 자체가 제한적이다. 두 개의 팔[15, 26, 28] 또는 다중 손가락 손으로 데모 데이터 수집을 개선하는 것을 목표로 하는 여러 작업이 존재하지만[2, 3, 16, 42, 43], 지연 및 리타겟팅 오류는 실제 적용 가능성과 확장성을 제한한다. 제안된 방법은 시뮬레이션에서 RL을 사용하므로 시연 접근법에서 학습이 직면한 하드웨어 및 데이터 수집 인프라 문제에 제한되지 않는다.\n' +
      '\n' +
      '**Sim-to-Real.** 로봇 공학에 대한 sim-to-real 접근법, 즉 시뮬레이션에서의 학습 정책을 실제 세계로 이전하는 것에 대한 관심이 증가하고 있다 - 최근 몇 년 동안 운동[19, 25, 32]에서 조작[8, 17, 34, 40]에 이르는 몇 가지 주목할 만한 성공에 의해 자극되었다. 그러나 조작의 기존 작업은 대부분 단일 다손가락 손[7, 10, 21, 38, 41, 44, 52, 55, 56] 또는 더 간단한 엔드 이펙터를 가진 두 팔[20, 27, 30]로 수행된다. Chen et al. [9] 및 Zakka et al. [57]은 손재주가 있는 쌍안 작업을 특징으로 하는 반면, 시뮬레이션 결과만 보여준다. 아마도 우리와 가장 관련이 있는 작품은 황 등[18]일 것이며, 여기서 저자들은 두 손재주를 사용하여 물건을 던지고 잡는 것을 시연한다. 그러나 우리의 작업은 훨씬 더 접촉이 풍부하고 항상 물체 안정성을 유지하기 위해 실질적으로 더 도전적인 양손 조정(bimanual coordination)이 필요하다. 우리의 작업은 두 개의 다손가락 손을 사용하여 극도로 접촉이 풍부한 작업에 대한 조작을 고려함으로써 문헌과 구별되며, 특히 우리가 작업하는 대상은 두 부분으로 구성된 관절체이며 이전 작업은 단일 부분으로 구성된 강체에 중점을 둔다.\n' +
      '\n' +
      '## III 과제 정형화\n' +
      '\n' +
      '컨테이너 객체의 비틀림 뚜껑은 복잡한 손내 조작 프로세스로 여러 손가락의 동적 능숙함과 두 손 사이의 정밀한 조정이 필요하다. 아래에서는 이 작업에서 고려되고 있는 구체적인 과제를 정의한다.\n' +
      '\n' +
      '**작업 초기화.** 관심 객체 각각은 두 개의 단단하고 원기둥에 가까운 부분("몸체" 및 "뚜껑")으로 구성되며, 두 부분은 연속적인 회전 조인트를 통해 연결되어 서로에 대해 회전할 수 있다. 양방향 비틀기 기능을 더 잘 벤치마킹하기 위해 무한히 비틀 수 있는 뚜껑이 있는 관절형 병 클래스를 고려한다(자세한 내용은 섹션 IV-B 참조). 두 로봇 손은 위쪽을 향하는 손바닥을 가진 정적 포즈로 초기화된다. 각 에피소드의 시작 부분에서 병 모양의 물체를 부드럽게 떨어뜨리거나 손가락에 놓는다. 객체의 초기 포즈는 고정된 기본 포즈로의 병진 및 회전 모두에서 랜덤화되고, 손의 초기 관절 위치는 가우시안 노이즈를 추가하여 미리 정의된 표준 포즈에 대해 랜덤화된다. 작업 초기화 시 안정적인 파지 구성을 가정하지 않기 때문에 제어 정책은 연속적인 조작을 수행하기 위해 객체를 안정적인 위치에 배치하기 위해 파지 내 재배향을 학습할 필요가 있다.\n' +
      '\n' +
      '**과제 목표.** 이 과제의 목표는 가능한 한 한 방향으로 물체의 회전축을 중심으로 뚜껑을 비틀어 놓는 것이다; 이 과정 동안, 물체는 항상 손에 있어야 한다. 이를 달성하는 것은 섬세한 움직임의 시퀀스를 수반한다: 1) 초기화 후, 로봇 핸드는 병을 적절한 포즈로 단단히 잡고 약간 회전시켜야 한다; 2) 물체 뚜껑에 더 가까운 손은 비틀기 동작을 개시하기 위해 뚜껑 주위에 손가락을 놓아야 한다; 3) 한 손이 뚜껑을 비틀면서 물체가 떨어지지 않도록 두 손이 조정해야 한다. 이 작업에서 발생하는 운동 기술은 다른 많은 가정용 물체, 특히 루빅의 큐브, 전구 및 항아리와 같은 회전 관절을 가진 물체를 조작하는 데 필요한 기술에 대한 일반적인 추상화 역할을 할 수 있다.\n' +
      '\n' +
      '**과제 복잡성.** 이 과제 고유의 복잡성은 다음의 세 가지 측면으로 요약될 수 있다.\n' +
      '\n' +
      '_1) In-Hand Perception 난이도._1) 손에 있는 물체를 둘러싸는 손가락으로 인해 조작 중에 무거운 폐색이 발생할 수 있다. 이러한 폐색 및 지각적 잡음에 대해 강건한 쌍방향 조작을 수행할 수 있는 실제 로봇 시스템을 구축하는 방법은 여전히 미해결 문제로 남아 있다.\n' +
      '\n' +
      '_2) 미세접촉._2 이 작업은 두 손이 균형을 유지하기 위해 조정해야 합니다. 접촉점이나 힘이 잘못되면 병이 떨어질 수 있습니다.\n' +
      '\n' +
      '_3) 고차원 제어._3) 이 시스템은 자유도가 32\\(32\\)이며, 조작 과정에서 거의 모든 손가락이 맞물린다. 이를 위해서는 많은 수의 강체 간의 매우 정밀한 조정이 필요하며, 이는 기존의 학습 기반 방법에 대한 알려져 있는 과제이다.\n' +
      '\n' +
      '## IV 시스템 설정\n' +
      '\n' +
      '양방향 조작 및 특정 작업 선택에서 앞서 언급한 문제를 해결하기 위해 심층 RL 접근 방식을 기반으로 심투리얼 파이프라인을 개발한다. 이 섹션에서는 전체 시스템 설정을 설명합니다.\n' +
      '\n' +
      '### _Real-world System_\n' +
      '\n' +
      '**하드웨어 설정.** 도 1에 도시된 바와 같이, 실험을 위해 원익 로보틱스의 16-DoF 알레그로 핸즈 2개를 사용한다. 각 알레그로 핸드는 고정된 UR5e 암에 장착됩니다. 단일 RealSense D435 깊이 카메라를 이용하여 영상 정보를 제공하고, 이로부터 객체 상태 정보를 추출한다. 리눅스 워크스테이션을 통해 제어명령을 10(10,\\mathrm{Hz}\\)의 주파수로 로봇에 전송한다.\n' +
      '\n' +
      '**지각.** 도 3은 우리의 지각 파이프라인의 개요를 나타낸다. RL 에이전트의 픽셀로 직접 사용하는 대신\n' +
      '\n' +
      '도 3: 실시간 인지 시스템. _ Top_: scene overview. _ Bottom_: RGB 프레임들로부터 객체 부분들을 분할 및 추적하고(왼쪽), 마스크 중심들을 객체 부분 중심들로 취하고(중간), 카메라로부터의 잡음 깊이 정보를 이용하여 3D 객체 키포인트들을 추정한다(오른쪽).\n' +
      '\n' +
      '도 2: 병 URDF 모델. 병 모델의 핵심 디자인은 브레이크 링크와 프리즘식 "브레이크" 조인트입니다. 브레이크 링크는 베이스 링크와 리드 링크 사이의 정적 마찰을 시뮬레이션하기 위해 리드 링크에 대해 지속적으로 가압한다. 브레이크가 없으면 뚜껑 링크가 손가락과 접촉하지 않아도 쉽게 회전하기 시작할 수 있습니다.\n' +
      '\n' +
      '정책 입력, 우리는 객체 분할 및 추적을 통해 제어를 위한 이미지에서 객체 키포인트를 추출할 것을 제안한다. 구체적으로, Segment Anything 모델[22]을 사용하여 병 본체와 첫 번째 프레임의 뚜껑에 대해 두 개의 개별 마스크를 생성하고 XMem[11]을 사용하여 나머지 모든 프레임 전체에 걸쳐 마스크를 추적한다. 병 몸체와 뚜껑의 3차원 질량 중심 좌표를 근사하기 위해 이미지 평면에서 마스크의 중심 위치를 계산한 다음 깊이 카메라로부터 잡음이 있는 깊이 판독값을 얻어 해당 3차원 위치를 복원한다. 인식 파이프라인은 신경망 정책의 제어 주파수와 일치하도록 \\(10\\,\\mathrm{Hz}\\)에서 실행된다.\n' +
      '\n' +
      '### _Bottle Models_\n' +
      '\n' +
      '** 시뮬레이트된 병.** 뚜껑-트위스팅 태스크를 시뮬레이팅하는 데 있어서의 중심 과제는 병 몸체와 뚜껑 사이의 마찰, 특히 정적 마찰을 적절하게 모델링하는 방법이다. 이러한 유형의 물리적 힘을 시뮬레이션하는 것은 그래픽에서 오랜 문제였다[33]. 우리는 물리적 시뮬레이션 동안 충실도와 속도 사이의 균형을 맞추는 간단한 모델링 근사치를 설계한다. 결과적으로, 성공적인 정책은 현실 세계로 이전하기에 충분히 현실적이면서도 합리적인 양의 벽-시계 시간 내에 시뮬레이션에서 학습될 수 있다. 우리는 그림 2에서 병과 같은 물체 모델을 보여준다. 우리의 디자인은 (프리즘 조인트를 통해) 병 뚜껑을 지속적으로 누르는 특별한 브레이크 링크를 특징으로 한다. 이것은 병 본체(베이스 링크)와 뚜껑(리드 링크) 사이의 마찰력을 인위적으로 발생시켜 그들 사이의 상대 회전을 방지하는데, 뚜껑이 나사로 고정된 병과 유사하다.\n' +
      '\n' +
      '**실세계 다관절 병** 우리 사물의 모양과 크기를 쉽게 변경하고 재현성을 장려하기 위해 CAD 소프트웨어에서 다관절 병 모양의 사물 세트를 설계하고 다양한 색상과 재료로 3D 인쇄합니다. 뚜껑을 무한히 회전시켜 이상적인 회전 관절을 시뮬레이션할 수 있습니다. 도 5(중간)에 주문형 병의 예를 보여드립니다. 우리는 또한 그림 5(아래)에 묘사된 바와 같이 방법의 일반화 가능성을 입증하기 위해 다양한 물리적 특성을 가진 가정용 병 모양의 물체를 수집한다.\n' +
      '\n' +
      '트위스트 뚜껑에 대한 V 학습\n' +
      '\n' +
      '양손 손재주 조작은 매우 복잡한 손-물체 접촉을 포함하며 전통적인 방법으로 해결하기 어려운 상태로 남아 있다. 이 작업에서 우리는 RL을 통해 제어 문제를 해결한다. 본 논문에서는 제어 문제를 부분 관측 가능한 마르코프 결정 과정(\\(\\mathcal{M}=(\\mathcal{S},\\mathcal{O},\\mathcal{A},\\mathcal{R},\\mathcal{P})\\)으로 공식화하고, 여기서 \\(\\mathcal{S}\\)은 상태 공간, \\(\\mathcal{A}\\)은 동작 공간, \\(\\mathcal{O}\\)은 관찰 공간, \\(\\mathcal{R}\\)은 보상 함수, \\(\\mathcal{P}\\)은 환경 역학이다. 제어정책은 관측치(o_{t}\\in\\mathcal{O}\\)가 주어졌을 때 누적 보상을 최대화하기 위한 액션(a_{t}\\in\\mathcal{A}\\)을 생성한다.\n' +
      '\n' +
      '### _관찰 및 Action_\n' +
      '\n' +
      '각 시간 단계에서 제어 정책은 고유수용성 손 관절 위치\\(q_{t}\\), 병 받침대와 뚜껑의 추정된 질량 중심 3D 위치 및 이전에 명령된 목표 관절 위치\\(\\tilde{q}_{t}\\)의 환경으로부터 다음과 같은 정보를 관찰한다.\n' +
      '\n' +
      '**액션 스페이스** 임피던스 PD 컨트롤러를 사용하여 로봇 핸드를 구동합니다. 제어 정책은 상대 목표 조인트 위치를 작용(a_{t}\\)으로 생성하며, 이는 현재 목표 조인트 위치(q_{t}\\)에 추가되어 다음 목표(\\(\\tilde{q}_{t+1}=\\tilde{q}_{t}+\\eta\\mathrm{EMA}(a_{t})\\)를 생성한다. (\\eta\\)는 스케일링 팩터이다. 우리는 부드러운 움직임을 생성하기 위해 지수 이동 평균(EMA)으로 동작을 부드럽게 한다는 점에 유의한다. 다음 목표 위치는 각 관절에 토크를 생성하기 위해 PD 제어기로 보내진다.\n' +
      '\n' +
      '### _Reward Design_\n' +
      '\n' +
      '보상 함수는 로봇의 행동을 정의하는 데 중요한 역할을 한다. 양손 조작 시스템은 단일 손 시나리오에 비해 더 높은 액션 공간 차원과 훨씬 더 복잡한 접촉 패턴을 갖는다. 이는 RL 탐색 프로세스에 내재된 도전을 확대하여 보다 정교한 보상 설계의 필요성을 강조한다. 경성 탐색 문제에 접근하는 한 가지 방법은 내재적 보상[4, 29]을 추가하는 것이지만, 우리는 이 작업에 대해 세밀한 행동 인식 보상 설계를 제안한다. 우리의 보상 함수는 다음과 같은 용어들을 포함한다:\n' +
      '\n' +
      '**트위스팅 리워드**트위스팅 리워드**트위스팅 리워드를 *Twisting Reward *\n' +
      '\n' +
      '\\[r_{\\mathrm{twisting}}=\\Delta\\theta=q_{bottle}^{t+1}-q_{bottle}^{t}, \\tag{1}\\]\n' +
      '\n' +
      '이는 1단계 실행 중 뚜껑의 회전 각도이다.\n' +
      '\n' +
      '**Finger Contact Reward.** 손가락들의 상호작용을 안내하기 위해 객체 상에 위치하는 키포인트들의 세트를 정의함으로써 자연스러운 객체 조작 행동을 장려한다(도 4). 병받침과 뚜껑에 각각 부착된 키포인트\\(\\mathbf{X}^{L}\\in\\mathbb{R}^{n\\times 3}\\)과 \\(\\mathbf{X}^{R}\\in\\mathbb{R}^{m\\times 3}\\)을 정의한다. 그런 다음 손가락 접촉 보상을 다음과 같이 정의합니다.\n' +
      '\n' +
      '[r_{\\mathrm{contact}}=\\sum_{i}\\left[\\frac{1}{1+\\alpha d(\\mathbf{X}^{L},\\mathbf{F}_{i}^{L}}}+\\frac{1}{1+\\alpha d(\\mathbf{X}^{R},\\mathbf{F}_{i}^{R}}}}}}}\\right], \\tag{2}\\tag{2}}}}\n' +
      '\n' +
      '여기서 \\(\\mathbf{F}^{L}\\in\\mathbbb{R}^{4\\times 3}\\) 및 \\(\\mathbff{F}^{R}\\in\\mathbbb{R}^{4\\times 3}\\)은 좌우 손가락 끝의 위치이고, \\(\\alpha\\)은 스케일링 하이퍼파라미터이고, \\(d\\)은\n' +
      '\n' +
      '그림 4: 보상 디자인의 일러스트레이션입니다. 우리의 과제별 보상은 손가락 접촉 보상(노란색 화살표), 비틀림 보상(흰색 화살표), 포즈 보상(파란색 화살표)의 세 가지 용어를 포함한다. 특히, 키포인트 기반 손가락 보상은 로봇이 손가락 끝과 기준 접촉 키포인트 사이의 거리를 최소화하도록 권장하는 원하는 나사 결합 행동을 학습하는데 중요하다.\n' +
      '\n' +
      '로 정의되는 거리 함수\n' +
      '\n' +
      '\\[d(\\mathbf{A},\\mathbf{x})=\\min_{i}\\|\\mathbf{A}_{i}-\\mathbf{x}\\|_{2}. \\tag{3}\\]\n' +
      '\n' +
      '따라서 각 손가락 끝이 가능한 한 키포인트 중 하나에 가깝게 유지되도록 권장합니다. 병 받침대와 뚜껑 주변의 키포인트를 적절히 선택함으로써 자연스러운 파지 및 뚜껑 비틀림 행동을 이끌어낼 수 있습니다.\n' +
      '\n' +
      '또한 병의 주축(\\(\\mathbf{x}_{axis}\\)과 미리 정의된 방향(\\(\\mathbf{v}\\)으로 정렬된 병의 주축(\\(\\mathbf{x}_{axis}\\)을 격려하기 위한 포즈 매칭 보상항을 소개한다. 이 용어는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[r_{\\mathrm{pose}}=-\\arccos(\\langle\\mathbf{x}_{axis},\\mathbf{v}\\rangle). \\tag{4}\\]\n' +
      '\n' +
      '**기타 규칙화** 위의 세 가지 보상 조건은 우리 과제의 목적을 명시한다. 이 세 가지 보상 외에도 작업 벌칙과 큰 육포 동작을 벌하기 위한 액션 벌칙을 포함하여 이전 작업 [55]에서와 같은 또 다른 몇 가지 정규화 용어를 소개한다.\n' +
      '\n' +
      '\\[r_{\\mathrm{action}}=-\\|a_{t}\\|^{2}, \\tag{5}\\]\n' +
      '\n' +
      '\\[r_{\\mathrm{work}}=-\\langle|\\tau|,|\\hat{q}_{t}|\\rangle. \\tag{6}\\]\n' +
      '\n' +
      '상기 보상 함수는 상기 용어들의 가중 합이다:\n' +
      '\n' +
      '\\[r=\\alpha_{1}r_{\\mathrm{contact}}+\\alpha_{2}r_{\\mathrm{twist}}+\\alpha_{3}r_{\\mathrm{pose}+\\alpha_{4}r_{\\mathrm{action}}+\\alpha_{5}r_{\\mathrm{work}. \\tag{7}\\\n' +
      '\n' +
      '** 재설정 전략** 학습 문제의 높은 차원성으로 인해 로봇이 객체와 상호작용하는 많은 가능한 방법이 있으며, 이 중 대부분의 모드는 복구를 넘어 객체를 떨어뜨리는 것과 같은 고장으로 이어진다. 이러한 모드를 탐구하는 것은 의미 있는 학습 진전으로 이어지는 경우가 거의 없다. 이 문제를 피하기 위해 우리는 몇 가지 조기 종료 기준을 도입한다. 가장 중요한 것은 로봇 손이 짧은 시간 제한 내에 양손 비틀기를 위해 병을 원하는 포즈로 회전시키지 못하면 에피소드를 재설정한다는 것이다. 또한, 두 손의 손가락 끝이 병을 손바닥으로 재배치할 수 없이 낮은 위치에서 꼬집을 수 있기 때문에 병의 \\(z\\) 위치가 특정 임계값 미만일 때 재설정한다.\n' +
      '\n' +
      '### _Domain Randomization_\n' +
      '\n' +
      '우리는 물리적 랜덤화와 비물리적 랜덤화를 포함하여 제로 샷 심투리얼 전송을 보장하기 위해 광범위한 도메인 랜덤화를 적용한다. 물리적 무작위화는 물체 마찰, 질량, 스케일의 무작위화를 포함한다. 또한 시뮬레이터에 의해 구현되지 않은 물리적 효과를 시뮬레이션하기 위해 물체에 무작위 힘을 적용한다. 비물리적 랜덤화는 관찰(예를 들어, 관절 위치 측정 및 검출된 물체 위치) 및 액션에서의 잡음을 모델링한다. 우리의 무작위화 속성과 매개변수에 대한 요약은 표 I에 나와 있다.\n' +
      '\n' +
      '### _Training_\n' +
      '\n' +
      '우리는 RL 정책을 학습하기 위해 근위 정책 최적화(PPO) 알고리즘을 사용한다. 우리는 이점 클리핑 계수 \\(\\epsilon=0.2\\); 수평선 길이 16, \\(\\gamma=0.99\\) 및 일반화된 이점 추정량 \\(\\tau=0.95\\)을 사용한다. 정책 네트워크는 ELU [13] 활성화를 갖는 3-계층 MLP이며, 그의 은닉 층은 [256, 256, 128]이다. 정책 네트워크는 학습 가능한 상태 독립 표준 편차를 갖는 가우시안 분포를 출력한다. 값 네트워크는 또한 ELU 활성화를 갖는 MLP이며, 그의 은닉 층은 [512, 512, 512]이다. 우리는 KL 임계값이 0.016[19]인 적응 학습률을 사용한다. 훈련하는 동안 상태 입력, 값 및 이점을 정규화한다. 기울기 규범은 1.0, 미니배치 크기는 8192로 설정하였으며, 정책 및 가치 네트워크에는 비대칭 관측[37]을 사용하였으며, 가치 네트워크 입력에는 특권 정보(무작위 물리 파라미터, 큐브 속도 및 각속도, 관절 속도 및 각속도)를 추가하였다. 이 권한 있는 정보는 정책 네트워크에서 액세스할 수 없습니다.\n' +
      '\n' +
      '## VI 모의실험\n' +
      '\n' +
      '우리는 시뮬레이션에서 다음과 같은 질문을 연구한다.\n' +
      '\n' +
      '_Reward Design.___ 양손 조작에서 원하는 비틀림 행동을 이끌어내기 위한 키포인트 기반 보상은 얼마나 중요한가? 최근 작품에서 사용되는 다른 보상 함수에 비해 얼마나 효과적인가?\n' +
      '\n' +
      '_Perception.____ 이 과제를 해결하는 데 시각적 정보가 얼마나 중요한가요? 희박하고 키포인트 표현이 여러 개체를 처리할 수 있는 정책을 학습하기에 충분한가요?\n' +
      '\n' +
      '### _Setup_\n' +
      '\n' +
      '**객체 집합.** 시뮬레이션된 실험에서 우리는 훈련과 평가 모두에 다양한 종횡비를 갖는 시뮬레이션된 원통형 병의 모음을 활용한다. 일부 샘플은 그림 5에서 시각화되었다. 시뮬레이션에서는 1) 모든 객체가 사용되는 다중 객체 설정과 2) 데이터 세트의 평균을 나타내는 중간 크기의 병만 사용하는 단일 객체 설정의 두 가지 설정을 고려한다.\n' +
      '\n' +
      '**평가 메트릭.** 성능 평가를 위한 다음 메트릭을 소개한다:\n' +
      '\n' +
      '_각도 변위(AD)_는 뚜껑이 꼬인 도의 총 개수이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Object: Mass (kg) & [0.03, 0.1] \\\\ Object: Friction & [0.5, 1.5] \\\\ Object: Shape & \\(\\times\\mathcal{U}(0.95,1.05)\\) \\\\ Object: Initial Position (cm) & \\(\\mathcal{U}(-0.02,0.02)\\) \\\\ Object: Initial \\(z\\)-orientation & \\(\\mathcal{U}(-0.75,0.75)\\) \\\\ Hand: Friction & [0.5, 1.5] \\\\ \\hline PD Controller: P Gain & \\(\\times\\mathcal{U}(0.8,1.1)\\) \\\\ PD Controller: D Gain & \\(\\times\\mathcal{U}(0.7,1.2)\\) \\\\ \\hline Random Force: Scale & 2.0 \\\\ Random Force: Probability & 0.2 \\\\ Random Force: Decay Coeff. and Interval & 0.99 every 0.1s \\\\ \\hline Bottle Pos Observation: Noise & 0.02 \\\\ Joint Observation Noise. & \\(+\\mathcal{N}(0,0.4)\\) \\\\ Action Noise. & \\(+\\mathcal{N}(0,0.1)\\) \\\\ \\hline Frame Lag Probability & 0.1 \\\\ Action Lag Probability & 0.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 I: 도메인 랜덤화 설정.\n' +
      '\n' +
      'TTF(Time-to-Fail)_는 병이 파지되는 순간부터 손에서 미끄러지거나 박히는 시점까지 측정된 주기이다.\n' +
      '\n' +
      '**기준.** 우리의 정책을 다음의 기준들과 비교한다:\n' +
      '\n' +
      '비전 없는__Policy without Vision.___Policy 이것은 시각적 정보(병 키포인트)가 없는 신경망 정책이다. 우리는 이것을 시각의 중요성을 평가하기 위해 사용한다.\n' +
      '\n' +
      '연락처 보상이 감소된___Policy with Reduced Contact Reward.___Policy 이 정책을 훈련할 때, 우리는 제안된 손가락 접촉 보상의 강도를 줄인다. 이를 활용하여 정책 학습과 정책 행동 형성에 있어 우리의 접점 보상의 역할을 연구한다.\n' +
      '\n' +
      '보행 보상을 갖는__Policy with Gait Reward.__Policy 이 정책을 훈련할 때, 우리는 접촉 보상을 손 내 재배향 작업(예를 들어 [40])에 사용되는 것과 유사한 보행 제약 보상 함수로 대체한다. 이 기준선은 정성적 분석에만 사용됩니다.\n' +
      '\n' +
      '### _Main Results_\n' +
      '\n' +
      '**Reward Design.** 우리는 먼저 우리의 접근법을 감소된 손가락 보상 기준선과 비교한다(도 6). 손가락 접촉 보상의 규모를 줄인 후, 학습된 정책은 원하는 뚜껑 비틀기 기술을 마스터하지 못하고 일반적으로 성능이 낮다. 우리는 뚜껑 비틀림의 움직임이 물체를 잡기 위해 매우 특정한 포즈 패턴을 요구하기 때문이라고 가정한다; 그러한 포즈 패턴을 명시적으로 장려하지 않고(예를 들어, 접촉 모드를 통해), RL 탐사는 가용 훈련 시간 내에 풀 수 없을 정도로 어렵게 된다. 또한 손가락 접촉 보상의 강도와 1) 학습 중 샘플 효율성과 2) 학습된 정책의 수행(AD 점수에 의해 반영됨) 사이의 양의 상관 관계를 관찰한다.\n' +
      '\n' +
      '**비전 vs No Vision.** 우리는 또한 우리의 고려된 과제를 해결하는 데 있어 비전 양식의 중요성을 연구합니다. 기존 연구들은 암시적 촉각 감지(관절 고유 감각 데이터)를 통해 일정한 회전 행동을 달성할 수 있음을 보여주지만[40], 쌍방향 뚜껑 비틀기 작업에 대해서도 동일한 결론을 도출할 수 있는지는 불분명하다. 우리의 경험적 결과는 단일 및 다중 객체 설정 모두에서 비젼 기준선이 전체 방법보다 실질적으로 더 나쁘다는 것을 보여준다. 이는 성공적인 뚜껑 비틀기를 위해서는 병 키포인트의 위치에 대한 지식이 필수적임을 시사한다.\n' +
      '\n' +
      '**단일 객체 대 다중 객체.** 두 가지 객체 설정으로 RL 교육을 실행합니다. (1) 단일 병 모양 객체를 사용하는 것과 (2) 병 받침대와 뚜껑 사이의 비율이 더 많은 여러 병 모양 객체를 사용하는 것. 두 가지 설정은 전문화와 일반화 사이의 절충점을 제시하는데, 단일 객체 시나리오에서 정책은 성공적인 행동을 더 쉽게 배울 수 있지만 보이지 않는 객체에 대해 일반화하기는 더 어려울 수 있으며 그 반대의 경우도 마찬가지이다. 놀랍게도, 우리는 다중 객체 훈련이 단일 객체 훈련에 비해 약간 더 나은 성능을 산출한다는 것을 관찰한다. 본 연구에서는 다중 객체(multi-object)가 훈련 중 쉽고 단단한 객체 인스턴스를 모두 포괄하는 객체 커리큘럼을 도입함으로써 뚜껑 꼬임 행동을 탐색하는 것을 더 쉬운 프로세스로 만들 수 있다고 가정한다.\n' +
      '\n' +
      '### _Qualitative Results_\n' +
      '\n' +
      '본 절에서는 이들의 유발 행동을 조사하여 보상 설계를 연구한다. 구체적으로, 우리는 우리의 방법을 감소된 접촉 보상 기준선 및 획득 보상 기준선과 비교한다. 각 방법으로 훈련된 정책에서 궤적을 시각화하는 것은 그림 7과 같다. 손가락 접촉 보상 없이 훈련된 정책은 매우 불안정한 파지 또는 나사 동작을 나타내며, 시뮬레이션에서는 행동이 부자연스럽게 보일 뿐만 아니라 실제 설정으로 완전히 전달되지 않는다. 각각이 높은 자유도를 가진 두 손을 사용하면 손 내 물체 조작을 위한 방대한 검색 공간이 발생한다고 가정하며, 따라서 행동 인식 접촉 보상과 같은 작업 전적을 도입하는 것은 검색 공간을 줄이고 자연스러운 행동을 이끌어내는 데 필수적이다.\n' +
      '\n' +
      '## VII 현실 세계 실험\n' +
      '\n' +
      '이 절에서는 먼저 우리의 최종 학습된 정책이 제로 샷 방식으로 실제 세계로 직접 이전될 수 있음을 입증한다. 그런 다음 몇 가지 주요 설계 선택이 심 투 리얼 전송 성공에 미치는 영향을 조사한다. 마지막으로, 우리는 우리의 정책이 일반화하는 능력과 다양한 종류의 교란에 대한 견고성을 조사한다.\n' +
      '\n' +
      '### _Setup_\n' +
      '\n' +
      '정량적 평가를 위해 5개의 서로 다른 관절 병 객체에 대한 정책의 심투리얼 전달 능력을 평가한다(그림 5). 이 중 4개는 유통중입니다.\n' +
      '\n' +
      '도 5: 실험에 사용된 병들. _ Top_: 시뮬레이션된 병들_ Middle_: Custom-made bottles(in-distribution except the rightmost square bottle) _ Bottom_: 가정용 물건병(out-of-distribution).\n' +
      '\n' +
      '(원형 몸체 병) 및 하나는 훈련 분포(사각형 몸체 병) 외부에 있습니다. 우리는 20번의 시험에서 AD와 TTF를 모두 측정하며 각 시험은 최대 30초 동안 지속된다. 평가된 각 방법에 대해 10개의 다른 무작위 시드에 대해 훈련된 10개의 정책 중 3개의 최상의 정책을 선택한다. 병이 손바닥에서 떨어지면 재판을 마칩니다.\n' +
      '\n' +
      '**기준.** 파이프라인의 여러 주요 설계 선택 효과를 연구하기 위해 최종 정책을 다음 기준선과 비교합니다.\n' +
      '\n' +
      '_Open-loop Replay Policy (Replay). 우리는 시뮬레이션에서 학습된 정책의 성공적인 시행을 기록하고 실제 로봇을 다시 재생할 궤적을 무작위로 선택한다. 이 베이스라인은 작업이 결정론적 모션 패턴에 의해 해결될 수 있는지 여부를 평가하는 데 사용된다.\n' +
      '\n' +
      '_Policy without Vision (No-Vis) 이 기준 정책은 객체 상태에 대한 정보 없이 고유수용성 상태 정보만을 입력으로 한다.\n' +
      '\n' +
      '_Policy without Asymmetric Training (No-Asym). 우리는 비대칭 PPO 없이 정책이 훈련되는 기준선과 비교하고 가치 네트워크에 추가 특권 정보를 도입하는 것이 이전 성능에 영향을 미칠지 여부를 평가한다.\n' +
      '\n' +
      '__Larger Policy Network Size (Large).__Larger Policy Network Size (Large). 우리는 배우-네트워크의 크기를 늘리고 축소된 정책을 훈련한다. 이를 활용하여 과모수화가 정책 성과에 부정적인 영향을 미치는지 여부를 평가한다.\n' +
      '\n' +
      '### _Sim-to-Real Results_\n' +
      '\n' +
      '우리는 우리의 정책을 표 II의 기준 정책과 비교한 정량적 결과를 보여준다. 두 메트릭 모두에 대해 우리의 정책은 평가된 모든 개체에서 모든 기준선을 능가합니다. 또한, 5개의 물체 중 3개를 합리적인 속도로 회전시킬 수 있어 안정적인 파지 성능을 얻을 수 있다. 특히 파란색 병의 경우 배치된 정책 중 하나가 평균 30초 만에 4회전(360도)을 달성할 수 있다. 대조적으로, 거의 모든 기준선은 병에 걸리거나 병을 땅에 떨어뜨리는 효과적인 회전을 달성하지 못한다. 우리가 찾은 건\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{BlueBottle} & \\multicolumn{2}{c}{WoodBottle} & \\multicolumn{2}{c}{RedBottle} & \\multicolumn{2}{c}{GoldBottle} & \\multicolumn{2}{c}{SquareBottle} \\\\ Method & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) \\\\ \\hline Replay & 128.33\\(\\pm\\)217.96 & 7.67\\(\\pm\\)4.93 & 2.67\\(\\pm\\)4.62 & 7.67\\(\\pm\\)5.86 & 15.00\\(\\pm\\)25.98 & 4.67\\(\\pm\\)4.62 & 28.33\\(\\pm\\)0.04 & 7.67\\(\\pm\\)4.04 & 29.67\\(\\pm\\)5.62 & 10.00\\(\\pm\\)0.00 \\\\ No-Vis & 1.33\\(\\pm\\)2.31 & 21.67\\(\\pm\\)14.43 & 1.07\\(\\pm\\)1.85 & 14.67\\(\\pm\\)13.61 & 1.90\\(\\pm\\)3.29 & 8.33\\(\\pm\\)6.11 & 0.67\\(\\pm\\)1.15 & 16.33\\(\\pm\\)13.05 & 5.00\\(\\pm\\)6.24 & 20.33\\(\\pm\\)11.24 \\\\ No-Asym & 18.67\\(\\pm\\)28.94 & 30.00\\(\\pm\\)0.00 & 0.67\\(\\pm\\)1.15 & 19.33\\(\\pm\\)15.14 & 8.33\\(\\pm\\)14.43 & 13.00\\(\\pm\\)15.13 & 4.3\\(\\pm\\)27.51 & 3.67\\(\\pm\\)3.79 & 0.00\\(\\pm\\)0.00 & 2.33\\(\\pm\\)1.53 \\\\ Large & 2.00\\(\\pm\\)2.00 & 22.33\\(\\pm\\)13.28 & 0.00\\(\\pm\\)0.00 & 24.00\\(\\pm\\)10.99 & 2.33\\(\\pm\\)2.52 & 9.33\\(\\pm\\)4.73 & 2.67\\(\\pm\\)3.06 & 22.33\\(\\pm\\)13.28 & 1.67\\(\\pm\\)2.89 & 30.00\\(\\pm\\)0.00 \\\\ \\hline Ours & **946.33\\(\\pm\\)**83.83.83 & **23.67\\(\\pm\\)**10.97 & **499.50\\(\\pm\\)**78.23 & **30.0\\(\\pm\\)**0.90 & **150.67\\(\\pm\\)**113.47 & **30.00\\(\\pm\\)**0.90 & **98.67\\(\\pm\\)**69.51 & **30.00\\(\\pm\\)**0.90 & **43.00\\(\\pm\\)**12.12 & **30.00\\(\\pm\\)**0.40 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 II: 실제 설정에 대한 기준선과의 비교. 각 방법에 대해 3개의 다른 종자에 대해 훈련된 3개의 정책을 배치하고 결과를 평균화한다. 각 배포 시도는 30초 동안 수행됩니다. 각도 변위(AD) 메트릭은 한 번의 시도에서 물체 뚜껑이 손으로 꼬이는 정도의 수를 나타냅니다. TTF(Time-to-Fail) 메트릭은 물체가 로봇 손에서 떨어지기 전에 지나간 초의 수를 말하며, 이 수는 그러한 장애가 있을 때만 카운트된다. 우리의 성공 정책은 두 측정 기준 모두에서 모든 기준보다 우수합니다.\n' +
      '\n' +
      '도 6: 상이한 설정에서의 트레이닝 곡선들 _ Top_: 단일 객체 설정 결과_ Bottom_: 다중 객체 설정 결과_ 왼쪽 절반_: 서로 다른 보상 설정의 비교; _ 오른쪽 반쪽: 시력 사용에 대한 설명. 결과는 5개의 종자에 대해 평균을 낸다. 음영 처리된 영역은 표준 편차를 표시합니다. AD 점수는 총 실행 단계에 의해 평균화된다.\n' +
      '\n' +
      '개방 루프 정책은 TTF 점수가 가장 낮습니다. 성공적인 궤적을 재생하는 것은 대부분의 시간 동안 안정적인 파악으로 이어지지 않을 것이고, 병은 손가락에 직접 굴러서 손바닥을 떨어뜨릴 것이다. 이는 고려된 작업이 매우 세밀한 접촉을 포함하고 대상 상태에 따라 매우 정밀하게 행동할 수 있는 정책이 필요함을 시사한다. 또 다른 흥미로운 관찰은 시뮬레이션에서 우리의 전체 정책과 유사한 성능을 달성할 수 있음을 확인하지만 큰 정책이 현실 세계로 이전되지 않는다는 것이다. 이것은 일부 과적합이 발생함을 시사하며, 정책 네트워크의 크기를 제어하는 것은 고려된 연락처가 풍부한 작업의 성공적인 실제 전달을 위해 매우 중요하다.\n' +
      '\n' +
      '###_New Objects_로의 일반화\n' +
      '\n' +
      '우리는 가구에서 흔히 볼 수 있는 추가 10개의 새로운 대상에 대해 테스트하여 정책의 일반화 능력을 추가로 테스트한다(그림 1). 이러한 객체는 형상, 크기, 질량, 재료 및 색상 측면에서 우리의 훈련 객체와 실질적으로 다르다. 게다가, 그것들은 또한 기계적인 디자인 면에서 근본적으로 다르다. 시뮬레이션 훈련과 실제 테스트 모두에 사용하는 합성 병의 뚜껑은 무한히 비틀 수 있지만 이러한 가정용 물체의 뚜껑은 비틀 수 없다. 따라서 이러한 새로운 대상에 대한 뚜껑 비틀림 기술을 일반화하는 정책의 능력을 평가하기 위해 뚜껑 제거를 성공 기준으로 사용한다. 우리는 그림 1에서 뚜껑이 로봇 손에서 떨어질 때 볼 수 있듯이 뚜껑 제거를 물체 몸체에서 완전히 분리되는 것으로 정의하며, 우리의 정책은 약 30%의 성공률을 달성할 수 있음을 발견했다. 그 결과는 비디오 보충 자료에서 찾을 수 있다.\n' +
      '\n' +
      '###_erturbation_에 대한 강인성\n' +
      '\n' +
      '마지막으로, 우리는 또한 힘 섭동에 대한 우리 정책의 견고성을 평가한다. 구체적으로, 우리는 픽커 도구를 사용하여 임의의 방향을 따라 찌르거나 밀어서 임의의 시간에 배치 중에 객체를 교란시킨다(도 8의 좌측 참조). 우리는 우리의 정책이 물체를 계속해서 조작하기 위해 중심을 다시 향하게 하고 번역할 수 있다는 것을 발견했으며, 이는 외부 힘에 대한 약간의 견고성을 가지고 이러한 예상치 못한 변화에 적응할 수 있음을 나타낸다. 이 실험에서 우리는 시각적 폐색 효과를 풀기 위해 마커 기반 객체 검출 시스템을 사용한다는 점에 유의한다.\n' +
      '\n' +
      '## VIII Conclusion\n' +
      '\n' +
      '본 논문에서는 병과 같은 물체의 뚜껑을 두 손으로 비틀거나 제거하는 작업을 고려한다. 본 논문에서는 이러한 문제를 해결하기 위해 RL 기반의 Sim-to-real 시스템을 제안하고, 새로운 보상 설계, 실시간 인식을 위한 희소 객체 표현, 비틀림 병뚜껑을 시뮬레이션하기 위한 효율적이면서도 고충실도 방법 등 발생하는 문제를 해결하기 위한 다양한 기법을 제안한다. 우리는 시뮬레이션과 실제 세계 모두에서 실험을 수행하여 접근법의 효율성을 입증한다. 우리의 실제 결과는 광범위한 볼 수 있는 물체와 볼 수 없는 물체에 걸쳐 일반화를 보여준다. 우리는 우리의 시스템이 미래의 연구자들이 RL과 심투리얼로 쌍방향 손재주 문제를 해결할 수 있도록 영감을 줄 수 있기를 바란다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '알레그로 핸드의 하드웨어 설정 및 시뮬레이션에 대한 유용한 논의에 대해 첸 왕과 유체 진에게 감사드린다. TL은 국립 과학 재단과 UC 버클리의 펠로우쉽에 의해 지원된다. ZY는 InnoHK 물류로봇센터와 ONR MURI N00014-22-1-2773의 지원으로 지원되며, 본사는 DARPA Machine Common Sense와 ONR MURI N00014-21-1-2801의 지원을 받는다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Alexandre Amice, Peter Werner, and Russ Tedrake. Certifying bimanual rrt motion plans in a second. _arXiv:2310.16603_, 2023.\n' +
      '* [2] Sridhar Pandian Arunachalam, Irmak Guzey, Soumith Chintala, and Lerrel Pinto. Holo-dex: Teaching dexterity with immersive mixed reality. In _ICRA_, 2023.\n' +
      '\n' +
      '그림 8: 학습된 정책을 임의의 외력으로 교란시킨다. 우리의 정책은 이러한 외부 힘에 탄력적이며 정상으로 회복된다. 비디오 시각화는 당사 웹사이트를 참조하십시오.\n' +
      '\n' +
      '도 7: 서로 다른 보상함수의 행동. _ Top_: 우리의 완전한 보상 함수는 부드러운, 자연스러운, 그리고 인간과 같은 비틀림 동작뿐만 아니라 안정적인 파악을 달성한다. _ Middle_: 접촉 힌트가 없는 순진한 보행 제약 보상은 불규칙한 손가락 모션과 부자연스러운 파악으로 이어진다. _ Bottom_: 감소된 접촉 보상은 다소 자연스러운 행동을 산출하지만, 완전한 접촉 보상 사례에 비해 파악이 느슨하고 비정상적이다.\n' +
      '\n' +
      '* [3] Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans, and Lerrel Pinto. Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation. In _ICRA_, 2023.\n' +
      '* [4] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In _ICLR_, 2019.\n' +
      '* [5] Fabrizio Caccavale, Pasquale Chiacchio, Alessandro Marino, and Luigi Villani. Six-dof impedance control of dual-arm cooperative manipulators. _Transactions on Mechatronics_, 2008.\n' +
      '* [6] Konstantinos Chatzilygeroudis, Bernardo Fichera, Ilaria Lauzana, Fanjun Bu, Kunpeng Yao, Farshad Khadivar, and Aude Billard. Benchmark for bimanual robotic manipulation of semi-deformable objects. _RA-L_, 2020.\n' +
      '* [7] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for general in-hand object re-orientation. In _CoRL_, 2021.\n' +
      '* [8] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. _Science Robotics_, 2023.\n' +
      '* [9] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, Hao Dong, Song-Chun Zhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. In _NeurIPS_, 2022.\n' +
      '* [10] Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen Liu. Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. In _CoRL_, 2023.\n' +
      '* [11] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an atkinson-shiftrin memory model. In _ECCV_, 2022.\n' +
      '* [12] Zih-Yun Chiu, Florian Richter, Emily K Funk, Ryan K Orosco, and Michael C Yip. Bimanual regrasping for suture needles using reinforcement learning for rapid motion planning. In _ICRA_, 2021.\n' +
      '* [13] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units. _arXiv:1511.07289_, 2015.\n' +
      '* [14] Thomas Cohn, Seiji Shaw, Max Simchowitz, and Russ Tedrake. Constrained bimanual planning with analytic inverse kinematics. _arXiv:2309.08770_, 2023.\n' +
      '* [15] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren, Jingjing Chen, Ruo Zhang, Weiming Wang, and Cewu Lu. Low-cost exoskeletons for learning whole-arm manipulation in the wild. _arXiv:2309.14975_, 2023.\n' +
      '* [16] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff, and Dieter Fox. Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system. In _ICRA_, 2020.\n' +
      '* [17] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, and Gavriel State. Dexterne: Transfer of agile in-hand manipulation from simulation to reality. In _ICRA_, 2023.\n' +
      '* [18] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, and Xiaolong Wang. Dynamic handover: Throw and catch with bimanual hands. In _CoRL_, 2023.\n' +
      '* [19] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. _Science Robotics_, 2019.\n' +
      '* [20] Satoshi Kataoka, Seyed Kamyar Seyed Ghasemipour, Daniel Freeman, and Igor Mordatch. Bi-manual manipulation and attachment via sim-to-real reinforcement learning. _arXiv:2203.08277_, 2022.\n' +
      '* [21] Gagan Khandate, Siqi Shang, Eric T Chang, Tristan Luca Saidi, Johnson Adams, and Matei Ciocarlie. Sampling-based exploration for reinforcement learning of dexterous manipulation. In _RSS_, 2023.\n' +
      '* [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _ICCV_, 2023.\n' +
      '* [23] Franziska Krebs and Tamim Asfour. A bimanual manipulation taxonomy. _RA-L_, 2022.\n' +
      '* [24] Franziska Krebs, Andre Meixner, Isabel Patzer, and Tamim Asfour. The kit bimanual manipulation dataset. In _Humanoids_, 2021.\n' +
      '* [25] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In _RSS_, 2021.\n' +
      '* [26] Marco Laghi, Michele Maimeri, Mathieu Marchand, Clara Leparoux, Manuel Catalano, Arash Ajoudani, and Antonio Bicchi. Shared-autonomy control for intuitive bimanual tele-manipulation. In _Humanoids_, 2018.\n' +
      '* [27] Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, and Yi Wu. Efficient bimanual handover and rearrangement via symmetry-aware actor-critic learning. In _ICRA_, 2023.\n' +
      '* [28] Zhijun Li, Bo Huang, Arash Ajoudani, Chenguang Yang, Chun-Yi Su, and Antonio Bicchi. Asymmetric bimanual control of dual-arm exoskeletons for human-cooperative manipulations. _Transactions on Robotics_, 2017.\n' +
      '* [29] Toru Lin and Allan Jabri. Mimex: Intrinsic rewards from masked input modeling. In _NeurIPS_, 2023.\n' +
      '* [30] Yijiong Lin, Alex Church, Max Yang, Haoran Li, John Lloyd, Dandan Zhang, and Nathan F Lepora. Bi-touch: Bimanual tactile manipulation with sim-to-real deep reinforcement learning. _RA-L_, 2023.\n' +
      '* [31] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning. _arXiv:2108.10470_, 2021.\n' +
      '* [32] Takahiro Miki, Joonho Lee, Jemin Hwangbo, LorenzWellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. _Science Robotics_, 2022.\n' +
      '* [33] Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Morawanszky, Gavriel State, Michelle Lu, Ankur Handa, and Dieter Fox. Factory: Fast contact for robotic assembly. In _RSS_, 2022.\n' +
      '* [34] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik\'s cube with a robot hand. _arXiv:1910.07113_, 2019.\n' +
      '* [35] Christian Ott, Oliver Eiberger, Werner Friedl, Berthold Bauml, Ulrich Hillenbrand, Christoph Borst, Alin Albushaffer, Bernhard Brunner, Heiko Hirschmuller, Simon Kielhofer, Rainer Konietschke, Michael Suppa, Thomas Wimbock, Franziska Zacharias, and Gerhard Hirzinger. A humanoid two-arm system for dexterous manipulation. In _Humanoids_, 2006.\n' +
      '* [36] American Academy Of Pediatrics. _Caring for Your Baby and Young Child: Birth to Age 5_. American Academy Of Pediatrics, 2019.\n' +
      '* [37] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. In _RSS_, 2018.\n' +
      '* [38] Johannes Pitz, Lennart Rostel, Leon Sievers, and Berthold Bauml. Dextrous tactile in-hand manipulation using a modular reinforcement learning architecture. In _ICRA_, 2023.\n' +
      '* [39] Robert Platt, Andrew H. Fagg, and Roderic A. Grupen. Manipulation Gaits: Sequences of Grasp Control Tasks. In _ICRA_, 2004.\n' +
      '* [40] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-hand object rotation via rapid motor adaptation. In _CoRL_, 2022.\n' +
      '* [41] Haozhi Qi, Brent Yi, Yi Ma, Sudharshan Suresh, Mike Lambeta, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. In _CoRL_, 2023.\n' +
      '* [42] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation. _RA-L_, 2022.\n' +
      '* [43] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dietor Fox. Anyteletop: A general vision-based dexterous robot arm-hand teleoperation system. In _RSS_, 2023.\n' +
      '* [44] Lennart Rostel, Johannes Pitz, Leon Sievers, and Berthold Bauml. Estimator-coupled reinforcement learning for robust purely tactile in-hand manipulation. In _Humanoids_, 2023.\n' +
      '* [45] Nilanjan Sarkar, Xiaoping Yun, and Vijay R. Kumar. Dynamic control of 3-d rolling contacts in two-arm manipulation. In _ICRA_, 1993.\n' +
      '* [46] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. _arXiv:1506.02438_, 2015.\n' +
      '* [47] Lucy Xiaoyang Shi, Archit Sharma, Tony Z Zhao, and Chelsea Finn. Waypoint-based imitation learning for robotic manipulation. In _CoRL_, 2023.\n' +
      '* Survey. _ Robotics and Autonomous Systems_, 2012.\n' +
      '* [49] Nicolas Sommer, Miao Li, and Aude Billard. Bimanual compliant tactile exploration for grasping unknown objects. In _ICRA_, 2014.\n' +
      '* [50] Jan Steffen, Christof Elbrecht, Robert Haschke, and Helge Ritter. Bio-inspired motion strategies for a bimanual manipulation task. In _Humanoids_, 2010.\n' +
      '* [51] Simon Stepputtis, Maryam Bandari, Stefan Schaal, and Heni Ben Amor. A system for imitation learning of contact-rich bimanual manipulation policies. In _IROS_, 2022.\n' +
      '* [52] Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik, Mrinal Kalakrishnan, Roberto Calandra, Michael Kaess, Joe Ortiz, and Mustafa Mukadam. Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation. _arXiv:2312.13469_, 2023.\n' +
      '* [53] Nikolaus Vahrenkamp, Markus Przybylski, Tamim Asfour, and Rudiger Dillmann. Bimanual grasp planning. In _Humanoids_, 2011.\n' +
      '* [54] Renee Watling. _Peabody Developmental Motor Scales_. Springer, 2013.\n' +
      '* [55] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. Rotating without seeing: Towards in-hand dexterity through touch. In _RSS_, 2023.\n' +
      '* [56] Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Kang-Won Lee, Yi Wu, Soo-Chul Lim, and Xiaolong Wang. Robot synesthesia: In-hand manipulation with visuotactile sensing. In _ICRA_, 2024.\n' +
      '* [57] Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi, Taylor Howell, Xue Bin Peng, Sumeet Singh, Yuval Tassa, Pete Florence, Andy Zeng, et al. Robopianist: Dexterous piano playing with deep reinforcement learning. In _CoRL_, 2023.\n' +
      '* [58] Kevin Zhang, Mohit Sharma, Manuela Veloso, and Oliver Kroemer. Leveraging multimodal haptic sensory data for robust cutting. In _Humanoids_, 2019.\n' +
      '* [59] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In _RSS_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>