<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Twisting Lids Off with Two Hands\n' +
      '\n' +
      'Toru Lin1, Zhao-Heng Yin1, Haozhi Qi, Pieter Abbeel, Jitendra Malik\n' +
      '\n' +
      'UC Berkeley\n' +
      '\n' +
      '[https://toruowo.github.io/bimanual-twist](https://toruowo.github.io/bimanual-twist)\n' +
      '\n' +
      'Footnote 1: Equal Contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      'We study the task of twisting or removing lids with two multi-fingered robot hands. This task is both practically important and profoundly interesting. For one, the ability to twist or remove lids from containers is a crucial motor skill that toddlers acquire during their early developmental stages [36, 54]. For another, the manipulation skills required for this task, such as the coordination of fingers to manipulate a multi-part object, can be generally useful across a large collection of practical tasks.\n' +
      '\n' +
      'Our work demonstrates the feasibility of learning a dexterous and dynamic bimanual manipulation policy purely in simulation and zero-shot transferring it to the real world. By abstracting a class of articulated objects into a simplistic two-part object model with randomized physical properties, we successfully train a policy that can generalize to a diverse set of novel objects with different shapes, sizes, visual appearances, and other physical properties (Figure 1). Our method does not require precise modeling of any individual object, or hardcoding prior knowledge on object properties. Instead, it allows for stable and natural twisting behaviors to emerge through large-scale reinforcement learning (RL) training.\n' +
      '\n' +
      'In developing a system with these capabilities, we uncover a number of novel engineering insights, which we outline below.\n' +
      '\n' +
      '**Physical Modeling.** Our work features a novel class of objects for in-hand manipulation: articulated objects defined as two rigid bodies connected via a revolute joint with threaded structure. Accurately modeling friction and contact with revolute joints and threaded structure has traditionally been a\n' +
      '\n' +
      'Figure 1: We train two anthropomorphic robot hands to twist (off) lids of various articulated objects. The control policy is first trained in simulation with deep reinforcement learning, then zero-shot transferred to a real-world setup. A single policy trained with simplistic, simulated bottle-like objects generalizes to real objects with drastically different physical properties (e.g. shape, size, color, material, mass). The length, diameter (or diagonal length), and mass of each object are annotated at the bottom. See our video for more visualizations.\n' +
      '\n' +
      'hard challenge in robotics simulation [33]. To address this, we introduce a brake-based design to model the interaction between lid and body of bottle-like objects. This design is fast to simulate while maintaining a high fidelity to real-world physical dynamics, enabling efficient policy learning and successful sim-to-real transfer.\n' +
      '\n' +
      '**Perception.** We initially hypothesize that a fine-grained, contact-rich manipulation task like lid-twisting must require precise perceptual information on object states. To our surprise, a two-point sparse object representation, extracted from off-the-shelf object segmentation and tracking tools, is sufficient to solve the perception problem. With simple domain randomization techniques, we train policies that are robust against occlusion and camera noise. This discovery suggests that a minimal amount of object perception information can be adequate for complicated bimanual manipulation tasks.\n' +
      '\n' +
      '**Reward Design.** Previously performant reward designs for tasks like in-hand reorientation cannot be straightforwardly applied to our task, since those tasks focus on manipulating single-part rigid bodies with one hand rather than multi-part articulated bodies with two hands. Solving this task is more challenging since it involves more complex and precise contact (e.g. using two hands to hold a lid). In addressing this challenge, we discover a simple keypoint-based contact reward that yields natural lid-twisting behavior on the robot fingers.\n' +
      '\n' +
      'We conduct several controlled experiments in both simulation and the real world. Through empirical analysis, we verify that our simulation modeling, perception module, and reward design can reliably lead to the desired behavior of lid twisting. Our final successful policy manifests natural behavior across test objects with various physical properties such as shapes, sizes, and masses in simulation. Moreover, the learned policy can be zero-shot transferred to a wide range of novel household objects whose lids can be removed (Figure 1), and it is robust against perturbations.\n' +
      '\n' +
      '## II Background\n' +
      '\n' +
      '### _Classic Approaches for Bimanual Manipulation_\n' +
      '\n' +
      'For decades, the problem of bimanual manipulation has remained a fascinating unsolved challenge in robotics [53, 48, 49, 6, 23, 5]. While multi-fingered robot hands seem to be a natural choice for bimanual robot systems in theory, designing controllers for high-dimensional action spaces remains an open problem due to difficulties in mechanical design and reliable actuation. Most previous works in bimanual manipulation thus use simple and durable parallel jaw-grippers as end-effectors. For example, Caccavale et al. [5] demonstrate carrying and moving objects using two arms with impedance control. Sarkar et al. [45] show controlling object states using rolling contact. Unlike most previous works, our work focuses on bimanual manipulation with two multi-fingered hands.\n' +
      '\n' +
      'While there exist prior works that utilize two multi-fingered hand for manipulation, they differ substantially from our work in terms of approach and results. Vahrenkamp et al. [53] presents a bimanual grasp planner that could be used to build grasps for large objects using multi-fingered hands, but without demonstrating any real-world results. Platt et al. [39] achieve object reconfiguration using two three-fingered robot hands, but do so by planning a sequence of low-level controllers. Steffen et al. [50] study a screwing task similar to our task of interest but synthesize the control motion sequence with a kernel approach applied to human motion data captured with a data glove. Our work fills a missing space in the bimanual multi-fingered manipulation literature, as we do not rely on any planner, accurate object model, or human data. Instead, we directly work on a high-dimensional action space with RL on randomized environments.\n' +
      '\n' +
      '### _Learning Approaches for Bimanual Manipulation_\n' +
      '\n' +
      'In recent years, bimanual manipulation has been more actively studied with learning-based methods, as a result of progress in learning algorithms and compute infrastructure. These learning-based approaches can generally be categorized into two types: 1) learning from real-world data; 2) learning in simulation, then transferring to the real world (sim-to-real).\n' +
      '\n' +
      '**Learning from Real-World Data.** Rapid progress has been made in reinforcement learning (RL) in the real world. Zhang et al. [58] learn to chain motor primitives for vegetable cutting, with relatively motion primitives; much of the task difficulty is bypassed via the use of specialized end-effectors [1, 14]. Chiu et al. [12] learn precise needle manipulation with two grippers by integrating RL with a sampling-based planner. While impressive, these works cannot easily scale to higher dimensional action space due to their sample inefficiency or the need to define heuristic-based action primitives.\n' +
      '\n' +
      'Most recent successes in bimanual manipulation are achieved by learning from demonstrations [59, 47, 51]. However, successes so far are largely limited to simple end-effectors like parallel jaw grippers. One reason is the lack of high-quality demonstration data from multi-fingered robot hands [24]; the availability of such hand hardware is itself limited, not to mention the sophisticated data collection infrastructure. Although several works aiming to improve demonstration data collection with two arms [15, 26, 28] or multi-fingered hands exist [2, 3, 16, 42, 43], their latency and retargeting errors limit their practical applicability and scalability. Our method uses RL in simulation and is thus not limited by the hardware and data collection infrastructure problems faced by learning from demonstration approaches.\n' +
      '\n' +
      '**Sim-to-Real.** There has been growing interest in sim-to-real approaches for robotics - i.e. learning policies in simulation and transferring them to the real world - stimulated by several notable successes in recent years ranging from locomotion [19, 25, 32] to manipulation [8, 17, 34, 40]. Existing works in manipulation, however, are mostly done with either a single multi-fingered hand [7, 10, 21, 38, 41, 44, 52, 55, 56], or two arms with simpler end-effectors [20, 27, 30]. While Chen et al. [9] and Zakka et al. [57] feature bimanual tasks with dexterous hands, only simulation results are shown. Perhaps the work most related to ours is Huang et al. [18], where the authors demonstrate throwing and catching objects usingtwo dexterous hands. However, our task is significantly more contact-rich and requires substantially more challenging bimanual coordination to maintain object stability at all times. Our work distinguishes itself from the literature by considering manipulation for an extremely contact-rich task using two multi-fingered hands; notably, the objects that we work with are two-part articulated bodies, while previous work focuses on single-part rigid bodies.\n' +
      '\n' +
      '## III Task Formulation\n' +
      '\n' +
      'Twisting lids of container objects is a complex in-hand manipulation process that requires dynamic dexterity of multiple fingers and precise coordination between two hands. Below, we define the specific task being considered in this work.\n' +
      '\n' +
      '**Task Initialization.** Each object of interest consists of two rigid, near-cylindrical parts (a "body" and a "lid"); the two parts are connected via a continuous revolute joint, allowing them to rotate about each other. To better benchmark the bimanual twisting capability, we consider a class of articulated bottles with lids that can be twisted infinitely (see Section IV-B for more details). The two robotic hands are initialized in a static pose with upward-facing palms. At the beginning of each episode, a bottle-like object is gently dropped or placed onto the fingers. The initial pose of the object is randomized both in translation and rotation to a fixed default pose; the initial joint positions of the hands are randomized about a predefined canonical pose by adding Gaussian noise. Note that since we do not assume a stable grasp configuration at task initialization, the control policy needs to learn in-grasp reorientation to place the object in a stable location to perform successive manipulation.\n' +
      '\n' +
      '**Task Objective.** The goal of this task is to twist the lid about the object\'s axis of rotation in one direction as much as possible; during this process, the object should always stay in hand. Achieving this involves a sequence of delicate movements: 1) after initialization, the robot hand should firmly grasp and slightly rotate the bottle to a suitable pose; 2) the hand that is closer to the object lid should place its finger around the lid to initiate twisting motion; 3) the two hands should coordinate to avoid dropping the object while one hand twists the lid. Motor skills that arise from this task could serve as generic abstractions for skills necessary to manipulate many other household objects, especially those with revolute joints such as Rubik\'s cubes, light bulbs, and jars.\n' +
      '\n' +
      '**Task Complexity.** The complexity inherent in this task can be summarized into the following three aspects.\n' +
      '\n' +
      '_1) In-Hand Perception Difficulty._ Heavy occlusion can occur during manipulation due to fingers surrounding the object in-hand. How to build a real robot system that can perform robust bimanual manipulation against such occlusion and perceptual noises remains an open problem.\n' +
      '\n' +
      '_2) Fine-grained Contact._ This task requires both hands to coordinate to maintain stability. Incorrect contact points or forces could result in the bottle dropping.\n' +
      '\n' +
      '_3) High-Dimensional Control._ Our system has \\(32\\) degrees of freedom, and almost all the fingers are engaged throughout the manipulation process. This requires extremely precise coordination among a large number of rigid bodies, which is a known challenge for existing learning-based methods.\n' +
      '\n' +
      '## IV System Setup\n' +
      '\n' +
      'To address the aforementioned challenges in bimanual manipulation and our specific task choice, we develop a sim-to-real pipeline based on a deep RL approach. In this section, we describe our overall system setup.\n' +
      '\n' +
      '### _Real-world System_\n' +
      '\n' +
      '**Hardware Setup.** As shown in Figure 1, we use two 16-DoF Allegro Hands from Wonik Robotics for our experiments. Each Allegro Hand is mounted on a fixed UR5e arm. We employ a single RealSense D435 depth camera to provide visual information, from which we extract object state information. We send control commands to the robot at a frequency of \\(10\\,\\mathrm{Hz}\\) via a Linux workstation.\n' +
      '\n' +
      '**Perception.** Figure 3 shows an overview of our perception pipeline. Instead of directly using pixels as the RL agent\'s\n' +
      '\n' +
      'Figure 3: Real-time perception system. _Top_: scene overview. _Bottom_: we segment and track object parts from the RGB frames (left), take mask centers as object part centers (middle), and estimate 3D object keypoints using noisy depth information from the camera (right).\n' +
      '\n' +
      'Figure 2: Bottle URDF model. A key design in the bottle model is the brake link and the prismatic “brake” joint. The brake link constantly presses against the lid link to simulate the static friction between the base link and the lid link. Without the brake, the lid link can easily start to rotate even when it is not in contact with the fingers.\n' +
      '\n' +
      'policy input, we propose to extract object keypoints from images for control through object segmentation and tracking. Specifically, we utilize the Segment Anything model [22] to generate two separate masks for the bottle body and the lid on the first frame, and XMem [11] to track the masks throughout all remaining frames. To approximate the 3D center-of-mass coordinates of the bottle body and lid, we calculate the center position of their masks in the image plane, then obtain noisy depth readings from a depth camera to recover a corresponding 3D position. The perception pipeline runs at \\(10\\,\\mathrm{Hz}\\) to match the neural network policy\'s control frequency.\n' +
      '\n' +
      '### _Bottle Models_\n' +
      '\n' +
      '**Simulated Bottles.** A central challenge in simulating the lid-twisting task is how to properly model friction between the bottle body and the lid, particularly static friction. Simulating this type of physical force has been a long-standing problem in graphics [33]. We design a simple modeling approximation that strikes a balance between fidelity and speed during physical simulation. As a result, a successful policy can be learned in simulation within a reasonable amount of wall-clock time while being realistic enough for transfer to the real world. We illustrate our bottle-like object model in Figure 2. Our design features a special Brake Link that constantly presses against the bottle lid (through a prismatic joint). This artificially generates frictional forces between the bottle body (Base Link) and the lid (Lid Link), preventing relative rotation between them--similar to a bottle with its lid screwed on.\n' +
      '\n' +
      '**Real-World Articulated Bottles.** To easily vary the shapes and sizes of our objects and encourage reproducibility, we design a set of articulated bottle-like objects in CAD software and 3D print them in a range of different colors and materials. The lids can be rotated infinitely, simulating an ideal revolute joint. We show examples of our custom bottles in Figure 5 (middle). We also collect household bottle-like objects with varying physical properties to demonstrate the generalizability of our method, as depicted in Figure 5 (bottom).\n' +
      '\n' +
      '## V Learning to Twist Lids\n' +
      '\n' +
      'Bimanual in-hand dexterous manipulation involves highly complex hand-object contacts, and remains challenging to solve with traditional methods. In this work, we address the control challenge through RL. We formulate our control problem as a partially observable Markov Decision Process \\(\\mathcal{M}=(\\mathcal{S},\\mathcal{O},\\mathcal{A},\\mathcal{R},\\mathcal{P})\\), where \\(\\mathcal{S}\\) is the state space, \\(\\mathcal{A}\\) is the action space, \\(\\mathcal{O}\\) is the observation space, \\(\\mathcal{R}\\) is the reward function, and \\(\\mathcal{P}\\) is the environment dynamics. The control policy generates an action \\(a_{t}\\in\\mathcal{A}\\) to maximize the cumulative reward given an observation \\(o_{t}\\in\\mathcal{O}\\).\n' +
      '\n' +
      '### _Observation and Action_\n' +
      '\n' +
      '**Observation Space.** At each time step \\(t\\), the control policy observes the following information from the environment: the proprioceptive hand joint positions \\(q_{t}\\), the estimated center-of-mass 3D positions of the bottle base and lid, and previously commanded target joint positions \\(\\tilde{q}_{t}\\).\n' +
      '\n' +
      '**Action Space.** We use an impedance PD controller to drive the robot hand. The control policy produces a relative target joint position as the action \\(a_{t}\\), which is added to the current target joint position \\(q_{t}\\) to produce the next target: \\(\\tilde{q}_{t+1}=\\tilde{q}_{t}+\\eta\\mathrm{EMA}(a_{t})\\). \\(\\eta\\) is a scaling factor. Note that we smooth the action with its exponential moving average (EMA) to produce smooth motion. The next target position is sent to the PD controller to generate torque on each joint.\n' +
      '\n' +
      '### _Reward Design_\n' +
      '\n' +
      'The reward function plays a critical role in defining the robot\'s behavior. The bimanual in-hand manipulation system has a higher action space dimension and much more complex contact patterns compared to single-hand scenarios. This magnifies the challenges inherent in the RL exploration process, underlining the need for more sophisticated reward designs. While one way to approach hard exploration problems is to add intrinsic rewards [4, 29], we propose a fine-grained, behavior-aware reward design for this task. Our reward function contains the following terms:\n' +
      '\n' +
      '**Twisting Reward.** We define the twisting reward as\n' +
      '\n' +
      '\\[r_{\\mathrm{twisting}}=\\Delta\\theta=q_{bottle}^{t+1}-q_{bottle}^{t}, \\tag{1}\\]\n' +
      '\n' +
      'which is the rotation angle of the lid during one-step execution.\n' +
      '\n' +
      '**Finger Contact Reward.** We encourage natural object manipulation behavior by defining a set of keypoints positioned on the object to guide the fingers\' interactions (Figure 4). We define two set of keypoints \\(\\mathbf{X}^{L}\\in\\mathbb{R}^{n\\times 3}\\) and \\(\\mathbf{X}^{R}\\in\\mathbb{R}^{m\\times 3}\\) attached on the bottle base and lid respectively. Then, we define the finger contact reward as\n' +
      '\n' +
      '\\[r_{\\mathrm{contact}}=\\sum_{i}\\left[\\frac{1}{1+\\alpha d(\\mathbf{X}^{L},\\mathbf{ F}_{i}^{L})}+\\frac{1}{1+\\alpha d(\\mathbf{X}^{R},\\mathbf{F}_{i}^{R})}\\right], \\tag{2}\\]\n' +
      '\n' +
      'where \\(\\mathbf{F}^{L}\\in\\mathbb{R}^{4\\times 3}\\) and \\(\\mathbf{F}^{R}\\in\\mathbb{R}^{4\\times 3}\\) are the position of left and right fingertips, \\(\\alpha\\) is a scaling hyperparameter, and \\(d\\) is a\n' +
      '\n' +
      'Figure 4: Illustration of our reward design. Our task-specific reward contains three terms: finger contact reward (yellow arrows), twisting reward (white arrow), and pose reward (blue arrow). In particular, our keypoint-based finger reward is crucial for learning desired screwing behavior, which encourages the robot to minimize the distance between fingertips and reference contact keypoints.\n' +
      '\n' +
      'distance function defined as\n' +
      '\n' +
      '\\[d(\\mathbf{A},\\mathbf{x})=\\min_{i}\\|\\mathbf{A}_{i}-\\mathbf{x}\\|_{2}. \\tag{3}\\]\n' +
      '\n' +
      'Therefore, we encourage each fingertip to stay as close to one of the keypoints as possible. By choosing the keypoint properly around the bottle base and lid, we can elicit natural grasping and lid-twisting behavior.\n' +
      '\n' +
      '**Pose Reward.** We also introduce a pose matching reward term to encourage the bottle main axis \\(\\mathbf{x}_{axis}\\) aligned with a predefined direction \\(\\mathbf{v}\\). This term is defined as\n' +
      '\n' +
      '\\[r_{\\mathrm{pose}}=-\\arccos(\\langle\\mathbf{x}_{axis},\\mathbf{v}\\rangle). \\tag{4}\\]\n' +
      '\n' +
      '**Other Regularizations.** The above three reward terms specify the objective of our task. Besides these three rewards, we also introduce another few regularization terms as in previous works [55], including work penalty and action penalty to penalize large, jerky motions:\n' +
      '\n' +
      '\\[r_{\\mathrm{action}}=-\\|a_{t}\\|^{2}, \\tag{5}\\]\n' +
      '\n' +
      '\\[r_{\\mathrm{work}}=-\\langle|\\tau|,|\\hat{q}_{t}|\\rangle. \\tag{6}\\]\n' +
      '\n' +
      'The reward function is a weighted sum of the above terms:\n' +
      '\n' +
      '\\[r=\\alpha_{1}r_{\\mathrm{contact}}+\\alpha_{2}r_{\\mathrm{twist}}+\\alpha_{3}r_{ \\mathrm{pose}}+\\alpha_{4}r_{\\mathrm{action}}+\\alpha_{5}r_{\\mathrm{work}}. \\tag{7}\\]\n' +
      '\n' +
      '**Reset Strategy.** Due to the high dimensionality of our learning problem, there exist many possible ways how the robot interact with the object; among these, most modes lead to failures such as dropping the object beyond recovery. Exploring these modes rarely leads to meaningful learning progress. To circumvent this issue, we introduce several early termination criteria. Most importantly, we reset an episode if the robot hands fail to rotate the bottle into a desired pose for bimanual twisting within a short time limit. Additionally, we also reset when the bottle\'s \\(z\\)-position is below a certain threshold, as the fingertips of the two hands can pinch the bottle at a low position without being able to reposition it into the palm.\n' +
      '\n' +
      '### _Domain Randomization_\n' +
      '\n' +
      'We apply a wide range of domain randomizations to ensure zero-shot sim-to-real transfer, including both physical and non-physical randomizations. Physical randomizations include the randomization of object friction, mass, and scale. We also apply random forces to the object to simulate the physical effects that are not implemented by the simulator. Non-physical randomizations model the noise in observation (e.g. joint position measurement and detected object positions) and action. A summary of our randomization attributes and parameters is shown in Table I.\n' +
      '\n' +
      '### _Training_\n' +
      '\n' +
      'We use the proximal policy optimization (PPO) algorithm to learn RL policies. We use an advantage clipping coefficient \\(\\epsilon=0.2\\); a horizon length of 16, with \\(\\gamma=0.99\\), and generalized advantage estimator (GAE) [46] coefficient \\(\\tau=0.95\\). The policy network is a three-layer MLP with ELU [13] activation, whose hidden layer is [256, 256, 128]. The policy network outputs a Gaussian distribution with a learnable state-independent standard deviation. The value network is also an MLP with ELU activation, whose hidden layer is [512, 512, 512]. We use an adaptive learning rate with KL threshold of 0.016 [19]. During training, we normalize the state input, value, and advantage. The gradient norm is set to 1.0 and the minibatch size is set to 8192. We use asymmetric observation [37] for the policy and value network, adding privileged information (randomized physical parameters, cube velocity and angular velocity, joint velocity and angular velocity) to the value network inputs. This privileged information is not accessible by the policy network.\n' +
      '\n' +
      '## VI Simulated Experiments\n' +
      '\n' +
      'We study the following questions in simulation:\n' +
      '\n' +
      '_Reward Design._ How important is the keypoint-based reward for eliciting desired twisting behavior in bimanual manipulation? How effective is it compared to the other reward functions used in recent works?\n' +
      '\n' +
      '_Perception._ How important is the visual information in solving this task? Is a sparse, keypoint representation enough for learning a policy that can handle multiple objects?\n' +
      '\n' +
      '### _Setup_\n' +
      '\n' +
      '**Object Set.** In the simulated experiments, we utilize a collection of simulated cylindrical bottles with varying aspect ratios for both training and evaluation. Some samples are visualized in Figure 5. We consider two setups in simulation: 1) Multi-object setup, in which all the objects are used, and 2) Single-object setup, in which we only use a medium-sized bottle that represents the mean of the dataset.\n' +
      '\n' +
      '**Evaluation Metric.** We introduce the following metrics for evaluating the performance:\n' +
      '\n' +
      '_Angular Displacement (AD)_ is the total number of degrees through which the lid has been twisted.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Object: Mass (kg) & [0.03, 0.1] \\\\ Object: Friction & [0.5, 1.5] \\\\ Object: Shape & \\(\\times\\mathcal{U}(0.95,1.05)\\) \\\\ Object: Initial Position (cm) & \\(\\mathcal{U}(-0.02,0.02)\\) \\\\ Object: Initial \\(z\\)-orientation & \\(\\mathcal{U}(-0.75,0.75)\\) \\\\ Hand: Friction & [0.5, 1.5] \\\\ \\hline PD Controller: P Gain & \\(\\times\\mathcal{U}(0.8,1.1)\\) \\\\ PD Controller: D Gain & \\(\\times\\mathcal{U}(0.7,1.2)\\) \\\\ \\hline Random Force: Scale & 2.0 \\\\ Random Force: Probability & 0.2 \\\\ Random Force: Decay Coeff. and Interval & 0.99 every 0.1s \\\\ \\hline Bottle Pos Observation: Noise & 0.02 \\\\ Joint Observation Noise. & \\(+\\mathcal{N}(0,0.4)\\) \\\\ Action Noise. & \\(+\\mathcal{N}(0,0.1)\\) \\\\ \\hline Frame Lag Probability & 0.1 \\\\ Action Lag Probability & 0.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table I: Domain Randomization Setup.\n' +
      '\n' +
      '_Time-to-Fail (TTF)_ is the period measured from the moment the bottle is held to the point when it either slips from the hand or becomes lodged.\n' +
      '\n' +
      '**Baselines.** We compare our policy with the following baselines:\n' +
      '\n' +
      '_Policy without Vision._ This is a neural network policy without visual information (bottle keypoints). We use this to evaluate the importance of vision.\n' +
      '\n' +
      '_Policy with Reduced Contact Reward._ In training this policy, we reduce the intensity of our proposed finger contact reward. We use this to study the role of our contact reward in policy learning and shaping the policy\'s behavior.\n' +
      '\n' +
      '_Policy with Gait Reward._ In training this policy, we replace our contact reward with a gait constraint reward function similar to ones used for in-hand reorientation tasks (e.g. [40]). This baseline is only used for qualitative analysis.\n' +
      '\n' +
      '### _Main Results_\n' +
      '\n' +
      '**Reward Design.** We first compare our approach with the reduced finger reward baseline (Figure 6). After decreasing the scale of finger contact reward, learned policies fail to master the desired lid-twisting skill and have low performance in general. We hypothesize that this is because the motion of lid-twisting requires a very specific pose pattern for holding the object; without explicitly encouraging such a pose pattern (e.g., via its contact modes), RL exploration becomes so hard that it is unsolvable within the available training time. We also observe a positive correlation between the intensity of finger contact reward and both 1) sample efficiency during learning and 2) performance of learned policies (as reflected by the AD score).\n' +
      '\n' +
      '**Vision vs No Vision.** We also study the importance of vision modality in solving our considered task. Existing works show that certain rotation behavior can be achieved through implicit tactile sensing (joint proprioception data) [40], but it is unclear whether the same conclusion can be drawn for the bimanual lid-twisting task. Our empirical results show that, in both single and multi-object setups, the no-vision baseline performs substantially worse than our full method. This suggests that knowledge of the position of bottle keypoints is essential for successful lid-twisting.\n' +
      '\n' +
      '**Single Object vs Multi Object.** We run RL training with two object settings: (1) using a single bottle-like object and (2) using multiple bottle-like objects with more variation in the ratio between the bottle base and lid. The two settings pose a trade-off between specialization and generalization: in the single-object scenario, the policy might learn successful behaviors more easily but find it harder to generalize to unseen objects, and vice versa. To our surprise, we observe that multi-object training yields slightly better performance compared to single-object training. We hypothesize that multi-object makes exploring lid-twisting behavior an easier process by introducing an object curriculum that covers both easy and hard object instances during training.\n' +
      '\n' +
      '### _Qualitative Results_\n' +
      '\n' +
      'In this section, we study the reward design by examining their induced behavior. Specifically, we compare our method to the reduced contact reward baseline and the gaining reward baseline. Visualization of trajectories from policies trained with each method is shown in Figure 7. Policies trained without the finger contact reward exhibit extremely unstable grasping or screwing motions; the behaviors not only appear unnatural in simulation, but also completely fail to transfer to a real-world setup. We hypothesize that the use of two hands, each with a high degree of freedom, leads to a massive search space for in-hand object manipulation; introducing task priors like behavior-aware contact reward is therefore essential for reducing the search space and eliciting natural behavior.\n' +
      '\n' +
      '## VII Real-world Experiments\n' +
      '\n' +
      'In this section, we first demonstrate that our final learned policy can be directly transferred to the real world in a zero-shot manner. Then, we investigate the effect of several key design choices on the success of sim-to-real transfer. Finally, we examine our policy\'s ability to generalize, as well as its robustness against various kinds of perturbations.\n' +
      '\n' +
      '### _Setup_\n' +
      '\n' +
      'For quantitative evaluation, we evaluate the sim-to-real transfer capabilities of our policies on five different articulated bottle objects (Figure 5). Among them, four are in-distribution\n' +
      '\n' +
      'Figure 5: Used bottles in our experiments. _Top_: Simulated bottles. _Middle_: Custom-made bottles (in-distribution except for the rightmost square bottle). _Bottom_: Household object bottles (out-of-distribution).\n' +
      '\n' +
      '(round-body bottles) and one is outside of the training distribution (square-body bottle). We measure both AD and TTF in 20 trials, with each trial lasting for a maximum of 30 seconds. For each evaluated method, we select the three best policies out of ten policies trained on ten different random seeds. We end a trial if the bottle falls off the palm.\n' +
      '\n' +
      '**Baselines.** We compare our final policy with the following baselines to study the effect several key design choices in our pipeline.\n' +
      '\n' +
      '_Open-loop Replay Policy (Replay)._ We record successful trials of our learned policy in the simulation and randomly select a trajectory to replay on the real robot. This baseline is used to evaluate whether the task can be solved by a deterministic motion pattern.\n' +
      '\n' +
      '_Policy without Vision (No-Vis)._ This baseline policy only takes proprioceptive state information as input, without information about the object state.\n' +
      '\n' +
      '_Policy without Asymmetric Training (No-Asym)._ We compare with a baseline where policy is trained without asymmetric PPO, and evaluate whether introducing additional privileged information into the value network will affect the transfer performance.\n' +
      '\n' +
      '_Larger Policy Network Size (Large)._ We increase the size of our actor-network and train a reduced-size policy. We use this to evaluate whether over-parameterization harms policy performance.\n' +
      '\n' +
      '### _Sim-to-Real Results_\n' +
      '\n' +
      'We show quantitative results comparing our policy with baseline policies in Table II. For both metrics, our policy outperforms all baselines across all evaluated objects. Our method also achieves a stable grasp and can rotate 3 out of 5 objects at a reasonable speed. In particular, for the blue bottle, one of the deployed policies can achieve 4 full turns (360 degrees) in 30 seconds on average. In contrast, almost all the baselines fail to achieve any effective rotations, either getting stuck or dropping the bottle to the ground. We find that\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{3}{c}{BlueBottle} & \\multicolumn{2}{c}{WoodBottle} & \\multicolumn{2}{c}{RedBottle} & \\multicolumn{2}{c}{GoldBottle} & \\multicolumn{2}{c}{SquareBottle} \\\\ Method & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) & AD (deg) \\(\\uparrow\\) & TTF (s) \\(\\uparrow\\) \\\\ \\hline Replay & 128.33\\(\\pm\\)217.96 & 7.67\\(\\pm\\)4.93 & 2.67\\(\\pm\\)4.62 & 7.67\\(\\pm\\)5.86 & 15.00\\(\\pm\\)25.98 & 4.67\\(\\pm\\)4.62 & 28.33\\(\\pm\\)0.04 & 7.67\\(\\pm\\)4.04 & 29.67\\(\\pm\\)5.62 & 10.00\\(\\pm\\)0.00 \\\\ No-Vis & 1.33\\(\\pm\\)2.31 & 21.67\\(\\pm\\)14.43 & 1.07\\(\\pm\\)1.85 & 14.67\\(\\pm\\)13.61 & 1.90\\(\\pm\\)3.29 & 8.33\\(\\pm\\)6.11 & 0.67\\(\\pm\\)1.15 & 16.33\\(\\pm\\)13.05 & 5.00\\(\\pm\\)6.24 & 20.33\\(\\pm\\)11.24 \\\\ No-Asym & 18.67\\(\\pm\\)28.94 & 30.00\\(\\pm\\)0.00 & 0.67\\(\\pm\\)1.15 & 19.33\\(\\pm\\)15.14 & 8.33\\(\\pm\\)14.43 & 13.00\\(\\pm\\)15.13 & 4.3\\(\\pm\\)27.51 & 3.67\\(\\pm\\)3.79 & 0.00\\(\\pm\\)0.00 & 2.33\\(\\pm\\)1.53 \\\\ Large & 2.00\\(\\pm\\)2.00 & 22.33\\(\\pm\\)13.28 & 0.00\\(\\pm\\)0.00 & 24.00\\(\\pm\\)10.99 & 2.33\\(\\pm\\)2.52 & 9.33\\(\\pm\\)4.73 & 2.67\\(\\pm\\)3.06 & 22.33\\(\\pm\\)13.28 & 1.67\\(\\pm\\)2.89 & 30.00\\(\\pm\\)0.00 \\\\ \\hline Ours & **946.33\\(\\pm\\)**83.83.83 & **23.67\\(\\pm\\)**10.97 & **499.50\\(\\pm\\)**78.23 & **30.0\\(\\pm\\)**0.90 & **150.67\\(\\pm\\)**113.47 & **30.00\\(\\pm\\)**0.90 & **98.67\\(\\pm\\)**69.51 & **30.00\\(\\pm\\)**0.90 & **43.00\\(\\pm\\)**12.12 & **30.00\\(\\pm\\)**0.40 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table II: Comparison with baselines on real setup. For each method, we deploy 3 policies trained on 3 different seeds and average the results. Each deployment trial is conducted for 30 seconds. The Angular Displacement (AD) metric refers to number of degrees that object lid is twisted by hands in a single trial. The Time-to-Fail (TTF) metric refers to number of seconds that has passed before the object drops out of robot hands; this number is only counted when there is such a failure. Our success policy outperforms all baselines on both metrics.\n' +
      '\n' +
      'Figure 6: Training curves in different settings. _Top_: Results in single-object setup. _Bottom_: Results in multi-object setup. _Left half_: Comparisons of different reward setups. _Right half_: Ablations on the use of vision. The results are averaged on 5 seeds. The shaded area shows the standard deviation. The AD score is averaged by the total execution steps.\n' +
      '\n' +
      'the open-loop policy has the lowest TTF score. Replaying a successful trajectory will not lead to a stable grasp for most of the time, and the bottle will directly roll on the fingers and then drop off the palm. This suggests that the considered task involves very fine-grained contacts and requires the policy to act very precisely according to the object state. Another interesting observation is that the large policy does not transfer to the real world, although we confirm that it can achieve similar performance to our full policy in simulation. This suggests that some overfitting occurs, and controlling the size of the policy network is very important for the successful sim-to-real transfer of our considered contact-rich task.\n' +
      '\n' +
      '### _Generalization to Novel Objects_\n' +
      '\n' +
      'We further test our policy\'s ability to generalize by testing it on an additional 10 novel objects commonly found in households (Figure 1). These objects substantially differ from our training objects in terms of shape, size, mass, material, and color. Besides, they are also fundamentally different in terms of mechanical design. While the lids of the synthetic bottles that we use for both simulation training and real-world testing can be twisted infinitely, the lids of these household objects cannot. Therefore, to evaluate our policy\'s ability to generalize the lid-twisting skill to these novel objects, we use lid removal as a success criterion. We define lid removal as the object lid being completely detached from the object body, as can be seen in the case when the lids fall from the robot hands in Figure 1. We find that our policy can achieve about a 30% success rate. The results can be found in the video supplementary materials.\n' +
      '\n' +
      '### _Robustness against Perturbation_\n' +
      '\n' +
      'Finally, we also evaluate our policy\'s robustness against force perturbation. Specifically, we perturb the object during deployment at random times by poking or pushing it along random directions using a picker tool (see the left of Figure 8). We find that our policy can reorient and translate the object back to the center for continual manipulation, indicating that it has some robustness to external forces and can adapt to these unexpected changes. Note that we use a marker-based object detection system in this experiment to disentangle the visual occlusion effect.\n' +
      '\n' +
      '## VIII Conclusion\n' +
      '\n' +
      'In this paper, we consider the task of twisting or removing lids of bottle-like objects with two hands. We present a RL-based sim-to-real system to solve this problem, and propose various techniques to handle the challenges that arise - including a novel reward design, a sparse object representation for real-time perception, and an efficient yet high-fidelity method to simulate twisting bottle caps. We conduct experiments in both simulation and real world to demonstrate the effectiveness of our approach. Our real-world results show generalization across a wide range of seen and unseen objects. We hope our system can inspire future researchers to tackle bimanual dexterous hands challenges with RL and sim-to-real.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'We thank Chen Wang and Yuzhe Qin for helpful discussions on hardware setup and simulation of the Allegro Hand. TL is supported by fellowships from the National Science Foundation and UC Berkeley. ZY is supported by funding from InnoHK Centre for Logistics Robotics and ONR MURI N00014-22-1-2773. HQ is supported by the DARPA Machine Common Sense and ONR MURI N00014-21-1-2801.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Alexandre Amice, Peter Werner, and Russ Tedrake. Certifying bimanual rrt motion plans in a second. _arXiv:2310.16603_, 2023.\n' +
      '* [2] Sridhar Pandian Arunachalam, Irmak Guzey, Soumith Chintala, and Lerrel Pinto. Holo-dex: Teaching dexterity with immersive mixed reality. In _ICRA_, 2023.\n' +
      '\n' +
      'Figure 8: Perturbing a learned policy with random external force. Our policy is resilient to these external forces and recovers to normal. See our website for video visualizations.\n' +
      '\n' +
      'Figure 7: Behavior of different reward functions. _Top_: Our full reward function achieves a stable grasp, as well as a smooth, natural, and human-like twisting motion. _Middle_: A naive gait constraint reward without any contact hints leads to erratic finger motion and unnatural grasps. _Bottom_: A reduced contact reward yields somewhat natural behavior, but the grasp is loose and ungainly compared to the full contact reward case.\n' +
      '\n' +
      '* [3] Sridhar Pandian Arunachalam, Sneha Silwal, Ben Evans, and Lerrel Pinto. Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation. In _ICRA_, 2023.\n' +
      '* [4] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. In _ICLR_, 2019.\n' +
      '* [5] Fabrizio Caccavale, Pasquale Chiacchio, Alessandro Marino, and Luigi Villani. Six-dof impedance control of dual-arm cooperative manipulators. _Transactions on Mechatronics_, 2008.\n' +
      '* [6] Konstantinos Chatzilygeroudis, Bernardo Fichera, Ilaria Lauzana, Fanjun Bu, Kunpeng Yao, Farshad Khadivar, and Aude Billard. Benchmark for bimanual robotic manipulation of semi-deformable objects. _RA-L_, 2020.\n' +
      '* [7] Tao Chen, Jie Xu, and Pulkit Agrawal. A system for general in-hand object re-orientation. In _CoRL_, 2021.\n' +
      '* [8] Tao Chen, Megha Tippur, Siyang Wu, Vikash Kumar, Edward Adelson, and Pulkit Agrawal. Visual dexterity: In-hand reorientation of novel and complex object shapes. _Science Robotics_, 2023.\n' +
      '* [9] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, Hao Dong, Song-Chun Zhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. In _NeurIPS_, 2022.\n' +
      '* [10] Yuanpei Chen, Chen Wang, Li Fei-Fei, and C Karen Liu. Sequential dexterity: Chaining dexterous policies for long-horizon manipulation. In _CoRL_, 2023.\n' +
      '* [11] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an atkinson-shiftrin memory model. In _ECCV_, 2022.\n' +
      '* [12] Zih-Yun Chiu, Florian Richter, Emily K Funk, Ryan K Orosco, and Michael C Yip. Bimanual regrasping for suture needles using reinforcement learning for rapid motion planning. In _ICRA_, 2021.\n' +
      '* [13] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units. _arXiv:1511.07289_, 2015.\n' +
      '* [14] Thomas Cohn, Seiji Shaw, Max Simchowitz, and Russ Tedrake. Constrained bimanual planning with analytic inverse kinematics. _arXiv:2309.08770_, 2023.\n' +
      '* [15] Hongjie Fang, Hao-Shu Fang, Yiming Wang, Jieji Ren, Jingjing Chen, Ruo Zhang, Weiming Wang, and Cewu Lu. Low-cost exoskeletons for learning whole-arm manipulation in the wild. _arXiv:2309.14975_, 2023.\n' +
      '* [16] Ankur Handa, Karl Van Wyk, Wei Yang, Jacky Liang, Yu-Wei Chao, Qian Wan, Stan Birchfield, Nathan Ratliff, and Dieter Fox. Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system. In _ICRA_, 2020.\n' +
      '* [17] Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, and Gavriel State. Dexterne: Transfer of agile in-hand manipulation from simulation to reality. In _ICRA_, 2023.\n' +
      '* [18] Binghao Huang, Yuanpei Chen, Tianyu Wang, Yuzhe Qin, Yaodong Yang, Nikolay Atanasov, and Xiaolong Wang. Dynamic handover: Throw and catch with bimanual hands. In _CoRL_, 2023.\n' +
      '* [19] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. _Science Robotics_, 2019.\n' +
      '* [20] Satoshi Kataoka, Seyed Kamyar Seyed Ghasemipour, Daniel Freeman, and Igor Mordatch. Bi-manual manipulation and attachment via sim-to-real reinforcement learning. _arXiv:2203.08277_, 2022.\n' +
      '* [21] Gagan Khandate, Siqi Shang, Eric T Chang, Tristan Luca Saidi, Johnson Adams, and Matei Ciocarlie. Sampling-based exploration for reinforcement learning of dexterous manipulation. In _RSS_, 2023.\n' +
      '* [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _ICCV_, 2023.\n' +
      '* [23] Franziska Krebs and Tamim Asfour. A bimanual manipulation taxonomy. _RA-L_, 2022.\n' +
      '* [24] Franziska Krebs, Andre Meixner, Isabel Patzer, and Tamim Asfour. The kit bimanual manipulation dataset. In _Humanoids_, 2021.\n' +
      '* [25] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In _RSS_, 2021.\n' +
      '* [26] Marco Laghi, Michele Maimeri, Mathieu Marchand, Clara Leparoux, Manuel Catalano, Arash Ajoudani, and Antonio Bicchi. Shared-autonomy control for intuitive bimanual tele-manipulation. In _Humanoids_, 2018.\n' +
      '* [27] Yunfei Li, Chaoyi Pan, Huazhe Xu, Xiaolong Wang, and Yi Wu. Efficient bimanual handover and rearrangement via symmetry-aware actor-critic learning. In _ICRA_, 2023.\n' +
      '* [28] Zhijun Li, Bo Huang, Arash Ajoudani, Chenguang Yang, Chun-Yi Su, and Antonio Bicchi. Asymmetric bimanual control of dual-arm exoskeletons for human-cooperative manipulations. _Transactions on Robotics_, 2017.\n' +
      '* [29] Toru Lin and Allan Jabri. Mimex: Intrinsic rewards from masked input modeling. In _NeurIPS_, 2023.\n' +
      '* [30] Yijiong Lin, Alex Church, Max Yang, Haoran Li, John Lloyd, Dandan Zhang, and Nathan F Lepora. Bi-touch: Bimanual tactile manipulation with sim-to-real deep reinforcement learning. _RA-L_, 2023.\n' +
      '* [31] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning. _arXiv:2108.10470_, 2021.\n' +
      '* [32] Takahiro Miki, Joonho Lee, Jemin Hwangbo, LorenzWellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. _Science Robotics_, 2022.\n' +
      '* [33] Yashraj Narang, Kier Storey, Iretiayo Akinola, Miles Macklin, Philipp Reist, Lukasz Wawrzyniak, Yunrong Guo, Adam Morawanszky, Gavriel State, Michelle Lu, Ankur Handa, and Dieter Fox. Factory: Fast contact for robotic assembly. In _RSS_, 2022.\n' +
      '* [34] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving rubik\'s cube with a robot hand. _arXiv:1910.07113_, 2019.\n' +
      '* [35] Christian Ott, Oliver Eiberger, Werner Friedl, Berthold Bauml, Ulrich Hillenbrand, Christoph Borst, Alin Albushaffer, Bernhard Brunner, Heiko Hirschmuller, Simon Kielhofer, Rainer Konietschke, Michael Suppa, Thomas Wimbock, Franziska Zacharias, and Gerhard Hirzinger. A humanoid two-arm system for dexterous manipulation. In _Humanoids_, 2006.\n' +
      '* [36] American Academy Of Pediatrics. _Caring for Your Baby and Young Child: Birth to Age 5_. American Academy Of Pediatrics, 2019.\n' +
      '* [37] Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. In _RSS_, 2018.\n' +
      '* [38] Johannes Pitz, Lennart Rostel, Leon Sievers, and Berthold Bauml. Dextrous tactile in-hand manipulation using a modular reinforcement learning architecture. In _ICRA_, 2023.\n' +
      '* [39] Robert Platt, Andrew H. Fagg, and Roderic A. Grupen. Manipulation Gaits: Sequences of Grasp Control Tasks. In _ICRA_, 2004.\n' +
      '* [40] Haozhi Qi, Ashish Kumar, Roberto Calandra, Yi Ma, and Jitendra Malik. In-hand object rotation via rapid motor adaptation. In _CoRL_, 2022.\n' +
      '* [41] Haozhi Qi, Brent Yi, Yi Ma, Sudharshan Suresh, Mike Lambeta, Roberto Calandra, and Jitendra Malik. General in-hand object rotation with vision and touch. In _CoRL_, 2023.\n' +
      '* [42] Yuzhe Qin, Hao Su, and Xiaolong Wang. From one hand to multiple hands: Imitation learning for dexterous manipulation from single-camera teleoperation. _RA-L_, 2022.\n' +
      '* [43] Yuzhe Qin, Wei Yang, Binghao Huang, Karl Van Wyk, Hao Su, Xiaolong Wang, Yu-Wei Chao, and Dietor Fox. Anyteletop: A general vision-based dexterous robot arm-hand teleoperation system. In _RSS_, 2023.\n' +
      '* [44] Lennart Rostel, Johannes Pitz, Leon Sievers, and Berthold Bauml. Estimator-coupled reinforcement learning for robust purely tactile in-hand manipulation. In _Humanoids_, 2023.\n' +
      '* [45] Nilanjan Sarkar, Xiaoping Yun, and Vijay R. Kumar. Dynamic control of 3-d rolling contacts in two-arm manipulation. In _ICRA_, 1993.\n' +
      '* [46] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. _arXiv:1506.02438_, 2015.\n' +
      '* [47] Lucy Xiaoyang Shi, Archit Sharma, Tony Z Zhao, and Chelsea Finn. Waypoint-based imitation learning for robotic manipulation. In _CoRL_, 2023.\n' +
      '* a survey. _Robotics and Autonomous Systems_, 2012.\n' +
      '* [49] Nicolas Sommer, Miao Li, and Aude Billard. Bimanual compliant tactile exploration for grasping unknown objects. In _ICRA_, 2014.\n' +
      '* [50] Jan Steffen, Christof Elbrecht, Robert Haschke, and Helge Ritter. Bio-inspired motion strategies for a bimanual manipulation task. In _Humanoids_, 2010.\n' +
      '* [51] Simon Stepputtis, Maryam Bandari, Stefan Schaal, and Heni Ben Amor. A system for imitation learning of contact-rich bimanual manipulation policies. In _IROS_, 2022.\n' +
      '* [52] Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik, Mrinal Kalakrishnan, Roberto Calandra, Michael Kaess, Joe Ortiz, and Mustafa Mukadam. Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation. _arXiv:2312.13469_, 2023.\n' +
      '* [53] Nikolaus Vahrenkamp, Markus Przybylski, Tamim Asfour, and Rudiger Dillmann. Bimanual grasp planning. In _Humanoids_, 2011.\n' +
      '* [54] Renee Watling. _Peabody Developmental Motor Scales_. Springer, 2013.\n' +
      '* [55] Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. Rotating without seeing: Towards in-hand dexterity through touch. In _RSS_, 2023.\n' +
      '* [56] Ying Yuan, Haichuan Che, Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Kang-Won Lee, Yi Wu, Soo-Chul Lim, and Xiaolong Wang. Robot synesthesia: In-hand manipulation with visuotactile sensing. In _ICRA_, 2024.\n' +
      '* [57] Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi, Taylor Howell, Xue Bin Peng, Sumeet Singh, Yuval Tassa, Pete Florence, Andy Zeng, et al. Robopianist: Dexterous piano playing with deep reinforcement learning. In _CoRL_, 2023.\n' +
      '* [58] Kevin Zhang, Mohit Sharma, Manuela Veloso, and Oliver Kroemer. Leveraging multimodal haptic sensory data for robust cutting. In _Humanoids_, 2019.\n' +
      '* [59] Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In _RSS_, 2023.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>