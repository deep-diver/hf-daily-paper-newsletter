<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      'Tang et al., 2023a). 우리는 LLM을 훈련하기 위해 널리 채택되고 안정적인 방법을 기반으로 다른 접근법을 사용한다(Ouyang et al., 2022). 구체적으로, 사전 훈련 및 감독 미세 조정(SFT)의 두 가지 훈련 단계를 사용하며, 각각은 서로 다른 하위 집합과 훈련 기술을 사용한다. 이러한 혁신으로 Audio Flamingo는 Chu et al. (2023) 및 Gong et al. (2023c)와 같이 \\(<\\frac{1}{3}\\) 수의 매개변수를 사용하여 여러 오디오 이해 벤치마크에서 최첨단 결과를 얻을 수 있다.\n' +
      '\n' +
      '세 번째 과제는 오디오 언어 모델에 예를 들어, ICL(in-context learning) 및 검색을 통해 미세 조정 없이 새로운 작업에 신속하게 적응할 수 있는 능력을 부여하는 것이다(Brown et al., 2020). 최근의 오디오 언어 모델들은 제로-샷 능력들(Deshmukh et al., 2023; Gong et al., 2023c)을 보여주었지만, 이들은 새로운 태스크들에 대해 문맥 내 수-샷 학습을 수행하는 능력이 부족하다. 본 논문에서는 이러한 능력을 실현하기 위한 일련의 기법을 소개한다. 효율적인 검색 방법을 구현하고, ICL 템플릿을 소개하고, 검색된 샘플을 사용하여 인터리브된 ICL 데이터 세트를 생성한다. 또한 인터리브 샘플용 새로운 크로스 어텐션 마스크를 소개합니다. 결과적으로 오디오 플라밍고는 태스크별 미세 조정 없이 ICL과 검색을 통해 새로운 태스크에 빠르게 적응할 수 있다. 우리의 결과는 우리의 접근법의 유효성을 확인하고 새로운 최첨단 몇 가지 벤치마크를 설정한다.\n' +
      '\n' +
      '마지막 과제는 오디오 언어 모델에 많은 라운드를 위해 사용자와 채팅할 수 있는 능력을 부여하는 것이다. 이전의 방법들은 대화의 데모(Gong et al., 2023c; Chu et al., 2023)를 보여주었지만, 그것들은 체계적이고 정량적인 증거가 부족하다. 이 문제를 해결하기 위해 상관 컨텍스트를 강조하여 두 데이터 세트의 상세한 주석을 기반으로 GPT-4(Achiam et al., 2023)와 함께 두 개의 다중 턴 대화 데이터 세트를 생성한다. 이러한 데이터셋에서 오디오 플라밍고를 미세 조정함으로써 채팅 모델을 얻는다. 본 논문에서 제안한 채팅 모델은 멀티턴 대화 능력이 뛰어나고, 기존 방법보다 뛰어난 성능을 보임을 보인다.\n' +
      '\n' +
      '우리는 크고 다양한 근접 및 개방형 벤치마크 세트에서 오디오 플라밍고를 평가한다. _single_ Audio Flamingo 모델은 대부분의 벤치마크에서 이전의 최신 기술을 능가하며, Audio Flamingo의 _chat_ 버전은 대화 벤치마크에서 벤치마크를 상당히 능가한다. <그림 1>은 오디오 플라밍고의 벤치마크 결과를 요약한 것이다. 또한 실험에서 신경 구조와 하이퍼 파라미터에 대해 간략하게 논의한다. 우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. 우리는 일련의 혁신으로 오디오 이해를 위한 Audio Flamingo: Flamingo 기반의 오디오 언어 모델을 제안한다. 오디오 플라밍고는 여러 개의 클로즈엔드 및 오픈엔드 오디오 이해 작업에서 최첨단 결과를 달성합니다.\n' +
      '2. ICL과 검색의 효율적인 사용을 위한 일련의 방법론을 설계하여 최신 소수의 학습 결과를 도출한다.\n' +
      '3. 오디오 플라밍고가 강한 멀티턴 대화 능력을 가질 수 있도록 하며, 베이스라인 방식에 비해 월등히 우수한 결과를 보인다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '**멀티모달 LLMs.** 멀티모달 LLMs의 영역에서 엄청난 진전이 있었다. 텍스트 외에도, 이들 모델은 비전(Tsimpoukelli et al., 2021; Alayrac et al., 2022; Yang et al., 2023; Driess et al., 2023; Liu et al., 2023a; Li et al., 2023a), 오디오(Deshmukh et al., 2023; Gong et al., 2023b; Rubenstein et al., 2023), 또는 이들의 다중(Han et al., 2023; Tang et al., 2023b; Moon et al., 2023; Zhao et al., 2023)과 같은 다양한 모달리티로부터의 입력을 취하고, 각각은 상이한 통합 방법을 갖는다. 오디오 모달리티에서, 이전 작업들은 스피치 태스크들(Chen et al., 2023; Rubenstein et al., 2023), 일반적인 오디오 이해(Deshmukh et al., 2023; Gong et al., 2023c), 음악 이해(Gardner et al., 2023; Won et al., 2023; Li et al., 2023c; Liu et al., 2023b; Doh et al., 2023) 또는 이들의 조합(Gong et al., 2023b; Tang et al., 2023a; Chu et al., 2023)을 살펴보았다. 본 논문의 초점은 오디오 이해이며, 이는 비-음성 사운드 및 음악을 포함하고, 비-언어 스피치를 포함한다. 이전 작업들과 달리, 우리의 모델은 더 강한 오디오 이해 능력을 가지며, _i)_context few-shot 학습 능력, _ii)_검색 증강 생성 능력, 및 _iii)을 갖는 첫 번째 오디오 이해 모델이다. … 강한 멀티턴 대화 능력.\n' +
      '\n' +
      '도 1: 오디오 플라밍고 대 이전의 최신 상태(Deshmukh et al., 2023; Chu et al., 2023; Gong et al., 2023b;c Tang et al., 2023a; Ghosh et al., 2023)에 대한 다수의 오디오 이해 벤치마크. 모든 모델의 최대값이 각 작업에서 100%가 되도록 숫자를 정규화한다. 오디오 플라밍고는 이러한 대부분의 작업에 새로운 최첨단 결과를 설정한다.\n' +
      '\n' +
      '**오디오 인코더 및 표현.** 많은 오디오 인코더는 CNN 기반 방법(Kong et al., 2020) 및 Transformer 기반 방법(Gong et al., 2021; Chen et al., 2022; Defossez et al., 2022; Radford et al., 2023; Gong et al., 2023a)을 포함하여 스펙트로그램으로부터 오디오 특징을 추출한다. 이러한 방법들은 주로 음성 인식 또는 이벤트 검출과 같은 특정 문제를 해결하는 것을 목표로 한다. 이러한 인코더들에 기초하여, 많은 공동 오디오-언어 임베딩들이 제안되었다(Elizalde et al., 2023a, b; Wu et al., 2023; Huang et al., 2022; Li et al., 2023c). 이러한 방법들은 오디오 및 언어 임베딩을 동일한 공간에 매핑하기 위해 대비 학습을 사용하며, 종종 매우 다양한 오디오 및 언어에 대해 트레이닝된다. 그러나, 이러한 방법들 중 다수는 오디오에 대한 단일 임베딩을 계산하므로 시간적 정보를 잃을 수 있다. 본 논문에서는 ClapCap(Elizalde et al., 2023b)을 기반으로 슬라이딩 윈도우를 갖는 오디오 인코더를 구축하여 장거리 및 시간 정보를 더 잘 포착한다.\n' +
      '\n' +
      '**Data augmentation.** 음성 전사 외에 제한된 양의 고품질 인간 주석이 달린 소리들로 인해, 많은 작업들이 GPT-4와 같은 기존의 LLM들로 텍스처 디스크립션을 증강하기 위해 제안되었다(Achiam et al., 2023). 공통 전략은 주석이 달린 태그, 타임스탬프 및 기타 잡다한 정보를 LLM에 제공한 후, 캡션(Wu et al., 2023; Doh et al., 2023; Mei et al., 2023; Gardner et al., 2023) 또는 질문-응답 데이터 쌍(Gong et al., 2023c;b; Liu et al., 2023b)을 생성하도록 요청하는 것이다. 본 논문에서는 기존의 LLM을 활용하여 상세한 주석을 기반으로 두 개의 다중회전 대화 데이터셋을 생성함으로써 모델의 강력한 대화 능력을 가능하게 한다.\n' +
      '\n' +
      '**in-context learning(ICL)**in-context learning은 일종의 few-shot learning 능력으로, 프롬프트에서 몇 가지 예를 살펴본 후에야 LLM이 추론 시간에 원하는 작업에 빠르게 적응한다(Brown et al., 2020). 자연어 과제(Wei et al., 2021) 및 시각 언어 과제(Alayrac et al., 2022; Yang et al., 2023)에서 널리 성공을 보여주었다. 음성 도메인에서, ICL은 음성 인식, 번역, 및 프로세싱과 같은 음성 관련 태스크들을 돕는 것으로 나타났다(Gao et al., 2022; Wang et al., 2023; Hsu et al., 2023; Chen et al., 2023). 그러나, 일반적인 오디오 이해를 위한 ICL은 훨씬 덜 탐구된다. 본 논문에서는 ICL 기능을 갖는 첫 번째 오디오 이해 모델을 제안한다.\n' +
      '\n' +
      '** 검색-증강 생성(RAG. Retrieval-augmented generation)** LLMs에 대한 검색-증강 생성은, 예를 들어 유용하고 관련된 지식을 포함하는 외부 데이터베이스로부터의 외부 지식을 이용함으로써 생성 품질을 향상시키는 것이다. 자연어 과제(Guu et al., 2020; Karpukhin et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022) 및 시각 언어 모델(Yang et al., 2023)에 널리 적용되어 왔다. 오디오 언어 도메인에서 Ghosh et al. (2023)은 유사한 오디오로부터 프롬프트에 대한 캡션을 프리펜딩함으로써 오디오 캡션을 위한 검색 방법을 제안하였다. 그러나 검색된 오디오를 모델에 제공하지 않습니다. 결과적으로, 모델은 검색된 오디오가 테스트 오디오와 얼마나 유사한지에 대한 정보를 상실한다. 대조적으로, 우리는 검색된 오디오와 텍스트를 모델에 모두 제공합니다. 이 접근법의 장점은 테스트와 검색된 오디오 사이의 유사성을 기반으로 검색의 시간과 방법을 결정할 수 있다는 것이다. 우리는 몇 안 되는 실험에서 비교를 제공한다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '본 절에서는 ICL과 RAG를 통한 소수의 샷 학습이 가능한 오디오 이해 언어 모델인 Audio Flamingo를 소개한다. 3.1절에서는 오디오 특징 추출기, 오디오 표현 변환 계층, 언어 모델 및 컨디셔닝 방법을 포함하여 오디오 플라밍고에서 사용되는 아키텍처를 소개한다. 3.2절에서는 학습 목표, 마스크 설계, 학습 단계를 포함한 오디오 플라밍고의 학습 방법을 소개한다.\n' +
      '\n' +
      '### Architecture\n' +
      '\n' +
      '제안하는 신경망 구조는 슬라이딩 윈도우를 갖는 오디오 특징 추출기, 오디오 표현 변환 계층, 디코더 전용 언어 모델, 그리고 gated xattn-dense 계층으로 구성된다. 그림 2는 아키텍처를 요약한 것이다.\n' +
      '\n' +
      '_i)_** 슬라이딩 윈도우를 갖는 오디오 특징 추출기.** 오디오 특징 추출기 백본으로서 ClapCap(Elizalde et al., 2023b)을 사용하며, 이는 \\(\\mathcal{E}\\)으로 나타낸다. ClapCap은 7초 44.1kHz 원시 오디오를 입력으로 한 다음, 오디오를 홉 길이 320, 윈도우 길이 1024, 64 멜 빈의 Mel-스펙트로그램으로 변환하고 마지막으로 1024차원 벡터 표현을 출력한다.\n' +
      '\n' +
      '우리는 각 7초 세그먼트를 창으로 간주하고 슬라이딩 창을 사용하여 더 긴 오디오를 위한 특징을 추출한다. 연속 윈도우 사이의 중첩은 \\(7\\times 0.75=5.25\\)초이다. 최대 16\\(m=16\\)의 슬라이딩 윈도우를 사용한다. 형식적으로, \\(s\\!:\\!t)\\)을 오디오 \\(x\\)에서 \\(s\\)에서 \\(t\\)초의 세그먼트로 하자. 그리고 추출된 특징은 \\(\\Big{[}\\mathcal{E}(x(0\\!:\\!7)),\\mathcal{E}(x(\\frac{7}{4}\\!:\\!\\frac{7\\times 5}{4})),\\cdots,\\mathcal{E}(x(\\frac{7(m-1)}{4}\\!:\\!\\frac{7(m+3)}{4})\\Big{] }\\), 최대 33.25초를 지원한다. 이 설계의 목표는 단일 융합 표현 벡터에서 무시될 수 있는 장거리 및 시간 정보를 캡처하는 것이다(Wu et al., 2023). 긴 오디오는 자르고 짧은 오디오는 제로 패딩됩니다.\n' +
      '\n' +
      '전체 세그먼트가 제로 패딩되면 교차 주의에서 해당 임베딩을 마스킹할 것이다. 만약 입력이 \\(>1\\) 오디오와 인터리빙된 데이터라면, 우리는 그들의 슬라이딩 윈도우 표현들을 연결한다.\n' +
      '\n' +
      '_ii)_ **오디오 표현 변환 계층.** 앞에서 설명한 연접된 오디오 특징 표현들에 몇 개의 오디오 표현 변환 계층들을 추가로 적용함으로써 모델 용량을 증가시킨다. 그것은 3개의 자기 주의 층(Vaswani et al., 2017)으로 구성되며, 각각 8개의 헤드와 내부 치수 2048이 있다. 이 모듈은 완전히 훈련할 수 있습니다.\n' +
      '\n' +
      '_iii)_**언어 모델.** 우리는 디코더 전용 인과 LM을 우리의 아키텍처에서 사용한다. 본 논문에서는 24개의 LM 블록을 갖는 1.3B 파라미터 모델인 OPT-IML-MAX-1.3B (Iyer et al., 2022)를 사용한다. 그것은 많은 자연 언어 과제에 대해 지도 조정되었다.\n' +
      '\n' +
      '_iv)_ **Conditioning LM on audio representation.** We use gated xattn-dense layers from Flamingo (Alayrac et al., 2022) to achieve conditioning on audio input. Each layer has two block: 1) residual block with cross attention and tanh gating, followed then 2) 밀도가 높은 층과 탄 게이팅이 있는 잔차 블록입니다. 이러한 도면층은 각 LM 블록에 미리 지정됩니다.\n' +
      '\n' +
      '### Training Method\n' +
      '\n' +
      '● 단채널 오디오 입력은 \\(x\\), 명령 텍스트는 \\(y_{\\text{ins}}\\), 출력 텍스트는 \\(y_{\\text{out}}\\)으로 한다. 간결성을 위해 각 훈련 표본을 표현하기 위해 \\(z=(x,y_{\\text{ins}},y_{\\text{out}})\\)을 사용한다.\n' +
      '\n' +
      '우리는 모델을 훈련하기 위해 최대 우도 추정(MLE)을 사용한다. \\((y_{\\text{out}})_{t}\\)을 \\(t\\)번째 토큰으로 하고, \\((y_{\\text{out}})_{<t}\\)을 출력의 첫번째 \\(t-1\\)번째 토큰으로 한다. 인터리빙되지 않은 샘플\\(z=(x,y_{\\text{ins},y_{\\text{out})})에 대해 로그 우도는\n' +
      '\n' +
      '\\[\\mathcal{L}(z)=\\sum_{t=1}^{|y_{\\text{out}}|}\\log p_{\\theta}\\left((y_{\\text{out}})_{t}|x,y_{\\text{ins}},(y_{\\text{out}})_{<t}\\right). \\tag{1}\\\n' +
      '\n' +
      '\\(j\\)개의 샘플로 구성된 인터리빙된 훈련 샘플에 대해, \\(z_{\\text{int}}=\\{z^{1},\\cdots,z^{J}\\}), 여기서 \\(z^{j}=(x^{j},y^{j}_{\\text{ins},y^{j}_{\\text{out})\\)의 로그 우도는 모든 출력에서 계산된다:\n' +
      '\n' +
      '{L}_{\\text{int}}(z_{\\text{int}}=\\{z^{1}, \\cdots,z^{J}\\})=\\\\\\\\sum_{j=1}^{j}\\sum_{t=1}^{|y^{j}_{\\text{out}}|}\\log p_{\\theta}\\left((y^{j}_{\\text{out}}_{t}|z^{<j},x^{j},y^{j}_{\\text{ins},(y^{j}_{\\text{out}})_{<t}\\right.\\end{split}\\tag{2}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\t}\\\n' +
      '\n' +
      '이 인터리빙된 데이터 손실은 마지막 출력\\(y^{J}_{\\text{out}}\\)(Yang et al., 2023)에서만 손실을 계산하거나 이전 멀티모달 입력\\(x^{<j}\\)(Alayrac et al., 2022; Ghosh et al., 2023)에서 손실이 없거나 간접 컨디셔닝을 갖는 이전 모델과 다르다. 우리는 (2)가 모델이 연관된 오디오뿐만 아니라 ICL 샘플들의 다양한 수(0 when \\(j=1\\)를 포함하는)를 살펴보는 데 도움을 줄 수 있을 것으로 기대하며, 따라서 버킷팅(Khomenko et al., 2016)과 유사한 방식으로 견고성 및 트레이닝 효율을 향상시킬 수 있으며, 특히 ICL 샘플들이 유사한 샘플들을 검색할 때 더욱 그러하다. 해당 손실 마스크는 그림 3의 오른쪽에 나와 있다.\n' +
      '\n' +
      '(\\{\\mathcal{D}^{i},i\\in\\mathcal{I}\\}\\)을 모든 비인터리브 학습 데이터셋으로 하고, \\(\\{\\mathcal{D}^{i}^{prime}_{\\text{int},i^{\\prime}\\in\\mathcal{I}_{\\text{int}\\}\\)을 모든 인터리브 학습 데이터셋으로 한다. 전체 트레이닝 목표는 각각의 데이터세트 상의 손실들의 가중된 혼합물이다:\n' +
      '\n' +
      '\\lambda_{i}\\mathcal{E}_{z\\sim\\mathcal{I}}\\mathcal{L}(L=-\\sum_{i\\in\\mathcal{L}(z)-\\sum_{i^{\\prime}\\mathcal{I}{text{int}}\\lambda_{i^{\\prime}\\mathbb{E}_{z\\sim\\mathcal{D}^{i^{\\prime}}_{\\text{int}}(z_{\\text{int}}), \\tag{3}\\mathcal{L}{i^{\\text{int}}\n' +
      '\n' +
      '여기서 \\(\\lambda_{i}\\)\'s는 각 데이터 세트에 대한 가중치이다. 가중치는 일정한 하이퍼-파라미터이며 최종 모델에 큰 영향을 미친다. 각 데이터 세트에 대해 미리 정의된 에포크 수(직관은 섹션 4.1 참조, 세부 사항은 부록 A 참조)에서 계산됩니다.\n' +
      '\n' +
      '_ii)_ ** 교차 어텐션 마스크.** 블록 상부 삼각 교차 어텐션 마스크를 인터리브 샘플에 사용하여 첫 번째 오디오 입력(x^{\\leq j}\\)에서만 \\(j\\)번째 출력(p^{theta}(y^{j}_{\\text{out}))의 우도가 조정되도록 한다. 모델이 이전 오디오를 보는 데 도움이 될 것으로 기대합니다. 도 3은 마스크를 설명한다.\n' +
      '\n' +
      '_iii)_*2개의 훈련 단계.** 훈련은 사전 훈련과 널리 사용되는 SFT(supervised fine-tuning)로 나눈다.\n' +
      '\n' +
      '도 2: 오디오 플라밍고의 신경망 아키텍처. 인터리빙된 오디오와 텍스트를 입력으로 하고 자유 형식의 텍스트를 출력한다.\n' +
      '\n' +
      'LMs(Ouyang et al., 2022) 훈련에서 채택되고 안정적인 방법. 사전 훈련 동안 우리는 오디오 표현 변환 계층과 게이트 xattn-밀도 계층만 훈련한다. 목적은 이들 계층에 대한 양호한 초기화 가중치 세트를 얻는 것이다. SFT 동안 우리는 전체 LM을 동결 해제하고 오디오 인코더를 제외한 모든 모듈을 훈련한다. 2\n' +
      '\n' +
      '각주 2: 초기 실험에서 우리는 오디오 인코더가 얼지 않는 것이 훨씬 더 나쁜 결과를 초래한다는 것을 발견했다.\n' +
      '\n' +
      '## 4 Data\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '이 섹션에서는 데이터 세트 수집, 생성 및 혼합을 포함한 데이터 전략을 소개한다. 또한 각 데이터 세트 유형에 대한 템플릿을 소개합니다.\n' +
      '\n' +
      '**데이터셋 소스.** 우리는 음악, 비음성 일반 사운드, 비언어 스피치의 세 가지 유형으로 대별할 수 있는 다양한 오디오 데이터세트에 대해 모델을 훈련한다. 본 논문에서는 최첨단 음성 전사 모델의 즉각적인 가용성을 고려하여 이러한 데이터에 초점을 맞춘다. 우리는 (1)_audio captioning_ (CAP)와 (2)_audio question-answering_ (AQA)와 (3)_audio classification_ (CLS)의 세 가지 유형의 작업을 살펴본다. (2)_audio question-answering_ (AQA)는 모델이 오디오에 대한 질문에 답하기를 원하는 것이고, (3)_audio classification_ (CLS)는 모델이 사운드를 이벤트, 장면, 음악 장르, 악기, 품질 등에 해당하는 하나 이상의 레이블로 분류하기를 원한다. 모든 훈련 데이터 세트의 개요는 부록의 표 6에 나와 있다.\n' +
      '\n' +
      '**ICL 데이터 세트.** 모델 내 학습 및 검색 증강 능력을 제공하기 위해 오디오 임베딩에 대해 계산된 \\(k\\)NN을 기반으로 각 원시 데이터 세트에 대한 ICL 데이터 세트를 구성한다. \\(\\mathcal{D}^{i}\\)을 \\(i\\)번째 훈련 데이터세트로 하자. 각 \\(z=(x,y_{\\text{ins}},y_{\\text{out}})\\in\\mathcal{D}^{i}\\)에 대해, 우리는 \\(z\\)을 제외한 \\(\\mathcal{D}^{i}\\)에서 그 상위\\(k\\)의 가장 가까운 훈련 샘플을 찾는데, 여기서 거리 함수는 융합된 LAION-CLAP 임베딩 공간(Wu et al., 2023)에서 \\(\\ell_{2}\\)이다. 우리는 검색을 가속화하기 위해 Faiss-gpu(Johnson et al., 2019)를 사용한다. 도 4는 이러한 과정을 설명한다.\n' +
      '\n' +
      '**Dataset stageaging and blending.** Pre-training 단계와 Supervised Fine-tuning(SFT) 단계에서 서로 다른 데이터셋을 사용한다. 선택은 아래와 같이 데이터 품질, 다양성, 소스, 크기를 기준으로 한다. 1) 데이터 품질: 저품질 또는 잡음이 있는 오디오, 저품질 텍스트 및 부정확한 텍스트 주석을 포함하는 저품질 데이터 세트가 사전 훈련에 사용된다. 2) 데이터 다양성: 레이블 분포에서 다양성이 적거나 강한 편향을 갖는 데이터 세트가 사전 훈련에 사용된다. 3) 데이터 소스: AI 생성 콘텐츠를 포함하는 데이터 세트는 대부분 사전 훈련에 사용되는 반면, 일부 고품질 하위 세트는 SFT에 사용될 수 있다. 4) 데이터 크기: 매우 큰 데이터 세트는 사전 훈련 및 SFT 모두에 사용될 수 있다. 5) ICL 데이터셋은 SFT 단계에서 사용된다.\n' +
      '\n' +
      '크기, 품질 및 다양성에 따라 서로 다른 데이터 세트에서 샘플링할 때 서로 다른 가중치\\(\\lambda_{i}\\)를 할당한다. 가중치는 각 데이터 세트에 대한 에폭의 수로부터 계산된다. 준비 및 가중치의 세부 사항은 부록 A에서 확인할 수 있다.\n' +
      '\n' +
      '### Templates\n' +
      '\n' +
      '우리의 템플릿은 OPT-IML의 템플릿(Iyer et al., 2022)과 플라밍고의 멀티모달 템플릿(Alayrac et al., 2022)을 기반으로 한다. 인터리빙되지 않은 샘플의 경우 템플릿이 아래에 설명되어 있습니다.\n' +
      '\n' +
      '도 4: RAG를 기반으로 한 ICL 샘플의 구성. LAION-CLAP을 사용하여 데이터베이스에서 가장 유사한 상위\\(k\\)개의 샘플을 찾고, 검색된 오디오와 텍스트를 사용하여 ICL 트레이닝 샘플을 구성한다.\n' +
      '\n' +
      '그림 3: 왼쪽: 텍스트 토큰과 오디오 임베딩 사이의 블록 상부-삼각형 교차 주의 마스크. 오른쪽: 인터리빙된 트레이닝 샘플의 손실 마스크.\n' +
      '\n' +
      '<audio>{instruction} Options:n- option1n- option2<SEP>{output}<EOC>\n' +
      '이 템플릿에서 <audio>는 언어 모델에 컨텍스트에서 오디오의 위치를 알려주는 특수 토큰이다. {태스크 디스크립션}은 언어 모델에게 자신이 어떤 태스크를 처리하고 있는지를 알려주는 자연어로서, 예를 들어 "_태스크는 이벤트 분류_"이다. {instruction}는 _question_와 같은 언어 명령어이다. 옵션 문장은 언어 모델에 분류를 위한 모든 옵션을 알려주어 자유로운 형태의 텍스트를 출력하여 오디오를 분류할 수 있도록 하는 것이다. {출력}은 훈련될 그라운드 트루스 출력이다. <SEP> 토큰은 명령어 끝을 나타내는 구분자이고, <EOC>는 샘플의 끝을 나타내는 end-of-chunk 토큰이다. 아래는 (오디오, 명령어, 출력)의 \\(k+1\\) 튜플을 갖는 인터리브된 (ICL) 샘플에 대한 템플릿이다.\n' +
      '<audio>{instruction1}<SEP>{output1}<EOC>...<audio>{instruction3}<SEP>{output1}<EOC><audio>{instruction4} optionions:n- option1n- option2<SEP>{output}<EOC>\n' +
      '#멀티턴 다이얼로그 데이터세트\n' +
      '\n' +
      '우리는 복잡한 멀티턴 대화를 다룰 때 모델에게 더 강력한 능력을 제공하는 것을 목표로 합니다. 이를 위해 GPT-4(Achiam et al., 2023)를 이용하여 두 개의 다회전 대화 데이터셋을 생성한다. 강하게 라벨링된 AudioSet-SL(Hershey et al., 2021) 및 MusicCaps(Agostinelli et al., 2023)를 기반으로 이러한 데이터 세트를 구성하고, 바람직하지 않은 샘플을 필터링하기 위해 LAION-CLAP 임베딩(Wu et al., 2023)을 기반으로 임계값을 설정한다. 생성된 두 데이터 세트의 이름은 각각 AF-Dialogue-AudioSetSL 및 AF-Dialogue-MusicCaps이다. 자세한 설명서, 필터링 방법, 데이터 세트 통계 및 예는 부록 B에 있습니다. 우리는 \\(s\\)-턴 대화 데이터 샘플에 다음 템플릿을 사용합니다.\n' +
      '<audio>user:{instruction1} assistant:<SEP>{output1}<EOC>...user:{instruction1} assistant:<SEP>{output1}<EOC>...``\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '이 절에서는 다음과 같은 질문에 답한다.\n' +
      '\n' +
      '**Q1.** 오디오 플라밍고가 최첨단 기준선보다 오디오를 더 잘 이해합니까?\n' +
      '\n' +
      '**Q2.** ICL 기반 RAG가 오디오 플라밍고가 새로운 작업에 적응하는 데 얼마나 큰 도움이 됩니까?\n' +
      '\n' +
      '**Q3.** 오디오 플라밍고가 사용자와 멀티턴 대화를 할 수 있는 능력은 무엇입니까?\n' +
      '\n' +
      '**Q4.** 오디오 플라밍고의 특정 구성이 전체적으로 가장 잘 작동합니까?\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리는 8개의 NVIDIA A100 GPU를 사용하여 모델을 훈련합니다. 사전 학습 시 batch size = 384, AdamW optimizer (Loshchilov and Hutter, 2017)를 사용하였으며 학습률은 10^{-4}\\, 무게감소는 0.1\\이다. 효율성을 위해 자동 혼합 정밀도로 _bf16_를 사용한다. SFT(supervised fine-tuning) 과정에서 배치 크기를 128, 학습률을 \\(2\\times 10^{-5}\\)으로 줄이고, 더 나은 수치 정밀도를 위해 _fp32_를 사용한다. 우리는 명시되지 않는 한 인터리브된 샘플의 최대 수를 8개로 했다. 우리는 텍스트 토큰의 최대 수를 512로 설정했다.\n' +
      '\n' +
      '우리는 여러 벤치마크에 대한 최신 최신 기준선과 비교합니다. 각 데이터 세트에서 모든 SOTA 기준선 중 가장 좋은 점수를 기준값으로 선택한다. 명시되지 않는 한, 우리는 질문-답변 및 단일-라벨 분류에 대한 정확도, 다중-라벨 분류에 대한 F1, 캡션 및 다이얼로그에 대한 CIDEr(Vedantam et al., 2015)를 보고한다. 주 우리는 자유 형식의 텍스트 출력을 사용하여 항상 모델을 평가합니다. 우리는 대화 상자를 제외한 모든 벤치마크에 대해 평가하려면 _single_ 모델을 사용하고 대화 상자에는 _chat_ 모델을 사용한다.\n' +
      '\n' +
      '제로 샷 및 소수 샷 벤치마크의 경우 이러한 데이터 세트는 사전 훈련 세트 및 SFT 세트에서 제외된다. 부모 데이터셋에서 파생된 것(예: AudioCaps audio는 AudioSet에서 파생된 것)에 대해, 우리는 부모 집합뿐만 아니라 그 부모 집합에서 파생된 다른 자식 집합에서 훈련 샘플을 제거했다.\n' +
      '\n' +
      '###Q1 : 강한 오디오 이해 능력\n' +
      '\n' +
      '본 논문에서 제안한 모델을 여러 가지 배포(열차 테스트) 벤치마크에 대해 평가하고, 최신 오디오 언어 모델 베이스라인들과 비교한다. 결과는 표 1과 같다. 우리는 FSD50k에서 부정확하지만 유사한 예측 레이블을 측정하기 위해 F1\\({}_{\\text{approx}}\\)을 정의하는데, 여기서 SentenceBERT_similarity(output, ground_truth) \\(>0.8\\)(Reimers and Gurevych, 2019, 2020)의 경우 예측이 정확하다고 간주한다. 이 메트릭은 기준선의 출력에도 적용됩니다.\n' +
      '\n' +
      '오디오 플라밍고는 대부분의 작업에서 훨씬 더 큰 LLMs(7B(Gong et al., 2023c;b; Chu et al., 2023) 또는 13B(Tang et al., 2023a))인 SOTA 기준선을 일치시키거나 능가할 수 있으며, 이는 제안된 방법이 강력한 오디오 이해 능력을 가지고 있음을 나타낸다. 우리의 모델은 또한 오디오를 더 잘 듣는다. ClothoAQA에서 본 모델은 수치질문에 대한 기준보다 10.4% 더 높은 정확도를 보여, 본 모델이 발생 횟수를 더 잘 이해하고 있음을 나타낸다. NSynth에서, 우리의 모델은 품질 예측에서 20.4% 더 높은 F1과 소스 예측에서 18.6% 더 높은 정확도를 가지고 있어, 우리의 모델이 오디오의 전반적인 품질을 더 잘 이해한다는 것을 나타낸다. 부록 C.2에서 우리는 정성적 샘플을 사용하여 우리 모델이 다른 소리의 출현 순서를 이해하고 시간에 따른 라우드니스와 그 변화를 인식하고 다른 물체로부터의 소리의 거리를 인식한다는 것을 보여준다.\n' +
      '\n' +
      '### Q2 : In-Context Few-Shot Learning\n' +
      '\n' +
      '우리는 보이지 않는 데이터 세트에 대해 평가될 때 오디오 플라밍고에서 ICL 기반 RAG의 효과를 측정하는 것을 목표로 한다.\n' +
      '\n' +
      '먼저, 표 2의 몇 가지 제로샷 벤치마크에 대한 결과와 SOTA 제로샷 방법과의 비교를 보고하며, 그 결과는 우리의 방법이 대부분의 작업에서 더 우수하고 강력한 일반화 능력을 가지고 있음을 나타낸다.\n' +
      '\n' +
      '그런 다음 ICL 기반 RAG를 이러한 벤치마크에 적용한다. 우리는 제로샷 결과와 오디오캡스에서 오디오 캡션의 SOTA 기준선과 비교한다. 분류 결과는 표 3과 같고, 검색-증강 오디오 캡셔닝에 대한 비교는 표 4와 같다. 예상대로 제로-샷 결과에 비해 일관된 개선이 있으며 분류에 대해 평균 10% 이상 개선되었다. 제안한 방법은 오디오캡스에서 SOTA 검색-증강 오디오 자막화 방법보다 우수한 성능을 보인다. 부록 C.1에서 오디오 플라밍고가 보이지 않는 레이블에 적응할 수 있음을 보여준다. 부록 C.3에서는 Audio Flamingo _looks at related retrieval_ (예를 들어, retrieved captions에서 주요 단어를 취하여), _ignores noisy retrieval_를 보인다.\n' +
      '\n' +
      '###Q3 : 멀티턴 다이얼로그\n' +
      '\n' +
      '멀티턴 대화 설정으로 질문에 답하는 오디오 플라밍고의 능력을 측정합니다. 맥락은 라운드들 사이에 더 복잡하고 강한 상관 관계가 있다(예를 들어, 많은 대명사와 후속 질문들이 존재한다). 채팅 모델을 얻기 위해 생성된 두 집합(AF-Dialogue-AudioSetSL과 AF-Dialogue-MusicCaps)에서 오디오 플라밍고를 미세 조정한다. 우리는 이 두 대화 데이터 세트의 테스트 분할에 대한 채팅 모델을 평가한다. 우리는 사용자 지시를 받고\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & Task & Metric & Previous SOTA \\(\\uparrow\\) & Ours \\(\\uparrow\\) \\\\ \\hline Clotho-v2 & CAP & CIDEr & 0.441 (Chu et al., 2023) & **0.465** \\\\ Clotho-v4 & AQA & ACC & 74.9\\% (Chu et al., 2023) & **86.9\\%** \\\\ Clotho-v4 & AQA & ACC & 29.1\\% (Deshmukh et al., 2023) & **49.5\\%** \\\\ Clotho-v4 & AQA & ACC & 26.2\\% (Deshmukh et al., 2023) & **36.4\\%** \\\\ MusicAVQAudio-only & AQA & ACC & **72.1\\%**(Chu et al., 2023) & 71.6\\% \\\\ CochlScene & CLS & ACC & **91.6**\\%(Deshmukh et al., 2023) & 83.0\\% \\\\ NonSpeech17k & CLS & ACC & 79.0\\% (Rashid et al., 2023) & **85.1\\%** \\\\ FSD50k & CLS & F1\\({}_{\\text{approx}}\\) & 65.6\\% (Deshmukh et al., 2023) & **69.7\\%** \\\\ NS\\({}_{\\text{instrument}}\\) & CLS & ACC & **78.8**\\%(Chu et al., 2023) & 77.1\\% \\\\ NS\\({}_{\\text{quality}}\\) & CLS & F1 & 46.3\\% (Deshmukh et al., 2023) & **66.7\\%** \\\\ NS\\({}_{\\text{source}}\\) & CLS & ACC & 60.1\\% (Deshmukh et al., 2023) & **78.7\\%** \\\\ - Average (besides CAP) & & 62.4\\% & **70.5\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 배포 중인 벤치마크에 대한 오디오 플라밍고 대 SOTA 기준선 방법의 평가. 참조 값은 각 작업에 대한 SOTA 모델입니다. 오디오 플라밍고는 자막, 질의응답, 오디오 분류에 대한 강한 오디오 이해 능력을 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Dataset & Task & Metric & Previous SOTA (0-shot) \\(\\uparrow\\) & Ours (0-shot) \\(\\uparrow\\) \\\\ \\hline AudioCaps (Kim et al., 2019) & CAP & CIDEr & 0.281 (Salewski et al., 2023) & **0.502** \\\\ CREMA-D (Cao et al., 2014) & CLS & ACC & 18.5\\% (Deshmukh et al., 2023) & **26.5\\%** \\\\ Ravdess (Livingstone \\& Russo, 2018) & CLS & ACC & **21.7\\%**(Elizalde et al., 2023b) & 20.9\\% \\\\ US8K (Salamon et al., 2014) & CLS & ACC & 71.9\\% (Deshmukh et al., 2023) & **75.0\\%** \\\\ GTZAN (Sturm, 2013) & CLS & ACC & **71.0\\%**(Han et al., 2023) & 67.9\\% \\\\ Medley-solos-DB (Lostanlen et al., 2019) & CLS & ACC & 61.3\\% (Deshmukh et al., 2023) & **92.7\\%** \\\\ - Average (besides CAP) & & 48.9\\% & **56.6\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 제로샷 벤치마크에 대한 오디오 플라밍고 대 SOTA 기준선 방법의 평가. 참조 값은 각 작업에 대한 SOTA 모델입니다. 오디오 플라밍고는 강력한 제로샷 일반화 능력을 보인다.\n' +
      '\n' +
      '모델은 답변들을 턴바이턴으로 생성한다(여기서 이전에 생성된 답변들은 다음 세대의 채팅 히스토리가 된다). 우리는 표 5에서 Qwen-Audio (Chu et al., 2023) 및 LTU (Gong et al., 2023c)와 비교한다. 3 우리의 채팅 모델은 베이스라인 방법들보다 상당히 더 나은 결과들을 달성한다. 부록 C.4에서, 우리는 우리의 채팅 모델이 사전 정보 및 대명사와 같은 컨텍스트_를 더 잘 포착한다는 것을 보여주기 위해 정성적 샘플을 사용한다.\n' +
      '\n' +
      '각주 3: 기준선 방법이 다중 회전 대화를 지원한다고 주장했지만 정량적 증거를 찾을 수 없었다.\n' +
      '\n' +
      '### Q4 : 절제 연구\n' +
      '\n' +
      '**수-샷 샘플 수의 영향.** 문맥 내 수-샷 샘플의 수를 4로 줄이고 수-샷 벤치마크에 대해 평가한다. 그림 5에서 우리는 0-shot 결과에 대한 상대적인 개선을 그림으로 표시했다. 우리는 더 많은 ICL 샘플을 추가하면 소수의 샷 결과가 일관되게 개선되며 이점은 데이터 세트에 따라 다르다.\n' +
      '\n' +
      '**LM의 효과** 명령어 조정된 LLM을 사용하여 오디오 이해 성공의 열쇠임을 발견했다. 초기 실험에서 우리는 opt-1.3b/2.7b (Zhang et al., 2022)와 phi-1.5 (Li et al., 2023b)를 조사했으며 오디오 이해 결과는 약간이나 일관되게 더 나쁘다는 것을 발견했다.\n' +
      '\n' +
      '**오디오 인코더의 효과.** 초기 실험에서, 우리는 오디오 인코더 백본으로서 LAION-CLAP(Wu et al., 2023) 및 마이크로소프트-CLAP(Elizalde et al., 2023a;b)의 모든 변형을 테스트하였다. ClapCap(Elizalde et al., 2023b)은 평가 결과와 컴퓨팅 속도 면에서 전반적으로 가장 우수한 성능을 보이는 반면, 더 많은 음악 데이터에 대해 훈련된 대규모 LAION-CLAP는 음악 정보 검색에서 더 나은 정확도를 보였다.\n' +
      '\n' +
      '##6 결론 및 향후 과제\n' +
      '\n' +
      '본 논문에서는 오디오 언어 모델인 오디오 플라밍고(Audio Flamingo)를 제안한다. 오디오 언어 모델은 태스크 특정 미세 조정 없이 여러 개의 클로즈엔드 및 오픈엔드 오디오 이해 태스크에 대해 최첨단 결과를 달성하는 일련의 혁신을 가지고 있다. 또한 강력한 ICL 및 RAG 능력을 가지고 있으며 최첨단 소수의 학습 결과를 가지고 있습니다. 또한, 데이터셋 생성 전략을 설계하고 두 개의 대화 데이터셋을 소개하고, 오디오 플라밍고가 사용자와 채팅하고 대화 벤치마크에 대한 최신 결과를 얻을 수 있도록 한다.\n' +
      '\n' +
      '한 가지 중요한 미래 방향은 더 큰 LM을 사용하기 위한 스케일링 전략을 조사하는 것이다. 더 큰 LM이 더 나은 지식과 지침을 따르는 더 강한 능력을 가질 수 있다고 가정하면 오디오 플라밍고가 더 큰 LM으로부터 이익을 얻을 수 있다고 믿는다. 두 번째 미래 방향은 전사를 넘어 복잡한 발화 관련 과제를 조사하는 것이다. 세 번째 미래 방향은 우리 모델의 오디오 이해 능력을 시각적 언어 모델과 결합하여 하나의 모델이 비디오와 함께 오디오를 이해할 수 있도록 하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Method & RECAP & Ours & Ours \\\\ \\hline \\# Shots & 4 & 4 & 8 \\\\ \\hline CIDEr \\(\\uparrow\\) & 0.362 & 0.518 & **0.538** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: AudioCaps 상의 검색-증강 오디오 캡셔닝의 평가. 우리는 Audio Flamingo를 SOTA baseline RECAP(Ghosh et al., 2023)와 비교한다. 오디오 플라밍고는 RECAP보다 훨씬 더 나은 결과를 달성한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Dataset & Ours (8-shot) \\(\\uparrow\\) & \\(\\Delta\\uparrow\\) \\\\ \\hline CREMA-D & 31.8\\% & 5.3\\% \\\\ Ravdess & 35.2\\% & 14.3\\% \\\\ US8K & 94.7\\% & 19.4\\% \\\\ GTZAN & 79.5\\% & 11.6\\% \\\\ Medley-solos-DB & 95.7\\% & 3.0\\% \\\\ Average & 67.4\\% & 10.7\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: ICL 기반 RAG를 이용한 오디오 플라밍고의 수샷 결과 평가. \\ (\\Delta\\)는 표 2의 제로-샷 결과보다 소수-샷의 _absolute_ 개선이다. ICL 기반 RAG는 제로-샷 결과보다 일관된 개선을 이끈다.\n' +
      '\n' +
      '도 5: 상이한 수의 ICL 샘플 하에서 제로-샷 결과들에 대한 소수의-샷 결과들의 _Relative_ 개선. 더 많은 ICL 샘플을 사용하면 소수의 샷 결과가 일관되게 개선되며 이점은 데이터 세트에 따라 다르다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Testset & Method & CIDEr \\(\\uparrow\\) & Bleu4 \\(\\uparrow\\) & R-L \\(\\uparrow\\) \\\\ \\hline A & LTU & 0.580 & 0.122 & 0.324 \\\\ A & Qwen-Audio & 0.507 & 0.060 & 0.292 \\\\ A & Ours & **1.622** & **0.237** & **0.473** \\\\ M & LTU & 0.168 & 0.065 & 0.217 \\\\ M & Ours & **1.143** & **0.142** & **0.417** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 멀티턴 대화 테스트 세트에 대한 오디오 플라밍고 대 베이스라인 방법의 평가. A는 AF-Dialogue-AudioSetSL을, M은 AF-Dialogue-MusicCaps를 의미한다. 우리는 CIDEr, Bleu4 (Papineni et al., 2002), 및 Rouge-L (R-L)(Lin, 2004)을 보고한다. 오디오 플라밍고는 더 큰 기준선 모델을 훨씬 능가하여 제안된 모델의 강력한 대화 능력을 나타낸다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '우리는 도움이 되는 토론에 대해 싯다르트 구루라니, 지한 류, 모스토파 파트와리, 슈라이 프라부모예, 첸 주에게 감사드린다. 데이터 세트 공유에 도움을 주신 Ke Chen과 Yuan Gong에게 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, S., Altman, S., Anadkat, S., et al. Gpt-4 기술 보고서. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Adigwe et al. (2018) Adigwe, A., Tits, N., Haddad, K. E., Ostadabbas, S., and Dutoit, T. 감정 음성 데이터베이스는: 음성 생성 시스템에서 감정 차원을 제어하기 위한 것이다. _ arXiv preprint arXiv:1806.09514_, 2018.\n' +
      '* Agostinelli et al. (2023) Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., et al. Musi-clm: 텍스트로부터 음악을 생성하는 단계; _ arXiv preprint arXiv:2301.11325_, 2023.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. _ 신경 정보 처리 시스템_, 35:23716-23736, 2022에서의 발전.\n' +
      '* Barros et al. (2018) Barros, P., Churamani, N., Lakomkin, E., Siqueira, H., Sutherland, A., and Wermter, S. omg-감성 동작 데이터 세트입니다. In _2018 International Joint Conference on Neural Networks (IJCNN)_, pp. 1-7. IEEE, 2018.\n' +
      '* Bogdanov et al. (2019) Bogdanov, D., Won, M., Tovstogan, P., Porter, A., and Serra, X. 자동 음악 태깅을 위한 mtg-자멘도 데이터세트. ICML, 2019년\n' +
      '* Borgeaud et al. (2022) Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. 수조의 토큰으로부터 인출함으로써 언어 모델을 개선한다. In _International conference on machine learning_, pp. 2206-2240. PMLR, 2022.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Cao et al. (2014) Cao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C., Nenkova, A., and Verma, R. Crema-d: 크라우드소싱된 감성 멀티모달 행위자 데이터셋, _ IEEE transactions on affective computing_, 5(4):377-390, 2014.\n' +
      '* Cartwright et al. (2019) Cartwright, M., Mendez, A. E. M., Cramer, A., Lostanlen, V., Dove, G., Wu, H.-H., Salamon, J., Nov, O., and Bello, J. Sony urban sound tagging (sonyc-ust): 도시 음향 센서 네트워크로부터의 다중 라벨 데이터세트. 2019년\n' +
      '* Chen et al. (2022) Chen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick, T., and Dubnov, S. Hts-at: 사운드 분류 및 검출을 위한 계층적 토큰-의미 오디오 변환기. In _ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 646-650. IEEE, 2022.\n' +
      '* Chen et al. (2023) Chen, Z., Huang, H., Andrusenko, A., Hrinchuk, O., Puvvada, K. C., Li, J., Ghosh, S., Balam, J., and Ginsburg, B. Salm: Speech-augmented language model with in-context learning for speech recognition and translation. _ arXiv preprint arXiv:2310.09424_, 2023.\n' +
      '* Chu et al. (2023) Chu, Y., Xu, J., Zhou, X., Yang, Q., Zhang, S., Yan, Z., Zhou, C., and Zhou, J. Qwen-audio: Advancing universal audio understanding via the unified large-scale audio-language models. _ arXiv preprint arXiv:2311.07919_, 2023.\n' +
      '* Defferrard et al. (2016) Defferrard, M., Benzi, K., Vandergheynst, P., and Bresson, X. Fma: 음악 분석을 위한 데이터셋. _ ArXiv preprint arXiv:1612.01840_, 2016.\n' +
      '* Defossez et al. (2022) Defossez, A., Copet, J., Synnaeve, G., and Adi, Y. 고충실도 신경 오디오 압축 ARXiv 프리프린트 arXiv:2210.13438_, 2022.\n' +
      '* Deshmukh et al. (2022) Deshmukh, S., Elizalde, B., and Wang, H. Audio retrieval with wavtext5k and clap training. _ ArXiv:2209.14275_, 2022.\n' +
      '* Deshmukh et al. (2023) Deshmukh, S., Elizalde, B., Singh, R., and Wang, H. Pengi: An audio language model for audio tasks. _ arXiv preprint arXiv:2305.11834_, 2023.\n' +
      '* Doh et al. (2023) Doh, S., Choi, K., Lee, J., and Nam, J. Lp-musiccaps: Llm-based pseudo music captioning. _ arXiv preprint arXiv:2307.16372_, 2023.\n' +
      '* Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdh-ery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: embodied multimodal language model. _ arXiv preprint arXiv:2303.03378_, 2023.\n' +
      '* Drossos et al. (2020) Drossos, K., Lipping, S., and Virtanen, T. 클로토: 오디오 캡션 데이터세트. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP)_, pp. 736-740. Cited by: SS2.\n' +
      '* H. Duan, J. Wei, C. Wang, H. Liu, Y. 팽승 장동린, K. Chen(2023)Botchat: 멀티턴 다이얼로그를 갖는 llms의 능력을 평가하는 것. ArXiv:2310.13650. 인용: SS2.\n' +
      '* B. Elizalde, S. 데쉬무크 Ismail, and H. Wang (2023) Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS2.\n' +
      '* B. Elizalde, S. Deshmukh, and H. Wang (2023) natural language supervision for general-purpose audio representation. 참고: URL[https://arxiv.org/abs/2309.05767](https://arxiv.org/abs/2309.05767) Cited by: SS2.\n' +
      '* J. Engel, C. Resnick, A. Roberts, S. Dieleman D. Eck K. 시모니안, M. Norouzi (2017)Neural audio synthesis of musical notes with wavenet autoencoder. 참고: 인용: SS2.\n' +
      '* E. Fonseca, X. Favory, J. Pons, F. Font, X. Serra(2021)Fsd50k: 인간-표지된 사운드 이벤트들의 오픈 데이터세트. IEEE/ACM Transactions on Audio, Speech, and Language Processing30, pp. 829-852. Cited by: SS2.\n' +
      '* P. 포스터, S. Sigtia Krstulovic, J. Barker, and M. D. Plumbley (2015)Chime-home: 국내 환경에서 음원 인식을 위한 데이터셋. 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pp. 1-5. Cited by: SS2.\n' +
      '* H. Gao, J. Ni, K. 건영 장승 장모 Hasegawa-Johnson (2022)Wavprompt: 냉동 언어 모델을 사용한 소수의 음성 언어 이해. ArXiv:2203.15863. 인용: SS2.\n' +
      '* J. Gardner, S. Durand, D. Stoller, and R. M. Bittner (2023)Lark: 음악용 멀티모달 파운데이션 모델. ArXiv:2310.07160. 인용: SS2.\n' +
      '* J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. 로렌스 R. C. 무어, M. Plakal, M. 리터(2017)오디오 세트: 오디오 이벤트에 대한 온톨로지 및 인간 라벨링된 데이터세트. 2017년 IEEE 국제학술대회 on Acoustics, Speech and Signal Processing (ICASSP), pp. 776-780. Cited by: SS2.\n' +
      '* S. 고시 쿠마르, C. K. R. 에부루, R. Duraiswami, and D. Manocha(2023)Recap: retrieval-augmented audio captioning. ArXiv:2309.09836. 인용: SS2.\n' +
      '*Y. 공영 쿠라나 Karlinsky, and J. Glass (2023)Whisper-at: noise-robust automatic speech recognitionizers is also strong general audio event tagger. ArXiv:2307.03183. 인용: SS2.\n' +
      '*Y. 공아류 Karlinsky, and J. Glass (2023) Joint audio and speech understanding. IEEE 자동음성인식 및 이해 워크숍에서, pp. 인용: SS2.\n' +
      '*Y. 공홍루오 Karlinsky와 J. Glass (2023)는 듣고, 생각하고, 이해한다. ArXiv:2305.10790. 인용: SS2.\n' +
      '*K. 구경 이지환 Tung, P. Pasupat, M. Chang(2020) Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929-3938. Cited by: SS2.\n' +
      '*J. Han, R 장원 샤오, P. 가오, P. 쉬, H. 샤오, K. 장창류 원지호 Guo, et al.(2023)ImageBind-llm: multi-modality instruction tuning. ArXiv:2309.03905. 인용: SS2.\n' +
      '* S. 허쉬, D. P. 엘리스, E. 폰세카, A. 잰슨, C. 류, R. C. 무어, M. Plakal(2021) 오디오 이벤트 분류에서 시간적으로 강한 라벨의 이점. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 366-370. Cited by: SS2.\n' +
      '* M. 허경 장승 Li, and H. Lee (2023)An exploration of in-context learning for speech language model. ArXiv:2310.12477. 인용: SS2.\n' +
      '*Q. 황아잔센 Ganti, J. Y. Li, and D. P. Allan(2022)Mulan: 공동 임베딩된 음악 오디오 및 자연 언어. ArXiv:2208.12415. 인용: SS2.\n' +
      '*R. 황민 이덕양 장진 예영 우진 Hong, J. Huang, J. Liu, et al. (2023)Audiogpt: 이해 및 생성 스피치, 음악, 사운드, 및 말하는 머리. ArXiv:2304.12995. 인용: SS2.\n' +
      '* S. Iyer, X. V. Lin, R. 파수누루 미하일로프, D. 시미그, P. P. 유, K. 슈스터, T 왕규 Liu, P. S. Koura, et al.(2022)Opt-iml: scaling language model instruction meta learning through the lens of generalization. ArXiv:2212.12017. 인용: SS2.\n' +
      '\n' +
      '* James et al. (2018) James, J., Tian, L., and Watson, C. An open source emotional speech corpus for human robot interaction applications. _ Interspeech 2018_, 2018.\n' +
      '* Jeong & Park (2022) Jeong I.-Y. and Park, J. Cochlscene: Acquisition of acoustic scene data using crowdsourcing. In _2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)_, pp. 17-21. IEEE, 2022.\n' +
      '* Johnson et al. (2019) Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with GPUs. _ IEEE Transactions on Big Data_, 7(3):535-547, 2019.\n' +
      '* Karpukhin et al. (2020) Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. -t. 개방형 도메인 질문 응답을 위한 밀도 통로 검색 arXiv preprint arXiv:2004.04906_, 2020.\n' +
      '* Khomenko et al. (2016) Khomenko, V., Shyshkov, O., Radyvonenko, O., and Bokhan, K. 시퀀스 버킷 및 다중 gpu 데이터 병렬화를 사용하여 순환 신경망 학습을 가속화한다. In _2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP)_, pp. 100-103. IEEE, 2016.\n' +
      '* Kim et al. (2019) Kim, C. D., Kim, B., Lee, H., and Kim, G. Audio-caps: Generating caption for audioios in the wild. [Proceedings of the 2019 Conference of the North American chapter of the Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 119-132, 2019).\n' +
      '* Kong et al. (2020) Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. Panns: Large-scale prerained audio neural networks for audio pattern recognition. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 28:2880-2894, 2020.\n' +
      '* Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W. -t., Rocktaschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ 2020년, 신경망 정보 처리 시스템_, 33:9459-9474의 발전.\n' +
      '* Li et al. (2022) Li, G., Wei, Y., Tian, Y., Xu, C., Wen, J.-R., and Hu, D. Learning to answer questions in dynamic audio-visual scenarios. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 19108-19118, 2022.\n' +
      '* Li 등(2023a) Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: 냉동 이미지 인코더 및 대형 언어 모델을 사용한 부트스트래핑 언어-이미지 사전 트레이닝_ arXiv preprint arXiv:2301.12597_, 2023a.\n' +
      '* Li et al. (2023b) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: phi-1.5 technical report. _ arXiv preprint arXiv:2309.05463_, 2023b.\n' +
      '* Li et al. (2023c) Li, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H., Lin, C., Ragni, A., Benetos, E., Gyenge, N., et al. Mert: 대규모 자기 지도 훈련을 갖는 음향 음악 이해 모델 _ arXiv preprint arXiv:2306.00107_, 2023c.\n' +
      '* Lin(2004) Lin, C.-Y. 루즈: 요약 자동 평가를 위한 패키지입니다. _Text 요약에서 branch out_, pp. 74-81, 2004.\n' +
      '* Lipping et al. (2022) Lipping, S., Sudarsanam, P., Drossos, K., and Virtanen, T. Clotho-aqa: 오디오 질문 응답을 위한 크라우드소싱 데이터세트. _2022 30th European Signal Processing Conference (EUSIPCO)_, pp. 1140-1144. IEEE, 2022.\n' +
      '* Liu et al. (2023a) Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. _ arXiv preprint arXiv:2304.08485_, 2023a.\n' +
      '*Liu et al. (2023b) Liu, S., Hussain, A. S., Sun, C., and Shan, Y. 음악 이해 라마: 질문 응답 및 캡셔닝과 함께 텍스트-음악 생성을 발전시킨다. _ arXiv preprint arXiv:2308.11276_, 2023b.\n' +
      '* Livingstone & Russo (2018) Livingstone, S. R. and Russo, F. A. The ryerson audio-visual database of emotional speech and song (ravdess): Dynamic, multimodal set of facial and vocal expressions in north American English. _ PloS one_, 13(5):e0196391, 2018.\n' +
      '* Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. Decoupled Weight decay regularization. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Lostanlen et al. (2019) Lostanlen, V., Cella, C.-E., Bittner, R., and Essid, S. Medley-solos-DB: a cross-collection dataset for musical instrument recognition, February 2019. URL[https://doi.org/10.5281/zenodo.1344103](https://doi.org/10.5281/zenodo.1344103).\n' +
      '* Lotfian & Busso (2017) Lotfian, R. and Busso, C. Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recording. _ IEEE Transactions on Affective Computing_, 10(4):471-483, 2017.\n' +
      '*Lyu 등 (2023) Lyu, C., Wu, M., Wang, L., Huang, X., Liu, B., Du, Z., Shi, S., and Tu, Z. Macaw-llm: 이미지, 오디오, 비디오 및 텍스트 통합을 갖는 멀티모달 언어 모델링. _ arXiv preprint arXiv:2306.09093_, 2023.\n' +
      '* Loshchilov & Hutter (2017)* Martin Morato and Mesaros (2021) Martin Morato, I. and Mesaros, A. Diversity and bias in audio captioning datasets. 2021년\n' +
      '*Mei et al. (2023) Mei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C., Plumbley, M. D., Zou, Y., and Wang, W. Wavcaps: 오디오 언어 멀티모달 연구를 위한 채팅 보조 약 레이블 오디오 캡션 데이터 세트__ arXiv preprint arXiv:2303.17395_, 2023.\n' +
      '* Mohanty(2022) Mohanty, S. P. Sound of 114 Species of Birds Till 2022. URL[https://www.kaggle.com/datasets/soumendraprasad/sound-of-114-species-of-birds-till-2022](https://www.kaggle.com/datasets/soumendraprasad/sound-of-114-species-till-2022)\n' +
      '*Moon et al. (2023) Moon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith, M., Jain, S., Yeh, C.-F., Murugesan, P., Heidari, P., Liu, Y., et al. Anymal: An efficient and scalable any-modality augmented language model. _ arXiv preprint arXiv:2309.16058_, 2023.\n' +
      '* Oancescu et al. (2021) Oancescu, A.-M., Koepke, A., Henriques, J. F., Akata, Z., and Albanie, S. 자연어 질의를 이용한 오디오 검색 arXiv preprint arXiv:2105.02192_, 2021.\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. training language models to follow instructions with human feedback. _ 신경 정보 처리 시스템_, 35:27730-27744, 2022에서의 발전.\n' +
      '* Papineni et al. (2002) Papineni, K., Roukos, S., Ward, T., and Zhu, W. - J Bleu: 기계 번역의 자동 평가 방법. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pp. 311-318, 2002.\n' +
      '* Park et al. (2022) Park, J., Cho, Y., Sim, G., Lee, H., and Choo, J. Enemy spotted: In-game gun sound dataset for gunshot classification and localization. In _2022 IEEE Conference on Games (CoG)_, pp. 56-63. IEEE, 2022.\n' +
      '* Pichora-Fuller & Dupuis (2020) Pichora-Fuller, M. K. and Dupuis, K. 토론토 감성 스피치 세트(TESS), 2020. URL[https://doi.org/10.5683/SP2/E8H2MF](https://doi.org/10.5683/SP2/E8H2MF)\n' +
      '*Poria et al. (2018) Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., and Mihalcea, R. Meld: 대화에서 감정 인식을 위한 멀티모달 다자간 데이터셋. _ arXiv preprint arXiv:1810.02508_, 2018.\n' +
      '* Radford et al. (2023) Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via large-scale weak supervision. In _International Conference on Machine Learning_, pp. 28492-28518. PMLR, 2023.\n' +
      '* a uncompressed version of musdb18, August 2019. URL [https://doi.org/10.5281/zenodo.3338373](https://doi.org/10.5281/zenodo.3338373).\n' +
      '* Rashid et al.(2023) Rashid, M. M., Li, G., and Du, C. Nonspeech7k dataset: Classification and analysis human non-speech sound. _ IET Signal Processing_, 17(6):e12233, 2023.\n' +
      '* Reimers & Gurevych (2019) Reimers, N. 그리고 Gurevych, I. Sentence-bert: siamese bert-networks를 이용한 Sentence embedding. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 11 2019. URL[https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084).\n' +
      '* Reimers & Gurevych (2019) Reimers, N. 그리고 Gurevych, I. 지식 증류를 사용하여 다국어를 포함하는 단일 언어 문장 임베딩 만들기. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 11 2020. URL[https://arxiv.org/abs/2004.09813](https://arxiv.org/abs/2004.09813).\n' +
      '* Rubenstein et al. (2023) Rubenstein, P. K., Asawaorengchai, C., Nguyen, D. D., Bapna, A., Boros, Z., Quitry, F. d. C., Chen, P., Badawy, D. E., Han, W., Kharitonov, E., et al. Audiopalm: 말하고 들을 수 있는 큰 언어 모델. _ arXiv preprint arXiv:2306.12925_, 2023.\n' +
      '* Salamon et al. (2014) Salamon, J., Jacoby, C., and Bello, J. P. A dataset and taxonomy for urban sound research. In _Proceedings of the 22nd ACM international conference on Multimedia_, pp. 1041-1044, 2014.\n' +
      '* Salewski et al. (2023) Salewski, L., Fauth, S., Koepke, A., and Akata, Z. 오디오 언어 모델 안내 및 오디오 컨텍스트 키워드가 포함된 제로샷 오디오 캡션. _ arXiv preprint arXiv:2311.08396_, 2023.\n' +
      '* Sturm(2013) Sturm, B. L. gtzan dataset: 그 내용, 그 결함, 평가에 미치는 영향 및 향후 이용. _ ArXiv:1306.1461_, 2013.\n' +
      '* Tang et al. (2023a) Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. Salmonn: Towards generic hearing ability for large language models. _ arXiv preprint arXiv:2310.13289_, 2023a.\n' +
      '* Tang 등(2023b) Tang, Z., Yang, Z., Khademi, M., Liu, Y., Zhu, C., and Bansal, M. Codi-2: In-context, interleaved, interactive any-to-any generation. _ arXiv preprint arXiv:2311.18775_, 2023b.\n' +
      '* Tsimpoukelli et al. (2021) Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. _ 신경 정보 처리 시스템_, 34:200-212, 2021에서의 발전.\n' +
      '* Tsimpoukelli et al. (2021)*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Vedantam et al. (2015) Vedantam, R., Lawrence Zitnick, C., and Parikh, D. Cider: Consensus-based image description evaluation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 4566-4575, 2015.\n' +
      '* Wang et al. (2023) Wang, S., Yang, C.-H. H., Wu, J., and Zhang, C. Can whisper perform speech-based in-context learning. _ arXiv preprint arXiv:2309.07081_, 2023.\n' +
      '* Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Fine-tuned language models is zero-shot learners. _ arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* Won et al. (2023) Won, M., Hung, Y. - N., and Le, D. A foundation model for music informationatics. _ arXiv preprint arXiv:2311.03318_, 2023.\n' +
      '* Wu et al. (2023) Wu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T., and Dubnov, S. 특징 융합과 키워드-캡션 증강을 이용한 대규모 대조 언어-오디오 사전 훈련. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 1-5. IEEE, 2023.\n' +
      '* Yang et al. (2023) Yang, Z., Ping, W., Liu, Z., Korthikanti, V., Nie, W., Huang, D.-A., Fan, L., Yu, Z., Lan, S., Li, B., et al. Re-vilm: Retrieval-augmented visual language model for zero and few-shot image captioning. _EMNLP_, 2023.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zhao et al. (2023) Zhao, Z., Guo, L., Yue, T., Chen, S., Shao, S., Zhu, X., Yuan, Z., and Liu, J. Chatbridge: Bridging modalities with large language model as language catalyst. _ arXiv preprint arXiv:2305.16103_, 2023.\n' +
      '\n' +
      '## 부록 A 데이터 세트 스테이징, 가중치 및 템플릿\n' +
      '\n' +
      '표 6은 오디오 플라밍고를 훈련하기 위해 사용하는 데이터 세트의 개요(유형별)를 포함한다.\n' +
      '\n' +
      '각 작업 및 데이터 세트에 대한 지침을 구성합니다. 아래는 우리가 사용하는 모든 지침 템플릿입니다.\n' +
      '\n' +
      '_Audio Captioning:_\n' +
      '\n' +
      '한 문장으로 소리/음악을 묘사하세요.\n' +
      '\n' +
      '사운드/음악에 대해 자세히 설명하십시오.\n' +
      '\n' +
      '__오디오 질문 답변:___오디오 질문 답변:__\n' +
      '\n' +
      '{question}\n' +
      '\n' +
      '이 질문에 답해 주세요: {question}\n' +
      '\n' +
      '이 질문에 답해 주세요. {질문} 선택사항:\\n- yes\\n- no\n' +
      '\n' +
      '_Audio Classification:_\n' +
      '\n' +
      '이 소리를 분류합니다. (선택사항:...)\n' +
      '\n' +
      '소리를 {숫자} 단어로 설명합니다.\n' +
      '\n' +
      '이 연설의 감정은 무엇입니까? (선택사항:...)\n' +
      '\n' +
      '이 음악의 악기/장르는 무엇인가? (선택사항:...)\n' +
      '\n' +
      '이 음반은…\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline Audio Type & Task & \\multicolumn{2}{c}{Datasets} & \\#Audio-Text Pairs \\\\ \\hline \\multirow{4}{*}{General Sound} & \\multirow{2}{*}{CAP} & WavCaps (Mei et al., 2023), Macs (Martin Morato \\& Mesaros, 2021), & \\\\  & & SoundDeses (Oncescu et al., 2021), Clotho-v2 (Drossos et al., 2020), & \\\\  & & WavText5K (Deshmukh et al., 2022), LAION-630k (Wu et al., 2023) & \\\\ \\cline{2-4}  & \\multirow{2}{*}{AQA} & Clotho-AQA (Lipping et al., 2022), Open-AQA (Gong et al., 2023b) & \\(\\sim\\)1970 K \\\\ \\cline{3-4}  & & AudioSet (Gemmeke et al., 2017), FSD50k (Fonseca et al., 2021), & \\\\ \\cline{3-4}  & \\multirow{2}{*}{CLS} & CochlScene (Jeong \\& Park, 2022), NonSpeech7K (Rashid et al., 2023), & \\\\  & & Chime-Home (Foster et al., 2015), Sony-UST (Cartwright et al., 2019) & \\\\ \\hline \\multirow{4}{*}{Music} & \\multirow{2}{*}{CAP} & LP-MusicCaps (Doh et al., 2023), MusicCaps (Agostinelli et al., 2023) & \\(\\sim\\)1389 K \\\\ \\cline{2-4}  & & AQA & MusicQA (Liu et al., 2023b), MusicAVQA (Li et al., 2022) & \\(\\sim\\)94 K \\\\ \\cline{2-4}  & \\multirow{2}{*}{CLS} & NSynth (Engel et al., 2017), MTG-Jamedo (Bogdanov et al., 2019), & \\\\  & & FMA (Defferrard et al., 2016), MusDB-HQ (Rafii et al., 2019), & \\\\ \\hline \\multirow{4}{*}{Speech} & \\multirow{2}{*}{CLS} & MSP-Podcast (Lotfian \\& Busso, 2017), Emov-DB (Adigwe et al., 2018) & \\\\  & & JL-Corpus (James et al., 2018), Tess (Pichora-Fuller \\& Dupuis, 2020), & \\\\ \\cline{1-1}  & & MELD (Poria et al., 2018), OMGEmotion (Barros et al., 2018) & \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 우리 모델을 훈련시키는 데 사용되는 모든 데이터 세트. 오디오 텍스트 쌍의 총 수는 약 590만 개이다. 오디오의 총 길이는 약 18.1,000시간입니다.\n' +
      '\n' +
      '세부 사전 훈련 데이터 세트와 에폭의 수는 표 7과 같다. 세부 SFT 데이터 세트와 에폭의 수는 표 8과 같다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c} \\hline Dataset & Audio Length & \\#Audio-Text Pairs & Epochs \\\\ \\hline OpenAQA & 693.2 hrs & 1959.8K & 1.0 \\\\ Laion630k\\({}_{\\text{BBCsoundEffects}}\\) & 456.9 hrs & 15.1K & 5.0 \\\\ Laion630k\\({}_{\\text{Freesound}}\\) & 2494.8 hrs & 306.5K & 1.0 \\\\ SoundDescs & 749.7 hrs & 23.1K & 1.0 \\\\ WavCaps & 3793.3 hrs & 402.6 K & 1.75 \\\\ AudioSet & 2617.8 hrs & 950.8K & 1.0 \\\\ WavText5K & 23.8 hrs & 4.3K & 3.0 \\\\ MSP-Podcast & 73.9 hrs & 45.1K & 1.2 \\\\ MELD & 8.7 hrs & 32.9K & 2.4 \\\\ MusicAVQA\\({}_{\\text{audio-visual}}\\) & 142.4 hrs & 17.9K & 3.0 \\\\ MusicQA & 62.9 hrs & 70K & 1.2 \\\\ LP-MusicCaps\\({}_{\\text{MSD}}\\) & 5805.7 hrs & 1331.8K & 1.0 \\\\ NSynth & 321.3 hrs & 289.2K & 0.4 \\\\ MTG-Jamendo & 3768.9 hrs & 55.6K & 1.0 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 사전 훈련 데이터 세트 및 에포크.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline Dataset & Audio Length & \\#Audio-Text Pairs & Epochs & ICL Dataset Epochs \\\\ \\hline ClothoAQA & 7.4 hrs & 9.7K & 3.5 & 0.5 \\\\ OpenAQA & 693.2 hrs & 1959.8K & 0.1 & - \\\\ Clotho-v2 & 24.0 hrs & 19.2K & 2.0 & 0.5 \\\\ Laion630k\\({}_{\\text{Epidemic}}\\) & 209.4 hrs & 40.7K & 0.8 & 0.2 \\\\ MACS & 10.9 hrs & 17.3K & 0.8 & 0.2 \\\\ FSD50k & 80.8 hrs & 41.0K & 0.9 & 0.3 \\\\ CochlScene & 169.0 hrs & 60.9K & 1.2 & 0.3 \\\\ NonSpeech 7k & 6.2 hrs & 6.3K & 2.4 & 0.6 \\\\ Chime-home & 5.0 hrs & 4.5K & 1.5 & 0.5 \\\\ Sony-UST & 34.9 hrs & 27.9K & 0.8 & 0.2 \\\\ Emov-DB & 7.8 hrs & 6.8K & 1.6 & 0.4 \\\\ JL-Corpus & 1.4 hrs & 2.4K & 6.0 & 1.5 \\\\ Tess & 1.6 hrs & 2.8K & 2.0 & 0.5 \\\\ OMGEmotion & 3.0 hrs & 1.7K & 3.0 & - \\\\ MusicAVQA\\({}_{\\text{audio-only}}\\) & 77.1 hrs & 5.7K & 5.0 & 1.0 \\\\ MusicQA & 62.9 hrs & 70K & 0.35 & 0.05 \\\\ LP-MusicCaps\\({}_{\\text{MSD}}\\) & 5805.7 hrs & 1331.8K & 0.025 & 0.007 \\\\ LP-MusicCaps\\({}_{\\text{MTT}}\\) & 126.4 hrs & 46.9K & 0.8 & 0.2 \\\\ LP-MusicCaps\\({}_{\\text{MC}}\\) & 7.4 hrs & 7.9K & 2.0 & - \\\\ MusicCaps & 7.4 hrs & 2.6K & 6.0 & - \\\\ NSWth & 321.3 hrs & 289.2K & 1.0 & 1.0 \\\\ MTG-Jamendo & 3768.9 hrs & 55.6K & 0.1 & - \\\\ MusDB-HQ & 29.1 hrs & 10.2K & 1.0 & - \\\\ FMA & 860.7 hrs & 104.2K & 0.4 & 0.1 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: SFT 데이터세트 및 에폭스.\n' +
      '\n' +
      '## 부록 B 생성 대화 데이터셋\n' +
      '\n' +
      '### Overview\n' +
      '\n' +
      '이 섹션에서는 GPT-4를 사용하여 AF-Dialogue-AudioSetSL 및 AF-Dialogue-MusicCaps 데이터 세트를 생성하는 방법을 소개한다(Achiam et al., 2023). AF-Dialogue-AudioSetSL은 강하게 라벨링된 AudioSet-SL의 주석된 이벤트들 및 타임스탬프들에 기초하여 생성된다(Gemmeke et al., 2017; Hershey et al., 2021). 열차 분할에는 76k 대화 상자가 있고 테스트 분할에는 1.5k 대화 상자가 있습니다. AF-Dialogue-MusicCaps는 MusicCaps(Agostinelli et al., 2023)의 태그 및 설명에 기초하여 생성된다. 열차 분할에는 3.3k 대화 상자가 있고 테스트 분할에는 1.4k 대화 상자가 있습니다. 각 훈련 샘플은 1~4개의 라운드(QA 쌍)를 가지며, 각 테스트 샘플은 3~4개의 라운드를 갖는다.\n' +
      '\n' +
      '### Prompts\n' +
      '\n' +
      '아래에서는 AF-Dialogue-AudioSetSL 및 AF-Dialogue-MusicCaps 생성에 사용된 프롬프트를 보여준다.\n' +
      '\n' +
      '```\n' +
      'sys 프롬프트: 사운드 이벤트들에 기초하여, 오디오 내의 이벤트들에 대해 당신(보조자)과 사람(사용자) 사이의 대화를 생성한다. 각각의 대화는 다음과 같이 구성되어야 한다.\n' +
      '\n' +
      '1. 사용자가 오디오를 검사하고, 어시스턴트에게 합리적이고 창의적인 메시지를 전송한다. 2. 일단 오디오가 제공되면, 어시스턴트는 그것들을 철저히 인식하고 이해하며, 포괄적인 추론을 제공하는 유용한 답변으로 응답한다. 어시스턴트에 의해 제공된 답변에 타임스탬프를 포함하지 않는다. 3. 과거 대화 즉, 질문 및 이전 타임스탬프에서의 답변을 고려하여, 사용자는 다른 질문 또는 추가 질의를 선택한다. 4. 어시스턴트는 오디오 콘텐츠에 관한 포괄적인 추론을 제공하는 유용한 답변을 제공한다.\n' +
      '\n' +
      '오디오의 길이는 10초이고 사운드 이벤트는 오디오에서 발생하는 위치를 기준으로 분할된다. 사용자가 질문하는 모든 질문은 다양하고 복잡해야 한다.\n' +
      '"실시예 1: 사운드 이벤트들: 앰뷸런스(사이렌)의 사운드: [0.0s-1.0s]; 교통 소음, 도로 소음(드론링, 시끄럽고 강렬함): [0.0s-10.0s]; 스피치 소리: [4.557s-4.935s]; 스피치 소리: [4.557s-4.935s]; 스피치 소리: [2.557s-4.680s]; 스피치 소리: [4.249s-5.292s]; 스피치 소리: [2.557s-4.680s]; 스피치 소리: [6.7s-6.8s]; 스피치 소리: 스피치 소리, 시끄럽고 강렬함: 사용자: 스피치 소리, 시끄럽고 강렬함: 사용자: 스피치 소리, 시끄럽고 강렬함: 사용자: 스피치 소리: 외부, 시끄럽고 강렬함: 외부, 시끄럽고 강렬함: 외부, 시끄럽고 강렬함: 외부, 시끄럽고 강렬함: 외부, 시끄럽고 강렬 각 QA 쌍을 JSON 사전(질의의 경우 키 "사용자", 답변의 경우 "보조", { 및 }로 싸임)으로 한 줄로 포맷합니다. 다른 설명은 포함하지 마세요. 예3:시스 프롬프트: 음악에 기초하여, 음악 오디오 내의 이벤트들에 관한 당신(보조자)과 사람(사용자) 사이의 대화를 생성한다. 각각의 대화는 다음과 같이 구성되어야 한다.\n' +
      '\n' +
      '1. 사용자가 음악을 조사하고, 합리적이고 창의적인 메시지를 어시스턴트에게 전송한다. 2. 일단 음악 오디오가 제공되면, 어시스턴트는 그것들을 철저히 인식하고 이해하며, 포괄적인 추론을 제공하는 유용한 답변으로 응답한다. 보조자가 제공한 답변에 타임스탬프를 포함하지 마십시오.\n' +
      '\n' +
      '3. 과거 대화 즉, 이전 타임스텝에서의 질문 및 답변을 고려하여, 사용자는 다른 질문 또는 추가 질의를 선택한다. 4. 어시스턴트는 음악 콘텐츠에 관한 포괄적인 추론을 제공하는 유용한 답변을 제공한다. 질문이 복잡하고 다양할수록 좋다. 아래 질문 목록을 사용하여 질문을 할 수 있습니다.\n' +
      '\n' +
      '**질문 목록 :** 라이브 녹음인가요, 스튜디오 녹음인가요?\n' +
      '\n' +
      '기타 어쿠스틱인가요 일렉트릭인가요?\n' +
      '\n' +
      '화음 연주야, 리프야, 솔로야?\n' +
      '\n' +
      '어떤 음악 스타일이나 장르인가요?\n' +
      '\n' +
      '무슨 열쇠에요?\n' +
      '\n' +
      '타임 서명은요?\n' +
      '\n' +
      '박자가 어떻게 되죠? 느린 거, 미디엄 아니면 빠른 거?\n' +
      '\n' +
      '보컬이 있나요?\n' +
      '\n' +
      '낮은 투구인가요, 높은 투구인가요?\n' +
      '\n' +
      '말하는 사람이 빠른가요, 느린가요?\n' +
      '\n' +
      '언어와 억양이 어떻게 되나요?\n' +
      '\n' +
      '목소리로 전달되는 감정은 무엇인가?\n' +
      '\n' +
      '그 감정의 음성적 특징은 무엇인가?\n' +
      '\n' +
      '그 사람은 몇 살 때 목소리의 특징을 받을 것 같습니까?\n' +
      '\n' +
      '**Example 1:**\n' +
      '\n' +
      '음악 묘사: 초기에 생각을 자극하는 메시지를 가진 하우스 음악은 결국 축적되고 사회 혁명의 감각을 창조하는 데 사용될 수 있다.\n' +
      '\n' +
      '**Dialogue:**\n' +
      '\n' +
      '사용자: 이것은 어떤 종류의 음악인가요?\n' +
      '\n' +
      '조력자: 그것은 신시스와 드럼이 있는 하우스 음악이고, 맨 처음에 말하는 남자입니다.\n' +
      '\n' +
      '사용자: 말하는 동안 그의 어조는 무엇입니까?\n' +
      '\n' +
      '예 2: 그는 목소리에 강렬하고 활기찬 톤을 가지고 있습니다.\n' +
      '\n' +
      '그는 끝까지 말하나요?\n' +
      '\n' +
      '도우미: 아니요, 그는 중간에서 말하는 것을 멈추고 집음악이 뒤따릅니다.\n' +
      '\n' +
      '사용자: 이 음악의 출처는 무엇입니까?\n' +
      '\n' +
      '보조: 이것은 전자 음악입니다.\n' +
      '\n' +
      '**Example 2:**\n' +
      '\n' +
      '음악적 묘사: 음악적 대화에서 하모니카와 호른 섹션이 있는 활기찬 블루시 노래.\n' +
      '\n' +
      '**Dialogue:**\n' +
      '\n' +
      '사용자: 이 음악에서 어떤 악기가 연주되고 있나요?\n' +
      '\n' +
      '보조: 이것은 하모니카, 트럼펫, 드럼이 연주되는 재즈 곡입니다.\n' +
      '\n' +
      '사용자: 어떤 키에서 음악이 재생되고 있나요?\n' +
      '\n' +
      '요원은 소령이다\n' +
      '\n' +
      '사용자: 이 작품의 템포를 어떻게 설명하시겠습니까?\n' +
      '\n' +
      '보조: 음악은 초기에 분당 높은 박자로 통통하고 긍정적이며, 그리고 나서 서명을 중간에 바꾼다.\n' +
      '\n' +
      '사용자: 이 음악의 출처는 무엇입니까?\n' +
      '\n' +
      '어시스턴트: 이것은 베이스와 신시사이저의 요소를 가지고 있는 퓨전 재즈입니다.\n' +
      '\n' +
      '위의 예에 기초하여, 아래에 주어진 예에 대해 사용자와 어시스턴트 사이의 4회전 대화를 설계한다. 사용자가 묻는 질문은 과거 질문이나 답변에서 이미 발생한 개체를 언급하면서 \'그\', \'그녀\', \'그것\', \'그들\' 및 \'그들\'과 같은 대명사를 사용해야 한다. 각 QA 쌍을 JSON 사전(질의의 경우 키 "사용자", 답변의 경우 "보조", { 및 }로 싸임)으로 한 줄로 포맷합니다. 다른 설명은 포함하지 마세요.\n' +
      '\n' +
      '**Example 3:**\n' +
      '\n' +
      '### Dialogue filtering\n' +
      '\n' +
      '부록 B.2에서 논의된 바와 같이 GPT-4에 의해 생성된 대화는 항상 프롬프트를 따르는 것은 아니며, "지정하지 않음", "결정할 수 없음", "추가 컨텍스트 없음" 등과 같은 구문을 갖는 답변을 초래한다. 따라서 Gardner et al.(2023)에 따라 데이터 품질을 개선하고 모델로부터 바람직한 출력을 보장하기 위해 이러한 QA 쌍을 필터링한다. 수동 필터링 단계와 별도로 GPT-4에서 생성된 답변과 오디오 샘플의 유사성을 기반으로 샘플을 필터링한다. 구체적으로, 각 대화에서 주어진 QA 쌍에 대해 LAION-CLAP 텍스트 임베딩과 오디오 임베딩 사이의 코사인 유사도를 계산한다(Wu et al., 2023). 유사도의 분포는 그림 6과 같으며 유사도가 0.3 이하이면 샘플을 제거한다.\n' +
      '\n' +
      '### 생성된 대화의 예\n' +
      '\n' +
      '[주석: 기계음의 소리(기계적 수단을 통해 소리를 생성함): [0.119s-10.000s]; 일반적인 충격음의 소리(짧은 소리, 날카로운 소리, 큰 소리 파열음): [0.217s-0.387s], [2.394s-2.727s], [2.889s-3.255s], [3.799s-4.132s], [4.473s-4.766s], [4.928s-5.188s], [8.129s-10.000s]; 유리의 산산조각이 나는 소리(높은 소리, 날카로운 소리, 갑작스러운 소리, 큰 소리 충돌음): [1.622s-4.652s]; 걷기의 소리, 발소리(부드럽고, 리듬적이며, 울리는 소리): [5.724s-6.106s], [6.455s-6.650s], [7.024s-7.479s], [7.625s-7.901s]]\n' +
      '\n' +
      '사용자: 오디오의 주요 이벤트를 설명할 수 있습니까?\n' +
      '\n' +
      '보조: 오디오는 주로 기계적 소리, 날카로운 충격 소리, 유리의 부서짐, 발자국 소리로 구성됩니다.\n' +
      '\n' +
      '사용자: 충격음에 대해 말씀해 주시겠습니까?\n' +
      '\n' +
      '보조: 충격음은 짧고 날카로우며 큰 파열입니다. 그것들은 오디오 전체에 걸쳐 여러 지점에서 발생하는데, 이는 일종의 반복적인 행동이나 사건을 암시한다.\n' +
      '\n' +
      '사용자: 유리가 깨지는 것은 어떻습니까? 한 번 이상 발생합니까?\n' +
      '\n' +
      '보조: 유리 깨지는 소리는 고음, 날카롭고, 갑작스럽고, 큰 충돌입니다. 오디오에서 한 번 발생하며 약 3초 동안 지속됩니다.\n' +
      '\n' +
      '[주석: 카우벨의 녹음이다. 카우벨은 드럼 스틱의 측면과 끝으로 반복적으로 재생되고 있다. 이 녹음은 특히 라틴 댄스 음악과 같은 카우벨의 사용이 많은 장르에 대해 비트 메이킹에 사용하기 위해 샘플링될 수 있다]\n' +
      '\n' +
      '사용자: 이 오디오에서 어떤 악기가 연주되고 있습니까?\n' +
      '\n' +
      '보조: 이 오디오에서 연주되는 악기는 카우벨입니다.\n' +
      '\n' +
      '사용자: 어떻게 재생되고 있습니까?\n' +
      '\n' +
      '보조: 카우벨은 드럼 스틱의 측면과 끝을 반복적으로 가지고 놀고 있습니다.\n' +
      '\n' +
      '사용자: 카우벨의 톤은 무엇입니까?\n' +
      '\n' +
      '보조: 카우벨의 음색은 밝고 울리며 관통합니다.\n' +
      '\n' +
      '도 6: 필터링 전의 LAION-CLAP 유사성의 분포.\n' +
      '\n' +
      '## 부록 C 부가예 및 오디오 플라밍고 논의\n' +
      '\n' +
      '우리는 독자들이 포괄적인 데모를 위해 데모 웹사이트[https://audioflamingo.github.io/](https://audioflamingo.github.io/])를 방문하도록 권장한다.\n' +
      '\n' +
      '보이지 않는 라벨에 대한 추가 소수 샷 실험\n' +
      '\n' +
      '오디오 플라밍고는 ICL 기반 RAG의 도움으로 완전히 보이지 않는 라벨로 분류에 적응할 수 있다. 표 9에서 우리는 오디오 플라밍고가 훈련 중에 라벨이 보이지 않는 새로운 분류 작업에 적응할 수 있음을 보여준다. 라벨은 각각 새 종과 총명이다. 0-샷에서 소수의 샷으로 크게 개선된 것은 오디오 플라밍고가 검색된 컨텍스트 샘플을 추적하는 강력한 능력을 나타낸다.\n' +
      '\n' +
      'BG-Sun-Sound의 라벨은 다음과 같다.\n' +
      '\n' +
      '총, SLR, M4, VSS, AK, SKS, PP, P90, AUG, SCAR, UMP, QBZ, P92, M249, QBU, DEAGLE, TOMY, PUMP, P1911, K2, DP, GRO, AWM, VEC, UZI, M24, KAR, VERL, MINI, WIN, MK, R45, M16, P118C, G36C, S12K, DBS, R1895.\n' +
      '\n' +
      'Birds-Dataset-Till-2022의 라벨은:\n' +
      '\n' +
      '브라질 티나무, 붉은 다리 티나무, 후드 티나무, 밤나무 배 관, 난쟁이\n' +
      '\n' +
      '카소워리, 리틀 스포티드 키위, 왜소 티나무, 바우도 관, 리틀 차칼라카, 팔팔한 눈썹\n' +
      '\n' +
      '티나무, 레아, 오스트레일리아 브루쉬터키, 베를렙쉬의 티나무, 스포티드 노투라\n' +
      '\n' +
      '푸나 티나무, 브루즐랜드 티나무, 안데스 관, 검은부리 브루슈터키, 푸른목\n' +
      '\n' +
      '관, 소말리아 타조, 와틀드브러쉬터키, 흰배 차칼라카, 바레드\n' +
      '\n' +
      'Tinamou, Chaco Chachalaca, Rufous head Chachalaca, Plain Chachalaca, Emu, Southern\n' +
      '\n' +
      '캐서워리, 흰배 노투라, 언듈레이티드 틴타무, 타우니 가슴 틴타무, 두꺼운\n' +
      '\n' +
      '티나무, 수염 구안, 퀘베초 크레스트 티나무, 스케일 차칼라카, 마이크로네시아 메가포드\n' +
      '\n' +
      '니코바르 메가포드, 흰목 티나무, 말레폴, 크레스트드 관, 밤나무 날개, 흰목 티나무, 말레폴, 크레스트드 관\n' +
      '\n' +
      'Chachalaca, Speckled Chachalaca, Ornate Tinamou, Curve-billed Tinamou, Tataupa Tinamou\n' +
      '\n' +
      '화이트 크레스트 관, 남부 브라운 키위, 술라 메가포드 빅시 스쿠보폴, 바누아투 메가포드\n' +
      '\n' +
      '파타고니안 티나무, 몰루칸 메가포드, 바리게티드 티나무, 흰머리 관, 마라일 관\n' +
      '\n' +
      '체스트넛 머리 차칼라카, 노던 카소워리 바틀렛의 티나무, 러스티 마진 관\n' +
      '\n' +
      '스픽스의 관, 띠꼬리 관, 안데스 티나무 트리니다드 파이핑 관, 우아한 크레스트 티나무\n' +
      '\n' +
      '노스아일랜드 브라운 키위, 시네러우스 티나무 흰날개 관, 회색머리 차칼라카\n' +
      '\n' +
      '오카리토 키위, 루푸스 배 차칼라카, 붉은 날개 틴타무, 루푸스 배 차칼라카\n' +
      '\n' +
      '검은 머리 파이핑 관, 타닌바르 메가포드, 초코 티나무, 큰 점박이 키위, 리틀\n' +
      '\n' +
      '티나무, 슬래티 가슴 티나무, 테푸이 티나무, 그레이터 레아, 칠레 티나무, 코카\n' +
      '\n' +
      '관, 노랑다리 티나무, 블랙 티나무, 레서 노투라, 러스티 티나무, 그레이 레그\n' +
      '\n' +
      'Tinamou, Dusky Megapode, Redbilled Brushturkey, Dusky-Legg\n' +
      '\n' +
      '관, 칼라드 브루슈터키, 커먼 타조, 콜롬비아 차칼라카 멜라네시안 메가포드, 말레오, 뉴기니 스크루보폴\n' +
      '\n' +
      '붉은 목의 파이핑 관, 화이코 티나무, 버프 브라운 차칼라카, 검은 머리 티나무\n' +
      '\n' +
      '필리핀 메가포드, 그레이트 티나무, 오렌지발 스크루보폴, 브라운 티나무, 솔리티나무\n' +
      '\n' +
      'Taczanowski\'s Tinamou, Grey Tinamou, Smallbilled Tinamou, Tongan Megapode, Highland Tinamou,\n' +
      '\n' +
      '서부 멕시코 차칼라카, 동부 브라질 차칼라카, 붉은 얼굴의 관, 다윈의 노투라.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c} \\hline Dataset & Zero-shot & Few-shot \\\\ \\hline BG-Gun-Sound (Park et al., 2022) & 1.6\\% & 53.5\\% \\\\ Birds-Dataset-Till-2022 (Mohanty) & 0.0\\% & 42.2\\% \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 보이지 않는 레이블이 있는 새로운 분류 작업에 대한 오디오 플라밍고의 정확도.\n' +
      '\n' +
      '###오디오 플라밍고는 사운드를 깊이 있게 이해한다.\n' +
      '\n' +
      '```\n' +
      '오디오 플라밍고는 오디오에서 발생하는 것을 이해한다. [오디오에 대한 주관적인 설명: 세 사람이 순서대로 말하고, 녹음에 잡음이 많다.] 작업: 오디오 질문 답변 지시: 몇 개의 다른 목소리가 있는가? 지상 진실 3 하나 3\n' +
      '```\n' +
      '\n' +
      '오디오 플라밍고는 소리의 순서를 이해한다\n' +
      '\n' +
      '```\n' +
      '[오디오에 대한 주관적인 설명: 한 남자가 토크쇼에서 연설한 다음, 청중들은 웃고 박수를 칩니다.] 과제: 오디오 캡션 지시: 이 소리를 묘사하세요. 근본 진실: 한 남자가 박수갈채를 받으며 연설하고 있다. 기준: 한 무리의 사람들이 말하고 박수를 치고 있다. 오디오 플라밍고: 한 남자가 말하고 있고 청중들은 박수를 칩니다.\n' +
      '```\n' +
      '\n' +
      '오디오 플라밍고는 오디오의 품질을 이해한다.\n' +
      '\n' +
      '```\n' +
      '[오디오에 대한 주관적인 설명: 키보드 연주 화음으로 시작하는 아름답고 슬픈 기악곡의 곡이 천천히 연주되고 부드러운 바이올린이 뒤따른다.] 과제: 오디오 질문 답변 명령어: 오디오의 대기 질은 무엇인가? 지상 진실: 오디오의 대기 품질은 공간과 깊이감을 창조하는 능력으로, 신비감과 경이로움을 창조하는 것에 초점을 맞춘다. 기준: 오디오는 풍부하고 역동적인 품질을 가지며, 이는 아마도 음악 또는 음악 공연과 연관될 수 있는 활기차고 매력적인 분위기를 시사하지만, 주어진 오디오 이벤트들에서 특정되지 않는다. 오디오 플라밍고: 그 오디오는 우울하고 내성적인 분위기를 가지고 있다.\n' +
      '```\n' +
      '\n' +
      '**오디오 플라밍고는 시끄럽고 시간에 따른 변화를 인지합니다**\n' +
      '\n' +
      '```\n' +
      '[오디오에 대한 주관적인 묘사: 남자가 작은 볼륨으로 말하고, 그 다음에 매우 큰 폭발이 있고, 그 다음에 남자는 매우 짧은 시간 동안 놀란 방법으로 웃는다.] 과제: 오디오 캡션 지시: 소리를 문장으로 묘사한다. 근본 진실: 한 남자가 잠시 이야기를 나눈 후 크게 터져 웃음을 터뜨렸다. 기준: 한 사람이 웃고 있고 총이 발사된다. 오디오 플라밍고: 한 남자가 말하고 있다가 큰 소리가 난다.\n' +
      '```\n' +
      '\n' +
      '오디오 플라밍고는 소리의 거리를 이해한다\n' +
      '\n' +
      '```\n' +
      '[오디오에 대한 주관적인 설명: 한 남자가 빗속에서 말하고, 그 다음에 다른 남자가 매우 약간 웃고, 그 다음에 뇌우 소리.] 과제: 오디오 캡션 지시: 이 소리를 묘사하세요. 땅 진리: 비와 천둥소리가 멀리서 울리자 다른 사람이 웃으며 이야기를 이어가는 남자. 기준: 한 남자가 말하고 있고 비가 내리고 있다. 오디오 플라밍고: 한 남자가 천둥을 배경으로 빗속에서 이야기하고 있습니다.\n' +
      '```\n' +
      '\n' +
      '**C.3. 오디오 플라밍고는 검색된 샘플을 본다**\n' +
      '\n' +
      '**오디오 플라밍고는 검색된 샘플에서 핵심 단어를 차용합니다**\n' +
      '\n' +
      '태스크: _Audio Captioning_\n' +
      '\n' +
      '명령:_이 소리를 설명합니다._\n' +
      '\n' +
      'Ground Truth:_Paper가 구겨지고 구겨지며, 쿵쿵 소리가 발생한다._\n' +
      '\n' +
      '기준: - 조리대에 냄비와 프라이팬이 있는 주방에서 사람의 움직임._\n' +
      '\n' +
      'Retrieved samples:\n' +
      '\n' +
      '_1) 일부 재료가 구겨진다._1\n' +
      '\n' +
      '_2) 플라스틱이 구지고 있습니다._2\n' +
      '\n' +
      '_3) 작은 단차로 종이의 크링클링._\n' +
      '\n' +
      '오디오 플라밍고: 한 사람이 종이 한 장을 구겨서 구겨낸다._\n' +
      '\n' +
      '태스크: _Audio Captioning_\n' +
      '\n' +
      '명령:_이 소리를 설명합니다._\n' +
      '\n' +
      '땅 진리: 여성이 말하고 수도꼭지 물이 흐른다._\n' +
      '\n' +
      '기준: 한 여성이 아이와 이야기하고 있습니다._\n' +
      '\n' +
      'Retrieved samples:\n' +
      '\n' +
      '_1) 말하는 여자와 물놀이._1)\n' +
      '\n' +
      '2) 여성의 목소리에 이어 수도꼭지에서 물이 흘러나온다._2)\n' +
      '\n' +
      '_3) 수도꼭지에서 물이 뿜어져 나오면 사람이 말을 한다._3)\n' +
      '\n' +
      '오디오 플라밍고: 여인은 수도꼭지에서 물이 흐르는 동안 말한다._\n' +
      '\n' +
      '**오디오 플라밍고는 시끄러운 검색된 샘플을 무시하고 정확한 예측을 할 수 있다.**\n' +
      '\n' +
      '과제: _Music Genre Classification_\n' +
      '\n' +
      '명령: _이 음악의 장르가 무엇입니까?_\n' +
      '\n' +
      '지상 진실: _pop._\n' +
      '\n' +
      'Retrieved samples:\n' +
      '\n' +
      '_1) reggae._\n' +
      '\n' +
      '_2) jazz._\n' +
      '\n' +
      '_3) disco._\n' +
      '\n' +
      '오디오 플라밍고: _pop._\n' +
      '\n' +
      '과제: _Music Instrument Classification_\n' +
      '\n' +
      '명령: _ 이 음악의 악기는 무엇인가?_\n' +
      '\n' +
      '지상 진실: _flute._\n' +
      '\n' +
      'Retrieved samples:\n' +
      '\n' +
      '_1) clarinet._\n' +
      '\n' +
      '_2) clarinet._\n' +
      '\n' +
      '_3) clarinet._\n' +
      '\n' +
      '오디오 플라밍고: _flute._\n' +
      '\n' +
      '###오디오 플라밍고는 강력한 대화 능력\n' +
      '\n' +
      '[Audio annotations: Sound of Chirp, Twit(High-pitched, short, repetitive bird sound)] [0.000s-1.157s], [1.543s-2.055s], [3.220s-4.409s], [4.748s-5.709s], [5.921s-4.386s], [4.772s-5.666s], [8.016s-8.719s], [9.134s-10.000s], [2.024s-3.764s-5.706s], [3.535s-2.039s], [3.7213s-4.386s], [8.016s-8.719s], [9.134s-10.000s], [2.024s-3.764s-6.706s], [3.535s-2.039s], [3.921s-6.386s], [8.336s-6.706s], [9.134\n' +
      '\n' +
      '접지 진실:_오디오는 주로 처핑과 새 발성을 포함한 새 소리로 구성된다. 다른 동물들의 소리도 있습니다. 또한 특정 간격으로 말하는 여성이 있습니다._\n' +
      '\n' +
      '기준:_버드들이 짹짹거린다._\n' +
      '\n' +
      '오디오 플라밍고:_오디오는 새의 발성, 기계적 소리, 충격 소리, 사람이 말하는 것, 째깍거리는 소리를 포함한 다양한 소리를 특징으로 한다._\n' +
      '\n' +
      'User: _새소리가 계속 들리는가?_\n' +
      '\n' +
      '접지 진실: - 새 소리가 연속적이지 않습니다. 그것들은 오디오 전체에 걸쳐 다른 간격으로 발생하며, 일부는 여성의 연설과 겹친다._\n' +
      '\n' +
      '기준:_예, 새 소리는 연속적입니다._\n' +
      '\n' +
      '오디오 플라밍고: 아니요, 새 소리는 연속적이지 않습니다. 오디오 전반에 걸쳐 서로 다른 간격으로 발생합니다._\n' +
      '\n' +
      '사용자:_새 소리와 사람의 말이 겹치나요?_\n' +
      '\n' +
      '접지 진실: _예, 새 소리와 여성의 말이 겹치는 경우가 있습니다._\n' +
      '\n' +
      '기준:_예, 새 소리와 여성의 연설이 겹칩니다._\n' +
      '\n' +
      '오디오 플라밍고: _예, 새 소리와 사람의 연설은 오디오의 특정 지점에서 겹칩니다._\n' +
      '\n' +
      '[오디오 주석: 사운드 오브 뮤직(진동 악기에 의해 생성된 소리): [0.000s-10.000s]; 사운드 오브 랩핑(리듬 스피치 오버 비트): [0.000s-10.000s]; 사운드 오브 남성 노래(심층, 울림, 그리고 강력한 음색): [0.000s-10.000s]] 사용자: _남성 목소리가 랩핑하는가 아니면 노래하는가? Ground Truth: 남성 목소리는 오디오에서 랩과 노래 두 가지 모두를 하고 있다._\n' +
      '\n' +
      '기준:_남성 목소리가 랩을 하고 있습니다._\n' +
      '\n' +
      '오디오 플라밍고:_남성 목소리가 랩을 하고 있습니다._\n' +
      '\n' +
      'User:_어떤 음악인가요?_\n' +
      '\n' +
      '접지 진실: _ 음악의 종류는 명시되어 있지 않지만 랩과 남성노래를 동반한다._\n' +
      '\n' +
      '기준:__음악의 종류는 힙합입니다._\n' +
      '\n' +
      'Audio Flamingo: _The music is a rap song._\n' +
      '\n' +
      '사용자: _남자 목소리가 유일한 존재인가요?_\n' +
      '\n' +
      '접지 진실: _Yes, male voice is only one specified in the audio, both rapping and singing._\n' +
      '\n' +
      '기준: _Yes, male voice is only one present._\n' +
      '\n' +
      '오디오 플라밍고: _Yes, male voice is only one presence in the audio._ [오디오 주석: 소리 오브 뮤직(진동 악기들이 내는 소리): [0.000s-10.000s]; 소리 오브 남성 노래(심층, 공명, 그리고 강력한 음색): [1.341s-3.734s], [9.289s-10.000s]] 사용자: _이 음악은 어떤 장르에 속하는가?_ Ground Truth: _이 음악은 R&B/Disco 장르에 속한다._ 기준: _지정된 오디오 클립에 지정되지 않음._ Audio Flamingo: _이 음악은 R&B 장르에 속한다._ User:__이 트랙의 보컬에 대해 자세히 알려주실 수 있나요?_ Ground Truth: _Sure, 이 곡의 보컬은 더 높은 레지스터에서 노래하고 있는 남성 가수의 것이다. 그의 목소리는 효과들로 수정되어 마치 음성 샘플처럼 들렸다._ 기준: _ 이 트랙의 보컬들은 풍부하고 풀바디인 것으로 기술되지만, 오디오 클립에서 특정 가수 또는 노래되고 있는 가사에 대해 더 이상의 세부사항은 주어지지 않는다._ 오디오 플라밍고: 이 곡의 보컬은 남자 가수의 것이다. 그의 목소리는 깊고 울려 퍼지며 강렬하여 노래의 전반적인 정서적 영향을 더한다._\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>