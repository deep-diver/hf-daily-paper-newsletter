<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ViewFusion: Interpolated Denoising을 통한 Multi-View 일관성에 대한 연구\n' +
      '\n' +
      '양생희({}^{1,2}\\), 옌 주오({}^{1}\\), 사메라 라마싱헤({}^{1}\\), 로리스 바자니({}^{1}\\), 길 아브라함({}^{1}\\), 안톤 반 덴 헝겔({}^{1,3}\\)\n' +
      '\n' +
      'Adelaide University, \\({}^{3}\\)The University of Sydney, \\({}^{1}\\)Amazon, \\({}^{2}\\)The University of Sydney, \\({}^{3}\\)The University of Sydney, \\({}^{1}\\)Amazon, \\({}^{2}\\)The University of Sydney, \\({}^{3}\\)\n' +
      '\n' +
      '아마존 인턴십 때 했던 일\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델을 통한 새로운 시점 합성은 다양하고 고품질의 이미지를 생성할 수 있는 놀라운 잠재력을 보여주었다. 그러나 이러한 기존의 방법에서 이미지 생성의 독립적인 프로세스는 다중 뷰 일관성을 유지하는 데 어려움을 초래한다. 이를 해결하기 위해, 우리는 기존의 사전 훈련된 확산 모델에 매끄럽게 통합될 수 있는 새로운 훈련 없는 알고리즘인 ViewFusion을 소개한다. 우리의 접근 방식은 이전에 생성된 뷰를 다음 뷰 생성을 위한 컨텍스트로 암묵적으로 활용하는 자동 회귀 방법을 채택하여 새로운 뷰 생성 프로세스 동안 강력한 다중 뷰 일관성을 보장한다. 보간된 잡음제거를 통해 알려진 뷰 정보를 융합하는 확산 과정을 통해, 본 프레임워크는 단일 뷰 조건 모델들을 추가적인 미세 조정 없이 다중 뷰 조건 설정에서 동작하도록 성공적으로 확장한다. 광범위한 실험 결과는 일관되고 상세한 새로운 뷰를 생성하는 데 ViewFusion의 효과를 보여준다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '인간은 하나의 이미지 뷰에서 보이지 않는 관점을 시각화하는 놀라운 능력을 가지고 있으며, 이는 모델에 복잡하게 남아 있는 직관적인 프로세스이다. 이러한 능력은 NVS(Novel View Synthesis)로 알려져 있으며 평평한 이미지에서 3차원 세부 사항을 정확하게 추론하기 위해 강력한 기하학적 사전이 필요하며, 2차원 투영에서 3차원 형태로 리프팅은 객체와 공간의 특성에 대한 가정과 지식을 포함한다. 최근 NVS의 상당한 발전은 신경망[15, 16, 31, 42, 61, 65, 66, 73, 76, 77]에 의해 진전되었으며, 여기서 다운스트림 재구성을 위한 새로운 뷰 생성은 유망한 잠재력을 보여준다[35, 70].\n' +
      '\n' +
      '구체적으로, 확산 모델[20, 52] 및 고품질 2D 이미지를 생성하는 능력은 3D 도메인에서 상당한 관심을 얻었으며, 여기서 사전 훈련된 텍스트 조절된 2D 확산 모델은 증류[4, 32, 41, 48, 50, 58, 64, 69, 74]를 통해 3D 적용을 위해 재목적화되었다. 후속 접근법 [35, 70]은 텍스트 컨디셔닝의 요건을 제거하고 대신에 NVS를 위한 조건으로서 이미지 및 타겟 포즈를 취한다. 그러나, 확산 모델이 특정 다운스트림 작업(_e.g._, 최적화 신경 복사 필드(NeRFs) [42])에 적합한 멀티뷰 일관된 출력을 생성할 수 없기 때문에 증류[64]가 여전히 요구된다.\n' +
      '\n' +
      '단일-뷰 설정 하에서, 멀티-뷰 일관성을 유지하는 것은 주어진 입력 이미지와 정렬되는 신규 뷰에 대해 몇 개의 그럴듯한 출력들이 존재할 수 있기 때문에 특히 도전적이다. 독립적인 방식으로 새로운 뷰를 생성하는 확산 기반 접근법의 경우[35, 70], 이는 다중 뷰 불일치의 아티팩트를 포함하는 합성 뷰를 생성한다(도 1a). 이전 작업[34, 37, 39, 54, 75, 78]은 NeRF[34]의 특징 투영 레이어를 포함하거나 NeRF 최적화[37, 78]를 제한하기 위해 3차원 전조를 사용하는 불일치 문제를 해결하기 위해 다운스트림 재구성의 견고성을 개선하는 데 중점을 두지만, 이러한 기술은 추가 모듈을 원래 확산 모델에 정렬하기 위해 훈련 또는 미세 조정이 필요하다.\n' +
      '\n' +
      '이 작업에서는 뷰 합성 과정에서 발생하는 다중 뷰 불일치를 다룬다. 오히려\n' +
      '\n' +
      '그림 1: 확산 기반 신규 시점 합성 모델에서 다시점 불일치의 원인. (a) 확산 모델들은 다양성 및 더 나은 분포 모델링을 위해 랜덤성을 통합한다; 이러한 독립적인 생성 프로세스는 특정 인스턴스들 하에서 사실적인 뷰들을 생성하지만, 인접한 뷰들에 걸쳐 정렬이 결여된 다양한 인스턴스들에 대해 상이한 그럴듯한 뷰들을 생성할 수 있다. (b) 대조적으로, ViewFusion은 동일한 고밀도 영역에서 끝나는 상관된 잡음 제거 프로세스를 보장함으로써 불확실성을 줄이고 다중 뷰 일관성을 달성하기 위해 자동 회귀 프로세스를 통합하여 뷰 전반에 걸쳐 일관성을 육성한다.\n' +
      '\n' +
      '초기 참조 영상에만 조건화된 뷰들을 독립적으로 합성함으로써, 이후에 생성된 뷰들 각각이 이전에 생성된 뷰들의 _entire set_에도 조건화되는 새로운 접근 방식을 개발한다. 구체적으로, 본 방법은 확산 프로세스에 자동 회귀 프로세스를 통합하여 뷰의 공동 분포를 모델링하고, 이미 생성된 뷰의 동일한 고밀도 영역을 향해 노이즈 제거 방향을 유지하여 새로운 뷰 합성을 안내한다(도 1b).\n' +
      '\n' +
      '뷰퓨전이라는 프레임워크는 보간된 노이즈 제거 프로세스를 통해 일반적인 확산 모델의 단일 뷰 컨디셔닝 요구 사항을 완화한다. ViewFusion은 몇 가지 추가적인 장점을 제공한다: 1) 사용 가능한 모든 뷰를 안내로 활용하여 더 많은 정보를 통합함으로써 생성된 이미지의 품질을 향상시킬 수 있다; 2) 미리 훈련된 단일 뷰 조건 확산 모델을 다중 뷰 조건 확산 모델로 쉽게 변환할 수 있는 추가적인 미세 조정이 필요하지 않다; 3) 목표 뷰에 대한 상대적인 뷰 거리에 기초하여 조건 이미지에 대한 적응 가중치를 설정하는 데 더 큰 유연성을 제공한다.\n' +
      '\n' +
      '본 논문의 기여도는 다음과 같다.\n' +
      '\n' +
      '* 합성 뷰의 멀티뷰 일관성을 향상시키고 다중 조건 입력을 지원하기 위해 미리 학습된 확산 모델에 직접 적용할 수 있는 _training-free_ 알고리즘을 제안한다.\n' +
      '* 우리의 방법은 뷰 합성을 위한 이전의 자동 회귀 접근법의 주요 한계를 암묵적으로 해결하는 _Interpolated Denoising_라고 부르는 새로운 자동 회귀 접근법을 사용한다.\n' +
      '* ABO[6] 및 GSO[10]에 대한 광범위한 경험적 분석은 우리의 방법이 이미지 생성에서 더 나은 3D 일관성을 달성할 수 있음을 보여주며, 이는 다른 기준 방법보다 단일 뷰 및 다중 뷰 이미지 설정 하에서 새로운 뷰 합성 및 형상의 3D 재구성에 상당한 개선으로 이어진다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '### 3D 적응 확산 모델\n' +
      '\n' +
      '확산 모델은 조건부 입력[21, 47, 51, 85]을 사용하여 이미지 생성에 탁월했으며 2D 도메인에서 이러한 성공을 감안할 때 최근 연구에서는 확산 모델을 3D 콘텐츠 생성[1, 3, 5, 11, 17, 19, 23, 25, 26, 28, 38, 44, 45, 67, 83, 84, 46]으로 확장하려고 시도했다. 그럼에도 불구하고 드림퓨전[48] 및 스코어 자코비안 체인[64]과 같은 선구적인 작업은 미리 훈련된 텍스트 조건 확산 모델을 활용하여 증류를 통해 3D 모델을 제작한다. 후속 접근법 [4, 32, 58, 69]는 속도, 분해능 및 형상 품질 측면에서 이러한 증류를 개선한다. [41, 50, 58, 74]와 같은 접근법은 텍스트 역전의 사소한 특성으로 인해 제한된 성공으로 캡션의 사용을 통해 이미지 조건을 지원하기 위해 확장된다[14].\n' +
      '\n' +
      '새로운 뷰 합성 확산 모델\n' +
      '\n' +
      '다른 연구 라인 [2, 9, 18, 29, 36, 57, 59, 62, 63, 70, 72, 79, 81, 87]은 형상 재구성을 위한 멀티뷰 이미지를 생성하기 위해 2D 확산 모델을 직접 적용한다. 텍스트 조절 확산 모델의 약점을 피하기 위해 새로운 뷰를 생성하기 위한 조건으로 이미지와 목표 포즈를 취하는 새로운 뷰 합성 확산 모델[35, 70]도 탐색되었다. 그러나 이러한 접근법의 경우 3D 일관된 모양을 복구하는 것이 여전히 핵심 과제이다. 3D 불일치를 완화하기 위해, Liu et al. [34]는 특징 투영 층들을 갖는 신경 복사 필드(NeRF)를 트레이닝하는 것을 제안한다. 동시에, 다른 작업들[75, 75, 39, 71, 37]은 에피폴라 어텐션[78], 동기화된 멀티뷰 노이즈 예측기[37] 및 교차 뷰 어텐션[71, 39]을 포함하는 멀티뷰 일관성을 위해 원래 확산 모델에 모듈을 추가한다; 이러한 방법들은 이미 미리 트레이닝된 모델을 미세 조정해야 하지만. 다중 뷰 조건을 포함하는 추가 학습 가능한 모델을 사용하여 단일 뷰 확산 모델을 확장하는 대신, 사전 학습된 확산 모델이 노이즈 제거 단계를 통해 이전에 생성된 뷰를 통합하고 이러한 모델을 전체론적으로 다중 뷰 설정으로 확장할 수 있는 훈련 없는 방법을 채택한다.\n' +
      '\n' +
      '### 기타 단일 시점 재구성 방법\n' +
      '\n' +
      '3D 복원에 사용되는 생성 모델이 번성하기 전에 많은 작업[12, 13, 15, 16, 27, 30, 31, 60, 65, 76]이 회귀[15, 16, 30, 65, 76] 또는 검색[60]을 사용하여 단일 뷰 이미지에서 3D 모양을 재구성했으며, 이 두 작업 모두 실제 데이터 또는 새로운 범주로 일반화하는 데 어려움을 겪는다. NeRFs(Neural Radiance Fields) [42]에 기반한 방법들은 새로운 시점 합성에서 성공을 발견했지만, 이러한 접근법들은 일반적으로 정확하게 보정된 카메라 위치들을 갖는 조밀하게 캡처된 이미지들에 의존한다. 현재 여러 연구에서 단일 뷰 설정[22, 33, 53, 80]에 대한 NeRF의 적응을 조사하고 있지만 단일 뷰 이미지에서 임의의 객체를 재구성하는 것은 여전히 어려운 문제이다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '확산 확률 모델의 노이즈 제거\n' +
      '\n' +
      '디노이징 확산 확률 모델(DDPM) [20, 55]는 샘플을 반복적으로 디노이징하는 학습을 통해 실제 데이터 분포\\(q(x_{0})\\)를 다루기 쉬운 모델 분포\\(p_{\\theta}(x_{0})\\)로 모델링하는 생성 모델의 한 종류이다. 확률 모델 \\(p_{\\theta}(\\mathbf{x}_{0})=\\int p_{\\theta}(\\mathbf{x}_{0:T})d\\mathbf{x}_{1:T}\\)을 학습하여 비정형 잡음 \\(\\mathbf{x}_{T}\\)을 마르코프 체인 형태로 실제 샘플 \\(\\mathbf{x}_{0}\\)으로 변환하고 가우시안 천이한다. 상기 가우시안 천이는 다음과 같이 정의된다:\n' +
      '\n' +
      '[q(\\mathbf{x}_{T}|\\mathbf{x}_{0})=\\prod_{t=1}^{T}q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\prod_{t=1}^{T}\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1}, \\beta_{t}\\mathbf{I}), \\tag{1}\\t}\n' +
      '\n' +
      '여기서 \\(\\beta_{t},t\\in\\{1,...,T\\}\\)는 각각 디노이징 프로세스에서의 분산 스케쥴 파라미터 및 타임스텝이다. 역 잡음 제거 과정은 가우시안 분포 \\(q(\\mathbf{x}_{T})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)에서 샘플링된 잡음으로부터 시작되어 다음과 같이 구성된다:\n' +
      '\n' +
      '(\\mathbf{x}_{0}|\\mathbf{x}_{T})=\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{t})=\\prod_{t=1}^{T}\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_{ \\theta}(\\mathbf{x}_{t},t),\\sigma_{t}^{2}\\mathbf{I}), \\tag{2}\\t{t}=1}^{t}\\mathcal{N}(\\mathbf{x}_{t},t),\\sigma_{t}^{2}\\mathbf{I},\n' +
      '\n' +
      '여기서 분산 \\(\\sigma_{t}^{2}\\)은 시간 종속 상수 [20], \\(\\mu_{\\theta}(\\mathbf{x}_{t},t)\\)는 학습된 _noise predictor_\\(\\epsilon_{\\theta}\\):\n' +
      '\n' +
      '\\frac{1}{\\sqrt{\\alpha_{t}}\\left(\\mathbf{x}_{t}-\\frac{\\beta_{t}}{\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon_{\\theta}(\\mathbf{x}_{t},t)\\right)\\tag{3}\\t.\n' +
      '\n' +
      '여기서, \\(\\alpha_{t}\\) 및 \\(\\bar{\\alpha_{t}\\)는 \\(\\beta_{t}\\)으로부터 유도된 상수이다. 잡음 예측기의 목적은 다음과 같다.\n' +
      '\n' +
      '\\mathbb{E}_{t,\\mathbf{x}_{0},\\epsilon}\\left[\\|\\epsilon-\\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}\\epsilon,t}\\|_{2}\\right], \\tag{4}\\heta}\n' +
      '\n' +
      '여기서 \\(\\epsilon\\)은 \\(\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)[20]에서 샘플링된 랜덤 변수이다.\n' +
      '\n' +
      '### 포즈-조건부 확산 모델\n' +
      '\n' +
      '다른 생성 모델[43, 56]과 유사하게 확산 모델은 본질적으로 \\(p_{\\theta}(x_{t-1}|x_{t},y)\\) 형태의 조건부 분포를 모델링할 수 있는 능력을 가지고 있다. 본 논문에서는 텍스트 기술[51], 시맨틱 맵[21, 47] 또는 다른 이미지 대 이미지 변환 작업[21]을 포함한 다양한 입력 양식을 통해 합성 과정을 제어할 수 있는 조건부 디노이징 오토인코더(\\(\\epsilon_{\\theta}(\\mathbf{x}_{t},t,y)\\)를 사용한다. 다음에서는 새로운 시점 합성에 대한 다양한 접근법을 제시하며, 우리 자신의 것을 포함하여 다양한 작업이 단일 역확산 단계의 개념에 접근하는 방법을 탐구한다. 이러한 비교를 통해 우리는 이러한 다른 방법론 간의 근본적인 관계를 명확히 하고 확립한다. 표기는 하단 첨자\\((\\cdot)_{t}\\)는 확산 단계를 나타내고 상단 첨자\\((\\cdot)^{i}\\)는 뷰 인덱스에 관한 것이다. 이어서, \\(i\\)번째 조건영상과 목표시점에 대한 상대포즈를 각각 \\(\\mathbf{y}^{i}\\)와 \\(\\pi^{i}\\)으로 정의하고, timestep \\(t\\)에서 잡음제거될 잡음영상을 \\(\\mathbf{x}_{t}\\)으로 정의한다.\n' +
      '\n' +
      '단일 입력 이미지 및 목표 포즈 \\(\\mathbf{y}^{1},\\pi^{1}\\)이 주어졌을 때, 직접 조건은 제로 1-to-3 [35]에 의해 역 프로세스에 적용되었다:\n' +
      '\n' +
      '\\[p(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{y}^{1},\\pi^{1}). \\tag{5}\\]\n' +
      '\n' +
      '확률적 컨디셔닝은 뷰 모음\\(p_{\\mathbf{y},\\pi}(\\mathcal{Y},\\pi)\\)에서 샘플링된 여러 뷰를 활용할 수 있는 [70]에 의해 공식화되었다:\n' +
      '\n' +
      '\\[p(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{y}^{i},\\pi^{i}),\\{\\mathbf{y}^{i},\\pi^{i}\\sim p_{\\mathbf{y},\\pi}(\\mathcal{Y},\\pi),\\tag{6}\\]]\n' +
      '\n' +
      '그림 2: 자동 회귀 생성 프로세스의 그림입니다. 제안하는 방법에서는 미리 학습된 확산 모델을 단일 단계에서 다단계 생성으로 확장하고 생성된 뷰를 모두 포함하는 뷰 집합을 유지한다. 각 단계에 대해, 우리는 \\(N\\) 역확산 과정을 구성하고 공통 시작 잡음을 공유한다. 이 생성 단계 내의 각 시간 단계에서 확산 모델은 \\(N\\)개의 잡음을 개별적으로 예측한다. 그런 다음, 이 \\(N\\) 잡음들은 _Noise Interpolation Module_을 통해 가중 보간되어, 후속 디노이징 단계들을 위해 공유된 보간 잡음으로 디노이징 단계를 마무리한다.\n' +
      '\n' +
      '여기서 이미지 및 포즈의 샘플링은 각각의 확산 단계 \\(t\\)에서 일어난다.\n' +
      '\n' +
      '이미지 조건\\(y^{1}\\)이 주어졌을 때 많은 뷰들의 조인트 분포를 학습하는 SyncDreamer[37]에서 조인트 출력 분포를 보였다:\n' +
      '\n' +
      '\\[p(\\mathbf{x}_{t-1}^{1:N}|\\mathbf{x}_{t}^{1:N},\\mathbf{y}^{1},e^{1}), \\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(N\\)은 생성된 신규 뷰의 수이고 \\(e\\)은 고도 조건(부분 포즈 정보)이다. 우리는 이 공식에서 목표 포즈가 출력의 다양한 포즈 생성을 허용하는 조건의 일부로 완전히 지정되지 않았다는 점에 주목한다.\n' +
      '\n' +
      '자기회귀분포는 \\(\\mathbf{y}^{1:N-1},\\pi^{1:N-1}\\)의 집합에 포함된 단일 또는 다중 조건 이미지와 포즈들이 주어진 임의의 수의 뷰들을 생성할 수 있는 자기회귀분포 설정이다:\n' +
      '\n' +
      '\\[p(\\mathbf{x}_{t-1}^{N}|\\mathbf{x}_{t}^{N},\\mathbf{y}^{1:N-1},\\pi^{1:N-1}). \\tag{8}\\]\n' +
      '\n' +
      '우리의 접근법은 자동 회귀 범주에 속하며 이 섹션의 나머지 부분에 대해 이 샘플링 전략을 달성하기 위한 구현을 자세히 설명한다.\n' +
      '\n' +
      '### Interpolated Denoising\n' +
      '\n' +
      '표준 DDPM 모델은 이미지 및 타겟 포즈(_i.e_, 회전 및 병진 오프셋들)를 조건부 입력들로서 사용함으로써 신규-뷰 이미지 합성에 적응되었다[70]. 대규모 데이터 세트에 대한 훈련에 이어, 이 접근법은 제로 샷 재구성을 위한 능력을 입증했다[35]. 다시점 일관성을 유지하는 문제를 해결하기 위해 순차 프레임을 생성하기 위해 자동 회귀 접근법을 사용한다(도 2 참조). 입력 영상들만으로 각 프레임을 독립적으로 생성하는 대신, 인접한 영상들 사이에 상당한 변화가 발생하기 쉬운 과정인 자동 회귀 알고리즘을 확산 과정에 통합한다. 이러한 통합을 통해 조건부 결합 분포를 모델링할 수 있어 프레임 간의 보다 부드럽고 일관된 전환을 보장할 수 있다.\n' +
      '\n' +
      '서로 다른 뷰에서 이미지를 사용하여 새로운 뷰의 합성을 유도하기 위해 보간된 잡음 제거 프로세스를 설계한다. 이 도출을 위해, 우리는 \\(\\{\\mathbf{y}^{1},...,\\mathbf{y}^{N-1}\\})으로 표시된 \\(N-1}) 이미지를 포함하는 이미지 세트에 대한 액세스를 가정한다. 이러한 \\(N-1\\)개의 뷰(q(\\mathbf{x}_{1:T}^{N}|\\mathbf{y}^{1:N-1})\\)에 조건화된 \\(N\\)번째 뷰 이미지의 분포를 모델링하고자 한다. 여기서 조건 이미지(\\(\\mathbf{y}^{1},...,\\mathbf{y}^{N-1}\\)와 목표 이미지(\\(\\mathbf{x}_{0}^{N}\\) 사이의 상대적 포즈 오프셋 \\(\\pi^{i},i\\in\\{1,N-1}\\)은 단순화를 위해 생략된다. 다시점 조건부 확산 모델의 전방 과정은 Eq에서 바닐라 DDPM의 직접 확장이다. 1, 여기서 노이즈는 모든 뷰에 독립적으로 부가되는\n' +
      '\n' +
      '\\\\[q(\\mathbf{x}_{1:T}^{N}|\\mathbf{y}^{1:N})=\\prod_{t=1}^{T}q(\\mathbf{x}_{t}^{N}|\\mathbf{x}_{t-1}^{N},\\mathbf{y}^{1:N}} \\tag{9}\\]\n' +
      '\n' +
      '여기서 \\(q(\\mathbf{x}_{t}^{N}|\\mathbf{x}_{t-1}^{N},\\mathbf{y}^{1:N})=\\mathcal{N}(\\mathbf{x}_{t}^{N};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1}^{N},\\beta_{t}\\mathbf{I})\\). 초기값은 \\(\\mathbf{x}_{0}^{N}:=\\mathbf{y}^{N}\\)으로 정의된다. 마찬가지로, 이하 Eq. 도 2에서, \\(\\log\\) 역과정은 다음과 같이 구성된다.\n' +
      '\n' +
      '\\mathbf{x}_{t-1}^{N}(\\mathbf{x}_{t-1}^{N},\\mathbf{y}^{n}\\sum_{t=1}^{N}(\\mathbf{x}_{t-1}^{N},\\mathbf{y}^{t}^{I})\\sum_{t=1}^{t}^{N}(\\mathbf{x}_{t-1}^{N},\\mathbf{y}^{n}\\sum_{t=1}^{t}^{n}(\\mathbf{x}_{t}^{n},\\mathbf{t}^{2}\\mathbf{ I}\\right)\n' +
      '\n' +
      '여기서 \\(\\bar{\\mu}_theta},\\bar{\\sigma_{t}^{2}\\)는 \\(N-1\\)_log_-정규 분포의 합산의 평균과 분산으로 취해진다. Eq.10의 첨자(\\((1)\\)에 대한 노트; 도출의 어수선함을 피하기 위해, 우리는 동일한 랜덤 변수 \\(\\mathbf{x}_{t-1}^{N}\\)의 독립적인 추론을 서로 다른 _a_\\(\\mathbf{y}^{n}\\)을 사용하여 가정하며, 이는 명확성을 위해 생략된 추가 첨자를 필요로 한다.\n' +
      '\n' +
      '### Single and Multi-view 잡음제거\n' +
      '\n' +
      '그러나 실제_In practice_는 모든 \\(N-1\\) 뷰가 아닌 단일 뷰 또는 소수의 뷰를 가질 수 있다. 본 절의 상기를 위해 전체 역확산 과정을 거친 후 추정된 뷰를 \\(\\mathbf{x}_{0}^{n}\\), \\(n\\)번째 뷰로 처리한다. 우리는 \\(\\mu_{\\theta}(\\mathbf{x}_{t},\\mathbf{y}^{1:N-1},t)\\)을 \\(\\mu_{\\theta}^{n}(\\mathbf{x}_{t},\\mathbf{y}^{n},t)\\의 가중 평균으로 사용한다. 주어진 뷰와 추정된 뷰를 모두 사용하여 \\(\\bar{\\mu}_theta}\\)을 계산하기 위해 서로 다른 뷰가 목표 뷰에 다르게 기여하는 접근법을 채택하고, 제약 조건 \\(\\sum_{n=1}^{N-1}w_{n}=1\\)을 만족시키면서 실제 \\(\\omega_{n}\\)번째 뷰에 대해 가중치 \\(\\omega_{n}\\)을 할당한다. 그림의 _Noise 보간 모듈_입니다. 도 2는 다음과 같이 모델링된다:\n' +
      '\n' +
      '{split}\\bar{\\theta}(\\mathbf{x}_{t},\\mathbf{y}^{n}\\sum_{n=1}^{n}{mu}(\\mathbf{x}_{t},\\mathbf{y}^{t}\\sum_{n=1}^{n}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{y}^{t}}\\sum_{n=1}^{n}\\epsilon_{\\theta}(\\mathbf{x}_{t},\\mathbf{y}^{t}}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\\um_{n}\n' +
      '\n' +
      '우리의 접근법에서는 전체 뷰 집합이 주어지지 않기 때문에 이 과정을 자동 회귀 방식으로 근사화하고 세대 동안 조건 집합을 성장시킨다. 각 오프셋, 방위각(\\(\\Delta_{a}^{n}\\)), 고도(\\(\\Delta_{e}^{n}\\)), 거리(\\(\\Delta_{d}^{n}\\)), 목표 뷰와 조건 뷰 사이의 가중치 매개변수 \\(\\omega_{n}\\)를 정의한다. 핵심 아이디어는 타겟 뷰가 거의 반대각에 위치하더라도 초기 조건 이미지에 대한 가중치가 너무 빠르게 감소하지 않도록 보장하면서 잡음 제거 프로세스 동안 근시점 이미지에 더 높은 중요도를 할당하는 것이다. 우리는 초기 조건 영상에 지수 감쇠 가중치 함수를 사용하여 \\(\\omega_{n}=e^{-\\frac{\\Delta_{a}^{n}}{\\tau_{c}}\\)로 정의한다. 여기서, \\(\\tau_{c}\\)는 감쇠 속도를 조절하는 온도 파라미터이고, \\(\\Delta^{n}\\)은 표적과 조건 포즈 사이의 절대 상대 방위각(\\(\\Delta_{a}^{n}\\)), 고도(\\(\\Delta_{e}^{n}\\)), 거리(\\(\\Delta_{d}^{n}\\))의 합이다. 우리는 \\(\\Delta^{n}\\)을 \\(\\Delta^{n}=|\\Delta_{a}^{n}|/\\pi+|\\Delta_{e}^{n}|/\\pi+|\\Delta_{d}^{n}|\\으로 계산한다.\n' +
      '\n' +
      '나머지 영상들의 가중치들을 \\(\\{x_{0}^{2},...,x_{0}^{N}\\})으로 나타내면, 모두 초기 조건 영상 \\(y^{1}:=x_{0}^{1}\\)에서 생성된 값이며, 가중치들을 정의하기 위해 softmax 함수를 사용한다.\n' +
      '\n' +
      '\\[\\omega_{n}=\\text{Softmax}(\\frac{e^{-\\frac{\\Delta^{n}}{\\tau_{g}}}{\\sum_{n=2}^{N}e^{-\\frac{\\Delta^{n}}{\\tau_{g}}}),n=2,...,N\\tag{12}}}\n' +
      '\n' +
      '마찬가지로, \\(\\Delta_{n}\\)은 타겟 뷰와 \\(n\\)번째 생성 뷰 사이의 상대적인 포즈 오프셋을 나타내고, \\(\\tau_{g}\\)은 생성 뷰에 대한 온도 파라미터를 나타낸다. 일 예로서, 상기 싱글뷰의 경우, 상기 가중치들은 다음과 같이 표현되고,\n' +
      '\n' +
      '\\begin{cases}exp(-\\frac{\\Delta^{n}}{\\tau_{c}}),&n=1\\\\(1-\\omega_{1})\\text{Softmax}(\\frac{e^{-\\frac{\\Delta^{n}}{\\tau_{g}}}{\\sum_{n=2}}^{N}e^{-\\frac{\\Delta^{n}}{\\tau_{g}}}),&n\\neq 1\\end{cases}\\tag{13}}}}}\n' +
      '\n' +
      '생성된 이미지 가중치에 \\(1-\\omega_{1}\\)이라는 용어를 적용하여 \\(\\sum_{n=1}^{N-1}w_{n}=1\\)의 요구사항을 만족시킬 수 있도록 한다. 실제로, Eq.16은 조건 세트가 \\(1\\), _i.e_, 멀티뷰 생성(보충 참조)보다 클 수 있도록 일반화된다.\n' +
      '\n' +
      '### Step-by-step Generation\n' +
      '\n' +
      '**단일 이미지 생성.** 이미지 생성에 자동 회귀 접근법을 적용할 때 그림 1과 같이 생성 궤적을 고안했다. 2(a) 우리는 이 궤적을 따라 일련의 뷰들을 균일하게 샘플링한다. 이 궤적 상에서 이전에 생성된 각각의 뷰 이미지는 조건 세트에 통합되어, 우리의 보간된 잡음 제거 방법을 통해 후속 잡음 제거 프로세스에 대한 안내를 제공한다. 이 궤적에 필요한 \\(S\\)으로 표시된 단계 수를 결정하기 위해 다음 공식을 사용한다.\n' +
      '\n' +
      '\\[S=max\\left(\\lceil\\frac{\\Delta_{a}^{N}{\\delta}\\rceil,\\lceil\\frac{\\Delta_{e}^{N}{\\delta}\\rceil\\right)\\tag{14}\\]\n' +
      '\n' +
      '여기서는 목표 시점 오프셋(\\Delta_{a}^{N}\\)과 \\(\\Delta_{e}^{N}\\)을 기반으로 스텝 카운트(S\\)를 결정하기 위해 스텝당 최대 오프셋(\\delta\\)을 설정한다. 그런 다음 다음 방정식을 사용하여 \\(n\\)번째 뷰를 샘플링한다.\n' +
      '\n' +
      '[(\\Delta_{a}^{n},\\Delta_{e}^{n},\\Delta_{d}^{n})=(\\frac{\\Delta_{a}^{N}}{S}*n,\\frac{\\Delta_{e}^{N}}{S}*n,\\frac{\\Delta_{d}^{N}}{S}*n,\\frac{\\Delta_{d}^{N}}{S}*n}{S}*n}}{S}*n}\\tag{15}\\frac{\\Delta_{e}^{n}}}\n' +
      '\n' +
      '**스핀 비디오 생성.** 단일 타겟 이미지를 생성하는 것과는 대조적으로, 스핀 비디오 생성의 과정은 초기 이미지로부터 시작하여 동일한 위치에서 마무리된다. 이를 위해서는 단순히 회전 정도 범위([0^{\\circ},360^{\\circ}])를 따르는 것이 아니라 넓은 범위의 회전 영상을 활용하기 위해 생성 순서를 수정해야 한다. 이것은 \\(\\Delta_{a}=\\pi\\)에서 뷰가 컨디셔닝 뷰와 반대이므로 생성 프로세스의 종료를 표시하기 때문이다. 스핀 비디오 생성을 위한 생성 순서를 설정하기 위해 최소 방위각 오프셋을 \\(\\delta\\)이라고 하고, 그림에 표시된 \\(\\delta, -\\delta, 2\\delta, -2\\delta...,N\\delta\\}\\)의 순서로 스킵 궤적을 사용한다. 2(b). 단순화를 위해 우리는 이 맥락에서 방위각 차원을 따른 회전만을 고려한다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '**Datasets.** 우리는 우리의 방법을 평가하고 ABO[6] 및 GSO[10] 데이터 세트의 기준선과 비교한다. 이러한 데이터 세트는 모든 기준선이 Objavverse[8]에서 훈련되기 때문에 배포되지 않습니다. 또한, 실제 영상에 대한 정성적인 결과를 제시함으로써 본 논문에서 제안한 방법의 성능을 보완적으로 보여준다. 추가적인 결과는 보충에 포함된 영상을 참고하시기 바랍니다.\n' +
      '\n' +
      '**메트릭.** 세 가지 주요 기준으로 새로운 시점 합성을 평가합니다.\n' +
      '\n' +
      '1. [leftmargin=*]\n' +
      '2. _Image Quality:_ LPIPS[86], PSNR, 및 SSIM[68] 메트릭들은 합성된 뷰들과 지상-진실 뷰들 사이의 유사성을 측정하는 것을 돕는다.\n' +
      '\n' +
      '도 3: 단계별 생성의 일러스트레이션. (a) 이 궤적을 따라 연속적으로 뷰들을 균일하게 샘플링하여 새로운 뷰 이미지를 생성하고, (b) 뷰 거리에 따라 가장 가까운 뷰에서 가장 먼 뷰까지 샘플링하여 \\(360^{\\circ}\\) 스핀 비디오를 생성한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:6]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '[70]에서 제안된 dard auto-regression, 보간된 조건, 보간된 출력 및 확률적 컨디셔닝.\n' +
      '\n' +
      '표준 자동 회귀.자동 회귀에 대한 하나의 초기 접근법은 마지막 생성된 뷰를 후속 컨디셔닝으로 사용하는 것을 포함한다. 그러나 이 방법은 순차적인 생성 과정에서 오차가 누적되어 불량한 생성 품질을 나타낸다. 각 후속 뷰가 이전 뷰의 정확도에 의존하기 때문에 초기 단계의 부정확성 또는 불완전성이 복합되어 전체 이미지 품질이 저하될 수 있다. 이러한 한계는 오류 전파 문제를 해결하고 생성된 뷰의 품질을 향상시키기 위해 보다 정교한 자동 회귀 전략의 필요성을 강조한다.\n' +
      '\n' +
      'Interpolated Conditions와 Interpolated Outputs.Interpolated Conditions와 Interpolated Outputs는 기존의 확산 모델에 자동 회귀 생성을 도입하기 위한 두 가지 간단한 접근법이다. 전자의 방법은 조건 이미지 및 포즈로부터 특징 임베딩을 보간하는 것을 포함하고, 후자의 방법은 모델에 의해 생성된 최종 이미지 특징 맵을 보간한다. SSIM 및 PSNR 메트릭이 탭의 다른 것보다 보간 출력에 유리한 결과를 보여주음에도 불구하고. 도 5에 도시된 바와 같이, 시각적 비교에 도시된 바와 같다. 도 7은 그것이 출력 뷰들의 블러링으로 이어지고 이것이 더 큰 LPIPS 거리에 의해 확증된다는 것을 도시한다.\n' +
      '\n' +
      'Stochastic Conditioning.또한 [70]에서 제안한 Stochastic Conditioning Sampler를 Zero-1-to-3 모델에 적용하여 보았다. 우리는 확률적 컨디셔닝이 원하는 자동 회귀 생성 결과를 전달하지 못하는 것을 관찰한다; 이것은 왓슨 등이 사용한 확산 모델이 훈련된 특정 범주에 기인하여 그럴듯한 조건 이미지를 더 효과적으로 처리할 수 있다. 대조적으로, Zero-1-to-3 [35]는 교차 카테고리 데이터 세트에 대해 훈련되었고 제로 샷 재구성을 위해 설계되었다. 추가적으로, 우리의 평가 데이터는 유통 외 데이터(_i.e_., ABO[6] 및 GSO[10])를 포함한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '이 연구에서 우리는 확산 모델을 사용한 새로운 뷰 합성에서 다중 뷰 일관성의 문제를 해결하는 새로운 알고리즘인 ViewFusion을 개발했다. 우리의 접근법은 이전에 생성된 뷰의 전체 이력을 활용하여 뷰 합성을 점진적으로 개선하는 자동 회귀 메커니즘을 통합하여 미세 조정 또는 추가 모듈의 필요성을 우회한다. 제안하는 확산 보간 기법은 사전 학습된 확산 모델의 잡음 제거 과정을 단일 시점 설정에서 훈련 요구 없이 다중 시점 설정으로 확장한다. 경험적 증거는 일관되게 고품질 뷰를 생성하는 ViewFusion의 능력을 강조하며, 우리는 새로운 뷰 합성 및 3D 재구성 응용 프로그램에서 중요한 단계를 달성한다.\n' +
      '\n' +
      '그림 6: 단일 뷰(1 조건 이미지) 및 멀티 뷰(3 조건 이미지) 3D 재구성에 대한 정성적 결과. 우리의 방법은 다양한 재구성된 모양에 걸쳐 가장 일관된 결과를 제공한다.\n' +
      '\n' +
      '그림 7: 다양한 자동 회귀 세대에 대한 질적 비교.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:9]\n' +
      '\n' +
      '* [31] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. Soff-srn: Learning signed distance 3d object reconstruction from static images. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.\n' +
      '* [32] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _CVPR_, 2023.\n' +
      '* [33] Kai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer for nerf-based view synthesis from a single input image. In _WACV_, 2023.\n' +
      '* [34] Minghua Liu, Chao Xu, Haien Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization, 2023.\n' +
      '* [35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9298-9309, 2023.\n' +
      '* [36] Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, and Chi-Keung Tang. Deceptive-nerf: Enhancing nerf reconstruction using pseudo-observations from diffusion models. _arXiv preprint arXiv:2305.15171_, 2023.\n' +
      '* [37] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncderamer: Learning to generate multiview-consistent images from a single-view image. _arXiv preprint arXiv:2309.03453_, 2023.\n' +
      '* [38] Zhen Liu, Yao Feng, Michael J Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffusion: Score-based generative 3d mesh modeling. In _ICLR_, 2023.\n' +
      '* [39] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion, 2023.\n' +
      '* [40] D.G. Lowe. Object recognition from local scale-invariant features. In _Proceedings of the Seventh IEEE International Conference on Computer Vision_, pages 1150-1157 vol.2, 1999.\n' +
      '* [41] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from a single image. In _CVPR_, 2023.\n' +
      '* [42] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In _ECCV_, 2020.\n' +
      '* [43] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. _CoRR_, abs/1411.1784, 2014.\n' +
      '* [44] Norman Muller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Niessner. Diffrf: Rendering-guided 3d radiance field diffusion. In _CVPR_, 2023.\n' +
      '* [45] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. _arXiv preprint arXiv:2212.08751_, 2022.\n' +
      '* [46] Evangelos Navelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov. Autodecoding latent 3d diffusion models. _arXiv preprint arXiv:2307.05445_, 2023.\n' +
      '* [47] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [48] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In _ICLR_, 2023.\n' +
      '* [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '* [50] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dream-booth3d: Subject-driven text-to-3d generation. _arXiv preprint arXiv:2303.13508_, 2023.\n' +
      '* [51] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In _International conference on machine learning_, pages 1060-1069. PMLR, 2016.\n' +
      '* [52] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.\n' +
      '* [53] Kyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun Wu, and Deqing Sun. Vq3d: Learning a 3d-aware generative model on imagenet. _arXiv preprint arXiv:2302.06833_, 2023.\n' +
      '* [54] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation, 2023.\n' +
      '* [55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, 2015.\n' +
      '* [56] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In _Neural Information Processing Systems_, 2015.\n' +
      '* [57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion:(0-) image-conditioned 3d generative models from 2d data. _arXiv preprint arXiv:2306.07881_, 2023.\n' +
      '* [58] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior. In _ICCV_, 2023.\n' +
      '* [59] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion. _arXiv preprint arXiv:2307.01097_, 2023.\n' +
      '\n' +
      '* [60] Maxim Tatarchenko, Stephan R Richter, Rene Ranftl, Zhuwen Li, Vladlen Koltun, and Thomas Brox. What do single-view 3d reconstruction networks learn? In _CVPR_, 2019.\n' +
      '* [61] A. Tewari, O. Fried, J. Thies, V. Sitzmann, S. Lombardi, K. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M. Niessner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y. Zhu, C. Theobalt, M. Agrawala, E. Shechtman, D. B Goldman, and M. Zollhofer. State of the art on neural rendering. _Computer Graphics Forum_, 39(2):701-727, 2020.\n' +
      '* [62] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B Tenenbaum, Fredo Durand, William T Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. _arXiv preprint arXiv:2306.11719_, 2023.\n' +
      '* [63] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf. Consistent view synthesis with pose-guided diffusion models. In _CVPR_, 2023.\n' +
      '* [64] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In _CVPR_, 2023.\n' +
      '* [65] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In _ECCV_, 2018.\n' +
      '* [66] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In _NeurIPS_, 2021.\n' +
      '* [67] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using diffusion. In _CVPR_, 2023.\n' +
      '* [68] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE Transactions on Image Processing_, 13(4):600-612, 2004.\n' +
      '* [69] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.\n' +
      '* [70] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. _arXiv preprint arXiv:2210.04628_, 2022.\n' +
      '* [71] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, C. L. Philip Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis, 2023.\n' +
      '* [72] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 3d-aware image generation using 2d diffusion models. _arXiv preprint arXiv:2303.17905_, 2023.\n' +
      '* [73] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. In _Computer Graphics Forum_, 2022.\n' +
      '* [74] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360 views. _arXiv e-prints_, pages arXiv-2211, 2022.\n' +
      '* [75] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistent: Enforcing 3d consistency for multi-view images diffusion, 2023.\n' +
      '* [76] Xianghui Yang, Guosheng Lin, and Luping Zhou. Single-view 3d mesh reconstruction for seen and unseen categories. _IEEE Transactions on Image Processing_, pages 1-1, 2023.\n' +
      '* [77] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In _ECCV_, 2018.\n' +
      '* [78] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models, 2023.\n' +
      '* [79] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane Gu. Dreamsparse: Escaping from plato\'s cave with 2d frozen diffusion model given sparse views. _CoRR_, 2023.\n' +
      '* [80] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2021.\n' +
      '* [81] Jason J. Yu, Fereshteh Forghani, Konstantinos G. Derpanis, and Marcus A. Brubaker. Long-term photometric consistent novel view synthesis with diffusion models. In _ICCV_, 2023.\n' +
      '* [82] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.\n' +
      '* [83] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In _NeurIPS_, 2022.\n' +
      '* [84] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models. In _SIGGRAPH_, 2023.\n' +
      '* [85] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\n' +
      '* [86] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.\n' +
      '* [87] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In _CVPR_, 2023.\n' +
      '\n' +
      '##7 멀티뷰 세대.\n' +
      '\n' +
      '우리는 식 13에서 단일 뷰 생성에 대한 가중치를 공식화한다. 일반적인 경우 k개의 뷰가 주어졌을 때, 가중치는 다음과 같이 표현되며,\n' +
      '\n' +
      '{cases}exp(-\\frac{\\Delta^{n}}{\\tau_{c}})\\text{Softmax}(\\frac{e^{-\\frac{\\Delta^{n}}{\\tau_{c}}}{\\sum_n}e^{-\\frac{\\Delta^{n}}{\\tau_{c}}}}{\\sum_n}e^{-\\frac{\\delta^{n}}{\\tau_{c}}}}&n=1,\\dots,k\\\\(1-\\sum_i=1}^{k}\\omega_{i}}\\text{Softmax}(\\frac{e^{-\\frac{\\Delta^{n}}{\\tau_{g}}}}},&n>k\\end{n}e^{-\\frac{\\delta^{n}}{\\tau_{g}}}}{\\tau_{n}}}}{\\tau_{n}}}}{\\tau_{n}}}\n' +
      '\n' +
      '생성된 이미지 가중치에 \\(1-\\sum_{i=1}^{k}\\omega_{i}\\)이라는 용어를 적용하여 모든 가중치의 합이 목적 \\(\\sum_{n=1}^{N}w_{n}=1\\)의 요건으로 \\(1\\)이 되도록 한다.\n' +
      '\n' +
      '##8 이미지 렌더링\n' +
      '\n' +
      '우리는 Zero-1-to-3과 SyncDreamer가 각각 제공하는 렌더링 스크립트를 이용하여 테스트 데이터를 구성한다. 두 접근법 사이에는 카메라와 조명 설정에 약간의 차이가 있다는 점에 유의하는 것이 중요합니다.\n' +
      '\n' +
      '**카메라.** Zero-1-to-3은 \\([1.5,2.2]\\)의 범위 내에서 카메라 거리에 대한 랜덤 샘플링을 채용한다. 조건 및 대상 이미지 모두에 대한 방위각 및 고도각이 랜덤하게 선택됩니다. SyncDreamer는 고정된 카메라 거리를 1.5로 유지하고, 조건 및 목표 영상 모두에 대해 이산각(\\{0^{\\circ},22.5^{\\circ},45^{\\circ},...,337.5^{\\circ}\\})에서 방위각을 샘플링한다. 조건표고는 \\(0^{\\circ},30^{\\circ}]\\) 범위 내에서 랜덤하게 샘플링되며, 목표표고는 \\(30^{\\circ}\\)으로 고정된다.\n' +
      '\n' +
      '**조명.** Zero-1-to-3은 점광을 조명 모델로 사용한다. 반면에 SyncDreamer는 균일한 환경 조명 설정을 사용합니다. 이러한 조명의 선택은 렌더링 결과의 차이로 이어진다. 구체적으로, Zero-1-to-3의 렌더링은 객체의 뒷면에 그림자를 나타내는 반면 SyncDreamer의 렌더링은 그렇지 않다.\n' +
      '\n' +
      '렌더링의 이러한 불일치는 3D 재구성의 평가에 영향을 미친다. Zero-1-to-3을 기준으로 할 때, 우리는 공정한 비교를 위해 테스트 데이터를 구성하기 위해 Zero-1-to-3과 일관된 렌더링 설정을 채택한다.\n' +
      '\n' +
      '##9 SSIM과 PSNR\n' +
      '\n' +
      '본 원고에서는 탭 5에 자세히 설명된 대로 블러를 효과적으로 캡처하는 데 있어 SSIM 및 PSNR의 한계를 언급했으며 그림 5에 설명된 대로 예시적인 예로 이러한 한계를 더욱 강조한다. 도 8에 도시된 바와 같이, SSIM 및 PSNR 점수가 더 높은 이미지는 현저한 흐릿함을 나타낸다. 본 연구 결과는 확산 보간법과 비교할 때 출력 보간법의 비교 단점을 강조한다. 중요한 것은 LPIPS가 이미지 품질에 대한 보다 정확한 평가를 제공한다는 것이다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      '## 11 Application\n' +
      '\n' +
      '**다시점 생성.** 본 원고에서 언급한 바와 같이, 도입된 보간 잡음 제거 프로세스에 의한 다중 뷰 조건화 능력 덕분에 단일 뷰 조건화 모델을 쉽게 다중 뷰 조건화 모델로 확장할 수 있어 다중 뷰 재구성을 지원할 수 있다. 수량은 탭에 나와 있습니다. 3 및 그림 3에서 정성적 비교를 제공한다. 여기에서 11은 증가하는 수의 뷰로 개선된 재구성을 일관되게 산출하기 때문에 본 방법의 이점을 추가로 입증한다. 이러한 개선은 다시점 상태 영상을 처리하는데 있어서 제안된 기법의 효율성을 입증한다.\n' +
      '\n' +
      '**일관된 BRDF 분해.** 실험 관찰에서 사전 훈련된 디코더가 직면한 특정 문제를 식별했으며, 이는 종종 이미지에서 그림자와 표면 텍스처를 효과적으로 구별하기 위해 어려움을 겪는다. 이러한 한계를 극복하기 위해 우리는 이러한 시각적 요소를 세심하게 분리하도록 특별히 설계된 전용 분해 디코더를 도입했다. 이 분해 디코더는 보간된 잡음 제거 접근법과 통합될 때, 멀티 뷰 일관성을 유지할 뿐만 아니라 새로운 뷰 분해 및 렌더링 작업에서 탁월할 수 있는 잠재력을 나타낸다.\n' +
      '\n' +
      '이 새로운 기술의 조합은 유망한 가능성을 제공한다. 분해된 BRDF(Bidirectional Reflectance Distribution Function) 지도를 활용함으로써 장면의 조명과 형상 형상에 대한 보다 큰 제어를 얻을 수 있다. 일반 지도의 가용성은 조명 조건을 조작하는 능력을 향상시켜 그림 1과 같이 렌더링에 더 많은 유연성을 약속한다. 13. 이러한 제어 수준을 통해 동적 재조명, 창의적인 장면 구성 및 매혹적인 시각적 효과 생성과 같은 다양한 흥미진진한 응용 프로그램을 탐색할 수 있다. 이것은 예술적이고 실용적인 이미지와 비디오 조작을 위한 새로운 길을 열어 아티스트와 전문가에게 매력적이고 시각적으로 놀라운 콘텐츠를 제작할 수 있는 도구를 제공한다.\n' +
      '\n' +
      '도 9: 고장 사례에 대한 시각적 예. 고장 사례는 주로 특정 뷰(1번과 2번 행), 얼굴(3번 행), 세부 텍스처(4번 행), 복잡한 장면(5번 행), 고도각 모호성(6번 행)하에서의 고장을 포함한다. 도 10: 실제 영상에 대한 시각화. 이미지는 온라인으로 다운로드되었으며, 여기서 전경 객체는 분할되고 이미지는 사전 훈련 이미지와 정렬되도록 크기가 조정되었다.\n' +
      '\n' +
      '그림 12: BRDF 분해 w/o 대 w/자기회귀에 대한 질적 비교.\n' +
      '\n' +
      '그림 13: 재조명에 대한 질적 비교.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>