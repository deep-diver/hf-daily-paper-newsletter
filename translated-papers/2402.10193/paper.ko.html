<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# BitDelta: Your Fine-Tune May Only on One Bit\n' +
      '\n' +
      ' 제임스 류\n' +
      '\n' +
      'Guangxuan Xiao\n' +
      '\n' +
      'Kai Li\n' +
      '\n' +
      '이제이슨\n' +
      '\n' +
      'Song Han\n' +
      '\n' +
      'Tri Dao\n' +
      '\n' +
      'Tianle Cai\n' +
      '\n' +
      '동등한 기여도 1 매사추세츠 공과대학교 2프린스턴 대학교 3Together AI. 대응: James Liu <jamesll@mit.edu>, Tianle Cai <tainle.cai@princeton.edu>. 톈의 기여는 투게더 AI에서 자문하는 과정에서 일부 이뤄졌다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 일반적으로 대규모 인터넷 데이터 세트에 대한 사전 훈련과 다운스트림 작업에 대한 미세 조정이라는 두 단계로 훈련된다. 사전 훈련의 더 높은 계산 요구를 고려할 때, 미세 조정은 모델에 더 적은 새로운 정보를 추가하므로 더 압축성이 있다고 가정하는 것이 직관적이다. 우리는 미세 조정된 모델의 가중치를 미리 훈련된 구성요소와 추가 _delta_로 분해하여 이 가정을 탐구한다. 본 논문에서는 비트델타를 이용하여 성능을 저하시키지 않고 1비트까지 성공적으로 양자화할 수 있는 간단한 방법을 소개한다. 이 흥미로운 발견은 미세 조정 동안 추가된 정보의 잠재적인 중복성을 강조할 뿐만 아니라 미세 조정 모델의 다중 테넌트 서빙 및 다중 테넌트 저장에 중요한 의미를 갖는다. 다중 1비트 델타와 함께 단일 고정밀 베이스 모델의 사용을 가능하게 함으로써 비트델타는 GPU 메모리 요구량을 10\\(\\times\\) 이상 획기적으로 줄일 수 있으며, 이는 다중 테넌트 설정에서 향상된 생성 지연으로 변환될 수 있다. Llama-2 및 미스트랄 모델 패밀리에 걸친 실험을 통해 BitDelta를 검증하고 최대 70B 매개변수 모델에 대해 테스트된 모든 설정에서 최소 성능 저하를 보여준다. 코드는 [https://github.com/FasterDecoding/BitDelta](https://github.com/FasterDecoding/BitDelta)에서 유효하다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 사전 훈련 후, 기초 모델은 전형적으로 특정 다운스트림 작업에 대해 미세 조정된다(Devlin et al., 2019; Radford et al., 2018, 2019). 이러한 _pretrain-finetune_ 패러다임은 기계 학습에 혁명을 일으켰고; LLM은 명령어 팔로우 및 정렬과 같은 중요한 태스크에 대해 효과적인 것으로 입증되었을 뿐만 아니라(Ouyang et al., 2022), 넓은 어레이의 틈새에 대해 수행적이지만 매우 영향력 있는 애플리케이션이다(Xu et al., 2024; Qiu et al., 2023). 미세 조정을 통해 LLM은 뚜렷한 사용자 선호도 또는 특수 작업 요구 사항에 맞게 능숙하게 장착되어 전례 없는 수준의 적응성을 보여준다. 따라서, 개별 작업 및 사용자 요구에 각각 맞춤화된 수백만 개의 고유 미세 조정 모델을 제공할 것이라는 전망은 기계 학습의 미래에 대한 유망한 비전을 제시한다.\n' +
      '\n' +
      '이 비전을 실현하는 것은 두 가지 중요한 이유 때문에 어렵다: 1) **비싼 스토리지.** 각각의 새로운 미세 조정 모델은 비록 우리가 비교적 적은 기본 모델을 가지고 있더라도 크기 때문에 저장 비용이 많이 들고 디스크에 관리하기가 어렵다. 2) **비싼 서빙.** 독특한 미세 조정 모델은 각각 상당한 GPU 메모리를 요구하므로 눈에 띄는 다운타임 없이 이러한 모델을 동시에 서비스하는 것이 어렵고 비용이 많이 든다. 이러한 문제를 해결하기 위해 미세 조정 모델 가중치를 기본 사전 훈련 모델과 미세 조정 프로세스에 의해 유도된 _delta_의 가중치로 분해한다. 모델 성능을 유지하면서 이 델타를 압축함으로써 스토리지 및 GPU 메모리 요구와 관련된 엄청난 비용을 피하는 것을 목표로 한다.\n' +
      '\n' +
      '델타 분해 관점에서, LoRA(Hu et al., 2021; Houlsby et al., 2019; Rebuffi et al., 2017; Dettmers et al., 2023; Chen et al., 2023d)와 같은 파라미터-효율적 미세-튜닝(PEFT) 방법들은 PEFT-기반 미세-튜닝들의 모델 서빙을 위한 강력한 통찰력인 미세-튜닝_ 동안 고도로 구조화되고 압축된 형태의 델타 _during delta를 효과적으로 집행한다. (Sheng et al., 2023) and (Chen et al., 2023b) explore multi-tenant serving of LoRA-based fine-tunes.\n' +
      '\n' +
      '그럼에도 불구하고 최근 연구에 따르면 PEFT 방법은 특히 높은 자원 작업(Chen et al., 2022)에서 전체 매개변수 미세 조정의 모델 품질과 아직 일치하지 않을 수 있으며 하이퍼 매개변수 선택 및 프롬프트 방법(Niederfahrenhorst et al., 2023)에 상당히 민감하다. 그 결과, 유효한 README 파일을 가진 Open LLM Leaderboard(Beeching et al., 2023) 상의 2307개의 LLM들 중 단지 \\(<20\\%\\)만이 LoRA를 배타적으로 사용함을 알 수 있다. 대부분의 모델은 완전 파라미터 미세-튜닝, 완전 파라미터 미세-튜닝의 모델 병합(Yu et al., 2023; Jin et al., 2023; Wortsman et al., 2022) 또는 LoRA 기반 미세-튜닝의 모델 병합(효과적으로 높은 순위)이다.\n' +
      '\n' +
      '낮은 순위 행렬 _post-training_로 일반 델타를 근사화하는 것도 매력적이다. 그러나, 실험 결과는 전체 파라미터 미세 조정으로부터의 델타들이 상당히 높은 순위인 경향이 있기 때문에 이것이 도전적이라는 것을 보여준다(표 1).\n' +
      '\n' +
      '대신, 일반적으로 PEFT 방법을 동기화하는 통찰에서 도출한다: 사전 훈련의 더 높은 계산 요구를 감안할 때 미세 조정이 모델에 더 적은 새로운 정보를 추가하므로 _much_ 압축성이 더 높다고 가정하는 것이 직관적이다. 실제로, 우리는 성능 저하가 거의 없는 단지 _1 bit_로 델타를 효율적으로 _quantize_할 수 있음을 발견한다. 본 논문에서는 비트델타(BitDelta)를 제안한다. 비트델타는 미세 조정 모델과 기본 모델 사이의 가중치 델타(weight delta)에 작용하는 효율적인 훈련 후 양자화(post-training quantization; PTQ) 해이다.\n' +
      '\n' +
      '비트델타는 1) 각 가중치 행렬의 델타를 이진 행렬에 곱한 스케일링 팩터로 양자화한다. 구체적으로, 우리는 이진 행렬을 형성하기 위해 델타의 부호를 취하고 스케일링 팩터를 델타의 절대값의 평균으로 초기화하며, 이는 \\(L_{2}\\) 규범을 최소화한다. 2) 우리는 이진 행렬을 냉동 상태로 유지하면서 작은 보정 데이터 세트에 대한 모델 증류를 통해 스케일링 인자를 추가로 보정한다. 훈련 가능한 매개변수와 훈련 단계가 적음에도 불구하고 이 증류 공정이 모델 품질을 추가로 회복하는 데 효과적임을 발견했다. 비트델타는 다른 양자화 방법에 비해 매우 효율적이며, 70B 모델을 대략 10분 안에 압축할 수 있는 반면, GPTQ(Frantar et al., 2022) 및 AWQ(Lin et al., 2023)와 같은 방법은 여러 GPU 시간이 걸릴 수 있다. 17개의 인기 있는 미세 조정 모델에 대한 실험은 BitDelta가 성능에 미치는 영향을 최소화하면서 다양한 모델 유형 및 크기에 걸쳐 적용될 수 있음을 확인한다.\n' +
      '\n' +
      '비트델타는 공유 서버를 사용하여 여러 미세 조정 모델을 효율적으로 서비스할 수 있는 기회를 열어줍니다: 단일 완전 정밀 기본 모델만 저장하고 (동역학적으로) 부하\n' +
      '\n' +
      '도 1: **BitDelta의 개요. BitDelta는 미세 조정된 모델과 기본 모델 사이의 가중치 델타에 1비트 양자화를 적용한다. 각 가중치 행렬에 대해 델타를 부호 비트와 훈련 가능한 고정밀 스케일 팩터로 양자화한다. 스케일 팩터는 \\(L_{2}\\) norm에서 최적의 근사 오차를 얻기 위해 초기화되고 몇 가지 증류 단계로 더욱 정제된다. BitDelta는 단일 고정밀 기본 모델과 다중 1비트 델타로 여러 개의 미세 조정 모델을 표현함으로써 모델 성능의 최소 저하를 보여주고 다중 테넌시 서빙에서 메모리 소비를 줄인다.**\n' +
      '\n' +
      '그림 2: _Llama 2-7B_와 _Vicuna-7B v1.5_ 사이의 \\(4096\\times 4096\\) 중량 델타의 CEV 도표. 전체 매개변수 미세 조정에서 나온 델타들은 상당히 높은 순위를 차지하여 낮은 순위 근사치를 어렵게 만든다.\n' +
      '\n' +
      '다중 1비트 델타에서 배치 추론을 수행하고, 다중 미세 조정 모델을 효율적으로 나타낼 수 있다. 비트델타에 의해 압축된 델타(deltas)는 완전 정밀 미세 조정 모델을 사용하지 않는 것에 비해 10\\(\\times\\) 이상 작고 GPU 메모리를 적게 소모하므로 더 빨리 로드할 수 있다. 이것은 스토리지 문제를 해결합니다. 더욱이, LLM 추론이 메모리-바운드(Leviathan et al., 2022; Chen et al., 2023; Cai et al., 2024)이기 때문에, 각각의 디코딩 단계의 레이턴시는 모델 가중치의 GPU 메모리 소비에 비례한다. 효율적인 CUDA 커널 구현으로, 우리는 이러한 메모리 감소를 다른 양자화 방법과 유사하게 레이턴시 속도 향상으로 변환할 수 있다(Frantar et al., 2022; Lin et al., 2023). 예비 시도로서 트리톤(Tillet et al., 2019)에서 1비트 행렬 곱셈 커널을 구현하고, 다중 테넌트 서빙 레이턴시를 약 2\\(\\times\\) 개선함으로써 서빙 챌린지를 해결할 수 있음을 보인다.\n' +
      '\n' +
      '마지막으로, 우리는 비트델타의 몇 가지 확장을 연구하고, 기본 모델을 양자화하고, 비트델타를 반복적으로 적용한다. 실험 결과는 제안된 방법이 매우 일반적이며 다양한 사용 사례에 적용될 수 있음을 보여준다.\n' +
      '\n' +
      '##2 관련 업무\n' +
      '\n' +
      '##### 전체 모델 압축\n' +
      '\n' +
      '양자화.양자화 기술은 메모리 소비를 줄이고 LLM의 생성 대기 시간을 개선하기 위해 널리 사용된다. Xiao et al. (2023)은 활성화들과 파라미터들 사이에서 재스케일하는 기술을 구현하여, 보다 원활한 양자화를 용이하게 하기 위해 이상치 활성화들을 효과적으로 완화한다. Dettmers et al.(2022)은 행렬 곱셈을 8비트 계산으로 분해하는 접근법을 개발하며, 이상치를 처리하기 위한 추가적인 16비트 프로세스를 포함한다. 더 탐색하여, Frantar et al.(2022)은 가중치 열들을 3-4 비트의 정밀도로 반복적으로 라운딩하는 방법을 소개한다. 마찬가지로 Lin et al.(2023)은 다수를 3-4비트로 압축하면서 중요한 가중치를 선택적으로 보존하는 활성화 인식 양자화 방식을 제안한다. 다른 맥락에서, Kim et al.(2023)은 작지만 중요한 가중치 세트에 초점을 맞춘 희박하고 낮은 정밀도의 패턴을 고안한다. 마지막으로, Chee et al.(2023)은 비간섭성 처리를 활용하여 성능에 미치는 영향을 최소화하면서 모델 가중치를 최대 2비트로 양자화한다.\n' +
      '\n' +
      'Pruning.Pruning은 또한 신경망의 메모리 소모를 줄이는 것을 목표로 한다. 이는 특정 매개변수 값을 0으로 밀어 모델에서 희소성을 유도하여 이를 달성한다(LeCun et al., 1989; Han et al., 2015, 2016; Zhu and Gupta, 2017). 그러나, 이러한 방법들은 2:4(50%) 희소성(Mishra et al., 2021)과 같은 특정 구조화된 희소성 패턴들을 사용하지 않는 한 GPU와 같은 현대 하드웨어의 이점을 이용하지 못할 수 있다. Frantar and Alistarh (2023)는 2:4 희소성 패턴을 성공적으로 활용하고 50% 희소성 비율을 달성하는 LLM에서 가지치기 방법을 보여준다. 하드웨어 친화적이면서도 더 높은 희소성을 얻는 것은 어렵다.\n' +
      '\n' +
      '### Delta Compression\n' +
      '\n' +
      'Parameter-efficient fine-tuning.Parameter-efficient fine-tuning (PEFT) 기법은 fine-tuning 동안 훈련 가능한 파라미터의 수를 감소시키며, 메모리 및 컴퓨팅 요구를 감소시키면서 유망한 정확도를 달성한다. Houlsby et al.(2019)은 어댑터 층을 도입하여 미세 조정 중에 베이스 모델을 동결시킨다. Hu et al. (2021)은 파인-튜닝 동안 가중치 업데이트가 로우 랭크인 것을 강제하는 로우 랭크 적응(Low-Rank Adaptation; LoRA)을 제안한다.\n' +
      '\n' +
      '미세 조정 정보에서 구조를 강제함으로써 PEFT 기술은 _압축 인식 훈련_으로 볼 수 있다. 우리의 방법과 유사하게 PEFT 기술에 의해 유도된 이러한 압축은 또한 다중 테넌트 서빙을 향상시킬 수 있다. Chen et al. (2023) 및 Sheng et al. (2023)은 미세 조정 가중치 델타들의 낮은 순위 구조를 이용하는 커스터마이징된 CUDA 커널들에 기초하여 LoRA 기반 미세 조정을 위한 확장가능한 멀티-테넌트 시스템들을 개발함으로써 이 아이디어를 탐색한다.\n' +
      '\n' +
      '훈련 후 델타 압축에 대한 초기 연구.대부분의 연구는 훈련 후 델타 압축의 개념을 GPTQ, 비정형 프루닝(Han et al., 2016) 또는 고전적인 무손실 압축 알고리즘과 같은 기존 압축 기술을 채택함으로써 탐구한다(Isik et al., 2023; Yao and Klimovic, 2023; Yu et al., 2023). Isik et al.(2023)은 저장소를 절약하기 위해 델타 크기를 줄이는 것에 중점을 둔다. Yu et al.(2023)은 프루닝(pruning)을 활용하여 모델 병합 애플리케이션을 개선한다. Yao와 Klimovic(2023)의 동시적이고 독립적인 작업은 또한 멀티 테넌트 서빙을 개선하기 위해 델타 압축을 사용하여 탐색하지만, 디스크에서 GPU로의 모델 로딩 시간을 줄이는 데 더 중점을 둔다. 기존 작업에 비해 훨씬 간단하고 빠른 비트델타를 제안하여 10\\(\\times\\) 이상의 압축률을 달성하면서도 현대의 가속기와도 친근하다.\n' +
      '\n' +
      '## 3 BitDelta\n' +
      '\n' +
      '### Method\n' +
      '\n' +
      'BitDelta는 1) 각 가중치 행렬을 이진 행렬*을 곱한 스칼라로 양자화 한다. 2) 증류로 스칼라 인자를 추가로 보정한다. 본 논문에서는 1비트 양자화 단계에서의 각 단계를 설명한다. 기본 모델과 미세 조정 모델로부터 각각 가중치 행렬(W_{\\text{base},W_{\\text{fine}\\in\\mathbb{R}^{n\\times m}\\)로 한다. 가중치 델타를 \\(\\Delta=W_{\\text{fine}}-W_{\\text{base}}\\)으로 정의하며, 이는 미세 조정 후 가중치의 수정을 나타낸다. 효율적인 표현을 위해 부호 비트를 부호화하여 \\(\\hat{\\Delta}\\)으로 표기된 이 가중치 델타의 이진화 추정기를 구하는 것을 목표로 한다.\n' +
      '\n' +
      '\\[\\hat{\\Delta}=\\alpha\\odot\\text{Sign}(\\Delta), \\tag{1}\\]\n' +
      '\n' +
      'where\n' +
      '\n' +
      '\\[\\text{Sign}(W_{ij})=\\begin{cases}+1,&\\text{if }W_{ij}>0,\\\\-1,&\\text{if }W_{ij}\\leq 0,\\end{cases}\\tag{2}\\]\n' +
      '\n' +
      '그리고 \\(\\alpha\\)는 전체 행렬에 대한 고정밀 스케일링 팩터이다. \\(L_{2}\\) norm에서 양자화 오차를 최소화하기 위해:\n' +
      '\n' +
      '\\[\\left\\lVert\\Delta-\\hat{\\Delta}\\right\\rVert^{2}_{2}=\\sum_{ij}\\left(|W_{ij}|-\\alpha\\right)^{2}, \\tag{3}\\]\n' +
      '\n' +
      'we take\n' +
      '\n' +
      '\\[\\alpha=\\frac{1}{nm}\\sum_{ij}|W_{ij}|. \\tag{4}\\]\n' +
      '\n' +
      '놀랍게도, 우리는 위의 양자화 접근법이 이미 꽤 잘 작동하고 대부분의 미세 조정 모델의 성능을 유지한다는 것을 발견했다.\n' +
      '\n' +
      '스케일 증류.직관적으로, 스케일링 인자\\(\\alpha\\)는 저비트 체제에서 더 중요한 역할을 한다. 또한, 행렬당 가중치 오차(L_{2}\\)는 모델 품질 저하의 완벽한 척도가 아니다. 따라서 우리는 양자화된 모델의 출력 로짓을 원래 미세 조정 모델의 출력 로짓과 정렬하기 위해 모델 증류를 수행하여 이러한 척도를 추가로 최적화한다. 보다 구체적으로, 우리는 모델 가중치를 동결하고 다음 목적을 위해 최적화한다:\n' +
      '\n' +
      '\\arg\\min_{\\boldsymbol{\\alpha}}\\mathbbb{E}_{x\\sim\\mathbf{X}}\\left[\\left\\lVert\\mathbf{Z}_{\\text{fine}(x)-\\mathbf{Z}_{\\text{ bin}(x;\\boldsymbol{\\alpha}\\right\\rVert^{2}\\right]\\tag{5}\\text{5}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\\text{6}\n' +
      '\n' +
      '여기서 \\(\\mathbf{X}\\)는 교정 데이터세트이고 \\(\\mathbf{Z}(\\cdot)\\)는 각 모델의 로짓이다. 본 연구에서는 스케일 증류가 선택(\\(\\mathbf{X}\\)에 상당히 둔감하다는 것을 발견하였다. 1) 공정이 매우 매개변수 효율적이며, 2) 공정의 중요한 측면은 실제 텍스트 내용에 관계없이 미세 조정 모델과 로짓하는 것이다.\n' +
      '\n' +
      '실험을 위해 C4 데이터세트(Raffel et al., 2023)에 길이 128의 800개의 샘플을 사용하여 사전 훈련에 널리 사용되는 증류기를 사용하고, \\(lr=10^{-4}\\), \\(\\beta=(0.9,0.999)\\), \\(\\epsilon=10^{-8}\\)의 Adam optimizer(Kingma & Ba, 2017)를 사용한다. 1x80GB A100 GPU는 7B 및 13B 모델을 증류하는 데 사용되며, 6x80GB A100 GPU는 70B 모델(finetune의 경우 2x, 이진화의 경우 4x)을 증류하는 데 사용된다. 우리는 스케일 증류가 매우 빠르다는 것을 발견했고, 70B 모델을 대략 10분 안에 압축할 수 있는 반면 GPTQ(Frantar et al., 2022) 및 AWQ(Lin et al., 2023)와 같은 방법은 여러 시간이 걸린다.\n' +
      '\n' +
      '### Implication\n' +
      '\n' +
      '델타를 단지 1-비트로 압축하는 능력은 효율성을 개선하기 위한 다수의 기회를 열어, 보다 효과적인 모델 저장을 가능하게 한다(Isik et al., 2023). 여기서 단일 베이스 모델은 다수의 압축된 델타들과 함께 유지될 수 있고 모델 핫-스왑을 용이하게 한다(Chen et al., 2023; Sheng et al., 2023). 핫-스왑을 통해, 기본 모델은 GPU 메모리에 남아 있고, 압축된 델타들은 들어오는 요청들에 따라 동적으로 로딩된다. 두 경우 모두 압축 비율은 스토리지 요구 및 로딩 시간의 감소로 직접 변환될 수 있다.\n' +
      '\n' +
      '더욱이, BitDelta는 Punica(Chen et al., 2023) 또는 S-LoRA(Sheng et al., 2023)와 같은 다중 테넌트 서빙 시스템의 가능성을 가능하게 하지만, 단지 LoRA 모델들 대신에 일반적인 미세 조정 모델들에 대해 가능하다. 구체적으로, 동일한 기본 모델에서 미세 조정된 여러 모델이 동일한 서버로 서비스되는 시나리오를 고려한다. 이 설정은 GPU 리소스를 크게 활용하고 트래픽이 낮거나 불균형할 때 각 미세 조정 모델의 추론 비용을 절약한다. 비트델타를 사용하면 GPU 메모리에 여러 개의 압축된 델타로 하나의 고정밀 기본 모델을 유지할 수 있다. 여러 개의 미세 조정 모델을 직접 제공하는 것과 비교하여, 이 접근법은 메모리 소비를 크게 절약한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Model/Method & Train Loss & TruthfulQA & GSM8K & MT-Bench & Adjusted Average\\(\\dagger\\)\\(\\uparrow\\) \\\\ \\hline _Llama 2-7B_ & – & 38.96 & 13.57 & – & 60.53 \\\\ \\hline _Vicuna-7B v1.5_ & – & 50.36 & 19.03 & 6.04 & 60.51 \\\\ \\hline BitDelta-Initial & 0.41 & 47.63 & 19.56 & 5.67 & 60.99 \\\\ BitDelta & 0.052 & 49.97 & 20.17 & 5.99 & 60.68 \\\\ \\hline SVD-Initial & 0.78 & 44.56 & 15.92 & 4.45 & 60.60 \\\\ SVD & 0.34 & 44.55 & 14.18 & 4.98 & 60.20 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: BitDelta와 SVD 기반 방법(\\(r=16\\)), _Llama 2-7B_ 및 _Vicuna-7B v1.5_를 기본 및 미세 조정 모델로 사용한 비교. BitDelta는 전반적으로 수행적이지만 SVD 기반 방법은 미세 조정된 정보를 충분히 포착하지 못한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline Model & Method & TruthfulQA & GSM8K & MT-Bench & Adjusted Average\\({}^{\\dagger}\\uparrow\\) \\\\ \\hline _Llama 2-7B_ & – & 38.96 & 13.57 & – & 60.53 \\\\ \\hline \\multirow{3}{*}{_Llama 2-7B Chat_} & Baseline & 45.32 & 22.74 & 6.56 & 59.81 \\\\  & BitDelta-Initial & 41.10 & 18.27 & 6.31 & 60.7 \\\\  & BitDelta & 44.95 & 20.24 & 6.47 & 59.88 \\\\ \\hline \\multirow{3}{*}{_Vicuna-7B v1.5 16k_} & Baseline & 50.38 & 14.18 & 6.06 & 57.50 \\\\  & BitDelta-Initial & 45.58 & 13.95 & 5.69 & 58.51 \\\\  & BitDelta & 48.75 & 14.48 & 6.24 & 57.64 \\\\ \\hline \\multirow{3}{*}{_Llama 2-13B_} & – & 36.90 & 22.74 & – & 64.68 \\\\ \\hline \\multirow{3}{*}{_Llama 2-13B Chat_} & Baseline & 43.95 & 33.13 & 6.98 & 63.99 \\\\  & BitDelta-Initial & 41.70 & 33.36 & 7.06 & 64.25 \\\\  & BitDelta & 43.47 & 31.92 & 6.95 & 63.96 \\\\ \\hline \\multirow{3}{*}{_Vicuna-13B v1.5 16k_} & Baseline & 50.38 & 29.72 & 6.90 & 57.5 \\\\  & BitDelta-Initial & 41.7 & 26.76 & 6.60 & 64.25 \\\\  & BitDelta & 48.75 & 28.73 & 6.88 & 57.64 \\\\ \\hline \\multirow{3}{*}{_WizardLM-13B v1.2_} & Baseline & 47.17 & 42.38 & 6.95 & 61.61 \\\\  & BitDelta-Initial & 44.89 & 42.08 & 6.73 & 61.91 \\\\  & BitDelta & 46.67 & 41.62 & 6.93 & 61.86 \\\\ \\hline \\multirow{3}{*}{_Llama 2-70B_} & – & 44.82 & 52.69 & – & 71.81 \\\\ \\hline \\multirow{3}{*}{_Llama 2-70B Chat_} & Baseline & 52.77 & 47.61 & 7.12 & 68.82 \\\\  & BitDelta-Initial & 41.63 & 42.38 & 6.85 & 66.01 \\\\  & BitDelta & 51.37 & 48.82 & 7.06 & 69.14 \\\\ \\hline \\multirow{3}{*}{_Solar-0-70B_} & Baseline & 62.03 & 56.18 & 7.07 & 73.77 \\\\  & BitDelta-Initial & 59.08 & 56.79 & 6.79 & 73.14 \\\\  & BitDelta & 62.03 & 56.63 & 6.82 & 73.57 \\\\ \\hline \\multirow{3}{*}{_Mistral-7B v0.1_} & – & 42.60 & 37.76 & – & 65.98 \\\\ \\hline \\multirow{3}{*}{_Mistral-7B v0.1_} & Baseline & 55.93 & 32.75 & 6.86 & 60.36 \\\\  & BitDelta-Initial & 51.27 & 38.82 & 6.54 & 63.83 \\\\  & BitDelta & 55.23 & 31.54 & 6.43 & 61.10 \\\\ \\hline \\multirow{3}{*}{_Zephyr-7B-\\(\\beta\\)_} & Baseline & 55.12 & 34.34 & 7.18 & 65.22 \\\\  & BitDelta-Initial & 54.53 & 40.26 & 6.70 & 66.12 \\\\  & BitDelta & 58.39 & 31.92 & 7.00 & 66.20 \\\\ \\hline \\multirow{3}{*}{_Dolphin 2.2.1_} & Baseline & 54.02 & 54.28 & 7.36 & 67.31 \\\\  & BitDelta-Initial & 48.14 & 50.27 & 7.10 & 67.58 \\\\ \\cline{1-1}  & BitDelta & 54.91 & 52.84 & 7.20 & 66.97 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: BitDelta는 Llama-2 및 Mistral 패밀리와 7B에서 70B 매개변수 범위의 광범위한 모델 크기에서 작동한다. BitDelta는 SFT 기반 방법, RLHF 기반 방법 및 문맥 확장 방법(RoPE 스케일링)을 포함한 많은 유형의 미세 조정 정보에 대해 작동한다. 스케일 증류가 효과적이며, 이는 TruthfulQA/GSM8K 점수를 기준 미세 조정에서 1-2점 이내로, MT-벤치 점수를 0.1-0.2점 이내로 상승시킨다.\n' +
      '\n' +
      'LLM 추론은 생성 지연이 모델 가중치에 의해 사용되는 GPU 메모리에 비례하는 메모리 결합 계산 패턴을 따르기 때문에, 더 낮은 메모리 소비는 또한 서빙 레이턴시를 개선할 수 있는 기회를 제시한다. 예를 들어, Puncia와 S-LoRA는 낮은 랭크 델타들에 대해 배치 델타-활성화 곱을 효율적으로 계산할 수 있는 CUDA 커널을 개발함으로써 LoRA의 구조와 메모리 절약을 이용한다. 마찬가지로, 우리는 다음과 같이 각 선형 층의 순방향 패스를 분해한다:\n' +
      '\n' +
      '\\[X_{i}^{\\prime}=W_{\\text{fine},i}X_{i}\\approx W_{\\text{base}X_{i}+\\underbrace{ \\hat{\\Delta}_{i}X_{i}}_{\\text{Kernel} \\tag{6}\\brace{\n' +
      '\n' +
      '여기서 \\(X_{i}\\)와 \\(X_{i}^{\\prime}\\)은 \\(i\\)번째 미세 조정 모델에 대한 입력 및 출력 특징을 나타내며, 기본 모델 가중치와 델타는 별도로 계산된다. 일괄적인 요청의 경우, 고전적인 배치 GEMM 커널을 사용하여 \\(W_{\\text{base}}X_{i}\\)를 계산할 수 있다. 트리톤(Tillet et al., 2019)에서는 GPU 캐시로 전달될 때까지 1비트 델타 양자화를 유지하면서 배치 설정에서 \\(\\hat{\\Delta}_{i}X\\)을 계산할 수 있는 융합 이진 GEMM 커널을 구현한다. 이 커널은 역양자화 연산을 GEMM 계산과 융합하여 데이터 이동 오버헤드를 크게 줄인다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      '기준선.우리의 주요 기준선은 압축이 없는 원래 미세 조정 모델입니다. 또한 8-bit RTN과 4-bit GPTQ(Frantar et al., 2022)와 양자화된 기반 모델에서 BitDelta를 실행하는 평가를 비교한다.\n' +
      '\n' +
      '모델 및 데이터세트.우리는 Llama-2(Touvron et al., 2023) 및 Mistral(Jiang et al., 2023) 모델 패밀리: Vicuna, Xwin-LM, Solar-70B, Zephyr, OpenChat 3.5, Dolphin 2.2.1, and OpenOrca(Chiang et al., 2023; Team, 2023; Upstage, 2023; Tunstall et al., 2023; Wang et al., 2023; Hartford, 2023; Mukherjee et al., 2023). 우리는 8개의 과제를 평가한다: MT-Bench, 25-shot ARC Challenge, 5-shot BBH, 10-shot HellaSwag, Zero-shot TruthfulQA, Zero-shot LAMBADA, Zero-shot Winogrande, 및 5-shot GSM8K (Zheng et al., 2023; Clark et al., 2018; Suzgun et al., 2022; Zellers et al., 2019; Lin et al., 2022; Paperno et al., 2016; Sakaguchi et al., 2019; Cobbe et al., 2021). MT-Bench를 평가하기 위해 FastChat(Zheng et al., 2023)을 사용하고, 다른 작업을 평가하기 위해 1m-평가-harness(Gao et al., 2023)를 사용한다. 우리는 스케일 증류가 BitDelta-Initial로 적용되기 전에 방법론을 나타낸다.\n' +
      '\n' +
      '우리는 주로 미세 조정이 상당한 영향을 미치고 다른 메트릭을 집계하는 고마진 메트릭에 중점을 둔다. 전체 결과는 부록의 표 6~9를 참조하십시오. BitDelta는 집계된 메트릭에서 상당히 잘 수행되며, 심지어 많은 경우에 기준선을 능가한다. 그러나 이러한 결과를 기본 모델 자체 및 이러한 메트릭에 대해 수행하는 것과 관련하여 맥락화하는 것이 중요하다.\n' +
      '\n' +
      '이러한 경우 성능을 우리의 방법론이나 기본 기본 모델에 귀속시키는 것은 어렵다. 이 때문에 우리는 비트델타가 미세 조정 정보를 정확하게 보존한다는 것을 보여주기 위해 기본 모델이 어려움을 겪는 경향이 있는 TruthfulQA, GSM8K 및 MT-벤치를 강조한다.\n' +
      '\n' +
      '### Accurate Quantization\n' +
      '\n' +
      'SVD 비교.BitDelta를 약 낮은 랭크와 비교한다. of the weight delta on _Vicuna-7B v1.5_. 낮은 순위의 경우, 우리는 \\(\\Delta=U\\Sigma V\\)과 근사 \\(\\hat{\\Delta}=AB\\)을 분해한다. 여기서 \\(A=U\\sqrt{\\hat{\\Sigma}\\), \\(B=\\sqrt{\\hat{\\Sigma}V\\). 증류 동안 우리는 낮은 순위 행렬의 모든 항목을 훈련 가능한 매개변수로 처리한다. 우리는 낮은 계급이 약하다는 것을 안다. 미세한 곡조 정보를 완전히 포착하지 못하고, 전반적으로 저조한 성능을 보인다(표 1). 흥미로운 사실은 증류가 낮은 순위 약물에 대해 효과적이지 않다는 것을 발견했다. BitDelta와 비교되지만, 부분적으로 이것은 훈련 불안정성으로 이어지는 훈련 가능한 매개변수의 초과에 기인한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt} p{227.6pt}} \\hline \\hline\n' +
      '**Prompt** & Please create a slogan about bioenergy that satisfies the following: 1. Make the tone sound casual 2. Embed an advertisement for a company called “FlexPower” 3. Fewer than 10 words. \\\\ \\hline \\multirow{4}{*}{**BitDelta**} & “FlexPower”s groundbreaking bioenergy solution: Produce fuel without compromising food production!” (10 words) \\\\  & This headline is catchy, casual, and highlights the unique selling point of the... \\\\  & **GPT-4 Score: 8** \\\\ \\hline \\multirow{2}{*}{**BitDelta-Initial**} & “FlexPower”s groundbreaking technology unlocks the key to harnessing the power of renewable bioenergy while minimizing ethical concerns” \\\\  & **GPT-4 Score: 4** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 간결한 광고 작업인 MT-Bench에서 질문 9에 대한 _Zephyr-7B_-\\(\\beta\\)의 모델 응답 비교. 비트델타-이니셜은 지시를 따를 수 없으며, 지나치게 형식적이고 단어 제한을 준수하려는 시도를 하지 않는 광고를 생성한다. 스케일 증류를 추가하여 BitDelta는 단어 한계를 약간 넘는 간결하고 눈에 띄는 광고를 성공적으로 생성했다. *명확성을 위해 약간 수정하라는 프롬프트.\n' +
      '\n' +
      '주요 결과.BitDelta는 다양한 모델 패밀리, 광범위한 모델 크기 및 많은 미세 조정 기술에 걸쳐 수행됩니다. 우리는 라마-2 및 미스트랄 계열과 7B에서 70B 매개변수 범위의 모델을 벤치마킹한다. 표 2에 도시된 바와 같이, 우리는 BitDelta가 매우 일반적이며, _Mistral-7B v0.1 Instruct_ 상의 SFT 기반 방법들(Radford et al., 2018), _Llama 2 Chat_ 상의 RLHF 기반 방법들(Christiano et al., 2023), 및 _Vicuna-7B v1.5 16k_ 상의 컨텍스트 확장 방법들(RoPE scaling)을 포함하는 모든 유형의 피네튠 정보를 복구할 수 있다는 것을 발견한다(Chen et al., 2023; Press et al., 2022).\n' +
      '\n' +
      '우리는 BitDelta-Initial on _Mistral-7B v0.1 Instruct_와 _Zephyr-7B-\\(\\beta\\)_에 대한 GSM8K가 비정상적으로 높다는 점에 주목한다; 우리는 비교에서 기본 모델 _Mistral-7B v0.1_가 이 작업에 어떻게 수행되었는지에 기인한다.\n' +
      '\n' +
      '규모 증류가 효과적입니다. 스케일 증류를 도입하면 TruthfulQA 및 GSM8K 점수가 기준선 미세 조정에서 1-2점 이내로 증가하고 일반적으로 MT-벤치 점수가 0.1-0.2점 이내로 증가한다.\n' +
      '\n' +
      '사례 연구.표 3의 _Zephyr-7B-\\(\\beta\\)_의 샘플 반응을 제시하여 스케일 증류의 효능을 강조한다. BitDelta-Initial은 캐주얼한 톤을 갖지 않으며, 단어 제한을 고수하려는 시도를 하지 않는다. 스케일 증류의 도입으로 BitDelta는 더 큰 지시 추종 능력을 나타내어 단어 한계를 약간 초과하는 눈에 띄는 반응을 생성한다.\n' +
      '\n' +
      '양자화된 기본 모델.8비트 RTN과 GPTQ는 16비트 활성화로 작동하기 때문에 미세 조정 가중치\\(W_{\\text{fine}}\\)와 스케일링 인자\\(\\alpha\\)을 고정밀도로 유지할 수 있으며, 기본 가중치\\(W_{\\text{base}}\\)만을 양자화할 수 있다. 표 4에서 볼 수 있듯이, 우리는 BitDelta가 양자화된 기본 모델에 적용될 때 여전히 수행적이라는 것을 발견했다.\n' +
      '\n' +
      '우리는 FP16 + \\(\\Delta\\)이 전반적으로 GPTQ보다 우수하다는 것을 관찰한다. 다중 테넌시 서비스의 성능 엔지니어링 맥락에서 더 많은 모델을 추가함에 따라 많은 양자화된 미세 조정 모델을 저장하는 것보다 많은 1비트 델타로 단일 고정밀 기본 모델을 저장하는 것이 좋다. 이 흥미로운 결과는 위의 내용이 다중 테넌시 서빙의 _모델 품질_ 컨텍스트에서도 참임을 의미한다.\n' +
      '\n' +
      '우리는 기본 모델과 미세 조정 모델 모두 _Llama 2-7B Chat_를 사용하여 시도하며, GPTQ를 사용하여 기본 모델을 양자화하고, 많은 평가에서 기준 GPTQ를 능가할 수 있음을 발견했다. 우리는 1비트 델타를 포함하는 비용으로 고정밀 스케일링 인자를 통해 16비트 정보를 모델에 확산시킬 수 있기 때문이라고 가정한다.\n' +
      '\n' +
      'Ablation over fidelity \\(\\Delta\\). BitDelta를 연속적으로 적용하여, 이전 반복으로부터 압축된 모델을 기본 모델로 취급함으로써, 델타 상의 입도를 여러 개의 1비트 마스크와 연관시킬 수 있다. 이를 수행하는 한 가지 장점은 각 1비트 마스크에 임의의 스케일 팩터를 할당할 수 있다는 것이다. 대조적으로, 비트 크기를 증가시킬 때, 스케일 팩터들은 서로에 대해 암묵적으로 고정된다. 그림 3은 _Llama 2-7B_의 TruthfulQA와 점점 더 세분화된 델타가 _Vicuna-7B v1.5_의 TruthfulQA에 접근하는 방법을 보여준다. 전체 결과는 표 8에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline Base Model & Method & TruthfulQA & GSM8K & MT-Bench & Adjusted Average\\(\\dagger\\)\\(\\uparrow\\) \\\\ \\hline \\multirow{3}{*}{Baseline} & FP16 & 45.32 & 22.74 & 6.56 & 59.81 \\\\  & INT8 RTN & 45.02 & 22.29 & 6.28 & 59.63 \\\\  & GPTQ & 44.92 & 19.48 & 5.90 & 58.67 \\\\ \\hline \\multirow{3}{*}{_Llama 2-7B_} & FP16 + \\(\\Delta\\) & 44.95 & 20.24 & 6.47 & 59.88 \\\\  & INT8 RTN + \\(\\Delta\\) & 44.71 & 19.86 & 6.16 & 59.85 \\\\ \\cline{1-1}  & GPTQ + \\(\\Delta\\) & 42.52 & 19.94 & 6.02 & 59.22 \\\\ \\hline \\multirow{3}{*}{_Llama 2-7B Chat_} & GPTQ + \\(\\Delta\\) & 44.63 & 22.14 & 6.11 & 59.17 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: BitDelta를 _Llama 2-7B Chat_에 적용하고, 기저 기저 모델이 다양한 레벨에서 양자화될 때 이를 유지한다. FP16 + \\(\\Delta\\)는 전반적으로 기준선 GPTQ를 능가하며, 이는 모델 품질 측면에서 많은 양자화된 미세 조정 모델을 저장하는 것보다 많은 1비트 델타로 단일 고정밀 기본 모델을 저장하는 것이 낫다는 것을 의미한다. GPTQ + \\(\\Delta\\)_Llama 2-7B Chat_를 기본 모델로 하는 GPTQ는 많은 평가에서 _baseline_ GPTQ보다 우수하며, 이는 델타가 고정밀 스케일링 인자를 통해 16비트 정보를 확산시키기 때문이다.\n' +
      '\n' +
      '그림 3: \\(\\Delta\\)의 충실도가 증가함에 따라 _Llama 2-7B_ + \\(\\Delta\\)의 TruthfulQA 점수는 _Vicuna-7B v1.5_에 접근한다.\n' +
      '\n' +
      '### Latency Improvement\n' +
      '\n' +
      '메모리 절약을 개선된 지연 시간으로 변환하는 아이디어를 설명하기 위해 비트 델타에서와 같이 이진 행렬과 스케일링 팩터를 사용하여 GEMM을 위한 간단한 트리톤 커널을 구현한다. 우리는 같은 GPU에서 제공되는 \\(B\\) 미세 조정 모델이 있다고 가정한다. 단순화를 위해 요청이 균일하게 분포되는 설정, 즉 각 모델이 하나의 요청을 동시에 수신하는 설정을 고려한다. 식에서의 분해 후입니다. (6) 커널은 \\(B\\) 이진 행렬(\\(N\\times M\\))과 \\(B\\) 고정밀 활성화(\\(L\\times N\\)) 사이의 배치 행렬 곱셈을 계산하는 데 사용되며, 여기서 \\(N,M\\)은 중간 차원이고 \\(L\\)은 서열 길이이다. 대부분의 시간을 소비하기 때문에 프리필 레이턴시와 반대로 디코딩 레이턴시에 중점을 둡니다. 토큰은 디코딩에서 하나씩 생성되며, 이는 \\(L\\)이 항상 1임을 의미한다.\n' +
      '\n' +
      '커널 레이턴시.우리는 커널의 디코딩 레이턴시를 벤치마킹한다. 우리는 Eq에서와 같이 단일 기본 모델을 사용하여 다중 델타에서 배치 선형 연산이다. 단일 선형 계층에 해당하는 (6)과 각 모델에 대해 개별적으로 순방향 패스를 순진하게 계산하는 것과 비교한다. 우리는 배치 크기와 숨겨진 크기 차원을 축소하고 커널이 약 2\\(\\times\\)의 속도 향상을 일관되게 달성한다는 것을 발견한다.\n' +
      '\n' +
      '또한 입력 길이가 128인 _Llama 2-7B_ 변이체에 대한 종단간 디코딩 레이턴시를 벤치마킹하여 배치 크기에 따라 제거되었다. 비트델타는 배치 크기가 작을 때 오버헤드가 발생함을 알 수 있다. 그러나 BitDelta는 \\(B=4\\)에서 시작하여 저장된 GPU 메모리를 더 잘 스케일링하고 성공적으로 디코딩 레이턴시로 변환한다. 이것은 더 높은 배치 크기에서 악화되며, 여기서 순진한 접근법은 메모리 외 문제에 굴복하고 BitDelta는 여전히 수행적이다.\n' +
      '\n' +
      '##5 결론 및 논의\n' +
      '\n' +
      '본 논문에서는 LLM의 미세 조정에서 발생하는 가중치 델타를 1비트로 효율적으로 양자화하기 위한 간단하면서도 효과적인 방법인 BitDelta를 제안한다. BitDelta는 증류를 통해 추가로 보정된 중량 델타 및 중량당 매트릭스 스케일링 인자의 부호 비트를 인코딩한다. 이를 통해 하나의 기본 모델과 여러 개의 1비트 델타로 여러 개의 풀 파라미터 미세 조정 모델을 표현할 수 있어 GPU 메모리 요구 사항을 줄이고 생성 대기 시간을 개선하여 멀티 테넌시 서비스에서의 응용 프로그램을 향상시킬 수 있다. BitDelta는 빠르고 정확하며, 성능 저하를 최소화하며, 머신 러닝에서 효율적인 모델 배치 및 자원 활용을 위한 새로운 길을 열어준다.\n' +
      '\n' +
      '향후 연구.우리는 대부분의 양자화 방법이 어려움을 겪는 주요 과제이기 때문에 주로 BitDelta의 정성 평가에 중점을 둔다. 본 논문에서는 BitDelta가 기존의 미세 조정 모델들의 품질을 유지할 수 있음을 보이고, 퓨전 커널을 사용하여 메모리 절약이 향상된 지연 시간으로 변환되는 방법을 설명한다. 그러나, 이진 행렬의 작은 메모리 풋프린트를 고려할 때, 우리의 커널은 이론적인 한계에 비해 상당히 느리다. 또한 두드러진 무게 델타를 보존하여 더 나은 모델 품질을 달성하는 것이 가능할 수 있다. 마지막으로 증류를 통해 특정 스케일 인자를 보정하는 아이디어는 PTQ 방법에 더 일반적으로 적용될 수 있다. 우리는 우리의 작업이 저비트 양자화된 LLM을 더 견고하게 만들기 위해 스케일 인자의 향후 개선에 동기를 부여하기를 바란다.\n' +
      '\n' +
      '도 4: BitDelta가 있거나 없는 선형 계층의 디코딩 레이턴시. 파란색: \\(B\\) 뚜렷한 미세 조정 모델이 있는 나이브 포워드 패스입니다. 노란색: 하나의 기본 모델에 해당하는 비트 델타와 트리톤 커널을 사용하는 \\(B\\)1비트 델타로 정방향 패스를 결합했다. 왼쪽: \\(N=M\\)과 \\(B=8\\)이라고 가정할 때 숨겨진 크기에 대한 절제. 오른쪽: \\(N=M=8192\\)이라고 가정할 때 배치 크기 이상의 절제.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Base Model & Size & \\(\\Delta\\) Size & Comp. Factor \\\\ \\hline _Llama 2-7B_ & 13.48 GB & 1.24 GB & 10.87 \\\\ _Llama 2-13B_ & 26.03 GB & 2.09 GB & 12.45 \\\\ _Llama 2-70B_ & 137.95 GB & 8.95 GB & 15.41 \\\\ _Mistral-7B v0.1_ & 14.48 GB & 1.30 GB & 11.14 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: BitDelta는 10\\(\\times\\) 이상의 압축을 달성한다. 임베딩 및 LM 헤드 레이어를 더 압축할 수 있지만, 토큰화기 어휘의 불일치로 인해 향후 작업에 맡길 수 있다.\n' +
      '\n' +
      '도 5: BitDelta가 있거나 없는 _Llama 2-7B_ 변형들의 종단간 디코딩 레이턴시. 파란색: \\(B\\) 뚜렷한 미세 조정 모델이 있는 나이브 포워드 패스입니다. 오렌지: 순진한 전진 패스에 대해 투영된 값입니다. 그린: 비트델타와 정면 패스를 맞췄다. 순진한 순방향 패스는 더 높은 배치 크기에서 GPU 메모리 문제에 굴복하는 반면 BitDelta는 여전히 수행적이다.\n' +
      '\n' +
      '## 6 충격성명\n' +
      '\n' +
      '환경 지속 가능성 및 비용 절감 비트델타를 통한 GPU 메모리 요구 사항의 감소는 에너지 소비를 낮추고 여러 미세 조정 모델을 제공하는 것과 관련된 비용을 절감하는 것으로 해석된다. 비트델타는 하드웨어 요구 사항을 대폭 낮추어 AI 기술을 보다 친환경적이고 경제적으로 실행 가능하게 만드는 데 기여한다. 이 접근법은 대규모 AI 기술의 배치에서 지속 가능하고 비용 효율적인 컴퓨팅 솔루션에 대한 필요성이 증가하고 있는 것과 일치한다.\n' +
      '\n' +
      '미세 조정 모델의 민주화.미세 조정 모델을 제공하기 위한 하드웨어 요구 사항을 극적으로 감소시킴으로써 BitDelta는 더 작은 엔티티가 최첨단 모델을 더 실행 가능하게 배치할 수 있게 한다. 이는 다양한 산업 및 학문 분야에 걸친 혁신 및 애플리케이션 개발을 가속화하여 미세 조정된 모델이 더 많은 청중에게 접근할 수 있도록 할 수 있다.\n' +
      '\n' +
      'Dealignment Mitigation.BitDelta는 LLM의 미세 조정 정보에 대한 손실 압축 방법이다. 이와 같이, 압축 과정에서 중요한 정렬 정보가 손실될 수 있다. 비트델타가 이러한 딜런션 문제를 악화시킬 수 있는 다중 테넌트 애플리케이션을 민주화하기 때문에 이것이 강조할 중요한 결과라고 믿는다. 우리는 비트델타의 정렬 손실을 감지하기 위한 평가 기술에 대한 추가 작업을 권장하며, 이는 완화를 위한 강력한 방법을 생성할 수 있다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '이번 연구를 지원해준 AI, 마이셸 AI, 국립과학재단(NSF), MIT-IBM 왓슨 AI랩, MIT 아마존 사이언스 허브 등에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* E. Beeching, C. Fourrier, N. 하빕 한남 N. 램버트 오자니 산세비에로 툰스톨, T Wolf(2023) 오픈 llm 리더보드. 참고: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) Cited by: SS1.\n' +
      '*T. 채영 이종욱 Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao(2024)Medusa: 다중 디코딩 헤드를 갖는 간단한 llm 추론 가속 프레임워크. ArXiv: 2401.10774. 인용: SS1.\n' +
      '*J. Chee, Y. 카이비 Kuleshov, and C. De Sa(2023)Quip: 2-bit quantization of large language models with guarantee. ArXiv:2307.13304. 인용: SS1.\n' +
      '*C. 첸, S. 보르헤오, G. 어빙, J. 레스파우, L. Sifre, and J. Jumper (2023) Accelerating large language model decoding with speculation sampling. 2023년 2월 doi: 10.48550/ARXIV.2302.01318. 인용: SS1.\n' +
      '* G. Chen, F. Liu, Z. 멍, S. Liang(2022) 파라미터-효율 튜닝을 다시 하는 것: 우리는 정말 아직 거기에 있나요? 인용: SS1.\n' +
      '* L. 천진 예영 우동주 Ceze, and A. Krishnamurthy (2023)Punica: 멀티 테넌트 로라 서빙. 외부 링크: 2023b. 인용: SS1.\n' +
      '* S. 천성호 왕락 천영 Tian (2023)Extending context window of large language models via positional interpolation. 외부 링크: 2307.13304 인용: SS1.\n' +
      '*Y. 천성호 기안홍당 라이자 류승 Han, and J. Jia (2023)Longlora: long-context large language models의 효율적인 미세 조정. 외부 링크: 2307.13304 인용: SS1.\n' +
      '*W. 장종 이종욱 임영식 성진 우현장 정승 장영 Zohuang, J. E. Gonzalez, I. Stoica, and E. P. Xing(2023)Vicuna: 90%* 채팅 품질로 gpt-4를 인상하는 오픈 소스 챗봇. 외부 링크: 2307.13304 인용: SS1.\n' +
      '* P. Christiano, J. Leike, T. B. Brown, M. 마틱 Legg, and D. Amodei (2023) Deep reinforcement learning from human preferences. 외부 링크: 2307.13304 인용: SS1.\n' +
      '* P. Clark, I. Cowhey, O. 에치오니, T. Khot, A. Sabharwal, C. Schoenick, O. 타피오르(2018) 질의응답을 해결했다고 생각하십니까? try arc, the ai2 reasoning challenge. 외부 링크: 인용된 1803.06615: SS1.\n' +
      '*K. 코비, V 고사라주 바이에른 M. 천현준 카이저 플래퍼트, J 트워렉, J 힐튼, R. Nakano, et al.(2021)Training verifiers to solve math word problems. ArXiv:2110.14168. 인용: SS1.\n' +
      '*T. 디트머스 루이스 벨카다, L. 제틀모이어(2022) Llm. int8(0): 스케일에서 변압기에 대한 8비트 행렬 곱셈. ArXiv:2208.07339. 인용: SS1.\n' +
      '*T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer (2023)Qlora: 양자화된 llms의 효율적인 미세조정. ArXiv:2305.14314. 인용: SS1.\n' +
      '* J. 데블린, M. 장경 이경호 Toutanova (2019)Bert: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. In Proceedings of the 2019 Conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pp. 4171-4186. Cited by: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformer. _ ArXiv:2210.17323_, 2022.\n' +
      '* Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. URL[https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)\n' +
      '* Han et al. (2015) Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks, 2015.\n' +
      '* Han et al. (2016) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016.\n' +
      '* Hartford (2023) Hartford, E. Cognitivecomputations/dolphin-2.2.1-mistral-7b, hugging face, 2023. URL[https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b](https://huggingface.co/cognitivecomputations/dolphin-2.2.1-mistral-7b].\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. nlp를 위한 파라미터 효율적인 전이 학습. In _International Conference on Machine Learning_, pp. 2790-2799. PMLR, 2019a.\n' +
      '* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. nlp, 2019b를 위한 파라미터 효율적인 전이 학습.\n' +
      '* Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. Lora: 대형 언어 모델의 낮은 랭크 적응. _ ICLR_, 2021.\n' +
      '* Isik et al. (2023) Isik, B., Kumbong, H., Ning, W., Yao, X., Koyejo, S., and Zhang, C. GPT-zip: Deep compression of finetuned large language models. In _Workshop on Efficient Systems for Foundation Models @ ICML2023_, 2023. URL[https://openreview.net/forum?id=h00C2tG2xL](https://openreview.net/forum?id=h00C2tG2xL).\n' +
      '* Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M. - A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023.\n' +
      '* Jin et al. (2023) Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Dataless knowledge fusion by merging weights of language models, 2023.\n' +
      '* Kim et al. (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Dense-and-sparse 양자화. _ arXiv preprint arXiv:2306.07629_, 2023.\n' +
      '* Kingma & Ba (2017) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017.\n' +
      '* LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. 최적의 뇌 손상 Touretzky, D. (ed.), _Advances in Neural Information Processing Systems_, volume 2. Morgan-Kaufmann, 1989. URL [https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf]).\n' +
      '* Leviathan et al. (2022) Leviathan, Y., Kalman, M., and Matias, Y. 추측 디코딩을 통한 변압기로부터의 빠른 추론. 2022년 11월 10.48550/ARXIV.2211.17192\n' +
      '* Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: llm 압축 및 가속을 위한 활성화 인식 가중치 양자화. _ arXiv preprint arXiv:2306.00978_, 2023.\n' +
      '* Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. 진실: 모델이 인간의 거짓을 어떻게 모방하는지 측정하는 것, 2022년.\n' +
      '* Mishra et al. (2021) Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural networks. _ arXiv preprint arXiv: 2104.08378_, 2021.\n' +
      '* Mukherjee et al. (2023) Mukherjee, S., Mitra, A., Jawahar, G., Agarwal, S., Palangi, H., and Awadallah, A. Orca: Progressive learning from complex explanation trace of gpt-4, 2023.\n' +
      '* Niederfahrenhorst et al. (2023) Niederfahrenhorst, A., Hakhamaneshi, K., and Ahmad, R. 미세 조정 llms: llama-2를 사용한 심층 분석, 9월 2023. URL[https://www.anyscale.com/blog/fine-tuning-lms-lora-or-full-parameter-with-llama-2](https://www.anyscale.com/blog/fine-tuning-lms-lora-or-full-parameter-with-llama-2]).\n' +
      '* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. training language models to follow instructions with human feedback. _ ArXiv:2203.02155_, 2022.\n' +
      '* Paperno et al. (2016) Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. 람다 데이터세트: 광범위한 담화 맥락을 필요로 하는 단어 예측, 2016.\n' +
      '* Press et al. (2022) Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear bias enables input length extrapolation, 2022.\n' +
      '\n' +
      'Qiu, J., Li, L., Sun, J., Peng, J., Shi, P., Zhang, R., Dong, Y., Lam, K., Lo, F. P.-W., Xiao, B., Yuan, W., Wang, N., Xu, D., and Lo, B. Large ai models in health informationatics: Applications, challenges and future. _ IEEE Journal of Biomedical and Health Informatics_, 27(12):6074-6087, 2023. doi: 10.1109/JBHI.2023.3316750.\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Generative Preraining에 의한 언어 이해력 향상. 2018년\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. _ OpenAI blog_, 1(8):9, 2019.\n' +
      '* Raffel et al. (2023) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J., Exploring the limit of transfer learning with a unified text-to-text transformer, 2023.\n' +
      '* Rebuffi et al. (2017) Rebuffi, S. - A., Bilen, H., and Vedaldi, A. Learning multiple visual domain with residual adapters. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Sakaguchi et al. (2019) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: Anversarial winograd schema challenge at scale, 2019.\n' +
      '* Sheng et al. (2023) Sheng, Y., Cao, S., Li, D., Hooper, C., Lee, N., Yang, S., Chou, C., Zhu, B., Zheng, L., Keutzer, K., Gonzalez, J. E., and Stoica, I. S-Iora: Serving thousands of concurrent lora adapters. _ arXiv preprint arXiv:2311.03285_, 2023.\n' +
      '* Suzgun et al. (2022) Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challengeing big-bench tasks and whether the chain-of-thought can solve them. _ arXiv preprint arXiv:2210.09261_, 2022.\n' +
      '*팀(2023)팀, X. - L. Xwin-lm, 9 2023. URL[https://github.com/Xwin-LM/Xwin-LM](https://github.com/Xwin-LM/Xwin-LM).\n' +
      '* Tillet et al. (2019) Tillet, P., Kung, H.-T., and Cox, D. D. Triton: intermediate language and compiler for tiled neural network computationations. _ 제3차 ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, 2019. URL[https://api.semanticscholar.org/CorpusID:184488182](https://api.semanticscholar.org/CorpusID:184488182)의 진행.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Tunstall et al. (2023) Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., and Wolf, T. 제퍼: lm 정렬의 직접 증류, 2023.\n' +
      '* Upstage(2023) Upstage/solar-0-70b-16bit. hugging face, 2023. URL[https://huggingface.co/upstage/](https://huggingface.co/upstage/) SOLAR-0-70b-16bit.\n' +
      '* Wang et al. (2023) Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. 오픈챗: 혼합 품질 데이터가 있는 오픈 소스 언어 모델 발전, 2023.\n' +
      '* Wortsman et al. (2022) Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., and Schmidt, L. 모델 스프: 다중 미세 조정 모델의 가중치를 평균하면 추론 시간이 증가하지 않고 정확도가 향상된다.\n' +
      '* Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: 대형 언어 모델을 위한 정확하고 효율적인 훈련 후 양자화. In _International Conference on Machine Learning_, pp. 38087-38099. PMLR, 2023.\n' +
      '* Xu et al. (2024) Xu, M., Du, H., Niyato, D., Kang, J., Xiong, Z., Mao, S., Han, Z., Jamalipour, A., Kim, D. I., Shen, X., Leung, V. C. M., and Poor, H. V. Unleashing of edge-cloud generative ai in mobile networks: A survey of aigc services. _ IEEE Communications Surveys & Tutorial_, pp. 1-1, 2024. doi: 10.1109/COMST.2024.3353265.\n' +
      '* Yao & Klimovic (2023) Yao, X. and Klimovic, A. Deltazip: Multi-tenant language model serving via delta compression. _ arXiv preprint arXiv:2312.05215_, 2023.\n' +
      '* Yu et al. (2023) Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. 언어 모델은 슈퍼 마리오입니다: 무료 점심 식사로 동종 모델의 능력을 흡수합니다. _ arXiv preprint arXiv:2311.03099_, 2023.\n' +
      '* Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. 헬라스바그: 기계가 정말로 당신의 문장을 끝낼 수 있나요? 2019년.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W. - L., Sheng, Y., Zhu, S., Wu, Z., Zang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-juding with mt-bench and chatbot arena, 2023.\n' +
      '* Zhu & Gupta (2017) Zhu, M. 및 굽타, S. 가지치기를 하거나 가지치기를 하지 않는 방법: 모델 압축을 위한 가지치기의 효능을 탐구합니다. _ International Conference on Learning Representations_, 2017.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:12]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>