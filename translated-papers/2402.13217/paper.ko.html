<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VideoPrism: 비디오 이해를 위한 기초 시각 인코더\n' +
      '\n' +
      ' 장자오\n' +
      '\n' +
      '근다바라푸 니테시\n' +
      '\n' +
      'Liangzhe Yuan\n' +
      '\n' +
      'Hao Zhou\n' +
      '\n' +
      'Shen Yan\n' +
      '\n' +
      '제니퍼 J. 선\n' +
      '\n' +
      'Luke Friedman\n' +
      '\n' +
      'Rui Qian\n' +
      '\n' +
      'Tobias Weyand\n' +
      '\n' +
      'Yue Zhao\n' +
      '\n' +
      'Rachel Hornung\n' +
      '\n' +
      'Florian Schroff\n' +
      '\n' +
      'Ming-Hsuan Yang\n' +
      '\n' +
      '데이비드 A. 로스\n' +
      '\n' +
      'Huisheng Wang\n' +
      '\n' +
      'Hartwig Adam\n' +
      '\n' +
      'Mikhail Sirotenko\n' +
      '\n' +
      'Ting Liu\n' +
      '\n' +
      'Boqing Gong\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      '1차 기여도 같고요 핵심 기술 기여도 동일합니다. 동일한 선임 기여, 프로젝트 선도. (<\\)longzh@google.com\\(>\\), Mikhail Sirotenko \\(<\\)msirotenko@google.com\\(>\\), Ting Liu \\(<\\)liuti@google.com\\(>\\), Boqing Gong \\(<\\)bgong@google.com\\(>\\)에 해당한다.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '단일 냉동 모델로 다양한 비디오 이해 작업을 처리하는 범용 비디오 인코더인 VideoPrism을 소개합니다. 우리는 36M 고품질 비디오 캡션 쌍과 잡음이 있는 병렬 텍스트(예: ASR 전사체)가 있는 582M 비디오 클립을 포함하는 이질적인 말뭉치에서 비디오 프리즘을 사전 훈련한다. 사전 훈련 접근법은 시맨틱 비디오 임베딩의 전역-로컬 증류 및 토큰 셔플링 방식에 의해 마스킹된 자동 인코딩 시 개선되어 VideoPrism이 비디오와 관련된 귀중한 텍스트를 활용하면서 비디오 촬영 방식에 주로 집중할 수 있게 한다. 우리는 웹 비디오 질문 응답부터 과학용 CV까지 4개의 광범위한 비디오 이해 태스크 그룹에 대해 비디오 프리즘을 광범위하게 테스트하여 33개의 비디오 이해 벤치마크 중 30개에서 최첨단 성능을 달성했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '비디오는 일상 생활에서 과학적 관찰에 이르기까지 다양한 영역에 걸쳐 있는 현실 세계의 지각적 경험의 풍부하고 역동적인 아카이브이다. 비디오 기반 모델(ViFM)은 이 방대한 말뭉치 내에서 새로운 통찰력을 풀 수 있는 엄청난 잠재력을 가지고 있다. 이전 작업은 일반적인 비디오 이해에 큰 진전을 이루었지만(Xu et al., 2021; Wang et al., 2022; Yan et al., 2022; Tong et al., 2022; Li et al., 2023; Wang et al., 2023c), 진정한 기반 비디오 모델을 구축하는 것은 여전히 파악하기 어려운 목표이다. 기존의 모델들은 종종 많은 벤치마크들(Yuan et al., 2023)에 걸쳐 태스크-전문화된 모델들보다 뒤처지는 모션-중심 추론과 함께 외관-무거운 태스크들의 균형을 맞추기 위해 고군분투한다.\n' +
      '\n' +
      '우리는 분류, 현지화, 검색, 캡션 및 질의 응답(QA)을 포함한 광범위한 비디오 이해 작업을 해결하기 위해 설계된 범용 비디오 인코더인 VideoPrism을 소개한다(그림 1). 신경과학 및 생태학과 같은 과학 영역에 대한 컴퓨터 비전(CV) 데이터 세트 및 CV에 대해 광범위하게 평가된 VideoPrism은 _single frozen_ 모델을 사용하여 최소한의 적응으로 최첨단 성능을 달성한다. 우리는 선행 작업(Radford et al., 2021; Alayrac et al., 2022; Tang et al., 2023; Li et al., 2023a)에 후속하는 이러한 동결-인코더 설정을 강조하고, 그렇지 않으면 미세조정 비디오 모델의 높은 계산 및 메모리 비용을 고려할 때 그 실용성을 강조한다.\n' +
      '\n' +
      'VideoPrism 이면의 디자인 철학은 다음과 같다. 프리트레이닝 데이터는 기초 모델(FM)(Bommasani et al., 2021)의 기본이며, ViFM에 대한 이상적인 프리트레이닝 데이터는 전 세계 모든 비디오의 대표적인 샘플일 것이다. 이 샘플로부터의 대부분의 비디오들은 콘텐츠를 기술하는 (또는 매우 시끄러운) 병렬 텍스트가 없을 것이다; 그러나, 그러한 텍스트가 존재할 때, 비디오 공간에 대한 값비싼 의미적 단서들을 제공한다. 따라서 사전 훈련 전략은 주로 비디오 촬영 방식에 초점을 맞추어야 하지만 사용 가능한 비디오 텍스트 쌍을 최대한 활용해야 한다.\n' +
      '\n' +
      '데이터 측면에서는 36M 고품질 비디오 캡션 쌍과 582M 비디오 클립을 노이즈 병렬 텍스트(예: ASR 전사체, 생성된 캡션 및 검색된 텍스트)로 조립하여 원하는 사전 훈련 코퍼스를 근사화한다. 모델링 측면에서, 먼저 다양한 품질의 모든 비디오-텍스트 쌍으로부터 시맨틱 비디오 임베딩(Radford et al., 2021; Jia et al., 2021)을 대조적으로 학습한다. 이어서, 마스킹된 비디오 모델링(Tong et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023c) 시맨틱 임베딩을 전역적 및 토큰적으로 증류함으로써 광범위한 비디오 전용 데이터를 활용한다.\n' +
      '\n' +
      '자연어에 대한 성공에도 불구하고(Devlin et al., 2019; Brown et al., 2020; Anil et al., 2023), 원시 시각 신호가 의미론이 부족하기 때문에 마스킹된 데이터 모델링은 CV에 대해 여전히 도전적이다. 기존 작업들은 간접 의미론(_e.g._, CLIP(Radford et al., 2021)을 사용하여 부트스트랩 모델들(Fang et al., 2022; 2023) 또는 토큰라이저들(Peng et al., 2022))을 차용하거나, 이를 암시적으로 홍보(_e.g._, 토큰타이징 비주얼 패치들(Zhou et al., 2022; Bao et al., 2022; Oquab et al., 2023), 높은 마스킹 비율과 경량 디코더(He et al., 2022))를 결합함으로써 이러한 도전을 접근한다.\n' +
      '\n' +
      '우리는 사전 훈련 데이터에 맞춘 2단계 접근법으로 위의 아이디어를 기반으로 한다. 우리는 먼저 대조 목적(Gutmann and Hyvarinen, 2010; Radford et al., 2021)을 사용하여 비디오-텍스트 쌍에 대해 쌍을 이루는 텍스트 인코더와 함께 비디오 인코더를 트레이닝한다. 다음으로 두 가지 개선점을 가지고 마스킹된 비디오 모델링에 의해 모든 비디오 전용 데이터에 대해 인코더를 계속 학습한다. (1) 마스킹되지 않은 입력 비디오 패치를 기반으로 첫 단계부터 비디오 레벨 전역 임베딩과 토큰별 임베딩을 모두 예측하기 위해 모델이 필요하며, (2) 바로가기 학습을 피하기 위해 인코더의 출력 토큰에 랜덤 셔플을 적용한다. 특히, 사전 훈련은 비디오의 텍스트 설명과 맥락적 자체 감독이라는 두 가지 감독 신호를 활용하여 비디오 프리즘이 외모와 모션에 초점을 맞춘 작업 모두에서 탁월할 수 있도록 한다. 실제로, 이전 작품들은 비디오 캡션이 주로 출현 큐들을 드러내고(Wang et al., 2023f), 맥락적 자기-감독은 학습 동작을 용이하게 한다는 것을 보여주었다(Tong et al., 2022).\n' +
      '\n' +
      '**Contributions.** VideoPrism은 최첨단 범용 비디오 인코더이다. 우리는 수동으로 캡션된 비디오와 잡음이 있는 텍스트 설명이 포함된 비디오를 결합하여 사전 훈련 비디오를 수집하기 위한 확장 가능한 전략을 옹호한다. 이 하이브리드 데이터에 맞춘 독특한 2단계 사전 훈련 접근법을 설계하여 비디오 언어 대조 학습을 활용하여 의미론을 채집한 다음 글로벌-로컬 증류 및 토큰 셔플링을 사용하여 개선된 마스킹 비디오 모델링을 사용한다. 마지막으로, 웹의 비디오, 스크립팅된 공연, 과학 실험을 포함한 33개의 다양한 벤치마크에 걸쳐 4가지 광범위한 이해 과제에 대한 포괄적인 평가를 제시한다. 결과는 VideoPrism이 30개의 벤치마크에서 기존 ViFM을 상당히 능가한다는 것을 보여준다(그림 2). 중요한 것은 단일 기준선 모델이 일관되게 두 번째로 우수한 성능을 달성하지 않아 VideoPrism의 강력한 일반화 가능성을 나타낸다.\n' +
      '\n' +
      '도 2 : **VideoPrism _vs._ 이전에 가장 성능이 좋은 FM** 부록 D에서 이 수치의 세부 정보를 찾으십시오.\n' +
      '\n' +
      '## 2 Approach\n' +
      '\n' +
      '### Pretraining data\n' +
      '\n' +
      '사전 훈련 데이터는 표 1에 요약된 바와 같이, 잡음 병렬 _text_를 갖는 고품질 수동 라벨링된 _captions_ 및 582M 클립(275M 비디오로부터 샘플링됨)을 갖는 36M 클립(36M 비디오로부터 샘플링됨)으로 구성된다. Anonymous-Corpus #1의 36M 고품질 비디오-캡션 쌍은 ViFM에 대해 가장 큰 종류이지만, 여전히 이미지 FM에 연료를 공급하기 위해 사용되는 이미지-언어 데이터보다 작은 크기의 순서이다(Radford et al., 2021; Yu et al., 2022). 따라서, 우리는 또한 ASR, 메타데이터 및 대형 멀티모달 모델(Wang et al., 2023; Zhao et al., 2024), _etc._ 이 비디오의 하위 집합은 표 1의 WTS-70M에서 익명-Corpus #3까지의 행에 해당하며 부록 A에 더 자세한 내용을 제공한다.\n' +
      '\n' +
      '중요하게는, 이전 작업들(Tong et al., 2022; Wang et al., 2022; Li et al., 2023; Wang et al., 2023)과 달리, 우리는 사전 훈련 또는 사후 사전 훈련 중 하나를 위해 평가 벤치마크들, _e.g._, Kinetics(Kay et al., 2017)로부터 임의의 훈련 세트들을 통합하지 않는다. 이러한 의식적인 선택은 특정 평가 벤치마크에 대한 모델을 과도하게 조정하는 것을 방지합니다. 또한, 본 논문에서 사용된 평가 벤치마크에서 비디오에 대한 사전 훈련 코퍼스를 신중하게 복제 해제하여 데이터 유출이 없는지 확인한다.\n' +
      '\n' +
      '### Model architecture\n' +
      '\n' +
      'VideoPrism 모델 아키텍처는 ViViT(Arnab et al., 2021)에 이어 시공간적으로 인수분해된 설계를 가진 표준 Vision Transformer(ViT)(도소비츠키 et al., 2021)에서 비롯된다. 그러나 공간 인코더 직후에 ViViT의 전역 평균 풀링 레이어를 제거하여 시공간 차원이 출력 토큰 시퀀스에 남아서 세밀한 특징(_e.g._, 시공간 액션 로컬리제이션)을 필요로 하는 다운스트림 작업을 용이하게 한다. 우리는 _VideoPrism-g_와 _VideoPrism-B_의 두 가지 모델 구성을 실험한다. VideoPrism-g는 공간 인코더에서 1B 파라미터를 갖는 ViT-거인 네트워크(Zhai et al., 2022)이고, VideoPrism-B는 ViT-Base 네트워크를 갖는 더 작은 변형이다(Dosovitskiy et al., 2021). 부록 B는 두 네트워크 아키텍처에 대해 자세히 설명한다.\n' +
      '\n' +
      '### Training algorithm\n' +
      '\n' +
      '우리의 목표는 비디오-텍스트 쌍과 섹션 2.1에서 큐레이션된 비디오 전용 데이터를 모두 활용하여 비디오 프리즘을 확장 가능하게 훈련하여 비디오 프리즘을 비디오에서 외관과 모션 의미 모두를 캡처할 수 있는 기본 비디오 인코더로 만드는 것이다. 우리는 대규모 사전 훈련 코퍼스의 텍스트가 일부 비디오에 대해 매우 시끄럽기 때문에 비디오 텍스트에만 의존하기보다는 비디오 전용 양식을 강조한다. 도 3에 도시된 바와 같이 VideoPrism의 트레이닝 파이프라인은 _video-text contrastive training_와 _masked video modeling_의 두 단계를 포함한다.\n' +
      '\n' +
      '1단계 : 영상-텍스트 대비 훈련\n' +
      '\n' +
      '첫 번째 단계에서는 모든 비디오-텍스트 쌍을 사용하여 비디오 인코더와 텍스트 인코더를 정렬하기 위해 대비 학습을 수행한다. 종래 기술들(Radford et al., 2021; Jia et al., 2021; Cheng et al., 2023)에 이어서, 우리는 미니-배치 내의 모든 비디오-텍스트 쌍들의 유사성 점수에 걸쳐 대칭 교차 엔트로피 손실을 최소화하고, CoCa의 이미지 모델을 사용하여 공간 인코딩 모듈들을 초기화하고(Yu et al., 2022), WebLI(Chen et al., 2023)( alt-텍스트를 갖는 약 1B 이미지들)를 사전 훈련에 포함한다. 비디오 인코더의 특징들은 손실 계산 전에 다중-헤드 어텐션 풀러(MAP)(Lee et al., 2019)를 통해 집성된다. 이 단계는 비디오 인코더가 언어 감독으로부터 풍부한 시각적 의미론을 학습할 수 있게 하고, 결과 모델은 2단계 훈련을 위해 의미론적 비디오 임베딩을 제공한다.\n' +
      '\n' +
      '2단계 : 마스킹 비디오 모델링\n' +
      '\n' +
      '스테이지 1에서와 같이 비전-텍스트 데이터에 대해서만 훈련하는 것은 과제를 제시한다: 텍스트 설명은 시끄러울 수 있고, 그들은 종종 움직임보다 외모를 더 포착한다(Hendricks and Nematzadeh, 2021; Momeni et al., 2023). 이를 해결하기 위해 2단계 교육은 비디오 전용 데이터에서 외관과 동작 정보를 모두 학습하는 데 중점을 둡니다. 모션 이해를 위한 마스킹된 오토인코딩의 성공(Wang et al., 2022; 2023)을 기반으로, 우리는 모델이 첫 번째 단계에서 획득된 의미적 지식을 유지하는 것을 보장하면서, 이 접근법을 두 번째 단계에 적용한다.\n' +
      '\n' +
      '이 단계에서는 개선된 마스킹 비디오 모델링을 사용하여 비디오 인코더를 비디오 전용 데이터에 대해 계속 트레이닝한다. 이러한 개선은 (1) 새로운 토큰 셔플링 방식으로 구성된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Pretraining datasets & Public & Domain & Caption source & Caption quality & \\# of videos & \\# of clips \\\\ \\hline Anonymous-Corpus \\#1 & ✗ & Web video & Manual labelled & High & 36.1M & 36.1M \\\\ \\hline WTS-70M (Stroud et al., 2020) & ✓ & YouTube video & Metadata & Low & 55.1M & 55.1M \\\\ VT-Temporal-180M (Zellers et al., 2021) & ✓ & YouTube video & ASR & Low & 2.3M & 87.8M \\\\ VideoCC (Nagrani et al., 2022) & ✗ & YouTube video & Image captions for mining & Low & 133.5M & 191.1M \\\\ Internetyl (Wang et al., 2023) & ✓ & YouTube video & Generated by VLMLM & Medium & 2.8M & 7.0M \\\\ Anonymous-Corpus \\#2 & ✗ & YouTube video & ASR & Low & 44.6M & 170.3M \\\\ Anonymous-Corpus \\#3 & ✗ & YouTube video & Generated by VLMLM & Medium & 36.7M & 71.5M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: **사전 훈련 코퍼스의 구성.**사전 훈련 중에 액세스할 수 있었던 비디오 및 클립의 수를 보고한다.\n' +
      '\n' +
      '디코딩 단축키 및 (2) 글로벌 및 토큰별 증류 손실을 방지하여 첫 번째 단계에서 획득한 지식을 효과적으로 활용합니다. 도 3에 예시된 바와 같이, 제2 단계(학생) 모델은 마스킹된 비디오에 기초하여 제1 단계(교사) 모델의 _all_ 토큰들의 임베딩을 예측하도록 학습한다. 인코더-디코더 트랜스포머는 He et al.(2022)의 설계에 따라 디커플링된다.\n' +
      '\n' +
      '**토큰 셔플링.**1단계부터 2단계 모델을 효과적으로 초기화하기 때문에, 한 가지 문제는 모델이 마스킹되지 않은 토큰만을 예측하면서 마스킹되지 않은 토큰을 복사하고 붙여넣기 위한 단축키를 생성할 수 있어 모든 토큰을 예측하는 것보다 해결하기 쉬운 작업이 될 수 있다는 것이다. 이 문제를 해결하기 위해 인코더가 출력하는 토큰 시퀀스를 디코더에 공급하기 전에 랜덤하게 셔플하고, 디코더는 셔플 후 이 시퀀스에 위치 임베딩을 추가한다. 이 셔플링 작업은 디코더가 잠재적으로 탐색할 수 있는 마스킹되지 않은 토큰의 복사 및 붙여넣기 바로 가기를 방지한다는 점에 유의한다. 또한 가면을 벗은 토큰을 예측하면서 노루지와 파바로(2016)의 직소 퍼즐과 유사하게 볼 수 있다.\n' +
      '\n' +
      '**Global-local distillation.** 이미지에 대한 마스킹된 증류와는 달리 Fang et al.(2022); (2023) 우리는 마스킹된 모델링 손실만이 사용될 때, 우리의 2단계 모델이 1단계 교사보다 외모-무거운 작업에서 더 우수하다는 것을 발견하며, 이는 아마도 2단계 사전 훈련에서 맥클로스키와 코헨(1989)을 파멸적으로 잊는 것에 기인할 것이다. 이 문제를 완화하기 위해 2단계 모델이 가시적인 토큰을 사용하여 1단계 교사로부터 전체 온전한 비디오의 전역 임베딩을 증류하도록 추가 손실을 추가한다. 따라서 2단계 학습 손실은 토큰 마스킹 비디오 모델링과 글로벌 증류를 결합한다. 공간 한계로 인해 자세한 구현 및 교육 구성은 독자에게 부록 C를 참조합니다.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      '우리는 비디오 프리즘의 기능과 일반화 가능성을 입증하기 위해 광범위한 비디오 중심 이해 작업에서 비디오 프리즘을 평가한다. 우리는 (1) 분류 및 시공간 위치화(섹션 3.1), (2) 제로샷 비디오 텍스트 검색(섹션 3.2), (3) 제로샷 비디오 캡션 및 QA(섹션 3.3), (4) 과학용 CV(섹션 3.4)의 네 가지 범주로 작업을 그룹화한다. 본 논문의 모든 실험을 위해 비디오 인코더로서 VideoPrism을 동결하고 그룹 (1), (2), (4) 및 (3)에 대해 VideoPrism을 LLM에 연결하는 일부 적응 계층에 대한 작업별 구성 요소만 훈련한다. 부록에서 우리는 엔드 투 엔드 및 어댑터 미세 조정에 대한 더 많은 결과를 보고한다. 비주얼 인코더를 동결하는 우리의 평가 전략은 이전 작업 He et al. (2022); Singh et al. (2022); Yuan et al. (2023) 및 VideoLLMs Tang et al. (2023)을 구축하기 위한 거의 유일한 선택이다. ViFM을 미세 조정하는 것은 엄청나게 비싼 반면 동결 ViFM은 여러 작업에 걸쳐 비디오 인코딩 비용을 상각할 수 있기 때문에 비디오에 특히 필요하다.\n' +
      '\n' +
      '### 분류 및 시공간적 지역화\n' +
      '\n' +
      '우리는 비디오 전용 이해 벤치마크: VideoGLUE Yuan et al.(2023)에서 VideoPrism과 최신 FMs를 비교한다. 디자인에 따라 VideoGLUE는 8개의 특징 데이터 세트에 걸쳐 4가지 적응 방법을 통해 FM을 평가하며, 이는 외관 중심 행동 인식(VC)을 나타낸다.\n' +
      '\n' +
      '그림 3: **2단계 사전 훈련의 그림.** 1단계는 비디오와 텍스트 인코더를 비디오-텍스트 쌍에 대비 손실로 훈련하여 다음 단계에 시맨틱 비디오 임베딩을 제공한다. 스테이지 2는 비디오 전용 클립에 개선된 마스킹된 오토인코딩을 사용하여 비디오 인코더를 계속 트레이닝한다. 인코더는 마스킹되지 않은 3D 비디오 패치를 사용하여 전체 비디오의 전역 시맨틱 임베딩 및 토큰-와이즈 임베딩을 예측한다. 디코더 2는 위치 임베딩으로 셔플된 토큰을 처리하는 반면, 디코더 1은 위치 임베딩이 없다.\n' +
      '\n' +
      '(A)), 모션이 풍부한 액션 인식(VC(M)), 멀티 라벨 비디오 분류(VC(ML)), 시간적 액션 로컬리제이션(TAL), 시공간 액션 로컬리제이션(STAL) 중 적어도 하나를 포함할 수 있다. 또한, 이 벤치마크는 비디오 전용 이해 작업에 대한 FM의 능력을 총체적으로 보기 위해 적응 비용과 성능 간의 균형을 고려한 비디오GLUE 점수(VGS)를 도입한다. 본 논문에서는 동결-백본 평가 결과를 제시하고 나머지는 부록 E에 남긴다. 동작 인식(MAP probing) 및 시공간적 국소화에 MAP 헤드(Yuan et al., 2023)를 사용하고, 시간적 국소화에 G-TAD(Xu et al., 2020)를 사용한다(자세한 내용은 부록 E.1 참조).\n' +
      '\n' +
      '**Datasets.** VideoGLUE 내 8개의 데이터셋은 다음과 같다. 외관 집중 액션 인식을 위해, Kinetics-400(K400)(Kay et al., 2017) 및 Moments-in-Time(MiT)(Monfort et al., 2019)은 웹 비디오로부터 소스된다. Something-Something v2(SSv2)(Goyal et al., 2017) 및 Diving48(D48)(Li et al., 2018)은 세립 모션이 풍부한 액션 인식 데이터 세트이다. 또한, Charades(Sigurdsson et al., 2016)는 스크립팅된 실내 비디오들을 이용하여 멀티-라벨 분류 문제를 제공한다. 시간적 국소화 작업은 하나의 데이터세트, ActivityNet v1.3(Caba Heilbron et al., 2015)을 수반하며, 시공간적 국소화는 ARA(Atomic Visual Actions)(Gu et al., 2018) 및 AVA-Kinetics(AVA-K)(Li et al., 2020)를 포함한다.\n' +
      '\n' +
      '**주요 결과.** 표 2는 VideoGLUE에 대한 동결-백본 결과를 나타낸다. VideoPrism은 모든 데이터 세트에서 기준선을 크게 능가합니다. 또한 VideoPrism의 기본 모델 크기를 ViT-B에서 ViT-g로 증가시키면 성능이 크게 향상된다. 특히, 어떤 기준도 모든 벤치마크에서 2위를 수행할 수 없으며, 이는 이전 방법이 비디오 이해의 특정 측면을 향해 개발될 수 있는 반면 VideoPrism은 이러한 광범위한 작업에서 일관되게 개선됨을 나타낸다. 이 결과는 VideoPrism이 다양한 비디오 신호를 하나의 인코더로 패킹했음을 의미한다: 다중 입도의 의미, 외관 _vs._ motion cues, 시공간 정보 및 다양한 비디오 소스에 대한 강건성(_e.g._, 웹 비디오 _vs._ scripted performance).\n' +
      '\n' +
      '부록 E.3에서는 VideoGLUE 설정에 따라 종단 간 및 매개변수 효율적인 미세 조정, 다층 주의 풀링을 포함한 다른 적응 방법에 대한 실험을 수행한다. 다양한 적응 방법은 계산 비용을 성능과 교환하고 실제 적용 고려 사항을 설명하며 VGS는 이를 스칼라 값으로 집계한다. VideoPrism은 VGS \\(51.25\\)을 달성하여 표 16의 모든 기준 FM을 능가하고 두 번째 베스트 모델(UMT)보다 높은 점수 \\(13.6\\%\\)을 달성했다.\n' +
      '\n' +
      '### Zero-shot 비디오-텍스트 검색 및 분류\n' +
      '\n' +
      'VideoPrism의 제로샷 비디오-텍스트 검색 및 비디오 분류 능력을 가능하게 하기 위해, LiT(Zhai et al., 2022b)를 따라 VideoPrism으로부터 그들의 대응하는 비디오 임베딩에 매칭된 텍스트 임베딩을 생성하는 텍스트 인코더를 학습한다. 우리는 1단계 훈련에서 LiT 텍스트 인코더를 선택하고 비디오 인코더에 MAP 헤드를 부착한다. LiT 튜닝은 첫 번째 단계에서부터 동일한 사전 훈련 데이터에 걸쳐 있다. 자세한 내용은 부록 F.1에 나와 있습니다.\n' +
      '\n' +
      '**Datasets.** We evaluate VideoPrism\'s zero-shot video-text retrieval performance on three benchmarks: MSRVTT(Xu et al., 2016), VATEX(Wang et al., 2019), ActivityNet(Caba Heilbron et al., 2015). 제로-샷 비디오 분류 태스크에 대해, Kinetics-400(Kay et al., 2017), Charades(Sigurdsson et al., 2016), SSv2-Temporal and SSv2-Events(Sevilla-Lara et al., 2021; Bagad et al., 2023), 및 NExT-QA의 ATP-Hard 서브세트(Buch et al., 2022)를 사용하여 실험한다. SSv2와 NExT-QA(ATP-Hard)는 각각 움직임과 시간적 추론에 초점을 맞춘다. 또한, 테스트 세트의 각 샘플을 다중 선택 검색 문제로 재구성하여 샤레이드-STA(Gao et al., 2017)를 제로 샷 분류 시나리오에 적용한다(자세한 내용은 부록 F.2 참조). 각 벤치마크에 대한 표준 평가 메트릭에 따른 결과를 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c|c|c c|c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{V (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} \\\\  & **K400** & **MiT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** \\\\ \\hline _Base-scale models_ & & & & & & & & \\\\ CLIP-B (Radford et al., 2021) & 75.2 & 32.6 & 41.0 & 44.1 & 11.2 & 32.7 & 21.1 & 25.9 \\\\ VATT-B (Akbari et al., 2021) & 75.1 & 32.1 & 57.8 & 49.7 & 33.3 & 35.3 & 20.3 & 22.2 \\\\ InterVideo-B (Wang et al., 2022c) & 69.3 & 26.3 & 58.2 & 55.6 & 13.0 & 33.3 & 13.4 & 15.7 \\\\ UMT-B (Li et al., 2023b) & 77.1 & 34.0 & 47.7 & 47.8 & 30.1 & 35.8 & 20.7 & 21.1 \\\\\n' +
      '**VideoPrism-B** & **84.2**(71.7) & **40.8**(76.8) & **63.6**(51.4) & **67.4**(112.2) & **40.4**(71.7) & **36.6**(70.8) & **30.6**(79.5) & **31.8**(15.9) \\\\ \\hline _Large-scale models_ & & & & & & & & \\\\ VideoMAE-v2.9 (Wang et al., 2023b) & 82.1 & 35.0 & 56.1 & 60.5 & 22.4 & 35.3 & 21.5 & 23.3 \\\\ Intern video-(Wang et al., 2022c) & 78.6 & 33.7 & 67.4 & 69.6 & 20.9 & 35.9 & 20.8 & 21.3 \\\\ UMT-L (Li et al., 2023b) & 82.8 & 40.3 & 54.5 & 49.0 & 39.9 & 36.7 & 24.4 & 26.2 \\\\\n' +
      '**VideoPrism-g** & **87.2**(14.4) & **45.5**(15.2) & **68.5**(11.1) & **71.3**(11.7) & **62.3**(122.2) & **37.8**(11.1) & **36.2**(122.2) & **37.3**(111.1) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 동결된 백본들을 갖는 VideoGLUE 벤치마크(Yuan et al., 2023) 상의 FM들을 평가하는 **Evalating FMs.** 태스크 헤드들 내의 가중치들만이 다운스트림 태스크들의 트레이닝 세트들을 사용하여 트레이닝된다. 샤레이드(Charades)를 제외한 모든 비디오 분류(VC) 작업에서 상위 1의 정확도를 보고한다. 샤레이드, 시간 국소화(TAL) 및 시공간 국소화(STAL) 작업에서 평균 평균 정밀도(mAP)를 평가 메트릭으로 사용한다. - B, -L, -g는 기본 모델이 각각 베이스, 대형 및 자이언트 ViT(Dosovitskiy et al., 2021)임을 나타낸다.\n' +
      '\n' +
      '**주요 결과.** 표 3 및 표 4는 각각 비디오-텍스트 검색 및 비디오 분류의 결과를 요약한 것이다. VideoPrism은 대부분의 벤치마크에서 새로운 기술의 상태를 설정하며, 선행 기술에 대한 이득은 ActivityNet에서 도전적인 데이터 세트(_예:_, \\(9.5\\%\\), SSV2-Events에서 \\(4.4\\%\\), Charades에서 \\(6.6\\) mAP)에서 예외적으로 상당하다. 기본 규모 VideoPrism-B의 대부분의 결과는 실제로 기존 대규모 모델의 결과보다 더 좋다. 또한, VideoPrism은 표 4의 도메인 내 데이터 및 추가 모달리티(_e.g._, 오디오)로 사전 훈련된 모델과 동등하거나 더 우수하다. 이러한 제로 샷 검색 및 분류 작업의 개선은 VideoPrism의 강력한 일반화 능력을 제시한다.\n' +
      '\n' +
      '### Zero-shot video captioning and QA\n' +
      '\n' +
      '우리는 생성 비디오-언어 태스크들, _i.e._, 캡션링 및 QA에 대한 VideoPrism의 고유한 능력들을 추가로 평가하는데, 여기서 우리는 VideoPrism을 언어 디코더, PaLM-2와 페어링한다(Anil et al., 2023). 두 모델을 연결하기 위해 VideoPrism과 언어 디코더를 모두 냉동 상태로 유지하면서 여러 접착 레이어를 소개하고 훈련한다. 그런 다음 비디오 캡션 및 QA 벤치마크에 대해 제로 샷 구성 하에서 평가를 수행한다. 캡셔닝 및 QA 작업을 위해 모델을 별도로 조정하지 않습니다. 구현 내용은 부록 G를 참조하시기 바랍니다.\n' +
      '\n' +
      '**Datasets.** 우리는 MSRVTT (Xu et al., 2017), VATEX (Wang et al., 2019), 및 YouCook2 (Zhou et al., 2018), 및 MSRVTT-QA (Xu et al., 2017), MSVD-QA (Xu et al., 2017), 및 NExT-QA (Xiao et al., 2021)를 포함하는 비디오 QA 벤치마크들의 세트의 테스트 분할들에 대한 제로-샷 설정에서 모델을 평가한다. 모델의 답변의 길이와 스타일을 Groundtruth와 일치시키는 것이 필수적인 비디오 QA의 경우, Flamingo(Alayrac et al., 2022)의 제로 샷 접근법을 채택하고 다운스트림 태스크의 트레이닝 세트로부터 2 샷 텍스트 전용 프롬프트를 사용한다. 추가적으로, MSRVTT-QA 및 MSVD-QA에 대해, 폐쇄 어휘 평가 구성(Li et al., 2022; Yang et al., 2022)을 실험한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 4: **영점 비디오 분류에 대한 최신 결과 비교. 결과는 Kinetics-400 및 Something-Something v2에 대한 Top-1/5 정확도(%), NEXT-QA(ATP-Hard) 및 Charades-STA에 대한 다중 선택(MC) 검색 정확도(%), Charades에 대한 평균 평균 정밀도(mAP)로 보고된다. Ni 등(2022)에 따라 단순화를 위해 단일 시점 평가 프로토콜을 따른다. 시각 및 언어 외에 추가 모달리티(_e.g._, 오디오)로 사전 훈련된 모델은 회색으로 표시된다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      '표 3: ** 제로 샷 비디오-텍스트 검색의 결과. 우리는 모든 벤치마크에 대해 Recall@1(R@1) 및 R@5를 보고한다.**이 설정에서 모델이 후보자의 로그 우도에 따라 답변을 점수화하고 상위 항목을 반환한다.\n' +
      '\n' +
      '**주요 결과.** 표 5 및 표 6은 각각 제로-샷 비디오 캡션 및 QA의 결과를 나타낸다. 모델 아키텍처의 단순성과 적은 수의 어댑터 매개변수에도 불구하고, 모델은 경쟁적이며 VATEX를 제외하고 비전과 언어 모델을 모두 동결하는 방법 중 최고이다. 본 논문에서 제안한 VideoPrism 인코더는 비디오-언어 생성 작업을 일반화할 수 있음을 보인다.\n' +
      '\n' +
      '### 과학 과제의 CV\n' +
      '\n' +
      '기존의 비디오 분석 벤치마크는 일반적으로 인간 중심 데이터에 초점을 맞추지만, 우리는 과학 데이터 세트의 광범위한 비디오 세트에 대해 비디오 프리즘을 평가하여 일반화 가능성과 과학 응용 프로그램에 사용할 가능성을 평가한다. 이들 데이터세트에는 에톨로지(Eyjolfsott et al., 2014), 행동신경과학(Sun et al., 2021; Burgos-Artizzu et al., 2012), 인지과학(Ma et al., 2023), 생태학(Kholiavchenko et al., 2024) 등의 분야가 포함된다. 우리가 아는 한, 이 작업은 과학 데이터 세트에서 ViFM의 사용을 처음으로 연구하여 전문 모델의 성능과 일치하거나 능가하는 능력을 강조한다. 우리는 다양한 과학 분야에 도움이 되는 ViFM의 잠재력을 잠금 해제하기 위해 실제 과학 실험에서 더 열린 소스 데이터 세트를 만들 것을 권장한다.\n' +
      '\n' +
      '**데이터세트.** 우리는 과학 실험에서 캡처된 도메인 전문지식으로 주석이 달린 대규모 비디오 데이터세트에 초점을 맞춘다. 이러한 데이터 세트는 파리(플라이 대 플라이)로 구성된다. 플라이(Eyjolfsott et al., 2014)), 마우스(CalMS21(Sun et al., 2021), CRIM13(Burgos-Artizzu et al., 2012)), 침팬지(Chimpact(Ma et al., 2023)), 및 케냐 동물(KABR(Kholiavchenko et al., 2024))을 포함할 수 있다. 시공간 액션 로컬리제이션을 위한 ChimpACT 데이터세트를 제외하고 모든 데이터세트는 동작의 비디오 분류를 위해 주석이 달렸다. 우리는 케이지에 수직인 측면의 카메라("S")와 상부, 오버헤드 뷰("T")로부터 CRIM13을 평가한다. 이러한 데이터셋에 대해 기존 연구에서 정의된 표준 데이터 분할을 사용하며, 매크로 정확도를 사용하는 KABR을 제외한 모든 데이터셋은 mAP 메트릭을 사용하여 평가한다. 추가 구현 세부 사항은 부록 H에 나와 있다.\n' +
      '\n' +
      '**주요 결과.** 모든 평가에 걸쳐 공유 냉동 인코더를 사용하는 일반 ViFM은 개별 작업에 특화된 도메인 특정 모델에 필적(또는 초과)하는 성능을 달성한다(표 7). 특히 VideoPrism은 일반적으로 베스트를 수행하고 베이스 스케일 모델로 도메인 전문가 모델을 능가한다. 대규모 모델로 확장하면 모든 데이터 세트에서 성능이 더욱 향상됩니다. 이러한 결과는 ViFM이 다양한 분야에서 비디오 분석을 상당히 가속화할 가능성이 있음을 보여준다.\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      'VideoPrism의 주요 원동력은 사전 훈련 데이터를 수집하기 위한 전략과 노력, 2단계 사전 훈련 프레임워크인 글로벌 증류 및 토큰 셔플링에 의해 마스킹된 자동 인코딩 시 개선되는 사전 훈련 접근법을 포함한다. 우리는 이러한 구성 요소의 효과를 평가하기 위해 절제 연구를 실행한다. 먼저, WTS-70M, YT-Temporal-180M 및 InternVid를 포함하여 더 작은 규모의 공개 코퍼스(총 150M 비디오 클립)에 대해 섹션 2.3.1에 제시된 비디오 텍스트 대조 기준선을 훈련한다. 그런 다음 기본값에 주요 구성 요소(더 큰 사전 훈련 데이터, 2단계 훈련, 손실 및 토큰 셔플)를 한 번에 하나씩 추가하여 모델이 어떻게 되는지 확인합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & **MSRVTT-QA** & **MSVD-QA** & **NExT-QA** \\\\ \\hline \\multicolumn{4}{l}{_Question-answering-only models_} \\\\ FrozenBtM-M (Yang et al., 2022) & 22.2 & 39.0 & - \\\\ \\hline \\multicolumn{4}{l}{_All-in-one models_} \\\\ BLIP-B (Li et al., 2022) & 19.2 & 35.2 & - \\\\ HiFLo-B (Ye et al., 2023) & 21.7 & 37.4 & - \\\\ mPLUQ-2 (Xu et al., 2023) & 43.8 & 55.3 & - \\\\ Flanning-3B (Alayra et al., 2022) & 11.0 & 27.5 & 21.3 \\\\ Flanning-3B (Alayra et al., 2022) & 13.7 & 30.2 & 23.0 \\\\\n' +
      'VideoPrism-B** w/PALM-2-1B & 28.5 (\\(\\uparrow\\)6.3) & 39.5 (\\(\\uparrow\\)0.5) & 23.8 (\\(\\uparrow\\)0.8) \\\\\\\n' +
      '**VideoPrism-B** w/ PALM-2-8B & **32.0** (\\(\\uparrow\\)9.8) & **47.1** (\\(\\uparrow\\)8.1) & **27.4** (\\(\\uparrow\\)4.4) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **영점 비디오 QA**에 대한 최신 방법과의 비교. 우리는 NExT-QA에 대한 WUPS 지수(Wu and Palmer, 1994)와 다른 것들에 대한 Top-1 정확도를 보고한다. 언어 모델의 동결을 해제하는 방법은 회색으로 표시됩니다.\n' +
      '\n' +
      '그림 4: **절제 연구.** 위에서 아래로: 우리는 비디오 텍스트 대조 기준선으로 시작하여 점진적으로 주요 구성 요소를 추가한다. 각 행은 바로 앞의 행의 수정에 기초한다. 우리는 단일 냉동 인코더만을 사용하여 K400과 SSv2 모두에서 잘 수행하기가 어렵지만 모든 개선이 있는 최종 모델은 두 데이터 세트에서 탁월하다는 점에 주목한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & **MSRVTT** & **VATEX** & **YouCook2** \\\\ \\hline \\multicolumn{4}{l}{_Capitining-only models_} \\\\ VideoCoCoCo-g (Yan et al., 2022) & 27.1 & 22.8 & 34.3 \\\\ DeCap-B (Li et al., 2023) & 18.6 & 18.7 & - \\\\ \\hline \\multicolumn{4}{l}{_All-in-one models_} \\\\ Flanning-3B (Alayra et al., 2022) & - & **40.1** & 55.8 \\\\ Flanning-3B (Alayra et al., 2022) & - & 39.5 & 55.0 \\\\\n' +
      'VideoPrism-B** w/PALM-2-1B & **40.3** (\\(\\uparrow\\)13.3) & 24.2 (\\(\\downarrow\\)12.2) & 52.3 (\\(\\downarrow\\)3.5) \\\\\\\n' +
      '**VideoPrism-B** w/ PALM-2-8B & 38.5 (\\(\\uparrow\\)11.3) & 31.7 (\\(\\uparrow\\)8.4) & **63.6** (\\(\\uparrow\\)7.8) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: ** zero-shot video captioning에 대한 최신 방법과의 비교.** 모든 벤치마크에 대한 CIDEr 점수를 보고한다.\n' +
      '\n' +
      '성과는 그 과정에서 진화한다. 또한 2단계 훈련 파이프라인의 효율성을 강조하기 위해 마스킹된 오토인코딩(Feichtenhofer et al., 2022)과 대조적 손실을 결합하여 실험한다.\n' +
      '\n' +
      '그림 4는 모션이 풍부한 SSv2와 외모 중심의 K400에서 다양한 성능 진화의 궤적을 관찰하는 절제 결과를 보여준다. 특히 SSv2에서 VideoPrism의 일관된 개선은 비디오에서 모션 이해를 돕기 위한 데이터 큐레이션 및 모델 설계 노력의 효과를 시사한다. 대조 기준선은 이미 K400에서 경쟁적인 결과를 달성했지만 제안된 글로벌 증류 및 토큰 셔플링은 정확도를 더욱 향상시킨다. 우리는 부록 I에서 보다 포괄적인 절제 연구를 제공한다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '제안된 방법의 한 가지 한계점은 사전 훈련의 일부로 잡음이 많은 텍스트가 포함된 비디오 코퍼스를 활용한다는 것이다. 이 시끄러운 텍스트는 잠재적으로 불완전하고 편향되어 모델 성능에 영향을 미칠 수 있습니다. 또한 비디오 프리즘의 입력으로 \\(16\\) 프레임을 샘플링하는 짧은 비디오 클립에 초점을 맞추고 있기 때문에 긴 비디오 이해는 여전히 과제로 남아 있다. 이 방향으로의 향후 작업은 긴 비디오 이해 시스템의 일부로 인코더를 활용할 수 있다. 마지막으로 동결 백본 평가를 옹호하지만 종단 간 미세 조정 및 매개변수 효율적인 적응으로 더 많은 이점을 얻는 시나리오가 있음을 인정한다. 이러한 한계에도 불구하고, 결과는 비디오 프리즘이 다양한 실제 비디오 이해 작업에 미치는 잠재적 영향을 보여준다.\n' +
      '\n' +
      '##4 관련 사항\n' +
      '\n' +
      '**Foundation model(FM)**(Bommasani et al., 2021)은 LLMs(Devlin et al., 2019; Brown et al., 2020)에서 초기 작업과 함께 엄청난 가능성을 보여주었다. 일부 ViFM은 LLMs(Wang et al., 2022; Li et al., 2023; Zhang et al., 2023; Chen et al., 2023) 주위에 구축되며, ASR 전사체 및 기계 생성 캡션과 같은 LLMs에 관련 텍스트를 공급함으로써 비디오를 분석한다. 대조적으로, VideoPrism은 비디오 중심 뷰를 취하며, 우리는 더 넓은 범위의 비디오 이해 작업을 해결하는 것을 목표로 한다.\n' +
      '\n' +
      '**ViFMs.** CV에서 가장 최근의 FMs는 이미지에 초점을 맞춘다(Radford et al., 2021; Yuan et al., 2021; Jia et al., 2021; Yu et al., 2022; Alayrac et al., 2022; Yan et al., 2022; Wang et al., 2022; Chen et al., 2023; Xu et al., 2023; Girdhar et al., 2023; Zhu et al., 2024). 그들의 사전 훈련 데이터는 비디오의 일부가 없거나 작은 부분만을 포함하고, 모델 아키텍처 및 학습 방법은 설계에 의한 이미지에 대한 것이다. 이러한 FM들은 비디오 프레임들을 입력으로서 수용할 수 있지만, 이들은 모션 및 시간적 모델링에 부족하다(Yuan et al., 2023). 우리의 작업은 비디오 인코더를 개발하여 이 격차를 직접 해결합니다.\n' +
      '\n' +
      '비디오의 경우, 기존의 작업들은 주로 비디오 전용 모달리티(Qian et al., 2021; Feichtenhofer et al., 2021; Recasens et al., 2021; Singh et al., 2021; Wei et al., 2022; Yuan et al., 2022; Qian et al., 2022; Tong et al., 2022; Wang et al., 2023) 또는 잡음이 있는 텍스트를 갖는 비디오의 비디오 언어 모델링(Zellers et al., 2021; Fu et al., 2021; Li et al., 2023; Wang et al., 2023; Cheng et al., 2023; Piergiovanni et al., 2023; Xiong et al., 2023). W Wang et al.(2023)이 지적한 바와 같이, 기존의 비디오 언어 모델은 액션에 대한 지식이 부족하지만 비디오 전용 데이터로부터 자체 감독된 모델은 의미론적으로 어려움을 겪는다. 대신 우리는 둘 중 최고를 함께 모읍니다. 우리의 작업과 관련하여, InternVideo(Wang et al., 2022)는 자기 감독 비디오MAE 모델(Wang et al., 2023)과 비디오 언어 모델을 교차 주의 모듈을 사용하여 함께 접착한다. 그러나 VideoPrism과 달리 두 모델은 사전 훈련 동안 상호 영향을 미치지 않으며 동일한 비디오를 처음부터 동시에 중복 처리한다.\n' +
      '\n' +
      '**대규모 비디오 데이터 세트**는 ViFM에 중추적이며 관심 대상이었다. HowTo100M(Miech et al., 2019), YT-Temporal-1B(Zellers et al., 2022), 및 HD-VILA-100M(Xue et al., 2022)은 스피치 전사들을 비디오들과 연관시킨다. WebVid2M(Bain et al., 2021) 및 WTS70M(Stroud et al., 2020)은 알트-텍스트 및 다른 메타데이터를 비디오와 페어링한다. VideoCC3M(Nagrani et al., 2022)은 이미지와 유사하게 보이는 비디오를 검색하여 전송한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Methods & **Fly vs. Fly** & **CalMS21** & **CRIM13 (S/T)** & **KABR** & **ChimpACT** \\\\ \\hline Domain experts & \\(88.6\\) & \\(88.9\\) & - & \\(61.9\\) & \\(24.4\\) \\\\ \\hline _Base-scale models_ & & & & & \\\\ CoCa-B (Yu et al., 2022) & 80.1 & 89.2 & 58.2 / 58.4 & **62.0** & 12.6 \\\\ InternVideo-B (Wang et al., 2022) & 78.9 & 89.0 & 63.2 / 63.6 & 49.9 & 24.0 \\\\ UMT-B (Li et al., 2023) & **84.6** & 88.7 & 59.3 / 58.5 & 58.9 & 25.0 \\\\\n' +
      '**VideoPrism-B** & **89.1** (\\(74.5\\)) & **91.1** (\\(70.9\\)) & **64.5** (\\(71.3\\)) & **64.9** (\\(71.3\\)) & 61.6 (\\(1.04\\)) & **28.8** (\\(73.8\\)) \\\\ \\hline \\multicolumn{6}{l}{_Large-scale models_} \\\\ InternVideo-L (Wang et al., 2022) & 86.6 & **91.5** & 65.7 / 65.2 & 51.4 & 25.7 \\\\ UMT-L (Li et al., 2023) & 86.4 & 89.5 & **60.5** / 61.4 & 62.7 & 24.7 \\\\\n' +
      '**VideoPrism-g** & **92.0** (\\(15.4\\)) & **91.5** (\\(70.0\\)) & **65.9** (\\(10.2\\)) & **66.8** (\\(11.6\\)) & **63.3** (\\(70.6\\)) & **31.5** (\\(75.8\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: **사이언스 벤치마크를 위한 CV에 대한 최신 방법 및 도메인 전문가와의 비교. 매크로 정확도** 이미지 캡션을 해당 비디오에 사용하는 KABR을 제외하고 모든 데이터 세트에 대한 평균 정밀도(mAP)를 보고한다. VAST-27M(Chen et al., 2023b) 및 InternVid(Wang et al., 2023e)는 멀티모달 및 언어 모델을 캡션 비디오에 사용한다. 여전히, 이러한 비디오-텍스트 데이터세트는 이미지에 대한 그들의 대응물보다 상당히 작으며, 많은 ViFM은 미리 훈련된 이미지-텍스트 모델을 비디오 공간에 적응시킨다(Fang et al., 2021; Luo et al., 2022; Xue et al., 2023; Liu et al., 2023; He et al., 2023; Wu et al., 2024). 사전 훈련 코퍼스는 ASR 전사체, 생성된 캡션 및 고품질 수동 주석이 달린 캡션의 하이브리드 믹스에서 텍스트 연관성을 가지고 있다.\n' +
      '\n' +
      '**Pretraining strategy.** Our pretraining integrates vision-language contrastive learning (Radford et al., 2021; Xu et al., 2021; Bain et al., 2022) and masked data modeling (Devlin et al., 2019; He et al., 2022). 전자는 CLIP(Radford et al., 2021), ALIGN(Jia et al., 2021), CoCa(Yu et al., 2022)와 같은 강력한 후기 융합 모델로 이어졌고, 후자는 언어와 같은 단일 모달리티 데이터로부터 학습하는 데 효과적인 것으로 입증되었다(Devlin et al., 2019; Anil et al., 2023), 오디오(Borosos et al., 2023), 이미지(He et al., 2022; Wang et al., 2023d; Oquab et al., 2023), 및 비디오(Tong et al., 2022; Wang et al., 2023b). EVA(Fang et al., 2022; 2023) 및 UMT(Li et al., 2023b)가 CLIP(Radford et al., 2021)로부터 마스킹 모델링으로 간접 의미학을 전달하는 동안, 비디오-네이티브 의미학을 학습한다. 또한 마스크 비디오 모델링에 글로벌 증류 및 토큰 셔플링을 도입하여 외관과 모션 신호를 모두 조정한다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '우리는 광범위한 비디오 이해 작업에 걸쳐 최첨단 성능을 달성하는 기반 비디오 인코더인 VideoPrism을 제시한다. 본 논문의 설계는 데이터 및 모델링 접근 방식을 모두 강조하며, 그 종류의 가장 큰 사전 훈련 데이터 세트를 조립하고, 이로부터 외관과 움직임 정보를 효과적으로 학습하는 사전 훈련 전략을 개발한다. 우리의 종합적인 평가에서 VideoPrism은 대부분의 벤치마크에서 최고의 결과를 얻습니다. 특히, 다른 기준 모델은 우리의 고유한 일반화 가능성을 강조하면서 일관되게 두 번째 최고를 달성하지 못한다.\n' +
      '\n' +
      '## Broader impacts\n' +
      '\n' +
      '비디오 이해의 발전은 과학 연구, 교육, 로봇 공학, 의료 및 콘텐츠 추천을 포함한 다양한 분야에서 발전을 가속화할 가능성이 있다. 이러한 기술은 새로운 과학적 발견에 힘을 실어주고, 학습 경험을 향상시키며, 보안 및 안전을 개선하고, 보다 반응성이 높은 대화형 시스템을 가능하게 할 수 있다. 그러나 관련 모델을 실제 세계에 배포하기 전에 잠재적인 편견과 오용을 해결하는 것이 중요합니다. 여기에는 알고리즘 편향 완화, 개인 정보 보호, 책임 있는 연구의 규칙 및 정책 존중이 포함된다. 이 기술의 이점이 책임감 있게 활용되도록 하기 위해 우리는 이러한 새로운 기술의 개발을 둘러싼 커뮤니티에서 지속적인 공개 토론을 권장한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '제품 관리 노력에 대해 데이빗 헨던과 프로그램 및 자원 관리 노력에 대해 알렉스 시그만, 라마 가네샨, 빅터 고메스에 진심으로 감사드립니다. 또한 하산 악바리, 셰리 벤, 요니 벤-메슐람, 춘테 추, 샘 클리어워터, 인 쿠이, 일야 피고틴, 안자 하우트, 세르게이 이오페, 쉬후이 지아, 예칭 리, 루 장, 주 김, 단 콘드라턱, 빌 마크, 아르샤 나그라니, 캐롤린 판토파루, 슈샨트 프라카쉬, 코델리아 슈미드, 브라이언 세이볼드, 모히타바 세예도세이니, 아만다 새들러, 리프 A. 사우루스, 레이첼 스티글러, 폴 보이그틀렌더, 핑메이 쉬, 차차오 얀, 쉬안 양, 유쿤 주에게 본 논문에 크게 기여한 토론과 지지와 피드백에 대해 감사드린다. Jay Yagnik, Rahul Sukthankar, 그리고 Tomas Izo가 이 프로젝트에 열정적으로 지원해준 것에 대해 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Akbari et al. (2021) Akbari, H., Yuan, L., Qian, R., Chuang, W. - H, Chang S. - F., Cui, Y., and Gong, B. VATT: 트랜스포머 for multimodal self-supervised learning from raw video, audio and text. 2021년 _NeurIPS_에서.\n' +
      '* Akbari et al. (2023) Akbari, H., Kondratyuk, D., Cui, Y., Hornung, R., Wang, H., and Adam, H. Alternating gradient descent and mixture-of- experts for integrated multimodal perception. _NeurIPS_, 2023.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: few-shot learning을 위한 시각적 언어 모델. 2022년 _NeurIPS_에서.\n' +
      '* Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. PaLM 2 technical report. _ arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Arnab et al. (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., and Schmid, C. ViViT: A video vision transformer. 2021년 _ICCV_에서.\n' +
      '* Bagad et al. (2023) Bagad, P., Tapaswi, M., and Snoek, C. G. Test of time: Instilling video-language models with the sense of time. _CVPR_, 2023.\n' +
      '* Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021년 _ICCV_에서.\n' +
      '\n' +
      'Bain, M., Nagrani, A., Varol, G., and Zisserman, A. CLIP-Hitchhiker\'s guide to long video retrieval. _ arXiv preprint arXiv:2205.08508_, 2022.\n' +
      '* Bansal et al. (2023) Bansal, H., Bitton, Y., Szepktor, I., Chang, K. - W., and Grover, A. VideoCon: Robust Video-language alignment via contrast caption. _ arXiv preprint arXiv:2311.10111_, 2023.\n' +
      '* Bao et al. (2022) Bao, H., Dong, L., Piao, S., and Wei, F. BEiT: BERT pre-training of image transformer. 2022년 _ICLR_에서\n' +
      '* Bommasani et al. (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* Borosos et al. (2023) Borosos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., et al. AudioLM: A language modeling approach to audio generation. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31:2523-2533, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. 2020년 _NeurIPS_에서.\n' +
      '* Buch et al. (2022) Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., and Niebles, J. C. Revisiting the "video" in video-language understanding. 2022년 _CVPR_에서.\n' +
      '* Burgos-Artizzu et al. (2012) Burgos-Artizzu, X. P., Dollar, P., Lin, D., Anderson, D. J., and Perona, P. Social behavior recognition in continuous video. 2012년 _CVPR_에서.\n' +
      '* Caba Heilbron et al. (2015) Caba Heilbron, F., Escorcia, V., Ghanem, B., and Carlos Niebles, J. ActivityNet: A large-scale video benchmark for human activity understanding. 2015년 _CVPR_에서.\n' +
      '* Carreira et al. (2018) Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A. A short note about Kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* Chen et al.(2023a) Chen, G., Zheng, Y. -D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al. VideoLLM: 모델링 비디오 시퀀스 with large language models. _ arXiv preprint arXiv:2305.13292_, 2023a.\n' +
      '* Chen & Huang (2021) Chen, S. Hang, D. Elaborative rehearsal for zero-shot action recognition. 2021년 _ICCV_에서.\n' +
      '* Chen et al. (2023b) Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., and Liu, J. VAST: A vision-audio-subtitle-text omni-modality foundation model and dataset. _NeurIPS_, 2023b.\n' +
      '* Chen et al. (2023c) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. PaLI: A jointly-scaled multiilingual language-image model. _ICLR_, 2023c.\n' +
      '* Cheng et al. (2023d) Cheng, F., Wang, X., Lei, J., Crandall, D., Bansal, M., and Bertasius, G. VindLU: A recipe for effective video-and-language pretraining. _CVPR_, 2023d.\n' +
      '* Devlin et al. (2019) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. 2019년 _NAACL-HLT_에서.\n' +
      '* Dima et al. (2022) Dima, D., Doughty, H., Farinella, G. M., Antonino, F., Evangelos, K., Ma, J., Davide, M., Munro, J., Toby, P., Price, W., et al. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. _IJCV_, 130(1):33-55, 2022).\n' +
      '* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. 이미지는 16x16 단어들의 가치: 스케일에서 이미지 인식을 위한 트랜스포머들이다. 2021년 _ICLR_에서.\n' +
      '* Eyjolfsdottir et al. (2014) Eyjolfsdottir, E., Branson, S., Burgos-Artizzu, X. P., Hoopfer, E. D., Schor, J., Anderson, D. J., and Perona, P. 2014년 _ECCV_에서.\n' +
      '* Fang et al. (2021) Fang, H., Xiong, P., Xu, L., and Chen, Y. CLIP2Video: 이미지 CLIP를 통한 비디오-텍스트 검색의 마스터링 arXiv preprint arXiv:2106.11097_, 2021.\n' +
      '* Fang et al. (2022) Fang, Y., Wang, W., Xie, B., Sun, Q. -S., Wu, L. Y., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA: 스케일에서 마스킹된 시각적 표현 학습의 한계를 탐구한다. 2022년 _CVPR_에서.\n' +
      '* Fang et al. (2023) Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA-02: 네온 유전자에 대한 시각적 표현. _ arXiv preprint arXiv:2303.11331_, 2023.\n' +
      '* Feichtenhofer et al. (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. 비디오 인식을 위한 저속 고속 네트워크입니다. 2019년 _ICCV_에서\n' +
      '* Feichtenhofer et al. (2021) Feichtenhofer, C., Fan, H., Xiong, B., Girshick, R., and He, K. 비지도 시공간 표상 학습에 대한 대규모 연구 2021년 _CVPR_에서.\n' +
      '* Feichtenhofer et al. (2022) Feichtenhofer, C., Fan, H., Li, Y., and He, K. 시공간 학습자로 마스킹된 오토인코더. 2022년 _NeurIPS_에서.\n' +
      '* Fu et al. (2021) Fu, T. - J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., and Liu, Z. VIOLET: 마스킹된 시각-토큰 모델링을 갖는 종단간 비디오-언어 트랜스포머. _ arXiv preprint arXiv:2111.12681_, 2021.\n' +
      '\n' +
      '* Gao et al. (2017) Gao, J., Sun, C., Yang, Z., and Nevatia, R. TALL: 언어 쿼리를 통한 시간 활동 현지화. 2017년 _ICCV_에서.\n' +
      '* Girdhar et al. (2023) Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., and Misra, I. ImageBind: One embedding space to bind them all. _CVPR_, 2023.\n' +
      '* Goyal et al. (2017a) Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al. The "something something" video database for learning and evaluating visual common sense. _ICCV_, 2017a.\n' +
      '* Goyal et al. (2017b) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. _CVPR_, 2017b에서.\n' +
      '* Grauman et al. (2022) Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4D: 3,000시간의 자기중심적 비디오에서 세계를 중심으로. 2022년 _CVPR_에서.\n' +
      '* Gu et al. (2018) Gu, C., Sun, C., Ross, D. A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al. AVA: 시공간적으로 국부화된 원자 시각적 액션들의 비디오 데이터세트. 2018년 _CVPR_에서.\n' +
      '* Gutmann & Hyvarinen (2010) Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: A new estimation principle for nonnormalized statistical models. 2010년 _AISTATS_에서.\n' +
      '* He et al. (2022) He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. 마스크 자동 인코더는 확장 가능한 비전 학습자입니다. 2022년 _CVPR_에서.\n' +
      '* He et al. (2023) He, X., Chen, S., Ma, F., Huang, Z., Jin, X., Liu, Z., Fu, D., Yang, Y., Liu, J., and Feng, J. VLAB: Enhancing video language pre-training by feature adaptation and blending. _ arXiv preprint arXiv:2305.13167_, 2023.\n' +
      '* Hendricks & Nematzadeh (2021) Hendricks, L. A. and Nematzadeh, A. Probing image-language transformer for verb understanding. 2021년 _ACL_에서.\n' +
      '* Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: 대형 언어 모델의 낮은 순위 적응. 2022년 _ICLR_에서\n' +
      '* Huang et al. (2020) Huang, G., Pang, B., Zhu, Z., Rivera, C., and Soricut, R. 밀집 비디오 캡션을 위한 멀티모달 프리트레이닝. _ arXiv preprint arXiv:2011.11760_, 2020.\n' +
      '* Huang et al. (2016) Huang, T. - H., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He, X., Kohli, P., Batra, D., et al. Visual storytelling. 2016년 _NAACL-HLT_에서.\n' +
      '* Jain et al. (2017) Jain, P., Kar, P., et al. Non-convex optimization for machine learning. _ Foundations and Trends(r) in Machine Learning_, 10(3-4):142-363, 2017.\n' +
      '* Jia et al. (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y. - T., Parekh, Z., Pham, H., Le, Q., Sung, Y. - H., Li, Z., and Duerig, T. 시끄러운 텍스트 감독으로 시각 및 시각 언어 표현 학습을 확장합니다. 2021년 _ICML_에서.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The Kinetics human action video dataset. _ ArXiv:1705.06950_, 2017.\n' +
      '* Kholiavchenko et al. (2024) Kholiavchenko, M., Kline, J., Ramirez, M., Stevens, S., Sheets, A., Babu, R., Banerji, N., Campolongo, E., Thompson, M., Van Tiel, N., et al. KABR: in-situ dataset for kenyan animal behavior recognition from drone video. 2024년 _WACV_에서\n' +
      '* Kingma & Ba (2015) Kingma, D. P. and Ba, J. Adam: a method for stochastic optimization. 2015년 _ICLR_에서.\n' +
      '* Lee et al. (2019) Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set Transformer: A framework for attention-based permutation-invariant neural networks. 2019년 _ICML_에서.\n' +
      '* Lei et al.(2023) Lei, J., Berg, T. L., and Bansal, M. 비디오 및 언어 학습을 위한 단일 프레임 편향을 드러냅니다. 2023년 _ACL_에서\n' +
      '* Li et al. (2020) Li, A., Thotakuri, M., Ross, D. A., Carreira, J., Vostrikov, A., and Zisserman, A. The AVA-Kinetics localized human action video dataset. _ arXiv preprint arXiv:2005.00214_, 2020.\n' +
      '* Li 등 (2022) Li, J., Li, D., Xiong, C., and Hoi, S. BLIP: 통일된 시각 언어 이해 및 생성을 위한 언어 이미지 사전 훈련. 2022년 _ICML_에서\n' +
      '* Li 등(2023a) Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: 냉동 이미지 인코더 및 대형 언어 모델로 부트스트래핑 언어-이미지 사전 트레이닝 arXiv preprint arXiv:2301.12597_, 2023a.\n' +
      '*Li 등(2023b) Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., and Qiao, Y. 가면을 벗은 교사: 효율적인 비디오 기반 모델을 교육하기 위한 것입니다. _ICCV_, 2023b에서.\n' +
      '*Li 등(2023c) Li, L., Gan, Z., Lin, K., Lin, C.-C., Liu, Z., Liu, C., and Wang, L. 라벤더: 비디오 언어 이해를 마스킹 언어 모델링으로 통합합니다. _CVPR_, 2023c.\n' +
      '*Li 등(2023d) Li, W., Zhu, L., Wen, L., and Yang, Y. DeCap: 텍스트 전용 교육을 통해 제로 샷 캡션을 위한 CLIP 래턴트를 디코딩합니다. _ICLR_, 2023d.\n' +
      '* Li 등 (2018) Li, Y., Li, Y., and Vasconcelos, N. RESOUND: 표현 편향 없이 행동 인식을 향한다. _ECCV_, 2018.\n' +
      '*Li 등(2018)*Li 등(2023) Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. 마스킹을 통한 언어-이미지 사전 트레이닝의 스케일링. _CVPR_, 2023e.\n' +
      '* Li et al. (2023) Li, Y., Wang, C., and Jia, J. LLaMA-VID: An image is worth 2 tokens in large language models. _ arXiv preprint arXiv:2311.17043_, 2023f.\n' +
      '* Lin et al. (2023a) Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-LLaVA: 투영 전에 정렬에 의해 통합된 시각적 표현을 학습하는 단계 _ arXiv preprint arXiv:2311.10122_, 2023a.\n' +
      '* Lin et al. (2022) Lin, K. Q., Wang, J., Soldan, M., Wray, M., Yan, R., XU, E. Z., Gao, D., Tu, R. - C., Zhao, W., Kong, W., et al. Egocentric video-language preraining. 2022년 _NeurIPS_에서.\n' +
      '* Lin et al. (2023b) Lin, W., Karlinsky, L., Shvetsova, N., Possegger, H., Kozinski, M., Panda, R., Feris, R., Kuehne, H., and Bischof, H. Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge. _ICCV_, 2023b에서.\n' +
      '* Liu et al. (2023) Liu, R., Huang, J., Li, G., Feng, J., Wu, X., and Li, T. H. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. _CVPR_, 2023.\n' +
      '* Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled Weight decay regularization. 2019년 _ICLR_에서.\n' +
      '* Luo et al. (2022) Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., and Li, T. CLIP4Clip: End to End 비디오 클립 검색 및 캡셔닝을 위한 클립에 대한 실증적 연구 _ Neurocomputing_, 508:293-304, 2022.\n' +
      '* Ma et al. (2023) Ma, X., Kaufhold, S. P., Su, J., Zhu, W., Terwilliger, J., Meza, A., Zhu, Y., Rossano, F., and Wang, Y. 침팬지 행동 이해를 위한 종단 데이터세트. _ arXiv preprint arXiv:2310.16447_, 2023.\n' +
      '* Maaz et al. (2023) Maaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-ChatGPT: Towards detailed video understanding via large vision and language models. _ arXiv preprint arXiv:2306.05424_, 2023.\n' +
      '* McCloskey & Cohen (1989) McCloskey, M. 그리고 Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pp. 109-165. Elsevier, 1989.\n' +
      '* Miech et al. (2019) Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J. HowTo100M: Learning a text-video embedding by watching 1억 narrated video clips. 2019년 _ICCV_에서\n' +
      '* Momeni et al. (2023) Momeni, L., Caron, M., Nagrani, A., Zisserman, A., and Schmid, C. verb in action: Improving verb understanding in video-language models. _ICCV_, 2023.\n' +
      '* Monfort et al. (2019) Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S. A., Yan, T., Brown, L., Fan, Q., Gutfreund, D., Vondrick, C., et al. Moments in Time dataset: one million videos for event understanding. _ IEEE TPAMI_, 42(2):502-508, 2019.\n' +
      '* Monfort et al. (2021) Monfort, M., Jin, S., Liu, A., Harwath, D., Feris, R., Glass, J., and Oliva, A. Spoken Moments: Learning joint audio-visual representation from video descriptions. 2021년 _CVPR_에서.\n' +
      '* Nagrani et al. (2022) Nagrani, A., Seo, P. H., Seybold, B., Hauth, A., Manen, S., Sun, C., and Schmid, C. Learning audio-video modalities from image captions. 2022년 _ECCV_에서.\n' +
      '*Ni 등 (2022) Ni, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu, J., Xiang, S., and Ling, H. Expanding language-image prerained model for general video recognition. 2022년 _ECCV_에서.\n' +
      '* Noroozi & Favaro (2016) Noroozi, M. 그리고 Favaro, P. Unsupervised learning of visual representations by solve Jigsaw puzzle. 2016년 _ECCV_에서.\n' +
      '* Oquab et al. (2023) Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al. DINOv2: 학습 robust visual features without supervision. _ arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* Peng et al. (2022) Peng, Z., Dong, L., Bao, H., Ye, Q., and Wei, F. BEiT v2: Masked image modeling with vector-quantized visual tokenizers. _ ArXiv:2208.06366_, 2022.\n' +
      '* Piergiovanni et al. (2023) Piergiovanni, A., Nobel, I., Kim, D., Ryoo, M. S., Gomes, V., and Angelova, A. Mirasol3B: multiimodal autoregressive model for time-aligned and context modalities. _ arXiv preprint arXiv:2311.05698_, 2023.\n' +
      '* Pitcher-Cooper et al. (2023) Pitcher-Cooper, C., Seth, M., Kao, B., Coughlan, J. M., and Yoon, I. You Described, We Archived: a rich audio description dataset. _ Journal on Technology and Persons with Disabilities_, 2023.\n' +
      '* Qian et al.(2021) Qian, R., Meng, T., Gong, B., Yang, M. - H., Wang, H., Belongie, S., and Cui, Y. 시공간 대조적 영상 표현 학습. 2021년 _CVPR_에서.\n' +
      '* Qian et al.(2022) Qian, R., Li, Y., Yuan, L., Gong, B., Liu, T., Brown, M., Yang, M. - H., Adam, H., and Cui, Y. 자기 지도 영상 표현 학습에서 시간적 세분성에 관한 연구 2022년 _BMVC_에서\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. 2021년 _ICML_에서.\n' +
      '\n' +
      'Recasens, A., Luc, P., Alayrac, J.-B., Wang, L., Strub, F., Tallec, C., Malinowski, M., Patrucean, V., Altche, F., Valko, M., et al. 2021년 _ICCV_에서.\n' +
      '* Regneri et al. (2013) Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., and Pinkal, M. 동영상에서 작업 설명 접지 The Association for Computational Linguistics_, 1:25-36, 2013.\n' +
      '* Ren et al. (2015) Ren, S., He, K., Girshick, R., and Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. _NeurIPS_, 2015.\n' +
      '* Sevilla-Lara et al. (2021) Sevilla-Lara, L., Zha, S., Yan, Z., Goswami, V., Feiszli, M., and Torresani, L. 시간만이 알 수 있다: 시간 모델링을 위한 시간 데이터를 발견하는 것. 2021년 _WACV_에서.\n' +
      '* Shazeer & Stern (2018) Shazeer, N. 및 스턴, M. 보조인자: 하위 선형 메모리 비용을 갖는 적응형 학습 속도. 2018년 _ICML_에서.\n' +
      '* Sigurdsson et al. (2016) Sigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., and Gupta, A. Hollywood in Homes: Crowdsourcing data collection for activity understanding. 2016년 _ECCV_에서.\n' +
      '* Singh et al. (2021) Singh, A., Chakraborty, O., Varshney, A., Panda, R., Feris, R., Saenko, K., and Das, A. Semi-supervised action recognition with temporal contrastive learning. 2021년 _CVPR_에서.\n' +
      '* Singh et al. (2022) Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. FLAVA: A foundationational language and vision alignment model. 2022년 _CVPR_에서.\n' +
      '* Stroud et al. (2020) Stroud, J. C., Lu, Z., Sun, C., Deng, J., Sukthankar, R., Schmid, C., and Ross, D. A. Learning video representations from textual web supervision. _ arXiv preprint arXiv:2007.14937_, 2020.\n' +
      '* Sun et al. (2021a) Sun, J. J., Karigo, T., Chakraborty, D., Mohanty, S. P., Wild, B., Sun, Q., Chen, C., Anderson, D. J., Perona, P., Yue, Y., et al. The multi-agent behavior dataset: Mouse dyadic social interaction. 2021a _NeurIPS D&B_에서.\n' +
      '* Sun et al. (2021b) Sun, J. J., Kennedy, A., Zhan, E., Anderson, D. J., Yue, Y., and Perona, P. Task programming: Learning data efficient behavior representations. _CVPR_, 2021b에서.\n' +
      '* Tan 등(2020) Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C., and Yan, J. Equalization loss for long-tailed object recognition. 2020년 _CVPR_에서.\n' +
      '* Tang et al. (2019) Tang, Y., Ding, D., Rao, Y., Zheng, Y., Zhang, D., Zhao, L., Lu, J., and Zhou, J. COIN: comprehensive instructional video analysis를 위한 대규모 데이터셋. 2019년 _CVPR_에서.\n' +
      '* Tang et al. (2023) Tang, Y., Bi, J., Xu, S., Song, L., Liang, S., Wang, T., Zhang, D., An, J., Lin, J., Zhu, R., et al. Video understanding with large language models: A survey. _ arXiv preprint arXiv:2312.17432_, 2023.\n' +
      '* Tong et al. (2022) Tong, Z., Song, Y., Wang, J., and Wang, L. VideoMAE: 마스킹 오토인코더는 자체 감독 비디오 사전 교육을 위한 데이터 효율적인 학습자이다. 2022년 _NeurIPS_에서.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention이면 된다. 2017년 _NeurIPS_에서.\n' +
      '* Voigtlaender et al. (2023) Voigtlaender, P., Changpinyo, S., Pont-Tuset, J., Soricut, R., and Ferrari, V. 시각과 언어를 비디오 현지화 내러티브와 연결합니다. _CVPR_, 2023.\n' +
      '* Wang et al. (2022a) Wang, J., Chen, D., Wu, Z., Luo, C., Zhou, L., Zhao, Y., Xie, Y., Liu, C., Jiang, Y. - G., and Yuan, L. OmniVL: 이미지 언어 및 비디오 언어 작업을 위한 하나의 기초 모델. _NeurIPS_, 2022a.\n' +
      '* Wang et al. (2023a) Wang, J., Ge, Y., Yan, R., Ge, Y., Lin, K. Q., Tsutsui, S., Lin, X., Cai, G., Wu, J., Shan, Y., et al. All in one: Exploring unified video-language pre-training. _CVPR_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., Wang, Y., and Qiao, Y. VideoMAE v2: 비디오 마스킹된 오토인코더를 듀얼 마스킹으로 스케일링한다. _CVPR_, 2023b에서.\n' +
      '* Wang et al. (2022b) Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y. - G., Zhou, L., and Yuan, L. BEVT: 비디오 트랜스포머의 BERT 사전 트레이닝. _CVPR_, 2022b에서.\n' +
      '* Wang et al. (2023c) Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Yuan, L., and Jiang, Y. - G. 가면 비디오 증류: 자가 지도 비디오 표현 학습을 위한 마스킹된 특징 모델링을 재해석한다. _CVPR_, 2023c.\n' +
      '* Wang et al. (2023d) Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., et al. Image as a foreign language: BEiT preraining for vision and vision-language tasks. _CVPR_, 2023d.\n' +
      '* Wang et al. (2019) Wang, X., Wu, J., Chen, J., Li, L., Wang, Y. - F., and Wang, W. Y. VATEX: 비디오 및 언어 연구를 위한 대규모 고품질 다국어 데이터세트. 2019년 _ICCV_에서\n' +
      '* Wang et al. (2022c) Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al. InternVideo: Generative and discriminative learning을 통한 일반적인 비디오 기초 모델들_ arXiv preprint arXiv:2212.03191_, 2022c.\n' +
      '* Wang et al. (2023e) Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., et al. InternVid: multiimodal understanding and generation. _ arXiv preprint arXiv:2307.06942_, 2023e.\n' +
      '\n' +
      'Wang, Z., Li, M., Xu, R., Zhou, L., Lei, J., Lin, X., Wang, S., Yang, Z., Zhu, C., Hoiem, D., et al. 이미지 기술자들을 갖는 언어 모델들은 강한 소수의-샷 비디오-언어 학습자들이다. _NeurIPS_, 2022d.\n' +
      '* Wang et al. (2023) Wang, Z., Blume, A., Li, S., Liu, G., Cho, J., Tang, Z., Bansal, M., and Ji, H. Paxion: Patching action knowledge in video-language foundation models. _NeurIPS_, 2023f.\n' +
      '* Wei et al. (2022) Wei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C. Masked feature prediction for self-supervised visual pre-training. 2022년 _CVPR_에서.\n' +
      '* Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G., and Duan, N. 고디바: 자연 기술로부터 오픈 도메인 비디오를 생성하는 단계; _ arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* Wu et al. (2023) Wu, W., Sun, Z., and Ouyang, W. 재방문 분류기: 비디오 인식을 위한 비전 언어 모델 전달. 2023년, _AAAI_\n' +
      '* Wu & Palmer (1994) Wu, Z. 및 Palmer, M. 동사 의미 및 어휘 선택. 1994년 _ACL_에서\n' +
      '* Wu et al. (2024) Wu, Z., Weng, Z., Peng, W., Yang, X., Li, A., Davis, L. S., and Jiang, Y. - G. 더 나은 아키텍처, 최적화 및 데이터로 개방형 어휘 비디오 CLIP 모델을 구축합니다. _ IEEE TPAMI_, 2024.\n' +
      '* Xiao et al. (2021) Xiao, J., Shang, X., Yao, A., and Chua, T. - S. NExT-QA: 시간적 행동을 설명하기 위한 질문-답변의 다음 단계. 2021년 _CVPR_에서.\n' +
      '* Xiong et al.(2023) Xiong, Y., Zhao, L., Gong, B., Yang, M. -H., Schroff, F., Liu, T., Hsieh, C.-J., and Yuan, L. 텍스트 접지를 이용한 시공간 차별적 비디오 언어 사전 훈련. _ arXiv preprint arXiv:2303.16341_, 2023.\n' +
      '* Xu et al.(2017) Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., and Zhuang, Y. 외모와 모션에 대해 점차 세련된 주의를 통해 비디오 질문에 대답합니다. 2017년 _ACM MM_에서.\n' +
      '* Xu et al. (2021) Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. 2021년 _EMNLP_에서.\n' +
      '* Xu et al. (2023) Xu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi, B., Qian, Q., Wang, W., et al. mLUG-2: 텍스트, 이미지 및 비디오에 걸친 모듈화된 다중-모달 기초 모델. 2023년 _ICML_에서\n' +
      '* Xu et al.(2016) Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: 브리징 비디오 및 언어를 위한 대규모 비디오 기술 데이터세트. 2016년 _CVPR_에서.\n' +
      '* Xu et al. (2020) Xu, M., Zhao, C., Rojas, D. S., Thabet, A., and Ghanem, B. G-TAD: Sub-graph localization for temporal action detection. 2020년 _CVPR_에서.\n' +
      '* Xue et al. (2022) Xue, H., Hang, T., Zeng, Y., Sun, Y., Liu, B., Yang, H., Fu, J., and Guo, B. Advancing high-resolution video-language representation with large-scale video transcriptions. 2022년 _CVPR_에서.\n' +
      '* Xue et al. (2023) Xue, H., Sun, Y., Liu, B., Fu, J., Song, R., Li, H., and Luo, J. CLIP-ViP: Adapting pre-trained image-text model to video-language representation alignment. 2023년 _ICLR_에서\n' +
      '* Yan et al. (2022) Yan, S., Zhu, T., Wang, Z., Cao, Y., Zhang, M., Ghosh, S., Wu, Y., and Yu, J. VideoCoCa: Video-text modeling with zero-shot transfer with contrastive captioners. _ ARXiv 프리프린트 arXiv:2212.04979_, 2022.\n' +
      '* Yang et al. (2022) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Zero-shot video question answering via frozen bidirectional language models. 2022년 _NeurIPS_에서.\n' +
      '* Yarom et al. (2023) Yarom, M., Bitton, Y., Changpinyo, S., Aharoni, R., Herzig, J., Lang, O., Ofek, E., and Szpektor, I. What you see what you read what? 텍스트 이미지 정렬 평가 개선 _NeurIPS_, 2023.\n' +
      '* Ye et al. (2023) Ye, Q., Xu, G., Yan, M., Xu, H., Qian, Q., Zhang, J., and Huang, F. HiTeA: Hierarchical temporal-aware video-language pre-training. _ICCV_, 2023.\n' +
      '* Yu et al. (2022) Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. CoCa: 대조적 캡션들은 이미지-텍스트 기초 모델들이다. _ TMLR_, 2022. ISSN 2835-8856.\n' +
      '* Yuan et al. (2021) Yuan, L., Chen, D., Chen, Y. -L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al. Florence: 컴퓨터 비전을 위한 새로운 기초 모델. _ arXiv preprint arXiv:2111.11432_, 2021.\n' +
      '* Yuan et al. (2022) Yuan, L., Qian, R., Cui, Y., Gong, B., Schroff, F., Yang, M. - H., Adam, H., and Liu, T. 자기 감독과 함께 맥락화된 시공간 대조 학습. 2022년 _CVPR_에서.\n' +
      '* Yuan et al. (2023) Yuan, L., Gundavarapu, N. B., Zhao, L., Zhou, H., Cui, Y., Jiang, L., Yang, X., Jia, M., Weyand, T., Friedman, L., et al. VideoGLUE: Video general understanding evaluation of foundation models. _ arXiv preprint arXiv:2307.03166_, 2023.\n' +
      '* Zellers et al. (2021) Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. MERLOT: 멀티모달 신경망 스크립트 지식 모델. 2021년 _NeurIPS_에서.\n' +
      '*Zellers et al. (2022) Zellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., and Choi, Y. MERLOT Reserve: 시각과 언어와 소리를 통한 신경 스크립트 지식. 2022년 _CVPR_에서.\n' +
      '\n' +
      'Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. 비전 트랜스포머를 확장합니다. 2022a _CVPR_에서.\n' +
      '* Zhai et al. (2022b) Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., and Beyer, L. LiT: 잠긴 이미지 텍스트 튜닝을 통한 제로샷 전송. _CVPR_, 2022b에서.\n' +
      '* Zhang et al.(2023) Zhang, H., Li, X., and Bing, L. Video-LLaMA: 비디오 이해를 위한 명령어-조정된 시청각 언어 모델. _ arXiv preprint arXiv:2306.02858_, 2023.\n' +
      '* Zhao et al. (2024) Zhao, Y., Zhao, L., Zhou, X., Wu, J., Chu, C.-T., Miao, H., Schroff, F., Adam, H., Liu, T., Gong, B., et al. Distilling vision-language models on 수백만 비디오들. _ arXiv preprint arXiv:2401.06129_, 2024.\n' +
      '* Zhou et al. (2022) Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T. iBOT: 온라인 토큰화기로 이미지 BERT 사전 교육. 2022년 _ICLR_에서\n' +
      '* Zhou et al. (2018) Zhou, L., Xu, C., and Corso, J. Towards of procedures of web instruction video. 2018년 _AAAI_에서.\n' +
      '* Zhu et al. (2024) Zhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., Wang, H., Pang, Y., Jiang, W., Zhang, J., Li, Z., et al. LanguageBind: Extending video-language pretraining to N-modality by language-based semantic alignment. 2024년 _ICLR_에서\n' +
      '\n' +
      '## 부록 A 사전 학습 데이터\n' +
      '\n' +
      '### Data curation\n' +
      '\n' +
      '섹션 2.1과 표 1은 비디오의 사전 훈련 코퍼스에 대해 설명했으며 다음은 세 가지 사내 데이터 세트에 대한 자세한 내용을 제공한다.\n' +
      '\n' +
      '**익명-Corpus #1.** 이 사전 훈련 코퍼스는 약 36M 상업적으로 허가된 스톡 비디오-캡션 쌍으로 구성되며, 여기서 비디오 및 텍스트는 전문 기여자에 의해 수동으로 업로드된다. 따라서 이 말뭉치에서는 나머지 말뭉치에 비해 동영상과 자막의 품질이 높다.\n' +
      '\n' +
      '**익명-Corpus #2.** 이 데이터 코퍼스는 44.6M 유튜브 비디오로부터 170M(비디오, ASR 전사체) 쌍을 포함한다. 그것의 구축 과정은 HowTo100M(Miech et al., 2019)과 유사하지만 전체 말뭉치는 더 크고 다양하다. 나아가, 클립-텍스트 쌍들은 CLIP의 유사성 점수와 유사한 접지성 점수에 기초하여 필터링된다(Wu et al., 2021).\n' +
      '\n' +
      '**익명-Corpus #3.** 이 익명 말뭉치는 36.7M 유튜브 비디오에서 71.5M(클립, 머신 생성 캡션) 쌍을 포함한다. 클립들은 비전-언어 모델들(Chen et al., 2023c)을 사용하여 캡션되고, LLM(Anil et al., 2023)을 사용하여 추가로 요약된다. 코퍼스는 구성 면에서 InternVid(Wang et al., 2023e)와 유사하지만 크기와 다양성에서 크기가 더 크다.\n' +
      '\n' +
      '### Corpus analysis\n' +
      '\n' +
      '비디오-텍스트 사전 훈련 데이터에서 100K 비디오를 무작위로 샘플링하고 그림 5에 분석 결과를 보여준다. 대부분의 클립은 길이가 5~10초이고 병렬 텍스트에 10~20개의 단어가 포함되어 있음을 알 수 있다. 또한, 클립의 상당 부분은 10초보다 긴 지속 시간 또는 20단어보다 긴 캡션을 갖는다. 우리는 도 5c에서 우리의 코퍼스의 CLIP 유사성 점수(Wu et al., 2021)를 추가로 보여준다. CLIP 유사도 점수의 큰 변화는 학습 데이터의 다양한 캡션 품질을 보여주며, 이는 텍스트를 채집하는 데 사용되는 다양한 방법의 부산물이라고 믿는다.\n' +
      '\n' +
      '## 부록 B 모델 아키텍처\n' +
      '\n' +
      '표 8은 VideoPrism 모델 아키텍처를 나타낸다. 섹션 2.2에서 언급된 바와 같이, 아키텍처는 ViViT의 인수분해 설계(Arnab et al., 2021)를 따른다. 그것은 공간 모듈과 시간 모듈의 두 개의 분리된 트랜스포머 모듈로 구성된다. 입력 비디오가 여러 개의 비중첩 패치(토큰)로 분할된 후, 공간 모듈은 먼저 동일한 시간 인덱스로부터 토큰들 간의 상호 작용을 모델링한다. 그런 다음 토큰 임베딩의 출력 시퀀스가 시간 모듈을 통해 전달되어 서로 다른 시간 인덱스로부터 토큰 간의 상호 작용을 모델링한다. 시간 모듈은 가장 큰 VideoPrism 모델에 더 많은 레이어가 추가되어 성능 개선이 관찰되지 않기 때문에 레이어 수를 4개로 고정한다는 점을 제외하고는 동일한 공간 대응 장치의 설정을 공유한다. 모델들의 위치 임베딩은 학습가능하며(Devlin et al., 2019), 공간 및 시간 차원들에서 디커플링된다. 이들은 각각 시공간에서 입력 토큰들의 위치 정보를 인코딩하기 위해 사용된다. 1단계 사전 학습에 이미지 텍스트 데이터를 추가하면 이미지는 한 프레임 비디오로 처리되며 이미지 입력을 처리할 때 시간적 위치 임베딩을 크롭한다. CoCa (Yu et al., 2022)에 이어서 공간 분해능 \\(288\\times 288\\)과 패치 크기 \\(18\\times 18\\)으로 모델을 사전 훈련한다. 비디오 인코더의 시간적 위치 임베딩을 보간하여 각 비디오에서 사전 학습을 위한 \\(8\\) 프레임과 평가를 위한 \\(16\\) 프레임을 균일하게 샘플링한다.\n' +
      '\n' +
      '## 부록 C 구현 상세\n' +
      '\n' +
      '본 절에서는 VideoPrism의 구현 세부 사항과 훈련 설정에 대해 설명한다. 우리는 표 9의 사전 훈련 구성을 요약한다.\n' +
      '\n' +
      '### Stage 1\n' +
      '\n' +
      '**모델 설계.** 1단계 모델의 텍스트 인코더는 표준 트랜스포머(Vaswani et al., 2017)이다. 인코더의 공간 모듈과 함께 초기화됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l} \\hline \\hline Step & Block & Output shape \\\\ \\hline Data & - & \\(8\\times 288\\times 288\\times 3\\) \\\\ Preprocess & Patchify \\([1,18,18]\\) & \\(8\\times 256\\times 1408\\) \\\\ Drop token / Mask & Tube / BEVT & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Spatial encoder & MSA (6144) \\(\\times 40\\) & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Normalization & LayerNorm & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Transpose & Switch dimension & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Temporal encoder & MSA (6144) \\(\\times 4\\) & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Normalization & Layer Norm & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Transpose & Switch dimension & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Reshape & Merge dimension & \\([2048\\times(1-\\rho)]\\times 1408\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: VideoPrism-g.**의 **Encoder 아키텍처 출력 형태를 설명할 때, 적용 가능한 경우 {temporal, spatial, and channel}을 치수의 차수로 사용하고, 단순화를 위해 배치 크기를 생략하였다. 밑줄로 스텝이 적용되는 차원을 강조합니다. 드롭 토큰 또는 마스킹 비율 \\(\\rho\\)은 1단계에서 \\(0.5\\)으로 설정되고 2단계에서는 \\(0.65\\)으로 설정됨을 유의한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l} \\hline \\hline Configuration & Stage 1 & Stage 2 \\\\ \\hline Optimizer & AdaFactor & AdaFactor \\\\ Base learning rate & \\(5\\times 10^{-4}\\) & \\(5\\times 10^{-4}\\) \\\\ Learning rate schedule & linear decay & cosine decay \\\\ Warmup iterations & \\(2\\times 10^{4}\\) & \\(2.5\\times 10^{4}\\) \\\\ Training iterations & \\(2\\times 10^{5}\\) & \\(3\\times 10^{5}\\) \\\\ Weight decay & \\(1\\times 10^{-4}\\) & \\(0.05\\) \\\\ Batch size & \\(4096\\) & \\(4096\\) \\\\ Drop token or Mask & \\(0.5\\) (Tube mask) & \\(0.65\\) (BEVT mask) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: ** our preraining configuration의 요약.**the unimodal text decoder of CoCa(Yu et al., 2022). 비디오 인코더의 끝단에 MAP 계층(Lee et al., 2019; Yu et al., 2022)을 부착하여 인코더 출력으로부터 전역 임베딩을 추출한다. 텍스트 인코더는 입력 문장의 끝에 학습 가능한 클래스 토큰을 추가하고 해당 출력을 텍스트 임베딩으로 사용한다.\n' +
      '\n' +
      '**Training.** 배치 혼합을 사용하는 기존의 방법들과 대조적으로, 우리는 다수의 데이터 세트로 우리의 1단계 모델을 대조적으로 훈련시키기 위해 교대 경사 하강(AGD)(Jain et al., 2017)을 채택한다. 트레이닝 동안 미니-배치로서 상이한 데이터세트로부터의 샘플을 교체하며, 멀티-태스크 및 멀티-데이터세트 시나리오에서 효과적인 것으로 나타났다(Akbari et al., 2023). 이는 동일한 데이터 세트의 샘플이 일반적으로 동일한 분포를 따르고 구별하기 어렵기 때문에 배치 내에서 쉬운 음수를 피하는 데 특히 유용하다. 또한, 더 많은 데이터 세트를 추가하거나 말뭉치의 크기를 증가시킬수록 AGD 접근법이 잘 확장된다는 것을 관찰한다.\n' +
      '\n' +
      '1단계 모델의 트레이닝은 비전 언어 대비 학습의 종래의 셋업을 따른다(Radford et al., 2021). 프리트레이닝 시 메모리 비용을 줄이기 위해 Li et al.(2023)과 같이 비디오 토큰의 \\(50\\%\\)을 드롭하고, 토큰 드롭을 위해 튜브 마스킹 전략(Feichtenhofer et al., 2022)을 사용한다. 교사 모델은 배치 크기가 \\(4096\\)인 Adafactor (Shazeer and Stern, 2018)를 사용하여 최적화된다. 학습률은 기본 모델의 경우 \\(1\\times 10^{-4}\\), 거대 모델의 경우 \\(5\\times 10^{-5}\\)으로 설정하였다. 본 논문에서는 1단계 모델을 학습하여 1단계 모델인 2(2)×10^{5}\\(2)×10^{4}\\(2)×10^{4}\\(2)×10^{5}}\\(2)×10^{5}\\(2)×10^{4}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{4}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{4}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{4}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{5}\\(2)×10^{4}\\(2)×10^{5}\\(2) 1단계 훈련에는 대칭 교차 엔트로피 손실(Gutmann and Hyvarinen, 2010; Radford et al., 2021; Jia et al., 2021; Cheng et al., 2023)이 사용된다.\n' +
      '\n' +
      '### Stage 2\n' +
      '\n' +
      '**토큰wise 증류.** 섹션 2.3.2에서 논의한 바와 같이 대조적 학습으로 1단계 모델을 훈련한 후 1단계 모델에서 시공간 임베딩을 재구성하기 위해 마스킹 모델링이 있는 VideoPrism 비디오 인코더를 훈련한다. 도 3에 도시된 바와 같이, 제2 스테이지의 트레이닝 파이프라인은 MVD(Wang et al., 2023)와 유사하다. 입력된 비디오 시퀀스를 토큰 집합에 패치한 후, 마스킹 비율 \\(0.65\\)을 갖는 BEVT 마스킹(Wang et al., 2022)을 적용하여 토큰들 중 일부를 랜덤하게 제거한다. 1단계 인코더로부터 초기화된 2단계 비디오 인코더는 나머지 가시 토큰들을 입력으로 하고 그들의 임베딩을 예측한다. 이어서, 학습가능한 마스크 토큰이 마스킹된 토큰들의 위치를 채우기 위해 사용되어 이러한 가시적 임베딩들과 함께 전체 시퀀스를 형성한다. 그런 다음 임베딩의 전체 시퀀스를 무작위로 셔플링하고 위치 임베딩과 함께 추가한 후 4층 트랜스포머인 얕은 디코더에 공급한다. 그런 다음 선형 계층을 사용하여 디코더의 출력을 코사인 거리를 최소화하여 1단계 비디오 인코더의 임베딩과 정렬한다. 알고리즘 1은 마스킹된 비디오 모델링을 위해 제안된 토큰 셔플링의 의사 코드 구현을 제시한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l l} \\hline \\hline \\multirow{2}{*}{Step} & \\multirow{2}{*}{Block} & \\multicolumn{2}{c}{Decoder output shape} \\\\  & & Local & Global \\\\ \\hline \\multirow{2}{*}{Data} & - & \\(2048\\times 1408\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(1408\\) \\\\  & MLP & \\(2048\\times 512\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(512\\) \\\\  & MSA (2048) \\(\\times\\)\\(512\\) & \\(2048\\times 512\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(512\\) \\\\  & MLP & \\(2048\\times 1408\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(1408\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: VideoPrism-g**의 **디코더 아키텍처. 밑줄로 스텝이 적용되는 차원을 강조합니다. 마스킹 비율 \\(\\rho\\)은 \\(0.65\\)으로 설정됨을 유의한다.\n' +
      '\n' +
      '도 5: **비디오-텍스트 사전 훈련 코퍼스에 대한 분석.**\n' +
      '\n' +
      '**글로벌 증류.** 1단계 모델에서 글로벌 비주얼 임베딩을 증류하기 위해 4단계 트랜스포머 디코더와 MAP 레이어를 사용하여 2단계 비디오 인코더에서 보이는 임베딩을 입력으로 하고 글로벌 임베딩을 출력한다. 우리는 이 디코더에 토큰 셔플링을 적용하거나 위치 임베딩을 추가하지 않는다. 그런 다음 코사인 거리 손실을 사용하여 이 2단계 전역 임베딩을 1단계 모델의 전역 시각적 임베딩에 정렬한다. 1단계 모델의 전역적 시각적 임베딩은 1단계 대비 훈련의 동일한 MAP 헤드에 의해 예측된다는 점에 유의한다. 표 10은 이 단계에서의 디코더 아키텍처를 나타낸다.\n' +
      '\n' +
      '**Training.** 이미지 기반 데이터셋인 WebLI(Chen et al., 2023c)를 제외하고 1단계 모델에 대해 동일한 비디오 클립을 이용하여 2단계 비디오 인코더를 학습한다. 최적화를 위해 Adafactor(Shazeer and Stern, 2018)를 사용한다. 2단계 비디오 인코더는 배치 크기(4096\\)와 시작 학습률(5\\times 10^{-4}\\)로 학습된다. 학습률은 코사인 어닐링 스케줄에 의해 \\(1\\times 10^{-5}\\)으로 감쇠된다. \\ (2.5\\times 10^{4}\\) 워밍업 단계 또한 초기에 학습률을 \\(0\\)에서 \\(5\\times 10^{-4}\\)으로 선형적으로 증가시키기 위해 사용된다. 2단계 비디오 인코더는 \\(3\\times 10^{5}\\) 단계로 학습된다. 2단계 훈련에서 토큰별 증류 손실과 전역적 증류 손실에 대해 동일한 가중치를 적용한다.\n' +
      '\n' +
      '## 부록 D 평가 데이터\n' +
      '\n' +
      '표 11은 본 논문에서 평가에 사용된 모든 데이터 세트와 해당 메트릭을 요약한 것이다. 평가 데이터 세트는 일반적인 비디오 전용 이해(VideoGLUE(Yuan et al., 2023)), 비디오 텍스트 검색, 캡션 & QA, 과학용 CV의 네 부분으로 분류된다. 각 범주 내에서 대표 데이터 세트를 선택하고 각각에 대한 표준 메트릭을 보고한다.\n' +
      '\n' +
      '비디오 프리즘의 성능을 그림 2의 가장 성능이 좋은 기초 모델과 비교한다. 각 데이터 세트 및 작업에 대해 이미지 또는 비디오 기초 모델이 달성한 가장 성능이 좋은 보고 수에 대한 성능 이득(\\(\\Delta\\)Score)을 계산한다. 우리는 그것들을 모두 모아서 내림차순으로 플롯한다.\n' +
      '\n' +
      '## 부록 E VideoGLUE\n' +
      '\n' +
      'VideoPrism을 위한 태스크 및 태스크 헤드\n' +
      '\n' +
      '우리는 비디오-전용 평가들을 위한 VideoGLUE(Yuan et al., 2023) 설정을 따른다. 비디오 프리즘은 형태\\(T\\times H\\times W\\times 3\\)의 비디오 클립이 주어졌을 때, 형태\\(T\\times\\frac{H}{18}\\times\\frac{W}{18}\\times D\\)의 시각적 토큰 집합을 생성하는데, 여기서 \\(T\\), \\(H\\) 및 \\(W\\)은 각각 프레임 수, 이미지 높이 및 이미지 너비이고 \\(D\\)은 특징 길이이다.\n' +
      '\n' +
      '모든 비디오 분류 작업에서, 태스크 헤드로 MAP(multi-head attention pooling) 레이어를 사용하며, 이 레이어는 \\(12\\) 헤드와 숨겨진 크기 \\(768\\)의 Transformer 레이어로 구성된다. 클래스 토큰은 최종 분류를 위해 VideoPrism의 모든 시각적 토큰에 교차 출석하도록 미리 준비됩니다. 작업 헤드를 훈련할 때 배치 크기\\(256\\)를 사용합니다. VideoGLUE에 기술된 바와 같이 각각의 개별 데이터 세트에 대해 동일한 데이터 증강 전략 및 트레이닝 레시피를 적용하고 멀티뷰 평가를 수행한다.\n' +
      '\n' +
      '시공간 액션 로컬화는 입력 비디오에서 인물 인스턴스들을 로컬화하고 그들의 액션들을 인식할 것을 요구한다. 우리의 실험에서, 인스턴스 레벨 특징들은 대응하는 인스턴스 박스들을 사용함으로써 시각적 토큰들로부터 먼저 RoIPooled(Ren et al., 2015)된다. 그런 다음 이러한 기능은 교차 주의 계층을 통해 다른 모든 시각적 토큰을 쿼리하는 데 사용됩니다. 작업 헤드로는 \\(12\\) 헤드와 숨겨진 크기 \\(768\\)의 트랜스포머 레이어를 사용한다. 최종 쿼리 토큰들은 선형 분류기를 통해 분류된다. 우리는 훈련을 위해 관련 작업 레이블과 함께 지상 진실 인스턴스 상자를 사용한다. 테스트 시간에는 AVA 상의 사람 검출을 위해 Feicintenhofer et al.(2019)과 동일한 사전 훈련된 사람 검출기를 사용한다. VA-Kinetics 상에서, 우리는 Li 등(2020)에 기술된 검출기를 사용한다. 배치크기가 256\\(256\\)인 모델을 학습하였다.\n' +
      '\n' +
      '시간적 액션 로컬리제이션을 위해, 긴 비디오 샘플은 종단간 튜닝을 허용하지 않기 때문에, 우리는 동결 및 다층 주의 풀러(MLAP) 설정 하에서 VideoPrism만을 적용한다. MLAP 설정에서 우리는 특징을 풀링하여 G-TAD 헤드(Xu et al., 2020)에 입력한다. 우리는 배치 크기\\(32\\)를 사용하고 ActivityNet v1.3에서 G-TAD를 10\\ 에폭에 대해 훈련한다.\n' +
      '\n' +
      '우리는 모든 비디오 전용 실험에서 AdamW(Loshchilov and Hutter, 2019) 최적화기와 코사인 학습 속도 감쇠를 사용한다. 실험 설정에 대한 자세한 내용은 VideoGLUE 논문(Yuan et al., 2023)을 참조하십시오.\n' +
      '\n' +
      '다양한 입력 비디오 크기에 대해 두 가지 구성 하에서 VideoPrism을 실험한다. 첫 번째 구성(표 \\(12\\) ~ \\(15\\)에서 별표 "*"로 표시됨)에서 비디오 분류 작업 헤드를 훈련할 때 특징 추출을 위해 \\(8\\) 프레임과 \\(252\\times 252\\) 이미지 해상도를 사용하며, 이는 \\(8\\times 14\\times 14\\) 토큰의 시퀀스를 생성한다. AVA와 AVA-Kinetics에서 모양 \\(8\\times 288\\times 288\\)(_i.e._, 토큰 길이 \\(8\\times 16\\times 16\\))의 비디오 클립은 훈련과 평가 모두에 사용된다. 이러한 구성은 VideoPrism의 훈련가능한 FLOP들을 Yuan 등에 보고된 다른 베이스라인 모델들과 정렬시킨다(2023). 두 번째 구성에서는 모든 실험에 입력으로 모양 \\(16\\times 288\\times 288\\)의 비디오 클립을 사용한다. 이 구성은 더 높은 훈련 가능한 FLOP를 사용하는 사전 훈련 설정과 일치하며 섹션 3.1의 결과를 설명한다.\n' +
      '\n' +
      '### Adaptations\n' +
      '\n' +
      '우리는 Yuan et al. (2023)을 따라 4가지 적응 설정, 즉 간단한 MAP 헤드를 가진 동결 모델 백본, MLAP 헤드를 가진 모델 성능을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Datasets & \\multicolumn{2}{c|}{Tasks} & \\multicolumn{2}{c|}{Zero-shot} & \\multicolumn{1}{c}{Abbr.} & \\multicolumn{1}{c}{Metrics} \\\\ \\hline Kinetics-400 (Kay et al., 2017) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ MiT (Momfort et al., 2019) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ SS2G (Goyal et al., 2017a) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ Dwingles (Li et al., 2018) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ Charades (Sigurdsson et al., 2016) & Video Classification & ✗ & VC & mAP \\\\ ActivityNet (Caho Heilbron et al., 2015) & Temporal Action Localization & ✗ & TAL & mAP \\\\ AVA (Gu et al., 2018) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ AVA-Kinetics (Li et al., 2020) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ \\hline MSRVT (Xu et al., 2016) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ MSRVTT (Xu et al., 2016) & Video-to-Text Retrieval & ✓ & 2S2VT & Recall@1, Recall@1 \\\\ VATEX (Wang et al., 2019) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ VATEX (Wang et al., 2019) & Video-to-Text Retrieval & ✓ & 2S2VT & Recall@1, Recall@5 \\\\ ActivityNet (Caho Heilbron et al., 2015) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ Kinetics-400 (Xay et al., 2017) & Video Classification & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ Kinetics-600 (Carreira et al., 2018) & Video Classification & ✓ & 2SC & Top-1 \\& Rep-5 Acc. \\\\ SS2-Temporal (Sevilla-Lara et al., 2021) & Video Classification & ✓ & ZSC & Top-1 Acc. \\\\ SS2-Events (Bagd et al., 2023) & Video Classification & ✓ & ZSC & Top-1 Acc. \\\\ NEX-QA (Atrp-Hard) (Xiao et al., 2021) & Video Classification & ✓ & ZSC & MC Acc. \\\\ Charades (Sigurdsson et al., 2016) & Video Classification & ✓ & ZSC & mAP \\\\ Charades-STA (Gao et al., 2017) & Video Classification & ✓ & ZSC & MC Acc. \\\\ \\hline MSRVT (Xu et al., 2016) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ VATEX (Wang et al., 2019) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ VooCoCo2 (Zhou et al., 2018) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ MSRTT-QA (Xia et al., 2017) & Video Question Answering & ✓ & ZSMa & Top-1 Acc. \\\\ MSVD-QA (Xia et al., 2017) & Video Question Answering & ✓ & ZSMa & WUPS \\\\ NEXT-QA (Xiao et al., 2021) & Video Classification & ✗ & VC & mAP \\\\ \\hline Fly vs. Fly (Eyjöldsottint et al., 2014) & Video Classification & ✗ & VC & mAP \\\\ CaTM21 (Sun et al., 2021) & Video Classification & ✗ & VC & mAP \\\\ CRIM13 (Sdee view) (Burgos-Arrizzu et al., 2012) & Video Classification & ✗ & VC & mAP \\\\ CRIM13 (Top view) (Burgos-Arrizzu et al., 2012) & Video Classification & ✗ & VC & mAP \\\\ KABR (Chulavchenko et al., 2024) & Video Classification & ✗ & VC & Macro Acc. \\\\ ChimpACT (Ma et al., 2023) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: **평가 데이터셋 요약.** 분류 및 질의응답을 위한 Top-1/5 Accuracy(Acc.), 다중 레이블 분류를 위한 평균 평균 Precision(mAP), 검색을 위한 Recall@1/5, 다중 선택 검색을 위한 다중 선택 검색 정확도(MC Acc.), 캡셔닝을 위한 CIDEr 점수, 질문 응답을 위한 Wu-Palmer Similarity(WUPS) 인덱스, KABR 데이터셋을 위한 매크로 정확도(Macro Acc.) 등 각 데이터셋에 대한 해당 표준 메트릭을 보고한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline  & VC (A) & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{1}{c}{Trainable} \\\\ Methods & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 75.2 & 32.6 & 41.0 & 44.1 & 11.2 & 32.7 & 21.1 & 25.9 & 3.72 \\\\ VATT (Akhavi et al., 2021) & 75.1 & 32.1 & 57.8 & 49.7 & 33.3 & 35.3 & 20.3 & 22.2 & 3.72 \\\\ CoCo-B (Vu et al., 2022) & 73.1 & 32.0 & 41.5 & 34.1 & 8.8 & 33.0 & 23.3 & 24.7 & 3.72 \\\\ FLAVA-B (Singh et al., 2022) & 71.3 & 29.7 & 40.6 & 45.9 & 12.6 & 32.2 & 18.8 & 21.5 & 3.72 \\\\ VideoAdNE-B (Tong et al., 2022) & 65.1 & 23.0 & 53.9 & 59.5 & 11.3 & 33.0 & 16.0 & 19.9 & 3.72 \\\\ InterVideo-B (Wang et al., 2022) & 69.3 & 26.3 & 58.2 & 55.6 & 13.0 & 33.3 & 13.4 & 15.7 & 3.72 \\\\ UMT-B (Li et al., 2023b) & 77.1 & 34.0 & 47.7 & 47.8 & 30.1 & 35.8 & 20.7 & 21.1 & 3.72 \\\\\n' +
      '**VideoPrism-B\\({}^{*}}** & 82.8 & 40.0 & 61.8 & 59.5 & 38.7 & 36.6 & 29.9 & 32.0 & 3.72 \\\\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.2 & 40.8 & 63.6 & 67.4 & 40.4 & 36.6 & 30.6 & 31.8 & 9.71 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: **비디오 이해 태스크들에 대한 동결된 특징들을 이용한 FM 적응의 결과.** 모델 백본들은 동결되고, 다운스트림 태스크들의 트레이닝 세트들을 이용하여 태스크 헤드들 내의 가중치들만이 업데이트된다. \\ ({}^{*}\\)는 모델이 훈련 가능한 FLOP 정렬로 설정에서 평가됨을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c|}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 81.0 & 39.0 & 46.6 & 75.7 & 54.3 & - & 27.1 & 28.9 & 367 \\\\ VATT-B (Akbari et al., 2021) & 77.1 & 34.8 & 65.1 & 77.6 & 55.7 & - & 27.0 & 28.4 & 371 \\\\ CoA-B (Wu et al., 2022) & 82.6 & 43.6 & 66.8 & 79.6 & 55.0 & - & 27.7 & 31.0 & 367 \\\\ FLAVA-B (Singh et al., 2022) & 79.1 & 38.3 & 61.1 & 72.0 & 48.6 & - & 22.0 & 25.6 & 367 \\\\ VideoMAE-B (Tong et al., 2022) & 78.7 & 36.1 & 65.5 & 75.5 & 51.4 & - & 23.5 & 26.2 & 367 \\\\ InterVideo-B (Wang et al., 2022) & 80.1 & 35.9 & 67.0 & 75.8 & 52.2 & - & 27.2 & 29.8 & 367 \\\\ UMT-B (Li et al., 2023b) & 83.3 & 38.7 & 67.0 & 79.2 & 57.1 & - & 28.8 & 30.9 & 367 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.4 & 43.9 & 68.2 & 82.3 & 58.1 & - & 33.3 & 35.3 & 374\\\\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 85.7 & 44.2 & 70.0 & 84.9 & 60.1 & - & 33.4 & 35.9 & 977 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 16: **4개의 적응 방법 및 최종 VGS에 따른 계층화된 평균 점수. \\ ({}^{*}\\)는 모델이 훈련 가능한 FLOP 정렬로 설정에서 평가됨을 나타낸다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c|}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 77.1 & 39.0 & 50.1 & 55.8 & 41.5 & 33.9 & 27.7 & 29.6 & 14.9 \\\\ VATT-B (Akbari et al., 2021) & 75.1 & 35.6 & 58.7 & 60.1 & 58.2 & 35.0 & 22.9 & 24.1 & 14.9 \\\\ CoCa-B (Yu et al., 2022) & 74.2 & 37.2 & 45.9 & 48.4 & 19.6 & 33.3 & 24.4 & 27.0 & 14.9 \\\\ FLAVA-B (Singh et al., 2022) & 71.5 & 34.5 & 43.1 & 58.5 & 38.2 & 32.4 & 21.3 & 23.2 & 14.9 \\\\ VideoMAE-B (Tong et al., 2022) & 71.7 & 32.2 & 57.4 & 69.6 & 35.9 & 33.4 & 19.6 & 22.1 & 14.9 \\\\ InterVideo-B (Wang et al., 2022c) & 73.7 & 34.7 & 60.3 & 71.9 & 40.5 & 33.6 & 15.9 & 17.7 & 14.9 \\\\ UMT-B (Li et al., 2023b) & 77.5 & 38.0 & 51.2 & 55.5 & 55.8 & 36.0 & 24.6 & 25.8 & 14.9 \\\\\n' +
      '**VideoPrism-B\\({}^{*}}** & 83.7 & 43.9 & 64.6 & 70.7 & 56.6 & 37.2 & 31.5 & 33.1 & 14.9 \\\\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.5 & 43.8 & 66.3 & 73.6 & 58.6 & 37.2 & 31.4 & 33.0 & 38.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: ** 비디오 이해 작업에 대한 MLAP 헤드가 있는 냉동 백본을 사용한 FM 적응의 결과. MLAP는 FM에서 여러 개의 냉동 기능을 입력으로 사용하여 최종 작업 예측을 위해 계층적으로 매핑한다. 다운스트림 작업의 훈련 세트를 사용하여 MLAP 계층 가중치만 업데이트됩니다. \\ ({}^{*}\\)는 모델이 훈련 가능한 FLOP 정렬로 설정에서 평가됨을 나타낸다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 80.2 & 39.7 & 56.0 & 77.2 & 44.2 & - & 24.5 & 28.0 & 6.44 \\\\ VATT-B (Akbari et al., 2021) & 75.0 & 36.5 & 63.5 & 68.9 & 53.5 & - & 22.3 & 25.8 & 6.44 \\\\ CoCa-B (Yu et al., 2022) & 80.9 & 41.4 & 56.1 & 67.1 & 45.8 & - & 26.6 & 28.7 & 6.44 \\\\ FLAVA-B (Singh et al., 2022) & 74.7 & 34.1 & 52.1 & 68.4 & 40.8 & - & 17.9 & 23.8 & 6.44 \\\\ VideoMAE-B (Tong et al., 2022) & 73.6 & 30.6 & 61.4 & 76.0 & 43.0 & - & 16.6 & 23.3 & 6.44 \\\\ InterVideo-B (Wang et al., 2022c) & 75.5 & 31.3 & 63.9 & 73.6 & 46.2 & - & 19.2 & 25.5 & 6.44 \\\\ UMT-B (Li et al., 2023b) & 81.5 & 40.4 & 61.8 & 78.5 & 50.0 & - & 27.8 & 29.4 & 6.44 \\\\\n' +
      '**VideoPrism-B\\({}^{*}}** & 84.5 & 44.0 & 66.3 & 83.0 & 57.8 & - & 33.6 & 35.7 & 8.71 \\\\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 85.7 & 43.9 & 68.8 & 85.1 & 60.6 & - & 34.1 & 35.8 & 22.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 14: ** 저순위 어댑터 및 작업 헤드가 있는 냉동 백본을 사용한 FM 적응의 결과. 하위 어댑터와 작업 헤드의 가중치만 다운스트림 작업의 훈련 세트를 사용하여 업데이트됩니다. \\ ({}^{*}\\)는 모델이 훈련 가능한 FLOP 정렬로 설정에서 평가됨을 나타낸다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 80.2 & 39.7 & 56.0 & 77.2 & 44.2 & - & 24.5 & 28.0 & 6.44 \\\\ VATT-B (Akbari et al., 2021) & 75.0 & 36.5 & 63.5 & 68.9 & 53.5 & - & 22.3 & 25.8 & 6.44 \\\\ CoCa-B (Yu et al., 2022) & 80.9 & 41\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '(K600)(Carreira et al., 2018) 이 절에서. 표 17에 나타난 바와 같이, 우리는 VideoPrism이 시각 및 언어 양식으로 사전 훈련된 최신 FM에 비해 최상의 결과를 달성한다는 것을 알 수 있다. Language-Bind(Zhu et al., 2024) 및 IMP(Akbari et al., 2023)가 프리트레이닝 동안 추가적인 모달리티들(_e.g._, 오디오)을 사용하지만, 우리의 결과는 여전히 이들에 필적한다. 더 중요한 것은 기본 규모 모델이 훨씬 더 큰 규모로 대부분의 방법을 능가할 수 있다는 것이다. 이러한 관찰은 본문의 표 4에서 그린 관찰과 일치한다.\n' +
      '\n' +
      '## 부록 G Gluluing VideoPrism with PaLM-2\n' +
      '\n' +
      '섹션 3.3에서는 비디오 캡션 및 비디오 QA와 같은 언어로 생성되는 작업에 대해 좋은 성능을 위해 추가 훈련 단계에서 미리 훈련된 LLM 디코더와 쉽게 융합할 수 있음을 보여줌으로써 VideoPrism의 강도와 일반화 가능성에 대한 증거를 제공했다. 우리는 모델 훈련과 평가 프로토콜에 대한 세부 사항을 다음과 같이 제공합니다.\n' +
      '\n' +
      '**구현.** 입력 비디오를 나타내는 고정된 수의 연속 토큰을 출력하는 One-layer Perceiver Resampler(Alayrac et al., 2022)를 통해 비디오 인코더의 특징을 전달한다. 실험에서는 항상 \\(256\\)으로 설정되었다. 그런 다음 이러한 토큰들은 내장된 텍스트 프롬프트에 프리펜딩되고 LLM 디코더, _i.e._, PaLM-2(Anil et al., 2023)에 공급된다. 그런 다음 재샘플링된 피쳐는 스킵 연결을 통해 원래 쿼리 피쳐와 추가됩니다. 본래의 구현과는 두 가지 차이점이 있음에 유의한다(Alayrac et al., 2022). 첫째, 질의와 주요 기능에는 공유 LayerNorm보다 별도의 LayerNorm을 사용한다. 둘째, VideoPrism의 특징치수가 사전학습된 PaLM-2와 다르기 때문에 핵심특징과 질의특징을 연결하지 않는다. 그렇지 않으면, 특징치수는 연결되기 전에 선형 투영 레이어를 통해 투영되어야 하며, 불안정한 학습으로 이어진다는 것을 알 수 있다. 본 논문에서는 서로 다른 수의 Resampler layer (_i.e., \\(1\\), \\(3\\), \\(6\\))를 이용하여 제거하였으며, 1-layer Resampler가 가장 잘 동작함을 확인하였다.\n' +
      '\n' +
      '**모델 트레이닝.** 우리는 이 멀티모달 모델을 표준 자기회귀 언어 모델링 손실을 사용하여 사전 트레이닝 단계로부터의 비디오-텍스트 캡션 데이터, 집계된 Academic-Corpus, 및 VQAv2(Goyal et al., 2017)(이미지 QA 데이터세트)의 조합에 대해 트레이닝한다. 표 18은 Ego4D(Grauman et al., 2022), EPIC-Kitchens(Dima et al., 2022), Spoken Moments In Time(Monfort et al., 2021), _etc._ 총 4.4M 동영상입니다.\n' +
      '\n' +
      '비디오 인코더와 LLM은 모두 트레이닝 동안 완전히 동결된 상태로 유지되며, 오직 하나의 레이어 리샘플러만이 최적화된다. 우리는 PaLM-2-1B와 PaLM-2-8B를 별도로 사용하여 VideoPrism-B를 훈련한다. 배치크기는 PaLM-2-1B의 경우 \\(256\\), PaLM-2-8B의 경우 \\(64\\)으로 설정하고 \\(2\\times 10^{5}\\) 단계로 학습하였다. 체중감퇴(1\\times 10^{-4}\\)를 갖는 Adam optimizer(Kingma and Ba, 2015)를 사용하였으며, 학습률은 warmup 단계 \\(1\\times 10^{-4}\\)로 \\(5\\times 10^{-4}\\)에서 최고치를 보인 후 선형적으로 감소하였다. Beta1은 \\(0.9\\)으로 설정되고 Beta2는 \\(0.999\\)으로 설정된다. 우리는 훈련에서 EMA 붕괴, L2 레귤러라이저 중량 붕괴 및 구배 클리핑을 설정하지 않는다. 각 프레임은 훈련 중에 임의로 \\(288\\)으로 크롭되기 전에 \\(346\\)으로 중앙 크롭되고 평가 중에 \\(288\\)으로 중앙 크롭된다. 본 연구의 데이터 셋은 비교적 짧은 해답을 가지고 있기 때문에 최대 복호화 단계를 \\(32\\)으로 설정하였다. 우리는 모든 실험을 위해 탐욕스러운 디코딩을 사용한다.\n' +
      '\n' +
      '각주 1: 우리가 사용한 최종 텍스트 전용 2발 프롬프트는 “질문: 누가 그의 가족과 이야기하고 있는가? 대답: 남자”이다. _ 및 “_question: 한 여성이 무엇을 하고 있는가? answer: talk._” on MSRVTT-QA, and “_question:Who using wrench on the pipe fitting? answer: man._” 그리고 “_질문: 누가 달걀을 그릇에 깨뜨리는가? 대답: 여자._” MSVD-QA.\n' +
      '\n' +
      '**모델 평가.** 표 19의 MSRVTT-QA 및 MSVD-QA에 대한 개방-어휘 및 폐쇄-어휘 평가 결과를 모두 보고한다. 개방-어휘 구성에 대해, 플라밍고의 제로-샷 접근법을 채택하고(Alayrac et al., 2022) 각각의 다운스트림 데이터세트 상의 트레이닝 세트로부터 2-샷 텍스트-전용 프롬프트를 사용한다. 두 개의 텍스트 전용 프롬프트를 사용하면 답변의 출력 스타일을 안내할 수 있습니다. 다음 프로세스를 사용하여 각 데이터 세트에 대한 두 개의 샷 프롬프트를 선택합니다. 먼저 데이터셋의 학습 집합에서 가장 일반적인 두 가지 답을 선택한 후 각각에 대해 해당 답이 있는 학습 집합에서 무작위로 질문을 뽑는다. Flamingo-9B와 비교하여 PaLM-2-8B가 있는 VideoPrism-B는 MSRVTT-QA와 MSVD-QA에서 각각 절대 \\(11.1\\%\\)과 \\(12.5\\%\\)의 이득을 보인다.\n' +
      '\n' +
      '각주 2: 우리가 사용한 최종 텍스트 전용 2발 프롬프트는 “질문: 누가 그의 가족과 이야기하고 있는가? 대답: 남자”이다. _ 및 “_question: 한 여성이 무엇을 하고 있는가? answer: talk._” on MSRVTT-QA, and “_question:Who using wrench on the pipe fitting? answer: man._” 그리고 “_질문: 누가 달걀을 그릇에 깨뜨리는가? 대답: 여자._” MSVD-QA.\n' +
      '\n' +
      '또한, MSRVTT-QA 및 MSVD-QA의 경우, Li 등(2022), Yang 등(2022)에 따라 폐쇄어휘 평가 구성을 실험한다. 이 경우, 언어 디코더를 통해 직접 답변을 출력하는 대신 디코더의 로그 우도를 이용하여 후보 답변을 점수화하고, 가장 높은 점수를 갖는 답변을 선택한다. 상기 후보 답변은,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Datasets & \\# of clips \\\\ \\hline Video Story Telling (Huang et al., 2016) & 3K \\\\ TACoS (Regneri et al., 2013) & 4K \\\\ YouDescribe (Pücher-Cooper et al., 2023) & 19K \\\\ Charades (Sigurdsson et al., 2016; Gao et al., 2017) & 20k \\\\ COIN (Tang et al., 2019) & 24K \\\\ VITT (Huang et al., 2020) & 35K \\\\ VLN (Voigtlaender et al., 2023) & 37K \\\\ EPIC-Kitchens-100 (Dima et al., 2022) & 67K \\\\ Spoken Moments in Time (Monfort et al., 2021) & 481K \\\\ Ego4D (Grauman et al., 2022; Lin et al., 2022) & 3.8M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 18: Academic-Corpus.**top-\\(K\\)에 포함된 **Datasets.**top-\\(K\\)이 데이터세트의 훈련 및 검증 세트에서 가장 자주 나타나는 One-token 답변이며, 여기서 \\(K\\)은 값 \\(\\{100,250,500,1000,2000\\}\\)을 제거함으로써 검증 세트보다 최적화된다. MSRVTT-QA와 MSVD-QA 모두 \\(K=250\\)이 가장 잘 작동하는 것으로 나타났다. 근본적 답변이 후보 답변들 중 하나가 아닌 임의의 예는 자동으로 부정확한 것으로 마킹된다. 이 방법은 또한 특정 데이터 세트의 정확한 스타일에 맞는 답변을 향해 모델을 조종하고 성능을 더욱 향상시킨다. 폐쇄 어휘 평가 구성에서 PaLM-2-8B를 사용한 VideoPrism-B는 MSRVTT-QA 및 MSVD-QA에서 각각 \\(1.2\\%\\) 및 \\(4.4\\%\\)의 절대 마진으로 FrozenBiLM-L을 능가한다.\n' +
      '\n' +
      '최근, 다수의 작업 [11, 12, 13]은 LLM-in-the-loop 프로토콜을 사용하여 캡션 및 VideoQA 태스크를 평가하기 시작했으며, 여기서 ChatGPT3와 같은 LLM은 예측들을 다수의 상이한 차원들(_e.g._, 정보의 정확성, 시간적 이해, 일관성)을 따라 지상-진실 답변들과 비교하는 데 사용된다. 이는 정확 매칭 및 BLEU 점수와 같은 메트릭이 표면적 토큰 매칭에 과도하게 의존하는 문제를 완화시키는 데 도움이 될 수 있다. 우리는 이러한 새로운 프로토콜을 사용하여 이러한 모델과 비교하는 향후 작업에 맡긴다.\n' +
      '\n' +
      '각주 3: [https://chat.openai.com](https://chat.openai.com)\n' +
      '\n' +
      '## 부록 H CV for Science\n' +
      '\n' +
      'E.1절에서 비디오GLUE 태스크와 동일한 특징 추출 설정(MAP 프로빙)을 사용하여 동결된 특징을 사용하여 과학 데이터 세트에 대한 CV를 평가한다. 데이터 세트는 Fly 대 Fly이다. 플라이 비디오 분류를 위한 플라이 [10], 탑 뷰로부터의 마우스 비디오 분류를 위한 CalMS21 [13], 탑 뷰 및 사이드 뷰를 갖는 마우스 비디오 분류를 위한 CRIM13 [12], 침팬지 시공간 액션 로컬화를 위한 침팩 [13], 케냐 동물을 갖는 비디오 분류를 위한 KABR [14]. 본 논문에서 보고된 도메인 전문가 모델은 각 데이터 세트의 훈련 분할에 대해 훈련되며, Fly 대 Fly에 대해 원래 태스크 프로그래밍[13]에서 보고된다. Fly, CalMS21의 경우 추가 라벨이 없는 데이터[13], KABR의 경우 KABR X3D[14], ChimpACT의 경우 ChimpACT SlowFast[13]가 있는 CalMS21 1D ConvNet. 각 데이터 세트에 대해 동일한 메트릭(클래스에 걸쳐 평균화된 매크로 정확도를 사용하는 KABR을 제외한 모든 작업에 대해 mAP)을 사용하여 기존 작업에 의해 정의된 열차와 테스트 분할을 사용한다. Fly vs. Fly, 우리는 [13]에서 정의된 데이터 분할을 사용하는데, 이는 훈련 세트에서 \\(1000\\) 프레임 이상의 주석이 있는 모든 동작을 포함한다. 우리는 이전 작업[13, 12]에 이어 배경 클래스가 있는 데이터 세트에서 메트릭은 관심 행동(배경 클래스를 포함하지 않음) 전체에 걸쳐 평균화된다는 점에 주목한다.\n' +
      '\n' +
      '우리는 각 데이터 세트의 원래 FPS에서 비디오에서 모든 프레임을 추출한다. Fly 대 Fly의 입력으로 \\(16\\) 프레임을 사용한다. Fly, CalMS21, CRIM13, ChimpACT의 경우 \\(64\\) 프레임, KABR의 경우 \\(5\\) 보폭의 \\(16\\) 프레임, 기준선을 따른다. 침팩트의 경우 벤치마크는 훈련 및 테스트 중에 지상진실 경계 상자를 사용하며, 이는 우리가 따른다.\n' +
      '\n' +
      '학습 설정 및 구현 세부 사항은 부록 E.1의 VideoGLUE 동결-백본 설정(MAP 프로빙)과 유사하며, 과학 실험을 위해 CV에서 AdamW [14] 최적화기와 코사인 학습 속도 감쇠를 사용한다. 데이터 증강을 위해 비디오 분류 데이터 세트에 대해 VideoGLUE(예:_e.g._, Charades, Diving48 및 MiT)의 다른 비디오 분류 데이터 세트와 동일한 것을 사용한다. ChimpACT(시공간 액션 로컬리제이션)의 경우 AVA 데이터 증강을 사용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c} \\hline \\hline Methods & Two-shot prompting & Closed-vocab & **MSRVTT-QA** & **MSVD-QA** \\\\ \\hline \\multicolumn{5}{l}{_Question-answering-only models_} \\\\ FrozenBiLM-L [13] & ✗ & ✓ & 22.2 & 39.0 \\\\ \\hline \\multicolumn{5}{l}{_All-some models_} \\\\ BILP-B [10] & ✗ & ✓ & 19.2 & 35.2 \\\\ HTEA-B [10] & ✗ & ✓ & 21.7 & 37.4 \\\\ mTLEQ-2 [13] & ✗ & ✓ & 43.8 & 55.3 \\\\ Flanning-3B [11] & ✗ & ✗ & 11.0 & 27.5 \\\\ Flanning-9B [11] & ✓ & ✗ & 13.7 & 30.2 \\\\ \\hline \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✓ & ✗ & 19.5 & 36.7 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✗ & ✓ & 23.1 & 43.2 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✓ & ✓ & 28.5 & 39.5 \\\\ \\hline \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✓ & ✗ & 24.8 & 42.7 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✗ & ✓ & 23.4 & 43.4 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✓ & ✓ & **32.0** & **47.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 19: ** zero-shot 비디오 질문 응답에 대한 최신 방법들과의 보다 상세한 비교. 우리는 2발 프롬프트 및 폐쇄 어휘 설정에 따라 추가 결과를 포함한다. MSRVTT-QA와 MSVD-QA 모두에 대한 Top-1 정확도를 보고한다. 학습 중 언어 모델의 동결을 해제하는 방법은 회색으로 표기된다.**KABR을 제외한 동영상 분류와 시공간적 행동 국소화를 위해 학습률 \\(5\\times 10^{-5}\\)을 사용하며, 베이스 스케일 모델은 \\(5\\times 10^{-6}\\)을 사용하고, 대규모 모델은 \\(1\\times 10^{-6}\\)을 사용한다. 베이스라인 다음에, KABR은 또한 클래스 불균형을 설명하기 위해 EQL 손실(Tan et al., 2020)로 트레이닝된다. 마지막으로, 모든 모델은 \\(0.5\\)의 탈락률로 훈련된다.\n' +
      '\n' +
      '## 부록 I 절제 연구\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      '우리는 비디오 텍스트 대조 모델을 훈련할 때 캡션 품질, 양 및 분포가 다른 데이터 세트를 결합하는 방법에 대해 연구한다. 표 20에서는 (1) 단순히 서로 다른 데이터셋(AGD에 대해 "+" 및 "\\(\\mathbf{\\mathcal{K}}\\)"으로 표기)을 혼합하는 단계; (2) 하나의 데이터셋으로 훈련한 다음, 또 다른 데이터셋(AGD에 대해 "\\(\\rightarrow\\)" 및 "\\(\\mathbf{\\mathcal{K}}\\)"으로 훈련하는 단계; (3) 서로 다른 데이터셋과 AGD에 대해 "+" 및 "\\(\\mathbf{\\mathcal{V}}\\"으로 표기)을 결합하는 단계를 포함한다. 본 연구를 위해 InternVid(Wang et al., 2023)와 YTT180M(Zellers et al., 2021)의 두 가지 대표적인 데이터 세트를 선택하고, Zero-shot video-text(ZSV2T)와 text-to-video(ZST2V) 검색을 위해 Recall@1(R@1)을 보고한다. 우리는 단순히 InternVid와 YTT180M을 혼합하면 InternVid만 사용하는 경우와 비교할 때 VATEX에서 큰 성능 저하를 초래한다는 것을 알 수 있다. 반면에, 한 데이터 세트에 대한 트레이닝은 그 다음 다른 데이터 세트에 대한 트레이닝을 계속하는 것은 데이터 세트의 순서에 의해 크게 영향을 받는다. 예를 들어, InternVid를 사용하는 것과 비교하여, InternVid\\(\\rightarrow\\) YTT180M의 성능은 MSRVTT와 VATEX 모두에서 큰 마진만큼 떨어지는 반면, YTT180M\\(\\rightarrow\\) InternVid는 네 가지 메트릭 중 세 가지 메트릭에서 개선된다. 따라서 이 접근 방식은 데이터 세트의 수로 확장할 수 없다. 또는 AGD는 VATEX의 ZST2V가 약간 떨어지는 YTT180M\\(\\rightarrow\\) InternVid 및 InternVid에 비해 VATEX의 MSRVTT 및 ZSV2T에서 지속적으로 성능을 향상시킨다. 결과적으로, AGD는 다른 훈련 데이터 세트를 결합하는 데 사용하도록 선택된다.\n' +
      '\n' +
      '또한 표 20의 마지막 행에 있는 모든 사전 훈련 코퍼스를 사용하여 AGD의 성능을 보고하며, AGD를 사용하여 모든 메트릭에서 큰 개선을 관찰하여 AGD가 데이터 세트의 수에 따라 잘 확장됨을 보여준다.\n' +
      '\n' +
      '### Model design\n' +
      '\n' +
      '**마스킹 방법.** VideoPrism-B를 예로 들어 마스킹 방법과 마스킹 비율이 2단계 모델에 미치는 영향을 먼저 연구한다. 그림 6에서, 우리는 튜브 마스킹(Tong et al., 2022) 및 BEVT 마스킹(Wang et al., 2022) 하에서 2단계 모델의 성능을 상이한 비디오 포커싱된 태스크들에 대한 다양한 마스킹 비율들과 비교한다. 우리는 대부분의 경우 BEVT 마스킹이 튜브 마스킹보다 우수하다는 것을 알아챘다. 마스킹 비율에 있어서는 마스킹 비율 \\(0.65\\)과 \\(0.75\\)을 갖는 BEVT 마스킹이 유사한 성능을 가지며 다른 마스킹 비율보다 우수하다. 그 결과, 달리 명시되지 않은 경우, 모든 2단계 모델들은 \\(0.65\\)의 마스킹 비율을 갖는 BEVT 마스킹으로 트레이닝된다.\n' +
      '\n' +
      '**토큰 셔플링 및 글로벌 증류.** 이후 마스킹 증류 방법에 도입된 두 가지 새로운 기술인 토큰 셔플링 및 글로벌 증류의 성능을 연구한다. 토큰 셔플링 또는 글로벌 증류가 없는 2단계 모델(VideoPrism-B)의 결과를 보여주고 표 21의 비디오 분류(K400 및 SSV2) 및 시공간 작용 위치(AVA) 작업에 대한 전체 2단계 모델과 비교한다. 이 표에서 우리는 토큰 셔플링과 글로벌 증류가 2단계 모델의 성능을 큰 마진만큼 향상시키는 데 도움이 된다는 것을 알 수 있다. 특히, 토큰 셔플링은 2단계 모델의 성능을 향상시킨다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Data} & \\multirow{2}{*}{AGD} & \\multicolumn{2}{c}{**MSRVTT**} & \\multicolumn{2}{c}{**VATEX**} \\\\  & & ZST2V & ZSV2T & ZST2V & ZSV2T \\\\ \\hline InternVid & ✗ & 28.9 & 55.8 & 40.8 & 57.3 \\\\ YTT180M & ✗ & 19.6 & 47.5 & 26.3 & 49.7 \\\\ \\hline InterVid + YTT180M & ✗ & 29.5 & 56.7 & 29.9 & 41.0 \\\\ InternVid \\(\\rightarrow\\) YTT180M & ✗ & 21.7 & 54.7 & 29.8 & 54.8 \\\\ YTT180M \\(\\rightarrow\\) InternVid & ✗ & 29.3 & 56.4 & 39.9 & 57.5 \\\\ InterVid + YTT180M & ✓ & 29.8 & 57.4 & 39.4 & 58.2 \\\\ \\hline Full pretraining corpus & ✓ & **34.3** & **64.4** & **52.0** & **69.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 20: 1단계 모델에 대한 **데이터 절제. Recall@1의 평가 메트릭 하에서 각 구성에 대한 VideoPrism-B의 ZST2V(zero-shot text-to-video) 및 ZSV2T(video-to-text) 검색 결과를 보고한다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Models & **K400** & **SSv2** & **AVA** \\\\ \\hline Full configuration & **84.2** & 63.6 & **30.6** \\\\ w/o token shuffling & 83.6 (\\(\\downarrow 0.6\\)) & 61.8 (\\(\\downarrow 1.8\\)) & 29.4 (\\(\\downarrow 1.2\\)) \\\\ w/o global distillation & 83.4 (\\(\\downarrow 0.8\\)) & **64.2** (\\(\\uparrow 0.6\\)) & 29.0 (\\(\\downarrow 1.6\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 21: 2단계 모델 훈련 전략에 대한 **절제 연구. VideoPrism-B에 대한 동결 백본으로 MAP 프로빙을 사용한 결과가 보고된다. K400 및 SSV2에 대한 Top-1 정확도와 AVA에 대한 평균 평균 정밀도(mAP)를 보고한다.**\n' +
      '\n' +
      '그림 6: 2단계 마스킹 전략과 마스킹 비율에 대한 **절제 연구. VideoPrism-B에 대해 동결 백본을 사용한 MAP 프로빙을 사용한 결과가 보고된다.**\n' +
      '\n' +
      'motion-focused video classification dataset SSv2 by \\(1.8\\%\\) 우리는 토큰 셔플링이 직소 퍼즐과 유사한 2단계 모델(노루지와 파바로, 2016)에 더 어려운 학습 목표를 도입하여 2단계 모델이 비디오의 동작을 더 잘 이해할 수 있도록 강요한다고 믿는다. 반면, 글로벌 증류는 외관 기반 영상 작업(K400의 경우 0.8\\%\\, AVA의 경우 1.6\\%\\)에서 2단계 모델의 성능을 더욱 향상시킬 수 있으며, 좋은 2단계 모델을 훈련하는데 중요한 역할을 한다.\n' +
      '\n' +
      '**2단계 훈련.** 그림 7은 VideoPrism-B를 사용하여 다양한 다른 작업에 걸쳐 다른 비디오 데이터세트에 대한 2단계 모델의 성능을 1단계 모델과 비교한다. 구체적으로, 비디오 분류(K400[VC] 및 SSv2[VC]) 및 시공간 액션 로컬라이제이션(AVA[STAL])과 같은 비디오 전용 태스크에 대해, 동결된 특징을 적용하고 태스크 헤드 내의 가중치만을 업데이트한다. 영상 분류의 경우 Top-1의 정확도를, 시공간적 행동 위치 추정의 경우 mAP@IoU=\\(0.5\\)의 정확도를 각각 보고한다. 제로샷 비디오 대 텍스트 검색(MSRVTT[ZSV2T], VATEX[ZSV2T], 및 ActivityNet[ZSV2T]), 제로샷 텍스트 대 비디오 검색(MSRVTT[ZST2V], VATEX[ZST2V], 및 ActivityNet[ZST2V]), 제로샷 비디오 분류(K400[ZSC], K600[ZSC], SSv2[ZSC], 및 NExt-QA-ATP[ZSC]) 작업에 대해, LiT(Zhai et al., 2022)를 적용하여 섹션 3.2에서 논의된 바와 같이 2단계 모델을 준수하는 텍스트 인코더를 학습하고 Recall@1(R@1)을 보고한다. 우리는 2단계 훈련이 다른 데이터 세트의 모든 비디오 작업에서 1단계 모델과 비교하여 비디오 인코더의 성능을 크게 향상시킴을 알 수 있으며, 제안된 2단계 훈련의 효과를 강력하게 입증한다.\n' +
      '\n' +
      '### Scaling properties\n' +
      '\n' +
      '그림 7(a)에서 우리는 데이터를 고정하여 모델의 스케일링 동작을 연구한다. 1단계 모델과 2단계 모델 모두 모델 크기와 잘 일치함을 알 수 있었다. 흥미롭게도 2단계 모델은 SSv2에서 약 8\\%(8\\%\\), AVA에서 약 2.2\\%\\의 1단계 모델보다 모델 크기에 따라 일관된 개선을 보였다. 그림 7(b)에서 1단계 모델을 Base 크기로 고정하여 2단계 모델을 스케일링한다. _Large_와 _giant_ 2단계 모델의 경우 Base size의 1단계 모델과 호환되지 않기 때문에 초기화를 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Data & \\# of clips & **K400** & **SSv2** & **AVA** \\\\ \\hline Full pretraining corpus & 618M & 85.8 & 66.4 & 33.1 \\\\ + additional video-only & 898M & 86.1 & 66.7 & 33.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 22: **2단계 모델에 대한 데이터 스케일링에 대한 연구.** 냉동 백본으로 MAP 프로빙을 사용하여 _Large_ 모델의 결과를 보고하고 전체 사전 훈련 말뭉치만 사용하여 1단계 모델을 훈련한다.\n' +
      '\n' +
      '그림 8: **모델 스케일링에 대한 예비 연구.** 냉동 백본으로 MAP 프로빙을 사용한 결과가 보고된다. 우리는 K400과 SSv2에 대한 Top-1 정확도와 AVA에 대한 평균 평균 정밀도(mAP)를 보고한다.\n' +
      '\n' +
      '그림 7: 비디오 이해 작업에 대한 VideoPrism-B의 1단계 모델과 2단계 모델의 **비교.** 비디오 전용 작업의 경우 결과는 냉동 백본으로 MAP 프로빙을 사용한 결과이다.\n' +
      '\n' +
      'corresponding image model of CoCa(Yu et al., 2022). 우리는 고정된 1단계 모델에서도 2단계 모델이 여전히 합리적인 스케일링 능력을 보인다는 것을 관찰한다.\n' +
      '\n' +
      '표 20에서, 우리는 사전 훈련 코퍼스에 훈련된 모델이 MSRVTT ZST2V 검색에서 5.4%, VATEX ZST2V 검색에서 11.2%로 InternVid(Wang et al., 2023)에 훈련된 모델보다 우수한 1단계 모델의 강력한 데이터 스케일링 능력을 보여준다. 이는 2단계 모델의 데이터 스케일링 능력도 연구하도록 동기를 부여한다. 2단계 교육의 흥미로운 측면은 주석 없이 비디오 전용 데이터로 작동한다는 것입니다. 이것은 2단계 훈련 동안 말뭉치 크기를 경제적으로 증가시키는 데 도움이 된다. 데이터 스케일링의 이점을 테스트하기 위해 유튜브에서 주석 없이 280M 비디오 클립을 추가로 채굴하고 2단계 교육에 추가합니다. 본 논문에서는 _Large_ 크기를 갖는 모델을 예로 들어 사전 학습 코퍼스를 이용하여 1단계 모델을 학습한다. 그런 다음 표 22의 사전 훈련 말뭉치 및 추가 클립으로 훈련된 2단계 모델과 사전 훈련 말뭉치에서만 훈련된 2단계 모델을 비교하여 2단계 모델이 데이터와 잘 확장됨을 알 수 있다. 마스크드 모델링에 대한 이전 작업은 좋은 데이터-스케일링 특성을 나타내지 않거나 데이터 스케일링으로 한계 개선을 보여준다(Feichtenhofer et al., 2022; Tong et al., 2022).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>