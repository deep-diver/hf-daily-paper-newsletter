<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# VideoPrism: A Foundational Visual Encoder for Video Understanding\n' +
      '\n' +
      ' Long Zhao\n' +
      '\n' +
      'Nitesh B. Gundavarapu\n' +
      '\n' +
      'Liangzhe Yuan\n' +
      '\n' +
      'Hao Zhou\n' +
      '\n' +
      'Shen Yan\n' +
      '\n' +
      'Jennifer J. Sun\n' +
      '\n' +
      'Luke Friedman\n' +
      '\n' +
      'Rui Qian\n' +
      '\n' +
      'Tobias Weyand\n' +
      '\n' +
      'Yue Zhao\n' +
      '\n' +
      'Rachel Hornung\n' +
      '\n' +
      'Florian Schroff\n' +
      '\n' +
      'Ming-Hsuan Yang\n' +
      '\n' +
      'David A. Ross\n' +
      '\n' +
      'Huisheng Wang\n' +
      '\n' +
      'Hartwig Adam\n' +
      '\n' +
      'Mikhail Sirotenko\n' +
      '\n' +
      'Ting Liu\n' +
      '\n' +
      'Boqing Gong\n' +
      '\n' +
      'Google Research\n' +
      '\n' +
      'Equal primary contribution. Equal core technical contribution. Equal senior contribution, project leads. Correspondence to: Long Zhao \\(<\\)longzh@google.com\\(>\\), Mikhail Sirotenko \\(<\\)msirotenko@google.com\\(>\\), Ting Liu \\(<\\)liuti@google.com\\(>\\), Boqing Gong \\(<\\)bgong@google.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (_e.g._, ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Videos are a rich and dynamic archive of real-world perceptual experience, spanning diverse domains from everyday life to scientific observations. Video foundation models (ViFMs) hold enormous potential to unlock new insights within this vast corpus. While prior work has made great progress towards general video understanding (Xu et al., 2021; Wang et al., 2022; Yan et al., 2022; Tong et al., 2022; Li et al., 2023; Wang et al., 2023c), building a truly foundational video model is still an elusive goal. Existing models often struggle to balance appearance-heavy tasks with motion-centric reasoning, falling behind task-specialized models across many benchmarks (Yuan et al., 2023).\n' +
      '\n' +
      'We introduce VideoPrism, a general-purpose video encoder designed to tackle a wide spectrum of video understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA) (Figure 1). Evaluated extensively on computer vision (CV) datasets and CV for science domains like neuroscience and ecology, VideoPrism achieves state-of-the-art performance with minimal adaptation, using a _single frozen_ model. We emphasize this frozen-encoder setting following prior work (Radford et al., 2021; Alayrac et al., 2022; Tang et al., 2023; Li et al., 2023a) and for its practical utility given the otherwise high computational and memory cost of finetuning video models.\n' +
      '\n' +
      'The design philosophy behind VideoPrism is as follows. Pretraining data is fundamental to foundation models (FMs) (Bommasani et al., 2021), and the ideal pretraining data for ViFMs would be a representative sample of all videos in the world. Most videos from this sample will have no (or very noisy) parallel text describing the content; however, when such text exists, it provides priceless semantic clues about the video space. Accordingly, our pretraining strategy should focus primarily on the video modality and yet take full advantage of any available video-text pairs.\n' +
      '\n' +
      'On the data side, we approximate the desired pretraining corpus by assembling 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (_e.g._, ASR transcripts, generated captions, and retrieved text). On the modeling side, we first contrastively learn semantic video embeddings (Radford et al., 2021; Jia et al., 2021) from all our video-text pairs of various qualities. Subsequently, we capitalize on the extensive video-only data by distilling the semantic embeddings globally and token-wise, improving upon masked video modeling (Tong et al., 2022; Feichtenhofer et al., 2022; Wang et al., 2023c) described below.\n' +
      '\n' +
      'Despite its success for natural language (Devlin et al., 2019; Brown et al., 2020; Anil et al., 2023), masked data modeling remains challenging for CV as raw visual signals lack semantics. Existing works approach this challenge by borrowing indirect semantics (_e.g._, using CLIP (Radford et al., 2021) to bootstrap models (Fang et al., 2022; 2023) or tokenizers (Peng et al., 2022)) or implicitly promoting them (_e.g._, tokenizing visual patches (Zhou et al., 2022; Bao et al., 2022; Oquab et al., 2023), combining a high masking ratio and lightweight decoder (He et al., 2022)).\n' +
      '\n' +
      'We build on the above ideas with a two-stage approach tailored to our pretraining data. We first train a video encoder, along with a paired text encoder, over the video-text pairs using a contrastive objective (Gutmann and Hyvarinen, 2010; Radford et al., 2021). Next, we continue training the encoder over all video-only data by masked video modeling with two improvements: (1) the model is required to predict both the video-level global embedding and token-wise embeddings from the first stage based on unmasked input video patches; (2) random shuffle is applied to the encoder\'s output tokens before they are passed to the decoder to avoid learning shortcuts. Notably, our pretraining utilizes two supervisory signals: a video\'s text description and its contextual self-supervision, enabling VideoPrism to excel on both appearance- and motion-focused tasks. Indeed, previous works have shown that video captions mainly reveal appearance cues (Wang et al., 2023f), and contextual self-supervision facilitates learning motion (Tong et al., 2022).\n' +
      '\n' +
      '**Contributions.** VideoPrism is a state-of-the-art, general-purpose video encoder. We advocate for a scalable strategy for collecting pretraining videos, combining manually captioned videos with those containing noisy textual descriptions. We design a unique two-stage pretraining approach tailored to this hybrid data, leveraging video-language contrastive learning to harvest semantics, followed by improved masked video modeling with global-local distillation and token shuffling. Finally, we present a comprehensive evaluation on four broad categories of understanding tasks across 33 diverse benchmarks, including videos from the web, scripted performances, and scientific experiments. Results demonstrate that VideoPrism significantly outperforms existing ViFMs on 30 benchmarks (Figure 2). Importantly, no single baseline model consistently achieves second-best performance, indicating VideoPrism\'s robust generalizability.\n' +
      '\n' +
      'Figure 2: **VideoPrism _vs._ the previous best-performing FMs.** Please find the details of this figure in Appendix D.\n' +
      '\n' +
      '## 2 Approach\n' +
      '\n' +
      '### Pretraining data\n' +
      '\n' +
      'Our pretraining data consists of 36M clips (sampled from 36M videos) with high-quality manually labelled _captions_ and 582M clips (from 275M videos) with noisy parallel _text_, as summarized in Table 1. The 36M high-quality video-caption pairs in Anonymous-Corpus #1 are the largest of its kind for ViFMs, to our knowledge, but they are still an order of magnitude smaller than the image-language data used to fuel image FMs (Radford et al., 2021; Yu et al., 2022). Hence, we also collect large-scale video-text data whose noisy text is generated through ASR, metadata, and large multimodal models (Wang et al., 2023; Zhao et al., 2024), _etc._ This subset of videos correspond to the rows from WTS-70M to Anonymous-Corpus #3 in Table 1, and we provide more details in Appendix A.\n' +
      '\n' +
      'Importantly, unlike previous works (Tong et al., 2022; Wang et al., 2022; Li et al., 2023; Wang et al., 2023), we do not incorporate any training sets from the evaluation benchmarks, _e.g._, Kinetics (Kay et al., 2017), for either pretraining or post-pretraining. This conscious choice avoids overly tuning our model towards certain evaluation benchmarks. Moreover, we carefully de-duplicate the pretraining corpus against the videos in the evaluation benchmarks used in this paper to ensure that there is no data leakage.\n' +
      '\n' +
      '### Model architecture\n' +
      '\n' +
      'The VideoPrism model architecture stems from the standard Vision Transformer (ViT) (Dosovitskiy et al., 2021), with a factorized design in space and time following ViViT (Arnab et al., 2021). However, we remove the global average pooling layer of ViViT immediately after the spatial encoder so that the spatiotemporal dimensions remain in the output token sequence, facilitating the downstream tasks that require fine-grained features (_e.g._, spatiotemporal action localization). We experiment with two model configurations: _VideoPrism-g_ and _VideoPrism-B_. VideoPrism-g is the ViT-giant network (Zhai et al., 2022) with 1B parameters in the spatial encoder, and VideoPrism-B is a smaller variant with the ViT-Base network (Dosovitskiy et al., 2021). Appendix B describes the two network architectures in detail.\n' +
      '\n' +
      '### Training algorithm\n' +
      '\n' +
      'Our goal is to leverage both video-text pairs and the video-only data curated in Section 2.1 to train VideoPrism scalably, so as to make VideoPrism a foundational video encoder capable of capturing both appearance and motion semantics from videos. We highlight the video-only modality rather than solely relying on video-text because the text in our large-scale pretraining corpus is very noisy for some videos. As shown in Figure 3, the training pipeline of VideoPrism includes two stages: _video-text contrastive training_ and _masked video modeling_.\n' +
      '\n' +
      '#### 2.3.1 Stage 1: Video-text contrastive training\n' +
      '\n' +
      'In the first stage, we conduct contrastive learning to align a video encoder with a text encoder using all the video-text pairs. Following prior arts (Radford et al., 2021; Jia et al., 2021; Cheng et al., 2023), we minimize a symmetric cross-entropy loss over the similarity scores of all video-text pairs in a mini-batch, initialize the spatial encoding modules using the image model of CoCa (Yu et al., 2022), and include WebLI (Chen et al., 2023) (about 1B images with alt-text) to the pretraining. The video encoder\'s features are aggregated through a multi-head attention pooler (MAP) (Lee et al., 2019) before the loss computation. This stage allows the video encoder to learn rich visual semantics from language supervision, and the resulting model supplies semantic video embeddings for the second-stage training.\n' +
      '\n' +
      '#### 2.3.2 Stage 2: Masked video modeling\n' +
      '\n' +
      'Training solely on vision-text data as in Stage 1 presents challenges: text descriptions can be noisy, and they often capture appearance more than motion (Hendricks and Nematzadeh, 2021; Momeni et al., 2023). To address this, our second-stage training focuses on learning both appearance and motion information from video-only data. Building upon the success of masked autoencoding for motion understanding (Wang et al., 2022; 2023), we adapt this approach for the second stage, while ensuring that the model retains the semantic knowledge acquired in the first stage.\n' +
      '\n' +
      'In this stage, we continue to train the video encoder on video-only data using improved masked video modeling. These improvements consist of (1) a novel token shuffling scheme\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Pretraining datasets & Public & Domain & Caption source & Caption quality & \\# of videos & \\# of clips \\\\ \\hline Anonymous-Corpus \\#1 & ✗ & Web video & Manual labelled & High & 36.1M & 36.1M \\\\ \\hline WTS-70M (Stroud et al., 2020) & ✓ & YouTube video & Metadata & Low & 55.1M & 55.1M \\\\ VT-Temporal-180M (Zellers et al., 2021) & ✓ & YouTube video & ASR & Low & 2.3M & 87.8M \\\\ VideoCC (Nagrani et al., 2022) & ✗ & YouTube video & Image captions for mining & Low & 133.5M & 191.1M \\\\ Internetyl (Wang et al., 2023) & ✓ & YouTube video & Generated by VLMLM & Medium & 2.8M & 7.0M \\\\ Anonymous-Corpus \\#2 & ✗ & YouTube video & ASR & Low & 44.6M & 170.3M \\\\ Anonymous-Corpus \\#3 & ✗ & YouTube video & Generated by VLMLM & Medium & 36.7M & 71.5M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Composition of our pretraining corpus.** We report the numbers of videos and clips we were able to access during pretraining.\n' +
      '\n' +
      'to prevent decoding shortcuts and (2) global and token-wise distillation losses to effectively leverage the knowledge acquired in the first stage. As illustrated in Figure 3, the second-stage (student) model learns to predict the first-stage (teacher) model\'s embeddings of _all_ tokens based on a masked video. The encoder-decoder Transformers are decoupled following He et al. (2022)\'s design.\n' +
      '\n' +
      '**Token shuffling.** As we effectively initialize the second-stage model from the first stage, one issue is that the model may create a shortcut for the decoder to copy and paste the unmasked tokens while predicting only the masked ones, making it an easier task to solve than predicting all tokens. To address this issue, we _randomly shuffle_ the token sequence output by the encoder before feeding it to the decoder, and the decoder adds positional embeddings to this sequence after the shuffling. Note that this shuffling operation avoids the copy-and-paste shortcut of unmasked tokens that the decoder can potentially explore. One can also view it akin to Jigsaw puzzles Noroozi and Favaro (2016) for the unmasked tokens while predicting the masked ones.\n' +
      '\n' +
      '**Global-local distillation.** Unlike the masked distillation for images Fang et al. (2022); (2023), we find that our second-stage model underperforms the first-stage teacher on appearance-heavy tasks when only the masked modeling loss is utilized, probably attributing to catastrophic forgetting McCloskey and Cohen (1989) in the two-stage pretraining. To mitigate this issue, we add an additional loss to let the second-stage model distill the global embedding of the full intact video from the first-stage teacher using the visible tokens. Hence, the second-stage training loss combines the token-wise masked video modeling and global distillation. Due to space limit, we refer readers to Appendix C for the detailed implementation and training configurations.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      'We evaluate VideoPrism on a wide spectrum of video-centric understanding tasks to demonstrate its capability and generalizability. We group the tasks into four categories: (1) general video-only understanding, including classification and spatiotemporal localization (Section 3.1), (2) zero-shot video-text retrieval (Section 3.2), (3) zero-shot video captioning and QA (Section 3.3), and (4) CV for science (Section 3.4). For all experiments in the main paper, we freeze VideoPrism as a video encoder and only train task-specific components for the tasks in groups (1), (2), and (4) and some adaptation layers connecting VideoPrism to an LLM for (3). In the appendices, we report more results of end-to-end and adapter finetuning. Note that our evaluation strategy, freezing the visual encoder, aligns with prior works He et al. (2022); Singh et al. (2022); Yuan et al. (2023) and is almost a go-to choice for building VideoLLMs Tang et al. (2023). It is especially needed for videos because finetuning a ViFM is prohibitively expensive, while a frozen ViFM allows one to amortize the cost of video encoding across multiple tasks.\n' +
      '\n' +
      '### Classification and spatiotemporal localization\n' +
      '\n' +
      'We compare VideoPrism with state-of-the-art FMs on a video-only understanding benchmark: VideoGLUE Yuan et al. (2023). By design, VideoGLUE evaluates FMs through four adaptation methods over eight hallmark datasets, representing appearance-focused action recognition (VC\n' +
      '\n' +
      'Figure 3: **Illustration of our two-stage pretraining.** Stage 1 trains video and text encoders with contrastive loss on video-text pairs, supplying semantic video embeddings to the next stage. Stage 2 continues to train the video encoder using improved masked autoencoding on video-only clips. The encoder uses unmasked 3D video patches to predict a global semantic embedding of the whole video and token-wise embeddings. Decoder 2 processes shuffled tokens with positional embedding, while Decoder 1 has no positional embedding.\n' +
      '\n' +
      '(A)), motion-rich action recognition (VC (M)), multi-label video classification (VC (ML)), temporal action localization (TAL), and spatiotemporal action localization (STAL). Moreover, this benchmark introduces a VideoGLUE score (VGS), considering the tradeoff between adaptation costs and performance, to provide a holistic view of FMs\' capabilities on the video-only understanding tasks. We present the frozen-backbone evaluation results in the main paper and leave the rest to Appendix E. We employ an MAP head (Yuan et al., 2023) in action recognition (MAP probing) and spatiotemporal localization and use G-TAD (Xu et al., 2020) for temporal localization (see Appendix E.1 for details).\n' +
      '\n' +
      '**Datasets.** The eight datasets in VideoGLUE are as follows. For appearance-focused action recognition, Kinetics-400 (K400) (Kay et al., 2017) and Moments-in-Time (MiT) (Monfort et al., 2019) are sourced from web videos. Something-Something v2 (SSv2) (Goyal et al., 2017) and Diving48 (D48) (Li et al., 2018) are fine-grained motion-rich action recognition datasets. Besides, Charades (Sigurdsson et al., 2016) provides a multi-label classification problem using scripted indoor videos. The temporal localization task entails one dataset, ActivityNet v1.3 (Caba Heilbron et al., 2015), and the spatiotemporal localization contains Atomic Visual Actions (AVA) (Gu et al., 2018) and AVA-Kinetics (AVA-K) (Li et al., 2020).\n' +
      '\n' +
      '**Main results.** Table 2 shows the frozen-backbone results on VideoGLUE. VideoPrism outperforms the baselines on all datasets by a large margin. Besides, increasing VideoPrism\'s underlying model size from ViT-B to ViT-g significantly improves the performance. Notably, no baselines can perform second best on all benchmarks, indicating the previous methods might be developed towards certain aspects of video understanding, while VideoPrism consistently improves on this wide range of tasks. This result implies that VideoPrism packed various video signals into one encoder: semantics at multiple granularities, appearance _vs._ motion cues, spatiotemporal information, and robustness to diverse video sources (_e.g._, web videos _vs._ scripted performance).\n' +
      '\n' +
      'In Appendix E.3, following the VideoGLUE setup, we conduct experiments on other adaptation methods, including end-to-end and parameter-efficient finetuning, and multi-layer attention pooling. Different adaption methods trade off computational cost with performance, accounting for real-world application considerations, and the VGS aggregates them into a scalar value. VideoPrism achieves VGS \\(51.25\\), outperforming all baseline FMs in Table 16 and scoring \\(13.6\\%\\) higher than the second best model (UMT).\n' +
      '\n' +
      '### Zero-shot video-text retrieval and classification\n' +
      '\n' +
      'To enable zero-shot video-text retrieval and video classification capabilities of VideoPrism, we follow LiT (Zhai et al., 2022b) to learn a text encoder producing the text embeddings matched to their corresponding video embeddings out of VideoPrism. We choose the LiT text encoder to mirror the one in the first-stage training and attach an MAP head to the video encoder. The LiT tuning is over the same pretraining data from the first stage. More details are in Appendix F.1.\n' +
      '\n' +
      '**Datasets.** We evaluate VideoPrism\'s zero-shot video-text retrieval performance on three benchmarks: MSRVTT (Xu et al., 2016), VATEX (Wang et al., 2019), and ActivityNet (Caba Heilbron et al., 2015). For zero-shot video classification tasks, we experiment with Kinetics-400 (Kay et al., 2017), Charades (Sigurdsson et al., 2016), SSv2-Temporal and SSv2-Events (Sevilla-Lara et al., 2021; Bagad et al., 2023), and the ATP-Hard subset of NExT-QA (Buch et al., 2022). SSv2 and NExT-QA (ATP-Hard) focus on motion and temporal reasoning, respectively. Moreover, we adapt Charades-STA (Gao et al., 2017) to the zero-shot classification scenario by reformulating each of its samples in the test set into a multi-choice retrieval problem (see Appendix F.2 for more details). We report results following the standard evaluation metric for each benchmark.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c|c|c c|c c} \\hline \\hline \\multirow{2}{*}{Methods} & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{V (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} \\\\  & **K400** & **MiT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** \\\\ \\hline _Base-scale models_ & & & & & & & & \\\\ CLIP-B (Radford et al., 2021) & 75.2 & 32.6 & 41.0 & 44.1 & 11.2 & 32.7 & 21.1 & 25.9 \\\\ VATT-B (Akbari et al., 2021) & 75.1 & 32.1 & 57.8 & 49.7 & 33.3 & 35.3 & 20.3 & 22.2 \\\\ InterVideo-B (Wang et al., 2022c) & 69.3 & 26.3 & 58.2 & 55.6 & 13.0 & 33.3 & 13.4 & 15.7 \\\\ UMT-B (Li et al., 2023b) & 77.1 & 34.0 & 47.7 & 47.8 & 30.1 & 35.8 & 20.7 & 21.1 \\\\\n' +
      '**VideoPrism-B** & **84.2**(71.7) & **40.8**(76.8) & **63.6**(51.4) & **67.4**(112.2) & **40.4**(71.7) & **36.6**(70.8) & **30.6**(79.5) & **31.8**(15.9) \\\\ \\hline _Large-scale models_ & & & & & & & & \\\\ VideoMAE-v2.9 (Wang et al., 2023b) & 82.1 & 35.0 & 56.1 & 60.5 & 22.4 & 35.3 & 21.5 & 23.3 \\\\ Intern video-(Wang et al., 2022c) & 78.6 & 33.7 & 67.4 & 69.6 & 20.9 & 35.9 & 20.8 & 21.3 \\\\ UMT-L (Li et al., 2023b) & 82.8 & 40.3 & 54.5 & 49.0 & 39.9 & 36.7 & 24.4 & 26.2 \\\\\n' +
      '**VideoPrism-g** & **87.2**(14.4) & **45.5**(15.2) & **68.5**(11.1) & **71.3**(11.7) & **62.3**(122.2) & **37.8**(11.1) & **36.2**(122.2) & **37.3**(111.1) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Evaluating FMs on the VideoGLUE benchmark (Yuan et al., 2023) with frozen backbones.** Only weights in the task heads are trained using the downstream tasks’ training sets. On all video classification (VC) tasks except Charades, we report top-1 accuracy. On Charades, temporal localization (TAL), and spatiotemporal localization (STAL) tasks, we use mean average precision (mAP) as the evaluation metric. -B, -L, -g indicate that the underlying models are respectively the base, large, and giant ViT (Dosovitskiy et al., 2021).\n' +
      '\n' +
      '**Main results.** Tables 3 and 4 summarize the results of video-text retrieval and video classification, respectively. VideoPrism sets the new state of the art on most benchmarks, and the gains over the prior arts are exceptionally substantial on the challenging datasets (_e.g._, \\(9.5\\%\\) on ActivityNet, \\(4.4\\%\\) on SSV2-Events, and \\(6.6\\) mAP on Charades). Most results from our base-scale VideoPrism-B are actually better than those of existing larger scale models. Additionally, VideoPrism is on par with or better than the models pretrained with in-domain data and extra modalities (_e.g._, audios) in Table 4. These improvements in zero-shot retrieval and classification tasks present VideoPrism\'s strong generalization capabilities.\n' +
      '\n' +
      '### Zero-shot video captioning and QA\n' +
      '\n' +
      'We further evaluate the inherent capabilities of VideoPrism on generative video-language tasks, _i.e._, captioning and QA, where we pair VideoPrism with a language decoder, PaLM-2 (Anil et al., 2023). To connect the two models, we introduce and train several gluing layers while keeping both VideoPrism and the language decoder frozen. We then conduct evaluation under the zero-shot configuration on video captioning and QA benchmarks. Note that we do not tune our models separately for captioning and QA tasks. Please refer to Appendix G for implementation details.\n' +
      '\n' +
      '**Datasets.** We evaluate the model in the zero-shot setting on the test splits of a suite of standard video captioning datasets including MSRVTT (Xu et al., 2017), VATEX (Wang et al., 2019), and YouCook2 (Zhou et al., 2018), and video QA benchmarks including MSRVTT-QA (Xu et al., 2017), MSVD-QA (Xu et al., 2017), and NExT-QA (Xiao et al., 2021). For video QA, where it is imperative to match the length and style of the model\'s answers with groundtruths, we adopt the zero-shot approach of Flamingo (Alayrac et al., 2022) and use two-shot text-only prompts from the training set of the downstream task. Additionally, for MSRVTT-QA and MSVD-QA, we experiment with the closed-vocabulary evaluation configuration (Li et al., 2022; Yang et al., 2022).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 4: **Comparison to state-of-the-art results on zero-shot video classification. Results are reported in Top-1/5 accuracy (%) on Kinetics-400 and Something-Something v2, multi-choice (MC) retrieval accuracy (%) on NEXT-QA (ATP-Hard) and Charades-STA, and mean average precision (mAP) on Charades. In line with Ni et al. (2022), we follow the single-view evaluation protocol for simplicity. Models pretrained with extra modalities (_e.g._, audio) in addition to vision and language are marked in gray.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\n' +
      '\\end{table}\n' +
      'Table 3: **Results of zero-shot video-text retrieval. We report the Recall@1 (R@1) and R@5 for all the benchmarks.**In this setting, we let the model score candidate answers according to their log-likelihoods and return the top one.\n' +
      '\n' +
      '**Main results.** Tables 5 and 6 show the results of zero-shot video captioning and QA, respectively. Despite the simplicity of our model architecture and the small number of adapter parameters, our models are competitive and top the methods freezing both vision and language models except on VATEX. The results demonstrate that our VideoPrism encoder is able to generalize well to video-to-language generation tasks.\n' +
      '\n' +
      '### CV for science tasks\n' +
      '\n' +
      'While existing video analysis benchmarks commonly focus on human-centric data, we evaluate VideoPrism on a broad set of videos from scientific datasets to assess its generalizability and potential to be used in scientific applications. These datasets include fields such as ethology (Eyjolfsott et al., 2014), behavioral neuroscience (Sun et al., 2021; Burgos-Artizzu et al., 2012), cognitive science (Ma et al., 2023), and ecology (Kholiavchenko et al., 2024). To the best of our knowledge, this work is the first to study the use of ViFMs on scientific datasets, highlighting their ability to match or surpass the performance of specialized models. We encourage the creation of more open-sourced datasets from real-world scientific experiments to unlock the potential of ViFMs to benefit various fields of science.\n' +
      '\n' +
      '**Datasets.** We focus on large-scale video datasets annotated with domain expertise, captured in scientific experiments. These datasets consist of flies (Fly vs. Fly (Eyjolfsott et al., 2014)), mice (CalMS21 (Sun et al., 2021), CRIM13 (Burgos-Artizzu et al., 2012)), chimpanzees (ChimpACT (Ma et al., 2023)), and Kenyan animals (KABR (Kholiavchenko et al., 2024)). All the datasets are annotated for video classification of behavior, except for the ChimpACT dataset for spatiotemporal action localization. We evaluate CRIM13 from cameras on the side perpendicular to the cage ("S"), as well as a top, overhead view ("T"). We use standard data splits defined in previous works on these datasets, and all datasets are evaluated using the mAP metric, except KABR which uses macro-accuracy. Further implementation details are in Appendix H.\n' +
      '\n' +
      '**Main results.** General ViFMs, using a shared frozen encoder across all evaluations, achieve performance comparable to (or exceeding) domain-specific models specialized for individual tasks (Table 7). In particular, VideoPrism generally performs the best and surpasses domain expert models with the base-scale model. Scaling to large-scale models further improves performance across all datasets. These results demonstrate that ViFMs have the potential to significantly accelerate video analysis across diverse fields.\n' +
      '\n' +
      '### Ablation study\n' +
      '\n' +
      'The main driving force behind VideoPrism includes the strategy and effort for collecting the pretraining data and the pretraining approach that improves upon masked autoencoding by the two-stage pretraining framework, the global distillation, and token shuffling. We run ablation studies to evaluate the effectiveness of these components. First, we train a video-text contrastive baseline as presented in Section 2.3.1 over a smaller scale, publicly available corpus (150M video clips in total), including WTS-70M, YT-Temporal-180M, and InternVid. We then add our main components (larger pretraining data, two-stage training, losses, and token shuffling) to the baseline one at a time to see how the model\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & **MSRVTT-QA** & **MSVD-QA** & **NExT-QA** \\\\ \\hline \\multicolumn{4}{l}{_Question-answering-only models_} \\\\ FrozenBtM-M (Yang et al., 2022) & 22.2 & 39.0 & - \\\\ \\hline \\multicolumn{4}{l}{_All-in-one models_} \\\\ BLIP-B (Li et al., 2022) & 19.2 & 35.2 & - \\\\ HiFLo-B (Ye et al., 2023) & 21.7 & 37.4 & - \\\\ mPLUQ-2 (Xu et al., 2023) & 43.8 & 55.3 & - \\\\ Flanning-3B (Alayra et al., 2022) & 11.0 & 27.5 & 21.3 \\\\ Flanning-3B (Alayra et al., 2022) & 13.7 & 30.2 & 23.0 \\\\\n' +
      '**VideoPrism-B** w/ PALM-2-1B & 28.5 (\\(\\uparrow\\)6.3) & 39.5 (\\(\\uparrow\\)0.5) & 23.8 (\\(\\uparrow\\)0.8) \\\\\n' +
      '**VideoPrism-B** w/ PALM-2-8B & **32.0** (\\(\\uparrow\\)9.8) & **47.1** (\\(\\uparrow\\)8.1) & **27.4** (\\(\\uparrow\\)4.4) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **Comparison to state-of-the-art methods on zero-shot video QA**. We report the WUPS index (Wu and Palmer, 1994) for NExT-QA and Top-1 accuracy for the others. Methods that unfreeze their language models are marked in gray.\n' +
      '\n' +
      'Figure 4: **Ablation study.** From top to bottom: we begin by a video-text contrastive baseline and gradually add our major components to it. Each row is based on a modification of the immediately preceding row. We note that it is difficult to perform well on both K400 and SSv2 using only a single frozen encoder, but our final model with all improvements excels on both datasets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c} \\hline \\hline Methods & **MSRVTT** & **VATEX** & **YouCook2** \\\\ \\hline \\multicolumn{4}{l}{_Capitining-only models_} \\\\ VideoCoCoCo-g (Yan et al., 2022) & 27.1 & 22.8 & 34.3 \\\\ DeCap-B (Li et al., 2023) & 18.6 & 18.7 & - \\\\ \\hline \\multicolumn{4}{l}{_All-in-one models_} \\\\ Flanning-3B (Alayra et al., 2022) & - & **40.1** & 55.8 \\\\ Flanning-3B (Alayra et al., 2022) & - & 39.5 & 55.0 \\\\\n' +
      '**VideoPrism-B** w/ PALM-2-1B & **40.3** (\\(\\uparrow\\)13.3) & 24.2 (\\(\\downarrow\\)12.2) & 52.3 (\\(\\downarrow\\)3.5) \\\\\n' +
      '**VideoPrism-B** w/ PALM-2-8B & 38.5 (\\(\\uparrow\\)11.3) & 31.7 (\\(\\uparrow\\)8.4) & **63.6** (\\(\\uparrow\\)7.8) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **Comparison to state-of-the-art methods on zero-shot video captioning.** We report the CIDEr score for all benchmarks.\n' +
      '\n' +
      'performance evolves along the way. We also experiment with combining contrastive loss with masked autoencoding (Feichtenhofer et al., 2022) in one stage to highlight the effectiveness of our two-stage training pipeline.\n' +
      '\n' +
      'Figure 4 exhibits the ablation results, where we observe different performance evolving trajectories on motion-rich SSv2 and appearance-driven K400. Notably, the consistent improvements of VideoPrism on SSv2 suggest the effectiveness of our data curation and model designing efforts for facilitating motion understanding in videos. Although the contrastive baseline has already achieved competitive results on K400, the proposed global distillation and token shuffling further boost the accuracy. We provide more comprehensive ablation studies in Appendix I.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'One limitation of our approach is that we leverage a video corpus with noisy text as part of pretraining. This noisy text is potentially incomplete and biased, which could impact model performance. Moreover, long video understanding remains a challenge, since our current focus is on short video clips from which we sample \\(16\\) frames as input to VideoPrism. Future work in this direction could leverage our encoder as part of a long video understanding system. Finally, while we advocate for the frozen-backbone evaluation, we acknowledge that there are scenarios that benefit more from end-to-end finetuning and parameter-efficient adaptation. Despite these limitations, results demonstrate the potential impact of VideoPrism has on a range of real-world video understanding tasks.\n' +
      '\n' +
      '## 4 Related work\n' +
      '\n' +
      '**Foundation models (FMs)**(Bommasani et al., 2021) have demonstrated tremendous promise with early work in LLMs (Devlin et al., 2019; Brown et al., 2020). Some ViFMs are built around LLMs (Wang et al., 2022; Li et al., 2023; Zhang et al., 2023; Chen et al., 2023), analyzing videos by feeding associated text to LLMs, such as ASR transcripts and machine-generated captions. In contrast, VideoPrism takes a video-centric view, and we aim to tackle a broader range of video understanding tasks.\n' +
      '\n' +
      '**ViFMs.** Most recent FMs in CV focus on images (Radford et al., 2021; Yuan et al., 2021; Jia et al., 2021; Yu et al., 2022; Alayrac et al., 2022; Yan et al., 2022; Wang et al., 2022; Chen et al., 2023; Xu et al., 2023; Girdhar et al., 2023; Zhu et al., 2024). Their pretraining data contains no or only a small portion of videos, and the model architectures and learning methods are for images by design. While these FMs can accept video frames as input, they fall short on motion and temporal modeling (Yuan et al., 2023). Our work directly addresses this gap by developing a video encoder.\n' +
      '\n' +
      'For videos, existing works mainly train FMs using self-supervised learning over the video-only modality (Qian et al., 2021; Feichtenhofer et al., 2021; Recasens et al., 2021; Singh et al., 2021; Wei et al., 2022; Yuan et al., 2022; Qian et al., 2022; Tong et al., 2022; Wang et al., 2023) or video-language modeling of videos with noisy text (Zellers et al., 2021; Fu et al., 2021; Li et al., 2023; Wang et al., 2023; Cheng et al., 2023; Piergiovanni et al., 2023; Xiong et al., 2023). As Wang et al. (2023) point out, existing video-language models lack knowledge of actions, and yet self-supervised models from video-only data struggle with semantics. We instead bring the best of the two together. Related to our work, InternVideo (Wang et al., 2022) glues a self-supervised VideoMAE model (Wang et al., 2023) and a video-language model together using cross-attention modules. Unlike VideoPrism, however, the two models have no mutual influence during pretraining and they redundantly process the same video from scratch simultaneously.\n' +
      '\n' +
      '**Large-scale video datasets** are pivotal for ViFMs and have been a subject of interest. HowTo100M (Miech et al., 2019), YT-Temporal-1B (Zellers et al., 2022), and HD-VILA-100M (Xue et al., 2022) associate speech transcriptions with videos. WebVid2M (Bain et al., 2021) and WTS70M (Stroud et al., 2020) pair alt-text and other metadata with videos. VideoCC3M (Nagrani et al., 2022) retrieves videos that appear similar to images and transfer the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c} \\hline \\hline Methods & **Fly vs. Fly** & **CalMS21** & **CRIM13 (S/T)** & **KABR** & **ChimpACT** \\\\ \\hline Domain experts & \\(88.6\\) & \\(88.9\\) & - & \\(61.9\\) & \\(24.4\\) \\\\ \\hline _Base-scale models_ & & & & & \\\\ CoCa-B (Yu et al., 2022) & 80.1 & 89.2 & 58.2 / 58.4 & **62.0** & 12.6 \\\\ InternVideo-B (Wang et al., 2022) & 78.9 & 89.0 & 63.2 / 63.6 & 49.9 & 24.0 \\\\ UMT-B (Li et al., 2023) & **84.6** & 88.7 & 59.3 / 58.5 & 58.9 & 25.0 \\\\\n' +
      '**VideoPrism-B** & **89.1** (\\(74.5\\)) & **91.1** (\\(70.9\\)) & **64.5** (\\(71.3\\)) & **64.9** (\\(71.3\\)) & 61.6 (\\(1.04\\)) & **28.8** (\\(73.8\\)) \\\\ \\hline \\multicolumn{6}{l}{_Large-scale models_} \\\\ InternVideo-L (Wang et al., 2022) & 86.6 & **91.5** & 65.7 / 65.2 & 51.4 & 25.7 \\\\ UMT-L (Li et al., 2023) & 86.4 & 89.5 & **60.5** / 61.4 & 62.7 & 24.7 \\\\\n' +
      '**VideoPrism-g** & **92.0** (\\(15.4\\)) & **91.5** (\\(70.0\\)) & **65.9** (\\(10.2\\)) & **66.8** (\\(11.6\\)) & **63.3** (\\(70.6\\)) & **31.5** (\\(75.8\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: **Comparison to state-of-the-art methods and domain experts on CV for Science benchmarks. We report mean average precision (mAP) for all datasets, except for KABR which uses macro-accuracy.**image captions to corresponding videos. VAST-27M (Chen et al., 2023b) and InternVid (Wang et al., 2023e) use multi-modal and language models to caption videos. Still, these video-text datasets are significantly smaller than their counterparts for images, and many ViFMs adapt pretrained image-text models to the video space (Fang et al., 2021; Luo et al., 2022; Xue et al., 2023; Liu et al., 2023; He et al., 2023; Wu et al., 2024). Our pretraining corpus has text associations from a hybrid mix of ASR transcripts, generated captions, and high-quality manually annotated captions.\n' +
      '\n' +
      '**Pretraining strategy.** Our pretraining integrates vision-language contrastive learning (Radford et al., 2021; Xu et al., 2021; Bain et al., 2022) and masked data modeling (Devlin et al., 2019; He et al., 2022). The former has led to strong late-fusion models like CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), CoCa (Yu et al., 2022), and the latter is proven effective to learn from single-modality data like language (Devlin et al., 2019; Anil et al., 2023), audio (Borosos et al., 2023), images (He et al., 2022; Wang et al., 2023d; Oquab et al., 2023), and videos (Tong et al., 2022; Wang et al., 2023b). While EVA (Fang et al., 2022; 2023) and UMT (Li et al., 2023b) transfer indirect semantics from CLIP (Radford et al., 2021) to masked modeling, we learn video-native semantics. We also introduce global distillation and token shuffling to the masked video modeling to orchestrate both appearance and motion cues.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'We present VideoPrism, a foundational video encoder that achieves state-of-the-art performance across a wide range of video understanding tasks. Our design emphasizes both the data and modeling approach: we assemble the largest pretraining dataset of its kind, as well as develop a pretraining strategy that effectively learns appearance and motion information from it. In our comprehensive evaluation, VideoPrism achieves the best results on a majority of benchmarks. Notably, no other baseline models consistently achieve the second best, highlighting our unique generalizability.\n' +
      '\n' +
      '## Broader impacts\n' +
      '\n' +
      'Advancements in video understanding have the potential to accelerate progress across various fields, including scientific research, education, robotics, healthcare, and content recommendation. These technologies could empower new scientific discoveries, enhance learning experiences, improve security and safety, and enable more responsive interactive systems. However, it is crucial to address potential biases and misuses before one deploys related models to the real world. This includes mitigating algorithmic biases, safeguarding privacy, and respecting rules and policies of responsible research. To ensure that the benefits of this technology are harnessed responsibly, we encourage continued open discussions in the community around the development of these new technologies.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We sincerely thank David Hendon for their product management efforts, and Alex Siegman, Ramya Ganeshan, and Victor Gomes for their program and resource management efforts. We also thank Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu for the discussions, support, and feedback that greatly contributed to this paper. We thank Jay Yagnik, Rahul Sukthankar, and Tomas Izo for their enthusiastic support for this project.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Akbari et al. (2021) Akbari, H., Yuan, L., Qian, R., Chuang, W.-H., Chang, S.-F., Cui, Y., and Gong, B. VATT: Transformers for multimodal self-supervised learning from raw video, audio and text. In _NeurIPS_, 2021.\n' +
      '* Akbari et al. (2023) Akbari, H., Kondratyuk, D., Cui, Y., Hornung, R., Wang, H., and Adam, H. Alternating gradient descent and mixture-of-experts for integrated multimodal perception. In _NeurIPS_, 2023.\n' +
      '* Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: A visual language model for few-shot learning. In _NeurIPS_, 2022.\n' +
      '* Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. PaLM 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.\n' +
      '* Arnab et al. (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., and Schmid, C. ViViT: A video vision transformer. In _ICCV_, 2021.\n' +
      '* Bagad et al. (2023) Bagad, P., Tapaswi, M., and Snoek, C. G. Test of time: Instilling video-language models with a sense of time. In _CVPR_, 2023.\n' +
      '* Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. In _ICCV_, 2021.\n' +
      '\n' +
      'Bain, M., Nagrani, A., Varol, G., and Zisserman, A. A CLIP-Hitchhiker\'s guide to long video retrieval. _arXiv preprint arXiv:2205.08508_, 2022.\n' +
      '* Bansal et al. (2023) Bansal, H., Bitton, Y., Szepktor, I., Chang, K.-W., and Grover, A. VideoCon: Robust video-language alignment via contrast captions. _arXiv preprint arXiv:2311.10111_, 2023.\n' +
      '* Bao et al. (2022) Bao, H., Dong, L., Piao, S., and Wei, F. BEiT: BERT pre-training of image transformers. In _ICLR_, 2022.\n' +
      '* Bommasani et al. (2021) Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.\n' +
      '* Borosos et al. (2023) Borosos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., et al. AudioLM: A language modeling approach to audio generation. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 31:2523-2533, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In _NeurIPS_, 2020.\n' +
      '* Buch et al. (2022) Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., and Niebles, J. C. Revisiting the "video" in video-language understanding. In _CVPR_, 2022.\n' +
      '* Burgos-Artizzu et al. (2012) Burgos-Artizzu, X. P., Dollar, P., Lin, D., Anderson, D. J., and Perona, P. Social behavior recognition in continuous video. In _CVPR_, 2012.\n' +
      '* Caba Heilbron et al. (2015) Caba Heilbron, F., Escorcia, V., Ghanem, B., and Carlos Niebles, J. ActivityNet: A large-scale video benchmark for human activity understanding. In _CVPR_, 2015.\n' +
      '* Carreira et al. (2018) Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A. A short note about Kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* Chen et al. (2023a) Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al. VideoLLM: Modeling video sequence with large language models. _arXiv preprint arXiv:2305.13292_, 2023a.\n' +
      '* Chen & Huang (2021) Chen, S. and Huang, D. Elaborative rehearsal for zero-shot action recognition. In _ICCV_, 2021.\n' +
      '* Chen et al. (2023b) Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., and Liu, J. VAST: A vision-audio-subtitle-text omni-modality foundation model and dataset. In _NeurIPS_, 2023b.\n' +
      '* Chen et al. (2023c) Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. PaLI: A jointly-scaled multilingual language-image model. In _ICLR_, 2023c.\n' +
      '* Cheng et al. (2023d) Cheng, F., Wang, X., Lei, J., Crandall, D., Bansal, M., and Bertasius, G. VindLU: A recipe for effective video-and-language pretraining. In _CVPR_, 2023d.\n' +
      '* Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT_, 2019.\n' +
      '* Dima et al. (2022) Dima, D., Doughty, H., Farinella, G. M., Antonino, F., Evangelos, K., Ma, J., Davide, M., Munro, J., Toby, P., Price, W., et al. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. _IJCV_, 130(1):33-55, 2022.\n' +
      '* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.\n' +
      '* Eyjolfsdottir et al. (2014) Eyjolfsdottir, E., Branson, S., Burgos-Artizzu, X. P., Hoopfer, E. D., Schor, J., Anderson, D. J., and Perona, P. Detecting social actions of fruit flies. In _ECCV_, 2014.\n' +
      '* Fang et al. (2021) Fang, H., Xiong, P., Xu, L., and Chen, Y. CLIP2Video: Mastering video-text retrieval via image CLIP. _arXiv preprint arXiv:2106.11097_, 2021.\n' +
      '* Fang et al. (2022) Fang, Y., Wang, W., Xie, B., Sun, Q.-S., Wu, L. Y., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA: Exploring the limits of masked visual representation learning at scale. In _CVPR_, 2022.\n' +
      '* Fang et al. (2023) Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., and Cao, Y. EVA-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023.\n' +
      '* Feichtenhofer et al. (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. SlowFast networks for video recognition. In _ICCV_, 2019.\n' +
      '* Feichtenhofer et al. (2021) Feichtenhofer, C., Fan, H., Xiong, B., Girshick, R., and He, K. A large-scale study on unsupervised spatiotemporal representation learning. In _CVPR_, 2021.\n' +
      '* Feichtenhofer et al. (2022) Feichtenhofer, C., Fan, H., Li, Y., and He, K. Masked autoencoders as spatiotemporal learners. In _NeurIPS_, 2022.\n' +
      '* Fu et al. (2021) Fu, T.-J., Li, L., Gan, Z., Lin, K., Wang, W. Y., Wang, L., and Liu, Z. VIOLET: End-to-end video-language transformers with masked visual-token modeling. _arXiv preprint arXiv:2111.12681_, 2021.\n' +
      '\n' +
      '* Gao et al. (2017) Gao, J., Sun, C., Yang, Z., and Nevatia, R. TALL: Temporal activity localization via language query. In _ICCV_, 2017.\n' +
      '* Girdhar et al. (2023) Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., and Misra, I. ImageBind: One embedding space to bind them all. In _CVPR_, 2023.\n' +
      '* Goyal et al. (2017a) Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al. The "something something" video database for learning and evaluating visual common sense. In _ICCV_, 2017a.\n' +
      '* Goyal et al. (2017b) Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In _CVPR_, 2017b.\n' +
      '* Grauman et al. (2022) Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et al. Ego4D: Around the world in 3,000 hours of egocentric video. In _CVPR_, 2022.\n' +
      '* Gu et al. (2018) Gu, C., Sun, C., Ross, D. A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al. AVA: A video dataset of spatio-temporally localized atomic visual actions. In _CVPR_, 2018.\n' +
      '* Gutmann & Hyvarinen (2010) Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In _AISTATS_, 2010.\n' +
      '* He et al. (2022) He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.\n' +
      '* He et al. (2023) He, X., Chen, S., Ma, F., Huang, Z., Jin, X., Liu, Z., Fu, D., Yang, Y., Liu, J., and Feng, J. VLAB: Enhancing video language pre-training by feature adapting and blending. _arXiv preprint arXiv:2305.13167_, 2023.\n' +
      '* Hendricks & Nematzadeh (2021) Hendricks, L. A. and Nematzadeh, A. Probing image-language transformers for verb understanding. In _ACL_, 2021.\n' +
      '* Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In _ICLR_, 2022.\n' +
      '* Huang et al. (2020) Huang, G., Pang, B., Zhu, Z., Rivera, C., and Soricut, R. Multimodal pretraining for dense video captioning. _arXiv preprint arXiv:2011.11760_, 2020.\n' +
      '* Huang et al. (2016) Huang, T.-H., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He, X., Kohli, P., Batra, D., et al. Visual storytelling. In _NAACL-HLT_, 2016.\n' +
      '* Jain et al. (2017) Jain, P., Kar, P., et al. Non-convex optimization for machine learning. _Foundations and Trends(r) in Machine Learning_, 10(3-4):142-363, 2017.\n' +
      '* Jia et al. (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In _ICML_, 2021.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The Kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_, 2017.\n' +
      '* Kholiavchenko et al. (2024) Kholiavchenko, M., Kline, J., Ramirez, M., Stevens, S., Sheets, A., Babu, R., Banerji, N., Campolongo, E., Thompson, M., Van Tiel, N., et al. KABR: In-situ dataset for kenyan animal behavior recognition from drone videos. In _WACV_, 2024.\n' +
      '* Kingma & Ba (2015) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In _ICLR_, 2015.\n' +
      '* Lee et al. (2019) Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set Transformer: A framework for attention-based permutation-invariant neural networks. In _ICML_, 2019.\n' +
      '* Lei et al. (2023) Lei, J., Berg, T. L., and Bansal, M. Revealing single frame bias for video-and-language learning. In _ACL_, 2023.\n' +
      '* Li et al. (2020) Li, A., Thotakuri, M., Ross, D. A., Carreira, J., Vostrikov, A., and Zisserman, A. The AVA-Kinetics localized human actions video dataset. _arXiv preprint arXiv:2005.00214_, 2020.\n' +
      '* Li et al. (2022) Li, J., Li, D., Xiong, C., and Hoi, S. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022.\n' +
      '* Li et al. (2023a) Li, J., Li, D., Savarese, S., and Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023a.\n' +
      '* Li et al. (2023b) Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., and Qiao, Y. Unmasked teacher: Towards training-efficient video foundation models. In _ICCV_, 2023b.\n' +
      '* Li et al. (2023c) Li, L., Gan, Z., Lin, K., Lin, C.-C., Liu, Z., Liu, C., and Wang, L. LAVENDER: Unifying video-language understanding as masked language modeling. In _CVPR_, 2023c.\n' +
      '* Li et al. (2023d) Li, W., Zhu, L., Wen, L., and Yang, Y. DeCap: Decoding CLIP latents for zero-shot captioning via text-only training. In _ICLR_, 2023d.\n' +
      '* Li et al. (2018) Li, Y., Li, Y., and Vasconcelos, N. RESOUND: Towards action recognition without representation bias. In _ECCV_, 2018.\n' +
      '* Li et al. (2018)* Li et al. (2023) Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. Scaling language-image pre-training via masking. In _CVPR_, 2023e.\n' +
      '* Li et al. (2023) Li, Y., Wang, C., and Jia, J. LLaMA-VID: An image is worth 2 tokens in large language models. _arXiv preprint arXiv:2311.17043_, 2023f.\n' +
      '* Lin et al. (2023a) Lin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan, L. Video-LLaVA: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023a.\n' +
      '* Lin et al. (2022) Lin, K. Q., Wang, J., Soldan, M., Wray, M., Yan, R., XU, E. Z., Gao, D., Tu, R.-C., Zhao, W., Kong, W., et al. Egocentric video-language pretraining. In _NeurIPS_, 2022.\n' +
      '* Lin et al. (2023b) Lin, W., Karlinsky, L., Shvetsova, N., Possegger, H., Kozinski, M., Panda, R., Feris, R., Kuehne, H., and Bischof, H. Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge. In _ICCV_, 2023b.\n' +
      '* Liu et al. (2023) Liu, R., Huang, J., Li, G., Feng, J., Wu, X., and Li, T. H. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. In _CVPR_, 2023.\n' +
      '* Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In _ICLR_, 2019.\n' +
      '* Luo et al. (2022) Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., and Li, T. CLIP4Clip: An empirical study of clip for end to end video clip retrieval and captioning. _Neurocomputing_, 508:293-304, 2022.\n' +
      '* Ma et al. (2023) Ma, X., Kaufhold, S. P., Su, J., Zhu, W., Terwilliger, J., Meza, A., Zhu, Y., Rossano, F., and Wang, Y. ChimpACT: A longitudinal dataset for understanding chimpanzee behaviors. _arXiv preprint arXiv:2310.16447_, 2023.\n' +
      '* Maaz et al. (2023) Maaz, M., Rasheed, H., Khan, S., and Khan, F. S. Video-ChatGPT: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_, 2023.\n' +
      '* McCloskey & Cohen (1989) McCloskey, M. and Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pp. 109-165. Elsevier, 1989.\n' +
      '* Miech et al. (2019) Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J. HowTo100M: Learning a text-video embedding by watching hundred million narrated video clips. In _ICCV_, 2019.\n' +
      '* Momeni et al. (2023) Momeni, L., Caron, M., Nagrani, A., Zisserman, A., and Schmid, C. Verbs in action: Improving verb understanding in video-language models. In _ICCV_, 2023.\n' +
      '* Monfort et al. (2019) Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S. A., Yan, T., Brown, L., Fan, Q., Gutfreund, D., Vondrick, C., et al. Moments in Time dataset: one million videos for event understanding. _IEEE TPAMI_, 42(2):502-508, 2019.\n' +
      '* Monfort et al. (2021) Monfort, M., Jin, S., Liu, A., Harwath, D., Feris, R., Glass, J., and Oliva, A. Spoken Moments: Learning joint audio-visual representations from video descriptions. In _CVPR_, 2021.\n' +
      '* Nagrani et al. (2022) Nagrani, A., Seo, P. H., Seybold, B., Hauth, A., Manen, S., Sun, C., and Schmid, C. Learning audio-video modalities from image captions. In _ECCV_, 2022.\n' +
      '* Ni et al. (2022) Ni, B., Peng, H., Chen, M., Zhang, S., Meng, G., Fu, J., Xiang, S., and Ling, H. Expanding language-image pretrained models for general video recognition. In _ECCV_, 2022.\n' +
      '* Noroozi & Favaro (2016) Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving Jigsaw puzzles. In _ECCV_, 2016.\n' +
      '* Oquab et al. (2023) Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al. DINOv2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.\n' +
      '* Peng et al. (2022) Peng, Z., Dong, L., Bao, H., Ye, Q., and Wei, F. BEiT v2: Masked image modeling with vector-quantized visual tokenizers. _arXiv preprint arXiv:2208.06366_, 2022.\n' +
      '* Piergiovanni et al. (2023) Piergiovanni, A., Nobel, I., Kim, D., Ryoo, M. S., Gomes, V., and Angelova, A. Mirasol3B: A multimodal autoregressive model for time-aligned and contextual modalities. _arXiv preprint arXiv:2311.05698_, 2023.\n' +
      '* Pitcher-Cooper et al. (2023) Pitcher-Cooper, C., Seth, M., Kao, B., Coughlan, J. M., and Yoon, I. You Described, We Archived: A rich audio description dataset. _Journal on Technology and Persons with Disabilities_, 2023.\n' +
      '* Qian et al. (2021) Qian, R., Meng, T., Gong, B., Yang, M.-H., Wang, H., Belongie, S., and Cui, Y. Spatiotemporal contrastive video representation learning. In _CVPR_, 2021.\n' +
      '* Qian et al. (2022) Qian, R., Li, Y., Yuan, L., Gong, B., Liu, T., Brown, M., Yang, M.-H., Adam, H., and Cui, Y. On temporal granularity in self-supervised video representation learning. In _BMVC_, 2022.\n' +
      '* Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.\n' +
      '\n' +
      'Recasens, A., Luc, P., Alayrac, J.-B., Wang, L., Strub, F., Tallec, C., Malinowski, M., Patrucean, V., Altche, F., Valko, M., et al. Broaden your views for self-supervised video learning. In _ICCV_, 2021.\n' +
      '* Regneri et al. (2013) Regneri, M., Rohrbach, M., Wetzel, D., Thater, S., Schiele, B., and Pinkal, M. Grounding action descriptions in videos. _Transactions of the Association for Computational Linguistics_, 1:25-36, 2013.\n' +
      '* Ren et al. (2015) Ren, S., He, K., Girshick, R., and Sun, J. Faster R-CNN: Towards real-time object detection with region proposal networks. In _NeurIPS_, 2015.\n' +
      '* Sevilla-Lara et al. (2021) Sevilla-Lara, L., Zha, S., Yan, Z., Goswami, V., Feiszli, M., and Torresani, L. Only time can tell: Discovering temporal data for temporal modeling. In _WACV_, 2021.\n' +
      '* Shazeer & Stern (2018) Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In _ICML_, 2018.\n' +
      '* Sigurdsson et al. (2016) Sigurdsson, G. A., Varol, G., Wang, X., Farhadi, A., Laptev, I., and Gupta, A. Hollywood in Homes: Crowdsourcing data collection for activity understanding. In _ECCV_, 2016.\n' +
      '* Singh et al. (2021) Singh, A., Chakraborty, O., Varshney, A., Panda, R., Feris, R., Saenko, K., and Das, A. Semi-supervised action recognition with temporal contrastive learning. In _CVPR_, 2021.\n' +
      '* Singh et al. (2022) Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. FLAVA: A foundational language and vision alignment model. In _CVPR_, 2022.\n' +
      '* Stroud et al. (2020) Stroud, J. C., Lu, Z., Sun, C., Deng, J., Sukthankar, R., Schmid, C., and Ross, D. A. Learning video representations from textual web supervision. _arXiv preprint arXiv:2007.14937_, 2020.\n' +
      '* Sun et al. (2021a) Sun, J. J., Karigo, T., Chakraborty, D., Mohanty, S. P., Wild, B., Sun, Q., Chen, C., Anderson, D. J., Perona, P., Yue, Y., et al. The multi-agent behavior dataset: Mouse dyadic social interactions. In _NeurIPS D&B_, 2021a.\n' +
      '* Sun et al. (2021b) Sun, J. J., Kennedy, A., Zhan, E., Anderson, D. J., Yue, Y., and Perona, P. Task programming: Learning data efficient behavior representations. In _CVPR_, 2021b.\n' +
      '* Tan et al. (2020) Tan, J., Wang, C., Li, B., Li, Q., Ouyang, W., Yin, C., and Yan, J. Equalization loss for long-tailed object recognition. In _CVPR_, 2020.\n' +
      '* Tang et al. (2019) Tang, Y., Ding, D., Rao, Y., Zheng, Y., Zhang, D., Zhao, L., Lu, J., and Zhou, J. COIN: A large-scale dataset for comprehensive instructional video analysis. In _CVPR_, 2019.\n' +
      '* Tang et al. (2023) Tang, Y., Bi, J., Xu, S., Song, L., Liang, S., Wang, T., Zhang, D., An, J., Lin, J., Zhu, R., et al. Video understanding with large language models: A survey. _arXiv preprint arXiv:2312.17432_, 2023.\n' +
      '* Tong et al. (2022) Tong, Z., Song, Y., Wang, J., and Wang, L. VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In _NeurIPS_, 2022.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In _NeurIPS_, 2017.\n' +
      '* Voigtlaender et al. (2023) Voigtlaender, P., Changpinyo, S., Pont-Tuset, J., Soricut, R., and Ferrari, V. Connecting vision and language with video localized narratives. In _CVPR_, 2023.\n' +
      '* Wang et al. (2022a) Wang, J., Chen, D., Wu, Z., Luo, C., Zhou, L., Zhao, Y., Xie, Y., Liu, C., Jiang, Y.-G., and Yuan, L. OmniVL: One foundation model for image-language and video-language tasks. In _NeurIPS_, 2022a.\n' +
      '* Wang et al. (2023a) Wang, J., Ge, Y., Yan, R., Ge, Y., Lin, K. Q., Tsutsui, S., Lin, X., Cai, G., Wu, J., Shan, Y., et al. All in one: Exploring unified video-language pre-training. In _CVPR_, 2023a.\n' +
      '* Wang et al. (2023b) Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., and Qiao, Y. VideoMAE v2: Scaling video masked autoencoders with dual masking. In _CVPR_, 2023b.\n' +
      '* Wang et al. (2022b) Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.-G., Zhou, L., and Yuan, L. BEVT: BERT pre-training of video transformers. In _CVPR_, 2022b.\n' +
      '* Wang et al. (2023c) Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Yuan, L., and Jiang, Y.-G. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In _CVPR_, 2023c.\n' +
      '* Wang et al. (2023d) Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., et al. Image as a foreign language: BEiT pretraining for vision and vision-language tasks. In _CVPR_, 2023d.\n' +
      '* Wang et al. (2019) Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.-F., and Wang, W. Y. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In _ICCV_, 2019.\n' +
      '* Wang et al. (2022c) Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu, Y., Wang, Z., et al. InternVideo: General video foundation models via generative and discriminative learning. _arXiv preprint arXiv:2212.03191_, 2022c.\n' +
      '* Wang et al. (2023e) Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X., Chen, X., Wang, Y., Luo, P., Liu, Z., et al. InternVid: A large-scale video-text dataset for multimodal understanding and generation. _arXiv preprint arXiv:2307.06942_, 2023e.\n' +
      '\n' +
      'Wang, Z., Li, M., Xu, R., Zhou, L., Lei, J., Lin, X., Wang, S., Yang, Z., Zhu, C., Hoiem, D., et al. Language models with image descriptors are strong few-shot video-language learners. In _NeurIPS_, 2022d.\n' +
      '* Wang et al. (2023) Wang, Z., Blume, A., Li, S., Liu, G., Cho, J., Tang, Z., Bansal, M., and Ji, H. Paxion: Patching action knowledge in video-language foundation models. In _NeurIPS_, 2023f.\n' +
      '* Wei et al. (2022) Wei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C. Masked feature prediction for self-supervised visual pre-training. In _CVPR_, 2022.\n' +
      '* Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G., and Duan, N. Godiva: Generating open-domain videos from natural descriptions. _arXiv preprint arXiv:2104.14806_, 2021.\n' +
      '* Wu et al. (2023) Wu, W., Sun, Z., and Ouyang, W. Revisiting classifier: Transferring vision-language models for video recognition. In _AAAI_, 2023.\n' +
      '* Wu & Palmer (1994) Wu, Z. and Palmer, M. Verb semantics and lexical selection. In _ACL_, 1994.\n' +
      '* Wu et al. (2024) Wu, Z., Weng, Z., Peng, W., Yang, X., Li, A., Davis, L. S., and Jiang, Y.-G. Building an open-vocabulary video CLIP model with better architectures, optimization and data. _IEEE TPAMI_, 2024.\n' +
      '* Xiao et al. (2021) Xiao, J., Shang, X., Yao, A., and Chua, T.-S. NExT-QA: Next phase of question-answering to explaining temporal actions. In _CVPR_, 2021.\n' +
      '* Xiong et al. (2023) Xiong, Y., Zhao, L., Gong, B., Yang, M.-H., Schroff, F., Liu, T., Hsieh, C.-J., and Yuan, L. Spatiotemporally discriminative video-language pre-training with text grounding. _arXiv preprint arXiv:2303.16341_, 2023.\n' +
      '* Xu et al. (2017) Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., and Zhuang, Y. Video question answering via gradually refined attention over appearance and motion. In _ACM MM_, 2017.\n' +
      '* Xu et al. (2021) Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In _EMNLP_, 2021.\n' +
      '* Xu et al. (2023) Xu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi, B., Qian, Q., Wang, W., et al. mLUG-2: A modularized multi-modal foundation model across text, image and video. In _ICML_, 2023.\n' +
      '* Xu et al. (2016) Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large video description dataset for bridging video and language. In _CVPR_, 2016.\n' +
      '* Xu et al. (2020) Xu, M., Zhao, C., Rojas, D. S., Thabet, A., and Ghanem, B. G-TAD: Sub-graph localization for temporal action detection. In _CVPR_, 2020.\n' +
      '* Xue et al. (2022) Xue, H., Hang, T., Zeng, Y., Sun, Y., Liu, B., Yang, H., Fu, J., and Guo, B. Advancing high-resolution video-language representation with large-scale video transcriptions. In _CVPR_, 2022.\n' +
      '* Xue et al. (2023) Xue, H., Sun, Y., Liu, B., Fu, J., Song, R., Li, H., and Luo, J. CLIP-ViP: Adapting pre-trained image-text model to video-language representation alignment. In _ICLR_, 2023.\n' +
      '* Yan et al. (2022) Yan, S., Zhu, T., Wang, Z., Cao, Y., Zhang, M., Ghosh, S., Wu, Y., and Yu, J. VideoCoCa: Video-text modeling with zero-shot transfer from contrastive captioners. _arXiv preprint arXiv:2212.04979_, 2022.\n' +
      '* Yang et al. (2022) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Zero-shot video question answering via frozen bidirectional language models. In _NeurIPS_, 2022.\n' +
      '* Yarom et al. (2023) Yarom, M., Bitton, Y., Changpinyo, S., Aharoni, R., Herzig, J., Lang, O., Ofek, E., and Szpektor, I. What you see is what you read? improving text-image alignment evaluation. In _NeurIPS_, 2023.\n' +
      '* Ye et al. (2023) Ye, Q., Xu, G., Yan, M., Xu, H., Qian, Q., Zhang, J., and Huang, F. HiTeA: Hierarchical temporal-aware video-language pre-training. In _ICCV_, 2023.\n' +
      '* Yu et al. (2022) Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. CoCa: Contrastive captioners are image-text foundation models. _TMLR_, 2022. ISSN 2835-8856.\n' +
      '* Yuan et al. (2021) Yuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al. Florence: A new foundation model for computer vision. _arXiv preprint arXiv:2111.11432_, 2021.\n' +
      '* Yuan et al. (2022) Yuan, L., Qian, R., Cui, Y., Gong, B., Schroff, F., Yang, M.-H., Adam, H., and Liu, T. Contextualized spatio-temporal contrastive learning with self-supervision. In _CVPR_, 2022.\n' +
      '* Yuan et al. (2023) Yuan, L., Gundavarapu, N. B., Zhao, L., Zhou, H., Cui, Y., Jiang, L., Yang, X., Jia, M., Weyand, T., Friedman, L., et al. VideoGLUE: Video general understanding evaluation of foundation models. _arXiv preprint arXiv:2307.03166_, 2023.\n' +
      '* Zellers et al. (2021) Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. MERLOT: Multimodal neural script knowledge models. In _NeurIPS_, 2021.\n' +
      '* Zellers et al. (2022) Zellers, R., Lu, J., Lu, X., Yu, Y., Zhao, Y., Salehi, M., Kusupati, A., Hessel, J., Farhadi, A., and Choi, Y. MERLOT Reserve: Neural script knowledge through vision and language and sound. In _CVPR_, 2022.\n' +
      '\n' +
      'Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. Scaling vision transformers. In _CVPR_, 2022a.\n' +
      '* Zhai et al. (2022b) Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., and Beyer, L. LiT: Zero-shot transfer with locked-image text tuning. In _CVPR_, 2022b.\n' +
      '* Zhang et al. (2023) Zhang, H., Li, X., and Bing, L. Video-LLaMA: An instruction-tuned audio-visual language model for video understanding. _arXiv preprint arXiv:2306.02858_, 2023.\n' +
      '* Zhao et al. (2024) Zhao, Y., Zhao, L., Zhou, X., Wu, J., Chu, C.-T., Miao, H., Schroff, F., Adam, H., Liu, T., Gong, B., et al. Distilling vision-language models on millions of videos. _arXiv preprint arXiv:2401.06129_, 2024.\n' +
      '* Zhou et al. (2022) Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T. iBOT: Image BERT pre-training with online tokenizer. In _ICLR_, 2022.\n' +
      '* Zhou et al. (2018) Zhou, L., Xu, C., and Corso, J. Towards automatic learning of procedures from web instructional videos. In _AAAI_, 2018.\n' +
      '* Zhu et al. (2024) Zhu, B., Lin, B., Ning, M., Yan, Y., Cui, J., Wang, H., Pang, Y., Jiang, W., Zhang, J., Li, Z., et al. LanguageBind: Extending video-language pretraining to N-modality by language-based semantic alignment. In _ICLR_, 2024.\n' +
      '\n' +
      '## Appendix A Pretraining data\n' +
      '\n' +
      '### Data curation\n' +
      '\n' +
      'Section 2.1 and Table 1 have described the pretraining corpus of videos, and the following provides more details about the three in-house datasets.\n' +
      '\n' +
      '**Anonymous-Corpus #1.** This pretraining corpus consists of about 36M commercially licensed stock video-caption pairs, where the videos and text are manually uploaded by professional contributors. Hence, the quality of the videos and captions is high in this corpus compared with the rest.\n' +
      '\n' +
      '**Anonymous-Corpus #2.** This data corpus contains 170M (video, ASR transcript) pairs from 44.6M YouTube videos. Its construction process is similar to HowTo100M (Miech et al., 2019), but the whole corpus is larger and more diverse. Furthermore, the clip-text pairs are filtered based on a groundedness score similar to the CLIP\'s similarity score (Wu et al., 2021).\n' +
      '\n' +
      '**Anonymous-Corpus #3.** This anonymous corpus includes 71.5M (clip, machine-generated caption) pairs from 36.7M YouTube videos. The clips are captioned using vision-language models (Chen et al., 2023c) and further summarized using an LLM (Anil et al., 2023). The corpus is similar to InternVid (Wang et al., 2023e) in terms of construction but a magnitude larger in size and diversity.\n' +
      '\n' +
      '### Corpus analysis\n' +
      '\n' +
      'We randomly sample 100K videos from our video-text pretraining data and show the breakdown analysis in Figure 5. We notice that most of our clips are between 5 to 10 seconds in length and contain 10 to 20 words in the parallel text. In addition, a considerable proportion of clips has duration longer than 10 seconds or captions longer than 20 words. We further show the the CLIP similarity score (Wu et al., 2021) of our corpus in Figure 5c. The large variations of the CLIP similarity scores demonstrate the diverse caption quality of our training data, which we believe is a byproduct of the various ways used to harvest the text.\n' +
      '\n' +
      '## Appendix B Model architecture\n' +
      '\n' +
      'Table 8 shows the VideoPrism model architecture. As mentioned in Section 2.2, the architecture follows the factorized design of ViViT (Arnab et al., 2021). It consists of two separate Transformer modules: a spatial module and a temporal module. After an input video is partitioned into several non-overlapping patches (tokens), the spatial module first models interactions between tokens from the same temporal index. Then the output sequence of token embeddings are forwarded through the temporal module to model interactions between tokens from different temporal indices. The temporal module shares the same setup of the spatial counterpart, except that its number of layers is fixed to four because no performance improvements are observed with more layers added to our largest VideoPrism model. The positional embeddings of our models are learnable (Devlin et al., 2019) and decoupled in spatial and temporal dimensions. They are utilized to encode the position information of the input tokens in space and time, respectively. When we add image-text data to the first-stage pretraining, the images are treated as one-frame videos, and we crop the temporal positional embeddings when handling the image input. Following CoCa (Yu et al., 2022), we pretrain the model with spatial resolution of \\(288\\times 288\\) and patch size \\(18\\times 18\\). We uniformly sample \\(8\\) frames from each video for pretraining and \\(16\\) frames for evaluation by interpolating the temporal positional embedding of our video encoder.\n' +
      '\n' +
      '## Appendix C Implementation details\n' +
      '\n' +
      'In this section, we describe the implementation details and training setups of VideoPrism. We summarize the pretraining configurations in Table 9.\n' +
      '\n' +
      '### Stage 1\n' +
      '\n' +
      '**Model design.** The text encoder of the first-stage model is a standard Transformer (Vaswani et al., 2017). Together with the spatial module in our encoder, it is initialized from\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l} \\hline \\hline Step & Block & Output shape \\\\ \\hline Data & - & \\(8\\times 288\\times 288\\times 3\\) \\\\ Preprocess & Patchify \\([1,18,18]\\) & \\(8\\times 256\\times 1408\\) \\\\ Drop token / Mask & Tube / BEVT & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Spatial encoder & MSA (6144) \\(\\times 40\\) & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Normalization & LayerNorm & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Transpose & Switch dimension & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Temporal encoder & MSA (6144) \\(\\times 4\\) & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Normalization & Layer Norm & \\(256\\times[8\\times(1-\\rho)]\\times 1408\\) \\\\ Transpose & Switch dimension & \\([8\\times(1-\\rho)]\\times 256\\times 1408\\) \\\\ Reshape & Merge dimension & \\([2048\\times(1-\\rho)]\\times 1408\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: **Encoder architecture of VideoPrism-g.** When describing the output shape, we use {temporal, spatial, and channel} as the order of dimensions when applicable, and we omit the batch size for simplicity. We highlight the dimension that a step applies to by underline. Note that the drop token or masking ratio \\(\\rho\\) is set to \\(0.5\\) in Stage 1 and \\(0.65\\) in Stage 2.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l} \\hline \\hline Configuration & Stage 1 & Stage 2 \\\\ \\hline Optimizer & AdaFactor & AdaFactor \\\\ Base learning rate & \\(5\\times 10^{-4}\\) & \\(5\\times 10^{-4}\\) \\\\ Learning rate schedule & linear decay & cosine decay \\\\ Warmup iterations & \\(2\\times 10^{4}\\) & \\(2.5\\times 10^{4}\\) \\\\ Training iterations & \\(2\\times 10^{5}\\) & \\(3\\times 10^{5}\\) \\\\ Weight decay & \\(1\\times 10^{-4}\\) & \\(0.05\\) \\\\ Batch size & \\(4096\\) & \\(4096\\) \\\\ Drop token or Mask & \\(0.5\\) (Tube mask) & \\(0.65\\) (BEVT mask) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: **Summary of our pretraining configurations.**the unimodal text decoder of CoCa (Yu et al., 2022). We attach a MAP layer (Lee et al., 2019; Yu et al., 2022) to the end of the video encoder to extract the global embedding from the encoder output. For the text encoder, we append a learnable class token at the end of the input sentence and use its corresponding output as the text embedding.\n' +
      '\n' +
      '**Training.** In contrast to existing methods that use batch mixing, we adopt alternating gradient descent (AGD) (Jain et al., 2017) to contrastively train our first-stage model with multiple datasets. It alternates samples from different datasets as mini-batches during training, shown effective in a multi-task and multi-dataset scenario (Akbari et al., 2023). This is particularly useful for our model to avoid easy negatives within a batch, since samples from the same dataset usually follow the same distribution and are harder to distinguish. Furthermore, we observe that the AGD approach scales well as we add more datasets or increase the size of the corpus.\n' +
      '\n' +
      'The training of the first-stage model follows the conventional setup of vision-language contrastive learning (Radford et al., 2021). To reduce the memory cost during pretraining, we drop \\(50\\%\\) of video tokens as in Li et al. (2023) and the tube masking strategy (Feichtenhofer et al., 2022) is employed for dropping tokens. The teacher model is optimized using Adafactor (Shazeer and Stern, 2018) with the batch size of \\(4096\\). We set the learning rate to \\(1\\times 10^{-4}\\) for our base model and \\(5\\times 10^{-5}\\) for the giant model. We train the first-stage model for \\(2\\times 10^{5}\\) steps with \\(2\\times 10^{4}\\) warm up steps and linear learning rate decay. A symmetric cross-entropy loss (Gutmann and Hyvarinen, 2010; Radford et al., 2021; Jia et al., 2021; Cheng et al., 2023) is used in the first-stage training.\n' +
      '\n' +
      '### Stage 2\n' +
      '\n' +
      '**Token-wise distillation.** As discussed in Section 2.3.2, after training the first-stage model with contrastive learning, we train the VideoPrism video encoder with masked modeling to reconstruct the spatiotemporal embeddings from the first-stage model. As shown in Figure 3, the training pipeline of the second stage is similar to MVD (Wang et al., 2023). After patchifying the input video sequence to a set of tokens, we apply BEVT masking (Wang et al., 2022) with a masking ratio of \\(0.65\\) to randomly remove some of the tokens. The second-stage video encoder, which is initialized from the first-stage encoder, takes the remaining visible tokens as input and predict their embeddings. A learnable mask token is then used to fill in the position of the masked tokens to form a full sequence together with these visible embeddings. The full sequence of embeddings is then randomly shuffled and added with positional embedding before being fed into a shallow decoder which is a four-layer Transformer. A linear layer is then used to align the output of the decoder with the embeddings of the first-stage video encoder by minimizing their cosine distance. Algorithm 1 presents a pseudocode implementation of the proposed token shuffling for masked video modeling.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l l l} \\hline \\hline \\multirow{2}{*}{Step} & \\multirow{2}{*}{Block} & \\multicolumn{2}{c}{Decoder output shape} \\\\  & & Local & Global \\\\ \\hline \\multirow{2}{*}{Data} & - & \\(2048\\times 1408\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(1408\\) \\\\  & MLP & \\(2048\\times 512\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(512\\) \\\\  & MSA (2048) \\(\\times\\)\\(512\\) & \\(2048\\times 512\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(512\\) \\\\  & MLP & \\(2048\\times 1408\\) & \\(2048\\times(1-\\rho)\\)\\(\\times\\)\\(1408\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: **Decoder architectures of VideoPrism-g**. We highlight the dimension that a step applies to by underline. Note that the masking ratio \\(\\rho\\) is set to \\(0.65\\).\n' +
      '\n' +
      'Figure 5: **Analysis on the video-text pretraining corpus.**\n' +
      '\n' +
      '**Global distillation.** To distill the global visual embedding from the first-stage model, we employ a four-layer Transformer decoder followed by a MAP layer to take the visible embeddings from the second-stage video encoder as input and output a global embedding. We do not apply token shuffling or add positional embedding for this decoder. We then align this second-stage global embedding to the global visual embedding from the first-stage model using a cosine distance loss. Please note that the global visual embedding from the first-stage model is predicted by the same MAP head of contrastive training in the first stage. Table 10 shows the decoder architectures in this stage.\n' +
      '\n' +
      '**Training.** We train the second-stage video encoder using the same video clips for the first-stage model, excluding WebLI (Chen et al., 2023c), the image-based dataset. We use Adafactor (Shazeer and Stern, 2018) for optimization. The second-stage video encoder is trained with batch size \\(4096\\) and a starting learning rate of \\(5\\times 10^{-4}\\). The learning rate is decayed to \\(1\\times 10^{-5}\\) with a cosine annealing schedule. \\(2.5\\times 10^{4}\\) warm up steps are also used to linearly increase the learning rate from \\(0\\) to \\(5\\times 10^{-4}\\) at the beginning. The second-stage video encoder is trained for \\(3\\times 10^{5}\\) steps. We apply the same weight for token-wise distillation loss and global distillation loss in the second-stage training.\n' +
      '\n' +
      '## Appendix D Evaluation data\n' +
      '\n' +
      'Table 11 summarizes all the datasets and their corresponding metrics utilized for evaluation in this paper. The evaluation datasets are categorized into four parts: general video-only understanding (VideoGLUE (Yuan et al., 2023)), video-text retrieval, captioning & QA, and CV for science. Within each category, we select representative datasets and report the standard metric on each of them.\n' +
      '\n' +
      'We compare the performance of VideoPrism to the previous best-performing foundation models in Figure 2. For each dataset and task, we compute the performance gain (\\(\\Delta\\)Score) with respect to the best reported number achieved by an image or video foundation model. We collect all of them and plot in descending order.\n' +
      '\n' +
      '## Appendix E VideoGLUE\n' +
      '\n' +
      '### Tasks and task heads for VideoPrism\n' +
      '\n' +
      'We follow the VideoGLUE (Yuan et al., 2023) setup for video-only evaluations. Given a video clip of shape \\(T\\times H\\times W\\times 3\\), VideoPrism produces a set of visual tokens of shape \\(T\\times\\frac{H}{18}\\times\\frac{W}{18}\\times D\\), where \\(T\\), \\(H\\), and \\(W\\) are the number of frames, image height, and image width, respectively, and \\(D\\) is the feature length.\n' +
      '\n' +
      'In all our video classification tasks, we employ a multi-head attention pooling (MAP) layer as our task head, which consists of Transformer layers with \\(12\\) heads and hidden size \\(768\\). A class token is prepended to cross-attend to all visual tokens from VideoPrism for final classifications. We use batch size \\(256\\) when training the task heads. We apply the same data augmentation strategies and training recipes for each individual dataset as described in VideoGLUE and perform multi-view evaluations.\n' +
      '\n' +
      'Spatiotemporal action localization requires to localize person instances in an input video and recognize their actions. In our experiments, instance-level features are first RoIPooled (Ren et al., 2015) from visual tokens by using corresponding instance boxes. These features are then used to query all other visual tokens through cross-attention layers. We use a Transformer layer with \\(12\\) heads and hidden size \\(768\\) as the task heads. Final query tokens are classified via a linear classifier. We use the groundtruth instance boxes with their associated action labels for training. At test time, we use the same pretrained person detector as in Feicintenhofer et al. (2019) for person detection on AVA. On AVA-Kinetics, we use the detector described in Li et al. (2020). We train the models with batch size \\(256\\).\n' +
      '\n' +
      'For temporal action localization, we only apply VideoPrism under frozen and multi-layer attention pooler (MLAP) settings, since the long video samples do not allow end-to-end tuning. In the MLAP setting, we pool features and input them to a G-TAD head (Xu et al., 2020). We use batch size \\(32\\) and train G-TAD on ActivityNet v1.3 for \\(10\\) epochs.\n' +
      '\n' +
      'We employ AdamW (Loshchilov and Hutter, 2019) optimizer and cosine learning rate decay in all video-only experiments. For more details on the experiment setups, we refer readers to the VideoGLUE paper (Yuan et al., 2023).\n' +
      '\n' +
      'We experiment VideoPrism under two configurations regarding different input video sizes. In the first configuration (marked by asterisk "*" in Tables \\(12\\) to \\(15\\)), we use \\(8\\) frames and \\(252\\times 252\\) image resolution for feature extraction when training video classification task heads, which results in a sequence of \\(8\\times 14\\times 14\\) tokens. On AVA and AVA-Kinetics, video clips of shape \\(8\\times 288\\times 288\\) (_i.e._, token length \\(8\\times 16\\times 16\\)) are used for both training and evaluation. This configuration aligns the trainable FLOPs of VideoPrism with the other baseline models reported in Yuan et al. (2023). In the second configuration, we use video clips of shape \\(16\\times 288\\times 288\\) as input for all the experiments. This configuration aligns with the pretraining setup with higher trainable FLOPs and accounts for the results in Section 3.1.\n' +
      '\n' +
      '### Adaptations\n' +
      '\n' +
      'We follow Yuan et al. (2023) to report model performances under four adaptation settings, namely frozen model backbone with simple MAP heads, with MLAP heads, and with\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Datasets & \\multicolumn{2}{c|}{Tasks} & \\multicolumn{2}{c|}{Zero-shot} & \\multicolumn{1}{c}{Abbr.} & \\multicolumn{1}{c}{Metrics} \\\\ \\hline Kinetics-400 (Kay et al., 2017) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ MiT (Momfort et al., 2019) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ SS2G (Goyal et al., 2017a) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ Dwingles (Li et al., 2018) & Video Classification & ✗ & VC & Top-1 Acc. \\\\ Charades (Sigurdsson et al., 2016) & Video Classification & ✗ & VC & mAP \\\\ ActivityNet (Caho Heilbron et al., 2015) & Temporal Action Localization & ✗ & TAL & mAP \\\\ AVA (Gu et al., 2018) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ AVA-Kinetics (Li et al., 2020) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ \\hline MSRVT (Xu et al., 2016) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ MSRVTT (Xu et al., 2016) & Video-to-Text Retrieval & ✓ & 2S2VT & Recall@1, Recall@1 \\\\ VATEX (Wang et al., 2019) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ VATEX (Wang et al., 2019) & Video-to-Text Retrieval & ✓ & 2S2VT & Recall@1, Recall@5 \\\\ ActivityNet (Caho Heilbron et al., 2015) & Text-to-Video Retrieval & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ Kinetics-400 (Xay et al., 2017) & Video Classification & ✓ & 2ST2V & Recall@1, Recall@5 \\\\ Kinetics-600 (Carreira et al., 2018) & Video Classification & ✓ & 2SC & Top-1 \\& Rep-5 Acc. \\\\ SS2-Temporal (Sevilla-Lara et al., 2021) & Video Classification & ✓ & ZSC & Top-1 Acc. \\\\ SS2-Events (Bagd et al., 2023) & Video Classification & ✓ & ZSC & Top-1 Acc. \\\\ NEX-QA (Atrp-Hard) (Xiao et al., 2021) & Video Classification & ✓ & ZSC & MC Acc. \\\\ Charades (Sigurdsson et al., 2016) & Video Classification & ✓ & ZSC & mAP \\\\ Charades-STA (Gao et al., 2017) & Video Classification & ✓ & ZSC & MC Acc. \\\\ \\hline MSRVT (Xu et al., 2016) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ VATEX (Wang et al., 2019) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ VooCoCo2 (Zhou et al., 2018) & Video Captioning & ✓ & ZSCap & CIDEr \\\\ MSRTT-QA (Xia et al., 2017) & Video Question Answering & ✓ & ZSMa & Top-1 Acc. \\\\ MSVD-QA (Xia et al., 2017) & Video Question Answering & ✓ & ZSMa & WUPS \\\\ NEXT-QA (Xiao et al., 2021) & Video Classification & ✗ & VC & mAP \\\\ \\hline Fly vs. Fly (Eyjöldsottint et al., 2014) & Video Classification & ✗ & VC & mAP \\\\ CaTM21 (Sun et al., 2021) & Video Classification & ✗ & VC & mAP \\\\ CRIM13 (Sdee view) (Burgos-Arrizzu et al., 2012) & Video Classification & ✗ & VC & mAP \\\\ CRIM13 (Top view) (Burgos-Arrizzu et al., 2012) & Video Classification & ✗ & VC & mAP \\\\ KABR (Chulavchenko et al., 2024) & Video Classification & ✗ & VC & Macro Acc. \\\\ ChimpACT (Ma et al., 2023) & Spatiotemporal Action Localization & ✗ & STAL & mAP \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: **Summary of evaluation datasets.** We report the corresponding standard metric for each dataset, including Top-1/5 Accuracy (Acc.) for classification and question answering, mean Average Precision (mAP) for multi-label classification, Recall@1/5 for retrieval, multi-choice retrieval accuracy (MC Acc.) for multi-choice retrieval, CIDEr score for captioning, Wu-Palmer Similarity (WUPS) index for question answering, and macro-accuracy (Macro Acc.) for the KABR dataset.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline  & VC (A) & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{1}{c}{Trainable} \\\\ Methods & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 75.2 & 32.6 & 41.0 & 44.1 & 11.2 & 32.7 & 21.1 & 25.9 & 3.72 \\\\ VATT (Akhavi et al., 2021) & 75.1 & 32.1 & 57.8 & 49.7 & 33.3 & 35.3 & 20.3 & 22.2 & 3.72 \\\\ CoCo-B (Vu et al., 2022) & 73.1 & 32.0 & 41.5 & 34.1 & 8.8 & 33.0 & 23.3 & 24.7 & 3.72 \\\\ FLAVA-B (Singh et al., 2022) & 71.3 & 29.7 & 40.6 & 45.9 & 12.6 & 32.2 & 18.8 & 21.5 & 3.72 \\\\ VideoAdNE-B (Tong et al., 2022) & 65.1 & 23.0 & 53.9 & 59.5 & 11.3 & 33.0 & 16.0 & 19.9 & 3.72 \\\\ InterVideo-B (Wang et al., 2022) & 69.3 & 26.3 & 58.2 & 55.6 & 13.0 & 33.3 & 13.4 & 15.7 & 3.72 \\\\ UMT-B (Li et al., 2023b) & 77.1 & 34.0 & 47.7 & 47.8 & 30.1 & 35.8 & 20.7 & 21.1 & 3.72 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 82.8 & 40.0 & 61.8 & 59.5 & 38.7 & 36.6 & 29.9 & 32.0 & 3.72 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.2 & 40.8 & 63.6 & 67.4 & 40.4 & 36.6 & 30.6 & 31.8 & 9.71 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 12: **Results of FM adaptation using frozen features on video understanding tasks.** The model backbones are frozen and only weights in the task heads are updated using the downstream tasks’ training sets. \\({}^{*}\\) indicates the model is evaluated under the setting with trainable FLOPs alignment.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c|}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 81.0 & 39.0 & 46.6 & 75.7 & 54.3 & - & 27.1 & 28.9 & 367 \\\\ VATT-B (Akbari et al., 2021) & 77.1 & 34.8 & 65.1 & 77.6 & 55.7 & - & 27.0 & 28.4 & 371 \\\\ CoA-B (Wu et al., 2022) & 82.6 & 43.6 & 66.8 & 79.6 & 55.0 & - & 27.7 & 31.0 & 367 \\\\ FLAVA-B (Singh et al., 2022) & 79.1 & 38.3 & 61.1 & 72.0 & 48.6 & - & 22.0 & 25.6 & 367 \\\\ VideoMAE-B (Tong et al., 2022) & 78.7 & 36.1 & 65.5 & 75.5 & 51.4 & - & 23.5 & 26.2 & 367 \\\\ InterVideo-B (Wang et al., 2022) & 80.1 & 35.9 & 67.0 & 75.8 & 52.2 & - & 27.2 & 29.8 & 367 \\\\ UMT-B (Li et al., 2023b) & 83.3 & 38.7 & 67.0 & 79.2 & 57.1 & - & 28.8 & 30.9 & 367 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.4 & 43.9 & 68.2 & 82.3 & 58.1 & - & 33.3 & 35.3 & 374 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 85.7 & 44.2 & 70.0 & 84.9 & 60.1 & - & 33.4 & 35.9 & 977 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 16: **Stratified average scores under four adaptation methods and the final VGS. \\({}^{*}\\) indicates the model is evaluated under the setting with trainable FLOPs alignment.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c|}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 77.1 & 39.0 & 50.1 & 55.8 & 41.5 & 33.9 & 27.7 & 29.6 & 14.9 \\\\ VATT-B (Akbari et al., 2021) & 75.1 & 35.6 & 58.7 & 60.1 & 58.2 & 35.0 & 22.9 & 24.1 & 14.9 \\\\ CoCa-B (Yu et al., 2022) & 74.2 & 37.2 & 45.9 & 48.4 & 19.6 & 33.3 & 24.4 & 27.0 & 14.9 \\\\ FLAVA-B (Singh et al., 2022) & 71.5 & 34.5 & 43.1 & 58.5 & 38.2 & 32.4 & 21.3 & 23.2 & 14.9 \\\\ VideoMAE-B (Tong et al., 2022) & 71.7 & 32.2 & 57.4 & 69.6 & 35.9 & 33.4 & 19.6 & 22.1 & 14.9 \\\\ InterVideo-B (Wang et al., 2022c) & 73.7 & 34.7 & 60.3 & 71.9 & 40.5 & 33.6 & 15.9 & 17.7 & 14.9 \\\\ UMT-B (Li et al., 2023b) & 77.5 & 38.0 & 51.2 & 55.5 & 55.8 & 36.0 & 24.6 & 25.8 & 14.9 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 83.7 & 43.9 & 64.6 & 70.7 & 56.6 & 37.2 & 31.5 & 33.1 & 14.9 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.5 & 43.8 & 66.3 & 73.6 & 58.6 & 37.2 & 31.4 & 33.0 & 38.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: **Results of FM adaptation using frozen backbones with MLAP heads on video understanding tasks. MLAP takes multiple frozen features from an FM as inputs and map them hierarchically for the final task prediction. Only the MLAP layer weights are updated using the downstream tasks’ training sets. \\({}^{*}\\) indicates the model is evaluated under the setting with trainable FLOPs alignment.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 80.2 & 39.7 & 56.0 & 77.2 & 44.2 & - & 24.5 & 28.0 & 6.44 \\\\ VATT-B (Akbari et al., 2021) & 75.0 & 36.5 & 63.5 & 68.9 & 53.5 & - & 22.3 & 25.8 & 6.44 \\\\ CoCa-B (Yu et al., 2022) & 80.9 & 41.4 & 56.1 & 67.1 & 45.8 & - & 26.6 & 28.7 & 6.44 \\\\ FLAVA-B (Singh et al., 2022) & 74.7 & 34.1 & 52.1 & 68.4 & 40.8 & - & 17.9 & 23.8 & 6.44 \\\\ VideoMAE-B (Tong et al., 2022) & 73.6 & 30.6 & 61.4 & 76.0 & 43.0 & - & 16.6 & 23.3 & 6.44 \\\\ InterVideo-B (Wang et al., 2022c) & 75.5 & 31.3 & 63.9 & 73.6 & 46.2 & - & 19.2 & 25.5 & 6.44 \\\\ UMT-B (Li et al., 2023b) & 81.5 & 40.4 & 61.8 & 78.5 & 50.0 & - & 27.8 & 29.4 & 6.44 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 84.5 & 44.0 & 66.3 & 83.0 & 57.8 & - & 33.6 & 35.7 & 8.71 \\\\\n' +
      '**VideoPrism-B\\({}^{*}\\)** & 85.7 & 43.9 & 68.8 & 85.1 & 60.6 & - & 34.1 & 35.8 & 22.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: **Results of FM adaptation using frozen backbones with low-rank adapters and task heads. Only the weights of the low-rank adapters and task heads are updated using downstream tasks’ training sets. \\({}^{*}\\) indicates the model is evaluated under the setting with trainable FLOPs alignment.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c|c|c|c|c} \\hline \\hline Methods & \\multicolumn{2}{c|}{VC (A)} & \\multicolumn{2}{c|}{VC (M)} & \\multicolumn{2}{c|}{VC (ML)} & \\multicolumn{2}{c|}{TAL} & \\multicolumn{2}{c}{STAL} & \\multicolumn{2}{c}{Trainable} \\\\  & **K400** & **MIT** & **SSv2** & **D48** & **Charades** & **ActivityNet** & **AVA** & **AVA-K** & FLOPs (B) \\\\ \\hline CLIP-B (Radford et al., 2021) & 80.2 & 39.7 & 56.0 & 77.2 & 44.2 & - & 24.5 & 28.0 & 6.44 \\\\ VATT-B (Akbari et al., 2021) & 75.0 & 36.5 & 63.5 & 68.9 & 53.5 & - & 22.3 & 25.8 & 6.44 \\\\ CoCa-B (Yu et al., 2022) & 80.9 & 41\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:21]\n' +
      '\n' +
      '(K600) (Carreira et al., 2018) in this section. As shown in Table 17, we can find that VideoPrism achieves the best results compared with state-of-the-art FMs that are pretrained with vision and language modalities. Although Language-Bind (Zhu et al., 2024) and IMP (Akbari et al., 2023) use additional modalities (_e.g._, audio) during pretrainig, our results are still comparable to them. More importantly, our base-scale model is able to outperform a majority of methods with even larger scales. These observations are consistent with the ones we draw from Table 4 in the main text.\n' +
      '\n' +
      '## Appendix G Gluing VideoPrism with PaLM-2\n' +
      '\n' +
      'In Section 3.3, we provided evidence of the strength and generalizability of VideoPrism by showing that we can easily fuse it with a pretrained LLM decoder in a further training stage for good performance on tasks that are generative in language such as video captioning and video QA. We provide details about model training and our evaluation protocols in what follows.\n' +
      '\n' +
      '**Implementation.** We pass the features of our video encoder through a one-layer Perceiver Resampler (Alayrac et al., 2022) that outputs a fixed number of continuous tokens representing the input video. It is always set to be \\(256\\) in our experiments. These tokens are then prepended to the embedded text prompt and fed into a LLM decoder, _i.e._, PaLM-2 (Anil et al., 2023). The resampled features are then added with the original query features via skip connection. Note that there are two differences from the original implementation (Alayrac et al., 2022). First, a separate LayerNorm is used for query and key features as we find it works better than the shared LayerNorm. Second, we do not concatenate the key features with the query features before the cross attention, since the feature dimensions from VideoPrism is different from the pretrained PaLM-2. Otherwise, the feature dimensions would need to be projected via a linear projection layer before the concatenation, and we find it leads to unstable training. We ablate with different number of Resampler layers (_i.e._, \\(1\\), \\(3\\), and \\(6\\)) and find that the one-layer Resampler works the best in our experiments.\n' +
      '\n' +
      '**Model training.** We train this multimodal model on a combination of video-text captioning data from the pretraining stage, an aggregated Academic-Corpus, and VQAv2 (Goyal et al., 2017) (an image QA dataset) using a standard autoregressive language modeling loss. Table 18 lists all the datasets in Academic-Corpus, which includes Ego4D (Grauman et al., 2022), EPIC-Kitchens (Dima et al., 2022), Spoken Moments In Time (Monfort et al., 2021), _etc._, totalling 4.4M video clips.\n' +
      '\n' +
      'Both the video encoder and the LLM are kept entirely frozen during training, only the one-layer Resampler is optimized. We train VideoPrism-B with PaLM-2-1B and PaLM-2-8B separately. We set batch size to be \\(256\\) for PaLM-2-1B and \\(64\\) for PaLM-2-8B and trained for \\(2\\times 10^{5}\\) steps. We use Adam optimizer (Kingma and Ba, 2015) with weight decay \\(1\\times 10^{-4}\\) and the learning rate is set to be peaked at \\(5\\times 10^{-4}\\) with warmup steps \\(1\\times 10^{4}\\) and then linearly decreased. Beta1 is set to be \\(0.9\\) and Beta2 is set to be \\(0.999\\). We do not set EMA decay, L2 regularizer weight decay, and gradient clipping in the training. Each frame is center-cropped to \\(346\\) before being randomly cropped to \\(288\\) during the training and center-cropped to \\(288\\) during the evaluation. We set the maximum decoding steps to be \\(32\\) since the datasets in this work have relatively short answers. We use greedy decoding for all our experiments.\n' +
      '\n' +
      'Footnote 1: The final text-only two-shot prompts we employed are “_question: who is talking to his family? answer: man.”_ and “_question: what is a woman doing? answer: talk._” on MSRVTT-QA, and “_question: who is using a wrench on a pipe fitting? answer: man._” and “_question: who breaks an egg into a bowl? answer: woman._” on MSVD-QA.\n' +
      '\n' +
      '**Model evaluation.** We report both open-vocabulary and closed-vocabulary evaluation results for MSRVTT-QA and MSVD-QA in Table 19. For the open-vocabulary configuration, we adopt the zero-shot approach of Flamingo (Alayrac et al., 2022) and use two-shot text-only prompts from the training set on each downstream dataset. The use of two-shot text-only prompts is to guide the output style of the answers. We use the following process to select the two-shot prompts for each dataset. We first choose the two most common answers from the training set of the dataset, and then for each of them, a question is randomly drawn from ones in the training set with the corresponding answer.2 Compared to Flamingo-9B, VideoPrism-B with PaLM-2-8B shows an absolute \\(11.1\\%\\) and \\(12.5\\%\\) gain on MSRVTT-QA and MSVD-QA, respectively.\n' +
      '\n' +
      'Footnote 2: The final text-only two-shot prompts we employed are “_question: who is talking to his family? answer: man.”_ and “_question: what is a woman doing? answer: talk._” on MSRVTT-QA, and “_question: who is using a wrench on a pipe fitting? answer: man._” and “_question: who breaks an egg into a bowl? answer: woman._” on MSVD-QA.\n' +
      '\n' +
      'Additionally, for MSRVTT-QA and MSVD-QA, we experiment with the closed-vocabulary evaluation configuration, following Li et al. (2022); Yang et al. (2022). In this case, instead of directly outputting an answer via the language decoder, we score candidate answers using the log-likelihood of the decoder and choose the answer with the top score. The candidate answers are picked by taking the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Datasets & \\# of clips \\\\ \\hline Video Story Telling (Huang et al., 2016) & 3K \\\\ TACoS (Regneri et al., 2013) & 4K \\\\ YouDescribe (Pücher-Cooper et al., 2023) & 19K \\\\ Charades (Sigurdsson et al., 2016; Gao et al., 2017) & 20k \\\\ COIN (Tang et al., 2019) & 24K \\\\ VITT (Huang et al., 2020) & 35K \\\\ VLN (Voigtlaender et al., 2023) & 37K \\\\ EPIC-Kitchens-100 (Dima et al., 2022) & 67K \\\\ Spoken Moments in Time (Monfort et al., 2021) & 481K \\\\ Ego4D (Grauman et al., 2022; Lin et al., 2022) & 3.8M \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 18: **Datasets included in Academic-Corpus.**top-\\(K\\) most frequently appearing one-token answers from the training and validation sets of the dataset, where \\(K\\) is optimized over the validation set by ablating over the values \\(\\{100,250,500,1000,2000\\}\\). For both MSRVTT-QA and MSVD-QA, we find \\(K=250\\) to work best. Any example where the groundtruth answer is not one of the candidate answers is automatically marked as incorrect. This method additionally steers the model towards answers that fit the exact style of the particular dataset and boosts performance further. In the closed-vocabulary evaluation configuration, VideoPrism-B with PaLM-2-8B outperforms FrozenBiLM-L by an absolute margin of \\(1.2\\%\\) and \\(4.4\\%\\) on MSRVTT-QA and MSVD-QA, respectively.\n' +
      '\n' +
      'Recently, a number of works [11, 12, 13] have begun evaluating captioning and VideoQA tasks using an LLM-in-the-loop protocol, where an LLM such as ChatGPT3 is used to compare predictions to ground-truth answers along a number of different dimensions (_e.g._, correctness of information, temporal understanding, consistency). This can help mitigate the issue of metrics like exact-match and BLEU score being overly reliant on superficial token matching. We leave it to future work to compare against these models using these new protocols.\n' +
      '\n' +
      'Footnote 3: [https://chat.openai.com](https://chat.openai.com)\n' +
      '\n' +
      '## Appendix H CV for Science\n' +
      '\n' +
      'We evaluate the CV for Science datasets using frozen features with the same feature extraction setup (MAP probing) as the VideoGLUE tasks in Section E.1. The datasets are: Fly vs. Fly [10] for fly video classification, CalMS21 [13] for mouse video classification from top view, CRIM13 [12] for mouse video classification with top and side views, ChimpACT [13] for chimp spatiotemporal action localization, and KABR [14] for video classification with Kenyan animals. The domain expert models reported in the main paper are trained on the training split of each dataset, and reported originally in task programming [13] for Fly vs. Fly, CalMS21 1D ConvNet with extra unlabelled data [13] for CalMS21, KABR X3D [14] for KABR, and ChimpACT SlowFast [13] for ChimpACT. For each dataset, we use the train and test splits defined by existing work, with the same metrics (mAP for all works, except KABR, which uses macro-accuracy averaged across classes). For Fly vs. Fly, we use the data split defined in [13], which includes all behaviors with more than \\(1000\\) frames of annotations in the training set. We note that following previous work [13, 12], in datasets where there are background classes, the metric is only averaged across behaviors-of-interest (not including background classes).\n' +
      '\n' +
      'We extract all frames from the video at the original FPS of each dataset. We use \\(16\\) frames as input in Fly vs. Fly, CalMS21, and CRIM13, \\(64\\) frames for ChimpACT, and \\(16\\) frames with a stride of \\(5\\) for KABR, following baselines. Note that for ChimpACT, the benchmark uses groundtruth bounding boxes during training and testing, which we follow.\n' +
      '\n' +
      'The training setup and implementation details are similar to the VideoGLUE frozen-backbone setting (MAP probing) in Appendix E.1. We use the AdamW [14] optimizer and cosine learning rate decay in the CV for science experiments. For data augmentation, we use the same ones as other video classification datasets in VideoGLUE (_e.g._, Charades, Diving48, and MiT) for our video classification datasets. For ChimpACT (spatiotemporal action localization), we use the AVA data augmentation.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c|c c} \\hline \\hline Methods & Two-shot prompting & Closed-vocab & **MSRVTT-QA** & **MSVD-QA** \\\\ \\hline \\multicolumn{5}{l}{_Question-answering-only models_} \\\\ FrozenBiLM-L [13] & ✗ & ✓ & 22.2 & 39.0 \\\\ \\hline \\multicolumn{5}{l}{_All-some models_} \\\\ BILP-B [10] & ✗ & ✓ & 19.2 & 35.2 \\\\ HTEA-B [10] & ✗ & ✓ & 21.7 & 37.4 \\\\ mTLEQ-2 [13] & ✗ & ✓ & 43.8 & 55.3 \\\\ Flanning-3B [11] & ✗ & ✗ & 11.0 & 27.5 \\\\ Flanning-9B [11] & ✓ & ✗ & 13.7 & 30.2 \\\\ \\hline \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✓ & ✗ & 19.5 & 36.7 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✗ & ✓ & 23.1 & 43.2 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-1B} & ✓ & ✓ & 28.5 & 39.5 \\\\ \\hline \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✓ & ✗ & 24.8 & 42.7 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✗ & ✓ & 23.4 & 43.4 \\\\ \\multicolumn{5}{l}{_VideoPrism-B_ w/ PaLM-2-8B} & ✓ & ✓ & **32.0** & **47.1** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 19: **More detailed comparison to state-of-the-art methods on zero-shot video question answering. We include additional results under the two-shot prompting and closed-vocabulary settings. We report Top-1 accuracy for both MSRVTT-QA and MSVD-QA. Methods that unfreeze their language models during training are marked in gray.**We use a learning rate of \\(5\\times 10^{-5}\\) for video classification and spatiotemporal action localization, except for KABR, where the base-scale model uses \\(5\\times 10^{-6}\\) and large-scale model uses \\(1\\times 10^{-6}\\). Following the baseline, KABR is also trained with the EQL loss (Tan et al., 2020) to account for class imbalance. Finally, all models are trained with \\(0.5\\) dropout rate.\n' +
      '\n' +
      '## Appendix I Ablation studies\n' +
      '\n' +
      '### Data\n' +
      '\n' +
      'We study how to combine datasets with different caption qualities, quantities, and distributions when training a video-text contrastive model. In Table 20, three different combination methods are considered: (1) simply mixing different datasets (denoted with "+" and "\\(\\mathbf{\\mathcal{K}}\\)" for AGD); (2) training with one dataset and then continue training with another dataset (denoted with "\\(\\rightarrow\\)" and "\\(\\mathbf{\\mathcal{K}}\\)" for AGD); (3) combining different methods with AGD (denoted with "+" and "\\(\\mathbf{\\mathcal{V}}\\)" for AGD). We choose two representative datasets, namely InternVid (Wang et al., 2023) and YTT180M (Zellers et al., 2021), and report Recall@1 (R@1) for zero-shot video-text (ZSV2T) and text-to-video (ZST2V) retrieval for this study. We notice that simply mixing InternVid and YTT180M results in a large performance drop on VATEX when compared with only using InternVid. On the other hand, training on one dataset then continue training on the other dataset is highly affected by the order of datasets. For instance, compared with only using InternVid, the performance of InternVid \\(\\rightarrow\\) YTT180M drops by a large margin on both MSRVTT and VATEX, while YTT180M \\(\\rightarrow\\) InternVid improves on three out of four metrics. Hence, this approach is not scalable with number of datasets. Alternatively, AGD consistently improves the performance on MSRVTT and ZSV2T of VATEX compared with YTT180M \\(\\rightarrow\\) InternVid and InternVid with only a slightly drop in ZST2V of VATEX. As a result, AGD is chosen to be used in combining different training datasets.\n' +
      '\n' +
      'We further report the performance of AGD with all our pre-training corpus in the last row of Table 20. We observe a large improvement across all metrics with AGD, demonstrating that AGD scales well with the number of datasets.\n' +
      '\n' +
      '### Model design\n' +
      '\n' +
      '**Masking method.** We first study the impact of masking method and masking ratio to the second-stage model using VideoPrism-B as an example. In Figure 6, we compare the performance of the second-stage model under tube masking (Tong et al., 2022) and BEVT masking (Wang et al., 2022) with various masking ratios on different video focused tasks. We notice that BEVT masking outperforms tube masking in most cases. When comes to the masking ratio, BEVT masking with masking ratio \\(0.65\\) and \\(0.75\\) have similar performance and outperform the other masking ratios. As a result, if not otherwise specified, all the second-stage models are trained with BEVT masking with \\(0.65\\) masking ratio.\n' +
      '\n' +
      '**Token shuffling and global distillation.** We then study the performance of token shuffling and global distillation which are the two new techniques introduced in our masking distillation method. We show the results of the second-stage model (VideoPrism-B) without token shuffling or global distillation and compare the results with the full second-stage model on video classification (K400 and SSV2) and spatiotemporal action location (AVA) tasks in Table 21. From this table, we notice that both token shuffling and global distillation help improving the performance of the second-stage model by a large margin. Especially, token shuffling improves the performance of the second-stage model on\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline \\multirow{2}{*}{Data} & \\multirow{2}{*}{AGD} & \\multicolumn{2}{c}{**MSRVTT**} & \\multicolumn{2}{c}{**VATEX**} \\\\  & & ZST2V & ZSV2T & ZST2V & ZSV2T \\\\ \\hline InternVid & ✗ & 28.9 & 55.8 & 40.8 & 57.3 \\\\ YTT180M & ✗ & 19.6 & 47.5 & 26.3 & 49.7 \\\\ \\hline InterVid + YTT180M & ✗ & 29.5 & 56.7 & 29.9 & 41.0 \\\\ InternVid \\(\\rightarrow\\) YTT180M & ✗ & 21.7 & 54.7 & 29.8 & 54.8 \\\\ YTT180M \\(\\rightarrow\\) InternVid & ✗ & 29.3 & 56.4 & 39.9 & 57.5 \\\\ InterVid + YTT180M & ✓ & 29.8 & 57.4 & 39.4 & 58.2 \\\\ \\hline Full pretraining corpus & ✓ & **34.3** & **64.4** & **52.0** & **69.7** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 20: **Data ablation for the first-stage model. We report both zero-shot text-to-video (ZST2V) and video-to-text (ZSV2T) retrieval results of VideoPrism-B for each configuration under the evaluation metric of Recall@1.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline Models & **K400** & **SSv2** & **AVA** \\\\ \\hline Full configuration & **84.2** & 63.6 & **30.6** \\\\ w/o token shuffling & 83.6 (\\(\\downarrow 0.6\\)) & 61.8 (\\(\\downarrow 1.8\\)) & 29.4 (\\(\\downarrow 1.2\\)) \\\\ w/o global distillation & 83.4 (\\(\\downarrow 0.8\\)) & **64.2** (\\(\\uparrow 0.6\\)) & 29.0 (\\(\\downarrow 1.6\\)) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 21: **Ablation study for the second-stage model training strategy. Results using MAP probing with frozen backbone for VideoPrism-B are reported. We report Top-1 accuracy on K400 and SSV2, and mean average precision (mAP) on AVA.**\n' +
      '\n' +
      'Figure 6: **Ablation study for second-stage masking strategy and masking ratio. Results using MAP probing with frozen backbone for VideoPrism-B are reported.**\n' +
      '\n' +
      'motion-focused video classification dataset SSv2 by \\(1.8\\%\\). We believe that token shuffling introduces a harder learning objective to the second-stage model that is akin to Jigsaw puzzle (Noroozi and Favaro, 2016), forcing the second-stage model to better understand the motion in the video. Global distillation, on the other hand, can further boost the performance of the second-stage model on appearance-based video tasks (\\(0.8\\%\\) on K400 and \\(1.6\\%\\) on AVA) and plays an important role in training a good second-stage model.\n' +
      '\n' +
      '**The second-stage training.** Figure 7 compares the performance of the second-stage model with the first-stage model on different video datasets across a variety of different tasks using VideoPrism-B. Specifically, for video only tasks such as video classification (K400 [VC] and SSv2 [VC]) and spatiotemporal action localization (AVA [STAL]), we apply the frozen feature and only update the weights in the task head. We report top-1 accuracy for video classification and mAP@IoU=\\(0.5\\) for spatiotemporal action localization, respectively. For zero-shot video-to-text retrieval (MSRVTT [ZSV2T], VATEX [ZSV2T], and ActivityNet [ZSV2T]), zero-shot text-to-video retrieval (MSRVTT [ZST2V], VATEX [ZST2V], and ActivityNet [ZST2V]), and zero-shot video classification (K400 [ZSC], K600 [ZSC], SSv2 [ZSC], and NExt-QA-ATP [ZSC]) tasks, we apply LiT (Zhai et al., 2022) to learn a text encoder complied to the second-stage model as discussed in Section 3.2 and report Recall@1 (R@1). We notice that the second-stage training significantly improves the performance of the video encoder compared with the first-stage model across all video tasks on different datasets, strongly demonstrating the effectiveness of the proposed second-stage training.\n' +
      '\n' +
      '### Scaling properties\n' +
      '\n' +
      'In Figure 7(a), we study the scaling behavior of our models by keeping the data fixed. We find that both our first-stage model and second-stage model scale well with the model size. Interestingly, the second-stage model shows consistent improvements over the first-stage model of around \\(8\\%\\) on SSv2 and \\(2.2\\%\\) on AVA, across the model sizes. In Figure 7(b), we scale the second-stage model by fixing the first-stage model to be of Base size. For _Large_ and _giant_ second-stage models, as they are incompatible with the first-stage model of Base size, we initialize them with the\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Data & \\# of clips & **K400** & **SSv2** & **AVA** \\\\ \\hline Full pretraining corpus & 618M & 85.8 & 66.4 & 33.1 \\\\ + additional video-only & 898M & 86.1 & 66.7 & 33.7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 22: **Studies on data scaling for the second-stage model.** We report results of the _Large_ model using MAP probing with frozen backbone and only the full pretraining corpus is used to train the first-stage model.\n' +
      '\n' +
      'Figure 8: **Preliminary studies on model scaling.** Results using MAP probing with frozen backbone are reported. We report Top-1 accuracy on K400 and SSv2, and mean average precision (mAP) on AVA.\n' +
      '\n' +
      'Figure 7: **Comparison between the first-stage and second-stage model of VideoPrism-B on video understanding tasks.** For video-only tasks, results are from using MAP probing with frozen backbone.\n' +
      '\n' +
      'corresponding image model of CoCa (Yu et al., 2022). We observe that even with a fixed first-stage model, our second-stage models still show a reasonable scaling capability.\n' +
      '\n' +
      'In Table 20, we demonstrate strong data scaling capability of the first-stage model where the model trained on the pretraining corpus outperforms the one trained on InternVid (Wang et al., 2023) by 5.4% on MSRVTT ZST2V retrieval and 11.2% on VATEX ZST2V retrieval. This motivates us to also study the data scaling ability of our second-stage model. An interesting aspect of our second-stage training is that it works with video-only data without annotations. This equips us to economically increase the corpus size during the second-stage training. To test the benefit of data scaling, we mine additional 280M video clips without annotations from YouTube and add them to our second-stage training. We use model with _Large_ size as an example and train the first-stage model using pretraining corpus. We then compare the second-stage model trained only on the pretraining corpus with that trained with both the pretraining corpus and the additional clips in Table 22. We can see that our second-stage model scales well with data. Note that prior work on masked modeling either does not demonstrate good data-scaling properties or shows marginal improvements with data scaling (Feichtenhofer et al., 2022; Tong et al., 2022).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>