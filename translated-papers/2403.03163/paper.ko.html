<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Design2Code : Front-End Engineering을 자동화하는데 얼마나 먼가?\n' +
      '\n' +
      ' 청글레이 Si\\({}^{*}\\)\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      'clsi@stanford.edu\n' +
      '\n' +
      '&Yanzhe Zhang\\({}^{*}\\)\n' +
      '\n' +
      'Georgia Tech\n' +
      '\n' +
      'z_yanzhe@gatech.edu\n' +
      '\n' +
      'Zhengyuan Yang\n' +
      '\n' +
      'Microsoft\n' +
      '\n' +
      '&Ruibo Liu\n' +
      '\n' +
      'Google DeepMind\n' +
      '\n' +
      '&Diyi Yang\n' +
      '\n' +
      'Stanford University\n' +
      '\n' +
      '등등 기여도\\({}^{*}\\)\n' +
      '\n' +
      '프로젝트 페이지: [https://salt-nlp.github.io/Design2Code/](https://salt-nlp.github.io/Design2Code/)\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '생성 AI는 최근 몇 년 동안 급속한 발전을 이루어 멀티모달 이해 및 코드 생성에서 전례 없는 능력을 달성했다. 이는 멀티모달 LLM이 시각적 디자인을 코드 구현으로 직접 변환할 수 있는 프론트 엔드 개발의 새로운 패러다임을 가능하게 할 수 있다. 이 작업에서는 이를 Design2Code 과제로 공식화하고 종합적인 벤치마킹을 수행한다. 구체적으로, 484개의 다양한 실제 웹 페이지의 벤치마크를 테스트 사례로 수동으로 큐레이션하고 스크린샷을 입력으로 하여 현재 멀티모달 LLM이 주어진 참조 웹 페이지에 직접 렌더링하는 코드 구현을 얼마나 잘 생성할 수 있는지 평가하기 위한 자동 평가 메트릭 세트를 개발한다. 또한 포괄적인 인간 평가로 자동 측정 기준을 보완합니다. 본 논문에서는 GPT-4V와 Gemini Pro Vision에서 멀티모달 프롬프트 방법을 개발하고 그 효과를 보인다. 우리는 또한 쌍둥이자리 프로 비전**의 성능과 성공적으로 일치하는 오픈 소스 Design2Code-18B 모델을 세분화한다. 인간 평가 및 자동 측정 기준 모두 GPT-4V가 다른 모델에 비해 이 작업에서 가장 우수한 성능을 발휘함을 보여준다. 더욱이, 주석자는 GPT-4V 생성 웹페이지가 시각적 외관 및 내용 면에서 사례의 **49%**에서 원본 참조 웹페이지를 대체할 수 있다고 생각하며, 아마도 놀랍게도 사례의 **64%**에서 GPT-4V 생성 웹페이지가 원본 참조 웹페이지보다 더 나은 것으로 간주된다. 세밀한 분류 메트릭은 오픈 소스 모델이 입력 웹 페이지에서 시각적 요소를 회상하고 올바른 레이아웃 디자인을 생성하는 데 대부분 지연되는 반면 텍스트 콘텐츠 및 컬러링과 같은 측면은 적절한 미세 조정으로 크게 개선할 수 있음을 나타낸다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '웹사이트의 시각적 디자인을 기능 코드로 구현하는 것은 시각적 요소와 레이아웃을 이해하고 구조화된 코드로 변환해야 하기 때문에 어려운 작업이다. 정교한 기술에 대한 이러한 의존성은 많은 일반인들이 무엇을 구축하거나 디자인할지에 대한 구체적인 아이디어를 가지고 있는 경우에도 자신의 웹 애플리케이션을 구축하는 것을 방해했다. 또한, 도메인 전문성에 대한 요구는 전체 웹페이지 생산 파이프라인을 복잡하게 하여, 상이한 기술 세트를 갖는 사람들 간의 협업을 필요로 하고 의도된 설계와 실제 구현 사이에 불일치를 잠재적으로 야기시킨다. 비주얼 디자인에서 효과적인 기능 코드 자동 생성은 프런트 엔드 웹 애플리케이션(Nguyen and Csallner, 2015) 개발을 민주화하여 비전문가가 쉽고 빠르게 애플리케이션을 구축할 수 있는 잠재력을 가지고 있다.\n' +
      '\n' +
      '자연어로부터의 코드 생성은 최근 몇 년 동안 급속히 발전한 반면(Yin and Neubig, 2017; Le et al., 2020; Li et al., 2023b), 사용자 인터페이스(UI) 설계로부터 코드 구현을 생성하는 것은 사용자 인터페이스 상의 시각적 및 텍스트 신호들의 다양성 및 결과적인 코드 내의 광대한 검색 공간과 같은 광범위한 도전들로 인해 크게 주목받지 못했다. 벨트라멜리(2018)는 2017년에 CNN 및 RNN 모델을 사용하여 좁은 세트의 단순한 사용자 인터페이스 디자인에서 주목할 만한 시도를 했다. 수년에 걸쳐, 이러한 퀘스트를 따라 많은 후속 시도들에도 불구하고(로빈슨, 2019; 소셀리아 등, 2023), 이들은 모두 좁은 레이아웃 설계 세트를 갖는 단순하거나 합성 예들로 제한되며, 실제-세계 프론트-엔드 개발 애플리케이션들에 거의 유용하지 않다. 최근까지 멀티모달 LLM의 개발은 대규모 사전 훈련 모델이 시각적 및 텍스트 입력을 모두 처리하고 다양한 시각적 근거 작업에 대한 텍스트 출력을 생성할 수 있는 새로운 시대로 진입했으며 대표적인 예가 플라밍고(Alayrac et al., 2022), GPT-4V(OpenAI, 2023), 제미니(Google, 2023)이다. 이러한 발전은 이 오랜 미해결 작업에 대한 새로운 패러다임의 잠금을 해제했다 1: 사용자의 웹사이트 디자인의 스크린샷을 취하고 이 이미지를 시스템에 제공하여 완전한 종단간 방식으로 원하는 웹페이지로 렌더링할 수 있는 풀 코드 구현을 얻는다. 우리는 이 작업을 **Design2Code**라고 명명하고 프런트 엔드 엔지니어링 자동화에서 얼마나 멀리 떨어져 있는지 벤치마킹하고 이해하기 위해 도구 상자에 있는 현재 멀티모달 모델로 해결한다.\n' +
      '\n' +
      '각주 1: GPT-4 릴리스 동안 Greg Brockman의 프레젠테이션: [https://www.youtube.com/live/outc6tbnMuQ?si=5Yge32m5nmB85r4Ekt=980](https://www.youtube.com/live/outc6tbnMuQ?si=5Yge32m5nmB85r4Ekt=980)\n' +
      '\n' +
      '체계적이고 엄격한 벤치마킹을 위해 디자인2코드(그림 1의 예제)에 대한 최초의 실제 벤치마크를 구축한다. 사실적인 사용 사례를 가장 잘 반영하기 위해, 우리는 이전 작업에서와 같이 합성적으로 생성된 것이 아니라 야생에서 실제 웹 페이지를 테스트 예로 사용한다(Soselia et al., 2023; Huggingface, 2024). C4(Raffel et al., 2019) 검증 세트의 웹 페이지를 긁어내고 모든 예에 대해 신중한 수동 큐레이션을 수행하여 다양한 수준의 복잡성을 가진 다양한 실제 사용 사례를 나타내는 484개의 고품질, 도전적 및 다양한 웹 페이지 세트를 얻는다. 우리는 우리의 벤치마크가 광범위한 HTML을 포괄한다는 것을 양적으로 그리고 정성적으로 보여준다\n' +
      '\n' +
      '도 1: 이전 웹사이트 데이터세트(첫 번째 행) 및 우리의 새로운 디자인2코드 벤치마크(마지막 두 행)로부터의 예제. 우리는 실제 웹 페이지를 벤치마킹에 사용하여 실제 웹 페이지가 사실적이고 다양한지 확인하는 반면 웹사이트는 확장성을 위해 합성적으로 생성된 웹 페이지를 사용한다.\n' +
      '\n' +
      ' 태그 사용, 도메인 및 복잡성 수준입니다. 효율적인 평가 및 모델 개발을 용이하게 하기 위해, 생성된 웹페이지의 스크린샷을 주어진 스크린샷 입력과 비교하는 이 작업에 대한 자동 메트릭을 개발한다. 우리의 메트릭은 웹 페이지의 모든 일치된 시각적 요소의 경계 상자 일치, 텍스트 내용, 위치 및 색상을 포함한 포괄적인 차원 세트를 고려하며, 이는 나중에 인간의 판단과 높은 상관 관계를 보여준다.\n' +
      '\n' +
      '그런 다음 GPT-4V 및 제미니와 같은 현재 멀티모달 LLM이 이 작업에서 수행하는 방법을 조사한다. 본 논문에서는 OCR의 부하를 줄이기 위해 웹 페이지에서 추출된 텍스트 요소로 시각적 입력을 보완하는 텍스트 증강 프롬프트와 모델에 이전 세대와 자기 개선을 위한 입력 웹 페이지를 비교하도록 요청하는 자기 수정 프롬프트 방법을 포함한 다양한 프롬프트 방법을 소개한다. 우리는 GPT-4V와 제미니 프로 모두에 대한 직접 프롬프트와 비교하여 텍스트 증강 프롬프트 방법에서 일관된 개선을 보는 반면 GPT-4V에 대한 자체 수정의 긍정적인 효과만 관찰한다.\n' +
      '\n' +
      '최첨단 성능을 입증했음에도 불구하고 이러한 상업용 모델은 투명성이 제한된 블랙박스입니다. 이를 위해, 우리는 이 작업인 Design2Code-18B를 위한 오픈소스 18B 파인튜닝 모델에 기여한다. 구체적으로, 우리는 최신 오픈소스 모델인 CogAgent(Hong et al., 2023)를 기반으로 합성적으로 생성된 Design2Code 데이터(Huggingface, 2024)로 미세화한다. 놀랍게도, 이 "작은" 오픈 소스 모델은 합성 훈련 데이터와 사실적인 테스트 데이터 사이의 불일치에도 불구하고 제미니 프로 비전의 성능과 일치하여 벤치마크에서 경쟁적으로 수행되며(자동 평가 결과의 개요는 그림 3) 특수 "작은" 오픈 모델과 합성 데이터로부터의 기술 획득 가능성을 나타낸다.\n' +
      '\n' +
      '요약하자면, 이 작업에 대한 우리의 기여는 다음과 같습니다.\n' +
      '\n' +
      '1. Design2Code 태스크를 정형화하고 484개의 다양한 실세계 테스트 예제와 함께 수동으로 큐레이션된 Design2Code 벤치마크를 구성한다.\n' +
      '2. 인간 평가를 보완하는 고-레벨 시각적 유사성 및 저-레벨 요소 매칭을 모두 캡처하는 포괄적인 자동 평가 메트릭 세트를 개발한다.\n' +
      '3. 직접 프롬프트 기준선보다 개선된 새로운 멀티모달 프롬프트 방법을 제안한다.\n' +
      '4. Open-source Design2Code-18B 모델을 통해 Gemini Pro Vision의 성능과 일치함을 확인하였다.\n' +
      '\n' +
      '##2. Design2Code Benchmark\n' +
      '\n' +
      '이 섹션에서는 벤치마크 데이터의 큐레이션 및 처리에 대해 설명한다. 우리는 먼저 C4(Raffel et al., 2019) 검증 세트의 모든 웹사이트 링크를 긁는다. 그런 다음 모든 CSS 코드를 HTML 파일에 삽입하여 각 웹페이지에 대해 하나의 단일 코드 구현 파일을 얻는다. 이는 총 127.9k 웹 페이지를 생성하며, 이는 아래에 설명된 대로 추가 필터링 및 처리를 수행한다.\n' +
      '\n' +
      '#테스트 세트 큐레이션\n' +
      '\n' +
      '우리의 전반적인 목표는 다양한 실제 사용 사례를 나타내는 잘 형성된 웹 페이지 세트를 얻는 것이다. 자동 처리 및 수동 필터링을 위해 다음 단계를 따릅니다.\n' +
      '\n' +
      '자동 길이 및 레이아웃 필터링 먼저 자동 필터링 라운드를 적용합니다. 우리는 현재 멀티모달 LLM이 입력으로 처리할 수 없거나 그러한 긴 출력을 디코딩할 수 없는 과도하게 긴 웹 페이지를 피하기 위한 방법으로 코드 파일에서 모든 코멘트를 제거한 다음 길이 필터를 적용하여 소스 코드 파일이 100k 이상의 토큰(GPT-2 토큰화기를 기반으로)을 갖는 예를 배제한다. 다음으로, 레이아웃이 이미지 또는 텍스트로만 구성된 모든 웹 페이지를 필터링하며, 이러한 경우 레이아웃 디자인이 너무 단순하여 벤치마킹에 흥미롭지 않은 경향이 있다. 이렇게 하면 필터링 및 중복 제거 후 14k 웹 페이지가 생성됩니다.\n' +
      '\n' +
      '웹 페이지를 독립형으로 만드는 것은, 멀티미디어 파일(이미지, 오디오, 비디오 등)과 같은 모든 외부 종속성을 제공하지 않고, 모델에 대한 웹페이지의 스크린샷만을 제공할 설정을 가정한다. 이를 가능하게 하기 위해, 모든 외부 파일 종속성을 스트립하여 모든 웹 페이지를 독립형으로 만드는 것이다. 이것은, 모든 <스크립트><오디오><iframe><map><svg> 태그를 제거하는 것, 외부 사이트로 링크하는 모든 <링크> 태그를 제거하는 것, <a> 태그의 모든 href 링크를 제거하는 것, <object> 요소의 모든 외부 파일을 제거하는 것을 포함한다. 모든 이미지 및 비디오 파일에 대해 플레이스홀더 파일로 교체하고 벤치마킹하는 동안 모델에 원래 레이아웃을 보존하기 위해 해당 위치에 이 플레이스홀더 파일을 삽입하도록 지시합니다.\n' +
      '\n' +
      '수작업 큐레이션을 수행한 후, 다음 기준에 따라 예를 필터링하기 위해 최종 수동 큐레이션 라운드를 수행한다. (1) 웹 페이지는 외부 파일 종속성이 없으며 처리된 코드 파일 및 제공된 플레이스홀더 이미지 파일로부터 독립형 방식으로 렌더링할 수 있다. (2) 웹 페이지는 어떠한 사적, 민감성, 또는 잠재적으로 유해한 정보를 포함하지 않는다(예를 들어, 우리는 데이트 웹사이트로부터 프로파일 페이지를 제거하였다). (3) 렌더링된 웹 페이지는 잘 포맷된다(예를 들어, 상이한 레이아웃 엘리먼트들 사이에 중첩이 없어야 하고 상기 자동 처리는 웹 페이지 설계의 임의의 부분을 방해하지 않아야 한다). 이 논문의 처음 두 저자는 샘플링된 7k개의 예에서 모든 예를 확인하여 이 큐레이션 단계를 수행했다. 그들은 먼저 200개의 예제에 함께 주석을 달아서 \\(75\\%\\) 일치에 도달한 다음 위의 14k 예제의 필터링된 세트에서 무작위로 샘플링된 7k 예제에 대한 주석 작업을 분할했다. 이 전체 수동 큐레이션 프로세스는 약 1주일이 소요되었습니다. 우리는 다양한 HTML 및 CSS 요소가 포함된 고품질 웹 페이지만 유지하기 위해 수동 필터링 프로세스에서 더 공격적인 경향이 있다. 결국, 우리는 벤치마크로 사용하는 484개의 테스트 예제를 얻었다.\n' +
      '\n' +
      '### 데이터 통계 및 다양성\n' +
      '\n' +
      '정량적 척도(Quantitative Metrics)는 테스트 예제의 난이도를 추정하기 위해 몇 가지 정량적 척도를 제공한다. 우리는 표 1의 WebSight [Huggingface, 2024]의 가장 최근 및 가장 유사한 기존 데이터셋과 비교한다. **(1) Length**: GPT-2 토큰화기로 스크래핑된 코드 파일을 토큰화한다. 파일당 평균 토큰 수는 31215.6개(min=784, max=98637, std=23902.9)이다. 이것은 현대 언어 모델들의 전형적인 최대 출력 길이인 WebSight보다 훨씬 더 길어서, 독특한 도전을 제기한다(비록 우리가 여기 코드 파일들이 단지 참조 구현을 위한 것이라는 것에 주목하지만, 주어진 웹 페이지들을 재현하는 훨씬 더 간결한 방법들이 있을 수 있다). **(2) 총 태그 수**: 관련된 HTML 태그의 총 수를 카운트하는데, 이는 평균 158.3개이다(min=12, max=528, std=100.4). 우리의 벤치마크에 있는 예는 84가지 유형의 표준 HTML5 태그를 포함한다. 표 5(부록 A)에서 가장 많이 사용되는 태그에 대한 차트를 제시한다. **(3) DOM 트리 깊이**: 복잡도의 또 다른 척도로서 문서 객체 모델(DOM) 트리의 깊이를 측정한다. 평균 깊이는 12.5(min=4, max=32, std=4.7)이다. **(4) 고유 태그 수**: 마지막으로 각 예에서 고유 HTML 태그의 수를 계산하며 평균은 22.2(min=8, max=45, std=6.0)로 벤치마크가 광범위한 HTML 태그를 포함하고 있음을 시사한다. 전반적으로 벤치마크의 예는 웹사이트와 같은 이전 노력보다 훨씬 더 어렵고 광범위한 복잡성을 포함한다.\n' +
      '\n' +
      '도메인 배포는 벤치마크에서 다루는 도메인의 범위를 이해하기 위해 벤치마크에서 무작위로 25% 예제(N=120)를 샘플링하고 기능에 따라 어떤 유형의 웹 페이지에 수동으로 주석을 달았다. 우리는 그림 2에서 가장 빈번한 도메인의 파이 차트를 제시한다. 가장 두드러진 장르는 회사 또는 조직의 웹사이트, 개인 블로그(기술 블로그 포함), 개인 홈페이지이다. 다른 장르에는 정보 공유 사이트(예: 위키피디아 페이지, FAQ 페이지, 세금 정책 페이지, 온라인 사전), 온라인 포럼, 뉴스 기사 페이지 및 제품 설명 페이지가 포함된다. 샘플링된 예는 그림 1에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c} \\hline \\hline  & **WebSight (Huggingface)** & **Design2Code (Ours)** \\\\ \\hline Purpose & Training & Testing \\\\ Source & Synthetic (Deepsek-Coder) & Real-World (C4) \\\\ Size & 823K & 484 \\\\ Avg Length (tokens) & 647\\(\\pm 216\\) & 31216\\(\\pm 23902\\) \\\\ Avg Tag Count & 19\\(\\pm 8\\) & 158\\(\\pm 100\\) \\\\ Avg DOM Depth & 5\\(\\pm 1\\) & 13\\(\\pm 5\\) \\\\ Avg Unique Tags & 10\\(\\pm 3\\) & 22\\(\\pm 6\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: WebSight 데이터셋과 우리의 새로운 Design2Code 벤치마크 간의 데이터셋 통계량 비교. WebSight는 훈련 세트만 제공하고 Design2Code는 테스트 세트만 제공한다. 디자인2코드 벤치마크의 예는 모든 측정에서 훨씬 더 복잡하며 더 큰 표준 편차로 표시되는 바와 같이 난이도가 더 다양하다.\n' +
      '\n' +
      '### Automatic Metrics\n' +
      '\n' +
      '이전에, 생성된 HTML 코드는 보통 정규화된 편집 거리(Lv 등, 2023) 및 htmlBLEU(소셀리아 등, 2023)와 같은 텍스트 기반 유사성 메트릭에 의해 평가된다. 그러나, 그러한 메트릭들은 동일한 웹페이지를 구현하는 많은 상이한 방식들이 있을 수 있기 때문에 원래의 스크린샷의 시각적 디자인이 정확하게 생성되는지를 직접적으로 평가할 수 없으며, 생성된 코드의 사소한 차이들은 렌더링된 출력에 주요한 시각적 차이들을 초래할 수 있다. 이를 위해 참조 웹페이지의 스크린샷(I_{R}\\)과 생성된 웹페이지의 렌더링된 스크린샷(I_{G}\\) 사이의 유사도를 계산하여 생성된 웹페이지를 자동으로 평가할 것을 제안한다. 우리는 평가를 높은 수준의 시각적 유사성과 낮은 수준의 요소 매칭으로 구분한다.\n' +
      '\n' +
      '높은 수준의 시각적 유사성은 \\(I_{R}\\)과 \\(I_{G}\\)의 시각적 유사성을 평가하기 위해 그들의 CLIP(Radford et al., 2021) 임베딩의 유사성을 사용하여 \\(\\mathbf{CLIP}(I_{R},I_{G})\\으로 표시한다. 구체적으로, 스크린샷을 정사각형으로 리사이징한 후 CLIP-ViT-B/32를 이용하여 특징을 추출한다. 스크린샷에서 텍스트를 배제하기 위해 Telea(2004)의 인페인팅 알고리즘을 사용하여 경계 상자 좌표를 사용하여 탐지된 모든 텍스트 상자를 마스킹한다. 2\n' +
      '\n' +
      '각주 2: [https://docs.opencv.org/4.3.0/df/d3d/tutorial_py_inpainting.html](https://docs.opencv.org/4.3.0/df/d3d/tutorial_py_inpainting.html)\n' +
      '\n' +
      'CLIP 유사도와 같은 낮은 수준의 요소 매칭 메트릭은 텍스트와 같은 모든 세부 정보의 매칭보다는 전체 이미지의 유사성만을 캡처한다. 또한 메트릭 자체는 모델 약점을 진단하는 데 도움이 되는 세밀한 분해를 제공하지 않는다. 이를 보완하기 위해 요소 매칭 메트릭 세트를 소개한다. 구체적으로, 생성된 웹 페이지들이 모든 시각적 요소들을 회상할 수 있는지, 그리고 참조 및 생성된 웹 페이지들 내의 대응하는 시각적 요소들이 정렬된 텍스트 내용, 위치 및 색상을 갖는지 여부를 고려한다.\n' +
      '\n' +
      '참조 웹페이지 스크린샷\\(I_{R}\\)과 생성된 웹페이지 스크린샷\\(I_{G}\\)이 주어지면, 텍스트 검출 모듈을 사용하여 각각 \\(R=\\{r_{1},r_{2},...,r_{m}\\}) 및 \\(G=\\{g_{1},g_{2},...,g_{n}\\}\\})의 시각적 요소 블록 세트를 출력하는데, 여기서 각 블록은 텍스트 콘텐츠와 바운딩 박스 좌표를 포함한다. 블록 검출 모듈을 구현하는 자세한 내용은 부록 B를 참조하십시오. 검출된 두 블록을 기반으로 Jonker-Volgenant 알고리즘(Crouse, 2016)을 사용하여 텍스트 유사도를 기반으로 \\(R\\)과 \\(G\\) 사이의 최적의 매칭 \\(M\\)을 얻었으며, 여기서 \\((p,q)\\in M\\)은 \\(r_{p}\\)이 \\(g_{q}\\)과 매칭됨을 나타낸다. \\(R\\), \\(G\\) 및 \\(M\\)에서 일치하는 쌍이 주어지면 다음과 같은 측면에 따라 유사성을 평가한다.\n' +
      '\n' +
      '***Block-Match**: 태스크의 첫 번째 desideratum은 기준 웹페이지로부터의 모든 시각적 요소들이 생성된 웹페이지에서 재생되어야 하고, 생성된 웹페이지가 존재하지 않는 새로운 요소들을 환각해서는 안 된다는 것이다. 매칭되지 않은 블록을 포함한 모든 블록의 전체 크기를 계산하여 이를 측정한다. (생성한 웹페이지가 놓쳤기 때문에 또는 생성된 웹페이지가 환각된 블록을 포함하기 때문에)\\[\\mathbf{match_{block}}(r_{p},g_{q})=\\frac{S(r_{p})+S(g_{q})}{\\sum_{i,j)\\in M}(S(r_{i})+S(g_{j}))+(\\sum_{i\\in U_{R}S(r_{i})+\\sum_{j\\in U_{G}S(g_{q}),\\(S(\\cdot)\\)는 M}\\mathbf{match_{block}(r_{p},g_{q}),\\(S(\\cdot)\\)에서 매칭되지 않은 블록의 크기를 반환한다. 여기서 직관은 타의 추종을 불허하는 블록들이 원래의 블록들을 놓치거나 환각된 블록들을 발생시킴에 따라 점수를 낮출 것이고, 타의 추종을 불허하는 블록들이 클수록 이 점수는 낮아질 것이다.\n' +
      '**Text**: 두 개의 일치된 블록에서 두 문자열이 주어지면, 텍스트 유사도 \\(r_{p}\\)와 \\(g_{q}\\)는 두 문자열의 전체 글자 수로 나눈 겹친 글자 수의 두 배(문자 수준 Sorensen-Dice 유사성)로 계산된다. 전체 점수는 모든 일치된 쌍에 걸쳐 평균화된다.\n' +
      '***위치**: 블록의 위치는 전체 레이아웃에 크게 영향을 미친다. 매칭된 각 쌍 \\((p,q)\\)에 대해, 우리는 위치 유사성 \\(\\mathbf{sim_{pos}}(r_{p},g_{q})=1-max(abs(x_{q}-x_{p}),abs(y_{q}-y_{p}))를 계산하며, 여기서 \\((x_{p},y_{p})\\) 및 \\((x_{q},y_{q})\\)는 \\(r_{p}\\) 및 \\(g_{q}\\)의 중심을 정규화된 좌표([0,1]\\))이다. 전체 점수는 모든 일치된 쌍에 걸쳐 평균화된다.\n' +
      '**Color**: CIEDE2000 색차 공식 [Luo et al., 2001]을 사용하여 블록 \\(g_{q}\\)에서 생성된 텍스트의 색상과 블록 \\(r_{p}\\)에서 참조 텍스트의 색상 간의 지각적 차이를 평가하며, \\(\\mathbf{sim_{color}}(r_{p},g_{q})\\)으로 표시되며, 여기서 공식은 인간의 색각의 복잡성을 고려한다. 전체 점수는 모든 일치된 쌍에 걸쳐 평균화된다.\n' +
      '\n' +
      '이러한 다른 차원에 대한 집계 점수는 세립 진단 점수로 설계되기 때문에 의도적으로 계산하지 않는다. 이상적으로, 모델 및 방법은 이러한 모든 차원을 따라 점수를 잘 매겨야 한다.\n' +
      '\n' +
      '##3 벤치마킹 : 프롬프트 및 파인튜닝\n' +
      '\n' +
      '우리는 상용 API 모델과 오픈 소스 모델을 모두 포함하여 벤치마크에서 성능을 비교하기 위해 다양한 모델과 방법을 벤치마킹한다.\n' +
      '\n' +
      '### 멀티모달 프롬프팅 방법\n' +
      '\n' +
      '현대 딥 러닝 관행에 따라, 우리는 첫 번째 기준선 세트로서 상업용 LLM을 촉구하는 데 의존한다. 우리는 벤치마크를 위한 일련의 멀티모달 프롬프트 방법을 개발합니다. 우리는 이미지 입력과 텍스트 프롬프트를 모두 취한 다음 코드를 출력으로 생성할 수 있는 모델에 대한 액세스를 가정한다. 우리는 GPT-4V[OpenAI, 2023]와 Gemini Pro Vision[Google, 2023]을 두 개의 가장 성능이 좋은 공개 API로 실험할 것이다.\n' +
      '\n' +
      '다이렉트 프롬프팅 가장 간단한 다이렉트 프롬프팅 베이스라인으로부터 시작하여, 다음의 지시와 함께 참조 웹페이지 스크린샷을 제공한다:\n' +
      '\n' +
      '당신은 HTML과 CSS를 전문으로 하는 전문 웹 개발자입니다. 사용자는 웹 페이지의 스크린샷을 제공합니다. HTML과 CSS를 사용하여 주어진 웹 사이트를 복제하는 단일 html 파일을 반환해야 합니다. HTML 파일 자체에 모든 CSS 코드를 포함합니다. 이미지가 포함된 경우 "rick.jpg"를 자리 표시자로 사용합니다. 웹 페이지의 일부 이미지는 자리 표시자로 파란색 직사각형으로 대체되며, "rick.jpg"를 사용하십시오. 외부 파일에 대한 종속성을 환각으로 보지 마십시오. 동적 상호 작용을 위해 JavaScript 스크립트를 포함할 필요가 없습니다. 전체 레이아웃뿐만 아니라 모든 요소의 크기, 텍스트, 위치 및 색상과 같은 사항에 주의하십시오. HTML+CSS 파일의 내용으로 응답합니다.\n' +
      '\n' +
      '텍스트-증강 프롬프트 위의 프롬프트는 모델에게 모든 것을 한번에 수행하도록 요청한다: 모든 텍스트 및 레이아웃 요소를 인식하고 대응하는 코드를 생성한다. 실제로, 사용자들은 종종 자신의 웹페이지에 어떤 콘텐츠를 올리고 싶은지에 대한 아이디어를 가지고 있다. 대신, 그들은 디자인을 코드 구현으로 변환하는 전문 지식만을 찾고 있다. 이러한 설정을 반영하기 위해, 우리는 또한 원본 웹페이지의 모든 텍스트 요소를 처음 3에서 추출하고 스크린샷 입력과 함께 지시 프롬프트 후에 이러한 텍스트를 추가하는 텍스트 증강 프롬프트 방법을 탐구한다. 이 설정에서는 OCR 수행의 어려움을 완화하고 대신 모델이 레이아웃 설계에 더 집중할 수 있도록 하며, 여기서 모델은 프롬프트에서 텍스트 내용을 복사하여 올바른 위치에 삽입할 수 있다.\n' +
      '\n' +
      '각주 3: 우리는 아름다운 수프 도서관을 사용합니다.\n' +
      '\n' +
      '자체 수정 프롬프트(Self-Revision PromptingInspired on LLMs using their self- improved generation; Madaan et al., 2023; Shinn et al., 2023) 또한, 입력으로서 다음을 제공하는 자기 수정 프롬프트를 개발한다: (1) 입력 웹페이지의 스크린샷, (2) 텍스트-증강 프롬프트로부터 생성된 웹페이지의 스크린샷, (3) 텍스트-증강 프롬프트로부터 생성된 코드를 초기 해결책으로서 제공하고, 그 결과가 참조 웹페이지에 더 가깝게 보일 수 있도록, 생성된 구현 코드를 모델에게 개선하도록 요청한다(full prompt is in Appendix C).\n' +
      '\n' +
      'GPT-4V와 Gemini Pro Vision 모두 동일한 프롬프트를 사용하며, 모든 세대에 대해 최대 출력 토큰 4096과 온도 0.0이 포함된 고해상도 모드를 사용한다.\n' +
      '\n' +
      '### Finetuning Design2Code-18B\n' +
      '\n' +
      '상업용 API 모델은 성능이 뛰어나고 사용하기 쉽지만 불투명하고 투명도가 제한적이다. 오픈 소스 대안을 활성화하기 위해 이 작업에 대한 오픈 소스 모델을 세분화하고 상용 API 모델과 비교한다.\n' +
      '\n' +
      'Base ModelWe는 고해상도 입력(\\(1120\\times 1120\\))을 지원하는 CogAgent-18B Hong et al. (2023)을 기본 모델로 사용하며, 집중적인 텍스트 이미지 쌍(Byeon et al., 2022; Schuhmann et al., 2022), 합성 문서(Kim et al., 2022), LaTeX 논문(Blecher et al., 2023) 및 소량의 웹사이트 데이터를 사전 훈련한다.\n' +
      '\n' +
      '트레이닝 데이터 우리는 웹사이트 스크린샷과 코드 구현 쌍으로 구성된 최근 출시된 허깅페이스 웹사이트 데이터 세트 4로 기본 모델을 미세 조정한다. 데이터세트는 두 단계로 생성된다: (i) Mistral-7B-v0.1로부터 랜덤 웹사이트 아이디어를 생성한다(Jiang et al., 2023). (ii) 간단하고 짧은 웹사이트를 생성하기 위해 생성된 아이디어를 갖는 Prompt Deepseek-Coder-33b-Instruct(Guo et al., 2024a)를 포함한다. 원본 WebSight 데이터셋은 823K 예제를 가지고 있지만, 제한된 연산 자원으로 인해 학습을 위해 무작위로 20%만 샘플링한다. 우리는 또한 예비 실험에서 더 낮은 손실로 이어진다는 것을 발견함에 따라 HTML 스타일과 신체의 순서를 뒤집는다. 우리는 또한 C4 훈련 세트에서 긁어낸 실제 웹페이지 데이터에 대한 훈련을 실험했다. 이러한 훈련은 실제 코드 구현 데이터가 매우 길고 시끄러운 경향이 있어 합성 데이터에 대한 훈련보다 성능이 훨씬 낮기 때문에 매우 불안정하고 어렵다. 따라서 우리는 그러한 탐구를 미래의 일에 맡긴다.\n' +
      '\n' +
      '각주 4: [https://huggingface.co/datasets/HuggingFaceM4/WebSight](https://huggingface.co/datasets/HuggingFaceM4/WebSight)\n' +
      '\n' +
      'SettingsWe는 LoRA(Hu et al., 2021)를 사용하여 기본 모델을 미세 조정하는데, 여기서 LoRA 모듈은 LoRA 랭크\\(8\\)로 언어 디코더에 추가된다. 배치크기의 \\(32\\)과 학습률 1e-5를 사용하여 \\(100\\)의 워밍업으로 \\(5000\\) 단계의 모델을 미세조정한다. 4\\(4\\times\\) NVIDIA A6000을 사용하면 약 2일의 훈련이 소요됩니다. 추론 시 온도\\(0.5\\)와 반복 패널티\\(1.1\\)을 사용하고, 작은 디브 세트(20예)에서 모든 자동 메트릭의 평균을 기반으로 최적의 체크포인트를 선택한다.\n' +
      '\n' +
      '자체 파인튜닝된 Design2Code-18B 외에 추가 베이스라인도 두 개의 다른 오픈 소스 베이스라인과 비교합니다. 두 기준선의 경우 기본 샘플링 설정을 따릅니다. 먼저, 기존의 CogAgent-18B 모델과 비교하여 미세조정의 이득을 분석할 수 있다. 우리는 스크린샷과 간단한 프롬프트를 사용하여 HTML 코드를 작성한다. 를 투입으로 하는 것을 특징으로 하는 반도체 소자의 제조 방법. 둘째, 기본 모델(SigLIP(Zhai et al., 2023) 및 Mistral-7B(Jiang et al., 2023))이 완전히 세분화된 전체 웹사이트 데이터셋에서 (적어도) 세분화된 것으로 추정되는 Huggingface WebSight VLM-8B와 비교한다. 이 버전은 문서 릴리스 또는 교육 세부 정보에 대한 자세한 설명서가 없는 베타 버전입니다. 다양한 기본 모델과 학습 데이터의 양을 감안할 때 디자인2Code-18B와의 사과 대 사과 비교가 아님에도 불구하고 순수하게 포괄성을 위해 이 기준선을 포함한다.\n' +
      '\n' +
      '##4 결과: 자동 및 인체 평가\n' +
      '\n' +
      '### Automatic Evaluation\n' +
      '\n' +
      '우리는 모든 자동 평가 결과를 표 2와 그림 3에 제시한다. 여기서의 비교는 모델 크기와 훈련 데이터의 차이를 감안할 때 결코 공정한 비교가 아니라는 점에 유의한다. 우리는 그들이 우리의 벤치마크에 가장 적절하고 접근 가능한 기준이기 때문에 그들을 비교한다. 우리는 (1) GPT-4V가 WebSight VLM-8B가 주도하는 색상과 다른 모든 차원에서 가장 우수하다는 것을 관찰한다. (2) 텍스트 증강 프롬프트는 GPT-4V와 Gemini Pro Vision 모두에서 블록 매칭 점수와 텍스트 유사도 점수를 성공적으로 증가시켜 추출된 텍스트 요소를 제공하는 유용성을 나타낸다. (3) Self-revision은 GPT-4V에 대한 블록 매칭 및 위치 유사도에 약간의 개선이 있지만, Gemini Pro Vision에는 개선되지 않는데, 이는 잠재적으로 외부 피드백 없이 내재적 self-correction을 수행하는 LLM의 제한된 능력 때문이다[23]. (4)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c|c} \\hline \\hline  & Block-Match & Text & Position & Color & CLIP \\\\ \\hline \\multicolumn{5}{c}{GPT-4V} \\\\ \\hline Direct Prompting & 85.8 & 97.4 & 80.5 & 73.3 & 86.9 \\\\ Text-Augmented Prompting & 87.6 & **98.2** & 80.2 & 73.0 & **87.2** \\\\ Self-Revision Prompting & **88.8** & 98.1 & **81.1** & 72.9 & **87.2** \\\\ \\hline \\multicolumn{5}{c}{Gemini Pro Vision} \\\\ \\hline Direct Prompting & 80.2 & 94.6 & 72.3 & 66.2 & 83.9 \\\\ Text-Augmented Prompting & 84.8 & 96.9 & 70.4 & 66.3 & 84.0 \\\\ Self-Revision Prompting & 84.1 & 96.6 & 70.1 & 66.2 & 83.7 \\\\ \\hline \\multicolumn{5}{c}{Open-Source Models} \\\\ \\hline WebSight VLM-8B & 55.9 & 86.6 & 77.3 & **79.4** & 86.5 \\\\ CogAgent-Chat-18B & 7.1 & 18.1 & 13.3 & 13.0 & 75.5 \\\\ Design2Code-18B & 78.5 & 96.4 & 74.3 & 67.0 & 85.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: CLIP와 높은 수준의 시각적 유사성뿐만 아니라 네 가지 세립 유사성 측정의 자동 평가 결과. 치수당 최상의 결과는 굵게 강조 표시됩니다. GPT-4V는 WebSight VLM-8B가 선도하고 있는 색상 외에 모든 차원에서 가장 우수하다. WebSight VLM-8B는 디자인2Code-18B보다 5배 더 많은 데이터에서 미세 조정됩니다.\n' +
      '\n' +
      '도 4: Gemini Pro Vision Direct Prompting을 기준선으로 한 인간 쌍별 선호도 평가 결과(이 방법 자체는 쌍별 비교를 위한 기준선 역할을 하기 때문에 표에는 표시되지 않음). 우리는 100개의 예를 샘플링하고 각 비교 쌍에 대해 5명의 주석자를 요청하고 각 예에 대한 다수결을 취한다. 승률이 높고 손실률이 낮으면 인간 주석자가 판단할 때 최상의 품질을 제안합니다.\n' +
      '\n' +
      'Finetuning은 Design2Code-18B와 기본 버전 CogAgent-18B의 비교에 의해 나타난 바와 같이 모든 차원들에서 큰 개선을 달성한다. (5) Finetuned Design2Code-18B는 WebSight VLM-8B에 비해 블록 매칭과 텍스트 유사도는 우수하나, 위치 유사도와 색상 유사도는 저조하다. 우리는 잠재적으로 처음 두 가지를 더 강력하고 더 큰 기본 모델에, 후자의 두 가지를 더 많은 양의 미세 조정 데이터에 기인할 수 있다. 섹션 5.2에서 미세화된 모델의 학습 과정에 대한 심층 분석을 제공합니다.\n' +
      '\n' +
      '### Human Evaluation\n' +
      '\n' +
      '위의 자동 메트릭은 모델 성능의 세분화된 분해를 제공하지만 이러한 웹 페이지의 궁극적인 청중인 인간이 생성된 웹 페이지를 어떻게 생각하는지 묻는 것도 중요하다. Prolific 5에서 인간 주석자(시간당 $16의 비율로 지불)를 모집하여 일련의 인간 평가를 수행하여 모델과 방법을 비교하고 가장 성능이 좋은 모델의 품질을 직접 평가했다. 우리는 인간 평가에 대한 벤치마크에서 100개의 예를 샘플링한다. 모든 인간 평가에서 각 질문은 \\(5\\)의 인간 주석자에 의해 주석이 달리고, 우리는 다수결 투표에 의해 결과를 도출한다. 부록 D의 주석자에게 제공한 모든 지침을 제공하고 아래 주요 프로토콜과 결과를 요약한다.\n' +
      '\n' +
      '각주 5: 우리는 주석자를 98% 이상의 합격률로 2,500건의 조사를 완료한 미국 국민으로 제한한다.\n' +
      '\n' +
      'Pairwise Model Comparison of evaluating instruction-following LLMs(예를 들어, [Zhou et al., 2023, Dubois et al., 2023])에 따라, 우리는 인간 주석이 생성된 웹 페이지 쌍(하나는 기준선에서, 다른 하나는 테스트된 방법에서)의 순위를 매겨서 어느 것이 참조와 더 유사한지를 결정하도록 요청한다. 우리는 Gemini Pro Vision Direct Prompting을 기준선으로 사용하고 이 기준선에 대한 다른 7가지 방법의 Win/Tie/Lose 비율을 수집한다(위치 편향을 피하기 위해 순서를 무작위로 섞는다). 각 쌍은 승리(Lose)가 다수결(\\(\\geq 3\\))을 받아야 승리(Lose)로 간주됩니다. 다른 사건들은 모두 타이로 간주됩니다.\n' +
      '\n' +
      '그림 4의 인간 평가에 기초하여, 우리는 (1) GPT-4V가 다른 기준선보다 실질적으로 더 나은 반면, 텍스트 증강 프롬프트 및 자기 수정 프롬프트는 모두 직접 프롬프트보다 더 개선될 수 있다는 것을 발견했다. (2) 텍스트 증강 프롬프트는 제미니 직접 프롬프트 기준선을 약간 개선할 수 있는 반면, 자기 수정을 추가로 추가하는 것은 도움이 되지 않는다. 직관적으로, 자기 수정 모델은 주어진 두 이미지(초기 모델 생성의 참조 스크린샷과 스크린샷) 간의 차이를 이해하고 이를 수정된 HTML 코드에 대응시켜 반영해야 하며, 이는 텍스트 증강을 활용하는 것보다 어렵기 때문에 더 향상된 모델 기능을 요구할 수 있다. (3) WebSight VLM-8B가 Gemini Direct prompting(54% win rate, 35% lose rate)보다 우수한 성능을 보여 대량의 데이터에 대한 미세조정이 특정 도메인에서 상용 모델과 일치할 수 있음을 시사한다. (4) 우리의 모델 Design2Code-18B는 Gemini Pro Vision direct prompting (38% win rate, 37% lose rate)의 성능과 일치한다.\n' +
      '\n' +
      '직접 평가 자동 및 인간 평가는 다양한 모델과 방법 간의 비교를 제공하지만 독자는 여전히 "_프론트 엔드 엔지니어링을 자동화하는 데 얼마나 멀지?"라고 궁금해할 수 있다. 이 질문에 대한 보다 직관적인 답변을 제공하기 위해 인간 주석자에게 각 참조 웹 페이지를 최고의 AI 생성 웹 페이지(GPT-4V 자체 수정 프롬프트 사용)와 비교할 것을 추가로 요청한다. 모든 예시는 5명의 주석자에 의해 주석이 달리고, 우리는 다수결을 취한다. 완전\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r} \\hline \\hline  & **coef** & **std err** & **p** \\\\ \\hline\n' +
      '**Block-Match** & 2.0442 & 0.648 & 0.002\\\\\n' +
      '**Text** & 1.3758 & 2.217 & 0.535\\\\\n' +
      '*** & 7.8037 & 1.312 & 0.000 \\\\\n' +
      '**Color** & 2.0731 & 0.757 & 0.006\\\\\n' +
      '**CLIP** & 10.2353 & 2.855 & 0.000 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 동일한 쌍의 자동 메트릭의 차이에 대한 로지스틱 회귀를 사용하여 인간 주석(윈/손실)을 예측하는 계수. 인간 평가는 대부분 위치 및 CLIP 유사성과 상관관계가 있다.\n' +
      '\n' +
      '주석에 주어진 지침은 부록 D에서 찾을 수 있다. 구체적으로, 우리는 두 가지 관점에서 직접 평가를 수행한다:\n' +
      '\n' +
      '1. **AI 생성 웹페이지가 원래의 웹페이지를 대체할 수 있는가?** 모든 예시의 순서를 뒤섞고 주석자에게 두 웹페이지가 외관 및 내용 면에서 충분히 유사한지 여부를 판단하도록 요청하여 상호 교환적으로 전개될 수 있도록 한다. 우리는 AI 생성 웹 페이지의 **49%가 참조 웹 페이지**와 교환 가능한 것으로 간주된다는 것을 발견했다.\n' +
      '2. **참조 웹페이지 또는 AI 생성이 더 나은가요?** 다음에 다른 질문을 하는데, 여기서 우리는 예제 순서를 섞고 주석자에게 어떤 웹페이지가 더 잘 설계되었는지 질문한다(주석자는 어떤 웹페이지가 참조인지 모르고 어떤 웹페이지가 AI 생성인지 모른다). 아마도 놀랍게도 GPT-4V에 의해 생성된 **웹페이지는 64% 사례에서 선호된다. 우리는 이 모델이 현대적이고 인기 있는 웹페이지 디자인 원칙(Ivory and Megraw, 2005; Beaird et al., 2020)에 더 많은 액세스 권한을 가질 수 있으므로 이러한 모범 사례를 기반으로 원래 디자인을 자동으로 개선할 수 있다고 가정한다. 이것은 또한 웹사이트 디자인 개선 도구에서 향후 작업에 대한 많은 새로운 기회를 열어준다.\n' +
      '\n' +
      '##### 자동평가 vs 인간평가\n' +
      '\n' +
      '자동 평가 결과와 인간 평가 결과 사이에 몇 가지 흥미로운 불일치가 있다는 점에 주목할 필요가 있다. 예를 들어, 인간 평가는 텍스트 증강 프롬프트보다 GPT-4V 자체 수정 프롬프트의 순위를 더 잘 매기는 반면, 자동 메트릭은 혼합된 결과를 보여준다. 더욱이, 인간이 WebSight VLM-8B를 Design2Code-18B보다 더 나은 것으로 평가하더라도, 자동 메트릭에 의해 측정된 바와 같이 훨씬 더 나쁜 블록-매치 및 텍스트 유사성을 갖는다. 이 부분에서 우리는 그러한 불일치를 자세히 살펴보고 그러한 불일치가 버그가 아닌 기능인 이유에 대해 논의한다.\n' +
      '\n' +
      '우리는 자동 메트릭과 인간 쌍별 선호도 사이의 상관 관계를 연구한다. 구체적으로, 588개의 쌍별 인간 주석(Win/Lose only)을 50% 훈련 세트와 50% 테스트 세트로 무작위로 분할한다. 1개의 기준 \\(R\\)과 2개의 후보 \\(G_{1},G_{2}\\)이 주어지면, 각 차원(예: \\(\\mathbf{match_{block}}(R,G_{1})-\\mathbf{match_{block}}(R,G_{2})\\)의 차이를 특징으로 하고 로지스틱 회귀에 의해 Win(1) 또는 Lose(\\(0\\))를 예측한다. 도출된 로지스틱 회귀 모델은 76.9%의 정확도를 달성했으며, 특징의 계수와 유의성은 표 3에 나와 있으며 텍스트 유사성은 인간과 거의 상관관계가 없음을 발견했다. 대조적으로, 정합된 블록의 위치 유사도와 CLIP 유사도는 가장 상관관계가 높은 두 개의 자동 메트릭이다. 이는 **인간이 보통 인간의 하향식 처리(길버트와 리, 2013)를 반영하여 세부 내용보다는 높은 수준의 시각적 효과와 레이아웃에 더 많은 관심을 기울인다는 것을 시사한다. 요약하면, 우리는 인간의 평가가 웹페이지의 "원칙적 구성요소"만을 고려하려는 인지적 편향으로 인해 여기서 신탁으로 맹목적으로 믿어서는 안 된다고 주장한다. 대신 새로운 모델과 방법을 평가할 때 높은 수준의 유사성(인간 쌍별 선호도 및 CLIP 유사성)과 낮은 수준의 요소(미세 훈련된 블록별 유사성)를 모두 고려해야 하며 이상적으로는 양쪽 전선에서 점수를 잘 매겨야 한다.\n' +
      '\n' +
      '## 5 Analysis\n' +
      '\n' +
      '웹페이지를 어렵게 만드는게 뭐에요?\n' +
      '\n' +
      '웹 페이지의 생성을 어렵게 하는 것을 이해하기 위해, 우리는 (1) 기준 구현의 총 태그 수; (2) 기준 구현의 고유 태그 수; 및 (3) 기준 구현의 DOM 트리 깊이를 포함하여 자동 메트릭과 다양한 난이도 지표 사이의 상관 관계를 계산한다. 표 4는 태그의 총 수가 난이도의 강력한 지표이며, 태그가 더 많은 웹 페이지는 모든 세립 차원에 따라 점수가 더 낮은 경향이 있음을 보여준다.\n' +
      '\n' +
      '차원이 다른 학습과정은 무엇인가?\n' +
      '\n' +
      '우리는 표 2의 성능 차이를 더 잘 이해할 수 있도록 그림 5에 다른 자동 평가 차원에 대한 학습 과정을 추가로 플로팅한다. 구체적으로 기본 모델 체크포인트와 모든 훈련 체크포인트에 대해 각 측면(훈련 전 \\(0\\) 및 훈련 후 \\(1\\))의 정규화된 성능을 보여준다. 한편, 블록-매치, 텍스트 및 포지션에 대한 성능은 \\(2000\\) 단계에 대한 훈련 후 빠르게 포화되고 이후 안정적으로 유지되며, 이는 아마도 기본 모델에서 가장 전이 가능한 능력이기 때문일 수 있다. 반면에 색상 유사도와 CLIP 유사도는 \\(4000-5000\\) 단계까지 꾸준히 증가한다. 텍스트 및 배경에 대한 올바른 색상 코드를 생성하는 것은 다른 측면보다 HTML 훈련 데이터에서 더 많은 이점을 얻을 수 있으며 전체 웹사이트 데이터 세트를 사용하고 완전히 미세 조정함으로써 더 향상될 수 있다고 가정한다.\n' +
      '\n' +
      '### Qualitative Analysis\n' +
      '\n' +
      '우리는 제안된 텍스트 증강 프롬프트 및 자체 수정 프롬프트가 GPT-4V에서 개선을 달성하는 예를 통해 수동으로 확인한다.\n' +
      '\n' +
      '직접 프롬프트를 개선하는 텍스트-증강 프롬프트의 예는 그림 6에서 예시된 바와 같이 텍스트-증강 프롬프트가 생성된 콘텐츠, 특히 텍스트에서 더 높은 리콜을 가짐으로써 직접 프롬프트보다 대부분 개선된다는 것을 발견하며, 여기서 직접 프롬프트로부터의 출력은 텍스트 콘텐츠의 대부분을 놓치지만 텍스트-증강 프롬프트가 이를 복구하여 블록-일치 점수를 0.25에서 0.84로 개선한다.\n' +
      '\n' +
      '텍스트 증강 프롬프트에 비해 자가 수정 프롬프트가 개선되는 예를 우리는 다음으로 텍스트 증강 프롬프트에서 초기 세대에 따라 자가 수정 프롬프트가 개선되는 예를 분석한다. 우리는 개선의 두 가지 주요 원인을 찾는다. 그림 7의 첫 번째 예는 자체 수정이 웹페이지에서 누락된 요소를 다시 가져와 블록-매치를 증가시키는 경우를 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c|l c|l c} \\hline \\hline \\multicolumn{2}{c}{Total Num of Tags} & \\multicolumn{2}{c}{Num of Unique Tags} & \\multicolumn{2}{c}{DOM Tree Depth} \\\\ \\hline Metric & Corr & Metric & Corr & Metric & Corr \\\\ \\hline Block-Match & -0.28* & Block-Match & -0.16* & Block-Match & -0.04 \\\\ Text & -0.13* & Text & -0.08 & Text & 0.01 \\\\ Position & -0.19* & Position & -0.15* & Position & -0.10* \\\\ Color & -0.13* & Color & -0.09 & Color & -0.04 \\\\ CLIP & -0.12 & CLIP & -0.02 & CLIP & 0.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: GPT-4V 자체 수정 프롬프트에 대한 자동 메트릭과 3가지 프록시 난이도 지표 변수 간의 상관 관계. 태그의 총 수는 가장 강력한 지표이며, 태그가 더 많은 웹 페이지는 모델에 대해 더 어려운 경향이 있다. *는 p-값 \\(<0.05\\)을 나타낸다.\n' +
      '\n' +
      '그림 5: 다양한 자동 평가 차원에 대한 학습 프로세스, 여기서 우리는 기본 모델 체크포인트 및 모든 훈련 체크포인트에 대한 성능을 플롯한다. 각 차원에 대해, 점수는 훈련 전 (\\(0\\) 단계) 및 훈련 후 (\\(5000\\) 단계) \\(1\\)이 되도록 재스케일링된다. y축은 더 큰 값의 차이를 강조하기 위해 다시 조정됩니다.\n' +
      '\n' +
      '0.48에서 1.00까지의 스코어, 결과적으로 CLIP 유사도는 0.87에서 0.91까지의 스코어이다. 도 7의 두 번째 예는 레이아웃 오류가 자체 수정을 통해 고정되어 전체 CLIP 유사도가 0.85에서 0.91로 개선된 경우를 나타낸다.\n' +
      '\n' +
      'WebSight VLM-8B vs Design2Code-18B 우리는 그림 8에서 대표적인 예를 보여주는데, WebSight VLM-8B가 Design2Code-18B(색채 점수 \\(0.99\\) vs \\(0.66\\))보다 채색에서 훨씬 더 뛰어나고 전체 레이아웃(위치 점수 \\(0.91\\) vs \\(0.63\\) 및 CLIP 유사성 \\(0.90\\) vs \\(0.83\\))보다 더 우수하다. 그러나 WebSight VLM-8B는 텍스트를 환각하는 경향이 있으며 블록 일치(\\(0.85\\) vs \\(0.99\\))와 텍스트 유사성 점수(\\(0.98\\) vs \\(1.0\\))가 더 낮다. 일반적으로 WebSight VLM-8B는 텍스트 매칭 측면에서 본 모델보다 정확도와 재현율이 낮은 경향이 있음을 알 수 있다.\n' +
      '\n' +
      '##6 관련 업무\n' +
      '\n' +
      '멀티모달 LLMs는 멀티모달 이해 및 접지 생성을 가능하게 하기 위해 일반적으로 멀티모달 입력을 받아들이기 위해 추가 인코더로 증강된다.\n' +
      '\n' +
      '그림 6: 결측 텍스트가 성공적으로 생성되는 직접 프롬프트 기준선에 대해 개선되는 텍스트 증강 프롬프트의 예.\n' +
      '\n' +
      '그림 7: 텍스트 증강 프롬프트에 비해 개선 프롬프트의 자체 수정 예. 자가 수정은 누락된 텍스트를 추가하거나 배치 오류를 수정할 수 있습니다.\n' +
      '\n' +
      '입력한다. 대표적인 예로 BLIP-2[Li et al., 2023a]는 ViT[Dosovitskiy et al., 2020]를 Q-Former와 함께 큰 언어 모델들과 연결한다. 보이지 않는 작업에 대한 일반화 능력을 더욱 향상시키기 위해, 명령어 튜닝이 멀티모달 LLM에 도입되며, 여기서 LLaVA[Liu et al., 2023]은 COCO 캡션을 갖는 GPT-4[OpenAI, 2023]를 프롬프트하는 것에 기초하여 복잡한 이미지 기반 QA를 생성하고 InstructBLIP[Dai et al., 2023]은 13개의 데이터 세트를 동일한 포맷의 명령어 팔로우잉으로 변환한다. Ye et al. [2023]은 프리트레이닝 데이터를 더 스케일링하는 한편 Bai et al. [2023]은 접지 및 OCR 데이터를 멀티태스크 미세조정으로 포함한다. GPT-4V와 같은 상용 모델은 광범위한 비전 언어 작업에서 유망한 성능을 보여주었지만, Yan et al. [2023], Zhang et al. [2023], Zheng et al. [2024]는 스마트폰 UI 및 웹 사이트를 운영하기 위해 이를 조정한다. 우리의 작업은 현실적인 프론트 엔드 엔지니어링 작업에서 능력을 평가하기 위한 새로운 도전적인 벤치마크를 제공한다.\n' +
      '\n' +
      'UI Code GenerationNguyen and Csallner[2015]는 고전적인 텍스트 인식 및 컴퓨터 비전 기술(OCR, 에지 검출 등)을 통해 요소를 식별하고 그 위에 코드를 생성함으로써 모바일 UI를 역공학한다. Pix2Code[Beltramelli, 2018]는 CNN 및 RNN을 기반으로 UI-to-code 변환을 위한 end-to-end 시스템을 구축하는데, 이는 실제 세계 UI를 다루면서 복잡한 시각적 인코딩 및 긴 텍스트 디코딩의 도전에 직면한다. Robinson[2019], Asorglu et al. [2019]는 신경망 기반 객체 검출 및 시맨틱 세분화를 파이프라인으로 더 통합한다. 최근, Soselia et al. [2023]은 진보된 시각적 인코더들(예를 들어, ViT, Dosovitskiy et al., 2020) 및 언어 디코더들(예를 들어, LLaMA, Touvron et al., 2023a,b)을 활용하고 시각적 유사성을 신호로 사용하여 파이프라인을 미세조정한다. 그러나, 그들의 훈련 및 테스트 예들은 주로 소수의 간단한 요소들(예를 들어, 사각형, 원형, 버튼)을 포함한다.\n' +
      '\n' +
      '코드 LLM 및 프로그래밍 지원 도구 우리의 작업은 또한 코드 언어 모델 및 프로그래밍 지원 도구와 연결됩니다. Codex[Chen et al., 2021], StarCoder Li et al., [2023b], InCoder[Fried et al., 2022], CodeLlama[Roziere et al., 2023], 및 DeepSeek-Coder[Guo et al., 2024b]와 같은 코드에 대해 트레이닝된 LLM들은 자동 코드 완성 및 주입과 같은 프로그래밍 지원 애플리케이션들의 웨이브를 가능하게 하고, 사용자들이 코드베이스 6과 채팅할 수 있게 한다. 이것은 또한 인간-AI 협업을 용이하게 하기 위해 더 나은 프로그래밍 툴들을 설계하는 방법에 대한 HCI 연구들의 새로운 웨이브를 유도한다[Kallianvakou, 2022, Vasconcelos et al., 2023, Liang et al., 2023]. 우리의 벤치마크는 코드 LLM에 대한 현실적인 평가를 제공하며, 스스로 코딩할 필요가 없고 LLM과 협업만 할 수 있는 프론트엔드 디자이너에게 보다 강력한 프로그래밍 지원을 가능하게 하는 것을 목표로 한다.\n' +
      '\n' +
      '각주 6: [https://github.com/features/copilot](https://github.com/features/copilot)\n' +
      '\n' +
      '##7 결론 및 향후 과제\n' +
      '\n' +
      '본 연구에서는 다양한 실세계 웹페이지로 구성된 Design2Code 벤치마크를 테스트 사례로 소개하였다. 우리는 포괄적인 자동 메트릭을 개발하고 다양한 멀티모달 코드 LLM을 비교하기 위해 일련의 인간 평가를 수행하여 미세 조정된 오픈 소스 모델이 프롬프트 제미니 프로 비전과 일치할 수 있지만 여전히 GPT-4V에 뒤처져 있음을 보여준다. 또한, 인간 주석자는 GPT-4V 세대의 49%가 원래 참조를 대체할 수 있을 만큼 충분하다고 생각하는 반면 64%는 원래 참조보다 훨씬 더 잘 설계된 것으로 판단된다.\n' +
      '\n' +
      '우리는 Design2Code가 향후 많은 연구 방향에 힘을 실어주는 유용한 벤치마크가 될 수 있다고 믿는다. 우리는 그들 중 몇 명을 강조한다.\n' +
      '\n' +
      '도 8: WebSight VLM-8B와 Design2Code-18B의 비교. WebSight VLM-8B는 색상 인식에 탁월하지만 텍스트 내용을 환각한다.\n' +
      '\n' +
      '1. 멀티모달 LLM들에 대한 더 나은 프롬프트 기법들, 특히 예를 들어 웹페이지의 상이한 부분들을 점진적으로 생성함으로써 복잡한 웹페이지들을 핸들링하는 데 있어서.\n' +
      '2. 실제 웹 페이지로 개방형 멀티모달 LLM을 교육한다. 우리의 예비 실험은 실제 웹 페이지가 너무 길고 시끄러워서 직접 교육하는 것이 어렵다는 것을 보여주었고, 향후 연구는 이러한 교육을 안정적으로 만들기 위해 데이터 청소 파이프라인을 탐색할 수 있다.\n' +
      '3. 스크린샷 입력들을 넘어, 예를 들어, 테스트 입력으로서 프론트-엔드 설계자들로부터의 피그마 프레임들 또는 스케치 설계들을 수집하기 위해 확장한다. 이러한 확장은 평가 패러다임의 신중한 재설계도 필요하다.\n' +
      '4. 정적 웹 페이지에서 또한 동적 웹 페이지를 포함하도록 확장한다. 이것은 또한 단순한 시각적 유사성을 넘어 상호작용 기능을 고려하기 위한 평가가 필요하다.\n' +
      '\n' +
      '## Ethical Considerations\n' +
      '\n' +
      '프라이버시 우리는 ODC-By 라이선스에 따라 릴리스되는 데이터 세트 C4를 사용하여 속성 요구 사항에 따라 무료 공유, 수정 및 사용을 허용했다. 동일한 라이선스로 데이터 세트를 출시합니다. 또한 수동 필터링을 수행할 때 개인 정보 또는 민감한 정보(예: 데이트 웹사이트 프로필)가 포함된 웹 페이지를 명시적으로 필터링했다.\n' +
      '\n' +
      '웹 페이지 구축을 민주화하려는 의도에도 불구하고, 우리는 악성 웹사이트의 자동화된 생성 또는 라이선스 웹사이트의 코드 생성과 같은 Design2Code 기술의 잠재적인 이중 사용 위험을 인식한다. 우리는 연구 목적과 커뮤니티가 복합 LLM 능력을 더 잘 이해하기 위한 것임을 강조한다. 모든 데이터, 코드 및 모델 릴리스에 대한 명확한 윤리적 사용 지침을 제공하여 허용 및 허용되지 않는 사용 사례를 정의합니다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      '우리는 아리만 아로라, 지현 제, 이레나 가오, 윌 헬드, 라이언 루이, 웨이옌 시, 도라 자오, 로즈 왕, 케일럽 즈엠스, 마이클 라이언, 카밀 해리스, 개똥 조시, 이지아 샤오, 지아첸, 오마르 샤이크, 줄리 칼리니, 루시아 정, 줄리아 크룩, 티안유 가오, 트리스탄 스러쉬에 대해 그들의 도움된 논평과 토론에 감사드린다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Alayrac et al. (2022) J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. 하손기 Lenc, A. Mensch, K 밀리컨, M. 레이놀즈, R 링, E. 러더포드, S. 카비태 한진 공성호 사만귀에 몬테이로, J. 메닉, S. Borgeaud, A. Brock, A. Nematzadeh, S. 샤리프자데 빈코스키, R 바레이라, 오 빈얼스, A. 지서먼, K. 사이먼 플라밍고: 수발 학습을 위한 시각적 언어 모델. _ ArXiv_, abs/2204.14198, 2022. URL[https://api.semanticscholar.org/CorpusID:248476411](https://api.semanticscholar.org/CorpusID:248476411).\n' +
      '* Asroglu et al. (2019) B. Asroglu, B. R. Mete, E. Yildz, Y. 날카칸, A. 세젠, M. 다그테킨과 T 엔사리 기계 학습 기법을 사용하여 목업 이미지에서 자동 html 코드 생성 _2019 Scientific Meeting on Electrical-Electronics & Biomedical Engineering and Computer Science (EBBT)_, pages 1-4, 2019. doi: 10.1109/EBBT.2019.8741736.\n' +
      '* Bai et al.(2023) J. Bai, S. 배승 양승 왕승 Tan, P. Wang, J. Lin, C. Zhou, 및 J. Zhou. Qwen-vl: 이해, 현지화, 텍스트 읽기 및 2023년 이후의 다양한 비전 언어 모델입니다.\n' +
      '* Beaird et al. (2020) J. Beaird, A. Walker, and J. George. _ 아름다운 웹 디자인의 원리. (주)사이트포인트티 2020\n' +
      '* 벨트라멜리(2018) T. 벨트라멜리 pix2code: 그래픽 사용자 인터페이스 스크린샷으로부터 코드를 생성하는 단계. [Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems_, pages 1-6, 2018].\n' +
      '* Blecher et al.(2023) L. Blecher, G. Cucurull, T. 사이알롬과 R 스토닉 Nougat: Neural optical understanding for academic documents, 2023.\n' +
      '*Byeon et al.(2022) M. 변병현 이원 백승훈 킴 Coyo-700m: Image-Text pair dataset. [https://github.com/kakaobrain/coyo-dataset] (https://github.com/kakaobrain/coyo-dataset), 2022.\n' +
      '* Bregrego et al. (2021)M. 천진욱 Wuan, H. Ponde, J. Kaplan, H. Edwards, Y. 부르다 Joseph, G. Brockman, A. Ray, R. 푸리, G. 크루거, M. 페트로프, H. 클라프, G. 사스트리, P. 미슈킨, B. 찬, S. 회색남 라이더 파블로프 A. 파워 L. 카이저 Bavarian, C. Winter, P. Tillet, F. P. Such, D. W. Cummings, M. Plappert, F. Chantzis, E. Vannes, A. Herbert-Voss, W. H. Guss, A. Nichol, I. Babuschkin, S. 발라지 Jain, A. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. M. Knight, M. 황폐화 무라티 Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, W. 자렘바 코드에서 훈련된 대규모 언어 모델 평가. _ ArXiv_, abs/2107.03374, 2021. URL[https://api.semanticscholar.org/CorpusID:235755472](https://api.semanticscholar.org/CorpusID:235755472).\n' +
      '* Crouse(2016) D. F. Crouse. 2D 직사각형 할당 알고리즘을 구현할 때 _ IEEE Transactions on Aerospace and Electronic Systems_, 52(4):1679-1696, 2016. doi: 10.1109/TAES.2016.140952.\n' +
      '* Dai et al.(2023) W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. 왕병리, P. 펑, S. 호이 인스트럭션 블립: 명령어 튜닝이 있는 범용 비전 언어 모델에 대해, 2023.\n' +
      '* Dosovitskiy et al.(2020) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. 자이태 Unterthiner, M 데하니 민더러, G. 헤이골드, S. Gelly, J. Uszkoreit, N. 홀스비 이미지는 16x16 단어의 가치가 있습니다: 2020년 규모에서 이미지 인식을 위한 트랜스포머입니다.\n' +
      '* Dubois et al.(2023) Y. 두부아, 엑스 이룡 타오리 장일자, J. Ba, C. Guestrin, P. Liang, T. 하시모토 Alpacafarm: 인간의 피드백으로부터 학습하는 방법들에 대한 시뮬레이션 프레임워크. _ ArXiv_, abs/2305.14387, 2023. URL[https://api.semanticscholar.org/CorpusID:258865545](https://api.semanticscholar.org/CorpusID:258865545).\n' +
      '* Fried et al. (2022) D. Fried, A. Aghajanyan, J. Lin, S. I. Wang, E. Wallace, F. Shi, R. 중원 타우이 제틀모이어와 M 루이스 인코더: 코드 주입 및 합성을 위한 생성 모델. _ ArXiv_, abs/2204.05999, 2022. URL[https://api.semanticscholar.org/CorpusID:248157108](https://api.semanticscholar.org/CorpusID:248157108).\n' +
      '* Gilbert and Li(2013) C. D. Gilbert and W. 리 Top-down이 영상처리에 미치는 영향 Nature Reviews Neuroscience_, 14(5):350-363, 2013.\n' +
      '* 구글(2023) 구글. 제미니: 매우 유능한 멀티모달 모델의 가족입니다. _ ArXiv_, abs/2312.11805, 2023. URL[https://api.semanticscholar.org/CorpusID:266361876](https://api.semanticscholar.org/CorpusID:266361876)\n' +
      '* 코드 인텔리전스의 상승, 2024a.\n' +
      '* 코드 인텔리전스의 상승, 2024b.\n' +
      '* Hong et al.(2023) W. 홍원 왕규 Lv, J. Xu, W. 유재지 왕주영 왕영 장재리 동민 딩, 그리고 제탱 Cogagent: A visual language model for gui agent, 2023.\n' +
      '* Hu et al. (2021) E. J. Hu, Y. Shen, P. Wallis, Z. 알렌주 이성 왕락 왕, W. 첸 Lora: 2021년, 대형 언어 모델의 낮은 순위 적응.\n' +
      '* Huang et al. (2023) J. Huang, X. 천성호 Mishra H. S. Zheng, A. W. Yu, X. 송동주 대형 언어 모델은 아직 스스로 추론을 수정할 수 없다. _ ArXiv_, abs/2310.01798, 2023. URL[https://api.semanticscholar.org/CorpusID:263609132](https://api.semanticscholar.org/CorpusID:263609132)\n' +
      '* 허깅페이스(2024) 허깅페이스. Huggingface webight, 2024. URL[https://huggingface.co/datasets/HuggingFaceM4/WebSight](https://huggingface.co/datasets/HuggingFaceM4/WebSight).\n' +
      '* Ivory and Megraw (2005) M. Y. Ivory and R. 메그로 웹 사이트 디자인 패턴의 진화. _ ACM Transactions on Information Systems (TOIS)_, 23(4):463-497, 2005.\n' +
      '* Jiang et al. (2023) A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. - A. 라초, P. 스톡, T. L. 스카오, T. 라브릴 왕태 라크루아와 W. E. 사예드 미스트랄 7b, 2023\n' +
      '* 칼리암바쿠(2022) E. 칼리암바쿠. 지텁 부조종사가 개발자 생산성과 행복에 미치는 영향을 정량화하면 2022년이다.\n' +
      '* Kalliamvakou et al. (2023)G. 김태환 홍민 임재남 황성호 윤동한, S 박 선생 Ocr이 없는 문서 이해 변압기. 유럽 컴퓨터 비전 회의에서, 페이지 498-517. 2022년 스프링어.\n' +
      '* Le et al. [2020] T. H. Le, H. Chen, and M. A. Babar. Deep learning for source code modeling and generation: Models, applications, and challenges. _ACM Computing Surveys (CSUR)_, 53(3):1-38, 2020.\n' +
      '* Li 등 [2023a] J. Li, D. Li, S. Savarese, S. 호이 Blip-2: Bootstrapping language-image pre-training with frozen image encoder and large language models, 2023a.\n' +
      '* Li 등 [2023b] R. 이엘비알 지남 Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, Q. 류철철 왕오 데해인 다바도르즈, J. 라미-포이리에, J. 몬테이루, O. N. 슐리아스코 곤티에, 노 미드 A. 제바즈, M. -H. Lee, L. K. Umapathi, J. Zhu, B. Lipkin, M. 오블로쿨로프 왕래 Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. 조카 데이지 장남 파미, 유 바타차리야 유승 싱상 루치오니, P. 빌레가스, M. 쿠나코프, F. 자다노프, M. 로메로, T 이남 Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. 다오만 Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. 레드, D. 프라이드, D. 바다나우, Y. Jernite, C. M. Ferrandis, S. M. Hughes, T. 울프 A 구하 L 본 웨라, 그리고 H. 드 브리스 스타코더: 출처가 당신과 함께 할 수 있기를! _ ArXiv_, abs/2305.06161, 2023b. URL[https://api.semanticscholar.org/CorpusID:258588247](https://api.semanticscholar.org/CorpusID:258588247)\n' +
      '* Liang et al. [2023] J. T. Liang, C. Yang, and B. A. Myers. A large-scale survey on the usability of ai programming assistants: Successes and challenges. In _International Conference on Software Engineering_, 2023. URL [https://api.semanticscholar.org/CorpusID:257833548](https://api.semanticscholar.org/CorpusID:257833548).\n' +
      '* Liu et al. [2023] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning, 2023.\n' +
      '* Luo et al. [2001] M. R. Luo, G. Cui, and B. Rigg. The development of the cie 2000 colour-difference formula: Ciede2000. _Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color; Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Francais de la Couleur_, 26(5):340-350, 2001.\n' +
      '* Lv et al. [2023] T. Lv, Y. Huang, J. Chen, L. Cui, S. Ma, Y. Chang, S. Huang, W. Wang, L. Dong, W. Luo, S. Wu, G. Wang, C. Zhang, and F. Wei. Kosmos-2.5: A multimodal literate model, 2023.\n' +
      '* Madaan et al. [2023] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. _ArXiv_, abs/2303.17651, 2023. URL [https://api.semanticscholar.org/CorpusID:257900871](https://api.semanticscholar.org/CorpusID:257900871).\n' +
      '* Mori et al. [1992] S. Mori, C. Y. Suen, and K. Yamamoto. Historical review of ocr research and development. _Proceedings of the IEEE_, 80(7):1029-1058, 1992.\n' +
      '* Nguyen and Csallner[2015] T. A. Nguyen and C. Csallner. 리마우이(t)와의 역방향 엔지니어링 모바일 애플리케이션 사용자 인터페이스. In _2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)_, pages 248-259. IEEE Computer Society, 2015.\n' +
      '* OpenAI[2023] OpenAI. Gpt-4v(sion) 시스템 카드. 2023. URL[https://api.semanticscholar.org/CorpusID:263218031](https://api.semanticscholar.org/CorpusID:263218031)\n' +
      '* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021.\n' +
      '* Raffel et al. [2019] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv e-prints_, 2019.\n' +
      '* 로빈슨[2019] A. 로빈슨. 스케치2코드: 종이 목업으로부터 웹사이트를 생성하는 단계. _ ArXiv_, abs/1905.13750, 2019. URL[https://api.semanticscholar.org/CorpusID:173188440](https://api.semanticscholar.org/CorpusID:173188440).\n' +
      '* Raffel et al. [2019]B. 로지에르, J. 게링, F. 글로클, S. 수틀라, 아이 갓, 엑스 탄영 아디, 제이류, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. P. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D\'efossez, J. Copet, F. Azhar, H. Touvron, L. 마틴 우수니에, T 사이알롬과 G. 신네베 코드 라마: 코드에 대한 기초 모델을 엽니다. _ ArXiv_, abs/2308.12950, 2023. URL[https://api.semanticscholar.org/CorpusID:261100919](https://api.semanticscholar.org/CorpusID:261100919)\n' +
      '* Schuhmann et al. (2022) C. Schuhmann, R. 보몬트 Vencu, C. Gordon, R 와이트먼 체리, T 쿰베스, A. 카타, C. 멀리스, M. Wortsman, P. Schramowski, S. 건더티 크로슨, L. 슈미트, R. Kaczmarczyk and J. Jitsev. Laion-5b: 차세대 이미지-텍스트 모델을 훈련하기 위한 개방형 대규모 데이터세트, 2022.\n' +
      '* Shinn et al.(2023) N. 신F. Cassano, B. Labash, A. Gopinath, K. 나라심한, S. 야오 반사: 언어 강화 학습을 하는 언어 에이전트. 2023. URL[https://api.semanticscholar.org/CorpusID:258833055](https://api.semanticscholar.org/CorpusID:258833055)\n' +
      '* Soselia et al. (2023) D. Soselia, K. 사이풀라와 티 주 렌더링 없이 비주얼 비평가(Visual critic)를 이용하여 ui-to-code 역생성기를 학습한다. 2023. URL[https://api.semanticscholar.org/CorpusID:265302631](https://api.semanticscholar.org/CorpusID:265302631)\n' +
      '* Telea(2004) A. Telea. 패스트 마칭 방법에 기반한 영상 인페인팅 기법. _ Journal of graphics tools_, 9(1):23-34, 2004.\n' +
      '* Touvron et al.(2023a) H. Touvron, T. 라브릴, G. 이자카드, X. 마티넷 - A. 라초, T. 라크루아, B. 로지에르, N. 고얄, E. 함브로, F. 아즈하르, A. 로드리게스, A. 줄린, E. 그레이브, G. 램플. Llama: Open and efficient foundation language models, 2023a.\n' +
      '* Touvron et al.(2023b) H. Touvron, L. 마틴기 스톤, P. 알버트, A. 알마하일리, Y. 바배이 바슐리코프 바트라, P. 바가바, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.\n' +
      '* Vasconcelos et al. (2023) H. Vasconcelos, G. Bansal, A. Fourney, Q. V. Liao, and J. W. Vaughan. 생성 확률은 충분하지 않다: Ai-powered 코드 완성에서 불확실성 강조의 유효성을 탐색한다. _ ArXiv_, abs/2302.07248, 2023. URL[https://api.semanticscholar.org/CorpusID:256846746](https://api.semanticscholar.org/CorpusID:256846746)\n' +
      '* Yan et al.(2023) A. Yan, Z. 양원 주경호 린, L 이정왕 Zhong, J. McAuley, J. Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. _ arXiv preprint arXiv:2311.07562_, 2023.\n' +
      '* Ye et al.(2023) Q. 예홍수 연영 주종왕 시창리 서현천 Qi, J. Zhang, F. Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.\n' +
      '* 음과 노이빅(2017) P. 음과 G. 노이빅. 범용코드 생성을 위한 구문신경 모델 _ ArXiv:1704.01696_, 2017.\n' +
      '* Zhai 등(2023) X. Zhai, B. Mustafa, A. Kolesnikov, L. 베이어 언어 이미지 사전 훈련에 대한 시그모이드 손실 In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, Oct. 2023. doi: 10.1109/iccv51070.2023.01100. URL[http://dx.doi.org/10.1109/ICCV51070.2023.01100](http://dx.doi.org/10.1109/ICCV51070.2023.01100)\n' +
      '* Zhang et al.(2023) C. Zhang, Z. 양종유 한익 천진 황, B. 푸, G. 유. 겉보기: 스마트폰 사용자로서의 멀티모달 에이전트, 2023.\n' +
      '* Zheng et al. (2024) B. Zheng, B. Gou, J. Kil, H. Sun, and Y. 수 Gpt-4v(sion)는 일반 웹 에이전트이며, 접지된 경우, 2024이다.\n' +
      '* Zhou et al. (2023) C. Zhou, P. Liu, P. Xu, S. 이어재선 마오진 마아에프랫 유승 장근고 루이스 제틀모이어, O 레비 리마: 2023년, 정렬이 더 적습니다.\n' +
      '\n' +
      '## 부록 추가 데이터세트 통계\n' +
      '\n' +
      '표 5에서 가장 빈번한 HTML 태그의 테이블을 제시한다.\n' +
      '\n' +
      '## 부록 B 텍스트 검출 및 병합 상세 정보\n' +
      '\n' +
      '주어진 스크린샷에서 텍스트들을 검출하기 위한 일반적인 접근법은 OCR 툴들[14]을 사용하는 것이며, 이는 그들의 경계 박스들과 함께 텍스트 세그먼트들의 리스트를 반환한다. 그러나 우리의 경우 오픈 소스 OCR 도구가 일반적으로 노이즈 출력을 출력하므로 다운스트림 평가의 안정성에 영향을 미칠 수 있다. 우리는 이미 참조 웹 페이지 스크린샷을 위한 소스 HTML 코드를 가지고 있기 때문에, 소스 HTML 코드에서 다른 텍스트 세그먼트에 대해 색상을 다르게 변경하고 두 개의 추가 스크린샷을 취하고 다른 색상을 가진 픽셀을 추적함으로써 웹 페이지 내의 텍스트 세그먼트를 검출하는 대안적인 접근법을 적용한다. 이것은 텍스트 인식 오류 없이 스크린샷에서 HTML 소스 코드에서 텍스트 세그먼트를 찾는 데 도움이 된다.\n' +
      '\n' +
      '검출된 두 개의 블록 집합을 기반으로 Jonker-Volgenant 알고리즘 [11] (Scipy 7에서 구현)을 사용하여 \\(R\\)과 \\(G\\) 사이의 최적 정합 \\(M\\)을 구하고, 여기서 \\((p,q)\\in M\\)은 \\(r_{p}\\)을 \\(g_{q}\\)과 정합한다. 구체적으로, 텍스트 콘텐츠 간의 음의 시퀀스 유사도(\\(-\\mathbf{sim_{text}}(,)\\))를 이용하여 비용 행렬을 초기화하고, 시퀀스 유사도 0.5\\보다 낮은 매칭 쌍을 무시한다. 검출된 텍스트 블록들은 서로 다른 입도에 있을 수 있기 때문에, 우리는 또한 가장 높은 유사성을 갖는 매칭을 검색하기 위해 병합 이웃 텍스트 블록들을 열거한다. 그러나, 매칭은 여전히 완벽하지 않을 수 있으며, 특히 큰 입상차이가 있을 때(우리의 검색은 비연속적인 블록을 병합하는 것을 고려하지 않는다).\n' +
      '\n' +
      '각주 7: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)\n' +
      '\n' +
      '## 부록 C 프롬프트\n' +
      '\n' +
      '자체 수정 프롬프트를 위해 다음과 같은 프롬프트를 사용합니다.\n' +
      '\n' +
      '당신은 HTML과 CSS를 전문으로 하는 전문 웹 개발자입니다. 웹 페이지를 구현하기 위한 HTML 파일이 있지만 원래 웹 페이지와 다른 몇 가지 누락되거나 잘못된 요소가 있습니다. 현재 구현은 [텍스트 증강 프롬프트에서 생성된 코드]입니다. 현재 구현의 렌더링된 웹 페이지뿐만 아니라 만들고 싶은 참조 웹 페이지를 제공할 것입니다. 또한 여기 웹페이지에 포함하고 싶은 모든 텍스트를 제공합니다. [원본 웹페이지에서 추출한 텍스트]. 두 개의 웹 페이지를 비교하고 제공된 텍스트 요소가 포함될 것을 참조하고 원본 HTML 구현을 수정하여 참조 웹 페이지와 똑같이 만들어 주십시오. 코드가 구문적으로 올바른지 확인하고 잘 형성된 웹 페이지로 렌더링할 수 있습니다. 자리 표시자 이미지 파일로 "rick.jpg"를 사용할 수 있습니다. 페이페이\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|l l|l l} \\hline Tag & Frequency & Tag & Frequency & Tag & Frequency \\\\ \\hline \\textless{}**div\\textgreater{}** & 17790 & **\\textless{}style\\textgreater{}** & 1181 & **\\textless{}head\\textgreater{}** & 486 \\\\ \\textless{}**a\\textgreater{}** & 13309 & **\\textless{}td\\textgreater{}** & 997 & **\\textless{}body\\textgreater{}** & 486 \\\\ \\textless{}**di\\textgreater{}** & 6883 & **\\textless{}input\\textgreater{}** & 995 & **\\textless{}tr\\textgreater{}** & 436 \\\\ \\textless{}**span\\textgreater{}** & 6813 & **\\textless{}h3\\textgreater{}** & 759 & **\\textless{}b\\textgreater{}** & 429 \\\\ \\textless{}**meta\\textgreater{}** & 4629 & **\\textless{}h2\\textgreater{}** & 709 & **\\textless{}nav\\textgreater{}** & 416 \\\\ \\textless{}**p\\textgreater{}** & 3413 & **\\textless{}strong\\textgreater{}** & 595 & **\\textless{}i\\textgreater{}** & 400 \\\\ \\textless{}**br\\textgreater{}** & 2453 & **\\textless{}h1\\textgreater{}** & 536 & **\\textless{}section\\textgreater{}** & 381 \\\\ \\textless{}**u\\textgreater{}** & 2078 & **\\textless{}button\\textgreater{}** & 525 & **\\textless{}label\\textgreater{}** & 339 \\\\ \\textless{}**img\\textgreater{}** & 1870 & **\\textless{}title\\textgreater{}** & 492 & **\\textless{}form\\textgreater{}** & 292 \\\\ \\textless{}**option\\textgreater{}** & 1194 & **\\textless{}html\\textgreater{}** & 486 & **\\textless{}h4\\textgreater{}** & 289 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: 벤치마크 예제의 참조 구현에서 가장 빈번한 HTML 태그.\n' +
      '\n' +
      '전체 레이아웃뿐만 아니라 모든 요소의 크기, 텍스트, 위치 및 색상과 같은 사항에 주의를 기울입니다. 추가 설명 없이 새로운 수정 및 개선된 HTML 파일의 내용으로 직접 응답합니다.\n' +
      '\n' +
      '## 부록 D 인간 주석 상세\n' +
      '\n' +
      '설명서에서 주석자는 우선 순위(내용 \\(>\\) 레이아웃 \\(>\\) 스타일)의 순서에 따라 쌍을 검사하도록 요청받는다. 이 우선순위 목록은 두 가지 직관에 기초한다: (i) 레이아웃 비교는 콘텐츠가 (거의) 완성되었을 때만 의미가 있다. (ii) 독립 요소의 스타일은 복수의 요소의 레이아웃보다 고정하기가 더 쉽다. 자세한 지침은 다음과 같다.\n' +
      '\n' +
      '_Task Overview_\n' +
      '\n' +
      '_이 설문조사에서는 참조 웹페이지의 스크린샷과 참조 웹페이지를 복제하려는 두 개의 후보 웹페이지(예제 1 및 예제 2)가 제공됩니다. 당신의 임무는 두 후보 중 어느 것이 참조에 더 가까운지를 판단하는 것이다. 각각의 (참조, 예 1, 예 2)는 일렬로 제시되며, 여기서 스크린샷의 원래 경계는 검정색으로 표시된다._\n' +
      '\n' +
      '_Comparison Guide_\n' +
      '\n' +
      ': Content Check__\n' +
      '\n' +
      '_텍스트 내용: 후보 웹 페이지의 텍스트가 참조와 일치하는지 검사합니다. 결측 또는 추가 내용, 특히 제목과 같은 핵심 요소에 특별한 주의를 기울입니다._\n' +
      '\n' +
      '_Image Content: (이미지의 경우) 파란색 자리 표시자 블록의 배치를 평가한다.__Image Content: 파란색 자리 표시자 블록의 배치를 평가한다.\n' +
      '\n' +
      '하나의 예시가 다른 예시와 비교하여 상당한 누락 또는 추가 콘텐츠를 갖는 경우, 참조.__1차 판단 기준: 하나의 예시가 다른 예시와 비교하여 상당한 누락 또는 추가 콘텐츠를 갖는 경우, 참조와 덜 유사한 것으로 간주되어야 한다.\n' +
      '\n' +
      '_두 번째 단계: 레이아웃 체크_\n' +
      '\n' +
      '_Element Arrangement: 두 예의 내용(텍스트 및 이미지)이 유사하게 양호하거나 나쁜 경우, 이들 요소의 배열을 평가하기 위해 진행한다. 조직, 순서 및 계층 구조가 참조와 일치하는지 확인합니다._\n' +
      '\n' +
      ': 레이아웃의 차이가 관찰될 경우, 레이아웃이 기준과 가장 유사한 예는 더 높게 평가되어야 한다.__2차 판정 기준: 레이아웃의 차이가 관찰될 경우, 레이아웃이 기준과 가장 유사한 예는 더 높게 평가되어야 한다.\n' +
      '\n' +
      '_최종 단계: Style Check__\n' +
      '\n' +
      '예제 1과 예제 2가 내용 및 레이아웃에서 비슷할 경우에만 글꼴 스타일, 색상 및 크기와 같은 스타일 요소를 조사합니다.__스타일 속성: 예제 1과 예제 2가 내용 및 레이아웃에서 비슷할 경우에만 글꼴 스타일, 색상 및 크기와 같은 스타일 요소를 조사합니다._\n' +
      '\n' +
      '콘텐트와 레이아웃이 동등하게 매칭되는 경우, 기준.___에 더 가까운 스타일 속성을 갖는 예제를 선호해야 한다.\n' +
      '\n' +
      '_Overall Judgment_\n' +
      '\n' +
      '_우선순위(Content > Layout > Style) 순으로 기준을 기준으로, 어떤 예(예 1 또는 예 2)가 기준 웹페이지와 더 유사한지에 대한 전반적인 판단을 한다._\n' +
      '\n' +
      '_Judgment Options_\n' +
      '\n' +
      '_1. 예 1이 참조.__1에 더 가까우면 "예 1 더 잘"을 선택합니다.\n' +
      '\n' +
      '_2. 예 2가 참조.__2에 더 가까우면 "예 2 더 잘"를 선택합니다.\n' +
      '\n' +
      '_3. 두 예들이 기준.__3으로부터 유사하게 정확하거나 동등하게 떨어져 있는 경우에만 "타이"에 대한 옵션\n' +
      '\n' +
      '_Additional Tips_\n' +
      '\n' +
      '_1. 상세검사에 zoom-in을 이용.__\n' +
      '\n' +
      '_2. 다음.__2로 이동하기 전 각 단계의 주요 불일치에 초점을 맞춘다.\n' +
      '\n' +
      '_3. 당신의 판단은 콘텐츠, 레이아웃 및 스타일의 누적 평가에 기초해야 한다.__3. 또한 수업 후 8가지 예를 제공합니다. 주석문항의 UI는 그림 9이며, 쌍대 모델 비교를 위한 Fleiss의 카파는 \\(0.36\\)(\\(5\\) 주석자)이다.\n' +
      '\n' +
      '또한, 직접 평가를 위한 지침(GPT-4V 자체 수정 프롬프트에 의해 생성된 참조 및 웹 페이지 비교)을 제공한다. Fleiss의 카파는 첫 번째 질문의 경우 \\(0.32\\)(\\(5\\) 주석자), 두 번째 질문의 경우 \\(0.26\\)(\\(5\\) 주석자)이다.\n' +
      '\n' +
      '**AI 생성 웹페이지가 원래 웹페이지를 대체할 수 있습니까?**\n' +
      '\n' +
      '**Task Overview**\n' +
      '\n' +
      '각 질문에는 두 개의 웹페이지 스크린샷이 주어집니다._\n' +
      '\n' +
      '두 웹 페이지를 비교하면 교환 가능한지 여부를 결정해야 합니다.__\n' +
      '\n' +
      '필요한 경우 스크린샷을 자세히 보려면 확대하십시오.__\n' +
      '\n' +
      '"Yes"에 답해야 합니다. 만약:__\n' +
      '\n' +
      '_1. 그것들은 대략 비슷하게 보인다.__1.\n' +
      '\n' +
      '2. 유사한 content.__2.\n' +
      '\n' +
      '_3. 동일한 기능을 제공할 수 있다.__3.\n' +
      '\n' +
      '_(마이너 디테일은 그다지 중요하지 않다)__\n' +
      '\n' +
      '_Otherwise, you should answer "No."_\n' +
      '\n' +
      '** 참조 웹페이지나 AI 생성이 더 나은가요?**\n' +
      '\n' +
      '**Task Overview**\n' +
      '\n' +
      '각 질문에는 두 개의 웹페이지 스크린샷이 주어집니다._\n' +
      '\n' +
      '두 웹 페이지를 비교하면 어느 것이 더 나은지 결정해야 한다.__\n' +
      '\n' +
      '필요한 경우 스크린샷을 자세히 보려면 확대하십시오.__\n' +
      '\n' +
      '어느 것이 더 나은지 결정하기 위해 다음 측면을 고려할 수 있다:__\n' +
      '\n' +
      '_1. 더 판독 가능한__1\n' +
      '\n' +
      '그림 9: 쌍대 모델 비교를 위한 사용자 인터페이스.\n' +
      '\n' +
      '2. 레이아웃이 개선됨\n' +
      '\n' +
      '3. 더 나은 스타일\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>