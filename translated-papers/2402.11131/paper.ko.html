<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#추론 스트리밍: 보조모델이 없는 빠른 LLM 추론\n' +
      '\n' +
      ' 니킬 베네다웨드\n' +
      '\n' +
      'Irina Belousova\n' +
      '\n' +
      'Qichen Fu\n' +
      '\n' +
      'Henry Mason\n' +
      '\n' +
      'Mohammad Rastegari\n' +
      '\n' +
      'Mahyar Najibi\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '투기적 디코딩은 보조 초안 모델의 예측들에 기초하여 큰 타겟 언어 모델의 추론을 가속화하기 위한 두드러진 기술이다. 효과적이지만 애플리케이션별 설정에서는 높은 수용률을 달성하기 위해 초안 및 대상 모델을 모두 미세 조정하는 작업이 종종 포함됩니다. 다운스트림 작업의 수가 증가함에 따라 이러한 초안 모델은 추론 시스템에 상당한 복잡성을 추가한다. 본 논문에서는 미세 조정 목표를 다음 토큰 예측에서 미래 n-그램 예측으로 변경하여 드래프팅을 목표 모델에 융합하는 단일 모델 투기적 복호화 방법인 투기적 스트리밍(Speculative Streaming)을 제안한다. 추론 스트리밍은 생성 품질을 저하시키지 않고 요약, 구조화된 쿼리 및 의미 표현과 같은 다양한 작업 세트에서 1.8 - 3.1X만큼 디코딩 속도를 높입니다. 또한 투기 스트리밍은 매개변수 효율적입니다. 그것은 메두사 스타일의 아키텍처보다 더 빠른 속도를 달성하면서 \\(\\sim\\)10000X의 추가 매개변수를 더 적게 사용하여 자원이 제한된 장치에 적합하다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 변압기는 오늘날 언어 모델링을 위한 탁월한 도구입니다. 이러한 모델들의 품질은 스케일(Kaplan et al., 2020)에 따라 개선되어, 최첨단 수십억 파라미터 모델들의 도입으로 이어진다(Brown et al., 2020; Thoppilan et al., 2022; Chowdrey et al., 2023; Touvron et al., 2023). 이러한 모델들은 토큰 생성에 매우 효과적이지만, 모델 및 그 과도 상태들이 후속적으로 생성된 각각의 토큰에 대해 컴퓨팅 메모리에 로딩될 필요가 있기 때문에 높은 추론 비용을 발생시킨다. 더욱이, 이러한 모델들을 스케일링하는 것은 각각의 호출을 더 계산 집약적으로 만드는 것 외에도, 그들의 자기회귀 생성 메모리 바운드를 만들어(Pope et al., 2023), 그들이 이용가능한 컴퓨팅을 효과적으로 사용하는 것을 방지한다. 이는 특히 대기 시간이 긴 사용자 대면 애플리케이션에 대해 대형 자동 회귀 변압기의 배치에 상당한 도전을 제기한다.\n' +
      '\n' +
      '최근 연구(Leviathan et al., 2023; Chen et al., 2023)는 대용량 언어 모델(LLM) 추론의 메모리 바인딩 특성을 고려하여, 추론 연산에서 차용된 개념을 기반으로 복호화를 가속화하기 위한 효과적인 기법으로 추론 복호화를 제안하였다 (Burton, 1985). 추측적 디코딩의 핵심은 다수의 후보 미래 토큰을 먼저 추측한 후, 이를 모두 병렬적으로 검증하는 아이디어이다. 이를 달성하기 위해, 도 1(a)에 도시된 바와 같이, 후보 투기를 위한 작은 보조 "초안" 모델과 검증을 위한 큰 "타겟" 모델(Leviathan et al., 2023; Chen et al., 2023)의 두 모델 패러다임 접근법이 사용된다. LLM을 가속하는 데 효과적이지만, 추측 디코딩은 배치를 복잡하게 만든다. 또한 별도의 초안 모델을 훈련하고 목표 모델과 정렬해야 하기 때문에 훈련은 더 까다롭고 복잡해진다. 또한, 토큰 예측 중에 메모리에 두 개의 모델을 호스트해야 하는 것은 자원 효율적이지 않다. 이러한 증가된 풋프린트는 특히 자원 제약형 디바이스들에 대해 만족스럽지 않다.\n' +
      '\n' +
      '본 논문에서는 단일인 _Speculative Streaming_을 제안한다.\n' +
      '\n' +
      '그림 1: (a) 투기적 디코딩은 추측을 위해 자동으로 실행되는 별도의 초안 모델을 필요로 한다. (b) 투기적 스트리밍은 단일 스트림 융합 모델 내에서 모두 추측과 검증을 동시에 수행함으로써 시스템을 상당히 단순화한다.\n' +
      '\n' +
      '투기와 검증을 통합하는 모델 추측 디코딩 접근법으로, 그림 1(b)와 같은 별도의 초안 모델의 필요성을 제거한다. 이는 다중 스트림 주의를 표적 모델에 통합하여 미래의 후보 추측 역할을 하는 n-그램 예측을 수행함으로써 달성된다. 결과적으로, 순방향 모델 패스는 미래 토큰들에 대해 동시에 추측하면서 이전에 생성된 토큰들을 검증할 수 있다. 또한, 이전 접근법과 비교하여 투기 스트리밍은 종단 간 훈련되어 자연스럽게 투기와 검증 단계를 정렬한다.\n' +
      '\n' +
      '시스템을 상당히 단순하고 자원 효율적으로 만들면서, 투기적 스트리밍은 다양한 다운스트림 태스크 세트에 대한 품질 메트릭을 저하시키지 않으면서 2단계 투기적 디코딩(Leviathan et al., 2023)에 필적하는 속도 향상을 달성한다. 또한, 다수의 추가적인 고차원 예측 헤드를 도입하는 보다 최근의 블록별 디코딩 모델인 Medusa(Cai et al., 2023)보다 온-파 또는 더 나은 속도 향상을 유도한다. 더욱이, 투기적 스트리밍은 메두사(Cai et al., 2023)보다 10000X 더 적은 추가 파라미터를 필요로 하므로 자원 제약 장치에 이상적인 방법이다.\n' +
      '\n' +
      '요약하면, 투기 스트리밍의 장점은 다음과 같다:\n' +
      '\n' +
      '* 간소화된 미세 조정을 통해 실질적인 속도 향상을 달성하고 별도의 드래프트 모델이 필요하지 않습니다.\n' +
      '* 다양한 범위의 작업에 걸쳐 품질을 손상시키지 않으면서 더 나은 속도 향상을 달성하면서 (Cai et al., 2023)에 비해 10000X 더 적은 여분의 파라미터로 리소스 효율성을 입증하는 것.\n' +
      '*(Leviathan et al., 2023)에서 관찰된 바와 같이, 실행 중에 두 모델 간의 관리, 정렬 및 전환의 필요성을 제거함으로써 배치 프로세스를 단순화한다.\n' +
      '\n' +
      '##2 관련 작품\n' +
      '\n' +
      '큰 언어 모델들의 추론은 종종 자동-회귀 디코딩의 순차적인 성질에 의해 제한되며, 여기서 각각의 토큰 생성은 완전한 네트워크 순방향 패스를 요구한다. LLM의 메모리 공간을 직접 줄임으로써 대규모 언어 모델의 높은 추론 지연을 해결하기 위한 몇 가지 접근법이 제안되었다. 모델 양자화(Frantar et al., 2022; Yao et al., 2022; Dettmers et al., 2023), 더 작은 모델로 지식 증류(Gu et al., 2023; Agarwal et al., 2023), 및 프루닝(Frantar and Alistarh, 2023; Sun et al., 2023)이 이들 기술 중 하나이다. 최근 자기회귀 복호화를 가속화하기 위한 중요한 기법으로 추측 복호(speculative decoding, SD)가 등장하였다.\n' +
      '\n' +
      '원래의 추측 디코딩 접근법(Chen et al., 2023; Leviathan et al., 2023)은 더 작은 LLM(_a.k.a._ the _draft model_)을 활용하여, _target model_에 의해 검증될 토큰들의 후보 시퀀스를 생성한다. 잘 조정된 초안 모델을 사용하면 이 기술은 2-3배 추론 속도를 달성할 수 있다. 최근의 SD 변형들은 타겟 모델에 의해 추측된 토큰들의 수용률을 향상시키고 성능을 더욱 향상시키기 위해 배치 축(Sun et al., 2023), 및 트리-구조화된 배치들(Miao et al., 2023; Spector and Re, 2023)을 따른 병렬 계산을 제안한다. 그러나 이러한 방법은 정확하고 독립적인 초안 모델 개발의 필요성이라는 공통적 한계에 직면한다. 먼저, 메인 모델과 정렬된 그러한 드래프트 모델을 트레이닝하는 것은 사소한 것이 아니다(Zhou et al., 2023). 둘째, 두 개의 상이한 모델을 호스팅하는 것은 시스템 복잡성을 증가시키고, 유지하는데 더 계산적이고 동작적으로 비싸다.\n' +
      '\n' +
      '매우 최근에, 단일 모델 추측도 고려되었다. 특히, (Qi et al., 2020; Stern et al., 2018), Medusa (Cai et al., 2023)에 의해 영감을 받아, 다수의 출력 헤드를 트레이닝함으로써 미래의 토큰을 병렬로 예측하기 위해 메인 모델을 확장한다. 모델 초안을 필요로 하지 않지만, 각각의 메두사 머리 크기(_hidden_size_\\(\\times\\)_vocab_size_)는 협상할 수 없는 추가 파라미터를 필요로 한다. 자동-회귀 생성은 전형적으로 메모리-바운드 컴퓨트 패턴을 따르므로, 상당한 양의 여분의 파라미터들은 자원-제약된 디바이스들에 대한 배포 도전들을 도입한다. 대안적으로, 룩어헤드 디코딩(Fu et al., 2023)은 새로운 파라미터를 학습하지 않고 병렬 디코딩 방법을 제안한다. 그것은 자코비 디코딩과 n-그램 히스토리 궤적 캐시를 사용하여 미래의 n-그램 예측을 생성하고 검증한다. 이와는 달리, 투기적 스트리밍은 토큰 임베딩의 세트를 학습함으로써 n-그램 생성을 달성하고 투기와 검증을 동시에 실행함으로써 디코딩을 가속화한다. 특히, 우리의 접근법은 온-디바이스 배치를 단순화하면서, 기존의 추측적 디코딩 방법들(Chen et al., 2023; Leviathan et al., 2023; Cai et al., 2023)에 비해 훨씬 적은 수의 여분의 파라미터들로 더 나은 속도 향상을 달성할 수 있다.\n' +
      '\n' +
      '## 3 Method\n' +
      '\n' +
      '### Motivation\n' +
      '\n' +
      '대부분의 추측 디코딩 접근법들은 드래프트 및 타겟 모델들의 트레이닝 프로세스들에서 뚜렷한 분리를 나타낸다. 그러나, 기성품 초안 모델을 직접 사용하는 것은 종종 많은 다운스트림 애플리케이션에서 최적이 아닌 성능으로 이어진다. 투기된 토큰은 초안 및 대상 모델이 정렬되지 않을 때 대상 모델의 검증에 실패하는 경우가 많다. 향상된 속도 향상을 달성하기 위해 다운스트림 애플리케이션에서 드래프트 및 타겟 모델 모두를 미세 조정해야 한다. 우리의 목표는 다음 토큰을 동시에 예측하고 미래 토큰을 추측할 수 있는 종단간 훈련 가능한 단일 모델 프레임워크를 고안하는 것이다. 이는 (Leviathan et al., 2023)에서 보고된 것과 유사한 스피드 업을 달성하면서 보조 드래프트 모델의 필요성을 제거한다. 우리는 발전 품질의 저하 없이 자동 회귀 변압기 호출의 산술 강도를 증가시켜 이러한 속도 향상을 달성하는 것을 목표로 한다.\n' +
      '\n' +
      '### Speculative Streaming\n' +
      '\n' +
      '본 논문에서는 다운스트림 응용에서 디코더 전용 모델의 파라미터 효율적인 투기적 미세 조정 및 추론을 가능하게 하는 투기적 스트리밍을 제안한다. 표준 드래프트-타겟 투기적 디코딩(Leviathan et al., 2023) 및 보다 최근에 제안된 블록-와이즈 디코딩(Cai et al., 2023)과 비교하여, 투기적 스트리밍은 다음의 수정들을 도입한다. (a) 섹션 3.2.1 (c) 섹션 3.2.2 (d) 병렬 추론 및 검증, 섹션 3.2.3 및 마지막으로 (e) 섹션 3.2.4에 설명된 바와 같은 훈련 목표.\n' +
      '\n' +
      '###### 3.2.1 스트림 설계 및 초기화\n' +
      '\n' +
      '디코더 전용 사전 학습된 언어 모델의 파라미터 효율적인 미세 조정(Hu et al., 2022)은 다운스트림 애플리케이션에서 다음 타겟 토큰 \\(y_{t}\\)을 예측하기 위해 저순위 어댑터를 훈련하는 것을 포함한다. 미래 토큰 계획의 개념을 내재하기 위해, 우리는 다중 스트림 주의(Qi et al., 2020; Yang et al., 2019) 개념을 사용하여 다음 토큰 예측에서 n-그램 예측으로 목표 모델의 훈련 목표를 수정한다. 이 목표를 통해 모델은 향후 토큰을 계획하고 로컬 상관 관계에 대한 과적합을 방지할 수 있다. 또한, 각각의 \\(\\gamma\\) 스트림은 모델이 메모리 바인딩될 때 무시할 수 있는 레이턴시 오버헤드를 갖는 추측 토큰을 생성한다. 구체적으로, 각각의 추가된 스트림은 \\(p(y_{t+j}|y_{<t},x)\\), 여기서 \\(1<=j<=\\gamma\\), 메인 스트림은 \\(p(y_{t}|y_{<t},x)\\)를 예측한다. 우리는 (Vaswani et al., 2017)에 묘사된 멀티 헤드 어텐션 메커니즘을 메인 스트림 셀프 어텐션으로 지칭하고, 향후 토큰을 추측하기 위해 \\(\\gamma\\) 추가 셀프 어텐션 스트림을 소개한다.\n' +
      '\n' +
      '메인 스트림의 어텐션 메커니즘은 표준 멀티 헤드 어텐션(Vaswani et al., 2017)과 동일하다.\n' +
      '\n' +
      '\\[M_{t}^{k+1}=\\text{MHA}(M_{t}^{k},M_{\\leq t}^{k},M_{\\leq t}^{k}}}\\tag{1}\\text{MHA}(M_{t}^{k},M_{\\leq t}^{k})\n' +
      '\n' +
      '여기서 \\(M_{t}^{k}\\)는 계층 \\(k\\)과 시간 \\(t\\)에서 주 스트림의 은닉 상태를 나타내고 \\(MHA(H,H,H)\\)는 (Vaswani et al., 2017)에 기술된 바와 같이 질의 \\(HW^{Q}\\), 키 \\(HW^{K}\\)와 값 \\(HW^{V}\\) 사이의 주의를 나타낸다. 한편, 시간 단계 \\(t\\)에서의 각각의 투기 스트림 \\(j\\)은 투기 스트림 은닉 상태뿐만 아니라 이전의 주 스트림 은닉 상태에 따른다:\n' +
      '\n' +
      '[S_{tj}^{k+1}=\\text{MHA}(S_{tj}^{k},M_{\\leq t}^{k}\\oplus S_{t(\\leq j)}^{k},M_{\\leq t}^{k}\\oplus S_{t(\\leq j)}^{k}) \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(M_{t}^{k+1}\\) 및 \\(S_{t}^{k+1}\\)은 주요 및 추측을 나타낸다.\n' +
      '\n' +
      '도 2: 아키텍처: 우리는 (2)에 기술된 바와 같이 기본 모델의 상위 \\(N_{s}\\) 다중-헤드 어텐션(MHA) 층들을 다중-스트림 어텐션(MSA) 층들로 대체한다. 투기 스트림은 (3)에 설명된 바와 같이 레이어 \\(N-N_{s}\\) 및 스트림 식별자 임베딩(SE)의 숨겨진 상태를 사용하여 초기화되며 트리 형태의 투기 드래프트를 생성하는 데 사용된다. 이전 반복의 투기 트리 드래프트는 검증을 위해 배치되고 스트림 삽입 전에 가지치기된다. 각 포워드 패스 동안 이전 트리 드래프트가 검증되고 시간 단계 \\(t\\) 및 계층 \\(k\\)에서 3.2.2 스트림에 설명된 바와 같이 투기 스트림을 사용하여 새로운 트리 드래프트가 발행된다. 마지막 변압층의 은닉상태는 \\(N\\), \\(M_{t}^{N}\\)을 예측하는데 사용되는 반면, 마지막 층의 각 추측 스트림은 \\(S_{tj}^{N}\\)을 예측한다. 우리는 식 (1)에서 어텐션 메커니즘을 포함하는 층들을 MHA 층들로서 지칭하는 반면, 추측 스트림 어텐션 식 (2)를 포함하는 층들은 MSA 층들로서 지칭된다.\n' +
      '\n' +
      '주 스트림 은닉 상태의 키/값 투영은 재계산을 피하기 위해 추론 동안 캐싱되는 반면, 추측 스트림 주의는 개별 스트림과 관련된 추가 키/값 투영을 저장하는 것을 피하기 위해 특별히 설계된다. 추측 스트림은 메인 스트림 키/값 컨텍스트로부터 컨텍스트 특징을 학습하도록 훈련되기 때문에 추론 동안 추가적인 캐싱 오버헤드를 도입하지 않고 리소스 제약된 디바이스의 메모리 바운드 내에서 동작할 수 있다. 우리는 투기 스트림의 은닉 상태를 임베딩 레이어로부터 초기화하는 대신 레이어 \\(N-N_{s}\\)에서 초기화한다. 여기서 \\(N_{s}<N\\) 구체적으로, 시간 \\(t\\)에서의 스트림 \\(j\\)은 층 \\(N-N_{s}\\)에서 초기화되며,\n' +
      '\n' +
      '\\[S_{tj}^{N-N_{s}}=f_{\\eta}(M_{t}^{N-Ns})+P_{j}^{N-N_{s}} \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(P_{j}\\)는 상대적인 위치감을 스트림에 임베딩하고 계산을 메인 스트림과 구별하는 스트림 식별자 임베딩이다. \\(P_{j}\\) (f_{\\eta}\\)는 주요 스트림 은닉 상태를 추측 스트림 은닉 상태로 변환하기 위해 순위\\(\\eta\\)의 선형 변환이다. 이 초기화는 주 스트림만 \\(N-N_{s}\\) 레이어를 통과하면 되기 때문에 순방향 통과당 계산량을 줄이는 데 도움이 되지만, 투기 스트림은 마지막 \\(N_{s}\\) 레이어를 통과하므로 투기 FLOP 기여도가 \\((N-N_{s})/N\\) 감소하고 결과적으로 장치의 피크 전력 소비를 돕는다. 순방향 통과 지연 시간 측면에서 FLOP는 모델이 메모리 바인딩일 때 크게 기여하지 않지만 섹션 3.2.2에서 설명한 대로 모델을 계산 바인딩으로 만들기 위해 추가 토큰을 샘플링하므로 FLOP 감소가 중요하다. 중간-상위 변압기 층들의 숨겨진 상태를 갖는 초기화는 또한 \\(M^{(N-Ns)}\\) 자체가 미래 n-그램들의 예측을 돕기 위해 높은 레벨의 콘텍스트 특징들을 포함함에 따라 미래 토큰 예측에 도움을 줄 수 있다(Pal et al., 2023). 또한 식별자 임베딩이 필요하지 않고 B에 설명된 대로 매개변수 오버헤드가 발생하지 않는 값 회전 기반 스트림 설계를 실험했다.\n' +
      '\n' +
      '3.2.2 병렬 추측 및 검증\n' +
      '\n' +
      '표준 드래프트-타겟 추측 디코딩(Leviathan et al., 2023)에서, 추측 및 검증 프로세스들이 순차적으로 일어난다. 초안 모델은 다음 초안을 생성하기 전에 대상 모델이 수정을 발행하기를 기다린다. 대상 모델도 투기 초안이 생성될 때까지 기다려야 한다. 투기 스트리밍은 투기와 검증을 병행하여 이 프로세스를 더 효율적으로 만듭니다. 각각의 포워드 패스에서, 이전 단계에서 생성된 드래프트가 검증되고 새로운 드래프트가 그림 2와 같이 생성된다. 예를 들어, 단계 \\(s\\)에서, 드래프트 토큰들 \\((\\tilde{y_{1}}...\\ tilde{y_{\\delta}})\\)는 수정토큰을 발행하기 위해 \\(0<\\delta\\leq\\gamma\\), 주 스트림 \\(M_{\\delta}\\)을 사용하고, 단계 \\(s+1\\)의 드래프트를 생성하기 위해 투기 스트림 \\(S_{\\delta(1...\\gamma)}\\)에서 로짓들을 사용한다.\n' +
      '\n' +
      '검증을 위해 추측된 토큰의 선형 시퀀스를 사용하는 대신, 우리는 트리의 각 경로가 하나의 가능한 검증 후보가 되도록 주 스트림 및 추측 스트림으로부터 토큰의 트리를 샘플링한다. 트리 드래프팅은 가장 긴 매칭 후보 시퀀스를 수용하는 것을 가능하게 하고, 더 많은 토큰들이 각각의 순방향 패스 동안 진보될 수 있다. 트리 드래프트를 생성하기 위해 투기 스트림의 로짓인 \\((z_{1}...z_{\\gamma})\\에서 1개의 토큰을 샘플링하는 대신 상위 토큰(k\\)을 샘플링하고 그림 2와 같이 샘플링된 토큰의 트리를 형성하여 스트림에서 샘플링된 토큰이 스트림에서 샘플링된 토큰의 전신이다. 트리의 각 노드가 이전 노드에 참석하도록 추가 주의 마스크(Vaswani et al., 2017)를 생성하여 하나의 순방향 패스에서 투기 토큰의 트리 드래프트를 처리한다. \\(j\\),\\(\\tilde{y}_{jk}\\)의 로짓으로부터 샘플링된 \\(k^{th}\\) 토큰과 \\(n\\),\\(\\tilde{y}_{nm}\\)의 로짓으로부터 샘플링된 \\(m^{th}\\) 토큰 사이의 어텐션 마스크\n' +
      '\n' +
      '\\[a_{\\tilde{y}_{jk}\\tilde{y}_{nm}}=\\begin{cases}0&\\text{if j=n+1,}\\\\\\-\\infty&\\text{otherwise}\\end{cases}\\tag{4}\\text{if\n' +
      '\n' +
      '자세한 내용은 그림 8을 참고하시기 바랍니다. 고정된 \\(\\gamma\\)와 \\(k\\)에 대해, 주의 마스크는 각각의 전방 패스에서 일정하게 유지되고 효과적인 배칭을 가능하게 한다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '###### 3.2.3 병렬 트리 가지치기\n' +
      '\n' +
      '투기 트리 초안의 순열 생성의 문제 중 하나는 각 스트림에서 샘플링된 \\(k\\) 토큰 간의 모든 순열이 다음 검증 통과를 위한 실행 가능한 투기 후보로 간주되어야 한다는 것이다. 예를 들어, 각 \\(\\gamma\\) 스트림에서 \\(k\\) 토큰을 샘플링하면 \\(1+\\sum_{g=1}^{\\gamma}k^{g}\\) 크기의 트리 드래프트가 생성된다. 또한, 각 드래프트 토큰은 MSA 계층에서 \\(\\gamma\\)의 추측 스트림으로 배치됨으로써 다음 드래프트의 생성이 동일한 순방향 패스에서 발생함을 보장하여 \\((1+\\gamma)*(1+\\sum_{g=1}^{\\gamma}k^{g})\\의 배치 크기를 갖는다. 배치 크기가 증가함에 따라 대상 모델 추론은 계산 결합(comput-bound)이 되어 더 많은 토큰을 샘플링하는 지연 이점을 제거한다. 우리는 부모 토큰과 즉시 자식 토큰 사이의 전이 확률에 기초하여 입력 트리 드래프트로부터 토큰들 중 일부를 프루닝하는 병렬 트리 드래프트 프루닝 계층을 도입함으로써 이 문제를 완화한다. 프록시 모델을 사용하지 않고 전이 확률을 얻기 위해 조기 종료 기반 기법을 사용한다. 구체적으로, 계층 \\(l\\), \\(M^{l}\\)에서 주 스트림의 은닉 상태는 낮은 랭크 선형 변환 \\(o_{\\theta}\\)을 통과하며, 여기서 랭크 \\(\\theta\\)은 일반적으로 파라미터 오버헤드를 작게 유지하기 위해 8과 같은 작은 값으로 설정된다. 원어 모델링 헤드, \\(H\\)을 사용하여 조기 퇴장 로짓, \\(\\tilde{z}=H(o_{\\theta}(M^{l}))를 구한다. \\ 부모 토큰 \\(p\\)과 자식 토큰 \\(c\\) 사이의 전이 확률을 근사화하기 위해 (\\tilde{z}_{pc}\\)를 사용한다. 프루닝 계층은 순방향 패스 레이턴시와 프루닝 정확도 사이의 트레이드-오프에 의해 안내되는, 네트워크의 임의의 지점에서 삽입될 수 있다. 조기 삽입은 대기 시간을 줄이지만 잠재적으로 가치 있는 토큰을 가지치기할 위험이 있다. 반대로, 늦은 삽입은 더 많은 "좋은" 토큰을 보유하지만, 증가된 순방향 패스 레이턴시의 비용으로 온다. 섹션 4.1에 설명된 모든 실험에서 우리는 경험적으로 추측 스트림 삽입 직전에 가지치기 레이어를 삽입한다. 보다 자세한 내용은 그림 7에서 확인할 수 있다.\n' +
      '\n' +
      '3.2.4 훈련목표 3.2.4 훈련목표\n' +
      '\n' +
      '미래 n-그램을 효율적으로 생성하기 위해, 우리는 다음 토큰뿐만 아니라 \\(\\gamma\\) 미래 토큰의 예측 손실에 대해 공동으로 기본 모델을 세밀하게 조정한다.\n' +
      '\n' +
      '[L_{ss}= -\\alpha_{0}(\\sum_{t=1}^{T}\\log p_{\\theta}(y_{t}|y_{<t},x)) \\tag{5}\\] \\[-\\sum_{j=1}^{\\gamma}\\alpha_{j}(\\sum_{t=1}^{T-j}\\log p_{\\theta}(y_{t+j}|y_{<t},x))\\\\tag{5}\\[-\\sum_{j=1}^{\\gamma}\\alpha_{j}(\\sum_{t=1}^{T-j}\\log p_{\\theta}(y_{t+j}|y_{<t},x))\\\\mu_{t=1}^{\\gamma}\\alpha_{j}(\\sum_{t=1}^{T-j}\\log p_{\\theta}(y_{t+j}|y_{<t\n' +
      '\n' +
      '여기서 \\(\\alpha_{0}\\) 및 \\(\\alpha_{j}\\)는 다음 토큰의 손실과 투기적 토큰 예측을 정규화하기 위해 경험적으로 설정된다. 섹션 3.2.3에 설명된 트리-프루닝 어댑터는 메인 및 투기 스트림과 공동으로 또는 스트림의 사후 트레이닝과 함께 다음 토큰 예측 손실에 대해 트레이닝될 수 있다. 훈련 시간은 MSA 층의 수에 따라 다르지만 \\(N_{s}=4\\)에 대한 (Cai et al., 2023) 스타일 접근법에 필적한다. 4에 설명된 실험의 경우, 우리의 레시피는 AdamQ 최적화기, 5e-4의 학습 속도 및 선형 스케줄러를 사용하여 BFloat16의 다운스트림 데이터 세트에 대한 5 에폭에 대한 LoRA 어댑터를 훈련하는 것을 포함한다. 트리 가지치기의 경우(섹션 3.2.3 참조) 매개변수 오버헤드를 최소화하기 위해 순위 8의 낮은 순위 선형 변환을 사용합니다.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '우리는 다양한 규모와 다양한 다운스트림 응용 프로그램의 사전 훈련된 모델에 대해 방법을 평가한다.\n' +
      '\n' +
      '**Dataset.** 우리는 온-디바이스 AI 어시스턴트에 필수적인 다양한 애플리케이션 세트, 즉 구조화된 쿼리, 텍스트 요약 및 의미 표현에 대해 우리의 방법을 테스트한다. 기본 모델을 공유하고 사용자 대면 애플리케이션에 애플리케이션별 어댑터를 사용하는 것이 표준이었기 때문에 우리는 특히 미세 조정 설정을 선택한다. Text Summarization을 위한 Dialogum (Chen et al., 2021) dataset, Structured Queries를 위한 WikiSQL (Zhong et al., 2017) 및 SPIDER (Yu et al., 2018)로부터 구축된 sql-create-context dataset, Meaning Representation을 위한 e2e-nlg dataset (Dusek et al., 2020)을 사용한다.\n' +
      '\n' +
      '**Model Configuration.** Phi(1.3B)(Li et al., 2023), Openllama(7B)(Touvron et al., 2023), OPT(1.3B, 6.7B)(Zhang et al., 2022). 본 논문에서 제안한 방법을 표준 draft-target speculative decoding ((Leviathan et al., 2023)) 및 single-model speculative decoding framework, Medusa (Cai et al., 2023)와 비교한다. 표준 초안 목표 접근법의 경우 사용 가능한 오픈 소스 OPT 모델의 가장 작은 구성인 OPT-125m을 초안 모델로 사용한다.\n' +
      '\n' +
      '**베이스라인** 메두사(Cai et al., 2023) 스타일 접근법과 비교하기 위해, 랭크 32 및 메두사 헤드의 LoRA 어댑터(Hu et al., 2022)를 베이스라인으로 하고, 동일한 베이스 모델, 스트림 임베딩 및 LoRA 어댑터를 타겟으로 하는 투기적 스트리밍을 사용하는 사전 훈련된 베이스 모델을 사용한다. 메두사 헤드는 (Cai et al., 2023)에 기술된 레시피에 따라 트레이닝된다. 메두사 헤드 및 최대 스트림 수는 모두 4로 고정되고 메두사에서 사용되는 헤드당 잔차 블록은 1로 설정된다. 표준 드래프트-타겟 추측 디코딩(Leviathan et al., 2023)과의 비교를 위해, 우리는 상이한 구성 및 크기를 갖기 때문에 OPT 모델을 사용한다. OPT-125m은 초안 모델로 전개되고 OPT-1.3b 및 OPT-6.7b는 일반적으로 10-100X의 비율이 최적의 것으로 간주되기 때문에 목표 모델로 사용된다. 타겟 모델과 유사하게, 각각의 다운스트림 애플리케이션에서 전체 드래프트 모델을 미세 조정하는 것은 온-디바이스 설정에서 실용적이지 않기 때문에, 드래프트 모델의 LoRA 어댑터만이 다운스트림 애플리케이션에서 미세 조정된다. 또한 LoRA fine-tuning은 full-model fine-tuning(Hu et al., 2022)에 On-par 성능을 달성하는 경향이 있다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '#### 4.1.1 Overview\n' +
      '\n' +
      '단일 Nvidia A100-80G GPU에서 배치 크기 1을 사용하여 테스트 분할에 대한 벽 시간 속도 향상 및 생성 품질 메트릭을 보고한다. 추론은 그리디 샘플링과 \\(T=0\\)을 사용하여 플로트16에서 수행된다. 자세한 실험 내용은 부록 A.2, top-k 표본 추출 및 \\(T=1\\)에 대한 절제는 부록 B를 참조하시기 바랍니다. 구조화된 질의 태스크는 Exact Match (EM) 정확도 메트릭을 사용하고, 대화 요약 및 의미 표현 태스크는 Rouge1/RougeLSum 메트릭을 사용한다. 구조화 질의 작업은 \\(N_{s}/N\\)의 \\(N_{s}/N\\)을, 요약 및 의미 표현 작업은 \\(1/2\\)의 \\(N_{s}/N\\)을 사용한다. 생성 메트릭이 기준선과 동등하다는 것을 보장하기 위해 (N_{s}\\)이 선택된다. 생성 메트릭에 대한 \\(N_{s}\\)의 영향에 대한 자세한 내용은 섹션 4.2에서 찾을 수 있다.\n' +
      '\n' +
      '표 1은 표준 자동 회귀 디코딩 기준선인 메두사와 우리의 접근법 간의 비교를 속도 향상, 호출 감소 비율 및 추가 매개변수의 수로 나타낸다. 우리는 다양한 다운스트림 작업에서 투기 스트리밍의 벽 시간 속도 및 호출 감소 비율이 메두사보다 일관되게 동일/높으면서 훨씬 적은 매개변수를 발생시킨다는 것을 발견했다.\n' +
      '\n' +
      '오버헤드. 또한, 표 2에 요약된 바와 같이, 우리의 접근법은 두 접근법 사이의 타겟 호출 수의 차이가 자동 회귀 드래프팅 오버헤드를 상쇄하기에 충분히 크지 않기 때문에 표준 드래프트-타겟 추측 디코딩보다 더 나은 벽-시간 레이턴시를 달성한다. 모든 월-타임 레이턴시는 (Wolf et al., 2019)에서 이용 가능한 모델의 오픈 소스 버전을 사용하여 보고되며, 효율적인 추론 기술(Nvidia, 2024) 또는 양자화(int4/8)를 사용하여 드래프트 및 타겟 모델을 추가로 최적화하는 것이 더 낮은 레이턴시로 이어질 수 있다. 마지막으로, 이 방법의 생성 메트릭은 LoRA 미세 조정 기반 모델과 일관되게 유사하므로 다음 토큰 예측 기반 미세 조정에 대한 탁월한 대안이 된다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '4.1.2 분석과 통찰력\n' +
      '\n' +
      '보조 모델 없이 메두사 헤드는 마지막 레이어의 공유 은닉 상태와 독립적으로 각 토큰을 생성하고, 메두사 헤드가 예측한 투기 토큰 간의 종속성, 시간 단계 \\(t\\)에서 기본 모델에 의해 예측한 \\(y_{(t+1..t+\\gamma)}\\) 및 다음 토큰 \\(y_{t}\\)은 주의 메커니즘이 없기 때문에 잘 포착되지 않을 수 있다. 반면, 투기성 스트림은 메인 스트림과 서로 참석하여 토큰 의존성을 포착하여 메두사보다 더 나은 호출 감소 비율을 초래한다. 매개변수 측면에서 각 메두사 머리에는 \\(h^{2}+hv\\) 매개 변수가 추가되는데, 여기서 \\(h\\)은 숨겨진 크기이고 \\(v\\)은 어휘 크기이다. 또한 Medusa head의 수는 \\(\\gamma\\)으로 선형적으로 확장되며, 이는 \\(\\gamma\\)에 따라 매개변수 오버헤드가 선형적으로 증가한다. 반면, 추측 스트리밍은 \\(\\gamma\\)으로 확장되지 않는 추측 어댑터를 사용한다. 스트림 식별자 임베딩은 \\(\\gamma\\)의 스케일을 갖지만, 각 임베딩과 관련된 매개변수 오버헤드는 \\(h\\)에 선형적이다. 또한 미세 조정 설정에서 "추측 어댑터" 매개변수는 기본 모델 어댑터와 공유되므로 본 접근법과 관련된 매개변수 오버헤드는 \\(\\gamma h\\)에 불과하다.\n' +
      '\n' +
      '**보조 모델** 투기 스트리밍 일관성 있음\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|l c c c c} \\hline \\hline Dataset & Model & Method & SpeedUp (\\(\\uparrow\\)) & CR Ratio (\\(\\uparrow\\)) & Metric (\\(\\uparrow\\)) & \\# Extra Parameters (\\(\\downarrow\\)) \\\\ \\hline \\multirow{8}{*}{SqlContext} & \\multirow{2}{*}{OPT-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(84.98\\) & \\(-\\) \\\\  & & Medusa & \\(2.07\\) & \\(2.79\\) & \\(84.98\\) & \\(4.28E8\\) \\\\  & & SS (ours) & \\(\\mathbf{2.39}\\) & \\(\\mathbf{3.57}\\) & \\(\\mathbf{87.40}\\) & \\(\\underline{4.096E4}\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{PHI-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(88.71\\) & \\(-\\) \\\\  & & Medusa & \\(2.58\\) & \\(3.25\\) & \\(88.71\\) & \\(4.36E8\\) \\\\  & & SS (ours) & \\(\\mathbf{2.62}\\) & \\(\\mathbf{3.53}\\) & \\(\\mathbf{89.90}\\) & \\(\\underline{4.096E4}\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{OpenLlama-7b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(89.88\\) & \\(-\\) \\\\  & & Medusa & \\(\\mathbf{3.20}\\) & \\(4.10\\) & \\(90.11\\) & \\(5.91E8\\) \\\\  & & SS (ours) & \\(3.14\\) & \\(\\mathbf{4.13}\\) & \\(\\mathbf{91.70}\\) & \\(\\underline{8.19E4}\\) \\\\ \\hline \\multirow{8}{*}{DialogSum} & \\multirow{2}{*}{OPT-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(43.40/35.56\\) & \\(-\\) \\\\  & & Medusa & \\(1.56\\) & \\(1.91\\) & \\(43.40/35.50\\) & \\(4.28E8\\) \\\\  & & SS (ours) & \\(\\mathbf{1.94}\\) & \\(\\mathbf{2.62}\\) & \\(\\mathbf{44.07/35.99}\\) & \\(\\underline{4.096E4}\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{PHI-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(\\mathbf{43.57/35.60}\\) & \\(-\\) \\\\  & & Medusa & \\(\\mathbf{1.89}\\) & \\(2.28\\) & \\(\\mathbf{43.57/35.60}\\) & \\(4.36E8\\) \\\\  & & SS (ours) & \\(1.83\\) & \\(\\mathbf{2.34}\\) & \\(43.36/35.31\\) & \\(4.096E4\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{OpenLlama-7b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(\\mathbf{44.20/36.50}\\) & \\(-\\) \\\\  & & Medusa & \\(1.76\\) & \\(2.25\\) & \\(\\mathbf{44.20/36.50}\\) & \\(5.91E8\\) \\\\  & & SS (ours) & \\(\\mathbf{1.87}\\) & \\(2.51\\) & \\(43.92/35.70\\) & \\(8.19E4\\) \\\\ \\hline \\multirow{8}{*}{E2E} & \\multirow{4}{*}{OPT-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(\\mathbf{69.48/50.17}\\) & \\(-\\) \\\\  & & Medusa & \\(2.13\\) & \\(2.95\\) & \\(\\mathbf{69.48/50.17}\\) & \\(4.28E8\\) \\\\  & & SS (ours) & \\(\\mathbf{2.45}\\) & \\(\\mathbf{3.72}\\) & \\(69.32/\\mathbf{50.51}\\) & \\(4.096E4\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{PHI-1.3b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(\\mathbf{67.90/48.50}\\) & \\(-\\) \\\\  & & SS (ours) & \\(\\mathbf{2.84}\\) & \\(\\mathbf{3.69}\\) & \\(67.40/\\mathbf{48.52}\\) & \\(4.096E4\\) \\\\ \\cline{2-7}  & \\multirow{4}{*}{OpenLlama-7b} & Baseline & \\(1.00\\) & \\(1.00\\) & \\(\\mathbf{69.50/50.30}\\) & \\(-\\) \\\\  & & Medusa & \\(2.70\\) & \\(3.22\\) & \\(\\mathbf{69.50/50.30}\\) & \\(5.91E8\\) \\\\ \\cline{2-7}  & & SS (ours) & \\(\\mathbf{2.96}\\) & \\(\\mathbf{3.55}\\) & \\(68.66/49.56\\) & \\(\\underline{8.19E4}\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다운스트림 애플리케이션에서 미세 조정된 다른 모델을 사용하는 월타임 속도 향상, CR 비율, 매개변수 오버헤드 및 미터법 비교. CR 비율은 가속 불가지론 목표 모델 호출 감소 비율을 나타낸다. 우리는 정확한 일치 정확도를 SqlContext에 대한 메트릭으로 사용하고, Rougel1/RougeLSum을 Dialogum 및 E2E-NLG 작업에 대한 메트릭으로 사용한다.\n' +
      '\n' +
      '표 2에 묘사된 바와 같이, 드래프트-타겟 추측 디코딩의 타겟 모델 호출은 표준 드래프트-타겟 추측 디코딩보다 낮은 벽 시간 지연을 달성한다. 주목할 점은, 드래프트-타겟 추측 디코딩의 타겟 모델 호출은 추측 스트리밍보다 낮지만, 추측 드래프트를 생성하기 위해 자동 회귀적으로 실행시키는 드래프트 모델 \\(\\gamma\\) 시간의 비용을 수반한다는 것이다. 반면에, 투기적 스트리밍을 사용한 드래프트 생성은 트리 드래프트 크기가 증가하더라도 대상 모델 디코딩이 메모리 바인딩되는 경향이 있기 때문에 추가 지연 오버헤드가 거의 발생하지 않는다. 이는 그림 3과 같이 커널 활용도와 산술 집약도가 증가한 것으로 해석된다. 반면 초안 기반 접근법은 자동 회귀 초안의 메모리 바인딩 특성으로 인해 커널 활용도가 낮다.\n' +
      '\n' +
      '드래프팅 비용이 적게 들기 때문에 더 작은 드래프트 모델이 더 잘 수행될 수 있지만 수락률이 떨어지고 드래프트 모델 크기가 감소할 수 있다는 주장이 제기될 수 있다. 표준 드래프트-타겟 추측 디코딩과의 비교를 정형화하기 위해, 다음의 분석을 수행한다. 예를 들어, \\(C_{draft}\\)은 드래프트 모델을 통한 순방향 패스와 관련된 레이턴시 비용이고, \\(C_{target}\\)은 타겟 모델을 통한 순방향 패스와 관련된 비용이고, \\(C_{ss}\\)은 투기 스트리밍 순방향 패스와 관련된 비용이다. \\(C_{ss}\\)은 투기 스트리밍 순방향 패스와 관련된 비용이다. (\\zeta\\)는 드래프트-타겟 접근법에 대한 검증 단계에서 진보된 디코딩 토큰의 수이고, \\(\\beta\\)는 투기적 스트리밍에서 진보된 토큰의 수이다. 우리는 두 접근 방식을 비교하기 위해 단일 토큰 진출과 관련된 지연 비용을 동일시한다.\n' +
      '\n' +
      '\\[(\\gamma*C_{draft}+C_{target})/\\zeta=C_{ss}/\\beta\\tag{6}\\] \\[(\\gamma+C_{target}/C_{draft})/\\zeta=(C_{ss}/C_{draft})/\\beta\\\n' +
      '\n' +
      '\\(\\gamma=4,C_{target}/C_{draft}=10\\) 및 \\(C_{ss\\approx C_{target}\\), \\(\\zeta=1.4\\beta\\)를 가정하면, 베리당 발전량을 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c c c c c} \\hline Dataset & Target & Method & Target calls & Draft Calls & Walltime Latency (\\(ms\\), \\(\\downarrow\\)) & Metric (\\(\\uparrow\\)) \\\\ \\hline \\multirow{4}{*}{SqlContext} & OPT-1.3b & Two-model SD & \\(6.59\\) & \\(22.35\\) & \\(269.24\\) & \\(84.98\\) \\\\  & & SS (ours) & \\(7.79\\) & \\(0\\) & \\(\\mathbf{133.48}\\) & \\(\\mathbf{87.40}\\) \\\\ \\cline{2-7}  & OPT-6.7b & Two-model SD & \\(6.60\\) & \\(22.41\\) & \\(301.10\\) & \\(89.13\\) \\\\  & & SS (ours) & \\(6.88\\) & \\(0\\) & \\(\\mathbf{157.04}\\) & \\(\\mathbf{89.34}\\) \\\\ \\hline \\multirow{4}{*}{Dialogsum} & OPT-1.3b & Two-model SD & \\(11.65\\) & \\(42.59\\) & \\(493.59\\) & \\(43.40/35.60\\) \\\\  & & SS (ours) & \\(13.41\\) & \\(0\\) & \\(\\mathbf{248.26}\\) & \\(\\mathbf{44.07/35.99}\\) \\\\ \\cline{2-7}  & OPT-6.7b & Two-model SD & \\(12.15\\) & \\(35.76\\) & \\(555.99\\) & \\(\\mathbf{44.40/36.60}\\) \\\\  & & SS (ours) & \\(14.39\\) & \\(0\\) & \\(\\mathbf{442.83}\\) & \\(44.30/36.30\\) \\\\ \\hline \\multirow{4}{*}{E2E-NLG} & OPT-1.3b & Two-model SD & \\(8.86\\) & \\(31.47\\) & \\(345.72\\) & \\(\\mathbf{69.48/50.17}\\) \\\\  & SS (ours) & \\(9.80\\) & \\(0\\) & \\(\\mathbf{164.23}\\) & \\(69.32/\\mathbf{50.51}\\) \\\\ \\cline{1-1} \\cline{2-7}  & OPT-6.7b & Two-model SD & \\(8.90\\) & \\(31.58\\) & \\(412.02\\) & \\(\\mathbf{69.34/49.88}\\) \\\\ \\cline{1-1} \\cline{2-7}  & & SS (ours) & \\(10.26\\) & \\(0\\) & \\(\\mathbf{243.62}\\) & \\(69.07/49.69\\) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: \\(\\gamma=4\\)에 대한 드래프트 모델로서 OPT-125m을 사용하는 표준 드래프트-타겟 기반 추측 디코딩 접근법과의 월타임 레이턴시(샘플당) 비교. 본 논문에서 제안한 방법을 이용한 표적 모델 호출은 초안 모델 기반 추론 디코딩보다 높지만, 자동 회귀 초안 오버헤드가 발생하지 않아 OPT-1.3b 및 OPT-6.7b 모델에서 더 나은 대기 시간을 얻을 수 있다. 우리는 정확한 일치 정확도를 SqlContext에 대한 메트릭으로 사용하는 반면, Rouge1/RougeLSum은 Dialogum 및 E2E-NLG 작업에 대한 메트릭으로 사용된다.\n' +
      '\n' +
      '그림 4: 다른 \\(\\zeta/\\beta\\) 및 타겟/드래프트 레이턴시 비율에 대한 드래프트 기반 투기적 디코딩에 대한 투기적 스트리밍 속도 향상, 여기서 \\(\\zeta\\)은 드래프트 기반 투기적 디코딩에 대한 검증 단계당 진보 횟수를 나타내고, \\(\\beta\\)은 투기적 스트리밍에 대해 동일한 것을 나타낸다.\n' +
      '\n' +
      '도 3: 투기적 스트리밍은 메모리 바인딩된 자동-회귀 디코딩 단계의 산술 강도를 증가시킴으로써 디코딩을 가속화시킨다. 메두사식 접근법과 초안 모델(OPT-125m) 기반 추측 디코딩 접근법에 의한 OPT-1.3b 모델의 커널 및 메모리 활용도 비교를 위해 보인다.\n' +
      '\n' +
      '표준 초안 목표 접근법의 정의 단계는 벽 시간 대기 시간 패리티를 달성하기 위해 1.4X의 투기 스트리밍이어야 한다. 이 분석은 캐시 조정 오버헤드와 신속한 처리 오버헤드를 무시하지만 드래프트 대상과 투기적 스트리밍 접근법 사이의 선택을 안내하는 귀중한 직관을 제공한다는 점에 유의한다. 또한 표준 초안 목표 접근법과 비교하여 어떤 설정에서 투기 스트리밍이 더 많은 이점을 제공할 가능성이 있는지 분석한다. 도. 도 4는 상이한 타겟 대 드래프트 레이턴시 비율에 대한 드래프트-타겟 기반 접근법에 대한 투기적 스트리밍의 이론적 속도를 도시한다. 지연률이 증가함에 따라 초안-목표 접근법은 \\(\\zeta/\\beta>1\\)일 때 더 많은 속도 향상 이점을 제공할 가능성이 있으며, 이는 초안 모델이 투기적 스트리밍보다 목표 모델 검증 단계당 더 많은 토큰 발전을 달성할 수 있을 만큼 정확하고 더 높은 지연률을 산출할 만큼 작을 때 더 많은 이점을 제공할 가능성이 있음을 의미한다. 이러한 모델을 찾거나 만들려면 일반적으로 상당한 엔지니어링 노력이 필요합니다. 다운스트림 응용 환경에서 이상적인 초안 모델을 찾는 것은 응용 프로그램에 따라 달라지는 경향이 있기 때문에 훨씬 더 어려워진다. 응용 프로그램이 초안 모델을 공유하고 트레인 어댑터만 공유하는 경우 초안 모델은 목표 대 초안 대기 시간 비율을 충족할 만큼 충분히 작지 않아 투기 스트리밍보다 더 많은 속도를 달성하기 어려울 수 있다.\n' +
      '\n' +
      '### Ablations\n' +
      '\n' +
      '**투기 드래프트 크기.** 트리 드래프트의 수락률을 향상시키기 위해 투기 포지션 수 \\(\\gamma\\), 투기 포지션당 표본 토큰 수 \\(k\\)의 다양한 설정을 시도한다. 그림 5는 \\(\\gamma=3\\)에 대한 벽시간 속도를 보여준다. 각 투기적 위치에서 더 많은 토큰을 샘플링할 때, 전진 패스당 승진은 검증에 사용할 수 있는 후보가 많아짐에 따라 \\(\\beta\\)이 증가하여 속도가 빨라진다. 그러나, 우리가 \\(k\\)을 계속 증가시킴에 따라, 모델이 컴퓨트-바운드 페이즈로 전환되고 스피드 업이 코스를 역전시킴에 따라 순방향 패스 레이턴시 오버헤드가 더 널리 퍼지게 된다. 3.2.3에 기술된 바와 같이 순진하게 트리 드래프트를 형성하는 것은 \\(k\\)으로 배치 크기가 기하급수적으로 증가하기 때문이다. 우리는 가능성이 낮은 경로를 제거하고 트리 드래프트의 크기를 줄이기 위해 트리 가지치기 층을 삽입한다. 가지치기 트리 드래프트는 순방향 패스 레이턴시를 감소시키고, 잘 보정된 임계치는 트리 내의 잡음 경로들만이 가지치기되는 것을 보장한다. 나무 가지치기는 그림 5와 같이 \\(k\\)이 계속 증가함에 따라 벽 시간 속도 향상에 도움이 되는 경향이 있다.\n' +
      '\n' +
      '**MSA 레이어의 수** 다운스트림 생성 메트릭, 트레이닝 시간 및 FLOP의 증가 측면에서 통합할 MSA 레이어의 수를 결정하는 데 관여하는 트레이드오프가 있다. 우리가 MSA 계층의 수를 증가시킴에 따라, 생성 메트릭이 개선되고 이러한 경향은 상이한 다운스트림 태스크들에 걸쳐 동일하게 유지된다. 일반적으로 상위 2-8개 레이어에 MSA를 통합하면 메트릭, FLOP 증가 및 훈련 시간 사이의 좋은 트레이드오프가 제공된다. [그림 6]은 Structured Query와 Summarization 태스크에 대한 OPT-1.3b 모델의 생성 성능을 보여준다.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      '본 논문에서는 대용량 언어 모델의 복호화를 가속화하는 기법인 Speculative Streaming을 제안한다. 표준 추측 디코딩 접근법과 비교하여, 추측 스트리밍은 보조 "초안" 모델의 필요성을 제거한다. 대신 여러 투기 스트림을 단일 "표적" 모델에 효율적으로 융합하여 추측과 검증을 통합한다. 추측 스트리밍은 미세 조정 프로세스를 단순화하고 이전 접근법에 비해 온-파 또는 더 나은 속도 향상 및 품질을 달성합니다. 또한 파라미터 효율적이며 두 가지 모델을 메모리에 로드할 필요성을 제거하여 자원 제약 시나리오에 적합한 접근 방식이다.\n' +
      '\n' +
      '그림 5: 트리 드래프트 생성을 위해 고정된 \\(\\gamma\\)을 유지하는 각 스트림에서 더 많은 토큰(\\(k\\))이 샘플링됨에 따라 후보의 수 증가로 인해 월타임 속도가 증가한다. 이 추세는 \\(k\\)이 계속 증가하고 모델이 계산 결합 단계로 전환됨에 따라 역전된다. 트리 드래프트로부터 더 적은 확률의 경로를 프런닝하는 것은 더 높은 값 \\(k\\)에 대한 계산을 감소시킴으로써 순방향 패스당 대기 시간을 감소시키고 더 많은 속도를 제공하는 데 도움이 된다.\n' +
      '\n' +
      '도 6: 멀티-스트림 주의 계층들의 수가 증가함에 따라, 다운스트림 태스크들에 대한 메트릭들 또한 개선된다. Dialogum 태스크의 메트릭으로는 RougelSum을 사용하고 ContextSQL 태스크의 메트릭으로는 Exact Match (EM) 정확도를 사용한다.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      '사친 메타, 모인 나비, 안토니 린, 민식 조, 아르살란 파루크, 제이슨 윌리엄스의 소중한 피드백과 토론에 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Agarwal et al. (2023) Agarwal, R., Vieillard, N., Stanczyk, P., Ramos, S., Geist, M., and Bachem, O. Gkd: 자동 회귀 시퀀스 모델에 대한 일반화된 지식 증류__ arXiv preprint arXiv:2306.13649_, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models is few-shot learners. _ 신경 정보 처리 시스템_, 33:1877-1901, 2020의 발전.\n' +
      '* Burton(1985) Burton, F. W. Speculative computation, parallelism and functional programming. _ IEEE Transactions on Computers_, 100(12):1190-1193, 1985.\n' +
      '* Cai et al. (2023) Cai, T., Li, Y., Geng, Z., Peng, H., and Dao, T. Medusa: 다중 디코딩 헤드를 갖는 llm 생성을 가속화하기 위한 간단한 프레임워크. [https://github.com/FasterDecoding/Medusa] (https://github.com/FasterDecoding/Medusa), 2023.\n' +
      '* Chen et al. (2023) Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculation sampling. _ arXiv preprint arXiv:2302.01318_, 2023.\n' +
      '* Chen et al. (2021) Chen, Y., Liu, Y., Chen, L., and Zhang, Y. 대화 합: 실제 시나리오 대화 요약 데이터 세트입니다. Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pp. 5062-5074, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.449. URL[https://aclanthology.org/2021.findings-acl.449](https://aclanthology.org/2021.findings-acl.449).\n' +
      '* Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* Dettmers et al. (2023) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuzmedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr: A sparse-quantized representation for near-lossless llm weight compression. _ arXiv preprint arXiv:2306.03078_, 2023.\n' +
      '* Dusek et al. (2020) Dusek, O., Novikova, J., and Rieser, V. End-to-End 자연언어 생성의 최신 기술 평가: E2E NLG 챌린지. _ Computer Speech & Language_, 59:123-156, January 2020. doi: 10.1016/j.csl.2019.06.009.\n' +
      '* Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsept: Massive 언어 모델은 원샷으로 정확하게 가지치기될 수 있다. In _International Conference on Machine Learning_, pp. 10323-10337. PMLR, 2023.\n' +
      '* Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre-trained transformer. _ ArXiv:2210.17323_, 2022.\n' +
      '* Fu et al. (2023) Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Breaking the sequential dependency of llm inference using lookahead decoding, November 2023. URL[https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)\n' +
      '* Gu et al. (2023) Gu, Y., Dong, L., Wei, F., and Huang, M. 대형 언어 모델의 지식 증류. _ arXiv preprint arXiv:2306.08543_, 2023.\n' +
      '* Hu et al. (2022) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: 대형 언어 모델의 낮은 순위 적응. _International Conference on Learning Representations_, 2022. URL[https://openreview.net/forum?id=n2eVKeeFYf9](https://openreview.net/forum?id=n2eVKeeFYf9)이다.\n' +
      '* Kaplan 등 (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Leviathan et al. (2023) Leviathan, Y., Kalman, M., and Matias, Y. 추측 디코딩을 통한 변압기로부터의 빠른 추론. In _International Conference on Machine Learning_, pp. 19274-19286. PMLR, 2023.\n' +
      '* Li et al. (2023) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: **phi-1.5** technical report. _ arXiv preprint arXiv:2309.05463_, 2023.\n' +
      '* Miao et al. (2023) Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y. Y., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: 추론 및 토큰 트리 검증 기능을 제공하는 생성 llm 가속화. _ arXiv preprint arXiv:2305.09781_, 2023.\n' +
      '* Nvidia(2024) Nvidia. Fastertransformer, 2024. URL[https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n' +
      '* Pal et al. (2023) Pal, K., Sun, J., Yuan, A., Wallace, B. C., and Bau, D. Future lens: Anticipating subsequent tokens from a single hidden state. _ arXiv preprint arXiv:2311.04897_, 2023.\n' +
      '\n' +
      '* Pope et al. (2023) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J., Efficiently scaling transformer inference. _ Proceedings of Machine Learning and Systems_, 5, 2023.\n' +
      '* Qi et al. (2020) Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J., Zhang, R., and Zhou, M. Prophetnet: sequence-to-sequence pre-training을 위한 미래 n-gram 예측 _ arXiv preprint arXiv:2001.04063_, 2020.\n' +
      '* Spector & Re(2023) Spector, B. and Re, C. Accelerated llm inference with stage speculative decoding. _ arXiv preprint arXiv:2308.04623_, 2023.\n' +
      '* Stern et al. (2018) Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models. _ 신경 정보 처리 시스템_, 31, 2018의 발전.\n' +
      '* Sun et al. (2023a) Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. _ arXiv preprint arXiv:2306.11695_, 2023a.\n' +
      '* Sun et al. (2023b) Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H., and Yu, F. Spectr: Fast speculative decoding via optimal transport. _ arXiv preprint arXiv:2310.15141_, 2023b.\n' +
      '* Thoppilan et al. (2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_, 2022.\n' +
      '* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '*Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention all you need. _ 신경 정보 처리 시스템_, 30, 2017의 발전.\n' +
      '* Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface\'s transformerers: State-of-the-art natural language processing. _ ArXiv preprint arXiv:1910.03771_, 2019.\n' +
      '* Yang et al. (2019) Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V. Xlnet: Generalized autoregressive pretraining for language understanding. _ 신경 정보 처리 시스템_, 32, 2019의 발전.\n' +
      '* Yao et al. (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y. 제로양자: 대규모 변압기를 위한 효율적이고 저렴한 훈련 후 양자화 신경 정보 처리 시스템_, 35:27168-27183, 2022에서의 발전.\n' +
      '* Yu et al. (2018) Yu, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., et al. 스파이더: 복잡하고 교차 도메인 시맨틱 파싱 및 텍스트 투-sql 태스크를 위한 대규모 인간-레이블 데이터세트. _ arXiv preprint arXiv:1809.08887_, 2018.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_, 2022.\n' +
      '* Zhong et al.(2017) Zhong, V., Xiong, C., and Socher, R. Seq2sql: 강화학습을 이용하여 자연어로부터 구조화된 질의를 생성하는 단계; _ CoRR_, abs/1709.00103, 2017.\n' +
      '* Zhou et al. (2023) Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Rostamizadeh, A., Kumar, S., Kagg, J.-F., and Agarwal, R. Distillpec: 지식 증류를 통한 추측 디코딩 개선 arXiv preprint arXiv:2310.08461_, 2023.\n' +
      '\n' +
      '## 부록 구현 세부사항\n' +
      '\n' +
      '#트리 드래프트 관리\n' +
      '\n' +
      '이 섹션에서는 트리 드래프트 샘플링, 평탄화 및 가지치기에 대해 더 자세히 설명한다. 본 논문에서 보여지는 바와 같이, 프롬프트 \\((x_{1}...x_{t})\\)를 처리할 때, 마지막 토큰과 함께 투기 스트림을 삽입하여 로짓들을 생성하고, 주 스트림에 해당하는 \\(z_{t}\\)과 투기 스트림에 해당하는 \\((z_{t1}...z_{t\\gamma})\\)을 생성한다. 트리 드래프트는 섹션 3.2.2에 설명된 절차에 따라 샘플링된다. 샘플링된 드래프트는 이어서 시퀀스 길이 차원을 따라 평탄화되며, 어텐션 마스크는 그림 7 및 그림 8에 도시된 바와 같이 자식 노드들이 루트로 시작하여 그들의 전임자들을 돌보도록 구성된다. 트리 드래프트의 루트 토큰은 메인 스트림에 의해 발행된 정정이다. 신속한 처리 후 각 반복은 이전 트리 드래프트를 확인하고 새 트리 드래프트를 샘플링하는 것을 포함한다. 트리 드래프트를 \\(N-N_{s}\\) 계층에 통과시킨 후, 부모 토큰과 자식 토큰 사이의 전이 확률을 근사화하기 위해 중간 계층에 의해 학습된 문맥 특징을 사용한다. 도 7에 도시된 바와 같이, 토큰 \\(`parameter^{\\prime\\prime}\\)과 \\(`compare^{\\prime\\prime}\\) 사이의 전이 확률이 설정된 임계값보다 작기 때문에, 특징 영역의 \\(`compare"\\)에서 시작하는 서브 트리를 프루닝하고, \\(m_{2},m_{5},m_{6}\\)을 프루닝한다. 프루닝 레이어 이전의 레이어 \\(0..(N-N_{s}-1)\\)의 키 값 캐시는 프루닝 레이턴시 오버헤드를 최소로 유지하기 위해 이 시점에서 트리밍되지 않는다는 점에 유의하십시오. 키 값 캐시 역추적은 각 생성 단계 후에 게으르게 수행됩니다. 추측 스트림은 가지치기된 드래프트의 각 노드와 함께 삽입됩니다. 층 \\((N-N_{s}...N)\\)은 수학식 2에 기재된 바와 같이 멀티스트림 어텐션을 이용한다. 검증 절차는 메인 스트림이 수용할 수 있는 프루닝된 트리에서 가장 긴 매칭 경로를 찾는다. 도 7에 도시된 바와 같이, 경로\\((`parameter^{\\prime\\prime}\\), \\(`efficient^{\\prime\\prime},`speculative^{\\prime\\prime})\\)이 수용된다. 마지막으로 수락된 토큰에 해당하는 주 스트림의 로짓으로부터 샘플링된 수정 토큰 \\(m_{1}\\)은 새로운 루트가 되는 반면 스트림의 로짓으로부터 샘플링된 토큰 \\((s_{10},s_{11}))은 서브 트리를 형성한다.\n' +
      '\n' +
      '그림 7: 병렬 트리 초안 추측 및 검증: 이전 반복으로부터의 트리 초안은 검증을 위해 평탄화된다. [\\(N-N_{s}\\) MHA 계층 후, 트리 프루닝 절차는 부모 토큰과 자식 토큰 사이의 전이 확률에 기초하여 덜 가능성 있는 토큰을 제거한다. 이 그림에서 \\(Z_{i}\\)은 인덱스 \\(i\\), \\(m_{i}\\)에서 주 스트림에 대응하는 정규화된 초기 출구 로짓들을 나타내고, \\(Z_{ij}\\)은 평탄화된 트리 드래프트에서 인덱스 \\(i\\) 및 \\(j\\)에서 토큰 사이의 전이 확률을 나타낸다. 검증 절차는 프루닝된 트리 상에서 후속적으로 실행되고, 추측성 토큰들은 최신 수락된 토큰에 대응하는 스트림들로부터 샘플링된다. 위의 예시에서, \\(m_{1}\\), \\(s_{10}\\) 및 \\(s_{11}\\)의 스트림으로부터 \\(`speculative^{\\prime\\prime}\\), \\(Fine,decoding^{\\prime\\prime}\\) 및 \\(`looking,tuning^{\\prime\\prime}\\)을 샘플링한다.\n' +
      '\n' +
      '실험 설정 세부사항\n' +
      '\n' +
      '섹션 4의 논문에 설명된 기준선 및 목표 접근법 모두에 대한 미세 조정 절차는 5개의 에폭에 대한 LoRa 어댑터를 훈련하는 것을 포함한다. 기본 방법과 목표 방법 모두에서 다음 토큰 예측 손실에 대한 추측 손실을 측정하기 위해 \\(\\alpha_{0}=1\\)과 \\(\\alpha_{j}=0.1\\)을 \\(j=1...\\gamma\\)으로 설정하였다. 우리는 식 (3)에 설명된 대로 주 스트림에서 투기 스트림을 초기화하기 위해 서로 다른 순위의 선형 변환을 실험했지만, 단순히 동일성 변환을 사용하면 훨씬 적은 매개변수 오버헤드로 유사한 성능을 달성한다는 것을 발견했다. 섹션 4에 기술된 모든 실험을 위해 아이덴티티 변환을 사용한다. 우리는 메두사와 우리의 접근법에 대해 서로 다른 \\(\\gamma\\) 및 \\(k\\) 값에 대해 최상의 결과를 보고하고, 표준 드래프트-타겟 추측 디코딩 접근법에 대해 \\(k\\)은 1로 고정된다. 또한, 가속기(_e.g. 모바일 장치 신경 엔진 _vs_. Nvidia A100)에 따라 검증 및 드래프트 구성 절차의 지연 시간, 검증 및 드래프트 구성 절차가 크게 달라짐에 따라 무시할 수 있는 검증 및 드래프트 구성 오버헤드를 가정한 가속기 불가지론 속도 증가(호출 감소 비율)를 보고한다. 호출 감소 비율 메트릭은 달성 가능한 속도 향상을 위한 루프 라인 역할을 하는 경향이 있다. 마지막으로 투기성 초안의 검증을 위해 "하드" 매칭 기준을 사용한다. 이러한 기준을 "소프트" 매칭으로 완화시키는 것은 더 높은 스피드 업을 산출할 수 있다(Cai et al., 2023).\n' +
      '\n' +
      '그림 8: 트리 드래프트에 대한 주의 마스크는 자식 토큰이 루트로부터 시작하여 모든 전임자에게 참석할 수 있는 방식으로 구성되며, 루트는 본 스트림에 의해 발행되는 수정이다. 이 그림에서 "\\(초기\\)"는 트리에서 하나의 경로를 형성하기 때문에 "\\(매개변수\\)" 및 "\\(계수\\)" 및 "\\(계수\\)" 자체에 주목한다. “\\(early\\)”는 또한 다른 경로 “\\(parameter-compare-early\\)”를 형성하기 위해 복제된다. 이 어텐션 마스크는 후보의 수가 증가함에 따라 다중 경로를 배치하며 합격률을 높일 수 있다.\n' +
      '\n' +
      '그림 9: (a) 서로 다른 수의 MSA 레이어에 대한 기본 모델로 PHI-1.3b를 사용하여 다이얼로그 요약 작업의 루지LSum 점수에 대한 값 투영 회전의 영향을 분석한다. 각 스트림은 메인 스트림으로부터의 거리에 비례하여 회전된다. (b) 본 연구에서는 OPT-1.3b를 기본 모델로 하는 메두사 스타일 접근법과 사행 스트리밍(Speculative Streaming, SS)에 대한 탑-k 샘플링이 벽 시간 속도 향상 및 통화 감소 비율에 미치는 영향을 의미 표현 작업에 대한 기본 모델로 연구한다.\n' +
      '\n' +
      '## 부록 B 절제:\n' +
      '\n' +
      '**값 회전** 추측 스트림과 주 스트림의 계산이 다른 더 많은 방법을 분석했다. 전용 스트림 임베딩을 사용하는 것과는 별도로, 상대 위치 감각을 통합하면서 계산을 구별하는 한 가지 방법은 단순히 서로에 대해 스트림을 회전시키는 것이다. 이 제거에서 각 스트림을 주 스트림 숨김 상태로 초기화하고 주 스트림과의 상대 거리의 비율로 주의 계산 동안 값 투영을 회전한다:\n' +
      '\n' +
      '[V_{tn}^{k}=V_{t}^{k}e^{i\\epsilon n}\\tag{7}\\]\n' +
      '\n' +
      '여기서 \\(1<=n<=\\gamma\\)은 스트림 인덱스이고, \\(V_{t}^{k}\\)은 시간 단계 \\(t\\) 및 층 \\(k\\)에서 주 스트림의 값 투영을 나타내는 반면, \\(V_{tn}^{k}\\)은 스트림 n의 값 투영을 나타내고, \\(0\\leq\\epsilon\\leq\\frac{\\pi}{2N}\\)은 임의의 회전 단계를 나타내며, \\(N\\)은 최대 시퀀스 길이와 스트림 수의 합을 나타낸다. 그림 9(a)는 Phi-1.3b 모형으로 대화 요약 과제에 대한 Rouge 점수에 대한 값 회전을 사용한 효과를 보여준다. 값 회전 기반 접근법에 대한 다운스트림 메트릭은 MSA 계층들의 상이한 설정들에 걸쳐 전용 스트림 임베딩을 사용하는 것보다 낮은 경향이 있지만, MSA 계층들이 추가된 메트릭의 증가 경향은 동일하게 유지된다. \\(N_{s}=16\\)의 경우, 전용 스트림 임베딩과 함께 \\(N_{s}=4\\)을 사용하는 것보다 단순히 회전 값 투영이 더 나은 메트릭을 달성한다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '*Top-k 샘플링** 본 논문에서는 탐욕 샘플링과 T=0을 사용하여 속도 향상 결과를 보고했으며, Top-k 샘플링 체제에서 속도 향상을 분석하기 위해 메두사 스타일과 투기적 스트리밍 접근법 모두에 대해 \\(k\\) 및 T=1의 다양한 값을 시도했다. 그림 9(b)는 \\(k\\)의 증가가 월타임 속도 향상 및 통화 감소율에 미치는 영향을 보여준다. 비록 \\(k\\)의 증가는 토큰의 확률적 거절로 인해 기준 방법과 목표 방법 모두에 대해 더 낮은 월-타임 속도 향상을 가져오지만, 우리의 접근법은 \\(k\\)의 다른 값들에 걸쳐 더 나은 콜 감소 비율 및 월-타임 속도 향상을 달성한다.\n' +
      '\n' +
      '## 부록 C 계산 및 기억 프로파일링\n' +
      '\n' +
      '표준 드래프트-타겟 추측 디코딩 접근법과 관련된 드래프트 오버헤드는 특히 타겟과 드래프트 모델들 사이의 레이턴시 비가 \\(c_{target}/c_{draft}<=10\\일 때 자명하지 않은 경향이 있다. 투기와 검증 절차가 연쇄적으로 진행되기 때문이다. 그림 10은 OPT-125m을 초안으로 사용한 반면 OPT-1.3b 모델을 대상으로 사용한 경우의 커널 활용 타임라인을 보여준다. 자동 회귀 드래프트 생성은 드래프트-타겟 접근법에서 전체 커널 활용률을 감소시키는 반면, MSA 계층에 포함된 추가 계산은 투기적 스트리밍의 경우 커널 활용률을 증가시켜 가속기를 효율적으로 활용하고 디코딩 프로세스를 빠르게 한다. 무시할 수 있는 비용 초안 모델은 초안 목표 접근법의 경우 커널 활용을 더 높은 수준으로 유지하기 위해 더 나은 선택을 제공할 수 있지만 초안 모델 크기가 감소함에 따라 수락률이 감소하는 경향이 있다.\n' +
      '\n' +
      '## 부록 D 정성적 예\n' +
      '\n' +
      '이 절에서는 사행 스트리밍의 효과를 설명하기 위한 정성적인 예를 제시한다. 특정 인스턴스들을 조사함으로써, 우리는 이 접근법이 디코딩 프로세스의 전체 성능을 향상시키는 방법을 강조하는 것을 목표로 한다. SQL 쿼리 생성 작업의 예는 도 11에 도시되어 있는 반면, 대화 요약 예는 도 12에 도시되어 있다. 각 행은 수락된 초안 토큰들의 이전 시퀀스(검정색) 및 생성된 토큰들의 새로운 시퀀스를 녹색/빨간색으로 나타낸다. 디코딩 과정을 설명하기 위해 \\(\\gamma=4\\)와 \\(k=1\\)을 사용한다. 각 행의 녹색 토큰은 다음 순방향 패스에서 승인된 토큰을 나타내는 반면, 적색 토큰은 다음 순방향 패스에서 거부된 토큰을 나타낸다. 투기적 스트리밍은 토큰 간의 종속성을 비자동 회귀 방식으로 생성함에도 불구하고 상당히 효과적으로 포착하여 수용률이 높은 의미 있는 초안을 생성하는 것으로 판단된다.\n' +
      '\n' +
      '그림 11: \\(\\gamma=4\\) 및 \\(k=1\\)에 대한 SQL 생성 태스크에 대한 투기 스트리밍은 각 패스가 이전 드래프트를 검증하고 최대 5개의 토큰을 생성한다. 예를 들어, 패스 4에서, "\\(credit\\)" 및 "_"(적색으로 표시됨)는 거절되고, "\\(hour\\)", "_", "_", "_", "_", "_"는 추측된다.\n' +
      '\n' +
      '그림 12: \\(\\gamma=4\\) 및 \\(k=1\\)에 대한 다이얼로그 요약 태스크 상의 투기적 스트리밍은 각 패스가 이전 드래프트를 검증하고 최대 5개의 토큰을 생성한다. 예를 들어, 패스 3에서 “\\(is\\)”, “\\(a\\)”, “\\(character\\)”는 거부되고 “\\(was\\)”, “\\(a\\)”, “\\(character\\)”, “\\(and\\)”, “\\(he\\)”는 추측된다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>