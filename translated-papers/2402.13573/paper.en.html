<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ToDo: Token Downsampling for Efficient Generation of High-Resolution Images\n' +
      '\n' +
      ' Ethan Smith\\({}^{1}\\)\n' +
      '\n' +
      'Nayan Saxena\\({}^{1}\\)\n' +
      '\n' +
      'Aninda Saha\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)Leonardo AI\n' +
      '\n' +
      '{ethan, nayan.saxena, aninda}@leonardo.ai\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Attention has been a crucial component in the success of image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like \\(2048\\times 2048\\). We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Transformers, and their key component, attention, have been integral to the success and improvements in generative models in recent years [23]. Their global receptive fields, ability to compute dynamically based on input context, and large capacities have made them useful building blocks across many tasks [14]. The main drawback of Transformer architectures is their quadratic scaling of computational complexity with sequence length, affecting both time and memory requirements. When looking to generate a Stable Diffusion image at \\(2048\\times 2048\\) resolution, the attention maps of the largest U-Net blocks incur a memory cost of approximately 69 GB in half-precision, calculated as (\\(1\\) batch\\(\\times 8\\) heads\\(\\times(256^{2}\\) tokens\\()^{2}\\times 2\\) bytes). This exceeds the capabilities of most consumer GPUs [15]. Specialized kernels, such as those used in Flash Attention, have greatly improved speed and reduced memory costs [11], however, challenges due to the unfavorable quadratic scaling with sequence length are persistent.\n' +
      '\n' +
      'In the quest for computational efficiency, the concept of sparse attention has gained traction. Methods like Token Merging (ToMe) [1] and its application in latent image diffusion models [1] have reduced the computation time required by condensing tokens with high similarity, thereby retaining the essence of the information with fewer tokens. Similarly, approaches like Neighborhood Attention [17] and Focal Transformers [21] have introduced mechanisms where query tokens attend only to a select neighborhood, balancing the trade-off between receptive field and computational load. These strategies aim to efficiently approximate the attention mechanism\'s output. While performant, these methods typically require training-time modifications to be successful, incurring significant logistical overheads to leverage their optimizations.\n' +
      '\n' +
      'Complementing the sparse attention frameworks, attention approximation methods offer an alternative avenue by exploiting mathematical properties to simplify the attention operation. Techniques ranging from replacing the softmax with more computationally friendly nonlinearities [3], to fully linearizing attention [1], and leveraging the kernel trick for dimensionality reduction [1], have been explored to approximate attention efficiently but are also generally required to be trained into the model.\n' +
      '\n' +
      'Building upon these works and aiming to address the pre-training requirement, we propose a novel post-hoc method for\n' +
      '\n' +
      'Figure 1: A visualization of our method. From a given latent or image, we subsample positions on the grid in a strided fashion for the keys and values used in attention maintaining the full set of query tokens. Link to demo video is here.\n' +
      '\n' +
      'accelerating inference, which we refer to as Token Downsampling (ToDo). Our approach, ToDo, is inspired by the observation that adjacent pixels in images tend to exhibit similar values to their neighbors. Hence, we employ a downsampling technique to reduce tokens, akin to grid-based subsampling in image processing. Compared to prior method ToMe [18], our method not only simplifies the merging process but also significantly reduces computational overhead, as it eliminates the need for exhaustive similarity calculations. In summary, our main contributions are:\n' +
      '\n' +
      '* A training-free method that can accelerate inference for Stable Diffusion up to 4.5x faster, beating previous methods in balancing throughput and fidelity.\n' +
      '* An in-depth analysis of attention features within the U-Net, and hypotheses on why attention can be approximated sparsely without substantially hurting fidelity.\n' +
      '\n' +
      '## 2 Methods\n' +
      '\n' +
      '### Background\n' +
      '\n' +
      'Diffusion Models for Image GenerationThe diffusion model [14] employs a U-Net architecture [15] with transformer-based blocks that utilize self-attention layers [13]. This setup flattens spatial dimensions into a series of tokens, which are then fed through multiple transformer blocks to predict the denoised image.\n' +
      '\n' +
      'Original Token Merging SchemeIn the original ToMe [18] framework, tokens are categorized into source (src) and destination (dst) sets. The merging process involves identifying the \\(r\\) most similar tokens from the src set and merging them into the dst set, effectively reducing the total token count by \\(r\\). This merging is defined as \\(x_{\\text{merged}}=\\frac{1}{r}\\sum_{i=1}^{r}x_{i}\\) where \\(x_{i}\\) represents individual tokens to be merged.\n' +
      '\n' +
      'Overall, the original ToMe method is predicated on the reduction of computational load through merging of similar tokens prior to being input to attention layers. This process involves the computation of a similarity matrix, where tokens exhibiting the highest similarity are merged. Subsequently, the unmerging process aims to redistribute the merged token information back to the original token locations. This approach, however, introduces two critical bottlenecks:\n' +
      '\n' +
      '* **Computational Complexity:** The similarity matrix calculation, \\(\\mathcal{O}(n^{2})\\) complexity, is costly in itself, especially when required at every step of the process.\n' +
      '* **Quality Degradation:** The merge-unmerge cycle inherent to ToMe can lead to significant loss of image detail, particularly at higher merging ratios.\n' +
      '\n' +
      '### Training Free Enhancements\n' +
      '\n' +
      'Our proposed token downsampling (ToDo) methodology extends the original ToMe approach, addressing its computational bottlenecks and quality degradation issues when applied to Stable Diffusion models. We introduce two principal modifications with ToDo: an optimized token merging method based on spatial contiguity and a refined attention mechanism that mitigates the need for unmerging.\n' +
      '\n' +
      'Optimized Merging Through Spatial ContiguityWe introduce a novel token merging strategy that leverages the inherent spatial contiguity of image tokens. Recognizing that tokens in close spatial proximity exhibit higher similarity, thus providing a basis for merging without the extensive computation of pairwise similarities. Therefore, we employ a downsampling function \\(D(\\cdot)\\) using the Nearest-Neighbor algorithm [1]. We note this approach is akin to strided convolutions, as shown in Figure 1. Formally, let \\(T=\\{t_{1},t_{2}\\dots t_{n}\\}\\) denote the original set of image tokens arranged in a two-dimensional grid reflecting their spatial relationships. The proposed downsampling operation, \\(D\\) is applied to \\(T\\) to yield a reduced set of merged tokens \\(T^{\\prime}\\), as such:\n' +
      '\n' +
      '\\[T^{\\prime}=D(T)=\\{D(t_{1}),D(t_{2})\\dots D(t_{m})\\}\\enspace,\\text{where }m<n\\]\n' +
      '\n' +
      'This enhancement mitigates the computational overhead associated with the pairwise similarity computation inherent in ToMe. By leveraging the assumption that spatially adjacent tokens are more likely to be similar, we bypass the need for \\(\\mathcal{O}(n^{2})\\) similarity calculations, instead employing a more computationally efficient \\(\\mathcal{O}(n)\\) downsampling operation.\n' +
      '\n' +
      'Enhanced Attention Mechanism with DownsamplingTo mitigate the information loss inherent to the unmerging process in conventional token merging approaches, we introduce a refinement to the attention mechanism within the transformer architecture [16]. This modification entails the application of the downsampling operation \\(D(\\cdot)\\) to the keys, \\(K\\), and values \\(V\\) of the attention mechanism while preserving the original queries \\(Q\\). The modified attention function can be mathematically articulated as follows, with \\(d_{k}\\) denoting the dimensionality of the keys, ensuring proper scaling within the softmax operation.\n' +
      '\n' +
      '\\[\\text{Attention}(Q,K,V)=\\text{softmax}\\bigg{(}\\frac{Q\\cdot D(K)^{T}}{\\sqrt{d_ {k}}}\\cdot D(V)\\bigg{)}\\]\n' +
      '\n' +
      'This refinement ensures that the integrity of the queries is preserved, thereby maintaining the fidelity of the attention process while reducing the dimensionality of the matrices involved in the attention computation.\n' +
      '\n' +
      '## 3 Experiments\n' +
      '\n' +
      'Experimental SetupFor our empirical evaluation, we employ the finetuned DreamshaperV7 model [15], noted for its superior handling of larger image dimensions which are central to this study. All experiments are conducted on a single A6000 GPU, utilizing float16 precision and flash attention [1] for inference as this has become the norm for many users. We use the DDIM sampler [14] with 50 diffusion steps and a guidance scale of 7.5 [13]. Each experiment involves averaging 10 generations comparing ToDo against ToMe with baseline referring to standard generations without token merging. The resolutions benchmarked include: \\(1024\\times 1024\\), \\(1536\\times 1536\\) and \\(2048\\times 2048\\) across two token merging ratios, 0.75 and 0.89 which denotes the proportion of tokens removed. This is equivalent to 2x and 3x downsample respectively. For the comparison in Figure 2 we also use a merge ratio of 0.9375 for the \\(2048\\times 2048\\) images, equivalent to a 4x downsample.\n' +
      '\n' +
      'Image Quality and ThroughputTo assess the fidelity and detail preservation of generated images, we utilized Mean Squared Error (MSE) to quantify each method\'s deviation from the baseline, and High Pass Filter (HPF) a standard for evaluating image sharpness and texture preservation [11]. Our analysis, substantiated by Figure 2 and Table 1, demonstrates that our method not only closely mirrors the baseline in terms of MSE but also maintains comparable HPF values, underscoring its efficiency in retaining image features while ensuring higher throughput, as depicted in Figure 3.\n' +
      '\n' +
      'Latent Feature RedundancyWe investigated latent feature redundancy in the Stable Diffusion U-Net, assessing similarity among adjacent latent features. By extracting latent representations at various stages and noise levels, we constructed cosine similarity matrices, focusing on the proportion of tokens with top-3 similarities within a \\(3\\times 3\\) area, and the highest, mean, and lowest similarities within \\(3\\times 3\\) and \\(5\\times 5\\) areas. We observed high similarity among neighboring tokens within the hidden features and notable trends as seen in Figure 4. Similarity trends varied across different depths without a distinct pattern, possibly due to the increasing spatial compression and consequent reduction in information redundancy with values diminishing as the denoising progresses, likely because diffusion models initially generate broad details and later refine them.\n' +
      '\n' +
      '## 4 Conclusion\n' +
      '\n' +
      'We demonstrate that our approach ToDo is capable of maintaining the balance between efficient throughput and fidelity, especially in high-frequency components.We also show that nearby features within the U-Net might be redundant and postulate that our method can benefit other attention based generative image models, especially those operating on a large number of tokens. Future work can explore the differentiability of our method, and leverage it to efficiently finetune Stable Diffusion at previously unseen larger image dimensions.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Method** & **Merge Ratio** & **MSE** & **HPF** \\\\ \\hline _Baseline_ & - & - & _4.846_ \\\\ ToMe & 0.75 & \\(2.686\\times 10^{-2}\\) & 4.022 \\\\  & 0.89 & \\(2.671\\times 10^{-2}\\) & 4.003 \\\\ ToDo (ours) & 0.75 & \\(6.247\\times 10^{-3}\\) & **4.887** \\\\  & 0.89 & \\(9.207\\times 10^{-3}\\) & **4.733** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: Metrics from various attention methods, averaged over 10 generations of different prompts at \\(1536\\times 1536\\) resolution. MSE denotes the mean squared error relative to the baseline, while HPF represents the mean absolute magnitude post-high pass filtering.\n' +
      '\n' +
      'Figure 4: Lowest cosine similarity between tokens in a \\(3\\times 3\\) area across diffusion timesteps and U-Net locations extracted from 10 generations of different prompts at \\(1024\\times 1024\\). Timesteps out of 50 indicate noise reduction; Depth 0 is initial resolution, Depth 1 is after 2x downsampling. Up/down denotes encoder/decoder blocks.\n' +
      '\n' +
      'Figure 3: Inference throughput across resolutions using different attention methods at various merge ratios, with bars representing the relative performance increase against the baseline.\n' +
      '\n' +
      'Figure 2: Qualitative comparison of attention methods with: 25% of tokens at \\(1024\\times 1024\\), 11% at \\(1536\\times 1536\\), and 6% at \\(2048\\times 2048\\), maintaining a consistent token count of 4096 post-merging.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1]I. Bankman (2008) Handbook of medical image processing and analysis. Elsevier. Cited by: SS1.\n' +
      '* [2]D. Bolya and J. Hoffman (2023) Token merging for fast stable diffusion. CVPR Workshop on Efficient Deep Learning for Computer Vision. Cited by: SS1.\n' +
      '* [3]D. Bolya, C. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman (2023) Token merging: your viet but faster. In International Conference on Learning Representations, Cited by: SS1.\n' +
      '* [4]D. Chen, J. Li, and K. Xu (2020) Arelu: attention-based rectified linear unit. Cited by: SS1.\n' +
      '* [5]K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. Colwell, and A. Weller (2022) Rethinking attention with performers. Cited by: SS1.\n' +
      '* [6]T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re (2022) FlashAttention: fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, Cited by: SS1.\n' +
      '* [7]R. C. Gonzalez (2009) Digital image processing. Pearson education india. Cited by: SS1.\n' +
      '* [8]A. Hassani, S. Walton, J. Li, S. Li, and H. Shi (2023) Neighborhood attention transformer. Cited by: SS1.\n' +
      '* [9]A. Katharopoulos, A. V. V. Pappas, N. Pappas, and F. Fleuret (2020) Transformers are rnns: fast autoregressive transformers with linear attention. Cited by: SS1.\n' +
      '* [10]S. Khan, M. Naseer, M. Hayat, S. Waqas Zamir, F. Shahbaz Khan, and M. Shah (2022-07) Transformers in vision: a survey. ACM Computing Surveys54 (10s), pp. 1-41. Cited by: SS1.\n' +
      '* [11]S. Luo, Y. Tan, L. Huang, J. Li, and H. Zhao (2023) Latent consistency models: synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378. Cited by: SS1.\n' +
      '* [12]R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2021) High-resolution image synthesis with latent diffusion models. Cited by: SS1.\n' +
      '* [13]O. Ronneberger, P. Fischer, and T. Brox (2015) U-net: convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234-241. Cited by: SS1.\n' +
      '* [14]Y. Song and S. Ermon (2019) Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems32. Cited by: SS1.\n' +
      '* [15]J. Song, C. Meng, and S. Ermon (2020-10) Denoising diffusion implicit models. arXiv:2010.02502. Cited by: SS1.\n' +
      '* [16]H. T. Team (2024) Speed up inference. Cited by: SS1.\n' +
      '* [17]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2023) Attention is all you need. Cited by: SS1.\n' +
      '* [18]J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao (2021) Focal self-attention for local-global interactions in vision transformers. CoRRabs/2107.00641. Cited by: SS1.\n' +
      '* [19]B. Zhuang, J. Liu, Z. Pan, H. He, Y. Weng, and C. Shen (2023) A survey on efficient training of transformers. arXiv preprint arXiv:2302.01107. Cited by: SS1.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>